[INFO 2017-06-27 20:02:15,523 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=25, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/youtube-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/youtube-64', train_epoch=10000)
[INFO 2017-06-27 20:02:17,912 main.py:51] epoch 0, training loss: 25110.11, average training loss: 25110.11, base loss: 3228.54
[INFO 2017-06-27 20:02:18,229 main.py:51] epoch 1, training loss: 20493.00, average training loss: 22801.56, base loss: 3832.49
[INFO 2017-06-27 20:02:18,527 main.py:51] epoch 2, training loss: 18647.65, average training loss: 21416.92, base loss: 4301.72
[INFO 2017-06-27 20:02:18,838 main.py:51] epoch 3, training loss: 14721.04, average training loss: 19742.95, base loss: 4361.70
[INFO 2017-06-27 20:02:19,150 main.py:51] epoch 4, training loss: 13644.51, average training loss: 18523.26, base loss: 4420.65
[INFO 2017-06-27 20:02:19,453 main.py:51] epoch 5, training loss: 11815.91, average training loss: 17405.37, base loss: 4330.50
[INFO 2017-06-27 20:02:19,769 main.py:51] epoch 6, training loss: 10289.07, average training loss: 16388.76, base loss: 4319.28
[INFO 2017-06-27 20:02:20,074 main.py:51] epoch 7, training loss: 9088.94, average training loss: 15476.28, base loss: 4209.41
[INFO 2017-06-27 20:02:20,386 main.py:51] epoch 8, training loss: 8877.16, average training loss: 14743.04, base loss: 4154.39
[INFO 2017-06-27 20:02:20,690 main.py:51] epoch 9, training loss: 7818.26, average training loss: 14050.57, base loss: 4070.22
[INFO 2017-06-27 20:02:20,992 main.py:51] epoch 10, training loss: 8303.61, average training loss: 13528.11, base loss: 4086.18
[INFO 2017-06-27 20:02:21,301 main.py:51] epoch 11, training loss: 7773.05, average training loss: 13048.53, base loss: 4090.35
[INFO 2017-06-27 20:02:21,604 main.py:51] epoch 12, training loss: 7776.77, average training loss: 12643.01, base loss: 4103.60
[INFO 2017-06-27 20:02:21,912 main.py:51] epoch 13, training loss: 6411.90, average training loss: 12197.93, base loss: 4061.00
[INFO 2017-06-27 20:02:22,221 main.py:51] epoch 14, training loss: 6247.34, average training loss: 11801.22, base loss: 4024.53
[INFO 2017-06-27 20:02:22,525 main.py:51] epoch 15, training loss: 7600.26, average training loss: 11538.66, base loss: 4094.49
[INFO 2017-06-27 20:02:22,836 main.py:51] epoch 16, training loss: 6740.97, average training loss: 11256.44, base loss: 4098.55
[INFO 2017-06-27 20:02:23,147 main.py:51] epoch 17, training loss: 6953.06, average training loss: 11017.37, base loss: 4131.39
[INFO 2017-06-27 20:02:23,450 main.py:51] epoch 18, training loss: 6287.21, average training loss: 10768.41, base loss: 4135.81
[INFO 2017-06-27 20:02:23,755 main.py:51] epoch 19, training loss: 5740.49, average training loss: 10517.02, base loss: 4120.37
[INFO 2017-06-27 20:02:24,056 main.py:51] epoch 20, training loss: 6048.88, average training loss: 10304.25, base loss: 4124.83
[INFO 2017-06-27 20:02:24,364 main.py:51] epoch 21, training loss: 5326.87, average training loss: 10078.00, base loss: 4101.07
[INFO 2017-06-27 20:02:24,678 main.py:51] epoch 22, training loss: 5813.35, average training loss: 9892.58, base loss: 4108.96
[INFO 2017-06-27 20:02:24,993 main.py:51] epoch 23, training loss: 5245.58, average training loss: 9698.96, base loss: 4086.49
[INFO 2017-06-27 20:02:25,308 main.py:51] epoch 24, training loss: 5023.74, average training loss: 9511.95, base loss: 4079.27
[INFO 2017-06-27 20:02:25,613 main.py:51] epoch 25, training loss: 4802.28, average training loss: 9330.81, base loss: 4058.80
[INFO 2017-06-27 20:02:25,927 main.py:51] epoch 26, training loss: 5626.59, average training loss: 9193.61, base loss: 4077.01
[INFO 2017-06-27 20:02:26,240 main.py:51] epoch 27, training loss: 5212.05, average training loss: 9051.42, base loss: 4083.62
[INFO 2017-06-27 20:02:26,545 main.py:51] epoch 28, training loss: 4432.89, average training loss: 8892.16, base loss: 4063.66
[INFO 2017-06-27 20:02:26,861 main.py:51] epoch 29, training loss: 8587.80, average training loss: 8882.01, base loss: 4183.51
[INFO 2017-06-27 20:02:27,168 main.py:51] epoch 30, training loss: 4693.15, average training loss: 8746.89, base loss: 4168.28
[INFO 2017-06-27 20:02:27,474 main.py:51] epoch 31, training loss: 4183.67, average training loss: 8604.29, base loss: 4147.02
[INFO 2017-06-27 20:02:27,779 main.py:51] epoch 32, training loss: 4428.12, average training loss: 8477.74, base loss: 4121.99
[INFO 2017-06-27 20:02:28,098 main.py:51] epoch 33, training loss: 4732.54, average training loss: 8367.58, base loss: 4112.58
[INFO 2017-06-27 20:02:28,405 main.py:51] epoch 34, training loss: 4161.21, average training loss: 8247.40, base loss: 4094.11
[INFO 2017-06-27 20:02:28,710 main.py:51] epoch 35, training loss: 4182.09, average training loss: 8134.48, base loss: 4079.80
[INFO 2017-06-27 20:02:29,017 main.py:51] epoch 36, training loss: 4719.77, average training loss: 8042.19, base loss: 4080.64
[INFO 2017-06-27 20:02:29,325 main.py:51] epoch 37, training loss: 4912.07, average training loss: 7959.82, base loss: 4086.68
[INFO 2017-06-27 20:02:29,630 main.py:51] epoch 38, training loss: 4079.16, average training loss: 7860.31, base loss: 4071.11
[INFO 2017-06-27 20:02:29,942 main.py:51] epoch 39, training loss: 4557.45, average training loss: 7777.74, base loss: 4071.86
[INFO 2017-06-27 20:02:30,251 main.py:51] epoch 40, training loss: 8232.66, average training loss: 7788.84, base loss: 4160.75
[INFO 2017-06-27 20:02:30,557 main.py:51] epoch 41, training loss: 8399.76, average training loss: 7803.38, base loss: 4251.55
[INFO 2017-06-27 20:02:30,865 main.py:51] epoch 42, training loss: 3349.48, average training loss: 7699.80, base loss: 4222.35
[INFO 2017-06-27 20:02:31,183 main.py:51] epoch 43, training loss: 3896.94, average training loss: 7613.37, base loss: 4205.97
[INFO 2017-06-27 20:02:31,494 main.py:51] epoch 44, training loss: 4659.25, average training loss: 7547.73, base loss: 4207.67
[INFO 2017-06-27 20:02:31,804 main.py:51] epoch 45, training loss: 7875.10, average training loss: 7554.84, base loss: 4280.94
[INFO 2017-06-27 20:02:32,119 main.py:51] epoch 46, training loss: 4412.09, average training loss: 7487.98, base loss: 4276.45
[INFO 2017-06-27 20:02:32,431 main.py:51] epoch 47, training loss: 4601.03, average training loss: 7427.83, base loss: 4277.93
[INFO 2017-06-27 20:02:32,742 main.py:51] epoch 48, training loss: 5209.86, average training loss: 7382.57, base loss: 4292.77
[INFO 2017-06-27 20:02:33,055 main.py:51] epoch 49, training loss: 3945.99, average training loss: 7313.83, base loss: 4280.89
[INFO 2017-06-27 20:02:33,367 main.py:51] epoch 50, training loss: 4749.04, average training loss: 7263.54, base loss: 4286.04
[INFO 2017-06-27 20:02:33,681 main.py:51] epoch 51, training loss: 4102.25, average training loss: 7202.75, base loss: 4277.85
[INFO 2017-06-27 20:02:33,991 main.py:51] epoch 52, training loss: 3599.82, average training loss: 7134.77, base loss: 4261.12
[INFO 2017-06-27 20:02:34,298 main.py:51] epoch 53, training loss: 4753.08, average training loss: 7090.67, base loss: 4266.98
[INFO 2017-06-27 20:02:34,613 main.py:51] epoch 54, training loss: 3668.34, average training loss: 7028.44, base loss: 4252.96
[INFO 2017-06-27 20:02:34,926 main.py:51] epoch 55, training loss: 3975.82, average training loss: 6973.93, base loss: 4244.68
[INFO 2017-06-27 20:02:35,264 main.py:51] epoch 56, training loss: 4557.44, average training loss: 6931.54, base loss: 4248.09
[INFO 2017-06-27 20:02:35,591 main.py:51] epoch 57, training loss: 8009.35, average training loss: 6950.12, base loss: 4310.87
[INFO 2017-06-27 20:02:35,909 main.py:51] epoch 58, training loss: 7420.61, average training loss: 6958.09, base loss: 4361.64
[INFO 2017-06-27 20:02:36,222 main.py:51] epoch 59, training loss: 3181.46, average training loss: 6895.15, base loss: 4339.65
[INFO 2017-06-27 20:02:36,535 main.py:51] epoch 60, training loss: 3939.68, average training loss: 6846.70, base loss: 4330.99
[INFO 2017-06-27 20:02:36,847 main.py:51] epoch 61, training loss: 3904.22, average training loss: 6799.24, base loss: 4322.17
[INFO 2017-06-27 20:02:37,160 main.py:51] epoch 62, training loss: 4792.84, average training loss: 6767.39, base loss: 4328.89
[INFO 2017-06-27 20:02:37,469 main.py:51] epoch 63, training loss: 3585.36, average training loss: 6717.67, base loss: 4315.75
[INFO 2017-06-27 20:02:37,778 main.py:51] epoch 64, training loss: 3987.12, average training loss: 6675.66, base loss: 4309.93
[INFO 2017-06-27 20:02:38,087 main.py:51] epoch 65, training loss: 4052.75, average training loss: 6635.92, base loss: 4305.03
[INFO 2017-06-27 20:02:38,399 main.py:51] epoch 66, training loss: 3901.09, average training loss: 6595.10, base loss: 4297.69
[INFO 2017-06-27 20:02:38,705 main.py:51] epoch 67, training loss: 4424.34, average training loss: 6563.18, base loss: 4299.14
[INFO 2017-06-27 20:02:39,018 main.py:51] epoch 68, training loss: 3960.75, average training loss: 6525.46, base loss: 4293.24
[INFO 2017-06-27 20:02:39,322 main.py:51] epoch 69, training loss: 3647.91, average training loss: 6484.36, base loss: 4283.35
[INFO 2017-06-27 20:02:39,629 main.py:51] epoch 70, training loss: 4349.47, average training loss: 6454.29, base loss: 4283.85
[INFO 2017-06-27 20:02:39,936 main.py:51] epoch 71, training loss: 3964.11, average training loss: 6419.70, base loss: 4278.43
[INFO 2017-06-27 20:02:40,248 main.py:51] epoch 72, training loss: 4188.81, average training loss: 6389.14, base loss: 4276.48
[INFO 2017-06-27 20:02:40,561 main.py:51] epoch 73, training loss: 4287.45, average training loss: 6360.74, base loss: 4276.00
[INFO 2017-06-27 20:02:40,873 main.py:51] epoch 74, training loss: 3948.21, average training loss: 6328.57, base loss: 4270.94
[INFO 2017-06-27 20:02:41,184 main.py:51] epoch 75, training loss: 4974.62, average training loss: 6310.76, base loss: 4280.00
[INFO 2017-06-27 20:02:41,496 main.py:51] epoch 76, training loss: 4366.34, average training loss: 6285.51, base loss: 4281.04
[INFO 2017-06-27 20:02:41,807 main.py:51] epoch 77, training loss: 4287.55, average training loss: 6259.89, base loss: 4280.91
[INFO 2017-06-27 20:02:42,109 main.py:51] epoch 78, training loss: 3787.58, average training loss: 6228.60, base loss: 4274.13
[INFO 2017-06-27 20:02:42,417 main.py:51] epoch 79, training loss: 4571.06, average training loss: 6207.88, base loss: 4277.78
[INFO 2017-06-27 20:02:42,734 main.py:51] epoch 80, training loss: 4292.56, average training loss: 6184.23, base loss: 4277.84
[INFO 2017-06-27 20:02:43,047 main.py:51] epoch 81, training loss: 3408.73, average training loss: 6150.38, base loss: 4266.77
[INFO 2017-06-27 20:02:43,356 main.py:51] epoch 82, training loss: 4449.29, average training loss: 6129.89, base loss: 4268.71
[INFO 2017-06-27 20:02:43,662 main.py:51] epoch 83, training loss: 3586.03, average training loss: 6099.60, base loss: 4260.16
[INFO 2017-06-27 20:02:43,979 main.py:51] epoch 84, training loss: 4086.75, average training loss: 6075.92, base loss: 4258.08
[INFO 2017-06-27 20:02:44,298 main.py:51] epoch 85, training loss: 3792.08, average training loss: 6049.37, base loss: 4252.65
[INFO 2017-06-27 20:02:44,613 main.py:51] epoch 86, training loss: 3377.23, average training loss: 6018.65, base loss: 4242.40
[INFO 2017-06-27 20:02:44,923 main.py:51] epoch 87, training loss: 3368.04, average training loss: 5988.53, base loss: 4231.94
[INFO 2017-06-27 20:02:45,229 main.py:51] epoch 88, training loss: 4450.75, average training loss: 5971.25, base loss: 4234.60
[INFO 2017-06-27 20:02:45,531 main.py:51] epoch 89, training loss: 3926.03, average training loss: 5948.53, base loss: 4231.14
[INFO 2017-06-27 20:02:45,848 main.py:51] epoch 90, training loss: 3829.06, average training loss: 5925.24, base loss: 4226.60
[INFO 2017-06-27 20:02:46,169 main.py:51] epoch 91, training loss: 5101.17, average training loss: 5916.28, base loss: 4236.71
[INFO 2017-06-27 20:02:46,482 main.py:51] epoch 92, training loss: 4119.48, average training loss: 5896.96, base loss: 4235.46
[INFO 2017-06-27 20:02:46,791 main.py:51] epoch 93, training loss: 3732.91, average training loss: 5873.94, base loss: 4230.02
[INFO 2017-06-27 20:02:47,106 main.py:51] epoch 94, training loss: 3431.35, average training loss: 5848.23, base loss: 4221.43
[INFO 2017-06-27 20:02:47,421 main.py:51] epoch 95, training loss: 4101.46, average training loss: 5830.03, base loss: 4220.60
[INFO 2017-06-27 20:02:47,734 main.py:51] epoch 96, training loss: 4581.88, average training loss: 5817.16, base loss: 4224.96
[INFO 2017-06-27 20:02:48,052 main.py:51] epoch 97, training loss: 3809.62, average training loss: 5796.68, base loss: 4220.98
[INFO 2017-06-27 20:02:48,354 main.py:51] epoch 98, training loss: 3822.70, average training loss: 5776.74, base loss: 4217.27
[INFO 2017-06-27 20:02:48,669 main.py:51] epoch 99, training loss: 4213.03, average training loss: 5761.10, base loss: 4217.51
[INFO 2017-06-27 20:02:48,669 main.py:53] epoch 99, testing
[INFO 2017-06-27 20:02:50,058 main.py:105] average testing loss: 3790.43, base loss: 3808.88
[INFO 2017-06-27 20:02:50,058 main.py:106] improve_loss: 18.45, improve_percent: 0.00
[INFO 2017-06-27 20:02:50,059 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:02:50,071 main.py:76] current best improved percent: 0.00
[INFO 2017-06-27 20:02:50,386 main.py:51] epoch 100, training loss: 4320.04, average training loss: 5746.84, base loss: 4218.90
[INFO 2017-06-27 20:02:50,699 main.py:51] epoch 101, training loss: 3690.44, average training loss: 5726.67, base loss: 4213.67
[INFO 2017-06-27 20:02:51,015 main.py:51] epoch 102, training loss: 4092.44, average training loss: 5710.81, base loss: 4212.91
[INFO 2017-06-27 20:02:51,325 main.py:51] epoch 103, training loss: 3657.54, average training loss: 5691.07, base loss: 4207.79
[INFO 2017-06-27 20:02:51,631 main.py:51] epoch 104, training loss: 4979.72, average training loss: 5684.29, base loss: 4215.61
[INFO 2017-06-27 20:02:51,941 main.py:51] epoch 105, training loss: 4019.22, average training loss: 5668.58, base loss: 4214.06
[INFO 2017-06-27 20:02:52,243 main.py:51] epoch 106, training loss: 4058.48, average training loss: 5653.53, base loss: 4212.84
[INFO 2017-06-27 20:02:52,553 main.py:51] epoch 107, training loss: 5139.98, average training loss: 5648.78, base loss: 4222.40
[INFO 2017-06-27 20:02:52,860 main.py:51] epoch 108, training loss: 3626.88, average training loss: 5630.23, base loss: 4217.13
[INFO 2017-06-27 20:02:53,184 main.py:51] epoch 109, training loss: 4511.93, average training loss: 5620.06, base loss: 4220.35
[INFO 2017-06-27 20:02:53,495 main.py:51] epoch 110, training loss: 5242.63, average training loss: 5616.66, base loss: 4230.32
[INFO 2017-06-27 20:02:53,811 main.py:51] epoch 111, training loss: 4474.52, average training loss: 5606.47, base loss: 4233.00
[INFO 2017-06-27 20:02:54,126 main.py:51] epoch 112, training loss: 4293.12, average training loss: 5594.84, base loss: 4234.10
[INFO 2017-06-27 20:02:54,441 main.py:51] epoch 113, training loss: 4181.55, average training loss: 5582.45, base loss: 4234.15
[INFO 2017-06-27 20:02:54,755 main.py:51] epoch 114, training loss: 5203.86, average training loss: 5579.15, base loss: 4243.44
[INFO 2017-06-27 20:02:55,069 main.py:51] epoch 115, training loss: 3271.89, average training loss: 5559.26, base loss: 4235.09
[INFO 2017-06-27 20:02:55,380 main.py:51] epoch 116, training loss: 3506.90, average training loss: 5541.72, base loss: 4228.90
[INFO 2017-06-27 20:02:55,699 main.py:51] epoch 117, training loss: 4197.14, average training loss: 5530.33, base loss: 4229.05
[INFO 2017-06-27 20:02:56,014 main.py:51] epoch 118, training loss: 3547.45, average training loss: 5513.66, base loss: 4223.59
[INFO 2017-06-27 20:02:56,328 main.py:51] epoch 119, training loss: 4681.77, average training loss: 5506.73, base loss: 4228.08
[INFO 2017-06-27 20:02:56,641 main.py:51] epoch 120, training loss: 4218.49, average training loss: 5496.09, base loss: 4228.62
[INFO 2017-06-27 20:02:56,955 main.py:51] epoch 121, training loss: 3603.63, average training loss: 5480.57, base loss: 4223.76
[INFO 2017-06-27 20:02:57,266 main.py:51] epoch 122, training loss: 3294.58, average training loss: 5462.80, base loss: 4216.45
[INFO 2017-06-27 20:02:57,582 main.py:51] epoch 123, training loss: 3688.59, average training loss: 5448.49, base loss: 4212.30
[INFO 2017-06-27 20:02:57,895 main.py:51] epoch 124, training loss: 4124.75, average training loss: 5437.90, base loss: 4212.24
[INFO 2017-06-27 20:02:58,209 main.py:51] epoch 125, training loss: 4244.40, average training loss: 5428.43, base loss: 4213.09
[INFO 2017-06-27 20:02:58,527 main.py:51] epoch 126, training loss: 3797.21, average training loss: 5415.59, base loss: 4210.16
[INFO 2017-06-27 20:02:58,842 main.py:51] epoch 127, training loss: 3982.13, average training loss: 5404.39, base loss: 4208.91
[INFO 2017-06-27 20:02:59,156 main.py:51] epoch 128, training loss: 5190.83, average training loss: 5402.73, base loss: 4217.61
[INFO 2017-06-27 20:02:59,475 main.py:51] epoch 129, training loss: 4624.47, average training loss: 5396.75, base loss: 4221.51
[INFO 2017-06-27 20:02:59,788 main.py:51] epoch 130, training loss: 4720.19, average training loss: 5391.58, base loss: 4226.09
[INFO 2017-06-27 20:03:00,106 main.py:51] epoch 131, training loss: 3914.50, average training loss: 5380.39, base loss: 4224.17
[INFO 2017-06-27 20:03:00,420 main.py:51] epoch 132, training loss: 4815.89, average training loss: 5376.15, base loss: 4229.47
[INFO 2017-06-27 20:03:00,738 main.py:51] epoch 133, training loss: 3669.86, average training loss: 5363.41, base loss: 4225.64
[INFO 2017-06-27 20:03:01,054 main.py:51] epoch 134, training loss: 4439.58, average training loss: 5356.57, base loss: 4227.84
[INFO 2017-06-27 20:03:01,368 main.py:51] epoch 135, training loss: 4297.28, average training loss: 5348.78, base loss: 4228.88
[INFO 2017-06-27 20:03:01,687 main.py:51] epoch 136, training loss: 5298.06, average training loss: 5348.41, base loss: 4237.72
[INFO 2017-06-27 20:03:02,002 main.py:51] epoch 137, training loss: 4429.03, average training loss: 5341.75, base loss: 4239.54
[INFO 2017-06-27 20:03:02,318 main.py:51] epoch 138, training loss: 4108.20, average training loss: 5332.87, base loss: 4239.10
[INFO 2017-06-27 20:03:02,636 main.py:51] epoch 139, training loss: 7452.97, average training loss: 5348.02, base loss: 4262.65
[INFO 2017-06-27 20:03:02,948 main.py:51] epoch 140, training loss: 4087.38, average training loss: 5339.08, base loss: 4261.93
[INFO 2017-06-27 20:03:03,263 main.py:51] epoch 141, training loss: 3740.43, average training loss: 5327.82, base loss: 4258.58
[INFO 2017-06-27 20:03:03,580 main.py:51] epoch 142, training loss: 3648.90, average training loss: 5316.08, base loss: 4254.77
[INFO 2017-06-27 20:03:03,894 main.py:51] epoch 143, training loss: 3782.82, average training loss: 5305.43, base loss: 4251.87
[INFO 2017-06-27 20:03:04,209 main.py:51] epoch 144, training loss: 4250.06, average training loss: 5298.15, base loss: 4252.48
[INFO 2017-06-27 20:03:04,518 main.py:51] epoch 145, training loss: 4288.94, average training loss: 5291.24, base loss: 4253.47
[INFO 2017-06-27 20:03:04,832 main.py:51] epoch 146, training loss: 3758.63, average training loss: 5280.81, base loss: 4250.24
[INFO 2017-06-27 20:03:05,149 main.py:51] epoch 147, training loss: 6905.16, average training loss: 5291.79, base loss: 4268.62
[INFO 2017-06-27 20:03:05,462 main.py:51] epoch 148, training loss: 4188.86, average training loss: 5284.39, base loss: 4268.91
[INFO 2017-06-27 20:03:05,775 main.py:51] epoch 149, training loss: 3917.31, average training loss: 5275.27, base loss: 4267.10
[INFO 2017-06-27 20:03:06,108 main.py:51] epoch 150, training loss: 3826.55, average training loss: 5265.68, base loss: 4264.86
[INFO 2017-06-27 20:03:06,421 main.py:51] epoch 151, training loss: 3210.44, average training loss: 5252.16, base loss: 4258.24
[INFO 2017-06-27 20:03:06,734 main.py:51] epoch 152, training loss: 3751.89, average training loss: 5242.35, base loss: 4255.33
[INFO 2017-06-27 20:03:07,052 main.py:51] epoch 153, training loss: 3842.48, average training loss: 5233.26, base loss: 4253.22
[INFO 2017-06-27 20:03:07,366 main.py:51] epoch 154, training loss: 4223.82, average training loss: 5226.75, base loss: 4253.96
[INFO 2017-06-27 20:03:07,680 main.py:51] epoch 155, training loss: 4730.79, average training loss: 5223.57, base loss: 4258.08
[INFO 2017-06-27 20:03:07,993 main.py:51] epoch 156, training loss: 4074.91, average training loss: 5216.25, base loss: 4257.64
[INFO 2017-06-27 20:03:08,307 main.py:51] epoch 157, training loss: 4081.26, average training loss: 5209.07, base loss: 4257.02
[INFO 2017-06-27 20:03:08,621 main.py:51] epoch 158, training loss: 3816.07, average training loss: 5200.31, base loss: 4254.65
[INFO 2017-06-27 20:03:08,933 main.py:51] epoch 159, training loss: 4516.57, average training loss: 5196.04, base loss: 4257.29
[INFO 2017-06-27 20:03:09,248 main.py:51] epoch 160, training loss: 3436.30, average training loss: 5185.11, base loss: 4252.77
[INFO 2017-06-27 20:03:09,562 main.py:51] epoch 161, training loss: 4686.18, average training loss: 5182.03, base loss: 4255.95
[INFO 2017-06-27 20:03:09,875 main.py:51] epoch 162, training loss: 4582.25, average training loss: 5178.35, base loss: 4258.96
[INFO 2017-06-27 20:03:10,188 main.py:51] epoch 163, training loss: 3684.20, average training loss: 5169.24, base loss: 4255.66
[INFO 2017-06-27 20:03:10,508 main.py:51] epoch 164, training loss: 3869.87, average training loss: 5161.36, base loss: 4253.58
[INFO 2017-06-27 20:03:10,821 main.py:51] epoch 165, training loss: 3135.93, average training loss: 5149.16, base loss: 4247.18
[INFO 2017-06-27 20:03:11,136 main.py:51] epoch 166, training loss: 3709.25, average training loss: 5140.54, base loss: 4244.32
[INFO 2017-06-27 20:03:11,452 main.py:51] epoch 167, training loss: 3713.65, average training loss: 5132.04, base loss: 4241.67
[INFO 2017-06-27 20:03:11,769 main.py:51] epoch 168, training loss: 3951.38, average training loss: 5125.06, base loss: 4240.71
[INFO 2017-06-27 20:03:12,082 main.py:51] epoch 169, training loss: 4145.35, average training loss: 5119.29, base loss: 4240.80
[INFO 2017-06-27 20:03:12,403 main.py:51] epoch 170, training loss: 3575.56, average training loss: 5110.27, base loss: 4237.32
[INFO 2017-06-27 20:03:12,717 main.py:51] epoch 171, training loss: 4322.64, average training loss: 5105.69, base loss: 4238.33
[INFO 2017-06-27 20:03:13,035 main.py:51] epoch 172, training loss: 3940.69, average training loss: 5098.95, base loss: 4237.27
[INFO 2017-06-27 20:03:13,349 main.py:51] epoch 173, training loss: 4022.84, average training loss: 5092.77, base loss: 4236.61
[INFO 2017-06-27 20:03:13,668 main.py:51] epoch 174, training loss: 3942.53, average training loss: 5086.20, base loss: 4235.50
[INFO 2017-06-27 20:03:13,984 main.py:51] epoch 175, training loss: 7699.06, average training loss: 5101.04, base loss: 4255.69
[INFO 2017-06-27 20:03:14,304 main.py:51] epoch 176, training loss: 4894.89, average training loss: 5099.88, base loss: 4260.31
[INFO 2017-06-27 20:03:14,620 main.py:51] epoch 177, training loss: 3093.34, average training loss: 5088.60, base loss: 4254.07
[INFO 2017-06-27 20:03:14,935 main.py:51] epoch 178, training loss: 2984.27, average training loss: 5076.85, base loss: 4247.30
[INFO 2017-06-27 20:03:15,246 main.py:51] epoch 179, training loss: 3212.32, average training loss: 5066.49, base loss: 4242.12
[INFO 2017-06-27 20:03:15,561 main.py:51] epoch 180, training loss: 3382.78, average training loss: 5057.19, base loss: 4237.59
[INFO 2017-06-27 20:03:15,874 main.py:51] epoch 181, training loss: 7284.85, average training loss: 5069.43, base loss: 4254.66
[INFO 2017-06-27 20:03:16,196 main.py:51] epoch 182, training loss: 3947.90, average training loss: 5063.30, base loss: 4253.43
[INFO 2017-06-27 20:03:16,515 main.py:51] epoch 183, training loss: 4995.11, average training loss: 5062.93, base loss: 4258.46
[INFO 2017-06-27 20:03:16,834 main.py:51] epoch 184, training loss: 3729.26, average training loss: 5055.72, base loss: 4256.12
[INFO 2017-06-27 20:03:17,153 main.py:51] epoch 185, training loss: 3953.46, average training loss: 5049.79, base loss: 4255.24
[INFO 2017-06-27 20:03:17,470 main.py:51] epoch 186, training loss: 3836.19, average training loss: 5043.30, base loss: 4253.43
[INFO 2017-06-27 20:03:17,787 main.py:51] epoch 187, training loss: 4629.28, average training loss: 5041.10, base loss: 4256.22
[INFO 2017-06-27 20:03:18,102 main.py:51] epoch 188, training loss: 3673.04, average training loss: 5033.86, base loss: 4253.50
[INFO 2017-06-27 20:03:18,418 main.py:51] epoch 189, training loss: 4043.50, average training loss: 5028.65, base loss: 4253.22
[INFO 2017-06-27 20:03:18,733 main.py:51] epoch 190, training loss: 4106.70, average training loss: 5023.82, base loss: 4253.30
[INFO 2017-06-27 20:03:19,047 main.py:51] epoch 191, training loss: 3953.11, average training loss: 5018.25, base loss: 4252.40
[INFO 2017-06-27 20:03:19,359 main.py:51] epoch 192, training loss: 7385.71, average training loss: 5030.51, base loss: 4269.28
[INFO 2017-06-27 20:03:19,673 main.py:51] epoch 193, training loss: 4055.96, average training loss: 5025.49, base loss: 4269.25
[INFO 2017-06-27 20:03:19,987 main.py:51] epoch 194, training loss: 4828.12, average training loss: 5024.48, base loss: 4272.95
[INFO 2017-06-27 20:03:20,306 main.py:51] epoch 195, training loss: 3994.78, average training loss: 5019.22, base loss: 4272.13
[INFO 2017-06-27 20:03:20,624 main.py:51] epoch 196, training loss: 3763.62, average training loss: 5012.85, base loss: 4270.17
[INFO 2017-06-27 20:03:20,937 main.py:51] epoch 197, training loss: 5237.27, average training loss: 5013.98, base loss: 4276.45
[INFO 2017-06-27 20:03:21,250 main.py:51] epoch 198, training loss: 5018.38, average training loss: 5014.01, base loss: 4281.22
[INFO 2017-06-27 20:03:21,568 main.py:51] epoch 199, training loss: 4769.66, average training loss: 5012.78, base loss: 4284.57
[INFO 2017-06-27 20:03:21,569 main.py:53] epoch 199, testing
[INFO 2017-06-27 20:03:22,962 main.py:105] average testing loss: 4112.57, base loss: 4245.10
[INFO 2017-06-27 20:03:22,962 main.py:106] improve_loss: 132.53, improve_percent: 0.03
[INFO 2017-06-27 20:03:22,963 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:03:22,975 main.py:76] current best improved percent: 0.03
[INFO 2017-06-27 20:03:23,291 main.py:51] epoch 200, training loss: 4135.57, average training loss: 5008.42, base loss: 4284.62
[INFO 2017-06-27 20:03:23,606 main.py:51] epoch 201, training loss: 3917.52, average training loss: 5003.02, base loss: 4283.39
[INFO 2017-06-27 20:03:23,922 main.py:51] epoch 202, training loss: 3537.02, average training loss: 4995.80, base loss: 4280.07
[INFO 2017-06-27 20:03:24,235 main.py:51] epoch 203, training loss: 3845.61, average training loss: 4990.16, base loss: 4278.59
[INFO 2017-06-27 20:03:24,550 main.py:51] epoch 204, training loss: 3332.20, average training loss: 4982.07, base loss: 4274.29
[INFO 2017-06-27 20:03:24,864 main.py:51] epoch 205, training loss: 4156.17, average training loss: 4978.06, base loss: 4274.34
[INFO 2017-06-27 20:03:25,180 main.py:51] epoch 206, training loss: 4188.20, average training loss: 4974.25, base loss: 4274.76
[INFO 2017-06-27 20:03:25,491 main.py:51] epoch 207, training loss: 3997.44, average training loss: 4969.55, base loss: 4274.19
[INFO 2017-06-27 20:03:25,802 main.py:51] epoch 208, training loss: 3783.89, average training loss: 4963.88, base loss: 4272.42
[INFO 2017-06-27 20:03:26,118 main.py:51] epoch 209, training loss: 3377.92, average training loss: 4956.33, base loss: 4268.53
[INFO 2017-06-27 20:03:26,433 main.py:51] epoch 210, training loss: 2893.28, average training loss: 4946.55, base loss: 4262.38
[INFO 2017-06-27 20:03:26,748 main.py:51] epoch 211, training loss: 5020.39, average training loss: 4946.90, base loss: 4267.00
[INFO 2017-06-27 20:03:27,079 main.py:51] epoch 212, training loss: 3685.76, average training loss: 4940.98, base loss: 4264.77
[INFO 2017-06-27 20:03:27,394 main.py:51] epoch 213, training loss: 3531.33, average training loss: 4934.39, base loss: 4261.95
[INFO 2017-06-27 20:03:27,709 main.py:51] epoch 214, training loss: 7653.96, average training loss: 4947.04, base loss: 4278.48
[INFO 2017-06-27 20:03:28,023 main.py:51] epoch 215, training loss: 4111.83, average training loss: 4943.17, base loss: 4278.64
[INFO 2017-06-27 20:03:28,332 main.py:51] epoch 216, training loss: 3909.15, average training loss: 4938.41, base loss: 4277.63
[INFO 2017-06-27 20:03:28,645 main.py:51] epoch 217, training loss: 4395.25, average training loss: 4935.91, base loss: 4279.26
[INFO 2017-06-27 20:03:28,956 main.py:51] epoch 218, training loss: 4462.52, average training loss: 4933.75, base loss: 4281.02
[INFO 2017-06-27 20:03:29,263 main.py:51] epoch 219, training loss: 3818.89, average training loss: 4928.69, base loss: 4279.58
[INFO 2017-06-27 20:03:29,576 main.py:51] epoch 220, training loss: 4163.26, average training loss: 4925.22, base loss: 4279.70
[INFO 2017-06-27 20:03:29,890 main.py:51] epoch 221, training loss: 4721.55, average training loss: 4924.30, base loss: 4282.71
[INFO 2017-06-27 20:03:30,203 main.py:51] epoch 222, training loss: 3617.38, average training loss: 4918.44, base loss: 4280.57
[INFO 2017-06-27 20:03:30,513 main.py:51] epoch 223, training loss: 3681.48, average training loss: 4912.92, base loss: 4278.44
[INFO 2017-06-27 20:03:30,824 main.py:51] epoch 224, training loss: 7751.94, average training loss: 4925.54, base loss: 4294.92
[INFO 2017-06-27 20:03:31,138 main.py:51] epoch 225, training loss: 3523.04, average training loss: 4919.33, base loss: 4292.21
[INFO 2017-06-27 20:03:31,454 main.py:51] epoch 226, training loss: 3782.18, average training loss: 4914.32, base loss: 4290.46
[INFO 2017-06-27 20:03:31,770 main.py:51] epoch 227, training loss: 3725.04, average training loss: 4909.11, base loss: 4288.70
[INFO 2017-06-27 20:03:32,085 main.py:51] epoch 228, training loss: 3525.91, average training loss: 4903.07, base loss: 4285.90
[INFO 2017-06-27 20:03:32,403 main.py:51] epoch 229, training loss: 3836.70, average training loss: 4898.43, base loss: 4284.71
[INFO 2017-06-27 20:03:32,715 main.py:51] epoch 230, training loss: 3837.32, average training loss: 4893.84, base loss: 4283.48
[INFO 2017-06-27 20:03:33,016 main.py:51] epoch 231, training loss: 4583.94, average training loss: 4892.50, base loss: 4285.82
[INFO 2017-06-27 20:03:33,329 main.py:51] epoch 232, training loss: 3120.04, average training loss: 4884.90, base loss: 4281.11
[INFO 2017-06-27 20:03:33,635 main.py:51] epoch 233, training loss: 4722.99, average training loss: 4884.20, base loss: 4283.42
[INFO 2017-06-27 20:03:33,949 main.py:51] epoch 234, training loss: 3476.15, average training loss: 4878.21, base loss: 4280.83
[INFO 2017-06-27 20:03:34,261 main.py:51] epoch 235, training loss: 5088.60, average training loss: 4879.10, base loss: 4285.71
[INFO 2017-06-27 20:03:34,576 main.py:51] epoch 236, training loss: 3773.82, average training loss: 4874.44, base loss: 4284.23
[INFO 2017-06-27 20:03:34,887 main.py:51] epoch 237, training loss: 3737.59, average training loss: 4869.66, base loss: 4282.41
[INFO 2017-06-27 20:03:35,201 main.py:51] epoch 238, training loss: 4383.77, average training loss: 4867.63, base loss: 4283.76
[INFO 2017-06-27 20:03:35,515 main.py:51] epoch 239, training loss: 3815.82, average training loss: 4863.25, base loss: 4282.46
[INFO 2017-06-27 20:03:35,828 main.py:51] epoch 240, training loss: 3664.27, average training loss: 4858.27, base loss: 4280.51
[INFO 2017-06-27 20:03:36,141 main.py:51] epoch 241, training loss: 3818.82, average training loss: 4853.98, base loss: 4279.07
[INFO 2017-06-27 20:03:36,452 main.py:51] epoch 242, training loss: 3510.23, average training loss: 4848.45, base loss: 4276.17
[INFO 2017-06-27 20:03:36,759 main.py:51] epoch 243, training loss: 3749.96, average training loss: 4843.95, base loss: 4274.75
[INFO 2017-06-27 20:03:37,064 main.py:51] epoch 244, training loss: 3719.26, average training loss: 4839.35, base loss: 4273.33
[INFO 2017-06-27 20:03:37,372 main.py:51] epoch 245, training loss: 5095.13, average training loss: 4840.39, base loss: 4277.97
[INFO 2017-06-27 20:03:37,686 main.py:51] epoch 246, training loss: 4001.86, average training loss: 4837.00, base loss: 4277.47
[INFO 2017-06-27 20:03:37,998 main.py:51] epoch 247, training loss: 4077.40, average training loss: 4833.94, base loss: 4277.44
[INFO 2017-06-27 20:03:38,310 main.py:51] epoch 248, training loss: 4078.83, average training loss: 4830.90, base loss: 4277.60
[INFO 2017-06-27 20:03:38,621 main.py:51] epoch 249, training loss: 3739.09, average training loss: 4826.54, base loss: 4276.21
[INFO 2017-06-27 20:03:38,929 main.py:51] epoch 250, training loss: 4597.07, average training loss: 4825.62, base loss: 4278.44
[INFO 2017-06-27 20:03:39,236 main.py:51] epoch 251, training loss: 7172.07, average training loss: 4834.93, base loss: 4290.39
[INFO 2017-06-27 20:03:39,546 main.py:51] epoch 252, training loss: 3852.72, average training loss: 4831.05, base loss: 4289.34
[INFO 2017-06-27 20:03:39,860 main.py:51] epoch 253, training loss: 3891.62, average training loss: 4827.35, base loss: 4288.67
[INFO 2017-06-27 20:03:40,164 main.py:51] epoch 254, training loss: 3702.80, average training loss: 4822.94, base loss: 4286.93
[INFO 2017-06-27 20:03:40,477 main.py:51] epoch 255, training loss: 3500.84, average training loss: 4817.78, base loss: 4284.36
[INFO 2017-06-27 20:03:40,788 main.py:51] epoch 256, training loss: 4339.53, average training loss: 4815.92, base loss: 4285.79
[INFO 2017-06-27 20:03:41,097 main.py:51] epoch 257, training loss: 4788.47, average training loss: 4815.81, base loss: 4288.97
[INFO 2017-06-27 20:03:41,409 main.py:51] epoch 258, training loss: 4006.19, average training loss: 4812.69, base loss: 4288.71
[INFO 2017-06-27 20:03:41,723 main.py:51] epoch 259, training loss: 4344.10, average training loss: 4810.88, base loss: 4289.82
[INFO 2017-06-27 20:03:42,032 main.py:51] epoch 260, training loss: 3970.23, average training loss: 4807.66, base loss: 4289.37
[INFO 2017-06-27 20:03:42,339 main.py:51] epoch 261, training loss: 3804.07, average training loss: 4803.83, base loss: 4288.55
[INFO 2017-06-27 20:03:42,644 main.py:51] epoch 262, training loss: 6836.00, average training loss: 4811.56, base loss: 4298.63
[INFO 2017-06-27 20:03:42,949 main.py:51] epoch 263, training loss: 3655.70, average training loss: 4807.18, base loss: 4296.76
[INFO 2017-06-27 20:03:43,260 main.py:51] epoch 264, training loss: 3439.73, average training loss: 4802.02, base loss: 4294.22
[INFO 2017-06-27 20:03:43,564 main.py:51] epoch 265, training loss: 3689.08, average training loss: 4797.84, base loss: 4292.62
[INFO 2017-06-27 20:03:43,870 main.py:51] epoch 266, training loss: 3457.97, average training loss: 4792.82, base loss: 4290.14
[INFO 2017-06-27 20:03:44,193 main.py:51] epoch 267, training loss: 4281.11, average training loss: 4790.91, base loss: 4290.73
[INFO 2017-06-27 20:03:44,498 main.py:51] epoch 268, training loss: 7083.93, average training loss: 4799.43, base loss: 4301.83
[INFO 2017-06-27 20:03:44,809 main.py:51] epoch 269, training loss: 3429.88, average training loss: 4794.36, base loss: 4299.11
[INFO 2017-06-27 20:03:45,119 main.py:51] epoch 270, training loss: 9824.49, average training loss: 4812.92, base loss: 4320.15
[INFO 2017-06-27 20:03:45,431 main.py:51] epoch 271, training loss: 3259.76, average training loss: 4807.21, base loss: 4316.60
[INFO 2017-06-27 20:03:45,733 main.py:51] epoch 272, training loss: 3782.30, average training loss: 4803.46, base loss: 4315.17
[INFO 2017-06-27 20:03:46,036 main.py:51] epoch 273, training loss: 4064.32, average training loss: 4800.76, base loss: 4315.27
[INFO 2017-06-27 20:03:46,347 main.py:51] epoch 274, training loss: 3447.27, average training loss: 4795.84, base loss: 4312.72
[INFO 2017-06-27 20:03:46,662 main.py:51] epoch 275, training loss: 4582.61, average training loss: 4795.07, base loss: 4314.75
[INFO 2017-06-27 20:03:46,969 main.py:51] epoch 276, training loss: 3686.30, average training loss: 4791.06, base loss: 4313.00
[INFO 2017-06-27 20:03:47,279 main.py:51] epoch 277, training loss: 4195.04, average training loss: 4788.92, base loss: 4313.70
[INFO 2017-06-27 20:03:47,580 main.py:51] epoch 278, training loss: 4391.25, average training loss: 4787.49, base loss: 4315.08
[INFO 2017-06-27 20:03:47,885 main.py:51] epoch 279, training loss: 4391.75, average training loss: 4786.08, base loss: 4316.27
[INFO 2017-06-27 20:03:48,192 main.py:51] epoch 280, training loss: 4243.89, average training loss: 4784.15, base loss: 4317.03
[INFO 2017-06-27 20:03:48,502 main.py:51] epoch 281, training loss: 4162.93, average training loss: 4781.95, base loss: 4317.37
[INFO 2017-06-27 20:03:48,819 main.py:51] epoch 282, training loss: 3566.01, average training loss: 4777.65, base loss: 4315.38
[INFO 2017-06-27 20:03:49,133 main.py:51] epoch 283, training loss: 3451.76, average training loss: 4772.98, base loss: 4312.78
[INFO 2017-06-27 20:03:49,443 main.py:51] epoch 284, training loss: 3993.11, average training loss: 4770.25, base loss: 4312.50
[INFO 2017-06-27 20:03:49,755 main.py:51] epoch 285, training loss: 4110.72, average training loss: 4767.94, base loss: 4312.50
[INFO 2017-06-27 20:03:50,073 main.py:51] epoch 286, training loss: 3521.34, average training loss: 4763.60, base loss: 4310.39
[INFO 2017-06-27 20:03:50,384 main.py:51] epoch 287, training loss: 3633.19, average training loss: 4759.67, base loss: 4308.32
[INFO 2017-06-27 20:03:50,691 main.py:51] epoch 288, training loss: 3936.58, average training loss: 4756.82, base loss: 4307.81
[INFO 2017-06-27 20:03:50,996 main.py:51] epoch 289, training loss: 3890.12, average training loss: 4753.83, base loss: 4307.57
[INFO 2017-06-27 20:03:51,302 main.py:51] epoch 290, training loss: 3323.48, average training loss: 4748.92, base loss: 4304.95
[INFO 2017-06-27 20:03:51,610 main.py:51] epoch 291, training loss: 3735.47, average training loss: 4745.45, base loss: 4303.56
[INFO 2017-06-27 20:03:51,918 main.py:51] epoch 292, training loss: 7296.98, average training loss: 4754.16, base loss: 4314.31
[INFO 2017-06-27 20:03:52,227 main.py:51] epoch 293, training loss: 4589.24, average training loss: 4753.60, base loss: 4316.34
[INFO 2017-06-27 20:03:52,536 main.py:51] epoch 294, training loss: 3838.35, average training loss: 4750.49, base loss: 4315.43
[INFO 2017-06-27 20:03:52,845 main.py:51] epoch 295, training loss: 4457.65, average training loss: 4749.50, base loss: 4317.02
[INFO 2017-06-27 20:03:53,155 main.py:51] epoch 296, training loss: 4814.27, average training loss: 4749.72, base loss: 4319.85
[INFO 2017-06-27 20:03:53,464 main.py:51] epoch 297, training loss: 3867.62, average training loss: 4746.76, base loss: 4319.17
[INFO 2017-06-27 20:03:53,772 main.py:51] epoch 298, training loss: 3455.14, average training loss: 4742.44, base loss: 4317.40
[INFO 2017-06-27 20:03:54,081 main.py:51] epoch 299, training loss: 4348.11, average training loss: 4741.13, base loss: 4318.41
[INFO 2017-06-27 20:03:54,081 main.py:53] epoch 299, testing
[INFO 2017-06-27 20:03:55,470 main.py:105] average testing loss: 3650.88, base loss: 3834.42
[INFO 2017-06-27 20:03:55,470 main.py:106] improve_loss: 183.54, improve_percent: 0.05
[INFO 2017-06-27 20:03:55,471 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:03:55,483 main.py:76] current best improved percent: 0.05
[INFO 2017-06-27 20:03:55,794 main.py:51] epoch 300, training loss: 3177.70, average training loss: 4735.93, base loss: 4314.74
[INFO 2017-06-27 20:03:56,106 main.py:51] epoch 301, training loss: 4272.39, average training loss: 4734.40, base loss: 4315.30
[INFO 2017-06-27 20:03:56,414 main.py:51] epoch 302, training loss: 3313.74, average training loss: 4729.71, base loss: 4312.37
[INFO 2017-06-27 20:03:56,724 main.py:51] epoch 303, training loss: 3640.79, average training loss: 4726.13, base loss: 4310.79
[INFO 2017-06-27 20:03:57,033 main.py:51] epoch 304, training loss: 3237.30, average training loss: 4721.25, base loss: 4307.89
[INFO 2017-06-27 20:03:57,340 main.py:51] epoch 305, training loss: 4038.76, average training loss: 4719.02, base loss: 4307.80
[INFO 2017-06-27 20:03:57,645 main.py:51] epoch 306, training loss: 3816.49, average training loss: 4716.08, base loss: 4307.02
[INFO 2017-06-27 20:03:57,954 main.py:51] epoch 307, training loss: 7450.37, average training loss: 4724.95, base loss: 4318.00
[INFO 2017-06-27 20:03:58,272 main.py:51] epoch 308, training loss: 3068.52, average training loss: 4719.59, base loss: 4314.40
[INFO 2017-06-27 20:03:58,583 main.py:51] epoch 309, training loss: 4405.64, average training loss: 4718.58, base loss: 4315.43
[INFO 2017-06-27 20:03:58,896 main.py:51] epoch 310, training loss: 3676.42, average training loss: 4715.23, base loss: 4314.29
[INFO 2017-06-27 20:03:59,206 main.py:51] epoch 311, training loss: 4402.98, average training loss: 4714.23, base loss: 4315.69
[INFO 2017-06-27 20:03:59,511 main.py:51] epoch 312, training loss: 4195.53, average training loss: 4712.57, base loss: 4316.66
[INFO 2017-06-27 20:03:59,821 main.py:51] epoch 313, training loss: 3680.32, average training loss: 4709.28, base loss: 4315.39
[INFO 2017-06-27 20:04:00,127 main.py:51] epoch 314, training loss: 3328.16, average training loss: 4704.90, base loss: 4312.97
[INFO 2017-06-27 20:04:00,442 main.py:51] epoch 315, training loss: 5053.09, average training loss: 4706.00, base loss: 4316.70
[INFO 2017-06-27 20:04:00,754 main.py:51] epoch 316, training loss: 3562.12, average training loss: 4702.39, base loss: 4314.82
[INFO 2017-06-27 20:04:01,063 main.py:51] epoch 317, training loss: 3317.45, average training loss: 4698.04, base loss: 4312.27
[INFO 2017-06-27 20:04:01,369 main.py:51] epoch 318, training loss: 3854.92, average training loss: 4695.40, base loss: 4311.60
[INFO 2017-06-27 20:04:01,677 main.py:51] epoch 319, training loss: 3790.05, average training loss: 4692.57, base loss: 4310.70
[INFO 2017-06-27 20:04:01,985 main.py:51] epoch 320, training loss: 4740.97, average training loss: 4692.72, base loss: 4313.53
[INFO 2017-06-27 20:04:02,293 main.py:51] epoch 321, training loss: 3802.49, average training loss: 4689.95, base loss: 4312.81
[INFO 2017-06-27 20:04:02,604 main.py:51] epoch 322, training loss: 3627.71, average training loss: 4686.66, base loss: 4311.17
[INFO 2017-06-27 20:04:02,915 main.py:51] epoch 323, training loss: 3629.35, average training loss: 4683.40, base loss: 4309.84
[INFO 2017-06-27 20:04:03,223 main.py:51] epoch 324, training loss: 3907.27, average training loss: 4681.01, base loss: 4309.57
[INFO 2017-06-27 20:04:03,534 main.py:51] epoch 325, training loss: 4309.35, average training loss: 4679.87, base loss: 4310.60
[INFO 2017-06-27 20:04:03,847 main.py:51] epoch 326, training loss: 3838.67, average training loss: 4677.30, base loss: 4309.84
[INFO 2017-06-27 20:04:04,161 main.py:51] epoch 327, training loss: 3000.94, average training loss: 4672.19, base loss: 4306.27
[INFO 2017-06-27 20:04:04,469 main.py:51] epoch 328, training loss: 3042.34, average training loss: 4667.23, base loss: 4302.72
[INFO 2017-06-27 20:04:04,787 main.py:51] epoch 329, training loss: 3495.04, average training loss: 4663.68, base loss: 4301.05
[INFO 2017-06-27 20:04:05,099 main.py:51] epoch 330, training loss: 3255.46, average training loss: 4659.43, base loss: 4298.35
[INFO 2017-06-27 20:04:05,412 main.py:51] epoch 331, training loss: 3287.28, average training loss: 4655.30, base loss: 4295.92
[INFO 2017-06-27 20:04:05,722 main.py:51] epoch 332, training loss: 7285.31, average training loss: 4663.19, base loss: 4305.79
[INFO 2017-06-27 20:04:06,035 main.py:51] epoch 333, training loss: 3844.68, average training loss: 4660.74, base loss: 4305.03
[INFO 2017-06-27 20:04:06,350 main.py:51] epoch 334, training loss: 4213.31, average training loss: 4659.41, base loss: 4305.66
[INFO 2017-06-27 20:04:06,660 main.py:51] epoch 335, training loss: 3521.41, average training loss: 4656.02, base loss: 4303.65
[INFO 2017-06-27 20:04:06,980 main.py:51] epoch 336, training loss: 4220.41, average training loss: 4654.73, base loss: 4304.70
[INFO 2017-06-27 20:04:07,297 main.py:51] epoch 337, training loss: 3761.13, average training loss: 4652.08, base loss: 4304.03
[INFO 2017-06-27 20:04:07,615 main.py:51] epoch 338, training loss: 3431.65, average training loss: 4648.48, base loss: 4302.24
[INFO 2017-06-27 20:04:07,933 main.py:51] epoch 339, training loss: 3673.80, average training loss: 4645.62, base loss: 4301.09
[INFO 2017-06-27 20:04:08,250 main.py:51] epoch 340, training loss: 4386.06, average training loss: 4644.86, base loss: 4301.68
[INFO 2017-06-27 20:04:08,566 main.py:51] epoch 341, training loss: 3510.87, average training loss: 4641.54, base loss: 4299.92
[INFO 2017-06-27 20:04:08,878 main.py:51] epoch 342, training loss: 3810.43, average training loss: 4639.12, base loss: 4299.18
[INFO 2017-06-27 20:04:09,197 main.py:51] epoch 343, training loss: 3939.70, average training loss: 4637.08, base loss: 4298.61
[INFO 2017-06-27 20:04:09,509 main.py:51] epoch 344, training loss: 2996.90, average training loss: 4632.33, base loss: 4295.11
[INFO 2017-06-27 20:04:09,824 main.py:51] epoch 345, training loss: 4185.93, average training loss: 4631.04, base loss: 4295.52
[INFO 2017-06-27 20:04:10,135 main.py:51] epoch 346, training loss: 4442.65, average training loss: 4630.50, base loss: 4296.70
[INFO 2017-06-27 20:04:10,455 main.py:51] epoch 347, training loss: 5073.57, average training loss: 4631.77, base loss: 4300.29
[INFO 2017-06-27 20:04:10,771 main.py:51] epoch 348, training loss: 3311.53, average training loss: 4627.99, base loss: 4297.80
[INFO 2017-06-27 20:04:11,087 main.py:51] epoch 349, training loss: 4008.47, average training loss: 4626.22, base loss: 4297.83
[INFO 2017-06-27 20:04:11,409 main.py:51] epoch 350, training loss: 3689.27, average training loss: 4623.55, base loss: 4296.84
[INFO 2017-06-27 20:04:11,725 main.py:51] epoch 351, training loss: 4865.97, average training loss: 4624.24, base loss: 4299.76
[INFO 2017-06-27 20:04:12,041 main.py:51] epoch 352, training loss: 3340.81, average training loss: 4620.60, base loss: 4297.80
[INFO 2017-06-27 20:04:12,363 main.py:51] epoch 353, training loss: 3085.10, average training loss: 4616.26, base loss: 4294.76
[INFO 2017-06-27 20:04:12,683 main.py:51] epoch 354, training loss: 7457.48, average training loss: 4624.27, base loss: 4304.21
[INFO 2017-06-27 20:04:13,004 main.py:51] epoch 355, training loss: 2754.06, average training loss: 4619.01, base loss: 4300.20
[INFO 2017-06-27 20:04:13,316 main.py:51] epoch 356, training loss: 3513.97, average training loss: 4615.92, base loss: 4298.61
[INFO 2017-06-27 20:04:13,632 main.py:51] epoch 357, training loss: 3833.26, average training loss: 4613.73, base loss: 4297.87
[INFO 2017-06-27 20:04:13,942 main.py:51] epoch 358, training loss: 3573.52, average training loss: 4610.83, base loss: 4296.70
[INFO 2017-06-27 20:04:14,261 main.py:51] epoch 359, training loss: 3168.37, average training loss: 4606.83, base loss: 4294.11
[INFO 2017-06-27 20:04:14,582 main.py:51] epoch 360, training loss: 3563.50, average training loss: 4603.94, base loss: 4292.80
[INFO 2017-06-27 20:04:14,902 main.py:51] epoch 361, training loss: 3770.92, average training loss: 4601.64, base loss: 4292.12
[INFO 2017-06-27 20:04:15,224 main.py:51] epoch 362, training loss: 3216.95, average training loss: 4597.82, base loss: 4289.65
[INFO 2017-06-27 20:04:15,542 main.py:51] epoch 363, training loss: 3592.76, average training loss: 4595.06, base loss: 4288.72
[INFO 2017-06-27 20:04:15,859 main.py:51] epoch 364, training loss: 4100.49, average training loss: 4593.70, base loss: 4289.03
[INFO 2017-06-27 20:04:16,173 main.py:51] epoch 365, training loss: 4272.96, average training loss: 4592.83, base loss: 4290.11
[INFO 2017-06-27 20:04:16,493 main.py:51] epoch 366, training loss: 3640.79, average training loss: 4590.23, base loss: 4288.87
[INFO 2017-06-27 20:04:16,808 main.py:51] epoch 367, training loss: 3429.52, average training loss: 4587.08, base loss: 4287.32
[INFO 2017-06-27 20:04:17,121 main.py:51] epoch 368, training loss: 4021.70, average training loss: 4585.55, base loss: 4287.91
[INFO 2017-06-27 20:04:17,433 main.py:51] epoch 369, training loss: 3565.16, average training loss: 4582.79, base loss: 4286.68
[INFO 2017-06-27 20:04:17,749 main.py:51] epoch 370, training loss: 3787.20, average training loss: 4580.65, base loss: 4286.46
[INFO 2017-06-27 20:04:18,068 main.py:51] epoch 371, training loss: 3556.17, average training loss: 4577.89, base loss: 4285.08
[INFO 2017-06-27 20:04:18,384 main.py:51] epoch 372, training loss: 3579.06, average training loss: 4575.21, base loss: 4283.91
[INFO 2017-06-27 20:04:18,699 main.py:51] epoch 373, training loss: 3469.11, average training loss: 4572.26, base loss: 4282.83
[INFO 2017-06-27 20:04:19,028 main.py:51] epoch 374, training loss: 4055.97, average training loss: 4570.88, base loss: 4282.85
[INFO 2017-06-27 20:04:19,354 main.py:51] epoch 375, training loss: 3787.51, average training loss: 4568.80, base loss: 4282.37
[INFO 2017-06-27 20:04:19,673 main.py:51] epoch 376, training loss: 7414.92, average training loss: 4576.35, base loss: 4291.57
[INFO 2017-06-27 20:04:19,997 main.py:51] epoch 377, training loss: 3660.37, average training loss: 4573.92, base loss: 4290.96
[INFO 2017-06-27 20:04:20,323 main.py:51] epoch 378, training loss: 3973.58, average training loss: 4572.34, base loss: 4291.13
[INFO 2017-06-27 20:04:20,649 main.py:51] epoch 379, training loss: 3245.08, average training loss: 4568.85, base loss: 4288.95
[INFO 2017-06-27 20:04:20,972 main.py:51] epoch 380, training loss: 3311.42, average training loss: 4565.55, base loss: 4287.27
[INFO 2017-06-27 20:04:21,289 main.py:51] epoch 381, training loss: 4344.37, average training loss: 4564.97, base loss: 4288.21
[INFO 2017-06-27 20:04:21,608 main.py:51] epoch 382, training loss: 3677.00, average training loss: 4562.65, base loss: 4287.20
[INFO 2017-06-27 20:04:21,931 main.py:51] epoch 383, training loss: 4164.26, average training loss: 4561.61, base loss: 4287.96
[INFO 2017-06-27 20:04:22,263 main.py:51] epoch 384, training loss: 3184.46, average training loss: 4558.03, base loss: 4285.71
[INFO 2017-06-27 20:04:22,585 main.py:51] epoch 385, training loss: 3629.21, average training loss: 4555.63, base loss: 4284.91
[INFO 2017-06-27 20:04:22,905 main.py:51] epoch 386, training loss: 3400.30, average training loss: 4552.64, base loss: 4283.59
[INFO 2017-06-27 20:04:23,225 main.py:51] epoch 387, training loss: 3951.83, average training loss: 4551.09, base loss: 4283.86
[INFO 2017-06-27 20:04:23,548 main.py:51] epoch 388, training loss: 4186.22, average training loss: 4550.16, base loss: 4284.42
[INFO 2017-06-27 20:04:23,870 main.py:51] epoch 389, training loss: 4365.69, average training loss: 4549.68, base loss: 4285.93
[INFO 2017-06-27 20:04:24,194 main.py:51] epoch 390, training loss: 3153.98, average training loss: 4546.11, base loss: 4283.46
[INFO 2017-06-27 20:04:24,516 main.py:51] epoch 391, training loss: 3879.69, average training loss: 4544.41, base loss: 4283.69
[INFO 2017-06-27 20:04:24,836 main.py:51] epoch 392, training loss: 3485.13, average training loss: 4541.72, base loss: 4282.30
[INFO 2017-06-27 20:04:25,160 main.py:51] epoch 393, training loss: 2997.91, average training loss: 4537.80, base loss: 4279.72
[INFO 2017-06-27 20:04:25,486 main.py:51] epoch 394, training loss: 4621.55, average training loss: 4538.01, base loss: 4282.07
[INFO 2017-06-27 20:04:25,807 main.py:51] epoch 395, training loss: 3354.43, average training loss: 4535.02, base loss: 4280.51
[INFO 2017-06-27 20:04:26,129 main.py:51] epoch 396, training loss: 3587.86, average training loss: 4532.64, base loss: 4279.74
[INFO 2017-06-27 20:04:26,444 main.py:51] epoch 397, training loss: 6933.42, average training loss: 4538.67, base loss: 4287.28
[INFO 2017-06-27 20:04:26,761 main.py:51] epoch 398, training loss: 3814.85, average training loss: 4536.85, base loss: 4286.96
[INFO 2017-06-27 20:04:27,081 main.py:51] epoch 399, training loss: 3834.89, average training loss: 4535.10, base loss: 4286.71
[INFO 2017-06-27 20:04:27,081 main.py:53] epoch 399, testing
[INFO 2017-06-27 20:04:28,507 main.py:105] average testing loss: 3772.52, base loss: 4130.73
[INFO 2017-06-27 20:04:28,507 main.py:106] improve_loss: 358.21, improve_percent: 0.09
[INFO 2017-06-27 20:04:28,507 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:04:28,519 main.py:76] current best improved percent: 0.09
[INFO 2017-06-27 20:04:28,838 main.py:51] epoch 400, training loss: 4054.51, average training loss: 4533.90, base loss: 4287.37
[INFO 2017-06-27 20:04:29,154 main.py:51] epoch 401, training loss: 3654.79, average training loss: 4531.71, base loss: 4286.57
[INFO 2017-06-27 20:04:29,472 main.py:51] epoch 402, training loss: 3439.16, average training loss: 4529.00, base loss: 4285.27
[INFO 2017-06-27 20:04:29,791 main.py:51] epoch 403, training loss: 3946.56, average training loss: 4527.56, base loss: 4285.60
[INFO 2017-06-27 20:04:30,108 main.py:51] epoch 404, training loss: 3643.90, average training loss: 4525.38, base loss: 4284.87
[INFO 2017-06-27 20:04:30,428 main.py:51] epoch 405, training loss: 3521.88, average training loss: 4522.91, base loss: 4283.92
[INFO 2017-06-27 20:04:30,756 main.py:51] epoch 406, training loss: 3328.41, average training loss: 4519.97, base loss: 4282.07
[INFO 2017-06-27 20:04:31,083 main.py:51] epoch 407, training loss: 3197.24, average training loss: 4516.73, base loss: 4280.34
[INFO 2017-06-27 20:04:31,403 main.py:51] epoch 408, training loss: 3450.11, average training loss: 4514.12, base loss: 4278.75
[INFO 2017-06-27 20:04:31,726 main.py:51] epoch 409, training loss: 3873.50, average training loss: 4512.56, base loss: 4279.01
[INFO 2017-06-27 20:04:32,043 main.py:51] epoch 410, training loss: 3658.81, average training loss: 4510.48, base loss: 4278.38
[INFO 2017-06-27 20:04:32,368 main.py:51] epoch 411, training loss: 3869.01, average training loss: 4508.93, base loss: 4278.48
[INFO 2017-06-27 20:04:32,687 main.py:51] epoch 412, training loss: 3357.07, average training loss: 4506.14, base loss: 4277.12
[INFO 2017-06-27 20:04:33,007 main.py:51] epoch 413, training loss: 3670.17, average training loss: 4504.12, base loss: 4276.51
[INFO 2017-06-27 20:04:33,331 main.py:51] epoch 414, training loss: 3816.62, average training loss: 4502.46, base loss: 4276.60
[INFO 2017-06-27 20:04:33,653 main.py:51] epoch 415, training loss: 3581.94, average training loss: 4500.25, base loss: 4276.12
[INFO 2017-06-27 20:04:33,980 main.py:51] epoch 416, training loss: 3304.23, average training loss: 4497.38, base loss: 4274.54
[INFO 2017-06-27 20:04:34,311 main.py:51] epoch 417, training loss: 3186.39, average training loss: 4494.24, base loss: 4272.40
[INFO 2017-06-27 20:04:34,636 main.py:51] epoch 418, training loss: 4068.07, average training loss: 4493.23, base loss: 4273.42
[INFO 2017-06-27 20:04:34,988 main.py:51] epoch 419, training loss: 3539.27, average training loss: 4490.96, base loss: 4272.70
[INFO 2017-06-27 20:04:35,322 main.py:51] epoch 420, training loss: 4403.95, average training loss: 4490.75, base loss: 4274.23
[INFO 2017-06-27 20:04:35,657 main.py:51] epoch 421, training loss: 3600.29, average training loss: 4488.64, base loss: 4273.66
[INFO 2017-06-27 20:04:35,987 main.py:51] epoch 422, training loss: 3648.35, average training loss: 4486.65, base loss: 4273.34
[INFO 2017-06-27 20:04:36,320 main.py:51] epoch 423, training loss: 3295.58, average training loss: 4483.84, base loss: 4272.15
[INFO 2017-06-27 20:04:36,650 main.py:51] epoch 424, training loss: 3478.92, average training loss: 4481.48, base loss: 4270.93
[INFO 2017-06-27 20:04:37,015 main.py:51] epoch 425, training loss: 3734.43, average training loss: 4479.73, base loss: 4270.84
[INFO 2017-06-27 20:04:37,392 main.py:51] epoch 426, training loss: 3575.08, average training loss: 4477.61, base loss: 4270.04
[INFO 2017-06-27 20:04:37,774 main.py:51] epoch 427, training loss: 3592.79, average training loss: 4475.54, base loss: 4269.11
[INFO 2017-06-27 20:04:38,172 main.py:51] epoch 428, training loss: 3006.63, average training loss: 4472.12, base loss: 4266.87
[INFO 2017-06-27 20:04:38,553 main.py:51] epoch 429, training loss: 3529.33, average training loss: 4469.92, base loss: 4266.24
[INFO 2017-06-27 20:04:38,937 main.py:51] epoch 430, training loss: 2958.53, average training loss: 4466.42, base loss: 4263.37
[INFO 2017-06-27 20:04:39,271 main.py:51] epoch 431, training loss: 3569.75, average training loss: 4464.34, base loss: 4262.63
[INFO 2017-06-27 20:04:39,640 main.py:51] epoch 432, training loss: 3457.75, average training loss: 4462.02, base loss: 4261.35
[INFO 2017-06-27 20:04:40,045 main.py:51] epoch 433, training loss: 4157.76, average training loss: 4461.31, base loss: 4262.06
[INFO 2017-06-27 20:04:40,404 main.py:51] epoch 434, training loss: 4039.50, average training loss: 4460.34, base loss: 4262.64
[INFO 2017-06-27 20:04:40,742 main.py:51] epoch 435, training loss: 3636.46, average training loss: 4458.46, base loss: 4261.97
[INFO 2017-06-27 20:04:41,067 main.py:51] epoch 436, training loss: 3496.67, average training loss: 4456.25, base loss: 4261.06
[INFO 2017-06-27 20:04:41,391 main.py:51] epoch 437, training loss: 3550.60, average training loss: 4454.19, base loss: 4260.38
[INFO 2017-06-27 20:04:41,715 main.py:51] epoch 438, training loss: 8151.12, average training loss: 4462.61, base loss: 4270.46
[INFO 2017-06-27 20:04:42,040 main.py:51] epoch 439, training loss: 4245.76, average training loss: 4462.12, base loss: 4271.80
[INFO 2017-06-27 20:04:42,366 main.py:51] epoch 440, training loss: 7038.20, average training loss: 4467.96, base loss: 4278.65
[INFO 2017-06-27 20:04:42,767 main.py:51] epoch 441, training loss: 3898.36, average training loss: 4466.67, base loss: 4278.69
[INFO 2017-06-27 20:04:43,100 main.py:51] epoch 442, training loss: 3336.40, average training loss: 4464.12, base loss: 4277.22
[INFO 2017-06-27 20:04:43,423 main.py:51] epoch 443, training loss: 3324.38, average training loss: 4461.55, base loss: 4275.78
[INFO 2017-06-27 20:04:43,749 main.py:51] epoch 444, training loss: 3801.96, average training loss: 4460.07, base loss: 4275.80
[INFO 2017-06-27 20:04:44,077 main.py:51] epoch 445, training loss: 3111.21, average training loss: 4457.04, base loss: 4274.05
[INFO 2017-06-27 20:04:44,402 main.py:51] epoch 446, training loss: 4418.92, average training loss: 4456.96, base loss: 4275.74
[INFO 2017-06-27 20:04:44,724 main.py:51] epoch 447, training loss: 3522.53, average training loss: 4454.87, base loss: 4275.15
[INFO 2017-06-27 20:04:45,049 main.py:51] epoch 448, training loss: 3713.74, average training loss: 4453.22, base loss: 4274.82
[INFO 2017-06-27 20:04:45,392 main.py:51] epoch 449, training loss: 3289.67, average training loss: 4450.64, base loss: 4273.37
[INFO 2017-06-27 20:04:45,820 main.py:51] epoch 450, training loss: 3649.21, average training loss: 4448.86, base loss: 4272.32
[INFO 2017-06-27 20:04:46,182 main.py:51] epoch 451, training loss: 3560.43, average training loss: 4446.89, base loss: 4271.76
[INFO 2017-06-27 20:04:46,508 main.py:51] epoch 452, training loss: 3701.39, average training loss: 4445.25, base loss: 4271.28
[INFO 2017-06-27 20:04:46,855 main.py:51] epoch 453, training loss: 4004.19, average training loss: 4444.28, base loss: 4271.81
[INFO 2017-06-27 20:04:47,279 main.py:51] epoch 454, training loss: 3740.71, average training loss: 4442.73, base loss: 4271.54
[INFO 2017-06-27 20:04:47,628 main.py:51] epoch 455, training loss: 3937.76, average training loss: 4441.62, base loss: 4271.95
[INFO 2017-06-27 20:04:47,965 main.py:51] epoch 456, training loss: 3890.62, average training loss: 4440.42, base loss: 4272.10
[INFO 2017-06-27 20:04:48,379 main.py:51] epoch 457, training loss: 3090.66, average training loss: 4437.47, base loss: 4269.81
[INFO 2017-06-27 20:04:48,763 main.py:51] epoch 458, training loss: 3131.66, average training loss: 4434.62, base loss: 4267.76
[INFO 2017-06-27 20:04:49,087 main.py:51] epoch 459, training loss: 3708.86, average training loss: 4433.05, base loss: 4267.21
[INFO 2017-06-27 20:04:49,504 main.py:51] epoch 460, training loss: 3682.83, average training loss: 4431.42, base loss: 4266.50
[INFO 2017-06-27 20:04:49,860 main.py:51] epoch 461, training loss: 3146.41, average training loss: 4428.64, base loss: 4264.53
[INFO 2017-06-27 20:04:50,192 main.py:51] epoch 462, training loss: 2857.34, average training loss: 4425.24, base loss: 4261.85
[INFO 2017-06-27 20:04:50,624 main.py:51] epoch 463, training loss: 3687.08, average training loss: 4423.65, base loss: 4261.26
[INFO 2017-06-27 20:04:50,980 main.py:51] epoch 464, training loss: 4111.71, average training loss: 4422.98, base loss: 4261.92
[INFO 2017-06-27 20:04:51,311 main.py:51] epoch 465, training loss: 3060.84, average training loss: 4420.06, base loss: 4260.03
[INFO 2017-06-27 20:04:51,642 main.py:51] epoch 466, training loss: 4204.16, average training loss: 4419.60, base loss: 4260.94
[INFO 2017-06-27 20:04:51,966 main.py:51] epoch 467, training loss: 3403.38, average training loss: 4417.43, base loss: 4259.78
[INFO 2017-06-27 20:04:52,287 main.py:51] epoch 468, training loss: 3760.85, average training loss: 4416.03, base loss: 4260.09
[INFO 2017-06-27 20:04:52,605 main.py:51] epoch 469, training loss: 3265.02, average training loss: 4413.58, base loss: 4258.68
[INFO 2017-06-27 20:04:52,931 main.py:51] epoch 470, training loss: 3858.20, average training loss: 4412.40, base loss: 4258.64
[INFO 2017-06-27 20:04:53,253 main.py:51] epoch 471, training loss: 3926.23, average training loss: 4411.37, base loss: 4258.88
[INFO 2017-06-27 20:04:53,574 main.py:51] epoch 472, training loss: 4634.11, average training loss: 4411.84, base loss: 4260.94
[INFO 2017-06-27 20:04:53,897 main.py:51] epoch 473, training loss: 4220.71, average training loss: 4411.44, base loss: 4262.14
[INFO 2017-06-27 20:04:54,215 main.py:51] epoch 474, training loss: 3404.24, average training loss: 4409.31, base loss: 4261.13
[INFO 2017-06-27 20:04:54,536 main.py:51] epoch 475, training loss: 3701.43, average training loss: 4407.83, base loss: 4260.60
[INFO 2017-06-27 20:04:54,863 main.py:51] epoch 476, training loss: 3347.47, average training loss: 4405.60, base loss: 4259.37
[INFO 2017-06-27 20:04:55,188 main.py:51] epoch 477, training loss: 3906.85, average training loss: 4404.56, base loss: 4260.13
[INFO 2017-06-27 20:04:55,512 main.py:51] epoch 478, training loss: 3911.17, average training loss: 4403.53, base loss: 4260.40
[INFO 2017-06-27 20:04:55,912 main.py:51] epoch 479, training loss: 3580.09, average training loss: 4401.82, base loss: 4260.00
[INFO 2017-06-27 20:04:56,266 main.py:51] epoch 480, training loss: 3886.37, average training loss: 4400.74, base loss: 4260.08
[INFO 2017-06-27 20:04:56,598 main.py:51] epoch 481, training loss: 4297.70, average training loss: 4400.53, base loss: 4261.52
[INFO 2017-06-27 20:04:56,964 main.py:51] epoch 482, training loss: 4308.51, average training loss: 4400.34, base loss: 4262.70
[INFO 2017-06-27 20:04:57,303 main.py:51] epoch 483, training loss: 3443.02, average training loss: 4398.36, base loss: 4261.70
[INFO 2017-06-27 20:04:57,672 main.py:51] epoch 484, training loss: 3780.76, average training loss: 4397.09, base loss: 4261.75
[INFO 2017-06-27 20:04:58,074 main.py:51] epoch 485, training loss: 3552.10, average training loss: 4395.35, base loss: 4261.28
[INFO 2017-06-27 20:04:58,400 main.py:51] epoch 486, training loss: 3852.42, average training loss: 4394.24, base loss: 4261.43
[INFO 2017-06-27 20:04:58,742 main.py:51] epoch 487, training loss: 3722.91, average training loss: 4392.86, base loss: 4261.26
[INFO 2017-06-27 20:04:59,106 main.py:51] epoch 488, training loss: 4030.90, average training loss: 4392.12, base loss: 4261.99
[INFO 2017-06-27 20:04:59,443 main.py:51] epoch 489, training loss: 3346.53, average training loss: 4389.99, base loss: 4260.63
[INFO 2017-06-27 20:04:59,799 main.py:51] epoch 490, training loss: 3502.81, average training loss: 4388.18, base loss: 4260.17
[INFO 2017-06-27 20:05:00,151 main.py:51] epoch 491, training loss: 3727.80, average training loss: 4386.84, base loss: 4260.06
[INFO 2017-06-27 20:05:00,473 main.py:51] epoch 492, training loss: 3187.50, average training loss: 4384.40, base loss: 4258.33
[INFO 2017-06-27 20:05:00,887 main.py:51] epoch 493, training loss: 3824.53, average training loss: 4383.27, base loss: 4258.51
[INFO 2017-06-27 20:05:01,243 main.py:51] epoch 494, training loss: 3879.94, average training loss: 4382.25, base loss: 4258.78
[INFO 2017-06-27 20:05:01,575 main.py:51] epoch 495, training loss: 3623.40, average training loss: 4380.72, base loss: 4258.29
[INFO 2017-06-27 20:05:01,956 main.py:51] epoch 496, training loss: 3297.97, average training loss: 4378.54, base loss: 4257.14
[INFO 2017-06-27 20:05:02,295 main.py:51] epoch 497, training loss: 3359.22, average training loss: 4376.50, base loss: 4256.08
[INFO 2017-06-27 20:05:02,711 main.py:51] epoch 498, training loss: 4081.03, average training loss: 4375.91, base loss: 4256.81
[INFO 2017-06-27 20:05:03,077 main.py:51] epoch 499, training loss: 4068.56, average training loss: 4375.29, base loss: 4257.46
[INFO 2017-06-27 20:05:03,077 main.py:53] epoch 499, testing
[INFO 2017-06-27 20:05:04,677 main.py:105] average testing loss: 4115.29, base loss: 4596.75
[INFO 2017-06-27 20:05:04,677 main.py:106] improve_loss: 481.45, improve_percent: 0.10
[INFO 2017-06-27 20:05:04,678 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:05:04,702 main.py:76] current best improved percent: 0.10
[INFO 2017-06-27 20:05:05,056 main.py:51] epoch 500, training loss: 3359.22, average training loss: 4373.26, base loss: 4256.22
[INFO 2017-06-27 20:05:05,387 main.py:51] epoch 501, training loss: 3832.86, average training loss: 4372.19, base loss: 4256.70
[INFO 2017-06-27 20:05:05,728 main.py:51] epoch 502, training loss: 3119.86, average training loss: 4369.70, base loss: 4255.24
[INFO 2017-06-27 20:05:06,058 main.py:51] epoch 503, training loss: 3738.58, average training loss: 4368.44, base loss: 4255.05
[INFO 2017-06-27 20:05:06,479 main.py:51] epoch 504, training loss: 3282.63, average training loss: 4366.29, base loss: 4253.92
[INFO 2017-06-27 20:05:06,828 main.py:51] epoch 505, training loss: 3893.51, average training loss: 4365.36, base loss: 4254.08
[INFO 2017-06-27 20:05:07,160 main.py:51] epoch 506, training loss: 4134.97, average training loss: 4364.91, base loss: 4255.10
[INFO 2017-06-27 20:05:07,568 main.py:51] epoch 507, training loss: 4340.91, average training loss: 4364.86, base loss: 4256.14
[INFO 2017-06-27 20:05:07,943 main.py:51] epoch 508, training loss: 3634.34, average training loss: 4363.42, base loss: 4255.86
[INFO 2017-06-27 20:05:08,292 main.py:51] epoch 509, training loss: 3450.59, average training loss: 4361.63, base loss: 4255.36
[INFO 2017-06-27 20:05:08,656 main.py:51] epoch 510, training loss: 3498.57, average training loss: 4359.94, base loss: 4254.81
[INFO 2017-06-27 20:05:08,986 main.py:51] epoch 511, training loss: 3515.83, average training loss: 4358.30, base loss: 4253.91
[INFO 2017-06-27 20:05:09,315 main.py:51] epoch 512, training loss: 4087.35, average training loss: 4357.77, base loss: 4254.44
[INFO 2017-06-27 20:05:09,670 main.py:51] epoch 513, training loss: 3380.97, average training loss: 4355.87, base loss: 4253.54
[INFO 2017-06-27 20:05:10,023 main.py:51] epoch 514, training loss: 3262.54, average training loss: 4353.74, base loss: 4252.37
[INFO 2017-06-27 20:05:10,359 main.py:51] epoch 515, training loss: 4202.25, average training loss: 4353.45, base loss: 4253.00
[INFO 2017-06-27 20:05:10,679 main.py:51] epoch 516, training loss: 2780.86, average training loss: 4350.41, base loss: 4250.63
[INFO 2017-06-27 20:05:11,004 main.py:51] epoch 517, training loss: 3300.37, average training loss: 4348.38, base loss: 4249.27
[INFO 2017-06-27 20:05:11,330 main.py:51] epoch 518, training loss: 3612.34, average training loss: 4346.96, base loss: 4249.04
[INFO 2017-06-27 20:05:11,650 main.py:51] epoch 519, training loss: 3904.27, average training loss: 4346.11, base loss: 4248.99
[INFO 2017-06-27 20:05:12,034 main.py:51] epoch 520, training loss: 3814.46, average training loss: 4345.09, base loss: 4248.91
[INFO 2017-06-27 20:05:12,394 main.py:51] epoch 521, training loss: 3620.84, average training loss: 4343.70, base loss: 4248.26
[INFO 2017-06-27 20:05:12,723 main.py:51] epoch 522, training loss: 3793.82, average training loss: 4342.65, base loss: 4248.46
[INFO 2017-06-27 20:05:13,071 main.py:51] epoch 523, training loss: 3441.86, average training loss: 4340.93, base loss: 4247.99
[INFO 2017-06-27 20:05:13,445 main.py:51] epoch 524, training loss: 3941.22, average training loss: 4340.17, base loss: 4248.26
[INFO 2017-06-27 20:05:13,780 main.py:51] epoch 525, training loss: 4394.18, average training loss: 4340.28, base loss: 4250.00
[INFO 2017-06-27 20:05:14,119 main.py:51] epoch 526, training loss: 3263.33, average training loss: 4338.23, base loss: 4248.81
[INFO 2017-06-27 20:05:14,456 main.py:51] epoch 527, training loss: 3828.09, average training loss: 4337.27, base loss: 4248.81
[INFO 2017-06-27 20:05:14,869 main.py:51] epoch 528, training loss: 2990.36, average training loss: 4334.72, base loss: 4246.54
[INFO 2017-06-27 20:05:15,228 main.py:51] epoch 529, training loss: 3288.37, average training loss: 4332.75, base loss: 4245.27
[INFO 2017-06-27 20:05:15,565 main.py:51] epoch 530, training loss: 3458.85, average training loss: 4331.10, base loss: 4244.37
[INFO 2017-06-27 20:05:15,896 main.py:51] epoch 531, training loss: 4076.40, average training loss: 4330.62, base loss: 4244.91
[INFO 2017-06-27 20:05:16,228 main.py:51] epoch 532, training loss: 3721.84, average training loss: 4329.48, base loss: 4244.64
[INFO 2017-06-27 20:05:16,561 main.py:51] epoch 533, training loss: 3358.52, average training loss: 4327.66, base loss: 4243.58
[INFO 2017-06-27 20:05:16,887 main.py:51] epoch 534, training loss: 3811.46, average training loss: 4326.70, base loss: 4243.68
[INFO 2017-06-27 20:05:17,213 main.py:51] epoch 535, training loss: 4104.35, average training loss: 4326.28, base loss: 4244.76
[INFO 2017-06-27 20:05:17,547 main.py:51] epoch 536, training loss: 3593.21, average training loss: 4324.92, base loss: 4244.56
[INFO 2017-06-27 20:05:17,874 main.py:51] epoch 537, training loss: 3691.19, average training loss: 4323.74, base loss: 4244.40
[INFO 2017-06-27 20:05:18,204 main.py:51] epoch 538, training loss: 3785.72, average training loss: 4322.74, base loss: 4244.28
[INFO 2017-06-27 20:05:18,611 main.py:51] epoch 539, training loss: 3363.93, average training loss: 4320.96, base loss: 4243.35
[INFO 2017-06-27 20:05:18,970 main.py:51] epoch 540, training loss: 3396.20, average training loss: 4319.25, base loss: 4242.65
[INFO 2017-06-27 20:05:19,313 main.py:51] epoch 541, training loss: 3437.83, average training loss: 4317.63, base loss: 4241.70
[INFO 2017-06-27 20:05:19,745 main.py:51] epoch 542, training loss: 3423.27, average training loss: 4315.98, base loss: 4241.06
[INFO 2017-06-27 20:05:20,101 main.py:51] epoch 543, training loss: 3740.40, average training loss: 4314.92, base loss: 4241.00
[INFO 2017-06-27 20:05:20,529 main.py:51] epoch 544, training loss: 3907.71, average training loss: 4314.18, base loss: 4241.43
[INFO 2017-06-27 20:05:20,942 main.py:51] epoch 545, training loss: 3645.04, average training loss: 4312.95, base loss: 4240.92
[INFO 2017-06-27 20:05:21,286 main.py:51] epoch 546, training loss: 3590.67, average training loss: 4311.63, base loss: 4240.15
[INFO 2017-06-27 20:05:21,689 main.py:51] epoch 547, training loss: 3693.96, average training loss: 4310.50, base loss: 4240.30
[INFO 2017-06-27 20:05:22,062 main.py:51] epoch 548, training loss: 3099.25, average training loss: 4308.30, base loss: 4238.73
[INFO 2017-06-27 20:05:22,402 main.py:51] epoch 549, training loss: 3605.97, average training loss: 4307.02, base loss: 4238.28
[INFO 2017-06-27 20:05:22,753 main.py:51] epoch 550, training loss: 3693.99, average training loss: 4305.91, base loss: 4238.18
[INFO 2017-06-27 20:05:23,084 main.py:51] epoch 551, training loss: 2829.51, average training loss: 4303.23, base loss: 4235.79
[INFO 2017-06-27 20:05:23,443 main.py:51] epoch 552, training loss: 3560.51, average training loss: 4301.89, base loss: 4235.31
[INFO 2017-06-27 20:05:23,797 main.py:51] epoch 553, training loss: 2857.95, average training loss: 4299.28, base loss: 4233.36
[INFO 2017-06-27 20:05:24,143 main.py:51] epoch 554, training loss: 4071.92, average training loss: 4298.87, base loss: 4233.66
[INFO 2017-06-27 20:05:24,578 main.py:51] epoch 555, training loss: 3277.84, average training loss: 4297.04, base loss: 4232.47
[INFO 2017-06-27 20:05:24,934 main.py:51] epoch 556, training loss: 4181.78, average training loss: 4296.83, base loss: 4233.38
[INFO 2017-06-27 20:05:25,341 main.py:51] epoch 557, training loss: 3544.28, average training loss: 4295.48, base loss: 4233.12
[INFO 2017-06-27 20:05:25,715 main.py:51] epoch 558, training loss: 3634.14, average training loss: 4294.30, base loss: 4233.13
[INFO 2017-06-27 20:05:26,103 main.py:51] epoch 559, training loss: 3235.10, average training loss: 4292.41, base loss: 4231.91
[INFO 2017-06-27 20:05:26,463 main.py:51] epoch 560, training loss: 3780.47, average training loss: 4291.49, base loss: 4232.04
[INFO 2017-06-27 20:05:26,798 main.py:51] epoch 561, training loss: 2934.16, average training loss: 4289.08, base loss: 4230.39
[INFO 2017-06-27 20:05:27,221 main.py:51] epoch 562, training loss: 3260.62, average training loss: 4287.25, base loss: 4229.40
[INFO 2017-06-27 20:05:27,595 main.py:51] epoch 563, training loss: 4540.67, average training loss: 4287.70, base loss: 4231.33
[INFO 2017-06-27 20:05:27,954 main.py:51] epoch 564, training loss: 3129.13, average training loss: 4285.65, base loss: 4229.88
[INFO 2017-06-27 20:05:28,320 main.py:51] epoch 565, training loss: 3182.26, average training loss: 4283.70, base loss: 4228.55
[INFO 2017-06-27 20:05:28,662 main.py:51] epoch 566, training loss: 3951.98, average training loss: 4283.12, base loss: 4228.95
[INFO 2017-06-27 20:05:29,106 main.py:51] epoch 567, training loss: 3782.26, average training loss: 4282.23, base loss: 4229.26
[INFO 2017-06-27 20:05:29,480 main.py:51] epoch 568, training loss: 3177.43, average training loss: 4280.29, base loss: 4228.00
[INFO 2017-06-27 20:05:29,880 main.py:51] epoch 569, training loss: 3562.63, average training loss: 4279.03, base loss: 4227.71
[INFO 2017-06-27 20:05:30,255 main.py:51] epoch 570, training loss: 3575.94, average training loss: 4277.80, base loss: 4227.60
[INFO 2017-06-27 20:05:30,598 main.py:51] epoch 571, training loss: 3708.87, average training loss: 4276.81, base loss: 4227.26
[INFO 2017-06-27 20:05:31,036 main.py:51] epoch 572, training loss: 3879.35, average training loss: 4276.11, base loss: 4227.24
[INFO 2017-06-27 20:05:31,391 main.py:51] epoch 573, training loss: 3587.45, average training loss: 4274.91, base loss: 4227.13
[INFO 2017-06-27 20:05:31,813 main.py:51] epoch 574, training loss: 4066.87, average training loss: 4274.55, base loss: 4227.96
[INFO 2017-06-27 20:05:32,205 main.py:51] epoch 575, training loss: 3968.23, average training loss: 4274.02, base loss: 4228.41
[INFO 2017-06-27 20:05:32,639 main.py:51] epoch 576, training loss: 3097.79, average training loss: 4271.98, base loss: 4227.21
[INFO 2017-06-27 20:05:33,060 main.py:51] epoch 577, training loss: 3126.53, average training loss: 4270.00, base loss: 4226.04
[INFO 2017-06-27 20:05:33,519 main.py:51] epoch 578, training loss: 3376.65, average training loss: 4268.46, base loss: 4225.31
[INFO 2017-06-27 20:05:33,906 main.py:51] epoch 579, training loss: 3457.45, average training loss: 4267.06, base loss: 4224.80
[INFO 2017-06-27 20:05:34,350 main.py:51] epoch 580, training loss: 3900.49, average training loss: 4266.43, base loss: 4225.46
[INFO 2017-06-27 20:05:34,717 main.py:51] epoch 581, training loss: 3231.67, average training loss: 4264.65, base loss: 4224.55
[INFO 2017-06-27 20:05:35,075 main.py:51] epoch 582, training loss: 3275.49, average training loss: 4262.95, base loss: 4223.66
[INFO 2017-06-27 20:05:35,418 main.py:51] epoch 583, training loss: 3496.23, average training loss: 4261.64, base loss: 4223.40
[INFO 2017-06-27 20:05:35,763 main.py:51] epoch 584, training loss: 4068.98, average training loss: 4261.31, base loss: 4224.38
[INFO 2017-06-27 20:05:36,137 main.py:51] epoch 585, training loss: 3864.27, average training loss: 4260.63, base loss: 4225.02
[INFO 2017-06-27 20:05:36,475 main.py:51] epoch 586, training loss: 4184.63, average training loss: 4260.50, base loss: 4226.23
[INFO 2017-06-27 20:05:36,814 main.py:51] epoch 587, training loss: 3566.04, average training loss: 4259.32, base loss: 4225.73
[INFO 2017-06-27 20:05:37,154 main.py:51] epoch 588, training loss: 3390.25, average training loss: 4257.85, base loss: 4225.20
[INFO 2017-06-27 20:05:37,503 main.py:51] epoch 589, training loss: 3208.19, average training loss: 4256.07, base loss: 4224.36
[INFO 2017-06-27 20:05:37,839 main.py:51] epoch 590, training loss: 3277.85, average training loss: 4254.41, base loss: 4223.49
[INFO 2017-06-27 20:05:38,175 main.py:51] epoch 591, training loss: 3177.25, average training loss: 4252.59, base loss: 4222.63
[INFO 2017-06-27 20:05:38,529 main.py:51] epoch 592, training loss: 3227.70, average training loss: 4250.87, base loss: 4221.28
[INFO 2017-06-27 20:05:38,865 main.py:51] epoch 593, training loss: 3262.99, average training loss: 4249.20, base loss: 4220.41
[INFO 2017-06-27 20:05:39,203 main.py:51] epoch 594, training loss: 3639.24, average training loss: 4248.18, base loss: 4220.28
[INFO 2017-06-27 20:05:39,540 main.py:51] epoch 595, training loss: 3114.62, average training loss: 4246.28, base loss: 4219.10
[INFO 2017-06-27 20:05:39,877 main.py:51] epoch 596, training loss: 3316.44, average training loss: 4244.72, base loss: 4218.55
[INFO 2017-06-27 20:05:40,217 main.py:51] epoch 597, training loss: 4081.30, average training loss: 4244.45, base loss: 4219.06
[INFO 2017-06-27 20:05:40,552 main.py:51] epoch 598, training loss: 4294.08, average training loss: 4244.53, base loss: 4220.46
[INFO 2017-06-27 20:05:40,902 main.py:51] epoch 599, training loss: 3596.83, average training loss: 4243.45, base loss: 4220.17
[INFO 2017-06-27 20:05:40,902 main.py:53] epoch 599, testing
[INFO 2017-06-27 20:05:42,417 main.py:105] average testing loss: 4397.35, base loss: 4921.88
[INFO 2017-06-27 20:05:42,417 main.py:106] improve_loss: 524.53, improve_percent: 0.11
[INFO 2017-06-27 20:05:42,418 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:05:42,430 main.py:76] current best improved percent: 0.11
[INFO 2017-06-27 20:05:42,777 main.py:51] epoch 600, training loss: 3589.47, average training loss: 4242.36, base loss: 4220.04
[INFO 2017-06-27 20:05:43,124 main.py:51] epoch 601, training loss: 3619.46, average training loss: 4241.33, base loss: 4219.59
[INFO 2017-06-27 20:05:43,474 main.py:51] epoch 602, training loss: 3218.85, average training loss: 4239.63, base loss: 4218.85
[INFO 2017-06-27 20:05:43,834 main.py:51] epoch 603, training loss: 3697.86, average training loss: 4238.73, base loss: 4218.95
[INFO 2017-06-27 20:05:44,183 main.py:51] epoch 604, training loss: 3671.74, average training loss: 4237.80, base loss: 4218.94
[INFO 2017-06-27 20:05:44,528 main.py:51] epoch 605, training loss: 3228.11, average training loss: 4236.13, base loss: 4217.89
[INFO 2017-06-27 20:05:44,872 main.py:51] epoch 606, training loss: 4119.87, average training loss: 4235.94, base loss: 4218.69
[INFO 2017-06-27 20:05:45,219 main.py:51] epoch 607, training loss: 3772.45, average training loss: 4235.18, base loss: 4218.96
[INFO 2017-06-27 20:05:45,563 main.py:51] epoch 608, training loss: 4149.07, average training loss: 4235.03, base loss: 4219.95
[INFO 2017-06-27 20:05:45,910 main.py:51] epoch 609, training loss: 3449.46, average training loss: 4233.75, base loss: 4219.48
[INFO 2017-06-27 20:05:46,256 main.py:51] epoch 610, training loss: 3775.66, average training loss: 4233.00, base loss: 4220.16
[INFO 2017-06-27 20:05:46,601 main.py:51] epoch 611, training loss: 3459.21, average training loss: 4231.73, base loss: 4219.65
[INFO 2017-06-27 20:05:46,945 main.py:51] epoch 612, training loss: 3461.16, average training loss: 4230.48, base loss: 4218.86
[INFO 2017-06-27 20:05:47,294 main.py:51] epoch 613, training loss: 3527.64, average training loss: 4229.33, base loss: 4218.34
[INFO 2017-06-27 20:05:47,640 main.py:51] epoch 614, training loss: 7263.46, average training loss: 4234.26, base loss: 4223.53
[INFO 2017-06-27 20:05:47,989 main.py:51] epoch 615, training loss: 3917.55, average training loss: 4233.75, base loss: 4224.15
[INFO 2017-06-27 20:05:48,333 main.py:51] epoch 616, training loss: 3543.47, average training loss: 4232.63, base loss: 4223.57
[INFO 2017-06-27 20:05:48,680 main.py:51] epoch 617, training loss: 3540.90, average training loss: 4231.51, base loss: 4223.23
[INFO 2017-06-27 20:05:49,024 main.py:51] epoch 618, training loss: 4374.22, average training loss: 4231.74, base loss: 4224.49
[INFO 2017-06-27 20:05:49,369 main.py:51] epoch 619, training loss: 3966.43, average training loss: 4231.31, base loss: 4225.22
[INFO 2017-06-27 20:05:49,715 main.py:51] epoch 620, training loss: 4455.54, average training loss: 4231.68, base loss: 4226.90
[INFO 2017-06-27 20:05:50,060 main.py:51] epoch 621, training loss: 3781.47, average training loss: 4230.95, base loss: 4226.81
[INFO 2017-06-27 20:05:50,404 main.py:51] epoch 622, training loss: 4358.77, average training loss: 4231.16, base loss: 4228.06
[INFO 2017-06-27 20:05:50,749 main.py:51] epoch 623, training loss: 3558.12, average training loss: 4230.08, base loss: 4227.51
[INFO 2017-06-27 20:05:51,097 main.py:51] epoch 624, training loss: 3778.14, average training loss: 4229.36, base loss: 4227.78
[INFO 2017-06-27 20:05:51,443 main.py:51] epoch 625, training loss: 4248.22, average training loss: 4229.39, base loss: 4228.79
[INFO 2017-06-27 20:05:51,790 main.py:51] epoch 626, training loss: 3470.65, average training loss: 4228.18, base loss: 4228.15
[INFO 2017-06-27 20:05:52,136 main.py:51] epoch 627, training loss: 3361.62, average training loss: 4226.80, base loss: 4227.41
[INFO 2017-06-27 20:05:52,487 main.py:51] epoch 628, training loss: 3049.36, average training loss: 4224.92, base loss: 4226.28
[INFO 2017-06-27 20:05:52,832 main.py:51] epoch 629, training loss: 3294.70, average training loss: 4223.45, base loss: 4225.44
[INFO 2017-06-27 20:05:53,176 main.py:51] epoch 630, training loss: 3634.55, average training loss: 4222.51, base loss: 4225.26
[INFO 2017-06-27 20:05:53,521 main.py:51] epoch 631, training loss: 3432.21, average training loss: 4221.26, base loss: 4224.78
[INFO 2017-06-27 20:05:53,869 main.py:51] epoch 632, training loss: 3169.53, average training loss: 4219.60, base loss: 4223.71
[INFO 2017-06-27 20:05:54,214 main.py:51] epoch 633, training loss: 4034.24, average training loss: 4219.31, base loss: 4224.22
[INFO 2017-06-27 20:05:54,558 main.py:51] epoch 634, training loss: 3358.22, average training loss: 4217.95, base loss: 4223.56
[INFO 2017-06-27 20:05:54,907 main.py:51] epoch 635, training loss: 3730.44, average training loss: 4217.19, base loss: 4223.64
[INFO 2017-06-27 20:05:55,254 main.py:51] epoch 636, training loss: 3465.90, average training loss: 4216.01, base loss: 4223.10
[INFO 2017-06-27 20:05:55,604 main.py:51] epoch 637, training loss: 3452.45, average training loss: 4214.81, base loss: 4222.49
[INFO 2017-06-27 20:05:55,954 main.py:51] epoch 638, training loss: 3629.82, average training loss: 4213.90, base loss: 4222.69
[INFO 2017-06-27 20:05:56,299 main.py:51] epoch 639, training loss: 3507.63, average training loss: 4212.79, base loss: 4222.14
[INFO 2017-06-27 20:05:56,647 main.py:51] epoch 640, training loss: 7631.89, average training loss: 4218.13, base loss: 4228.74
[INFO 2017-06-27 20:05:56,996 main.py:51] epoch 641, training loss: 4417.00, average training loss: 4218.44, base loss: 4230.40
[INFO 2017-06-27 20:05:57,349 main.py:51] epoch 642, training loss: 3819.97, average training loss: 4217.82, base loss: 4230.64
[INFO 2017-06-27 20:05:57,698 main.py:51] epoch 643, training loss: 3297.30, average training loss: 4216.39, base loss: 4229.78
[INFO 2017-06-27 20:05:58,052 main.py:51] epoch 644, training loss: 3128.49, average training loss: 4214.70, base loss: 4228.42
[INFO 2017-06-27 20:05:58,408 main.py:51] epoch 645, training loss: 3152.68, average training loss: 4213.06, base loss: 4227.15
[INFO 2017-06-27 20:05:58,765 main.py:51] epoch 646, training loss: 3123.41, average training loss: 4211.37, base loss: 4225.98
[INFO 2017-06-27 20:05:59,118 main.py:51] epoch 647, training loss: 3496.61, average training loss: 4210.27, base loss: 4225.55
[INFO 2017-06-27 20:05:59,470 main.py:51] epoch 648, training loss: 3335.83, average training loss: 4208.92, base loss: 4224.66
[INFO 2017-06-27 20:05:59,824 main.py:51] epoch 649, training loss: 4089.15, average training loss: 4208.74, base loss: 4225.48
[INFO 2017-06-27 20:06:00,178 main.py:51] epoch 650, training loss: 3253.82, average training loss: 4207.27, base loss: 4224.73
[INFO 2017-06-27 20:06:00,535 main.py:51] epoch 651, training loss: 3930.52, average training loss: 4206.85, base loss: 4225.34
[INFO 2017-06-27 20:06:00,894 main.py:51] epoch 652, training loss: 3997.99, average training loss: 4206.53, base loss: 4226.25
[INFO 2017-06-27 20:06:01,257 main.py:51] epoch 653, training loss: 3450.47, average training loss: 4205.37, base loss: 4225.39
[INFO 2017-06-27 20:06:01,628 main.py:51] epoch 654, training loss: 2935.15, average training loss: 4203.43, base loss: 4223.80
[INFO 2017-06-27 20:06:01,992 main.py:51] epoch 655, training loss: 3123.94, average training loss: 4201.78, base loss: 4222.63
[INFO 2017-06-27 20:06:02,361 main.py:51] epoch 656, training loss: 3537.69, average training loss: 4200.77, base loss: 4222.49
[INFO 2017-06-27 20:06:02,731 main.py:51] epoch 657, training loss: 3533.33, average training loss: 4199.76, base loss: 4221.99
[INFO 2017-06-27 20:06:03,095 main.py:51] epoch 658, training loss: 3612.61, average training loss: 4198.87, base loss: 4221.84
[INFO 2017-06-27 20:06:03,460 main.py:51] epoch 659, training loss: 4486.93, average training loss: 4199.31, base loss: 4223.45
[INFO 2017-06-27 20:06:03,825 main.py:51] epoch 660, training loss: 3075.67, average training loss: 4197.61, base loss: 4222.19
[INFO 2017-06-27 20:06:04,191 main.py:51] epoch 661, training loss: 4215.91, average training loss: 4197.63, base loss: 4223.23
[INFO 2017-06-27 20:06:04,561 main.py:51] epoch 662, training loss: 2917.52, average training loss: 4195.70, base loss: 4221.77
[INFO 2017-06-27 20:06:04,931 main.py:51] epoch 663, training loss: 4112.07, average training loss: 4195.58, base loss: 4222.78
[INFO 2017-06-27 20:06:05,308 main.py:51] epoch 664, training loss: 3331.29, average training loss: 4194.28, base loss: 4222.27
[INFO 2017-06-27 20:06:05,681 main.py:51] epoch 665, training loss: 3578.38, average training loss: 4193.35, base loss: 4222.24
[INFO 2017-06-27 20:06:06,059 main.py:51] epoch 666, training loss: 3151.76, average training loss: 4191.79, base loss: 4221.17
[INFO 2017-06-27 20:06:06,434 main.py:51] epoch 667, training loss: 3468.13, average training loss: 4190.71, base loss: 4220.72
[INFO 2017-06-27 20:06:06,809 main.py:51] epoch 668, training loss: 3647.53, average training loss: 4189.89, base loss: 4220.86
[INFO 2017-06-27 20:06:07,179 main.py:51] epoch 669, training loss: 3458.53, average training loss: 4188.80, base loss: 4220.55
[INFO 2017-06-27 20:06:07,555 main.py:51] epoch 670, training loss: 3137.56, average training loss: 4187.24, base loss: 4219.47
[INFO 2017-06-27 20:06:07,925 main.py:51] epoch 671, training loss: 2803.50, average training loss: 4185.18, base loss: 4217.93
[INFO 2017-06-27 20:06:08,299 main.py:51] epoch 672, training loss: 3339.57, average training loss: 4183.92, base loss: 4217.36
[INFO 2017-06-27 20:06:08,667 main.py:51] epoch 673, training loss: 3075.93, average training loss: 4182.28, base loss: 4216.13
[INFO 2017-06-27 20:06:09,043 main.py:51] epoch 674, training loss: 3742.09, average training loss: 4181.63, base loss: 4216.16
[INFO 2017-06-27 20:06:09,420 main.py:51] epoch 675, training loss: 3891.62, average training loss: 4181.20, base loss: 4216.32
[INFO 2017-06-27 20:06:09,797 main.py:51] epoch 676, training loss: 6942.81, average training loss: 4185.28, base loss: 4220.77
[INFO 2017-06-27 20:06:10,172 main.py:51] epoch 677, training loss: 4031.42, average training loss: 4185.05, base loss: 4221.65
[INFO 2017-06-27 20:06:10,547 main.py:51] epoch 678, training loss: 3634.20, average training loss: 4184.24, base loss: 4221.57
[INFO 2017-06-27 20:06:10,923 main.py:51] epoch 679, training loss: 3860.95, average training loss: 4183.76, base loss: 4221.91
[INFO 2017-06-27 20:06:11,293 main.py:51] epoch 680, training loss: 3196.77, average training loss: 4182.31, base loss: 4220.92
[INFO 2017-06-27 20:06:11,662 main.py:51] epoch 681, training loss: 3567.68, average training loss: 4181.41, base loss: 4220.19
[INFO 2017-06-27 20:06:12,043 main.py:51] epoch 682, training loss: 3689.17, average training loss: 4180.69, base loss: 4220.21
[INFO 2017-06-27 20:06:12,416 main.py:51] epoch 683, training loss: 3239.05, average training loss: 4179.31, base loss: 4219.41
[INFO 2017-06-27 20:06:12,793 main.py:51] epoch 684, training loss: 3946.88, average training loss: 4178.97, base loss: 4220.05
[INFO 2017-06-27 20:06:13,167 main.py:51] epoch 685, training loss: 2853.43, average training loss: 4177.04, base loss: 4218.63
[INFO 2017-06-27 20:06:13,542 main.py:51] epoch 686, training loss: 3551.50, average training loss: 4176.13, base loss: 4218.49
[INFO 2017-06-27 20:06:13,915 main.py:51] epoch 687, training loss: 7523.40, average training loss: 4181.00, base loss: 4224.39
[INFO 2017-06-27 20:06:14,289 main.py:51] epoch 688, training loss: 3972.36, average training loss: 4180.69, base loss: 4225.21
[INFO 2017-06-27 20:06:14,658 main.py:51] epoch 689, training loss: 3152.79, average training loss: 4179.20, base loss: 4224.08
[INFO 2017-06-27 20:06:15,035 main.py:51] epoch 690, training loss: 3356.16, average training loss: 4178.01, base loss: 4223.68
[INFO 2017-06-27 20:06:15,415 main.py:51] epoch 691, training loss: 3624.89, average training loss: 4177.21, base loss: 4223.99
[INFO 2017-06-27 20:06:15,789 main.py:51] epoch 692, training loss: 3240.50, average training loss: 4175.86, base loss: 4223.01
[INFO 2017-06-27 20:06:16,164 main.py:51] epoch 693, training loss: 4387.77, average training loss: 4176.17, base loss: 4224.07
[INFO 2017-06-27 20:06:16,541 main.py:51] epoch 694, training loss: 3974.93, average training loss: 4175.88, base loss: 4224.74
[INFO 2017-06-27 20:06:16,917 main.py:51] epoch 695, training loss: 3354.62, average training loss: 4174.70, base loss: 4224.46
[INFO 2017-06-27 20:06:17,291 main.py:51] epoch 696, training loss: 7270.23, average training loss: 4179.14, base loss: 4229.63
[INFO 2017-06-27 20:06:17,661 main.py:51] epoch 697, training loss: 6719.36, average training loss: 4182.78, base loss: 4234.10
[INFO 2017-06-27 20:06:18,041 main.py:51] epoch 698, training loss: 3653.24, average training loss: 4182.02, base loss: 4233.91
[INFO 2017-06-27 20:06:18,416 main.py:51] epoch 699, training loss: 2959.01, average training loss: 4180.27, base loss: 4232.62
[INFO 2017-06-27 20:06:18,417 main.py:53] epoch 699, testing
[INFO 2017-06-27 20:06:20,013 main.py:105] average testing loss: 4181.83, base loss: 4742.54
[INFO 2017-06-27 20:06:20,014 main.py:106] improve_loss: 560.71, improve_percent: 0.12
[INFO 2017-06-27 20:06:20,014 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:06:20,026 main.py:76] current best improved percent: 0.12
[INFO 2017-06-27 20:06:20,401 main.py:51] epoch 700, training loss: 3221.95, average training loss: 4178.91, base loss: 4231.61
[INFO 2017-06-27 20:06:20,785 main.py:51] epoch 701, training loss: 3533.11, average training loss: 4177.99, base loss: 4231.34
[INFO 2017-06-27 20:06:21,158 main.py:51] epoch 702, training loss: 3821.27, average training loss: 4177.48, base loss: 4231.44
[INFO 2017-06-27 20:06:21,534 main.py:51] epoch 703, training loss: 6871.46, average training loss: 4181.31, base loss: 4236.09
[INFO 2017-06-27 20:06:21,911 main.py:51] epoch 704, training loss: 3604.65, average training loss: 4180.49, base loss: 4235.94
[INFO 2017-06-27 20:06:22,281 main.py:51] epoch 705, training loss: 3804.66, average training loss: 4179.96, base loss: 4236.22
[INFO 2017-06-27 20:06:22,651 main.py:51] epoch 706, training loss: 3508.84, average training loss: 4179.01, base loss: 4235.83
[INFO 2017-06-27 20:06:23,027 main.py:51] epoch 707, training loss: 3712.34, average training loss: 4178.35, base loss: 4236.10
[INFO 2017-06-27 20:06:23,400 main.py:51] epoch 708, training loss: 3147.21, average training loss: 4176.89, base loss: 4235.13
[INFO 2017-06-27 20:06:23,775 main.py:51] epoch 709, training loss: 3018.79, average training loss: 4175.26, base loss: 4233.78
[INFO 2017-06-27 20:06:24,148 main.py:51] epoch 710, training loss: 3246.63, average training loss: 4173.96, base loss: 4233.23
[INFO 2017-06-27 20:06:24,523 main.py:51] epoch 711, training loss: 3031.91, average training loss: 4172.35, base loss: 4232.13
[INFO 2017-06-27 20:06:24,898 main.py:51] epoch 712, training loss: 4614.03, average training loss: 4172.97, base loss: 4233.86
[INFO 2017-06-27 20:06:25,271 main.py:51] epoch 713, training loss: 2832.01, average training loss: 4171.09, base loss: 4232.45
[INFO 2017-06-27 20:06:25,644 main.py:51] epoch 714, training loss: 3494.88, average training loss: 4170.15, base loss: 4232.17
[INFO 2017-06-27 20:06:26,013 main.py:51] epoch 715, training loss: 3159.71, average training loss: 4168.74, base loss: 4231.35
[INFO 2017-06-27 20:06:26,386 main.py:51] epoch 716, training loss: 3615.08, average training loss: 4167.96, base loss: 4231.23
[INFO 2017-06-27 20:06:26,764 main.py:51] epoch 717, training loss: 6753.71, average training loss: 4171.57, base loss: 4235.94
[INFO 2017-06-27 20:06:27,133 main.py:51] epoch 718, training loss: 3378.49, average training loss: 4170.46, base loss: 4235.73
[INFO 2017-06-27 20:06:27,510 main.py:51] epoch 719, training loss: 3683.67, average training loss: 4169.79, base loss: 4236.07
[INFO 2017-06-27 20:06:27,900 main.py:51] epoch 720, training loss: 2989.25, average training loss: 4168.15, base loss: 4234.82
[INFO 2017-06-27 20:06:28,275 main.py:51] epoch 721, training loss: 3723.68, average training loss: 4167.53, base loss: 4234.98
[INFO 2017-06-27 20:06:28,650 main.py:51] epoch 722, training loss: 3443.84, average training loss: 4166.53, base loss: 4234.82
[INFO 2017-06-27 20:06:29,024 main.py:51] epoch 723, training loss: 3733.74, average training loss: 4165.93, base loss: 4235.21
[INFO 2017-06-27 20:06:29,399 main.py:51] epoch 724, training loss: 3496.06, average training loss: 4165.01, base loss: 4234.88
[INFO 2017-06-27 20:06:29,774 main.py:51] epoch 725, training loss: 3351.27, average training loss: 4163.89, base loss: 4234.81
[INFO 2017-06-27 20:06:30,150 main.py:51] epoch 726, training loss: 3983.69, average training loss: 4163.64, base loss: 4235.22
[INFO 2017-06-27 20:06:30,525 main.py:51] epoch 727, training loss: 3614.49, average training loss: 4162.89, base loss: 4235.28
[INFO 2017-06-27 20:06:30,916 main.py:51] epoch 728, training loss: 3471.37, average training loss: 4161.94, base loss: 4235.26
[INFO 2017-06-27 20:06:31,294 main.py:51] epoch 729, training loss: 3699.98, average training loss: 4161.31, base loss: 4235.32
[INFO 2017-06-27 20:06:31,664 main.py:51] epoch 730, training loss: 3847.39, average training loss: 4160.88, base loss: 4235.79
[INFO 2017-06-27 20:06:32,040 main.py:51] epoch 731, training loss: 3337.67, average training loss: 4159.75, base loss: 4235.33
[INFO 2017-06-27 20:06:32,415 main.py:51] epoch 732, training loss: 4134.58, average training loss: 4159.72, base loss: 4236.16
[INFO 2017-06-27 20:06:32,793 main.py:51] epoch 733, training loss: 3778.04, average training loss: 4159.20, base loss: 4236.49
[INFO 2017-06-27 20:06:33,167 main.py:51] epoch 734, training loss: 2928.51, average training loss: 4157.52, base loss: 4235.25
[INFO 2017-06-27 20:06:33,546 main.py:51] epoch 735, training loss: 3909.13, average training loss: 4157.19, base loss: 4235.77
[INFO 2017-06-27 20:06:33,921 main.py:51] epoch 736, training loss: 3193.48, average training loss: 4155.88, base loss: 4235.04
[INFO 2017-06-27 20:06:34,298 main.py:51] epoch 737, training loss: 3059.62, average training loss: 4154.39, base loss: 4233.83
[INFO 2017-06-27 20:06:34,672 main.py:51] epoch 738, training loss: 3537.74, average training loss: 4153.56, base loss: 4233.72
[INFO 2017-06-27 20:06:35,046 main.py:51] epoch 739, training loss: 3988.29, average training loss: 4153.33, base loss: 4234.45
[INFO 2017-06-27 20:06:35,421 main.py:51] epoch 740, training loss: 3427.77, average training loss: 4152.36, base loss: 4234.02
[INFO 2017-06-27 20:06:35,794 main.py:51] epoch 741, training loss: 3765.95, average training loss: 4151.84, base loss: 4234.24
[INFO 2017-06-27 20:06:36,172 main.py:51] epoch 742, training loss: 3587.10, average training loss: 4151.07, base loss: 4234.32
[INFO 2017-06-27 20:06:36,547 main.py:51] epoch 743, training loss: 3160.12, average training loss: 4149.74, base loss: 4233.49
[INFO 2017-06-27 20:06:36,921 main.py:51] epoch 744, training loss: 3839.08, average training loss: 4149.33, base loss: 4234.05
[INFO 2017-06-27 20:06:37,297 main.py:51] epoch 745, training loss: 3065.78, average training loss: 4147.87, base loss: 4232.83
[INFO 2017-06-27 20:06:37,674 main.py:51] epoch 746, training loss: 4119.68, average training loss: 4147.84, base loss: 4233.71
[INFO 2017-06-27 20:06:38,047 main.py:51] epoch 747, training loss: 3378.12, average training loss: 4146.81, base loss: 4233.18
[INFO 2017-06-27 20:06:38,426 main.py:51] epoch 748, training loss: 4155.56, average training loss: 4146.82, base loss: 4234.06
[INFO 2017-06-27 20:06:38,798 main.py:51] epoch 749, training loss: 3900.75, average training loss: 4146.49, base loss: 4234.72
[INFO 2017-06-27 20:06:39,177 main.py:51] epoch 750, training loss: 2942.12, average training loss: 4144.89, base loss: 4233.46
[INFO 2017-06-27 20:06:39,552 main.py:51] epoch 751, training loss: 3505.90, average training loss: 4144.04, base loss: 4233.11
[INFO 2017-06-27 20:06:39,927 main.py:51] epoch 752, training loss: 3758.51, average training loss: 4143.52, base loss: 4233.21
[INFO 2017-06-27 20:06:40,298 main.py:51] epoch 753, training loss: 3876.31, average training loss: 4143.17, base loss: 4233.75
[INFO 2017-06-27 20:06:40,676 main.py:51] epoch 754, training loss: 4091.82, average training loss: 4143.10, base loss: 4234.53
[INFO 2017-06-27 20:06:41,050 main.py:51] epoch 755, training loss: 3069.75, average training loss: 4141.68, base loss: 4233.46
[INFO 2017-06-27 20:06:41,428 main.py:51] epoch 756, training loss: 3449.70, average training loss: 4140.77, base loss: 4233.15
[INFO 2017-06-27 20:06:41,802 main.py:51] epoch 757, training loss: 3526.77, average training loss: 4139.96, base loss: 4233.03
[INFO 2017-06-27 20:06:42,178 main.py:51] epoch 758, training loss: 3954.31, average training loss: 4139.71, base loss: 4233.47
[INFO 2017-06-27 20:06:42,556 main.py:51] epoch 759, training loss: 3583.69, average training loss: 4138.98, base loss: 4233.54
[INFO 2017-06-27 20:06:42,938 main.py:51] epoch 760, training loss: 3744.07, average training loss: 4138.46, base loss: 4233.81
[INFO 2017-06-27 20:06:43,312 main.py:51] epoch 761, training loss: 3135.03, average training loss: 4137.15, base loss: 4232.90
[INFO 2017-06-27 20:06:43,682 main.py:51] epoch 762, training loss: 3430.61, average training loss: 4136.22, base loss: 4232.53
[INFO 2017-06-27 20:06:44,059 main.py:51] epoch 763, training loss: 7233.24, average training loss: 4140.27, base loss: 4237.02
[INFO 2017-06-27 20:06:44,436 main.py:51] epoch 764, training loss: 4036.82, average training loss: 4140.14, base loss: 4237.96
[INFO 2017-06-27 20:06:44,805 main.py:51] epoch 765, training loss: 3419.65, average training loss: 4139.20, base loss: 4237.46
[INFO 2017-06-27 20:06:45,180 main.py:51] epoch 766, training loss: 3309.97, average training loss: 4138.12, base loss: 4236.92
[INFO 2017-06-27 20:06:45,552 main.py:51] epoch 767, training loss: 3149.84, average training loss: 4136.83, base loss: 4235.92
[INFO 2017-06-27 20:06:45,922 main.py:51] epoch 768, training loss: 4153.83, average training loss: 4136.85, base loss: 4236.84
[INFO 2017-06-27 20:06:46,298 main.py:51] epoch 769, training loss: 3743.54, average training loss: 4136.34, base loss: 4237.12
[INFO 2017-06-27 20:06:46,669 main.py:51] epoch 770, training loss: 3906.10, average training loss: 4136.04, base loss: 4237.85
[INFO 2017-06-27 20:06:47,043 main.py:51] epoch 771, training loss: 3748.78, average training loss: 4135.54, base loss: 4237.82
[INFO 2017-06-27 20:06:47,418 main.py:51] epoch 772, training loss: 3145.74, average training loss: 4134.26, base loss: 4237.07
[INFO 2017-06-27 20:06:47,793 main.py:51] epoch 773, training loss: 3090.50, average training loss: 4132.91, base loss: 4236.14
[INFO 2017-06-27 20:06:48,171 main.py:51] epoch 774, training loss: 6772.38, average training loss: 4136.32, base loss: 4240.09
[INFO 2017-06-27 20:06:48,541 main.py:51] epoch 775, training loss: 3493.96, average training loss: 4135.49, base loss: 4239.69
[INFO 2017-06-27 20:06:48,917 main.py:51] epoch 776, training loss: 3784.77, average training loss: 4135.04, base loss: 4239.96
[INFO 2017-06-27 20:06:49,293 main.py:51] epoch 777, training loss: 3430.14, average training loss: 4134.13, base loss: 4239.35
[INFO 2017-06-27 20:06:49,668 main.py:51] epoch 778, training loss: 3528.19, average training loss: 4133.36, base loss: 4239.24
[INFO 2017-06-27 20:06:50,045 main.py:51] epoch 779, training loss: 2627.76, average training loss: 4131.43, base loss: 4237.49
[INFO 2017-06-27 20:06:50,423 main.py:51] epoch 780, training loss: 3881.57, average training loss: 4131.11, base loss: 4238.02
[INFO 2017-06-27 20:06:50,802 main.py:51] epoch 781, training loss: 3026.07, average training loss: 4129.69, base loss: 4236.99
[INFO 2017-06-27 20:06:51,177 main.py:51] epoch 782, training loss: 3382.29, average training loss: 4128.74, base loss: 4236.54
[INFO 2017-06-27 20:06:51,546 main.py:51] epoch 783, training loss: 3567.59, average training loss: 4128.02, base loss: 4236.57
[INFO 2017-06-27 20:06:51,925 main.py:51] epoch 784, training loss: 3901.79, average training loss: 4127.73, base loss: 4237.03
[INFO 2017-06-27 20:06:52,302 main.py:51] epoch 785, training loss: 6459.16, average training loss: 4130.70, base loss: 4240.17
[INFO 2017-06-27 20:06:52,674 main.py:51] epoch 786, training loss: 3696.06, average training loss: 4130.15, base loss: 4240.10
[INFO 2017-06-27 20:06:53,050 main.py:51] epoch 787, training loss: 3462.95, average training loss: 4129.30, base loss: 4240.05
[INFO 2017-06-27 20:06:53,420 main.py:51] epoch 788, training loss: 3283.32, average training loss: 4128.23, base loss: 4239.58
[INFO 2017-06-27 20:06:53,789 main.py:51] epoch 789, training loss: 3051.17, average training loss: 4126.87, base loss: 4238.66
[INFO 2017-06-27 20:06:54,161 main.py:51] epoch 790, training loss: 2736.19, average training loss: 4125.11, base loss: 4237.25
[INFO 2017-06-27 20:06:54,538 main.py:51] epoch 791, training loss: 3356.81, average training loss: 4124.14, base loss: 4236.68
[INFO 2017-06-27 20:06:54,921 main.py:51] epoch 792, training loss: 3096.20, average training loss: 4122.84, base loss: 4235.83
[INFO 2017-06-27 20:06:55,296 main.py:51] epoch 793, training loss: 3313.68, average training loss: 4121.82, base loss: 4235.15
[INFO 2017-06-27 20:06:55,668 main.py:51] epoch 794, training loss: 3970.04, average training loss: 4121.63, base loss: 4235.62
[INFO 2017-06-27 20:06:56,042 main.py:51] epoch 795, training loss: 2667.80, average training loss: 4119.80, base loss: 4233.85
[INFO 2017-06-27 20:06:56,414 main.py:51] epoch 796, training loss: 3726.35, average training loss: 4119.31, base loss: 4233.93
[INFO 2017-06-27 20:06:56,790 main.py:51] epoch 797, training loss: 3594.12, average training loss: 4118.65, base loss: 4234.19
[INFO 2017-06-27 20:06:57,160 main.py:51] epoch 798, training loss: 3597.36, average training loss: 4118.00, base loss: 4234.11
[INFO 2017-06-27 20:06:57,536 main.py:51] epoch 799, training loss: 2945.72, average training loss: 4116.53, base loss: 4232.84
[INFO 2017-06-27 20:06:57,536 main.py:53] epoch 799, testing
[INFO 2017-06-27 20:06:59,144 main.py:105] average testing loss: 3918.47, base loss: 4509.26
[INFO 2017-06-27 20:06:59,144 main.py:106] improve_loss: 590.79, improve_percent: 0.13
[INFO 2017-06-27 20:06:59,145 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:06:59,157 main.py:76] current best improved percent: 0.13
[INFO 2017-06-27 20:06:59,535 main.py:51] epoch 800, training loss: 3131.86, average training loss: 4115.31, base loss: 4232.31
[INFO 2017-06-27 20:06:59,913 main.py:51] epoch 801, training loss: 3250.65, average training loss: 4114.23, base loss: 4231.79
[INFO 2017-06-27 20:07:00,293 main.py:51] epoch 802, training loss: 3615.85, average training loss: 4113.61, base loss: 4231.94
[INFO 2017-06-27 20:07:00,683 main.py:51] epoch 803, training loss: 3516.11, average training loss: 4112.86, base loss: 4231.83
[INFO 2017-06-27 20:07:01,062 main.py:51] epoch 804, training loss: 3287.65, average training loss: 4111.84, base loss: 4231.49
[INFO 2017-06-27 20:07:01,444 main.py:51] epoch 805, training loss: 4249.58, average training loss: 4112.01, base loss: 4232.55
[INFO 2017-06-27 20:07:01,821 main.py:51] epoch 806, training loss: 7141.11, average training loss: 4115.76, base loss: 4237.07
[INFO 2017-06-27 20:07:02,197 main.py:51] epoch 807, training loss: 3025.59, average training loss: 4114.41, base loss: 4236.04
[INFO 2017-06-27 20:07:02,569 main.py:51] epoch 808, training loss: 3534.33, average training loss: 4113.70, base loss: 4236.01
[INFO 2017-06-27 20:07:02,948 main.py:51] epoch 809, training loss: 3791.51, average training loss: 4113.30, base loss: 4236.46
[INFO 2017-06-27 20:07:03,319 main.py:51] epoch 810, training loss: 3286.47, average training loss: 4112.28, base loss: 4236.05
[INFO 2017-06-27 20:07:03,692 main.py:51] epoch 811, training loss: 2928.52, average training loss: 4110.82, base loss: 4234.84
[INFO 2017-06-27 20:07:04,070 main.py:51] epoch 812, training loss: 3348.74, average training loss: 4109.88, base loss: 4234.38
[INFO 2017-06-27 20:07:04,445 main.py:51] epoch 813, training loss: 3609.35, average training loss: 4109.27, base loss: 4234.14
[INFO 2017-06-27 20:07:04,823 main.py:51] epoch 814, training loss: 3130.98, average training loss: 4108.07, base loss: 4233.40
[INFO 2017-06-27 20:07:05,193 main.py:51] epoch 815, training loss: 3201.24, average training loss: 4106.96, base loss: 4232.74
[INFO 2017-06-27 20:07:05,563 main.py:51] epoch 816, training loss: 4065.84, average training loss: 4106.91, base loss: 4233.52
[INFO 2017-06-27 20:07:05,937 main.py:51] epoch 817, training loss: 3746.06, average training loss: 4106.47, base loss: 4234.03
[INFO 2017-06-27 20:07:06,313 main.py:51] epoch 818, training loss: 3608.28, average training loss: 4105.86, base loss: 4234.01
[INFO 2017-06-27 20:07:06,688 main.py:51] epoch 819, training loss: 3721.10, average training loss: 4105.39, base loss: 4234.22
[INFO 2017-06-27 20:07:07,065 main.py:51] epoch 820, training loss: 3745.76, average training loss: 4104.95, base loss: 4234.46
[INFO 2017-06-27 20:07:07,440 main.py:51] epoch 821, training loss: 3504.35, average training loss: 4104.22, base loss: 4234.33
[INFO 2017-06-27 20:07:07,816 main.py:51] epoch 822, training loss: 3375.80, average training loss: 4103.33, base loss: 4234.18
[INFO 2017-06-27 20:07:08,184 main.py:51] epoch 823, training loss: 3385.07, average training loss: 4102.46, base loss: 4233.67
[INFO 2017-06-27 20:07:08,558 main.py:51] epoch 824, training loss: 3774.75, average training loss: 4102.07, base loss: 4233.74
[INFO 2017-06-27 20:07:08,934 main.py:51] epoch 825, training loss: 3293.99, average training loss: 4101.09, base loss: 4233.34
[INFO 2017-06-27 20:07:09,309 main.py:51] epoch 826, training loss: 3347.33, average training loss: 4100.18, base loss: 4233.12
[INFO 2017-06-27 20:07:09,684 main.py:51] epoch 827, training loss: 3450.57, average training loss: 4099.39, base loss: 4232.84
[INFO 2017-06-27 20:07:10,054 main.py:51] epoch 828, training loss: 3390.22, average training loss: 4098.54, base loss: 4232.55
[INFO 2017-06-27 20:07:10,430 main.py:51] epoch 829, training loss: 4055.69, average training loss: 4098.48, base loss: 4233.26
[INFO 2017-06-27 20:07:10,805 main.py:51] epoch 830, training loss: 3312.06, average training loss: 4097.54, base loss: 4233.01
[INFO 2017-06-27 20:07:11,182 main.py:51] epoch 831, training loss: 3680.47, average training loss: 4097.04, base loss: 4233.16
[INFO 2017-06-27 20:07:11,554 main.py:51] epoch 832, training loss: 3189.15, average training loss: 4095.95, base loss: 4232.34
[INFO 2017-06-27 20:07:11,929 main.py:51] epoch 833, training loss: 3281.44, average training loss: 4094.97, base loss: 4231.70
[INFO 2017-06-27 20:07:12,305 main.py:51] epoch 834, training loss: 3788.21, average training loss: 4094.60, base loss: 4232.23
[INFO 2017-06-27 20:07:12,681 main.py:51] epoch 835, training loss: 2929.81, average training loss: 4093.21, base loss: 4231.12
[INFO 2017-06-27 20:07:13,058 main.py:51] epoch 836, training loss: 3654.86, average training loss: 4092.69, base loss: 4231.16
[INFO 2017-06-27 20:07:13,443 main.py:51] epoch 837, training loss: 3870.58, average training loss: 4092.42, base loss: 4231.53
[INFO 2017-06-27 20:07:13,818 main.py:51] epoch 838, training loss: 3708.84, average training loss: 4091.96, base loss: 4231.53
[INFO 2017-06-27 20:07:14,192 main.py:51] epoch 839, training loss: 7399.25, average training loss: 4095.90, base loss: 4236.02
[INFO 2017-06-27 20:07:14,561 main.py:51] epoch 840, training loss: 3428.90, average training loss: 4095.11, base loss: 4235.82
[INFO 2017-06-27 20:07:14,936 main.py:51] epoch 841, training loss: 3213.12, average training loss: 4094.06, base loss: 4235.16
[INFO 2017-06-27 20:07:15,314 main.py:51] epoch 842, training loss: 3381.05, average training loss: 4093.21, base loss: 4234.74
[INFO 2017-06-27 20:07:15,692 main.py:51] epoch 843, training loss: 7391.48, average training loss: 4097.12, base loss: 4239.46
[INFO 2017-06-27 20:07:16,062 main.py:51] epoch 844, training loss: 3963.59, average training loss: 4096.96, base loss: 4239.86
[INFO 2017-06-27 20:07:16,431 main.py:51] epoch 845, training loss: 3909.02, average training loss: 4096.74, base loss: 4240.31
[INFO 2017-06-27 20:07:16,811 main.py:51] epoch 846, training loss: 7102.76, average training loss: 4100.29, base loss: 4244.45
[INFO 2017-06-27 20:07:17,184 main.py:51] epoch 847, training loss: 3339.62, average training loss: 4099.39, base loss: 4243.77
[INFO 2017-06-27 20:07:17,561 main.py:51] epoch 848, training loss: 3811.64, average training loss: 4099.06, base loss: 4244.06
[INFO 2017-06-27 20:07:17,939 main.py:51] epoch 849, training loss: 3802.82, average training loss: 4098.71, base loss: 4244.35
[INFO 2017-06-27 20:07:18,318 main.py:51] epoch 850, training loss: 3759.39, average training loss: 4098.31, base loss: 4244.39
[INFO 2017-06-27 20:07:18,693 main.py:51] epoch 851, training loss: 7254.18, average training loss: 4102.01, base loss: 4248.98
[INFO 2017-06-27 20:07:19,072 main.py:51] epoch 852, training loss: 4358.62, average training loss: 4102.31, base loss: 4250.06
[INFO 2017-06-27 20:07:19,447 main.py:51] epoch 853, training loss: 2994.83, average training loss: 4101.02, base loss: 4249.02
[INFO 2017-06-27 20:07:19,843 main.py:51] epoch 854, training loss: 6964.10, average training loss: 4104.36, base loss: 4252.88
[INFO 2017-06-27 20:07:20,222 main.py:51] epoch 855, training loss: 7083.86, average training loss: 4107.85, base loss: 4256.69
[INFO 2017-06-27 20:07:20,599 main.py:51] epoch 856, training loss: 3639.69, average training loss: 4107.30, base loss: 4256.44
[INFO 2017-06-27 20:07:20,977 main.py:51] epoch 857, training loss: 4420.41, average training loss: 4107.66, base loss: 4257.48
[INFO 2017-06-27 20:07:21,355 main.py:51] epoch 858, training loss: 3406.13, average training loss: 4106.85, base loss: 4257.12
[INFO 2017-06-27 20:07:21,739 main.py:51] epoch 859, training loss: 3656.05, average training loss: 4106.32, base loss: 4257.18
[INFO 2017-06-27 20:07:22,119 main.py:51] epoch 860, training loss: 3353.09, average training loss: 4105.45, base loss: 4256.92
[INFO 2017-06-27 20:07:22,528 main.py:51] epoch 861, training loss: 3993.84, average training loss: 4105.32, base loss: 4257.63
[INFO 2017-06-27 20:07:22,915 main.py:51] epoch 862, training loss: 3460.35, average training loss: 4104.57, base loss: 4257.37
[INFO 2017-06-27 20:07:23,297 main.py:51] epoch 863, training loss: 3593.55, average training loss: 4103.98, base loss: 4257.56
[INFO 2017-06-27 20:07:23,679 main.py:51] epoch 864, training loss: 3203.29, average training loss: 4102.94, base loss: 4256.66
[INFO 2017-06-27 20:07:24,071 main.py:51] epoch 865, training loss: 3777.36, average training loss: 4102.56, base loss: 4256.96
[INFO 2017-06-27 20:07:24,541 main.py:51] epoch 866, training loss: 3525.17, average training loss: 4101.90, base loss: 4256.93
[INFO 2017-06-27 20:07:24,987 main.py:51] epoch 867, training loss: 3965.29, average training loss: 4101.74, base loss: 4257.55
[INFO 2017-06-27 20:07:25,418 main.py:51] epoch 868, training loss: 4090.42, average training loss: 4101.73, base loss: 4258.23
[INFO 2017-06-27 20:07:25,859 main.py:51] epoch 869, training loss: 3717.67, average training loss: 4101.29, base loss: 4258.59
[INFO 2017-06-27 20:07:26,292 main.py:51] epoch 870, training loss: 3443.41, average training loss: 4100.53, base loss: 4258.18
[INFO 2017-06-27 20:07:26,686 main.py:51] epoch 871, training loss: 3451.36, average training loss: 4099.79, base loss: 4257.74
[INFO 2017-06-27 20:07:27,154 main.py:51] epoch 872, training loss: 3218.73, average training loss: 4098.78, base loss: 4256.93
[INFO 2017-06-27 20:07:27,621 main.py:51] epoch 873, training loss: 3891.03, average training loss: 4098.54, base loss: 4257.36
[INFO 2017-06-27 20:07:28,023 main.py:51] epoch 874, training loss: 6849.08, average training loss: 4101.68, base loss: 4261.19
[INFO 2017-06-27 20:07:28,401 main.py:51] epoch 875, training loss: 3161.89, average training loss: 4100.61, base loss: 4260.41
[INFO 2017-06-27 20:07:28,777 main.py:51] epoch 876, training loss: 3436.29, average training loss: 4099.85, base loss: 4260.24
[INFO 2017-06-27 20:07:29,155 main.py:51] epoch 877, training loss: 3516.84, average training loss: 4099.19, base loss: 4260.42
[INFO 2017-06-27 20:07:29,619 main.py:51] epoch 878, training loss: 3735.27, average training loss: 4098.77, base loss: 4260.75
[INFO 2017-06-27 20:07:30,010 main.py:51] epoch 879, training loss: 3627.13, average training loss: 4098.24, base loss: 4260.83
[INFO 2017-06-27 20:07:30,417 main.py:51] epoch 880, training loss: 3478.08, average training loss: 4097.53, base loss: 4260.62
[INFO 2017-06-27 20:07:30,832 main.py:51] epoch 881, training loss: 3647.56, average training loss: 4097.02, base loss: 4260.94
[INFO 2017-06-27 20:07:31,212 main.py:51] epoch 882, training loss: 3876.08, average training loss: 4096.77, base loss: 4261.59
[INFO 2017-06-27 20:07:31,677 main.py:51] epoch 883, training loss: 7359.15, average training loss: 4100.46, base loss: 4265.88
[INFO 2017-06-27 20:07:32,090 main.py:51] epoch 884, training loss: 3491.49, average training loss: 4099.78, base loss: 4265.84
[INFO 2017-06-27 20:07:32,474 main.py:51] epoch 885, training loss: 6497.19, average training loss: 4102.48, base loss: 4269.03
[INFO 2017-06-27 20:07:32,872 main.py:51] epoch 886, training loss: 3715.98, average training loss: 4102.05, base loss: 4269.14
[INFO 2017-06-27 20:07:33,253 main.py:51] epoch 887, training loss: 3777.17, average training loss: 4101.68, base loss: 4269.18
[INFO 2017-06-27 20:07:33,633 main.py:51] epoch 888, training loss: 3071.04, average training loss: 4100.52, base loss: 4268.22
[INFO 2017-06-27 20:07:34,009 main.py:51] epoch 889, training loss: 3204.34, average training loss: 4099.51, base loss: 4267.47
[INFO 2017-06-27 20:07:34,491 main.py:51] epoch 890, training loss: 3895.15, average training loss: 4099.28, base loss: 4267.88
[INFO 2017-06-27 20:07:34,869 main.py:51] epoch 891, training loss: 3971.07, average training loss: 4099.14, base loss: 4268.46
[INFO 2017-06-27 20:07:35,257 main.py:51] epoch 892, training loss: 3463.87, average training loss: 4098.43, base loss: 4268.45
[INFO 2017-06-27 20:07:35,640 main.py:51] epoch 893, training loss: 2985.32, average training loss: 4097.18, base loss: 4267.46
[INFO 2017-06-27 20:07:36,017 main.py:51] epoch 894, training loss: 3761.57, average training loss: 4096.81, base loss: 4267.93
[INFO 2017-06-27 20:07:36,392 main.py:51] epoch 895, training loss: 3461.30, average training loss: 4096.10, base loss: 4267.62
[INFO 2017-06-27 20:07:36,789 main.py:51] epoch 896, training loss: 3266.84, average training loss: 4095.18, base loss: 4266.97
[INFO 2017-06-27 20:07:37,166 main.py:51] epoch 897, training loss: 2696.49, average training loss: 4093.62, base loss: 4265.39
[INFO 2017-06-27 20:07:37,541 main.py:51] epoch 898, training loss: 3364.72, average training loss: 4092.81, base loss: 4265.03
[INFO 2017-06-27 20:07:37,918 main.py:51] epoch 899, training loss: 3246.36, average training loss: 4091.87, base loss: 4264.42
[INFO 2017-06-27 20:07:37,918 main.py:53] epoch 899, testing
[INFO 2017-06-27 20:07:39,528 main.py:105] average testing loss: 4000.44, base loss: 4475.14
[INFO 2017-06-27 20:07:39,528 main.py:106] improve_loss: 474.70, improve_percent: 0.11
[INFO 2017-06-27 20:07:39,529 main.py:76] current best improved percent: 0.13
[INFO 2017-06-27 20:07:39,910 main.py:51] epoch 900, training loss: 3366.32, average training loss: 4091.06, base loss: 4263.83
[INFO 2017-06-27 20:07:40,286 main.py:51] epoch 901, training loss: 3424.94, average training loss: 4090.32, base loss: 4263.48
[INFO 2017-06-27 20:07:40,662 main.py:51] epoch 902, training loss: 3269.66, average training loss: 4089.41, base loss: 4262.84
[INFO 2017-06-27 20:07:41,035 main.py:51] epoch 903, training loss: 2958.89, average training loss: 4088.16, base loss: 4261.95
[INFO 2017-06-27 20:07:41,415 main.py:51] epoch 904, training loss: 4066.45, average training loss: 4088.14, base loss: 4262.50
[INFO 2017-06-27 20:07:41,790 main.py:51] epoch 905, training loss: 3052.31, average training loss: 4087.00, base loss: 4261.56
[INFO 2017-06-27 20:07:42,163 main.py:51] epoch 906, training loss: 3274.06, average training loss: 4086.10, base loss: 4261.05
[INFO 2017-06-27 20:07:42,536 main.py:51] epoch 907, training loss: 3197.94, average training loss: 4085.12, base loss: 4260.46
[INFO 2017-06-27 20:07:42,912 main.py:51] epoch 908, training loss: 3971.41, average training loss: 4085.00, base loss: 4260.85
[INFO 2017-06-27 20:07:43,287 main.py:51] epoch 909, training loss: 3612.24, average training loss: 4084.48, base loss: 4260.85
[INFO 2017-06-27 20:07:43,665 main.py:51] epoch 910, training loss: 3895.10, average training loss: 4084.27, base loss: 4261.35
[INFO 2017-06-27 20:07:44,038 main.py:51] epoch 911, training loss: 3132.99, average training loss: 4083.23, base loss: 4260.63
[INFO 2017-06-27 20:07:44,416 main.py:51] epoch 912, training loss: 3472.30, average training loss: 4082.56, base loss: 4260.52
[INFO 2017-06-27 20:07:44,793 main.py:51] epoch 913, training loss: 4143.10, average training loss: 4082.62, base loss: 4261.18
[INFO 2017-06-27 20:07:45,170 main.py:51] epoch 914, training loss: 3538.21, average training loss: 4082.03, base loss: 4261.22
[INFO 2017-06-27 20:07:45,546 main.py:51] epoch 915, training loss: 3645.07, average training loss: 4081.55, base loss: 4261.01
[INFO 2017-06-27 20:07:45,924 main.py:51] epoch 916, training loss: 3868.05, average training loss: 4081.32, base loss: 4261.04
[INFO 2017-06-27 20:07:46,302 main.py:51] epoch 917, training loss: 3984.48, average training loss: 4081.21, base loss: 4261.60
[INFO 2017-06-27 20:07:46,680 main.py:51] epoch 918, training loss: 3469.70, average training loss: 4080.55, base loss: 4261.38
[INFO 2017-06-27 20:07:47,062 main.py:51] epoch 919, training loss: 4039.94, average training loss: 4080.50, base loss: 4262.13
[INFO 2017-06-27 20:07:47,440 main.py:51] epoch 920, training loss: 3271.83, average training loss: 4079.63, base loss: 4261.57
[INFO 2017-06-27 20:07:47,814 main.py:51] epoch 921, training loss: 2699.50, average training loss: 4078.13, base loss: 4260.10
[INFO 2017-06-27 20:07:48,188 main.py:51] epoch 922, training loss: 2761.96, average training loss: 4076.70, base loss: 4258.73
[INFO 2017-06-27 20:07:48,562 main.py:51] epoch 923, training loss: 3881.22, average training loss: 4076.49, base loss: 4259.38
[INFO 2017-06-27 20:07:48,937 main.py:51] epoch 924, training loss: 3212.89, average training loss: 4075.56, base loss: 4258.87
[INFO 2017-06-27 20:07:49,311 main.py:51] epoch 925, training loss: 3601.99, average training loss: 4075.05, base loss: 4259.04
[INFO 2017-06-27 20:07:49,686 main.py:51] epoch 926, training loss: 4048.69, average training loss: 4075.02, base loss: 4259.61
[INFO 2017-06-27 20:07:50,060 main.py:51] epoch 927, training loss: 3847.77, average training loss: 4074.77, base loss: 4259.80
[INFO 2017-06-27 20:07:50,434 main.py:51] epoch 928, training loss: 3478.38, average training loss: 4074.13, base loss: 4259.66
[INFO 2017-06-27 20:07:50,808 main.py:51] epoch 929, training loss: 3895.57, average training loss: 4073.94, base loss: 4260.32
[INFO 2017-06-27 20:07:51,179 main.py:51] epoch 930, training loss: 3791.43, average training loss: 4073.64, base loss: 4260.80
[INFO 2017-06-27 20:07:51,550 main.py:51] epoch 931, training loss: 6957.08, average training loss: 4076.73, base loss: 4264.30
[INFO 2017-06-27 20:07:51,930 main.py:51] epoch 932, training loss: 3398.63, average training loss: 4076.00, base loss: 4264.25
[INFO 2017-06-27 20:07:52,304 main.py:51] epoch 933, training loss: 3202.50, average training loss: 4075.07, base loss: 4263.64
[INFO 2017-06-27 20:07:52,680 main.py:51] epoch 934, training loss: 3191.87, average training loss: 4074.12, base loss: 4262.99
[INFO 2017-06-27 20:07:53,058 main.py:51] epoch 935, training loss: 2791.06, average training loss: 4072.75, base loss: 4261.76
[INFO 2017-06-27 20:07:53,439 main.py:51] epoch 936, training loss: 3876.97, average training loss: 4072.54, base loss: 4262.15
[INFO 2017-06-27 20:07:53,816 main.py:51] epoch 937, training loss: 3507.55, average training loss: 4071.94, base loss: 4261.95
[INFO 2017-06-27 20:07:54,191 main.py:51] epoch 938, training loss: 3177.34, average training loss: 4070.99, base loss: 4261.26
[INFO 2017-06-27 20:07:54,565 main.py:51] epoch 939, training loss: 3783.27, average training loss: 4070.68, base loss: 4261.18
[INFO 2017-06-27 20:07:54,941 main.py:51] epoch 940, training loss: 3132.15, average training loss: 4069.68, base loss: 4260.41
[INFO 2017-06-27 20:07:55,413 main.py:51] epoch 941, training loss: 3773.72, average training loss: 4069.37, base loss: 4260.64
[INFO 2017-06-27 20:07:55,812 main.py:51] epoch 942, training loss: 2809.35, average training loss: 4068.03, base loss: 4259.33
[INFO 2017-06-27 20:07:56,189 main.py:51] epoch 943, training loss: 3698.22, average training loss: 4067.64, base loss: 4259.35
[INFO 2017-06-27 20:07:56,582 main.py:51] epoch 944, training loss: 3235.33, average training loss: 4066.76, base loss: 4258.66
[INFO 2017-06-27 20:07:56,958 main.py:51] epoch 945, training loss: 3365.90, average training loss: 4066.02, base loss: 4258.32
[INFO 2017-06-27 20:07:57,336 main.py:51] epoch 946, training loss: 3352.26, average training loss: 4065.27, base loss: 4257.85
[INFO 2017-06-27 20:07:57,710 main.py:51] epoch 947, training loss: 2947.39, average training loss: 4064.09, base loss: 4256.93
[INFO 2017-06-27 20:07:58,083 main.py:51] epoch 948, training loss: 3592.69, average training loss: 4063.59, base loss: 4256.88
[INFO 2017-06-27 20:07:58,463 main.py:51] epoch 949, training loss: 3236.39, average training loss: 4062.72, base loss: 4256.37
[INFO 2017-06-27 20:07:58,840 main.py:51] epoch 950, training loss: 3408.66, average training loss: 4062.03, base loss: 4256.07
[INFO 2017-06-27 20:07:59,213 main.py:51] epoch 951, training loss: 3699.49, average training loss: 4061.65, base loss: 4256.21
[INFO 2017-06-27 20:07:59,589 main.py:51] epoch 952, training loss: 3581.78, average training loss: 4061.15, base loss: 4256.04
[INFO 2017-06-27 20:07:59,960 main.py:51] epoch 953, training loss: 3945.30, average training loss: 4061.03, base loss: 4256.66
[INFO 2017-06-27 20:08:00,333 main.py:51] epoch 954, training loss: 3529.85, average training loss: 4060.47, base loss: 4256.81
[INFO 2017-06-27 20:08:00,718 main.py:51] epoch 955, training loss: 3743.85, average training loss: 4060.14, base loss: 4257.12
[INFO 2017-06-27 20:08:01,093 main.py:51] epoch 956, training loss: 3342.06, average training loss: 4059.39, base loss: 4256.91
[INFO 2017-06-27 20:08:01,473 main.py:51] epoch 957, training loss: 3066.25, average training loss: 4058.35, base loss: 4256.19
[INFO 2017-06-27 20:08:01,850 main.py:51] epoch 958, training loss: 2888.19, average training loss: 4057.13, base loss: 4255.04
[INFO 2017-06-27 20:08:02,230 main.py:51] epoch 959, training loss: 3543.09, average training loss: 4056.60, base loss: 4254.86
[INFO 2017-06-27 20:08:02,604 main.py:51] epoch 960, training loss: 3034.92, average training loss: 4055.53, base loss: 4254.13
[INFO 2017-06-27 20:08:03,081 main.py:51] epoch 961, training loss: 3662.32, average training loss: 4055.12, base loss: 4254.16
[INFO 2017-06-27 20:08:03,460 main.py:51] epoch 962, training loss: 3721.50, average training loss: 4054.78, base loss: 4254.33
[INFO 2017-06-27 20:08:03,839 main.py:51] epoch 963, training loss: 3438.97, average training loss: 4054.14, base loss: 4254.05
[INFO 2017-06-27 20:08:04,221 main.py:51] epoch 964, training loss: 3259.39, average training loss: 4053.32, base loss: 4253.66
[INFO 2017-06-27 20:08:04,605 main.py:51] epoch 965, training loss: 3527.45, average training loss: 4052.77, base loss: 4253.31
[INFO 2017-06-27 20:08:04,977 main.py:51] epoch 966, training loss: 3249.00, average training loss: 4051.94, base loss: 4252.90
[INFO 2017-06-27 20:08:05,350 main.py:51] epoch 967, training loss: 3441.01, average training loss: 4051.31, base loss: 4252.69
[INFO 2017-06-27 20:08:05,721 main.py:51] epoch 968, training loss: 3105.39, average training loss: 4050.33, base loss: 4251.90
[INFO 2017-06-27 20:08:06,096 main.py:51] epoch 969, training loss: 3393.45, average training loss: 4049.66, base loss: 4251.62
[INFO 2017-06-27 20:08:06,469 main.py:51] epoch 970, training loss: 3249.92, average training loss: 4048.83, base loss: 4250.91
[INFO 2017-06-27 20:08:06,850 main.py:51] epoch 971, training loss: 3048.71, average training loss: 4047.80, base loss: 4249.99
[INFO 2017-06-27 20:08:07,228 main.py:51] epoch 972, training loss: 3885.34, average training loss: 4047.64, base loss: 4250.64
[INFO 2017-06-27 20:08:07,603 main.py:51] epoch 973, training loss: 4168.50, average training loss: 4047.76, base loss: 4251.33
[INFO 2017-06-27 20:08:07,982 main.py:51] epoch 974, training loss: 3200.91, average training loss: 4046.89, base loss: 4250.76
[INFO 2017-06-27 20:08:08,358 main.py:51] epoch 975, training loss: 2903.32, average training loss: 4045.72, base loss: 4249.74
[INFO 2017-06-27 20:08:08,732 main.py:51] epoch 976, training loss: 6696.73, average training loss: 4048.43, base loss: 4252.85
[INFO 2017-06-27 20:08:09,123 main.py:51] epoch 977, training loss: 3777.43, average training loss: 4048.16, base loss: 4253.32
[INFO 2017-06-27 20:08:09,499 main.py:51] epoch 978, training loss: 3584.31, average training loss: 4047.68, base loss: 4253.44
[INFO 2017-06-27 20:08:09,880 main.py:51] epoch 979, training loss: 3476.02, average training loss: 4047.10, base loss: 4253.24
[INFO 2017-06-27 20:08:10,256 main.py:51] epoch 980, training loss: 3295.18, average training loss: 4046.33, base loss: 4252.72
[INFO 2017-06-27 20:08:10,634 main.py:51] epoch 981, training loss: 3084.93, average training loss: 4045.35, base loss: 4251.95
[INFO 2017-06-27 20:08:11,011 main.py:51] epoch 982, training loss: 3668.92, average training loss: 4044.97, base loss: 4251.95
[INFO 2017-06-27 20:08:11,385 main.py:51] epoch 983, training loss: 3392.55, average training loss: 4044.31, base loss: 4251.61
[INFO 2017-06-27 20:08:11,764 main.py:51] epoch 984, training loss: 3312.88, average training loss: 4043.56, base loss: 4251.20
[INFO 2017-06-27 20:08:12,144 main.py:51] epoch 985, training loss: 4338.10, average training loss: 4043.86, base loss: 4252.13
[INFO 2017-06-27 20:08:12,520 main.py:51] epoch 986, training loss: 3116.48, average training loss: 4042.92, base loss: 4251.47
[INFO 2017-06-27 20:08:12,895 main.py:51] epoch 987, training loss: 3531.93, average training loss: 4042.41, base loss: 4251.23
[INFO 2017-06-27 20:08:13,265 main.py:51] epoch 988, training loss: 3455.91, average training loss: 4041.81, base loss: 4251.10
[INFO 2017-06-27 20:08:13,642 main.py:51] epoch 989, training loss: 2792.91, average training loss: 4040.55, base loss: 4250.15
[INFO 2017-06-27 20:08:14,016 main.py:51] epoch 990, training loss: 3927.27, average training loss: 4040.44, base loss: 4250.70
[INFO 2017-06-27 20:08:14,395 main.py:51] epoch 991, training loss: 3536.59, average training loss: 4039.93, base loss: 4250.80
[INFO 2017-06-27 20:08:14,771 main.py:51] epoch 992, training loss: 3378.70, average training loss: 4039.26, base loss: 4250.58
[INFO 2017-06-27 20:08:15,146 main.py:51] epoch 993, training loss: 2992.38, average training loss: 4038.21, base loss: 4249.93
[INFO 2017-06-27 20:08:15,521 main.py:51] epoch 994, training loss: 3570.58, average training loss: 4037.74, base loss: 4249.83
[INFO 2017-06-27 20:08:15,899 main.py:51] epoch 995, training loss: 3389.06, average training loss: 4037.09, base loss: 4249.66
[INFO 2017-06-27 20:08:16,277 main.py:51] epoch 996, training loss: 4046.84, average training loss: 4037.10, base loss: 4250.24
[INFO 2017-06-27 20:08:16,652 main.py:51] epoch 997, training loss: 3195.71, average training loss: 4036.26, base loss: 4249.77
[INFO 2017-06-27 20:08:17,026 main.py:51] epoch 998, training loss: 3588.96, average training loss: 4035.81, base loss: 4249.81
[INFO 2017-06-27 20:08:17,403 main.py:51] epoch 999, training loss: 3174.16, average training loss: 4034.95, base loss: 4249.22
[INFO 2017-06-27 20:08:17,403 main.py:53] epoch 999, testing
[INFO 2017-06-27 20:08:19,002 main.py:105] average testing loss: 3611.66, base loss: 4319.22
[INFO 2017-06-27 20:08:19,002 main.py:106] improve_loss: 707.55, improve_percent: 0.16
[INFO 2017-06-27 20:08:19,002 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:08:19,015 main.py:76] current best improved percent: 0.16
[INFO 2017-06-27 20:08:19,394 main.py:51] epoch 1000, training loss: 3545.80, average training loss: 4013.38, base loss: 4250.15
[INFO 2017-06-27 20:08:19,769 main.py:51] epoch 1001, training loss: 3765.57, average training loss: 3996.66, base loss: 4250.34
[INFO 2017-06-27 20:08:20,145 main.py:51] epoch 1002, training loss: 3203.65, average training loss: 3981.21, base loss: 4248.95
[INFO 2017-06-27 20:08:20,517 main.py:51] epoch 1003, training loss: 3721.40, average training loss: 3970.21, base loss: 4248.78
[INFO 2017-06-27 20:08:20,893 main.py:51] epoch 1004, training loss: 3077.68, average training loss: 3959.64, base loss: 4247.66
[INFO 2017-06-27 20:08:21,271 main.py:51] epoch 1005, training loss: 2696.84, average training loss: 3950.53, base loss: 4246.73
[INFO 2017-06-27 20:08:21,646 main.py:51] epoch 1006, training loss: 3898.99, average training loss: 3944.14, base loss: 4247.31
[INFO 2017-06-27 20:08:22,025 main.py:51] epoch 1007, training loss: 2998.11, average training loss: 3938.04, base loss: 4247.35
[INFO 2017-06-27 20:08:22,401 main.py:51] epoch 1008, training loss: 3344.69, average training loss: 3932.51, base loss: 4247.59
[INFO 2017-06-27 20:08:22,777 main.py:51] epoch 1009, training loss: 2716.13, average training loss: 3927.41, base loss: 4247.49
[INFO 2017-06-27 20:08:23,242 main.py:51] epoch 1010, training loss: 3415.62, average training loss: 3922.52, base loss: 4247.36
[INFO 2017-06-27 20:08:23,643 main.py:51] epoch 1011, training loss: 3167.72, average training loss: 3917.92, base loss: 4247.02
[INFO 2017-06-27 20:08:24,038 main.py:51] epoch 1012, training loss: 3430.40, average training loss: 3913.57, base loss: 4246.84
[INFO 2017-06-27 20:08:24,471 main.py:51] epoch 1013, training loss: 2980.99, average training loss: 3910.14, base loss: 4246.91
[INFO 2017-06-27 20:08:24,853 main.py:51] epoch 1014, training loss: 3398.77, average training loss: 3907.29, base loss: 4247.44
[INFO 2017-06-27 20:08:25,250 main.py:51] epoch 1015, training loss: 3012.11, average training loss: 3902.70, base loss: 4245.52
[INFO 2017-06-27 20:08:25,637 main.py:51] epoch 1016, training loss: 2978.47, average training loss: 3898.94, base loss: 4244.75
[INFO 2017-06-27 20:08:26,030 main.py:51] epoch 1017, training loss: 3728.13, average training loss: 3895.72, base loss: 4244.41
[INFO 2017-06-27 20:08:26,407 main.py:51] epoch 1018, training loss: 3873.67, average training loss: 3893.30, base loss: 4244.70
[INFO 2017-06-27 20:08:26,790 main.py:51] epoch 1019, training loss: 3012.38, average training loss: 3890.57, base loss: 4244.36
[INFO 2017-06-27 20:08:27,182 main.py:51] epoch 1020, training loss: 2916.00, average training loss: 3887.44, base loss: 4243.41
[INFO 2017-06-27 20:08:27,565 main.py:51] epoch 1021, training loss: 3128.03, average training loss: 3885.24, base loss: 4243.45
[INFO 2017-06-27 20:08:27,948 main.py:51] epoch 1022, training loss: 3173.17, average training loss: 3882.60, base loss: 4242.90
[INFO 2017-06-27 20:08:28,326 main.py:51] epoch 1023, training loss: 3622.93, average training loss: 3880.98, base loss: 4243.72
[INFO 2017-06-27 20:08:28,725 main.py:51] epoch 1024, training loss: 3296.82, average training loss: 3879.25, base loss: 4243.56
[INFO 2017-06-27 20:08:29,115 main.py:51] epoch 1025, training loss: 3339.02, average training loss: 3877.79, base loss: 4243.82
[INFO 2017-06-27 20:08:29,501 main.py:51] epoch 1026, training loss: 3416.07, average training loss: 3875.58, base loss: 4243.54
[INFO 2017-06-27 20:08:29,885 main.py:51] epoch 1027, training loss: 3821.19, average training loss: 3874.19, base loss: 4243.98
[INFO 2017-06-27 20:08:30,277 main.py:51] epoch 1028, training loss: 3338.21, average training loss: 3873.09, base loss: 4244.61
[INFO 2017-06-27 20:08:30,735 main.py:51] epoch 1029, training loss: 3955.09, average training loss: 3868.46, base loss: 4241.91
[INFO 2017-06-27 20:08:31,171 main.py:51] epoch 1030, training loss: 3237.35, average training loss: 3867.00, base loss: 4242.05
[INFO 2017-06-27 20:08:31,590 main.py:51] epoch 1031, training loss: 3196.33, average training loss: 3866.02, base loss: 4242.38
[INFO 2017-06-27 20:08:32,035 main.py:51] epoch 1032, training loss: 3526.58, average training loss: 3865.12, base loss: 4243.14
[INFO 2017-06-27 20:08:32,474 main.py:51] epoch 1033, training loss: 2577.97, average training loss: 3862.96, base loss: 4242.20
[INFO 2017-06-27 20:08:32,872 main.py:51] epoch 1034, training loss: 3516.23, average training loss: 3862.32, base loss: 4243.05
[INFO 2017-06-27 20:08:33,316 main.py:51] epoch 1035, training loss: 3317.82, average training loss: 3861.45, base loss: 4243.29
[INFO 2017-06-27 20:08:33,796 main.py:51] epoch 1036, training loss: 3460.48, average training loss: 3860.19, base loss: 4243.50
[INFO 2017-06-27 20:08:34,206 main.py:51] epoch 1037, training loss: 3589.43, average training loss: 3858.87, base loss: 4243.53
[INFO 2017-06-27 20:08:34,582 main.py:51] epoch 1038, training loss: 3107.53, average training loss: 3857.90, base loss: 4243.77
[INFO 2017-06-27 20:08:34,958 main.py:51] epoch 1039, training loss: 3537.16, average training loss: 3856.88, base loss: 4243.86
[INFO 2017-06-27 20:08:35,337 main.py:51] epoch 1040, training loss: 3338.82, average training loss: 3851.98, base loss: 4240.16
[INFO 2017-06-27 20:08:35,718 main.py:51] epoch 1041, training loss: 3101.20, average training loss: 3846.69, base loss: 4235.93
[INFO 2017-06-27 20:08:36,179 main.py:51] epoch 1042, training loss: 2961.18, average training loss: 3846.30, base loss: 4236.41
[INFO 2017-06-27 20:08:36,577 main.py:51] epoch 1043, training loss: 3224.70, average training loss: 3845.63, base loss: 4236.65
[INFO 2017-06-27 20:08:36,957 main.py:51] epoch 1044, training loss: 2880.24, average training loss: 3843.85, base loss: 4235.67
[INFO 2017-06-27 20:08:37,336 main.py:51] epoch 1045, training loss: 3435.30, average training loss: 3839.41, base loss: 4232.24
[INFO 2017-06-27 20:08:37,711 main.py:51] epoch 1046, training loss: 4242.44, average training loss: 3839.24, base loss: 4233.57
[INFO 2017-06-27 20:08:38,085 main.py:51] epoch 1047, training loss: 3057.23, average training loss: 3837.69, base loss: 4232.72
[INFO 2017-06-27 20:08:38,456 main.py:51] epoch 1048, training loss: 3007.68, average training loss: 3835.49, base loss: 4231.29
[INFO 2017-06-27 20:08:38,903 main.py:51] epoch 1049, training loss: 3760.81, average training loss: 3835.31, base loss: 4232.22
[INFO 2017-06-27 20:08:39,303 main.py:51] epoch 1050, training loss: 3358.65, average training loss: 3833.92, base loss: 4231.55
[INFO 2017-06-27 20:08:39,719 main.py:51] epoch 1051, training loss: 3519.33, average training loss: 3833.33, base loss: 4231.82
[INFO 2017-06-27 20:08:40,193 main.py:51] epoch 1052, training loss: 3354.73, average training loss: 3833.09, base loss: 4232.31
[INFO 2017-06-27 20:08:40,599 main.py:51] epoch 1053, training loss: 3972.95, average training loss: 3832.31, base loss: 4232.40
[INFO 2017-06-27 20:08:40,983 main.py:51] epoch 1054, training loss: 4412.36, average training loss: 3833.05, base loss: 4234.14
[INFO 2017-06-27 20:08:41,365 main.py:51] epoch 1055, training loss: 3017.64, average training loss: 3832.09, base loss: 4233.85
[INFO 2017-06-27 20:08:41,758 main.py:51] epoch 1056, training loss: 3562.42, average training loss: 3831.10, base loss: 4233.75
[INFO 2017-06-27 20:08:42,137 main.py:51] epoch 1057, training loss: 3223.29, average training loss: 3826.31, base loss: 4229.75
[INFO 2017-06-27 20:08:42,510 main.py:51] epoch 1058, training loss: 7187.50, average training loss: 3826.08, base loss: 4230.62
[INFO 2017-06-27 20:08:42,885 main.py:51] epoch 1059, training loss: 3324.38, average training loss: 3826.22, base loss: 4231.42
[INFO 2017-06-27 20:08:43,261 main.py:51] epoch 1060, training loss: 3731.66, average training loss: 3826.01, base loss: 4232.24
[INFO 2017-06-27 20:08:43,637 main.py:51] epoch 1061, training loss: 3289.72, average training loss: 3825.40, base loss: 4232.17
[INFO 2017-06-27 20:08:44,008 main.py:51] epoch 1062, training loss: 3097.42, average training loss: 3823.70, base loss: 4230.87
[INFO 2017-06-27 20:08:44,389 main.py:51] epoch 1063, training loss: 3608.96, average training loss: 3823.73, base loss: 4231.79
[INFO 2017-06-27 20:08:44,764 main.py:51] epoch 1064, training loss: 4141.60, average training loss: 3823.88, base loss: 4232.90
[INFO 2017-06-27 20:08:45,139 main.py:51] epoch 1065, training loss: 4000.43, average training loss: 3823.83, base loss: 4233.82
[INFO 2017-06-27 20:08:45,511 main.py:51] epoch 1066, training loss: 3295.81, average training loss: 3823.22, base loss: 4233.77
[INFO 2017-06-27 20:08:45,887 main.py:51] epoch 1067, training loss: 3428.29, average training loss: 3822.23, base loss: 4233.25
[INFO 2017-06-27 20:08:46,262 main.py:51] epoch 1068, training loss: 3078.10, average training loss: 3821.35, base loss: 4233.10
[INFO 2017-06-27 20:08:46,638 main.py:51] epoch 1069, training loss: 3777.59, average training loss: 3821.48, base loss: 4233.83
[INFO 2017-06-27 20:08:47,014 main.py:51] epoch 1070, training loss: 2999.66, average training loss: 3820.13, base loss: 4233.00
[INFO 2017-06-27 20:08:47,389 main.py:51] epoch 1071, training loss: 4309.08, average training loss: 3820.47, base loss: 4234.45
[INFO 2017-06-27 20:08:47,784 main.py:51] epoch 1072, training loss: 3036.05, average training loss: 3819.32, base loss: 4233.80
[INFO 2017-06-27 20:08:48,158 main.py:51] epoch 1073, training loss: 3101.51, average training loss: 3818.13, base loss: 4233.15
[INFO 2017-06-27 20:08:48,534 main.py:51] epoch 1074, training loss: 3323.83, average training loss: 3817.51, base loss: 4233.12
[INFO 2017-06-27 20:08:48,908 main.py:51] epoch 1075, training loss: 2790.30, average training loss: 3815.32, base loss: 4231.30
[INFO 2017-06-27 20:08:49,283 main.py:51] epoch 1076, training loss: 3090.61, average training loss: 3814.05, base loss: 4230.63
[INFO 2017-06-27 20:08:49,659 main.py:51] epoch 1077, training loss: 3502.75, average training loss: 3813.26, base loss: 4230.71
[INFO 2017-06-27 20:08:50,034 main.py:51] epoch 1078, training loss: 3832.23, average training loss: 3813.31, base loss: 4231.70
[INFO 2017-06-27 20:08:50,408 main.py:51] epoch 1079, training loss: 3514.23, average training loss: 3812.25, base loss: 4231.26
[INFO 2017-06-27 20:08:50,784 main.py:51] epoch 1080, training loss: 3200.13, average training loss: 3811.16, base loss: 4230.70
[INFO 2017-06-27 20:08:51,159 main.py:51] epoch 1081, training loss: 3683.76, average training loss: 3811.43, base loss: 4231.86
[INFO 2017-06-27 20:08:51,536 main.py:51] epoch 1082, training loss: 3365.04, average training loss: 3810.35, base loss: 4231.50
[INFO 2017-06-27 20:08:51,915 main.py:51] epoch 1083, training loss: 3656.34, average training loss: 3810.42, base loss: 4232.45
[INFO 2017-06-27 20:08:52,289 main.py:51] epoch 1084, training loss: 2909.16, average training loss: 3809.24, base loss: 4231.54
[INFO 2017-06-27 20:08:52,667 main.py:51] epoch 1085, training loss: 2986.02, average training loss: 3808.44, base loss: 4231.28
[INFO 2017-06-27 20:08:53,042 main.py:51] epoch 1086, training loss: 7555.90, average training loss: 3812.61, base loss: 4236.38
[INFO 2017-06-27 20:08:53,420 main.py:51] epoch 1087, training loss: 3580.93, average training loss: 3812.83, base loss: 4237.39
[INFO 2017-06-27 20:08:53,795 main.py:51] epoch 1088, training loss: 3679.09, average training loss: 3812.06, base loss: 4237.32
[INFO 2017-06-27 20:08:54,166 main.py:51] epoch 1089, training loss: 3046.66, average training loss: 3811.18, base loss: 4236.91
[INFO 2017-06-27 20:08:54,553 main.py:51] epoch 1090, training loss: 3032.87, average training loss: 3810.38, base loss: 4236.76
[INFO 2017-06-27 20:08:54,930 main.py:51] epoch 1091, training loss: 2607.45, average training loss: 3807.89, base loss: 4234.51
[INFO 2017-06-27 20:08:55,302 main.py:51] epoch 1092, training loss: 3374.90, average training loss: 3807.14, base loss: 4234.24
[INFO 2017-06-27 20:08:55,750 main.py:51] epoch 1093, training loss: 3479.66, average training loss: 3806.89, base loss: 4234.57
[INFO 2017-06-27 20:08:56,157 main.py:51] epoch 1094, training loss: 3373.87, average training loss: 3806.83, base loss: 4235.14
[INFO 2017-06-27 20:08:56,570 main.py:51] epoch 1095, training loss: 3209.83, average training loss: 3805.94, base loss: 4234.89
[INFO 2017-06-27 20:08:56,982 main.py:51] epoch 1096, training loss: 3871.72, average training loss: 3805.23, base loss: 4235.01
[INFO 2017-06-27 20:08:57,392 main.py:51] epoch 1097, training loss: 3359.05, average training loss: 3804.78, base loss: 4234.96
[INFO 2017-06-27 20:08:57,848 main.py:51] epoch 1098, training loss: 3308.34, average training loss: 3804.26, base loss: 4235.00
[INFO 2017-06-27 20:08:58,227 main.py:51] epoch 1099, training loss: 3584.76, average training loss: 3803.64, base loss: 4234.98
[INFO 2017-06-27 20:08:58,227 main.py:53] epoch 1099, testing
[INFO 2017-06-27 20:09:00,124 main.py:105] average testing loss: 3293.15, base loss: 3917.47
[INFO 2017-06-27 20:09:00,124 main.py:106] improve_loss: 624.32, improve_percent: 0.16
[INFO 2017-06-27 20:09:00,124 main.py:76] current best improved percent: 0.16
[INFO 2017-06-27 20:09:00,568 main.py:51] epoch 1100, training loss: 3495.27, average training loss: 3802.81, base loss: 4234.82
[INFO 2017-06-27 20:09:00,967 main.py:51] epoch 1101, training loss: 2873.48, average training loss: 3801.99, base loss: 4234.49
[INFO 2017-06-27 20:09:01,346 main.py:51] epoch 1102, training loss: 3713.56, average training loss: 3801.61, base loss: 4234.69
[INFO 2017-06-27 20:09:01,734 main.py:51] epoch 1103, training loss: 2991.90, average training loss: 3800.95, base loss: 4234.49
[INFO 2017-06-27 20:09:02,136 main.py:51] epoch 1104, training loss: 4302.68, average training loss: 3800.27, base loss: 4234.74
[INFO 2017-06-27 20:09:02,535 main.py:51] epoch 1105, training loss: 3329.00, average training loss: 3799.58, base loss: 4234.77
[INFO 2017-06-27 20:09:02,921 main.py:51] epoch 1106, training loss: 3273.43, average training loss: 3798.80, base loss: 4234.46
[INFO 2017-06-27 20:09:03,378 main.py:51] epoch 1107, training loss: 3407.09, average training loss: 3797.06, base loss: 4233.25
[INFO 2017-06-27 20:09:03,782 main.py:51] epoch 1108, training loss: 3167.26, average training loss: 3796.60, base loss: 4233.39
[INFO 2017-06-27 20:09:04,255 main.py:51] epoch 1109, training loss: 3413.69, average training loss: 3795.51, base loss: 4233.13
[INFO 2017-06-27 20:09:04,732 main.py:51] epoch 1110, training loss: 3023.33, average training loss: 3793.29, base loss: 4231.37
[INFO 2017-06-27 20:09:05,183 main.py:51] epoch 1111, training loss: 3269.14, average training loss: 3792.08, base loss: 4230.55
[INFO 2017-06-27 20:09:05,598 main.py:51] epoch 1112, training loss: 3367.22, average training loss: 3791.16, base loss: 4230.12
[INFO 2017-06-27 20:09:05,975 main.py:51] epoch 1113, training loss: 3608.45, average training loss: 3790.58, base loss: 4230.23
[INFO 2017-06-27 20:09:06,445 main.py:51] epoch 1114, training loss: 3593.91, average training loss: 3788.97, base loss: 4229.49
[INFO 2017-06-27 20:09:06,860 main.py:51] epoch 1115, training loss: 2880.12, average training loss: 3788.58, base loss: 4229.53
[INFO 2017-06-27 20:09:07,327 main.py:51] epoch 1116, training loss: 2924.54, average training loss: 3788.00, base loss: 4229.45
[INFO 2017-06-27 20:09:07,740 main.py:51] epoch 1117, training loss: 3606.97, average training loss: 3787.41, base loss: 4229.58
[INFO 2017-06-27 20:09:08,133 main.py:51] epoch 1118, training loss: 3660.11, average training loss: 3787.52, base loss: 4230.49
[INFO 2017-06-27 20:09:08,514 main.py:51] epoch 1119, training loss: 3575.49, average training loss: 3786.41, base loss: 4230.30
[INFO 2017-06-27 20:09:08,897 main.py:51] epoch 1120, training loss: 3012.38, average training loss: 3785.21, base loss: 4229.42
[INFO 2017-06-27 20:09:09,273 main.py:51] epoch 1121, training loss: 3252.52, average training loss: 3784.86, base loss: 4229.56
[INFO 2017-06-27 20:09:09,650 main.py:51] epoch 1122, training loss: 3002.25, average training loss: 3784.57, base loss: 4229.58
[INFO 2017-06-27 20:09:10,026 main.py:51] epoch 1123, training loss: 4291.79, average training loss: 3785.17, base loss: 4231.43
[INFO 2017-06-27 20:09:10,402 main.py:51] epoch 1124, training loss: 3516.46, average training loss: 3784.56, base loss: 4231.39
[INFO 2017-06-27 20:09:10,785 main.py:51] epoch 1125, training loss: 3561.75, average training loss: 3783.88, base loss: 4231.42
[INFO 2017-06-27 20:09:11,160 main.py:51] epoch 1126, training loss: 3537.08, average training loss: 3783.62, base loss: 4231.80
[INFO 2017-06-27 20:09:11,537 main.py:51] epoch 1127, training loss: 2987.21, average training loss: 3782.62, base loss: 4231.22
[INFO 2017-06-27 20:09:11,911 main.py:51] epoch 1128, training loss: 3344.64, average training loss: 3780.78, base loss: 4230.04
[INFO 2017-06-27 20:09:12,287 main.py:51] epoch 1129, training loss: 3373.66, average training loss: 3779.53, base loss: 4229.50
[INFO 2017-06-27 20:09:12,667 main.py:51] epoch 1130, training loss: 3507.82, average training loss: 3778.31, base loss: 4228.95
[INFO 2017-06-27 20:09:13,042 main.py:51] epoch 1131, training loss: 3345.84, average training loss: 3777.74, base loss: 4229.05
[INFO 2017-06-27 20:09:13,417 main.py:51] epoch 1132, training loss: 3997.18, average training loss: 3776.93, base loss: 4229.30
[INFO 2017-06-27 20:09:13,797 main.py:51] epoch 1133, training loss: 4229.11, average training loss: 3777.48, base loss: 4230.93
[INFO 2017-06-27 20:09:14,175 main.py:51] epoch 1134, training loss: 3327.20, average training loss: 3776.37, base loss: 4230.35
[INFO 2017-06-27 20:09:14,552 main.py:51] epoch 1135, training loss: 3284.29, average training loss: 3775.36, base loss: 4229.88
[INFO 2017-06-27 20:09:14,933 main.py:51] epoch 1136, training loss: 3348.55, average training loss: 3773.41, base loss: 4228.47
[INFO 2017-06-27 20:09:15,308 main.py:51] epoch 1137, training loss: 3792.99, average training loss: 3772.77, base loss: 4228.63
[INFO 2017-06-27 20:09:15,689 main.py:51] epoch 1138, training loss: 2961.94, average training loss: 3771.63, base loss: 4227.96
[INFO 2017-06-27 20:09:16,069 main.py:51] epoch 1139, training loss: 3068.83, average training loss: 3767.24, base loss: 4224.21
[INFO 2017-06-27 20:09:16,527 main.py:51] epoch 1140, training loss: 3905.72, average training loss: 3767.06, base loss: 4224.58
[INFO 2017-06-27 20:09:16,922 main.py:51] epoch 1141, training loss: 3463.51, average training loss: 3766.78, base loss: 4224.99
[INFO 2017-06-27 20:09:17,376 main.py:51] epoch 1142, training loss: 3263.95, average training loss: 3766.40, base loss: 4225.11
[INFO 2017-06-27 20:09:17,850 main.py:51] epoch 1143, training loss: 3059.35, average training loss: 3765.68, base loss: 4225.03
[INFO 2017-06-27 20:09:18,246 main.py:51] epoch 1144, training loss: 3065.87, average training loss: 3764.49, base loss: 4224.16
[INFO 2017-06-27 20:09:18,627 main.py:51] epoch 1145, training loss: 3846.56, average training loss: 3764.05, base loss: 4224.39
[INFO 2017-06-27 20:09:19,022 main.py:51] epoch 1146, training loss: 3631.73, average training loss: 3763.92, base loss: 4224.80
[INFO 2017-06-27 20:09:19,400 main.py:51] epoch 1147, training loss: 3269.69, average training loss: 3760.29, base loss: 4221.70
[INFO 2017-06-27 20:09:19,775 main.py:51] epoch 1148, training loss: 3506.65, average training loss: 3759.61, base loss: 4221.66
[INFO 2017-06-27 20:09:20,150 main.py:51] epoch 1149, training loss: 3482.39, average training loss: 3759.17, base loss: 4221.67
[INFO 2017-06-27 20:09:20,528 main.py:51] epoch 1150, training loss: 3416.60, average training loss: 3758.76, base loss: 4221.75
[INFO 2017-06-27 20:09:20,906 main.py:51] epoch 1151, training loss: 3686.26, average training loss: 3759.24, base loss: 4222.88
[INFO 2017-06-27 20:09:21,279 main.py:51] epoch 1152, training loss: 3644.28, average training loss: 3759.13, base loss: 4223.33
[INFO 2017-06-27 20:09:21,661 main.py:51] epoch 1153, training loss: 3179.44, average training loss: 3758.47, base loss: 4223.37
[INFO 2017-06-27 20:09:22,037 main.py:51] epoch 1154, training loss: 3268.05, average training loss: 3757.51, base loss: 4222.94
[INFO 2017-06-27 20:09:22,415 main.py:51] epoch 1155, training loss: 3793.41, average training loss: 3756.57, base loss: 4222.53
[INFO 2017-06-27 20:09:22,789 main.py:51] epoch 1156, training loss: 3082.50, average training loss: 3755.58, base loss: 4221.67
[INFO 2017-06-27 20:09:23,170 main.py:51] epoch 1157, training loss: 3416.36, average training loss: 3754.92, base loss: 4221.56
[INFO 2017-06-27 20:09:23,551 main.py:51] epoch 1158, training loss: 3940.18, average training loss: 3755.04, base loss: 4222.56
[INFO 2017-06-27 20:09:23,935 main.py:51] epoch 1159, training loss: 3300.97, average training loss: 3753.82, base loss: 4222.05
[INFO 2017-06-27 20:09:24,321 main.py:51] epoch 1160, training loss: 3157.10, average training loss: 3753.54, base loss: 4222.13
[INFO 2017-06-27 20:09:24,728 main.py:51] epoch 1161, training loss: 2788.69, average training loss: 3751.65, base loss: 4220.79
[INFO 2017-06-27 20:09:25,130 main.py:51] epoch 1162, training loss: 3266.46, average training loss: 3750.33, base loss: 4220.01
[INFO 2017-06-27 20:09:25,515 main.py:51] epoch 1163, training loss: 3849.22, average training loss: 3750.50, base loss: 4221.14
[INFO 2017-06-27 20:09:25,891 main.py:51] epoch 1164, training loss: 2833.73, average training loss: 3749.46, base loss: 4220.48
[INFO 2017-06-27 20:09:26,269 main.py:51] epoch 1165, training loss: 3235.16, average training loss: 3749.56, base loss: 4221.05
[INFO 2017-06-27 20:09:26,643 main.py:51] epoch 1166, training loss: 3133.93, average training loss: 3748.98, base loss: 4220.97
[INFO 2017-06-27 20:09:27,019 main.py:51] epoch 1167, training loss: 3486.24, average training loss: 3748.76, base loss: 4221.13
[INFO 2017-06-27 20:09:27,395 main.py:51] epoch 1168, training loss: 3126.20, average training loss: 3747.93, base loss: 4220.74
[INFO 2017-06-27 20:09:27,776 main.py:51] epoch 1169, training loss: 3592.92, average training loss: 3747.38, base loss: 4220.83
[INFO 2017-06-27 20:09:28,151 main.py:51] epoch 1170, training loss: 3985.99, average training loss: 3747.79, base loss: 4222.13
[INFO 2017-06-27 20:09:28,523 main.py:51] epoch 1171, training loss: 3644.72, average training loss: 3747.11, base loss: 4222.01
[INFO 2017-06-27 20:09:28,899 main.py:51] epoch 1172, training loss: 3055.88, average training loss: 3746.23, base loss: 4221.31
[INFO 2017-06-27 20:09:29,273 main.py:51] epoch 1173, training loss: 2967.95, average training loss: 3745.17, base loss: 4220.77
[INFO 2017-06-27 20:09:29,647 main.py:51] epoch 1174, training loss: 3950.25, average training loss: 3745.18, base loss: 4221.55
[INFO 2017-06-27 20:09:30,020 main.py:51] epoch 1175, training loss: 3376.85, average training loss: 3740.86, base loss: 4217.74
[INFO 2017-06-27 20:09:30,398 main.py:51] epoch 1176, training loss: 3488.23, average training loss: 3739.45, base loss: 4216.95
[INFO 2017-06-27 20:09:30,774 main.py:51] epoch 1177, training loss: 3208.33, average training loss: 3739.57, base loss: 4217.59
[INFO 2017-06-27 20:09:31,144 main.py:51] epoch 1178, training loss: 3493.71, average training loss: 3740.07, base loss: 4218.73
[INFO 2017-06-27 20:09:31,523 main.py:51] epoch 1179, training loss: 3247.76, average training loss: 3740.11, base loss: 4219.22
[INFO 2017-06-27 20:09:31,899 main.py:51] epoch 1180, training loss: 2792.93, average training loss: 3739.52, base loss: 4218.85
[INFO 2017-06-27 20:09:32,275 main.py:51] epoch 1181, training loss: 3679.87, average training loss: 3735.92, base loss: 4216.05
[INFO 2017-06-27 20:09:32,644 main.py:51] epoch 1182, training loss: 3242.04, average training loss: 3735.21, base loss: 4215.93
[INFO 2017-06-27 20:09:33,020 main.py:51] epoch 1183, training loss: 2895.94, average training loss: 3733.11, base loss: 4214.18
[INFO 2017-06-27 20:09:33,395 main.py:51] epoch 1184, training loss: 3973.18, average training loss: 3733.35, base loss: 4215.17
[INFO 2017-06-27 20:09:33,767 main.py:51] epoch 1185, training loss: 3481.89, average training loss: 3732.88, base loss: 4215.36
[INFO 2017-06-27 20:09:34,143 main.py:51] epoch 1186, training loss: 3651.38, average training loss: 3732.70, base loss: 4215.72
[INFO 2017-06-27 20:09:34,516 main.py:51] epoch 1187, training loss: 3145.43, average training loss: 3731.21, base loss: 4214.87
[INFO 2017-06-27 20:09:34,891 main.py:51] epoch 1188, training loss: 3943.56, average training loss: 3731.48, base loss: 4215.96
[INFO 2017-06-27 20:09:35,263 main.py:51] epoch 1189, training loss: 7038.44, average training loss: 3734.48, base loss: 4219.39
[INFO 2017-06-27 20:09:35,651 main.py:51] epoch 1190, training loss: 3350.68, average training loss: 3733.72, base loss: 4219.06
[INFO 2017-06-27 20:09:36,028 main.py:51] epoch 1191, training loss: 2942.35, average training loss: 3732.71, base loss: 4218.26
[INFO 2017-06-27 20:09:36,406 main.py:51] epoch 1192, training loss: 3273.52, average training loss: 3728.60, base loss: 4214.67
[INFO 2017-06-27 20:09:36,785 main.py:51] epoch 1193, training loss: 3378.05, average training loss: 3727.92, base loss: 4214.40
[INFO 2017-06-27 20:09:37,167 main.py:51] epoch 1194, training loss: 3139.46, average training loss: 3726.23, base loss: 4213.08
[INFO 2017-06-27 20:09:37,544 main.py:51] epoch 1195, training loss: 4320.88, average training loss: 3726.56, base loss: 4214.29
[INFO 2017-06-27 20:09:37,921 main.py:51] epoch 1196, training loss: 3366.80, average training loss: 3726.16, base loss: 4214.52
[INFO 2017-06-27 20:09:38,298 main.py:51] epoch 1197, training loss: 2925.50, average training loss: 3723.85, base loss: 4212.49
[INFO 2017-06-27 20:09:38,673 main.py:51] epoch 1198, training loss: 3372.03, average training loss: 3722.21, base loss: 4211.18
[INFO 2017-06-27 20:09:39,056 main.py:51] epoch 1199, training loss: 3602.40, average training loss: 3721.04, base loss: 4210.55
[INFO 2017-06-27 20:09:39,056 main.py:53] epoch 1199, testing
[INFO 2017-06-27 20:09:40,653 main.py:105] average testing loss: 3795.83, base loss: 4548.40
[INFO 2017-06-27 20:09:40,653 main.py:106] improve_loss: 752.57, improve_percent: 0.17
[INFO 2017-06-27 20:09:40,653 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:09:40,666 main.py:76] current best improved percent: 0.17
[INFO 2017-06-27 20:09:41,043 main.py:51] epoch 1200, training loss: 3440.74, average training loss: 3720.34, base loss: 4210.38
[INFO 2017-06-27 20:09:41,418 main.py:51] epoch 1201, training loss: 2848.82, average training loss: 3719.27, base loss: 4209.67
[INFO 2017-06-27 20:09:41,795 main.py:51] epoch 1202, training loss: 3231.93, average training loss: 3718.97, base loss: 4209.97
[INFO 2017-06-27 20:09:42,167 main.py:51] epoch 1203, training loss: 3395.86, average training loss: 3718.52, base loss: 4210.21
[INFO 2017-06-27 20:09:42,543 main.py:51] epoch 1204, training loss: 3754.28, average training loss: 3718.94, base loss: 4211.25
[INFO 2017-06-27 20:09:42,929 main.py:51] epoch 1205, training loss: 4180.76, average training loss: 3718.97, base loss: 4212.17
[INFO 2017-06-27 20:09:43,308 main.py:51] epoch 1206, training loss: 3157.92, average training loss: 3717.94, base loss: 4211.66
[INFO 2017-06-27 20:09:43,689 main.py:51] epoch 1207, training loss: 3154.92, average training loss: 3717.09, base loss: 4211.05
[INFO 2017-06-27 20:09:44,075 main.py:51] epoch 1208, training loss: 3287.71, average training loss: 3716.60, base loss: 4211.19
[INFO 2017-06-27 20:09:44,460 main.py:51] epoch 1209, training loss: 3308.80, average training loss: 3716.53, base loss: 4211.42
[INFO 2017-06-27 20:09:44,850 main.py:51] epoch 1210, training loss: 3174.98, average training loss: 3716.81, base loss: 4212.31
[INFO 2017-06-27 20:09:45,225 main.py:51] epoch 1211, training loss: 3537.42, average training loss: 3715.33, base loss: 4211.11
[INFO 2017-06-27 20:09:45,599 main.py:51] epoch 1212, training loss: 3092.68, average training loss: 3714.73, base loss: 4210.89
[INFO 2017-06-27 20:09:45,976 main.py:51] epoch 1213, training loss: 3376.24, average training loss: 3714.58, base loss: 4211.32
[INFO 2017-06-27 20:09:46,353 main.py:51] epoch 1214, training loss: 3296.17, average training loss: 3710.22, base loss: 4207.38
[INFO 2017-06-27 20:09:46,727 main.py:51] epoch 1215, training loss: 3153.80, average training loss: 3709.26, base loss: 4206.76
[INFO 2017-06-27 20:09:47,103 main.py:51] epoch 1216, training loss: 3434.22, average training loss: 3708.79, base loss: 4206.93
[INFO 2017-06-27 20:09:47,484 main.py:51] epoch 1217, training loss: 2917.99, average training loss: 3707.31, base loss: 4205.72
[INFO 2017-06-27 20:09:47,859 main.py:51] epoch 1218, training loss: 3795.69, average training loss: 3706.64, base loss: 4205.65
[INFO 2017-06-27 20:09:48,331 main.py:51] epoch 1219, training loss: 3831.61, average training loss: 3706.66, base loss: 4206.56
[INFO 2017-06-27 20:09:48,728 main.py:51] epoch 1220, training loss: 3672.55, average training loss: 3706.17, base loss: 4206.91
[INFO 2017-06-27 20:09:49,105 main.py:51] epoch 1221, training loss: 3229.36, average training loss: 3704.67, base loss: 4205.77
[INFO 2017-06-27 20:09:49,560 main.py:51] epoch 1222, training loss: 3246.06, average training loss: 3704.30, base loss: 4205.95
[INFO 2017-06-27 20:09:49,937 main.py:51] epoch 1223, training loss: 3078.90, average training loss: 3703.70, base loss: 4205.45
[INFO 2017-06-27 20:09:50,404 main.py:51] epoch 1224, training loss: 7206.44, average training loss: 3703.15, base loss: 4205.51
[INFO 2017-06-27 20:09:50,794 main.py:51] epoch 1225, training loss: 3303.16, average training loss: 3702.93, base loss: 4205.67
[INFO 2017-06-27 20:09:51,191 main.py:51] epoch 1226, training loss: 3415.99, average training loss: 3702.57, base loss: 4205.81
[INFO 2017-06-27 20:09:51,583 main.py:51] epoch 1227, training loss: 4010.16, average training loss: 3702.85, base loss: 4206.77
[INFO 2017-06-27 20:09:51,956 main.py:51] epoch 1228, training loss: 3690.19, average training loss: 3703.02, base loss: 4207.70
[INFO 2017-06-27 20:09:52,333 main.py:51] epoch 1229, training loss: 3738.31, average training loss: 3702.92, base loss: 4208.33
[INFO 2017-06-27 20:09:52,710 main.py:51] epoch 1230, training loss: 3184.80, average training loss: 3702.27, base loss: 4208.03
[INFO 2017-06-27 20:09:53,086 main.py:51] epoch 1231, training loss: 3697.04, average training loss: 3701.38, base loss: 4207.86
[INFO 2017-06-27 20:09:53,456 main.py:51] epoch 1232, training loss: 2899.77, average training loss: 3701.16, base loss: 4208.22
[INFO 2017-06-27 20:09:53,831 main.py:51] epoch 1233, training loss: 6834.24, average training loss: 3703.27, base loss: 4210.90
[INFO 2017-06-27 20:09:54,206 main.py:51] epoch 1234, training loss: 2999.73, average training loss: 3702.79, base loss: 4210.91
[INFO 2017-06-27 20:09:54,581 main.py:51] epoch 1235, training loss: 3457.48, average training loss: 3701.16, base loss: 4209.61
[INFO 2017-06-27 20:09:54,952 main.py:51] epoch 1236, training loss: 3201.37, average training loss: 3700.59, base loss: 4209.51
[INFO 2017-06-27 20:09:55,325 main.py:51] epoch 1237, training loss: 3484.71, average training loss: 3700.34, base loss: 4209.90
[INFO 2017-06-27 20:09:55,695 main.py:51] epoch 1238, training loss: 3837.73, average training loss: 3699.79, base loss: 4210.08
[INFO 2017-06-27 20:09:56,071 main.py:51] epoch 1239, training loss: 3708.89, average training loss: 3699.69, base loss: 4210.63
[INFO 2017-06-27 20:09:56,454 main.py:51] epoch 1240, training loss: 3765.41, average training loss: 3699.79, base loss: 4211.54
[INFO 2017-06-27 20:09:56,830 main.py:51] epoch 1241, training loss: 3163.52, average training loss: 3699.13, base loss: 4211.42
[INFO 2017-06-27 20:09:57,212 main.py:51] epoch 1242, training loss: 3475.35, average training loss: 3699.10, base loss: 4211.94
[INFO 2017-06-27 20:09:57,588 main.py:51] epoch 1243, training loss: 3402.18, average training loss: 3698.75, base loss: 4212.35
[INFO 2017-06-27 20:09:57,969 main.py:51] epoch 1244, training loss: 3717.46, average training loss: 3698.75, base loss: 4212.78
[INFO 2017-06-27 20:09:58,343 main.py:51] epoch 1245, training loss: 3837.21, average training loss: 3697.49, base loss: 4211.82
[INFO 2017-06-27 20:09:58,718 main.py:51] epoch 1246, training loss: 3690.48, average training loss: 3697.18, base loss: 4212.36
[INFO 2017-06-27 20:09:59,100 main.py:51] epoch 1247, training loss: 3181.18, average training loss: 3696.28, base loss: 4211.73
[INFO 2017-06-27 20:09:59,475 main.py:51] epoch 1248, training loss: 3820.04, average training loss: 3696.02, base loss: 4211.83
[INFO 2017-06-27 20:09:59,944 main.py:51] epoch 1249, training loss: 3346.75, average training loss: 3695.63, base loss: 4212.04
[INFO 2017-06-27 20:10:00,355 main.py:51] epoch 1250, training loss: 3988.69, average training loss: 3695.02, base loss: 4212.10
[INFO 2017-06-27 20:10:00,736 main.py:51] epoch 1251, training loss: 3537.89, average training loss: 3691.39, base loss: 4209.16
[INFO 2017-06-27 20:10:01,113 main.py:51] epoch 1252, training loss: 3608.29, average training loss: 3691.14, base loss: 4209.36
[INFO 2017-06-27 20:10:01,490 main.py:51] epoch 1253, training loss: 3204.00, average training loss: 3690.46, base loss: 4208.85
[INFO 2017-06-27 20:10:01,897 main.py:51] epoch 1254, training loss: 3019.17, average training loss: 3689.77, base loss: 4208.62
[INFO 2017-06-27 20:10:02,275 main.py:51] epoch 1255, training loss: 2733.23, average training loss: 3689.00, base loss: 4208.02
[INFO 2017-06-27 20:10:02,648 main.py:51] epoch 1256, training loss: 3979.88, average training loss: 3688.64, base loss: 4208.31
[INFO 2017-06-27 20:10:03,025 main.py:51] epoch 1257, training loss: 3241.43, average training loss: 3687.10, base loss: 4206.93
[INFO 2017-06-27 20:10:03,402 main.py:51] epoch 1258, training loss: 3534.53, average training loss: 3686.63, base loss: 4206.93
[INFO 2017-06-27 20:10:03,778 main.py:51] epoch 1259, training loss: 3239.96, average training loss: 3685.52, base loss: 4206.34
[INFO 2017-06-27 20:10:04,156 main.py:51] epoch 1260, training loss: 3370.69, average training loss: 3684.92, base loss: 4206.19
[INFO 2017-06-27 20:10:04,537 main.py:51] epoch 1261, training loss: 2977.90, average training loss: 3684.10, base loss: 4205.75
[INFO 2017-06-27 20:10:04,915 main.py:51] epoch 1262, training loss: 3253.70, average training loss: 3680.51, base loss: 4202.71
[INFO 2017-06-27 20:10:05,350 main.py:51] epoch 1263, training loss: 3190.02, average training loss: 3680.05, base loss: 4202.80
[INFO 2017-06-27 20:10:05,740 main.py:51] epoch 1264, training loss: 2956.04, average training loss: 3679.56, base loss: 4202.70
[INFO 2017-06-27 20:10:06,117 main.py:51] epoch 1265, training loss: 3536.27, average training loss: 3679.41, base loss: 4202.76
[INFO 2017-06-27 20:10:06,515 main.py:51] epoch 1266, training loss: 3316.28, average training loss: 3679.27, base loss: 4203.15
[INFO 2017-06-27 20:10:06,891 main.py:51] epoch 1267, training loss: 3320.21, average training loss: 3678.31, base loss: 4202.76
[INFO 2017-06-27 20:10:07,274 main.py:51] epoch 1268, training loss: 3326.47, average training loss: 3674.55, base loss: 4199.40
[INFO 2017-06-27 20:10:07,650 main.py:51] epoch 1269, training loss: 3564.24, average training loss: 3674.69, base loss: 4200.09
[INFO 2017-06-27 20:10:08,027 main.py:51] epoch 1270, training loss: 3740.13, average training loss: 3668.60, base loss: 4194.75
[INFO 2017-06-27 20:10:08,400 main.py:51] epoch 1271, training loss: 3399.43, average training loss: 3668.74, base loss: 4195.48
[INFO 2017-06-27 20:10:08,776 main.py:51] epoch 1272, training loss: 3638.73, average training loss: 3668.60, base loss: 4195.86
[INFO 2017-06-27 20:10:09,152 main.py:51] epoch 1273, training loss: 3209.99, average training loss: 3667.74, base loss: 4195.54
[INFO 2017-06-27 20:10:09,534 main.py:51] epoch 1274, training loss: 9605.29, average training loss: 3673.90, base loss: 4201.78
[INFO 2017-06-27 20:10:09,908 main.py:51] epoch 1275, training loss: 3410.75, average training loss: 3672.73, base loss: 4201.13
[INFO 2017-06-27 20:10:10,356 main.py:51] epoch 1276, training loss: 3243.05, average training loss: 3672.29, base loss: 4201.37
[INFO 2017-06-27 20:10:10,739 main.py:51] epoch 1277, training loss: 7041.67, average training loss: 3675.13, base loss: 4204.59
[INFO 2017-06-27 20:10:11,194 main.py:51] epoch 1278, training loss: 6594.88, average training loss: 3677.34, base loss: 4206.90
[INFO 2017-06-27 20:10:11,596 main.py:51] epoch 1279, training loss: 3584.73, average training loss: 3676.53, base loss: 4206.53
[INFO 2017-06-27 20:10:12,085 main.py:51] epoch 1280, training loss: 3627.11, average training loss: 3675.91, base loss: 4206.27
[INFO 2017-06-27 20:10:12,509 main.py:51] epoch 1281, training loss: 2932.57, average training loss: 3674.68, base loss: 4205.35
[INFO 2017-06-27 20:10:12,969 main.py:51] epoch 1282, training loss: 3257.96, average training loss: 3674.37, base loss: 4205.59
[INFO 2017-06-27 20:10:13,434 main.py:51] epoch 1283, training loss: 4298.02, average training loss: 3675.22, base loss: 4207.51
[INFO 2017-06-27 20:10:13,831 main.py:51] epoch 1284, training loss: 3578.51, average training loss: 3674.81, base loss: 4207.88
[INFO 2017-06-27 20:10:14,210 main.py:51] epoch 1285, training loss: 7272.75, average training loss: 3677.97, base loss: 4211.77
[INFO 2017-06-27 20:10:14,605 main.py:51] epoch 1286, training loss: 3647.97, average training loss: 3678.09, base loss: 4212.64
[INFO 2017-06-27 20:10:14,982 main.py:51] epoch 1287, training loss: 2810.03, average training loss: 3677.27, base loss: 4212.25
[INFO 2017-06-27 20:10:15,359 main.py:51] epoch 1288, training loss: 3092.36, average training loss: 3676.43, base loss: 4211.52
[INFO 2017-06-27 20:10:15,738 main.py:51] epoch 1289, training loss: 3757.60, average training loss: 3676.29, base loss: 4212.13
[INFO 2017-06-27 20:10:16,120 main.py:51] epoch 1290, training loss: 3756.14, average training loss: 3676.73, base loss: 4213.18
[INFO 2017-06-27 20:10:16,498 main.py:51] epoch 1291, training loss: 3491.10, average training loss: 3676.48, base loss: 4213.65
[INFO 2017-06-27 20:10:16,897 main.py:51] epoch 1292, training loss: 3001.10, average training loss: 3672.19, base loss: 4209.71
[INFO 2017-06-27 20:10:17,294 main.py:51] epoch 1293, training loss: 3784.14, average training loss: 3671.38, base loss: 4209.59
[INFO 2017-06-27 20:10:17,672 main.py:51] epoch 1294, training loss: 3105.03, average training loss: 3670.65, base loss: 4209.42
[INFO 2017-06-27 20:10:18,052 main.py:51] epoch 1295, training loss: 3450.23, average training loss: 3669.64, base loss: 4208.75
[INFO 2017-06-27 20:10:18,430 main.py:51] epoch 1296, training loss: 3508.58, average training loss: 3668.34, base loss: 4208.16
[INFO 2017-06-27 20:10:18,810 main.py:51] epoch 1297, training loss: 3267.61, average training loss: 3667.74, base loss: 4207.94
[INFO 2017-06-27 20:10:19,189 main.py:51] epoch 1298, training loss: 3490.21, average training loss: 3667.77, base loss: 4208.37
[INFO 2017-06-27 20:10:19,578 main.py:51] epoch 1299, training loss: 3872.15, average training loss: 3667.29, base loss: 4208.49
[INFO 2017-06-27 20:10:19,578 main.py:53] epoch 1299, testing
[INFO 2017-06-27 20:10:21,185 main.py:105] average testing loss: 3331.65, base loss: 4069.33
[INFO 2017-06-27 20:10:21,185 main.py:106] improve_loss: 737.68, improve_percent: 0.18
[INFO 2017-06-27 20:10:21,186 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:10:21,198 main.py:76] current best improved percent: 0.18
[INFO 2017-06-27 20:10:21,575 main.py:51] epoch 1300, training loss: 3219.14, average training loss: 3667.34, base loss: 4209.04
[INFO 2017-06-27 20:10:21,950 main.py:51] epoch 1301, training loss: 3628.75, average training loss: 3666.69, base loss: 4208.91
[INFO 2017-06-27 20:10:22,370 main.py:51] epoch 1302, training loss: 3102.37, average training loss: 3666.48, base loss: 4209.34
[INFO 2017-06-27 20:10:22,770 main.py:51] epoch 1303, training loss: 3170.11, average training loss: 3666.01, base loss: 4209.26
[INFO 2017-06-27 20:10:23,149 main.py:51] epoch 1304, training loss: 4199.83, average training loss: 3666.97, base loss: 4211.10
[INFO 2017-06-27 20:10:23,526 main.py:51] epoch 1305, training loss: 3503.10, average training loss: 3666.44, base loss: 4211.29
[INFO 2017-06-27 20:10:23,910 main.py:51] epoch 1306, training loss: 3226.34, average training loss: 3665.85, base loss: 4211.12
[INFO 2017-06-27 20:10:24,285 main.py:51] epoch 1307, training loss: 3260.44, average training loss: 3661.66, base loss: 4207.24
[INFO 2017-06-27 20:10:24,699 main.py:51] epoch 1308, training loss: 3224.52, average training loss: 3661.81, base loss: 4207.97
[INFO 2017-06-27 20:10:25,077 main.py:51] epoch 1309, training loss: 3236.06, average training loss: 3660.64, base loss: 4207.23
[INFO 2017-06-27 20:10:25,455 main.py:51] epoch 1310, training loss: 3370.15, average training loss: 3660.34, base loss: 4207.35
[INFO 2017-06-27 20:10:25,834 main.py:51] epoch 1311, training loss: 3088.20, average training loss: 3659.02, base loss: 4206.10
[INFO 2017-06-27 20:10:26,213 main.py:51] epoch 1312, training loss: 2850.20, average training loss: 3657.68, base loss: 4204.72
[INFO 2017-06-27 20:10:26,597 main.py:51] epoch 1313, training loss: 3313.10, average training loss: 3657.31, base loss: 4204.68
[INFO 2017-06-27 20:10:26,978 main.py:51] epoch 1314, training loss: 3225.57, average training loss: 3657.21, base loss: 4205.25
[INFO 2017-06-27 20:10:27,385 main.py:51] epoch 1315, training loss: 3266.22, average training loss: 3655.42, base loss: 4203.78
[INFO 2017-06-27 20:10:27,769 main.py:51] epoch 1316, training loss: 3312.59, average training loss: 3655.17, base loss: 4204.00
[INFO 2017-06-27 20:10:28,151 main.py:51] epoch 1317, training loss: 3345.36, average training loss: 3655.20, base loss: 4204.64
[INFO 2017-06-27 20:10:28,533 main.py:51] epoch 1318, training loss: 7011.52, average training loss: 3658.36, base loss: 4208.08
[INFO 2017-06-27 20:10:28,929 main.py:51] epoch 1319, training loss: 3770.80, average training loss: 3658.34, base loss: 4208.92
[INFO 2017-06-27 20:10:29,404 main.py:51] epoch 1320, training loss: 3218.75, average training loss: 3656.81, base loss: 4207.55
[INFO 2017-06-27 20:10:29,852 main.py:51] epoch 1321, training loss: 3579.72, average training loss: 3656.59, base loss: 4207.70
[INFO 2017-06-27 20:10:30,286 main.py:51] epoch 1322, training loss: 3163.34, average training loss: 3656.13, base loss: 4207.93
[INFO 2017-06-27 20:10:30,715 main.py:51] epoch 1323, training loss: 3483.01, average training loss: 3655.98, base loss: 4208.42
[INFO 2017-06-27 20:10:31,162 main.py:51] epoch 1324, training loss: 3294.52, average training loss: 3655.37, base loss: 4208.27
[INFO 2017-06-27 20:10:31,563 main.py:51] epoch 1325, training loss: 2889.38, average training loss: 3653.95, base loss: 4206.79
[INFO 2017-06-27 20:10:32,022 main.py:51] epoch 1326, training loss: 2958.54, average training loss: 3653.07, base loss: 4206.29
[INFO 2017-06-27 20:10:32,485 main.py:51] epoch 1327, training loss: 3341.08, average training loss: 3653.41, base loss: 4207.16
[INFO 2017-06-27 20:10:32,900 main.py:51] epoch 1328, training loss: 3286.43, average training loss: 3653.65, base loss: 4208.00
[INFO 2017-06-27 20:10:33,279 main.py:51] epoch 1329, training loss: 3322.54, average training loss: 3653.48, base loss: 4208.23
[INFO 2017-06-27 20:10:33,656 main.py:51] epoch 1330, training loss: 3186.43, average training loss: 3653.41, base loss: 4208.59
[INFO 2017-06-27 20:10:34,054 main.py:51] epoch 1331, training loss: 3426.42, average training loss: 3653.55, base loss: 4209.25
[INFO 2017-06-27 20:10:34,457 main.py:51] epoch 1332, training loss: 2873.61, average training loss: 3649.14, base loss: 4205.07
[INFO 2017-06-27 20:10:34,839 main.py:51] epoch 1333, training loss: 3522.21, average training loss: 3648.82, base loss: 4205.56
[INFO 2017-06-27 20:10:35,281 main.py:51] epoch 1334, training loss: 2874.56, average training loss: 3647.48, base loss: 4204.44
[INFO 2017-06-27 20:10:35,728 main.py:51] epoch 1335, training loss: 3145.33, average training loss: 3647.10, base loss: 4204.75
[INFO 2017-06-27 20:10:36,107 main.py:51] epoch 1336, training loss: 3442.54, average training loss: 3646.32, base loss: 4204.28
[INFO 2017-06-27 20:10:36,503 main.py:51] epoch 1337, training loss: 3163.36, average training loss: 3645.72, base loss: 4204.14
[INFO 2017-06-27 20:10:36,884 main.py:51] epoch 1338, training loss: 3722.28, average training loss: 3646.02, base loss: 4205.06
[INFO 2017-06-27 20:10:37,275 main.py:51] epoch 1339, training loss: 3173.24, average training loss: 3645.51, base loss: 4204.74
[INFO 2017-06-27 20:10:37,651 main.py:51] epoch 1340, training loss: 3308.50, average training loss: 3644.44, base loss: 4204.15
[INFO 2017-06-27 20:10:38,034 main.py:51] epoch 1341, training loss: 3328.67, average training loss: 3644.25, base loss: 4204.51
[INFO 2017-06-27 20:10:38,414 main.py:51] epoch 1342, training loss: 3345.29, average training loss: 3643.79, base loss: 4204.53
[INFO 2017-06-27 20:10:38,790 main.py:51] epoch 1343, training loss: 3521.14, average training loss: 3643.37, base loss: 4204.78
[INFO 2017-06-27 20:10:39,166 main.py:51] epoch 1344, training loss: 3481.13, average training loss: 3643.86, base loss: 4206.27
[INFO 2017-06-27 20:10:39,547 main.py:51] epoch 1345, training loss: 3340.89, average training loss: 3643.01, base loss: 4206.06
[INFO 2017-06-27 20:10:39,928 main.py:51] epoch 1346, training loss: 3198.90, average training loss: 3641.77, base loss: 4205.07
[INFO 2017-06-27 20:10:40,299 main.py:51] epoch 1347, training loss: 3152.92, average training loss: 3639.85, base loss: 4203.48
[INFO 2017-06-27 20:10:40,679 main.py:51] epoch 1348, training loss: 3429.15, average training loss: 3639.96, base loss: 4204.01
[INFO 2017-06-27 20:10:41,061 main.py:51] epoch 1349, training loss: 3725.72, average training loss: 3639.68, base loss: 4204.41
[INFO 2017-06-27 20:10:41,437 main.py:51] epoch 1350, training loss: 3247.09, average training loss: 3639.24, base loss: 4204.36
[INFO 2017-06-27 20:10:41,813 main.py:51] epoch 1351, training loss: 3012.73, average training loss: 3637.39, base loss: 4202.76
[INFO 2017-06-27 20:10:42,283 main.py:51] epoch 1352, training loss: 3671.62, average training loss: 3637.72, base loss: 4203.71
[INFO 2017-06-27 20:10:42,665 main.py:51] epoch 1353, training loss: 2534.91, average training loss: 3637.17, base loss: 4203.50
[INFO 2017-06-27 20:10:43,042 main.py:51] epoch 1354, training loss: 3171.46, average training loss: 3632.88, base loss: 4199.62
[INFO 2017-06-27 20:10:43,507 main.py:51] epoch 1355, training loss: 4081.58, average training loss: 3634.21, base loss: 4202.00
[INFO 2017-06-27 20:10:43,922 main.py:51] epoch 1356, training loss: 3552.13, average training loss: 3634.25, base loss: 4202.41
[INFO 2017-06-27 20:10:44,381 main.py:51] epoch 1357, training loss: 2901.09, average training loss: 3633.31, base loss: 4201.75
[INFO 2017-06-27 20:10:44,787 main.py:51] epoch 1358, training loss: 3263.25, average training loss: 3633.00, base loss: 4201.81
[INFO 2017-06-27 20:10:45,199 main.py:51] epoch 1359, training loss: 3154.06, average training loss: 3632.99, base loss: 4202.20
[INFO 2017-06-27 20:10:45,609 main.py:51] epoch 1360, training loss: 2900.39, average training loss: 3632.33, base loss: 4201.83
[INFO 2017-06-27 20:10:45,998 main.py:51] epoch 1361, training loss: 3394.32, average training loss: 3631.95, base loss: 4201.94
[INFO 2017-06-27 20:10:46,385 main.py:51] epoch 1362, training loss: 3242.18, average training loss: 3631.97, base loss: 4202.50
[INFO 2017-06-27 20:10:46,771 main.py:51] epoch 1363, training loss: 3287.62, average training loss: 3631.67, base loss: 4202.53
[INFO 2017-06-27 20:10:47,146 main.py:51] epoch 1364, training loss: 3071.06, average training loss: 3630.64, base loss: 4201.96
[INFO 2017-06-27 20:10:47,531 main.py:51] epoch 1365, training loss: 3208.41, average training loss: 3629.58, base loss: 4201.24
[INFO 2017-06-27 20:10:47,906 main.py:51] epoch 1366, training loss: 2896.46, average training loss: 3628.83, base loss: 4200.67
[INFO 2017-06-27 20:10:48,289 main.py:51] epoch 1367, training loss: 3209.50, average training loss: 3628.61, base loss: 4200.93
[INFO 2017-06-27 20:10:48,669 main.py:51] epoch 1368, training loss: 3217.95, average training loss: 3627.81, base loss: 4200.39
[INFO 2017-06-27 20:10:49,047 main.py:51] epoch 1369, training loss: 3430.30, average training loss: 3627.67, base loss: 4200.62
[INFO 2017-06-27 20:10:49,423 main.py:51] epoch 1370, training loss: 3255.45, average training loss: 3627.14, base loss: 4200.50
[INFO 2017-06-27 20:10:49,800 main.py:51] epoch 1371, training loss: 3423.86, average training loss: 3627.01, base loss: 4200.80
[INFO 2017-06-27 20:10:50,176 main.py:51] epoch 1372, training loss: 3242.73, average training loss: 3626.67, base loss: 4201.01
[INFO 2017-06-27 20:10:50,552 main.py:51] epoch 1373, training loss: 3016.48, average training loss: 3626.22, base loss: 4200.55
[INFO 2017-06-27 20:10:50,927 main.py:51] epoch 1374, training loss: 3143.66, average training loss: 3625.31, base loss: 4199.99
[INFO 2017-06-27 20:10:51,303 main.py:51] epoch 1375, training loss: 3234.66, average training loss: 3624.75, base loss: 4199.86
[INFO 2017-06-27 20:10:51,685 main.py:51] epoch 1376, training loss: 2731.50, average training loss: 3620.07, base loss: 4195.17
[INFO 2017-06-27 20:10:52,059 main.py:51] epoch 1377, training loss: 3697.32, average training loss: 3620.11, base loss: 4195.74
[INFO 2017-06-27 20:10:52,433 main.py:51] epoch 1378, training loss: 3682.68, average training loss: 3619.82, base loss: 4195.69
[INFO 2017-06-27 20:10:52,808 main.py:51] epoch 1379, training loss: 7391.43, average training loss: 3623.96, base loss: 4200.39
[INFO 2017-06-27 20:10:53,182 main.py:51] epoch 1380, training loss: 3120.41, average training loss: 3623.77, base loss: 4200.58
[INFO 2017-06-27 20:10:53,556 main.py:51] epoch 1381, training loss: 3165.57, average training loss: 3622.59, base loss: 4199.81
[INFO 2017-06-27 20:10:53,941 main.py:51] epoch 1382, training loss: 3559.66, average training loss: 3622.48, base loss: 4200.27
[INFO 2017-06-27 20:10:54,320 main.py:51] epoch 1383, training loss: 3579.75, average training loss: 3621.89, base loss: 4200.14
[INFO 2017-06-27 20:10:54,692 main.py:51] epoch 1384, training loss: 3485.36, average training loss: 3622.19, base loss: 4201.10
[INFO 2017-06-27 20:10:55,065 main.py:51] epoch 1385, training loss: 2969.28, average training loss: 3621.53, base loss: 4200.76
[INFO 2017-06-27 20:10:55,441 main.py:51] epoch 1386, training loss: 3775.22, average training loss: 3621.91, base loss: 4201.83
[INFO 2017-06-27 20:10:55,819 main.py:51] epoch 1387, training loss: 3708.97, average training loss: 3621.66, base loss: 4202.05
[INFO 2017-06-27 20:10:56,198 main.py:51] epoch 1388, training loss: 6185.75, average training loss: 3623.66, base loss: 4204.15
[INFO 2017-06-27 20:10:56,571 main.py:51] epoch 1389, training loss: 3554.01, average training loss: 3622.85, base loss: 4203.73
[INFO 2017-06-27 20:10:56,949 main.py:51] epoch 1390, training loss: 3350.06, average training loss: 3623.05, base loss: 4204.67
[INFO 2017-06-27 20:10:57,326 main.py:51] epoch 1391, training loss: 3058.59, average training loss: 3622.23, base loss: 4204.05
[INFO 2017-06-27 20:10:57,699 main.py:51] epoch 1392, training loss: 3058.47, average training loss: 3621.80, base loss: 4203.92
[INFO 2017-06-27 20:10:58,077 main.py:51] epoch 1393, training loss: 3226.44, average training loss: 3622.03, base loss: 4204.43
[INFO 2017-06-27 20:10:58,450 main.py:51] epoch 1394, training loss: 6648.29, average training loss: 3624.06, base loss: 4206.28
[INFO 2017-06-27 20:10:58,823 main.py:51] epoch 1395, training loss: 3939.41, average training loss: 3624.64, base loss: 4207.41
[INFO 2017-06-27 20:10:59,197 main.py:51] epoch 1396, training loss: 3404.35, average training loss: 3624.46, base loss: 4207.49
[INFO 2017-06-27 20:10:59,572 main.py:51] epoch 1397, training loss: 3367.07, average training loss: 3620.89, base loss: 4204.54
[INFO 2017-06-27 20:10:59,945 main.py:51] epoch 1398, training loss: 2729.59, average training loss: 3619.81, base loss: 4203.51
[INFO 2017-06-27 20:11:00,320 main.py:51] epoch 1399, training loss: 3229.34, average training loss: 3619.20, base loss: 4203.25
[INFO 2017-06-27 20:11:00,320 main.py:53] epoch 1399, testing
[INFO 2017-06-27 20:11:01,930 main.py:105] average testing loss: 3591.65, base loss: 4351.06
[INFO 2017-06-27 20:11:01,931 main.py:106] improve_loss: 759.41, improve_percent: 0.17
[INFO 2017-06-27 20:11:01,931 main.py:76] current best improved percent: 0.18
[INFO 2017-06-27 20:11:02,397 main.py:51] epoch 1400, training loss: 3423.54, average training loss: 3618.57, base loss: 4202.97
[INFO 2017-06-27 20:11:02,780 main.py:51] epoch 1401, training loss: 3548.94, average training loss: 3618.46, base loss: 4203.19
[INFO 2017-06-27 20:11:03,161 main.py:51] epoch 1402, training loss: 3376.91, average training loss: 3618.40, base loss: 4203.77
[INFO 2017-06-27 20:11:03,538 main.py:51] epoch 1403, training loss: 3103.96, average training loss: 3617.56, base loss: 4202.81
[INFO 2017-06-27 20:11:03,949 main.py:51] epoch 1404, training loss: 2998.17, average training loss: 3616.91, base loss: 4202.43
[INFO 2017-06-27 20:11:04,332 main.py:51] epoch 1405, training loss: 3511.53, average training loss: 3616.90, base loss: 4202.78
[INFO 2017-06-27 20:11:04,813 main.py:51] epoch 1406, training loss: 2718.95, average training loss: 3616.29, base loss: 4202.37
[INFO 2017-06-27 20:11:05,203 main.py:51] epoch 1407, training loss: 4089.25, average training loss: 3617.19, base loss: 4203.63
[INFO 2017-06-27 20:11:05,663 main.py:51] epoch 1408, training loss: 3225.35, average training loss: 3616.96, base loss: 4203.74
[INFO 2017-06-27 20:11:06,083 main.py:51] epoch 1409, training loss: 3072.88, average training loss: 3616.16, base loss: 4203.06
[INFO 2017-06-27 20:11:06,575 main.py:51] epoch 1410, training loss: 3164.13, average training loss: 3615.67, base loss: 4202.85
[INFO 2017-06-27 20:11:07,010 main.py:51] epoch 1411, training loss: 3069.06, average training loss: 3614.87, base loss: 4202.14
[INFO 2017-06-27 20:11:07,500 main.py:51] epoch 1412, training loss: 2702.47, average training loss: 3614.21, base loss: 4201.42
[INFO 2017-06-27 20:11:07,903 main.py:51] epoch 1413, training loss: 3218.01, average training loss: 3613.76, base loss: 4201.27
[INFO 2017-06-27 20:11:08,366 main.py:51] epoch 1414, training loss: 3402.90, average training loss: 3613.34, base loss: 4200.92
[INFO 2017-06-27 20:11:08,784 main.py:51] epoch 1415, training loss: 2963.35, average training loss: 3612.73, base loss: 4200.39
[INFO 2017-06-27 20:11:09,187 main.py:51] epoch 1416, training loss: 3481.68, average training loss: 3612.90, base loss: 4201.17
[INFO 2017-06-27 20:11:09,641 main.py:51] epoch 1417, training loss: 3475.65, average training loss: 3613.19, base loss: 4202.05
[INFO 2017-06-27 20:11:10,044 main.py:51] epoch 1418, training loss: 4024.42, average training loss: 3613.15, base loss: 4202.60
[INFO 2017-06-27 20:11:10,518 main.py:51] epoch 1419, training loss: 3887.39, average training loss: 3613.50, base loss: 4203.67
[INFO 2017-06-27 20:11:10,896 main.py:51] epoch 1420, training loss: 3096.61, average training loss: 3612.19, base loss: 4202.33
[INFO 2017-06-27 20:11:11,381 main.py:51] epoch 1421, training loss: 3341.25, average training loss: 3611.93, base loss: 4202.26
[INFO 2017-06-27 20:11:11,776 main.py:51] epoch 1422, training loss: 3719.61, average training loss: 3612.00, base loss: 4202.54
[INFO 2017-06-27 20:11:12,228 main.py:51] epoch 1423, training loss: 3548.18, average training loss: 3612.25, base loss: 4203.37
[INFO 2017-06-27 20:11:12,639 main.py:51] epoch 1424, training loss: 3169.06, average training loss: 3611.94, base loss: 4203.37
[INFO 2017-06-27 20:11:13,040 main.py:51] epoch 1425, training loss: 2900.95, average training loss: 3611.11, base loss: 4202.42
[INFO 2017-06-27 20:11:13,484 main.py:51] epoch 1426, training loss: 3715.82, average training loss: 3611.25, base loss: 4203.13
[INFO 2017-06-27 20:11:13,863 main.py:51] epoch 1427, training loss: 3295.03, average training loss: 3610.95, base loss: 4203.46
[INFO 2017-06-27 20:11:14,278 main.py:51] epoch 1428, training loss: 3339.94, average training loss: 3611.29, base loss: 4204.20
[INFO 2017-06-27 20:11:14,708 main.py:51] epoch 1429, training loss: 3561.59, average training loss: 3611.32, base loss: 4204.56
[INFO 2017-06-27 20:11:15,089 main.py:51] epoch 1430, training loss: 3366.93, average training loss: 3611.73, base loss: 4205.62
[INFO 2017-06-27 20:11:15,552 main.py:51] epoch 1431, training loss: 3185.87, average training loss: 3611.34, base loss: 4205.57
[INFO 2017-06-27 20:11:15,975 main.py:51] epoch 1432, training loss: 4301.37, average training loss: 3612.19, base loss: 4207.44
[INFO 2017-06-27 20:11:16,356 main.py:51] epoch 1433, training loss: 3746.23, average training loss: 3611.78, base loss: 4207.73
[INFO 2017-06-27 20:11:16,755 main.py:51] epoch 1434, training loss: 3569.23, average training loss: 3611.31, base loss: 4207.52
[INFO 2017-06-27 20:11:17,153 main.py:51] epoch 1435, training loss: 3812.32, average training loss: 3611.48, base loss: 4208.35
[INFO 2017-06-27 20:11:17,528 main.py:51] epoch 1436, training loss: 3256.30, average training loss: 3611.24, base loss: 4208.54
[INFO 2017-06-27 20:11:17,900 main.py:51] epoch 1437, training loss: 3371.78, average training loss: 3611.06, base loss: 4208.63
[INFO 2017-06-27 20:11:18,268 main.py:51] epoch 1438, training loss: 3300.57, average training loss: 3606.21, base loss: 4203.68
[INFO 2017-06-27 20:11:18,642 main.py:51] epoch 1439, training loss: 3331.99, average training loss: 3605.30, base loss: 4202.93
[INFO 2017-06-27 20:11:19,020 main.py:51] epoch 1440, training loss: 3353.73, average training loss: 3601.61, base loss: 4199.83
[INFO 2017-06-27 20:11:19,393 main.py:51] epoch 1441, training loss: 3869.61, average training loss: 3601.59, base loss: 4200.44
[INFO 2017-06-27 20:11:19,764 main.py:51] epoch 1442, training loss: 3278.52, average training loss: 3601.53, base loss: 4200.62
[INFO 2017-06-27 20:11:20,138 main.py:51] epoch 1443, training loss: 3310.11, average training loss: 3601.51, base loss: 4201.00
[INFO 2017-06-27 20:11:20,516 main.py:51] epoch 1444, training loss: 2847.88, average training loss: 3600.56, base loss: 4199.92
[INFO 2017-06-27 20:11:20,892 main.py:51] epoch 1445, training loss: 3307.80, average training loss: 3600.76, base loss: 4200.50
[INFO 2017-06-27 20:11:21,265 main.py:51] epoch 1446, training loss: 3457.06, average training loss: 3599.79, base loss: 4199.91
[INFO 2017-06-27 20:11:21,635 main.py:51] epoch 1447, training loss: 3233.60, average training loss: 3599.51, base loss: 4199.78
[INFO 2017-06-27 20:11:22,011 main.py:51] epoch 1448, training loss: 2915.49, average training loss: 3598.71, base loss: 4199.03
[INFO 2017-06-27 20:11:22,385 main.py:51] epoch 1449, training loss: 5695.83, average training loss: 3601.11, base loss: 4201.54
[INFO 2017-06-27 20:11:22,758 main.py:51] epoch 1450, training loss: 3347.05, average training loss: 3600.81, base loss: 4201.85
[INFO 2017-06-27 20:11:23,131 main.py:51] epoch 1451, training loss: 3771.95, average training loss: 3601.02, base loss: 4202.41
[INFO 2017-06-27 20:11:23,503 main.py:51] epoch 1452, training loss: 3012.43, average training loss: 3600.33, base loss: 4201.76
[INFO 2017-06-27 20:11:23,876 main.py:51] epoch 1453, training loss: 3295.60, average training loss: 3599.62, base loss: 4201.27
[INFO 2017-06-27 20:11:24,246 main.py:51] epoch 1454, training loss: 3311.85, average training loss: 3599.20, base loss: 4201.08
[INFO 2017-06-27 20:11:24,621 main.py:51] epoch 1455, training loss: 3189.64, average training loss: 3598.45, base loss: 4200.47
[INFO 2017-06-27 20:11:24,991 main.py:51] epoch 1456, training loss: 3203.11, average training loss: 3597.76, base loss: 4199.88
[INFO 2017-06-27 20:11:25,363 main.py:51] epoch 1457, training loss: 3122.96, average training loss: 3597.79, base loss: 4200.53
[INFO 2017-06-27 20:11:25,734 main.py:51] epoch 1458, training loss: 3319.88, average training loss: 3597.98, base loss: 4201.29
[INFO 2017-06-27 20:11:26,107 main.py:51] epoch 1459, training loss: 2840.52, average training loss: 3597.11, base loss: 4200.68
[INFO 2017-06-27 20:11:26,482 main.py:51] epoch 1460, training loss: 3249.28, average training loss: 3596.68, base loss: 4200.60
[INFO 2017-06-27 20:11:26,853 main.py:51] epoch 1461, training loss: 3394.16, average training loss: 3596.93, base loss: 4201.50
[INFO 2017-06-27 20:11:27,226 main.py:51] epoch 1462, training loss: 3017.50, average training loss: 3597.09, base loss: 4202.00
[INFO 2017-06-27 20:11:27,595 main.py:51] epoch 1463, training loss: 3676.73, average training loss: 3597.08, base loss: 4202.66
[INFO 2017-06-27 20:11:27,974 main.py:51] epoch 1464, training loss: 3415.56, average training loss: 3596.38, base loss: 4202.27
[INFO 2017-06-27 20:11:28,345 main.py:51] epoch 1465, training loss: 3438.14, average training loss: 3596.76, base loss: 4203.29
[INFO 2017-06-27 20:11:28,718 main.py:51] epoch 1466, training loss: 3275.04, average training loss: 3595.83, base loss: 4202.33
[INFO 2017-06-27 20:11:29,091 main.py:51] epoch 1467, training loss: 3125.46, average training loss: 3595.55, base loss: 4202.67
[INFO 2017-06-27 20:11:29,472 main.py:51] epoch 1468, training loss: 3093.53, average training loss: 3594.88, base loss: 4202.16
[INFO 2017-06-27 20:11:29,847 main.py:51] epoch 1469, training loss: 3683.88, average training loss: 3595.30, base loss: 4203.03
[INFO 2017-06-27 20:11:30,223 main.py:51] epoch 1470, training loss: 3124.26, average training loss: 3594.57, base loss: 4202.84
[INFO 2017-06-27 20:11:30,602 main.py:51] epoch 1471, training loss: 3099.89, average training loss: 3593.74, base loss: 4202.07
[INFO 2017-06-27 20:11:30,974 main.py:51] epoch 1472, training loss: 2971.15, average training loss: 3592.08, base loss: 4200.50
[INFO 2017-06-27 20:11:31,370 main.py:51] epoch 1473, training loss: 3276.81, average training loss: 3591.13, base loss: 4199.67
[INFO 2017-06-27 20:11:31,753 main.py:51] epoch 1474, training loss: 3455.49, average training loss: 3591.19, base loss: 4200.24
[INFO 2017-06-27 20:11:32,129 main.py:51] epoch 1475, training loss: 3026.26, average training loss: 3590.51, base loss: 4199.89
[INFO 2017-06-27 20:11:32,504 main.py:51] epoch 1476, training loss: 7537.29, average training loss: 3594.70, base loss: 4204.77
[INFO 2017-06-27 20:11:32,878 main.py:51] epoch 1477, training loss: 3204.75, average training loss: 3594.00, base loss: 4204.09
[INFO 2017-06-27 20:11:33,256 main.py:51] epoch 1478, training loss: 3206.64, average training loss: 3593.29, base loss: 4203.58
[INFO 2017-06-27 20:11:33,631 main.py:51] epoch 1479, training loss: 3206.61, average training loss: 3592.92, base loss: 4203.09
[INFO 2017-06-27 20:11:34,007 main.py:51] epoch 1480, training loss: 3438.78, average training loss: 3592.47, base loss: 4203.00
[INFO 2017-06-27 20:11:34,381 main.py:51] epoch 1481, training loss: 2988.04, average training loss: 3591.16, base loss: 4201.58
[INFO 2017-06-27 20:11:34,751 main.py:51] epoch 1482, training loss: 3516.06, average training loss: 3590.37, base loss: 4200.98
[INFO 2017-06-27 20:11:35,127 main.py:51] epoch 1483, training loss: 3101.56, average training loss: 3590.03, base loss: 4200.97
[INFO 2017-06-27 20:11:35,502 main.py:51] epoch 1484, training loss: 3212.26, average training loss: 3589.46, base loss: 4200.71
[INFO 2017-06-27 20:11:35,877 main.py:51] epoch 1485, training loss: 3420.66, average training loss: 3589.33, base loss: 4200.87
[INFO 2017-06-27 20:11:36,249 main.py:51] epoch 1486, training loss: 3732.38, average training loss: 3589.21, base loss: 4201.17
[INFO 2017-06-27 20:11:36,628 main.py:51] epoch 1487, training loss: 3308.91, average training loss: 3588.80, base loss: 4200.86
[INFO 2017-06-27 20:11:37,007 main.py:51] epoch 1488, training loss: 3215.27, average training loss: 3587.98, base loss: 4199.92
[INFO 2017-06-27 20:11:37,382 main.py:51] epoch 1489, training loss: 3680.73, average training loss: 3588.31, base loss: 4200.65
[INFO 2017-06-27 20:11:37,761 main.py:51] epoch 1490, training loss: 2610.46, average training loss: 3587.42, base loss: 4199.69
[INFO 2017-06-27 20:11:38,132 main.py:51] epoch 1491, training loss: 3586.68, average training loss: 3587.28, base loss: 4199.66
[INFO 2017-06-27 20:11:38,506 main.py:51] epoch 1492, training loss: 2847.98, average training loss: 3586.94, base loss: 4199.40
[INFO 2017-06-27 20:11:38,878 main.py:51] epoch 1493, training loss: 3087.81, average training loss: 3586.20, base loss: 4198.70
[INFO 2017-06-27 20:11:39,262 main.py:51] epoch 1494, training loss: 3261.66, average training loss: 3585.59, base loss: 4198.34
[INFO 2017-06-27 20:11:39,645 main.py:51] epoch 1495, training loss: 3324.21, average training loss: 3585.29, base loss: 4198.51
[INFO 2017-06-27 20:11:40,017 main.py:51] epoch 1496, training loss: 3329.25, average training loss: 3585.32, base loss: 4198.92
[INFO 2017-06-27 20:11:40,391 main.py:51] epoch 1497, training loss: 3483.16, average training loss: 3585.44, base loss: 4199.31
[INFO 2017-06-27 20:11:40,859 main.py:51] epoch 1498, training loss: 3322.37, average training loss: 3584.68, base loss: 4198.79
[INFO 2017-06-27 20:11:41,247 main.py:51] epoch 1499, training loss: 3297.89, average training loss: 3583.91, base loss: 4198.20
[INFO 2017-06-27 20:11:41,248 main.py:53] epoch 1499, testing
[INFO 2017-06-27 20:11:42,869 main.py:105] average testing loss: 3672.06, base loss: 4382.01
[INFO 2017-06-27 20:11:42,869 main.py:106] improve_loss: 709.95, improve_percent: 0.16
[INFO 2017-06-27 20:11:42,870 main.py:76] current best improved percent: 0.18
[INFO 2017-06-27 20:11:43,244 main.py:51] epoch 1500, training loss: 3226.45, average training loss: 3583.78, base loss: 4198.67
[INFO 2017-06-27 20:11:43,625 main.py:51] epoch 1501, training loss: 3716.10, average training loss: 3583.66, base loss: 4198.98
[INFO 2017-06-27 20:11:44,002 main.py:51] epoch 1502, training loss: 3277.78, average training loss: 3583.82, base loss: 4199.54
[INFO 2017-06-27 20:11:44,376 main.py:51] epoch 1503, training loss: 3569.06, average training loss: 3583.65, base loss: 4199.64
[INFO 2017-06-27 20:11:44,746 main.py:51] epoch 1504, training loss: 3080.81, average training loss: 3583.45, base loss: 4199.68
[INFO 2017-06-27 20:11:45,119 main.py:51] epoch 1505, training loss: 3450.38, average training loss: 3583.01, base loss: 4199.64
[INFO 2017-06-27 20:11:45,493 main.py:51] epoch 1506, training loss: 2830.83, average training loss: 3581.70, base loss: 4198.20
[INFO 2017-06-27 20:11:45,863 main.py:51] epoch 1507, training loss: 3377.02, average training loss: 3580.74, base loss: 4197.47
[INFO 2017-06-27 20:11:46,235 main.py:51] epoch 1508, training loss: 3262.38, average training loss: 3580.37, base loss: 4197.19
[INFO 2017-06-27 20:11:46,604 main.py:51] epoch 1509, training loss: 3396.15, average training loss: 3580.31, base loss: 4197.49
[INFO 2017-06-27 20:11:46,975 main.py:51] epoch 1510, training loss: 3421.74, average training loss: 3580.24, base loss: 4197.57
[INFO 2017-06-27 20:11:47,348 main.py:51] epoch 1511, training loss: 3395.71, average training loss: 3580.12, base loss: 4197.94
[INFO 2017-06-27 20:11:47,717 main.py:51] epoch 1512, training loss: 3168.21, average training loss: 3579.20, base loss: 4197.42
[INFO 2017-06-27 20:11:48,089 main.py:51] epoch 1513, training loss: 3703.68, average training loss: 3579.52, base loss: 4198.30
[INFO 2017-06-27 20:11:48,463 main.py:51] epoch 1514, training loss: 3687.68, average training loss: 3579.94, base loss: 4199.26
[INFO 2017-06-27 20:11:48,838 main.py:51] epoch 1515, training loss: 3086.67, average training loss: 3578.83, base loss: 4198.53
[INFO 2017-06-27 20:11:49,216 main.py:51] epoch 1516, training loss: 3844.61, average training loss: 3579.89, base loss: 4200.69
[INFO 2017-06-27 20:11:49,585 main.py:51] epoch 1517, training loss: 2692.66, average training loss: 3579.28, base loss: 4200.34
[INFO 2017-06-27 20:11:49,964 main.py:51] epoch 1518, training loss: 3471.76, average training loss: 3579.14, base loss: 4200.68
[INFO 2017-06-27 20:11:50,337 main.py:51] epoch 1519, training loss: 3189.49, average training loss: 3578.43, base loss: 4200.31
[INFO 2017-06-27 20:11:50,711 main.py:51] epoch 1520, training loss: 3278.60, average training loss: 3577.89, base loss: 4200.24
[INFO 2017-06-27 20:11:51,085 main.py:51] epoch 1521, training loss: 3067.70, average training loss: 3577.34, base loss: 4199.99
[INFO 2017-06-27 20:11:51,461 main.py:51] epoch 1522, training loss: 3472.21, average training loss: 3577.02, base loss: 4200.18
[INFO 2017-06-27 20:11:51,835 main.py:51] epoch 1523, training loss: 3802.42, average training loss: 3577.38, base loss: 4200.86
[INFO 2017-06-27 20:11:52,207 main.py:51] epoch 1524, training loss: 3624.90, average training loss: 3577.06, base loss: 4200.86
[INFO 2017-06-27 20:11:52,582 main.py:51] epoch 1525, training loss: 3815.57, average training loss: 3576.48, base loss: 4200.83
[INFO 2017-06-27 20:11:52,953 main.py:51] epoch 1526, training loss: 2919.09, average training loss: 3576.14, base loss: 4200.69
[INFO 2017-06-27 20:11:53,322 main.py:51] epoch 1527, training loss: 2831.21, average training loss: 3575.14, base loss: 4199.75
[INFO 2017-06-27 20:11:53,703 main.py:51] epoch 1528, training loss: 3271.79, average training loss: 3575.42, base loss: 4200.80
[INFO 2017-06-27 20:11:54,077 main.py:51] epoch 1529, training loss: 3314.15, average training loss: 3575.45, base loss: 4201.36
[INFO 2017-06-27 20:11:54,446 main.py:51] epoch 1530, training loss: 2674.81, average training loss: 3574.67, base loss: 4200.54
[INFO 2017-06-27 20:11:54,815 main.py:51] epoch 1531, training loss: 3501.23, average training loss: 3574.09, base loss: 4200.33
[INFO 2017-06-27 20:11:55,185 main.py:51] epoch 1532, training loss: 3179.26, average training loss: 3573.55, base loss: 4200.03
[INFO 2017-06-27 20:11:55,559 main.py:51] epoch 1533, training loss: 3031.84, average training loss: 3573.22, base loss: 4200.18
[INFO 2017-06-27 20:11:55,943 main.py:51] epoch 1534, training loss: 3524.48, average training loss: 3572.93, base loss: 4200.27
[INFO 2017-06-27 20:11:56,317 main.py:51] epoch 1535, training loss: 3716.92, average training loss: 3572.55, base loss: 4200.10
[INFO 2017-06-27 20:11:56,690 main.py:51] epoch 1536, training loss: 3109.67, average training loss: 3572.06, base loss: 4199.37
[INFO 2017-06-27 20:11:57,064 main.py:51] epoch 1537, training loss: 3472.65, average training loss: 3571.85, base loss: 4199.27
[INFO 2017-06-27 20:11:57,434 main.py:51] epoch 1538, training loss: 3814.01, average training loss: 3571.87, base loss: 4199.82
[INFO 2017-06-27 20:11:57,806 main.py:51] epoch 1539, training loss: 2867.56, average training loss: 3571.38, base loss: 4199.68
[INFO 2017-06-27 20:11:58,178 main.py:51] epoch 1540, training loss: 3464.79, average training loss: 3571.45, base loss: 4199.93
[INFO 2017-06-27 20:11:58,549 main.py:51] epoch 1541, training loss: 3909.90, average training loss: 3571.92, base loss: 4201.02
[INFO 2017-06-27 20:11:58,919 main.py:51] epoch 1542, training loss: 3206.30, average training loss: 3571.70, base loss: 4201.06
[INFO 2017-06-27 20:11:59,291 main.py:51] epoch 1543, training loss: 3188.60, average training loss: 3571.15, base loss: 4200.62
[INFO 2017-06-27 20:11:59,665 main.py:51] epoch 1544, training loss: 3301.34, average training loss: 3570.54, base loss: 4200.18
[INFO 2017-06-27 20:12:00,034 main.py:51] epoch 1545, training loss: 3800.52, average training loss: 3570.70, base loss: 4201.38
[INFO 2017-06-27 20:12:00,416 main.py:51] epoch 1546, training loss: 2862.16, average training loss: 3569.97, base loss: 4201.05
[INFO 2017-06-27 20:12:00,795 main.py:51] epoch 1547, training loss: 3760.34, average training loss: 3570.04, base loss: 4201.63
[INFO 2017-06-27 20:12:01,170 main.py:51] epoch 1548, training loss: 2995.88, average training loss: 3569.93, base loss: 4201.85
[INFO 2017-06-27 20:12:01,545 main.py:51] epoch 1549, training loss: 3955.72, average training loss: 3570.28, base loss: 4203.26
[INFO 2017-06-27 20:12:01,915 main.py:51] epoch 1550, training loss: 3088.24, average training loss: 3569.68, base loss: 4202.70
[INFO 2017-06-27 20:12:02,290 main.py:51] epoch 1551, training loss: 3980.23, average training loss: 3570.83, base loss: 4204.67
[INFO 2017-06-27 20:12:02,660 main.py:51] epoch 1552, training loss: 3186.14, average training loss: 3570.45, base loss: 4204.40
[INFO 2017-06-27 20:12:03,030 main.py:51] epoch 1553, training loss: 3029.20, average training loss: 3570.62, base loss: 4205.01
[INFO 2017-06-27 20:12:03,407 main.py:51] epoch 1554, training loss: 3390.75, average training loss: 3569.94, base loss: 4204.59
[INFO 2017-06-27 20:12:03,782 main.py:51] epoch 1555, training loss: 3425.83, average training loss: 3570.09, base loss: 4205.24
[INFO 2017-06-27 20:12:04,157 main.py:51] epoch 1556, training loss: 3259.49, average training loss: 3569.17, base loss: 4204.27
[INFO 2017-06-27 20:12:04,530 main.py:51] epoch 1557, training loss: 3009.73, average training loss: 3568.63, base loss: 4203.79
[INFO 2017-06-27 20:12:04,911 main.py:51] epoch 1558, training loss: 2947.07, average training loss: 3567.95, base loss: 4203.32
[INFO 2017-06-27 20:12:05,287 main.py:51] epoch 1559, training loss: 3296.67, average training loss: 3568.01, base loss: 4203.82
[INFO 2017-06-27 20:12:05,662 main.py:51] epoch 1560, training loss: 3405.29, average training loss: 3567.63, base loss: 4203.75
[INFO 2017-06-27 20:12:06,037 main.py:51] epoch 1561, training loss: 3041.63, average training loss: 3567.74, base loss: 4204.23
[INFO 2017-06-27 20:12:06,501 main.py:51] epoch 1562, training loss: 2967.28, average training loss: 3567.45, base loss: 4204.26
[INFO 2017-06-27 20:12:06,886 main.py:51] epoch 1563, training loss: 3558.75, average training loss: 3566.47, base loss: 4203.52
[INFO 2017-06-27 20:12:07,260 main.py:51] epoch 1564, training loss: 3728.03, average training loss: 3567.06, base loss: 4204.92
[INFO 2017-06-27 20:12:07,640 main.py:51] epoch 1565, training loss: 3484.75, average training loss: 3567.37, base loss: 4205.91
[INFO 2017-06-27 20:12:08,015 main.py:51] epoch 1566, training loss: 3350.15, average training loss: 3566.77, base loss: 4205.63
[INFO 2017-06-27 20:12:08,404 main.py:51] epoch 1567, training loss: 3054.06, average training loss: 3566.04, base loss: 4204.87
[INFO 2017-06-27 20:12:08,780 main.py:51] epoch 1568, training loss: 2901.95, average training loss: 3565.76, base loss: 4204.90
[INFO 2017-06-27 20:12:09,157 main.py:51] epoch 1569, training loss: 2790.11, average training loss: 3564.99, base loss: 4204.19
[INFO 2017-06-27 20:12:09,533 main.py:51] epoch 1570, training loss: 3018.09, average training loss: 3564.43, base loss: 4203.64
[INFO 2017-06-27 20:12:09,902 main.py:51] epoch 1571, training loss: 2822.25, average training loss: 3563.54, base loss: 4202.82
[INFO 2017-06-27 20:12:10,273 main.py:51] epoch 1572, training loss: 3782.95, average training loss: 3563.45, base loss: 4203.25
[INFO 2017-06-27 20:12:10,648 main.py:51] epoch 1573, training loss: 3358.06, average training loss: 3563.22, base loss: 4203.26
[INFO 2017-06-27 20:12:11,020 main.py:51] epoch 1574, training loss: 3147.46, average training loss: 3562.30, base loss: 4202.06
[INFO 2017-06-27 20:12:11,391 main.py:51] epoch 1575, training loss: 3745.15, average training loss: 3562.08, base loss: 4202.06
[INFO 2017-06-27 20:12:11,765 main.py:51] epoch 1576, training loss: 3200.79, average training loss: 3562.18, base loss: 4202.43
[INFO 2017-06-27 20:12:12,136 main.py:51] epoch 1577, training loss: 2758.12, average training loss: 3561.81, base loss: 4202.14
[INFO 2017-06-27 20:12:12,511 main.py:51] epoch 1578, training loss: 3120.19, average training loss: 3561.55, base loss: 4202.09
[INFO 2017-06-27 20:12:12,979 main.py:51] epoch 1579, training loss: 3272.39, average training loss: 3561.37, base loss: 4202.21
[INFO 2017-06-27 20:12:13,362 main.py:51] epoch 1580, training loss: 3094.29, average training loss: 3560.56, base loss: 4201.27
[INFO 2017-06-27 20:12:13,801 main.py:51] epoch 1581, training loss: 3602.76, average training loss: 3560.93, base loss: 4202.19
[INFO 2017-06-27 20:12:14,202 main.py:51] epoch 1582, training loss: 2864.53, average training loss: 3560.52, base loss: 4202.04
[INFO 2017-06-27 20:12:14,670 main.py:51] epoch 1583, training loss: 3394.15, average training loss: 3560.42, base loss: 4202.19
[INFO 2017-06-27 20:12:15,068 main.py:51] epoch 1584, training loss: 3394.01, average training loss: 3559.75, base loss: 4201.81
[INFO 2017-06-27 20:12:15,449 main.py:51] epoch 1585, training loss: 3486.57, average training loss: 3559.37, base loss: 4201.35
[INFO 2017-06-27 20:12:15,927 main.py:51] epoch 1586, training loss: 3494.88, average training loss: 3558.68, base loss: 4200.72
[INFO 2017-06-27 20:12:16,316 main.py:51] epoch 1587, training loss: 3481.83, average training loss: 3558.59, base loss: 4201.05
[INFO 2017-06-27 20:12:16,775 main.py:51] epoch 1588, training loss: 3013.99, average training loss: 3558.22, base loss: 4200.74
[INFO 2017-06-27 20:12:17,205 main.py:51] epoch 1589, training loss: 3040.19, average training loss: 3558.05, base loss: 4200.83
[INFO 2017-06-27 20:12:17,585 main.py:51] epoch 1590, training loss: 3370.03, average training loss: 3558.14, base loss: 4201.07
[INFO 2017-06-27 20:12:17,967 main.py:51] epoch 1591, training loss: 3212.72, average training loss: 3558.18, base loss: 4201.39
[INFO 2017-06-27 20:12:18,446 main.py:51] epoch 1592, training loss: 2873.12, average training loss: 3557.82, base loss: 4201.64
[INFO 2017-06-27 20:12:18,843 main.py:51] epoch 1593, training loss: 3629.48, average training loss: 3558.19, base loss: 4202.54
[INFO 2017-06-27 20:12:19,280 main.py:51] epoch 1594, training loss: 3744.13, average training loss: 3558.29, base loss: 4203.24
[INFO 2017-06-27 20:12:19,695 main.py:51] epoch 1595, training loss: 3094.49, average training loss: 3558.27, base loss: 4203.40
[INFO 2017-06-27 20:12:20,129 main.py:51] epoch 1596, training loss: 3893.39, average training loss: 3558.85, base loss: 4204.38
[INFO 2017-06-27 20:12:20,552 main.py:51] epoch 1597, training loss: 6697.79, average training loss: 3561.47, base loss: 4207.36
[INFO 2017-06-27 20:12:20,982 main.py:51] epoch 1598, training loss: 2993.25, average training loss: 3560.17, base loss: 4205.87
[INFO 2017-06-27 20:12:21,416 main.py:51] epoch 1599, training loss: 3292.11, average training loss: 3559.86, base loss: 4206.02
[INFO 2017-06-27 20:12:21,417 main.py:53] epoch 1599, testing
[INFO 2017-06-27 20:12:23,355 main.py:105] average testing loss: 3349.57, base loss: 4123.10
[INFO 2017-06-27 20:12:23,355 main.py:106] improve_loss: 773.53, improve_percent: 0.19
[INFO 2017-06-27 20:12:23,355 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:12:23,369 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 20:12:23,786 main.py:51] epoch 1600, training loss: 3484.67, average training loss: 3559.76, base loss: 4206.22
[INFO 2017-06-27 20:12:24,171 main.py:51] epoch 1601, training loss: 4617.67, average training loss: 3560.76, base loss: 4208.32
[INFO 2017-06-27 20:12:24,569 main.py:51] epoch 1602, training loss: 3642.67, average training loss: 3561.18, base loss: 4209.30
[INFO 2017-06-27 20:12:24,948 main.py:51] epoch 1603, training loss: 3388.52, average training loss: 3560.87, base loss: 4209.14
[INFO 2017-06-27 20:12:25,325 main.py:51] epoch 1604, training loss: 3122.38, average training loss: 3560.32, base loss: 4208.77
[INFO 2017-06-27 20:12:25,702 main.py:51] epoch 1605, training loss: 3250.75, average training loss: 3560.34, base loss: 4209.19
[INFO 2017-06-27 20:12:26,081 main.py:51] epoch 1606, training loss: 2988.76, average training loss: 3559.21, base loss: 4208.37
[INFO 2017-06-27 20:12:26,464 main.py:51] epoch 1607, training loss: 3572.77, average training loss: 3559.01, base loss: 4208.68
[INFO 2017-06-27 20:12:26,841 main.py:51] epoch 1608, training loss: 3691.84, average training loss: 3558.56, base loss: 4208.71
[INFO 2017-06-27 20:12:27,217 main.py:51] epoch 1609, training loss: 3118.24, average training loss: 3558.22, base loss: 4208.74
[INFO 2017-06-27 20:12:27,595 main.py:51] epoch 1610, training loss: 3824.00, average training loss: 3558.27, base loss: 4209.35
[INFO 2017-06-27 20:12:27,975 main.py:51] epoch 1611, training loss: 3167.99, average training loss: 3557.98, base loss: 4209.31
[INFO 2017-06-27 20:12:28,360 main.py:51] epoch 1612, training loss: 3233.08, average training loss: 3557.75, base loss: 4209.44
[INFO 2017-06-27 20:12:28,763 main.py:51] epoch 1613, training loss: 3354.26, average training loss: 3557.58, base loss: 4209.75
[INFO 2017-06-27 20:12:29,198 main.py:51] epoch 1614, training loss: 3679.93, average training loss: 3554.00, base loss: 4206.70
[INFO 2017-06-27 20:12:29,587 main.py:51] epoch 1615, training loss: 3909.98, average training loss: 3553.99, base loss: 4207.15
[INFO 2017-06-27 20:12:29,977 main.py:51] epoch 1616, training loss: 3673.73, average training loss: 3554.12, base loss: 4208.00
[INFO 2017-06-27 20:12:30,362 main.py:51] epoch 1617, training loss: 3735.98, average training loss: 3554.31, base loss: 4208.73
[INFO 2017-06-27 20:12:30,752 main.py:51] epoch 1618, training loss: 3205.01, average training loss: 3553.15, base loss: 4207.26
[INFO 2017-06-27 20:12:31,242 main.py:51] epoch 1619, training loss: 3491.75, average training loss: 3552.67, base loss: 4206.87
[INFO 2017-06-27 20:12:31,680 main.py:51] epoch 1620, training loss: 3533.98, average training loss: 3551.75, base loss: 4205.76
[INFO 2017-06-27 20:12:32,132 main.py:51] epoch 1621, training loss: 2803.64, average training loss: 3550.77, base loss: 4204.89
[INFO 2017-06-27 20:12:32,570 main.py:51] epoch 1622, training loss: 6717.95, average training loss: 3553.13, base loss: 4207.52
[INFO 2017-06-27 20:12:32,989 main.py:51] epoch 1623, training loss: 3149.73, average training loss: 3552.72, base loss: 4207.47
[INFO 2017-06-27 20:12:33,389 main.py:51] epoch 1624, training loss: 4291.65, average training loss: 3553.24, base loss: 4208.83
[INFO 2017-06-27 20:12:33,847 main.py:51] epoch 1625, training loss: 3381.55, average training loss: 3552.37, base loss: 4207.92
[INFO 2017-06-27 20:12:34,309 main.py:51] epoch 1626, training loss: 3435.45, average training loss: 3552.33, base loss: 4208.23
[INFO 2017-06-27 20:12:34,699 main.py:51] epoch 1627, training loss: 3352.90, average training loss: 3552.32, base loss: 4208.61
[INFO 2017-06-27 20:12:35,081 main.py:51] epoch 1628, training loss: 3147.82, average training loss: 3552.42, base loss: 4208.97
[INFO 2017-06-27 20:12:35,459 main.py:51] epoch 1629, training loss: 3830.78, average training loss: 3552.96, base loss: 4210.16
[INFO 2017-06-27 20:12:35,842 main.py:51] epoch 1630, training loss: 7347.11, average training loss: 3556.67, base loss: 4214.59
[INFO 2017-06-27 20:12:36,222 main.py:51] epoch 1631, training loss: 3380.91, average training loss: 3556.62, base loss: 4214.77
[INFO 2017-06-27 20:12:36,597 main.py:51] epoch 1632, training loss: 3724.36, average training loss: 3557.18, base loss: 4215.99
[INFO 2017-06-27 20:12:36,974 main.py:51] epoch 1633, training loss: 3369.21, average training loss: 3556.51, base loss: 4215.57
[INFO 2017-06-27 20:12:37,367 main.py:51] epoch 1634, training loss: 3230.58, average training loss: 3556.38, base loss: 4215.93
[INFO 2017-06-27 20:12:37,747 main.py:51] epoch 1635, training loss: 3523.93, average training loss: 3556.18, base loss: 4215.90
[INFO 2017-06-27 20:12:38,128 main.py:51] epoch 1636, training loss: 2724.78, average training loss: 3555.44, base loss: 4215.08
[INFO 2017-06-27 20:12:38,502 main.py:51] epoch 1637, training loss: 3350.12, average training loss: 3555.33, base loss: 4215.50
[INFO 2017-06-27 20:12:38,874 main.py:51] epoch 1638, training loss: 4200.53, average training loss: 3555.90, base loss: 4216.63
[INFO 2017-06-27 20:12:39,251 main.py:51] epoch 1639, training loss: 7090.46, average training loss: 3559.49, base loss: 4220.60
[INFO 2017-06-27 20:12:39,630 main.py:51] epoch 1640, training loss: 3273.87, average training loss: 3555.13, base loss: 4216.11
[INFO 2017-06-27 20:12:40,007 main.py:51] epoch 1641, training loss: 3182.56, average training loss: 3553.89, base loss: 4214.63
[INFO 2017-06-27 20:12:40,382 main.py:51] epoch 1642, training loss: 3509.32, average training loss: 3553.58, base loss: 4214.54
[INFO 2017-06-27 20:12:40,753 main.py:51] epoch 1643, training loss: 3514.13, average training loss: 3553.80, base loss: 4215.29
[INFO 2017-06-27 20:12:41,126 main.py:51] epoch 1644, training loss: 3081.38, average training loss: 3553.75, base loss: 4215.71
[INFO 2017-06-27 20:12:41,513 main.py:51] epoch 1645, training loss: 3128.27, average training loss: 3553.73, base loss: 4216.25
[INFO 2017-06-27 20:12:41,890 main.py:51] epoch 1646, training loss: 2909.26, average training loss: 3553.51, base loss: 4216.33
[INFO 2017-06-27 20:12:42,267 main.py:51] epoch 1647, training loss: 3277.38, average training loss: 3553.30, base loss: 4216.40
[INFO 2017-06-27 20:12:42,643 main.py:51] epoch 1648, training loss: 3222.45, average training loss: 3553.18, base loss: 4216.78
[INFO 2017-06-27 20:12:43,019 main.py:51] epoch 1649, training loss: 3765.16, average training loss: 3552.86, base loss: 4216.84
[INFO 2017-06-27 20:12:43,393 main.py:51] epoch 1650, training loss: 3037.49, average training loss: 3552.64, base loss: 4216.90
[INFO 2017-06-27 20:12:43,768 main.py:51] epoch 1651, training loss: 3945.13, average training loss: 3552.66, base loss: 4217.62
[INFO 2017-06-27 20:12:44,144 main.py:51] epoch 1652, training loss: 3495.85, average training loss: 3552.15, base loss: 4217.39
[INFO 2017-06-27 20:12:44,520 main.py:51] epoch 1653, training loss: 4121.29, average training loss: 3552.82, base loss: 4218.98
[INFO 2017-06-27 20:12:44,895 main.py:51] epoch 1654, training loss: 3194.99, average training loss: 3553.08, base loss: 4219.54
[INFO 2017-06-27 20:12:45,271 main.py:51] epoch 1655, training loss: 3279.31, average training loss: 3553.24, base loss: 4220.05
[INFO 2017-06-27 20:12:45,655 main.py:51] epoch 1656, training loss: 3427.75, average training loss: 3553.13, base loss: 4220.29
[INFO 2017-06-27 20:12:46,031 main.py:51] epoch 1657, training loss: 3460.66, average training loss: 3553.06, base loss: 4220.88
[INFO 2017-06-27 20:12:46,406 main.py:51] epoch 1658, training loss: 3355.05, average training loss: 3552.80, base loss: 4220.65
[INFO 2017-06-27 20:12:46,781 main.py:51] epoch 1659, training loss: 3147.04, average training loss: 3551.46, base loss: 4219.13
[INFO 2017-06-27 20:12:47,161 main.py:51] epoch 1660, training loss: 2963.10, average training loss: 3551.35, base loss: 4219.25
[INFO 2017-06-27 20:12:47,534 main.py:51] epoch 1661, training loss: 3098.25, average training loss: 3550.23, base loss: 4218.22
[INFO 2017-06-27 20:12:47,910 main.py:51] epoch 1662, training loss: 3113.74, average training loss: 3550.43, base loss: 4218.69
[INFO 2017-06-27 20:12:48,286 main.py:51] epoch 1663, training loss: 3342.65, average training loss: 3549.66, base loss: 4218.14
[INFO 2017-06-27 20:12:48,663 main.py:51] epoch 1664, training loss: 3607.51, average training loss: 3549.93, base loss: 4218.94
[INFO 2017-06-27 20:12:49,040 main.py:51] epoch 1665, training loss: 3104.00, average training loss: 3549.46, base loss: 4218.49
[INFO 2017-06-27 20:12:49,496 main.py:51] epoch 1666, training loss: 3421.13, average training loss: 3549.73, base loss: 4219.49
[INFO 2017-06-27 20:12:49,916 main.py:51] epoch 1667, training loss: 3193.93, average training loss: 3549.45, base loss: 4219.46
[INFO 2017-06-27 20:12:50,294 main.py:51] epoch 1668, training loss: 3372.09, average training loss: 3549.18, base loss: 4219.38
[INFO 2017-06-27 20:12:50,676 main.py:51] epoch 1669, training loss: 3499.66, average training loss: 3549.22, base loss: 4219.55
[INFO 2017-06-27 20:12:51,067 main.py:51] epoch 1670, training loss: 3335.61, average training loss: 3549.42, base loss: 4220.22
[INFO 2017-06-27 20:12:51,493 main.py:51] epoch 1671, training loss: 3066.91, average training loss: 3549.68, base loss: 4220.62
[INFO 2017-06-27 20:12:51,872 main.py:51] epoch 1672, training loss: 3509.16, average training loss: 3549.85, base loss: 4221.02
[INFO 2017-06-27 20:12:52,302 main.py:51] epoch 1673, training loss: 2785.44, average training loss: 3549.56, base loss: 4220.89
[INFO 2017-06-27 20:12:52,719 main.py:51] epoch 1674, training loss: 3199.41, average training loss: 3549.02, base loss: 4220.47
[INFO 2017-06-27 20:12:53,125 main.py:51] epoch 1675, training loss: 3417.34, average training loss: 3548.54, base loss: 4220.06
[INFO 2017-06-27 20:12:53,562 main.py:51] epoch 1676, training loss: 3390.23, average training loss: 3544.99, base loss: 4216.91
[INFO 2017-06-27 20:12:53,940 main.py:51] epoch 1677, training loss: 3490.42, average training loss: 3544.45, base loss: 4216.40
[INFO 2017-06-27 20:12:54,324 main.py:51] epoch 1678, training loss: 3556.58, average training loss: 3544.37, base loss: 4216.73
[INFO 2017-06-27 20:12:54,704 main.py:51] epoch 1679, training loss: 3285.73, average training loss: 3543.80, base loss: 4216.34
[INFO 2017-06-27 20:12:55,108 main.py:51] epoch 1680, training loss: 3903.42, average training loss: 3544.50, base loss: 4217.78
[INFO 2017-06-27 20:12:55,489 main.py:51] epoch 1681, training loss: 3610.09, average training loss: 3544.55, base loss: 4218.70
[INFO 2017-06-27 20:12:55,867 main.py:51] epoch 1682, training loss: 3401.67, average training loss: 3544.26, base loss: 4218.53
[INFO 2017-06-27 20:12:56,244 main.py:51] epoch 1683, training loss: 3713.45, average training loss: 3544.73, base loss: 4219.43
[INFO 2017-06-27 20:12:56,621 main.py:51] epoch 1684, training loss: 3357.48, average training loss: 3544.14, base loss: 4219.15
[INFO 2017-06-27 20:12:56,998 main.py:51] epoch 1685, training loss: 2949.51, average training loss: 3544.24, base loss: 4219.35
[INFO 2017-06-27 20:12:57,379 main.py:51] epoch 1686, training loss: 3141.07, average training loss: 3543.83, base loss: 4219.12
[INFO 2017-06-27 20:12:57,757 main.py:51] epoch 1687, training loss: 3029.04, average training loss: 3539.33, base loss: 4214.95
[INFO 2017-06-27 20:12:58,134 main.py:51] epoch 1688, training loss: 3570.30, average training loss: 3538.93, base loss: 4214.61
[INFO 2017-06-27 20:12:58,515 main.py:51] epoch 1689, training loss: 3777.40, average training loss: 3539.56, base loss: 4215.87
[INFO 2017-06-27 20:12:58,893 main.py:51] epoch 1690, training loss: 7070.27, average training loss: 3543.27, base loss: 4219.99
[INFO 2017-06-27 20:12:59,274 main.py:51] epoch 1691, training loss: 2893.88, average training loss: 3542.54, base loss: 4219.03
[INFO 2017-06-27 20:12:59,657 main.py:51] epoch 1692, training loss: 3247.01, average training loss: 3542.55, base loss: 4219.51
[INFO 2017-06-27 20:13:00,034 main.py:51] epoch 1693, training loss: 3562.34, average training loss: 3541.72, base loss: 4218.94
[INFO 2017-06-27 20:13:00,412 main.py:51] epoch 1694, training loss: 3431.18, average training loss: 3541.18, base loss: 4218.57
[INFO 2017-06-27 20:13:00,794 main.py:51] epoch 1695, training loss: 3128.86, average training loss: 3540.95, base loss: 4218.52
[INFO 2017-06-27 20:13:01,175 main.py:51] epoch 1696, training loss: 3384.61, average training loss: 3537.07, base loss: 4214.91
[INFO 2017-06-27 20:13:01,560 main.py:51] epoch 1697, training loss: 3034.78, average training loss: 3533.38, base loss: 4211.18
[INFO 2017-06-27 20:13:01,938 main.py:51] epoch 1698, training loss: 2997.95, average training loss: 3532.73, base loss: 4210.66
[INFO 2017-06-27 20:13:02,359 main.py:51] epoch 1699, training loss: 6724.61, average training loss: 3536.49, base loss: 4215.08
[INFO 2017-06-27 20:13:02,360 main.py:53] epoch 1699, testing
[INFO 2017-06-27 20:13:04,018 main.py:105] average testing loss: 3629.77, base loss: 4394.88
[INFO 2017-06-27 20:13:04,018 main.py:106] improve_loss: 765.12, improve_percent: 0.17
[INFO 2017-06-27 20:13:04,019 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 20:13:04,429 main.py:51] epoch 1700, training loss: 2871.00, average training loss: 3536.14, base loss: 4214.96
[INFO 2017-06-27 20:13:04,810 main.py:51] epoch 1701, training loss: 2999.29, average training loss: 3535.61, base loss: 4214.50
[INFO 2017-06-27 20:13:05,189 main.py:51] epoch 1702, training loss: 2472.09, average training loss: 3534.26, base loss: 4213.04
[INFO 2017-06-27 20:13:05,569 main.py:51] epoch 1703, training loss: 3592.09, average training loss: 3530.98, base loss: 4210.06
[INFO 2017-06-27 20:13:05,951 main.py:51] epoch 1704, training loss: 2955.30, average training loss: 3530.33, base loss: 4209.53
[INFO 2017-06-27 20:13:06,328 main.py:51] epoch 1705, training loss: 2739.61, average training loss: 3529.26, base loss: 4208.21
[INFO 2017-06-27 20:13:06,708 main.py:51] epoch 1706, training loss: 3588.40, average training loss: 3529.34, base loss: 4208.76
[INFO 2017-06-27 20:13:07,091 main.py:51] epoch 1707, training loss: 2959.70, average training loss: 3528.59, base loss: 4207.98
[INFO 2017-06-27 20:13:07,468 main.py:51] epoch 1708, training loss: 2904.24, average training loss: 3528.35, base loss: 4207.94
[INFO 2017-06-27 20:13:07,849 main.py:51] epoch 1709, training loss: 3237.20, average training loss: 3528.57, base loss: 4208.77
[INFO 2017-06-27 20:13:08,225 main.py:51] epoch 1710, training loss: 3116.40, average training loss: 3528.44, base loss: 4208.60
[INFO 2017-06-27 20:13:08,607 main.py:51] epoch 1711, training loss: 3162.75, average training loss: 3528.57, base loss: 4209.03
[INFO 2017-06-27 20:13:08,986 main.py:51] epoch 1712, training loss: 3589.10, average training loss: 3527.54, base loss: 4208.07
[INFO 2017-06-27 20:13:09,363 main.py:51] epoch 1713, training loss: 2883.00, average training loss: 3527.59, base loss: 4208.33
[INFO 2017-06-27 20:13:09,739 main.py:51] epoch 1714, training loss: 2790.26, average training loss: 3526.89, base loss: 4207.78
[INFO 2017-06-27 20:13:10,119 main.py:51] epoch 1715, training loss: 3074.16, average training loss: 3526.80, base loss: 4208.04
[INFO 2017-06-27 20:13:10,582 main.py:51] epoch 1716, training loss: 3102.39, average training loss: 3526.29, base loss: 4207.61
[INFO 2017-06-27 20:13:10,984 main.py:51] epoch 1717, training loss: 3509.09, average training loss: 3523.05, base loss: 4204.34
[INFO 2017-06-27 20:13:11,361 main.py:51] epoch 1718, training loss: 3269.38, average training loss: 3522.94, base loss: 4204.39
[INFO 2017-06-27 20:13:11,742 main.py:51] epoch 1719, training loss: 2854.28, average training loss: 3522.11, base loss: 4203.52
[INFO 2017-06-27 20:13:12,118 main.py:51] epoch 1720, training loss: 3546.60, average training loss: 3522.66, base loss: 4204.70
[INFO 2017-06-27 20:13:12,492 main.py:51] epoch 1721, training loss: 2764.11, average training loss: 3521.70, base loss: 4203.60
[INFO 2017-06-27 20:13:12,972 main.py:51] epoch 1722, training loss: 3244.40, average training loss: 3521.51, base loss: 4203.35
[INFO 2017-06-27 20:13:13,372 main.py:51] epoch 1723, training loss: 2914.62, average training loss: 3520.69, base loss: 4202.23
[INFO 2017-06-27 20:13:13,828 main.py:51] epoch 1724, training loss: 3348.15, average training loss: 3520.54, base loss: 4202.45
[INFO 2017-06-27 20:13:14,249 main.py:51] epoch 1725, training loss: 3784.58, average training loss: 3520.97, base loss: 4203.20
[INFO 2017-06-27 20:13:14,625 main.py:51] epoch 1726, training loss: 3080.72, average training loss: 3520.07, base loss: 4202.47
[INFO 2017-06-27 20:13:15,000 main.py:51] epoch 1727, training loss: 3120.63, average training loss: 3519.57, base loss: 4202.02
[INFO 2017-06-27 20:13:15,430 main.py:51] epoch 1728, training loss: 4066.33, average training loss: 3520.17, base loss: 4202.70
[INFO 2017-06-27 20:13:15,845 main.py:51] epoch 1729, training loss: 2991.96, average training loss: 3519.46, base loss: 4202.13
[INFO 2017-06-27 20:13:16,225 main.py:51] epoch 1730, training loss: 3389.47, average training loss: 3519.00, base loss: 4201.59
[INFO 2017-06-27 20:13:16,713 main.py:51] epoch 1731, training loss: 2794.05, average training loss: 3518.46, base loss: 4201.15
[INFO 2017-06-27 20:13:17,095 main.py:51] epoch 1732, training loss: 3420.56, average training loss: 3517.75, base loss: 4200.69
[INFO 2017-06-27 20:13:17,484 main.py:51] epoch 1733, training loss: 3892.39, average training loss: 3517.86, base loss: 4200.95
[INFO 2017-06-27 20:13:17,956 main.py:51] epoch 1734, training loss: 2837.52, average training loss: 3517.77, base loss: 4200.85
[INFO 2017-06-27 20:13:18,337 main.py:51] epoch 1735, training loss: 3347.19, average training loss: 3517.21, base loss: 4200.39
[INFO 2017-06-27 20:13:18,816 main.py:51] epoch 1736, training loss: 3215.58, average training loss: 3517.23, base loss: 4200.93
[INFO 2017-06-27 20:13:19,205 main.py:51] epoch 1737, training loss: 3459.51, average training loss: 3517.63, base loss: 4201.83
[INFO 2017-06-27 20:13:19,689 main.py:51] epoch 1738, training loss: 2889.76, average training loss: 3516.98, base loss: 4201.12
[INFO 2017-06-27 20:13:20,108 main.py:51] epoch 1739, training loss: 2927.60, average training loss: 3515.92, base loss: 4199.82
[INFO 2017-06-27 20:13:20,490 main.py:51] epoch 1740, training loss: 3287.93, average training loss: 3515.78, base loss: 4200.09
[INFO 2017-06-27 20:13:20,869 main.py:51] epoch 1741, training loss: 3502.22, average training loss: 3515.52, base loss: 4200.10
[INFO 2017-06-27 20:13:21,255 main.py:51] epoch 1742, training loss: 3002.27, average training loss: 3514.93, base loss: 4199.40
[INFO 2017-06-27 20:13:21,635 main.py:51] epoch 1743, training loss: 2881.06, average training loss: 3514.65, base loss: 4199.44
[INFO 2017-06-27 20:13:22,010 main.py:51] epoch 1744, training loss: 3160.28, average training loss: 3513.97, base loss: 4198.73
[INFO 2017-06-27 20:13:22,387 main.py:51] epoch 1745, training loss: 6859.36, average training loss: 3517.77, base loss: 4203.13
[INFO 2017-06-27 20:13:22,756 main.py:51] epoch 1746, training loss: 3395.54, average training loss: 3517.04, base loss: 4202.56
[INFO 2017-06-27 20:13:23,133 main.py:51] epoch 1747, training loss: 3261.55, average training loss: 3516.93, base loss: 4202.77
[INFO 2017-06-27 20:13:23,507 main.py:51] epoch 1748, training loss: 3954.55, average training loss: 3516.73, base loss: 4203.00
[INFO 2017-06-27 20:13:23,884 main.py:51] epoch 1749, training loss: 3174.62, average training loss: 3516.00, base loss: 4201.98
[INFO 2017-06-27 20:13:24,259 main.py:51] epoch 1750, training loss: 3407.71, average training loss: 3516.47, base loss: 4202.99
[INFO 2017-06-27 20:13:24,634 main.py:51] epoch 1751, training loss: 3289.22, average training loss: 3516.25, base loss: 4203.03
[INFO 2017-06-27 20:13:25,008 main.py:51] epoch 1752, training loss: 3405.14, average training loss: 3515.90, base loss: 4202.92
[INFO 2017-06-27 20:13:25,386 main.py:51] epoch 1753, training loss: 3373.06, average training loss: 3515.39, base loss: 4202.33
[INFO 2017-06-27 20:13:25,761 main.py:51] epoch 1754, training loss: 3558.25, average training loss: 3514.86, base loss: 4201.91
[INFO 2017-06-27 20:13:26,137 main.py:51] epoch 1755, training loss: 3601.32, average training loss: 3515.39, base loss: 4203.27
[INFO 2017-06-27 20:13:26,509 main.py:51] epoch 1756, training loss: 2707.53, average training loss: 3514.65, base loss: 4202.45
[INFO 2017-06-27 20:13:26,879 main.py:51] epoch 1757, training loss: 3429.90, average training loss: 3514.55, base loss: 4202.46
[INFO 2017-06-27 20:13:27,257 main.py:51] epoch 1758, training loss: 2810.59, average training loss: 3513.41, base loss: 4201.22
[INFO 2017-06-27 20:13:27,631 main.py:51] epoch 1759, training loss: 3199.21, average training loss: 3513.02, base loss: 4200.81
[INFO 2017-06-27 20:13:28,004 main.py:51] epoch 1760, training loss: 3309.62, average training loss: 3512.59, base loss: 4200.57
[INFO 2017-06-27 20:13:28,386 main.py:51] epoch 1761, training loss: 3409.96, average training loss: 3512.86, base loss: 4201.08
[INFO 2017-06-27 20:13:28,762 main.py:51] epoch 1762, training loss: 2910.81, average training loss: 3512.34, base loss: 4200.60
[INFO 2017-06-27 20:13:29,152 main.py:51] epoch 1763, training loss: 3147.04, average training loss: 3508.26, base loss: 4197.10
[INFO 2017-06-27 20:13:29,529 main.py:51] epoch 1764, training loss: 3030.28, average training loss: 3507.25, base loss: 4195.79
[INFO 2017-06-27 20:13:29,906 main.py:51] epoch 1765, training loss: 3086.98, average training loss: 3506.92, base loss: 4195.68
[INFO 2017-06-27 20:13:30,283 main.py:51] epoch 1766, training loss: 2936.49, average training loss: 3506.55, base loss: 4195.65
[INFO 2017-06-27 20:13:30,656 main.py:51] epoch 1767, training loss: 3297.73, average training loss: 3506.69, base loss: 4196.28
[INFO 2017-06-27 20:13:31,028 main.py:51] epoch 1768, training loss: 3023.50, average training loss: 3505.56, base loss: 4194.84
[INFO 2017-06-27 20:13:31,401 main.py:51] epoch 1769, training loss: 3268.25, average training loss: 3505.09, base loss: 4194.35
[INFO 2017-06-27 20:13:31,774 main.py:51] epoch 1770, training loss: 2653.53, average training loss: 3503.83, base loss: 4192.77
[INFO 2017-06-27 20:13:32,151 main.py:51] epoch 1771, training loss: 3067.28, average training loss: 3503.15, base loss: 4192.13
[INFO 2017-06-27 20:13:32,524 main.py:51] epoch 1772, training loss: 2973.00, average training loss: 3502.98, base loss: 4192.11
[INFO 2017-06-27 20:13:32,901 main.py:51] epoch 1773, training loss: 3301.01, average training loss: 3503.19, base loss: 4192.45
[INFO 2017-06-27 20:13:33,277 main.py:51] epoch 1774, training loss: 3253.19, average training loss: 3499.67, base loss: 4189.33
[INFO 2017-06-27 20:13:33,654 main.py:51] epoch 1775, training loss: 3060.80, average training loss: 3499.24, base loss: 4189.05
[INFO 2017-06-27 20:13:34,112 main.py:51] epoch 1776, training loss: 3502.74, average training loss: 3498.96, base loss: 4189.03
[INFO 2017-06-27 20:13:34,507 main.py:51] epoch 1777, training loss: 3425.99, average training loss: 3498.95, base loss: 4189.76
[INFO 2017-06-27 20:13:34,888 main.py:51] epoch 1778, training loss: 3486.00, average training loss: 3498.91, base loss: 4189.58
[INFO 2017-06-27 20:13:35,267 main.py:51] epoch 1779, training loss: 3102.22, average training loss: 3499.38, base loss: 4190.55
[INFO 2017-06-27 20:13:35,645 main.py:51] epoch 1780, training loss: 3269.34, average training loss: 3498.77, base loss: 4190.21
[INFO 2017-06-27 20:13:36,019 main.py:51] epoch 1781, training loss: 2925.04, average training loss: 3498.67, base loss: 4190.42
[INFO 2017-06-27 20:13:36,405 main.py:51] epoch 1782, training loss: 3542.89, average training loss: 3498.83, base loss: 4191.25
[INFO 2017-06-27 20:13:36,779 main.py:51] epoch 1783, training loss: 2765.99, average training loss: 3498.03, base loss: 4190.11
[INFO 2017-06-27 20:13:37,158 main.py:51] epoch 1784, training loss: 3428.42, average training loss: 3497.56, base loss: 4189.84
[INFO 2017-06-27 20:13:37,533 main.py:51] epoch 1785, training loss: 3791.60, average training loss: 3494.89, base loss: 4187.96
[INFO 2017-06-27 20:13:37,904 main.py:51] epoch 1786, training loss: 3241.10, average training loss: 3494.43, base loss: 4187.85
[INFO 2017-06-27 20:13:38,276 main.py:51] epoch 1787, training loss: 6511.54, average training loss: 3497.48, base loss: 4190.75
[INFO 2017-06-27 20:13:38,652 main.py:51] epoch 1788, training loss: 3078.30, average training loss: 3497.28, base loss: 4190.71
[INFO 2017-06-27 20:13:39,022 main.py:51] epoch 1789, training loss: 3418.63, average training loss: 3497.65, base loss: 4191.56
[INFO 2017-06-27 20:13:39,396 main.py:51] epoch 1790, training loss: 3627.19, average training loss: 3498.54, base loss: 4193.06
[INFO 2017-06-27 20:13:39,771 main.py:51] epoch 1791, training loss: 2948.04, average training loss: 3498.13, base loss: 4192.92
[INFO 2017-06-27 20:13:40,149 main.py:51] epoch 1792, training loss: 3012.14, average training loss: 3498.04, base loss: 4193.19
[INFO 2017-06-27 20:13:40,524 main.py:51] epoch 1793, training loss: 2839.09, average training loss: 3497.57, base loss: 4192.81
[INFO 2017-06-27 20:13:40,903 main.py:51] epoch 1794, training loss: 2773.88, average training loss: 3496.37, base loss: 4191.49
[INFO 2017-06-27 20:13:41,275 main.py:51] epoch 1795, training loss: 3745.24, average training loss: 3497.45, base loss: 4193.30
[INFO 2017-06-27 20:13:41,652 main.py:51] epoch 1796, training loss: 3380.45, average training loss: 3497.10, base loss: 4193.17
[INFO 2017-06-27 20:13:42,030 main.py:51] epoch 1797, training loss: 3422.63, average training loss: 3496.93, base loss: 4193.03
[INFO 2017-06-27 20:13:42,406 main.py:51] epoch 1798, training loss: 3563.35, average training loss: 3496.90, base loss: 4193.37
[INFO 2017-06-27 20:13:42,788 main.py:51] epoch 1799, training loss: 3225.50, average training loss: 3497.18, base loss: 4194.37
[INFO 2017-06-27 20:13:42,788 main.py:53] epoch 1799, testing
[INFO 2017-06-27 20:13:44,388 main.py:105] average testing loss: 3188.65, base loss: 3924.46
[INFO 2017-06-27 20:13:44,388 main.py:106] improve_loss: 735.81, improve_percent: 0.19
[INFO 2017-06-27 20:13:44,389 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 20:13:44,760 main.py:51] epoch 1800, training loss: 3349.02, average training loss: 3497.40, base loss: 4194.67
[INFO 2017-06-27 20:13:45,137 main.py:51] epoch 1801, training loss: 6576.73, average training loss: 3500.72, base loss: 4198.08
[INFO 2017-06-27 20:13:45,510 main.py:51] epoch 1802, training loss: 3476.13, average training loss: 3500.58, base loss: 4198.12
[INFO 2017-06-27 20:13:45,885 main.py:51] epoch 1803, training loss: 3227.36, average training loss: 3500.29, base loss: 4198.10
[INFO 2017-06-27 20:13:46,268 main.py:51] epoch 1804, training loss: 3155.00, average training loss: 3500.16, base loss: 4198.08
[INFO 2017-06-27 20:13:46,643 main.py:51] epoch 1805, training loss: 7096.74, average training loss: 3503.01, base loss: 4200.96
[INFO 2017-06-27 20:13:47,018 main.py:51] epoch 1806, training loss: 2808.59, average training loss: 3498.68, base loss: 4196.40
[INFO 2017-06-27 20:13:47,390 main.py:51] epoch 1807, training loss: 3157.34, average training loss: 3498.81, base loss: 4196.93
[INFO 2017-06-27 20:13:47,766 main.py:51] epoch 1808, training loss: 2961.62, average training loss: 3498.23, base loss: 4196.33
[INFO 2017-06-27 20:13:48,140 main.py:51] epoch 1809, training loss: 3647.31, average training loss: 3498.09, base loss: 4196.58
[INFO 2017-06-27 20:13:48,514 main.py:51] epoch 1810, training loss: 3064.71, average training loss: 3497.87, base loss: 4196.49
[INFO 2017-06-27 20:13:48,892 main.py:51] epoch 1811, training loss: 3412.96, average training loss: 3498.35, base loss: 4197.41
[INFO 2017-06-27 20:13:49,272 main.py:51] epoch 1812, training loss: 3362.96, average training loss: 3498.37, base loss: 4198.00
[INFO 2017-06-27 20:13:49,647 main.py:51] epoch 1813, training loss: 3153.69, average training loss: 3497.91, base loss: 4197.63
[INFO 2017-06-27 20:13:50,030 main.py:51] epoch 1814, training loss: 3501.56, average training loss: 3498.28, base loss: 4198.07
[INFO 2017-06-27 20:13:50,497 main.py:51] epoch 1815, training loss: 3825.14, average training loss: 3498.91, base loss: 4198.90
[INFO 2017-06-27 20:13:50,879 main.py:51] epoch 1816, training loss: 3442.46, average training loss: 3498.28, base loss: 4198.43
[INFO 2017-06-27 20:13:51,338 main.py:51] epoch 1817, training loss: 3434.20, average training loss: 3497.97, base loss: 4197.89
[INFO 2017-06-27 20:13:51,758 main.py:51] epoch 1818, training loss: 3089.53, average training loss: 3497.45, base loss: 4197.61
[INFO 2017-06-27 20:13:52,146 main.py:51] epoch 1819, training loss: 3527.89, average training loss: 3497.26, base loss: 4197.55
[INFO 2017-06-27 20:13:52,527 main.py:51] epoch 1820, training loss: 6130.95, average training loss: 3499.64, base loss: 4199.95
[INFO 2017-06-27 20:13:52,921 main.py:51] epoch 1821, training loss: 3305.19, average training loss: 3499.45, base loss: 4200.00
[INFO 2017-06-27 20:13:53,297 main.py:51] epoch 1822, training loss: 3436.42, average training loss: 3499.51, base loss: 4200.12
[INFO 2017-06-27 20:13:53,672 main.py:51] epoch 1823, training loss: 3391.88, average training loss: 3499.51, base loss: 4200.54
[INFO 2017-06-27 20:13:54,049 main.py:51] epoch 1824, training loss: 2853.58, average training loss: 3498.59, base loss: 4199.70
[INFO 2017-06-27 20:13:54,427 main.py:51] epoch 1825, training loss: 2992.91, average training loss: 3498.29, base loss: 4199.27
[INFO 2017-06-27 20:13:54,803 main.py:51] epoch 1826, training loss: 3219.86, average training loss: 3498.16, base loss: 4199.16
[INFO 2017-06-27 20:13:55,178 main.py:51] epoch 1827, training loss: 3305.57, average training loss: 3498.02, base loss: 4198.99
[INFO 2017-06-27 20:13:55,553 main.py:51] epoch 1828, training loss: 2849.79, average training loss: 3497.48, base loss: 4198.49
[INFO 2017-06-27 20:13:55,929 main.py:51] epoch 1829, training loss: 2958.84, average training loss: 3496.38, base loss: 4197.26
[INFO 2017-06-27 20:13:56,306 main.py:51] epoch 1830, training loss: 3884.22, average training loss: 3496.95, base loss: 4198.09
[INFO 2017-06-27 20:13:56,678 main.py:51] epoch 1831, training loss: 3253.77, average training loss: 3496.53, base loss: 4197.78
[INFO 2017-06-27 20:13:57,052 main.py:51] epoch 1832, training loss: 3532.62, average training loss: 3496.87, base loss: 4198.98
[INFO 2017-06-27 20:13:57,427 main.py:51] epoch 1833, training loss: 6856.53, average training loss: 3500.44, base loss: 4202.87
[INFO 2017-06-27 20:13:57,806 main.py:51] epoch 1834, training loss: 2954.37, average training loss: 3499.61, base loss: 4201.88
[INFO 2017-06-27 20:13:58,179 main.py:51] epoch 1835, training loss: 3305.09, average training loss: 3499.99, base loss: 4202.95
[INFO 2017-06-27 20:13:58,558 main.py:51] epoch 1836, training loss: 3653.69, average training loss: 3499.98, base loss: 4203.50
[INFO 2017-06-27 20:13:58,934 main.py:51] epoch 1837, training loss: 3000.29, average training loss: 3499.11, base loss: 4202.79
[INFO 2017-06-27 20:13:59,313 main.py:51] epoch 1838, training loss: 2965.08, average training loss: 3498.37, base loss: 4202.28
[INFO 2017-06-27 20:13:59,689 main.py:51] epoch 1839, training loss: 2874.66, average training loss: 3493.85, base loss: 4197.77
[INFO 2017-06-27 20:14:00,066 main.py:51] epoch 1840, training loss: 3497.98, average training loss: 3493.92, base loss: 4198.36
[INFO 2017-06-27 20:14:00,533 main.py:51] epoch 1841, training loss: 3637.52, average training loss: 3494.34, base loss: 4199.29
[INFO 2017-06-27 20:14:00,931 main.py:51] epoch 1842, training loss: 2962.33, average training loss: 3493.92, base loss: 4198.95
[INFO 2017-06-27 20:14:01,373 main.py:51] epoch 1843, training loss: 3052.20, average training loss: 3489.58, base loss: 4194.46
[INFO 2017-06-27 20:14:01,816 main.py:51] epoch 1844, training loss: 3290.55, average training loss: 3488.91, base loss: 4194.06
[INFO 2017-06-27 20:14:02,194 main.py:51] epoch 1845, training loss: 3248.23, average training loss: 3488.25, base loss: 4193.64
[INFO 2017-06-27 20:14:02,573 main.py:51] epoch 1846, training loss: 3490.14, average training loss: 3484.64, base loss: 4190.03
[INFO 2017-06-27 20:14:02,968 main.py:51] epoch 1847, training loss: 3013.35, average training loss: 3484.31, base loss: 4189.97
[INFO 2017-06-27 20:14:03,342 main.py:51] epoch 1848, training loss: 3282.27, average training loss: 3483.78, base loss: 4189.26
[INFO 2017-06-27 20:14:03,717 main.py:51] epoch 1849, training loss: 3259.39, average training loss: 3483.24, base loss: 4188.98
[INFO 2017-06-27 20:14:04,093 main.py:51] epoch 1850, training loss: 3478.40, average training loss: 3482.96, base loss: 4188.99
[INFO 2017-06-27 20:14:04,468 main.py:51] epoch 1851, training loss: 2996.79, average training loss: 3478.70, base loss: 4184.68
[INFO 2017-06-27 20:14:04,849 main.py:51] epoch 1852, training loss: 3453.50, average training loss: 3477.79, base loss: 4184.14
[INFO 2017-06-27 20:14:05,225 main.py:51] epoch 1853, training loss: 3748.74, average training loss: 3478.55, base loss: 4185.48
[INFO 2017-06-27 20:14:05,601 main.py:51] epoch 1854, training loss: 3309.87, average training loss: 3474.89, base loss: 4182.14
[INFO 2017-06-27 20:14:05,978 main.py:51] epoch 1855, training loss: 3079.13, average training loss: 3470.89, base loss: 4178.37
[INFO 2017-06-27 20:14:06,355 main.py:51] epoch 1856, training loss: 3039.62, average training loss: 3470.29, base loss: 4177.94
[INFO 2017-06-27 20:14:06,732 main.py:51] epoch 1857, training loss: 3008.45, average training loss: 3468.88, base loss: 4176.41
[INFO 2017-06-27 20:14:07,107 main.py:51] epoch 1858, training loss: 3182.76, average training loss: 3468.65, base loss: 4176.33
[INFO 2017-06-27 20:14:07,483 main.py:51] epoch 1859, training loss: 3232.71, average training loss: 3468.23, base loss: 4175.88
[INFO 2017-06-27 20:14:07,860 main.py:51] epoch 1860, training loss: 3180.67, average training loss: 3468.06, base loss: 4175.66
[INFO 2017-06-27 20:14:08,241 main.py:51] epoch 1861, training loss: 3116.51, average training loss: 3467.18, base loss: 4174.79
[INFO 2017-06-27 20:14:08,621 main.py:51] epoch 1862, training loss: 2788.64, average training loss: 3466.51, base loss: 4174.17
[INFO 2017-06-27 20:14:08,997 main.py:51] epoch 1863, training loss: 3378.58, average training loss: 3466.29, base loss: 4174.11
[INFO 2017-06-27 20:14:09,373 main.py:51] epoch 1864, training loss: 3071.20, average training loss: 3466.16, base loss: 4174.52
[INFO 2017-06-27 20:14:09,749 main.py:51] epoch 1865, training loss: 3393.82, average training loss: 3465.78, base loss: 4174.33
[INFO 2017-06-27 20:14:10,162 main.py:51] epoch 1866, training loss: 2750.25, average training loss: 3465.00, base loss: 4173.35
[INFO 2017-06-27 20:14:10,589 main.py:51] epoch 1867, training loss: 3282.33, average training loss: 3464.32, base loss: 4172.91
[INFO 2017-06-27 20:14:10,975 main.py:51] epoch 1868, training loss: 3417.96, average training loss: 3463.65, base loss: 4172.40
[INFO 2017-06-27 20:14:11,436 main.py:51] epoch 1869, training loss: 2962.60, average training loss: 3462.89, base loss: 4171.66
[INFO 2017-06-27 20:14:11,824 main.py:51] epoch 1870, training loss: 3118.33, average training loss: 3462.57, base loss: 4171.61
[INFO 2017-06-27 20:14:12,261 main.py:51] epoch 1871, training loss: 2870.76, average training loss: 3461.99, base loss: 4171.32
[INFO 2017-06-27 20:14:12,691 main.py:51] epoch 1872, training loss: 3547.53, average training loss: 3462.31, base loss: 4172.33
[INFO 2017-06-27 20:14:13,075 main.py:51] epoch 1873, training loss: 3026.51, average training loss: 3461.45, base loss: 4171.51
[INFO 2017-06-27 20:14:13,512 main.py:51] epoch 1874, training loss: 3170.79, average training loss: 3457.77, base loss: 4167.99
[INFO 2017-06-27 20:14:13,927 main.py:51] epoch 1875, training loss: 3739.79, average training loss: 3458.35, base loss: 4169.46
[INFO 2017-06-27 20:14:14,310 main.py:51] epoch 1876, training loss: 3349.93, average training loss: 3458.26, base loss: 4169.34
[INFO 2017-06-27 20:14:14,780 main.py:51] epoch 1877, training loss: 3446.98, average training loss: 3458.19, base loss: 4169.16
[INFO 2017-06-27 20:14:15,173 main.py:51] epoch 1878, training loss: 2816.21, average training loss: 3457.27, base loss: 4168.14
[INFO 2017-06-27 20:14:15,551 main.py:51] epoch 1879, training loss: 3106.42, average training loss: 3456.75, base loss: 4167.71
[INFO 2017-06-27 20:14:16,022 main.py:51] epoch 1880, training loss: 3328.04, average training loss: 3456.60, base loss: 4167.90
[INFO 2017-06-27 20:14:16,442 main.py:51] epoch 1881, training loss: 2968.53, average training loss: 3455.92, base loss: 4166.78
[INFO 2017-06-27 20:14:16,901 main.py:51] epoch 1882, training loss: 2983.93, average training loss: 3455.03, base loss: 4165.47
[INFO 2017-06-27 20:14:17,338 main.py:51] epoch 1883, training loss: 3403.15, average training loss: 3451.08, base loss: 4161.82
[INFO 2017-06-27 20:14:17,717 main.py:51] epoch 1884, training loss: 3753.67, average training loss: 3451.34, base loss: 4162.46
[INFO 2017-06-27 20:14:18,094 main.py:51] epoch 1885, training loss: 2669.06, average training loss: 3447.51, base loss: 4158.76
[INFO 2017-06-27 20:14:18,471 main.py:51] epoch 1886, training loss: 2929.86, average training loss: 3446.72, base loss: 4157.76
[INFO 2017-06-27 20:14:18,860 main.py:51] epoch 1887, training loss: 3514.39, average training loss: 3446.46, base loss: 4158.16
[INFO 2017-06-27 20:14:19,235 main.py:51] epoch 1888, training loss: 3527.57, average training loss: 3446.92, base loss: 4159.04
[INFO 2017-06-27 20:14:19,616 main.py:51] epoch 1889, training loss: 3073.19, average training loss: 3446.79, base loss: 4159.13
[INFO 2017-06-27 20:14:19,990 main.py:51] epoch 1890, training loss: 3414.33, average training loss: 3446.31, base loss: 4158.73
[INFO 2017-06-27 20:14:20,366 main.py:51] epoch 1891, training loss: 3483.40, average training loss: 3445.82, base loss: 4158.35
[INFO 2017-06-27 20:14:20,739 main.py:51] epoch 1892, training loss: 3805.08, average training loss: 3446.16, base loss: 4159.13
[INFO 2017-06-27 20:14:21,115 main.py:51] epoch 1893, training loss: 3388.38, average training loss: 3446.56, base loss: 4160.08
[INFO 2017-06-27 20:14:21,491 main.py:51] epoch 1894, training loss: 3602.87, average training loss: 3446.40, base loss: 4160.08
[INFO 2017-06-27 20:14:21,870 main.py:51] epoch 1895, training loss: 3828.49, average training loss: 3446.77, base loss: 4161.00
[INFO 2017-06-27 20:14:22,243 main.py:51] epoch 1896, training loss: 3001.20, average training loss: 3446.51, base loss: 4161.07
[INFO 2017-06-27 20:14:22,613 main.py:51] epoch 1897, training loss: 3450.60, average training loss: 3447.26, base loss: 4162.42
[INFO 2017-06-27 20:14:22,989 main.py:51] epoch 1898, training loss: 3013.71, average training loss: 3446.91, base loss: 4162.25
[INFO 2017-06-27 20:14:23,360 main.py:51] epoch 1899, training loss: 3026.62, average training loss: 3446.69, base loss: 4162.10
[INFO 2017-06-27 20:14:23,360 main.py:53] epoch 1899, testing
[INFO 2017-06-27 20:14:24,965 main.py:105] average testing loss: 3231.34, base loss: 4051.60
[INFO 2017-06-27 20:14:24,965 main.py:106] improve_loss: 820.26, improve_percent: 0.20
[INFO 2017-06-27 20:14:24,966 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:14:24,978 main.py:76] current best improved percent: 0.20
[INFO 2017-06-27 20:14:25,349 main.py:51] epoch 1900, training loss: 3086.76, average training loss: 3446.41, base loss: 4162.18
[INFO 2017-06-27 20:14:25,718 main.py:51] epoch 1901, training loss: 2735.82, average training loss: 3445.72, base loss: 4161.51
[INFO 2017-06-27 20:14:26,095 main.py:51] epoch 1902, training loss: 2931.97, average training loss: 3445.38, base loss: 4161.40
[INFO 2017-06-27 20:14:26,471 main.py:51] epoch 1903, training loss: 3170.07, average training loss: 3445.59, base loss: 4161.75
[INFO 2017-06-27 20:14:26,854 main.py:51] epoch 1904, training loss: 2821.42, average training loss: 3444.35, base loss: 4160.31
[INFO 2017-06-27 20:14:27,234 main.py:51] epoch 1905, training loss: 3370.04, average training loss: 3444.67, base loss: 4161.12
[INFO 2017-06-27 20:14:27,608 main.py:51] epoch 1906, training loss: 3252.90, average training loss: 3444.65, base loss: 4161.42
[INFO 2017-06-27 20:14:27,984 main.py:51] epoch 1907, training loss: 3531.30, average training loss: 3444.98, base loss: 4162.16
[INFO 2017-06-27 20:14:28,361 main.py:51] epoch 1908, training loss: 3420.59, average training loss: 3444.43, base loss: 4161.98
[INFO 2017-06-27 20:14:28,773 main.py:51] epoch 1909, training loss: 6030.90, average training loss: 3446.85, base loss: 4164.48
[INFO 2017-06-27 20:14:29,151 main.py:51] epoch 1910, training loss: 4025.52, average training loss: 3446.98, base loss: 4164.74
[INFO 2017-06-27 20:14:29,527 main.py:51] epoch 1911, training loss: 2522.27, average training loss: 3446.37, base loss: 4164.08
[INFO 2017-06-27 20:14:29,915 main.py:51] epoch 1912, training loss: 2937.98, average training loss: 3445.83, base loss: 4163.58
[INFO 2017-06-27 20:14:30,299 main.py:51] epoch 1913, training loss: 3622.68, average training loss: 3445.31, base loss: 4163.31
[INFO 2017-06-27 20:14:30,687 main.py:51] epoch 1914, training loss: 2653.69, average training loss: 3444.43, base loss: 4162.06
[INFO 2017-06-27 20:14:31,090 main.py:51] epoch 1915, training loss: 3443.42, average training loss: 3444.23, base loss: 4162.35
[INFO 2017-06-27 20:14:31,473 main.py:51] epoch 1916, training loss: 3225.62, average training loss: 3443.58, base loss: 4162.20
[INFO 2017-06-27 20:14:31,862 main.py:51] epoch 1917, training loss: 3199.76, average training loss: 3442.80, base loss: 4161.45
[INFO 2017-06-27 20:14:32,243 main.py:51] epoch 1918, training loss: 3262.45, average training loss: 3442.59, base loss: 4161.68
[INFO 2017-06-27 20:14:32,642 main.py:51] epoch 1919, training loss: 3727.98, average training loss: 3442.28, base loss: 4161.21
[INFO 2017-06-27 20:14:33,113 main.py:51] epoch 1920, training loss: 3498.94, average training loss: 3442.51, base loss: 4162.02
[INFO 2017-06-27 20:14:33,542 main.py:51] epoch 1921, training loss: 3248.50, average training loss: 3443.06, base loss: 4163.26
[INFO 2017-06-27 20:14:33,989 main.py:51] epoch 1922, training loss: 3781.58, average training loss: 3444.07, base loss: 4165.01
[INFO 2017-06-27 20:14:34,433 main.py:51] epoch 1923, training loss: 3127.68, average training loss: 3443.32, base loss: 4164.19
[INFO 2017-06-27 20:14:34,864 main.py:51] epoch 1924, training loss: 3438.91, average training loss: 3443.55, base loss: 4164.72
[INFO 2017-06-27 20:14:35,265 main.py:51] epoch 1925, training loss: 3898.50, average training loss: 3443.84, base loss: 4165.43
[INFO 2017-06-27 20:14:35,714 main.py:51] epoch 1926, training loss: 2851.03, average training loss: 3442.65, base loss: 4164.00
[INFO 2017-06-27 20:14:36,180 main.py:51] epoch 1927, training loss: 2865.18, average training loss: 3441.66, base loss: 4162.98
[INFO 2017-06-27 20:14:36,570 main.py:51] epoch 1928, training loss: 3115.79, average training loss: 3441.30, base loss: 4162.68
[INFO 2017-06-27 20:14:36,951 main.py:51] epoch 1929, training loss: 2938.53, average training loss: 3440.34, base loss: 4161.31
[INFO 2017-06-27 20:14:37,333 main.py:51] epoch 1930, training loss: 3369.28, average training loss: 3439.92, base loss: 4161.13
[INFO 2017-06-27 20:14:37,715 main.py:51] epoch 1931, training loss: 3656.00, average training loss: 3436.62, base loss: 4158.43
[INFO 2017-06-27 20:14:38,112 main.py:51] epoch 1932, training loss: 3088.73, average training loss: 3436.31, base loss: 4157.91
[INFO 2017-06-27 20:14:38,489 main.py:51] epoch 1933, training loss: 2827.72, average training loss: 3435.94, base loss: 4157.53
[INFO 2017-06-27 20:14:38,867 main.py:51] epoch 1934, training loss: 4251.76, average training loss: 3437.00, base loss: 4159.55
[INFO 2017-06-27 20:14:39,243 main.py:51] epoch 1935, training loss: 2908.26, average training loss: 3437.11, base loss: 4159.94
[INFO 2017-06-27 20:14:39,624 main.py:51] epoch 1936, training loss: 3100.30, average training loss: 3436.34, base loss: 4159.12
[INFO 2017-06-27 20:14:39,999 main.py:51] epoch 1937, training loss: 3722.12, average training loss: 3436.55, base loss: 4159.91
[INFO 2017-06-27 20:14:40,374 main.py:51] epoch 1938, training loss: 3180.77, average training loss: 3436.55, base loss: 4160.19
[INFO 2017-06-27 20:14:40,753 main.py:51] epoch 1939, training loss: 2881.59, average training loss: 3435.65, base loss: 4159.41
[INFO 2017-06-27 20:14:41,129 main.py:51] epoch 1940, training loss: 3455.25, average training loss: 3435.98, base loss: 4160.17
[INFO 2017-06-27 20:14:41,504 main.py:51] epoch 1941, training loss: 3377.78, average training loss: 3435.58, base loss: 4159.79
[INFO 2017-06-27 20:14:41,880 main.py:51] epoch 1942, training loss: 2859.69, average training loss: 3435.63, base loss: 4160.23
[INFO 2017-06-27 20:14:42,256 main.py:51] epoch 1943, training loss: 3323.62, average training loss: 3435.26, base loss: 4160.00
[INFO 2017-06-27 20:14:42,637 main.py:51] epoch 1944, training loss: 2902.70, average training loss: 3434.92, base loss: 4159.78
[INFO 2017-06-27 20:14:43,012 main.py:51] epoch 1945, training loss: 3789.58, average training loss: 3435.35, base loss: 4160.61
[INFO 2017-06-27 20:14:43,388 main.py:51] epoch 1946, training loss: 3388.45, average training loss: 3435.38, base loss: 4161.04
[INFO 2017-06-27 20:14:43,763 main.py:51] epoch 1947, training loss: 3389.32, average training loss: 3435.82, base loss: 4161.89
[INFO 2017-06-27 20:14:44,146 main.py:51] epoch 1948, training loss: 3577.52, average training loss: 3435.81, base loss: 4162.31
[INFO 2017-06-27 20:14:44,530 main.py:51] epoch 1949, training loss: 6528.50, average training loss: 3439.10, base loss: 4165.69
[INFO 2017-06-27 20:14:44,909 main.py:51] epoch 1950, training loss: 2956.59, average training loss: 3438.65, base loss: 4165.32
[INFO 2017-06-27 20:14:45,291 main.py:51] epoch 1951, training loss: 2908.92, average training loss: 3437.86, base loss: 4164.45
[INFO 2017-06-27 20:14:45,763 main.py:51] epoch 1952, training loss: 2757.61, average training loss: 3437.03, base loss: 4163.65
[INFO 2017-06-27 20:14:46,146 main.py:51] epoch 1953, training loss: 3084.94, average training loss: 3436.17, base loss: 4162.58
[INFO 2017-06-27 20:14:46,549 main.py:51] epoch 1954, training loss: 3360.83, average training loss: 3436.01, base loss: 4162.18
[INFO 2017-06-27 20:14:46,931 main.py:51] epoch 1955, training loss: 3251.29, average training loss: 3435.51, base loss: 4161.65
[INFO 2017-06-27 20:14:47,320 main.py:51] epoch 1956, training loss: 2998.59, average training loss: 3435.17, base loss: 4161.26
[INFO 2017-06-27 20:14:47,696 main.py:51] epoch 1957, training loss: 3198.75, average training loss: 3435.30, base loss: 4161.51
[INFO 2017-06-27 20:14:48,076 main.py:51] epoch 1958, training loss: 3204.92, average training loss: 3435.62, base loss: 4162.34
[INFO 2017-06-27 20:14:48,450 main.py:51] epoch 1959, training loss: 3183.74, average training loss: 3435.26, base loss: 4162.14
[INFO 2017-06-27 20:14:48,826 main.py:51] epoch 1960, training loss: 3006.76, average training loss: 3435.23, base loss: 4162.52
[INFO 2017-06-27 20:14:49,203 main.py:51] epoch 1961, training loss: 3368.49, average training loss: 3434.94, base loss: 4162.55
[INFO 2017-06-27 20:14:49,579 main.py:51] epoch 1962, training loss: 3299.06, average training loss: 3434.51, base loss: 4162.40
[INFO 2017-06-27 20:14:49,954 main.py:51] epoch 1963, training loss: 3714.42, average training loss: 3434.79, base loss: 4163.40
[INFO 2017-06-27 20:14:50,329 main.py:51] epoch 1964, training loss: 2962.26, average training loss: 3434.49, base loss: 4163.09
[INFO 2017-06-27 20:14:50,709 main.py:51] epoch 1965, training loss: 2852.93, average training loss: 3433.82, base loss: 4162.74
[INFO 2017-06-27 20:14:51,086 main.py:51] epoch 1966, training loss: 3318.60, average training loss: 3433.89, base loss: 4163.16
[INFO 2017-06-27 20:14:51,468 main.py:51] epoch 1967, training loss: 3695.79, average training loss: 3434.14, base loss: 4163.67
[INFO 2017-06-27 20:14:51,850 main.py:51] epoch 1968, training loss: 3068.39, average training loss: 3434.11, base loss: 4164.01
[INFO 2017-06-27 20:14:52,229 main.py:51] epoch 1969, training loss: 2726.98, average training loss: 3433.44, base loss: 4163.20
[INFO 2017-06-27 20:14:52,605 main.py:51] epoch 1970, training loss: 3195.07, average training loss: 3433.38, base loss: 4163.62
[INFO 2017-06-27 20:14:52,981 main.py:51] epoch 1971, training loss: 3047.31, average training loss: 3433.38, base loss: 4164.07
[INFO 2017-06-27 20:14:53,358 main.py:51] epoch 1972, training loss: 6389.44, average training loss: 3435.89, base loss: 4166.28
[INFO 2017-06-27 20:14:53,737 main.py:51] epoch 1973, training loss: 3097.55, average training loss: 3434.82, base loss: 4164.88
[INFO 2017-06-27 20:14:54,116 main.py:51] epoch 1974, training loss: 3252.11, average training loss: 3434.87, base loss: 4165.38
[INFO 2017-06-27 20:14:54,491 main.py:51] epoch 1975, training loss: 4278.19, average training loss: 3436.24, base loss: 4167.60
[INFO 2017-06-27 20:14:54,869 main.py:51] epoch 1976, training loss: 6026.87, average training loss: 3435.57, base loss: 4166.67
[INFO 2017-06-27 20:14:55,245 main.py:51] epoch 1977, training loss: 2827.20, average training loss: 3434.62, base loss: 4165.33
[INFO 2017-06-27 20:14:55,621 main.py:51] epoch 1978, training loss: 2817.74, average training loss: 3433.86, base loss: 4164.23
[INFO 2017-06-27 20:14:55,996 main.py:51] epoch 1979, training loss: 3482.58, average training loss: 3433.86, base loss: 4164.52
[INFO 2017-06-27 20:14:56,371 main.py:51] epoch 1980, training loss: 3003.59, average training loss: 3433.57, base loss: 4164.45
[INFO 2017-06-27 20:14:56,746 main.py:51] epoch 1981, training loss: 3188.95, average training loss: 3433.67, base loss: 4165.03
[INFO 2017-06-27 20:14:57,126 main.py:51] epoch 1982, training loss: 2823.26, average training loss: 3432.83, base loss: 4164.15
[INFO 2017-06-27 20:14:57,507 main.py:51] epoch 1983, training loss: 6600.68, average training loss: 3436.04, base loss: 4167.51
[INFO 2017-06-27 20:14:57,883 main.py:51] epoch 1984, training loss: 3012.20, average training loss: 3435.74, base loss: 4167.23
[INFO 2017-06-27 20:14:58,347 main.py:51] epoch 1985, training loss: 2851.77, average training loss: 3434.25, base loss: 4165.64
[INFO 2017-06-27 20:14:58,744 main.py:51] epoch 1986, training loss: 3325.31, average training loss: 3434.46, base loss: 4165.98
[INFO 2017-06-27 20:14:59,122 main.py:51] epoch 1987, training loss: 2859.75, average training loss: 3433.79, base loss: 4165.25
[INFO 2017-06-27 20:14:59,585 main.py:51] epoch 1988, training loss: 6949.60, average training loss: 3437.28, base loss: 4168.74
[INFO 2017-06-27 20:14:59,984 main.py:51] epoch 1989, training loss: 3254.82, average training loss: 3437.74, base loss: 4169.43
[INFO 2017-06-27 20:15:00,363 main.py:51] epoch 1990, training loss: 3225.88, average training loss: 3437.04, base loss: 4168.70
[INFO 2017-06-27 20:15:00,787 main.py:51] epoch 1991, training loss: 3386.01, average training loss: 3436.89, base loss: 4168.65
[INFO 2017-06-27 20:15:01,169 main.py:51] epoch 1992, training loss: 3455.89, average training loss: 3436.97, base loss: 4168.94
[INFO 2017-06-27 20:15:01,553 main.py:51] epoch 1993, training loss: 3049.08, average training loss: 3437.02, base loss: 4168.91
[INFO 2017-06-27 20:15:01,933 main.py:51] epoch 1994, training loss: 3472.02, average training loss: 3436.93, base loss: 4169.27
[INFO 2017-06-27 20:15:02,306 main.py:51] epoch 1995, training loss: 3071.22, average training loss: 3436.61, base loss: 4169.16
[INFO 2017-06-27 20:15:02,764 main.py:51] epoch 1996, training loss: 3158.70, average training loss: 3435.72, base loss: 4168.42
[INFO 2017-06-27 20:15:03,190 main.py:51] epoch 1997, training loss: 3062.23, average training loss: 3435.59, base loss: 4168.56
[INFO 2017-06-27 20:15:03,670 main.py:51] epoch 1998, training loss: 3140.99, average training loss: 3435.14, base loss: 4168.37
[INFO 2017-06-27 20:15:04,079 main.py:51] epoch 1999, training loss: 2885.32, average training loss: 3434.85, base loss: 4168.20
[INFO 2017-06-27 20:15:04,079 main.py:53] epoch 1999, testing
[INFO 2017-06-27 20:15:05,722 main.py:105] average testing loss: 3375.66, base loss: 4328.67
[INFO 2017-06-27 20:15:05,722 main.py:106] improve_loss: 953.01, improve_percent: 0.22
[INFO 2017-06-27 20:15:05,723 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:15:05,735 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 20:15:06,114 main.py:51] epoch 2000, training loss: 6779.85, average training loss: 3438.08, base loss: 4171.68
[INFO 2017-06-27 20:15:06,490 main.py:51] epoch 2001, training loss: 2669.47, average training loss: 3436.99, base loss: 4170.49
[INFO 2017-06-27 20:15:06,863 main.py:51] epoch 2002, training loss: 3563.92, average training loss: 3437.35, base loss: 4171.28
[INFO 2017-06-27 20:15:07,241 main.py:51] epoch 2003, training loss: 3418.99, average training loss: 3437.05, base loss: 4171.27
[INFO 2017-06-27 20:15:07,616 main.py:51] epoch 2004, training loss: 3008.05, average training loss: 3436.98, base loss: 4171.37
[INFO 2017-06-27 20:15:07,993 main.py:51] epoch 2005, training loss: 3213.66, average training loss: 3437.49, base loss: 4172.50
[INFO 2017-06-27 20:15:08,370 main.py:51] epoch 2006, training loss: 6437.44, average training loss: 3440.03, base loss: 4174.69
[INFO 2017-06-27 20:15:08,751 main.py:51] epoch 2007, training loss: 4384.60, average training loss: 3441.42, base loss: 4176.97
[INFO 2017-06-27 20:15:09,131 main.py:51] epoch 2008, training loss: 3500.69, average training loss: 3441.57, base loss: 4177.46
[INFO 2017-06-27 20:15:09,509 main.py:51] epoch 2009, training loss: 2550.79, average training loss: 3441.41, base loss: 4177.13
[INFO 2017-06-27 20:15:09,910 main.py:51] epoch 2010, training loss: 3862.20, average training loss: 3441.85, base loss: 4178.26
[INFO 2017-06-27 20:15:10,290 main.py:51] epoch 2011, training loss: 3563.36, average training loss: 3442.25, base loss: 4179.07
[INFO 2017-06-27 20:15:10,671 main.py:51] epoch 2012, training loss: 3567.48, average training loss: 3442.39, base loss: 4179.81
[INFO 2017-06-27 20:15:11,049 main.py:51] epoch 2013, training loss: 3315.06, average training loss: 3442.72, base loss: 4180.27
[INFO 2017-06-27 20:15:11,434 main.py:51] epoch 2014, training loss: 3624.96, average training loss: 3442.95, base loss: 4181.20
[INFO 2017-06-27 20:15:11,817 main.py:51] epoch 2015, training loss: 3119.75, average training loss: 3443.06, base loss: 4182.24
[INFO 2017-06-27 20:15:12,198 main.py:51] epoch 2016, training loss: 3058.92, average training loss: 3443.14, base loss: 4182.64
[INFO 2017-06-27 20:15:12,584 main.py:51] epoch 2017, training loss: 3178.27, average training loss: 3442.59, base loss: 4182.48
[INFO 2017-06-27 20:15:12,999 main.py:51] epoch 2018, training loss: 2964.51, average training loss: 3441.68, base loss: 4181.67
[INFO 2017-06-27 20:15:13,444 main.py:51] epoch 2019, training loss: 2895.64, average training loss: 3441.56, base loss: 4181.65
[INFO 2017-06-27 20:15:13,883 main.py:51] epoch 2020, training loss: 3383.39, average training loss: 3442.03, base loss: 4182.22
[INFO 2017-06-27 20:15:14,322 main.py:51] epoch 2021, training loss: 3451.88, average training loss: 3442.35, base loss: 4182.93
[INFO 2017-06-27 20:15:14,778 main.py:51] epoch 2022, training loss: 2880.97, average training loss: 3442.06, base loss: 4182.70
[INFO 2017-06-27 20:15:15,188 main.py:51] epoch 2023, training loss: 3291.82, average training loss: 3441.73, base loss: 4182.38
[INFO 2017-06-27 20:15:15,631 main.py:51] epoch 2024, training loss: 6826.74, average training loss: 3445.26, base loss: 4186.67
[INFO 2017-06-27 20:15:16,109 main.py:51] epoch 2025, training loss: 3672.24, average training loss: 3445.59, base loss: 4187.74
[INFO 2017-06-27 20:15:16,520 main.py:51] epoch 2026, training loss: 3261.19, average training loss: 3445.44, base loss: 4187.67
[INFO 2017-06-27 20:15:16,897 main.py:51] epoch 2027, training loss: 2956.45, average training loss: 3444.57, base loss: 4186.45
[INFO 2017-06-27 20:15:17,279 main.py:51] epoch 2028, training loss: 3495.82, average training loss: 3444.73, base loss: 4186.98
[INFO 2017-06-27 20:15:17,651 main.py:51] epoch 2029, training loss: 3102.92, average training loss: 3443.88, base loss: 4185.99
[INFO 2017-06-27 20:15:18,067 main.py:51] epoch 2030, training loss: 3375.88, average training loss: 3444.02, base loss: 4186.53
[INFO 2017-06-27 20:15:18,487 main.py:51] epoch 2031, training loss: 3260.62, average training loss: 3444.08, base loss: 4186.69
[INFO 2017-06-27 20:15:18,868 main.py:51] epoch 2032, training loss: 2837.81, average training loss: 3443.39, base loss: 4186.09
[INFO 2017-06-27 20:15:19,329 main.py:51] epoch 2033, training loss: 3746.53, average training loss: 3444.56, base loss: 4188.13
[INFO 2017-06-27 20:15:19,710 main.py:51] epoch 2034, training loss: 3014.59, average training loss: 3444.06, base loss: 4187.71
[INFO 2017-06-27 20:15:20,179 main.py:51] epoch 2035, training loss: 3125.65, average training loss: 3443.87, base loss: 4187.78
[INFO 2017-06-27 20:15:20,560 main.py:51] epoch 2036, training loss: 3251.41, average training loss: 3443.66, base loss: 4187.54
[INFO 2017-06-27 20:15:20,938 main.py:51] epoch 2037, training loss: 4002.95, average training loss: 3444.07, base loss: 4188.85
[INFO 2017-06-27 20:15:21,360 main.py:51] epoch 2038, training loss: 2940.80, average training loss: 3443.90, base loss: 4188.89
[INFO 2017-06-27 20:15:21,764 main.py:51] epoch 2039, training loss: 3548.75, average training loss: 3443.92, base loss: 4189.26
[INFO 2017-06-27 20:15:22,153 main.py:51] epoch 2040, training loss: 3730.24, average training loss: 3444.31, base loss: 4190.33
[INFO 2017-06-27 20:15:22,576 main.py:51] epoch 2041, training loss: 2741.89, average training loss: 3443.95, base loss: 4189.79
[INFO 2017-06-27 20:15:22,968 main.py:51] epoch 2042, training loss: 3196.37, average training loss: 3444.18, base loss: 4189.98
[INFO 2017-06-27 20:15:23,435 main.py:51] epoch 2043, training loss: 7158.77, average training loss: 3448.12, base loss: 4194.43
[INFO 2017-06-27 20:15:23,823 main.py:51] epoch 2044, training loss: 3365.23, average training loss: 3448.60, base loss: 4195.38
[INFO 2017-06-27 20:15:24,225 main.py:51] epoch 2045, training loss: 2802.04, average training loss: 3447.97, base loss: 4194.41
[INFO 2017-06-27 20:15:24,601 main.py:51] epoch 2046, training loss: 3079.44, average training loss: 3446.81, base loss: 4192.92
[INFO 2017-06-27 20:15:24,975 main.py:51] epoch 2047, training loss: 3264.06, average training loss: 3447.01, base loss: 4193.28
[INFO 2017-06-27 20:15:25,355 main.py:51] epoch 2048, training loss: 3156.15, average training loss: 3447.16, base loss: 4193.74
[INFO 2017-06-27 20:15:25,733 main.py:51] epoch 2049, training loss: 3465.23, average training loss: 3446.87, base loss: 4193.65
[INFO 2017-06-27 20:15:26,111 main.py:51] epoch 2050, training loss: 3438.78, average training loss: 3446.95, base loss: 4194.17
[INFO 2017-06-27 20:15:26,488 main.py:51] epoch 2051, training loss: 3248.31, average training loss: 3446.67, base loss: 4194.24
[INFO 2017-06-27 20:15:26,863 main.py:51] epoch 2052, training loss: 2867.48, average training loss: 3446.19, base loss: 4193.65
[INFO 2017-06-27 20:15:27,255 main.py:51] epoch 2053, training loss: 2816.66, average training loss: 3445.03, base loss: 4192.39
[INFO 2017-06-27 20:15:27,643 main.py:51] epoch 2054, training loss: 3334.72, average training loss: 3443.95, base loss: 4191.24
[INFO 2017-06-27 20:15:28,019 main.py:51] epoch 2055, training loss: 3178.16, average training loss: 3444.11, base loss: 4191.88
[INFO 2017-06-27 20:15:28,393 main.py:51] epoch 2056, training loss: 3911.81, average training loss: 3444.46, base loss: 4192.49
[INFO 2017-06-27 20:15:28,765 main.py:51] epoch 2057, training loss: 2814.21, average training loss: 3444.05, base loss: 4192.03
[INFO 2017-06-27 20:15:29,140 main.py:51] epoch 2058, training loss: 3568.11, average training loss: 3440.43, base loss: 4188.58
[INFO 2017-06-27 20:15:29,515 main.py:51] epoch 2059, training loss: 2969.26, average training loss: 3440.08, base loss: 4188.44
[INFO 2017-06-27 20:15:29,894 main.py:51] epoch 2060, training loss: 3555.33, average training loss: 3439.90, base loss: 4188.64
[INFO 2017-06-27 20:15:30,270 main.py:51] epoch 2061, training loss: 2877.05, average training loss: 3439.49, base loss: 4188.43
[INFO 2017-06-27 20:15:30,645 main.py:51] epoch 2062, training loss: 3211.67, average training loss: 3439.60, base loss: 4189.01
[INFO 2017-06-27 20:15:31,018 main.py:51] epoch 2063, training loss: 2927.65, average training loss: 3438.92, base loss: 4188.12
[INFO 2017-06-27 20:15:31,393 main.py:51] epoch 2064, training loss: 3666.09, average training loss: 3438.45, base loss: 4187.87
[INFO 2017-06-27 20:15:31,770 main.py:51] epoch 2065, training loss: 2950.26, average training loss: 3437.40, base loss: 4186.52
[INFO 2017-06-27 20:15:32,146 main.py:51] epoch 2066, training loss: 3664.06, average training loss: 3437.77, base loss: 4187.43
[INFO 2017-06-27 20:15:32,521 main.py:51] epoch 2067, training loss: 3316.69, average training loss: 3437.65, base loss: 4188.02
[INFO 2017-06-27 20:15:32,899 main.py:51] epoch 2068, training loss: 3431.79, average training loss: 3438.01, base loss: 4188.72
[INFO 2017-06-27 20:15:33,276 main.py:51] epoch 2069, training loss: 6351.12, average training loss: 3440.58, base loss: 4191.34
[INFO 2017-06-27 20:15:33,647 main.py:51] epoch 2070, training loss: 3446.63, average training loss: 3441.03, base loss: 4192.20
[INFO 2017-06-27 20:15:34,039 main.py:51] epoch 2071, training loss: 3525.52, average training loss: 3440.24, base loss: 4191.53
[INFO 2017-06-27 20:15:34,436 main.py:51] epoch 2072, training loss: 3003.01, average training loss: 3440.21, base loss: 4191.60
[INFO 2017-06-27 20:15:34,815 main.py:51] epoch 2073, training loss: 2988.61, average training loss: 3440.10, base loss: 4191.85
[INFO 2017-06-27 20:15:35,294 main.py:51] epoch 2074, training loss: 3279.97, average training loss: 3440.06, base loss: 4192.28
[INFO 2017-06-27 20:15:35,694 main.py:51] epoch 2075, training loss: 3274.94, average training loss: 3440.54, base loss: 4193.36
[INFO 2017-06-27 20:15:36,154 main.py:51] epoch 2076, training loss: 3295.05, average training loss: 3440.74, base loss: 4193.57
[INFO 2017-06-27 20:15:36,566 main.py:51] epoch 2077, training loss: 2653.35, average training loss: 3439.89, base loss: 4192.38
[INFO 2017-06-27 20:15:36,952 main.py:51] epoch 2078, training loss: 3171.42, average training loss: 3439.23, base loss: 4191.74
[INFO 2017-06-27 20:15:37,334 main.py:51] epoch 2079, training loss: 6796.25, average training loss: 3442.52, base loss: 4195.34
[INFO 2017-06-27 20:15:37,787 main.py:51] epoch 2080, training loss: 2898.54, average training loss: 3442.21, base loss: 4195.17
[INFO 2017-06-27 20:15:38,190 main.py:51] epoch 2081, training loss: 3144.32, average training loss: 3441.67, base loss: 4194.44
[INFO 2017-06-27 20:15:38,590 main.py:51] epoch 2082, training loss: 2816.92, average training loss: 3441.13, base loss: 4193.90
[INFO 2017-06-27 20:15:38,989 main.py:51] epoch 2083, training loss: 2992.66, average training loss: 3440.46, base loss: 4193.05
[INFO 2017-06-27 20:15:39,384 main.py:51] epoch 2084, training loss: 2824.58, average training loss: 3440.38, base loss: 4193.54
[INFO 2017-06-27 20:15:39,803 main.py:51] epoch 2085, training loss: 3079.70, average training loss: 3440.47, base loss: 4193.91
[INFO 2017-06-27 20:15:40,183 main.py:51] epoch 2086, training loss: 6539.15, average training loss: 3439.46, base loss: 4192.83
[INFO 2017-06-27 20:15:40,631 main.py:51] epoch 2087, training loss: 6654.53, average training loss: 3442.53, base loss: 4195.98
[INFO 2017-06-27 20:15:41,048 main.py:51] epoch 2088, training loss: 3315.95, average training loss: 3442.17, base loss: 4195.89
[INFO 2017-06-27 20:15:41,433 main.py:51] epoch 2089, training loss: 3292.93, average training loss: 3442.41, base loss: 4196.31
[INFO 2017-06-27 20:15:41,896 main.py:51] epoch 2090, training loss: 2475.16, average training loss: 3441.85, base loss: 4195.65
[INFO 2017-06-27 20:15:42,298 main.py:51] epoch 2091, training loss: 2492.93, average training loss: 3441.74, base loss: 4195.59
[INFO 2017-06-27 20:15:42,762 main.py:51] epoch 2092, training loss: 3271.46, average training loss: 3441.64, base loss: 4195.83
[INFO 2017-06-27 20:15:43,167 main.py:51] epoch 2093, training loss: 3757.59, average training loss: 3441.91, base loss: 4196.61
[INFO 2017-06-27 20:15:43,586 main.py:51] epoch 2094, training loss: 3293.03, average training loss: 3441.83, base loss: 4196.61
[INFO 2017-06-27 20:15:43,999 main.py:51] epoch 2095, training loss: 2856.78, average training loss: 3441.48, base loss: 4196.35
[INFO 2017-06-27 20:15:44,386 main.py:51] epoch 2096, training loss: 3411.83, average training loss: 3441.02, base loss: 4195.96
[INFO 2017-06-27 20:15:44,816 main.py:51] epoch 2097, training loss: 7148.22, average training loss: 3444.81, base loss: 4200.48
[INFO 2017-06-27 20:15:45,213 main.py:51] epoch 2098, training loss: 3155.11, average training loss: 3444.66, base loss: 4200.43
[INFO 2017-06-27 20:15:45,630 main.py:51] epoch 2099, training loss: 2913.46, average training loss: 3443.99, base loss: 4199.93
[INFO 2017-06-27 20:15:45,630 main.py:53] epoch 2099, testing
[INFO 2017-06-27 20:15:47,295 main.py:105] average testing loss: 3404.26, base loss: 4175.41
[INFO 2017-06-27 20:15:47,295 main.py:106] improve_loss: 771.15, improve_percent: 0.18
[INFO 2017-06-27 20:15:47,296 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 20:15:47,673 main.py:51] epoch 2100, training loss: 2810.39, average training loss: 3443.30, base loss: 4199.17
[INFO 2017-06-27 20:15:48,094 main.py:51] epoch 2101, training loss: 6960.18, average training loss: 3447.39, base loss: 4203.89
[INFO 2017-06-27 20:15:48,471 main.py:51] epoch 2102, training loss: 3356.22, average training loss: 3447.03, base loss: 4203.48
[INFO 2017-06-27 20:15:48,850 main.py:51] epoch 2103, training loss: 3292.16, average training loss: 3447.33, base loss: 4204.15
[INFO 2017-06-27 20:15:49,225 main.py:51] epoch 2104, training loss: 3583.54, average training loss: 3446.61, base loss: 4203.61
[INFO 2017-06-27 20:15:49,600 main.py:51] epoch 2105, training loss: 3135.75, average training loss: 3446.42, base loss: 4203.44
[INFO 2017-06-27 20:15:49,975 main.py:51] epoch 2106, training loss: 3181.66, average training loss: 3446.33, base loss: 4203.48
[INFO 2017-06-27 20:15:50,355 main.py:51] epoch 2107, training loss: 3363.39, average training loss: 3446.28, base loss: 4203.79
[INFO 2017-06-27 20:15:50,732 main.py:51] epoch 2108, training loss: 3549.16, average training loss: 3446.66, base loss: 4204.57
[INFO 2017-06-27 20:15:51,107 main.py:51] epoch 2109, training loss: 3046.90, average training loss: 3446.30, base loss: 4204.03
[INFO 2017-06-27 20:15:51,486 main.py:51] epoch 2110, training loss: 3308.60, average training loss: 3446.58, base loss: 4204.74
[INFO 2017-06-27 20:15:51,865 main.py:51] epoch 2111, training loss: 2915.76, average training loss: 3446.23, base loss: 4204.74
[INFO 2017-06-27 20:15:52,241 main.py:51] epoch 2112, training loss: 2928.18, average training loss: 3445.79, base loss: 4204.32
[INFO 2017-06-27 20:15:52,617 main.py:51] epoch 2113, training loss: 3473.58, average training loss: 3445.66, base loss: 4204.55
[INFO 2017-06-27 20:15:52,992 main.py:51] epoch 2114, training loss: 3084.74, average training loss: 3445.15, base loss: 4204.00
[INFO 2017-06-27 20:15:53,371 main.py:51] epoch 2115, training loss: 2800.17, average training loss: 3445.07, base loss: 4204.12
[INFO 2017-06-27 20:15:53,750 main.py:51] epoch 2116, training loss: 3014.58, average training loss: 3445.16, base loss: 4204.75
[INFO 2017-06-27 20:15:54,125 main.py:51] epoch 2117, training loss: 3878.39, average training loss: 3445.43, base loss: 4205.67
[INFO 2017-06-27 20:15:54,501 main.py:51] epoch 2118, training loss: 3223.40, average training loss: 3444.99, base loss: 4205.36
[INFO 2017-06-27 20:15:54,876 main.py:51] epoch 2119, training loss: 3044.53, average training loss: 3444.46, base loss: 4204.46
[INFO 2017-06-27 20:15:55,251 main.py:51] epoch 2120, training loss: 3467.54, average training loss: 3444.92, base loss: 4205.72
[INFO 2017-06-27 20:15:55,627 main.py:51] epoch 2121, training loss: 3417.08, average training loss: 3445.08, base loss: 4206.54
[INFO 2017-06-27 20:15:56,003 main.py:51] epoch 2122, training loss: 6185.39, average training loss: 3448.26, base loss: 4210.02
[INFO 2017-06-27 20:15:56,384 main.py:51] epoch 2123, training loss: 3385.85, average training loss: 3447.36, base loss: 4208.97
[INFO 2017-06-27 20:15:56,760 main.py:51] epoch 2124, training loss: 3057.22, average training loss: 3446.90, base loss: 4208.61
[INFO 2017-06-27 20:15:57,140 main.py:51] epoch 2125, training loss: 2878.46, average training loss: 3446.21, base loss: 4207.71
[INFO 2017-06-27 20:15:57,515 main.py:51] epoch 2126, training loss: 3016.02, average training loss: 3445.69, base loss: 4207.18
[INFO 2017-06-27 20:15:57,890 main.py:51] epoch 2127, training loss: 3034.75, average training loss: 3445.74, base loss: 4207.55
[INFO 2017-06-27 20:15:58,270 main.py:51] epoch 2128, training loss: 2947.29, average training loss: 3445.34, base loss: 4206.89
[INFO 2017-06-27 20:15:58,652 main.py:51] epoch 2129, training loss: 3355.81, average training loss: 3445.33, base loss: 4206.92
[INFO 2017-06-27 20:15:59,034 main.py:51] epoch 2130, training loss: 2995.10, average training loss: 3444.81, base loss: 4206.60
[INFO 2017-06-27 20:15:59,414 main.py:51] epoch 2131, training loss: 3172.79, average training loss: 3444.64, base loss: 4206.62
[INFO 2017-06-27 20:15:59,790 main.py:51] epoch 2132, training loss: 2933.60, average training loss: 3443.58, base loss: 4205.24
[INFO 2017-06-27 20:16:00,166 main.py:51] epoch 2133, training loss: 3027.46, average training loss: 3442.37, base loss: 4203.72
[INFO 2017-06-27 20:16:00,543 main.py:51] epoch 2134, training loss: 3211.40, average training loss: 3442.26, base loss: 4203.96
[INFO 2017-06-27 20:16:00,921 main.py:51] epoch 2135, training loss: 3041.95, average training loss: 3442.02, base loss: 4203.83
[INFO 2017-06-27 20:16:01,300 main.py:51] epoch 2136, training loss: 3250.06, average training loss: 3441.92, base loss: 4203.67
[INFO 2017-06-27 20:16:01,679 main.py:51] epoch 2137, training loss: 3459.91, average training loss: 3441.58, base loss: 4203.73
[INFO 2017-06-27 20:16:02,062 main.py:51] epoch 2138, training loss: 2958.64, average training loss: 3441.58, base loss: 4203.73
[INFO 2017-06-27 20:16:02,439 main.py:51] epoch 2139, training loss: 3154.54, average training loss: 3441.67, base loss: 4203.89
[INFO 2017-06-27 20:16:02,824 main.py:51] epoch 2140, training loss: 3505.13, average training loss: 3441.27, base loss: 4203.92
[INFO 2017-06-27 20:16:03,200 main.py:51] epoch 2141, training loss: 3077.44, average training loss: 3440.88, base loss: 4203.54
[INFO 2017-06-27 20:16:03,575 main.py:51] epoch 2142, training loss: 3081.58, average training loss: 3440.70, base loss: 4203.59
[INFO 2017-06-27 20:16:03,957 main.py:51] epoch 2143, training loss: 3074.68, average training loss: 3440.71, base loss: 4203.66
[INFO 2017-06-27 20:16:04,331 main.py:51] epoch 2144, training loss: 3290.93, average training loss: 3440.94, base loss: 4204.44
[INFO 2017-06-27 20:16:04,707 main.py:51] epoch 2145, training loss: 3269.71, average training loss: 3440.36, base loss: 4203.82
[INFO 2017-06-27 20:16:05,088 main.py:51] epoch 2146, training loss: 2927.55, average training loss: 3439.66, base loss: 4203.31
[INFO 2017-06-27 20:16:05,464 main.py:51] epoch 2147, training loss: 3221.88, average training loss: 3439.61, base loss: 4203.48
[INFO 2017-06-27 20:16:05,839 main.py:51] epoch 2148, training loss: 2768.55, average training loss: 3438.87, base loss: 4202.57
[INFO 2017-06-27 20:16:06,222 main.py:51] epoch 2149, training loss: 3567.68, average training loss: 3438.96, base loss: 4203.18
[INFO 2017-06-27 20:16:06,598 main.py:51] epoch 2150, training loss: 3016.65, average training loss: 3438.56, base loss: 4202.91
[INFO 2017-06-27 20:16:06,977 main.py:51] epoch 2151, training loss: 3291.83, average training loss: 3438.16, base loss: 4202.58
[INFO 2017-06-27 20:16:07,359 main.py:51] epoch 2152, training loss: 3631.62, average training loss: 3438.15, base loss: 4203.10
[INFO 2017-06-27 20:16:07,741 main.py:51] epoch 2153, training loss: 3411.12, average training loss: 3438.38, base loss: 4203.60
[INFO 2017-06-27 20:16:08,122 main.py:51] epoch 2154, training loss: 3089.15, average training loss: 3438.20, base loss: 4203.64
[INFO 2017-06-27 20:16:08,498 main.py:51] epoch 2155, training loss: 3586.14, average training loss: 3438.00, base loss: 4203.88
[INFO 2017-06-27 20:16:08,877 main.py:51] epoch 2156, training loss: 3703.41, average training loss: 3438.62, base loss: 4205.21
[INFO 2017-06-27 20:16:09,258 main.py:51] epoch 2157, training loss: 2911.75, average training loss: 3438.11, base loss: 4204.74
[INFO 2017-06-27 20:16:09,634 main.py:51] epoch 2158, training loss: 3151.25, average training loss: 3437.32, base loss: 4203.72
[INFO 2017-06-27 20:16:10,006 main.py:51] epoch 2159, training loss: 3136.81, average training loss: 3437.16, base loss: 4203.60
[INFO 2017-06-27 20:16:10,385 main.py:51] epoch 2160, training loss: 3123.24, average training loss: 3437.12, base loss: 4203.71
[INFO 2017-06-27 20:16:10,762 main.py:51] epoch 2161, training loss: 3744.88, average training loss: 3438.08, base loss: 4205.25
[INFO 2017-06-27 20:16:11,134 main.py:51] epoch 2162, training loss: 2955.63, average training loss: 3437.77, base loss: 4204.87
[INFO 2017-06-27 20:16:11,507 main.py:51] epoch 2163, training loss: 3397.94, average training loss: 3437.32, base loss: 4204.41
[INFO 2017-06-27 20:16:11,883 main.py:51] epoch 2164, training loss: 3596.51, average training loss: 3438.08, base loss: 4205.97
[INFO 2017-06-27 20:16:12,255 main.py:51] epoch 2165, training loss: 3266.51, average training loss: 3438.11, base loss: 4206.28
[INFO 2017-06-27 20:16:12,626 main.py:51] epoch 2166, training loss: 3361.60, average training loss: 3438.34, base loss: 4207.07
[INFO 2017-06-27 20:16:12,998 main.py:51] epoch 2167, training loss: 3310.95, average training loss: 3438.17, base loss: 4207.40
[INFO 2017-06-27 20:16:13,369 main.py:51] epoch 2168, training loss: 3542.32, average training loss: 3438.58, base loss: 4208.27
[INFO 2017-06-27 20:16:13,742 main.py:51] epoch 2169, training loss: 2694.95, average training loss: 3437.68, base loss: 4206.87
[INFO 2017-06-27 20:16:14,115 main.py:51] epoch 2170, training loss: 3338.98, average training loss: 3437.04, base loss: 4206.60
[INFO 2017-06-27 20:16:14,491 main.py:51] epoch 2171, training loss: 3037.58, average training loss: 3436.43, base loss: 4205.95
[INFO 2017-06-27 20:16:14,863 main.py:51] epoch 2172, training loss: 3117.47, average training loss: 3436.49, base loss: 4206.66
[INFO 2017-06-27 20:16:15,237 main.py:51] epoch 2173, training loss: 2873.16, average training loss: 3436.40, base loss: 4206.55
[INFO 2017-06-27 20:16:15,613 main.py:51] epoch 2174, training loss: 3175.74, average training loss: 3435.62, base loss: 4205.78
[INFO 2017-06-27 20:16:15,986 main.py:51] epoch 2175, training loss: 2994.88, average training loss: 3435.24, base loss: 4205.78
[INFO 2017-06-27 20:16:16,363 main.py:51] epoch 2176, training loss: 2705.61, average training loss: 3434.46, base loss: 4204.93
[INFO 2017-06-27 20:16:16,738 main.py:51] epoch 2177, training loss: 3632.84, average training loss: 3434.88, base loss: 4205.81
[INFO 2017-06-27 20:16:17,113 main.py:51] epoch 2178, training loss: 3138.48, average training loss: 3434.53, base loss: 4205.48
[INFO 2017-06-27 20:16:17,485 main.py:51] epoch 2179, training loss: 3186.13, average training loss: 3434.46, base loss: 4205.91
[INFO 2017-06-27 20:16:17,863 main.py:51] epoch 2180, training loss: 3189.55, average training loss: 3434.86, base loss: 4206.77
[INFO 2017-06-27 20:16:18,237 main.py:51] epoch 2181, training loss: 3263.74, average training loss: 3434.45, base loss: 4206.46
[INFO 2017-06-27 20:16:18,610 main.py:51] epoch 2182, training loss: 3309.14, average training loss: 3434.51, base loss: 4206.58
[INFO 2017-06-27 20:16:18,985 main.py:51] epoch 2183, training loss: 2975.10, average training loss: 3434.59, base loss: 4207.11
[INFO 2017-06-27 20:16:19,357 main.py:51] epoch 2184, training loss: 3260.04, average training loss: 3433.88, base loss: 4206.55
[INFO 2017-06-27 20:16:19,731 main.py:51] epoch 2185, training loss: 3541.14, average training loss: 3433.94, base loss: 4207.03
[INFO 2017-06-27 20:16:20,106 main.py:51] epoch 2186, training loss: 3187.84, average training loss: 3433.47, base loss: 4206.79
[INFO 2017-06-27 20:16:20,483 main.py:51] epoch 2187, training loss: 2897.50, average training loss: 3433.23, base loss: 4206.49
[INFO 2017-06-27 20:16:20,857 main.py:51] epoch 2188, training loss: 2937.43, average training loss: 3432.22, base loss: 4205.17
[INFO 2017-06-27 20:16:21,232 main.py:51] epoch 2189, training loss: 3091.83, average training loss: 3428.27, base loss: 4201.51
[INFO 2017-06-27 20:16:21,603 main.py:51] epoch 2190, training loss: 3043.49, average training loss: 3427.97, base loss: 4201.26
[INFO 2017-06-27 20:16:21,973 main.py:51] epoch 2191, training loss: 3172.80, average training loss: 3428.20, base loss: 4201.83
[INFO 2017-06-27 20:16:22,351 main.py:51] epoch 2192, training loss: 3126.95, average training loss: 3428.05, base loss: 4202.03
[INFO 2017-06-27 20:16:22,726 main.py:51] epoch 2193, training loss: 3789.64, average training loss: 3428.46, base loss: 4203.04
[INFO 2017-06-27 20:16:23,100 main.py:51] epoch 2194, training loss: 3274.58, average training loss: 3428.60, base loss: 4203.50
[INFO 2017-06-27 20:16:23,478 main.py:51] epoch 2195, training loss: 3435.26, average training loss: 3427.71, base loss: 4202.31
[INFO 2017-06-27 20:16:23,852 main.py:51] epoch 2196, training loss: 2815.69, average training loss: 3427.16, base loss: 4201.58
[INFO 2017-06-27 20:16:24,230 main.py:51] epoch 2197, training loss: 2942.48, average training loss: 3427.18, base loss: 4201.80
[INFO 2017-06-27 20:16:24,604 main.py:51] epoch 2198, training loss: 3226.52, average training loss: 3427.03, base loss: 4201.85
[INFO 2017-06-27 20:16:24,981 main.py:51] epoch 2199, training loss: 3328.09, average training loss: 3426.76, base loss: 4201.53
[INFO 2017-06-27 20:16:24,981 main.py:53] epoch 2199, testing
[INFO 2017-06-27 20:16:26,594 main.py:105] average testing loss: 3550.14, base loss: 4446.63
[INFO 2017-06-27 20:16:26,595 main.py:106] improve_loss: 896.49, improve_percent: 0.20
[INFO 2017-06-27 20:16:26,595 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 20:16:26,966 main.py:51] epoch 2200, training loss: 3151.60, average training loss: 3426.47, base loss: 4201.49
[INFO 2017-06-27 20:16:27,345 main.py:51] epoch 2201, training loss: 7033.55, average training loss: 3430.65, base loss: 4206.47
[INFO 2017-06-27 20:16:27,715 main.py:51] epoch 2202, training loss: 3467.42, average training loss: 3430.89, base loss: 4207.08
[INFO 2017-06-27 20:16:28,088 main.py:51] epoch 2203, training loss: 3015.77, average training loss: 3430.51, base loss: 4206.55
[INFO 2017-06-27 20:16:28,462 main.py:51] epoch 2204, training loss: 2860.34, average training loss: 3429.61, base loss: 4205.68
[INFO 2017-06-27 20:16:28,834 main.py:51] epoch 2205, training loss: 6912.78, average training loss: 3432.35, base loss: 4208.30
[INFO 2017-06-27 20:16:29,206 main.py:51] epoch 2206, training loss: 3252.13, average training loss: 3432.44, base loss: 4208.35
[INFO 2017-06-27 20:16:29,581 main.py:51] epoch 2207, training loss: 3175.53, average training loss: 3432.46, base loss: 4208.79
[INFO 2017-06-27 20:16:29,958 main.py:51] epoch 2208, training loss: 2726.91, average training loss: 3431.90, base loss: 4207.93
[INFO 2017-06-27 20:16:30,336 main.py:51] epoch 2209, training loss: 3116.24, average training loss: 3431.71, base loss: 4208.08
[INFO 2017-06-27 20:16:30,714 main.py:51] epoch 2210, training loss: 2794.20, average training loss: 3431.33, base loss: 4207.89
[INFO 2017-06-27 20:16:31,089 main.py:51] epoch 2211, training loss: 5697.23, average training loss: 3433.49, base loss: 4210.17
[INFO 2017-06-27 20:16:31,464 main.py:51] epoch 2212, training loss: 3864.36, average training loss: 3434.26, base loss: 4211.33
[INFO 2017-06-27 20:16:31,844 main.py:51] epoch 2213, training loss: 3825.36, average training loss: 3434.71, base loss: 4212.28
[INFO 2017-06-27 20:16:32,219 main.py:51] epoch 2214, training loss: 2994.70, average training loss: 3434.41, base loss: 4212.18
[INFO 2017-06-27 20:16:32,594 main.py:51] epoch 2215, training loss: 4027.02, average training loss: 3435.28, base loss: 4214.07
[INFO 2017-06-27 20:16:32,975 main.py:51] epoch 2216, training loss: 3244.06, average training loss: 3435.09, base loss: 4214.21
[INFO 2017-06-27 20:16:33,353 main.py:51] epoch 2217, training loss: 2831.26, average training loss: 3435.00, base loss: 4214.52
[INFO 2017-06-27 20:16:33,727 main.py:51] epoch 2218, training loss: 3057.34, average training loss: 3434.26, base loss: 4213.82
[INFO 2017-06-27 20:16:34,113 main.py:51] epoch 2219, training loss: 2810.77, average training loss: 3433.24, base loss: 4212.42
[INFO 2017-06-27 20:16:34,487 main.py:51] epoch 2220, training loss: 3465.12, average training loss: 3433.04, base loss: 4212.12
[INFO 2017-06-27 20:16:34,868 main.py:51] epoch 2221, training loss: 3350.55, average training loss: 3433.16, base loss: 4212.51
[INFO 2017-06-27 20:16:35,243 main.py:51] epoch 2222, training loss: 2885.35, average training loss: 3432.80, base loss: 4212.26
[INFO 2017-06-27 20:16:35,619 main.py:51] epoch 2223, training loss: 3344.29, average training loss: 3433.06, base loss: 4213.15
[INFO 2017-06-27 20:16:35,993 main.py:51] epoch 2224, training loss: 2840.99, average training loss: 3428.70, base loss: 4208.79
[INFO 2017-06-27 20:16:36,370 main.py:51] epoch 2225, training loss: 3860.12, average training loss: 3429.25, base loss: 4209.71
[INFO 2017-06-27 20:16:36,745 main.py:51] epoch 2226, training loss: 3269.43, average training loss: 3429.11, base loss: 4210.05
[INFO 2017-06-27 20:16:37,121 main.py:51] epoch 2227, training loss: 2887.93, average training loss: 3427.98, base loss: 4208.81
[INFO 2017-06-27 20:16:37,500 main.py:51] epoch 2228, training loss: 3233.77, average training loss: 3427.53, base loss: 4208.54
[INFO 2017-06-27 20:16:37,871 main.py:51] epoch 2229, training loss: 3324.08, average training loss: 3427.11, base loss: 4208.12
[INFO 2017-06-27 20:16:38,276 main.py:51] epoch 2230, training loss: 3053.96, average training loss: 3426.98, base loss: 4208.36
[INFO 2017-06-27 20:16:38,672 main.py:51] epoch 2231, training loss: 3835.31, average training loss: 3427.12, base loss: 4208.62
[INFO 2017-06-27 20:16:39,048 main.py:51] epoch 2232, training loss: 3372.72, average training loss: 3427.59, base loss: 4209.04
[INFO 2017-06-27 20:16:39,436 main.py:51] epoch 2233, training loss: 3174.73, average training loss: 3423.93, base loss: 4205.40
[INFO 2017-06-27 20:16:39,817 main.py:51] epoch 2234, training loss: 3033.71, average training loss: 3423.97, base loss: 4205.48
[INFO 2017-06-27 20:16:40,203 main.py:51] epoch 2235, training loss: 3649.73, average training loss: 3424.16, base loss: 4206.31
[INFO 2017-06-27 20:16:40,586 main.py:51] epoch 2236, training loss: 3264.84, average training loss: 3424.22, base loss: 4206.67
[INFO 2017-06-27 20:16:40,963 main.py:51] epoch 2237, training loss: 3443.20, average training loss: 3424.18, base loss: 4206.83
[INFO 2017-06-27 20:16:41,335 main.py:51] epoch 2238, training loss: 3434.38, average training loss: 3423.78, base loss: 4206.43
[INFO 2017-06-27 20:16:41,707 main.py:51] epoch 2239, training loss: 3578.47, average training loss: 3423.65, base loss: 4206.53
[INFO 2017-06-27 20:16:42,083 main.py:51] epoch 2240, training loss: 3966.28, average training loss: 3423.85, base loss: 4206.90
[INFO 2017-06-27 20:16:42,454 main.py:51] epoch 2241, training loss: 3038.99, average training loss: 3423.73, base loss: 4206.99
[INFO 2017-06-27 20:16:42,824 main.py:51] epoch 2242, training loss: 3504.20, average training loss: 3423.75, base loss: 4207.44
[INFO 2017-06-27 20:16:43,195 main.py:51] epoch 2243, training loss: 6824.12, average training loss: 3427.18, base loss: 4210.80
[INFO 2017-06-27 20:16:43,571 main.py:51] epoch 2244, training loss: 3445.98, average training loss: 3426.90, base loss: 4211.07
[INFO 2017-06-27 20:16:43,946 main.py:51] epoch 2245, training loss: 7090.80, average training loss: 3430.16, base loss: 4214.55
[INFO 2017-06-27 20:16:44,321 main.py:51] epoch 2246, training loss: 3523.45, average training loss: 3429.99, base loss: 4214.28
[INFO 2017-06-27 20:16:44,696 main.py:51] epoch 2247, training loss: 3299.84, average training loss: 3430.11, base loss: 4214.86
[INFO 2017-06-27 20:16:45,072 main.py:51] epoch 2248, training loss: 3218.39, average training loss: 3429.51, base loss: 4214.54
[INFO 2017-06-27 20:16:45,446 main.py:51] epoch 2249, training loss: 3353.54, average training loss: 3429.52, base loss: 4214.54
[INFO 2017-06-27 20:16:45,889 main.py:51] epoch 2250, training loss: 3332.58, average training loss: 3428.86, base loss: 4213.76
[INFO 2017-06-27 20:16:46,291 main.py:51] epoch 2251, training loss: 2989.08, average training loss: 3428.31, base loss: 4213.29
[INFO 2017-06-27 20:16:46,674 main.py:51] epoch 2252, training loss: 3245.19, average training loss: 3427.95, base loss: 4213.04
[INFO 2017-06-27 20:16:47,053 main.py:51] epoch 2253, training loss: 3280.63, average training loss: 3428.02, base loss: 4213.43
[INFO 2017-06-27 20:16:47,465 main.py:51] epoch 2254, training loss: 3673.67, average training loss: 3428.68, base loss: 4214.70
[INFO 2017-06-27 20:16:47,864 main.py:51] epoch 2255, training loss: 3247.91, average training loss: 3429.19, base loss: 4215.69
[INFO 2017-06-27 20:16:48,244 main.py:51] epoch 2256, training loss: 3308.10, average training loss: 3428.52, base loss: 4214.97
[INFO 2017-06-27 20:16:48,668 main.py:51] epoch 2257, training loss: 3333.08, average training loss: 3428.61, base loss: 4215.47
[INFO 2017-06-27 20:16:49,080 main.py:51] epoch 2258, training loss: 3135.96, average training loss: 3428.21, base loss: 4215.25
[INFO 2017-06-27 20:16:49,459 main.py:51] epoch 2259, training loss: 3119.08, average training loss: 3428.09, base loss: 4215.10
[INFO 2017-06-27 20:16:49,837 main.py:51] epoch 2260, training loss: 3277.94, average training loss: 3428.00, base loss: 4215.04
[INFO 2017-06-27 20:16:50,219 main.py:51] epoch 2261, training loss: 3025.75, average training loss: 3428.05, base loss: 4215.15
[INFO 2017-06-27 20:16:50,616 main.py:51] epoch 2262, training loss: 3774.84, average training loss: 3428.57, base loss: 4216.10
[INFO 2017-06-27 20:16:50,991 main.py:51] epoch 2263, training loss: 3086.70, average training loss: 3428.47, base loss: 4215.85
[INFO 2017-06-27 20:16:51,366 main.py:51] epoch 2264, training loss: 3212.09, average training loss: 3428.72, base loss: 4216.41
[INFO 2017-06-27 20:16:51,741 main.py:51] epoch 2265, training loss: 3346.41, average training loss: 3428.53, base loss: 4216.59
[INFO 2017-06-27 20:16:52,122 main.py:51] epoch 2266, training loss: 3622.82, average training loss: 3428.84, base loss: 4217.37
[INFO 2017-06-27 20:16:52,496 main.py:51] epoch 2267, training loss: 3520.71, average training loss: 3429.04, base loss: 4217.61
[INFO 2017-06-27 20:16:52,868 main.py:51] epoch 2268, training loss: 2929.68, average training loss: 3428.64, base loss: 4217.32
[INFO 2017-06-27 20:16:53,242 main.py:51] epoch 2269, training loss: 2942.64, average training loss: 3428.02, base loss: 4216.78
[INFO 2017-06-27 20:16:53,614 main.py:51] epoch 2270, training loss: 2825.19, average training loss: 3427.11, base loss: 4215.65
[INFO 2017-06-27 20:16:53,986 main.py:51] epoch 2271, training loss: 3134.21, average training loss: 3426.84, base loss: 4215.73
[INFO 2017-06-27 20:16:54,373 main.py:51] epoch 2272, training loss: 4008.02, average training loss: 3427.21, base loss: 4216.94
[INFO 2017-06-27 20:16:54,749 main.py:51] epoch 2273, training loss: 3994.50, average training loss: 3427.99, base loss: 4218.28
[INFO 2017-06-27 20:16:55,145 main.py:51] epoch 2274, training loss: 2809.06, average training loss: 3421.20, base loss: 4211.83
[INFO 2017-06-27 20:16:55,549 main.py:51] epoch 2275, training loss: 3175.59, average training loss: 3420.96, base loss: 4211.24
[INFO 2017-06-27 20:16:55,937 main.py:51] epoch 2276, training loss: 3355.63, average training loss: 3421.08, base loss: 4211.60
[INFO 2017-06-27 20:16:56,329 main.py:51] epoch 2277, training loss: 2848.13, average training loss: 3416.88, base loss: 4207.31
[INFO 2017-06-27 20:16:56,794 main.py:51] epoch 2278, training loss: 3353.32, average training loss: 3413.64, base loss: 4204.36
[INFO 2017-06-27 20:16:57,187 main.py:51] epoch 2279, training loss: 3078.66, average training loss: 3413.13, base loss: 4203.79
[INFO 2017-06-27 20:16:57,574 main.py:51] epoch 2280, training loss: 3912.98, average training loss: 3413.42, base loss: 4204.31
[INFO 2017-06-27 20:16:57,951 main.py:51] epoch 2281, training loss: 3477.86, average training loss: 3413.97, base loss: 4205.24
[INFO 2017-06-27 20:16:58,339 main.py:51] epoch 2282, training loss: 2748.91, average training loss: 3413.46, base loss: 4204.68
[INFO 2017-06-27 20:16:58,724 main.py:51] epoch 2283, training loss: 3370.57, average training loss: 3412.53, base loss: 4203.32
[INFO 2017-06-27 20:16:59,097 main.py:51] epoch 2284, training loss: 3107.42, average training loss: 3412.06, base loss: 4202.56
[INFO 2017-06-27 20:16:59,468 main.py:51] epoch 2285, training loss: 3142.34, average training loss: 3407.93, base loss: 4198.31
[INFO 2017-06-27 20:16:59,845 main.py:51] epoch 2286, training loss: 3029.15, average training loss: 3407.31, base loss: 4197.48
[INFO 2017-06-27 20:17:00,226 main.py:51] epoch 2287, training loss: 3516.76, average training loss: 3408.02, base loss: 4198.58
[INFO 2017-06-27 20:17:00,602 main.py:51] epoch 2288, training loss: 2954.51, average training loss: 3407.88, base loss: 4198.71
[INFO 2017-06-27 20:17:00,978 main.py:51] epoch 2289, training loss: 2949.06, average training loss: 3407.07, base loss: 4197.35
[INFO 2017-06-27 20:17:01,362 main.py:51] epoch 2290, training loss: 3460.11, average training loss: 3406.77, base loss: 4197.47
[INFO 2017-06-27 20:17:01,741 main.py:51] epoch 2291, training loss: 3401.39, average training loss: 3406.68, base loss: 4197.42
[INFO 2017-06-27 20:17:02,116 main.py:51] epoch 2292, training loss: 3706.03, average training loss: 3407.39, base loss: 4198.83
[INFO 2017-06-27 20:17:02,491 main.py:51] epoch 2293, training loss: 2671.50, average training loss: 3406.28, base loss: 4197.17
[INFO 2017-06-27 20:17:02,866 main.py:51] epoch 2294, training loss: 3432.78, average training loss: 3406.60, base loss: 4197.67
[INFO 2017-06-27 20:17:03,243 main.py:51] epoch 2295, training loss: 3364.64, average training loss: 3406.52, base loss: 4198.04
[INFO 2017-06-27 20:17:03,619 main.py:51] epoch 2296, training loss: 3154.96, average training loss: 3406.16, base loss: 4197.42
[INFO 2017-06-27 20:17:03,993 main.py:51] epoch 2297, training loss: 3083.77, average training loss: 3405.98, base loss: 4197.43
[INFO 2017-06-27 20:17:04,373 main.py:51] epoch 2298, training loss: 2842.57, average training loss: 3405.33, base loss: 4196.64
[INFO 2017-06-27 20:17:04,747 main.py:51] epoch 2299, training loss: 3137.16, average training loss: 3404.60, base loss: 4195.94
[INFO 2017-06-27 20:17:04,747 main.py:53] epoch 2299, testing
[INFO 2017-06-27 20:17:06,356 main.py:105] average testing loss: 3748.88, base loss: 4527.50
[INFO 2017-06-27 20:17:06,356 main.py:106] improve_loss: 778.62, improve_percent: 0.17
[INFO 2017-06-27 20:17:06,356 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 20:17:06,732 main.py:51] epoch 2300, training loss: 3004.78, average training loss: 3404.38, base loss: 4196.08
[INFO 2017-06-27 20:17:07,110 main.py:51] epoch 2301, training loss: 5939.12, average training loss: 3406.69, base loss: 4198.22
[INFO 2017-06-27 20:17:07,485 main.py:51] epoch 2302, training loss: 3296.13, average training loss: 3406.89, base loss: 4198.66
[INFO 2017-06-27 20:17:07,861 main.py:51] epoch 2303, training loss: 3348.20, average training loss: 3407.07, base loss: 4199.02
[INFO 2017-06-27 20:17:08,242 main.py:51] epoch 2304, training loss: 3463.24, average training loss: 3406.33, base loss: 4197.68
[INFO 2017-06-27 20:17:08,617 main.py:51] epoch 2305, training loss: 6776.18, average training loss: 3409.60, base loss: 4201.03
[INFO 2017-06-27 20:17:08,987 main.py:51] epoch 2306, training loss: 3208.37, average training loss: 3409.58, base loss: 4201.31
[INFO 2017-06-27 20:17:09,400 main.py:51] epoch 2307, training loss: 2877.96, average training loss: 3409.20, base loss: 4201.16
[INFO 2017-06-27 20:17:09,813 main.py:51] epoch 2308, training loss: 3167.24, average training loss: 3409.14, base loss: 4201.11
[INFO 2017-06-27 20:17:10,196 main.py:51] epoch 2309, training loss: 3299.26, average training loss: 3409.21, base loss: 4201.35
[INFO 2017-06-27 20:17:10,632 main.py:51] epoch 2310, training loss: 3815.49, average training loss: 3409.65, base loss: 4202.31
[INFO 2017-06-27 20:17:11,016 main.py:51] epoch 2311, training loss: 3224.64, average training loss: 3409.79, base loss: 4202.87
[INFO 2017-06-27 20:17:11,466 main.py:51] epoch 2312, training loss: 3305.80, average training loss: 3410.25, base loss: 4203.80
[INFO 2017-06-27 20:17:11,884 main.py:51] epoch 2313, training loss: 3582.41, average training loss: 3410.51, base loss: 4204.77
[INFO 2017-06-27 20:17:12,332 main.py:51] epoch 2314, training loss: 3006.07, average training loss: 3410.30, base loss: 4204.73
[INFO 2017-06-27 20:17:12,772 main.py:51] epoch 2315, training loss: 2939.35, average training loss: 3409.97, base loss: 4204.42
[INFO 2017-06-27 20:17:13,151 main.py:51] epoch 2316, training loss: 3447.94, average training loss: 3410.10, base loss: 4204.99
[INFO 2017-06-27 20:17:13,537 main.py:51] epoch 2317, training loss: 3443.48, average training loss: 3410.20, base loss: 4205.41
[INFO 2017-06-27 20:17:13,914 main.py:51] epoch 2318, training loss: 2925.25, average training loss: 3406.12, base loss: 4201.66
[INFO 2017-06-27 20:17:14,289 main.py:51] epoch 2319, training loss: 3569.05, average training loss: 3405.91, base loss: 4201.40
[INFO 2017-06-27 20:17:14,683 main.py:51] epoch 2320, training loss: 3071.22, average training loss: 3405.77, base loss: 4201.46
[INFO 2017-06-27 20:17:15,057 main.py:51] epoch 2321, training loss: 3214.70, average training loss: 3405.40, base loss: 4201.29
[INFO 2017-06-27 20:17:15,432 main.py:51] epoch 2322, training loss: 3235.65, average training loss: 3405.47, base loss: 4201.29
[INFO 2017-06-27 20:17:15,812 main.py:51] epoch 2323, training loss: 3229.91, average training loss: 3405.22, base loss: 4201.14
[INFO 2017-06-27 20:17:16,195 main.py:51] epoch 2324, training loss: 3434.58, average training loss: 3405.36, base loss: 4201.45
[INFO 2017-06-27 20:17:16,567 main.py:51] epoch 2325, training loss: 3039.51, average training loss: 3405.51, base loss: 4202.17
[INFO 2017-06-27 20:17:16,942 main.py:51] epoch 2326, training loss: 3903.82, average training loss: 3406.46, base loss: 4203.57
[INFO 2017-06-27 20:17:17,319 main.py:51] epoch 2327, training loss: 3490.33, average training loss: 3406.61, base loss: 4204.22
[INFO 2017-06-27 20:17:17,694 main.py:51] epoch 2328, training loss: 5801.53, average training loss: 3409.12, base loss: 4206.66
[INFO 2017-06-27 20:17:18,141 main.py:51] epoch 2329, training loss: 3286.52, average training loss: 3409.08, base loss: 4207.04
[INFO 2017-06-27 20:17:18,518 main.py:51] epoch 2330, training loss: 3068.77, average training loss: 3408.97, base loss: 4207.33
[INFO 2017-06-27 20:17:19,000 main.py:51] epoch 2331, training loss: 3027.37, average training loss: 3408.57, base loss: 4206.98
[INFO 2017-06-27 20:17:19,393 main.py:51] epoch 2332, training loss: 5850.87, average training loss: 3411.54, base loss: 4210.22
[INFO 2017-06-27 20:17:19,792 main.py:51] epoch 2333, training loss: 2820.26, average training loss: 3410.84, base loss: 4209.08
[INFO 2017-06-27 20:17:20,269 main.py:51] epoch 2334, training loss: 3012.54, average training loss: 3410.98, base loss: 4209.62
[INFO 2017-06-27 20:17:20,650 main.py:51] epoch 2335, training loss: 2992.19, average training loss: 3410.83, base loss: 4209.35
[INFO 2017-06-27 20:17:21,083 main.py:51] epoch 2336, training loss: 3740.50, average training loss: 3411.13, base loss: 4210.17
[INFO 2017-06-27 20:17:21,479 main.py:51] epoch 2337, training loss: 3463.81, average training loss: 3411.43, base loss: 4210.58
[INFO 2017-06-27 20:17:21,877 main.py:51] epoch 2338, training loss: 3310.33, average training loss: 3411.01, base loss: 4210.11
[INFO 2017-06-27 20:17:22,265 main.py:51] epoch 2339, training loss: 2970.16, average training loss: 3410.81, base loss: 4210.48
[INFO 2017-06-27 20:17:22,658 main.py:51] epoch 2340, training loss: 2680.87, average training loss: 3410.18, base loss: 4209.88
[INFO 2017-06-27 20:17:23,038 main.py:51] epoch 2341, training loss: 3152.11, average training loss: 3410.01, base loss: 4209.80
[INFO 2017-06-27 20:17:23,411 main.py:51] epoch 2342, training loss: 3236.66, average training loss: 3409.90, base loss: 4209.91
[INFO 2017-06-27 20:17:23,791 main.py:51] epoch 2343, training loss: 2864.49, average training loss: 3409.24, base loss: 4208.99
[INFO 2017-06-27 20:17:24,172 main.py:51] epoch 2344, training loss: 3572.18, average training loss: 3409.33, base loss: 4209.10
[INFO 2017-06-27 20:17:24,553 main.py:51] epoch 2345, training loss: 2981.21, average training loss: 3408.97, base loss: 4208.77
[INFO 2017-06-27 20:17:24,927 main.py:51] epoch 2346, training loss: 3043.12, average training loss: 3408.82, base loss: 4208.82
[INFO 2017-06-27 20:17:25,304 main.py:51] epoch 2347, training loss: 3103.26, average training loss: 3408.77, base loss: 4208.68
[INFO 2017-06-27 20:17:25,680 main.py:51] epoch 2348, training loss: 3074.77, average training loss: 3408.41, base loss: 4208.62
[INFO 2017-06-27 20:17:26,056 main.py:51] epoch 2349, training loss: 3297.19, average training loss: 3407.98, base loss: 4208.28
[INFO 2017-06-27 20:17:26,430 main.py:51] epoch 2350, training loss: 3454.74, average training loss: 3408.19, base loss: 4208.81
[INFO 2017-06-27 20:17:26,803 main.py:51] epoch 2351, training loss: 2922.18, average training loss: 3408.10, base loss: 4208.79
[INFO 2017-06-27 20:17:27,177 main.py:51] epoch 2352, training loss: 3608.89, average training loss: 3408.04, base loss: 4209.05
[INFO 2017-06-27 20:17:27,552 main.py:51] epoch 2353, training loss: 3294.50, average training loss: 3408.80, base loss: 4210.28
[INFO 2017-06-27 20:17:27,928 main.py:51] epoch 2354, training loss: 10212.31, average training loss: 3415.84, base loss: 4217.44
[INFO 2017-06-27 20:17:28,305 main.py:51] epoch 2355, training loss: 3454.88, average training loss: 3415.21, base loss: 4216.77
[INFO 2017-06-27 20:17:28,681 main.py:51] epoch 2356, training loss: 3028.29, average training loss: 3414.69, base loss: 4216.27
[INFO 2017-06-27 20:17:29,057 main.py:51] epoch 2357, training loss: 3341.64, average training loss: 3415.13, base loss: 4217.28
[INFO 2017-06-27 20:17:29,434 main.py:51] epoch 2358, training loss: 3519.93, average training loss: 3415.39, base loss: 4217.82
[INFO 2017-06-27 20:17:29,809 main.py:51] epoch 2359, training loss: 3012.31, average training loss: 3415.24, base loss: 4217.96
[INFO 2017-06-27 20:17:30,186 main.py:51] epoch 2360, training loss: 3208.48, average training loss: 3415.55, base loss: 4218.28
[INFO 2017-06-27 20:17:30,566 main.py:51] epoch 2361, training loss: 3264.33, average training loss: 3415.42, base loss: 4218.09
[INFO 2017-06-27 20:17:30,940 main.py:51] epoch 2362, training loss: 3214.93, average training loss: 3415.40, base loss: 4218.20
[INFO 2017-06-27 20:17:31,316 main.py:51] epoch 2363, training loss: 3064.77, average training loss: 3415.17, base loss: 4218.04
[INFO 2017-06-27 20:17:31,695 main.py:51] epoch 2364, training loss: 3004.26, average training loss: 3415.11, base loss: 4217.89
[INFO 2017-06-27 20:17:32,069 main.py:51] epoch 2365, training loss: 3633.94, average training loss: 3415.53, base loss: 4218.75
[INFO 2017-06-27 20:17:32,452 main.py:51] epoch 2366, training loss: 3455.00, average training loss: 3416.09, base loss: 4219.88
[INFO 2017-06-27 20:17:32,827 main.py:51] epoch 2367, training loss: 3744.03, average training loss: 3416.62, base loss: 4220.84
[INFO 2017-06-27 20:17:33,200 main.py:51] epoch 2368, training loss: 3438.31, average training loss: 3416.84, base loss: 4221.34
[INFO 2017-06-27 20:17:33,571 main.py:51] epoch 2369, training loss: 3684.20, average training loss: 3417.10, base loss: 4222.12
[INFO 2017-06-27 20:17:33,954 main.py:51] epoch 2370, training loss: 3132.48, average training loss: 3416.98, base loss: 4221.95
[INFO 2017-06-27 20:17:34,333 main.py:51] epoch 2371, training loss: 2865.90, average training loss: 3416.42, base loss: 4221.41
[INFO 2017-06-27 20:17:34,731 main.py:51] epoch 2372, training loss: 2839.23, average training loss: 3416.01, base loss: 4221.28
[INFO 2017-06-27 20:17:35,115 main.py:51] epoch 2373, training loss: 2861.65, average training loss: 3415.86, base loss: 4221.37
[INFO 2017-06-27 20:17:35,489 main.py:51] epoch 2374, training loss: 3253.33, average training loss: 3415.97, base loss: 4221.64
[INFO 2017-06-27 20:17:35,866 main.py:51] epoch 2375, training loss: 2969.73, average training loss: 3415.70, base loss: 4221.20
[INFO 2017-06-27 20:17:36,241 main.py:51] epoch 2376, training loss: 2742.58, average training loss: 3415.71, base loss: 4221.71
[INFO 2017-06-27 20:17:36,695 main.py:51] epoch 2377, training loss: 2766.05, average training loss: 3414.78, base loss: 4220.52
[INFO 2017-06-27 20:17:37,108 main.py:51] epoch 2378, training loss: 2884.37, average training loss: 3413.99, base loss: 4219.80
[INFO 2017-06-27 20:17:37,537 main.py:51] epoch 2379, training loss: 3422.24, average training loss: 3410.02, base loss: 4215.92
[INFO 2017-06-27 20:17:37,977 main.py:51] epoch 2380, training loss: 3407.26, average training loss: 3410.30, base loss: 4216.78
[INFO 2017-06-27 20:17:38,373 main.py:51] epoch 2381, training loss: 3297.33, average training loss: 3410.43, base loss: 4217.24
[INFO 2017-06-27 20:17:38,857 main.py:51] epoch 2382, training loss: 3591.42, average training loss: 3410.47, base loss: 4217.61
[INFO 2017-06-27 20:17:39,243 main.py:51] epoch 2383, training loss: 2981.25, average training loss: 3409.87, base loss: 4216.86
[INFO 2017-06-27 20:17:39,705 main.py:51] epoch 2384, training loss: 3345.92, average training loss: 3409.73, base loss: 4216.83
[INFO 2017-06-27 20:17:40,117 main.py:51] epoch 2385, training loss: 3024.77, average training loss: 3409.78, base loss: 4217.05
[INFO 2017-06-27 20:17:40,506 main.py:51] epoch 2386, training loss: 3179.12, average training loss: 3409.19, base loss: 4215.97
[INFO 2017-06-27 20:17:40,906 main.py:51] epoch 2387, training loss: 3218.10, average training loss: 3408.70, base loss: 4215.34
[INFO 2017-06-27 20:17:41,282 main.py:51] epoch 2388, training loss: 4061.96, average training loss: 3406.57, base loss: 4214.28
[INFO 2017-06-27 20:17:41,660 main.py:51] epoch 2389, training loss: 3238.80, average training loss: 3406.26, base loss: 4214.15
[INFO 2017-06-27 20:17:42,030 main.py:51] epoch 2390, training loss: 3284.23, average training loss: 3406.19, base loss: 4214.04
[INFO 2017-06-27 20:17:42,406 main.py:51] epoch 2391, training loss: 2745.56, average training loss: 3405.88, base loss: 4213.68
[INFO 2017-06-27 20:17:42,778 main.py:51] epoch 2392, training loss: 2848.99, average training loss: 3405.67, base loss: 4213.52
[INFO 2017-06-27 20:17:43,154 main.py:51] epoch 2393, training loss: 3045.02, average training loss: 3405.49, base loss: 4213.52
[INFO 2017-06-27 20:17:43,525 main.py:51] epoch 2394, training loss: 3217.75, average training loss: 3402.06, base loss: 4210.95
[INFO 2017-06-27 20:17:44,005 main.py:51] epoch 2395, training loss: 3778.15, average training loss: 3401.90, base loss: 4211.13
[INFO 2017-06-27 20:17:44,410 main.py:51] epoch 2396, training loss: 3523.54, average training loss: 3402.02, base loss: 4211.45
[INFO 2017-06-27 20:17:44,860 main.py:51] epoch 2397, training loss: 3227.92, average training loss: 3401.88, base loss: 4211.38
[INFO 2017-06-27 20:17:45,331 main.py:51] epoch 2398, training loss: 3291.96, average training loss: 3402.44, base loss: 4212.56
[INFO 2017-06-27 20:17:45,800 main.py:51] epoch 2399, training loss: 3231.02, average training loss: 3402.44, base loss: 4212.89
[INFO 2017-06-27 20:17:45,801 main.py:53] epoch 2399, testing
[INFO 2017-06-27 20:17:47,680 main.py:105] average testing loss: 3177.19, base loss: 4039.68
[INFO 2017-06-27 20:17:47,680 main.py:106] improve_loss: 862.49, improve_percent: 0.21
[INFO 2017-06-27 20:17:47,681 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 20:17:48,115 main.py:51] epoch 2400, training loss: 3611.72, average training loss: 3402.63, base loss: 4213.27
[INFO 2017-06-27 20:17:48,493 main.py:51] epoch 2401, training loss: 3956.75, average training loss: 3403.04, base loss: 4214.32
[INFO 2017-06-27 20:17:48,871 main.py:51] epoch 2402, training loss: 3089.69, average training loss: 3402.75, base loss: 4213.77
[INFO 2017-06-27 20:17:49,249 main.py:51] epoch 2403, training loss: 3379.98, average training loss: 3403.03, base loss: 4214.86
[INFO 2017-06-27 20:17:49,729 main.py:51] epoch 2404, training loss: 3073.81, average training loss: 3403.10, base loss: 4215.23
[INFO 2017-06-27 20:17:50,113 main.py:51] epoch 2405, training loss: 2517.38, average training loss: 3402.11, base loss: 4213.94
[INFO 2017-06-27 20:17:50,512 main.py:51] epoch 2406, training loss: 2764.25, average training loss: 3402.15, base loss: 4214.17
[INFO 2017-06-27 20:17:50,902 main.py:51] epoch 2407, training loss: 3204.69, average training loss: 3401.27, base loss: 4213.51
[INFO 2017-06-27 20:17:51,280 main.py:51] epoch 2408, training loss: 3081.79, average training loss: 3401.12, base loss: 4213.54
[INFO 2017-06-27 20:17:51,680 main.py:51] epoch 2409, training loss: 3016.55, average training loss: 3401.07, base loss: 4213.97
[INFO 2017-06-27 20:17:52,061 main.py:51] epoch 2410, training loss: 3475.66, average training loss: 3401.38, base loss: 4214.67
[INFO 2017-06-27 20:17:52,438 main.py:51] epoch 2411, training loss: 3132.63, average training loss: 3401.44, base loss: 4215.02
[INFO 2017-06-27 20:17:52,815 main.py:51] epoch 2412, training loss: 3155.88, average training loss: 3401.90, base loss: 4216.22
[INFO 2017-06-27 20:17:53,192 main.py:51] epoch 2413, training loss: 2963.01, average training loss: 3401.64, base loss: 4216.02
[INFO 2017-06-27 20:17:53,572 main.py:51] epoch 2414, training loss: 3499.58, average training loss: 3401.74, base loss: 4216.66
[INFO 2017-06-27 20:17:53,947 main.py:51] epoch 2415, training loss: 3094.71, average training loss: 3401.87, base loss: 4217.20
[INFO 2017-06-27 20:17:54,329 main.py:51] epoch 2416, training loss: 3154.98, average training loss: 3401.54, base loss: 4216.78
[INFO 2017-06-27 20:17:54,704 main.py:51] epoch 2417, training loss: 2926.43, average training loss: 3400.99, base loss: 4216.18
[INFO 2017-06-27 20:17:55,082 main.py:51] epoch 2418, training loss: 3162.65, average training loss: 3400.13, base loss: 4215.12
[INFO 2017-06-27 20:17:55,461 main.py:51] epoch 2419, training loss: 6151.06, average training loss: 3402.40, base loss: 4216.72
[INFO 2017-06-27 20:17:55,836 main.py:51] epoch 2420, training loss: 3090.27, average training loss: 3402.39, base loss: 4216.89
[INFO 2017-06-27 20:17:56,219 main.py:51] epoch 2421, training loss: 3164.68, average training loss: 3402.21, base loss: 4216.81
[INFO 2017-06-27 20:17:56,686 main.py:51] epoch 2422, training loss: 3706.12, average training loss: 3402.20, base loss: 4217.18
[INFO 2017-06-27 20:17:57,094 main.py:51] epoch 2423, training loss: 3475.72, average training loss: 3402.13, base loss: 4217.09
[INFO 2017-06-27 20:17:57,486 main.py:51] epoch 2424, training loss: 3556.62, average training loss: 3402.51, base loss: 4218.28
[INFO 2017-06-27 20:17:57,919 main.py:51] epoch 2425, training loss: 3058.63, average training loss: 3402.67, base loss: 4218.82
[INFO 2017-06-27 20:17:58,299 main.py:51] epoch 2426, training loss: 3599.03, average training loss: 3402.55, base loss: 4218.96
[INFO 2017-06-27 20:17:58,774 main.py:51] epoch 2427, training loss: 2996.58, average training loss: 3402.26, base loss: 4218.49
[INFO 2017-06-27 20:17:59,201 main.py:51] epoch 2428, training loss: 3078.90, average training loss: 3402.00, base loss: 4218.26
[INFO 2017-06-27 20:17:59,620 main.py:51] epoch 2429, training loss: 2961.63, average training loss: 3401.40, base loss: 4217.66
[INFO 2017-06-27 20:18:00,041 main.py:51] epoch 2430, training loss: 2926.36, average training loss: 3400.95, base loss: 4217.22
[INFO 2017-06-27 20:18:00,476 main.py:51] epoch 2431, training loss: 3296.48, average training loss: 3401.07, base loss: 4217.16
[INFO 2017-06-27 20:18:00,897 main.py:51] epoch 2432, training loss: 3807.17, average training loss: 3400.57, base loss: 4216.49
[INFO 2017-06-27 20:18:01,305 main.py:51] epoch 2433, training loss: 2682.12, average training loss: 3399.51, base loss: 4214.82
[INFO 2017-06-27 20:18:01,717 main.py:51] epoch 2434, training loss: 3664.43, average training loss: 3399.60, base loss: 4215.46
[INFO 2017-06-27 20:18:02,099 main.py:51] epoch 2435, training loss: 3605.21, average training loss: 3399.40, base loss: 4215.53
[INFO 2017-06-27 20:18:02,478 main.py:51] epoch 2436, training loss: 3166.26, average training loss: 3399.31, base loss: 4215.39
[INFO 2017-06-27 20:18:02,857 main.py:51] epoch 2437, training loss: 2912.13, average training loss: 3398.85, base loss: 4214.85
[INFO 2017-06-27 20:18:03,233 main.py:51] epoch 2438, training loss: 3131.30, average training loss: 3398.68, base loss: 4215.14
[INFO 2017-06-27 20:18:03,608 main.py:51] epoch 2439, training loss: 3110.27, average training loss: 3398.45, base loss: 4214.76
[INFO 2017-06-27 20:18:03,990 main.py:51] epoch 2440, training loss: 3554.53, average training loss: 3398.66, base loss: 4215.21
[INFO 2017-06-27 20:18:04,365 main.py:51] epoch 2441, training loss: 3007.37, average training loss: 3397.79, base loss: 4214.02
[INFO 2017-06-27 20:18:04,753 main.py:51] epoch 2442, training loss: 3035.72, average training loss: 3397.55, base loss: 4213.99
[INFO 2017-06-27 20:18:05,129 main.py:51] epoch 2443, training loss: 3038.08, average training loss: 3397.28, base loss: 4213.73
[INFO 2017-06-27 20:18:05,507 main.py:51] epoch 2444, training loss: 3352.77, average training loss: 3397.78, base loss: 4214.85
[INFO 2017-06-27 20:18:05,886 main.py:51] epoch 2445, training loss: 2968.94, average training loss: 3397.44, base loss: 4214.43
[INFO 2017-06-27 20:18:06,271 main.py:51] epoch 2446, training loss: 3443.69, average training loss: 3397.43, base loss: 4214.60
[INFO 2017-06-27 20:18:06,646 main.py:51] epoch 2447, training loss: 3233.37, average training loss: 3397.43, base loss: 4215.00
[INFO 2017-06-27 20:18:07,022 main.py:51] epoch 2448, training loss: 6888.08, average training loss: 3401.40, base loss: 4219.23
[INFO 2017-06-27 20:18:07,396 main.py:51] epoch 2449, training loss: 3508.88, average training loss: 3399.22, base loss: 4217.80
[INFO 2017-06-27 20:18:07,778 main.py:51] epoch 2450, training loss: 2910.58, average training loss: 3398.78, base loss: 4217.32
[INFO 2017-06-27 20:18:08,153 main.py:51] epoch 2451, training loss: 2992.76, average training loss: 3398.00, base loss: 4216.57
[INFO 2017-06-27 20:18:08,604 main.py:51] epoch 2452, training loss: 6774.47, average training loss: 3401.76, base loss: 4220.64
[INFO 2017-06-27 20:18:09,007 main.py:51] epoch 2453, training loss: 3187.35, average training loss: 3401.65, base loss: 4220.60
[INFO 2017-06-27 20:18:09,447 main.py:51] epoch 2454, training loss: 3480.68, average training loss: 3401.82, base loss: 4221.04
[INFO 2017-06-27 20:18:09,970 main.py:51] epoch 2455, training loss: 2734.58, average training loss: 3401.37, base loss: 4220.56
[INFO 2017-06-27 20:18:10,395 main.py:51] epoch 2456, training loss: 6532.57, average training loss: 3404.70, base loss: 4224.08
[INFO 2017-06-27 20:18:10,833 main.py:51] epoch 2457, training loss: 2599.67, average training loss: 3404.17, base loss: 4223.26
[INFO 2017-06-27 20:18:11,209 main.py:51] epoch 2458, training loss: 3236.92, average training loss: 3404.09, base loss: 4223.38
[INFO 2017-06-27 20:18:11,684 main.py:51] epoch 2459, training loss: 3465.33, average training loss: 3404.72, base loss: 4224.55
[INFO 2017-06-27 20:18:12,075 main.py:51] epoch 2460, training loss: 3074.83, average training loss: 3404.54, base loss: 4224.66
[INFO 2017-06-27 20:18:12,516 main.py:51] epoch 2461, training loss: 3562.18, average training loss: 3404.71, base loss: 4225.11
[INFO 2017-06-27 20:18:12,927 main.py:51] epoch 2462, training loss: 2994.31, average training loss: 3404.69, base loss: 4225.29
[INFO 2017-06-27 20:18:13,325 main.py:51] epoch 2463, training loss: 2777.22, average training loss: 3403.79, base loss: 4224.01
[INFO 2017-06-27 20:18:13,811 main.py:51] epoch 2464, training loss: 2754.62, average training loss: 3403.13, base loss: 4223.20
[INFO 2017-06-27 20:18:14,197 main.py:51] epoch 2465, training loss: 3287.67, average training loss: 3402.98, base loss: 4222.89
[INFO 2017-06-27 20:18:14,686 main.py:51] epoch 2466, training loss: 3252.18, average training loss: 3402.95, base loss: 4223.17
[INFO 2017-06-27 20:18:15,118 main.py:51] epoch 2467, training loss: 3542.72, average training loss: 3403.37, base loss: 4224.14
[INFO 2017-06-27 20:18:15,606 main.py:51] epoch 2468, training loss: 3096.80, average training loss: 3403.37, base loss: 4224.24
[INFO 2017-06-27 20:18:16,002 main.py:51] epoch 2469, training loss: 3520.11, average training loss: 3403.21, base loss: 4224.32
[INFO 2017-06-27 20:18:16,381 main.py:51] epoch 2470, training loss: 2969.54, average training loss: 3403.05, base loss: 4223.68
[INFO 2017-06-27 20:18:16,835 main.py:51] epoch 2471, training loss: 3460.96, average training loss: 3403.42, base loss: 4224.60
[INFO 2017-06-27 20:18:17,246 main.py:51] epoch 2472, training loss: 3005.20, average training loss: 3403.45, base loss: 4224.58
[INFO 2017-06-27 20:18:17,630 main.py:51] epoch 2473, training loss: 3117.02, average training loss: 3403.29, base loss: 4224.65
[INFO 2017-06-27 20:18:18,083 main.py:51] epoch 2474, training loss: 3192.65, average training loss: 3403.03, base loss: 4224.56
[INFO 2017-06-27 20:18:18,496 main.py:51] epoch 2475, training loss: 3690.92, average training loss: 3403.69, base loss: 4225.74
[INFO 2017-06-27 20:18:18,942 main.py:51] epoch 2476, training loss: 6980.35, average training loss: 3403.14, base loss: 4225.27
[INFO 2017-06-27 20:18:19,348 main.py:51] epoch 2477, training loss: 3220.77, average training loss: 3403.15, base loss: 4225.41
[INFO 2017-06-27 20:18:19,734 main.py:51] epoch 2478, training loss: 3422.03, average training loss: 3403.37, base loss: 4225.81
[INFO 2017-06-27 20:18:20,116 main.py:51] epoch 2479, training loss: 4063.22, average training loss: 3404.22, base loss: 4227.70
[INFO 2017-06-27 20:18:20,527 main.py:51] epoch 2480, training loss: 3160.83, average training loss: 3403.95, base loss: 4227.46
[INFO 2017-06-27 20:18:20,905 main.py:51] epoch 2481, training loss: 3352.04, average training loss: 3404.31, base loss: 4228.49
[INFO 2017-06-27 20:18:21,279 main.py:51] epoch 2482, training loss: 6994.70, average training loss: 3407.79, base loss: 4232.40
[INFO 2017-06-27 20:18:21,653 main.py:51] epoch 2483, training loss: 2985.02, average training loss: 3407.67, base loss: 4232.32
[INFO 2017-06-27 20:18:22,028 main.py:51] epoch 2484, training loss: 3080.05, average training loss: 3407.54, base loss: 4232.30
[INFO 2017-06-27 20:18:22,404 main.py:51] epoch 2485, training loss: 3188.80, average training loss: 3407.31, base loss: 4232.31
[INFO 2017-06-27 20:18:22,774 main.py:51] epoch 2486, training loss: 3142.37, average training loss: 3406.72, base loss: 4231.47
[INFO 2017-06-27 20:18:23,151 main.py:51] epoch 2487, training loss: 3192.02, average training loss: 3406.60, base loss: 4231.70
[INFO 2017-06-27 20:18:23,529 main.py:51] epoch 2488, training loss: 3386.96, average training loss: 3406.77, base loss: 4232.49
[INFO 2017-06-27 20:18:23,906 main.py:51] epoch 2489, training loss: 3065.70, average training loss: 3406.16, base loss: 4231.72
[INFO 2017-06-27 20:18:24,285 main.py:51] epoch 2490, training loss: 3300.32, average training loss: 3406.85, base loss: 4232.84
[INFO 2017-06-27 20:18:24,656 main.py:51] epoch 2491, training loss: 3136.51, average training loss: 3406.40, base loss: 4232.64
[INFO 2017-06-27 20:18:25,028 main.py:51] epoch 2492, training loss: 3379.58, average training loss: 3406.93, base loss: 4234.14
[INFO 2017-06-27 20:18:25,406 main.py:51] epoch 2493, training loss: 3495.24, average training loss: 3407.34, base loss: 4235.27
[INFO 2017-06-27 20:18:25,784 main.py:51] epoch 2494, training loss: 3336.43, average training loss: 3407.41, base loss: 4235.92
[INFO 2017-06-27 20:18:26,161 main.py:51] epoch 2495, training loss: 2924.12, average training loss: 3407.01, base loss: 4235.35
[INFO 2017-06-27 20:18:26,541 main.py:51] epoch 2496, training loss: 2979.68, average training loss: 3406.66, base loss: 4234.78
[INFO 2017-06-27 20:18:26,917 main.py:51] epoch 2497, training loss: 2720.66, average training loss: 3405.90, base loss: 4233.84
[INFO 2017-06-27 20:18:27,382 main.py:51] epoch 2498, training loss: 2974.28, average training loss: 3405.55, base loss: 4233.58
[INFO 2017-06-27 20:18:27,772 main.py:51] epoch 2499, training loss: 2775.03, average training loss: 3405.03, base loss: 4232.94
[INFO 2017-06-27 20:18:27,773 main.py:53] epoch 2499, testing
[INFO 2017-06-27 20:18:29,381 main.py:105] average testing loss: 3176.93, base loss: 3995.90
[INFO 2017-06-27 20:18:29,381 main.py:106] improve_loss: 818.97, improve_percent: 0.20
[INFO 2017-06-27 20:18:29,381 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 20:18:29,760 main.py:51] epoch 2500, training loss: 3395.84, average training loss: 3405.20, base loss: 4233.10
[INFO 2017-06-27 20:18:30,132 main.py:51] epoch 2501, training loss: 3048.34, average training loss: 3404.53, base loss: 4231.99
[INFO 2017-06-27 20:18:30,505 main.py:51] epoch 2502, training loss: 2989.24, average training loss: 3404.24, base loss: 4231.74
[INFO 2017-06-27 20:18:30,880 main.py:51] epoch 2503, training loss: 2890.31, average training loss: 3403.56, base loss: 4231.02
[INFO 2017-06-27 20:18:31,257 main.py:51] epoch 2504, training loss: 3021.59, average training loss: 3403.50, base loss: 4231.03
[INFO 2017-06-27 20:18:31,720 main.py:51] epoch 2505, training loss: 3319.96, average training loss: 3403.37, base loss: 4231.14
[INFO 2017-06-27 20:18:32,112 main.py:51] epoch 2506, training loss: 3032.44, average training loss: 3403.57, base loss: 4231.54
[INFO 2017-06-27 20:18:32,502 main.py:51] epoch 2507, training loss: 3323.54, average training loss: 3403.52, base loss: 4231.90
[INFO 2017-06-27 20:18:32,885 main.py:51] epoch 2508, training loss: 2926.67, average training loss: 3403.18, base loss: 4231.71
[INFO 2017-06-27 20:18:33,261 main.py:51] epoch 2509, training loss: 3263.68, average training loss: 3403.05, base loss: 4231.59
[INFO 2017-06-27 20:18:33,637 main.py:51] epoch 2510, training loss: 3104.55, average training loss: 3402.73, base loss: 4231.24
[INFO 2017-06-27 20:18:34,010 main.py:51] epoch 2511, training loss: 3246.61, average training loss: 3402.59, base loss: 4231.48
[INFO 2017-06-27 20:18:34,385 main.py:51] epoch 2512, training loss: 3062.55, average training loss: 3402.48, base loss: 4231.28
[INFO 2017-06-27 20:18:34,759 main.py:51] epoch 2513, training loss: 3062.86, average training loss: 3401.84, base loss: 4230.46
[INFO 2017-06-27 20:18:35,136 main.py:51] epoch 2514, training loss: 3602.40, average training loss: 3401.75, base loss: 4230.56
[INFO 2017-06-27 20:18:35,612 main.py:51] epoch 2515, training loss: 2795.32, average training loss: 3401.46, base loss: 4230.10
[INFO 2017-06-27 20:18:35,997 main.py:51] epoch 2516, training loss: 3345.87, average training loss: 3400.96, base loss: 4229.11
[INFO 2017-06-27 20:18:36,382 main.py:51] epoch 2517, training loss: 2739.33, average training loss: 3401.01, base loss: 4229.51
[INFO 2017-06-27 20:18:36,837 main.py:51] epoch 2518, training loss: 3302.47, average training loss: 3400.84, base loss: 4229.60
[INFO 2017-06-27 20:18:37,219 main.py:51] epoch 2519, training loss: 2519.66, average training loss: 3400.17, base loss: 4228.76
[INFO 2017-06-27 20:18:37,616 main.py:51] epoch 2520, training loss: 3512.85, average training loss: 3400.41, base loss: 4229.11
[INFO 2017-06-27 20:18:37,993 main.py:51] epoch 2521, training loss: 2750.84, average training loss: 3400.09, base loss: 4228.83
[INFO 2017-06-27 20:18:38,370 main.py:51] epoch 2522, training loss: 3022.33, average training loss: 3399.64, base loss: 4228.18
[INFO 2017-06-27 20:18:38,742 main.py:51] epoch 2523, training loss: 3323.48, average training loss: 3399.16, base loss: 4227.65
[INFO 2017-06-27 20:18:39,114 main.py:51] epoch 2524, training loss: 2993.76, average training loss: 3398.53, base loss: 4226.95
[INFO 2017-06-27 20:18:39,489 main.py:51] epoch 2525, training loss: 3130.08, average training loss: 3397.84, base loss: 4226.01
[INFO 2017-06-27 20:18:39,955 main.py:51] epoch 2526, training loss: 7210.97, average training loss: 3402.14, base loss: 4230.51
[INFO 2017-06-27 20:18:40,360 main.py:51] epoch 2527, training loss: 6074.61, average training loss: 3405.38, base loss: 4233.61
[INFO 2017-06-27 20:18:40,743 main.py:51] epoch 2528, training loss: 3271.08, average training loss: 3405.38, base loss: 4233.56
[INFO 2017-06-27 20:18:41,120 main.py:51] epoch 2529, training loss: 3179.87, average training loss: 3405.24, base loss: 4233.42
[INFO 2017-06-27 20:18:41,587 main.py:51] epoch 2530, training loss: 7074.58, average training loss: 3409.64, base loss: 4238.79
[INFO 2017-06-27 20:18:41,968 main.py:51] epoch 2531, training loss: 2861.21, average training loss: 3409.00, base loss: 4237.75
[INFO 2017-06-27 20:18:42,360 main.py:51] epoch 2532, training loss: 3103.92, average training loss: 3408.93, base loss: 4237.71
[INFO 2017-06-27 20:18:42,744 main.py:51] epoch 2533, training loss: 3250.05, average training loss: 3409.15, base loss: 4238.12
[INFO 2017-06-27 20:18:43,150 main.py:51] epoch 2534, training loss: 3020.70, average training loss: 3408.64, base loss: 4237.35
[INFO 2017-06-27 20:18:43,566 main.py:51] epoch 2535, training loss: 3349.46, average training loss: 3408.27, base loss: 4236.98
[INFO 2017-06-27 20:18:43,952 main.py:51] epoch 2536, training loss: 2465.67, average training loss: 3407.63, base loss: 4236.42
[INFO 2017-06-27 20:18:44,423 main.py:51] epoch 2537, training loss: 3268.16, average training loss: 3407.43, base loss: 4236.39
[INFO 2017-06-27 20:18:44,835 main.py:51] epoch 2538, training loss: 2927.00, average training loss: 3406.54, base loss: 4235.34
[INFO 2017-06-27 20:18:45,218 main.py:51] epoch 2539, training loss: 3235.32, average training loss: 3406.91, base loss: 4235.80
[INFO 2017-06-27 20:18:45,597 main.py:51] epoch 2540, training loss: 3273.99, average training loss: 3406.72, base loss: 4235.99
[INFO 2017-06-27 20:18:46,035 main.py:51] epoch 2541, training loss: 3092.89, average training loss: 3405.90, base loss: 4235.06
[INFO 2017-06-27 20:18:46,438 main.py:51] epoch 2542, training loss: 2986.06, average training loss: 3405.68, base loss: 4235.05
[INFO 2017-06-27 20:18:46,885 main.py:51] epoch 2543, training loss: 3623.99, average training loss: 3406.11, base loss: 4236.03
[INFO 2017-06-27 20:18:47,330 main.py:51] epoch 2544, training loss: 3144.91, average training loss: 3405.96, base loss: 4235.97
[INFO 2017-06-27 20:18:47,784 main.py:51] epoch 2545, training loss: 3414.23, average training loss: 3405.57, base loss: 4235.52
[INFO 2017-06-27 20:18:48,185 main.py:51] epoch 2546, training loss: 3116.39, average training loss: 3405.83, base loss: 4235.97
[INFO 2017-06-27 20:18:48,647 main.py:51] epoch 2547, training loss: 3700.84, average training loss: 3405.77, base loss: 4235.92
[INFO 2017-06-27 20:18:49,065 main.py:51] epoch 2548, training loss: 3276.24, average training loss: 3406.05, base loss: 4236.42
[INFO 2017-06-27 20:18:49,527 main.py:51] epoch 2549, training loss: 3090.90, average training loss: 3405.18, base loss: 4235.03
[INFO 2017-06-27 20:18:49,957 main.py:51] epoch 2550, training loss: 3060.26, average training loss: 3405.15, base loss: 4235.40
[INFO 2017-06-27 20:18:50,340 main.py:51] epoch 2551, training loss: 3596.14, average training loss: 3404.77, base loss: 4235.21
[INFO 2017-06-27 20:18:50,808 main.py:51] epoch 2552, training loss: 3050.52, average training loss: 3404.63, base loss: 4235.48
[INFO 2017-06-27 20:18:51,213 main.py:51] epoch 2553, training loss: 3119.64, average training loss: 3404.72, base loss: 4236.06
[INFO 2017-06-27 20:18:51,598 main.py:51] epoch 2554, training loss: 3276.30, average training loss: 3404.61, base loss: 4236.30
[INFO 2017-06-27 20:18:52,018 main.py:51] epoch 2555, training loss: 3317.49, average training loss: 3404.50, base loss: 4236.54
[INFO 2017-06-27 20:18:52,454 main.py:51] epoch 2556, training loss: 3032.86, average training loss: 3404.28, base loss: 4236.58
[INFO 2017-06-27 20:18:52,832 main.py:51] epoch 2557, training loss: 3410.66, average training loss: 3404.68, base loss: 4237.54
[INFO 2017-06-27 20:18:53,308 main.py:51] epoch 2558, training loss: 2857.29, average training loss: 3404.59, base loss: 4237.42
[INFO 2017-06-27 20:18:53,700 main.py:51] epoch 2559, training loss: 2915.30, average training loss: 3404.21, base loss: 4237.01
[INFO 2017-06-27 20:18:54,153 main.py:51] epoch 2560, training loss: 3586.76, average training loss: 3404.39, base loss: 4237.49
[INFO 2017-06-27 20:18:54,554 main.py:51] epoch 2561, training loss: 3325.64, average training loss: 3404.67, base loss: 4237.80
[INFO 2017-06-27 20:18:55,002 main.py:51] epoch 2562, training loss: 3631.94, average training loss: 3405.34, base loss: 4238.98
[INFO 2017-06-27 20:18:55,409 main.py:51] epoch 2563, training loss: 3152.01, average training loss: 3404.93, base loss: 4238.74
[INFO 2017-06-27 20:18:55,883 main.py:51] epoch 2564, training loss: 3218.64, average training loss: 3404.42, base loss: 4237.94
[INFO 2017-06-27 20:18:56,284 main.py:51] epoch 2565, training loss: 3178.45, average training loss: 3404.11, base loss: 4237.61
[INFO 2017-06-27 20:18:56,754 main.py:51] epoch 2566, training loss: 3213.58, average training loss: 3403.98, base loss: 4237.78
[INFO 2017-06-27 20:18:57,160 main.py:51] epoch 2567, training loss: 2773.45, average training loss: 3403.70, base loss: 4237.85
[INFO 2017-06-27 20:18:57,593 main.py:51] epoch 2568, training loss: 3095.71, average training loss: 3403.89, base loss: 4238.48
[INFO 2017-06-27 20:18:57,974 main.py:51] epoch 2569, training loss: 3109.15, average training loss: 3404.21, base loss: 4239.15
[INFO 2017-06-27 20:18:58,451 main.py:51] epoch 2570, training loss: 3401.78, average training loss: 3404.59, base loss: 4240.16
[INFO 2017-06-27 20:18:58,844 main.py:51] epoch 2571, training loss: 3073.72, average training loss: 3404.84, base loss: 4241.09
[INFO 2017-06-27 20:18:59,262 main.py:51] epoch 2572, training loss: 3436.65, average training loss: 3404.50, base loss: 4240.94
[INFO 2017-06-27 20:18:59,698 main.py:51] epoch 2573, training loss: 3393.96, average training loss: 3404.53, base loss: 4241.27
[INFO 2017-06-27 20:19:00,084 main.py:51] epoch 2574, training loss: 3084.50, average training loss: 3404.47, base loss: 4241.96
[INFO 2017-06-27 20:19:00,547 main.py:51] epoch 2575, training loss: 2967.22, average training loss: 3403.69, base loss: 4241.28
[INFO 2017-06-27 20:19:00,982 main.py:51] epoch 2576, training loss: 2937.71, average training loss: 3403.43, base loss: 4241.29
[INFO 2017-06-27 20:19:01,430 main.py:51] epoch 2577, training loss: 3603.06, average training loss: 3404.27, base loss: 4242.91
[INFO 2017-06-27 20:19:01,884 main.py:51] epoch 2578, training loss: 3763.41, average training loss: 3404.92, base loss: 4243.95
[INFO 2017-06-27 20:19:02,289 main.py:51] epoch 2579, training loss: 3064.12, average training loss: 3404.71, base loss: 4243.65
[INFO 2017-06-27 20:19:02,693 main.py:51] epoch 2580, training loss: 3349.44, average training loss: 3404.96, base loss: 4244.48
[INFO 2017-06-27 20:19:03,099 main.py:51] epoch 2581, training loss: 3072.24, average training loss: 3404.43, base loss: 4244.03
[INFO 2017-06-27 20:19:03,476 main.py:51] epoch 2582, training loss: 3204.05, average training loss: 3404.77, base loss: 4244.66
[INFO 2017-06-27 20:19:03,850 main.py:51] epoch 2583, training loss: 2929.41, average training loss: 3404.31, base loss: 4244.29
[INFO 2017-06-27 20:19:04,228 main.py:51] epoch 2584, training loss: 2930.96, average training loss: 3403.85, base loss: 4243.70
[INFO 2017-06-27 20:19:04,602 main.py:51] epoch 2585, training loss: 3559.77, average training loss: 3403.92, base loss: 4244.32
[INFO 2017-06-27 20:19:04,982 main.py:51] epoch 2586, training loss: 3103.70, average training loss: 3403.53, base loss: 4243.72
[INFO 2017-06-27 20:19:05,358 main.py:51] epoch 2587, training loss: 3420.57, average training loss: 3403.47, base loss: 4244.07
[INFO 2017-06-27 20:19:05,763 main.py:51] epoch 2588, training loss: 7236.15, average training loss: 3407.69, base loss: 4248.90
[INFO 2017-06-27 20:19:06,143 main.py:51] epoch 2589, training loss: 3097.00, average training loss: 3407.75, base loss: 4249.19
[INFO 2017-06-27 20:19:06,518 main.py:51] epoch 2590, training loss: 2685.12, average training loss: 3407.06, base loss: 4248.65
[INFO 2017-06-27 20:19:06,895 main.py:51] epoch 2591, training loss: 2792.09, average training loss: 3406.64, base loss: 4248.15
[INFO 2017-06-27 20:19:07,272 main.py:51] epoch 2592, training loss: 3238.63, average training loss: 3407.01, base loss: 4248.75
[INFO 2017-06-27 20:19:07,644 main.py:51] epoch 2593, training loss: 3543.04, average training loss: 3406.92, base loss: 4248.87
[INFO 2017-06-27 20:19:08,016 main.py:51] epoch 2594, training loss: 2887.44, average training loss: 3406.06, base loss: 4247.55
[INFO 2017-06-27 20:19:08,389 main.py:51] epoch 2595, training loss: 3082.24, average training loss: 3406.05, base loss: 4247.92
[INFO 2017-06-27 20:19:08,762 main.py:51] epoch 2596, training loss: 3087.95, average training loss: 3405.24, base loss: 4247.26
[INFO 2017-06-27 20:19:09,132 main.py:51] epoch 2597, training loss: 2667.01, average training loss: 3401.21, base loss: 4243.00
[INFO 2017-06-27 20:19:09,505 main.py:51] epoch 2598, training loss: 3019.93, average training loss: 3401.24, base loss: 4243.56
[INFO 2017-06-27 20:19:09,878 main.py:51] epoch 2599, training loss: 3389.79, average training loss: 3401.34, base loss: 4244.03
[INFO 2017-06-27 20:19:09,878 main.py:53] epoch 2599, testing
[INFO 2017-06-27 20:19:11,477 main.py:105] average testing loss: 3031.47, base loss: 3913.22
[INFO 2017-06-27 20:19:11,477 main.py:106] improve_loss: 881.75, improve_percent: 0.23
[INFO 2017-06-27 20:19:11,478 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:19:11,490 main.py:76] current best improved percent: 0.23
[INFO 2017-06-27 20:19:11,867 main.py:51] epoch 2600, training loss: 3976.43, average training loss: 3401.83, base loss: 4245.00
[INFO 2017-06-27 20:19:12,244 main.py:51] epoch 2601, training loss: 3247.59, average training loss: 3400.46, base loss: 4243.27
[INFO 2017-06-27 20:19:12,620 main.py:51] epoch 2602, training loss: 2957.70, average training loss: 3399.77, base loss: 4242.40
[INFO 2017-06-27 20:19:12,998 main.py:51] epoch 2603, training loss: 3003.10, average training loss: 3399.39, base loss: 4242.05
[INFO 2017-06-27 20:19:13,373 main.py:51] epoch 2604, training loss: 3299.93, average training loss: 3399.57, base loss: 4242.31
[INFO 2017-06-27 20:19:13,746 main.py:51] epoch 2605, training loss: 3084.69, average training loss: 3399.40, base loss: 4242.18
[INFO 2017-06-27 20:19:14,125 main.py:51] epoch 2606, training loss: 3180.46, average training loss: 3399.59, base loss: 4242.07
[INFO 2017-06-27 20:19:14,507 main.py:51] epoch 2607, training loss: 3723.90, average training loss: 3399.74, base loss: 4242.19
[INFO 2017-06-27 20:19:14,883 main.py:51] epoch 2608, training loss: 2986.93, average training loss: 3399.04, base loss: 4241.18
[INFO 2017-06-27 20:19:15,273 main.py:51] epoch 2609, training loss: 2898.29, average training loss: 3398.82, base loss: 4240.73
[INFO 2017-06-27 20:19:15,653 main.py:51] epoch 2610, training loss: 3144.95, average training loss: 3398.14, base loss: 4239.18
[INFO 2017-06-27 20:19:16,041 main.py:51] epoch 2611, training loss: 3067.51, average training loss: 3398.04, base loss: 4238.96
[INFO 2017-06-27 20:19:16,424 main.py:51] epoch 2612, training loss: 2550.66, average training loss: 3397.36, base loss: 4238.27
[INFO 2017-06-27 20:19:16,814 main.py:51] epoch 2613, training loss: 2848.36, average training loss: 3396.85, base loss: 4237.48
[INFO 2017-06-27 20:19:17,200 main.py:51] epoch 2614, training loss: 3209.31, average training loss: 3396.38, base loss: 4237.29
[INFO 2017-06-27 20:19:17,587 main.py:51] epoch 2615, training loss: 3543.58, average training loss: 3396.01, base loss: 4236.93
[INFO 2017-06-27 20:19:17,980 main.py:51] epoch 2616, training loss: 6368.45, average training loss: 3398.71, base loss: 4239.37
[INFO 2017-06-27 20:19:18,370 main.py:51] epoch 2617, training loss: 2953.73, average training loss: 3397.93, base loss: 4238.41
[INFO 2017-06-27 20:19:18,758 main.py:51] epoch 2618, training loss: 3378.60, average training loss: 3398.10, base loss: 4239.54
[INFO 2017-06-27 20:19:19,149 main.py:51] epoch 2619, training loss: 3176.95, average training loss: 3397.79, base loss: 4239.44
[INFO 2017-06-27 20:19:19,555 main.py:51] epoch 2620, training loss: 3092.48, average training loss: 3397.34, base loss: 4239.30
[INFO 2017-06-27 20:19:19,986 main.py:51] epoch 2621, training loss: 3595.94, average training loss: 3398.14, base loss: 4240.81
[INFO 2017-06-27 20:19:20,426 main.py:51] epoch 2622, training loss: 3344.10, average training loss: 3394.76, base loss: 4237.68
[INFO 2017-06-27 20:19:20,869 main.py:51] epoch 2623, training loss: 2966.54, average training loss: 3394.58, base loss: 4237.55
[INFO 2017-06-27 20:19:21,330 main.py:51] epoch 2624, training loss: 2947.59, average training loss: 3393.23, base loss: 4235.66
[INFO 2017-06-27 20:19:21,750 main.py:51] epoch 2625, training loss: 3021.40, average training loss: 3392.87, base loss: 4235.93
[INFO 2017-06-27 20:19:22,150 main.py:51] epoch 2626, training loss: 3253.32, average training loss: 3392.69, base loss: 4236.03
[INFO 2017-06-27 20:19:22,622 main.py:51] epoch 2627, training loss: 3463.80, average training loss: 3392.80, base loss: 4236.47
[INFO 2017-06-27 20:19:23,082 main.py:51] epoch 2628, training loss: 3071.58, average training loss: 3392.73, base loss: 4236.48
[INFO 2017-06-27 20:19:23,477 main.py:51] epoch 2629, training loss: 3009.37, average training loss: 3391.91, base loss: 4235.73
[INFO 2017-06-27 20:19:23,857 main.py:51] epoch 2630, training loss: 3325.08, average training loss: 3387.88, base loss: 4231.43
[INFO 2017-06-27 20:19:24,233 main.py:51] epoch 2631, training loss: 3298.76, average training loss: 3387.80, base loss: 4231.83
[INFO 2017-06-27 20:19:24,607 main.py:51] epoch 2632, training loss: 3218.23, average training loss: 3387.30, base loss: 4231.14
[INFO 2017-06-27 20:19:24,983 main.py:51] epoch 2633, training loss: 3402.09, average training loss: 3387.33, base loss: 4231.47
[INFO 2017-06-27 20:19:25,376 main.py:51] epoch 2634, training loss: 2845.02, average training loss: 3386.94, base loss: 4230.76
[INFO 2017-06-27 20:19:25,756 main.py:51] epoch 2635, training loss: 2607.90, average training loss: 3386.03, base loss: 4229.51
[INFO 2017-06-27 20:19:26,134 main.py:51] epoch 2636, training loss: 3021.65, average training loss: 3386.32, base loss: 4230.24
[INFO 2017-06-27 20:19:26,515 main.py:51] epoch 2637, training loss: 3209.75, average training loss: 3386.18, base loss: 4230.24
[INFO 2017-06-27 20:19:26,889 main.py:51] epoch 2638, training loss: 3314.37, average training loss: 3385.30, base loss: 4229.08
[INFO 2017-06-27 20:19:27,261 main.py:51] epoch 2639, training loss: 3072.21, average training loss: 3381.28, base loss: 4225.30
[INFO 2017-06-27 20:19:27,638 main.py:51] epoch 2640, training loss: 2703.12, average training loss: 3380.71, base loss: 4224.63
[INFO 2017-06-27 20:19:28,015 main.py:51] epoch 2641, training loss: 2857.27, average training loss: 3380.38, base loss: 4224.50
[INFO 2017-06-27 20:19:28,486 main.py:51] epoch 2642, training loss: 2967.76, average training loss: 3379.84, base loss: 4224.08
[INFO 2017-06-27 20:19:28,875 main.py:51] epoch 2643, training loss: 2836.45, average training loss: 3379.16, base loss: 4223.24
[INFO 2017-06-27 20:19:29,317 main.py:51] epoch 2644, training loss: 6057.52, average training loss: 3382.14, base loss: 4226.61
[INFO 2017-06-27 20:19:29,792 main.py:51] epoch 2645, training loss: 2947.35, average training loss: 3381.96, base loss: 4226.48
[INFO 2017-06-27 20:19:30,251 main.py:51] epoch 2646, training loss: 2895.88, average training loss: 3381.95, base loss: 4226.64
[INFO 2017-06-27 20:19:30,699 main.py:51] epoch 2647, training loss: 3706.81, average training loss: 3382.37, base loss: 4227.76
[INFO 2017-06-27 20:19:31,195 main.py:51] epoch 2648, training loss: 2559.49, average training loss: 3381.71, base loss: 4226.89
[INFO 2017-06-27 20:19:31,587 main.py:51] epoch 2649, training loss: 3138.75, average training loss: 3381.09, base loss: 4225.99
[INFO 2017-06-27 20:19:32,057 main.py:51] epoch 2650, training loss: 2580.93, average training loss: 3380.63, base loss: 4225.28
[INFO 2017-06-27 20:19:32,469 main.py:51] epoch 2651, training loss: 2866.52, average training loss: 3379.55, base loss: 4223.45
[INFO 2017-06-27 20:19:32,850 main.py:51] epoch 2652, training loss: 3090.62, average training loss: 3379.15, base loss: 4222.64
[INFO 2017-06-27 20:19:33,230 main.py:51] epoch 2653, training loss: 2820.08, average training loss: 3377.84, base loss: 4220.79
[INFO 2017-06-27 20:19:33,615 main.py:51] epoch 2654, training loss: 3085.04, average training loss: 3377.73, base loss: 4221.16
[INFO 2017-06-27 20:19:34,005 main.py:51] epoch 2655, training loss: 3135.80, average training loss: 3377.59, base loss: 4221.34
[INFO 2017-06-27 20:19:34,381 main.py:51] epoch 2656, training loss: 2847.88, average training loss: 3377.01, base loss: 4220.62
[INFO 2017-06-27 20:19:34,758 main.py:51] epoch 2657, training loss: 2568.66, average training loss: 3376.12, base loss: 4219.22
[INFO 2017-06-27 20:19:35,129 main.py:51] epoch 2658, training loss: 2607.32, average training loss: 3375.37, base loss: 4218.47
[INFO 2017-06-27 20:19:35,499 main.py:51] epoch 2659, training loss: 3751.53, average training loss: 3375.98, base loss: 4219.79
[INFO 2017-06-27 20:19:35,873 main.py:51] epoch 2660, training loss: 2708.96, average training loss: 3375.72, base loss: 4219.70
[INFO 2017-06-27 20:19:36,249 main.py:51] epoch 2661, training loss: 3116.63, average training loss: 3375.74, base loss: 4219.69
[INFO 2017-06-27 20:19:36,624 main.py:51] epoch 2662, training loss: 3124.51, average training loss: 3375.75, base loss: 4219.83
[INFO 2017-06-27 20:19:37,002 main.py:51] epoch 2663, training loss: 3174.83, average training loss: 3375.58, base loss: 4219.59
[INFO 2017-06-27 20:19:37,377 main.py:51] epoch 2664, training loss: 3279.14, average training loss: 3375.25, base loss: 4219.32
[INFO 2017-06-27 20:19:37,753 main.py:51] epoch 2665, training loss: 6494.99, average training loss: 3378.65, base loss: 4222.88
[INFO 2017-06-27 20:19:38,130 main.py:51] epoch 2666, training loss: 3348.60, average training loss: 3378.57, base loss: 4222.70
[INFO 2017-06-27 20:19:38,505 main.py:51] epoch 2667, training loss: 3334.95, average training loss: 3378.71, base loss: 4223.25
[INFO 2017-06-27 20:19:38,881 main.py:51] epoch 2668, training loss: 2918.16, average training loss: 3378.26, base loss: 4222.41
[INFO 2017-06-27 20:19:39,259 main.py:51] epoch 2669, training loss: 2762.94, average training loss: 3377.52, base loss: 4221.82
[INFO 2017-06-27 20:19:39,636 main.py:51] epoch 2670, training loss: 3430.01, average training loss: 3377.62, base loss: 4222.42
[INFO 2017-06-27 20:19:40,014 main.py:51] epoch 2671, training loss: 3118.25, average training loss: 3377.67, base loss: 4222.76
[INFO 2017-06-27 20:19:40,390 main.py:51] epoch 2672, training loss: 3420.23, average training loss: 3377.58, base loss: 4223.06
[INFO 2017-06-27 20:19:40,843 main.py:51] epoch 2673, training loss: 3184.92, average training loss: 3377.98, base loss: 4223.64
[INFO 2017-06-27 20:19:41,246 main.py:51] epoch 2674, training loss: 5601.03, average training loss: 3380.38, base loss: 4226.11
[INFO 2017-06-27 20:19:41,676 main.py:51] epoch 2675, training loss: 3005.89, average training loss: 3379.97, base loss: 4225.94
[INFO 2017-06-27 20:19:42,098 main.py:51] epoch 2676, training loss: 3603.55, average training loss: 3380.18, base loss: 4226.84
[INFO 2017-06-27 20:19:42,515 main.py:51] epoch 2677, training loss: 2944.71, average training loss: 3379.64, base loss: 4226.15
[INFO 2017-06-27 20:19:42,938 main.py:51] epoch 2678, training loss: 3274.62, average training loss: 3379.36, base loss: 4225.87
[INFO 2017-06-27 20:19:43,317 main.py:51] epoch 2679, training loss: 2851.31, average training loss: 3378.92, base loss: 4225.33
[INFO 2017-06-27 20:19:43,781 main.py:51] epoch 2680, training loss: 3464.65, average training loss: 3378.48, base loss: 4224.69
[INFO 2017-06-27 20:19:44,213 main.py:51] epoch 2681, training loss: 3125.98, average training loss: 3378.00, base loss: 4224.29
[INFO 2017-06-27 20:19:44,677 main.py:51] epoch 2682, training loss: 2661.92, average training loss: 3377.26, base loss: 4223.49
[INFO 2017-06-27 20:19:45,100 main.py:51] epoch 2683, training loss: 2868.07, average training loss: 3376.41, base loss: 4222.42
[INFO 2017-06-27 20:19:45,556 main.py:51] epoch 2684, training loss: 2841.33, average training loss: 3375.90, base loss: 4221.70
[INFO 2017-06-27 20:19:45,961 main.py:51] epoch 2685, training loss: 3091.73, average training loss: 3376.04, base loss: 4222.12
[INFO 2017-06-27 20:19:46,439 main.py:51] epoch 2686, training loss: 3320.75, average training loss: 3376.22, base loss: 4222.64
[INFO 2017-06-27 20:19:46,849 main.py:51] epoch 2687, training loss: 2814.45, average training loss: 3376.00, base loss: 4222.27
[INFO 2017-06-27 20:19:47,321 main.py:51] epoch 2688, training loss: 3128.70, average training loss: 3375.56, base loss: 4221.85
[INFO 2017-06-27 20:19:47,752 main.py:51] epoch 2689, training loss: 3490.63, average training loss: 3375.28, base loss: 4221.65
[INFO 2017-06-27 20:19:48,193 main.py:51] epoch 2690, training loss: 2869.39, average training loss: 3371.07, base loss: 4217.20
[INFO 2017-06-27 20:19:48,575 main.py:51] epoch 2691, training loss: 3638.60, average training loss: 3371.82, base loss: 4218.70
[INFO 2017-06-27 20:19:48,996 main.py:51] epoch 2692, training loss: 2847.04, average training loss: 3371.42, base loss: 4218.18
[INFO 2017-06-27 20:19:49,374 main.py:51] epoch 2693, training loss: 2861.78, average training loss: 3370.72, base loss: 4217.45
[INFO 2017-06-27 20:19:49,774 main.py:51] epoch 2694, training loss: 4697.01, average training loss: 3371.98, base loss: 4219.93
[INFO 2017-06-27 20:19:50,209 main.py:51] epoch 2695, training loss: 3576.90, average training loss: 3372.43, base loss: 4221.09
[INFO 2017-06-27 20:19:50,599 main.py:51] epoch 2696, training loss: 3234.13, average training loss: 3372.28, base loss: 4220.79
[INFO 2017-06-27 20:19:51,082 main.py:51] epoch 2697, training loss: 3822.21, average training loss: 3373.07, base loss: 4222.39
[INFO 2017-06-27 20:19:51,467 main.py:51] epoch 2698, training loss: 3074.75, average training loss: 3373.15, base loss: 4222.40
[INFO 2017-06-27 20:19:51,940 main.py:51] epoch 2699, training loss: 3274.07, average training loss: 3369.70, base loss: 4219.08
[INFO 2017-06-27 20:19:51,941 main.py:53] epoch 2699, testing
[INFO 2017-06-27 20:19:53,624 main.py:105] average testing loss: 3245.00, base loss: 4254.82
[INFO 2017-06-27 20:19:53,624 main.py:106] improve_loss: 1009.82, improve_percent: 0.24
[INFO 2017-06-27 20:19:53,625 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:19:53,642 main.py:76] current best improved percent: 0.24
[INFO 2017-06-27 20:19:54,026 main.py:51] epoch 2700, training loss: 3193.07, average training loss: 3370.02, base loss: 4219.81
[INFO 2017-06-27 20:19:54,402 main.py:51] epoch 2701, training loss: 3112.40, average training loss: 3370.13, base loss: 4220.19
[INFO 2017-06-27 20:19:54,778 main.py:51] epoch 2702, training loss: 3826.95, average training loss: 3371.49, base loss: 4222.41
[INFO 2017-06-27 20:19:55,154 main.py:51] epoch 2703, training loss: 3658.40, average training loss: 3371.55, base loss: 4222.90
[INFO 2017-06-27 20:19:55,530 main.py:51] epoch 2704, training loss: 3119.73, average training loss: 3371.72, base loss: 4223.27
[INFO 2017-06-27 20:19:55,903 main.py:51] epoch 2705, training loss: 3622.17, average training loss: 3372.60, base loss: 4225.06
[INFO 2017-06-27 20:19:56,278 main.py:51] epoch 2706, training loss: 3300.39, average training loss: 3372.31, base loss: 4224.85
[INFO 2017-06-27 20:19:56,654 main.py:51] epoch 2707, training loss: 3200.56, average training loss: 3372.55, base loss: 4225.48
[INFO 2017-06-27 20:19:57,042 main.py:51] epoch 2708, training loss: 3158.40, average training loss: 3372.81, base loss: 4225.89
[INFO 2017-06-27 20:19:57,417 main.py:51] epoch 2709, training loss: 3100.14, average training loss: 3372.67, base loss: 4226.11
[INFO 2017-06-27 20:19:57,792 main.py:51] epoch 2710, training loss: 3524.59, average training loss: 3373.08, base loss: 4227.22
[INFO 2017-06-27 20:19:58,171 main.py:51] epoch 2711, training loss: 3307.76, average training loss: 3373.22, base loss: 4227.82
[INFO 2017-06-27 20:19:58,550 main.py:51] epoch 2712, training loss: 3293.20, average training loss: 3372.93, base loss: 4227.64
[INFO 2017-06-27 20:19:58,923 main.py:51] epoch 2713, training loss: 3104.63, average training loss: 3373.15, base loss: 4228.40
[INFO 2017-06-27 20:19:59,299 main.py:51] epoch 2714, training loss: 3448.13, average training loss: 3373.81, base loss: 4229.48
[INFO 2017-06-27 20:19:59,673 main.py:51] epoch 2715, training loss: 6325.94, average training loss: 3377.06, base loss: 4232.59
[INFO 2017-06-27 20:20:00,049 main.py:51] epoch 2716, training loss: 3046.91, average training loss: 3377.00, base loss: 4232.73
[INFO 2017-06-27 20:20:00,425 main.py:51] epoch 2717, training loss: 3417.38, average training loss: 3376.91, base loss: 4232.64
[INFO 2017-06-27 20:20:00,801 main.py:51] epoch 2718, training loss: 3042.95, average training loss: 3376.68, base loss: 4232.50
[INFO 2017-06-27 20:20:01,175 main.py:51] epoch 2719, training loss: 2977.27, average training loss: 3376.81, base loss: 4232.51
[INFO 2017-06-27 20:20:01,554 main.py:51] epoch 2720, training loss: 2749.83, average training loss: 3376.01, base loss: 4231.47
[INFO 2017-06-27 20:20:01,942 main.py:51] epoch 2721, training loss: 6703.23, average training loss: 3379.95, base loss: 4235.56
[INFO 2017-06-27 20:20:02,316 main.py:51] epoch 2722, training loss: 3254.57, average training loss: 3379.96, base loss: 4235.89
[INFO 2017-06-27 20:20:02,690 main.py:51] epoch 2723, training loss: 2889.62, average training loss: 3379.93, base loss: 4236.24
[INFO 2017-06-27 20:20:03,074 main.py:51] epoch 2724, training loss: 2829.64, average training loss: 3379.42, base loss: 4235.58
[INFO 2017-06-27 20:20:03,457 main.py:51] epoch 2725, training loss: 3188.56, average training loss: 3378.82, base loss: 4234.90
[INFO 2017-06-27 20:20:03,868 main.py:51] epoch 2726, training loss: 3167.53, average training loss: 3378.91, base loss: 4235.07
[INFO 2017-06-27 20:20:04,250 main.py:51] epoch 2727, training loss: 6511.21, average training loss: 3382.30, base loss: 4238.75
[INFO 2017-06-27 20:20:04,636 main.py:51] epoch 2728, training loss: 3541.48, average training loss: 3381.77, base loss: 4238.48
[INFO 2017-06-27 20:20:05,026 main.py:51] epoch 2729, training loss: 3545.74, average training loss: 3382.33, base loss: 4239.37
[INFO 2017-06-27 20:20:05,403 main.py:51] epoch 2730, training loss: 2885.27, average training loss: 3381.82, base loss: 4239.00
[INFO 2017-06-27 20:20:05,782 main.py:51] epoch 2731, training loss: 3489.74, average training loss: 3382.52, base loss: 4240.41
[INFO 2017-06-27 20:20:06,166 main.py:51] epoch 2732, training loss: 3080.63, average training loss: 3382.18, base loss: 4240.18
[INFO 2017-06-27 20:20:06,552 main.py:51] epoch 2733, training loss: 4183.75, average training loss: 3382.47, base loss: 4241.43
[INFO 2017-06-27 20:20:06,964 main.py:51] epoch 2734, training loss: 3305.26, average training loss: 3382.94, base loss: 4242.61
[INFO 2017-06-27 20:20:07,351 main.py:51] epoch 2735, training loss: 3415.63, average training loss: 3383.01, base loss: 4242.97
[INFO 2017-06-27 20:20:07,735 main.py:51] epoch 2736, training loss: 3275.00, average training loss: 3383.06, base loss: 4243.07
[INFO 2017-06-27 20:20:08,119 main.py:51] epoch 2737, training loss: 3115.46, average training loss: 3382.72, base loss: 4243.11
[INFO 2017-06-27 20:20:08,519 main.py:51] epoch 2738, training loss: 3717.74, average training loss: 3383.55, base loss: 4244.80
[INFO 2017-06-27 20:20:08,997 main.py:51] epoch 2739, training loss: 2772.91, average training loss: 3383.39, base loss: 4244.82
[INFO 2017-06-27 20:20:09,434 main.py:51] epoch 2740, training loss: 3226.56, average training loss: 3383.33, base loss: 4245.11
[INFO 2017-06-27 20:20:09,885 main.py:51] epoch 2741, training loss: 2476.44, average training loss: 3382.31, base loss: 4243.51
[INFO 2017-06-27 20:20:10,341 main.py:51] epoch 2742, training loss: 2615.36, average training loss: 3381.92, base loss: 4243.28
[INFO 2017-06-27 20:20:10,771 main.py:51] epoch 2743, training loss: 3489.38, average training loss: 3382.53, base loss: 4244.20
[INFO 2017-06-27 20:20:11,175 main.py:51] epoch 2744, training loss: 3181.10, average training loss: 3382.55, base loss: 4244.49
[INFO 2017-06-27 20:20:11,644 main.py:51] epoch 2745, training loss: 2915.95, average training loss: 3378.61, base loss: 4240.61
[INFO 2017-06-27 20:20:12,103 main.py:51] epoch 2746, training loss: 3483.04, average training loss: 3378.69, base loss: 4240.60
[INFO 2017-06-27 20:20:12,497 main.py:51] epoch 2747, training loss: 6647.58, average training loss: 3382.08, base loss: 4244.12
[INFO 2017-06-27 20:20:12,875 main.py:51] epoch 2748, training loss: 3554.48, average training loss: 3381.68, base loss: 4243.64
[INFO 2017-06-27 20:20:13,250 main.py:51] epoch 2749, training loss: 3153.34, average training loss: 3381.66, base loss: 4244.10
[INFO 2017-06-27 20:20:13,625 main.py:51] epoch 2750, training loss: 3405.32, average training loss: 3381.66, base loss: 4244.32
[INFO 2017-06-27 20:20:14,004 main.py:51] epoch 2751, training loss: 2910.90, average training loss: 3381.28, base loss: 4243.98
[INFO 2017-06-27 20:20:14,448 main.py:51] epoch 2752, training loss: 2784.20, average training loss: 3380.66, base loss: 4243.24
[INFO 2017-06-27 20:20:14,833 main.py:51] epoch 2753, training loss: 2961.29, average training loss: 3380.24, base loss: 4242.80
[INFO 2017-06-27 20:20:15,312 main.py:51] epoch 2754, training loss: 2942.71, average training loss: 3379.63, base loss: 4242.27
[INFO 2017-06-27 20:20:15,711 main.py:51] epoch 2755, training loss: 3175.08, average training loss: 3379.20, base loss: 4241.66
[INFO 2017-06-27 20:20:16,194 main.py:51] epoch 2756, training loss: 6722.43, average training loss: 3383.22, base loss: 4245.78
[INFO 2017-06-27 20:20:16,576 main.py:51] epoch 2757, training loss: 3030.22, average training loss: 3382.82, base loss: 4245.57
[INFO 2017-06-27 20:20:16,960 main.py:51] epoch 2758, training loss: 3153.23, average training loss: 3383.16, base loss: 4246.23
[INFO 2017-06-27 20:20:17,337 main.py:51] epoch 2759, training loss: 3803.88, average training loss: 3383.77, base loss: 4247.66
[INFO 2017-06-27 20:20:17,714 main.py:51] epoch 2760, training loss: 2733.83, average training loss: 3383.19, base loss: 4246.89
[INFO 2017-06-27 20:20:18,090 main.py:51] epoch 2761, training loss: 3046.62, average training loss: 3382.83, base loss: 4246.45
[INFO 2017-06-27 20:20:18,473 main.py:51] epoch 2762, training loss: 3318.68, average training loss: 3383.23, base loss: 4247.30
[INFO 2017-06-27 20:20:18,869 main.py:51] epoch 2763, training loss: 2866.54, average training loss: 3382.95, base loss: 4246.72
[INFO 2017-06-27 20:20:19,249 main.py:51] epoch 2764, training loss: 3135.57, average training loss: 3383.06, base loss: 4247.25
[INFO 2017-06-27 20:20:19,627 main.py:51] epoch 2765, training loss: 2947.13, average training loss: 3382.92, base loss: 4247.47
[INFO 2017-06-27 20:20:20,005 main.py:51] epoch 2766, training loss: 3282.01, average training loss: 3383.26, base loss: 4247.88
[INFO 2017-06-27 20:20:20,381 main.py:51] epoch 2767, training loss: 2708.13, average training loss: 3382.68, base loss: 4246.89
[INFO 2017-06-27 20:20:20,762 main.py:51] epoch 2768, training loss: 3349.91, average training loss: 3383.00, base loss: 4248.01
[INFO 2017-06-27 20:20:21,141 main.py:51] epoch 2769, training loss: 3497.09, average training loss: 3383.23, base loss: 4248.89
[INFO 2017-06-27 20:20:21,519 main.py:51] epoch 2770, training loss: 3297.74, average training loss: 3383.87, base loss: 4250.20
[INFO 2017-06-27 20:20:21,896 main.py:51] epoch 2771, training loss: 3296.57, average training loss: 3384.10, base loss: 4251.18
[INFO 2017-06-27 20:20:22,275 main.py:51] epoch 2772, training loss: 3085.69, average training loss: 3384.22, base loss: 4251.64
[INFO 2017-06-27 20:20:22,653 main.py:51] epoch 2773, training loss: 2919.47, average training loss: 3383.83, base loss: 4251.38
[INFO 2017-06-27 20:20:23,032 main.py:51] epoch 2774, training loss: 2954.01, average training loss: 3383.54, base loss: 4250.84
[INFO 2017-06-27 20:20:23,410 main.py:51] epoch 2775, training loss: 3316.30, average training loss: 3383.79, base loss: 4251.61
[INFO 2017-06-27 20:20:23,786 main.py:51] epoch 2776, training loss: 3049.40, average training loss: 3383.34, base loss: 4251.11
[INFO 2017-06-27 20:20:24,159 main.py:51] epoch 2777, training loss: 3013.87, average training loss: 3382.93, base loss: 4250.58
[INFO 2017-06-27 20:20:24,534 main.py:51] epoch 2778, training loss: 3306.91, average training loss: 3382.75, base loss: 4250.66
[INFO 2017-06-27 20:20:24,912 main.py:51] epoch 2779, training loss: 3154.62, average training loss: 3382.80, base loss: 4250.82
[INFO 2017-06-27 20:20:25,292 main.py:51] epoch 2780, training loss: 3642.05, average training loss: 3383.17, base loss: 4251.40
[INFO 2017-06-27 20:20:25,664 main.py:51] epoch 2781, training loss: 3124.12, average training loss: 3383.37, base loss: 4251.83
[INFO 2017-06-27 20:20:26,038 main.py:51] epoch 2782, training loss: 3768.82, average training loss: 3383.60, base loss: 4252.41
[INFO 2017-06-27 20:20:26,422 main.py:51] epoch 2783, training loss: 2882.26, average training loss: 3383.71, base loss: 4252.82
[INFO 2017-06-27 20:20:26,805 main.py:51] epoch 2784, training loss: 2779.14, average training loss: 3383.06, base loss: 4252.03
[INFO 2017-06-27 20:20:27,269 main.py:51] epoch 2785, training loss: 3207.95, average training loss: 3382.48, base loss: 4251.39
[INFO 2017-06-27 20:20:27,664 main.py:51] epoch 2786, training loss: 3370.92, average training loss: 3382.61, base loss: 4251.83
[INFO 2017-06-27 20:20:28,110 main.py:51] epoch 2787, training loss: 3170.31, average training loss: 3379.27, base loss: 4248.84
[INFO 2017-06-27 20:20:28,529 main.py:51] epoch 2788, training loss: 2933.46, average training loss: 3379.12, base loss: 4248.66
[INFO 2017-06-27 20:20:28,913 main.py:51] epoch 2789, training loss: 2614.89, average training loss: 3378.32, base loss: 4247.32
[INFO 2017-06-27 20:20:29,292 main.py:51] epoch 2790, training loss: 3187.26, average training loss: 3377.88, base loss: 4246.96
[INFO 2017-06-27 20:20:29,669 main.py:51] epoch 2791, training loss: 2983.30, average training loss: 3377.92, base loss: 4247.02
[INFO 2017-06-27 20:20:30,065 main.py:51] epoch 2792, training loss: 3213.06, average training loss: 3378.12, base loss: 4247.69
[INFO 2017-06-27 20:20:30,445 main.py:51] epoch 2793, training loss: 3203.94, average training loss: 3378.48, base loss: 4248.77
[INFO 2017-06-27 20:20:30,819 main.py:51] epoch 2794, training loss: 3127.99, average training loss: 3378.84, base loss: 4249.45
[INFO 2017-06-27 20:20:31,194 main.py:51] epoch 2795, training loss: 3300.91, average training loss: 3378.39, base loss: 4248.75
[INFO 2017-06-27 20:20:31,568 main.py:51] epoch 2796, training loss: 3064.93, average training loss: 3378.08, base loss: 4248.81
[INFO 2017-06-27 20:20:31,944 main.py:51] epoch 2797, training loss: 3168.24, average training loss: 3377.82, base loss: 4248.42
[INFO 2017-06-27 20:20:32,320 main.py:51] epoch 2798, training loss: 3574.18, average training loss: 3377.83, base loss: 4248.81
[INFO 2017-06-27 20:20:32,692 main.py:51] epoch 2799, training loss: 2990.03, average training loss: 3377.60, base loss: 4248.45
[INFO 2017-06-27 20:20:32,692 main.py:53] epoch 2799, testing
[INFO 2017-06-27 20:20:34,303 main.py:105] average testing loss: 3338.25, base loss: 4141.96
[INFO 2017-06-27 20:20:34,303 main.py:106] improve_loss: 803.70, improve_percent: 0.19
[INFO 2017-06-27 20:20:34,304 main.py:76] current best improved percent: 0.24
[INFO 2017-06-27 20:20:34,676 main.py:51] epoch 2800, training loss: 3427.21, average training loss: 3377.67, base loss: 4248.78
[INFO 2017-06-27 20:20:35,051 main.py:51] epoch 2801, training loss: 4132.27, average training loss: 3375.23, base loss: 4247.03
[INFO 2017-06-27 20:20:35,426 main.py:51] epoch 2802, training loss: 3275.58, average training loss: 3375.03, base loss: 4247.02
[INFO 2017-06-27 20:20:35,814 main.py:51] epoch 2803, training loss: 3392.19, average training loss: 3375.19, base loss: 4247.19
[INFO 2017-06-27 20:20:36,192 main.py:51] epoch 2804, training loss: 3091.71, average training loss: 3375.13, base loss: 4247.17
[INFO 2017-06-27 20:20:36,571 main.py:51] epoch 2805, training loss: 2457.08, average training loss: 3370.49, base loss: 4242.18
[INFO 2017-06-27 20:20:36,944 main.py:51] epoch 2806, training loss: 3074.98, average training loss: 3370.76, base loss: 4242.82
[INFO 2017-06-27 20:20:37,318 main.py:51] epoch 2807, training loss: 2963.73, average training loss: 3370.56, base loss: 4242.92
[INFO 2017-06-27 20:20:37,694 main.py:51] epoch 2808, training loss: 2833.83, average training loss: 3370.44, base loss: 4242.93
[INFO 2017-06-27 20:20:38,160 main.py:51] epoch 2809, training loss: 2865.29, average training loss: 3369.65, base loss: 4241.71
[INFO 2017-06-27 20:20:38,555 main.py:51] epoch 2810, training loss: 3226.70, average training loss: 3369.82, base loss: 4242.15
[INFO 2017-06-27 20:20:38,941 main.py:51] epoch 2811, training loss: 6413.87, average training loss: 3372.82, base loss: 4245.32
[INFO 2017-06-27 20:20:39,401 main.py:51] epoch 2812, training loss: 3005.33, average training loss: 3372.46, base loss: 4244.56
[INFO 2017-06-27 20:20:39,804 main.py:51] epoch 2813, training loss: 2971.50, average training loss: 3372.28, base loss: 4244.97
[INFO 2017-06-27 20:20:40,180 main.py:51] epoch 2814, training loss: 3123.35, average training loss: 3371.90, base loss: 4245.16
[INFO 2017-06-27 20:20:40,557 main.py:51] epoch 2815, training loss: 3211.55, average training loss: 3371.29, base loss: 4244.76
[INFO 2017-06-27 20:20:40,934 main.py:51] epoch 2816, training loss: 2891.37, average training loss: 3370.73, base loss: 4244.01
[INFO 2017-06-27 20:20:41,387 main.py:51] epoch 2817, training loss: 2964.59, average training loss: 3370.27, base loss: 4243.85
[INFO 2017-06-27 20:20:41,768 main.py:51] epoch 2818, training loss: 2548.30, average training loss: 3369.72, base loss: 4242.78
[INFO 2017-06-27 20:20:42,231 main.py:51] epoch 2819, training loss: 3319.02, average training loss: 3369.52, base loss: 4242.56
[INFO 2017-06-27 20:20:42,613 main.py:51] epoch 2820, training loss: 2969.79, average training loss: 3366.35, base loss: 4239.64
[INFO 2017-06-27 20:20:43,080 main.py:51] epoch 2821, training loss: 2990.27, average training loss: 3366.04, base loss: 4239.26
[INFO 2017-06-27 20:20:43,470 main.py:51] epoch 2822, training loss: 2823.76, average training loss: 3365.43, base loss: 4238.54
[INFO 2017-06-27 20:20:43,923 main.py:51] epoch 2823, training loss: 2777.21, average training loss: 3364.81, base loss: 4237.84
[INFO 2017-06-27 20:20:44,324 main.py:51] epoch 2824, training loss: 3104.83, average training loss: 3365.06, base loss: 4238.34
[INFO 2017-06-27 20:20:44,740 main.py:51] epoch 2825, training loss: 3340.48, average training loss: 3365.41, base loss: 4239.12
[INFO 2017-06-27 20:20:45,126 main.py:51] epoch 2826, training loss: 2911.16, average training loss: 3365.10, base loss: 4238.87
[INFO 2017-06-27 20:20:45,527 main.py:51] epoch 2827, training loss: 3076.58, average training loss: 3364.87, base loss: 4239.06
[INFO 2017-06-27 20:20:45,987 main.py:51] epoch 2828, training loss: 2996.98, average training loss: 3365.02, base loss: 4239.69
[INFO 2017-06-27 20:20:46,390 main.py:51] epoch 2829, training loss: 2975.09, average training loss: 3365.04, base loss: 4240.17
[INFO 2017-06-27 20:20:46,811 main.py:51] epoch 2830, training loss: 3231.22, average training loss: 3364.38, base loss: 4239.82
[INFO 2017-06-27 20:20:47,188 main.py:51] epoch 2831, training loss: 3292.51, average training loss: 3364.42, base loss: 4240.29
[INFO 2017-06-27 20:20:47,651 main.py:51] epoch 2832, training loss: 3184.74, average training loss: 3364.07, base loss: 4239.58
[INFO 2017-06-27 20:20:48,049 main.py:51] epoch 2833, training loss: 2733.48, average training loss: 3359.95, base loss: 4235.15
[INFO 2017-06-27 20:20:48,508 main.py:51] epoch 2834, training loss: 3032.82, average training loss: 3360.03, base loss: 4235.35
[INFO 2017-06-27 20:20:48,927 main.py:51] epoch 2835, training loss: 3208.29, average training loss: 3359.93, base loss: 4235.22
[INFO 2017-06-27 20:20:49,398 main.py:51] epoch 2836, training loss: 2880.92, average training loss: 3359.16, base loss: 4234.15
[INFO 2017-06-27 20:20:49,793 main.py:51] epoch 2837, training loss: 3300.36, average training loss: 3359.46, base loss: 4234.80
[INFO 2017-06-27 20:20:50,261 main.py:51] epoch 2838, training loss: 2872.82, average training loss: 3359.37, base loss: 4234.69
[INFO 2017-06-27 20:20:50,643 main.py:51] epoch 2839, training loss: 2722.74, average training loss: 3359.22, base loss: 4234.55
[INFO 2017-06-27 20:20:51,109 main.py:51] epoch 2840, training loss: 3043.35, average training loss: 3358.76, base loss: 4233.66
[INFO 2017-06-27 20:20:51,527 main.py:51] epoch 2841, training loss: 3116.90, average training loss: 3358.24, base loss: 4232.86
[INFO 2017-06-27 20:20:51,954 main.py:51] epoch 2842, training loss: 3865.58, average training loss: 3359.14, base loss: 4234.81
[INFO 2017-06-27 20:20:52,342 main.py:51] epoch 2843, training loss: 3371.38, average training loss: 3359.46, base loss: 4235.83
[INFO 2017-06-27 20:20:52,775 main.py:51] epoch 2844, training loss: 3374.81, average training loss: 3359.55, base loss: 4236.04
[INFO 2017-06-27 20:20:53,167 main.py:51] epoch 2845, training loss: 3380.14, average training loss: 3359.68, base loss: 4236.14
[INFO 2017-06-27 20:20:53,592 main.py:51] epoch 2846, training loss: 2965.52, average training loss: 3359.15, base loss: 4235.66
[INFO 2017-06-27 20:20:53,973 main.py:51] epoch 2847, training loss: 2755.36, average training loss: 3358.90, base loss: 4235.64
[INFO 2017-06-27 20:20:54,469 main.py:51] epoch 2848, training loss: 3030.11, average training loss: 3358.64, base loss: 4235.91
[INFO 2017-06-27 20:20:54,861 main.py:51] epoch 2849, training loss: 2922.99, average training loss: 3358.31, base loss: 4235.52
[INFO 2017-06-27 20:20:55,292 main.py:51] epoch 2850, training loss: 2706.26, average training loss: 3357.54, base loss: 4234.47
[INFO 2017-06-27 20:20:55,682 main.py:51] epoch 2851, training loss: 3076.64, average training loss: 3357.62, base loss: 4234.63
[INFO 2017-06-27 20:20:56,101 main.py:51] epoch 2852, training loss: 3638.94, average training loss: 3357.80, base loss: 4234.80
[INFO 2017-06-27 20:20:56,497 main.py:51] epoch 2853, training loss: 3543.74, average training loss: 3357.60, base loss: 4234.77
[INFO 2017-06-27 20:20:56,903 main.py:51] epoch 2854, training loss: 2996.15, average training loss: 3357.28, base loss: 4234.33
[INFO 2017-06-27 20:20:57,294 main.py:51] epoch 2855, training loss: 3114.77, average training loss: 3357.32, base loss: 4234.78
[INFO 2017-06-27 20:20:57,687 main.py:51] epoch 2856, training loss: 2603.74, average training loss: 3356.88, base loss: 4234.44
[INFO 2017-06-27 20:20:58,064 main.py:51] epoch 2857, training loss: 3266.35, average training loss: 3357.14, base loss: 4235.10
[INFO 2017-06-27 20:20:58,450 main.py:51] epoch 2858, training loss: 6898.18, average training loss: 3360.86, base loss: 4238.97
[INFO 2017-06-27 20:20:58,860 main.py:51] epoch 2859, training loss: 3001.80, average training loss: 3360.62, base loss: 4238.82
[INFO 2017-06-27 20:20:59,241 main.py:51] epoch 2860, training loss: 3523.92, average training loss: 3360.97, base loss: 4239.81
[INFO 2017-06-27 20:20:59,612 main.py:51] epoch 2861, training loss: 3080.92, average training loss: 3360.93, base loss: 4239.57
[INFO 2017-06-27 20:20:59,984 main.py:51] epoch 2862, training loss: 2617.65, average training loss: 3360.76, base loss: 4239.39
[INFO 2017-06-27 20:21:00,359 main.py:51] epoch 2863, training loss: 3303.98, average training loss: 3360.69, base loss: 4239.29
[INFO 2017-06-27 20:21:00,734 main.py:51] epoch 2864, training loss: 2933.73, average training loss: 3360.55, base loss: 4239.17
[INFO 2017-06-27 20:21:01,103 main.py:51] epoch 2865, training loss: 2503.87, average training loss: 3359.66, base loss: 4237.85
[INFO 2017-06-27 20:21:01,476 main.py:51] epoch 2866, training loss: 3158.74, average training loss: 3360.07, base loss: 4238.44
[INFO 2017-06-27 20:21:01,853 main.py:51] epoch 2867, training loss: 3420.32, average training loss: 3360.21, base loss: 4238.75
[INFO 2017-06-27 20:21:02,234 main.py:51] epoch 2868, training loss: 2734.76, average training loss: 3359.52, base loss: 4237.92
[INFO 2017-06-27 20:21:02,613 main.py:51] epoch 2869, training loss: 2952.54, average training loss: 3359.51, base loss: 4237.75
[INFO 2017-06-27 20:21:02,986 main.py:51] epoch 2870, training loss: 3239.81, average training loss: 3359.63, base loss: 4238.32
[INFO 2017-06-27 20:21:03,410 main.py:51] epoch 2871, training loss: 2841.01, average training loss: 3359.60, base loss: 4238.47
[INFO 2017-06-27 20:21:03,813 main.py:51] epoch 2872, training loss: 2947.06, average training loss: 3359.00, base loss: 4237.81
[INFO 2017-06-27 20:21:04,197 main.py:51] epoch 2873, training loss: 2905.80, average training loss: 3358.88, base loss: 4237.64
[INFO 2017-06-27 20:21:04,582 main.py:51] epoch 2874, training loss: 3051.64, average training loss: 3358.76, base loss: 4237.38
[INFO 2017-06-27 20:21:04,965 main.py:51] epoch 2875, training loss: 3407.67, average training loss: 3358.43, base loss: 4236.71
[INFO 2017-06-27 20:21:05,344 main.py:51] epoch 2876, training loss: 2603.52, average training loss: 3357.69, base loss: 4235.93
[INFO 2017-06-27 20:21:05,730 main.py:51] epoch 2877, training loss: 3598.08, average training loss: 3357.84, base loss: 4236.53
[INFO 2017-06-27 20:21:06,105 main.py:51] epoch 2878, training loss: 3306.36, average training loss: 3358.33, base loss: 4237.22
[INFO 2017-06-27 20:21:06,475 main.py:51] epoch 2879, training loss: 3178.36, average training loss: 3358.40, base loss: 4237.46
[INFO 2017-06-27 20:21:06,847 main.py:51] epoch 2880, training loss: 3584.86, average training loss: 3358.66, base loss: 4237.97
[INFO 2017-06-27 20:21:07,228 main.py:51] epoch 2881, training loss: 3573.20, average training loss: 3359.26, base loss: 4239.02
[INFO 2017-06-27 20:21:07,603 main.py:51] epoch 2882, training loss: 2941.05, average training loss: 3359.22, base loss: 4239.36
[INFO 2017-06-27 20:21:07,975 main.py:51] epoch 2883, training loss: 3760.53, average training loss: 3359.57, base loss: 4240.01
[INFO 2017-06-27 20:21:08,346 main.py:51] epoch 2884, training loss: 3218.78, average training loss: 3359.04, base loss: 4239.50
[INFO 2017-06-27 20:21:08,715 main.py:51] epoch 2885, training loss: 3160.81, average training loss: 3359.53, base loss: 4240.38
[INFO 2017-06-27 20:21:09,090 main.py:51] epoch 2886, training loss: 3563.13, average training loss: 3360.16, base loss: 4241.89
[INFO 2017-06-27 20:21:09,465 main.py:51] epoch 2887, training loss: 3601.60, average training loss: 3360.25, base loss: 4242.06
[INFO 2017-06-27 20:21:09,840 main.py:51] epoch 2888, training loss: 3199.88, average training loss: 3359.92, base loss: 4241.96
[INFO 2017-06-27 20:21:10,213 main.py:51] epoch 2889, training loss: 3039.19, average training loss: 3359.89, base loss: 4242.22
[INFO 2017-06-27 20:21:10,589 main.py:51] epoch 2890, training loss: 2983.89, average training loss: 3359.46, base loss: 4241.82
[INFO 2017-06-27 20:21:10,963 main.py:51] epoch 2891, training loss: 3260.00, average training loss: 3359.24, base loss: 4241.80
[INFO 2017-06-27 20:21:11,339 main.py:51] epoch 2892, training loss: 3409.22, average training loss: 3358.84, base loss: 4241.32
[INFO 2017-06-27 20:21:11,718 main.py:51] epoch 2893, training loss: 3350.92, average training loss: 3358.80, base loss: 4241.40
[INFO 2017-06-27 20:21:12,092 main.py:51] epoch 2894, training loss: 3086.88, average training loss: 3358.29, base loss: 4240.92
[INFO 2017-06-27 20:21:12,469 main.py:51] epoch 2895, training loss: 3375.08, average training loss: 3357.83, base loss: 4240.62
[INFO 2017-06-27 20:21:12,842 main.py:51] epoch 2896, training loss: 3089.36, average training loss: 3357.92, base loss: 4240.79
[INFO 2017-06-27 20:21:13,216 main.py:51] epoch 2897, training loss: 3611.40, average training loss: 3358.08, base loss: 4241.54
[INFO 2017-06-27 20:21:13,591 main.py:51] epoch 2898, training loss: 2998.85, average training loss: 3358.07, base loss: 4241.53
[INFO 2017-06-27 20:21:14,006 main.py:51] epoch 2899, training loss: 2703.58, average training loss: 3357.74, base loss: 4241.44
[INFO 2017-06-27 20:21:14,006 main.py:53] epoch 2899, testing
[INFO 2017-06-27 20:21:15,744 main.py:105] average testing loss: 3073.93, base loss: 3913.10
[INFO 2017-06-27 20:21:15,744 main.py:106] improve_loss: 839.17, improve_percent: 0.21
[INFO 2017-06-27 20:21:15,745 main.py:76] current best improved percent: 0.24
[INFO 2017-06-27 20:21:16,122 main.py:51] epoch 2900, training loss: 3142.32, average training loss: 3357.80, base loss: 4241.34
[INFO 2017-06-27 20:21:16,495 main.py:51] epoch 2901, training loss: 3141.75, average training loss: 3358.21, base loss: 4242.26
[INFO 2017-06-27 20:21:16,871 main.py:51] epoch 2902, training loss: 3142.07, average training loss: 3358.42, base loss: 4242.57
[INFO 2017-06-27 20:21:17,248 main.py:51] epoch 2903, training loss: 3465.87, average training loss: 3358.71, base loss: 4243.50
[INFO 2017-06-27 20:21:17,723 main.py:51] epoch 2904, training loss: 2764.43, average training loss: 3358.65, base loss: 4243.72
[INFO 2017-06-27 20:21:18,116 main.py:51] epoch 2905, training loss: 3702.62, average training loss: 3358.99, base loss: 4244.53
[INFO 2017-06-27 20:21:18,614 main.py:51] epoch 2906, training loss: 2956.10, average training loss: 3358.69, base loss: 4244.34
[INFO 2017-06-27 20:21:19,010 main.py:51] epoch 2907, training loss: 3200.65, average training loss: 3358.36, base loss: 4244.08
[INFO 2017-06-27 20:21:19,477 main.py:51] epoch 2908, training loss: 3198.94, average training loss: 3358.14, base loss: 4244.08
[INFO 2017-06-27 20:21:19,902 main.py:51] epoch 2909, training loss: 9546.61, average training loss: 3361.65, base loss: 4247.71
[INFO 2017-06-27 20:21:20,289 main.py:51] epoch 2910, training loss: 2825.25, average training loss: 3360.45, base loss: 4246.57
[INFO 2017-06-27 20:21:20,680 main.py:51] epoch 2911, training loss: 3462.59, average training loss: 3361.39, base loss: 4248.11
[INFO 2017-06-27 20:21:21,133 main.py:51] epoch 2912, training loss: 3462.15, average training loss: 3361.92, base loss: 4249.39
[INFO 2017-06-27 20:21:21,542 main.py:51] epoch 2913, training loss: 2851.11, average training loss: 3361.15, base loss: 4248.57
[INFO 2017-06-27 20:21:22,012 main.py:51] epoch 2914, training loss: 2916.54, average training loss: 3361.41, base loss: 4249.38
[INFO 2017-06-27 20:21:22,434 main.py:51] epoch 2915, training loss: 3098.01, average training loss: 3361.06, base loss: 4249.23
[INFO 2017-06-27 20:21:22,882 main.py:51] epoch 2916, training loss: 3425.25, average training loss: 3361.26, base loss: 4250.10
[INFO 2017-06-27 20:21:23,282 main.py:51] epoch 2917, training loss: 3594.00, average training loss: 3361.66, base loss: 4250.90
[INFO 2017-06-27 20:21:23,751 main.py:51] epoch 2918, training loss: 3404.07, average training loss: 3361.80, base loss: 4251.22
[INFO 2017-06-27 20:21:24,177 main.py:51] epoch 2919, training loss: 3426.85, average training loss: 3361.50, base loss: 4251.21
[INFO 2017-06-27 20:21:24,629 main.py:51] epoch 2920, training loss: 2977.63, average training loss: 3360.98, base loss: 4250.38
[INFO 2017-06-27 20:21:25,047 main.py:51] epoch 2921, training loss: 3114.90, average training loss: 3360.84, base loss: 4250.10
[INFO 2017-06-27 20:21:25,512 main.py:51] epoch 2922, training loss: 2756.11, average training loss: 3359.82, base loss: 4248.60
[INFO 2017-06-27 20:21:25,954 main.py:51] epoch 2923, training loss: 6982.09, average training loss: 3363.67, base loss: 4252.83
[INFO 2017-06-27 20:21:26,441 main.py:51] epoch 2924, training loss: 3086.64, average training loss: 3363.32, base loss: 4252.65
[INFO 2017-06-27 20:21:26,846 main.py:51] epoch 2925, training loss: 3136.16, average training loss: 3362.56, base loss: 4251.36
[INFO 2017-06-27 20:21:27,330 main.py:51] epoch 2926, training loss: 3012.97, average training loss: 3362.72, base loss: 4251.78
[INFO 2017-06-27 20:21:27,757 main.py:51] epoch 2927, training loss: 3210.50, average training loss: 3363.07, base loss: 4252.42
[INFO 2017-06-27 20:21:28,191 main.py:51] epoch 2928, training loss: 3351.85, average training loss: 3363.30, base loss: 4253.09
[INFO 2017-06-27 20:21:28,588 main.py:51] epoch 2929, training loss: 3310.15, average training loss: 3363.67, base loss: 4253.95
[INFO 2017-06-27 20:21:29,000 main.py:51] epoch 2930, training loss: 3077.54, average training loss: 3363.38, base loss: 4253.49
[INFO 2017-06-27 20:21:29,401 main.py:51] epoch 2931, training loss: 2902.51, average training loss: 3362.63, base loss: 4252.31
[INFO 2017-06-27 20:21:29,826 main.py:51] epoch 2932, training loss: 3408.31, average training loss: 3362.95, base loss: 4253.24
[INFO 2017-06-27 20:21:30,252 main.py:51] epoch 2933, training loss: 5906.16, average training loss: 3366.03, base loss: 4256.82
[INFO 2017-06-27 20:21:30,708 main.py:51] epoch 2934, training loss: 3486.50, average training loss: 3365.26, base loss: 4255.80
[INFO 2017-06-27 20:21:31,166 main.py:51] epoch 2935, training loss: 3178.45, average training loss: 3365.53, base loss: 4256.61
[INFO 2017-06-27 20:21:31,608 main.py:51] epoch 2936, training loss: 3089.79, average training loss: 3365.52, base loss: 4256.78
[INFO 2017-06-27 20:21:32,048 main.py:51] epoch 2937, training loss: 2816.65, average training loss: 3364.61, base loss: 4255.42
[INFO 2017-06-27 20:21:32,481 main.py:51] epoch 2938, training loss: 3048.79, average training loss: 3364.48, base loss: 4255.54
[INFO 2017-06-27 20:21:32,861 main.py:51] epoch 2939, training loss: 3468.35, average training loss: 3365.07, base loss: 4257.06
[INFO 2017-06-27 20:21:33,246 main.py:51] epoch 2940, training loss: 2463.36, average training loss: 3364.08, base loss: 4255.64
[INFO 2017-06-27 20:21:33,626 main.py:51] epoch 2941, training loss: 3208.58, average training loss: 3363.91, base loss: 4255.73
[INFO 2017-06-27 20:21:34,006 main.py:51] epoch 2942, training loss: 3577.63, average training loss: 3364.63, base loss: 4257.06
[INFO 2017-06-27 20:21:34,407 main.py:51] epoch 2943, training loss: 3392.24, average training loss: 3364.69, base loss: 4257.64
[INFO 2017-06-27 20:21:34,787 main.py:51] epoch 2944, training loss: 3172.14, average training loss: 3364.96, base loss: 4258.32
[INFO 2017-06-27 20:21:35,168 main.py:51] epoch 2945, training loss: 3123.53, average training loss: 3364.30, base loss: 4257.64
[INFO 2017-06-27 20:21:35,553 main.py:51] epoch 2946, training loss: 6684.54, average training loss: 3367.59, base loss: 4260.94
[INFO 2017-06-27 20:21:35,937 main.py:51] epoch 2947, training loss: 2680.47, average training loss: 3366.89, base loss: 4259.92
[INFO 2017-06-27 20:21:36,315 main.py:51] epoch 2948, training loss: 3240.93, average training loss: 3366.55, base loss: 4259.66
[INFO 2017-06-27 20:21:36,695 main.py:51] epoch 2949, training loss: 3249.53, average training loss: 3363.27, base loss: 4256.87
[INFO 2017-06-27 20:21:37,072 main.py:51] epoch 2950, training loss: 2974.21, average training loss: 3363.29, base loss: 4256.91
[INFO 2017-06-27 20:21:37,447 main.py:51] epoch 2951, training loss: 3612.05, average training loss: 3363.99, base loss: 4258.36
[INFO 2017-06-27 20:21:37,826 main.py:51] epoch 2952, training loss: 2885.36, average training loss: 3364.12, base loss: 4258.46
[INFO 2017-06-27 20:21:38,196 main.py:51] epoch 2953, training loss: 2550.04, average training loss: 3363.58, base loss: 4257.83
[INFO 2017-06-27 20:21:38,574 main.py:51] epoch 2954, training loss: 3044.54, average training loss: 3363.27, base loss: 4257.66
[INFO 2017-06-27 20:21:38,956 main.py:51] epoch 2955, training loss: 3130.83, average training loss: 3363.15, base loss: 4257.66
[INFO 2017-06-27 20:21:39,336 main.py:51] epoch 2956, training loss: 2693.45, average training loss: 3362.84, base loss: 4257.36
[INFO 2017-06-27 20:21:39,708 main.py:51] epoch 2957, training loss: 3248.90, average training loss: 3362.89, base loss: 4258.10
[INFO 2017-06-27 20:21:40,083 main.py:51] epoch 2958, training loss: 3437.82, average training loss: 3363.12, base loss: 4258.81
[INFO 2017-06-27 20:21:40,457 main.py:51] epoch 2959, training loss: 3110.02, average training loss: 3363.05, base loss: 4258.95
[INFO 2017-06-27 20:21:40,838 main.py:51] epoch 2960, training loss: 3147.50, average training loss: 3363.19, base loss: 4259.03
[INFO 2017-06-27 20:21:41,219 main.py:51] epoch 2961, training loss: 2561.81, average training loss: 3362.39, base loss: 4258.11
[INFO 2017-06-27 20:21:41,591 main.py:51] epoch 2962, training loss: 3033.98, average training loss: 3362.12, base loss: 4257.67
[INFO 2017-06-27 20:21:41,968 main.py:51] epoch 2963, training loss: 2910.13, average training loss: 3361.32, base loss: 4256.57
[INFO 2017-06-27 20:21:42,345 main.py:51] epoch 2964, training loss: 2780.25, average training loss: 3361.13, base loss: 4256.52
[INFO 2017-06-27 20:21:42,717 main.py:51] epoch 2965, training loss: 3137.90, average training loss: 3361.42, base loss: 4256.98
[INFO 2017-06-27 20:21:43,089 main.py:51] epoch 2966, training loss: 3098.49, average training loss: 3361.20, base loss: 4256.72
[INFO 2017-06-27 20:21:43,461 main.py:51] epoch 2967, training loss: 3158.68, average training loss: 3360.66, base loss: 4256.32
[INFO 2017-06-27 20:21:43,832 main.py:51] epoch 2968, training loss: 3491.65, average training loss: 3361.08, base loss: 4257.11
[INFO 2017-06-27 20:21:44,204 main.py:51] epoch 2969, training loss: 2783.56, average training loss: 3361.14, base loss: 4257.13
[INFO 2017-06-27 20:21:44,574 main.py:51] epoch 2970, training loss: 3197.85, average training loss: 3361.14, base loss: 4257.35
[INFO 2017-06-27 20:21:44,949 main.py:51] epoch 2971, training loss: 3038.15, average training loss: 3361.14, base loss: 4257.46
[INFO 2017-06-27 20:21:45,319 main.py:51] epoch 2972, training loss: 2768.96, average training loss: 3357.51, base loss: 4253.97
[INFO 2017-06-27 20:21:45,691 main.py:51] epoch 2973, training loss: 3174.14, average training loss: 3357.59, base loss: 4254.76
[INFO 2017-06-27 20:21:46,065 main.py:51] epoch 2974, training loss: 3122.21, average training loss: 3357.46, base loss: 4254.56
[INFO 2017-06-27 20:21:46,440 main.py:51] epoch 2975, training loss: 3654.79, average training loss: 3356.84, base loss: 4254.10
[INFO 2017-06-27 20:21:46,817 main.py:51] epoch 2976, training loss: 3206.08, average training loss: 3354.02, base loss: 4251.83
[INFO 2017-06-27 20:21:47,194 main.py:51] epoch 2977, training loss: 2975.15, average training loss: 3354.16, base loss: 4252.38
[INFO 2017-06-27 20:21:47,572 main.py:51] epoch 2978, training loss: 3139.20, average training loss: 3354.49, base loss: 4253.65
[INFO 2017-06-27 20:21:47,948 main.py:51] epoch 2979, training loss: 3102.79, average training loss: 3354.11, base loss: 4253.29
[INFO 2017-06-27 20:21:48,325 main.py:51] epoch 2980, training loss: 3110.76, average training loss: 3354.21, base loss: 4253.55
[INFO 2017-06-27 20:21:48,701 main.py:51] epoch 2981, training loss: 2597.20, average training loss: 3353.62, base loss: 4252.76
[INFO 2017-06-27 20:21:49,078 main.py:51] epoch 2982, training loss: 3013.81, average training loss: 3353.81, base loss: 4252.98
[INFO 2017-06-27 20:21:49,454 main.py:51] epoch 2983, training loss: 3339.05, average training loss: 3350.55, base loss: 4250.10
[INFO 2017-06-27 20:21:49,825 main.py:51] epoch 2984, training loss: 3374.37, average training loss: 3350.91, base loss: 4251.08
[INFO 2017-06-27 20:21:50,196 main.py:51] epoch 2985, training loss: 3236.26, average training loss: 3351.30, base loss: 4251.72
[INFO 2017-06-27 20:21:50,569 main.py:51] epoch 2986, training loss: 3139.08, average training loss: 3351.11, base loss: 4251.89
[INFO 2017-06-27 20:21:50,946 main.py:51] epoch 2987, training loss: 3395.81, average training loss: 3351.65, base loss: 4253.27
[INFO 2017-06-27 20:21:51,322 main.py:51] epoch 2988, training loss: 2504.98, average training loss: 3347.20, base loss: 4248.41
[INFO 2017-06-27 20:21:51,694 main.py:51] epoch 2989, training loss: 2893.08, average training loss: 3346.84, base loss: 4247.92
[INFO 2017-06-27 20:21:52,071 main.py:51] epoch 2990, training loss: 2943.01, average training loss: 3346.56, base loss: 4247.65
[INFO 2017-06-27 20:21:52,442 main.py:51] epoch 2991, training loss: 2853.91, average training loss: 3346.03, base loss: 4246.98
[INFO 2017-06-27 20:21:52,813 main.py:51] epoch 2992, training loss: 3194.91, average training loss: 3345.77, base loss: 4246.82
[INFO 2017-06-27 20:21:53,188 main.py:51] epoch 2993, training loss: 6767.70, average training loss: 3349.48, base loss: 4250.89
[INFO 2017-06-27 20:21:53,560 main.py:51] epoch 2994, training loss: 3285.08, average training loss: 3349.30, base loss: 4250.76
[INFO 2017-06-27 20:21:53,931 main.py:51] epoch 2995, training loss: 3227.81, average training loss: 3349.45, base loss: 4251.28
[INFO 2017-06-27 20:21:54,303 main.py:51] epoch 2996, training loss: 2778.01, average training loss: 3349.07, base loss: 4250.92
[INFO 2017-06-27 20:21:54,676 main.py:51] epoch 2997, training loss: 2994.15, average training loss: 3349.00, base loss: 4250.96
[INFO 2017-06-27 20:21:55,053 main.py:51] epoch 2998, training loss: 3241.86, average training loss: 3349.11, base loss: 4250.88
[INFO 2017-06-27 20:21:55,429 main.py:51] epoch 2999, training loss: 3119.94, average training loss: 3349.34, base loss: 4251.35
[INFO 2017-06-27 20:21:55,429 main.py:53] epoch 2999, testing
[INFO 2017-06-27 20:21:57,036 main.py:105] average testing loss: 3487.39, base loss: 4471.37
[INFO 2017-06-27 20:21:57,036 main.py:106] improve_loss: 983.98, improve_percent: 0.22
[INFO 2017-06-27 20:21:57,036 main.py:76] current best improved percent: 0.24
[INFO 2017-06-27 20:21:57,406 main.py:51] epoch 3000, training loss: 3608.70, average training loss: 3346.17, base loss: 4248.67
[INFO 2017-06-27 20:21:57,784 main.py:51] epoch 3001, training loss: 3152.54, average training loss: 3346.65, base loss: 4249.25
[INFO 2017-06-27 20:21:58,155 main.py:51] epoch 3002, training loss: 3734.28, average training loss: 3346.82, base loss: 4249.69
[INFO 2017-06-27 20:21:58,525 main.py:51] epoch 3003, training loss: 3126.98, average training loss: 3346.53, base loss: 4249.38
