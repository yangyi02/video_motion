[INFO 2017-06-28 14:19:22,522 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-28 14:19:26,717 main.py:51] epoch 0, training loss: 29945.11, average training loss: 29945.11, base loss: 14051.16
[INFO 2017-06-28 14:19:27,361 main.py:51] epoch 1, training loss: 22131.52, average training loss: 26038.31, base loss: 13004.98
[INFO 2017-06-28 14:19:28,117 main.py:51] epoch 2, training loss: 22618.50, average training loss: 24898.38, base loss: 13328.81
[INFO 2017-06-28 14:19:28,917 main.py:51] epoch 3, training loss: 19624.90, average training loss: 23580.01, base loss: 13428.20
[INFO 2017-06-28 14:19:29,733 main.py:51] epoch 4, training loss: 18751.57, average training loss: 22614.32, base loss: 13537.49
[INFO 2017-06-28 14:19:30,368 main.py:51] epoch 5, training loss: 19224.25, average training loss: 22049.31, base loss: 13946.38
[INFO 2017-06-28 14:19:31,181 main.py:51] epoch 6, training loss: 16587.96, average training loss: 21269.12, base loss: 14000.34
[INFO 2017-06-28 14:19:32,001 main.py:51] epoch 7, training loss: 17003.54, average training loss: 20735.92, base loss: 13999.25
[INFO 2017-06-28 14:19:32,837 main.py:51] epoch 8, training loss: 17135.40, average training loss: 20335.86, base loss: 14153.01
[INFO 2017-06-28 14:19:33,485 main.py:51] epoch 9, training loss: 18107.79, average training loss: 20113.05, base loss: 14398.99
[INFO 2017-06-28 14:19:34,203 main.py:51] epoch 10, training loss: 15425.52, average training loss: 19686.91, base loss: 14364.59
[INFO 2017-06-28 14:19:35,042 main.py:51] epoch 11, training loss: 14620.93, average training loss: 19264.75, base loss: 14337.30
[INFO 2017-06-28 14:19:35,767 main.py:51] epoch 12, training loss: 15314.10, average training loss: 18960.85, base loss: 14360.23
[INFO 2017-06-28 14:19:36,494 main.py:51] epoch 13, training loss: 14062.75, average training loss: 18610.99, base loss: 14254.09
[INFO 2017-06-28 14:19:37,301 main.py:51] epoch 14, training loss: 12829.53, average training loss: 18225.56, base loss: 14119.24
[INFO 2017-06-28 14:19:38,110 main.py:51] epoch 15, training loss: 13283.71, average training loss: 17916.69, base loss: 14038.68
[INFO 2017-06-28 14:19:38,792 main.py:51] epoch 16, training loss: 15496.28, average training loss: 17774.32, base loss: 14128.40
[INFO 2017-06-28 14:19:39,528 main.py:51] epoch 17, training loss: 13750.21, average training loss: 17550.75, base loss: 14097.26
[INFO 2017-06-28 14:19:40,342 main.py:51] epoch 18, training loss: 16008.97, average training loss: 17469.61, base loss: 14194.05
[INFO 2017-06-28 14:19:41,161 main.py:51] epoch 19, training loss: 15479.77, average training loss: 17370.12, base loss: 14268.81
[INFO 2017-06-28 14:19:41,862 main.py:51] epoch 20, training loss: 15678.68, average training loss: 17289.57, base loss: 14338.57
[INFO 2017-06-28 14:19:42,609 main.py:51] epoch 21, training loss: 13483.16, average training loss: 17116.55, base loss: 14305.00
[INFO 2017-06-28 14:19:43,385 main.py:51] epoch 22, training loss: 13599.05, average training loss: 16963.62, base loss: 14282.17
[INFO 2017-06-28 14:19:44,016 main.py:51] epoch 23, training loss: 13169.03, average training loss: 16805.51, base loss: 14239.24
[INFO 2017-06-28 14:19:44,778 main.py:51] epoch 24, training loss: 14060.33, average training loss: 16695.70, base loss: 14252.41
[INFO 2017-06-28 14:19:45,602 main.py:51] epoch 25, training loss: 11398.34, average training loss: 16491.96, base loss: 14146.43
[INFO 2017-06-28 14:19:46,275 main.py:51] epoch 26, training loss: 13801.72, average training loss: 16392.32, base loss: 14151.39
[INFO 2017-06-28 14:19:47,027 main.py:51] epoch 27, training loss: 15020.98, average training loss: 16343.34, base loss: 14199.02
[INFO 2017-06-28 14:19:47,835 main.py:51] epoch 28, training loss: 15471.31, average training loss: 16313.27, base loss: 14263.03
[INFO 2017-06-28 14:19:48,707 main.py:51] epoch 29, training loss: 14516.59, average training loss: 16253.38, base loss: 14292.00
[INFO 2017-06-28 14:19:49,394 main.py:51] epoch 30, training loss: 11083.49, average training loss: 16086.61, base loss: 14192.17
[INFO 2017-06-28 14:19:50,107 main.py:51] epoch 31, training loss: 16479.11, average training loss: 16098.88, base loss: 14287.97
[INFO 2017-06-28 14:19:50,963 main.py:51] epoch 32, training loss: 11176.36, average training loss: 15949.71, base loss: 14200.55
[INFO 2017-06-28 14:19:51,762 main.py:51] epoch 33, training loss: 12524.01, average training loss: 15848.96, base loss: 14164.62
[INFO 2017-06-28 14:19:52,445 main.py:51] epoch 34, training loss: 12867.90, average training loss: 15763.78, base loss: 14142.82
[INFO 2017-06-28 14:19:53,207 main.py:51] epoch 35, training loss: 13880.40, average training loss: 15711.47, base loss: 14151.00
[INFO 2017-06-28 14:19:54,001 main.py:51] epoch 36, training loss: 13636.09, average training loss: 15655.37, base loss: 14153.85
[INFO 2017-06-28 14:19:54,695 main.py:51] epoch 37, training loss: 14573.08, average training loss: 15626.89, base loss: 14187.22
[INFO 2017-06-28 14:19:55,475 main.py:51] epoch 38, training loss: 13240.44, average training loss: 15565.70, base loss: 14176.93
[INFO 2017-06-28 14:19:56,273 main.py:51] epoch 39, training loss: 15179.01, average training loss: 15556.03, base loss: 14229.40
[INFO 2017-06-28 14:19:57,068 main.py:51] epoch 40, training loss: 12194.51, average training loss: 15474.05, base loss: 14200.65
[INFO 2017-06-28 14:19:57,689 main.py:51] epoch 41, training loss: 11840.48, average training loss: 15387.53, base loss: 14155.47
[INFO 2017-06-28 14:19:58,485 main.py:51] epoch 42, training loss: 13381.81, average training loss: 15340.89, base loss: 14158.30
[INFO 2017-06-28 14:19:59,316 main.py:51] epoch 43, training loss: 12244.71, average training loss: 15270.52, base loss: 14130.25
[INFO 2017-06-28 14:20:00,007 main.py:51] epoch 44, training loss: 14301.59, average training loss: 15248.99, base loss: 14158.61
[INFO 2017-06-28 14:20:00,738 main.py:51] epoch 45, training loss: 14630.17, average training loss: 15235.54, base loss: 14188.14
[INFO 2017-06-28 14:20:01,549 main.py:51] epoch 46, training loss: 13125.17, average training loss: 15190.63, base loss: 14178.95
[INFO 2017-06-28 14:20:02,390 main.py:51] epoch 47, training loss: 13615.00, average training loss: 15157.81, base loss: 14187.58
[INFO 2017-06-28 14:20:03,030 main.py:51] epoch 48, training loss: 14436.75, average training loss: 15143.09, base loss: 14217.07
[INFO 2017-06-28 14:20:03,806 main.py:51] epoch 49, training loss: 14397.59, average training loss: 15128.18, base loss: 14245.73
[INFO 2017-06-28 14:20:04,616 main.py:51] epoch 50, training loss: 12125.07, average training loss: 15069.30, base loss: 14219.50
[INFO 2017-06-28 14:20:05,417 main.py:51] epoch 51, training loss: 11884.37, average training loss: 15008.05, base loss: 14190.87
[INFO 2017-06-28 14:20:06,074 main.py:51] epoch 52, training loss: 11122.24, average training loss: 14934.73, base loss: 14138.39
[INFO 2017-06-28 14:20:06,838 main.py:51] epoch 53, training loss: 13419.53, average training loss: 14906.67, base loss: 14140.40
[INFO 2017-06-28 14:20:07,687 main.py:51] epoch 54, training loss: 16429.34, average training loss: 14934.36, base loss: 14207.79
[INFO 2017-06-28 14:20:08,423 main.py:51] epoch 55, training loss: 15359.47, average training loss: 14941.95, base loss: 14251.17
[INFO 2017-06-28 14:20:09,155 main.py:51] epoch 56, training loss: 13287.73, average training loss: 14912.93, base loss: 14255.17
[INFO 2017-06-28 14:20:09,957 main.py:51] epoch 57, training loss: 11750.19, average training loss: 14858.40, base loss: 14225.45
[INFO 2017-06-28 14:20:10,794 main.py:51] epoch 58, training loss: 13203.81, average training loss: 14830.35, base loss: 14220.38
[INFO 2017-06-28 14:20:11,477 main.py:51] epoch 59, training loss: 13188.55, average training loss: 14802.99, base loss: 14216.85
[INFO 2017-06-28 14:20:12,211 main.py:51] epoch 60, training loss: 12755.07, average training loss: 14769.42, base loss: 14212.42
[INFO 2017-06-28 14:20:13,036 main.py:51] epoch 61, training loss: 13103.74, average training loss: 14742.55, base loss: 14209.51
[INFO 2017-06-28 14:20:13,812 main.py:51] epoch 62, training loss: 15576.60, average training loss: 14755.79, base loss: 14259.27
[INFO 2017-06-28 14:20:14,497 main.py:51] epoch 63, training loss: 13893.15, average training loss: 14742.31, base loss: 14272.79
[INFO 2017-06-28 14:20:15,219 main.py:51] epoch 64, training loss: 13654.92, average training loss: 14725.58, base loss: 14283.81
[INFO 2017-06-28 14:20:16,052 main.py:51] epoch 65, training loss: 12894.08, average training loss: 14697.83, base loss: 14275.96
[INFO 2017-06-28 14:20:16,714 main.py:51] epoch 66, training loss: 12630.05, average training loss: 14666.97, base loss: 14270.20
[INFO 2017-06-28 14:20:17,484 main.py:51] epoch 67, training loss: 13192.18, average training loss: 14645.28, base loss: 14271.79
[INFO 2017-06-28 14:20:18,297 main.py:51] epoch 68, training loss: 13021.06, average training loss: 14621.74, base loss: 14268.44
[INFO 2017-06-28 14:20:19,041 main.py:51] epoch 69, training loss: 16145.69, average training loss: 14643.51, base loss: 14318.74
[INFO 2017-06-28 14:20:19,738 main.py:51] epoch 70, training loss: 12501.32, average training loss: 14613.34, base loss: 14305.07
[INFO 2017-06-28 14:20:20,408 main.py:51] epoch 71, training loss: 12170.07, average training loss: 14579.41, base loss: 14284.95
[INFO 2017-06-28 14:20:21,054 main.py:51] epoch 72, training loss: 13638.95, average training loss: 14566.53, base loss: 14294.13
[INFO 2017-06-28 14:20:21,678 main.py:51] epoch 73, training loss: 11327.57, average training loss: 14522.76, base loss: 14265.76
[INFO 2017-06-28 14:20:22,309 main.py:51] epoch 74, training loss: 11487.49, average training loss: 14482.29, base loss: 14239.48
[INFO 2017-06-28 14:20:23,000 main.py:51] epoch 75, training loss: 12237.18, average training loss: 14452.74, base loss: 14224.68
[INFO 2017-06-28 14:20:23,684 main.py:51] epoch 76, training loss: 13194.10, average training loss: 14436.40, base loss: 14227.98
[INFO 2017-06-28 14:20:24,430 main.py:51] epoch 77, training loss: 12484.06, average training loss: 14411.37, base loss: 14222.99
[INFO 2017-06-28 14:20:25,238 main.py:51] epoch 78, training loss: 12381.69, average training loss: 14385.68, base loss: 14216.85
[INFO 2017-06-28 14:20:26,070 main.py:51] epoch 79, training loss: 14253.49, average training loss: 14384.02, base loss: 14236.90
[INFO 2017-06-28 14:20:26,732 main.py:51] epoch 80, training loss: 15021.57, average training loss: 14391.90, base loss: 14264.85
[INFO 2017-06-28 14:20:27,494 main.py:51] epoch 81, training loss: 13563.14, average training loss: 14381.79, base loss: 14277.53
[INFO 2017-06-28 14:20:28,362 main.py:51] epoch 82, training loss: 12862.55, average training loss: 14363.48, base loss: 14280.01
[INFO 2017-06-28 14:20:29,067 main.py:51] epoch 83, training loss: 14947.18, average training loss: 14370.43, base loss: 14315.47
[INFO 2017-06-28 14:20:29,765 main.py:51] epoch 84, training loss: 12247.58, average training loss: 14345.46, base loss: 14311.77
[INFO 2017-06-28 14:20:30,579 main.py:51] epoch 85, training loss: 13181.58, average training loss: 14331.92, base loss: 14318.57
[INFO 2017-06-28 14:20:31,427 main.py:51] epoch 86, training loss: 12175.62, average training loss: 14307.14, base loss: 14308.57
[INFO 2017-06-28 14:20:32,080 main.py:51] epoch 87, training loss: 13201.23, average training loss: 14294.57, base loss: 14314.81
[INFO 2017-06-28 14:20:32,866 main.py:51] epoch 88, training loss: 12092.08, average training loss: 14269.83, base loss: 14305.01
[INFO 2017-06-28 14:20:33,695 main.py:51] epoch 89, training loss: 11835.45, average training loss: 14242.78, base loss: 14298.90
[INFO 2017-06-28 14:20:34,523 main.py:51] epoch 90, training loss: 12234.08, average training loss: 14220.70, base loss: 14297.65
[INFO 2017-06-28 14:20:35,218 main.py:51] epoch 91, training loss: 14946.24, average training loss: 14228.59, base loss: 14322.85
[INFO 2017-06-28 14:20:36,017 main.py:51] epoch 92, training loss: 11909.99, average training loss: 14203.66, base loss: 14309.55
[INFO 2017-06-28 14:20:36,859 main.py:51] epoch 93, training loss: 12466.58, average training loss: 14185.18, base loss: 14308.07
[INFO 2017-06-28 14:20:37,738 main.py:51] epoch 94, training loss: 13366.42, average training loss: 14176.56, base loss: 14313.91
[INFO 2017-06-28 14:20:38,438 main.py:51] epoch 95, training loss: 13441.63, average training loss: 14168.90, base loss: 14315.65
[INFO 2017-06-28 14:20:39,198 main.py:51] epoch 96, training loss: 11016.64, average training loss: 14136.41, base loss: 14294.55
[INFO 2017-06-28 14:20:40,009 main.py:51] epoch 97, training loss: 12459.29, average training loss: 14119.29, base loss: 14295.99
[INFO 2017-06-28 14:20:40,782 main.py:51] epoch 98, training loss: 12658.68, average training loss: 14104.54, base loss: 14302.50
[INFO 2017-06-28 14:20:41,414 main.py:51] epoch 99, training loss: 12981.06, average training loss: 14093.31, base loss: 14308.69
[INFO 2017-06-28 14:20:41,414 main.py:53] epoch 99, testing
[INFO 2017-06-28 14:20:44,293 main.py:105] average testing loss: 14460.56, base loss: 16054.30
[INFO 2017-06-28 14:20:44,293 main.py:106] improve_loss: 1593.74, improve_percent: 0.10
[INFO 2017-06-28 14:20:44,293 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:20:44,308 main.py:76] current best improved percent: 0.10
[INFO 2017-06-28 14:20:45,042 main.py:51] epoch 100, training loss: 12080.50, average training loss: 14073.38, base loss: 14305.33
[INFO 2017-06-28 14:20:45,906 main.py:51] epoch 101, training loss: 12576.07, average training loss: 14058.70, base loss: 14307.08
[INFO 2017-06-28 14:20:46,603 main.py:51] epoch 102, training loss: 12780.36, average training loss: 14046.29, base loss: 14314.32
[INFO 2017-06-28 14:20:47,327 main.py:51] epoch 103, training loss: 12864.15, average training loss: 14034.92, base loss: 14319.17
[INFO 2017-06-28 14:20:48,139 main.py:51] epoch 104, training loss: 11708.38, average training loss: 14012.76, base loss: 14309.59
[INFO 2017-06-28 14:20:49,008 main.py:51] epoch 105, training loss: 11694.14, average training loss: 13990.89, base loss: 14302.49
[INFO 2017-06-28 14:20:49,721 main.py:51] epoch 106, training loss: 11415.59, average training loss: 13966.82, base loss: 14289.26
[INFO 2017-06-28 14:20:50,462 main.py:51] epoch 107, training loss: 14139.00, average training loss: 13968.41, base loss: 14302.08
[INFO 2017-06-28 14:20:51,277 main.py:51] epoch 108, training loss: 11304.04, average training loss: 13943.97, base loss: 14286.95
[INFO 2017-06-28 14:20:52,119 main.py:51] epoch 109, training loss: 11593.54, average training loss: 13922.60, base loss: 14281.22
[INFO 2017-06-28 14:20:52,757 main.py:51] epoch 110, training loss: 12627.14, average training loss: 13910.93, base loss: 14284.50
[INFO 2017-06-28 14:20:53,548 main.py:51] epoch 111, training loss: 14552.72, average training loss: 13916.66, base loss: 14307.83
[INFO 2017-06-28 14:20:54,373 main.py:51] epoch 112, training loss: 12921.37, average training loss: 13907.85, base loss: 14313.19
[INFO 2017-06-28 14:20:55,233 main.py:51] epoch 113, training loss: 10631.91, average training loss: 13879.12, base loss: 14291.56
[INFO 2017-06-28 14:20:55,865 main.py:51] epoch 114, training loss: 13448.54, average training loss: 13875.37, base loss: 14302.18
[INFO 2017-06-28 14:20:56,608 main.py:51] epoch 115, training loss: 12128.27, average training loss: 13860.31, base loss: 14296.01
[INFO 2017-06-28 14:20:57,477 main.py:51] epoch 116, training loss: 14038.18, average training loss: 13861.83, base loss: 14314.04
[INFO 2017-06-28 14:20:58,220 main.py:51] epoch 117, training loss: 10988.99, average training loss: 13837.49, base loss: 14298.55
[INFO 2017-06-28 14:20:58,928 main.py:51] epoch 118, training loss: 14175.68, average training loss: 13840.33, base loss: 14314.18
[INFO 2017-06-28 14:20:59,725 main.py:51] epoch 119, training loss: 11394.80, average training loss: 13819.95, base loss: 14297.93
[INFO 2017-06-28 14:21:00,526 main.py:51] epoch 120, training loss: 10781.38, average training loss: 13794.84, base loss: 14279.20
[INFO 2017-06-28 14:21:01,208 main.py:51] epoch 121, training loss: 12540.36, average training loss: 13784.55, base loss: 14280.71
[INFO 2017-06-28 14:21:01,956 main.py:51] epoch 122, training loss: 12276.88, average training loss: 13772.30, base loss: 14281.39
[INFO 2017-06-28 14:21:02,778 main.py:51] epoch 123, training loss: 13266.44, average training loss: 13768.22, base loss: 14296.36
[INFO 2017-06-28 14:21:03,523 main.py:51] epoch 124, training loss: 12068.09, average training loss: 13754.62, base loss: 14293.08
[INFO 2017-06-28 14:21:04,198 main.py:51] epoch 125, training loss: 11620.36, average training loss: 13737.68, base loss: 14288.03
[INFO 2017-06-28 14:21:05,010 main.py:51] epoch 126, training loss: 12840.91, average training loss: 13730.62, base loss: 14296.91
[INFO 2017-06-28 14:21:05,709 main.py:51] epoch 127, training loss: 11372.69, average training loss: 13712.20, base loss: 14284.67
[INFO 2017-06-28 14:21:06,438 main.py:51] epoch 128, training loss: 10810.14, average training loss: 13689.70, base loss: 14266.07
[INFO 2017-06-28 14:21:07,227 main.py:51] epoch 129, training loss: 11949.11, average training loss: 13676.31, base loss: 14268.36
[INFO 2017-06-28 14:21:08,077 main.py:51] epoch 130, training loss: 13042.86, average training loss: 13671.47, base loss: 14274.00
[INFO 2017-06-28 14:21:08,701 main.py:51] epoch 131, training loss: 12697.77, average training loss: 13664.10, base loss: 14281.48
[INFO 2017-06-28 14:21:09,507 main.py:51] epoch 132, training loss: 12164.62, average training loss: 13652.82, base loss: 14279.90
[INFO 2017-06-28 14:21:10,326 main.py:51] epoch 133, training loss: 10723.82, average training loss: 13630.97, base loss: 14266.38
[INFO 2017-06-28 14:21:11,121 main.py:51] epoch 134, training loss: 14051.33, average training loss: 13634.08, base loss: 14288.41
[INFO 2017-06-28 14:21:11,709 main.py:51] epoch 135, training loss: 12418.86, average training loss: 13625.14, base loss: 14292.96
[INFO 2017-06-28 14:21:12,503 main.py:51] epoch 136, training loss: 13596.48, average training loss: 13624.93, base loss: 14299.75
[INFO 2017-06-28 14:21:13,293 main.py:51] epoch 137, training loss: 11958.99, average training loss: 13612.86, base loss: 14299.04
[INFO 2017-06-28 14:21:13,945 main.py:51] epoch 138, training loss: 12398.84, average training loss: 13604.13, base loss: 14300.61
[INFO 2017-06-28 14:21:14,762 main.py:51] epoch 139, training loss: 11869.88, average training loss: 13591.74, base loss: 14300.43
[INFO 2017-06-28 14:21:15,583 main.py:51] epoch 140, training loss: 12221.17, average training loss: 13582.02, base loss: 14306.73
[INFO 2017-06-28 14:21:16,305 main.py:51] epoch 141, training loss: 12435.82, average training loss: 13573.95, base loss: 14305.22
[INFO 2017-06-28 14:21:16,990 main.py:51] epoch 142, training loss: 11435.45, average training loss: 13558.99, base loss: 14298.47
[INFO 2017-06-28 14:21:17,796 main.py:51] epoch 143, training loss: 13836.70, average training loss: 13560.92, base loss: 14315.04
[INFO 2017-06-28 14:21:18,621 main.py:51] epoch 144, training loss: 13223.08, average training loss: 13558.59, base loss: 14324.72
[INFO 2017-06-28 14:21:19,257 main.py:51] epoch 145, training loss: 11786.91, average training loss: 13546.46, base loss: 14320.58
[INFO 2017-06-28 14:21:20,034 main.py:51] epoch 146, training loss: 12257.69, average training loss: 13537.69, base loss: 14319.22
[INFO 2017-06-28 14:21:20,878 main.py:51] epoch 147, training loss: 15204.11, average training loss: 13548.95, base loss: 14346.52
[INFO 2017-06-28 14:21:21,656 main.py:51] epoch 148, training loss: 13883.67, average training loss: 13551.20, base loss: 14358.31
[INFO 2017-06-28 14:21:22,357 main.py:51] epoch 149, training loss: 14148.59, average training loss: 13555.18, base loss: 14375.08
[INFO 2017-06-28 14:21:23,162 main.py:51] epoch 150, training loss: 13542.67, average training loss: 13555.10, base loss: 14386.53
[INFO 2017-06-28 14:21:24,003 main.py:51] epoch 151, training loss: 13988.17, average training loss: 13557.95, base loss: 14405.40
[INFO 2017-06-28 14:21:24,729 main.py:51] epoch 152, training loss: 12517.84, average training loss: 13551.15, base loss: 14407.82
[INFO 2017-06-28 14:21:25,431 main.py:51] epoch 153, training loss: 12278.40, average training loss: 13542.88, base loss: 14412.02
[INFO 2017-06-28 14:21:26,223 main.py:51] epoch 154, training loss: 13354.40, average training loss: 13541.67, base loss: 14428.55
[INFO 2017-06-28 14:21:27,058 main.py:51] epoch 155, training loss: 12551.54, average training loss: 13535.32, base loss: 14429.19
[INFO 2017-06-28 14:21:27,671 main.py:51] epoch 156, training loss: 13199.56, average training loss: 13533.18, base loss: 14435.05
[INFO 2017-06-28 14:21:28,453 main.py:51] epoch 157, training loss: 14192.33, average training loss: 13537.35, base loss: 14450.78
[INFO 2017-06-28 14:21:29,308 main.py:51] epoch 158, training loss: 10681.75, average training loss: 13519.39, base loss: 14435.88
[INFO 2017-06-28 14:21:29,933 main.py:51] epoch 159, training loss: 10877.41, average training loss: 13502.88, base loss: 14425.93
[INFO 2017-06-28 14:21:30,726 main.py:51] epoch 160, training loss: 13697.06, average training loss: 13504.09, base loss: 14434.18
[INFO 2017-06-28 14:21:31,553 main.py:51] epoch 161, training loss: 12017.55, average training loss: 13494.91, base loss: 14431.64
[INFO 2017-06-28 14:21:32,376 main.py:51] epoch 162, training loss: 10897.24, average training loss: 13478.97, base loss: 14424.78
[INFO 2017-06-28 14:21:33,027 main.py:51] epoch 163, training loss: 11964.50, average training loss: 13469.74, base loss: 14425.56
[INFO 2017-06-28 14:21:33,778 main.py:51] epoch 164, training loss: 11534.45, average training loss: 13458.01, base loss: 14421.49
[INFO 2017-06-28 14:21:34,595 main.py:51] epoch 165, training loss: 12957.90, average training loss: 13455.00, base loss: 14426.62
[INFO 2017-06-28 14:21:35,229 main.py:51] epoch 166, training loss: 12233.15, average training loss: 13447.68, base loss: 14429.49
[INFO 2017-06-28 14:21:36,012 main.py:51] epoch 167, training loss: 13579.06, average training loss: 13448.46, base loss: 14445.33
[INFO 2017-06-28 14:21:36,838 main.py:51] epoch 168, training loss: 10360.20, average training loss: 13430.19, base loss: 14429.11
[INFO 2017-06-28 14:21:37,628 main.py:51] epoch 169, training loss: 12190.06, average training loss: 13422.89, base loss: 14434.02
[INFO 2017-06-28 14:21:38,277 main.py:51] epoch 170, training loss: 12489.03, average training loss: 13417.43, base loss: 14440.14
[INFO 2017-06-28 14:21:39,048 main.py:51] epoch 171, training loss: 13373.10, average training loss: 13417.18, base loss: 14447.79
[INFO 2017-06-28 14:21:39,859 main.py:51] epoch 172, training loss: 13139.53, average training loss: 13415.57, base loss: 14460.45
[INFO 2017-06-28 14:21:40,524 main.py:51] epoch 173, training loss: 12815.33, average training loss: 13412.12, base loss: 14460.74
[INFO 2017-06-28 14:21:41,300 main.py:51] epoch 174, training loss: 12926.37, average training loss: 13409.35, base loss: 14467.27
[INFO 2017-06-28 14:21:42,133 main.py:51] epoch 175, training loss: 11995.66, average training loss: 13401.31, base loss: 14468.94
[INFO 2017-06-28 14:21:42,942 main.py:51] epoch 176, training loss: 11011.36, average training loss: 13387.81, base loss: 14467.67
[INFO 2017-06-28 14:21:43,619 main.py:51] epoch 177, training loss: 11098.55, average training loss: 13374.95, base loss: 14461.89
[INFO 2017-06-28 14:21:44,416 main.py:51] epoch 178, training loss: 11953.76, average training loss: 13367.01, base loss: 14461.97
[INFO 2017-06-28 14:21:45,260 main.py:51] epoch 179, training loss: 11090.87, average training loss: 13354.36, base loss: 14452.29
[INFO 2017-06-28 14:21:45,925 main.py:51] epoch 180, training loss: 10827.59, average training loss: 13340.40, base loss: 14444.63
[INFO 2017-06-28 14:21:46,664 main.py:51] epoch 181, training loss: 11726.00, average training loss: 13331.53, base loss: 14444.63
[INFO 2017-06-28 14:21:47,496 main.py:51] epoch 182, training loss: 12020.92, average training loss: 13324.37, base loss: 14446.10
[INFO 2017-06-28 14:21:48,169 main.py:51] epoch 183, training loss: 11779.00, average training loss: 13315.97, base loss: 14444.91
[INFO 2017-06-28 14:21:48,915 main.py:51] epoch 184, training loss: 10390.29, average training loss: 13300.16, base loss: 14437.58
[INFO 2017-06-28 14:21:49,724 main.py:51] epoch 185, training loss: 14475.01, average training loss: 13306.48, base loss: 14453.44
[INFO 2017-06-28 14:21:50,452 main.py:51] epoch 186, training loss: 13523.75, average training loss: 13307.64, base loss: 14461.35
[INFO 2017-06-28 14:21:51,155 main.py:51] epoch 187, training loss: 13201.04, average training loss: 13307.07, base loss: 14474.72
[INFO 2017-06-28 14:21:51,972 main.py:51] epoch 188, training loss: 12021.34, average training loss: 13300.27, base loss: 14477.47
[INFO 2017-06-28 14:21:52,822 main.py:51] epoch 189, training loss: 11905.50, average training loss: 13292.93, base loss: 14481.47
[INFO 2017-06-28 14:21:53,530 main.py:51] epoch 190, training loss: 12470.04, average training loss: 13288.62, base loss: 14490.36
[INFO 2017-06-28 14:21:54,260 main.py:51] epoch 191, training loss: 11457.97, average training loss: 13279.08, base loss: 14486.18
[INFO 2017-06-28 14:21:55,048 main.py:51] epoch 192, training loss: 11771.30, average training loss: 13271.27, base loss: 14487.65
[INFO 2017-06-28 14:21:55,869 main.py:51] epoch 193, training loss: 11077.46, average training loss: 13259.96, base loss: 14482.41
[INFO 2017-06-28 14:21:56,507 main.py:51] epoch 194, training loss: 12014.15, average training loss: 13253.57, base loss: 14485.55
[INFO 2017-06-28 14:21:57,291 main.py:51] epoch 195, training loss: 11499.95, average training loss: 13244.63, base loss: 14483.61
[INFO 2017-06-28 14:21:58,132 main.py:51] epoch 196, training loss: 13959.47, average training loss: 13248.26, base loss: 14497.53
[INFO 2017-06-28 14:21:58,831 main.py:51] epoch 197, training loss: 13208.69, average training loss: 13248.06, base loss: 14509.58
[INFO 2017-06-28 14:21:59,554 main.py:51] epoch 198, training loss: 12256.01, average training loss: 13243.07, base loss: 14513.13
[INFO 2017-06-28 14:22:00,341 main.py:51] epoch 199, training loss: 13379.99, average training loss: 13243.76, base loss: 14525.37
[INFO 2017-06-28 14:22:00,342 main.py:53] epoch 199, testing
[INFO 2017-06-28 14:22:03,209 main.py:105] average testing loss: 12887.73, base loss: 15137.41
[INFO 2017-06-28 14:22:03,209 main.py:106] improve_loss: 2249.69, improve_percent: 0.15
[INFO 2017-06-28 14:22:03,210 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:22:03,223 main.py:76] current best improved percent: 0.15
[INFO 2017-06-28 14:22:03,966 main.py:51] epoch 200, training loss: 12034.25, average training loss: 13237.74, base loss: 14519.33
[INFO 2017-06-28 14:22:04,652 main.py:51] epoch 201, training loss: 11697.04, average training loss: 13230.11, base loss: 14515.79
[INFO 2017-06-28 14:22:05,468 main.py:51] epoch 202, training loss: 12023.74, average training loss: 13224.17, base loss: 14520.99
[INFO 2017-06-28 14:22:06,222 main.py:51] epoch 203, training loss: 11408.75, average training loss: 13215.27, base loss: 14518.68
[INFO 2017-06-28 14:22:06,897 main.py:51] epoch 204, training loss: 11567.36, average training loss: 13207.23, base loss: 14516.13
[INFO 2017-06-28 14:22:07,700 main.py:51] epoch 205, training loss: 12160.11, average training loss: 13202.15, base loss: 14520.31
[INFO 2017-06-28 14:22:08,493 main.py:51] epoch 206, training loss: 11096.08, average training loss: 13191.97, base loss: 14518.04
[INFO 2017-06-28 14:22:09,137 main.py:51] epoch 207, training loss: 13141.04, average training loss: 13191.73, base loss: 14522.28
[INFO 2017-06-28 14:22:09,808 main.py:51] epoch 208, training loss: 10899.38, average training loss: 13180.76, base loss: 14514.86
[INFO 2017-06-28 14:22:10,408 main.py:51] epoch 209, training loss: 10789.32, average training loss: 13169.37, base loss: 14509.73
[INFO 2017-06-28 14:22:11,043 main.py:51] epoch 210, training loss: 12857.47, average training loss: 13167.89, base loss: 14513.97
[INFO 2017-06-28 14:22:11,706 main.py:51] epoch 211, training loss: 10193.52, average training loss: 13153.86, base loss: 14504.86
[INFO 2017-06-28 14:22:12,304 main.py:51] epoch 212, training loss: 12193.65, average training loss: 13149.36, base loss: 14509.65
[INFO 2017-06-28 14:22:12,972 main.py:51] epoch 213, training loss: 9436.85, average training loss: 13132.01, base loss: 14495.87
[INFO 2017-06-28 14:22:13,791 main.py:51] epoch 214, training loss: 11285.56, average training loss: 13123.42, base loss: 14491.29
[INFO 2017-06-28 14:22:14,623 main.py:51] epoch 215, training loss: 12150.34, average training loss: 13118.91, base loss: 14493.67
[INFO 2017-06-28 14:22:15,487 main.py:51] epoch 216, training loss: 11906.31, average training loss: 13113.33, base loss: 14494.32
[INFO 2017-06-28 14:22:16,124 main.py:51] epoch 217, training loss: 9753.49, average training loss: 13097.91, base loss: 14480.83
[INFO 2017-06-28 14:22:16,856 main.py:51] epoch 218, training loss: 10442.69, average training loss: 13085.79, base loss: 14473.02
[INFO 2017-06-28 14:22:17,674 main.py:51] epoch 219, training loss: 12171.45, average training loss: 13081.63, base loss: 14474.23
[INFO 2017-06-28 14:22:18,352 main.py:51] epoch 220, training loss: 9745.08, average training loss: 13066.54, base loss: 14459.58
[INFO 2017-06-28 14:22:19,104 main.py:51] epoch 221, training loss: 10500.96, average training loss: 13054.98, base loss: 14452.68
[INFO 2017-06-28 14:22:19,905 main.py:51] epoch 222, training loss: 10619.00, average training loss: 13044.06, base loss: 14445.82
[INFO 2017-06-28 14:22:20,740 main.py:51] epoch 223, training loss: 11750.62, average training loss: 13038.28, base loss: 14448.21
[INFO 2017-06-28 14:22:21,355 main.py:51] epoch 224, training loss: 10823.25, average training loss: 13028.44, base loss: 14442.53
[INFO 2017-06-28 14:22:22,090 main.py:51] epoch 225, training loss: 10510.41, average training loss: 13017.30, base loss: 14434.96
[INFO 2017-06-28 14:22:22,880 main.py:51] epoch 226, training loss: 10227.06, average training loss: 13005.00, base loss: 14423.86
[INFO 2017-06-28 14:22:23,519 main.py:51] epoch 227, training loss: 11395.56, average training loss: 12997.95, base loss: 14422.15
[INFO 2017-06-28 14:22:24,308 main.py:51] epoch 228, training loss: 12273.56, average training loss: 12994.78, base loss: 14423.29
[INFO 2017-06-28 14:22:25,139 main.py:51] epoch 229, training loss: 12506.93, average training loss: 12992.66, base loss: 14428.41
[INFO 2017-06-28 14:22:25,985 main.py:51] epoch 230, training loss: 10706.85, average training loss: 12982.77, base loss: 14420.05
[INFO 2017-06-28 14:22:26,640 main.py:51] epoch 231, training loss: 12197.26, average training loss: 12979.38, base loss: 14425.27
[INFO 2017-06-28 14:22:27,421 main.py:51] epoch 232, training loss: 9154.54, average training loss: 12962.96, base loss: 14415.44
[INFO 2017-06-28 14:22:28,279 main.py:51] epoch 233, training loss: 11479.70, average training loss: 12956.63, base loss: 14414.27
[INFO 2017-06-28 14:22:28,972 main.py:51] epoch 234, training loss: 12865.99, average training loss: 12956.24, base loss: 14422.54
[INFO 2017-06-28 14:22:29,721 main.py:51] epoch 235, training loss: 12506.55, average training loss: 12954.33, base loss: 14431.23
[INFO 2017-06-28 14:22:30,520 main.py:51] epoch 236, training loss: 12299.02, average training loss: 12951.57, base loss: 14433.41
[INFO 2017-06-28 14:22:31,356 main.py:51] epoch 237, training loss: 11878.14, average training loss: 12947.06, base loss: 14432.98
[INFO 2017-06-28 14:22:31,984 main.py:51] epoch 238, training loss: 10595.20, average training loss: 12937.22, base loss: 14422.58
[INFO 2017-06-28 14:22:32,794 main.py:51] epoch 239, training loss: 11096.74, average training loss: 12929.55, base loss: 14423.06
[INFO 2017-06-28 14:22:33,630 main.py:51] epoch 240, training loss: 12416.77, average training loss: 12927.42, base loss: 14426.29
[INFO 2017-06-28 14:22:34,380 main.py:51] epoch 241, training loss: 11907.95, average training loss: 12923.21, base loss: 14425.70
[INFO 2017-06-28 14:22:35,077 main.py:51] epoch 242, training loss: 10966.25, average training loss: 12915.16, base loss: 14426.20
[INFO 2017-06-28 14:22:35,864 main.py:51] epoch 243, training loss: 11095.67, average training loss: 12907.70, base loss: 14419.43
[INFO 2017-06-28 14:22:36,644 main.py:51] epoch 244, training loss: 11864.37, average training loss: 12903.44, base loss: 14420.07
[INFO 2017-06-28 14:22:37,309 main.py:51] epoch 245, training loss: 11195.27, average training loss: 12896.50, base loss: 14418.16
[INFO 2017-06-28 14:22:38,100 main.py:51] epoch 246, training loss: 10041.58, average training loss: 12884.94, base loss: 14414.96
[INFO 2017-06-28 14:22:38,938 main.py:51] epoch 247, training loss: 10245.05, average training loss: 12874.29, base loss: 14408.42
[INFO 2017-06-28 14:22:39,622 main.py:51] epoch 248, training loss: 12628.64, average training loss: 12873.31, base loss: 14417.43
[INFO 2017-06-28 14:22:40,353 main.py:51] epoch 249, training loss: 13188.26, average training loss: 12874.57, base loss: 14424.60
[INFO 2017-06-28 14:22:41,176 main.py:51] epoch 250, training loss: 11200.99, average training loss: 12867.90, base loss: 14421.49
[INFO 2017-06-28 14:22:41,996 main.py:51] epoch 251, training loss: 10792.25, average training loss: 12859.66, base loss: 14416.63
[INFO 2017-06-28 14:22:42,652 main.py:51] epoch 252, training loss: 11358.75, average training loss: 12853.73, base loss: 14416.55
[INFO 2017-06-28 14:22:43,424 main.py:51] epoch 253, training loss: 12143.91, average training loss: 12850.94, base loss: 14420.50
[INFO 2017-06-28 14:22:44,242 main.py:51] epoch 254, training loss: 15126.92, average training loss: 12859.86, base loss: 14436.59
[INFO 2017-06-28 14:22:44,906 main.py:51] epoch 255, training loss: 11447.08, average training loss: 12854.34, base loss: 14435.93
[INFO 2017-06-28 14:22:45,630 main.py:51] epoch 256, training loss: 9612.25, average training loss: 12841.73, base loss: 14426.73
[INFO 2017-06-28 14:22:46,473 main.py:51] epoch 257, training loss: 11715.61, average training loss: 12837.36, base loss: 14426.44
[INFO 2017-06-28 14:22:47,156 main.py:51] epoch 258, training loss: 11276.07, average training loss: 12831.33, base loss: 14426.40
[INFO 2017-06-28 14:22:47,895 main.py:51] epoch 259, training loss: 13405.01, average training loss: 12833.54, base loss: 14434.10
[INFO 2017-06-28 14:22:48,712 main.py:51] epoch 260, training loss: 10204.46, average training loss: 12823.47, base loss: 14427.84
[INFO 2017-06-28 14:22:49,553 main.py:51] epoch 261, training loss: 11021.58, average training loss: 12816.59, base loss: 14422.96
[INFO 2017-06-28 14:22:50,146 main.py:51] epoch 262, training loss: 10625.27, average training loss: 12808.26, base loss: 14418.51
[INFO 2017-06-28 14:22:50,897 main.py:51] epoch 263, training loss: 10188.79, average training loss: 12798.34, base loss: 14412.06
[INFO 2017-06-28 14:22:51,696 main.py:51] epoch 264, training loss: 11236.53, average training loss: 12792.44, base loss: 14410.38
[INFO 2017-06-28 14:22:52,383 main.py:51] epoch 265, training loss: 10561.77, average training loss: 12784.06, base loss: 14402.05
[INFO 2017-06-28 14:22:53,141 main.py:51] epoch 266, training loss: 13516.42, average training loss: 12786.80, base loss: 14409.99
[INFO 2017-06-28 14:22:53,970 main.py:51] epoch 267, training loss: 11049.71, average training loss: 12780.32, base loss: 14409.40
[INFO 2017-06-28 14:22:54,820 main.py:51] epoch 268, training loss: 13800.89, average training loss: 12784.11, base loss: 14418.12
[INFO 2017-06-28 14:22:55,462 main.py:51] epoch 269, training loss: 11657.30, average training loss: 12779.94, base loss: 14419.73
[INFO 2017-06-28 14:22:56,217 main.py:51] epoch 270, training loss: 12182.34, average training loss: 12777.73, base loss: 14421.88
[INFO 2017-06-28 14:22:57,032 main.py:51] epoch 271, training loss: 11358.95, average training loss: 12772.52, base loss: 14417.84
[INFO 2017-06-28 14:22:57,690 main.py:51] epoch 272, training loss: 11452.18, average training loss: 12767.68, base loss: 14418.90
[INFO 2017-06-28 14:22:58,434 main.py:51] epoch 273, training loss: 10130.58, average training loss: 12758.06, base loss: 14413.26
[INFO 2017-06-28 14:22:59,255 main.py:51] epoch 274, training loss: 11662.94, average training loss: 12754.07, base loss: 14412.92
[INFO 2017-06-28 14:23:00,040 main.py:51] epoch 275, training loss: 11578.60, average training loss: 12749.82, base loss: 14413.51
[INFO 2017-06-28 14:23:00,714 main.py:51] epoch 276, training loss: 9430.95, average training loss: 12737.83, base loss: 14405.67
[INFO 2017-06-28 14:23:01,494 main.py:51] epoch 277, training loss: 12093.69, average training loss: 12735.52, base loss: 14408.57
[INFO 2017-06-28 14:23:02,329 main.py:51] epoch 278, training loss: 12421.61, average training loss: 12734.39, base loss: 14413.32
[INFO 2017-06-28 14:23:02,965 main.py:51] epoch 279, training loss: 11703.92, average training loss: 12730.71, base loss: 14414.26
[INFO 2017-06-28 14:23:03,735 main.py:51] epoch 280, training loss: 10053.79, average training loss: 12721.18, base loss: 14407.95
[INFO 2017-06-28 14:23:04,566 main.py:51] epoch 281, training loss: 10909.28, average training loss: 12714.76, base loss: 14404.55
[INFO 2017-06-28 14:23:05,411 main.py:51] epoch 282, training loss: 10583.76, average training loss: 12707.23, base loss: 14401.66
[INFO 2017-06-28 14:23:06,015 main.py:51] epoch 283, training loss: 12965.45, average training loss: 12708.14, base loss: 14405.10
[INFO 2017-06-28 14:23:06,773 main.py:51] epoch 284, training loss: 10924.09, average training loss: 12701.88, base loss: 14403.23
[INFO 2017-06-28 14:23:07,569 main.py:51] epoch 285, training loss: 9843.30, average training loss: 12691.88, base loss: 14396.87
[INFO 2017-06-28 14:23:08,220 main.py:51] epoch 286, training loss: 10088.23, average training loss: 12682.81, base loss: 14390.92
[INFO 2017-06-28 14:23:08,979 main.py:51] epoch 287, training loss: 10330.48, average training loss: 12674.64, base loss: 14385.12
[INFO 2017-06-28 14:23:09,768 main.py:51] epoch 288, training loss: 10946.57, average training loss: 12668.66, base loss: 14381.98
[INFO 2017-06-28 14:23:10,381 main.py:51] epoch 289, training loss: 10609.59, average training loss: 12661.56, base loss: 14379.19
[INFO 2017-06-28 14:23:11,189 main.py:51] epoch 290, training loss: 12940.14, average training loss: 12662.52, base loss: 14384.51
[INFO 2017-06-28 14:23:12,022 main.py:51] epoch 291, training loss: 12402.48, average training loss: 12661.63, base loss: 14389.18
[INFO 2017-06-28 14:23:12,818 main.py:51] epoch 292, training loss: 10959.59, average training loss: 12655.82, base loss: 14386.54
[INFO 2017-06-28 14:23:13,459 main.py:51] epoch 293, training loss: 11271.25, average training loss: 12651.11, base loss: 14386.62
[INFO 2017-06-28 14:23:14,254 main.py:51] epoch 294, training loss: 12401.08, average training loss: 12650.27, base loss: 14392.24
[INFO 2017-06-28 14:23:15,088 main.py:51] epoch 295, training loss: 12657.41, average training loss: 12650.29, base loss: 14397.04
[INFO 2017-06-28 14:23:15,751 main.py:51] epoch 296, training loss: 12165.52, average training loss: 12648.66, base loss: 14402.02
[INFO 2017-06-28 14:23:16,512 main.py:51] epoch 297, training loss: 12485.82, average training loss: 12648.11, base loss: 14405.97
[INFO 2017-06-28 14:23:17,335 main.py:51] epoch 298, training loss: 10900.76, average training loss: 12642.27, base loss: 14403.06
[INFO 2017-06-28 14:23:18,055 main.py:51] epoch 299, training loss: 12033.30, average training loss: 12640.24, base loss: 14408.72
[INFO 2017-06-28 14:23:18,055 main.py:53] epoch 299, testing
[INFO 2017-06-28 14:23:20,905 main.py:105] average testing loss: 12258.90, base loss: 14975.58
[INFO 2017-06-28 14:23:20,905 main.py:106] improve_loss: 2716.68, improve_percent: 0.18
[INFO 2017-06-28 14:23:20,905 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:23:20,921 main.py:76] current best improved percent: 0.18
[INFO 2017-06-28 14:23:21,588 main.py:51] epoch 300, training loss: 9458.60, average training loss: 12629.67, base loss: 14398.79
[INFO 2017-06-28 14:23:22,420 main.py:51] epoch 301, training loss: 13185.81, average training loss: 12631.51, base loss: 14407.13
[INFO 2017-06-28 14:23:23,161 main.py:51] epoch 302, training loss: 12673.89, average training loss: 12631.65, base loss: 14412.53
[INFO 2017-06-28 14:23:23,862 main.py:51] epoch 303, training loss: 11455.51, average training loss: 12627.78, base loss: 14410.87
[INFO 2017-06-28 14:23:24,661 main.py:51] epoch 304, training loss: 10556.10, average training loss: 12620.99, base loss: 14407.59
[INFO 2017-06-28 14:23:25,452 main.py:51] epoch 305, training loss: 10611.71, average training loss: 12614.42, base loss: 14401.89
[INFO 2017-06-28 14:23:26,052 main.py:51] epoch 306, training loss: 10460.65, average training loss: 12607.40, base loss: 14397.44
[INFO 2017-06-28 14:23:26,832 main.py:51] epoch 307, training loss: 10105.37, average training loss: 12599.28, base loss: 14393.47
[INFO 2017-06-28 14:23:27,594 main.py:51] epoch 308, training loss: 12591.13, average training loss: 12599.25, base loss: 14395.68
[INFO 2017-06-28 14:23:28,262 main.py:51] epoch 309, training loss: 12591.85, average training loss: 12599.23, base loss: 14399.47
[INFO 2017-06-28 14:23:29,014 main.py:51] epoch 310, training loss: 11972.65, average training loss: 12597.22, base loss: 14401.66
[INFO 2017-06-28 14:23:29,826 main.py:51] epoch 311, training loss: 10325.18, average training loss: 12589.93, base loss: 14395.40
[INFO 2017-06-28 14:23:30,486 main.py:51] epoch 312, training loss: 9757.78, average training loss: 12580.89, base loss: 14388.27
[INFO 2017-06-28 14:23:31,257 main.py:51] epoch 313, training loss: 13231.56, average training loss: 12582.96, base loss: 14397.04
[INFO 2017-06-28 14:23:32,068 main.py:51] epoch 314, training loss: 12091.32, average training loss: 12581.40, base loss: 14400.53
[INFO 2017-06-28 14:23:32,842 main.py:51] epoch 315, training loss: 10834.86, average training loss: 12575.87, base loss: 14399.17
[INFO 2017-06-28 14:23:33,536 main.py:51] epoch 316, training loss: 12776.42, average training loss: 12576.50, base loss: 14407.18
[INFO 2017-06-28 14:23:34,315 main.py:51] epoch 317, training loss: 9538.44, average training loss: 12566.95, base loss: 14398.26
[INFO 2017-06-28 14:23:35,150 main.py:51] epoch 318, training loss: 11158.55, average training loss: 12562.53, base loss: 14396.42
[INFO 2017-06-28 14:23:35,819 main.py:51] epoch 319, training loss: 11448.64, average training loss: 12559.05, base loss: 14397.40
[INFO 2017-06-28 14:23:36,619 main.py:51] epoch 320, training loss: 12462.01, average training loss: 12558.75, base loss: 14402.71
[INFO 2017-06-28 14:23:37,449 main.py:51] epoch 321, training loss: 11005.26, average training loss: 12553.93, base loss: 14398.83
[INFO 2017-06-28 14:23:38,226 main.py:51] epoch 322, training loss: 11682.29, average training loss: 12551.23, base loss: 14401.44
[INFO 2017-06-28 14:23:38,918 main.py:51] epoch 323, training loss: 11372.11, average training loss: 12547.59, base loss: 14402.31
[INFO 2017-06-28 14:23:39,689 main.py:51] epoch 324, training loss: 10959.29, average training loss: 12542.70, base loss: 14399.42
[INFO 2017-06-28 14:23:40,502 main.py:51] epoch 325, training loss: 11953.02, average training loss: 12540.89, base loss: 14405.15
[INFO 2017-06-28 14:23:41,180 main.py:51] epoch 326, training loss: 9863.54, average training loss: 12532.71, base loss: 14395.72
[INFO 2017-06-28 14:23:41,925 main.py:51] epoch 327, training loss: 11584.31, average training loss: 12529.81, base loss: 14397.42
[INFO 2017-06-28 14:23:42,740 main.py:51] epoch 328, training loss: 10779.89, average training loss: 12524.49, base loss: 14398.00
[INFO 2017-06-28 14:23:43,490 main.py:51] epoch 329, training loss: 11866.39, average training loss: 12522.50, base loss: 14399.01
[INFO 2017-06-28 14:23:44,183 main.py:51] epoch 330, training loss: 10655.53, average training loss: 12516.86, base loss: 14396.18
[INFO 2017-06-28 14:23:44,980 main.py:51] epoch 331, training loss: 10620.70, average training loss: 12511.15, base loss: 14390.73
[INFO 2017-06-28 14:23:45,812 main.py:51] epoch 332, training loss: 11698.62, average training loss: 12508.71, base loss: 14393.34
[INFO 2017-06-28 14:23:46,463 main.py:51] epoch 333, training loss: 9908.42, average training loss: 12500.92, base loss: 14389.28
[INFO 2017-06-28 14:23:47,225 main.py:51] epoch 334, training loss: 14599.71, average training loss: 12507.19, base loss: 14401.77
[INFO 2017-06-28 14:23:48,063 main.py:51] epoch 335, training loss: 10891.38, average training loss: 12502.38, base loss: 14401.40
[INFO 2017-06-28 14:23:48,868 main.py:51] epoch 336, training loss: 9832.63, average training loss: 12494.46, base loss: 14396.31
[INFO 2017-06-28 14:23:49,535 main.py:51] epoch 337, training loss: 13807.75, average training loss: 12498.34, base loss: 14408.16
[INFO 2017-06-28 14:23:50,332 main.py:51] epoch 338, training loss: 10548.99, average training loss: 12492.59, base loss: 14402.77
[INFO 2017-06-28 14:23:51,162 main.py:51] epoch 339, training loss: 10707.11, average training loss: 12487.34, base loss: 14401.61
[INFO 2017-06-28 14:23:51,836 main.py:51] epoch 340, training loss: 12382.81, average training loss: 12487.03, base loss: 14406.43
[INFO 2017-06-28 14:23:52,580 main.py:51] epoch 341, training loss: 10858.21, average training loss: 12482.27, base loss: 14404.74
[INFO 2017-06-28 14:23:53,411 main.py:51] epoch 342, training loss: 10764.26, average training loss: 12477.26, base loss: 14402.74
[INFO 2017-06-28 14:23:54,217 main.py:51] epoch 343, training loss: 11405.41, average training loss: 12474.15, base loss: 14401.61
[INFO 2017-06-28 14:23:54,918 main.py:51] epoch 344, training loss: 10329.06, average training loss: 12467.93, base loss: 14398.17
[INFO 2017-06-28 14:23:55,724 main.py:51] epoch 345, training loss: 12776.97, average training loss: 12468.82, base loss: 14400.65
[INFO 2017-06-28 14:23:56,560 main.py:51] epoch 346, training loss: 12282.12, average training loss: 12468.28, base loss: 14407.30
[INFO 2017-06-28 14:23:57,469 main.py:51] epoch 347, training loss: 12237.12, average training loss: 12467.62, base loss: 14410.80
[INFO 2017-06-28 14:23:58,189 main.py:51] epoch 348, training loss: 12163.60, average training loss: 12466.75, base loss: 14417.00
[INFO 2017-06-28 14:23:58,976 main.py:51] epoch 349, training loss: 11327.53, average training loss: 12463.49, base loss: 14417.57
[INFO 2017-06-28 14:23:59,631 main.py:51] epoch 350, training loss: 11205.00, average training loss: 12459.91, base loss: 14418.62
[INFO 2017-06-28 14:24:00,234 main.py:51] epoch 351, training loss: 12485.74, average training loss: 12459.98, base loss: 14426.56
[INFO 2017-06-28 14:24:00,909 main.py:51] epoch 352, training loss: 10973.39, average training loss: 12455.77, base loss: 14427.34
[INFO 2017-06-28 14:24:01,587 main.py:51] epoch 353, training loss: 11680.40, average training loss: 12453.58, base loss: 14425.68
[INFO 2017-06-28 14:24:02,257 main.py:51] epoch 354, training loss: 11488.80, average training loss: 12450.86, base loss: 14429.49
[INFO 2017-06-28 14:24:02,869 main.py:51] epoch 355, training loss: 11640.72, average training loss: 12448.59, base loss: 14429.76
[INFO 2017-06-28 14:24:03,676 main.py:51] epoch 356, training loss: 9734.43, average training loss: 12440.98, base loss: 14423.49
[INFO 2017-06-28 14:24:04,498 main.py:51] epoch 357, training loss: 10701.38, average training loss: 12436.13, base loss: 14422.85
[INFO 2017-06-28 14:24:05,281 main.py:51] epoch 358, training loss: 10345.55, average training loss: 12430.30, base loss: 14420.60
[INFO 2017-06-28 14:24:05,936 main.py:51] epoch 359, training loss: 11953.35, average training loss: 12428.98, base loss: 14422.97
[INFO 2017-06-28 14:24:06,731 main.py:51] epoch 360, training loss: 9804.65, average training loss: 12421.71, base loss: 14415.99
[INFO 2017-06-28 14:24:07,540 main.py:51] epoch 361, training loss: 13869.65, average training loss: 12425.71, base loss: 14425.20
[INFO 2017-06-28 14:24:08,194 main.py:51] epoch 362, training loss: 10251.98, average training loss: 12419.72, base loss: 14423.61
[INFO 2017-06-28 14:24:08,983 main.py:51] epoch 363, training loss: 11493.43, average training loss: 12417.17, base loss: 14424.28
[INFO 2017-06-28 14:24:09,828 main.py:51] epoch 364, training loss: 10988.91, average training loss: 12413.26, base loss: 14421.47
[INFO 2017-06-28 14:24:10,583 main.py:51] epoch 365, training loss: 10977.30, average training loss: 12409.34, base loss: 14418.65
[INFO 2017-06-28 14:24:11,273 main.py:51] epoch 366, training loss: 10662.96, average training loss: 12404.58, base loss: 14416.65
[INFO 2017-06-28 14:24:12,098 main.py:51] epoch 367, training loss: 10720.88, average training loss: 12400.00, base loss: 14417.55
[INFO 2017-06-28 14:24:12,868 main.py:51] epoch 368, training loss: 11000.66, average training loss: 12396.21, base loss: 14417.64
[INFO 2017-06-28 14:24:13,558 main.py:51] epoch 369, training loss: 13018.52, average training loss: 12397.89, base loss: 14425.17
[INFO 2017-06-28 14:24:14,364 main.py:51] epoch 370, training loss: 11015.25, average training loss: 12394.17, base loss: 14423.97
[INFO 2017-06-28 14:24:15,158 main.py:51] epoch 371, training loss: 10588.25, average training loss: 12389.31, base loss: 14422.85
[INFO 2017-06-28 14:24:15,828 main.py:51] epoch 372, training loss: 12008.47, average training loss: 12388.29, base loss: 14426.39
[INFO 2017-06-28 14:24:16,598 main.py:51] epoch 373, training loss: 13197.92, average training loss: 12390.46, base loss: 14433.38
[INFO 2017-06-28 14:24:17,380 main.py:51] epoch 374, training loss: 12647.28, average training loss: 12391.14, base loss: 14437.34
[INFO 2017-06-28 14:24:18,160 main.py:51] epoch 375, training loss: 11571.10, average training loss: 12388.96, base loss: 14434.35
[INFO 2017-06-28 14:24:18,785 main.py:51] epoch 376, training loss: 12385.08, average training loss: 12388.95, base loss: 14436.32
[INFO 2017-06-28 14:24:19,603 main.py:51] epoch 377, training loss: 11492.82, average training loss: 12386.58, base loss: 14439.57
[INFO 2017-06-28 14:24:20,454 main.py:51] epoch 378, training loss: 11142.74, average training loss: 12383.30, base loss: 14439.93
[INFO 2017-06-28 14:24:21,095 main.py:51] epoch 379, training loss: 12037.78, average training loss: 12382.39, base loss: 14442.64
[INFO 2017-06-28 14:24:21,904 main.py:51] epoch 380, training loss: 10253.68, average training loss: 12376.80, base loss: 14439.64
[INFO 2017-06-28 14:24:22,718 main.py:51] epoch 381, training loss: 10002.12, average training loss: 12370.58, base loss: 14434.14
[INFO 2017-06-28 14:24:23,453 main.py:51] epoch 382, training loss: 11222.61, average training loss: 12367.59, base loss: 14437.47
[INFO 2017-06-28 14:24:24,139 main.py:51] epoch 383, training loss: 9991.24, average training loss: 12361.40, base loss: 14434.68
[INFO 2017-06-28 14:24:24,963 main.py:51] epoch 384, training loss: 12455.53, average training loss: 12361.64, base loss: 14440.60
[INFO 2017-06-28 14:24:25,748 main.py:51] epoch 385, training loss: 10957.64, average training loss: 12358.01, base loss: 14440.91
[INFO 2017-06-28 14:24:26,452 main.py:51] epoch 386, training loss: 10206.19, average training loss: 12352.45, base loss: 14438.81
[INFO 2017-06-28 14:24:27,272 main.py:51] epoch 387, training loss: 11471.46, average training loss: 12350.17, base loss: 14439.12
[INFO 2017-06-28 14:24:28,088 main.py:51] epoch 388, training loss: 10932.35, average training loss: 12346.53, base loss: 14440.18
[INFO 2017-06-28 14:24:28,921 main.py:51] epoch 389, training loss: 11076.69, average training loss: 12343.27, base loss: 14441.11
[INFO 2017-06-28 14:24:29,598 main.py:51] epoch 390, training loss: 10146.78, average training loss: 12337.66, base loss: 14438.25
[INFO 2017-06-28 14:24:30,352 main.py:51] epoch 391, training loss: 12778.43, average training loss: 12338.78, base loss: 14445.21
[INFO 2017-06-28 14:24:31,258 main.py:51] epoch 392, training loss: 11378.07, average training loss: 12336.34, base loss: 14446.95
[INFO 2017-06-28 14:24:31,926 main.py:51] epoch 393, training loss: 12528.20, average training loss: 12336.82, base loss: 14451.50
[INFO 2017-06-28 14:24:32,679 main.py:51] epoch 394, training loss: 11047.33, average training loss: 12333.56, base loss: 14452.76
[INFO 2017-06-28 14:24:33,504 main.py:51] epoch 395, training loss: 12588.73, average training loss: 12334.20, base loss: 14459.95
[INFO 2017-06-28 14:24:34,261 main.py:51] epoch 396, training loss: 10581.44, average training loss: 12329.79, base loss: 14458.45
[INFO 2017-06-28 14:24:34,975 main.py:51] epoch 397, training loss: 10511.40, average training loss: 12325.22, base loss: 14455.42
[INFO 2017-06-28 14:24:35,851 main.py:51] epoch 398, training loss: 10817.78, average training loss: 12321.44, base loss: 14454.52
[INFO 2017-06-28 14:24:36,694 main.py:51] epoch 399, training loss: 10730.14, average training loss: 12317.46, base loss: 14452.62
[INFO 2017-06-28 14:24:36,695 main.py:53] epoch 399, testing
[INFO 2017-06-28 14:24:39,588 main.py:105] average testing loss: 12195.18, base loss: 15112.49
[INFO 2017-06-28 14:24:39,588 main.py:106] improve_loss: 2917.31, improve_percent: 0.19
[INFO 2017-06-28 14:24:39,589 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:24:39,602 main.py:76] current best improved percent: 0.19
[INFO 2017-06-28 14:24:40,419 main.py:51] epoch 400, training loss: 11535.97, average training loss: 12315.51, base loss: 14452.40
[INFO 2017-06-28 14:24:41,056 main.py:51] epoch 401, training loss: 11686.83, average training loss: 12313.95, base loss: 14454.51
[INFO 2017-06-28 14:24:41,839 main.py:51] epoch 402, training loss: 11908.29, average training loss: 12312.94, base loss: 14454.05
[INFO 2017-06-28 14:24:42,635 main.py:51] epoch 403, training loss: 10429.30, average training loss: 12308.28, base loss: 14451.43
[INFO 2017-06-28 14:24:43,317 main.py:51] epoch 404, training loss: 12279.41, average training loss: 12308.21, base loss: 14453.97
[INFO 2017-06-28 14:24:44,145 main.py:51] epoch 405, training loss: 10689.93, average training loss: 12304.22, base loss: 14453.08
[INFO 2017-06-28 14:24:44,942 main.py:51] epoch 406, training loss: 13618.23, average training loss: 12307.45, base loss: 14462.09
[INFO 2017-06-28 14:24:45,811 main.py:51] epoch 407, training loss: 10730.35, average training loss: 12303.59, base loss: 14462.28
[INFO 2017-06-28 14:24:46,412 main.py:51] epoch 408, training loss: 11237.91, average training loss: 12300.98, base loss: 14462.02
[INFO 2017-06-28 14:24:47,195 main.py:51] epoch 409, training loss: 10785.75, average training loss: 12297.29, base loss: 14461.45
[INFO 2017-06-28 14:24:48,068 main.py:51] epoch 410, training loss: 12507.65, average training loss: 12297.80, base loss: 14466.07
[INFO 2017-06-28 14:24:48,708 main.py:51] epoch 411, training loss: 11129.48, average training loss: 12294.96, base loss: 14465.11
[INFO 2017-06-28 14:24:49,489 main.py:51] epoch 412, training loss: 11144.69, average training loss: 12292.18, base loss: 14467.35
[INFO 2017-06-28 14:24:50,393 main.py:51] epoch 413, training loss: 9705.98, average training loss: 12285.93, base loss: 14463.14
[INFO 2017-06-28 14:24:51,170 main.py:51] epoch 414, training loss: 10814.35, average training loss: 12282.38, base loss: 14461.75
[INFO 2017-06-28 14:24:51,872 main.py:51] epoch 415, training loss: 11081.25, average training loss: 12279.50, base loss: 14459.44
[INFO 2017-06-28 14:24:52,629 main.py:51] epoch 416, training loss: 11148.08, average training loss: 12276.78, base loss: 14458.27
[INFO 2017-06-28 14:24:53,424 main.py:51] epoch 417, training loss: 12887.97, average training loss: 12278.25, base loss: 14464.30
[INFO 2017-06-28 14:24:54,059 main.py:51] epoch 418, training loss: 11186.71, average training loss: 12275.64, base loss: 14465.51
[INFO 2017-06-28 14:24:54,863 main.py:51] epoch 419, training loss: 10831.77, average training loss: 12272.20, base loss: 14463.27
[INFO 2017-06-28 14:24:55,682 main.py:51] epoch 420, training loss: 13291.42, average training loss: 12274.62, base loss: 14470.60
[INFO 2017-06-28 14:24:56,497 main.py:51] epoch 421, training loss: 10131.15, average training loss: 12269.54, base loss: 14466.33
[INFO 2017-06-28 14:24:57,166 main.py:51] epoch 422, training loss: 10590.74, average training loss: 12265.58, base loss: 14466.59
[INFO 2017-06-28 14:24:57,962 main.py:51] epoch 423, training loss: 9668.30, average training loss: 12259.45, base loss: 14464.42
[INFO 2017-06-28 14:24:58,830 main.py:51] epoch 424, training loss: 11582.78, average training loss: 12257.86, base loss: 14464.62
[INFO 2017-06-28 14:24:59,602 main.py:51] epoch 425, training loss: 11940.31, average training loss: 12257.11, base loss: 14468.18
[INFO 2017-06-28 14:25:00,401 main.py:51] epoch 426, training loss: 10770.38, average training loss: 12253.63, base loss: 14467.34
[INFO 2017-06-28 14:25:01,233 main.py:51] epoch 427, training loss: 10423.01, average training loss: 12249.35, base loss: 14463.73
[INFO 2017-06-28 14:25:02,072 main.py:51] epoch 428, training loss: 10035.58, average training loss: 12244.19, base loss: 14461.04
[INFO 2017-06-28 14:25:02,766 main.py:51] epoch 429, training loss: 11133.68, average training loss: 12241.61, base loss: 14462.10
[INFO 2017-06-28 14:25:03,513 main.py:51] epoch 430, training loss: 12537.00, average training loss: 12242.30, base loss: 14466.31
[INFO 2017-06-28 14:25:04,333 main.py:51] epoch 431, training loss: 9560.17, average training loss: 12236.09, base loss: 14461.98
[INFO 2017-06-28 14:25:05,207 main.py:51] epoch 432, training loss: 9322.99, average training loss: 12229.36, base loss: 14458.63
[INFO 2017-06-28 14:25:05,857 main.py:51] epoch 433, training loss: 11334.11, average training loss: 12227.30, base loss: 14459.65
[INFO 2017-06-28 14:25:06,668 main.py:51] epoch 434, training loss: 9524.86, average training loss: 12221.08, base loss: 14453.15
[INFO 2017-06-28 14:25:07,485 main.py:51] epoch 435, training loss: 10306.81, average training loss: 12216.69, base loss: 14452.08
[INFO 2017-06-28 14:25:08,345 main.py:51] epoch 436, training loss: 9430.85, average training loss: 12210.32, base loss: 14446.75
[INFO 2017-06-28 14:25:09,058 main.py:51] epoch 437, training loss: 11680.82, average training loss: 12209.11, base loss: 14448.75
[INFO 2017-06-28 14:25:09,801 main.py:51] epoch 438, training loss: 13228.31, average training loss: 12211.43, base loss: 14453.75
[INFO 2017-06-28 14:25:10,628 main.py:51] epoch 439, training loss: 11504.55, average training loss: 12209.82, base loss: 14456.74
[INFO 2017-06-28 14:25:11,448 main.py:51] epoch 440, training loss: 9474.15, average training loss: 12203.62, base loss: 14452.62
[INFO 2017-06-28 14:25:12,168 main.py:51] epoch 441, training loss: 9600.35, average training loss: 12197.73, base loss: 14448.67
[INFO 2017-06-28 14:25:12,938 main.py:51] epoch 442, training loss: 12120.97, average training loss: 12197.56, base loss: 14450.76
[INFO 2017-06-28 14:25:13,770 main.py:51] epoch 443, training loss: 11437.67, average training loss: 12195.85, base loss: 14450.81
[INFO 2017-06-28 14:25:14,512 main.py:51] epoch 444, training loss: 11216.25, average training loss: 12193.65, base loss: 14450.39
[INFO 2017-06-28 14:25:15,229 main.py:51] epoch 445, training loss: 11036.00, average training loss: 12191.05, base loss: 14449.62
[INFO 2017-06-28 14:25:16,019 main.py:51] epoch 446, training loss: 11808.85, average training loss: 12190.19, base loss: 14451.81
[INFO 2017-06-28 14:25:16,835 main.py:51] epoch 447, training loss: 10426.87, average training loss: 12186.26, base loss: 14448.11
[INFO 2017-06-28 14:25:17,490 main.py:51] epoch 448, training loss: 11026.08, average training loss: 12183.68, base loss: 14448.37
[INFO 2017-06-28 14:25:18,281 main.py:51] epoch 449, training loss: 11899.72, average training loss: 12183.04, base loss: 14450.97
[INFO 2017-06-28 14:25:19,065 main.py:51] epoch 450, training loss: 10853.44, average training loss: 12180.10, base loss: 14449.65
[INFO 2017-06-28 14:25:19,884 main.py:51] epoch 451, training loss: 10604.00, average training loss: 12176.61, base loss: 14449.48
[INFO 2017-06-28 14:25:20,511 main.py:51] epoch 452, training loss: 11972.18, average training loss: 12176.16, base loss: 14452.17
[INFO 2017-06-28 14:25:21,243 main.py:51] epoch 453, training loss: 11217.91, average training loss: 12174.05, base loss: 14450.95
[INFO 2017-06-28 14:25:22,058 main.py:51] epoch 454, training loss: 11245.96, average training loss: 12172.01, base loss: 14450.92
[INFO 2017-06-28 14:25:22,678 main.py:51] epoch 455, training loss: 10525.01, average training loss: 12168.40, base loss: 14450.40
[INFO 2017-06-28 14:25:23,461 main.py:51] epoch 456, training loss: 11266.75, average training loss: 12166.42, base loss: 14451.85
[INFO 2017-06-28 14:25:24,252 main.py:51] epoch 457, training loss: 14117.70, average training loss: 12170.68, base loss: 14459.61
[INFO 2017-06-28 14:25:24,988 main.py:51] epoch 458, training loss: 9858.26, average training loss: 12165.64, base loss: 14455.58
[INFO 2017-06-28 14:25:25,686 main.py:51] epoch 459, training loss: 10921.98, average training loss: 12162.94, base loss: 14454.94
[INFO 2017-06-28 14:25:26,515 main.py:51] epoch 460, training loss: 11513.95, average training loss: 12161.53, base loss: 14456.28
[INFO 2017-06-28 14:25:27,343 main.py:51] epoch 461, training loss: 9978.10, average training loss: 12156.81, base loss: 14452.20
[INFO 2017-06-28 14:25:27,950 main.py:51] epoch 462, training loss: 11239.87, average training loss: 12154.83, base loss: 14451.50
[INFO 2017-06-28 14:25:28,745 main.py:51] epoch 463, training loss: 12071.09, average training loss: 12154.65, base loss: 14450.92
[INFO 2017-06-28 14:25:29,572 main.py:51] epoch 464, training loss: 10011.39, average training loss: 12150.04, base loss: 14447.99
[INFO 2017-06-28 14:25:30,316 main.py:51] epoch 465, training loss: 9597.21, average training loss: 12144.56, base loss: 14442.36
[INFO 2017-06-28 14:25:31,004 main.py:51] epoch 466, training loss: 10259.90, average training loss: 12140.52, base loss: 14440.92
[INFO 2017-06-28 14:25:31,808 main.py:51] epoch 467, training loss: 10814.98, average training loss: 12137.69, base loss: 14442.06
[INFO 2017-06-28 14:25:32,529 main.py:51] epoch 468, training loss: 12047.31, average training loss: 12137.50, base loss: 14444.48
[INFO 2017-06-28 14:25:33,203 main.py:51] epoch 469, training loss: 10832.40, average training loss: 12134.72, base loss: 14445.43
[INFO 2017-06-28 14:25:34,054 main.py:51] epoch 470, training loss: 10477.38, average training loss: 12131.20, base loss: 14442.68
[INFO 2017-06-28 14:25:34,815 main.py:51] epoch 471, training loss: 12385.13, average training loss: 12131.74, base loss: 14444.91
[INFO 2017-06-28 14:25:35,509 main.py:51] epoch 472, training loss: 11619.85, average training loss: 12130.66, base loss: 14447.56
[INFO 2017-06-28 14:25:36,293 main.py:51] epoch 473, training loss: 10560.31, average training loss: 12127.35, base loss: 14446.12
[INFO 2017-06-28 14:25:37,123 main.py:51] epoch 474, training loss: 9101.73, average training loss: 12120.98, base loss: 14441.17
[INFO 2017-06-28 14:25:37,816 main.py:51] epoch 475, training loss: 10390.66, average training loss: 12117.34, base loss: 14440.27
[INFO 2017-06-28 14:25:38,609 main.py:51] epoch 476, training loss: 10865.34, average training loss: 12114.72, base loss: 14440.11
[INFO 2017-06-28 14:25:39,428 main.py:51] epoch 477, training loss: 11855.99, average training loss: 12114.17, base loss: 14441.19
[INFO 2017-06-28 14:25:40,244 main.py:51] epoch 478, training loss: 12543.61, average training loss: 12115.07, base loss: 14445.34
[INFO 2017-06-28 14:25:40,907 main.py:51] epoch 479, training loss: 10314.60, average training loss: 12111.32, base loss: 14442.66
[INFO 2017-06-28 14:25:41,716 main.py:51] epoch 480, training loss: 11647.81, average training loss: 12110.36, base loss: 14444.90
[INFO 2017-06-28 14:25:42,549 main.py:51] epoch 481, training loss: 10424.52, average training loss: 12106.86, base loss: 14443.91
[INFO 2017-06-28 14:25:43,299 main.py:51] epoch 482, training loss: 9997.21, average training loss: 12102.49, base loss: 14438.91
[INFO 2017-06-28 14:25:43,974 main.py:51] epoch 483, training loss: 10389.78, average training loss: 12098.95, base loss: 14438.51
[INFO 2017-06-28 14:25:44,788 main.py:51] epoch 484, training loss: 9541.95, average training loss: 12093.68, base loss: 14435.62
[INFO 2017-06-28 14:25:45,593 main.py:51] epoch 485, training loss: 10924.12, average training loss: 12091.27, base loss: 14435.17
[INFO 2017-06-28 14:25:46,232 main.py:51] epoch 486, training loss: 10698.20, average training loss: 12088.41, base loss: 14434.74
[INFO 2017-06-28 14:25:47,006 main.py:51] epoch 487, training loss: 10724.95, average training loss: 12085.62, base loss: 14434.61
[INFO 2017-06-28 14:25:47,847 main.py:51] epoch 488, training loss: 11104.58, average training loss: 12083.61, base loss: 14437.00
[INFO 2017-06-28 14:25:48,570 main.py:51] epoch 489, training loss: 10790.45, average training loss: 12080.97, base loss: 14434.69
[INFO 2017-06-28 14:25:49,299 main.py:51] epoch 490, training loss: 10755.83, average training loss: 12078.28, base loss: 14433.32
[INFO 2017-06-28 14:25:49,982 main.py:51] epoch 491, training loss: 11804.02, average training loss: 12077.72, base loss: 14435.72
[INFO 2017-06-28 14:25:50,598 main.py:51] epoch 492, training loss: 11059.29, average training loss: 12075.65, base loss: 14438.84
[INFO 2017-06-28 14:25:51,239 main.py:51] epoch 493, training loss: 14781.75, average training loss: 12081.13, base loss: 14447.90
[INFO 2017-06-28 14:25:51,975 main.py:51] epoch 494, training loss: 9342.98, average training loss: 12075.60, base loss: 14442.54
[INFO 2017-06-28 14:25:52,619 main.py:51] epoch 495, training loss: 9209.01, average training loss: 12069.82, base loss: 14436.22
[INFO 2017-06-28 14:25:53,344 main.py:51] epoch 496, training loss: 12589.90, average training loss: 12070.87, base loss: 14442.32
[INFO 2017-06-28 14:25:54,085 main.py:51] epoch 497, training loss: 11357.95, average training loss: 12069.43, base loss: 14444.01
[INFO 2017-06-28 14:25:54,912 main.py:51] epoch 498, training loss: 9972.76, average training loss: 12065.23, base loss: 14440.76
[INFO 2017-06-28 14:25:55,704 main.py:51] epoch 499, training loss: 10325.75, average training loss: 12061.75, base loss: 14438.48
[INFO 2017-06-28 14:25:55,704 main.py:53] epoch 499, testing
[INFO 2017-06-28 14:25:58,614 main.py:105] average testing loss: 11762.44, base loss: 15326.36
[INFO 2017-06-28 14:25:58,614 main.py:106] improve_loss: 3563.92, improve_percent: 0.23
[INFO 2017-06-28 14:25:58,615 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:25:58,631 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 14:25:59,285 main.py:51] epoch 500, training loss: 9769.19, average training loss: 12057.18, base loss: 14436.34
[INFO 2017-06-28 14:26:00,061 main.py:51] epoch 501, training loss: 11697.90, average training loss: 12056.46, base loss: 14437.38
[INFO 2017-06-28 14:26:00,867 main.py:51] epoch 502, training loss: 11521.29, average training loss: 12055.40, base loss: 14437.91
[INFO 2017-06-28 14:26:01,532 main.py:51] epoch 503, training loss: 11233.61, average training loss: 12053.77, base loss: 14437.80
[INFO 2017-06-28 14:26:02,302 main.py:51] epoch 504, training loss: 11343.93, average training loss: 12052.36, base loss: 14439.10
[INFO 2017-06-28 14:26:03,113 main.py:51] epoch 505, training loss: 14345.75, average training loss: 12056.89, base loss: 14448.54
[INFO 2017-06-28 14:26:03,894 main.py:51] epoch 506, training loss: 11559.17, average training loss: 12055.91, base loss: 14449.62
[INFO 2017-06-28 14:26:04,548 main.py:51] epoch 507, training loss: 12387.30, average training loss: 12056.56, base loss: 14452.14
[INFO 2017-06-28 14:26:05,387 main.py:51] epoch 508, training loss: 9747.74, average training loss: 12052.03, base loss: 14450.41
[INFO 2017-06-28 14:26:06,245 main.py:51] epoch 509, training loss: 12419.14, average training loss: 12052.75, base loss: 14452.92
[INFO 2017-06-28 14:26:06,956 main.py:51] epoch 510, training loss: 10013.84, average training loss: 12048.76, base loss: 14450.33
[INFO 2017-06-28 14:26:07,700 main.py:51] epoch 511, training loss: 9405.00, average training loss: 12043.59, base loss: 14446.67
[INFO 2017-06-28 14:26:08,534 main.py:51] epoch 512, training loss: 12322.41, average training loss: 12044.14, base loss: 14448.35
[INFO 2017-06-28 14:26:09,327 main.py:51] epoch 513, training loss: 10685.38, average training loss: 12041.49, base loss: 14445.87
[INFO 2017-06-28 14:26:09,991 main.py:51] epoch 514, training loss: 10543.80, average training loss: 12038.59, base loss: 14444.56
[INFO 2017-06-28 14:26:10,786 main.py:51] epoch 515, training loss: 10957.56, average training loss: 12036.49, base loss: 14444.83
[INFO 2017-06-28 14:26:11,614 main.py:51] epoch 516, training loss: 10119.63, average training loss: 12032.78, base loss: 14442.21
[INFO 2017-06-28 14:26:12,283 main.py:51] epoch 517, training loss: 10701.50, average training loss: 12030.21, base loss: 14442.49
[INFO 2017-06-28 14:26:13,027 main.py:51] epoch 518, training loss: 10299.54, average training loss: 12026.88, base loss: 14440.44
[INFO 2017-06-28 14:26:13,842 main.py:51] epoch 519, training loss: 12156.51, average training loss: 12027.13, base loss: 14445.11
[INFO 2017-06-28 14:26:14,686 main.py:51] epoch 520, training loss: 9901.61, average training loss: 12023.05, base loss: 14441.37
[INFO 2017-06-28 14:26:15,336 main.py:51] epoch 521, training loss: 12707.58, average training loss: 12024.36, base loss: 14441.92
[INFO 2017-06-28 14:26:16,122 main.py:51] epoch 522, training loss: 10096.07, average training loss: 12020.67, base loss: 14440.30
[INFO 2017-06-28 14:26:16,925 main.py:51] epoch 523, training loss: 8762.58, average training loss: 12014.46, base loss: 14435.26
[INFO 2017-06-28 14:26:17,622 main.py:51] epoch 524, training loss: 10327.65, average training loss: 12011.24, base loss: 14431.89
[INFO 2017-06-28 14:26:18,374 main.py:51] epoch 525, training loss: 10698.38, average training loss: 12008.75, base loss: 14429.82
[INFO 2017-06-28 14:26:19,172 main.py:51] epoch 526, training loss: 10530.14, average training loss: 12005.94, base loss: 14428.50
[INFO 2017-06-28 14:26:19,990 main.py:51] epoch 527, training loss: 14150.41, average training loss: 12010.00, base loss: 14434.59
[INFO 2017-06-28 14:26:20,624 main.py:51] epoch 528, training loss: 12496.82, average training loss: 12010.92, base loss: 14438.55
[INFO 2017-06-28 14:26:21,410 main.py:51] epoch 529, training loss: 11795.05, average training loss: 12010.52, base loss: 14440.39
[INFO 2017-06-28 14:26:22,203 main.py:51] epoch 530, training loss: 11135.47, average training loss: 12008.87, base loss: 14442.11
[INFO 2017-06-28 14:26:22,846 main.py:51] epoch 531, training loss: 11804.36, average training loss: 12008.48, base loss: 14443.37
[INFO 2017-06-28 14:26:23,623 main.py:51] epoch 532, training loss: 10161.53, average training loss: 12005.02, base loss: 14441.35
[INFO 2017-06-28 14:26:24,427 main.py:51] epoch 533, training loss: 11122.82, average training loss: 12003.37, base loss: 14441.36
[INFO 2017-06-28 14:26:25,266 main.py:51] epoch 534, training loss: 11268.30, average training loss: 12001.99, base loss: 14444.26
[INFO 2017-06-28 14:26:25,919 main.py:51] epoch 535, training loss: 9793.87, average training loss: 11997.87, base loss: 14440.37
[INFO 2017-06-28 14:26:26,669 main.py:51] epoch 536, training loss: 10280.03, average training loss: 11994.67, base loss: 14439.08
[INFO 2017-06-28 14:26:27,519 main.py:51] epoch 537, training loss: 11373.27, average training loss: 11993.52, base loss: 14440.58
[INFO 2017-06-28 14:26:28,258 main.py:51] epoch 538, training loss: 10415.60, average training loss: 11990.59, base loss: 14439.21
[INFO 2017-06-28 14:26:28,991 main.py:51] epoch 539, training loss: 10382.57, average training loss: 11987.61, base loss: 14435.39
[INFO 2017-06-28 14:26:29,780 main.py:51] epoch 540, training loss: 11804.94, average training loss: 11987.28, base loss: 14439.36
[INFO 2017-06-28 14:26:30,603 main.py:51] epoch 541, training loss: 11631.95, average training loss: 11986.62, base loss: 14440.47
[INFO 2017-06-28 14:26:31,315 main.py:51] epoch 542, training loss: 11192.86, average training loss: 11985.16, base loss: 14440.11
[INFO 2017-06-28 14:26:32,045 main.py:51] epoch 543, training loss: 10993.58, average training loss: 11983.34, base loss: 14439.80
[INFO 2017-06-28 14:26:32,857 main.py:51] epoch 544, training loss: 9196.32, average training loss: 11978.22, base loss: 14436.08
[INFO 2017-06-28 14:26:33,691 main.py:51] epoch 545, training loss: 11002.71, average training loss: 11976.43, base loss: 14437.72
[INFO 2017-06-28 14:26:34,318 main.py:51] epoch 546, training loss: 10184.33, average training loss: 11973.16, base loss: 14436.46
[INFO 2017-06-28 14:26:35,073 main.py:51] epoch 547, training loss: 9679.72, average training loss: 11968.97, base loss: 14433.40
[INFO 2017-06-28 14:26:35,904 main.py:51] epoch 548, training loss: 9803.88, average training loss: 11965.03, base loss: 14430.24
[INFO 2017-06-28 14:26:36,566 main.py:51] epoch 549, training loss: 10717.88, average training loss: 11962.76, base loss: 14428.37
[INFO 2017-06-28 14:26:37,327 main.py:51] epoch 550, training loss: 13596.18, average training loss: 11965.73, base loss: 14435.80
[INFO 2017-06-28 14:26:38,148 main.py:51] epoch 551, training loss: 10916.19, average training loss: 11963.83, base loss: 14435.66
[INFO 2017-06-28 14:26:39,003 main.py:51] epoch 552, training loss: 9684.82, average training loss: 11959.70, base loss: 14432.12
[INFO 2017-06-28 14:26:39,645 main.py:51] epoch 553, training loss: 10519.76, average training loss: 11957.10, base loss: 14431.63
[INFO 2017-06-28 14:26:40,380 main.py:51] epoch 554, training loss: 11151.05, average training loss: 11955.65, base loss: 14433.59
[INFO 2017-06-28 14:26:41,201 main.py:51] epoch 555, training loss: 9674.99, average training loss: 11951.55, base loss: 14431.99
[INFO 2017-06-28 14:26:41,887 main.py:51] epoch 556, training loss: 8592.24, average training loss: 11945.52, base loss: 14426.18
[INFO 2017-06-28 14:26:42,636 main.py:51] epoch 557, training loss: 10985.43, average training loss: 11943.80, base loss: 14427.35
[INFO 2017-06-28 14:26:43,413 main.py:51] epoch 558, training loss: 11473.25, average training loss: 11942.96, base loss: 14429.27
[INFO 2017-06-28 14:26:44,210 main.py:51] epoch 559, training loss: 11633.38, average training loss: 11942.40, base loss: 14430.41
[INFO 2017-06-28 14:26:44,877 main.py:51] epoch 560, training loss: 10898.02, average training loss: 11940.54, base loss: 14430.15
[INFO 2017-06-28 14:26:45,625 main.py:51] epoch 561, training loss: 10880.12, average training loss: 11938.66, base loss: 14430.88
[INFO 2017-06-28 14:26:46,404 main.py:51] epoch 562, training loss: 10669.13, average training loss: 11936.40, base loss: 14429.17
[INFO 2017-06-28 14:26:47,005 main.py:51] epoch 563, training loss: 10969.10, average training loss: 11934.69, base loss: 14431.04
[INFO 2017-06-28 14:26:47,818 main.py:51] epoch 564, training loss: 10772.55, average training loss: 11932.63, base loss: 14431.80
[INFO 2017-06-28 14:26:48,628 main.py:51] epoch 565, training loss: 9587.17, average training loss: 11928.48, base loss: 14427.21
[INFO 2017-06-28 14:26:49,487 main.py:51] epoch 566, training loss: 9961.19, average training loss: 11925.02, base loss: 14424.54
[INFO 2017-06-28 14:26:50,152 main.py:51] epoch 567, training loss: 11973.39, average training loss: 11925.10, base loss: 14427.62
[INFO 2017-06-28 14:26:50,916 main.py:51] epoch 568, training loss: 10801.61, average training loss: 11923.13, base loss: 14428.39
[INFO 2017-06-28 14:26:51,742 main.py:51] epoch 569, training loss: 11978.38, average training loss: 11923.22, base loss: 14430.08
[INFO 2017-06-28 14:26:52,453 main.py:51] epoch 570, training loss: 11099.22, average training loss: 11921.78, base loss: 14431.70
[INFO 2017-06-28 14:26:53,181 main.py:51] epoch 571, training loss: 10836.72, average training loss: 11919.88, base loss: 14431.90
[INFO 2017-06-28 14:26:53,959 main.py:51] epoch 572, training loss: 12821.27, average training loss: 11921.46, base loss: 14437.97
[INFO 2017-06-28 14:26:54,783 main.py:51] epoch 573, training loss: 10214.06, average training loss: 11918.48, base loss: 14435.24
[INFO 2017-06-28 14:26:55,425 main.py:51] epoch 574, training loss: 9945.75, average training loss: 11915.05, base loss: 14430.40
[INFO 2017-06-28 14:26:56,192 main.py:51] epoch 575, training loss: 10474.73, average training loss: 11912.55, base loss: 14431.06
[INFO 2017-06-28 14:26:57,010 main.py:51] epoch 576, training loss: 11134.06, average training loss: 11911.20, base loss: 14430.74
[INFO 2017-06-28 14:26:57,672 main.py:51] epoch 577, training loss: 9106.39, average training loss: 11906.35, base loss: 14426.36
[INFO 2017-06-28 14:26:58,445 main.py:51] epoch 578, training loss: 11490.17, average training loss: 11905.63, base loss: 14426.69
[INFO 2017-06-28 14:26:59,258 main.py:51] epoch 579, training loss: 11154.55, average training loss: 11904.33, base loss: 14426.37
[INFO 2017-06-28 14:27:00,052 main.py:51] epoch 580, training loss: 12092.90, average training loss: 11904.66, base loss: 14429.11
[INFO 2017-06-28 14:27:00,689 main.py:51] epoch 581, training loss: 10578.76, average training loss: 11902.38, base loss: 14427.94
[INFO 2017-06-28 14:27:01,492 main.py:51] epoch 582, training loss: 11076.17, average training loss: 11900.96, base loss: 14427.82
[INFO 2017-06-28 14:27:02,316 main.py:51] epoch 583, training loss: 10434.11, average training loss: 11898.45, base loss: 14428.14
[INFO 2017-06-28 14:27:03,092 main.py:51] epoch 584, training loss: 9904.95, average training loss: 11895.04, base loss: 14424.91
[INFO 2017-06-28 14:27:03,795 main.py:51] epoch 585, training loss: 11028.86, average training loss: 11893.57, base loss: 14424.97
[INFO 2017-06-28 14:27:04,627 main.py:51] epoch 586, training loss: 10098.78, average training loss: 11890.51, base loss: 14423.25
[INFO 2017-06-28 14:27:05,428 main.py:51] epoch 587, training loss: 9703.59, average training loss: 11886.79, base loss: 14422.39
[INFO 2017-06-28 14:27:06,097 main.py:51] epoch 588, training loss: 8575.32, average training loss: 11881.17, base loss: 14418.36
[INFO 2017-06-28 14:27:06,835 main.py:51] epoch 589, training loss: 10784.98, average training loss: 11879.31, base loss: 14417.58
[INFO 2017-06-28 14:27:07,640 main.py:51] epoch 590, training loss: 11232.38, average training loss: 11878.21, base loss: 14419.28
[INFO 2017-06-28 14:27:08,413 main.py:51] epoch 591, training loss: 10088.53, average training loss: 11875.19, base loss: 14417.36
[INFO 2017-06-28 14:27:09,066 main.py:51] epoch 592, training loss: 10316.61, average training loss: 11872.56, base loss: 14415.51
[INFO 2017-06-28 14:27:09,825 main.py:51] epoch 593, training loss: 9200.52, average training loss: 11868.06, base loss: 14411.89
[INFO 2017-06-28 14:27:10,629 main.py:51] epoch 594, training loss: 11718.34, average training loss: 11867.81, base loss: 14413.03
[INFO 2017-06-28 14:27:11,309 main.py:51] epoch 595, training loss: 11910.64, average training loss: 11867.89, base loss: 14413.56
[INFO 2017-06-28 14:27:12,062 main.py:51] epoch 596, training loss: 9371.71, average training loss: 11863.70, base loss: 14409.95
[INFO 2017-06-28 14:27:12,873 main.py:51] epoch 597, training loss: 11918.52, average training loss: 11863.80, base loss: 14413.57
[INFO 2017-06-28 14:27:13,736 main.py:51] epoch 598, training loss: 9567.74, average training loss: 11859.96, base loss: 14409.36
[INFO 2017-06-28 14:27:14,410 main.py:51] epoch 599, training loss: 11965.42, average training loss: 11860.14, base loss: 14410.64
[INFO 2017-06-28 14:27:14,410 main.py:53] epoch 599, testing
[INFO 2017-06-28 14:27:17,255 main.py:105] average testing loss: 11778.74, base loss: 15079.98
[INFO 2017-06-28 14:27:17,255 main.py:106] improve_loss: 3301.24, improve_percent: 0.22
[INFO 2017-06-28 14:27:17,256 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 14:27:18,028 main.py:51] epoch 600, training loss: 9994.30, average training loss: 11857.03, base loss: 14408.20
[INFO 2017-06-28 14:27:18,827 main.py:51] epoch 601, training loss: 10423.08, average training loss: 11854.65, base loss: 14406.98
[INFO 2017-06-28 14:27:19,513 main.py:51] epoch 602, training loss: 10650.05, average training loss: 11852.65, base loss: 14405.60
[INFO 2017-06-28 14:27:20,278 main.py:51] epoch 603, training loss: 9049.28, average training loss: 11848.01, base loss: 14401.76
[INFO 2017-06-28 14:27:21,081 main.py:51] epoch 604, training loss: 9870.91, average training loss: 11844.74, base loss: 14401.45
[INFO 2017-06-28 14:27:21,906 main.py:51] epoch 605, training loss: 9934.52, average training loss: 11841.59, base loss: 14398.31
[INFO 2017-06-28 14:27:22,578 main.py:51] epoch 606, training loss: 11071.70, average training loss: 11840.32, base loss: 14398.47
[INFO 2017-06-28 14:27:23,330 main.py:51] epoch 607, training loss: 10846.24, average training loss: 11838.69, base loss: 14397.52
[INFO 2017-06-28 14:27:24,140 main.py:51] epoch 608, training loss: 10959.66, average training loss: 11837.25, base loss: 14399.50
[INFO 2017-06-28 14:27:24,970 main.py:51] epoch 609, training loss: 9963.35, average training loss: 11834.17, base loss: 14396.98
[INFO 2017-06-28 14:27:25,618 main.py:51] epoch 610, training loss: 11225.33, average training loss: 11833.18, base loss: 14399.73
[INFO 2017-06-28 14:27:26,368 main.py:51] epoch 611, training loss: 11174.15, average training loss: 11832.10, base loss: 14401.39
[INFO 2017-06-28 14:27:27,161 main.py:51] epoch 612, training loss: 11589.74, average training loss: 11831.71, base loss: 14402.04
[INFO 2017-06-28 14:27:27,848 main.py:51] epoch 613, training loss: 9953.30, average training loss: 11828.65, base loss: 14400.72
[INFO 2017-06-28 14:27:28,603 main.py:51] epoch 614, training loss: 12037.62, average training loss: 11828.99, base loss: 14402.02
[INFO 2017-06-28 14:27:29,384 main.py:51] epoch 615, training loss: 9663.08, average training loss: 11825.47, base loss: 14398.14
[INFO 2017-06-28 14:27:30,195 main.py:51] epoch 616, training loss: 9414.97, average training loss: 11821.56, base loss: 14395.51
[INFO 2017-06-28 14:27:30,831 main.py:51] epoch 617, training loss: 9781.42, average training loss: 11818.26, base loss: 14393.71
[INFO 2017-06-28 14:27:31,637 main.py:51] epoch 618, training loss: 9976.80, average training loss: 11815.29, base loss: 14392.25
[INFO 2017-06-28 14:27:32,452 main.py:51] epoch 619, training loss: 12668.79, average training loss: 11816.66, base loss: 14395.11
[INFO 2017-06-28 14:27:33,263 main.py:51] epoch 620, training loss: 12283.05, average training loss: 11817.41, base loss: 14398.09
[INFO 2017-06-28 14:27:33,924 main.py:51] epoch 621, training loss: 9501.37, average training loss: 11813.69, base loss: 14395.71
[INFO 2017-06-28 14:27:34,721 main.py:51] epoch 622, training loss: 11040.36, average training loss: 11812.45, base loss: 14396.62
[INFO 2017-06-28 14:27:35,557 main.py:51] epoch 623, training loss: 10574.14, average training loss: 11810.46, base loss: 14395.04
[INFO 2017-06-28 14:27:36,327 main.py:51] epoch 624, training loss: 10866.98, average training loss: 11808.96, base loss: 14395.72
[INFO 2017-06-28 14:27:37,010 main.py:51] epoch 625, training loss: 11258.68, average training loss: 11808.08, base loss: 14395.81
[INFO 2017-06-28 14:27:37,800 main.py:51] epoch 626, training loss: 10786.59, average training loss: 11806.45, base loss: 14396.56
[INFO 2017-06-28 14:27:38,441 main.py:51] epoch 627, training loss: 9683.87, average training loss: 11803.07, base loss: 14392.78
[INFO 2017-06-28 14:27:39,145 main.py:51] epoch 628, training loss: 10379.03, average training loss: 11800.80, base loss: 14392.91
[INFO 2017-06-28 14:27:39,810 main.py:51] epoch 629, training loss: 11203.30, average training loss: 11799.85, base loss: 14394.27
[INFO 2017-06-28 14:27:40,508 main.py:51] epoch 630, training loss: 13592.54, average training loss: 11802.70, base loss: 14399.00
[INFO 2017-06-28 14:27:41,125 main.py:51] epoch 631, training loss: 10616.00, average training loss: 11800.82, base loss: 14399.45
[INFO 2017-06-28 14:27:41,721 main.py:51] epoch 632, training loss: 9626.88, average training loss: 11797.38, base loss: 14396.83
[INFO 2017-06-28 14:27:42,516 main.py:51] epoch 633, training loss: 11369.10, average training loss: 11796.71, base loss: 14399.42
[INFO 2017-06-28 14:27:43,368 main.py:51] epoch 634, training loss: 10494.27, average training loss: 11794.66, base loss: 14398.60
[INFO 2017-06-28 14:27:44,112 main.py:51] epoch 635, training loss: 10974.51, average training loss: 11793.37, base loss: 14398.28
[INFO 2017-06-28 14:27:44,805 main.py:51] epoch 636, training loss: 10697.20, average training loss: 11791.65, base loss: 14398.46
[INFO 2017-06-28 14:27:45,608 main.py:51] epoch 637, training loss: 9592.94, average training loss: 11788.20, base loss: 14396.74
[INFO 2017-06-28 14:27:46,459 main.py:51] epoch 638, training loss: 11995.19, average training loss: 11788.52, base loss: 14398.97
[INFO 2017-06-28 14:27:47,098 main.py:51] epoch 639, training loss: 10706.20, average training loss: 11786.83, base loss: 14399.62
[INFO 2017-06-28 14:27:47,854 main.py:51] epoch 640, training loss: 9905.44, average training loss: 11783.90, base loss: 14395.63
[INFO 2017-06-28 14:27:48,696 main.py:51] epoch 641, training loss: 9601.38, average training loss: 11780.50, base loss: 14393.81
[INFO 2017-06-28 14:27:49,441 main.py:51] epoch 642, training loss: 12084.55, average training loss: 11780.97, base loss: 14396.77
[INFO 2017-06-28 14:27:50,104 main.py:51] epoch 643, training loss: 10405.89, average training loss: 11778.84, base loss: 14395.76
[INFO 2017-06-28 14:27:50,923 main.py:51] epoch 644, training loss: 10140.89, average training loss: 11776.30, base loss: 14395.32
[INFO 2017-06-28 14:27:51,671 main.py:51] epoch 645, training loss: 10111.67, average training loss: 11773.72, base loss: 14392.18
[INFO 2017-06-28 14:27:52,355 main.py:51] epoch 646, training loss: 11727.08, average training loss: 11773.65, base loss: 14396.53
[INFO 2017-06-28 14:27:53,151 main.py:51] epoch 647, training loss: 9042.55, average training loss: 11769.43, base loss: 14392.86
[INFO 2017-06-28 14:27:53,974 main.py:51] epoch 648, training loss: 9883.73, average training loss: 11766.53, base loss: 14392.19
[INFO 2017-06-28 14:27:54,614 main.py:51] epoch 649, training loss: 10777.62, average training loss: 11765.01, base loss: 14393.92
[INFO 2017-06-28 14:27:55,430 main.py:51] epoch 650, training loss: 10726.52, average training loss: 11763.41, base loss: 14393.89
[INFO 2017-06-28 14:27:56,266 main.py:51] epoch 651, training loss: 10088.40, average training loss: 11760.84, base loss: 14393.28
[INFO 2017-06-28 14:27:57,016 main.py:51] epoch 652, training loss: 11837.78, average training loss: 11760.96, base loss: 14396.47
[INFO 2017-06-28 14:27:57,738 main.py:51] epoch 653, training loss: 10961.76, average training loss: 11759.74, base loss: 14397.43
[INFO 2017-06-28 14:27:58,508 main.py:51] epoch 654, training loss: 9883.88, average training loss: 11756.87, base loss: 14395.64
[INFO 2017-06-28 14:27:59,349 main.py:51] epoch 655, training loss: 10225.12, average training loss: 11754.54, base loss: 14393.99
[INFO 2017-06-28 14:28:00,033 main.py:51] epoch 656, training loss: 11560.55, average training loss: 11754.24, base loss: 14396.14
[INFO 2017-06-28 14:28:00,768 main.py:51] epoch 657, training loss: 9668.36, average training loss: 11751.07, base loss: 14392.48
[INFO 2017-06-28 14:28:01,606 main.py:51] epoch 658, training loss: 8906.97, average training loss: 11746.76, base loss: 14388.78
[INFO 2017-06-28 14:28:02,429 main.py:51] epoch 659, training loss: 11683.88, average training loss: 11746.66, base loss: 14389.67
[INFO 2017-06-28 14:28:03,091 main.py:51] epoch 660, training loss: 11616.16, average training loss: 11746.47, base loss: 14391.56
[INFO 2017-06-28 14:28:03,829 main.py:51] epoch 661, training loss: 10011.35, average training loss: 11743.84, base loss: 14389.09
[INFO 2017-06-28 14:28:04,650 main.py:51] epoch 662, training loss: 12303.48, average training loss: 11744.69, base loss: 14392.21
[INFO 2017-06-28 14:28:05,317 main.py:51] epoch 663, training loss: 9865.28, average training loss: 11741.86, base loss: 14390.55
[INFO 2017-06-28 14:28:06,065 main.py:51] epoch 664, training loss: 11050.95, average training loss: 11740.82, base loss: 14390.19
[INFO 2017-06-28 14:28:06,884 main.py:51] epoch 665, training loss: 10313.76, average training loss: 11738.68, base loss: 14389.34
[INFO 2017-06-28 14:28:07,659 main.py:51] epoch 666, training loss: 9425.93, average training loss: 11735.21, base loss: 14387.24
[INFO 2017-06-28 14:28:08,361 main.py:51] epoch 667, training loss: 9428.40, average training loss: 11731.76, base loss: 14384.92
[INFO 2017-06-28 14:28:09,158 main.py:51] epoch 668, training loss: 9518.56, average training loss: 11728.45, base loss: 14382.31
[INFO 2017-06-28 14:28:09,987 main.py:51] epoch 669, training loss: 8541.56, average training loss: 11723.69, base loss: 14378.18
[INFO 2017-06-28 14:28:10,681 main.py:51] epoch 670, training loss: 10967.47, average training loss: 11722.56, base loss: 14377.80
[INFO 2017-06-28 14:28:11,432 main.py:51] epoch 671, training loss: 10254.03, average training loss: 11720.38, base loss: 14375.57
[INFO 2017-06-28 14:28:12,253 main.py:51] epoch 672, training loss: 8782.83, average training loss: 11716.01, base loss: 14370.78
[INFO 2017-06-28 14:28:13,049 main.py:51] epoch 673, training loss: 11108.28, average training loss: 11715.11, base loss: 14370.61
[INFO 2017-06-28 14:28:13,701 main.py:51] epoch 674, training loss: 10839.18, average training loss: 11713.81, base loss: 14370.64
[INFO 2017-06-28 14:28:14,481 main.py:51] epoch 675, training loss: 11977.30, average training loss: 11714.20, base loss: 14372.61
[INFO 2017-06-28 14:28:15,295 main.py:51] epoch 676, training loss: 10395.14, average training loss: 11712.26, base loss: 14371.98
[INFO 2017-06-28 14:28:15,956 main.py:51] epoch 677, training loss: 11033.68, average training loss: 11711.25, base loss: 14372.94
[INFO 2017-06-28 14:28:16,696 main.py:51] epoch 678, training loss: 10994.21, average training loss: 11710.20, base loss: 14373.05
[INFO 2017-06-28 14:28:17,536 main.py:51] epoch 679, training loss: 12136.31, average training loss: 11710.83, base loss: 14375.22
[INFO 2017-06-28 14:28:18,259 main.py:51] epoch 680, training loss: 10435.15, average training loss: 11708.95, base loss: 14373.61
[INFO 2017-06-28 14:28:18,974 main.py:51] epoch 681, training loss: 10022.71, average training loss: 11706.48, base loss: 14370.76
[INFO 2017-06-28 14:28:19,781 main.py:51] epoch 682, training loss: 10724.53, average training loss: 11705.04, base loss: 14370.62
[INFO 2017-06-28 14:28:20,622 main.py:51] epoch 683, training loss: 11949.69, average training loss: 11705.40, base loss: 14373.25
[INFO 2017-06-28 14:28:21,269 main.py:51] epoch 684, training loss: 10912.15, average training loss: 11704.24, base loss: 14372.63
[INFO 2017-06-28 14:28:22,021 main.py:51] epoch 685, training loss: 11709.21, average training loss: 11704.25, base loss: 14374.68
[INFO 2017-06-28 14:28:22,850 main.py:51] epoch 686, training loss: 12223.30, average training loss: 11705.00, base loss: 14376.51
[INFO 2017-06-28 14:28:23,668 main.py:51] epoch 687, training loss: 11450.12, average training loss: 11704.63, base loss: 14378.77
[INFO 2017-06-28 14:28:24,278 main.py:51] epoch 688, training loss: 9883.04, average training loss: 11701.99, base loss: 14377.09
[INFO 2017-06-28 14:28:25,073 main.py:51] epoch 689, training loss: 10546.25, average training loss: 11700.32, base loss: 14376.25
[INFO 2017-06-28 14:28:25,921 main.py:51] epoch 690, training loss: 9751.82, average training loss: 11697.50, base loss: 14373.83
[INFO 2017-06-28 14:28:26,591 main.py:51] epoch 691, training loss: 10739.79, average training loss: 11696.11, base loss: 14375.24
[INFO 2017-06-28 14:28:27,337 main.py:51] epoch 692, training loss: 10864.84, average training loss: 11694.91, base loss: 14376.25
[INFO 2017-06-28 14:28:28,168 main.py:51] epoch 693, training loss: 10837.04, average training loss: 11693.68, base loss: 14377.73
[INFO 2017-06-28 14:28:28,981 main.py:51] epoch 694, training loss: 11849.51, average training loss: 11693.90, base loss: 14380.20
[INFO 2017-06-28 14:28:29,644 main.py:51] epoch 695, training loss: 10665.97, average training loss: 11692.42, base loss: 14379.72
[INFO 2017-06-28 14:28:30,408 main.py:51] epoch 696, training loss: 9721.68, average training loss: 11689.60, base loss: 14378.47
[INFO 2017-06-28 14:28:31,222 main.py:51] epoch 697, training loss: 13002.27, average training loss: 11691.48, base loss: 14382.49
[INFO 2017-06-28 14:28:31,872 main.py:51] epoch 698, training loss: 9707.62, average training loss: 11688.64, base loss: 14379.01
[INFO 2017-06-28 14:28:32,598 main.py:51] epoch 699, training loss: 10013.38, average training loss: 11686.24, base loss: 14376.71
[INFO 2017-06-28 14:28:32,598 main.py:53] epoch 699, testing
[INFO 2017-06-28 14:28:35,503 main.py:105] average testing loss: 11805.49, base loss: 15000.13
[INFO 2017-06-28 14:28:35,503 main.py:106] improve_loss: 3194.64, improve_percent: 0.21
[INFO 2017-06-28 14:28:35,504 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 14:28:36,322 main.py:51] epoch 700, training loss: 9356.87, average training loss: 11682.92, base loss: 14375.20
[INFO 2017-06-28 14:28:37,158 main.py:51] epoch 701, training loss: 10429.70, average training loss: 11681.14, base loss: 14374.93
[INFO 2017-06-28 14:28:37,987 main.py:51] epoch 702, training loss: 11659.75, average training loss: 11681.11, base loss: 14376.35
[INFO 2017-06-28 14:28:38,660 main.py:51] epoch 703, training loss: 11470.78, average training loss: 11680.81, base loss: 14376.67
[INFO 2017-06-28 14:28:39,410 main.py:51] epoch 704, training loss: 11031.71, average training loss: 11679.89, base loss: 14378.11
[INFO 2017-06-28 14:28:40,226 main.py:51] epoch 705, training loss: 10140.34, average training loss: 11677.71, base loss: 14378.40
[INFO 2017-06-28 14:28:40,880 main.py:51] epoch 706, training loss: 9342.00, average training loss: 11674.40, base loss: 14374.89
[INFO 2017-06-28 14:28:41,663 main.py:51] epoch 707, training loss: 10098.50, average training loss: 11672.18, base loss: 14372.88
[INFO 2017-06-28 14:28:42,491 main.py:51] epoch 708, training loss: 9574.84, average training loss: 11669.22, base loss: 14371.00
[INFO 2017-06-28 14:28:43,346 main.py:51] epoch 709, training loss: 11328.83, average training loss: 11668.74, base loss: 14371.19
[INFO 2017-06-28 14:28:44,010 main.py:51] epoch 710, training loss: 10249.74, average training loss: 11666.74, base loss: 14370.50
[INFO 2017-06-28 14:28:44,838 main.py:51] epoch 711, training loss: 10540.87, average training loss: 11665.16, base loss: 14369.91
[INFO 2017-06-28 14:28:45,667 main.py:51] epoch 712, training loss: 10370.32, average training loss: 11663.35, base loss: 14370.08
[INFO 2017-06-28 14:28:46,506 main.py:51] epoch 713, training loss: 10626.79, average training loss: 11661.89, base loss: 14369.11
[INFO 2017-06-28 14:28:47,122 main.py:51] epoch 714, training loss: 12305.29, average training loss: 11662.79, base loss: 14373.22
[INFO 2017-06-28 14:28:47,894 main.py:51] epoch 715, training loss: 10022.65, average training loss: 11660.50, base loss: 14371.65
[INFO 2017-06-28 14:28:48,700 main.py:51] epoch 716, training loss: 10896.96, average training loss: 11659.44, base loss: 14373.61
[INFO 2017-06-28 14:28:49,348 main.py:51] epoch 717, training loss: 9714.00, average training loss: 11656.73, base loss: 14371.57
[INFO 2017-06-28 14:28:50,113 main.py:51] epoch 718, training loss: 10675.91, average training loss: 11655.36, base loss: 14371.47
[INFO 2017-06-28 14:28:50,971 main.py:51] epoch 719, training loss: 9625.67, average training loss: 11652.55, base loss: 14370.36
[INFO 2017-06-28 14:28:51,737 main.py:51] epoch 720, training loss: 10439.41, average training loss: 11650.86, base loss: 14370.11
[INFO 2017-06-28 14:28:52,434 main.py:51] epoch 721, training loss: 11054.59, average training loss: 11650.04, base loss: 14370.49
[INFO 2017-06-28 14:28:53,217 main.py:51] epoch 722, training loss: 12514.08, average training loss: 11651.23, base loss: 14375.22
[INFO 2017-06-28 14:28:53,998 main.py:51] epoch 723, training loss: 10843.81, average training loss: 11650.12, base loss: 14375.98
[INFO 2017-06-28 14:28:54,639 main.py:51] epoch 724, training loss: 11795.08, average training loss: 11650.32, base loss: 14379.00
[INFO 2017-06-28 14:28:55,425 main.py:51] epoch 725, training loss: 11268.78, average training loss: 11649.79, base loss: 14381.25
[INFO 2017-06-28 14:28:56,256 main.py:51] epoch 726, training loss: 11722.60, average training loss: 11649.89, base loss: 14384.18
[INFO 2017-06-28 14:28:57,011 main.py:51] epoch 727, training loss: 10227.36, average training loss: 11647.94, base loss: 14384.47
[INFO 2017-06-28 14:28:57,702 main.py:51] epoch 728, training loss: 10176.61, average training loss: 11645.92, base loss: 14383.90
[INFO 2017-06-28 14:28:58,511 main.py:51] epoch 729, training loss: 10527.63, average training loss: 11644.39, base loss: 14383.88
[INFO 2017-06-28 14:28:59,360 main.py:51] epoch 730, training loss: 10762.64, average training loss: 11643.18, base loss: 14383.63
[INFO 2017-06-28 14:29:00,014 main.py:51] epoch 731, training loss: 10431.70, average training loss: 11641.53, base loss: 14382.19
[INFO 2017-06-28 14:29:00,800 main.py:51] epoch 732, training loss: 11783.52, average training loss: 11641.72, base loss: 14384.71
[INFO 2017-06-28 14:29:01,624 main.py:51] epoch 733, training loss: 11343.32, average training loss: 11641.31, base loss: 14385.58
[INFO 2017-06-28 14:29:02,432 main.py:51] epoch 734, training loss: 10667.92, average training loss: 11639.99, base loss: 14384.53
[INFO 2017-06-28 14:29:03,114 main.py:51] epoch 735, training loss: 12350.64, average training loss: 11640.95, base loss: 14387.15
[INFO 2017-06-28 14:29:03,885 main.py:51] epoch 736, training loss: 11530.93, average training loss: 11640.81, base loss: 14388.44
[INFO 2017-06-28 14:29:04,700 main.py:51] epoch 737, training loss: 10637.01, average training loss: 11639.45, base loss: 14388.54
[INFO 2017-06-28 14:29:05,379 main.py:51] epoch 738, training loss: 10192.13, average training loss: 11637.49, base loss: 14385.96
[INFO 2017-06-28 14:29:06,137 main.py:51] epoch 739, training loss: 9501.59, average training loss: 11634.60, base loss: 14383.63
[INFO 2017-06-28 14:29:06,926 main.py:51] epoch 740, training loss: 11600.39, average training loss: 11634.55, base loss: 14385.51
[INFO 2017-06-28 14:29:07,736 main.py:51] epoch 741, training loss: 9398.84, average training loss: 11631.54, base loss: 14382.26
[INFO 2017-06-28 14:29:08,360 main.py:51] epoch 742, training loss: 11819.09, average training loss: 11631.79, base loss: 14382.34
[INFO 2017-06-28 14:29:09,119 main.py:51] epoch 743, training loss: 10393.26, average training loss: 11630.13, base loss: 14381.69
[INFO 2017-06-28 14:29:09,895 main.py:51] epoch 744, training loss: 10571.76, average training loss: 11628.71, base loss: 14380.71
[INFO 2017-06-28 14:29:10,521 main.py:51] epoch 745, training loss: 11917.84, average training loss: 11629.10, base loss: 14382.53
[INFO 2017-06-28 14:29:11,219 main.py:51] epoch 746, training loss: 11283.93, average training loss: 11628.63, base loss: 14383.90
[INFO 2017-06-28 14:29:12,025 main.py:51] epoch 747, training loss: 11103.43, average training loss: 11627.93, base loss: 14385.02
[INFO 2017-06-28 14:29:12,648 main.py:51] epoch 748, training loss: 10537.76, average training loss: 11626.48, base loss: 14383.43
[INFO 2017-06-28 14:29:13,397 main.py:51] epoch 749, training loss: 11430.28, average training loss: 11626.21, base loss: 14386.13
[INFO 2017-06-28 14:29:14,193 main.py:51] epoch 750, training loss: 11597.70, average training loss: 11626.18, base loss: 14385.85
[INFO 2017-06-28 14:29:14,873 main.py:51] epoch 751, training loss: 9971.59, average training loss: 11623.98, base loss: 14385.16
[INFO 2017-06-28 14:29:15,572 main.py:51] epoch 752, training loss: 9902.15, average training loss: 11621.69, base loss: 14383.51
[INFO 2017-06-28 14:29:16,435 main.py:51] epoch 753, training loss: 10016.35, average training loss: 11619.56, base loss: 14382.95
[INFO 2017-06-28 14:29:17,217 main.py:51] epoch 754, training loss: 10957.79, average training loss: 11618.68, base loss: 14383.82
[INFO 2017-06-28 14:29:17,936 main.py:51] epoch 755, training loss: 11598.10, average training loss: 11618.66, base loss: 14385.50
[INFO 2017-06-28 14:29:18,758 main.py:51] epoch 756, training loss: 11816.01, average training loss: 11618.92, base loss: 14388.30
[INFO 2017-06-28 14:29:19,594 main.py:51] epoch 757, training loss: 9863.38, average training loss: 11616.60, base loss: 14387.36
[INFO 2017-06-28 14:29:20,377 main.py:51] epoch 758, training loss: 12355.81, average training loss: 11617.58, base loss: 14391.02
[INFO 2017-06-28 14:29:21,035 main.py:51] epoch 759, training loss: 10841.10, average training loss: 11616.55, base loss: 14392.01
[INFO 2017-06-28 14:29:21,801 main.py:51] epoch 760, training loss: 11891.39, average training loss: 11616.91, base loss: 14394.84
[INFO 2017-06-28 14:29:22,572 main.py:51] epoch 761, training loss: 10658.16, average training loss: 11615.66, base loss: 14394.61
[INFO 2017-06-28 14:29:23,190 main.py:51] epoch 762, training loss: 10716.59, average training loss: 11614.48, base loss: 14393.26
[INFO 2017-06-28 14:29:24,054 main.py:51] epoch 763, training loss: 12055.24, average training loss: 11615.06, base loss: 14393.51
[INFO 2017-06-28 14:29:24,860 main.py:51] epoch 764, training loss: 10094.39, average training loss: 11613.07, base loss: 14392.59
[INFO 2017-06-28 14:29:25,645 main.py:51] epoch 765, training loss: 10763.70, average training loss: 11611.96, base loss: 14393.54
[INFO 2017-06-28 14:29:26,252 main.py:51] epoch 766, training loss: 11410.05, average training loss: 11611.70, base loss: 14395.40
[INFO 2017-06-28 14:29:27,013 main.py:51] epoch 767, training loss: 11262.52, average training loss: 11611.24, base loss: 14396.71
[INFO 2017-06-28 14:29:27,651 main.py:51] epoch 768, training loss: 12588.26, average training loss: 11612.51, base loss: 14399.31
[INFO 2017-06-28 14:29:28,351 main.py:51] epoch 769, training loss: 11305.46, average training loss: 11612.11, base loss: 14399.84
[INFO 2017-06-28 14:29:28,956 main.py:51] epoch 770, training loss: 10286.29, average training loss: 11610.39, base loss: 14399.09
[INFO 2017-06-28 14:29:29,588 main.py:51] epoch 771, training loss: 11300.68, average training loss: 11609.99, base loss: 14399.63
[INFO 2017-06-28 14:29:30,279 main.py:51] epoch 772, training loss: 10296.39, average training loss: 11608.29, base loss: 14399.05
[INFO 2017-06-28 14:29:30,859 main.py:51] epoch 773, training loss: 9837.00, average training loss: 11606.00, base loss: 14398.69
[INFO 2017-06-28 14:29:31,667 main.py:51] epoch 774, training loss: 12268.59, average training loss: 11606.86, base loss: 14402.47
[INFO 2017-06-28 14:29:32,236 main.py:51] epoch 775, training loss: 10167.47, average training loss: 11605.00, base loss: 14401.61
[INFO 2017-06-28 14:29:33,005 main.py:51] epoch 776, training loss: 10661.01, average training loss: 11603.79, base loss: 14402.00
[INFO 2017-06-28 14:29:33,821 main.py:51] epoch 777, training loss: 9926.49, average training loss: 11601.63, base loss: 14401.73
[INFO 2017-06-28 14:29:34,428 main.py:51] epoch 778, training loss: 10365.57, average training loss: 11600.05, base loss: 14401.35
[INFO 2017-06-28 14:29:35,214 main.py:51] epoch 779, training loss: 11210.12, average training loss: 11599.55, base loss: 14401.99
[INFO 2017-06-28 14:29:36,081 main.py:51] epoch 780, training loss: 12000.94, average training loss: 11600.06, base loss: 14405.46
[INFO 2017-06-28 14:29:36,944 main.py:51] epoch 781, training loss: 9099.18, average training loss: 11596.86, base loss: 14403.33
[INFO 2017-06-28 14:29:37,546 main.py:51] epoch 782, training loss: 10884.25, average training loss: 11595.95, base loss: 14403.61
[INFO 2017-06-28 14:29:38,326 main.py:51] epoch 783, training loss: 9362.71, average training loss: 11593.10, base loss: 14401.56
[INFO 2017-06-28 14:29:39,135 main.py:51] epoch 784, training loss: 15438.87, average training loss: 11598.00, base loss: 14407.07
[INFO 2017-06-28 14:29:39,792 main.py:51] epoch 785, training loss: 9704.06, average training loss: 11595.59, base loss: 14405.25
[INFO 2017-06-28 14:29:40,576 main.py:51] epoch 786, training loss: 10815.17, average training loss: 11594.60, base loss: 14404.06
[INFO 2017-06-28 14:29:41,404 main.py:51] epoch 787, training loss: 9923.38, average training loss: 11592.48, base loss: 14402.68
[INFO 2017-06-28 14:29:42,122 main.py:51] epoch 788, training loss: 12180.43, average training loss: 11593.23, base loss: 14405.28
[INFO 2017-06-28 14:29:42,838 main.py:51] epoch 789, training loss: 10209.21, average training loss: 11591.47, base loss: 14405.16
[INFO 2017-06-28 14:29:43,642 main.py:51] epoch 790, training loss: 10730.12, average training loss: 11590.38, base loss: 14406.34
[INFO 2017-06-28 14:29:44,477 main.py:51] epoch 791, training loss: 11614.29, average training loss: 11590.42, base loss: 14406.67
[INFO 2017-06-28 14:29:45,135 main.py:51] epoch 792, training loss: 9530.18, average training loss: 11587.82, base loss: 14403.49
[INFO 2017-06-28 14:29:45,949 main.py:51] epoch 793, training loss: 10802.43, average training loss: 11586.83, base loss: 14404.72
[INFO 2017-06-28 14:29:46,774 main.py:51] epoch 794, training loss: 10231.18, average training loss: 11585.12, base loss: 14405.62
[INFO 2017-06-28 14:29:47,566 main.py:51] epoch 795, training loss: 9824.63, average training loss: 11582.91, base loss: 14403.37
[INFO 2017-06-28 14:29:48,234 main.py:51] epoch 796, training loss: 9028.71, average training loss: 11579.71, base loss: 14400.47
[INFO 2017-06-28 14:29:48,989 main.py:51] epoch 797, training loss: 9909.08, average training loss: 11577.61, base loss: 14399.10
[INFO 2017-06-28 14:29:49,828 main.py:51] epoch 798, training loss: 10677.79, average training loss: 11576.49, base loss: 14399.89
[INFO 2017-06-28 14:29:50,585 main.py:51] epoch 799, training loss: 11087.87, average training loss: 11575.88, base loss: 14401.03
[INFO 2017-06-28 14:29:50,585 main.py:53] epoch 799, testing
[INFO 2017-06-28 14:29:53,436 main.py:105] average testing loss: 12147.24, base loss: 15624.65
[INFO 2017-06-28 14:29:53,436 main.py:106] improve_loss: 3477.40, improve_percent: 0.22
[INFO 2017-06-28 14:29:53,437 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 14:29:54,154 main.py:51] epoch 800, training loss: 10960.79, average training loss: 11575.11, base loss: 14400.12
[INFO 2017-06-28 14:29:54,964 main.py:51] epoch 801, training loss: 10310.69, average training loss: 11573.53, base loss: 14400.52
[INFO 2017-06-28 14:29:55,739 main.py:51] epoch 802, training loss: 9903.20, average training loss: 11571.45, base loss: 14399.79
[INFO 2017-06-28 14:29:56,400 main.py:51] epoch 803, training loss: 9595.75, average training loss: 11568.99, base loss: 14397.77
[INFO 2017-06-28 14:29:57,159 main.py:51] epoch 804, training loss: 10990.59, average training loss: 11568.28, base loss: 14397.78
[INFO 2017-06-28 14:29:57,943 main.py:51] epoch 805, training loss: 10916.69, average training loss: 11567.47, base loss: 14398.88
[INFO 2017-06-28 14:29:58,550 main.py:51] epoch 806, training loss: 9447.55, average training loss: 11564.84, base loss: 14397.41
[INFO 2017-06-28 14:29:59,315 main.py:51] epoch 807, training loss: 12519.97, average training loss: 11566.02, base loss: 14400.37
[INFO 2017-06-28 14:30:00,141 main.py:51] epoch 808, training loss: 10785.69, average training loss: 11565.06, base loss: 14401.00
[INFO 2017-06-28 14:30:00,891 main.py:51] epoch 809, training loss: 12263.46, average training loss: 11565.92, base loss: 14402.30
[INFO 2017-06-28 14:30:01,579 main.py:51] epoch 810, training loss: 10616.05, average training loss: 11564.75, base loss: 14402.37
[INFO 2017-06-28 14:30:02,380 main.py:51] epoch 811, training loss: 9945.08, average training loss: 11562.75, base loss: 14401.78
[INFO 2017-06-28 14:30:03,213 main.py:51] epoch 812, training loss: 10968.50, average training loss: 11562.02, base loss: 14401.35
[INFO 2017-06-28 14:30:03,862 main.py:51] epoch 813, training loss: 11663.62, average training loss: 11562.15, base loss: 14403.01
[INFO 2017-06-28 14:30:04,648 main.py:51] epoch 814, training loss: 10397.77, average training loss: 11560.72, base loss: 14402.61
[INFO 2017-06-28 14:30:05,454 main.py:51] epoch 815, training loss: 10865.89, average training loss: 11559.87, base loss: 14403.79
[INFO 2017-06-28 14:30:06,106 main.py:51] epoch 816, training loss: 11244.12, average training loss: 11559.48, base loss: 14405.12
[INFO 2017-06-28 14:30:06,825 main.py:51] epoch 817, training loss: 11443.90, average training loss: 11559.34, base loss: 14407.02
[INFO 2017-06-28 14:30:07,695 main.py:51] epoch 818, training loss: 11394.45, average training loss: 11559.14, base loss: 14407.47
[INFO 2017-06-28 14:30:08,438 main.py:51] epoch 819, training loss: 11887.93, average training loss: 11559.54, base loss: 14407.47
[INFO 2017-06-28 14:30:09,147 main.py:51] epoch 820, training loss: 10907.50, average training loss: 11558.75, base loss: 14407.56
[INFO 2017-06-28 14:30:09,933 main.py:51] epoch 821, training loss: 10460.91, average training loss: 11557.41, base loss: 14406.94
[INFO 2017-06-28 14:30:10,688 main.py:51] epoch 822, training loss: 10375.79, average training loss: 11555.97, base loss: 14406.83
[INFO 2017-06-28 14:30:11,365 main.py:51] epoch 823, training loss: 10413.22, average training loss: 11554.59, base loss: 14405.42
[INFO 2017-06-28 14:30:12,134 main.py:51] epoch 824, training loss: 12129.24, average training loss: 11555.28, base loss: 14407.93
[INFO 2017-06-28 14:30:12,921 main.py:51] epoch 825, training loss: 11221.84, average training loss: 11554.88, base loss: 14408.95
[INFO 2017-06-28 14:30:13,621 main.py:51] epoch 826, training loss: 12135.45, average training loss: 11555.58, base loss: 14411.31
[INFO 2017-06-28 14:30:14,363 main.py:51] epoch 827, training loss: 12591.41, average training loss: 11556.83, base loss: 14414.51
[INFO 2017-06-28 14:30:15,180 main.py:51] epoch 828, training loss: 9214.12, average training loss: 11554.01, base loss: 14411.93
[INFO 2017-06-28 14:30:16,051 main.py:51] epoch 829, training loss: 11050.51, average training loss: 11553.40, base loss: 14411.47
[INFO 2017-06-28 14:30:16,686 main.py:51] epoch 830, training loss: 9966.56, average training loss: 11551.49, base loss: 14410.64
[INFO 2017-06-28 14:30:17,396 main.py:51] epoch 831, training loss: 9785.25, average training loss: 11549.37, base loss: 14410.41
[INFO 2017-06-28 14:30:18,241 main.py:51] epoch 832, training loss: 11575.98, average training loss: 11549.40, base loss: 14411.03
[INFO 2017-06-28 14:30:18,922 main.py:51] epoch 833, training loss: 11668.30, average training loss: 11549.54, base loss: 14413.41
[INFO 2017-06-28 14:30:19,664 main.py:51] epoch 834, training loss: 11656.04, average training loss: 11549.67, base loss: 14415.91
[INFO 2017-06-28 14:30:20,497 main.py:51] epoch 835, training loss: 10753.20, average training loss: 11548.72, base loss: 14416.07
[INFO 2017-06-28 14:30:21,291 main.py:51] epoch 836, training loss: 9974.14, average training loss: 11546.84, base loss: 14415.45
[INFO 2017-06-28 14:30:21,964 main.py:51] epoch 837, training loss: 9654.94, average training loss: 11544.58, base loss: 14413.60
[INFO 2017-06-28 14:30:22,748 main.py:51] epoch 838, training loss: 10330.23, average training loss: 11543.13, base loss: 14413.10
[INFO 2017-06-28 14:30:23,576 main.py:51] epoch 839, training loss: 9496.74, average training loss: 11540.69, base loss: 14410.59
[INFO 2017-06-28 14:30:24,246 main.py:51] epoch 840, training loss: 10237.18, average training loss: 11539.14, base loss: 14410.25
[INFO 2017-06-28 14:30:24,989 main.py:51] epoch 841, training loss: 11615.75, average training loss: 11539.24, base loss: 14411.96
[INFO 2017-06-28 14:30:25,810 main.py:51] epoch 842, training loss: 9291.25, average training loss: 11536.57, base loss: 14409.15
[INFO 2017-06-28 14:30:26,635 main.py:51] epoch 843, training loss: 9432.87, average training loss: 11534.08, base loss: 14407.78
[INFO 2017-06-28 14:30:27,260 main.py:51] epoch 844, training loss: 10795.86, average training loss: 11533.20, base loss: 14407.82
[INFO 2017-06-28 14:30:28,047 main.py:51] epoch 845, training loss: 11482.79, average training loss: 11533.14, base loss: 14409.30
[INFO 2017-06-28 14:30:28,873 main.py:51] epoch 846, training loss: 12600.89, average training loss: 11534.40, base loss: 14411.55
[INFO 2017-06-28 14:30:29,590 main.py:51] epoch 847, training loss: 10154.42, average training loss: 11532.78, base loss: 14411.40
[INFO 2017-06-28 14:30:30,328 main.py:51] epoch 848, training loss: 11842.30, average training loss: 11533.14, base loss: 14412.26
[INFO 2017-06-28 14:30:31,135 main.py:51] epoch 849, training loss: 11138.33, average training loss: 11532.68, base loss: 14414.44
[INFO 2017-06-28 14:30:32,000 main.py:51] epoch 850, training loss: 10219.99, average training loss: 11531.13, base loss: 14414.73
[INFO 2017-06-28 14:30:32,702 main.py:51] epoch 851, training loss: 10977.57, average training loss: 11530.48, base loss: 14413.81
[INFO 2017-06-28 14:30:33,434 main.py:51] epoch 852, training loss: 12294.90, average training loss: 11531.38, base loss: 14415.20
[INFO 2017-06-28 14:30:34,248 main.py:51] epoch 853, training loss: 13139.01, average training loss: 11533.26, base loss: 14417.23
[INFO 2017-06-28 14:30:35,009 main.py:51] epoch 854, training loss: 10996.92, average training loss: 11532.64, base loss: 14418.31
[INFO 2017-06-28 14:30:35,720 main.py:51] epoch 855, training loss: 10892.09, average training loss: 11531.89, base loss: 14419.43
[INFO 2017-06-28 14:30:36,496 main.py:51] epoch 856, training loss: 11224.29, average training loss: 11531.53, base loss: 14421.62
[INFO 2017-06-28 14:30:37,299 main.py:51] epoch 857, training loss: 11364.75, average training loss: 11531.33, base loss: 14422.80
[INFO 2017-06-28 14:30:37,956 main.py:51] epoch 858, training loss: 9580.69, average training loss: 11529.06, base loss: 14421.14
[INFO 2017-06-28 14:30:38,643 main.py:51] epoch 859, training loss: 9304.09, average training loss: 11526.48, base loss: 14418.86
[INFO 2017-06-28 14:30:39,425 main.py:51] epoch 860, training loss: 10026.35, average training loss: 11524.73, base loss: 14417.53
[INFO 2017-06-28 14:30:40,099 main.py:51] epoch 861, training loss: 9252.82, average training loss: 11522.10, base loss: 14414.72
[INFO 2017-06-28 14:30:40,869 main.py:51] epoch 862, training loss: 12208.39, average training loss: 11522.89, base loss: 14418.56
[INFO 2017-06-28 14:30:41,693 main.py:51] epoch 863, training loss: 12332.97, average training loss: 11523.83, base loss: 14421.07
[INFO 2017-06-28 14:30:42,501 main.py:51] epoch 864, training loss: 10577.14, average training loss: 11522.74, base loss: 14422.05
[INFO 2017-06-28 14:30:43,157 main.py:51] epoch 865, training loss: 10928.52, average training loss: 11522.05, base loss: 14422.91
[INFO 2017-06-28 14:30:43,964 main.py:51] epoch 866, training loss: 11590.36, average training loss: 11522.13, base loss: 14424.75
[INFO 2017-06-28 14:30:44,812 main.py:51] epoch 867, training loss: 10185.11, average training loss: 11520.59, base loss: 14422.89
[INFO 2017-06-28 14:30:45,540 main.py:51] epoch 868, training loss: 10183.18, average training loss: 11519.05, base loss: 14422.21
[INFO 2017-06-28 14:30:46,239 main.py:51] epoch 869, training loss: 11078.36, average training loss: 11518.54, base loss: 14423.12
[INFO 2017-06-28 14:30:47,030 main.py:51] epoch 870, training loss: 10738.78, average training loss: 11517.65, base loss: 14423.91
[INFO 2017-06-28 14:30:47,808 main.py:51] epoch 871, training loss: 11059.58, average training loss: 11517.12, base loss: 14425.03
[INFO 2017-06-28 14:30:48,478 main.py:51] epoch 872, training loss: 11226.37, average training loss: 11516.79, base loss: 14426.64
[INFO 2017-06-28 14:30:49,252 main.py:51] epoch 873, training loss: 11274.77, average training loss: 11516.51, base loss: 14427.71
[INFO 2017-06-28 14:30:50,073 main.py:51] epoch 874, training loss: 9796.99, average training loss: 11514.55, base loss: 14427.56
[INFO 2017-06-28 14:30:50,734 main.py:51] epoch 875, training loss: 10603.41, average training loss: 11513.51, base loss: 14427.70
[INFO 2017-06-28 14:30:51,486 main.py:51] epoch 876, training loss: 10768.71, average training loss: 11512.66, base loss: 14427.66
[INFO 2017-06-28 14:30:52,337 main.py:51] epoch 877, training loss: 10215.20, average training loss: 11511.18, base loss: 14426.02
[INFO 2017-06-28 14:30:53,078 main.py:51] epoch 878, training loss: 11942.88, average training loss: 11511.67, base loss: 14428.66
[INFO 2017-06-28 14:30:53,789 main.py:51] epoch 879, training loss: 10686.65, average training loss: 11510.73, base loss: 14427.68
[INFO 2017-06-28 14:30:54,556 main.py:51] epoch 880, training loss: 12035.15, average training loss: 11511.33, base loss: 14429.93
[INFO 2017-06-28 14:30:55,346 main.py:51] epoch 881, training loss: 12238.66, average training loss: 11512.15, base loss: 14432.05
[INFO 2017-06-28 14:30:55,968 main.py:51] epoch 882, training loss: 11437.06, average training loss: 11512.07, base loss: 14433.24
[INFO 2017-06-28 14:30:56,740 main.py:51] epoch 883, training loss: 9418.57, average training loss: 11509.70, base loss: 14430.36
[INFO 2017-06-28 14:30:57,607 main.py:51] epoch 884, training loss: 11235.86, average training loss: 11509.39, base loss: 14431.85
[INFO 2017-06-28 14:30:58,448 main.py:51] epoch 885, training loss: 11311.18, average training loss: 11509.17, base loss: 14434.02
[INFO 2017-06-28 14:30:59,127 main.py:51] epoch 886, training loss: 11181.65, average training loss: 11508.80, base loss: 14435.27
[INFO 2017-06-28 14:30:59,884 main.py:51] epoch 887, training loss: 12706.51, average training loss: 11510.15, base loss: 14437.91
[INFO 2017-06-28 14:31:00,760 main.py:51] epoch 888, training loss: 11228.73, average training loss: 11509.83, base loss: 14437.31
[INFO 2017-06-28 14:31:01,512 main.py:51] epoch 889, training loss: 10956.98, average training loss: 11509.21, base loss: 14436.93
[INFO 2017-06-28 14:31:02,230 main.py:51] epoch 890, training loss: 10426.84, average training loss: 11507.99, base loss: 14437.19
[INFO 2017-06-28 14:31:03,012 main.py:51] epoch 891, training loss: 10751.92, average training loss: 11507.15, base loss: 14437.81
[INFO 2017-06-28 14:31:03,864 main.py:51] epoch 892, training loss: 9779.54, average training loss: 11505.21, base loss: 14436.40
[INFO 2017-06-28 14:31:04,563 main.py:51] epoch 893, training loss: 11044.92, average training loss: 11504.70, base loss: 14437.28
[INFO 2017-06-28 14:31:05,289 main.py:51] epoch 894, training loss: 9479.44, average training loss: 11502.43, base loss: 14436.36
[INFO 2017-06-28 14:31:06,127 main.py:51] epoch 895, training loss: 12472.70, average training loss: 11503.52, base loss: 14439.29
[INFO 2017-06-28 14:31:06,986 main.py:51] epoch 896, training loss: 8205.52, average training loss: 11499.84, base loss: 14436.41
[INFO 2017-06-28 14:31:07,629 main.py:51] epoch 897, training loss: 10314.33, average training loss: 11498.52, base loss: 14435.95
[INFO 2017-06-28 14:31:08,342 main.py:51] epoch 898, training loss: 10592.68, average training loss: 11497.51, base loss: 14436.32
[INFO 2017-06-28 14:31:09,186 main.py:51] epoch 899, training loss: 10182.99, average training loss: 11496.05, base loss: 14435.60
[INFO 2017-06-28 14:31:09,186 main.py:53] epoch 899, testing
[INFO 2017-06-28 14:31:11,946 main.py:105] average testing loss: 11527.24, base loss: 15044.59
[INFO 2017-06-28 14:31:11,946 main.py:106] improve_loss: 3517.35, improve_percent: 0.23
[INFO 2017-06-28 14:31:11,947 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:31:11,963 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 14:31:12,612 main.py:51] epoch 900, training loss: 10610.60, average training loss: 11495.07, base loss: 14434.04
[INFO 2017-06-28 14:31:13,371 main.py:51] epoch 901, training loss: 11678.43, average training loss: 11495.27, base loss: 14434.09
[INFO 2017-06-28 14:31:14,216 main.py:51] epoch 902, training loss: 10153.01, average training loss: 11493.79, base loss: 14434.25
[INFO 2017-06-28 14:31:14,947 main.py:51] epoch 903, training loss: 9654.09, average training loss: 11491.75, base loss: 14432.84
[INFO 2017-06-28 14:31:15,646 main.py:51] epoch 904, training loss: 10001.00, average training loss: 11490.10, base loss: 14431.69
[INFO 2017-06-28 14:31:16,415 main.py:51] epoch 905, training loss: 8247.47, average training loss: 11486.53, base loss: 14427.76
[INFO 2017-06-28 14:31:17,026 main.py:51] epoch 906, training loss: 11270.73, average training loss: 11486.29, base loss: 14427.33
[INFO 2017-06-28 14:31:17,713 main.py:51] epoch 907, training loss: 11202.27, average training loss: 11485.97, base loss: 14428.34
[INFO 2017-06-28 14:31:18,328 main.py:51] epoch 908, training loss: 8873.24, average training loss: 11483.10, base loss: 14426.21
[INFO 2017-06-28 14:31:18,962 main.py:51] epoch 909, training loss: 9474.22, average training loss: 11480.89, base loss: 14424.77
[INFO 2017-06-28 14:31:19,686 main.py:51] epoch 910, training loss: 9402.08, average training loss: 11478.61, base loss: 14423.84
[INFO 2017-06-28 14:31:20,365 main.py:51] epoch 911, training loss: 9824.04, average training loss: 11476.80, base loss: 14422.91
[INFO 2017-06-28 14:31:21,162 main.py:51] epoch 912, training loss: 10452.38, average training loss: 11475.67, base loss: 14422.13
[INFO 2017-06-28 14:31:22,011 main.py:51] epoch 913, training loss: 12436.64, average training loss: 11476.73, base loss: 14424.79
[INFO 2017-06-28 14:31:22,709 main.py:51] epoch 914, training loss: 9583.92, average training loss: 11474.66, base loss: 14423.52
[INFO 2017-06-28 14:31:23,491 main.py:51] epoch 915, training loss: 9798.56, average training loss: 11472.83, base loss: 14423.80
[INFO 2017-06-28 14:31:24,269 main.py:51] epoch 916, training loss: 11011.22, average training loss: 11472.32, base loss: 14424.23
[INFO 2017-06-28 14:31:25,082 main.py:51] epoch 917, training loss: 9845.61, average training loss: 11470.55, base loss: 14423.08
[INFO 2017-06-28 14:31:25,758 main.py:51] epoch 918, training loss: 10148.94, average training loss: 11469.11, base loss: 14422.52
[INFO 2017-06-28 14:31:26,518 main.py:51] epoch 919, training loss: 10336.95, average training loss: 11467.88, base loss: 14422.17
[INFO 2017-06-28 14:31:27,346 main.py:51] epoch 920, training loss: 8957.82, average training loss: 11465.16, base loss: 14419.11
[INFO 2017-06-28 14:31:28,147 main.py:51] epoch 921, training loss: 13232.56, average training loss: 11467.08, base loss: 14422.56
[INFO 2017-06-28 14:31:28,837 main.py:51] epoch 922, training loss: 10743.58, average training loss: 11466.29, base loss: 14422.25
[INFO 2017-06-28 14:31:29,645 main.py:51] epoch 923, training loss: 10304.63, average training loss: 11465.03, base loss: 14421.94
[INFO 2017-06-28 14:31:30,492 main.py:51] epoch 924, training loss: 10286.35, average training loss: 11463.76, base loss: 14420.73
[INFO 2017-06-28 14:31:31,267 main.py:51] epoch 925, training loss: 9781.70, average training loss: 11461.94, base loss: 14419.37
[INFO 2017-06-28 14:31:31,976 main.py:51] epoch 926, training loss: 11018.25, average training loss: 11461.46, base loss: 14419.60
[INFO 2017-06-28 14:31:32,811 main.py:51] epoch 927, training loss: 10010.50, average training loss: 11459.90, base loss: 14419.50
[INFO 2017-06-28 14:31:33,635 main.py:51] epoch 928, training loss: 12906.54, average training loss: 11461.46, base loss: 14422.18
[INFO 2017-06-28 14:31:34,400 main.py:51] epoch 929, training loss: 11466.90, average training loss: 11461.46, base loss: 14422.92
[INFO 2017-06-28 14:31:35,107 main.py:51] epoch 930, training loss: 10842.83, average training loss: 11460.80, base loss: 14423.02
[INFO 2017-06-28 14:31:35,891 main.py:51] epoch 931, training loss: 10366.45, average training loss: 11459.63, base loss: 14422.01
[INFO 2017-06-28 14:31:36,718 main.py:51] epoch 932, training loss: 11739.45, average training loss: 11459.93, base loss: 14423.37
[INFO 2017-06-28 14:31:37,374 main.py:51] epoch 933, training loss: 10817.07, average training loss: 11459.24, base loss: 14423.67
[INFO 2017-06-28 14:31:38,139 main.py:51] epoch 934, training loss: 11156.88, average training loss: 11458.91, base loss: 14424.88
[INFO 2017-06-28 14:31:38,974 main.py:51] epoch 935, training loss: 10930.35, average training loss: 11458.35, base loss: 14424.31
[INFO 2017-06-28 14:31:39,837 main.py:51] epoch 936, training loss: 9871.50, average training loss: 11456.66, base loss: 14423.25
[INFO 2017-06-28 14:31:40,505 main.py:51] epoch 937, training loss: 10625.02, average training loss: 11455.77, base loss: 14423.67
[INFO 2017-06-28 14:31:41,247 main.py:51] epoch 938, training loss: 10973.38, average training loss: 11455.26, base loss: 14424.47
[INFO 2017-06-28 14:31:42,062 main.py:51] epoch 939, training loss: 10430.92, average training loss: 11454.17, base loss: 14423.95
[INFO 2017-06-28 14:31:42,685 main.py:51] epoch 940, training loss: 10765.23, average training loss: 11453.43, base loss: 14423.98
[INFO 2017-06-28 14:31:43,477 main.py:51] epoch 941, training loss: 10307.41, average training loss: 11452.22, base loss: 14424.45
[INFO 2017-06-28 14:31:44,287 main.py:51] epoch 942, training loss: 11760.25, average training loss: 11452.54, base loss: 14425.80
[INFO 2017-06-28 14:31:45,137 main.py:51] epoch 943, training loss: 11228.51, average training loss: 11452.31, base loss: 14426.34
[INFO 2017-06-28 14:31:45,809 main.py:51] epoch 944, training loss: 10506.92, average training loss: 11451.31, base loss: 14426.90
[INFO 2017-06-28 14:31:46,626 main.py:51] epoch 945, training loss: 10525.22, average training loss: 11450.33, base loss: 14425.32
[INFO 2017-06-28 14:31:47,464 main.py:51] epoch 946, training loss: 11907.44, average training loss: 11450.81, base loss: 14426.34
[INFO 2017-06-28 14:31:48,309 main.py:51] epoch 947, training loss: 10598.65, average training loss: 11449.91, base loss: 14426.69
[INFO 2017-06-28 14:31:48,968 main.py:51] epoch 948, training loss: 11022.41, average training loss: 11449.46, base loss: 14427.91
[INFO 2017-06-28 14:31:49,727 main.py:51] epoch 949, training loss: 12950.12, average training loss: 11451.04, base loss: 14429.70
[INFO 2017-06-28 14:31:50,550 main.py:51] epoch 950, training loss: 9944.21, average training loss: 11449.46, base loss: 14429.06
[INFO 2017-06-28 14:31:51,176 main.py:51] epoch 951, training loss: 10575.02, average training loss: 11448.54, base loss: 14428.58
[INFO 2017-06-28 14:31:51,971 main.py:51] epoch 952, training loss: 11430.96, average training loss: 11448.52, base loss: 14429.76
[INFO 2017-06-28 14:31:52,786 main.py:51] epoch 953, training loss: 10500.61, average training loss: 11447.52, base loss: 14429.91
[INFO 2017-06-28 14:31:53,599 main.py:51] epoch 954, training loss: 11112.71, average training loss: 11447.17, base loss: 14430.43
[INFO 2017-06-28 14:31:54,245 main.py:51] epoch 955, training loss: 9957.80, average training loss: 11445.62, base loss: 14431.17
[INFO 2017-06-28 14:31:55,014 main.py:51] epoch 956, training loss: 9750.50, average training loss: 11443.84, base loss: 14430.17
[INFO 2017-06-28 14:31:55,838 main.py:51] epoch 957, training loss: 10354.70, average training loss: 11442.71, base loss: 14430.45
[INFO 2017-06-28 14:31:56,469 main.py:51] epoch 958, training loss: 10355.81, average training loss: 11441.57, base loss: 14431.67
[INFO 2017-06-28 14:31:57,237 main.py:51] epoch 959, training loss: 11266.02, average training loss: 11441.39, base loss: 14432.27
[INFO 2017-06-28 14:31:58,054 main.py:51] epoch 960, training loss: 11170.19, average training loss: 11441.11, base loss: 14433.77
[INFO 2017-06-28 14:31:58,843 main.py:51] epoch 961, training loss: 10516.67, average training loss: 11440.15, base loss: 14434.28
[INFO 2017-06-28 14:31:59,513 main.py:51] epoch 962, training loss: 9704.71, average training loss: 11438.35, base loss: 14432.95
[INFO 2017-06-28 14:32:00,296 main.py:51] epoch 963, training loss: 9265.24, average training loss: 11436.09, base loss: 14430.23
[INFO 2017-06-28 14:32:01,117 main.py:51] epoch 964, training loss: 10652.78, average training loss: 11435.28, base loss: 14429.84
[INFO 2017-06-28 14:32:01,723 main.py:51] epoch 965, training loss: 9108.17, average training loss: 11432.87, base loss: 14428.81
[INFO 2017-06-28 14:32:02,507 main.py:51] epoch 966, training loss: 9941.83, average training loss: 11431.33, base loss: 14426.12
[INFO 2017-06-28 14:32:03,357 main.py:51] epoch 967, training loss: 10489.50, average training loss: 11430.36, base loss: 14425.80
[INFO 2017-06-28 14:32:03,988 main.py:51] epoch 968, training loss: 11154.46, average training loss: 11430.07, base loss: 14427.43
[INFO 2017-06-28 14:32:04,775 main.py:51] epoch 969, training loss: 11129.63, average training loss: 11429.76, base loss: 14428.09
[INFO 2017-06-28 14:32:05,611 main.py:51] epoch 970, training loss: 10275.84, average training loss: 11428.57, base loss: 14428.33
[INFO 2017-06-28 14:32:06,393 main.py:51] epoch 971, training loss: 10458.59, average training loss: 11427.58, base loss: 14427.88
[INFO 2017-06-28 14:32:07,065 main.py:51] epoch 972, training loss: 10615.82, average training loss: 11426.74, base loss: 14428.92
[INFO 2017-06-28 14:32:07,860 main.py:51] epoch 973, training loss: 12427.57, average training loss: 11427.77, base loss: 14431.60
[INFO 2017-06-28 14:32:08,710 main.py:51] epoch 974, training loss: 10947.83, average training loss: 11427.28, base loss: 14431.05
[INFO 2017-06-28 14:32:09,408 main.py:51] epoch 975, training loss: 9353.65, average training loss: 11425.15, base loss: 14427.61
[INFO 2017-06-28 14:32:10,097 main.py:51] epoch 976, training loss: 9571.98, average training loss: 11423.26, base loss: 14426.66
[INFO 2017-06-28 14:32:10,963 main.py:51] epoch 977, training loss: 11272.16, average training loss: 11423.10, base loss: 14426.87
[INFO 2017-06-28 14:32:11,676 main.py:51] epoch 978, training loss: 11110.46, average training loss: 11422.78, base loss: 14427.82
[INFO 2017-06-28 14:32:12,395 main.py:51] epoch 979, training loss: 12357.74, average training loss: 11423.74, base loss: 14429.37
[INFO 2017-06-28 14:32:13,195 main.py:51] epoch 980, training loss: 10744.56, average training loss: 11423.04, base loss: 14428.74
[INFO 2017-06-28 14:32:14,050 main.py:51] epoch 981, training loss: 10636.79, average training loss: 11422.24, base loss: 14428.41
[INFO 2017-06-28 14:32:14,667 main.py:51] epoch 982, training loss: 10886.25, average training loss: 11421.70, base loss: 14428.33
[INFO 2017-06-28 14:32:15,417 main.py:51] epoch 983, training loss: 11006.85, average training loss: 11421.28, base loss: 14428.43
[INFO 2017-06-28 14:32:16,248 main.py:51] epoch 984, training loss: 9632.46, average training loss: 11419.46, base loss: 14427.37
[INFO 2017-06-28 14:32:16,908 main.py:51] epoch 985, training loss: 11522.00, average training loss: 11419.56, base loss: 14428.19
[INFO 2017-06-28 14:32:17,663 main.py:51] epoch 986, training loss: 9158.44, average training loss: 11417.27, base loss: 14425.23
[INFO 2017-06-28 14:32:18,507 main.py:51] epoch 987, training loss: 12252.06, average training loss: 11418.12, base loss: 14427.65
[INFO 2017-06-28 14:32:19,334 main.py:51] epoch 988, training loss: 10110.15, average training loss: 11416.80, base loss: 14426.71
[INFO 2017-06-28 14:32:19,984 main.py:51] epoch 989, training loss: 14739.05, average training loss: 11420.15, base loss: 14430.79
[INFO 2017-06-28 14:32:20,756 main.py:51] epoch 990, training loss: 10281.81, average training loss: 11419.00, base loss: 14430.04
[INFO 2017-06-28 14:32:21,623 main.py:51] epoch 991, training loss: 11187.05, average training loss: 11418.77, base loss: 14430.51
[INFO 2017-06-28 14:32:22,269 main.py:51] epoch 992, training loss: 10451.14, average training loss: 11417.79, base loss: 14430.33
[INFO 2017-06-28 14:32:23,022 main.py:51] epoch 993, training loss: 9394.70, average training loss: 11415.76, base loss: 14428.62
[INFO 2017-06-28 14:32:23,843 main.py:51] epoch 994, training loss: 11185.38, average training loss: 11415.53, base loss: 14428.76
[INFO 2017-06-28 14:32:24,685 main.py:51] epoch 995, training loss: 10863.23, average training loss: 11414.97, base loss: 14428.89
[INFO 2017-06-28 14:32:25,323 main.py:51] epoch 996, training loss: 11407.27, average training loss: 11414.96, base loss: 14429.81
[INFO 2017-06-28 14:32:26,145 main.py:51] epoch 997, training loss: 11191.37, average training loss: 11414.74, base loss: 14430.97
[INFO 2017-06-28 14:32:26,973 main.py:51] epoch 998, training loss: 8700.92, average training loss: 11412.02, base loss: 14428.83
[INFO 2017-06-28 14:32:27,738 main.py:51] epoch 999, training loss: 9124.27, average training loss: 11409.74, base loss: 14426.99
[INFO 2017-06-28 14:32:27,738 main.py:53] epoch 999, testing
[INFO 2017-06-28 14:32:30,648 main.py:105] average testing loss: 11515.07, base loss: 15313.19
[INFO 2017-06-28 14:32:30,648 main.py:106] improve_loss: 3798.12, improve_percent: 0.25
[INFO 2017-06-28 14:32:30,649 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:32:30,665 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:32:31,307 main.py:51] epoch 1000, training loss: 10104.33, average training loss: 11389.90, base loss: 14426.87
[INFO 2017-06-28 14:32:32,110 main.py:51] epoch 1001, training loss: 8922.45, average training loss: 11376.69, base loss: 14427.02
[INFO 2017-06-28 14:32:32,957 main.py:51] epoch 1002, training loss: 11130.23, average training loss: 11365.20, base loss: 14427.15
[INFO 2017-06-28 14:32:33,726 main.py:51] epoch 1003, training loss: 11176.22, average training loss: 11356.75, base loss: 14428.19
[INFO 2017-06-28 14:32:34,388 main.py:51] epoch 1004, training loss: 10395.16, average training loss: 11348.39, base loss: 14428.09
[INFO 2017-06-28 14:32:35,181 main.py:51] epoch 1005, training loss: 10791.97, average training loss: 11339.96, base loss: 14427.66
[INFO 2017-06-28 14:32:36,025 main.py:51] epoch 1006, training loss: 10378.98, average training loss: 11333.75, base loss: 14427.44
[INFO 2017-06-28 14:32:36,680 main.py:51] epoch 1007, training loss: 9789.60, average training loss: 11326.54, base loss: 14427.10
[INFO 2017-06-28 14:32:37,426 main.py:51] epoch 1008, training loss: 12202.35, average training loss: 11321.61, base loss: 14427.90
[INFO 2017-06-28 14:32:38,261 main.py:51] epoch 1009, training loss: 10862.91, average training loss: 11314.36, base loss: 14426.65
[INFO 2017-06-28 14:32:39,032 main.py:51] epoch 1010, training loss: 9907.25, average training loss: 11308.84, base loss: 14426.07
[INFO 2017-06-28 14:32:39,726 main.py:51] epoch 1011, training loss: 11284.53, average training loss: 11305.51, base loss: 14428.16
[INFO 2017-06-28 14:32:40,503 main.py:51] epoch 1012, training loss: 9164.17, average training loss: 11299.36, base loss: 14426.45
[INFO 2017-06-28 14:32:41,320 main.py:51] epoch 1013, training loss: 9920.46, average training loss: 11295.21, base loss: 14427.18
[INFO 2017-06-28 14:32:41,945 main.py:51] epoch 1014, training loss: 10281.85, average training loss: 11292.67, base loss: 14429.15
[INFO 2017-06-28 14:32:42,769 main.py:51] epoch 1015, training loss: 9308.32, average training loss: 11288.69, base loss: 14429.69
[INFO 2017-06-28 14:32:43,590 main.py:51] epoch 1016, training loss: 11607.13, average training loss: 11284.80, base loss: 14430.30
[INFO 2017-06-28 14:32:44,396 main.py:51] epoch 1017, training loss: 9280.45, average training loss: 11280.33, base loss: 14429.34
[INFO 2017-06-28 14:32:45,064 main.py:51] epoch 1018, training loss: 10988.96, average training loss: 11275.31, base loss: 14427.24
[INFO 2017-06-28 14:32:45,852 main.py:51] epoch 1019, training loss: 12319.12, average training loss: 11272.15, base loss: 14428.51
[INFO 2017-06-28 14:32:46,664 main.py:51] epoch 1020, training loss: 10169.11, average training loss: 11266.64, base loss: 14426.58
[INFO 2017-06-28 14:32:47,317 main.py:51] epoch 1021, training loss: 10942.53, average training loss: 11264.10, base loss: 14427.38
[INFO 2017-06-28 14:32:48,034 main.py:51] epoch 1022, training loss: 10853.69, average training loss: 11261.36, base loss: 14428.22
[INFO 2017-06-28 14:32:48,842 main.py:51] epoch 1023, training loss: 12637.03, average training loss: 11260.82, base loss: 14431.76
[INFO 2017-06-28 14:32:49,508 main.py:51] epoch 1024, training loss: 9881.74, average training loss: 11256.64, base loss: 14430.63
[INFO 2017-06-28 14:32:50,270 main.py:51] epoch 1025, training loss: 11320.57, average training loss: 11256.57, base loss: 14434.04
[INFO 2017-06-28 14:32:51,044 main.py:51] epoch 1026, training loss: 10779.06, average training loss: 11253.54, base loss: 14434.20
[INFO 2017-06-28 14:32:51,805 main.py:51] epoch 1027, training loss: 10181.64, average training loss: 11248.70, base loss: 14433.41
[INFO 2017-06-28 14:32:52,473 main.py:51] epoch 1028, training loss: 9492.11, average training loss: 11242.73, base loss: 14429.56
[INFO 2017-06-28 14:32:53,242 main.py:51] epoch 1029, training loss: 11741.32, average training loss: 11239.95, base loss: 14430.27
[INFO 2017-06-28 14:32:54,075 main.py:51] epoch 1030, training loss: 10083.36, average training loss: 11238.95, base loss: 14433.10
[INFO 2017-06-28 14:32:54,795 main.py:51] epoch 1031, training loss: 11164.07, average training loss: 11233.64, base loss: 14430.50
[INFO 2017-06-28 14:32:55,505 main.py:51] epoch 1032, training loss: 10805.61, average training loss: 11233.26, base loss: 14434.18
[INFO 2017-06-28 14:32:56,323 main.py:51] epoch 1033, training loss: 10120.63, average training loss: 11230.86, base loss: 14434.56
[INFO 2017-06-28 14:32:57,148 main.py:51] epoch 1034, training loss: 11194.40, average training loss: 11229.19, base loss: 14437.05
[INFO 2017-06-28 14:32:57,847 main.py:51] epoch 1035, training loss: 11768.50, average training loss: 11227.08, base loss: 14437.30
[INFO 2017-06-28 14:32:58,588 main.py:51] epoch 1036, training loss: 11287.99, average training loss: 11224.73, base loss: 14438.43
[INFO 2017-06-28 14:32:59,414 main.py:51] epoch 1037, training loss: 10111.89, average training loss: 11220.27, base loss: 14437.55
[INFO 2017-06-28 14:33:00,286 main.py:51] epoch 1038, training loss: 10341.92, average training loss: 11217.37, base loss: 14438.59
[INFO 2017-06-28 14:33:00,922 main.py:51] epoch 1039, training loss: 12588.62, average training loss: 11214.78, base loss: 14439.00
[INFO 2017-06-28 14:33:01,732 main.py:51] epoch 1040, training loss: 11334.49, average training loss: 11213.92, base loss: 14441.15
[INFO 2017-06-28 14:33:02,546 main.py:51] epoch 1041, training loss: 11093.38, average training loss: 11213.17, base loss: 14444.72
[INFO 2017-06-28 14:33:03,330 main.py:51] epoch 1042, training loss: 11058.94, average training loss: 11210.85, base loss: 14445.75
[INFO 2017-06-28 14:33:04,003 main.py:51] epoch 1043, training loss: 11882.16, average training loss: 11210.48, base loss: 14448.90
[INFO 2017-06-28 14:33:04,803 main.py:51] epoch 1044, training loss: 8953.27, average training loss: 11205.14, base loss: 14446.42
[INFO 2017-06-28 14:33:05,676 main.py:51] epoch 1045, training loss: 9634.45, average training loss: 11200.14, base loss: 14444.41
[INFO 2017-06-28 14:33:06,381 main.py:51] epoch 1046, training loss: 10131.94, average training loss: 11197.15, base loss: 14443.29
[INFO 2017-06-28 14:33:07,078 main.py:51] epoch 1047, training loss: 12733.15, average training loss: 11196.27, base loss: 14446.03
[INFO 2017-06-28 14:33:07,789 main.py:51] epoch 1048, training loss: 10961.33, average training loss: 11192.79, base loss: 14444.69
[INFO 2017-06-28 14:33:08,449 main.py:51] epoch 1049, training loss: 9178.71, average training loss: 11187.57, base loss: 14441.23
[INFO 2017-06-28 14:33:09,042 main.py:51] epoch 1050, training loss: 9897.11, average training loss: 11185.34, base loss: 14442.19
[INFO 2017-06-28 14:33:09,663 main.py:51] epoch 1051, training loss: 11292.81, average training loss: 11184.75, base loss: 14446.22
[INFO 2017-06-28 14:33:10,464 main.py:51] epoch 1052, training loss: 10646.54, average training loss: 11184.28, base loss: 14449.84
[INFO 2017-06-28 14:33:11,287 main.py:51] epoch 1053, training loss: 9827.53, average training loss: 11180.68, base loss: 14449.36
[INFO 2017-06-28 14:33:11,978 main.py:51] epoch 1054, training loss: 10810.35, average training loss: 11175.07, base loss: 14446.26
[INFO 2017-06-28 14:33:12,716 main.py:51] epoch 1055, training loss: 10654.08, average training loss: 11170.36, base loss: 14445.36
[INFO 2017-06-28 14:33:13,538 main.py:51] epoch 1056, training loss: 10834.20, average training loss: 11167.91, base loss: 14446.06
[INFO 2017-06-28 14:33:14,340 main.py:51] epoch 1057, training loss: 9794.87, average training loss: 11165.95, base loss: 14446.96
[INFO 2017-06-28 14:33:15,006 main.py:51] epoch 1058, training loss: 9342.41, average training loss: 11162.09, base loss: 14447.38
[INFO 2017-06-28 14:33:15,763 main.py:51] epoch 1059, training loss: 11971.40, average training loss: 11160.87, base loss: 14449.28
[INFO 2017-06-28 14:33:16,550 main.py:51] epoch 1060, training loss: 9589.47, average training loss: 11157.71, base loss: 14449.46
[INFO 2017-06-28 14:33:17,250 main.py:51] epoch 1061, training loss: 11029.92, average training loss: 11155.63, base loss: 14449.33
[INFO 2017-06-28 14:33:18,001 main.py:51] epoch 1062, training loss: 9327.31, average training loss: 11149.38, base loss: 14444.59
[INFO 2017-06-28 14:33:18,807 main.py:51] epoch 1063, training loss: 9706.85, average training loss: 11145.20, base loss: 14442.38
[INFO 2017-06-28 14:33:19,640 main.py:51] epoch 1064, training loss: 9853.11, average training loss: 11141.40, base loss: 14440.58
[INFO 2017-06-28 14:33:20,307 main.py:51] epoch 1065, training loss: 11221.02, average training loss: 11139.72, base loss: 14441.40
[INFO 2017-06-28 14:33:21,097 main.py:51] epoch 1066, training loss: 9630.07, average training loss: 11136.72, base loss: 14441.44
[INFO 2017-06-28 14:33:21,910 main.py:51] epoch 1067, training loss: 9588.83, average training loss: 11133.12, base loss: 14439.38
[INFO 2017-06-28 14:33:22,741 main.py:51] epoch 1068, training loss: 9988.33, average training loss: 11130.09, base loss: 14438.48
[INFO 2017-06-28 14:33:23,382 main.py:51] epoch 1069, training loss: 10935.96, average training loss: 11124.88, base loss: 14434.84
[INFO 2017-06-28 14:33:24,160 main.py:51] epoch 1070, training loss: 11462.35, average training loss: 11123.84, base loss: 14438.21
[INFO 2017-06-28 14:33:25,008 main.py:51] epoch 1071, training loss: 9724.09, average training loss: 11121.39, base loss: 14438.54
[INFO 2017-06-28 14:33:25,829 main.py:51] epoch 1072, training loss: 9360.13, average training loss: 11117.11, base loss: 14436.92
[INFO 2017-06-28 14:33:26,452 main.py:51] epoch 1073, training loss: 11020.59, average training loss: 11116.81, base loss: 14439.34
[INFO 2017-06-28 14:33:27,239 main.py:51] epoch 1074, training loss: 11326.49, average training loss: 11116.65, base loss: 14441.87
[INFO 2017-06-28 14:33:28,091 main.py:51] epoch 1075, training loss: 11408.97, average training loss: 11115.82, base loss: 14444.12
[INFO 2017-06-28 14:33:28,786 main.py:51] epoch 1076, training loss: 9323.04, average training loss: 11111.95, base loss: 14442.63
[INFO 2017-06-28 14:33:29,540 main.py:51] epoch 1077, training loss: 9097.96, average training loss: 11108.56, base loss: 14440.71
[INFO 2017-06-28 14:33:30,356 main.py:51] epoch 1078, training loss: 10821.10, average training loss: 11107.00, base loss: 14440.98
[INFO 2017-06-28 14:33:31,157 main.py:51] epoch 1079, training loss: 11583.20, average training loss: 11104.33, base loss: 14440.55
[INFO 2017-06-28 14:33:31,843 main.py:51] epoch 1080, training loss: 11485.46, average training loss: 11100.79, base loss: 14440.63
[INFO 2017-06-28 14:33:32,585 main.py:51] epoch 1081, training loss: 8352.56, average training loss: 11095.58, base loss: 14436.66
[INFO 2017-06-28 14:33:33,402 main.py:51] epoch 1082, training loss: 11408.25, average training loss: 11094.13, base loss: 14438.16
[INFO 2017-06-28 14:33:34,249 main.py:51] epoch 1083, training loss: 10168.43, average training loss: 11089.35, base loss: 14434.76
[INFO 2017-06-28 14:33:34,917 main.py:51] epoch 1084, training loss: 12258.37, average training loss: 11089.36, base loss: 14437.84
[INFO 2017-06-28 14:33:35,688 main.py:51] epoch 1085, training loss: 11771.52, average training loss: 11087.95, base loss: 14439.39
[INFO 2017-06-28 14:33:36,492 main.py:51] epoch 1086, training loss: 10552.01, average training loss: 11086.33, base loss: 14439.10
[INFO 2017-06-28 14:33:37,197 main.py:51] epoch 1087, training loss: 9743.10, average training loss: 11082.87, base loss: 14438.62
[INFO 2017-06-28 14:33:37,887 main.py:51] epoch 1088, training loss: 9665.56, average training loss: 11080.44, base loss: 14439.28
[INFO 2017-06-28 14:33:38,700 main.py:51] epoch 1089, training loss: 10639.55, average training loss: 11079.25, base loss: 14439.60
[INFO 2017-06-28 14:33:39,529 main.py:51] epoch 1090, training loss: 10965.21, average training loss: 11077.98, base loss: 14442.19
[INFO 2017-06-28 14:33:40,203 main.py:51] epoch 1091, training loss: 10675.55, average training loss: 11073.71, base loss: 14439.18
[INFO 2017-06-28 14:33:40,987 main.py:51] epoch 1092, training loss: 9801.88, average training loss: 11071.60, base loss: 14438.73
[INFO 2017-06-28 14:33:41,777 main.py:51] epoch 1093, training loss: 11174.89, average training loss: 11070.31, base loss: 14439.25
[INFO 2017-06-28 14:33:42,457 main.py:51] epoch 1094, training loss: 9566.34, average training loss: 11066.51, base loss: 14437.45
[INFO 2017-06-28 14:33:43,214 main.py:51] epoch 1095, training loss: 13036.38, average training loss: 11066.10, base loss: 14440.68
[INFO 2017-06-28 14:33:44,006 main.py:51] epoch 1096, training loss: 11011.06, average training loss: 11066.10, base loss: 14444.84
[INFO 2017-06-28 14:33:44,838 main.py:51] epoch 1097, training loss: 10123.44, average training loss: 11063.76, base loss: 14443.94
[INFO 2017-06-28 14:33:45,459 main.py:51] epoch 1098, training loss: 10770.48, average training loss: 11061.87, base loss: 14443.93
[INFO 2017-06-28 14:33:46,267 main.py:51] epoch 1099, training loss: 10884.68, average training loss: 11059.77, base loss: 14443.88
[INFO 2017-06-28 14:33:46,268 main.py:53] epoch 1099, testing
[INFO 2017-06-28 14:33:49,192 main.py:105] average testing loss: 11921.12, base loss: 15413.32
[INFO 2017-06-28 14:33:49,192 main.py:106] improve_loss: 3492.20, improve_percent: 0.23
[INFO 2017-06-28 14:33:49,193 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:33:49,947 main.py:51] epoch 1100, training loss: 9542.58, average training loss: 11057.24, base loss: 14443.20
[INFO 2017-06-28 14:33:50,758 main.py:51] epoch 1101, training loss: 10696.21, average training loss: 11055.36, base loss: 14442.17
[INFO 2017-06-28 14:33:51,381 main.py:51] epoch 1102, training loss: 11418.65, average training loss: 11054.00, base loss: 14441.09
[INFO 2017-06-28 14:33:52,201 main.py:51] epoch 1103, training loss: 10954.25, average training loss: 11052.09, base loss: 14440.26
[INFO 2017-06-28 14:33:53,022 main.py:51] epoch 1104, training loss: 8055.43, average training loss: 11048.43, base loss: 14438.91
[INFO 2017-06-28 14:33:53,734 main.py:51] epoch 1105, training loss: 11828.23, average training loss: 11048.57, base loss: 14441.34
[INFO 2017-06-28 14:33:54,448 main.py:51] epoch 1106, training loss: 12361.57, average training loss: 11049.51, base loss: 14445.39
[INFO 2017-06-28 14:33:55,233 main.py:51] epoch 1107, training loss: 11255.32, average training loss: 11046.63, base loss: 14444.53
[INFO 2017-06-28 14:33:56,038 main.py:51] epoch 1108, training loss: 12403.40, average training loss: 11047.73, base loss: 14448.97
[INFO 2017-06-28 14:33:56,659 main.py:51] epoch 1109, training loss: 10705.15, average training loss: 11046.84, base loss: 14450.07
[INFO 2017-06-28 14:33:57,410 main.py:51] epoch 1110, training loss: 9069.66, average training loss: 11043.28, base loss: 14448.16
[INFO 2017-06-28 14:33:58,185 main.py:51] epoch 1111, training loss: 10167.60, average training loss: 11038.90, base loss: 14444.94
[INFO 2017-06-28 14:33:58,800 main.py:51] epoch 1112, training loss: 11927.47, average training loss: 11037.90, base loss: 14447.07
[INFO 2017-06-28 14:33:59,602 main.py:51] epoch 1113, training loss: 9040.21, average training loss: 11036.31, base loss: 14447.07
[INFO 2017-06-28 14:34:00,437 main.py:51] epoch 1114, training loss: 11742.98, average training loss: 11034.61, base loss: 14445.78
[INFO 2017-06-28 14:34:01,262 main.py:51] epoch 1115, training loss: 9007.82, average training loss: 11031.49, base loss: 14444.52
[INFO 2017-06-28 14:34:01,940 main.py:51] epoch 1116, training loss: 11629.10, average training loss: 11029.08, base loss: 14443.79
[INFO 2017-06-28 14:34:02,764 main.py:51] epoch 1117, training loss: 9650.59, average training loss: 11027.74, base loss: 14444.50
[INFO 2017-06-28 14:34:03,586 main.py:51] epoch 1118, training loss: 11635.76, average training loss: 11025.20, base loss: 14443.76
[INFO 2017-06-28 14:34:04,367 main.py:51] epoch 1119, training loss: 9566.55, average training loss: 11023.37, base loss: 14444.53
[INFO 2017-06-28 14:34:05,001 main.py:51] epoch 1120, training loss: 9760.67, average training loss: 11022.35, base loss: 14446.09
[INFO 2017-06-28 14:34:05,768 main.py:51] epoch 1121, training loss: 10916.63, average training loss: 11020.73, base loss: 14447.17
[INFO 2017-06-28 14:34:06,599 main.py:51] epoch 1122, training loss: 10550.38, average training loss: 11019.00, base loss: 14446.32
[INFO 2017-06-28 14:34:07,241 main.py:51] epoch 1123, training loss: 10105.96, average training loss: 11015.84, base loss: 14443.91
[INFO 2017-06-28 14:34:07,989 main.py:51] epoch 1124, training loss: 9816.80, average training loss: 11013.59, base loss: 14442.80
[INFO 2017-06-28 14:34:08,813 main.py:51] epoch 1125, training loss: 11139.26, average training loss: 11013.11, base loss: 14444.06
[INFO 2017-06-28 14:34:09,505 main.py:51] epoch 1126, training loss: 12074.46, average training loss: 11012.34, base loss: 14445.35
[INFO 2017-06-28 14:34:10,249 main.py:51] epoch 1127, training loss: 9523.00, average training loss: 11010.49, base loss: 14446.49
[INFO 2017-06-28 14:34:11,047 main.py:51] epoch 1128, training loss: 11083.27, average training loss: 11010.76, base loss: 14449.29
[INFO 2017-06-28 14:34:11,902 main.py:51] epoch 1129, training loss: 10624.22, average training loss: 11009.44, base loss: 14449.45
[INFO 2017-06-28 14:34:12,537 main.py:51] epoch 1130, training loss: 10988.29, average training loss: 11007.38, base loss: 14449.94
[INFO 2017-06-28 14:34:13,316 main.py:51] epoch 1131, training loss: 9499.52, average training loss: 11004.19, base loss: 14446.97
[INFO 2017-06-28 14:34:14,111 main.py:51] epoch 1132, training loss: 9769.84, average training loss: 11001.79, base loss: 14445.93
[INFO 2017-06-28 14:34:14,853 main.py:51] epoch 1133, training loss: 9129.94, average training loss: 11000.20, base loss: 14446.84
[INFO 2017-06-28 14:34:15,539 main.py:51] epoch 1134, training loss: 10001.58, average training loss: 10996.15, base loss: 14444.24
[INFO 2017-06-28 14:34:16,368 main.py:51] epoch 1135, training loss: 9684.79, average training loss: 10993.41, base loss: 14443.33
[INFO 2017-06-28 14:34:17,236 main.py:51] epoch 1136, training loss: 11346.89, average training loss: 10991.16, base loss: 14443.36
[INFO 2017-06-28 14:34:17,871 main.py:51] epoch 1137, training loss: 11248.21, average training loss: 10990.45, base loss: 14444.65
[INFO 2017-06-28 14:34:18,688 main.py:51] epoch 1138, training loss: 11550.77, average training loss: 10989.60, base loss: 14445.24
[INFO 2017-06-28 14:34:19,519 main.py:51] epoch 1139, training loss: 10922.01, average training loss: 10988.66, base loss: 14445.77
[INFO 2017-06-28 14:34:20,319 main.py:51] epoch 1140, training loss: 9991.97, average training loss: 10986.43, base loss: 14444.96
[INFO 2017-06-28 14:34:20,986 main.py:51] epoch 1141, training loss: 9941.12, average training loss: 10983.93, base loss: 14444.78
[INFO 2017-06-28 14:34:21,766 main.py:51] epoch 1142, training loss: 8780.29, average training loss: 10981.28, base loss: 14443.44
[INFO 2017-06-28 14:34:22,624 main.py:51] epoch 1143, training loss: 11675.84, average training loss: 10979.12, base loss: 14441.26
[INFO 2017-06-28 14:34:23,330 main.py:51] epoch 1144, training loss: 9985.03, average training loss: 10975.88, base loss: 14439.61
[INFO 2017-06-28 14:34:24,058 main.py:51] epoch 1145, training loss: 11011.54, average training loss: 10975.10, base loss: 14441.95
[INFO 2017-06-28 14:34:24,873 main.py:51] epoch 1146, training loss: 9886.35, average training loss: 10972.73, base loss: 14441.12
[INFO 2017-06-28 14:34:25,715 main.py:51] epoch 1147, training loss: 10839.09, average training loss: 10968.37, base loss: 14437.94
[INFO 2017-06-28 14:34:26,405 main.py:51] epoch 1148, training loss: 10379.25, average training loss: 10964.86, base loss: 14436.32
[INFO 2017-06-28 14:34:27,126 main.py:51] epoch 1149, training loss: 9865.29, average training loss: 10960.58, base loss: 14434.32
[INFO 2017-06-28 14:34:27,947 main.py:51] epoch 1150, training loss: 10536.05, average training loss: 10957.57, base loss: 14433.38
[INFO 2017-06-28 14:34:28,780 main.py:51] epoch 1151, training loss: 10024.39, average training loss: 10953.61, base loss: 14429.07
[INFO 2017-06-28 14:34:29,432 main.py:51] epoch 1152, training loss: 12266.54, average training loss: 10953.36, base loss: 14430.63
[INFO 2017-06-28 14:34:30,160 main.py:51] epoch 1153, training loss: 11086.90, average training loss: 10952.17, base loss: 14431.44
[INFO 2017-06-28 14:34:31,007 main.py:51] epoch 1154, training loss: 10798.75, average training loss: 10949.61, base loss: 14428.46
[INFO 2017-06-28 14:34:31,691 main.py:51] epoch 1155, training loss: 9564.12, average training loss: 10946.62, base loss: 14427.58
[INFO 2017-06-28 14:34:32,417 main.py:51] epoch 1156, training loss: 9736.45, average training loss: 10943.16, base loss: 14425.68
[INFO 2017-06-28 14:34:33,236 main.py:51] epoch 1157, training loss: 11130.66, average training loss: 10940.10, base loss: 14424.92
[INFO 2017-06-28 14:34:34,050 main.py:51] epoch 1158, training loss: 9156.07, average training loss: 10938.57, base loss: 14425.99
[INFO 2017-06-28 14:34:34,724 main.py:51] epoch 1159, training loss: 10605.30, average training loss: 10938.30, base loss: 14427.76
[INFO 2017-06-28 14:34:35,540 main.py:51] epoch 1160, training loss: 11264.73, average training loss: 10935.87, base loss: 14427.41
[INFO 2017-06-28 14:34:36,357 main.py:51] epoch 1161, training loss: 10337.46, average training loss: 10934.19, base loss: 14428.69
[INFO 2017-06-28 14:34:37,167 main.py:51] epoch 1162, training loss: 10326.02, average training loss: 10933.62, base loss: 14430.38
[INFO 2017-06-28 14:34:37,808 main.py:51] epoch 1163, training loss: 9607.76, average training loss: 10931.26, base loss: 14428.72
[INFO 2017-06-28 14:34:38,581 main.py:51] epoch 1164, training loss: 9673.89, average training loss: 10929.40, base loss: 14428.93
[INFO 2017-06-28 14:34:39,385 main.py:51] epoch 1165, training loss: 11015.81, average training loss: 10927.46, base loss: 14429.39
[INFO 2017-06-28 14:34:40,020 main.py:51] epoch 1166, training loss: 10287.76, average training loss: 10925.51, base loss: 14428.66
[INFO 2017-06-28 14:34:40,791 main.py:51] epoch 1167, training loss: 10267.08, average training loss: 10922.20, base loss: 14425.53
[INFO 2017-06-28 14:34:41,629 main.py:51] epoch 1168, training loss: 10991.17, average training loss: 10922.83, base loss: 14428.87
[INFO 2017-06-28 14:34:42,368 main.py:51] epoch 1169, training loss: 9950.53, average training loss: 10920.59, base loss: 14428.21
[INFO 2017-06-28 14:34:43,070 main.py:51] epoch 1170, training loss: 10794.73, average training loss: 10918.90, base loss: 14428.49
[INFO 2017-06-28 14:34:43,853 main.py:51] epoch 1171, training loss: 9796.63, average training loss: 10915.32, base loss: 14426.96
[INFO 2017-06-28 14:34:44,707 main.py:51] epoch 1172, training loss: 11215.63, average training loss: 10913.40, base loss: 14426.13
[INFO 2017-06-28 14:34:45,374 main.py:51] epoch 1173, training loss: 11747.51, average training loss: 10912.33, base loss: 14427.64
[INFO 2017-06-28 14:34:46,122 main.py:51] epoch 1174, training loss: 9986.04, average training loss: 10909.39, base loss: 14425.92
[INFO 2017-06-28 14:34:46,945 main.py:51] epoch 1175, training loss: 11649.90, average training loss: 10909.04, base loss: 14426.50
[INFO 2017-06-28 14:34:47,677 main.py:51] epoch 1176, training loss: 11882.90, average training loss: 10909.91, base loss: 14428.71
[INFO 2017-06-28 14:34:48,364 main.py:51] epoch 1177, training loss: 9800.34, average training loss: 10908.62, base loss: 14428.17
[INFO 2017-06-28 14:34:49,172 main.py:51] epoch 1178, training loss: 10952.96, average training loss: 10907.62, base loss: 14429.03
[INFO 2017-06-28 14:34:49,897 main.py:51] epoch 1179, training loss: 11346.68, average training loss: 10907.87, base loss: 14432.13
[INFO 2017-06-28 14:34:50,597 main.py:51] epoch 1180, training loss: 9848.83, average training loss: 10906.89, base loss: 14432.54
[INFO 2017-06-28 14:34:51,427 main.py:51] epoch 1181, training loss: 9906.27, average training loss: 10905.07, base loss: 14430.65
[INFO 2017-06-28 14:34:52,243 main.py:51] epoch 1182, training loss: 9540.39, average training loss: 10902.59, base loss: 14429.19
[INFO 2017-06-28 14:34:52,911 main.py:51] epoch 1183, training loss: 10973.96, average training loss: 10901.79, base loss: 14429.79
[INFO 2017-06-28 14:34:53,696 main.py:51] epoch 1184, training loss: 11641.30, average training loss: 10903.04, base loss: 14433.00
[INFO 2017-06-28 14:34:54,535 main.py:51] epoch 1185, training loss: 11670.01, average training loss: 10900.23, base loss: 14430.58
[INFO 2017-06-28 14:34:55,227 main.py:51] epoch 1186, training loss: 9898.05, average training loss: 10896.61, base loss: 14428.87
[INFO 2017-06-28 14:34:55,818 main.py:51] epoch 1187, training loss: 10666.10, average training loss: 10894.07, base loss: 14426.92
[INFO 2017-06-28 14:34:56,448 main.py:51] epoch 1188, training loss: 9990.91, average training loss: 10892.04, base loss: 14425.53
[INFO 2017-06-28 14:34:57,125 main.py:51] epoch 1189, training loss: 9448.67, average training loss: 10889.59, base loss: 14423.09
[INFO 2017-06-28 14:34:57,726 main.py:51] epoch 1190, training loss: 10106.53, average training loss: 10887.22, base loss: 14421.56
[INFO 2017-06-28 14:34:58,359 main.py:51] epoch 1191, training loss: 9625.97, average training loss: 10885.39, base loss: 14422.20
[INFO 2017-06-28 14:34:59,041 main.py:51] epoch 1192, training loss: 10330.73, average training loss: 10883.95, base loss: 14420.92
[INFO 2017-06-28 14:34:59,757 main.py:51] epoch 1193, training loss: 9668.42, average training loss: 10882.54, base loss: 14420.71
[INFO 2017-06-28 14:35:00,594 main.py:51] epoch 1194, training loss: 11411.64, average training loss: 10881.94, base loss: 14420.62
[INFO 2017-06-28 14:35:01,410 main.py:51] epoch 1195, training loss: 11035.32, average training loss: 10881.47, base loss: 14421.44
[INFO 2017-06-28 14:35:02,054 main.py:51] epoch 1196, training loss: 10640.18, average training loss: 10878.15, base loss: 14418.40
[INFO 2017-06-28 14:35:02,818 main.py:51] epoch 1197, training loss: 9898.97, average training loss: 10874.84, base loss: 14415.16
[INFO 2017-06-28 14:35:03,678 main.py:51] epoch 1198, training loss: 10149.04, average training loss: 10872.74, base loss: 14413.18
[INFO 2017-06-28 14:35:04,403 main.py:51] epoch 1199, training loss: 9352.82, average training loss: 10868.71, base loss: 14410.04
[INFO 2017-06-28 14:35:04,403 main.py:53] epoch 1199, testing
[INFO 2017-06-28 14:35:07,235 main.py:105] average testing loss: 12392.89, base loss: 16220.34
[INFO 2017-06-28 14:35:07,235 main.py:106] improve_loss: 3827.45, improve_percent: 0.24
[INFO 2017-06-28 14:35:07,236 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:35:07,960 main.py:51] epoch 1200, training loss: 10441.74, average training loss: 10867.12, base loss: 14410.96
[INFO 2017-06-28 14:35:08,773 main.py:51] epoch 1201, training loss: 10067.67, average training loss: 10865.49, base loss: 14411.56
[INFO 2017-06-28 14:35:09,500 main.py:51] epoch 1202, training loss: 9075.27, average training loss: 10862.54, base loss: 14409.15
[INFO 2017-06-28 14:35:10,186 main.py:51] epoch 1203, training loss: 9764.73, average training loss: 10860.90, base loss: 14409.20
[INFO 2017-06-28 14:35:11,000 main.py:51] epoch 1204, training loss: 10594.63, average training loss: 10859.92, base loss: 14409.94
[INFO 2017-06-28 14:35:11,809 main.py:51] epoch 1205, training loss: 12283.88, average training loss: 10860.05, base loss: 14410.24
[INFO 2017-06-28 14:35:12,461 main.py:51] epoch 1206, training loss: 10717.96, average training loss: 10859.67, base loss: 14410.61
[INFO 2017-06-28 14:35:13,233 main.py:51] epoch 1207, training loss: 9678.75, average training loss: 10856.21, base loss: 14408.74
[INFO 2017-06-28 14:35:14,038 main.py:51] epoch 1208, training loss: 11257.33, average training loss: 10856.56, base loss: 14410.77
[INFO 2017-06-28 14:35:14,666 main.py:51] epoch 1209, training loss: 11555.00, average training loss: 10857.33, base loss: 14412.08
[INFO 2017-06-28 14:35:15,426 main.py:51] epoch 1210, training loss: 9957.25, average training loss: 10854.43, base loss: 14410.16
[INFO 2017-06-28 14:35:16,280 main.py:51] epoch 1211, training loss: 9791.20, average training loss: 10854.03, base loss: 14410.84
[INFO 2017-06-28 14:35:17,054 main.py:51] epoch 1212, training loss: 11117.59, average training loss: 10852.95, base loss: 14410.98
[INFO 2017-06-28 14:35:17,751 main.py:51] epoch 1213, training loss: 10312.37, average training loss: 10853.83, base loss: 14414.17
[INFO 2017-06-28 14:35:18,503 main.py:51] epoch 1214, training loss: 9663.25, average training loss: 10852.20, base loss: 14414.30
[INFO 2017-06-28 14:35:19,275 main.py:51] epoch 1215, training loss: 11316.56, average training loss: 10851.37, base loss: 14414.68
[INFO 2017-06-28 14:35:19,902 main.py:51] epoch 1216, training loss: 10925.62, average training loss: 10850.39, base loss: 14415.03
[INFO 2017-06-28 14:35:20,675 main.py:51] epoch 1217, training loss: 9159.79, average training loss: 10849.80, base loss: 14417.12
[INFO 2017-06-28 14:35:21,508 main.py:51] epoch 1218, training loss: 9139.99, average training loss: 10848.49, base loss: 14416.44
[INFO 2017-06-28 14:35:22,130 main.py:51] epoch 1219, training loss: 8757.93, average training loss: 10845.08, base loss: 14413.79
[INFO 2017-06-28 14:35:22,901 main.py:51] epoch 1220, training loss: 10409.36, average training loss: 10845.74, base loss: 14417.03
[INFO 2017-06-28 14:35:23,694 main.py:51] epoch 1221, training loss: 9671.90, average training loss: 10844.92, base loss: 14417.53
[INFO 2017-06-28 14:35:24,339 main.py:51] epoch 1222, training loss: 12627.40, average training loss: 10846.92, base loss: 14421.79
[INFO 2017-06-28 14:35:25,118 main.py:51] epoch 1223, training loss: 9793.46, average training loss: 10844.97, base loss: 14420.54
[INFO 2017-06-28 14:35:25,925 main.py:51] epoch 1224, training loss: 13172.43, average training loss: 10847.32, base loss: 14425.72
[INFO 2017-06-28 14:35:26,620 main.py:51] epoch 1225, training loss: 9307.71, average training loss: 10846.11, base loss: 14425.27
[INFO 2017-06-28 14:35:27,368 main.py:51] epoch 1226, training loss: 10100.21, average training loss: 10845.99, base loss: 14427.49
[INFO 2017-06-28 14:35:28,192 main.py:51] epoch 1227, training loss: 9787.03, average training loss: 10844.38, base loss: 14426.73
[INFO 2017-06-28 14:35:29,039 main.py:51] epoch 1228, training loss: 11239.98, average training loss: 10843.34, base loss: 14427.03
[INFO 2017-06-28 14:35:29,660 main.py:51] epoch 1229, training loss: 9809.70, average training loss: 10840.65, base loss: 14424.11
[INFO 2017-06-28 14:35:30,423 main.py:51] epoch 1230, training loss: 10289.38, average training loss: 10840.23, base loss: 14424.62
[INFO 2017-06-28 14:35:31,206 main.py:51] epoch 1231, training loss: 10868.94, average training loss: 10838.90, base loss: 14424.92
[INFO 2017-06-28 14:35:31,871 main.py:51] epoch 1232, training loss: 10399.69, average training loss: 10840.15, base loss: 14427.28
[INFO 2017-06-28 14:35:32,636 main.py:51] epoch 1233, training loss: 12512.55, average training loss: 10841.18, base loss: 14430.51
[INFO 2017-06-28 14:35:33,436 main.py:51] epoch 1234, training loss: 9666.41, average training loss: 10837.98, base loss: 14427.48
[INFO 2017-06-28 14:35:34,114 main.py:51] epoch 1235, training loss: 11424.55, average training loss: 10836.90, base loss: 14427.46
[INFO 2017-06-28 14:35:34,869 main.py:51] epoch 1236, training loss: 13168.65, average training loss: 10837.77, base loss: 14430.93
[INFO 2017-06-28 14:35:35,681 main.py:51] epoch 1237, training loss: 10424.42, average training loss: 10836.31, base loss: 14432.06
[INFO 2017-06-28 14:35:36,467 main.py:51] epoch 1238, training loss: 11133.38, average training loss: 10836.85, base loss: 14435.21
[INFO 2017-06-28 14:35:37,114 main.py:51] epoch 1239, training loss: 9773.25, average training loss: 10835.53, base loss: 14434.63
[INFO 2017-06-28 14:35:37,887 main.py:51] epoch 1240, training loss: 12214.56, average training loss: 10835.33, base loss: 14435.54
[INFO 2017-06-28 14:35:38,713 main.py:51] epoch 1241, training loss: 9105.10, average training loss: 10832.52, base loss: 14433.42
[INFO 2017-06-28 14:35:39,472 main.py:51] epoch 1242, training loss: 10412.96, average training loss: 10831.97, base loss: 14432.67
[INFO 2017-06-28 14:35:40,156 main.py:51] epoch 1243, training loss: 12429.12, average training loss: 10833.30, base loss: 14436.47
[INFO 2017-06-28 14:35:40,934 main.py:51] epoch 1244, training loss: 9712.08, average training loss: 10831.15, base loss: 14435.17
[INFO 2017-06-28 14:35:41,763 main.py:51] epoch 1245, training loss: 9855.31, average training loss: 10829.81, base loss: 14434.26
[INFO 2017-06-28 14:35:42,429 main.py:51] epoch 1246, training loss: 10351.97, average training loss: 10830.12, base loss: 14434.83
[INFO 2017-06-28 14:35:43,211 main.py:51] epoch 1247, training loss: 8540.40, average training loss: 10828.42, base loss: 14433.73
[INFO 2017-06-28 14:35:44,036 main.py:51] epoch 1248, training loss: 10020.15, average training loss: 10825.81, base loss: 14430.74
[INFO 2017-06-28 14:35:44,870 main.py:51] epoch 1249, training loss: 9612.18, average training loss: 10822.23, base loss: 14428.42
[INFO 2017-06-28 14:35:45,557 main.py:51] epoch 1250, training loss: 10235.10, average training loss: 10821.27, base loss: 14429.43
[INFO 2017-06-28 14:35:46,352 main.py:51] epoch 1251, training loss: 10680.88, average training loss: 10821.15, base loss: 14430.63
[INFO 2017-06-28 14:35:47,170 main.py:51] epoch 1252, training loss: 10770.54, average training loss: 10820.57, base loss: 14430.95
[INFO 2017-06-28 14:35:47,950 main.py:51] epoch 1253, training loss: 12424.12, average training loss: 10820.85, base loss: 14432.49
[INFO 2017-06-28 14:35:48,655 main.py:51] epoch 1254, training loss: 9794.26, average training loss: 10815.51, base loss: 14427.85
[INFO 2017-06-28 14:35:49,401 main.py:51] epoch 1255, training loss: 10881.00, average training loss: 10814.95, base loss: 14428.05
[INFO 2017-06-28 14:35:50,171 main.py:51] epoch 1256, training loss: 10522.04, average training loss: 10815.86, base loss: 14430.13
[INFO 2017-06-28 14:35:50,818 main.py:51] epoch 1257, training loss: 10359.70, average training loss: 10814.50, base loss: 14429.43
[INFO 2017-06-28 14:35:51,606 main.py:51] epoch 1258, training loss: 10422.65, average training loss: 10813.65, base loss: 14429.03
[INFO 2017-06-28 14:35:52,425 main.py:51] epoch 1259, training loss: 13437.04, average training loss: 10813.68, base loss: 14431.34
[INFO 2017-06-28 14:35:53,194 main.py:51] epoch 1260, training loss: 10420.22, average training loss: 10813.90, base loss: 14433.54
[INFO 2017-06-28 14:35:53,860 main.py:51] epoch 1261, training loss: 14826.65, average training loss: 10817.70, base loss: 14441.07
[INFO 2017-06-28 14:35:54,663 main.py:51] epoch 1262, training loss: 9673.02, average training loss: 10816.75, base loss: 14440.35
[INFO 2017-06-28 14:35:55,533 main.py:51] epoch 1263, training loss: 10492.21, average training loss: 10817.05, base loss: 14442.94
[INFO 2017-06-28 14:35:56,313 main.py:51] epoch 1264, training loss: 11409.90, average training loss: 10817.23, base loss: 14445.43
[INFO 2017-06-28 14:35:56,999 main.py:51] epoch 1265, training loss: 10183.09, average training loss: 10816.85, base loss: 14446.79
[INFO 2017-06-28 14:35:57,779 main.py:51] epoch 1266, training loss: 10263.00, average training loss: 10813.59, base loss: 14444.61
[INFO 2017-06-28 14:35:58,572 main.py:51] epoch 1267, training loss: 11630.32, average training loss: 10814.17, base loss: 14446.19
[INFO 2017-06-28 14:35:59,186 main.py:51] epoch 1268, training loss: 10405.76, average training loss: 10810.78, base loss: 14443.66
[INFO 2017-06-28 14:35:59,962 main.py:51] epoch 1269, training loss: 9470.67, average training loss: 10808.59, base loss: 14442.01
[INFO 2017-06-28 14:36:00,804 main.py:51] epoch 1270, training loss: 8956.31, average training loss: 10805.37, base loss: 14438.40
[INFO 2017-06-28 14:36:01,534 main.py:51] epoch 1271, training loss: 10687.43, average training loss: 10804.70, base loss: 14439.62
[INFO 2017-06-28 14:36:02,230 main.py:51] epoch 1272, training loss: 9355.07, average training loss: 10802.60, base loss: 14437.89
[INFO 2017-06-28 14:36:03,039 main.py:51] epoch 1273, training loss: 10879.91, average training loss: 10803.35, base loss: 14438.87
[INFO 2017-06-28 14:36:03,781 main.py:51] epoch 1274, training loss: 9960.65, average training loss: 10801.65, base loss: 14439.06
[INFO 2017-06-28 14:36:04,471 main.py:51] epoch 1275, training loss: 10107.23, average training loss: 10800.17, base loss: 14438.73
[INFO 2017-06-28 14:36:05,263 main.py:51] epoch 1276, training loss: 9283.00, average training loss: 10800.03, base loss: 14439.01
[INFO 2017-06-28 14:36:06,053 main.py:51] epoch 1277, training loss: 12023.40, average training loss: 10799.96, base loss: 14440.99
[INFO 2017-06-28 14:36:06,693 main.py:51] epoch 1278, training loss: 9986.70, average training loss: 10797.52, base loss: 14438.70
[INFO 2017-06-28 14:36:07,461 main.py:51] epoch 1279, training loss: 9796.11, average training loss: 10795.61, base loss: 14436.53
[INFO 2017-06-28 14:36:08,286 main.py:51] epoch 1280, training loss: 12983.10, average training loss: 10798.54, base loss: 14441.68
[INFO 2017-06-28 14:36:08,973 main.py:51] epoch 1281, training loss: 10056.09, average training loss: 10797.69, base loss: 14442.58
[INFO 2017-06-28 14:36:09,725 main.py:51] epoch 1282, training loss: 10425.18, average training loss: 10797.53, base loss: 14443.59
[INFO 2017-06-28 14:36:10,536 main.py:51] epoch 1283, training loss: 10459.12, average training loss: 10795.02, base loss: 14442.43
[INFO 2017-06-28 14:36:11,377 main.py:51] epoch 1284, training loss: 9900.91, average training loss: 10794.00, base loss: 14441.86
[INFO 2017-06-28 14:36:11,959 main.py:51] epoch 1285, training loss: 9535.51, average training loss: 10793.69, base loss: 14442.91
[INFO 2017-06-28 14:36:12,750 main.py:51] epoch 1286, training loss: 9966.72, average training loss: 10793.57, base loss: 14443.86
[INFO 2017-06-28 14:36:13,606 main.py:51] epoch 1287, training loss: 10470.33, average training loss: 10793.71, base loss: 14444.97
[INFO 2017-06-28 14:36:14,186 main.py:51] epoch 1288, training loss: 11525.60, average training loss: 10794.29, base loss: 14447.76
[INFO 2017-06-28 14:36:14,998 main.py:51] epoch 1289, training loss: 11663.27, average training loss: 10795.34, base loss: 14450.37
[INFO 2017-06-28 14:36:15,833 main.py:51] epoch 1290, training loss: 9121.03, average training loss: 10791.52, base loss: 14446.87
[INFO 2017-06-28 14:36:16,605 main.py:51] epoch 1291, training loss: 10690.43, average training loss: 10789.81, base loss: 14444.78
[INFO 2017-06-28 14:36:17,334 main.py:51] epoch 1292, training loss: 10813.25, average training loss: 10789.67, base loss: 14446.32
[INFO 2017-06-28 14:36:18,168 main.py:51] epoch 1293, training loss: 9613.09, average training loss: 10788.01, base loss: 14443.67
[INFO 2017-06-28 14:36:18,986 main.py:51] epoch 1294, training loss: 11533.26, average training loss: 10787.14, base loss: 14443.55
[INFO 2017-06-28 14:36:19,726 main.py:51] epoch 1295, training loss: 10475.78, average training loss: 10784.96, base loss: 14443.11
[INFO 2017-06-28 14:36:20,410 main.py:51] epoch 1296, training loss: 11795.60, average training loss: 10784.59, base loss: 14443.11
[INFO 2017-06-28 14:36:21,220 main.py:51] epoch 1297, training loss: 10556.27, average training loss: 10782.66, base loss: 14442.22
[INFO 2017-06-28 14:36:22,061 main.py:51] epoch 1298, training loss: 10135.04, average training loss: 10781.89, base loss: 14442.58
[INFO 2017-06-28 14:36:22,731 main.py:51] epoch 1299, training loss: 10914.19, average training loss: 10780.77, base loss: 14442.16
[INFO 2017-06-28 14:36:22,732 main.py:53] epoch 1299, testing
[INFO 2017-06-28 14:36:25,605 main.py:105] average testing loss: 11628.92, base loss: 15150.36
[INFO 2017-06-28 14:36:25,605 main.py:106] improve_loss: 3521.44, improve_percent: 0.23
[INFO 2017-06-28 14:36:25,606 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:36:26,309 main.py:51] epoch 1300, training loss: 10805.28, average training loss: 10782.12, base loss: 14445.93
[INFO 2017-06-28 14:36:27,126 main.py:51] epoch 1301, training loss: 12915.18, average training loss: 10781.85, base loss: 14446.28
[INFO 2017-06-28 14:36:27,907 main.py:51] epoch 1302, training loss: 10193.13, average training loss: 10779.37, base loss: 14445.97
[INFO 2017-06-28 14:36:28,578 main.py:51] epoch 1303, training loss: 8308.42, average training loss: 10776.22, base loss: 14444.33
[INFO 2017-06-28 14:36:29,359 main.py:51] epoch 1304, training loss: 10707.77, average training loss: 10776.37, base loss: 14446.06
[INFO 2017-06-28 14:36:30,179 main.py:51] epoch 1305, training loss: 10012.22, average training loss: 10775.77, base loss: 14447.30
[INFO 2017-06-28 14:36:30,846 main.py:51] epoch 1306, training loss: 11885.57, average training loss: 10777.20, base loss: 14448.47
[INFO 2017-06-28 14:36:31,610 main.py:51] epoch 1307, training loss: 10588.80, average training loss: 10777.68, base loss: 14449.76
[INFO 2017-06-28 14:36:32,430 main.py:51] epoch 1308, training loss: 10332.38, average training loss: 10775.42, base loss: 14448.46
[INFO 2017-06-28 14:36:33,221 main.py:51] epoch 1309, training loss: 12986.87, average training loss: 10775.82, base loss: 14451.41
[INFO 2017-06-28 14:36:33,920 main.py:51] epoch 1310, training loss: 9582.35, average training loss: 10773.43, base loss: 14450.07
[INFO 2017-06-28 14:36:34,676 main.py:51] epoch 1311, training loss: 13310.78, average training loss: 10776.41, base loss: 14454.86
[INFO 2017-06-28 14:36:35,523 main.py:51] epoch 1312, training loss: 11135.62, average training loss: 10777.79, base loss: 14456.63
[INFO 2017-06-28 14:36:36,216 main.py:51] epoch 1313, training loss: 12334.98, average training loss: 10776.90, base loss: 14456.98
[INFO 2017-06-28 14:36:36,963 main.py:51] epoch 1314, training loss: 10601.69, average training loss: 10775.41, base loss: 14456.12
[INFO 2017-06-28 14:36:37,764 main.py:51] epoch 1315, training loss: 9190.93, average training loss: 10773.76, base loss: 14453.60
[INFO 2017-06-28 14:36:38,605 main.py:51] epoch 1316, training loss: 11259.36, average training loss: 10772.25, base loss: 14452.70
[INFO 2017-06-28 14:36:39,262 main.py:51] epoch 1317, training loss: 9808.92, average training loss: 10772.52, base loss: 14453.59
[INFO 2017-06-28 14:36:40,036 main.py:51] epoch 1318, training loss: 10022.66, average training loss: 10771.38, base loss: 14454.23
[INFO 2017-06-28 14:36:40,856 main.py:51] epoch 1319, training loss: 9482.78, average training loss: 10769.41, base loss: 14452.32
[INFO 2017-06-28 14:36:41,695 main.py:51] epoch 1320, training loss: 11736.04, average training loss: 10768.69, base loss: 14451.77
[INFO 2017-06-28 14:36:42,384 main.py:51] epoch 1321, training loss: 10782.00, average training loss: 10768.47, base loss: 14452.18
[INFO 2017-06-28 14:36:43,172 main.py:51] epoch 1322, training loss: 9730.51, average training loss: 10766.51, base loss: 14449.39
[INFO 2017-06-28 14:36:43,969 main.py:51] epoch 1323, training loss: 9495.41, average training loss: 10764.64, base loss: 14448.22
[INFO 2017-06-28 14:36:44,621 main.py:51] epoch 1324, training loss: 8611.25, average training loss: 10762.29, base loss: 14445.95
[INFO 2017-06-28 14:36:45,321 main.py:51] epoch 1325, training loss: 10813.25, average training loss: 10761.15, base loss: 14444.86
[INFO 2017-06-28 14:36:45,960 main.py:51] epoch 1326, training loss: 10368.04, average training loss: 10761.65, base loss: 14447.80
[INFO 2017-06-28 14:36:46,562 main.py:51] epoch 1327, training loss: 11041.97, average training loss: 10761.11, base loss: 14447.93
[INFO 2017-06-28 14:36:47,230 main.py:51] epoch 1328, training loss: 9436.02, average training loss: 10759.77, base loss: 14446.75
[INFO 2017-06-28 14:36:47,850 main.py:51] epoch 1329, training loss: 9227.15, average training loss: 10757.13, base loss: 14444.61
[INFO 2017-06-28 14:36:48,660 main.py:51] epoch 1330, training loss: 10123.98, average training loss: 10756.60, base loss: 14444.55
[INFO 2017-06-28 14:36:49,375 main.py:51] epoch 1331, training loss: 11148.11, average training loss: 10757.12, base loss: 14447.73
[INFO 2017-06-28 14:36:50,080 main.py:51] epoch 1332, training loss: 12404.93, average training loss: 10757.83, base loss: 14448.81
[INFO 2017-06-28 14:36:50,903 main.py:51] epoch 1333, training loss: 8791.36, average training loss: 10756.71, base loss: 14447.69
[INFO 2017-06-28 14:36:51,748 main.py:51] epoch 1334, training loss: 10611.41, average training loss: 10752.72, base loss: 14443.87
[INFO 2017-06-28 14:36:52,385 main.py:51] epoch 1335, training loss: 11221.83, average training loss: 10753.06, base loss: 14444.98
[INFO 2017-06-28 14:36:53,156 main.py:51] epoch 1336, training loss: 11124.71, average training loss: 10754.35, base loss: 14447.91
[INFO 2017-06-28 14:36:53,992 main.py:51] epoch 1337, training loss: 9240.55, average training loss: 10749.78, base loss: 14442.50
[INFO 2017-06-28 14:36:54,683 main.py:51] epoch 1338, training loss: 9540.24, average training loss: 10748.77, base loss: 14442.79
[INFO 2017-06-28 14:36:55,430 main.py:51] epoch 1339, training loss: 9220.35, average training loss: 10747.28, base loss: 14441.52
[INFO 2017-06-28 14:36:56,238 main.py:51] epoch 1340, training loss: 9527.59, average training loss: 10744.43, base loss: 14438.51
[INFO 2017-06-28 14:36:57,054 main.py:51] epoch 1341, training loss: 9038.48, average training loss: 10742.61, base loss: 14437.01
[INFO 2017-06-28 14:36:57,664 main.py:51] epoch 1342, training loss: 11862.98, average training loss: 10743.71, base loss: 14440.42
[INFO 2017-06-28 14:36:58,430 main.py:51] epoch 1343, training loss: 11831.10, average training loss: 10744.13, base loss: 14442.55
[INFO 2017-06-28 14:36:59,285 main.py:51] epoch 1344, training loss: 11028.02, average training loss: 10744.83, base loss: 14444.64
[INFO 2017-06-28 14:36:59,960 main.py:51] epoch 1345, training loss: 11348.19, average training loss: 10743.40, base loss: 14444.78
[INFO 2017-06-28 14:37:00,706 main.py:51] epoch 1346, training loss: 9676.75, average training loss: 10740.80, base loss: 14442.32
[INFO 2017-06-28 14:37:01,520 main.py:51] epoch 1347, training loss: 10732.93, average training loss: 10739.29, base loss: 14441.43
[INFO 2017-06-28 14:37:02,341 main.py:51] epoch 1348, training loss: 11490.01, average training loss: 10738.62, base loss: 14440.90
[INFO 2017-06-28 14:37:03,014 main.py:51] epoch 1349, training loss: 10621.43, average training loss: 10737.91, base loss: 14441.53
[INFO 2017-06-28 14:37:03,754 main.py:51] epoch 1350, training loss: 10888.72, average training loss: 10737.60, base loss: 14441.88
[INFO 2017-06-28 14:37:04,577 main.py:51] epoch 1351, training loss: 9503.96, average training loss: 10734.62, base loss: 14437.75
[INFO 2017-06-28 14:37:05,286 main.py:51] epoch 1352, training loss: 10735.77, average training loss: 10734.38, base loss: 14438.64
[INFO 2017-06-28 14:37:05,983 main.py:51] epoch 1353, training loss: 10759.54, average training loss: 10733.46, base loss: 14439.74
[INFO 2017-06-28 14:37:06,793 main.py:51] epoch 1354, training loss: 9584.95, average training loss: 10731.55, base loss: 14436.10
[INFO 2017-06-28 14:37:07,523 main.py:51] epoch 1355, training loss: 11909.35, average training loss: 10731.82, base loss: 14438.08
[INFO 2017-06-28 14:37:08,231 main.py:51] epoch 1356, training loss: 10667.76, average training loss: 10732.76, base loss: 14440.72
[INFO 2017-06-28 14:37:09,023 main.py:51] epoch 1357, training loss: 9877.24, average training loss: 10731.93, base loss: 14438.88
[INFO 2017-06-28 14:37:09,814 main.py:51] epoch 1358, training loss: 10134.67, average training loss: 10731.72, base loss: 14440.10
[INFO 2017-06-28 14:37:10,448 main.py:51] epoch 1359, training loss: 10480.67, average training loss: 10730.25, base loss: 14439.03
[INFO 2017-06-28 14:37:11,227 main.py:51] epoch 1360, training loss: 9890.81, average training loss: 10730.33, base loss: 14439.08
[INFO 2017-06-28 14:37:12,022 main.py:51] epoch 1361, training loss: 10678.24, average training loss: 10727.14, base loss: 14435.64
[INFO 2017-06-28 14:37:12,652 main.py:51] epoch 1362, training loss: 9315.80, average training loss: 10726.21, base loss: 14434.52
[INFO 2017-06-28 14:37:13,453 main.py:51] epoch 1363, training loss: 9245.57, average training loss: 10723.96, base loss: 14431.75
[INFO 2017-06-28 14:37:14,258 main.py:51] epoch 1364, training loss: 9393.93, average training loss: 10722.36, base loss: 14430.77
[INFO 2017-06-28 14:37:14,954 main.py:51] epoch 1365, training loss: 11308.38, average training loss: 10722.70, base loss: 14432.70
[INFO 2017-06-28 14:37:15,689 main.py:51] epoch 1366, training loss: 8640.30, average training loss: 10720.67, base loss: 14430.79
[INFO 2017-06-28 14:37:16,515 main.py:51] epoch 1367, training loss: 8614.10, average training loss: 10718.57, base loss: 14428.06
[INFO 2017-06-28 14:37:17,360 main.py:51] epoch 1368, training loss: 10275.43, average training loss: 10717.84, base loss: 14427.68
[INFO 2017-06-28 14:37:18,021 main.py:51] epoch 1369, training loss: 11193.07, average training loss: 10716.02, base loss: 14425.75
[INFO 2017-06-28 14:37:18,795 main.py:51] epoch 1370, training loss: 11861.29, average training loss: 10716.86, base loss: 14428.73
[INFO 2017-06-28 14:37:19,623 main.py:51] epoch 1371, training loss: 8797.53, average training loss: 10715.07, base loss: 14425.64
[INFO 2017-06-28 14:37:20,315 main.py:51] epoch 1372, training loss: 10569.69, average training loss: 10713.63, base loss: 14424.11
[INFO 2017-06-28 14:37:21,037 main.py:51] epoch 1373, training loss: 11655.36, average training loss: 10712.09, base loss: 14423.60
[INFO 2017-06-28 14:37:21,871 main.py:51] epoch 1374, training loss: 9499.89, average training loss: 10708.94, base loss: 14421.21
[INFO 2017-06-28 14:37:22,678 main.py:51] epoch 1375, training loss: 11112.26, average training loss: 10708.48, base loss: 14423.74
[INFO 2017-06-28 14:37:23,384 main.py:51] epoch 1376, training loss: 12106.25, average training loss: 10708.20, base loss: 14425.03
[INFO 2017-06-28 14:37:24,136 main.py:51] epoch 1377, training loss: 11857.32, average training loss: 10708.57, base loss: 14426.64
[INFO 2017-06-28 14:37:24,933 main.py:51] epoch 1378, training loss: 11506.63, average training loss: 10708.93, base loss: 14427.32
[INFO 2017-06-28 14:37:25,687 main.py:51] epoch 1379, training loss: 10298.96, average training loss: 10707.19, base loss: 14425.02
[INFO 2017-06-28 14:37:26,358 main.py:51] epoch 1380, training loss: 9910.87, average training loss: 10706.85, base loss: 14425.89
[INFO 2017-06-28 14:37:27,158 main.py:51] epoch 1381, training loss: 11170.66, average training loss: 10708.02, base loss: 14429.97
[INFO 2017-06-28 14:37:27,973 main.py:51] epoch 1382, training loss: 10854.90, average training loss: 10707.65, base loss: 14429.17
[INFO 2017-06-28 14:37:28,628 main.py:51] epoch 1383, training loss: 9780.68, average training loss: 10707.44, base loss: 14429.65
[INFO 2017-06-28 14:37:29,336 main.py:51] epoch 1384, training loss: 12581.09, average training loss: 10707.57, base loss: 14431.25
[INFO 2017-06-28 14:37:30,177 main.py:51] epoch 1385, training loss: 9311.25, average training loss: 10705.92, base loss: 14429.31
[INFO 2017-06-28 14:37:30,909 main.py:51] epoch 1386, training loss: 12728.50, average training loss: 10708.44, base loss: 14431.45
[INFO 2017-06-28 14:37:31,623 main.py:51] epoch 1387, training loss: 10629.99, average training loss: 10707.60, base loss: 14430.98
[INFO 2017-06-28 14:37:32,418 main.py:51] epoch 1388, training loss: 12419.50, average training loss: 10709.09, base loss: 14433.79
[INFO 2017-06-28 14:37:33,280 main.py:51] epoch 1389, training loss: 10447.33, average training loss: 10708.46, base loss: 14433.43
[INFO 2017-06-28 14:37:33,947 main.py:51] epoch 1390, training loss: 9914.05, average training loss: 10708.23, base loss: 14435.42
[INFO 2017-06-28 14:37:34,708 main.py:51] epoch 1391, training loss: 9626.02, average training loss: 10705.07, base loss: 14432.11
[INFO 2017-06-28 14:37:35,539 main.py:51] epoch 1392, training loss: 9950.58, average training loss: 10703.65, base loss: 14431.65
[INFO 2017-06-28 14:37:36,287 main.py:51] epoch 1393, training loss: 10750.45, average training loss: 10701.87, base loss: 14430.12
[INFO 2017-06-28 14:37:36,998 main.py:51] epoch 1394, training loss: 9828.13, average training loss: 10700.65, base loss: 14428.58
[INFO 2017-06-28 14:37:37,773 main.py:51] epoch 1395, training loss: 8315.45, average training loss: 10696.38, base loss: 14423.69
[INFO 2017-06-28 14:37:38,607 main.py:51] epoch 1396, training loss: 9686.48, average training loss: 10695.48, base loss: 14423.80
[INFO 2017-06-28 14:37:39,298 main.py:51] epoch 1397, training loss: 9852.41, average training loss: 10694.82, base loss: 14425.06
[INFO 2017-06-28 14:37:40,017 main.py:51] epoch 1398, training loss: 9660.55, average training loss: 10693.67, base loss: 14423.95
[INFO 2017-06-28 14:37:40,840 main.py:51] epoch 1399, training loss: 10910.55, average training loss: 10693.85, base loss: 14425.11
[INFO 2017-06-28 14:37:40,841 main.py:53] epoch 1399, testing
[INFO 2017-06-28 14:37:43,696 main.py:105] average testing loss: 12049.38, base loss: 15488.84
[INFO 2017-06-28 14:37:43,696 main.py:106] improve_loss: 3439.46, improve_percent: 0.22
[INFO 2017-06-28 14:37:43,697 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:37:44,416 main.py:51] epoch 1400, training loss: 11272.45, average training loss: 10693.58, base loss: 14426.48
[INFO 2017-06-28 14:37:45,135 main.py:51] epoch 1401, training loss: 9940.54, average training loss: 10691.84, base loss: 14423.19
[INFO 2017-06-28 14:37:45,897 main.py:51] epoch 1402, training loss: 9430.21, average training loss: 10689.36, base loss: 14422.02
[INFO 2017-06-28 14:37:46,731 main.py:51] epoch 1403, training loss: 9029.61, average training loss: 10687.96, base loss: 14421.91
[INFO 2017-06-28 14:37:47,377 main.py:51] epoch 1404, training loss: 10740.87, average training loss: 10686.42, base loss: 14421.83
[INFO 2017-06-28 14:37:48,149 main.py:51] epoch 1405, training loss: 9656.87, average training loss: 10685.39, base loss: 14421.17
[INFO 2017-06-28 14:37:48,980 main.py:51] epoch 1406, training loss: 10162.28, average training loss: 10681.93, base loss: 14417.30
[INFO 2017-06-28 14:37:49,705 main.py:51] epoch 1407, training loss: 9169.76, average training loss: 10680.37, base loss: 14415.02
[INFO 2017-06-28 14:37:50,376 main.py:51] epoch 1408, training loss: 10240.60, average training loss: 10679.37, base loss: 14414.84
[INFO 2017-06-28 14:37:51,176 main.py:51] epoch 1409, training loss: 9544.39, average training loss: 10678.13, base loss: 14414.10
[INFO 2017-06-28 14:37:51,981 main.py:51] epoch 1410, training loss: 10919.57, average training loss: 10676.54, base loss: 14414.83
[INFO 2017-06-28 14:37:52,649 main.py:51] epoch 1411, training loss: 10771.86, average training loss: 10676.19, base loss: 14415.35
[INFO 2017-06-28 14:37:53,426 main.py:51] epoch 1412, training loss: 13251.63, average training loss: 10678.29, base loss: 14416.80
[INFO 2017-06-28 14:37:54,267 main.py:51] epoch 1413, training loss: 9766.06, average training loss: 10678.35, base loss: 14417.47
[INFO 2017-06-28 14:37:55,043 main.py:51] epoch 1414, training loss: 10815.04, average training loss: 10678.35, base loss: 14418.50
[INFO 2017-06-28 14:37:55,760 main.py:51] epoch 1415, training loss: 11750.48, average training loss: 10679.02, base loss: 14422.82
[INFO 2017-06-28 14:37:56,539 main.py:51] epoch 1416, training loss: 10104.41, average training loss: 10677.98, base loss: 14422.76
[INFO 2017-06-28 14:37:57,326 main.py:51] epoch 1417, training loss: 10095.31, average training loss: 10675.19, base loss: 14420.89
[INFO 2017-06-28 14:37:57,952 main.py:51] epoch 1418, training loss: 10151.67, average training loss: 10674.15, base loss: 14419.39
[INFO 2017-06-28 14:37:58,744 main.py:51] epoch 1419, training loss: 11563.78, average training loss: 10674.88, base loss: 14421.89
[INFO 2017-06-28 14:37:59,569 main.py:51] epoch 1420, training loss: 9328.90, average training loss: 10670.92, base loss: 14417.57
[INFO 2017-06-28 14:38:00,353 main.py:51] epoch 1421, training loss: 9254.56, average training loss: 10670.04, base loss: 14418.47
[INFO 2017-06-28 14:38:01,040 main.py:51] epoch 1422, training loss: 9721.13, average training loss: 10669.17, base loss: 14417.73
[INFO 2017-06-28 14:38:01,828 main.py:51] epoch 1423, training loss: 10389.90, average training loss: 10669.90, base loss: 14418.87
[INFO 2017-06-28 14:38:02,657 main.py:51] epoch 1424, training loss: 10909.85, average training loss: 10669.22, base loss: 14418.96
[INFO 2017-06-28 14:38:03,322 main.py:51] epoch 1425, training loss: 9201.31, average training loss: 10666.48, base loss: 14416.64
[INFO 2017-06-28 14:38:04,057 main.py:51] epoch 1426, training loss: 9075.11, average training loss: 10664.79, base loss: 14414.23
[INFO 2017-06-28 14:38:04,889 main.py:51] epoch 1427, training loss: 8775.46, average training loss: 10663.14, base loss: 14413.91
[INFO 2017-06-28 14:38:05,655 main.py:51] epoch 1428, training loss: 10104.21, average training loss: 10663.21, base loss: 14414.68
[INFO 2017-06-28 14:38:06,346 main.py:51] epoch 1429, training loss: 9681.79, average training loss: 10661.76, base loss: 14414.27
[INFO 2017-06-28 14:38:07,135 main.py:51] epoch 1430, training loss: 9714.89, average training loss: 10658.94, base loss: 14412.05
[INFO 2017-06-28 14:38:07,964 main.py:51] epoch 1431, training loss: 9718.24, average training loss: 10659.09, base loss: 14412.78
[INFO 2017-06-28 14:38:08,599 main.py:51] epoch 1432, training loss: 10427.10, average training loss: 10660.20, base loss: 14413.09
[INFO 2017-06-28 14:38:09,397 main.py:51] epoch 1433, training loss: 10906.45, average training loss: 10659.77, base loss: 14413.35
[INFO 2017-06-28 14:38:10,208 main.py:51] epoch 1434, training loss: 13521.00, average training loss: 10663.77, base loss: 14419.83
[INFO 2017-06-28 14:38:10,954 main.py:51] epoch 1435, training loss: 9275.83, average training loss: 10662.74, base loss: 14419.10
[INFO 2017-06-28 14:38:11,628 main.py:51] epoch 1436, training loss: 10816.74, average training loss: 10664.12, base loss: 14422.18
[INFO 2017-06-28 14:38:12,436 main.py:51] epoch 1437, training loss: 11073.97, average training loss: 10663.51, base loss: 14421.86
[INFO 2017-06-28 14:38:13,267 main.py:51] epoch 1438, training loss: 9975.08, average training loss: 10660.26, base loss: 14418.62
[INFO 2017-06-28 14:38:13,916 main.py:51] epoch 1439, training loss: 10578.67, average training loss: 10659.34, base loss: 14418.36
[INFO 2017-06-28 14:38:14,717 main.py:51] epoch 1440, training loss: 12179.34, average training loss: 10662.04, base loss: 14423.68
[INFO 2017-06-28 14:38:15,549 main.py:51] epoch 1441, training loss: 10155.94, average training loss: 10662.60, base loss: 14425.80
[INFO 2017-06-28 14:38:16,309 main.py:51] epoch 1442, training loss: 10552.49, average training loss: 10661.03, base loss: 14425.03
[INFO 2017-06-28 14:38:16,996 main.py:51] epoch 1443, training loss: 11156.07, average training loss: 10660.75, base loss: 14426.89
[INFO 2017-06-28 14:38:17,786 main.py:51] epoch 1444, training loss: 9244.41, average training loss: 10658.77, base loss: 14423.87
[INFO 2017-06-28 14:38:18,570 main.py:51] epoch 1445, training loss: 10887.95, average training loss: 10658.63, base loss: 14425.04
[INFO 2017-06-28 14:38:19,223 main.py:51] epoch 1446, training loss: 10465.13, average training loss: 10657.28, base loss: 14423.50
[INFO 2017-06-28 14:38:20,001 main.py:51] epoch 1447, training loss: 10484.10, average training loss: 10657.34, base loss: 14425.31
[INFO 2017-06-28 14:38:20,828 main.py:51] epoch 1448, training loss: 9205.42, average training loss: 10655.52, base loss: 14422.22
[INFO 2017-06-28 14:38:21,607 main.py:51] epoch 1449, training loss: 10502.57, average training loss: 10654.12, base loss: 14422.02
[INFO 2017-06-28 14:38:22,257 main.py:51] epoch 1450, training loss: 10021.28, average training loss: 10653.29, base loss: 14422.51
[INFO 2017-06-28 14:38:23,058 main.py:51] epoch 1451, training loss: 10836.75, average training loss: 10653.52, base loss: 14423.63
[INFO 2017-06-28 14:38:23,852 main.py:51] epoch 1452, training loss: 8984.91, average training loss: 10650.54, base loss: 14420.15
[INFO 2017-06-28 14:38:24,520 main.py:51] epoch 1453, training loss: 10487.42, average training loss: 10649.80, base loss: 14420.59
[INFO 2017-06-28 14:38:25,302 main.py:51] epoch 1454, training loss: 8520.51, average training loss: 10647.08, base loss: 14418.53
[INFO 2017-06-28 14:38:26,142 main.py:51] epoch 1455, training loss: 10241.50, average training loss: 10646.80, base loss: 14419.03
[INFO 2017-06-28 14:38:26,810 main.py:51] epoch 1456, training loss: 9877.86, average training loss: 10645.41, base loss: 14417.35
[INFO 2017-06-28 14:38:27,577 main.py:51] epoch 1457, training loss: 8800.02, average training loss: 10640.09, base loss: 14410.97
[INFO 2017-06-28 14:38:28,378 main.py:51] epoch 1458, training loss: 9986.74, average training loss: 10640.22, base loss: 14413.17
[INFO 2017-06-28 14:38:29,230 main.py:51] epoch 1459, training loss: 11931.33, average training loss: 10641.23, base loss: 14416.04
[INFO 2017-06-28 14:38:29,879 main.py:51] epoch 1460, training loss: 11247.32, average training loss: 10640.96, base loss: 14416.59
[INFO 2017-06-28 14:38:30,677 main.py:51] epoch 1461, training loss: 10397.92, average training loss: 10641.38, base loss: 14419.86
[INFO 2017-06-28 14:38:31,508 main.py:51] epoch 1462, training loss: 10259.10, average training loss: 10640.40, base loss: 14420.30
[INFO 2017-06-28 14:38:32,331 main.py:51] epoch 1463, training loss: 9039.55, average training loss: 10637.37, base loss: 14418.98
[INFO 2017-06-28 14:38:33,020 main.py:51] epoch 1464, training loss: 9788.38, average training loss: 10637.14, base loss: 14418.88
[INFO 2017-06-28 14:38:33,813 main.py:51] epoch 1465, training loss: 10046.68, average training loss: 10637.59, base loss: 14421.21
[INFO 2017-06-28 14:38:34,436 main.py:51] epoch 1466, training loss: 9602.14, average training loss: 10636.94, base loss: 14420.86
[INFO 2017-06-28 14:38:35,052 main.py:51] epoch 1467, training loss: 11448.52, average training loss: 10637.57, base loss: 14421.26
[INFO 2017-06-28 14:38:35,737 main.py:51] epoch 1468, training loss: 10237.34, average training loss: 10635.76, base loss: 14419.48
[INFO 2017-06-28 14:38:36,422 main.py:51] epoch 1469, training loss: 9615.91, average training loss: 10634.54, base loss: 14417.71
[INFO 2017-06-28 14:38:37,125 main.py:51] epoch 1470, training loss: 9966.39, average training loss: 10634.03, base loss: 14418.93
[INFO 2017-06-28 14:38:37,740 main.py:51] epoch 1471, training loss: 10064.81, average training loss: 10631.71, base loss: 14417.87
[INFO 2017-06-28 14:38:38,542 main.py:51] epoch 1472, training loss: 10349.29, average training loss: 10630.44, base loss: 14416.79
[INFO 2017-06-28 14:38:39,370 main.py:51] epoch 1473, training loss: 8853.11, average training loss: 10628.73, base loss: 14415.67
[INFO 2017-06-28 14:38:40,174 main.py:51] epoch 1474, training loss: 10679.11, average training loss: 10630.31, base loss: 14418.83
[INFO 2017-06-28 14:38:40,861 main.py:51] epoch 1475, training loss: 12458.59, average training loss: 10632.38, base loss: 14422.90
[INFO 2017-06-28 14:38:41,611 main.py:51] epoch 1476, training loss: 9481.48, average training loss: 10631.00, base loss: 14421.84
[INFO 2017-06-28 14:38:42,411 main.py:51] epoch 1477, training loss: 10320.07, average training loss: 10629.46, base loss: 14421.64
[INFO 2017-06-28 14:38:43,118 main.py:51] epoch 1478, training loss: 11288.88, average training loss: 10628.21, base loss: 14421.41
[INFO 2017-06-28 14:38:43,847 main.py:51] epoch 1479, training loss: 10074.45, average training loss: 10627.97, base loss: 14423.34
[INFO 2017-06-28 14:38:44,645 main.py:51] epoch 1480, training loss: 10578.33, average training loss: 10626.90, base loss: 14424.08
[INFO 2017-06-28 14:38:45,509 main.py:51] epoch 1481, training loss: 9782.05, average training loss: 10626.25, base loss: 14423.50
[INFO 2017-06-28 14:38:46,124 main.py:51] epoch 1482, training loss: 10137.23, average training loss: 10626.39, base loss: 14425.18
[INFO 2017-06-28 14:38:46,930 main.py:51] epoch 1483, training loss: 12089.53, average training loss: 10628.09, base loss: 14428.46
[INFO 2017-06-28 14:38:47,759 main.py:51] epoch 1484, training loss: 10578.47, average training loss: 10629.13, base loss: 14429.79
[INFO 2017-06-28 14:38:48,485 main.py:51] epoch 1485, training loss: 10080.04, average training loss: 10628.29, base loss: 14428.69
[INFO 2017-06-28 14:38:49,195 main.py:51] epoch 1486, training loss: 10665.99, average training loss: 10628.25, base loss: 14429.03
[INFO 2017-06-28 14:38:49,970 main.py:51] epoch 1487, training loss: 10444.71, average training loss: 10627.97, base loss: 14429.90
[INFO 2017-06-28 14:38:50,739 main.py:51] epoch 1488, training loss: 10488.70, average training loss: 10627.36, base loss: 14428.77
[INFO 2017-06-28 14:38:51,373 main.py:51] epoch 1489, training loss: 10687.17, average training loss: 10627.25, base loss: 14430.76
[INFO 2017-06-28 14:38:52,177 main.py:51] epoch 1490, training loss: 9734.55, average training loss: 10626.23, base loss: 14431.10
[INFO 2017-06-28 14:38:52,989 main.py:51] epoch 1491, training loss: 10425.44, average training loss: 10624.85, base loss: 14429.60
[INFO 2017-06-28 14:38:53,793 main.py:51] epoch 1492, training loss: 9754.21, average training loss: 10623.55, base loss: 14427.48
[INFO 2017-06-28 14:38:54,450 main.py:51] epoch 1493, training loss: 10612.97, average training loss: 10619.38, base loss: 14423.46
[INFO 2017-06-28 14:38:55,267 main.py:51] epoch 1494, training loss: 11063.75, average training loss: 10621.10, base loss: 14426.21
[INFO 2017-06-28 14:38:56,082 main.py:51] epoch 1495, training loss: 10170.99, average training loss: 10622.06, base loss: 14428.51
[INFO 2017-06-28 14:38:56,919 main.py:51] epoch 1496, training loss: 11105.77, average training loss: 10620.58, base loss: 14426.95
[INFO 2017-06-28 14:38:57,573 main.py:51] epoch 1497, training loss: 10060.22, average training loss: 10619.28, base loss: 14425.29
[INFO 2017-06-28 14:38:58,345 main.py:51] epoch 1498, training loss: 10531.98, average training loss: 10619.84, base loss: 14427.21
[INFO 2017-06-28 14:38:59,159 main.py:51] epoch 1499, training loss: 10360.88, average training loss: 10619.88, base loss: 14427.98
[INFO 2017-06-28 14:38:59,159 main.py:53] epoch 1499, testing
[INFO 2017-06-28 14:39:02,179 main.py:105] average testing loss: 11804.81, base loss: 15815.89
[INFO 2017-06-28 14:39:02,180 main.py:106] improve_loss: 4011.08, improve_percent: 0.25
[INFO 2017-06-28 14:39:02,180 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:39:02,194 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:39:02,953 main.py:51] epoch 1500, training loss: 12777.72, average training loss: 10622.88, base loss: 14431.49
[INFO 2017-06-28 14:39:03,767 main.py:51] epoch 1501, training loss: 9861.74, average training loss: 10621.05, base loss: 14430.41
[INFO 2017-06-28 14:39:04,424 main.py:51] epoch 1502, training loss: 11566.04, average training loss: 10621.09, base loss: 14432.55
[INFO 2017-06-28 14:39:05,138 main.py:51] epoch 1503, training loss: 8782.13, average training loss: 10618.64, base loss: 14430.50
[INFO 2017-06-28 14:39:05,952 main.py:51] epoch 1504, training loss: 11268.54, average training loss: 10618.57, base loss: 14430.59
[INFO 2017-06-28 14:39:06,655 main.py:51] epoch 1505, training loss: 9729.23, average training loss: 10613.95, base loss: 14425.35
[INFO 2017-06-28 14:39:07,409 main.py:51] epoch 1506, training loss: 10181.96, average training loss: 10612.57, base loss: 14424.07
[INFO 2017-06-28 14:39:08,207 main.py:51] epoch 1507, training loss: 10442.37, average training loss: 10610.63, base loss: 14423.64
[INFO 2017-06-28 14:39:09,028 main.py:51] epoch 1508, training loss: 12347.80, average training loss: 10613.23, base loss: 14426.55
[INFO 2017-06-28 14:39:09,704 main.py:51] epoch 1509, training loss: 10685.31, average training loss: 10611.49, base loss: 14423.84
[INFO 2017-06-28 14:39:10,475 main.py:51] epoch 1510, training loss: 8970.84, average training loss: 10610.45, base loss: 14423.42
[INFO 2017-06-28 14:39:11,290 main.py:51] epoch 1511, training loss: 10578.12, average training loss: 10611.62, base loss: 14424.81
[INFO 2017-06-28 14:39:12,026 main.py:51] epoch 1512, training loss: 10191.04, average training loss: 10609.49, base loss: 14423.66
[INFO 2017-06-28 14:39:12,733 main.py:51] epoch 1513, training loss: 11760.52, average training loss: 10610.57, base loss: 14426.98
[INFO 2017-06-28 14:39:13,500 main.py:51] epoch 1514, training loss: 11117.91, average training loss: 10611.14, base loss: 14429.12
[INFO 2017-06-28 14:39:14,281 main.py:51] epoch 1515, training loss: 9621.81, average training loss: 10609.81, base loss: 14427.96
[INFO 2017-06-28 14:39:14,926 main.py:51] epoch 1516, training loss: 11844.95, average training loss: 10611.53, base loss: 14430.83
[INFO 2017-06-28 14:39:15,770 main.py:51] epoch 1517, training loss: 11044.83, average training loss: 10611.87, base loss: 14432.19
[INFO 2017-06-28 14:39:16,607 main.py:51] epoch 1518, training loss: 10516.34, average training loss: 10612.09, base loss: 14434.46
[INFO 2017-06-28 14:39:17,449 main.py:51] epoch 1519, training loss: 13180.23, average training loss: 10613.11, base loss: 14434.89
[INFO 2017-06-28 14:39:18,100 main.py:51] epoch 1520, training loss: 10323.05, average training loss: 10613.54, base loss: 14435.93
[INFO 2017-06-28 14:39:18,895 main.py:51] epoch 1521, training loss: 10353.88, average training loss: 10611.18, base loss: 14434.99
[INFO 2017-06-28 14:39:19,706 main.py:51] epoch 1522, training loss: 8841.76, average training loss: 10609.93, base loss: 14434.04
[INFO 2017-06-28 14:39:20,548 main.py:51] epoch 1523, training loss: 10863.94, average training loss: 10612.03, base loss: 14437.37
[INFO 2017-06-28 14:39:21,222 main.py:51] epoch 1524, training loss: 9402.16, average training loss: 10611.10, base loss: 14436.36
[INFO 2017-06-28 14:39:21,980 main.py:51] epoch 1525, training loss: 10134.95, average training loss: 10610.54, base loss: 14437.41
[INFO 2017-06-28 14:39:22,791 main.py:51] epoch 1526, training loss: 9528.62, average training loss: 10609.54, base loss: 14437.31
[INFO 2017-06-28 14:39:23,522 main.py:51] epoch 1527, training loss: 9642.45, average training loss: 10605.03, base loss: 14433.37
[INFO 2017-06-28 14:39:24,247 main.py:51] epoch 1528, training loss: 9912.82, average training loss: 10602.45, base loss: 14430.31
[INFO 2017-06-28 14:39:25,010 main.py:51] epoch 1529, training loss: 10266.67, average training loss: 10600.92, base loss: 14428.24
[INFO 2017-06-28 14:39:25,833 main.py:51] epoch 1530, training loss: 8483.08, average training loss: 10598.27, base loss: 14424.77
[INFO 2017-06-28 14:39:26,480 main.py:51] epoch 1531, training loss: 11097.11, average training loss: 10597.56, base loss: 14424.77
[INFO 2017-06-28 14:39:27,265 main.py:51] epoch 1532, training loss: 8782.44, average training loss: 10596.18, base loss: 14424.21
[INFO 2017-06-28 14:39:28,097 main.py:51] epoch 1533, training loss: 10766.91, average training loss: 10595.82, base loss: 14424.96
[INFO 2017-06-28 14:39:28,946 main.py:51] epoch 1534, training loss: 9826.18, average training loss: 10594.38, base loss: 14422.67
[INFO 2017-06-28 14:39:29,597 main.py:51] epoch 1535, training loss: 11915.35, average training loss: 10596.50, base loss: 14426.77
[INFO 2017-06-28 14:39:30,333 main.py:51] epoch 1536, training loss: 10323.34, average training loss: 10596.55, base loss: 14426.50
[INFO 2017-06-28 14:39:31,111 main.py:51] epoch 1537, training loss: 11187.80, average training loss: 10596.36, base loss: 14427.18
[INFO 2017-06-28 14:39:31,742 main.py:51] epoch 1538, training loss: 8811.63, average training loss: 10594.76, base loss: 14425.49
[INFO 2017-06-28 14:39:32,541 main.py:51] epoch 1539, training loss: 9838.94, average training loss: 10594.21, base loss: 14426.12
[INFO 2017-06-28 14:39:33,352 main.py:51] epoch 1540, training loss: 9447.97, average training loss: 10591.86, base loss: 14423.30
[INFO 2017-06-28 14:39:34,178 main.py:51] epoch 1541, training loss: 11700.03, average training loss: 10591.92, base loss: 14424.76
[INFO 2017-06-28 14:39:34,796 main.py:51] epoch 1542, training loss: 12830.44, average training loss: 10593.56, base loss: 14427.43
[INFO 2017-06-28 14:39:35,610 main.py:51] epoch 1543, training loss: 9141.24, average training loss: 10591.71, base loss: 14426.86
[INFO 2017-06-28 14:39:36,423 main.py:51] epoch 1544, training loss: 11472.74, average training loss: 10593.99, base loss: 14430.96
[INFO 2017-06-28 14:39:37,206 main.py:51] epoch 1545, training loss: 10223.87, average training loss: 10593.21, base loss: 14429.97
[INFO 2017-06-28 14:39:37,860 main.py:51] epoch 1546, training loss: 9959.10, average training loss: 10592.98, base loss: 14430.78
[INFO 2017-06-28 14:39:38,643 main.py:51] epoch 1547, training loss: 11666.88, average training loss: 10594.97, base loss: 14435.10
[INFO 2017-06-28 14:39:39,409 main.py:51] epoch 1548, training loss: 11451.31, average training loss: 10596.62, base loss: 14437.63
[INFO 2017-06-28 14:39:40,045 main.py:51] epoch 1549, training loss: 10964.23, average training loss: 10596.86, base loss: 14440.77
[INFO 2017-06-28 14:39:40,852 main.py:51] epoch 1550, training loss: 10471.47, average training loss: 10593.74, base loss: 14435.93
[INFO 2017-06-28 14:39:41,674 main.py:51] epoch 1551, training loss: 9749.57, average training loss: 10592.57, base loss: 14434.49
[INFO 2017-06-28 14:39:42,505 main.py:51] epoch 1552, training loss: 10802.69, average training loss: 10593.69, base loss: 14437.02
[INFO 2017-06-28 14:39:43,153 main.py:51] epoch 1553, training loss: 11560.85, average training loss: 10594.73, base loss: 14439.03
[INFO 2017-06-28 14:39:43,931 main.py:51] epoch 1554, training loss: 10547.42, average training loss: 10594.13, base loss: 14438.96
[INFO 2017-06-28 14:39:44,769 main.py:51] epoch 1555, training loss: 10639.57, average training loss: 10595.09, base loss: 14439.87
[INFO 2017-06-28 14:39:45,539 main.py:51] epoch 1556, training loss: 10491.00, average training loss: 10596.99, base loss: 14442.83
[INFO 2017-06-28 14:39:46,234 main.py:51] epoch 1557, training loss: 10334.04, average training loss: 10596.34, base loss: 14442.56
[INFO 2017-06-28 14:39:47,053 main.py:51] epoch 1558, training loss: 10109.89, average training loss: 10594.98, base loss: 14441.84
[INFO 2017-06-28 14:39:47,889 main.py:51] epoch 1559, training loss: 12214.69, average training loss: 10595.56, base loss: 14443.67
[INFO 2017-06-28 14:39:48,587 main.py:51] epoch 1560, training loss: 10159.66, average training loss: 10594.82, base loss: 14443.89
[INFO 2017-06-28 14:39:49,324 main.py:51] epoch 1561, training loss: 10127.47, average training loss: 10594.07, base loss: 14443.83
[INFO 2017-06-28 14:39:50,122 main.py:51] epoch 1562, training loss: 9576.44, average training loss: 10592.97, base loss: 14442.74
[INFO 2017-06-28 14:39:50,958 main.py:51] epoch 1563, training loss: 9653.44, average training loss: 10591.66, base loss: 14439.44
[INFO 2017-06-28 14:39:51,654 main.py:51] epoch 1564, training loss: 10057.85, average training loss: 10590.94, base loss: 14439.83
[INFO 2017-06-28 14:39:52,414 main.py:51] epoch 1565, training loss: 11503.81, average training loss: 10592.86, base loss: 14443.75
[INFO 2017-06-28 14:39:53,238 main.py:51] epoch 1566, training loss: 12029.62, average training loss: 10594.93, base loss: 14446.67
[INFO 2017-06-28 14:39:54,041 main.py:51] epoch 1567, training loss: 11228.07, average training loss: 10594.18, base loss: 14444.98
[INFO 2017-06-28 14:39:54,748 main.py:51] epoch 1568, training loss: 10751.23, average training loss: 10594.13, base loss: 14445.49
[INFO 2017-06-28 14:39:55,530 main.py:51] epoch 1569, training loss: 12137.43, average training loss: 10594.29, base loss: 14446.80
[INFO 2017-06-28 14:39:56,384 main.py:51] epoch 1570, training loss: 11733.13, average training loss: 10594.93, base loss: 14448.17
[INFO 2017-06-28 14:39:57,167 main.py:51] epoch 1571, training loss: 8981.67, average training loss: 10593.07, base loss: 14444.99
[INFO 2017-06-28 14:39:57,856 main.py:51] epoch 1572, training loss: 10265.22, average training loss: 10590.51, base loss: 14441.42
[INFO 2017-06-28 14:39:58,642 main.py:51] epoch 1573, training loss: 9505.86, average training loss: 10589.81, base loss: 14440.96
[INFO 2017-06-28 14:39:59,485 main.py:51] epoch 1574, training loss: 10634.13, average training loss: 10590.49, base loss: 14444.96
[INFO 2017-06-28 14:40:00,197 main.py:51] epoch 1575, training loss: 12399.92, average training loss: 10592.42, base loss: 14446.74
[INFO 2017-06-28 14:40:00,935 main.py:51] epoch 1576, training loss: 10884.24, average training loss: 10592.17, base loss: 14447.53
[INFO 2017-06-28 14:40:01,747 main.py:51] epoch 1577, training loss: 10346.67, average training loss: 10593.41, base loss: 14448.40
[INFO 2017-06-28 14:40:02,601 main.py:51] epoch 1578, training loss: 10038.23, average training loss: 10591.96, base loss: 14447.44
[INFO 2017-06-28 14:40:03,264 main.py:51] epoch 1579, training loss: 9210.69, average training loss: 10590.01, base loss: 14446.32
[INFO 2017-06-28 14:40:04,044 main.py:51] epoch 1580, training loss: 11233.82, average training loss: 10589.16, base loss: 14445.68
[INFO 2017-06-28 14:40:04,858 main.py:51] epoch 1581, training loss: 12654.12, average training loss: 10591.23, base loss: 14448.75
[INFO 2017-06-28 14:40:05,582 main.py:51] epoch 1582, training loss: 11418.97, average training loss: 10591.57, base loss: 14449.33
[INFO 2017-06-28 14:40:06,316 main.py:51] epoch 1583, training loss: 10200.72, average training loss: 10591.34, base loss: 14448.72
[INFO 2017-06-28 14:40:07,088 main.py:51] epoch 1584, training loss: 10497.67, average training loss: 10591.93, base loss: 14450.31
[INFO 2017-06-28 14:40:07,885 main.py:51] epoch 1585, training loss: 10786.53, average training loss: 10591.69, base loss: 14450.96
[INFO 2017-06-28 14:40:08,527 main.py:51] epoch 1586, training loss: 10239.45, average training loss: 10591.83, base loss: 14452.79
[INFO 2017-06-28 14:40:09,341 main.py:51] epoch 1587, training loss: 12039.46, average training loss: 10594.17, base loss: 14455.23
[INFO 2017-06-28 14:40:10,156 main.py:51] epoch 1588, training loss: 10502.61, average training loss: 10596.09, base loss: 14457.95
[INFO 2017-06-28 14:40:11,010 main.py:51] epoch 1589, training loss: 10539.19, average training loss: 10595.85, base loss: 14459.14
[INFO 2017-06-28 14:40:11,640 main.py:51] epoch 1590, training loss: 11252.80, average training loss: 10595.87, base loss: 14459.08
[INFO 2017-06-28 14:40:12,436 main.py:51] epoch 1591, training loss: 10989.50, average training loss: 10596.77, base loss: 14461.64
[INFO 2017-06-28 14:40:13,248 main.py:51] epoch 1592, training loss: 9477.62, average training loss: 10595.93, base loss: 14461.33
[INFO 2017-06-28 14:40:13,964 main.py:51] epoch 1593, training loss: 11469.86, average training loss: 10598.20, base loss: 14464.59
[INFO 2017-06-28 14:40:14,685 main.py:51] epoch 1594, training loss: 12310.79, average training loss: 10598.79, base loss: 14467.10
[INFO 2017-06-28 14:40:15,467 main.py:51] epoch 1595, training loss: 10317.89, average training loss: 10597.20, base loss: 14466.47
[INFO 2017-06-28 14:40:16,310 main.py:51] epoch 1596, training loss: 9391.02, average training loss: 10597.22, base loss: 14467.03
[INFO 2017-06-28 14:40:16,982 main.py:51] epoch 1597, training loss: 12741.53, average training loss: 10598.04, base loss: 14466.87
[INFO 2017-06-28 14:40:17,758 main.py:51] epoch 1598, training loss: 10816.57, average training loss: 10599.29, base loss: 14470.15
[INFO 2017-06-28 14:40:18,569 main.py:51] epoch 1599, training loss: 9487.04, average training loss: 10596.81, base loss: 14468.15
[INFO 2017-06-28 14:40:18,569 main.py:53] epoch 1599, testing
[INFO 2017-06-28 14:40:21,376 main.py:105] average testing loss: 11445.75, base loss: 14835.46
[INFO 2017-06-28 14:40:21,376 main.py:106] improve_loss: 3389.71, improve_percent: 0.23
[INFO 2017-06-28 14:40:21,377 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:40:22,012 main.py:51] epoch 1600, training loss: 10727.12, average training loss: 10597.55, base loss: 14468.19
[INFO 2017-06-28 14:40:22,663 main.py:51] epoch 1601, training loss: 10577.53, average training loss: 10597.70, base loss: 14468.62
[INFO 2017-06-28 14:40:23,268 main.py:51] epoch 1602, training loss: 12295.69, average training loss: 10599.35, base loss: 14473.16
[INFO 2017-06-28 14:40:23,935 main.py:51] epoch 1603, training loss: 11207.30, average training loss: 10601.50, base loss: 14476.81
[INFO 2017-06-28 14:40:24,627 main.py:51] epoch 1604, training loss: 10435.57, average training loss: 10602.07, base loss: 14476.63
[INFO 2017-06-28 14:40:25,274 main.py:51] epoch 1605, training loss: 10783.35, average training loss: 10602.92, base loss: 14480.12
[INFO 2017-06-28 14:40:26,120 main.py:51] epoch 1606, training loss: 10095.85, average training loss: 10601.94, base loss: 14479.78
[INFO 2017-06-28 14:40:26,920 main.py:51] epoch 1607, training loss: 9914.29, average training loss: 10601.01, base loss: 14479.48
[INFO 2017-06-28 14:40:27,580 main.py:51] epoch 1608, training loss: 9374.42, average training loss: 10599.42, base loss: 14477.06
[INFO 2017-06-28 14:40:28,399 main.py:51] epoch 1609, training loss: 11618.25, average training loss: 10601.08, base loss: 14480.89
[INFO 2017-06-28 14:40:29,225 main.py:51] epoch 1610, training loss: 9563.20, average training loss: 10599.42, base loss: 14477.87
[INFO 2017-06-28 14:40:29,970 main.py:51] epoch 1611, training loss: 9315.93, average training loss: 10597.56, base loss: 14474.98
[INFO 2017-06-28 14:40:30,690 main.py:51] epoch 1612, training loss: 9155.46, average training loss: 10595.12, base loss: 14471.97
[INFO 2017-06-28 14:40:31,480 main.py:51] epoch 1613, training loss: 8327.78, average training loss: 10593.50, base loss: 14469.30
[INFO 2017-06-28 14:40:32,303 main.py:51] epoch 1614, training loss: 10905.58, average training loss: 10592.37, base loss: 14469.35
[INFO 2017-06-28 14:40:32,934 main.py:51] epoch 1615, training loss: 10857.23, average training loss: 10593.56, base loss: 14472.69
[INFO 2017-06-28 14:40:33,689 main.py:51] epoch 1616, training loss: 12842.27, average training loss: 10596.99, base loss: 14477.26
[INFO 2017-06-28 14:40:34,503 main.py:51] epoch 1617, training loss: 10372.70, average training loss: 10597.58, base loss: 14478.68
[INFO 2017-06-28 14:40:35,234 main.py:51] epoch 1618, training loss: 11357.44, average training loss: 10598.96, base loss: 14481.13
[INFO 2017-06-28 14:40:35,933 main.py:51] epoch 1619, training loss: 9451.90, average training loss: 10595.74, base loss: 14478.40
[INFO 2017-06-28 14:40:36,730 main.py:51] epoch 1620, training loss: 10680.52, average training loss: 10594.14, base loss: 14476.06
[INFO 2017-06-28 14:40:37,509 main.py:51] epoch 1621, training loss: 9388.19, average training loss: 10594.03, base loss: 14475.25
[INFO 2017-06-28 14:40:38,149 main.py:51] epoch 1622, training loss: 10801.02, average training loss: 10593.79, base loss: 14475.99
[INFO 2017-06-28 14:40:38,918 main.py:51] epoch 1623, training loss: 11294.58, average training loss: 10594.51, base loss: 14478.90
[INFO 2017-06-28 14:40:39,744 main.py:51] epoch 1624, training loss: 9993.21, average training loss: 10593.63, base loss: 14478.57
[INFO 2017-06-28 14:40:40,475 main.py:51] epoch 1625, training loss: 11295.27, average training loss: 10593.67, base loss: 14480.32
[INFO 2017-06-28 14:40:41,159 main.py:51] epoch 1626, training loss: 10069.14, average training loss: 10592.95, base loss: 14478.35
[INFO 2017-06-28 14:40:41,990 main.py:51] epoch 1627, training loss: 9338.18, average training loss: 10592.61, base loss: 14479.46
[INFO 2017-06-28 14:40:42,709 main.py:51] epoch 1628, training loss: 9303.53, average training loss: 10591.53, base loss: 14477.38
[INFO 2017-06-28 14:40:43,427 main.py:51] epoch 1629, training loss: 10820.98, average training loss: 10591.15, base loss: 14475.93
[INFO 2017-06-28 14:40:44,210 main.py:51] epoch 1630, training loss: 10215.47, average training loss: 10587.77, base loss: 14473.17
[INFO 2017-06-28 14:40:45,037 main.py:51] epoch 1631, training loss: 11470.26, average training loss: 10588.63, base loss: 14474.90
[INFO 2017-06-28 14:40:45,699 main.py:51] epoch 1632, training loss: 9951.30, average training loss: 10588.95, base loss: 14475.70
[INFO 2017-06-28 14:40:46,510 main.py:51] epoch 1633, training loss: 9892.72, average training loss: 10587.48, base loss: 14473.15
[INFO 2017-06-28 14:40:47,347 main.py:51] epoch 1634, training loss: 10755.27, average training loss: 10587.74, base loss: 14474.34
[INFO 2017-06-28 14:40:48,136 main.py:51] epoch 1635, training loss: 10580.11, average training loss: 10587.34, base loss: 14474.73
[INFO 2017-06-28 14:40:48,850 main.py:51] epoch 1636, training loss: 13004.58, average training loss: 10589.65, base loss: 14477.71
[INFO 2017-06-28 14:40:49,622 main.py:51] epoch 1637, training loss: 10521.29, average training loss: 10590.58, base loss: 14479.76
[INFO 2017-06-28 14:40:50,400 main.py:51] epoch 1638, training loss: 10436.72, average training loss: 10589.02, base loss: 14478.44
[INFO 2017-06-28 14:40:51,018 main.py:51] epoch 1639, training loss: 10835.95, average training loss: 10589.15, base loss: 14478.57
[INFO 2017-06-28 14:40:51,785 main.py:51] epoch 1640, training loss: 11331.06, average training loss: 10590.58, base loss: 14482.69
[INFO 2017-06-28 14:40:52,590 main.py:51] epoch 1641, training loss: 8854.15, average training loss: 10589.83, base loss: 14482.54
[INFO 2017-06-28 14:40:53,260 main.py:51] epoch 1642, training loss: 9969.48, average training loss: 10587.71, base loss: 14479.99
[INFO 2017-06-28 14:40:53,950 main.py:51] epoch 1643, training loss: 12590.19, average training loss: 10589.90, base loss: 14482.54
[INFO 2017-06-28 14:40:54,761 main.py:51] epoch 1644, training loss: 10155.20, average training loss: 10589.91, base loss: 14482.39
[INFO 2017-06-28 14:40:55,474 main.py:51] epoch 1645, training loss: 12293.94, average training loss: 10592.09, base loss: 14487.31
[INFO 2017-06-28 14:40:56,188 main.py:51] epoch 1646, training loss: 13571.11, average training loss: 10593.94, base loss: 14487.60
[INFO 2017-06-28 14:40:57,017 main.py:51] epoch 1647, training loss: 10204.89, average training loss: 10595.10, base loss: 14490.40
[INFO 2017-06-28 14:40:57,775 main.py:51] epoch 1648, training loss: 10461.37, average training loss: 10595.68, base loss: 14491.10
[INFO 2017-06-28 14:40:58,456 main.py:51] epoch 1649, training loss: 10726.10, average training loss: 10595.63, base loss: 14490.35
[INFO 2017-06-28 14:40:59,232 main.py:51] epoch 1650, training loss: 11724.58, average training loss: 10596.62, base loss: 14492.72
[INFO 2017-06-28 14:41:00,009 main.py:51] epoch 1651, training loss: 10744.23, average training loss: 10597.28, base loss: 14493.83
[INFO 2017-06-28 14:41:00,643 main.py:51] epoch 1652, training loss: 10895.93, average training loss: 10596.34, base loss: 14492.46
[INFO 2017-06-28 14:41:01,462 main.py:51] epoch 1653, training loss: 10229.08, average training loss: 10595.61, base loss: 14491.76
[INFO 2017-06-28 14:41:02,279 main.py:51] epoch 1654, training loss: 9011.25, average training loss: 10594.73, base loss: 14490.94
[INFO 2017-06-28 14:41:03,018 main.py:51] epoch 1655, training loss: 9608.03, average training loss: 10594.12, base loss: 14490.99
[INFO 2017-06-28 14:41:03,725 main.py:51] epoch 1656, training loss: 10798.43, average training loss: 10593.35, base loss: 14489.96
[INFO 2017-06-28 14:41:04,498 main.py:51] epoch 1657, training loss: 10200.84, average training loss: 10593.89, base loss: 14492.73
[INFO 2017-06-28 14:41:05,316 main.py:51] epoch 1658, training loss: 11285.11, average training loss: 10596.26, base loss: 14496.24
[INFO 2017-06-28 14:41:05,978 main.py:51] epoch 1659, training loss: 10546.04, average training loss: 10595.13, base loss: 14496.09
[INFO 2017-06-28 14:41:06,727 main.py:51] epoch 1660, training loss: 11349.83, average training loss: 10594.86, base loss: 14496.75
[INFO 2017-06-28 14:41:07,523 main.py:51] epoch 1661, training loss: 9281.20, average training loss: 10594.13, base loss: 14496.26
[INFO 2017-06-28 14:41:08,211 main.py:51] epoch 1662, training loss: 9671.01, average training loss: 10591.50, base loss: 14493.35
[INFO 2017-06-28 14:41:08,966 main.py:51] epoch 1663, training loss: 11588.16, average training loss: 10593.22, base loss: 14495.59
[INFO 2017-06-28 14:41:09,767 main.py:51] epoch 1664, training loss: 11655.05, average training loss: 10593.82, base loss: 14497.40
[INFO 2017-06-28 14:41:10,558 main.py:51] epoch 1665, training loss: 10992.55, average training loss: 10594.50, base loss: 14499.71
[INFO 2017-06-28 14:41:11,173 main.py:51] epoch 1666, training loss: 10169.25, average training loss: 10595.25, base loss: 14501.10
[INFO 2017-06-28 14:41:11,983 main.py:51] epoch 1667, training loss: 10581.04, average training loss: 10596.40, base loss: 14502.43
[INFO 2017-06-28 14:41:12,810 main.py:51] epoch 1668, training loss: 9140.01, average training loss: 10596.02, base loss: 14502.85
[INFO 2017-06-28 14:41:13,624 main.py:51] epoch 1669, training loss: 11262.53, average training loss: 10598.74, base loss: 14506.27
[INFO 2017-06-28 14:41:14,293 main.py:51] epoch 1670, training loss: 11102.68, average training loss: 10598.88, base loss: 14507.54
[INFO 2017-06-28 14:41:15,114 main.py:51] epoch 1671, training loss: 10689.21, average training loss: 10599.31, base loss: 14509.82
[INFO 2017-06-28 14:41:15,924 main.py:51] epoch 1672, training loss: 10704.66, average training loss: 10601.23, base loss: 14512.41
[INFO 2017-06-28 14:41:16,752 main.py:51] epoch 1673, training loss: 12255.67, average training loss: 10602.38, base loss: 14515.14
[INFO 2017-06-28 14:41:17,398 main.py:51] epoch 1674, training loss: 9583.87, average training loss: 10601.13, base loss: 14514.25
[INFO 2017-06-28 14:41:18,213 main.py:51] epoch 1675, training loss: 10534.90, average training loss: 10599.68, base loss: 14513.02
[INFO 2017-06-28 14:41:19,028 main.py:51] epoch 1676, training loss: 11277.69, average training loss: 10600.57, base loss: 14513.72
[INFO 2017-06-28 14:41:19,884 main.py:51] epoch 1677, training loss: 11951.74, average training loss: 10601.48, base loss: 14514.04
[INFO 2017-06-28 14:41:20,548 main.py:51] epoch 1678, training loss: 11392.74, average training loss: 10601.88, base loss: 14514.79
[INFO 2017-06-28 14:41:21,371 main.py:51] epoch 1679, training loss: 9694.78, average training loss: 10599.44, base loss: 14512.30
[INFO 2017-06-28 14:41:22,204 main.py:51] epoch 1680, training loss: 10660.51, average training loss: 10599.67, base loss: 14514.65
[INFO 2017-06-28 14:41:23,001 main.py:51] epoch 1681, training loss: 9137.09, average training loss: 10598.78, base loss: 14513.89
[INFO 2017-06-28 14:41:23,681 main.py:51] epoch 1682, training loss: 9992.18, average training loss: 10598.05, base loss: 14513.39
[INFO 2017-06-28 14:41:24,510 main.py:51] epoch 1683, training loss: 11179.16, average training loss: 10597.28, base loss: 14513.39
[INFO 2017-06-28 14:41:25,325 main.py:51] epoch 1684, training loss: 9544.12, average training loss: 10595.91, base loss: 14512.44
[INFO 2017-06-28 14:41:26,103 main.py:51] epoch 1685, training loss: 9752.72, average training loss: 10593.95, base loss: 14510.75
[INFO 2017-06-28 14:41:26,747 main.py:51] epoch 1686, training loss: 10735.91, average training loss: 10592.47, base loss: 14510.68
[INFO 2017-06-28 14:41:27,523 main.py:51] epoch 1687, training loss: 11371.79, average training loss: 10592.39, base loss: 14511.31
[INFO 2017-06-28 14:41:28,336 main.py:51] epoch 1688, training loss: 9601.60, average training loss: 10592.11, base loss: 14511.15
[INFO 2017-06-28 14:41:28,982 main.py:51] epoch 1689, training loss: 8963.45, average training loss: 10590.52, base loss: 14509.94
[INFO 2017-06-28 14:41:29,743 main.py:51] epoch 1690, training loss: 10395.26, average training loss: 10591.17, base loss: 14511.19
[INFO 2017-06-28 14:41:30,551 main.py:51] epoch 1691, training loss: 9587.41, average training loss: 10590.01, base loss: 14509.18
[INFO 2017-06-28 14:41:31,379 main.py:51] epoch 1692, training loss: 11567.39, average training loss: 10590.72, base loss: 14510.45
[INFO 2017-06-28 14:41:32,037 main.py:51] epoch 1693, training loss: 8998.65, average training loss: 10588.88, base loss: 14507.35
[INFO 2017-06-28 14:41:32,839 main.py:51] epoch 1694, training loss: 11342.54, average training loss: 10588.37, base loss: 14507.92
[INFO 2017-06-28 14:41:33,664 main.py:51] epoch 1695, training loss: 10927.06, average training loss: 10588.63, base loss: 14508.19
[INFO 2017-06-28 14:41:34,432 main.py:51] epoch 1696, training loss: 11276.46, average training loss: 10590.19, base loss: 14509.92
[INFO 2017-06-28 14:41:35,148 main.py:51] epoch 1697, training loss: 9224.65, average training loss: 10586.41, base loss: 14505.16
[INFO 2017-06-28 14:41:35,977 main.py:51] epoch 1698, training loss: 9346.62, average training loss: 10586.05, base loss: 14507.01
[INFO 2017-06-28 14:41:36,769 main.py:51] epoch 1699, training loss: 11245.08, average training loss: 10587.28, base loss: 14510.16
[INFO 2017-06-28 14:41:36,770 main.py:53] epoch 1699, testing
[INFO 2017-06-28 14:41:39,651 main.py:105] average testing loss: 12075.66, base loss: 15965.90
[INFO 2017-06-28 14:41:39,651 main.py:106] improve_loss: 3890.24, improve_percent: 0.24
[INFO 2017-06-28 14:41:39,652 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:41:40,389 main.py:51] epoch 1700, training loss: 11069.60, average training loss: 10588.99, base loss: 14512.67
[INFO 2017-06-28 14:41:41,086 main.py:51] epoch 1701, training loss: 8631.79, average training loss: 10587.20, base loss: 14510.11
[INFO 2017-06-28 14:41:41,880 main.py:51] epoch 1702, training loss: 10019.13, average training loss: 10585.55, base loss: 14508.30
[INFO 2017-06-28 14:41:42,686 main.py:51] epoch 1703, training loss: 9264.67, average training loss: 10583.35, base loss: 14506.65
[INFO 2017-06-28 14:41:43,377 main.py:51] epoch 1704, training loss: 10120.95, average training loss: 10582.44, base loss: 14506.10
[INFO 2017-06-28 14:41:44,113 main.py:51] epoch 1705, training loss: 9890.34, average training loss: 10582.19, base loss: 14506.02
[INFO 2017-06-28 14:41:44,933 main.py:51] epoch 1706, training loss: 10535.39, average training loss: 10583.38, base loss: 14509.23
[INFO 2017-06-28 14:41:45,706 main.py:51] epoch 1707, training loss: 11531.18, average training loss: 10584.81, base loss: 14512.32
[INFO 2017-06-28 14:41:46,368 main.py:51] epoch 1708, training loss: 11743.79, average training loss: 10586.98, base loss: 14515.03
[INFO 2017-06-28 14:41:47,138 main.py:51] epoch 1709, training loss: 10734.62, average training loss: 10586.39, base loss: 14515.77
[INFO 2017-06-28 14:41:47,954 main.py:51] epoch 1710, training loss: 8808.95, average training loss: 10584.95, base loss: 14514.22
[INFO 2017-06-28 14:41:48,799 main.py:51] epoch 1711, training loss: 11331.60, average training loss: 10585.74, base loss: 14514.46
[INFO 2017-06-28 14:41:49,440 main.py:51] epoch 1712, training loss: 9936.35, average training loss: 10585.30, base loss: 14515.26
[INFO 2017-06-28 14:41:50,208 main.py:51] epoch 1713, training loss: 10823.28, average training loss: 10585.50, base loss: 14516.92
[INFO 2017-06-28 14:41:51,030 main.py:51] epoch 1714, training loss: 11539.99, average training loss: 10584.74, base loss: 14515.99
[INFO 2017-06-28 14:41:51,694 main.py:51] epoch 1715, training loss: 8671.34, average training loss: 10583.38, base loss: 14514.70
[INFO 2017-06-28 14:41:52,447 main.py:51] epoch 1716, training loss: 9926.55, average training loss: 10582.41, base loss: 14512.74
[INFO 2017-06-28 14:41:53,276 main.py:51] epoch 1717, training loss: 10945.07, average training loss: 10583.65, base loss: 14515.66
[INFO 2017-06-28 14:41:53,999 main.py:51] epoch 1718, training loss: 8420.93, average training loss: 10581.39, base loss: 14512.99
[INFO 2017-06-28 14:41:54,698 main.py:51] epoch 1719, training loss: 11447.01, average training loss: 10583.21, base loss: 14514.30
[INFO 2017-06-28 14:41:55,483 main.py:51] epoch 1720, training loss: 10719.59, average training loss: 10583.49, base loss: 14514.49
[INFO 2017-06-28 14:41:56,279 main.py:51] epoch 1721, training loss: 9809.75, average training loss: 10582.25, base loss: 14513.95
[INFO 2017-06-28 14:41:56,927 main.py:51] epoch 1722, training loss: 10538.14, average training loss: 10580.27, base loss: 14511.08
[INFO 2017-06-28 14:41:57,760 main.py:51] epoch 1723, training loss: 9877.91, average training loss: 10579.31, base loss: 14509.78
[INFO 2017-06-28 14:41:58,590 main.py:51] epoch 1724, training loss: 9250.82, average training loss: 10576.76, base loss: 14506.10
[INFO 2017-06-28 14:41:59,405 main.py:51] epoch 1725, training loss: 10407.64, average training loss: 10575.90, base loss: 14506.29
[INFO 2017-06-28 14:42:00,069 main.py:51] epoch 1726, training loss: 10452.65, average training loss: 10574.63, base loss: 14503.63
[INFO 2017-06-28 14:42:00,840 main.py:51] epoch 1727, training loss: 9935.33, average training loss: 10574.34, base loss: 14503.14
[INFO 2017-06-28 14:42:01,676 main.py:51] epoch 1728, training loss: 8929.14, average training loss: 10573.09, base loss: 14500.79
[INFO 2017-06-28 14:42:02,319 main.py:51] epoch 1729, training loss: 11114.56, average training loss: 10573.68, base loss: 14502.65
[INFO 2017-06-28 14:42:03,054 main.py:51] epoch 1730, training loss: 9052.67, average training loss: 10571.97, base loss: 14501.93
[INFO 2017-06-28 14:42:03,897 main.py:51] epoch 1731, training loss: 9352.90, average training loss: 10570.89, base loss: 14502.14
[INFO 2017-06-28 14:42:04,657 main.py:51] epoch 1732, training loss: 11303.87, average training loss: 10570.41, base loss: 14501.83
[INFO 2017-06-28 14:42:05,356 main.py:51] epoch 1733, training loss: 11073.19, average training loss: 10570.14, base loss: 14502.38
[INFO 2017-06-28 14:42:06,163 main.py:51] epoch 1734, training loss: 11046.43, average training loss: 10570.52, base loss: 14502.90
[INFO 2017-06-28 14:42:06,934 main.py:51] epoch 1735, training loss: 10234.77, average training loss: 10568.40, base loss: 14501.15
[INFO 2017-06-28 14:42:07,567 main.py:51] epoch 1736, training loss: 9048.15, average training loss: 10565.92, base loss: 14499.43
[INFO 2017-06-28 14:42:08,338 main.py:51] epoch 1737, training loss: 9896.65, average training loss: 10565.18, base loss: 14499.48
[INFO 2017-06-28 14:42:09,156 main.py:51] epoch 1738, training loss: 10791.21, average training loss: 10565.78, base loss: 14502.12
[INFO 2017-06-28 14:42:09,939 main.py:51] epoch 1739, training loss: 9898.80, average training loss: 10566.17, base loss: 14502.95
[INFO 2017-06-28 14:42:10,675 main.py:51] epoch 1740, training loss: 11573.49, average training loss: 10566.15, base loss: 14503.05
[INFO 2017-06-28 14:42:11,350 main.py:51] epoch 1741, training loss: 11496.23, average training loss: 10568.25, base loss: 14507.69
[INFO 2017-06-28 14:42:12,029 main.py:51] epoch 1742, training loss: 9350.85, average training loss: 10565.78, base loss: 14507.07
[INFO 2017-06-28 14:42:12,630 main.py:51] epoch 1743, training loss: 8820.86, average training loss: 10564.20, base loss: 14505.61
[INFO 2017-06-28 14:42:13,252 main.py:51] epoch 1744, training loss: 10230.78, average training loss: 10563.86, base loss: 14507.66
[INFO 2017-06-28 14:42:13,939 main.py:51] epoch 1745, training loss: 9489.26, average training loss: 10561.43, base loss: 14506.06
[INFO 2017-06-28 14:42:14,650 main.py:51] epoch 1746, training loss: 9581.19, average training loss: 10559.73, base loss: 14504.42
[INFO 2017-06-28 14:42:15,402 main.py:51] epoch 1747, training loss: 10509.87, average training loss: 10559.14, base loss: 14504.06
[INFO 2017-06-28 14:42:16,179 main.py:51] epoch 1748, training loss: 10168.03, average training loss: 10558.77, base loss: 14505.04
[INFO 2017-06-28 14:42:17,011 main.py:51] epoch 1749, training loss: 10263.07, average training loss: 10557.60, base loss: 14503.10
[INFO 2017-06-28 14:42:17,691 main.py:51] epoch 1750, training loss: 10967.40, average training loss: 10556.97, base loss: 14504.04
[INFO 2017-06-28 14:42:18,458 main.py:51] epoch 1751, training loss: 9393.63, average training loss: 10556.39, base loss: 14502.95
[INFO 2017-06-28 14:42:19,275 main.py:51] epoch 1752, training loss: 10756.27, average training loss: 10557.25, base loss: 14505.77
[INFO 2017-06-28 14:42:20,111 main.py:51] epoch 1753, training loss: 11367.40, average training loss: 10558.60, base loss: 14508.35
[INFO 2017-06-28 14:42:20,752 main.py:51] epoch 1754, training loss: 10194.58, average training loss: 10557.84, base loss: 14506.90
[INFO 2017-06-28 14:42:21,539 main.py:51] epoch 1755, training loss: 9867.40, average training loss: 10556.10, base loss: 14505.73
[INFO 2017-06-28 14:42:22,378 main.py:51] epoch 1756, training loss: 9957.28, average training loss: 10554.25, base loss: 14503.29
[INFO 2017-06-28 14:42:23,153 main.py:51] epoch 1757, training loss: 11427.44, average training loss: 10555.81, base loss: 14505.57
[INFO 2017-06-28 14:42:23,891 main.py:51] epoch 1758, training loss: 10305.02, average training loss: 10553.76, base loss: 14503.32
[INFO 2017-06-28 14:42:24,694 main.py:51] epoch 1759, training loss: 12296.85, average training loss: 10555.21, base loss: 14505.24
[INFO 2017-06-28 14:42:25,498 main.py:51] epoch 1760, training loss: 12102.62, average training loss: 10555.43, base loss: 14505.13
[INFO 2017-06-28 14:42:26,268 main.py:51] epoch 1761, training loss: 10042.01, average training loss: 10554.81, base loss: 14504.44
[INFO 2017-06-28 14:42:26,974 main.py:51] epoch 1762, training loss: 10368.05, average training loss: 10554.46, base loss: 14506.19
[INFO 2017-06-28 14:42:27,747 main.py:51] epoch 1763, training loss: 9861.34, average training loss: 10552.27, base loss: 14505.20
[INFO 2017-06-28 14:42:28,570 main.py:51] epoch 1764, training loss: 10016.80, average training loss: 10552.19, base loss: 14505.54
[INFO 2017-06-28 14:42:29,192 main.py:51] epoch 1765, training loss: 10434.40, average training loss: 10551.86, base loss: 14506.28
[INFO 2017-06-28 14:42:29,982 main.py:51] epoch 1766, training loss: 11830.37, average training loss: 10552.28, base loss: 14507.62
[INFO 2017-06-28 14:42:30,795 main.py:51] epoch 1767, training loss: 12263.96, average training loss: 10553.28, base loss: 14508.53
[INFO 2017-06-28 14:42:31,559 main.py:51] epoch 1768, training loss: 11091.88, average training loss: 10551.79, base loss: 14507.66
[INFO 2017-06-28 14:42:32,258 main.py:51] epoch 1769, training loss: 12449.78, average training loss: 10552.93, base loss: 14508.85
[INFO 2017-06-28 14:42:33,047 main.py:51] epoch 1770, training loss: 10766.79, average training loss: 10553.41, base loss: 14509.54
[INFO 2017-06-28 14:42:33,917 main.py:51] epoch 1771, training loss: 10694.87, average training loss: 10552.80, base loss: 14509.90
[INFO 2017-06-28 14:42:34,641 main.py:51] epoch 1772, training loss: 10879.87, average training loss: 10553.39, base loss: 14510.59
[INFO 2017-06-28 14:42:35,361 main.py:51] epoch 1773, training loss: 11150.95, average training loss: 10554.70, base loss: 14511.90
[INFO 2017-06-28 14:42:36,152 main.py:51] epoch 1774, training loss: 10702.55, average training loss: 10553.14, base loss: 14509.44
[INFO 2017-06-28 14:42:36,930 main.py:51] epoch 1775, training loss: 9625.24, average training loss: 10552.59, base loss: 14508.91
[INFO 2017-06-28 14:42:37,590 main.py:51] epoch 1776, training loss: 10297.19, average training loss: 10552.23, base loss: 14508.47
[INFO 2017-06-28 14:42:38,322 main.py:51] epoch 1777, training loss: 10341.77, average training loss: 10552.65, base loss: 14508.03
[INFO 2017-06-28 14:42:39,125 main.py:51] epoch 1778, training loss: 10601.06, average training loss: 10552.88, base loss: 14508.36
[INFO 2017-06-28 14:42:39,850 main.py:51] epoch 1779, training loss: 10176.77, average training loss: 10551.85, base loss: 14507.12
[INFO 2017-06-28 14:42:40,544 main.py:51] epoch 1780, training loss: 10002.29, average training loss: 10549.85, base loss: 14504.06
[INFO 2017-06-28 14:42:41,358 main.py:51] epoch 1781, training loss: 10853.14, average training loss: 10551.60, base loss: 14506.11
[INFO 2017-06-28 14:42:42,178 main.py:51] epoch 1782, training loss: 11834.68, average training loss: 10552.55, base loss: 14508.17
[INFO 2017-06-28 14:42:42,842 main.py:51] epoch 1783, training loss: 10746.77, average training loss: 10553.94, base loss: 14510.65
[INFO 2017-06-28 14:42:43,629 main.py:51] epoch 1784, training loss: 10776.06, average training loss: 10549.27, base loss: 14507.37
[INFO 2017-06-28 14:42:44,439 main.py:51] epoch 1785, training loss: 10084.52, average training loss: 10549.66, base loss: 14508.03
[INFO 2017-06-28 14:42:45,231 main.py:51] epoch 1786, training loss: 11351.09, average training loss: 10550.19, base loss: 14510.25
[INFO 2017-06-28 14:42:45,850 main.py:51] epoch 1787, training loss: 10464.55, average training loss: 10550.73, base loss: 14512.08
[INFO 2017-06-28 14:42:46,641 main.py:51] epoch 1788, training loss: 10545.33, average training loss: 10549.10, base loss: 14510.35
[INFO 2017-06-28 14:42:47,410 main.py:51] epoch 1789, training loss: 10047.07, average training loss: 10548.94, base loss: 14511.02
[INFO 2017-06-28 14:42:48,084 main.py:51] epoch 1790, training loss: 9341.11, average training loss: 10547.55, base loss: 14508.65
[INFO 2017-06-28 14:42:48,871 main.py:51] epoch 1791, training loss: 12748.09, average training loss: 10548.68, base loss: 14509.26
[INFO 2017-06-28 14:42:49,696 main.py:51] epoch 1792, training loss: 11271.31, average training loss: 10550.42, base loss: 14513.47
[INFO 2017-06-28 14:42:50,490 main.py:51] epoch 1793, training loss: 8483.41, average training loss: 10548.10, base loss: 14509.97
[INFO 2017-06-28 14:42:51,151 main.py:51] epoch 1794, training loss: 10019.95, average training loss: 10547.89, base loss: 14508.92
[INFO 2017-06-28 14:42:51,978 main.py:51] epoch 1795, training loss: 9389.05, average training loss: 10547.46, base loss: 14509.04
[INFO 2017-06-28 14:42:52,775 main.py:51] epoch 1796, training loss: 11316.53, average training loss: 10549.74, base loss: 14512.44
[INFO 2017-06-28 14:42:53,607 main.py:51] epoch 1797, training loss: 11164.49, average training loss: 10551.00, base loss: 14514.35
[INFO 2017-06-28 14:42:54,272 main.py:51] epoch 1798, training loss: 10214.20, average training loss: 10550.53, base loss: 14512.82
[INFO 2017-06-28 14:42:55,034 main.py:51] epoch 1799, training loss: 9143.55, average training loss: 10548.59, base loss: 14510.56
[INFO 2017-06-28 14:42:55,034 main.py:53] epoch 1799, testing
[INFO 2017-06-28 14:42:57,837 main.py:105] average testing loss: 12107.77, base loss: 16149.75
[INFO 2017-06-28 14:42:57,837 main.py:106] improve_loss: 4041.97, improve_percent: 0.25
[INFO 2017-06-28 14:42:57,838 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:42:58,684 main.py:51] epoch 1800, training loss: 10239.10, average training loss: 10547.87, base loss: 14510.18
[INFO 2017-06-28 14:42:59,420 main.py:51] epoch 1801, training loss: 12934.10, average training loss: 10550.49, base loss: 14512.52
[INFO 2017-06-28 14:43:00,103 main.py:51] epoch 1802, training loss: 10510.20, average training loss: 10551.10, base loss: 14513.86
[INFO 2017-06-28 14:43:00,914 main.py:51] epoch 1803, training loss: 10035.52, average training loss: 10551.54, base loss: 14515.11
[INFO 2017-06-28 14:43:01,765 main.py:51] epoch 1804, training loss: 10601.61, average training loss: 10551.15, base loss: 14515.29
[INFO 2017-06-28 14:43:02,435 main.py:51] epoch 1805, training loss: 11641.51, average training loss: 10551.87, base loss: 14516.97
[INFO 2017-06-28 14:43:03,189 main.py:51] epoch 1806, training loss: 10408.99, average training loss: 10552.84, base loss: 14517.22
[INFO 2017-06-28 14:43:04,011 main.py:51] epoch 1807, training loss: 9081.84, average training loss: 10549.40, base loss: 14514.05
[INFO 2017-06-28 14:43:04,853 main.py:51] epoch 1808, training loss: 9919.32, average training loss: 10548.53, base loss: 14512.62
[INFO 2017-06-28 14:43:05,524 main.py:51] epoch 1809, training loss: 10116.18, average training loss: 10546.38, base loss: 14512.18
[INFO 2017-06-28 14:43:06,339 main.py:51] epoch 1810, training loss: 8478.56, average training loss: 10544.25, base loss: 14510.33
[INFO 2017-06-28 14:43:07,199 main.py:51] epoch 1811, training loss: 9421.03, average training loss: 10543.72, base loss: 14509.78
[INFO 2017-06-28 14:43:07,955 main.py:51] epoch 1812, training loss: 12392.69, average training loss: 10545.15, base loss: 14512.93
[INFO 2017-06-28 14:43:08,662 main.py:51] epoch 1813, training loss: 9651.02, average training loss: 10543.13, base loss: 14511.21
[INFO 2017-06-28 14:43:09,482 main.py:51] epoch 1814, training loss: 10153.09, average training loss: 10542.89, base loss: 14511.97
[INFO 2017-06-28 14:43:10,342 main.py:51] epoch 1815, training loss: 12003.22, average training loss: 10544.03, base loss: 14512.70
[INFO 2017-06-28 14:43:11,094 main.py:51] epoch 1816, training loss: 9596.52, average training loss: 10542.38, base loss: 14509.86
[INFO 2017-06-28 14:43:11,800 main.py:51] epoch 1817, training loss: 12179.63, average training loss: 10543.12, base loss: 14510.94
[INFO 2017-06-28 14:43:12,596 main.py:51] epoch 1818, training loss: 10119.79, average training loss: 10541.84, base loss: 14510.12
[INFO 2017-06-28 14:43:13,419 main.py:51] epoch 1819, training loss: 11193.43, average training loss: 10541.15, base loss: 14511.78
[INFO 2017-06-28 14:43:14,121 main.py:51] epoch 1820, training loss: 9654.18, average training loss: 10539.89, base loss: 14511.56
[INFO 2017-06-28 14:43:14,890 main.py:51] epoch 1821, training loss: 9315.36, average training loss: 10538.75, base loss: 14510.24
[INFO 2017-06-28 14:43:15,673 main.py:51] epoch 1822, training loss: 9668.52, average training loss: 10538.04, base loss: 14509.31
[INFO 2017-06-28 14:43:16,477 main.py:51] epoch 1823, training loss: 9554.56, average training loss: 10537.18, base loss: 14509.73
[INFO 2017-06-28 14:43:17,124 main.py:51] epoch 1824, training loss: 11096.66, average training loss: 10536.15, base loss: 14507.23
[INFO 2017-06-28 14:43:17,924 main.py:51] epoch 1825, training loss: 10703.74, average training loss: 10535.63, base loss: 14506.43
[INFO 2017-06-28 14:43:18,737 main.py:51] epoch 1826, training loss: 11877.28, average training loss: 10535.37, base loss: 14507.10
[INFO 2017-06-28 14:43:19,566 main.py:51] epoch 1827, training loss: 12093.99, average training loss: 10534.88, base loss: 14507.44
[INFO 2017-06-28 14:43:20,188 main.py:51] epoch 1828, training loss: 9303.96, average training loss: 10534.96, base loss: 14507.39
[INFO 2017-06-28 14:43:21,000 main.py:51] epoch 1829, training loss: 9700.27, average training loss: 10533.61, base loss: 14507.12
[INFO 2017-06-28 14:43:21,869 main.py:51] epoch 1830, training loss: 10533.70, average training loss: 10534.18, base loss: 14507.12
[INFO 2017-06-28 14:43:22,626 main.py:51] epoch 1831, training loss: 10672.94, average training loss: 10535.07, base loss: 14508.08
[INFO 2017-06-28 14:43:23,319 main.py:51] epoch 1832, training loss: 10308.19, average training loss: 10533.80, base loss: 14507.15
[INFO 2017-06-28 14:43:24,109 main.py:51] epoch 1833, training loss: 10100.02, average training loss: 10532.23, base loss: 14504.66
[INFO 2017-06-28 14:43:24,954 main.py:51] epoch 1834, training loss: 9558.01, average training loss: 10530.14, base loss: 14501.91
[INFO 2017-06-28 14:43:25,578 main.py:51] epoch 1835, training loss: 10611.46, average training loss: 10529.99, base loss: 14502.15
[INFO 2017-06-28 14:43:26,358 main.py:51] epoch 1836, training loss: 11674.47, average training loss: 10531.69, base loss: 14503.56
[INFO 2017-06-28 14:43:27,183 main.py:51] epoch 1837, training loss: 10335.79, average training loss: 10532.37, base loss: 14505.68
[INFO 2017-06-28 14:43:28,066 main.py:51] epoch 1838, training loss: 11093.38, average training loss: 10533.14, base loss: 14508.03
[INFO 2017-06-28 14:43:28,701 main.py:51] epoch 1839, training loss: 8349.69, average training loss: 10531.99, base loss: 14507.44
[INFO 2017-06-28 14:43:29,504 main.py:51] epoch 1840, training loss: 10978.97, average training loss: 10532.73, base loss: 14508.81
[INFO 2017-06-28 14:43:30,334 main.py:51] epoch 1841, training loss: 13254.64, average training loss: 10534.37, base loss: 14510.44
[INFO 2017-06-28 14:43:31,150 main.py:51] epoch 1842, training loss: 9302.18, average training loss: 10534.38, base loss: 14510.35
[INFO 2017-06-28 14:43:31,819 main.py:51] epoch 1843, training loss: 8540.34, average training loss: 10533.49, base loss: 14507.92
[INFO 2017-06-28 14:43:32,627 main.py:51] epoch 1844, training loss: 9509.25, average training loss: 10532.20, base loss: 14507.43
[INFO 2017-06-28 14:43:33,442 main.py:51] epoch 1845, training loss: 10572.53, average training loss: 10531.29, base loss: 14506.10
[INFO 2017-06-28 14:43:34,130 main.py:51] epoch 1846, training loss: 9577.16, average training loss: 10528.27, base loss: 14503.31
[INFO 2017-06-28 14:43:34,891 main.py:51] epoch 1847, training loss: 10538.70, average training loss: 10528.65, base loss: 14504.62
[INFO 2017-06-28 14:43:35,675 main.py:51] epoch 1848, training loss: 10633.48, average training loss: 10527.44, base loss: 14504.01
[INFO 2017-06-28 14:43:36,525 main.py:51] epoch 1849, training loss: 9076.64, average training loss: 10525.38, base loss: 14499.61
[INFO 2017-06-28 14:43:37,231 main.py:51] epoch 1850, training loss: 9494.53, average training loss: 10524.66, base loss: 14498.18
[INFO 2017-06-28 14:43:37,979 main.py:51] epoch 1851, training loss: 9984.40, average training loss: 10523.66, base loss: 14498.03
[INFO 2017-06-28 14:43:38,778 main.py:51] epoch 1852, training loss: 9920.43, average training loss: 10521.29, base loss: 14496.33
[INFO 2017-06-28 14:43:39,622 main.py:51] epoch 1853, training loss: 9717.88, average training loss: 10517.87, base loss: 14494.33
[INFO 2017-06-28 14:43:40,292 main.py:51] epoch 1854, training loss: 10727.28, average training loss: 10517.60, base loss: 14494.76
[INFO 2017-06-28 14:43:41,078 main.py:51] epoch 1855, training loss: 11029.68, average training loss: 10517.74, base loss: 14494.39
[INFO 2017-06-28 14:43:41,892 main.py:51] epoch 1856, training loss: 11506.43, average training loss: 10518.02, base loss: 14494.90
[INFO 2017-06-28 14:43:42,719 main.py:51] epoch 1857, training loss: 9894.87, average training loss: 10516.55, base loss: 14493.85
[INFO 2017-06-28 14:43:43,367 main.py:51] epoch 1858, training loss: 9355.78, average training loss: 10516.32, base loss: 14492.56
[INFO 2017-06-28 14:43:44,165 main.py:51] epoch 1859, training loss: 11030.72, average training loss: 10518.05, base loss: 14496.09
[INFO 2017-06-28 14:43:44,970 main.py:51] epoch 1860, training loss: 9343.57, average training loss: 10517.37, base loss: 14496.53
[INFO 2017-06-28 14:43:45,671 main.py:51] epoch 1861, training loss: 11723.97, average training loss: 10519.84, base loss: 14500.18
[INFO 2017-06-28 14:43:46,425 main.py:51] epoch 1862, training loss: 9590.25, average training loss: 10517.22, base loss: 14495.28
[INFO 2017-06-28 14:43:47,218 main.py:51] epoch 1863, training loss: 9872.38, average training loss: 10514.76, base loss: 14492.29
[INFO 2017-06-28 14:43:48,020 main.py:51] epoch 1864, training loss: 9812.14, average training loss: 10514.00, base loss: 14490.83
[INFO 2017-06-28 14:43:48,695 main.py:51] epoch 1865, training loss: 9973.20, average training loss: 10513.04, base loss: 14489.47
[INFO 2017-06-28 14:43:49,454 main.py:51] epoch 1866, training loss: 10892.92, average training loss: 10512.34, base loss: 14487.40
[INFO 2017-06-28 14:43:50,279 main.py:51] epoch 1867, training loss: 9831.13, average training loss: 10511.99, base loss: 14488.87
[INFO 2017-06-28 14:43:51,021 main.py:51] epoch 1868, training loss: 11641.51, average training loss: 10513.45, base loss: 14490.87
[INFO 2017-06-28 14:43:51,722 main.py:51] epoch 1869, training loss: 9484.25, average training loss: 10511.85, base loss: 14487.60
[INFO 2017-06-28 14:43:52,513 main.py:51] epoch 1870, training loss: 11092.08, average training loss: 10512.21, base loss: 14487.71
[INFO 2017-06-28 14:43:53,287 main.py:51] epoch 1871, training loss: 10852.86, average training loss: 10512.00, base loss: 14488.38
[INFO 2017-06-28 14:43:53,912 main.py:51] epoch 1872, training loss: 9477.84, average training loss: 10510.25, base loss: 14485.65
[INFO 2017-06-28 14:43:54,716 main.py:51] epoch 1873, training loss: 9773.53, average training loss: 10508.75, base loss: 14483.84
[INFO 2017-06-28 14:43:55,547 main.py:51] epoch 1874, training loss: 10513.68, average training loss: 10509.47, base loss: 14483.33
[INFO 2017-06-28 14:43:56,401 main.py:51] epoch 1875, training loss: 9992.85, average training loss: 10508.86, base loss: 14483.22
[INFO 2017-06-28 14:43:57,040 main.py:51] epoch 1876, training loss: 12307.78, average training loss: 10510.39, base loss: 14485.06
[INFO 2017-06-28 14:43:57,851 main.py:51] epoch 1877, training loss: 12185.44, average training loss: 10512.37, base loss: 14487.53
[INFO 2017-06-28 14:43:58,661 main.py:51] epoch 1878, training loss: 10310.72, average training loss: 10510.73, base loss: 14485.12
[INFO 2017-06-28 14:43:59,362 main.py:51] epoch 1879, training loss: 8984.43, average training loss: 10509.03, base loss: 14484.25
[INFO 2017-06-28 14:44:00,027 main.py:51] epoch 1880, training loss: 10989.82, average training loss: 10507.99, base loss: 14484.90
[INFO 2017-06-28 14:44:00,674 main.py:51] epoch 1881, training loss: 10107.03, average training loss: 10505.85, base loss: 14481.52
[INFO 2017-06-28 14:44:01,269 main.py:51] epoch 1882, training loss: 9200.19, average training loss: 10503.62, base loss: 14478.31
[INFO 2017-06-28 14:44:01,974 main.py:51] epoch 1883, training loss: 11955.23, average training loss: 10506.15, base loss: 14481.75
[INFO 2017-06-28 14:44:02,580 main.py:51] epoch 1884, training loss: 9744.17, average training loss: 10504.66, base loss: 14478.35
[INFO 2017-06-28 14:44:03,413 main.py:51] epoch 1885, training loss: 10400.26, average training loss: 10503.75, base loss: 14474.88
[INFO 2017-06-28 14:44:04,089 main.py:51] epoch 1886, training loss: 11543.66, average training loss: 10504.11, base loss: 14476.13
[INFO 2017-06-28 14:44:04,828 main.py:51] epoch 1887, training loss: 10318.06, average training loss: 10501.72, base loss: 14474.28
[INFO 2017-06-28 14:44:05,652 main.py:51] epoch 1888, training loss: 12014.57, average training loss: 10502.51, base loss: 14476.83
[INFO 2017-06-28 14:44:06,505 main.py:51] epoch 1889, training loss: 12014.17, average training loss: 10503.57, base loss: 14479.36
[INFO 2017-06-28 14:44:07,152 main.py:51] epoch 1890, training loss: 10241.82, average training loss: 10503.38, base loss: 14479.08
[INFO 2017-06-28 14:44:07,945 main.py:51] epoch 1891, training loss: 11690.04, average training loss: 10504.32, base loss: 14480.60
[INFO 2017-06-28 14:44:08,748 main.py:51] epoch 1892, training loss: 10977.95, average training loss: 10505.52, base loss: 14482.11
[INFO 2017-06-28 14:44:09,486 main.py:51] epoch 1893, training loss: 11292.03, average training loss: 10505.77, base loss: 14482.73
[INFO 2017-06-28 14:44:10,207 main.py:51] epoch 1894, training loss: 10105.64, average training loss: 10506.39, base loss: 14482.34
[INFO 2017-06-28 14:44:10,979 main.py:51] epoch 1895, training loss: 10991.93, average training loss: 10504.91, base loss: 14480.66
[INFO 2017-06-28 14:44:11,766 main.py:51] epoch 1896, training loss: 10565.89, average training loss: 10507.27, base loss: 14483.69
[INFO 2017-06-28 14:44:12,402 main.py:51] epoch 1897, training loss: 9658.10, average training loss: 10506.62, base loss: 14483.17
[INFO 2017-06-28 14:44:13,191 main.py:51] epoch 1898, training loss: 9851.01, average training loss: 10505.87, base loss: 14482.09
[INFO 2017-06-28 14:44:14,000 main.py:51] epoch 1899, training loss: 10414.65, average training loss: 10506.11, base loss: 14483.26
[INFO 2017-06-28 14:44:14,000 main.py:53] epoch 1899, testing
[INFO 2017-06-28 14:44:16,776 main.py:105] average testing loss: 11934.67, base loss: 15931.78
[INFO 2017-06-28 14:44:16,777 main.py:106] improve_loss: 3997.11, improve_percent: 0.25
[INFO 2017-06-28 14:44:16,777 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:44:17,427 main.py:51] epoch 1900, training loss: 9393.51, average training loss: 10504.89, base loss: 14482.89
[INFO 2017-06-28 14:44:18,229 main.py:51] epoch 1901, training loss: 11587.13, average training loss: 10504.80, base loss: 14484.53
[INFO 2017-06-28 14:44:19,060 main.py:51] epoch 1902, training loss: 11711.49, average training loss: 10506.36, base loss: 14485.09
[INFO 2017-06-28 14:44:19,877 main.py:51] epoch 1903, training loss: 10530.33, average training loss: 10507.23, base loss: 14486.91
[INFO 2017-06-28 14:44:20,549 main.py:51] epoch 1904, training loss: 10458.70, average training loss: 10507.69, base loss: 14487.86
[INFO 2017-06-28 14:44:21,331 main.py:51] epoch 1905, training loss: 10121.10, average training loss: 10509.56, base loss: 14490.29
[INFO 2017-06-28 14:44:22,178 main.py:51] epoch 1906, training loss: 8748.99, average training loss: 10507.04, base loss: 14488.30
[INFO 2017-06-28 14:44:22,912 main.py:51] epoch 1907, training loss: 9849.43, average training loss: 10505.69, base loss: 14487.42
[INFO 2017-06-28 14:44:23,588 main.py:51] epoch 1908, training loss: 11309.03, average training loss: 10508.12, base loss: 14491.30
[INFO 2017-06-28 14:44:24,412 main.py:51] epoch 1909, training loss: 8643.09, average training loss: 10507.29, base loss: 14490.46
[INFO 2017-06-28 14:44:25,166 main.py:51] epoch 1910, training loss: 10997.48, average training loss: 10508.89, base loss: 14492.98
[INFO 2017-06-28 14:44:25,877 main.py:51] epoch 1911, training loss: 10733.26, average training loss: 10509.80, base loss: 14494.72
[INFO 2017-06-28 14:44:26,649 main.py:51] epoch 1912, training loss: 10575.28, average training loss: 10509.92, base loss: 14495.50
[INFO 2017-06-28 14:44:27,436 main.py:51] epoch 1913, training loss: 8507.74, average training loss: 10505.99, base loss: 14489.79
[INFO 2017-06-28 14:44:28,083 main.py:51] epoch 1914, training loss: 11074.04, average training loss: 10507.48, base loss: 14492.98
[INFO 2017-06-28 14:44:28,859 main.py:51] epoch 1915, training loss: 10105.25, average training loss: 10507.79, base loss: 14492.10
[INFO 2017-06-28 14:44:29,678 main.py:51] epoch 1916, training loss: 10375.98, average training loss: 10507.15, base loss: 14492.21
[INFO 2017-06-28 14:44:30,424 main.py:51] epoch 1917, training loss: 11018.85, average training loss: 10508.33, base loss: 14495.19
[INFO 2017-06-28 14:44:31,097 main.py:51] epoch 1918, training loss: 9436.56, average training loss: 10507.61, base loss: 14494.74
[INFO 2017-06-28 14:44:31,909 main.py:51] epoch 1919, training loss: 11165.41, average training loss: 10508.44, base loss: 14497.41
[INFO 2017-06-28 14:44:32,754 main.py:51] epoch 1920, training loss: 10289.70, average training loss: 10509.78, base loss: 14499.85
[INFO 2017-06-28 14:44:33,412 main.py:51] epoch 1921, training loss: 9997.55, average training loss: 10506.54, base loss: 14496.53
[INFO 2017-06-28 14:44:34,220 main.py:51] epoch 1922, training loss: 10781.88, average training loss: 10506.58, base loss: 14498.20
[INFO 2017-06-28 14:44:35,057 main.py:51] epoch 1923, training loss: 10894.23, average training loss: 10507.17, base loss: 14498.81
[INFO 2017-06-28 14:44:35,913 main.py:51] epoch 1924, training loss: 10347.23, average training loss: 10507.23, base loss: 14500.23
[INFO 2017-06-28 14:44:36,560 main.py:51] epoch 1925, training loss: 11638.92, average training loss: 10509.09, base loss: 14503.91
[INFO 2017-06-28 14:44:37,372 main.py:51] epoch 1926, training loss: 10260.28, average training loss: 10508.33, base loss: 14505.36
[INFO 2017-06-28 14:44:38,203 main.py:51] epoch 1927, training loss: 9643.60, average training loss: 10507.96, base loss: 14505.26
[INFO 2017-06-28 14:44:38,948 main.py:51] epoch 1928, training loss: 8862.38, average training loss: 10503.92, base loss: 14501.28
[INFO 2017-06-28 14:44:39,645 main.py:51] epoch 1929, training loss: 10153.52, average training loss: 10502.60, base loss: 14500.37
[INFO 2017-06-28 14:44:40,432 main.py:51] epoch 1930, training loss: 11940.15, average training loss: 10503.70, base loss: 14502.21
[INFO 2017-06-28 14:44:41,214 main.py:51] epoch 1931, training loss: 8861.75, average training loss: 10502.20, base loss: 14500.01
[INFO 2017-06-28 14:44:41,888 main.py:51] epoch 1932, training loss: 9229.04, average training loss: 10499.69, base loss: 14497.56
[INFO 2017-06-28 14:44:42,659 main.py:51] epoch 1933, training loss: 9977.18, average training loss: 10498.85, base loss: 14497.17
[INFO 2017-06-28 14:44:43,453 main.py:51] epoch 1934, training loss: 9972.19, average training loss: 10497.66, base loss: 14495.00
[INFO 2017-06-28 14:44:44,110 main.py:51] epoch 1935, training loss: 9504.22, average training loss: 10496.24, base loss: 14493.25
[INFO 2017-06-28 14:44:44,873 main.py:51] epoch 1936, training loss: 11443.15, average training loss: 10497.81, base loss: 14494.78
[INFO 2017-06-28 14:44:45,679 main.py:51] epoch 1937, training loss: 10632.26, average training loss: 10497.81, base loss: 14494.45
[INFO 2017-06-28 14:44:46,463 main.py:51] epoch 1938, training loss: 10110.85, average training loss: 10496.95, base loss: 14493.49
[INFO 2017-06-28 14:44:47,105 main.py:51] epoch 1939, training loss: 10394.11, average training loss: 10496.91, base loss: 14494.71
[INFO 2017-06-28 14:44:47,875 main.py:51] epoch 1940, training loss: 9362.68, average training loss: 10495.51, base loss: 14492.92
[INFO 2017-06-28 14:44:48,676 main.py:51] epoch 1941, training loss: 10420.96, average training loss: 10495.63, base loss: 14492.97
[INFO 2017-06-28 14:44:49,362 main.py:51] epoch 1942, training loss: 10583.88, average training loss: 10494.45, base loss: 14492.41
[INFO 2017-06-28 14:44:50,119 main.py:51] epoch 1943, training loss: 10068.25, average training loss: 10493.29, base loss: 14491.35
[INFO 2017-06-28 14:44:50,930 main.py:51] epoch 1944, training loss: 10577.68, average training loss: 10493.36, base loss: 14491.31
[INFO 2017-06-28 14:44:51,702 main.py:51] epoch 1945, training loss: 11438.20, average training loss: 10494.27, base loss: 14494.51
[INFO 2017-06-28 14:44:52,362 main.py:51] epoch 1946, training loss: 10267.60, average training loss: 10492.63, base loss: 14492.67
[INFO 2017-06-28 14:44:53,134 main.py:51] epoch 1947, training loss: 9599.73, average training loss: 10491.63, base loss: 14491.88
[INFO 2017-06-28 14:44:53,963 main.py:51] epoch 1948, training loss: 9639.58, average training loss: 10490.25, base loss: 14488.66
[INFO 2017-06-28 14:44:54,761 main.py:51] epoch 1949, training loss: 9795.09, average training loss: 10487.10, base loss: 14486.13
[INFO 2017-06-28 14:44:55,463 main.py:51] epoch 1950, training loss: 10267.55, average training loss: 10487.42, base loss: 14486.80
[INFO 2017-06-28 14:44:56,237 main.py:51] epoch 1951, training loss: 9608.97, average training loss: 10486.45, base loss: 14486.79
[INFO 2017-06-28 14:44:57,081 main.py:51] epoch 1952, training loss: 9477.15, average training loss: 10484.50, base loss: 14484.92
[INFO 2017-06-28 14:44:57,849 main.py:51] epoch 1953, training loss: 9650.99, average training loss: 10483.65, base loss: 14483.43
[INFO 2017-06-28 14:44:58,552 main.py:51] epoch 1954, training loss: 11305.32, average training loss: 10483.84, base loss: 14485.61
[INFO 2017-06-28 14:44:59,339 main.py:51] epoch 1955, training loss: 11339.05, average training loss: 10485.22, base loss: 14486.93
[INFO 2017-06-28 14:45:00,200 main.py:51] epoch 1956, training loss: 9408.40, average training loss: 10484.88, base loss: 14485.80
[INFO 2017-06-28 14:45:00,876 main.py:51] epoch 1957, training loss: 10338.92, average training loss: 10484.87, base loss: 14486.46
[INFO 2017-06-28 14:45:01,625 main.py:51] epoch 1958, training loss: 12660.75, average training loss: 10487.17, base loss: 14487.73
[INFO 2017-06-28 14:45:02,465 main.py:51] epoch 1959, training loss: 10072.55, average training loss: 10485.98, base loss: 14487.40
[INFO 2017-06-28 14:45:03,254 main.py:51] epoch 1960, training loss: 10158.51, average training loss: 10484.97, base loss: 14485.07
[INFO 2017-06-28 14:45:03,951 main.py:51] epoch 1961, training loss: 11923.39, average training loss: 10486.37, base loss: 14487.95
[INFO 2017-06-28 14:45:04,791 main.py:51] epoch 1962, training loss: 9319.84, average training loss: 10485.99, base loss: 14488.43
[INFO 2017-06-28 14:45:05,617 main.py:51] epoch 1963, training loss: 11601.98, average training loss: 10488.32, base loss: 14493.07
[INFO 2017-06-28 14:45:06,376 main.py:51] epoch 1964, training loss: 12074.55, average training loss: 10489.75, base loss: 14496.52
[INFO 2017-06-28 14:45:07,085 main.py:51] epoch 1965, training loss: 10117.43, average training loss: 10490.76, base loss: 14497.80
[INFO 2017-06-28 14:45:07,887 main.py:51] epoch 1966, training loss: 9696.88, average training loss: 10490.51, base loss: 14499.84
[INFO 2017-06-28 14:45:08,694 main.py:51] epoch 1967, training loss: 10338.65, average training loss: 10490.36, base loss: 14500.60
[INFO 2017-06-28 14:45:09,492 main.py:51] epoch 1968, training loss: 9568.99, average training loss: 10488.77, base loss: 14498.32
[INFO 2017-06-28 14:45:10,151 main.py:51] epoch 1969, training loss: 10817.09, average training loss: 10488.46, base loss: 14498.20
[INFO 2017-06-28 14:45:10,962 main.py:51] epoch 1970, training loss: 10165.73, average training loss: 10488.35, base loss: 14496.42
[INFO 2017-06-28 14:45:11,798 main.py:51] epoch 1971, training loss: 10431.17, average training loss: 10488.32, base loss: 14496.38
[INFO 2017-06-28 14:45:12,601 main.py:51] epoch 1972, training loss: 11730.74, average training loss: 10489.44, base loss: 14497.91
[INFO 2017-06-28 14:45:13,253 main.py:51] epoch 1973, training loss: 8686.59, average training loss: 10485.70, base loss: 14492.74
[INFO 2017-06-28 14:45:14,017 main.py:51] epoch 1974, training loss: 9457.52, average training loss: 10484.21, base loss: 14492.50
[INFO 2017-06-28 14:45:14,808 main.py:51] epoch 1975, training loss: 10506.55, average training loss: 10485.36, base loss: 14495.45
[INFO 2017-06-28 14:45:15,451 main.py:51] epoch 1976, training loss: 10194.55, average training loss: 10485.98, base loss: 14496.53
[INFO 2017-06-28 14:45:16,203 main.py:51] epoch 1977, training loss: 8668.48, average training loss: 10483.38, base loss: 14494.42
[INFO 2017-06-28 14:45:17,065 main.py:51] epoch 1978, training loss: 9198.72, average training loss: 10481.47, base loss: 14491.66
[INFO 2017-06-28 14:45:17,805 main.py:51] epoch 1979, training loss: 10206.34, average training loss: 10479.32, base loss: 14490.68
[INFO 2017-06-28 14:45:18,508 main.py:51] epoch 1980, training loss: 10906.16, average training loss: 10479.48, base loss: 14492.27
[INFO 2017-06-28 14:45:19,359 main.py:51] epoch 1981, training loss: 8767.90, average training loss: 10477.61, base loss: 14491.00
[INFO 2017-06-28 14:45:20,201 main.py:51] epoch 1982, training loss: 9072.07, average training loss: 10475.79, base loss: 14489.60
[INFO 2017-06-28 14:45:20,954 main.py:51] epoch 1983, training loss: 11390.26, average training loss: 10476.18, base loss: 14491.23
[INFO 2017-06-28 14:45:21,634 main.py:51] epoch 1984, training loss: 9303.09, average training loss: 10475.85, base loss: 14490.23
[INFO 2017-06-28 14:45:22,445 main.py:51] epoch 1985, training loss: 12154.17, average training loss: 10476.48, base loss: 14492.31
[INFO 2017-06-28 14:45:23,259 main.py:51] epoch 1986, training loss: 10945.66, average training loss: 10478.27, base loss: 14495.80
[INFO 2017-06-28 14:45:23,909 main.py:51] epoch 1987, training loss: 10740.02, average training loss: 10476.76, base loss: 14494.27
[INFO 2017-06-28 14:45:24,675 main.py:51] epoch 1988, training loss: 10349.21, average training loss: 10477.00, base loss: 14495.21
[INFO 2017-06-28 14:45:25,515 main.py:51] epoch 1989, training loss: 11778.46, average training loss: 10474.03, base loss: 14493.65
[INFO 2017-06-28 14:45:26,345 main.py:51] epoch 1990, training loss: 10075.70, average training loss: 10473.83, base loss: 14494.76
[INFO 2017-06-28 14:45:27,036 main.py:51] epoch 1991, training loss: 9212.76, average training loss: 10471.85, base loss: 14493.46
[INFO 2017-06-28 14:45:27,809 main.py:51] epoch 1992, training loss: 10063.56, average training loss: 10471.47, base loss: 14493.75
[INFO 2017-06-28 14:45:28,662 main.py:51] epoch 1993, training loss: 9657.96, average training loss: 10471.73, base loss: 14495.83
[INFO 2017-06-28 14:45:29,478 main.py:51] epoch 1994, training loss: 9282.72, average training loss: 10469.83, base loss: 14493.54
[INFO 2017-06-28 14:45:30,125 main.py:51] epoch 1995, training loss: 10080.34, average training loss: 10469.04, base loss: 14493.74
[INFO 2017-06-28 14:45:30,885 main.py:51] epoch 1996, training loss: 8820.43, average training loss: 10466.46, base loss: 14489.84
[INFO 2017-06-28 14:45:31,694 main.py:51] epoch 1997, training loss: 10148.57, average training loss: 10465.41, base loss: 14488.50
[INFO 2017-06-28 14:45:32,312 main.py:51] epoch 1998, training loss: 9077.56, average training loss: 10465.79, base loss: 14488.73
[INFO 2017-06-28 14:45:33,127 main.py:51] epoch 1999, training loss: 9487.13, average training loss: 10466.15, base loss: 14489.63
[INFO 2017-06-28 14:45:33,127 main.py:53] epoch 1999, testing
[INFO 2017-06-28 14:45:36,019 main.py:105] average testing loss: 10685.99, base loss: 14198.07
[INFO 2017-06-28 14:45:36,019 main.py:106] improve_loss: 3512.09, improve_percent: 0.25
[INFO 2017-06-28 14:45:36,020 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:45:36,837 main.py:51] epoch 2000, training loss: 10344.67, average training loss: 10466.39, base loss: 14489.10
[INFO 2017-06-28 14:45:37,644 main.py:51] epoch 2001, training loss: 10054.19, average training loss: 10467.53, base loss: 14490.16
[INFO 2017-06-28 14:45:38,333 main.py:51] epoch 2002, training loss: 10368.30, average training loss: 10466.76, base loss: 14489.91
[INFO 2017-06-28 14:45:39,167 main.py:51] epoch 2003, training loss: 9695.39, average training loss: 10465.28, base loss: 14488.28
[INFO 2017-06-28 14:45:39,988 main.py:51] epoch 2004, training loss: 10159.90, average training loss: 10465.05, base loss: 14488.69
[INFO 2017-06-28 14:45:40,808 main.py:51] epoch 2005, training loss: 11501.88, average training loss: 10465.76, base loss: 14488.79
[INFO 2017-06-28 14:45:41,443 main.py:51] epoch 2006, training loss: 10640.26, average training loss: 10466.02, base loss: 14490.06
[INFO 2017-06-28 14:45:42,247 main.py:51] epoch 2007, training loss: 8247.06, average training loss: 10464.48, base loss: 14488.45
[INFO 2017-06-28 14:45:43,092 main.py:51] epoch 2008, training loss: 10558.66, average training loss: 10462.83, base loss: 14487.97
[INFO 2017-06-28 14:45:43,799 main.py:51] epoch 2009, training loss: 11641.22, average training loss: 10463.61, base loss: 14488.09
[INFO 2017-06-28 14:45:44,491 main.py:51] epoch 2010, training loss: 9257.66, average training loss: 10462.96, base loss: 14487.85
[INFO 2017-06-28 14:45:45,304 main.py:51] epoch 2011, training loss: 8951.21, average training loss: 10460.63, base loss: 14483.62
[INFO 2017-06-28 14:45:46,164 main.py:51] epoch 2012, training loss: 9931.54, average training loss: 10461.40, base loss: 14483.56
[INFO 2017-06-28 14:45:46,760 main.py:51] epoch 2013, training loss: 9529.48, average training loss: 10461.01, base loss: 14483.53
[INFO 2017-06-28 14:45:47,532 main.py:51] epoch 2014, training loss: 9610.77, average training loss: 10460.33, base loss: 14483.14
[INFO 2017-06-28 14:45:48,174 main.py:51] epoch 2015, training loss: 9832.60, average training loss: 10460.86, base loss: 14483.28
[INFO 2017-06-28 14:45:48,886 main.py:51] epoch 2016, training loss: 9990.42, average training loss: 10459.24, base loss: 14481.32
[INFO 2017-06-28 14:45:49,580 main.py:51] epoch 2017, training loss: 10849.35, average training loss: 10460.81, base loss: 14483.90
[INFO 2017-06-28 14:45:50,268 main.py:51] epoch 2018, training loss: 11272.14, average training loss: 10461.09, base loss: 14486.47
[INFO 2017-06-28 14:45:50,918 main.py:51] epoch 2019, training loss: 12339.56, average training loss: 10461.11, base loss: 14485.50
[INFO 2017-06-28 14:45:51,562 main.py:51] epoch 2020, training loss: 9261.02, average training loss: 10460.21, base loss: 14483.74
[INFO 2017-06-28 14:45:52,350 main.py:51] epoch 2021, training loss: 11008.15, average training loss: 10460.27, base loss: 14484.42
[INFO 2017-06-28 14:45:53,179 main.py:51] epoch 2022, training loss: 10385.21, average training loss: 10459.80, base loss: 14483.97
[INFO 2017-06-28 14:45:53,896 main.py:51] epoch 2023, training loss: 12098.95, average training loss: 10459.27, base loss: 14483.63
[INFO 2017-06-28 14:45:54,604 main.py:51] epoch 2024, training loss: 12423.95, average training loss: 10461.81, base loss: 14486.02
[INFO 2017-06-28 14:45:55,420 main.py:51] epoch 2025, training loss: 9215.33, average training loss: 10459.70, base loss: 14484.72
[INFO 2017-06-28 14:45:56,265 main.py:51] epoch 2026, training loss: 9704.80, average training loss: 10458.63, base loss: 14483.28
[INFO 2017-06-28 14:45:56,945 main.py:51] epoch 2027, training loss: 10419.98, average training loss: 10458.87, base loss: 14482.76
[INFO 2017-06-28 14:45:57,691 main.py:51] epoch 2028, training loss: 10438.51, average training loss: 10459.81, base loss: 14483.98
[INFO 2017-06-28 14:45:58,517 main.py:51] epoch 2029, training loss: 9550.12, average training loss: 10457.62, base loss: 14481.25
[INFO 2017-06-28 14:45:59,336 main.py:51] epoch 2030, training loss: 9697.68, average training loss: 10457.24, base loss: 14481.13
[INFO 2017-06-28 14:46:00,002 main.py:51] epoch 2031, training loss: 10455.94, average training loss: 10456.53, base loss: 14481.57
[INFO 2017-06-28 14:46:00,838 main.py:51] epoch 2032, training loss: 11766.26, average training loss: 10457.49, base loss: 14483.54
[INFO 2017-06-28 14:46:01,678 main.py:51] epoch 2033, training loss: 9218.89, average training loss: 10456.59, base loss: 14482.97
[INFO 2017-06-28 14:46:02,392 main.py:51] epoch 2034, training loss: 9395.67, average training loss: 10454.79, base loss: 14481.53
[INFO 2017-06-28 14:46:03,099 main.py:51] epoch 2035, training loss: 9933.58, average training loss: 10452.95, base loss: 14481.94
[INFO 2017-06-28 14:46:03,891 main.py:51] epoch 2036, training loss: 9232.97, average training loss: 10450.90, base loss: 14479.49
[INFO 2017-06-28 14:46:04,738 main.py:51] epoch 2037, training loss: 9591.39, average training loss: 10450.38, base loss: 14477.46
[INFO 2017-06-28 14:46:05,386 main.py:51] epoch 2038, training loss: 9194.78, average training loss: 10449.23, base loss: 14475.63
[INFO 2017-06-28 14:46:06,171 main.py:51] epoch 2039, training loss: 10633.71, average training loss: 10447.28, base loss: 14474.66
[INFO 2017-06-28 14:46:06,972 main.py:51] epoch 2040, training loss: 7907.59, average training loss: 10443.85, base loss: 14470.17
[INFO 2017-06-28 14:46:07,737 main.py:51] epoch 2041, training loss: 9153.04, average training loss: 10441.91, base loss: 14467.59
[INFO 2017-06-28 14:46:08,456 main.py:51] epoch 2042, training loss: 9152.67, average training loss: 10440.00, base loss: 14465.37
[INFO 2017-06-28 14:46:09,247 main.py:51] epoch 2043, training loss: 10494.48, average training loss: 10438.61, base loss: 14462.55
[INFO 2017-06-28 14:46:10,087 main.py:51] epoch 2044, training loss: 10866.80, average training loss: 10440.53, base loss: 14465.70
[INFO 2017-06-28 14:46:10,788 main.py:51] epoch 2045, training loss: 11270.05, average training loss: 10442.16, base loss: 14467.30
[INFO 2017-06-28 14:46:11,522 main.py:51] epoch 2046, training loss: 10116.51, average training loss: 10442.15, base loss: 14468.75
[INFO 2017-06-28 14:46:12,345 main.py:51] epoch 2047, training loss: 9443.88, average training loss: 10438.86, base loss: 14465.11
[INFO 2017-06-28 14:46:13,137 main.py:51] epoch 2048, training loss: 10975.62, average training loss: 10438.87, base loss: 14467.22
[INFO 2017-06-28 14:46:13,831 main.py:51] epoch 2049, training loss: 9201.81, average training loss: 10438.90, base loss: 14468.20
[INFO 2017-06-28 14:46:14,613 main.py:51] epoch 2050, training loss: 12501.89, average training loss: 10441.50, base loss: 14472.13
[INFO 2017-06-28 14:46:15,473 main.py:51] epoch 2051, training loss: 11825.50, average training loss: 10442.03, base loss: 14473.44
[INFO 2017-06-28 14:46:16,154 main.py:51] epoch 2052, training loss: 10761.43, average training loss: 10442.15, base loss: 14473.32
[INFO 2017-06-28 14:46:16,889 main.py:51] epoch 2053, training loss: 10382.55, average training loss: 10442.70, base loss: 14473.95
[INFO 2017-06-28 14:46:17,715 main.py:51] epoch 2054, training loss: 9557.34, average training loss: 10441.45, base loss: 14472.62
[INFO 2017-06-28 14:46:18,512 main.py:51] epoch 2055, training loss: 10005.99, average training loss: 10440.80, base loss: 14472.37
[INFO 2017-06-28 14:46:19,175 main.py:51] epoch 2056, training loss: 10506.60, average training loss: 10440.47, base loss: 14472.64
[INFO 2017-06-28 14:46:19,979 main.py:51] epoch 2057, training loss: 9537.62, average training loss: 10440.22, base loss: 14472.87
[INFO 2017-06-28 14:46:20,813 main.py:51] epoch 2058, training loss: 9695.56, average training loss: 10440.57, base loss: 14471.94
[INFO 2017-06-28 14:46:21,550 main.py:51] epoch 2059, training loss: 10417.79, average training loss: 10439.02, base loss: 14470.69
[INFO 2017-06-28 14:46:22,278 main.py:51] epoch 2060, training loss: 9936.05, average training loss: 10439.36, base loss: 14469.96
[INFO 2017-06-28 14:46:23,078 main.py:51] epoch 2061, training loss: 10106.70, average training loss: 10438.44, base loss: 14471.74
[INFO 2017-06-28 14:46:23,940 main.py:51] epoch 2062, training loss: 10665.67, average training loss: 10439.78, base loss: 14473.66
[INFO 2017-06-28 14:46:24,633 main.py:51] epoch 2063, training loss: 13193.30, average training loss: 10443.26, base loss: 14479.10
[INFO 2017-06-28 14:46:25,372 main.py:51] epoch 2064, training loss: 11894.21, average training loss: 10445.31, base loss: 14483.65
[INFO 2017-06-28 14:46:26,196 main.py:51] epoch 2065, training loss: 11841.49, average training loss: 10445.93, base loss: 14485.99
[INFO 2017-06-28 14:46:26,961 main.py:51] epoch 2066, training loss: 10862.69, average training loss: 10447.16, base loss: 14488.05
[INFO 2017-06-28 14:46:27,637 main.py:51] epoch 2067, training loss: 9575.22, average training loss: 10447.15, base loss: 14488.37
[INFO 2017-06-28 14:46:28,451 main.py:51] epoch 2068, training loss: 9919.32, average training loss: 10447.08, base loss: 14488.77
[INFO 2017-06-28 14:46:29,248 main.py:51] epoch 2069, training loss: 10226.02, average training loss: 10446.37, base loss: 14489.22
[INFO 2017-06-28 14:46:29,880 main.py:51] epoch 2070, training loss: 9403.96, average training loss: 10444.31, base loss: 14486.35
[INFO 2017-06-28 14:46:30,631 main.py:51] epoch 2071, training loss: 12093.46, average training loss: 10446.68, base loss: 14488.43
[INFO 2017-06-28 14:46:31,453 main.py:51] epoch 2072, training loss: 8435.18, average training loss: 10445.75, base loss: 14486.89
[INFO 2017-06-28 14:46:32,119 main.py:51] epoch 2073, training loss: 10004.08, average training loss: 10444.74, base loss: 14485.68
[INFO 2017-06-28 14:46:32,883 main.py:51] epoch 2074, training loss: 10420.79, average training loss: 10443.83, base loss: 14484.65
[INFO 2017-06-28 14:46:33,652 main.py:51] epoch 2075, training loss: 10263.47, average training loss: 10442.68, base loss: 14482.88
[INFO 2017-06-28 14:46:34,454 main.py:51] epoch 2076, training loss: 9291.43, average training loss: 10442.65, base loss: 14482.24
[INFO 2017-06-28 14:46:35,119 main.py:51] epoch 2077, training loss: 10552.00, average training loss: 10444.11, base loss: 14485.05
[INFO 2017-06-28 14:46:35,886 main.py:51] epoch 2078, training loss: 11731.52, average training loss: 10445.02, base loss: 14486.56
[INFO 2017-06-28 14:46:36,729 main.py:51] epoch 2079, training loss: 11137.71, average training loss: 10444.57, base loss: 14486.95
[INFO 2017-06-28 14:46:37,419 main.py:51] epoch 2080, training loss: 10170.41, average training loss: 10443.26, base loss: 14484.80
[INFO 2017-06-28 14:46:38,142 main.py:51] epoch 2081, training loss: 10836.03, average training loss: 10445.74, base loss: 14488.48
[INFO 2017-06-28 14:46:38,939 main.py:51] epoch 2082, training loss: 10463.40, average training loss: 10444.80, base loss: 14488.34
[INFO 2017-06-28 14:46:39,738 main.py:51] epoch 2083, training loss: 10061.37, average training loss: 10444.69, base loss: 14489.19
[INFO 2017-06-28 14:46:40,416 main.py:51] epoch 2084, training loss: 9538.73, average training loss: 10441.97, base loss: 14485.64
[INFO 2017-06-28 14:46:41,152 main.py:51] epoch 2085, training loss: 11213.78, average training loss: 10441.41, base loss: 14484.99
[INFO 2017-06-28 14:46:41,981 main.py:51] epoch 2086, training loss: 11330.54, average training loss: 10442.19, base loss: 14488.16
[INFO 2017-06-28 14:46:42,677 main.py:51] epoch 2087, training loss: 9843.23, average training loss: 10442.29, base loss: 14487.30
[INFO 2017-06-28 14:46:43,432 main.py:51] epoch 2088, training loss: 9871.60, average training loss: 10442.50, base loss: 14485.85
[INFO 2017-06-28 14:46:44,232 main.py:51] epoch 2089, training loss: 10415.02, average training loss: 10442.27, base loss: 14486.65
[INFO 2017-06-28 14:46:45,023 main.py:51] epoch 2090, training loss: 8900.41, average training loss: 10440.21, base loss: 14481.84
[INFO 2017-06-28 14:46:45,648 main.py:51] epoch 2091, training loss: 8186.08, average training loss: 10437.72, base loss: 14479.62
[INFO 2017-06-28 14:46:46,419 main.py:51] epoch 2092, training loss: 10884.01, average training loss: 10438.80, base loss: 14482.25
[INFO 2017-06-28 14:46:47,278 main.py:51] epoch 2093, training loss: 10160.29, average training loss: 10437.78, base loss: 14482.02
[INFO 2017-06-28 14:46:47,991 main.py:51] epoch 2094, training loss: 9382.91, average training loss: 10437.60, base loss: 14482.40
[INFO 2017-06-28 14:46:48,697 main.py:51] epoch 2095, training loss: 9529.70, average training loss: 10434.09, base loss: 14477.76
[INFO 2017-06-28 14:46:49,516 main.py:51] epoch 2096, training loss: 9625.94, average training loss: 10432.71, base loss: 14475.87
[INFO 2017-06-28 14:46:50,338 main.py:51] epoch 2097, training loss: 10209.11, average training loss: 10432.80, base loss: 14476.95
[INFO 2017-06-28 14:46:51,020 main.py:51] epoch 2098, training loss: 11061.94, average training loss: 10433.09, base loss: 14477.54
[INFO 2017-06-28 14:46:51,769 main.py:51] epoch 2099, training loss: 9562.49, average training loss: 10431.76, base loss: 14476.16
[INFO 2017-06-28 14:46:51,769 main.py:53] epoch 2099, testing
[INFO 2017-06-28 14:46:54,646 main.py:105] average testing loss: 11752.28, base loss: 15498.58
[INFO 2017-06-28 14:46:54,646 main.py:106] improve_loss: 3746.30, improve_percent: 0.24
[INFO 2017-06-28 14:46:54,647 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:46:55,505 main.py:51] epoch 2100, training loss: 9751.75, average training loss: 10431.97, base loss: 14477.53
[INFO 2017-06-28 14:46:56,249 main.py:51] epoch 2101, training loss: 10737.10, average training loss: 10432.01, base loss: 14478.76
[INFO 2017-06-28 14:46:56,957 main.py:51] epoch 2102, training loss: 9832.39, average training loss: 10430.43, base loss: 14478.69
[INFO 2017-06-28 14:46:57,758 main.py:51] epoch 2103, training loss: 9986.39, average training loss: 10429.46, base loss: 14477.87
[INFO 2017-06-28 14:46:58,583 main.py:51] epoch 2104, training loss: 8778.88, average training loss: 10430.18, base loss: 14478.67
[INFO 2017-06-28 14:46:59,226 main.py:51] epoch 2105, training loss: 11689.09, average training loss: 10430.04, base loss: 14479.36
[INFO 2017-06-28 14:47:00,011 main.py:51] epoch 2106, training loss: 9606.58, average training loss: 10427.29, base loss: 14476.78
[INFO 2017-06-28 14:47:00,831 main.py:51] epoch 2107, training loss: 10699.07, average training loss: 10426.73, base loss: 14477.51
[INFO 2017-06-28 14:47:01,704 main.py:51] epoch 2108, training loss: 10043.56, average training loss: 10424.37, base loss: 14474.16
[INFO 2017-06-28 14:47:02,352 main.py:51] epoch 2109, training loss: 10126.33, average training loss: 10423.79, base loss: 14473.24
[INFO 2017-06-28 14:47:03,139 main.py:51] epoch 2110, training loss: 9841.13, average training loss: 10424.57, base loss: 14474.87
[INFO 2017-06-28 14:47:03,966 main.py:51] epoch 2111, training loss: 10837.96, average training loss: 10425.24, base loss: 14477.39
[INFO 2017-06-28 14:47:04,799 main.py:51] epoch 2112, training loss: 9640.30, average training loss: 10422.95, base loss: 14474.24
[INFO 2017-06-28 14:47:05,424 main.py:51] epoch 2113, training loss: 10275.93, average training loss: 10424.19, base loss: 14476.47
[INFO 2017-06-28 14:47:06,219 main.py:51] epoch 2114, training loss: 9892.94, average training loss: 10422.34, base loss: 14476.44
[INFO 2017-06-28 14:47:07,066 main.py:51] epoch 2115, training loss: 8851.97, average training loss: 10422.18, base loss: 14475.82
[INFO 2017-06-28 14:47:07,844 main.py:51] epoch 2116, training loss: 9689.38, average training loss: 10420.24, base loss: 14474.74
[INFO 2017-06-28 14:47:08,522 main.py:51] epoch 2117, training loss: 10503.08, average training loss: 10421.09, base loss: 14476.30
[INFO 2017-06-28 14:47:09,331 main.py:51] epoch 2118, training loss: 9995.09, average training loss: 10419.45, base loss: 14475.42
[INFO 2017-06-28 14:47:10,167 main.py:51] epoch 2119, training loss: 8619.09, average training loss: 10418.50, base loss: 14473.25
[INFO 2017-06-28 14:47:10,825 main.py:51] epoch 2120, training loss: 10172.47, average training loss: 10418.92, base loss: 14474.91
[INFO 2017-06-28 14:47:11,588 main.py:51] epoch 2121, training loss: 9153.11, average training loss: 10417.15, base loss: 14472.04
[INFO 2017-06-28 14:47:12,413 main.py:51] epoch 2122, training loss: 10281.48, average training loss: 10416.88, base loss: 14473.01
[INFO 2017-06-28 14:47:13,272 main.py:51] epoch 2123, training loss: 10214.94, average training loss: 10416.99, base loss: 14474.55
[INFO 2017-06-28 14:47:13,918 main.py:51] epoch 2124, training loss: 8700.75, average training loss: 10415.88, base loss: 14473.73
[INFO 2017-06-28 14:47:14,696 main.py:51] epoch 2125, training loss: 11287.64, average training loss: 10416.02, base loss: 14475.22
[INFO 2017-06-28 14:47:15,512 main.py:51] epoch 2126, training loss: 9951.19, average training loss: 10413.90, base loss: 14472.45
[INFO 2017-06-28 14:47:16,234 main.py:51] epoch 2127, training loss: 10836.43, average training loss: 10415.21, base loss: 14474.49
[INFO 2017-06-28 14:47:16,929 main.py:51] epoch 2128, training loss: 10226.64, average training loss: 10414.36, base loss: 14473.85
[INFO 2017-06-28 14:47:17,743 main.py:51] epoch 2129, training loss: 9886.99, average training loss: 10413.62, base loss: 14473.10
[INFO 2017-06-28 14:47:18,588 main.py:51] epoch 2130, training loss: 9304.92, average training loss: 10411.94, base loss: 14471.65
[INFO 2017-06-28 14:47:19,228 main.py:51] epoch 2131, training loss: 10473.58, average training loss: 10412.91, base loss: 14473.48
[INFO 2017-06-28 14:47:20,031 main.py:51] epoch 2132, training loss: 10097.50, average training loss: 10413.24, base loss: 14474.87
[INFO 2017-06-28 14:47:20,876 main.py:51] epoch 2133, training loss: 8562.94, average training loss: 10412.67, base loss: 14473.69
[INFO 2017-06-28 14:47:21,702 main.py:51] epoch 2134, training loss: 10088.66, average training loss: 10412.76, base loss: 14472.63
[INFO 2017-06-28 14:47:22,373 main.py:51] epoch 2135, training loss: 10500.12, average training loss: 10413.57, base loss: 14472.40
[INFO 2017-06-28 14:47:23,183 main.py:51] epoch 2136, training loss: 10626.92, average training loss: 10412.85, base loss: 14472.39
[INFO 2017-06-28 14:47:24,022 main.py:51] epoch 2137, training loss: 9350.96, average training loss: 10410.96, base loss: 14469.37
[INFO 2017-06-28 14:47:24,791 main.py:51] epoch 2138, training loss: 9809.26, average training loss: 10409.22, base loss: 14468.48
[INFO 2017-06-28 14:47:25,459 main.py:51] epoch 2139, training loss: 12001.20, average training loss: 10410.30, base loss: 14469.89
[INFO 2017-06-28 14:47:26,246 main.py:51] epoch 2140, training loss: 9372.87, average training loss: 10409.68, base loss: 14467.91
[INFO 2017-06-28 14:47:26,970 main.py:51] epoch 2141, training loss: 10276.09, average training loss: 10410.01, base loss: 14468.59
[INFO 2017-06-28 14:47:27,690 main.py:51] epoch 2142, training loss: 9292.33, average training loss: 10410.52, base loss: 14469.19
[INFO 2017-06-28 14:47:28,491 main.py:51] epoch 2143, training loss: 10830.90, average training loss: 10409.68, base loss: 14468.78
[INFO 2017-06-28 14:47:29,314 main.py:51] epoch 2144, training loss: 10784.63, average training loss: 10410.48, base loss: 14468.99
[INFO 2017-06-28 14:47:29,989 main.py:51] epoch 2145, training loss: 9723.88, average training loss: 10409.19, base loss: 14466.08
[INFO 2017-06-28 14:47:30,725 main.py:51] epoch 2146, training loss: 10408.21, average training loss: 10409.71, base loss: 14467.07
[INFO 2017-06-28 14:47:31,545 main.py:51] epoch 2147, training loss: 10052.99, average training loss: 10408.93, base loss: 14465.82
[INFO 2017-06-28 14:47:32,363 main.py:51] epoch 2148, training loss: 10386.16, average training loss: 10408.93, base loss: 14465.55
[INFO 2017-06-28 14:47:33,017 main.py:51] epoch 2149, training loss: 9790.93, average training loss: 10408.86, base loss: 14463.64
[INFO 2017-06-28 14:47:33,839 main.py:51] epoch 2150, training loss: 8661.48, average training loss: 10406.98, base loss: 14459.95
[INFO 2017-06-28 14:47:34,679 main.py:51] epoch 2151, training loss: 10533.17, average training loss: 10407.49, base loss: 14461.82
[INFO 2017-06-28 14:47:35,418 main.py:51] epoch 2152, training loss: 10466.39, average training loss: 10405.69, base loss: 14459.47
[INFO 2017-06-28 14:47:36,111 main.py:51] epoch 2153, training loss: 10914.35, average training loss: 10405.52, base loss: 14458.88
[INFO 2017-06-28 14:47:36,909 main.py:51] epoch 2154, training loss: 11559.95, average training loss: 10406.28, base loss: 14461.49
[INFO 2017-06-28 14:47:37,566 main.py:51] epoch 2155, training loss: 9956.79, average training loss: 10406.67, base loss: 14461.21
[INFO 2017-06-28 14:47:38,259 main.py:51] epoch 2156, training loss: 8802.22, average training loss: 10405.74, base loss: 14460.85
[INFO 2017-06-28 14:47:38,859 main.py:51] epoch 2157, training loss: 12404.83, average training loss: 10407.01, base loss: 14461.43
[INFO 2017-06-28 14:47:39,495 main.py:51] epoch 2158, training loss: 10327.21, average training loss: 10408.18, base loss: 14462.10
[INFO 2017-06-28 14:47:40,192 main.py:51] epoch 2159, training loss: 10021.07, average training loss: 10407.60, base loss: 14461.90
[INFO 2017-06-28 14:47:40,884 main.py:51] epoch 2160, training loss: 12358.32, average training loss: 10408.69, base loss: 14465.09
[INFO 2017-06-28 14:47:41,694 main.py:51] epoch 2161, training loss: 11183.41, average training loss: 10409.54, base loss: 14465.56
[INFO 2017-06-28 14:47:42,444 main.py:51] epoch 2162, training loss: 10133.45, average training loss: 10409.35, base loss: 14463.62
[INFO 2017-06-28 14:47:43,122 main.py:51] epoch 2163, training loss: 9356.89, average training loss: 10409.10, base loss: 14462.98
[INFO 2017-06-28 14:47:43,919 main.py:51] epoch 2164, training loss: 10048.47, average training loss: 10409.47, base loss: 14463.19
[INFO 2017-06-28 14:47:44,763 main.py:51] epoch 2165, training loss: 9422.39, average training loss: 10407.88, base loss: 14461.41
[INFO 2017-06-28 14:47:45,415 main.py:51] epoch 2166, training loss: 10016.70, average training loss: 10407.61, base loss: 14463.23
[INFO 2017-06-28 14:47:46,183 main.py:51] epoch 2167, training loss: 10408.39, average training loss: 10407.75, base loss: 14462.94
[INFO 2017-06-28 14:47:47,011 main.py:51] epoch 2168, training loss: 10377.12, average training loss: 10407.13, base loss: 14461.49
[INFO 2017-06-28 14:47:47,799 main.py:51] epoch 2169, training loss: 9941.32, average training loss: 10407.12, base loss: 14459.75
[INFO 2017-06-28 14:47:48,477 main.py:51] epoch 2170, training loss: 9296.26, average training loss: 10405.63, base loss: 14457.48
[INFO 2017-06-28 14:47:49,277 main.py:51] epoch 2171, training loss: 10707.62, average training loss: 10406.54, base loss: 14457.72
[INFO 2017-06-28 14:47:50,126 main.py:51] epoch 2172, training loss: 10165.88, average training loss: 10405.49, base loss: 14456.16
[INFO 2017-06-28 14:47:50,902 main.py:51] epoch 2173, training loss: 10148.22, average training loss: 10403.89, base loss: 14453.68
[INFO 2017-06-28 14:47:51,642 main.py:51] epoch 2174, training loss: 10921.31, average training loss: 10404.82, base loss: 14455.27
[INFO 2017-06-28 14:47:52,391 main.py:51] epoch 2175, training loss: 9925.71, average training loss: 10403.10, base loss: 14452.14
[INFO 2017-06-28 14:47:53,206 main.py:51] epoch 2176, training loss: 9662.69, average training loss: 10400.88, base loss: 14449.74
[INFO 2017-06-28 14:47:53,887 main.py:51] epoch 2177, training loss: 11489.19, average training loss: 10402.57, base loss: 14452.88
[INFO 2017-06-28 14:47:54,622 main.py:51] epoch 2178, training loss: 8734.52, average training loss: 10400.35, base loss: 14449.99
[INFO 2017-06-28 14:47:55,449 main.py:51] epoch 2179, training loss: 8585.72, average training loss: 10397.59, base loss: 14447.08
[INFO 2017-06-28 14:47:56,212 main.py:51] epoch 2180, training loss: 10278.28, average training loss: 10398.02, base loss: 14448.48
[INFO 2017-06-28 14:47:56,904 main.py:51] epoch 2181, training loss: 10313.49, average training loss: 10398.43, base loss: 14450.60
[INFO 2017-06-28 14:47:57,686 main.py:51] epoch 2182, training loss: 11230.54, average training loss: 10400.12, base loss: 14453.97
[INFO 2017-06-28 14:47:58,548 main.py:51] epoch 2183, training loss: 9736.93, average training loss: 10398.88, base loss: 14451.73
[INFO 2017-06-28 14:47:59,267 main.py:51] epoch 2184, training loss: 8502.34, average training loss: 10395.74, base loss: 14446.96
[INFO 2017-06-28 14:47:59,995 main.py:51] epoch 2185, training loss: 9626.70, average training loss: 10393.70, base loss: 14444.66
[INFO 2017-06-28 14:48:00,768 main.py:51] epoch 2186, training loss: 11979.23, average training loss: 10395.78, base loss: 14446.00
[INFO 2017-06-28 14:48:01,609 main.py:51] epoch 2187, training loss: 10765.23, average training loss: 10395.88, base loss: 14445.66
[INFO 2017-06-28 14:48:02,272 main.py:51] epoch 2188, training loss: 9109.35, average training loss: 10394.99, base loss: 14445.74
[INFO 2017-06-28 14:48:03,030 main.py:51] epoch 2189, training loss: 11414.90, average training loss: 10396.96, base loss: 14450.34
[INFO 2017-06-28 14:48:03,854 main.py:51] epoch 2190, training loss: 11129.17, average training loss: 10397.98, base loss: 14451.52
[INFO 2017-06-28 14:48:04,697 main.py:51] epoch 2191, training loss: 12213.53, average training loss: 10400.57, base loss: 14454.16
[INFO 2017-06-28 14:48:05,329 main.py:51] epoch 2192, training loss: 10173.57, average training loss: 10400.41, base loss: 14454.01
[INFO 2017-06-28 14:48:06,125 main.py:51] epoch 2193, training loss: 9564.32, average training loss: 10400.31, base loss: 14454.03
[INFO 2017-06-28 14:48:06,938 main.py:51] epoch 2194, training loss: 11311.41, average training loss: 10400.21, base loss: 14453.33
[INFO 2017-06-28 14:48:07,765 main.py:51] epoch 2195, training loss: 10101.73, average training loss: 10399.28, base loss: 14452.09
[INFO 2017-06-28 14:48:08,453 main.py:51] epoch 2196, training loss: 8761.30, average training loss: 10397.40, base loss: 14449.89
[INFO 2017-06-28 14:48:09,216 main.py:51] epoch 2197, training loss: 11532.65, average training loss: 10399.03, base loss: 14453.26
[INFO 2017-06-28 14:48:10,100 main.py:51] epoch 2198, training loss: 8772.32, average training loss: 10397.65, base loss: 14451.85
[INFO 2017-06-28 14:48:10,780 main.py:51] epoch 2199, training loss: 9485.40, average training loss: 10397.79, base loss: 14450.67
[INFO 2017-06-28 14:48:10,781 main.py:53] epoch 2199, testing
[INFO 2017-06-28 14:48:13,710 main.py:105] average testing loss: 11498.24, base loss: 15196.88
[INFO 2017-06-28 14:48:13,710 main.py:106] improve_loss: 3698.65, improve_percent: 0.24
[INFO 2017-06-28 14:48:13,711 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:48:14,391 main.py:51] epoch 2200, training loss: 9812.09, average training loss: 10397.16, base loss: 14448.94
[INFO 2017-06-28 14:48:15,187 main.py:51] epoch 2201, training loss: 10609.36, average training loss: 10397.70, base loss: 14450.04
[INFO 2017-06-28 14:48:16,013 main.py:51] epoch 2202, training loss: 9921.55, average training loss: 10398.55, base loss: 14450.18
[INFO 2017-06-28 14:48:16,700 main.py:51] epoch 2203, training loss: 11077.38, average training loss: 10399.86, base loss: 14451.51
[INFO 2017-06-28 14:48:17,445 main.py:51] epoch 2204, training loss: 12067.05, average training loss: 10401.33, base loss: 14453.40
[INFO 2017-06-28 14:48:18,271 main.py:51] epoch 2205, training loss: 10345.09, average training loss: 10399.39, base loss: 14453.18
[INFO 2017-06-28 14:48:19,105 main.py:51] epoch 2206, training loss: 11896.33, average training loss: 10400.57, base loss: 14456.29
[INFO 2017-06-28 14:48:19,758 main.py:51] epoch 2207, training loss: 8867.98, average training loss: 10399.76, base loss: 14455.39
[INFO 2017-06-28 14:48:20,551 main.py:51] epoch 2208, training loss: 13936.35, average training loss: 10402.44, base loss: 14459.68
[INFO 2017-06-28 14:48:21,394 main.py:51] epoch 2209, training loss: 9227.65, average training loss: 10400.11, base loss: 14458.63
[INFO 2017-06-28 14:48:22,131 main.py:51] epoch 2210, training loss: 9841.61, average training loss: 10400.00, base loss: 14457.40
[INFO 2017-06-28 14:48:22,830 main.py:51] epoch 2211, training loss: 10764.50, average training loss: 10400.97, base loss: 14459.07
[INFO 2017-06-28 14:48:23,631 main.py:51] epoch 2212, training loss: 8554.27, average training loss: 10398.41, base loss: 14454.46
[INFO 2017-06-28 14:48:24,427 main.py:51] epoch 2213, training loss: 11380.98, average training loss: 10399.47, base loss: 14456.10
[INFO 2017-06-28 14:48:25,060 main.py:51] epoch 2214, training loss: 9900.31, average training loss: 10399.71, base loss: 14456.20
[INFO 2017-06-28 14:48:25,820 main.py:51] epoch 2215, training loss: 10783.43, average training loss: 10399.18, base loss: 14457.01
[INFO 2017-06-28 14:48:26,660 main.py:51] epoch 2216, training loss: 9362.70, average training loss: 10397.61, base loss: 14455.86
[INFO 2017-06-28 14:48:27,357 main.py:51] epoch 2217, training loss: 9261.16, average training loss: 10397.72, base loss: 14455.31
[INFO 2017-06-28 14:48:28,099 main.py:51] epoch 2218, training loss: 10068.05, average training loss: 10398.64, base loss: 14456.98
[INFO 2017-06-28 14:48:28,906 main.py:51] epoch 2219, training loss: 9663.26, average training loss: 10399.55, base loss: 14457.96
[INFO 2017-06-28 14:48:29,723 main.py:51] epoch 2220, training loss: 9255.41, average training loss: 10398.40, base loss: 14457.13
[INFO 2017-06-28 14:48:30,415 main.py:51] epoch 2221, training loss: 9639.88, average training loss: 10398.36, base loss: 14458.08
[INFO 2017-06-28 14:48:31,221 main.py:51] epoch 2222, training loss: 11352.33, average training loss: 10397.09, base loss: 14457.28
[INFO 2017-06-28 14:48:32,052 main.py:51] epoch 2223, training loss: 10182.45, average training loss: 10397.48, base loss: 14458.07
[INFO 2017-06-28 14:48:32,874 main.py:51] epoch 2224, training loss: 10491.85, average training loss: 10394.80, base loss: 14453.49
[INFO 2017-06-28 14:48:33,560 main.py:51] epoch 2225, training loss: 10181.10, average training loss: 10395.67, base loss: 14455.15
[INFO 2017-06-28 14:48:34,300 main.py:51] epoch 2226, training loss: 10603.93, average training loss: 10396.17, base loss: 14456.77
[INFO 2017-06-28 14:48:35,104 main.py:51] epoch 2227, training loss: 10349.01, average training loss: 10396.74, base loss: 14458.20
[INFO 2017-06-28 14:48:35,732 main.py:51] epoch 2228, training loss: 10523.72, average training loss: 10396.02, base loss: 14458.27
[INFO 2017-06-28 14:48:36,493 main.py:51] epoch 2229, training loss: 9897.42, average training loss: 10396.11, base loss: 14459.70
[INFO 2017-06-28 14:48:37,334 main.py:51] epoch 2230, training loss: 10143.29, average training loss: 10395.96, base loss: 14460.88
[INFO 2017-06-28 14:48:38,077 main.py:51] epoch 2231, training loss: 10227.60, average training loss: 10395.32, base loss: 14460.20
[INFO 2017-06-28 14:48:38,761 main.py:51] epoch 2232, training loss: 10162.54, average training loss: 10395.08, base loss: 14461.90
[INFO 2017-06-28 14:48:39,593 main.py:51] epoch 2233, training loss: 8718.52, average training loss: 10391.29, base loss: 14456.58
[INFO 2017-06-28 14:48:40,355 main.py:51] epoch 2234, training loss: 9753.16, average training loss: 10391.38, base loss: 14457.23
[INFO 2017-06-28 14:48:41,043 main.py:51] epoch 2235, training loss: 10111.34, average training loss: 10390.06, base loss: 14455.26
[INFO 2017-06-28 14:48:41,808 main.py:51] epoch 2236, training loss: 9708.80, average training loss: 10386.60, base loss: 14450.27
[INFO 2017-06-28 14:48:42,635 main.py:51] epoch 2237, training loss: 9018.57, average training loss: 10385.20, base loss: 14446.91
[INFO 2017-06-28 14:48:43,266 main.py:51] epoch 2238, training loss: 10761.98, average training loss: 10384.83, base loss: 14447.26
[INFO 2017-06-28 14:48:44,090 main.py:51] epoch 2239, training loss: 10674.24, average training loss: 10385.73, base loss: 14448.69
[INFO 2017-06-28 14:48:44,907 main.py:51] epoch 2240, training loss: 11201.50, average training loss: 10384.71, base loss: 14448.43
[INFO 2017-06-28 14:48:45,609 main.py:51] epoch 2241, training loss: 10933.31, average training loss: 10386.54, base loss: 14451.53
[INFO 2017-06-28 14:48:46,301 main.py:51] epoch 2242, training loss: 9535.61, average training loss: 10385.66, base loss: 14451.37
[INFO 2017-06-28 14:48:47,116 main.py:51] epoch 2243, training loss: 9155.79, average training loss: 10382.39, base loss: 14448.27
[INFO 2017-06-28 14:48:47,893 main.py:51] epoch 2244, training loss: 12489.82, average training loss: 10385.17, base loss: 14452.88
[INFO 2017-06-28 14:48:48,573 main.py:51] epoch 2245, training loss: 9409.10, average training loss: 10384.72, base loss: 14451.97
[INFO 2017-06-28 14:48:49,319 main.py:51] epoch 2246, training loss: 10156.74, average training loss: 10384.53, base loss: 14453.56
[INFO 2017-06-28 14:48:50,163 main.py:51] epoch 2247, training loss: 10235.25, average training loss: 10386.22, base loss: 14456.41
[INFO 2017-06-28 14:48:50,848 main.py:51] epoch 2248, training loss: 11293.75, average training loss: 10387.50, base loss: 14457.92
[INFO 2017-06-28 14:48:51,588 main.py:51] epoch 2249, training loss: 12545.70, average training loss: 10390.43, base loss: 14461.90
[INFO 2017-06-28 14:48:52,403 main.py:51] epoch 2250, training loss: 10885.85, average training loss: 10391.08, base loss: 14463.23
[INFO 2017-06-28 14:48:53,255 main.py:51] epoch 2251, training loss: 10264.41, average training loss: 10390.66, base loss: 14462.25
[INFO 2017-06-28 14:48:53,872 main.py:51] epoch 2252, training loss: 10015.81, average training loss: 10389.91, base loss: 14461.77
[INFO 2017-06-28 14:48:54,666 main.py:51] epoch 2253, training loss: 9634.03, average training loss: 10387.12, base loss: 14460.07
[INFO 2017-06-28 14:48:55,494 main.py:51] epoch 2254, training loss: 11368.69, average training loss: 10388.69, base loss: 14462.61
[INFO 2017-06-28 14:48:56,098 main.py:51] epoch 2255, training loss: 9546.99, average training loss: 10387.36, base loss: 14460.85
[INFO 2017-06-28 14:48:56,888 main.py:51] epoch 2256, training loss: 10063.56, average training loss: 10386.90, base loss: 14459.72
[INFO 2017-06-28 14:48:57,727 main.py:51] epoch 2257, training loss: 8699.82, average training loss: 10385.24, base loss: 14457.86
[INFO 2017-06-28 14:48:58,450 main.py:51] epoch 2258, training loss: 10345.58, average training loss: 10385.16, base loss: 14459.29
[INFO 2017-06-28 14:48:59,141 main.py:51] epoch 2259, training loss: 9782.04, average training loss: 10381.51, base loss: 14453.89
[INFO 2017-06-28 14:48:59,963 main.py:51] epoch 2260, training loss: 9106.41, average training loss: 10380.19, base loss: 14451.23
[INFO 2017-06-28 14:49:00,796 main.py:51] epoch 2261, training loss: 11474.52, average training loss: 10376.84, base loss: 14446.62
[INFO 2017-06-28 14:49:01,451 main.py:51] epoch 2262, training loss: 11997.44, average training loss: 10379.17, base loss: 14451.33
[INFO 2017-06-28 14:49:02,219 main.py:51] epoch 2263, training loss: 11753.10, average training loss: 10380.43, base loss: 14453.93
[INFO 2017-06-28 14:49:03,059 main.py:51] epoch 2264, training loss: 11530.13, average training loss: 10380.55, base loss: 14454.93
[INFO 2017-06-28 14:49:03,771 main.py:51] epoch 2265, training loss: 10648.71, average training loss: 10381.01, base loss: 14455.59
[INFO 2017-06-28 14:49:04,500 main.py:51] epoch 2266, training loss: 9686.58, average training loss: 10380.44, base loss: 14453.36
[INFO 2017-06-28 14:49:05,315 main.py:51] epoch 2267, training loss: 11184.90, average training loss: 10379.99, base loss: 14452.00
[INFO 2017-06-28 14:49:06,136 main.py:51] epoch 2268, training loss: 12861.83, average training loss: 10382.45, base loss: 14456.56
[INFO 2017-06-28 14:49:06,791 main.py:51] epoch 2269, training loss: 10359.68, average training loss: 10383.34, base loss: 14456.65
[INFO 2017-06-28 14:49:07,567 main.py:51] epoch 2270, training loss: 10856.62, average training loss: 10385.24, base loss: 14459.60
[INFO 2017-06-28 14:49:08,374 main.py:51] epoch 2271, training loss: 11439.31, average training loss: 10385.99, base loss: 14460.66
[INFO 2017-06-28 14:49:09,111 main.py:51] epoch 2272, training loss: 10540.06, average training loss: 10387.17, base loss: 14461.84
[INFO 2017-06-28 14:49:09,795 main.py:51] epoch 2273, training loss: 9860.10, average training loss: 10386.15, base loss: 14461.90
[INFO 2017-06-28 14:49:10,620 main.py:51] epoch 2274, training loss: 10001.69, average training loss: 10386.20, base loss: 14460.43
[INFO 2017-06-28 14:49:11,438 main.py:51] epoch 2275, training loss: 9794.30, average training loss: 10385.88, base loss: 14459.98
[INFO 2017-06-28 14:49:12,094 main.py:51] epoch 2276, training loss: 11310.23, average training loss: 10387.91, base loss: 14463.89
[INFO 2017-06-28 14:49:12,881 main.py:51] epoch 2277, training loss: 10539.88, average training loss: 10386.43, base loss: 14461.27
[INFO 2017-06-28 14:49:13,737 main.py:51] epoch 2278, training loss: 10370.66, average training loss: 10386.81, base loss: 14462.69
[INFO 2017-06-28 14:49:14,484 main.py:51] epoch 2279, training loss: 11404.13, average training loss: 10388.42, base loss: 14465.60
[INFO 2017-06-28 14:49:15,184 main.py:51] epoch 2280, training loss: 11431.37, average training loss: 10386.87, base loss: 14463.37
[INFO 2017-06-28 14:49:15,982 main.py:51] epoch 2281, training loss: 10560.53, average training loss: 10387.37, base loss: 14462.66
[INFO 2017-06-28 14:49:16,806 main.py:51] epoch 2282, training loss: 8548.65, average training loss: 10385.49, base loss: 14460.28
[INFO 2017-06-28 14:49:17,494 main.py:51] epoch 2283, training loss: 10687.30, average training loss: 10385.72, base loss: 14461.64
[INFO 2017-06-28 14:49:18,237 main.py:51] epoch 2284, training loss: 10062.07, average training loss: 10385.88, base loss: 14461.97
[INFO 2017-06-28 14:49:19,073 main.py:51] epoch 2285, training loss: 10267.83, average training loss: 10386.62, base loss: 14462.70
[INFO 2017-06-28 14:49:19,829 main.py:51] epoch 2286, training loss: 10303.41, average training loss: 10386.95, base loss: 14463.78
[INFO 2017-06-28 14:49:20,519 main.py:51] epoch 2287, training loss: 9822.88, average training loss: 10386.31, base loss: 14464.39
[INFO 2017-06-28 14:49:21,335 main.py:51] epoch 2288, training loss: 8592.63, average training loss: 10383.37, base loss: 14460.53
[INFO 2017-06-28 14:49:22,167 main.py:51] epoch 2289, training loss: 10659.21, average training loss: 10382.37, base loss: 14458.90
[INFO 2017-06-28 14:49:22,850 main.py:51] epoch 2290, training loss: 9484.93, average training loss: 10382.73, base loss: 14459.55
[INFO 2017-06-28 14:49:23,577 main.py:51] epoch 2291, training loss: 9328.75, average training loss: 10381.37, base loss: 14459.74
[INFO 2017-06-28 14:49:24,411 main.py:51] epoch 2292, training loss: 9702.61, average training loss: 10380.26, base loss: 14457.26
[INFO 2017-06-28 14:49:25,146 main.py:51] epoch 2293, training loss: 9783.27, average training loss: 10380.43, base loss: 14458.71
[INFO 2017-06-28 14:49:25,838 main.py:51] epoch 2294, training loss: 10523.19, average training loss: 10379.42, base loss: 14458.11
[INFO 2017-06-28 14:49:26,626 main.py:51] epoch 2295, training loss: 11107.67, average training loss: 10380.05, base loss: 14458.02
[INFO 2017-06-28 14:49:27,300 main.py:51] epoch 2296, training loss: 10258.77, average training loss: 10378.51, base loss: 14456.87
[INFO 2017-06-28 14:49:27,986 main.py:51] epoch 2297, training loss: 10394.18, average training loss: 10378.35, base loss: 14457.28
[INFO 2017-06-28 14:49:28,580 main.py:51] epoch 2298, training loss: 8675.78, average training loss: 10376.89, base loss: 14455.90
[INFO 2017-06-28 14:49:29,211 main.py:51] epoch 2299, training loss: 8919.30, average training loss: 10374.90, base loss: 14452.32
[INFO 2017-06-28 14:49:29,211 main.py:53] epoch 2299, testing
[INFO 2017-06-28 14:49:31,893 main.py:105] average testing loss: 11798.65, base loss: 15690.41
[INFO 2017-06-28 14:49:31,893 main.py:106] improve_loss: 3891.77, improve_percent: 0.25
[INFO 2017-06-28 14:49:31,894 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:49:32,558 main.py:51] epoch 2300, training loss: 9721.06, average training loss: 10373.81, base loss: 14451.49
[INFO 2017-06-28 14:49:33,324 main.py:51] epoch 2301, training loss: 10038.20, average training loss: 10370.94, base loss: 14447.08
[INFO 2017-06-28 14:49:34,187 main.py:51] epoch 2302, training loss: 10143.75, average training loss: 10370.89, base loss: 14445.24
[INFO 2017-06-28 14:49:34,885 main.py:51] epoch 2303, training loss: 8880.35, average training loss: 10371.46, base loss: 14445.68
[INFO 2017-06-28 14:49:35,603 main.py:51] epoch 2304, training loss: 9998.75, average training loss: 10370.75, base loss: 14442.94
[INFO 2017-06-28 14:49:36,424 main.py:51] epoch 2305, training loss: 11405.58, average training loss: 10372.14, base loss: 14444.98
[INFO 2017-06-28 14:49:37,171 main.py:51] epoch 2306, training loss: 10729.71, average training loss: 10370.99, base loss: 14447.01
[INFO 2017-06-28 14:49:37,861 main.py:51] epoch 2307, training loss: 11415.38, average training loss: 10371.82, base loss: 14449.53
[INFO 2017-06-28 14:49:38,667 main.py:51] epoch 2308, training loss: 10048.32, average training loss: 10371.53, base loss: 14449.93
[INFO 2017-06-28 14:49:39,496 main.py:51] epoch 2309, training loss: 9049.84, average training loss: 10367.59, base loss: 14444.44
[INFO 2017-06-28 14:49:40,150 main.py:51] epoch 2310, training loss: 12557.85, average training loss: 10370.57, base loss: 14448.05
[INFO 2017-06-28 14:49:40,924 main.py:51] epoch 2311, training loss: 9508.86, average training loss: 10366.77, base loss: 14442.62
[INFO 2017-06-28 14:49:41,738 main.py:51] epoch 2312, training loss: 8578.19, average training loss: 10364.21, base loss: 14440.26
[INFO 2017-06-28 14:49:42,547 main.py:51] epoch 2313, training loss: 11215.81, average training loss: 10363.09, base loss: 14439.08
[INFO 2017-06-28 14:49:43,183 main.py:51] epoch 2314, training loss: 12160.41, average training loss: 10364.65, base loss: 14441.38
[INFO 2017-06-28 14:49:43,959 main.py:51] epoch 2315, training loss: 9956.85, average training loss: 10365.42, base loss: 14444.79
[INFO 2017-06-28 14:49:44,794 main.py:51] epoch 2316, training loss: 9823.02, average training loss: 10363.98, base loss: 14441.69
[INFO 2017-06-28 14:49:45,447 main.py:51] epoch 2317, training loss: 10454.28, average training loss: 10364.62, base loss: 14443.90
[INFO 2017-06-28 14:49:46,204 main.py:51] epoch 2318, training loss: 9494.96, average training loss: 10364.10, base loss: 14442.75
[INFO 2017-06-28 14:49:47,027 main.py:51] epoch 2319, training loss: 11619.01, average training loss: 10366.23, base loss: 14446.16
[INFO 2017-06-28 14:49:47,814 main.py:51] epoch 2320, training loss: 10523.16, average training loss: 10365.02, base loss: 14446.07
[INFO 2017-06-28 14:49:48,484 main.py:51] epoch 2321, training loss: 10398.03, average training loss: 10364.64, base loss: 14446.45
[INFO 2017-06-28 14:49:49,260 main.py:51] epoch 2322, training loss: 10112.27, average training loss: 10365.02, base loss: 14447.71
[INFO 2017-06-28 14:49:50,117 main.py:51] epoch 2323, training loss: 10455.86, average training loss: 10365.98, base loss: 14448.50
[INFO 2017-06-28 14:49:50,862 main.py:51] epoch 2324, training loss: 8711.35, average training loss: 10366.08, base loss: 14450.10
[INFO 2017-06-28 14:49:51,560 main.py:51] epoch 2325, training loss: 9685.61, average training loss: 10364.95, base loss: 14446.97
[INFO 2017-06-28 14:49:52,348 main.py:51] epoch 2326, training loss: 11160.73, average training loss: 10365.74, base loss: 14448.59
[INFO 2017-06-28 14:49:53,184 main.py:51] epoch 2327, training loss: 10726.02, average training loss: 10365.43, base loss: 14447.82
[INFO 2017-06-28 14:49:53,875 main.py:51] epoch 2328, training loss: 10023.28, average training loss: 10366.02, base loss: 14448.86
[INFO 2017-06-28 14:49:54,631 main.py:51] epoch 2329, training loss: 9218.14, average training loss: 10366.01, base loss: 14449.87
[INFO 2017-06-28 14:49:55,456 main.py:51] epoch 2330, training loss: 9210.35, average training loss: 10365.09, base loss: 14449.84
[INFO 2017-06-28 14:49:56,318 main.py:51] epoch 2331, training loss: 10118.79, average training loss: 10364.06, base loss: 14448.13
[INFO 2017-06-28 14:49:56,953 main.py:51] epoch 2332, training loss: 11112.78, average training loss: 10362.77, base loss: 14447.77
[INFO 2017-06-28 14:49:57,713 main.py:51] epoch 2333, training loss: 10675.09, average training loss: 10364.65, base loss: 14452.32
[INFO 2017-06-28 14:49:58,537 main.py:51] epoch 2334, training loss: 10906.02, average training loss: 10364.95, base loss: 14451.97
[INFO 2017-06-28 14:49:59,211 main.py:51] epoch 2335, training loss: 11780.11, average training loss: 10365.51, base loss: 14453.99
[INFO 2017-06-28 14:49:59,951 main.py:51] epoch 2336, training loss: 10057.50, average training loss: 10364.44, base loss: 14453.18
[INFO 2017-06-28 14:50:00,790 main.py:51] epoch 2337, training loss: 9510.09, average training loss: 10364.71, base loss: 14453.49
[INFO 2017-06-28 14:50:01,628 main.py:51] epoch 2338, training loss: 9605.49, average training loss: 10364.78, base loss: 14454.45
[INFO 2017-06-28 14:50:02,249 main.py:51] epoch 2339, training loss: 11417.87, average training loss: 10366.97, base loss: 14457.77
[INFO 2017-06-28 14:50:03,042 main.py:51] epoch 2340, training loss: 9515.81, average training loss: 10366.96, base loss: 14458.13
[INFO 2017-06-28 14:50:03,874 main.py:51] epoch 2341, training loss: 10224.74, average training loss: 10368.15, base loss: 14460.13
[INFO 2017-06-28 14:50:04,551 main.py:51] epoch 2342, training loss: 9122.91, average training loss: 10365.41, base loss: 14456.07
[INFO 2017-06-28 14:50:05,296 main.py:51] epoch 2343, training loss: 10696.07, average training loss: 10364.27, base loss: 14455.02
[INFO 2017-06-28 14:50:06,122 main.py:51] epoch 2344, training loss: 8786.44, average training loss: 10362.03, base loss: 14452.52
[INFO 2017-06-28 14:50:06,825 main.py:51] epoch 2345, training loss: 9430.14, average training loss: 10360.11, base loss: 14450.07
[INFO 2017-06-28 14:50:07,544 main.py:51] epoch 2346, training loss: 9528.72, average training loss: 10359.96, base loss: 14448.65
[INFO 2017-06-28 14:50:08,324 main.py:51] epoch 2347, training loss: 9671.90, average training loss: 10358.90, base loss: 14447.19
[INFO 2017-06-28 14:50:09,152 main.py:51] epoch 2348, training loss: 8966.69, average training loss: 10356.38, base loss: 14443.84
[INFO 2017-06-28 14:50:09,796 main.py:51] epoch 2349, training loss: 11293.16, average training loss: 10357.05, base loss: 14444.60
[INFO 2017-06-28 14:50:10,591 main.py:51] epoch 2350, training loss: 11073.81, average training loss: 10357.24, base loss: 14444.27
[INFO 2017-06-28 14:50:11,439 main.py:51] epoch 2351, training loss: 9140.11, average training loss: 10356.87, base loss: 14444.16
[INFO 2017-06-28 14:50:12,200 main.py:51] epoch 2352, training loss: 10439.24, average training loss: 10356.58, base loss: 14442.80
[INFO 2017-06-28 14:50:12,881 main.py:51] epoch 2353, training loss: 9447.06, average training loss: 10355.26, base loss: 14440.65
[INFO 2017-06-28 14:50:13,685 main.py:51] epoch 2354, training loss: 8575.56, average training loss: 10354.25, base loss: 14440.97
[INFO 2017-06-28 14:50:14,519 main.py:51] epoch 2355, training loss: 10299.66, average training loss: 10352.64, base loss: 14439.35
[INFO 2017-06-28 14:50:15,170 main.py:51] epoch 2356, training loss: 9479.90, average training loss: 10351.46, base loss: 14438.60
[INFO 2017-06-28 14:50:15,952 main.py:51] epoch 2357, training loss: 9591.95, average training loss: 10351.17, base loss: 14439.18
[INFO 2017-06-28 14:50:16,787 main.py:51] epoch 2358, training loss: 11180.08, average training loss: 10352.22, base loss: 14440.28
[INFO 2017-06-28 14:50:17,540 main.py:51] epoch 2359, training loss: 9874.19, average training loss: 10351.61, base loss: 14439.29
[INFO 2017-06-28 14:50:18,219 main.py:51] epoch 2360, training loss: 9565.38, average training loss: 10351.29, base loss: 14440.15
[INFO 2017-06-28 14:50:19,017 main.py:51] epoch 2361, training loss: 10155.64, average training loss: 10350.76, base loss: 14440.43
[INFO 2017-06-28 14:50:19,849 main.py:51] epoch 2362, training loss: 9848.69, average training loss: 10351.30, base loss: 14442.56
[INFO 2017-06-28 14:50:20,502 main.py:51] epoch 2363, training loss: 9485.85, average training loss: 10351.54, base loss: 14444.57
[INFO 2017-06-28 14:50:21,221 main.py:51] epoch 2364, training loss: 10144.69, average training loss: 10352.29, base loss: 14446.04
[INFO 2017-06-28 14:50:22,052 main.py:51] epoch 2365, training loss: 9636.83, average training loss: 10350.62, base loss: 14444.57
[INFO 2017-06-28 14:50:22,721 main.py:51] epoch 2366, training loss: 10321.40, average training loss: 10352.30, base loss: 14446.49
[INFO 2017-06-28 14:50:23,456 main.py:51] epoch 2367, training loss: 10721.28, average training loss: 10354.40, base loss: 14449.36
[INFO 2017-06-28 14:50:24,270 main.py:51] epoch 2368, training loss: 10366.35, average training loss: 10354.49, base loss: 14448.40
[INFO 2017-06-28 14:50:25,106 main.py:51] epoch 2369, training loss: 8821.11, average training loss: 10352.12, base loss: 14445.08
[INFO 2017-06-28 14:50:25,755 main.py:51] epoch 2370, training loss: 9345.16, average training loss: 10349.61, base loss: 14441.63
[INFO 2017-06-28 14:50:26,530 main.py:51] epoch 2371, training loss: 10355.79, average training loss: 10351.16, base loss: 14446.10
[INFO 2017-06-28 14:50:27,371 main.py:51] epoch 2372, training loss: 10188.69, average training loss: 10350.78, base loss: 14445.68
[INFO 2017-06-28 14:50:28,070 main.py:51] epoch 2373, training loss: 9810.39, average training loss: 10348.94, base loss: 14443.29
[INFO 2017-06-28 14:50:28,799 main.py:51] epoch 2374, training loss: 9063.12, average training loss: 10348.50, base loss: 14442.60
[INFO 2017-06-28 14:50:29,623 main.py:51] epoch 2375, training loss: 10006.97, average training loss: 10347.40, base loss: 14441.27
[INFO 2017-06-28 14:50:30,459 main.py:51] epoch 2376, training loss: 10948.17, average training loss: 10346.24, base loss: 14440.48
[INFO 2017-06-28 14:50:31,098 main.py:51] epoch 2377, training loss: 9871.65, average training loss: 10344.25, base loss: 14437.30
[INFO 2017-06-28 14:50:31,919 main.py:51] epoch 2378, training loss: 9349.25, average training loss: 10342.10, base loss: 14435.88
[INFO 2017-06-28 14:50:32,769 main.py:51] epoch 2379, training loss: 10020.85, average training loss: 10341.82, base loss: 14436.21
[INFO 2017-06-28 14:50:33,635 main.py:51] epoch 2380, training loss: 10591.50, average training loss: 10342.50, base loss: 14436.88
[INFO 2017-06-28 14:50:34,266 main.py:51] epoch 2381, training loss: 9414.27, average training loss: 10340.74, base loss: 14433.05
[INFO 2017-06-28 14:50:35,046 main.py:51] epoch 2382, training loss: 9692.67, average training loss: 10339.58, base loss: 14432.40
[INFO 2017-06-28 14:50:35,904 main.py:51] epoch 2383, training loss: 12366.67, average training loss: 10342.17, base loss: 14435.33
[INFO 2017-06-28 14:50:36,633 main.py:51] epoch 2384, training loss: 9571.84, average training loss: 10339.16, base loss: 14430.09
[INFO 2017-06-28 14:50:37,342 main.py:51] epoch 2385, training loss: 8275.66, average training loss: 10338.12, base loss: 14429.74
[INFO 2017-06-28 14:50:38,134 main.py:51] epoch 2386, training loss: 11382.45, average training loss: 10336.77, base loss: 14429.46
[INFO 2017-06-28 14:50:38,956 main.py:51] epoch 2387, training loss: 10796.48, average training loss: 10336.94, base loss: 14430.37
[INFO 2017-06-28 14:50:39,543 main.py:51] epoch 2388, training loss: 10530.89, average training loss: 10335.05, base loss: 14427.73
[INFO 2017-06-28 14:50:40,325 main.py:51] epoch 2389, training loss: 8938.37, average training loss: 10333.54, base loss: 14424.64
[INFO 2017-06-28 14:50:41,200 main.py:51] epoch 2390, training loss: 8728.43, average training loss: 10332.36, base loss: 14421.58
[INFO 2017-06-28 14:50:41,949 main.py:51] epoch 2391, training loss: 10538.06, average training loss: 10333.27, base loss: 14421.79
[INFO 2017-06-28 14:50:42,641 main.py:51] epoch 2392, training loss: 11193.80, average training loss: 10334.51, base loss: 14422.48
[INFO 2017-06-28 14:50:43,444 main.py:51] epoch 2393, training loss: 11256.49, average training loss: 10335.02, base loss: 14424.06
[INFO 2017-06-28 14:50:44,313 main.py:51] epoch 2394, training loss: 11150.91, average training loss: 10336.34, base loss: 14425.25
[INFO 2017-06-28 14:50:45,107 main.py:51] epoch 2395, training loss: 10117.49, average training loss: 10338.14, base loss: 14426.91
[INFO 2017-06-28 14:50:45,790 main.py:51] epoch 2396, training loss: 9890.57, average training loss: 10338.35, base loss: 14426.46
[INFO 2017-06-28 14:50:46,582 main.py:51] epoch 2397, training loss: 9745.03, average training loss: 10338.24, base loss: 14424.87
[INFO 2017-06-28 14:50:47,402 main.py:51] epoch 2398, training loss: 9742.69, average training loss: 10338.32, base loss: 14425.24
[INFO 2017-06-28 14:50:48,056 main.py:51] epoch 2399, training loss: 10183.59, average training loss: 10337.60, base loss: 14425.45
[INFO 2017-06-28 14:50:48,056 main.py:53] epoch 2399, testing
[INFO 2017-06-28 14:50:51,001 main.py:105] average testing loss: 10979.51, base loss: 14637.31
[INFO 2017-06-28 14:50:51,001 main.py:106] improve_loss: 3657.80, improve_percent: 0.25
[INFO 2017-06-28 14:50:51,002 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:50:51,732 main.py:51] epoch 2400, training loss: 11739.68, average training loss: 10338.06, base loss: 14425.86
[INFO 2017-06-28 14:50:52,511 main.py:51] epoch 2401, training loss: 8989.53, average training loss: 10337.11, base loss: 14426.96
[INFO 2017-06-28 14:50:53,299 main.py:51] epoch 2402, training loss: 11055.04, average training loss: 10338.74, base loss: 14429.55
[INFO 2017-06-28 14:50:53,969 main.py:51] epoch 2403, training loss: 9237.52, average training loss: 10338.94, base loss: 14430.05
[INFO 2017-06-28 14:50:54,729 main.py:51] epoch 2404, training loss: 9115.30, average training loss: 10337.32, base loss: 14427.10
[INFO 2017-06-28 14:50:55,598 main.py:51] epoch 2405, training loss: 12570.48, average training loss: 10340.23, base loss: 14432.14
[INFO 2017-06-28 14:50:56,352 main.py:51] epoch 2406, training loss: 9855.73, average training loss: 10339.93, base loss: 14433.18
[INFO 2017-06-28 14:50:57,056 main.py:51] epoch 2407, training loss: 9779.04, average training loss: 10340.54, base loss: 14435.37
[INFO 2017-06-28 14:50:57,867 main.py:51] epoch 2408, training loss: 9706.90, average training loss: 10340.00, base loss: 14434.80
[INFO 2017-06-28 14:50:58,688 main.py:51] epoch 2409, training loss: 10606.99, average training loss: 10341.06, base loss: 14436.28
[INFO 2017-06-28 14:50:59,350 main.py:51] epoch 2410, training loss: 9769.42, average training loss: 10339.91, base loss: 14432.46
[INFO 2017-06-28 14:51:00,117 main.py:51] epoch 2411, training loss: 9006.44, average training loss: 10338.15, base loss: 14429.80
[INFO 2017-06-28 14:51:00,952 main.py:51] epoch 2412, training loss: 10117.92, average training loss: 10335.01, base loss: 14427.11
[INFO 2017-06-28 14:51:01,725 main.py:51] epoch 2413, training loss: 9992.55, average training loss: 10335.24, base loss: 14426.91
[INFO 2017-06-28 14:51:02,413 main.py:51] epoch 2414, training loss: 8034.38, average training loss: 10332.46, base loss: 14422.61
[INFO 2017-06-28 14:51:03,185 main.py:51] epoch 2415, training loss: 10019.53, average training loss: 10330.73, base loss: 14418.21
[INFO 2017-06-28 14:51:03,991 main.py:51] epoch 2416, training loss: 11585.64, average training loss: 10332.21, base loss: 14420.39
[INFO 2017-06-28 14:51:04,614 main.py:51] epoch 2417, training loss: 12092.71, average training loss: 10334.21, base loss: 14421.23
[INFO 2017-06-28 14:51:05,392 main.py:51] epoch 2418, training loss: 10661.77, average training loss: 10334.72, base loss: 14422.41
[INFO 2017-06-28 14:51:06,225 main.py:51] epoch 2419, training loss: 9842.67, average training loss: 10333.00, base loss: 14419.85
[INFO 2017-06-28 14:51:06,962 main.py:51] epoch 2420, training loss: 9888.17, average training loss: 10333.56, base loss: 14421.41
[INFO 2017-06-28 14:51:07,668 main.py:51] epoch 2421, training loss: 7717.77, average training loss: 10332.02, base loss: 14418.15
[INFO 2017-06-28 14:51:08,458 main.py:51] epoch 2422, training loss: 11179.32, average training loss: 10333.48, base loss: 14419.49
[INFO 2017-06-28 14:51:09,247 main.py:51] epoch 2423, training loss: 9092.50, average training loss: 10332.18, base loss: 14418.00
[INFO 2017-06-28 14:51:09,934 main.py:51] epoch 2424, training loss: 12686.57, average training loss: 10333.96, base loss: 14419.90
[INFO 2017-06-28 14:51:10,706 main.py:51] epoch 2425, training loss: 9962.68, average training loss: 10334.72, base loss: 14420.15
[INFO 2017-06-28 14:51:11,516 main.py:51] epoch 2426, training loss: 10421.77, average training loss: 10336.07, base loss: 14424.11
[INFO 2017-06-28 14:51:12,371 main.py:51] epoch 2427, training loss: 8668.50, average training loss: 10335.96, base loss: 14424.41
[INFO 2017-06-28 14:51:13,032 main.py:51] epoch 2428, training loss: 10419.12, average training loss: 10336.27, base loss: 14425.25
[INFO 2017-06-28 14:51:13,808 main.py:51] epoch 2429, training loss: 10990.24, average training loss: 10337.58, base loss: 14427.02
[INFO 2017-06-28 14:51:14,630 main.py:51] epoch 2430, training loss: 9059.14, average training loss: 10336.93, base loss: 14424.96
[INFO 2017-06-28 14:51:15,411 main.py:51] epoch 2431, training loss: 11618.25, average training loss: 10338.83, base loss: 14428.07
[INFO 2017-06-28 14:51:16,085 main.py:51] epoch 2432, training loss: 9571.83, average training loss: 10337.97, base loss: 14427.82
[INFO 2017-06-28 14:51:16,773 main.py:51] epoch 2433, training loss: 9553.47, average training loss: 10336.62, base loss: 14424.83
[INFO 2017-06-28 14:51:17,414 main.py:51] epoch 2434, training loss: 9829.70, average training loss: 10332.93, base loss: 14420.79
[INFO 2017-06-28 14:51:18,018 main.py:51] epoch 2435, training loss: 8739.11, average training loss: 10332.39, base loss: 14420.14
[INFO 2017-06-28 14:51:18,726 main.py:51] epoch 2436, training loss: 10272.99, average training loss: 10331.85, base loss: 14420.47
[INFO 2017-06-28 14:51:19,423 main.py:51] epoch 2437, training loss: 10624.09, average training loss: 10331.40, base loss: 14421.59
[INFO 2017-06-28 14:51:20,115 main.py:51] epoch 2438, training loss: 10896.39, average training loss: 10332.32, base loss: 14423.93
[INFO 2017-06-28 14:51:20,845 main.py:51] epoch 2439, training loss: 11904.23, average training loss: 10333.64, base loss: 14425.48
[INFO 2017-06-28 14:51:21,662 main.py:51] epoch 2440, training loss: 10512.60, average training loss: 10331.98, base loss: 14422.84
[INFO 2017-06-28 14:51:22,499 main.py:51] epoch 2441, training loss: 11146.88, average training loss: 10332.97, base loss: 14423.50
[INFO 2017-06-28 14:51:23,160 main.py:51] epoch 2442, training loss: 9960.43, average training loss: 10332.38, base loss: 14422.64
[INFO 2017-06-28 14:51:23,975 main.py:51] epoch 2443, training loss: 9839.57, average training loss: 10331.06, base loss: 14420.24
[INFO 2017-06-28 14:51:24,783 main.py:51] epoch 2444, training loss: 11166.68, average training loss: 10332.98, base loss: 14425.62
[INFO 2017-06-28 14:51:25,525 main.py:51] epoch 2445, training loss: 10282.00, average training loss: 10332.38, base loss: 14424.33
[INFO 2017-06-28 14:51:26,243 main.py:51] epoch 2446, training loss: 10030.23, average training loss: 10331.94, base loss: 14424.67
[INFO 2017-06-28 14:51:27,041 main.py:51] epoch 2447, training loss: 9320.71, average training loss: 10330.78, base loss: 14423.67
[INFO 2017-06-28 14:51:27,872 main.py:51] epoch 2448, training loss: 11368.88, average training loss: 10332.94, base loss: 14428.49
[INFO 2017-06-28 14:51:28,534 main.py:51] epoch 2449, training loss: 9448.60, average training loss: 10331.89, base loss: 14426.07
[INFO 2017-06-28 14:51:29,272 main.py:51] epoch 2450, training loss: 10511.67, average training loss: 10332.38, base loss: 14427.58
[INFO 2017-06-28 14:51:30,072 main.py:51] epoch 2451, training loss: 10549.68, average training loss: 10332.09, base loss: 14427.30
[INFO 2017-06-28 14:51:30,922 main.py:51] epoch 2452, training loss: 10993.22, average training loss: 10334.10, base loss: 14429.31
[INFO 2017-06-28 14:51:31,564 main.py:51] epoch 2453, training loss: 8620.74, average training loss: 10332.23, base loss: 14426.99
[INFO 2017-06-28 14:51:32,349 main.py:51] epoch 2454, training loss: 11028.44, average training loss: 10334.74, base loss: 14429.91
[INFO 2017-06-28 14:51:33,172 main.py:51] epoch 2455, training loss: 9842.63, average training loss: 10334.34, base loss: 14429.14
[INFO 2017-06-28 14:51:33,943 main.py:51] epoch 2456, training loss: 10634.06, average training loss: 10335.10, base loss: 14430.02
[INFO 2017-06-28 14:51:34,647 main.py:51] epoch 2457, training loss: 10944.49, average training loss: 10337.24, base loss: 14434.15
[INFO 2017-06-28 14:51:35,440 main.py:51] epoch 2458, training loss: 11698.94, average training loss: 10338.95, base loss: 14435.97
[INFO 2017-06-28 14:51:36,228 main.py:51] epoch 2459, training loss: 12339.96, average training loss: 10339.36, base loss: 14435.57
[INFO 2017-06-28 14:51:36,867 main.py:51] epoch 2460, training loss: 11239.66, average training loss: 10339.35, base loss: 14435.03
[INFO 2017-06-28 14:51:37,658 main.py:51] epoch 2461, training loss: 9176.14, average training loss: 10338.13, base loss: 14433.29
[INFO 2017-06-28 14:51:38,491 main.py:51] epoch 2462, training loss: 9271.09, average training loss: 10337.14, base loss: 14431.52
[INFO 2017-06-28 14:51:39,267 main.py:51] epoch 2463, training loss: 10627.00, average training loss: 10338.73, base loss: 14434.32
[INFO 2017-06-28 14:51:39,940 main.py:51] epoch 2464, training loss: 8866.97, average training loss: 10337.81, base loss: 14433.51
[INFO 2017-06-28 14:51:40,733 main.py:51] epoch 2465, training loss: 10261.90, average training loss: 10338.03, base loss: 14434.44
[INFO 2017-06-28 14:51:41,521 main.py:51] epoch 2466, training loss: 9797.36, average training loss: 10338.22, base loss: 14434.97
[INFO 2017-06-28 14:51:42,177 main.py:51] epoch 2467, training loss: 9390.19, average training loss: 10336.16, base loss: 14432.96
[INFO 2017-06-28 14:51:42,962 main.py:51] epoch 2468, training loss: 10041.94, average training loss: 10335.97, base loss: 14432.04
[INFO 2017-06-28 14:51:43,800 main.py:51] epoch 2469, training loss: 11462.14, average training loss: 10337.81, base loss: 14434.31
[INFO 2017-06-28 14:51:44,526 main.py:51] epoch 2470, training loss: 10391.41, average training loss: 10338.24, base loss: 14434.79
[INFO 2017-06-28 14:51:45,236 main.py:51] epoch 2471, training loss: 9375.59, average training loss: 10337.55, base loss: 14433.65
[INFO 2017-06-28 14:51:46,027 main.py:51] epoch 2472, training loss: 9792.39, average training loss: 10336.99, base loss: 14432.70
[INFO 2017-06-28 14:51:46,879 main.py:51] epoch 2473, training loss: 8657.42, average training loss: 10336.80, base loss: 14432.03
[INFO 2017-06-28 14:51:47,557 main.py:51] epoch 2474, training loss: 10381.27, average training loss: 10336.50, base loss: 14431.84
[INFO 2017-06-28 14:51:48,320 main.py:51] epoch 2475, training loss: 10090.18, average training loss: 10334.13, base loss: 14427.99
[INFO 2017-06-28 14:51:49,114 main.py:51] epoch 2476, training loss: 10091.03, average training loss: 10334.74, base loss: 14428.76
[INFO 2017-06-28 14:51:49,882 main.py:51] epoch 2477, training loss: 11466.58, average training loss: 10335.89, base loss: 14430.58
[INFO 2017-06-28 14:51:50,525 main.py:51] epoch 2478, training loss: 9831.65, average training loss: 10334.43, base loss: 14428.64
[INFO 2017-06-28 14:51:51,313 main.py:51] epoch 2479, training loss: 9393.54, average training loss: 10333.75, base loss: 14426.84
[INFO 2017-06-28 14:51:52,122 main.py:51] epoch 2480, training loss: 11470.10, average training loss: 10334.64, base loss: 14427.35
[INFO 2017-06-28 14:51:52,922 main.py:51] epoch 2481, training loss: 10024.31, average training loss: 10334.88, base loss: 14427.57
[INFO 2017-06-28 14:51:53,564 main.py:51] epoch 2482, training loss: 10564.05, average training loss: 10335.31, base loss: 14429.18
[INFO 2017-06-28 14:51:54,381 main.py:51] epoch 2483, training loss: 9558.02, average training loss: 10332.78, base loss: 14425.01
[INFO 2017-06-28 14:51:55,199 main.py:51] epoch 2484, training loss: 9672.56, average training loss: 10331.87, base loss: 14424.17
[INFO 2017-06-28 14:51:56,001 main.py:51] epoch 2485, training loss: 12196.87, average training loss: 10333.99, base loss: 14427.80
[INFO 2017-06-28 14:51:56,660 main.py:51] epoch 2486, training loss: 9092.27, average training loss: 10332.41, base loss: 14426.75
[INFO 2017-06-28 14:51:57,492 main.py:51] epoch 2487, training loss: 9463.43, average training loss: 10331.43, base loss: 14424.96
[INFO 2017-06-28 14:51:58,319 main.py:51] epoch 2488, training loss: 10825.69, average training loss: 10331.77, base loss: 14424.61
[INFO 2017-06-28 14:51:59,097 main.py:51] epoch 2489, training loss: 10052.60, average training loss: 10331.14, base loss: 14423.41
[INFO 2017-06-28 14:51:59,762 main.py:51] epoch 2490, training loss: 9063.84, average training loss: 10330.47, base loss: 14422.72
[INFO 2017-06-28 14:52:00,549 main.py:51] epoch 2491, training loss: 10858.41, average training loss: 10330.90, base loss: 14423.74
[INFO 2017-06-28 14:52:01,369 main.py:51] epoch 2492, training loss: 9421.90, average training loss: 10330.57, base loss: 14423.90
[INFO 2017-06-28 14:52:02,037 main.py:51] epoch 2493, training loss: 9815.14, average training loss: 10329.77, base loss: 14423.45
[INFO 2017-06-28 14:52:02,804 main.py:51] epoch 2494, training loss: 11082.24, average training loss: 10329.79, base loss: 14425.00
[INFO 2017-06-28 14:52:03,603 main.py:51] epoch 2495, training loss: 10895.04, average training loss: 10330.51, base loss: 14426.44
[INFO 2017-06-28 14:52:04,453 main.py:51] epoch 2496, training loss: 10426.89, average training loss: 10329.83, base loss: 14425.34
[INFO 2017-06-28 14:52:05,079 main.py:51] epoch 2497, training loss: 9597.54, average training loss: 10329.37, base loss: 14424.47
[INFO 2017-06-28 14:52:05,865 main.py:51] epoch 2498, training loss: 10579.73, average training loss: 10329.42, base loss: 14425.95
[INFO 2017-06-28 14:52:06,679 main.py:51] epoch 2499, training loss: 11110.78, average training loss: 10330.17, base loss: 14428.07
[INFO 2017-06-28 14:52:06,679 main.py:53] epoch 2499, testing
[INFO 2017-06-28 14:52:09,561 main.py:105] average testing loss: 11308.02, base loss: 15040.35
[INFO 2017-06-28 14:52:09,561 main.py:106] improve_loss: 3732.33, improve_percent: 0.25
[INFO 2017-06-28 14:52:09,562 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:52:10,386 main.py:51] epoch 2500, training loss: 10879.82, average training loss: 10328.27, base loss: 14426.02
[INFO 2017-06-28 14:52:11,036 main.py:51] epoch 2501, training loss: 9538.81, average training loss: 10327.95, base loss: 14425.53
[INFO 2017-06-28 14:52:11,767 main.py:51] epoch 2502, training loss: 8967.32, average training loss: 10325.35, base loss: 14420.97
[INFO 2017-06-28 14:52:12,581 main.py:51] epoch 2503, training loss: 12216.22, average training loss: 10328.78, base loss: 14426.27
[INFO 2017-06-28 14:52:13,224 main.py:51] epoch 2504, training loss: 9452.23, average training loss: 10326.96, base loss: 14423.18
[INFO 2017-06-28 14:52:13,983 main.py:51] epoch 2505, training loss: 9524.97, average training loss: 10326.76, base loss: 14423.21
[INFO 2017-06-28 14:52:14,805 main.py:51] epoch 2506, training loss: 9561.93, average training loss: 10326.14, base loss: 14423.90
[INFO 2017-06-28 14:52:15,542 main.py:51] epoch 2507, training loss: 9158.96, average training loss: 10324.86, base loss: 14421.59
[INFO 2017-06-28 14:52:16,226 main.py:51] epoch 2508, training loss: 10235.29, average training loss: 10322.74, base loss: 14418.16
[INFO 2017-06-28 14:52:17,032 main.py:51] epoch 2509, training loss: 10789.46, average training loss: 10322.85, base loss: 14421.75
[INFO 2017-06-28 14:52:17,757 main.py:51] epoch 2510, training loss: 10453.71, average training loss: 10324.33, base loss: 14423.61
[INFO 2017-06-28 14:52:18,482 main.py:51] epoch 2511, training loss: 8935.63, average training loss: 10322.69, base loss: 14420.96
[INFO 2017-06-28 14:52:19,280 main.py:51] epoch 2512, training loss: 9652.56, average training loss: 10322.15, base loss: 14420.95
[INFO 2017-06-28 14:52:20,047 main.py:51] epoch 2513, training loss: 9924.24, average training loss: 10320.31, base loss: 14419.79
[INFO 2017-06-28 14:52:20,733 main.py:51] epoch 2514, training loss: 9888.43, average training loss: 10319.08, base loss: 14416.71
[INFO 2017-06-28 14:52:21,446 main.py:51] epoch 2515, training loss: 8930.75, average training loss: 10318.39, base loss: 14416.15
[INFO 2017-06-28 14:52:22,315 main.py:51] epoch 2516, training loss: 9545.00, average training loss: 10316.09, base loss: 14414.12
[INFO 2017-06-28 14:52:23,011 main.py:51] epoch 2517, training loss: 14160.35, average training loss: 10319.21, base loss: 14417.64
[INFO 2017-06-28 14:52:23,732 main.py:51] epoch 2518, training loss: 10423.60, average training loss: 10319.12, base loss: 14415.56
[INFO 2017-06-28 14:52:24,528 main.py:51] epoch 2519, training loss: 10064.50, average training loss: 10316.00, base loss: 14412.40
[INFO 2017-06-28 14:52:25,360 main.py:51] epoch 2520, training loss: 8980.72, average training loss: 10314.66, base loss: 14411.04
[INFO 2017-06-28 14:52:26,008 main.py:51] epoch 2521, training loss: 12279.50, average training loss: 10316.58, base loss: 14413.12
[INFO 2017-06-28 14:52:26,812 main.py:51] epoch 2522, training loss: 10873.36, average training loss: 10318.62, base loss: 14416.11
[INFO 2017-06-28 14:52:27,642 main.py:51] epoch 2523, training loss: 8741.45, average training loss: 10316.49, base loss: 14413.80
[INFO 2017-06-28 14:52:28,358 main.py:51] epoch 2524, training loss: 9802.61, average training loss: 10316.89, base loss: 14416.92
[INFO 2017-06-28 14:52:29,090 main.py:51] epoch 2525, training loss: 10128.53, average training loss: 10316.89, base loss: 14416.29
[INFO 2017-06-28 14:52:29,876 main.py:51] epoch 2526, training loss: 9874.45, average training loss: 10317.23, base loss: 14416.61
[INFO 2017-06-28 14:52:30,675 main.py:51] epoch 2527, training loss: 9708.86, average training loss: 10317.30, base loss: 14416.02
[INFO 2017-06-28 14:52:31,279 main.py:51] epoch 2528, training loss: 10463.37, average training loss: 10317.85, base loss: 14417.71
[INFO 2017-06-28 14:52:32,058 main.py:51] epoch 2529, training loss: 10490.24, average training loss: 10318.07, base loss: 14418.01
[INFO 2017-06-28 14:52:32,911 main.py:51] epoch 2530, training loss: 11210.02, average training loss: 10320.80, base loss: 14422.10
[INFO 2017-06-28 14:52:33,645 main.py:51] epoch 2531, training loss: 10026.36, average training loss: 10319.73, base loss: 14421.24
[INFO 2017-06-28 14:52:34,362 main.py:51] epoch 2532, training loss: 9225.34, average training loss: 10320.17, base loss: 14420.67
[INFO 2017-06-28 14:52:35,140 main.py:51] epoch 2533, training loss: 10218.36, average training loss: 10319.62, base loss: 14420.14
[INFO 2017-06-28 14:52:35,927 main.py:51] epoch 2534, training loss: 12054.35, average training loss: 10321.85, base loss: 14423.19
[INFO 2017-06-28 14:52:36,595 main.py:51] epoch 2535, training loss: 10341.25, average training loss: 10320.28, base loss: 14419.38
[INFO 2017-06-28 14:52:37,328 main.py:51] epoch 2536, training loss: 9526.81, average training loss: 10319.48, base loss: 14419.49
[INFO 2017-06-28 14:52:38,151 main.py:51] epoch 2537, training loss: 10499.74, average training loss: 10318.79, base loss: 14416.92
[INFO 2017-06-28 14:52:38,834 main.py:51] epoch 2538, training loss: 11254.60, average training loss: 10321.24, base loss: 14420.25
[INFO 2017-06-28 14:52:39,555 main.py:51] epoch 2539, training loss: 9367.40, average training loss: 10320.77, base loss: 14421.10
[INFO 2017-06-28 14:52:40,386 main.py:51] epoch 2540, training loss: 10566.59, average training loss: 10321.88, base loss: 14422.03
[INFO 2017-06-28 14:52:41,235 main.py:51] epoch 2541, training loss: 9947.41, average training loss: 10320.13, base loss: 14420.11
[INFO 2017-06-28 14:52:41,915 main.py:51] epoch 2542, training loss: 9377.11, average training loss: 10316.68, base loss: 14416.83
[INFO 2017-06-28 14:52:42,693 main.py:51] epoch 2543, training loss: 10581.22, average training loss: 10318.12, base loss: 14418.92
[INFO 2017-06-28 14:52:43,503 main.py:51] epoch 2544, training loss: 8372.48, average training loss: 10315.02, base loss: 14413.50
[INFO 2017-06-28 14:52:44,326 main.py:51] epoch 2545, training loss: 9921.89, average training loss: 10314.72, base loss: 14411.83
[INFO 2017-06-28 14:52:44,995 main.py:51] epoch 2546, training loss: 9818.04, average training loss: 10314.57, base loss: 14410.84
[INFO 2017-06-28 14:52:45,799 main.py:51] epoch 2547, training loss: 11704.30, average training loss: 10314.61, base loss: 14409.42
[INFO 2017-06-28 14:52:46,618 main.py:51] epoch 2548, training loss: 9215.13, average training loss: 10312.38, base loss: 14406.77
[INFO 2017-06-28 14:52:47,374 main.py:51] epoch 2549, training loss: 8450.03, average training loss: 10309.86, base loss: 14402.47
[INFO 2017-06-28 14:52:48,086 main.py:51] epoch 2550, training loss: 10315.95, average training loss: 10309.71, base loss: 14404.26
[INFO 2017-06-28 14:52:48,856 main.py:51] epoch 2551, training loss: 11425.34, average training loss: 10311.38, base loss: 14407.19
[INFO 2017-06-28 14:52:49,710 main.py:51] epoch 2552, training loss: 9420.15, average training loss: 10310.00, base loss: 14405.65
[INFO 2017-06-28 14:52:50,473 main.py:51] epoch 2553, training loss: 11364.36, average training loss: 10309.80, base loss: 14406.37
[INFO 2017-06-28 14:52:51,177 main.py:51] epoch 2554, training loss: 9630.09, average training loss: 10308.89, base loss: 14404.89
[INFO 2017-06-28 14:52:51,945 main.py:51] epoch 2555, training loss: 10765.82, average training loss: 10309.01, base loss: 14405.76
[INFO 2017-06-28 14:52:52,760 main.py:51] epoch 2556, training loss: 11714.38, average training loss: 10310.24, base loss: 14406.83
[INFO 2017-06-28 14:52:53,397 main.py:51] epoch 2557, training loss: 9414.14, average training loss: 10309.32, base loss: 14405.75
[INFO 2017-06-28 14:52:54,176 main.py:51] epoch 2558, training loss: 11099.68, average training loss: 10310.31, base loss: 14405.30
[INFO 2017-06-28 14:52:54,998 main.py:51] epoch 2559, training loss: 10909.10, average training loss: 10309.00, base loss: 14403.13
[INFO 2017-06-28 14:52:55,703 main.py:51] epoch 2560, training loss: 10586.27, average training loss: 10309.43, base loss: 14403.52
[INFO 2017-06-28 14:52:56,456 main.py:51] epoch 2561, training loss: 10489.78, average training loss: 10309.79, base loss: 14403.82
[INFO 2017-06-28 14:52:57,260 main.py:51] epoch 2562, training loss: 9184.70, average training loss: 10309.40, base loss: 14404.26
[INFO 2017-06-28 14:52:58,106 main.py:51] epoch 2563, training loss: 9618.48, average training loss: 10309.36, base loss: 14405.91
[INFO 2017-06-28 14:52:58,747 main.py:51] epoch 2564, training loss: 8659.62, average training loss: 10307.96, base loss: 14402.23
[INFO 2017-06-28 14:52:59,540 main.py:51] epoch 2565, training loss: 9221.44, average training loss: 10305.68, base loss: 14399.91
[INFO 2017-06-28 14:53:00,373 main.py:51] epoch 2566, training loss: 10138.24, average training loss: 10303.79, base loss: 14397.98
[INFO 2017-06-28 14:53:01,157 main.py:51] epoch 2567, training loss: 10561.98, average training loss: 10303.12, base loss: 14398.22
[INFO 2017-06-28 14:53:01,849 main.py:51] epoch 2568, training loss: 10564.31, average training loss: 10302.94, base loss: 14397.87
[INFO 2017-06-28 14:53:02,601 main.py:51] epoch 2569, training loss: 9885.49, average training loss: 10300.68, base loss: 14395.22
[INFO 2017-06-28 14:53:03,412 main.py:51] epoch 2570, training loss: 11654.47, average training loss: 10300.61, base loss: 14395.81
[INFO 2017-06-28 14:53:04,057 main.py:51] epoch 2571, training loss: 9503.23, average training loss: 10301.13, base loss: 14397.02
[INFO 2017-06-28 14:53:04,869 main.py:51] epoch 2572, training loss: 13050.87, average training loss: 10303.91, base loss: 14400.82
[INFO 2017-06-28 14:53:05,525 main.py:51] epoch 2573, training loss: 9324.91, average training loss: 10303.73, base loss: 14400.15
[INFO 2017-06-28 14:53:06,135 main.py:51] epoch 2574, training loss: 8480.73, average training loss: 10301.58, base loss: 14396.47
[INFO 2017-06-28 14:53:06,798 main.py:51] epoch 2575, training loss: 9285.35, average training loss: 10298.46, base loss: 14392.55
[INFO 2017-06-28 14:53:07,410 main.py:51] epoch 2576, training loss: 9722.16, average training loss: 10297.30, base loss: 14390.81
[INFO 2017-06-28 14:53:08,035 main.py:51] epoch 2577, training loss: 11047.38, average training loss: 10298.00, base loss: 14393.45
[INFO 2017-06-28 14:53:08,711 main.py:51] epoch 2578, training loss: 10324.11, average training loss: 10298.29, base loss: 14395.35
[INFO 2017-06-28 14:53:09,542 main.py:51] epoch 2579, training loss: 11442.15, average training loss: 10300.52, base loss: 14398.43
[INFO 2017-06-28 14:53:10,258 main.py:51] epoch 2580, training loss: 11481.66, average training loss: 10300.77, base loss: 14399.37
[INFO 2017-06-28 14:53:10,981 main.py:51] epoch 2581, training loss: 9440.03, average training loss: 10297.55, base loss: 14396.57
[INFO 2017-06-28 14:53:11,770 main.py:51] epoch 2582, training loss: 8933.63, average training loss: 10295.07, base loss: 14394.42
[INFO 2017-06-28 14:53:12,603 main.py:51] epoch 2583, training loss: 8448.95, average training loss: 10293.32, base loss: 14392.56
[INFO 2017-06-28 14:53:13,230 main.py:51] epoch 2584, training loss: 9911.91, average training loss: 10292.73, base loss: 14392.45
[INFO 2017-06-28 14:53:14,030 main.py:51] epoch 2585, training loss: 10595.49, average training loss: 10292.54, base loss: 14392.70
[INFO 2017-06-28 14:53:14,836 main.py:51] epoch 2586, training loss: 10722.83, average training loss: 10293.02, base loss: 14393.57
[INFO 2017-06-28 14:53:15,600 main.py:51] epoch 2587, training loss: 10514.46, average training loss: 10291.50, base loss: 14391.16
[INFO 2017-06-28 14:53:16,270 main.py:51] epoch 2588, training loss: 10332.66, average training loss: 10291.33, base loss: 14390.38
[INFO 2017-06-28 14:53:17,044 main.py:51] epoch 2589, training loss: 9750.70, average training loss: 10290.54, base loss: 14387.78
[INFO 2017-06-28 14:53:17,828 main.py:51] epoch 2590, training loss: 9972.83, average training loss: 10289.26, base loss: 14386.38
[INFO 2017-06-28 14:53:18,484 main.py:51] epoch 2591, training loss: 9886.63, average training loss: 10288.16, base loss: 14383.82
[INFO 2017-06-28 14:53:19,302 main.py:51] epoch 2592, training loss: 11624.80, average training loss: 10290.30, base loss: 14386.09
[INFO 2017-06-28 14:53:20,118 main.py:51] epoch 2593, training loss: 8866.14, average training loss: 10287.70, base loss: 14383.18
[INFO 2017-06-28 14:53:20,799 main.py:51] epoch 2594, training loss: 9131.66, average training loss: 10284.52, base loss: 14378.33
[INFO 2017-06-28 14:53:21,519 main.py:51] epoch 2595, training loss: 9927.51, average training loss: 10284.13, base loss: 14378.88
[INFO 2017-06-28 14:53:22,339 main.py:51] epoch 2596, training loss: 9616.47, average training loss: 10284.36, base loss: 14380.10
[INFO 2017-06-28 14:53:23,079 main.py:51] epoch 2597, training loss: 10164.66, average training loss: 10281.78, base loss: 14378.11
[INFO 2017-06-28 14:53:23,759 main.py:51] epoch 2598, training loss: 13005.80, average training loss: 10283.97, base loss: 14381.12
[INFO 2017-06-28 14:53:24,555 main.py:51] epoch 2599, training loss: 10264.48, average training loss: 10284.75, base loss: 14382.26
[INFO 2017-06-28 14:53:24,555 main.py:53] epoch 2599, testing
[INFO 2017-06-28 14:53:27,418 main.py:105] average testing loss: 11179.22, base loss: 14650.47
[INFO 2017-06-28 14:53:27,418 main.py:106] improve_loss: 3471.25, improve_percent: 0.24
[INFO 2017-06-28 14:53:27,419 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 14:53:28,148 main.py:51] epoch 2600, training loss: 8909.77, average training loss: 10282.93, base loss: 14381.71
[INFO 2017-06-28 14:53:28,856 main.py:51] epoch 2601, training loss: 9465.69, average training loss: 10281.82, base loss: 14381.53
[INFO 2017-06-28 14:53:29,645 main.py:51] epoch 2602, training loss: 10540.96, average training loss: 10280.06, base loss: 14378.07
[INFO 2017-06-28 14:53:30,438 main.py:51] epoch 2603, training loss: 10293.60, average training loss: 10279.15, base loss: 14376.98
[INFO 2017-06-28 14:53:31,120 main.py:51] epoch 2604, training loss: 9560.18, average training loss: 10278.27, base loss: 14377.64
[INFO 2017-06-28 14:53:31,882 main.py:51] epoch 2605, training loss: 10512.53, average training loss: 10278.00, base loss: 14375.75
[INFO 2017-06-28 14:53:32,724 main.py:51] epoch 2606, training loss: 11100.24, average training loss: 10279.01, base loss: 14376.91
[INFO 2017-06-28 14:53:33,412 main.py:51] epoch 2607, training loss: 11434.25, average training loss: 10280.53, base loss: 14378.69
[INFO 2017-06-28 14:53:34,133 main.py:51] epoch 2608, training loss: 11040.66, average training loss: 10282.19, base loss: 14379.57
[INFO 2017-06-28 14:53:34,954 main.py:51] epoch 2609, training loss: 11211.47, average training loss: 10281.79, base loss: 14378.16
[INFO 2017-06-28 14:53:35,770 main.py:51] epoch 2610, training loss: 12051.54, average training loss: 10284.27, base loss: 14381.09
[INFO 2017-06-28 14:53:36,411 main.py:51] epoch 2611, training loss: 9634.31, average training loss: 10284.59, base loss: 14382.08
[INFO 2017-06-28 14:53:37,215 main.py:51] epoch 2612, training loss: 9884.90, average training loss: 10285.32, base loss: 14384.44
[INFO 2017-06-28 14:53:38,029 main.py:51] epoch 2613, training loss: 11286.94, average training loss: 10288.28, base loss: 14389.71
[INFO 2017-06-28 14:53:38,699 main.py:51] epoch 2614, training loss: 9610.54, average training loss: 10286.99, base loss: 14388.28
[INFO 2017-06-28 14:53:39,459 main.py:51] epoch 2615, training loss: 11072.73, average training loss: 10287.20, base loss: 14389.12
[INFO 2017-06-28 14:53:40,270 main.py:51] epoch 2616, training loss: 10566.43, average training loss: 10284.93, base loss: 14384.79
[INFO 2017-06-28 14:53:41,027 main.py:51] epoch 2617, training loss: 9846.17, average training loss: 10284.40, base loss: 14384.56
[INFO 2017-06-28 14:53:41,687 main.py:51] epoch 2618, training loss: 11076.90, average training loss: 10284.12, base loss: 14383.84
[INFO 2017-06-28 14:53:42,486 main.py:51] epoch 2619, training loss: 9223.70, average training loss: 10283.89, base loss: 14384.00
[INFO 2017-06-28 14:53:43,281 main.py:51] epoch 2620, training loss: 9983.09, average training loss: 10283.19, base loss: 14384.83
[INFO 2017-06-28 14:53:43,944 main.py:51] epoch 2621, training loss: 9936.62, average training loss: 10283.74, base loss: 14386.16
[INFO 2017-06-28 14:53:44,718 main.py:51] epoch 2622, training loss: 11551.70, average training loss: 10284.49, base loss: 14386.61
[INFO 2017-06-28 14:53:45,531 main.py:51] epoch 2623, training loss: 11087.53, average training loss: 10284.29, base loss: 14386.22
[INFO 2017-06-28 14:53:46,185 main.py:51] epoch 2624, training loss: 11737.27, average training loss: 10286.03, base loss: 14387.25
[INFO 2017-06-28 14:53:46,948 main.py:51] epoch 2625, training loss: 11712.44, average training loss: 10286.45, base loss: 14387.87
[INFO 2017-06-28 14:53:47,778 main.py:51] epoch 2626, training loss: 9404.82, average training loss: 10285.78, base loss: 14388.09
[INFO 2017-06-28 14:53:48,486 main.py:51] epoch 2627, training loss: 10192.25, average training loss: 10286.64, base loss: 14389.59
[INFO 2017-06-28 14:53:49,186 main.py:51] epoch 2628, training loss: 9556.28, average training loss: 10286.89, base loss: 14391.93
[INFO 2017-06-28 14:53:49,991 main.py:51] epoch 2629, training loss: 10329.09, average training loss: 10286.40, base loss: 14393.61
[INFO 2017-06-28 14:53:50,844 main.py:51] epoch 2630, training loss: 9793.94, average training loss: 10285.98, base loss: 14391.56
[INFO 2017-06-28 14:53:51,544 main.py:51] epoch 2631, training loss: 12183.15, average training loss: 10286.69, base loss: 14393.19
[INFO 2017-06-28 14:53:52,249 main.py:51] epoch 2632, training loss: 9708.21, average training loss: 10286.45, base loss: 14393.65
[INFO 2017-06-28 14:53:53,083 main.py:51] epoch 2633, training loss: 9799.28, average training loss: 10286.35, base loss: 14393.93
[INFO 2017-06-28 14:53:53,874 main.py:51] epoch 2634, training loss: 10609.60, average training loss: 10286.21, base loss: 14394.03
[INFO 2017-06-28 14:53:54,568 main.py:51] epoch 2635, training loss: 11416.49, average training loss: 10287.04, base loss: 14395.29
[INFO 2017-06-28 14:53:55,381 main.py:51] epoch 2636, training loss: 10972.64, average training loss: 10285.01, base loss: 14392.91
[INFO 2017-06-28 14:53:56,208 main.py:51] epoch 2637, training loss: 9501.28, average training loss: 10283.99, base loss: 14390.58
[INFO 2017-06-28 14:53:56,996 main.py:51] epoch 2638, training loss: 10509.51, average training loss: 10284.06, base loss: 14390.81
[INFO 2017-06-28 14:53:57,628 main.py:51] epoch 2639, training loss: 10483.19, average training loss: 10283.71, base loss: 14390.71
[INFO 2017-06-28 14:53:58,442 main.py:51] epoch 2640, training loss: 9789.84, average training loss: 10282.17, base loss: 14389.11
[INFO 2017-06-28 14:53:59,264 main.py:51] epoch 2641, training loss: 9284.71, average training loss: 10282.60, base loss: 14389.41
[INFO 2017-06-28 14:53:59,946 main.py:51] epoch 2642, training loss: 10059.76, average training loss: 10282.69, base loss: 14388.93
[INFO 2017-06-28 14:54:00,683 main.py:51] epoch 2643, training loss: 9296.81, average training loss: 10279.40, base loss: 14386.27
[INFO 2017-06-28 14:54:01,494 main.py:51] epoch 2644, training loss: 10516.29, average training loss: 10279.76, base loss: 14387.50
[INFO 2017-06-28 14:54:02,298 main.py:51] epoch 2645, training loss: 9561.85, average training loss: 10277.03, base loss: 14384.38
[INFO 2017-06-28 14:54:02,951 main.py:51] epoch 2646, training loss: 10224.80, average training loss: 10273.68, base loss: 14381.05
[INFO 2017-06-28 14:54:03,746 main.py:51] epoch 2647, training loss: 9067.88, average training loss: 10272.54, base loss: 14378.28
[INFO 2017-06-28 14:54:04,574 main.py:51] epoch 2648, training loss: 9381.11, average training loss: 10271.46, base loss: 14377.75
[INFO 2017-06-28 14:54:05,353 main.py:51] epoch 2649, training loss: 9006.04, average training loss: 10269.74, base loss: 14374.88
[INFO 2017-06-28 14:54:06,026 main.py:51] epoch 2650, training loss: 10362.93, average training loss: 10268.38, base loss: 14372.97
[INFO 2017-06-28 14:54:06,819 main.py:51] epoch 2651, training loss: 10250.64, average training loss: 10267.89, base loss: 14372.90
[INFO 2017-06-28 14:54:07,625 main.py:51] epoch 2652, training loss: 10090.83, average training loss: 10267.08, base loss: 14372.61
[INFO 2017-06-28 14:54:08,222 main.py:51] epoch 2653, training loss: 10856.56, average training loss: 10267.71, base loss: 14373.42
[INFO 2017-06-28 14:54:09,017 main.py:51] epoch 2654, training loss: 9063.87, average training loss: 10267.76, base loss: 14374.49
[INFO 2017-06-28 14:54:09,847 main.py:51] epoch 2655, training loss: 9647.18, average training loss: 10267.80, base loss: 14374.79
[INFO 2017-06-28 14:54:10,548 main.py:51] epoch 2656, training loss: 10089.10, average training loss: 10267.09, base loss: 14375.53
[INFO 2017-06-28 14:54:11,226 main.py:51] epoch 2657, training loss: 10299.43, average training loss: 10267.19, base loss: 14376.43
[INFO 2017-06-28 14:54:12,085 main.py:51] epoch 2658, training loss: 9959.46, average training loss: 10265.87, base loss: 14375.07
[INFO 2017-06-28 14:54:12,776 main.py:51] epoch 2659, training loss: 10684.90, average training loss: 10266.00, base loss: 14376.50
[INFO 2017-06-28 14:54:13,489 main.py:51] epoch 2660, training loss: 10313.83, average training loss: 10264.97, base loss: 14374.32
[INFO 2017-06-28 14:54:14,296 main.py:51] epoch 2661, training loss: 10188.06, average training loss: 10265.88, base loss: 14376.07
[INFO 2017-06-28 14:54:15,121 main.py:51] epoch 2662, training loss: 11187.74, average training loss: 10267.39, base loss: 14379.63
[INFO 2017-06-28 14:54:15,770 main.py:51] epoch 2663, training loss: 10225.86, average training loss: 10266.03, base loss: 14378.18
[INFO 2017-06-28 14:54:16,523 main.py:51] epoch 2664, training loss: 9621.52, average training loss: 10264.00, base loss: 14375.61
[INFO 2017-06-28 14:54:17,347 main.py:51] epoch 2665, training loss: 10152.71, average training loss: 10263.16, base loss: 14372.88
[INFO 2017-06-28 14:54:18,059 main.py:51] epoch 2666, training loss: 10163.98, average training loss: 10263.15, base loss: 14373.15
[INFO 2017-06-28 14:54:18,804 main.py:51] epoch 2667, training loss: 10162.78, average training loss: 10262.73, base loss: 14374.33
[INFO 2017-06-28 14:54:19,599 main.py:51] epoch 2668, training loss: 9805.89, average training loss: 10263.40, base loss: 14374.86
[INFO 2017-06-28 14:54:20,422 main.py:51] epoch 2669, training loss: 10646.09, average training loss: 10262.78, base loss: 14373.92
[INFO 2017-06-28 14:54:21,039 main.py:51] epoch 2670, training loss: 8205.71, average training loss: 10259.89, base loss: 14370.63
[INFO 2017-06-28 14:54:21,830 main.py:51] epoch 2671, training loss: 10654.19, average training loss: 10259.85, base loss: 14371.32
[INFO 2017-06-28 14:54:22,655 main.py:51] epoch 2672, training loss: 8858.11, average training loss: 10258.00, base loss: 14369.76
[INFO 2017-06-28 14:54:23,417 main.py:51] epoch 2673, training loss: 10178.88, average training loss: 10255.93, base loss: 14366.05
[INFO 2017-06-28 14:54:24,112 main.py:51] epoch 2674, training loss: 10674.21, average training loss: 10257.02, base loss: 14366.90
[INFO 2017-06-28 14:54:24,875 main.py:51] epoch 2675, training loss: 10300.62, average training loss: 10256.78, base loss: 14367.56
[INFO 2017-06-28 14:54:25,698 main.py:51] epoch 2676, training loss: 9868.11, average training loss: 10255.37, base loss: 14368.10
[INFO 2017-06-28 14:54:26,329 main.py:51] epoch 2677, training loss: 9008.69, average training loss: 10252.43, base loss: 14366.34
[INFO 2017-06-28 14:54:27,145 main.py:51] epoch 2678, training loss: 10192.04, average training loss: 10251.23, base loss: 14366.12
[INFO 2017-06-28 14:54:27,974 main.py:51] epoch 2679, training loss: 10671.90, average training loss: 10252.21, base loss: 14368.22
[INFO 2017-06-28 14:54:28,817 main.py:51] epoch 2680, training loss: 9985.10, average training loss: 10251.53, base loss: 14367.73
[INFO 2017-06-28 14:54:29,468 main.py:51] epoch 2681, training loss: 9472.58, average training loss: 10251.87, base loss: 14369.88
[INFO 2017-06-28 14:54:30,245 main.py:51] epoch 2682, training loss: 11120.04, average training loss: 10252.99, base loss: 14370.45
[INFO 2017-06-28 14:54:31,088 main.py:51] epoch 2683, training loss: 11711.69, average training loss: 10253.53, base loss: 14370.53
[INFO 2017-06-28 14:54:31,794 main.py:51] epoch 2684, training loss: 10632.78, average training loss: 10254.62, base loss: 14372.50
[INFO 2017-06-28 14:54:32,504 main.py:51] epoch 2685, training loss: 9157.17, average training loss: 10254.02, base loss: 14371.71
[INFO 2017-06-28 14:54:33,329 main.py:51] epoch 2686, training loss: 9759.00, average training loss: 10253.04, base loss: 14369.84
[INFO 2017-06-28 14:54:34,083 main.py:51] epoch 2687, training loss: 8802.19, average training loss: 10250.47, base loss: 14364.82
[INFO 2017-06-28 14:54:34,742 main.py:51] epoch 2688, training loss: 9886.11, average training loss: 10250.76, base loss: 14365.98
[INFO 2017-06-28 14:54:35,533 main.py:51] epoch 2689, training loss: 9711.44, average training loss: 10251.51, base loss: 14367.20
[INFO 2017-06-28 14:54:36,371 main.py:51] epoch 2690, training loss: 9856.51, average training loss: 10250.97, base loss: 14367.66
[INFO 2017-06-28 14:54:37,026 main.py:51] epoch 2691, training loss: 8690.74, average training loss: 10250.07, base loss: 14366.74
[INFO 2017-06-28 14:54:37,793 main.py:51] epoch 2692, training loss: 8834.01, average training loss: 10247.34, base loss: 14363.13
[INFO 2017-06-28 14:54:38,622 main.py:51] epoch 2693, training loss: 11264.99, average training loss: 10249.60, base loss: 14366.18
[INFO 2017-06-28 14:54:39,391 main.py:51] epoch 2694, training loss: 10113.45, average training loss: 10248.37, base loss: 14363.93
[INFO 2017-06-28 14:54:40,116 main.py:51] epoch 2695, training loss: 10237.34, average training loss: 10247.68, base loss: 14364.61
[INFO 2017-06-28 14:54:40,874 main.py:51] epoch 2696, training loss: 10344.08, average training loss: 10246.75, base loss: 14363.06
[INFO 2017-06-28 14:54:41,706 main.py:51] epoch 2697, training loss: 10269.22, average training loss: 10247.80, base loss: 14365.32
[INFO 2017-06-28 14:54:42,384 main.py:51] epoch 2698, training loss: 9717.52, average training loss: 10248.17, base loss: 14365.71
[INFO 2017-06-28 14:54:43,136 main.py:51] epoch 2699, training loss: 10446.02, average training loss: 10247.37, base loss: 14364.17
[INFO 2017-06-28 14:54:43,136 main.py:53] epoch 2699, testing
[INFO 2017-06-28 14:54:46,008 main.py:105] average testing loss: 10970.08, base loss: 14992.52
[INFO 2017-06-28 14:54:46,008 main.py:106] improve_loss: 4022.44, improve_percent: 0.27
[INFO 2017-06-28 14:54:46,009 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:54:46,025 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 14:54:46,810 main.py:51] epoch 2700, training loss: 11014.56, average training loss: 10247.31, base loss: 14363.78
[INFO 2017-06-28 14:54:47,627 main.py:51] epoch 2701, training loss: 9176.48, average training loss: 10247.86, base loss: 14365.01
[INFO 2017-06-28 14:54:48,292 main.py:51] epoch 2702, training loss: 8832.99, average training loss: 10246.67, base loss: 14364.72
[INFO 2017-06-28 14:54:49,057 main.py:51] epoch 2703, training loss: 10558.82, average training loss: 10247.97, base loss: 14366.84
[INFO 2017-06-28 14:54:49,885 main.py:51] epoch 2704, training loss: 11757.39, average training loss: 10249.60, base loss: 14368.78
[INFO 2017-06-28 14:54:50,710 main.py:51] epoch 2705, training loss: 9877.96, average training loss: 10249.59, base loss: 14367.46
[INFO 2017-06-28 14:54:51,350 main.py:51] epoch 2706, training loss: 10843.80, average training loss: 10249.90, base loss: 14367.83
[INFO 2017-06-28 14:54:52,143 main.py:51] epoch 2707, training loss: 8852.11, average training loss: 10247.22, base loss: 14364.77
[INFO 2017-06-28 14:54:52,990 main.py:51] epoch 2708, training loss: 9498.78, average training loss: 10244.97, base loss: 14362.30
[INFO 2017-06-28 14:54:53,733 main.py:51] epoch 2709, training loss: 10764.41, average training loss: 10245.00, base loss: 14363.51
[INFO 2017-06-28 14:54:54,424 main.py:51] epoch 2710, training loss: 10173.63, average training loss: 10246.37, base loss: 14366.58
[INFO 2017-06-28 14:54:55,101 main.py:51] epoch 2711, training loss: 11858.11, average training loss: 10246.90, base loss: 14368.78
[INFO 2017-06-28 14:54:55,783 main.py:51] epoch 2712, training loss: 10076.49, average training loss: 10247.04, base loss: 14370.45
[INFO 2017-06-28 14:54:56,397 main.py:51] epoch 2713, training loss: 9899.59, average training loss: 10246.11, base loss: 14370.10
[INFO 2017-06-28 14:54:57,022 main.py:51] epoch 2714, training loss: 12216.68, average training loss: 10246.79, base loss: 14370.02
[INFO 2017-06-28 14:54:57,685 main.py:51] epoch 2715, training loss: 10039.19, average training loss: 10248.16, base loss: 14372.18
[INFO 2017-06-28 14:54:58,376 main.py:51] epoch 2716, training loss: 11031.36, average training loss: 10249.26, base loss: 14374.50
[INFO 2017-06-28 14:54:59,121 main.py:51] epoch 2717, training loss: 9793.80, average training loss: 10248.11, base loss: 14371.37
[INFO 2017-06-28 14:54:59,954 main.py:51] epoch 2718, training loss: 9952.99, average training loss: 10249.64, base loss: 14374.76
[INFO 2017-06-28 14:55:00,663 main.py:51] epoch 2719, training loss: 9830.00, average training loss: 10248.03, base loss: 14374.22
[INFO 2017-06-28 14:55:01,370 main.py:51] epoch 2720, training loss: 12115.39, average training loss: 10249.42, base loss: 14376.57
[INFO 2017-06-28 14:55:02,168 main.py:51] epoch 2721, training loss: 9501.86, average training loss: 10249.11, base loss: 14376.33
[INFO 2017-06-28 14:55:02,945 main.py:51] epoch 2722, training loss: 9922.23, average training loss: 10248.50, base loss: 14375.56
[INFO 2017-06-28 14:55:03,588 main.py:51] epoch 2723, training loss: 9239.42, average training loss: 10247.86, base loss: 14374.43
[INFO 2017-06-28 14:55:04,370 main.py:51] epoch 2724, training loss: 10717.54, average training loss: 10249.33, base loss: 14376.06
[INFO 2017-06-28 14:55:05,204 main.py:51] epoch 2725, training loss: 10008.12, average training loss: 10248.93, base loss: 14375.49
[INFO 2017-06-28 14:55:05,878 main.py:51] epoch 2726, training loss: 9239.62, average training loss: 10247.71, base loss: 14374.99
[INFO 2017-06-28 14:55:06,620 main.py:51] epoch 2727, training loss: 10035.94, average training loss: 10247.81, base loss: 14374.62
[INFO 2017-06-28 14:55:07,452 main.py:51] epoch 2728, training loss: 9779.47, average training loss: 10248.66, base loss: 14375.99
[INFO 2017-06-28 14:55:08,294 main.py:51] epoch 2729, training loss: 10560.75, average training loss: 10248.11, base loss: 14375.70
[INFO 2017-06-28 14:55:08,922 main.py:51] epoch 2730, training loss: 9364.53, average training loss: 10248.42, base loss: 14375.18
[INFO 2017-06-28 14:55:09,714 main.py:51] epoch 2731, training loss: 9559.04, average training loss: 10248.63, base loss: 14376.49
[INFO 2017-06-28 14:55:10,528 main.py:51] epoch 2732, training loss: 11360.78, average training loss: 10248.69, base loss: 14376.90
[INFO 2017-06-28 14:55:11,221 main.py:51] epoch 2733, training loss: 10504.23, average training loss: 10248.12, base loss: 14375.68
[INFO 2017-06-28 14:55:11,954 main.py:51] epoch 2734, training loss: 9546.06, average training loss: 10246.62, base loss: 14374.49
[INFO 2017-06-28 14:55:12,775 main.py:51] epoch 2735, training loss: 10008.17, average training loss: 10246.39, base loss: 14372.79
[INFO 2017-06-28 14:55:13,525 main.py:51] epoch 2736, training loss: 10722.18, average training loss: 10248.06, base loss: 14374.16
[INFO 2017-06-28 14:55:14,235 main.py:51] epoch 2737, training loss: 9786.14, average training loss: 10247.95, base loss: 14373.37
[INFO 2017-06-28 14:55:15,007 main.py:51] epoch 2738, training loss: 10504.44, average training loss: 10247.67, base loss: 14372.78
[INFO 2017-06-28 14:55:15,854 main.py:51] epoch 2739, training loss: 9954.17, average training loss: 10247.72, base loss: 14372.84
[INFO 2017-06-28 14:55:16,541 main.py:51] epoch 2740, training loss: 10342.65, average training loss: 10246.49, base loss: 14371.74
[INFO 2017-06-28 14:55:17,263 main.py:51] epoch 2741, training loss: 9908.07, average training loss: 10244.90, base loss: 14370.52
[INFO 2017-06-28 14:55:18,104 main.py:51] epoch 2742, training loss: 8934.50, average training loss: 10244.49, base loss: 14369.38
[INFO 2017-06-28 14:55:18,955 main.py:51] epoch 2743, training loss: 9385.14, average training loss: 10245.05, base loss: 14370.53
[INFO 2017-06-28 14:55:19,633 main.py:51] epoch 2744, training loss: 8871.74, average training loss: 10243.69, base loss: 14367.80
[INFO 2017-06-28 14:55:20,430 main.py:51] epoch 2745, training loss: 11164.63, average training loss: 10245.37, base loss: 14369.26
[INFO 2017-06-28 14:55:21,254 main.py:51] epoch 2746, training loss: 9275.66, average training loss: 10245.06, base loss: 14368.07
[INFO 2017-06-28 14:55:22,053 main.py:51] epoch 2747, training loss: 9356.54, average training loss: 10243.91, base loss: 14366.69
[INFO 2017-06-28 14:55:22,718 main.py:51] epoch 2748, training loss: 9140.23, average training loss: 10242.88, base loss: 14365.71
[INFO 2017-06-28 14:55:23,489 main.py:51] epoch 2749, training loss: 9233.30, average training loss: 10241.85, base loss: 14364.96
[INFO 2017-06-28 14:55:24,306 main.py:51] epoch 2750, training loss: 9080.78, average training loss: 10239.96, base loss: 14362.89
[INFO 2017-06-28 14:55:24,969 main.py:51] epoch 2751, training loss: 11161.18, average training loss: 10241.73, base loss: 14364.78
[INFO 2017-06-28 14:55:25,746 main.py:51] epoch 2752, training loss: 10311.35, average training loss: 10241.29, base loss: 14363.35
[INFO 2017-06-28 14:55:26,561 main.py:51] epoch 2753, training loss: 10015.42, average training loss: 10239.93, base loss: 14360.37
[INFO 2017-06-28 14:55:27,342 main.py:51] epoch 2754, training loss: 9135.39, average training loss: 10238.87, base loss: 14358.90
[INFO 2017-06-28 14:55:27,945 main.py:51] epoch 2755, training loss: 10165.39, average training loss: 10239.17, base loss: 14358.82
[INFO 2017-06-28 14:55:28,739 main.py:51] epoch 2756, training loss: 9691.88, average training loss: 10238.91, base loss: 14358.64
[INFO 2017-06-28 14:55:29,543 main.py:51] epoch 2757, training loss: 10733.48, average training loss: 10238.21, base loss: 14357.50
[INFO 2017-06-28 14:55:30,169 main.py:51] epoch 2758, training loss: 8655.21, average training loss: 10236.56, base loss: 14353.93
[INFO 2017-06-28 14:55:30,947 main.py:51] epoch 2759, training loss: 10541.70, average training loss: 10234.81, base loss: 14351.85
[INFO 2017-06-28 14:55:31,767 main.py:51] epoch 2760, training loss: 9071.92, average training loss: 10231.78, base loss: 14348.52
[INFO 2017-06-28 14:55:32,481 main.py:51] epoch 2761, training loss: 10140.53, average training loss: 10231.88, base loss: 14349.17
[INFO 2017-06-28 14:55:33,200 main.py:51] epoch 2762, training loss: 9457.37, average training loss: 10230.97, base loss: 14347.76
[INFO 2017-06-28 14:55:33,977 main.py:51] epoch 2763, training loss: 10378.33, average training loss: 10231.48, base loss: 14348.97
[INFO 2017-06-28 14:55:34,817 main.py:51] epoch 2764, training loss: 10977.54, average training loss: 10232.44, base loss: 14349.60
[INFO 2017-06-28 14:55:35,503 main.py:51] epoch 2765, training loss: 9597.98, average training loss: 10231.61, base loss: 14348.50
[INFO 2017-06-28 14:55:36,260 main.py:51] epoch 2766, training loss: 9436.58, average training loss: 10229.21, base loss: 14345.15
[INFO 2017-06-28 14:55:37,086 main.py:51] epoch 2767, training loss: 10008.57, average training loss: 10226.96, base loss: 14342.58
[INFO 2017-06-28 14:55:37,914 main.py:51] epoch 2768, training loss: 10397.51, average training loss: 10226.26, base loss: 14340.89
[INFO 2017-06-28 14:55:38,578 main.py:51] epoch 2769, training loss: 10676.99, average training loss: 10224.49, base loss: 14339.66
[INFO 2017-06-28 14:55:39,347 main.py:51] epoch 2770, training loss: 10541.71, average training loss: 10224.27, base loss: 14339.29
[INFO 2017-06-28 14:55:40,159 main.py:51] epoch 2771, training loss: 9955.47, average training loss: 10223.53, base loss: 14338.51
[INFO 2017-06-28 14:55:40,871 main.py:51] epoch 2772, training loss: 10088.08, average training loss: 10222.73, base loss: 14337.60
[INFO 2017-06-28 14:55:41,570 main.py:51] epoch 2773, training loss: 10024.95, average training loss: 10221.61, base loss: 14335.83
[INFO 2017-06-28 14:55:42,387 main.py:51] epoch 2774, training loss: 11066.96, average training loss: 10221.97, base loss: 14336.49
[INFO 2017-06-28 14:55:43,223 main.py:51] epoch 2775, training loss: 9298.68, average training loss: 10221.65, base loss: 14336.76
[INFO 2017-06-28 14:55:43,852 main.py:51] epoch 2776, training loss: 10968.41, average training loss: 10222.32, base loss: 14337.64
[INFO 2017-06-28 14:55:44,592 main.py:51] epoch 2777, training loss: 9847.91, average training loss: 10221.82, base loss: 14338.06
[INFO 2017-06-28 14:55:45,422 main.py:51] epoch 2778, training loss: 9435.61, average training loss: 10220.66, base loss: 14335.99
[INFO 2017-06-28 14:55:46,108 main.py:51] epoch 2779, training loss: 9566.92, average training loss: 10220.05, base loss: 14335.88
[INFO 2017-06-28 14:55:46,836 main.py:51] epoch 2780, training loss: 8665.14, average training loss: 10218.71, base loss: 14333.87
[INFO 2017-06-28 14:55:47,675 main.py:51] epoch 2781, training loss: 9902.88, average training loss: 10217.76, base loss: 14334.30
[INFO 2017-06-28 14:55:48,410 main.py:51] epoch 2782, training loss: 9552.70, average training loss: 10215.48, base loss: 14331.42
[INFO 2017-06-28 14:55:49,096 main.py:51] epoch 2783, training loss: 10968.19, average training loss: 10215.70, base loss: 14331.00
[INFO 2017-06-28 14:55:49,920 main.py:51] epoch 2784, training loss: 9382.14, average training loss: 10214.31, base loss: 14329.51
[INFO 2017-06-28 14:55:50,739 main.py:51] epoch 2785, training loss: 11460.58, average training loss: 10215.68, base loss: 14330.04
[INFO 2017-06-28 14:55:51,356 main.py:51] epoch 2786, training loss: 9372.24, average training loss: 10213.70, base loss: 14327.84
[INFO 2017-06-28 14:55:52,120 main.py:51] epoch 2787, training loss: 8631.32, average training loss: 10211.87, base loss: 14325.59
[INFO 2017-06-28 14:55:52,956 main.py:51] epoch 2788, training loss: 9114.94, average training loss: 10210.44, base loss: 14323.59
[INFO 2017-06-28 14:55:53,557 main.py:51] epoch 2789, training loss: 10410.50, average training loss: 10210.80, base loss: 14322.79
[INFO 2017-06-28 14:55:54,353 main.py:51] epoch 2790, training loss: 10646.41, average training loss: 10212.11, base loss: 14324.73
[INFO 2017-06-28 14:55:55,167 main.py:51] epoch 2791, training loss: 10150.89, average training loss: 10209.51, base loss: 14323.28
[INFO 2017-06-28 14:55:55,893 main.py:51] epoch 2792, training loss: 9441.81, average training loss: 10207.68, base loss: 14320.15
[INFO 2017-06-28 14:55:56,584 main.py:51] epoch 2793, training loss: 11308.93, average training loss: 10210.51, base loss: 14325.00
[INFO 2017-06-28 14:55:57,404 main.py:51] epoch 2794, training loss: 9642.28, average training loss: 10210.13, base loss: 14325.86
[INFO 2017-06-28 14:55:58,239 main.py:51] epoch 2795, training loss: 8871.92, average training loss: 10209.61, base loss: 14325.58
[INFO 2017-06-28 14:55:58,884 main.py:51] epoch 2796, training loss: 9367.50, average training loss: 10207.66, base loss: 14323.51
[INFO 2017-06-28 14:55:59,670 main.py:51] epoch 2797, training loss: 9533.20, average training loss: 10206.03, base loss: 14321.99
[INFO 2017-06-28 14:56:00,514 main.py:51] epoch 2798, training loss: 9868.18, average training loss: 10205.69, base loss: 14322.95
[INFO 2017-06-28 14:56:01,373 main.py:51] epoch 2799, training loss: 9512.18, average training loss: 10206.06, base loss: 14323.09
[INFO 2017-06-28 14:56:01,374 main.py:53] epoch 2799, testing
[INFO 2017-06-28 14:56:04,332 main.py:105] average testing loss: 11608.77, base loss: 15468.72
[INFO 2017-06-28 14:56:04,332 main.py:106] improve_loss: 3859.95, improve_percent: 0.25
[INFO 2017-06-28 14:56:04,333 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 14:56:04,927 main.py:51] epoch 2800, training loss: 12536.97, average training loss: 10208.35, base loss: 14327.28
[INFO 2017-06-28 14:56:05,736 main.py:51] epoch 2801, training loss: 9503.43, average training loss: 10204.92, base loss: 14324.15
[INFO 2017-06-28 14:56:06,543 main.py:51] epoch 2802, training loss: 9863.12, average training loss: 10204.28, base loss: 14322.10
[INFO 2017-06-28 14:56:07,378 main.py:51] epoch 2803, training loss: 9805.05, average training loss: 10204.04, base loss: 14323.32
[INFO 2017-06-28 14:56:08,012 main.py:51] epoch 2804, training loss: 9489.28, average training loss: 10202.93, base loss: 14321.62
[INFO 2017-06-28 14:56:08,768 main.py:51] epoch 2805, training loss: 10565.04, average training loss: 10201.86, base loss: 14320.23
[INFO 2017-06-28 14:56:09,578 main.py:51] epoch 2806, training loss: 9216.91, average training loss: 10200.66, base loss: 14318.67
[INFO 2017-06-28 14:56:10,286 main.py:51] epoch 2807, training loss: 9485.13, average training loss: 10201.07, base loss: 14318.84
[INFO 2017-06-28 14:56:11,006 main.py:51] epoch 2808, training loss: 10556.78, average training loss: 10201.70, base loss: 14320.60
[INFO 2017-06-28 14:56:11,820 main.py:51] epoch 2809, training loss: 10455.36, average training loss: 10202.04, base loss: 14320.20
[INFO 2017-06-28 14:56:12,671 main.py:51] epoch 2810, training loss: 9091.27, average training loss: 10202.66, base loss: 14320.28
[INFO 2017-06-28 14:56:13,307 main.py:51] epoch 2811, training loss: 9152.07, average training loss: 10202.39, base loss: 14319.60
[INFO 2017-06-28 14:56:14,112 main.py:51] epoch 2812, training loss: 9558.91, average training loss: 10199.55, base loss: 14315.97
[INFO 2017-06-28 14:56:14,942 main.py:51] epoch 2813, training loss: 11565.28, average training loss: 10201.47, base loss: 14318.86
[INFO 2017-06-28 14:56:15,782 main.py:51] epoch 2814, training loss: 10683.07, average training loss: 10202.00, base loss: 14318.19
[INFO 2017-06-28 14:56:16,455 main.py:51] epoch 2815, training loss: 12024.74, average training loss: 10202.02, base loss: 14319.88
[INFO 2017-06-28 14:56:17,215 main.py:51] epoch 2816, training loss: 10382.73, average training loss: 10202.81, base loss: 14322.28
[INFO 2017-06-28 14:56:18,047 main.py:51] epoch 2817, training loss: 9488.95, average training loss: 10200.12, base loss: 14318.09
[INFO 2017-06-28 14:56:18,820 main.py:51] epoch 2818, training loss: 10497.35, average training loss: 10200.49, base loss: 14319.47
[INFO 2017-06-28 14:56:19,532 main.py:51] epoch 2819, training loss: 10103.36, average training loss: 10199.40, base loss: 14318.49
[INFO 2017-06-28 14:56:20,306 main.py:51] epoch 2820, training loss: 11672.56, average training loss: 10201.42, base loss: 14321.21
[INFO 2017-06-28 14:56:21,138 main.py:51] epoch 2821, training loss: 9532.99, average training loss: 10201.64, base loss: 14323.00
[INFO 2017-06-28 14:56:21,826 main.py:51] epoch 2822, training loss: 9849.79, average training loss: 10201.82, base loss: 14323.63
[INFO 2017-06-28 14:56:22,578 main.py:51] epoch 2823, training loss: 10099.66, average training loss: 10202.36, base loss: 14326.03
[INFO 2017-06-28 14:56:23,390 main.py:51] epoch 2824, training loss: 11368.11, average training loss: 10202.64, base loss: 14328.19
[INFO 2017-06-28 14:56:24,235 main.py:51] epoch 2825, training loss: 11452.64, average training loss: 10203.39, base loss: 14330.78
[INFO 2017-06-28 14:56:24,936 main.py:51] epoch 2826, training loss: 10835.30, average training loss: 10202.34, base loss: 14329.07
[INFO 2017-06-28 14:56:25,643 main.py:51] epoch 2827, training loss: 10634.43, average training loss: 10200.88, base loss: 14326.44
[INFO 2017-06-28 14:56:26,489 main.py:51] epoch 2828, training loss: 9917.39, average training loss: 10201.50, base loss: 14328.07
[INFO 2017-06-28 14:56:27,184 main.py:51] epoch 2829, training loss: 11405.64, average training loss: 10203.20, base loss: 14331.08
[INFO 2017-06-28 14:56:27,906 main.py:51] epoch 2830, training loss: 10235.28, average training loss: 10202.90, base loss: 14333.41
[INFO 2017-06-28 14:56:28,703 main.py:51] epoch 2831, training loss: 9679.69, average training loss: 10201.91, base loss: 14332.08
[INFO 2017-06-28 14:56:29,536 main.py:51] epoch 2832, training loss: 10415.41, average training loss: 10202.02, base loss: 14331.77
[INFO 2017-06-28 14:56:30,163 main.py:51] epoch 2833, training loss: 10738.50, average training loss: 10202.66, base loss: 14333.71
[INFO 2017-06-28 14:56:30,978 main.py:51] epoch 2834, training loss: 9055.26, average training loss: 10202.15, base loss: 14332.91
[INFO 2017-06-28 14:56:31,809 main.py:51] epoch 2835, training loss: 10914.98, average training loss: 10202.46, base loss: 14333.35
[INFO 2017-06-28 14:56:32,560 main.py:51] epoch 2836, training loss: 9788.17, average training loss: 10200.57, base loss: 14332.34
[INFO 2017-06-28 14:56:33,274 main.py:51] epoch 2837, training loss: 10436.62, average training loss: 10200.67, base loss: 14331.45
[INFO 2017-06-28 14:56:34,038 main.py:51] epoch 2838, training loss: 10218.52, average training loss: 10199.80, base loss: 14331.00
[INFO 2017-06-28 14:56:34,828 main.py:51] epoch 2839, training loss: 9420.11, average training loss: 10200.87, base loss: 14332.22
[INFO 2017-06-28 14:56:35,479 main.py:51] epoch 2840, training loss: 10190.19, average training loss: 10200.08, base loss: 14331.87
[INFO 2017-06-28 14:56:36,293 main.py:51] epoch 2841, training loss: 9905.83, average training loss: 10196.73, base loss: 14330.19
[INFO 2017-06-28 14:56:37,110 main.py:51] epoch 2842, training loss: 10287.39, average training loss: 10197.72, base loss: 14333.04
[INFO 2017-06-28 14:56:37,898 main.py:51] epoch 2843, training loss: 9255.65, average training loss: 10198.43, base loss: 14335.00
[INFO 2017-06-28 14:56:38,547 main.py:51] epoch 2844, training loss: 9351.34, average training loss: 10198.27, base loss: 14335.03
[INFO 2017-06-28 14:56:39,358 main.py:51] epoch 2845, training loss: 9279.54, average training loss: 10196.98, base loss: 14333.53
[INFO 2017-06-28 14:56:40,192 main.py:51] epoch 2846, training loss: 9528.82, average training loss: 10196.93, base loss: 14332.40
[INFO 2017-06-28 14:56:40,946 main.py:51] epoch 2847, training loss: 9477.09, average training loss: 10195.87, base loss: 14330.41
[INFO 2017-06-28 14:56:41,628 main.py:51] epoch 2848, training loss: 10165.79, average training loss: 10195.40, base loss: 14329.80
[INFO 2017-06-28 14:56:42,438 main.py:51] epoch 2849, training loss: 12900.80, average training loss: 10199.23, base loss: 14336.15
[INFO 2017-06-28 14:56:43,277 main.py:51] epoch 2850, training loss: 9142.60, average training loss: 10198.87, base loss: 14335.36
[INFO 2017-06-28 14:56:43,902 main.py:51] epoch 2851, training loss: 9474.73, average training loss: 10198.36, base loss: 14334.84
[INFO 2017-06-28 14:56:44,557 main.py:51] epoch 2852, training loss: 9777.03, average training loss: 10198.22, base loss: 14335.20
[INFO 2017-06-28 14:56:45,183 main.py:51] epoch 2853, training loss: 8730.27, average training loss: 10197.23, base loss: 14333.30
[INFO 2017-06-28 14:56:45,787 main.py:51] epoch 2854, training loss: 8992.89, average training loss: 10195.50, base loss: 14331.15
[INFO 2017-06-28 14:56:46,457 main.py:51] epoch 2855, training loss: 10199.86, average training loss: 10194.67, base loss: 14331.07
[INFO 2017-06-28 14:56:47,162 main.py:51] epoch 2856, training loss: 11357.24, average training loss: 10194.52, base loss: 14328.40
[INFO 2017-06-28 14:56:47,839 main.py:51] epoch 2857, training loss: 11057.61, average training loss: 10195.68, base loss: 14331.32
[INFO 2017-06-28 14:56:48,580 main.py:51] epoch 2858, training loss: 10269.58, average training loss: 10196.60, base loss: 14334.84
[INFO 2017-06-28 14:56:49,387 main.py:51] epoch 2859, training loss: 10947.26, average training loss: 10196.51, base loss: 14333.24
[INFO 2017-06-28 14:56:50,207 main.py:51] epoch 2860, training loss: 9380.67, average training loss: 10196.55, base loss: 14332.96
[INFO 2017-06-28 14:56:50,864 main.py:51] epoch 2861, training loss: 9878.21, average training loss: 10194.70, base loss: 14332.71
[INFO 2017-06-28 14:56:51,675 main.py:51] epoch 2862, training loss: 9117.37, average training loss: 10194.23, base loss: 14333.09
[INFO 2017-06-28 14:56:52,484 main.py:51] epoch 2863, training loss: 9613.51, average training loss: 10193.97, base loss: 14333.17
[INFO 2017-06-28 14:56:53,231 main.py:51] epoch 2864, training loss: 10103.11, average training loss: 10194.26, base loss: 14333.99
[INFO 2017-06-28 14:56:53,943 main.py:51] epoch 2865, training loss: 10103.28, average training loss: 10194.39, base loss: 14335.83
[INFO 2017-06-28 14:56:54,724 main.py:51] epoch 2866, training loss: 9342.56, average training loss: 10192.84, base loss: 14334.69
[INFO 2017-06-28 14:56:55,575 main.py:51] epoch 2867, training loss: 11024.39, average training loss: 10194.04, base loss: 14336.95
[INFO 2017-06-28 14:56:56,272 main.py:51] epoch 2868, training loss: 10548.21, average training loss: 10192.94, base loss: 14336.29
[INFO 2017-06-28 14:56:57,027 main.py:51] epoch 2869, training loss: 10377.95, average training loss: 10193.84, base loss: 14339.30
[INFO 2017-06-28 14:56:57,850 main.py:51] epoch 2870, training loss: 10885.95, average training loss: 10193.63, base loss: 14338.99
[INFO 2017-06-28 14:56:58,657 main.py:51] epoch 2871, training loss: 12904.46, average training loss: 10195.68, base loss: 14341.92
[INFO 2017-06-28 14:56:59,295 main.py:51] epoch 2872, training loss: 9914.81, average training loss: 10196.12, base loss: 14342.06
[INFO 2017-06-28 14:57:00,115 main.py:51] epoch 2873, training loss: 12406.08, average training loss: 10198.75, base loss: 14345.02
[INFO 2017-06-28 14:57:00,928 main.py:51] epoch 2874, training loss: 9485.88, average training loss: 10197.72, base loss: 14344.28
[INFO 2017-06-28 14:57:01,676 main.py:51] epoch 2875, training loss: 9764.56, average training loss: 10197.50, base loss: 14343.03
[INFO 2017-06-28 14:57:02,370 main.py:51] epoch 2876, training loss: 9877.81, average training loss: 10195.07, base loss: 14340.78
[INFO 2017-06-28 14:57:03,194 main.py:51] epoch 2877, training loss: 9989.08, average training loss: 10192.87, base loss: 14340.89
[INFO 2017-06-28 14:57:04,010 main.py:51] epoch 2878, training loss: 9910.92, average training loss: 10192.47, base loss: 14341.96
[INFO 2017-06-28 14:57:04,722 main.py:51] epoch 2879, training loss: 11243.55, average training loss: 10194.73, base loss: 14345.45
[INFO 2017-06-28 14:57:05,456 main.py:51] epoch 2880, training loss: 9651.16, average training loss: 10193.39, base loss: 14341.98
[INFO 2017-06-28 14:57:06,259 main.py:51] epoch 2881, training loss: 9867.61, average training loss: 10193.15, base loss: 14343.79
[INFO 2017-06-28 14:57:07,095 main.py:51] epoch 2882, training loss: 9338.73, average training loss: 10193.29, base loss: 14345.00
[INFO 2017-06-28 14:57:07,760 main.py:51] epoch 2883, training loss: 10597.48, average training loss: 10191.93, base loss: 14344.55
[INFO 2017-06-28 14:57:08,534 main.py:51] epoch 2884, training loss: 9555.67, average training loss: 10191.74, base loss: 14345.90
[INFO 2017-06-28 14:57:09,371 main.py:51] epoch 2885, training loss: 11830.05, average training loss: 10193.17, base loss: 14349.97
[INFO 2017-06-28 14:57:10,173 main.py:51] epoch 2886, training loss: 10459.18, average training loss: 10192.09, base loss: 14347.66
[INFO 2017-06-28 14:57:10,846 main.py:51] epoch 2887, training loss: 10907.49, average training loss: 10192.68, base loss: 14348.63
[INFO 2017-06-28 14:57:11,635 main.py:51] epoch 2888, training loss: 8058.61, average training loss: 10188.72, base loss: 14343.87
[INFO 2017-06-28 14:57:12,443 main.py:51] epoch 2889, training loss: 10135.77, average training loss: 10186.84, base loss: 14341.57
[INFO 2017-06-28 14:57:13,131 main.py:51] epoch 2890, training loss: 10347.66, average training loss: 10186.95, base loss: 14343.17
[INFO 2017-06-28 14:57:13,850 main.py:51] epoch 2891, training loss: 8834.90, average training loss: 10184.09, base loss: 14339.94
[INFO 2017-06-28 14:57:14,657 main.py:51] epoch 2892, training loss: 13503.38, average training loss: 10186.62, base loss: 14342.42
[INFO 2017-06-28 14:57:15,463 main.py:51] epoch 2893, training loss: 8806.35, average training loss: 10184.13, base loss: 14338.49
[INFO 2017-06-28 14:57:16,136 main.py:51] epoch 2894, training loss: 10664.78, average training loss: 10184.69, base loss: 14339.69
[INFO 2017-06-28 14:57:16,953 main.py:51] epoch 2895, training loss: 9861.89, average training loss: 10183.56, base loss: 14337.80
[INFO 2017-06-28 14:57:17,782 main.py:51] epoch 2896, training loss: 10793.06, average training loss: 10183.79, base loss: 14337.74
[INFO 2017-06-28 14:57:18,573 main.py:51] epoch 2897, training loss: 9326.87, average training loss: 10183.46, base loss: 14337.95
[INFO 2017-06-28 14:57:19,283 main.py:51] epoch 2898, training loss: 10871.26, average training loss: 10184.48, base loss: 14339.50
[INFO 2017-06-28 14:57:20,090 main.py:51] epoch 2899, training loss: 9902.95, average training loss: 10183.97, base loss: 14339.02
[INFO 2017-06-28 14:57:20,091 main.py:53] epoch 2899, testing
[INFO 2017-06-28 14:57:22,986 main.py:105] average testing loss: 10770.36, base loss: 14463.29
[INFO 2017-06-28 14:57:22,987 main.py:106] improve_loss: 3692.93, improve_percent: 0.26
[INFO 2017-06-28 14:57:22,987 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 14:57:23,785 main.py:51] epoch 2900, training loss: 9293.03, average training loss: 10183.87, base loss: 14339.26
[INFO 2017-06-28 14:57:24,567 main.py:51] epoch 2901, training loss: 11807.88, average training loss: 10184.09, base loss: 14341.29
[INFO 2017-06-28 14:57:25,241 main.py:51] epoch 2902, training loss: 8570.71, average training loss: 10180.95, base loss: 14338.77
[INFO 2017-06-28 14:57:26,017 main.py:51] epoch 2903, training loss: 11416.95, average training loss: 10181.83, base loss: 14338.04
[INFO 2017-06-28 14:57:26,822 main.py:51] epoch 2904, training loss: 9803.44, average training loss: 10181.18, base loss: 14338.09
[INFO 2017-06-28 14:57:27,675 main.py:51] epoch 2905, training loss: 10531.21, average training loss: 10181.59, base loss: 14341.50
[INFO 2017-06-28 14:57:28,355 main.py:51] epoch 2906, training loss: 10705.14, average training loss: 10183.54, base loss: 14344.36
[INFO 2017-06-28 14:57:29,109 main.py:51] epoch 2907, training loss: 9422.39, average training loss: 10183.12, base loss: 14342.92
[INFO 2017-06-28 14:57:29,929 main.py:51] epoch 2908, training loss: 10002.71, average training loss: 10181.81, base loss: 14340.39
[INFO 2017-06-28 14:57:30,695 main.py:51] epoch 2909, training loss: 9963.64, average training loss: 10183.13, base loss: 14342.95
[INFO 2017-06-28 14:57:31,375 main.py:51] epoch 2910, training loss: 10220.54, average training loss: 10182.35, base loss: 14342.26
[INFO 2017-06-28 14:57:32,169 main.py:51] epoch 2911, training loss: 10062.03, average training loss: 10181.68, base loss: 14340.87
[INFO 2017-06-28 14:57:33,005 main.py:51] epoch 2912, training loss: 10211.41, average training loss: 10181.32, base loss: 14341.55
[INFO 2017-06-28 14:57:33,677 main.py:51] epoch 2913, training loss: 9896.17, average training loss: 10182.71, base loss: 14344.18
[INFO 2017-06-28 14:57:34,437 main.py:51] epoch 2914, training loss: 11049.73, average training loss: 10182.68, base loss: 14343.27
[INFO 2017-06-28 14:57:35,233 main.py:51] epoch 2915, training loss: 9632.93, average training loss: 10182.21, base loss: 14343.36
[INFO 2017-06-28 14:57:36,049 main.py:51] epoch 2916, training loss: 10631.47, average training loss: 10182.47, base loss: 14344.67
[INFO 2017-06-28 14:57:36,704 main.py:51] epoch 2917, training loss: 10676.59, average training loss: 10182.12, base loss: 14343.29
[INFO 2017-06-28 14:57:37,483 main.py:51] epoch 2918, training loss: 9695.04, average training loss: 10182.38, base loss: 14343.85
[INFO 2017-06-28 14:57:38,304 main.py:51] epoch 2919, training loss: 10100.35, average training loss: 10181.32, base loss: 14340.76
[INFO 2017-06-28 14:57:39,091 main.py:51] epoch 2920, training loss: 10919.31, average training loss: 10181.95, base loss: 14342.78
[INFO 2017-06-28 14:57:39,755 main.py:51] epoch 2921, training loss: 9344.95, average training loss: 10181.30, base loss: 14341.16
[INFO 2017-06-28 14:57:40,532 main.py:51] epoch 2922, training loss: 11940.55, average training loss: 10182.45, base loss: 14340.97
[INFO 2017-06-28 14:57:41,358 main.py:51] epoch 2923, training loss: 9002.81, average training loss: 10180.56, base loss: 14338.98
[INFO 2017-06-28 14:57:42,075 main.py:51] epoch 2924, training loss: 10752.57, average training loss: 10180.97, base loss: 14340.93
[INFO 2017-06-28 14:57:42,831 main.py:51] epoch 2925, training loss: 10585.57, average training loss: 10179.91, base loss: 14337.97
[INFO 2017-06-28 14:57:43,623 main.py:51] epoch 2926, training loss: 10807.92, average training loss: 10180.46, base loss: 14338.38
[INFO 2017-06-28 14:57:44,467 main.py:51] epoch 2927, training loss: 9902.30, average training loss: 10180.72, base loss: 14338.64
[INFO 2017-06-28 14:57:45,163 main.py:51] epoch 2928, training loss: 9207.66, average training loss: 10181.07, base loss: 14338.46
[INFO 2017-06-28 14:57:45,952 main.py:51] epoch 2929, training loss: 10113.31, average training loss: 10181.03, base loss: 14339.65
[INFO 2017-06-28 14:57:46,759 main.py:51] epoch 2930, training loss: 10919.19, average training loss: 10180.00, base loss: 14339.20
[INFO 2017-06-28 14:57:47,596 main.py:51] epoch 2931, training loss: 10236.98, average training loss: 10181.38, base loss: 14343.11
[INFO 2017-06-28 14:57:48,318 main.py:51] epoch 2932, training loss: 12375.80, average training loss: 10184.53, base loss: 14347.46
[INFO 2017-06-28 14:57:49,044 main.py:51] epoch 2933, training loss: 9514.04, average training loss: 10184.06, base loss: 14347.18
[INFO 2017-06-28 14:57:49,839 main.py:51] epoch 2934, training loss: 9893.68, average training loss: 10183.99, base loss: 14346.99
[INFO 2017-06-28 14:57:50,677 main.py:51] epoch 2935, training loss: 10691.34, average training loss: 10185.17, base loss: 14350.67
[INFO 2017-06-28 14:57:51,354 main.py:51] epoch 2936, training loss: 10926.71, average training loss: 10184.66, base loss: 14350.91
[INFO 2017-06-28 14:57:52,125 main.py:51] epoch 2937, training loss: 9647.72, average training loss: 10183.67, base loss: 14350.69
[INFO 2017-06-28 14:57:52,940 main.py:51] epoch 2938, training loss: 11944.95, average training loss: 10185.51, base loss: 14353.44
[INFO 2017-06-28 14:57:53,791 main.py:51] epoch 2939, training loss: 10294.97, average training loss: 10185.41, base loss: 14352.95
[INFO 2017-06-28 14:57:54,462 main.py:51] epoch 2940, training loss: 11281.85, average training loss: 10187.33, base loss: 14356.17
[INFO 2017-06-28 14:57:55,213 main.py:51] epoch 2941, training loss: 10863.73, average training loss: 10187.77, base loss: 14356.57
[INFO 2017-06-28 14:57:56,026 main.py:51] epoch 2942, training loss: 9963.95, average training loss: 10187.15, base loss: 14356.51
[INFO 2017-06-28 14:57:56,860 main.py:51] epoch 2943, training loss: 10672.18, average training loss: 10187.75, base loss: 14358.02
[INFO 2017-06-28 14:57:57,529 main.py:51] epoch 2944, training loss: 10002.31, average training loss: 10187.18, base loss: 14356.35
[INFO 2017-06-28 14:57:58,347 main.py:51] epoch 2945, training loss: 11230.08, average training loss: 10186.97, base loss: 14356.39
[INFO 2017-06-28 14:57:59,162 main.py:51] epoch 2946, training loss: 9393.66, average training loss: 10186.09, base loss: 14356.16
[INFO 2017-06-28 14:58:00,004 main.py:51] epoch 2947, training loss: 10161.81, average training loss: 10186.66, base loss: 14356.94
[INFO 2017-06-28 14:58:00,630 main.py:51] epoch 2948, training loss: 9048.48, average training loss: 10186.07, base loss: 14356.83
[INFO 2017-06-28 14:58:01,367 main.py:51] epoch 2949, training loss: 9206.54, average training loss: 10185.48, base loss: 14355.26
[INFO 2017-06-28 14:58:02,196 main.py:51] epoch 2950, training loss: 11529.11, average training loss: 10186.74, base loss: 14357.96
[INFO 2017-06-28 14:58:02,945 main.py:51] epoch 2951, training loss: 11138.12, average training loss: 10188.27, base loss: 14359.07
[INFO 2017-06-28 14:58:03,664 main.py:51] epoch 2952, training loss: 10452.71, average training loss: 10189.24, base loss: 14360.46
[INFO 2017-06-28 14:58:04,448 main.py:51] epoch 2953, training loss: 9231.90, average training loss: 10188.82, base loss: 14360.99
[INFO 2017-06-28 14:58:05,270 main.py:51] epoch 2954, training loss: 10505.34, average training loss: 10188.02, base loss: 14359.20
[INFO 2017-06-28 14:58:05,964 main.py:51] epoch 2955, training loss: 10728.01, average training loss: 10187.41, base loss: 14358.01
[INFO 2017-06-28 14:58:06,721 main.py:51] epoch 2956, training loss: 9566.72, average training loss: 10187.57, base loss: 14360.11
[INFO 2017-06-28 14:58:07,543 main.py:51] epoch 2957, training loss: 9256.59, average training loss: 10186.49, base loss: 14358.46
[INFO 2017-06-28 14:58:08,339 main.py:51] epoch 2958, training loss: 10148.88, average training loss: 10183.98, base loss: 14356.18
[INFO 2017-06-28 14:58:08,975 main.py:51] epoch 2959, training loss: 11542.34, average training loss: 10185.45, base loss: 14355.40
[INFO 2017-06-28 14:58:09,754 main.py:51] epoch 2960, training loss: 9348.45, average training loss: 10184.64, base loss: 14354.21
[INFO 2017-06-28 14:58:10,546 main.py:51] epoch 2961, training loss: 11304.68, average training loss: 10184.02, base loss: 14351.65
[INFO 2017-06-28 14:58:11,261 main.py:51] epoch 2962, training loss: 11273.60, average training loss: 10185.97, base loss: 14355.54
[INFO 2017-06-28 14:58:12,006 main.py:51] epoch 2963, training loss: 11282.62, average training loss: 10185.65, base loss: 14354.67
[INFO 2017-06-28 14:58:12,838 main.py:51] epoch 2964, training loss: 9840.23, average training loss: 10183.42, base loss: 14350.24
[INFO 2017-06-28 14:58:13,599 main.py:51] epoch 2965, training loss: 11984.83, average training loss: 10185.29, base loss: 14352.06
[INFO 2017-06-28 14:58:14,287 main.py:51] epoch 2966, training loss: 9123.62, average training loss: 10184.71, base loss: 14350.73
[INFO 2017-06-28 14:58:15,075 main.py:51] epoch 2967, training loss: 10666.35, average training loss: 10185.04, base loss: 14349.65
[INFO 2017-06-28 14:58:15,863 main.py:51] epoch 2968, training loss: 11514.42, average training loss: 10186.99, base loss: 14351.75
[INFO 2017-06-28 14:58:16,486 main.py:51] epoch 2969, training loss: 8707.92, average training loss: 10184.88, base loss: 14348.80
[INFO 2017-06-28 14:58:17,296 main.py:51] epoch 2970, training loss: 8755.47, average training loss: 10183.47, base loss: 14348.07
[INFO 2017-06-28 14:58:18,101 main.py:51] epoch 2971, training loss: 9337.40, average training loss: 10182.37, base loss: 14347.06
[INFO 2017-06-28 14:58:18,898 main.py:51] epoch 2972, training loss: 9808.34, average training loss: 10180.45, base loss: 14344.22
[INFO 2017-06-28 14:58:19,523 main.py:51] epoch 2973, training loss: 10158.75, average training loss: 10181.92, base loss: 14347.35
[INFO 2017-06-28 14:58:20,275 main.py:51] epoch 2974, training loss: 9179.70, average training loss: 10181.64, base loss: 14345.67
[INFO 2017-06-28 14:58:21,156 main.py:51] epoch 2975, training loss: 9048.78, average training loss: 10180.19, base loss: 14345.57
[INFO 2017-06-28 14:58:21,965 main.py:51] epoch 2976, training loss: 9452.22, average training loss: 10179.44, base loss: 14344.65
[INFO 2017-06-28 14:58:22,668 main.py:51] epoch 2977, training loss: 10758.28, average training loss: 10181.53, base loss: 14346.58
[INFO 2017-06-28 14:58:23,462 main.py:51] epoch 2978, training loss: 9535.74, average training loss: 10181.87, base loss: 14346.87
[INFO 2017-06-28 14:58:24,299 main.py:51] epoch 2979, training loss: 11159.01, average training loss: 10182.82, base loss: 14347.68
[INFO 2017-06-28 14:58:25,064 main.py:51] epoch 2980, training loss: 9231.91, average training loss: 10181.15, base loss: 14345.47
[INFO 2017-06-28 14:58:25,781 main.py:51] epoch 2981, training loss: 9022.32, average training loss: 10181.40, base loss: 14344.64
[INFO 2017-06-28 14:58:26,564 main.py:51] epoch 2982, training loss: 8883.87, average training loss: 10181.22, base loss: 14344.46
[INFO 2017-06-28 14:58:27,431 main.py:51] epoch 2983, training loss: 9657.90, average training loss: 10179.48, base loss: 14342.37
[INFO 2017-06-28 14:58:28,146 main.py:51] epoch 2984, training loss: 9031.88, average training loss: 10179.21, base loss: 14342.34
[INFO 2017-06-28 14:58:28,870 main.py:51] epoch 2985, training loss: 8658.16, average training loss: 10175.72, base loss: 14337.12
[INFO 2017-06-28 14:58:29,665 main.py:51] epoch 2986, training loss: 10744.29, average training loss: 10175.52, base loss: 14336.45
[INFO 2017-06-28 14:58:30,459 main.py:51] epoch 2987, training loss: 11081.35, average training loss: 10175.86, base loss: 14335.59
[INFO 2017-06-28 14:58:31,164 main.py:51] epoch 2988, training loss: 8819.08, average training loss: 10174.33, base loss: 14333.12
[INFO 2017-06-28 14:58:31,981 main.py:51] epoch 2989, training loss: 9073.82, average training loss: 10171.62, base loss: 14328.72
[INFO 2017-06-28 14:58:32,619 main.py:51] epoch 2990, training loss: 10379.74, average training loss: 10171.93, base loss: 14329.21
[INFO 2017-06-28 14:58:33,268 main.py:51] epoch 2991, training loss: 11212.88, average training loss: 10173.93, base loss: 14331.11
[INFO 2017-06-28 14:58:33,901 main.py:51] epoch 2992, training loss: 10267.42, average training loss: 10174.13, base loss: 14330.73
[INFO 2017-06-28 14:58:34,579 main.py:51] epoch 2993, training loss: 9331.66, average training loss: 10173.80, base loss: 14328.79
[INFO 2017-06-28 14:58:35,277 main.py:51] epoch 2994, training loss: 11375.51, average training loss: 10175.90, base loss: 14330.98
[INFO 2017-06-28 14:58:35,942 main.py:51] epoch 2995, training loss: 10497.83, average training loss: 10176.31, base loss: 14331.82
[INFO 2017-06-28 14:58:36,691 main.py:51] epoch 2996, training loss: 11907.42, average training loss: 10179.40, base loss: 14337.56
[INFO 2017-06-28 14:58:37,508 main.py:51] epoch 2997, training loss: 9708.76, average training loss: 10178.96, base loss: 14337.31
[INFO 2017-06-28 14:58:38,299 main.py:51] epoch 2998, training loss: 9946.26, average training loss: 10179.83, base loss: 14340.08
[INFO 2017-06-28 14:58:38,965 main.py:51] epoch 2999, training loss: 9518.31, average training loss: 10179.86, base loss: 14340.51
[INFO 2017-06-28 14:58:38,965 main.py:53] epoch 2999, testing
[INFO 2017-06-28 14:58:41,802 main.py:105] average testing loss: 11946.64, base loss: 15691.21
[INFO 2017-06-28 14:58:41,802 main.py:106] improve_loss: 3744.57, improve_percent: 0.24
[INFO 2017-06-28 14:58:41,803 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 14:58:42,588 main.py:51] epoch 3000, training loss: 9365.63, average training loss: 10178.88, base loss: 14341.06
[INFO 2017-06-28 14:58:43,355 main.py:51] epoch 3001, training loss: 10396.42, average training loss: 10179.22, base loss: 14342.65
[INFO 2017-06-28 14:58:44,035 main.py:51] epoch 3002, training loss: 12228.88, average training loss: 10181.08, base loss: 14346.58
[INFO 2017-06-28 14:58:44,818 main.py:51] epoch 3003, training loss: 9902.26, average training loss: 10181.29, base loss: 14348.34
[INFO 2017-06-28 14:58:45,595 main.py:51] epoch 3004, training loss: 8452.12, average training loss: 10179.58, base loss: 14346.22
[INFO 2017-06-28 14:58:46,242 main.py:51] epoch 3005, training loss: 10150.93, average training loss: 10178.23, base loss: 14344.34
[INFO 2017-06-28 14:58:47,029 main.py:51] epoch 3006, training loss: 10627.88, average training loss: 10178.22, base loss: 14343.59
[INFO 2017-06-28 14:58:47,855 main.py:51] epoch 3007, training loss: 9005.01, average training loss: 10178.98, base loss: 14344.06
[INFO 2017-06-28 14:58:48,661 main.py:51] epoch 3008, training loss: 11783.52, average training loss: 10180.20, base loss: 14345.66
[INFO 2017-06-28 14:58:49,337 main.py:51] epoch 3009, training loss: 9203.24, average training loss: 10177.77, base loss: 14343.63
[INFO 2017-06-28 14:58:50,083 main.py:51] epoch 3010, training loss: 11381.09, average training loss: 10179.89, base loss: 14346.46
[INFO 2017-06-28 14:58:50,908 main.py:51] epoch 3011, training loss: 11450.35, average training loss: 10182.39, base loss: 14351.42
[INFO 2017-06-28 14:58:51,595 main.py:51] epoch 3012, training loss: 10467.59, average training loss: 10182.92, base loss: 14354.78
[INFO 2017-06-28 14:58:52,331 main.py:51] epoch 3013, training loss: 10565.36, average training loss: 10183.96, base loss: 14356.03
[INFO 2017-06-28 14:58:53,149 main.py:51] epoch 3014, training loss: 12103.45, average training loss: 10186.45, base loss: 14359.60
[INFO 2017-06-28 14:58:53,996 main.py:51] epoch 3015, training loss: 10078.60, average training loss: 10186.70, base loss: 14359.87
[INFO 2017-06-28 14:58:54,636 main.py:51] epoch 3016, training loss: 10997.54, average training loss: 10187.71, base loss: 14361.86
[INFO 2017-06-28 14:58:55,445 main.py:51] epoch 3017, training loss: 11536.41, average training loss: 10188.39, base loss: 14362.84
[INFO 2017-06-28 14:58:56,277 main.py:51] epoch 3018, training loss: 9968.66, average training loss: 10187.09, base loss: 14360.26
[INFO 2017-06-28 14:58:57,022 main.py:51] epoch 3019, training loss: 9285.04, average training loss: 10184.03, base loss: 14356.43
[INFO 2017-06-28 14:58:57,713 main.py:51] epoch 3020, training loss: 10033.84, average training loss: 10184.81, base loss: 14358.98
[INFO 2017-06-28 14:58:58,536 main.py:51] epoch 3021, training loss: 9890.13, average training loss: 10183.69, base loss: 14357.71
[INFO 2017-06-28 14:58:59,358 main.py:51] epoch 3022, training loss: 11451.64, average training loss: 10184.76, base loss: 14359.21
[INFO 2017-06-28 14:58:59,996 main.py:51] epoch 3023, training loss: 11562.97, average training loss: 10184.22, base loss: 14359.12
[INFO 2017-06-28 14:59:00,819 main.py:51] epoch 3024, training loss: 10734.70, average training loss: 10182.53, base loss: 14358.33
[INFO 2017-06-28 14:59:01,660 main.py:51] epoch 3025, training loss: 9458.11, average training loss: 10182.77, base loss: 14358.35
[INFO 2017-06-28 14:59:02,383 main.py:51] epoch 3026, training loss: 9450.59, average training loss: 10182.52, base loss: 14359.01
[INFO 2017-06-28 14:59:03,077 main.py:51] epoch 3027, training loss: 11221.09, average training loss: 10183.32, base loss: 14359.88
[INFO 2017-06-28 14:59:03,874 main.py:51] epoch 3028, training loss: 9483.68, average training loss: 10182.37, base loss: 14359.87
[INFO 2017-06-28 14:59:04,725 main.py:51] epoch 3029, training loss: 10015.84, average training loss: 10182.83, base loss: 14361.39
[INFO 2017-06-28 14:59:05,420 main.py:51] epoch 3030, training loss: 9583.39, average training loss: 10182.72, base loss: 14361.18
[INFO 2017-06-28 14:59:06,180 main.py:51] epoch 3031, training loss: 11587.42, average training loss: 10183.85, base loss: 14364.02
[INFO 2017-06-28 14:59:07,015 main.py:51] epoch 3032, training loss: 9768.59, average training loss: 10181.85, base loss: 14360.62
[INFO 2017-06-28 14:59:07,770 main.py:51] epoch 3033, training loss: 9881.62, average training loss: 10182.51, base loss: 14362.12
[INFO 2017-06-28 14:59:08,447 main.py:51] epoch 3034, training loss: 10688.47, average training loss: 10183.81, base loss: 14363.93
[INFO 2017-06-28 14:59:09,225 main.py:51] epoch 3035, training loss: 10646.12, average training loss: 10184.52, base loss: 14363.15
[INFO 2017-06-28 14:59:10,049 main.py:51] epoch 3036, training loss: 10020.95, average training loss: 10185.31, base loss: 14364.29
[INFO 2017-06-28 14:59:10,679 main.py:51] epoch 3037, training loss: 8331.92, average training loss: 10184.05, base loss: 14363.22
[INFO 2017-06-28 14:59:11,435 main.py:51] epoch 3038, training loss: 9095.18, average training loss: 10183.95, base loss: 14363.80
[INFO 2017-06-28 14:59:12,304 main.py:51] epoch 3039, training loss: 11224.46, average training loss: 10184.54, base loss: 14364.23
[INFO 2017-06-28 14:59:13,038 main.py:51] epoch 3040, training loss: 13158.90, average training loss: 10189.79, base loss: 14372.93
[INFO 2017-06-28 14:59:13,733 main.py:51] epoch 3041, training loss: 11540.28, average training loss: 10192.18, base loss: 14374.66
[INFO 2017-06-28 14:59:14,529 main.py:51] epoch 3042, training loss: 10335.26, average training loss: 10193.36, base loss: 14375.95
[INFO 2017-06-28 14:59:15,288 main.py:51] epoch 3043, training loss: 11162.42, average training loss: 10194.03, base loss: 14379.22
[INFO 2017-06-28 14:59:15,964 main.py:51] epoch 3044, training loss: 12416.72, average training loss: 10195.58, base loss: 14380.73
[INFO 2017-06-28 14:59:16,751 main.py:51] epoch 3045, training loss: 9923.30, average training loss: 10194.23, base loss: 14380.43
[INFO 2017-06-28 14:59:17,590 main.py:51] epoch 3046, training loss: 10040.18, average training loss: 10194.15, base loss: 14380.53
[INFO 2017-06-28 14:59:18,253 main.py:51] epoch 3047, training loss: 8926.92, average training loss: 10193.64, base loss: 14380.31
[INFO 2017-06-28 14:59:19,027 main.py:51] epoch 3048, training loss: 9953.77, average training loss: 10192.62, base loss: 14378.12
[INFO 2017-06-28 14:59:19,846 main.py:51] epoch 3049, training loss: 9774.84, average training loss: 10193.19, base loss: 14379.56
[INFO 2017-06-28 14:59:20,614 main.py:51] epoch 3050, training loss: 8604.49, average training loss: 10189.29, base loss: 14373.90
[INFO 2017-06-28 14:59:21,292 main.py:51] epoch 3051, training loss: 10795.70, average training loss: 10188.26, base loss: 14371.08
[INFO 2017-06-28 14:59:22,067 main.py:51] epoch 3052, training loss: 9305.34, average training loss: 10186.81, base loss: 14369.13
[INFO 2017-06-28 14:59:22,854 main.py:51] epoch 3053, training loss: 11854.19, average training loss: 10188.28, base loss: 14371.88
[INFO 2017-06-28 14:59:23,507 main.py:51] epoch 3054, training loss: 9795.90, average training loss: 10188.52, base loss: 14372.82
[INFO 2017-06-28 14:59:24,289 main.py:51] epoch 3055, training loss: 10340.38, average training loss: 10188.85, base loss: 14372.29
[INFO 2017-06-28 14:59:25,140 main.py:51] epoch 3056, training loss: 12848.93, average training loss: 10191.19, base loss: 14375.78
[INFO 2017-06-28 14:59:25,818 main.py:51] epoch 3057, training loss: 11850.58, average training loss: 10193.51, base loss: 14380.14
[INFO 2017-06-28 14:59:26,575 main.py:51] epoch 3058, training loss: 10673.40, average training loss: 10194.48, base loss: 14383.02
[INFO 2017-06-28 14:59:27,373 main.py:51] epoch 3059, training loss: 8935.03, average training loss: 10193.00, base loss: 14380.84
[INFO 2017-06-28 14:59:28,203 main.py:51] epoch 3060, training loss: 11660.43, average training loss: 10194.72, base loss: 14385.53
[INFO 2017-06-28 14:59:28,832 main.py:51] epoch 3061, training loss: 11400.17, average training loss: 10196.02, base loss: 14388.15
[INFO 2017-06-28 14:59:29,597 main.py:51] epoch 3062, training loss: 9785.45, average training loss: 10195.14, base loss: 14388.02
[INFO 2017-06-28 14:59:30,454 main.py:51] epoch 3063, training loss: 10215.26, average training loss: 10192.16, base loss: 14384.94
[INFO 2017-06-28 14:59:31,182 main.py:51] epoch 3064, training loss: 11468.76, average training loss: 10191.73, base loss: 14381.73
[INFO 2017-06-28 14:59:31,865 main.py:51] epoch 3065, training loss: 8741.97, average training loss: 10188.63, base loss: 14376.75
[INFO 2017-06-28 14:59:32,691 main.py:51] epoch 3066, training loss: 11680.85, average training loss: 10189.45, base loss: 14377.17
[INFO 2017-06-28 14:59:33,529 main.py:51] epoch 3067, training loss: 10230.12, average training loss: 10190.11, base loss: 14379.81
[INFO 2017-06-28 14:59:34,144 main.py:51] epoch 3068, training loss: 9860.49, average training loss: 10190.05, base loss: 14380.92
[INFO 2017-06-28 14:59:34,934 main.py:51] epoch 3069, training loss: 9334.67, average training loss: 10189.16, base loss: 14380.05
[INFO 2017-06-28 14:59:35,784 main.py:51] epoch 3070, training loss: 10076.65, average training loss: 10189.83, base loss: 14379.81
[INFO 2017-06-28 14:59:36,489 main.py:51] epoch 3071, training loss: 10116.95, average training loss: 10187.85, base loss: 14378.21
[INFO 2017-06-28 14:59:37,207 main.py:51] epoch 3072, training loss: 9740.54, average training loss: 10189.16, base loss: 14380.99
[INFO 2017-06-28 14:59:38,014 main.py:51] epoch 3073, training loss: 9706.70, average training loss: 10188.86, base loss: 14380.81
[INFO 2017-06-28 14:59:38,805 main.py:51] epoch 3074, training loss: 11500.30, average training loss: 10189.94, base loss: 14382.78
[INFO 2017-06-28 14:59:39,486 main.py:51] epoch 3075, training loss: 9920.35, average training loss: 10189.60, base loss: 14382.57
[INFO 2017-06-28 14:59:40,241 main.py:51] epoch 3076, training loss: 10351.11, average training loss: 10190.66, base loss: 14385.87
[INFO 2017-06-28 14:59:41,117 main.py:51] epoch 3077, training loss: 8909.19, average training loss: 10189.02, base loss: 14383.97
[INFO 2017-06-28 14:59:41,977 main.py:51] epoch 3078, training loss: 10853.76, average training loss: 10188.14, base loss: 14383.97
[INFO 2017-06-28 14:59:42,630 main.py:51] epoch 3079, training loss: 8613.89, average training loss: 10185.61, base loss: 14380.32
[INFO 2017-06-28 14:59:43,453 main.py:51] epoch 3080, training loss: 9745.44, average training loss: 10185.19, base loss: 14380.04
[INFO 2017-06-28 14:59:44,270 main.py:51] epoch 3081, training loss: 10846.59, average training loss: 10185.20, base loss: 14378.28
[INFO 2017-06-28 14:59:45,046 main.py:51] epoch 3082, training loss: 12904.32, average training loss: 10187.64, base loss: 14379.34
[INFO 2017-06-28 14:59:45,664 main.py:51] epoch 3083, training loss: 11983.10, average training loss: 10189.56, base loss: 14381.47
[INFO 2017-06-28 14:59:46,473 main.py:51] epoch 3084, training loss: 10892.68, average training loss: 10190.92, base loss: 14383.02
[INFO 2017-06-28 14:59:47,231 main.py:51] epoch 3085, training loss: 10387.74, average training loss: 10190.09, base loss: 14383.04
[INFO 2017-06-28 14:59:47,869 main.py:51] epoch 3086, training loss: 11055.13, average training loss: 10189.81, base loss: 14382.20
[INFO 2017-06-28 14:59:48,618 main.py:51] epoch 3087, training loss: 9632.13, average training loss: 10189.60, base loss: 14382.78
[INFO 2017-06-28 14:59:49,474 main.py:51] epoch 3088, training loss: 9684.22, average training loss: 10189.42, base loss: 14385.16
[INFO 2017-06-28 14:59:50,174 main.py:51] epoch 3089, training loss: 10955.83, average training loss: 10189.96, base loss: 14386.08
[INFO 2017-06-28 14:59:50,909 main.py:51] epoch 3090, training loss: 9254.75, average training loss: 10190.31, base loss: 14387.81
[INFO 2017-06-28 14:59:51,742 main.py:51] epoch 3091, training loss: 10219.04, average training loss: 10192.34, base loss: 14390.38
[INFO 2017-06-28 14:59:52,460 main.py:51] epoch 3092, training loss: 11153.34, average training loss: 10192.61, base loss: 14392.39
[INFO 2017-06-28 14:59:53,155 main.py:51] epoch 3093, training loss: 10394.74, average training loss: 10192.85, base loss: 14392.88
[INFO 2017-06-28 14:59:53,955 main.py:51] epoch 3094, training loss: 10101.40, average training loss: 10193.57, base loss: 14394.22
[INFO 2017-06-28 14:59:54,782 main.py:51] epoch 3095, training loss: 10261.76, average training loss: 10194.30, base loss: 14395.42
[INFO 2017-06-28 14:59:55,407 main.py:51] epoch 3096, training loss: 10037.21, average training loss: 10194.71, base loss: 14394.88
[INFO 2017-06-28 14:59:56,178 main.py:51] epoch 3097, training loss: 9721.43, average training loss: 10194.22, base loss: 14395.07
[INFO 2017-06-28 14:59:56,993 main.py:51] epoch 3098, training loss: 10123.93, average training loss: 10193.28, base loss: 14394.96
[INFO 2017-06-28 14:59:57,663 main.py:51] epoch 3099, training loss: 10510.66, average training loss: 10194.23, base loss: 14396.45
[INFO 2017-06-28 14:59:57,663 main.py:53] epoch 3099, testing
[INFO 2017-06-28 15:00:00,587 main.py:105] average testing loss: 11393.00, base loss: 15403.31
[INFO 2017-06-28 15:00:00,587 main.py:106] improve_loss: 4010.32, improve_percent: 0.26
[INFO 2017-06-28 15:00:00,588 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 15:00:01,279 main.py:51] epoch 3100, training loss: 10253.08, average training loss: 10194.73, base loss: 14396.14
[INFO 2017-06-28 15:00:02,094 main.py:51] epoch 3101, training loss: 10620.54, average training loss: 10194.62, base loss: 14396.51
[INFO 2017-06-28 15:00:02,937 main.py:51] epoch 3102, training loss: 8798.22, average training loss: 10193.58, base loss: 14395.11
[INFO 2017-06-28 15:00:03,542 main.py:51] epoch 3103, training loss: 9829.46, average training loss: 10193.43, base loss: 14396.85
[INFO 2017-06-28 15:00:04,364 main.py:51] epoch 3104, training loss: 10264.42, average training loss: 10194.91, base loss: 14399.49
[INFO 2017-06-28 15:00:05,199 main.py:51] epoch 3105, training loss: 9794.52, average training loss: 10193.02, base loss: 14396.50
[INFO 2017-06-28 15:00:05,894 main.py:51] epoch 3106, training loss: 9422.25, average training loss: 10192.83, base loss: 14395.21
[INFO 2017-06-28 15:00:06,612 main.py:51] epoch 3107, training loss: 10764.21, average training loss: 10192.90, base loss: 14395.19
[INFO 2017-06-28 15:00:07,402 main.py:51] epoch 3108, training loss: 9019.17, average training loss: 10191.87, base loss: 14394.29
[INFO 2017-06-28 15:00:08,185 main.py:51] epoch 3109, training loss: 8979.18, average training loss: 10190.73, base loss: 14393.37
[INFO 2017-06-28 15:00:08,789 main.py:51] epoch 3110, training loss: 9108.01, average training loss: 10189.99, base loss: 14391.52
[INFO 2017-06-28 15:00:09,528 main.py:51] epoch 3111, training loss: 10733.06, average training loss: 10189.89, base loss: 14390.28
[INFO 2017-06-28 15:00:10,384 main.py:51] epoch 3112, training loss: 8649.22, average training loss: 10188.90, base loss: 14388.01
[INFO 2017-06-28 15:00:11,065 main.py:51] epoch 3113, training loss: 12108.83, average training loss: 10190.73, base loss: 14390.34
[INFO 2017-06-28 15:00:11,796 main.py:51] epoch 3114, training loss: 10334.96, average training loss: 10191.17, base loss: 14391.68
[INFO 2017-06-28 15:00:12,618 main.py:51] epoch 3115, training loss: 10060.45, average training loss: 10192.38, base loss: 14394.74
[INFO 2017-06-28 15:00:13,468 main.py:51] epoch 3116, training loss: 10896.36, average training loss: 10193.59, base loss: 14395.08
[INFO 2017-06-28 15:00:14,106 main.py:51] epoch 3117, training loss: 10774.36, average training loss: 10193.86, base loss: 14396.80
[INFO 2017-06-28 15:00:14,858 main.py:51] epoch 3118, training loss: 11814.08, average training loss: 10195.68, base loss: 14398.03
[INFO 2017-06-28 15:00:15,657 main.py:51] epoch 3119, training loss: 10662.18, average training loss: 10197.72, base loss: 14401.85
[INFO 2017-06-28 15:00:16,291 main.py:51] epoch 3120, training loss: 10469.00, average training loss: 10198.02, base loss: 14400.95
[INFO 2017-06-28 15:00:17,066 main.py:51] epoch 3121, training loss: 9291.01, average training loss: 10198.15, base loss: 14402.42
[INFO 2017-06-28 15:00:17,869 main.py:51] epoch 3122, training loss: 10791.77, average training loss: 10198.67, base loss: 14402.95
[INFO 2017-06-28 15:00:18,602 main.py:51] epoch 3123, training loss: 10309.96, average training loss: 10198.76, base loss: 14402.14
[INFO 2017-06-28 15:00:19,291 main.py:51] epoch 3124, training loss: 9180.83, average training loss: 10199.24, base loss: 14402.78
[INFO 2017-06-28 15:00:20,110 main.py:51] epoch 3125, training loss: 10336.05, average training loss: 10198.29, base loss: 14400.31
[INFO 2017-06-28 15:00:20,970 main.py:51] epoch 3126, training loss: 11518.28, average training loss: 10199.86, base loss: 14402.74
[INFO 2017-06-28 15:00:21,592 main.py:51] epoch 3127, training loss: 10365.85, average training loss: 10199.39, base loss: 14402.35
[INFO 2017-06-28 15:00:22,312 main.py:51] epoch 3128, training loss: 10050.81, average training loss: 10199.21, base loss: 14402.51
[INFO 2017-06-28 15:00:22,993 main.py:51] epoch 3129, training loss: 9495.42, average training loss: 10198.82, base loss: 14402.32
[INFO 2017-06-28 15:00:23,600 main.py:51] epoch 3130, training loss: 10228.90, average training loss: 10199.74, base loss: 14403.20
[INFO 2017-06-28 15:00:24,225 main.py:51] epoch 3131, training loss: 11657.51, average training loss: 10200.93, base loss: 14405.18
[INFO 2017-06-28 15:00:24,920 main.py:51] epoch 3132, training loss: 8481.04, average training loss: 10199.31, base loss: 14402.54
[INFO 2017-06-28 15:00:25,590 main.py:51] epoch 3133, training loss: 9411.22, average training loss: 10200.16, base loss: 14405.49
[INFO 2017-06-28 15:00:26,310 main.py:51] epoch 3134, training loss: 10873.18, average training loss: 10200.94, base loss: 14408.09
[INFO 2017-06-28 15:00:27,165 main.py:51] epoch 3135, training loss: 9208.83, average training loss: 10199.65, base loss: 14407.43
[INFO 2017-06-28 15:00:27,878 main.py:51] epoch 3136, training loss: 10105.38, average training loss: 10199.13, base loss: 14406.39
[INFO 2017-06-28 15:00:28,584 main.py:51] epoch 3137, training loss: 9520.59, average training loss: 10199.30, base loss: 14406.52
[INFO 2017-06-28 15:00:29,371 main.py:51] epoch 3138, training loss: 9598.83, average training loss: 10199.09, base loss: 14407.16
[INFO 2017-06-28 15:00:30,178 main.py:51] epoch 3139, training loss: 10358.61, average training loss: 10197.45, base loss: 14404.66
[INFO 2017-06-28 15:00:30,807 main.py:51] epoch 3140, training loss: 10083.70, average training loss: 10198.16, base loss: 14407.58
[INFO 2017-06-28 15:00:31,560 main.py:51] epoch 3141, training loss: 9489.19, average training loss: 10197.37, base loss: 14407.11
[INFO 2017-06-28 15:00:32,347 main.py:51] epoch 3142, training loss: 8917.15, average training loss: 10196.99, base loss: 14407.15
[INFO 2017-06-28 15:00:33,022 main.py:51] epoch 3143, training loss: 9225.94, average training loss: 10195.39, base loss: 14406.16
[INFO 2017-06-28 15:00:33,785 main.py:51] epoch 3144, training loss: 10506.31, average training loss: 10195.11, base loss: 14408.19
[INFO 2017-06-28 15:00:34,574 main.py:51] epoch 3145, training loss: 9851.30, average training loss: 10195.24, base loss: 14409.16
[INFO 2017-06-28 15:00:35,422 main.py:51] epoch 3146, training loss: 9866.09, average training loss: 10194.70, base loss: 14409.53
[INFO 2017-06-28 15:00:36,109 main.py:51] epoch 3147, training loss: 9735.94, average training loss: 10194.38, base loss: 14410.10
[INFO 2017-06-28 15:00:36,850 main.py:51] epoch 3148, training loss: 9476.57, average training loss: 10193.47, base loss: 14409.29
[INFO 2017-06-28 15:00:37,651 main.py:51] epoch 3149, training loss: 10675.87, average training loss: 10194.35, base loss: 14411.53
[INFO 2017-06-28 15:00:38,465 main.py:51] epoch 3150, training loss: 9266.93, average training loss: 10194.96, base loss: 14414.29
[INFO 2017-06-28 15:00:39,141 main.py:51] epoch 3151, training loss: 9610.57, average training loss: 10194.04, base loss: 14413.13
[INFO 2017-06-28 15:00:39,940 main.py:51] epoch 3152, training loss: 11201.75, average training loss: 10194.77, base loss: 14415.20
[INFO 2017-06-28 15:00:40,719 main.py:51] epoch 3153, training loss: 9717.11, average training loss: 10193.58, base loss: 14414.85
[INFO 2017-06-28 15:00:41,555 main.py:51] epoch 3154, training loss: 9412.74, average training loss: 10191.43, base loss: 14410.63
[INFO 2017-06-28 15:00:42,186 main.py:51] epoch 3155, training loss: 10389.67, average training loss: 10191.86, base loss: 14412.64
[INFO 2017-06-28 15:00:42,991 main.py:51] epoch 3156, training loss: 10017.95, average training loss: 10193.08, base loss: 14414.17
[INFO 2017-06-28 15:00:43,836 main.py:51] epoch 3157, training loss: 9792.10, average training loss: 10190.46, base loss: 14411.81
[INFO 2017-06-28 15:00:44,552 main.py:51] epoch 3158, training loss: 9331.70, average training loss: 10189.47, base loss: 14412.10
[INFO 2017-06-28 15:00:45,265 main.py:51] epoch 3159, training loss: 10565.98, average training loss: 10190.01, base loss: 14411.95
[INFO 2017-06-28 15:00:46,059 main.py:51] epoch 3160, training loss: 10032.09, average training loss: 10187.69, base loss: 14408.39
[INFO 2017-06-28 15:00:46,901 main.py:51] epoch 3161, training loss: 9286.01, average training loss: 10185.79, base loss: 14405.89
[INFO 2017-06-28 15:00:47,540 main.py:51] epoch 3162, training loss: 9601.46, average training loss: 10185.26, base loss: 14406.46
[INFO 2017-06-28 15:00:48,318 main.py:51] epoch 3163, training loss: 11286.30, average training loss: 10187.19, base loss: 14410.55
[INFO 2017-06-28 15:00:49,136 main.py:51] epoch 3164, training loss: 9271.32, average training loss: 10186.41, base loss: 14410.26
[INFO 2017-06-28 15:00:49,855 main.py:51] epoch 3165, training loss: 10406.48, average training loss: 10187.39, base loss: 14411.40
[INFO 2017-06-28 15:00:50,578 main.py:51] epoch 3166, training loss: 10524.91, average training loss: 10187.90, base loss: 14409.38
[INFO 2017-06-28 15:00:51,378 main.py:51] epoch 3167, training loss: 11529.53, average training loss: 10189.02, base loss: 14411.24
[INFO 2017-06-28 15:00:52,184 main.py:51] epoch 3168, training loss: 9251.56, average training loss: 10187.90, base loss: 14410.54
[INFO 2017-06-28 15:00:52,809 main.py:51] epoch 3169, training loss: 10868.76, average training loss: 10188.83, base loss: 14413.47
[INFO 2017-06-28 15:00:53,626 main.py:51] epoch 3170, training loss: 10208.54, average training loss: 10189.74, base loss: 14414.36
[INFO 2017-06-28 15:00:54,450 main.py:51] epoch 3171, training loss: 8733.44, average training loss: 10187.76, base loss: 14412.51
[INFO 2017-06-28 15:00:55,266 main.py:51] epoch 3172, training loss: 9864.74, average training loss: 10187.46, base loss: 14412.59
[INFO 2017-06-28 15:00:55,933 main.py:51] epoch 3173, training loss: 9464.11, average training loss: 10186.78, base loss: 14412.75
[INFO 2017-06-28 15:00:56,717 main.py:51] epoch 3174, training loss: 10225.26, average training loss: 10186.08, base loss: 14412.28
[INFO 2017-06-28 15:00:57,568 main.py:51] epoch 3175, training loss: 9470.23, average training loss: 10185.63, base loss: 14414.19
[INFO 2017-06-28 15:00:58,397 main.py:51] epoch 3176, training loss: 9556.99, average training loss: 10185.52, base loss: 14413.23
[INFO 2017-06-28 15:00:59,048 main.py:51] epoch 3177, training loss: 10065.37, average training loss: 10184.10, base loss: 14410.96
[INFO 2017-06-28 15:00:59,820 main.py:51] epoch 3178, training loss: 10542.49, average training loss: 10185.91, base loss: 14414.07
[INFO 2017-06-28 15:01:00,642 main.py:51] epoch 3179, training loss: 9625.24, average training loss: 10186.95, base loss: 14414.83
[INFO 2017-06-28 15:01:01,430 main.py:51] epoch 3180, training loss: 10524.30, average training loss: 10187.19, base loss: 14414.58
[INFO 2017-06-28 15:01:02,122 main.py:51] epoch 3181, training loss: 8776.71, average training loss: 10185.65, base loss: 14413.01
[INFO 2017-06-28 15:01:02,920 main.py:51] epoch 3182, training loss: 9737.09, average training loss: 10184.16, base loss: 14409.72
[INFO 2017-06-28 15:01:03,747 main.py:51] epoch 3183, training loss: 8883.65, average training loss: 10183.31, base loss: 14410.44
[INFO 2017-06-28 15:01:04,419 main.py:51] epoch 3184, training loss: 10002.63, average training loss: 10184.81, base loss: 14412.82
[INFO 2017-06-28 15:01:05,174 main.py:51] epoch 3185, training loss: 9395.77, average training loss: 10184.58, base loss: 14414.01
[INFO 2017-06-28 15:01:05,992 main.py:51] epoch 3186, training loss: 10773.99, average training loss: 10183.37, base loss: 14414.54
[INFO 2017-06-28 15:01:06,828 main.py:51] epoch 3187, training loss: 10610.31, average training loss: 10183.22, base loss: 14415.15
[INFO 2017-06-28 15:01:07,466 main.py:51] epoch 3188, training loss: 10211.54, average training loss: 10184.32, base loss: 14416.60
[INFO 2017-06-28 15:01:08,267 main.py:51] epoch 3189, training loss: 9919.23, average training loss: 10182.82, base loss: 14413.85
[INFO 2017-06-28 15:01:09,090 main.py:51] epoch 3190, training loss: 11642.21, average training loss: 10183.34, base loss: 14415.06
[INFO 2017-06-28 15:01:09,827 main.py:51] epoch 3191, training loss: 9421.43, average training loss: 10180.54, base loss: 14411.59
[INFO 2017-06-28 15:01:10,514 main.py:51] epoch 3192, training loss: 10146.17, average training loss: 10180.52, base loss: 14412.82
[INFO 2017-06-28 15:01:11,302 main.py:51] epoch 3193, training loss: 9683.75, average training loss: 10180.64, base loss: 14413.43
[INFO 2017-06-28 15:01:12,119 main.py:51] epoch 3194, training loss: 11123.13, average training loss: 10180.45, base loss: 14414.77
[INFO 2017-06-28 15:01:12,740 main.py:51] epoch 3195, training loss: 10631.85, average training loss: 10180.98, base loss: 14415.36
[INFO 2017-06-28 15:01:13,482 main.py:51] epoch 3196, training loss: 9209.78, average training loss: 10181.43, base loss: 14416.63
[INFO 2017-06-28 15:01:14,286 main.py:51] epoch 3197, training loss: 9725.61, average training loss: 10179.62, base loss: 14414.16
[INFO 2017-06-28 15:01:14,900 main.py:51] epoch 3198, training loss: 10278.53, average training loss: 10181.13, base loss: 14416.61
[INFO 2017-06-28 15:01:15,652 main.py:51] epoch 3199, training loss: 11529.05, average training loss: 10183.17, base loss: 14421.73
[INFO 2017-06-28 15:01:15,652 main.py:53] epoch 3199, testing
[INFO 2017-06-28 15:01:18,578 main.py:105] average testing loss: 10692.76, base loss: 14943.63
[INFO 2017-06-28 15:01:18,578 main.py:106] improve_loss: 4250.87, improve_percent: 0.28
[INFO 2017-06-28 15:01:18,579 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 15:01:18,595 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 15:01:19,420 main.py:51] epoch 3200, training loss: 10069.87, average training loss: 10183.43, base loss: 14423.11
[INFO 2017-06-28 15:01:20,210 main.py:51] epoch 3201, training loss: 9893.76, average training loss: 10182.71, base loss: 14421.79
[INFO 2017-06-28 15:01:20,911 main.py:51] epoch 3202, training loss: 9838.56, average training loss: 10182.63, base loss: 14423.08
[INFO 2017-06-28 15:01:21,634 main.py:51] epoch 3203, training loss: 8503.96, average training loss: 10180.06, base loss: 14419.78
[INFO 2017-06-28 15:01:22,444 main.py:51] epoch 3204, training loss: 10510.96, average training loss: 10178.50, base loss: 14418.38
[INFO 2017-06-28 15:01:23,285 main.py:51] epoch 3205, training loss: 9075.45, average training loss: 10177.23, base loss: 14415.77
[INFO 2017-06-28 15:01:23,983 main.py:51] epoch 3206, training loss: 8403.80, average training loss: 10173.74, base loss: 14409.91
[INFO 2017-06-28 15:01:24,707 main.py:51] epoch 3207, training loss: 9605.42, average training loss: 10174.47, base loss: 14411.33
[INFO 2017-06-28 15:01:25,512 main.py:51] epoch 3208, training loss: 9817.10, average training loss: 10170.36, base loss: 14406.15
[INFO 2017-06-28 15:01:26,339 main.py:51] epoch 3209, training loss: 8549.57, average training loss: 10169.68, base loss: 14403.97
[INFO 2017-06-28 15:01:27,048 main.py:51] epoch 3210, training loss: 8643.88, average training loss: 10168.48, base loss: 14404.02
[INFO 2017-06-28 15:01:27,797 main.py:51] epoch 3211, training loss: 12216.07, average training loss: 10169.93, base loss: 14405.94
[INFO 2017-06-28 15:01:28,623 main.py:51] epoch 3212, training loss: 9330.24, average training loss: 10170.71, base loss: 14408.02
[INFO 2017-06-28 15:01:29,427 main.py:51] epoch 3213, training loss: 11266.46, average training loss: 10170.59, base loss: 14405.18
[INFO 2017-06-28 15:01:30,032 main.py:51] epoch 3214, training loss: 9162.06, average training loss: 10169.85, base loss: 14404.54
[INFO 2017-06-28 15:01:30,812 main.py:51] epoch 3215, training loss: 10471.89, average training loss: 10169.54, base loss: 14403.45
[INFO 2017-06-28 15:01:31,599 main.py:51] epoch 3216, training loss: 9812.09, average training loss: 10169.99, base loss: 14404.38
[INFO 2017-06-28 15:01:32,239 main.py:51] epoch 3217, training loss: 10104.05, average training loss: 10170.83, base loss: 14406.51
[INFO 2017-06-28 15:01:32,968 main.py:51] epoch 3218, training loss: 11202.99, average training loss: 10171.97, base loss: 14408.18
[INFO 2017-06-28 15:01:33,779 main.py:51] epoch 3219, training loss: 9319.35, average training loss: 10171.63, base loss: 14407.78
[INFO 2017-06-28 15:01:34,448 main.py:51] epoch 3220, training loss: 9757.54, average training loss: 10172.13, base loss: 14408.35
[INFO 2017-06-28 15:01:35,197 main.py:51] epoch 3221, training loss: 9514.76, average training loss: 10172.00, base loss: 14407.64
[INFO 2017-06-28 15:01:36,025 main.py:51] epoch 3222, training loss: 9950.43, average training loss: 10170.60, base loss: 14405.08
[INFO 2017-06-28 15:01:36,846 main.py:51] epoch 3223, training loss: 11211.79, average training loss: 10171.63, base loss: 14406.65
[INFO 2017-06-28 15:01:37,494 main.py:51] epoch 3224, training loss: 10554.17, average training loss: 10171.69, base loss: 14408.75
[INFO 2017-06-28 15:01:38,264 main.py:51] epoch 3225, training loss: 9884.77, average training loss: 10171.40, base loss: 14408.53
[INFO 2017-06-28 15:01:39,111 main.py:51] epoch 3226, training loss: 11489.85, average training loss: 10172.28, base loss: 14408.68
[INFO 2017-06-28 15:01:39,789 main.py:51] epoch 3227, training loss: 10047.57, average training loss: 10171.98, base loss: 14408.48
[INFO 2017-06-28 15:01:40,472 main.py:51] epoch 3228, training loss: 9562.14, average training loss: 10171.02, base loss: 14406.22
[INFO 2017-06-28 15:01:41,343 main.py:51] epoch 3229, training loss: 11008.30, average training loss: 10172.13, base loss: 14408.39
[INFO 2017-06-28 15:01:42,082 main.py:51] epoch 3230, training loss: 10349.94, average training loss: 10172.34, base loss: 14408.47
[INFO 2017-06-28 15:01:42,792 main.py:51] epoch 3231, training loss: 8403.25, average training loss: 10170.51, base loss: 14404.52
[INFO 2017-06-28 15:01:43,558 main.py:51] epoch 3232, training loss: 10740.14, average training loss: 10171.09, base loss: 14404.54
[INFO 2017-06-28 15:01:44,385 main.py:51] epoch 3233, training loss: 11759.39, average training loss: 10174.13, base loss: 14409.08
[INFO 2017-06-28 15:01:45,012 main.py:51] epoch 3234, training loss: 9776.54, average training loss: 10174.15, base loss: 14409.48
[INFO 2017-06-28 15:01:45,810 main.py:51] epoch 3235, training loss: 9953.40, average training loss: 10174.00, base loss: 14409.76
[INFO 2017-06-28 15:01:46,635 main.py:51] epoch 3236, training loss: 10117.16, average training loss: 10174.40, base loss: 14410.96
[INFO 2017-06-28 15:01:47,441 main.py:51] epoch 3237, training loss: 11531.34, average training loss: 10176.92, base loss: 14414.23
[INFO 2017-06-28 15:01:48,078 main.py:51] epoch 3238, training loss: 9450.73, average training loss: 10175.61, base loss: 14411.94
[INFO 2017-06-28 15:01:48,852 main.py:51] epoch 3239, training loss: 10286.69, average training loss: 10175.22, base loss: 14410.80
[INFO 2017-06-28 15:01:49,678 main.py:51] epoch 3240, training loss: 9570.39, average training loss: 10173.59, base loss: 14409.21
[INFO 2017-06-28 15:01:50,321 main.py:51] epoch 3241, training loss: 9780.01, average training loss: 10172.43, base loss: 14407.10
[INFO 2017-06-28 15:01:51,082 main.py:51] epoch 3242, training loss: 10278.76, average training loss: 10173.18, base loss: 14408.20
[INFO 2017-06-28 15:01:51,911 main.py:51] epoch 3243, training loss: 8932.35, average training loss: 10172.95, base loss: 14407.49
[INFO 2017-06-28 15:01:52,754 main.py:51] epoch 3244, training loss: 9445.70, average training loss: 10169.91, base loss: 14402.57
[INFO 2017-06-28 15:01:53,420 main.py:51] epoch 3245, training loss: 12041.41, average training loss: 10172.54, base loss: 14405.99
[INFO 2017-06-28 15:01:54,206 main.py:51] epoch 3246, training loss: 10343.69, average training loss: 10172.73, base loss: 14405.48
[INFO 2017-06-28 15:01:55,040 main.py:51] epoch 3247, training loss: 9669.30, average training loss: 10172.16, base loss: 14404.76
[INFO 2017-06-28 15:01:55,712 main.py:51] epoch 3248, training loss: 9588.77, average training loss: 10170.46, base loss: 14403.88
[INFO 2017-06-28 15:01:56,467 main.py:51] epoch 3249, training loss: 10211.53, average training loss: 10168.12, base loss: 14401.23
[INFO 2017-06-28 15:01:57,283 main.py:51] epoch 3250, training loss: 8925.19, average training loss: 10166.16, base loss: 14398.22
[INFO 2017-06-28 15:01:58,126 main.py:51] epoch 3251, training loss: 9770.81, average training loss: 10165.67, base loss: 14398.38
[INFO 2017-06-28 15:01:58,760 main.py:51] epoch 3252, training loss: 10539.97, average training loss: 10166.19, base loss: 14398.31
[INFO 2017-06-28 15:01:59,536 main.py:51] epoch 3253, training loss: 10004.10, average training loss: 10166.56, base loss: 14397.56
[INFO 2017-06-28 15:02:00,354 main.py:51] epoch 3254, training loss: 10325.71, average training loss: 10165.52, base loss: 14396.18
[INFO 2017-06-28 15:02:01,011 main.py:51] epoch 3255, training loss: 10323.28, average training loss: 10166.30, base loss: 14398.11
[INFO 2017-06-28 15:02:01,783 main.py:51] epoch 3256, training loss: 8851.73, average training loss: 10165.09, base loss: 14397.57
[INFO 2017-06-28 15:02:02,608 main.py:51] epoch 3257, training loss: 9569.00, average training loss: 10165.95, base loss: 14399.40
[INFO 2017-06-28 15:02:03,464 main.py:51] epoch 3258, training loss: 12808.31, average training loss: 10168.42, base loss: 14402.54
[INFO 2017-06-28 15:02:04,104 main.py:51] epoch 3259, training loss: 9238.74, average training loss: 10167.87, base loss: 14402.68
[INFO 2017-06-28 15:02:04,913 main.py:51] epoch 3260, training loss: 9414.21, average training loss: 10168.18, base loss: 14403.55
[INFO 2017-06-28 15:02:05,743 main.py:51] epoch 3261, training loss: 9534.46, average training loss: 10166.24, base loss: 14400.86
[INFO 2017-06-28 15:02:06,575 main.py:51] epoch 3262, training loss: 10327.66, average training loss: 10164.57, base loss: 14399.33
[INFO 2017-06-28 15:02:07,232 main.py:51] epoch 3263, training loss: 10033.17, average training loss: 10162.85, base loss: 14396.77
[INFO 2017-06-28 15:02:08,015 main.py:51] epoch 3264, training loss: 9373.42, average training loss: 10160.70, base loss: 14392.51
[INFO 2017-06-28 15:02:08,842 main.py:51] epoch 3265, training loss: 10225.19, average training loss: 10160.27, base loss: 14392.55
[INFO 2017-06-28 15:02:09,518 main.py:51] epoch 3266, training loss: 9896.63, average training loss: 10160.48, base loss: 14394.89
[INFO 2017-06-28 15:02:10,256 main.py:51] epoch 3267, training loss: 8993.13, average training loss: 10158.29, base loss: 14393.91
[INFO 2017-06-28 15:02:10,879 main.py:51] epoch 3268, training loss: 9843.47, average training loss: 10155.27, base loss: 14388.67
[INFO 2017-06-28 15:02:11,495 main.py:51] epoch 3269, training loss: 9270.13, average training loss: 10154.18, base loss: 14390.10
[INFO 2017-06-28 15:02:12,181 main.py:51] epoch 3270, training loss: 9262.21, average training loss: 10152.59, base loss: 14389.08
[INFO 2017-06-28 15:02:12,828 main.py:51] epoch 3271, training loss: 9307.38, average training loss: 10150.46, base loss: 14386.75
[INFO 2017-06-28 15:02:13,437 main.py:51] epoch 3272, training loss: 10145.29, average training loss: 10150.06, base loss: 14387.79
[INFO 2017-06-28 15:02:14,129 main.py:51] epoch 3273, training loss: 10021.28, average training loss: 10150.22, base loss: 14388.25
[INFO 2017-06-28 15:02:14,909 main.py:51] epoch 3274, training loss: 10000.09, average training loss: 10150.22, base loss: 14388.82
[INFO 2017-06-28 15:02:15,693 main.py:51] epoch 3275, training loss: 9052.51, average training loss: 10149.48, base loss: 14387.53
[INFO 2017-06-28 15:02:16,331 main.py:51] epoch 3276, training loss: 8998.82, average training loss: 10147.17, base loss: 14383.69
[INFO 2017-06-28 15:02:17,075 main.py:51] epoch 3277, training loss: 11029.17, average training loss: 10147.66, base loss: 14382.00
[INFO 2017-06-28 15:02:17,924 main.py:51] epoch 3278, training loss: 9425.99, average training loss: 10146.71, base loss: 14380.46
[INFO 2017-06-28 15:02:18,655 main.py:51] epoch 3279, training loss: 9549.76, average training loss: 10144.86, base loss: 14378.68
[INFO 2017-06-28 15:02:19,360 main.py:51] epoch 3280, training loss: 9120.26, average training loss: 10142.55, base loss: 14375.60
[INFO 2017-06-28 15:02:20,146 main.py:51] epoch 3281, training loss: 9667.09, average training loss: 10141.65, base loss: 14376.05
[INFO 2017-06-28 15:02:20,940 main.py:51] epoch 3282, training loss: 10096.19, average training loss: 10143.20, base loss: 14377.63
[INFO 2017-06-28 15:02:21,603 main.py:51] epoch 3283, training loss: 10893.44, average training loss: 10143.41, base loss: 14376.61
[INFO 2017-06-28 15:02:22,357 main.py:51] epoch 3284, training loss: 9890.24, average training loss: 10143.24, base loss: 14377.72
[INFO 2017-06-28 15:02:23,179 main.py:51] epoch 3285, training loss: 9008.67, average training loss: 10141.98, base loss: 14375.71
[INFO 2017-06-28 15:02:23,865 main.py:51] epoch 3286, training loss: 10006.21, average training loss: 10141.68, base loss: 14375.55
[INFO 2017-06-28 15:02:24,612 main.py:51] epoch 3287, training loss: 11114.14, average training loss: 10142.97, base loss: 14376.93
[INFO 2017-06-28 15:02:25,430 main.py:51] epoch 3288, training loss: 9599.01, average training loss: 10143.98, base loss: 14377.91
[INFO 2017-06-28 15:02:26,261 main.py:51] epoch 3289, training loss: 9489.04, average training loss: 10142.81, base loss: 14376.14
[INFO 2017-06-28 15:02:26,944 main.py:51] epoch 3290, training loss: 8879.85, average training loss: 10142.20, base loss: 14375.63
[INFO 2017-06-28 15:02:27,671 main.py:51] epoch 3291, training loss: 10814.74, average training loss: 10143.69, base loss: 14376.95
[INFO 2017-06-28 15:02:28,504 main.py:51] epoch 3292, training loss: 10539.14, average training loss: 10144.52, base loss: 14378.62
[INFO 2017-06-28 15:02:29,236 main.py:51] epoch 3293, training loss: 10431.33, average training loss: 10145.17, base loss: 14380.56
[INFO 2017-06-28 15:02:29,927 main.py:51] epoch 3294, training loss: 10423.39, average training loss: 10145.07, base loss: 14379.55
[INFO 2017-06-28 15:02:30,747 main.py:51] epoch 3295, training loss: 10819.26, average training loss: 10144.78, base loss: 14378.47
[INFO 2017-06-28 15:02:31,562 main.py:51] epoch 3296, training loss: 10568.42, average training loss: 10145.09, base loss: 14379.97
[INFO 2017-06-28 15:02:32,207 main.py:51] epoch 3297, training loss: 9277.67, average training loss: 10143.98, base loss: 14378.51
[INFO 2017-06-28 15:02:32,961 main.py:51] epoch 3298, training loss: 9841.81, average training loss: 10145.14, base loss: 14379.81
[INFO 2017-06-28 15:02:33,807 main.py:51] epoch 3299, training loss: 10757.10, average training loss: 10146.98, base loss: 14382.08
[INFO 2017-06-28 15:02:33,807 main.py:53] epoch 3299, testing
[INFO 2017-06-28 15:02:36,731 main.py:105] average testing loss: 11189.83, base loss: 15213.97
[INFO 2017-06-28 15:02:36,731 main.py:106] improve_loss: 4024.14, improve_percent: 0.26
[INFO 2017-06-28 15:02:36,732 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 15:02:37,500 main.py:51] epoch 3300, training loss: 9569.14, average training loss: 10146.83, base loss: 14381.50
[INFO 2017-06-28 15:02:38,166 main.py:51] epoch 3301, training loss: 12611.82, average training loss: 10149.40, base loss: 14384.54
[INFO 2017-06-28 15:02:38,977 main.py:51] epoch 3302, training loss: 9630.49, average training loss: 10148.89, base loss: 14384.22
[INFO 2017-06-28 15:02:39,801 main.py:51] epoch 3303, training loss: 11177.28, average training loss: 10151.19, base loss: 14387.91
[INFO 2017-06-28 15:02:40,433 main.py:51] epoch 3304, training loss: 10577.73, average training loss: 10151.76, base loss: 14390.08
[INFO 2017-06-28 15:02:41,225 main.py:51] epoch 3305, training loss: 10104.83, average training loss: 10150.46, base loss: 14388.33
[INFO 2017-06-28 15:02:42,060 main.py:51] epoch 3306, training loss: 10874.53, average training loss: 10150.61, base loss: 14388.07
[INFO 2017-06-28 15:02:42,899 main.py:51] epoch 3307, training loss: 10775.83, average training loss: 10149.97, base loss: 14386.64
[INFO 2017-06-28 15:02:43,535 main.py:51] epoch 3308, training loss: 10663.38, average training loss: 10150.58, base loss: 14387.89
[INFO 2017-06-28 15:02:44,309 main.py:51] epoch 3309, training loss: 9728.50, average training loss: 10151.26, base loss: 14389.08
[INFO 2017-06-28 15:02:45,143 main.py:51] epoch 3310, training loss: 10642.25, average training loss: 10149.35, base loss: 14387.03
[INFO 2017-06-28 15:02:45,825 main.py:51] epoch 3311, training loss: 9637.75, average training loss: 10149.48, base loss: 14389.56
[INFO 2017-06-28 15:02:46,581 main.py:51] epoch 3312, training loss: 9199.27, average training loss: 10150.10, base loss: 14390.60
[INFO 2017-06-28 15:02:47,400 main.py:51] epoch 3313, training loss: 10350.79, average training loss: 10149.23, base loss: 14389.01
[INFO 2017-06-28 15:02:48,226 main.py:51] epoch 3314, training loss: 11165.69, average training loss: 10148.24, base loss: 14387.33
[INFO 2017-06-28 15:02:48,850 main.py:51] epoch 3315, training loss: 10074.19, average training loss: 10148.35, base loss: 14387.06
[INFO 2017-06-28 15:02:49,648 main.py:51] epoch 3316, training loss: 8843.29, average training loss: 10147.38, base loss: 14386.83
[INFO 2017-06-28 15:02:50,494 main.py:51] epoch 3317, training loss: 11176.95, average training loss: 10148.10, base loss: 14389.19
[INFO 2017-06-28 15:02:51,194 main.py:51] epoch 3318, training loss: 10971.53, average training loss: 10149.57, base loss: 14391.58
[INFO 2017-06-28 15:02:51,925 main.py:51] epoch 3319, training loss: 10423.15, average training loss: 10148.38, base loss: 14389.80
[INFO 2017-06-28 15:02:52,711 main.py:51] epoch 3320, training loss: 10534.01, average training loss: 10148.39, base loss: 14389.69
[INFO 2017-06-28 15:02:53,547 main.py:51] epoch 3321, training loss: 9440.24, average training loss: 10147.43, base loss: 14388.09
[INFO 2017-06-28 15:02:54,180 main.py:51] epoch 3322, training loss: 9798.26, average training loss: 10147.12, base loss: 14387.09
[INFO 2017-06-28 15:02:54,877 main.py:51] epoch 3323, training loss: 9159.61, average training loss: 10145.82, base loss: 14385.17
[INFO 2017-06-28 15:02:55,714 main.py:51] epoch 3324, training loss: 10666.40, average training loss: 10147.78, base loss: 14388.36
[INFO 2017-06-28 15:02:56,384 main.py:51] epoch 3325, training loss: 9811.27, average training loss: 10147.90, base loss: 14390.33
[INFO 2017-06-28 15:02:57,116 main.py:51] epoch 3326, training loss: 9747.71, average training loss: 10146.49, base loss: 14388.01
[INFO 2017-06-28 15:02:57,936 main.py:51] epoch 3327, training loss: 12252.58, average training loss: 10148.02, base loss: 14391.27
[INFO 2017-06-28 15:02:58,743 main.py:51] epoch 3328, training loss: 9241.99, average training loss: 10147.23, base loss: 14390.31
[INFO 2017-06-28 15:02:59,410 main.py:51] epoch 3329, training loss: 10431.47, average training loss: 10148.45, base loss: 14390.51
[INFO 2017-06-28 15:03:00,224 main.py:51] epoch 3330, training loss: 10981.29, average training loss: 10150.22, base loss: 14392.21
[INFO 2017-06-28 15:03:01,055 main.py:51] epoch 3331, training loss: 10462.83, average training loss: 10150.56, base loss: 14393.32
[INFO 2017-06-28 15:03:01,892 main.py:51] epoch 3332, training loss: 10180.30, average training loss: 10149.63, base loss: 14391.65
[INFO 2017-06-28 15:03:02,541 main.py:51] epoch 3333, training loss: 9461.15, average training loss: 10148.42, base loss: 14389.42
[INFO 2017-06-28 15:03:03,295 main.py:51] epoch 3334, training loss: 10621.77, average training loss: 10148.13, base loss: 14389.91
[INFO 2017-06-28 15:03:04,132 main.py:51] epoch 3335, training loss: 9643.82, average training loss: 10146.00, base loss: 14387.31
[INFO 2017-06-28 15:03:04,829 main.py:51] epoch 3336, training loss: 9946.19, average training loss: 10145.88, base loss: 14384.82
[INFO 2017-06-28 15:03:05,541 main.py:51] epoch 3337, training loss: 10515.09, average training loss: 10146.89, base loss: 14388.42
[INFO 2017-06-28 15:03:06,364 main.py:51] epoch 3338, training loss: 10790.80, average training loss: 10148.07, base loss: 14389.97
[INFO 2017-06-28 15:03:07,185 main.py:51] epoch 3339, training loss: 11675.41, average training loss: 10148.33, base loss: 14390.03
[INFO 2017-06-28 15:03:07,857 main.py:51] epoch 3340, training loss: 9755.78, average training loss: 10148.57, base loss: 14391.25
[INFO 2017-06-28 15:03:08,656 main.py:51] epoch 3341, training loss: 8624.90, average training loss: 10146.97, base loss: 14388.50
[INFO 2017-06-28 15:03:09,519 main.py:51] epoch 3342, training loss: 12193.51, average training loss: 10150.04, base loss: 14391.87
[INFO 2017-06-28 15:03:10,332 main.py:51] epoch 3343, training loss: 9208.77, average training loss: 10148.56, base loss: 14392.42
[INFO 2017-06-28 15:03:11,035 main.py:51] epoch 3344, training loss: 9309.09, average training loss: 10149.08, base loss: 14393.24
[INFO 2017-06-28 15:03:11,807 main.py:51] epoch 3345, training loss: 10506.99, average training loss: 10150.16, base loss: 14396.48
[INFO 2017-06-28 15:03:12,637 main.py:51] epoch 3346, training loss: 11128.50, average training loss: 10151.75, base loss: 14399.24
[INFO 2017-06-28 15:03:13,319 main.py:51] epoch 3347, training loss: 9818.25, average training loss: 10151.90, base loss: 14399.41
[INFO 2017-06-28 15:03:14,075 main.py:51] epoch 3348, training loss: 9660.52, average training loss: 10152.60, base loss: 14400.57
[INFO 2017-06-28 15:03:14,869 main.py:51] epoch 3349, training loss: 9666.34, average training loss: 10150.97, base loss: 14398.80
[INFO 2017-06-28 15:03:15,714 main.py:51] epoch 3350, training loss: 12117.86, average training loss: 10152.01, base loss: 14403.18
[INFO 2017-06-28 15:03:16,314 main.py:51] epoch 3351, training loss: 9315.55, average training loss: 10152.19, base loss: 14403.08
[INFO 2017-06-28 15:03:17,069 main.py:51] epoch 3352, training loss: 9846.77, average training loss: 10151.60, base loss: 14402.91
[INFO 2017-06-28 15:03:17,926 main.py:51] epoch 3353, training loss: 9375.80, average training loss: 10151.52, base loss: 14403.70
[INFO 2017-06-28 15:03:18,697 main.py:51] epoch 3354, training loss: 8959.97, average training loss: 10151.91, base loss: 14404.24
[INFO 2017-06-28 15:03:19,421 main.py:51] epoch 3355, training loss: 12301.78, average training loss: 10153.91, base loss: 14407.36
[INFO 2017-06-28 15:03:20,245 main.py:51] epoch 3356, training loss: 10252.11, average training loss: 10154.68, base loss: 14408.13
[INFO 2017-06-28 15:03:21,070 main.py:51] epoch 3357, training loss: 12811.01, average training loss: 10157.90, base loss: 14413.55
[INFO 2017-06-28 15:03:21,880 main.py:51] epoch 3358, training loss: 9107.86, average training loss: 10155.83, base loss: 14410.60
[INFO 2017-06-28 15:03:22,498 main.py:51] epoch 3359, training loss: 9975.49, average training loss: 10155.93, base loss: 14409.86
[INFO 2017-06-28 15:03:23,296 main.py:51] epoch 3360, training loss: 11868.23, average training loss: 10158.23, base loss: 14412.83
[INFO 2017-06-28 15:03:24,133 main.py:51] epoch 3361, training loss: 10244.31, average training loss: 10158.32, base loss: 14412.73
[INFO 2017-06-28 15:03:24,791 main.py:51] epoch 3362, training loss: 9077.86, average training loss: 10157.55, base loss: 14410.36
[INFO 2017-06-28 15:03:25,529 main.py:51] epoch 3363, training loss: 11732.28, average training loss: 10159.80, base loss: 14413.53
[INFO 2017-06-28 15:03:26,356 main.py:51] epoch 3364, training loss: 9127.54, average training loss: 10158.78, base loss: 14413.39
[INFO 2017-06-28 15:03:27,219 main.py:51] epoch 3365, training loss: 11076.27, average training loss: 10160.22, base loss: 14415.50
[INFO 2017-06-28 15:03:27,844 main.py:51] epoch 3366, training loss: 11549.09, average training loss: 10161.45, base loss: 14417.90
[INFO 2017-06-28 15:03:28,660 main.py:51] epoch 3367, training loss: 9556.08, average training loss: 10160.28, base loss: 14416.40
[INFO 2017-06-28 15:03:29,487 main.py:51] epoch 3368, training loss: 10393.50, average training loss: 10160.31, base loss: 14418.52
[INFO 2017-06-28 15:03:30,285 main.py:51] epoch 3369, training loss: 10015.50, average training loss: 10161.50, base loss: 14421.17
[INFO 2017-06-28 15:03:30,968 main.py:51] epoch 3370, training loss: 9696.67, average training loss: 10161.86, base loss: 14419.60
[INFO 2017-06-28 15:03:31,742 main.py:51] epoch 3371, training loss: 10172.70, average training loss: 10161.67, base loss: 14418.40
[INFO 2017-06-28 15:03:32,561 main.py:51] epoch 3372, training loss: 10302.38, average training loss: 10161.79, base loss: 14419.59
[INFO 2017-06-28 15:03:33,243 main.py:51] epoch 3373, training loss: 9622.58, average training loss: 10161.60, base loss: 14419.62
[INFO 2017-06-28 15:03:33,951 main.py:51] epoch 3374, training loss: 10325.81, average training loss: 10162.86, base loss: 14421.96
[INFO 2017-06-28 15:03:34,811 main.py:51] epoch 3375, training loss: 10860.41, average training loss: 10163.71, base loss: 14422.55
[INFO 2017-06-28 15:03:35,497 main.py:51] epoch 3376, training loss: 10166.06, average training loss: 10162.93, base loss: 14421.58
[INFO 2017-06-28 15:03:36,239 main.py:51] epoch 3377, training loss: 9283.91, average training loss: 10162.34, base loss: 14422.27
[INFO 2017-06-28 15:03:37,033 main.py:51] epoch 3378, training loss: 8789.16, average training loss: 10161.78, base loss: 14420.93
[INFO 2017-06-28 15:03:37,805 main.py:51] epoch 3379, training loss: 10451.93, average training loss: 10162.22, base loss: 14422.79
[INFO 2017-06-28 15:03:38,429 main.py:51] epoch 3380, training loss: 10540.98, average training loss: 10162.17, base loss: 14422.64
[INFO 2017-06-28 15:03:39,203 main.py:51] epoch 3381, training loss: 9304.00, average training loss: 10162.06, base loss: 14422.91
[INFO 2017-06-28 15:03:40,029 main.py:51] epoch 3382, training loss: 11824.36, average training loss: 10164.19, base loss: 14425.17
[INFO 2017-06-28 15:03:40,664 main.py:51] epoch 3383, training loss: 10457.76, average training loss: 10162.28, base loss: 14423.11
[INFO 2017-06-28 15:03:41,451 main.py:51] epoch 3384, training loss: 11726.67, average training loss: 10164.43, base loss: 14426.07
[INFO 2017-06-28 15:03:42,305 main.py:51] epoch 3385, training loss: 10405.28, average training loss: 10166.56, base loss: 14428.65
[INFO 2017-06-28 15:03:43,066 main.py:51] epoch 3386, training loss: 9441.64, average training loss: 10164.62, base loss: 14426.74
[INFO 2017-06-28 15:03:43,740 main.py:51] epoch 3387, training loss: 11196.84, average training loss: 10165.02, base loss: 14427.93
[INFO 2017-06-28 15:03:44,539 main.py:51] epoch 3388, training loss: 10001.88, average training loss: 10164.49, base loss: 14427.39
[INFO 2017-06-28 15:03:45,305 main.py:51] epoch 3389, training loss: 10893.79, average training loss: 10166.45, base loss: 14431.18
[INFO 2017-06-28 15:03:45,921 main.py:51] epoch 3390, training loss: 9110.28, average training loss: 10166.83, base loss: 14431.87
[INFO 2017-06-28 15:03:46,699 main.py:51] epoch 3391, training loss: 11022.38, average training loss: 10167.31, base loss: 14433.32
[INFO 2017-06-28 15:03:47,528 main.py:51] epoch 3392, training loss: 9792.33, average training loss: 10165.91, base loss: 14432.84
[INFO 2017-06-28 15:03:48,142 main.py:51] epoch 3393, training loss: 11042.62, average training loss: 10165.70, base loss: 14432.48
[INFO 2017-06-28 15:03:48,937 main.py:51] epoch 3394, training loss: 9688.52, average training loss: 10164.24, base loss: 14430.71
[INFO 2017-06-28 15:03:49,768 main.py:51] epoch 3395, training loss: 10636.40, average training loss: 10164.76, base loss: 14432.50
[INFO 2017-06-28 15:03:50,613 main.py:51] epoch 3396, training loss: 9390.42, average training loss: 10164.26, base loss: 14433.02
[INFO 2017-06-28 15:03:51,268 main.py:51] epoch 3397, training loss: 10579.52, average training loss: 10165.09, base loss: 14434.62
[INFO 2017-06-28 15:03:52,038 main.py:51] epoch 3398, training loss: 10010.93, average training loss: 10165.36, base loss: 14436.58
[INFO 2017-06-28 15:03:52,856 main.py:51] epoch 3399, training loss: 11139.58, average training loss: 10166.31, base loss: 14437.82
[INFO 2017-06-28 15:03:52,856 main.py:53] epoch 3399, testing
[INFO 2017-06-28 15:03:55,640 main.py:105] average testing loss: 11305.65, base loss: 15645.92
[INFO 2017-06-28 15:03:55,640 main.py:106] improve_loss: 4340.28, improve_percent: 0.28
[INFO 2017-06-28 15:03:55,641 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 15:03:56,259 main.py:51] epoch 3400, training loss: 10054.85, average training loss: 10164.63, base loss: 14434.45
[INFO 2017-06-28 15:03:57,002 main.py:51] epoch 3401, training loss: 9747.72, average training loss: 10165.39, base loss: 14435.35
[INFO 2017-06-28 15:03:57,825 main.py:51] epoch 3402, training loss: 10184.22, average training loss: 10164.52, base loss: 14435.43
[INFO 2017-06-28 15:03:58,508 main.py:51] epoch 3403, training loss: 9661.19, average training loss: 10164.94, base loss: 14436.74
[INFO 2017-06-28 15:03:59,239 main.py:51] epoch 3404, training loss: 9572.21, average training loss: 10165.40, base loss: 14439.25
[INFO 2017-06-28 15:03:59,885 main.py:51] epoch 3405, training loss: 12351.68, average training loss: 10165.18, base loss: 14437.65
[INFO 2017-06-28 15:04:00,512 main.py:51] epoch 3406, training loss: 10521.18, average training loss: 10165.84, base loss: 14438.54
[INFO 2017-06-28 15:04:01,151 main.py:51] epoch 3407, training loss: 9906.43, average training loss: 10165.97, base loss: 14437.77
[INFO 2017-06-28 15:04:01,838 main.py:51] epoch 3408, training loss: 10008.12, average training loss: 10166.27, base loss: 14439.39
[INFO 2017-06-28 15:04:02,447 main.py:51] epoch 3409, training loss: 9616.92, average training loss: 10165.28, base loss: 14439.44
[INFO 2017-06-28 15:04:03,070 main.py:51] epoch 3410, training loss: 10477.88, average training loss: 10165.99, base loss: 14441.41
[INFO 2017-06-28 15:04:03,851 main.py:51] epoch 3411, training loss: 10357.79, average training loss: 10167.34, base loss: 14444.59
[INFO 2017-06-28 15:04:04,711 main.py:51] epoch 3412, training loss: 10050.62, average training loss: 10167.27, base loss: 14443.97
[INFO 2017-06-28 15:04:05,443 main.py:51] epoch 3413, training loss: 10906.81, average training loss: 10168.19, base loss: 14447.62
[INFO 2017-06-28 15:04:06,125 main.py:51] epoch 3414, training loss: 10482.25, average training loss: 10170.64, base loss: 14452.14
[INFO 2017-06-28 15:04:06,951 main.py:51] epoch 3415, training loss: 9998.17, average training loss: 10170.62, base loss: 14453.77
[INFO 2017-06-28 15:04:07,693 main.py:51] epoch 3416, training loss: 10661.98, average training loss: 10169.69, base loss: 14452.87
[INFO 2017-06-28 15:04:08,395 main.py:51] epoch 3417, training loss: 9468.48, average training loss: 10167.07, base loss: 14449.93
[INFO 2017-06-28 15:04:09,184 main.py:51] epoch 3418, training loss: 11147.62, average training loss: 10167.55, base loss: 14450.79
[INFO 2017-06-28 15:04:10,030 main.py:51] epoch 3419, training loss: 9544.60, average training loss: 10167.26, base loss: 14450.91
[INFO 2017-06-28 15:04:10,612 main.py:51] epoch 3420, training loss: 9698.39, average training loss: 10167.07, base loss: 14450.11
[INFO 2017-06-28 15:04:11,372 main.py:51] epoch 3421, training loss: 9745.06, average training loss: 10169.09, base loss: 14453.50
[INFO 2017-06-28 15:04:12,174 main.py:51] epoch 3422, training loss: 10814.99, average training loss: 10168.73, base loss: 14453.24
[INFO 2017-06-28 15:04:12,804 main.py:51] epoch 3423, training loss: 10559.72, average training loss: 10170.20, base loss: 14453.96
[INFO 2017-06-28 15:04:13,574 main.py:51] epoch 3424, training loss: 9716.16, average training loss: 10167.23, base loss: 14451.69
[INFO 2017-06-28 15:04:14,385 main.py:51] epoch 3425, training loss: 9609.59, average training loss: 10166.87, base loss: 14451.62
[INFO 2017-06-28 15:04:15,057 main.py:51] epoch 3426, training loss: 9901.42, average training loss: 10166.35, base loss: 14450.87
[INFO 2017-06-28 15:04:15,791 main.py:51] epoch 3427, training loss: 10068.19, average training loss: 10167.75, base loss: 14452.48
[INFO 2017-06-28 15:04:16,618 main.py:51] epoch 3428, training loss: 11307.72, average training loss: 10168.64, base loss: 14453.10
[INFO 2017-06-28 15:04:17,430 main.py:51] epoch 3429, training loss: 9064.53, average training loss: 10166.71, base loss: 14449.42
[INFO 2017-06-28 15:04:18,086 main.py:51] epoch 3430, training loss: 9625.53, average training loss: 10167.28, base loss: 14450.66
[INFO 2017-06-28 15:04:18,846 main.py:51] epoch 3431, training loss: 9623.29, average training loss: 10165.29, base loss: 14447.55
[INFO 2017-06-28 15:04:19,667 main.py:51] epoch 3432, training loss: 9273.93, average training loss: 10164.99, base loss: 14447.82
[INFO 2017-06-28 15:04:20,321 main.py:51] epoch 3433, training loss: 10506.04, average training loss: 10165.94, base loss: 14451.51
[INFO 2017-06-28 15:04:21,039 main.py:51] epoch 3434, training loss: 11921.17, average training loss: 10168.03, base loss: 14454.61
[INFO 2017-06-28 15:04:21,874 main.py:51] epoch 3435, training loss: 9272.32, average training loss: 10168.57, base loss: 14455.44
[INFO 2017-06-28 15:04:22,518 main.py:51] epoch 3436, training loss: 9553.20, average training loss: 10167.85, base loss: 14452.88
[INFO 2017-06-28 15:04:23,265 main.py:51] epoch 3437, training loss: 10341.74, average training loss: 10167.56, base loss: 14451.73
[INFO 2017-06-28 15:04:24,127 main.py:51] epoch 3438, training loss: 10196.40, average training loss: 10166.86, base loss: 14451.04
[INFO 2017-06-28 15:04:24,888 main.py:51] epoch 3439, training loss: 9599.63, average training loss: 10164.56, base loss: 14447.84
[INFO 2017-06-28 15:04:25,573 main.py:51] epoch 3440, training loss: 11070.25, average training loss: 10165.12, base loss: 14448.21
[INFO 2017-06-28 15:04:26,390 main.py:51] epoch 3441, training loss: 12228.34, average training loss: 10166.20, base loss: 14448.97
[INFO 2017-06-28 15:04:27,216 main.py:51] epoch 3442, training loss: 9756.28, average training loss: 10165.99, base loss: 14448.16
[INFO 2017-06-28 15:04:27,856 main.py:51] epoch 3443, training loss: 11569.45, average training loss: 10167.72, base loss: 14451.43
[INFO 2017-06-28 15:04:28,636 main.py:51] epoch 3444, training loss: 8921.05, average training loss: 10165.48, base loss: 14446.49
[INFO 2017-06-28 15:04:29,492 main.py:51] epoch 3445, training loss: 11142.95, average training loss: 10166.34, base loss: 14446.87
[INFO 2017-06-28 15:04:30,181 main.py:51] epoch 3446, training loss: 10296.53, average training loss: 10166.61, base loss: 14447.00
[INFO 2017-06-28 15:04:30,908 main.py:51] epoch 3447, training loss: 10011.45, average training loss: 10167.30, base loss: 14448.10
[INFO 2017-06-28 15:04:31,734 main.py:51] epoch 3448, training loss: 11102.15, average training loss: 10167.03, base loss: 14448.18
[INFO 2017-06-28 15:04:32,571 main.py:51] epoch 3449, training loss: 10015.58, average training loss: 10167.60, base loss: 14449.48
[INFO 2017-06-28 15:04:33,213 main.py:51] epoch 3450, training loss: 9806.93, average training loss: 10166.89, base loss: 14449.20
[INFO 2017-06-28 15:04:33,975 main.py:51] epoch 3451, training loss: 10502.51, average training loss: 10166.84, base loss: 14449.41
[INFO 2017-06-28 15:04:34,814 main.py:51] epoch 3452, training loss: 11877.40, average training loss: 10167.73, base loss: 14452.00
[INFO 2017-06-28 15:04:35,456 main.py:51] epoch 3453, training loss: 9877.93, average training loss: 10168.99, base loss: 14454.96
[INFO 2017-06-28 15:04:36,267 main.py:51] epoch 3454, training loss: 10112.45, average training loss: 10168.07, base loss: 14453.85
[INFO 2017-06-28 15:04:37,099 main.py:51] epoch 3455, training loss: 10212.87, average training loss: 10168.44, base loss: 14455.53
[INFO 2017-06-28 15:04:37,891 main.py:51] epoch 3456, training loss: 10336.19, average training loss: 10168.14, base loss: 14456.18
[INFO 2017-06-28 15:04:38,595 main.py:51] epoch 3457, training loss: 11270.66, average training loss: 10168.47, base loss: 14456.63
[INFO 2017-06-28 15:04:39,411 main.py:51] epoch 3458, training loss: 9576.81, average training loss: 10166.35, base loss: 14453.53
[INFO 2017-06-28 15:04:40,262 main.py:51] epoch 3459, training loss: 9612.25, average training loss: 10163.62, base loss: 14449.46
[INFO 2017-06-28 15:04:41,072 main.py:51] epoch 3460, training loss: 9560.25, average training loss: 10161.94, base loss: 14447.83
[INFO 2017-06-28 15:04:41,746 main.py:51] epoch 3461, training loss: 9832.16, average training loss: 10162.60, base loss: 14448.44
[INFO 2017-06-28 15:04:42,570 main.py:51] epoch 3462, training loss: 10714.25, average training loss: 10164.04, base loss: 14452.38
[INFO 2017-06-28 15:04:43,402 main.py:51] epoch 3463, training loss: 11694.76, average training loss: 10165.11, base loss: 14453.79
[INFO 2017-06-28 15:04:44,246 main.py:51] epoch 3464, training loss: 10973.51, average training loss: 10167.21, base loss: 14456.77
[INFO 2017-06-28 15:04:44,879 main.py:51] epoch 3465, training loss: 10406.35, average training loss: 10167.36, base loss: 14456.02
[INFO 2017-06-28 15:04:45,646 main.py:51] epoch 3466, training loss: 11646.45, average training loss: 10169.21, base loss: 14458.74
[INFO 2017-06-28 15:04:46,509 main.py:51] epoch 3467, training loss: 9363.27, average training loss: 10169.18, base loss: 14458.24
[INFO 2017-06-28 15:04:47,285 main.py:51] epoch 3468, training loss: 11160.67, average training loss: 10170.30, base loss: 14461.74
[INFO 2017-06-28 15:04:47,961 main.py:51] epoch 3469, training loss: 9856.54, average training loss: 10168.69, base loss: 14459.91
[INFO 2017-06-28 15:04:48,742 main.py:51] epoch 3470, training loss: 10475.83, average training loss: 10168.78, base loss: 14459.53
[INFO 2017-06-28 15:04:49,566 main.py:51] epoch 3471, training loss: 8148.35, average training loss: 10167.55, base loss: 14458.20
[INFO 2017-06-28 15:04:50,189 main.py:51] epoch 3472, training loss: 11318.40, average training loss: 10169.08, base loss: 14459.87
[INFO 2017-06-28 15:04:50,984 main.py:51] epoch 3473, training loss: 8673.04, average training loss: 10169.09, base loss: 14459.34
[INFO 2017-06-28 15:04:51,818 main.py:51] epoch 3474, training loss: 11005.83, average training loss: 10169.72, base loss: 14460.09
[INFO 2017-06-28 15:04:52,618 main.py:51] epoch 3475, training loss: 12579.00, average training loss: 10172.20, base loss: 14462.40
[INFO 2017-06-28 15:04:53,322 main.py:51] epoch 3476, training loss: 9673.57, average training loss: 10171.79, base loss: 14461.43
[INFO 2017-06-28 15:04:54,103 main.py:51] epoch 3477, training loss: 10063.99, average training loss: 10170.38, base loss: 14458.93
[INFO 2017-06-28 15:04:54,946 main.py:51] epoch 3478, training loss: 8748.32, average training loss: 10169.30, base loss: 14457.83
[INFO 2017-06-28 15:04:55,708 main.py:51] epoch 3479, training loss: 8747.70, average training loss: 10168.66, base loss: 14456.46
[INFO 2017-06-28 15:04:56,377 main.py:51] epoch 3480, training loss: 9939.26, average training loss: 10167.12, base loss: 14454.80
[INFO 2017-06-28 15:04:57,182 main.py:51] epoch 3481, training loss: 8986.20, average training loss: 10166.09, base loss: 14454.47
[INFO 2017-06-28 15:04:58,001 main.py:51] epoch 3482, training loss: 9127.46, average training loss: 10164.65, base loss: 14452.85
[INFO 2017-06-28 15:04:58,648 main.py:51] epoch 3483, training loss: 9525.18, average training loss: 10164.62, base loss: 14452.57
[INFO 2017-06-28 15:04:59,460 main.py:51] epoch 3484, training loss: 10858.09, average training loss: 10165.80, base loss: 14453.97
[INFO 2017-06-28 15:05:00,293 main.py:51] epoch 3485, training loss: 9419.55, average training loss: 10163.03, base loss: 14450.03
[INFO 2017-06-28 15:05:01,110 main.py:51] epoch 3486, training loss: 9609.90, average training loss: 10163.54, base loss: 14450.40
[INFO 2017-06-28 15:05:01,740 main.py:51] epoch 3487, training loss: 10264.34, average training loss: 10164.34, base loss: 14451.05
[INFO 2017-06-28 15:05:02,531 main.py:51] epoch 3488, training loss: 10426.61, average training loss: 10163.94, base loss: 14451.66
[INFO 2017-06-28 15:05:03,415 main.py:51] epoch 3489, training loss: 9543.28, average training loss: 10163.44, base loss: 14450.02
[INFO 2017-06-28 15:05:04,168 main.py:51] epoch 3490, training loss: 8389.38, average training loss: 10162.76, base loss: 14448.11
[INFO 2017-06-28 15:05:04,854 main.py:51] epoch 3491, training loss: 9512.50, average training loss: 10161.41, base loss: 14446.00
[INFO 2017-06-28 15:05:05,662 main.py:51] epoch 3492, training loss: 9884.68, average training loss: 10161.88, base loss: 14446.66
[INFO 2017-06-28 15:05:06,500 main.py:51] epoch 3493, training loss: 9574.46, average training loss: 10161.64, base loss: 14445.35
[INFO 2017-06-28 15:05:07,144 main.py:51] epoch 3494, training loss: 10131.36, average training loss: 10160.69, base loss: 14444.27
[INFO 2017-06-28 15:05:07,904 main.py:51] epoch 3495, training loss: 9233.99, average training loss: 10159.02, base loss: 14441.85
[INFO 2017-06-28 15:05:08,763 main.py:51] epoch 3496, training loss: 10882.03, average training loss: 10159.48, base loss: 14443.37
[INFO 2017-06-28 15:05:09,526 main.py:51] epoch 3497, training loss: 10087.02, average training loss: 10159.97, base loss: 14445.28
[INFO 2017-06-28 15:05:10,234 main.py:51] epoch 3498, training loss: 10304.29, average training loss: 10159.69, base loss: 14443.58
[INFO 2017-06-28 15:05:10,991 main.py:51] epoch 3499, training loss: 9708.05, average training loss: 10158.29, base loss: 14442.44
[INFO 2017-06-28 15:05:10,991 main.py:53] epoch 3499, testing
[INFO 2017-06-28 15:05:13,830 main.py:105] average testing loss: 11043.00, base loss: 15293.01
[INFO 2017-06-28 15:05:13,830 main.py:106] improve_loss: 4250.01, improve_percent: 0.28
[INFO 2017-06-28 15:05:13,831 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 15:05:14,670 main.py:51] epoch 3500, training loss: 8791.96, average training loss: 10156.20, base loss: 14440.45
[INFO 2017-06-28 15:05:15,332 main.py:51] epoch 3501, training loss: 11473.90, average training loss: 10158.14, base loss: 14444.71
[INFO 2017-06-28 15:05:16,084 main.py:51] epoch 3502, training loss: 10237.85, average training loss: 10159.41, base loss: 14446.71
[INFO 2017-06-28 15:05:16,906 main.py:51] epoch 3503, training loss: 10592.65, average training loss: 10157.79, base loss: 14443.14
[INFO 2017-06-28 15:05:17,762 main.py:51] epoch 3504, training loss: 10721.74, average training loss: 10159.06, base loss: 14445.80
[INFO 2017-06-28 15:05:18,432 main.py:51] epoch 3505, training loss: 9910.22, average training loss: 10159.44, base loss: 14446.08
[INFO 2017-06-28 15:05:19,178 main.py:51] epoch 3506, training loss: 8742.77, average training loss: 10158.62, base loss: 14444.34
[INFO 2017-06-28 15:05:20,002 main.py:51] epoch 3507, training loss: 10308.14, average training loss: 10159.77, base loss: 14446.28
[INFO 2017-06-28 15:05:20,845 main.py:51] epoch 3508, training loss: 10887.98, average training loss: 10160.42, base loss: 14448.25
[INFO 2017-06-28 15:05:21,487 main.py:51] epoch 3509, training loss: 10092.69, average training loss: 10159.73, base loss: 14446.83
[INFO 2017-06-28 15:05:22,281 main.py:51] epoch 3510, training loss: 9223.03, average training loss: 10158.50, base loss: 14445.95
[INFO 2017-06-28 15:05:23,096 main.py:51] epoch 3511, training loss: 9487.50, average training loss: 10159.05, base loss: 14449.09
[INFO 2017-06-28 15:05:23,915 main.py:51] epoch 3512, training loss: 9235.45, average training loss: 10158.63, base loss: 14447.78
[INFO 2017-06-28 15:05:24,596 main.py:51] epoch 3513, training loss: 10859.99, average training loss: 10159.57, base loss: 14448.21
[INFO 2017-06-28 15:05:25,365 main.py:51] epoch 3514, training loss: 8960.53, average training loss: 10158.64, base loss: 14448.05
[INFO 2017-06-28 15:05:26,192 main.py:51] epoch 3515, training loss: 10323.03, average training loss: 10160.03, base loss: 14450.77
[INFO 2017-06-28 15:05:26,878 main.py:51] epoch 3516, training loss: 10562.35, average training loss: 10161.05, base loss: 14452.14
[INFO 2017-06-28 15:05:27,639 main.py:51] epoch 3517, training loss: 9575.17, average training loss: 10156.46, base loss: 14447.06
[INFO 2017-06-28 15:05:28,411 main.py:51] epoch 3518, training loss: 9121.01, average training loss: 10155.16, base loss: 14446.53
[INFO 2017-06-28 15:05:29,191 main.py:51] epoch 3519, training loss: 10209.60, average training loss: 10155.31, base loss: 14447.30
[INFO 2017-06-28 15:05:29,862 main.py:51] epoch 3520, training loss: 10441.58, average training loss: 10156.77, base loss: 14450.41
[INFO 2017-06-28 15:05:30,620 main.py:51] epoch 3521, training loss: 10019.42, average training loss: 10154.51, base loss: 14450.24
[INFO 2017-06-28 15:05:31,465 main.py:51] epoch 3522, training loss: 8857.08, average training loss: 10152.49, base loss: 14448.97
[INFO 2017-06-28 15:05:32,087 main.py:51] epoch 3523, training loss: 8889.75, average training loss: 10152.64, base loss: 14449.37
[INFO 2017-06-28 15:05:32,790 main.py:51] epoch 3524, training loss: 11101.95, average training loss: 10153.94, base loss: 14451.43
[INFO 2017-06-28 15:05:33,607 main.py:51] epoch 3525, training loss: 11018.16, average training loss: 10154.83, base loss: 14454.03
[INFO 2017-06-28 15:05:34,217 main.py:51] epoch 3526, training loss: 10220.43, average training loss: 10155.17, base loss: 14454.72
[INFO 2017-06-28 15:05:34,960 main.py:51] epoch 3527, training loss: 10344.29, average training loss: 10155.81, base loss: 14456.09
[INFO 2017-06-28 15:05:35,795 main.py:51] epoch 3528, training loss: 9752.39, average training loss: 10155.10, base loss: 14454.94
[INFO 2017-06-28 15:05:36,479 main.py:51] epoch 3529, training loss: 10600.46, average training loss: 10155.21, base loss: 14455.88
[INFO 2017-06-28 15:05:37,199 main.py:51] epoch 3530, training loss: 10877.42, average training loss: 10154.88, base loss: 14456.02
[INFO 2017-06-28 15:05:38,012 main.py:51] epoch 3531, training loss: 11203.34, average training loss: 10156.05, base loss: 14457.85
[INFO 2017-06-28 15:05:38,824 main.py:51] epoch 3532, training loss: 10650.85, average training loss: 10157.48, base loss: 14460.59
[INFO 2017-06-28 15:05:39,456 main.py:51] epoch 3533, training loss: 9769.96, average training loss: 10157.03, base loss: 14460.57
[INFO 2017-06-28 15:05:40,243 main.py:51] epoch 3534, training loss: 10715.48, average training loss: 10155.69, base loss: 14460.11
[INFO 2017-06-28 15:05:41,057 main.py:51] epoch 3535, training loss: 9253.37, average training loss: 10154.60, base loss: 14460.05
[INFO 2017-06-28 15:05:41,745 main.py:51] epoch 3536, training loss: 10763.84, average training loss: 10155.84, base loss: 14461.63
[INFO 2017-06-28 15:05:42,493 main.py:51] epoch 3537, training loss: 9989.05, average training loss: 10155.33, base loss: 14462.69
[INFO 2017-06-28 15:05:43,311 main.py:51] epoch 3538, training loss: 9316.46, average training loss: 10153.39, base loss: 14459.82
[INFO 2017-06-28 15:05:44,131 main.py:51] epoch 3539, training loss: 11073.00, average training loss: 10155.10, base loss: 14461.82
[INFO 2017-06-28 15:05:44,800 main.py:51] epoch 3540, training loss: 9881.48, average training loss: 10154.41, base loss: 14460.87
[INFO 2017-06-28 15:05:45,547 main.py:51] epoch 3541, training loss: 9506.45, average training loss: 10153.97, base loss: 14460.19
[INFO 2017-06-28 15:05:46,380 main.py:51] epoch 3542, training loss: 8616.69, average training loss: 10153.21, base loss: 14459.87
[INFO 2017-06-28 15:05:47,071 main.py:51] epoch 3543, training loss: 10185.70, average training loss: 10152.81, base loss: 14457.93
[INFO 2017-06-28 15:05:47,779 main.py:51] epoch 3544, training loss: 9796.19, average training loss: 10154.24, base loss: 14461.01
[INFO 2017-06-28 15:05:48,610 main.py:51] epoch 3545, training loss: 11347.58, average training loss: 10155.66, base loss: 14463.49
[INFO 2017-06-28 15:05:49,312 main.py:51] epoch 3546, training loss: 10645.13, average training loss: 10156.49, base loss: 14465.58
[INFO 2017-06-28 15:05:49,970 main.py:51] epoch 3547, training loss: 11242.16, average training loss: 10156.03, base loss: 14465.70
[INFO 2017-06-28 15:05:50,588 main.py:51] epoch 3548, training loss: 11249.23, average training loss: 10158.06, base loss: 14469.18
[INFO 2017-06-28 15:05:51,215 main.py:51] epoch 3549, training loss: 11303.37, average training loss: 10160.92, base loss: 14472.00
[INFO 2017-06-28 15:05:51,897 main.py:51] epoch 3550, training loss: 8398.47, average training loss: 10159.00, base loss: 14469.14
[INFO 2017-06-28 15:05:52,510 main.py:51] epoch 3551, training loss: 8996.23, average training loss: 10156.57, base loss: 14465.58
[INFO 2017-06-28 15:05:53,331 main.py:51] epoch 3552, training loss: 10128.13, average training loss: 10157.28, base loss: 14465.96
[INFO 2017-06-28 15:05:53,990 main.py:51] epoch 3553, training loss: 8854.88, average training loss: 10154.77, base loss: 14461.50
[INFO 2017-06-28 15:05:54,747 main.py:51] epoch 3554, training loss: 11494.20, average training loss: 10156.63, base loss: 14463.38
[INFO 2017-06-28 15:05:55,578 main.py:51] epoch 3555, training loss: 9073.33, average training loss: 10154.94, base loss: 14461.68
[INFO 2017-06-28 15:05:56,394 main.py:51] epoch 3556, training loss: 9125.04, average training loss: 10152.35, base loss: 14458.31
[INFO 2017-06-28 15:05:57,056 main.py:51] epoch 3557, training loss: 10965.72, average training loss: 10153.90, base loss: 14459.60
[INFO 2017-06-28 15:05:57,813 main.py:51] epoch 3558, training loss: 11173.12, average training loss: 10153.98, base loss: 14460.47
[INFO 2017-06-28 15:05:58,637 main.py:51] epoch 3559, training loss: 9665.99, average training loss: 10152.73, base loss: 14460.08
[INFO 2017-06-28 15:05:59,303 main.py:51] epoch 3560, training loss: 9792.06, average training loss: 10151.94, base loss: 14459.01
[INFO 2017-06-28 15:06:00,048 main.py:51] epoch 3561, training loss: 9619.76, average training loss: 10151.07, base loss: 14457.75
[INFO 2017-06-28 15:06:00,854 main.py:51] epoch 3562, training loss: 11504.76, average training loss: 10153.39, base loss: 14461.97
[INFO 2017-06-28 15:06:01,573 main.py:51] epoch 3563, training loss: 10025.97, average training loss: 10153.80, base loss: 14462.56
[INFO 2017-06-28 15:06:02,266 main.py:51] epoch 3564, training loss: 11038.23, average training loss: 10156.17, base loss: 14466.01
[INFO 2017-06-28 15:06:03,066 main.py:51] epoch 3565, training loss: 9048.67, average training loss: 10156.00, base loss: 14465.99
[INFO 2017-06-28 15:06:03,851 main.py:51] epoch 3566, training loss: 10881.16, average training loss: 10156.74, base loss: 14468.09
[INFO 2017-06-28 15:06:04,518 main.py:51] epoch 3567, training loss: 9229.90, average training loss: 10155.41, base loss: 14466.38
[INFO 2017-06-28 15:06:05,327 main.py:51] epoch 3568, training loss: 11538.88, average training loss: 10156.39, base loss: 14467.43
[INFO 2017-06-28 15:06:06,140 main.py:51] epoch 3569, training loss: 10086.24, average training loss: 10156.59, base loss: 14467.89
[INFO 2017-06-28 15:06:06,868 main.py:51] epoch 3570, training loss: 9964.45, average training loss: 10154.90, base loss: 14463.59
[INFO 2017-06-28 15:06:07,552 main.py:51] epoch 3571, training loss: 11872.83, average training loss: 10157.27, base loss: 14467.52
[INFO 2017-06-28 15:06:08,385 main.py:51] epoch 3572, training loss: 9634.19, average training loss: 10153.85, base loss: 14464.18
[INFO 2017-06-28 15:06:09,225 main.py:51] epoch 3573, training loss: 10005.47, average training loss: 10154.53, base loss: 14467.24
[INFO 2017-06-28 15:06:09,852 main.py:51] epoch 3574, training loss: 8590.38, average training loss: 10154.64, base loss: 14467.94
[INFO 2017-06-28 15:06:10,634 main.py:51] epoch 3575, training loss: 9735.47, average training loss: 10155.09, base loss: 14468.95
[INFO 2017-06-28 15:06:11,477 main.py:51] epoch 3576, training loss: 10453.91, average training loss: 10155.82, base loss: 14470.69
[INFO 2017-06-28 15:06:12,144 main.py:51] epoch 3577, training loss: 8994.85, average training loss: 10153.77, base loss: 14469.18
[INFO 2017-06-28 15:06:12,885 main.py:51] epoch 3578, training loss: 10966.03, average training loss: 10154.41, base loss: 14468.80
[INFO 2017-06-28 15:06:13,703 main.py:51] epoch 3579, training loss: 10671.10, average training loss: 10153.64, base loss: 14467.45
[INFO 2017-06-28 15:06:14,531 main.py:51] epoch 3580, training loss: 8812.45, average training loss: 10150.97, base loss: 14463.92
[INFO 2017-06-28 15:06:15,177 main.py:51] epoch 3581, training loss: 10496.13, average training loss: 10152.03, base loss: 14465.08
[INFO 2017-06-28 15:06:15,950 main.py:51] epoch 3582, training loss: 9305.97, average training loss: 10152.40, base loss: 14465.00
[INFO 2017-06-28 15:06:16,770 main.py:51] epoch 3583, training loss: 10955.20, average training loss: 10154.91, base loss: 14468.55
[INFO 2017-06-28 15:06:17,436 main.py:51] epoch 3584, training loss: 10205.58, average training loss: 10155.20, base loss: 14469.84
[INFO 2017-06-28 15:06:18,185 main.py:51] epoch 3585, training loss: 10929.80, average training loss: 10155.53, base loss: 14470.28
[INFO 2017-06-28 15:06:19,003 main.py:51] epoch 3586, training loss: 10192.91, average training loss: 10155.00, base loss: 14468.14
[INFO 2017-06-28 15:06:19,837 main.py:51] epoch 3587, training loss: 9210.70, average training loss: 10153.70, base loss: 14466.32
[INFO 2017-06-28 15:06:20,516 main.py:51] epoch 3588, training loss: 9636.19, average training loss: 10153.00, base loss: 14466.14
[INFO 2017-06-28 15:06:21,270 main.py:51] epoch 3589, training loss: 10954.42, average training loss: 10154.21, base loss: 14469.95
[INFO 2017-06-28 15:06:22,116 main.py:51] epoch 3590, training loss: 9449.12, average training loss: 10153.68, base loss: 14468.80
[INFO 2017-06-28 15:06:22,813 main.py:51] epoch 3591, training loss: 8762.65, average training loss: 10152.56, base loss: 14468.33
[INFO 2017-06-28 15:06:23,536 main.py:51] epoch 3592, training loss: 11092.72, average training loss: 10152.03, base loss: 14468.92
[INFO 2017-06-28 15:06:24,362 main.py:51] epoch 3593, training loss: 9409.46, average training loss: 10152.57, base loss: 14469.76
[INFO 2017-06-28 15:06:25,183 main.py:51] epoch 3594, training loss: 10655.31, average training loss: 10154.09, base loss: 14472.96
[INFO 2017-06-28 15:06:25,802 main.py:51] epoch 3595, training loss: 11755.76, average training loss: 10155.92, base loss: 14475.70
[INFO 2017-06-28 15:06:26,555 main.py:51] epoch 3596, training loss: 10067.58, average training loss: 10156.37, base loss: 14475.63
[INFO 2017-06-28 15:06:27,370 main.py:51] epoch 3597, training loss: 10078.27, average training loss: 10156.29, base loss: 14477.05
[INFO 2017-06-28 15:06:28,027 main.py:51] epoch 3598, training loss: 12598.23, average training loss: 10155.88, base loss: 14477.94
[INFO 2017-06-28 15:06:28,794 main.py:51] epoch 3599, training loss: 8033.64, average training loss: 10153.65, base loss: 14476.45
[INFO 2017-06-28 15:06:28,795 main.py:53] epoch 3599, testing
[INFO 2017-06-28 15:06:31,553 main.py:105] average testing loss: 10887.14, base loss: 15151.39
[INFO 2017-06-28 15:06:31,553 main.py:106] improve_loss: 4264.26, improve_percent: 0.28
[INFO 2017-06-28 15:06:31,553 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 15:06:32,384 main.py:51] epoch 3600, training loss: 11659.99, average training loss: 10156.40, base loss: 14480.93
[INFO 2017-06-28 15:06:33,117 main.py:51] epoch 3601, training loss: 8997.60, average training loss: 10155.93, base loss: 14480.80
[INFO 2017-06-28 15:06:33,800 main.py:51] epoch 3602, training loss: 11170.07, average training loss: 10156.56, base loss: 14481.62
[INFO 2017-06-28 15:06:34,618 main.py:51] epoch 3603, training loss: 9507.13, average training loss: 10155.77, base loss: 14481.68
[INFO 2017-06-28 15:06:35,502 main.py:51] epoch 3604, training loss: 10446.07, average training loss: 10156.66, base loss: 14483.10
[INFO 2017-06-28 15:06:36,136 main.py:51] epoch 3605, training loss: 9985.97, average training loss: 10156.13, base loss: 14480.78
[INFO 2017-06-28 15:06:36,929 main.py:51] epoch 3606, training loss: 9917.10, average training loss: 10154.95, base loss: 14478.79
[INFO 2017-06-28 15:06:37,762 main.py:51] epoch 3607, training loss: 9341.00, average training loss: 10152.86, base loss: 14477.41
[INFO 2017-06-28 15:06:38,620 main.py:51] epoch 3608, training loss: 9364.29, average training loss: 10151.18, base loss: 14477.02
[INFO 2017-06-28 15:06:39,282 main.py:51] epoch 3609, training loss: 9552.87, average training loss: 10149.52, base loss: 14475.61
[INFO 2017-06-28 15:06:40,071 main.py:51] epoch 3610, training loss: 10017.83, average training loss: 10147.49, base loss: 14473.89
[INFO 2017-06-28 15:06:40,880 main.py:51] epoch 3611, training loss: 9207.50, average training loss: 10147.06, base loss: 14471.95
[INFO 2017-06-28 15:06:41,571 main.py:51] epoch 3612, training loss: 9935.91, average training loss: 10147.11, base loss: 14471.32
[INFO 2017-06-28 15:06:42,299 main.py:51] epoch 3613, training loss: 9948.78, average training loss: 10145.77, base loss: 14469.09
[INFO 2017-06-28 15:06:43,106 main.py:51] epoch 3614, training loss: 10326.80, average training loss: 10146.49, base loss: 14469.88
[INFO 2017-06-28 15:06:43,877 main.py:51] epoch 3615, training loss: 9214.35, average training loss: 10144.63, base loss: 14467.93
[INFO 2017-06-28 15:06:44,578 main.py:51] epoch 3616, training loss: 12047.85, average training loss: 10146.11, base loss: 14472.23
[INFO 2017-06-28 15:06:45,395 main.py:51] epoch 3617, training loss: 9982.06, average training loss: 10146.25, base loss: 14471.39
[INFO 2017-06-28 15:06:46,180 main.py:51] epoch 3618, training loss: 9574.46, average training loss: 10144.75, base loss: 14469.88
[INFO 2017-06-28 15:06:46,849 main.py:51] epoch 3619, training loss: 8423.50, average training loss: 10143.95, base loss: 14468.17
[INFO 2017-06-28 15:06:47,589 main.py:51] epoch 3620, training loss: 11772.40, average training loss: 10145.74, base loss: 14468.43
[INFO 2017-06-28 15:06:48,421 main.py:51] epoch 3621, training loss: 9196.14, average training loss: 10145.00, base loss: 14467.91
[INFO 2017-06-28 15:06:49,207 main.py:51] epoch 3622, training loss: 9119.41, average training loss: 10142.56, base loss: 14465.26
[INFO 2017-06-28 15:06:49,879 main.py:51] epoch 3623, training loss: 8701.82, average training loss: 10140.18, base loss: 14461.68
[INFO 2017-06-28 15:06:50,639 main.py:51] epoch 3624, training loss: 9306.21, average training loss: 10137.75, base loss: 14458.62
[INFO 2017-06-28 15:06:51,461 main.py:51] epoch 3625, training loss: 11710.50, average training loss: 10137.74, base loss: 14457.77
[INFO 2017-06-28 15:06:52,101 main.py:51] epoch 3626, training loss: 10505.85, average training loss: 10138.85, base loss: 14460.32
[INFO 2017-06-28 15:06:52,856 main.py:51] epoch 3627, training loss: 10736.33, average training loss: 10139.39, base loss: 14461.33
[INFO 2017-06-28 15:06:53,676 main.py:51] epoch 3628, training loss: 10878.82, average training loss: 10140.71, base loss: 14461.57
[INFO 2017-06-28 15:06:54,506 main.py:51] epoch 3629, training loss: 9717.98, average training loss: 10140.10, base loss: 14460.38
[INFO 2017-06-28 15:06:55,148 main.py:51] epoch 3630, training loss: 9865.30, average training loss: 10140.17, base loss: 14461.94
[INFO 2017-06-28 15:06:55,965 main.py:51] epoch 3631, training loss: 9115.38, average training loss: 10137.11, base loss: 14456.58
[INFO 2017-06-28 15:06:56,804 main.py:51] epoch 3632, training loss: 9765.07, average training loss: 10137.16, base loss: 14457.36
[INFO 2017-06-28 15:06:57,556 main.py:51] epoch 3633, training loss: 10983.89, average training loss: 10138.35, base loss: 14459.97
[INFO 2017-06-28 15:06:58,292 main.py:51] epoch 3634, training loss: 10483.95, average training loss: 10138.22, base loss: 14459.70
[INFO 2017-06-28 15:06:59,067 main.py:51] epoch 3635, training loss: 10812.55, average training loss: 10137.62, base loss: 14458.81
[INFO 2017-06-28 15:06:59,926 main.py:51] epoch 3636, training loss: 10753.49, average training loss: 10137.40, base loss: 14459.33
[INFO 2017-06-28 15:07:00,607 main.py:51] epoch 3637, training loss: 9890.20, average training loss: 10137.79, base loss: 14461.31
[INFO 2017-06-28 15:07:01,327 main.py:51] epoch 3638, training loss: 10790.90, average training loss: 10138.07, base loss: 14462.10
[INFO 2017-06-28 15:07:02,150 main.py:51] epoch 3639, training loss: 9670.20, average training loss: 10137.26, base loss: 14459.21
[INFO 2017-06-28 15:07:03,029 main.py:51] epoch 3640, training loss: 8881.40, average training loss: 10136.35, base loss: 14457.36
[INFO 2017-06-28 15:07:03,717 main.py:51] epoch 3641, training loss: 9909.75, average training loss: 10136.97, base loss: 14458.11
[INFO 2017-06-28 15:07:04,461 main.py:51] epoch 3642, training loss: 11034.77, average training loss: 10137.95, base loss: 14460.51
[INFO 2017-06-28 15:07:05,291 main.py:51] epoch 3643, training loss: 9864.06, average training loss: 10138.51, base loss: 14461.15
[INFO 2017-06-28 15:07:06,096 main.py:51] epoch 3644, training loss: 9693.87, average training loss: 10137.69, base loss: 14459.97
[INFO 2017-06-28 15:07:06,798 main.py:51] epoch 3645, training loss: 10256.24, average training loss: 10138.39, base loss: 14459.92
[INFO 2017-06-28 15:07:07,619 main.py:51] epoch 3646, training loss: 9537.95, average training loss: 10137.70, base loss: 14458.87
[INFO 2017-06-28 15:07:08,454 main.py:51] epoch 3647, training loss: 8685.01, average training loss: 10137.32, base loss: 14459.95
[INFO 2017-06-28 15:07:09,222 main.py:51] epoch 3648, training loss: 10967.03, average training loss: 10138.90, base loss: 14462.06
[INFO 2017-06-28 15:07:09,913 main.py:51] epoch 3649, training loss: 9071.76, average training loss: 10138.97, base loss: 14463.20
[INFO 2017-06-28 15:07:10,699 main.py:51] epoch 3650, training loss: 10285.34, average training loss: 10138.89, base loss: 14462.67
[INFO 2017-06-28 15:07:11,553 main.py:51] epoch 3651, training loss: 9694.06, average training loss: 10138.33, base loss: 14461.86
[INFO 2017-06-28 15:07:12,207 main.py:51] epoch 3652, training loss: 9672.90, average training loss: 10137.92, base loss: 14460.02
[INFO 2017-06-28 15:07:12,970 main.py:51] epoch 3653, training loss: 9360.48, average training loss: 10136.42, base loss: 14458.94
[INFO 2017-06-28 15:07:13,777 main.py:51] epoch 3654, training loss: 8364.71, average training loss: 10135.72, base loss: 14456.81
[INFO 2017-06-28 15:07:14,613 main.py:51] epoch 3655, training loss: 10447.03, average training loss: 10136.52, base loss: 14459.27
[INFO 2017-06-28 15:07:15,246 main.py:51] epoch 3656, training loss: 11079.25, average training loss: 10137.51, base loss: 14459.79
[INFO 2017-06-28 15:07:16,007 main.py:51] epoch 3657, training loss: 9604.24, average training loss: 10136.82, base loss: 14458.00
[INFO 2017-06-28 15:07:16,823 main.py:51] epoch 3658, training loss: 9234.30, average training loss: 10136.09, base loss: 14457.46
[INFO 2017-06-28 15:07:17,498 main.py:51] epoch 3659, training loss: 10356.79, average training loss: 10135.76, base loss: 14456.00
[INFO 2017-06-28 15:07:18,246 main.py:51] epoch 3660, training loss: 9953.50, average training loss: 10135.40, base loss: 14456.76
[INFO 2017-06-28 15:07:19,071 main.py:51] epoch 3661, training loss: 8930.35, average training loss: 10134.14, base loss: 14455.79
[INFO 2017-06-28 15:07:19,857 main.py:51] epoch 3662, training loss: 9479.10, average training loss: 10132.44, base loss: 14452.96
[INFO 2017-06-28 15:07:20,542 main.py:51] epoch 3663, training loss: 8624.50, average training loss: 10130.83, base loss: 14450.69
[INFO 2017-06-28 15:07:21,333 main.py:51] epoch 3664, training loss: 10223.60, average training loss: 10131.44, base loss: 14451.33
[INFO 2017-06-28 15:07:22,164 main.py:51] epoch 3665, training loss: 8985.88, average training loss: 10130.27, base loss: 14451.04
[INFO 2017-06-28 15:07:22,855 main.py:51] epoch 3666, training loss: 11044.94, average training loss: 10131.15, base loss: 14451.33
[INFO 2017-06-28 15:07:23,593 main.py:51] epoch 3667, training loss: 9201.21, average training loss: 10130.19, base loss: 14449.46
[INFO 2017-06-28 15:07:24,419 main.py:51] epoch 3668, training loss: 10118.96, average training loss: 10130.50, base loss: 14450.16
[INFO 2017-06-28 15:07:25,146 main.py:51] epoch 3669, training loss: 9496.46, average training loss: 10129.35, base loss: 14449.40
[INFO 2017-06-28 15:07:25,843 main.py:51] epoch 3670, training loss: 9086.99, average training loss: 10130.23, base loss: 14450.91
[INFO 2017-06-28 15:07:26,644 main.py:51] epoch 3671, training loss: 9171.10, average training loss: 10128.75, base loss: 14448.79
[INFO 2017-06-28 15:07:27,479 main.py:51] epoch 3672, training loss: 8923.12, average training loss: 10128.82, base loss: 14449.63
[INFO 2017-06-28 15:07:28,120 main.py:51] epoch 3673, training loss: 10452.23, average training loss: 10129.09, base loss: 14450.92
[INFO 2017-06-28 15:07:28,934 main.py:51] epoch 3674, training loss: 9875.92, average training loss: 10128.29, base loss: 14451.63
[INFO 2017-06-28 15:07:29,744 main.py:51] epoch 3675, training loss: 10137.91, average training loss: 10128.13, base loss: 14452.28
[INFO 2017-06-28 15:07:30,580 main.py:51] epoch 3676, training loss: 10659.53, average training loss: 10128.92, base loss: 14451.94
[INFO 2017-06-28 15:07:31,184 main.py:51] epoch 3677, training loss: 9525.45, average training loss: 10129.44, base loss: 14452.38
[INFO 2017-06-28 15:07:31,970 main.py:51] epoch 3678, training loss: 10950.12, average training loss: 10130.19, base loss: 14451.88
[INFO 2017-06-28 15:07:32,774 main.py:51] epoch 3679, training loss: 11392.49, average training loss: 10130.91, base loss: 14452.33
[INFO 2017-06-28 15:07:33,387 main.py:51] epoch 3680, training loss: 11698.48, average training loss: 10132.63, base loss: 14453.58
[INFO 2017-06-28 15:07:34,160 main.py:51] epoch 3681, training loss: 10558.74, average training loss: 10133.71, base loss: 14455.94
[INFO 2017-06-28 15:07:34,993 main.py:51] epoch 3682, training loss: 9079.21, average training loss: 10131.67, base loss: 14454.71
[INFO 2017-06-28 15:07:35,631 main.py:51] epoch 3683, training loss: 9283.98, average training loss: 10129.25, base loss: 14451.26
[INFO 2017-06-28 15:07:36,427 main.py:51] epoch 3684, training loss: 8320.96, average training loss: 10126.93, base loss: 14447.35
[INFO 2017-06-28 15:07:37,256 main.py:51] epoch 3685, training loss: 10410.06, average training loss: 10128.19, base loss: 14449.32
[INFO 2017-06-28 15:07:38,087 main.py:51] epoch 3686, training loss: 9020.52, average training loss: 10127.45, base loss: 14450.04
[INFO 2017-06-28 15:07:38,742 main.py:51] epoch 3687, training loss: 9300.41, average training loss: 10127.95, base loss: 14451.24
[INFO 2017-06-28 15:07:39,415 main.py:51] epoch 3688, training loss: 10551.99, average training loss: 10128.61, base loss: 14452.67
[INFO 2017-06-28 15:07:40,030 main.py:51] epoch 3689, training loss: 10660.49, average training loss: 10129.56, base loss: 14454.22
[INFO 2017-06-28 15:07:40,623 main.py:51] epoch 3690, training loss: 11424.77, average training loss: 10131.13, base loss: 14455.62
[INFO 2017-06-28 15:07:41,222 main.py:51] epoch 3691, training loss: 9830.06, average training loss: 10132.27, base loss: 14457.74
[INFO 2017-06-28 15:07:41,849 main.py:51] epoch 3692, training loss: 10654.58, average training loss: 10134.09, base loss: 14460.63
[INFO 2017-06-28 15:07:42,449 main.py:51] epoch 3693, training loss: 10393.67, average training loss: 10133.22, base loss: 14460.14
[INFO 2017-06-28 15:07:43,292 main.py:51] epoch 3694, training loss: 9437.28, average training loss: 10132.54, base loss: 14460.45
[INFO 2017-06-28 15:07:43,983 main.py:51] epoch 3695, training loss: 9686.59, average training loss: 10131.99, base loss: 14459.35
[INFO 2017-06-28 15:07:44,705 main.py:51] epoch 3696, training loss: 10566.99, average training loss: 10132.21, base loss: 14459.59
[INFO 2017-06-28 15:07:45,534 main.py:51] epoch 3697, training loss: 9237.94, average training loss: 10131.18, base loss: 14457.95
[INFO 2017-06-28 15:07:46,334 main.py:51] epoch 3698, training loss: 10811.15, average training loss: 10132.28, base loss: 14460.44
[INFO 2017-06-28 15:07:47,001 main.py:51] epoch 3699, training loss: 9889.62, average training loss: 10131.72, base loss: 14459.73
[INFO 2017-06-28 15:07:47,001 main.py:53] epoch 3699, testing
[INFO 2017-06-28 15:07:49,864 main.py:105] average testing loss: 11832.53, base loss: 15550.87
[INFO 2017-06-28 15:07:49,864 main.py:106] improve_loss: 3718.34, improve_percent: 0.24
[INFO 2017-06-28 15:07:49,865 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 15:07:50,677 main.py:51] epoch 3700, training loss: 9529.34, average training loss: 10130.23, base loss: 14457.59
[INFO 2017-06-28 15:07:51,512 main.py:51] epoch 3701, training loss: 9204.84, average training loss: 10130.26, base loss: 14456.88
[INFO 2017-06-28 15:07:52,185 main.py:51] epoch 3702, training loss: 10348.65, average training loss: 10131.78, base loss: 14458.44
[INFO 2017-06-28 15:07:52,917 main.py:51] epoch 3703, training loss: 8743.83, average training loss: 10129.96, base loss: 14454.93
[INFO 2017-06-28 15:07:53,728 main.py:51] epoch 3704, training loss: 10294.22, average training loss: 10128.50, base loss: 14452.55
[INFO 2017-06-28 15:07:54,572 main.py:51] epoch 3705, training loss: 10626.28, average training loss: 10129.25, base loss: 14454.79
[INFO 2017-06-28 15:07:55,229 main.py:51] epoch 3706, training loss: 10842.36, average training loss: 10129.25, base loss: 14454.82
[INFO 2017-06-28 15:07:56,011 main.py:51] epoch 3707, training loss: 9529.10, average training loss: 10129.92, base loss: 14456.25
[INFO 2017-06-28 15:07:56,843 main.py:51] epoch 3708, training loss: 8426.68, average training loss: 10128.85, base loss: 14456.03
[INFO 2017-06-28 15:07:57,640 main.py:51] epoch 3709, training loss: 9821.43, average training loss: 10127.91, base loss: 14454.99
[INFO 2017-06-28 15:07:58,320 main.py:51] epoch 3710, training loss: 9354.17, average training loss: 10127.09, base loss: 14453.08
[INFO 2017-06-28 15:07:59,129 main.py:51] epoch 3711, training loss: 10186.01, average training loss: 10125.42, base loss: 14450.72
[INFO 2017-06-28 15:07:59,961 main.py:51] epoch 3712, training loss: 8832.81, average training loss: 10124.17, base loss: 14445.97
[INFO 2017-06-28 15:08:00,821 main.py:51] epoch 3713, training loss: 10169.68, average training loss: 10124.44, base loss: 14444.37
[INFO 2017-06-28 15:08:01,475 main.py:51] epoch 3714, training loss: 10027.52, average training loss: 10122.26, base loss: 14443.97
[INFO 2017-06-28 15:08:02,243 main.py:51] epoch 3715, training loss: 9875.56, average training loss: 10122.09, base loss: 14444.59
[INFO 2017-06-28 15:08:03,079 main.py:51] epoch 3716, training loss: 10326.87, average training loss: 10121.39, base loss: 14443.66
[INFO 2017-06-28 15:08:03,761 main.py:51] epoch 3717, training loss: 11140.01, average training loss: 10122.73, base loss: 14446.73
[INFO 2017-06-28 15:08:04,486 main.py:51] epoch 3718, training loss: 10048.68, average training loss: 10122.83, base loss: 14445.83
[INFO 2017-06-28 15:08:05,305 main.py:51] epoch 3719, training loss: 10003.78, average training loss: 10123.00, base loss: 14446.66
[INFO 2017-06-28 15:08:06,044 main.py:51] epoch 3720, training loss: 10802.54, average training loss: 10121.69, base loss: 14444.19
[INFO 2017-06-28 15:08:06,716 main.py:51] epoch 3721, training loss: 9175.90, average training loss: 10121.36, base loss: 14443.09
[INFO 2017-06-28 15:08:07,527 main.py:51] epoch 3722, training loss: 10003.46, average training loss: 10121.45, base loss: 14442.73
[INFO 2017-06-28 15:08:08,331 main.py:51] epoch 3723, training loss: 9868.29, average training loss: 10122.07, base loss: 14444.21
[INFO 2017-06-28 15:08:08,966 main.py:51] epoch 3724, training loss: 11587.12, average training loss: 10122.94, base loss: 14446.11
[INFO 2017-06-28 15:08:09,743 main.py:51] epoch 3725, training loss: 11021.32, average training loss: 10123.96, base loss: 14445.97
[INFO 2017-06-28 15:08:10,562 main.py:51] epoch 3726, training loss: 11360.69, average training loss: 10126.08, base loss: 14448.91
[INFO 2017-06-28 15:08:11,220 main.py:51] epoch 3727, training loss: 11044.82, average training loss: 10127.09, base loss: 14452.30
[INFO 2017-06-28 15:08:11,952 main.py:51] epoch 3728, training loss: 9559.79, average training loss: 10126.87, base loss: 14453.24
[INFO 2017-06-28 15:08:12,803 main.py:51] epoch 3729, training loss: 9353.69, average training loss: 10125.66, base loss: 14449.39
[INFO 2017-06-28 15:08:13,529 main.py:51] epoch 3730, training loss: 9936.49, average training loss: 10126.23, base loss: 14450.18
[INFO 2017-06-28 15:08:14,201 main.py:51] epoch 3731, training loss: 10865.16, average training loss: 10127.54, base loss: 14451.93
[INFO 2017-06-28 15:08:15,017 main.py:51] epoch 3732, training loss: 11303.77, average training loss: 10127.48, base loss: 14452.04
[INFO 2017-06-28 15:08:15,837 main.py:51] epoch 3733, training loss: 12145.82, average training loss: 10129.12, base loss: 14455.94
[INFO 2017-06-28 15:08:16,443 main.py:51] epoch 3734, training loss: 11152.09, average training loss: 10130.73, base loss: 14457.95
[INFO 2017-06-28 15:08:17,242 main.py:51] epoch 3735, training loss: 10290.97, average training loss: 10131.01, base loss: 14457.99
[INFO 2017-06-28 15:08:18,085 main.py:51] epoch 3736, training loss: 9024.86, average training loss: 10129.31, base loss: 14455.30
[INFO 2017-06-28 15:08:18,747 main.py:51] epoch 3737, training loss: 11102.68, average training loss: 10130.63, base loss: 14457.65
[INFO 2017-06-28 15:08:19,506 main.py:51] epoch 3738, training loss: 9701.72, average training loss: 10129.83, base loss: 14456.61
[INFO 2017-06-28 15:08:20,333 main.py:51] epoch 3739, training loss: 9186.41, average training loss: 10129.06, base loss: 14456.89
[INFO 2017-06-28 15:08:21,178 main.py:51] epoch 3740, training loss: 12007.52, average training loss: 10130.73, base loss: 14459.10
[INFO 2017-06-28 15:08:21,837 main.py:51] epoch 3741, training loss: 9371.68, average training loss: 10130.19, base loss: 14456.58
[INFO 2017-06-28 15:08:22,607 main.py:51] epoch 3742, training loss: 12440.42, average training loss: 10133.69, base loss: 14460.59
[INFO 2017-06-28 15:08:23,445 main.py:51] epoch 3743, training loss: 9117.57, average training loss: 10133.43, base loss: 14459.03
[INFO 2017-06-28 15:08:24,130 main.py:51] epoch 3744, training loss: 11374.24, average training loss: 10135.93, base loss: 14461.56
[INFO 2017-06-28 15:08:24,861 main.py:51] epoch 3745, training loss: 9123.19, average training loss: 10133.89, base loss: 14457.72
[INFO 2017-06-28 15:08:25,682 main.py:51] epoch 3746, training loss: 8946.25, average training loss: 10133.56, base loss: 14458.08
[INFO 2017-06-28 15:08:26,450 main.py:51] epoch 3747, training loss: 9015.98, average training loss: 10133.22, base loss: 14458.12
[INFO 2017-06-28 15:08:27,141 main.py:51] epoch 3748, training loss: 11960.59, average training loss: 10136.04, base loss: 14462.96
[INFO 2017-06-28 15:08:27,931 main.py:51] epoch 3749, training loss: 9825.01, average training loss: 10136.63, base loss: 14463.18
[INFO 2017-06-28 15:08:28,738 main.py:51] epoch 3750, training loss: 9100.29, average training loss: 10136.65, base loss: 14464.49
[INFO 2017-06-28 15:08:29,331 main.py:51] epoch 3751, training loss: 9365.42, average training loss: 10134.85, base loss: 14463.68
[INFO 2017-06-28 15:08:30,090 main.py:51] epoch 3752, training loss: 8639.03, average training loss: 10133.18, base loss: 14461.70
[INFO 2017-06-28 15:08:30,899 main.py:51] epoch 3753, training loss: 10382.66, average training loss: 10133.55, base loss: 14463.11
[INFO 2017-06-28 15:08:31,561 main.py:51] epoch 3754, training loss: 8902.41, average training loss: 10133.32, base loss: 14464.35
[INFO 2017-06-28 15:08:32,342 main.py:51] epoch 3755, training loss: 11058.39, average training loss: 10134.21, base loss: 14465.78
[INFO 2017-06-28 15:08:33,179 main.py:51] epoch 3756, training loss: 8853.40, average training loss: 10133.37, base loss: 14464.84
[INFO 2017-06-28 15:08:34,011 main.py:51] epoch 3757, training loss: 10504.59, average training loss: 10133.14, base loss: 14466.67
[INFO 2017-06-28 15:08:34,618 main.py:51] epoch 3758, training loss: 8860.83, average training loss: 10133.35, base loss: 14467.17
[INFO 2017-06-28 15:08:35,384 main.py:51] epoch 3759, training loss: 11950.99, average training loss: 10134.76, base loss: 14469.09
[INFO 2017-06-28 15:08:36,163 main.py:51] epoch 3760, training loss: 10195.23, average training loss: 10135.88, base loss: 14469.78
[INFO 2017-06-28 15:08:36,828 main.py:51] epoch 3761, training loss: 9903.95, average training loss: 10135.64, base loss: 14469.79
[INFO 2017-06-28 15:08:37,591 main.py:51] epoch 3762, training loss: 10836.27, average training loss: 10137.02, base loss: 14471.71
[INFO 2017-06-28 15:08:38,376 main.py:51] epoch 3763, training loss: 8681.25, average training loss: 10135.33, base loss: 14469.52
[INFO 2017-06-28 15:08:38,988 main.py:51] epoch 3764, training loss: 10672.94, average training loss: 10135.02, base loss: 14471.53
[INFO 2017-06-28 15:08:39,811 main.py:51] epoch 3765, training loss: 9073.91, average training loss: 10134.50, base loss: 14470.01
[INFO 2017-06-28 15:08:40,644 main.py:51] epoch 3766, training loss: 10433.71, average training loss: 10135.49, base loss: 14471.66
[INFO 2017-06-28 15:08:41,514 main.py:51] epoch 3767, training loss: 10491.42, average training loss: 10135.98, base loss: 14471.94
[INFO 2017-06-28 15:08:42,129 main.py:51] epoch 3768, training loss: 11203.58, average training loss: 10136.78, base loss: 14475.22
[INFO 2017-06-28 15:08:42,884 main.py:51] epoch 3769, training loss: 10939.98, average training loss: 10137.05, base loss: 14476.04
[INFO 2017-06-28 15:08:43,698 main.py:51] epoch 3770, training loss: 9267.49, average training loss: 10135.77, base loss: 14474.87
[INFO 2017-06-28 15:08:44,384 main.py:51] epoch 3771, training loss: 9694.56, average training loss: 10135.51, base loss: 14474.25
[INFO 2017-06-28 15:08:45,068 main.py:51] epoch 3772, training loss: 10526.88, average training loss: 10135.95, base loss: 14475.53
[INFO 2017-06-28 15:08:45,903 main.py:51] epoch 3773, training loss: 10913.24, average training loss: 10136.84, base loss: 14478.44
[INFO 2017-06-28 15:08:46,567 main.py:51] epoch 3774, training loss: 9393.19, average training loss: 10135.16, base loss: 14476.36
[INFO 2017-06-28 15:08:47,348 main.py:51] epoch 3775, training loss: 9601.49, average training loss: 10135.47, base loss: 14476.37
[INFO 2017-06-28 15:08:48,152 main.py:51] epoch 3776, training loss: 9888.65, average training loss: 10134.39, base loss: 14475.21
[INFO 2017-06-28 15:08:48,968 main.py:51] epoch 3777, training loss: 12059.36, average training loss: 10136.60, base loss: 14477.94
[INFO 2017-06-28 15:08:49,588 main.py:51] epoch 3778, training loss: 8950.33, average training loss: 10136.11, base loss: 14477.89
[INFO 2017-06-28 15:08:50,378 main.py:51] epoch 3779, training loss: 9332.78, average training loss: 10135.88, base loss: 14477.89
[INFO 2017-06-28 15:08:51,216 main.py:51] epoch 3780, training loss: 9735.28, average training loss: 10136.95, base loss: 14479.64
[INFO 2017-06-28 15:08:51,980 main.py:51] epoch 3781, training loss: 9261.85, average training loss: 10136.31, base loss: 14477.73
[INFO 2017-06-28 15:08:52,650 main.py:51] epoch 3782, training loss: 11094.23, average training loss: 10137.85, base loss: 14480.48
[INFO 2017-06-28 15:08:53,447 main.py:51] epoch 3783, training loss: 9224.66, average training loss: 10136.11, base loss: 14479.09
[INFO 2017-06-28 15:08:54,220 main.py:51] epoch 3784, training loss: 10489.37, average training loss: 10137.21, base loss: 14479.85
[INFO 2017-06-28 15:08:54,889 main.py:51] epoch 3785, training loss: 10562.52, average training loss: 10136.31, base loss: 14481.25
[INFO 2017-06-28 15:08:55,686 main.py:51] epoch 3786, training loss: 10230.14, average training loss: 10137.17, base loss: 14483.89
[INFO 2017-06-28 15:08:56,477 main.py:51] epoch 3787, training loss: 12068.64, average training loss: 10140.61, base loss: 14488.00
[INFO 2017-06-28 15:08:57,168 main.py:51] epoch 3788, training loss: 10060.87, average training loss: 10141.56, base loss: 14487.95
[INFO 2017-06-28 15:08:57,884 main.py:51] epoch 3789, training loss: 9265.71, average training loss: 10140.41, base loss: 14487.00
[INFO 2017-06-28 15:08:58,709 main.py:51] epoch 3790, training loss: 11036.74, average training loss: 10140.80, base loss: 14488.34
[INFO 2017-06-28 15:08:59,448 main.py:51] epoch 3791, training loss: 10683.31, average training loss: 10141.33, base loss: 14489.10
[INFO 2017-06-28 15:09:00,127 main.py:51] epoch 3792, training loss: 11190.06, average training loss: 10143.08, base loss: 14492.05
[INFO 2017-06-28 15:09:00,949 main.py:51] epoch 3793, training loss: 11019.08, average training loss: 10142.79, base loss: 14491.20
[INFO 2017-06-28 15:09:01,740 main.py:51] epoch 3794, training loss: 9043.65, average training loss: 10142.19, base loss: 14489.71
[INFO 2017-06-28 15:09:02,395 main.py:51] epoch 3795, training loss: 11569.66, average training loss: 10144.89, base loss: 14492.42
[INFO 2017-06-28 15:09:03,168 main.py:51] epoch 3796, training loss: 9210.19, average training loss: 10144.73, base loss: 14491.71
[INFO 2017-06-28 15:09:03,996 main.py:51] epoch 3797, training loss: 10354.38, average training loss: 10145.56, base loss: 14491.86
[INFO 2017-06-28 15:09:04,624 main.py:51] epoch 3798, training loss: 10647.93, average training loss: 10146.34, base loss: 14491.99
[INFO 2017-06-28 15:09:05,397 main.py:51] epoch 3799, training loss: 9408.00, average training loss: 10146.23, base loss: 14491.97
[INFO 2017-06-28 15:09:05,397 main.py:53] epoch 3799, testing
[INFO 2017-06-28 15:09:08,298 main.py:105] average testing loss: 10749.53, base loss: 15040.82
[INFO 2017-06-28 15:09:08,298 main.py:106] improve_loss: 4291.30, improve_percent: 0.29
[INFO 2017-06-28 15:09:08,299 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 15:09:08,312 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:09:09,096 main.py:51] epoch 3800, training loss: 10111.98, average training loss: 10143.81, base loss: 14489.66
[INFO 2017-06-28 15:09:09,872 main.py:51] epoch 3801, training loss: 9529.68, average training loss: 10143.83, base loss: 14489.98
[INFO 2017-06-28 15:09:10,488 main.py:51] epoch 3802, training loss: 10275.90, average training loss: 10144.25, base loss: 14492.20
[INFO 2017-06-28 15:09:11,284 main.py:51] epoch 3803, training loss: 9927.53, average training loss: 10144.37, base loss: 14491.48
[INFO 2017-06-28 15:09:12,110 main.py:51] epoch 3804, training loss: 9135.21, average training loss: 10144.01, base loss: 14491.67
[INFO 2017-06-28 15:09:12,894 main.py:51] epoch 3805, training loss: 9938.91, average training loss: 10143.39, base loss: 14490.79
[INFO 2017-06-28 15:09:13,542 main.py:51] epoch 3806, training loss: 9342.51, average training loss: 10143.51, base loss: 14492.15
[INFO 2017-06-28 15:09:14,314 main.py:51] epoch 3807, training loss: 9056.70, average training loss: 10143.08, base loss: 14491.03
[INFO 2017-06-28 15:09:15,111 main.py:51] epoch 3808, training loss: 9398.50, average training loss: 10141.93, base loss: 14489.71
[INFO 2017-06-28 15:09:15,783 main.py:51] epoch 3809, training loss: 8641.53, average training loss: 10140.11, base loss: 14487.78
[INFO 2017-06-28 15:09:16,548 main.py:51] epoch 3810, training loss: 8805.59, average training loss: 10139.83, base loss: 14487.28
[INFO 2017-06-28 15:09:17,356 main.py:51] epoch 3811, training loss: 9689.21, average training loss: 10140.36, base loss: 14488.76
[INFO 2017-06-28 15:09:18,205 main.py:51] epoch 3812, training loss: 10786.40, average training loss: 10141.59, base loss: 14489.60
[INFO 2017-06-28 15:09:18,840 main.py:51] epoch 3813, training loss: 10031.96, average training loss: 10140.06, base loss: 14487.83
[INFO 2017-06-28 15:09:19,577 main.py:51] epoch 3814, training loss: 9547.33, average training loss: 10138.92, base loss: 14487.83
[INFO 2017-06-28 15:09:20,420 main.py:51] epoch 3815, training loss: 9906.61, average training loss: 10136.80, base loss: 14485.15
[INFO 2017-06-28 15:09:21,103 main.py:51] epoch 3816, training loss: 10028.43, average training loss: 10136.45, base loss: 14485.12
[INFO 2017-06-28 15:09:21,847 main.py:51] epoch 3817, training loss: 10263.58, average training loss: 10137.22, base loss: 14486.82
[INFO 2017-06-28 15:09:22,667 main.py:51] epoch 3818, training loss: 9229.38, average training loss: 10135.96, base loss: 14484.53
[INFO 2017-06-28 15:09:23,477 main.py:51] epoch 3819, training loss: 9543.53, average training loss: 10135.40, base loss: 14483.24
[INFO 2017-06-28 15:09:24,103 main.py:51] epoch 3820, training loss: 10767.67, average training loss: 10134.49, base loss: 14482.55
[INFO 2017-06-28 15:09:24,875 main.py:51] epoch 3821, training loss: 9873.35, average training loss: 10134.83, base loss: 14482.78
[INFO 2017-06-28 15:09:25,726 main.py:51] epoch 3822, training loss: 11030.47, average training loss: 10136.01, base loss: 14484.91
[INFO 2017-06-28 15:09:26,361 main.py:51] epoch 3823, training loss: 11921.71, average training loss: 10137.83, base loss: 14484.70
[INFO 2017-06-28 15:09:27,157 main.py:51] epoch 3824, training loss: 9909.29, average training loss: 10136.38, base loss: 14482.82
[INFO 2017-06-28 15:09:27,969 main.py:51] epoch 3825, training loss: 8738.29, average training loss: 10133.66, base loss: 14476.84
[INFO 2017-06-28 15:09:28,651 main.py:51] epoch 3826, training loss: 10730.07, average training loss: 10133.56, base loss: 14476.58
[INFO 2017-06-28 15:09:29,327 main.py:51] epoch 3827, training loss: 9489.70, average training loss: 10132.41, base loss: 14474.75
[INFO 2017-06-28 15:09:29,988 main.py:51] epoch 3828, training loss: 8896.01, average training loss: 10131.39, base loss: 14473.43
[INFO 2017-06-28 15:09:30,586 main.py:51] epoch 3829, training loss: 10226.94, average training loss: 10130.21, base loss: 14471.88
[INFO 2017-06-28 15:09:31,224 main.py:51] epoch 3830, training loss: 9820.60, average training loss: 10129.80, base loss: 14469.74
[INFO 2017-06-28 15:09:31,900 main.py:51] epoch 3831, training loss: 8900.36, average training loss: 10129.02, base loss: 14468.74
[INFO 2017-06-28 15:09:32,715 main.py:51] epoch 3832, training loss: 10190.58, average training loss: 10128.79, base loss: 14469.63
[INFO 2017-06-28 15:09:33,490 main.py:51] epoch 3833, training loss: 9451.75, average training loss: 10127.51, base loss: 14466.44
[INFO 2017-06-28 15:09:34,164 main.py:51] epoch 3834, training loss: 10228.49, average training loss: 10128.68, base loss: 14467.98
[INFO 2017-06-28 15:09:34,995 main.py:51] epoch 3835, training loss: 10669.33, average training loss: 10128.43, base loss: 14466.89
[INFO 2017-06-28 15:09:35,822 main.py:51] epoch 3836, training loss: 8877.42, average training loss: 10127.52, base loss: 14465.24
[INFO 2017-06-28 15:09:36,639 main.py:51] epoch 3837, training loss: 8809.62, average training loss: 10125.90, base loss: 14464.15
[INFO 2017-06-28 15:09:37,276 main.py:51] epoch 3838, training loss: 11407.74, average training loss: 10127.09, base loss: 14464.41
[INFO 2017-06-28 15:09:38,063 main.py:51] epoch 3839, training loss: 10730.42, average training loss: 10128.40, base loss: 14467.47
[INFO 2017-06-28 15:09:38,848 main.py:51] epoch 3840, training loss: 9269.91, average training loss: 10127.48, base loss: 14465.84
[INFO 2017-06-28 15:09:39,490 main.py:51] epoch 3841, training loss: 9865.30, average training loss: 10127.43, base loss: 14464.60
[INFO 2017-06-28 15:09:40,268 main.py:51] epoch 3842, training loss: 10379.68, average training loss: 10127.53, base loss: 14463.32
[INFO 2017-06-28 15:09:41,091 main.py:51] epoch 3843, training loss: 9692.86, average training loss: 10127.96, base loss: 14464.27
[INFO 2017-06-28 15:09:41,934 main.py:51] epoch 3844, training loss: 9536.64, average training loss: 10128.15, base loss: 14463.55
[INFO 2017-06-28 15:09:42,596 main.py:51] epoch 3845, training loss: 13262.20, average training loss: 10132.13, base loss: 14469.05
[INFO 2017-06-28 15:09:43,334 main.py:51] epoch 3846, training loss: 9504.84, average training loss: 10132.11, base loss: 14469.53
[INFO 2017-06-28 15:09:44,179 main.py:51] epoch 3847, training loss: 10197.42, average training loss: 10132.83, base loss: 14469.20
[INFO 2017-06-28 15:09:44,858 main.py:51] epoch 3848, training loss: 10480.73, average training loss: 10133.14, base loss: 14470.47
[INFO 2017-06-28 15:09:45,599 main.py:51] epoch 3849, training loss: 11246.51, average training loss: 10131.49, base loss: 14468.80
[INFO 2017-06-28 15:09:46,400 main.py:51] epoch 3850, training loss: 11861.07, average training loss: 10134.21, base loss: 14472.30
[INFO 2017-06-28 15:09:47,238 main.py:51] epoch 3851, training loss: 8898.65, average training loss: 10133.63, base loss: 14471.80
[INFO 2017-06-28 15:09:47,861 main.py:51] epoch 3852, training loss: 9221.83, average training loss: 10133.08, base loss: 14470.61
[INFO 2017-06-28 15:09:48,639 main.py:51] epoch 3853, training loss: 10505.11, average training loss: 10134.85, base loss: 14472.74
[INFO 2017-06-28 15:09:49,486 main.py:51] epoch 3854, training loss: 10828.70, average training loss: 10136.69, base loss: 14475.15
[INFO 2017-06-28 15:09:50,177 main.py:51] epoch 3855, training loss: 10780.12, average training loss: 10137.27, base loss: 14475.86
[INFO 2017-06-28 15:09:50,912 main.py:51] epoch 3856, training loss: 9893.87, average training loss: 10135.80, base loss: 14475.97
[INFO 2017-06-28 15:09:51,728 main.py:51] epoch 3857, training loss: 10162.36, average training loss: 10134.91, base loss: 14472.76
[INFO 2017-06-28 15:09:52,556 main.py:51] epoch 3858, training loss: 9573.29, average training loss: 10134.21, base loss: 14471.53
[INFO 2017-06-28 15:09:53,194 main.py:51] epoch 3859, training loss: 10291.02, average training loss: 10133.56, base loss: 14470.77
[INFO 2017-06-28 15:09:53,963 main.py:51] epoch 3860, training loss: 10875.71, average training loss: 10135.05, base loss: 14473.37
[INFO 2017-06-28 15:09:54,821 main.py:51] epoch 3861, training loss: 10762.65, average training loss: 10135.94, base loss: 14472.78
[INFO 2017-06-28 15:09:55,522 main.py:51] epoch 3862, training loss: 11008.14, average training loss: 10137.83, base loss: 14474.51
[INFO 2017-06-28 15:09:56,212 main.py:51] epoch 3863, training loss: 8503.15, average training loss: 10136.72, base loss: 14473.18
[INFO 2017-06-28 15:09:57,032 main.py:51] epoch 3864, training loss: 12338.84, average training loss: 10138.95, base loss: 14476.28
[INFO 2017-06-28 15:09:57,777 main.py:51] epoch 3865, training loss: 11197.28, average training loss: 10140.05, base loss: 14475.88
[INFO 2017-06-28 15:09:58,496 main.py:51] epoch 3866, training loss: 12481.81, average training loss: 10143.18, base loss: 14480.41
[INFO 2017-06-28 15:09:59,310 main.py:51] epoch 3867, training loss: 10422.79, average training loss: 10142.58, base loss: 14477.67
[INFO 2017-06-28 15:10:00,188 main.py:51] epoch 3868, training loss: 9768.41, average training loss: 10141.80, base loss: 14476.07
[INFO 2017-06-28 15:10:00,964 main.py:51] epoch 3869, training loss: 10787.91, average training loss: 10142.21, base loss: 14476.17
[INFO 2017-06-28 15:10:01,644 main.py:51] epoch 3870, training loss: 9671.17, average training loss: 10141.00, base loss: 14474.50
[INFO 2017-06-28 15:10:02,444 main.py:51] epoch 3871, training loss: 10533.73, average training loss: 10138.63, base loss: 14470.59
[INFO 2017-06-28 15:10:03,265 main.py:51] epoch 3872, training loss: 10054.26, average training loss: 10138.77, base loss: 14473.06
[INFO 2017-06-28 15:10:03,968 main.py:51] epoch 3873, training loss: 9422.41, average training loss: 10135.78, base loss: 14470.05
[INFO 2017-06-28 15:10:04,635 main.py:51] epoch 3874, training loss: 9828.94, average training loss: 10136.13, base loss: 14471.22
[INFO 2017-06-28 15:10:05,494 main.py:51] epoch 3875, training loss: 12111.60, average training loss: 10138.47, base loss: 14475.47
[INFO 2017-06-28 15:10:06,183 main.py:51] epoch 3876, training loss: 9746.14, average training loss: 10138.34, base loss: 14475.60
[INFO 2017-06-28 15:10:06,927 main.py:51] epoch 3877, training loss: 10321.11, average training loss: 10138.67, base loss: 14475.14
[INFO 2017-06-28 15:10:07,744 main.py:51] epoch 3878, training loss: 9032.96, average training loss: 10137.80, base loss: 14471.97
[INFO 2017-06-28 15:10:08,584 main.py:51] epoch 3879, training loss: 9558.85, average training loss: 10136.11, base loss: 14468.84
[INFO 2017-06-28 15:10:09,203 main.py:51] epoch 3880, training loss: 12862.97, average training loss: 10139.32, base loss: 14471.95
[INFO 2017-06-28 15:10:09,997 main.py:51] epoch 3881, training loss: 11251.91, average training loss: 10140.71, base loss: 14472.16
[INFO 2017-06-28 15:10:10,848 main.py:51] epoch 3882, training loss: 8923.03, average training loss: 10140.29, base loss: 14471.41
[INFO 2017-06-28 15:10:11,587 main.py:51] epoch 3883, training loss: 12193.47, average training loss: 10141.89, base loss: 14473.20
[INFO 2017-06-28 15:10:12,301 main.py:51] epoch 3884, training loss: 10031.95, average training loss: 10142.36, base loss: 14473.91
[INFO 2017-06-28 15:10:13,097 main.py:51] epoch 3885, training loss: 10288.87, average training loss: 10140.82, base loss: 14471.22
[INFO 2017-06-28 15:10:13,894 main.py:51] epoch 3886, training loss: 10372.07, average training loss: 10140.74, base loss: 14472.07
[INFO 2017-06-28 15:10:14,505 main.py:51] epoch 3887, training loss: 9535.69, average training loss: 10139.36, base loss: 14469.93
[INFO 2017-06-28 15:10:15,266 main.py:51] epoch 3888, training loss: 9029.58, average training loss: 10140.34, base loss: 14470.28
[INFO 2017-06-28 15:10:16,072 main.py:51] epoch 3889, training loss: 9063.50, average training loss: 10139.26, base loss: 14469.51
[INFO 2017-06-28 15:10:16,769 main.py:51] epoch 3890, training loss: 9879.45, average training loss: 10138.79, base loss: 14465.80
[INFO 2017-06-28 15:10:17,496 main.py:51] epoch 3891, training loss: 10297.80, average training loss: 10140.26, base loss: 14466.65
[INFO 2017-06-28 15:10:18,285 main.py:51] epoch 3892, training loss: 11251.91, average training loss: 10138.01, base loss: 14466.05
[INFO 2017-06-28 15:10:19,052 main.py:51] epoch 3893, training loss: 9775.15, average training loss: 10138.97, base loss: 14468.77
[INFO 2017-06-28 15:10:19,714 main.py:51] epoch 3894, training loss: 10523.29, average training loss: 10138.83, base loss: 14469.68
[INFO 2017-06-28 15:10:20,472 main.py:51] epoch 3895, training loss: 10836.99, average training loss: 10139.81, base loss: 14472.66
[INFO 2017-06-28 15:10:21,294 main.py:51] epoch 3896, training loss: 8708.63, average training loss: 10137.72, base loss: 14470.25
[INFO 2017-06-28 15:10:21,924 main.py:51] epoch 3897, training loss: 8367.91, average training loss: 10136.77, base loss: 14468.43
[INFO 2017-06-28 15:10:22,686 main.py:51] epoch 3898, training loss: 8756.32, average training loss: 10134.65, base loss: 14466.01
[INFO 2017-06-28 15:10:23,520 main.py:51] epoch 3899, training loss: 9828.76, average training loss: 10134.58, base loss: 14466.27
[INFO 2017-06-28 15:10:23,520 main.py:53] epoch 3899, testing
[INFO 2017-06-28 15:10:26,312 main.py:105] average testing loss: 11550.80, base loss: 15535.62
[INFO 2017-06-28 15:10:26,312 main.py:106] improve_loss: 3984.82, improve_percent: 0.26
[INFO 2017-06-28 15:10:26,313 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:10:26,948 main.py:51] epoch 3900, training loss: 10176.06, average training loss: 10135.46, base loss: 14468.51
[INFO 2017-06-28 15:10:27,757 main.py:51] epoch 3901, training loss: 10179.47, average training loss: 10133.83, base loss: 14465.47
[INFO 2017-06-28 15:10:28,555 main.py:51] epoch 3902, training loss: 9145.46, average training loss: 10134.41, base loss: 14466.14
[INFO 2017-06-28 15:10:29,277 main.py:51] epoch 3903, training loss: 10811.54, average training loss: 10133.80, base loss: 14468.27
[INFO 2017-06-28 15:10:29,970 main.py:51] epoch 3904, training loss: 8121.92, average training loss: 10132.12, base loss: 14464.90
[INFO 2017-06-28 15:10:30,791 main.py:51] epoch 3905, training loss: 10534.32, average training loss: 10132.12, base loss: 14462.50
[INFO 2017-06-28 15:10:31,624 main.py:51] epoch 3906, training loss: 9658.72, average training loss: 10131.08, base loss: 14462.19
[INFO 2017-06-28 15:10:32,264 main.py:51] epoch 3907, training loss: 10249.43, average training loss: 10131.90, base loss: 14463.39
[INFO 2017-06-28 15:10:32,999 main.py:51] epoch 3908, training loss: 10420.24, average training loss: 10132.32, base loss: 14465.18
[INFO 2017-06-28 15:10:33,857 main.py:51] epoch 3909, training loss: 11223.28, average training loss: 10133.58, base loss: 14466.74
[INFO 2017-06-28 15:10:34,541 main.py:51] epoch 3910, training loss: 10451.46, average training loss: 10133.81, base loss: 14465.44
[INFO 2017-06-28 15:10:35,282 main.py:51] epoch 3911, training loss: 10983.42, average training loss: 10134.73, base loss: 14467.16
[INFO 2017-06-28 15:10:36,098 main.py:51] epoch 3912, training loss: 9017.37, average training loss: 10133.54, base loss: 14465.10
[INFO 2017-06-28 15:10:36,878 main.py:51] epoch 3913, training loss: 11291.19, average training loss: 10134.93, base loss: 14467.35
[INFO 2017-06-28 15:10:37,569 main.py:51] epoch 3914, training loss: 10689.02, average training loss: 10134.57, base loss: 14467.60
[INFO 2017-06-28 15:10:38,350 main.py:51] epoch 3915, training loss: 11674.53, average training loss: 10136.61, base loss: 14470.12
[INFO 2017-06-28 15:10:39,180 main.py:51] epoch 3916, training loss: 9517.79, average training loss: 10135.50, base loss: 14468.18
[INFO 2017-06-28 15:10:39,821 main.py:51] epoch 3917, training loss: 10177.59, average training loss: 10135.00, base loss: 14467.66
[INFO 2017-06-28 15:10:40,631 main.py:51] epoch 3918, training loss: 9125.58, average training loss: 10134.43, base loss: 14465.53
[INFO 2017-06-28 15:10:41,452 main.py:51] epoch 3919, training loss: 10326.51, average training loss: 10134.66, base loss: 14466.04
[INFO 2017-06-28 15:10:42,220 main.py:51] epoch 3920, training loss: 10414.45, average training loss: 10134.15, base loss: 14465.74
[INFO 2017-06-28 15:10:42,880 main.py:51] epoch 3921, training loss: 8990.52, average training loss: 10133.80, base loss: 14466.77
[INFO 2017-06-28 15:10:43,685 main.py:51] epoch 3922, training loss: 10119.75, average training loss: 10131.98, base loss: 14465.25
[INFO 2017-06-28 15:10:44,450 main.py:51] epoch 3923, training loss: 9859.46, average training loss: 10132.83, base loss: 14466.55
[INFO 2017-06-28 15:10:45,127 main.py:51] epoch 3924, training loss: 10254.40, average training loss: 10132.34, base loss: 14464.04
[INFO 2017-06-28 15:10:45,949 main.py:51] epoch 3925, training loss: 9885.30, average training loss: 10131.64, base loss: 14464.75
[INFO 2017-06-28 15:10:46,764 main.py:51] epoch 3926, training loss: 9416.31, average training loss: 10130.24, base loss: 14462.31
[INFO 2017-06-28 15:10:47,403 main.py:51] epoch 3927, training loss: 12503.47, average training loss: 10132.85, base loss: 14464.13
[INFO 2017-06-28 15:10:48,159 main.py:51] epoch 3928, training loss: 10452.51, average training loss: 10134.09, base loss: 14465.83
[INFO 2017-06-28 15:10:48,953 main.py:51] epoch 3929, training loss: 12305.72, average training loss: 10136.28, base loss: 14466.85
[INFO 2017-06-28 15:10:49,641 main.py:51] epoch 3930, training loss: 12079.41, average training loss: 10137.44, base loss: 14467.52
[INFO 2017-06-28 15:10:50,400 main.py:51] epoch 3931, training loss: 8643.17, average training loss: 10135.85, base loss: 14463.24
[INFO 2017-06-28 15:10:51,227 main.py:51] epoch 3932, training loss: 9346.63, average training loss: 10132.82, base loss: 14459.38
[INFO 2017-06-28 15:10:51,956 main.py:51] epoch 3933, training loss: 9294.55, average training loss: 10132.60, base loss: 14459.11
[INFO 2017-06-28 15:10:52,635 main.py:51] epoch 3934, training loss: 10451.55, average training loss: 10133.16, base loss: 14461.81
[INFO 2017-06-28 15:10:53,428 main.py:51] epoch 3935, training loss: 10300.14, average training loss: 10132.77, base loss: 14461.46
[INFO 2017-06-28 15:10:54,225 main.py:51] epoch 3936, training loss: 9984.62, average training loss: 10131.82, base loss: 14461.12
[INFO 2017-06-28 15:10:54,873 main.py:51] epoch 3937, training loss: 10643.53, average training loss: 10132.82, base loss: 14461.00
[INFO 2017-06-28 15:10:55,596 main.py:51] epoch 3938, training loss: 10737.75, average training loss: 10131.61, base loss: 14460.68
[INFO 2017-06-28 15:10:56,384 main.py:51] epoch 3939, training loss: 9119.98, average training loss: 10130.44, base loss: 14459.64
[INFO 2017-06-28 15:10:57,006 main.py:51] epoch 3940, training loss: 10685.36, average training loss: 10129.84, base loss: 14458.84
[INFO 2017-06-28 15:10:57,765 main.py:51] epoch 3941, training loss: 12414.76, average training loss: 10131.39, base loss: 14460.65
[INFO 2017-06-28 15:10:58,559 main.py:51] epoch 3942, training loss: 11140.77, average training loss: 10132.57, base loss: 14461.77
[INFO 2017-06-28 15:10:59,175 main.py:51] epoch 3943, training loss: 10641.37, average training loss: 10132.54, base loss: 14461.75
[INFO 2017-06-28 15:10:59,973 main.py:51] epoch 3944, training loss: 9588.78, average training loss: 10132.13, base loss: 14461.96
[INFO 2017-06-28 15:11:00,790 main.py:51] epoch 3945, training loss: 9234.23, average training loss: 10130.13, base loss: 14459.56
[INFO 2017-06-28 15:11:01,595 main.py:51] epoch 3946, training loss: 10862.42, average training loss: 10131.60, base loss: 14459.63
[INFO 2017-06-28 15:11:02,271 main.py:51] epoch 3947, training loss: 9554.43, average training loss: 10130.99, base loss: 14458.79
[INFO 2017-06-28 15:11:03,040 main.py:51] epoch 3948, training loss: 8803.06, average training loss: 10130.75, base loss: 14459.40
[INFO 2017-06-28 15:11:03,848 main.py:51] epoch 3949, training loss: 9190.43, average training loss: 10130.73, base loss: 14459.68
[INFO 2017-06-28 15:11:04,540 main.py:51] epoch 3950, training loss: 10318.52, average training loss: 10129.52, base loss: 14457.24
[INFO 2017-06-28 15:11:05,300 main.py:51] epoch 3951, training loss: 10024.62, average training loss: 10128.41, base loss: 14456.54
[INFO 2017-06-28 15:11:06,120 main.py:51] epoch 3952, training loss: 9459.47, average training loss: 10127.41, base loss: 14454.72
[INFO 2017-06-28 15:11:06,975 main.py:51] epoch 3953, training loss: 10942.31, average training loss: 10129.12, base loss: 14456.95
[INFO 2017-06-28 15:11:07,601 main.py:51] epoch 3954, training loss: 8957.68, average training loss: 10127.57, base loss: 14453.82
[INFO 2017-06-28 15:11:08,379 main.py:51] epoch 3955, training loss: 9367.05, average training loss: 10126.21, base loss: 14452.56
[INFO 2017-06-28 15:11:09,208 main.py:51] epoch 3956, training loss: 8505.59, average training loss: 10125.15, base loss: 14449.62
[INFO 2017-06-28 15:11:09,889 main.py:51] epoch 3957, training loss: 10740.16, average training loss: 10126.64, base loss: 14451.07
[INFO 2017-06-28 15:11:10,620 main.py:51] epoch 3958, training loss: 9711.85, average training loss: 10126.20, base loss: 14450.13
[INFO 2017-06-28 15:11:11,422 main.py:51] epoch 3959, training loss: 8570.16, average training loss: 10123.23, base loss: 14447.71
[INFO 2017-06-28 15:11:12,259 main.py:51] epoch 3960, training loss: 9530.41, average training loss: 10123.41, base loss: 14449.63
[INFO 2017-06-28 15:11:12,924 main.py:51] epoch 3961, training loss: 9324.51, average training loss: 10121.43, base loss: 14448.49
[INFO 2017-06-28 15:11:13,699 main.py:51] epoch 3962, training loss: 10267.85, average training loss: 10120.42, base loss: 14446.45
[INFO 2017-06-28 15:11:14,521 main.py:51] epoch 3963, training loss: 10032.81, average training loss: 10119.17, base loss: 14445.90
[INFO 2017-06-28 15:11:15,368 main.py:51] epoch 3964, training loss: 11470.58, average training loss: 10120.80, base loss: 14449.00
[INFO 2017-06-28 15:11:16,017 main.py:51] epoch 3965, training loss: 8640.91, average training loss: 10117.46, base loss: 14444.96
[INFO 2017-06-28 15:11:16,817 main.py:51] epoch 3966, training loss: 11137.44, average training loss: 10119.47, base loss: 14447.14
[INFO 2017-06-28 15:11:17,645 main.py:51] epoch 3967, training loss: 8550.19, average training loss: 10117.36, base loss: 14444.88
[INFO 2017-06-28 15:11:18,277 main.py:51] epoch 3968, training loss: 8888.75, average training loss: 10114.73, base loss: 14441.80
[INFO 2017-06-28 15:11:18,989 main.py:51] epoch 3969, training loss: 10623.11, average training loss: 10116.65, base loss: 14445.86
[INFO 2017-06-28 15:11:19,656 main.py:51] epoch 3970, training loss: 9170.70, average training loss: 10117.06, base loss: 14447.14
[INFO 2017-06-28 15:11:20,317 main.py:51] epoch 3971, training loss: 9732.72, average training loss: 10117.46, base loss: 14447.71
[INFO 2017-06-28 15:11:20,917 main.py:51] epoch 3972, training loss: 10403.03, average training loss: 10118.05, base loss: 14447.15
[INFO 2017-06-28 15:11:21,576 main.py:51] epoch 3973, training loss: 9708.80, average training loss: 10117.60, base loss: 14445.94
[INFO 2017-06-28 15:11:22,374 main.py:51] epoch 3974, training loss: 10770.07, average training loss: 10119.19, base loss: 14451.41
[INFO 2017-06-28 15:11:23,163 main.py:51] epoch 3975, training loss: 10586.21, average training loss: 10120.73, base loss: 14452.76
[INFO 2017-06-28 15:11:23,828 main.py:51] epoch 3976, training loss: 10082.74, average training loss: 10121.36, base loss: 14452.32
[INFO 2017-06-28 15:11:24,580 main.py:51] epoch 3977, training loss: 11354.73, average training loss: 10121.96, base loss: 14453.55
[INFO 2017-06-28 15:11:25,414 main.py:51] epoch 3978, training loss: 11112.70, average training loss: 10123.53, base loss: 14458.23
[INFO 2017-06-28 15:11:26,262 main.py:51] epoch 3979, training loss: 9019.91, average training loss: 10121.39, base loss: 14454.91
[INFO 2017-06-28 15:11:26,877 main.py:51] epoch 3980, training loss: 9086.79, average training loss: 10121.25, base loss: 14454.77
[INFO 2017-06-28 15:11:27,650 main.py:51] epoch 3981, training loss: 8769.21, average training loss: 10121.00, base loss: 14455.46
[INFO 2017-06-28 15:11:28,454 main.py:51] epoch 3982, training loss: 9778.51, average training loss: 10121.89, base loss: 14456.61
[INFO 2017-06-28 15:11:29,103 main.py:51] epoch 3983, training loss: 11596.54, average training loss: 10123.83, base loss: 14458.24
[INFO 2017-06-28 15:11:29,818 main.py:51] epoch 3984, training loss: 9751.73, average training loss: 10124.55, base loss: 14460.08
[INFO 2017-06-28 15:11:30,586 main.py:51] epoch 3985, training loss: 9335.11, average training loss: 10125.23, base loss: 14460.71
[INFO 2017-06-28 15:11:31,215 main.py:51] epoch 3986, training loss: 9946.83, average training loss: 10124.43, base loss: 14460.20
[INFO 2017-06-28 15:11:32,005 main.py:51] epoch 3987, training loss: 10271.50, average training loss: 10123.62, base loss: 14460.71
[INFO 2017-06-28 15:11:32,816 main.py:51] epoch 3988, training loss: 10319.85, average training loss: 10125.12, base loss: 14463.60
[INFO 2017-06-28 15:11:33,608 main.py:51] epoch 3989, training loss: 10147.80, average training loss: 10126.19, base loss: 14466.12
[INFO 2017-06-28 15:11:34,306 main.py:51] epoch 3990, training loss: 9637.22, average training loss: 10125.45, base loss: 14464.34
[INFO 2017-06-28 15:11:35,051 main.py:51] epoch 3991, training loss: 9531.89, average training loss: 10123.77, base loss: 14462.99
[INFO 2017-06-28 15:11:35,875 main.py:51] epoch 3992, training loss: 11042.89, average training loss: 10124.55, base loss: 14465.18
[INFO 2017-06-28 15:11:36,570 main.py:51] epoch 3993, training loss: 10204.96, average training loss: 10125.42, base loss: 14467.27
[INFO 2017-06-28 15:11:37,307 main.py:51] epoch 3994, training loss: 10139.61, average training loss: 10124.18, base loss: 14466.70
[INFO 2017-06-28 15:11:38,119 main.py:51] epoch 3995, training loss: 9897.96, average training loss: 10123.58, base loss: 14464.75
[INFO 2017-06-28 15:11:38,917 main.py:51] epoch 3996, training loss: 9397.95, average training loss: 10121.07, base loss: 14461.20
[INFO 2017-06-28 15:11:39,591 main.py:51] epoch 3997, training loss: 10491.76, average training loss: 10121.86, base loss: 14461.03
[INFO 2017-06-28 15:11:40,333 main.py:51] epoch 3998, training loss: 10941.71, average training loss: 10122.85, base loss: 14461.52
[INFO 2017-06-28 15:11:41,176 main.py:51] epoch 3999, training loss: 11427.10, average training loss: 10124.76, base loss: 14465.25
[INFO 2017-06-28 15:11:41,177 main.py:53] epoch 3999, testing
[INFO 2017-06-28 15:11:44,035 main.py:105] average testing loss: 11171.88, base loss: 15381.81
[INFO 2017-06-28 15:11:44,035 main.py:106] improve_loss: 4209.92, improve_percent: 0.27
[INFO 2017-06-28 15:11:44,036 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:11:44,750 main.py:51] epoch 4000, training loss: 10166.70, average training loss: 10125.56, base loss: 14465.46
[INFO 2017-06-28 15:11:45,469 main.py:51] epoch 4001, training loss: 10048.33, average training loss: 10125.21, base loss: 14466.28
[INFO 2017-06-28 15:11:46,249 main.py:51] epoch 4002, training loss: 10846.32, average training loss: 10123.83, base loss: 14464.25
[INFO 2017-06-28 15:11:47,073 main.py:51] epoch 4003, training loss: 8771.45, average training loss: 10122.70, base loss: 14461.19
[INFO 2017-06-28 15:11:47,721 main.py:51] epoch 4004, training loss: 10673.69, average training loss: 10124.92, base loss: 14464.06
[INFO 2017-06-28 15:11:48,487 main.py:51] epoch 4005, training loss: 11031.62, average training loss: 10125.80, base loss: 14466.61
[INFO 2017-06-28 15:11:49,321 main.py:51] epoch 4006, training loss: 10279.30, average training loss: 10125.45, base loss: 14466.99
[INFO 2017-06-28 15:11:50,097 main.py:51] epoch 4007, training loss: 11103.46, average training loss: 10127.55, base loss: 14468.81
[INFO 2017-06-28 15:11:50,743 main.py:51] epoch 4008, training loss: 10188.34, average training loss: 10125.96, base loss: 14465.46
[INFO 2017-06-28 15:11:51,573 main.py:51] epoch 4009, training loss: 10364.35, average training loss: 10127.12, base loss: 14465.84
[INFO 2017-06-28 15:11:52,401 main.py:51] epoch 4010, training loss: 10274.01, average training loss: 10126.01, base loss: 14463.81
[INFO 2017-06-28 15:11:53,223 main.py:51] epoch 4011, training loss: 9842.79, average training loss: 10124.40, base loss: 14461.53
[INFO 2017-06-28 15:11:53,847 main.py:51] epoch 4012, training loss: 8470.61, average training loss: 10122.41, base loss: 14457.46
[INFO 2017-06-28 15:11:54,625 main.py:51] epoch 4013, training loss: 10202.46, average training loss: 10122.04, base loss: 14457.27
[INFO 2017-06-28 15:11:55,490 main.py:51] epoch 4014, training loss: 9268.45, average training loss: 10119.21, base loss: 14452.48
[INFO 2017-06-28 15:11:56,241 main.py:51] epoch 4015, training loss: 11533.67, average training loss: 10120.66, base loss: 14454.58
[INFO 2017-06-28 15:11:56,958 main.py:51] epoch 4016, training loss: 10855.22, average training loss: 10120.52, base loss: 14453.67
[INFO 2017-06-28 15:11:57,724 main.py:51] epoch 4017, training loss: 10265.19, average training loss: 10119.25, base loss: 14451.62
[INFO 2017-06-28 15:11:58,523 main.py:51] epoch 4018, training loss: 11425.45, average training loss: 10120.71, base loss: 14453.71
[INFO 2017-06-28 15:11:59,222 main.py:51] epoch 4019, training loss: 10645.63, average training loss: 10122.07, base loss: 14455.69
[INFO 2017-06-28 15:11:59,949 main.py:51] epoch 4020, training loss: 10111.74, average training loss: 10122.15, base loss: 14455.62
[INFO 2017-06-28 15:12:00,749 main.py:51] epoch 4021, training loss: 9935.96, average training loss: 10122.19, base loss: 14455.71
[INFO 2017-06-28 15:12:01,585 main.py:51] epoch 4022, training loss: 9510.82, average training loss: 10120.25, base loss: 14453.91
[INFO 2017-06-28 15:12:02,191 main.py:51] epoch 4023, training loss: 11404.85, average training loss: 10120.09, base loss: 14454.29
[INFO 2017-06-28 15:12:03,001 main.py:51] epoch 4024, training loss: 7805.67, average training loss: 10117.16, base loss: 14449.97
[INFO 2017-06-28 15:12:03,825 main.py:51] epoch 4025, training loss: 9794.54, average training loss: 10117.50, base loss: 14450.87
[INFO 2017-06-28 15:12:04,597 main.py:51] epoch 4026, training loss: 11579.36, average training loss: 10119.63, base loss: 14454.06
[INFO 2017-06-28 15:12:05,270 main.py:51] epoch 4027, training loss: 10188.78, average training loss: 10118.60, base loss: 14453.42
[INFO 2017-06-28 15:12:06,052 main.py:51] epoch 4028, training loss: 10032.12, average training loss: 10119.15, base loss: 14453.87
[INFO 2017-06-28 15:12:06,903 main.py:51] epoch 4029, training loss: 8676.34, average training loss: 10117.81, base loss: 14451.23
[INFO 2017-06-28 15:12:07,552 main.py:51] epoch 4030, training loss: 10418.12, average training loss: 10118.64, base loss: 14450.98
[INFO 2017-06-28 15:12:08,357 main.py:51] epoch 4031, training loss: 9790.04, average training loss: 10116.84, base loss: 14447.13
[INFO 2017-06-28 15:12:09,170 main.py:51] epoch 4032, training loss: 9264.20, average training loss: 10116.34, base loss: 14447.03
[INFO 2017-06-28 15:12:10,023 main.py:51] epoch 4033, training loss: 11024.01, average training loss: 10117.48, base loss: 14449.50
[INFO 2017-06-28 15:12:10,654 main.py:51] epoch 4034, training loss: 10112.87, average training loss: 10116.91, base loss: 14448.15
[INFO 2017-06-28 15:12:11,481 main.py:51] epoch 4035, training loss: 10162.52, average training loss: 10116.42, base loss: 14449.84
[INFO 2017-06-28 15:12:12,316 main.py:51] epoch 4036, training loss: 10885.33, average training loss: 10117.29, base loss: 14450.84
[INFO 2017-06-28 15:12:13,151 main.py:51] epoch 4037, training loss: 10632.48, average training loss: 10119.59, base loss: 14453.58
[INFO 2017-06-28 15:12:13,805 main.py:51] epoch 4038, training loss: 10190.72, average training loss: 10120.68, base loss: 14454.37
[INFO 2017-06-28 15:12:14,610 main.py:51] epoch 4039, training loss: 9578.71, average training loss: 10119.04, base loss: 14452.87
[INFO 2017-06-28 15:12:15,463 main.py:51] epoch 4040, training loss: 9665.79, average training loss: 10115.54, base loss: 14447.81
[INFO 2017-06-28 15:12:16,230 main.py:51] epoch 4041, training loss: 10721.67, average training loss: 10114.73, base loss: 14448.44
[INFO 2017-06-28 15:12:16,919 main.py:51] epoch 4042, training loss: 9311.12, average training loss: 10113.70, base loss: 14446.66
[INFO 2017-06-28 15:12:17,698 main.py:51] epoch 4043, training loss: 11011.78, average training loss: 10113.55, base loss: 14445.90
[INFO 2017-06-28 15:12:18,509 main.py:51] epoch 4044, training loss: 9009.57, average training loss: 10110.14, base loss: 14442.25
[INFO 2017-06-28 15:12:19,174 main.py:51] epoch 4045, training loss: 9141.21, average training loss: 10109.36, base loss: 14440.69
[INFO 2017-06-28 15:12:19,966 main.py:51] epoch 4046, training loss: 10113.28, average training loss: 10109.43, base loss: 14442.17
[INFO 2017-06-28 15:12:20,787 main.py:51] epoch 4047, training loss: 9250.00, average training loss: 10109.76, base loss: 14442.23
[INFO 2017-06-28 15:12:21,613 main.py:51] epoch 4048, training loss: 8847.25, average training loss: 10108.65, base loss: 14440.11
[INFO 2017-06-28 15:12:22,289 main.py:51] epoch 4049, training loss: 11908.52, average training loss: 10110.78, base loss: 14442.78
[INFO 2017-06-28 15:12:23,107 main.py:51] epoch 4050, training loss: 9217.43, average training loss: 10111.40, base loss: 14443.80
[INFO 2017-06-28 15:12:23,929 main.py:51] epoch 4051, training loss: 11073.70, average training loss: 10111.68, base loss: 14444.06
[INFO 2017-06-28 15:12:24,777 main.py:51] epoch 4052, training loss: 9865.98, average training loss: 10112.24, base loss: 14445.91
[INFO 2017-06-28 15:12:25,450 main.py:51] epoch 4053, training loss: 11233.33, average training loss: 10111.62, base loss: 14443.88
[INFO 2017-06-28 15:12:26,224 main.py:51] epoch 4054, training loss: 9414.89, average training loss: 10111.23, base loss: 14442.78
[INFO 2017-06-28 15:12:27,034 main.py:51] epoch 4055, training loss: 10424.65, average training loss: 10111.32, base loss: 14443.05
[INFO 2017-06-28 15:12:27,759 main.py:51] epoch 4056, training loss: 10719.47, average training loss: 10109.19, base loss: 14438.74
[INFO 2017-06-28 15:12:28,449 main.py:51] epoch 4057, training loss: 10157.80, average training loss: 10107.50, base loss: 14434.59
[INFO 2017-06-28 15:12:29,244 main.py:51] epoch 4058, training loss: 11989.43, average training loss: 10108.81, base loss: 14436.03
[INFO 2017-06-28 15:12:30,081 main.py:51] epoch 4059, training loss: 9865.07, average training loss: 10109.74, base loss: 14438.40
[INFO 2017-06-28 15:12:30,734 main.py:51] epoch 4060, training loss: 9652.47, average training loss: 10107.73, base loss: 14433.94
[INFO 2017-06-28 15:12:31,556 main.py:51] epoch 4061, training loss: 10540.96, average training loss: 10106.88, base loss: 14430.26
[INFO 2017-06-28 15:12:32,384 main.py:51] epoch 4062, training loss: 10348.86, average training loss: 10107.44, base loss: 14431.41
[INFO 2017-06-28 15:12:33,171 main.py:51] epoch 4063, training loss: 10546.49, average training loss: 10107.77, base loss: 14431.74
[INFO 2017-06-28 15:12:33,880 main.py:51] epoch 4064, training loss: 9443.65, average training loss: 10105.74, base loss: 14431.94
[INFO 2017-06-28 15:12:34,717 main.py:51] epoch 4065, training loss: 9052.85, average training loss: 10106.06, base loss: 14432.98
[INFO 2017-06-28 15:12:35,538 main.py:51] epoch 4066, training loss: 9628.67, average training loss: 10104.00, base loss: 14431.04
[INFO 2017-06-28 15:12:36,337 main.py:51] epoch 4067, training loss: 10274.83, average training loss: 10104.05, base loss: 14431.39
[INFO 2017-06-28 15:12:36,982 main.py:51] epoch 4068, training loss: 9530.22, average training loss: 10103.72, base loss: 14431.23
[INFO 2017-06-28 15:12:37,791 main.py:51] epoch 4069, training loss: 12041.51, average training loss: 10106.42, base loss: 14434.95
[INFO 2017-06-28 15:12:38,608 main.py:51] epoch 4070, training loss: 10076.62, average training loss: 10106.42, base loss: 14436.28
[INFO 2017-06-28 15:12:39,491 main.py:51] epoch 4071, training loss: 8967.33, average training loss: 10105.28, base loss: 14435.02
[INFO 2017-06-28 15:12:40,145 main.py:51] epoch 4072, training loss: 12060.80, average training loss: 10107.60, base loss: 14438.16
[INFO 2017-06-28 15:12:40,948 main.py:51] epoch 4073, training loss: 10693.18, average training loss: 10108.58, base loss: 14439.93
[INFO 2017-06-28 15:12:41,792 main.py:51] epoch 4074, training loss: 9847.98, average training loss: 10106.93, base loss: 14438.49
[INFO 2017-06-28 15:12:42,601 main.py:51] epoch 4075, training loss: 11824.75, average training loss: 10108.83, base loss: 14442.02
[INFO 2017-06-28 15:12:43,275 main.py:51] epoch 4076, training loss: 10957.28, average training loss: 10109.44, base loss: 14442.20
[INFO 2017-06-28 15:12:44,116 main.py:51] epoch 4077, training loss: 10579.71, average training loss: 10111.11, base loss: 14445.11
[INFO 2017-06-28 15:12:44,970 main.py:51] epoch 4078, training loss: 10586.88, average training loss: 10110.84, base loss: 14444.84
[INFO 2017-06-28 15:12:45,698 main.py:51] epoch 4079, training loss: 10079.72, average training loss: 10112.31, base loss: 14446.17
[INFO 2017-06-28 15:12:46,408 main.py:51] epoch 4080, training loss: 9477.69, average training loss: 10112.04, base loss: 14445.84
[INFO 2017-06-28 15:12:47,188 main.py:51] epoch 4081, training loss: 8798.75, average training loss: 10109.99, base loss: 14445.72
[INFO 2017-06-28 15:12:48,024 main.py:51] epoch 4082, training loss: 9353.56, average training loss: 10106.44, base loss: 14441.85
[INFO 2017-06-28 15:12:48,684 main.py:51] epoch 4083, training loss: 9746.32, average training loss: 10104.21, base loss: 14438.19
[INFO 2017-06-28 15:12:49,455 main.py:51] epoch 4084, training loss: 9920.24, average training loss: 10103.23, base loss: 14436.73
[INFO 2017-06-28 15:12:50,269 main.py:51] epoch 4085, training loss: 10320.73, average training loss: 10103.17, base loss: 14436.14
[INFO 2017-06-28 15:12:51,090 main.py:51] epoch 4086, training loss: 10362.85, average training loss: 10102.47, base loss: 14435.63
[INFO 2017-06-28 15:12:51,743 main.py:51] epoch 4087, training loss: 10749.12, average training loss: 10103.59, base loss: 14436.10
[INFO 2017-06-28 15:12:52,565 main.py:51] epoch 4088, training loss: 10674.46, average training loss: 10104.58, base loss: 14436.54
[INFO 2017-06-28 15:12:53,401 main.py:51] epoch 4089, training loss: 10005.75, average training loss: 10103.63, base loss: 14435.31
[INFO 2017-06-28 15:12:54,246 main.py:51] epoch 4090, training loss: 8931.16, average training loss: 10103.31, base loss: 14433.84
[INFO 2017-06-28 15:12:54,900 main.py:51] epoch 4091, training loss: 9739.52, average training loss: 10102.83, base loss: 14433.26
[INFO 2017-06-28 15:12:55,666 main.py:51] epoch 4092, training loss: 9437.16, average training loss: 10101.11, base loss: 14428.73
[INFO 2017-06-28 15:12:56,529 main.py:51] epoch 4093, training loss: 10464.88, average training loss: 10101.18, base loss: 14429.08
[INFO 2017-06-28 15:12:57,218 main.py:51] epoch 4094, training loss: 8871.57, average training loss: 10099.95, base loss: 14426.93
[INFO 2017-06-28 15:12:57,940 main.py:51] epoch 4095, training loss: 11056.51, average training loss: 10100.75, base loss: 14430.13
[INFO 2017-06-28 15:12:58,764 main.py:51] epoch 4096, training loss: 10627.62, average training loss: 10101.34, base loss: 14431.72
[INFO 2017-06-28 15:12:59,518 main.py:51] epoch 4097, training loss: 10442.72, average training loss: 10102.06, base loss: 14432.11
[INFO 2017-06-28 15:13:00,223 main.py:51] epoch 4098, training loss: 9074.56, average training loss: 10101.01, base loss: 14429.56
[INFO 2017-06-28 15:13:01,009 main.py:51] epoch 4099, training loss: 11184.76, average training loss: 10101.68, base loss: 14429.95
[INFO 2017-06-28 15:13:01,009 main.py:53] epoch 4099, testing
[INFO 2017-06-28 15:13:03,828 main.py:105] average testing loss: 10514.72, base loss: 14627.98
[INFO 2017-06-28 15:13:03,828 main.py:106] improve_loss: 4113.25, improve_percent: 0.28
[INFO 2017-06-28 15:13:03,829 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:13:04,657 main.py:51] epoch 4100, training loss: 10894.95, average training loss: 10102.33, base loss: 14431.91
[INFO 2017-06-28 15:13:05,308 main.py:51] epoch 4101, training loss: 11587.60, average training loss: 10103.29, base loss: 14432.49
[INFO 2017-06-28 15:13:06,031 main.py:51] epoch 4102, training loss: 8797.10, average training loss: 10103.29, base loss: 14433.57
[INFO 2017-06-28 15:13:06,656 main.py:51] epoch 4103, training loss: 9943.99, average training loss: 10103.41, base loss: 14432.91
[INFO 2017-06-28 15:13:07,305 main.py:51] epoch 4104, training loss: 8186.10, average training loss: 10101.33, base loss: 14429.75
[INFO 2017-06-28 15:13:07,922 main.py:51] epoch 4105, training loss: 8949.35, average training loss: 10100.48, base loss: 14428.23
[INFO 2017-06-28 15:13:08,529 main.py:51] epoch 4106, training loss: 9627.30, average training loss: 10100.69, base loss: 14428.20
[INFO 2017-06-28 15:13:09,184 main.py:51] epoch 4107, training loss: 9652.39, average training loss: 10099.58, base loss: 14427.77
[INFO 2017-06-28 15:13:09,813 main.py:51] epoch 4108, training loss: 10249.42, average training loss: 10100.81, base loss: 14429.20
[INFO 2017-06-28 15:13:10,645 main.py:51] epoch 4109, training loss: 12697.83, average training loss: 10104.52, base loss: 14434.48
[INFO 2017-06-28 15:13:11,350 main.py:51] epoch 4110, training loss: 10185.91, average training loss: 10105.60, base loss: 14436.64
[INFO 2017-06-28 15:13:12,095 main.py:51] epoch 4111, training loss: 9718.86, average training loss: 10104.59, base loss: 14436.16
[INFO 2017-06-28 15:13:12,863 main.py:51] epoch 4112, training loss: 10064.32, average training loss: 10106.00, base loss: 14437.39
[INFO 2017-06-28 15:13:13,695 main.py:51] epoch 4113, training loss: 9638.49, average training loss: 10103.53, base loss: 14435.72
[INFO 2017-06-28 15:13:14,316 main.py:51] epoch 4114, training loss: 9417.41, average training loss: 10102.62, base loss: 14434.84
[INFO 2017-06-28 15:13:15,079 main.py:51] epoch 4115, training loss: 11882.95, average training loss: 10104.44, base loss: 14438.36
[INFO 2017-06-28 15:13:15,925 main.py:51] epoch 4116, training loss: 8448.89, average training loss: 10101.99, base loss: 14435.83
[INFO 2017-06-28 15:13:16,590 main.py:51] epoch 4117, training loss: 9337.83, average training loss: 10100.55, base loss: 14431.63
[INFO 2017-06-28 15:13:17,343 main.py:51] epoch 4118, training loss: 9961.39, average training loss: 10098.70, base loss: 14429.97
[INFO 2017-06-28 15:13:18,164 main.py:51] epoch 4119, training loss: 10235.97, average training loss: 10098.28, base loss: 14429.04
[INFO 2017-06-28 15:13:18,970 main.py:51] epoch 4120, training loss: 9688.11, average training loss: 10097.49, base loss: 14428.28
[INFO 2017-06-28 15:13:19,599 main.py:51] epoch 4121, training loss: 11397.40, average training loss: 10099.60, base loss: 14429.88
[INFO 2017-06-28 15:13:20,425 main.py:51] epoch 4122, training loss: 9816.11, average training loss: 10098.63, base loss: 14429.00
[INFO 2017-06-28 15:13:21,256 main.py:51] epoch 4123, training loss: 10314.16, average training loss: 10098.63, base loss: 14429.21
[INFO 2017-06-28 15:13:21,995 main.py:51] epoch 4124, training loss: 9087.43, average training loss: 10098.54, base loss: 14429.87
[INFO 2017-06-28 15:13:22,704 main.py:51] epoch 4125, training loss: 9600.60, average training loss: 10097.80, base loss: 14429.79
[INFO 2017-06-28 15:13:23,507 main.py:51] epoch 4126, training loss: 9918.23, average training loss: 10096.20, base loss: 14428.54
[INFO 2017-06-28 15:13:24,335 main.py:51] epoch 4127, training loss: 9199.09, average training loss: 10095.03, base loss: 14426.63
[INFO 2017-06-28 15:13:24,955 main.py:51] epoch 4128, training loss: 10861.53, average training loss: 10095.84, base loss: 14428.32
[INFO 2017-06-28 15:13:25,721 main.py:51] epoch 4129, training loss: 11440.03, average training loss: 10097.79, base loss: 14429.61
[INFO 2017-06-28 15:13:26,536 main.py:51] epoch 4130, training loss: 8063.74, average training loss: 10095.62, base loss: 14425.41
[INFO 2017-06-28 15:13:27,232 main.py:51] epoch 4131, training loss: 9149.16, average training loss: 10093.12, base loss: 14423.90
[INFO 2017-06-28 15:13:27,942 main.py:51] epoch 4132, training loss: 10695.63, average training loss: 10095.33, base loss: 14426.73
[INFO 2017-06-28 15:13:28,763 main.py:51] epoch 4133, training loss: 10149.60, average training loss: 10096.07, base loss: 14426.28
[INFO 2017-06-28 15:13:29,597 main.py:51] epoch 4134, training loss: 9255.60, average training loss: 10094.45, base loss: 14422.68
[INFO 2017-06-28 15:13:30,221 main.py:51] epoch 4135, training loss: 8534.73, average training loss: 10093.78, base loss: 14420.59
[INFO 2017-06-28 15:13:30,973 main.py:51] epoch 4136, training loss: 9851.98, average training loss: 10093.52, base loss: 14421.58
[INFO 2017-06-28 15:13:31,763 main.py:51] epoch 4137, training loss: 11725.21, average training loss: 10095.73, base loss: 14423.92
[INFO 2017-06-28 15:13:32,465 main.py:51] epoch 4138, training loss: 11395.12, average training loss: 10097.52, base loss: 14425.92
[INFO 2017-06-28 15:13:33,216 main.py:51] epoch 4139, training loss: 9000.78, average training loss: 10096.17, base loss: 14425.45
[INFO 2017-06-28 15:13:34,047 main.py:51] epoch 4140, training loss: 9900.65, average training loss: 10095.98, base loss: 14423.36
[INFO 2017-06-28 15:13:34,846 main.py:51] epoch 4141, training loss: 8975.75, average training loss: 10095.47, base loss: 14422.13
[INFO 2017-06-28 15:13:35,514 main.py:51] epoch 4142, training loss: 9485.37, average training loss: 10096.04, base loss: 14424.39
[INFO 2017-06-28 15:13:36,281 main.py:51] epoch 4143, training loss: 12174.62, average training loss: 10098.99, base loss: 14429.58
[INFO 2017-06-28 15:13:37,077 main.py:51] epoch 4144, training loss: 10103.46, average training loss: 10098.58, base loss: 14425.86
[INFO 2017-06-28 15:13:37,719 main.py:51] epoch 4145, training loss: 12378.61, average training loss: 10101.11, base loss: 14427.55
[INFO 2017-06-28 15:13:38,498 main.py:51] epoch 4146, training loss: 10591.18, average training loss: 10101.84, base loss: 14427.77
[INFO 2017-06-28 15:13:39,359 main.py:51] epoch 4147, training loss: 8883.96, average training loss: 10100.98, base loss: 14425.63
[INFO 2017-06-28 15:13:40,113 main.py:51] epoch 4148, training loss: 11177.00, average training loss: 10102.69, base loss: 14429.14
[INFO 2017-06-28 15:13:40,815 main.py:51] epoch 4149, training loss: 10108.58, average training loss: 10102.12, base loss: 14427.76
[INFO 2017-06-28 15:13:41,633 main.py:51] epoch 4150, training loss: 9356.78, average training loss: 10102.21, base loss: 14427.03
[INFO 2017-06-28 15:13:42,492 main.py:51] epoch 4151, training loss: 10103.12, average training loss: 10102.70, base loss: 14428.51
[INFO 2017-06-28 15:13:43,112 main.py:51] epoch 4152, training loss: 10836.30, average training loss: 10102.33, base loss: 14426.98
[INFO 2017-06-28 15:13:43,911 main.py:51] epoch 4153, training loss: 9820.74, average training loss: 10102.44, base loss: 14426.54
[INFO 2017-06-28 15:13:44,711 main.py:51] epoch 4154, training loss: 9214.74, average training loss: 10102.24, base loss: 14427.63
[INFO 2017-06-28 15:13:45,495 main.py:51] epoch 4155, training loss: 9506.42, average training loss: 10101.36, base loss: 14426.49
[INFO 2017-06-28 15:13:46,200 main.py:51] epoch 4156, training loss: 11217.87, average training loss: 10102.56, base loss: 14428.58
[INFO 2017-06-28 15:13:47,029 main.py:51] epoch 4157, training loss: 10349.73, average training loss: 10103.11, base loss: 14428.81
[INFO 2017-06-28 15:13:47,839 main.py:51] epoch 4158, training loss: 9051.22, average training loss: 10102.83, base loss: 14427.50
[INFO 2017-06-28 15:13:48,580 main.py:51] epoch 4159, training loss: 8801.70, average training loss: 10101.07, base loss: 14425.49
[INFO 2017-06-28 15:13:49,305 main.py:51] epoch 4160, training loss: 8900.38, average training loss: 10099.94, base loss: 14423.75
[INFO 2017-06-28 15:13:50,133 main.py:51] epoch 4161, training loss: 10232.98, average training loss: 10100.89, base loss: 14425.54
[INFO 2017-06-28 15:13:50,958 main.py:51] epoch 4162, training loss: 9598.01, average training loss: 10100.88, base loss: 14425.71
[INFO 2017-06-28 15:13:51,672 main.py:51] epoch 4163, training loss: 10379.53, average training loss: 10099.97, base loss: 14424.59
[INFO 2017-06-28 15:13:52,384 main.py:51] epoch 4164, training loss: 9043.51, average training loss: 10099.75, base loss: 14423.83
[INFO 2017-06-28 15:13:53,165 main.py:51] epoch 4165, training loss: 10008.96, average training loss: 10099.35, base loss: 14421.80
[INFO 2017-06-28 15:13:53,931 main.py:51] epoch 4166, training loss: 9902.10, average training loss: 10098.73, base loss: 14422.81
[INFO 2017-06-28 15:13:54,558 main.py:51] epoch 4167, training loss: 9963.89, average training loss: 10097.16, base loss: 14421.67
[INFO 2017-06-28 15:13:55,337 main.py:51] epoch 4168, training loss: 9479.97, average training loss: 10097.39, base loss: 14422.73
[INFO 2017-06-28 15:13:56,151 main.py:51] epoch 4169, training loss: 10432.51, average training loss: 10096.95, base loss: 14420.07
[INFO 2017-06-28 15:13:56,770 main.py:51] epoch 4170, training loss: 10120.92, average training loss: 10096.87, base loss: 14420.08
[INFO 2017-06-28 15:13:57,519 main.py:51] epoch 4171, training loss: 10127.12, average training loss: 10098.26, base loss: 14421.08
[INFO 2017-06-28 15:13:58,374 main.py:51] epoch 4172, training loss: 9807.85, average training loss: 10098.20, base loss: 14421.05
[INFO 2017-06-28 15:13:59,092 main.py:51] epoch 4173, training loss: 11774.44, average training loss: 10100.51, base loss: 14423.48
[INFO 2017-06-28 15:13:59,792 main.py:51] epoch 4174, training loss: 10459.43, average training loss: 10100.75, base loss: 14422.72
[INFO 2017-06-28 15:14:00,600 main.py:51] epoch 4175, training loss: 11060.91, average training loss: 10102.34, base loss: 14425.35
[INFO 2017-06-28 15:14:01,445 main.py:51] epoch 4176, training loss: 9981.42, average training loss: 10102.76, base loss: 14426.94
[INFO 2017-06-28 15:14:02,100 main.py:51] epoch 4177, training loss: 11073.29, average training loss: 10103.77, base loss: 14428.36
[INFO 2017-06-28 15:14:02,923 main.py:51] epoch 4178, training loss: 9654.29, average training loss: 10102.88, base loss: 14426.53
[INFO 2017-06-28 15:14:03,710 main.py:51] epoch 4179, training loss: 9631.06, average training loss: 10102.89, base loss: 14426.75
[INFO 2017-06-28 15:14:04,542 main.py:51] epoch 4180, training loss: 8981.96, average training loss: 10101.35, base loss: 14425.18
[INFO 2017-06-28 15:14:05,162 main.py:51] epoch 4181, training loss: 9137.04, average training loss: 10101.71, base loss: 14425.73
[INFO 2017-06-28 15:14:05,954 main.py:51] epoch 4182, training loss: 11467.06, average training loss: 10103.44, base loss: 14428.05
[INFO 2017-06-28 15:14:06,754 main.py:51] epoch 4183, training loss: 10512.66, average training loss: 10105.06, base loss: 14430.33
[INFO 2017-06-28 15:14:07,440 main.py:51] epoch 4184, training loss: 9149.85, average training loss: 10104.21, base loss: 14430.76
[INFO 2017-06-28 15:14:08,187 main.py:51] epoch 4185, training loss: 8381.84, average training loss: 10103.20, base loss: 14427.44
[INFO 2017-06-28 15:14:09,008 main.py:51] epoch 4186, training loss: 10510.61, average training loss: 10102.93, base loss: 14426.80
[INFO 2017-06-28 15:14:09,861 main.py:51] epoch 4187, training loss: 10580.19, average training loss: 10102.90, base loss: 14426.10
[INFO 2017-06-28 15:14:10,516 main.py:51] epoch 4188, training loss: 10730.11, average training loss: 10103.42, base loss: 14426.06
[INFO 2017-06-28 15:14:11,314 main.py:51] epoch 4189, training loss: 9855.91, average training loss: 10103.36, base loss: 14424.75
[INFO 2017-06-28 15:14:12,145 main.py:51] epoch 4190, training loss: 10153.50, average training loss: 10101.87, base loss: 14421.93
[INFO 2017-06-28 15:14:12,972 main.py:51] epoch 4191, training loss: 9447.95, average training loss: 10101.90, base loss: 14421.06
[INFO 2017-06-28 15:14:13,647 main.py:51] epoch 4192, training loss: 10647.57, average training loss: 10102.40, base loss: 14422.74
[INFO 2017-06-28 15:14:14,460 main.py:51] epoch 4193, training loss: 10844.73, average training loss: 10103.56, base loss: 14423.05
[INFO 2017-06-28 15:14:15,297 main.py:51] epoch 4194, training loss: 12357.07, average training loss: 10104.79, base loss: 14426.65
[INFO 2017-06-28 15:14:16,147 main.py:51] epoch 4195, training loss: 10800.84, average training loss: 10104.96, base loss: 14425.75
[INFO 2017-06-28 15:14:16,819 main.py:51] epoch 4196, training loss: 8217.90, average training loss: 10103.97, base loss: 14423.43
[INFO 2017-06-28 15:14:17,629 main.py:51] epoch 4197, training loss: 9571.55, average training loss: 10103.82, base loss: 14423.30
[INFO 2017-06-28 15:14:18,459 main.py:51] epoch 4198, training loss: 8252.30, average training loss: 10101.79, base loss: 14420.52
[INFO 2017-06-28 15:14:19,271 main.py:51] epoch 4199, training loss: 9914.02, average training loss: 10100.18, base loss: 14416.02
[INFO 2017-06-28 15:14:19,271 main.py:53] epoch 4199, testing
[INFO 2017-06-28 15:14:22,181 main.py:105] average testing loss: 11454.68, base loss: 15652.62
[INFO 2017-06-28 15:14:22,181 main.py:106] improve_loss: 4197.94, improve_percent: 0.27
[INFO 2017-06-28 15:14:22,182 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:14:22,831 main.py:51] epoch 4200, training loss: 11317.41, average training loss: 10101.42, base loss: 14417.93
[INFO 2017-06-28 15:14:23,599 main.py:51] epoch 4201, training loss: 10501.89, average training loss: 10102.03, base loss: 14419.49
[INFO 2017-06-28 15:14:24,454 main.py:51] epoch 4202, training loss: 10989.72, average training loss: 10103.18, base loss: 14421.76
[INFO 2017-06-28 15:14:25,131 main.py:51] epoch 4203, training loss: 9668.88, average training loss: 10104.35, base loss: 14422.96
[INFO 2017-06-28 15:14:25,886 main.py:51] epoch 4204, training loss: 8674.01, average training loss: 10102.51, base loss: 14420.23
[INFO 2017-06-28 15:14:26,700 main.py:51] epoch 4205, training loss: 9272.15, average training loss: 10102.71, base loss: 14421.30
[INFO 2017-06-28 15:14:27,546 main.py:51] epoch 4206, training loss: 10559.50, average training loss: 10104.86, base loss: 14425.37
[INFO 2017-06-28 15:14:28,169 main.py:51] epoch 4207, training loss: 10941.96, average training loss: 10106.20, base loss: 14426.10
[INFO 2017-06-28 15:14:28,920 main.py:51] epoch 4208, training loss: 8613.07, average training loss: 10105.00, base loss: 14423.77
[INFO 2017-06-28 15:14:29,797 main.py:51] epoch 4209, training loss: 9560.02, average training loss: 10106.01, base loss: 14425.54
[INFO 2017-06-28 15:14:30,528 main.py:51] epoch 4210, training loss: 10644.12, average training loss: 10108.01, base loss: 14428.51
[INFO 2017-06-28 15:14:31,209 main.py:51] epoch 4211, training loss: 10314.54, average training loss: 10106.10, base loss: 14426.78
[INFO 2017-06-28 15:14:32,023 main.py:51] epoch 4212, training loss: 11464.35, average training loss: 10108.24, base loss: 14431.02
[INFO 2017-06-28 15:14:32,817 main.py:51] epoch 4213, training loss: 10502.19, average training loss: 10107.47, base loss: 14431.80
[INFO 2017-06-28 15:14:33,453 main.py:51] epoch 4214, training loss: 8628.67, average training loss: 10106.94, base loss: 14431.07
[INFO 2017-06-28 15:14:34,234 main.py:51] epoch 4215, training loss: 9078.93, average training loss: 10105.55, base loss: 14429.37
[INFO 2017-06-28 15:14:35,058 main.py:51] epoch 4216, training loss: 10276.38, average training loss: 10106.01, base loss: 14428.34
[INFO 2017-06-28 15:14:35,747 main.py:51] epoch 4217, training loss: 9012.98, average training loss: 10104.92, base loss: 14426.29
[INFO 2017-06-28 15:14:36,490 main.py:51] epoch 4218, training loss: 10192.55, average training loss: 10103.91, base loss: 14425.96
[INFO 2017-06-28 15:14:37,299 main.py:51] epoch 4219, training loss: 9030.75, average training loss: 10103.62, base loss: 14426.10
[INFO 2017-06-28 15:14:38,007 main.py:51] epoch 4220, training loss: 10174.68, average training loss: 10104.04, base loss: 14426.50
[INFO 2017-06-28 15:14:38,711 main.py:51] epoch 4221, training loss: 9453.53, average training loss: 10103.98, base loss: 14425.92
[INFO 2017-06-28 15:14:39,522 main.py:51] epoch 4222, training loss: 8865.87, average training loss: 10102.89, base loss: 14424.11
[INFO 2017-06-28 15:14:40,345 main.py:51] epoch 4223, training loss: 9738.01, average training loss: 10101.42, base loss: 14423.33
[INFO 2017-06-28 15:14:40,999 main.py:51] epoch 4224, training loss: 9928.76, average training loss: 10100.79, base loss: 14422.22
[INFO 2017-06-28 15:14:41,721 main.py:51] epoch 4225, training loss: 11142.35, average training loss: 10102.05, base loss: 14423.35
[INFO 2017-06-28 15:14:42,547 main.py:51] epoch 4226, training loss: 10705.38, average training loss: 10101.27, base loss: 14422.88
[INFO 2017-06-28 15:14:43,163 main.py:51] epoch 4227, training loss: 9737.69, average training loss: 10100.96, base loss: 14422.33
[INFO 2017-06-28 15:14:43,905 main.py:51] epoch 4228, training loss: 8342.24, average training loss: 10099.74, base loss: 14422.24
[INFO 2017-06-28 15:14:44,768 main.py:51] epoch 4229, training loss: 10181.80, average training loss: 10098.91, base loss: 14420.75
[INFO 2017-06-28 15:14:45,521 main.py:51] epoch 4230, training loss: 9568.02, average training loss: 10098.13, base loss: 14420.99
[INFO 2017-06-28 15:14:46,199 main.py:51] epoch 4231, training loss: 9173.81, average training loss: 10098.90, base loss: 14423.63
[INFO 2017-06-28 15:14:47,017 main.py:51] epoch 4232, training loss: 11409.75, average training loss: 10099.57, base loss: 14422.25
[INFO 2017-06-28 15:14:47,838 main.py:51] epoch 4233, training loss: 11082.25, average training loss: 10098.89, base loss: 14420.85
[INFO 2017-06-28 15:14:48,519 main.py:51] epoch 4234, training loss: 9273.82, average training loss: 10098.39, base loss: 14420.03
[INFO 2017-06-28 15:14:49,298 main.py:51] epoch 4235, training loss: 9226.64, average training loss: 10097.66, base loss: 14418.65
[INFO 2017-06-28 15:14:50,134 main.py:51] epoch 4236, training loss: 11469.94, average training loss: 10099.02, base loss: 14420.69
[INFO 2017-06-28 15:14:50,872 main.py:51] epoch 4237, training loss: 10505.85, average training loss: 10097.99, base loss: 14420.90
[INFO 2017-06-28 15:14:51,580 main.py:51] epoch 4238, training loss: 10259.22, average training loss: 10098.80, base loss: 14421.84
[INFO 2017-06-28 15:14:52,391 main.py:51] epoch 4239, training loss: 8434.13, average training loss: 10096.95, base loss: 14420.06
[INFO 2017-06-28 15:14:53,174 main.py:51] epoch 4240, training loss: 8127.65, average training loss: 10095.50, base loss: 14416.89
[INFO 2017-06-28 15:14:53,827 main.py:51] epoch 4241, training loss: 11652.24, average training loss: 10097.38, base loss: 14419.69
[INFO 2017-06-28 15:14:54,655 main.py:51] epoch 4242, training loss: 9606.97, average training loss: 10096.70, base loss: 14418.44
[INFO 2017-06-28 15:14:55,482 main.py:51] epoch 4243, training loss: 9908.64, average training loss: 10097.68, base loss: 14419.99
[INFO 2017-06-28 15:14:56,179 main.py:51] epoch 4244, training loss: 8600.90, average training loss: 10096.84, base loss: 14420.20
[INFO 2017-06-28 15:14:56,880 main.py:51] epoch 4245, training loss: 9432.02, average training loss: 10094.23, base loss: 14418.77
[INFO 2017-06-28 15:14:57,487 main.py:51] epoch 4246, training loss: 10938.35, average training loss: 10094.82, base loss: 14418.29
[INFO 2017-06-28 15:14:58,118 main.py:51] epoch 4247, training loss: 10621.61, average training loss: 10095.77, base loss: 14419.27
[INFO 2017-06-28 15:14:58,802 main.py:51] epoch 4248, training loss: 10210.81, average training loss: 10096.39, base loss: 14419.37
[INFO 2017-06-28 15:14:59,496 main.py:51] epoch 4249, training loss: 8567.27, average training loss: 10094.75, base loss: 14416.88
[INFO 2017-06-28 15:15:00,305 main.py:51] epoch 4250, training loss: 10215.75, average training loss: 10096.04, base loss: 14417.88
[INFO 2017-06-28 15:15:01,107 main.py:51] epoch 4251, training loss: 9099.26, average training loss: 10095.37, base loss: 14418.15
[INFO 2017-06-28 15:15:01,785 main.py:51] epoch 4252, training loss: 10244.88, average training loss: 10095.07, base loss: 14418.80
[INFO 2017-06-28 15:15:02,577 main.py:51] epoch 4253, training loss: 8660.75, average training loss: 10093.73, base loss: 14417.04
[INFO 2017-06-28 15:15:03,384 main.py:51] epoch 4254, training loss: 11202.85, average training loss: 10094.61, base loss: 14419.17
[INFO 2017-06-28 15:15:04,081 main.py:51] epoch 4255, training loss: 9694.13, average training loss: 10093.98, base loss: 14418.43
[INFO 2017-06-28 15:15:04,799 main.py:51] epoch 4256, training loss: 10137.38, average training loss: 10095.26, base loss: 14420.09
[INFO 2017-06-28 15:15:05,603 main.py:51] epoch 4257, training loss: 9463.70, average training loss: 10095.16, base loss: 14420.66
[INFO 2017-06-28 15:15:06,425 main.py:51] epoch 4258, training loss: 11331.71, average training loss: 10093.68, base loss: 14418.14
[INFO 2017-06-28 15:15:07,066 main.py:51] epoch 4259, training loss: 8852.25, average training loss: 10093.30, base loss: 14415.83
[INFO 2017-06-28 15:15:07,807 main.py:51] epoch 4260, training loss: 9362.39, average training loss: 10093.24, base loss: 14416.83
[INFO 2017-06-28 15:15:08,621 main.py:51] epoch 4261, training loss: 11180.77, average training loss: 10094.89, base loss: 14418.75
[INFO 2017-06-28 15:15:09,280 main.py:51] epoch 4262, training loss: 9459.83, average training loss: 10094.02, base loss: 14417.25
[INFO 2017-06-28 15:15:10,056 main.py:51] epoch 4263, training loss: 8858.76, average training loss: 10092.85, base loss: 14415.74
[INFO 2017-06-28 15:15:10,866 main.py:51] epoch 4264, training loss: 10347.29, average training loss: 10093.82, base loss: 14417.42
[INFO 2017-06-28 15:15:11,649 main.py:51] epoch 4265, training loss: 11311.66, average training loss: 10094.91, base loss: 14419.88
[INFO 2017-06-28 15:15:12,322 main.py:51] epoch 4266, training loss: 9607.74, average training loss: 10094.62, base loss: 14418.51
[INFO 2017-06-28 15:15:13,096 main.py:51] epoch 4267, training loss: 11353.79, average training loss: 10096.98, base loss: 14421.89
[INFO 2017-06-28 15:15:13,936 main.py:51] epoch 4268, training loss: 10629.46, average training loss: 10097.77, base loss: 14423.40
[INFO 2017-06-28 15:15:14,622 main.py:51] epoch 4269, training loss: 10623.88, average training loss: 10099.12, base loss: 14423.91
[INFO 2017-06-28 15:15:15,310 main.py:51] epoch 4270, training loss: 9929.07, average training loss: 10099.79, base loss: 14424.09
[INFO 2017-06-28 15:15:16,144 main.py:51] epoch 4271, training loss: 10748.54, average training loss: 10101.23, base loss: 14427.57
[INFO 2017-06-28 15:15:16,859 main.py:51] epoch 4272, training loss: 11016.82, average training loss: 10102.10, base loss: 14428.31
[INFO 2017-06-28 15:15:17,582 main.py:51] epoch 4273, training loss: 10945.15, average training loss: 10103.02, base loss: 14430.42
[INFO 2017-06-28 15:15:18,380 main.py:51] epoch 4274, training loss: 9532.70, average training loss: 10102.56, base loss: 14430.25
[INFO 2017-06-28 15:15:19,220 main.py:51] epoch 4275, training loss: 10094.96, average training loss: 10103.60, base loss: 14432.76
[INFO 2017-06-28 15:15:19,834 main.py:51] epoch 4276, training loss: 10492.12, average training loss: 10105.09, base loss: 14435.30
[INFO 2017-06-28 15:15:20,639 main.py:51] epoch 4277, training loss: 11676.08, average training loss: 10105.74, base loss: 14439.92
[INFO 2017-06-28 15:15:21,462 main.py:51] epoch 4278, training loss: 10246.16, average training loss: 10106.56, base loss: 14442.14
[INFO 2017-06-28 15:15:22,250 main.py:51] epoch 4279, training loss: 10262.18, average training loss: 10107.27, base loss: 14443.43
[INFO 2017-06-28 15:15:22,933 main.py:51] epoch 4280, training loss: 12127.80, average training loss: 10110.28, base loss: 14447.74
[INFO 2017-06-28 15:15:23,797 main.py:51] epoch 4281, training loss: 8226.64, average training loss: 10108.84, base loss: 14445.81
[INFO 2017-06-28 15:15:24,628 main.py:51] epoch 4282, training loss: 10851.69, average training loss: 10109.59, base loss: 14446.78
[INFO 2017-06-28 15:15:25,411 main.py:51] epoch 4283, training loss: 9610.48, average training loss: 10108.31, base loss: 14445.82
[INFO 2017-06-28 15:15:26,134 main.py:51] epoch 4284, training loss: 10887.64, average training loss: 10109.31, base loss: 14444.56
[INFO 2017-06-28 15:15:26,882 main.py:51] epoch 4285, training loss: 10671.80, average training loss: 10110.97, base loss: 14448.20
[INFO 2017-06-28 15:15:27,673 main.py:51] epoch 4286, training loss: 11300.60, average training loss: 10112.27, base loss: 14449.74
[INFO 2017-06-28 15:15:28,383 main.py:51] epoch 4287, training loss: 9811.73, average training loss: 10110.96, base loss: 14447.33
[INFO 2017-06-28 15:15:29,131 main.py:51] epoch 4288, training loss: 10574.77, average training loss: 10111.94, base loss: 14449.63
[INFO 2017-06-28 15:15:29,926 main.py:51] epoch 4289, training loss: 8709.20, average training loss: 10111.16, base loss: 14449.80
[INFO 2017-06-28 15:15:30,728 main.py:51] epoch 4290, training loss: 11133.97, average training loss: 10113.41, base loss: 14453.37
[INFO 2017-06-28 15:15:31,420 main.py:51] epoch 4291, training loss: 9273.50, average training loss: 10111.87, base loss: 14452.18
[INFO 2017-06-28 15:15:32,153 main.py:51] epoch 4292, training loss: 12828.39, average training loss: 10114.16, base loss: 14456.74
[INFO 2017-06-28 15:15:32,973 main.py:51] epoch 4293, training loss: 11048.35, average training loss: 10114.78, base loss: 14457.06
[INFO 2017-06-28 15:15:33,811 main.py:51] epoch 4294, training loss: 10966.72, average training loss: 10115.32, base loss: 14458.13
[INFO 2017-06-28 15:15:34,462 main.py:51] epoch 4295, training loss: 10439.14, average training loss: 10114.94, base loss: 14457.27
[INFO 2017-06-28 15:15:35,220 main.py:51] epoch 4296, training loss: 9176.91, average training loss: 10113.55, base loss: 14454.19
[INFO 2017-06-28 15:15:36,078 main.py:51] epoch 4297, training loss: 9382.37, average training loss: 10113.66, base loss: 14453.44
[INFO 2017-06-28 15:15:36,765 main.py:51] epoch 4298, training loss: 11117.39, average training loss: 10114.93, base loss: 14455.57
[INFO 2017-06-28 15:15:37,504 main.py:51] epoch 4299, training loss: 8952.85, average training loss: 10113.13, base loss: 14454.94
[INFO 2017-06-28 15:15:37,505 main.py:53] epoch 4299, testing
[INFO 2017-06-28 15:15:40,330 main.py:105] average testing loss: 11737.46, base loss: 15773.97
[INFO 2017-06-28 15:15:40,330 main.py:106] improve_loss: 4036.51, improve_percent: 0.26
[INFO 2017-06-28 15:15:40,331 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:15:41,156 main.py:51] epoch 4300, training loss: 9277.59, average training loss: 10112.84, base loss: 14455.28
[INFO 2017-06-28 15:15:41,929 main.py:51] epoch 4301, training loss: 12196.70, average training loss: 10112.42, base loss: 14455.63
[INFO 2017-06-28 15:15:42,631 main.py:51] epoch 4302, training loss: 11358.03, average training loss: 10114.15, base loss: 14458.56
[INFO 2017-06-28 15:15:43,396 main.py:51] epoch 4303, training loss: 11154.77, average training loss: 10114.13, base loss: 14458.09
[INFO 2017-06-28 15:15:44,242 main.py:51] epoch 4304, training loss: 11281.57, average training loss: 10114.83, base loss: 14460.36
[INFO 2017-06-28 15:15:44,929 main.py:51] epoch 4305, training loss: 9780.04, average training loss: 10114.50, base loss: 14459.94
[INFO 2017-06-28 15:15:45,683 main.py:51] epoch 4306, training loss: 11384.28, average training loss: 10115.01, base loss: 14460.77
[INFO 2017-06-28 15:15:46,515 main.py:51] epoch 4307, training loss: 10116.91, average training loss: 10114.35, base loss: 14459.54
[INFO 2017-06-28 15:15:47,377 main.py:51] epoch 4308, training loss: 10432.34, average training loss: 10114.12, base loss: 14460.74
[INFO 2017-06-28 15:15:48,032 main.py:51] epoch 4309, training loss: 10009.09, average training loss: 10114.40, base loss: 14460.75
[INFO 2017-06-28 15:15:48,768 main.py:51] epoch 4310, training loss: 9298.20, average training loss: 10113.06, base loss: 14459.67
[INFO 2017-06-28 15:15:49,607 main.py:51] epoch 4311, training loss: 11688.67, average training loss: 10115.11, base loss: 14461.16
[INFO 2017-06-28 15:15:50,376 main.py:51] epoch 4312, training loss: 13013.41, average training loss: 10118.93, base loss: 14467.30
[INFO 2017-06-28 15:15:51,049 main.py:51] epoch 4313, training loss: 14106.18, average training loss: 10122.68, base loss: 14472.00
[INFO 2017-06-28 15:15:51,858 main.py:51] epoch 4314, training loss: 11775.51, average training loss: 10123.29, base loss: 14472.28
[INFO 2017-06-28 15:15:52,570 main.py:51] epoch 4315, training loss: 10942.84, average training loss: 10124.16, base loss: 14471.90
[INFO 2017-06-28 15:15:53,271 main.py:51] epoch 4316, training loss: 10983.61, average training loss: 10126.30, base loss: 14475.21
[INFO 2017-06-28 15:15:54,077 main.py:51] epoch 4317, training loss: 10824.39, average training loss: 10125.95, base loss: 14473.63
[INFO 2017-06-28 15:15:54,931 main.py:51] epoch 4318, training loss: 10591.32, average training loss: 10125.57, base loss: 14472.56
[INFO 2017-06-28 15:15:55,616 main.py:51] epoch 4319, training loss: 13315.70, average training loss: 10128.46, base loss: 14475.79
[INFO 2017-06-28 15:15:56,371 main.py:51] epoch 4320, training loss: 9435.71, average training loss: 10127.36, base loss: 14475.55
[INFO 2017-06-28 15:15:57,188 main.py:51] epoch 4321, training loss: 11010.24, average training loss: 10128.93, base loss: 14477.71
[INFO 2017-06-28 15:15:58,036 main.py:51] epoch 4322, training loss: 9432.32, average training loss: 10128.57, base loss: 14478.06
[INFO 2017-06-28 15:15:58,707 main.py:51] epoch 4323, training loss: 10624.30, average training loss: 10130.03, base loss: 14480.33
[INFO 2017-06-28 15:15:59,506 main.py:51] epoch 4324, training loss: 11934.51, average training loss: 10131.30, base loss: 14480.67
[INFO 2017-06-28 15:16:00,330 main.py:51] epoch 4325, training loss: 9287.11, average training loss: 10130.77, base loss: 14479.22
[INFO 2017-06-28 15:16:01,175 main.py:51] epoch 4326, training loss: 9423.76, average training loss: 10130.45, base loss: 14479.01
[INFO 2017-06-28 15:16:01,844 main.py:51] epoch 4327, training loss: 10760.75, average training loss: 10128.96, base loss: 14477.55
[INFO 2017-06-28 15:16:02,653 main.py:51] epoch 4328, training loss: 8783.86, average training loss: 10128.50, base loss: 14477.42
[INFO 2017-06-28 15:16:03,490 main.py:51] epoch 4329, training loss: 9637.56, average training loss: 10127.71, base loss: 14478.86
[INFO 2017-06-28 15:16:04,278 main.py:51] epoch 4330, training loss: 8343.52, average training loss: 10125.07, base loss: 14476.14
[INFO 2017-06-28 15:16:04,887 main.py:51] epoch 4331, training loss: 9745.66, average training loss: 10124.35, base loss: 14474.49
[INFO 2017-06-28 15:16:05,694 main.py:51] epoch 4332, training loss: 9086.13, average training loss: 10123.26, base loss: 14472.88
[INFO 2017-06-28 15:16:06,521 main.py:51] epoch 4333, training loss: 10675.66, average training loss: 10124.47, base loss: 14473.77
[INFO 2017-06-28 15:16:07,181 main.py:51] epoch 4334, training loss: 9855.40, average training loss: 10123.71, base loss: 14472.16
[INFO 2017-06-28 15:16:07,930 main.py:51] epoch 4335, training loss: 8805.77, average training loss: 10122.87, base loss: 14470.67
[INFO 2017-06-28 15:16:08,753 main.py:51] epoch 4336, training loss: 9266.53, average training loss: 10122.19, base loss: 14471.03
[INFO 2017-06-28 15:16:09,549 main.py:51] epoch 4337, training loss: 11468.40, average training loss: 10123.14, base loss: 14469.80
[INFO 2017-06-28 15:16:10,218 main.py:51] epoch 4338, training loss: 8379.82, average training loss: 10120.73, base loss: 14465.93
[INFO 2017-06-28 15:16:11,002 main.py:51] epoch 4339, training loss: 12696.02, average training loss: 10121.75, base loss: 14468.77
[INFO 2017-06-28 15:16:11,784 main.py:51] epoch 4340, training loss: 11045.79, average training loss: 10123.04, base loss: 14470.74
[INFO 2017-06-28 15:16:12,390 main.py:51] epoch 4341, training loss: 9701.83, average training loss: 10124.12, base loss: 14473.57
[INFO 2017-06-28 15:16:13,194 main.py:51] epoch 4342, training loss: 9537.00, average training loss: 10121.46, base loss: 14470.24
[INFO 2017-06-28 15:16:14,028 main.py:51] epoch 4343, training loss: 8874.93, average training loss: 10121.13, base loss: 14467.65
[INFO 2017-06-28 15:16:14,780 main.py:51] epoch 4344, training loss: 9400.85, average training loss: 10121.22, base loss: 14466.75
[INFO 2017-06-28 15:16:15,460 main.py:51] epoch 4345, training loss: 9801.75, average training loss: 10120.51, base loss: 14464.74
[INFO 2017-06-28 15:16:16,273 main.py:51] epoch 4346, training loss: 8868.93, average training loss: 10118.25, base loss: 14462.52
[INFO 2017-06-28 15:16:16,964 main.py:51] epoch 4347, training loss: 10127.62, average training loss: 10118.56, base loss: 14464.93
[INFO 2017-06-28 15:16:17,693 main.py:51] epoch 4348, training loss: 9043.35, average training loss: 10117.95, base loss: 14464.37
[INFO 2017-06-28 15:16:18,516 main.py:51] epoch 4349, training loss: 10023.52, average training loss: 10118.30, base loss: 14464.91
[INFO 2017-06-28 15:16:19,295 main.py:51] epoch 4350, training loss: 10566.46, average training loss: 10116.75, base loss: 14460.39
[INFO 2017-06-28 15:16:19,977 main.py:51] epoch 4351, training loss: 10386.71, average training loss: 10117.82, base loss: 14462.78
[INFO 2017-06-28 15:16:20,744 main.py:51] epoch 4352, training loss: 8958.76, average training loss: 10116.93, base loss: 14461.89
[INFO 2017-06-28 15:16:21,601 main.py:51] epoch 4353, training loss: 9498.53, average training loss: 10117.06, base loss: 14462.00
[INFO 2017-06-28 15:16:22,326 main.py:51] epoch 4354, training loss: 9513.96, average training loss: 10117.61, base loss: 14462.32
[INFO 2017-06-28 15:16:23,032 main.py:51] epoch 4355, training loss: 9566.41, average training loss: 10114.88, base loss: 14458.43
[INFO 2017-06-28 15:16:23,811 main.py:51] epoch 4356, training loss: 10869.63, average training loss: 10115.49, base loss: 14458.52
[INFO 2017-06-28 15:16:24,645 main.py:51] epoch 4357, training loss: 9335.83, average training loss: 10112.02, base loss: 14454.75
[INFO 2017-06-28 15:16:25,291 main.py:51] epoch 4358, training loss: 8526.73, average training loss: 10111.44, base loss: 14455.05
[INFO 2017-06-28 15:16:26,097 main.py:51] epoch 4359, training loss: 11289.71, average training loss: 10112.75, base loss: 14458.61
[INFO 2017-06-28 15:16:26,930 main.py:51] epoch 4360, training loss: 11288.24, average training loss: 10112.17, base loss: 14460.39
[INFO 2017-06-28 15:16:27,691 main.py:51] epoch 4361, training loss: 9086.55, average training loss: 10111.01, base loss: 14458.85
[INFO 2017-06-28 15:16:28,372 main.py:51] epoch 4362, training loss: 9702.17, average training loss: 10111.64, base loss: 14460.39
[INFO 2017-06-28 15:16:29,170 main.py:51] epoch 4363, training loss: 12266.86, average training loss: 10112.17, base loss: 14460.94
[INFO 2017-06-28 15:16:30,010 main.py:51] epoch 4364, training loss: 9814.85, average training loss: 10112.86, base loss: 14461.09
[INFO 2017-06-28 15:16:30,751 main.py:51] epoch 4365, training loss: 9757.32, average training loss: 10111.54, base loss: 14459.29
[INFO 2017-06-28 15:16:31,473 main.py:51] epoch 4366, training loss: 9587.39, average training loss: 10109.58, base loss: 14457.07
[INFO 2017-06-28 15:16:32,249 main.py:51] epoch 4367, training loss: 9968.36, average training loss: 10109.99, base loss: 14457.81
[INFO 2017-06-28 15:16:33,060 main.py:51] epoch 4368, training loss: 10575.95, average training loss: 10110.17, base loss: 14458.67
[INFO 2017-06-28 15:16:33,692 main.py:51] epoch 4369, training loss: 10177.95, average training loss: 10110.34, base loss: 14459.84
[INFO 2017-06-28 15:16:34,486 main.py:51] epoch 4370, training loss: 9480.04, average training loss: 10110.12, base loss: 14462.29
[INFO 2017-06-28 15:16:35,318 main.py:51] epoch 4371, training loss: 10956.06, average training loss: 10110.90, base loss: 14464.05
[INFO 2017-06-28 15:16:35,920 main.py:51] epoch 4372, training loss: 10204.16, average training loss: 10110.81, base loss: 14462.84
[INFO 2017-06-28 15:16:36,685 main.py:51] epoch 4373, training loss: 7968.60, average training loss: 10109.15, base loss: 14460.16
[INFO 2017-06-28 15:16:37,538 main.py:51] epoch 4374, training loss: 11136.79, average training loss: 10109.96, base loss: 14461.16
[INFO 2017-06-28 15:16:38,246 main.py:51] epoch 4375, training loss: 9073.69, average training loss: 10108.18, base loss: 14459.41
[INFO 2017-06-28 15:16:38,953 main.py:51] epoch 4376, training loss: 8816.01, average training loss: 10106.83, base loss: 14457.29
[INFO 2017-06-28 15:16:39,779 main.py:51] epoch 4377, training loss: 10488.07, average training loss: 10108.03, base loss: 14457.69
[INFO 2017-06-28 15:16:40,521 main.py:51] epoch 4378, training loss: 9592.65, average training loss: 10108.83, base loss: 14459.45
[INFO 2017-06-28 15:16:41,211 main.py:51] epoch 4379, training loss: 9762.47, average training loss: 10108.14, base loss: 14458.01
[INFO 2017-06-28 15:16:41,998 main.py:51] epoch 4380, training loss: 11433.74, average training loss: 10109.04, base loss: 14460.07
[INFO 2017-06-28 15:16:42,786 main.py:51] epoch 4381, training loss: 12352.34, average training loss: 10112.08, base loss: 14464.42
[INFO 2017-06-28 15:16:43,422 main.py:51] epoch 4382, training loss: 9858.14, average training loss: 10110.12, base loss: 14462.22
[INFO 2017-06-28 15:16:44,224 main.py:51] epoch 4383, training loss: 11597.95, average training loss: 10111.26, base loss: 14463.93
[INFO 2017-06-28 15:16:45,058 main.py:51] epoch 4384, training loss: 11769.95, average training loss: 10111.30, base loss: 14464.80
[INFO 2017-06-28 15:16:45,753 main.py:51] epoch 4385, training loss: 10053.44, average training loss: 10110.95, base loss: 14463.41
[INFO 2017-06-28 15:16:46,364 main.py:51] epoch 4386, training loss: 9944.94, average training loss: 10111.45, base loss: 14464.46
[INFO 2017-06-28 15:16:46,994 main.py:51] epoch 4387, training loss: 10187.87, average training loss: 10110.44, base loss: 14463.15
[INFO 2017-06-28 15:16:47,657 main.py:51] epoch 4388, training loss: 9989.61, average training loss: 10110.43, base loss: 14463.34
[INFO 2017-06-28 15:16:48,361 main.py:51] epoch 4389, training loss: 10624.66, average training loss: 10110.16, base loss: 14463.62
[INFO 2017-06-28 15:16:48,978 main.py:51] epoch 4390, training loss: 9750.51, average training loss: 10110.80, base loss: 14464.94
[INFO 2017-06-28 15:16:49,854 main.py:51] epoch 4391, training loss: 10154.92, average training loss: 10109.94, base loss: 14464.91
[INFO 2017-06-28 15:16:50,594 main.py:51] epoch 4392, training loss: 9354.55, average training loss: 10109.50, base loss: 14463.64
[INFO 2017-06-28 15:16:51,277 main.py:51] epoch 4393, training loss: 10738.22, average training loss: 10109.19, base loss: 14464.65
[INFO 2017-06-28 15:16:52,081 main.py:51] epoch 4394, training loss: 9388.36, average training loss: 10108.89, base loss: 14465.89
[INFO 2017-06-28 15:16:52,930 main.py:51] epoch 4395, training loss: 9665.36, average training loss: 10107.92, base loss: 14463.82
[INFO 2017-06-28 15:16:53,677 main.py:51] epoch 4396, training loss: 8793.02, average training loss: 10107.33, base loss: 14462.35
[INFO 2017-06-28 15:16:54,358 main.py:51] epoch 4397, training loss: 9264.10, average training loss: 10106.01, base loss: 14461.38
[INFO 2017-06-28 15:16:55,169 main.py:51] epoch 4398, training loss: 8965.34, average training loss: 10104.96, base loss: 14458.92
[INFO 2017-06-28 15:16:55,945 main.py:51] epoch 4399, training loss: 9848.47, average training loss: 10103.67, base loss: 14456.90
[INFO 2017-06-28 15:16:55,945 main.py:53] epoch 4399, testing
[INFO 2017-06-28 15:16:58,840 main.py:105] average testing loss: 10726.23, base loss: 14731.87
[INFO 2017-06-28 15:16:58,840 main.py:106] improve_loss: 4005.64, improve_percent: 0.27
[INFO 2017-06-28 15:16:58,841 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:16:59,545 main.py:51] epoch 4400, training loss: 10207.64, average training loss: 10103.83, base loss: 14458.91
[INFO 2017-06-28 15:17:00,286 main.py:51] epoch 4401, training loss: 11008.36, average training loss: 10105.09, base loss: 14460.15
[INFO 2017-06-28 15:17:01,103 main.py:51] epoch 4402, training loss: 9118.05, average training loss: 10104.02, base loss: 14457.66
[INFO 2017-06-28 15:17:01,955 main.py:51] epoch 4403, training loss: 9862.46, average training loss: 10104.22, base loss: 14456.48
[INFO 2017-06-28 15:17:02,603 main.py:51] epoch 4404, training loss: 10114.72, average training loss: 10104.76, base loss: 14455.22
[INFO 2017-06-28 15:17:03,405 main.py:51] epoch 4405, training loss: 10344.03, average training loss: 10102.76, base loss: 14453.46
[INFO 2017-06-28 15:17:04,231 main.py:51] epoch 4406, training loss: 10324.93, average training loss: 10102.56, base loss: 14452.71
[INFO 2017-06-28 15:17:05,029 main.py:51] epoch 4407, training loss: 9880.32, average training loss: 10102.53, base loss: 14454.55
[INFO 2017-06-28 15:17:05,741 main.py:51] epoch 4408, training loss: 8997.44, average training loss: 10101.52, base loss: 14452.35
[INFO 2017-06-28 15:17:06,512 main.py:51] epoch 4409, training loss: 9149.98, average training loss: 10101.06, base loss: 14450.08
[INFO 2017-06-28 15:17:07,367 main.py:51] epoch 4410, training loss: 8950.35, average training loss: 10099.53, base loss: 14447.64
[INFO 2017-06-28 15:17:08,099 main.py:51] epoch 4411, training loss: 10955.26, average training loss: 10100.13, base loss: 14448.78
[INFO 2017-06-28 15:17:08,802 main.py:51] epoch 4412, training loss: 10165.82, average training loss: 10100.24, base loss: 14449.00
[INFO 2017-06-28 15:17:09,578 main.py:51] epoch 4413, training loss: 10056.80, average training loss: 10099.39, base loss: 14445.61
[INFO 2017-06-28 15:17:10,410 main.py:51] epoch 4414, training loss: 9232.16, average training loss: 10098.14, base loss: 14443.48
[INFO 2017-06-28 15:17:11,026 main.py:51] epoch 4415, training loss: 9930.14, average training loss: 10098.07, base loss: 14441.87
[INFO 2017-06-28 15:17:11,825 main.py:51] epoch 4416, training loss: 8708.00, average training loss: 10096.12, base loss: 14439.88
[INFO 2017-06-28 15:17:12,643 main.py:51] epoch 4417, training loss: 10998.74, average training loss: 10097.65, base loss: 14441.52
[INFO 2017-06-28 15:17:13,505 main.py:51] epoch 4418, training loss: 9627.46, average training loss: 10096.13, base loss: 14440.44
[INFO 2017-06-28 15:17:14,131 main.py:51] epoch 4419, training loss: 8797.44, average training loss: 10095.38, base loss: 14439.82
[INFO 2017-06-28 15:17:14,954 main.py:51] epoch 4420, training loss: 8739.79, average training loss: 10094.42, base loss: 14438.07
[INFO 2017-06-28 15:17:15,767 main.py:51] epoch 4421, training loss: 9249.43, average training loss: 10093.93, base loss: 14438.41
[INFO 2017-06-28 15:17:16,543 main.py:51] epoch 4422, training loss: 9052.32, average training loss: 10092.17, base loss: 14436.51
[INFO 2017-06-28 15:17:17,218 main.py:51] epoch 4423, training loss: 9117.79, average training loss: 10090.72, base loss: 14435.53
[INFO 2017-06-28 15:17:18,003 main.py:51] epoch 4424, training loss: 10489.07, average training loss: 10091.50, base loss: 14436.81
[INFO 2017-06-28 15:17:18,825 main.py:51] epoch 4425, training loss: 10352.36, average training loss: 10092.24, base loss: 14437.92
[INFO 2017-06-28 15:17:19,494 main.py:51] epoch 4426, training loss: 10173.64, average training loss: 10092.51, base loss: 14436.40
[INFO 2017-06-28 15:17:20,225 main.py:51] epoch 4427, training loss: 9296.58, average training loss: 10091.74, base loss: 14436.48
[INFO 2017-06-28 15:17:21,055 main.py:51] epoch 4428, training loss: 10346.88, average training loss: 10090.78, base loss: 14435.05
[INFO 2017-06-28 15:17:21,766 main.py:51] epoch 4429, training loss: 11353.81, average training loss: 10093.07, base loss: 14438.76
[INFO 2017-06-28 15:17:22,472 main.py:51] epoch 4430, training loss: 9904.56, average training loss: 10093.35, base loss: 14440.11
[INFO 2017-06-28 15:17:23,286 main.py:51] epoch 4431, training loss: 8083.38, average training loss: 10091.81, base loss: 14438.42
[INFO 2017-06-28 15:17:24,037 main.py:51] epoch 4432, training loss: 10918.79, average training loss: 10093.45, base loss: 14441.16
[INFO 2017-06-28 15:17:24,717 main.py:51] epoch 4433, training loss: 9862.75, average training loss: 10092.81, base loss: 14439.43
[INFO 2017-06-28 15:17:25,528 main.py:51] epoch 4434, training loss: 9613.04, average training loss: 10090.50, base loss: 14436.10
[INFO 2017-06-28 15:17:26,381 main.py:51] epoch 4435, training loss: 10559.45, average training loss: 10091.79, base loss: 14439.44
[INFO 2017-06-28 15:17:27,063 main.py:51] epoch 4436, training loss: 8680.44, average training loss: 10090.92, base loss: 14438.78
[INFO 2017-06-28 15:17:27,812 main.py:51] epoch 4437, training loss: 10812.90, average training loss: 10091.39, base loss: 14439.68
[INFO 2017-06-28 15:17:28,630 main.py:51] epoch 4438, training loss: 11989.18, average training loss: 10093.18, base loss: 14440.09
[INFO 2017-06-28 15:17:29,420 main.py:51] epoch 4439, training loss: 10435.81, average training loss: 10094.02, base loss: 14440.90
[INFO 2017-06-28 15:17:30,065 main.py:51] epoch 4440, training loss: 9594.62, average training loss: 10092.54, base loss: 14439.64
[INFO 2017-06-28 15:17:30,866 main.py:51] epoch 4441, training loss: 11288.41, average training loss: 10091.60, base loss: 14439.51
[INFO 2017-06-28 15:17:31,688 main.py:51] epoch 4442, training loss: 8790.43, average training loss: 10090.63, base loss: 14439.54
[INFO 2017-06-28 15:17:32,538 main.py:51] epoch 4443, training loss: 9950.39, average training loss: 10089.01, base loss: 14436.82
[INFO 2017-06-28 15:17:33,167 main.py:51] epoch 4444, training loss: 10525.75, average training loss: 10090.62, base loss: 14439.66
[INFO 2017-06-28 15:17:33,944 main.py:51] epoch 4445, training loss: 10026.75, average training loss: 10089.50, base loss: 14440.13
[INFO 2017-06-28 15:17:34,770 main.py:51] epoch 4446, training loss: 9430.68, average training loss: 10088.64, base loss: 14439.88
[INFO 2017-06-28 15:17:35,471 main.py:51] epoch 4447, training loss: 9001.02, average training loss: 10087.63, base loss: 14438.77
[INFO 2017-06-28 15:17:36,204 main.py:51] epoch 4448, training loss: 10361.56, average training loss: 10086.89, base loss: 14436.60
[INFO 2017-06-28 15:17:37,004 main.py:51] epoch 4449, training loss: 10747.68, average training loss: 10087.62, base loss: 14438.14
[INFO 2017-06-28 15:17:37,843 main.py:51] epoch 4450, training loss: 10576.07, average training loss: 10088.39, base loss: 14437.79
[INFO 2017-06-28 15:17:38,557 main.py:51] epoch 4451, training loss: 10405.02, average training loss: 10088.29, base loss: 14437.93
[INFO 2017-06-28 15:17:39,307 main.py:51] epoch 4452, training loss: 9158.44, average training loss: 10085.57, base loss: 14434.54
[INFO 2017-06-28 15:17:40,183 main.py:51] epoch 4453, training loss: 9131.77, average training loss: 10084.83, base loss: 14431.69
[INFO 2017-06-28 15:17:41,080 main.py:51] epoch 4454, training loss: 9301.60, average training loss: 10084.01, base loss: 14430.53
[INFO 2017-06-28 15:17:41,697 main.py:51] epoch 4455, training loss: 9839.05, average training loss: 10083.64, base loss: 14428.79
[INFO 2017-06-28 15:17:42,508 main.py:51] epoch 4456, training loss: 9293.76, average training loss: 10082.60, base loss: 14426.06
[INFO 2017-06-28 15:17:43,338 main.py:51] epoch 4457, training loss: 11293.71, average training loss: 10082.62, base loss: 14426.07
[INFO 2017-06-28 15:17:44,148 main.py:51] epoch 4458, training loss: 9675.39, average training loss: 10082.72, base loss: 14426.41
[INFO 2017-06-28 15:17:44,861 main.py:51] epoch 4459, training loss: 9652.18, average training loss: 10082.76, base loss: 14427.68
[INFO 2017-06-28 15:17:45,680 main.py:51] epoch 4460, training loss: 11156.30, average training loss: 10084.36, base loss: 14429.06
[INFO 2017-06-28 15:17:46,486 main.py:51] epoch 4461, training loss: 9415.71, average training loss: 10083.94, base loss: 14428.32
[INFO 2017-06-28 15:17:47,141 main.py:51] epoch 4462, training loss: 9504.70, average training loss: 10082.73, base loss: 14425.04
[INFO 2017-06-28 15:17:47,913 main.py:51] epoch 4463, training loss: 9584.07, average training loss: 10080.62, base loss: 14419.94
[INFO 2017-06-28 15:17:48,734 main.py:51] epoch 4464, training loss: 9393.30, average training loss: 10079.04, base loss: 14417.16
[INFO 2017-06-28 15:17:49,542 main.py:51] epoch 4465, training loss: 8708.69, average training loss: 10077.34, base loss: 14415.01
[INFO 2017-06-28 15:17:50,216 main.py:51] epoch 4466, training loss: 8730.45, average training loss: 10074.42, base loss: 14410.07
[INFO 2017-06-28 15:17:51,015 main.py:51] epoch 4467, training loss: 10090.28, average training loss: 10075.15, base loss: 14410.66
[INFO 2017-06-28 15:17:51,859 main.py:51] epoch 4468, training loss: 8893.37, average training loss: 10072.88, base loss: 14406.51
[INFO 2017-06-28 15:17:52,684 main.py:51] epoch 4469, training loss: 10193.21, average training loss: 10073.22, base loss: 14407.93
[INFO 2017-06-28 15:17:53,346 main.py:51] epoch 4470, training loss: 11792.21, average training loss: 10074.54, base loss: 14410.35
[INFO 2017-06-28 15:17:54,154 main.py:51] epoch 4471, training loss: 8254.21, average training loss: 10074.64, base loss: 14410.78
[INFO 2017-06-28 15:17:55,041 main.py:51] epoch 4472, training loss: 10034.80, average training loss: 10073.36, base loss: 14409.96
[INFO 2017-06-28 15:17:55,758 main.py:51] epoch 4473, training loss: 9615.37, average training loss: 10074.30, base loss: 14411.09
[INFO 2017-06-28 15:17:56,478 main.py:51] epoch 4474, training loss: 8101.37, average training loss: 10071.40, base loss: 14406.11
[INFO 2017-06-28 15:17:57,262 main.py:51] epoch 4475, training loss: 10325.35, average training loss: 10069.14, base loss: 14405.20
[INFO 2017-06-28 15:17:58,050 main.py:51] epoch 4476, training loss: 8882.68, average training loss: 10068.35, base loss: 14403.89
[INFO 2017-06-28 15:17:58,632 main.py:51] epoch 4477, training loss: 10002.79, average training loss: 10068.29, base loss: 14405.16
[INFO 2017-06-28 15:17:59,421 main.py:51] epoch 4478, training loss: 9548.52, average training loss: 10069.09, base loss: 14405.12
[INFO 2017-06-28 15:18:00,235 main.py:51] epoch 4479, training loss: 11109.90, average training loss: 10071.45, base loss: 14409.13
[INFO 2017-06-28 15:18:00,842 main.py:51] epoch 4480, training loss: 8790.07, average training loss: 10070.31, base loss: 14406.96
[INFO 2017-06-28 15:18:01,598 main.py:51] epoch 4481, training loss: 9929.60, average training loss: 10071.25, base loss: 14408.60
[INFO 2017-06-28 15:18:02,459 main.py:51] epoch 4482, training loss: 10621.57, average training loss: 10072.74, base loss: 14409.86
[INFO 2017-06-28 15:18:03,275 main.py:51] epoch 4483, training loss: 10785.97, average training loss: 10074.00, base loss: 14412.19
[INFO 2017-06-28 15:18:03,950 main.py:51] epoch 4484, training loss: 11761.56, average training loss: 10074.91, base loss: 14414.57
[INFO 2017-06-28 15:18:04,740 main.py:51] epoch 4485, training loss: 9762.14, average training loss: 10075.25, base loss: 14415.73
[INFO 2017-06-28 15:18:05,584 main.py:51] epoch 4486, training loss: 9931.55, average training loss: 10075.57, base loss: 14416.34
[INFO 2017-06-28 15:18:06,343 main.py:51] epoch 4487, training loss: 9078.00, average training loss: 10074.39, base loss: 14415.45
[INFO 2017-06-28 15:18:07,006 main.py:51] epoch 4488, training loss: 11116.48, average training loss: 10075.07, base loss: 14417.52
[INFO 2017-06-28 15:18:07,844 main.py:51] epoch 4489, training loss: 11213.98, average training loss: 10076.75, base loss: 14421.87
[INFO 2017-06-28 15:18:08,610 main.py:51] epoch 4490, training loss: 8983.30, average training loss: 10077.34, base loss: 14422.94
[INFO 2017-06-28 15:18:09,283 main.py:51] epoch 4491, training loss: 10538.83, average training loss: 10078.37, base loss: 14425.75
[INFO 2017-06-28 15:18:10,057 main.py:51] epoch 4492, training loss: 9574.03, average training loss: 10078.06, base loss: 14425.06
[INFO 2017-06-28 15:18:10,893 main.py:51] epoch 4493, training loss: 8878.87, average training loss: 10077.36, base loss: 14424.80
[INFO 2017-06-28 15:18:11,524 main.py:51] epoch 4494, training loss: 8667.24, average training loss: 10075.90, base loss: 14423.00
[INFO 2017-06-28 15:18:12,334 main.py:51] epoch 4495, training loss: 10191.85, average training loss: 10076.85, base loss: 14426.09
[INFO 2017-06-28 15:18:13,147 main.py:51] epoch 4496, training loss: 11737.27, average training loss: 10077.71, base loss: 14426.56
[INFO 2017-06-28 15:18:13,955 main.py:51] epoch 4497, training loss: 10520.31, average training loss: 10078.14, base loss: 14428.00
[INFO 2017-06-28 15:18:14,615 main.py:51] epoch 4498, training loss: 10970.16, average training loss: 10078.81, base loss: 14428.95
[INFO 2017-06-28 15:18:15,417 main.py:51] epoch 4499, training loss: 8733.24, average training loss: 10077.83, base loss: 14425.86
[INFO 2017-06-28 15:18:15,417 main.py:53] epoch 4499, testing
[INFO 2017-06-28 15:18:18,395 main.py:105] average testing loss: 10907.53, base loss: 15014.05
[INFO 2017-06-28 15:18:18,396 main.py:106] improve_loss: 4106.51, improve_percent: 0.27
[INFO 2017-06-28 15:18:18,396 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:18:19,213 main.py:51] epoch 4500, training loss: 10578.43, average training loss: 10079.62, base loss: 14428.85
[INFO 2017-06-28 15:18:20,074 main.py:51] epoch 4501, training loss: 10388.49, average training loss: 10078.53, base loss: 14424.71
[INFO 2017-06-28 15:18:20,916 main.py:51] epoch 4502, training loss: 10764.98, average training loss: 10079.06, base loss: 14424.51
[INFO 2017-06-28 15:18:21,577 main.py:51] epoch 4503, training loss: 9906.51, average training loss: 10078.37, base loss: 14423.51
[INFO 2017-06-28 15:18:22,404 main.py:51] epoch 4504, training loss: 9212.99, average training loss: 10076.87, base loss: 14422.22
[INFO 2017-06-28 15:18:23,235 main.py:51] epoch 4505, training loss: 9057.84, average training loss: 10076.01, base loss: 14421.30
[INFO 2017-06-28 15:18:24,085 main.py:51] epoch 4506, training loss: 10827.35, average training loss: 10078.10, base loss: 14423.81
[INFO 2017-06-28 15:18:24,725 main.py:51] epoch 4507, training loss: 10663.37, average training loss: 10078.45, base loss: 14424.77
[INFO 2017-06-28 15:18:25,549 main.py:51] epoch 4508, training loss: 9873.10, average training loss: 10077.44, base loss: 14423.88
[INFO 2017-06-28 15:18:26,358 main.py:51] epoch 4509, training loss: 11015.29, average training loss: 10078.36, base loss: 14424.04
[INFO 2017-06-28 15:18:27,166 main.py:51] epoch 4510, training loss: 10074.11, average training loss: 10079.21, base loss: 14423.66
[INFO 2017-06-28 15:18:27,839 main.py:51] epoch 4511, training loss: 9731.01, average training loss: 10079.46, base loss: 14421.68
[INFO 2017-06-28 15:18:28,654 main.py:51] epoch 4512, training loss: 9588.84, average training loss: 10079.81, base loss: 14422.52
[INFO 2017-06-28 15:18:29,475 main.py:51] epoch 4513, training loss: 10492.04, average training loss: 10079.44, base loss: 14422.04
[INFO 2017-06-28 15:18:30,292 main.py:51] epoch 4514, training loss: 10022.54, average training loss: 10080.50, base loss: 14423.83
[INFO 2017-06-28 15:18:30,958 main.py:51] epoch 4515, training loss: 9436.23, average training loss: 10079.62, base loss: 14422.97
[INFO 2017-06-28 15:18:31,727 main.py:51] epoch 4516, training loss: 9076.88, average training loss: 10078.13, base loss: 14420.64
[INFO 2017-06-28 15:18:32,615 main.py:51] epoch 4517, training loss: 10599.03, average training loss: 10079.15, base loss: 14421.61
[INFO 2017-06-28 15:18:33,465 main.py:51] epoch 4518, training loss: 12538.90, average training loss: 10082.57, base loss: 14425.98
[INFO 2017-06-28 15:18:34,136 main.py:51] epoch 4519, training loss: 9914.16, average training loss: 10082.28, base loss: 14424.76
[INFO 2017-06-28 15:18:34,803 main.py:51] epoch 4520, training loss: 11280.98, average training loss: 10083.12, base loss: 14425.02
[INFO 2017-06-28 15:18:35,427 main.py:51] epoch 4521, training loss: 10366.48, average training loss: 10083.46, base loss: 14423.37
[INFO 2017-06-28 15:18:36,055 main.py:51] epoch 4522, training loss: 9570.21, average training loss: 10084.18, base loss: 14424.21
[INFO 2017-06-28 15:18:36,752 main.py:51] epoch 4523, training loss: 10699.63, average training loss: 10085.99, base loss: 14427.39
[INFO 2017-06-28 15:18:37,460 main.py:51] epoch 4524, training loss: 10212.28, average training loss: 10085.10, base loss: 14424.64
[INFO 2017-06-28 15:18:38,138 main.py:51] epoch 4525, training loss: 9965.74, average training loss: 10084.04, base loss: 14423.35
[INFO 2017-06-28 15:18:38,896 main.py:51] epoch 4526, training loss: 12560.28, average training loss: 10086.38, base loss: 14427.56
[INFO 2017-06-28 15:18:39,695 main.py:51] epoch 4527, training loss: 10386.33, average training loss: 10086.43, base loss: 14426.86
[INFO 2017-06-28 15:18:40,516 main.py:51] epoch 4528, training loss: 10176.23, average training loss: 10086.85, base loss: 14427.81
[INFO 2017-06-28 15:18:41,208 main.py:51] epoch 4529, training loss: 9598.36, average training loss: 10085.85, base loss: 14427.89
[INFO 2017-06-28 15:18:41,987 main.py:51] epoch 4530, training loss: 10281.29, average training loss: 10085.25, base loss: 14425.95
[INFO 2017-06-28 15:18:42,806 main.py:51] epoch 4531, training loss: 10264.82, average training loss: 10084.31, base loss: 14424.92
[INFO 2017-06-28 15:18:43,522 main.py:51] epoch 4532, training loss: 10123.12, average training loss: 10083.79, base loss: 14425.77
[INFO 2017-06-28 15:18:44,232 main.py:51] epoch 4533, training loss: 9731.11, average training loss: 10083.75, base loss: 14425.32
[INFO 2017-06-28 15:18:45,038 main.py:51] epoch 4534, training loss: 9828.88, average training loss: 10082.86, base loss: 14424.21
[INFO 2017-06-28 15:18:45,859 main.py:51] epoch 4535, training loss: 9537.54, average training loss: 10083.14, base loss: 14424.87
[INFO 2017-06-28 15:18:46,512 main.py:51] epoch 4536, training loss: 9959.86, average training loss: 10082.34, base loss: 14424.06
[INFO 2017-06-28 15:18:47,271 main.py:51] epoch 4537, training loss: 9468.57, average training loss: 10081.82, base loss: 14423.40
[INFO 2017-06-28 15:18:48,096 main.py:51] epoch 4538, training loss: 8865.28, average training loss: 10081.37, base loss: 14423.81
[INFO 2017-06-28 15:18:48,966 main.py:51] epoch 4539, training loss: 9225.41, average training loss: 10079.52, base loss: 14421.33
[INFO 2017-06-28 15:18:49,538 main.py:51] epoch 4540, training loss: 9838.49, average training loss: 10079.48, base loss: 14422.01
[INFO 2017-06-28 15:18:50,343 main.py:51] epoch 4541, training loss: 10431.85, average training loss: 10080.40, base loss: 14423.12
[INFO 2017-06-28 15:18:51,168 main.py:51] epoch 4542, training loss: 10032.50, average training loss: 10081.82, base loss: 14422.58
[INFO 2017-06-28 15:18:51,829 main.py:51] epoch 4543, training loss: 8988.96, average training loss: 10080.62, base loss: 14421.63
[INFO 2017-06-28 15:18:52,589 main.py:51] epoch 4544, training loss: 11060.84, average training loss: 10081.89, base loss: 14422.74
[INFO 2017-06-28 15:18:53,403 main.py:51] epoch 4545, training loss: 11188.93, average training loss: 10081.73, base loss: 14423.75
[INFO 2017-06-28 15:18:54,210 main.py:51] epoch 4546, training loss: 9064.83, average training loss: 10080.15, base loss: 14422.34
[INFO 2017-06-28 15:18:54,844 main.py:51] epoch 4547, training loss: 10785.69, average training loss: 10079.69, base loss: 14420.73
[INFO 2017-06-28 15:18:55,592 main.py:51] epoch 4548, training loss: 10154.71, average training loss: 10078.60, base loss: 14418.72
[INFO 2017-06-28 15:18:56,439 main.py:51] epoch 4549, training loss: 10781.29, average training loss: 10078.08, base loss: 14420.03
[INFO 2017-06-28 15:18:57,140 main.py:51] epoch 4550, training loss: 8512.04, average training loss: 10078.19, base loss: 14419.08
[INFO 2017-06-28 15:18:57,836 main.py:51] epoch 4551, training loss: 9680.43, average training loss: 10078.87, base loss: 14420.09
[INFO 2017-06-28 15:18:58,696 main.py:51] epoch 4552, training loss: 9266.66, average training loss: 10078.01, base loss: 14419.36
[INFO 2017-06-28 15:18:59,451 main.py:51] epoch 4553, training loss: 10341.33, average training loss: 10079.50, base loss: 14422.76
[INFO 2017-06-28 15:19:00,184 main.py:51] epoch 4554, training loss: 8673.84, average training loss: 10076.68, base loss: 14418.98
[INFO 2017-06-28 15:19:00,963 main.py:51] epoch 4555, training loss: 10954.12, average training loss: 10078.56, base loss: 14421.26
[INFO 2017-06-28 15:19:01,780 main.py:51] epoch 4556, training loss: 11454.51, average training loss: 10080.89, base loss: 14426.62
[INFO 2017-06-28 15:19:02,427 main.py:51] epoch 4557, training loss: 8280.87, average training loss: 10078.20, base loss: 14423.47
[INFO 2017-06-28 15:19:03,231 main.py:51] epoch 4558, training loss: 10773.51, average training loss: 10077.80, base loss: 14422.71
[INFO 2017-06-28 15:19:04,067 main.py:51] epoch 4559, training loss: 9459.46, average training loss: 10077.60, base loss: 14422.06
[INFO 2017-06-28 15:19:04,868 main.py:51] epoch 4560, training loss: 8793.42, average training loss: 10076.60, base loss: 14420.47
[INFO 2017-06-28 15:19:05,614 main.py:51] epoch 4561, training loss: 9850.21, average training loss: 10076.83, base loss: 14419.27
[INFO 2017-06-28 15:19:06,391 main.py:51] epoch 4562, training loss: 12029.22, average training loss: 10077.35, base loss: 14420.99
[INFO 2017-06-28 15:19:07,209 main.py:51] epoch 4563, training loss: 10147.71, average training loss: 10077.48, base loss: 14421.36
[INFO 2017-06-28 15:19:07,855 main.py:51] epoch 4564, training loss: 10973.94, average training loss: 10077.41, base loss: 14422.70
[INFO 2017-06-28 15:19:08,651 main.py:51] epoch 4565, training loss: 11072.48, average training loss: 10079.43, base loss: 14424.48
[INFO 2017-06-28 15:19:09,486 main.py:51] epoch 4566, training loss: 8647.03, average training loss: 10077.20, base loss: 14419.64
[INFO 2017-06-28 15:19:10,281 main.py:51] epoch 4567, training loss: 8943.04, average training loss: 10076.91, base loss: 14419.71
[INFO 2017-06-28 15:19:10,913 main.py:51] epoch 4568, training loss: 10759.80, average training loss: 10076.13, base loss: 14419.24
[INFO 2017-06-28 15:19:11,709 main.py:51] epoch 4569, training loss: 10687.38, average training loss: 10076.74, base loss: 14418.97
[INFO 2017-06-28 15:19:12,509 main.py:51] epoch 4570, training loss: 8894.38, average training loss: 10075.67, base loss: 14419.48
[INFO 2017-06-28 15:19:13,199 main.py:51] epoch 4571, training loss: 9076.43, average training loss: 10072.87, base loss: 14416.89
[INFO 2017-06-28 15:19:13,968 main.py:51] epoch 4572, training loss: 10866.89, average training loss: 10074.10, base loss: 14416.39
[INFO 2017-06-28 15:19:14,757 main.py:51] epoch 4573, training loss: 8767.20, average training loss: 10072.86, base loss: 14414.52
[INFO 2017-06-28 15:19:15,576 main.py:51] epoch 4574, training loss: 8056.89, average training loss: 10072.33, base loss: 14414.51
[INFO 2017-06-28 15:19:16,261 main.py:51] epoch 4575, training loss: 10459.36, average training loss: 10073.05, base loss: 14416.68
[INFO 2017-06-28 15:19:17,022 main.py:51] epoch 4576, training loss: 9353.10, average training loss: 10071.95, base loss: 14414.54
[INFO 2017-06-28 15:19:17,850 main.py:51] epoch 4577, training loss: 8984.74, average training loss: 10071.94, base loss: 14414.37
[INFO 2017-06-28 15:19:18,499 main.py:51] epoch 4578, training loss: 9988.17, average training loss: 10070.97, base loss: 14413.71
[INFO 2017-06-28 15:19:19,246 main.py:51] epoch 4579, training loss: 10938.25, average training loss: 10071.23, base loss: 14415.17
[INFO 2017-06-28 15:19:20,070 main.py:51] epoch 4580, training loss: 10672.35, average training loss: 10073.09, base loss: 14417.60
[INFO 2017-06-28 15:19:20,926 main.py:51] epoch 4581, training loss: 9670.84, average training loss: 10072.27, base loss: 14416.49
[INFO 2017-06-28 15:19:21,579 main.py:51] epoch 4582, training loss: 11228.81, average training loss: 10074.19, base loss: 14420.56
[INFO 2017-06-28 15:19:22,387 main.py:51] epoch 4583, training loss: 11058.95, average training loss: 10074.29, base loss: 14419.84
[INFO 2017-06-28 15:19:23,207 main.py:51] epoch 4584, training loss: 10858.96, average training loss: 10074.95, base loss: 14418.37
[INFO 2017-06-28 15:19:23,921 main.py:51] epoch 4585, training loss: 10406.28, average training loss: 10074.42, base loss: 14418.29
[INFO 2017-06-28 15:19:24,605 main.py:51] epoch 4586, training loss: 9429.61, average training loss: 10073.66, base loss: 14418.06
[INFO 2017-06-28 15:19:25,430 main.py:51] epoch 4587, training loss: 9554.03, average training loss: 10074.00, base loss: 14418.96
[INFO 2017-06-28 15:19:26,242 main.py:51] epoch 4588, training loss: 11966.15, average training loss: 10076.33, base loss: 14422.58
[INFO 2017-06-28 15:19:26,862 main.py:51] epoch 4589, training loss: 10237.77, average training loss: 10075.62, base loss: 14421.49
[INFO 2017-06-28 15:19:27,674 main.py:51] epoch 4590, training loss: 9565.01, average training loss: 10075.73, base loss: 14423.98
[INFO 2017-06-28 15:19:28,511 main.py:51] epoch 4591, training loss: 10133.74, average training loss: 10077.10, base loss: 14426.11
[INFO 2017-06-28 15:19:29,211 main.py:51] epoch 4592, training loss: 9696.73, average training loss: 10075.71, base loss: 14423.96
[INFO 2017-06-28 15:19:29,933 main.py:51] epoch 4593, training loss: 10998.13, average training loss: 10077.30, base loss: 14427.60
[INFO 2017-06-28 15:19:30,726 main.py:51] epoch 4594, training loss: 10830.62, average training loss: 10077.47, base loss: 14427.67
[INFO 2017-06-28 15:19:31,517 main.py:51] epoch 4595, training loss: 11320.73, average training loss: 10077.04, base loss: 14428.14
[INFO 2017-06-28 15:19:32,167 main.py:51] epoch 4596, training loss: 11539.11, average training loss: 10078.51, base loss: 14431.21
[INFO 2017-06-28 15:19:32,951 main.py:51] epoch 4597, training loss: 11511.37, average training loss: 10079.94, base loss: 14432.00
[INFO 2017-06-28 15:19:33,746 main.py:51] epoch 4598, training loss: 9968.07, average training loss: 10077.31, base loss: 14426.34
[INFO 2017-06-28 15:19:34,390 main.py:51] epoch 4599, training loss: 11053.25, average training loss: 10080.33, base loss: 14430.05
[INFO 2017-06-28 15:19:34,390 main.py:53] epoch 4599, testing
[INFO 2017-06-28 15:19:37,218 main.py:105] average testing loss: 10712.14, base loss: 14436.11
[INFO 2017-06-28 15:19:37,218 main.py:106] improve_loss: 3723.98, improve_percent: 0.26
[INFO 2017-06-28 15:19:37,219 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 15:19:37,999 main.py:51] epoch 4600, training loss: 9475.11, average training loss: 10078.15, base loss: 14425.81
[INFO 2017-06-28 15:19:38,818 main.py:51] epoch 4601, training loss: 9209.81, average training loss: 10078.36, base loss: 14424.96
[INFO 2017-06-28 15:19:39,589 main.py:51] epoch 4602, training loss: 9639.57, average training loss: 10076.83, base loss: 14422.76
[INFO 2017-06-28 15:19:40,284 main.py:51] epoch 4603, training loss: 10048.76, average training loss: 10077.37, base loss: 14421.73
[INFO 2017-06-28 15:19:41,085 main.py:51] epoch 4604, training loss: 10363.61, average training loss: 10077.29, base loss: 14419.80
[INFO 2017-06-28 15:19:41,889 main.py:51] epoch 4605, training loss: 10028.69, average training loss: 10077.33, base loss: 14422.56
