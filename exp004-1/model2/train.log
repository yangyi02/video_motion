[INFO 2017-06-28 12:25:46,406 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=10, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-28 12:25:49,093 main.py:51] epoch 0, training loss: 28837.05, average training loss: 28837.05, base loss: 14763.41
[INFO 2017-06-28 12:25:49,476 main.py:51] epoch 1, training loss: 22679.18, average training loss: 25758.12, base loss: 13585.18
[INFO 2017-06-28 12:25:49,850 main.py:51] epoch 2, training loss: 21797.11, average training loss: 24437.78, base loss: 13739.45
[INFO 2017-06-28 12:25:50,214 main.py:51] epoch 3, training loss: 19373.09, average training loss: 23171.61, base loss: 13797.92
[INFO 2017-06-28 12:25:50,586 main.py:51] epoch 4, training loss: 19151.83, average training loss: 22367.65, base loss: 13924.30
[INFO 2017-06-28 12:25:50,959 main.py:51] epoch 5, training loss: 18867.97, average training loss: 21784.37, base loss: 14209.93
[INFO 2017-06-28 12:25:51,335 main.py:51] epoch 6, training loss: 17388.81, average training loss: 21156.44, base loss: 14235.84
[INFO 2017-06-28 12:25:51,719 main.py:51] epoch 7, training loss: 15669.51, average training loss: 20470.57, base loss: 14123.35
[INFO 2017-06-28 12:25:52,098 main.py:51] epoch 8, training loss: 16080.85, average training loss: 19982.82, base loss: 14143.22
[INFO 2017-06-28 12:25:52,488 main.py:51] epoch 9, training loss: 15058.00, average training loss: 19490.34, base loss: 14088.38
[INFO 2017-06-28 12:25:52,859 main.py:51] epoch 10, training loss: 15013.15, average training loss: 19083.32, base loss: 14083.00
[INFO 2017-06-28 12:25:53,229 main.py:51] epoch 11, training loss: 14619.35, average training loss: 18711.33, base loss: 14027.35
[INFO 2017-06-28 12:25:53,598 main.py:51] epoch 12, training loss: 15235.46, average training loss: 18443.95, base loss: 14084.02
[INFO 2017-06-28 12:25:53,975 main.py:51] epoch 13, training loss: 15698.54, average training loss: 18247.85, base loss: 14206.77
[INFO 2017-06-28 12:25:54,331 main.py:51] epoch 14, training loss: 15408.34, average training loss: 18058.55, base loss: 14289.92
[INFO 2017-06-28 12:25:54,694 main.py:51] epoch 15, training loss: 14146.72, average training loss: 17814.06, base loss: 14306.75
[INFO 2017-06-28 12:25:55,065 main.py:51] epoch 16, training loss: 13533.15, average training loss: 17562.24, base loss: 14265.07
[INFO 2017-06-28 12:25:55,440 main.py:51] epoch 17, training loss: 16726.27, average training loss: 17515.80, base loss: 14458.92
[INFO 2017-06-28 12:25:55,791 main.py:51] epoch 18, training loss: 14561.60, average training loss: 17360.31, base loss: 14493.33
[INFO 2017-06-28 12:25:56,150 main.py:51] epoch 19, training loss: 14694.52, average training loss: 17227.03, base loss: 14538.25
[INFO 2017-06-28 12:25:56,523 main.py:51] epoch 20, training loss: 12782.62, average training loss: 17015.39, base loss: 14469.65
[INFO 2017-06-28 12:25:56,888 main.py:51] epoch 21, training loss: 11653.91, average training loss: 16771.68, base loss: 14359.55
[INFO 2017-06-28 12:25:57,250 main.py:51] epoch 22, training loss: 12881.90, average training loss: 16602.56, base loss: 14323.23
[INFO 2017-06-28 12:25:57,617 main.py:51] epoch 23, training loss: 14898.86, average training loss: 16531.57, base loss: 14379.92
[INFO 2017-06-28 12:25:57,965 main.py:51] epoch 24, training loss: 12760.72, average training loss: 16380.74, base loss: 14340.98
[INFO 2017-06-28 12:25:58,331 main.py:51] epoch 25, training loss: 13853.97, average training loss: 16283.56, base loss: 14338.21
[INFO 2017-06-28 12:25:58,710 main.py:51] epoch 26, training loss: 11655.21, average training loss: 16112.14, base loss: 14257.98
[INFO 2017-06-28 12:25:59,088 main.py:51] epoch 27, training loss: 11825.19, average training loss: 15959.03, base loss: 14200.87
[INFO 2017-06-28 12:25:59,439 main.py:51] epoch 28, training loss: 13711.29, average training loss: 15881.52, base loss: 14218.80
[INFO 2017-06-28 12:25:59,779 main.py:51] epoch 29, training loss: 13338.95, average training loss: 15796.77, base loss: 14215.27
[INFO 2017-06-28 12:26:00,164 main.py:51] epoch 30, training loss: 14039.42, average training loss: 15740.08, base loss: 14243.36
[INFO 2017-06-28 12:26:00,529 main.py:51] epoch 31, training loss: 15300.34, average training loss: 15726.34, base loss: 14304.90
[INFO 2017-06-28 12:26:00,879 main.py:51] epoch 32, training loss: 13951.24, average training loss: 15672.55, base loss: 14323.10
[INFO 2017-06-28 12:26:01,252 main.py:51] epoch 33, training loss: 15150.96, average training loss: 15657.21, base loss: 14381.73
[INFO 2017-06-28 12:26:01,614 main.py:51] epoch 34, training loss: 16715.85, average training loss: 15687.46, base loss: 14479.13
[INFO 2017-06-28 12:26:01,980 main.py:51] epoch 35, training loss: 13132.37, average training loss: 15616.48, base loss: 14477.59
[INFO 2017-06-28 12:26:02,352 main.py:51] epoch 36, training loss: 13018.44, average training loss: 15546.26, base loss: 14458.99
[INFO 2017-06-28 12:26:02,724 main.py:51] epoch 37, training loss: 15590.10, average training loss: 15547.42, base loss: 14518.37
[INFO 2017-06-28 12:26:03,112 main.py:51] epoch 38, training loss: 15211.42, average training loss: 15538.80, base loss: 14570.82
[INFO 2017-06-28 12:26:03,469 main.py:51] epoch 39, training loss: 13537.60, average training loss: 15488.77, base loss: 14577.90
[INFO 2017-06-28 12:26:03,820 main.py:51] epoch 40, training loss: 11430.80, average training loss: 15389.80, base loss: 14523.85
[INFO 2017-06-28 12:26:04,183 main.py:51] epoch 41, training loss: 11469.16, average training loss: 15296.45, base loss: 14466.99
[INFO 2017-06-28 12:26:04,561 main.py:51] epoch 42, training loss: 14480.92, average training loss: 15277.48, base loss: 14500.85
[INFO 2017-06-28 12:26:04,942 main.py:51] epoch 43, training loss: 12551.13, average training loss: 15215.52, base loss: 14485.88
[INFO 2017-06-28 12:26:05,286 main.py:51] epoch 44, training loss: 13941.21, average training loss: 15187.20, base loss: 14502.50
[INFO 2017-06-28 12:26:05,656 main.py:51] epoch 45, training loss: 13571.68, average training loss: 15152.08, base loss: 14501.18
[INFO 2017-06-28 12:26:06,015 main.py:51] epoch 46, training loss: 12660.08, average training loss: 15099.06, base loss: 14484.49
[INFO 2017-06-28 12:26:06,371 main.py:51] epoch 47, training loss: 14231.16, average training loss: 15080.98, base loss: 14511.57
[INFO 2017-06-28 12:26:06,766 main.py:51] epoch 48, training loss: 14275.26, average training loss: 15064.54, base loss: 14530.18
[INFO 2017-06-28 12:26:07,146 main.py:51] epoch 49, training loss: 14236.59, average training loss: 15047.98, base loss: 14546.82
[INFO 2017-06-28 12:26:07,511 main.py:51] epoch 50, training loss: 12869.50, average training loss: 15005.26, base loss: 14543.15
[INFO 2017-06-28 12:26:07,896 main.py:51] epoch 51, training loss: 12952.73, average training loss: 14965.79, base loss: 14534.35
[INFO 2017-06-28 12:26:08,255 main.py:51] epoch 52, training loss: 12728.60, average training loss: 14923.58, base loss: 14530.57
[INFO 2017-06-28 12:26:08,594 main.py:51] epoch 53, training loss: 15427.79, average training loss: 14932.92, base loss: 14563.13
[INFO 2017-06-28 12:26:08,975 main.py:51] epoch 54, training loss: 13291.07, average training loss: 14903.06, base loss: 14563.14
[INFO 2017-06-28 12:26:09,351 main.py:51] epoch 55, training loss: 13371.76, average training loss: 14875.72, base loss: 14562.69
[INFO 2017-06-28 12:26:09,699 main.py:51] epoch 56, training loss: 15499.02, average training loss: 14886.66, base loss: 14605.03
[INFO 2017-06-28 12:26:10,066 main.py:51] epoch 57, training loss: 12538.69, average training loss: 14846.17, base loss: 14592.51
[INFO 2017-06-28 12:26:10,417 main.py:51] epoch 58, training loss: 13888.80, average training loss: 14829.95, base loss: 14596.73
[INFO 2017-06-28 12:26:10,800 main.py:51] epoch 59, training loss: 17011.02, average training loss: 14866.30, base loss: 14664.13
[INFO 2017-06-28 12:26:11,182 main.py:51] epoch 60, training loss: 12124.85, average training loss: 14821.36, base loss: 14646.57
[INFO 2017-06-28 12:26:11,548 main.py:51] epoch 61, training loss: 14110.97, average training loss: 14809.90, base loss: 14666.29
[INFO 2017-06-28 12:26:11,898 main.py:51] epoch 62, training loss: 13487.48, average training loss: 14788.91, base loss: 14671.72
[INFO 2017-06-28 12:26:12,267 main.py:51] epoch 63, training loss: 12223.97, average training loss: 14748.83, base loss: 14654.75
[INFO 2017-06-28 12:26:12,633 main.py:51] epoch 64, training loss: 11699.83, average training loss: 14701.92, base loss: 14627.76
[INFO 2017-06-28 12:26:13,009 main.py:51] epoch 65, training loss: 13987.64, average training loss: 14691.10, base loss: 14637.84
[INFO 2017-06-28 12:26:13,384 main.py:51] epoch 66, training loss: 13075.55, average training loss: 14666.99, base loss: 14635.24
[INFO 2017-06-28 12:26:13,769 main.py:51] epoch 67, training loss: 13481.29, average training loss: 14649.55, base loss: 14639.34
[INFO 2017-06-28 12:26:14,154 main.py:51] epoch 68, training loss: 13090.37, average training loss: 14626.95, base loss: 14642.57
[INFO 2017-06-28 12:26:14,520 main.py:51] epoch 69, training loss: 12992.18, average training loss: 14603.60, base loss: 14643.53
[INFO 2017-06-28 12:26:14,875 main.py:51] epoch 70, training loss: 9966.85, average training loss: 14538.29, base loss: 14585.50
[INFO 2017-06-28 12:26:15,218 main.py:51] epoch 71, training loss: 14047.86, average training loss: 14531.48, base loss: 14599.94
[INFO 2017-06-28 12:26:15,581 main.py:51] epoch 72, training loss: 16624.59, average training loss: 14560.15, base loss: 14657.37
[INFO 2017-06-28 12:26:15,974 main.py:51] epoch 73, training loss: 13348.36, average training loss: 14543.78, base loss: 14659.10
[INFO 2017-06-28 12:26:16,338 main.py:51] epoch 74, training loss: 14383.12, average training loss: 14541.64, base loss: 14677.91
[INFO 2017-06-28 12:26:16,691 main.py:51] epoch 75, training loss: 12664.56, average training loss: 14516.94, base loss: 14666.93
[INFO 2017-06-28 12:26:17,047 main.py:51] epoch 76, training loss: 13608.91, average training loss: 14505.15, base loss: 14686.12
[INFO 2017-06-28 12:26:17,424 main.py:51] epoch 77, training loss: 12469.04, average training loss: 14479.04, base loss: 14678.04
[INFO 2017-06-28 12:26:17,763 main.py:51] epoch 78, training loss: 12726.09, average training loss: 14456.85, base loss: 14673.64
[INFO 2017-06-28 12:26:18,160 main.py:51] epoch 79, training loss: 13558.33, average training loss: 14445.62, base loss: 14685.34
[INFO 2017-06-28 12:26:18,556 main.py:51] epoch 80, training loss: 12378.96, average training loss: 14420.11, base loss: 14680.11
[INFO 2017-06-28 12:26:18,949 main.py:51] epoch 81, training loss: 12824.61, average training loss: 14400.65, base loss: 14682.68
[INFO 2017-06-28 12:26:19,329 main.py:51] epoch 82, training loss: 12396.40, average training loss: 14376.50, base loss: 14680.32
[INFO 2017-06-28 12:26:19,692 main.py:51] epoch 83, training loss: 13906.57, average training loss: 14370.91, base loss: 14699.95
[INFO 2017-06-28 12:26:20,050 main.py:51] epoch 84, training loss: 13422.98, average training loss: 14359.76, base loss: 14707.80
[INFO 2017-06-28 12:26:20,459 main.py:51] epoch 85, training loss: 13456.65, average training loss: 14349.25, base loss: 14713.57
[INFO 2017-06-28 12:26:20,822 main.py:51] epoch 86, training loss: 11621.71, average training loss: 14317.90, base loss: 14700.79
[INFO 2017-06-28 12:26:21,188 main.py:51] epoch 87, training loss: 11765.52, average training loss: 14288.90, base loss: 14689.26
[INFO 2017-06-28 12:26:21,546 main.py:51] epoch 88, training loss: 12062.81, average training loss: 14263.89, base loss: 14686.60
[INFO 2017-06-28 12:26:21,931 main.py:51] epoch 89, training loss: 15089.08, average training loss: 14273.06, base loss: 14715.48
[INFO 2017-06-28 12:26:22,283 main.py:51] epoch 90, training loss: 16173.06, average training loss: 14293.93, base loss: 14758.80
[INFO 2017-06-28 12:26:22,660 main.py:51] epoch 91, training loss: 11554.24, average training loss: 14264.16, base loss: 14737.60
[INFO 2017-06-28 12:26:23,035 main.py:51] epoch 92, training loss: 15152.99, average training loss: 14273.71, base loss: 14767.19
[INFO 2017-06-28 12:26:23,413 main.py:51] epoch 93, training loss: 12729.53, average training loss: 14257.29, base loss: 14765.89
[INFO 2017-06-28 12:26:23,776 main.py:51] epoch 94, training loss: 14047.56, average training loss: 14255.08, base loss: 14777.77
[INFO 2017-06-28 12:26:24,148 main.py:51] epoch 95, training loss: 12718.51, average training loss: 14239.07, base loss: 14783.21
[INFO 2017-06-28 12:26:24,541 main.py:51] epoch 96, training loss: 12966.17, average training loss: 14225.95, base loss: 14783.59
[INFO 2017-06-28 12:26:24,917 main.py:51] epoch 97, training loss: 11853.94, average training loss: 14201.74, base loss: 14783.41
[INFO 2017-06-28 12:26:25,285 main.py:51] epoch 98, training loss: 13061.67, average training loss: 14190.23, base loss: 14785.42
[INFO 2017-06-28 12:26:25,649 main.py:51] epoch 99, training loss: 13754.47, average training loss: 14185.87, base loss: 14797.43
[INFO 2017-06-28 12:26:25,649 main.py:53] epoch 99, testing
[INFO 2017-06-28 12:26:27,416 main.py:105] average testing loss: 13619.57, base loss: 15394.10
[INFO 2017-06-28 12:26:27,416 main.py:106] improve_loss: 1774.52, improve_percent: 0.12
[INFO 2017-06-28 12:26:27,416 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:26:27,428 main.py:76] current best improved percent: 0.12
[INFO 2017-06-28 12:26:27,794 main.py:51] epoch 100, training loss: 12361.64, average training loss: 14167.81, base loss: 14797.35
[INFO 2017-06-28 12:26:28,193 main.py:51] epoch 101, training loss: 14880.33, average training loss: 14174.79, base loss: 14821.35
[INFO 2017-06-28 12:26:28,582 main.py:51] epoch 102, training loss: 12729.29, average training loss: 14160.76, base loss: 14823.93
[INFO 2017-06-28 12:26:28,937 main.py:51] epoch 103, training loss: 15503.39, average training loss: 14173.67, base loss: 14854.99
[INFO 2017-06-28 12:26:29,304 main.py:51] epoch 104, training loss: 15429.10, average training loss: 14185.63, base loss: 14891.90
[INFO 2017-06-28 12:26:29,646 main.py:51] epoch 105, training loss: 14511.46, average training loss: 14188.70, base loss: 14915.49
[INFO 2017-06-28 12:26:30,021 main.py:51] epoch 106, training loss: 12635.38, average training loss: 14174.18, base loss: 14912.46
[INFO 2017-06-28 12:26:30,395 main.py:51] epoch 107, training loss: 15167.65, average training loss: 14183.38, base loss: 14939.14
[INFO 2017-06-28 12:26:30,759 main.py:51] epoch 108, training loss: 12143.62, average training loss: 14164.67, base loss: 14933.65
[INFO 2017-06-28 12:26:31,111 main.py:51] epoch 109, training loss: 13303.24, average training loss: 14156.84, base loss: 14941.72
[INFO 2017-06-28 12:26:31,473 main.py:51] epoch 110, training loss: 13153.75, average training loss: 14147.80, base loss: 14945.80
[INFO 2017-06-28 12:26:31,830 main.py:51] epoch 111, training loss: 14499.40, average training loss: 14150.94, base loss: 14951.16
[INFO 2017-06-28 12:26:32,191 main.py:51] epoch 112, training loss: 11135.91, average training loss: 14124.26, base loss: 14926.28
[INFO 2017-06-28 12:26:32,562 main.py:51] epoch 113, training loss: 10735.97, average training loss: 14094.54, base loss: 14900.34
[INFO 2017-06-28 12:26:32,941 main.py:51] epoch 114, training loss: 13665.13, average training loss: 14090.80, base loss: 14908.74
[INFO 2017-06-28 12:26:33,304 main.py:51] epoch 115, training loss: 11061.88, average training loss: 14064.69, base loss: 14888.56
[INFO 2017-06-28 12:26:33,682 main.py:51] epoch 116, training loss: 11206.09, average training loss: 14040.26, base loss: 14877.99
[INFO 2017-06-28 12:26:34,063 main.py:51] epoch 117, training loss: 12947.90, average training loss: 14031.00, base loss: 14881.32
[INFO 2017-06-28 12:26:34,440 main.py:51] epoch 118, training loss: 12362.19, average training loss: 14016.98, base loss: 14884.38
[INFO 2017-06-28 12:26:34,808 main.py:51] epoch 119, training loss: 11769.88, average training loss: 13998.25, base loss: 14878.52
[INFO 2017-06-28 12:26:35,184 main.py:51] epoch 120, training loss: 13176.92, average training loss: 13991.46, base loss: 14881.88
[INFO 2017-06-28 12:26:35,555 main.py:51] epoch 121, training loss: 12141.28, average training loss: 13976.30, base loss: 14883.82
[INFO 2017-06-28 12:26:35,930 main.py:51] epoch 122, training loss: 13746.71, average training loss: 13974.43, base loss: 14897.24
[INFO 2017-06-28 12:26:36,296 main.py:51] epoch 123, training loss: 12545.44, average training loss: 13962.91, base loss: 14900.53
[INFO 2017-06-28 12:26:36,693 main.py:51] epoch 124, training loss: 12073.80, average training loss: 13947.80, base loss: 14897.76
[INFO 2017-06-28 12:26:37,046 main.py:51] epoch 125, training loss: 11632.44, average training loss: 13929.42, base loss: 14887.31
[INFO 2017-06-28 12:26:37,398 main.py:51] epoch 126, training loss: 12503.83, average training loss: 13918.19, base loss: 14883.75
[INFO 2017-06-28 12:26:37,797 main.py:51] epoch 127, training loss: 11709.95, average training loss: 13900.94, base loss: 14873.91
[INFO 2017-06-28 12:26:38,150 main.py:51] epoch 128, training loss: 13565.56, average training loss: 13898.34, base loss: 14888.27
[INFO 2017-06-28 12:26:38,520 main.py:51] epoch 129, training loss: 13047.23, average training loss: 13891.80, base loss: 14891.52
[INFO 2017-06-28 12:26:38,893 main.py:51] epoch 130, training loss: 12121.72, average training loss: 13878.28, base loss: 14885.30
[INFO 2017-06-28 12:26:39,262 main.py:51] epoch 131, training loss: 12803.09, average training loss: 13870.14, base loss: 14888.35
[INFO 2017-06-28 12:26:39,618 main.py:51] epoch 132, training loss: 10671.41, average training loss: 13846.09, base loss: 14870.47
[INFO 2017-06-28 12:26:40,014 main.py:51] epoch 133, training loss: 10683.21, average training loss: 13822.48, base loss: 14853.29
[INFO 2017-06-28 12:26:40,390 main.py:51] epoch 134, training loss: 12032.40, average training loss: 13809.22, base loss: 14849.65
[INFO 2017-06-28 12:26:40,743 main.py:51] epoch 135, training loss: 13026.37, average training loss: 13803.47, base loss: 14857.15
[INFO 2017-06-28 12:26:41,099 main.py:51] epoch 136, training loss: 11579.82, average training loss: 13787.24, base loss: 14853.77
[INFO 2017-06-28 12:26:41,501 main.py:51] epoch 137, training loss: 10965.36, average training loss: 13766.79, base loss: 14839.33
[INFO 2017-06-28 12:26:41,892 main.py:51] epoch 138, training loss: 11668.08, average training loss: 13751.69, base loss: 14834.61
[INFO 2017-06-28 12:26:42,264 main.py:51] epoch 139, training loss: 10927.88, average training loss: 13731.52, base loss: 14820.07
[INFO 2017-06-28 12:26:42,643 main.py:51] epoch 140, training loss: 13084.65, average training loss: 13726.93, base loss: 14831.41
[INFO 2017-06-28 12:26:43,007 main.py:51] epoch 141, training loss: 11823.53, average training loss: 13713.53, base loss: 14826.47
[INFO 2017-06-28 12:26:43,371 main.py:51] epoch 142, training loss: 12939.20, average training loss: 13708.11, base loss: 14834.31
[INFO 2017-06-28 12:26:43,742 main.py:51] epoch 143, training loss: 12664.78, average training loss: 13700.87, base loss: 14839.61
[INFO 2017-06-28 12:26:44,129 main.py:51] epoch 144, training loss: 11384.89, average training loss: 13684.90, base loss: 14829.40
[INFO 2017-06-28 12:26:44,508 main.py:51] epoch 145, training loss: 12546.49, average training loss: 13677.10, base loss: 14833.89
[INFO 2017-06-28 12:26:44,887 main.py:51] epoch 146, training loss: 12409.81, average training loss: 13668.48, base loss: 14844.30
[INFO 2017-06-28 12:26:45,259 main.py:51] epoch 147, training loss: 11569.95, average training loss: 13654.30, base loss: 14840.12
[INFO 2017-06-28 12:26:45,630 main.py:51] epoch 148, training loss: 11473.19, average training loss: 13639.66, base loss: 14834.81
[INFO 2017-06-28 12:26:46,040 main.py:51] epoch 149, training loss: 10285.12, average training loss: 13617.30, base loss: 14813.29
[INFO 2017-06-28 12:26:46,413 main.py:51] epoch 150, training loss: 11038.93, average training loss: 13600.22, base loss: 14803.94
[INFO 2017-06-28 12:26:46,807 main.py:51] epoch 151, training loss: 11706.04, average training loss: 13587.76, base loss: 14800.90
[INFO 2017-06-28 12:26:47,163 main.py:51] epoch 152, training loss: 13126.98, average training loss: 13584.75, base loss: 14809.89
[INFO 2017-06-28 12:26:47,523 main.py:51] epoch 153, training loss: 11114.62, average training loss: 13568.71, base loss: 14803.49
[INFO 2017-06-28 12:26:47,916 main.py:51] epoch 154, training loss: 13183.19, average training loss: 13566.22, base loss: 14808.57
[INFO 2017-06-28 12:26:48,269 main.py:51] epoch 155, training loss: 14505.41, average training loss: 13572.24, base loss: 14825.76
[INFO 2017-06-28 12:26:48,635 main.py:51] epoch 156, training loss: 12642.25, average training loss: 13566.32, base loss: 14827.83
[INFO 2017-06-28 12:26:48,990 main.py:51] epoch 157, training loss: 12110.44, average training loss: 13557.10, base loss: 14825.50
[INFO 2017-06-28 12:26:49,371 main.py:51] epoch 158, training loss: 12246.81, average training loss: 13548.86, base loss: 14827.33
[INFO 2017-06-28 12:26:49,766 main.py:51] epoch 159, training loss: 11480.89, average training loss: 13535.94, base loss: 14820.90
[INFO 2017-06-28 12:26:50,134 main.py:51] epoch 160, training loss: 12075.64, average training loss: 13526.87, base loss: 14822.10
[INFO 2017-06-28 12:26:50,497 main.py:51] epoch 161, training loss: 13361.82, average training loss: 13525.85, base loss: 14828.03
[INFO 2017-06-28 12:26:50,860 main.py:51] epoch 162, training loss: 10938.80, average training loss: 13509.98, base loss: 14814.48
[INFO 2017-06-28 12:26:51,200 main.py:51] epoch 163, training loss: 11679.04, average training loss: 13498.81, base loss: 14813.15
[INFO 2017-06-28 12:26:51,577 main.py:51] epoch 164, training loss: 11095.37, average training loss: 13484.25, base loss: 14805.54
[INFO 2017-06-28 12:26:51,964 main.py:51] epoch 165, training loss: 12408.65, average training loss: 13477.77, base loss: 14807.56
[INFO 2017-06-28 12:26:52,330 main.py:51] epoch 166, training loss: 12917.35, average training loss: 13474.41, base loss: 14812.36
[INFO 2017-06-28 12:26:52,695 main.py:51] epoch 167, training loss: 12446.74, average training loss: 13468.29, base loss: 14812.85
[INFO 2017-06-28 12:26:53,058 main.py:51] epoch 168, training loss: 11796.27, average training loss: 13458.40, base loss: 14812.02
[INFO 2017-06-28 12:26:53,435 main.py:51] epoch 169, training loss: 10670.34, average training loss: 13442.00, base loss: 14803.41
[INFO 2017-06-28 12:26:53,803 main.py:51] epoch 170, training loss: 13147.34, average training loss: 13440.28, base loss: 14804.50
[INFO 2017-06-28 12:26:54,169 main.py:51] epoch 171, training loss: 12401.84, average training loss: 13434.24, base loss: 14813.54
[INFO 2017-06-28 12:26:54,550 main.py:51] epoch 172, training loss: 12201.53, average training loss: 13427.11, base loss: 14815.31
[INFO 2017-06-28 12:26:54,935 main.py:51] epoch 173, training loss: 11926.55, average training loss: 13418.49, base loss: 14816.11
[INFO 2017-06-28 12:26:55,316 main.py:51] epoch 174, training loss: 11539.21, average training loss: 13407.75, base loss: 14813.45
[INFO 2017-06-28 12:26:55,675 main.py:51] epoch 175, training loss: 10520.47, average training loss: 13391.35, base loss: 14801.31
[INFO 2017-06-28 12:26:56,042 main.py:51] epoch 176, training loss: 11723.55, average training loss: 13381.92, base loss: 14797.73
[INFO 2017-06-28 12:26:56,428 main.py:51] epoch 177, training loss: 12614.63, average training loss: 13377.61, base loss: 14803.44
[INFO 2017-06-28 12:26:56,818 main.py:51] epoch 178, training loss: 10605.56, average training loss: 13362.13, base loss: 14792.37
[INFO 2017-06-28 12:26:57,181 main.py:51] epoch 179, training loss: 12174.36, average training loss: 13355.53, base loss: 14789.81
[INFO 2017-06-28 12:26:57,580 main.py:51] epoch 180, training loss: 13521.70, average training loss: 13356.45, base loss: 14800.05
[INFO 2017-06-28 12:26:57,939 main.py:51] epoch 181, training loss: 11333.59, average training loss: 13345.33, base loss: 14800.61
[INFO 2017-06-28 12:26:58,337 main.py:51] epoch 182, training loss: 11852.12, average training loss: 13337.17, base loss: 14801.49
[INFO 2017-06-28 12:26:58,697 main.py:51] epoch 183, training loss: 11406.31, average training loss: 13326.68, base loss: 14795.51
[INFO 2017-06-28 12:26:59,065 main.py:51] epoch 184, training loss: 11418.09, average training loss: 13316.36, base loss: 14790.73
[INFO 2017-06-28 12:26:59,435 main.py:51] epoch 185, training loss: 10561.65, average training loss: 13301.55, base loss: 14783.62
[INFO 2017-06-28 12:26:59,813 main.py:51] epoch 186, training loss: 11419.67, average training loss: 13291.49, base loss: 14781.96
[INFO 2017-06-28 12:27:00,167 main.py:51] epoch 187, training loss: 10785.01, average training loss: 13278.16, base loss: 14774.34
[INFO 2017-06-28 12:27:00,532 main.py:51] epoch 188, training loss: 11224.25, average training loss: 13267.29, base loss: 14773.23
[INFO 2017-06-28 12:27:00,889 main.py:51] epoch 189, training loss: 11772.86, average training loss: 13259.42, base loss: 14774.19
[INFO 2017-06-28 12:27:01,266 main.py:51] epoch 190, training loss: 9750.54, average training loss: 13241.05, base loss: 14758.96
[INFO 2017-06-28 12:27:01,642 main.py:51] epoch 191, training loss: 10356.00, average training loss: 13226.03, base loss: 14746.61
[INFO 2017-06-28 12:27:02,020 main.py:51] epoch 192, training loss: 12385.74, average training loss: 13221.67, base loss: 14752.55
[INFO 2017-06-28 12:27:02,382 main.py:51] epoch 193, training loss: 11036.68, average training loss: 13210.41, base loss: 14747.26
[INFO 2017-06-28 12:27:02,742 main.py:51] epoch 194, training loss: 11542.74, average training loss: 13201.86, base loss: 14744.57
[INFO 2017-06-28 12:27:03,143 main.py:51] epoch 195, training loss: 10467.24, average training loss: 13187.90, base loss: 14737.91
[INFO 2017-06-28 12:27:03,516 main.py:51] epoch 196, training loss: 11788.57, average training loss: 13180.80, base loss: 14734.83
[INFO 2017-06-28 12:27:03,873 main.py:51] epoch 197, training loss: 9471.19, average training loss: 13162.07, base loss: 14719.64
[INFO 2017-06-28 12:27:04,242 main.py:51] epoch 198, training loss: 12580.18, average training loss: 13159.14, base loss: 14722.53
[INFO 2017-06-28 12:27:04,615 main.py:51] epoch 199, training loss: 12555.67, average training loss: 13156.12, base loss: 14727.87
[INFO 2017-06-28 12:27:04,615 main.py:53] epoch 199, testing
[INFO 2017-06-28 12:27:06,415 main.py:105] average testing loss: 13080.04, base loss: 15820.20
[INFO 2017-06-28 12:27:06,415 main.py:106] improve_loss: 2740.16, improve_percent: 0.17
[INFO 2017-06-28 12:27:06,415 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:27:06,428 main.py:76] current best improved percent: 0.17
[INFO 2017-06-28 12:27:06,787 main.py:51] epoch 200, training loss: 10377.17, average training loss: 13142.30, base loss: 14724.17
[INFO 2017-06-28 12:27:07,145 main.py:51] epoch 201, training loss: 12321.77, average training loss: 13138.24, base loss: 14727.10
[INFO 2017-06-28 12:27:07,511 main.py:51] epoch 202, training loss: 10631.67, average training loss: 13125.89, base loss: 14719.43
[INFO 2017-06-28 12:27:07,873 main.py:51] epoch 203, training loss: 11130.87, average training loss: 13116.11, base loss: 14712.76
[INFO 2017-06-28 12:27:08,266 main.py:51] epoch 204, training loss: 10694.49, average training loss: 13104.30, base loss: 14708.28
[INFO 2017-06-28 12:27:08,615 main.py:51] epoch 205, training loss: 10256.12, average training loss: 13090.47, base loss: 14697.30
[INFO 2017-06-28 12:27:08,990 main.py:51] epoch 206, training loss: 9553.47, average training loss: 13073.38, base loss: 14686.83
[INFO 2017-06-28 12:27:09,373 main.py:51] epoch 207, training loss: 11452.42, average training loss: 13065.59, base loss: 14682.87
[INFO 2017-06-28 12:27:09,722 main.py:51] epoch 208, training loss: 10776.61, average training loss: 13054.64, base loss: 14678.34
[INFO 2017-06-28 12:27:10,079 main.py:51] epoch 209, training loss: 12235.97, average training loss: 13050.74, base loss: 14686.07
[INFO 2017-06-28 12:27:10,456 main.py:51] epoch 210, training loss: 10194.47, average training loss: 13037.20, base loss: 14675.70
[INFO 2017-06-28 12:27:10,824 main.py:51] epoch 211, training loss: 10238.92, average training loss: 13024.00, base loss: 14665.98
[INFO 2017-06-28 12:27:11,176 main.py:51] epoch 212, training loss: 10720.58, average training loss: 13013.19, base loss: 14662.43
[INFO 2017-06-28 12:27:11,528 main.py:51] epoch 213, training loss: 11828.39, average training loss: 13007.65, base loss: 14663.73
[INFO 2017-06-28 12:27:11,887 main.py:51] epoch 214, training loss: 9925.22, average training loss: 12993.32, base loss: 14652.39
[INFO 2017-06-28 12:27:12,257 main.py:51] epoch 215, training loss: 11342.92, average training loss: 12985.68, base loss: 14647.07
[INFO 2017-06-28 12:27:12,630 main.py:51] epoch 216, training loss: 10823.93, average training loss: 12975.71, base loss: 14641.42
[INFO 2017-06-28 12:27:12,967 main.py:51] epoch 217, training loss: 14126.80, average training loss: 12980.99, base loss: 14652.47
[INFO 2017-06-28 12:27:13,347 main.py:51] epoch 218, training loss: 10364.59, average training loss: 12969.05, base loss: 14643.03
[INFO 2017-06-28 12:27:13,726 main.py:51] epoch 219, training loss: 10252.03, average training loss: 12956.70, base loss: 14635.93
[INFO 2017-06-28 12:27:14,122 main.py:51] epoch 220, training loss: 11927.92, average training loss: 12952.04, base loss: 14641.76
[INFO 2017-06-28 12:27:14,500 main.py:51] epoch 221, training loss: 9839.18, average training loss: 12938.02, base loss: 14631.36
[INFO 2017-06-28 12:27:14,882 main.py:51] epoch 222, training loss: 10194.18, average training loss: 12925.72, base loss: 14621.65
[INFO 2017-06-28 12:27:15,246 main.py:51] epoch 223, training loss: 10989.98, average training loss: 12917.07, base loss: 14620.96
[INFO 2017-06-28 12:27:15,599 main.py:51] epoch 224, training loss: 11885.35, average training loss: 12912.49, base loss: 14622.38
[INFO 2017-06-28 12:27:15,972 main.py:51] epoch 225, training loss: 10962.36, average training loss: 12903.86, base loss: 14614.83
[INFO 2017-06-28 12:27:16,344 main.py:51] epoch 226, training loss: 12974.74, average training loss: 12904.17, base loss: 14623.84
[INFO 2017-06-28 12:27:16,734 main.py:51] epoch 227, training loss: 11793.95, average training loss: 12899.30, base loss: 14626.41
[INFO 2017-06-28 12:27:17,113 main.py:51] epoch 228, training loss: 12999.86, average training loss: 12899.74, base loss: 14634.73
[INFO 2017-06-28 12:27:17,503 main.py:51] epoch 229, training loss: 10847.62, average training loss: 12890.82, base loss: 14631.74
[INFO 2017-06-28 12:27:17,860 main.py:51] epoch 230, training loss: 12490.39, average training loss: 12889.09, base loss: 14635.36
[INFO 2017-06-28 12:27:18,227 main.py:51] epoch 231, training loss: 11254.33, average training loss: 12882.04, base loss: 14631.83
[INFO 2017-06-28 12:27:18,617 main.py:51] epoch 232, training loss: 11287.93, average training loss: 12875.20, base loss: 14630.59
[INFO 2017-06-28 12:27:18,992 main.py:51] epoch 233, training loss: 11133.70, average training loss: 12867.76, base loss: 14626.87
[INFO 2017-06-28 12:27:19,382 main.py:51] epoch 234, training loss: 10821.94, average training loss: 12859.05, base loss: 14621.80
[INFO 2017-06-28 12:27:19,745 main.py:51] epoch 235, training loss: 9846.85, average training loss: 12846.29, base loss: 14612.99
[INFO 2017-06-28 12:27:20,113 main.py:51] epoch 236, training loss: 10713.69, average training loss: 12837.29, base loss: 14607.97
[INFO 2017-06-28 12:27:20,472 main.py:51] epoch 237, training loss: 9438.49, average training loss: 12823.01, base loss: 14596.69
[INFO 2017-06-28 12:27:20,839 main.py:51] epoch 238, training loss: 12920.20, average training loss: 12823.41, base loss: 14603.36
[INFO 2017-06-28 12:27:21,232 main.py:51] epoch 239, training loss: 11699.68, average training loss: 12818.73, base loss: 14604.54
[INFO 2017-06-28 12:27:21,597 main.py:51] epoch 240, training loss: 11742.81, average training loss: 12814.27, base loss: 14606.11
[INFO 2017-06-28 12:27:21,967 main.py:51] epoch 241, training loss: 9543.87, average training loss: 12800.75, base loss: 14594.76
[INFO 2017-06-28 12:27:22,345 main.py:51] epoch 242, training loss: 13601.70, average training loss: 12804.05, base loss: 14608.94
[INFO 2017-06-28 12:27:22,728 main.py:51] epoch 243, training loss: 12665.80, average training loss: 12803.48, base loss: 14614.28
[INFO 2017-06-28 12:27:23,122 main.py:51] epoch 244, training loss: 11226.98, average training loss: 12797.05, base loss: 14612.19
[INFO 2017-06-28 12:27:23,509 main.py:51] epoch 245, training loss: 11099.71, average training loss: 12790.15, base loss: 14612.12
[INFO 2017-06-28 12:27:23,904 main.py:51] epoch 246, training loss: 11378.27, average training loss: 12784.43, base loss: 14607.94
[INFO 2017-06-28 12:27:24,301 main.py:51] epoch 247, training loss: 11796.01, average training loss: 12780.45, base loss: 14607.53
[INFO 2017-06-28 12:27:24,698 main.py:51] epoch 248, training loss: 9865.86, average training loss: 12768.74, base loss: 14599.20
[INFO 2017-06-28 12:27:25,071 main.py:51] epoch 249, training loss: 10244.01, average training loss: 12758.64, base loss: 14595.71
[INFO 2017-06-28 12:27:25,445 main.py:51] epoch 250, training loss: 11545.88, average training loss: 12753.81, base loss: 14598.58
[INFO 2017-06-28 12:27:25,817 main.py:51] epoch 251, training loss: 10452.10, average training loss: 12744.68, base loss: 14592.91
[INFO 2017-06-28 12:27:26,202 main.py:51] epoch 252, training loss: 10685.84, average training loss: 12736.54, base loss: 14585.87
[INFO 2017-06-28 12:27:26,592 main.py:51] epoch 253, training loss: 10817.57, average training loss: 12728.98, base loss: 14585.16
[INFO 2017-06-28 12:27:26,977 main.py:51] epoch 254, training loss: 10787.96, average training loss: 12721.37, base loss: 14581.72
[INFO 2017-06-28 12:27:27,336 main.py:51] epoch 255, training loss: 10196.73, average training loss: 12711.51, base loss: 14573.12
[INFO 2017-06-28 12:27:27,718 main.py:51] epoch 256, training loss: 12778.88, average training loss: 12711.77, base loss: 14580.36
[INFO 2017-06-28 12:27:28,098 main.py:51] epoch 257, training loss: 10851.59, average training loss: 12704.56, base loss: 14576.69
[INFO 2017-06-28 12:27:28,460 main.py:51] epoch 258, training loss: 11386.27, average training loss: 12699.47, base loss: 14576.83
[INFO 2017-06-28 12:27:28,816 main.py:51] epoch 259, training loss: 9145.71, average training loss: 12685.80, base loss: 14563.14
[INFO 2017-06-28 12:27:29,188 main.py:51] epoch 260, training loss: 11476.01, average training loss: 12681.17, base loss: 14563.08
[INFO 2017-06-28 12:27:29,585 main.py:51] epoch 261, training loss: 10970.47, average training loss: 12674.64, base loss: 14558.48
[INFO 2017-06-28 12:27:29,964 main.py:51] epoch 262, training loss: 11036.13, average training loss: 12668.41, base loss: 14553.54
[INFO 2017-06-28 12:27:30,356 main.py:51] epoch 263, training loss: 10774.26, average training loss: 12661.23, base loss: 14548.62
[INFO 2017-06-28 12:27:30,735 main.py:51] epoch 264, training loss: 11460.11, average training loss: 12656.70, base loss: 14550.53
[INFO 2017-06-28 12:27:31,122 main.py:51] epoch 265, training loss: 12005.92, average training loss: 12654.26, base loss: 14551.87
[INFO 2017-06-28 12:27:31,499 main.py:51] epoch 266, training loss: 10140.89, average training loss: 12644.84, base loss: 14546.29
[INFO 2017-06-28 12:27:31,897 main.py:51] epoch 267, training loss: 9465.53, average training loss: 12632.98, base loss: 14534.99
[INFO 2017-06-28 12:27:32,275 main.py:51] epoch 268, training loss: 11036.32, average training loss: 12627.04, base loss: 14531.70
[INFO 2017-06-28 12:27:32,652 main.py:51] epoch 269, training loss: 11230.08, average training loss: 12621.87, base loss: 14530.44
[INFO 2017-06-28 12:27:33,025 main.py:51] epoch 270, training loss: 12179.44, average training loss: 12620.24, base loss: 14533.51
[INFO 2017-06-28 12:27:33,411 main.py:51] epoch 271, training loss: 10753.09, average training loss: 12613.37, base loss: 14533.18
[INFO 2017-06-28 12:27:33,781 main.py:51] epoch 272, training loss: 11984.58, average training loss: 12611.07, base loss: 14534.44
[INFO 2017-06-28 12:27:34,174 main.py:51] epoch 273, training loss: 9875.93, average training loss: 12601.09, base loss: 14527.13
[INFO 2017-06-28 12:27:34,568 main.py:51] epoch 274, training loss: 9692.09, average training loss: 12590.51, base loss: 14519.23
[INFO 2017-06-28 12:27:34,933 main.py:51] epoch 275, training loss: 12983.44, average training loss: 12591.93, base loss: 14526.19
[INFO 2017-06-28 12:27:35,328 main.py:51] epoch 276, training loss: 12257.71, average training loss: 12590.73, base loss: 14529.60
[INFO 2017-06-28 12:27:35,715 main.py:51] epoch 277, training loss: 11581.60, average training loss: 12587.10, base loss: 14529.27
[INFO 2017-06-28 12:27:36,102 main.py:51] epoch 278, training loss: 11823.38, average training loss: 12584.36, base loss: 14534.44
[INFO 2017-06-28 12:27:36,475 main.py:51] epoch 279, training loss: 11702.94, average training loss: 12581.21, base loss: 14533.20
[INFO 2017-06-28 12:27:36,869 main.py:51] epoch 280, training loss: 13344.99, average training loss: 12583.93, base loss: 14541.79
[INFO 2017-06-28 12:27:37,250 main.py:51] epoch 281, training loss: 10742.18, average training loss: 12577.40, base loss: 14537.44
[INFO 2017-06-28 12:27:37,643 main.py:51] epoch 282, training loss: 11914.79, average training loss: 12575.06, base loss: 14538.10
[INFO 2017-06-28 12:27:38,027 main.py:51] epoch 283, training loss: 12729.14, average training loss: 12575.60, base loss: 14543.12
[INFO 2017-06-28 12:27:38,436 main.py:51] epoch 284, training loss: 9868.48, average training loss: 12566.10, base loss: 14535.83
[INFO 2017-06-28 12:27:38,819 main.py:51] epoch 285, training loss: 9262.87, average training loss: 12554.55, base loss: 14525.69
[INFO 2017-06-28 12:27:39,184 main.py:51] epoch 286, training loss: 11215.10, average training loss: 12549.88, base loss: 14524.93
[INFO 2017-06-28 12:27:39,555 main.py:51] epoch 287, training loss: 10164.38, average training loss: 12541.60, base loss: 14521.12
[INFO 2017-06-28 12:27:39,953 main.py:51] epoch 288, training loss: 10287.45, average training loss: 12533.80, base loss: 14515.92
[INFO 2017-06-28 12:27:40,326 main.py:51] epoch 289, training loss: 10547.23, average training loss: 12526.95, base loss: 14513.37
[INFO 2017-06-28 12:27:40,708 main.py:51] epoch 290, training loss: 12274.39, average training loss: 12526.08, base loss: 14515.80
[INFO 2017-06-28 12:27:41,084 main.py:51] epoch 291, training loss: 10638.21, average training loss: 12519.62, base loss: 14510.80
[INFO 2017-06-28 12:27:41,506 main.py:51] epoch 292, training loss: 11831.75, average training loss: 12517.27, base loss: 14514.07
[INFO 2017-06-28 12:27:41,907 main.py:51] epoch 293, training loss: 10486.87, average training loss: 12510.36, base loss: 14510.46
[INFO 2017-06-28 12:27:42,310 main.py:51] epoch 294, training loss: 12202.66, average training loss: 12509.32, base loss: 14513.61
[INFO 2017-06-28 12:27:42,697 main.py:51] epoch 295, training loss: 11675.50, average training loss: 12506.50, base loss: 14514.29
[INFO 2017-06-28 12:27:43,080 main.py:51] epoch 296, training loss: 10116.49, average training loss: 12498.46, base loss: 14509.35
[INFO 2017-06-28 12:27:43,465 main.py:51] epoch 297, training loss: 9865.11, average training loss: 12489.62, base loss: 14504.74
[INFO 2017-06-28 12:27:43,841 main.py:51] epoch 298, training loss: 10267.40, average training loss: 12482.19, base loss: 14496.33
[INFO 2017-06-28 12:27:44,234 main.py:51] epoch 299, training loss: 10710.84, average training loss: 12476.28, base loss: 14493.59
[INFO 2017-06-28 12:27:44,234 main.py:53] epoch 299, testing
[INFO 2017-06-28 12:27:46,095 main.py:105] average testing loss: 12255.26, base loss: 15111.82
[INFO 2017-06-28 12:27:46,095 main.py:106] improve_loss: 2856.56, improve_percent: 0.19
[INFO 2017-06-28 12:27:46,095 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:27:46,107 main.py:76] current best improved percent: 0.19
[INFO 2017-06-28 12:27:46,477 main.py:51] epoch 300, training loss: 10793.11, average training loss: 12470.69, base loss: 14488.26
[INFO 2017-06-28 12:27:46,869 main.py:51] epoch 301, training loss: 11241.17, average training loss: 12466.62, base loss: 14488.72
[INFO 2017-06-28 12:27:47,268 main.py:51] epoch 302, training loss: 13106.48, average training loss: 12468.73, base loss: 14495.20
[INFO 2017-06-28 12:27:47,657 main.py:51] epoch 303, training loss: 10758.82, average training loss: 12463.11, base loss: 14493.67
[INFO 2017-06-28 12:27:48,034 main.py:51] epoch 304, training loss: 12654.62, average training loss: 12463.73, base loss: 14498.82
[INFO 2017-06-28 12:27:48,455 main.py:51] epoch 305, training loss: 10294.22, average training loss: 12456.64, base loss: 14496.05
[INFO 2017-06-28 12:27:48,841 main.py:51] epoch 306, training loss: 12837.01, average training loss: 12457.88, base loss: 14501.01
[INFO 2017-06-28 12:27:49,250 main.py:51] epoch 307, training loss: 11583.78, average training loss: 12455.05, base loss: 14503.35
[INFO 2017-06-28 12:27:49,638 main.py:51] epoch 308, training loss: 10891.96, average training loss: 12449.99, base loss: 14500.41
[INFO 2017-06-28 12:27:50,010 main.py:51] epoch 309, training loss: 12363.38, average training loss: 12449.71, base loss: 14504.18
[INFO 2017-06-28 12:27:50,408 main.py:51] epoch 310, training loss: 13899.96, average training loss: 12454.37, base loss: 14516.29
[INFO 2017-06-28 12:27:50,789 main.py:51] epoch 311, training loss: 12148.96, average training loss: 12453.39, base loss: 14520.93
[INFO 2017-06-28 12:27:51,175 main.py:51] epoch 312, training loss: 10835.47, average training loss: 12448.22, base loss: 14517.92
[INFO 2017-06-28 12:27:51,557 main.py:51] epoch 313, training loss: 11441.57, average training loss: 12445.02, base loss: 14519.55
[INFO 2017-06-28 12:27:51,934 main.py:51] epoch 314, training loss: 12136.46, average training loss: 12444.04, base loss: 14524.02
[INFO 2017-06-28 12:27:52,299 main.py:51] epoch 315, training loss: 10716.12, average training loss: 12438.57, base loss: 14522.02
[INFO 2017-06-28 12:27:52,683 main.py:51] epoch 316, training loss: 12641.79, average training loss: 12439.21, base loss: 14528.16
[INFO 2017-06-28 12:27:53,083 main.py:51] epoch 317, training loss: 10416.58, average training loss: 12432.85, base loss: 14526.76
[INFO 2017-06-28 12:27:53,461 main.py:51] epoch 318, training loss: 10487.65, average training loss: 12426.75, base loss: 14523.43
[INFO 2017-06-28 12:27:53,857 main.py:51] epoch 319, training loss: 10668.05, average training loss: 12421.26, base loss: 14519.05
[INFO 2017-06-28 12:27:54,239 main.py:51] epoch 320, training loss: 10348.87, average training loss: 12414.80, base loss: 14515.64
[INFO 2017-06-28 12:27:54,608 main.py:51] epoch 321, training loss: 9823.70, average training loss: 12406.75, base loss: 14510.99
[INFO 2017-06-28 12:27:54,978 main.py:51] epoch 322, training loss: 11160.66, average training loss: 12402.90, base loss: 14507.81
[INFO 2017-06-28 12:27:55,357 main.py:51] epoch 323, training loss: 10023.60, average training loss: 12395.55, base loss: 14504.12
[INFO 2017-06-28 12:27:55,726 main.py:51] epoch 324, training loss: 11198.90, average training loss: 12391.87, base loss: 14506.21
[INFO 2017-06-28 12:27:56,111 main.py:51] epoch 325, training loss: 11446.26, average training loss: 12388.97, base loss: 14507.04
[INFO 2017-06-28 12:27:56,518 main.py:51] epoch 326, training loss: 9082.54, average training loss: 12378.86, base loss: 14495.63
[INFO 2017-06-28 12:27:56,895 main.py:51] epoch 327, training loss: 12686.75, average training loss: 12379.80, base loss: 14500.95
[INFO 2017-06-28 12:27:57,277 main.py:51] epoch 328, training loss: 10804.99, average training loss: 12375.01, base loss: 14498.50
[INFO 2017-06-28 12:27:57,653 main.py:51] epoch 329, training loss: 12675.02, average training loss: 12375.92, base loss: 14503.81
[INFO 2017-06-28 12:27:58,041 main.py:51] epoch 330, training loss: 10114.82, average training loss: 12369.09, base loss: 14501.16
[INFO 2017-06-28 12:27:58,430 main.py:51] epoch 331, training loss: 11676.11, average training loss: 12367.00, base loss: 14504.19
[INFO 2017-06-28 12:27:58,820 main.py:51] epoch 332, training loss: 12083.09, average training loss: 12366.15, base loss: 14508.63
[INFO 2017-06-28 12:27:59,227 main.py:51] epoch 333, training loss: 10116.79, average training loss: 12359.41, base loss: 14503.73
[INFO 2017-06-28 12:27:59,606 main.py:51] epoch 334, training loss: 12056.60, average training loss: 12358.51, base loss: 14508.87
[INFO 2017-06-28 12:27:59,983 main.py:51] epoch 335, training loss: 9668.70, average training loss: 12350.50, base loss: 14501.54
[INFO 2017-06-28 12:28:00,372 main.py:51] epoch 336, training loss: 12378.36, average training loss: 12350.59, base loss: 14509.85
[INFO 2017-06-28 12:28:00,767 main.py:51] epoch 337, training loss: 10354.89, average training loss: 12344.68, base loss: 14504.99
[INFO 2017-06-28 12:28:01,144 main.py:51] epoch 338, training loss: 10854.28, average training loss: 12340.29, base loss: 14504.44
[INFO 2017-06-28 12:28:01,521 main.py:51] epoch 339, training loss: 12132.88, average training loss: 12339.68, base loss: 14510.40
[INFO 2017-06-28 12:28:01,902 main.py:51] epoch 340, training loss: 11697.48, average training loss: 12337.79, base loss: 14510.39
[INFO 2017-06-28 12:28:02,283 main.py:51] epoch 341, training loss: 11105.72, average training loss: 12334.19, base loss: 14510.14
[INFO 2017-06-28 12:28:02,694 main.py:51] epoch 342, training loss: 10873.32, average training loss: 12329.93, base loss: 14507.81
[INFO 2017-06-28 12:28:03,095 main.py:51] epoch 343, training loss: 9512.77, average training loss: 12321.74, base loss: 14503.53
[INFO 2017-06-28 12:28:03,487 main.py:51] epoch 344, training loss: 10688.63, average training loss: 12317.01, base loss: 14503.67
[INFO 2017-06-28 12:28:03,885 main.py:51] epoch 345, training loss: 11045.57, average training loss: 12313.33, base loss: 14504.31
[INFO 2017-06-28 12:28:04,272 main.py:51] epoch 346, training loss: 11994.60, average training loss: 12312.41, base loss: 14507.34
[INFO 2017-06-28 12:28:04,661 main.py:51] epoch 347, training loss: 11174.39, average training loss: 12309.14, base loss: 14505.64
[INFO 2017-06-28 12:28:05,062 main.py:51] epoch 348, training loss: 11653.21, average training loss: 12307.27, base loss: 14507.52
[INFO 2017-06-28 12:28:05,485 main.py:51] epoch 349, training loss: 10869.62, average training loss: 12303.16, base loss: 14508.00
[INFO 2017-06-28 12:28:05,891 main.py:51] epoch 350, training loss: 12033.51, average training loss: 12302.39, base loss: 14510.67
[INFO 2017-06-28 12:28:06,308 main.py:51] epoch 351, training loss: 11613.05, average training loss: 12300.43, base loss: 14513.16
[INFO 2017-06-28 12:28:06,690 main.py:51] epoch 352, training loss: 11273.56, average training loss: 12297.52, base loss: 14515.22
[INFO 2017-06-28 12:28:07,102 main.py:51] epoch 353, training loss: 10023.32, average training loss: 12291.10, base loss: 14511.52
[INFO 2017-06-28 12:28:07,495 main.py:51] epoch 354, training loss: 11386.95, average training loss: 12288.55, base loss: 14514.89
[INFO 2017-06-28 12:28:07,867 main.py:51] epoch 355, training loss: 10854.52, average training loss: 12284.52, base loss: 14515.86
[INFO 2017-06-28 12:28:08,283 main.py:51] epoch 356, training loss: 9902.40, average training loss: 12277.85, base loss: 14511.87
[INFO 2017-06-28 12:28:08,713 main.py:51] epoch 357, training loss: 10187.84, average training loss: 12272.01, base loss: 14507.00
[INFO 2017-06-28 12:28:09,093 main.py:51] epoch 358, training loss: 10286.88, average training loss: 12266.48, base loss: 14503.63
[INFO 2017-06-28 12:28:09,502 main.py:51] epoch 359, training loss: 10509.25, average training loss: 12261.60, base loss: 14500.41
[INFO 2017-06-28 12:28:09,889 main.py:51] epoch 360, training loss: 10012.85, average training loss: 12255.37, base loss: 14495.52
[INFO 2017-06-28 12:28:10,283 main.py:51] epoch 361, training loss: 12726.80, average training loss: 12256.67, base loss: 14502.25
[INFO 2017-06-28 12:28:10,697 main.py:51] epoch 362, training loss: 9800.85, average training loss: 12249.91, base loss: 14496.67
[INFO 2017-06-28 12:28:11,088 main.py:51] epoch 363, training loss: 10809.07, average training loss: 12245.95, base loss: 14496.99
[INFO 2017-06-28 12:28:11,486 main.py:51] epoch 364, training loss: 12413.07, average training loss: 12246.41, base loss: 14499.08
[INFO 2017-06-28 12:28:11,892 main.py:51] epoch 365, training loss: 10324.01, average training loss: 12241.16, base loss: 14498.32
[INFO 2017-06-28 12:28:12,296 main.py:51] epoch 366, training loss: 9264.91, average training loss: 12233.05, base loss: 14493.15
[INFO 2017-06-28 12:28:12,682 main.py:51] epoch 367, training loss: 9025.58, average training loss: 12224.33, base loss: 14486.59
[INFO 2017-06-28 12:28:13,076 main.py:51] epoch 368, training loss: 10729.47, average training loss: 12220.28, base loss: 14482.63
[INFO 2017-06-28 12:28:13,465 main.py:51] epoch 369, training loss: 12228.26, average training loss: 12220.30, base loss: 14485.55
[INFO 2017-06-28 12:28:13,851 main.py:51] epoch 370, training loss: 9125.26, average training loss: 12211.96, base loss: 14478.02
[INFO 2017-06-28 12:28:14,242 main.py:51] epoch 371, training loss: 11165.92, average training loss: 12209.15, base loss: 14477.81
[INFO 2017-06-28 12:28:14,636 main.py:51] epoch 372, training loss: 10686.24, average training loss: 12205.06, base loss: 14476.18
[INFO 2017-06-28 12:28:15,039 main.py:51] epoch 373, training loss: 9909.54, average training loss: 12198.93, base loss: 14471.44
[INFO 2017-06-28 12:28:15,425 main.py:51] epoch 374, training loss: 9871.25, average training loss: 12192.72, base loss: 14467.94
[INFO 2017-06-28 12:28:15,809 main.py:51] epoch 375, training loss: 10499.15, average training loss: 12188.21, base loss: 14467.90
[INFO 2017-06-28 12:28:16,208 main.py:51] epoch 376, training loss: 10238.40, average training loss: 12183.04, base loss: 14466.79
[INFO 2017-06-28 12:28:16,595 main.py:51] epoch 377, training loss: 10409.42, average training loss: 12178.35, base loss: 14465.70
[INFO 2017-06-28 12:28:16,991 main.py:51] epoch 378, training loss: 11199.94, average training loss: 12175.77, base loss: 14467.04
[INFO 2017-06-28 12:28:17,397 main.py:51] epoch 379, training loss: 10695.40, average training loss: 12171.87, base loss: 14466.70
[INFO 2017-06-28 12:28:17,810 main.py:51] epoch 380, training loss: 12203.75, average training loss: 12171.96, base loss: 14470.95
[INFO 2017-06-28 12:28:18,198 main.py:51] epoch 381, training loss: 10340.39, average training loss: 12167.16, base loss: 14468.45
[INFO 2017-06-28 12:28:18,610 main.py:51] epoch 382, training loss: 15124.57, average training loss: 12174.88, base loss: 14481.66
[INFO 2017-06-28 12:28:19,043 main.py:51] epoch 383, training loss: 12100.92, average training loss: 12174.69, base loss: 14487.96
[INFO 2017-06-28 12:28:19,434 main.py:51] epoch 384, training loss: 11287.17, average training loss: 12172.39, base loss: 14488.90
[INFO 2017-06-28 12:28:19,846 main.py:51] epoch 385, training loss: 11770.62, average training loss: 12171.35, base loss: 14489.46
[INFO 2017-06-28 12:28:20,247 main.py:51] epoch 386, training loss: 10281.64, average training loss: 12166.46, base loss: 14487.85
[INFO 2017-06-28 12:28:20,643 main.py:51] epoch 387, training loss: 12677.23, average training loss: 12167.78, base loss: 14491.98
[INFO 2017-06-28 12:28:21,033 main.py:51] epoch 388, training loss: 10894.27, average training loss: 12164.50, base loss: 14493.60
[INFO 2017-06-28 12:28:21,440 main.py:51] epoch 389, training loss: 10875.66, average training loss: 12161.20, base loss: 14491.70
[INFO 2017-06-28 12:28:21,840 main.py:51] epoch 390, training loss: 10577.57, average training loss: 12157.15, base loss: 14490.76
[INFO 2017-06-28 12:28:22,244 main.py:51] epoch 391, training loss: 10578.95, average training loss: 12153.12, base loss: 14488.87
[INFO 2017-06-28 12:28:22,658 main.py:51] epoch 392, training loss: 11121.42, average training loss: 12150.50, base loss: 14489.45
[INFO 2017-06-28 12:28:23,064 main.py:51] epoch 393, training loss: 9831.93, average training loss: 12144.61, base loss: 14484.03
[INFO 2017-06-28 12:28:23,474 main.py:51] epoch 394, training loss: 11057.54, average training loss: 12141.86, base loss: 14485.03
[INFO 2017-06-28 12:28:23,905 main.py:51] epoch 395, training loss: 10597.72, average training loss: 12137.96, base loss: 14485.75
[INFO 2017-06-28 12:28:24,293 main.py:51] epoch 396, training loss: 12832.55, average training loss: 12139.71, base loss: 14493.33
[INFO 2017-06-28 12:28:24,681 main.py:51] epoch 397, training loss: 11204.09, average training loss: 12137.36, base loss: 14492.91
[INFO 2017-06-28 12:28:25,075 main.py:51] epoch 398, training loss: 10532.23, average training loss: 12133.34, base loss: 14491.29
[INFO 2017-06-28 12:28:25,476 main.py:51] epoch 399, training loss: 11211.38, average training loss: 12131.03, base loss: 14492.70
[INFO 2017-06-28 12:28:25,476 main.py:53] epoch 399, testing
[INFO 2017-06-28 12:28:27,314 main.py:105] average testing loss: 12160.79, base loss: 15388.10
[INFO 2017-06-28 12:28:27,314 main.py:106] improve_loss: 3227.31, improve_percent: 0.21
[INFO 2017-06-28 12:28:27,314 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:28:27,327 main.py:76] current best improved percent: 0.21
[INFO 2017-06-28 12:28:27,755 main.py:51] epoch 400, training loss: 9898.21, average training loss: 12125.47, base loss: 14489.45
[INFO 2017-06-28 12:28:28,146 main.py:51] epoch 401, training loss: 11609.28, average training loss: 12124.18, base loss: 14491.69
[INFO 2017-06-28 12:28:28,561 main.py:51] epoch 402, training loss: 10454.25, average training loss: 12120.04, base loss: 14489.76
[INFO 2017-06-28 12:28:28,966 main.py:51] epoch 403, training loss: 10659.19, average training loss: 12116.42, base loss: 14489.74
[INFO 2017-06-28 12:28:29,363 main.py:51] epoch 404, training loss: 12957.73, average training loss: 12118.50, base loss: 14497.31
[INFO 2017-06-28 12:28:29,769 main.py:51] epoch 405, training loss: 9384.45, average training loss: 12111.76, base loss: 14493.25
[INFO 2017-06-28 12:28:30,156 main.py:51] epoch 406, training loss: 12216.69, average training loss: 12112.02, base loss: 14500.33
[INFO 2017-06-28 12:28:30,588 main.py:51] epoch 407, training loss: 9898.46, average training loss: 12106.60, base loss: 14494.76
[INFO 2017-06-28 12:28:30,965 main.py:51] epoch 408, training loss: 11352.16, average training loss: 12104.75, base loss: 14496.12
[INFO 2017-06-28 12:28:31,329 main.py:51] epoch 409, training loss: 10353.37, average training loss: 12100.48, base loss: 14495.91
[INFO 2017-06-28 12:28:31,734 main.py:51] epoch 410, training loss: 10827.55, average training loss: 12097.38, base loss: 14495.41
[INFO 2017-06-28 12:28:32,127 main.py:51] epoch 411, training loss: 10658.46, average training loss: 12093.89, base loss: 14492.09
[INFO 2017-06-28 12:28:32,524 main.py:51] epoch 412, training loss: 9478.39, average training loss: 12087.56, base loss: 14487.85
[INFO 2017-06-28 12:28:32,915 main.py:51] epoch 413, training loss: 12157.55, average training loss: 12087.73, base loss: 14489.66
[INFO 2017-06-28 12:28:33,302 main.py:51] epoch 414, training loss: 12218.57, average training loss: 12088.04, base loss: 14492.61
[INFO 2017-06-28 12:28:33,683 main.py:51] epoch 415, training loss: 10431.46, average training loss: 12084.06, base loss: 14492.90
[INFO 2017-06-28 12:28:34,071 main.py:51] epoch 416, training loss: 11573.16, average training loss: 12082.84, base loss: 14493.24
[INFO 2017-06-28 12:28:34,461 main.py:51] epoch 417, training loss: 10692.41, average training loss: 12079.51, base loss: 14492.41
[INFO 2017-06-28 12:28:34,850 main.py:51] epoch 418, training loss: 10517.08, average training loss: 12075.78, base loss: 14490.07
[INFO 2017-06-28 12:28:35,225 main.py:51] epoch 419, training loss: 12811.22, average training loss: 12077.53, base loss: 14495.11
[INFO 2017-06-28 12:28:35,613 main.py:51] epoch 420, training loss: 9774.48, average training loss: 12072.06, base loss: 14489.44
[INFO 2017-06-28 12:28:35,994 main.py:51] epoch 421, training loss: 10200.29, average training loss: 12067.63, base loss: 14485.50
[INFO 2017-06-28 12:28:36,378 main.py:51] epoch 422, training loss: 11026.60, average training loss: 12065.16, base loss: 14484.20
[INFO 2017-06-28 12:28:36,778 main.py:51] epoch 423, training loss: 10622.48, average training loss: 12061.76, base loss: 14484.13
[INFO 2017-06-28 12:28:37,163 main.py:51] epoch 424, training loss: 11001.21, average training loss: 12059.27, base loss: 14483.12
[INFO 2017-06-28 12:28:37,570 main.py:51] epoch 425, training loss: 9993.11, average training loss: 12054.42, base loss: 14479.73
[INFO 2017-06-28 12:28:37,975 main.py:51] epoch 426, training loss: 11341.63, average training loss: 12052.75, base loss: 14479.60
[INFO 2017-06-28 12:28:38,367 main.py:51] epoch 427, training loss: 10235.95, average training loss: 12048.50, base loss: 14476.86
[INFO 2017-06-28 12:28:38,733 main.py:51] epoch 428, training loss: 12030.67, average training loss: 12048.46, base loss: 14477.85
[INFO 2017-06-28 12:28:39,124 main.py:51] epoch 429, training loss: 10158.17, average training loss: 12044.06, base loss: 14477.19
[INFO 2017-06-28 12:28:39,533 main.py:51] epoch 430, training loss: 12147.84, average training loss: 12044.31, base loss: 14481.78
[INFO 2017-06-28 12:28:39,936 main.py:51] epoch 431, training loss: 9369.94, average training loss: 12038.11, base loss: 14478.73
[INFO 2017-06-28 12:28:40,343 main.py:51] epoch 432, training loss: 8600.16, average training loss: 12030.17, base loss: 14473.46
[INFO 2017-06-28 12:28:40,739 main.py:51] epoch 433, training loss: 10267.01, average training loss: 12026.11, base loss: 14468.79
[INFO 2017-06-28 12:28:41,147 main.py:51] epoch 434, training loss: 10010.16, average training loss: 12021.48, base loss: 14466.48
[INFO 2017-06-28 12:28:41,524 main.py:51] epoch 435, training loss: 10518.90, average training loss: 12018.03, base loss: 14462.50
[INFO 2017-06-28 12:28:41,922 main.py:51] epoch 436, training loss: 11030.91, average training loss: 12015.77, base loss: 14463.50
[INFO 2017-06-28 12:28:42,336 main.py:51] epoch 437, training loss: 11114.35, average training loss: 12013.71, base loss: 14463.49
[INFO 2017-06-28 12:28:42,711 main.py:51] epoch 438, training loss: 10213.27, average training loss: 12009.61, base loss: 14459.09
[INFO 2017-06-28 12:28:43,112 main.py:51] epoch 439, training loss: 10533.16, average training loss: 12006.26, base loss: 14458.77
[INFO 2017-06-28 12:28:43,508 main.py:51] epoch 440, training loss: 10916.46, average training loss: 12003.79, base loss: 14458.41
[INFO 2017-06-28 12:28:43,944 main.py:51] epoch 441, training loss: 10429.50, average training loss: 12000.22, base loss: 14458.49
[INFO 2017-06-28 12:28:44,335 main.py:51] epoch 442, training loss: 9778.60, average training loss: 11995.21, base loss: 14452.13
[INFO 2017-06-28 12:28:44,731 main.py:51] epoch 443, training loss: 10652.28, average training loss: 11992.19, base loss: 14450.34
[INFO 2017-06-28 12:28:45,132 main.py:51] epoch 444, training loss: 9501.44, average training loss: 11986.59, base loss: 14444.45
[INFO 2017-06-28 12:28:45,530 main.py:51] epoch 445, training loss: 10682.61, average training loss: 11983.66, base loss: 14443.43
[INFO 2017-06-28 12:28:45,931 main.py:51] epoch 446, training loss: 13437.55, average training loss: 11986.92, base loss: 14447.86
[INFO 2017-06-28 12:28:46,362 main.py:51] epoch 447, training loss: 10076.70, average training loss: 11982.65, base loss: 14446.25
[INFO 2017-06-28 12:28:46,754 main.py:51] epoch 448, training loss: 12140.10, average training loss: 11983.00, base loss: 14449.94
[INFO 2017-06-28 12:28:47,136 main.py:51] epoch 449, training loss: 11195.15, average training loss: 11981.25, base loss: 14452.56
[INFO 2017-06-28 12:28:47,537 main.py:51] epoch 450, training loss: 8732.10, average training loss: 11974.05, base loss: 14445.61
[INFO 2017-06-28 12:28:47,940 main.py:51] epoch 451, training loss: 12831.41, average training loss: 11975.95, base loss: 14450.79
[INFO 2017-06-28 12:28:48,349 main.py:51] epoch 452, training loss: 11474.54, average training loss: 11974.84, base loss: 14453.33
[INFO 2017-06-28 12:28:48,763 main.py:51] epoch 453, training loss: 12670.79, average training loss: 11976.37, base loss: 14458.14
[INFO 2017-06-28 12:28:49,152 main.py:51] epoch 454, training loss: 11058.00, average training loss: 11974.35, base loss: 14456.37
[INFO 2017-06-28 12:28:49,549 main.py:51] epoch 455, training loss: 11504.94, average training loss: 11973.32, base loss: 14456.48
[INFO 2017-06-28 12:28:49,956 main.py:51] epoch 456, training loss: 10283.68, average training loss: 11969.63, base loss: 14454.89
[INFO 2017-06-28 12:28:50,363 main.py:51] epoch 457, training loss: 11582.66, average training loss: 11968.78, base loss: 14456.12
[INFO 2017-06-28 12:28:50,754 main.py:51] epoch 458, training loss: 12167.09, average training loss: 11969.21, base loss: 14462.38
[INFO 2017-06-28 12:28:51,186 main.py:51] epoch 459, training loss: 10900.48, average training loss: 11966.89, base loss: 14462.85
[INFO 2017-06-28 12:28:51,585 main.py:51] epoch 460, training loss: 10764.92, average training loss: 11964.28, base loss: 14463.56
[INFO 2017-06-28 12:28:51,996 main.py:51] epoch 461, training loss: 11389.26, average training loss: 11963.04, base loss: 14464.93
[INFO 2017-06-28 12:28:52,396 main.py:51] epoch 462, training loss: 11723.01, average training loss: 11962.52, base loss: 14466.49
[INFO 2017-06-28 12:28:52,802 main.py:51] epoch 463, training loss: 10299.29, average training loss: 11958.94, base loss: 14466.62
[INFO 2017-06-28 12:28:53,202 main.py:51] epoch 464, training loss: 9347.28, average training loss: 11953.32, base loss: 14462.77
[INFO 2017-06-28 12:28:53,599 main.py:51] epoch 465, training loss: 12668.94, average training loss: 11954.85, base loss: 14467.07
[INFO 2017-06-28 12:28:53,985 main.py:51] epoch 466, training loss: 10599.88, average training loss: 11951.95, base loss: 14466.12
[INFO 2017-06-28 12:28:54,395 main.py:51] epoch 467, training loss: 11226.89, average training loss: 11950.40, base loss: 14468.65
[INFO 2017-06-28 12:28:54,778 main.py:51] epoch 468, training loss: 9849.39, average training loss: 11945.92, base loss: 14465.23
[INFO 2017-06-28 12:28:55,181 main.py:51] epoch 469, training loss: 9878.66, average training loss: 11941.53, base loss: 14459.98
[INFO 2017-06-28 12:28:55,581 main.py:51] epoch 470, training loss: 11401.11, average training loss: 11940.38, base loss: 14461.83
[INFO 2017-06-28 12:28:56,000 main.py:51] epoch 471, training loss: 12029.99, average training loss: 11940.57, base loss: 14463.47
[INFO 2017-06-28 12:28:56,394 main.py:51] epoch 472, training loss: 9669.86, average training loss: 11935.77, base loss: 14462.97
[INFO 2017-06-28 12:28:56,817 main.py:51] epoch 473, training loss: 12499.53, average training loss: 11936.96, base loss: 14465.17
[INFO 2017-06-28 12:28:57,226 main.py:51] epoch 474, training loss: 12014.73, average training loss: 11937.12, base loss: 14468.50
[INFO 2017-06-28 12:28:57,616 main.py:51] epoch 475, training loss: 10378.28, average training loss: 11933.85, base loss: 14466.41
[INFO 2017-06-28 12:28:58,035 main.py:51] epoch 476, training loss: 11422.19, average training loss: 11932.77, base loss: 14467.88
[INFO 2017-06-28 12:28:58,433 main.py:51] epoch 477, training loss: 10279.02, average training loss: 11929.31, base loss: 14468.59
[INFO 2017-06-28 12:28:58,850 main.py:51] epoch 478, training loss: 11889.74, average training loss: 11929.23, base loss: 14472.83
[INFO 2017-06-28 12:28:59,258 main.py:51] epoch 479, training loss: 10022.77, average training loss: 11925.26, base loss: 14471.98
[INFO 2017-06-28 12:28:59,677 main.py:51] epoch 480, training loss: 10265.80, average training loss: 11921.81, base loss: 14468.99
[INFO 2017-06-28 12:29:00,065 main.py:51] epoch 481, training loss: 10085.00, average training loss: 11918.00, base loss: 14466.45
[INFO 2017-06-28 12:29:00,467 main.py:51] epoch 482, training loss: 9851.50, average training loss: 11913.72, base loss: 14465.13
[INFO 2017-06-28 12:29:00,873 main.py:51] epoch 483, training loss: 10640.60, average training loss: 11911.09, base loss: 14464.63
[INFO 2017-06-28 12:29:01,261 main.py:51] epoch 484, training loss: 11255.84, average training loss: 11909.74, base loss: 14467.12
[INFO 2017-06-28 12:29:01,659 main.py:51] epoch 485, training loss: 10316.04, average training loss: 11906.46, base loss: 14467.26
[INFO 2017-06-28 12:29:02,082 main.py:51] epoch 486, training loss: 9793.60, average training loss: 11902.12, base loss: 14464.60
[INFO 2017-06-28 12:29:02,489 main.py:51] epoch 487, training loss: 10747.65, average training loss: 11899.75, base loss: 14464.11
[INFO 2017-06-28 12:29:02,897 main.py:51] epoch 488, training loss: 10514.99, average training loss: 11896.92, base loss: 14462.41
[INFO 2017-06-28 12:29:03,299 main.py:51] epoch 489, training loss: 11277.52, average training loss: 11895.66, base loss: 14463.31
[INFO 2017-06-28 12:29:03,709 main.py:51] epoch 490, training loss: 11164.09, average training loss: 11894.17, base loss: 14463.60
[INFO 2017-06-28 12:29:04,141 main.py:51] epoch 491, training loss: 11820.15, average training loss: 11894.02, base loss: 14467.36
[INFO 2017-06-28 12:29:04,553 main.py:51] epoch 492, training loss: 10716.20, average training loss: 11891.63, base loss: 14464.21
[INFO 2017-06-28 12:29:04,966 main.py:51] epoch 493, training loss: 10951.48, average training loss: 11889.73, base loss: 14464.38
[INFO 2017-06-28 12:29:05,367 main.py:51] epoch 494, training loss: 12429.00, average training loss: 11890.82, base loss: 14468.19
[INFO 2017-06-28 12:29:05,771 main.py:51] epoch 495, training loss: 10809.74, average training loss: 11888.64, base loss: 14467.83
[INFO 2017-06-28 12:29:06,166 main.py:51] epoch 496, training loss: 10520.75, average training loss: 11885.88, base loss: 14467.69
[INFO 2017-06-28 12:29:06,574 main.py:51] epoch 497, training loss: 10077.77, average training loss: 11882.25, base loss: 14466.42
[INFO 2017-06-28 12:29:06,992 main.py:51] epoch 498, training loss: 11697.27, average training loss: 11881.88, base loss: 14471.16
[INFO 2017-06-28 12:29:07,399 main.py:51] epoch 499, training loss: 10409.14, average training loss: 11878.94, base loss: 14467.03
[INFO 2017-06-28 12:29:07,399 main.py:53] epoch 499, testing
[INFO 2017-06-28 12:29:09,350 main.py:105] average testing loss: 11882.43, base loss: 15194.31
[INFO 2017-06-28 12:29:09,350 main.py:106] improve_loss: 3311.88, improve_percent: 0.22
[INFO 2017-06-28 12:29:09,350 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:29:09,363 main.py:76] current best improved percent: 0.22
[INFO 2017-06-28 12:29:09,778 main.py:51] epoch 500, training loss: 12418.55, average training loss: 11880.01, base loss: 14471.53
[INFO 2017-06-28 12:29:10,185 main.py:51] epoch 501, training loss: 12740.19, average training loss: 11881.73, base loss: 14478.13
[INFO 2017-06-28 12:29:10,615 main.py:51] epoch 502, training loss: 9990.62, average training loss: 11877.97, base loss: 14476.75
[INFO 2017-06-28 12:29:11,042 main.py:51] epoch 503, training loss: 11224.11, average training loss: 11876.67, base loss: 14477.44
[INFO 2017-06-28 12:29:11,467 main.py:51] epoch 504, training loss: 9820.84, average training loss: 11872.60, base loss: 14476.19
[INFO 2017-06-28 12:29:11,862 main.py:51] epoch 505, training loss: 11556.74, average training loss: 11871.97, base loss: 14476.05
[INFO 2017-06-28 12:29:12,276 main.py:51] epoch 506, training loss: 10499.70, average training loss: 11869.27, base loss: 14473.64
[INFO 2017-06-28 12:29:12,698 main.py:51] epoch 507, training loss: 10386.37, average training loss: 11866.35, base loss: 14472.30
[INFO 2017-06-28 12:29:13,090 main.py:51] epoch 508, training loss: 11032.91, average training loss: 11864.71, base loss: 14473.04
[INFO 2017-06-28 12:29:13,480 main.py:51] epoch 509, training loss: 10855.73, average training loss: 11862.73, base loss: 14473.60
[INFO 2017-06-28 12:29:13,886 main.py:51] epoch 510, training loss: 11223.46, average training loss: 11861.48, base loss: 14473.12
[INFO 2017-06-28 12:29:14,284 main.py:51] epoch 511, training loss: 10883.13, average training loss: 11859.57, base loss: 14470.90
[INFO 2017-06-28 12:29:14,699 main.py:51] epoch 512, training loss: 10631.04, average training loss: 11857.18, base loss: 14469.33
[INFO 2017-06-28 12:29:15,109 main.py:51] epoch 513, training loss: 12043.29, average training loss: 11857.54, base loss: 14471.95
[INFO 2017-06-28 12:29:15,506 main.py:51] epoch 514, training loss: 10017.52, average training loss: 11853.97, base loss: 14470.88
[INFO 2017-06-28 12:29:15,901 main.py:51] epoch 515, training loss: 11124.05, average training loss: 11852.55, base loss: 14474.01
[INFO 2017-06-28 12:29:16,299 main.py:51] epoch 516, training loss: 11510.92, average training loss: 11851.89, base loss: 14475.42
[INFO 2017-06-28 12:29:16,692 main.py:51] epoch 517, training loss: 13465.25, average training loss: 11855.01, base loss: 14483.88
[INFO 2017-06-28 12:29:17,096 main.py:51] epoch 518, training loss: 10148.83, average training loss: 11851.72, base loss: 14483.05
[INFO 2017-06-28 12:29:17,530 main.py:51] epoch 519, training loss: 11561.37, average training loss: 11851.16, base loss: 14487.35
[INFO 2017-06-28 12:29:17,937 main.py:51] epoch 520, training loss: 10369.76, average training loss: 11848.32, base loss: 14486.28
[INFO 2017-06-28 12:29:18,360 main.py:51] epoch 521, training loss: 10811.13, average training loss: 11846.33, base loss: 14486.25
[INFO 2017-06-28 12:29:18,769 main.py:51] epoch 522, training loss: 11061.81, average training loss: 11844.83, base loss: 14487.36
[INFO 2017-06-28 12:29:19,190 main.py:51] epoch 523, training loss: 11849.35, average training loss: 11844.84, base loss: 14488.83
[INFO 2017-06-28 12:29:19,591 main.py:51] epoch 524, training loss: 11066.96, average training loss: 11843.36, base loss: 14489.13
[INFO 2017-06-28 12:29:19,997 main.py:51] epoch 525, training loss: 9818.48, average training loss: 11839.51, base loss: 14484.93
[INFO 2017-06-28 12:29:20,442 main.py:51] epoch 526, training loss: 10952.78, average training loss: 11837.82, base loss: 14484.72
[INFO 2017-06-28 12:29:20,920 main.py:51] epoch 527, training loss: 10589.54, average training loss: 11835.46, base loss: 14483.95
[INFO 2017-06-28 12:29:21,352 main.py:51] epoch 528, training loss: 11258.83, average training loss: 11834.37, base loss: 14484.16
[INFO 2017-06-28 12:29:21,781 main.py:51] epoch 529, training loss: 11230.58, average training loss: 11833.23, base loss: 14483.42
[INFO 2017-06-28 12:29:22,179 main.py:51] epoch 530, training loss: 11602.77, average training loss: 11832.80, base loss: 14483.71
[INFO 2017-06-28 12:29:22,596 main.py:51] epoch 531, training loss: 11399.60, average training loss: 11831.98, base loss: 14482.77
[INFO 2017-06-28 12:29:23,027 main.py:51] epoch 532, training loss: 12093.18, average training loss: 11832.47, base loss: 14485.18
[INFO 2017-06-28 12:29:23,452 main.py:51] epoch 533, training loss: 10153.33, average training loss: 11829.33, base loss: 14483.60
[INFO 2017-06-28 12:29:23,849 main.py:51] epoch 534, training loss: 9673.17, average training loss: 11825.30, base loss: 14479.91
[INFO 2017-06-28 12:29:24,278 main.py:51] epoch 535, training loss: 10174.90, average training loss: 11822.22, base loss: 14476.49
[INFO 2017-06-28 12:29:24,688 main.py:51] epoch 536, training loss: 9974.67, average training loss: 11818.78, base loss: 14476.25
[INFO 2017-06-28 12:29:25,108 main.py:51] epoch 537, training loss: 12629.36, average training loss: 11820.28, base loss: 14481.50
[INFO 2017-06-28 12:29:25,541 main.py:51] epoch 538, training loss: 9314.73, average training loss: 11815.64, base loss: 14477.65
[INFO 2017-06-28 12:29:25,954 main.py:51] epoch 539, training loss: 9948.18, average training loss: 11812.18, base loss: 14476.20
[INFO 2017-06-28 12:29:26,359 main.py:51] epoch 540, training loss: 9031.29, average training loss: 11807.04, base loss: 14471.02
[INFO 2017-06-28 12:29:26,794 main.py:51] epoch 541, training loss: 10992.21, average training loss: 11805.53, base loss: 14471.14
[INFO 2017-06-28 12:29:27,192 main.py:51] epoch 542, training loss: 11985.82, average training loss: 11805.87, base loss: 14474.34
[INFO 2017-06-28 12:29:27,606 main.py:51] epoch 543, training loss: 9298.62, average training loss: 11801.26, base loss: 14470.65
[INFO 2017-06-28 12:29:28,010 main.py:51] epoch 544, training loss: 10627.33, average training loss: 11799.10, base loss: 14468.40
[INFO 2017-06-28 12:29:28,406 main.py:51] epoch 545, training loss: 12622.77, average training loss: 11800.61, base loss: 14473.28
[INFO 2017-06-28 12:29:28,819 main.py:51] epoch 546, training loss: 10046.87, average training loss: 11797.41, base loss: 14470.34
[INFO 2017-06-28 12:29:29,254 main.py:51] epoch 547, training loss: 10863.40, average training loss: 11795.70, base loss: 14472.16
[INFO 2017-06-28 12:29:29,655 main.py:51] epoch 548, training loss: 11366.89, average training loss: 11794.92, base loss: 14474.35
[INFO 2017-06-28 12:29:30,056 main.py:51] epoch 549, training loss: 10338.23, average training loss: 11792.27, base loss: 14472.30
[INFO 2017-06-28 12:29:30,468 main.py:51] epoch 550, training loss: 10095.73, average training loss: 11789.19, base loss: 14469.51
[INFO 2017-06-28 12:29:30,863 main.py:51] epoch 551, training loss: 11993.50, average training loss: 11789.56, base loss: 14473.25
[INFO 2017-06-28 12:29:31,298 main.py:51] epoch 552, training loss: 9048.77, average training loss: 11784.61, base loss: 14468.47
[INFO 2017-06-28 12:29:31,765 main.py:51] epoch 553, training loss: 11904.75, average training loss: 11784.82, base loss: 14470.91
[INFO 2017-06-28 12:29:32,169 main.py:51] epoch 554, training loss: 9993.19, average training loss: 11781.60, base loss: 14470.25
[INFO 2017-06-28 12:29:32,589 main.py:51] epoch 555, training loss: 12083.60, average training loss: 11782.14, base loss: 14473.67
[INFO 2017-06-28 12:29:33,016 main.py:51] epoch 556, training loss: 11182.43, average training loss: 11781.06, base loss: 14474.58
[INFO 2017-06-28 12:29:33,454 main.py:51] epoch 557, training loss: 10601.21, average training loss: 11778.95, base loss: 14473.87
[INFO 2017-06-28 12:29:33,867 main.py:51] epoch 558, training loss: 10429.45, average training loss: 11776.53, base loss: 14473.92
[INFO 2017-06-28 12:29:34,291 main.py:51] epoch 559, training loss: 10048.85, average training loss: 11773.45, base loss: 14471.23
[INFO 2017-06-28 12:29:34,716 main.py:51] epoch 560, training loss: 9490.78, average training loss: 11769.38, base loss: 14465.97
[INFO 2017-06-28 12:29:35,137 main.py:51] epoch 561, training loss: 11720.78, average training loss: 11769.29, base loss: 14466.81
[INFO 2017-06-28 12:29:35,561 main.py:51] epoch 562, training loss: 10717.78, average training loss: 11767.42, base loss: 14465.84
[INFO 2017-06-28 12:29:35,971 main.py:51] epoch 563, training loss: 9564.60, average training loss: 11763.52, base loss: 14463.06
[INFO 2017-06-28 12:29:36,398 main.py:51] epoch 564, training loss: 11480.16, average training loss: 11763.02, base loss: 14465.04
[INFO 2017-06-28 12:29:36,807 main.py:51] epoch 565, training loss: 11912.62, average training loss: 11763.28, base loss: 14466.67
[INFO 2017-06-28 12:29:37,248 main.py:51] epoch 566, training loss: 12028.98, average training loss: 11763.75, base loss: 14468.38
[INFO 2017-06-28 12:29:37,686 main.py:51] epoch 567, training loss: 11189.26, average training loss: 11762.74, base loss: 14469.18
[INFO 2017-06-28 12:29:38,117 main.py:51] epoch 568, training loss: 10640.41, average training loss: 11760.77, base loss: 14469.62
[INFO 2017-06-28 12:29:38,564 main.py:51] epoch 569, training loss: 9726.02, average training loss: 11757.20, base loss: 14469.36
[INFO 2017-06-28 12:29:38,999 main.py:51] epoch 570, training loss: 11571.26, average training loss: 11756.87, base loss: 14471.62
[INFO 2017-06-28 12:29:39,446 main.py:51] epoch 571, training loss: 10982.32, average training loss: 11755.52, base loss: 14471.76
[INFO 2017-06-28 12:29:39,864 main.py:51] epoch 572, training loss: 10779.45, average training loss: 11753.81, base loss: 14470.69
[INFO 2017-06-28 12:29:40,302 main.py:51] epoch 573, training loss: 11403.28, average training loss: 11753.20, base loss: 14471.57
[INFO 2017-06-28 12:29:40,752 main.py:51] epoch 574, training loss: 10814.35, average training loss: 11751.57, base loss: 14472.34
[INFO 2017-06-28 12:29:41,189 main.py:51] epoch 575, training loss: 11545.35, average training loss: 11751.21, base loss: 14474.04
[INFO 2017-06-28 12:29:41,631 main.py:51] epoch 576, training loss: 12232.79, average training loss: 11752.05, base loss: 14477.95
[INFO 2017-06-28 12:29:42,058 main.py:51] epoch 577, training loss: 11631.24, average training loss: 11751.84, base loss: 14480.46
[INFO 2017-06-28 12:29:42,486 main.py:51] epoch 578, training loss: 10474.22, average training loss: 11749.63, base loss: 14479.40
[INFO 2017-06-28 12:29:42,923 main.py:51] epoch 579, training loss: 10920.40, average training loss: 11748.20, base loss: 14480.15
[INFO 2017-06-28 12:29:43,342 main.py:51] epoch 580, training loss: 10846.91, average training loss: 11746.65, base loss: 14479.49
[INFO 2017-06-28 12:29:43,742 main.py:51] epoch 581, training loss: 10089.87, average training loss: 11743.80, base loss: 14476.24
[INFO 2017-06-28 12:29:44,177 main.py:51] epoch 582, training loss: 11537.32, average training loss: 11743.45, base loss: 14478.45
[INFO 2017-06-28 12:29:44,599 main.py:51] epoch 583, training loss: 10685.07, average training loss: 11741.64, base loss: 14476.51
[INFO 2017-06-28 12:29:45,046 main.py:51] epoch 584, training loss: 10943.73, average training loss: 11740.27, base loss: 14476.02
[INFO 2017-06-28 12:29:45,456 main.py:51] epoch 585, training loss: 10073.48, average training loss: 11737.43, base loss: 14475.93
[INFO 2017-06-28 12:29:45,911 main.py:51] epoch 586, training loss: 10062.04, average training loss: 11734.57, base loss: 14476.40
[INFO 2017-06-28 12:29:46,344 main.py:51] epoch 587, training loss: 10418.70, average training loss: 11732.34, base loss: 14474.06
[INFO 2017-06-28 12:29:46,770 main.py:51] epoch 588, training loss: 10533.65, average training loss: 11730.30, base loss: 14473.76
[INFO 2017-06-28 12:29:47,221 main.py:51] epoch 589, training loss: 10613.59, average training loss: 11728.41, base loss: 14472.87
[INFO 2017-06-28 12:29:47,665 main.py:51] epoch 590, training loss: 9455.78, average training loss: 11724.56, base loss: 14470.16
[INFO 2017-06-28 12:29:48,117 main.py:51] epoch 591, training loss: 9791.19, average training loss: 11721.30, base loss: 14467.70
[INFO 2017-06-28 12:29:48,558 main.py:51] epoch 592, training loss: 12822.94, average training loss: 11723.16, base loss: 14473.37
[INFO 2017-06-28 12:29:49,012 main.py:51] epoch 593, training loss: 9875.25, average training loss: 11720.04, base loss: 14470.95
[INFO 2017-06-28 12:29:49,444 main.py:51] epoch 594, training loss: 10837.30, average training loss: 11718.56, base loss: 14471.08
[INFO 2017-06-28 12:29:49,871 main.py:51] epoch 595, training loss: 10427.78, average training loss: 11716.40, base loss: 14470.59
[INFO 2017-06-28 12:29:50,292 main.py:51] epoch 596, training loss: 9301.48, average training loss: 11712.35, base loss: 14466.73
[INFO 2017-06-28 12:29:50,732 main.py:51] epoch 597, training loss: 11422.20, average training loss: 11711.86, base loss: 14469.22
[INFO 2017-06-28 12:29:51,167 main.py:51] epoch 598, training loss: 11156.36, average training loss: 11710.94, base loss: 14470.74
[INFO 2017-06-28 12:29:51,590 main.py:51] epoch 599, training loss: 11931.65, average training loss: 11711.31, base loss: 14472.79
[INFO 2017-06-28 12:29:51,590 main.py:53] epoch 599, testing
[INFO 2017-06-28 12:29:53,560 main.py:105] average testing loss: 11099.56, base loss: 14294.40
[INFO 2017-06-28 12:29:53,560 main.py:106] improve_loss: 3194.84, improve_percent: 0.22
[INFO 2017-06-28 12:29:53,561 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:29:53,573 main.py:76] current best improved percent: 0.22
[INFO 2017-06-28 12:29:54,008 main.py:51] epoch 600, training loss: 9975.83, average training loss: 11708.42, base loss: 14471.38
[INFO 2017-06-28 12:29:54,437 main.py:51] epoch 601, training loss: 11042.24, average training loss: 11707.31, base loss: 14474.29
[INFO 2017-06-28 12:29:54,856 main.py:51] epoch 602, training loss: 10298.05, average training loss: 11704.97, base loss: 14474.77
[INFO 2017-06-28 12:29:55,275 main.py:51] epoch 603, training loss: 9986.69, average training loss: 11702.13, base loss: 14474.24
[INFO 2017-06-28 12:29:55,724 main.py:51] epoch 604, training loss: 11278.17, average training loss: 11701.43, base loss: 14475.76
[INFO 2017-06-28 12:29:56,147 main.py:51] epoch 605, training loss: 11662.15, average training loss: 11701.36, base loss: 14478.93
[INFO 2017-06-28 12:29:56,571 main.py:51] epoch 606, training loss: 12050.26, average training loss: 11701.94, base loss: 14480.11
[INFO 2017-06-28 12:29:56,997 main.py:51] epoch 607, training loss: 10625.57, average training loss: 11700.17, base loss: 14479.96
[INFO 2017-06-28 12:29:57,417 main.py:51] epoch 608, training loss: 10341.23, average training loss: 11697.94, base loss: 14479.92
[INFO 2017-06-28 12:29:57,833 main.py:51] epoch 609, training loss: 10768.46, average training loss: 11696.41, base loss: 14479.41
[INFO 2017-06-28 12:29:58,272 main.py:51] epoch 610, training loss: 9676.41, average training loss: 11693.11, base loss: 14477.00
[INFO 2017-06-28 12:29:58,680 main.py:51] epoch 611, training loss: 9820.19, average training loss: 11690.05, base loss: 14474.53
[INFO 2017-06-28 12:29:59,113 main.py:51] epoch 612, training loss: 11040.77, average training loss: 11688.99, base loss: 14474.97
[INFO 2017-06-28 12:29:59,539 main.py:51] epoch 613, training loss: 11148.98, average training loss: 11688.11, base loss: 14473.35
[INFO 2017-06-28 12:29:59,982 main.py:51] epoch 614, training loss: 10110.59, average training loss: 11685.54, base loss: 14472.01
[INFO 2017-06-28 12:30:00,385 main.py:51] epoch 615, training loss: 9371.81, average training loss: 11681.79, base loss: 14469.07
[INFO 2017-06-28 12:30:00,833 main.py:51] epoch 616, training loss: 9679.42, average training loss: 11678.54, base loss: 14465.85
[INFO 2017-06-28 12:30:01,278 main.py:51] epoch 617, training loss: 10769.32, average training loss: 11677.07, base loss: 14466.10
[INFO 2017-06-28 12:30:01,712 main.py:51] epoch 618, training loss: 10064.52, average training loss: 11674.47, base loss: 14462.28
[INFO 2017-06-28 12:30:02,154 main.py:51] epoch 619, training loss: 11272.60, average training loss: 11673.82, base loss: 14462.97
[INFO 2017-06-28 12:30:02,584 main.py:51] epoch 620, training loss: 10219.42, average training loss: 11671.47, base loss: 14461.20
[INFO 2017-06-28 12:30:03,032 main.py:51] epoch 621, training loss: 12053.03, average training loss: 11672.09, base loss: 14465.28
[INFO 2017-06-28 12:30:03,471 main.py:51] epoch 622, training loss: 11154.08, average training loss: 11671.26, base loss: 14466.04
[INFO 2017-06-28 12:30:03,920 main.py:51] epoch 623, training loss: 10188.19, average training loss: 11668.88, base loss: 14464.85
[INFO 2017-06-28 12:30:04,351 main.py:51] epoch 624, training loss: 11257.59, average training loss: 11668.22, base loss: 14465.80
[INFO 2017-06-28 12:30:04,791 main.py:51] epoch 625, training loss: 9054.65, average training loss: 11664.05, base loss: 14461.24
[INFO 2017-06-28 12:30:05,221 main.py:51] epoch 626, training loss: 10413.18, average training loss: 11662.05, base loss: 14459.92
[INFO 2017-06-28 12:30:05,644 main.py:51] epoch 627, training loss: 9940.06, average training loss: 11659.31, base loss: 14456.60
[INFO 2017-06-28 12:30:06,074 main.py:51] epoch 628, training loss: 11416.19, average training loss: 11658.92, base loss: 14457.30
[INFO 2017-06-28 12:30:06,504 main.py:51] epoch 629, training loss: 10169.43, average training loss: 11656.56, base loss: 14455.87
[INFO 2017-06-28 12:30:06,950 main.py:51] epoch 630, training loss: 10157.58, average training loss: 11654.18, base loss: 14454.96
[INFO 2017-06-28 12:30:07,392 main.py:51] epoch 631, training loss: 9723.34, average training loss: 11651.13, base loss: 14451.95
[INFO 2017-06-28 12:30:07,825 main.py:51] epoch 632, training loss: 10961.27, average training loss: 11650.04, base loss: 14451.20
[INFO 2017-06-28 12:30:08,242 main.py:51] epoch 633, training loss: 10747.07, average training loss: 11648.61, base loss: 14451.06
[INFO 2017-06-28 12:30:08,655 main.py:51] epoch 634, training loss: 11605.21, average training loss: 11648.55, base loss: 14452.79
[INFO 2017-06-28 12:30:09,065 main.py:51] epoch 635, training loss: 11509.94, average training loss: 11648.33, base loss: 14453.05
[INFO 2017-06-28 12:30:09,484 main.py:51] epoch 636, training loss: 11592.17, average training loss: 11648.24, base loss: 14455.98
[INFO 2017-06-28 12:30:09,912 main.py:51] epoch 637, training loss: 10859.44, average training loss: 11647.00, base loss: 14456.93
[INFO 2017-06-28 12:30:10,329 main.py:51] epoch 638, training loss: 10516.21, average training loss: 11645.23, base loss: 14455.71
[INFO 2017-06-28 12:30:10,767 main.py:51] epoch 639, training loss: 10822.57, average training loss: 11643.95, base loss: 14457.43
[INFO 2017-06-28 12:30:11,197 main.py:51] epoch 640, training loss: 10484.04, average training loss: 11642.14, base loss: 14455.83
[INFO 2017-06-28 12:30:11,627 main.py:51] epoch 641, training loss: 10480.53, average training loss: 11640.33, base loss: 14455.93
[INFO 2017-06-28 12:30:12,088 main.py:51] epoch 642, training loss: 10063.25, average training loss: 11637.88, base loss: 14454.80
[INFO 2017-06-28 12:30:12,509 main.py:51] epoch 643, training loss: 9894.00, average training loss: 11635.17, base loss: 14452.54
[INFO 2017-06-28 12:30:12,942 main.py:51] epoch 644, training loss: 11834.22, average training loss: 11635.48, base loss: 14453.65
[INFO 2017-06-28 12:30:13,370 main.py:51] epoch 645, training loss: 12412.43, average training loss: 11636.68, base loss: 14455.84
[INFO 2017-06-28 12:30:13,800 main.py:51] epoch 646, training loss: 10077.48, average training loss: 11634.27, base loss: 14455.87
[INFO 2017-06-28 12:30:14,217 main.py:51] epoch 647, training loss: 9698.34, average training loss: 11631.28, base loss: 14453.07
[INFO 2017-06-28 12:30:14,641 main.py:51] epoch 648, training loss: 11151.99, average training loss: 11630.54, base loss: 14453.73
[INFO 2017-06-28 12:30:15,073 main.py:51] epoch 649, training loss: 10675.02, average training loss: 11629.07, base loss: 14452.74
[INFO 2017-06-28 12:30:15,497 main.py:51] epoch 650, training loss: 12646.36, average training loss: 11630.64, base loss: 14456.90
[INFO 2017-06-28 12:30:15,928 main.py:51] epoch 651, training loss: 11103.80, average training loss: 11629.83, base loss: 14457.41
[INFO 2017-06-28 12:30:16,347 main.py:51] epoch 652, training loss: 9618.01, average training loss: 11626.75, base loss: 14455.01
[INFO 2017-06-28 12:30:16,814 main.py:51] epoch 653, training loss: 11151.61, average training loss: 11626.02, base loss: 14454.15
[INFO 2017-06-28 12:30:17,234 main.py:51] epoch 654, training loss: 10643.11, average training loss: 11624.52, base loss: 14453.39
[INFO 2017-06-28 12:30:17,671 main.py:51] epoch 655, training loss: 11080.14, average training loss: 11623.69, base loss: 14454.56
[INFO 2017-06-28 12:30:18,099 main.py:51] epoch 656, training loss: 9568.68, average training loss: 11620.56, base loss: 14452.77
[INFO 2017-06-28 12:30:18,527 main.py:51] epoch 657, training loss: 8887.73, average training loss: 11616.41, base loss: 14450.01
[INFO 2017-06-28 12:30:18,976 main.py:51] epoch 658, training loss: 12323.16, average training loss: 11617.48, base loss: 14453.48
[INFO 2017-06-28 12:30:19,430 main.py:51] epoch 659, training loss: 10877.60, average training loss: 11616.36, base loss: 14452.80
[INFO 2017-06-28 12:30:19,863 main.py:51] epoch 660, training loss: 8942.97, average training loss: 11612.32, base loss: 14449.01
[INFO 2017-06-28 12:30:20,314 main.py:51] epoch 661, training loss: 12177.87, average training loss: 11613.17, base loss: 14452.09
[INFO 2017-06-28 12:30:20,741 main.py:51] epoch 662, training loss: 9225.89, average training loss: 11609.57, base loss: 14448.27
[INFO 2017-06-28 12:30:21,178 main.py:51] epoch 663, training loss: 10651.34, average training loss: 11608.13, base loss: 14447.69
[INFO 2017-06-28 12:30:21,615 main.py:51] epoch 664, training loss: 11111.17, average training loss: 11607.38, base loss: 14449.06
[INFO 2017-06-28 12:30:22,056 main.py:51] epoch 665, training loss: 10208.46, average training loss: 11605.28, base loss: 14447.54
[INFO 2017-06-28 12:30:22,506 main.py:51] epoch 666, training loss: 11404.51, average training loss: 11604.98, base loss: 14449.30
[INFO 2017-06-28 12:30:22,951 main.py:51] epoch 667, training loss: 10387.54, average training loss: 11603.16, base loss: 14447.23
[INFO 2017-06-28 12:30:23,398 main.py:51] epoch 668, training loss: 11889.48, average training loss: 11603.58, base loss: 14449.37
[INFO 2017-06-28 12:30:23,877 main.py:51] epoch 669, training loss: 12514.34, average training loss: 11604.94, base loss: 14451.00
[INFO 2017-06-28 12:30:24,304 main.py:51] epoch 670, training loss: 12917.30, average training loss: 11606.90, base loss: 14455.24
[INFO 2017-06-28 12:30:24,738 main.py:51] epoch 671, training loss: 10174.07, average training loss: 11604.77, base loss: 14454.59
[INFO 2017-06-28 12:30:25,167 main.py:51] epoch 672, training loss: 10084.68, average training loss: 11602.51, base loss: 14453.48
[INFO 2017-06-28 12:30:25,604 main.py:51] epoch 673, training loss: 10586.58, average training loss: 11601.00, base loss: 14454.18
[INFO 2017-06-28 12:30:26,035 main.py:51] epoch 674, training loss: 10644.38, average training loss: 11599.58, base loss: 14454.22
[INFO 2017-06-28 12:30:26,474 main.py:51] epoch 675, training loss: 11202.87, average training loss: 11599.00, base loss: 14455.62
[INFO 2017-06-28 12:30:26,913 main.py:51] epoch 676, training loss: 9332.92, average training loss: 11595.65, base loss: 14453.08
[INFO 2017-06-28 12:30:27,323 main.py:51] epoch 677, training loss: 10388.88, average training loss: 11593.87, base loss: 14452.78
[INFO 2017-06-28 12:30:27,734 main.py:51] epoch 678, training loss: 11146.08, average training loss: 11593.21, base loss: 14453.91
[INFO 2017-06-28 12:30:28,195 main.py:51] epoch 679, training loss: 11335.34, average training loss: 11592.83, base loss: 14454.35
[INFO 2017-06-28 12:30:28,625 main.py:51] epoch 680, training loss: 10170.33, average training loss: 11590.74, base loss: 14453.81
[INFO 2017-06-28 12:30:29,057 main.py:51] epoch 681, training loss: 10889.73, average training loss: 11589.71, base loss: 14454.43
[INFO 2017-06-28 12:30:29,482 main.py:51] epoch 682, training loss: 11560.23, average training loss: 11589.67, base loss: 14454.43
[INFO 2017-06-28 12:30:29,927 main.py:51] epoch 683, training loss: 10811.94, average training loss: 11588.53, base loss: 14454.76
[INFO 2017-06-28 12:30:30,346 main.py:51] epoch 684, training loss: 10933.94, average training loss: 11587.58, base loss: 14454.60
[INFO 2017-06-28 12:30:30,793 main.py:51] epoch 685, training loss: 11517.27, average training loss: 11587.48, base loss: 14456.75
[INFO 2017-06-28 12:30:31,229 main.py:51] epoch 686, training loss: 9696.86, average training loss: 11584.72, base loss: 14453.53
[INFO 2017-06-28 12:30:31,645 main.py:51] epoch 687, training loss: 10695.73, average training loss: 11583.43, base loss: 14453.21
[INFO 2017-06-28 12:30:32,070 main.py:51] epoch 688, training loss: 10491.69, average training loss: 11581.85, base loss: 14453.56
[INFO 2017-06-28 12:30:32,496 main.py:51] epoch 689, training loss: 11064.17, average training loss: 11581.10, base loss: 14454.34
[INFO 2017-06-28 12:30:32,925 main.py:51] epoch 690, training loss: 10219.37, average training loss: 11579.13, base loss: 14453.47
[INFO 2017-06-28 12:30:33,358 main.py:51] epoch 691, training loss: 10718.73, average training loss: 11577.88, base loss: 14453.26
[INFO 2017-06-28 12:30:33,802 main.py:51] epoch 692, training loss: 11985.79, average training loss: 11578.47, base loss: 14456.10
[INFO 2017-06-28 12:30:34,237 main.py:51] epoch 693, training loss: 10733.19, average training loss: 11577.25, base loss: 14457.17
[INFO 2017-06-28 12:30:34,669 main.py:51] epoch 694, training loss: 9765.21, average training loss: 11574.65, base loss: 14456.19
[INFO 2017-06-28 12:30:35,087 main.py:51] epoch 695, training loss: 10470.83, average training loss: 11573.06, base loss: 14456.68
[INFO 2017-06-28 12:30:35,504 main.py:51] epoch 696, training loss: 10423.41, average training loss: 11571.41, base loss: 14456.11
[INFO 2017-06-28 12:30:35,940 main.py:51] epoch 697, training loss: 9521.71, average training loss: 11568.47, base loss: 14453.24
[INFO 2017-06-28 12:30:36,365 main.py:51] epoch 698, training loss: 12315.72, average training loss: 11569.54, base loss: 14456.80
[INFO 2017-06-28 12:30:36,800 main.py:51] epoch 699, training loss: 10171.38, average training loss: 11567.55, base loss: 14456.84
[INFO 2017-06-28 12:30:36,800 main.py:53] epoch 699, testing
[INFO 2017-06-28 12:30:38,804 main.py:105] average testing loss: 11746.60, base loss: 15238.45
[INFO 2017-06-28 12:30:38,804 main.py:106] improve_loss: 3491.86, improve_percent: 0.23
[INFO 2017-06-28 12:30:38,805 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:30:38,817 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 12:30:39,239 main.py:51] epoch 700, training loss: 10457.85, average training loss: 11565.96, base loss: 14456.24
[INFO 2017-06-28 12:30:39,670 main.py:51] epoch 701, training loss: 10445.61, average training loss: 11564.37, base loss: 14455.94
[INFO 2017-06-28 12:30:40,097 main.py:51] epoch 702, training loss: 12262.01, average training loss: 11565.36, base loss: 14459.16
[INFO 2017-06-28 12:30:40,541 main.py:51] epoch 703, training loss: 11530.31, average training loss: 11565.31, base loss: 14460.51
[INFO 2017-06-28 12:30:40,977 main.py:51] epoch 704, training loss: 9952.89, average training loss: 11563.02, base loss: 14460.11
[INFO 2017-06-28 12:30:41,403 main.py:51] epoch 705, training loss: 11269.06, average training loss: 11562.61, base loss: 14460.17
[INFO 2017-06-28 12:30:41,832 main.py:51] epoch 706, training loss: 10156.58, average training loss: 11560.62, base loss: 14458.73
[INFO 2017-06-28 12:30:42,269 main.py:51] epoch 707, training loss: 10057.05, average training loss: 11558.49, base loss: 14458.78
[INFO 2017-06-28 12:30:42,709 main.py:51] epoch 708, training loss: 11575.81, average training loss: 11558.52, base loss: 14459.24
[INFO 2017-06-28 12:30:43,129 main.py:51] epoch 709, training loss: 10558.05, average training loss: 11557.11, base loss: 14458.59
[INFO 2017-06-28 12:30:43,553 main.py:51] epoch 710, training loss: 11229.92, average training loss: 11556.65, base loss: 14459.79
[INFO 2017-06-28 12:30:43,998 main.py:51] epoch 711, training loss: 10857.68, average training loss: 11555.67, base loss: 14458.10
[INFO 2017-06-28 12:30:44,432 main.py:51] epoch 712, training loss: 12415.35, average training loss: 11556.87, base loss: 14459.52
[INFO 2017-06-28 12:30:44,834 main.py:51] epoch 713, training loss: 9277.54, average training loss: 11553.68, base loss: 14455.68
[INFO 2017-06-28 12:30:45,249 main.py:51] epoch 714, training loss: 10278.05, average training loss: 11551.90, base loss: 14453.34
[INFO 2017-06-28 12:30:45,664 main.py:51] epoch 715, training loss: 10311.50, average training loss: 11550.16, base loss: 14452.11
[INFO 2017-06-28 12:30:46,066 main.py:51] epoch 716, training loss: 10642.38, average training loss: 11548.90, base loss: 14451.76
[INFO 2017-06-28 12:30:46,530 main.py:51] epoch 717, training loss: 10544.64, average training loss: 11547.50, base loss: 14451.61
[INFO 2017-06-28 12:30:46,976 main.py:51] epoch 718, training loss: 10361.58, average training loss: 11545.85, base loss: 14451.43
[INFO 2017-06-28 12:30:47,412 main.py:51] epoch 719, training loss: 9661.12, average training loss: 11543.23, base loss: 14450.16
[INFO 2017-06-28 12:30:47,825 main.py:51] epoch 720, training loss: 8671.20, average training loss: 11539.25, base loss: 14445.94
[INFO 2017-06-28 12:30:48,296 main.py:51] epoch 721, training loss: 10849.78, average training loss: 11538.29, base loss: 14445.28
[INFO 2017-06-28 12:30:48,712 main.py:51] epoch 722, training loss: 11058.25, average training loss: 11537.63, base loss: 14445.93
[INFO 2017-06-28 12:30:49,144 main.py:51] epoch 723, training loss: 10645.19, average training loss: 11536.40, base loss: 14446.26
[INFO 2017-06-28 12:30:49,577 main.py:51] epoch 724, training loss: 11346.09, average training loss: 11536.14, base loss: 14447.80
[INFO 2017-06-28 12:30:50,018 main.py:51] epoch 725, training loss: 9719.37, average training loss: 11533.63, base loss: 14446.04
[INFO 2017-06-28 12:30:50,453 main.py:51] epoch 726, training loss: 11077.69, average training loss: 11533.01, base loss: 14445.55
[INFO 2017-06-28 12:30:50,859 main.py:51] epoch 727, training loss: 10277.77, average training loss: 11531.28, base loss: 14444.75
[INFO 2017-06-28 12:30:51,293 main.py:51] epoch 728, training loss: 10522.42, average training loss: 11529.90, base loss: 14444.78
[INFO 2017-06-28 12:30:51,744 main.py:51] epoch 729, training loss: 11568.05, average training loss: 11529.95, base loss: 14448.00
[INFO 2017-06-28 12:30:52,175 main.py:51] epoch 730, training loss: 10305.43, average training loss: 11528.27, base loss: 14447.73
[INFO 2017-06-28 12:30:52,618 main.py:51] epoch 731, training loss: 10384.26, average training loss: 11526.71, base loss: 14447.51
[INFO 2017-06-28 12:30:53,049 main.py:51] epoch 732, training loss: 10943.12, average training loss: 11525.92, base loss: 14447.87
[INFO 2017-06-28 12:30:53,518 main.py:51] epoch 733, training loss: 11005.32, average training loss: 11525.21, base loss: 14448.44
[INFO 2017-06-28 12:30:53,954 main.py:51] epoch 734, training loss: 10190.88, average training loss: 11523.39, base loss: 14448.32
[INFO 2017-06-28 12:30:54,374 main.py:51] epoch 735, training loss: 9050.57, average training loss: 11520.03, base loss: 14445.09
[INFO 2017-06-28 12:30:54,800 main.py:51] epoch 736, training loss: 10735.81, average training loss: 11518.97, base loss: 14444.57
[INFO 2017-06-28 12:30:55,220 main.py:51] epoch 737, training loss: 9337.17, average training loss: 11516.01, base loss: 14441.54
[INFO 2017-06-28 12:30:55,666 main.py:51] epoch 738, training loss: 10278.98, average training loss: 11514.34, base loss: 14440.33
[INFO 2017-06-28 12:30:56,123 main.py:51] epoch 739, training loss: 11232.18, average training loss: 11513.96, base loss: 14440.50
[INFO 2017-06-28 12:30:56,564 main.py:51] epoch 740, training loss: 9045.52, average training loss: 11510.62, base loss: 14438.09
[INFO 2017-06-28 12:30:56,987 main.py:51] epoch 741, training loss: 10073.75, average training loss: 11508.69, base loss: 14436.41
[INFO 2017-06-28 12:30:57,413 main.py:51] epoch 742, training loss: 9739.07, average training loss: 11506.31, base loss: 14434.96
[INFO 2017-06-28 12:30:57,834 main.py:51] epoch 743, training loss: 9824.02, average training loss: 11504.04, base loss: 14432.93
[INFO 2017-06-28 12:30:58,259 main.py:51] epoch 744, training loss: 9096.58, average training loss: 11500.81, base loss: 14430.63
[INFO 2017-06-28 12:30:58,696 main.py:51] epoch 745, training loss: 8951.22, average training loss: 11497.40, base loss: 14428.35
[INFO 2017-06-28 12:30:59,111 main.py:51] epoch 746, training loss: 10061.38, average training loss: 11495.47, base loss: 14427.00
[INFO 2017-06-28 12:30:59,538 main.py:51] epoch 747, training loss: 8396.26, average training loss: 11491.33, base loss: 14423.74
[INFO 2017-06-28 12:30:59,968 main.py:51] epoch 748, training loss: 10510.40, average training loss: 11490.02, base loss: 14423.11
[INFO 2017-06-28 12:31:00,410 main.py:51] epoch 749, training loss: 12056.11, average training loss: 11490.78, base loss: 14425.55
[INFO 2017-06-28 12:31:00,860 main.py:51] epoch 750, training loss: 11782.46, average training loss: 11491.16, base loss: 14427.85
[INFO 2017-06-28 12:31:01,306 main.py:51] epoch 751, training loss: 10969.71, average training loss: 11490.47, base loss: 14428.66
[INFO 2017-06-28 12:31:01,723 main.py:51] epoch 752, training loss: 10535.33, average training loss: 11489.20, base loss: 14427.85
[INFO 2017-06-28 12:31:02,153 main.py:51] epoch 753, training loss: 10195.96, average training loss: 11487.49, base loss: 14427.08
[INFO 2017-06-28 12:31:02,605 main.py:51] epoch 754, training loss: 10768.24, average training loss: 11486.53, base loss: 14426.62
[INFO 2017-06-28 12:31:03,032 main.py:51] epoch 755, training loss: 11236.27, average training loss: 11486.20, base loss: 14427.36
[INFO 2017-06-28 12:31:03,505 main.py:51] epoch 756, training loss: 11291.96, average training loss: 11485.95, base loss: 14427.85
[INFO 2017-06-28 12:31:03,918 main.py:51] epoch 757, training loss: 11505.85, average training loss: 11485.97, base loss: 14429.45
[INFO 2017-06-28 12:31:04,368 main.py:51] epoch 758, training loss: 11474.29, average training loss: 11485.96, base loss: 14432.61
[INFO 2017-06-28 12:31:04,820 main.py:51] epoch 759, training loss: 9916.96, average training loss: 11483.89, base loss: 14429.92
[INFO 2017-06-28 12:31:05,242 main.py:51] epoch 760, training loss: 10432.23, average training loss: 11482.51, base loss: 14429.36
[INFO 2017-06-28 12:31:05,685 main.py:51] epoch 761, training loss: 11613.29, average training loss: 11482.68, base loss: 14430.64
[INFO 2017-06-28 12:31:06,121 main.py:51] epoch 762, training loss: 12175.83, average training loss: 11483.59, base loss: 14433.31
[INFO 2017-06-28 12:31:06,567 main.py:51] epoch 763, training loss: 9842.73, average training loss: 11481.44, base loss: 14431.78
[INFO 2017-06-28 12:31:07,015 main.py:51] epoch 764, training loss: 10596.22, average training loss: 11480.29, base loss: 14432.58
[INFO 2017-06-28 12:31:07,455 main.py:51] epoch 765, training loss: 12873.56, average training loss: 11482.10, base loss: 14435.44
[INFO 2017-06-28 12:31:07,899 main.py:51] epoch 766, training loss: 11513.31, average training loss: 11482.15, base loss: 14436.84
[INFO 2017-06-28 12:31:08,324 main.py:51] epoch 767, training loss: 10444.61, average training loss: 11480.79, base loss: 14436.78
[INFO 2017-06-28 12:31:08,756 main.py:51] epoch 768, training loss: 11103.29, average training loss: 11480.30, base loss: 14437.87
[INFO 2017-06-28 12:31:09,196 main.py:51] epoch 769, training loss: 9183.27, average training loss: 11477.32, base loss: 14434.25
[INFO 2017-06-28 12:31:09,642 main.py:51] epoch 770, training loss: 9251.98, average training loss: 11474.43, base loss: 14432.44
[INFO 2017-06-28 12:31:10,060 main.py:51] epoch 771, training loss: 9751.36, average training loss: 11472.20, base loss: 14429.38
[INFO 2017-06-28 12:31:10,475 main.py:51] epoch 772, training loss: 10598.74, average training loss: 11471.07, base loss: 14430.01
[INFO 2017-06-28 12:31:10,919 main.py:51] epoch 773, training loss: 8185.49, average training loss: 11466.83, base loss: 14425.23
[INFO 2017-06-28 12:31:11,358 main.py:51] epoch 774, training loss: 10047.69, average training loss: 11465.00, base loss: 14425.05
[INFO 2017-06-28 12:31:11,786 main.py:51] epoch 775, training loss: 10589.21, average training loss: 11463.87, base loss: 14424.59
[INFO 2017-06-28 12:31:12,221 main.py:51] epoch 776, training loss: 10538.27, average training loss: 11462.68, base loss: 14424.17
[INFO 2017-06-28 12:31:12,649 main.py:51] epoch 777, training loss: 10052.81, average training loss: 11460.86, base loss: 14423.98
[INFO 2017-06-28 12:31:13,078 main.py:51] epoch 778, training loss: 11144.68, average training loss: 11460.46, base loss: 14424.23
[INFO 2017-06-28 12:31:13,508 main.py:51] epoch 779, training loss: 10600.01, average training loss: 11459.36, base loss: 14422.54
[INFO 2017-06-28 12:31:13,945 main.py:51] epoch 780, training loss: 11383.37, average training loss: 11459.26, base loss: 14424.49
[INFO 2017-06-28 12:31:14,375 main.py:51] epoch 781, training loss: 9908.59, average training loss: 11457.27, base loss: 14423.72
[INFO 2017-06-28 12:31:14,814 main.py:51] epoch 782, training loss: 10958.15, average training loss: 11456.64, base loss: 14424.60
[INFO 2017-06-28 12:31:15,234 main.py:51] epoch 783, training loss: 9919.73, average training loss: 11454.68, base loss: 14423.21
[INFO 2017-06-28 12:31:15,682 main.py:51] epoch 784, training loss: 9933.79, average training loss: 11452.74, base loss: 14422.39
[INFO 2017-06-28 12:31:16,124 main.py:51] epoch 785, training loss: 11746.31, average training loss: 11453.11, base loss: 14424.85
[INFO 2017-06-28 12:31:16,565 main.py:51] epoch 786, training loss: 11244.26, average training loss: 11452.85, base loss: 14426.20
[INFO 2017-06-28 12:31:17,002 main.py:51] epoch 787, training loss: 10945.56, average training loss: 11452.20, base loss: 14427.40
[INFO 2017-06-28 12:31:17,438 main.py:51] epoch 788, training loss: 10542.74, average training loss: 11451.05, base loss: 14427.43
[INFO 2017-06-28 12:31:17,871 main.py:51] epoch 789, training loss: 12259.42, average training loss: 11452.07, base loss: 14429.52
[INFO 2017-06-28 12:31:18,299 main.py:51] epoch 790, training loss: 11060.84, average training loss: 11451.58, base loss: 14429.64
[INFO 2017-06-28 12:31:18,732 main.py:51] epoch 791, training loss: 11467.59, average training loss: 11451.60, base loss: 14430.23
[INFO 2017-06-28 12:31:19,176 main.py:51] epoch 792, training loss: 11362.42, average training loss: 11451.49, base loss: 14431.22
[INFO 2017-06-28 12:31:19,607 main.py:51] epoch 793, training loss: 11321.68, average training loss: 11451.32, base loss: 14432.16
[INFO 2017-06-28 12:31:20,053 main.py:51] epoch 794, training loss: 12127.40, average training loss: 11452.17, base loss: 14434.09
[INFO 2017-06-28 12:31:20,467 main.py:51] epoch 795, training loss: 9938.33, average training loss: 11450.27, base loss: 14432.64
[INFO 2017-06-28 12:31:20,907 main.py:51] epoch 796, training loss: 10295.47, average training loss: 11448.82, base loss: 14432.34
[INFO 2017-06-28 12:31:21,359 main.py:51] epoch 797, training loss: 11338.08, average training loss: 11448.69, base loss: 14433.65
[INFO 2017-06-28 12:31:21,794 main.py:51] epoch 798, training loss: 9019.92, average training loss: 11445.65, base loss: 14431.28
[INFO 2017-06-28 12:31:22,232 main.py:51] epoch 799, training loss: 11752.98, average training loss: 11446.03, base loss: 14433.75
[INFO 2017-06-28 12:31:22,232 main.py:53] epoch 799, testing
[INFO 2017-06-28 12:31:24,208 main.py:105] average testing loss: 11747.94, base loss: 15403.43
[INFO 2017-06-28 12:31:24,208 main.py:106] improve_loss: 3655.49, improve_percent: 0.24
[INFO 2017-06-28 12:31:24,208 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:31:24,221 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:31:24,642 main.py:51] epoch 800, training loss: 11366.20, average training loss: 11445.93, base loss: 14434.31
[INFO 2017-06-28 12:31:25,074 main.py:51] epoch 801, training loss: 10965.25, average training loss: 11445.33, base loss: 14435.77
[INFO 2017-06-28 12:31:25,498 main.py:51] epoch 802, training loss: 10194.37, average training loss: 11443.77, base loss: 14434.94
[INFO 2017-06-28 12:31:25,917 main.py:51] epoch 803, training loss: 10342.55, average training loss: 11442.40, base loss: 14435.37
[INFO 2017-06-28 12:31:26,350 main.py:51] epoch 804, training loss: 9499.33, average training loss: 11439.99, base loss: 14433.56
[INFO 2017-06-28 12:31:26,783 main.py:51] epoch 805, training loss: 10618.04, average training loss: 11438.97, base loss: 14433.30
[INFO 2017-06-28 12:31:27,191 main.py:51] epoch 806, training loss: 10445.66, average training loss: 11437.74, base loss: 14432.80
[INFO 2017-06-28 12:31:27,623 main.py:51] epoch 807, training loss: 9929.47, average training loss: 11435.87, base loss: 14432.40
[INFO 2017-06-28 12:31:28,050 main.py:51] epoch 808, training loss: 10656.24, average training loss: 11434.91, base loss: 14430.72
[INFO 2017-06-28 12:31:28,498 main.py:51] epoch 809, training loss: 9289.96, average training loss: 11432.26, base loss: 14427.86
[INFO 2017-06-28 12:31:28,959 main.py:51] epoch 810, training loss: 10747.44, average training loss: 11431.42, base loss: 14427.36
[INFO 2017-06-28 12:31:29,390 main.py:51] epoch 811, training loss: 9896.33, average training loss: 11429.53, base loss: 14425.67
[INFO 2017-06-28 12:31:29,817 main.py:51] epoch 812, training loss: 9005.40, average training loss: 11426.54, base loss: 14422.97
[INFO 2017-06-28 12:31:30,229 main.py:51] epoch 813, training loss: 10281.13, average training loss: 11425.14, base loss: 14421.58
[INFO 2017-06-28 12:31:30,655 main.py:51] epoch 814, training loss: 10548.92, average training loss: 11424.06, base loss: 14421.88
[INFO 2017-06-28 12:31:31,072 main.py:51] epoch 815, training loss: 11025.23, average training loss: 11423.57, base loss: 14422.14
[INFO 2017-06-28 12:31:31,516 main.py:51] epoch 816, training loss: 12562.21, average training loss: 11424.97, base loss: 14424.78
[INFO 2017-06-28 12:31:31,952 main.py:51] epoch 817, training loss: 9861.46, average training loss: 11423.05, base loss: 14423.56
[INFO 2017-06-28 12:31:32,364 main.py:51] epoch 818, training loss: 9983.67, average training loss: 11421.30, base loss: 14422.95
[INFO 2017-06-28 12:31:32,823 main.py:51] epoch 819, training loss: 10089.05, average training loss: 11419.67, base loss: 14422.16
[INFO 2017-06-28 12:31:33,232 main.py:51] epoch 820, training loss: 10207.42, average training loss: 11418.20, base loss: 14422.51
[INFO 2017-06-28 12:31:33,656 main.py:51] epoch 821, training loss: 10044.21, average training loss: 11416.52, base loss: 14423.14
[INFO 2017-06-28 12:31:34,083 main.py:51] epoch 822, training loss: 11286.68, average training loss: 11416.37, base loss: 14424.93
[INFO 2017-06-28 12:31:34,521 main.py:51] epoch 823, training loss: 12215.92, average training loss: 11417.34, base loss: 14427.37
[INFO 2017-06-28 12:31:34,940 main.py:51] epoch 824, training loss: 9970.35, average training loss: 11415.58, base loss: 14426.43
[INFO 2017-06-28 12:31:35,353 main.py:51] epoch 825, training loss: 9820.29, average training loss: 11413.65, base loss: 14425.04
[INFO 2017-06-28 12:31:35,784 main.py:51] epoch 826, training loss: 10842.33, average training loss: 11412.96, base loss: 14425.15
[INFO 2017-06-28 12:31:36,227 main.py:51] epoch 827, training loss: 10699.72, average training loss: 11412.10, base loss: 14424.00
[INFO 2017-06-28 12:31:36,631 main.py:51] epoch 828, training loss: 11196.45, average training loss: 11411.84, base loss: 14422.87
[INFO 2017-06-28 12:31:37,060 main.py:51] epoch 829, training loss: 11390.48, average training loss: 11411.81, base loss: 14424.46
[INFO 2017-06-28 12:31:37,475 main.py:51] epoch 830, training loss: 12540.86, average training loss: 11413.17, base loss: 14424.93
[INFO 2017-06-28 12:31:37,916 main.py:51] epoch 831, training loss: 12344.28, average training loss: 11414.29, base loss: 14425.83
[INFO 2017-06-28 12:31:38,347 main.py:51] epoch 832, training loss: 11937.93, average training loss: 11414.92, base loss: 14427.95
[INFO 2017-06-28 12:31:38,754 main.py:51] epoch 833, training loss: 11944.74, average training loss: 11415.56, base loss: 14431.11
[INFO 2017-06-28 12:31:39,183 main.py:51] epoch 834, training loss: 12382.82, average training loss: 11416.71, base loss: 14434.27
[INFO 2017-06-28 12:31:39,612 main.py:51] epoch 835, training loss: 10847.63, average training loss: 11416.03, base loss: 14436.30
[INFO 2017-06-28 12:31:40,032 main.py:51] epoch 836, training loss: 11123.57, average training loss: 11415.68, base loss: 14438.55
[INFO 2017-06-28 12:31:40,470 main.py:51] epoch 837, training loss: 11483.48, average training loss: 11415.76, base loss: 14440.84
[INFO 2017-06-28 12:31:40,895 main.py:51] epoch 838, training loss: 10763.56, average training loss: 11414.99, base loss: 14440.55
[INFO 2017-06-28 12:31:41,316 main.py:51] epoch 839, training loss: 11956.81, average training loss: 11415.63, base loss: 14442.70
[INFO 2017-06-28 12:31:41,748 main.py:51] epoch 840, training loss: 10514.37, average training loss: 11414.56, base loss: 14442.17
[INFO 2017-06-28 12:31:42,167 main.py:51] epoch 841, training loss: 9362.09, average training loss: 11412.12, base loss: 14439.37
[INFO 2017-06-28 12:31:42,597 main.py:51] epoch 842, training loss: 10955.79, average training loss: 11411.58, base loss: 14439.39
[INFO 2017-06-28 12:31:43,036 main.py:51] epoch 843, training loss: 10223.07, average training loss: 11410.17, base loss: 14438.38
[INFO 2017-06-28 12:31:43,468 main.py:51] epoch 844, training loss: 12230.26, average training loss: 11411.14, base loss: 14440.41
[INFO 2017-06-28 12:31:43,910 main.py:51] epoch 845, training loss: 10778.86, average training loss: 11410.40, base loss: 14440.36
[INFO 2017-06-28 12:31:44,333 main.py:51] epoch 846, training loss: 9916.41, average training loss: 11408.63, base loss: 14438.51
[INFO 2017-06-28 12:31:44,758 main.py:51] epoch 847, training loss: 10703.17, average training loss: 11407.80, base loss: 14438.87
[INFO 2017-06-28 12:31:45,202 main.py:51] epoch 848, training loss: 9310.88, average training loss: 11405.33, base loss: 14436.81
[INFO 2017-06-28 12:31:45,612 main.py:51] epoch 849, training loss: 9425.61, average training loss: 11403.00, base loss: 14433.84
[INFO 2017-06-28 12:31:46,055 main.py:51] epoch 850, training loss: 10570.89, average training loss: 11402.02, base loss: 14432.94
[INFO 2017-06-28 12:31:46,462 main.py:51] epoch 851, training loss: 11069.72, average training loss: 11401.63, base loss: 14432.39
[INFO 2017-06-28 12:31:46,893 main.py:51] epoch 852, training loss: 10897.53, average training loss: 11401.04, base loss: 14432.28
[INFO 2017-06-28 12:31:47,339 main.py:51] epoch 853, training loss: 10453.15, average training loss: 11399.93, base loss: 14432.05
[INFO 2017-06-28 12:31:47,766 main.py:51] epoch 854, training loss: 9061.10, average training loss: 11397.20, base loss: 14429.53
[INFO 2017-06-28 12:31:48,204 main.py:51] epoch 855, training loss: 10280.38, average training loss: 11395.89, base loss: 14429.24
[INFO 2017-06-28 12:31:48,643 main.py:51] epoch 856, training loss: 14073.63, average training loss: 11399.02, base loss: 14434.93
[INFO 2017-06-28 12:31:49,093 main.py:51] epoch 857, training loss: 9661.13, average training loss: 11396.99, base loss: 14433.76
[INFO 2017-06-28 12:31:49,532 main.py:51] epoch 858, training loss: 9999.83, average training loss: 11395.37, base loss: 14433.22
[INFO 2017-06-28 12:31:49,976 main.py:51] epoch 859, training loss: 10348.13, average training loss: 11394.15, base loss: 14432.11
[INFO 2017-06-28 12:31:50,427 main.py:51] epoch 860, training loss: 10330.80, average training loss: 11392.91, base loss: 14432.20
[INFO 2017-06-28 12:31:50,854 main.py:51] epoch 861, training loss: 10945.64, average training loss: 11392.39, base loss: 14433.39
[INFO 2017-06-28 12:31:51,295 main.py:51] epoch 862, training loss: 13115.33, average training loss: 11394.39, base loss: 14436.52
[INFO 2017-06-28 12:31:51,732 main.py:51] epoch 863, training loss: 10299.37, average training loss: 11393.12, base loss: 14435.45
[INFO 2017-06-28 12:31:52,157 main.py:51] epoch 864, training loss: 13504.33, average training loss: 11395.56, base loss: 14439.67
[INFO 2017-06-28 12:31:52,596 main.py:51] epoch 865, training loss: 10286.54, average training loss: 11394.28, base loss: 14440.45
[INFO 2017-06-28 12:31:53,013 main.py:51] epoch 866, training loss: 10263.12, average training loss: 11392.98, base loss: 14439.37
[INFO 2017-06-28 12:31:53,428 main.py:51] epoch 867, training loss: 11322.86, average training loss: 11392.90, base loss: 14440.71
[INFO 2017-06-28 12:31:53,858 main.py:51] epoch 868, training loss: 9827.63, average training loss: 11391.10, base loss: 14439.04
[INFO 2017-06-28 12:31:54,291 main.py:51] epoch 869, training loss: 10214.82, average training loss: 11389.74, base loss: 14437.96
[INFO 2017-06-28 12:31:54,717 main.py:51] epoch 870, training loss: 10847.54, average training loss: 11389.12, base loss: 14438.54
[INFO 2017-06-28 12:31:55,141 main.py:51] epoch 871, training loss: 9857.49, average training loss: 11387.37, base loss: 14438.28
[INFO 2017-06-28 12:31:55,566 main.py:51] epoch 872, training loss: 9814.68, average training loss: 11385.56, base loss: 14436.97
[INFO 2017-06-28 12:31:55,992 main.py:51] epoch 873, training loss: 10741.73, average training loss: 11384.83, base loss: 14437.12
[INFO 2017-06-28 12:31:56,456 main.py:51] epoch 874, training loss: 11762.02, average training loss: 11385.26, base loss: 14438.77
[INFO 2017-06-28 12:31:56,899 main.py:51] epoch 875, training loss: 9962.79, average training loss: 11383.63, base loss: 14438.54
[INFO 2017-06-28 12:31:57,331 main.py:51] epoch 876, training loss: 8896.64, average training loss: 11380.80, base loss: 14435.77
[INFO 2017-06-28 12:31:57,751 main.py:51] epoch 877, training loss: 10564.51, average training loss: 11379.87, base loss: 14434.63
[INFO 2017-06-28 12:31:58,185 main.py:51] epoch 878, training loss: 10356.86, average training loss: 11378.70, base loss: 14435.17
[INFO 2017-06-28 12:31:58,612 main.py:51] epoch 879, training loss: 10058.45, average training loss: 11377.20, base loss: 14434.06
[INFO 2017-06-28 12:31:59,045 main.py:51] epoch 880, training loss: 9745.90, average training loss: 11375.35, base loss: 14431.71
[INFO 2017-06-28 12:31:59,484 main.py:51] epoch 881, training loss: 10096.55, average training loss: 11373.90, base loss: 14431.36
[INFO 2017-06-28 12:31:59,904 main.py:51] epoch 882, training loss: 9123.83, average training loss: 11371.35, base loss: 14428.66
[INFO 2017-06-28 12:32:00,333 main.py:51] epoch 883, training loss: 8978.86, average training loss: 11368.65, base loss: 14425.96
[INFO 2017-06-28 12:32:00,763 main.py:51] epoch 884, training loss: 9837.83, average training loss: 11366.92, base loss: 14424.33
[INFO 2017-06-28 12:32:01,206 main.py:51] epoch 885, training loss: 11358.96, average training loss: 11366.91, base loss: 14425.24
[INFO 2017-06-28 12:32:01,633 main.py:51] epoch 886, training loss: 9636.55, average training loss: 11364.96, base loss: 14424.46
[INFO 2017-06-28 12:32:02,076 main.py:51] epoch 887, training loss: 10760.04, average training loss: 11364.28, base loss: 14425.95
[INFO 2017-06-28 12:32:02,520 main.py:51] epoch 888, training loss: 10944.43, average training loss: 11363.81, base loss: 14427.14
[INFO 2017-06-28 12:32:02,939 main.py:51] epoch 889, training loss: 9774.09, average training loss: 11362.02, base loss: 14426.33
[INFO 2017-06-28 12:32:03,381 main.py:51] epoch 890, training loss: 9919.05, average training loss: 11360.40, base loss: 14424.88
[INFO 2017-06-28 12:32:03,806 main.py:51] epoch 891, training loss: 12042.18, average training loss: 11361.16, base loss: 14426.67
[INFO 2017-06-28 12:32:04,236 main.py:51] epoch 892, training loss: 9553.18, average training loss: 11359.14, base loss: 14424.22
[INFO 2017-06-28 12:32:04,644 main.py:51] epoch 893, training loss: 11163.28, average training loss: 11358.92, base loss: 14425.52
[INFO 2017-06-28 12:32:05,103 main.py:51] epoch 894, training loss: 10809.07, average training loss: 11358.31, base loss: 14426.04
[INFO 2017-06-28 12:32:05,539 main.py:51] epoch 895, training loss: 9993.53, average training loss: 11356.78, base loss: 14427.16
[INFO 2017-06-28 12:32:05,982 main.py:51] epoch 896, training loss: 9189.49, average training loss: 11354.37, base loss: 14424.64
[INFO 2017-06-28 12:32:06,413 main.py:51] epoch 897, training loss: 10148.50, average training loss: 11353.02, base loss: 14423.77
[INFO 2017-06-28 12:32:06,827 main.py:51] epoch 898, training loss: 12475.62, average training loss: 11354.27, base loss: 14427.75
[INFO 2017-06-28 12:32:07,264 main.py:51] epoch 899, training loss: 9657.43, average training loss: 11352.39, base loss: 14427.05
[INFO 2017-06-28 12:32:07,264 main.py:53] epoch 899, testing
[INFO 2017-06-28 12:32:09,225 main.py:105] average testing loss: 11357.16, base loss: 14919.08
[INFO 2017-06-28 12:32:09,225 main.py:106] improve_loss: 3561.92, improve_percent: 0.24
[INFO 2017-06-28 12:32:09,226 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:32:09,238 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:32:09,663 main.py:51] epoch 900, training loss: 10353.50, average training loss: 11351.28, base loss: 14426.83
[INFO 2017-06-28 12:32:10,080 main.py:51] epoch 901, training loss: 9503.00, average training loss: 11349.23, base loss: 14425.80
[INFO 2017-06-28 12:32:10,510 main.py:51] epoch 902, training loss: 11646.19, average training loss: 11349.56, base loss: 14428.82
[INFO 2017-06-28 12:32:10,935 main.py:51] epoch 903, training loss: 10755.33, average training loss: 11348.90, base loss: 14428.25
[INFO 2017-06-28 12:32:11,360 main.py:51] epoch 904, training loss: 12574.02, average training loss: 11350.25, base loss: 14431.17
[INFO 2017-06-28 12:32:11,786 main.py:51] epoch 905, training loss: 11066.22, average training loss: 11349.94, base loss: 14432.12
[INFO 2017-06-28 12:32:12,208 main.py:51] epoch 906, training loss: 10868.56, average training loss: 11349.41, base loss: 14432.54
[INFO 2017-06-28 12:32:12,629 main.py:51] epoch 907, training loss: 11607.05, average training loss: 11349.69, base loss: 14433.52
[INFO 2017-06-28 12:32:13,054 main.py:51] epoch 908, training loss: 11786.66, average training loss: 11350.17, base loss: 14435.53
[INFO 2017-06-28 12:32:13,493 main.py:51] epoch 909, training loss: 10854.70, average training loss: 11349.63, base loss: 14436.17
[INFO 2017-06-28 12:32:13,928 main.py:51] epoch 910, training loss: 11619.74, average training loss: 11349.93, base loss: 14437.48
[INFO 2017-06-28 12:32:14,370 main.py:51] epoch 911, training loss: 10095.28, average training loss: 11348.55, base loss: 14436.39
[INFO 2017-06-28 12:32:14,797 main.py:51] epoch 912, training loss: 10783.54, average training loss: 11347.93, base loss: 14436.79
[INFO 2017-06-28 12:32:15,202 main.py:51] epoch 913, training loss: 11051.20, average training loss: 11347.61, base loss: 14437.73
[INFO 2017-06-28 12:32:15,614 main.py:51] epoch 914, training loss: 11228.71, average training loss: 11347.48, base loss: 14439.11
[INFO 2017-06-28 12:32:16,039 main.py:51] epoch 915, training loss: 11706.64, average training loss: 11347.87, base loss: 14441.40
[INFO 2017-06-28 12:32:16,453 main.py:51] epoch 916, training loss: 10290.22, average training loss: 11346.72, base loss: 14440.67
[INFO 2017-06-28 12:32:16,880 main.py:51] epoch 917, training loss: 11339.04, average training loss: 11346.71, base loss: 14442.35
[INFO 2017-06-28 12:32:17,326 main.py:51] epoch 918, training loss: 10716.82, average training loss: 11346.02, base loss: 14442.87
[INFO 2017-06-28 12:32:17,750 main.py:51] epoch 919, training loss: 11613.30, average training loss: 11346.31, base loss: 14443.85
[INFO 2017-06-28 12:32:18,187 main.py:51] epoch 920, training loss: 11353.54, average training loss: 11346.32, base loss: 14444.99
[INFO 2017-06-28 12:32:18,618 main.py:51] epoch 921, training loss: 10597.45, average training loss: 11345.51, base loss: 14444.62
[INFO 2017-06-28 12:32:19,054 main.py:51] epoch 922, training loss: 10002.69, average training loss: 11344.05, base loss: 14443.84
[INFO 2017-06-28 12:32:19,480 main.py:51] epoch 923, training loss: 11134.32, average training loss: 11343.83, base loss: 14444.10
[INFO 2017-06-28 12:32:19,932 main.py:51] epoch 924, training loss: 10496.18, average training loss: 11342.91, base loss: 14444.21
[INFO 2017-06-28 12:32:20,373 main.py:51] epoch 925, training loss: 10769.36, average training loss: 11342.29, base loss: 14445.04
[INFO 2017-06-28 12:32:20,812 main.py:51] epoch 926, training loss: 10664.22, average training loss: 11341.56, base loss: 14444.84
[INFO 2017-06-28 12:32:21,244 main.py:51] epoch 927, training loss: 10716.36, average training loss: 11340.89, base loss: 14444.75
[INFO 2017-06-28 12:32:21,685 main.py:51] epoch 928, training loss: 10243.01, average training loss: 11339.70, base loss: 14443.97
[INFO 2017-06-28 12:32:22,108 main.py:51] epoch 929, training loss: 10223.17, average training loss: 11338.50, base loss: 14443.62
[INFO 2017-06-28 12:32:22,536 main.py:51] epoch 930, training loss: 10818.11, average training loss: 11337.94, base loss: 14443.90
[INFO 2017-06-28 12:32:22,976 main.py:51] epoch 931, training loss: 11176.69, average training loss: 11337.77, base loss: 14443.81
[INFO 2017-06-28 12:32:23,388 main.py:51] epoch 932, training loss: 10071.18, average training loss: 11336.41, base loss: 14442.78
[INFO 2017-06-28 12:32:23,801 main.py:51] epoch 933, training loss: 9158.84, average training loss: 11334.08, base loss: 14440.95
[INFO 2017-06-28 12:32:24,241 main.py:51] epoch 934, training loss: 9788.18, average training loss: 11332.43, base loss: 14440.01
[INFO 2017-06-28 12:32:24,680 main.py:51] epoch 935, training loss: 11554.51, average training loss: 11332.67, base loss: 14442.27
[INFO 2017-06-28 12:32:25,125 main.py:51] epoch 936, training loss: 9578.64, average training loss: 11330.79, base loss: 14440.50
[INFO 2017-06-28 12:32:25,540 main.py:51] epoch 937, training loss: 9266.23, average training loss: 11328.59, base loss: 14438.42
[INFO 2017-06-28 12:32:25,954 main.py:51] epoch 938, training loss: 9944.60, average training loss: 11327.12, base loss: 14437.60
[INFO 2017-06-28 12:32:26,403 main.py:51] epoch 939, training loss: 11954.63, average training loss: 11327.79, base loss: 14439.59
[INFO 2017-06-28 12:32:26,849 main.py:51] epoch 940, training loss: 9955.74, average training loss: 11326.33, base loss: 14438.57
[INFO 2017-06-28 12:32:27,254 main.py:51] epoch 941, training loss: 13192.99, average training loss: 11328.31, base loss: 14441.86
[INFO 2017-06-28 12:32:27,686 main.py:51] epoch 942, training loss: 10860.72, average training loss: 11327.82, base loss: 14441.96
[INFO 2017-06-28 12:32:28,125 main.py:51] epoch 943, training loss: 10402.63, average training loss: 11326.83, base loss: 14442.38
[INFO 2017-06-28 12:32:28,582 main.py:51] epoch 944, training loss: 9464.15, average training loss: 11324.86, base loss: 14441.67
[INFO 2017-06-28 12:32:29,025 main.py:51] epoch 945, training loss: 11782.93, average training loss: 11325.35, base loss: 14443.74
[INFO 2017-06-28 12:32:29,456 main.py:51] epoch 946, training loss: 9431.70, average training loss: 11323.35, base loss: 14441.76
[INFO 2017-06-28 12:32:29,894 main.py:51] epoch 947, training loss: 9929.11, average training loss: 11321.88, base loss: 14441.08
[INFO 2017-06-28 12:32:30,359 main.py:51] epoch 948, training loss: 11786.41, average training loss: 11322.37, base loss: 14442.31
[INFO 2017-06-28 12:32:30,811 main.py:51] epoch 949, training loss: 9224.32, average training loss: 11320.16, base loss: 14440.85
[INFO 2017-06-28 12:32:31,239 main.py:51] epoch 950, training loss: 9961.01, average training loss: 11318.73, base loss: 14439.18
[INFO 2017-06-28 12:32:31,663 main.py:51] epoch 951, training loss: 12654.83, average training loss: 11320.13, base loss: 14442.70
[INFO 2017-06-28 12:32:32,096 main.py:51] epoch 952, training loss: 10696.50, average training loss: 11319.48, base loss: 14443.01
[INFO 2017-06-28 12:32:32,524 main.py:51] epoch 953, training loss: 9665.38, average training loss: 11317.74, base loss: 14441.60
[INFO 2017-06-28 12:32:32,952 main.py:51] epoch 954, training loss: 9598.71, average training loss: 11315.94, base loss: 14441.43
[INFO 2017-06-28 12:32:33,373 main.py:51] epoch 955, training loss: 11169.16, average training loss: 11315.79, base loss: 14442.53
[INFO 2017-06-28 12:32:33,816 main.py:51] epoch 956, training loss: 9907.23, average training loss: 11314.32, base loss: 14441.86
[INFO 2017-06-28 12:32:34,251 main.py:51] epoch 957, training loss: 9769.52, average training loss: 11312.71, base loss: 14439.73
[INFO 2017-06-28 12:32:34,674 main.py:51] epoch 958, training loss: 10241.33, average training loss: 11311.59, base loss: 14438.95
[INFO 2017-06-28 12:32:35,097 main.py:51] epoch 959, training loss: 11262.21, average training loss: 11311.54, base loss: 14439.07
[INFO 2017-06-28 12:32:35,538 main.py:51] epoch 960, training loss: 9047.72, average training loss: 11309.18, base loss: 14437.52
[INFO 2017-06-28 12:32:35,964 main.py:51] epoch 961, training loss: 10848.82, average training loss: 11308.70, base loss: 14437.14
[INFO 2017-06-28 12:32:36,395 main.py:51] epoch 962, training loss: 11168.69, average training loss: 11308.56, base loss: 14437.33
[INFO 2017-06-28 12:32:36,832 main.py:51] epoch 963, training loss: 12240.02, average training loss: 11309.52, base loss: 14438.44
[INFO 2017-06-28 12:32:37,255 main.py:51] epoch 964, training loss: 11142.01, average training loss: 11309.35, base loss: 14439.23
[INFO 2017-06-28 12:32:37,693 main.py:51] epoch 965, training loss: 10951.06, average training loss: 11308.98, base loss: 14440.75
[INFO 2017-06-28 12:32:38,118 main.py:51] epoch 966, training loss: 10457.67, average training loss: 11308.10, base loss: 14441.08
[INFO 2017-06-28 12:32:38,543 main.py:51] epoch 967, training loss: 12190.02, average training loss: 11309.01, base loss: 14442.35
[INFO 2017-06-28 12:32:38,982 main.py:51] epoch 968, training loss: 9565.03, average training loss: 11307.21, base loss: 14440.21
[INFO 2017-06-28 12:32:39,428 main.py:51] epoch 969, training loss: 9842.08, average training loss: 11305.70, base loss: 14441.13
[INFO 2017-06-28 12:32:39,858 main.py:51] epoch 970, training loss: 10301.93, average training loss: 11304.67, base loss: 14440.97
[INFO 2017-06-28 12:32:40,288 main.py:51] epoch 971, training loss: 11646.88, average training loss: 11305.02, base loss: 14442.36
[INFO 2017-06-28 12:32:40,728 main.py:51] epoch 972, training loss: 10919.93, average training loss: 11304.62, base loss: 14442.95
[INFO 2017-06-28 12:32:41,132 main.py:51] epoch 973, training loss: 10584.16, average training loss: 11303.88, base loss: 14443.56
[INFO 2017-06-28 12:32:41,556 main.py:51] epoch 974, training loss: 9661.14, average training loss: 11302.20, base loss: 14442.62
[INFO 2017-06-28 12:32:41,996 main.py:51] epoch 975, training loss: 10076.56, average training loss: 11300.94, base loss: 14441.91
[INFO 2017-06-28 12:32:42,436 main.py:51] epoch 976, training loss: 11324.53, average training loss: 11300.97, base loss: 14443.56
[INFO 2017-06-28 12:32:42,882 main.py:51] epoch 977, training loss: 10305.23, average training loss: 11299.95, base loss: 14443.37
[INFO 2017-06-28 12:32:43,313 main.py:51] epoch 978, training loss: 10063.27, average training loss: 11298.69, base loss: 14444.08
[INFO 2017-06-28 12:32:43,728 main.py:51] epoch 979, training loss: 11456.82, average training loss: 11298.85, base loss: 14444.74
[INFO 2017-06-28 12:32:44,146 main.py:51] epoch 980, training loss: 11654.19, average training loss: 11299.21, base loss: 14446.04
[INFO 2017-06-28 12:32:44,573 main.py:51] epoch 981, training loss: 9946.24, average training loss: 11297.83, base loss: 14445.49
[INFO 2017-06-28 12:32:45,001 main.py:51] epoch 982, training loss: 10872.11, average training loss: 11297.40, base loss: 14446.22
[INFO 2017-06-28 12:32:45,447 main.py:51] epoch 983, training loss: 11545.76, average training loss: 11297.65, base loss: 14447.13
[INFO 2017-06-28 12:32:45,861 main.py:51] epoch 984, training loss: 10577.55, average training loss: 11296.92, base loss: 14446.17
[INFO 2017-06-28 12:32:46,288 main.py:51] epoch 985, training loss: 11344.67, average training loss: 11296.97, base loss: 14446.25
[INFO 2017-06-28 12:32:46,745 main.py:51] epoch 986, training loss: 11055.93, average training loss: 11296.72, base loss: 14446.89
[INFO 2017-06-28 12:32:47,166 main.py:51] epoch 987, training loss: 12359.34, average training loss: 11297.80, base loss: 14449.11
[INFO 2017-06-28 12:32:47,598 main.py:51] epoch 988, training loss: 10802.31, average training loss: 11297.30, base loss: 14448.94
[INFO 2017-06-28 12:32:48,020 main.py:51] epoch 989, training loss: 9206.80, average training loss: 11295.19, base loss: 14447.17
[INFO 2017-06-28 12:32:48,444 main.py:51] epoch 990, training loss: 10951.60, average training loss: 11294.84, base loss: 14448.62
[INFO 2017-06-28 12:32:48,866 main.py:51] epoch 991, training loss: 9524.90, average training loss: 11293.06, base loss: 14447.50
[INFO 2017-06-28 12:32:49,272 main.py:51] epoch 992, training loss: 10962.23, average training loss: 11292.72, base loss: 14447.82
[INFO 2017-06-28 12:32:49,708 main.py:51] epoch 993, training loss: 8709.16, average training loss: 11290.12, base loss: 14445.05
[INFO 2017-06-28 12:32:50,139 main.py:51] epoch 994, training loss: 10381.62, average training loss: 11289.21, base loss: 14444.94
[INFO 2017-06-28 12:32:50,555 main.py:51] epoch 995, training loss: 10627.35, average training loss: 11288.55, base loss: 14445.44
[INFO 2017-06-28 12:32:50,991 main.py:51] epoch 996, training loss: 9064.61, average training loss: 11286.32, base loss: 14442.69
[INFO 2017-06-28 12:32:51,411 main.py:51] epoch 997, training loss: 11083.58, average training loss: 11286.11, base loss: 14443.97
[INFO 2017-06-28 12:32:51,850 main.py:51] epoch 998, training loss: 10980.87, average training loss: 11285.81, base loss: 14445.13
[INFO 2017-06-28 12:32:52,280 main.py:51] epoch 999, training loss: 10298.28, average training loss: 11284.82, base loss: 14444.85
[INFO 2017-06-28 12:32:52,281 main.py:53] epoch 999, testing
[INFO 2017-06-28 12:32:54,236 main.py:105] average testing loss: 11062.61, base loss: 14618.26
[INFO 2017-06-28 12:32:54,236 main.py:106] improve_loss: 3555.66, improve_percent: 0.24
[INFO 2017-06-28 12:32:54,236 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:32:54,249 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:32:54,699 main.py:51] epoch 1000, training loss: 10328.93, average training loss: 11266.31, base loss: 14443.73
[INFO 2017-06-28 12:32:55,148 main.py:51] epoch 1001, training loss: 10981.44, average training loss: 11254.61, base loss: 14446.42
[INFO 2017-06-28 12:32:55,562 main.py:51] epoch 1002, training loss: 9534.73, average training loss: 11242.35, base loss: 14446.23
[INFO 2017-06-28 12:32:55,998 main.py:51] epoch 1003, training loss: 10681.05, average training loss: 11233.66, base loss: 14446.84
[INFO 2017-06-28 12:32:56,420 main.py:51] epoch 1004, training loss: 9988.77, average training loss: 11224.50, base loss: 14446.07
[INFO 2017-06-28 12:32:56,846 main.py:51] epoch 1005, training loss: 10192.19, average training loss: 11215.82, base loss: 14445.00
[INFO 2017-06-28 12:32:57,273 main.py:51] epoch 1006, training loss: 11431.87, average training loss: 11209.86, base loss: 14447.08
[INFO 2017-06-28 12:32:57,707 main.py:51] epoch 1007, training loss: 11849.88, average training loss: 11206.04, base loss: 14450.52
[INFO 2017-06-28 12:32:58,126 main.py:51] epoch 1008, training loss: 10956.57, average training loss: 11200.92, base loss: 14449.73
[INFO 2017-06-28 12:32:58,577 main.py:51] epoch 1009, training loss: 9546.21, average training loss: 11195.41, base loss: 14449.05
[INFO 2017-06-28 12:32:59,000 main.py:51] epoch 1010, training loss: 9889.95, average training loss: 11190.28, base loss: 14449.43
[INFO 2017-06-28 12:32:59,435 main.py:51] epoch 1011, training loss: 10245.65, average training loss: 11185.91, base loss: 14450.57
[INFO 2017-06-28 12:32:59,870 main.py:51] epoch 1012, training loss: 10314.45, average training loss: 11180.99, base loss: 14450.00
[INFO 2017-06-28 12:33:00,294 main.py:51] epoch 1013, training loss: 11239.95, average training loss: 11176.53, base loss: 14450.01
[INFO 2017-06-28 12:33:00,708 main.py:51] epoch 1014, training loss: 10613.66, average training loss: 11171.74, base loss: 14449.70
[INFO 2017-06-28 12:33:01,143 main.py:51] epoch 1015, training loss: 9380.48, average training loss: 11166.97, base loss: 14448.00
[INFO 2017-06-28 12:33:01,562 main.py:51] epoch 1016, training loss: 9843.95, average training loss: 11163.28, base loss: 14448.40
[INFO 2017-06-28 12:33:01,968 main.py:51] epoch 1017, training loss: 10149.46, average training loss: 11156.70, base loss: 14445.08
[INFO 2017-06-28 12:33:02,393 main.py:51] epoch 1018, training loss: 10445.68, average training loss: 11152.59, base loss: 14444.94
[INFO 2017-06-28 12:33:02,820 main.py:51] epoch 1019, training loss: 11372.80, average training loss: 11149.27, base loss: 14445.94
[INFO 2017-06-28 12:33:03,238 main.py:51] epoch 1020, training loss: 9802.43, average training loss: 11146.29, base loss: 14445.81
[INFO 2017-06-28 12:33:03,667 main.py:51] epoch 1021, training loss: 11399.72, average training loss: 11146.03, base loss: 14449.27
[INFO 2017-06-28 12:33:04,091 main.py:51] epoch 1022, training loss: 10394.17, average training loss: 11143.54, base loss: 14450.80
[INFO 2017-06-28 12:33:04,526 main.py:51] epoch 1023, training loss: 11056.54, average training loss: 11139.70, base loss: 14450.12
[INFO 2017-06-28 12:33:04,961 main.py:51] epoch 1024, training loss: 9734.49, average training loss: 11136.68, base loss: 14449.81
[INFO 2017-06-28 12:33:05,377 main.py:51] epoch 1025, training loss: 10236.77, average training loss: 11133.06, base loss: 14448.97
[INFO 2017-06-28 12:33:05,792 main.py:51] epoch 1026, training loss: 9378.09, average training loss: 11130.78, base loss: 14449.14
[INFO 2017-06-28 12:33:06,223 main.py:51] epoch 1027, training loss: 12020.54, average training loss: 11130.98, base loss: 14452.57
[INFO 2017-06-28 12:33:06,630 main.py:51] epoch 1028, training loss: 10504.99, average training loss: 11127.77, base loss: 14452.40
[INFO 2017-06-28 12:33:07,056 main.py:51] epoch 1029, training loss: 9760.04, average training loss: 11124.19, base loss: 14451.12
[INFO 2017-06-28 12:33:07,487 main.py:51] epoch 1030, training loss: 9465.00, average training loss: 11119.62, base loss: 14450.09
[INFO 2017-06-28 12:33:07,924 main.py:51] epoch 1031, training loss: 11194.29, average training loss: 11115.51, base loss: 14450.09
[INFO 2017-06-28 12:33:08,361 main.py:51] epoch 1032, training loss: 9519.26, average training loss: 11111.08, base loss: 14447.92
[INFO 2017-06-28 12:33:08,778 main.py:51] epoch 1033, training loss: 9878.52, average training loss: 11105.81, base loss: 14445.55
[INFO 2017-06-28 12:33:09,226 main.py:51] epoch 1034, training loss: 9777.36, average training loss: 11098.87, base loss: 14441.83
[INFO 2017-06-28 12:33:09,663 main.py:51] epoch 1035, training loss: 9132.60, average training loss: 11094.87, base loss: 14439.19
[INFO 2017-06-28 12:33:10,082 main.py:51] epoch 1036, training loss: 9675.75, average training loss: 11091.53, base loss: 14438.91
[INFO 2017-06-28 12:33:10,503 main.py:51] epoch 1037, training loss: 12558.32, average training loss: 11088.49, base loss: 14438.73
[INFO 2017-06-28 12:33:10,941 main.py:51] epoch 1038, training loss: 9508.36, average training loss: 11082.79, base loss: 14435.84
[INFO 2017-06-28 12:33:11,369 main.py:51] epoch 1039, training loss: 12679.68, average training loss: 11081.93, base loss: 14438.75
[INFO 2017-06-28 12:33:11,804 main.py:51] epoch 1040, training loss: 11205.60, average training loss: 11081.71, base loss: 14441.06
[INFO 2017-06-28 12:33:12,221 main.py:51] epoch 1041, training loss: 11280.97, average training loss: 11081.52, base loss: 14443.88
[INFO 2017-06-28 12:33:12,661 main.py:51] epoch 1042, training loss: 9428.68, average training loss: 11076.47, base loss: 14440.26
[INFO 2017-06-28 12:33:13,085 main.py:51] epoch 1043, training loss: 10820.52, average training loss: 11074.74, base loss: 14439.94
[INFO 2017-06-28 12:33:13,531 main.py:51] epoch 1044, training loss: 10910.04, average training loss: 11071.71, base loss: 14439.71
[INFO 2017-06-28 12:33:13,957 main.py:51] epoch 1045, training loss: 9761.51, average training loss: 11067.90, base loss: 14438.87
[INFO 2017-06-28 12:33:14,404 main.py:51] epoch 1046, training loss: 10714.61, average training loss: 11065.95, base loss: 14440.57
[INFO 2017-06-28 12:33:14,819 main.py:51] epoch 1047, training loss: 9475.59, average training loss: 11061.19, base loss: 14437.03
[INFO 2017-06-28 12:33:15,243 main.py:51] epoch 1048, training loss: 10325.38, average training loss: 11057.24, base loss: 14436.19
[INFO 2017-06-28 12:33:15,669 main.py:51] epoch 1049, training loss: 10052.56, average training loss: 11053.06, base loss: 14434.61
[INFO 2017-06-28 12:33:16,115 main.py:51] epoch 1050, training loss: 10290.51, average training loss: 11050.48, base loss: 14435.21
[INFO 2017-06-28 12:33:16,547 main.py:51] epoch 1051, training loss: 12729.12, average training loss: 11050.26, base loss: 14438.86
[INFO 2017-06-28 12:33:16,994 main.py:51] epoch 1052, training loss: 10594.64, average training loss: 11048.12, base loss: 14438.67
[INFO 2017-06-28 12:33:17,431 main.py:51] epoch 1053, training loss: 10685.05, average training loss: 11043.38, base loss: 14436.77
[INFO 2017-06-28 12:33:17,861 main.py:51] epoch 1054, training loss: 9634.78, average training loss: 11039.72, base loss: 14435.29
[INFO 2017-06-28 12:33:18,317 main.py:51] epoch 1055, training loss: 12672.91, average training loss: 11039.03, base loss: 14438.35
[INFO 2017-06-28 12:33:18,759 main.py:51] epoch 1056, training loss: 11219.64, average training loss: 11034.75, base loss: 14437.84
[INFO 2017-06-28 12:33:19,181 main.py:51] epoch 1057, training loss: 9666.24, average training loss: 11031.87, base loss: 14437.40
[INFO 2017-06-28 12:33:19,636 main.py:51] epoch 1058, training loss: 9430.76, average training loss: 11027.42, base loss: 14435.64
[INFO 2017-06-28 12:33:20,076 main.py:51] epoch 1059, training loss: 9854.37, average training loss: 11020.26, base loss: 14431.56
[INFO 2017-06-28 12:33:20,522 main.py:51] epoch 1060, training loss: 9943.60, average training loss: 11018.08, base loss: 14432.28
[INFO 2017-06-28 12:33:20,940 main.py:51] epoch 1061, training loss: 11097.07, average training loss: 11015.06, base loss: 14432.34
[INFO 2017-06-28 12:33:21,373 main.py:51] epoch 1062, training loss: 9734.00, average training loss: 11011.31, base loss: 14430.52
[INFO 2017-06-28 12:33:21,804 main.py:51] epoch 1063, training loss: 11076.11, average training loss: 11010.16, base loss: 14432.35
[INFO 2017-06-28 12:33:22,238 main.py:51] epoch 1064, training loss: 11809.83, average training loss: 11010.27, base loss: 14435.44
[INFO 2017-06-28 12:33:22,681 main.py:51] epoch 1065, training loss: 9520.82, average training loss: 11005.81, base loss: 14432.64
[INFO 2017-06-28 12:33:23,097 main.py:51] epoch 1066, training loss: 10161.43, average training loss: 11002.89, base loss: 14431.28
[INFO 2017-06-28 12:33:23,508 main.py:51] epoch 1067, training loss: 12320.17, average training loss: 11001.73, base loss: 14433.16
[INFO 2017-06-28 12:33:23,969 main.py:51] epoch 1068, training loss: 9511.73, average training loss: 10998.15, base loss: 14431.15
[INFO 2017-06-28 12:33:24,405 main.py:51] epoch 1069, training loss: 11667.57, average training loss: 10996.83, base loss: 14431.96
[INFO 2017-06-28 12:33:24,839 main.py:51] epoch 1070, training loss: 9790.34, average training loss: 10996.65, base loss: 14434.65
[INFO 2017-06-28 12:33:25,272 main.py:51] epoch 1071, training loss: 11522.43, average training loss: 10994.13, base loss: 14436.47
[INFO 2017-06-28 12:33:25,687 main.py:51] epoch 1072, training loss: 10146.07, average training loss: 10987.65, base loss: 14430.48
[INFO 2017-06-28 12:33:26,119 main.py:51] epoch 1073, training loss: 10494.13, average training loss: 10984.79, base loss: 14429.74
[INFO 2017-06-28 12:33:26,552 main.py:51] epoch 1074, training loss: 9435.45, average training loss: 10979.85, base loss: 14427.60
[INFO 2017-06-28 12:33:27,007 main.py:51] epoch 1075, training loss: 9586.42, average training loss: 10976.77, base loss: 14426.20
[INFO 2017-06-28 12:33:27,438 main.py:51] epoch 1076, training loss: 10802.07, average training loss: 10973.96, base loss: 14425.13
[INFO 2017-06-28 12:33:27,852 main.py:51] epoch 1077, training loss: 10249.92, average training loss: 10971.74, base loss: 14425.56
[INFO 2017-06-28 12:33:28,281 main.py:51] epoch 1078, training loss: 10728.14, average training loss: 10969.74, base loss: 14426.12
[INFO 2017-06-28 12:33:28,714 main.py:51] epoch 1079, training loss: 10076.86, average training loss: 10966.26, base loss: 14424.79
[INFO 2017-06-28 12:33:29,148 main.py:51] epoch 1080, training loss: 9534.49, average training loss: 10963.42, base loss: 14423.96
[INFO 2017-06-28 12:33:29,583 main.py:51] epoch 1081, training loss: 10910.66, average training loss: 10961.50, base loss: 14423.50
[INFO 2017-06-28 12:33:30,016 main.py:51] epoch 1082, training loss: 10169.84, average training loss: 10959.28, base loss: 14422.84
[INFO 2017-06-28 12:33:30,423 main.py:51] epoch 1083, training loss: 8711.13, average training loss: 10954.08, base loss: 14418.50
[INFO 2017-06-28 12:33:30,843 main.py:51] epoch 1084, training loss: 9414.37, average training loss: 10950.07, base loss: 14415.58
[INFO 2017-06-28 12:33:31,282 main.py:51] epoch 1085, training loss: 11578.90, average training loss: 10948.20, base loss: 14414.95
[INFO 2017-06-28 12:33:31,698 main.py:51] epoch 1086, training loss: 9608.46, average training loss: 10946.18, base loss: 14413.52
[INFO 2017-06-28 12:33:32,127 main.py:51] epoch 1087, training loss: 10659.62, average training loss: 10945.08, base loss: 14414.12
[INFO 2017-06-28 12:33:32,556 main.py:51] epoch 1088, training loss: 12739.47, average training loss: 10945.75, base loss: 14417.51
[INFO 2017-06-28 12:33:33,000 main.py:51] epoch 1089, training loss: 10538.74, average training loss: 10941.20, base loss: 14414.20
[INFO 2017-06-28 12:33:33,427 main.py:51] epoch 1090, training loss: 9535.54, average training loss: 10934.56, base loss: 14407.55
[INFO 2017-06-28 12:33:33,902 main.py:51] epoch 1091, training loss: 9886.11, average training loss: 10932.90, base loss: 14407.57
[INFO 2017-06-28 12:33:34,344 main.py:51] epoch 1092, training loss: 9926.18, average training loss: 10927.67, base loss: 14403.97
[INFO 2017-06-28 12:33:34,788 main.py:51] epoch 1093, training loss: 10359.25, average training loss: 10925.30, base loss: 14404.41
[INFO 2017-06-28 12:33:35,224 main.py:51] epoch 1094, training loss: 11026.87, average training loss: 10922.28, base loss: 14404.09
[INFO 2017-06-28 12:33:35,638 main.py:51] epoch 1095, training loss: 10048.05, average training loss: 10919.61, base loss: 14402.83
[INFO 2017-06-28 12:33:36,061 main.py:51] epoch 1096, training loss: 9277.53, average training loss: 10915.92, base loss: 14400.49
[INFO 2017-06-28 12:33:36,535 main.py:51] epoch 1097, training loss: 10450.06, average training loss: 10914.52, base loss: 14400.48
[INFO 2017-06-28 12:33:36,978 main.py:51] epoch 1098, training loss: 10172.67, average training loss: 10911.63, base loss: 14399.62
[INFO 2017-06-28 12:33:37,387 main.py:51] epoch 1099, training loss: 9476.33, average training loss: 10907.35, base loss: 14397.94
[INFO 2017-06-28 12:33:37,388 main.py:53] epoch 1099, testing
[INFO 2017-06-28 12:33:39,363 main.py:105] average testing loss: 11762.58, base loss: 15772.63
[INFO 2017-06-28 12:33:39,363 main.py:106] improve_loss: 4010.04, improve_percent: 0.25
[INFO 2017-06-28 12:33:39,364 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:33:39,376 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:33:39,790 main.py:51] epoch 1100, training loss: 10488.41, average training loss: 10905.48, base loss: 14397.68
[INFO 2017-06-28 12:33:40,239 main.py:51] epoch 1101, training loss: 11345.81, average training loss: 10901.94, base loss: 14396.03
[INFO 2017-06-28 12:33:40,674 main.py:51] epoch 1102, training loss: 9874.24, average training loss: 10899.09, base loss: 14394.19
[INFO 2017-06-28 12:33:41,102 main.py:51] epoch 1103, training loss: 12150.14, average training loss: 10895.73, base loss: 14392.67
[INFO 2017-06-28 12:33:41,518 main.py:51] epoch 1104, training loss: 10548.38, average training loss: 10890.85, base loss: 14388.09
[INFO 2017-06-28 12:33:41,954 main.py:51] epoch 1105, training loss: 9593.47, average training loss: 10885.93, base loss: 14383.68
[INFO 2017-06-28 12:33:42,384 main.py:51] epoch 1106, training loss: 11100.22, average training loss: 10884.40, base loss: 14385.00
[INFO 2017-06-28 12:33:42,807 main.py:51] epoch 1107, training loss: 10179.44, average training loss: 10879.41, base loss: 14380.56
[INFO 2017-06-28 12:33:43,238 main.py:51] epoch 1108, training loss: 9655.47, average training loss: 10876.92, base loss: 14379.28
[INFO 2017-06-28 12:33:43,666 main.py:51] epoch 1109, training loss: 10450.60, average training loss: 10874.07, base loss: 14377.45
[INFO 2017-06-28 12:33:44,083 main.py:51] epoch 1110, training loss: 10378.99, average training loss: 10871.30, base loss: 14375.05
[INFO 2017-06-28 12:33:44,532 main.py:51] epoch 1111, training loss: 9843.32, average training loss: 10866.64, base loss: 14373.40
[INFO 2017-06-28 12:33:44,972 main.py:51] epoch 1112, training loss: 12297.52, average training loss: 10867.80, base loss: 14377.55
[INFO 2017-06-28 12:33:45,417 main.py:51] epoch 1113, training loss: 9750.95, average training loss: 10866.82, base loss: 14378.46
[INFO 2017-06-28 12:33:45,852 main.py:51] epoch 1114, training loss: 9771.46, average training loss: 10862.92, base loss: 14376.13
[INFO 2017-06-28 12:33:46,293 main.py:51] epoch 1115, training loss: 10577.32, average training loss: 10862.44, base loss: 14378.28
[INFO 2017-06-28 12:33:46,723 main.py:51] epoch 1116, training loss: 13259.72, average training loss: 10864.49, base loss: 14382.09
[INFO 2017-06-28 12:33:47,168 main.py:51] epoch 1117, training loss: 10565.67, average training loss: 10862.11, base loss: 14380.97
[INFO 2017-06-28 12:33:47,588 main.py:51] epoch 1118, training loss: 10406.83, average training loss: 10860.15, base loss: 14380.23
[INFO 2017-06-28 12:33:47,997 main.py:51] epoch 1119, training loss: 10334.06, average training loss: 10858.72, base loss: 14380.44
[INFO 2017-06-28 12:33:48,441 main.py:51] epoch 1120, training loss: 10611.82, average training loss: 10856.15, base loss: 14380.09
[INFO 2017-06-28 12:33:48,860 main.py:51] epoch 1121, training loss: 10538.17, average training loss: 10854.55, base loss: 14379.25
[INFO 2017-06-28 12:33:49,313 main.py:51] epoch 1122, training loss: 11922.89, average training loss: 10852.73, base loss: 14379.95
[INFO 2017-06-28 12:33:49,749 main.py:51] epoch 1123, training loss: 9732.93, average training loss: 10849.91, base loss: 14378.20
[INFO 2017-06-28 12:33:50,160 main.py:51] epoch 1124, training loss: 9908.99, average training loss: 10847.75, base loss: 14377.49
[INFO 2017-06-28 12:33:50,597 main.py:51] epoch 1125, training loss: 11413.68, average training loss: 10847.53, base loss: 14379.71
[INFO 2017-06-28 12:33:51,021 main.py:51] epoch 1126, training loss: 10923.14, average training loss: 10845.95, base loss: 14379.28
[INFO 2017-06-28 12:33:51,451 main.py:51] epoch 1127, training loss: 9522.50, average training loss: 10843.76, base loss: 14379.14
[INFO 2017-06-28 12:33:51,901 main.py:51] epoch 1128, training loss: 10349.86, average training loss: 10840.55, base loss: 14376.65
[INFO 2017-06-28 12:33:52,331 main.py:51] epoch 1129, training loss: 10562.61, average training loss: 10838.06, base loss: 14375.87
[INFO 2017-06-28 12:33:52,771 main.py:51] epoch 1130, training loss: 8688.67, average training loss: 10834.63, base loss: 14373.89
[INFO 2017-06-28 12:33:53,188 main.py:51] epoch 1131, training loss: 9256.38, average training loss: 10831.08, base loss: 14370.82
[INFO 2017-06-28 12:33:53,647 main.py:51] epoch 1132, training loss: 13128.62, average training loss: 10833.54, base loss: 14375.94
[INFO 2017-06-28 12:33:54,077 main.py:51] epoch 1133, training loss: 11203.71, average training loss: 10834.06, base loss: 14379.62
[INFO 2017-06-28 12:33:54,482 main.py:51] epoch 1134, training loss: 9110.36, average training loss: 10831.14, base loss: 14377.89
[INFO 2017-06-28 12:33:54,904 main.py:51] epoch 1135, training loss: 8507.06, average training loss: 10826.62, base loss: 14373.58
[INFO 2017-06-28 12:33:55,338 main.py:51] epoch 1136, training loss: 11835.66, average training loss: 10826.87, base loss: 14376.08
[INFO 2017-06-28 12:33:55,771 main.py:51] epoch 1137, training loss: 11094.09, average training loss: 10827.00, base loss: 14378.52
[INFO 2017-06-28 12:33:56,208 main.py:51] epoch 1138, training loss: 9308.35, average training loss: 10824.64, base loss: 14377.18
[INFO 2017-06-28 12:33:56,638 main.py:51] epoch 1139, training loss: 9908.30, average training loss: 10823.62, base loss: 14378.24
[INFO 2017-06-28 12:33:57,073 main.py:51] epoch 1140, training loss: 13027.27, average training loss: 10823.57, base loss: 14378.33
[INFO 2017-06-28 12:33:57,529 main.py:51] epoch 1141, training loss: 10119.13, average training loss: 10821.86, base loss: 14376.75
[INFO 2017-06-28 12:33:57,939 main.py:51] epoch 1142, training loss: 9587.43, average training loss: 10818.51, base loss: 14373.83
[INFO 2017-06-28 12:33:58,353 main.py:51] epoch 1143, training loss: 10085.29, average training loss: 10815.93, base loss: 14371.75
[INFO 2017-06-28 12:33:58,792 main.py:51] epoch 1144, training loss: 11588.91, average training loss: 10816.13, base loss: 14374.61
[INFO 2017-06-28 12:33:59,215 main.py:51] epoch 1145, training loss: 10573.32, average training loss: 10814.16, base loss: 14372.43
[INFO 2017-06-28 12:33:59,629 main.py:51] epoch 1146, training loss: 11729.24, average training loss: 10813.48, base loss: 14371.70
[INFO 2017-06-28 12:34:00,071 main.py:51] epoch 1147, training loss: 10379.17, average training loss: 10812.29, base loss: 14371.49
[INFO 2017-06-28 12:34:00,524 main.py:51] epoch 1148, training loss: 10365.99, average training loss: 10811.18, base loss: 14371.96
[INFO 2017-06-28 12:34:00,956 main.py:51] epoch 1149, training loss: 11194.00, average training loss: 10812.09, base loss: 14376.08
[INFO 2017-06-28 12:34:01,410 main.py:51] epoch 1150, training loss: 10302.07, average training loss: 10811.35, base loss: 14377.06
[INFO 2017-06-28 12:34:01,841 main.py:51] epoch 1151, training loss: 9416.78, average training loss: 10809.06, base loss: 14375.39
[INFO 2017-06-28 12:34:02,274 main.py:51] epoch 1152, training loss: 8870.33, average training loss: 10804.81, base loss: 14370.52
[INFO 2017-06-28 12:34:02,705 main.py:51] epoch 1153, training loss: 10822.14, average training loss: 10804.52, base loss: 14371.95
[INFO 2017-06-28 12:34:03,127 main.py:51] epoch 1154, training loss: 10415.47, average training loss: 10801.75, base loss: 14370.48
[INFO 2017-06-28 12:34:03,556 main.py:51] epoch 1155, training loss: 10621.26, average training loss: 10797.86, base loss: 14367.79
[INFO 2017-06-28 12:34:03,989 main.py:51] epoch 1156, training loss: 12653.55, average training loss: 10797.88, base loss: 14369.67
[INFO 2017-06-28 12:34:04,420 main.py:51] epoch 1157, training loss: 12116.81, average training loss: 10797.88, base loss: 14372.59
[INFO 2017-06-28 12:34:04,846 main.py:51] epoch 1158, training loss: 9944.49, average training loss: 10795.58, base loss: 14370.15
[INFO 2017-06-28 12:34:05,265 main.py:51] epoch 1159, training loss: 10044.86, average training loss: 10794.14, base loss: 14370.08
[INFO 2017-06-28 12:34:05,695 main.py:51] epoch 1160, training loss: 11943.78, average training loss: 10794.01, base loss: 14371.96
[INFO 2017-06-28 12:34:06,140 main.py:51] epoch 1161, training loss: 10803.59, average training loss: 10791.45, base loss: 14370.75
[INFO 2017-06-28 12:34:06,559 main.py:51] epoch 1162, training loss: 10977.80, average training loss: 10791.49, base loss: 14372.50
[INFO 2017-06-28 12:34:06,982 main.py:51] epoch 1163, training loss: 11534.25, average training loss: 10791.35, base loss: 14373.82
[INFO 2017-06-28 12:34:07,395 main.py:51] epoch 1164, training loss: 9224.31, average training loss: 10789.48, base loss: 14372.19
[INFO 2017-06-28 12:34:07,812 main.py:51] epoch 1165, training loss: 12081.82, average training loss: 10789.15, base loss: 14374.75
[INFO 2017-06-28 12:34:08,223 main.py:51] epoch 1166, training loss: 11997.80, average training loss: 10788.23, base loss: 14376.37
[INFO 2017-06-28 12:34:08,654 main.py:51] epoch 1167, training loss: 10230.46, average training loss: 10786.01, base loss: 14375.07
[INFO 2017-06-28 12:34:09,090 main.py:51] epoch 1168, training loss: 9384.62, average training loss: 10783.60, base loss: 14372.26
[INFO 2017-06-28 12:34:09,504 main.py:51] epoch 1169, training loss: 10310.05, average training loss: 10783.24, base loss: 14373.24
[INFO 2017-06-28 12:34:09,931 main.py:51] epoch 1170, training loss: 10498.28, average training loss: 10780.59, base loss: 14372.11
[INFO 2017-06-28 12:34:10,364 main.py:51] epoch 1171, training loss: 9309.20, average training loss: 10777.50, base loss: 14367.81
[INFO 2017-06-28 12:34:10,796 main.py:51] epoch 1172, training loss: 11159.46, average training loss: 10776.46, base loss: 14369.59
[INFO 2017-06-28 12:34:11,224 main.py:51] epoch 1173, training loss: 11258.22, average training loss: 10775.79, base loss: 14369.79
[INFO 2017-06-28 12:34:11,650 main.py:51] epoch 1174, training loss: 8850.36, average training loss: 10773.10, base loss: 14367.33
[INFO 2017-06-28 12:34:12,079 main.py:51] epoch 1175, training loss: 10526.36, average training loss: 10773.11, base loss: 14368.32
[INFO 2017-06-28 12:34:12,501 main.py:51] epoch 1176, training loss: 11016.98, average training loss: 10772.40, base loss: 14369.33
[INFO 2017-06-28 12:34:12,905 main.py:51] epoch 1177, training loss: 10958.68, average training loss: 10770.74, base loss: 14369.21
[INFO 2017-06-28 12:34:13,354 main.py:51] epoch 1178, training loss: 10488.49, average training loss: 10770.63, base loss: 14371.56
[INFO 2017-06-28 12:34:13,793 main.py:51] epoch 1179, training loss: 11161.24, average training loss: 10769.61, base loss: 14371.96
[INFO 2017-06-28 12:34:14,197 main.py:51] epoch 1180, training loss: 9851.58, average training loss: 10765.94, base loss: 14368.58
[INFO 2017-06-28 12:34:14,616 main.py:51] epoch 1181, training loss: 10827.65, average training loss: 10765.44, base loss: 14369.58
[INFO 2017-06-28 12:34:15,034 main.py:51] epoch 1182, training loss: 12856.39, average training loss: 10766.44, base loss: 14371.60
[INFO 2017-06-28 12:34:15,455 main.py:51] epoch 1183, training loss: 10424.15, average training loss: 10765.46, base loss: 14372.81
[INFO 2017-06-28 12:34:15,891 main.py:51] epoch 1184, training loss: 9627.11, average training loss: 10763.67, base loss: 14372.19
[INFO 2017-06-28 12:34:16,310 main.py:51] epoch 1185, training loss: 9826.93, average training loss: 10762.93, base loss: 14372.49
[INFO 2017-06-28 12:34:16,736 main.py:51] epoch 1186, training loss: 11587.47, average training loss: 10763.10, base loss: 14374.26
[INFO 2017-06-28 12:34:17,170 main.py:51] epoch 1187, training loss: 11541.31, average training loss: 10763.86, base loss: 14377.46
[INFO 2017-06-28 12:34:17,620 main.py:51] epoch 1188, training loss: 11464.70, average training loss: 10764.10, base loss: 14378.40
[INFO 2017-06-28 12:34:18,048 main.py:51] epoch 1189, training loss: 9582.07, average training loss: 10761.91, base loss: 14376.30
[INFO 2017-06-28 12:34:18,469 main.py:51] epoch 1190, training loss: 9999.84, average training loss: 10762.16, base loss: 14377.36
[INFO 2017-06-28 12:34:18,907 main.py:51] epoch 1191, training loss: 9875.88, average training loss: 10761.68, base loss: 14378.80
[INFO 2017-06-28 12:34:19,326 main.py:51] epoch 1192, training loss: 11794.00, average training loss: 10761.09, base loss: 14379.23
[INFO 2017-06-28 12:34:19,737 main.py:51] epoch 1193, training loss: 10321.28, average training loss: 10760.37, base loss: 14379.19
[INFO 2017-06-28 12:34:20,163 main.py:51] epoch 1194, training loss: 12158.61, average training loss: 10760.99, base loss: 14382.34
[INFO 2017-06-28 12:34:20,591 main.py:51] epoch 1195, training loss: 10350.01, average training loss: 10760.87, base loss: 14384.17
[INFO 2017-06-28 12:34:21,067 main.py:51] epoch 1196, training loss: 9827.98, average training loss: 10758.91, base loss: 14383.22
[INFO 2017-06-28 12:34:21,495 main.py:51] epoch 1197, training loss: 11081.10, average training loss: 10760.52, base loss: 14387.64
[INFO 2017-06-28 12:34:21,925 main.py:51] epoch 1198, training loss: 11418.86, average training loss: 10759.36, base loss: 14388.42
[INFO 2017-06-28 12:34:22,334 main.py:51] epoch 1199, training loss: 10020.29, average training loss: 10756.82, base loss: 14387.06
[INFO 2017-06-28 12:34:22,334 main.py:53] epoch 1199, testing
[INFO 2017-06-28 12:34:24,306 main.py:105] average testing loss: 10718.55, base loss: 14364.71
[INFO 2017-06-28 12:34:24,306 main.py:106] improve_loss: 3646.16, improve_percent: 0.25
[INFO 2017-06-28 12:34:24,307 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:34:24,751 main.py:51] epoch 1200, training loss: 10034.84, average training loss: 10756.48, base loss: 14387.01
[INFO 2017-06-28 12:34:25,177 main.py:51] epoch 1201, training loss: 10523.98, average training loss: 10754.68, base loss: 14385.93
[INFO 2017-06-28 12:34:25,609 main.py:51] epoch 1202, training loss: 10565.59, average training loss: 10754.62, base loss: 14386.86
[INFO 2017-06-28 12:34:26,029 main.py:51] epoch 1203, training loss: 9393.62, average training loss: 10752.88, base loss: 14386.33
[INFO 2017-06-28 12:34:26,452 main.py:51] epoch 1204, training loss: 10181.72, average training loss: 10752.37, base loss: 14386.91
[INFO 2017-06-28 12:34:26,873 main.py:51] epoch 1205, training loss: 11388.79, average training loss: 10753.50, base loss: 14390.78
[INFO 2017-06-28 12:34:27,300 main.py:51] epoch 1206, training loss: 10467.70, average training loss: 10754.41, base loss: 14394.90
[INFO 2017-06-28 12:34:27,734 main.py:51] epoch 1207, training loss: 11743.34, average training loss: 10754.70, base loss: 14397.00
[INFO 2017-06-28 12:34:28,164 main.py:51] epoch 1208, training loss: 11319.70, average training loss: 10755.25, base loss: 14399.80
[INFO 2017-06-28 12:34:28,577 main.py:51] epoch 1209, training loss: 9951.28, average training loss: 10752.96, base loss: 14397.22
[INFO 2017-06-28 12:34:29,010 main.py:51] epoch 1210, training loss: 9707.15, average training loss: 10752.47, base loss: 14398.29
[INFO 2017-06-28 12:34:29,426 main.py:51] epoch 1211, training loss: 9670.62, average training loss: 10751.91, base loss: 14397.55
[INFO 2017-06-28 12:34:29,850 main.py:51] epoch 1212, training loss: 9526.35, average training loss: 10750.71, base loss: 14397.92
[INFO 2017-06-28 12:34:30,269 main.py:51] epoch 1213, training loss: 10962.90, average training loss: 10749.85, base loss: 14397.59
[INFO 2017-06-28 12:34:30,716 main.py:51] epoch 1214, training loss: 10279.68, average training loss: 10750.20, base loss: 14399.94
[INFO 2017-06-28 12:34:31,147 main.py:51] epoch 1215, training loss: 10992.68, average training loss: 10749.85, base loss: 14401.81
[INFO 2017-06-28 12:34:31,578 main.py:51] epoch 1216, training loss: 10514.58, average training loss: 10749.54, base loss: 14402.05
[INFO 2017-06-28 12:34:32,000 main.py:51] epoch 1217, training loss: 10071.38, average training loss: 10745.49, base loss: 14398.34
[INFO 2017-06-28 12:34:32,448 main.py:51] epoch 1218, training loss: 10366.70, average training loss: 10745.49, base loss: 14401.40
[INFO 2017-06-28 12:34:32,870 main.py:51] epoch 1219, training loss: 10973.32, average training loss: 10746.21, base loss: 14403.90
[INFO 2017-06-28 12:34:33,287 main.py:51] epoch 1220, training loss: 9873.71, average training loss: 10744.15, base loss: 14401.70
[INFO 2017-06-28 12:34:33,696 main.py:51] epoch 1221, training loss: 12042.73, average training loss: 10746.36, base loss: 14406.39
[INFO 2017-06-28 12:34:34,096 main.py:51] epoch 1222, training loss: 10873.74, average training loss: 10747.04, base loss: 14408.46
[INFO 2017-06-28 12:34:34,525 main.py:51] epoch 1223, training loss: 8899.91, average training loss: 10744.95, base loss: 14405.43
[INFO 2017-06-28 12:34:34,989 main.py:51] epoch 1224, training loss: 10290.20, average training loss: 10743.35, base loss: 14405.61
[INFO 2017-06-28 12:34:35,410 main.py:51] epoch 1225, training loss: 11788.89, average training loss: 10744.18, base loss: 14407.84
[INFO 2017-06-28 12:34:35,835 main.py:51] epoch 1226, training loss: 9721.87, average training loss: 10740.93, base loss: 14404.29
[INFO 2017-06-28 12:34:36,246 main.py:51] epoch 1227, training loss: 9993.32, average training loss: 10739.13, base loss: 14403.36
[INFO 2017-06-28 12:34:36,680 main.py:51] epoch 1228, training loss: 10717.48, average training loss: 10736.84, base loss: 14401.96
[INFO 2017-06-28 12:34:37,102 main.py:51] epoch 1229, training loss: 10352.20, average training loss: 10736.35, base loss: 14402.35
[INFO 2017-06-28 12:34:37,507 main.py:51] epoch 1230, training loss: 8729.10, average training loss: 10732.59, base loss: 14398.81
[INFO 2017-06-28 12:34:37,917 main.py:51] epoch 1231, training loss: 9909.41, average training loss: 10731.24, base loss: 14398.98
[INFO 2017-06-28 12:34:38,349 main.py:51] epoch 1232, training loss: 9670.87, average training loss: 10729.62, base loss: 14397.05
[INFO 2017-06-28 12:34:38,805 main.py:51] epoch 1233, training loss: 10137.22, average training loss: 10728.63, base loss: 14397.45
[INFO 2017-06-28 12:34:39,244 main.py:51] epoch 1234, training loss: 10791.57, average training loss: 10728.60, base loss: 14399.39
[INFO 2017-06-28 12:34:39,679 main.py:51] epoch 1235, training loss: 9453.51, average training loss: 10728.20, base loss: 14399.46
[INFO 2017-06-28 12:34:40,105 main.py:51] epoch 1236, training loss: 10289.77, average training loss: 10727.78, base loss: 14399.70
[INFO 2017-06-28 12:34:40,536 main.py:51] epoch 1237, training loss: 9268.46, average training loss: 10727.61, base loss: 14400.41
[INFO 2017-06-28 12:34:40,951 main.py:51] epoch 1238, training loss: 11382.07, average training loss: 10726.07, base loss: 14399.25
[INFO 2017-06-28 12:34:41,365 main.py:51] epoch 1239, training loss: 9354.60, average training loss: 10723.73, base loss: 14397.62
[INFO 2017-06-28 12:34:41,777 main.py:51] epoch 1240, training loss: 9725.06, average training loss: 10721.71, base loss: 14397.09
[INFO 2017-06-28 12:34:42,214 main.py:51] epoch 1241, training loss: 9966.58, average training loss: 10722.13, base loss: 14398.36
[INFO 2017-06-28 12:34:42,634 main.py:51] epoch 1242, training loss: 10967.45, average training loss: 10719.50, base loss: 14395.65
[INFO 2017-06-28 12:34:43,075 main.py:51] epoch 1243, training loss: 10070.76, average training loss: 10716.90, base loss: 14394.83
[INFO 2017-06-28 12:34:43,500 main.py:51] epoch 1244, training loss: 9791.33, average training loss: 10715.47, base loss: 14394.54
[INFO 2017-06-28 12:34:43,930 main.py:51] epoch 1245, training loss: 9460.63, average training loss: 10713.83, base loss: 14393.27
[INFO 2017-06-28 12:34:44,370 main.py:51] epoch 1246, training loss: 9777.54, average training loss: 10712.23, base loss: 14392.56
[INFO 2017-06-28 12:34:44,794 main.py:51] epoch 1247, training loss: 10554.95, average training loss: 10710.99, base loss: 14392.20
[INFO 2017-06-28 12:34:45,224 main.py:51] epoch 1248, training loss: 9982.93, average training loss: 10711.10, base loss: 14393.33
[INFO 2017-06-28 12:34:45,664 main.py:51] epoch 1249, training loss: 10821.20, average training loss: 10711.68, base loss: 14394.96
[INFO 2017-06-28 12:34:46,109 main.py:51] epoch 1250, training loss: 10721.55, average training loss: 10710.86, base loss: 14394.39
[INFO 2017-06-28 12:34:46,536 main.py:51] epoch 1251, training loss: 11109.76, average training loss: 10711.51, base loss: 14396.30
[INFO 2017-06-28 12:34:46,952 main.py:51] epoch 1252, training loss: 9156.78, average training loss: 10709.98, base loss: 14396.36
[INFO 2017-06-28 12:34:47,398 main.py:51] epoch 1253, training loss: 11919.65, average training loss: 10711.09, base loss: 14398.24
[INFO 2017-06-28 12:34:47,827 main.py:51] epoch 1254, training loss: 10345.12, average training loss: 10710.64, base loss: 14398.80
[INFO 2017-06-28 12:34:48,242 main.py:51] epoch 1255, training loss: 9783.73, average training loss: 10710.23, base loss: 14399.45
[INFO 2017-06-28 12:34:48,694 main.py:51] epoch 1256, training loss: 11730.33, average training loss: 10709.18, base loss: 14399.87
[INFO 2017-06-28 12:34:49,123 main.py:51] epoch 1257, training loss: 10812.42, average training loss: 10709.14, base loss: 14400.75
[INFO 2017-06-28 12:34:49,552 main.py:51] epoch 1258, training loss: 9186.72, average training loss: 10706.94, base loss: 14398.48
[INFO 2017-06-28 12:34:49,977 main.py:51] epoch 1259, training loss: 10483.57, average training loss: 10708.28, base loss: 14402.45
[INFO 2017-06-28 12:34:50,409 main.py:51] epoch 1260, training loss: 9201.82, average training loss: 10706.01, base loss: 14400.57
[INFO 2017-06-28 12:34:50,825 main.py:51] epoch 1261, training loss: 9454.48, average training loss: 10704.49, base loss: 14399.71
[INFO 2017-06-28 12:34:51,256 main.py:51] epoch 1262, training loss: 10162.23, average training loss: 10703.62, base loss: 14400.33
[INFO 2017-06-28 12:34:51,682 main.py:51] epoch 1263, training loss: 9265.29, average training loss: 10702.11, base loss: 14399.39
[INFO 2017-06-28 12:34:52,108 main.py:51] epoch 1264, training loss: 10755.43, average training loss: 10701.40, base loss: 14398.71
[INFO 2017-06-28 12:34:52,535 main.py:51] epoch 1265, training loss: 8995.52, average training loss: 10698.39, base loss: 14396.98
[INFO 2017-06-28 12:34:52,960 main.py:51] epoch 1266, training loss: 10106.40, average training loss: 10698.36, base loss: 14398.85
[INFO 2017-06-28 12:34:53,377 main.py:51] epoch 1267, training loss: 11489.11, average training loss: 10700.38, base loss: 14404.16
[INFO 2017-06-28 12:34:53,807 main.py:51] epoch 1268, training loss: 9842.62, average training loss: 10699.19, base loss: 14403.96
[INFO 2017-06-28 12:34:54,240 main.py:51] epoch 1269, training loss: 10081.56, average training loss: 10698.04, base loss: 14405.17
[INFO 2017-06-28 12:34:54,684 main.py:51] epoch 1270, training loss: 11766.25, average training loss: 10697.63, base loss: 14405.92
[INFO 2017-06-28 12:34:55,096 main.py:51] epoch 1271, training loss: 10102.36, average training loss: 10696.98, base loss: 14405.00
[INFO 2017-06-28 12:34:55,528 main.py:51] epoch 1272, training loss: 9093.60, average training loss: 10694.09, base loss: 14404.45
[INFO 2017-06-28 12:34:55,952 main.py:51] epoch 1273, training loss: 12635.04, average training loss: 10696.84, base loss: 14409.94
[INFO 2017-06-28 12:34:56,368 main.py:51] epoch 1274, training loss: 9623.06, average training loss: 10696.78, base loss: 14410.27
[INFO 2017-06-28 12:34:56,791 main.py:51] epoch 1275, training loss: 11115.80, average training loss: 10694.91, base loss: 14408.59
[INFO 2017-06-28 12:34:57,236 main.py:51] epoch 1276, training loss: 11867.94, average training loss: 10694.52, base loss: 14410.43
[INFO 2017-06-28 12:34:57,667 main.py:51] epoch 1277, training loss: 10643.05, average training loss: 10693.58, base loss: 14412.10
[INFO 2017-06-28 12:34:58,088 main.py:51] epoch 1278, training loss: 11050.60, average training loss: 10692.81, base loss: 14411.68
[INFO 2017-06-28 12:34:58,502 main.py:51] epoch 1279, training loss: 8973.89, average training loss: 10690.08, base loss: 14409.86
[INFO 2017-06-28 12:34:58,924 main.py:51] epoch 1280, training loss: 9225.95, average training loss: 10685.96, base loss: 14405.47
[INFO 2017-06-28 12:34:59,330 main.py:51] epoch 1281, training loss: 10140.10, average training loss: 10685.36, base loss: 14406.37
[INFO 2017-06-28 12:34:59,758 main.py:51] epoch 1282, training loss: 9152.78, average training loss: 10682.59, base loss: 14403.76
[INFO 2017-06-28 12:35:00,180 main.py:51] epoch 1283, training loss: 9839.75, average training loss: 10679.71, base loss: 14402.02
[INFO 2017-06-28 12:35:00,607 main.py:51] epoch 1284, training loss: 11762.25, average training loss: 10681.60, base loss: 14406.02
[INFO 2017-06-28 12:35:01,017 main.py:51] epoch 1285, training loss: 10970.40, average training loss: 10683.31, base loss: 14409.56
[INFO 2017-06-28 12:35:01,437 main.py:51] epoch 1286, training loss: 10696.39, average training loss: 10682.79, base loss: 14409.84
[INFO 2017-06-28 12:35:01,869 main.py:51] epoch 1287, training loss: 10736.71, average training loss: 10683.36, base loss: 14411.51
[INFO 2017-06-28 12:35:02,291 main.py:51] epoch 1288, training loss: 10465.87, average training loss: 10683.54, base loss: 14412.29
[INFO 2017-06-28 12:35:02,716 main.py:51] epoch 1289, training loss: 9574.11, average training loss: 10682.57, base loss: 14410.82
[INFO 2017-06-28 12:35:03,171 main.py:51] epoch 1290, training loss: 11123.57, average training loss: 10681.41, base loss: 14410.76
[INFO 2017-06-28 12:35:03,570 main.py:51] epoch 1291, training loss: 10257.97, average training loss: 10681.03, base loss: 14412.08
[INFO 2017-06-28 12:35:04,006 main.py:51] epoch 1292, training loss: 9606.56, average training loss: 10678.81, base loss: 14409.89
[INFO 2017-06-28 12:35:04,461 main.py:51] epoch 1293, training loss: 10335.26, average training loss: 10678.66, base loss: 14410.75
[INFO 2017-06-28 12:35:04,879 main.py:51] epoch 1294, training loss: 10051.13, average training loss: 10676.51, base loss: 14408.30
[INFO 2017-06-28 12:35:05,310 main.py:51] epoch 1295, training loss: 9160.65, average training loss: 10673.99, base loss: 14405.85
[INFO 2017-06-28 12:35:05,729 main.py:51] epoch 1296, training loss: 8859.01, average training loss: 10672.73, base loss: 14406.02
[INFO 2017-06-28 12:35:06,166 main.py:51] epoch 1297, training loss: 11348.37, average training loss: 10674.22, base loss: 14408.32
[INFO 2017-06-28 12:35:06,603 main.py:51] epoch 1298, training loss: 11204.28, average training loss: 10675.15, base loss: 14412.09
[INFO 2017-06-28 12:35:07,035 main.py:51] epoch 1299, training loss: 9515.69, average training loss: 10673.96, base loss: 14411.78
[INFO 2017-06-28 12:35:07,035 main.py:53] epoch 1299, testing
[INFO 2017-06-28 12:35:09,051 main.py:105] average testing loss: 11545.42, base loss: 15401.39
[INFO 2017-06-28 12:35:09,051 main.py:106] improve_loss: 3855.97, improve_percent: 0.25
[INFO 2017-06-28 12:35:09,052 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:35:09,459 main.py:51] epoch 1300, training loss: 9298.25, average training loss: 10672.46, base loss: 14411.27
[INFO 2017-06-28 12:35:09,914 main.py:51] epoch 1301, training loss: 9827.09, average training loss: 10671.05, base loss: 14410.16
[INFO 2017-06-28 12:35:10,329 main.py:51] epoch 1302, training loss: 11249.35, average training loss: 10669.19, base loss: 14408.20
[INFO 2017-06-28 12:35:10,742 main.py:51] epoch 1303, training loss: 10425.39, average training loss: 10668.86, base loss: 14409.53
[INFO 2017-06-28 12:35:11,153 main.py:51] epoch 1304, training loss: 9435.92, average training loss: 10665.64, base loss: 14406.93
[INFO 2017-06-28 12:35:11,596 main.py:51] epoch 1305, training loss: 9777.92, average training loss: 10665.12, base loss: 14406.03
[INFO 2017-06-28 12:35:12,035 main.py:51] epoch 1306, training loss: 9870.21, average training loss: 10662.16, base loss: 14403.65
[INFO 2017-06-28 12:35:12,470 main.py:51] epoch 1307, training loss: 9554.78, average training loss: 10660.13, base loss: 14402.47
[INFO 2017-06-28 12:35:12,910 main.py:51] epoch 1308, training loss: 11185.28, average training loss: 10660.42, base loss: 14404.39
[INFO 2017-06-28 12:35:13,337 main.py:51] epoch 1309, training loss: 9166.23, average training loss: 10657.22, base loss: 14401.78
[INFO 2017-06-28 12:35:13,777 main.py:51] epoch 1310, training loss: 9640.96, average training loss: 10652.97, base loss: 14395.50
[INFO 2017-06-28 12:35:14,212 main.py:51] epoch 1311, training loss: 9548.45, average training loss: 10650.37, base loss: 14392.83
[INFO 2017-06-28 12:35:14,650 main.py:51] epoch 1312, training loss: 12457.20, average training loss: 10651.99, base loss: 14396.55
[INFO 2017-06-28 12:35:15,086 main.py:51] epoch 1313, training loss: 10531.52, average training loss: 10651.08, base loss: 14396.35
[INFO 2017-06-28 12:35:15,508 main.py:51] epoch 1314, training loss: 10389.08, average training loss: 10649.33, base loss: 14393.28
[INFO 2017-06-28 12:35:15,942 main.py:51] epoch 1315, training loss: 9937.49, average training loss: 10648.55, base loss: 14394.10
[INFO 2017-06-28 12:35:16,376 main.py:51] epoch 1316, training loss: 9877.43, average training loss: 10645.79, base loss: 14391.36
[INFO 2017-06-28 12:35:16,810 main.py:51] epoch 1317, training loss: 11377.98, average training loss: 10646.75, base loss: 14393.03
[INFO 2017-06-28 12:35:17,244 main.py:51] epoch 1318, training loss: 10812.30, average training loss: 10647.07, base loss: 14393.10
[INFO 2017-06-28 12:35:17,678 main.py:51] epoch 1319, training loss: 11345.63, average training loss: 10647.75, base loss: 14395.20
[INFO 2017-06-28 12:35:18,105 main.py:51] epoch 1320, training loss: 10577.57, average training loss: 10647.98, base loss: 14397.41
[INFO 2017-06-28 12:35:18,544 main.py:51] epoch 1321, training loss: 11319.72, average training loss: 10649.47, base loss: 14400.08
[INFO 2017-06-28 12:35:18,964 main.py:51] epoch 1322, training loss: 9248.00, average training loss: 10647.56, base loss: 14399.40
[INFO 2017-06-28 12:35:19,365 main.py:51] epoch 1323, training loss: 9807.14, average training loss: 10647.35, base loss: 14400.81
[INFO 2017-06-28 12:35:19,786 main.py:51] epoch 1324, training loss: 10119.61, average training loss: 10646.27, base loss: 14399.46
[INFO 2017-06-28 12:35:20,219 main.py:51] epoch 1325, training loss: 10433.59, average training loss: 10645.25, base loss: 14399.73
[INFO 2017-06-28 12:35:20,645 main.py:51] epoch 1326, training loss: 9143.30, average training loss: 10645.31, base loss: 14401.33
[INFO 2017-06-28 12:35:21,091 main.py:51] epoch 1327, training loss: 10783.20, average training loss: 10643.41, base loss: 14400.74
[INFO 2017-06-28 12:35:21,538 main.py:51] epoch 1328, training loss: 10260.91, average training loss: 10642.87, base loss: 14401.24
[INFO 2017-06-28 12:35:21,978 main.py:51] epoch 1329, training loss: 9428.31, average training loss: 10639.62, base loss: 14397.70
[INFO 2017-06-28 12:35:22,409 main.py:51] epoch 1330, training loss: 9710.67, average training loss: 10639.22, base loss: 14397.45
[INFO 2017-06-28 12:35:22,846 main.py:51] epoch 1331, training loss: 11879.54, average training loss: 10639.42, base loss: 14398.25
[INFO 2017-06-28 12:35:23,271 main.py:51] epoch 1332, training loss: 9244.55, average training loss: 10636.58, base loss: 14393.79
[INFO 2017-06-28 12:35:23,715 main.py:51] epoch 1333, training loss: 9501.77, average training loss: 10635.97, base loss: 14393.44
[INFO 2017-06-28 12:35:24,144 main.py:51] epoch 1334, training loss: 11427.71, average training loss: 10635.34, base loss: 14392.67
[INFO 2017-06-28 12:35:24,575 main.py:51] epoch 1335, training loss: 9940.87, average training loss: 10635.61, base loss: 14394.39
[INFO 2017-06-28 12:35:24,987 main.py:51] epoch 1336, training loss: 9146.69, average training loss: 10632.38, base loss: 14389.91
[INFO 2017-06-28 12:35:25,395 main.py:51] epoch 1337, training loss: 11016.79, average training loss: 10633.04, base loss: 14392.40
[INFO 2017-06-28 12:35:25,822 main.py:51] epoch 1338, training loss: 9071.25, average training loss: 10631.26, base loss: 14390.69
[INFO 2017-06-28 12:35:26,263 main.py:51] epoch 1339, training loss: 10606.89, average training loss: 10629.73, base loss: 14389.05
[INFO 2017-06-28 12:35:26,672 main.py:51] epoch 1340, training loss: 9280.78, average training loss: 10627.31, base loss: 14387.41
[INFO 2017-06-28 12:35:27,097 main.py:51] epoch 1341, training loss: 10269.60, average training loss: 10626.48, base loss: 14387.35
[INFO 2017-06-28 12:35:27,520 main.py:51] epoch 1342, training loss: 10436.85, average training loss: 10626.04, base loss: 14387.58
[INFO 2017-06-28 12:35:27,931 main.py:51] epoch 1343, training loss: 12307.17, average training loss: 10628.84, base loss: 14392.32
[INFO 2017-06-28 12:35:28,354 main.py:51] epoch 1344, training loss: 9592.41, average training loss: 10627.74, base loss: 14390.45
[INFO 2017-06-28 12:35:28,779 main.py:51] epoch 1345, training loss: 11043.67, average training loss: 10627.74, base loss: 14391.34
[INFO 2017-06-28 12:35:29,198 main.py:51] epoch 1346, training loss: 12036.08, average training loss: 10627.78, base loss: 14391.64
[INFO 2017-06-28 12:35:29,614 main.py:51] epoch 1347, training loss: 10544.33, average training loss: 10627.15, base loss: 14392.52
[INFO 2017-06-28 12:35:30,051 main.py:51] epoch 1348, training loss: 10732.86, average training loss: 10626.23, base loss: 14392.65
[INFO 2017-06-28 12:35:30,487 main.py:51] epoch 1349, training loss: 10223.46, average training loss: 10625.58, base loss: 14393.04
[INFO 2017-06-28 12:35:30,907 main.py:51] epoch 1350, training loss: 9839.47, average training loss: 10623.39, base loss: 14391.32
[INFO 2017-06-28 12:35:31,337 main.py:51] epoch 1351, training loss: 11486.23, average training loss: 10623.26, base loss: 14391.98
[INFO 2017-06-28 12:35:31,747 main.py:51] epoch 1352, training loss: 11220.22, average training loss: 10623.21, base loss: 14391.25
[INFO 2017-06-28 12:35:32,161 main.py:51] epoch 1353, training loss: 9670.39, average training loss: 10622.86, base loss: 14391.84
[INFO 2017-06-28 12:35:32,602 main.py:51] epoch 1354, training loss: 10502.84, average training loss: 10621.97, base loss: 14389.53
[INFO 2017-06-28 12:35:33,046 main.py:51] epoch 1355, training loss: 11717.62, average training loss: 10622.83, base loss: 14390.68
[INFO 2017-06-28 12:35:33,468 main.py:51] epoch 1356, training loss: 12326.15, average training loss: 10625.26, base loss: 14394.37
[INFO 2017-06-28 12:35:33,890 main.py:51] epoch 1357, training loss: 11190.99, average training loss: 10626.26, base loss: 14396.58
[INFO 2017-06-28 12:35:34,302 main.py:51] epoch 1358, training loss: 10205.17, average training loss: 10626.18, base loss: 14397.85
[INFO 2017-06-28 12:35:34,713 main.py:51] epoch 1359, training loss: 9899.51, average training loss: 10625.57, base loss: 14396.84
[INFO 2017-06-28 12:35:35,137 main.py:51] epoch 1360, training loss: 11595.49, average training loss: 10627.15, base loss: 14401.21
[INFO 2017-06-28 12:35:35,591 main.py:51] epoch 1361, training loss: 9565.04, average training loss: 10623.99, base loss: 14396.56
[INFO 2017-06-28 12:35:36,032 main.py:51] epoch 1362, training loss: 9269.47, average training loss: 10623.46, base loss: 14396.85
[INFO 2017-06-28 12:35:36,468 main.py:51] epoch 1363, training loss: 9387.13, average training loss: 10622.04, base loss: 14395.69
[INFO 2017-06-28 12:35:36,881 main.py:51] epoch 1364, training loss: 10855.19, average training loss: 10620.48, base loss: 14396.14
[INFO 2017-06-28 12:35:37,339 main.py:51] epoch 1365, training loss: 11147.99, average training loss: 10621.30, base loss: 14397.62
[INFO 2017-06-28 12:35:37,759 main.py:51] epoch 1366, training loss: 10825.97, average training loss: 10622.86, base loss: 14398.83
[INFO 2017-06-28 12:35:38,188 main.py:51] epoch 1367, training loss: 12361.98, average training loss: 10626.20, base loss: 14404.01
[INFO 2017-06-28 12:35:38,634 main.py:51] epoch 1368, training loss: 10414.85, average training loss: 10625.89, base loss: 14405.62
[INFO 2017-06-28 12:35:39,073 main.py:51] epoch 1369, training loss: 9643.91, average training loss: 10623.30, base loss: 14403.76
[INFO 2017-06-28 12:35:39,512 main.py:51] epoch 1370, training loss: 10850.32, average training loss: 10625.03, base loss: 14407.48
[INFO 2017-06-28 12:35:39,967 main.py:51] epoch 1371, training loss: 10343.58, average training loss: 10624.20, base loss: 14408.20
[INFO 2017-06-28 12:35:40,397 main.py:51] epoch 1372, training loss: 10618.76, average training loss: 10624.14, base loss: 14409.52
[INFO 2017-06-28 12:35:40,830 main.py:51] epoch 1373, training loss: 9781.49, average training loss: 10624.01, base loss: 14411.96
[INFO 2017-06-28 12:35:41,277 main.py:51] epoch 1374, training loss: 9097.79, average training loss: 10623.24, base loss: 14412.24
[INFO 2017-06-28 12:35:41,685 main.py:51] epoch 1375, training loss: 11835.88, average training loss: 10624.57, base loss: 14413.40
[INFO 2017-06-28 12:35:42,095 main.py:51] epoch 1376, training loss: 10714.97, average training loss: 10625.05, base loss: 14413.69
[INFO 2017-06-28 12:35:42,529 main.py:51] epoch 1377, training loss: 10352.09, average training loss: 10624.99, base loss: 14413.71
[INFO 2017-06-28 12:35:42,942 main.py:51] epoch 1378, training loss: 10163.41, average training loss: 10623.95, base loss: 14412.82
[INFO 2017-06-28 12:35:43,368 main.py:51] epoch 1379, training loss: 11016.74, average training loss: 10624.28, base loss: 14413.43
[INFO 2017-06-28 12:35:43,793 main.py:51] epoch 1380, training loss: 10244.40, average training loss: 10622.32, base loss: 14412.18
[INFO 2017-06-28 12:35:44,198 main.py:51] epoch 1381, training loss: 10152.46, average training loss: 10622.13, base loss: 14412.85
[INFO 2017-06-28 12:35:44,629 main.py:51] epoch 1382, training loss: 10371.83, average training loss: 10617.38, base loss: 14408.05
[INFO 2017-06-28 12:35:45,061 main.py:51] epoch 1383, training loss: 11210.63, average training loss: 10616.49, base loss: 14404.87
[INFO 2017-06-28 12:35:45,468 main.py:51] epoch 1384, training loss: 9877.59, average training loss: 10615.08, base loss: 14402.78
[INFO 2017-06-28 12:35:45,903 main.py:51] epoch 1385, training loss: 10032.41, average training loss: 10613.34, base loss: 14402.70
[INFO 2017-06-28 12:35:46,339 main.py:51] epoch 1386, training loss: 10128.12, average training loss: 10613.18, base loss: 14403.59
[INFO 2017-06-28 12:35:46,764 main.py:51] epoch 1387, training loss: 10173.05, average training loss: 10610.68, base loss: 14402.35
[INFO 2017-06-28 12:35:47,171 main.py:51] epoch 1388, training loss: 8725.23, average training loss: 10608.51, base loss: 14399.78
[INFO 2017-06-28 12:35:47,604 main.py:51] epoch 1389, training loss: 9255.07, average training loss: 10606.89, base loss: 14398.41
[INFO 2017-06-28 12:35:48,043 main.py:51] epoch 1390, training loss: 11358.92, average training loss: 10607.67, base loss: 14400.52
[INFO 2017-06-28 12:35:48,464 main.py:51] epoch 1391, training loss: 11140.43, average training loss: 10608.23, base loss: 14401.96
[INFO 2017-06-28 12:35:48,910 main.py:51] epoch 1392, training loss: 9478.30, average training loss: 10606.59, base loss: 14399.35
[INFO 2017-06-28 12:35:49,353 main.py:51] epoch 1393, training loss: 11002.71, average training loss: 10607.76, base loss: 14401.71
[INFO 2017-06-28 12:35:49,767 main.py:51] epoch 1394, training loss: 9519.61, average training loss: 10606.22, base loss: 14399.78
[INFO 2017-06-28 12:35:50,192 main.py:51] epoch 1395, training loss: 9418.07, average training loss: 10605.04, base loss: 14397.97
[INFO 2017-06-28 12:35:50,610 main.py:51] epoch 1396, training loss: 9185.29, average training loss: 10601.40, base loss: 14394.11
[INFO 2017-06-28 12:35:51,042 main.py:51] epoch 1397, training loss: 10429.80, average training loss: 10600.62, base loss: 14394.41
[INFO 2017-06-28 12:35:51,508 main.py:51] epoch 1398, training loss: 9269.20, average training loss: 10599.36, base loss: 14393.19
[INFO 2017-06-28 12:35:51,928 main.py:51] epoch 1399, training loss: 9636.49, average training loss: 10597.78, base loss: 14391.85
[INFO 2017-06-28 12:35:51,928 main.py:53] epoch 1399, testing
[INFO 2017-06-28 12:35:53,931 main.py:105] average testing loss: 11322.73, base loss: 15110.33
[INFO 2017-06-28 12:35:53,932 main.py:106] improve_loss: 3787.60, improve_percent: 0.25
[INFO 2017-06-28 12:35:53,932 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:35:54,368 main.py:51] epoch 1400, training loss: 8859.83, average training loss: 10596.75, base loss: 14391.70
[INFO 2017-06-28 12:35:54,777 main.py:51] epoch 1401, training loss: 10039.18, average training loss: 10595.18, base loss: 14389.99
[INFO 2017-06-28 12:35:55,211 main.py:51] epoch 1402, training loss: 8053.06, average training loss: 10592.77, base loss: 14387.79
[INFO 2017-06-28 12:35:55,643 main.py:51] epoch 1403, training loss: 10482.94, average training loss: 10592.60, base loss: 14387.76
[INFO 2017-06-28 12:35:56,072 main.py:51] epoch 1404, training loss: 9501.30, average training loss: 10589.14, base loss: 14384.65
[INFO 2017-06-28 12:35:56,494 main.py:51] epoch 1405, training loss: 10294.26, average training loss: 10590.05, base loss: 14386.87
[INFO 2017-06-28 12:35:56,926 main.py:51] epoch 1406, training loss: 9945.94, average training loss: 10587.78, base loss: 14384.33
[INFO 2017-06-28 12:35:57,365 main.py:51] epoch 1407, training loss: 11015.40, average training loss: 10588.90, base loss: 14386.90
[INFO 2017-06-28 12:35:57,779 main.py:51] epoch 1408, training loss: 10130.65, average training loss: 10587.68, base loss: 14386.91
[INFO 2017-06-28 12:35:58,209 main.py:51] epoch 1409, training loss: 9510.30, average training loss: 10586.83, base loss: 14386.96
[INFO 2017-06-28 12:35:58,661 main.py:51] epoch 1410, training loss: 9854.08, average training loss: 10585.86, base loss: 14385.79
[INFO 2017-06-28 12:35:59,074 main.py:51] epoch 1411, training loss: 11640.02, average training loss: 10586.84, base loss: 14387.38
[INFO 2017-06-28 12:35:59,503 main.py:51] epoch 1412, training loss: 9641.39, average training loss: 10587.00, base loss: 14386.94
[INFO 2017-06-28 12:35:59,931 main.py:51] epoch 1413, training loss: 10386.44, average training loss: 10585.23, base loss: 14386.46
[INFO 2017-06-28 12:36:00,361 main.py:51] epoch 1414, training loss: 10500.42, average training loss: 10583.52, base loss: 14384.52
[INFO 2017-06-28 12:36:00,802 main.py:51] epoch 1415, training loss: 10914.74, average training loss: 10584.00, base loss: 14385.81
[INFO 2017-06-28 12:36:01,249 main.py:51] epoch 1416, training loss: 9226.02, average training loss: 10581.65, base loss: 14383.21
[INFO 2017-06-28 12:36:01,684 main.py:51] epoch 1417, training loss: 9452.21, average training loss: 10580.41, base loss: 14382.56
[INFO 2017-06-28 12:36:02,137 main.py:51] epoch 1418, training loss: 9818.84, average training loss: 10579.71, base loss: 14383.25
[INFO 2017-06-28 12:36:02,564 main.py:51] epoch 1419, training loss: 9262.75, average training loss: 10576.16, base loss: 14380.20
[INFO 2017-06-28 12:36:03,000 main.py:51] epoch 1420, training loss: 10010.76, average training loss: 10576.40, base loss: 14381.66
[INFO 2017-06-28 12:36:03,434 main.py:51] epoch 1421, training loss: 10957.20, average training loss: 10577.16, base loss: 14383.75
[INFO 2017-06-28 12:36:03,863 main.py:51] epoch 1422, training loss: 10400.74, average training loss: 10576.53, base loss: 14384.59
[INFO 2017-06-28 12:36:04,305 main.py:51] epoch 1423, training loss: 10257.79, average training loss: 10576.17, base loss: 14384.43
[INFO 2017-06-28 12:36:04,733 main.py:51] epoch 1424, training loss: 10423.69, average training loss: 10575.59, base loss: 14386.06
[INFO 2017-06-28 12:36:05,161 main.py:51] epoch 1425, training loss: 9742.18, average training loss: 10575.34, base loss: 14386.95
[INFO 2017-06-28 12:36:05,600 main.py:51] epoch 1426, training loss: 10215.92, average training loss: 10574.21, base loss: 14386.62
[INFO 2017-06-28 12:36:06,048 main.py:51] epoch 1427, training loss: 10035.27, average training loss: 10574.01, base loss: 14386.36
[INFO 2017-06-28 12:36:06,465 main.py:51] epoch 1428, training loss: 9440.19, average training loss: 10571.42, base loss: 14385.65
[INFO 2017-06-28 12:36:06,892 main.py:51] epoch 1429, training loss: 9548.44, average training loss: 10570.81, base loss: 14383.45
[INFO 2017-06-28 12:36:07,320 main.py:51] epoch 1430, training loss: 10578.49, average training loss: 10569.24, base loss: 14380.49
[INFO 2017-06-28 12:36:07,753 main.py:51] epoch 1431, training loss: 10166.10, average training loss: 10570.04, base loss: 14382.55
[INFO 2017-06-28 12:36:08,180 main.py:51] epoch 1432, training loss: 10813.45, average training loss: 10572.25, base loss: 14385.48
[INFO 2017-06-28 12:36:08,608 main.py:51] epoch 1433, training loss: 10505.32, average training loss: 10572.49, base loss: 14388.26
[INFO 2017-06-28 12:36:09,037 main.py:51] epoch 1434, training loss: 12495.25, average training loss: 10574.98, base loss: 14391.94
[INFO 2017-06-28 12:36:09,457 main.py:51] epoch 1435, training loss: 10069.21, average training loss: 10574.53, base loss: 14392.69
[INFO 2017-06-28 12:36:09,871 main.py:51] epoch 1436, training loss: 9690.33, average training loss: 10573.19, base loss: 14390.75
[INFO 2017-06-28 12:36:10,295 main.py:51] epoch 1437, training loss: 9505.71, average training loss: 10571.58, base loss: 14389.52
[INFO 2017-06-28 12:36:10,738 main.py:51] epoch 1438, training loss: 11286.14, average training loss: 10572.65, base loss: 14392.86
[INFO 2017-06-28 12:36:11,177 main.py:51] epoch 1439, training loss: 10521.15, average training loss: 10572.64, base loss: 14393.13
[INFO 2017-06-28 12:36:11,617 main.py:51] epoch 1440, training loss: 9297.09, average training loss: 10571.02, base loss: 14392.30
[INFO 2017-06-28 12:36:12,040 main.py:51] epoch 1441, training loss: 11619.71, average training loss: 10572.21, base loss: 14395.33
[INFO 2017-06-28 12:36:12,481 main.py:51] epoch 1442, training loss: 11529.00, average training loss: 10573.96, base loss: 14397.37
[INFO 2017-06-28 12:36:12,913 main.py:51] epoch 1443, training loss: 11646.41, average training loss: 10574.95, base loss: 14399.52
[INFO 2017-06-28 12:36:13,346 main.py:51] epoch 1444, training loss: 11327.44, average training loss: 10576.78, base loss: 14404.21
[INFO 2017-06-28 12:36:13,772 main.py:51] epoch 1445, training loss: 9711.89, average training loss: 10575.81, base loss: 14403.31
[INFO 2017-06-28 12:36:14,182 main.py:51] epoch 1446, training loss: 9680.73, average training loss: 10572.05, base loss: 14401.07
[INFO 2017-06-28 12:36:14,600 main.py:51] epoch 1447, training loss: 11505.66, average training loss: 10573.48, base loss: 14402.25
[INFO 2017-06-28 12:36:15,042 main.py:51] epoch 1448, training loss: 9545.79, average training loss: 10570.89, base loss: 14399.83
[INFO 2017-06-28 12:36:15,476 main.py:51] epoch 1449, training loss: 9132.32, average training loss: 10568.82, base loss: 14397.09
[INFO 2017-06-28 12:36:15,906 main.py:51] epoch 1450, training loss: 10865.74, average training loss: 10570.96, base loss: 14400.09
[INFO 2017-06-28 12:36:16,345 main.py:51] epoch 1451, training loss: 9840.23, average training loss: 10567.97, base loss: 14397.07
[INFO 2017-06-28 12:36:16,776 main.py:51] epoch 1452, training loss: 9886.65, average training loss: 10566.38, base loss: 14395.11
[INFO 2017-06-28 12:36:17,204 main.py:51] epoch 1453, training loss: 9821.67, average training loss: 10563.53, base loss: 14391.12
[INFO 2017-06-28 12:36:17,622 main.py:51] epoch 1454, training loss: 8769.46, average training loss: 10561.24, base loss: 14389.02
[INFO 2017-06-28 12:36:18,057 main.py:51] epoch 1455, training loss: 9254.47, average training loss: 10558.99, base loss: 14387.03
[INFO 2017-06-28 12:36:18,485 main.py:51] epoch 1456, training loss: 10316.46, average training loss: 10559.02, base loss: 14387.93
[INFO 2017-06-28 12:36:18,900 main.py:51] epoch 1457, training loss: 11195.49, average training loss: 10558.63, base loss: 14388.58
[INFO 2017-06-28 12:36:19,348 main.py:51] epoch 1458, training loss: 9691.56, average training loss: 10556.16, base loss: 14384.41
[INFO 2017-06-28 12:36:19,768 main.py:51] epoch 1459, training loss: 10359.94, average training loss: 10555.62, base loss: 14384.67
[INFO 2017-06-28 12:36:20,186 main.py:51] epoch 1460, training loss: 10122.17, average training loss: 10554.98, base loss: 14383.27
[INFO 2017-06-28 12:36:20,619 main.py:51] epoch 1461, training loss: 9179.68, average training loss: 10552.77, base loss: 14380.99
[INFO 2017-06-28 12:36:21,046 main.py:51] epoch 1462, training loss: 11618.44, average training loss: 10552.66, base loss: 14382.00
[INFO 2017-06-28 12:36:21,502 main.py:51] epoch 1463, training loss: 11147.11, average training loss: 10553.51, base loss: 14382.61
[INFO 2017-06-28 12:36:21,936 main.py:51] epoch 1464, training loss: 10498.49, average training loss: 10554.66, base loss: 14384.11
[INFO 2017-06-28 12:36:22,419 main.py:51] epoch 1465, training loss: 10342.49, average training loss: 10552.33, base loss: 14381.32
[INFO 2017-06-28 12:36:22,834 main.py:51] epoch 1466, training loss: 10155.79, average training loss: 10551.89, base loss: 14381.09
[INFO 2017-06-28 12:36:23,257 main.py:51] epoch 1467, training loss: 9777.95, average training loss: 10550.44, base loss: 14378.70
[INFO 2017-06-28 12:36:23,700 main.py:51] epoch 1468, training loss: 10461.05, average training loss: 10551.05, base loss: 14379.22
[INFO 2017-06-28 12:36:24,123 main.py:51] epoch 1469, training loss: 10883.09, average training loss: 10552.06, base loss: 14382.73
[INFO 2017-06-28 12:36:24,552 main.py:51] epoch 1470, training loss: 8895.24, average training loss: 10549.55, base loss: 14379.50
[INFO 2017-06-28 12:36:24,986 main.py:51] epoch 1471, training loss: 9907.85, average training loss: 10547.43, base loss: 14378.21
[INFO 2017-06-28 12:36:25,425 main.py:51] epoch 1472, training loss: 9723.08, average training loss: 10547.48, base loss: 14379.01
[INFO 2017-06-28 12:36:25,841 main.py:51] epoch 1473, training loss: 10308.16, average training loss: 10545.29, base loss: 14377.16
[INFO 2017-06-28 12:36:26,254 main.py:51] epoch 1474, training loss: 12466.68, average training loss: 10545.74, base loss: 14377.93
[INFO 2017-06-28 12:36:26,700 main.py:51] epoch 1475, training loss: 9380.58, average training loss: 10544.75, base loss: 14378.13
[INFO 2017-06-28 12:36:27,139 main.py:51] epoch 1476, training loss: 10644.80, average training loss: 10543.97, base loss: 14378.00
[INFO 2017-06-28 12:36:27,554 main.py:51] epoch 1477, training loss: 10234.81, average training loss: 10543.92, base loss: 14377.45
[INFO 2017-06-28 12:36:27,985 main.py:51] epoch 1478, training loss: 8736.57, average training loss: 10540.77, base loss: 14373.83
[INFO 2017-06-28 12:36:28,414 main.py:51] epoch 1479, training loss: 9959.48, average training loss: 10540.71, base loss: 14373.55
[INFO 2017-06-28 12:36:28,836 main.py:51] epoch 1480, training loss: 10818.17, average training loss: 10541.26, base loss: 14375.17
[INFO 2017-06-28 12:36:29,263 main.py:51] epoch 1481, training loss: 9155.76, average training loss: 10540.33, base loss: 14375.37
[INFO 2017-06-28 12:36:29,696 main.py:51] epoch 1482, training loss: 10227.10, average training loss: 10540.71, base loss: 14375.75
[INFO 2017-06-28 12:36:30,122 main.py:51] epoch 1483, training loss: 12927.98, average training loss: 10542.99, base loss: 14378.42
[INFO 2017-06-28 12:36:30,537 main.py:51] epoch 1484, training loss: 9620.63, average training loss: 10541.36, base loss: 14375.54
[INFO 2017-06-28 12:36:30,979 main.py:51] epoch 1485, training loss: 9553.71, average training loss: 10540.60, base loss: 14374.46
[INFO 2017-06-28 12:36:31,405 main.py:51] epoch 1486, training loss: 12409.64, average training loss: 10543.21, base loss: 14379.02
[INFO 2017-06-28 12:36:31,808 main.py:51] epoch 1487, training loss: 11158.27, average training loss: 10543.62, base loss: 14381.35
[INFO 2017-06-28 12:36:32,232 main.py:51] epoch 1488, training loss: 10341.33, average training loss: 10543.45, base loss: 14382.65
[INFO 2017-06-28 12:36:32,658 main.py:51] epoch 1489, training loss: 10498.45, average training loss: 10542.67, base loss: 14382.15
[INFO 2017-06-28 12:36:33,078 main.py:51] epoch 1490, training loss: 9064.79, average training loss: 10540.57, base loss: 14380.07
[INFO 2017-06-28 12:36:33,515 main.py:51] epoch 1491, training loss: 10144.69, average training loss: 10538.90, base loss: 14379.12
[INFO 2017-06-28 12:36:33,929 main.py:51] epoch 1492, training loss: 9803.32, average training loss: 10537.98, base loss: 14379.13
[INFO 2017-06-28 12:36:34,365 main.py:51] epoch 1493, training loss: 9380.78, average training loss: 10536.41, base loss: 14377.83
[INFO 2017-06-28 12:36:34,768 main.py:51] epoch 1494, training loss: 9200.35, average training loss: 10533.18, base loss: 14373.89
[INFO 2017-06-28 12:36:35,201 main.py:51] epoch 1495, training loss: 10659.44, average training loss: 10533.03, base loss: 14375.05
[INFO 2017-06-28 12:36:35,644 main.py:51] epoch 1496, training loss: 9586.35, average training loss: 10532.10, base loss: 14374.57
[INFO 2017-06-28 12:36:36,075 main.py:51] epoch 1497, training loss: 10629.75, average training loss: 10532.65, base loss: 14375.72
[INFO 2017-06-28 12:36:36,503 main.py:51] epoch 1498, training loss: 8933.31, average training loss: 10529.89, base loss: 14370.53
[INFO 2017-06-28 12:36:36,933 main.py:51] epoch 1499, training loss: 11234.73, average training loss: 10530.71, base loss: 14373.73
[INFO 2017-06-28 12:36:36,933 main.py:53] epoch 1499, testing
[INFO 2017-06-28 12:36:38,930 main.py:105] average testing loss: 11085.08, base loss: 14866.37
[INFO 2017-06-28 12:36:38,930 main.py:106] improve_loss: 3781.29, improve_percent: 0.25
[INFO 2017-06-28 12:36:38,930 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:36:38,943 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:36:39,375 main.py:51] epoch 1500, training loss: 11381.14, average training loss: 10529.67, base loss: 14372.65
[INFO 2017-06-28 12:36:39,794 main.py:51] epoch 1501, training loss: 11837.06, average training loss: 10528.77, base loss: 14371.64
[INFO 2017-06-28 12:36:40,196 main.py:51] epoch 1502, training loss: 10188.25, average training loss: 10528.97, base loss: 14371.37
[INFO 2017-06-28 12:36:40,622 main.py:51] epoch 1503, training loss: 9521.89, average training loss: 10527.27, base loss: 14369.69
[INFO 2017-06-28 12:36:41,021 main.py:51] epoch 1504, training loss: 10550.58, average training loss: 10528.00, base loss: 14371.45
[INFO 2017-06-28 12:36:41,464 main.py:51] epoch 1505, training loss: 10302.76, average training loss: 10526.74, base loss: 14372.25
[INFO 2017-06-28 12:36:41,886 main.py:51] epoch 1506, training loss: 9909.05, average training loss: 10526.15, base loss: 14373.97
[INFO 2017-06-28 12:36:42,305 main.py:51] epoch 1507, training loss: 9784.08, average training loss: 10525.55, base loss: 14373.51
[INFO 2017-06-28 12:36:42,735 main.py:51] epoch 1508, training loss: 11793.15, average training loss: 10526.31, base loss: 14375.23
[INFO 2017-06-28 12:36:43,157 main.py:51] epoch 1509, training loss: 9337.91, average training loss: 10524.79, base loss: 14373.96
[INFO 2017-06-28 12:36:43,599 main.py:51] epoch 1510, training loss: 10330.27, average training loss: 10523.90, base loss: 14374.96
[INFO 2017-06-28 12:36:44,045 main.py:51] epoch 1511, training loss: 11071.32, average training loss: 10524.09, base loss: 14376.61
[INFO 2017-06-28 12:36:44,492 main.py:51] epoch 1512, training loss: 10140.26, average training loss: 10523.60, base loss: 14377.24
[INFO 2017-06-28 12:36:44,913 main.py:51] epoch 1513, training loss: 11635.59, average training loss: 10523.19, base loss: 14377.13
[INFO 2017-06-28 12:36:45,323 main.py:51] epoch 1514, training loss: 10732.22, average training loss: 10523.90, base loss: 14377.93
[INFO 2017-06-28 12:36:45,761 main.py:51] epoch 1515, training loss: 11041.36, average training loss: 10523.82, base loss: 14377.15
[INFO 2017-06-28 12:36:46,183 main.py:51] epoch 1516, training loss: 12159.51, average training loss: 10524.47, base loss: 14378.66
[INFO 2017-06-28 12:36:46,580 main.py:51] epoch 1517, training loss: 10746.11, average training loss: 10521.75, base loss: 14374.39
[INFO 2017-06-28 12:36:47,016 main.py:51] epoch 1518, training loss: 11587.14, average training loss: 10523.19, base loss: 14376.00
[INFO 2017-06-28 12:36:47,439 main.py:51] epoch 1519, training loss: 10829.95, average training loss: 10522.46, base loss: 14374.75
[INFO 2017-06-28 12:36:47,881 main.py:51] epoch 1520, training loss: 9394.58, average training loss: 10521.48, base loss: 14375.02
[INFO 2017-06-28 12:36:48,318 main.py:51] epoch 1521, training loss: 8750.06, average training loss: 10519.42, base loss: 14372.80
[INFO 2017-06-28 12:36:48,739 main.py:51] epoch 1522, training loss: 9029.18, average training loss: 10517.39, base loss: 14369.59
[INFO 2017-06-28 12:36:49,171 main.py:51] epoch 1523, training loss: 11398.03, average training loss: 10516.94, base loss: 14370.40
[INFO 2017-06-28 12:36:49,602 main.py:51] epoch 1524, training loss: 10154.59, average training loss: 10516.02, base loss: 14369.61
[INFO 2017-06-28 12:36:50,018 main.py:51] epoch 1525, training loss: 11723.35, average training loss: 10517.93, base loss: 14374.01
[INFO 2017-06-28 12:36:50,435 main.py:51] epoch 1526, training loss: 8688.21, average training loss: 10515.66, base loss: 14371.23
[INFO 2017-06-28 12:36:50,894 main.py:51] epoch 1527, training loss: 10215.88, average training loss: 10515.29, base loss: 14371.48
[INFO 2017-06-28 12:36:51,310 main.py:51] epoch 1528, training loss: 10788.43, average training loss: 10514.82, base loss: 14372.57
[INFO 2017-06-28 12:36:51,720 main.py:51] epoch 1529, training loss: 11382.36, average training loss: 10514.97, base loss: 14374.81
[INFO 2017-06-28 12:36:52,151 main.py:51] epoch 1530, training loss: 10151.40, average training loss: 10513.52, base loss: 14374.48
[INFO 2017-06-28 12:36:52,582 main.py:51] epoch 1531, training loss: 10934.65, average training loss: 10513.06, base loss: 14376.37
[INFO 2017-06-28 12:36:53,004 main.py:51] epoch 1532, training loss: 8785.34, average training loss: 10509.75, base loss: 14372.49
[INFO 2017-06-28 12:36:53,421 main.py:51] epoch 1533, training loss: 11574.14, average training loss: 10511.17, base loss: 14375.48
[INFO 2017-06-28 12:36:53,851 main.py:51] epoch 1534, training loss: 9983.69, average training loss: 10511.48, base loss: 14377.45
[INFO 2017-06-28 12:36:54,265 main.py:51] epoch 1535, training loss: 9158.11, average training loss: 10510.46, base loss: 14376.82
[INFO 2017-06-28 12:36:54,715 main.py:51] epoch 1536, training loss: 8920.12, average training loss: 10509.41, base loss: 14374.62
[INFO 2017-06-28 12:36:55,122 main.py:51] epoch 1537, training loss: 10676.99, average training loss: 10507.46, base loss: 14373.73
[INFO 2017-06-28 12:36:55,563 main.py:51] epoch 1538, training loss: 10134.89, average training loss: 10508.28, base loss: 14375.59
[INFO 2017-06-28 12:36:56,004 main.py:51] epoch 1539, training loss: 9755.46, average training loss: 10508.08, base loss: 14374.56
[INFO 2017-06-28 12:36:56,425 main.py:51] epoch 1540, training loss: 11004.02, average training loss: 10510.06, base loss: 14378.74
[INFO 2017-06-28 12:36:56,848 main.py:51] epoch 1541, training loss: 10343.08, average training loss: 10509.41, base loss: 14378.27
[INFO 2017-06-28 12:36:57,272 main.py:51] epoch 1542, training loss: 10353.23, average training loss: 10507.77, base loss: 14376.08
[INFO 2017-06-28 12:36:57,710 main.py:51] epoch 1543, training loss: 9805.75, average training loss: 10508.28, base loss: 14376.76
[INFO 2017-06-28 12:36:58,114 main.py:51] epoch 1544, training loss: 11610.19, average training loss: 10509.26, base loss: 14380.72
[INFO 2017-06-28 12:36:58,546 main.py:51] epoch 1545, training loss: 9317.37, average training loss: 10505.96, base loss: 14376.90
[INFO 2017-06-28 12:36:58,970 main.py:51] epoch 1546, training loss: 10012.61, average training loss: 10505.92, base loss: 14379.68
[INFO 2017-06-28 12:36:59,411 main.py:51] epoch 1547, training loss: 10113.29, average training loss: 10505.17, base loss: 14378.07
[INFO 2017-06-28 12:36:59,853 main.py:51] epoch 1548, training loss: 9638.85, average training loss: 10503.45, base loss: 14375.94
[INFO 2017-06-28 12:37:00,270 main.py:51] epoch 1549, training loss: 9697.75, average training loss: 10502.81, base loss: 14375.70
[INFO 2017-06-28 12:37:00,698 main.py:51] epoch 1550, training loss: 9212.32, average training loss: 10501.92, base loss: 14376.04
[INFO 2017-06-28 12:37:01,142 main.py:51] epoch 1551, training loss: 10551.62, average training loss: 10500.48, base loss: 14373.81
[INFO 2017-06-28 12:37:01,591 main.py:51] epoch 1552, training loss: 10565.75, average training loss: 10502.00, base loss: 14377.12
[INFO 2017-06-28 12:37:02,020 main.py:51] epoch 1553, training loss: 10609.02, average training loss: 10500.70, base loss: 14374.87
[INFO 2017-06-28 12:37:02,459 main.py:51] epoch 1554, training loss: 11521.14, average training loss: 10502.23, base loss: 14377.65
[INFO 2017-06-28 12:37:02,875 main.py:51] epoch 1555, training loss: 9936.28, average training loss: 10500.08, base loss: 14374.65
[INFO 2017-06-28 12:37:03,312 main.py:51] epoch 1556, training loss: 11627.72, average training loss: 10500.53, base loss: 14377.22
[INFO 2017-06-28 12:37:03,761 main.py:51] epoch 1557, training loss: 12415.18, average training loss: 10502.34, base loss: 14381.00
[INFO 2017-06-28 12:37:04,200 main.py:51] epoch 1558, training loss: 9330.20, average training loss: 10501.24, base loss: 14379.02
[INFO 2017-06-28 12:37:04,602 main.py:51] epoch 1559, training loss: 9602.66, average training loss: 10500.80, base loss: 14379.44
[INFO 2017-06-28 12:37:05,046 main.py:51] epoch 1560, training loss: 10153.10, average training loss: 10501.46, base loss: 14381.99
[INFO 2017-06-28 12:37:05,455 main.py:51] epoch 1561, training loss: 10346.65, average training loss: 10500.08, base loss: 14381.82
[INFO 2017-06-28 12:37:05,882 main.py:51] epoch 1562, training loss: 9600.90, average training loss: 10498.97, base loss: 14381.08
[INFO 2017-06-28 12:37:06,316 main.py:51] epoch 1563, training loss: 10307.38, average training loss: 10499.71, base loss: 14381.89
[INFO 2017-06-28 12:37:06,735 main.py:51] epoch 1564, training loss: 10954.31, average training loss: 10499.18, base loss: 14381.76
[INFO 2017-06-28 12:37:07,159 main.py:51] epoch 1565, training loss: 8834.48, average training loss: 10496.11, base loss: 14379.61
[INFO 2017-06-28 12:37:07,580 main.py:51] epoch 1566, training loss: 10244.99, average training loss: 10494.32, base loss: 14378.91
[INFO 2017-06-28 12:37:08,001 main.py:51] epoch 1567, training loss: 10863.99, average training loss: 10494.00, base loss: 14378.97
[INFO 2017-06-28 12:37:08,428 main.py:51] epoch 1568, training loss: 10986.92, average training loss: 10494.34, base loss: 14379.68
[INFO 2017-06-28 12:37:08,851 main.py:51] epoch 1569, training loss: 10236.45, average training loss: 10494.85, base loss: 14379.29
[INFO 2017-06-28 12:37:09,265 main.py:51] epoch 1570, training loss: 9301.39, average training loss: 10492.58, base loss: 14376.66
[INFO 2017-06-28 12:37:09,680 main.py:51] epoch 1571, training loss: 10891.51, average training loss: 10492.49, base loss: 14377.54
[INFO 2017-06-28 12:37:10,113 main.py:51] epoch 1572, training loss: 10493.46, average training loss: 10492.21, base loss: 14379.34
[INFO 2017-06-28 12:37:10,561 main.py:51] epoch 1573, training loss: 9870.13, average training loss: 10490.67, base loss: 14377.57
[INFO 2017-06-28 12:37:11,019 main.py:51] epoch 1574, training loss: 10167.27, average training loss: 10490.03, base loss: 14376.52
[INFO 2017-06-28 12:37:11,446 main.py:51] epoch 1575, training loss: 9673.89, average training loss: 10488.16, base loss: 14374.17
[INFO 2017-06-28 12:37:11,872 main.py:51] epoch 1576, training loss: 9118.95, average training loss: 10485.04, base loss: 14369.72
[INFO 2017-06-28 12:37:12,303 main.py:51] epoch 1577, training loss: 8693.96, average training loss: 10482.10, base loss: 14365.67
[INFO 2017-06-28 12:37:12,731 main.py:51] epoch 1578, training loss: 8504.22, average training loss: 10480.13, base loss: 14363.64
[INFO 2017-06-28 12:37:13,166 main.py:51] epoch 1579, training loss: 11698.40, average training loss: 10480.91, base loss: 14365.96
[INFO 2017-06-28 12:37:13,584 main.py:51] epoch 1580, training loss: 11397.60, average training loss: 10481.46, base loss: 14366.53
[INFO 2017-06-28 12:37:14,022 main.py:51] epoch 1581, training loss: 11761.04, average training loss: 10483.13, base loss: 14368.76
[INFO 2017-06-28 12:37:14,444 main.py:51] epoch 1582, training loss: 9609.99, average training loss: 10481.21, base loss: 14366.21
[INFO 2017-06-28 12:37:14,881 main.py:51] epoch 1583, training loss: 11248.72, average training loss: 10481.77, base loss: 14367.74
[INFO 2017-06-28 12:37:15,312 main.py:51] epoch 1584, training loss: 10487.46, average training loss: 10481.31, base loss: 14368.05
[INFO 2017-06-28 12:37:15,722 main.py:51] epoch 1585, training loss: 8774.37, average training loss: 10480.02, base loss: 14365.87
[INFO 2017-06-28 12:37:16,141 main.py:51] epoch 1586, training loss: 9272.82, average training loss: 10479.23, base loss: 14364.27
[INFO 2017-06-28 12:37:16,553 main.py:51] epoch 1587, training loss: 10655.96, average training loss: 10479.46, base loss: 14365.97
[INFO 2017-06-28 12:37:16,975 main.py:51] epoch 1588, training loss: 8500.40, average training loss: 10477.43, base loss: 14363.64
[INFO 2017-06-28 12:37:17,425 main.py:51] epoch 1589, training loss: 9808.00, average training loss: 10476.62, base loss: 14363.07
[INFO 2017-06-28 12:37:17,853 main.py:51] epoch 1590, training loss: 11134.18, average training loss: 10478.30, base loss: 14365.44
[INFO 2017-06-28 12:37:18,285 main.py:51] epoch 1591, training loss: 10059.22, average training loss: 10478.57, base loss: 14366.43
[INFO 2017-06-28 12:37:18,702 main.py:51] epoch 1592, training loss: 9672.50, average training loss: 10475.42, base loss: 14362.19
[INFO 2017-06-28 12:37:19,136 main.py:51] epoch 1593, training loss: 10615.08, average training loss: 10476.16, base loss: 14363.38
[INFO 2017-06-28 12:37:19,546 main.py:51] epoch 1594, training loss: 10258.50, average training loss: 10475.58, base loss: 14363.60
[INFO 2017-06-28 12:37:19,961 main.py:51] epoch 1595, training loss: 10999.42, average training loss: 10476.15, base loss: 14363.85
[INFO 2017-06-28 12:37:20,380 main.py:51] epoch 1596, training loss: 10559.42, average training loss: 10477.41, base loss: 14366.09
[INFO 2017-06-28 12:37:20,790 main.py:51] epoch 1597, training loss: 11256.22, average training loss: 10477.25, base loss: 14367.05
[INFO 2017-06-28 12:37:21,213 main.py:51] epoch 1598, training loss: 10282.74, average training loss: 10476.37, base loss: 14365.68
[INFO 2017-06-28 12:37:21,640 main.py:51] epoch 1599, training loss: 11367.60, average training loss: 10475.81, base loss: 14366.73
[INFO 2017-06-28 12:37:21,640 main.py:53] epoch 1599, testing
[INFO 2017-06-28 12:37:23,573 main.py:105] average testing loss: 11239.00, base loss: 15308.15
[INFO 2017-06-28 12:37:23,573 main.py:106] improve_loss: 4069.15, improve_percent: 0.27
[INFO 2017-06-28 12:37:23,574 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:37:23,586 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 12:37:24,012 main.py:51] epoch 1600, training loss: 10728.88, average training loss: 10476.56, base loss: 14369.28
[INFO 2017-06-28 12:37:24,437 main.py:51] epoch 1601, training loss: 9988.27, average training loss: 10475.51, base loss: 14365.78
[INFO 2017-06-28 12:37:24,857 main.py:51] epoch 1602, training loss: 11268.22, average training loss: 10476.48, base loss: 14366.33
[INFO 2017-06-28 12:37:25,301 main.py:51] epoch 1603, training loss: 9451.11, average training loss: 10475.94, base loss: 14365.53
[INFO 2017-06-28 12:37:25,722 main.py:51] epoch 1604, training loss: 9668.71, average training loss: 10474.33, base loss: 14363.80
[INFO 2017-06-28 12:37:26,139 main.py:51] epoch 1605, training loss: 9908.50, average training loss: 10472.58, base loss: 14361.67
[INFO 2017-06-28 12:37:26,579 main.py:51] epoch 1606, training loss: 11227.20, average training loss: 10471.76, base loss: 14362.13
[INFO 2017-06-28 12:37:27,013 main.py:51] epoch 1607, training loss: 10239.67, average training loss: 10471.37, base loss: 14362.46
[INFO 2017-06-28 12:37:27,443 main.py:51] epoch 1608, training loss: 10276.14, average training loss: 10471.30, base loss: 14363.68
[INFO 2017-06-28 12:37:27,862 main.py:51] epoch 1609, training loss: 11550.28, average training loss: 10472.09, base loss: 14365.88
[INFO 2017-06-28 12:37:28,269 main.py:51] epoch 1610, training loss: 10336.50, average training loss: 10472.75, base loss: 14367.30
[INFO 2017-06-28 12:37:28,688 main.py:51] epoch 1611, training loss: 10167.80, average training loss: 10473.09, base loss: 14368.19
[INFO 2017-06-28 12:37:29,102 main.py:51] epoch 1612, training loss: 10258.92, average training loss: 10472.31, base loss: 14367.22
[INFO 2017-06-28 12:37:29,527 main.py:51] epoch 1613, training loss: 10519.92, average training loss: 10471.68, base loss: 14367.61
[INFO 2017-06-28 12:37:29,950 main.py:51] epoch 1614, training loss: 10480.54, average training loss: 10472.05, base loss: 14368.18
[INFO 2017-06-28 12:37:30,350 main.py:51] epoch 1615, training loss: 10221.69, average training loss: 10472.90, base loss: 14370.03
[INFO 2017-06-28 12:37:30,793 main.py:51] epoch 1616, training loss: 8649.54, average training loss: 10471.87, base loss: 14370.14
[INFO 2017-06-28 12:37:31,252 main.py:51] epoch 1617, training loss: 9849.64, average training loss: 10470.95, base loss: 14368.27
[INFO 2017-06-28 12:37:31,686 main.py:51] epoch 1618, training loss: 9619.80, average training loss: 10470.51, base loss: 14370.26
[INFO 2017-06-28 12:37:32,110 main.py:51] epoch 1619, training loss: 9847.24, average training loss: 10469.08, base loss: 14368.77
[INFO 2017-06-28 12:37:32,562 main.py:51] epoch 1620, training loss: 9088.80, average training loss: 10467.95, base loss: 14368.64
[INFO 2017-06-28 12:37:32,973 main.py:51] epoch 1621, training loss: 9079.80, average training loss: 10464.98, base loss: 14365.05
[INFO 2017-06-28 12:37:33,424 main.py:51] epoch 1622, training loss: 11527.81, average training loss: 10465.35, base loss: 14365.65
[INFO 2017-06-28 12:37:33,839 main.py:51] epoch 1623, training loss: 11258.22, average training loss: 10466.42, base loss: 14366.73
[INFO 2017-06-28 12:37:34,274 main.py:51] epoch 1624, training loss: 9035.16, average training loss: 10464.20, base loss: 14363.46
[INFO 2017-06-28 12:37:34,698 main.py:51] epoch 1625, training loss: 9467.34, average training loss: 10464.61, base loss: 14365.55
[INFO 2017-06-28 12:37:35,149 main.py:51] epoch 1626, training loss: 10267.87, average training loss: 10464.47, base loss: 14366.02
[INFO 2017-06-28 12:37:35,571 main.py:51] epoch 1627, training loss: 11025.67, average training loss: 10465.55, base loss: 14368.68
[INFO 2017-06-28 12:37:36,002 main.py:51] epoch 1628, training loss: 10338.22, average training loss: 10464.48, base loss: 14369.32
[INFO 2017-06-28 12:37:36,438 main.py:51] epoch 1629, training loss: 10281.46, average training loss: 10464.59, base loss: 14370.04
[INFO 2017-06-28 12:37:36,873 main.py:51] epoch 1630, training loss: 10049.73, average training loss: 10464.48, base loss: 14370.39
[INFO 2017-06-28 12:37:37,292 main.py:51] epoch 1631, training loss: 9044.31, average training loss: 10463.80, base loss: 14370.41
[INFO 2017-06-28 12:37:37,726 main.py:51] epoch 1632, training loss: 11383.91, average training loss: 10464.22, base loss: 14372.36
[INFO 2017-06-28 12:37:38,153 main.py:51] epoch 1633, training loss: 10545.58, average training loss: 10464.02, base loss: 14372.17
[INFO 2017-06-28 12:37:38,581 main.py:51] epoch 1634, training loss: 9605.74, average training loss: 10462.02, base loss: 14369.56
[INFO 2017-06-28 12:37:38,982 main.py:51] epoch 1635, training loss: 9788.96, average training loss: 10460.30, base loss: 14369.88
[INFO 2017-06-28 12:37:39,396 main.py:51] epoch 1636, training loss: 10025.41, average training loss: 10458.73, base loss: 14368.48
[INFO 2017-06-28 12:37:39,833 main.py:51] epoch 1637, training loss: 9852.48, average training loss: 10457.73, base loss: 14367.03
[INFO 2017-06-28 12:37:40,276 main.py:51] epoch 1638, training loss: 9717.90, average training loss: 10456.93, base loss: 14367.46
[INFO 2017-06-28 12:37:40,691 main.py:51] epoch 1639, training loss: 11411.07, average training loss: 10457.52, base loss: 14367.56
[INFO 2017-06-28 12:37:41,081 main.py:51] epoch 1640, training loss: 10299.44, average training loss: 10457.33, base loss: 14369.13
[INFO 2017-06-28 12:37:41,525 main.py:51] epoch 1641, training loss: 10329.09, average training loss: 10457.18, base loss: 14368.77
[INFO 2017-06-28 12:37:41,937 main.py:51] epoch 1642, training loss: 10748.22, average training loss: 10457.87, base loss: 14369.62
[INFO 2017-06-28 12:37:42,374 main.py:51] epoch 1643, training loss: 9513.87, average training loss: 10457.49, base loss: 14370.00
[INFO 2017-06-28 12:37:42,817 main.py:51] epoch 1644, training loss: 11570.66, average training loss: 10457.22, base loss: 14370.81
[INFO 2017-06-28 12:37:43,233 main.py:51] epoch 1645, training loss: 9848.21, average training loss: 10454.66, base loss: 14369.72
[INFO 2017-06-28 12:37:43,680 main.py:51] epoch 1646, training loss: 10190.27, average training loss: 10454.77, base loss: 14370.25
[INFO 2017-06-28 12:37:44,100 main.py:51] epoch 1647, training loss: 9374.40, average training loss: 10454.45, base loss: 14371.31
[INFO 2017-06-28 12:37:44,519 main.py:51] epoch 1648, training loss: 9991.44, average training loss: 10453.29, base loss: 14370.29
[INFO 2017-06-28 12:37:44,939 main.py:51] epoch 1649, training loss: 9648.39, average training loss: 10452.26, base loss: 14370.84
[INFO 2017-06-28 12:37:45,386 main.py:51] epoch 1650, training loss: 9928.53, average training loss: 10449.54, base loss: 14367.34
[INFO 2017-06-28 12:37:45,809 main.py:51] epoch 1651, training loss: 9605.32, average training loss: 10448.04, base loss: 14366.68
[INFO 2017-06-28 12:37:46,262 main.py:51] epoch 1652, training loss: 10752.03, average training loss: 10449.18, base loss: 14368.36
[INFO 2017-06-28 12:37:46,686 main.py:51] epoch 1653, training loss: 10672.57, average training loss: 10448.70, base loss: 14369.79
[INFO 2017-06-28 12:37:47,112 main.py:51] epoch 1654, training loss: 9130.12, average training loss: 10447.19, base loss: 14369.65
[INFO 2017-06-28 12:37:47,541 main.py:51] epoch 1655, training loss: 9367.22, average training loss: 10445.47, base loss: 14367.38
[INFO 2017-06-28 12:37:47,969 main.py:51] epoch 1656, training loss: 9232.73, average training loss: 10445.14, base loss: 14366.82
[INFO 2017-06-28 12:37:48,410 main.py:51] epoch 1657, training loss: 10801.51, average training loss: 10447.05, base loss: 14368.87
[INFO 2017-06-28 12:37:48,846 main.py:51] epoch 1658, training loss: 11504.61, average training loss: 10446.23, base loss: 14369.22
[INFO 2017-06-28 12:37:49,273 main.py:51] epoch 1659, training loss: 8733.29, average training loss: 10444.09, base loss: 14368.49
[INFO 2017-06-28 12:37:49,699 main.py:51] epoch 1660, training loss: 9281.95, average training loss: 10444.43, base loss: 14369.23
[INFO 2017-06-28 12:37:50,123 main.py:51] epoch 1661, training loss: 10411.25, average training loss: 10442.66, base loss: 14366.45
[INFO 2017-06-28 12:37:50,551 main.py:51] epoch 1662, training loss: 9541.11, average training loss: 10442.98, base loss: 14367.98
[INFO 2017-06-28 12:37:50,986 main.py:51] epoch 1663, training loss: 10891.50, average training loss: 10443.22, base loss: 14367.70
[INFO 2017-06-28 12:37:51,413 main.py:51] epoch 1664, training loss: 8699.17, average training loss: 10440.80, base loss: 14365.63
[INFO 2017-06-28 12:37:51,834 main.py:51] epoch 1665, training loss: 9843.22, average training loss: 10440.44, base loss: 14367.41
[INFO 2017-06-28 12:37:52,276 main.py:51] epoch 1666, training loss: 9495.13, average training loss: 10438.53, base loss: 14365.54
[INFO 2017-06-28 12:37:52,686 main.py:51] epoch 1667, training loss: 9035.08, average training loss: 10437.18, base loss: 14365.91
[INFO 2017-06-28 12:37:53,134 main.py:51] epoch 1668, training loss: 9311.85, average training loss: 10434.60, base loss: 14362.85
[INFO 2017-06-28 12:37:53,550 main.py:51] epoch 1669, training loss: 11285.50, average training loss: 10433.37, base loss: 14363.56
[INFO 2017-06-28 12:37:53,992 main.py:51] epoch 1670, training loss: 9498.12, average training loss: 10429.95, base loss: 14359.64
[INFO 2017-06-28 12:37:54,442 main.py:51] epoch 1671, training loss: 10072.78, average training loss: 10429.85, base loss: 14361.93
[INFO 2017-06-28 12:37:54,872 main.py:51] epoch 1672, training loss: 9843.39, average training loss: 10429.61, base loss: 14361.83
[INFO 2017-06-28 12:37:55,299 main.py:51] epoch 1673, training loss: 10138.36, average training loss: 10429.16, base loss: 14359.52
[INFO 2017-06-28 12:37:55,752 main.py:51] epoch 1674, training loss: 9948.03, average training loss: 10428.46, base loss: 14359.59
[INFO 2017-06-28 12:37:56,175 main.py:51] epoch 1675, training loss: 9886.88, average training loss: 10427.15, base loss: 14357.73
[INFO 2017-06-28 12:37:56,612 main.py:51] epoch 1676, training loss: 10389.29, average training loss: 10428.20, base loss: 14359.61
[INFO 2017-06-28 12:37:57,039 main.py:51] epoch 1677, training loss: 10163.53, average training loss: 10427.98, base loss: 14360.69
[INFO 2017-06-28 12:37:57,468 main.py:51] epoch 1678, training loss: 10266.75, average training loss: 10427.10, base loss: 14360.26
[INFO 2017-06-28 12:37:57,886 main.py:51] epoch 1679, training loss: 11562.52, average training loss: 10427.33, base loss: 14362.41
[INFO 2017-06-28 12:37:58,319 main.py:51] epoch 1680, training loss: 10613.43, average training loss: 10427.77, base loss: 14364.09
[INFO 2017-06-28 12:37:58,724 main.py:51] epoch 1681, training loss: 10570.61, average training loss: 10427.45, base loss: 14364.71
[INFO 2017-06-28 12:37:59,150 main.py:51] epoch 1682, training loss: 12140.22, average training loss: 10428.03, base loss: 14367.34
[INFO 2017-06-28 12:37:59,608 main.py:51] epoch 1683, training loss: 10062.22, average training loss: 10427.28, base loss: 14365.88
[INFO 2017-06-28 12:38:00,032 main.py:51] epoch 1684, training loss: 10315.79, average training loss: 10426.66, base loss: 14365.37
[INFO 2017-06-28 12:38:00,481 main.py:51] epoch 1685, training loss: 10769.97, average training loss: 10425.92, base loss: 14364.18
[INFO 2017-06-28 12:38:00,901 main.py:51] epoch 1686, training loss: 10079.07, average training loss: 10426.30, base loss: 14365.84
[INFO 2017-06-28 12:38:01,343 main.py:51] epoch 1687, training loss: 10057.60, average training loss: 10425.66, base loss: 14365.67
[INFO 2017-06-28 12:38:01,768 main.py:51] epoch 1688, training loss: 9743.60, average training loss: 10424.91, base loss: 14364.67
[INFO 2017-06-28 12:38:02,205 main.py:51] epoch 1689, training loss: 9602.58, average training loss: 10423.45, base loss: 14363.00
[INFO 2017-06-28 12:38:02,637 main.py:51] epoch 1690, training loss: 11832.92, average training loss: 10425.06, base loss: 14366.13
[INFO 2017-06-28 12:38:03,050 main.py:51] epoch 1691, training loss: 10945.20, average training loss: 10425.29, base loss: 14367.11
[INFO 2017-06-28 12:38:03,498 main.py:51] epoch 1692, training loss: 10014.58, average training loss: 10423.32, base loss: 14364.69
[INFO 2017-06-28 12:38:03,915 main.py:51] epoch 1693, training loss: 9825.31, average training loss: 10422.41, base loss: 14362.88
[INFO 2017-06-28 12:38:04,353 main.py:51] epoch 1694, training loss: 9427.00, average training loss: 10422.07, base loss: 14363.06
[INFO 2017-06-28 12:38:04,785 main.py:51] epoch 1695, training loss: 11107.07, average training loss: 10422.71, base loss: 14362.79
[INFO 2017-06-28 12:38:05,209 main.py:51] epoch 1696, training loss: 10194.12, average training loss: 10422.48, base loss: 14364.14
[INFO 2017-06-28 12:38:05,651 main.py:51] epoch 1697, training loss: 12260.98, average training loss: 10425.22, base loss: 14368.59
[INFO 2017-06-28 12:38:06,108 main.py:51] epoch 1698, training loss: 10210.84, average training loss: 10423.11, base loss: 14365.90
[INFO 2017-06-28 12:38:06,551 main.py:51] epoch 1699, training loss: 10160.67, average training loss: 10423.10, base loss: 14366.09
[INFO 2017-06-28 12:38:06,551 main.py:53] epoch 1699, testing
[INFO 2017-06-28 12:38:08,491 main.py:105] average testing loss: 11008.81, base loss: 14960.99
[INFO 2017-06-28 12:38:08,491 main.py:106] improve_loss: 3952.18, improve_percent: 0.26
[INFO 2017-06-28 12:38:08,491 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 12:38:08,931 main.py:51] epoch 1700, training loss: 11557.90, average training loss: 10424.20, base loss: 14368.46
[INFO 2017-06-28 12:38:09,348 main.py:51] epoch 1701, training loss: 10633.26, average training loss: 10424.39, base loss: 14370.20
[INFO 2017-06-28 12:38:09,776 main.py:51] epoch 1702, training loss: 11543.62, average training loss: 10423.67, base loss: 14369.62
[INFO 2017-06-28 12:38:10,208 main.py:51] epoch 1703, training loss: 8889.13, average training loss: 10421.03, base loss: 14365.71
[INFO 2017-06-28 12:38:10,623 main.py:51] epoch 1704, training loss: 12294.89, average training loss: 10423.37, base loss: 14368.81
[INFO 2017-06-28 12:38:11,060 main.py:51] epoch 1705, training loss: 10295.49, average training loss: 10422.40, base loss: 14368.45
[INFO 2017-06-28 12:38:11,506 main.py:51] epoch 1706, training loss: 10213.71, average training loss: 10422.46, base loss: 14369.37
[INFO 2017-06-28 12:38:11,956 main.py:51] epoch 1707, training loss: 10676.45, average training loss: 10423.08, base loss: 14369.75
[INFO 2017-06-28 12:38:12,411 main.py:51] epoch 1708, training loss: 11406.14, average training loss: 10422.91, base loss: 14370.18
[INFO 2017-06-28 12:38:12,829 main.py:51] epoch 1709, training loss: 10507.54, average training loss: 10422.86, base loss: 14370.91
[INFO 2017-06-28 12:38:13,276 main.py:51] epoch 1710, training loss: 9489.06, average training loss: 10421.12, base loss: 14369.55
[INFO 2017-06-28 12:38:13,716 main.py:51] epoch 1711, training loss: 9834.63, average training loss: 10420.09, base loss: 14370.09
[INFO 2017-06-28 12:38:14,124 main.py:51] epoch 1712, training loss: 11255.44, average training loss: 10418.93, base loss: 14370.58
[INFO 2017-06-28 12:38:14,534 main.py:51] epoch 1713, training loss: 9918.21, average training loss: 10419.57, base loss: 14373.40
[INFO 2017-06-28 12:38:14,967 main.py:51] epoch 1714, training loss: 9121.45, average training loss: 10418.42, base loss: 14373.90
[INFO 2017-06-28 12:38:15,395 main.py:51] epoch 1715, training loss: 9078.29, average training loss: 10417.18, base loss: 14372.94
[INFO 2017-06-28 12:38:15,813 main.py:51] epoch 1716, training loss: 10374.46, average training loss: 10416.92, base loss: 14373.41
[INFO 2017-06-28 12:38:16,229 main.py:51] epoch 1717, training loss: 10317.82, average training loss: 10416.69, base loss: 14372.09
[INFO 2017-06-28 12:38:16,660 main.py:51] epoch 1718, training loss: 9163.62, average training loss: 10415.49, base loss: 14371.28
[INFO 2017-06-28 12:38:17,090 main.py:51] epoch 1719, training loss: 10215.39, average training loss: 10416.05, base loss: 14371.62
[INFO 2017-06-28 12:38:17,534 main.py:51] epoch 1720, training loss: 8705.24, average training loss: 10416.08, base loss: 14372.24
[INFO 2017-06-28 12:38:17,957 main.py:51] epoch 1721, training loss: 10259.10, average training loss: 10415.49, base loss: 14373.39
[INFO 2017-06-28 12:38:18,432 main.py:51] epoch 1722, training loss: 9911.89, average training loss: 10414.34, base loss: 14372.15
[INFO 2017-06-28 12:38:18,863 main.py:51] epoch 1723, training loss: 9125.61, average training loss: 10412.82, base loss: 14369.37
[INFO 2017-06-28 12:38:19,293 main.py:51] epoch 1724, training loss: 10230.05, average training loss: 10411.71, base loss: 14368.55
[INFO 2017-06-28 12:38:19,715 main.py:51] epoch 1725, training loss: 10197.12, average training loss: 10412.18, base loss: 14369.77
[INFO 2017-06-28 12:38:20,136 main.py:51] epoch 1726, training loss: 9210.73, average training loss: 10410.32, base loss: 14368.92
[INFO 2017-06-28 12:38:20,557 main.py:51] epoch 1727, training loss: 10662.24, average training loss: 10410.70, base loss: 14370.19
[INFO 2017-06-28 12:38:21,006 main.py:51] epoch 1728, training loss: 9860.97, average training loss: 10410.04, base loss: 14369.03
[INFO 2017-06-28 12:38:21,462 main.py:51] epoch 1729, training loss: 10382.98, average training loss: 10408.86, base loss: 14366.56
[INFO 2017-06-28 12:38:21,894 main.py:51] epoch 1730, training loss: 10138.88, average training loss: 10408.69, base loss: 14367.12
[INFO 2017-06-28 12:38:22,319 main.py:51] epoch 1731, training loss: 9409.14, average training loss: 10407.71, base loss: 14365.92
[INFO 2017-06-28 12:38:22,767 main.py:51] epoch 1732, training loss: 9798.57, average training loss: 10406.57, base loss: 14365.48
[INFO 2017-06-28 12:38:23,185 main.py:51] epoch 1733, training loss: 9431.36, average training loss: 10405.00, base loss: 14363.56
[INFO 2017-06-28 12:38:23,607 main.py:51] epoch 1734, training loss: 10570.61, average training loss: 10405.37, base loss: 14363.93
[INFO 2017-06-28 12:38:24,049 main.py:51] epoch 1735, training loss: 10558.76, average training loss: 10406.88, base loss: 14365.98
[INFO 2017-06-28 12:38:24,473 main.py:51] epoch 1736, training loss: 11514.10, average training loss: 10407.66, base loss: 14367.65
[INFO 2017-06-28 12:38:24,918 main.py:51] epoch 1737, training loss: 9967.06, average training loss: 10408.29, base loss: 14369.50
[INFO 2017-06-28 12:38:25,342 main.py:51] epoch 1738, training loss: 9672.80, average training loss: 10407.69, base loss: 14370.30
[INFO 2017-06-28 12:38:25,762 main.py:51] epoch 1739, training loss: 10931.68, average training loss: 10407.38, base loss: 14371.80
[INFO 2017-06-28 12:38:26,209 main.py:51] epoch 1740, training loss: 11260.52, average training loss: 10409.60, base loss: 14374.38
[INFO 2017-06-28 12:38:26,631 main.py:51] epoch 1741, training loss: 11593.73, average training loss: 10411.12, base loss: 14377.63
[INFO 2017-06-28 12:38:27,060 main.py:51] epoch 1742, training loss: 9572.14, average training loss: 10410.95, base loss: 14376.47
[INFO 2017-06-28 12:38:27,506 main.py:51] epoch 1743, training loss: 9535.95, average training loss: 10410.66, base loss: 14377.52
[INFO 2017-06-28 12:38:27,938 main.py:51] epoch 1744, training loss: 10681.13, average training loss: 10412.25, base loss: 14381.03
[INFO 2017-06-28 12:38:28,376 main.py:51] epoch 1745, training loss: 9281.63, average training loss: 10412.58, base loss: 14380.82
[INFO 2017-06-28 12:38:28,793 main.py:51] epoch 1746, training loss: 9505.43, average training loss: 10412.02, base loss: 14381.21
[INFO 2017-06-28 12:38:29,216 main.py:51] epoch 1747, training loss: 9510.68, average training loss: 10413.14, base loss: 14381.80
[INFO 2017-06-28 12:38:29,620 main.py:51] epoch 1748, training loss: 10163.36, average training loss: 10412.79, base loss: 14381.64
[INFO 2017-06-28 12:38:30,076 main.py:51] epoch 1749, training loss: 9721.94, average training loss: 10410.46, base loss: 14379.38
[INFO 2017-06-28 12:38:30,517 main.py:51] epoch 1750, training loss: 10121.84, average training loss: 10408.80, base loss: 14377.29
[INFO 2017-06-28 12:38:30,960 main.py:51] epoch 1751, training loss: 8848.26, average training loss: 10406.67, base loss: 14373.48
[INFO 2017-06-28 12:38:31,391 main.py:51] epoch 1752, training loss: 10288.07, average training loss: 10406.43, base loss: 14374.35
[INFO 2017-06-28 12:38:31,831 main.py:51] epoch 1753, training loss: 12862.99, average training loss: 10409.09, base loss: 14378.45
[INFO 2017-06-28 12:38:32,259 main.py:51] epoch 1754, training loss: 9700.38, average training loss: 10408.03, base loss: 14377.13
[INFO 2017-06-28 12:38:32,697 main.py:51] epoch 1755, training loss: 9930.99, average training loss: 10406.72, base loss: 14376.53
[INFO 2017-06-28 12:38:33,127 main.py:51] epoch 1756, training loss: 11384.88, average training loss: 10406.81, base loss: 14376.77
[INFO 2017-06-28 12:38:33,577 main.py:51] epoch 1757, training loss: 9389.90, average training loss: 10404.70, base loss: 14374.85
[INFO 2017-06-28 12:38:34,003 main.py:51] epoch 1758, training loss: 9733.58, average training loss: 10402.96, base loss: 14371.70
[INFO 2017-06-28 12:38:34,426 main.py:51] epoch 1759, training loss: 9643.36, average training loss: 10402.68, base loss: 14372.76
[INFO 2017-06-28 12:38:34,853 main.py:51] epoch 1760, training loss: 11091.21, average training loss: 10403.34, base loss: 14374.56
[INFO 2017-06-28 12:38:35,292 main.py:51] epoch 1761, training loss: 9743.76, average training loss: 10401.47, base loss: 14371.96
[INFO 2017-06-28 12:38:35,719 main.py:51] epoch 1762, training loss: 9727.41, average training loss: 10399.03, base loss: 14368.45
[INFO 2017-06-28 12:38:36,132 main.py:51] epoch 1763, training loss: 10446.17, average training loss: 10399.63, base loss: 14369.85
[INFO 2017-06-28 12:38:36,557 main.py:51] epoch 1764, training loss: 9500.28, average training loss: 10398.53, base loss: 14368.45
[INFO 2017-06-28 12:38:36,980 main.py:51] epoch 1765, training loss: 11303.83, average training loss: 10396.96, base loss: 14367.63
[INFO 2017-06-28 12:38:37,428 main.py:51] epoch 1766, training loss: 10930.55, average training loss: 10396.38, base loss: 14367.46
[INFO 2017-06-28 12:38:37,860 main.py:51] epoch 1767, training loss: 10647.79, average training loss: 10396.58, base loss: 14368.73
[INFO 2017-06-28 12:38:38,277 main.py:51] epoch 1768, training loss: 11086.47, average training loss: 10396.57, base loss: 14368.18
[INFO 2017-06-28 12:38:38,720 main.py:51] epoch 1769, training loss: 9738.29, average training loss: 10397.12, base loss: 14370.97
[INFO 2017-06-28 12:38:39,172 main.py:51] epoch 1770, training loss: 8890.35, average training loss: 10396.76, base loss: 14369.90
[INFO 2017-06-28 12:38:39,578 main.py:51] epoch 1771, training loss: 10549.25, average training loss: 10397.56, base loss: 14372.54
[INFO 2017-06-28 12:38:40,013 main.py:51] epoch 1772, training loss: 10691.72, average training loss: 10397.65, base loss: 14373.28
[INFO 2017-06-28 12:38:40,445 main.py:51] epoch 1773, training loss: 9514.96, average training loss: 10398.98, base loss: 14375.60
[INFO 2017-06-28 12:38:40,875 main.py:51] epoch 1774, training loss: 9046.42, average training loss: 10397.98, base loss: 14373.78
[INFO 2017-06-28 12:38:41,291 main.py:51] epoch 1775, training loss: 10058.66, average training loss: 10397.45, base loss: 14373.35
[INFO 2017-06-28 12:38:41,718 main.py:51] epoch 1776, training loss: 10761.30, average training loss: 10397.67, base loss: 14376.15
[INFO 2017-06-28 12:38:42,137 main.py:51] epoch 1777, training loss: 9878.27, average training loss: 10397.50, base loss: 14375.86
[INFO 2017-06-28 12:38:42,562 main.py:51] epoch 1778, training loss: 8337.70, average training loss: 10394.69, base loss: 14373.10
[INFO 2017-06-28 12:38:42,983 main.py:51] epoch 1779, training loss: 13297.76, average training loss: 10397.39, base loss: 14377.66
[INFO 2017-06-28 12:38:43,424 main.py:51] epoch 1780, training loss: 9773.55, average training loss: 10395.78, base loss: 14374.12
[INFO 2017-06-28 12:38:43,860 main.py:51] epoch 1781, training loss: 10691.94, average training loss: 10396.56, base loss: 14374.74
[INFO 2017-06-28 12:38:44,301 main.py:51] epoch 1782, training loss: 9107.97, average training loss: 10394.71, base loss: 14372.71
[INFO 2017-06-28 12:38:44,753 main.py:51] epoch 1783, training loss: 10460.33, average training loss: 10395.25, base loss: 14373.00
[INFO 2017-06-28 12:38:45,151 main.py:51] epoch 1784, training loss: 10272.86, average training loss: 10395.59, base loss: 14373.71
[INFO 2017-06-28 12:38:45,581 main.py:51] epoch 1785, training loss: 9601.58, average training loss: 10393.45, base loss: 14371.27
[INFO 2017-06-28 12:38:46,011 main.py:51] epoch 1786, training loss: 10923.75, average training loss: 10393.13, base loss: 14369.90
[INFO 2017-06-28 12:38:46,448 main.py:51] epoch 1787, training loss: 10449.29, average training loss: 10392.63, base loss: 14368.98
[INFO 2017-06-28 12:38:46,882 main.py:51] epoch 1788, training loss: 9145.06, average training loss: 10391.23, base loss: 14368.01
[INFO 2017-06-28 12:38:47,302 main.py:51] epoch 1789, training loss: 10383.75, average training loss: 10389.36, base loss: 14367.41
[INFO 2017-06-28 12:38:47,728 main.py:51] epoch 1790, training loss: 9535.76, average training loss: 10387.83, base loss: 14366.06
[INFO 2017-06-28 12:38:48,159 main.py:51] epoch 1791, training loss: 10972.73, average training loss: 10387.34, base loss: 14366.99
[INFO 2017-06-28 12:38:48,592 main.py:51] epoch 1792, training loss: 9165.38, average training loss: 10385.14, base loss: 14364.61
[INFO 2017-06-28 12:38:49,027 main.py:51] epoch 1793, training loss: 10104.95, average training loss: 10383.92, base loss: 14363.12
[INFO 2017-06-28 12:38:49,463 main.py:51] epoch 1794, training loss: 9473.64, average training loss: 10381.27, base loss: 14360.55
[INFO 2017-06-28 12:38:49,885 main.py:51] epoch 1795, training loss: 9128.54, average training loss: 10380.46, base loss: 14358.75
[INFO 2017-06-28 12:38:50,310 main.py:51] epoch 1796, training loss: 9318.32, average training loss: 10379.48, base loss: 14356.94
[INFO 2017-06-28 12:38:50,720 main.py:51] epoch 1797, training loss: 10500.42, average training loss: 10378.64, base loss: 14355.07
[INFO 2017-06-28 12:38:51,137 main.py:51] epoch 1798, training loss: 8825.54, average training loss: 10378.45, base loss: 14354.93
[INFO 2017-06-28 12:38:51,580 main.py:51] epoch 1799, training loss: 10263.86, average training loss: 10376.96, base loss: 14353.74
[INFO 2017-06-28 12:38:51,580 main.py:53] epoch 1799, testing
[INFO 2017-06-28 12:38:53,589 main.py:105] average testing loss: 10502.60, base loss: 14113.37
[INFO 2017-06-28 12:38:53,589 main.py:106] improve_loss: 3610.76, improve_percent: 0.26
[INFO 2017-06-28 12:38:53,589 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 12:38:54,016 main.py:51] epoch 1800, training loss: 8719.80, average training loss: 10374.31, base loss: 14350.88
[INFO 2017-06-28 12:38:54,443 main.py:51] epoch 1801, training loss: 10012.68, average training loss: 10373.36, base loss: 14349.36
[INFO 2017-06-28 12:38:54,867 main.py:51] epoch 1802, training loss: 11565.29, average training loss: 10374.73, base loss: 14352.37
[INFO 2017-06-28 12:38:55,312 main.py:51] epoch 1803, training loss: 11118.52, average training loss: 10375.51, base loss: 14353.02
[INFO 2017-06-28 12:38:55,769 main.py:51] epoch 1804, training loss: 10028.47, average training loss: 10376.04, base loss: 14353.01
[INFO 2017-06-28 12:38:56,207 main.py:51] epoch 1805, training loss: 9961.04, average training loss: 10375.38, base loss: 14353.07
[INFO 2017-06-28 12:38:56,623 main.py:51] epoch 1806, training loss: 10337.38, average training loss: 10375.27, base loss: 14354.14
[INFO 2017-06-28 12:38:57,063 main.py:51] epoch 1807, training loss: 11274.02, average training loss: 10376.62, base loss: 14355.55
[INFO 2017-06-28 12:38:57,480 main.py:51] epoch 1808, training loss: 10732.27, average training loss: 10376.69, base loss: 14355.58
[INFO 2017-06-28 12:38:57,916 main.py:51] epoch 1809, training loss: 10633.33, average training loss: 10378.04, base loss: 14358.00
[INFO 2017-06-28 12:38:58,342 main.py:51] epoch 1810, training loss: 10544.79, average training loss: 10377.83, base loss: 14358.76
[INFO 2017-06-28 12:38:58,770 main.py:51] epoch 1811, training loss: 8658.16, average training loss: 10376.60, base loss: 14357.75
[INFO 2017-06-28 12:38:59,198 main.py:51] epoch 1812, training loss: 10839.79, average training loss: 10378.43, base loss: 14360.30
[INFO 2017-06-28 12:38:59,638 main.py:51] epoch 1813, training loss: 9217.17, average training loss: 10377.37, base loss: 14360.07
[INFO 2017-06-28 12:39:00,091 main.py:51] epoch 1814, training loss: 10290.60, average training loss: 10377.11, base loss: 14360.62
[INFO 2017-06-28 12:39:00,509 main.py:51] epoch 1815, training loss: 10964.45, average training loss: 10377.05, base loss: 14361.77
[INFO 2017-06-28 12:39:00,933 main.py:51] epoch 1816, training loss: 9778.57, average training loss: 10374.26, base loss: 14359.54
[INFO 2017-06-28 12:39:01,366 main.py:51] epoch 1817, training loss: 9102.22, average training loss: 10373.50, base loss: 14359.65
[INFO 2017-06-28 12:39:01,799 main.py:51] epoch 1818, training loss: 9865.21, average training loss: 10373.39, base loss: 14358.99
[INFO 2017-06-28 12:39:02,238 main.py:51] epoch 1819, training loss: 10432.16, average training loss: 10373.73, base loss: 14358.51
[INFO 2017-06-28 12:39:02,662 main.py:51] epoch 1820, training loss: 10765.84, average training loss: 10374.29, base loss: 14358.81
[INFO 2017-06-28 12:39:03,074 main.py:51] epoch 1821, training loss: 10063.28, average training loss: 10374.31, base loss: 14357.76
[INFO 2017-06-28 12:39:03,492 main.py:51] epoch 1822, training loss: 10896.66, average training loss: 10373.92, base loss: 14357.62
[INFO 2017-06-28 12:39:03,912 main.py:51] epoch 1823, training loss: 11527.91, average training loss: 10373.23, base loss: 14357.53
[INFO 2017-06-28 12:39:04,369 main.py:51] epoch 1824, training loss: 11051.06, average training loss: 10374.31, base loss: 14359.07
[INFO 2017-06-28 12:39:04,804 main.py:51] epoch 1825, training loss: 9296.65, average training loss: 10373.78, base loss: 14358.20
[INFO 2017-06-28 12:39:05,249 main.py:51] epoch 1826, training loss: 10716.45, average training loss: 10373.66, base loss: 14358.88
[INFO 2017-06-28 12:39:05,696 main.py:51] epoch 1827, training loss: 10917.44, average training loss: 10373.88, base loss: 14361.47
[INFO 2017-06-28 12:39:06,124 main.py:51] epoch 1828, training loss: 9783.58, average training loss: 10372.46, base loss: 14360.99
[INFO 2017-06-28 12:39:06,552 main.py:51] epoch 1829, training loss: 10172.83, average training loss: 10371.25, base loss: 14358.94
[INFO 2017-06-28 12:39:06,985 main.py:51] epoch 1830, training loss: 9363.10, average training loss: 10368.07, base loss: 14357.49
[INFO 2017-06-28 12:39:07,405 main.py:51] epoch 1831, training loss: 10700.84, average training loss: 10366.42, base loss: 14357.59
[INFO 2017-06-28 12:39:07,835 main.py:51] epoch 1832, training loss: 10489.20, average training loss: 10364.98, base loss: 14357.40
[INFO 2017-06-28 12:39:08,261 main.py:51] epoch 1833, training loss: 11800.26, average training loss: 10364.83, base loss: 14357.85
[INFO 2017-06-28 12:39:08,709 main.py:51] epoch 1834, training loss: 9893.36, average training loss: 10362.34, base loss: 14353.85
[INFO 2017-06-28 12:39:09,141 main.py:51] epoch 1835, training loss: 11748.80, average training loss: 10363.24, base loss: 14353.96
[INFO 2017-06-28 12:39:09,551 main.py:51] epoch 1836, training loss: 9589.15, average training loss: 10361.71, base loss: 14350.23
[INFO 2017-06-28 12:39:09,992 main.py:51] epoch 1837, training loss: 10239.55, average training loss: 10360.46, base loss: 14348.08
[INFO 2017-06-28 12:39:10,399 main.py:51] epoch 1838, training loss: 11598.36, average training loss: 10361.30, base loss: 14349.58
[INFO 2017-06-28 12:39:10,835 main.py:51] epoch 1839, training loss: 10643.25, average training loss: 10359.99, base loss: 14347.47
[INFO 2017-06-28 12:39:11,244 main.py:51] epoch 1840, training loss: 10210.55, average training loss: 10359.68, base loss: 14348.35
[INFO 2017-06-28 12:39:11,673 main.py:51] epoch 1841, training loss: 11544.33, average training loss: 10361.86, base loss: 14352.47
[INFO 2017-06-28 12:39:12,106 main.py:51] epoch 1842, training loss: 9612.57, average training loss: 10360.52, base loss: 14351.63
[INFO 2017-06-28 12:39:12,545 main.py:51] epoch 1843, training loss: 9855.02, average training loss: 10360.15, base loss: 14352.27
[INFO 2017-06-28 12:39:12,975 main.py:51] epoch 1844, training loss: 10204.66, average training loss: 10358.13, base loss: 14350.22
[INFO 2017-06-28 12:39:13,407 main.py:51] epoch 1845, training loss: 10797.54, average training loss: 10358.15, base loss: 14350.34
[INFO 2017-06-28 12:39:13,825 main.py:51] epoch 1846, training loss: 10809.11, average training loss: 10359.04, base loss: 14352.56
[INFO 2017-06-28 12:39:14,249 main.py:51] epoch 1847, training loss: 7984.60, average training loss: 10356.32, base loss: 14348.94
[INFO 2017-06-28 12:39:14,693 main.py:51] epoch 1848, training loss: 10468.49, average training loss: 10357.48, base loss: 14351.32
[INFO 2017-06-28 12:39:15,129 main.py:51] epoch 1849, training loss: 11369.92, average training loss: 10359.42, base loss: 14355.15
[INFO 2017-06-28 12:39:15,545 main.py:51] epoch 1850, training loss: 11073.18, average training loss: 10359.92, base loss: 14357.29
[INFO 2017-06-28 12:39:15,961 main.py:51] epoch 1851, training loss: 11001.48, average training loss: 10359.86, base loss: 14358.63
[INFO 2017-06-28 12:39:16,375 main.py:51] epoch 1852, training loss: 10422.97, average training loss: 10359.38, base loss: 14358.06
[INFO 2017-06-28 12:39:16,809 main.py:51] epoch 1853, training loss: 10368.49, average training loss: 10359.30, base loss: 14358.37
[INFO 2017-06-28 12:39:17,223 main.py:51] epoch 1854, training loss: 9404.05, average training loss: 10359.64, base loss: 14359.16
[INFO 2017-06-28 12:39:17,666 main.py:51] epoch 1855, training loss: 9909.70, average training loss: 10359.27, base loss: 14358.31
[INFO 2017-06-28 12:39:18,125 main.py:51] epoch 1856, training loss: 10600.91, average training loss: 10355.80, base loss: 14355.36
[INFO 2017-06-28 12:39:18,543 main.py:51] epoch 1857, training loss: 10561.05, average training loss: 10356.70, base loss: 14356.10
[INFO 2017-06-28 12:39:18,974 main.py:51] epoch 1858, training loss: 10828.75, average training loss: 10357.53, base loss: 14356.96
[INFO 2017-06-28 12:39:19,412 main.py:51] epoch 1859, training loss: 9093.17, average training loss: 10356.27, base loss: 14355.86
[INFO 2017-06-28 12:39:19,836 main.py:51] epoch 1860, training loss: 10108.00, average training loss: 10356.05, base loss: 14355.17
[INFO 2017-06-28 12:39:20,278 main.py:51] epoch 1861, training loss: 9923.78, average training loss: 10355.03, base loss: 14353.98
[INFO 2017-06-28 12:39:20,744 main.py:51] epoch 1862, training loss: 10369.45, average training loss: 10352.28, base loss: 14351.45
[INFO 2017-06-28 12:39:21,171 main.py:51] epoch 1863, training loss: 9502.12, average training loss: 10351.48, base loss: 14350.53
[INFO 2017-06-28 12:39:21,592 main.py:51] epoch 1864, training loss: 10186.98, average training loss: 10348.17, base loss: 14345.90
[INFO 2017-06-28 12:39:22,014 main.py:51] epoch 1865, training loss: 9049.89, average training loss: 10346.93, base loss: 14344.67
[INFO 2017-06-28 12:39:22,428 main.py:51] epoch 1866, training loss: 10989.03, average training loss: 10347.65, base loss: 14347.04
[INFO 2017-06-28 12:39:22,853 main.py:51] epoch 1867, training loss: 10900.64, average training loss: 10347.23, base loss: 14347.14
[INFO 2017-06-28 12:39:23,276 main.py:51] epoch 1868, training loss: 10137.11, average training loss: 10347.54, base loss: 14348.41
[INFO 2017-06-28 12:39:23,702 main.py:51] epoch 1869, training loss: 9864.46, average training loss: 10347.19, base loss: 14349.04
[INFO 2017-06-28 12:39:24,140 main.py:51] epoch 1870, training loss: 10841.64, average training loss: 10347.19, base loss: 14349.64
[INFO 2017-06-28 12:39:24,546 main.py:51] epoch 1871, training loss: 11351.66, average training loss: 10348.68, base loss: 14352.25
[INFO 2017-06-28 12:39:24,962 main.py:51] epoch 1872, training loss: 9717.70, average training loss: 10348.58, base loss: 14353.43
[INFO 2017-06-28 12:39:25,364 main.py:51] epoch 1873, training loss: 9154.67, average training loss: 10347.00, base loss: 14350.21
[INFO 2017-06-28 12:39:25,781 main.py:51] epoch 1874, training loss: 9917.28, average training loss: 10345.15, base loss: 14347.75
[INFO 2017-06-28 12:39:26,215 main.py:51] epoch 1875, training loss: 10294.21, average training loss: 10345.48, base loss: 14348.86
[INFO 2017-06-28 12:39:26,627 main.py:51] epoch 1876, training loss: 9459.98, average training loss: 10346.05, base loss: 14350.23
[INFO 2017-06-28 12:39:27,036 main.py:51] epoch 1877, training loss: 10786.50, average training loss: 10346.27, base loss: 14353.12
[INFO 2017-06-28 12:39:27,451 main.py:51] epoch 1878, training loss: 10754.78, average training loss: 10346.67, base loss: 14353.83
[INFO 2017-06-28 12:39:27,882 main.py:51] epoch 1879, training loss: 10208.89, average training loss: 10346.82, base loss: 14354.72
[INFO 2017-06-28 12:39:28,318 main.py:51] epoch 1880, training loss: 10035.90, average training loss: 10347.11, base loss: 14356.61
[INFO 2017-06-28 12:39:28,748 main.py:51] epoch 1881, training loss: 10867.08, average training loss: 10347.88, base loss: 14358.81
[INFO 2017-06-28 12:39:29,165 main.py:51] epoch 1882, training loss: 9831.90, average training loss: 10348.58, base loss: 14360.89
[INFO 2017-06-28 12:39:29,601 main.py:51] epoch 1883, training loss: 9977.48, average training loss: 10349.58, base loss: 14363.36
[INFO 2017-06-28 12:39:30,025 main.py:51] epoch 1884, training loss: 9739.34, average training loss: 10349.49, base loss: 14364.31
[INFO 2017-06-28 12:39:30,453 main.py:51] epoch 1885, training loss: 10478.20, average training loss: 10348.60, base loss: 14364.85
[INFO 2017-06-28 12:39:30,870 main.py:51] epoch 1886, training loss: 10014.88, average training loss: 10348.98, base loss: 14363.74
[INFO 2017-06-28 12:39:31,304 main.py:51] epoch 1887, training loss: 8848.44, average training loss: 10347.07, base loss: 14360.84
[INFO 2017-06-28 12:39:31,742 main.py:51] epoch 1888, training loss: 9827.21, average training loss: 10345.95, base loss: 14359.53
[INFO 2017-06-28 12:39:32,186 main.py:51] epoch 1889, training loss: 10714.75, average training loss: 10346.89, base loss: 14360.61
[INFO 2017-06-28 12:39:32,611 main.py:51] epoch 1890, training loss: 9886.26, average training loss: 10346.86, base loss: 14361.80
[INFO 2017-06-28 12:39:33,047 main.py:51] epoch 1891, training loss: 10476.28, average training loss: 10345.30, base loss: 14360.87
[INFO 2017-06-28 12:39:33,481 main.py:51] epoch 1892, training loss: 8917.53, average training loss: 10344.66, base loss: 14360.94
[INFO 2017-06-28 12:39:33,925 main.py:51] epoch 1893, training loss: 10427.44, average training loss: 10343.92, base loss: 14359.81
[INFO 2017-06-28 12:39:34,366 main.py:51] epoch 1894, training loss: 10244.28, average training loss: 10343.36, base loss: 14359.26
[INFO 2017-06-28 12:39:34,809 main.py:51] epoch 1895, training loss: 11221.82, average training loss: 10344.59, base loss: 14359.79
[INFO 2017-06-28 12:39:35,235 main.py:51] epoch 1896, training loss: 9761.13, average training loss: 10345.16, base loss: 14361.25
[INFO 2017-06-28 12:39:35,652 main.py:51] epoch 1897, training loss: 9001.57, average training loss: 10344.01, base loss: 14359.93
[INFO 2017-06-28 12:39:36,073 main.py:51] epoch 1898, training loss: 8764.97, average training loss: 10340.30, base loss: 14353.90
[INFO 2017-06-28 12:39:36,511 main.py:51] epoch 1899, training loss: 8365.48, average training loss: 10339.01, base loss: 14351.31
[INFO 2017-06-28 12:39:36,511 main.py:53] epoch 1899, testing
[INFO 2017-06-28 12:39:38,505 main.py:105] average testing loss: 11123.37, base loss: 14902.81
[INFO 2017-06-28 12:39:38,505 main.py:106] improve_loss: 3779.44, improve_percent: 0.25
[INFO 2017-06-28 12:39:38,505 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 12:39:38,965 main.py:51] epoch 1900, training loss: 10564.35, average training loss: 10339.22, base loss: 14351.26
[INFO 2017-06-28 12:39:39,405 main.py:51] epoch 1901, training loss: 10523.25, average training loss: 10340.24, base loss: 14352.25
[INFO 2017-06-28 12:39:39,836 main.py:51] epoch 1902, training loss: 10384.72, average training loss: 10338.98, base loss: 14350.21
[INFO 2017-06-28 12:39:40,254 main.py:51] epoch 1903, training loss: 9958.44, average training loss: 10338.18, base loss: 14349.97
[INFO 2017-06-28 12:39:40,677 main.py:51] epoch 1904, training loss: 9892.99, average training loss: 10335.50, base loss: 14346.35
[INFO 2017-06-28 12:39:41,105 main.py:51] epoch 1905, training loss: 11159.55, average training loss: 10335.59, base loss: 14347.34
[INFO 2017-06-28 12:39:41,549 main.py:51] epoch 1906, training loss: 9451.83, average training loss: 10334.18, base loss: 14346.31
[INFO 2017-06-28 12:39:41,973 main.py:51] epoch 1907, training loss: 10565.83, average training loss: 10333.14, base loss: 14346.63
[INFO 2017-06-28 12:39:42,389 main.py:51] epoch 1908, training loss: 11733.56, average training loss: 10333.08, base loss: 14346.09
[INFO 2017-06-28 12:39:42,818 main.py:51] epoch 1909, training loss: 10605.73, average training loss: 10332.83, base loss: 14345.55
[INFO 2017-06-28 12:39:43,257 main.py:51] epoch 1910, training loss: 10211.70, average training loss: 10331.43, base loss: 14343.52
[INFO 2017-06-28 12:39:43,664 main.py:51] epoch 1911, training loss: 8997.34, average training loss: 10330.33, base loss: 14343.49
[INFO 2017-06-28 12:39:44,080 main.py:51] epoch 1912, training loss: 11896.11, average training loss: 10331.44, base loss: 14344.90
[INFO 2017-06-28 12:39:44,524 main.py:51] epoch 1913, training loss: 9370.90, average training loss: 10329.76, base loss: 14342.95
[INFO 2017-06-28 12:39:44,935 main.py:51] epoch 1914, training loss: 10030.91, average training loss: 10328.56, base loss: 14341.57
[INFO 2017-06-28 12:39:45,362 main.py:51] epoch 1915, training loss: 9676.79, average training loss: 10326.53, base loss: 14338.97
[INFO 2017-06-28 12:39:45,772 main.py:51] epoch 1916, training loss: 10151.85, average training loss: 10326.40, base loss: 14337.77
[INFO 2017-06-28 12:39:46,184 main.py:51] epoch 1917, training loss: 13537.12, average training loss: 10328.59, base loss: 14340.10
[INFO 2017-06-28 12:39:46,601 main.py:51] epoch 1918, training loss: 11717.37, average training loss: 10329.59, base loss: 14341.53
[INFO 2017-06-28 12:39:47,024 main.py:51] epoch 1919, training loss: 10527.52, average training loss: 10328.51, base loss: 14340.76
[INFO 2017-06-28 12:39:47,450 main.py:51] epoch 1920, training loss: 9479.58, average training loss: 10326.63, base loss: 14339.45
[INFO 2017-06-28 12:39:47,860 main.py:51] epoch 1921, training loss: 10065.91, average training loss: 10326.10, base loss: 14340.69
[INFO 2017-06-28 12:39:48,289 main.py:51] epoch 1922, training loss: 10159.52, average training loss: 10326.26, base loss: 14340.76
[INFO 2017-06-28 12:39:48,732 main.py:51] epoch 1923, training loss: 10966.42, average training loss: 10326.09, base loss: 14341.70
[INFO 2017-06-28 12:39:49,153 main.py:51] epoch 1924, training loss: 9403.05, average training loss: 10325.00, base loss: 14340.28
[INFO 2017-06-28 12:39:49,609 main.py:51] epoch 1925, training loss: 10069.70, average training loss: 10324.30, base loss: 14338.97
[INFO 2017-06-28 12:39:50,064 main.py:51] epoch 1926, training loss: 9066.67, average training loss: 10322.70, base loss: 14337.80
[INFO 2017-06-28 12:39:50,502 main.py:51] epoch 1927, training loss: 11366.68, average training loss: 10323.35, base loss: 14338.62
[INFO 2017-06-28 12:39:50,927 main.py:51] epoch 1928, training loss: 9914.16, average training loss: 10323.02, base loss: 14339.15
[INFO 2017-06-28 12:39:51,346 main.py:51] epoch 1929, training loss: 10832.60, average training loss: 10323.63, base loss: 14340.85
[INFO 2017-06-28 12:39:51,787 main.py:51] epoch 1930, training loss: 11601.48, average training loss: 10324.42, base loss: 14341.93
[INFO 2017-06-28 12:39:52,217 main.py:51] epoch 1931, training loss: 10543.28, average training loss: 10323.78, base loss: 14342.28
[INFO 2017-06-28 12:39:52,632 main.py:51] epoch 1932, training loss: 10521.70, average training loss: 10324.23, base loss: 14342.97
[INFO 2017-06-28 12:39:53,058 main.py:51] epoch 1933, training loss: 9496.16, average training loss: 10324.57, base loss: 14344.69
[INFO 2017-06-28 12:39:53,484 main.py:51] epoch 1934, training loss: 10378.31, average training loss: 10325.16, base loss: 14345.16
[INFO 2017-06-28 12:39:53,906 main.py:51] epoch 1935, training loss: 11619.83, average training loss: 10325.23, base loss: 14344.72
[INFO 2017-06-28 12:39:54,327 main.py:51] epoch 1936, training loss: 10354.25, average training loss: 10326.00, base loss: 14346.33
[INFO 2017-06-28 12:39:54,735 main.py:51] epoch 1937, training loss: 9754.91, average training loss: 10326.49, base loss: 14346.50
[INFO 2017-06-28 12:39:55,154 main.py:51] epoch 1938, training loss: 11214.03, average training loss: 10327.76, base loss: 14348.40
[INFO 2017-06-28 12:39:55,569 main.py:51] epoch 1939, training loss: 8732.21, average training loss: 10324.54, base loss: 14343.72
[INFO 2017-06-28 12:39:55,981 main.py:51] epoch 1940, training loss: 11912.29, average training loss: 10326.49, base loss: 14348.60
[INFO 2017-06-28 12:39:56,417 main.py:51] epoch 1941, training loss: 9111.25, average training loss: 10322.41, base loss: 14343.42
[INFO 2017-06-28 12:39:56,845 main.py:51] epoch 1942, training loss: 10322.20, average training loss: 10321.87, base loss: 14342.88
[INFO 2017-06-28 12:39:57,265 main.py:51] epoch 1943, training loss: 8434.62, average training loss: 10319.90, base loss: 14339.34
[INFO 2017-06-28 12:39:57,690 main.py:51] epoch 1944, training loss: 9873.51, average training loss: 10320.31, base loss: 14338.90
[INFO 2017-06-28 12:39:58,113 main.py:51] epoch 1945, training loss: 10881.76, average training loss: 10319.41, base loss: 14337.62
[INFO 2017-06-28 12:39:58,548 main.py:51] epoch 1946, training loss: 10000.81, average training loss: 10319.98, base loss: 14339.64
[INFO 2017-06-28 12:39:58,964 main.py:51] epoch 1947, training loss: 9710.81, average training loss: 10319.76, base loss: 14338.96
[INFO 2017-06-28 12:39:59,407 main.py:51] epoch 1948, training loss: 12500.78, average training loss: 10320.48, base loss: 14341.19
[INFO 2017-06-28 12:39:59,860 main.py:51] epoch 1949, training loss: 11165.42, average training loss: 10322.42, base loss: 14344.28
[INFO 2017-06-28 12:40:00,274 main.py:51] epoch 1950, training loss: 9584.94, average training loss: 10322.04, base loss: 14344.15
[INFO 2017-06-28 12:40:00,708 main.py:51] epoch 1951, training loss: 10563.33, average training loss: 10319.95, base loss: 14340.56
[INFO 2017-06-28 12:40:01,119 main.py:51] epoch 1952, training loss: 10788.26, average training loss: 10320.04, base loss: 14342.03
[INFO 2017-06-28 12:40:01,547 main.py:51] epoch 1953, training loss: 11262.72, average training loss: 10321.64, base loss: 14346.22
[INFO 2017-06-28 12:40:01,955 main.py:51] epoch 1954, training loss: 11051.14, average training loss: 10323.09, base loss: 14347.38
[INFO 2017-06-28 12:40:02,362 main.py:51] epoch 1955, training loss: 9805.44, average training loss: 10321.73, base loss: 14345.44
[INFO 2017-06-28 12:40:02,779 main.py:51] epoch 1956, training loss: 10638.32, average training loss: 10322.46, base loss: 14347.70
[INFO 2017-06-28 12:40:03,202 main.py:51] epoch 1957, training loss: 9533.62, average training loss: 10322.22, base loss: 14349.47
[INFO 2017-06-28 12:40:03,630 main.py:51] epoch 1958, training loss: 9517.57, average training loss: 10321.50, base loss: 14349.58
[INFO 2017-06-28 12:40:04,043 main.py:51] epoch 1959, training loss: 9848.11, average training loss: 10320.09, base loss: 14349.28
[INFO 2017-06-28 12:40:04,485 main.py:51] epoch 1960, training loss: 10629.96, average training loss: 10321.67, base loss: 14351.32
[INFO 2017-06-28 12:40:04,917 main.py:51] epoch 1961, training loss: 9275.19, average training loss: 10320.10, base loss: 14350.23
[INFO 2017-06-28 12:40:05,335 main.py:51] epoch 1962, training loss: 11518.79, average training loss: 10320.45, base loss: 14350.73
[INFO 2017-06-28 12:40:05,749 main.py:51] epoch 1963, training loss: 10007.13, average training loss: 10318.21, base loss: 14349.78
[INFO 2017-06-28 12:40:06,182 main.py:51] epoch 1964, training loss: 10577.49, average training loss: 10317.65, base loss: 14349.21
[INFO 2017-06-28 12:40:06,590 main.py:51] epoch 1965, training loss: 9520.42, average training loss: 10316.22, base loss: 14347.14
[INFO 2017-06-28 12:40:07,017 main.py:51] epoch 1966, training loss: 8567.06, average training loss: 10314.33, base loss: 14345.10
[INFO 2017-06-28 12:40:07,438 main.py:51] epoch 1967, training loss: 9210.67, average training loss: 10311.35, base loss: 14342.71
[INFO 2017-06-28 12:40:07,869 main.py:51] epoch 1968, training loss: 9619.96, average training loss: 10311.40, base loss: 14344.86
[INFO 2017-06-28 12:40:08,272 main.py:51] epoch 1969, training loss: 8965.49, average training loss: 10310.53, base loss: 14342.08
[INFO 2017-06-28 12:40:08,706 main.py:51] epoch 1970, training loss: 10123.20, average training loss: 10310.35, base loss: 14342.66
[INFO 2017-06-28 12:40:09,152 main.py:51] epoch 1971, training loss: 11779.62, average training loss: 10310.48, base loss: 14344.32
[INFO 2017-06-28 12:40:09,577 main.py:51] epoch 1972, training loss: 9999.14, average training loss: 10309.56, base loss: 14343.24
[INFO 2017-06-28 12:40:10,029 main.py:51] epoch 1973, training loss: 9857.25, average training loss: 10308.83, base loss: 14341.48
[INFO 2017-06-28 12:40:10,453 main.py:51] epoch 1974, training loss: 10644.98, average training loss: 10309.82, base loss: 14342.78
[INFO 2017-06-28 12:40:10,889 main.py:51] epoch 1975, training loss: 10210.17, average training loss: 10309.95, base loss: 14343.00
[INFO 2017-06-28 12:40:11,307 main.py:51] epoch 1976, training loss: 10731.72, average training loss: 10309.36, base loss: 14341.26
[INFO 2017-06-28 12:40:11,733 main.py:51] epoch 1977, training loss: 10800.88, average training loss: 10309.85, base loss: 14343.05
[INFO 2017-06-28 12:40:12,168 main.py:51] epoch 1978, training loss: 8846.10, average training loss: 10308.64, base loss: 14340.39
[INFO 2017-06-28 12:40:12,595 main.py:51] epoch 1979, training loss: 10064.53, average training loss: 10307.24, base loss: 14340.33
[INFO 2017-06-28 12:40:13,033 main.py:51] epoch 1980, training loss: 10892.29, average training loss: 10306.48, base loss: 14339.05
[INFO 2017-06-28 12:40:13,457 main.py:51] epoch 1981, training loss: 10245.52, average training loss: 10306.78, base loss: 14338.93
[INFO 2017-06-28 12:40:13,880 main.py:51] epoch 1982, training loss: 10757.79, average training loss: 10306.67, base loss: 14339.96
[INFO 2017-06-28 12:40:14,330 main.py:51] epoch 1983, training loss: 10974.44, average training loss: 10306.09, base loss: 14339.49
[INFO 2017-06-28 12:40:14,765 main.py:51] epoch 1984, training loss: 10250.58, average training loss: 10305.77, base loss: 14340.43
[INFO 2017-06-28 12:40:15,199 main.py:51] epoch 1985, training loss: 10013.44, average training loss: 10304.44, base loss: 14339.01
[INFO 2017-06-28 12:40:15,621 main.py:51] epoch 1986, training loss: 11125.18, average training loss: 10304.51, base loss: 14338.18
[INFO 2017-06-28 12:40:16,062 main.py:51] epoch 1987, training loss: 9219.41, average training loss: 10301.37, base loss: 14333.45
[INFO 2017-06-28 12:40:16,493 main.py:51] epoch 1988, training loss: 10485.18, average training loss: 10301.05, base loss: 14333.61
[INFO 2017-06-28 12:40:16,912 main.py:51] epoch 1989, training loss: 12014.60, average training loss: 10303.86, base loss: 14337.83
[INFO 2017-06-28 12:40:17,328 main.py:51] epoch 1990, training loss: 9234.01, average training loss: 10302.14, base loss: 14334.03
[INFO 2017-06-28 12:40:17,737 main.py:51] epoch 1991, training loss: 8791.87, average training loss: 10301.41, base loss: 14333.12
[INFO 2017-06-28 12:40:18,148 main.py:51] epoch 1992, training loss: 11548.23, average training loss: 10301.99, base loss: 14333.90
[INFO 2017-06-28 12:40:18,584 main.py:51] epoch 1993, training loss: 12501.56, average training loss: 10305.78, base loss: 14339.78
[INFO 2017-06-28 12:40:19,007 main.py:51] epoch 1994, training loss: 10000.48, average training loss: 10305.40, base loss: 14340.63
[INFO 2017-06-28 12:40:19,435 main.py:51] epoch 1995, training loss: 9161.96, average training loss: 10303.94, base loss: 14338.33
[INFO 2017-06-28 12:40:19,872 main.py:51] epoch 1996, training loss: 10168.09, average training loss: 10305.04, base loss: 14341.15
[INFO 2017-06-28 12:40:20,281 main.py:51] epoch 1997, training loss: 8336.20, average training loss: 10302.29, base loss: 14336.41
[INFO 2017-06-28 12:40:20,678 main.py:51] epoch 1998, training loss: 10503.86, average training loss: 10301.82, base loss: 14334.80
[INFO 2017-06-28 12:40:21,109 main.py:51] epoch 1999, training loss: 11054.14, average training loss: 10302.57, base loss: 14335.42
[INFO 2017-06-28 12:40:21,110 main.py:53] epoch 1999, testing
[INFO 2017-06-28 12:40:23,126 main.py:105] average testing loss: 10854.72, base loss: 14731.03
[INFO 2017-06-28 12:40:23,126 main.py:106] improve_loss: 3876.32, improve_percent: 0.26
[INFO 2017-06-28 12:40:23,126 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 12:40:23,544 main.py:51] epoch 2000, training loss: 10495.16, average training loss: 10302.74, base loss: 14336.65
[INFO 2017-06-28 12:40:23,975 main.py:51] epoch 2001, training loss: 10318.09, average training loss: 10302.08, base loss: 14335.29
[INFO 2017-06-28 12:40:24,386 main.py:51] epoch 2002, training loss: 11469.82, average training loss: 10304.01, base loss: 14337.08
[INFO 2017-06-28 12:40:24,817 main.py:51] epoch 2003, training loss: 11029.16, average training loss: 10304.36, base loss: 14338.71
[INFO 2017-06-28 12:40:25,254 main.py:51] epoch 2004, training loss: 10163.85, average training loss: 10304.53, base loss: 14338.92
[INFO 2017-06-28 12:40:25,708 main.py:51] epoch 2005, training loss: 9479.43, average training loss: 10303.82, base loss: 14337.14
[INFO 2017-06-28 12:40:26,140 main.py:51] epoch 2006, training loss: 8999.80, average training loss: 10301.39, base loss: 14332.30
[INFO 2017-06-28 12:40:26,575 main.py:51] epoch 2007, training loss: 9263.24, average training loss: 10298.80, base loss: 14328.65
[INFO 2017-06-28 12:40:26,999 main.py:51] epoch 2008, training loss: 9513.57, average training loss: 10297.36, base loss: 14329.55
[INFO 2017-06-28 12:40:27,430 main.py:51] epoch 2009, training loss: 8924.20, average training loss: 10296.74, base loss: 14329.04
[INFO 2017-06-28 12:40:27,858 main.py:51] epoch 2010, training loss: 9459.66, average training loss: 10296.31, base loss: 14328.70
[INFO 2017-06-28 12:40:28,299 main.py:51] epoch 2011, training loss: 9249.85, average training loss: 10295.31, base loss: 14328.46
[INFO 2017-06-28 12:40:28,724 main.py:51] epoch 2012, training loss: 11822.51, average training loss: 10296.82, base loss: 14331.27
[INFO 2017-06-28 12:40:29,142 main.py:51] epoch 2013, training loss: 10426.62, average training loss: 10296.01, base loss: 14329.23
[INFO 2017-06-28 12:40:29,568 main.py:51] epoch 2014, training loss: 11939.44, average training loss: 10297.33, base loss: 14332.67
[INFO 2017-06-28 12:40:29,988 main.py:51] epoch 2015, training loss: 10606.85, average training loss: 10298.56, base loss: 14335.08
[INFO 2017-06-28 12:40:30,416 main.py:51] epoch 2016, training loss: 10314.66, average training loss: 10299.03, base loss: 14336.41
[INFO 2017-06-28 12:40:30,853 main.py:51] epoch 2017, training loss: 10892.57, average training loss: 10299.77, base loss: 14337.55
[INFO 2017-06-28 12:40:31,274 main.py:51] epoch 2018, training loss: 9051.98, average training loss: 10298.38, base loss: 14335.40
[INFO 2017-06-28 12:40:31,720 main.py:51] epoch 2019, training loss: 9997.97, average training loss: 10297.00, base loss: 14332.49
[INFO 2017-06-28 12:40:32,127 main.py:51] epoch 2020, training loss: 10042.98, average training loss: 10297.24, base loss: 14334.71
[INFO 2017-06-28 12:40:32,536 main.py:51] epoch 2021, training loss: 10258.58, average training loss: 10296.10, base loss: 14333.91
[INFO 2017-06-28 12:40:32,966 main.py:51] epoch 2022, training loss: 10965.97, average training loss: 10296.67, base loss: 14335.05
[INFO 2017-06-28 12:40:33,400 main.py:51] epoch 2023, training loss: 10680.37, average training loss: 10296.30, base loss: 14334.42
[INFO 2017-06-28 12:40:33,850 main.py:51] epoch 2024, training loss: 10880.46, average training loss: 10297.44, base loss: 14336.93
[INFO 2017-06-28 12:40:34,290 main.py:51] epoch 2025, training loss: 10999.25, average training loss: 10298.21, base loss: 14339.63
[INFO 2017-06-28 12:40:34,743 main.py:51] epoch 2026, training loss: 8479.29, average training loss: 10297.31, base loss: 14340.53
[INFO 2017-06-28 12:40:35,186 main.py:51] epoch 2027, training loss: 10439.14, average training loss: 10295.73, base loss: 14339.25
[INFO 2017-06-28 12:40:35,602 main.py:51] epoch 2028, training loss: 11942.42, average training loss: 10297.16, base loss: 14342.68
[INFO 2017-06-28 12:40:36,050 main.py:51] epoch 2029, training loss: 10836.79, average training loss: 10298.24, base loss: 14343.66
[INFO 2017-06-28 12:40:36,486 main.py:51] epoch 2030, training loss: 8778.68, average training loss: 10297.55, base loss: 14341.39
[INFO 2017-06-28 12:40:36,918 main.py:51] epoch 2031, training loss: 10143.39, average training loss: 10296.50, base loss: 14338.95
[INFO 2017-06-28 12:40:37,354 main.py:51] epoch 2032, training loss: 11100.35, average training loss: 10298.08, base loss: 14342.12
[INFO 2017-06-28 12:40:37,820 main.py:51] epoch 2033, training loss: 10915.14, average training loss: 10299.12, base loss: 14343.39
[INFO 2017-06-28 12:40:38,236 main.py:51] epoch 2034, training loss: 10113.95, average training loss: 10299.46, base loss: 14344.01
[INFO 2017-06-28 12:40:38,644 main.py:51] epoch 2035, training loss: 12177.59, average training loss: 10302.50, base loss: 14348.83
[INFO 2017-06-28 12:40:39,084 main.py:51] epoch 2036, training loss: 10999.71, average training loss: 10303.83, base loss: 14350.92
[INFO 2017-06-28 12:40:39,537 main.py:51] epoch 2037, training loss: 9896.44, average training loss: 10301.17, base loss: 14348.90
[INFO 2017-06-28 12:40:39,963 main.py:51] epoch 2038, training loss: 9186.45, average training loss: 10300.84, base loss: 14348.46
[INFO 2017-06-28 12:40:40,396 main.py:51] epoch 2039, training loss: 9894.13, average training loss: 10298.06, base loss: 14345.03
[INFO 2017-06-28 12:40:40,818 main.py:51] epoch 2040, training loss: 10227.50, average training loss: 10297.08, base loss: 14344.63
[INFO 2017-06-28 12:40:41,243 main.py:51] epoch 2041, training loss: 9819.75, average training loss: 10295.62, base loss: 14343.79
[INFO 2017-06-28 12:40:41,716 main.py:51] epoch 2042, training loss: 11943.12, average training loss: 10298.13, base loss: 14347.85
[INFO 2017-06-28 12:40:42,161 main.py:51] epoch 2043, training loss: 8917.98, average training loss: 10296.23, base loss: 14346.93
[INFO 2017-06-28 12:40:42,600 main.py:51] epoch 2044, training loss: 10113.82, average training loss: 10295.43, base loss: 14346.67
[INFO 2017-06-28 12:40:43,038 main.py:51] epoch 2045, training loss: 9682.83, average training loss: 10295.36, base loss: 14346.26
[INFO 2017-06-28 12:40:43,465 main.py:51] epoch 2046, training loss: 11818.40, average training loss: 10296.46, base loss: 14348.86
[INFO 2017-06-28 12:40:43,891 main.py:51] epoch 2047, training loss: 10063.68, average training loss: 10297.05, base loss: 14351.30
[INFO 2017-06-28 12:40:44,320 main.py:51] epoch 2048, training loss: 11389.32, average training loss: 10298.11, base loss: 14352.89
[INFO 2017-06-28 12:40:44,741 main.py:51] epoch 2049, training loss: 10674.81, average training loss: 10298.73, base loss: 14354.78
[INFO 2017-06-28 12:40:45,135 main.py:51] epoch 2050, training loss: 8986.86, average training loss: 10297.43, base loss: 14353.35
[INFO 2017-06-28 12:40:45,562 main.py:51] epoch 2051, training loss: 10078.51, average training loss: 10294.78, base loss: 14349.69
[INFO 2017-06-28 12:40:45,989 main.py:51] epoch 2052, training loss: 10732.42, average training loss: 10294.92, base loss: 14350.79
[INFO 2017-06-28 12:40:46,411 main.py:51] epoch 2053, training loss: 9952.56, average training loss: 10294.18, base loss: 14350.19
[INFO 2017-06-28 12:40:46,825 main.py:51] epoch 2054, training loss: 10910.53, average training loss: 10295.46, base loss: 14352.19
[INFO 2017-06-28 12:40:47,241 main.py:51] epoch 2055, training loss: 9097.14, average training loss: 10291.88, base loss: 14347.13
[INFO 2017-06-28 12:40:47,663 main.py:51] epoch 2056, training loss: 10312.82, average training loss: 10290.98, base loss: 14344.73
[INFO 2017-06-28 12:40:48,114 main.py:51] epoch 2057, training loss: 10479.09, average training loss: 10291.79, base loss: 14346.16
[INFO 2017-06-28 12:40:48,552 main.py:51] epoch 2058, training loss: 10087.64, average training loss: 10292.45, base loss: 14347.57
[INFO 2017-06-28 12:40:48,989 main.py:51] epoch 2059, training loss: 8841.09, average training loss: 10291.43, base loss: 14345.43
[INFO 2017-06-28 12:40:49,418 main.py:51] epoch 2060, training loss: 10129.80, average training loss: 10291.62, base loss: 14345.55
[INFO 2017-06-28 12:40:49,869 main.py:51] epoch 2061, training loss: 10524.70, average training loss: 10291.05, base loss: 14343.92
[INFO 2017-06-28 12:40:50,285 main.py:51] epoch 2062, training loss: 12288.98, average training loss: 10293.60, base loss: 14348.20
[INFO 2017-06-28 12:40:50,716 main.py:51] epoch 2063, training loss: 9951.92, average training loss: 10292.48, base loss: 14346.75
[INFO 2017-06-28 12:40:51,137 main.py:51] epoch 2064, training loss: 11907.09, average training loss: 10292.58, base loss: 14347.28
[INFO 2017-06-28 12:40:51,568 main.py:51] epoch 2065, training loss: 8366.07, average training loss: 10291.42, base loss: 14345.48
[INFO 2017-06-28 12:40:51,992 main.py:51] epoch 2066, training loss: 9820.65, average training loss: 10291.08, base loss: 14345.71
[INFO 2017-06-28 12:40:52,427 main.py:51] epoch 2067, training loss: 10515.68, average training loss: 10289.28, base loss: 14344.86
[INFO 2017-06-28 12:40:52,871 main.py:51] epoch 2068, training loss: 10288.36, average training loss: 10290.05, base loss: 14346.28
[INFO 2017-06-28 12:40:53,303 main.py:51] epoch 2069, training loss: 9138.07, average training loss: 10287.52, base loss: 14343.40
[INFO 2017-06-28 12:40:53,731 main.py:51] epoch 2070, training loss: 9736.60, average training loss: 10287.47, base loss: 14344.02
[INFO 2017-06-28 12:40:54,167 main.py:51] epoch 2071, training loss: 8812.57, average training loss: 10284.76, base loss: 14339.96
[INFO 2017-06-28 12:40:54,602 main.py:51] epoch 2072, training loss: 9991.89, average training loss: 10284.61, base loss: 14341.21
[INFO 2017-06-28 12:40:55,003 main.py:51] epoch 2073, training loss: 9238.03, average training loss: 10283.35, base loss: 14339.90
[INFO 2017-06-28 12:40:55,440 main.py:51] epoch 2074, training loss: 11214.84, average training loss: 10285.13, base loss: 14341.93
[INFO 2017-06-28 12:40:55,868 main.py:51] epoch 2075, training loss: 11497.75, average training loss: 10287.04, base loss: 14346.56
[INFO 2017-06-28 12:40:56,286 main.py:51] epoch 2076, training loss: 10408.22, average training loss: 10286.65, base loss: 14346.03
[INFO 2017-06-28 12:40:56,725 main.py:51] epoch 2077, training loss: 11657.53, average training loss: 10288.05, base loss: 14349.16
[INFO 2017-06-28 12:40:57,136 main.py:51] epoch 2078, training loss: 10242.83, average training loss: 10287.57, base loss: 14348.97
[INFO 2017-06-28 12:40:57,561 main.py:51] epoch 2079, training loss: 9892.86, average training loss: 10287.38, base loss: 14347.92
[INFO 2017-06-28 12:40:57,997 main.py:51] epoch 2080, training loss: 9094.50, average training loss: 10286.94, base loss: 14347.78
[INFO 2017-06-28 12:40:58,429 main.py:51] epoch 2081, training loss: 9196.34, average training loss: 10285.23, base loss: 14346.63
[INFO 2017-06-28 12:40:58,868 main.py:51] epoch 2082, training loss: 9947.63, average training loss: 10285.01, base loss: 14347.05
[INFO 2017-06-28 12:40:59,294 main.py:51] epoch 2083, training loss: 10421.85, average training loss: 10286.72, base loss: 14350.45
[INFO 2017-06-28 12:40:59,722 main.py:51] epoch 2084, training loss: 9425.75, average training loss: 10286.73, base loss: 14350.55
[INFO 2017-06-28 12:41:00,154 main.py:51] epoch 2085, training loss: 8169.52, average training loss: 10283.32, base loss: 14347.14
[INFO 2017-06-28 12:41:00,591 main.py:51] epoch 2086, training loss: 10309.30, average training loss: 10284.02, base loss: 14349.81
[INFO 2017-06-28 12:41:01,029 main.py:51] epoch 2087, training loss: 10155.35, average training loss: 10283.52, base loss: 14349.95
[INFO 2017-06-28 12:41:01,448 main.py:51] epoch 2088, training loss: 10472.60, average training loss: 10281.25, base loss: 14346.95
[INFO 2017-06-28 12:41:01,897 main.py:51] epoch 2089, training loss: 10407.68, average training loss: 10281.12, base loss: 14347.21
[INFO 2017-06-28 12:41:02,322 main.py:51] epoch 2090, training loss: 10545.12, average training loss: 10282.13, base loss: 14350.69
[INFO 2017-06-28 12:41:02,752 main.py:51] epoch 2091, training loss: 9748.29, average training loss: 10281.99, base loss: 14351.91
[INFO 2017-06-28 12:41:03,185 main.py:51] epoch 2092, training loss: 9471.40, average training loss: 10281.54, base loss: 14351.44
[INFO 2017-06-28 12:41:03,611 main.py:51] epoch 2093, training loss: 9877.04, average training loss: 10281.05, base loss: 14350.39
[INFO 2017-06-28 12:41:04,048 main.py:51] epoch 2094, training loss: 9541.51, average training loss: 10279.57, base loss: 14349.02
[INFO 2017-06-28 12:41:04,480 main.py:51] epoch 2095, training loss: 10807.00, average training loss: 10280.33, base loss: 14349.91
[INFO 2017-06-28 12:41:04,925 main.py:51] epoch 2096, training loss: 9745.57, average training loss: 10280.80, base loss: 14352.07
[INFO 2017-06-28 12:41:05,341 main.py:51] epoch 2097, training loss: 10282.49, average training loss: 10280.63, base loss: 14350.69
[INFO 2017-06-28 12:41:05,760 main.py:51] epoch 2098, training loss: 10129.37, average training loss: 10280.58, base loss: 14350.93
[INFO 2017-06-28 12:41:06,174 main.py:51] epoch 2099, training loss: 10117.28, average training loss: 10281.23, base loss: 14350.82
[INFO 2017-06-28 12:41:06,175 main.py:53] epoch 2099, testing
[INFO 2017-06-28 12:41:08,111 main.py:105] average testing loss: 10682.80, base loss: 15056.88
[INFO 2017-06-28 12:41:08,111 main.py:106] improve_loss: 4374.08, improve_percent: 0.29
[INFO 2017-06-28 12:41:08,111 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:41:08,124 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:41:08,563 main.py:51] epoch 2100, training loss: 11029.62, average training loss: 10281.77, base loss: 14352.37
[INFO 2017-06-28 12:41:08,994 main.py:51] epoch 2101, training loss: 10982.14, average training loss: 10281.40, base loss: 14353.01
[INFO 2017-06-28 12:41:09,454 main.py:51] epoch 2102, training loss: 9537.23, average training loss: 10281.07, base loss: 14353.44
[INFO 2017-06-28 12:41:09,876 main.py:51] epoch 2103, training loss: 10275.19, average training loss: 10279.19, base loss: 14350.12
[INFO 2017-06-28 12:41:10,328 main.py:51] epoch 2104, training loss: 10157.48, average training loss: 10278.80, base loss: 14350.67
[INFO 2017-06-28 12:41:10,757 main.py:51] epoch 2105, training loss: 9976.04, average training loss: 10279.18, base loss: 14352.00
[INFO 2017-06-28 12:41:11,187 main.py:51] epoch 2106, training loss: 9309.73, average training loss: 10277.39, base loss: 14349.19
[INFO 2017-06-28 12:41:11,617 main.py:51] epoch 2107, training loss: 9483.39, average training loss: 10276.70, base loss: 14349.98
[INFO 2017-06-28 12:41:12,051 main.py:51] epoch 2108, training loss: 10513.10, average training loss: 10277.55, base loss: 14352.13
[INFO 2017-06-28 12:41:12,490 main.py:51] epoch 2109, training loss: 9365.80, average training loss: 10276.47, base loss: 14350.40
[INFO 2017-06-28 12:41:12,921 main.py:51] epoch 2110, training loss: 9328.95, average training loss: 10275.42, base loss: 14350.64
[INFO 2017-06-28 12:41:13,336 main.py:51] epoch 2111, training loss: 11473.62, average training loss: 10277.05, base loss: 14353.31
[INFO 2017-06-28 12:41:13,753 main.py:51] epoch 2112, training loss: 9368.99, average training loss: 10274.12, base loss: 14350.08
[INFO 2017-06-28 12:41:14,197 main.py:51] epoch 2113, training loss: 10262.20, average training loss: 10274.63, base loss: 14351.68
[INFO 2017-06-28 12:41:14,609 main.py:51] epoch 2114, training loss: 9532.24, average training loss: 10274.39, base loss: 14351.41
[INFO 2017-06-28 12:41:15,019 main.py:51] epoch 2115, training loss: 9929.69, average training loss: 10273.75, base loss: 14351.16
[INFO 2017-06-28 12:41:15,452 main.py:51] epoch 2116, training loss: 9495.36, average training loss: 10269.98, base loss: 14347.21
[INFO 2017-06-28 12:41:15,890 main.py:51] epoch 2117, training loss: 9621.15, average training loss: 10269.04, base loss: 14346.84
[INFO 2017-06-28 12:41:16,301 main.py:51] epoch 2118, training loss: 10849.17, average training loss: 10269.48, base loss: 14348.51
[INFO 2017-06-28 12:41:16,714 main.py:51] epoch 2119, training loss: 9161.49, average training loss: 10268.31, base loss: 14348.43
[INFO 2017-06-28 12:41:17,124 main.py:51] epoch 2120, training loss: 9435.26, average training loss: 10267.13, base loss: 14346.70
[INFO 2017-06-28 12:41:17,533 main.py:51] epoch 2121, training loss: 10594.60, average training loss: 10267.19, base loss: 14348.77
[INFO 2017-06-28 12:41:17,987 main.py:51] epoch 2122, training loss: 9661.07, average training loss: 10264.92, base loss: 14345.06
[INFO 2017-06-28 12:41:18,417 main.py:51] epoch 2123, training loss: 10342.91, average training loss: 10265.53, base loss: 14345.16
[INFO 2017-06-28 12:41:18,830 main.py:51] epoch 2124, training loss: 9499.79, average training loss: 10265.13, base loss: 14344.47
[INFO 2017-06-28 12:41:19,247 main.py:51] epoch 2125, training loss: 10060.06, average training loss: 10263.77, base loss: 14342.98
[INFO 2017-06-28 12:41:19,674 main.py:51] epoch 2126, training loss: 9102.84, average training loss: 10261.95, base loss: 14341.42
[INFO 2017-06-28 12:41:20,089 main.py:51] epoch 2127, training loss: 11121.16, average training loss: 10263.55, base loss: 14343.47
[INFO 2017-06-28 12:41:20,526 main.py:51] epoch 2128, training loss: 11005.39, average training loss: 10264.21, base loss: 14344.85
[INFO 2017-06-28 12:41:20,948 main.py:51] epoch 2129, training loss: 9982.60, average training loss: 10263.63, base loss: 14345.08
[INFO 2017-06-28 12:41:21,350 main.py:51] epoch 2130, training loss: 11544.38, average training loss: 10266.48, base loss: 14349.75
[INFO 2017-06-28 12:41:21,777 main.py:51] epoch 2131, training loss: 9244.36, average training loss: 10266.47, base loss: 14351.14
[INFO 2017-06-28 12:41:22,212 main.py:51] epoch 2132, training loss: 11176.97, average training loss: 10264.52, base loss: 14348.53
[INFO 2017-06-28 12:41:22,635 main.py:51] epoch 2133, training loss: 11054.02, average training loss: 10264.37, base loss: 14348.19
[INFO 2017-06-28 12:41:23,073 main.py:51] epoch 2134, training loss: 10363.50, average training loss: 10265.62, base loss: 14350.12
[INFO 2017-06-28 12:41:23,517 main.py:51] epoch 2135, training loss: 9653.28, average training loss: 10266.77, base loss: 14352.26
[INFO 2017-06-28 12:41:23,939 main.py:51] epoch 2136, training loss: 9778.41, average training loss: 10264.71, base loss: 14349.51
[INFO 2017-06-28 12:41:24,364 main.py:51] epoch 2137, training loss: 11123.57, average training loss: 10264.74, base loss: 14350.70
[INFO 2017-06-28 12:41:24,786 main.py:51] epoch 2138, training loss: 9249.80, average training loss: 10264.68, base loss: 14350.67
[INFO 2017-06-28 12:41:25,220 main.py:51] epoch 2139, training loss: 10393.22, average training loss: 10265.17, base loss: 14350.47
[INFO 2017-06-28 12:41:25,636 main.py:51] epoch 2140, training loss: 10275.90, average training loss: 10262.41, base loss: 14347.36
[INFO 2017-06-28 12:41:26,069 main.py:51] epoch 2141, training loss: 9822.24, average training loss: 10262.12, base loss: 14348.90
[INFO 2017-06-28 12:41:26,505 main.py:51] epoch 2142, training loss: 10594.23, average training loss: 10263.12, base loss: 14350.42
[INFO 2017-06-28 12:41:26,938 main.py:51] epoch 2143, training loss: 10646.49, average training loss: 10263.69, base loss: 14351.74
[INFO 2017-06-28 12:41:27,349 main.py:51] epoch 2144, training loss: 9999.50, average training loss: 10262.10, base loss: 14349.22
[INFO 2017-06-28 12:41:27,772 main.py:51] epoch 2145, training loss: 8790.72, average training loss: 10260.31, base loss: 14348.69
[INFO 2017-06-28 12:41:28,187 main.py:51] epoch 2146, training loss: 9891.03, average training loss: 10258.48, base loss: 14347.45
[INFO 2017-06-28 12:41:28,610 main.py:51] epoch 2147, training loss: 8631.85, average training loss: 10256.73, base loss: 14345.16
[INFO 2017-06-28 12:41:29,021 main.py:51] epoch 2148, training loss: 9170.85, average training loss: 10255.53, base loss: 14344.07
[INFO 2017-06-28 12:41:29,436 main.py:51] epoch 2149, training loss: 9411.97, average training loss: 10253.75, base loss: 14339.83
[INFO 2017-06-28 12:41:29,866 main.py:51] epoch 2150, training loss: 10425.04, average training loss: 10253.87, base loss: 14338.66
[INFO 2017-06-28 12:41:30,304 main.py:51] epoch 2151, training loss: 10481.37, average training loss: 10254.94, base loss: 14340.55
[INFO 2017-06-28 12:41:30,732 main.py:51] epoch 2152, training loss: 9067.57, average training loss: 10255.14, base loss: 14342.41
[INFO 2017-06-28 12:41:31,182 main.py:51] epoch 2153, training loss: 10884.01, average training loss: 10255.20, base loss: 14342.69
[INFO 2017-06-28 12:41:31,636 main.py:51] epoch 2154, training loss: 10872.43, average training loss: 10255.65, base loss: 14344.20
[INFO 2017-06-28 12:41:32,067 main.py:51] epoch 2155, training loss: 9422.24, average training loss: 10254.46, base loss: 14343.47
[INFO 2017-06-28 12:41:32,501 main.py:51] epoch 2156, training loss: 10378.30, average training loss: 10252.18, base loss: 14341.56
[INFO 2017-06-28 12:41:32,935 main.py:51] epoch 2157, training loss: 10745.02, average training loss: 10250.81, base loss: 14339.37
[INFO 2017-06-28 12:41:33,372 main.py:51] epoch 2158, training loss: 9329.74, average training loss: 10250.19, base loss: 14339.07
[INFO 2017-06-28 12:41:33,789 main.py:51] epoch 2159, training loss: 8953.42, average training loss: 10249.10, base loss: 14338.09
[INFO 2017-06-28 12:41:34,215 main.py:51] epoch 2160, training loss: 9484.34, average training loss: 10246.64, base loss: 14334.36
[INFO 2017-06-28 12:41:34,623 main.py:51] epoch 2161, training loss: 8993.98, average training loss: 10244.83, base loss: 14332.45
[INFO 2017-06-28 12:41:35,038 main.py:51] epoch 2162, training loss: 9937.83, average training loss: 10243.79, base loss: 14331.95
[INFO 2017-06-28 12:41:35,471 main.py:51] epoch 2163, training loss: 10885.41, average training loss: 10243.14, base loss: 14332.86
[INFO 2017-06-28 12:41:35,874 main.py:51] epoch 2164, training loss: 8751.75, average training loss: 10242.67, base loss: 14332.70
[INFO 2017-06-28 12:41:36,296 main.py:51] epoch 2165, training loss: 9478.16, average training loss: 10240.07, base loss: 14328.46
[INFO 2017-06-28 12:41:36,719 main.py:51] epoch 2166, training loss: 10701.60, average training loss: 10238.77, base loss: 14326.57
[INFO 2017-06-28 12:41:37,130 main.py:51] epoch 2167, training loss: 10096.08, average training loss: 10238.64, base loss: 14326.74
[INFO 2017-06-28 12:41:37,592 main.py:51] epoch 2168, training loss: 8725.50, average training loss: 10237.98, base loss: 14327.18
[INFO 2017-06-28 12:41:38,007 main.py:51] epoch 2169, training loss: 10576.99, average training loss: 10238.25, base loss: 14328.69
[INFO 2017-06-28 12:41:38,433 main.py:51] epoch 2170, training loss: 11572.30, average training loss: 10239.32, base loss: 14331.31
[INFO 2017-06-28 12:41:38,844 main.py:51] epoch 2171, training loss: 9295.46, average training loss: 10239.31, base loss: 14332.72
[INFO 2017-06-28 12:41:39,280 main.py:51] epoch 2172, training loss: 10534.50, average training loss: 10238.68, base loss: 14330.92
[INFO 2017-06-28 12:41:39,731 main.py:51] epoch 2173, training loss: 8533.29, average training loss: 10235.96, base loss: 14327.99
[INFO 2017-06-28 12:41:40,184 main.py:51] epoch 2174, training loss: 8941.18, average training loss: 10236.05, base loss: 14329.23
[INFO 2017-06-28 12:41:40,621 main.py:51] epoch 2175, training loss: 11585.40, average training loss: 10237.11, base loss: 14332.58
[INFO 2017-06-28 12:41:41,035 main.py:51] epoch 2176, training loss: 8907.09, average training loss: 10235.00, base loss: 14330.08
[INFO 2017-06-28 12:41:41,471 main.py:51] epoch 2177, training loss: 9458.73, average training loss: 10233.50, base loss: 14327.45
[INFO 2017-06-28 12:41:41,899 main.py:51] epoch 2178, training loss: 11096.09, average training loss: 10234.10, base loss: 14328.13
[INFO 2017-06-28 12:41:42,307 main.py:51] epoch 2179, training loss: 9174.06, average training loss: 10232.12, base loss: 14328.15
[INFO 2017-06-28 12:41:42,744 main.py:51] epoch 2180, training loss: 8606.88, average training loss: 10230.87, base loss: 14326.40
[INFO 2017-06-28 12:41:43,202 main.py:51] epoch 2181, training loss: 10528.84, average training loss: 10230.57, base loss: 14325.89
[INFO 2017-06-28 12:41:43,609 main.py:51] epoch 2182, training loss: 10061.36, average training loss: 10227.78, base loss: 14323.03
[INFO 2017-06-28 12:41:44,038 main.py:51] epoch 2183, training loss: 10424.09, average training loss: 10227.78, base loss: 14323.24
[INFO 2017-06-28 12:41:44,475 main.py:51] epoch 2184, training loss: 10191.83, average training loss: 10228.34, base loss: 14324.26
[INFO 2017-06-28 12:41:44,898 main.py:51] epoch 2185, training loss: 10168.40, average training loss: 10228.68, base loss: 14324.97
[INFO 2017-06-28 12:41:45,332 main.py:51] epoch 2186, training loss: 9277.47, average training loss: 10226.37, base loss: 14321.98
[INFO 2017-06-28 12:41:45,756 main.py:51] epoch 2187, training loss: 8702.58, average training loss: 10223.53, base loss: 14317.93
[INFO 2017-06-28 12:41:46,171 main.py:51] epoch 2188, training loss: 11420.47, average training loss: 10223.49, base loss: 14319.42
[INFO 2017-06-28 12:41:46,599 main.py:51] epoch 2189, training loss: 10485.18, average training loss: 10224.39, base loss: 14320.98
[INFO 2017-06-28 12:41:47,029 main.py:51] epoch 2190, training loss: 9643.42, average training loss: 10224.04, base loss: 14320.90
[INFO 2017-06-28 12:41:47,469 main.py:51] epoch 2191, training loss: 12056.34, average training loss: 10226.22, base loss: 14324.68
[INFO 2017-06-28 12:41:47,876 main.py:51] epoch 2192, training loss: 10040.63, average training loss: 10224.46, base loss: 14322.92
[INFO 2017-06-28 12:41:48,308 main.py:51] epoch 2193, training loss: 11232.62, average training loss: 10225.38, base loss: 14325.34
[INFO 2017-06-28 12:41:48,733 main.py:51] epoch 2194, training loss: 9806.49, average training loss: 10223.02, base loss: 14322.86
[INFO 2017-06-28 12:41:49,158 main.py:51] epoch 2195, training loss: 9508.86, average training loss: 10222.18, base loss: 14320.55
[INFO 2017-06-28 12:41:49,602 main.py:51] epoch 2196, training loss: 9387.81, average training loss: 10221.74, base loss: 14321.14
[INFO 2017-06-28 12:41:50,025 main.py:51] epoch 2197, training loss: 9707.45, average training loss: 10220.37, base loss: 14319.34
[INFO 2017-06-28 12:41:50,444 main.py:51] epoch 2198, training loss: 9624.57, average training loss: 10218.57, base loss: 14316.81
[INFO 2017-06-28 12:41:50,865 main.py:51] epoch 2199, training loss: 8810.04, average training loss: 10217.36, base loss: 14315.28
[INFO 2017-06-28 12:41:50,865 main.py:53] epoch 2199, testing
[INFO 2017-06-28 12:41:52,824 main.py:105] average testing loss: 11260.56, base loss: 15188.75
[INFO 2017-06-28 12:41:52,824 main.py:106] improve_loss: 3928.19, improve_percent: 0.26
[INFO 2017-06-28 12:41:52,825 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:41:53,264 main.py:51] epoch 2200, training loss: 10213.03, average training loss: 10217.54, base loss: 14315.56
[INFO 2017-06-28 12:41:53,675 main.py:51] epoch 2201, training loss: 10644.26, average training loss: 10217.66, base loss: 14316.14
[INFO 2017-06-28 12:41:54,123 main.py:51] epoch 2202, training loss: 10417.74, average training loss: 10217.51, base loss: 14316.78
[INFO 2017-06-28 12:41:54,544 main.py:51] epoch 2203, training loss: 9965.82, average training loss: 10218.09, base loss: 14317.84
[INFO 2017-06-28 12:41:54,967 main.py:51] epoch 2204, training loss: 9380.17, average training loss: 10217.29, base loss: 14315.93
[INFO 2017-06-28 12:41:55,387 main.py:51] epoch 2205, training loss: 9282.95, average training loss: 10215.18, base loss: 14312.40
[INFO 2017-06-28 12:41:55,823 main.py:51] epoch 2206, training loss: 11020.37, average training loss: 10215.73, base loss: 14310.81
[INFO 2017-06-28 12:41:56,252 main.py:51] epoch 2207, training loss: 10617.66, average training loss: 10214.61, base loss: 14309.31
[INFO 2017-06-28 12:41:56,673 main.py:51] epoch 2208, training loss: 10563.56, average training loss: 10213.85, base loss: 14307.11
[INFO 2017-06-28 12:41:57,122 main.py:51] epoch 2209, training loss: 10945.26, average training loss: 10214.84, base loss: 14308.77
[INFO 2017-06-28 12:41:57,542 main.py:51] epoch 2210, training loss: 11157.26, average training loss: 10216.29, base loss: 14311.73
[INFO 2017-06-28 12:41:57,953 main.py:51] epoch 2211, training loss: 10224.44, average training loss: 10216.85, base loss: 14313.96
[INFO 2017-06-28 12:41:58,371 main.py:51] epoch 2212, training loss: 10483.87, average training loss: 10217.81, base loss: 14315.37
[INFO 2017-06-28 12:41:58,781 main.py:51] epoch 2213, training loss: 10724.47, average training loss: 10217.57, base loss: 14315.75
[INFO 2017-06-28 12:41:59,218 main.py:51] epoch 2214, training loss: 9753.44, average training loss: 10217.04, base loss: 14314.70
[INFO 2017-06-28 12:41:59,674 main.py:51] epoch 2215, training loss: 9583.61, average training loss: 10215.63, base loss: 14313.30
[INFO 2017-06-28 12:42:00,103 main.py:51] epoch 2216, training loss: 9355.08, average training loss: 10214.47, base loss: 14312.85
[INFO 2017-06-28 12:42:00,556 main.py:51] epoch 2217, training loss: 9854.38, average training loss: 10214.26, base loss: 14312.55
[INFO 2017-06-28 12:42:00,980 main.py:51] epoch 2218, training loss: 12143.98, average training loss: 10216.03, base loss: 14313.77
[INFO 2017-06-28 12:42:01,406 main.py:51] epoch 2219, training loss: 10597.73, average training loss: 10215.66, base loss: 14312.01
[INFO 2017-06-28 12:42:01,839 main.py:51] epoch 2220, training loss: 11423.36, average training loss: 10217.21, base loss: 14313.33
[INFO 2017-06-28 12:42:02,277 main.py:51] epoch 2221, training loss: 11519.31, average training loss: 10216.68, base loss: 14313.10
[INFO 2017-06-28 12:42:02,718 main.py:51] epoch 2222, training loss: 9354.56, average training loss: 10215.16, base loss: 14312.66
[INFO 2017-06-28 12:42:03,135 main.py:51] epoch 2223, training loss: 10873.70, average training loss: 10217.14, base loss: 14316.02
[INFO 2017-06-28 12:42:03,569 main.py:51] epoch 2224, training loss: 10197.47, average training loss: 10217.05, base loss: 14315.13
[INFO 2017-06-28 12:42:04,020 main.py:51] epoch 2225, training loss: 9336.04, average training loss: 10214.59, base loss: 14312.62
[INFO 2017-06-28 12:42:04,447 main.py:51] epoch 2226, training loss: 11540.84, average training loss: 10216.41, base loss: 14316.00
[INFO 2017-06-28 12:42:04,850 main.py:51] epoch 2227, training loss: 10057.74, average training loss: 10216.48, base loss: 14316.25
[INFO 2017-06-28 12:42:05,257 main.py:51] epoch 2228, training loss: 9660.69, average training loss: 10215.42, base loss: 14315.40
[INFO 2017-06-28 12:42:05,671 main.py:51] epoch 2229, training loss: 10314.09, average training loss: 10215.38, base loss: 14314.69
[INFO 2017-06-28 12:42:06,092 main.py:51] epoch 2230, training loss: 9721.64, average training loss: 10216.37, base loss: 14316.00
[INFO 2017-06-28 12:42:06,519 main.py:51] epoch 2231, training loss: 10512.60, average training loss: 10216.98, base loss: 14317.14
[INFO 2017-06-28 12:42:06,926 main.py:51] epoch 2232, training loss: 9284.99, average training loss: 10216.59, base loss: 14318.11
[INFO 2017-06-28 12:42:07,368 main.py:51] epoch 2233, training loss: 9307.46, average training loss: 10215.76, base loss: 14317.63
[INFO 2017-06-28 12:42:07,775 main.py:51] epoch 2234, training loss: 9267.90, average training loss: 10214.24, base loss: 14316.10
[INFO 2017-06-28 12:42:08,210 main.py:51] epoch 2235, training loss: 10401.24, average training loss: 10215.19, base loss: 14317.09
[INFO 2017-06-28 12:42:08,641 main.py:51] epoch 2236, training loss: 9808.31, average training loss: 10214.70, base loss: 14316.71
[INFO 2017-06-28 12:42:09,086 main.py:51] epoch 2237, training loss: 8754.08, average training loss: 10214.19, base loss: 14315.71
[INFO 2017-06-28 12:42:09,543 main.py:51] epoch 2238, training loss: 10561.02, average training loss: 10213.37, base loss: 14315.86
[INFO 2017-06-28 12:42:09,988 main.py:51] epoch 2239, training loss: 9834.52, average training loss: 10213.85, base loss: 14316.98
[INFO 2017-06-28 12:42:10,436 main.py:51] epoch 2240, training loss: 11445.19, average training loss: 10215.57, base loss: 14318.22
[INFO 2017-06-28 12:42:10,866 main.py:51] epoch 2241, training loss: 9541.06, average training loss: 10215.14, base loss: 14320.03
[INFO 2017-06-28 12:42:11,301 main.py:51] epoch 2242, training loss: 10524.61, average training loss: 10214.70, base loss: 14320.02
[INFO 2017-06-28 12:42:11,741 main.py:51] epoch 2243, training loss: 8526.97, average training loss: 10213.16, base loss: 14316.35
[INFO 2017-06-28 12:42:12,166 main.py:51] epoch 2244, training loss: 9520.58, average training loss: 10212.89, base loss: 14315.87
[INFO 2017-06-28 12:42:12,591 main.py:51] epoch 2245, training loss: 10417.01, average training loss: 10213.84, base loss: 14317.49
[INFO 2017-06-28 12:42:13,020 main.py:51] epoch 2246, training loss: 14128.56, average training loss: 10218.19, base loss: 14325.00
[INFO 2017-06-28 12:42:13,446 main.py:51] epoch 2247, training loss: 10665.74, average training loss: 10218.30, base loss: 14325.48
[INFO 2017-06-28 12:42:13,907 main.py:51] epoch 2248, training loss: 9379.40, average training loss: 10217.70, base loss: 14325.25
[INFO 2017-06-28 12:42:14,333 main.py:51] epoch 2249, training loss: 9637.82, average training loss: 10216.52, base loss: 14322.39
[INFO 2017-06-28 12:42:14,777 main.py:51] epoch 2250, training loss: 10951.41, average training loss: 10216.75, base loss: 14324.40
[INFO 2017-06-28 12:42:15,193 main.py:51] epoch 2251, training loss: 9632.31, average training loss: 10215.27, base loss: 14322.98
[INFO 2017-06-28 12:42:15,619 main.py:51] epoch 2252, training loss: 9602.49, average training loss: 10215.71, base loss: 14323.34
[INFO 2017-06-28 12:42:16,038 main.py:51] epoch 2253, training loss: 10092.40, average training loss: 10213.89, base loss: 14321.20
[INFO 2017-06-28 12:42:16,467 main.py:51] epoch 2254, training loss: 10841.77, average training loss: 10214.38, base loss: 14321.84
[INFO 2017-06-28 12:42:16,904 main.py:51] epoch 2255, training loss: 12776.57, average training loss: 10217.38, base loss: 14326.78
[INFO 2017-06-28 12:42:17,351 main.py:51] epoch 2256, training loss: 8971.08, average training loss: 10214.62, base loss: 14322.87
[INFO 2017-06-28 12:42:17,810 main.py:51] epoch 2257, training loss: 10758.48, average training loss: 10214.56, base loss: 14323.14
[INFO 2017-06-28 12:42:18,234 main.py:51] epoch 2258, training loss: 7954.79, average training loss: 10213.33, base loss: 14322.50
[INFO 2017-06-28 12:42:18,667 main.py:51] epoch 2259, training loss: 9116.30, average training loss: 10211.96, base loss: 14320.41
[INFO 2017-06-28 12:42:19,079 main.py:51] epoch 2260, training loss: 9489.17, average training loss: 10212.25, base loss: 14321.68
[INFO 2017-06-28 12:42:19,535 main.py:51] epoch 2261, training loss: 9984.73, average training loss: 10212.78, base loss: 14323.17
[INFO 2017-06-28 12:42:19,965 main.py:51] epoch 2262, training loss: 9484.68, average training loss: 10212.10, base loss: 14323.96
[INFO 2017-06-28 12:42:20,428 main.py:51] epoch 2263, training loss: 10510.22, average training loss: 10213.35, base loss: 14326.50
[INFO 2017-06-28 12:42:20,856 main.py:51] epoch 2264, training loss: 10667.09, average training loss: 10213.26, base loss: 14327.36
[INFO 2017-06-28 12:42:21,263 main.py:51] epoch 2265, training loss: 8886.21, average training loss: 10213.15, base loss: 14326.80
[INFO 2017-06-28 12:42:21,675 main.py:51] epoch 2266, training loss: 10846.82, average training loss: 10213.89, base loss: 14326.20
[INFO 2017-06-28 12:42:22,101 main.py:51] epoch 2267, training loss: 11423.82, average training loss: 10213.83, base loss: 14325.74
[INFO 2017-06-28 12:42:22,548 main.py:51] epoch 2268, training loss: 9492.42, average training loss: 10213.48, base loss: 14325.30
[INFO 2017-06-28 12:42:23,005 main.py:51] epoch 2269, training loss: 11421.56, average training loss: 10214.82, base loss: 14325.38
[INFO 2017-06-28 12:42:23,419 main.py:51] epoch 2270, training loss: 9233.77, average training loss: 10212.28, base loss: 14323.28
[INFO 2017-06-28 12:42:23,822 main.py:51] epoch 2271, training loss: 11164.33, average training loss: 10213.35, base loss: 14324.71
[INFO 2017-06-28 12:42:24,277 main.py:51] epoch 2272, training loss: 9404.30, average training loss: 10213.66, base loss: 14324.03
[INFO 2017-06-28 12:42:24,678 main.py:51] epoch 2273, training loss: 10933.86, average training loss: 10211.96, base loss: 14320.80
[INFO 2017-06-28 12:42:25,138 main.py:51] epoch 2274, training loss: 12319.61, average training loss: 10214.65, base loss: 14326.58
[INFO 2017-06-28 12:42:25,565 main.py:51] epoch 2275, training loss: 8562.02, average training loss: 10212.10, base loss: 14323.78
[INFO 2017-06-28 12:42:26,002 main.py:51] epoch 2276, training loss: 10141.63, average training loss: 10210.37, base loss: 14320.90
[INFO 2017-06-28 12:42:26,403 main.py:51] epoch 2277, training loss: 10173.05, average training loss: 10209.90, base loss: 14319.35
[INFO 2017-06-28 12:42:26,836 main.py:51] epoch 2278, training loss: 8795.27, average training loss: 10207.65, base loss: 14316.15
[INFO 2017-06-28 12:42:27,261 main.py:51] epoch 2279, training loss: 11062.68, average training loss: 10209.74, base loss: 14319.90
[INFO 2017-06-28 12:42:27,674 main.py:51] epoch 2280, training loss: 9885.35, average training loss: 10210.40, base loss: 14321.76
[INFO 2017-06-28 12:42:28,103 main.py:51] epoch 2281, training loss: 9438.26, average training loss: 10209.69, base loss: 14321.18
[INFO 2017-06-28 12:42:28,548 main.py:51] epoch 2282, training loss: 9961.29, average training loss: 10210.50, base loss: 14322.87
[INFO 2017-06-28 12:42:28,967 main.py:51] epoch 2283, training loss: 10676.43, average training loss: 10211.34, base loss: 14323.85
[INFO 2017-06-28 12:42:29,393 main.py:51] epoch 2284, training loss: 9096.55, average training loss: 10208.67, base loss: 14321.12
[INFO 2017-06-28 12:42:29,832 main.py:51] epoch 2285, training loss: 10703.53, average training loss: 10208.41, base loss: 14321.48
[INFO 2017-06-28 12:42:30,265 main.py:51] epoch 2286, training loss: 9831.08, average training loss: 10207.54, base loss: 14320.96
[INFO 2017-06-28 12:42:30,704 main.py:51] epoch 2287, training loss: 10200.51, average training loss: 10207.00, base loss: 14321.10
[INFO 2017-06-28 12:42:31,128 main.py:51] epoch 2288, training loss: 9318.67, average training loss: 10205.86, base loss: 14320.70
[INFO 2017-06-28 12:42:31,549 main.py:51] epoch 2289, training loss: 11304.46, average training loss: 10207.59, base loss: 14324.46
[INFO 2017-06-28 12:42:31,977 main.py:51] epoch 2290, training loss: 7850.59, average training loss: 10204.31, base loss: 14320.60
[INFO 2017-06-28 12:42:32,436 main.py:51] epoch 2291, training loss: 11686.60, average training loss: 10205.74, base loss: 14323.62
[INFO 2017-06-28 12:42:32,865 main.py:51] epoch 2292, training loss: 11302.88, average training loss: 10207.44, base loss: 14326.62
[INFO 2017-06-28 12:42:33,308 main.py:51] epoch 2293, training loss: 10101.34, average training loss: 10207.21, base loss: 14327.35
[INFO 2017-06-28 12:42:33,714 main.py:51] epoch 2294, training loss: 9759.00, average training loss: 10206.91, base loss: 14327.37
[INFO 2017-06-28 12:42:34,134 main.py:51] epoch 2295, training loss: 9799.86, average training loss: 10207.55, base loss: 14328.69
[INFO 2017-06-28 12:42:34,574 main.py:51] epoch 2296, training loss: 10384.96, average training loss: 10209.08, base loss: 14329.41
[INFO 2017-06-28 12:42:35,015 main.py:51] epoch 2297, training loss: 9585.37, average training loss: 10207.32, base loss: 14327.39
[INFO 2017-06-28 12:42:35,452 main.py:51] epoch 2298, training loss: 10150.81, average training loss: 10206.26, base loss: 14326.22
[INFO 2017-06-28 12:42:35,899 main.py:51] epoch 2299, training loss: 10726.41, average training loss: 10207.47, base loss: 14327.89
[INFO 2017-06-28 12:42:35,899 main.py:53] epoch 2299, testing
[INFO 2017-06-28 12:42:37,883 main.py:105] average testing loss: 11053.09, base loss: 15120.10
[INFO 2017-06-28 12:42:37,883 main.py:106] improve_loss: 4067.01, improve_percent: 0.27
[INFO 2017-06-28 12:42:37,884 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:42:38,316 main.py:51] epoch 2300, training loss: 9494.87, average training loss: 10207.67, base loss: 14330.40
[INFO 2017-06-28 12:42:38,751 main.py:51] epoch 2301, training loss: 9952.42, average training loss: 10207.80, base loss: 14331.11
[INFO 2017-06-28 12:42:39,188 main.py:51] epoch 2302, training loss: 10556.11, average training loss: 10207.10, base loss: 14333.31
[INFO 2017-06-28 12:42:39,618 main.py:51] epoch 2303, training loss: 9914.22, average training loss: 10206.59, base loss: 14331.41
[INFO 2017-06-28 12:42:40,051 main.py:51] epoch 2304, training loss: 9414.21, average training loss: 10206.57, base loss: 14330.87
[INFO 2017-06-28 12:42:40,492 main.py:51] epoch 2305, training loss: 10990.87, average training loss: 10207.78, base loss: 14332.87
[INFO 2017-06-28 12:42:40,909 main.py:51] epoch 2306, training loss: 9726.61, average training loss: 10207.64, base loss: 14332.99
[INFO 2017-06-28 12:42:41,342 main.py:51] epoch 2307, training loss: 10104.89, average training loss: 10208.19, base loss: 14333.19
[INFO 2017-06-28 12:42:41,763 main.py:51] epoch 2308, training loss: 9638.14, average training loss: 10206.64, base loss: 14331.65
[INFO 2017-06-28 12:42:42,189 main.py:51] epoch 2309, training loss: 9528.16, average training loss: 10207.00, base loss: 14331.82
[INFO 2017-06-28 12:42:42,610 main.py:51] epoch 2310, training loss: 9703.42, average training loss: 10207.07, base loss: 14333.74
[INFO 2017-06-28 12:42:43,041 main.py:51] epoch 2311, training loss: 10163.46, average training loss: 10207.68, base loss: 14333.95
[INFO 2017-06-28 12:42:43,470 main.py:51] epoch 2312, training loss: 10463.24, average training loss: 10205.69, base loss: 14332.66
[INFO 2017-06-28 12:42:43,889 main.py:51] epoch 2313, training loss: 9845.45, average training loss: 10205.00, base loss: 14331.63
[INFO 2017-06-28 12:42:44,317 main.py:51] epoch 2314, training loss: 10370.29, average training loss: 10204.98, base loss: 14333.95
[INFO 2017-06-28 12:42:44,718 main.py:51] epoch 2315, training loss: 10378.79, average training loss: 10205.42, base loss: 14335.17
[INFO 2017-06-28 12:42:45,152 main.py:51] epoch 2316, training loss: 9805.48, average training loss: 10205.35, base loss: 14335.08
[INFO 2017-06-28 12:42:45,595 main.py:51] epoch 2317, training loss: 9798.52, average training loss: 10203.77, base loss: 14332.90
[INFO 2017-06-28 12:42:46,043 main.py:51] epoch 2318, training loss: 9211.62, average training loss: 10202.17, base loss: 14332.75
[INFO 2017-06-28 12:42:46,476 main.py:51] epoch 2319, training loss: 9801.92, average training loss: 10200.63, base loss: 14330.85
[INFO 2017-06-28 12:42:46,914 main.py:51] epoch 2320, training loss: 9193.61, average training loss: 10199.24, base loss: 14328.26
[INFO 2017-06-28 12:42:47,336 main.py:51] epoch 2321, training loss: 10138.21, average training loss: 10198.06, base loss: 14327.02
[INFO 2017-06-28 12:42:47,752 main.py:51] epoch 2322, training loss: 10785.56, average training loss: 10199.60, base loss: 14330.36
[INFO 2017-06-28 12:42:48,178 main.py:51] epoch 2323, training loss: 8986.03, average training loss: 10198.78, base loss: 14328.73
[INFO 2017-06-28 12:42:48,607 main.py:51] epoch 2324, training loss: 9777.34, average training loss: 10198.44, base loss: 14330.65
[INFO 2017-06-28 12:42:49,038 main.py:51] epoch 2325, training loss: 10159.99, average training loss: 10198.16, base loss: 14330.11
[INFO 2017-06-28 12:42:49,453 main.py:51] epoch 2326, training loss: 10945.03, average training loss: 10199.96, base loss: 14333.71
[INFO 2017-06-28 12:42:49,887 main.py:51] epoch 2327, training loss: 9969.10, average training loss: 10199.15, base loss: 14332.18
[INFO 2017-06-28 12:42:50,308 main.py:51] epoch 2328, training loss: 10531.14, average training loss: 10199.42, base loss: 14332.77
[INFO 2017-06-28 12:42:50,736 main.py:51] epoch 2329, training loss: 9884.22, average training loss: 10199.88, base loss: 14333.46
[INFO 2017-06-28 12:42:51,169 main.py:51] epoch 2330, training loss: 9904.25, average training loss: 10200.07, base loss: 14333.99
[INFO 2017-06-28 12:42:51,586 main.py:51] epoch 2331, training loss: 8721.58, average training loss: 10196.91, base loss: 14330.34
[INFO 2017-06-28 12:42:52,032 main.py:51] epoch 2332, training loss: 11268.19, average training loss: 10198.94, base loss: 14333.88
[INFO 2017-06-28 12:42:52,448 main.py:51] epoch 2333, training loss: 9724.85, average training loss: 10199.16, base loss: 14334.19
[INFO 2017-06-28 12:42:52,872 main.py:51] epoch 2334, training loss: 9084.56, average training loss: 10196.82, base loss: 14332.54
[INFO 2017-06-28 12:42:53,312 main.py:51] epoch 2335, training loss: 10275.88, average training loss: 10197.15, base loss: 14333.36
[INFO 2017-06-28 12:42:53,750 main.py:51] epoch 2336, training loss: 10920.69, average training loss: 10198.92, base loss: 14336.20
[INFO 2017-06-28 12:42:54,190 main.py:51] epoch 2337, training loss: 9133.35, average training loss: 10197.04, base loss: 14334.26
[INFO 2017-06-28 12:42:54,602 main.py:51] epoch 2338, training loss: 10055.19, average training loss: 10198.02, base loss: 14336.28
[INFO 2017-06-28 12:42:55,051 main.py:51] epoch 2339, training loss: 11412.06, average training loss: 10198.83, base loss: 14336.89
[INFO 2017-06-28 12:42:55,496 main.py:51] epoch 2340, training loss: 10715.54, average training loss: 10200.26, base loss: 14339.02
[INFO 2017-06-28 12:42:55,917 main.py:51] epoch 2341, training loss: 8978.97, average training loss: 10198.97, base loss: 14338.97
[INFO 2017-06-28 12:42:56,378 main.py:51] epoch 2342, training loss: 9557.97, average training loss: 10198.10, base loss: 14339.03
[INFO 2017-06-28 12:42:56,800 main.py:51] epoch 2343, training loss: 9718.46, average training loss: 10195.51, base loss: 14334.21
[INFO 2017-06-28 12:42:57,203 main.py:51] epoch 2344, training loss: 9486.03, average training loss: 10195.40, base loss: 14335.45
[INFO 2017-06-28 12:42:57,639 main.py:51] epoch 2345, training loss: 10160.27, average training loss: 10194.52, base loss: 14335.85
[INFO 2017-06-28 12:42:58,079 main.py:51] epoch 2346, training loss: 9733.09, average training loss: 10192.21, base loss: 14334.27
[INFO 2017-06-28 12:42:58,506 main.py:51] epoch 2347, training loss: 11715.62, average training loss: 10193.39, base loss: 14337.85
[INFO 2017-06-28 12:42:58,942 main.py:51] epoch 2348, training loss: 9295.80, average training loss: 10191.95, base loss: 14335.48
[INFO 2017-06-28 12:42:59,364 main.py:51] epoch 2349, training loss: 9438.51, average training loss: 10191.16, base loss: 14332.97
[INFO 2017-06-28 12:42:59,781 main.py:51] epoch 2350, training loss: 12580.50, average training loss: 10193.90, base loss: 14337.53
[INFO 2017-06-28 12:43:00,205 main.py:51] epoch 2351, training loss: 11494.24, average training loss: 10193.91, base loss: 14338.87
[INFO 2017-06-28 12:43:00,633 main.py:51] epoch 2352, training loss: 10578.89, average training loss: 10193.27, base loss: 14339.63
[INFO 2017-06-28 12:43:01,053 main.py:51] epoch 2353, training loss: 10662.16, average training loss: 10194.26, base loss: 14342.28
[INFO 2017-06-28 12:43:01,482 main.py:51] epoch 2354, training loss: 10734.46, average training loss: 10194.49, base loss: 14344.59
[INFO 2017-06-28 12:43:01,913 main.py:51] epoch 2355, training loss: 9589.84, average training loss: 10192.37, base loss: 14343.03
[INFO 2017-06-28 12:43:02,351 main.py:51] epoch 2356, training loss: 10576.01, average training loss: 10190.62, base loss: 14341.42
[INFO 2017-06-28 12:43:02,788 main.py:51] epoch 2357, training loss: 11029.84, average training loss: 10190.46, base loss: 14340.56
[INFO 2017-06-28 12:43:03,196 main.py:51] epoch 2358, training loss: 9823.04, average training loss: 10190.07, base loss: 14338.80
[INFO 2017-06-28 12:43:03,630 main.py:51] epoch 2359, training loss: 9936.77, average training loss: 10190.11, base loss: 14341.16
[INFO 2017-06-28 12:43:04,081 main.py:51] epoch 2360, training loss: 9648.49, average training loss: 10188.16, base loss: 14336.44
[INFO 2017-06-28 12:43:04,514 main.py:51] epoch 2361, training loss: 8464.80, average training loss: 10187.06, base loss: 14335.93
[INFO 2017-06-28 12:43:04,957 main.py:51] epoch 2362, training loss: 9200.52, average training loss: 10186.99, base loss: 14336.10
[INFO 2017-06-28 12:43:05,403 main.py:51] epoch 2363, training loss: 10961.84, average training loss: 10188.57, base loss: 14337.78
[INFO 2017-06-28 12:43:05,824 main.py:51] epoch 2364, training loss: 11582.06, average training loss: 10189.30, base loss: 14339.11
[INFO 2017-06-28 12:43:06,268 main.py:51] epoch 2365, training loss: 11253.55, average training loss: 10189.40, base loss: 14338.51
[INFO 2017-06-28 12:43:06,671 main.py:51] epoch 2366, training loss: 9812.55, average training loss: 10188.39, base loss: 14338.80
[INFO 2017-06-28 12:43:07,103 main.py:51] epoch 2367, training loss: 9681.35, average training loss: 10185.71, base loss: 14334.91
[INFO 2017-06-28 12:43:07,531 main.py:51] epoch 2368, training loss: 10759.91, average training loss: 10186.05, base loss: 14335.56
[INFO 2017-06-28 12:43:07,956 main.py:51] epoch 2369, training loss: 10078.41, average training loss: 10186.49, base loss: 14336.54
[INFO 2017-06-28 12:43:08,395 main.py:51] epoch 2370, training loss: 10325.26, average training loss: 10185.96, base loss: 14336.38
[INFO 2017-06-28 12:43:08,845 main.py:51] epoch 2371, training loss: 8888.60, average training loss: 10184.51, base loss: 14333.82
[INFO 2017-06-28 12:43:09,280 main.py:51] epoch 2372, training loss: 9398.67, average training loss: 10183.29, base loss: 14331.59
[INFO 2017-06-28 12:43:09,705 main.py:51] epoch 2373, training loss: 9220.10, average training loss: 10182.73, base loss: 14329.38
[INFO 2017-06-28 12:43:10,159 main.py:51] epoch 2374, training loss: 11869.05, average training loss: 10185.50, base loss: 14332.15
[INFO 2017-06-28 12:43:10,625 main.py:51] epoch 2375, training loss: 10592.30, average training loss: 10184.25, base loss: 14332.24
[INFO 2017-06-28 12:43:11,059 main.py:51] epoch 2376, training loss: 10682.58, average training loss: 10184.22, base loss: 14333.84
[INFO 2017-06-28 12:43:11,491 main.py:51] epoch 2377, training loss: 10874.22, average training loss: 10184.74, base loss: 14334.46
[INFO 2017-06-28 12:43:11,928 main.py:51] epoch 2378, training loss: 9618.60, average training loss: 10184.20, base loss: 14332.25
[INFO 2017-06-28 12:43:12,373 main.py:51] epoch 2379, training loss: 9395.17, average training loss: 10182.58, base loss: 14329.37
[INFO 2017-06-28 12:43:12,814 main.py:51] epoch 2380, training loss: 11117.93, average training loss: 10183.45, base loss: 14330.57
[INFO 2017-06-28 12:43:13,244 main.py:51] epoch 2381, training loss: 10230.30, average training loss: 10183.53, base loss: 14330.68
[INFO 2017-06-28 12:43:13,680 main.py:51] epoch 2382, training loss: 11039.53, average training loss: 10184.20, base loss: 14331.09
[INFO 2017-06-28 12:43:14,114 main.py:51] epoch 2383, training loss: 9935.85, average training loss: 10182.92, base loss: 14331.62
[INFO 2017-06-28 12:43:14,560 main.py:51] epoch 2384, training loss: 9394.81, average training loss: 10182.44, base loss: 14332.12
[INFO 2017-06-28 12:43:14,964 main.py:51] epoch 2385, training loss: 9578.72, average training loss: 10181.98, base loss: 14331.55
[INFO 2017-06-28 12:43:15,402 main.py:51] epoch 2386, training loss: 10197.89, average training loss: 10182.05, base loss: 14332.04
[INFO 2017-06-28 12:43:15,833 main.py:51] epoch 2387, training loss: 11270.22, average training loss: 10183.15, base loss: 14332.74
[INFO 2017-06-28 12:43:16,285 main.py:51] epoch 2388, training loss: 10052.20, average training loss: 10184.48, base loss: 14334.80
[INFO 2017-06-28 12:43:16,710 main.py:51] epoch 2389, training loss: 9294.15, average training loss: 10184.52, base loss: 14335.42
[INFO 2017-06-28 12:43:17,148 main.py:51] epoch 2390, training loss: 10886.60, average training loss: 10184.04, base loss: 14335.73
[INFO 2017-06-28 12:43:17,573 main.py:51] epoch 2391, training loss: 12386.44, average training loss: 10185.29, base loss: 14338.41
[INFO 2017-06-28 12:43:18,011 main.py:51] epoch 2392, training loss: 9695.11, average training loss: 10185.51, base loss: 14340.72
[INFO 2017-06-28 12:43:18,434 main.py:51] epoch 2393, training loss: 10208.05, average training loss: 10184.71, base loss: 14340.38
[INFO 2017-06-28 12:43:18,876 main.py:51] epoch 2394, training loss: 11209.77, average training loss: 10186.40, base loss: 14343.92
[INFO 2017-06-28 12:43:19,306 main.py:51] epoch 2395, training loss: 9634.91, average training loss: 10186.62, base loss: 14345.54
[INFO 2017-06-28 12:43:19,729 main.py:51] epoch 2396, training loss: 9060.76, average training loss: 10186.50, base loss: 14344.68
[INFO 2017-06-28 12:43:20,172 main.py:51] epoch 2397, training loss: 9787.80, average training loss: 10185.85, base loss: 14344.86
[INFO 2017-06-28 12:43:20,610 main.py:51] epoch 2398, training loss: 9965.85, average training loss: 10186.55, base loss: 14347.17
[INFO 2017-06-28 12:43:21,038 main.py:51] epoch 2399, training loss: 9784.52, average training loss: 10186.70, base loss: 14346.28
[INFO 2017-06-28 12:43:21,038 main.py:53] epoch 2399, testing
[INFO 2017-06-28 12:43:23,022 main.py:105] average testing loss: 11531.96, base loss: 15377.92
[INFO 2017-06-28 12:43:23,022 main.py:106] improve_loss: 3845.96, improve_percent: 0.25
[INFO 2017-06-28 12:43:23,023 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:43:23,448 main.py:51] epoch 2400, training loss: 10318.26, average training loss: 10188.16, base loss: 14348.03
[INFO 2017-06-28 12:43:23,892 main.py:51] epoch 2401, training loss: 8825.61, average training loss: 10186.94, base loss: 14347.23
[INFO 2017-06-28 12:43:24,326 main.py:51] epoch 2402, training loss: 9238.48, average training loss: 10188.13, base loss: 14348.73
[INFO 2017-06-28 12:43:24,752 main.py:51] epoch 2403, training loss: 8620.00, average training loss: 10186.27, base loss: 14347.15
[INFO 2017-06-28 12:43:25,206 main.py:51] epoch 2404, training loss: 9800.63, average training loss: 10186.56, base loss: 14347.35
[INFO 2017-06-28 12:43:25,619 main.py:51] epoch 2405, training loss: 10103.66, average training loss: 10186.37, base loss: 14347.66
[INFO 2017-06-28 12:43:26,056 main.py:51] epoch 2406, training loss: 8896.15, average training loss: 10185.32, base loss: 14345.26
[INFO 2017-06-28 12:43:26,488 main.py:51] epoch 2407, training loss: 10002.90, average training loss: 10184.31, base loss: 14344.92
[INFO 2017-06-28 12:43:26,930 main.py:51] epoch 2408, training loss: 9006.27, average training loss: 10183.19, base loss: 14342.64
[INFO 2017-06-28 12:43:27,346 main.py:51] epoch 2409, training loss: 8784.72, average training loss: 10182.46, base loss: 14341.39
[INFO 2017-06-28 12:43:27,786 main.py:51] epoch 2410, training loss: 9805.04, average training loss: 10182.41, base loss: 14342.12
[INFO 2017-06-28 12:43:28,201 main.py:51] epoch 2411, training loss: 10983.97, average training loss: 10181.76, base loss: 14343.12
[INFO 2017-06-28 12:43:28,629 main.py:51] epoch 2412, training loss: 9903.92, average training loss: 10182.02, base loss: 14345.10
[INFO 2017-06-28 12:43:29,074 main.py:51] epoch 2413, training loss: 9667.73, average training loss: 10181.30, base loss: 14343.93
[INFO 2017-06-28 12:43:29,491 main.py:51] epoch 2414, training loss: 9619.32, average training loss: 10180.42, base loss: 14344.36
[INFO 2017-06-28 12:43:29,912 main.py:51] epoch 2415, training loss: 8633.89, average training loss: 10178.14, base loss: 14340.99
[INFO 2017-06-28 12:43:30,344 main.py:51] epoch 2416, training loss: 9288.67, average training loss: 10178.20, base loss: 14342.24
[INFO 2017-06-28 12:43:30,773 main.py:51] epoch 2417, training loss: 10273.79, average training loss: 10179.02, base loss: 14342.57
[INFO 2017-06-28 12:43:31,195 main.py:51] epoch 2418, training loss: 10496.69, average training loss: 10179.70, base loss: 14343.62
[INFO 2017-06-28 12:43:31,643 main.py:51] epoch 2419, training loss: 8991.26, average training loss: 10179.43, base loss: 14342.77
[INFO 2017-06-28 12:43:32,061 main.py:51] epoch 2420, training loss: 9944.93, average training loss: 10179.36, base loss: 14344.20
[INFO 2017-06-28 12:43:32,501 main.py:51] epoch 2421, training loss: 10139.68, average training loss: 10178.55, base loss: 14344.61
[INFO 2017-06-28 12:43:32,945 main.py:51] epoch 2422, training loss: 11950.78, average training loss: 10180.10, base loss: 14347.22
[INFO 2017-06-28 12:43:33,381 main.py:51] epoch 2423, training loss: 9602.21, average training loss: 10179.44, base loss: 14345.94
[INFO 2017-06-28 12:43:33,833 main.py:51] epoch 2424, training loss: 10018.73, average training loss: 10179.04, base loss: 14345.70
[INFO 2017-06-28 12:43:34,257 main.py:51] epoch 2425, training loss: 10544.86, average training loss: 10179.84, base loss: 14347.53
[INFO 2017-06-28 12:43:34,689 main.py:51] epoch 2426, training loss: 10959.22, average training loss: 10180.58, base loss: 14348.95
[INFO 2017-06-28 12:43:35,098 main.py:51] epoch 2427, training loss: 11882.62, average training loss: 10182.43, base loss: 14352.95
[INFO 2017-06-28 12:43:35,540 main.py:51] epoch 2428, training loss: 9691.11, average training loss: 10182.68, base loss: 14352.06
[INFO 2017-06-28 12:43:35,963 main.py:51] epoch 2429, training loss: 9295.01, average training loss: 10182.43, base loss: 14354.10
[INFO 2017-06-28 12:43:36,375 main.py:51] epoch 2430, training loss: 11406.79, average training loss: 10183.25, base loss: 14357.06
[INFO 2017-06-28 12:43:36,830 main.py:51] epoch 2431, training loss: 9578.61, average training loss: 10182.67, base loss: 14357.01
[INFO 2017-06-28 12:43:37,270 main.py:51] epoch 2432, training loss: 11132.20, average training loss: 10182.99, base loss: 14358.90
[INFO 2017-06-28 12:43:37,696 main.py:51] epoch 2433, training loss: 9681.80, average training loss: 10182.16, base loss: 14357.29
[INFO 2017-06-28 12:43:38,140 main.py:51] epoch 2434, training loss: 8762.40, average training loss: 10178.43, base loss: 14352.29
[INFO 2017-06-28 12:43:38,576 main.py:51] epoch 2435, training loss: 10040.01, average training loss: 10178.40, base loss: 14353.85
[INFO 2017-06-28 12:43:39,005 main.py:51] epoch 2436, training loss: 9837.90, average training loss: 10178.55, base loss: 14355.62
[INFO 2017-06-28 12:43:39,428 main.py:51] epoch 2437, training loss: 8385.98, average training loss: 10177.43, base loss: 14355.09
[INFO 2017-06-28 12:43:39,839 main.py:51] epoch 2438, training loss: 10404.62, average training loss: 10176.55, base loss: 14354.32
[INFO 2017-06-28 12:43:40,264 main.py:51] epoch 2439, training loss: 9051.39, average training loss: 10175.08, base loss: 14352.47
[INFO 2017-06-28 12:43:40,717 main.py:51] epoch 2440, training loss: 12416.28, average training loss: 10178.20, base loss: 14357.04
[INFO 2017-06-28 12:43:41,147 main.py:51] epoch 2441, training loss: 10161.22, average training loss: 10176.74, base loss: 14354.10
[INFO 2017-06-28 12:43:41,569 main.py:51] epoch 2442, training loss: 11694.61, average training loss: 10176.90, base loss: 14355.44
[INFO 2017-06-28 12:43:41,988 main.py:51] epoch 2443, training loss: 9394.26, average training loss: 10174.65, base loss: 14351.95
[INFO 2017-06-28 12:43:42,425 main.py:51] epoch 2444, training loss: 10430.53, average training loss: 10173.75, base loss: 14350.92
[INFO 2017-06-28 12:43:42,846 main.py:51] epoch 2445, training loss: 9330.26, average training loss: 10173.37, base loss: 14350.69
[INFO 2017-06-28 12:43:43,277 main.py:51] epoch 2446, training loss: 9766.29, average training loss: 10173.46, base loss: 14350.53
[INFO 2017-06-28 12:43:43,700 main.py:51] epoch 2447, training loss: 11449.44, average training loss: 10173.40, base loss: 14351.20
[INFO 2017-06-28 12:43:44,144 main.py:51] epoch 2448, training loss: 10562.15, average training loss: 10174.42, base loss: 14353.13
[INFO 2017-06-28 12:43:44,613 main.py:51] epoch 2449, training loss: 10664.93, average training loss: 10175.95, base loss: 14354.57
[INFO 2017-06-28 12:43:45,049 main.py:51] epoch 2450, training loss: 9299.59, average training loss: 10174.38, base loss: 14352.82
[INFO 2017-06-28 12:43:45,491 main.py:51] epoch 2451, training loss: 10888.67, average training loss: 10175.43, base loss: 14354.11
[INFO 2017-06-28 12:43:45,904 main.py:51] epoch 2452, training loss: 11458.78, average training loss: 10177.01, base loss: 14355.66
[INFO 2017-06-28 12:43:46,329 main.py:51] epoch 2453, training loss: 10174.46, average training loss: 10177.36, base loss: 14356.61
[INFO 2017-06-28 12:43:46,755 main.py:51] epoch 2454, training loss: 11765.32, average training loss: 10180.35, base loss: 14362.78
[INFO 2017-06-28 12:43:47,175 main.py:51] epoch 2455, training loss: 10024.81, average training loss: 10181.12, base loss: 14364.76
[INFO 2017-06-28 12:43:47,602 main.py:51] epoch 2456, training loss: 10671.46, average training loss: 10181.48, base loss: 14365.50
[INFO 2017-06-28 12:43:48,053 main.py:51] epoch 2457, training loss: 10448.77, average training loss: 10180.73, base loss: 14365.65
[INFO 2017-06-28 12:43:48,486 main.py:51] epoch 2458, training loss: 8338.02, average training loss: 10179.38, base loss: 14364.31
[INFO 2017-06-28 12:43:48,909 main.py:51] epoch 2459, training loss: 10063.84, average training loss: 10179.08, base loss: 14364.01
[INFO 2017-06-28 12:43:49,339 main.py:51] epoch 2460, training loss: 10900.83, average training loss: 10179.86, base loss: 14365.97
[INFO 2017-06-28 12:43:49,794 main.py:51] epoch 2461, training loss: 9641.21, average training loss: 10180.32, base loss: 14366.90
[INFO 2017-06-28 12:43:50,224 main.py:51] epoch 2462, training loss: 9722.57, average training loss: 10178.43, base loss: 14365.24
[INFO 2017-06-28 12:43:50,641 main.py:51] epoch 2463, training loss: 9650.11, average training loss: 10176.93, base loss: 14362.51
[INFO 2017-06-28 12:43:51,074 main.py:51] epoch 2464, training loss: 9354.51, average training loss: 10175.79, base loss: 14360.78
[INFO 2017-06-28 12:43:51,525 main.py:51] epoch 2465, training loss: 9159.39, average training loss: 10174.60, base loss: 14359.59
[INFO 2017-06-28 12:43:51,951 main.py:51] epoch 2466, training loss: 10301.36, average training loss: 10174.75, base loss: 14359.56
[INFO 2017-06-28 12:43:52,384 main.py:51] epoch 2467, training loss: 11003.12, average training loss: 10175.97, base loss: 14361.76
[INFO 2017-06-28 12:43:52,826 main.py:51] epoch 2468, training loss: 8587.55, average training loss: 10174.10, base loss: 14361.05
[INFO 2017-06-28 12:43:53,285 main.py:51] epoch 2469, training loss: 9673.56, average training loss: 10172.89, base loss: 14359.28
[INFO 2017-06-28 12:43:53,733 main.py:51] epoch 2470, training loss: 11056.08, average training loss: 10175.05, base loss: 14362.30
[INFO 2017-06-28 12:43:54,153 main.py:51] epoch 2471, training loss: 10600.17, average training loss: 10175.74, base loss: 14363.58
[INFO 2017-06-28 12:43:54,564 main.py:51] epoch 2472, training loss: 10792.87, average training loss: 10176.81, base loss: 14363.67
[INFO 2017-06-28 12:43:54,990 main.py:51] epoch 2473, training loss: 10825.92, average training loss: 10177.33, base loss: 14364.35
[INFO 2017-06-28 12:43:55,427 main.py:51] epoch 2474, training loss: 10646.42, average training loss: 10175.51, base loss: 14363.04
[INFO 2017-06-28 12:43:55,859 main.py:51] epoch 2475, training loss: 10048.31, average training loss: 10176.18, base loss: 14364.11
[INFO 2017-06-28 12:43:56,311 main.py:51] epoch 2476, training loss: 11355.68, average training loss: 10176.89, base loss: 14365.45
[INFO 2017-06-28 12:43:56,782 main.py:51] epoch 2477, training loss: 10118.64, average training loss: 10176.77, base loss: 14365.24
[INFO 2017-06-28 12:43:57,255 main.py:51] epoch 2478, training loss: 10650.68, average training loss: 10178.69, base loss: 14368.85
[INFO 2017-06-28 12:43:57,696 main.py:51] epoch 2479, training loss: 10253.35, average training loss: 10178.98, base loss: 14368.98
[INFO 2017-06-28 12:43:58,119 main.py:51] epoch 2480, training loss: 10093.54, average training loss: 10178.26, base loss: 14369.16
[INFO 2017-06-28 12:43:58,553 main.py:51] epoch 2481, training loss: 7732.54, average training loss: 10176.83, base loss: 14366.06
[INFO 2017-06-28 12:43:58,995 main.py:51] epoch 2482, training loss: 10298.99, average training loss: 10176.91, base loss: 14367.41
[INFO 2017-06-28 12:43:59,427 main.py:51] epoch 2483, training loss: 10633.58, average training loss: 10174.61, base loss: 14366.80
[INFO 2017-06-28 12:43:59,854 main.py:51] epoch 2484, training loss: 12045.48, average training loss: 10177.04, base loss: 14371.35
[INFO 2017-06-28 12:44:00,273 main.py:51] epoch 2485, training loss: 10219.53, average training loss: 10177.70, base loss: 14372.71
[INFO 2017-06-28 12:44:00,707 main.py:51] epoch 2486, training loss: 11080.62, average training loss: 10176.37, base loss: 14369.70
[INFO 2017-06-28 12:44:01,144 main.py:51] epoch 2487, training loss: 9674.71, average training loss: 10174.89, base loss: 14367.42
[INFO 2017-06-28 12:44:01,598 main.py:51] epoch 2488, training loss: 9603.62, average training loss: 10174.15, base loss: 14366.56
[INFO 2017-06-28 12:44:02,084 main.py:51] epoch 2489, training loss: 11557.03, average training loss: 10175.21, base loss: 14368.92
[INFO 2017-06-28 12:44:02,518 main.py:51] epoch 2490, training loss: 9819.74, average training loss: 10175.97, base loss: 14369.28
[INFO 2017-06-28 12:44:02,986 main.py:51] epoch 2491, training loss: 11354.14, average training loss: 10177.17, base loss: 14370.54
[INFO 2017-06-28 12:44:03,434 main.py:51] epoch 2492, training loss: 9521.73, average training loss: 10176.89, base loss: 14371.11
[INFO 2017-06-28 12:44:03,868 main.py:51] epoch 2493, training loss: 9623.89, average training loss: 10177.14, base loss: 14372.01
[INFO 2017-06-28 12:44:04,295 main.py:51] epoch 2494, training loss: 10210.21, average training loss: 10178.15, base loss: 14373.65
[INFO 2017-06-28 12:44:04,725 main.py:51] epoch 2495, training loss: 11405.32, average training loss: 10178.89, base loss: 14374.54
[INFO 2017-06-28 12:44:05,150 main.py:51] epoch 2496, training loss: 11258.69, average training loss: 10180.56, base loss: 14375.94
[INFO 2017-06-28 12:44:05,565 main.py:51] epoch 2497, training loss: 9182.37, average training loss: 10179.12, base loss: 14373.93
[INFO 2017-06-28 12:44:06,002 main.py:51] epoch 2498, training loss: 9528.99, average training loss: 10179.71, base loss: 14377.60
[INFO 2017-06-28 12:44:06,460 main.py:51] epoch 2499, training loss: 9640.63, average training loss: 10178.12, base loss: 14375.66
[INFO 2017-06-28 12:44:06,460 main.py:53] epoch 2499, testing
[INFO 2017-06-28 12:44:08,534 main.py:105] average testing loss: 10980.92, base loss: 14924.66
[INFO 2017-06-28 12:44:08,534 main.py:106] improve_loss: 3943.74, improve_percent: 0.26
[INFO 2017-06-28 12:44:08,535 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:44:08,967 main.py:51] epoch 2500, training loss: 9048.77, average training loss: 10175.79, base loss: 14373.85
[INFO 2017-06-28 12:44:09,402 main.py:51] epoch 2501, training loss: 11296.59, average training loss: 10175.25, base loss: 14372.68
[INFO 2017-06-28 12:44:09,857 main.py:51] epoch 2502, training loss: 11340.94, average training loss: 10176.40, base loss: 14375.46
[INFO 2017-06-28 12:44:10,318 main.py:51] epoch 2503, training loss: 9850.95, average training loss: 10176.73, base loss: 14376.70
[INFO 2017-06-28 12:44:10,787 main.py:51] epoch 2504, training loss: 10742.98, average training loss: 10176.92, base loss: 14376.90
[INFO 2017-06-28 12:44:11,225 main.py:51] epoch 2505, training loss: 10999.91, average training loss: 10177.62, base loss: 14377.74
[INFO 2017-06-28 12:44:11,637 main.py:51] epoch 2506, training loss: 11213.68, average training loss: 10178.92, base loss: 14378.35
[INFO 2017-06-28 12:44:12,072 main.py:51] epoch 2507, training loss: 9470.90, average training loss: 10178.61, base loss: 14378.07
[INFO 2017-06-28 12:44:12,537 main.py:51] epoch 2508, training loss: 9622.48, average training loss: 10176.44, base loss: 14375.22
[INFO 2017-06-28 12:44:12,950 main.py:51] epoch 2509, training loss: 9125.69, average training loss: 10176.23, base loss: 14375.75
[INFO 2017-06-28 12:44:13,395 main.py:51] epoch 2510, training loss: 9750.50, average training loss: 10175.65, base loss: 14374.15
[INFO 2017-06-28 12:44:13,830 main.py:51] epoch 2511, training loss: 10355.66, average training loss: 10174.93, base loss: 14373.77
[INFO 2017-06-28 12:44:14,241 main.py:51] epoch 2512, training loss: 11101.62, average training loss: 10175.89, base loss: 14376.31
[INFO 2017-06-28 12:44:14,662 main.py:51] epoch 2513, training loss: 10416.97, average training loss: 10174.67, base loss: 14374.64
[INFO 2017-06-28 12:44:15,170 main.py:51] epoch 2514, training loss: 11040.86, average training loss: 10174.98, base loss: 14375.35
[INFO 2017-06-28 12:44:15,622 main.py:51] epoch 2515, training loss: 11294.13, average training loss: 10175.23, base loss: 14375.97
[INFO 2017-06-28 12:44:16,060 main.py:51] epoch 2516, training loss: 10205.33, average training loss: 10173.28, base loss: 14375.60
[INFO 2017-06-28 12:44:16,494 main.py:51] epoch 2517, training loss: 10396.54, average training loss: 10172.93, base loss: 14376.09
[INFO 2017-06-28 12:44:16,952 main.py:51] epoch 2518, training loss: 10754.08, average training loss: 10172.10, base loss: 14376.87
[INFO 2017-06-28 12:44:17,452 main.py:51] epoch 2519, training loss: 9200.79, average training loss: 10170.47, base loss: 14374.79
[INFO 2017-06-28 12:44:17,880 main.py:51] epoch 2520, training loss: 10595.09, average training loss: 10171.67, base loss: 14375.88
[INFO 2017-06-28 12:44:18,311 main.py:51] epoch 2521, training loss: 10908.56, average training loss: 10173.83, base loss: 14379.22
[INFO 2017-06-28 12:44:18,786 main.py:51] epoch 2522, training loss: 11891.99, average training loss: 10176.69, base loss: 14383.09
[INFO 2017-06-28 12:44:19,232 main.py:51] epoch 2523, training loss: 10454.39, average training loss: 10175.75, base loss: 14382.06
[INFO 2017-06-28 12:44:19,712 main.py:51] epoch 2524, training loss: 10137.34, average training loss: 10175.73, base loss: 14383.00
[INFO 2017-06-28 12:44:20,220 main.py:51] epoch 2525, training loss: 11153.01, average training loss: 10175.16, base loss: 14382.86
[INFO 2017-06-28 12:44:20,685 main.py:51] epoch 2526, training loss: 10522.24, average training loss: 10176.99, base loss: 14385.09
[INFO 2017-06-28 12:44:21,124 main.py:51] epoch 2527, training loss: 12153.68, average training loss: 10178.93, base loss: 14388.93
[INFO 2017-06-28 12:44:21,595 main.py:51] epoch 2528, training loss: 10263.62, average training loss: 10178.41, base loss: 14388.00
[INFO 2017-06-28 12:44:22,104 main.py:51] epoch 2529, training loss: 9682.43, average training loss: 10176.71, base loss: 14385.76
[INFO 2017-06-28 12:44:22,565 main.py:51] epoch 2530, training loss: 11286.94, average training loss: 10177.84, base loss: 14388.91
[INFO 2017-06-28 12:44:23,106 main.py:51] epoch 2531, training loss: 10024.69, average training loss: 10176.93, base loss: 14386.69
[INFO 2017-06-28 12:44:23,538 main.py:51] epoch 2532, training loss: 9200.22, average training loss: 10177.35, base loss: 14386.90
[INFO 2017-06-28 12:44:23,982 main.py:51] epoch 2533, training loss: 11298.41, average training loss: 10177.07, base loss: 14388.29
[INFO 2017-06-28 12:44:24,465 main.py:51] epoch 2534, training loss: 9710.14, average training loss: 10176.80, base loss: 14387.09
[INFO 2017-06-28 12:44:24,882 main.py:51] epoch 2535, training loss: 9419.23, average training loss: 10177.06, base loss: 14389.42
[INFO 2017-06-28 12:44:25,385 main.py:51] epoch 2536, training loss: 8999.03, average training loss: 10177.14, base loss: 14389.58
[INFO 2017-06-28 12:44:25,831 main.py:51] epoch 2537, training loss: 9962.68, average training loss: 10176.42, base loss: 14387.38
[INFO 2017-06-28 12:44:26,329 main.py:51] epoch 2538, training loss: 10716.71, average training loss: 10177.00, base loss: 14387.47
[INFO 2017-06-28 12:44:26,789 main.py:51] epoch 2539, training loss: 10826.29, average training loss: 10178.08, base loss: 14389.06
[INFO 2017-06-28 12:44:27,231 main.py:51] epoch 2540, training loss: 10783.01, average training loss: 10177.85, base loss: 14387.69
[INFO 2017-06-28 12:44:27,648 main.py:51] epoch 2541, training loss: 8731.87, average training loss: 10176.24, base loss: 14385.85
[INFO 2017-06-28 12:44:28,130 main.py:51] epoch 2542, training loss: 8865.57, average training loss: 10174.76, base loss: 14385.39
[INFO 2017-06-28 12:44:28,595 main.py:51] epoch 2543, training loss: 10314.28, average training loss: 10175.26, base loss: 14387.14
[INFO 2017-06-28 12:44:29,022 main.py:51] epoch 2544, training loss: 10754.63, average training loss: 10174.41, base loss: 14384.74
[INFO 2017-06-28 12:44:29,449 main.py:51] epoch 2545, training loss: 9833.23, average training loss: 10174.92, base loss: 14386.32
[INFO 2017-06-28 12:44:29,869 main.py:51] epoch 2546, training loss: 9191.57, average training loss: 10174.10, base loss: 14385.45
[INFO 2017-06-28 12:44:30,322 main.py:51] epoch 2547, training loss: 10089.35, average training loss: 10174.08, base loss: 14387.01
[INFO 2017-06-28 12:44:30,733 main.py:51] epoch 2548, training loss: 9518.34, average training loss: 10173.96, base loss: 14387.23
[INFO 2017-06-28 12:44:31,153 main.py:51] epoch 2549, training loss: 10444.67, average training loss: 10174.71, base loss: 14388.54
[INFO 2017-06-28 12:44:31,617 main.py:51] epoch 2550, training loss: 10166.00, average training loss: 10175.66, base loss: 14391.14
[INFO 2017-06-28 12:44:32,042 main.py:51] epoch 2551, training loss: 9880.13, average training loss: 10174.99, base loss: 14391.67
[INFO 2017-06-28 12:44:32,476 main.py:51] epoch 2552, training loss: 9218.58, average training loss: 10173.64, base loss: 14389.61
[INFO 2017-06-28 12:44:32,892 main.py:51] epoch 2553, training loss: 9702.91, average training loss: 10172.73, base loss: 14389.22
[INFO 2017-06-28 12:44:33,327 main.py:51] epoch 2554, training loss: 11215.91, average training loss: 10172.43, base loss: 14388.11
[INFO 2017-06-28 12:44:33,829 main.py:51] epoch 2555, training loss: 9552.97, average training loss: 10172.05, base loss: 14388.67
[INFO 2017-06-28 12:44:34,286 main.py:51] epoch 2556, training loss: 9294.17, average training loss: 10169.71, base loss: 14383.72
[INFO 2017-06-28 12:44:34,747 main.py:51] epoch 2557, training loss: 10622.69, average training loss: 10167.92, base loss: 14381.29
[INFO 2017-06-28 12:44:35,200 main.py:51] epoch 2558, training loss: 9447.57, average training loss: 10168.04, base loss: 14383.97
[INFO 2017-06-28 12:44:35,633 main.py:51] epoch 2559, training loss: 9467.50, average training loss: 10167.90, base loss: 14383.02
[INFO 2017-06-28 12:44:36,084 main.py:51] epoch 2560, training loss: 10524.79, average training loss: 10168.27, base loss: 14384.74
[INFO 2017-06-28 12:44:36,560 main.py:51] epoch 2561, training loss: 9603.41, average training loss: 10167.53, base loss: 14383.60
[INFO 2017-06-28 12:44:37,007 main.py:51] epoch 2562, training loss: 9416.13, average training loss: 10167.35, base loss: 14383.21
[INFO 2017-06-28 12:44:37,472 main.py:51] epoch 2563, training loss: 8694.09, average training loss: 10165.73, base loss: 14381.98
[INFO 2017-06-28 12:44:37,904 main.py:51] epoch 2564, training loss: 8523.22, average training loss: 10163.30, base loss: 14378.30
[INFO 2017-06-28 12:44:38,344 main.py:51] epoch 2565, training loss: 10186.93, average training loss: 10164.65, base loss: 14380.04
[INFO 2017-06-28 12:44:38,872 main.py:51] epoch 2566, training loss: 9306.02, average training loss: 10163.72, base loss: 14378.47
[INFO 2017-06-28 12:44:39,316 main.py:51] epoch 2567, training loss: 9870.48, average training loss: 10162.72, base loss: 14377.25
[INFO 2017-06-28 12:44:39,729 main.py:51] epoch 2568, training loss: 11087.83, average training loss: 10162.82, base loss: 14378.07
[INFO 2017-06-28 12:44:40,175 main.py:51] epoch 2569, training loss: 9726.46, average training loss: 10162.31, base loss: 14378.53
[INFO 2017-06-28 12:44:40,629 main.py:51] epoch 2570, training loss: 8801.43, average training loss: 10161.81, base loss: 14378.08
[INFO 2017-06-28 12:44:41,118 main.py:51] epoch 2571, training loss: 9221.34, average training loss: 10160.14, base loss: 14376.17
[INFO 2017-06-28 12:44:41,557 main.py:51] epoch 2572, training loss: 9909.13, average training loss: 10159.56, base loss: 14373.58
[INFO 2017-06-28 12:44:42,043 main.py:51] epoch 2573, training loss: 10291.17, average training loss: 10159.98, base loss: 14375.01
[INFO 2017-06-28 12:44:42,500 main.py:51] epoch 2574, training loss: 8734.40, average training loss: 10158.55, base loss: 14373.27
[INFO 2017-06-28 12:44:42,943 main.py:51] epoch 2575, training loss: 11297.27, average training loss: 10160.17, base loss: 14377.08
[INFO 2017-06-28 12:44:43,398 main.py:51] epoch 2576, training loss: 10463.04, average training loss: 10161.51, base loss: 14380.76
[INFO 2017-06-28 12:44:43,831 main.py:51] epoch 2577, training loss: 9848.97, average training loss: 10162.67, base loss: 14382.88
[INFO 2017-06-28 12:44:44,286 main.py:51] epoch 2578, training loss: 10672.61, average training loss: 10164.84, base loss: 14386.44
[INFO 2017-06-28 12:44:44,744 main.py:51] epoch 2579, training loss: 8471.16, average training loss: 10161.61, base loss: 14380.53
[INFO 2017-06-28 12:44:45,241 main.py:51] epoch 2580, training loss: 11104.07, average training loss: 10161.32, base loss: 14381.28
[INFO 2017-06-28 12:44:45,686 main.py:51] epoch 2581, training loss: 9750.02, average training loss: 10159.31, base loss: 14380.12
[INFO 2017-06-28 12:44:46,120 main.py:51] epoch 2582, training loss: 9274.17, average training loss: 10158.97, base loss: 14380.55
[INFO 2017-06-28 12:44:46,553 main.py:51] epoch 2583, training loss: 9552.20, average training loss: 10157.27, base loss: 14379.55
[INFO 2017-06-28 12:44:46,979 main.py:51] epoch 2584, training loss: 9236.33, average training loss: 10156.02, base loss: 14378.70
[INFO 2017-06-28 12:44:47,438 main.py:51] epoch 2585, training loss: 9827.67, average training loss: 10157.08, base loss: 14380.65
[INFO 2017-06-28 12:44:47,926 main.py:51] epoch 2586, training loss: 9488.18, average training loss: 10157.29, base loss: 14382.40
[INFO 2017-06-28 12:44:48,395 main.py:51] epoch 2587, training loss: 11535.77, average training loss: 10158.17, base loss: 14383.42
[INFO 2017-06-28 12:44:48,863 main.py:51] epoch 2588, training loss: 11213.85, average training loss: 10160.88, base loss: 14386.67
[INFO 2017-06-28 12:44:49,301 main.py:51] epoch 2589, training loss: 10358.52, average training loss: 10161.43, base loss: 14387.18
[INFO 2017-06-28 12:44:49,914 main.py:51] epoch 2590, training loss: 9063.92, average training loss: 10159.36, base loss: 14384.49
[INFO 2017-06-28 12:44:50,378 main.py:51] epoch 2591, training loss: 9718.31, average training loss: 10159.02, base loss: 14384.01
[INFO 2017-06-28 12:44:50,815 main.py:51] epoch 2592, training loss: 10636.84, average training loss: 10159.99, base loss: 14386.57
[INFO 2017-06-28 12:44:51,244 main.py:51] epoch 2593, training loss: 9363.68, average training loss: 10158.74, base loss: 14387.15
[INFO 2017-06-28 12:44:51,670 main.py:51] epoch 2594, training loss: 9026.44, average training loss: 10157.50, base loss: 14385.23
[INFO 2017-06-28 12:44:52,100 main.py:51] epoch 2595, training loss: 10514.24, average training loss: 10157.02, base loss: 14385.68
[INFO 2017-06-28 12:44:52,526 main.py:51] epoch 2596, training loss: 10777.46, average training loss: 10157.24, base loss: 14388.00
[INFO 2017-06-28 12:44:52,949 main.py:51] epoch 2597, training loss: 11280.94, average training loss: 10157.26, base loss: 14385.22
[INFO 2017-06-28 12:44:53,389 main.py:51] epoch 2598, training loss: 12310.33, average training loss: 10159.29, base loss: 14390.21
[INFO 2017-06-28 12:44:53,854 main.py:51] epoch 2599, training loss: 9000.44, average training loss: 10156.92, base loss: 14385.74
[INFO 2017-06-28 12:44:53,854 main.py:53] epoch 2599, testing
[INFO 2017-06-28 12:44:56,013 main.py:105] average testing loss: 11094.50, base loss: 14963.91
[INFO 2017-06-28 12:44:56,013 main.py:106] improve_loss: 3869.42, improve_percent: 0.26
[INFO 2017-06-28 12:44:56,014 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:44:56,469 main.py:51] epoch 2600, training loss: 11559.10, average training loss: 10157.75, base loss: 14386.52
[INFO 2017-06-28 12:44:56,978 main.py:51] epoch 2601, training loss: 9548.64, average training loss: 10157.31, base loss: 14387.56
[INFO 2017-06-28 12:44:57,427 main.py:51] epoch 2602, training loss: 8706.86, average training loss: 10154.75, base loss: 14384.57
[INFO 2017-06-28 12:44:57,901 main.py:51] epoch 2603, training loss: 9708.74, average training loss: 10155.01, base loss: 14385.32
[INFO 2017-06-28 12:44:58,406 main.py:51] epoch 2604, training loss: 8549.30, average training loss: 10153.89, base loss: 14384.55
[INFO 2017-06-28 12:44:58,831 main.py:51] epoch 2605, training loss: 9709.96, average training loss: 10153.69, base loss: 14384.23
[INFO 2017-06-28 12:44:59,296 main.py:51] epoch 2606, training loss: 9596.36, average training loss: 10152.06, base loss: 14381.56
[INFO 2017-06-28 12:44:59,763 main.py:51] epoch 2607, training loss: 10389.35, average training loss: 10152.21, base loss: 14381.71
[INFO 2017-06-28 12:45:00,222 main.py:51] epoch 2608, training loss: 10483.45, average training loss: 10152.42, base loss: 14380.91
[INFO 2017-06-28 12:45:00,698 main.py:51] epoch 2609, training loss: 11170.35, average training loss: 10152.04, base loss: 14380.59
[INFO 2017-06-28 12:45:01,145 main.py:51] epoch 2610, training loss: 9847.11, average training loss: 10151.55, base loss: 14380.19
[INFO 2017-06-28 12:45:01,639 main.py:51] epoch 2611, training loss: 11200.41, average training loss: 10152.58, base loss: 14380.50
[INFO 2017-06-28 12:45:02,086 main.py:51] epoch 2612, training loss: 10441.24, average training loss: 10152.76, base loss: 14381.80
[INFO 2017-06-28 12:45:02,547 main.py:51] epoch 2613, training loss: 9046.57, average training loss: 10151.29, base loss: 14380.46
[INFO 2017-06-28 12:45:03,064 main.py:51] epoch 2614, training loss: 10206.82, average training loss: 10151.02, base loss: 14380.57
[INFO 2017-06-28 12:45:03,559 main.py:51] epoch 2615, training loss: 9831.24, average training loss: 10150.63, base loss: 14380.10
[INFO 2017-06-28 12:45:04,106 main.py:51] epoch 2616, training loss: 11312.99, average training loss: 10153.29, base loss: 14382.67
[INFO 2017-06-28 12:45:04,600 main.py:51] epoch 2617, training loss: 8965.02, average training loss: 10152.40, base loss: 14383.17
[INFO 2017-06-28 12:45:05,084 main.py:51] epoch 2618, training loss: 9941.56, average training loss: 10152.73, base loss: 14382.85
[INFO 2017-06-28 12:45:05,599 main.py:51] epoch 2619, training loss: 11893.45, average training loss: 10154.77, base loss: 14386.05
[INFO 2017-06-28 12:45:06,176 main.py:51] epoch 2620, training loss: 9546.32, average training loss: 10155.23, base loss: 14386.44
[INFO 2017-06-28 12:45:06,619 main.py:51] epoch 2621, training loss: 9735.65, average training loss: 10155.89, base loss: 14387.78
[INFO 2017-06-28 12:45:07,126 main.py:51] epoch 2622, training loss: 11466.15, average training loss: 10155.82, base loss: 14389.16
[INFO 2017-06-28 12:45:07,617 main.py:51] epoch 2623, training loss: 9528.13, average training loss: 10154.09, base loss: 14388.13
[INFO 2017-06-28 12:45:08,108 main.py:51] epoch 2624, training loss: 9876.96, average training loss: 10154.94, base loss: 14392.44
[INFO 2017-06-28 12:45:08,618 main.py:51] epoch 2625, training loss: 9291.88, average training loss: 10154.76, base loss: 14392.82
[INFO 2017-06-28 12:45:09,064 main.py:51] epoch 2626, training loss: 9697.47, average training loss: 10154.19, base loss: 14392.83
[INFO 2017-06-28 12:45:09,544 main.py:51] epoch 2627, training loss: 8558.69, average training loss: 10151.72, base loss: 14389.68
[INFO 2017-06-28 12:45:09,974 main.py:51] epoch 2628, training loss: 10891.28, average training loss: 10152.28, base loss: 14389.86
[INFO 2017-06-28 12:45:10,461 main.py:51] epoch 2629, training loss: 8978.11, average training loss: 10150.97, base loss: 14387.81
[INFO 2017-06-28 12:45:10,878 main.py:51] epoch 2630, training loss: 9987.50, average training loss: 10150.91, base loss: 14387.41
[INFO 2017-06-28 12:45:11,334 main.py:51] epoch 2631, training loss: 9614.59, average training loss: 10151.48, base loss: 14388.80
[INFO 2017-06-28 12:45:11,817 main.py:51] epoch 2632, training loss: 10145.53, average training loss: 10150.24, base loss: 14387.54
[INFO 2017-06-28 12:45:12,233 main.py:51] epoch 2633, training loss: 9503.19, average training loss: 10149.20, base loss: 14386.76
[INFO 2017-06-28 12:45:12,757 main.py:51] epoch 2634, training loss: 9722.25, average training loss: 10149.32, base loss: 14388.24
[INFO 2017-06-28 12:45:13,319 main.py:51] epoch 2635, training loss: 10742.80, average training loss: 10150.27, base loss: 14388.32
[INFO 2017-06-28 12:45:13,814 main.py:51] epoch 2636, training loss: 10958.38, average training loss: 10151.20, base loss: 14388.57
[INFO 2017-06-28 12:45:14,332 main.py:51] epoch 2637, training loss: 11430.98, average training loss: 10152.78, base loss: 14391.91
[INFO 2017-06-28 12:45:14,836 main.py:51] epoch 2638, training loss: 10440.57, average training loss: 10153.50, base loss: 14392.50
[INFO 2017-06-28 12:45:15,333 main.py:51] epoch 2639, training loss: 10345.25, average training loss: 10152.44, base loss: 14392.14
[INFO 2017-06-28 12:45:15,813 main.py:51] epoch 2640, training loss: 9372.24, average training loss: 10151.51, base loss: 14390.89
[INFO 2017-06-28 12:45:16,274 main.py:51] epoch 2641, training loss: 8861.79, average training loss: 10150.04, base loss: 14389.76
[INFO 2017-06-28 12:45:16,721 main.py:51] epoch 2642, training loss: 9648.49, average training loss: 10148.94, base loss: 14389.28
[INFO 2017-06-28 12:45:17,135 main.py:51] epoch 2643, training loss: 8620.89, average training loss: 10148.05, base loss: 14388.71
[INFO 2017-06-28 12:45:17,554 main.py:51] epoch 2644, training loss: 11387.18, average training loss: 10147.87, base loss: 14387.77
[INFO 2017-06-28 12:45:18,000 main.py:51] epoch 2645, training loss: 10604.81, average training loss: 10148.62, base loss: 14389.42
[INFO 2017-06-28 12:45:18,446 main.py:51] epoch 2646, training loss: 10392.81, average training loss: 10148.83, base loss: 14389.48
[INFO 2017-06-28 12:45:18,893 main.py:51] epoch 2647, training loss: 9318.67, average training loss: 10148.77, base loss: 14388.24
[INFO 2017-06-28 12:45:19,344 main.py:51] epoch 2648, training loss: 9855.63, average training loss: 10148.64, base loss: 14389.30
[INFO 2017-06-28 12:45:19,797 main.py:51] epoch 2649, training loss: 9898.89, average training loss: 10148.89, base loss: 14389.31
[INFO 2017-06-28 12:45:20,225 main.py:51] epoch 2650, training loss: 8893.70, average training loss: 10147.85, base loss: 14389.18
[INFO 2017-06-28 12:45:20,665 main.py:51] epoch 2651, training loss: 11056.84, average training loss: 10149.30, base loss: 14390.75
[INFO 2017-06-28 12:45:21,133 main.py:51] epoch 2652, training loss: 11859.02, average training loss: 10150.41, base loss: 14393.24
[INFO 2017-06-28 12:45:21,563 main.py:51] epoch 2653, training loss: 11331.34, average training loss: 10151.07, base loss: 14395.37
[INFO 2017-06-28 12:45:21,980 main.py:51] epoch 2654, training loss: 9663.92, average training loss: 10151.60, base loss: 14395.44
[INFO 2017-06-28 12:45:22,419 main.py:51] epoch 2655, training loss: 10656.32, average training loss: 10152.89, base loss: 14396.92
[INFO 2017-06-28 12:45:22,850 main.py:51] epoch 2656, training loss: 8988.78, average training loss: 10152.65, base loss: 14398.00
[INFO 2017-06-28 12:45:23,295 main.py:51] epoch 2657, training loss: 9509.67, average training loss: 10151.36, base loss: 14397.07
[INFO 2017-06-28 12:45:23,725 main.py:51] epoch 2658, training loss: 9711.65, average training loss: 10149.56, base loss: 14394.41
[INFO 2017-06-28 12:45:24,161 main.py:51] epoch 2659, training loss: 11335.90, average training loss: 10152.16, base loss: 14398.03
[INFO 2017-06-28 12:45:24,586 main.py:51] epoch 2660, training loss: 9587.97, average training loss: 10152.47, base loss: 14399.75
[INFO 2017-06-28 12:45:25,003 main.py:51] epoch 2661, training loss: 9174.58, average training loss: 10151.23, base loss: 14399.41
[INFO 2017-06-28 12:45:25,454 main.py:51] epoch 2662, training loss: 9165.90, average training loss: 10150.86, base loss: 14399.83
[INFO 2017-06-28 12:45:25,891 main.py:51] epoch 2663, training loss: 10450.86, average training loss: 10150.42, base loss: 14401.26
[INFO 2017-06-28 12:45:26,319 main.py:51] epoch 2664, training loss: 9821.22, average training loss: 10151.54, base loss: 14402.54
[INFO 2017-06-28 12:45:26,747 main.py:51] epoch 2665, training loss: 9657.85, average training loss: 10151.36, base loss: 14400.62
[INFO 2017-06-28 12:45:27,175 main.py:51] epoch 2666, training loss: 10957.77, average training loss: 10152.82, base loss: 14403.12
[INFO 2017-06-28 12:45:27,579 main.py:51] epoch 2667, training loss: 9193.41, average training loss: 10152.98, base loss: 14402.66
[INFO 2017-06-28 12:45:28,008 main.py:51] epoch 2668, training loss: 9895.43, average training loss: 10153.56, base loss: 14404.48
[INFO 2017-06-28 12:45:28,441 main.py:51] epoch 2669, training loss: 9058.67, average training loss: 10151.33, base loss: 14401.21
[INFO 2017-06-28 12:45:28,873 main.py:51] epoch 2670, training loss: 10146.88, average training loss: 10151.98, base loss: 14401.96
[INFO 2017-06-28 12:45:29,287 main.py:51] epoch 2671, training loss: 9869.46, average training loss: 10151.78, base loss: 14399.49
[INFO 2017-06-28 12:45:29,716 main.py:51] epoch 2672, training loss: 10248.82, average training loss: 10152.18, base loss: 14399.37
[INFO 2017-06-28 12:45:30,137 main.py:51] epoch 2673, training loss: 9181.00, average training loss: 10151.23, base loss: 14399.30
[INFO 2017-06-28 12:45:30,556 main.py:51] epoch 2674, training loss: 9125.90, average training loss: 10150.40, base loss: 14399.16
[INFO 2017-06-28 12:45:30,972 main.py:51] epoch 2675, training loss: 11662.39, average training loss: 10152.18, base loss: 14400.71
[INFO 2017-06-28 12:45:31,427 main.py:51] epoch 2676, training loss: 8968.35, average training loss: 10150.76, base loss: 14398.63
[INFO 2017-06-28 12:45:31,845 main.py:51] epoch 2677, training loss: 9279.50, average training loss: 10149.87, base loss: 14396.58
[INFO 2017-06-28 12:45:32,286 main.py:51] epoch 2678, training loss: 9143.89, average training loss: 10148.75, base loss: 14394.50
[INFO 2017-06-28 12:45:32,713 main.py:51] epoch 2679, training loss: 10338.92, average training loss: 10147.53, base loss: 14391.83
[INFO 2017-06-28 12:45:33,138 main.py:51] epoch 2680, training loss: 10203.16, average training loss: 10147.12, base loss: 14391.29
[INFO 2017-06-28 12:45:33,583 main.py:51] epoch 2681, training loss: 10773.74, average training loss: 10147.32, base loss: 14390.58
[INFO 2017-06-28 12:45:34,004 main.py:51] epoch 2682, training loss: 10100.89, average training loss: 10145.28, base loss: 14387.78
[INFO 2017-06-28 12:45:34,433 main.py:51] epoch 2683, training loss: 10943.49, average training loss: 10146.16, base loss: 14390.06
[INFO 2017-06-28 12:45:34,856 main.py:51] epoch 2684, training loss: 8699.32, average training loss: 10144.55, base loss: 14388.42
[INFO 2017-06-28 12:45:35,285 main.py:51] epoch 2685, training loss: 8933.20, average training loss: 10142.71, base loss: 14386.23
[INFO 2017-06-28 12:45:35,712 main.py:51] epoch 2686, training loss: 8724.35, average training loss: 10141.36, base loss: 14385.02
[INFO 2017-06-28 12:45:36,134 main.py:51] epoch 2687, training loss: 9249.67, average training loss: 10140.55, base loss: 14384.52
[INFO 2017-06-28 12:45:36,566 main.py:51] epoch 2688, training loss: 11835.71, average training loss: 10142.64, base loss: 14387.63
[INFO 2017-06-28 12:45:37,018 main.py:51] epoch 2689, training loss: 9148.05, average training loss: 10142.18, base loss: 14387.16
[INFO 2017-06-28 12:45:37,440 main.py:51] epoch 2690, training loss: 9956.73, average training loss: 10140.31, base loss: 14383.79
[INFO 2017-06-28 12:45:37,863 main.py:51] epoch 2691, training loss: 12349.62, average training loss: 10141.71, base loss: 14386.27
[INFO 2017-06-28 12:45:38,290 main.py:51] epoch 2692, training loss: 11510.14, average training loss: 10143.21, base loss: 14389.06
[INFO 2017-06-28 12:45:38,705 main.py:51] epoch 2693, training loss: 9682.31, average training loss: 10143.07, base loss: 14390.99
[INFO 2017-06-28 12:45:39,143 main.py:51] epoch 2694, training loss: 10128.90, average training loss: 10143.77, base loss: 14392.55
[INFO 2017-06-28 12:45:39,609 main.py:51] epoch 2695, training loss: 9323.57, average training loss: 10141.98, base loss: 14391.55
[INFO 2017-06-28 12:45:40,052 main.py:51] epoch 2696, training loss: 9772.87, average training loss: 10141.56, base loss: 14389.31
[INFO 2017-06-28 12:45:40,484 main.py:51] epoch 2697, training loss: 10076.72, average training loss: 10139.38, base loss: 14387.44
[INFO 2017-06-28 12:45:40,905 main.py:51] epoch 2698, training loss: 9793.09, average training loss: 10138.96, base loss: 14386.94
[INFO 2017-06-28 12:45:41,335 main.py:51] epoch 2699, training loss: 9892.51, average training loss: 10138.69, base loss: 14386.14
[INFO 2017-06-28 12:45:41,335 main.py:53] epoch 2699, testing
[INFO 2017-06-28 12:45:43,318 main.py:105] average testing loss: 11266.83, base loss: 14963.45
[INFO 2017-06-28 12:45:43,318 main.py:106] improve_loss: 3696.62, improve_percent: 0.25
[INFO 2017-06-28 12:45:43,319 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 12:45:43,754 main.py:51] epoch 2700, training loss: 10470.67, average training loss: 10137.61, base loss: 14384.89
[INFO 2017-06-28 12:45:44,191 main.py:51] epoch 2701, training loss: 10229.50, average training loss: 10137.20, base loss: 14383.52
[INFO 2017-06-28 12:45:44,631 main.py:51] epoch 2702, training loss: 9882.92, average training loss: 10135.54, base loss: 14380.36
[INFO 2017-06-28 12:45:45,064 main.py:51] epoch 2703, training loss: 8770.77, average training loss: 10135.42, base loss: 14382.25
[INFO 2017-06-28 12:45:45,487 main.py:51] epoch 2704, training loss: 10293.07, average training loss: 10133.42, base loss: 14380.06
[INFO 2017-06-28 12:45:45,925 main.py:51] epoch 2705, training loss: 9559.33, average training loss: 10132.68, base loss: 14380.67
[INFO 2017-06-28 12:45:46,354 main.py:51] epoch 2706, training loss: 7787.52, average training loss: 10130.26, base loss: 14377.56
[INFO 2017-06-28 12:45:46,794 main.py:51] epoch 2707, training loss: 9764.54, average training loss: 10129.35, base loss: 14378.07
[INFO 2017-06-28 12:45:47,248 main.py:51] epoch 2708, training loss: 9712.49, average training loss: 10127.65, base loss: 14375.87
[INFO 2017-06-28 12:45:47,707 main.py:51] epoch 2709, training loss: 9292.58, average training loss: 10126.44, base loss: 14374.55
[INFO 2017-06-28 12:45:48,119 main.py:51] epoch 2710, training loss: 9145.42, average training loss: 10126.09, base loss: 14373.41
[INFO 2017-06-28 12:45:48,547 main.py:51] epoch 2711, training loss: 10805.91, average training loss: 10127.07, base loss: 14375.11
[INFO 2017-06-28 12:45:48,990 main.py:51] epoch 2712, training loss: 8731.26, average training loss: 10124.54, base loss: 14373.34
[INFO 2017-06-28 12:45:49,429 main.py:51] epoch 2713, training loss: 9935.72, average training loss: 10124.56, base loss: 14373.62
[INFO 2017-06-28 12:45:49,884 main.py:51] epoch 2714, training loss: 9598.19, average training loss: 10125.04, base loss: 14374.80
[INFO 2017-06-28 12:45:50,305 main.py:51] epoch 2715, training loss: 10085.00, average training loss: 10126.04, base loss: 14376.53
[INFO 2017-06-28 12:45:50,743 main.py:51] epoch 2716, training loss: 9789.38, average training loss: 10125.46, base loss: 14376.59
[INFO 2017-06-28 12:45:51,166 main.py:51] epoch 2717, training loss: 12418.61, average training loss: 10127.56, base loss: 14379.91
[INFO 2017-06-28 12:45:51,588 main.py:51] epoch 2718, training loss: 10552.65, average training loss: 10128.95, base loss: 14381.66
[INFO 2017-06-28 12:45:52,024 main.py:51] epoch 2719, training loss: 9925.02, average training loss: 10128.66, base loss: 14382.67
[INFO 2017-06-28 12:45:52,454 main.py:51] epoch 2720, training loss: 11095.21, average training loss: 10131.05, base loss: 14386.52
[INFO 2017-06-28 12:45:52,893 main.py:51] epoch 2721, training loss: 9569.13, average training loss: 10130.36, base loss: 14385.63
[INFO 2017-06-28 12:45:53,326 main.py:51] epoch 2722, training loss: 8739.23, average training loss: 10129.18, base loss: 14383.52
[INFO 2017-06-28 12:45:53,755 main.py:51] epoch 2723, training loss: 9732.88, average training loss: 10129.79, base loss: 14384.67
[INFO 2017-06-28 12:45:54,189 main.py:51] epoch 2724, training loss: 9940.85, average training loss: 10129.50, base loss: 14384.48
[INFO 2017-06-28 12:45:54,618 main.py:51] epoch 2725, training loss: 10194.21, average training loss: 10129.50, base loss: 14383.94
[INFO 2017-06-28 12:45:55,039 main.py:51] epoch 2726, training loss: 11662.28, average training loss: 10131.95, base loss: 14388.19
[INFO 2017-06-28 12:45:55,466 main.py:51] epoch 2727, training loss: 9003.60, average training loss: 10130.29, base loss: 14386.86
[INFO 2017-06-28 12:45:55,871 main.py:51] epoch 2728, training loss: 8128.11, average training loss: 10128.56, base loss: 14384.85
[INFO 2017-06-28 12:45:56,301 main.py:51] epoch 2729, training loss: 10631.69, average training loss: 10128.81, base loss: 14385.48
[INFO 2017-06-28 12:45:56,735 main.py:51] epoch 2730, training loss: 9417.05, average training loss: 10128.09, base loss: 14383.99
