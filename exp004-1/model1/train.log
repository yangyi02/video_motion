[INFO 2017-06-28 11:59:02,700 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=25, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-28 11:59:05,249 main.py:51] epoch 0, training loss: 28983.27, average training loss: 28983.27, base loss: 13381.08
[INFO 2017-06-28 11:59:05,565 main.py:51] epoch 1, training loss: 26327.70, average training loss: 27655.48, base loss: 13952.92
[INFO 2017-06-28 11:59:05,875 main.py:51] epoch 2, training loss: 25575.07, average training loss: 26962.01, base loss: 14066.81
[INFO 2017-06-28 11:59:06,161 main.py:51] epoch 3, training loss: 24435.67, average training loss: 26330.43, base loss: 14537.23
[INFO 2017-06-28 11:59:06,452 main.py:51] epoch 4, training loss: 22182.46, average training loss: 25500.83, base loss: 14217.44
[INFO 2017-06-28 11:59:06,754 main.py:51] epoch 5, training loss: 21098.49, average training loss: 24767.11, base loss: 14090.37
[INFO 2017-06-28 11:59:07,044 main.py:51] epoch 6, training loss: 19422.27, average training loss: 24003.56, base loss: 14047.25
[INFO 2017-06-28 11:59:07,311 main.py:51] epoch 7, training loss: 19803.12, average training loss: 23478.51, base loss: 14110.08
[INFO 2017-06-28 11:59:07,615 main.py:51] epoch 8, training loss: 19108.77, average training loss: 22992.98, base loss: 14163.44
[INFO 2017-06-28 11:59:07,908 main.py:51] epoch 9, training loss: 18002.15, average training loss: 22493.90, base loss: 14054.92
[INFO 2017-06-28 11:59:08,220 main.py:51] epoch 10, training loss: 16935.41, average training loss: 21988.58, base loss: 13974.15
[INFO 2017-06-28 11:59:08,508 main.py:51] epoch 11, training loss: 16656.28, average training loss: 21544.22, base loss: 13978.29
[INFO 2017-06-28 11:59:08,779 main.py:51] epoch 12, training loss: 19708.62, average training loss: 21403.02, base loss: 14230.54
[INFO 2017-06-28 11:59:09,071 main.py:51] epoch 13, training loss: 17296.43, average training loss: 21109.69, base loss: 14310.04
[INFO 2017-06-28 11:59:09,403 main.py:51] epoch 14, training loss: 16312.60, average training loss: 20789.89, base loss: 14302.67
[INFO 2017-06-28 11:59:09,720 main.py:51] epoch 15, training loss: 16067.56, average training loss: 20494.74, base loss: 14251.45
[INFO 2017-06-28 11:59:10,004 main.py:51] epoch 16, training loss: 16023.30, average training loss: 20231.72, base loss: 14285.11
[INFO 2017-06-28 11:59:10,338 main.py:51] epoch 17, training loss: 15507.89, average training loss: 19969.28, base loss: 14294.46
[INFO 2017-06-28 11:59:10,638 main.py:51] epoch 18, training loss: 14813.80, average training loss: 19697.94, base loss: 14268.18
[INFO 2017-06-28 11:59:10,945 main.py:51] epoch 19, training loss: 16389.53, average training loss: 19532.52, base loss: 14321.06
[INFO 2017-06-28 11:59:11,247 main.py:51] epoch 20, training loss: 14855.62, average training loss: 19309.81, base loss: 14299.18
[INFO 2017-06-28 11:59:11,549 main.py:51] epoch 21, training loss: 13632.13, average training loss: 19051.73, base loss: 14226.95
[INFO 2017-06-28 11:59:11,847 main.py:51] epoch 22, training loss: 15561.69, average training loss: 18899.99, base loss: 14246.41
[INFO 2017-06-28 11:59:12,137 main.py:51] epoch 23, training loss: 13692.92, average training loss: 18683.03, base loss: 14180.24
[INFO 2017-06-28 11:59:12,418 main.py:51] epoch 24, training loss: 13458.46, average training loss: 18474.05, base loss: 14135.05
[INFO 2017-06-28 11:59:12,704 main.py:51] epoch 25, training loss: 14458.13, average training loss: 18319.59, base loss: 14135.71
[INFO 2017-06-28 11:59:13,010 main.py:51] epoch 26, training loss: 16480.28, average training loss: 18251.47, base loss: 14201.60
[INFO 2017-06-28 11:59:13,300 main.py:51] epoch 27, training loss: 13840.27, average training loss: 18093.92, base loss: 14171.67
[INFO 2017-06-28 11:59:13,593 main.py:51] epoch 28, training loss: 16686.25, average training loss: 18045.38, base loss: 14257.58
[INFO 2017-06-28 11:59:13,875 main.py:51] epoch 29, training loss: 14758.52, average training loss: 17935.82, base loss: 14270.87
[INFO 2017-06-28 11:59:14,185 main.py:51] epoch 30, training loss: 16204.90, average training loss: 17879.99, base loss: 14337.94
[INFO 2017-06-28 11:59:14,477 main.py:51] epoch 31, training loss: 14413.18, average training loss: 17771.65, base loss: 14341.54
[INFO 2017-06-28 11:59:14,754 main.py:51] epoch 32, training loss: 15221.52, average training loss: 17694.37, base loss: 14360.38
[INFO 2017-06-28 11:59:15,017 main.py:51] epoch 33, training loss: 16395.13, average training loss: 17656.16, base loss: 14438.17
[INFO 2017-06-28 11:59:15,304 main.py:51] epoch 34, training loss: 13076.92, average training loss: 17525.32, base loss: 14402.25
[INFO 2017-06-28 11:59:15,595 main.py:51] epoch 35, training loss: 13171.36, average training loss: 17404.38, base loss: 14372.36
[INFO 2017-06-28 11:59:15,894 main.py:51] epoch 36, training loss: 11975.73, average training loss: 17257.66, base loss: 14312.67
[INFO 2017-06-28 11:59:16,192 main.py:51] epoch 37, training loss: 12844.11, average training loss: 17141.51, base loss: 14278.01
[INFO 2017-06-28 11:59:16,496 main.py:51] epoch 38, training loss: 13132.42, average training loss: 17038.72, base loss: 14251.10
[INFO 2017-06-28 11:59:16,805 main.py:51] epoch 39, training loss: 14226.16, average training loss: 16968.40, base loss: 14266.60
[INFO 2017-06-28 11:59:17,099 main.py:51] epoch 40, training loss: 14568.60, average training loss: 16909.87, base loss: 14281.02
[INFO 2017-06-28 11:59:17,380 main.py:51] epoch 41, training loss: 12607.37, average training loss: 16807.43, base loss: 14247.49
[INFO 2017-06-28 11:59:17,674 main.py:51] epoch 42, training loss: 13154.49, average training loss: 16722.48, base loss: 14229.58
[INFO 2017-06-28 11:59:17,978 main.py:51] epoch 43, training loss: 17254.46, average training loss: 16734.57, base loss: 14310.55
[INFO 2017-06-28 11:59:18,255 main.py:51] epoch 44, training loss: 14077.18, average training loss: 16675.51, base loss: 14315.18
[INFO 2017-06-28 11:59:18,534 main.py:51] epoch 45, training loss: 12658.30, average training loss: 16588.18, base loss: 14293.01
[INFO 2017-06-28 11:59:18,856 main.py:51] epoch 46, training loss: 14345.39, average training loss: 16540.47, base loss: 14304.59
[INFO 2017-06-28 11:59:19,132 main.py:51] epoch 47, training loss: 15440.99, average training loss: 16517.56, base loss: 14342.53
[INFO 2017-06-28 11:59:19,408 main.py:51] epoch 48, training loss: 13002.93, average training loss: 16445.83, base loss: 14323.90
[INFO 2017-06-28 11:59:19,697 main.py:51] epoch 49, training loss: 14461.10, average training loss: 16406.14, base loss: 14344.96
[INFO 2017-06-28 11:59:20,010 main.py:51] epoch 50, training loss: 12696.68, average training loss: 16333.40, base loss: 14324.44
[INFO 2017-06-28 11:59:20,316 main.py:51] epoch 51, training loss: 14794.67, average training loss: 16303.81, base loss: 14351.19
[INFO 2017-06-28 11:59:20,629 main.py:51] epoch 52, training loss: 13369.71, average training loss: 16248.45, base loss: 14344.15
[INFO 2017-06-28 11:59:20,908 main.py:51] epoch 53, training loss: 12514.46, average training loss: 16179.30, base loss: 14322.55
[INFO 2017-06-28 11:59:21,198 main.py:51] epoch 54, training loss: 13716.40, average training loss: 16134.52, base loss: 14325.78
[INFO 2017-06-28 11:59:21,499 main.py:51] epoch 55, training loss: 13985.16, average training loss: 16096.14, base loss: 14332.96
[INFO 2017-06-28 11:59:21,802 main.py:51] epoch 56, training loss: 13988.60, average training loss: 16059.17, base loss: 14336.74
[INFO 2017-06-28 11:59:22,098 main.py:51] epoch 57, training loss: 14754.11, average training loss: 16036.67, base loss: 14358.45
[INFO 2017-06-28 11:59:22,400 main.py:51] epoch 58, training loss: 14277.39, average training loss: 16006.85, base loss: 14376.43
[INFO 2017-06-28 11:59:22,678 main.py:51] epoch 59, training loss: 12423.66, average training loss: 15947.13, base loss: 14356.24
[INFO 2017-06-28 11:59:22,969 main.py:51] epoch 60, training loss: 14274.13, average training loss: 15919.70, base loss: 14370.58
[INFO 2017-06-28 11:59:23,256 main.py:51] epoch 61, training loss: 15680.96, average training loss: 15915.85, base loss: 14402.86
[INFO 2017-06-28 11:59:23,561 main.py:51] epoch 62, training loss: 13840.13, average training loss: 15882.90, base loss: 14408.38
[INFO 2017-06-28 11:59:23,853 main.py:51] epoch 63, training loss: 14117.16, average training loss: 15855.31, base loss: 14419.15
[INFO 2017-06-28 11:59:24,127 main.py:51] epoch 64, training loss: 13654.84, average training loss: 15821.46, base loss: 14417.51
[INFO 2017-06-28 11:59:24,424 main.py:51] epoch 65, training loss: 11948.65, average training loss: 15762.78, base loss: 14389.39
[INFO 2017-06-28 11:59:24,705 main.py:51] epoch 66, training loss: 13071.49, average training loss: 15722.61, base loss: 14384.28
[INFO 2017-06-28 11:59:24,992 main.py:51] epoch 67, training loss: 15956.05, average training loss: 15726.05, base loss: 14427.10
[INFO 2017-06-28 11:59:25,288 main.py:51] epoch 68, training loss: 12515.32, average training loss: 15679.51, base loss: 14411.02
[INFO 2017-06-28 11:59:25,593 main.py:51] epoch 69, training loss: 12640.49, average training loss: 15636.10, base loss: 14397.06
[INFO 2017-06-28 11:59:25,895 main.py:51] epoch 70, training loss: 15422.69, average training loss: 15633.09, base loss: 14432.17
[INFO 2017-06-28 11:59:26,199 main.py:51] epoch 71, training loss: 13020.42, average training loss: 15596.81, base loss: 14427.21
[INFO 2017-06-28 11:59:26,509 main.py:51] epoch 72, training loss: 12921.35, average training loss: 15560.16, base loss: 14418.79
[INFO 2017-06-28 11:59:26,837 main.py:51] epoch 73, training loss: 12243.56, average training loss: 15515.34, base loss: 14404.27
[INFO 2017-06-28 11:59:27,132 main.py:51] epoch 74, training loss: 11999.84, average training loss: 15468.46, base loss: 14383.65
[INFO 2017-06-28 11:59:27,449 main.py:51] epoch 75, training loss: 11503.51, average training loss: 15416.29, base loss: 14353.05
[INFO 2017-06-28 11:59:27,749 main.py:51] epoch 76, training loss: 13989.35, average training loss: 15397.76, base loss: 14359.22
[INFO 2017-06-28 11:59:28,023 main.py:51] epoch 77, training loss: 11676.59, average training loss: 15350.05, base loss: 14337.19
[INFO 2017-06-28 11:59:28,320 main.py:51] epoch 78, training loss: 12394.63, average training loss: 15312.64, base loss: 14326.38
[INFO 2017-06-28 11:59:28,625 main.py:51] epoch 79, training loss: 13463.01, average training loss: 15289.52, base loss: 14329.44
[INFO 2017-06-28 11:59:28,898 main.py:51] epoch 80, training loss: 13734.95, average training loss: 15270.33, base loss: 14334.21
[INFO 2017-06-28 11:59:29,191 main.py:51] epoch 81, training loss: 12739.74, average training loss: 15239.47, base loss: 14328.78
[INFO 2017-06-28 11:59:29,463 main.py:51] epoch 82, training loss: 13360.68, average training loss: 15216.83, base loss: 14323.99
[INFO 2017-06-28 11:59:29,787 main.py:51] epoch 83, training loss: 12421.21, average training loss: 15183.55, base loss: 14313.40
[INFO 2017-06-28 11:59:30,084 main.py:51] epoch 84, training loss: 11492.14, average training loss: 15140.12, base loss: 14289.04
[INFO 2017-06-28 11:59:30,387 main.py:51] epoch 85, training loss: 13130.29, average training loss: 15116.75, base loss: 14286.22
[INFO 2017-06-28 11:59:30,670 main.py:51] epoch 86, training loss: 13770.64, average training loss: 15101.28, base loss: 14294.48
[INFO 2017-06-28 11:59:30,962 main.py:51] epoch 87, training loss: 12911.55, average training loss: 15076.40, base loss: 14291.73
[INFO 2017-06-28 11:59:31,265 main.py:51] epoch 88, training loss: 12964.50, average training loss: 15052.67, base loss: 14287.65
[INFO 2017-06-28 11:59:31,564 main.py:51] epoch 89, training loss: 12175.14, average training loss: 15020.70, base loss: 14276.21
[INFO 2017-06-28 11:59:31,881 main.py:51] epoch 90, training loss: 11392.18, average training loss: 14980.82, base loss: 14255.38
[INFO 2017-06-28 11:59:32,183 main.py:51] epoch 91, training loss: 12788.90, average training loss: 14957.00, base loss: 14249.58
[INFO 2017-06-28 11:59:32,461 main.py:51] epoch 92, training loss: 12777.00, average training loss: 14933.56, base loss: 14248.88
[INFO 2017-06-28 11:59:32,754 main.py:51] epoch 93, training loss: 11506.06, average training loss: 14897.09, base loss: 14228.13
[INFO 2017-06-28 11:59:33,070 main.py:51] epoch 94, training loss: 12646.04, average training loss: 14873.40, base loss: 14224.07
[INFO 2017-06-28 11:59:33,367 main.py:51] epoch 95, training loss: 14629.77, average training loss: 14870.86, base loss: 14244.32
[INFO 2017-06-28 11:59:33,646 main.py:51] epoch 96, training loss: 12435.61, average training loss: 14845.76, base loss: 14237.21
[INFO 2017-06-28 11:59:33,964 main.py:51] epoch 97, training loss: 12705.55, average training loss: 14823.92, base loss: 14236.43
[INFO 2017-06-28 11:59:34,260 main.py:51] epoch 98, training loss: 13286.79, average training loss: 14808.39, base loss: 14240.64
[INFO 2017-06-28 11:59:34,580 main.py:51] epoch 99, training loss: 15104.06, average training loss: 14811.35, base loss: 14264.96
[INFO 2017-06-28 11:59:34,581 main.py:53] epoch 99, testing
[INFO 2017-06-28 11:59:36,084 main.py:105] average testing loss: 13129.47, base loss: 14170.98
[INFO 2017-06-28 11:59:36,084 main.py:106] improve_loss: 1041.51, improve_percent: 0.07
[INFO 2017-06-28 11:59:36,085 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 11:59:36,091 main.py:76] current best improved percent: 0.07
[INFO 2017-06-28 11:59:36,378 main.py:51] epoch 100, training loss: 13071.35, average training loss: 14794.12, base loss: 14265.40
[INFO 2017-06-28 11:59:36,676 main.py:51] epoch 101, training loss: 16336.98, average training loss: 14809.25, base loss: 14298.77
[INFO 2017-06-28 11:59:36,994 main.py:51] epoch 102, training loss: 12072.56, average training loss: 14782.68, base loss: 14289.82
[INFO 2017-06-28 11:59:37,292 main.py:51] epoch 103, training loss: 13214.70, average training loss: 14767.60, base loss: 14297.52
[INFO 2017-06-28 11:59:37,575 main.py:51] epoch 104, training loss: 14483.28, average training loss: 14764.89, base loss: 14313.01
[INFO 2017-06-28 11:59:37,846 main.py:51] epoch 105, training loss: 13460.98, average training loss: 14752.59, base loss: 14320.51
[INFO 2017-06-28 11:59:38,160 main.py:51] epoch 106, training loss: 12492.74, average training loss: 14731.47, base loss: 14315.87
[INFO 2017-06-28 11:59:38,454 main.py:51] epoch 107, training loss: 14549.88, average training loss: 14729.79, base loss: 14332.57
[INFO 2017-06-28 11:59:38,777 main.py:51] epoch 108, training loss: 12594.82, average training loss: 14710.20, base loss: 14332.40
[INFO 2017-06-28 11:59:39,068 main.py:51] epoch 109, training loss: 12457.12, average training loss: 14689.72, base loss: 14326.04
[INFO 2017-06-28 11:59:39,374 main.py:51] epoch 110, training loss: 11149.85, average training loss: 14657.83, base loss: 14307.73
[INFO 2017-06-28 11:59:39,654 main.py:51] epoch 111, training loss: 14533.46, average training loss: 14656.72, base loss: 14324.18
[INFO 2017-06-28 11:59:39,928 main.py:51] epoch 112, training loss: 11936.93, average training loss: 14632.65, base loss: 14311.13
[INFO 2017-06-28 11:59:40,209 main.py:51] epoch 113, training loss: 12236.02, average training loss: 14611.63, base loss: 14302.34
[INFO 2017-06-28 11:59:40,502 main.py:51] epoch 114, training loss: 12538.79, average training loss: 14593.60, base loss: 14304.98
[INFO 2017-06-28 11:59:40,800 main.py:51] epoch 115, training loss: 13587.58, average training loss: 14584.93, base loss: 14312.64
[INFO 2017-06-28 11:59:41,095 main.py:51] epoch 116, training loss: 12916.19, average training loss: 14570.67, base loss: 14312.96
[INFO 2017-06-28 11:59:41,408 main.py:51] epoch 117, training loss: 11272.70, average training loss: 14542.72, base loss: 14301.52
[INFO 2017-06-28 11:59:41,705 main.py:51] epoch 118, training loss: 13942.07, average training loss: 14537.67, base loss: 14315.98
[INFO 2017-06-28 11:59:42,024 main.py:51] epoch 119, training loss: 12820.07, average training loss: 14523.36, base loss: 14317.35
[INFO 2017-06-28 11:59:42,307 main.py:51] epoch 120, training loss: 13121.42, average training loss: 14511.77, base loss: 14324.99
[INFO 2017-06-28 11:59:42,606 main.py:51] epoch 121, training loss: 13097.06, average training loss: 14500.17, base loss: 14333.09
[INFO 2017-06-28 11:59:42,899 main.py:51] epoch 122, training loss: 12855.54, average training loss: 14486.80, base loss: 14331.78
[INFO 2017-06-28 11:59:43,208 main.py:51] epoch 123, training loss: 12079.37, average training loss: 14467.39, base loss: 14325.31
[INFO 2017-06-28 11:59:43,500 main.py:51] epoch 124, training loss: 11421.45, average training loss: 14443.02, base loss: 14316.20
[INFO 2017-06-28 11:59:43,792 main.py:51] epoch 125, training loss: 12454.59, average training loss: 14427.24, base loss: 14313.89
[INFO 2017-06-28 11:59:44,075 main.py:51] epoch 126, training loss: 13678.69, average training loss: 14421.35, base loss: 14322.54
[INFO 2017-06-28 11:59:44,395 main.py:51] epoch 127, training loss: 13754.91, average training loss: 14416.14, base loss: 14335.30
[INFO 2017-06-28 11:59:44,682 main.py:51] epoch 128, training loss: 12953.27, average training loss: 14404.80, base loss: 14338.96
[INFO 2017-06-28 11:59:44,973 main.py:51] epoch 129, training loss: 12070.07, average training loss: 14386.84, base loss: 14334.57
[INFO 2017-06-28 11:59:45,301 main.py:51] epoch 130, training loss: 12090.43, average training loss: 14369.31, base loss: 14331.07
[INFO 2017-06-28 11:59:45,621 main.py:51] epoch 131, training loss: 13322.51, average training loss: 14361.38, base loss: 14337.43
[INFO 2017-06-28 11:59:45,914 main.py:51] epoch 132, training loss: 13140.65, average training loss: 14352.20, base loss: 14345.03
[INFO 2017-06-28 11:59:46,203 main.py:51] epoch 133, training loss: 12356.95, average training loss: 14337.31, base loss: 14342.86
[INFO 2017-06-28 11:59:46,493 main.py:51] epoch 134, training loss: 11020.39, average training loss: 14312.74, base loss: 14331.79
[INFO 2017-06-28 11:59:46,797 main.py:51] epoch 135, training loss: 12962.78, average training loss: 14302.82, base loss: 14334.22
[INFO 2017-06-28 11:59:47,106 main.py:51] epoch 136, training loss: 11315.77, average training loss: 14281.01, base loss: 14322.77
[INFO 2017-06-28 11:59:47,395 main.py:51] epoch 137, training loss: 13267.64, average training loss: 14273.67, base loss: 14332.17
[INFO 2017-06-28 11:59:47,729 main.py:51] epoch 138, training loss: 12417.60, average training loss: 14260.32, base loss: 14333.21
[INFO 2017-06-28 11:59:48,003 main.py:51] epoch 139, training loss: 12183.29, average training loss: 14245.48, base loss: 14333.44
[INFO 2017-06-28 11:59:48,290 main.py:51] epoch 140, training loss: 12297.22, average training loss: 14231.66, base loss: 14336.08
[INFO 2017-06-28 11:59:48,612 main.py:51] epoch 141, training loss: 11849.68, average training loss: 14214.89, base loss: 14330.45
[INFO 2017-06-28 11:59:48,902 main.py:51] epoch 142, training loss: 13672.51, average training loss: 14211.09, base loss: 14341.11
[INFO 2017-06-28 11:59:49,202 main.py:51] epoch 143, training loss: 12070.80, average training loss: 14196.23, base loss: 14337.71
[INFO 2017-06-28 11:59:49,507 main.py:51] epoch 144, training loss: 11429.05, average training loss: 14177.15, base loss: 14326.68
[INFO 2017-06-28 11:59:49,808 main.py:51] epoch 145, training loss: 11325.22, average training loss: 14157.61, base loss: 14316.31
[INFO 2017-06-28 11:59:50,090 main.py:51] epoch 146, training loss: 11201.64, average training loss: 14137.51, base loss: 14303.18
[INFO 2017-06-28 11:59:50,368 main.py:51] epoch 147, training loss: 11532.69, average training loss: 14119.91, base loss: 14300.57
[INFO 2017-06-28 11:59:50,664 main.py:51] epoch 148, training loss: 13691.97, average training loss: 14117.03, base loss: 14314.92
[INFO 2017-06-28 11:59:50,941 main.py:51] epoch 149, training loss: 13787.13, average training loss: 14114.83, base loss: 14325.50
[INFO 2017-06-28 11:59:51,216 main.py:51] epoch 150, training loss: 18111.37, average training loss: 14141.30, base loss: 14368.62
[INFO 2017-06-28 11:59:51,512 main.py:51] epoch 151, training loss: 14665.67, average training loss: 14144.75, base loss: 14388.31
[INFO 2017-06-28 11:59:51,791 main.py:51] epoch 152, training loss: 13940.70, average training loss: 14143.42, base loss: 14402.17
[INFO 2017-06-28 11:59:52,078 main.py:51] epoch 153, training loss: 11748.95, average training loss: 14127.87, base loss: 14397.05
[INFO 2017-06-28 11:59:52,381 main.py:51] epoch 154, training loss: 10915.29, average training loss: 14107.14, base loss: 14378.66
[INFO 2017-06-28 11:59:52,676 main.py:51] epoch 155, training loss: 13448.13, average training loss: 14102.92, base loss: 14387.39
[INFO 2017-06-28 11:59:52,984 main.py:51] epoch 156, training loss: 10741.77, average training loss: 14081.51, base loss: 14371.29
[INFO 2017-06-28 11:59:53,280 main.py:51] epoch 157, training loss: 11256.96, average training loss: 14063.63, base loss: 14361.20
[INFO 2017-06-28 11:59:53,584 main.py:51] epoch 158, training loss: 12062.59, average training loss: 14051.05, base loss: 14360.11
[INFO 2017-06-28 11:59:53,866 main.py:51] epoch 159, training loss: 10072.07, average training loss: 14026.18, base loss: 14344.02
[INFO 2017-06-28 11:59:54,165 main.py:51] epoch 160, training loss: 11080.47, average training loss: 14007.88, base loss: 14330.64
[INFO 2017-06-28 11:59:54,466 main.py:51] epoch 161, training loss: 11227.01, average training loss: 13990.72, base loss: 14321.29
[INFO 2017-06-28 11:59:54,747 main.py:51] epoch 162, training loss: 12243.67, average training loss: 13980.00, base loss: 14321.49
[INFO 2017-06-28 11:59:55,030 main.py:51] epoch 163, training loss: 13979.76, average training loss: 13980.00, base loss: 14334.91
[INFO 2017-06-28 11:59:55,310 main.py:51] epoch 164, training loss: 10489.88, average training loss: 13958.84, base loss: 14322.28
[INFO 2017-06-28 11:59:55,591 main.py:51] epoch 165, training loss: 12263.18, average training loss: 13948.63, base loss: 14318.35
[INFO 2017-06-28 11:59:55,915 main.py:51] epoch 166, training loss: 10006.27, average training loss: 13925.02, base loss: 14302.09
[INFO 2017-06-28 11:59:56,198 main.py:51] epoch 167, training loss: 12749.32, average training loss: 13918.02, base loss: 14303.74
[INFO 2017-06-28 11:59:56,515 main.py:51] epoch 168, training loss: 10631.00, average training loss: 13898.57, base loss: 14297.73
[INFO 2017-06-28 11:59:56,797 main.py:51] epoch 169, training loss: 10935.05, average training loss: 13881.14, base loss: 14290.84
[INFO 2017-06-28 11:59:57,081 main.py:51] epoch 170, training loss: 13365.70, average training loss: 13878.13, base loss: 14298.21
[INFO 2017-06-28 11:59:57,383 main.py:51] epoch 171, training loss: 11499.06, average training loss: 13864.30, base loss: 14292.91
[INFO 2017-06-28 11:59:57,686 main.py:51] epoch 172, training loss: 12592.59, average training loss: 13856.95, base loss: 14295.52
[INFO 2017-06-28 11:59:57,972 main.py:51] epoch 173, training loss: 12553.42, average training loss: 13849.45, base loss: 14297.60
[INFO 2017-06-28 11:59:58,268 main.py:51] epoch 174, training loss: 12047.47, average training loss: 13839.16, base loss: 14294.08
[INFO 2017-06-28 11:59:58,547 main.py:51] epoch 175, training loss: 12413.72, average training loss: 13831.06, base loss: 14294.61
[INFO 2017-06-28 11:59:58,854 main.py:51] epoch 176, training loss: 11380.19, average training loss: 13817.21, base loss: 14287.02
[INFO 2017-06-28 11:59:59,172 main.py:51] epoch 177, training loss: 12376.51, average training loss: 13809.12, base loss: 14285.65
[INFO 2017-06-28 11:59:59,463 main.py:51] epoch 178, training loss: 10787.39, average training loss: 13792.24, base loss: 14277.12
[INFO 2017-06-28 11:59:59,764 main.py:51] epoch 179, training loss: 10866.93, average training loss: 13775.98, base loss: 14267.52
[INFO 2017-06-28 12:00:00,066 main.py:51] epoch 180, training loss: 11033.33, average training loss: 13760.83, base loss: 14260.04
[INFO 2017-06-28 12:00:00,350 main.py:51] epoch 181, training loss: 13707.46, average training loss: 13760.54, base loss: 14271.19
[INFO 2017-06-28 12:00:00,645 main.py:51] epoch 182, training loss: 11484.84, average training loss: 13748.10, base loss: 14268.34
[INFO 2017-06-28 12:00:00,956 main.py:51] epoch 183, training loss: 12545.28, average training loss: 13741.57, base loss: 14274.09
[INFO 2017-06-28 12:00:01,264 main.py:51] epoch 184, training loss: 11412.23, average training loss: 13728.97, base loss: 14269.90
[INFO 2017-06-28 12:00:01,557 main.py:51] epoch 185, training loss: 11975.88, average training loss: 13719.55, base loss: 14269.41
[INFO 2017-06-28 12:00:01,842 main.py:51] epoch 186, training loss: 11133.49, average training loss: 13705.72, base loss: 14267.69
[INFO 2017-06-28 12:00:02,136 main.py:51] epoch 187, training loss: 12578.42, average training loss: 13699.72, base loss: 14273.25
[INFO 2017-06-28 12:00:02,441 main.py:51] epoch 188, training loss: 11806.95, average training loss: 13689.71, base loss: 14271.96
[INFO 2017-06-28 12:00:02,756 main.py:51] epoch 189, training loss: 11547.12, average training loss: 13678.43, base loss: 14266.59
[INFO 2017-06-28 12:00:03,033 main.py:51] epoch 190, training loss: 12330.62, average training loss: 13671.38, base loss: 14270.64
[INFO 2017-06-28 12:00:03,338 main.py:51] epoch 191, training loss: 11632.23, average training loss: 13660.76, base loss: 14267.78
[INFO 2017-06-28 12:00:03,642 main.py:51] epoch 192, training loss: 11201.77, average training loss: 13648.01, base loss: 14260.25
[INFO 2017-06-28 12:00:03,952 main.py:51] epoch 193, training loss: 11169.77, average training loss: 13635.24, base loss: 14252.23
[INFO 2017-06-28 12:00:04,241 main.py:51] epoch 194, training loss: 12836.93, average training loss: 13631.15, base loss: 14258.20
[INFO 2017-06-28 12:00:04,541 main.py:51] epoch 195, training loss: 14941.10, average training loss: 13637.83, base loss: 14276.42
[INFO 2017-06-28 12:00:04,853 main.py:51] epoch 196, training loss: 11380.48, average training loss: 13626.37, base loss: 14273.81
[INFO 2017-06-28 12:00:05,150 main.py:51] epoch 197, training loss: 13022.25, average training loss: 13623.32, base loss: 14280.49
[INFO 2017-06-28 12:00:05,422 main.py:51] epoch 198, training loss: 11110.64, average training loss: 13610.69, base loss: 14276.19
[INFO 2017-06-28 12:00:05,726 main.py:51] epoch 199, training loss: 11630.72, average training loss: 13600.79, base loss: 14274.51
[INFO 2017-06-28 12:00:05,726 main.py:53] epoch 199, testing
[INFO 2017-06-28 12:00:07,265 main.py:105] average testing loss: 12768.88, base loss: 14881.33
[INFO 2017-06-28 12:00:07,265 main.py:106] improve_loss: 2112.46, improve_percent: 0.14
[INFO 2017-06-28 12:00:07,266 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:00:07,272 main.py:76] current best improved percent: 0.14
[INFO 2017-06-28 12:00:07,561 main.py:51] epoch 200, training loss: 13083.11, average training loss: 13598.22, base loss: 14280.57
[INFO 2017-06-28 12:00:07,841 main.py:51] epoch 201, training loss: 12075.17, average training loss: 13590.68, base loss: 14282.98
[INFO 2017-06-28 12:00:08,133 main.py:51] epoch 202, training loss: 12194.17, average training loss: 13583.80, base loss: 14282.58
[INFO 2017-06-28 12:00:08,428 main.py:51] epoch 203, training loss: 10732.87, average training loss: 13569.82, base loss: 14279.29
[INFO 2017-06-28 12:00:08,704 main.py:51] epoch 204, training loss: 12260.43, average training loss: 13563.44, base loss: 14280.79
[INFO 2017-06-28 12:00:09,020 main.py:51] epoch 205, training loss: 10524.25, average training loss: 13548.68, base loss: 14271.01
[INFO 2017-06-28 12:00:09,325 main.py:51] epoch 206, training loss: 11680.93, average training loss: 13539.66, base loss: 14267.47
[INFO 2017-06-28 12:00:09,616 main.py:51] epoch 207, training loss: 11014.79, average training loss: 13527.52, base loss: 14262.01
[INFO 2017-06-28 12:00:09,935 main.py:51] epoch 208, training loss: 12612.79, average training loss: 13523.14, base loss: 14267.29
[INFO 2017-06-28 12:00:10,249 main.py:51] epoch 209, training loss: 13327.14, average training loss: 13522.21, base loss: 14272.85
[INFO 2017-06-28 12:00:10,535 main.py:51] epoch 210, training loss: 11606.92, average training loss: 13513.13, base loss: 14270.63
[INFO 2017-06-28 12:00:10,831 main.py:51] epoch 211, training loss: 11524.38, average training loss: 13503.75, base loss: 14269.62
[INFO 2017-06-28 12:00:11,134 main.py:51] epoch 212, training loss: 12301.53, average training loss: 13498.11, base loss: 14267.49
[INFO 2017-06-28 12:00:11,444 main.py:51] epoch 213, training loss: 12987.62, average training loss: 13495.72, base loss: 14268.82
[INFO 2017-06-28 12:00:11,734 main.py:51] epoch 214, training loss: 14193.07, average training loss: 13498.97, base loss: 14278.39
[INFO 2017-06-28 12:00:12,030 main.py:51] epoch 215, training loss: 12312.16, average training loss: 13493.47, base loss: 14281.64
[INFO 2017-06-28 12:00:12,327 main.py:51] epoch 216, training loss: 11615.58, average training loss: 13484.82, base loss: 14282.29
[INFO 2017-06-28 12:00:12,633 main.py:51] epoch 217, training loss: 12684.02, average training loss: 13481.14, base loss: 14285.28
[INFO 2017-06-28 12:00:12,932 main.py:51] epoch 218, training loss: 12349.70, average training loss: 13475.98, base loss: 14287.36
[INFO 2017-06-28 12:00:13,211 main.py:51] epoch 219, training loss: 11543.05, average training loss: 13467.19, base loss: 14284.89
[INFO 2017-06-28 12:00:13,495 main.py:51] epoch 220, training loss: 11550.00, average training loss: 13458.52, base loss: 14288.52
[INFO 2017-06-28 12:00:13,789 main.py:51] epoch 221, training loss: 11436.42, average training loss: 13449.41, base loss: 14284.23
[INFO 2017-06-28 12:00:14,092 main.py:51] epoch 222, training loss: 11123.30, average training loss: 13438.98, base loss: 14278.45
[INFO 2017-06-28 12:00:14,405 main.py:51] epoch 223, training loss: 12127.80, average training loss: 13433.12, base loss: 14278.68
[INFO 2017-06-28 12:00:14,693 main.py:51] epoch 224, training loss: 9378.96, average training loss: 13415.11, base loss: 14267.71
[INFO 2017-06-28 12:00:14,987 main.py:51] epoch 225, training loss: 12293.07, average training loss: 13410.14, base loss: 14272.88
[INFO 2017-06-28 12:00:15,265 main.py:51] epoch 226, training loss: 11268.66, average training loss: 13400.71, base loss: 14271.52
[INFO 2017-06-28 12:00:15,569 main.py:51] epoch 227, training loss: 12419.41, average training loss: 13396.40, base loss: 14275.60
[INFO 2017-06-28 12:00:15,858 main.py:51] epoch 228, training loss: 13824.30, average training loss: 13398.27, base loss: 14285.53
[INFO 2017-06-28 12:00:16,149 main.py:51] epoch 229, training loss: 14620.48, average training loss: 13403.59, base loss: 14301.15
[INFO 2017-06-28 12:00:16,450 main.py:51] epoch 230, training loss: 11970.21, average training loss: 13397.38, base loss: 14302.10
[INFO 2017-06-28 12:00:16,757 main.py:51] epoch 231, training loss: 12725.25, average training loss: 13394.48, base loss: 14303.42
[INFO 2017-06-28 12:00:17,072 main.py:51] epoch 232, training loss: 15902.50, average training loss: 13405.25, base loss: 14322.57
[INFO 2017-06-28 12:00:17,378 main.py:51] epoch 233, training loss: 12182.04, average training loss: 13400.02, base loss: 14331.91
[INFO 2017-06-28 12:00:17,666 main.py:51] epoch 234, training loss: 13514.90, average training loss: 13400.51, base loss: 14340.44
[INFO 2017-06-28 12:00:17,967 main.py:51] epoch 235, training loss: 11351.01, average training loss: 13391.82, base loss: 14339.36
[INFO 2017-06-28 12:00:18,244 main.py:51] epoch 236, training loss: 12677.48, average training loss: 13388.81, base loss: 14344.70
[INFO 2017-06-28 12:00:18,531 main.py:51] epoch 237, training loss: 12594.40, average training loss: 13385.47, base loss: 14347.33
[INFO 2017-06-28 12:00:18,818 main.py:51] epoch 238, training loss: 10864.23, average training loss: 13374.92, base loss: 14340.82
[INFO 2017-06-28 12:00:19,111 main.py:51] epoch 239, training loss: 12175.79, average training loss: 13369.93, base loss: 14343.34
[INFO 2017-06-28 12:00:19,406 main.py:51] epoch 240, training loss: 10971.24, average training loss: 13359.97, base loss: 14341.83
[INFO 2017-06-28 12:00:19,729 main.py:51] epoch 241, training loss: 12001.99, average training loss: 13354.36, base loss: 14346.40
[INFO 2017-06-28 12:00:20,024 main.py:51] epoch 242, training loss: 10222.10, average training loss: 13341.47, base loss: 14340.67
[INFO 2017-06-28 12:00:20,322 main.py:51] epoch 243, training loss: 11116.23, average training loss: 13332.35, base loss: 14339.35
[INFO 2017-06-28 12:00:20,622 main.py:51] epoch 244, training loss: 10260.04, average training loss: 13319.81, base loss: 14329.27
[INFO 2017-06-28 12:00:20,914 main.py:51] epoch 245, training loss: 12349.93, average training loss: 13315.87, base loss: 14331.70
[INFO 2017-06-28 12:00:21,219 main.py:51] epoch 246, training loss: 13168.95, average training loss: 13315.28, base loss: 14338.41
[INFO 2017-06-28 12:00:21,529 main.py:51] epoch 247, training loss: 11830.35, average training loss: 13309.29, base loss: 14340.63
[INFO 2017-06-28 12:00:21,839 main.py:51] epoch 248, training loss: 13729.24, average training loss: 13310.97, base loss: 14349.98
[INFO 2017-06-28 12:00:22,146 main.py:51] epoch 249, training loss: 13193.36, average training loss: 13310.50, base loss: 14354.84
[INFO 2017-06-28 12:00:22,441 main.py:51] epoch 250, training loss: 10249.04, average training loss: 13298.31, base loss: 14346.09
[INFO 2017-06-28 12:00:22,737 main.py:51] epoch 251, training loss: 11786.00, average training loss: 13292.31, base loss: 14344.16
[INFO 2017-06-28 12:00:23,023 main.py:51] epoch 252, training loss: 11781.21, average training loss: 13286.33, base loss: 14345.33
[INFO 2017-06-28 12:00:23,298 main.py:51] epoch 253, training loss: 13924.07, average training loss: 13288.84, base loss: 14356.43
[INFO 2017-06-28 12:00:23,598 main.py:51] epoch 254, training loss: 12821.82, average training loss: 13287.01, base loss: 14357.44
[INFO 2017-06-28 12:00:23,924 main.py:51] epoch 255, training loss: 11086.32, average training loss: 13278.42, base loss: 14355.02
[INFO 2017-06-28 12:00:24,222 main.py:51] epoch 256, training loss: 13360.78, average training loss: 13278.74, base loss: 14362.84
[INFO 2017-06-28 12:00:24,523 main.py:51] epoch 257, training loss: 10767.01, average training loss: 13269.00, base loss: 14356.81
[INFO 2017-06-28 12:00:24,814 main.py:51] epoch 258, training loss: 10531.93, average training loss: 13258.43, base loss: 14349.16
[INFO 2017-06-28 12:00:25,092 main.py:51] epoch 259, training loss: 12802.01, average training loss: 13256.68, base loss: 14356.83
[INFO 2017-06-28 12:00:25,395 main.py:51] epoch 260, training loss: 9695.21, average training loss: 13243.03, base loss: 14349.11
[INFO 2017-06-28 12:00:25,694 main.py:51] epoch 261, training loss: 11010.10, average training loss: 13234.51, base loss: 14345.28
[INFO 2017-06-28 12:00:25,972 main.py:51] epoch 262, training loss: 14677.14, average training loss: 13239.99, base loss: 14360.20
[INFO 2017-06-28 12:00:26,263 main.py:51] epoch 263, training loss: 11113.08, average training loss: 13231.94, base loss: 14354.79
[INFO 2017-06-28 12:00:26,543 main.py:51] epoch 264, training loss: 12814.95, average training loss: 13230.36, base loss: 14358.41
[INFO 2017-06-28 12:00:26,840 main.py:51] epoch 265, training loss: 13228.68, average training loss: 13230.36, base loss: 14364.46
[INFO 2017-06-28 12:00:27,143 main.py:51] epoch 266, training loss: 12237.44, average training loss: 13226.64, base loss: 14366.21
[INFO 2017-06-28 12:00:27,434 main.py:51] epoch 267, training loss: 12478.44, average training loss: 13223.85, base loss: 14370.89
[INFO 2017-06-28 12:00:27,723 main.py:51] epoch 268, training loss: 11578.94, average training loss: 13217.73, base loss: 14369.08
[INFO 2017-06-28 12:00:28,001 main.py:51] epoch 269, training loss: 11426.84, average training loss: 13211.10, base loss: 14368.43
[INFO 2017-06-28 12:00:28,299 main.py:51] epoch 270, training loss: 12348.46, average training loss: 13207.92, base loss: 14373.02
[INFO 2017-06-28 12:00:28,613 main.py:51] epoch 271, training loss: 12823.36, average training loss: 13206.50, base loss: 14380.54
[INFO 2017-06-28 12:00:28,924 main.py:51] epoch 272, training loss: 13436.40, average training loss: 13207.35, base loss: 14389.34
[INFO 2017-06-28 12:00:29,215 main.py:51] epoch 273, training loss: 11874.58, average training loss: 13202.48, base loss: 14391.39
[INFO 2017-06-28 12:00:29,499 main.py:51] epoch 274, training loss: 13317.92, average training loss: 13202.90, base loss: 14396.85
[INFO 2017-06-28 12:00:29,772 main.py:51] epoch 275, training loss: 10525.68, average training loss: 13193.20, base loss: 14391.77
[INFO 2017-06-28 12:00:30,077 main.py:51] epoch 276, training loss: 12835.24, average training loss: 13191.91, base loss: 14400.03
[INFO 2017-06-28 12:00:30,380 main.py:51] epoch 277, training loss: 12168.00, average training loss: 13188.23, base loss: 14404.59
[INFO 2017-06-28 12:00:30,656 main.py:51] epoch 278, training loss: 13557.02, average training loss: 13189.55, base loss: 14413.54
[INFO 2017-06-28 12:00:30,956 main.py:51] epoch 279, training loss: 11795.61, average training loss: 13184.57, base loss: 14414.35
[INFO 2017-06-28 12:00:31,251 main.py:51] epoch 280, training loss: 11639.11, average training loss: 13179.07, base loss: 14415.47
[INFO 2017-06-28 12:00:31,540 main.py:51] epoch 281, training loss: 13635.84, average training loss: 13180.69, base loss: 14423.33
[INFO 2017-06-28 12:00:31,834 main.py:51] epoch 282, training loss: 13079.75, average training loss: 13180.33, base loss: 14429.08
[INFO 2017-06-28 12:00:32,137 main.py:51] epoch 283, training loss: 11974.24, average training loss: 13176.09, base loss: 14426.34
[INFO 2017-06-28 12:00:32,459 main.py:51] epoch 284, training loss: 11819.81, average training loss: 13171.33, base loss: 14428.18
[INFO 2017-06-28 12:00:32,753 main.py:51] epoch 285, training loss: 13752.84, average training loss: 13173.36, base loss: 14439.51
[INFO 2017-06-28 12:00:33,071 main.py:51] epoch 286, training loss: 11901.33, average training loss: 13168.93, base loss: 14441.87
[INFO 2017-06-28 12:00:33,372 main.py:51] epoch 287, training loss: 11931.31, average training loss: 13164.63, base loss: 14444.05
[INFO 2017-06-28 12:00:33,670 main.py:51] epoch 288, training loss: 13736.81, average training loss: 13166.61, base loss: 14451.80
[INFO 2017-06-28 12:00:33,947 main.py:51] epoch 289, training loss: 11552.40, average training loss: 13161.04, base loss: 14451.91
[INFO 2017-06-28 12:00:34,245 main.py:51] epoch 290, training loss: 11511.81, average training loss: 13155.38, base loss: 14452.78
[INFO 2017-06-28 12:00:34,569 main.py:51] epoch 291, training loss: 10783.21, average training loss: 13147.25, base loss: 14447.84
[INFO 2017-06-28 12:00:34,856 main.py:51] epoch 292, training loss: 10543.96, average training loss: 13138.37, base loss: 14439.36
[INFO 2017-06-28 12:00:35,143 main.py:51] epoch 293, training loss: 11422.03, average training loss: 13132.53, base loss: 14438.05
[INFO 2017-06-28 12:00:35,461 main.py:51] epoch 294, training loss: 12713.34, average training loss: 13131.11, base loss: 14442.77
[INFO 2017-06-28 12:00:35,754 main.py:51] epoch 295, training loss: 12601.71, average training loss: 13129.32, base loss: 14445.92
[INFO 2017-06-28 12:00:36,032 main.py:51] epoch 296, training loss: 9365.85, average training loss: 13116.65, base loss: 14434.74
[INFO 2017-06-28 12:00:36,329 main.py:51] epoch 297, training loss: 11196.08, average training loss: 13110.20, base loss: 14434.04
[INFO 2017-06-28 12:00:36,612 main.py:51] epoch 298, training loss: 13615.77, average training loss: 13111.89, base loss: 14442.19
[INFO 2017-06-28 12:00:36,915 main.py:51] epoch 299, training loss: 10047.74, average training loss: 13101.68, base loss: 14435.75
[INFO 2017-06-28 12:00:36,915 main.py:53] epoch 299, testing
[INFO 2017-06-28 12:00:38,439 main.py:105] average testing loss: 13755.82, base loss: 16480.30
[INFO 2017-06-28 12:00:38,439 main.py:106] improve_loss: 2724.48, improve_percent: 0.17
[INFO 2017-06-28 12:00:38,439 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:00:38,446 main.py:76] current best improved percent: 0.17
[INFO 2017-06-28 12:00:38,732 main.py:51] epoch 300, training loss: 11214.94, average training loss: 13095.41, base loss: 14434.91
[INFO 2017-06-28 12:00:39,019 main.py:51] epoch 301, training loss: 13912.27, average training loss: 13098.12, base loss: 14440.45
[INFO 2017-06-28 12:00:39,299 main.py:51] epoch 302, training loss: 9875.08, average training loss: 13087.48, base loss: 14431.66
[INFO 2017-06-28 12:00:39,578 main.py:51] epoch 303, training loss: 9303.95, average training loss: 13075.03, base loss: 14421.39
[INFO 2017-06-28 12:00:39,875 main.py:51] epoch 304, training loss: 11194.69, average training loss: 13068.87, base loss: 14421.01
[INFO 2017-06-28 12:00:40,173 main.py:51] epoch 305, training loss: 15279.83, average training loss: 13076.09, base loss: 14434.29
[INFO 2017-06-28 12:00:40,487 main.py:51] epoch 306, training loss: 13540.23, average training loss: 13077.61, base loss: 14443.43
[INFO 2017-06-28 12:00:40,787 main.py:51] epoch 307, training loss: 11706.41, average training loss: 13073.15, base loss: 14442.16
[INFO 2017-06-28 12:00:41,070 main.py:51] epoch 308, training loss: 11772.15, average training loss: 13068.94, base loss: 14443.12
[INFO 2017-06-28 12:00:41,354 main.py:51] epoch 309, training loss: 11590.19, average training loss: 13064.17, base loss: 14443.35
[INFO 2017-06-28 12:00:41,671 main.py:51] epoch 310, training loss: 9548.60, average training loss: 13052.87, base loss: 14435.21
[INFO 2017-06-28 12:00:41,967 main.py:51] epoch 311, training loss: 11756.89, average training loss: 13048.72, base loss: 14437.36
[INFO 2017-06-28 12:00:42,261 main.py:51] epoch 312, training loss: 11463.59, average training loss: 13043.65, base loss: 14435.92
[INFO 2017-06-28 12:00:42,558 main.py:51] epoch 313, training loss: 10544.66, average training loss: 13035.69, base loss: 14430.16
[INFO 2017-06-28 12:00:42,831 main.py:51] epoch 314, training loss: 12933.05, average training loss: 13035.37, base loss: 14437.90
[INFO 2017-06-28 12:00:43,142 main.py:51] epoch 315, training loss: 11129.57, average training loss: 13029.34, base loss: 14438.74
[INFO 2017-06-28 12:00:43,426 main.py:51] epoch 316, training loss: 11293.44, average training loss: 13023.86, base loss: 14439.25
[INFO 2017-06-28 12:00:43,716 main.py:51] epoch 317, training loss: 10416.52, average training loss: 13015.66, base loss: 14434.82
[INFO 2017-06-28 12:00:44,010 main.py:51] epoch 318, training loss: 10881.90, average training loss: 13008.97, base loss: 14434.20
[INFO 2017-06-28 12:00:44,311 main.py:51] epoch 319, training loss: 10924.75, average training loss: 13002.46, base loss: 14430.93
[INFO 2017-06-28 12:00:44,597 main.py:51] epoch 320, training loss: 10760.86, average training loss: 12995.48, base loss: 14425.44
[INFO 2017-06-28 12:00:44,897 main.py:51] epoch 321, training loss: 15004.58, average training loss: 13001.72, base loss: 14436.22
[INFO 2017-06-28 12:00:45,183 main.py:51] epoch 322, training loss: 10159.66, average training loss: 12992.92, base loss: 14430.97
[INFO 2017-06-28 12:00:45,495 main.py:51] epoch 323, training loss: 11522.77, average training loss: 12988.38, base loss: 14432.39
[INFO 2017-06-28 12:00:45,799 main.py:51] epoch 324, training loss: 10056.80, average training loss: 12979.36, base loss: 14424.78
[INFO 2017-06-28 12:00:46,084 main.py:51] epoch 325, training loss: 10181.98, average training loss: 12970.78, base loss: 14419.78
[INFO 2017-06-28 12:00:46,364 main.py:51] epoch 326, training loss: 10721.59, average training loss: 12963.90, base loss: 14416.61
[INFO 2017-06-28 12:00:46,677 main.py:51] epoch 327, training loss: 11800.75, average training loss: 12960.35, base loss: 14418.36
[INFO 2017-06-28 12:00:46,997 main.py:51] epoch 328, training loss: 11433.94, average training loss: 12955.71, base loss: 14418.38
[INFO 2017-06-28 12:00:47,293 main.py:51] epoch 329, training loss: 10385.66, average training loss: 12947.93, base loss: 14416.15
[INFO 2017-06-28 12:00:47,586 main.py:51] epoch 330, training loss: 11377.97, average training loss: 12943.18, base loss: 14414.57
[INFO 2017-06-28 12:00:47,896 main.py:51] epoch 331, training loss: 11211.88, average training loss: 12937.97, base loss: 14411.28
[INFO 2017-06-28 12:00:48,189 main.py:51] epoch 332, training loss: 10635.67, average training loss: 12931.05, base loss: 14408.86
[INFO 2017-06-28 12:00:48,486 main.py:51] epoch 333, training loss: 12162.24, average training loss: 12928.75, base loss: 14412.36
[INFO 2017-06-28 12:00:48,787 main.py:51] epoch 334, training loss: 11010.11, average training loss: 12923.03, base loss: 14410.97
[INFO 2017-06-28 12:00:49,091 main.py:51] epoch 335, training loss: 9856.21, average training loss: 12913.90, base loss: 14402.20
[INFO 2017-06-28 12:00:49,369 main.py:51] epoch 336, training loss: 10387.81, average training loss: 12906.40, base loss: 14397.10
[INFO 2017-06-28 12:00:49,669 main.py:51] epoch 337, training loss: 10588.56, average training loss: 12899.54, base loss: 14392.69
[INFO 2017-06-28 12:00:49,954 main.py:51] epoch 338, training loss: 11710.21, average training loss: 12896.04, base loss: 14393.57
[INFO 2017-06-28 12:00:50,247 main.py:51] epoch 339, training loss: 10464.36, average training loss: 12888.88, base loss: 14390.71
[INFO 2017-06-28 12:00:50,547 main.py:51] epoch 340, training loss: 10716.55, average training loss: 12882.51, base loss: 14388.85
[INFO 2017-06-28 12:00:50,845 main.py:51] epoch 341, training loss: 11556.80, average training loss: 12878.64, base loss: 14388.09
[INFO 2017-06-28 12:00:51,160 main.py:51] epoch 342, training loss: 9939.19, average training loss: 12870.07, base loss: 14381.19
[INFO 2017-06-28 12:00:51,466 main.py:51] epoch 343, training loss: 10947.98, average training loss: 12864.48, base loss: 14380.69
[INFO 2017-06-28 12:00:51,758 main.py:51] epoch 344, training loss: 11937.77, average training loss: 12861.79, base loss: 14383.09
[INFO 2017-06-28 12:00:52,042 main.py:51] epoch 345, training loss: 10942.93, average training loss: 12856.25, base loss: 14382.55
[INFO 2017-06-28 12:00:52,370 main.py:51] epoch 346, training loss: 12261.46, average training loss: 12854.53, base loss: 14386.82
[INFO 2017-06-28 12:00:52,692 main.py:51] epoch 347, training loss: 11241.63, average training loss: 12849.90, base loss: 14384.10
[INFO 2017-06-28 12:00:53,003 main.py:51] epoch 348, training loss: 12702.70, average training loss: 12849.48, base loss: 14388.39
[INFO 2017-06-28 12:00:53,338 main.py:51] epoch 349, training loss: 10428.67, average training loss: 12842.56, base loss: 14385.79
[INFO 2017-06-28 12:00:53,622 main.py:51] epoch 350, training loss: 10062.74, average training loss: 12834.64, base loss: 14379.05
[INFO 2017-06-28 12:00:53,930 main.py:51] epoch 351, training loss: 10332.21, average training loss: 12827.53, base loss: 14374.24
[INFO 2017-06-28 12:00:54,228 main.py:51] epoch 352, training loss: 10007.04, average training loss: 12819.54, base loss: 14368.04
[INFO 2017-06-28 12:00:54,506 main.py:51] epoch 353, training loss: 13686.26, average training loss: 12821.99, base loss: 14375.29
[INFO 2017-06-28 12:00:54,826 main.py:51] epoch 354, training loss: 12027.24, average training loss: 12819.75, base loss: 14376.47
[INFO 2017-06-28 12:00:55,141 main.py:51] epoch 355, training loss: 10388.84, average training loss: 12812.92, base loss: 14371.72
[INFO 2017-06-28 12:00:55,437 main.py:51] epoch 356, training loss: 10549.27, average training loss: 12806.58, base loss: 14369.14
[INFO 2017-06-28 12:00:55,726 main.py:51] epoch 357, training loss: 9772.16, average training loss: 12798.11, base loss: 14364.42
[INFO 2017-06-28 12:00:56,031 main.py:51] epoch 358, training loss: 13344.43, average training loss: 12799.63, base loss: 14370.46
[INFO 2017-06-28 12:00:56,357 main.py:51] epoch 359, training loss: 11689.24, average training loss: 12796.54, base loss: 14372.03
[INFO 2017-06-28 12:00:56,663 main.py:51] epoch 360, training loss: 12206.14, average training loss: 12794.91, base loss: 14378.52
[INFO 2017-06-28 12:00:56,987 main.py:51] epoch 361, training loss: 9878.18, average training loss: 12786.85, base loss: 14374.45
[INFO 2017-06-28 12:00:57,293 main.py:51] epoch 362, training loss: 10658.89, average training loss: 12780.99, base loss: 14371.32
[INFO 2017-06-28 12:00:57,618 main.py:51] epoch 363, training loss: 11679.13, average training loss: 12777.96, base loss: 14372.15
[INFO 2017-06-28 12:00:57,908 main.py:51] epoch 364, training loss: 12497.91, average training loss: 12777.19, base loss: 14374.91
[INFO 2017-06-28 12:00:58,214 main.py:51] epoch 365, training loss: 11692.74, average training loss: 12774.23, base loss: 14375.86
[INFO 2017-06-28 12:00:58,547 main.py:51] epoch 366, training loss: 12430.06, average training loss: 12773.29, base loss: 14380.95
[INFO 2017-06-28 12:00:58,838 main.py:51] epoch 367, training loss: 10038.89, average training loss: 12765.86, base loss: 14374.28
[INFO 2017-06-28 12:00:59,177 main.py:51] epoch 368, training loss: 10570.90, average training loss: 12759.91, base loss: 14373.97
[INFO 2017-06-28 12:00:59,482 main.py:51] epoch 369, training loss: 12938.10, average training loss: 12760.40, base loss: 14378.48
[INFO 2017-06-28 12:00:59,789 main.py:51] epoch 370, training loss: 10922.38, average training loss: 12755.44, base loss: 14375.13
[INFO 2017-06-28 12:01:00,084 main.py:51] epoch 371, training loss: 11509.37, average training loss: 12752.09, base loss: 14375.73
[INFO 2017-06-28 12:01:00,393 main.py:51] epoch 372, training loss: 11826.91, average training loss: 12749.61, base loss: 14378.90
[INFO 2017-06-28 12:01:00,695 main.py:51] epoch 373, training loss: 13432.02, average training loss: 12751.44, base loss: 14385.19
[INFO 2017-06-28 12:01:00,996 main.py:51] epoch 374, training loss: 10050.38, average training loss: 12744.23, base loss: 14380.55
[INFO 2017-06-28 12:01:01,339 main.py:51] epoch 375, training loss: 10151.86, average training loss: 12737.34, base loss: 14375.53
[INFO 2017-06-28 12:01:01,692 main.py:51] epoch 376, training loss: 11135.13, average training loss: 12733.09, base loss: 14376.46
[INFO 2017-06-28 12:01:02,026 main.py:51] epoch 377, training loss: 11069.39, average training loss: 12728.69, base loss: 14374.70
[INFO 2017-06-28 12:01:02,343 main.py:51] epoch 378, training loss: 10164.65, average training loss: 12721.92, base loss: 14369.47
[INFO 2017-06-28 12:01:02,651 main.py:51] epoch 379, training loss: 11141.91, average training loss: 12717.76, base loss: 14369.16
[INFO 2017-06-28 12:01:02,950 main.py:51] epoch 380, training loss: 11264.38, average training loss: 12713.95, base loss: 14369.91
[INFO 2017-06-28 12:01:03,256 main.py:51] epoch 381, training loss: 11242.42, average training loss: 12710.10, base loss: 14369.81
[INFO 2017-06-28 12:01:03,547 main.py:51] epoch 382, training loss: 9691.77, average training loss: 12702.22, base loss: 14362.37
[INFO 2017-06-28 12:01:03,843 main.py:51] epoch 383, training loss: 10717.77, average training loss: 12697.05, base loss: 14360.45
[INFO 2017-06-28 12:01:04,188 main.py:51] epoch 384, training loss: 12119.95, average training loss: 12695.55, base loss: 14364.53
[INFO 2017-06-28 12:01:04,503 main.py:51] epoch 385, training loss: 10922.59, average training loss: 12690.96, base loss: 14362.67
[INFO 2017-06-28 12:01:04,792 main.py:51] epoch 386, training loss: 11857.31, average training loss: 12688.80, base loss: 14361.93
[INFO 2017-06-28 12:01:05,095 main.py:51] epoch 387, training loss: 11946.06, average training loss: 12686.89, base loss: 14361.42
[INFO 2017-06-28 12:01:05,375 main.py:51] epoch 388, training loss: 13378.68, average training loss: 12688.67, base loss: 14369.11
[INFO 2017-06-28 12:01:05,684 main.py:51] epoch 389, training loss: 11512.40, average training loss: 12685.65, base loss: 14369.78
[INFO 2017-06-28 12:01:05,958 main.py:51] epoch 390, training loss: 11575.92, average training loss: 12682.81, base loss: 14370.90
[INFO 2017-06-28 12:01:06,251 main.py:51] epoch 391, training loss: 12559.77, average training loss: 12682.50, base loss: 14375.19
[INFO 2017-06-28 12:01:06,562 main.py:51] epoch 392, training loss: 13373.71, average training loss: 12684.26, base loss: 14380.98
[INFO 2017-06-28 12:01:06,857 main.py:51] epoch 393, training loss: 11071.79, average training loss: 12680.17, base loss: 14381.02
[INFO 2017-06-28 12:01:07,180 main.py:51] epoch 394, training loss: 9730.01, average training loss: 12672.70, base loss: 14376.72
[INFO 2017-06-28 12:01:07,516 main.py:51] epoch 395, training loss: 13266.83, average training loss: 12674.20, base loss: 14382.14
[INFO 2017-06-28 12:01:07,810 main.py:51] epoch 396, training loss: 11040.68, average training loss: 12670.08, base loss: 14383.37
[INFO 2017-06-28 12:01:08,141 main.py:51] epoch 397, training loss: 10859.79, average training loss: 12665.53, base loss: 14382.51
[INFO 2017-06-28 12:01:08,440 main.py:51] epoch 398, training loss: 11191.36, average training loss: 12661.84, base loss: 14382.09
[INFO 2017-06-28 12:01:08,717 main.py:51] epoch 399, training loss: 12255.88, average training loss: 12660.82, base loss: 14384.92
[INFO 2017-06-28 12:01:08,718 main.py:53] epoch 399, testing
[INFO 2017-06-28 12:01:10,261 main.py:105] average testing loss: 12118.64, base loss: 14646.71
[INFO 2017-06-28 12:01:10,261 main.py:106] improve_loss: 2528.06, improve_percent: 0.17
[INFO 2017-06-28 12:01:10,261 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:01:10,268 main.py:76] current best improved percent: 0.17
[INFO 2017-06-28 12:01:10,555 main.py:51] epoch 400, training loss: 12411.16, average training loss: 12660.20, base loss: 14386.04
[INFO 2017-06-28 12:01:10,856 main.py:51] epoch 401, training loss: 11308.47, average training loss: 12656.84, base loss: 14385.41
[INFO 2017-06-28 12:01:11,155 main.py:51] epoch 402, training loss: 10660.94, average training loss: 12651.89, base loss: 14383.77
[INFO 2017-06-28 12:01:11,458 main.py:51] epoch 403, training loss: 11289.50, average training loss: 12648.51, base loss: 14384.36
[INFO 2017-06-28 12:01:11,773 main.py:51] epoch 404, training loss: 10532.13, average training loss: 12643.29, base loss: 14379.23
[INFO 2017-06-28 12:01:12,070 main.py:51] epoch 405, training loss: 11582.97, average training loss: 12640.68, base loss: 14380.92
[INFO 2017-06-28 12:01:12,379 main.py:51] epoch 406, training loss: 11138.34, average training loss: 12636.99, base loss: 14380.82
[INFO 2017-06-28 12:01:12,676 main.py:51] epoch 407, training loss: 11984.11, average training loss: 12635.39, base loss: 14383.09
[INFO 2017-06-28 12:01:12,987 main.py:51] epoch 408, training loss: 12546.47, average training loss: 12635.17, base loss: 14388.74
[INFO 2017-06-28 12:01:13,267 main.py:51] epoch 409, training loss: 10716.17, average training loss: 12630.49, base loss: 14387.91
[INFO 2017-06-28 12:01:13,591 main.py:51] epoch 410, training loss: 10342.65, average training loss: 12624.92, base loss: 14384.39
[INFO 2017-06-28 12:01:13,893 main.py:51] epoch 411, training loss: 11229.37, average training loss: 12621.53, base loss: 14382.33
[INFO 2017-06-28 12:01:14,207 main.py:51] epoch 412, training loss: 10799.50, average training loss: 12617.12, base loss: 14381.40
[INFO 2017-06-28 12:01:14,497 main.py:51] epoch 413, training loss: 11305.61, average training loss: 12613.95, base loss: 14380.91
[INFO 2017-06-28 12:01:14,810 main.py:51] epoch 414, training loss: 11442.83, average training loss: 12611.13, base loss: 14381.57
[INFO 2017-06-28 12:01:15,102 main.py:51] epoch 415, training loss: 13551.08, average training loss: 12613.39, base loss: 14386.79
[INFO 2017-06-28 12:01:15,414 main.py:51] epoch 416, training loss: 11326.55, average training loss: 12610.31, base loss: 14386.56
[INFO 2017-06-28 12:01:15,719 main.py:51] epoch 417, training loss: 9798.60, average training loss: 12603.58, base loss: 14382.28
[INFO 2017-06-28 12:01:16,032 main.py:51] epoch 418, training loss: 11751.23, average training loss: 12601.55, base loss: 14381.83
[INFO 2017-06-28 12:01:16,349 main.py:51] epoch 419, training loss: 12574.32, average training loss: 12601.48, base loss: 14384.65
[INFO 2017-06-28 12:01:16,667 main.py:51] epoch 420, training loss: 10345.28, average training loss: 12596.12, base loss: 14381.41
[INFO 2017-06-28 12:01:16,958 main.py:51] epoch 421, training loss: 10905.54, average training loss: 12592.11, base loss: 14380.50
[INFO 2017-06-28 12:01:17,256 main.py:51] epoch 422, training loss: 11723.20, average training loss: 12590.06, base loss: 14378.49
[INFO 2017-06-28 12:01:17,539 main.py:51] epoch 423, training loss: 11025.73, average training loss: 12586.37, base loss: 14379.17
[INFO 2017-06-28 12:01:17,853 main.py:51] epoch 424, training loss: 11448.72, average training loss: 12583.69, base loss: 14378.52
[INFO 2017-06-28 12:01:18,183 main.py:51] epoch 425, training loss: 11348.94, average training loss: 12580.80, base loss: 14380.37
[INFO 2017-06-28 12:01:18,490 main.py:51] epoch 426, training loss: 11436.30, average training loss: 12578.12, base loss: 14379.86
[INFO 2017-06-28 12:01:18,811 main.py:51] epoch 427, training loss: 12783.98, average training loss: 12578.60, base loss: 14385.69
[INFO 2017-06-28 12:01:19,106 main.py:51] epoch 428, training loss: 11375.51, average training loss: 12575.79, base loss: 14387.83
[INFO 2017-06-28 12:01:19,390 main.py:51] epoch 429, training loss: 10724.50, average training loss: 12571.49, base loss: 14385.55
[INFO 2017-06-28 12:01:19,696 main.py:51] epoch 430, training loss: 11717.05, average training loss: 12569.50, base loss: 14388.11
[INFO 2017-06-28 12:01:20,010 main.py:51] epoch 431, training loss: 9587.02, average training loss: 12562.60, base loss: 14381.98
[INFO 2017-06-28 12:01:20,313 main.py:51] epoch 432, training loss: 10036.72, average training loss: 12556.77, base loss: 14377.50
[INFO 2017-06-28 12:01:20,606 main.py:51] epoch 433, training loss: 10451.95, average training loss: 12551.92, base loss: 14375.01
[INFO 2017-06-28 12:01:20,895 main.py:51] epoch 434, training loss: 10808.13, average training loss: 12547.91, base loss: 14371.59
[INFO 2017-06-28 12:01:21,195 main.py:51] epoch 435, training loss: 11257.53, average training loss: 12544.95, base loss: 14370.84
[INFO 2017-06-28 12:01:21,496 main.py:51] epoch 436, training loss: 10959.99, average training loss: 12541.32, base loss: 14369.49
[INFO 2017-06-28 12:01:21,780 main.py:51] epoch 437, training loss: 11907.13, average training loss: 12539.87, base loss: 14370.04
[INFO 2017-06-28 12:01:22,083 main.py:51] epoch 438, training loss: 10419.69, average training loss: 12535.04, base loss: 14367.35
[INFO 2017-06-28 12:01:22,397 main.py:51] epoch 439, training loss: 13016.64, average training loss: 12536.14, base loss: 14372.78
[INFO 2017-06-28 12:01:22,710 main.py:51] epoch 440, training loss: 9084.50, average training loss: 12528.31, base loss: 14366.73
[INFO 2017-06-28 12:01:23,009 main.py:51] epoch 441, training loss: 10690.18, average training loss: 12524.15, base loss: 14364.97
[INFO 2017-06-28 12:01:23,307 main.py:51] epoch 442, training loss: 12931.05, average training loss: 12525.07, base loss: 14368.49
[INFO 2017-06-28 12:01:23,593 main.py:51] epoch 443, training loss: 11084.37, average training loss: 12521.83, base loss: 14368.99
[INFO 2017-06-28 12:01:23,882 main.py:51] epoch 444, training loss: 12203.40, average training loss: 12521.11, base loss: 14369.09
[INFO 2017-06-28 12:01:24,182 main.py:51] epoch 445, training loss: 12356.14, average training loss: 12520.74, base loss: 14372.61
[INFO 2017-06-28 12:01:24,485 main.py:51] epoch 446, training loss: 11297.64, average training loss: 12518.01, base loss: 14373.43
[INFO 2017-06-28 12:01:24,786 main.py:51] epoch 447, training loss: 12103.89, average training loss: 12517.08, base loss: 14374.61
[INFO 2017-06-28 12:01:25,084 main.py:51] epoch 448, training loss: 10783.31, average training loss: 12513.22, base loss: 14372.52
[INFO 2017-06-28 12:01:25,395 main.py:51] epoch 449, training loss: 10137.88, average training loss: 12507.94, base loss: 14371.61
[INFO 2017-06-28 12:01:25,706 main.py:51] epoch 450, training loss: 10544.40, average training loss: 12503.59, base loss: 14370.54
[INFO 2017-06-28 12:01:26,007 main.py:51] epoch 451, training loss: 14269.46, average training loss: 12507.49, base loss: 14379.34
[INFO 2017-06-28 12:01:26,315 main.py:51] epoch 452, training loss: 9547.14, average training loss: 12500.96, base loss: 14375.65
[INFO 2017-06-28 12:01:26,640 main.py:51] epoch 453, training loss: 9965.41, average training loss: 12495.37, base loss: 14370.58
[INFO 2017-06-28 12:01:26,938 main.py:51] epoch 454, training loss: 10980.81, average training loss: 12492.05, base loss: 14369.84
[INFO 2017-06-28 12:01:27,246 main.py:51] epoch 455, training loss: 11284.61, average training loss: 12489.40, base loss: 14368.97
[INFO 2017-06-28 12:01:27,552 main.py:51] epoch 456, training loss: 12605.99, average training loss: 12489.65, base loss: 14373.17
[INFO 2017-06-28 12:01:27,838 main.py:51] epoch 457, training loss: 11808.93, average training loss: 12488.17, base loss: 14374.30
[INFO 2017-06-28 12:01:28,155 main.py:51] epoch 458, training loss: 10651.51, average training loss: 12484.17, base loss: 14370.90
[INFO 2017-06-28 12:01:28,453 main.py:51] epoch 459, training loss: 11057.12, average training loss: 12481.06, base loss: 14370.23
[INFO 2017-06-28 12:01:28,769 main.py:51] epoch 460, training loss: 13180.47, average training loss: 12482.58, base loss: 14374.55
[INFO 2017-06-28 12:01:29,085 main.py:51] epoch 461, training loss: 11459.04, average training loss: 12480.36, base loss: 14375.43
[INFO 2017-06-28 12:01:29,392 main.py:51] epoch 462, training loss: 11877.19, average training loss: 12479.06, base loss: 14377.24
[INFO 2017-06-28 12:01:29,705 main.py:51] epoch 463, training loss: 10609.51, average training loss: 12475.03, base loss: 14374.71
[INFO 2017-06-28 12:01:30,041 main.py:51] epoch 464, training loss: 11293.40, average training loss: 12472.49, base loss: 14374.33
[INFO 2017-06-28 12:01:30,339 main.py:51] epoch 465, training loss: 11234.15, average training loss: 12469.83, base loss: 14374.54
[INFO 2017-06-28 12:01:30,638 main.py:51] epoch 466, training loss: 12022.37, average training loss: 12468.88, base loss: 14377.07
[INFO 2017-06-28 12:01:30,948 main.py:51] epoch 467, training loss: 12733.26, average training loss: 12469.44, base loss: 14379.99
[INFO 2017-06-28 12:01:31,224 main.py:51] epoch 468, training loss: 10485.85, average training loss: 12465.21, base loss: 14378.89
[INFO 2017-06-28 12:01:31,544 main.py:51] epoch 469, training loss: 11795.54, average training loss: 12463.79, base loss: 14378.39
[INFO 2017-06-28 12:01:31,858 main.py:51] epoch 470, training loss: 11764.49, average training loss: 12462.30, base loss: 14380.72
[INFO 2017-06-28 12:01:32,151 main.py:51] epoch 471, training loss: 12255.31, average training loss: 12461.86, base loss: 14384.44
[INFO 2017-06-28 12:01:32,457 main.py:51] epoch 472, training loss: 10060.68, average training loss: 12456.79, base loss: 14378.85
[INFO 2017-06-28 12:01:32,741 main.py:51] epoch 473, training loss: 12081.88, average training loss: 12456.00, base loss: 14380.87
[INFO 2017-06-28 12:01:33,048 main.py:51] epoch 474, training loss: 10969.40, average training loss: 12452.87, base loss: 14380.44
[INFO 2017-06-28 12:01:33,345 main.py:51] epoch 475, training loss: 12171.53, average training loss: 12452.28, base loss: 14383.00
[INFO 2017-06-28 12:01:33,643 main.py:51] epoch 476, training loss: 10196.65, average training loss: 12447.55, base loss: 14378.12
[INFO 2017-06-28 12:01:33,954 main.py:51] epoch 477, training loss: 12267.02, average training loss: 12447.17, base loss: 14379.55
[INFO 2017-06-28 12:01:34,250 main.py:51] epoch 478, training loss: 11748.35, average training loss: 12445.71, base loss: 14380.71
[INFO 2017-06-28 12:01:34,562 main.py:51] epoch 479, training loss: 11250.23, average training loss: 12443.22, base loss: 14382.38
[INFO 2017-06-28 12:01:34,877 main.py:51] epoch 480, training loss: 12907.19, average training loss: 12444.18, base loss: 14383.24
[INFO 2017-06-28 12:01:35,198 main.py:51] epoch 481, training loss: 14107.73, average training loss: 12447.64, base loss: 14388.96
[INFO 2017-06-28 12:01:35,511 main.py:51] epoch 482, training loss: 10816.00, average training loss: 12444.26, base loss: 14387.82
[INFO 2017-06-28 12:01:35,822 main.py:51] epoch 483, training loss: 12014.94, average training loss: 12443.37, base loss: 14390.12
[INFO 2017-06-28 12:01:36,105 main.py:51] epoch 484, training loss: 11210.94, average training loss: 12440.83, base loss: 14389.62
[INFO 2017-06-28 12:01:36,421 main.py:51] epoch 485, training loss: 11604.01, average training loss: 12439.11, base loss: 14390.65
[INFO 2017-06-28 12:01:36,704 main.py:51] epoch 486, training loss: 11263.39, average training loss: 12436.69, base loss: 14391.98
[INFO 2017-06-28 12:01:37,005 main.py:51] epoch 487, training loss: 11166.91, average training loss: 12434.09, base loss: 14391.36
[INFO 2017-06-28 12:01:37,286 main.py:51] epoch 488, training loss: 10633.58, average training loss: 12430.41, base loss: 14389.92
[INFO 2017-06-28 12:01:37,578 main.py:51] epoch 489, training loss: 10984.21, average training loss: 12427.46, base loss: 14388.95
[INFO 2017-06-28 12:01:37,869 main.py:51] epoch 490, training loss: 10521.97, average training loss: 12423.58, base loss: 14386.70
[INFO 2017-06-28 12:01:38,171 main.py:51] epoch 491, training loss: 10451.14, average training loss: 12419.57, base loss: 14384.18
[INFO 2017-06-28 12:01:38,505 main.py:51] epoch 492, training loss: 11213.43, average training loss: 12417.12, base loss: 14384.46
[INFO 2017-06-28 12:01:38,828 main.py:51] epoch 493, training loss: 10687.25, average training loss: 12413.62, base loss: 14382.74
[INFO 2017-06-28 12:01:39,121 main.py:51] epoch 494, training loss: 10433.91, average training loss: 12409.62, base loss: 14380.01
[INFO 2017-06-28 12:01:39,449 main.py:51] epoch 495, training loss: 12887.06, average training loss: 12410.58, base loss: 14384.98
[INFO 2017-06-28 12:01:39,748 main.py:51] epoch 496, training loss: 11419.85, average training loss: 12408.59, base loss: 14387.51
[INFO 2017-06-28 12:01:40,088 main.py:51] epoch 497, training loss: 10128.62, average training loss: 12404.01, base loss: 14385.22
[INFO 2017-06-28 12:01:40,406 main.py:51] epoch 498, training loss: 10404.42, average training loss: 12400.00, base loss: 14381.46
[INFO 2017-06-28 12:01:40,723 main.py:51] epoch 499, training loss: 11215.71, average training loss: 12397.64, base loss: 14382.64
[INFO 2017-06-28 12:01:40,723 main.py:53] epoch 499, testing
[INFO 2017-06-28 12:01:42,348 main.py:105] average testing loss: 12149.91, base loss: 15037.41
[INFO 2017-06-28 12:01:42,348 main.py:106] improve_loss: 2887.50, improve_percent: 0.19
[INFO 2017-06-28 12:01:42,349 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:01:42,355 main.py:76] current best improved percent: 0.19
[INFO 2017-06-28 12:01:42,655 main.py:51] epoch 500, training loss: 10167.25, average training loss: 12393.18, base loss: 14380.71
[INFO 2017-06-28 12:01:42,950 main.py:51] epoch 501, training loss: 11289.21, average training loss: 12390.98, base loss: 14380.50
[INFO 2017-06-28 12:01:43,239 main.py:51] epoch 502, training loss: 10446.06, average training loss: 12387.12, base loss: 14377.98
[INFO 2017-06-28 12:01:43,533 main.py:51] epoch 503, training loss: 13556.34, average training loss: 12389.44, base loss: 14384.33
[INFO 2017-06-28 12:01:43,826 main.py:51] epoch 504, training loss: 10291.21, average training loss: 12385.28, base loss: 14383.34
[INFO 2017-06-28 12:01:44,126 main.py:51] epoch 505, training loss: 11433.03, average training loss: 12383.40, base loss: 14385.10
[INFO 2017-06-28 12:01:44,433 main.py:51] epoch 506, training loss: 10117.85, average training loss: 12378.93, base loss: 14381.93
[INFO 2017-06-28 12:01:44,743 main.py:51] epoch 507, training loss: 10089.81, average training loss: 12374.43, base loss: 14379.86
[INFO 2017-06-28 12:01:45,057 main.py:51] epoch 508, training loss: 10314.59, average training loss: 12370.38, base loss: 14378.57
[INFO 2017-06-28 12:01:45,357 main.py:51] epoch 509, training loss: 10659.86, average training loss: 12367.03, base loss: 14377.90
[INFO 2017-06-28 12:01:45,668 main.py:51] epoch 510, training loss: 10969.41, average training loss: 12364.29, base loss: 14380.07
[INFO 2017-06-28 12:01:45,970 main.py:51] epoch 511, training loss: 11413.04, average training loss: 12362.43, base loss: 14380.89
[INFO 2017-06-28 12:01:46,272 main.py:51] epoch 512, training loss: 10728.34, average training loss: 12359.25, base loss: 14379.05
[INFO 2017-06-28 12:01:46,591 main.py:51] epoch 513, training loss: 10398.99, average training loss: 12355.43, base loss: 14376.66
[INFO 2017-06-28 12:01:46,899 main.py:51] epoch 514, training loss: 12473.27, average training loss: 12355.66, base loss: 14379.83
[INFO 2017-06-28 12:01:47,220 main.py:51] epoch 515, training loss: 10695.66, average training loss: 12352.44, base loss: 14379.31
[INFO 2017-06-28 12:01:47,529 main.py:51] epoch 516, training loss: 12590.90, average training loss: 12352.91, base loss: 14385.08
[INFO 2017-06-28 12:01:47,848 main.py:51] epoch 517, training loss: 11178.23, average training loss: 12350.64, base loss: 14385.16
[INFO 2017-06-28 12:01:48,158 main.py:51] epoch 518, training loss: 11147.53, average training loss: 12348.32, base loss: 14384.40
[INFO 2017-06-28 12:01:48,515 main.py:51] epoch 519, training loss: 11285.85, average training loss: 12346.28, base loss: 14383.60
[INFO 2017-06-28 12:01:48,825 main.py:51] epoch 520, training loss: 11635.28, average training loss: 12344.91, base loss: 14382.54
[INFO 2017-06-28 12:01:49,162 main.py:51] epoch 521, training loss: 11712.14, average training loss: 12343.70, base loss: 14384.66
[INFO 2017-06-28 12:01:49,472 main.py:51] epoch 522, training loss: 11079.84, average training loss: 12341.28, base loss: 14384.51
[INFO 2017-06-28 12:01:49,774 main.py:51] epoch 523, training loss: 10743.33, average training loss: 12338.23, base loss: 14382.07
[INFO 2017-06-28 12:01:50,111 main.py:51] epoch 524, training loss: 11426.95, average training loss: 12336.50, base loss: 14382.81
[INFO 2017-06-28 12:01:50,422 main.py:51] epoch 525, training loss: 11555.15, average training loss: 12335.01, base loss: 14384.94
[INFO 2017-06-28 12:01:50,781 main.py:51] epoch 526, training loss: 10344.43, average training loss: 12331.24, base loss: 14383.57
[INFO 2017-06-28 12:01:51,113 main.py:51] epoch 527, training loss: 10101.40, average training loss: 12327.01, base loss: 14380.57
[INFO 2017-06-28 12:01:51,392 main.py:51] epoch 528, training loss: 13650.42, average training loss: 12329.51, base loss: 14385.25
[INFO 2017-06-28 12:01:51,682 main.py:51] epoch 529, training loss: 11714.22, average training loss: 12328.35, base loss: 14388.04
[INFO 2017-06-28 12:01:51,989 main.py:51] epoch 530, training loss: 12936.24, average training loss: 12329.50, base loss: 14392.66
[INFO 2017-06-28 12:01:52,284 main.py:51] epoch 531, training loss: 11315.72, average training loss: 12327.59, base loss: 14392.64
[INFO 2017-06-28 12:01:52,588 main.py:51] epoch 532, training loss: 11039.25, average training loss: 12325.18, base loss: 14392.12
[INFO 2017-06-28 12:01:52,884 main.py:51] epoch 533, training loss: 13574.39, average training loss: 12327.51, base loss: 14396.24
[INFO 2017-06-28 12:01:53,205 main.py:51] epoch 534, training loss: 11399.87, average training loss: 12325.78, base loss: 14397.42
[INFO 2017-06-28 12:01:53,541 main.py:51] epoch 535, training loss: 10721.97, average training loss: 12322.79, base loss: 14395.93
[INFO 2017-06-28 12:01:53,830 main.py:51] epoch 536, training loss: 10427.25, average training loss: 12319.26, base loss: 14393.74
[INFO 2017-06-28 12:01:54,139 main.py:51] epoch 537, training loss: 10202.84, average training loss: 12315.32, base loss: 14391.24
[INFO 2017-06-28 12:01:54,468 main.py:51] epoch 538, training loss: 11031.07, average training loss: 12312.94, base loss: 14392.17
[INFO 2017-06-28 12:01:54,764 main.py:51] epoch 539, training loss: 10226.70, average training loss: 12309.08, base loss: 14391.46
[INFO 2017-06-28 12:01:55,062 main.py:51] epoch 540, training loss: 12780.08, average training loss: 12309.95, base loss: 14395.04
[INFO 2017-06-28 12:01:55,377 main.py:51] epoch 541, training loss: 11146.73, average training loss: 12307.80, base loss: 14396.29
[INFO 2017-06-28 12:01:55,694 main.py:51] epoch 542, training loss: 11755.82, average training loss: 12306.79, base loss: 14398.89
[INFO 2017-06-28 12:01:56,005 main.py:51] epoch 543, training loss: 9909.96, average training loss: 12302.38, base loss: 14395.73
[INFO 2017-06-28 12:01:56,295 main.py:51] epoch 544, training loss: 10014.70, average training loss: 12298.18, base loss: 14391.40
[INFO 2017-06-28 12:01:56,624 main.py:51] epoch 545, training loss: 10275.68, average training loss: 12294.48, base loss: 14390.95
[INFO 2017-06-28 12:01:56,930 main.py:51] epoch 546, training loss: 10828.53, average training loss: 12291.80, base loss: 14390.35
[INFO 2017-06-28 12:01:57,218 main.py:51] epoch 547, training loss: 11871.30, average training loss: 12291.03, base loss: 14391.97
[INFO 2017-06-28 12:01:57,537 main.py:51] epoch 548, training loss: 9549.40, average training loss: 12286.04, base loss: 14387.80
[INFO 2017-06-28 12:01:57,885 main.py:51] epoch 549, training loss: 9843.80, average training loss: 12281.60, base loss: 14383.60
[INFO 2017-06-28 12:01:58,189 main.py:51] epoch 550, training loss: 10852.83, average training loss: 12279.00, base loss: 14382.52
[INFO 2017-06-28 12:01:58,510 main.py:51] epoch 551, training loss: 11867.19, average training loss: 12278.26, base loss: 14385.09
[INFO 2017-06-28 12:01:58,832 main.py:51] epoch 552, training loss: 11741.20, average training loss: 12277.29, base loss: 14386.60
[INFO 2017-06-28 12:01:59,151 main.py:51] epoch 553, training loss: 11039.23, average training loss: 12275.05, base loss: 14386.66
[INFO 2017-06-28 12:01:59,437 main.py:51] epoch 554, training loss: 10717.40, average training loss: 12272.25, base loss: 14387.57
[INFO 2017-06-28 12:01:59,745 main.py:51] epoch 555, training loss: 12065.18, average training loss: 12271.87, base loss: 14390.61
[INFO 2017-06-28 12:02:00,051 main.py:51] epoch 556, training loss: 11103.91, average training loss: 12269.78, base loss: 14390.27
[INFO 2017-06-28 12:02:00,366 main.py:51] epoch 557, training loss: 11142.65, average training loss: 12267.76, base loss: 14390.84
[INFO 2017-06-28 12:02:00,668 main.py:51] epoch 558, training loss: 9843.62, average training loss: 12263.42, base loss: 14387.34
[INFO 2017-06-28 12:02:00,960 main.py:51] epoch 559, training loss: 11027.21, average training loss: 12261.21, base loss: 14386.84
[INFO 2017-06-28 12:02:01,277 main.py:51] epoch 560, training loss: 11719.07, average training loss: 12260.25, base loss: 14387.88
[INFO 2017-06-28 12:02:01,602 main.py:51] epoch 561, training loss: 11238.70, average training loss: 12258.43, base loss: 14388.44
[INFO 2017-06-28 12:02:01,974 main.py:51] epoch 562, training loss: 12835.23, average training loss: 12259.45, base loss: 14392.80
[INFO 2017-06-28 12:02:02,339 main.py:51] epoch 563, training loss: 10229.56, average training loss: 12255.85, base loss: 14390.66
[INFO 2017-06-28 12:02:02,649 main.py:51] epoch 564, training loss: 11157.83, average training loss: 12253.91, base loss: 14390.27
[INFO 2017-06-28 12:02:02,960 main.py:51] epoch 565, training loss: 9996.45, average training loss: 12249.92, base loss: 14386.32
[INFO 2017-06-28 12:02:03,253 main.py:51] epoch 566, training loss: 10110.31, average training loss: 12246.15, base loss: 14385.16
[INFO 2017-06-28 12:02:03,583 main.py:51] epoch 567, training loss: 11365.57, average training loss: 12244.60, base loss: 14386.02
[INFO 2017-06-28 12:02:03,929 main.py:51] epoch 568, training loss: 11122.88, average training loss: 12242.63, base loss: 14386.00
[INFO 2017-06-28 12:02:04,239 main.py:51] epoch 569, training loss: 10597.32, average training loss: 12239.74, base loss: 14385.14
[INFO 2017-06-28 12:02:04,540 main.py:51] epoch 570, training loss: 9238.31, average training loss: 12234.48, base loss: 14381.01
[INFO 2017-06-28 12:02:04,836 main.py:51] epoch 571, training loss: 9819.00, average training loss: 12230.26, base loss: 14376.46
[INFO 2017-06-28 12:02:05,126 main.py:51] epoch 572, training loss: 10722.64, average training loss: 12227.63, base loss: 14373.64
[INFO 2017-06-28 12:02:05,427 main.py:51] epoch 573, training loss: 10776.04, average training loss: 12225.10, base loss: 14372.43
[INFO 2017-06-28 12:02:05,728 main.py:51] epoch 574, training loss: 12594.15, average training loss: 12225.74, base loss: 14374.63
[INFO 2017-06-28 12:02:06,031 main.py:51] epoch 575, training loss: 11094.98, average training loss: 12223.78, base loss: 14375.83
[INFO 2017-06-28 12:02:06,341 main.py:51] epoch 576, training loss: 11222.73, average training loss: 12222.04, base loss: 14374.04
[INFO 2017-06-28 12:02:06,631 main.py:51] epoch 577, training loss: 11274.31, average training loss: 12220.40, base loss: 14374.55
[INFO 2017-06-28 12:02:06,919 main.py:51] epoch 578, training loss: 11268.05, average training loss: 12218.76, base loss: 14375.14
[INFO 2017-06-28 12:02:07,227 main.py:51] epoch 579, training loss: 9562.40, average training loss: 12214.18, base loss: 14372.09
[INFO 2017-06-28 12:02:07,543 main.py:51] epoch 580, training loss: 11452.58, average training loss: 12212.87, base loss: 14373.19
[INFO 2017-06-28 12:02:07,855 main.py:51] epoch 581, training loss: 10682.24, average training loss: 12210.24, base loss: 14371.40
[INFO 2017-06-28 12:02:08,151 main.py:51] epoch 582, training loss: 10748.52, average training loss: 12207.73, base loss: 14370.19
[INFO 2017-06-28 12:02:08,471 main.py:51] epoch 583, training loss: 9763.55, average training loss: 12203.55, base loss: 14366.34
[INFO 2017-06-28 12:02:08,777 main.py:51] epoch 584, training loss: 9579.01, average training loss: 12199.06, base loss: 14364.33
[INFO 2017-06-28 12:02:09,088 main.py:51] epoch 585, training loss: 13729.55, average training loss: 12201.67, base loss: 14370.31
[INFO 2017-06-28 12:02:09,461 main.py:51] epoch 586, training loss: 11338.93, average training loss: 12200.20, base loss: 14370.57
[INFO 2017-06-28 12:02:09,817 main.py:51] epoch 587, training loss: 12523.05, average training loss: 12200.75, base loss: 14374.14
[INFO 2017-06-28 12:02:10,105 main.py:51] epoch 588, training loss: 12595.93, average training loss: 12201.42, base loss: 14378.61
[INFO 2017-06-28 12:02:10,391 main.py:51] epoch 589, training loss: 10125.17, average training loss: 12197.90, base loss: 14375.59
[INFO 2017-06-28 12:02:10,741 main.py:51] epoch 590, training loss: 11427.45, average training loss: 12196.60, base loss: 14374.95
[INFO 2017-06-28 12:02:11,111 main.py:51] epoch 591, training loss: 11609.74, average training loss: 12195.61, base loss: 14376.05
[INFO 2017-06-28 12:02:11,454 main.py:51] epoch 592, training loss: 11944.68, average training loss: 12195.19, base loss: 14377.99
[INFO 2017-06-28 12:02:11,767 main.py:51] epoch 593, training loss: 11708.92, average training loss: 12194.37, base loss: 14378.47
[INFO 2017-06-28 12:02:12,102 main.py:51] epoch 594, training loss: 10153.20, average training loss: 12190.94, base loss: 14376.43
[INFO 2017-06-28 12:02:12,428 main.py:51] epoch 595, training loss: 11099.10, average training loss: 12189.10, base loss: 14375.60
[INFO 2017-06-28 12:02:12,722 main.py:51] epoch 596, training loss: 11115.25, average training loss: 12187.31, base loss: 14374.91
[INFO 2017-06-28 12:02:13,054 main.py:51] epoch 597, training loss: 11773.83, average training loss: 12186.61, base loss: 14376.13
[INFO 2017-06-28 12:02:13,352 main.py:51] epoch 598, training loss: 14288.28, average training loss: 12190.12, base loss: 14382.59
[INFO 2017-06-28 12:02:13,670 main.py:51] epoch 599, training loss: 12535.19, average training loss: 12190.70, base loss: 14385.14
[INFO 2017-06-28 12:02:13,670 main.py:53] epoch 599, testing
[INFO 2017-06-28 12:02:15,290 main.py:105] average testing loss: 11554.88, base loss: 14304.29
[INFO 2017-06-28 12:02:15,290 main.py:106] improve_loss: 2749.42, improve_percent: 0.19
[INFO 2017-06-28 12:02:15,291 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:02:15,297 main.py:76] current best improved percent: 0.19
[INFO 2017-06-28 12:02:15,597 main.py:51] epoch 600, training loss: 11924.76, average training loss: 12190.26, base loss: 14388.03
[INFO 2017-06-28 12:02:15,891 main.py:51] epoch 601, training loss: 11279.73, average training loss: 12188.74, base loss: 14388.41
[INFO 2017-06-28 12:02:16,220 main.py:51] epoch 602, training loss: 10984.03, average training loss: 12186.74, base loss: 14387.64
[INFO 2017-06-28 12:02:16,515 main.py:51] epoch 603, training loss: 10249.77, average training loss: 12183.54, base loss: 14386.40
[INFO 2017-06-28 12:02:16,824 main.py:51] epoch 604, training loss: 10634.24, average training loss: 12180.98, base loss: 14387.02
[INFO 2017-06-28 12:02:17,152 main.py:51] epoch 605, training loss: 9415.77, average training loss: 12176.41, base loss: 14382.75
[INFO 2017-06-28 12:02:17,468 main.py:51] epoch 606, training loss: 10757.15, average training loss: 12174.08, base loss: 14380.51
[INFO 2017-06-28 12:02:17,777 main.py:51] epoch 607, training loss: 12745.72, average training loss: 12175.02, base loss: 14386.37
[INFO 2017-06-28 12:02:18,100 main.py:51] epoch 608, training loss: 11554.79, average training loss: 12174.00, base loss: 14388.31
[INFO 2017-06-28 12:02:18,449 main.py:51] epoch 609, training loss: 12134.09, average training loss: 12173.93, base loss: 14390.49
[INFO 2017-06-28 12:02:18,841 main.py:51] epoch 610, training loss: 10256.89, average training loss: 12170.79, base loss: 14387.87
[INFO 2017-06-28 12:02:19,154 main.py:51] epoch 611, training loss: 10793.72, average training loss: 12168.54, base loss: 14387.21
[INFO 2017-06-28 12:02:19,453 main.py:51] epoch 612, training loss: 10926.32, average training loss: 12166.52, base loss: 14387.60
[INFO 2017-06-28 12:02:19,771 main.py:51] epoch 613, training loss: 9033.47, average training loss: 12161.42, base loss: 14383.76
[INFO 2017-06-28 12:02:20,090 main.py:51] epoch 614, training loss: 10749.50, average training loss: 12159.12, base loss: 14384.35
[INFO 2017-06-28 12:02:20,394 main.py:51] epoch 615, training loss: 10215.68, average training loss: 12155.96, base loss: 14382.73
[INFO 2017-06-28 12:02:20,711 main.py:51] epoch 616, training loss: 11472.69, average training loss: 12154.86, base loss: 14383.28
[INFO 2017-06-28 12:02:21,017 main.py:51] epoch 617, training loss: 11366.56, average training loss: 12153.58, base loss: 14384.22
[INFO 2017-06-28 12:02:21,333 main.py:51] epoch 618, training loss: 13275.61, average training loss: 12155.39, base loss: 14386.25
[INFO 2017-06-28 12:02:21,642 main.py:51] epoch 619, training loss: 10357.91, average training loss: 12152.50, base loss: 14384.72
[INFO 2017-06-28 12:02:21,959 main.py:51] epoch 620, training loss: 11584.92, average training loss: 12151.58, base loss: 14387.37
[INFO 2017-06-28 12:02:22,262 main.py:51] epoch 621, training loss: 11351.94, average training loss: 12150.30, base loss: 14387.38
[INFO 2017-06-28 12:02:22,571 main.py:51] epoch 622, training loss: 10597.95, average training loss: 12147.80, base loss: 14387.42
[INFO 2017-06-28 12:02:22,881 main.py:51] epoch 623, training loss: 10407.44, average training loss: 12145.01, base loss: 14384.78
[INFO 2017-06-28 12:02:23,165 main.py:51] epoch 624, training loss: 9813.10, average training loss: 12141.28, base loss: 14382.54
[INFO 2017-06-28 12:02:23,498 main.py:51] epoch 625, training loss: 12135.97, average training loss: 12141.28, base loss: 14384.93
[INFO 2017-06-28 12:02:23,847 main.py:51] epoch 626, training loss: 10764.35, average training loss: 12139.08, base loss: 14383.48
[INFO 2017-06-28 12:02:24,175 main.py:51] epoch 627, training loss: 10847.19, average training loss: 12137.02, base loss: 14383.98
[INFO 2017-06-28 12:02:24,486 main.py:51] epoch 628, training loss: 10842.64, average training loss: 12134.96, base loss: 14382.73
[INFO 2017-06-28 12:02:24,846 main.py:51] epoch 629, training loss: 11172.93, average training loss: 12133.44, base loss: 14383.83
[INFO 2017-06-28 12:02:25,193 main.py:51] epoch 630, training loss: 11998.89, average training loss: 12133.22, base loss: 14385.94
[INFO 2017-06-28 12:02:25,495 main.py:51] epoch 631, training loss: 10853.89, average training loss: 12131.20, base loss: 14385.82
[INFO 2017-06-28 12:02:25,844 main.py:51] epoch 632, training loss: 10802.95, average training loss: 12129.10, base loss: 14383.85
[INFO 2017-06-28 12:02:26,129 main.py:51] epoch 633, training loss: 11098.21, average training loss: 12127.48, base loss: 14383.43
[INFO 2017-06-28 12:02:26,435 main.py:51] epoch 634, training loss: 10539.40, average training loss: 12124.97, base loss: 14383.61
[INFO 2017-06-28 12:02:26,732 main.py:51] epoch 635, training loss: 11628.58, average training loss: 12124.19, base loss: 14383.87
[INFO 2017-06-28 12:02:27,064 main.py:51] epoch 636, training loss: 12261.30, average training loss: 12124.41, base loss: 14386.97
[INFO 2017-06-28 12:02:27,362 main.py:51] epoch 637, training loss: 10686.85, average training loss: 12122.16, base loss: 14387.18
[INFO 2017-06-28 12:02:27,673 main.py:51] epoch 638, training loss: 10852.96, average training loss: 12120.17, base loss: 14385.66
[INFO 2017-06-28 12:02:27,984 main.py:51] epoch 639, training loss: 9468.00, average training loss: 12116.03, base loss: 14382.07
[INFO 2017-06-28 12:02:28,285 main.py:51] epoch 640, training loss: 12287.00, average training loss: 12116.29, base loss: 14385.78
[INFO 2017-06-28 12:02:28,617 main.py:51] epoch 641, training loss: 11071.91, average training loss: 12114.67, base loss: 14384.99
[INFO 2017-06-28 12:02:28,915 main.py:51] epoch 642, training loss: 10402.43, average training loss: 12112.00, base loss: 14384.05
[INFO 2017-06-28 12:02:29,220 main.py:51] epoch 643, training loss: 12234.98, average training loss: 12112.19, base loss: 14386.22
[INFO 2017-06-28 12:02:29,537 main.py:51] epoch 644, training loss: 10739.03, average training loss: 12110.06, base loss: 14384.85
[INFO 2017-06-28 12:02:29,869 main.py:51] epoch 645, training loss: 11236.97, average training loss: 12108.71, base loss: 14384.52
[INFO 2017-06-28 12:02:30,184 main.py:51] epoch 646, training loss: 9618.81, average training loss: 12104.86, base loss: 14381.80
[INFO 2017-06-28 12:02:30,554 main.py:51] epoch 647, training loss: 10373.93, average training loss: 12102.19, base loss: 14380.67
[INFO 2017-06-28 12:02:30,903 main.py:51] epoch 648, training loss: 12700.20, average training loss: 12103.12, base loss: 14384.47
[INFO 2017-06-28 12:02:31,209 main.py:51] epoch 649, training loss: 10677.85, average training loss: 12100.92, base loss: 14383.75
[INFO 2017-06-28 12:02:31,509 main.py:51] epoch 650, training loss: 11188.18, average training loss: 12099.52, base loss: 14383.33
[INFO 2017-06-28 12:02:31,829 main.py:51] epoch 651, training loss: 11798.24, average training loss: 12099.06, base loss: 14383.13
[INFO 2017-06-28 12:02:32,138 main.py:51] epoch 652, training loss: 10724.58, average training loss: 12096.95, base loss: 14382.09
[INFO 2017-06-28 12:02:32,464 main.py:51] epoch 653, training loss: 10724.65, average training loss: 12094.86, base loss: 14381.63
[INFO 2017-06-28 12:02:32,770 main.py:51] epoch 654, training loss: 11074.52, average training loss: 12093.30, base loss: 14382.11
[INFO 2017-06-28 12:02:33,076 main.py:51] epoch 655, training loss: 10670.14, average training loss: 12091.13, base loss: 14380.94
[INFO 2017-06-28 12:02:33,381 main.py:51] epoch 656, training loss: 10811.38, average training loss: 12089.18, base loss: 14380.76
[INFO 2017-06-28 12:02:33,688 main.py:51] epoch 657, training loss: 10598.22, average training loss: 12086.91, base loss: 14380.84
[INFO 2017-06-28 12:02:34,001 main.py:51] epoch 658, training loss: 10788.66, average training loss: 12084.94, base loss: 14381.75
[INFO 2017-06-28 12:02:34,368 main.py:51] epoch 659, training loss: 12801.24, average training loss: 12086.03, base loss: 14385.35
[INFO 2017-06-28 12:02:34,753 main.py:51] epoch 660, training loss: 10541.25, average training loss: 12083.69, base loss: 14384.92
[INFO 2017-06-28 12:02:35,051 main.py:51] epoch 661, training loss: 12000.54, average training loss: 12083.57, base loss: 14387.42
[INFO 2017-06-28 12:02:35,357 main.py:51] epoch 662, training loss: 10876.03, average training loss: 12081.75, base loss: 14388.10
[INFO 2017-06-28 12:02:35,750 main.py:51] epoch 663, training loss: 10420.32, average training loss: 12079.24, base loss: 14386.30
[INFO 2017-06-28 12:02:36,124 main.py:51] epoch 664, training loss: 10500.52, average training loss: 12076.87, base loss: 14386.23
[INFO 2017-06-28 12:02:36,483 main.py:51] epoch 665, training loss: 11279.97, average training loss: 12075.67, base loss: 14387.23
[INFO 2017-06-28 12:02:36,817 main.py:51] epoch 666, training loss: 12109.82, average training loss: 12075.72, base loss: 14390.34
[INFO 2017-06-28 12:02:37,237 main.py:51] epoch 667, training loss: 11452.36, average training loss: 12074.79, base loss: 14391.70
[INFO 2017-06-28 12:02:37,611 main.py:51] epoch 668, training loss: 10872.01, average training loss: 12072.99, base loss: 14391.52
[INFO 2017-06-28 12:02:38,032 main.py:51] epoch 669, training loss: 11816.20, average training loss: 12072.61, base loss: 14392.80
[INFO 2017-06-28 12:02:38,456 main.py:51] epoch 670, training loss: 10947.27, average training loss: 12070.93, base loss: 14393.23
[INFO 2017-06-28 12:02:38,808 main.py:51] epoch 671, training loss: 10107.08, average training loss: 12068.01, base loss: 14389.44
[INFO 2017-06-28 12:02:39,108 main.py:51] epoch 672, training loss: 9798.55, average training loss: 12064.64, base loss: 14387.53
[INFO 2017-06-28 12:02:39,446 main.py:51] epoch 673, training loss: 12370.25, average training loss: 12065.09, base loss: 14389.20
[INFO 2017-06-28 12:02:39,780 main.py:51] epoch 674, training loss: 10795.99, average training loss: 12063.21, base loss: 14387.61
[INFO 2017-06-28 12:02:40,095 main.py:51] epoch 675, training loss: 11365.27, average training loss: 12062.18, base loss: 14388.09
[INFO 2017-06-28 12:02:40,437 main.py:51] epoch 676, training loss: 11113.25, average training loss: 12060.78, base loss: 14388.76
[INFO 2017-06-28 12:02:40,734 main.py:51] epoch 677, training loss: 9828.44, average training loss: 12057.48, base loss: 14386.76
[INFO 2017-06-28 12:02:41,054 main.py:51] epoch 678, training loss: 11680.47, average training loss: 12056.93, base loss: 14387.12
[INFO 2017-06-28 12:02:41,347 main.py:51] epoch 679, training loss: 11737.78, average training loss: 12056.46, base loss: 14389.17
[INFO 2017-06-28 12:02:41,686 main.py:51] epoch 680, training loss: 11083.64, average training loss: 12055.03, base loss: 14387.56
[INFO 2017-06-28 12:02:42,012 main.py:51] epoch 681, training loss: 11925.69, average training loss: 12054.84, base loss: 14388.56
[INFO 2017-06-28 12:02:42,358 main.py:51] epoch 682, training loss: 10727.38, average training loss: 12052.90, base loss: 14389.35
[INFO 2017-06-28 12:02:42,688 main.py:51] epoch 683, training loss: 10923.78, average training loss: 12051.25, base loss: 14389.28
[INFO 2017-06-28 12:02:43,003 main.py:51] epoch 684, training loss: 11266.64, average training loss: 12050.10, base loss: 14390.04
[INFO 2017-06-28 12:02:43,320 main.py:51] epoch 685, training loss: 12850.26, average training loss: 12051.27, base loss: 14394.64
[INFO 2017-06-28 12:02:43,634 main.py:51] epoch 686, training loss: 10340.76, average training loss: 12048.78, base loss: 14392.59
[INFO 2017-06-28 12:02:43,965 main.py:51] epoch 687, training loss: 11682.50, average training loss: 12048.25, base loss: 14393.80
[INFO 2017-06-28 12:02:44,277 main.py:51] epoch 688, training loss: 10625.97, average training loss: 12046.18, base loss: 14393.60
[INFO 2017-06-28 12:02:44,589 main.py:51] epoch 689, training loss: 10562.48, average training loss: 12044.03, base loss: 14393.35
[INFO 2017-06-28 12:02:44,901 main.py:51] epoch 690, training loss: 10997.22, average training loss: 12042.52, base loss: 14392.82
[INFO 2017-06-28 12:02:45,217 main.py:51] epoch 691, training loss: 11288.88, average training loss: 12041.43, base loss: 14392.75
[INFO 2017-06-28 12:02:45,538 main.py:51] epoch 692, training loss: 11659.14, average training loss: 12040.88, base loss: 14396.60
[INFO 2017-06-28 12:02:45,848 main.py:51] epoch 693, training loss: 10780.24, average training loss: 12039.06, base loss: 14395.73
[INFO 2017-06-28 12:02:46,255 main.py:51] epoch 694, training loss: 13159.32, average training loss: 12040.67, base loss: 14400.58
[INFO 2017-06-28 12:02:46,627 main.py:51] epoch 695, training loss: 10442.06, average training loss: 12038.37, base loss: 14401.15
[INFO 2017-06-28 12:02:46,962 main.py:51] epoch 696, training loss: 10728.23, average training loss: 12036.49, base loss: 14400.41
[INFO 2017-06-28 12:02:47,370 main.py:51] epoch 697, training loss: 11121.03, average training loss: 12035.18, base loss: 14399.79
[INFO 2017-06-28 12:02:47,816 main.py:51] epoch 698, training loss: 11179.98, average training loss: 12033.96, base loss: 14401.18
[INFO 2017-06-28 12:02:48,136 main.py:51] epoch 699, training loss: 10506.92, average training loss: 12031.78, base loss: 14399.62
[INFO 2017-06-28 12:02:48,137 main.py:53] epoch 699, testing
[INFO 2017-06-28 12:02:49,859 main.py:105] average testing loss: 12474.95, base loss: 15631.12
[INFO 2017-06-28 12:02:49,859 main.py:106] improve_loss: 3156.17, improve_percent: 0.20
[INFO 2017-06-28 12:02:49,860 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:02:49,866 main.py:76] current best improved percent: 0.20
[INFO 2017-06-28 12:02:50,204 main.py:51] epoch 700, training loss: 12005.72, average training loss: 12031.74, base loss: 14402.10
[INFO 2017-06-28 12:02:50,525 main.py:51] epoch 701, training loss: 13057.50, average training loss: 12033.20, base loss: 14405.49
[INFO 2017-06-28 12:02:50,831 main.py:51] epoch 702, training loss: 10395.70, average training loss: 12030.87, base loss: 14404.69
[INFO 2017-06-28 12:02:51,143 main.py:51] epoch 703, training loss: 10375.35, average training loss: 12028.52, base loss: 14404.04
[INFO 2017-06-28 12:02:51,464 main.py:51] epoch 704, training loss: 10716.57, average training loss: 12026.66, base loss: 14403.44
[INFO 2017-06-28 12:02:51,768 main.py:51] epoch 705, training loss: 11467.75, average training loss: 12025.87, base loss: 14404.05
[INFO 2017-06-28 12:02:52,061 main.py:51] epoch 706, training loss: 9596.37, average training loss: 12022.43, base loss: 14401.70
[INFO 2017-06-28 12:02:52,359 main.py:51] epoch 707, training loss: 10720.99, average training loss: 12020.59, base loss: 14400.65
[INFO 2017-06-28 12:02:52,647 main.py:51] epoch 708, training loss: 9562.04, average training loss: 12017.13, base loss: 14397.51
[INFO 2017-06-28 12:02:52,993 main.py:51] epoch 709, training loss: 11753.58, average training loss: 12016.76, base loss: 14398.47
[INFO 2017-06-28 12:02:53,293 main.py:51] epoch 710, training loss: 11410.09, average training loss: 12015.90, base loss: 14398.19
[INFO 2017-06-28 12:02:53,594 main.py:51] epoch 711, training loss: 11839.10, average training loss: 12015.65, base loss: 14400.17
[INFO 2017-06-28 12:02:53,922 main.py:51] epoch 712, training loss: 10405.27, average training loss: 12013.40, base loss: 14399.83
[INFO 2017-06-28 12:02:54,251 main.py:51] epoch 713, training loss: 10425.76, average training loss: 12011.17, base loss: 14397.50
[INFO 2017-06-28 12:02:54,582 main.py:51] epoch 714, training loss: 11802.60, average training loss: 12010.88, base loss: 14399.51
[INFO 2017-06-28 12:02:54,903 main.py:51] epoch 715, training loss: 11722.13, average training loss: 12010.48, base loss: 14401.84
[INFO 2017-06-28 12:02:55,224 main.py:51] epoch 716, training loss: 11084.63, average training loss: 12009.19, base loss: 14401.28
[INFO 2017-06-28 12:02:55,521 main.py:51] epoch 717, training loss: 12234.28, average training loss: 12009.50, base loss: 14403.92
[INFO 2017-06-28 12:02:55,860 main.py:51] epoch 718, training loss: 11952.86, average training loss: 12009.42, base loss: 14406.10
[INFO 2017-06-28 12:02:56,172 main.py:51] epoch 719, training loss: 9621.50, average training loss: 12006.10, base loss: 14402.98
[INFO 2017-06-28 12:02:56,480 main.py:51] epoch 720, training loss: 12598.57, average training loss: 12006.93, base loss: 14404.49
[INFO 2017-06-28 12:02:56,778 main.py:51] epoch 721, training loss: 11634.03, average training loss: 12006.41, base loss: 14406.27
[INFO 2017-06-28 12:02:57,116 main.py:51] epoch 722, training loss: 11019.60, average training loss: 12005.04, base loss: 14406.20
[INFO 2017-06-28 12:02:57,491 main.py:51] epoch 723, training loss: 11220.78, average training loss: 12003.96, base loss: 14407.88
[INFO 2017-06-28 12:02:57,804 main.py:51] epoch 724, training loss: 10937.26, average training loss: 12002.49, base loss: 14407.56
[INFO 2017-06-28 12:02:58,119 main.py:51] epoch 725, training loss: 10527.93, average training loss: 12000.46, base loss: 14405.91
[INFO 2017-06-28 12:02:58,435 main.py:51] epoch 726, training loss: 10644.83, average training loss: 11998.59, base loss: 14405.53
[INFO 2017-06-28 12:02:58,772 main.py:51] epoch 727, training loss: 12568.99, average training loss: 11999.38, base loss: 14407.28
[INFO 2017-06-28 12:02:59,095 main.py:51] epoch 728, training loss: 13418.13, average training loss: 12001.32, base loss: 14411.25
[INFO 2017-06-28 12:02:59,428 main.py:51] epoch 729, training loss: 10772.45, average training loss: 11999.64, base loss: 14411.52
[INFO 2017-06-28 12:02:59,767 main.py:51] epoch 730, training loss: 9371.65, average training loss: 11996.04, base loss: 14407.30
[INFO 2017-06-28 12:03:00,090 main.py:51] epoch 731, training loss: 9897.45, average training loss: 11993.18, base loss: 14404.59
[INFO 2017-06-28 12:03:00,432 main.py:51] epoch 732, training loss: 10131.01, average training loss: 11990.64, base loss: 14402.32
[INFO 2017-06-28 12:03:00,779 main.py:51] epoch 733, training loss: 13074.11, average training loss: 11992.11, base loss: 14406.36
[INFO 2017-06-28 12:03:01,096 main.py:51] epoch 734, training loss: 10037.53, average training loss: 11989.45, base loss: 14404.92
[INFO 2017-06-28 12:03:01,454 main.py:51] epoch 735, training loss: 10752.47, average training loss: 11987.77, base loss: 14404.24
[INFO 2017-06-28 12:03:01,787 main.py:51] epoch 736, training loss: 9310.94, average training loss: 11984.14, base loss: 14401.77
[INFO 2017-06-28 12:03:02,110 main.py:51] epoch 737, training loss: 13016.32, average training loss: 11985.54, base loss: 14405.71
[INFO 2017-06-28 12:03:02,435 main.py:51] epoch 738, training loss: 11382.12, average training loss: 11984.72, base loss: 14406.88
[INFO 2017-06-28 12:03:02,782 main.py:51] epoch 739, training loss: 9952.18, average training loss: 11981.98, base loss: 14405.55
[INFO 2017-06-28 12:03:03,091 main.py:51] epoch 740, training loss: 9650.21, average training loss: 11978.83, base loss: 14403.07
[INFO 2017-06-28 12:03:03,420 main.py:51] epoch 741, training loss: 11692.66, average training loss: 11978.44, base loss: 14404.67
[INFO 2017-06-28 12:03:03,749 main.py:51] epoch 742, training loss: 11145.06, average training loss: 11977.32, base loss: 14404.99
[INFO 2017-06-28 12:03:04,064 main.py:51] epoch 743, training loss: 11274.77, average training loss: 11976.38, base loss: 14406.12
[INFO 2017-06-28 12:03:04,389 main.py:51] epoch 744, training loss: 10550.08, average training loss: 11974.46, base loss: 14405.22
[INFO 2017-06-28 12:03:04,714 main.py:51] epoch 745, training loss: 10509.04, average training loss: 11972.50, base loss: 14404.00
[INFO 2017-06-28 12:03:05,031 main.py:51] epoch 746, training loss: 10796.90, average training loss: 11970.93, base loss: 14403.51
[INFO 2017-06-28 12:03:05,331 main.py:51] epoch 747, training loss: 11187.44, average training loss: 11969.88, base loss: 14404.69
[INFO 2017-06-28 12:03:05,640 main.py:51] epoch 748, training loss: 11310.89, average training loss: 11969.00, base loss: 14405.73
[INFO 2017-06-28 12:03:05,952 main.py:51] epoch 749, training loss: 12119.00, average training loss: 11969.20, base loss: 14406.66
[INFO 2017-06-28 12:03:06,278 main.py:51] epoch 750, training loss: 10035.62, average training loss: 11966.62, base loss: 14404.29
[INFO 2017-06-28 12:03:06,609 main.py:51] epoch 751, training loss: 10453.44, average training loss: 11964.61, base loss: 14402.99
[INFO 2017-06-28 12:03:06,945 main.py:51] epoch 752, training loss: 10369.66, average training loss: 11962.49, base loss: 14402.16
[INFO 2017-06-28 12:03:07,266 main.py:51] epoch 753, training loss: 9321.02, average training loss: 11958.99, base loss: 14398.94
[INFO 2017-06-28 12:03:07,624 main.py:51] epoch 754, training loss: 10085.62, average training loss: 11956.51, base loss: 14397.01
[INFO 2017-06-28 12:03:07,945 main.py:51] epoch 755, training loss: 10577.28, average training loss: 11954.68, base loss: 14395.55
[INFO 2017-06-28 12:03:08,254 main.py:51] epoch 756, training loss: 12155.16, average training loss: 11954.95, base loss: 14397.35
[INFO 2017-06-28 12:03:08,585 main.py:51] epoch 757, training loss: 10273.23, average training loss: 11952.73, base loss: 14395.58
[INFO 2017-06-28 12:03:08,888 main.py:51] epoch 758, training loss: 9840.83, average training loss: 11949.95, base loss: 14393.00
[INFO 2017-06-28 12:03:09,211 main.py:51] epoch 759, training loss: 9967.08, average training loss: 11947.34, base loss: 14390.99
[INFO 2017-06-28 12:03:09,546 main.py:51] epoch 760, training loss: 12146.77, average training loss: 11947.60, base loss: 14393.40
[INFO 2017-06-28 12:03:09,870 main.py:51] epoch 761, training loss: 11092.39, average training loss: 11946.48, base loss: 14394.14
[INFO 2017-06-28 12:03:10,168 main.py:51] epoch 762, training loss: 10492.26, average training loss: 11944.57, base loss: 14393.16
[INFO 2017-06-28 12:03:10,483 main.py:51] epoch 763, training loss: 9580.50, average training loss: 11941.48, base loss: 14391.39
[INFO 2017-06-28 12:03:10,797 main.py:51] epoch 764, training loss: 10224.37, average training loss: 11939.23, base loss: 14390.95
[INFO 2017-06-28 12:03:11,091 main.py:51] epoch 765, training loss: 9687.69, average training loss: 11936.30, base loss: 14388.24
[INFO 2017-06-28 12:03:11,420 main.py:51] epoch 766, training loss: 10809.73, average training loss: 11934.83, base loss: 14388.10
[INFO 2017-06-28 12:03:11,741 main.py:51] epoch 767, training loss: 10667.93, average training loss: 11933.18, base loss: 14388.53
[INFO 2017-06-28 12:03:12,108 main.py:51] epoch 768, training loss: 10902.03, average training loss: 11931.84, base loss: 14389.40
[INFO 2017-06-28 12:03:12,425 main.py:51] epoch 769, training loss: 10771.94, average training loss: 11930.33, base loss: 14389.97
[INFO 2017-06-28 12:03:12,759 main.py:51] epoch 770, training loss: 10448.71, average training loss: 11928.41, base loss: 14389.34
[INFO 2017-06-28 12:03:13,103 main.py:51] epoch 771, training loss: 11992.50, average training loss: 11928.49, base loss: 14390.58
[INFO 2017-06-28 12:03:13,419 main.py:51] epoch 772, training loss: 10890.87, average training loss: 11927.15, base loss: 14390.05
[INFO 2017-06-28 12:03:13,731 main.py:51] epoch 773, training loss: 10728.24, average training loss: 11925.60, base loss: 14390.44
[INFO 2017-06-28 12:03:14,074 main.py:51] epoch 774, training loss: 11177.20, average training loss: 11924.63, base loss: 14390.51
[INFO 2017-06-28 12:03:14,394 main.py:51] epoch 775, training loss: 11658.06, average training loss: 11924.29, base loss: 14393.17
[INFO 2017-06-28 12:03:14,714 main.py:51] epoch 776, training loss: 11922.01, average training loss: 11924.29, base loss: 14395.38
[INFO 2017-06-28 12:03:15,017 main.py:51] epoch 777, training loss: 10399.92, average training loss: 11922.33, base loss: 14394.68
[INFO 2017-06-28 12:03:15,330 main.py:51] epoch 778, training loss: 10895.85, average training loss: 11921.01, base loss: 14395.24
[INFO 2017-06-28 12:03:15,647 main.py:51] epoch 779, training loss: 10332.67, average training loss: 11918.97, base loss: 14394.19
[INFO 2017-06-28 12:03:15,964 main.py:51] epoch 780, training loss: 10355.56, average training loss: 11916.97, base loss: 14393.55
[INFO 2017-06-28 12:03:16,293 main.py:51] epoch 781, training loss: 11481.16, average training loss: 11916.41, base loss: 14393.63
[INFO 2017-06-28 12:03:16,646 main.py:51] epoch 782, training loss: 11382.90, average training loss: 11915.73, base loss: 14394.46
[INFO 2017-06-28 12:03:17,033 main.py:51] epoch 783, training loss: 11729.67, average training loss: 11915.50, base loss: 14394.55
[INFO 2017-06-28 12:03:17,362 main.py:51] epoch 784, training loss: 11615.22, average training loss: 11915.11, base loss: 14396.26
[INFO 2017-06-28 12:03:17,704 main.py:51] epoch 785, training loss: 11361.21, average training loss: 11914.41, base loss: 14396.94
[INFO 2017-06-28 12:03:18,032 main.py:51] epoch 786, training loss: 8947.15, average training loss: 11910.64, base loss: 14394.27
[INFO 2017-06-28 12:03:18,378 main.py:51] epoch 787, training loss: 9498.50, average training loss: 11907.58, base loss: 14392.26
[INFO 2017-06-28 12:03:18,703 main.py:51] epoch 788, training loss: 10291.33, average training loss: 11905.53, base loss: 14391.14
[INFO 2017-06-28 12:03:19,019 main.py:51] epoch 789, training loss: 10743.08, average training loss: 11904.06, base loss: 14392.20
[INFO 2017-06-28 12:03:19,347 main.py:51] epoch 790, training loss: 10642.45, average training loss: 11902.46, base loss: 14392.23
[INFO 2017-06-28 12:03:19,647 main.py:51] epoch 791, training loss: 11357.98, average training loss: 11901.78, base loss: 14394.47
[INFO 2017-06-28 12:03:19,970 main.py:51] epoch 792, training loss: 12524.14, average training loss: 11902.56, base loss: 14396.58
[INFO 2017-06-28 12:03:20,297 main.py:51] epoch 793, training loss: 10233.83, average training loss: 11900.46, base loss: 14394.96
[INFO 2017-06-28 12:03:20,613 main.py:51] epoch 794, training loss: 10885.43, average training loss: 11899.18, base loss: 14396.87
[INFO 2017-06-28 12:03:20,958 main.py:51] epoch 795, training loss: 10052.22, average training loss: 11896.86, base loss: 14395.75
[INFO 2017-06-28 12:03:21,269 main.py:51] epoch 796, training loss: 11016.29, average training loss: 11895.76, base loss: 14394.94
[INFO 2017-06-28 12:03:21,695 main.py:51] epoch 797, training loss: 9949.98, average training loss: 11893.32, base loss: 14392.99
[INFO 2017-06-28 12:03:22,053 main.py:51] epoch 798, training loss: 10587.24, average training loss: 11891.68, base loss: 14392.87
[INFO 2017-06-28 12:03:22,368 main.py:51] epoch 799, training loss: 10506.71, average training loss: 11889.95, base loss: 14391.82
[INFO 2017-06-28 12:03:22,368 main.py:53] epoch 799, testing
[INFO 2017-06-28 12:03:24,150 main.py:105] average testing loss: 12419.99, base loss: 15724.86
[INFO 2017-06-28 12:03:24,150 main.py:106] improve_loss: 3304.88, improve_percent: 0.21
[INFO 2017-06-28 12:03:24,151 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:03:24,157 main.py:76] current best improved percent: 0.21
[INFO 2017-06-28 12:03:24,470 main.py:51] epoch 800, training loss: 11421.28, average training loss: 11889.37, base loss: 14392.35
[INFO 2017-06-28 12:03:24,820 main.py:51] epoch 801, training loss: 9447.57, average training loss: 11886.32, base loss: 14390.09
[INFO 2017-06-28 12:03:25,223 main.py:51] epoch 802, training loss: 10001.89, average training loss: 11883.98, base loss: 14389.78
[INFO 2017-06-28 12:03:25,601 main.py:51] epoch 803, training loss: 11831.57, average training loss: 11883.91, base loss: 14391.07
[INFO 2017-06-28 12:03:25,994 main.py:51] epoch 804, training loss: 10875.10, average training loss: 11882.66, base loss: 14391.30
[INFO 2017-06-28 12:03:26,346 main.py:51] epoch 805, training loss: 12201.94, average training loss: 11883.05, base loss: 14393.70
[INFO 2017-06-28 12:03:26,696 main.py:51] epoch 806, training loss: 9316.50, average training loss: 11879.87, base loss: 14390.66
[INFO 2017-06-28 12:03:27,023 main.py:51] epoch 807, training loss: 10962.76, average training loss: 11878.74, base loss: 14390.90
[INFO 2017-06-28 12:03:27,341 main.py:51] epoch 808, training loss: 10992.40, average training loss: 11877.64, base loss: 14391.41
[INFO 2017-06-28 12:03:27,654 main.py:51] epoch 809, training loss: 9414.98, average training loss: 11874.60, base loss: 14389.72
[INFO 2017-06-28 12:03:27,958 main.py:51] epoch 810, training loss: 9960.23, average training loss: 11872.24, base loss: 14388.54
[INFO 2017-06-28 12:03:28,263 main.py:51] epoch 811, training loss: 11642.69, average training loss: 11871.96, base loss: 14390.46
[INFO 2017-06-28 12:03:28,567 main.py:51] epoch 812, training loss: 9904.67, average training loss: 11869.54, base loss: 14388.67
[INFO 2017-06-28 12:03:28,903 main.py:51] epoch 813, training loss: 12426.32, average training loss: 11870.22, base loss: 14391.65
[INFO 2017-06-28 12:03:29,232 main.py:51] epoch 814, training loss: 10419.01, average training loss: 11868.44, base loss: 14391.53
[INFO 2017-06-28 12:03:29,585 main.py:51] epoch 815, training loss: 10076.98, average training loss: 11866.25, base loss: 14388.56
[INFO 2017-06-28 12:03:29,917 main.py:51] epoch 816, training loss: 10394.36, average training loss: 11864.45, base loss: 14387.82
[INFO 2017-06-28 12:03:30,240 main.py:51] epoch 817, training loss: 12019.15, average training loss: 11864.63, base loss: 14390.28
[INFO 2017-06-28 12:03:30,547 main.py:51] epoch 818, training loss: 10510.86, average training loss: 11862.98, base loss: 14388.88
[INFO 2017-06-28 12:03:30,892 main.py:51] epoch 819, training loss: 10415.04, average training loss: 11861.22, base loss: 14387.91
[INFO 2017-06-28 12:03:31,216 main.py:51] epoch 820, training loss: 11012.84, average training loss: 11860.18, base loss: 14388.13
[INFO 2017-06-28 12:03:31,541 main.py:51] epoch 821, training loss: 11458.43, average training loss: 11859.69, base loss: 14388.81
[INFO 2017-06-28 12:03:31,886 main.py:51] epoch 822, training loss: 10052.89, average training loss: 11857.50, base loss: 14387.81
[INFO 2017-06-28 12:03:32,208 main.py:51] epoch 823, training loss: 10379.95, average training loss: 11855.71, base loss: 14387.25
[INFO 2017-06-28 12:03:32,542 main.py:51] epoch 824, training loss: 11057.46, average training loss: 11854.74, base loss: 14387.15
[INFO 2017-06-28 12:03:32,873 main.py:51] epoch 825, training loss: 10434.53, average training loss: 11853.02, base loss: 14386.10
[INFO 2017-06-28 12:03:33,195 main.py:51] epoch 826, training loss: 10161.15, average training loss: 11850.97, base loss: 14384.82
[INFO 2017-06-28 12:03:33,524 main.py:51] epoch 827, training loss: 11047.14, average training loss: 11850.00, base loss: 14386.21
[INFO 2017-06-28 12:03:33,852 main.py:51] epoch 828, training loss: 10241.70, average training loss: 11848.06, base loss: 14386.19
[INFO 2017-06-28 12:03:34,200 main.py:51] epoch 829, training loss: 10130.43, average training loss: 11845.99, base loss: 14385.76
[INFO 2017-06-28 12:03:34,518 main.py:51] epoch 830, training loss: 10779.66, average training loss: 11844.71, base loss: 14386.43
[INFO 2017-06-28 12:03:34,856 main.py:51] epoch 831, training loss: 11163.97, average training loss: 11843.89, base loss: 14386.97
[INFO 2017-06-28 12:03:35,185 main.py:51] epoch 832, training loss: 9728.02, average training loss: 11841.35, base loss: 14385.25
[INFO 2017-06-28 12:03:35,530 main.py:51] epoch 833, training loss: 9971.39, average training loss: 11839.11, base loss: 14384.33
[INFO 2017-06-28 12:03:35,878 main.py:51] epoch 834, training loss: 11090.88, average training loss: 11838.21, base loss: 14384.80
[INFO 2017-06-28 12:03:36,208 main.py:51] epoch 835, training loss: 10699.86, average training loss: 11836.85, base loss: 14384.75
[INFO 2017-06-28 12:03:36,549 main.py:51] epoch 836, training loss: 9740.45, average training loss: 11834.35, base loss: 14383.54
[INFO 2017-06-28 12:03:36,883 main.py:51] epoch 837, training loss: 11136.38, average training loss: 11833.51, base loss: 14385.56
[INFO 2017-06-28 12:03:37,247 main.py:51] epoch 838, training loss: 10094.29, average training loss: 11831.44, base loss: 14384.01
[INFO 2017-06-28 12:03:37,618 main.py:51] epoch 839, training loss: 9429.75, average training loss: 11828.58, base loss: 14381.59
[INFO 2017-06-28 12:03:37,942 main.py:51] epoch 840, training loss: 11487.95, average training loss: 11828.18, base loss: 14383.28
[INFO 2017-06-28 12:03:38,255 main.py:51] epoch 841, training loss: 11043.65, average training loss: 11827.24, base loss: 14383.65
[INFO 2017-06-28 12:03:38,584 main.py:51] epoch 842, training loss: 10562.96, average training loss: 11825.74, base loss: 14383.72
[INFO 2017-06-28 12:03:38,918 main.py:51] epoch 843, training loss: 10640.68, average training loss: 11824.34, base loss: 14383.50
[INFO 2017-06-28 12:03:39,269 main.py:51] epoch 844, training loss: 9833.50, average training loss: 11821.98, base loss: 14382.73
[INFO 2017-06-28 12:03:39,605 main.py:51] epoch 845, training loss: 10529.63, average training loss: 11820.46, base loss: 14382.46
[INFO 2017-06-28 12:03:39,939 main.py:51] epoch 846, training loss: 9996.53, average training loss: 11818.30, base loss: 14381.28
[INFO 2017-06-28 12:03:40,263 main.py:51] epoch 847, training loss: 11198.96, average training loss: 11817.57, base loss: 14381.45
[INFO 2017-06-28 12:03:40,579 main.py:51] epoch 848, training loss: 10565.57, average training loss: 11816.10, base loss: 14381.05
[INFO 2017-06-28 12:03:40,902 main.py:51] epoch 849, training loss: 10337.90, average training loss: 11814.36, base loss: 14379.23
[INFO 2017-06-28 12:03:41,213 main.py:51] epoch 850, training loss: 11399.83, average training loss: 11813.87, base loss: 14380.76
[INFO 2017-06-28 12:03:41,541 main.py:51] epoch 851, training loss: 10813.57, average training loss: 11812.70, base loss: 14380.71
[INFO 2017-06-28 12:03:41,854 main.py:51] epoch 852, training loss: 9923.70, average training loss: 11810.48, base loss: 14379.31
[INFO 2017-06-28 12:03:42,188 main.py:51] epoch 853, training loss: 11868.09, average training loss: 11810.55, base loss: 14381.32
[INFO 2017-06-28 12:03:42,561 main.py:51] epoch 854, training loss: 10120.04, average training loss: 11808.57, base loss: 14379.91
[INFO 2017-06-28 12:03:42,870 main.py:51] epoch 855, training loss: 12777.59, average training loss: 11809.71, base loss: 14383.55
[INFO 2017-06-28 12:03:43,209 main.py:51] epoch 856, training loss: 10517.56, average training loss: 11808.20, base loss: 14382.75
[INFO 2017-06-28 12:03:43,552 main.py:51] epoch 857, training loss: 11617.68, average training loss: 11807.98, base loss: 14385.44
[INFO 2017-06-28 12:03:43,896 main.py:51] epoch 858, training loss: 11103.82, average training loss: 11807.16, base loss: 14387.27
[INFO 2017-06-28 12:03:44,241 main.py:51] epoch 859, training loss: 9907.32, average training loss: 11804.95, base loss: 14386.98
[INFO 2017-06-28 12:03:44,568 main.py:51] epoch 860, training loss: 10164.16, average training loss: 11803.04, base loss: 14384.68
[INFO 2017-06-28 12:03:44,934 main.py:51] epoch 861, training loss: 11055.01, average training loss: 11802.17, base loss: 14384.53
[INFO 2017-06-28 12:03:45,322 main.py:51] epoch 862, training loss: 13161.37, average training loss: 11803.75, base loss: 14387.87
[INFO 2017-06-28 12:03:45,672 main.py:51] epoch 863, training loss: 12394.57, average training loss: 11804.43, base loss: 14390.15
[INFO 2017-06-28 12:03:46,001 main.py:51] epoch 864, training loss: 10675.77, average training loss: 11803.13, base loss: 14390.21
[INFO 2017-06-28 12:03:46,369 main.py:51] epoch 865, training loss: 9288.56, average training loss: 11800.22, base loss: 14387.67
[INFO 2017-06-28 12:03:46,759 main.py:51] epoch 866, training loss: 10547.47, average training loss: 11798.78, base loss: 14386.91
[INFO 2017-06-28 12:03:47,137 main.py:51] epoch 867, training loss: 10712.32, average training loss: 11797.53, base loss: 14386.19
[INFO 2017-06-28 12:03:47,466 main.py:51] epoch 868, training loss: 9768.38, average training loss: 11795.19, base loss: 14384.66
[INFO 2017-06-28 12:03:47,826 main.py:51] epoch 869, training loss: 9550.55, average training loss: 11792.61, base loss: 14381.57
[INFO 2017-06-28 12:03:48,175 main.py:51] epoch 870, training loss: 10491.39, average training loss: 11791.12, base loss: 14381.66
[INFO 2017-06-28 12:03:48,580 main.py:51] epoch 871, training loss: 10084.40, average training loss: 11789.16, base loss: 14380.47
[INFO 2017-06-28 12:03:48,970 main.py:51] epoch 872, training loss: 12240.19, average training loss: 11789.68, base loss: 14382.87
[INFO 2017-06-28 12:03:49,303 main.py:51] epoch 873, training loss: 8812.49, average training loss: 11786.27, base loss: 14380.78
[INFO 2017-06-28 12:03:49,629 main.py:51] epoch 874, training loss: 11422.20, average training loss: 11785.86, base loss: 14382.09
[INFO 2017-06-28 12:03:49,954 main.py:51] epoch 875, training loss: 12627.74, average training loss: 11786.82, base loss: 14384.66
[INFO 2017-06-28 12:03:50,315 main.py:51] epoch 876, training loss: 12641.37, average training loss: 11787.79, base loss: 14386.84
[INFO 2017-06-28 12:03:50,712 main.py:51] epoch 877, training loss: 11481.75, average training loss: 11787.44, base loss: 14385.45
[INFO 2017-06-28 12:03:51,042 main.py:51] epoch 878, training loss: 10702.25, average training loss: 11786.21, base loss: 14384.21
[INFO 2017-06-28 12:03:51,373 main.py:51] epoch 879, training loss: 9806.82, average training loss: 11783.96, base loss: 14382.20
[INFO 2017-06-28 12:03:51,716 main.py:51] epoch 880, training loss: 10594.38, average training loss: 11782.61, base loss: 14381.87
[INFO 2017-06-28 12:03:52,070 main.py:51] epoch 881, training loss: 11378.55, average training loss: 11782.15, base loss: 14381.50
[INFO 2017-06-28 12:03:52,443 main.py:51] epoch 882, training loss: 12321.10, average training loss: 11782.76, base loss: 14383.73
[INFO 2017-06-28 12:03:52,790 main.py:51] epoch 883, training loss: 10109.96, average training loss: 11780.87, base loss: 14383.05
[INFO 2017-06-28 12:03:53,111 main.py:51] epoch 884, training loss: 10101.67, average training loss: 11778.97, base loss: 14382.30
[INFO 2017-06-28 12:03:53,472 main.py:51] epoch 885, training loss: 10235.36, average training loss: 11777.23, base loss: 14381.99
[INFO 2017-06-28 12:03:53,816 main.py:51] epoch 886, training loss: 11172.50, average training loss: 11776.55, base loss: 14382.28
[INFO 2017-06-28 12:03:54,149 main.py:51] epoch 887, training loss: 10700.60, average training loss: 11775.34, base loss: 14382.11
[INFO 2017-06-28 12:03:54,491 main.py:51] epoch 888, training loss: 10627.81, average training loss: 11774.04, base loss: 14381.37
[INFO 2017-06-28 12:03:54,835 main.py:51] epoch 889, training loss: 9431.50, average training loss: 11771.41, base loss: 14379.03
[INFO 2017-06-28 12:03:55,254 main.py:51] epoch 890, training loss: 10814.52, average training loss: 11770.34, base loss: 14379.18
[INFO 2017-06-28 12:03:55,603 main.py:51] epoch 891, training loss: 10676.94, average training loss: 11769.11, base loss: 14378.42
[INFO 2017-06-28 12:03:55,942 main.py:51] epoch 892, training loss: 11377.73, average training loss: 11768.67, base loss: 14379.05
[INFO 2017-06-28 12:03:56,294 main.py:51] epoch 893, training loss: 10985.77, average training loss: 11767.80, base loss: 14379.67
[INFO 2017-06-28 12:03:56,686 main.py:51] epoch 894, training loss: 10491.52, average training loss: 11766.37, base loss: 14378.43
[INFO 2017-06-28 12:03:57,016 main.py:51] epoch 895, training loss: 11844.65, average training loss: 11766.46, base loss: 14379.99
[INFO 2017-06-28 12:03:57,356 main.py:51] epoch 896, training loss: 9452.35, average training loss: 11763.88, base loss: 14377.88
[INFO 2017-06-28 12:03:57,729 main.py:51] epoch 897, training loss: 10113.69, average training loss: 11762.04, base loss: 14377.14
[INFO 2017-06-28 12:03:58,094 main.py:51] epoch 898, training loss: 10328.21, average training loss: 11760.45, base loss: 14375.75
[INFO 2017-06-28 12:03:58,442 main.py:51] epoch 899, training loss: 8873.07, average training loss: 11757.24, base loss: 14372.67
[INFO 2017-06-28 12:03:58,442 main.py:53] epoch 899, testing
[INFO 2017-06-28 12:04:00,134 main.py:105] average testing loss: 11057.24, base loss: 14180.08
[INFO 2017-06-28 12:04:00,134 main.py:106] improve_loss: 3122.84, improve_percent: 0.22
[INFO 2017-06-28 12:04:00,135 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:04:00,141 main.py:76] current best improved percent: 0.22
[INFO 2017-06-28 12:04:00,476 main.py:51] epoch 900, training loss: 9855.19, average training loss: 11755.13, base loss: 14372.16
[INFO 2017-06-28 12:04:00,800 main.py:51] epoch 901, training loss: 9486.58, average training loss: 11752.61, base loss: 14370.61
[INFO 2017-06-28 12:04:01,123 main.py:51] epoch 902, training loss: 9872.21, average training loss: 11750.53, base loss: 14369.57
[INFO 2017-06-28 12:04:01,472 main.py:51] epoch 903, training loss: 9655.05, average training loss: 11748.21, base loss: 14366.79
[INFO 2017-06-28 12:04:01,786 main.py:51] epoch 904, training loss: 9646.99, average training loss: 11745.89, base loss: 14364.58
[INFO 2017-06-28 12:04:02,120 main.py:51] epoch 905, training loss: 11025.23, average training loss: 11745.10, base loss: 14363.76
[INFO 2017-06-28 12:04:02,460 main.py:51] epoch 906, training loss: 11785.46, average training loss: 11745.14, base loss: 14364.16
[INFO 2017-06-28 12:04:02,791 main.py:51] epoch 907, training loss: 10782.40, average training loss: 11744.08, base loss: 14364.91
[INFO 2017-06-28 12:04:03,117 main.py:51] epoch 908, training loss: 11259.97, average training loss: 11743.55, base loss: 14365.37
[INFO 2017-06-28 12:04:03,475 main.py:51] epoch 909, training loss: 12553.76, average training loss: 11744.44, base loss: 14367.38
[INFO 2017-06-28 12:04:03,845 main.py:51] epoch 910, training loss: 11009.37, average training loss: 11743.63, base loss: 14367.62
[INFO 2017-06-28 12:04:04,189 main.py:51] epoch 911, training loss: 11235.97, average training loss: 11743.07, base loss: 14368.64
[INFO 2017-06-28 12:04:04,530 main.py:51] epoch 912, training loss: 10614.50, average training loss: 11741.84, base loss: 14368.67
[INFO 2017-06-28 12:04:04,892 main.py:51] epoch 913, training loss: 11881.64, average training loss: 11741.99, base loss: 14371.34
[INFO 2017-06-28 12:04:05,235 main.py:51] epoch 914, training loss: 9700.70, average training loss: 11739.76, base loss: 14369.28
[INFO 2017-06-28 12:04:05,576 main.py:51] epoch 915, training loss: 10025.52, average training loss: 11737.89, base loss: 14368.42
[INFO 2017-06-28 12:04:05,909 main.py:51] epoch 916, training loss: 11583.73, average training loss: 11737.72, base loss: 14369.44
[INFO 2017-06-28 12:04:06,253 main.py:51] epoch 917, training loss: 10772.10, average training loss: 11736.67, base loss: 14369.61
[INFO 2017-06-28 12:04:06,593 main.py:51] epoch 918, training loss: 11952.49, average training loss: 11736.90, base loss: 14371.13
[INFO 2017-06-28 12:04:06,952 main.py:51] epoch 919, training loss: 9015.29, average training loss: 11733.95, base loss: 14367.54
[INFO 2017-06-28 12:04:07,301 main.py:51] epoch 920, training loss: 10400.92, average training loss: 11732.50, base loss: 14367.95
[INFO 2017-06-28 12:04:07,655 main.py:51] epoch 921, training loss: 11366.45, average training loss: 11732.10, base loss: 14367.79
[INFO 2017-06-28 12:04:08,033 main.py:51] epoch 922, training loss: 10692.50, average training loss: 11730.97, base loss: 14367.79
[INFO 2017-06-28 12:04:08,384 main.py:51] epoch 923, training loss: 10545.91, average training loss: 11729.69, base loss: 14368.49
[INFO 2017-06-28 12:04:08,719 main.py:51] epoch 924, training loss: 10926.78, average training loss: 11728.82, base loss: 14369.65
[INFO 2017-06-28 12:04:09,082 main.py:51] epoch 925, training loss: 10299.41, average training loss: 11727.28, base loss: 14369.60
[INFO 2017-06-28 12:04:09,456 main.py:51] epoch 926, training loss: 9837.25, average training loss: 11725.24, base loss: 14368.97
[INFO 2017-06-28 12:04:09,834 main.py:51] epoch 927, training loss: 10586.73, average training loss: 11724.01, base loss: 14367.66
[INFO 2017-06-28 12:04:10,246 main.py:51] epoch 928, training loss: 9661.73, average training loss: 11721.79, base loss: 14367.07
[INFO 2017-06-28 12:04:10,591 main.py:51] epoch 929, training loss: 11624.46, average training loss: 11721.69, base loss: 14366.84
[INFO 2017-06-28 12:04:10,934 main.py:51] epoch 930, training loss: 11258.81, average training loss: 11721.19, base loss: 14369.41
[INFO 2017-06-28 12:04:11,297 main.py:51] epoch 931, training loss: 12205.34, average training loss: 11721.71, base loss: 14371.31
[INFO 2017-06-28 12:04:11,642 main.py:51] epoch 932, training loss: 10625.91, average training loss: 11720.54, base loss: 14370.85
[INFO 2017-06-28 12:04:12,094 main.py:51] epoch 933, training loss: 10836.13, average training loss: 11719.59, base loss: 14370.34
[INFO 2017-06-28 12:04:12,489 main.py:51] epoch 934, training loss: 11182.06, average training loss: 11719.02, base loss: 14370.92
[INFO 2017-06-28 12:04:12,852 main.py:51] epoch 935, training loss: 10456.16, average training loss: 11717.67, base loss: 14372.36
[INFO 2017-06-28 12:04:13,209 main.py:51] epoch 936, training loss: 14219.47, average training loss: 11720.34, base loss: 14375.94
[INFO 2017-06-28 12:04:13,544 main.py:51] epoch 937, training loss: 10122.36, average training loss: 11718.63, base loss: 14374.43
[INFO 2017-06-28 12:04:13,928 main.py:51] epoch 938, training loss: 11377.01, average training loss: 11718.27, base loss: 14375.97
[INFO 2017-06-28 12:04:14,274 main.py:51] epoch 939, training loss: 11318.50, average training loss: 11717.84, base loss: 14377.33
[INFO 2017-06-28 12:04:14,611 main.py:51] epoch 940, training loss: 10828.75, average training loss: 11716.90, base loss: 14377.33
[INFO 2017-06-28 12:04:14,950 main.py:51] epoch 941, training loss: 10785.10, average training loss: 11715.91, base loss: 14377.12
[INFO 2017-06-28 12:04:15,282 main.py:51] epoch 942, training loss: 11174.37, average training loss: 11715.34, base loss: 14377.75
[INFO 2017-06-28 12:04:15,611 main.py:51] epoch 943, training loss: 10712.13, average training loss: 11714.27, base loss: 14377.98
[INFO 2017-06-28 12:04:15,932 main.py:51] epoch 944, training loss: 11465.02, average training loss: 11714.01, base loss: 14379.52
[INFO 2017-06-28 12:04:16,271 main.py:51] epoch 945, training loss: 10890.39, average training loss: 11713.14, base loss: 14379.70
[INFO 2017-06-28 12:04:16,617 main.py:51] epoch 946, training loss: 10227.58, average training loss: 11711.57, base loss: 14378.87
[INFO 2017-06-28 12:04:16,953 main.py:51] epoch 947, training loss: 10606.70, average training loss: 11710.40, base loss: 14378.76
[INFO 2017-06-28 12:04:17,269 main.py:51] epoch 948, training loss: 11489.63, average training loss: 11710.17, base loss: 14379.72
[INFO 2017-06-28 12:04:17,601 main.py:51] epoch 949, training loss: 10924.07, average training loss: 11709.34, base loss: 14379.71
[INFO 2017-06-28 12:04:17,924 main.py:51] epoch 950, training loss: 11706.31, average training loss: 11709.34, base loss: 14380.59
[INFO 2017-06-28 12:04:18,271 main.py:51] epoch 951, training loss: 10398.02, average training loss: 11707.96, base loss: 14380.83
[INFO 2017-06-28 12:04:18,606 main.py:51] epoch 952, training loss: 10791.57, average training loss: 11707.00, base loss: 14380.82
[INFO 2017-06-28 12:04:18,951 main.py:51] epoch 953, training loss: 10969.10, average training loss: 11706.23, base loss: 14381.53
[INFO 2017-06-28 12:04:19,300 main.py:51] epoch 954, training loss: 11037.79, average training loss: 11705.53, base loss: 14382.33
[INFO 2017-06-28 12:04:19,641 main.py:51] epoch 955, training loss: 12456.76, average training loss: 11706.31, base loss: 14383.95
[INFO 2017-06-28 12:04:19,969 main.py:51] epoch 956, training loss: 13388.38, average training loss: 11708.07, base loss: 14387.51
[INFO 2017-06-28 12:04:20,312 main.py:51] epoch 957, training loss: 10787.63, average training loss: 11707.11, base loss: 14387.88
[INFO 2017-06-28 12:04:20,643 main.py:51] epoch 958, training loss: 9529.44, average training loss: 11704.84, base loss: 14385.57
[INFO 2017-06-28 12:04:20,985 main.py:51] epoch 959, training loss: 11264.88, average training loss: 11704.38, base loss: 14385.72
[INFO 2017-06-28 12:04:21,312 main.py:51] epoch 960, training loss: 10266.59, average training loss: 11702.89, base loss: 14385.79
[INFO 2017-06-28 12:04:21,634 main.py:51] epoch 961, training loss: 10227.82, average training loss: 11701.35, base loss: 14385.30
[INFO 2017-06-28 12:04:21,970 main.py:51] epoch 962, training loss: 9146.37, average training loss: 11698.70, base loss: 14383.78
[INFO 2017-06-28 12:04:22,318 main.py:51] epoch 963, training loss: 10691.20, average training loss: 11697.65, base loss: 14384.08
[INFO 2017-06-28 12:04:22,662 main.py:51] epoch 964, training loss: 9471.41, average training loss: 11695.35, base loss: 14382.50
[INFO 2017-06-28 12:04:22,973 main.py:51] epoch 965, training loss: 11417.94, average training loss: 11695.06, base loss: 14383.92
[INFO 2017-06-28 12:04:23,343 main.py:51] epoch 966, training loss: 11905.82, average training loss: 11695.28, base loss: 14386.19
[INFO 2017-06-28 12:04:23,749 main.py:51] epoch 967, training loss: 12395.67, average training loss: 11696.00, base loss: 14389.25
[INFO 2017-06-28 12:04:24,084 main.py:51] epoch 968, training loss: 10672.66, average training loss: 11694.95, base loss: 14389.22
[INFO 2017-06-28 12:04:24,428 main.py:51] epoch 969, training loss: 11397.29, average training loss: 11694.64, base loss: 14390.53
[INFO 2017-06-28 12:04:24,835 main.py:51] epoch 970, training loss: 11313.89, average training loss: 11694.25, base loss: 14391.16
[INFO 2017-06-28 12:04:25,219 main.py:51] epoch 971, training loss: 12030.52, average training loss: 11694.59, base loss: 14392.36
[INFO 2017-06-28 12:04:25,561 main.py:51] epoch 972, training loss: 11221.97, average training loss: 11694.11, base loss: 14392.73
[INFO 2017-06-28 12:04:25,931 main.py:51] epoch 973, training loss: 11627.90, average training loss: 11694.04, base loss: 14394.94
[INFO 2017-06-28 12:04:26,286 main.py:51] epoch 974, training loss: 10540.63, average training loss: 11692.86, base loss: 14394.40
[INFO 2017-06-28 12:04:26,650 main.py:51] epoch 975, training loss: 11245.16, average training loss: 11692.40, base loss: 14394.86
[INFO 2017-06-28 12:04:26,977 main.py:51] epoch 976, training loss: 9664.88, average training loss: 11690.32, base loss: 14392.75
[INFO 2017-06-28 12:04:27,323 main.py:51] epoch 977, training loss: 11059.78, average training loss: 11689.68, base loss: 14392.93
[INFO 2017-06-28 12:04:27,677 main.py:51] epoch 978, training loss: 11362.72, average training loss: 11689.34, base loss: 14394.34
[INFO 2017-06-28 12:04:28,007 main.py:51] epoch 979, training loss: 11466.80, average training loss: 11689.12, base loss: 14394.15
[INFO 2017-06-28 12:04:28,346 main.py:51] epoch 980, training loss: 12868.54, average training loss: 11690.32, base loss: 14396.81
[INFO 2017-06-28 12:04:28,683 main.py:51] epoch 981, training loss: 11472.37, average training loss: 11690.10, base loss: 14397.46
[INFO 2017-06-28 12:04:29,024 main.py:51] epoch 982, training loss: 10331.49, average training loss: 11688.71, base loss: 14396.17
[INFO 2017-06-28 12:04:29,360 main.py:51] epoch 983, training loss: 10894.43, average training loss: 11687.91, base loss: 14397.12
[INFO 2017-06-28 12:04:29,694 main.py:51] epoch 984, training loss: 11872.15, average training loss: 11688.09, base loss: 14398.66
[INFO 2017-06-28 12:04:30,035 main.py:51] epoch 985, training loss: 11235.81, average training loss: 11687.64, base loss: 14400.18
[INFO 2017-06-28 12:04:30,403 main.py:51] epoch 986, training loss: 12341.10, average training loss: 11688.30, base loss: 14402.50
[INFO 2017-06-28 12:04:30,735 main.py:51] epoch 987, training loss: 10675.92, average training loss: 11687.27, base loss: 14401.68
[INFO 2017-06-28 12:04:31,073 main.py:51] epoch 988, training loss: 12222.88, average training loss: 11687.81, base loss: 14403.17
[INFO 2017-06-28 12:04:31,441 main.py:51] epoch 989, training loss: 9502.06, average training loss: 11685.61, base loss: 14400.78
[INFO 2017-06-28 12:04:31,775 main.py:51] epoch 990, training loss: 10129.25, average training loss: 11684.04, base loss: 14399.95
[INFO 2017-06-28 12:04:32,135 main.py:51] epoch 991, training loss: 10638.40, average training loss: 11682.98, base loss: 14399.66
[INFO 2017-06-28 12:04:32,448 main.py:51] epoch 992, training loss: 9549.81, average training loss: 11680.83, base loss: 14397.64
[INFO 2017-06-28 12:04:32,808 main.py:51] epoch 993, training loss: 12388.17, average training loss: 11681.55, base loss: 14399.10
[INFO 2017-06-28 12:04:33,160 main.py:51] epoch 994, training loss: 10910.81, average training loss: 11680.77, base loss: 14398.77
[INFO 2017-06-28 12:04:33,496 main.py:51] epoch 995, training loss: 8565.82, average training loss: 11677.64, base loss: 14396.34
[INFO 2017-06-28 12:04:33,843 main.py:51] epoch 996, training loss: 10110.99, average training loss: 11676.07, base loss: 14395.86
[INFO 2017-06-28 12:04:34,172 main.py:51] epoch 997, training loss: 11319.12, average training loss: 11675.71, base loss: 14396.49
[INFO 2017-06-28 12:04:34,507 main.py:51] epoch 998, training loss: 10226.07, average training loss: 11674.26, base loss: 14396.39
[INFO 2017-06-28 12:04:34,855 main.py:51] epoch 999, training loss: 12542.60, average training loss: 11675.13, base loss: 14399.54
[INFO 2017-06-28 12:04:34,856 main.py:53] epoch 999, testing
[INFO 2017-06-28 12:04:36,673 main.py:105] average testing loss: 12028.29, base loss: 15371.05
[INFO 2017-06-28 12:04:36,673 main.py:106] improve_loss: 3342.76, improve_percent: 0.22
[INFO 2017-06-28 12:04:36,673 main.py:76] current best improved percent: 0.22
[INFO 2017-06-28 12:04:37,010 main.py:51] epoch 1000, training loss: 12331.71, average training loss: 11658.48, base loss: 14402.23
[INFO 2017-06-28 12:04:37,335 main.py:51] epoch 1001, training loss: 10977.59, average training loss: 11643.13, base loss: 14401.82
[INFO 2017-06-28 12:04:37,685 main.py:51] epoch 1002, training loss: 10503.31, average training loss: 11628.06, base loss: 14401.07
[INFO 2017-06-28 12:04:38,025 main.py:51] epoch 1003, training loss: 11046.93, average training loss: 11614.67, base loss: 14400.90
[INFO 2017-06-28 12:04:38,383 main.py:51] epoch 1004, training loss: 12111.72, average training loss: 11604.60, base loss: 14403.60
[INFO 2017-06-28 12:04:38,770 main.py:51] epoch 1005, training loss: 11238.87, average training loss: 11594.74, base loss: 14405.17
[INFO 2017-06-28 12:04:39,119 main.py:51] epoch 1006, training loss: 12846.00, average training loss: 11588.16, base loss: 14408.12
[INFO 2017-06-28 12:04:39,482 main.py:51] epoch 1007, training loss: 10655.94, average training loss: 11579.02, base loss: 14408.01
[INFO 2017-06-28 12:04:39,835 main.py:51] epoch 1008, training loss: 11807.51, average training loss: 11571.71, base loss: 14409.06
[INFO 2017-06-28 12:04:40,213 main.py:51] epoch 1009, training loss: 10338.21, average training loss: 11564.05, base loss: 14409.56
[INFO 2017-06-28 12:04:40,573 main.py:51] epoch 1010, training loss: 11801.22, average training loss: 11558.92, base loss: 14411.36
[INFO 2017-06-28 12:04:40,918 main.py:51] epoch 1011, training loss: 10464.83, average training loss: 11552.73, base loss: 14411.01
[INFO 2017-06-28 12:04:41,252 main.py:51] epoch 1012, training loss: 9488.84, average training loss: 11542.51, base loss: 14406.00
[INFO 2017-06-28 12:04:41,609 main.py:51] epoch 1013, training loss: 10697.10, average training loss: 11535.91, base loss: 14403.68
[INFO 2017-06-28 12:04:41,940 main.py:51] epoch 1014, training loss: 11960.42, average training loss: 11531.55, base loss: 14406.80
[INFO 2017-06-28 12:04:42,274 main.py:51] epoch 1015, training loss: 10231.59, average training loss: 11525.72, base loss: 14407.39
[INFO 2017-06-28 12:04:42,617 main.py:51] epoch 1016, training loss: 10820.89, average training loss: 11520.52, base loss: 14407.59
[INFO 2017-06-28 12:04:42,957 main.py:51] epoch 1017, training loss: 10506.36, average training loss: 11515.51, base loss: 14407.96
[INFO 2017-06-28 12:04:43,280 main.py:51] epoch 1018, training loss: 10619.80, average training loss: 11511.32, base loss: 14408.91
[INFO 2017-06-28 12:04:43,630 main.py:51] epoch 1019, training loss: 9341.23, average training loss: 11504.27, base loss: 14406.01
[INFO 2017-06-28 12:04:43,974 main.py:51] epoch 1020, training loss: 9836.31, average training loss: 11499.25, base loss: 14405.28
[INFO 2017-06-28 12:04:44,324 main.py:51] epoch 1021, training loss: 10087.98, average training loss: 11495.71, base loss: 14405.50
[INFO 2017-06-28 12:04:44,668 main.py:51] epoch 1022, training loss: 9365.18, average training loss: 11489.51, base loss: 14403.34
[INFO 2017-06-28 12:04:44,996 main.py:51] epoch 1023, training loss: 10368.01, average training loss: 11486.19, base loss: 14404.13
[INFO 2017-06-28 12:04:45,339 main.py:51] epoch 1024, training loss: 10867.75, average training loss: 11483.60, base loss: 14405.94
[INFO 2017-06-28 12:04:45,682 main.py:51] epoch 1025, training loss: 11270.97, average training loss: 11480.41, base loss: 14406.85
[INFO 2017-06-28 12:04:46,007 main.py:51] epoch 1026, training loss: 9791.06, average training loss: 11473.72, base loss: 14404.01
[INFO 2017-06-28 12:04:46,374 main.py:51] epoch 1027, training loss: 11463.71, average training loss: 11471.34, base loss: 14405.46
[INFO 2017-06-28 12:04:46,720 main.py:51] epoch 1028, training loss: 10844.60, average training loss: 11465.50, base loss: 14403.34
[INFO 2017-06-28 12:04:47,065 main.py:51] epoch 1029, training loss: 10168.32, average training loss: 11460.91, base loss: 14402.60
[INFO 2017-06-28 12:04:47,410 main.py:51] epoch 1030, training loss: 9527.68, average training loss: 11454.23, base loss: 14399.41
[INFO 2017-06-28 12:04:47,733 main.py:51] epoch 1031, training loss: 9617.43, average training loss: 11449.44, base loss: 14397.41
[INFO 2017-06-28 12:04:48,056 main.py:51] epoch 1032, training loss: 11660.45, average training loss: 11445.88, base loss: 14398.65
[INFO 2017-06-28 12:04:48,377 main.py:51] epoch 1033, training loss: 11054.41, average training loss: 11440.54, base loss: 14395.95
[INFO 2017-06-28 12:04:48,698 main.py:51] epoch 1034, training loss: 12060.82, average training loss: 11439.52, base loss: 14398.08
[INFO 2017-06-28 12:04:49,117 main.py:51] epoch 1035, training loss: 9774.39, average training loss: 11436.12, base loss: 14398.32
[INFO 2017-06-28 12:04:49,501 main.py:51] epoch 1036, training loss: 9465.63, average training loss: 11433.61, base loss: 14398.32
[INFO 2017-06-28 12:04:49,835 main.py:51] epoch 1037, training loss: 11299.00, average training loss: 11432.07, base loss: 14400.09
[INFO 2017-06-28 12:04:50,184 main.py:51] epoch 1038, training loss: 11845.64, average training loss: 11430.78, base loss: 14402.60
[INFO 2017-06-28 12:04:50,591 main.py:51] epoch 1039, training loss: 10084.79, average training loss: 11426.64, base loss: 14401.57
[INFO 2017-06-28 12:04:50,978 main.py:51] epoch 1040, training loss: 9721.67, average training loss: 11421.79, base loss: 14400.30
[INFO 2017-06-28 12:04:51,361 main.py:51] epoch 1041, training loss: 9902.23, average training loss: 11419.09, base loss: 14400.35
[INFO 2017-06-28 12:04:51,701 main.py:51] epoch 1042, training loss: 10132.23, average training loss: 11416.07, base loss: 14400.29
[INFO 2017-06-28 12:04:52,060 main.py:51] epoch 1043, training loss: 10813.24, average training loss: 11409.62, base loss: 14396.18
[INFO 2017-06-28 12:04:52,406 main.py:51] epoch 1044, training loss: 11555.45, average training loss: 11407.10, base loss: 14397.43
[INFO 2017-06-28 12:04:52,735 main.py:51] epoch 1045, training loss: 10761.62, average training loss: 11405.21, base loss: 14398.91
[INFO 2017-06-28 12:04:53,082 main.py:51] epoch 1046, training loss: 10125.35, average training loss: 11400.99, base loss: 14397.26
[INFO 2017-06-28 12:04:53,416 main.py:51] epoch 1047, training loss: 10893.91, average training loss: 11396.44, base loss: 14396.24
[INFO 2017-06-28 12:04:53,761 main.py:51] epoch 1048, training loss: 10037.87, average training loss: 11393.47, base loss: 14396.43
[INFO 2017-06-28 12:04:54,088 main.py:51] epoch 1049, training loss: 10414.19, average training loss: 11389.43, base loss: 14395.34
[INFO 2017-06-28 12:04:54,452 main.py:51] epoch 1050, training loss: 11041.95, average training loss: 11387.77, base loss: 14396.39
[INFO 2017-06-28 12:04:54,795 main.py:51] epoch 1051, training loss: 10575.00, average training loss: 11383.55, base loss: 14395.73
[INFO 2017-06-28 12:04:55,120 main.py:51] epoch 1052, training loss: 10533.84, average training loss: 11380.72, base loss: 14395.64
[INFO 2017-06-28 12:04:55,472 main.py:51] epoch 1053, training loss: 13158.36, average training loss: 11381.36, base loss: 14400.02
[INFO 2017-06-28 12:04:55,812 main.py:51] epoch 1054, training loss: 10969.70, average training loss: 11378.61, base loss: 14400.34
[INFO 2017-06-28 12:04:56,244 main.py:51] epoch 1055, training loss: 11533.49, average training loss: 11376.16, base loss: 14400.78
[INFO 2017-06-28 12:04:56,649 main.py:51] epoch 1056, training loss: 10601.92, average training loss: 11372.78, base loss: 14401.54
[INFO 2017-06-28 12:04:56,995 main.py:51] epoch 1057, training loss: 11022.58, average training loss: 11369.04, base loss: 14400.75
[INFO 2017-06-28 12:04:57,380 main.py:51] epoch 1058, training loss: 9042.37, average training loss: 11363.81, base loss: 14398.22
[INFO 2017-06-28 12:04:57,790 main.py:51] epoch 1059, training loss: 10973.19, average training loss: 11362.36, base loss: 14400.74
[INFO 2017-06-28 12:04:58,126 main.py:51] epoch 1060, training loss: 10861.43, average training loss: 11358.95, base loss: 14400.54
[INFO 2017-06-28 12:04:58,543 main.py:51] epoch 1061, training loss: 10552.62, average training loss: 11353.82, base loss: 14398.34
[INFO 2017-06-28 12:04:58,944 main.py:51] epoch 1062, training loss: 9249.99, average training loss: 11349.23, base loss: 14395.89
[INFO 2017-06-28 12:04:59,311 main.py:51] epoch 1063, training loss: 10126.95, average training loss: 11345.24, base loss: 14393.93
[INFO 2017-06-28 12:04:59,751 main.py:51] epoch 1064, training loss: 10888.02, average training loss: 11342.47, base loss: 14395.98
[INFO 2017-06-28 12:05:00,127 main.py:51] epoch 1065, training loss: 10474.28, average training loss: 11341.00, base loss: 14397.85
[INFO 2017-06-28 12:05:00,476 main.py:51] epoch 1066, training loss: 9651.06, average training loss: 11337.58, base loss: 14397.30
[INFO 2017-06-28 12:05:00,825 main.py:51] epoch 1067, training loss: 12331.08, average training loss: 11333.95, base loss: 14395.36
[INFO 2017-06-28 12:05:01,164 main.py:51] epoch 1068, training loss: 9854.64, average training loss: 11331.29, base loss: 14395.03
[INFO 2017-06-28 12:05:01,565 main.py:51] epoch 1069, training loss: 9932.50, average training loss: 11328.58, base loss: 14394.69
[INFO 2017-06-28 12:05:01,906 main.py:51] epoch 1070, training loss: 11998.52, average training loss: 11325.16, base loss: 14394.27
[INFO 2017-06-28 12:05:02,259 main.py:51] epoch 1071, training loss: 11543.25, average training loss: 11323.68, base loss: 14396.32
[INFO 2017-06-28 12:05:02,600 main.py:51] epoch 1072, training loss: 11534.87, average training loss: 11322.29, base loss: 14397.74
[INFO 2017-06-28 12:05:02,959 main.py:51] epoch 1073, training loss: 12133.63, average training loss: 11322.18, base loss: 14399.71
[INFO 2017-06-28 12:05:03,289 main.py:51] epoch 1074, training loss: 9415.63, average training loss: 11319.60, base loss: 14399.34
[INFO 2017-06-28 12:05:03,619 main.py:51] epoch 1075, training loss: 11818.06, average training loss: 11319.91, base loss: 14403.09
[INFO 2017-06-28 12:05:03,963 main.py:51] epoch 1076, training loss: 9865.26, average training loss: 11315.79, base loss: 14402.04
[INFO 2017-06-28 12:05:04,324 main.py:51] epoch 1077, training loss: 10603.53, average training loss: 11314.72, base loss: 14403.40
[INFO 2017-06-28 12:05:04,669 main.py:51] epoch 1078, training loss: 10486.64, average training loss: 11312.81, base loss: 14402.87
[INFO 2017-06-28 12:05:04,995 main.py:51] epoch 1079, training loss: 12466.42, average training loss: 11311.81, base loss: 14405.75
[INFO 2017-06-28 12:05:05,323 main.py:51] epoch 1080, training loss: 8629.63, average training loss: 11306.71, base loss: 14402.22
[INFO 2017-06-28 12:05:05,650 main.py:51] epoch 1081, training loss: 10662.58, average training loss: 11304.63, base loss: 14403.09
[INFO 2017-06-28 12:05:05,991 main.py:51] epoch 1082, training loss: 12609.90, average training loss: 11303.88, base loss: 14405.86
[INFO 2017-06-28 12:05:06,349 main.py:51] epoch 1083, training loss: 15533.44, average training loss: 11306.99, base loss: 14412.32
[INFO 2017-06-28 12:05:06,688 main.py:51] epoch 1084, training loss: 9853.71, average training loss: 11305.35, base loss: 14413.11
[INFO 2017-06-28 12:05:07,028 main.py:51] epoch 1085, training loss: 10085.36, average training loss: 11302.31, base loss: 14411.97
[INFO 2017-06-28 12:05:07,394 main.py:51] epoch 1086, training loss: 10246.63, average training loss: 11298.78, base loss: 14411.24
[INFO 2017-06-28 12:05:07,744 main.py:51] epoch 1087, training loss: 10732.41, average training loss: 11296.61, base loss: 14411.54
[INFO 2017-06-28 12:05:08,081 main.py:51] epoch 1088, training loss: 10409.96, average training loss: 11294.05, base loss: 14410.60
[INFO 2017-06-28 12:05:08,428 main.py:51] epoch 1089, training loss: 11518.18, average training loss: 11293.39, base loss: 14412.10
[INFO 2017-06-28 12:05:08,779 main.py:51] epoch 1090, training loss: 11097.69, average training loss: 11293.10, base loss: 14415.50
[INFO 2017-06-28 12:05:09,119 main.py:51] epoch 1091, training loss: 10173.59, average training loss: 11290.48, base loss: 14415.51
[INFO 2017-06-28 12:05:09,503 main.py:51] epoch 1092, training loss: 10092.95, average training loss: 11287.80, base loss: 14414.53
[INFO 2017-06-28 12:05:09,856 main.py:51] epoch 1093, training loss: 11572.91, average training loss: 11287.87, base loss: 14417.69
[INFO 2017-06-28 12:05:10,221 main.py:51] epoch 1094, training loss: 9756.40, average training loss: 11284.98, base loss: 14417.40
[INFO 2017-06-28 12:05:10,582 main.py:51] epoch 1095, training loss: 11810.39, average training loss: 11282.16, base loss: 14417.05
[INFO 2017-06-28 12:05:10,927 main.py:51] epoch 1096, training loss: 10545.11, average training loss: 11280.27, base loss: 14418.03
[INFO 2017-06-28 12:05:11,273 main.py:51] epoch 1097, training loss: 11506.79, average training loss: 11279.07, base loss: 14417.86
[INFO 2017-06-28 12:05:11,602 main.py:51] epoch 1098, training loss: 9917.78, average training loss: 11275.70, base loss: 14416.09
[INFO 2017-06-28 12:05:11,956 main.py:51] epoch 1099, training loss: 10965.53, average training loss: 11271.56, base loss: 14413.92
[INFO 2017-06-28 12:05:11,956 main.py:53] epoch 1099, testing
[INFO 2017-06-28 12:05:13,674 main.py:105] average testing loss: 11114.20, base loss: 14544.40
[INFO 2017-06-28 12:05:13,674 main.py:106] improve_loss: 3430.20, improve_percent: 0.24
[INFO 2017-06-28 12:05:13,675 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:05:13,681 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:05:14,104 main.py:51] epoch 1100, training loss: 10064.73, average training loss: 11268.55, base loss: 14412.33
[INFO 2017-06-28 12:05:14,496 main.py:51] epoch 1101, training loss: 12373.18, average training loss: 11264.59, base loss: 14411.23
[INFO 2017-06-28 12:05:14,835 main.py:51] epoch 1102, training loss: 9123.61, average training loss: 11261.64, base loss: 14409.41
[INFO 2017-06-28 12:05:15,193 main.py:51] epoch 1103, training loss: 10460.47, average training loss: 11258.89, base loss: 14409.27
[INFO 2017-06-28 12:05:15,530 main.py:51] epoch 1104, training loss: 9253.44, average training loss: 11253.66, base loss: 14406.35
[INFO 2017-06-28 12:05:15,876 main.py:51] epoch 1105, training loss: 9990.41, average training loss: 11250.19, base loss: 14406.24
[INFO 2017-06-28 12:05:16,206 main.py:51] epoch 1106, training loss: 11376.28, average training loss: 11249.07, base loss: 14407.38
[INFO 2017-06-28 12:05:16,524 main.py:51] epoch 1107, training loss: 9842.39, average training loss: 11244.36, base loss: 14404.58
[INFO 2017-06-28 12:05:16,877 main.py:51] epoch 1108, training loss: 9802.52, average training loss: 11241.57, base loss: 14403.98
[INFO 2017-06-28 12:05:17,213 main.py:51] epoch 1109, training loss: 11052.81, average training loss: 11240.17, base loss: 14405.24
[INFO 2017-06-28 12:05:17,563 main.py:51] epoch 1110, training loss: 11346.60, average training loss: 11240.36, base loss: 14409.17
[INFO 2017-06-28 12:05:17,921 main.py:51] epoch 1111, training loss: 11884.85, average training loss: 11237.71, base loss: 14409.73
[INFO 2017-06-28 12:05:18,265 main.py:51] epoch 1112, training loss: 12420.23, average training loss: 11238.20, base loss: 14412.51
[INFO 2017-06-28 12:05:18,604 main.py:51] epoch 1113, training loss: 10245.27, average training loss: 11236.21, base loss: 14412.91
[INFO 2017-06-28 12:05:18,942 main.py:51] epoch 1114, training loss: 11988.00, average training loss: 11235.66, base loss: 14415.14
[INFO 2017-06-28 12:05:19,282 main.py:51] epoch 1115, training loss: 11295.67, average training loss: 11233.36, base loss: 14414.08
[INFO 2017-06-28 12:05:19,634 main.py:51] epoch 1116, training loss: 11463.29, average training loss: 11231.91, base loss: 14415.21
[INFO 2017-06-28 12:05:19,940 main.py:51] epoch 1117, training loss: 9780.58, average training loss: 11230.42, base loss: 14415.82
[INFO 2017-06-28 12:05:20,294 main.py:51] epoch 1118, training loss: 9252.38, average training loss: 11225.73, base loss: 14411.64
[INFO 2017-06-28 12:05:20,641 main.py:51] epoch 1119, training loss: 10732.53, average training loss: 11223.64, base loss: 14411.73
[INFO 2017-06-28 12:05:20,981 main.py:51] epoch 1120, training loss: 9980.14, average training loss: 11220.50, base loss: 14409.78
[INFO 2017-06-28 12:05:21,319 main.py:51] epoch 1121, training loss: 10868.46, average training loss: 11218.27, base loss: 14408.17
[INFO 2017-06-28 12:05:21,656 main.py:51] epoch 1122, training loss: 10198.10, average training loss: 11215.61, base loss: 14407.38
[INFO 2017-06-28 12:05:22,011 main.py:51] epoch 1123, training loss: 9933.97, average training loss: 11213.47, base loss: 14407.14
[INFO 2017-06-28 12:05:22,365 main.py:51] epoch 1124, training loss: 9874.81, average training loss: 11211.92, base loss: 14405.91
[INFO 2017-06-28 12:05:22,747 main.py:51] epoch 1125, training loss: 11139.82, average training loss: 11210.61, base loss: 14406.25
[INFO 2017-06-28 12:05:23,098 main.py:51] epoch 1126, training loss: 13202.04, average training loss: 11210.13, base loss: 14408.19
[INFO 2017-06-28 12:05:23,431 main.py:51] epoch 1127, training loss: 10721.01, average training loss: 11207.10, base loss: 14406.32
[INFO 2017-06-28 12:05:23,791 main.py:51] epoch 1128, training loss: 12399.68, average training loss: 11206.54, base loss: 14408.17
[INFO 2017-06-28 12:05:24,135 main.py:51] epoch 1129, training loss: 9963.25, average training loss: 11204.44, base loss: 14406.81
[INFO 2017-06-28 12:05:24,492 main.py:51] epoch 1130, training loss: 9995.21, average training loss: 11202.34, base loss: 14407.07
[INFO 2017-06-28 12:05:24,874 main.py:51] epoch 1131, training loss: 9501.40, average training loss: 11198.52, base loss: 14405.12
[INFO 2017-06-28 12:05:25,226 main.py:51] epoch 1132, training loss: 11094.41, average training loss: 11196.47, base loss: 14405.17
[INFO 2017-06-28 12:05:25,595 main.py:51] epoch 1133, training loss: 10531.18, average training loss: 11194.65, base loss: 14405.23
[INFO 2017-06-28 12:05:25,927 main.py:51] epoch 1134, training loss: 10205.01, average training loss: 11193.83, base loss: 14406.73
[INFO 2017-06-28 12:05:26,273 main.py:51] epoch 1135, training loss: 10234.01, average training loss: 11191.10, base loss: 14406.62
[INFO 2017-06-28 12:05:26,620 main.py:51] epoch 1136, training loss: 9982.66, average training loss: 11189.77, base loss: 14406.28
[INFO 2017-06-28 12:05:26,950 main.py:51] epoch 1137, training loss: 10077.99, average training loss: 11186.58, base loss: 14404.09
[INFO 2017-06-28 12:05:27,305 main.py:51] epoch 1138, training loss: 10226.15, average training loss: 11184.39, base loss: 14402.96
[INFO 2017-06-28 12:05:27,653 main.py:51] epoch 1139, training loss: 8210.78, average training loss: 11180.42, base loss: 14399.35
[INFO 2017-06-28 12:05:28,016 main.py:51] epoch 1140, training loss: 10961.54, average training loss: 11179.08, base loss: 14399.15
[INFO 2017-06-28 12:05:28,364 main.py:51] epoch 1141, training loss: 10662.34, average training loss: 11177.89, base loss: 14399.64
[INFO 2017-06-28 12:05:28,694 main.py:51] epoch 1142, training loss: 9512.56, average training loss: 11173.73, base loss: 14396.73
[INFO 2017-06-28 12:05:29,019 main.py:51] epoch 1143, training loss: 12595.92, average training loss: 11174.26, base loss: 14399.06
[INFO 2017-06-28 12:05:29,363 main.py:51] epoch 1144, training loss: 10565.25, average training loss: 11173.40, base loss: 14401.80
[INFO 2017-06-28 12:05:29,720 main.py:51] epoch 1145, training loss: 10989.63, average training loss: 11173.06, base loss: 14403.63
[INFO 2017-06-28 12:05:30,063 main.py:51] epoch 1146, training loss: 10386.54, average training loss: 11172.25, base loss: 14405.24
[INFO 2017-06-28 12:05:30,412 main.py:51] epoch 1147, training loss: 11164.60, average training loss: 11171.88, base loss: 14406.24
[INFO 2017-06-28 12:05:30,760 main.py:51] epoch 1148, training loss: 9570.12, average training loss: 11167.76, base loss: 14401.91
[INFO 2017-06-28 12:05:31,112 main.py:51] epoch 1149, training loss: 10222.16, average training loss: 11164.19, base loss: 14399.70
[INFO 2017-06-28 12:05:31,459 main.py:51] epoch 1150, training loss: 9756.37, average training loss: 11155.84, base loss: 14392.13
[INFO 2017-06-28 12:05:31,805 main.py:51] epoch 1151, training loss: 10844.34, average training loss: 11152.01, base loss: 14390.16
[INFO 2017-06-28 12:05:32,148 main.py:51] epoch 1152, training loss: 10194.54, average training loss: 11148.27, base loss: 14388.05
[INFO 2017-06-28 12:05:32,504 main.py:51] epoch 1153, training loss: 9018.29, average training loss: 11145.54, base loss: 14386.50
[INFO 2017-06-28 12:05:32,872 main.py:51] epoch 1154, training loss: 10505.09, average training loss: 11145.13, base loss: 14389.03
[INFO 2017-06-28 12:05:33,193 main.py:51] epoch 1155, training loss: 11132.07, average training loss: 11142.81, base loss: 14388.14
[INFO 2017-06-28 12:05:33,513 main.py:51] epoch 1156, training loss: 9452.81, average training loss: 11141.52, base loss: 14388.77
[INFO 2017-06-28 12:05:33,915 main.py:51] epoch 1157, training loss: 9264.03, average training loss: 11139.53, base loss: 14389.03
[INFO 2017-06-28 12:05:34,302 main.py:51] epoch 1158, training loss: 11048.84, average training loss: 11138.52, base loss: 14390.64
[INFO 2017-06-28 12:05:34,632 main.py:51] epoch 1159, training loss: 9559.05, average training loss: 11138.00, base loss: 14391.04
[INFO 2017-06-28 12:05:34,987 main.py:51] epoch 1160, training loss: 11458.20, average training loss: 11138.38, base loss: 14394.52
[INFO 2017-06-28 12:05:35,315 main.py:51] epoch 1161, training loss: 11814.83, average training loss: 11138.97, base loss: 14398.30
[INFO 2017-06-28 12:05:35,675 main.py:51] epoch 1162, training loss: 9577.26, average training loss: 11136.30, base loss: 14396.84
[INFO 2017-06-28 12:05:36,024 main.py:51] epoch 1163, training loss: 10384.56, average training loss: 11132.71, base loss: 14393.34
[INFO 2017-06-28 12:05:36,341 main.py:51] epoch 1164, training loss: 11930.46, average training loss: 11134.15, base loss: 14397.46
[INFO 2017-06-28 12:05:36,679 main.py:51] epoch 1165, training loss: 10133.85, average training loss: 11132.02, base loss: 14397.35
[INFO 2017-06-28 12:05:37,012 main.py:51] epoch 1166, training loss: 10895.09, average training loss: 11132.91, base loss: 14399.81
[INFO 2017-06-28 12:05:37,344 main.py:51] epoch 1167, training loss: 11042.40, average training loss: 11131.20, base loss: 14400.47
[INFO 2017-06-28 12:05:37,683 main.py:51] epoch 1168, training loss: 10592.03, average training loss: 11131.16, base loss: 14402.10
[INFO 2017-06-28 12:05:38,010 main.py:51] epoch 1169, training loss: 11472.71, average training loss: 11131.70, base loss: 14404.21
[INFO 2017-06-28 12:05:38,333 main.py:51] epoch 1170, training loss: 10597.38, average training loss: 11128.93, base loss: 14402.95
[INFO 2017-06-28 12:05:38,680 main.py:51] epoch 1171, training loss: 9374.05, average training loss: 11126.80, base loss: 14402.69
[INFO 2017-06-28 12:05:39,024 main.py:51] epoch 1172, training loss: 11886.68, average training loss: 11126.10, base loss: 14403.67
[INFO 2017-06-28 12:05:39,368 main.py:51] epoch 1173, training loss: 11101.64, average training loss: 11124.65, base loss: 14403.51
[INFO 2017-06-28 12:05:39,716 main.py:51] epoch 1174, training loss: 9684.34, average training loss: 11122.28, base loss: 14402.46
[INFO 2017-06-28 12:05:40,073 main.py:51] epoch 1175, training loss: 9607.19, average training loss: 11119.48, base loss: 14402.24
[INFO 2017-06-28 12:05:40,411 main.py:51] epoch 1176, training loss: 10221.45, average training loss: 11118.32, base loss: 14403.24
[INFO 2017-06-28 12:05:40,764 main.py:51] epoch 1177, training loss: 10652.03, average training loss: 11116.59, base loss: 14403.18
[INFO 2017-06-28 12:05:41,117 main.py:51] epoch 1178, training loss: 10815.58, average training loss: 11116.62, base loss: 14404.84
[INFO 2017-06-28 12:05:41,480 main.py:51] epoch 1179, training loss: 10066.48, average training loss: 11115.82, base loss: 14404.88
[INFO 2017-06-28 12:05:41,813 main.py:51] epoch 1180, training loss: 10567.48, average training loss: 11115.36, base loss: 14405.45
[INFO 2017-06-28 12:05:42,150 main.py:51] epoch 1181, training loss: 9164.25, average training loss: 11110.81, base loss: 14401.37
[INFO 2017-06-28 12:05:42,486 main.py:51] epoch 1182, training loss: 11731.98, average training loss: 11111.06, base loss: 14402.83
[INFO 2017-06-28 12:05:42,826 main.py:51] epoch 1183, training loss: 9819.26, average training loss: 11108.33, base loss: 14400.87
[INFO 2017-06-28 12:05:43,165 main.py:51] epoch 1184, training loss: 8622.83, average training loss: 11105.54, base loss: 14399.31
[INFO 2017-06-28 12:05:43,491 main.py:51] epoch 1185, training loss: 9712.48, average training loss: 11103.28, base loss: 14398.44
[INFO 2017-06-28 12:05:43,852 main.py:51] epoch 1186, training loss: 10267.29, average training loss: 11102.42, base loss: 14397.68
[INFO 2017-06-28 12:05:44,189 main.py:51] epoch 1187, training loss: 11132.71, average training loss: 11100.97, base loss: 14397.74
[INFO 2017-06-28 12:05:44,537 main.py:51] epoch 1188, training loss: 10275.68, average training loss: 11099.44, base loss: 14397.63
[INFO 2017-06-28 12:05:44,915 main.py:51] epoch 1189, training loss: 10256.26, average training loss: 11098.15, base loss: 14398.43
[INFO 2017-06-28 12:05:45,251 main.py:51] epoch 1190, training loss: 9820.80, average training loss: 11095.64, base loss: 14397.06
[INFO 2017-06-28 12:05:45,601 main.py:51] epoch 1191, training loss: 10839.21, average training loss: 11094.84, base loss: 14396.67
[INFO 2017-06-28 12:05:45,931 main.py:51] epoch 1192, training loss: 9159.30, average training loss: 11092.80, base loss: 14396.31
[INFO 2017-06-28 12:05:46,284 main.py:51] epoch 1193, training loss: 9572.92, average training loss: 11091.21, base loss: 14395.95
[INFO 2017-06-28 12:05:46,615 main.py:51] epoch 1194, training loss: 11066.84, average training loss: 11089.43, base loss: 14396.55
[INFO 2017-06-28 12:05:46,938 main.py:51] epoch 1195, training loss: 9822.00, average training loss: 11084.32, base loss: 14392.04
[INFO 2017-06-28 12:05:47,298 main.py:51] epoch 1196, training loss: 9531.14, average training loss: 11082.47, base loss: 14391.49
[INFO 2017-06-28 12:05:47,655 main.py:51] epoch 1197, training loss: 9734.76, average training loss: 11079.18, base loss: 14388.70
[INFO 2017-06-28 12:05:47,991 main.py:51] epoch 1198, training loss: 9536.67, average training loss: 11077.61, base loss: 14387.94
[INFO 2017-06-28 12:05:48,330 main.py:51] epoch 1199, training loss: 9725.16, average training loss: 11075.70, base loss: 14386.68
[INFO 2017-06-28 12:05:48,330 main.py:53] epoch 1199, testing
[INFO 2017-06-28 12:05:50,042 main.py:105] average testing loss: 11686.89, base loss: 14920.80
[INFO 2017-06-28 12:05:50,042 main.py:106] improve_loss: 3233.91, improve_percent: 0.22
[INFO 2017-06-28 12:05:50,042 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:05:50,408 main.py:51] epoch 1200, training loss: 9459.22, average training loss: 11072.08, base loss: 14382.64
[INFO 2017-06-28 12:05:50,747 main.py:51] epoch 1201, training loss: 11130.73, average training loss: 11071.13, base loss: 14382.85
[INFO 2017-06-28 12:05:51,086 main.py:51] epoch 1202, training loss: 10310.85, average training loss: 11069.25, base loss: 14382.63
[INFO 2017-06-28 12:05:51,435 main.py:51] epoch 1203, training loss: 10978.80, average training loss: 11069.49, base loss: 14383.37
[INFO 2017-06-28 12:05:51,794 main.py:51] epoch 1204, training loss: 9983.43, average training loss: 11067.22, base loss: 14383.17
[INFO 2017-06-28 12:05:52,162 main.py:51] epoch 1205, training loss: 11905.24, average training loss: 11068.60, base loss: 14386.77
[INFO 2017-06-28 12:05:52,507 main.py:51] epoch 1206, training loss: 12396.39, average training loss: 11069.31, base loss: 14389.91
[INFO 2017-06-28 12:05:52,864 main.py:51] epoch 1207, training loss: 12364.60, average training loss: 11070.66, base loss: 14393.92
[INFO 2017-06-28 12:05:53,251 main.py:51] epoch 1208, training loss: 9713.96, average training loss: 11067.76, base loss: 14391.31
[INFO 2017-06-28 12:05:53,618 main.py:51] epoch 1209, training loss: 11909.16, average training loss: 11066.35, base loss: 14392.35
[INFO 2017-06-28 12:05:53,976 main.py:51] epoch 1210, training loss: 12494.17, average training loss: 11067.23, base loss: 14395.23
[INFO 2017-06-28 12:05:54,304 main.py:51] epoch 1211, training loss: 11058.18, average training loss: 11066.77, base loss: 14395.96
[INFO 2017-06-28 12:05:54,666 main.py:51] epoch 1212, training loss: 10692.36, average training loss: 11065.16, base loss: 14395.55
[INFO 2017-06-28 12:05:55,004 main.py:51] epoch 1213, training loss: 9169.14, average training loss: 11061.34, base loss: 14392.17
[INFO 2017-06-28 12:05:55,357 main.py:51] epoch 1214, training loss: 9888.47, average training loss: 11057.04, base loss: 14389.61
[INFO 2017-06-28 12:05:55,724 main.py:51] epoch 1215, training loss: 10492.55, average training loss: 11055.22, base loss: 14387.98
[INFO 2017-06-28 12:05:56,046 main.py:51] epoch 1216, training loss: 10935.25, average training loss: 11054.54, base loss: 14389.00
[INFO 2017-06-28 12:05:56,436 main.py:51] epoch 1217, training loss: 13480.67, average training loss: 11055.33, base loss: 14391.88
[INFO 2017-06-28 12:05:56,855 main.py:51] epoch 1218, training loss: 11337.53, average training loss: 11054.32, base loss: 14392.51
[INFO 2017-06-28 12:05:57,238 main.py:51] epoch 1219, training loss: 9380.63, average training loss: 11052.16, base loss: 14391.02
[INFO 2017-06-28 12:05:57,577 main.py:51] epoch 1220, training loss: 10636.71, average training loss: 11051.24, base loss: 14390.76
[INFO 2017-06-28 12:05:57,898 main.py:51] epoch 1221, training loss: 11399.81, average training loss: 11051.21, base loss: 14392.92
[INFO 2017-06-28 12:05:58,305 main.py:51] epoch 1222, training loss: 11231.86, average training loss: 11051.32, base loss: 14394.76
[INFO 2017-06-28 12:05:58,703 main.py:51] epoch 1223, training loss: 11825.39, average training loss: 11051.01, base loss: 14396.49
[INFO 2017-06-28 12:05:59,044 main.py:51] epoch 1224, training loss: 9345.16, average training loss: 11050.98, base loss: 14397.58
[INFO 2017-06-28 12:05:59,370 main.py:51] epoch 1225, training loss: 11523.55, average training loss: 11050.21, base loss: 14397.36
[INFO 2017-06-28 12:05:59,788 main.py:51] epoch 1226, training loss: 11917.12, average training loss: 11050.86, base loss: 14399.98
[INFO 2017-06-28 12:06:00,149 main.py:51] epoch 1227, training loss: 9826.35, average training loss: 11048.27, base loss: 14398.84
[INFO 2017-06-28 12:06:00,524 main.py:51] epoch 1228, training loss: 10346.49, average training loss: 11044.79, base loss: 14395.59
[INFO 2017-06-28 12:06:00,866 main.py:51] epoch 1229, training loss: 10483.53, average training loss: 11040.65, base loss: 14392.14
[INFO 2017-06-28 12:06:01,253 main.py:51] epoch 1230, training loss: 10526.94, average training loss: 11039.21, base loss: 14392.55
[INFO 2017-06-28 12:06:01,692 main.py:51] epoch 1231, training loss: 9987.41, average training loss: 11036.47, base loss: 14390.17
[INFO 2017-06-28 12:06:02,018 main.py:51] epoch 1232, training loss: 10543.43, average training loss: 11031.11, base loss: 14386.83
[INFO 2017-06-28 12:06:02,395 main.py:51] epoch 1233, training loss: 9418.56, average training loss: 11028.35, base loss: 14383.80
[INFO 2017-06-28 12:06:02,796 main.py:51] epoch 1234, training loss: 10013.67, average training loss: 11024.85, base loss: 14380.39
[INFO 2017-06-28 12:06:03,176 main.py:51] epoch 1235, training loss: 10314.44, average training loss: 11023.81, base loss: 14380.82
[INFO 2017-06-28 12:06:03,527 main.py:51] epoch 1236, training loss: 9591.63, average training loss: 11020.72, base loss: 14377.46
[INFO 2017-06-28 12:06:03,860 main.py:51] epoch 1237, training loss: 10753.61, average training loss: 11018.88, base loss: 14376.83
[INFO 2017-06-28 12:06:04,230 main.py:51] epoch 1238, training loss: 11101.88, average training loss: 11019.12, base loss: 14378.96
[INFO 2017-06-28 12:06:04,551 main.py:51] epoch 1239, training loss: 9781.46, average training loss: 11016.73, base loss: 14376.90
[INFO 2017-06-28 12:06:04,896 main.py:51] epoch 1240, training loss: 9296.66, average training loss: 11015.05, base loss: 14373.97
[INFO 2017-06-28 12:06:05,230 main.py:51] epoch 1241, training loss: 10748.98, average training loss: 11013.80, base loss: 14372.00
[INFO 2017-06-28 12:06:05,625 main.py:51] epoch 1242, training loss: 9391.38, average training loss: 11012.97, base loss: 14372.14
[INFO 2017-06-28 12:06:05,995 main.py:51] epoch 1243, training loss: 10803.26, average training loss: 11012.65, base loss: 14373.24
[INFO 2017-06-28 12:06:06,407 main.py:51] epoch 1244, training loss: 10921.74, average training loss: 11013.32, base loss: 14376.17
[INFO 2017-06-28 12:06:06,801 main.py:51] epoch 1245, training loss: 10037.99, average training loss: 11011.00, base loss: 14375.31
[INFO 2017-06-28 12:06:07,140 main.py:51] epoch 1246, training loss: 12945.89, average training loss: 11010.78, base loss: 14377.24
[INFO 2017-06-28 12:06:07,484 main.py:51] epoch 1247, training loss: 9791.22, average training loss: 11008.74, base loss: 14375.30
[INFO 2017-06-28 12:06:07,868 main.py:51] epoch 1248, training loss: 10422.93, average training loss: 11005.44, base loss: 14373.71
[INFO 2017-06-28 12:06:08,235 main.py:51] epoch 1249, training loss: 11391.24, average training loss: 11003.63, base loss: 14373.75
[INFO 2017-06-28 12:06:08,574 main.py:51] epoch 1250, training loss: 11252.68, average training loss: 11004.64, base loss: 14376.95
[INFO 2017-06-28 12:06:08,917 main.py:51] epoch 1251, training loss: 10144.16, average training loss: 11003.00, base loss: 14377.56
[INFO 2017-06-28 12:06:09,285 main.py:51] epoch 1252, training loss: 11771.25, average training loss: 11002.99, base loss: 14378.26
[INFO 2017-06-28 12:06:09,737 main.py:51] epoch 1253, training loss: 8779.31, average training loss: 10997.84, base loss: 14372.45
[INFO 2017-06-28 12:06:10,092 main.py:51] epoch 1254, training loss: 10545.03, average training loss: 10995.56, base loss: 14371.31
[INFO 2017-06-28 12:06:10,457 main.py:51] epoch 1255, training loss: 9918.33, average training loss: 10994.40, base loss: 14370.68
[INFO 2017-06-28 12:06:10,827 main.py:51] epoch 1256, training loss: 9522.92, average training loss: 10990.56, base loss: 14368.05
[INFO 2017-06-28 12:06:11,182 main.py:51] epoch 1257, training loss: 10539.48, average training loss: 10990.33, base loss: 14368.92
[INFO 2017-06-28 12:06:11,551 main.py:51] epoch 1258, training loss: 10136.91, average training loss: 10989.94, base loss: 14369.46
[INFO 2017-06-28 12:06:11,880 main.py:51] epoch 1259, training loss: 11618.84, average training loss: 10988.75, base loss: 14368.76
[INFO 2017-06-28 12:06:12,235 main.py:51] epoch 1260, training loss: 10448.41, average training loss: 10989.51, base loss: 14370.67
[INFO 2017-06-28 12:06:12,554 main.py:51] epoch 1261, training loss: 10883.08, average training loss: 10989.38, base loss: 14371.94
[INFO 2017-06-28 12:06:12,885 main.py:51] epoch 1262, training loss: 9762.05, average training loss: 10984.46, base loss: 14367.19
[INFO 2017-06-28 12:06:13,226 main.py:51] epoch 1263, training loss: 10388.11, average training loss: 10983.74, base loss: 14367.88
[INFO 2017-06-28 12:06:13,579 main.py:51] epoch 1264, training loss: 12679.07, average training loss: 10983.60, base loss: 14369.86
[INFO 2017-06-28 12:06:13,922 main.py:51] epoch 1265, training loss: 10854.98, average training loss: 10981.23, base loss: 14368.36
[INFO 2017-06-28 12:06:14,252 main.py:51] epoch 1266, training loss: 11129.47, average training loss: 10980.12, base loss: 14368.70
[INFO 2017-06-28 12:06:14,604 main.py:51] epoch 1267, training loss: 11854.41, average training loss: 10979.50, base loss: 14370.06
[INFO 2017-06-28 12:06:14,938 main.py:51] epoch 1268, training loss: 10437.69, average training loss: 10978.36, base loss: 14369.89
[INFO 2017-06-28 12:06:15,279 main.py:51] epoch 1269, training loss: 10551.52, average training loss: 10977.48, base loss: 14368.70
[INFO 2017-06-28 12:06:15,613 main.py:51] epoch 1270, training loss: 12810.72, average training loss: 10977.94, base loss: 14370.70
[INFO 2017-06-28 12:06:15,969 main.py:51] epoch 1271, training loss: 10002.75, average training loss: 10975.12, base loss: 14367.57
[INFO 2017-06-28 12:06:16,317 main.py:51] epoch 1272, training loss: 11006.40, average training loss: 10972.69, base loss: 14365.94
[INFO 2017-06-28 12:06:16,639 main.py:51] epoch 1273, training loss: 10022.48, average training loss: 10970.84, base loss: 14364.21
[INFO 2017-06-28 12:06:16,973 main.py:51] epoch 1274, training loss: 9963.12, average training loss: 10967.49, base loss: 14361.42
[INFO 2017-06-28 12:06:17,294 main.py:51] epoch 1275, training loss: 12818.79, average training loss: 10969.78, base loss: 14364.04
[INFO 2017-06-28 12:06:17,643 main.py:51] epoch 1276, training loss: 9783.38, average training loss: 10966.73, base loss: 14359.77
[INFO 2017-06-28 12:06:17,994 main.py:51] epoch 1277, training loss: 11489.42, average training loss: 10966.05, base loss: 14359.63
[INFO 2017-06-28 12:06:18,358 main.py:51] epoch 1278, training loss: 9332.32, average training loss: 10961.82, base loss: 14355.01
[INFO 2017-06-28 12:06:18,702 main.py:51] epoch 1279, training loss: 9346.20, average training loss: 10959.37, base loss: 14352.15
[INFO 2017-06-28 12:06:19,106 main.py:51] epoch 1280, training loss: 9861.43, average training loss: 10957.60, base loss: 14351.89
[INFO 2017-06-28 12:06:19,470 main.py:51] epoch 1281, training loss: 9966.86, average training loss: 10953.93, base loss: 14348.48
[INFO 2017-06-28 12:06:19,826 main.py:51] epoch 1282, training loss: 10793.20, average training loss: 10951.64, base loss: 14347.57
[INFO 2017-06-28 12:06:20,172 main.py:51] epoch 1283, training loss: 11681.47, average training loss: 10951.35, base loss: 14350.31
[INFO 2017-06-28 12:06:20,591 main.py:51] epoch 1284, training loss: 12695.49, average training loss: 10952.22, base loss: 14352.44
[INFO 2017-06-28 12:06:20,975 main.py:51] epoch 1285, training loss: 11782.48, average training loss: 10950.25, base loss: 14350.01
[INFO 2017-06-28 12:06:21,419 main.py:51] epoch 1286, training loss: 10137.10, average training loss: 10948.49, base loss: 14348.24
[INFO 2017-06-28 12:06:21,872 main.py:51] epoch 1287, training loss: 11073.29, average training loss: 10947.63, base loss: 14348.34
[INFO 2017-06-28 12:06:22,255 main.py:51] epoch 1288, training loss: 10672.15, average training loss: 10944.57, base loss: 14346.16
[INFO 2017-06-28 12:06:22,579 main.py:51] epoch 1289, training loss: 11171.75, average training loss: 10944.19, base loss: 14346.40
[INFO 2017-06-28 12:06:22,931 main.py:51] epoch 1290, training loss: 9558.07, average training loss: 10942.23, base loss: 14344.42
[INFO 2017-06-28 12:06:23,297 main.py:51] epoch 1291, training loss: 9247.47, average training loss: 10940.70, base loss: 14343.05
[INFO 2017-06-28 12:06:23,708 main.py:51] epoch 1292, training loss: 11133.13, average training loss: 10941.29, base loss: 14347.14
[INFO 2017-06-28 12:06:24,083 main.py:51] epoch 1293, training loss: 11096.72, average training loss: 10940.96, base loss: 14348.71
[INFO 2017-06-28 12:06:24,426 main.py:51] epoch 1294, training loss: 11058.52, average training loss: 10939.31, base loss: 14348.19
[INFO 2017-06-28 12:06:24,767 main.py:51] epoch 1295, training loss: 9631.23, average training loss: 10936.33, base loss: 14345.83
[INFO 2017-06-28 12:06:25,140 main.py:51] epoch 1296, training loss: 10747.06, average training loss: 10937.72, base loss: 14349.28
[INFO 2017-06-28 12:06:25,589 main.py:51] epoch 1297, training loss: 9609.90, average training loss: 10936.13, base loss: 14347.31
[INFO 2017-06-28 12:06:25,967 main.py:51] epoch 1298, training loss: 10042.99, average training loss: 10932.56, base loss: 14343.44
[INFO 2017-06-28 12:06:26,355 main.py:51] epoch 1299, training loss: 11713.27, average training loss: 10934.22, base loss: 14347.78
[INFO 2017-06-28 12:06:26,355 main.py:53] epoch 1299, testing
[INFO 2017-06-28 12:06:28,107 main.py:105] average testing loss: 11003.89, base loss: 14314.56
[INFO 2017-06-28 12:06:28,108 main.py:106] improve_loss: 3310.68, improve_percent: 0.23
[INFO 2017-06-28 12:06:28,108 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:06:28,464 main.py:51] epoch 1300, training loss: 9630.25, average training loss: 10932.64, base loss: 14347.42
[INFO 2017-06-28 12:06:28,798 main.py:51] epoch 1301, training loss: 10496.89, average training loss: 10929.22, base loss: 14345.22
[INFO 2017-06-28 12:06:29,126 main.py:51] epoch 1302, training loss: 8762.27, average training loss: 10928.11, base loss: 14345.79
[INFO 2017-06-28 12:06:29,519 main.py:51] epoch 1303, training loss: 9749.15, average training loss: 10928.55, base loss: 14348.29
[INFO 2017-06-28 12:06:29,882 main.py:51] epoch 1304, training loss: 8877.55, average training loss: 10926.24, base loss: 14345.22
[INFO 2017-06-28 12:06:30,231 main.py:51] epoch 1305, training loss: 9118.06, average training loss: 10920.08, base loss: 14338.98
[INFO 2017-06-28 12:06:30,654 main.py:51] epoch 1306, training loss: 9934.31, average training loss: 10916.47, base loss: 14334.72
[INFO 2017-06-28 12:06:31,081 main.py:51] epoch 1307, training loss: 8562.68, average training loss: 10913.33, base loss: 14331.40
[INFO 2017-06-28 12:06:31,469 main.py:51] epoch 1308, training loss: 11774.57, average training loss: 10913.33, base loss: 14331.26
[INFO 2017-06-28 12:06:31,858 main.py:51] epoch 1309, training loss: 10551.56, average training loss: 10912.29, base loss: 14330.56
[INFO 2017-06-28 12:06:32,210 main.py:51] epoch 1310, training loss: 11172.13, average training loss: 10913.91, base loss: 14333.31
[INFO 2017-06-28 12:06:32,605 main.py:51] epoch 1311, training loss: 10493.48, average training loss: 10912.65, base loss: 14332.64
[INFO 2017-06-28 12:06:32,958 main.py:51] epoch 1312, training loss: 10138.77, average training loss: 10911.33, base loss: 14333.37
[INFO 2017-06-28 12:06:33,328 main.py:51] epoch 1313, training loss: 10668.84, average training loss: 10911.45, base loss: 14334.89
[INFO 2017-06-28 12:06:33,762 main.py:51] epoch 1314, training loss: 9311.76, average training loss: 10907.83, base loss: 14330.90
[INFO 2017-06-28 12:06:34,186 main.py:51] epoch 1315, training loss: 10131.36, average training loss: 10906.83, base loss: 14330.94
[INFO 2017-06-28 12:06:34,534 main.py:51] epoch 1316, training loss: 9573.85, average training loss: 10905.11, base loss: 14329.50
[INFO 2017-06-28 12:06:34,968 main.py:51] epoch 1317, training loss: 9536.24, average training loss: 10904.23, base loss: 14328.33
[INFO 2017-06-28 12:06:35,358 main.py:51] epoch 1318, training loss: 10796.34, average training loss: 10904.14, base loss: 14329.18
[INFO 2017-06-28 12:06:35,691 main.py:51] epoch 1319, training loss: 9357.07, average training loss: 10902.58, base loss: 14328.33
[INFO 2017-06-28 12:06:36,089 main.py:51] epoch 1320, training loss: 10151.77, average training loss: 10901.97, base loss: 14328.66
[INFO 2017-06-28 12:06:36,474 main.py:51] epoch 1321, training loss: 11703.51, average training loss: 10898.67, base loss: 14325.79
[INFO 2017-06-28 12:06:36,830 main.py:51] epoch 1322, training loss: 10111.17, average training loss: 10898.62, base loss: 14326.85
[INFO 2017-06-28 12:06:37,216 main.py:51] epoch 1323, training loss: 11755.05, average training loss: 10898.85, base loss: 14326.93
[INFO 2017-06-28 12:06:37,590 main.py:51] epoch 1324, training loss: 8783.98, average training loss: 10897.58, base loss: 14327.17
[INFO 2017-06-28 12:06:37,929 main.py:51] epoch 1325, training loss: 10539.89, average training loss: 10897.94, base loss: 14329.06
[INFO 2017-06-28 12:06:38,264 main.py:51] epoch 1326, training loss: 10681.29, average training loss: 10897.90, base loss: 14330.79
[INFO 2017-06-28 12:06:38,638 main.py:51] epoch 1327, training loss: 11324.55, average training loss: 10897.42, base loss: 14330.85
[INFO 2017-06-28 12:06:39,021 main.py:51] epoch 1328, training loss: 10797.83, average training loss: 10896.78, base loss: 14331.69
[INFO 2017-06-28 12:06:39,364 main.py:51] epoch 1329, training loss: 9304.90, average training loss: 10895.70, base loss: 14331.17
[INFO 2017-06-28 12:06:39,742 main.py:51] epoch 1330, training loss: 9748.85, average training loss: 10894.07, base loss: 14329.83
[INFO 2017-06-28 12:06:40,204 main.py:51] epoch 1331, training loss: 8742.24, average training loss: 10891.60, base loss: 14327.97
[INFO 2017-06-28 12:06:40,534 main.py:51] epoch 1332, training loss: 10747.99, average training loss: 10891.72, base loss: 14328.72
[INFO 2017-06-28 12:06:40,904 main.py:51] epoch 1333, training loss: 13276.28, average training loss: 10892.83, base loss: 14330.94
[INFO 2017-06-28 12:06:41,337 main.py:51] epoch 1334, training loss: 10093.19, average training loss: 10891.91, base loss: 14330.39
[INFO 2017-06-28 12:06:41,716 main.py:51] epoch 1335, training loss: 11176.66, average training loss: 10893.23, base loss: 14334.59
[INFO 2017-06-28 12:06:42,088 main.py:51] epoch 1336, training loss: 9072.45, average training loss: 10891.92, base loss: 14334.16
[INFO 2017-06-28 12:06:42,487 main.py:51] epoch 1337, training loss: 11571.67, average training loss: 10892.90, base loss: 14336.33
[INFO 2017-06-28 12:06:42,858 main.py:51] epoch 1338, training loss: 11068.12, average training loss: 10892.26, base loss: 14337.51
[INFO 2017-06-28 12:06:43,189 main.py:51] epoch 1339, training loss: 9760.38, average training loss: 10891.56, base loss: 14337.14
[INFO 2017-06-28 12:06:43,537 main.py:51] epoch 1340, training loss: 12095.52, average training loss: 10892.93, base loss: 14339.92
[INFO 2017-06-28 12:06:43,924 main.py:51] epoch 1341, training loss: 10064.59, average training loss: 10891.44, base loss: 14339.41
[INFO 2017-06-28 12:06:44,282 main.py:51] epoch 1342, training loss: 9365.55, average training loss: 10890.87, base loss: 14340.09
[INFO 2017-06-28 12:06:44,624 main.py:51] epoch 1343, training loss: 9339.82, average training loss: 10889.26, base loss: 14338.19
[INFO 2017-06-28 12:06:44,975 main.py:51] epoch 1344, training loss: 10397.25, average training loss: 10887.72, base loss: 14336.96
[INFO 2017-06-28 12:06:45,334 main.py:51] epoch 1345, training loss: 11089.99, average training loss: 10887.87, base loss: 14338.18
[INFO 2017-06-28 12:06:45,679 main.py:51] epoch 1346, training loss: 11534.15, average training loss: 10887.14, base loss: 14337.66
[INFO 2017-06-28 12:06:46,056 main.py:51] epoch 1347, training loss: 9572.66, average training loss: 10885.47, base loss: 14337.93
[INFO 2017-06-28 12:06:46,401 main.py:51] epoch 1348, training loss: 10608.87, average training loss: 10883.38, base loss: 14336.29
[INFO 2017-06-28 12:06:46,786 main.py:51] epoch 1349, training loss: 11426.34, average training loss: 10884.37, base loss: 14337.71
[INFO 2017-06-28 12:06:47,179 main.py:51] epoch 1350, training loss: 9705.04, average training loss: 10884.02, base loss: 14338.47
[INFO 2017-06-28 12:06:47,522 main.py:51] epoch 1351, training loss: 9558.17, average training loss: 10883.24, base loss: 14338.39
[INFO 2017-06-28 12:06:47,889 main.py:51] epoch 1352, training loss: 13119.05, average training loss: 10886.35, base loss: 14343.11
[INFO 2017-06-28 12:06:48,240 main.py:51] epoch 1353, training loss: 10092.12, average training loss: 10882.76, base loss: 14339.38
[INFO 2017-06-28 12:06:48,712 main.py:51] epoch 1354, training loss: 11139.79, average training loss: 10881.87, base loss: 14340.30
[INFO 2017-06-28 12:06:49,126 main.py:51] epoch 1355, training loss: 11688.63, average training loss: 10883.17, base loss: 14343.92
[INFO 2017-06-28 12:06:49,546 main.py:51] epoch 1356, training loss: 12759.86, average training loss: 10885.38, base loss: 14347.08
[INFO 2017-06-28 12:06:49,924 main.py:51] epoch 1357, training loss: 10558.25, average training loss: 10886.17, base loss: 14347.57
[INFO 2017-06-28 12:06:50,267 main.py:51] epoch 1358, training loss: 11485.35, average training loss: 10884.31, base loss: 14346.55
[INFO 2017-06-28 12:06:50,637 main.py:51] epoch 1359, training loss: 10489.80, average training loss: 10883.11, base loss: 14346.25
[INFO 2017-06-28 12:06:51,032 main.py:51] epoch 1360, training loss: 11122.45, average training loss: 10882.03, base loss: 14343.80
[INFO 2017-06-28 12:06:51,375 main.py:51] epoch 1361, training loss: 12686.10, average training loss: 10884.83, base loss: 14347.70
[INFO 2017-06-28 12:06:51,743 main.py:51] epoch 1362, training loss: 12857.28, average training loss: 10887.03, base loss: 14352.21
[INFO 2017-06-28 12:06:52,094 main.py:51] epoch 1363, training loss: 12302.05, average training loss: 10887.66, base loss: 14354.12
[INFO 2017-06-28 12:06:52,441 main.py:51] epoch 1364, training loss: 11650.51, average training loss: 10886.81, base loss: 14353.79
[INFO 2017-06-28 12:06:52,774 main.py:51] epoch 1365, training loss: 9837.12, average training loss: 10884.95, base loss: 14350.81
[INFO 2017-06-28 12:06:53,118 main.py:51] epoch 1366, training loss: 11791.07, average training loss: 10884.31, base loss: 14350.57
[INFO 2017-06-28 12:06:53,457 main.py:51] epoch 1367, training loss: 11785.08, average training loss: 10886.06, base loss: 14355.81
[INFO 2017-06-28 12:06:53,823 main.py:51] epoch 1368, training loss: 11797.50, average training loss: 10887.29, base loss: 14358.12
[INFO 2017-06-28 12:06:54,167 main.py:51] epoch 1369, training loss: 10238.73, average training loss: 10884.59, base loss: 14356.17
[INFO 2017-06-28 12:06:54,506 main.py:51] epoch 1370, training loss: 11826.02, average training loss: 10885.49, base loss: 14359.48
[INFO 2017-06-28 12:06:54,843 main.py:51] epoch 1371, training loss: 11408.82, average training loss: 10885.39, base loss: 14359.34
[INFO 2017-06-28 12:06:55,191 main.py:51] epoch 1372, training loss: 10544.77, average training loss: 10884.11, base loss: 14358.25
[INFO 2017-06-28 12:06:55,525 main.py:51] epoch 1373, training loss: 10628.87, average training loss: 10881.31, base loss: 14355.45
[INFO 2017-06-28 12:06:55,849 main.py:51] epoch 1374, training loss: 12517.16, average training loss: 10883.77, base loss: 14359.93
[INFO 2017-06-28 12:06:56,193 main.py:51] epoch 1375, training loss: 11020.65, average training loss: 10884.64, base loss: 14362.22
[INFO 2017-06-28 12:06:56,527 main.py:51] epoch 1376, training loss: 10568.76, average training loss: 10884.07, base loss: 14361.33
[INFO 2017-06-28 12:06:56,860 main.py:51] epoch 1377, training loss: 10790.77, average training loss: 10883.80, base loss: 14363.12
[INFO 2017-06-28 12:06:57,195 main.py:51] epoch 1378, training loss: 10018.13, average training loss: 10883.65, base loss: 14365.28
[INFO 2017-06-28 12:06:57,548 main.py:51] epoch 1379, training loss: 12361.66, average training loss: 10884.87, base loss: 14368.02
[INFO 2017-06-28 12:06:57,908 main.py:51] epoch 1380, training loss: 11311.12, average training loss: 10884.92, base loss: 14368.97
[INFO 2017-06-28 12:06:58,264 main.py:51] epoch 1381, training loss: 11870.51, average training loss: 10885.54, base loss: 14371.20
[INFO 2017-06-28 12:06:58,619 main.py:51] epoch 1382, training loss: 10924.40, average training loss: 10886.78, base loss: 14373.94
[INFO 2017-06-28 12:06:59,031 main.py:51] epoch 1383, training loss: 11828.84, average training loss: 10887.89, base loss: 14378.78
[INFO 2017-06-28 12:06:59,414 main.py:51] epoch 1384, training loss: 10051.35, average training loss: 10885.82, base loss: 14376.76
[INFO 2017-06-28 12:06:59,756 main.py:51] epoch 1385, training loss: 9709.60, average training loss: 10884.61, base loss: 14376.35
[INFO 2017-06-28 12:07:00,108 main.py:51] epoch 1386, training loss: 10043.91, average training loss: 10882.79, base loss: 14376.20
[INFO 2017-06-28 12:07:00,522 main.py:51] epoch 1387, training loss: 12459.42, average training loss: 10883.31, base loss: 14380.09
[INFO 2017-06-28 12:07:00,916 main.py:51] epoch 1388, training loss: 11724.73, average training loss: 10881.65, base loss: 14378.15
[INFO 2017-06-28 12:07:01,237 main.py:51] epoch 1389, training loss: 11886.50, average training loss: 10882.03, base loss: 14379.15
[INFO 2017-06-28 12:07:01,584 main.py:51] epoch 1390, training loss: 10344.86, average training loss: 10880.80, base loss: 14378.60
[INFO 2017-06-28 12:07:02,028 main.py:51] epoch 1391, training loss: 11504.76, average training loss: 10879.74, base loss: 14377.70
[INFO 2017-06-28 12:07:02,423 main.py:51] epoch 1392, training loss: 10963.10, average training loss: 10877.33, base loss: 14376.30
[INFO 2017-06-28 12:07:02,879 main.py:51] epoch 1393, training loss: 10913.56, average training loss: 10877.17, base loss: 14377.56
[INFO 2017-06-28 12:07:03,297 main.py:51] epoch 1394, training loss: 12005.93, average training loss: 10879.45, base loss: 14380.96
[INFO 2017-06-28 12:07:03,697 main.py:51] epoch 1395, training loss: 11290.23, average training loss: 10877.47, base loss: 14379.84
[INFO 2017-06-28 12:07:04,106 main.py:51] epoch 1396, training loss: 11266.70, average training loss: 10877.70, base loss: 14379.74
[INFO 2017-06-28 12:07:04,545 main.py:51] epoch 1397, training loss: 11668.49, average training loss: 10878.51, base loss: 14381.39
[INFO 2017-06-28 12:07:04,912 main.py:51] epoch 1398, training loss: 9072.55, average training loss: 10876.39, base loss: 14380.20
[INFO 2017-06-28 12:07:05,318 main.py:51] epoch 1399, training loss: 10797.42, average training loss: 10874.93, base loss: 14380.34
[INFO 2017-06-28 12:07:05,318 main.py:53] epoch 1399, testing
[INFO 2017-06-28 12:07:07,155 main.py:105] average testing loss: 11745.74, base loss: 15164.20
[INFO 2017-06-28 12:07:07,156 main.py:106] improve_loss: 3418.46, improve_percent: 0.23
[INFO 2017-06-28 12:07:07,157 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:07:07,537 main.py:51] epoch 1400, training loss: 10447.56, average training loss: 10872.96, base loss: 14379.74
[INFO 2017-06-28 12:07:07,864 main.py:51] epoch 1401, training loss: 9945.25, average training loss: 10871.60, base loss: 14378.34
[INFO 2017-06-28 12:07:08,196 main.py:51] epoch 1402, training loss: 9566.16, average training loss: 10870.51, base loss: 14377.37
[INFO 2017-06-28 12:07:08,565 main.py:51] epoch 1403, training loss: 11210.11, average training loss: 10870.43, base loss: 14377.00
[INFO 2017-06-28 12:07:09,087 main.py:51] epoch 1404, training loss: 10470.88, average training loss: 10870.37, base loss: 14378.53
[INFO 2017-06-28 12:07:09,414 main.py:51] epoch 1405, training loss: 9500.62, average training loss: 10868.28, base loss: 14375.73
[INFO 2017-06-28 12:07:09,776 main.py:51] epoch 1406, training loss: 10053.06, average training loss: 10867.20, base loss: 14373.83
[INFO 2017-06-28 12:07:10,115 main.py:51] epoch 1407, training loss: 10043.90, average training loss: 10865.26, base loss: 14371.67
[INFO 2017-06-28 12:07:10,448 main.py:51] epoch 1408, training loss: 11776.21, average training loss: 10864.49, base loss: 14368.98
[INFO 2017-06-28 12:07:10,813 main.py:51] epoch 1409, training loss: 11492.25, average training loss: 10865.26, base loss: 14370.16
[INFO 2017-06-28 12:07:11,154 main.py:51] epoch 1410, training loss: 13261.28, average training loss: 10868.18, base loss: 14374.34
[INFO 2017-06-28 12:07:11,485 main.py:51] epoch 1411, training loss: 10187.71, average training loss: 10867.14, base loss: 14374.64
[INFO 2017-06-28 12:07:11,824 main.py:51] epoch 1412, training loss: 10359.15, average training loss: 10866.70, base loss: 14374.55
[INFO 2017-06-28 12:07:12,175 main.py:51] epoch 1413, training loss: 10327.75, average training loss: 10865.72, base loss: 14374.83
[INFO 2017-06-28 12:07:12,502 main.py:51] epoch 1414, training loss: 10266.26, average training loss: 10864.55, base loss: 14373.71
[INFO 2017-06-28 12:07:12,852 main.py:51] epoch 1415, training loss: 10244.64, average training loss: 10861.24, base loss: 14371.05
[INFO 2017-06-28 12:07:13,229 main.py:51] epoch 1416, training loss: 9552.95, average training loss: 10859.47, base loss: 14369.26
[INFO 2017-06-28 12:07:13,574 main.py:51] epoch 1417, training loss: 10645.49, average training loss: 10860.31, base loss: 14371.34
[INFO 2017-06-28 12:07:13,927 main.py:51] epoch 1418, training loss: 10436.84, average training loss: 10859.00, base loss: 14371.64
[INFO 2017-06-28 12:07:14,281 main.py:51] epoch 1419, training loss: 10864.25, average training loss: 10857.29, base loss: 14371.31
[INFO 2017-06-28 12:07:14,619 main.py:51] epoch 1420, training loss: 9314.93, average training loss: 10856.26, base loss: 14370.72
[INFO 2017-06-28 12:07:14,949 main.py:51] epoch 1421, training loss: 10242.23, average training loss: 10855.59, base loss: 14371.25
[INFO 2017-06-28 12:07:15,288 main.py:51] epoch 1422, training loss: 12687.69, average training loss: 10856.56, base loss: 14375.56
[INFO 2017-06-28 12:07:15,620 main.py:51] epoch 1423, training loss: 10415.53, average training loss: 10855.95, base loss: 14375.45
[INFO 2017-06-28 12:07:15,965 main.py:51] epoch 1424, training loss: 11280.86, average training loss: 10855.78, base loss: 14376.87
[INFO 2017-06-28 12:07:16,311 main.py:51] epoch 1425, training loss: 10561.04, average training loss: 10854.99, base loss: 14375.17
[INFO 2017-06-28 12:07:16,649 main.py:51] epoch 1426, training loss: 10583.24, average training loss: 10854.14, base loss: 14374.72
[INFO 2017-06-28 12:07:17,003 main.py:51] epoch 1427, training loss: 9962.88, average training loss: 10851.32, base loss: 14371.31
[INFO 2017-06-28 12:07:17,340 main.py:51] epoch 1428, training loss: 11361.51, average training loss: 10851.31, base loss: 14371.98
[INFO 2017-06-28 12:07:17,743 main.py:51] epoch 1429, training loss: 11719.46, average training loss: 10852.30, base loss: 14373.90
[INFO 2017-06-28 12:07:18,128 main.py:51] epoch 1430, training loss: 10012.49, average training loss: 10850.60, base loss: 14372.57
[INFO 2017-06-28 12:07:18,508 main.py:51] epoch 1431, training loss: 11866.63, average training loss: 10852.88, base loss: 14376.62
[INFO 2017-06-28 12:07:18,832 main.py:51] epoch 1432, training loss: 10708.80, average training loss: 10853.55, base loss: 14377.18
[INFO 2017-06-28 12:07:19,243 main.py:51] epoch 1433, training loss: 10226.95, average training loss: 10853.32, base loss: 14378.19
[INFO 2017-06-28 12:07:19,638 main.py:51] epoch 1434, training loss: 9675.81, average training loss: 10852.19, base loss: 14378.84
[INFO 2017-06-28 12:07:19,994 main.py:51] epoch 1435, training loss: 10677.43, average training loss: 10851.61, base loss: 14379.76
[INFO 2017-06-28 12:07:20,349 main.py:51] epoch 1436, training loss: 9401.17, average training loss: 10850.05, base loss: 14378.58
[INFO 2017-06-28 12:07:20,712 main.py:51] epoch 1437, training loss: 11705.85, average training loss: 10849.85, base loss: 14380.90
[INFO 2017-06-28 12:07:21,045 main.py:51] epoch 1438, training loss: 10145.44, average training loss: 10849.58, base loss: 14380.48
[INFO 2017-06-28 12:07:21,475 main.py:51] epoch 1439, training loss: 9726.48, average training loss: 10846.29, base loss: 14376.87
[INFO 2017-06-28 12:07:21,830 main.py:51] epoch 1440, training loss: 9802.92, average training loss: 10847.00, base loss: 14379.08
[INFO 2017-06-28 12:07:22,178 main.py:51] epoch 1441, training loss: 10397.73, average training loss: 10846.71, base loss: 14379.02
[INFO 2017-06-28 12:07:22,577 main.py:51] epoch 1442, training loss: 11301.18, average training loss: 10845.08, base loss: 14379.56
[INFO 2017-06-28 12:07:22,934 main.py:51] epoch 1443, training loss: 10979.51, average training loss: 10844.98, base loss: 14380.40
[INFO 2017-06-28 12:07:23,345 main.py:51] epoch 1444, training loss: 9130.51, average training loss: 10841.90, base loss: 14377.79
[INFO 2017-06-28 12:07:23,728 main.py:51] epoch 1445, training loss: 10324.27, average training loss: 10839.87, base loss: 14375.35
[INFO 2017-06-28 12:07:24,070 main.py:51] epoch 1446, training loss: 11384.27, average training loss: 10839.96, base loss: 14374.94
[INFO 2017-06-28 12:07:24,454 main.py:51] epoch 1447, training loss: 10242.98, average training loss: 10838.10, base loss: 14374.15
[INFO 2017-06-28 12:07:24,863 main.py:51] epoch 1448, training loss: 10388.17, average training loss: 10837.70, base loss: 14374.88
[INFO 2017-06-28 12:07:25,192 main.py:51] epoch 1449, training loss: 9601.83, average training loss: 10837.17, base loss: 14373.47
[INFO 2017-06-28 12:07:25,582 main.py:51] epoch 1450, training loss: 9552.08, average training loss: 10836.17, base loss: 14372.57
[INFO 2017-06-28 12:07:25,999 main.py:51] epoch 1451, training loss: 9484.92, average training loss: 10831.39, base loss: 14366.99
[INFO 2017-06-28 12:07:26,350 main.py:51] epoch 1452, training loss: 11473.46, average training loss: 10833.32, base loss: 14368.96
[INFO 2017-06-28 12:07:26,746 main.py:51] epoch 1453, training loss: 10213.77, average training loss: 10833.56, base loss: 14371.64
[INFO 2017-06-28 12:07:27,112 main.py:51] epoch 1454, training loss: 9393.83, average training loss: 10831.98, base loss: 14370.10
[INFO 2017-06-28 12:07:27,455 main.py:51] epoch 1455, training loss: 12040.85, average training loss: 10832.73, base loss: 14372.18
[INFO 2017-06-28 12:07:27,863 main.py:51] epoch 1456, training loss: 11081.20, average training loss: 10831.21, base loss: 14369.97
[INFO 2017-06-28 12:07:28,251 main.py:51] epoch 1457, training loss: 10220.19, average training loss: 10829.62, base loss: 14368.89
[INFO 2017-06-28 12:07:28,608 main.py:51] epoch 1458, training loss: 9820.09, average training loss: 10828.79, base loss: 14369.80
[INFO 2017-06-28 12:07:29,073 main.py:51] epoch 1459, training loss: 9712.05, average training loss: 10827.44, base loss: 14368.65
[INFO 2017-06-28 12:07:29,431 main.py:51] epoch 1460, training loss: 11166.17, average training loss: 10825.43, base loss: 14367.80
[INFO 2017-06-28 12:07:29,798 main.py:51] epoch 1461, training loss: 9374.11, average training loss: 10823.34, base loss: 14365.57
[INFO 2017-06-28 12:07:30,133 main.py:51] epoch 1462, training loss: 11085.85, average training loss: 10822.55, base loss: 14365.08
[INFO 2017-06-28 12:07:30,465 main.py:51] epoch 1463, training loss: 10413.49, average training loss: 10822.36, base loss: 14365.69
[INFO 2017-06-28 12:07:30,809 main.py:51] epoch 1464, training loss: 10122.10, average training loss: 10821.19, base loss: 14365.37
[INFO 2017-06-28 12:07:31,166 main.py:51] epoch 1465, training loss: 10177.10, average training loss: 10820.13, base loss: 14364.52
[INFO 2017-06-28 12:07:31,571 main.py:51] epoch 1466, training loss: 9832.77, average training loss: 10817.94, base loss: 14362.56
[INFO 2017-06-28 12:07:31,981 main.py:51] epoch 1467, training loss: 12544.84, average training loss: 10817.75, base loss: 14364.71
[INFO 2017-06-28 12:07:32,330 main.py:51] epoch 1468, training loss: 10380.71, average training loss: 10817.65, base loss: 14365.67
[INFO 2017-06-28 12:07:32,686 main.py:51] epoch 1469, training loss: 10920.68, average training loss: 10816.77, base loss: 14366.86
[INFO 2017-06-28 12:07:33,116 main.py:51] epoch 1470, training loss: 10706.73, average training loss: 10815.71, base loss: 14365.12
[INFO 2017-06-28 12:07:33,464 main.py:51] epoch 1471, training loss: 11668.39, average training loss: 10815.13, base loss: 14364.55
[INFO 2017-06-28 12:07:33,881 main.py:51] epoch 1472, training loss: 10747.49, average training loss: 10815.81, base loss: 14366.43
[INFO 2017-06-28 12:07:34,218 main.py:51] epoch 1473, training loss: 11007.32, average training loss: 10814.74, base loss: 14365.47
[INFO 2017-06-28 12:07:34,610 main.py:51] epoch 1474, training loss: 11876.07, average training loss: 10815.64, base loss: 14367.74
[INFO 2017-06-28 12:07:34,947 main.py:51] epoch 1475, training loss: 11445.25, average training loss: 10814.92, base loss: 14367.12
[INFO 2017-06-28 12:07:35,295 main.py:51] epoch 1476, training loss: 9978.23, average training loss: 10814.70, base loss: 14367.67
[INFO 2017-06-28 12:07:35,650 main.py:51] epoch 1477, training loss: 10009.07, average training loss: 10812.44, base loss: 14367.15
[INFO 2017-06-28 12:07:36,035 main.py:51] epoch 1478, training loss: 10243.11, average training loss: 10810.94, base loss: 14365.54
[INFO 2017-06-28 12:07:36,440 main.py:51] epoch 1479, training loss: 9756.51, average training loss: 10809.44, base loss: 14363.41
[INFO 2017-06-28 12:07:36,849 main.py:51] epoch 1480, training loss: 10942.56, average training loss: 10807.48, base loss: 14363.71
[INFO 2017-06-28 12:07:37,228 main.py:51] epoch 1481, training loss: 10003.90, average training loss: 10803.37, base loss: 14360.33
[INFO 2017-06-28 12:07:37,672 main.py:51] epoch 1482, training loss: 9651.75, average training loss: 10802.21, base loss: 14359.70
[INFO 2017-06-28 12:07:38,065 main.py:51] epoch 1483, training loss: 10305.65, average training loss: 10800.50, base loss: 14359.48
[INFO 2017-06-28 12:07:38,411 main.py:51] epoch 1484, training loss: 11578.31, average training loss: 10800.87, base loss: 14362.69
[INFO 2017-06-28 12:07:38,805 main.py:51] epoch 1485, training loss: 12455.51, average training loss: 10801.72, base loss: 14364.74
[INFO 2017-06-28 12:07:39,244 main.py:51] epoch 1486, training loss: 10342.57, average training loss: 10800.80, base loss: 14363.56
[INFO 2017-06-28 12:07:39,583 main.py:51] epoch 1487, training loss: 10130.13, average training loss: 10799.76, base loss: 14364.45
[INFO 2017-06-28 12:07:39,991 main.py:51] epoch 1488, training loss: 10400.38, average training loss: 10799.53, base loss: 14365.19
[INFO 2017-06-28 12:07:40,421 main.py:51] epoch 1489, training loss: 9162.75, average training loss: 10797.71, base loss: 14363.28
[INFO 2017-06-28 12:07:40,819 main.py:51] epoch 1490, training loss: 10315.49, average training loss: 10797.50, base loss: 14364.12
[INFO 2017-06-28 12:07:41,198 main.py:51] epoch 1491, training loss: 10773.26, average training loss: 10797.82, base loss: 14364.49
[INFO 2017-06-28 12:07:41,551 main.py:51] epoch 1492, training loss: 10151.78, average training loss: 10796.76, base loss: 14364.56
[INFO 2017-06-28 12:07:41,932 main.py:51] epoch 1493, training loss: 10189.92, average training loss: 10796.26, base loss: 14364.82
[INFO 2017-06-28 12:07:42,359 main.py:51] epoch 1494, training loss: 11584.93, average training loss: 10797.42, base loss: 14366.14
[INFO 2017-06-28 12:07:42,714 main.py:51] epoch 1495, training loss: 11416.85, average training loss: 10795.95, base loss: 14363.68
[INFO 2017-06-28 12:07:43,109 main.py:51] epoch 1496, training loss: 9662.48, average training loss: 10794.19, base loss: 14361.69
[INFO 2017-06-28 12:07:43,455 main.py:51] epoch 1497, training loss: 10259.90, average training loss: 10794.32, base loss: 14361.32
[INFO 2017-06-28 12:07:43,835 main.py:51] epoch 1498, training loss: 10430.49, average training loss: 10794.35, base loss: 14362.42
[INFO 2017-06-28 12:07:44,228 main.py:51] epoch 1499, training loss: 10397.50, average training loss: 10793.53, base loss: 14361.90
[INFO 2017-06-28 12:07:44,229 main.py:53] epoch 1499, testing
[INFO 2017-06-28 12:07:46,087 main.py:105] average testing loss: 11761.68, base loss: 15301.76
[INFO 2017-06-28 12:07:46,087 main.py:106] improve_loss: 3540.08, improve_percent: 0.23
[INFO 2017-06-28 12:07:46,088 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:07:46,431 main.py:51] epoch 1500, training loss: 9348.21, average training loss: 10792.71, base loss: 14361.40
[INFO 2017-06-28 12:07:46,810 main.py:51] epoch 1501, training loss: 10462.65, average training loss: 10791.88, base loss: 14361.55
[INFO 2017-06-28 12:07:47,194 main.py:51] epoch 1502, training loss: 11906.34, average training loss: 10793.34, base loss: 14364.53
[INFO 2017-06-28 12:07:47,544 main.py:51] epoch 1503, training loss: 9186.98, average training loss: 10788.97, base loss: 14358.68
[INFO 2017-06-28 12:07:47,957 main.py:51] epoch 1504, training loss: 9138.50, average training loss: 10787.82, base loss: 14357.67
[INFO 2017-06-28 12:07:48,340 main.py:51] epoch 1505, training loss: 10411.25, average training loss: 10786.80, base loss: 14355.60
[INFO 2017-06-28 12:07:48,703 main.py:51] epoch 1506, training loss: 11011.03, average training loss: 10787.69, base loss: 14357.10
[INFO 2017-06-28 12:07:49,044 main.py:51] epoch 1507, training loss: 9567.85, average training loss: 10787.17, base loss: 14357.07
[INFO 2017-06-28 12:07:49,464 main.py:51] epoch 1508, training loss: 12258.63, average training loss: 10789.11, base loss: 14360.33
[INFO 2017-06-28 12:07:49,888 main.py:51] epoch 1509, training loss: 9295.80, average training loss: 10787.75, base loss: 14358.44
[INFO 2017-06-28 12:07:50,253 main.py:51] epoch 1510, training loss: 9511.65, average training loss: 10786.29, base loss: 14355.55
[INFO 2017-06-28 12:07:50,637 main.py:51] epoch 1511, training loss: 11332.82, average training loss: 10786.21, base loss: 14356.77
[INFO 2017-06-28 12:07:51,021 main.py:51] epoch 1512, training loss: 8914.05, average training loss: 10784.40, base loss: 14355.44
[INFO 2017-06-28 12:07:51,394 main.py:51] epoch 1513, training loss: 10609.85, average training loss: 10784.61, base loss: 14356.79
[INFO 2017-06-28 12:07:51,741 main.py:51] epoch 1514, training loss: 10601.42, average training loss: 10782.74, base loss: 14355.53
[INFO 2017-06-28 12:07:52,135 main.py:51] epoch 1515, training loss: 10975.41, average training loss: 10783.02, base loss: 14357.57
[INFO 2017-06-28 12:07:52,501 main.py:51] epoch 1516, training loss: 12116.87, average training loss: 10782.54, base loss: 14356.64
[INFO 2017-06-28 12:07:52,875 main.py:51] epoch 1517, training loss: 11925.22, average training loss: 10783.29, base loss: 14357.92
[INFO 2017-06-28 12:07:53,299 main.py:51] epoch 1518, training loss: 10644.10, average training loss: 10782.78, base loss: 14358.76
[INFO 2017-06-28 12:07:53,688 main.py:51] epoch 1519, training loss: 10245.75, average training loss: 10781.74, base loss: 14358.53
[INFO 2017-06-28 12:07:54,052 main.py:51] epoch 1520, training loss: 10086.78, average training loss: 10780.20, base loss: 14359.44
[INFO 2017-06-28 12:07:54,420 main.py:51] epoch 1521, training loss: 13080.76, average training loss: 10781.56, base loss: 14362.02
[INFO 2017-06-28 12:07:54,822 main.py:51] epoch 1522, training loss: 10071.03, average training loss: 10780.56, base loss: 14361.86
[INFO 2017-06-28 12:07:55,184 main.py:51] epoch 1523, training loss: 9246.71, average training loss: 10779.06, base loss: 14361.33
[INFO 2017-06-28 12:07:55,526 main.py:51] epoch 1524, training loss: 10283.54, average training loss: 10777.92, base loss: 14360.08
[INFO 2017-06-28 12:07:55,868 main.py:51] epoch 1525, training loss: 9694.20, average training loss: 10776.06, base loss: 14357.60
[INFO 2017-06-28 12:07:56,225 main.py:51] epoch 1526, training loss: 9702.88, average training loss: 10775.41, base loss: 14357.02
[INFO 2017-06-28 12:07:56,576 main.py:51] epoch 1527, training loss: 11412.80, average training loss: 10776.72, base loss: 14358.95
[INFO 2017-06-28 12:07:56,965 main.py:51] epoch 1528, training loss: 10973.70, average training loss: 10774.05, base loss: 14357.00
[INFO 2017-06-28 12:07:57,339 main.py:51] epoch 1529, training loss: 12643.39, average training loss: 10774.98, base loss: 14358.16
[INFO 2017-06-28 12:07:57,694 main.py:51] epoch 1530, training loss: 11125.35, average training loss: 10773.17, base loss: 14357.26
[INFO 2017-06-28 12:07:58,030 main.py:51] epoch 1531, training loss: 11774.04, average training loss: 10773.62, base loss: 14359.38
[INFO 2017-06-28 12:07:58,376 main.py:51] epoch 1532, training loss: 9631.64, average training loss: 10772.22, base loss: 14358.03
[INFO 2017-06-28 12:07:58,718 main.py:51] epoch 1533, training loss: 10927.55, average training loss: 10769.57, base loss: 14356.82
[INFO 2017-06-28 12:07:59,076 main.py:51] epoch 1534, training loss: 9480.09, average training loss: 10767.65, base loss: 14354.81
[INFO 2017-06-28 12:07:59,413 main.py:51] epoch 1535, training loss: 11208.03, average training loss: 10768.14, base loss: 14355.99
[INFO 2017-06-28 12:07:59,763 main.py:51] epoch 1536, training loss: 10117.99, average training loss: 10767.83, base loss: 14355.92
[INFO 2017-06-28 12:08:00,094 main.py:51] epoch 1537, training loss: 11288.60, average training loss: 10768.91, base loss: 14357.02
[INFO 2017-06-28 12:08:00,419 main.py:51] epoch 1538, training loss: 9596.10, average training loss: 10767.48, base loss: 14355.56
[INFO 2017-06-28 12:08:00,775 main.py:51] epoch 1539, training loss: 10329.90, average training loss: 10767.58, base loss: 14355.23
[INFO 2017-06-28 12:08:01,109 main.py:51] epoch 1540, training loss: 10652.71, average training loss: 10765.45, base loss: 14353.72
[INFO 2017-06-28 12:08:01,465 main.py:51] epoch 1541, training loss: 10239.28, average training loss: 10764.55, base loss: 14352.24
[INFO 2017-06-28 12:08:01,808 main.py:51] epoch 1542, training loss: 11977.47, average training loss: 10764.77, base loss: 14351.79
[INFO 2017-06-28 12:08:02,143 main.py:51] epoch 1543, training loss: 11724.25, average training loss: 10766.58, base loss: 14354.55
[INFO 2017-06-28 12:08:02,487 main.py:51] epoch 1544, training loss: 9019.95, average training loss: 10765.59, base loss: 14354.46
[INFO 2017-06-28 12:08:02,828 main.py:51] epoch 1545, training loss: 11279.05, average training loss: 10766.59, base loss: 14355.62
[INFO 2017-06-28 12:08:03,160 main.py:51] epoch 1546, training loss: 9922.23, average training loss: 10765.68, base loss: 14355.35
[INFO 2017-06-28 12:08:03,498 main.py:51] epoch 1547, training loss: 11287.68, average training loss: 10765.10, base loss: 14355.98
[INFO 2017-06-28 12:08:03,852 main.py:51] epoch 1548, training loss: 9909.60, average training loss: 10765.46, base loss: 14357.61
[INFO 2017-06-28 12:08:04,179 main.py:51] epoch 1549, training loss: 10037.97, average training loss: 10765.66, base loss: 14359.85
[INFO 2017-06-28 12:08:04,529 main.py:51] epoch 1550, training loss: 10646.34, average training loss: 10765.45, base loss: 14361.53
[INFO 2017-06-28 12:08:04,860 main.py:51] epoch 1551, training loss: 10817.28, average training loss: 10764.40, base loss: 14361.94
[INFO 2017-06-28 12:08:05,207 main.py:51] epoch 1552, training loss: 10430.34, average training loss: 10763.09, base loss: 14361.19
[INFO 2017-06-28 12:08:05,564 main.py:51] epoch 1553, training loss: 10149.53, average training loss: 10762.20, base loss: 14361.39
[INFO 2017-06-28 12:08:05,896 main.py:51] epoch 1554, training loss: 11160.90, average training loss: 10762.64, base loss: 14360.47
[INFO 2017-06-28 12:08:06,229 main.py:51] epoch 1555, training loss: 10516.93, average training loss: 10761.09, base loss: 14357.37
[INFO 2017-06-28 12:08:06,569 main.py:51] epoch 1556, training loss: 10589.38, average training loss: 10760.58, base loss: 14356.79
[INFO 2017-06-28 12:08:06,902 main.py:51] epoch 1557, training loss: 9789.57, average training loss: 10759.23, base loss: 14355.16
[INFO 2017-06-28 12:08:07,242 main.py:51] epoch 1558, training loss: 9790.42, average training loss: 10759.17, base loss: 14355.10
[INFO 2017-06-28 12:08:07,589 main.py:51] epoch 1559, training loss: 9864.72, average training loss: 10758.01, base loss: 14353.81
[INFO 2017-06-28 12:08:07,941 main.py:51] epoch 1560, training loss: 9233.44, average training loss: 10755.52, base loss: 14350.54
[INFO 2017-06-28 12:08:08,289 main.py:51] epoch 1561, training loss: 10843.50, average training loss: 10755.13, base loss: 14349.89
[INFO 2017-06-28 12:08:08,617 main.py:51] epoch 1562, training loss: 12940.54, average training loss: 10755.24, base loss: 14350.60
[INFO 2017-06-28 12:08:08,951 main.py:51] epoch 1563, training loss: 12225.94, average training loss: 10757.23, base loss: 14354.21
[INFO 2017-06-28 12:08:09,270 main.py:51] epoch 1564, training loss: 10671.58, average training loss: 10756.75, base loss: 14353.36
[INFO 2017-06-28 12:08:09,614 main.py:51] epoch 1565, training loss: 10461.38, average training loss: 10757.21, base loss: 14355.64
[INFO 2017-06-28 12:08:09,962 main.py:51] epoch 1566, training loss: 12597.97, average training loss: 10759.70, base loss: 14358.44
[INFO 2017-06-28 12:08:10,293 main.py:51] epoch 1567, training loss: 10576.68, average training loss: 10758.91, base loss: 14357.92
[INFO 2017-06-28 12:08:10,661 main.py:51] epoch 1568, training loss: 10094.28, average training loss: 10757.88, base loss: 14358.65
[INFO 2017-06-28 12:08:11,031 main.py:51] epoch 1569, training loss: 11987.84, average training loss: 10759.27, base loss: 14361.83
[INFO 2017-06-28 12:08:11,377 main.py:51] epoch 1570, training loss: 10792.86, average training loss: 10760.83, base loss: 14363.60
[INFO 2017-06-28 12:08:11,705 main.py:51] epoch 1571, training loss: 11446.97, average training loss: 10762.45, base loss: 14366.78
[INFO 2017-06-28 12:08:12,036 main.py:51] epoch 1572, training loss: 11332.99, average training loss: 10763.06, base loss: 14369.93
[INFO 2017-06-28 12:08:12,358 main.py:51] epoch 1573, training loss: 10591.44, average training loss: 10762.88, base loss: 14370.82
[INFO 2017-06-28 12:08:12,706 main.py:51] epoch 1574, training loss: 9607.57, average training loss: 10759.89, base loss: 14367.74
[INFO 2017-06-28 12:08:13,047 main.py:51] epoch 1575, training loss: 12131.50, average training loss: 10760.93, base loss: 14369.14
[INFO 2017-06-28 12:08:13,384 main.py:51] epoch 1576, training loss: 12670.69, average training loss: 10762.38, base loss: 14373.37
[INFO 2017-06-28 12:08:13,716 main.py:51] epoch 1577, training loss: 11065.44, average training loss: 10762.17, base loss: 14374.14
[INFO 2017-06-28 12:08:14,069 main.py:51] epoch 1578, training loss: 12626.63, average training loss: 10763.53, base loss: 14376.59
[INFO 2017-06-28 12:08:14,409 main.py:51] epoch 1579, training loss: 10784.27, average training loss: 10764.75, base loss: 14379.39
[INFO 2017-06-28 12:08:14,745 main.py:51] epoch 1580, training loss: 11805.00, average training loss: 10765.10, base loss: 14379.20
[INFO 2017-06-28 12:08:15,089 main.py:51] epoch 1581, training loss: 12823.80, average training loss: 10767.24, base loss: 14382.95
[INFO 2017-06-28 12:08:15,452 main.py:51] epoch 1582, training loss: 9825.96, average training loss: 10766.32, base loss: 14382.05
[INFO 2017-06-28 12:08:15,785 main.py:51] epoch 1583, training loss: 11146.98, average training loss: 10767.70, base loss: 14385.26
[INFO 2017-06-28 12:08:16,142 main.py:51] epoch 1584, training loss: 10008.56, average training loss: 10768.13, base loss: 14386.39
[INFO 2017-06-28 12:08:16,489 main.py:51] epoch 1585, training loss: 9818.50, average training loss: 10764.22, base loss: 14381.34
[INFO 2017-06-28 12:08:16,818 main.py:51] epoch 1586, training loss: 9994.02, average training loss: 10762.88, base loss: 14379.92
[INFO 2017-06-28 12:08:17,169 main.py:51] epoch 1587, training loss: 11666.08, average training loss: 10762.02, base loss: 14378.23
[INFO 2017-06-28 12:08:17,518 main.py:51] epoch 1588, training loss: 9274.44, average training loss: 10758.70, base loss: 14372.94
[INFO 2017-06-28 12:08:17,867 main.py:51] epoch 1589, training loss: 9176.35, average training loss: 10757.75, base loss: 14373.05
[INFO 2017-06-28 12:08:18,228 main.py:51] epoch 1590, training loss: 10138.72, average training loss: 10756.46, base loss: 14373.16
[INFO 2017-06-28 12:08:18,564 main.py:51] epoch 1591, training loss: 10258.19, average training loss: 10755.11, base loss: 14372.04
[INFO 2017-06-28 12:08:18,873 main.py:51] epoch 1592, training loss: 9415.19, average training loss: 10752.58, base loss: 14369.55
[INFO 2017-06-28 12:08:19,214 main.py:51] epoch 1593, training loss: 9308.72, average training loss: 10750.18, base loss: 14367.89
[INFO 2017-06-28 12:08:19,572 main.py:51] epoch 1594, training loss: 11434.46, average training loss: 10751.46, base loss: 14369.34
[INFO 2017-06-28 12:08:19,926 main.py:51] epoch 1595, training loss: 9317.02, average training loss: 10749.68, base loss: 14368.34
[INFO 2017-06-28 12:08:20,281 main.py:51] epoch 1596, training loss: 11151.68, average training loss: 10749.72, base loss: 14370.22
[INFO 2017-06-28 12:08:20,607 main.py:51] epoch 1597, training loss: 9903.90, average training loss: 10747.85, base loss: 14368.88
[INFO 2017-06-28 12:08:20,940 main.py:51] epoch 1598, training loss: 10561.78, average training loss: 10744.12, base loss: 14366.22
[INFO 2017-06-28 12:08:21,282 main.py:51] epoch 1599, training loss: 11149.31, average training loss: 10742.73, base loss: 14364.95
[INFO 2017-06-28 12:08:21,282 main.py:53] epoch 1599, testing
[INFO 2017-06-28 12:08:22,969 main.py:105] average testing loss: 11524.62, base loss: 14756.05
[INFO 2017-06-28 12:08:22,969 main.py:106] improve_loss: 3231.43, improve_percent: 0.22
[INFO 2017-06-28 12:08:22,970 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:08:23,307 main.py:51] epoch 1600, training loss: 10104.67, average training loss: 10740.91, base loss: 14362.75
[INFO 2017-06-28 12:08:23,657 main.py:51] epoch 1601, training loss: 11620.88, average training loss: 10741.25, base loss: 14362.96
[INFO 2017-06-28 12:08:24,000 main.py:51] epoch 1602, training loss: 11095.14, average training loss: 10741.37, base loss: 14364.37
[INFO 2017-06-28 12:08:24,323 main.py:51] epoch 1603, training loss: 10436.47, average training loss: 10741.55, base loss: 14365.53
[INFO 2017-06-28 12:08:24,657 main.py:51] epoch 1604, training loss: 10938.19, average training loss: 10741.86, base loss: 14366.14
[INFO 2017-06-28 12:08:25,001 main.py:51] epoch 1605, training loss: 11408.56, average training loss: 10743.85, base loss: 14370.30
[INFO 2017-06-28 12:08:25,345 main.py:51] epoch 1606, training loss: 11124.25, average training loss: 10744.22, base loss: 14373.15
[INFO 2017-06-28 12:08:25,693 main.py:51] epoch 1607, training loss: 12587.16, average training loss: 10744.06, base loss: 14372.45
[INFO 2017-06-28 12:08:26,041 main.py:51] epoch 1608, training loss: 9185.20, average training loss: 10741.69, base loss: 14369.65
[INFO 2017-06-28 12:08:26,395 main.py:51] epoch 1609, training loss: 10661.11, average training loss: 10740.21, base loss: 14368.38
[INFO 2017-06-28 12:08:26,748 main.py:51] epoch 1610, training loss: 10528.29, average training loss: 10740.49, base loss: 14370.76
[INFO 2017-06-28 12:08:27,072 main.py:51] epoch 1611, training loss: 10516.90, average training loss: 10740.21, base loss: 14371.14
[INFO 2017-06-28 12:08:27,419 main.py:51] epoch 1612, training loss: 10738.51, average training loss: 10740.02, base loss: 14371.21
[INFO 2017-06-28 12:08:27,780 main.py:51] epoch 1613, training loss: 8513.81, average training loss: 10739.50, base loss: 14370.86
[INFO 2017-06-28 12:08:28,148 main.py:51] epoch 1614, training loss: 11605.72, average training loss: 10740.36, base loss: 14371.58
[INFO 2017-06-28 12:08:28,493 main.py:51] epoch 1615, training loss: 10243.88, average training loss: 10740.39, base loss: 14373.13
[INFO 2017-06-28 12:08:28,838 main.py:51] epoch 1616, training loss: 10657.28, average training loss: 10739.57, base loss: 14372.35
[INFO 2017-06-28 12:08:29,194 main.py:51] epoch 1617, training loss: 10547.67, average training loss: 10738.75, base loss: 14372.56
[INFO 2017-06-28 12:08:29,539 main.py:51] epoch 1618, training loss: 10223.62, average training loss: 10735.70, base loss: 14370.85
[INFO 2017-06-28 12:08:29,883 main.py:51] epoch 1619, training loss: 11328.40, average training loss: 10736.67, base loss: 14372.73
[INFO 2017-06-28 12:08:30,212 main.py:51] epoch 1620, training loss: 11896.07, average training loss: 10736.98, base loss: 14373.52
[INFO 2017-06-28 12:08:30,571 main.py:51] epoch 1621, training loss: 12136.14, average training loss: 10737.77, base loss: 14375.45
[INFO 2017-06-28 12:08:30,905 main.py:51] epoch 1622, training loss: 9942.42, average training loss: 10737.11, base loss: 14374.64
[INFO 2017-06-28 12:08:31,245 main.py:51] epoch 1623, training loss: 11436.56, average training loss: 10738.14, base loss: 14378.16
[INFO 2017-06-28 12:08:31,579 main.py:51] epoch 1624, training loss: 10486.00, average training loss: 10738.81, base loss: 14379.96
[INFO 2017-06-28 12:08:31,898 main.py:51] epoch 1625, training loss: 9504.47, average training loss: 10736.18, base loss: 14377.47
[INFO 2017-06-28 12:08:32,255 main.py:51] epoch 1626, training loss: 10445.80, average training loss: 10735.86, base loss: 14378.40
[INFO 2017-06-28 12:08:32,601 main.py:51] epoch 1627, training loss: 10062.55, average training loss: 10735.08, base loss: 14378.74
[INFO 2017-06-28 12:08:32,951 main.py:51] epoch 1628, training loss: 9730.45, average training loss: 10733.97, base loss: 14378.25
[INFO 2017-06-28 12:08:33,276 main.py:51] epoch 1629, training loss: 9935.72, average training loss: 10732.73, base loss: 14377.47
[INFO 2017-06-28 12:08:33,624 main.py:51] epoch 1630, training loss: 10063.65, average training loss: 10730.79, base loss: 14375.72
[INFO 2017-06-28 12:08:33,980 main.py:51] epoch 1631, training loss: 9498.27, average training loss: 10729.44, base loss: 14374.19
[INFO 2017-06-28 12:08:34,337 main.py:51] epoch 1632, training loss: 10434.51, average training loss: 10729.07, base loss: 14375.20
[INFO 2017-06-28 12:08:34,666 main.py:51] epoch 1633, training loss: 10634.51, average training loss: 10728.61, base loss: 14374.76
[INFO 2017-06-28 12:08:34,985 main.py:51] epoch 1634, training loss: 11676.24, average training loss: 10729.74, base loss: 14375.54
[INFO 2017-06-28 12:08:35,316 main.py:51] epoch 1635, training loss: 10511.62, average training loss: 10728.63, base loss: 14375.23
[INFO 2017-06-28 12:08:35,660 main.py:51] epoch 1636, training loss: 11217.38, average training loss: 10727.58, base loss: 14373.53
[INFO 2017-06-28 12:08:35,993 main.py:51] epoch 1637, training loss: 9047.49, average training loss: 10725.94, base loss: 14371.54
[INFO 2017-06-28 12:08:36,325 main.py:51] epoch 1638, training loss: 10167.16, average training loss: 10725.26, base loss: 14371.65
[INFO 2017-06-28 12:08:36,656 main.py:51] epoch 1639, training loss: 9217.16, average training loss: 10725.01, base loss: 14372.69
[INFO 2017-06-28 12:08:37,015 main.py:51] epoch 1640, training loss: 10911.51, average training loss: 10723.63, base loss: 14371.35
[INFO 2017-06-28 12:08:37,370 main.py:51] epoch 1641, training loss: 9742.92, average training loss: 10722.30, base loss: 14371.36
[INFO 2017-06-28 12:08:37,707 main.py:51] epoch 1642, training loss: 11569.82, average training loss: 10723.47, base loss: 14373.49
[INFO 2017-06-28 12:08:38,030 main.py:51] epoch 1643, training loss: 11039.42, average training loss: 10722.27, base loss: 14372.52
[INFO 2017-06-28 12:08:38,375 main.py:51] epoch 1644, training loss: 9490.41, average training loss: 10721.02, base loss: 14372.71
[INFO 2017-06-28 12:08:38,740 main.py:51] epoch 1645, training loss: 12108.08, average training loss: 10721.89, base loss: 14374.63
[INFO 2017-06-28 12:08:39,086 main.py:51] epoch 1646, training loss: 10333.28, average training loss: 10722.61, base loss: 14376.02
[INFO 2017-06-28 12:08:39,415 main.py:51] epoch 1647, training loss: 10490.66, average training loss: 10722.73, base loss: 14377.01
[INFO 2017-06-28 12:08:39,756 main.py:51] epoch 1648, training loss: 10267.28, average training loss: 10720.29, base loss: 14372.87
[INFO 2017-06-28 12:08:40,101 main.py:51] epoch 1649, training loss: 9408.54, average training loss: 10719.02, base loss: 14371.25
[INFO 2017-06-28 12:08:40,441 main.py:51] epoch 1650, training loss: 10807.10, average training loss: 10718.64, base loss: 14371.69
[INFO 2017-06-28 12:08:40,772 main.py:51] epoch 1651, training loss: 11202.59, average training loss: 10718.05, base loss: 14373.17
[INFO 2017-06-28 12:08:41,121 main.py:51] epoch 1652, training loss: 10924.74, average training loss: 10718.25, base loss: 14374.47
[INFO 2017-06-28 12:08:41,442 main.py:51] epoch 1653, training loss: 9289.24, average training loss: 10716.81, base loss: 14373.13
[INFO 2017-06-28 12:08:41,773 main.py:51] epoch 1654, training loss: 9937.78, average training loss: 10715.68, base loss: 14372.72
[INFO 2017-06-28 12:08:42,125 main.py:51] epoch 1655, training loss: 10086.39, average training loss: 10715.09, base loss: 14373.57
[INFO 2017-06-28 12:08:42,459 main.py:51] epoch 1656, training loss: 10424.60, average training loss: 10714.70, base loss: 14373.78
[INFO 2017-06-28 12:08:42,798 main.py:51] epoch 1657, training loss: 11172.58, average training loss: 10715.28, base loss: 14374.82
[INFO 2017-06-28 12:08:43,158 main.py:51] epoch 1658, training loss: 9752.93, average training loss: 10714.24, base loss: 14372.53
[INFO 2017-06-28 12:08:43,501 main.py:51] epoch 1659, training loss: 11849.45, average training loss: 10713.29, base loss: 14371.39
[INFO 2017-06-28 12:08:43,840 main.py:51] epoch 1660, training loss: 11088.15, average training loss: 10713.84, base loss: 14372.97
[INFO 2017-06-28 12:08:44,160 main.py:51] epoch 1661, training loss: 10062.03, average training loss: 10711.90, base loss: 14370.67
[INFO 2017-06-28 12:08:44,490 main.py:51] epoch 1662, training loss: 9971.13, average training loss: 10710.99, base loss: 14369.38
[INFO 2017-06-28 12:08:44,804 main.py:51] epoch 1663, training loss: 12497.98, average training loss: 10713.07, base loss: 14373.35
[INFO 2017-06-28 12:08:45,136 main.py:51] epoch 1664, training loss: 11303.41, average training loss: 10713.88, base loss: 14374.57
[INFO 2017-06-28 12:08:45,488 main.py:51] epoch 1665, training loss: 10770.85, average training loss: 10713.37, base loss: 14373.15
[INFO 2017-06-28 12:08:45,839 main.py:51] epoch 1666, training loss: 10872.87, average training loss: 10712.13, base loss: 14370.79
[INFO 2017-06-28 12:08:46,168 main.py:51] epoch 1667, training loss: 10840.64, average training loss: 10711.52, base loss: 14370.06
[INFO 2017-06-28 12:08:46,492 main.py:51] epoch 1668, training loss: 10807.10, average training loss: 10711.45, base loss: 14370.26
[INFO 2017-06-28 12:08:46,823 main.py:51] epoch 1669, training loss: 11394.84, average training loss: 10711.03, base loss: 14370.59
[INFO 2017-06-28 12:08:47,161 main.py:51] epoch 1670, training loss: 9678.39, average training loss: 10709.76, base loss: 14368.91
[INFO 2017-06-28 12:08:47,483 main.py:51] epoch 1671, training loss: 10275.94, average training loss: 10709.93, base loss: 14370.46
[INFO 2017-06-28 12:08:47,822 main.py:51] epoch 1672, training loss: 8802.27, average training loss: 10708.94, base loss: 14369.06
[INFO 2017-06-28 12:08:48,163 main.py:51] epoch 1673, training loss: 12239.70, average training loss: 10708.80, base loss: 14370.86
[INFO 2017-06-28 12:08:48,489 main.py:51] epoch 1674, training loss: 11090.20, average training loss: 10709.10, base loss: 14371.83
[INFO 2017-06-28 12:08:48,830 main.py:51] epoch 1675, training loss: 10711.35, average training loss: 10708.44, base loss: 14371.79
[INFO 2017-06-28 12:08:49,181 main.py:51] epoch 1676, training loss: 10222.75, average training loss: 10707.55, base loss: 14370.51
[INFO 2017-06-28 12:08:49,519 main.py:51] epoch 1677, training loss: 10473.85, average training loss: 10708.20, base loss: 14371.57
[INFO 2017-06-28 12:08:49,843 main.py:51] epoch 1678, training loss: 10777.84, average training loss: 10707.30, base loss: 14373.06
[INFO 2017-06-28 12:08:50,189 main.py:51] epoch 1679, training loss: 10738.63, average training loss: 10706.30, base loss: 14371.81
[INFO 2017-06-28 12:08:50,527 main.py:51] epoch 1680, training loss: 12732.30, average training loss: 10707.95, base loss: 14376.34
[INFO 2017-06-28 12:08:50,874 main.py:51] epoch 1681, training loss: 9566.76, average training loss: 10705.59, base loss: 14374.20
[INFO 2017-06-28 12:08:51,213 main.py:51] epoch 1682, training loss: 10877.90, average training loss: 10705.74, base loss: 14374.23
[INFO 2017-06-28 12:08:51,573 main.py:51] epoch 1683, training loss: 10810.74, average training loss: 10705.63, base loss: 14373.21
[INFO 2017-06-28 12:08:51,905 main.py:51] epoch 1684, training loss: 10857.21, average training loss: 10705.22, base loss: 14374.59
[INFO 2017-06-28 12:08:52,230 main.py:51] epoch 1685, training loss: 11714.45, average training loss: 10704.08, base loss: 14372.97
[INFO 2017-06-28 12:08:52,581 main.py:51] epoch 1686, training loss: 10390.65, average training loss: 10704.13, base loss: 14373.82
[INFO 2017-06-28 12:08:52,934 main.py:51] epoch 1687, training loss: 10296.13, average training loss: 10702.74, base loss: 14372.88
[INFO 2017-06-28 12:08:53,277 main.py:51] epoch 1688, training loss: 11601.95, average training loss: 10703.72, base loss: 14374.79
[INFO 2017-06-28 12:08:53,614 main.py:51] epoch 1689, training loss: 10285.02, average training loss: 10703.44, base loss: 14374.71
[INFO 2017-06-28 12:08:53,969 main.py:51] epoch 1690, training loss: 10821.23, average training loss: 10703.27, base loss: 14376.55
[INFO 2017-06-28 12:08:54,313 main.py:51] epoch 1691, training loss: 10613.42, average training loss: 10702.59, base loss: 14376.79
[INFO 2017-06-28 12:08:54,670 main.py:51] epoch 1692, training loss: 9859.51, average training loss: 10700.79, base loss: 14372.63
[INFO 2017-06-28 12:08:55,004 main.py:51] epoch 1693, training loss: 10297.44, average training loss: 10700.31, base loss: 14373.69
[INFO 2017-06-28 12:08:55,326 main.py:51] epoch 1694, training loss: 11485.50, average training loss: 10698.63, base loss: 14372.96
[INFO 2017-06-28 12:08:55,688 main.py:51] epoch 1695, training loss: 10257.24, average training loss: 10698.45, base loss: 14372.08
[INFO 2017-06-28 12:08:56,034 main.py:51] epoch 1696, training loss: 10826.46, average training loss: 10698.55, base loss: 14373.61
[INFO 2017-06-28 12:08:56,387 main.py:51] epoch 1697, training loss: 10916.24, average training loss: 10698.34, base loss: 14375.06
[INFO 2017-06-28 12:08:56,737 main.py:51] epoch 1698, training loss: 10974.27, average training loss: 10698.14, base loss: 14375.27
[INFO 2017-06-28 12:08:57,081 main.py:51] epoch 1699, training loss: 11313.03, average training loss: 10698.94, base loss: 14377.57
[INFO 2017-06-28 12:08:57,082 main.py:53] epoch 1699, testing
[INFO 2017-06-28 12:08:58,733 main.py:105] average testing loss: 11586.50, base loss: 15018.01
[INFO 2017-06-28 12:08:58,733 main.py:106] improve_loss: 3431.52, improve_percent: 0.23
[INFO 2017-06-28 12:08:58,733 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:08:59,110 main.py:51] epoch 1700, training loss: 10221.99, average training loss: 10697.16, base loss: 14376.19
[INFO 2017-06-28 12:08:59,463 main.py:51] epoch 1701, training loss: 9980.13, average training loss: 10694.08, base loss: 14373.96
[INFO 2017-06-28 12:08:59,816 main.py:51] epoch 1702, training loss: 11549.88, average training loss: 10695.24, base loss: 14377.04
[INFO 2017-06-28 12:09:00,156 main.py:51] epoch 1703, training loss: 10795.26, average training loss: 10695.66, base loss: 14378.30
[INFO 2017-06-28 12:09:00,492 main.py:51] epoch 1704, training loss: 9863.17, average training loss: 10694.80, base loss: 14377.39
[INFO 2017-06-28 12:09:00,818 main.py:51] epoch 1705, training loss: 9646.59, average training loss: 10692.98, base loss: 14375.63
[INFO 2017-06-28 12:09:01,153 main.py:51] epoch 1706, training loss: 10096.73, average training loss: 10693.48, base loss: 14375.97
[INFO 2017-06-28 12:09:01,474 main.py:51] epoch 1707, training loss: 9735.56, average training loss: 10692.50, base loss: 14375.93
[INFO 2017-06-28 12:09:01,819 main.py:51] epoch 1708, training loss: 9849.85, average training loss: 10692.78, base loss: 14377.53
[INFO 2017-06-28 12:09:02,163 main.py:51] epoch 1709, training loss: 10277.73, average training loss: 10691.31, base loss: 14376.91
[INFO 2017-06-28 12:09:02,533 main.py:51] epoch 1710, training loss: 10287.61, average training loss: 10690.19, base loss: 14377.44
[INFO 2017-06-28 12:09:02,859 main.py:51] epoch 1711, training loss: 9445.54, average training loss: 10687.79, base loss: 14374.62
[INFO 2017-06-28 12:09:03,210 main.py:51] epoch 1712, training loss: 10407.92, average training loss: 10687.80, base loss: 14376.36
[INFO 2017-06-28 12:09:03,538 main.py:51] epoch 1713, training loss: 11070.94, average training loss: 10688.44, base loss: 14379.28
[INFO 2017-06-28 12:09:03,881 main.py:51] epoch 1714, training loss: 9622.86, average training loss: 10686.26, base loss: 14376.46
[INFO 2017-06-28 12:09:04,221 main.py:51] epoch 1715, training loss: 9887.65, average training loss: 10684.43, base loss: 14374.28
[INFO 2017-06-28 12:09:04,553 main.py:51] epoch 1716, training loss: 11211.76, average training loss: 10684.55, base loss: 14374.60
[INFO 2017-06-28 12:09:04,873 main.py:51] epoch 1717, training loss: 9767.44, average training loss: 10682.09, base loss: 14370.94
[INFO 2017-06-28 12:09:05,223 main.py:51] epoch 1718, training loss: 10027.39, average training loss: 10680.16, base loss: 14368.45
[INFO 2017-06-28 12:09:05,569 main.py:51] epoch 1719, training loss: 9184.93, average training loss: 10679.72, base loss: 14369.23
[INFO 2017-06-28 12:09:05,905 main.py:51] epoch 1720, training loss: 10002.59, average training loss: 10677.13, base loss: 14366.51
[INFO 2017-06-28 12:09:06,243 main.py:51] epoch 1721, training loss: 10424.68, average training loss: 10675.92, base loss: 14364.99
[INFO 2017-06-28 12:09:06,590 main.py:51] epoch 1722, training loss: 10546.22, average training loss: 10675.45, base loss: 14365.25
[INFO 2017-06-28 12:09:06,957 main.py:51] epoch 1723, training loss: 10876.75, average training loss: 10675.10, base loss: 14363.25
[INFO 2017-06-28 12:09:07,275 main.py:51] epoch 1724, training loss: 11862.76, average training loss: 10676.03, base loss: 14365.79
[INFO 2017-06-28 12:09:07,612 main.py:51] epoch 1725, training loss: 10698.96, average training loss: 10676.20, base loss: 14367.67
[INFO 2017-06-28 12:09:07,947 main.py:51] epoch 1726, training loss: 10825.52, average training loss: 10676.38, base loss: 14369.52
[INFO 2017-06-28 12:09:08,283 main.py:51] epoch 1727, training loss: 10709.49, average training loss: 10674.52, base loss: 14368.74
[INFO 2017-06-28 12:09:08,619 main.py:51] epoch 1728, training loss: 11118.67, average training loss: 10672.22, base loss: 14366.99
[INFO 2017-06-28 12:09:08,957 main.py:51] epoch 1729, training loss: 10673.48, average training loss: 10672.12, base loss: 14367.12
[INFO 2017-06-28 12:09:09,297 main.py:51] epoch 1730, training loss: 9645.06, average training loss: 10672.39, base loss: 14369.15
[INFO 2017-06-28 12:09:09,667 main.py:51] epoch 1731, training loss: 11764.09, average training loss: 10674.26, base loss: 14374.14
[INFO 2017-06-28 12:09:10,006 main.py:51] epoch 1732, training loss: 13226.14, average training loss: 10677.36, base loss: 14378.91
[INFO 2017-06-28 12:09:10,368 main.py:51] epoch 1733, training loss: 10094.86, average training loss: 10674.38, base loss: 14376.08
[INFO 2017-06-28 12:09:10,701 main.py:51] epoch 1734, training loss: 11255.00, average training loss: 10675.59, base loss: 14376.90
[INFO 2017-06-28 12:09:11,042 main.py:51] epoch 1735, training loss: 9576.63, average training loss: 10674.42, base loss: 14374.87
[INFO 2017-06-28 12:09:11,394 main.py:51] epoch 1736, training loss: 10445.14, average training loss: 10675.55, base loss: 14378.12
[INFO 2017-06-28 12:09:11,732 main.py:51] epoch 1737, training loss: 10450.85, average training loss: 10672.99, base loss: 14374.51
[INFO 2017-06-28 12:09:12,072 main.py:51] epoch 1738, training loss: 10378.98, average training loss: 10671.98, base loss: 14372.85
[INFO 2017-06-28 12:09:12,429 main.py:51] epoch 1739, training loss: 8710.18, average training loss: 10670.74, base loss: 14371.79
[INFO 2017-06-28 12:09:12,774 main.py:51] epoch 1740, training loss: 10221.82, average training loss: 10671.31, base loss: 14373.02
[INFO 2017-06-28 12:09:13,136 main.py:51] epoch 1741, training loss: 9875.58, average training loss: 10669.50, base loss: 14370.81
[INFO 2017-06-28 12:09:13,481 main.py:51] epoch 1742, training loss: 12166.81, average training loss: 10670.52, base loss: 14372.54
[INFO 2017-06-28 12:09:13,822 main.py:51] epoch 1743, training loss: 10082.54, average training loss: 10669.33, base loss: 14370.93
[INFO 2017-06-28 12:09:14,180 main.py:51] epoch 1744, training loss: 11915.64, average training loss: 10670.69, base loss: 14373.60
[INFO 2017-06-28 12:09:14,514 main.py:51] epoch 1745, training loss: 10401.63, average training loss: 10670.58, base loss: 14374.80
[INFO 2017-06-28 12:09:14,854 main.py:51] epoch 1746, training loss: 10996.10, average training loss: 10670.78, base loss: 14374.80
[INFO 2017-06-28 12:09:15,177 main.py:51] epoch 1747, training loss: 9850.35, average training loss: 10669.45, base loss: 14373.53
[INFO 2017-06-28 12:09:15,508 main.py:51] epoch 1748, training loss: 13173.91, average training loss: 10671.31, base loss: 14376.07
[INFO 2017-06-28 12:09:15,901 main.py:51] epoch 1749, training loss: 9381.26, average training loss: 10668.57, base loss: 14373.65
[INFO 2017-06-28 12:09:16,224 main.py:51] epoch 1750, training loss: 10049.63, average training loss: 10668.59, base loss: 14374.11
[INFO 2017-06-28 12:09:16,578 main.py:51] epoch 1751, training loss: 12610.23, average training loss: 10670.74, base loss: 14377.10
[INFO 2017-06-28 12:09:16,913 main.py:51] epoch 1752, training loss: 10949.53, average training loss: 10671.32, base loss: 14378.50
[INFO 2017-06-28 12:09:17,282 main.py:51] epoch 1753, training loss: 10027.93, average training loss: 10672.03, base loss: 14379.49
[INFO 2017-06-28 12:09:17,635 main.py:51] epoch 1754, training loss: 9865.27, average training loss: 10671.81, base loss: 14380.49
[INFO 2017-06-28 12:09:17,983 main.py:51] epoch 1755, training loss: 10305.45, average training loss: 10671.54, base loss: 14380.55
[INFO 2017-06-28 12:09:18,305 main.py:51] epoch 1756, training loss: 8444.22, average training loss: 10667.83, base loss: 14375.55
[INFO 2017-06-28 12:09:18,658 main.py:51] epoch 1757, training loss: 9837.57, average training loss: 10667.39, base loss: 14376.60
[INFO 2017-06-28 12:09:18,984 main.py:51] epoch 1758, training loss: 11957.84, average training loss: 10669.51, base loss: 14380.79
[INFO 2017-06-28 12:09:19,324 main.py:51] epoch 1759, training loss: 11881.75, average training loss: 10671.42, base loss: 14384.46
[INFO 2017-06-28 12:09:19,657 main.py:51] epoch 1760, training loss: 11348.30, average training loss: 10670.62, base loss: 14383.62
[INFO 2017-06-28 12:09:20,022 main.py:51] epoch 1761, training loss: 10984.92, average training loss: 10670.52, base loss: 14383.05
[INFO 2017-06-28 12:09:20,376 main.py:51] epoch 1762, training loss: 11011.75, average training loss: 10671.04, base loss: 14384.23
[INFO 2017-06-28 12:09:20,735 main.py:51] epoch 1763, training loss: 8877.01, average training loss: 10670.33, base loss: 14382.73
[INFO 2017-06-28 12:09:21,065 main.py:51] epoch 1764, training loss: 9156.22, average training loss: 10669.26, base loss: 14381.95
[INFO 2017-06-28 12:09:21,409 main.py:51] epoch 1765, training loss: 10321.45, average training loss: 10669.90, base loss: 14383.36
[INFO 2017-06-28 12:09:21,749 main.py:51] epoch 1766, training loss: 9994.50, average training loss: 10669.08, base loss: 14382.57
[INFO 2017-06-28 12:09:22,125 main.py:51] epoch 1767, training loss: 12569.13, average training loss: 10670.98, base loss: 14384.96
[INFO 2017-06-28 12:09:22,514 main.py:51] epoch 1768, training loss: 9706.46, average training loss: 10669.79, base loss: 14382.57
[INFO 2017-06-28 12:09:22,858 main.py:51] epoch 1769, training loss: 10655.53, average training loss: 10669.67, base loss: 14382.31
[INFO 2017-06-28 12:09:23,224 main.py:51] epoch 1770, training loss: 10055.92, average training loss: 10669.28, base loss: 14381.07
[INFO 2017-06-28 12:09:23,553 main.py:51] epoch 1771, training loss: 10165.03, average training loss: 10667.45, base loss: 14379.90
[INFO 2017-06-28 12:09:23,914 main.py:51] epoch 1772, training loss: 9765.51, average training loss: 10666.33, base loss: 14379.25
[INFO 2017-06-28 12:09:24,264 main.py:51] epoch 1773, training loss: 10594.78, average training loss: 10666.19, base loss: 14378.46
[INFO 2017-06-28 12:09:24,593 main.py:51] epoch 1774, training loss: 9333.58, average training loss: 10664.35, base loss: 14377.12
[INFO 2017-06-28 12:09:24,926 main.py:51] epoch 1775, training loss: 9314.33, average training loss: 10662.01, base loss: 14373.00
[INFO 2017-06-28 12:09:25,255 main.py:51] epoch 1776, training loss: 10077.67, average training loss: 10660.16, base loss: 14370.36
[INFO 2017-06-28 12:09:25,607 main.py:51] epoch 1777, training loss: 8753.64, average training loss: 10658.51, base loss: 14368.83
[INFO 2017-06-28 12:09:25,950 main.py:51] epoch 1778, training loss: 10926.37, average training loss: 10658.55, base loss: 14369.49
[INFO 2017-06-28 12:09:26,315 main.py:51] epoch 1779, training loss: 9643.40, average training loss: 10657.86, base loss: 14369.79
[INFO 2017-06-28 12:09:26,656 main.py:51] epoch 1780, training loss: 10333.47, average training loss: 10657.83, base loss: 14370.16
[INFO 2017-06-28 12:09:27,008 main.py:51] epoch 1781, training loss: 10042.14, average training loss: 10656.40, base loss: 14368.87
[INFO 2017-06-28 12:09:27,357 main.py:51] epoch 1782, training loss: 11008.09, average training loss: 10656.02, base loss: 14369.31
[INFO 2017-06-28 12:09:27,724 main.py:51] epoch 1783, training loss: 11415.95, average training loss: 10655.71, base loss: 14371.59
[INFO 2017-06-28 12:09:28,080 main.py:51] epoch 1784, training loss: 10307.70, average training loss: 10654.40, base loss: 14370.18
[INFO 2017-06-28 12:09:28,426 main.py:51] epoch 1785, training loss: 10340.47, average training loss: 10653.38, base loss: 14369.92
[INFO 2017-06-28 12:09:28,781 main.py:51] epoch 1786, training loss: 11492.69, average training loss: 10655.92, base loss: 14372.62
[INFO 2017-06-28 12:09:29,112 main.py:51] epoch 1787, training loss: 11063.42, average training loss: 10657.49, base loss: 14375.20
[INFO 2017-06-28 12:09:29,503 main.py:51] epoch 1788, training loss: 11404.78, average training loss: 10658.60, base loss: 14377.25
[INFO 2017-06-28 12:09:29,845 main.py:51] epoch 1789, training loss: 12017.56, average training loss: 10659.88, base loss: 14377.96
[INFO 2017-06-28 12:09:30,195 main.py:51] epoch 1790, training loss: 10930.63, average training loss: 10660.16, base loss: 14378.90
[INFO 2017-06-28 12:09:30,540 main.py:51] epoch 1791, training loss: 9058.52, average training loss: 10657.87, base loss: 14375.04
[INFO 2017-06-28 12:09:30,884 main.py:51] epoch 1792, training loss: 11108.38, average training loss: 10656.45, base loss: 14373.46
[INFO 2017-06-28 12:09:31,211 main.py:51] epoch 1793, training loss: 10774.73, average training loss: 10656.99, base loss: 14375.24
[INFO 2017-06-28 12:09:31,540 main.py:51] epoch 1794, training loss: 10711.34, average training loss: 10656.82, base loss: 14374.54
[INFO 2017-06-28 12:09:31,886 main.py:51] epoch 1795, training loss: 9893.39, average training loss: 10656.66, base loss: 14374.77
[INFO 2017-06-28 12:09:32,228 main.py:51] epoch 1796, training loss: 10469.38, average training loss: 10656.11, base loss: 14375.40
[INFO 2017-06-28 12:09:32,590 main.py:51] epoch 1797, training loss: 9724.74, average training loss: 10655.89, base loss: 14375.77
[INFO 2017-06-28 12:09:32,929 main.py:51] epoch 1798, training loss: 10567.97, average training loss: 10655.87, base loss: 14374.94
[INFO 2017-06-28 12:09:33,304 main.py:51] epoch 1799, training loss: 12309.20, average training loss: 10657.67, base loss: 14378.39
[INFO 2017-06-28 12:09:33,304 main.py:53] epoch 1799, testing
[INFO 2017-06-28 12:09:35,021 main.py:105] average testing loss: 12031.60, base loss: 15782.45
[INFO 2017-06-28 12:09:35,021 main.py:106] improve_loss: 3750.85, improve_percent: 0.24
[INFO 2017-06-28 12:09:35,022 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:09:35,028 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 12:09:35,383 main.py:51] epoch 1800, training loss: 9404.83, average training loss: 10655.65, base loss: 14375.97
[INFO 2017-06-28 12:09:35,730 main.py:51] epoch 1801, training loss: 10150.33, average training loss: 10656.35, base loss: 14377.62
[INFO 2017-06-28 12:09:36,071 main.py:51] epoch 1802, training loss: 10694.08, average training loss: 10657.05, base loss: 14378.98
[INFO 2017-06-28 12:09:36,419 main.py:51] epoch 1803, training loss: 9675.16, average training loss: 10654.89, base loss: 14377.40
[INFO 2017-06-28 12:09:36,783 main.py:51] epoch 1804, training loss: 11465.57, average training loss: 10655.48, base loss: 14378.60
[INFO 2017-06-28 12:09:37,120 main.py:51] epoch 1805, training loss: 10255.14, average training loss: 10653.53, base loss: 14375.36
[INFO 2017-06-28 12:09:37,485 main.py:51] epoch 1806, training loss: 10901.36, average training loss: 10655.12, base loss: 14377.05
[INFO 2017-06-28 12:09:37,814 main.py:51] epoch 1807, training loss: 10512.93, average training loss: 10654.67, base loss: 14376.80
[INFO 2017-06-28 12:09:38,159 main.py:51] epoch 1808, training loss: 10852.75, average training loss: 10654.53, base loss: 14377.88
[INFO 2017-06-28 12:09:38,482 main.py:51] epoch 1809, training loss: 10810.91, average training loss: 10655.93, base loss: 14379.43
[INFO 2017-06-28 12:09:38,819 main.py:51] epoch 1810, training loss: 11424.95, average training loss: 10657.39, base loss: 14381.65
[INFO 2017-06-28 12:09:39,161 main.py:51] epoch 1811, training loss: 10016.68, average training loss: 10655.76, base loss: 14378.99
[INFO 2017-06-28 12:09:39,534 main.py:51] epoch 1812, training loss: 10529.06, average training loss: 10656.39, base loss: 14380.28
[INFO 2017-06-28 12:09:39,862 main.py:51] epoch 1813, training loss: 11272.19, average training loss: 10655.23, base loss: 14378.72
[INFO 2017-06-28 12:09:40,208 main.py:51] epoch 1814, training loss: 9059.90, average training loss: 10653.88, base loss: 14376.36
[INFO 2017-06-28 12:09:40,542 main.py:51] epoch 1815, training loss: 11578.39, average training loss: 10655.38, base loss: 14380.49
[INFO 2017-06-28 12:09:40,880 main.py:51] epoch 1816, training loss: 10008.54, average training loss: 10654.99, base loss: 14380.04
[INFO 2017-06-28 12:09:41,197 main.py:51] epoch 1817, training loss: 10204.29, average training loss: 10653.18, base loss: 14377.80
[INFO 2017-06-28 12:09:41,536 main.py:51] epoch 1818, training loss: 11626.56, average training loss: 10654.29, base loss: 14380.19
[INFO 2017-06-28 12:09:41,886 main.py:51] epoch 1819, training loss: 10343.47, average training loss: 10654.22, base loss: 14381.10
[INFO 2017-06-28 12:09:42,205 main.py:51] epoch 1820, training loss: 9270.03, average training loss: 10652.48, base loss: 14380.14
[INFO 2017-06-28 12:09:42,562 main.py:51] epoch 1821, training loss: 9681.80, average training loss: 10650.70, base loss: 14376.85
[INFO 2017-06-28 12:09:42,901 main.py:51] epoch 1822, training loss: 10853.22, average training loss: 10651.50, base loss: 14377.30
