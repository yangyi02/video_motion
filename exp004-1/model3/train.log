[INFO 2017-06-28 12:46:13,597 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=10, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-28 12:46:16,461 main.py:51] epoch 0, training loss: 28457.04, average training loss: 28457.04, base loss: 14545.54
[INFO 2017-06-28 12:46:17,034 main.py:51] epoch 1, training loss: 21443.09, average training loss: 24950.06, base loss: 14019.98
[INFO 2017-06-28 12:46:17,573 main.py:51] epoch 2, training loss: 17489.87, average training loss: 22463.33, base loss: 13216.94
[INFO 2017-06-28 12:46:18,159 main.py:51] epoch 3, training loss: 20091.29, average training loss: 21870.32, base loss: 13994.30
[INFO 2017-06-28 12:46:18,740 main.py:51] epoch 4, training loss: 16557.86, average training loss: 20807.83, base loss: 13861.86
[INFO 2017-06-28 12:46:19,304 main.py:51] epoch 5, training loss: 16666.68, average training loss: 20117.64, base loss: 13864.67
[INFO 2017-06-28 12:46:19,860 main.py:51] epoch 6, training loss: 16473.33, average training loss: 19597.02, base loss: 14066.26
[INFO 2017-06-28 12:46:20,397 main.py:51] epoch 7, training loss: 15755.80, average training loss: 19116.87, base loss: 14182.92
[INFO 2017-06-28 12:46:20,959 main.py:51] epoch 8, training loss: 13675.14, average training loss: 18512.23, base loss: 14046.49
[INFO 2017-06-28 12:46:21,537 main.py:51] epoch 9, training loss: 12539.52, average training loss: 17914.96, base loss: 13828.59
[INFO 2017-06-28 12:46:22,168 main.py:51] epoch 10, training loss: 13779.78, average training loss: 17539.04, base loss: 13805.79
[INFO 2017-06-28 12:46:22,825 main.py:51] epoch 11, training loss: 15820.58, average training loss: 17395.83, base loss: 13989.41
[INFO 2017-06-28 12:46:23,467 main.py:51] epoch 12, training loss: 13090.71, average training loss: 17064.67, base loss: 13930.98
[INFO 2017-06-28 12:46:24,121 main.py:51] epoch 13, training loss: 16871.67, average training loss: 17050.88, base loss: 14159.11
[INFO 2017-06-28 12:46:24,806 main.py:51] epoch 14, training loss: 13568.32, average training loss: 16818.71, base loss: 14147.71
[INFO 2017-06-28 12:46:25,446 main.py:51] epoch 15, training loss: 13675.34, average training loss: 16622.25, base loss: 14123.32
[INFO 2017-06-28 12:46:26,118 main.py:51] epoch 16, training loss: 15448.65, average training loss: 16553.22, base loss: 14225.49
[INFO 2017-06-28 12:46:26,781 main.py:51] epoch 17, training loss: 12238.37, average training loss: 16313.50, base loss: 14133.11
[INFO 2017-06-28 12:46:27,419 main.py:51] epoch 18, training loss: 12182.70, average training loss: 16096.09, base loss: 14041.46
[INFO 2017-06-28 12:46:28,072 main.py:51] epoch 19, training loss: 14554.57, average training loss: 16019.02, base loss: 14093.77
[INFO 2017-06-28 12:46:28,726 main.py:51] epoch 20, training loss: 15441.48, average training loss: 15991.51, base loss: 14185.40
[INFO 2017-06-28 12:46:29,373 main.py:51] epoch 21, training loss: 15310.26, average training loss: 15960.55, base loss: 14270.50
[INFO 2017-06-28 12:46:30,025 main.py:51] epoch 22, training loss: 16304.01, average training loss: 15975.48, base loss: 14383.68
[INFO 2017-06-28 12:46:30,677 main.py:51] epoch 23, training loss: 14280.64, average training loss: 15904.86, base loss: 14403.96
[INFO 2017-06-28 12:46:31,316 main.py:51] epoch 24, training loss: 12990.49, average training loss: 15788.29, base loss: 14363.99
[INFO 2017-06-28 12:46:31,965 main.py:51] epoch 25, training loss: 14595.12, average training loss: 15742.40, base loss: 14399.25
[INFO 2017-06-28 12:46:32,611 main.py:51] epoch 26, training loss: 13292.00, average training loss: 15651.64, base loss: 14377.66
[INFO 2017-06-28 12:46:33,308 main.py:51] epoch 27, training loss: 17429.71, average training loss: 15715.14, base loss: 14520.30
[INFO 2017-06-28 12:46:33,966 main.py:51] epoch 28, training loss: 13327.40, average training loss: 15632.81, base loss: 14507.32
[INFO 2017-06-28 12:46:34,631 main.py:51] epoch 29, training loss: 11574.07, average training loss: 15497.52, base loss: 14435.93
[INFO 2017-06-28 12:46:35,280 main.py:51] epoch 30, training loss: 15979.21, average training loss: 15513.06, base loss: 14515.60
[INFO 2017-06-28 12:46:35,923 main.py:51] epoch 31, training loss: 15775.73, average training loss: 15521.26, base loss: 14577.71
[INFO 2017-06-28 12:46:36,604 main.py:51] epoch 32, training loss: 13540.45, average training loss: 15461.24, base loss: 14577.41
[INFO 2017-06-28 12:46:37,252 main.py:51] epoch 33, training loss: 11730.04, average training loss: 15351.50, base loss: 14501.03
[INFO 2017-06-28 12:46:37,911 main.py:51] epoch 34, training loss: 14407.01, average training loss: 15324.51, base loss: 14525.40
[INFO 2017-06-28 12:46:38,568 main.py:51] epoch 35, training loss: 13766.49, average training loss: 15281.23, base loss: 14524.63
[INFO 2017-06-28 12:46:39,221 main.py:51] epoch 36, training loss: 13535.52, average training loss: 15234.05, base loss: 14527.63
[INFO 2017-06-28 12:46:39,849 main.py:51] epoch 37, training loss: 12334.67, average training loss: 15157.75, base loss: 14485.21
[INFO 2017-06-28 12:46:40,496 main.py:51] epoch 38, training loss: 12798.21, average training loss: 15097.25, base loss: 14468.81
[INFO 2017-06-28 12:46:41,172 main.py:51] epoch 39, training loss: 16328.06, average training loss: 15128.02, base loss: 14538.89
[INFO 2017-06-28 12:46:41,830 main.py:51] epoch 40, training loss: 11742.30, average training loss: 15045.44, base loss: 14483.80
[INFO 2017-06-28 12:46:42,474 main.py:51] epoch 41, training loss: 13128.38, average training loss: 14999.80, base loss: 14475.67
[INFO 2017-06-28 12:46:43,132 main.py:51] epoch 42, training loss: 16884.01, average training loss: 15043.62, base loss: 14571.30
[INFO 2017-06-28 12:46:43,794 main.py:51] epoch 43, training loss: 12776.81, average training loss: 14992.10, base loss: 14548.84
[INFO 2017-06-28 12:46:44,440 main.py:51] epoch 44, training loss: 12786.66, average training loss: 14943.09, base loss: 14533.27
[INFO 2017-06-28 12:46:45,095 main.py:51] epoch 45, training loss: 14131.03, average training loss: 14925.44, base loss: 14547.04
[INFO 2017-06-28 12:46:45,755 main.py:51] epoch 46, training loss: 14578.40, average training loss: 14918.05, base loss: 14579.59
[INFO 2017-06-28 12:46:46,429 main.py:51] epoch 47, training loss: 12516.76, average training loss: 14868.03, base loss: 14562.06
[INFO 2017-06-28 12:46:47,100 main.py:51] epoch 48, training loss: 15919.34, average training loss: 14889.48, base loss: 14616.91
[INFO 2017-06-28 12:46:47,766 main.py:51] epoch 49, training loss: 13848.10, average training loss: 14868.65, base loss: 14627.29
[INFO 2017-06-28 12:46:48,420 main.py:51] epoch 50, training loss: 13812.28, average training loss: 14847.94, base loss: 14638.45
[INFO 2017-06-28 12:46:49,083 main.py:51] epoch 51, training loss: 13694.55, average training loss: 14825.76, base loss: 14648.33
[INFO 2017-06-28 12:46:49,770 main.py:51] epoch 52, training loss: 12862.79, average training loss: 14788.72, base loss: 14641.86
[INFO 2017-06-28 12:46:50,421 main.py:51] epoch 53, training loss: 13787.49, average training loss: 14770.18, base loss: 14643.89
[INFO 2017-06-28 12:46:51,085 main.py:51] epoch 54, training loss: 12324.97, average training loss: 14725.72, base loss: 14624.53
[INFO 2017-06-28 12:46:51,735 main.py:51] epoch 55, training loss: 11663.92, average training loss: 14671.05, base loss: 14600.63
[INFO 2017-06-28 12:46:52,400 main.py:51] epoch 56, training loss: 11490.66, average training loss: 14615.25, base loss: 14566.14
[INFO 2017-06-28 12:46:53,044 main.py:51] epoch 57, training loss: 13707.69, average training loss: 14599.60, base loss: 14570.05
[INFO 2017-06-28 12:46:53,726 main.py:51] epoch 58, training loss: 14614.08, average training loss: 14599.85, base loss: 14595.98
[INFO 2017-06-28 12:46:54,374 main.py:51] epoch 59, training loss: 13170.81, average training loss: 14576.03, base loss: 14598.13
[INFO 2017-06-28 12:46:55,027 main.py:51] epoch 60, training loss: 12398.91, average training loss: 14540.34, base loss: 14579.35
[INFO 2017-06-28 12:46:55,686 main.py:51] epoch 61, training loss: 12578.14, average training loss: 14508.69, base loss: 14573.01
[INFO 2017-06-28 12:46:56,340 main.py:51] epoch 62, training loss: 14005.69, average training loss: 14500.71, base loss: 14591.73
[INFO 2017-06-28 12:46:56,987 main.py:51] epoch 63, training loss: 14150.88, average training loss: 14495.24, base loss: 14603.78
[INFO 2017-06-28 12:46:57,653 main.py:51] epoch 64, training loss: 12401.10, average training loss: 14463.03, base loss: 14589.43
[INFO 2017-06-28 12:46:58,275 main.py:51] epoch 65, training loss: 12529.98, average training loss: 14433.74, base loss: 14578.92
[INFO 2017-06-28 12:46:58,945 main.py:51] epoch 66, training loss: 12683.63, average training loss: 14407.62, base loss: 14573.01
[INFO 2017-06-28 12:46:59,628 main.py:51] epoch 67, training loss: 11860.06, average training loss: 14370.15, base loss: 14553.07
[INFO 2017-06-28 12:47:00,279 main.py:51] epoch 68, training loss: 13188.47, average training loss: 14353.03, base loss: 14559.58
[INFO 2017-06-28 12:47:00,930 main.py:51] epoch 69, training loss: 13793.19, average training loss: 14345.03, base loss: 14568.23
[INFO 2017-06-28 12:47:01,606 main.py:51] epoch 70, training loss: 11481.11, average training loss: 14304.69, base loss: 14539.54
[INFO 2017-06-28 12:47:02,250 main.py:51] epoch 71, training loss: 11794.53, average training loss: 14269.83, base loss: 14522.91
[INFO 2017-06-28 12:47:02,899 main.py:51] epoch 72, training loss: 13361.59, average training loss: 14257.39, base loss: 14529.80
[INFO 2017-06-28 12:47:03,558 main.py:51] epoch 73, training loss: 11616.08, average training loss: 14221.69, base loss: 14513.13
[INFO 2017-06-28 12:47:04,222 main.py:51] epoch 74, training loss: 11458.90, average training loss: 14184.86, base loss: 14482.47
[INFO 2017-06-28 12:47:04,894 main.py:51] epoch 75, training loss: 10506.83, average training loss: 14136.46, base loss: 14445.50
[INFO 2017-06-28 12:47:05,530 main.py:51] epoch 76, training loss: 11340.72, average training loss: 14100.15, base loss: 14421.78
[INFO 2017-06-28 12:47:06,221 main.py:51] epoch 77, training loss: 12807.06, average training loss: 14083.57, base loss: 14418.22
[INFO 2017-06-28 12:47:06,872 main.py:51] epoch 78, training loss: 13059.44, average training loss: 14070.61, base loss: 14429.68
[INFO 2017-06-28 12:47:07,527 main.py:51] epoch 79, training loss: 13707.74, average training loss: 14066.07, base loss: 14445.36
[INFO 2017-06-28 12:47:08,177 main.py:51] epoch 80, training loss: 14159.50, average training loss: 14067.23, base loss: 14458.71
[INFO 2017-06-28 12:47:08,839 main.py:51] epoch 81, training loss: 13089.78, average training loss: 14055.31, base loss: 14465.67
[INFO 2017-06-28 12:47:09,508 main.py:51] epoch 82, training loss: 13139.54, average training loss: 14044.27, base loss: 14474.35
[INFO 2017-06-28 12:47:10,167 main.py:51] epoch 83, training loss: 12865.76, average training loss: 14030.24, base loss: 14484.62
[INFO 2017-06-28 12:47:10,832 main.py:51] epoch 84, training loss: 12203.78, average training loss: 14008.76, base loss: 14488.34
[INFO 2017-06-28 12:47:11,508 main.py:51] epoch 85, training loss: 14766.36, average training loss: 14017.57, base loss: 14522.60
[INFO 2017-06-28 12:47:12,177 main.py:51] epoch 86, training loss: 13502.72, average training loss: 14011.65, base loss: 14530.83
[INFO 2017-06-28 12:47:12,824 main.py:51] epoch 87, training loss: 12091.98, average training loss: 13989.83, base loss: 14518.34
[INFO 2017-06-28 12:47:13,502 main.py:51] epoch 88, training loss: 11657.59, average training loss: 13963.63, base loss: 14501.50
[INFO 2017-06-28 12:47:14,156 main.py:51] epoch 89, training loss: 11632.60, average training loss: 13937.73, base loss: 14492.57
[INFO 2017-06-28 12:47:14,815 main.py:51] epoch 90, training loss: 12552.70, average training loss: 13922.51, base loss: 14485.31
[INFO 2017-06-28 12:47:15,470 main.py:51] epoch 91, training loss: 13565.62, average training loss: 13918.63, base loss: 14502.96
[INFO 2017-06-28 12:47:16,119 main.py:51] epoch 92, training loss: 9963.83, average training loss: 13876.10, base loss: 14464.91
[INFO 2017-06-28 12:47:16,807 main.py:51] epoch 93, training loss: 11823.55, average training loss: 13854.27, base loss: 14459.94
[INFO 2017-06-28 12:47:17,463 main.py:51] epoch 94, training loss: 13643.86, average training loss: 13852.05, base loss: 14467.76
[INFO 2017-06-28 12:47:18,145 main.py:51] epoch 95, training loss: 11614.45, average training loss: 13828.75, base loss: 14455.79
[INFO 2017-06-28 12:47:18,792 main.py:51] epoch 96, training loss: 12577.58, average training loss: 13815.85, base loss: 14456.40
[INFO 2017-06-28 12:47:19,434 main.py:51] epoch 97, training loss: 11182.57, average training loss: 13788.98, base loss: 14441.54
[INFO 2017-06-28 12:47:20,090 main.py:51] epoch 98, training loss: 13457.43, average training loss: 13785.63, base loss: 14451.09
[INFO 2017-06-28 12:47:20,760 main.py:51] epoch 99, training loss: 12047.79, average training loss: 13768.25, base loss: 14445.83
[INFO 2017-06-28 12:47:20,761 main.py:53] epoch 99, testing
[INFO 2017-06-28 12:47:23,346 main.py:105] average testing loss: 13147.96, base loss: 14862.49
[INFO 2017-06-28 12:47:23,346 main.py:106] improve_loss: 1714.54, improve_percent: 0.12
[INFO 2017-06-28 12:47:23,347 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:47:23,381 main.py:76] current best improved percent: 0.12
[INFO 2017-06-28 12:47:24,033 main.py:51] epoch 100, training loss: 11629.71, average training loss: 13747.08, base loss: 14435.10
[INFO 2017-06-28 12:47:24,727 main.py:51] epoch 101, training loss: 11176.12, average training loss: 13721.87, base loss: 14422.39
[INFO 2017-06-28 12:47:25,369 main.py:51] epoch 102, training loss: 11670.47, average training loss: 13701.95, base loss: 14413.13
[INFO 2017-06-28 12:47:26,031 main.py:51] epoch 103, training loss: 13019.04, average training loss: 13695.39, base loss: 14422.44
[INFO 2017-06-28 12:47:26,720 main.py:51] epoch 104, training loss: 14111.82, average training loss: 13699.35, base loss: 14449.44
[INFO 2017-06-28 12:47:27,378 main.py:51] epoch 105, training loss: 13022.18, average training loss: 13692.97, base loss: 14459.19
[INFO 2017-06-28 12:47:28,013 main.py:51] epoch 106, training loss: 12006.58, average training loss: 13677.20, base loss: 14453.68
[INFO 2017-06-28 12:47:28,669 main.py:51] epoch 107, training loss: 13149.00, average training loss: 13672.31, base loss: 14461.77
[INFO 2017-06-28 12:47:29,326 main.py:51] epoch 108, training loss: 13936.81, average training loss: 13674.74, base loss: 14476.37
[INFO 2017-06-28 12:47:29,996 main.py:51] epoch 109, training loss: 13545.47, average training loss: 13673.57, base loss: 14490.27
[INFO 2017-06-28 12:47:30,635 main.py:51] epoch 110, training loss: 12339.66, average training loss: 13661.55, base loss: 14487.39
[INFO 2017-06-28 12:47:31,288 main.py:51] epoch 111, training loss: 11860.69, average training loss: 13645.47, base loss: 14479.25
[INFO 2017-06-28 12:47:31,959 main.py:51] epoch 112, training loss: 11903.63, average training loss: 13630.05, base loss: 14478.46
[INFO 2017-06-28 12:47:32,617 main.py:51] epoch 113, training loss: 10185.41, average training loss: 13599.84, base loss: 14460.37
[INFO 2017-06-28 12:47:33,267 main.py:51] epoch 114, training loss: 11725.01, average training loss: 13583.54, base loss: 14454.13
[INFO 2017-06-28 12:47:33,914 main.py:51] epoch 115, training loss: 12417.40, average training loss: 13573.48, base loss: 14456.74
[INFO 2017-06-28 12:47:34,592 main.py:51] epoch 116, training loss: 10628.58, average training loss: 13548.31, base loss: 14442.90
[INFO 2017-06-28 12:47:35,246 main.py:51] epoch 117, training loss: 11202.11, average training loss: 13528.43, base loss: 14430.73
[INFO 2017-06-28 12:47:35,890 main.py:51] epoch 118, training loss: 12877.53, average training loss: 13522.96, base loss: 14437.18
[INFO 2017-06-28 12:47:36,517 main.py:51] epoch 119, training loss: 11987.04, average training loss: 13510.16, base loss: 14437.13
[INFO 2017-06-28 12:47:37,180 main.py:51] epoch 120, training loss: 11032.25, average training loss: 13489.68, base loss: 14418.71
[INFO 2017-06-28 12:47:37,842 main.py:51] epoch 121, training loss: 12672.86, average training loss: 13482.99, base loss: 14419.12
[INFO 2017-06-28 12:47:38,497 main.py:51] epoch 122, training loss: 11746.83, average training loss: 13468.87, base loss: 14411.59
[INFO 2017-06-28 12:47:39,139 main.py:51] epoch 123, training loss: 13466.91, average training loss: 13468.86, base loss: 14420.32
[INFO 2017-06-28 12:47:39,797 main.py:51] epoch 124, training loss: 11320.54, average training loss: 13451.67, base loss: 14414.24
[INFO 2017-06-28 12:47:40,457 main.py:51] epoch 125, training loss: 12601.95, average training loss: 13444.93, base loss: 14420.72
[INFO 2017-06-28 12:47:41,096 main.py:51] epoch 126, training loss: 13549.32, average training loss: 13445.75, base loss: 14430.94
[INFO 2017-06-28 12:47:41,736 main.py:51] epoch 127, training loss: 12727.42, average training loss: 13440.14, base loss: 14433.99
[INFO 2017-06-28 12:47:42,382 main.py:51] epoch 128, training loss: 12288.15, average training loss: 13431.21, base loss: 14434.65
[INFO 2017-06-28 12:47:43,044 main.py:51] epoch 129, training loss: 12000.62, average training loss: 13420.20, base loss: 14431.25
[INFO 2017-06-28 12:47:43,693 main.py:51] epoch 130, training loss: 11694.04, average training loss: 13407.02, base loss: 14425.43
[INFO 2017-06-28 12:47:44,377 main.py:51] epoch 131, training loss: 11335.88, average training loss: 13391.33, base loss: 14417.49
[INFO 2017-06-28 12:47:45,046 main.py:51] epoch 132, training loss: 10244.27, average training loss: 13367.67, base loss: 14402.84
[INFO 2017-06-28 12:47:45,718 main.py:51] epoch 133, training loss: 14764.67, average training loss: 13378.10, base loss: 14427.00
[INFO 2017-06-28 12:47:46,388 main.py:51] epoch 134, training loss: 10653.89, average training loss: 13357.92, base loss: 14416.33
[INFO 2017-06-28 12:47:47,029 main.py:51] epoch 135, training loss: 10639.42, average training loss: 13337.93, base loss: 14409.15
[INFO 2017-06-28 12:47:47,672 main.py:51] epoch 136, training loss: 13084.52, average training loss: 13336.08, base loss: 14419.33
[INFO 2017-06-28 12:47:48,327 main.py:51] epoch 137, training loss: 11399.21, average training loss: 13322.04, base loss: 14415.35
[INFO 2017-06-28 12:47:48,997 main.py:51] epoch 138, training loss: 11509.80, average training loss: 13309.01, base loss: 14411.23
[INFO 2017-06-28 12:47:49,671 main.py:51] epoch 139, training loss: 11799.41, average training loss: 13298.22, base loss: 14414.03
[INFO 2017-06-28 12:47:50,333 main.py:51] epoch 140, training loss: 11393.52, average training loss: 13284.71, base loss: 14405.02
[INFO 2017-06-28 12:47:50,990 main.py:51] epoch 141, training loss: 11693.44, average training loss: 13273.51, base loss: 14405.75
[INFO 2017-06-28 12:47:51,643 main.py:51] epoch 142, training loss: 12111.94, average training loss: 13265.39, base loss: 14405.11
[INFO 2017-06-28 12:47:52,301 main.py:51] epoch 143, training loss: 11830.63, average training loss: 13255.42, base loss: 14404.60
[INFO 2017-06-28 12:47:52,955 main.py:51] epoch 144, training loss: 10763.11, average training loss: 13238.23, base loss: 14399.88
[INFO 2017-06-28 12:47:53,604 main.py:51] epoch 145, training loss: 12319.27, average training loss: 13231.94, base loss: 14399.40
[INFO 2017-06-28 12:47:54,271 main.py:51] epoch 146, training loss: 12718.91, average training loss: 13228.45, base loss: 14406.04
[INFO 2017-06-28 12:47:54,933 main.py:51] epoch 147, training loss: 11992.82, average training loss: 13220.10, base loss: 14404.51
[INFO 2017-06-28 12:47:55,610 main.py:51] epoch 148, training loss: 11254.70, average training loss: 13206.91, base loss: 14403.21
[INFO 2017-06-28 12:47:56,267 main.py:51] epoch 149, training loss: 12272.79, average training loss: 13200.68, base loss: 14406.28
[INFO 2017-06-28 12:47:56,902 main.py:51] epoch 150, training loss: 12561.40, average training loss: 13196.45, base loss: 14412.23
[INFO 2017-06-28 12:47:57,548 main.py:51] epoch 151, training loss: 12476.29, average training loss: 13191.71, base loss: 14411.80
[INFO 2017-06-28 12:47:58,198 main.py:51] epoch 152, training loss: 12077.39, average training loss: 13184.43, base loss: 14412.82
[INFO 2017-06-28 12:47:58,865 main.py:51] epoch 153, training loss: 12415.30, average training loss: 13179.43, base loss: 14415.50
[INFO 2017-06-28 12:47:59,531 main.py:51] epoch 154, training loss: 12215.53, average training loss: 13173.21, base loss: 14421.69
[INFO 2017-06-28 12:48:00,191 main.py:51] epoch 155, training loss: 12317.80, average training loss: 13167.73, base loss: 14424.37
[INFO 2017-06-28 12:48:00,852 main.py:51] epoch 156, training loss: 12618.08, average training loss: 13164.23, base loss: 14430.40
[INFO 2017-06-28 12:48:01,522 main.py:51] epoch 157, training loss: 12046.12, average training loss: 13157.15, base loss: 14425.38
[INFO 2017-06-28 12:48:02,163 main.py:51] epoch 158, training loss: 10767.51, average training loss: 13142.12, base loss: 14417.61
[INFO 2017-06-28 12:48:02,848 main.py:51] epoch 159, training loss: 10450.06, average training loss: 13125.30, base loss: 14407.52
[INFO 2017-06-28 12:48:03,509 main.py:51] epoch 160, training loss: 12003.68, average training loss: 13118.33, base loss: 14410.06
[INFO 2017-06-28 12:48:04,190 main.py:51] epoch 161, training loss: 9836.57, average training loss: 13098.07, base loss: 14395.10
[INFO 2017-06-28 12:48:04,826 main.py:51] epoch 162, training loss: 12601.64, average training loss: 13095.03, base loss: 14400.03
[INFO 2017-06-28 12:48:05,496 main.py:51] epoch 163, training loss: 10833.01, average training loss: 13081.24, base loss: 14390.51
[INFO 2017-06-28 12:48:06,152 main.py:51] epoch 164, training loss: 12215.95, average training loss: 13075.99, base loss: 14392.42
[INFO 2017-06-28 12:48:06,791 main.py:51] epoch 165, training loss: 10766.59, average training loss: 13062.08, base loss: 14382.92
[INFO 2017-06-28 12:48:07,451 main.py:51] epoch 166, training loss: 11046.05, average training loss: 13050.01, base loss: 14375.99
[INFO 2017-06-28 12:48:08,114 main.py:51] epoch 167, training loss: 10585.69, average training loss: 13035.34, base loss: 14369.14
[INFO 2017-06-28 12:48:08,782 main.py:51] epoch 168, training loss: 11290.22, average training loss: 13025.01, base loss: 14367.19
[INFO 2017-06-28 12:48:09,426 main.py:51] epoch 169, training loss: 10949.28, average training loss: 13012.80, base loss: 14363.93
[INFO 2017-06-28 12:48:10,092 main.py:51] epoch 170, training loss: 12225.68, average training loss: 13008.20, base loss: 14367.09
[INFO 2017-06-28 12:48:10,731 main.py:51] epoch 171, training loss: 10609.52, average training loss: 12994.25, base loss: 14355.68
[INFO 2017-06-28 12:48:11,379 main.py:51] epoch 172, training loss: 12907.15, average training loss: 12993.75, base loss: 14368.13
[INFO 2017-06-28 12:48:12,022 main.py:51] epoch 173, training loss: 12461.48, average training loss: 12990.69, base loss: 14371.87
[INFO 2017-06-28 12:48:12,696 main.py:51] epoch 174, training loss: 11191.40, average training loss: 12980.41, base loss: 14366.54
[INFO 2017-06-28 12:48:13,338 main.py:51] epoch 175, training loss: 11386.70, average training loss: 12971.35, base loss: 14363.14
[INFO 2017-06-28 12:48:13,992 main.py:51] epoch 176, training loss: 11434.67, average training loss: 12962.67, base loss: 14362.20
[INFO 2017-06-28 12:48:14,658 main.py:51] epoch 177, training loss: 10064.42, average training loss: 12946.39, base loss: 14352.99
[INFO 2017-06-28 12:48:15,318 main.py:51] epoch 178, training loss: 12422.44, average training loss: 12943.46, base loss: 14358.35
[INFO 2017-06-28 12:48:15,966 main.py:51] epoch 179, training loss: 10041.70, average training loss: 12927.34, base loss: 14348.97
[INFO 2017-06-28 12:48:16,607 main.py:51] epoch 180, training loss: 10164.58, average training loss: 12912.08, base loss: 14336.77
[INFO 2017-06-28 12:48:17,273 main.py:51] epoch 181, training loss: 12782.05, average training loss: 12911.36, base loss: 14340.39
[INFO 2017-06-28 12:48:17,914 main.py:51] epoch 182, training loss: 13420.41, average training loss: 12914.15, base loss: 14354.60
[INFO 2017-06-28 12:48:18,577 main.py:51] epoch 183, training loss: 11595.22, average training loss: 12906.98, base loss: 14355.63
[INFO 2017-06-28 12:48:19,218 main.py:51] epoch 184, training loss: 11449.01, average training loss: 12899.10, base loss: 14359.24
[INFO 2017-06-28 12:48:19,867 main.py:51] epoch 185, training loss: 10895.91, average training loss: 12888.33, base loss: 14348.46
[INFO 2017-06-28 12:48:20,515 main.py:51] epoch 186, training loss: 11854.68, average training loss: 12882.80, base loss: 14350.66
[INFO 2017-06-28 12:48:21,170 main.py:51] epoch 187, training loss: 12487.71, average training loss: 12880.70, base loss: 14356.77
[INFO 2017-06-28 12:48:21,834 main.py:51] epoch 188, training loss: 9964.00, average training loss: 12865.27, base loss: 14344.03
[INFO 2017-06-28 12:48:22,500 main.py:51] epoch 189, training loss: 10501.34, average training loss: 12852.82, base loss: 14333.88
[INFO 2017-06-28 12:48:23,145 main.py:51] epoch 190, training loss: 11605.55, average training loss: 12846.29, base loss: 14333.30
[INFO 2017-06-28 12:48:23,825 main.py:51] epoch 191, training loss: 10838.42, average training loss: 12835.84, base loss: 14328.47
[INFO 2017-06-28 12:48:24,468 main.py:51] epoch 192, training loss: 10137.57, average training loss: 12821.86, base loss: 14321.55
[INFO 2017-06-28 12:48:25,122 main.py:51] epoch 193, training loss: 12473.57, average training loss: 12820.06, base loss: 14327.88
[INFO 2017-06-28 12:48:25,780 main.py:51] epoch 194, training loss: 11450.12, average training loss: 12813.03, base loss: 14327.15
[INFO 2017-06-28 12:48:26,444 main.py:51] epoch 195, training loss: 10865.02, average training loss: 12803.10, base loss: 14322.17
[INFO 2017-06-28 12:48:27,105 main.py:51] epoch 196, training loss: 10610.05, average training loss: 12791.96, base loss: 14315.42
[INFO 2017-06-28 12:48:27,770 main.py:51] epoch 197, training loss: 12077.53, average training loss: 12788.36, base loss: 14317.81
[INFO 2017-06-28 12:48:28,407 main.py:51] epoch 198, training loss: 10543.29, average training loss: 12777.07, base loss: 14309.91
[INFO 2017-06-28 12:48:29,074 main.py:51] epoch 199, training loss: 11165.88, average training loss: 12769.02, base loss: 14308.70
[INFO 2017-06-28 12:48:29,074 main.py:53] epoch 199, testing
[INFO 2017-06-28 12:48:31,699 main.py:105] average testing loss: 12436.71, base loss: 14956.07
[INFO 2017-06-28 12:48:31,700 main.py:106] improve_loss: 2519.35, improve_percent: 0.17
[INFO 2017-06-28 12:48:31,700 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:48:31,738 main.py:76] current best improved percent: 0.17
[INFO 2017-06-28 12:48:32,397 main.py:51] epoch 200, training loss: 11711.30, average training loss: 12763.76, base loss: 14310.79
[INFO 2017-06-28 12:48:33,054 main.py:51] epoch 201, training loss: 10775.81, average training loss: 12753.91, base loss: 14305.36
[INFO 2017-06-28 12:48:33,712 main.py:51] epoch 202, training loss: 9388.83, average training loss: 12737.34, base loss: 14292.42
[INFO 2017-06-28 12:48:34,375 main.py:51] epoch 203, training loss: 11935.01, average training loss: 12733.40, base loss: 14294.88
[INFO 2017-06-28 12:48:35,037 main.py:51] epoch 204, training loss: 10571.00, average training loss: 12722.86, base loss: 14285.42
[INFO 2017-06-28 12:48:35,696 main.py:51] epoch 205, training loss: 10129.65, average training loss: 12710.27, base loss: 14276.68
[INFO 2017-06-28 12:48:36,353 main.py:51] epoch 206, training loss: 11617.83, average training loss: 12704.99, base loss: 14280.02
[INFO 2017-06-28 12:48:37,026 main.py:51] epoch 207, training loss: 13272.95, average training loss: 12707.72, base loss: 14293.02
[INFO 2017-06-28 12:48:37,680 main.py:51] epoch 208, training loss: 11897.90, average training loss: 12703.85, base loss: 14295.68
[INFO 2017-06-28 12:48:38,362 main.py:51] epoch 209, training loss: 12340.30, average training loss: 12702.11, base loss: 14300.08
[INFO 2017-06-28 12:48:39,023 main.py:51] epoch 210, training loss: 12531.01, average training loss: 12701.30, base loss: 14303.06
[INFO 2017-06-28 12:48:39,666 main.py:51] epoch 211, training loss: 11442.56, average training loss: 12695.37, base loss: 14298.89
[INFO 2017-06-28 12:48:40,331 main.py:51] epoch 212, training loss: 10291.07, average training loss: 12684.08, base loss: 14291.93
[INFO 2017-06-28 12:48:40,974 main.py:51] epoch 213, training loss: 10614.95, average training loss: 12674.41, base loss: 14288.17
[INFO 2017-06-28 12:48:41,628 main.py:51] epoch 214, training loss: 11654.98, average training loss: 12669.67, base loss: 14287.62
[INFO 2017-06-28 12:48:42,287 main.py:51] epoch 215, training loss: 11072.46, average training loss: 12662.27, base loss: 14284.62
[INFO 2017-06-28 12:48:42,950 main.py:51] epoch 216, training loss: 10672.24, average training loss: 12653.10, base loss: 14278.95
[INFO 2017-06-28 12:48:43,619 main.py:51] epoch 217, training loss: 11264.54, average training loss: 12646.73, base loss: 14276.64
[INFO 2017-06-28 12:48:44,277 main.py:51] epoch 218, training loss: 11514.79, average training loss: 12641.56, base loss: 14279.08
[INFO 2017-06-28 12:48:44,937 main.py:51] epoch 219, training loss: 11922.52, average training loss: 12638.30, base loss: 14286.94
[INFO 2017-06-28 12:48:45,594 main.py:51] epoch 220, training loss: 10918.27, average training loss: 12630.51, base loss: 14282.57
[INFO 2017-06-28 12:48:46,277 main.py:51] epoch 221, training loss: 11631.44, average training loss: 12626.01, base loss: 14284.17
[INFO 2017-06-28 12:48:46,938 main.py:51] epoch 222, training loss: 10060.49, average training loss: 12614.51, base loss: 14275.79
[INFO 2017-06-28 12:48:47,591 main.py:51] epoch 223, training loss: 10952.91, average training loss: 12607.09, base loss: 14271.83
[INFO 2017-06-28 12:48:48,258 main.py:51] epoch 224, training loss: 12117.15, average training loss: 12604.91, base loss: 14274.39
[INFO 2017-06-28 12:48:48,902 main.py:51] epoch 225, training loss: 10459.61, average training loss: 12595.42, base loss: 14265.50
[INFO 2017-06-28 12:48:49,566 main.py:51] epoch 226, training loss: 12074.34, average training loss: 12593.12, base loss: 14267.44
[INFO 2017-06-28 12:48:50,202 main.py:51] epoch 227, training loss: 10238.80, average training loss: 12582.80, base loss: 14260.12
[INFO 2017-06-28 12:48:50,864 main.py:51] epoch 228, training loss: 9326.25, average training loss: 12568.58, base loss: 14246.49
[INFO 2017-06-28 12:48:51,512 main.py:51] epoch 229, training loss: 11331.89, average training loss: 12563.20, base loss: 14246.12
[INFO 2017-06-28 12:48:52,147 main.py:51] epoch 230, training loss: 10699.83, average training loss: 12555.13, base loss: 14243.84
[INFO 2017-06-28 12:48:52,802 main.py:51] epoch 231, training loss: 11578.87, average training loss: 12550.93, base loss: 14241.23
[INFO 2017-06-28 12:48:53,458 main.py:51] epoch 232, training loss: 10788.25, average training loss: 12543.36, base loss: 14239.90
[INFO 2017-06-28 12:48:54,105 main.py:51] epoch 233, training loss: 12136.06, average training loss: 12541.62, base loss: 14244.16
[INFO 2017-06-28 12:48:54,755 main.py:51] epoch 234, training loss: 12498.44, average training loss: 12541.44, base loss: 14251.20
[INFO 2017-06-28 12:48:55,401 main.py:51] epoch 235, training loss: 13240.01, average training loss: 12544.40, base loss: 14257.00
[INFO 2017-06-28 12:48:56,054 main.py:51] epoch 236, training loss: 11431.43, average training loss: 12539.70, base loss: 14258.64
[INFO 2017-06-28 12:48:56,698 main.py:51] epoch 237, training loss: 11394.47, average training loss: 12534.89, base loss: 14260.44
[INFO 2017-06-28 12:48:57,359 main.py:51] epoch 238, training loss: 10655.65, average training loss: 12527.03, base loss: 14256.10
[INFO 2017-06-28 12:48:58,008 main.py:51] epoch 239, training loss: 9485.95, average training loss: 12514.36, base loss: 14242.76
[INFO 2017-06-28 12:48:58,673 main.py:51] epoch 240, training loss: 11304.60, average training loss: 12509.34, base loss: 14245.14
[INFO 2017-06-28 12:48:59,333 main.py:51] epoch 241, training loss: 12514.37, average training loss: 12509.36, base loss: 14249.90
[INFO 2017-06-28 12:48:59,992 main.py:51] epoch 242, training loss: 11171.91, average training loss: 12503.85, base loss: 14248.78
[INFO 2017-06-28 12:49:00,637 main.py:51] epoch 243, training loss: 11917.67, average training loss: 12501.45, base loss: 14249.45
[INFO 2017-06-28 12:49:01,290 main.py:51] epoch 244, training loss: 11787.70, average training loss: 12498.54, base loss: 14250.42
[INFO 2017-06-28 12:49:01,939 main.py:51] epoch 245, training loss: 11255.82, average training loss: 12493.49, base loss: 14250.44
[INFO 2017-06-28 12:49:02,603 main.py:51] epoch 246, training loss: 10255.44, average training loss: 12484.42, base loss: 14244.55
[INFO 2017-06-28 12:49:03,250 main.py:51] epoch 247, training loss: 10319.50, average training loss: 12475.69, base loss: 14239.45
[INFO 2017-06-28 12:49:03,897 main.py:51] epoch 248, training loss: 10095.57, average training loss: 12466.14, base loss: 14232.17
[INFO 2017-06-28 12:49:04,539 main.py:51] epoch 249, training loss: 11005.14, average training loss: 12460.29, base loss: 14228.41
[INFO 2017-06-28 12:49:05,198 main.py:51] epoch 250, training loss: 12656.71, average training loss: 12461.07, base loss: 14234.27
[INFO 2017-06-28 12:49:05,833 main.py:51] epoch 251, training loss: 11886.24, average training loss: 12458.79, base loss: 14236.78
[INFO 2017-06-28 12:49:06,500 main.py:51] epoch 252, training loss: 13389.08, average training loss: 12462.47, base loss: 14245.77
[INFO 2017-06-28 12:49:07,152 main.py:51] epoch 253, training loss: 11358.83, average training loss: 12458.13, base loss: 14247.22
[INFO 2017-06-28 12:49:07,826 main.py:51] epoch 254, training loss: 10815.19, average training loss: 12451.68, base loss: 14242.85
[INFO 2017-06-28 12:49:08,462 main.py:51] epoch 255, training loss: 13919.86, average training loss: 12457.42, base loss: 14256.43
[INFO 2017-06-28 12:49:09,115 main.py:51] epoch 256, training loss: 11593.34, average training loss: 12454.06, base loss: 14258.08
[INFO 2017-06-28 12:49:09,777 main.py:51] epoch 257, training loss: 11556.32, average training loss: 12450.58, base loss: 14257.10
[INFO 2017-06-28 12:49:10,431 main.py:51] epoch 258, training loss: 11522.38, average training loss: 12446.99, base loss: 14257.02
[INFO 2017-06-28 12:49:11,099 main.py:51] epoch 259, training loss: 10634.26, average training loss: 12440.02, base loss: 14250.07
[INFO 2017-06-28 12:49:11,742 main.py:51] epoch 260, training loss: 11397.21, average training loss: 12436.02, base loss: 14250.38
[INFO 2017-06-28 12:49:12,418 main.py:51] epoch 261, training loss: 10771.99, average training loss: 12429.67, base loss: 14247.78
[INFO 2017-06-28 12:49:13,099 main.py:51] epoch 262, training loss: 10573.41, average training loss: 12422.62, base loss: 14241.85
[INFO 2017-06-28 12:49:13,760 main.py:51] epoch 263, training loss: 11791.97, average training loss: 12420.23, base loss: 14244.30
[INFO 2017-06-28 12:49:14,433 main.py:51] epoch 264, training loss: 11031.04, average training loss: 12414.98, base loss: 14244.89
[INFO 2017-06-28 12:49:15,124 main.py:51] epoch 265, training loss: 10928.39, average training loss: 12409.40, base loss: 14244.64
[INFO 2017-06-28 12:49:15,783 main.py:51] epoch 266, training loss: 9017.07, average training loss: 12396.69, base loss: 14232.91
[INFO 2017-06-28 12:49:16,435 main.py:51] epoch 267, training loss: 11802.73, average training loss: 12394.47, base loss: 14238.66
[INFO 2017-06-28 12:49:17,085 main.py:51] epoch 268, training loss: 10142.88, average training loss: 12386.10, base loss: 14229.88
[INFO 2017-06-28 12:49:17,769 main.py:51] epoch 269, training loss: 14323.88, average training loss: 12393.28, base loss: 14243.41
[INFO 2017-06-28 12:49:18,426 main.py:51] epoch 270, training loss: 11012.77, average training loss: 12388.19, base loss: 14244.44
[INFO 2017-06-28 12:49:19,089 main.py:51] epoch 271, training loss: 12776.69, average training loss: 12389.61, base loss: 14253.32
[INFO 2017-06-28 12:49:19,740 main.py:51] epoch 272, training loss: 10124.70, average training loss: 12381.32, base loss: 14247.36
[INFO 2017-06-28 12:49:20,402 main.py:51] epoch 273, training loss: 12620.66, average training loss: 12382.19, base loss: 14254.47
[INFO 2017-06-28 12:49:21,078 main.py:51] epoch 274, training loss: 10213.90, average training loss: 12374.31, base loss: 14250.58
[INFO 2017-06-28 12:49:21,735 main.py:51] epoch 275, training loss: 10723.92, average training loss: 12368.33, base loss: 14246.55
[INFO 2017-06-28 12:49:22,389 main.py:51] epoch 276, training loss: 9485.60, average training loss: 12357.92, base loss: 14238.41
[INFO 2017-06-28 12:49:23,055 main.py:51] epoch 277, training loss: 9672.30, average training loss: 12348.26, base loss: 14232.99
[INFO 2017-06-28 12:49:23,714 main.py:51] epoch 278, training loss: 10453.78, average training loss: 12341.47, base loss: 14230.74
[INFO 2017-06-28 12:49:24,378 main.py:51] epoch 279, training loss: 11546.16, average training loss: 12338.63, base loss: 14232.76
[INFO 2017-06-28 12:49:25,039 main.py:51] epoch 280, training loss: 10535.99, average training loss: 12332.21, base loss: 14231.07
[INFO 2017-06-28 12:49:25,695 main.py:51] epoch 281, training loss: 10894.36, average training loss: 12327.12, base loss: 14230.18
[INFO 2017-06-28 12:49:26,335 main.py:51] epoch 282, training loss: 10781.79, average training loss: 12321.66, base loss: 14228.32
[INFO 2017-06-28 12:49:26,974 main.py:51] epoch 283, training loss: 14344.04, average training loss: 12328.78, base loss: 14241.44
[INFO 2017-06-28 12:49:27,633 main.py:51] epoch 284, training loss: 11150.57, average training loss: 12324.64, base loss: 14242.33
[INFO 2017-06-28 12:49:28,297 main.py:51] epoch 285, training loss: 10695.33, average training loss: 12318.95, base loss: 14237.49
[INFO 2017-06-28 12:49:28,963 main.py:51] epoch 286, training loss: 11957.07, average training loss: 12317.68, base loss: 14242.97
[INFO 2017-06-28 12:49:29,613 main.py:51] epoch 287, training loss: 12317.97, average training loss: 12317.69, base loss: 14247.69
[INFO 2017-06-28 12:49:30,273 main.py:51] epoch 288, training loss: 11552.57, average training loss: 12315.04, base loss: 14246.04
[INFO 2017-06-28 12:49:30,946 main.py:51] epoch 289, training loss: 12206.28, average training loss: 12314.66, base loss: 14251.37
[INFO 2017-06-28 12:49:31,607 main.py:51] epoch 290, training loss: 10410.79, average training loss: 12308.12, base loss: 14245.44
[INFO 2017-06-28 12:49:32,260 main.py:51] epoch 291, training loss: 10522.35, average training loss: 12302.00, base loss: 14240.83
[INFO 2017-06-28 12:49:32,925 main.py:51] epoch 292, training loss: 10664.55, average training loss: 12296.42, base loss: 14240.70
[INFO 2017-06-28 12:49:33,605 main.py:51] epoch 293, training loss: 10054.87, average training loss: 12288.79, base loss: 14234.73
[INFO 2017-06-28 12:49:34,284 main.py:51] epoch 294, training loss: 12493.43, average training loss: 12289.49, base loss: 14241.11
[INFO 2017-06-28 12:49:34,932 main.py:51] epoch 295, training loss: 12401.20, average training loss: 12289.86, base loss: 14243.88
[INFO 2017-06-28 12:49:35,590 main.py:51] epoch 296, training loss: 11304.10, average training loss: 12286.54, base loss: 14245.87
[INFO 2017-06-28 12:49:36,254 main.py:51] epoch 297, training loss: 11448.56, average training loss: 12283.73, base loss: 14248.09
[INFO 2017-06-28 12:49:36,920 main.py:51] epoch 298, training loss: 12551.49, average training loss: 12284.63, base loss: 14253.52
[INFO 2017-06-28 12:49:37,570 main.py:51] epoch 299, training loss: 10907.35, average training loss: 12280.04, base loss: 14251.02
[INFO 2017-06-28 12:49:37,570 main.py:53] epoch 299, testing
[INFO 2017-06-28 12:49:40,121 main.py:105] average testing loss: 12096.60, base loss: 15051.27
[INFO 2017-06-28 12:49:40,121 main.py:106] improve_loss: 2954.67, improve_percent: 0.20
[INFO 2017-06-28 12:49:40,121 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:49:40,157 main.py:76] current best improved percent: 0.20
[INFO 2017-06-28 12:49:40,815 main.py:51] epoch 300, training loss: 10050.18, average training loss: 12272.63, base loss: 14248.66
[INFO 2017-06-28 12:49:41,464 main.py:51] epoch 301, training loss: 11664.33, average training loss: 12270.61, base loss: 14249.42
[INFO 2017-06-28 12:49:42,115 main.py:51] epoch 302, training loss: 10453.17, average training loss: 12264.62, base loss: 14244.49
[INFO 2017-06-28 12:49:42,763 main.py:51] epoch 303, training loss: 11021.43, average training loss: 12260.53, base loss: 14243.97
[INFO 2017-06-28 12:49:43,423 main.py:51] epoch 304, training loss: 13240.69, average training loss: 12263.74, base loss: 14250.50
[INFO 2017-06-28 12:49:44,103 main.py:51] epoch 305, training loss: 12275.36, average training loss: 12263.78, base loss: 14255.58
[INFO 2017-06-28 12:49:44,788 main.py:51] epoch 306, training loss: 10506.21, average training loss: 12258.05, base loss: 14252.06
[INFO 2017-06-28 12:49:45,439 main.py:51] epoch 307, training loss: 10411.11, average training loss: 12252.06, base loss: 14252.62
[INFO 2017-06-28 12:49:46,109 main.py:51] epoch 308, training loss: 9687.66, average training loss: 12243.76, base loss: 14248.47
[INFO 2017-06-28 12:49:46,787 main.py:51] epoch 309, training loss: 12180.60, average training loss: 12243.55, base loss: 14250.12
[INFO 2017-06-28 12:49:47,421 main.py:51] epoch 310, training loss: 11802.35, average training loss: 12242.14, base loss: 14255.87
[INFO 2017-06-28 12:49:48,079 main.py:51] epoch 311, training loss: 10747.08, average training loss: 12237.34, base loss: 14254.94
[INFO 2017-06-28 12:49:48,720 main.py:51] epoch 312, training loss: 12996.65, average training loss: 12239.77, base loss: 14261.99
[INFO 2017-06-28 12:49:49,397 main.py:51] epoch 313, training loss: 12056.66, average training loss: 12239.19, base loss: 14267.66
[INFO 2017-06-28 12:49:50,077 main.py:51] epoch 314, training loss: 13409.48, average training loss: 12242.90, base loss: 14279.84
[INFO 2017-06-28 12:49:50,721 main.py:51] epoch 315, training loss: 10381.31, average training loss: 12237.01, base loss: 14273.13
[INFO 2017-06-28 12:49:51,368 main.py:51] epoch 316, training loss: 10245.52, average training loss: 12230.73, base loss: 14270.34
[INFO 2017-06-28 12:49:52,030 main.py:51] epoch 317, training loss: 11105.49, average training loss: 12227.19, base loss: 14270.95
[INFO 2017-06-28 12:49:52,691 main.py:51] epoch 318, training loss: 10906.99, average training loss: 12223.05, base loss: 14269.92
[INFO 2017-06-28 12:49:53,343 main.py:51] epoch 319, training loss: 11205.62, average training loss: 12219.87, base loss: 14270.24
[INFO 2017-06-28 12:49:53,996 main.py:51] epoch 320, training loss: 11740.10, average training loss: 12218.38, base loss: 14272.72
[INFO 2017-06-28 12:49:54,649 main.py:51] epoch 321, training loss: 13882.57, average training loss: 12223.54, base loss: 14284.31
[INFO 2017-06-28 12:49:55,300 main.py:51] epoch 322, training loss: 12821.95, average training loss: 12225.40, base loss: 14290.51
[INFO 2017-06-28 12:49:55,955 main.py:51] epoch 323, training loss: 10202.88, average training loss: 12219.16, base loss: 14286.76
[INFO 2017-06-28 12:49:56,599 main.py:51] epoch 324, training loss: 11416.88, average training loss: 12216.69, base loss: 14288.03
[INFO 2017-06-28 12:49:57,247 main.py:51] epoch 325, training loss: 9852.44, average training loss: 12209.43, base loss: 14285.51
[INFO 2017-06-28 12:49:57,895 main.py:51] epoch 326, training loss: 9321.58, average training loss: 12200.60, base loss: 14279.72
[INFO 2017-06-28 12:49:58,555 main.py:51] epoch 327, training loss: 11614.10, average training loss: 12198.81, base loss: 14279.28
[INFO 2017-06-28 12:49:59,201 main.py:51] epoch 328, training loss: 10700.27, average training loss: 12194.26, base loss: 14278.99
[INFO 2017-06-28 12:49:59,843 main.py:51] epoch 329, training loss: 9176.73, average training loss: 12185.12, base loss: 14268.55
[INFO 2017-06-28 12:50:00,507 main.py:51] epoch 330, training loss: 10879.46, average training loss: 12181.17, base loss: 14268.27
[INFO 2017-06-28 12:50:01,145 main.py:51] epoch 331, training loss: 11024.14, average training loss: 12177.69, base loss: 14269.43
[INFO 2017-06-28 12:50:01,804 main.py:51] epoch 332, training loss: 12899.99, average training loss: 12179.86, base loss: 14276.13
[INFO 2017-06-28 12:50:02,455 main.py:51] epoch 333, training loss: 12025.11, average training loss: 12179.39, base loss: 14278.87
[INFO 2017-06-28 12:50:03,104 main.py:51] epoch 334, training loss: 11993.15, average training loss: 12178.84, base loss: 14283.77
[INFO 2017-06-28 12:50:03,759 main.py:51] epoch 335, training loss: 10279.96, average training loss: 12173.18, base loss: 14279.46
[INFO 2017-06-28 12:50:04,404 main.py:51] epoch 336, training loss: 11438.44, average training loss: 12171.00, base loss: 14279.41
[INFO 2017-06-28 12:50:05,053 main.py:51] epoch 337, training loss: 10270.79, average training loss: 12165.38, base loss: 14276.79
[INFO 2017-06-28 12:50:05,705 main.py:51] epoch 338, training loss: 10724.72, average training loss: 12161.13, base loss: 14276.36
[INFO 2017-06-28 12:50:06,355 main.py:51] epoch 339, training loss: 12150.59, average training loss: 12161.10, base loss: 14279.37
[INFO 2017-06-28 12:50:07,008 main.py:51] epoch 340, training loss: 10998.73, average training loss: 12157.69, base loss: 14278.16
[INFO 2017-06-28 12:50:07,663 main.py:51] epoch 341, training loss: 10590.58, average training loss: 12153.11, base loss: 14277.55
[INFO 2017-06-28 12:50:08,334 main.py:51] epoch 342, training loss: 10359.12, average training loss: 12147.88, base loss: 14275.81
[INFO 2017-06-28 12:50:08,980 main.py:51] epoch 343, training loss: 11205.45, average training loss: 12145.14, base loss: 14275.39
[INFO 2017-06-28 12:50:09,650 main.py:51] epoch 344, training loss: 11714.52, average training loss: 12143.89, base loss: 14278.19
[INFO 2017-06-28 12:50:10,322 main.py:51] epoch 345, training loss: 11632.21, average training loss: 12142.41, base loss: 14282.52
[INFO 2017-06-28 12:50:10,976 main.py:51] epoch 346, training loss: 11317.95, average training loss: 12140.04, base loss: 14283.17
[INFO 2017-06-28 12:50:11,653 main.py:51] epoch 347, training loss: 10192.12, average training loss: 12134.44, base loss: 14279.18
[INFO 2017-06-28 12:50:12,329 main.py:51] epoch 348, training loss: 11690.37, average training loss: 12133.17, base loss: 14281.32
[INFO 2017-06-28 12:50:12,989 main.py:51] epoch 349, training loss: 10208.97, average training loss: 12127.67, base loss: 14276.79
[INFO 2017-06-28 12:50:13,633 main.py:51] epoch 350, training loss: 11414.67, average training loss: 12125.64, base loss: 14276.75
[INFO 2017-06-28 12:50:14,298 main.py:51] epoch 351, training loss: 11540.96, average training loss: 12123.98, base loss: 14279.65
[INFO 2017-06-28 12:50:14,964 main.py:51] epoch 352, training loss: 9573.41, average training loss: 12116.75, base loss: 14273.05
[INFO 2017-06-28 12:50:15,606 main.py:51] epoch 353, training loss: 10855.35, average training loss: 12113.19, base loss: 14273.48
[INFO 2017-06-28 12:50:16,289 main.py:51] epoch 354, training loss: 10833.23, average training loss: 12109.58, base loss: 14274.52
[INFO 2017-06-28 12:50:16,940 main.py:51] epoch 355, training loss: 11017.23, average training loss: 12106.52, base loss: 14276.55
[INFO 2017-06-28 12:50:17,592 main.py:51] epoch 356, training loss: 11886.61, average training loss: 12105.90, base loss: 14280.98
[INFO 2017-06-28 12:50:18,253 main.py:51] epoch 357, training loss: 11104.95, average training loss: 12103.10, base loss: 14280.17
[INFO 2017-06-28 12:50:18,914 main.py:51] epoch 358, training loss: 9946.99, average training loss: 12097.10, base loss: 14276.62
[INFO 2017-06-28 12:50:19,554 main.py:51] epoch 359, training loss: 10708.67, average training loss: 12093.24, base loss: 14278.47
[INFO 2017-06-28 12:50:20,197 main.py:51] epoch 360, training loss: 11995.12, average training loss: 12092.97, base loss: 14278.60
[INFO 2017-06-28 12:50:20,846 main.py:51] epoch 361, training loss: 10773.28, average training loss: 12089.32, base loss: 14278.19
[INFO 2017-06-28 12:50:21,473 main.py:51] epoch 362, training loss: 11356.29, average training loss: 12087.30, base loss: 14281.49
[INFO 2017-06-28 12:50:22,131 main.py:51] epoch 363, training loss: 9730.85, average training loss: 12080.83, base loss: 14276.30
[INFO 2017-06-28 12:50:22,798 main.py:51] epoch 364, training loss: 13494.85, average training loss: 12084.70, base loss: 14285.32
[INFO 2017-06-28 12:50:23,426 main.py:51] epoch 365, training loss: 11521.40, average training loss: 12083.17, base loss: 14286.85
[INFO 2017-06-28 12:50:24,084 main.py:51] epoch 366, training loss: 11045.76, average training loss: 12080.34, base loss: 14288.07
[INFO 2017-06-28 12:50:24,728 main.py:51] epoch 367, training loss: 10271.17, average training loss: 12075.42, base loss: 14281.08
[INFO 2017-06-28 12:50:25,377 main.py:51] epoch 368, training loss: 10131.02, average training loss: 12070.15, base loss: 14280.15
[INFO 2017-06-28 12:50:26,026 main.py:51] epoch 369, training loss: 11174.27, average training loss: 12067.73, base loss: 14280.46
[INFO 2017-06-28 12:50:26,669 main.py:51] epoch 370, training loss: 11052.85, average training loss: 12065.00, base loss: 14281.80
[INFO 2017-06-28 12:50:27,352 main.py:51] epoch 371, training loss: 10763.20, average training loss: 12061.50, base loss: 14279.37
[INFO 2017-06-28 12:50:27,991 main.py:51] epoch 372, training loss: 11349.86, average training loss: 12059.59, base loss: 14280.33
[INFO 2017-06-28 12:50:28,655 main.py:51] epoch 373, training loss: 10748.81, average training loss: 12056.08, base loss: 14278.99
[INFO 2017-06-28 12:50:29,302 main.py:51] epoch 374, training loss: 10980.66, average training loss: 12053.22, base loss: 14278.89
[INFO 2017-06-28 12:50:29,965 main.py:51] epoch 375, training loss: 10371.47, average training loss: 12048.74, base loss: 14275.55
[INFO 2017-06-28 12:50:30,625 main.py:51] epoch 376, training loss: 10963.24, average training loss: 12045.86, base loss: 14275.23
[INFO 2017-06-28 12:50:31,291 main.py:51] epoch 377, training loss: 10845.01, average training loss: 12042.69, base loss: 14275.56
[INFO 2017-06-28 12:50:31,927 main.py:51] epoch 378, training loss: 9622.36, average training loss: 12036.30, base loss: 14272.66
[INFO 2017-06-28 12:50:32,572 main.py:51] epoch 379, training loss: 9776.72, average training loss: 12030.36, base loss: 14269.76
[INFO 2017-06-28 12:50:33,204 main.py:51] epoch 380, training loss: 11878.61, average training loss: 12029.96, base loss: 14276.27
[INFO 2017-06-28 12:50:33,866 main.py:51] epoch 381, training loss: 11851.95, average training loss: 12029.49, base loss: 14279.97
[INFO 2017-06-28 12:50:34,522 main.py:51] epoch 382, training loss: 11614.32, average training loss: 12028.41, base loss: 14283.18
[INFO 2017-06-28 12:50:35,176 main.py:51] epoch 383, training loss: 11628.33, average training loss: 12027.36, base loss: 14284.73
[INFO 2017-06-28 12:50:35,833 main.py:51] epoch 384, training loss: 11690.20, average training loss: 12026.49, base loss: 14289.14
[INFO 2017-06-28 12:50:36,497 main.py:51] epoch 385, training loss: 12473.79, average training loss: 12027.65, base loss: 14294.30
[INFO 2017-06-28 12:50:37,134 main.py:51] epoch 386, training loss: 10843.30, average training loss: 12024.59, base loss: 14292.97
[INFO 2017-06-28 12:50:37,804 main.py:51] epoch 387, training loss: 9793.65, average training loss: 12018.84, base loss: 14288.53
[INFO 2017-06-28 12:50:38,443 main.py:51] epoch 388, training loss: 10214.75, average training loss: 12014.20, base loss: 14287.12
[INFO 2017-06-28 12:50:39,073 main.py:51] epoch 389, training loss: 13186.40, average training loss: 12017.21, base loss: 14292.21
[INFO 2017-06-28 12:50:39,703 main.py:51] epoch 390, training loss: 11984.91, average training loss: 12017.12, base loss: 14294.20
[INFO 2017-06-28 12:50:40,354 main.py:51] epoch 391, training loss: 10484.14, average training loss: 12013.21, base loss: 14295.11
[INFO 2017-06-28 12:50:41,010 main.py:51] epoch 392, training loss: 10886.13, average training loss: 12010.34, base loss: 14295.27
[INFO 2017-06-28 12:50:41,662 main.py:51] epoch 393, training loss: 10580.69, average training loss: 12006.72, base loss: 14295.90
[INFO 2017-06-28 12:50:42,324 main.py:51] epoch 394, training loss: 10760.28, average training loss: 12003.56, base loss: 14296.58
[INFO 2017-06-28 12:50:42,974 main.py:51] epoch 395, training loss: 11235.06, average training loss: 12001.62, base loss: 14296.55
[INFO 2017-06-28 12:50:43,596 main.py:51] epoch 396, training loss: 9813.57, average training loss: 11996.11, base loss: 14292.66
[INFO 2017-06-28 12:50:44,240 main.py:51] epoch 397, training loss: 11522.93, average training loss: 11994.92, base loss: 14294.47
[INFO 2017-06-28 12:50:44,890 main.py:51] epoch 398, training loss: 12282.18, average training loss: 11995.64, base loss: 14301.38
[INFO 2017-06-28 12:50:45,537 main.py:51] epoch 399, training loss: 10486.33, average training loss: 11991.87, base loss: 14299.52
[INFO 2017-06-28 12:50:45,537 main.py:53] epoch 399, testing
[INFO 2017-06-28 12:50:48,132 main.py:105] average testing loss: 11700.61, base loss: 14910.29
[INFO 2017-06-28 12:50:48,132 main.py:106] improve_loss: 3209.68, improve_percent: 0.22
[INFO 2017-06-28 12:50:48,132 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:50:48,168 main.py:76] current best improved percent: 0.22
[INFO 2017-06-28 12:50:48,797 main.py:51] epoch 400, training loss: 10823.17, average training loss: 11988.95, base loss: 14298.28
[INFO 2017-06-28 12:50:49,454 main.py:51] epoch 401, training loss: 12499.64, average training loss: 11990.22, base loss: 14303.05
[INFO 2017-06-28 12:50:50,086 main.py:51] epoch 402, training loss: 10250.71, average training loss: 11985.91, base loss: 14300.85
[INFO 2017-06-28 12:50:50,730 main.py:51] epoch 403, training loss: 9154.49, average training loss: 11978.90, base loss: 14293.42
[INFO 2017-06-28 12:50:51,402 main.py:51] epoch 404, training loss: 10951.39, average training loss: 11976.36, base loss: 14294.46
[INFO 2017-06-28 12:50:52,042 main.py:51] epoch 405, training loss: 10694.97, average training loss: 11973.20, base loss: 14292.89
[INFO 2017-06-28 12:50:52,715 main.py:51] epoch 406, training loss: 9418.74, average training loss: 11966.93, base loss: 14288.60
[INFO 2017-06-28 12:50:53,375 main.py:51] epoch 407, training loss: 10752.51, average training loss: 11963.95, base loss: 14288.48
[INFO 2017-06-28 12:50:54,034 main.py:51] epoch 408, training loss: 9737.42, average training loss: 11958.51, base loss: 14284.64
[INFO 2017-06-28 12:50:54,707 main.py:51] epoch 409, training loss: 10617.80, average training loss: 11955.24, base loss: 14283.87
[INFO 2017-06-28 12:50:55,376 main.py:51] epoch 410, training loss: 8744.63, average training loss: 11947.43, base loss: 14279.14
[INFO 2017-06-28 12:50:56,054 main.py:51] epoch 411, training loss: 10482.17, average training loss: 11943.87, base loss: 14279.27
[INFO 2017-06-28 12:50:56,713 main.py:51] epoch 412, training loss: 12769.89, average training loss: 11945.87, base loss: 14283.28
[INFO 2017-06-28 12:50:57,372 main.py:51] epoch 413, training loss: 8953.38, average training loss: 11938.64, base loss: 14277.25
[INFO 2017-06-28 12:50:58,036 main.py:51] epoch 414, training loss: 11838.04, average training loss: 11938.40, base loss: 14281.35
[INFO 2017-06-28 12:50:58,690 main.py:51] epoch 415, training loss: 10461.27, average training loss: 11934.85, base loss: 14281.23
[INFO 2017-06-28 12:50:59,353 main.py:51] epoch 416, training loss: 10002.83, average training loss: 11930.21, base loss: 14278.56
[INFO 2017-06-28 12:51:00,015 main.py:51] epoch 417, training loss: 12964.47, average training loss: 11932.69, base loss: 14285.52
[INFO 2017-06-28 12:51:00,686 main.py:51] epoch 418, training loss: 10989.06, average training loss: 11930.44, base loss: 14287.87
[INFO 2017-06-28 12:51:01,341 main.py:51] epoch 419, training loss: 10431.63, average training loss: 11926.87, base loss: 14285.00
[INFO 2017-06-28 12:51:02,000 main.py:51] epoch 420, training loss: 10312.78, average training loss: 11923.03, base loss: 14281.98
[INFO 2017-06-28 12:51:02,647 main.py:51] epoch 421, training loss: 10375.78, average training loss: 11919.37, base loss: 14278.40
[INFO 2017-06-28 12:51:03,314 main.py:51] epoch 422, training loss: 11616.28, average training loss: 11918.65, base loss: 14282.41
[INFO 2017-06-28 12:51:03,979 main.py:51] epoch 423, training loss: 12660.20, average training loss: 11920.40, base loss: 14287.25
[INFO 2017-06-28 12:51:04,641 main.py:51] epoch 424, training loss: 9341.14, average training loss: 11914.33, base loss: 14281.84
[INFO 2017-06-28 12:51:05,291 main.py:51] epoch 425, training loss: 10728.59, average training loss: 11911.55, base loss: 14279.40
[INFO 2017-06-28 12:51:05,948 main.py:51] epoch 426, training loss: 11303.37, average training loss: 11910.12, base loss: 14281.18
[INFO 2017-06-28 12:51:06,601 main.py:51] epoch 427, training loss: 9532.93, average training loss: 11904.57, base loss: 14279.55
[INFO 2017-06-28 12:51:07,274 main.py:51] epoch 428, training loss: 11928.29, average training loss: 11904.62, base loss: 14281.97
[INFO 2017-06-28 12:51:07,951 main.py:51] epoch 429, training loss: 10574.88, average training loss: 11901.53, base loss: 14284.07
[INFO 2017-06-28 12:51:08,604 main.py:51] epoch 430, training loss: 11191.72, average training loss: 11899.89, base loss: 14284.55
[INFO 2017-06-28 12:51:09,234 main.py:51] epoch 431, training loss: 9891.32, average training loss: 11895.24, base loss: 14281.13
[INFO 2017-06-28 12:51:09,867 main.py:51] epoch 432, training loss: 11750.33, average training loss: 11894.90, base loss: 14284.00
[INFO 2017-06-28 12:51:10,519 main.py:51] epoch 433, training loss: 11031.55, average training loss: 11892.91, base loss: 14284.26
[INFO 2017-06-28 12:51:11,196 main.py:51] epoch 434, training loss: 12071.96, average training loss: 11893.32, base loss: 14291.27
[INFO 2017-06-28 12:51:11,856 main.py:51] epoch 435, training loss: 11666.13, average training loss: 11892.80, base loss: 14291.70
[INFO 2017-06-28 12:51:12,518 main.py:51] epoch 436, training loss: 9434.86, average training loss: 11887.18, base loss: 14288.84
[INFO 2017-06-28 12:51:13,179 main.py:51] epoch 437, training loss: 11667.58, average training loss: 11886.68, base loss: 14293.48
[INFO 2017-06-28 12:51:13,810 main.py:51] epoch 438, training loss: 9581.54, average training loss: 11881.43, base loss: 14289.78
[INFO 2017-06-28 12:51:14,462 main.py:51] epoch 439, training loss: 11163.82, average training loss: 11879.79, base loss: 14291.77
[INFO 2017-06-28 12:51:15,119 main.py:51] epoch 440, training loss: 11387.53, average training loss: 11878.68, base loss: 14294.98
[INFO 2017-06-28 12:51:15,778 main.py:51] epoch 441, training loss: 12221.05, average training loss: 11879.45, base loss: 14299.61
[INFO 2017-06-28 12:51:16,426 main.py:51] epoch 442, training loss: 12534.25, average training loss: 11880.93, base loss: 14307.20
[INFO 2017-06-28 12:51:17,089 main.py:51] epoch 443, training loss: 10914.39, average training loss: 11878.75, base loss: 14307.79
[INFO 2017-06-28 12:51:17,765 main.py:51] epoch 444, training loss: 11703.67, average training loss: 11878.36, base loss: 14312.36
[INFO 2017-06-28 12:51:18,415 main.py:51] epoch 445, training loss: 12352.07, average training loss: 11879.42, base loss: 14314.32
[INFO 2017-06-28 12:51:19,101 main.py:51] epoch 446, training loss: 10195.58, average training loss: 11875.66, base loss: 14313.13
[INFO 2017-06-28 12:51:19,775 main.py:51] epoch 447, training loss: 12472.08, average training loss: 11876.99, base loss: 14318.76
[INFO 2017-06-28 12:51:20,428 main.py:51] epoch 448, training loss: 10860.97, average training loss: 11874.72, base loss: 14319.17
[INFO 2017-06-28 12:51:21,081 main.py:51] epoch 449, training loss: 10671.80, average training loss: 11872.05, base loss: 14320.20
[INFO 2017-06-28 12:51:21,725 main.py:51] epoch 450, training loss: 11260.24, average training loss: 11870.69, base loss: 14320.52
[INFO 2017-06-28 12:51:22,379 main.py:51] epoch 451, training loss: 10187.09, average training loss: 11866.97, base loss: 14319.16
[INFO 2017-06-28 12:51:23,035 main.py:51] epoch 452, training loss: 9357.59, average training loss: 11861.43, base loss: 14314.37
[INFO 2017-06-28 12:51:23,679 main.py:51] epoch 453, training loss: 10798.48, average training loss: 11859.09, base loss: 14313.73
[INFO 2017-06-28 12:51:24,345 main.py:51] epoch 454, training loss: 9862.84, average training loss: 11854.70, base loss: 14308.97
[INFO 2017-06-28 12:51:25,012 main.py:51] epoch 455, training loss: 10612.36, average training loss: 11851.98, base loss: 14307.93
[INFO 2017-06-28 12:51:25,664 main.py:51] epoch 456, training loss: 9724.93, average training loss: 11847.32, base loss: 14305.47
[INFO 2017-06-28 12:51:26,330 main.py:51] epoch 457, training loss: 9973.57, average training loss: 11843.23, base loss: 14305.04
[INFO 2017-06-28 12:51:26,991 main.py:51] epoch 458, training loss: 11901.93, average training loss: 11843.36, base loss: 14309.11
[INFO 2017-06-28 12:51:27,638 main.py:51] epoch 459, training loss: 11536.55, average training loss: 11842.69, base loss: 14310.77
[INFO 2017-06-28 12:51:28,315 main.py:51] epoch 460, training loss: 11432.60, average training loss: 11841.80, base loss: 14311.40
[INFO 2017-06-28 12:51:28,980 main.py:51] epoch 461, training loss: 10365.46, average training loss: 11838.61, base loss: 14307.87
[INFO 2017-06-28 12:51:29,660 main.py:51] epoch 462, training loss: 10278.94, average training loss: 11835.24, base loss: 14308.27
[INFO 2017-06-28 12:51:30,321 main.py:51] epoch 463, training loss: 9350.99, average training loss: 11829.89, base loss: 14303.38
[INFO 2017-06-28 12:51:31,017 main.py:51] epoch 464, training loss: 11616.22, average training loss: 11829.43, base loss: 14304.78
[INFO 2017-06-28 12:51:31,657 main.py:51] epoch 465, training loss: 12268.26, average training loss: 11830.37, base loss: 14309.81
[INFO 2017-06-28 12:51:32,298 main.py:51] epoch 466, training loss: 9739.18, average training loss: 11825.89, base loss: 14305.65
[INFO 2017-06-28 12:51:32,942 main.py:51] epoch 467, training loss: 9900.18, average training loss: 11821.77, base loss: 14300.99
[INFO 2017-06-28 12:51:33,601 main.py:51] epoch 468, training loss: 11961.51, average training loss: 11822.07, base loss: 14305.42
[INFO 2017-06-28 12:51:34,244 main.py:51] epoch 469, training loss: 9962.26, average training loss: 11818.12, base loss: 14303.19
[INFO 2017-06-28 12:51:34,896 main.py:51] epoch 470, training loss: 9995.37, average training loss: 11814.25, base loss: 14302.55
[INFO 2017-06-28 12:51:35,556 main.py:51] epoch 471, training loss: 10416.33, average training loss: 11811.28, base loss: 14301.38
[INFO 2017-06-28 12:51:36,208 main.py:51] epoch 472, training loss: 10213.92, average training loss: 11807.91, base loss: 14300.56
[INFO 2017-06-28 12:51:36,869 main.py:51] epoch 473, training loss: 10710.96, average training loss: 11805.59, base loss: 14303.47
[INFO 2017-06-28 12:51:37,537 main.py:51] epoch 474, training loss: 10345.75, average training loss: 11802.52, base loss: 14302.44
[INFO 2017-06-28 12:51:38,208 main.py:51] epoch 475, training loss: 10368.27, average training loss: 11799.51, base loss: 14303.09
[INFO 2017-06-28 12:51:38,862 main.py:51] epoch 476, training loss: 10744.54, average training loss: 11797.29, base loss: 14304.41
[INFO 2017-06-28 12:51:39,530 main.py:51] epoch 477, training loss: 8650.91, average training loss: 11790.71, base loss: 14296.62
[INFO 2017-06-28 12:51:40,193 main.py:51] epoch 478, training loss: 11059.65, average training loss: 11789.19, base loss: 14297.88
[INFO 2017-06-28 12:51:40,845 main.py:51] epoch 479, training loss: 9501.89, average training loss: 11784.42, base loss: 14293.90
[INFO 2017-06-28 12:51:41,526 main.py:51] epoch 480, training loss: 10017.81, average training loss: 11780.75, base loss: 14291.35
[INFO 2017-06-28 12:51:42,210 main.py:51] epoch 481, training loss: 12128.20, average training loss: 11781.47, base loss: 14294.38
[INFO 2017-06-28 12:51:42,882 main.py:51] epoch 482, training loss: 11859.66, average training loss: 11781.63, base loss: 14295.46
[INFO 2017-06-28 12:51:43,543 main.py:51] epoch 483, training loss: 12390.96, average training loss: 11782.89, base loss: 14301.08
[INFO 2017-06-28 12:51:44,222 main.py:51] epoch 484, training loss: 10604.49, average training loss: 11780.46, base loss: 14301.90
[INFO 2017-06-28 12:51:44,877 main.py:51] epoch 485, training loss: 10251.14, average training loss: 11777.31, base loss: 14300.71
[INFO 2017-06-28 12:51:45,522 main.py:51] epoch 486, training loss: 12083.53, average training loss: 11777.94, base loss: 14304.89
[INFO 2017-06-28 12:51:46,170 main.py:51] epoch 487, training loss: 10844.24, average training loss: 11776.03, base loss: 14304.78
[INFO 2017-06-28 12:51:46,841 main.py:51] epoch 488, training loss: 11302.07, average training loss: 11775.06, base loss: 14305.42
[INFO 2017-06-28 12:51:47,506 main.py:51] epoch 489, training loss: 10364.87, average training loss: 11772.18, base loss: 14303.56
[INFO 2017-06-28 12:51:48,153 main.py:51] epoch 490, training loss: 10267.69, average training loss: 11769.12, base loss: 14302.17
[INFO 2017-06-28 12:51:48,815 main.py:51] epoch 491, training loss: 11315.34, average training loss: 11768.19, base loss: 14303.52
[INFO 2017-06-28 12:51:49,464 main.py:51] epoch 492, training loss: 10079.90, average training loss: 11764.77, base loss: 14300.64
[INFO 2017-06-28 12:51:50,125 main.py:51] epoch 493, training loss: 10491.54, average training loss: 11762.19, base loss: 14299.48
[INFO 2017-06-28 12:51:50,786 main.py:51] epoch 494, training loss: 11542.78, average training loss: 11761.75, base loss: 14302.79
[INFO 2017-06-28 12:51:51,462 main.py:51] epoch 495, training loss: 9617.79, average training loss: 11757.43, base loss: 14298.88
[INFO 2017-06-28 12:51:52,103 main.py:51] epoch 496, training loss: 9649.25, average training loss: 11753.19, base loss: 14295.96
[INFO 2017-06-28 12:51:52,775 main.py:51] epoch 497, training loss: 10986.09, average training loss: 11751.65, base loss: 14297.50
[INFO 2017-06-28 12:51:53,454 main.py:51] epoch 498, training loss: 11663.20, average training loss: 11751.47, base loss: 14298.36
[INFO 2017-06-28 12:51:54,122 main.py:51] epoch 499, training loss: 10819.87, average training loss: 11749.60, base loss: 14298.97
[INFO 2017-06-28 12:51:54,122 main.py:53] epoch 499, testing
[INFO 2017-06-28 12:51:56,723 main.py:105] average testing loss: 11956.73, base loss: 15090.17
[INFO 2017-06-28 12:51:56,723 main.py:106] improve_loss: 3133.45, improve_percent: 0.21
[INFO 2017-06-28 12:51:56,723 main.py:76] current best improved percent: 0.22
[INFO 2017-06-28 12:51:57,405 main.py:51] epoch 500, training loss: 12033.88, average training loss: 11750.17, base loss: 14302.51
[INFO 2017-06-28 12:51:58,094 main.py:51] epoch 501, training loss: 10580.24, average training loss: 11747.84, base loss: 14302.21
[INFO 2017-06-28 12:51:58,747 main.py:51] epoch 502, training loss: 10748.14, average training loss: 11745.85, base loss: 14302.42
[INFO 2017-06-28 12:51:59,397 main.py:51] epoch 503, training loss: 10157.92, average training loss: 11742.70, base loss: 14301.35
[INFO 2017-06-28 12:52:00,073 main.py:51] epoch 504, training loss: 11936.50, average training loss: 11743.09, base loss: 14304.05
[INFO 2017-06-28 12:52:00,741 main.py:51] epoch 505, training loss: 9827.99, average training loss: 11739.30, base loss: 14301.01
[INFO 2017-06-28 12:52:01,391 main.py:51] epoch 506, training loss: 11124.80, average training loss: 11738.09, base loss: 14302.89
[INFO 2017-06-28 12:52:02,049 main.py:51] epoch 507, training loss: 10609.49, average training loss: 11735.87, base loss: 14300.19
[INFO 2017-06-28 12:52:02,705 main.py:51] epoch 508, training loss: 11368.83, average training loss: 11735.15, base loss: 14301.89
[INFO 2017-06-28 12:52:03,358 main.py:51] epoch 509, training loss: 10331.48, average training loss: 11732.40, base loss: 14301.13
[INFO 2017-06-28 12:52:04,009 main.py:51] epoch 510, training loss: 10266.81, average training loss: 11729.53, base loss: 14301.64
[INFO 2017-06-28 12:52:04,672 main.py:51] epoch 511, training loss: 8349.35, average training loss: 11722.93, base loss: 14295.04
[INFO 2017-06-28 12:52:05,334 main.py:51] epoch 512, training loss: 12066.15, average training loss: 11723.59, base loss: 14298.77
[INFO 2017-06-28 12:52:06,014 main.py:51] epoch 513, training loss: 11457.23, average training loss: 11723.08, base loss: 14300.20
[INFO 2017-06-28 12:52:06,669 main.py:51] epoch 514, training loss: 10469.91, average training loss: 11720.64, base loss: 14299.56
[INFO 2017-06-28 12:52:07,310 main.py:51] epoch 515, training loss: 10142.44, average training loss: 11717.58, base loss: 14298.37
[INFO 2017-06-28 12:52:07,998 main.py:51] epoch 516, training loss: 13281.03, average training loss: 11720.61, base loss: 14303.90
[INFO 2017-06-28 12:52:08,683 main.py:51] epoch 517, training loss: 11459.65, average training loss: 11720.10, base loss: 14305.72
[INFO 2017-06-28 12:52:09,341 main.py:51] epoch 518, training loss: 11776.16, average training loss: 11720.21, base loss: 14310.98
[INFO 2017-06-28 12:52:10,012 main.py:51] epoch 519, training loss: 9783.38, average training loss: 11716.49, base loss: 14307.63
[INFO 2017-06-28 12:52:10,662 main.py:51] epoch 520, training loss: 11281.95, average training loss: 11715.65, base loss: 14307.88
[INFO 2017-06-28 12:52:11,310 main.py:51] epoch 521, training loss: 12351.73, average training loss: 11716.87, base loss: 14310.34
[INFO 2017-06-28 12:52:11,973 main.py:51] epoch 522, training loss: 10902.52, average training loss: 11715.32, base loss: 14312.55
[INFO 2017-06-28 12:52:12,633 main.py:51] epoch 523, training loss: 10983.47, average training loss: 11713.92, base loss: 14312.24
[INFO 2017-06-28 12:52:13,306 main.py:51] epoch 524, training loss: 9971.49, average training loss: 11710.60, base loss: 14309.33
[INFO 2017-06-28 12:52:13,950 main.py:51] epoch 525, training loss: 9540.01, average training loss: 11706.47, base loss: 14306.66
[INFO 2017-06-28 12:52:14,599 main.py:51] epoch 526, training loss: 9662.96, average training loss: 11702.60, base loss: 14306.20
[INFO 2017-06-28 12:52:15,262 main.py:51] epoch 527, training loss: 12266.90, average training loss: 11703.66, base loss: 14309.27
[INFO 2017-06-28 12:52:15,946 main.py:51] epoch 528, training loss: 8748.97, average training loss: 11698.08, base loss: 14304.64
[INFO 2017-06-28 12:52:16,598 main.py:51] epoch 529, training loss: 9904.48, average training loss: 11694.69, base loss: 14302.52
[INFO 2017-06-28 12:52:17,239 main.py:51] epoch 530, training loss: 12076.47, average training loss: 11695.41, base loss: 14306.67
[INFO 2017-06-28 12:52:17,908 main.py:51] epoch 531, training loss: 10731.52, average training loss: 11693.60, base loss: 14306.08
[INFO 2017-06-28 12:52:18,562 main.py:51] epoch 532, training loss: 9795.24, average training loss: 11690.04, base loss: 14304.31
[INFO 2017-06-28 12:52:19,254 main.py:51] epoch 533, training loss: 10322.71, average training loss: 11687.48, base loss: 14303.95
[INFO 2017-06-28 12:52:19,905 main.py:51] epoch 534, training loss: 11206.62, average training loss: 11686.58, base loss: 14306.89
[INFO 2017-06-28 12:52:20,572 main.py:51] epoch 535, training loss: 10442.10, average training loss: 11684.26, base loss: 14305.76
[INFO 2017-06-28 12:52:21,262 main.py:51] epoch 536, training loss: 11401.06, average training loss: 11683.73, base loss: 14307.03
[INFO 2017-06-28 12:52:21,909 main.py:51] epoch 537, training loss: 11887.95, average training loss: 11684.11, base loss: 14308.65
[INFO 2017-06-28 12:52:22,578 main.py:51] epoch 538, training loss: 11124.72, average training loss: 11683.07, base loss: 14310.98
[INFO 2017-06-28 12:52:23,235 main.py:51] epoch 539, training loss: 9730.24, average training loss: 11679.46, base loss: 14309.91
[INFO 2017-06-28 12:52:23,904 main.py:51] epoch 540, training loss: 10183.99, average training loss: 11676.69, base loss: 14309.46
[INFO 2017-06-28 12:52:24,582 main.py:51] epoch 541, training loss: 9581.38, average training loss: 11672.83, base loss: 14306.80
[INFO 2017-06-28 12:52:25,229 main.py:51] epoch 542, training loss: 9924.88, average training loss: 11669.61, base loss: 14306.80
[INFO 2017-06-28 12:52:25,878 main.py:51] epoch 543, training loss: 10526.65, average training loss: 11667.51, base loss: 14305.34
[INFO 2017-06-28 12:52:26,534 main.py:51] epoch 544, training loss: 10347.31, average training loss: 11665.08, base loss: 14305.09
[INFO 2017-06-28 12:52:27,185 main.py:51] epoch 545, training loss: 10462.19, average training loss: 11662.88, base loss: 14306.04
[INFO 2017-06-28 12:52:27,821 main.py:51] epoch 546, training loss: 10882.80, average training loss: 11661.46, base loss: 14305.79
[INFO 2017-06-28 12:52:28,486 main.py:51] epoch 547, training loss: 9289.41, average training loss: 11657.13, base loss: 14304.29
[INFO 2017-06-28 12:52:29,135 main.py:51] epoch 548, training loss: 10391.33, average training loss: 11654.82, base loss: 14304.65
[INFO 2017-06-28 12:52:29,773 main.py:51] epoch 549, training loss: 10624.30, average training loss: 11652.95, base loss: 14304.70
[INFO 2017-06-28 12:52:30,434 main.py:51] epoch 550, training loss: 11213.18, average training loss: 11652.15, base loss: 14308.50
[INFO 2017-06-28 12:52:31,102 main.py:51] epoch 551, training loss: 11435.53, average training loss: 11651.76, base loss: 14310.82
[INFO 2017-06-28 12:52:31,758 main.py:51] epoch 552, training loss: 11425.60, average training loss: 11651.35, base loss: 14311.11
[INFO 2017-06-28 12:52:32,420 main.py:51] epoch 553, training loss: 10075.16, average training loss: 11648.50, base loss: 14310.68
[INFO 2017-06-28 12:52:33,088 main.py:51] epoch 554, training loss: 10459.96, average training loss: 11646.36, base loss: 14309.99
[INFO 2017-06-28 12:52:33,730 main.py:51] epoch 555, training loss: 10988.17, average training loss: 11645.18, base loss: 14310.52
[INFO 2017-06-28 12:52:34,403 main.py:51] epoch 556, training loss: 10618.03, average training loss: 11643.33, base loss: 14311.60
[INFO 2017-06-28 12:52:35,071 main.py:51] epoch 557, training loss: 9953.51, average training loss: 11640.30, base loss: 14312.08
[INFO 2017-06-28 12:52:35,732 main.py:51] epoch 558, training loss: 10207.65, average training loss: 11637.74, base loss: 14311.87
[INFO 2017-06-28 12:52:36,407 main.py:51] epoch 559, training loss: 10054.09, average training loss: 11634.91, base loss: 14311.63
[INFO 2017-06-28 12:52:37,042 main.py:51] epoch 560, training loss: 10580.60, average training loss: 11633.03, base loss: 14310.12
[INFO 2017-06-28 12:52:37,694 main.py:51] epoch 561, training loss: 10970.98, average training loss: 11631.86, base loss: 14310.88
[INFO 2017-06-28 12:52:38,366 main.py:51] epoch 562, training loss: 9515.26, average training loss: 11628.10, base loss: 14309.98
[INFO 2017-06-28 12:52:39,039 main.py:51] epoch 563, training loss: 10211.32, average training loss: 11625.59, base loss: 14309.44
[INFO 2017-06-28 12:52:39,687 main.py:51] epoch 564, training loss: 11013.65, average training loss: 11624.50, base loss: 14310.87
[INFO 2017-06-28 12:52:40,342 main.py:51] epoch 565, training loss: 10213.97, average training loss: 11622.01, base loss: 14308.36
[INFO 2017-06-28 12:52:40,995 main.py:51] epoch 566, training loss: 10815.97, average training loss: 11620.59, base loss: 14309.04
[INFO 2017-06-28 12:52:41,630 main.py:51] epoch 567, training loss: 10620.84, average training loss: 11618.83, base loss: 14308.18
[INFO 2017-06-28 12:52:42,294 main.py:51] epoch 568, training loss: 12708.16, average training loss: 11620.74, base loss: 14313.08
[INFO 2017-06-28 12:52:42,955 main.py:51] epoch 569, training loss: 9926.12, average training loss: 11617.77, base loss: 14311.53
[INFO 2017-06-28 12:52:43,627 main.py:51] epoch 570, training loss: 11333.78, average training loss: 11617.27, base loss: 14314.51
[INFO 2017-06-28 12:52:44,291 main.py:51] epoch 571, training loss: 11028.45, average training loss: 11616.24, base loss: 14315.97
[INFO 2017-06-28 12:52:44,946 main.py:51] epoch 572, training loss: 10566.04, average training loss: 11614.41, base loss: 14315.39
[INFO 2017-06-28 12:52:45,623 main.py:51] epoch 573, training loss: 9751.84, average training loss: 11611.17, base loss: 14312.01
[INFO 2017-06-28 12:52:46,290 main.py:51] epoch 574, training loss: 11720.69, average training loss: 11611.36, base loss: 14316.77
[INFO 2017-06-28 12:52:46,951 main.py:51] epoch 575, training loss: 10558.05, average training loss: 11609.53, base loss: 14317.93
[INFO 2017-06-28 12:52:47,611 main.py:51] epoch 576, training loss: 10801.31, average training loss: 11608.13, base loss: 14319.08
[INFO 2017-06-28 12:52:48,260 main.py:51] epoch 577, training loss: 9727.14, average training loss: 11604.87, base loss: 14316.05
[INFO 2017-06-28 12:52:48,922 main.py:51] epoch 578, training loss: 10034.84, average training loss: 11602.16, base loss: 14316.51
[INFO 2017-06-28 12:52:49,564 main.py:51] epoch 579, training loss: 11020.66, average training loss: 11601.16, base loss: 14319.21
[INFO 2017-06-28 12:52:50,215 main.py:51] epoch 580, training loss: 10895.68, average training loss: 11599.94, base loss: 14319.74
[INFO 2017-06-28 12:52:50,872 main.py:51] epoch 581, training loss: 10568.07, average training loss: 11598.17, base loss: 14319.64
[INFO 2017-06-28 12:52:51,530 main.py:51] epoch 582, training loss: 10957.23, average training loss: 11597.07, base loss: 14321.35
[INFO 2017-06-28 12:52:52,218 main.py:51] epoch 583, training loss: 11274.76, average training loss: 11596.52, base loss: 14323.39
[INFO 2017-06-28 12:52:52,874 main.py:51] epoch 584, training loss: 10692.04, average training loss: 11594.97, base loss: 14323.38
[INFO 2017-06-28 12:52:53,525 main.py:51] epoch 585, training loss: 11062.70, average training loss: 11594.06, base loss: 14326.80
[INFO 2017-06-28 12:52:54,165 main.py:51] epoch 586, training loss: 9507.47, average training loss: 11590.51, base loss: 14324.79
[INFO 2017-06-28 12:52:54,836 main.py:51] epoch 587, training loss: 9559.18, average training loss: 11587.06, base loss: 14322.02
[INFO 2017-06-28 12:52:55,491 main.py:51] epoch 588, training loss: 10139.33, average training loss: 11584.60, base loss: 14321.69
[INFO 2017-06-28 12:52:56,164 main.py:51] epoch 589, training loss: 9815.38, average training loss: 11581.60, base loss: 14320.23
[INFO 2017-06-28 12:52:56,822 main.py:51] epoch 590, training loss: 10221.44, average training loss: 11579.30, base loss: 14319.33
[INFO 2017-06-28 12:52:57,473 main.py:51] epoch 591, training loss: 11035.92, average training loss: 11578.38, base loss: 14320.95
[INFO 2017-06-28 12:52:58,143 main.py:51] epoch 592, training loss: 12167.98, average training loss: 11579.37, base loss: 14324.96
[INFO 2017-06-28 12:52:58,797 main.py:51] epoch 593, training loss: 10688.17, average training loss: 11577.87, base loss: 14325.28
[INFO 2017-06-28 12:52:59,456 main.py:51] epoch 594, training loss: 10503.26, average training loss: 11576.07, base loss: 14323.67
[INFO 2017-06-28 12:53:00,129 main.py:51] epoch 595, training loss: 10693.85, average training loss: 11574.59, base loss: 14323.48
[INFO 2017-06-28 12:53:00,785 main.py:51] epoch 596, training loss: 10497.63, average training loss: 11572.78, base loss: 14321.38
[INFO 2017-06-28 12:53:01,443 main.py:51] epoch 597, training loss: 9793.42, average training loss: 11569.81, base loss: 14317.20
[INFO 2017-06-28 12:53:02,105 main.py:51] epoch 598, training loss: 10486.49, average training loss: 11568.00, base loss: 14316.52
[INFO 2017-06-28 12:53:02,767 main.py:51] epoch 599, training loss: 9594.25, average training loss: 11564.71, base loss: 14314.41
[INFO 2017-06-28 12:53:02,767 main.py:53] epoch 599, testing
[INFO 2017-06-28 12:53:05,340 main.py:105] average testing loss: 11530.90, base loss: 14940.33
[INFO 2017-06-28 12:53:05,340 main.py:106] improve_loss: 3409.44, improve_percent: 0.23
[INFO 2017-06-28 12:53:05,340 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:53:05,376 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 12:53:06,032 main.py:51] epoch 600, training loss: 9594.07, average training loss: 11561.43, base loss: 14312.40
[INFO 2017-06-28 12:53:06,693 main.py:51] epoch 601, training loss: 10554.18, average training loss: 11559.76, base loss: 14312.38
[INFO 2017-06-28 12:53:07,359 main.py:51] epoch 602, training loss: 8523.50, average training loss: 11554.72, base loss: 14306.58
[INFO 2017-06-28 12:53:07,993 main.py:51] epoch 603, training loss: 9979.94, average training loss: 11552.11, base loss: 14304.53
[INFO 2017-06-28 12:53:08,626 main.py:51] epoch 604, training loss: 11726.21, average training loss: 11552.40, base loss: 14305.88
[INFO 2017-06-28 12:53:09,290 main.py:51] epoch 605, training loss: 11083.84, average training loss: 11551.63, base loss: 14306.77
[INFO 2017-06-28 12:53:09,952 main.py:51] epoch 606, training loss: 10547.42, average training loss: 11549.98, base loss: 14307.83
[INFO 2017-06-28 12:53:10,606 main.py:51] epoch 607, training loss: 10272.15, average training loss: 11547.87, base loss: 14307.03
[INFO 2017-06-28 12:53:11,285 main.py:51] epoch 608, training loss: 11022.80, average training loss: 11547.01, base loss: 14307.59
[INFO 2017-06-28 12:53:11,940 main.py:51] epoch 609, training loss: 10214.94, average training loss: 11544.83, base loss: 14307.24
[INFO 2017-06-28 12:53:12,608 main.py:51] epoch 610, training loss: 11302.32, average training loss: 11544.43, base loss: 14309.80
[INFO 2017-06-28 12:53:13,288 main.py:51] epoch 611, training loss: 10742.52, average training loss: 11543.12, base loss: 14310.39
[INFO 2017-06-28 12:53:13,939 main.py:51] epoch 612, training loss: 9840.54, average training loss: 11540.34, base loss: 14307.39
[INFO 2017-06-28 12:53:14,592 main.py:51] epoch 613, training loss: 9404.55, average training loss: 11536.86, base loss: 14303.67
[INFO 2017-06-28 12:53:15,240 main.py:51] epoch 614, training loss: 8929.96, average training loss: 11532.63, base loss: 14299.08
[INFO 2017-06-28 12:53:15,905 main.py:51] epoch 615, training loss: 10782.21, average training loss: 11531.41, base loss: 14299.66
[INFO 2017-06-28 12:53:16,576 main.py:51] epoch 616, training loss: 10908.88, average training loss: 11530.40, base loss: 14301.05
[INFO 2017-06-28 12:53:17,245 main.py:51] epoch 617, training loss: 10061.50, average training loss: 11528.02, base loss: 14299.07
[INFO 2017-06-28 12:53:17,891 main.py:51] epoch 618, training loss: 10220.62, average training loss: 11525.91, base loss: 14297.79
[INFO 2017-06-28 12:53:18,565 main.py:51] epoch 619, training loss: 11035.07, average training loss: 11525.12, base loss: 14297.48
[INFO 2017-06-28 12:53:19,219 main.py:51] epoch 620, training loss: 10498.52, average training loss: 11523.46, base loss: 14297.00
[INFO 2017-06-28 12:53:19,874 main.py:51] epoch 621, training loss: 9851.58, average training loss: 11520.78, base loss: 14294.43
[INFO 2017-06-28 12:53:20,542 main.py:51] epoch 622, training loss: 10030.37, average training loss: 11518.38, base loss: 14294.56
[INFO 2017-06-28 12:53:21,204 main.py:51] epoch 623, training loss: 10340.25, average training loss: 11516.50, base loss: 14295.36
[INFO 2017-06-28 12:53:21,860 main.py:51] epoch 624, training loss: 11154.37, average training loss: 11515.92, base loss: 14296.62
[INFO 2017-06-28 12:53:22,493 main.py:51] epoch 625, training loss: 9932.04, average training loss: 11513.39, base loss: 14294.55
[INFO 2017-06-28 12:53:23,135 main.py:51] epoch 626, training loss: 11722.00, average training loss: 11513.72, base loss: 14298.19
[INFO 2017-06-28 12:53:23,809 main.py:51] epoch 627, training loss: 13364.23, average training loss: 11516.67, base loss: 14303.75
[INFO 2017-06-28 12:53:24,469 main.py:51] epoch 628, training loss: 9455.94, average training loss: 11513.39, base loss: 14301.25
[INFO 2017-06-28 12:53:25,147 main.py:51] epoch 629, training loss: 9775.05, average training loss: 11510.63, base loss: 14299.07
[INFO 2017-06-28 12:53:25,788 main.py:51] epoch 630, training loss: 10113.56, average training loss: 11508.42, base loss: 14297.57
[INFO 2017-06-28 12:53:26,457 main.py:51] epoch 631, training loss: 8701.43, average training loss: 11503.98, base loss: 14293.88
[INFO 2017-06-28 12:53:27,114 main.py:51] epoch 632, training loss: 9289.98, average training loss: 11500.48, base loss: 14291.22
[INFO 2017-06-28 12:53:27,759 main.py:51] epoch 633, training loss: 11574.39, average training loss: 11500.59, base loss: 14293.91
[INFO 2017-06-28 12:53:28,421 main.py:51] epoch 634, training loss: 11366.72, average training loss: 11500.38, base loss: 14296.09
[INFO 2017-06-28 12:53:29,057 main.py:51] epoch 635, training loss: 10334.38, average training loss: 11498.55, base loss: 14296.59
[INFO 2017-06-28 12:53:29,692 main.py:51] epoch 636, training loss: 9865.09, average training loss: 11495.99, base loss: 14294.31
[INFO 2017-06-28 12:53:30,342 main.py:51] epoch 637, training loss: 10958.68, average training loss: 11495.14, base loss: 14296.36
[INFO 2017-06-28 12:53:30,995 main.py:51] epoch 638, training loss: 10478.29, average training loss: 11493.55, base loss: 14294.93
[INFO 2017-06-28 12:53:31,643 main.py:51] epoch 639, training loss: 13271.31, average training loss: 11496.33, base loss: 14298.04
[INFO 2017-06-28 12:53:32,309 main.py:51] epoch 640, training loss: 11722.68, average training loss: 11496.68, base loss: 14300.42
[INFO 2017-06-28 12:53:32,965 main.py:51] epoch 641, training loss: 11241.39, average training loss: 11496.29, base loss: 14302.57
[INFO 2017-06-28 12:53:33,623 main.py:51] epoch 642, training loss: 11046.45, average training loss: 11495.59, base loss: 14303.17
[INFO 2017-06-28 12:53:34,285 main.py:51] epoch 643, training loss: 10282.39, average training loss: 11493.70, base loss: 14302.85
[INFO 2017-06-28 12:53:34,943 main.py:51] epoch 644, training loss: 11165.25, average training loss: 11493.19, base loss: 14305.46
[INFO 2017-06-28 12:53:35,610 main.py:51] epoch 645, training loss: 9884.00, average training loss: 11490.70, base loss: 14304.74
[INFO 2017-06-28 12:53:36,271 main.py:51] epoch 646, training loss: 10606.40, average training loss: 11489.33, base loss: 14303.64
[INFO 2017-06-28 12:53:36,939 main.py:51] epoch 647, training loss: 11039.48, average training loss: 11488.64, base loss: 14303.69
[INFO 2017-06-28 12:53:37,586 main.py:51] epoch 648, training loss: 11595.98, average training loss: 11488.81, base loss: 14307.35
[INFO 2017-06-28 12:53:38,245 main.py:51] epoch 649, training loss: 12368.42, average training loss: 11490.16, base loss: 14311.82
[INFO 2017-06-28 12:53:38,883 main.py:51] epoch 650, training loss: 11066.77, average training loss: 11489.51, base loss: 14311.51
[INFO 2017-06-28 12:53:39,528 main.py:51] epoch 651, training loss: 11859.37, average training loss: 11490.08, base loss: 14313.94
[INFO 2017-06-28 12:53:40,170 main.py:51] epoch 652, training loss: 10970.08, average training loss: 11489.28, base loss: 14315.22
[INFO 2017-06-28 12:53:40,821 main.py:51] epoch 653, training loss: 11192.52, average training loss: 11488.83, base loss: 14317.53
[INFO 2017-06-28 12:53:41,467 main.py:51] epoch 654, training loss: 11301.52, average training loss: 11488.54, base loss: 14318.00
[INFO 2017-06-28 12:53:42,164 main.py:51] epoch 655, training loss: 11139.58, average training loss: 11488.01, base loss: 14320.48
[INFO 2017-06-28 12:53:42,813 main.py:51] epoch 656, training loss: 12190.31, average training loss: 11489.08, base loss: 14324.34
[INFO 2017-06-28 12:53:43,489 main.py:51] epoch 657, training loss: 10542.94, average training loss: 11487.64, base loss: 14323.96
[INFO 2017-06-28 12:53:44,139 main.py:51] epoch 658, training loss: 10123.63, average training loss: 11485.57, base loss: 14322.18
[INFO 2017-06-28 12:53:44,795 main.py:51] epoch 659, training loss: 10890.92, average training loss: 11484.67, base loss: 14323.13
[INFO 2017-06-28 12:53:45,437 main.py:51] epoch 660, training loss: 9474.48, average training loss: 11481.63, base loss: 14320.26
[INFO 2017-06-28 12:53:46,093 main.py:51] epoch 661, training loss: 8592.77, average training loss: 11477.26, base loss: 14316.38
[INFO 2017-06-28 12:53:46,742 main.py:51] epoch 662, training loss: 10331.94, average training loss: 11475.54, base loss: 14316.14
[INFO 2017-06-28 12:53:47,402 main.py:51] epoch 663, training loss: 11073.27, average training loss: 11474.93, base loss: 14317.35
[INFO 2017-06-28 12:53:48,069 main.py:51] epoch 664, training loss: 11627.32, average training loss: 11475.16, base loss: 14320.32
[INFO 2017-06-28 12:53:48,754 main.py:51] epoch 665, training loss: 10493.53, average training loss: 11473.69, base loss: 14319.45
[INFO 2017-06-28 12:53:49,399 main.py:51] epoch 666, training loss: 10465.74, average training loss: 11472.17, base loss: 14318.27
[INFO 2017-06-28 12:53:50,068 main.py:51] epoch 667, training loss: 10620.62, average training loss: 11470.90, base loss: 14317.98
[INFO 2017-06-28 12:53:50,708 main.py:51] epoch 668, training loss: 10470.22, average training loss: 11469.40, base loss: 14317.49
[INFO 2017-06-28 12:53:51,382 main.py:51] epoch 669, training loss: 10660.99, average training loss: 11468.20, base loss: 14317.59
[INFO 2017-06-28 12:53:52,025 main.py:51] epoch 670, training loss: 9782.71, average training loss: 11465.69, base loss: 14314.19
[INFO 2017-06-28 12:53:52,671 main.py:51] epoch 671, training loss: 10092.02, average training loss: 11463.64, base loss: 14313.19
[INFO 2017-06-28 12:53:53,342 main.py:51] epoch 672, training loss: 10520.56, average training loss: 11462.24, base loss: 14313.88
[INFO 2017-06-28 12:53:53,995 main.py:51] epoch 673, training loss: 10480.74, average training loss: 11460.78, base loss: 14312.78
[INFO 2017-06-28 12:53:54,635 main.py:51] epoch 674, training loss: 9451.17, average training loss: 11457.81, base loss: 14310.99
[INFO 2017-06-28 12:53:55,280 main.py:51] epoch 675, training loss: 11697.17, average training loss: 11458.16, base loss: 14313.12
[INFO 2017-06-28 12:53:55,920 main.py:51] epoch 676, training loss: 13290.61, average training loss: 11460.87, base loss: 14318.84
[INFO 2017-06-28 12:53:56,577 main.py:51] epoch 677, training loss: 11162.33, average training loss: 11460.43, base loss: 14319.82
[INFO 2017-06-28 12:53:57,249 main.py:51] epoch 678, training loss: 9535.46, average training loss: 11457.59, base loss: 14317.08
[INFO 2017-06-28 12:53:57,916 main.py:51] epoch 679, training loss: 10233.87, average training loss: 11455.79, base loss: 14315.76
[INFO 2017-06-28 12:53:58,569 main.py:51] epoch 680, training loss: 11219.45, average training loss: 11455.45, base loss: 14317.33
[INFO 2017-06-28 12:53:59,219 main.py:51] epoch 681, training loss: 9997.79, average training loss: 11453.31, base loss: 14316.64
[INFO 2017-06-28 12:53:59,866 main.py:51] epoch 682, training loss: 10074.89, average training loss: 11451.29, base loss: 14316.25
[INFO 2017-06-28 12:54:00,533 main.py:51] epoch 683, training loss: 10334.29, average training loss: 11449.66, base loss: 14316.43
[INFO 2017-06-28 12:54:01,188 main.py:51] epoch 684, training loss: 9672.82, average training loss: 11447.06, base loss: 14315.21
[INFO 2017-06-28 12:54:01,855 main.py:51] epoch 685, training loss: 9153.82, average training loss: 11443.72, base loss: 14313.22
[INFO 2017-06-28 12:54:02,493 main.py:51] epoch 686, training loss: 9588.80, average training loss: 11441.02, base loss: 14310.95
[INFO 2017-06-28 12:54:03,141 main.py:51] epoch 687, training loss: 10684.56, average training loss: 11439.92, base loss: 14311.37
[INFO 2017-06-28 12:54:03,822 main.py:51] epoch 688, training loss: 12203.48, average training loss: 11441.03, base loss: 14315.39
[INFO 2017-06-28 12:54:04,474 main.py:51] epoch 689, training loss: 10344.33, average training loss: 11439.44, base loss: 14314.89
[INFO 2017-06-28 12:54:05,127 main.py:51] epoch 690, training loss: 12453.79, average training loss: 11440.91, base loss: 14318.33
[INFO 2017-06-28 12:54:05,777 main.py:51] epoch 691, training loss: 10793.69, average training loss: 11439.97, base loss: 14317.75
[INFO 2017-06-28 12:54:06,438 main.py:51] epoch 692, training loss: 10156.63, average training loss: 11438.12, base loss: 14315.45
[INFO 2017-06-28 12:54:07,106 main.py:51] epoch 693, training loss: 10648.80, average training loss: 11436.98, base loss: 14315.87
[INFO 2017-06-28 12:54:07,751 main.py:51] epoch 694, training loss: 10646.08, average training loss: 11435.84, base loss: 14317.33
[INFO 2017-06-28 12:54:08,412 main.py:51] epoch 695, training loss: 13567.06, average training loss: 11438.91, base loss: 14322.73
[INFO 2017-06-28 12:54:09,068 main.py:51] epoch 696, training loss: 11107.59, average training loss: 11438.43, base loss: 14323.90
[INFO 2017-06-28 12:54:09,720 main.py:51] epoch 697, training loss: 9213.35, average training loss: 11435.24, base loss: 14320.54
[INFO 2017-06-28 12:54:10,376 main.py:51] epoch 698, training loss: 10508.09, average training loss: 11433.92, base loss: 14321.78
[INFO 2017-06-28 12:54:11,041 main.py:51] epoch 699, training loss: 10151.33, average training loss: 11432.08, base loss: 14320.15
[INFO 2017-06-28 12:54:11,041 main.py:53] epoch 699, testing
[INFO 2017-06-28 12:54:13,612 main.py:105] average testing loss: 11140.68, base loss: 14285.85
[INFO 2017-06-28 12:54:13,612 main.py:106] improve_loss: 3145.16, improve_percent: 0.22
[INFO 2017-06-28 12:54:13,613 main.py:76] current best improved percent: 0.23
[INFO 2017-06-28 12:54:14,280 main.py:51] epoch 700, training loss: 10014.04, average training loss: 11430.06, base loss: 14319.57
[INFO 2017-06-28 12:54:14,948 main.py:51] epoch 701, training loss: 10512.21, average training loss: 11428.75, base loss: 14320.64
[INFO 2017-06-28 12:54:15,621 main.py:51] epoch 702, training loss: 11404.43, average training loss: 11428.72, base loss: 14323.56
[INFO 2017-06-28 12:54:16,267 main.py:51] epoch 703, training loss: 9659.10, average training loss: 11426.21, base loss: 14322.26
[INFO 2017-06-28 12:54:16,919 main.py:51] epoch 704, training loss: 12596.85, average training loss: 11427.87, base loss: 14325.43
[INFO 2017-06-28 12:54:17,574 main.py:51] epoch 705, training loss: 9016.67, average training loss: 11424.45, base loss: 14322.20
[INFO 2017-06-28 12:54:18,230 main.py:51] epoch 706, training loss: 9371.70, average training loss: 11421.55, base loss: 14320.60
[INFO 2017-06-28 12:54:18,887 main.py:51] epoch 707, training loss: 8334.91, average training loss: 11417.19, base loss: 14315.55
[INFO 2017-06-28 12:54:19,531 main.py:51] epoch 708, training loss: 9798.43, average training loss: 11414.91, base loss: 14314.13
[INFO 2017-06-28 12:54:20,188 main.py:51] epoch 709, training loss: 10325.66, average training loss: 11413.37, base loss: 14314.25
[INFO 2017-06-28 12:54:20,835 main.py:51] epoch 710, training loss: 9438.21, average training loss: 11410.59, base loss: 14311.72
[INFO 2017-06-28 12:54:21,495 main.py:51] epoch 711, training loss: 9479.61, average training loss: 11407.88, base loss: 14309.40
[INFO 2017-06-28 12:54:22,152 main.py:51] epoch 712, training loss: 10584.80, average training loss: 11406.73, base loss: 14309.15
[INFO 2017-06-28 12:54:22,811 main.py:51] epoch 713, training loss: 9366.36, average training loss: 11403.87, base loss: 14307.60
[INFO 2017-06-28 12:54:23,456 main.py:51] epoch 714, training loss: 9888.70, average training loss: 11401.75, base loss: 14305.99
[INFO 2017-06-28 12:54:24,103 main.py:51] epoch 715, training loss: 10404.27, average training loss: 11400.36, base loss: 14305.75
[INFO 2017-06-28 12:54:24,758 main.py:51] epoch 716, training loss: 9436.54, average training loss: 11397.62, base loss: 14305.17
[INFO 2017-06-28 12:54:25,423 main.py:51] epoch 717, training loss: 11966.10, average training loss: 11398.41, base loss: 14308.45
[INFO 2017-06-28 12:54:26,061 main.py:51] epoch 718, training loss: 10246.08, average training loss: 11396.81, base loss: 14308.40
[INFO 2017-06-28 12:54:26,716 main.py:51] epoch 719, training loss: 10292.51, average training loss: 11395.27, base loss: 14309.59
[INFO 2017-06-28 12:54:27,356 main.py:51] epoch 720, training loss: 11213.97, average training loss: 11395.02, base loss: 14311.30
[INFO 2017-06-28 12:54:27,994 main.py:51] epoch 721, training loss: 10691.39, average training loss: 11394.05, base loss: 14312.10
[INFO 2017-06-28 12:54:28,648 main.py:51] epoch 722, training loss: 10213.66, average training loss: 11392.41, base loss: 14313.09
[INFO 2017-06-28 12:54:29,312 main.py:51] epoch 723, training loss: 11405.24, average training loss: 11392.43, base loss: 14313.15
[INFO 2017-06-28 12:54:29,960 main.py:51] epoch 724, training loss: 11364.27, average training loss: 11392.39, base loss: 14316.28
[INFO 2017-06-28 12:54:30,606 main.py:51] epoch 725, training loss: 9747.89, average training loss: 11390.13, base loss: 14312.90
[INFO 2017-06-28 12:54:31,251 main.py:51] epoch 726, training loss: 9681.83, average training loss: 11387.78, base loss: 14309.66
[INFO 2017-06-28 12:54:31,906 main.py:51] epoch 727, training loss: 10004.12, average training loss: 11385.88, base loss: 14308.27
[INFO 2017-06-28 12:54:32,578 main.py:51] epoch 728, training loss: 11474.70, average training loss: 11386.00, base loss: 14310.84
[INFO 2017-06-28 12:54:33,226 main.py:51] epoch 729, training loss: 10912.29, average training loss: 11385.35, base loss: 14311.68
[INFO 2017-06-28 12:54:33,873 main.py:51] epoch 730, training loss: 8860.61, average training loss: 11381.90, base loss: 14308.64
[INFO 2017-06-28 12:54:34,520 main.py:51] epoch 731, training loss: 9800.29, average training loss: 11379.74, base loss: 14307.92
[INFO 2017-06-28 12:54:35,175 main.py:51] epoch 732, training loss: 9021.43, average training loss: 11376.52, base loss: 14304.95
[INFO 2017-06-28 12:54:35,820 main.py:51] epoch 733, training loss: 9082.15, average training loss: 11373.39, base loss: 14300.83
[INFO 2017-06-28 12:54:36,465 main.py:51] epoch 734, training loss: 10148.88, average training loss: 11371.73, base loss: 14301.30
[INFO 2017-06-28 12:54:37,110 main.py:51] epoch 735, training loss: 10249.88, average training loss: 11370.20, base loss: 14301.92
[INFO 2017-06-28 12:54:37,771 main.py:51] epoch 736, training loss: 9162.15, average training loss: 11367.21, base loss: 14299.72
[INFO 2017-06-28 12:54:38,472 main.py:51] epoch 737, training loss: 11154.02, average training loss: 11366.92, base loss: 14300.60
[INFO 2017-06-28 12:54:39,175 main.py:51] epoch 738, training loss: 10386.38, average training loss: 11365.59, base loss: 14299.79
[INFO 2017-06-28 12:54:39,826 main.py:51] epoch 739, training loss: 10121.34, average training loss: 11363.91, base loss: 14296.06
[INFO 2017-06-28 12:54:40,479 main.py:51] epoch 740, training loss: 10921.92, average training loss: 11363.31, base loss: 14296.13
[INFO 2017-06-28 12:54:41,134 main.py:51] epoch 741, training loss: 8339.39, average training loss: 11359.24, base loss: 14291.43
[INFO 2017-06-28 12:54:41,796 main.py:51] epoch 742, training loss: 11698.54, average training loss: 11359.69, base loss: 14295.12
[INFO 2017-06-28 12:54:42,470 main.py:51] epoch 743, training loss: 10893.62, average training loss: 11359.07, base loss: 14295.52
[INFO 2017-06-28 12:54:43,136 main.py:51] epoch 744, training loss: 12532.98, average training loss: 11360.64, base loss: 14298.90
[INFO 2017-06-28 12:54:43,784 main.py:51] epoch 745, training loss: 9841.08, average training loss: 11358.61, base loss: 14297.79
[INFO 2017-06-28 12:54:44,444 main.py:51] epoch 746, training loss: 11134.66, average training loss: 11358.31, base loss: 14298.84
[INFO 2017-06-28 12:54:45,113 main.py:51] epoch 747, training loss: 10348.51, average training loss: 11356.96, base loss: 14298.27
[INFO 2017-06-28 12:54:45,772 main.py:51] epoch 748, training loss: 11250.38, average training loss: 11356.81, base loss: 14300.13
[INFO 2017-06-28 12:54:46,439 main.py:51] epoch 749, training loss: 9661.35, average training loss: 11354.55, base loss: 14298.01
[INFO 2017-06-28 12:54:47,095 main.py:51] epoch 750, training loss: 11699.25, average training loss: 11355.01, base loss: 14298.67
[INFO 2017-06-28 12:54:47,776 main.py:51] epoch 751, training loss: 11789.89, average training loss: 11355.59, base loss: 14300.72
[INFO 2017-06-28 12:54:48,429 main.py:51] epoch 752, training loss: 10247.08, average training loss: 11354.12, base loss: 14301.88
[INFO 2017-06-28 12:54:49,102 main.py:51] epoch 753, training loss: 10616.68, average training loss: 11353.14, base loss: 14301.86
[INFO 2017-06-28 12:54:49,750 main.py:51] epoch 754, training loss: 9495.27, average training loss: 11350.68, base loss: 14301.09
[INFO 2017-06-28 12:54:50,416 main.py:51] epoch 755, training loss: 11542.64, average training loss: 11350.93, base loss: 14302.65
[INFO 2017-06-28 12:54:51,076 main.py:51] epoch 756, training loss: 10138.50, average training loss: 11349.33, base loss: 14301.86
[INFO 2017-06-28 12:54:51,760 main.py:51] epoch 757, training loss: 11503.61, average training loss: 11349.54, base loss: 14302.73
[INFO 2017-06-28 12:54:52,433 main.py:51] epoch 758, training loss: 10286.74, average training loss: 11348.14, base loss: 14303.26
[INFO 2017-06-28 12:54:53,103 main.py:51] epoch 759, training loss: 8562.73, average training loss: 11344.47, base loss: 14300.18
[INFO 2017-06-28 12:54:53,773 main.py:51] epoch 760, training loss: 9945.93, average training loss: 11342.63, base loss: 14301.25
[INFO 2017-06-28 12:54:54,431 main.py:51] epoch 761, training loss: 10605.62, average training loss: 11341.67, base loss: 14301.95
[INFO 2017-06-28 12:54:55,082 main.py:51] epoch 762, training loss: 10106.49, average training loss: 11340.05, base loss: 14300.92
[INFO 2017-06-28 12:54:55,715 main.py:51] epoch 763, training loss: 10192.56, average training loss: 11338.55, base loss: 14300.57
[INFO 2017-06-28 12:54:56,355 main.py:51] epoch 764, training loss: 10011.22, average training loss: 11336.81, base loss: 14299.47
[INFO 2017-06-28 12:54:57,015 main.py:51] epoch 765, training loss: 9952.03, average training loss: 11335.00, base loss: 14299.97
[INFO 2017-06-28 12:54:57,690 main.py:51] epoch 766, training loss: 11415.72, average training loss: 11335.11, base loss: 14302.40
[INFO 2017-06-28 12:54:58,361 main.py:51] epoch 767, training loss: 9516.12, average training loss: 11332.74, base loss: 14300.26
[INFO 2017-06-28 12:54:59,017 main.py:51] epoch 768, training loss: 10402.46, average training loss: 11331.53, base loss: 14300.62
[INFO 2017-06-28 12:54:59,678 main.py:51] epoch 769, training loss: 11220.33, average training loss: 11331.38, base loss: 14301.71
[INFO 2017-06-28 12:55:00,330 main.py:51] epoch 770, training loss: 11032.37, average training loss: 11331.00, base loss: 14300.73
[INFO 2017-06-28 12:55:01,002 main.py:51] epoch 771, training loss: 10030.46, average training loss: 11329.31, base loss: 14299.84
[INFO 2017-06-28 12:55:01,688 main.py:51] epoch 772, training loss: 10299.41, average training loss: 11327.98, base loss: 14300.12
[INFO 2017-06-28 12:55:02,385 main.py:51] epoch 773, training loss: 9369.73, average training loss: 11325.45, base loss: 14297.77
[INFO 2017-06-28 12:55:03,062 main.py:51] epoch 774, training loss: 10370.51, average training loss: 11324.22, base loss: 14297.28
[INFO 2017-06-28 12:55:03,734 main.py:51] epoch 775, training loss: 11102.60, average training loss: 11323.93, base loss: 14299.91
[INFO 2017-06-28 12:55:04,369 main.py:51] epoch 776, training loss: 9651.19, average training loss: 11321.78, base loss: 14297.60
[INFO 2017-06-28 12:55:05,025 main.py:51] epoch 777, training loss: 9345.05, average training loss: 11319.24, base loss: 14295.56
[INFO 2017-06-28 12:55:05,668 main.py:51] epoch 778, training loss: 10212.95, average training loss: 11317.82, base loss: 14295.21
[INFO 2017-06-28 12:55:06,329 main.py:51] epoch 779, training loss: 9935.21, average training loss: 11316.05, base loss: 14294.35
[INFO 2017-06-28 12:55:06,993 main.py:51] epoch 780, training loss: 10422.55, average training loss: 11314.90, base loss: 14292.44
[INFO 2017-06-28 12:55:07,653 main.py:51] epoch 781, training loss: 10793.83, average training loss: 11314.24, base loss: 14293.57
[INFO 2017-06-28 12:55:08,311 main.py:51] epoch 782, training loss: 11930.52, average training loss: 11315.02, base loss: 14296.68
[INFO 2017-06-28 12:55:08,987 main.py:51] epoch 783, training loss: 9690.84, average training loss: 11312.95, base loss: 14295.51
[INFO 2017-06-28 12:55:09,643 main.py:51] epoch 784, training loss: 10941.92, average training loss: 11312.48, base loss: 14294.55
[INFO 2017-06-28 12:55:10,311 main.py:51] epoch 785, training loss: 11467.18, average training loss: 11312.68, base loss: 14296.75
[INFO 2017-06-28 12:55:10,967 main.py:51] epoch 786, training loss: 9573.44, average training loss: 11310.47, base loss: 14294.44
[INFO 2017-06-28 12:55:11,617 main.py:51] epoch 787, training loss: 9830.92, average training loss: 11308.59, base loss: 14293.19
[INFO 2017-06-28 12:55:12,281 main.py:51] epoch 788, training loss: 10532.52, average training loss: 11307.60, base loss: 14293.06
[INFO 2017-06-28 12:55:12,940 main.py:51] epoch 789, training loss: 8728.27, average training loss: 11304.34, base loss: 14289.35
[INFO 2017-06-28 12:55:13,588 main.py:51] epoch 790, training loss: 12119.72, average training loss: 11305.37, base loss: 14292.47
[INFO 2017-06-28 12:55:14,251 main.py:51] epoch 791, training loss: 12655.93, average training loss: 11307.08, base loss: 14295.39
[INFO 2017-06-28 12:55:14,925 main.py:51] epoch 792, training loss: 10941.05, average training loss: 11306.61, base loss: 14297.58
[INFO 2017-06-28 12:55:15,578 main.py:51] epoch 793, training loss: 10958.54, average training loss: 11306.18, base loss: 14298.52
[INFO 2017-06-28 12:55:16,230 main.py:51] epoch 794, training loss: 9481.13, average training loss: 11303.88, base loss: 14296.58
[INFO 2017-06-28 12:55:16,871 main.py:51] epoch 795, training loss: 10698.19, average training loss: 11303.12, base loss: 14296.63
[INFO 2017-06-28 12:55:17,545 main.py:51] epoch 796, training loss: 11172.44, average training loss: 11302.95, base loss: 14297.63
[INFO 2017-06-28 12:55:18,211 main.py:51] epoch 797, training loss: 11090.11, average training loss: 11302.69, base loss: 14298.42
[INFO 2017-06-28 12:55:18,865 main.py:51] epoch 798, training loss: 12525.69, average training loss: 11304.22, base loss: 14301.30
[INFO 2017-06-28 12:55:19,518 main.py:51] epoch 799, training loss: 10994.15, average training loss: 11303.83, base loss: 14302.18
[INFO 2017-06-28 12:55:19,518 main.py:53] epoch 799, testing
[INFO 2017-06-28 12:55:22,113 main.py:105] average testing loss: 11424.59, base loss: 15135.31
[INFO 2017-06-28 12:55:22,113 main.py:106] improve_loss: 3710.73, improve_percent: 0.25
[INFO 2017-06-28 12:55:22,114 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:55:22,150 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:55:22,812 main.py:51] epoch 800, training loss: 9514.05, average training loss: 11301.60, base loss: 14300.19
[INFO 2017-06-28 12:55:23,470 main.py:51] epoch 801, training loss: 11428.16, average training loss: 11301.75, base loss: 14302.26
[INFO 2017-06-28 12:55:24,124 main.py:51] epoch 802, training loss: 9733.34, average training loss: 11299.80, base loss: 14299.26
[INFO 2017-06-28 12:55:24,792 main.py:51] epoch 803, training loss: 10548.18, average training loss: 11298.87, base loss: 14299.70
[INFO 2017-06-28 12:55:25,449 main.py:51] epoch 804, training loss: 11586.27, average training loss: 11299.22, base loss: 14301.15
[INFO 2017-06-28 12:55:26,120 main.py:51] epoch 805, training loss: 11401.42, average training loss: 11299.35, base loss: 14301.48
[INFO 2017-06-28 12:55:26,783 main.py:51] epoch 806, training loss: 9486.31, average training loss: 11297.10, base loss: 14300.11
[INFO 2017-06-28 12:55:27,428 main.py:51] epoch 807, training loss: 9872.40, average training loss: 11295.34, base loss: 14301.39
[INFO 2017-06-28 12:55:28,089 main.py:51] epoch 808, training loss: 10133.92, average training loss: 11293.90, base loss: 14299.59
[INFO 2017-06-28 12:55:28,779 main.py:51] epoch 809, training loss: 11657.52, average training loss: 11294.35, base loss: 14301.40
[INFO 2017-06-28 12:55:29,449 main.py:51] epoch 810, training loss: 10276.49, average training loss: 11293.10, base loss: 14301.12
[INFO 2017-06-28 12:55:30,115 main.py:51] epoch 811, training loss: 9301.84, average training loss: 11290.65, base loss: 14299.43
[INFO 2017-06-28 12:55:30,768 main.py:51] epoch 812, training loss: 9240.74, average training loss: 11288.12, base loss: 14297.11
[INFO 2017-06-28 12:55:31,428 main.py:51] epoch 813, training loss: 8800.24, average training loss: 11285.07, base loss: 14293.70
[INFO 2017-06-28 12:55:32,122 main.py:51] epoch 814, training loss: 10213.95, average training loss: 11283.75, base loss: 14292.44
[INFO 2017-06-28 12:55:32,781 main.py:51] epoch 815, training loss: 10791.61, average training loss: 11283.15, base loss: 14292.12
[INFO 2017-06-28 12:55:33,422 main.py:51] epoch 816, training loss: 9435.18, average training loss: 11280.89, base loss: 14290.32
[INFO 2017-06-28 12:55:34,100 main.py:51] epoch 817, training loss: 10415.88, average training loss: 11279.83, base loss: 14290.35
[INFO 2017-06-28 12:55:34,736 main.py:51] epoch 818, training loss: 10765.40, average training loss: 11279.20, base loss: 14290.52
[INFO 2017-06-28 12:55:35,382 main.py:51] epoch 819, training loss: 10774.75, average training loss: 11278.59, base loss: 14291.76
[INFO 2017-06-28 12:55:36,031 main.py:51] epoch 820, training loss: 9433.61, average training loss: 11276.34, base loss: 14291.37
[INFO 2017-06-28 12:55:36,673 main.py:51] epoch 821, training loss: 8972.72, average training loss: 11273.54, base loss: 14288.06
[INFO 2017-06-28 12:55:37,329 main.py:51] epoch 822, training loss: 10170.74, average training loss: 11272.20, base loss: 14287.54
[INFO 2017-06-28 12:55:37,992 main.py:51] epoch 823, training loss: 11333.82, average training loss: 11272.27, base loss: 14288.75
[INFO 2017-06-28 12:55:38,649 main.py:51] epoch 824, training loss: 10952.89, average training loss: 11271.89, base loss: 14289.23
[INFO 2017-06-28 12:55:39,293 main.py:51] epoch 825, training loss: 10856.73, average training loss: 11271.38, base loss: 14290.37
[INFO 2017-06-28 12:55:39,954 main.py:51] epoch 826, training loss: 10358.25, average training loss: 11270.28, base loss: 14290.09
[INFO 2017-06-28 12:55:40,606 main.py:51] epoch 827, training loss: 10611.10, average training loss: 11269.48, base loss: 14289.94
[INFO 2017-06-28 12:55:41,281 main.py:51] epoch 828, training loss: 10384.54, average training loss: 11268.42, base loss: 14291.02
[INFO 2017-06-28 12:55:41,934 main.py:51] epoch 829, training loss: 11019.59, average training loss: 11268.12, base loss: 14292.18
[INFO 2017-06-28 12:55:42,603 main.py:51] epoch 830, training loss: 9764.68, average training loss: 11266.31, base loss: 14291.67
[INFO 2017-06-28 12:55:43,264 main.py:51] epoch 831, training loss: 9455.39, average training loss: 11264.13, base loss: 14290.72
[INFO 2017-06-28 12:55:43,929 main.py:51] epoch 832, training loss: 10954.18, average training loss: 11263.76, base loss: 14290.96
[INFO 2017-06-28 12:55:44,575 main.py:51] epoch 833, training loss: 10617.05, average training loss: 11262.98, base loss: 14292.32
[INFO 2017-06-28 12:55:45,228 main.py:51] epoch 834, training loss: 11242.95, average training loss: 11262.96, base loss: 14293.66
[INFO 2017-06-28 12:55:45,886 main.py:51] epoch 835, training loss: 9584.21, average training loss: 11260.95, base loss: 14293.09
[INFO 2017-06-28 12:55:46,556 main.py:51] epoch 836, training loss: 8969.96, average training loss: 11258.21, base loss: 14291.26
[INFO 2017-06-28 12:55:47,229 main.py:51] epoch 837, training loss: 10173.33, average training loss: 11256.92, base loss: 14289.71
[INFO 2017-06-28 12:55:47,897 main.py:51] epoch 838, training loss: 10207.89, average training loss: 11255.67, base loss: 14289.68
[INFO 2017-06-28 12:55:48,568 main.py:51] epoch 839, training loss: 10216.52, average training loss: 11254.43, base loss: 14290.17
[INFO 2017-06-28 12:55:49,218 main.py:51] epoch 840, training loss: 10224.58, average training loss: 11253.21, base loss: 14290.70
[INFO 2017-06-28 12:55:49,873 main.py:51] epoch 841, training loss: 10159.90, average training loss: 11251.91, base loss: 14290.00
[INFO 2017-06-28 12:55:50,526 main.py:51] epoch 842, training loss: 10730.52, average training loss: 11251.29, base loss: 14291.39
[INFO 2017-06-28 12:55:51,178 main.py:51] epoch 843, training loss: 10743.17, average training loss: 11250.69, base loss: 14291.71
[INFO 2017-06-28 12:55:51,835 main.py:51] epoch 844, training loss: 10024.19, average training loss: 11249.24, base loss: 14291.31
[INFO 2017-06-28 12:55:52,481 main.py:51] epoch 845, training loss: 9935.29, average training loss: 11247.68, base loss: 14290.18
[INFO 2017-06-28 12:55:53,145 main.py:51] epoch 846, training loss: 10123.95, average training loss: 11246.36, base loss: 14289.95
[INFO 2017-06-28 12:55:53,784 main.py:51] epoch 847, training loss: 10618.35, average training loss: 11245.62, base loss: 14289.00
[INFO 2017-06-28 12:55:54,458 main.py:51] epoch 848, training loss: 9979.28, average training loss: 11244.12, base loss: 14286.93
[INFO 2017-06-28 12:55:55,100 main.py:51] epoch 849, training loss: 12778.11, average training loss: 11245.93, base loss: 14290.88
[INFO 2017-06-28 12:55:55,745 main.py:51] epoch 850, training loss: 8868.55, average training loss: 11243.14, base loss: 14287.61
[INFO 2017-06-28 12:55:56,404 main.py:51] epoch 851, training loss: 9871.00, average training loss: 11241.53, base loss: 14286.36
[INFO 2017-06-28 12:55:57,052 main.py:51] epoch 852, training loss: 9098.67, average training loss: 11239.01, base loss: 14282.65
[INFO 2017-06-28 12:55:57,701 main.py:51] epoch 853, training loss: 11183.00, average training loss: 11238.95, base loss: 14284.59
[INFO 2017-06-28 12:55:58,359 main.py:51] epoch 854, training loss: 9577.55, average training loss: 11237.00, base loss: 14283.89
[INFO 2017-06-28 12:55:59,019 main.py:51] epoch 855, training loss: 10536.72, average training loss: 11236.19, base loss: 14284.10
[INFO 2017-06-28 12:55:59,682 main.py:51] epoch 856, training loss: 11432.62, average training loss: 11236.42, base loss: 14284.14
[INFO 2017-06-28 12:56:00,362 main.py:51] epoch 857, training loss: 9612.96, average training loss: 11234.52, base loss: 14282.43
[INFO 2017-06-28 12:56:01,002 main.py:51] epoch 858, training loss: 10727.88, average training loss: 11233.93, base loss: 14283.00
[INFO 2017-06-28 12:56:01,654 main.py:51] epoch 859, training loss: 9162.98, average training loss: 11231.53, base loss: 14280.68
[INFO 2017-06-28 12:56:02,300 main.py:51] epoch 860, training loss: 10275.42, average training loss: 11230.41, base loss: 14279.78
[INFO 2017-06-28 12:56:02,958 main.py:51] epoch 861, training loss: 9265.37, average training loss: 11228.14, base loss: 14276.41
[INFO 2017-06-28 12:56:03,611 main.py:51] epoch 862, training loss: 11935.93, average training loss: 11228.96, base loss: 14279.20
[INFO 2017-06-28 12:56:04,259 main.py:51] epoch 863, training loss: 10031.93, average training loss: 11227.57, base loss: 14279.06
[INFO 2017-06-28 12:56:04,900 main.py:51] epoch 864, training loss: 11571.20, average training loss: 11227.97, base loss: 14281.02
[INFO 2017-06-28 12:56:05,556 main.py:51] epoch 865, training loss: 9757.67, average training loss: 11226.27, base loss: 14280.42
[INFO 2017-06-28 12:56:06,207 main.py:51] epoch 866, training loss: 9503.74, average training loss: 11224.28, base loss: 14279.58
[INFO 2017-06-28 12:56:06,847 main.py:51] epoch 867, training loss: 10101.78, average training loss: 11222.99, base loss: 14278.83
[INFO 2017-06-28 12:56:07,534 main.py:51] epoch 868, training loss: 10135.84, average training loss: 11221.74, base loss: 14277.72
[INFO 2017-06-28 12:56:08,189 main.py:51] epoch 869, training loss: 8636.03, average training loss: 11218.77, base loss: 14274.35
[INFO 2017-06-28 12:56:08,838 main.py:51] epoch 870, training loss: 10397.18, average training loss: 11217.82, base loss: 14273.74
[INFO 2017-06-28 12:56:09,485 main.py:51] epoch 871, training loss: 8813.82, average training loss: 11215.07, base loss: 14271.41
[INFO 2017-06-28 12:56:10,134 main.py:51] epoch 872, training loss: 10653.86, average training loss: 11214.42, base loss: 14270.74
[INFO 2017-06-28 12:56:10,782 main.py:51] epoch 873, training loss: 11054.33, average training loss: 11214.24, base loss: 14271.33
[INFO 2017-06-28 12:56:11,425 main.py:51] epoch 874, training loss: 11074.69, average training loss: 11214.08, base loss: 14272.64
[INFO 2017-06-28 12:56:12,072 main.py:51] epoch 875, training loss: 10923.11, average training loss: 11213.75, base loss: 14272.89
[INFO 2017-06-28 12:56:12,735 main.py:51] epoch 876, training loss: 11529.80, average training loss: 11214.11, base loss: 14274.67
[INFO 2017-06-28 12:56:13,385 main.py:51] epoch 877, training loss: 11432.71, average training loss: 11214.36, base loss: 14276.77
[INFO 2017-06-28 12:56:14,013 main.py:51] epoch 878, training loss: 12017.60, average training loss: 11215.27, base loss: 14280.02
[INFO 2017-06-28 12:56:14,672 main.py:51] epoch 879, training loss: 9877.08, average training loss: 11213.75, base loss: 14279.63
[INFO 2017-06-28 12:56:15,351 main.py:51] epoch 880, training loss: 10387.06, average training loss: 11212.81, base loss: 14279.69
[INFO 2017-06-28 12:56:16,027 main.py:51] epoch 881, training loss: 10960.73, average training loss: 11212.53, base loss: 14281.62
[INFO 2017-06-28 12:56:16,685 main.py:51] epoch 882, training loss: 10948.46, average training loss: 11212.23, base loss: 14281.98
[INFO 2017-06-28 12:56:17,376 main.py:51] epoch 883, training loss: 11175.16, average training loss: 11212.19, base loss: 14283.24
[INFO 2017-06-28 12:56:18,043 main.py:51] epoch 884, training loss: 9333.64, average training loss: 11210.06, base loss: 14280.75
[INFO 2017-06-28 12:56:18,685 main.py:51] epoch 885, training loss: 9610.00, average training loss: 11208.26, base loss: 14279.38
[INFO 2017-06-28 12:56:19,332 main.py:51] epoch 886, training loss: 11561.66, average training loss: 11208.66, base loss: 14280.74
[INFO 2017-06-28 12:56:19,987 main.py:51] epoch 887, training loss: 11230.21, average training loss: 11208.68, base loss: 14280.62
[INFO 2017-06-28 12:56:20,622 main.py:51] epoch 888, training loss: 10956.44, average training loss: 11208.40, base loss: 14282.39
[INFO 2017-06-28 12:56:21,267 main.py:51] epoch 889, training loss: 11100.20, average training loss: 11208.27, base loss: 14284.73
[INFO 2017-06-28 12:56:21,902 main.py:51] epoch 890, training loss: 10213.36, average training loss: 11207.16, base loss: 14284.66
[INFO 2017-06-28 12:56:22,579 main.py:51] epoch 891, training loss: 13451.40, average training loss: 11209.67, base loss: 14288.27
[INFO 2017-06-28 12:56:23,226 main.py:51] epoch 892, training loss: 9107.94, average training loss: 11207.32, base loss: 14286.41
[INFO 2017-06-28 12:56:23,891 main.py:51] epoch 893, training loss: 10607.04, average training loss: 11206.65, base loss: 14285.77
[INFO 2017-06-28 12:56:24,531 main.py:51] epoch 894, training loss: 9529.97, average training loss: 11204.78, base loss: 14284.76
[INFO 2017-06-28 12:56:25,184 main.py:51] epoch 895, training loss: 12027.34, average training loss: 11205.69, base loss: 14285.80
[INFO 2017-06-28 12:56:25,835 main.py:51] epoch 896, training loss: 10740.49, average training loss: 11205.17, base loss: 14285.08
[INFO 2017-06-28 12:56:26,485 main.py:51] epoch 897, training loss: 10223.08, average training loss: 11204.08, base loss: 14283.70
[INFO 2017-06-28 12:56:27,142 main.py:51] epoch 898, training loss: 11421.74, average training loss: 11204.32, base loss: 14286.26
[INFO 2017-06-28 12:56:27,791 main.py:51] epoch 899, training loss: 10099.81, average training loss: 11203.10, base loss: 14286.32
[INFO 2017-06-28 12:56:27,791 main.py:53] epoch 899, testing
[INFO 2017-06-28 12:56:30,386 main.py:105] average testing loss: 10878.00, base loss: 14490.52
[INFO 2017-06-28 12:56:30,386 main.py:106] improve_loss: 3612.52, improve_percent: 0.25
[INFO 2017-06-28 12:56:30,387 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:56:30,422 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:56:31,061 main.py:51] epoch 900, training loss: 11209.41, average training loss: 11203.10, base loss: 14287.63
[INFO 2017-06-28 12:56:31,737 main.py:51] epoch 901, training loss: 10310.97, average training loss: 11202.11, base loss: 14286.99
[INFO 2017-06-28 12:56:32,407 main.py:51] epoch 902, training loss: 9819.80, average training loss: 11200.58, base loss: 14284.62
[INFO 2017-06-28 12:56:33,075 main.py:51] epoch 903, training loss: 10203.98, average training loss: 11199.48, base loss: 14285.06
[INFO 2017-06-28 12:56:33,720 main.py:51] epoch 904, training loss: 9889.69, average training loss: 11198.03, base loss: 14285.09
[INFO 2017-06-28 12:56:34,365 main.py:51] epoch 905, training loss: 9544.97, average training loss: 11196.21, base loss: 14281.45
[INFO 2017-06-28 12:56:35,011 main.py:51] epoch 906, training loss: 8776.27, average training loss: 11193.54, base loss: 14279.27
[INFO 2017-06-28 12:56:35,669 main.py:51] epoch 907, training loss: 9500.70, average training loss: 11191.68, base loss: 14277.68
[INFO 2017-06-28 12:56:36,326 main.py:51] epoch 908, training loss: 10744.39, average training loss: 11191.18, base loss: 14276.59
[INFO 2017-06-28 12:56:36,976 main.py:51] epoch 909, training loss: 9291.61, average training loss: 11189.10, base loss: 14275.75
[INFO 2017-06-28 12:56:37,628 main.py:51] epoch 910, training loss: 13161.84, average training loss: 11191.26, base loss: 14280.09
[INFO 2017-06-28 12:56:38,291 main.py:51] epoch 911, training loss: 9919.68, average training loss: 11189.87, base loss: 14279.10
[INFO 2017-06-28 12:56:38,947 main.py:51] epoch 912, training loss: 12212.89, average training loss: 11190.99, base loss: 14282.13
[INFO 2017-06-28 12:56:39,606 main.py:51] epoch 913, training loss: 11245.38, average training loss: 11191.05, base loss: 14282.50
[INFO 2017-06-28 12:56:40,265 main.py:51] epoch 914, training loss: 10671.70, average training loss: 11190.48, base loss: 14282.22
[INFO 2017-06-28 12:56:40,914 main.py:51] epoch 915, training loss: 12151.12, average training loss: 11191.53, base loss: 14286.17
[INFO 2017-06-28 12:56:41,574 main.py:51] epoch 916, training loss: 10728.47, average training loss: 11191.02, base loss: 14287.00
[INFO 2017-06-28 12:56:42,262 main.py:51] epoch 917, training loss: 9968.01, average training loss: 11189.69, base loss: 14286.38
[INFO 2017-06-28 12:56:42,943 main.py:51] epoch 918, training loss: 10368.16, average training loss: 11188.80, base loss: 14286.72
[INFO 2017-06-28 12:56:43,601 main.py:51] epoch 919, training loss: 10471.26, average training loss: 11188.02, base loss: 14286.50
[INFO 2017-06-28 12:56:44,281 main.py:51] epoch 920, training loss: 10917.63, average training loss: 11187.72, base loss: 14286.41
[INFO 2017-06-28 12:56:44,940 main.py:51] epoch 921, training loss: 10679.15, average training loss: 11187.17, base loss: 14287.19
[INFO 2017-06-28 12:56:45,585 main.py:51] epoch 922, training loss: 11818.40, average training loss: 11187.86, base loss: 14290.07
[INFO 2017-06-28 12:56:46,241 main.py:51] epoch 923, training loss: 10187.68, average training loss: 11186.77, base loss: 14290.04
[INFO 2017-06-28 12:56:46,894 main.py:51] epoch 924, training loss: 10621.79, average training loss: 11186.16, base loss: 14290.37
[INFO 2017-06-28 12:56:47,568 main.py:51] epoch 925, training loss: 9371.43, average training loss: 11184.20, base loss: 14288.52
[INFO 2017-06-28 12:56:48,219 main.py:51] epoch 926, training loss: 10357.36, average training loss: 11183.31, base loss: 14288.03
[INFO 2017-06-28 12:56:48,851 main.py:51] epoch 927, training loss: 8577.79, average training loss: 11180.50, base loss: 14285.28
[INFO 2017-06-28 12:56:49,493 main.py:51] epoch 928, training loss: 10678.01, average training loss: 11179.96, base loss: 14285.92
[INFO 2017-06-28 12:56:50,148 main.py:51] epoch 929, training loss: 9699.62, average training loss: 11178.37, base loss: 14284.62
[INFO 2017-06-28 12:56:50,794 main.py:51] epoch 930, training loss: 11692.39, average training loss: 11178.92, base loss: 14287.55
[INFO 2017-06-28 12:56:51,475 main.py:51] epoch 931, training loss: 12152.42, average training loss: 11179.97, base loss: 14289.79
[INFO 2017-06-28 12:56:52,150 main.py:51] epoch 932, training loss: 9333.73, average training loss: 11177.99, base loss: 14288.80
[INFO 2017-06-28 12:56:52,801 main.py:51] epoch 933, training loss: 10450.42, average training loss: 11177.21, base loss: 14289.48
[INFO 2017-06-28 12:56:53,456 main.py:51] epoch 934, training loss: 12772.20, average training loss: 11178.92, base loss: 14294.55
[INFO 2017-06-28 12:56:54,120 main.py:51] epoch 935, training loss: 9744.02, average training loss: 11177.38, base loss: 14294.97
[INFO 2017-06-28 12:56:54,769 main.py:51] epoch 936, training loss: 11271.11, average training loss: 11177.48, base loss: 14297.15
[INFO 2017-06-28 12:56:55,431 main.py:51] epoch 937, training loss: 11557.79, average training loss: 11177.89, base loss: 14297.25
[INFO 2017-06-28 12:56:56,073 main.py:51] epoch 938, training loss: 10627.79, average training loss: 11177.30, base loss: 14297.39
[INFO 2017-06-28 12:56:56,725 main.py:51] epoch 939, training loss: 11438.51, average training loss: 11177.58, base loss: 14298.53
[INFO 2017-06-28 12:56:57,376 main.py:51] epoch 940, training loss: 10403.34, average training loss: 11176.76, base loss: 14297.15
[INFO 2017-06-28 12:56:58,025 main.py:51] epoch 941, training loss: 10256.76, average training loss: 11175.78, base loss: 14296.15
[INFO 2017-06-28 12:56:58,670 main.py:51] epoch 942, training loss: 11536.61, average training loss: 11176.16, base loss: 14298.38
[INFO 2017-06-28 12:56:59,319 main.py:51] epoch 943, training loss: 12004.14, average training loss: 11177.04, base loss: 14299.37
[INFO 2017-06-28 12:56:59,966 main.py:51] epoch 944, training loss: 9303.61, average training loss: 11175.06, base loss: 14297.99
[INFO 2017-06-28 12:57:00,640 main.py:51] epoch 945, training loss: 9015.94, average training loss: 11172.78, base loss: 14295.16
[INFO 2017-06-28 12:57:01,306 main.py:51] epoch 946, training loss: 9906.76, average training loss: 11171.44, base loss: 14295.03
[INFO 2017-06-28 12:57:01,969 main.py:51] epoch 947, training loss: 9605.73, average training loss: 11169.79, base loss: 14294.07
[INFO 2017-06-28 12:57:02,629 main.py:51] epoch 948, training loss: 10511.30, average training loss: 11169.09, base loss: 14294.76
[INFO 2017-06-28 12:57:03,288 main.py:51] epoch 949, training loss: 8971.98, average training loss: 11166.78, base loss: 14292.00
[INFO 2017-06-28 12:57:03,945 main.py:51] epoch 950, training loss: 8529.54, average training loss: 11164.01, base loss: 14288.16
[INFO 2017-06-28 12:57:04,599 main.py:51] epoch 951, training loss: 10746.26, average training loss: 11163.57, base loss: 14289.12
[INFO 2017-06-28 12:57:05,242 main.py:51] epoch 952, training loss: 9630.86, average training loss: 11161.96, base loss: 14287.94
[INFO 2017-06-28 12:57:05,918 main.py:51] epoch 953, training loss: 10871.14, average training loss: 11161.66, base loss: 14289.18
[INFO 2017-06-28 12:57:06,583 main.py:51] epoch 954, training loss: 10862.29, average training loss: 11161.34, base loss: 14290.06
[INFO 2017-06-28 12:57:07,242 main.py:51] epoch 955, training loss: 10929.48, average training loss: 11161.10, base loss: 14291.18
[INFO 2017-06-28 12:57:07,886 main.py:51] epoch 956, training loss: 10314.93, average training loss: 11160.22, base loss: 14291.47
[INFO 2017-06-28 12:57:08,553 main.py:51] epoch 957, training loss: 8951.51, average training loss: 11157.91, base loss: 14288.85
[INFO 2017-06-28 12:57:09,198 main.py:51] epoch 958, training loss: 10250.13, average training loss: 11156.96, base loss: 14288.54
[INFO 2017-06-28 12:57:09,872 main.py:51] epoch 959, training loss: 10242.03, average training loss: 11156.01, base loss: 14288.49
[INFO 2017-06-28 12:57:10,518 main.py:51] epoch 960, training loss: 8893.60, average training loss: 11153.66, base loss: 14286.83
[INFO 2017-06-28 12:57:11,176 main.py:51] epoch 961, training loss: 11778.05, average training loss: 11154.31, base loss: 14288.73
[INFO 2017-06-28 12:57:11,825 main.py:51] epoch 962, training loss: 13767.81, average training loss: 11157.02, base loss: 14292.98
[INFO 2017-06-28 12:57:12,482 main.py:51] epoch 963, training loss: 10057.70, average training loss: 11155.88, base loss: 14293.30
[INFO 2017-06-28 12:57:13,153 main.py:51] epoch 964, training loss: 11357.87, average training loss: 11156.09, base loss: 14294.77
[INFO 2017-06-28 12:57:13,803 main.py:51] epoch 965, training loss: 8962.49, average training loss: 11153.82, base loss: 14293.03
[INFO 2017-06-28 12:57:14,458 main.py:51] epoch 966, training loss: 10631.47, average training loss: 11153.28, base loss: 14294.73
[INFO 2017-06-28 12:57:15,101 main.py:51] epoch 967, training loss: 10295.56, average training loss: 11152.39, base loss: 14293.39
[INFO 2017-06-28 12:57:15,767 main.py:51] epoch 968, training loss: 11239.92, average training loss: 11152.48, base loss: 14293.79
[INFO 2017-06-28 12:57:16,420 main.py:51] epoch 969, training loss: 9905.52, average training loss: 11151.20, base loss: 14292.37
[INFO 2017-06-28 12:57:17,075 main.py:51] epoch 970, training loss: 10203.81, average training loss: 11150.22, base loss: 14292.71
[INFO 2017-06-28 12:57:17,725 main.py:51] epoch 971, training loss: 10120.94, average training loss: 11149.16, base loss: 14292.96
[INFO 2017-06-28 12:57:18,385 main.py:51] epoch 972, training loss: 11553.27, average training loss: 11149.58, base loss: 14294.46
[INFO 2017-06-28 12:57:19,036 main.py:51] epoch 973, training loss: 11536.96, average training loss: 11149.97, base loss: 14296.19
[INFO 2017-06-28 12:57:19,712 main.py:51] epoch 974, training loss: 9901.43, average training loss: 11148.69, base loss: 14295.43
[INFO 2017-06-28 12:57:20,365 main.py:51] epoch 975, training loss: 10082.80, average training loss: 11147.60, base loss: 14294.71
[INFO 2017-06-28 12:57:21,039 main.py:51] epoch 976, training loss: 9286.63, average training loss: 11145.70, base loss: 14292.51
[INFO 2017-06-28 12:57:21,697 main.py:51] epoch 977, training loss: 9558.93, average training loss: 11144.07, base loss: 14290.32
[INFO 2017-06-28 12:57:22,364 main.py:51] epoch 978, training loss: 9528.77, average training loss: 11142.42, base loss: 14288.75
[INFO 2017-06-28 12:57:23,033 main.py:51] epoch 979, training loss: 9765.95, average training loss: 11141.02, base loss: 14289.13
[INFO 2017-06-28 12:57:23,691 main.py:51] epoch 980, training loss: 10044.31, average training loss: 11139.90, base loss: 14288.35
[INFO 2017-06-28 12:57:24,349 main.py:51] epoch 981, training loss: 9776.69, average training loss: 11138.51, base loss: 14288.56
[INFO 2017-06-28 12:57:25,004 main.py:51] epoch 982, training loss: 9055.74, average training loss: 11136.40, base loss: 14286.28
[INFO 2017-06-28 12:57:25,657 main.py:51] epoch 983, training loss: 10472.41, average training loss: 11135.72, base loss: 14286.22
[INFO 2017-06-28 12:57:26,327 main.py:51] epoch 984, training loss: 10749.97, average training loss: 11135.33, base loss: 14287.51
[INFO 2017-06-28 12:57:26,986 main.py:51] epoch 985, training loss: 9630.65, average training loss: 11133.80, base loss: 14286.39
[INFO 2017-06-28 12:57:27,640 main.py:51] epoch 986, training loss: 9915.95, average training loss: 11132.57, base loss: 14286.10
[INFO 2017-06-28 12:57:28,293 main.py:51] epoch 987, training loss: 11302.68, average training loss: 11132.74, base loss: 14287.04
[INFO 2017-06-28 12:57:28,928 main.py:51] epoch 988, training loss: 9488.19, average training loss: 11131.08, base loss: 14286.65
[INFO 2017-06-28 12:57:29,578 main.py:51] epoch 989, training loss: 9680.21, average training loss: 11129.61, base loss: 14284.98
[INFO 2017-06-28 12:57:30,256 main.py:51] epoch 990, training loss: 11803.75, average training loss: 11130.29, base loss: 14287.62
[INFO 2017-06-28 12:57:30,929 main.py:51] epoch 991, training loss: 13562.75, average training loss: 11132.75, base loss: 14291.62
[INFO 2017-06-28 12:57:31,593 main.py:51] epoch 992, training loss: 10134.90, average training loss: 11131.74, base loss: 14290.74
[INFO 2017-06-28 12:57:32,238 main.py:51] epoch 993, training loss: 8916.88, average training loss: 11129.51, base loss: 14288.46
[INFO 2017-06-28 12:57:32,897 main.py:51] epoch 994, training loss: 11132.93, average training loss: 11129.52, base loss: 14289.61
[INFO 2017-06-28 12:57:33,578 main.py:51] epoch 995, training loss: 10125.12, average training loss: 11128.51, base loss: 14290.85
[INFO 2017-06-28 12:57:34,263 main.py:51] epoch 996, training loss: 9332.55, average training loss: 11126.71, base loss: 14289.59
[INFO 2017-06-28 12:57:34,927 main.py:51] epoch 997, training loss: 10696.41, average training loss: 11126.27, base loss: 14290.16
[INFO 2017-06-28 12:57:35,587 main.py:51] epoch 998, training loss: 12172.18, average training loss: 11127.32, base loss: 14292.69
[INFO 2017-06-28 12:57:36,242 main.py:51] epoch 999, training loss: 10327.48, average training loss: 11126.52, base loss: 14292.84
[INFO 2017-06-28 12:57:36,242 main.py:53] epoch 999, testing
[INFO 2017-06-28 12:57:38,816 main.py:105] average testing loss: 11672.89, base loss: 15094.99
[INFO 2017-06-28 12:57:38,817 main.py:106] improve_loss: 3422.10, improve_percent: 0.23
[INFO 2017-06-28 12:57:38,817 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:57:39,453 main.py:51] epoch 1000, training loss: 9383.96, average training loss: 11107.45, base loss: 14291.09
[INFO 2017-06-28 12:57:40,106 main.py:51] epoch 1001, training loss: 9206.45, average training loss: 11095.21, base loss: 14290.19
[INFO 2017-06-28 12:57:40,757 main.py:51] epoch 1002, training loss: 9269.51, average training loss: 11086.99, base loss: 14289.86
[INFO 2017-06-28 12:57:41,403 main.py:51] epoch 1003, training loss: 9235.44, average training loss: 11076.14, base loss: 14286.43
[INFO 2017-06-28 12:57:42,068 main.py:51] epoch 1004, training loss: 11012.58, average training loss: 11070.59, base loss: 14288.68
[INFO 2017-06-28 12:57:42,750 main.py:51] epoch 1005, training loss: 12057.89, average training loss: 11065.98, base loss: 14290.21
[INFO 2017-06-28 12:57:43,393 main.py:51] epoch 1006, training loss: 11518.00, average training loss: 11061.03, base loss: 14291.65
[INFO 2017-06-28 12:57:44,028 main.py:51] epoch 1007, training loss: 11211.61, average training loss: 11056.48, base loss: 14292.02
[INFO 2017-06-28 12:57:44,726 main.py:51] epoch 1008, training loss: 9788.13, average training loss: 11052.60, base loss: 14292.69
[INFO 2017-06-28 12:57:45,398 main.py:51] epoch 1009, training loss: 9142.10, average training loss: 11049.20, base loss: 14293.50
[INFO 2017-06-28 12:57:46,039 main.py:51] epoch 1010, training loss: 10485.39, average training loss: 11045.90, base loss: 14294.36
[INFO 2017-06-28 12:57:46,675 main.py:51] epoch 1011, training loss: 10504.34, average training loss: 11040.59, base loss: 14293.93
[INFO 2017-06-28 12:57:47,340 main.py:51] epoch 1012, training loss: 10056.81, average training loss: 11037.55, base loss: 14294.14
[INFO 2017-06-28 12:57:48,015 main.py:51] epoch 1013, training loss: 10463.64, average training loss: 11031.15, base loss: 14290.79
[INFO 2017-06-28 12:57:48,666 main.py:51] epoch 1014, training loss: 11135.03, average training loss: 11028.71, base loss: 14291.78
[INFO 2017-06-28 12:57:49,307 main.py:51] epoch 1015, training loss: 9013.50, average training loss: 11024.05, base loss: 14290.55
[INFO 2017-06-28 12:57:49,975 main.py:51] epoch 1016, training loss: 12507.89, average training loss: 11021.11, base loss: 14291.60
[INFO 2017-06-28 12:57:50,661 main.py:51] epoch 1017, training loss: 10125.00, average training loss: 11019.00, base loss: 14293.52
[INFO 2017-06-28 12:57:51,312 main.py:51] epoch 1018, training loss: 11255.26, average training loss: 11018.07, base loss: 14296.51
[INFO 2017-06-28 12:57:51,959 main.py:51] epoch 1019, training loss: 8676.16, average training loss: 11012.19, base loss: 14293.39
[INFO 2017-06-28 12:57:52,605 main.py:51] epoch 1020, training loss: 11583.90, average training loss: 11008.33, base loss: 14292.46
[INFO 2017-06-28 12:57:53,247 main.py:51] epoch 1021, training loss: 11127.90, average training loss: 11004.15, base loss: 14292.09
[INFO 2017-06-28 12:57:53,889 main.py:51] epoch 1022, training loss: 9538.57, average training loss: 10997.38, base loss: 14287.73
[INFO 2017-06-28 12:57:54,519 main.py:51] epoch 1023, training loss: 11090.33, average training loss: 10994.19, base loss: 14287.55
[INFO 2017-06-28 12:57:55,184 main.py:51] epoch 1024, training loss: 9916.90, average training loss: 10991.12, base loss: 14287.45
[INFO 2017-06-28 12:57:55,819 main.py:51] epoch 1025, training loss: 11847.61, average training loss: 10988.37, base loss: 14289.11
[INFO 2017-06-28 12:57:56,488 main.py:51] epoch 1026, training loss: 10092.00, average training loss: 10985.17, base loss: 14289.83
[INFO 2017-06-28 12:57:57,149 main.py:51] epoch 1027, training loss: 9919.88, average training loss: 10977.66, base loss: 14285.47
[INFO 2017-06-28 12:57:57,808 main.py:51] epoch 1028, training loss: 10052.39, average training loss: 10974.39, base loss: 14285.17
[INFO 2017-06-28 12:57:58,446 main.py:51] epoch 1029, training loss: 9846.09, average training loss: 10972.66, base loss: 14286.72
[INFO 2017-06-28 12:57:59,090 main.py:51] epoch 1030, training loss: 9981.33, average training loss: 10966.66, base loss: 14284.04
[INFO 2017-06-28 12:57:59,749 main.py:51] epoch 1031, training loss: 9663.19, average training loss: 10960.55, base loss: 14281.58
[INFO 2017-06-28 12:58:00,406 main.py:51] epoch 1032, training loss: 10312.70, average training loss: 10957.32, base loss: 14281.30
[INFO 2017-06-28 12:58:01,057 main.py:51] epoch 1033, training loss: 10572.93, average training loss: 10956.17, base loss: 14282.97
[INFO 2017-06-28 12:58:01,711 main.py:51] epoch 1034, training loss: 11812.00, average training loss: 10953.57, base loss: 14283.47
[INFO 2017-06-28 12:58:02,354 main.py:51] epoch 1035, training loss: 11319.68, average training loss: 10951.12, base loss: 14285.53
[INFO 2017-06-28 12:58:03,034 main.py:51] epoch 1036, training loss: 8912.77, average training loss: 10946.50, base loss: 14283.16
[INFO 2017-06-28 12:58:03,703 main.py:51] epoch 1037, training loss: 10429.11, average training loss: 10944.59, base loss: 14284.69
[INFO 2017-06-28 12:58:04,362 main.py:51] epoch 1038, training loss: 9958.41, average training loss: 10941.76, base loss: 14286.31
[INFO 2017-06-28 12:58:05,032 main.py:51] epoch 1039, training loss: 11979.12, average training loss: 10937.41, base loss: 14285.18
[INFO 2017-06-28 12:58:05,695 main.py:51] epoch 1040, training loss: 9590.76, average training loss: 10935.25, base loss: 14285.70
[INFO 2017-06-28 12:58:06,344 main.py:51] epoch 1041, training loss: 11574.91, average training loss: 10933.70, base loss: 14287.57
[INFO 2017-06-28 12:58:06,996 main.py:51] epoch 1042, training loss: 11312.07, average training loss: 10928.13, base loss: 14284.89
[INFO 2017-06-28 12:58:07,651 main.py:51] epoch 1043, training loss: 11714.23, average training loss: 10927.07, base loss: 14287.50
[INFO 2017-06-28 12:58:08,282 main.py:51] epoch 1044, training loss: 10010.54, average training loss: 10924.29, base loss: 14287.62
[INFO 2017-06-28 12:58:08,941 main.py:51] epoch 1045, training loss: 9964.78, average training loss: 10920.12, base loss: 14286.14
[INFO 2017-06-28 12:58:09,574 main.py:51] epoch 1046, training loss: 10668.66, average training loss: 10916.21, base loss: 14285.26
[INFO 2017-06-28 12:58:10,221 main.py:51] epoch 1047, training loss: 11683.29, average training loss: 10915.38, base loss: 14287.32
[INFO 2017-06-28 12:58:10,857 main.py:51] epoch 1048, training loss: 11064.35, average training loss: 10910.53, base loss: 14287.52
[INFO 2017-06-28 12:58:11,503 main.py:51] epoch 1049, training loss: 10101.13, average training loss: 10906.78, base loss: 14287.04
[INFO 2017-06-28 12:58:12,154 main.py:51] epoch 1050, training loss: 10835.57, average training loss: 10903.80, base loss: 14287.58
[INFO 2017-06-28 12:58:12,812 main.py:51] epoch 1051, training loss: 10337.23, average training loss: 10900.45, base loss: 14286.36
[INFO 2017-06-28 12:58:13,482 main.py:51] epoch 1052, training loss: 10354.84, average training loss: 10897.94, base loss: 14285.78
[INFO 2017-06-28 12:58:14,125 main.py:51] epoch 1053, training loss: 9547.63, average training loss: 10893.70, base loss: 14284.21
[INFO 2017-06-28 12:58:14,780 main.py:51] epoch 1054, training loss: 11263.45, average training loss: 10892.64, base loss: 14286.64
[INFO 2017-06-28 12:58:15,439 main.py:51] epoch 1055, training loss: 9095.97, average training loss: 10890.07, base loss: 14285.88
[INFO 2017-06-28 12:58:16,092 main.py:51] epoch 1056, training loss: 11955.78, average training loss: 10890.53, base loss: 14289.29
[INFO 2017-06-28 12:58:16,768 main.py:51] epoch 1057, training loss: 9898.01, average training loss: 10886.72, base loss: 14288.35
[INFO 2017-06-28 12:58:17,413 main.py:51] epoch 1058, training loss: 10019.19, average training loss: 10882.13, base loss: 14286.58
[INFO 2017-06-28 12:58:18,071 main.py:51] epoch 1059, training loss: 9615.52, average training loss: 10878.57, base loss: 14285.28
[INFO 2017-06-28 12:58:18,729 main.py:51] epoch 1060, training loss: 10123.80, average training loss: 10876.30, base loss: 14286.04
[INFO 2017-06-28 12:58:19,361 main.py:51] epoch 1061, training loss: 11465.43, average training loss: 10875.19, base loss: 14287.70
[INFO 2017-06-28 12:58:20,016 main.py:51] epoch 1062, training loss: 9815.32, average training loss: 10870.99, base loss: 14285.80
[INFO 2017-06-28 12:58:20,680 main.py:51] epoch 1063, training loss: 11373.62, average training loss: 10868.22, base loss: 14285.30
[INFO 2017-06-28 12:58:21,350 main.py:51] epoch 1064, training loss: 10435.81, average training loss: 10866.25, base loss: 14285.69
[INFO 2017-06-28 12:58:22,032 main.py:51] epoch 1065, training loss: 10256.23, average training loss: 10863.98, base loss: 14286.39
[INFO 2017-06-28 12:58:22,687 main.py:51] epoch 1066, training loss: 10473.04, average training loss: 10861.77, base loss: 14287.04
[INFO 2017-06-28 12:58:23,327 main.py:51] epoch 1067, training loss: 9990.06, average training loss: 10859.90, base loss: 14288.52
[INFO 2017-06-28 12:58:23,975 main.py:51] epoch 1068, training loss: 8818.73, average training loss: 10855.53, base loss: 14285.67
[INFO 2017-06-28 12:58:24,654 main.py:51] epoch 1069, training loss: 9305.92, average training loss: 10851.04, base loss: 14283.92
[INFO 2017-06-28 12:58:25,312 main.py:51] epoch 1070, training loss: 9911.06, average training loss: 10849.47, base loss: 14285.03
[INFO 2017-06-28 12:58:25,955 main.py:51] epoch 1071, training loss: 9621.21, average training loss: 10847.30, base loss: 14284.69
[INFO 2017-06-28 12:58:26,618 main.py:51] epoch 1072, training loss: 9910.60, average training loss: 10843.85, base loss: 14283.73
[INFO 2017-06-28 12:58:27,284 main.py:51] epoch 1073, training loss: 9052.91, average training loss: 10841.28, base loss: 14281.82
[INFO 2017-06-28 12:58:27,947 main.py:51] epoch 1074, training loss: 9696.79, average training loss: 10839.52, base loss: 14283.40
[INFO 2017-06-28 12:58:28,614 main.py:51] epoch 1075, training loss: 10671.81, average training loss: 10839.69, base loss: 14287.85
[INFO 2017-06-28 12:58:29,256 main.py:51] epoch 1076, training loss: 10011.78, average training loss: 10838.36, base loss: 14289.34
[INFO 2017-06-28 12:58:29,890 main.py:51] epoch 1077, training loss: 9916.00, average training loss: 10835.47, base loss: 14289.17
[INFO 2017-06-28 12:58:30,550 main.py:51] epoch 1078, training loss: 10154.94, average training loss: 10832.56, base loss: 14288.20
[INFO 2017-06-28 12:58:31,216 main.py:51] epoch 1079, training loss: 9547.86, average training loss: 10828.40, base loss: 14287.11
[INFO 2017-06-28 12:58:31,875 main.py:51] epoch 1080, training loss: 10434.59, average training loss: 10824.68, base loss: 14286.47
[INFO 2017-06-28 12:58:32,497 main.py:51] epoch 1081, training loss: 10673.46, average training loss: 10822.26, base loss: 14287.38
[INFO 2017-06-28 12:58:33,189 main.py:51] epoch 1082, training loss: 11918.10, average training loss: 10821.04, base loss: 14287.45
[INFO 2017-06-28 12:58:33,865 main.py:51] epoch 1083, training loss: 10208.90, average training loss: 10818.38, base loss: 14287.09
[INFO 2017-06-28 12:58:34,543 main.py:51] epoch 1084, training loss: 10131.49, average training loss: 10816.31, base loss: 14287.00
[INFO 2017-06-28 12:58:35,179 main.py:51] epoch 1085, training loss: 9052.65, average training loss: 10810.60, base loss: 14282.74
[INFO 2017-06-28 12:58:35,847 main.py:51] epoch 1086, training loss: 9330.62, average training loss: 10806.42, base loss: 14280.90
[INFO 2017-06-28 12:58:36,491 main.py:51] epoch 1087, training loss: 10727.33, average training loss: 10805.06, base loss: 14282.33
[INFO 2017-06-28 12:58:37,147 main.py:51] epoch 1088, training loss: 8359.06, average training loss: 10801.76, base loss: 14280.63
[INFO 2017-06-28 12:58:37,798 main.py:51] epoch 1089, training loss: 12180.21, average training loss: 10802.31, base loss: 14283.43
[INFO 2017-06-28 12:58:38,444 main.py:51] epoch 1090, training loss: 10385.93, average training loss: 10800.14, base loss: 14282.87
[INFO 2017-06-28 12:58:39,086 main.py:51] epoch 1091, training loss: 10565.71, average training loss: 10797.14, base loss: 14281.63
[INFO 2017-06-28 12:58:39,732 main.py:51] epoch 1092, training loss: 11234.80, average training loss: 10798.41, base loss: 14286.89
[INFO 2017-06-28 12:58:40,387 main.py:51] epoch 1093, training loss: 10739.35, average training loss: 10797.33, base loss: 14287.64
[INFO 2017-06-28 12:58:41,031 main.py:51] epoch 1094, training loss: 12276.39, average training loss: 10795.96, base loss: 14290.21
[INFO 2017-06-28 12:58:41,696 main.py:51] epoch 1095, training loss: 9822.50, average training loss: 10794.17, base loss: 14290.18
[INFO 2017-06-28 12:58:42,365 main.py:51] epoch 1096, training loss: 10305.76, average training loss: 10791.90, base loss: 14290.33
[INFO 2017-06-28 12:58:43,014 main.py:51] epoch 1097, training loss: 9219.47, average training loss: 10789.93, base loss: 14290.14
[INFO 2017-06-28 12:58:43,669 main.py:51] epoch 1098, training loss: 10389.55, average training loss: 10786.87, base loss: 14289.03
[INFO 2017-06-28 12:58:44,317 main.py:51] epoch 1099, training loss: 9417.97, average training loss: 10784.24, base loss: 14287.52
[INFO 2017-06-28 12:58:44,317 main.py:53] epoch 1099, testing
[INFO 2017-06-28 12:58:46,905 main.py:105] average testing loss: 11795.99, base loss: 15472.00
[INFO 2017-06-28 12:58:46,906 main.py:106] improve_loss: 3676.01, improve_percent: 0.24
[INFO 2017-06-28 12:58:46,906 main.py:76] current best improved percent: 0.25
[INFO 2017-06-28 12:58:47,560 main.py:51] epoch 1100, training loss: 9293.25, average training loss: 10781.90, base loss: 14286.96
[INFO 2017-06-28 12:58:48,231 main.py:51] epoch 1101, training loss: 9882.19, average training loss: 10780.61, base loss: 14287.83
[INFO 2017-06-28 12:58:48,885 main.py:51] epoch 1102, training loss: 11155.97, average training loss: 10780.09, base loss: 14290.81
[INFO 2017-06-28 12:58:49,543 main.py:51] epoch 1103, training loss: 12346.07, average training loss: 10779.42, base loss: 14292.66
[INFO 2017-06-28 12:58:50,178 main.py:51] epoch 1104, training loss: 10679.56, average training loss: 10775.99, base loss: 14291.07
[INFO 2017-06-28 12:58:50,854 main.py:51] epoch 1105, training loss: 9842.06, average training loss: 10772.81, base loss: 14288.61
[INFO 2017-06-28 12:58:51,507 main.py:51] epoch 1106, training loss: 9314.77, average training loss: 10770.11, base loss: 14286.96
[INFO 2017-06-28 12:58:52,177 main.py:51] epoch 1107, training loss: 9650.04, average training loss: 10766.62, base loss: 14284.17
[INFO 2017-06-28 12:58:52,825 main.py:51] epoch 1108, training loss: 11580.01, average training loss: 10764.26, base loss: 14284.61
[INFO 2017-06-28 12:58:53,465 main.py:51] epoch 1109, training loss: 8809.86, average training loss: 10759.52, base loss: 14280.58
[INFO 2017-06-28 12:58:54,131 main.py:51] epoch 1110, training loss: 10137.58, average training loss: 10757.32, base loss: 14279.56
[INFO 2017-06-28 12:58:54,776 main.py:51] epoch 1111, training loss: 10711.51, average training loss: 10756.17, base loss: 14280.36
[INFO 2017-06-28 12:58:55,448 main.py:51] epoch 1112, training loss: 9875.42, average training loss: 10754.14, base loss: 14279.86
[INFO 2017-06-28 12:58:56,120 main.py:51] epoch 1113, training loss: 10517.31, average training loss: 10754.48, base loss: 14282.40
[INFO 2017-06-28 12:58:56,764 main.py:51] epoch 1114, training loss: 10477.68, average training loss: 10753.23, base loss: 14284.31
[INFO 2017-06-28 12:58:57,418 main.py:51] epoch 1115, training loss: 11415.82, average training loss: 10752.23, base loss: 14285.50
[INFO 2017-06-28 12:58:58,071 main.py:51] epoch 1116, training loss: 9910.42, average training loss: 10751.51, base loss: 14287.12
[INFO 2017-06-28 12:58:58,716 main.py:51] epoch 1117, training loss: 10128.20, average training loss: 10750.43, base loss: 14288.07
[INFO 2017-06-28 12:58:59,366 main.py:51] epoch 1118, training loss: 9350.99, average training loss: 10746.91, base loss: 14285.45
[INFO 2017-06-28 12:59:00,011 main.py:51] epoch 1119, training loss: 9472.09, average training loss: 10744.39, base loss: 14284.16
[INFO 2017-06-28 12:59:00,668 main.py:51] epoch 1120, training loss: 12767.31, average training loss: 10746.13, base loss: 14289.90
[INFO 2017-06-28 12:59:01,324 main.py:51] epoch 1121, training loss: 10677.51, average training loss: 10744.13, base loss: 14290.53
[INFO 2017-06-28 12:59:01,993 main.py:51] epoch 1122, training loss: 8569.95, average training loss: 10740.96, base loss: 14289.30
[INFO 2017-06-28 12:59:02,664 main.py:51] epoch 1123, training loss: 10364.84, average training loss: 10737.85, base loss: 14288.55
[INFO 2017-06-28 12:59:03,311 main.py:51] epoch 1124, training loss: 10755.72, average training loss: 10737.29, base loss: 14289.55
[INFO 2017-06-28 12:59:03,951 main.py:51] epoch 1125, training loss: 10337.53, average training loss: 10735.02, base loss: 14288.91
[INFO 2017-06-28 12:59:04,606 main.py:51] epoch 1126, training loss: 9786.42, average training loss: 10731.26, base loss: 14286.23
[INFO 2017-06-28 12:59:05,279 main.py:51] epoch 1127, training loss: 10816.01, average training loss: 10729.35, base loss: 14285.52
[INFO 2017-06-28 12:59:06,012 main.py:51] epoch 1128, training loss: 9553.26, average training loss: 10726.62, base loss: 14283.83
[INFO 2017-06-28 12:59:06,679 main.py:51] epoch 1129, training loss: 9743.75, average training loss: 10724.36, base loss: 14283.28
[INFO 2017-06-28 12:59:07,332 main.py:51] epoch 1130, training loss: 10065.07, average training loss: 10722.73, base loss: 14283.39
[INFO 2017-06-28 12:59:07,990 main.py:51] epoch 1131, training loss: 9830.68, average training loss: 10721.22, base loss: 14283.36
[INFO 2017-06-28 12:59:08,632 main.py:51] epoch 1132, training loss: 9921.38, average training loss: 10720.90, base loss: 14284.44
[INFO 2017-06-28 12:59:09,291 main.py:51] epoch 1133, training loss: 10061.36, average training loss: 10716.20, base loss: 14281.49
[INFO 2017-06-28 12:59:09,956 main.py:51] epoch 1134, training loss: 9544.76, average training loss: 10715.09, base loss: 14281.77
[INFO 2017-06-28 12:59:10,607 main.py:51] epoch 1135, training loss: 10409.07, average training loss: 10714.86, base loss: 14282.29
[INFO 2017-06-28 12:59:11,259 main.py:51] epoch 1136, training loss: 9089.88, average training loss: 10710.86, base loss: 14278.09
[INFO 2017-06-28 12:59:11,906 main.py:51] epoch 1137, training loss: 9780.51, average training loss: 10709.25, base loss: 14278.18
[INFO 2017-06-28 12:59:12,558 main.py:51] epoch 1138, training loss: 9599.08, average training loss: 10707.33, base loss: 14277.40
[INFO 2017-06-28 12:59:13,243 main.py:51] epoch 1139, training loss: 11525.41, average training loss: 10707.06, base loss: 14278.11
[INFO 2017-06-28 12:59:13,918 main.py:51] epoch 1140, training loss: 11680.25, average training loss: 10707.35, base loss: 14280.64
[INFO 2017-06-28 12:59:14,581 main.py:51] epoch 1141, training loss: 8792.56, average training loss: 10704.45, base loss: 14278.25
[INFO 2017-06-28 12:59:15,236 main.py:51] epoch 1142, training loss: 10385.85, average training loss: 10702.72, base loss: 14279.03
[INFO 2017-06-28 12:59:15,895 main.py:51] epoch 1143, training loss: 8127.20, average training loss: 10699.02, base loss: 14275.46
[INFO 2017-06-28 12:59:16,560 main.py:51] epoch 1144, training loss: 9578.84, average training loss: 10697.83, base loss: 14274.59
[INFO 2017-06-28 12:59:17,197 main.py:51] epoch 1145, training loss: 13106.93, average training loss: 10698.62, base loss: 14278.49
[INFO 2017-06-28 12:59:17,855 main.py:51] epoch 1146, training loss: 9722.47, average training loss: 10695.62, base loss: 14276.48
[INFO 2017-06-28 12:59:18,518 main.py:51] epoch 1147, training loss: 10507.29, average training loss: 10694.14, base loss: 14275.70
[INFO 2017-06-28 12:59:19,170 main.py:51] epoch 1148, training loss: 10032.21, average training loss: 10692.92, base loss: 14275.78
[INFO 2017-06-28 12:59:19,836 main.py:51] epoch 1149, training loss: 9193.73, average training loss: 10689.84, base loss: 14273.38
[INFO 2017-06-28 12:59:20,500 main.py:51] epoch 1150, training loss: 8969.77, average training loss: 10686.25, base loss: 14270.14
[INFO 2017-06-28 12:59:21,153 main.py:51] epoch 1151, training loss: 10034.61, average training loss: 10683.80, base loss: 14269.15
[INFO 2017-06-28 12:59:21,816 main.py:51] epoch 1152, training loss: 9243.43, average training loss: 10680.97, base loss: 14267.50
[INFO 2017-06-28 12:59:22,487 main.py:51] epoch 1153, training loss: 10919.16, average training loss: 10679.47, base loss: 14268.23
[INFO 2017-06-28 12:59:23,148 main.py:51] epoch 1154, training loss: 11454.97, average training loss: 10678.71, base loss: 14268.30
[INFO 2017-06-28 12:59:23,807 main.py:51] epoch 1155, training loss: 10957.50, average training loss: 10677.35, base loss: 14267.99
[INFO 2017-06-28 12:59:24,476 main.py:51] epoch 1156, training loss: 9357.93, average training loss: 10674.09, base loss: 14265.51
[INFO 2017-06-28 12:59:25,107 main.py:51] epoch 1157, training loss: 9132.75, average training loss: 10671.18, base loss: 14264.53
[INFO 2017-06-28 12:59:25,741 main.py:51] epoch 1158, training loss: 12941.61, average training loss: 10673.35, base loss: 14269.30
[INFO 2017-06-28 12:59:26,367 main.py:51] epoch 1159, training loss: 11328.62, average training loss: 10674.23, base loss: 14272.97
[INFO 2017-06-28 12:59:27,023 main.py:51] epoch 1160, training loss: 11164.23, average training loss: 10673.39, base loss: 14274.21
[INFO 2017-06-28 12:59:27,678 main.py:51] epoch 1161, training loss: 8793.96, average training loss: 10672.35, base loss: 14273.81
[INFO 2017-06-28 12:59:28,328 main.py:51] epoch 1162, training loss: 9713.73, average training loss: 10669.46, base loss: 14273.05
[INFO 2017-06-28 12:59:28,980 main.py:51] epoch 1163, training loss: 10385.47, average training loss: 10669.01, base loss: 14274.47
[INFO 2017-06-28 12:59:29,655 main.py:51] epoch 1164, training loss: 9597.73, average training loss: 10666.40, base loss: 14273.42
[INFO 2017-06-28 12:59:30,315 main.py:51] epoch 1165, training loss: 9963.38, average training loss: 10665.59, base loss: 14274.82
[INFO 2017-06-28 12:59:30,959 main.py:51] epoch 1166, training loss: 9548.01, average training loss: 10664.10, base loss: 14275.32
[INFO 2017-06-28 12:59:31,618 main.py:51] epoch 1167, training loss: 10739.60, average training loss: 10664.25, base loss: 14277.18
[INFO 2017-06-28 12:59:32,281 main.py:51] epoch 1168, training loss: 9907.96, average training loss: 10662.87, base loss: 14277.03
[INFO 2017-06-28 12:59:32,924 main.py:51] epoch 1169, training loss: 9166.30, average training loss: 10661.08, base loss: 14276.14
[INFO 2017-06-28 12:59:33,567 main.py:51] epoch 1170, training loss: 10193.78, average training loss: 10659.05, base loss: 14276.17
[INFO 2017-06-28 12:59:34,227 main.py:51] epoch 1171, training loss: 12351.65, average training loss: 10660.79, base loss: 14279.93
[INFO 2017-06-28 12:59:34,888 main.py:51] epoch 1172, training loss: 9794.13, average training loss: 10657.68, base loss: 14277.44
[INFO 2017-06-28 12:59:35,532 main.py:51] epoch 1173, training loss: 10073.89, average training loss: 10655.29, base loss: 14276.63
[INFO 2017-06-28 12:59:36,173 main.py:51] epoch 1174, training loss: 8992.00, average training loss: 10653.09, base loss: 14274.82
[INFO 2017-06-28 12:59:36,836 main.py:51] epoch 1175, training loss: 8841.07, average training loss: 10650.55, base loss: 14272.85
[INFO 2017-06-28 12:59:37,519 main.py:51] epoch 1176, training loss: 8137.75, average training loss: 10647.25, base loss: 14271.05
[INFO 2017-06-28 12:59:38,201 main.py:51] epoch 1177, training loss: 12466.33, average training loss: 10649.65, base loss: 14274.97
[INFO 2017-06-28 12:59:38,843 main.py:51] epoch 1178, training loss: 9918.80, average training loss: 10647.15, base loss: 14273.46
[INFO 2017-06-28 12:59:39,485 main.py:51] epoch 1179, training loss: 9696.38, average training loss: 10646.80, base loss: 14274.60
[INFO 2017-06-28 12:59:40,149 main.py:51] epoch 1180, training loss: 10978.04, average training loss: 10647.62, base loss: 14276.91
[INFO 2017-06-28 12:59:40,800 main.py:51] epoch 1181, training loss: 10833.42, average training loss: 10645.67, base loss: 14276.23
[INFO 2017-06-28 12:59:41,439 main.py:51] epoch 1182, training loss: 10397.75, average training loss: 10642.65, base loss: 14274.30
[INFO 2017-06-28 12:59:42,089 main.py:51] epoch 1183, training loss: 12283.08, average training loss: 10643.33, base loss: 14277.06
[INFO 2017-06-28 12:59:42,731 main.py:51] epoch 1184, training loss: 10513.75, average training loss: 10642.40, base loss: 14276.34
[INFO 2017-06-28 12:59:43,391 main.py:51] epoch 1185, training loss: 9115.66, average training loss: 10640.62, base loss: 14277.61
[INFO 2017-06-28 12:59:44,063 main.py:51] epoch 1186, training loss: 10582.08, average training loss: 10639.35, base loss: 14277.05
[INFO 2017-06-28 12:59:44,727 main.py:51] epoch 1187, training loss: 9756.27, average training loss: 10636.61, base loss: 14274.12
[INFO 2017-06-28 12:59:45,377 main.py:51] epoch 1188, training loss: 9821.25, average training loss: 10636.47, base loss: 14275.42
[INFO 2017-06-28 12:59:46,032 main.py:51] epoch 1189, training loss: 9971.98, average training loss: 10635.94, base loss: 14277.76
[INFO 2017-06-28 12:59:46,676 main.py:51] epoch 1190, training loss: 12070.97, average training loss: 10636.41, base loss: 14280.01
[INFO 2017-06-28 12:59:47,329 main.py:51] epoch 1191, training loss: 9550.06, average training loss: 10635.12, base loss: 14280.11
[INFO 2017-06-28 12:59:47,982 main.py:51] epoch 1192, training loss: 10751.75, average training loss: 10635.73, base loss: 14281.82
[INFO 2017-06-28 12:59:48,630 main.py:51] epoch 1193, training loss: 10530.92, average training loss: 10633.79, base loss: 14281.28
[INFO 2017-06-28 12:59:49,294 main.py:51] epoch 1194, training loss: 12066.31, average training loss: 10634.41, base loss: 14283.55
[INFO 2017-06-28 12:59:49,957 main.py:51] epoch 1195, training loss: 11108.40, average training loss: 10634.65, base loss: 14285.22
[INFO 2017-06-28 12:59:50,595 main.py:51] epoch 1196, training loss: 9019.22, average training loss: 10633.06, base loss: 14285.75
[INFO 2017-06-28 12:59:51,273 main.py:51] epoch 1197, training loss: 9229.71, average training loss: 10630.21, base loss: 14283.54
[INFO 2017-06-28 12:59:51,936 main.py:51] epoch 1198, training loss: 10696.10, average training loss: 10630.37, base loss: 14286.39
[INFO 2017-06-28 12:59:52,584 main.py:51] epoch 1199, training loss: 8808.11, average training loss: 10628.01, base loss: 14285.14
[INFO 2017-06-28 12:59:52,584 main.py:53] epoch 1199, testing
[INFO 2017-06-28 12:59:55,207 main.py:105] average testing loss: 10794.89, base loss: 14625.19
[INFO 2017-06-28 12:59:55,207 main.py:106] improve_loss: 3830.29, improve_percent: 0.26
[INFO 2017-06-28 12:59:55,208 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 12:59:55,244 main.py:76] current best improved percent: 0.26
[INFO 2017-06-28 12:59:55,909 main.py:51] epoch 1200, training loss: 10706.67, average training loss: 10627.00, base loss: 14286.24
[INFO 2017-06-28 12:59:56,579 main.py:51] epoch 1201, training loss: 10250.57, average training loss: 10626.48, base loss: 14287.97
[INFO 2017-06-28 12:59:57,233 main.py:51] epoch 1202, training loss: 9860.78, average training loss: 10626.95, base loss: 14288.69
[INFO 2017-06-28 12:59:57,874 main.py:51] epoch 1203, training loss: 8976.07, average training loss: 10623.99, base loss: 14287.42
[INFO 2017-06-28 12:59:58,524 main.py:51] epoch 1204, training loss: 11054.77, average training loss: 10624.47, base loss: 14289.39
[INFO 2017-06-28 12:59:59,210 main.py:51] epoch 1205, training loss: 11148.38, average training loss: 10625.49, base loss: 14293.74
[INFO 2017-06-28 12:59:59,853 main.py:51] epoch 1206, training loss: 10555.07, average training loss: 10624.43, base loss: 14293.60
[INFO 2017-06-28 13:00:00,532 main.py:51] epoch 1207, training loss: 9007.87, average training loss: 10620.17, base loss: 14288.28
[INFO 2017-06-28 13:00:01,184 main.py:51] epoch 1208, training loss: 9525.34, average training loss: 10617.79, base loss: 14286.40
[INFO 2017-06-28 13:00:01,817 main.py:51] epoch 1209, training loss: 12005.29, average training loss: 10617.46, base loss: 14289.05
[INFO 2017-06-28 13:00:02,457 main.py:51] epoch 1210, training loss: 10358.40, average training loss: 10615.28, base loss: 14289.53
[INFO 2017-06-28 13:00:03,101 main.py:51] epoch 1211, training loss: 10304.67, average training loss: 10614.15, base loss: 14289.76
[INFO 2017-06-28 13:00:03,780 main.py:51] epoch 1212, training loss: 10703.92, average training loss: 10614.56, base loss: 14292.20
[INFO 2017-06-28 13:00:04,433 main.py:51] epoch 1213, training loss: 9632.26, average training loss: 10613.58, base loss: 14293.06
[INFO 2017-06-28 13:00:05,098 main.py:51] epoch 1214, training loss: 9177.84, average training loss: 10611.10, base loss: 14291.53
[INFO 2017-06-28 13:00:05,762 main.py:51] epoch 1215, training loss: 9277.78, average training loss: 10609.31, base loss: 14291.61
[INFO 2017-06-28 13:00:06,422 main.py:51] epoch 1216, training loss: 9630.97, average training loss: 10608.26, base loss: 14291.64
[INFO 2017-06-28 13:00:07,087 main.py:51] epoch 1217, training loss: 9536.22, average training loss: 10606.54, base loss: 14291.28
[INFO 2017-06-28 13:00:07,754 main.py:51] epoch 1218, training loss: 10938.04, average training loss: 10605.96, base loss: 14291.70
[INFO 2017-06-28 13:00:08,397 main.py:51] epoch 1219, training loss: 9533.65, average training loss: 10603.57, base loss: 14287.97
[INFO 2017-06-28 13:00:09,045 main.py:51] epoch 1220, training loss: 11487.71, average training loss: 10604.14, base loss: 14291.48
[INFO 2017-06-28 13:00:09,682 main.py:51] epoch 1221, training loss: 11050.00, average training loss: 10603.56, base loss: 14291.51
[INFO 2017-06-28 13:00:10,335 main.py:51] epoch 1222, training loss: 11851.20, average training loss: 10605.35, base loss: 14295.98
[INFO 2017-06-28 13:00:10,977 main.py:51] epoch 1223, training loss: 9244.69, average training loss: 10603.64, base loss: 14295.75
[INFO 2017-06-28 13:00:11,624 main.py:51] epoch 1224, training loss: 10524.68, average training loss: 10602.05, base loss: 14295.44
[INFO 2017-06-28 13:00:12,286 main.py:51] epoch 1225, training loss: 9779.97, average training loss: 10601.37, base loss: 14296.11
[INFO 2017-06-28 13:00:12,947 main.py:51] epoch 1226, training loss: 9283.17, average training loss: 10598.58, base loss: 14294.25
[INFO 2017-06-28 13:00:13,607 main.py:51] epoch 1227, training loss: 9328.31, average training loss: 10597.67, base loss: 14294.55
[INFO 2017-06-28 13:00:14,266 main.py:51] epoch 1228, training loss: 9503.17, average training loss: 10597.84, base loss: 14297.72
[INFO 2017-06-28 13:00:14,934 main.py:51] epoch 1229, training loss: 9986.21, average training loss: 10596.50, base loss: 14297.51
[INFO 2017-06-28 13:00:15,590 main.py:51] epoch 1230, training loss: 8967.21, average training loss: 10594.77, base loss: 14295.63
[INFO 2017-06-28 13:00:16,233 main.py:51] epoch 1231, training loss: 9734.95, average training loss: 10592.92, base loss: 14294.73
[INFO 2017-06-28 13:00:16,889 main.py:51] epoch 1232, training loss: 10455.16, average training loss: 10592.59, base loss: 14295.01
[INFO 2017-06-28 13:00:17,557 main.py:51] epoch 1233, training loss: 10473.20, average training loss: 10590.93, base loss: 14294.16
[INFO 2017-06-28 13:00:18,207 main.py:51] epoch 1234, training loss: 8990.99, average training loss: 10587.42, base loss: 14290.33
[INFO 2017-06-28 13:00:18,882 main.py:51] epoch 1235, training loss: 9877.78, average training loss: 10584.06, base loss: 14289.07
[INFO 2017-06-28 13:00:19,531 main.py:51] epoch 1236, training loss: 9591.85, average training loss: 10582.22, base loss: 14286.90
[INFO 2017-06-28 13:00:20,197 main.py:51] epoch 1237, training loss: 11473.22, average training loss: 10582.30, base loss: 14289.51
[INFO 2017-06-28 13:00:20,854 main.py:51] epoch 1238, training loss: 13339.11, average training loss: 10584.98, base loss: 14294.54
[INFO 2017-06-28 13:00:21,535 main.py:51] epoch 1239, training loss: 10018.72, average training loss: 10585.51, base loss: 14297.36
[INFO 2017-06-28 13:00:22,174 main.py:51] epoch 1240, training loss: 9393.49, average training loss: 10583.60, base loss: 14295.48
[INFO 2017-06-28 13:00:22,822 main.py:51] epoch 1241, training loss: 11468.31, average training loss: 10582.55, base loss: 14296.60
[INFO 2017-06-28 13:00:23,490 main.py:51] epoch 1242, training loss: 11405.52, average training loss: 10582.79, base loss: 14297.79
[INFO 2017-06-28 13:00:24,135 main.py:51] epoch 1243, training loss: 10726.08, average training loss: 10581.60, base loss: 14298.66
[INFO 2017-06-28 13:00:24,790 main.py:51] epoch 1244, training loss: 10423.85, average training loss: 10580.23, base loss: 14299.71
[INFO 2017-06-28 13:00:25,435 main.py:51] epoch 1245, training loss: 10183.01, average training loss: 10579.16, base loss: 14299.41
[INFO 2017-06-28 13:00:26,087 main.py:51] epoch 1246, training loss: 9622.66, average training loss: 10578.53, base loss: 14299.77
[INFO 2017-06-28 13:00:26,755 main.py:51] epoch 1247, training loss: 9572.89, average training loss: 10577.78, base loss: 14301.79
[INFO 2017-06-28 13:00:27,422 main.py:51] epoch 1248, training loss: 10836.01, average training loss: 10578.52, base loss: 14305.05
[INFO 2017-06-28 13:00:28,074 main.py:51] epoch 1249, training loss: 9121.98, average training loss: 10576.64, base loss: 14305.32
[INFO 2017-06-28 13:00:28,733 main.py:51] epoch 1250, training loss: 10444.29, average training loss: 10574.43, base loss: 14303.84
[INFO 2017-06-28 13:00:29,403 main.py:51] epoch 1251, training loss: 9734.54, average training loss: 10572.27, base loss: 14302.81
[INFO 2017-06-28 13:00:30,057 main.py:51] epoch 1252, training loss: 10856.05, average training loss: 10569.74, base loss: 14300.77
[INFO 2017-06-28 13:00:30,710 main.py:51] epoch 1253, training loss: 9798.13, average training loss: 10568.18, base loss: 14300.25
[INFO 2017-06-28 13:00:31,373 main.py:51] epoch 1254, training loss: 9937.09, average training loss: 10567.30, base loss: 14300.42
[INFO 2017-06-28 13:00:32,058 main.py:51] epoch 1255, training loss: 10106.09, average training loss: 10563.49, base loss: 14297.53
[INFO 2017-06-28 13:00:32,698 main.py:51] epoch 1256, training loss: 11176.39, average training loss: 10563.07, base loss: 14299.08
[INFO 2017-06-28 13:00:33,339 main.py:51] epoch 1257, training loss: 10058.32, average training loss: 10561.57, base loss: 14300.31
[INFO 2017-06-28 13:00:33,978 main.py:51] epoch 1258, training loss: 9967.42, average training loss: 10560.02, base loss: 14300.29
[INFO 2017-06-28 13:00:34,635 main.py:51] epoch 1259, training loss: 11082.24, average training loss: 10560.47, base loss: 14302.84
[INFO 2017-06-28 13:00:35,293 main.py:51] epoch 1260, training loss: 9647.67, average training loss: 10558.72, base loss: 14302.02
[INFO 2017-06-28 13:00:35,950 main.py:51] epoch 1261, training loss: 9153.03, average training loss: 10557.10, base loss: 14301.56
[INFO 2017-06-28 13:00:36,594 main.py:51] epoch 1262, training loss: 10045.98, average training loss: 10556.57, base loss: 14302.81
[INFO 2017-06-28 13:00:37,246 main.py:51] epoch 1263, training loss: 9790.66, average training loss: 10554.57, base loss: 14302.37
[INFO 2017-06-28 13:00:37,918 main.py:51] epoch 1264, training loss: 9691.76, average training loss: 10553.23, base loss: 14301.44
[INFO 2017-06-28 13:00:38,599 main.py:51] epoch 1265, training loss: 9709.61, average training loss: 10552.01, base loss: 14300.82
[INFO 2017-06-28 13:00:39,296 main.py:51] epoch 1266, training loss: 9936.25, average training loss: 10552.93, base loss: 14304.91
[INFO 2017-06-28 13:00:39,954 main.py:51] epoch 1267, training loss: 10039.67, average training loss: 10551.17, base loss: 14303.29
[INFO 2017-06-28 13:00:40,637 main.py:51] epoch 1268, training loss: 10201.60, average training loss: 10551.23, base loss: 14306.45
[INFO 2017-06-28 13:00:41,279 main.py:51] epoch 1269, training loss: 10375.10, average training loss: 10547.28, base loss: 14303.09
[INFO 2017-06-28 13:00:41,932 main.py:51] epoch 1270, training loss: 10476.01, average training loss: 10546.74, base loss: 14303.29
[INFO 2017-06-28 13:00:42,611 main.py:51] epoch 1271, training loss: 10777.17, average training loss: 10544.74, base loss: 14301.23
[INFO 2017-06-28 13:00:43,263 main.py:51] epoch 1272, training loss: 9988.78, average training loss: 10544.60, base loss: 14302.49
[INFO 2017-06-28 13:00:43,902 main.py:51] epoch 1273, training loss: 9845.15, average training loss: 10541.83, base loss: 14299.26
[INFO 2017-06-28 13:00:44,539 main.py:51] epoch 1274, training loss: 9550.71, average training loss: 10541.17, base loss: 14299.72
[INFO 2017-06-28 13:00:45,196 main.py:51] epoch 1275, training loss: 12620.90, average training loss: 10543.06, base loss: 14303.76
[INFO 2017-06-28 13:00:45,829 main.py:51] epoch 1276, training loss: 9438.54, average training loss: 10543.02, base loss: 14303.91
[INFO 2017-06-28 13:00:46,473 main.py:51] epoch 1277, training loss: 9562.92, average training loss: 10542.91, base loss: 14305.20
[INFO 2017-06-28 13:00:47,143 main.py:51] epoch 1278, training loss: 9807.57, average training loss: 10542.26, base loss: 14305.44
[INFO 2017-06-28 13:00:47,818 main.py:51] epoch 1279, training loss: 9696.60, average training loss: 10540.41, base loss: 14304.43
[INFO 2017-06-28 13:00:48,484 main.py:51] epoch 1280, training loss: 10475.61, average training loss: 10540.35, base loss: 14306.11
[INFO 2017-06-28 13:00:49,132 main.py:51] epoch 1281, training loss: 9941.06, average training loss: 10539.40, base loss: 14306.46
[INFO 2017-06-28 13:00:49,784 main.py:51] epoch 1282, training loss: 8375.04, average training loss: 10536.99, base loss: 14304.04
[INFO 2017-06-28 13:00:50,444 main.py:51] epoch 1283, training loss: 9594.00, average training loss: 10532.24, base loss: 14300.29
[INFO 2017-06-28 13:00:51,122 main.py:51] epoch 1284, training loss: 11222.48, average training loss: 10532.31, base loss: 14301.89
[INFO 2017-06-28 13:00:51,791 main.py:51] epoch 1285, training loss: 9361.39, average training loss: 10530.98, base loss: 14303.07
[INFO 2017-06-28 13:00:52,434 main.py:51] epoch 1286, training loss: 9883.01, average training loss: 10528.90, base loss: 14300.90
[INFO 2017-06-28 13:00:53,108 main.py:51] epoch 1287, training loss: 10248.69, average training loss: 10526.83, base loss: 14299.87
[INFO 2017-06-28 13:00:53,780 main.py:51] epoch 1288, training loss: 10263.38, average training loss: 10525.55, base loss: 14301.46
[INFO 2017-06-28 13:00:54,456 main.py:51] epoch 1289, training loss: 9660.47, average training loss: 10523.00, base loss: 14298.78
[INFO 2017-06-28 13:00:55,101 main.py:51] epoch 1290, training loss: 11068.02, average training loss: 10523.66, base loss: 14301.28
[INFO 2017-06-28 13:00:55,743 main.py:51] epoch 1291, training loss: 11105.36, average training loss: 10524.24, base loss: 14303.50
[INFO 2017-06-28 13:00:56,389 main.py:51] epoch 1292, training loss: 9873.22, average training loss: 10523.45, base loss: 14303.35
[INFO 2017-06-28 13:00:57,069 main.py:51] epoch 1293, training loss: 11511.11, average training loss: 10524.90, base loss: 14306.88
[INFO 2017-06-28 13:00:57,719 main.py:51] epoch 1294, training loss: 10700.97, average training loss: 10523.11, base loss: 14305.22
[INFO 2017-06-28 13:00:58,365 main.py:51] epoch 1295, training loss: 9420.03, average training loss: 10520.13, base loss: 14304.34
[INFO 2017-06-28 13:00:59,019 main.py:51] epoch 1296, training loss: 9936.01, average training loss: 10518.76, base loss: 14303.15
[INFO 2017-06-28 13:00:59,649 main.py:51] epoch 1297, training loss: 8492.34, average training loss: 10515.81, base loss: 14300.09
[INFO 2017-06-28 13:01:00,290 main.py:51] epoch 1298, training loss: 9505.23, average training loss: 10512.76, base loss: 14298.46
[INFO 2017-06-28 13:01:00,948 main.py:51] epoch 1299, training loss: 10476.42, average training loss: 10512.33, base loss: 14298.49
[INFO 2017-06-28 13:01:00,949 main.py:53] epoch 1299, testing
[INFO 2017-06-28 13:01:03,558 main.py:105] average testing loss: 11539.03, base loss: 15235.30
[INFO 2017-06-28 13:01:03,559 main.py:106] improve_loss: 3696.27, improve_percent: 0.24
[INFO 2017-06-28 13:01:03,559 main.py:76] current best improved percent: 0.26
[INFO 2017-06-28 13:01:04,218 main.py:51] epoch 1300, training loss: 11962.42, average training loss: 10514.24, base loss: 14302.61
[INFO 2017-06-28 13:01:04,862 main.py:51] epoch 1301, training loss: 11014.96, average training loss: 10513.59, base loss: 14303.34
[INFO 2017-06-28 13:01:05,507 main.py:51] epoch 1302, training loss: 9389.04, average training loss: 10512.53, base loss: 14304.18
[INFO 2017-06-28 13:01:06,155 main.py:51] epoch 1303, training loss: 9737.95, average training loss: 10511.24, base loss: 14303.57
[INFO 2017-06-28 13:01:06,790 main.py:51] epoch 1304, training loss: 12373.50, average training loss: 10510.38, base loss: 14304.69
[INFO 2017-06-28 13:01:07,423 main.py:51] epoch 1305, training loss: 9955.69, average training loss: 10508.06, base loss: 14304.07
[INFO 2017-06-28 13:01:08,096 main.py:51] epoch 1306, training loss: 9697.90, average training loss: 10507.25, base loss: 14304.80
[INFO 2017-06-28 13:01:08,765 main.py:51] epoch 1307, training loss: 10457.29, average training loss: 10507.30, base loss: 14305.21
[INFO 2017-06-28 13:01:09,438 main.py:51] epoch 1308, training loss: 12294.65, average training loss: 10509.90, base loss: 14310.17
[INFO 2017-06-28 13:01:10,094 main.py:51] epoch 1309, training loss: 9896.02, average training loss: 10507.62, base loss: 14309.71
[INFO 2017-06-28 13:01:10,766 main.py:51] epoch 1310, training loss: 10199.94, average training loss: 10506.02, base loss: 14307.79
[INFO 2017-06-28 13:01:11,411 main.py:51] epoch 1311, training loss: 10200.00, average training loss: 10505.47, base loss: 14307.96
[INFO 2017-06-28 13:01:12,089 main.py:51] epoch 1312, training loss: 9613.30, average training loss: 10502.09, base loss: 14304.94
[INFO 2017-06-28 13:01:12,753 main.py:51] epoch 1313, training loss: 9338.30, average training loss: 10499.37, base loss: 14302.26
[INFO 2017-06-28 13:01:13,390 main.py:51] epoch 1314, training loss: 10420.55, average training loss: 10496.38, base loss: 14299.04
[INFO 2017-06-28 13:01:14,070 main.py:51] epoch 1315, training loss: 10237.33, average training loss: 10496.23, base loss: 14301.83
[INFO 2017-06-28 13:01:14,745 main.py:51] epoch 1316, training loss: 8726.32, average training loss: 10494.72, base loss: 14301.18
[INFO 2017-06-28 13:01:15,391 main.py:51] epoch 1317, training loss: 9284.40, average training loss: 10492.89, base loss: 14300.68
[INFO 2017-06-28 13:01:16,041 main.py:51] epoch 1318, training loss: 9948.01, average training loss: 10491.93, base loss: 14300.63
[INFO 2017-06-28 13:01:16,708 main.py:51] epoch 1319, training loss: 9861.06, average training loss: 10490.59, base loss: 14300.97
[INFO 2017-06-28 13:01:17,351 main.py:51] epoch 1320, training loss: 10045.06, average training loss: 10488.90, base loss: 14300.75
[INFO 2017-06-28 13:01:18,015 main.py:51] epoch 1321, training loss: 10537.01, average training loss: 10485.55, base loss: 14296.76
[INFO 2017-06-28 13:01:18,679 main.py:51] epoch 1322, training loss: 9720.80, average training loss: 10482.45, base loss: 14293.74
[INFO 2017-06-28 13:01:19,342 main.py:51] epoch 1323, training loss: 9666.65, average training loss: 10481.91, base loss: 14294.82
[INFO 2017-06-28 13:01:20,010 main.py:51] epoch 1324, training loss: 10326.43, average training loss: 10480.82, base loss: 14294.88
[INFO 2017-06-28 13:01:20,669 main.py:51] epoch 1325, training loss: 9434.03, average training loss: 10480.40, base loss: 14295.58
[INFO 2017-06-28 13:01:21,324 main.py:51] epoch 1326, training loss: 10791.87, average training loss: 10481.87, base loss: 14298.06
[INFO 2017-06-28 13:01:21,982 main.py:51] epoch 1327, training loss: 11815.14, average training loss: 10482.07, base loss: 14299.68
[INFO 2017-06-28 13:01:22,625 main.py:51] epoch 1328, training loss: 10965.32, average training loss: 10482.34, base loss: 14299.62
[INFO 2017-06-28 13:01:23,284 main.py:51] epoch 1329, training loss: 9226.93, average training loss: 10482.39, base loss: 14301.50
[INFO 2017-06-28 13:01:23,940 main.py:51] epoch 1330, training loss: 8910.42, average training loss: 10480.42, base loss: 14301.21
[INFO 2017-06-28 13:01:24,587 main.py:51] epoch 1331, training loss: 11125.95, average training loss: 10480.52, base loss: 14301.73
[INFO 2017-06-28 13:01:25,221 main.py:51] epoch 1332, training loss: 10855.11, average training loss: 10478.48, base loss: 14301.19
[INFO 2017-06-28 13:01:25,866 main.py:51] epoch 1333, training loss: 10802.20, average training loss: 10477.26, base loss: 14300.89
[INFO 2017-06-28 13:01:26,528 main.py:51] epoch 1334, training loss: 10602.02, average training loss: 10475.86, base loss: 14300.36
[INFO 2017-06-28 13:01:27,173 main.py:51] epoch 1335, training loss: 8421.53, average training loss: 10474.01, base loss: 14299.33
[INFO 2017-06-28 13:01:27,834 main.py:51] epoch 1336, training loss: 11783.05, average training loss: 10474.35, base loss: 14301.65
[INFO 2017-06-28 13:01:28,520 main.py:51] epoch 1337, training loss: 10240.00, average training loss: 10474.32, base loss: 14302.60
[INFO 2017-06-28 13:01:29,165 main.py:51] epoch 1338, training loss: 10705.25, average training loss: 10474.30, base loss: 14304.07
[INFO 2017-06-28 13:01:29,818 main.py:51] epoch 1339, training loss: 10110.50, average training loss: 10472.26, base loss: 14302.30
[INFO 2017-06-28 13:01:30,468 main.py:51] epoch 1340, training loss: 9341.37, average training loss: 10470.60, base loss: 14302.00
[INFO 2017-06-28 13:01:31,110 main.py:51] epoch 1341, training loss: 9169.68, average training loss: 10469.18, base loss: 14299.78
[INFO 2017-06-28 13:01:31,767 main.py:51] epoch 1342, training loss: 11494.86, average training loss: 10470.32, base loss: 14302.21
[INFO 2017-06-28 13:01:32,420 main.py:51] epoch 1343, training loss: 11086.65, average training loss: 10470.20, base loss: 14303.74
[INFO 2017-06-28 13:01:33,081 main.py:51] epoch 1344, training loss: 9525.88, average training loss: 10468.01, base loss: 14300.97
[INFO 2017-06-28 13:01:33,741 main.py:51] epoch 1345, training loss: 10586.52, average training loss: 10466.96, base loss: 14299.72
[INFO 2017-06-28 13:01:34,397 main.py:51] epoch 1346, training loss: 12699.18, average training loss: 10468.35, base loss: 14302.95
[INFO 2017-06-28 13:01:35,053 main.py:51] epoch 1347, training loss: 10714.81, average training loss: 10468.87, base loss: 14305.82
[INFO 2017-06-28 13:01:35,718 main.py:51] epoch 1348, training loss: 9417.80, average training loss: 10466.60, base loss: 14304.97
[INFO 2017-06-28 13:01:36,381 main.py:51] epoch 1349, training loss: 8185.38, average training loss: 10464.57, base loss: 14303.63
[INFO 2017-06-28 13:01:37,045 main.py:51] epoch 1350, training loss: 9800.95, average training loss: 10462.96, base loss: 14302.59
[INFO 2017-06-28 13:01:37,708 main.py:51] epoch 1351, training loss: 9370.03, average training loss: 10460.79, base loss: 14300.29
[INFO 2017-06-28 13:01:38,361 main.py:51] epoch 1352, training loss: 8187.58, average training loss: 10459.40, base loss: 14300.39
[INFO 2017-06-28 13:01:39,005 main.py:51] epoch 1353, training loss: 9930.73, average training loss: 10458.48, base loss: 14300.12
[INFO 2017-06-28 13:01:39,667 main.py:51] epoch 1354, training loss: 8617.34, average training loss: 10456.26, base loss: 14298.60
[INFO 2017-06-28 13:01:40,332 main.py:51] epoch 1355, training loss: 10306.49, average training loss: 10455.55, base loss: 14298.80
[INFO 2017-06-28 13:01:40,978 main.py:51] epoch 1356, training loss: 10450.47, average training loss: 10454.11, base loss: 14297.82
[INFO 2017-06-28 13:01:41,623 main.py:51] epoch 1357, training loss: 10128.24, average training loss: 10453.14, base loss: 14297.94
[INFO 2017-06-28 13:01:42,279 main.py:51] epoch 1358, training loss: 9486.95, average training loss: 10452.68, base loss: 14298.83
[INFO 2017-06-28 13:01:42,941 main.py:51] epoch 1359, training loss: 9666.43, average training loss: 10451.64, base loss: 14297.64
[INFO 2017-06-28 13:01:43,582 main.py:51] epoch 1360, training loss: 9222.08, average training loss: 10448.86, base loss: 14295.78
[INFO 2017-06-28 13:01:44,234 main.py:51] epoch 1361, training loss: 10297.88, average training loss: 10448.39, base loss: 14296.31
[INFO 2017-06-28 13:01:44,880 main.py:51] epoch 1362, training loss: 10330.23, average training loss: 10447.36, base loss: 14295.52
[INFO 2017-06-28 13:01:45,558 main.py:51] epoch 1363, training loss: 11336.78, average training loss: 10448.97, base loss: 14298.95
[INFO 2017-06-28 13:01:46,193 main.py:51] epoch 1364, training loss: 9728.24, average training loss: 10445.20, base loss: 14295.61
[INFO 2017-06-28 13:01:46,835 main.py:51] epoch 1365, training loss: 10993.38, average training loss: 10444.67, base loss: 14297.48
[INFO 2017-06-28 13:01:47,530 main.py:51] epoch 1366, training loss: 9385.12, average training loss: 10443.01, base loss: 14295.67
[INFO 2017-06-28 13:01:48,177 main.py:51] epoch 1367, training loss: 9515.98, average training loss: 10442.26, base loss: 14298.11
[INFO 2017-06-28 13:01:48,818 main.py:51] epoch 1368, training loss: 8714.11, average training loss: 10440.84, base loss: 14297.09
[INFO 2017-06-28 13:01:49,494 main.py:51] epoch 1369, training loss: 9514.34, average training loss: 10439.18, base loss: 14296.72
[INFO 2017-06-28 13:01:50,145 main.py:51] epoch 1370, training loss: 11327.53, average training loss: 10439.45, base loss: 14297.86
[INFO 2017-06-28 13:01:50,815 main.py:51] epoch 1371, training loss: 9532.27, average training loss: 10438.22, base loss: 14297.95
[INFO 2017-06-28 13:01:51,476 main.py:51] epoch 1372, training loss: 10549.05, average training loss: 10437.42, base loss: 14298.14
[INFO 2017-06-28 13:01:52,119 main.py:51] epoch 1373, training loss: 10793.22, average training loss: 10437.47, base loss: 14300.67
[INFO 2017-06-28 13:01:52,796 main.py:51] epoch 1374, training loss: 9587.78, average training loss: 10436.07, base loss: 14299.26
[INFO 2017-06-28 13:01:53,459 main.py:51] epoch 1375, training loss: 11608.40, average training loss: 10437.31, base loss: 14303.08
[INFO 2017-06-28 13:01:54,113 main.py:51] epoch 1376, training loss: 9643.11, average training loss: 10435.99, base loss: 14303.08
[INFO 2017-06-28 13:01:54,785 main.py:51] epoch 1377, training loss: 9901.15, average training loss: 10435.05, base loss: 14302.87
[INFO 2017-06-28 13:01:55,443 main.py:51] epoch 1378, training loss: 10010.83, average training loss: 10435.44, base loss: 14303.83
[INFO 2017-06-28 13:01:56,099 main.py:51] epoch 1379, training loss: 9523.11, average training loss: 10435.18, base loss: 14303.69
[INFO 2017-06-28 13:01:56,748 main.py:51] epoch 1380, training loss: 9541.85, average training loss: 10432.84, base loss: 14300.18
[INFO 2017-06-28 13:01:57,423 main.py:51] epoch 1381, training loss: 10200.06, average training loss: 10431.19, base loss: 14298.08
[INFO 2017-06-28 13:01:58,074 main.py:51] epoch 1382, training loss: 8624.08, average training loss: 10428.20, base loss: 14295.15
[INFO 2017-06-28 13:01:58,727 main.py:51] epoch 1383, training loss: 9414.15, average training loss: 10425.99, base loss: 14294.03
[INFO 2017-06-28 13:01:59,370 main.py:51] epoch 1384, training loss: 10475.66, average training loss: 10424.77, base loss: 14292.56
[INFO 2017-06-28 13:02:00,031 main.py:51] epoch 1385, training loss: 10322.07, average training loss: 10422.62, base loss: 14290.52
[INFO 2017-06-28 13:02:00,690 main.py:51] epoch 1386, training loss: 10796.62, average training loss: 10422.58, base loss: 14290.90
[INFO 2017-06-28 13:02:01,346 main.py:51] epoch 1387, training loss: 10462.37, average training loss: 10423.24, base loss: 14293.10
[INFO 2017-06-28 13:02:02,030 main.py:51] epoch 1388, training loss: 11868.91, average training loss: 10424.90, base loss: 14295.96
[INFO 2017-06-28 13:02:02,692 main.py:51] epoch 1389, training loss: 9731.25, average training loss: 10421.44, base loss: 14293.14
[INFO 2017-06-28 13:02:03,344 main.py:51] epoch 1390, training loss: 10538.21, average training loss: 10420.00, base loss: 14292.86
[INFO 2017-06-28 13:02:04,006 main.py:51] epoch 1391, training loss: 11922.82, average training loss: 10421.44, base loss: 14293.99
[INFO 2017-06-28 13:02:04,649 main.py:51] epoch 1392, training loss: 9678.80, average training loss: 10420.23, base loss: 14293.26
[INFO 2017-06-28 13:02:05,308 main.py:51] epoch 1393, training loss: 13176.84, average training loss: 10422.82, base loss: 14297.44
[INFO 2017-06-28 13:02:05,970 main.py:51] epoch 1394, training loss: 9424.26, average training loss: 10421.49, base loss: 14296.05
[INFO 2017-06-28 13:02:06,634 main.py:51] epoch 1395, training loss: 8674.77, average training loss: 10418.93, base loss: 14293.30
[INFO 2017-06-28 13:02:07,280 main.py:51] epoch 1396, training loss: 9838.95, average training loss: 10418.95, base loss: 14294.32
[INFO 2017-06-28 13:02:07,946 main.py:51] epoch 1397, training loss: 10321.08, average training loss: 10417.75, base loss: 14294.33
[INFO 2017-06-28 13:02:08,612 main.py:51] epoch 1398, training loss: 10781.98, average training loss: 10416.25, base loss: 14292.97
[INFO 2017-06-28 13:02:09,278 main.py:51] epoch 1399, training loss: 8926.53, average training loss: 10414.69, base loss: 14291.82
[INFO 2017-06-28 13:02:09,278 main.py:53] epoch 1399, testing
[INFO 2017-06-28 13:02:11,816 main.py:105] average testing loss: 11201.05, base loss: 14884.30
[INFO 2017-06-28 13:02:11,816 main.py:106] improve_loss: 3683.25, improve_percent: 0.25
[INFO 2017-06-28 13:02:11,817 main.py:76] current best improved percent: 0.26
[INFO 2017-06-28 13:02:12,475 main.py:51] epoch 1400, training loss: 10467.91, average training loss: 10414.34, base loss: 14292.19
[INFO 2017-06-28 13:02:13,128 main.py:51] epoch 1401, training loss: 8179.56, average training loss: 10410.02, base loss: 14287.31
[INFO 2017-06-28 13:02:13,759 main.py:51] epoch 1402, training loss: 9465.37, average training loss: 10409.23, base loss: 14286.23
[INFO 2017-06-28 13:02:14,409 main.py:51] epoch 1403, training loss: 9111.12, average training loss: 10409.19, base loss: 14287.79
[INFO 2017-06-28 13:02:15,065 main.py:51] epoch 1404, training loss: 9328.79, average training loss: 10407.56, base loss: 14286.41
[INFO 2017-06-28 13:02:15,724 main.py:51] epoch 1405, training loss: 9656.21, average training loss: 10406.53, base loss: 14287.28
[INFO 2017-06-28 13:02:16,392 main.py:51] epoch 1406, training loss: 10486.62, average training loss: 10407.59, base loss: 14288.85
[INFO 2017-06-28 13:02:17,034 main.py:51] epoch 1407, training loss: 9983.89, average training loss: 10406.83, base loss: 14288.86
[INFO 2017-06-28 13:02:17,676 main.py:51] epoch 1408, training loss: 10185.38, average training loss: 10407.27, base loss: 14290.53
[INFO 2017-06-28 13:02:18,344 main.py:51] epoch 1409, training loss: 11690.20, average training loss: 10408.35, base loss: 14292.55
[INFO 2017-06-28 13:02:19,004 main.py:51] epoch 1410, training loss: 9618.58, average training loss: 10409.22, base loss: 14293.80
[INFO 2017-06-28 13:02:19,650 main.py:51] epoch 1411, training loss: 10035.72, average training loss: 10408.77, base loss: 14293.80
[INFO 2017-06-28 13:02:20,302 main.py:51] epoch 1412, training loss: 9536.55, average training loss: 10405.54, base loss: 14290.37
[INFO 2017-06-28 13:02:20,961 main.py:51] epoch 1413, training loss: 10131.62, average training loss: 10406.72, base loss: 14292.38
[INFO 2017-06-28 13:02:21,619 main.py:51] epoch 1414, training loss: 9811.79, average training loss: 10404.69, base loss: 14290.08
[INFO 2017-06-28 13:02:22,274 main.py:51] epoch 1415, training loss: 10688.46, average training loss: 10404.92, base loss: 14291.36
[INFO 2017-06-28 13:02:22,927 main.py:51] epoch 1416, training loss: 9946.89, average training loss: 10404.86, base loss: 14292.06
[INFO 2017-06-28 13:02:23,597 main.py:51] epoch 1417, training loss: 9556.16, average training loss: 10401.45, base loss: 14289.17
[INFO 2017-06-28 13:02:24,284 main.py:51] epoch 1418, training loss: 10131.11, average training loss: 10400.60, base loss: 14288.42
[INFO 2017-06-28 13:02:24,945 main.py:51] epoch 1419, training loss: 10998.41, average training loss: 10401.16, base loss: 14289.64
[INFO 2017-06-28 13:02:25,602 main.py:51] epoch 1420, training loss: 10359.95, average training loss: 10401.21, base loss: 14290.77
[INFO 2017-06-28 13:02:26,270 main.py:51] epoch 1421, training loss: 9333.22, average training loss: 10400.17, base loss: 14290.31
[INFO 2017-06-28 13:02:26,943 main.py:51] epoch 1422, training loss: 10823.67, average training loss: 10399.38, base loss: 14289.72
[INFO 2017-06-28 13:02:27,586 main.py:51] epoch 1423, training loss: 10921.77, average training loss: 10397.64, base loss: 14288.48
[INFO 2017-06-28 13:02:28,240 main.py:51] epoch 1424, training loss: 9353.46, average training loss: 10397.65, base loss: 14289.54
[INFO 2017-06-28 13:02:28,957 main.py:51] epoch 1425, training loss: 10104.76, average training loss: 10397.03, base loss: 14291.14
[INFO 2017-06-28 13:02:29,605 main.py:51] epoch 1426, training loss: 9734.63, average training loss: 10395.46, base loss: 14290.22
[INFO 2017-06-28 13:02:30,273 main.py:51] epoch 1427, training loss: 8710.05, average training loss: 10394.63, base loss: 14287.88
[INFO 2017-06-28 13:02:30,921 main.py:51] epoch 1428, training loss: 11242.73, average training loss: 10393.95, base loss: 14288.91
[INFO 2017-06-28 13:02:31,611 main.py:51] epoch 1429, training loss: 9927.18, average training loss: 10393.30, base loss: 14288.53
[INFO 2017-06-28 13:02:32,277 main.py:51] epoch 1430, training loss: 8903.54, average training loss: 10391.01, base loss: 14286.73
[INFO 2017-06-28 13:02:32,947 main.py:51] epoch 1431, training loss: 9458.51, average training loss: 10390.58, base loss: 14286.95
[INFO 2017-06-28 13:02:33,601 main.py:51] epoch 1432, training loss: 11769.47, average training loss: 10390.60, base loss: 14288.75
[INFO 2017-06-28 13:02:34,251 main.py:51] epoch 1433, training loss: 10671.20, average training loss: 10390.24, base loss: 14288.61
[INFO 2017-06-28 13:02:34,914 main.py:51] epoch 1434, training loss: 11060.84, average training loss: 10389.23, base loss: 14287.23
[INFO 2017-06-28 13:02:35,569 main.py:51] epoch 1435, training loss: 10722.03, average training loss: 10388.28, base loss: 14288.64
[INFO 2017-06-28 13:02:36,255 main.py:51] epoch 1436, training loss: 12017.17, average training loss: 10390.87, base loss: 14293.93
[INFO 2017-06-28 13:02:36,914 main.py:51] epoch 1437, training loss: 9637.21, average training loss: 10388.84, base loss: 14291.44
[INFO 2017-06-28 13:02:37,560 main.py:51] epoch 1438, training loss: 11168.32, average training loss: 10390.42, base loss: 14293.91
[INFO 2017-06-28 13:02:38,224 main.py:51] epoch 1439, training loss: 10889.82, average training loss: 10390.15, base loss: 14293.91
[INFO 2017-06-28 13:02:38,859 main.py:51] epoch 1440, training loss: 10550.28, average training loss: 10389.31, base loss: 14293.53
[INFO 2017-06-28 13:02:39,509 main.py:51] epoch 1441, training loss: 9386.53, average training loss: 10386.48, base loss: 14290.29
[INFO 2017-06-28 13:02:40,183 main.py:51] epoch 1442, training loss: 11221.74, average training loss: 10385.16, base loss: 14289.41
[INFO 2017-06-28 13:02:40,862 main.py:51] epoch 1443, training loss: 10605.25, average training loss: 10384.85, base loss: 14290.73
[INFO 2017-06-28 13:02:41,525 main.py:51] epoch 1444, training loss: 10929.79, average training loss: 10384.08, base loss: 14289.93
[INFO 2017-06-28 13:02:42,188 main.py:51] epoch 1445, training loss: 10199.95, average training loss: 10381.93, base loss: 14288.86
[INFO 2017-06-28 13:02:42,856 main.py:51] epoch 1446, training loss: 11577.61, average training loss: 10383.31, base loss: 14290.65
[INFO 2017-06-28 13:02:43,538 main.py:51] epoch 1447, training loss: 10273.20, average training loss: 10381.11, base loss: 14288.71
[INFO 2017-06-28 13:02:44,219 main.py:51] epoch 1448, training loss: 10180.32, average training loss: 10380.43, base loss: 14290.14
[INFO 2017-06-28 13:02:44,865 main.py:51] epoch 1449, training loss: 9876.49, average training loss: 10379.64, base loss: 14290.39
[INFO 2017-06-28 13:02:45,532 main.py:51] epoch 1450, training loss: 9461.95, average training loss: 10377.84, base loss: 14289.67
[INFO 2017-06-28 13:02:46,182 main.py:51] epoch 1451, training loss: 9108.54, average training loss: 10376.76, base loss: 14289.54
[INFO 2017-06-28 13:02:46,832 main.py:51] epoch 1452, training loss: 9189.56, average training loss: 10376.59, base loss: 14290.91
[INFO 2017-06-28 13:02:47,476 main.py:51] epoch 1453, training loss: 10228.88, average training loss: 10376.02, base loss: 14292.14
[INFO 2017-06-28 13:02:48,149 main.py:51] epoch 1454, training loss: 11009.59, average training loss: 10377.17, base loss: 14295.54
[INFO 2017-06-28 13:02:48,821 main.py:51] epoch 1455, training loss: 9300.77, average training loss: 10375.86, base loss: 14294.53
[INFO 2017-06-28 13:02:49,462 main.py:51] epoch 1456, training loss: 11985.72, average training loss: 10378.12, base loss: 14298.52
[INFO 2017-06-28 13:02:50,111 main.py:51] epoch 1457, training loss: 10631.41, average training loss: 10378.77, base loss: 14300.59
[INFO 2017-06-28 13:02:50,766 main.py:51] epoch 1458, training loss: 9065.32, average training loss: 10375.94, base loss: 14298.31
[INFO 2017-06-28 13:02:51,401 main.py:51] epoch 1459, training loss: 11310.79, average training loss: 10375.71, base loss: 14299.52
[INFO 2017-06-28 13:02:52,050 main.py:51] epoch 1460, training loss: 10676.61, average training loss: 10374.96, base loss: 14299.75
[INFO 2017-06-28 13:02:52,723 main.py:51] epoch 1461, training loss: 9697.93, average training loss: 10374.29, base loss: 14299.99
[INFO 2017-06-28 13:02:53,411 main.py:51] epoch 1462, training loss: 10339.37, average training loss: 10374.35, base loss: 14299.21
[INFO 2017-06-28 13:02:54,086 main.py:51] epoch 1463, training loss: 10378.66, average training loss: 10375.38, base loss: 14301.85
[INFO 2017-06-28 13:02:54,761 main.py:51] epoch 1464, training loss: 10033.88, average training loss: 10373.79, base loss: 14301.78
[INFO 2017-06-28 13:02:55,428 main.py:51] epoch 1465, training loss: 9552.70, average training loss: 10371.08, base loss: 14298.51
[INFO 2017-06-28 13:02:56,064 main.py:51] epoch 1466, training loss: 10654.38, average training loss: 10371.99, base loss: 14301.49
[INFO 2017-06-28 13:02:56,706 main.py:51] epoch 1467, training loss: 12227.35, average training loss: 10374.32, base loss: 14306.34
[INFO 2017-06-28 13:02:57,334 main.py:51] epoch 1468, training loss: 9735.12, average training loss: 10372.10, base loss: 14303.95
[INFO 2017-06-28 13:02:58,004 main.py:51] epoch 1469, training loss: 10156.20, average training loss: 10372.29, base loss: 14304.78
[INFO 2017-06-28 13:02:58,666 main.py:51] epoch 1470, training loss: 12602.78, average training loss: 10374.90, base loss: 14308.71
[INFO 2017-06-28 13:02:59,338 main.py:51] epoch 1471, training loss: 10663.29, average training loss: 10375.14, base loss: 14310.01
[INFO 2017-06-28 13:02:59,983 main.py:51] epoch 1472, training loss: 8378.37, average training loss: 10373.31, base loss: 14307.59
[INFO 2017-06-28 13:03:00,638 main.py:51] epoch 1473, training loss: 11230.66, average training loss: 10373.83, base loss: 14307.59
[INFO 2017-06-28 13:03:01,324 main.py:51] epoch 1474, training loss: 10623.40, average training loss: 10374.11, base loss: 14309.29
[INFO 2017-06-28 13:03:01,999 main.py:51] epoch 1475, training loss: 10366.87, average training loss: 10374.10, base loss: 14309.03
[INFO 2017-06-28 13:03:02,653 main.py:51] epoch 1476, training loss: 10918.86, average training loss: 10374.28, base loss: 14308.82
[INFO 2017-06-28 13:03:03,334 main.py:51] epoch 1477, training loss: 12276.57, average training loss: 10377.90, base loss: 14314.41
[INFO 2017-06-28 13:03:03,995 main.py:51] epoch 1478, training loss: 11778.62, average training loss: 10378.62, base loss: 14316.66
[INFO 2017-06-28 13:03:04,672 main.py:51] epoch 1479, training loss: 9950.88, average training loss: 10379.07, base loss: 14318.78
[INFO 2017-06-28 13:03:05,364 main.py:51] epoch 1480, training loss: 11214.11, average training loss: 10380.27, base loss: 14322.65
[INFO 2017-06-28 13:03:06,034 main.py:51] epoch 1481, training loss: 9955.06, average training loss: 10378.09, base loss: 14321.23
[INFO 2017-06-28 13:03:06,684 main.py:51] epoch 1482, training loss: 10711.75, average training loss: 10376.95, base loss: 14320.26
[INFO 2017-06-28 13:03:07,322 main.py:51] epoch 1483, training loss: 9569.50, average training loss: 10374.13, base loss: 14316.91
[INFO 2017-06-28 13:03:07,969 main.py:51] epoch 1484, training loss: 9478.39, average training loss: 10373.00, base loss: 14316.55
[INFO 2017-06-28 13:03:08,619 main.py:51] epoch 1485, training loss: 10192.60, average training loss: 10372.94, base loss: 14317.91
[INFO 2017-06-28 13:03:09,249 main.py:51] epoch 1486, training loss: 9606.95, average training loss: 10370.46, base loss: 14314.81
[INFO 2017-06-28 13:03:09,907 main.py:51] epoch 1487, training loss: 8899.21, average training loss: 10368.52, base loss: 14313.12
[INFO 2017-06-28 13:03:10,581 main.py:51] epoch 1488, training loss: 10018.80, average training loss: 10367.24, base loss: 14311.56
[INFO 2017-06-28 13:03:11,237 main.py:51] epoch 1489, training loss: 9807.61, average training loss: 10366.68, base loss: 14313.02
[INFO 2017-06-28 13:03:11,892 main.py:51] epoch 1490, training loss: 9686.44, average training loss: 10366.10, base loss: 14312.59
[INFO 2017-06-28 13:03:12,529 main.py:51] epoch 1491, training loss: 9070.57, average training loss: 10363.85, base loss: 14310.33
[INFO 2017-06-28 13:03:13,181 main.py:51] epoch 1492, training loss: 10015.49, average training loss: 10363.79, base loss: 14312.00
[INFO 2017-06-28 13:03:13,839 main.py:51] epoch 1493, training loss: 10854.06, average training loss: 10364.15, base loss: 14315.04
[INFO 2017-06-28 13:03:14,479 main.py:51] epoch 1494, training loss: 9885.24, average training loss: 10362.49, base loss: 14313.17
[INFO 2017-06-28 13:03:15,155 main.py:51] epoch 1495, training loss: 10226.65, average training loss: 10363.10, base loss: 14315.00
[INFO 2017-06-28 13:03:15,818 main.py:51] epoch 1496, training loss: 10416.17, average training loss: 10363.87, base loss: 14317.58
[INFO 2017-06-28 13:03:16,465 main.py:51] epoch 1497, training loss: 8647.43, average training loss: 10361.53, base loss: 14313.89
[INFO 2017-06-28 13:03:17,103 main.py:51] epoch 1498, training loss: 9882.12, average training loss: 10359.75, base loss: 14313.06
[INFO 2017-06-28 13:03:17,784 main.py:51] epoch 1499, training loss: 10530.59, average training loss: 10359.46, base loss: 14313.00
[INFO 2017-06-28 13:03:17,784 main.py:53] epoch 1499, testing
[INFO 2017-06-28 13:03:20,373 main.py:105] average testing loss: 10691.67, base loss: 14451.01
[INFO 2017-06-28 13:03:20,373 main.py:106] improve_loss: 3759.34, improve_percent: 0.26
[INFO 2017-06-28 13:03:20,375 main.py:76] current best improved percent: 0.26
[INFO 2017-06-28 13:03:21,026 main.py:51] epoch 1500, training loss: 10766.29, average training loss: 10358.19, base loss: 14311.78
[INFO 2017-06-28 13:03:21,687 main.py:51] epoch 1501, training loss: 12398.97, average training loss: 10360.01, base loss: 14314.55
[INFO 2017-06-28 13:03:22,354 main.py:51] epoch 1502, training loss: 9044.82, average training loss: 10358.31, base loss: 14313.65
[INFO 2017-06-28 13:03:23,026 main.py:51] epoch 1503, training loss: 10126.03, average training loss: 10358.28, base loss: 14314.62
[INFO 2017-06-28 13:03:23,694 main.py:51] epoch 1504, training loss: 10780.59, average training loss: 10357.12, base loss: 14313.95
[INFO 2017-06-28 13:03:24,381 main.py:51] epoch 1505, training loss: 10239.93, average training loss: 10357.53, base loss: 14315.55
[INFO 2017-06-28 13:03:25,063 main.py:51] epoch 1506, training loss: 11055.50, average training loss: 10357.46, base loss: 14315.86
[INFO 2017-06-28 13:03:25,734 main.py:51] epoch 1507, training loss: 10175.34, average training loss: 10357.03, base loss: 14317.22
[INFO 2017-06-28 13:03:26,433 main.py:51] epoch 1508, training loss: 9810.47, average training loss: 10355.47, base loss: 14316.32
[INFO 2017-06-28 13:03:27,137 main.py:51] epoch 1509, training loss: 9511.85, average training loss: 10354.65, base loss: 14315.61
[INFO 2017-06-28 13:03:27,810 main.py:51] epoch 1510, training loss: 10538.34, average training loss: 10354.92, base loss: 14316.68
[INFO 2017-06-28 13:03:28,478 main.py:51] epoch 1511, training loss: 11730.04, average training loss: 10358.30, base loss: 14321.35
[INFO 2017-06-28 13:03:29,125 main.py:51] epoch 1512, training loss: 9884.75, average training loss: 10356.12, base loss: 14319.09
[INFO 2017-06-28 13:03:29,812 main.py:51] epoch 1513, training loss: 10253.96, average training loss: 10354.92, base loss: 14319.87
[INFO 2017-06-28 13:03:30,470 main.py:51] epoch 1514, training loss: 11559.65, average training loss: 10356.01, base loss: 14321.84
[INFO 2017-06-28 13:03:31,129 main.py:51] epoch 1515, training loss: 9827.43, average training loss: 10355.69, base loss: 14322.84
[INFO 2017-06-28 13:03:31,814 main.py:51] epoch 1516, training loss: 10149.78, average training loss: 10352.56, base loss: 14320.34
[INFO 2017-06-28 13:03:32,471 main.py:51] epoch 1517, training loss: 10248.41, average training loss: 10351.35, base loss: 14319.52
[INFO 2017-06-28 13:03:33,130 main.py:51] epoch 1518, training loss: 9320.81, average training loss: 10348.90, base loss: 14314.99
[INFO 2017-06-28 13:03:33,786 main.py:51] epoch 1519, training loss: 9817.43, average training loss: 10348.93, base loss: 14316.16
[INFO 2017-06-28 13:03:34,468 main.py:51] epoch 1520, training loss: 9115.88, average training loss: 10346.76, base loss: 14313.18
[INFO 2017-06-28 13:03:35,121 main.py:51] epoch 1521, training loss: 9705.01, average training loss: 10344.12, base loss: 14311.16
[INFO 2017-06-28 13:03:35,787 main.py:51] epoch 1522, training loss: 10280.59, average training loss: 10343.49, base loss: 14311.19
[INFO 2017-06-28 13:03:36,462 main.py:51] epoch 1523, training loss: 11692.63, average training loss: 10344.20, base loss: 14313.13
[INFO 2017-06-28 13:03:37,128 main.py:51] epoch 1524, training loss: 10878.81, average training loss: 10345.11, base loss: 14315.65
[INFO 2017-06-28 13:03:37,797 main.py:51] epoch 1525, training loss: 9998.00, average training loss: 10345.57, base loss: 14316.12
[INFO 2017-06-28 13:03:38,447 main.py:51] epoch 1526, training loss: 9812.48, average training loss: 10345.72, base loss: 14315.66
[INFO 2017-06-28 13:03:39,098 main.py:51] epoch 1527, training loss: 9550.33, average training loss: 10343.00, base loss: 14313.64
[INFO 2017-06-28 13:03:39,767 main.py:51] epoch 1528, training loss: 9557.92, average training loss: 10343.81, base loss: 14314.79
[INFO 2017-06-28 13:03:40,430 main.py:51] epoch 1529, training loss: 10425.85, average training loss: 10344.33, base loss: 14315.97
[INFO 2017-06-28 13:03:41,108 main.py:51] epoch 1530, training loss: 11071.39, average training loss: 10343.33, base loss: 14315.05
[INFO 2017-06-28 13:03:41,774 main.py:51] epoch 1531, training loss: 9034.99, average training loss: 10341.63, base loss: 14313.18
[INFO 2017-06-28 13:03:42,419 main.py:51] epoch 1532, training loss: 10361.62, average training loss: 10342.20, base loss: 14313.33
[INFO 2017-06-28 13:03:43,076 main.py:51] epoch 1533, training loss: 8932.20, average training loss: 10340.81, base loss: 14311.86
[INFO 2017-06-28 13:03:43,743 main.py:51] epoch 1534, training loss: 10379.69, average training loss: 10339.98, base loss: 14311.43
[INFO 2017-06-28 13:03:44,399 main.py:51] epoch 1535, training loss: 11102.70, average training loss: 10340.64, base loss: 14314.37
[INFO 2017-06-28 13:03:45,055 main.py:51] epoch 1536, training loss: 10740.33, average training loss: 10339.98, base loss: 14315.23
[INFO 2017-06-28 13:03:45,748 main.py:51] epoch 1537, training loss: 10275.42, average training loss: 10338.37, base loss: 14315.23
[INFO 2017-06-28 13:03:46,463 main.py:51] epoch 1538, training loss: 9829.08, average training loss: 10337.07, base loss: 14314.04
[INFO 2017-06-28 13:03:47,154 main.py:51] epoch 1539, training loss: 9800.91, average training loss: 10337.14, base loss: 14314.22
[INFO 2017-06-28 13:03:47,829 main.py:51] epoch 1540, training loss: 10747.75, average training loss: 10337.71, base loss: 14316.04
[INFO 2017-06-28 13:03:48,537 main.py:51] epoch 1541, training loss: 10036.81, average training loss: 10338.16, base loss: 14317.89
[INFO 2017-06-28 13:03:49,181 main.py:51] epoch 1542, training loss: 10106.31, average training loss: 10338.34, base loss: 14318.53
[INFO 2017-06-28 13:03:49,864 main.py:51] epoch 1543, training loss: 9215.99, average training loss: 10337.03, base loss: 14318.45
[INFO 2017-06-28 13:03:50,558 main.py:51] epoch 1544, training loss: 11704.04, average training loss: 10338.39, base loss: 14320.40
[INFO 2017-06-28 13:03:51,248 main.py:51] epoch 1545, training loss: 10065.32, average training loss: 10337.99, base loss: 14320.37
[INFO 2017-06-28 13:03:51,910 main.py:51] epoch 1546, training loss: 10825.94, average training loss: 10337.93, base loss: 14321.66
[INFO 2017-06-28 13:03:52,566 main.py:51] epoch 1547, training loss: 10349.05, average training loss: 10338.99, base loss: 14323.07
[INFO 2017-06-28 13:03:53,228 main.py:51] epoch 1548, training loss: 9246.44, average training loss: 10337.85, base loss: 14320.46
[INFO 2017-06-28 13:03:53,894 main.py:51] epoch 1549, training loss: 9448.96, average training loss: 10336.67, base loss: 14319.41
[INFO 2017-06-28 13:03:54,557 main.py:51] epoch 1550, training loss: 11930.93, average training loss: 10337.39, base loss: 14320.49
[INFO 2017-06-28 13:03:55,219 main.py:51] epoch 1551, training loss: 9296.04, average training loss: 10335.25, base loss: 14318.93
[INFO 2017-06-28 13:03:55,871 main.py:51] epoch 1552, training loss: 9972.50, average training loss: 10333.80, base loss: 14318.99
[INFO 2017-06-28 13:03:56,555 main.py:51] epoch 1553, training loss: 9857.82, average training loss: 10333.58, base loss: 14318.66
[INFO 2017-06-28 13:03:57,203 main.py:51] epoch 1554, training loss: 10055.07, average training loss: 10333.18, base loss: 14318.84
[INFO 2017-06-28 13:03:57,875 main.py:51] epoch 1555, training loss: 10341.93, average training loss: 10332.53, base loss: 14318.80
[INFO 2017-06-28 13:03:58,564 main.py:51] epoch 1556, training loss: 9608.51, average training loss: 10331.52, base loss: 14318.61
[INFO 2017-06-28 13:03:59,236 main.py:51] epoch 1557, training loss: 9152.40, average training loss: 10330.72, base loss: 14317.94
[INFO 2017-06-28 13:03:59,917 main.py:51] epoch 1558, training loss: 9874.71, average training loss: 10330.39, base loss: 14317.14
[INFO 2017-06-28 13:04:00,592 main.py:51] epoch 1559, training loss: 8824.58, average training loss: 10329.16, base loss: 14315.15
[INFO 2017-06-28 13:04:01,251 main.py:51] epoch 1560, training loss: 9680.70, average training loss: 10328.26, base loss: 14315.71
[INFO 2017-06-28 13:04:01,887 main.py:51] epoch 1561, training loss: 9074.84, average training loss: 10326.36, base loss: 14313.62
[INFO 2017-06-28 13:04:02,537 main.py:51] epoch 1562, training loss: 10076.16, average training loss: 10326.92, base loss: 14313.88
[INFO 2017-06-28 13:04:03,203 main.py:51] epoch 1563, training loss: 10337.73, average training loss: 10327.05, base loss: 14313.99
[INFO 2017-06-28 13:04:03,890 main.py:51] epoch 1564, training loss: 9134.30, average training loss: 10325.17, base loss: 14311.94
[INFO 2017-06-28 13:04:04,557 main.py:51] epoch 1565, training loss: 9873.21, average training loss: 10324.83, base loss: 14313.32
[INFO 2017-06-28 13:04:05,223 main.py:51] epoch 1566, training loss: 8468.77, average training loss: 10322.48, base loss: 14311.30
[INFO 2017-06-28 13:04:05,882 main.py:51] epoch 1567, training loss: 10988.85, average training loss: 10322.85, base loss: 14312.05
[INFO 2017-06-28 13:04:06,533 main.py:51] epoch 1568, training loss: 9039.04, average training loss: 10319.18, base loss: 14308.14
[INFO 2017-06-28 13:04:07,191 main.py:51] epoch 1569, training loss: 10056.98, average training loss: 10319.31, base loss: 14309.05
[INFO 2017-06-28 13:04:07,836 main.py:51] epoch 1570, training loss: 8977.05, average training loss: 10316.95, base loss: 14305.46
[INFO 2017-06-28 13:04:08,528 main.py:51] epoch 1571, training loss: 11132.34, average training loss: 10317.06, base loss: 14307.18
[INFO 2017-06-28 13:04:09,194 main.py:51] epoch 1572, training loss: 9976.46, average training loss: 10316.47, base loss: 14307.58
[INFO 2017-06-28 13:04:09,859 main.py:51] epoch 1573, training loss: 9444.45, average training loss: 10316.16, base loss: 14308.27
[INFO 2017-06-28 13:04:10,562 main.py:51] epoch 1574, training loss: 10138.55, average training loss: 10314.58, base loss: 14306.37
[INFO 2017-06-28 13:04:11,237 main.py:51] epoch 1575, training loss: 9284.15, average training loss: 10313.31, base loss: 14303.71
[INFO 2017-06-28 13:04:11,918 main.py:51] epoch 1576, training loss: 8997.78, average training loss: 10311.50, base loss: 14300.67
[INFO 2017-06-28 13:04:12,593 main.py:51] epoch 1577, training loss: 11291.40, average training loss: 10313.07, base loss: 14304.47
[INFO 2017-06-28 13:04:13,260 main.py:51] epoch 1578, training loss: 10131.48, average training loss: 10313.16, base loss: 14305.63
[INFO 2017-06-28 13:04:13,948 main.py:51] epoch 1579, training loss: 12002.52, average training loss: 10314.14, base loss: 14306.86
[INFO 2017-06-28 13:04:14,597 main.py:51] epoch 1580, training loss: 10972.02, average training loss: 10314.22, base loss: 14309.45
[INFO 2017-06-28 13:04:15,260 main.py:51] epoch 1581, training loss: 10460.50, average training loss: 10314.11, base loss: 14310.02
[INFO 2017-06-28 13:04:15,931 main.py:51] epoch 1582, training loss: 8797.90, average training loss: 10311.95, base loss: 14306.48
[INFO 2017-06-28 13:04:16,603 main.py:51] epoch 1583, training loss: 9717.22, average training loss: 10310.40, base loss: 14305.38
[INFO 2017-06-28 13:04:17,257 main.py:51] epoch 1584, training loss: 9614.23, average training loss: 10309.32, base loss: 14304.97
[INFO 2017-06-28 13:04:17,926 main.py:51] epoch 1585, training loss: 9751.00, average training loss: 10308.01, base loss: 14302.70
[INFO 2017-06-28 13:04:18,587 main.py:51] epoch 1586, training loss: 10954.47, average training loss: 10309.45, base loss: 14304.48
[INFO 2017-06-28 13:04:19,319 main.py:51] epoch 1587, training loss: 9014.08, average training loss: 10308.91, base loss: 14305.28
[INFO 2017-06-28 13:04:20,025 main.py:51] epoch 1588, training loss: 8589.40, average training loss: 10307.36, base loss: 14303.65
[INFO 2017-06-28 13:04:20,685 main.py:51] epoch 1589, training loss: 9366.37, average training loss: 10306.91, base loss: 14303.53
[INFO 2017-06-28 13:04:21,406 main.py:51] epoch 1590, training loss: 8843.43, average training loss: 10305.53, base loss: 14302.66
[INFO 2017-06-28 13:04:22,120 main.py:51] epoch 1591, training loss: 10287.58, average training loss: 10304.78, base loss: 14302.71
[INFO 2017-06-28 13:04:22,846 main.py:51] epoch 1592, training loss: 10990.14, average training loss: 10303.61, base loss: 14301.77
[INFO 2017-06-28 13:04:23,511 main.py:51] epoch 1593, training loss: 10121.65, average training loss: 10303.04, base loss: 14301.90
[INFO 2017-06-28 13:04:24,171 main.py:51] epoch 1594, training loss: 10161.68, average training loss: 10302.70, base loss: 14303.01
[INFO 2017-06-28 13:04:24,917 main.py:51] epoch 1595, training loss: 9788.07, average training loss: 10301.79, base loss: 14301.57
[INFO 2017-06-28 13:04:25,579 main.py:51] epoch 1596, training loss: 8303.47, average training loss: 10299.60, base loss: 14300.18
[INFO 2017-06-28 13:04:26,234 main.py:51] epoch 1597, training loss: 9715.38, average training loss: 10299.52, base loss: 14301.73
[INFO 2017-06-28 13:04:26,927 main.py:51] epoch 1598, training loss: 10833.96, average training loss: 10299.87, base loss: 14302.00
[INFO 2017-06-28 13:04:27,674 main.py:51] epoch 1599, training loss: 9532.36, average training loss: 10299.81, base loss: 14301.84
[INFO 2017-06-28 13:04:27,675 main.py:53] epoch 1599, testing
[INFO 2017-06-28 13:04:30,467 main.py:105] average testing loss: 11102.02, base loss: 14736.98
[INFO 2017-06-28 13:04:30,467 main.py:106] improve_loss: 3634.95, improve_percent: 0.25
[INFO 2017-06-28 13:04:30,468 main.py:76] current best improved percent: 0.26
[INFO 2017-06-28 13:04:31,140 main.py:51] epoch 1600, training loss: 10164.69, average training loss: 10300.38, base loss: 14302.12
[INFO 2017-06-28 13:04:31,799 main.py:51] epoch 1601, training loss: 10620.74, average training loss: 10300.44, base loss: 14302.01
[INFO 2017-06-28 13:04:32,505 main.py:51] epoch 1602, training loss: 11248.70, average training loss: 10303.17, base loss: 14307.18
[INFO 2017-06-28 13:04:33,203 main.py:51] epoch 1603, training loss: 9845.44, average training loss: 10303.03, base loss: 14307.98
[INFO 2017-06-28 13:04:33,868 main.py:51] epoch 1604, training loss: 10007.59, average training loss: 10301.31, base loss: 14307.20
[INFO 2017-06-28 13:04:34,551 main.py:51] epoch 1605, training loss: 9985.97, average training loss: 10300.22, base loss: 14307.53
[INFO 2017-06-28 13:04:35,203 main.py:51] epoch 1606, training loss: 9786.54, average training loss: 10299.46, base loss: 14306.93
[INFO 2017-06-28 13:04:35,879 main.py:51] epoch 1607, training loss: 9485.74, average training loss: 10298.67, base loss: 14306.78
[INFO 2017-06-28 13:04:36,535 main.py:51] epoch 1608, training loss: 10266.29, average training loss: 10297.91, base loss: 14306.64
[INFO 2017-06-28 13:04:37,174 main.py:51] epoch 1609, training loss: 10104.46, average training loss: 10297.80, base loss: 14307.28
[INFO 2017-06-28 13:04:37,853 main.py:51] epoch 1610, training loss: 12153.89, average training loss: 10298.65, base loss: 14307.36
[INFO 2017-06-28 13:04:38,536 main.py:51] epoch 1611, training loss: 10782.84, average training loss: 10298.69, base loss: 14307.61
[INFO 2017-06-28 13:04:39,205 main.py:51] epoch 1612, training loss: 9502.30, average training loss: 10298.36, base loss: 14309.23
[INFO 2017-06-28 13:04:39,852 main.py:51] epoch 1613, training loss: 10369.43, average training loss: 10299.32, base loss: 14311.02
[INFO 2017-06-28 13:04:40,506 main.py:51] epoch 1614, training loss: 9469.36, average training loss: 10299.86, base loss: 14312.82
[INFO 2017-06-28 13:04:41,154 main.py:51] epoch 1615, training loss: 11105.82, average training loss: 10300.18, base loss: 14313.68
[INFO 2017-06-28 13:04:41,847 main.py:51] epoch 1616, training loss: 10544.77, average training loss: 10299.82, base loss: 14312.27
[INFO 2017-06-28 13:04:42,528 main.py:51] epoch 1617, training loss: 9056.55, average training loss: 10298.82, base loss: 14311.42
[INFO 2017-06-28 13:04:43,245 main.py:51] epoch 1618, training loss: 9977.24, average training loss: 10298.57, base loss: 14312.97
[INFO 2017-06-28 13:04:43,924 main.py:51] epoch 1619, training loss: 10734.35, average training loss: 10298.27, base loss: 14314.33
[INFO 2017-06-28 13:04:44,583 main.py:51] epoch 1620, training loss: 10112.06, average training loss: 10297.88, base loss: 14313.75
[INFO 2017-06-28 13:04:45,231 main.py:51] epoch 1621, training loss: 13931.37, average training loss: 10301.96, base loss: 14321.08
[INFO 2017-06-28 13:04:45,877 main.py:51] epoch 1622, training loss: 10924.33, average training loss: 10302.86, base loss: 14321.16
[INFO 2017-06-28 13:04:46,521 main.py:51] epoch 1623, training loss: 10036.46, average training loss: 10302.55, base loss: 14320.67
[INFO 2017-06-28 13:04:47,173 main.py:51] epoch 1624, training loss: 9635.48, average training loss: 10301.04, base loss: 14319.09
[INFO 2017-06-28 13:04:47,847 main.py:51] epoch 1625, training loss: 9894.43, average training loss: 10301.00, base loss: 14320.14
[INFO 2017-06-28 13:04:48,498 main.py:51] epoch 1626, training loss: 10667.86, average training loss: 10299.94, base loss: 14318.67
[INFO 2017-06-28 13:04:49,160 main.py:51] epoch 1627, training loss: 9498.92, average training loss: 10296.08, base loss: 14315.16
[INFO 2017-06-28 13:04:49,809 main.py:51] epoch 1628, training loss: 10502.02, average training loss: 10297.12, base loss: 14316.75
[INFO 2017-06-28 13:04:50,488 main.py:51] epoch 1629, training loss: 9128.45, average training loss: 10296.48, base loss: 14316.39
[INFO 2017-06-28 13:04:51,140 main.py:51] epoch 1630, training loss: 9604.69, average training loss: 10295.97, base loss: 14316.28
[INFO 2017-06-28 13:04:51,799 main.py:51] epoch 1631, training loss: 10947.71, average training loss: 10298.22, base loss: 14320.52
[INFO 2017-06-28 13:04:52,473 main.py:51] epoch 1632, training loss: 10553.81, average training loss: 10299.48, base loss: 14322.98
[INFO 2017-06-28 13:04:53,126 main.py:51] epoch 1633, training loss: 11244.33, average training loss: 10299.15, base loss: 14323.33
[INFO 2017-06-28 13:04:53,780 main.py:51] epoch 1634, training loss: 12687.21, average training loss: 10300.47, base loss: 14326.73
[INFO 2017-06-28 13:04:54,450 main.py:51] epoch 1635, training loss: 9247.79, average training loss: 10299.38, base loss: 14324.50
[INFO 2017-06-28 13:04:55,120 main.py:51] epoch 1636, training loss: 10071.46, average training loss: 10299.59, base loss: 14326.02
[INFO 2017-06-28 13:04:55,778 main.py:51] epoch 1637, training loss: 9590.82, average training loss: 10298.22, base loss: 14324.67
[INFO 2017-06-28 13:04:56,448 main.py:51] epoch 1638, training loss: 10809.49, average training loss: 10298.55, base loss: 14327.28
[INFO 2017-06-28 13:04:57,105 main.py:51] epoch 1639, training loss: 9595.82, average training loss: 10294.88, base loss: 14323.81
[INFO 2017-06-28 13:04:57,774 main.py:51] epoch 1640, training loss: 10181.85, average training loss: 10293.34, base loss: 14323.93
[INFO 2017-06-28 13:04:58,454 main.py:51] epoch 1641, training loss: 10170.22, average training loss: 10292.27, base loss: 14322.87
[INFO 2017-06-28 13:04:59,133 main.py:51] epoch 1642, training loss: 10582.03, average training loss: 10291.80, base loss: 14323.54
[INFO 2017-06-28 13:04:59,780 main.py:51] epoch 1643, training loss: 8538.69, average training loss: 10290.06, base loss: 14322.25
[INFO 2017-06-28 13:05:00,438 main.py:51] epoch 1644, training loss: 10271.58, average training loss: 10289.16, base loss: 14319.86
[INFO 2017-06-28 13:05:01,099 main.py:51] epoch 1645, training loss: 10298.15, average training loss: 10289.58, base loss: 14320.55
[INFO 2017-06-28 13:05:01,764 main.py:51] epoch 1646, training loss: 9277.42, average training loss: 10288.25, base loss: 14321.15
[INFO 2017-06-28 13:05:02,427 main.py:51] epoch 1647, training loss: 11069.06, average training loss: 10288.28, base loss: 14321.04
[INFO 2017-06-28 13:05:03,125 main.py:51] epoch 1648, training loss: 10249.37, average training loss: 10286.93, base loss: 14317.73
[INFO 2017-06-28 13:05:03,780 main.py:51] epoch 1649, training loss: 9533.03, average training loss: 10284.10, base loss: 14314.06
[INFO 2017-06-28 13:05:04,427 main.py:51] epoch 1650, training loss: 8140.70, average training loss: 10281.17, base loss: 14312.16
[INFO 2017-06-28 13:05:05,073 main.py:51] epoch 1651, training loss: 10041.40, average training loss: 10279.35, base loss: 14310.06
[INFO 2017-06-28 13:05:05,730 main.py:51] epoch 1652, training loss: 9938.82, average training loss: 10278.32, base loss: 14309.28
[INFO 2017-06-28 13:05:06,399 main.py:51] epoch 1653, training loss: 8096.97, average training loss: 10275.23, base loss: 14304.29
[INFO 2017-06-28 13:05:07,085 main.py:51] epoch 1654, training loss: 9015.96, average training loss: 10272.94, base loss: 14302.83
[INFO 2017-06-28 13:05:07,755 main.py:51] epoch 1655, training loss: 10872.71, average training loss: 10272.67, base loss: 14302.86
[INFO 2017-06-28 13:05:08,416 main.py:51] epoch 1656, training loss: 9437.50, average training loss: 10269.92, base loss: 14299.95
[INFO 2017-06-28 13:05:09,085 main.py:51] epoch 1657, training loss: 10153.51, average training loss: 10269.53, base loss: 14301.14
[INFO 2017-06-28 13:05:09,750 main.py:51] epoch 1658, training loss: 9380.00, average training loss: 10268.79, base loss: 14301.58
[INFO 2017-06-28 13:05:10,414 main.py:51] epoch 1659, training loss: 9689.64, average training loss: 10267.59, base loss: 14299.99
[INFO 2017-06-28 13:05:11,069 main.py:51] epoch 1660, training loss: 9029.19, average training loss: 10267.14, base loss: 14300.58
[INFO 2017-06-28 13:05:11,719 main.py:51] epoch 1661, training loss: 10660.21, average training loss: 10269.21, base loss: 14304.28
[INFO 2017-06-28 13:05:12,378 main.py:51] epoch 1662, training loss: 10223.37, average training loss: 10269.10, base loss: 14303.99
[INFO 2017-06-28 13:05:13,027 main.py:51] epoch 1663, training loss: 9549.88, average training loss: 10267.58, base loss: 14304.04
[INFO 2017-06-28 13:05:13,675 main.py:51] epoch 1664, training loss: 10394.84, average training loss: 10266.34, base loss: 14302.95
[INFO 2017-06-28 13:05:14,327 main.py:51] epoch 1665, training loss: 9842.32, average training loss: 10265.69, base loss: 14303.52
[INFO 2017-06-28 13:05:15,005 main.py:51] epoch 1666, training loss: 9748.77, average training loss: 10264.98, base loss: 14304.14
[INFO 2017-06-28 13:05:15,654 main.py:51] epoch 1667, training loss: 9300.66, average training loss: 10263.66, base loss: 14303.32
[INFO 2017-06-28 13:05:16,312 main.py:51] epoch 1668, training loss: 9124.62, average training loss: 10262.31, base loss: 14302.40
[INFO 2017-06-28 13:05:16,957 main.py:51] epoch 1669, training loss: 9761.75, average training loss: 10261.41, base loss: 14302.67
[INFO 2017-06-28 13:05:17,593 main.py:51] epoch 1670, training loss: 10589.23, average training loss: 10262.22, base loss: 14305.42
[INFO 2017-06-28 13:05:18,247 main.py:51] epoch 1671, training loss: 9930.26, average training loss: 10262.06, base loss: 14306.87
[INFO 2017-06-28 13:05:18,900 main.py:51] epoch 1672, training loss: 10082.63, average training loss: 10261.62, base loss: 14306.09
[INFO 2017-06-28 13:05:19,550 main.py:51] epoch 1673, training loss: 10908.95, average training loss: 10262.05, base loss: 14309.09
[INFO 2017-06-28 13:05:20,185 main.py:51] epoch 1674, training loss: 7958.37, average training loss: 10260.55, base loss: 14307.30
[INFO 2017-06-28 13:05:20,844 main.py:51] epoch 1675, training loss: 9478.38, average training loss: 10258.33, base loss: 14304.83
[INFO 2017-06-28 13:05:21,504 main.py:51] epoch 1676, training loss: 10130.64, average training loss: 10255.17, base loss: 14302.09
[INFO 2017-06-28 13:05:22,154 main.py:51] epoch 1677, training loss: 8985.62, average training loss: 10253.00, base loss: 14298.99
[INFO 2017-06-28 13:05:22,844 main.py:51] epoch 1678, training loss: 9594.11, average training loss: 10253.06, base loss: 14299.73
[INFO 2017-06-28 13:05:23,481 main.py:51] epoch 1679, training loss: 9337.24, average training loss: 10252.16, base loss: 14299.93
[INFO 2017-06-28 13:05:24,166 main.py:51] epoch 1680, training loss: 8451.66, average training loss: 10249.39, base loss: 14296.31
[INFO 2017-06-28 13:05:24,824 main.py:51] epoch 1681, training loss: 10769.52, average training loss: 10250.16, base loss: 14297.76
[INFO 2017-06-28 13:05:25,491 main.py:51] epoch 1682, training loss: 9575.15, average training loss: 10249.66, base loss: 14297.33
[INFO 2017-06-28 13:05:26,141 main.py:51] epoch 1683, training loss: 10530.61, average training loss: 10249.86, base loss: 14297.52
[INFO 2017-06-28 13:05:26,826 main.py:51] epoch 1684, training loss: 10387.21, average training loss: 10250.57, base loss: 14299.00
[INFO 2017-06-28 13:05:27,471 main.py:51] epoch 1685, training loss: 9303.81, average training loss: 10250.72, base loss: 14298.96
[INFO 2017-06-28 13:05:28,132 main.py:51] epoch 1686, training loss: 10095.85, average training loss: 10251.23, base loss: 14299.95
[INFO 2017-06-28 13:05:28,759 main.py:51] epoch 1687, training loss: 10719.35, average training loss: 10251.27, base loss: 14300.82
[INFO 2017-06-28 13:05:29,418 main.py:51] epoch 1688, training loss: 9501.36, average training loss: 10248.56, base loss: 14296.43
[INFO 2017-06-28 13:05:30,082 main.py:51] epoch 1689, training loss: 8964.55, average training loss: 10247.18, base loss: 14294.85
[INFO 2017-06-28 13:05:30,755 main.py:51] epoch 1690, training loss: 12417.77, average training loss: 10247.15, base loss: 14295.78
[INFO 2017-06-28 13:05:31,410 main.py:51] epoch 1691, training loss: 11898.58, average training loss: 10248.25, base loss: 14299.26
[INFO 2017-06-28 13:05:32,067 main.py:51] epoch 1692, training loss: 10519.38, average training loss: 10248.62, base loss: 14300.87
[INFO 2017-06-28 13:05:32,718 main.py:51] epoch 1693, training loss: 12707.52, average training loss: 10250.67, base loss: 14302.12
[INFO 2017-06-28 13:05:33,406 main.py:51] epoch 1694, training loss: 9126.46, average training loss: 10249.16, base loss: 14299.71
[INFO 2017-06-28 13:05:34,055 main.py:51] epoch 1695, training loss: 10359.88, average training loss: 10245.95, base loss: 14296.12
[INFO 2017-06-28 13:05:34,721 main.py:51] epoch 1696, training loss: 9460.15, average training loss: 10244.30, base loss: 14293.74
[INFO 2017-06-28 13:05:35,395 main.py:51] epoch 1697, training loss: 10352.74, average training loss: 10245.44, base loss: 14295.43
[INFO 2017-06-28 13:05:36,072 main.py:51] epoch 1698, training loss: 9429.78, average training loss: 10244.36, base loss: 14293.96
[INFO 2017-06-28 13:05:36,724 main.py:51] epoch 1699, training loss: 9787.69, average training loss: 10244.00, base loss: 14295.22
[INFO 2017-06-28 13:05:36,724 main.py:53] epoch 1699, testing
[INFO 2017-06-28 13:05:39,301 main.py:105] average testing loss: 11883.61, base loss: 15842.38
[INFO 2017-06-28 13:05:39,301 main.py:106] improve_loss: 3958.78, improve_percent: 0.25
[INFO 2017-06-28 13:05:39,301 main.py:76] current best improved percent: 0.26
[INFO 2017-06-28 13:05:40,008 main.py:51] epoch 1700, training loss: 9391.48, average training loss: 10243.38, base loss: 14294.75
[INFO 2017-06-28 13:05:40,663 main.py:51] epoch 1701, training loss: 10148.77, average training loss: 10243.01, base loss: 14294.76
[INFO 2017-06-28 13:05:41,336 main.py:51] epoch 1702, training loss: 10377.68, average training loss: 10241.99, base loss: 14291.92
[INFO 2017-06-28 13:05:42,016 main.py:51] epoch 1703, training loss: 11445.63, average training loss: 10243.77, base loss: 14294.63
[INFO 2017-06-28 13:05:42,666 main.py:51] epoch 1704, training loss: 9561.22, average training loss: 10240.74, base loss: 14292.02
[INFO 2017-06-28 13:05:43,328 main.py:51] epoch 1705, training loss: 9869.25, average training loss: 10241.59, base loss: 14294.74
[INFO 2017-06-28 13:05:43,974 main.py:51] epoch 1706, training loss: 8672.68, average training loss: 10240.89, base loss: 14292.59
[INFO 2017-06-28 13:05:44,638 main.py:51] epoch 1707, training loss: 9850.67, average training loss: 10242.41, base loss: 14296.17
[INFO 2017-06-28 13:05:45,390 main.py:51] epoch 1708, training loss: 9167.93, average training loss: 10241.77, base loss: 14295.51
[INFO 2017-06-28 13:05:46,080 main.py:51] epoch 1709, training loss: 10707.86, average training loss: 10242.16, base loss: 14296.28
[INFO 2017-06-28 13:05:46,750 main.py:51] epoch 1710, training loss: 9818.82, average training loss: 10242.54, base loss: 14297.77
[INFO 2017-06-28 13:05:47,438 main.py:51] epoch 1711, training loss: 8980.25, average training loss: 10242.04, base loss: 14298.77
[INFO 2017-06-28 13:05:48,086 main.py:51] epoch 1712, training loss: 11836.15, average training loss: 10243.29, base loss: 14301.02
[INFO 2017-06-28 13:05:48,769 main.py:51] epoch 1713, training loss: 9538.50, average training loss: 10243.46, base loss: 14301.37
[INFO 2017-06-28 13:05:49,431 main.py:51] epoch 1714, training loss: 10749.23, average training loss: 10244.32, base loss: 14305.09
[INFO 2017-06-28 13:05:50,082 main.py:51] epoch 1715, training loss: 11537.84, average training loss: 10245.46, base loss: 14306.99
[INFO 2017-06-28 13:05:50,734 main.py:51] epoch 1716, training loss: 11630.57, average training loss: 10247.65, base loss: 14309.14
[INFO 2017-06-28 13:05:51,395 main.py:51] epoch 1717, training loss: 10033.91, average training loss: 10245.72, base loss: 14306.99
[INFO 2017-06-28 13:05:52,061 main.py:51] epoch 1718, training loss: 9197.16, average training loss: 10244.67, base loss: 14305.97
[INFO 2017-06-28 13:05:52,717 main.py:51] epoch 1719, training loss: 10804.42, average training loss: 10245.18, base loss: 14307.26
[INFO 2017-06-28 13:05:53,355 main.py:51] epoch 1720, training loss: 9116.18, average training loss: 10243.08, base loss: 14305.32
[INFO 2017-06-28 13:05:53,998 main.py:51] epoch 1721, training loss: 8297.24, average training loss: 10240.69, base loss: 14301.97
[INFO 2017-06-28 13:05:54,669 main.py:51] epoch 1722, training loss: 9650.87, average training loss: 10240.13, base loss: 14301.35
[INFO 2017-06-28 13:05:55,350 main.py:51] epoch 1723, training loss: 10627.79, average training loss: 10239.35, base loss: 14301.96
[INFO 2017-06-28 13:05:56,003 main.py:51] epoch 1724, training loss: 9225.72, average training loss: 10237.21, base loss: 14297.95
[INFO 2017-06-28 13:05:56,703 main.py:51] epoch 1725, training loss: 9985.53, average training loss: 10237.45, base loss: 14299.39
[INFO 2017-06-28 13:05:57,353 main.py:51] epoch 1726, training loss: 9914.00, average training loss: 10237.68, base loss: 14301.50
[INFO 2017-06-28 13:05:58,024 main.py:51] epoch 1727, training loss: 9489.07, average training loss: 10237.16, base loss: 14300.50
[INFO 2017-06-28 13:05:58,670 main.py:51] epoch 1728, training loss: 11124.44, average training loss: 10236.81, base loss: 14300.50
[INFO 2017-06-28 13:05:59,324 main.py:51] epoch 1729, training loss: 9615.77, average training loss: 10235.52, base loss: 14299.00
[INFO 2017-06-28 13:05:59,996 main.py:51] epoch 1730, training loss: 11323.84, average training loss: 10237.98, base loss: 14302.32
[INFO 2017-06-28 13:06:00,646 main.py:51] epoch 1731, training loss: 11794.14, average training loss: 10239.98, base loss: 14305.29
[INFO 2017-06-28 13:06:01,334 main.py:51] epoch 1732, training loss: 10281.57, average training loss: 10241.24, base loss: 14308.33
[INFO 2017-06-28 13:06:01,988 main.py:51] epoch 1733, training loss: 10414.68, average training loss: 10242.57, base loss: 14312.59
[INFO 2017-06-28 13:06:02,650 main.py:51] epoch 1734, training loss: 9335.16, average training loss: 10241.75, base loss: 14311.03
[INFO 2017-06-28 13:06:03,309 main.py:51] epoch 1735, training loss: 8405.91, average training loss: 10239.91, base loss: 14309.13
[INFO 2017-06-28 13:06:03,983 main.py:51] epoch 1736, training loss: 9012.84, average training loss: 10239.76, base loss: 14309.87
[INFO 2017-06-28 13:06:04,631 main.py:51] epoch 1737, training loss: 9977.57, average training loss: 10238.58, base loss: 14309.43
[INFO 2017-06-28 13:06:05,295 main.py:51] epoch 1738, training loss: 9898.35, average training loss: 10238.10, base loss: 14309.74
[INFO 2017-06-28 13:06:05,967 main.py:51] epoch 1739, training loss: 10723.27, average training loss: 10238.70, base loss: 14313.68
[INFO 2017-06-28 13:06:06,631 main.py:51] epoch 1740, training loss: 9322.09, average training loss: 10237.10, base loss: 14312.27
[INFO 2017-06-28 13:06:07,315 main.py:51] epoch 1741, training loss: 10232.53, average training loss: 10238.99, base loss: 14315.09
[INFO 2017-06-28 13:06:08,004 main.py:51] epoch 1742, training loss: 11174.65, average training loss: 10238.47, base loss: 14315.17
[INFO 2017-06-28 13:06:08,661 main.py:51] epoch 1743, training loss: 9345.24, average training loss: 10236.92, base loss: 14314.92
[INFO 2017-06-28 13:06:09,323 main.py:51] epoch 1744, training loss: 10722.25, average training loss: 10235.11, base loss: 14314.75
[INFO 2017-06-28 13:06:09,984 main.py:51] epoch 1745, training loss: 9309.69, average training loss: 10234.58, base loss: 14314.40
[INFO 2017-06-28 13:06:10,646 main.py:51] epoch 1746, training loss: 9346.11, average training loss: 10232.79, base loss: 14313.19
[INFO 2017-06-28 13:06:11,279 main.py:51] epoch 1747, training loss: 9888.11, average training loss: 10232.33, base loss: 14313.38
[INFO 2017-06-28 13:06:11,949 main.py:51] epoch 1748, training loss: 9493.80, average training loss: 10230.57, base loss: 14310.21
[INFO 2017-06-28 13:06:12,610 main.py:51] epoch 1749, training loss: 9494.18, average training loss: 10230.40, base loss: 14311.30
[INFO 2017-06-28 13:06:13,277 main.py:51] epoch 1750, training loss: 10054.85, average training loss: 10228.76, base loss: 14310.15
[INFO 2017-06-28 13:06:13,953 main.py:51] epoch 1751, training loss: 10621.58, average training loss: 10227.59, base loss: 14308.91
[INFO 2017-06-28 13:06:14,599 main.py:51] epoch 1752, training loss: 9935.15, average training loss: 10227.28, base loss: 14309.16
[INFO 2017-06-28 13:06:15,290 main.py:51] epoch 1753, training loss: 9548.38, average training loss: 10226.21, base loss: 14307.57
[INFO 2017-06-28 13:06:15,957 main.py:51] epoch 1754, training loss: 9383.19, average training loss: 10226.10, base loss: 14307.12
[INFO 2017-06-28 13:06:16,607 main.py:51] epoch 1755, training loss: 9938.25, average training loss: 10224.49, base loss: 14306.24
[INFO 2017-06-28 13:06:17,283 main.py:51] epoch 1756, training loss: 10014.46, average training loss: 10224.37, base loss: 14307.08
[INFO 2017-06-28 13:06:17,936 main.py:51] epoch 1757, training loss: 9670.06, average training loss: 10222.54, base loss: 14305.61
[INFO 2017-06-28 13:06:18,581 main.py:51] epoch 1758, training loss: 9413.30, average training loss: 10221.66, base loss: 14304.60
[INFO 2017-06-28 13:06:19,256 main.py:51] epoch 1759, training loss: 8936.73, average training loss: 10222.04, base loss: 14305.86
[INFO 2017-06-28 13:06:19,897 main.py:51] epoch 1760, training loss: 11664.00, average training loss: 10223.76, base loss: 14307.46
[INFO 2017-06-28 13:06:20,550 main.py:51] epoch 1761, training loss: 10129.01, average training loss: 10223.28, base loss: 14305.76
[INFO 2017-06-28 13:06:21,223 main.py:51] epoch 1762, training loss: 9299.15, average training loss: 10222.47, base loss: 14305.71
[INFO 2017-06-28 13:06:21,879 main.py:51] epoch 1763, training loss: 9901.40, average training loss: 10222.18, base loss: 14304.48
[INFO 2017-06-28 13:06:22,563 main.py:51] epoch 1764, training loss: 9989.70, average training loss: 10222.16, base loss: 14304.72
[INFO 2017-06-28 13:06:23,233 main.py:51] epoch 1765, training loss: 10264.42, average training loss: 10222.47, base loss: 14305.52
[INFO 2017-06-28 13:06:23,909 main.py:51] epoch 1766, training loss: 10381.89, average training loss: 10221.44, base loss: 14304.38
[INFO 2017-06-28 13:06:24,567 main.py:51] epoch 1767, training loss: 9575.41, average training loss: 10221.50, base loss: 14304.84
[INFO 2017-06-28 13:06:25,213 main.py:51] epoch 1768, training loss: 10148.57, average training loss: 10221.24, base loss: 14304.46
[INFO 2017-06-28 13:06:25,866 main.py:51] epoch 1769, training loss: 10544.56, average training loss: 10220.57, base loss: 14305.35
[INFO 2017-06-28 13:06:26,530 main.py:51] epoch 1770, training loss: 9183.78, average training loss: 10218.72, base loss: 14304.26
[INFO 2017-06-28 13:06:27,193 main.py:51] epoch 1771, training loss: 9436.97, average training loss: 10218.13, base loss: 14303.47
[INFO 2017-06-28 13:06:27,857 main.py:51] epoch 1772, training loss: 10566.11, average training loss: 10218.39, base loss: 14304.44
[INFO 2017-06-28 13:06:28,512 main.py:51] epoch 1773, training loss: 9072.46, average training loss: 10218.09, base loss: 14304.81
[INFO 2017-06-28 13:06:29,345 main.py:51] epoch 1774, training loss: 11375.58, average training loss: 10219.10, base loss: 14305.88
[INFO 2017-06-28 13:06:30,129 main.py:51] epoch 1775, training loss: 10275.61, average training loss: 10218.27, base loss: 14304.17
[INFO 2017-06-28 13:06:30,867 main.py:51] epoch 1776, training loss: 9194.16, average training loss: 10217.82, base loss: 14305.26
[INFO 2017-06-28 13:06:31,535 main.py:51] epoch 1777, training loss: 10514.47, average training loss: 10218.99, base loss: 14307.67
[INFO 2017-06-28 13:06:32,191 main.py:51] epoch 1778, training loss: 12520.21, average training loss: 10221.29, base loss: 14311.51
[INFO 2017-06-28 13:06:32,845 main.py:51] epoch 1779, training loss: 9105.14, average training loss: 10220.46, base loss: 14311.14
[INFO 2017-06-28 13:06:33,504 main.py:51] epoch 1780, training loss: 11597.83, average training loss: 10221.64, base loss: 14314.37
[INFO 2017-06-28 13:06:34,158 main.py:51] epoch 1781, training loss: 9893.80, average training loss: 10220.74, base loss: 14313.09
[INFO 2017-06-28 13:06:34,828 main.py:51] epoch 1782, training loss: 9446.94, average training loss: 10218.25, base loss: 14310.70
[INFO 2017-06-28 13:06:35,480 main.py:51] epoch 1783, training loss: 10546.93, average training loss: 10219.11, base loss: 14312.44
[INFO 2017-06-28 13:06:36,118 main.py:51] epoch 1784, training loss: 9588.03, average training loss: 10217.76, base loss: 14313.32
[INFO 2017-06-28 13:06:36,790 main.py:51] epoch 1785, training loss: 10163.66, average training loss: 10216.45, base loss: 14312.54
[INFO 2017-06-28 13:06:37,468 main.py:51] epoch 1786, training loss: 9307.79, average training loss: 10216.19, base loss: 14312.77
[INFO 2017-06-28 13:06:38,120 main.py:51] epoch 1787, training loss: 11340.89, average training loss: 10217.70, base loss: 14317.03
[INFO 2017-06-28 13:06:38,760 main.py:51] epoch 1788, training loss: 9675.30, average training loss: 10216.84, base loss: 14316.89
[INFO 2017-06-28 13:06:39,435 main.py:51] epoch 1789, training loss: 11090.27, average training loss: 10219.20, base loss: 14321.94
[INFO 2017-06-28 13:06:40,092 main.py:51] epoch 1790, training loss: 8835.20, average training loss: 10215.92, base loss: 14317.22
[INFO 2017-06-28 13:06:40,760 main.py:51] epoch 1791, training loss: 9875.83, average training loss: 10213.14, base loss: 14315.25
[INFO 2017-06-28 13:06:41,412 main.py:51] epoch 1792, training loss: 10080.99, average training loss: 10212.28, base loss: 14315.10
[INFO 2017-06-28 13:06:42,075 main.py:51] epoch 1793, training loss: 9323.46, average training loss: 10210.64, base loss: 14314.05
[INFO 2017-06-28 13:06:42,730 main.py:51] epoch 1794, training loss: 10903.71, average training loss: 10212.06, base loss: 14318.73
[INFO 2017-06-28 13:06:43,399 main.py:51] epoch 1795, training loss: 8570.14, average training loss: 10209.94, base loss: 14316.51
[INFO 2017-06-28 13:06:44,061 main.py:51] epoch 1796, training loss: 10022.12, average training loss: 10208.79, base loss: 14316.17
[INFO 2017-06-28 13:06:44,731 main.py:51] epoch 1797, training loss: 10539.32, average training loss: 10208.24, base loss: 14315.16
[INFO 2017-06-28 13:06:45,399 main.py:51] epoch 1798, training loss: 11453.84, average training loss: 10207.16, base loss: 14315.70
[INFO 2017-06-28 13:06:46,062 main.py:51] epoch 1799, training loss: 10099.00, average training loss: 10206.27, base loss: 14315.66
[INFO 2017-06-28 13:06:46,063 main.py:53] epoch 1799, testing
[INFO 2017-06-28 13:06:48,673 main.py:105] average testing loss: 10605.19, base loss: 14718.04
[INFO 2017-06-28 13:06:48,673 main.py:106] improve_loss: 4112.85, improve_percent: 0.28
[INFO 2017-06-28 13:06:48,674 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 13:06:48,710 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:06:49,369 main.py:51] epoch 1800, training loss: 10576.17, average training loss: 10207.33, base loss: 14318.77
[INFO 2017-06-28 13:06:50,049 main.py:51] epoch 1801, training loss: 9273.22, average training loss: 10205.18, base loss: 14316.96
[INFO 2017-06-28 13:06:50,705 main.py:51] epoch 1802, training loss: 10493.35, average training loss: 10205.94, base loss: 14320.21
[INFO 2017-06-28 13:06:51,361 main.py:51] epoch 1803, training loss: 11790.62, average training loss: 10207.18, base loss: 14322.65
[INFO 2017-06-28 13:06:52,019 main.py:51] epoch 1804, training loss: 10968.67, average training loss: 10206.56, base loss: 14322.02
[INFO 2017-06-28 13:06:52,673 main.py:51] epoch 1805, training loss: 8918.27, average training loss: 10204.08, base loss: 14320.45
[INFO 2017-06-28 13:06:53,333 main.py:51] epoch 1806, training loss: 11959.35, average training loss: 10206.55, base loss: 14324.93
[INFO 2017-06-28 13:06:54,006 main.py:51] epoch 1807, training loss: 9836.22, average training loss: 10206.51, base loss: 14323.76
[INFO 2017-06-28 13:06:54,673 main.py:51] epoch 1808, training loss: 11843.54, average training loss: 10208.22, base loss: 14328.44
[INFO 2017-06-28 13:06:55,328 main.py:51] epoch 1809, training loss: 8532.45, average training loss: 10205.10, base loss: 14325.40
[INFO 2017-06-28 13:06:55,971 main.py:51] epoch 1810, training loss: 11022.64, average training loss: 10205.85, base loss: 14327.88
[INFO 2017-06-28 13:06:56,618 main.py:51] epoch 1811, training loss: 9655.19, average training loss: 10206.20, base loss: 14328.93
[INFO 2017-06-28 13:06:57,273 main.py:51] epoch 1812, training loss: 10121.01, average training loss: 10207.08, base loss: 14332.67
[INFO 2017-06-28 13:06:57,935 main.py:51] epoch 1813, training loss: 10126.74, average training loss: 10208.41, base loss: 14336.62
[INFO 2017-06-28 13:06:58,609 main.py:51] epoch 1814, training loss: 9663.65, average training loss: 10207.86, base loss: 14338.01
[INFO 2017-06-28 13:06:59,282 main.py:51] epoch 1815, training loss: 9601.45, average training loss: 10206.66, base loss: 14337.35
[INFO 2017-06-28 13:06:59,948 main.py:51] epoch 1816, training loss: 9732.98, average training loss: 10206.96, base loss: 14338.68
[INFO 2017-06-28 13:07:00,640 main.py:51] epoch 1817, training loss: 10063.18, average training loss: 10206.61, base loss: 14339.36
[INFO 2017-06-28 13:07:01,310 main.py:51] epoch 1818, training loss: 9189.36, average training loss: 10205.03, base loss: 14338.27
[INFO 2017-06-28 13:07:02,000 main.py:51] epoch 1819, training loss: 11081.09, average training loss: 10205.34, base loss: 14338.84
[INFO 2017-06-28 13:07:02,797 main.py:51] epoch 1820, training loss: 9198.37, average training loss: 10205.10, base loss: 14338.09
[INFO 2017-06-28 13:07:03,564 main.py:51] epoch 1821, training loss: 9398.51, average training loss: 10205.53, base loss: 14340.31
[INFO 2017-06-28 13:07:04,328 main.py:51] epoch 1822, training loss: 10616.14, average training loss: 10205.98, base loss: 14341.25
[INFO 2017-06-28 13:07:05,105 main.py:51] epoch 1823, training loss: 10055.61, average training loss: 10204.70, base loss: 14340.10
[INFO 2017-06-28 13:07:05,991 main.py:51] epoch 1824, training loss: 9947.77, average training loss: 10203.69, base loss: 14339.50
[INFO 2017-06-28 13:07:06,975 main.py:51] epoch 1825, training loss: 9792.42, average training loss: 10202.63, base loss: 14338.46
[INFO 2017-06-28 13:07:07,943 main.py:51] epoch 1826, training loss: 8990.52, average training loss: 10201.26, base loss: 14337.27
[INFO 2017-06-28 13:07:08,944 main.py:51] epoch 1827, training loss: 9563.60, average training loss: 10200.21, base loss: 14336.95
[INFO 2017-06-28 13:07:09,832 main.py:51] epoch 1828, training loss: 10395.96, average training loss: 10200.22, base loss: 14335.41
[INFO 2017-06-28 13:07:10,685 main.py:51] epoch 1829, training loss: 10320.52, average training loss: 10199.53, base loss: 14334.57
[INFO 2017-06-28 13:07:11,626 main.py:51] epoch 1830, training loss: 11511.23, average training loss: 10201.27, base loss: 14336.35
[INFO 2017-06-28 13:07:12,643 main.py:51] epoch 1831, training loss: 9235.91, average training loss: 10201.05, base loss: 14334.53
[INFO 2017-06-28 13:07:13,533 main.py:51] epoch 1832, training loss: 9228.44, average training loss: 10199.33, base loss: 14333.83
[INFO 2017-06-28 13:07:14,396 main.py:51] epoch 1833, training loss: 8482.89, average training loss: 10197.19, base loss: 14330.52
[INFO 2017-06-28 13:07:15,285 main.py:51] epoch 1834, training loss: 8972.84, average training loss: 10194.92, base loss: 14328.29
[INFO 2017-06-28 13:07:16,112 main.py:51] epoch 1835, training loss: 10379.50, average training loss: 10195.72, base loss: 14329.26
[INFO 2017-06-28 13:07:16,987 main.py:51] epoch 1836, training loss: 10574.98, average training loss: 10197.32, base loss: 14332.22
[INFO 2017-06-28 13:07:17,842 main.py:51] epoch 1837, training loss: 10323.95, average training loss: 10197.47, base loss: 14333.60
[INFO 2017-06-28 13:07:18,722 main.py:51] epoch 1838, training loss: 10276.94, average training loss: 10197.54, base loss: 14333.66
[INFO 2017-06-28 13:07:19,578 main.py:51] epoch 1839, training loss: 10925.95, average training loss: 10198.25, base loss: 14334.41
[INFO 2017-06-28 13:07:20,468 main.py:51] epoch 1840, training loss: 10458.52, average training loss: 10198.49, base loss: 14333.33
[INFO 2017-06-28 13:07:21,407 main.py:51] epoch 1841, training loss: 10774.38, average training loss: 10199.10, base loss: 14335.04
[INFO 2017-06-28 13:07:22,398 main.py:51] epoch 1842, training loss: 8352.32, average training loss: 10196.72, base loss: 14331.88
[INFO 2017-06-28 13:07:23,373 main.py:51] epoch 1843, training loss: 8260.57, average training loss: 10194.24, base loss: 14329.50
[INFO 2017-06-28 13:07:24,330 main.py:51] epoch 1844, training loss: 9798.24, average training loss: 10194.01, base loss: 14328.93
[INFO 2017-06-28 13:07:25,179 main.py:51] epoch 1845, training loss: 10614.49, average training loss: 10194.69, base loss: 14331.12
[INFO 2017-06-28 13:07:26,155 main.py:51] epoch 1846, training loss: 9358.70, average training loss: 10193.93, base loss: 14330.42
[INFO 2017-06-28 13:07:27,041 main.py:51] epoch 1847, training loss: 9344.27, average training loss: 10192.65, base loss: 14330.41
[INFO 2017-06-28 13:07:27,908 main.py:51] epoch 1848, training loss: 9915.83, average training loss: 10192.59, base loss: 14332.46
[INFO 2017-06-28 13:07:28,772 main.py:51] epoch 1849, training loss: 9881.71, average training loss: 10189.69, base loss: 14329.26
[INFO 2017-06-28 13:07:29,630 main.py:51] epoch 1850, training loss: 9134.92, average training loss: 10189.96, base loss: 14330.80
[INFO 2017-06-28 13:07:30,507 main.py:51] epoch 1851, training loss: 10020.17, average training loss: 10190.11, base loss: 14332.31
[INFO 2017-06-28 13:07:31,343 main.py:51] epoch 1852, training loss: 9348.91, average training loss: 10190.36, base loss: 14335.73
[INFO 2017-06-28 13:07:32,334 main.py:51] epoch 1853, training loss: 10121.03, average training loss: 10189.30, base loss: 14333.69
[INFO 2017-06-28 13:07:33,266 main.py:51] epoch 1854, training loss: 8696.98, average training loss: 10188.42, base loss: 14332.95
[INFO 2017-06-28 13:07:34,302 main.py:51] epoch 1855, training loss: 9268.79, average training loss: 10187.15, base loss: 14331.92
[INFO 2017-06-28 13:07:35,288 main.py:51] epoch 1856, training loss: 9387.55, average training loss: 10185.10, base loss: 14330.68
[INFO 2017-06-28 13:07:36,163 main.py:51] epoch 1857, training loss: 10253.66, average training loss: 10185.74, base loss: 14332.13
[INFO 2017-06-28 13:07:37,008 main.py:51] epoch 1858, training loss: 10462.77, average training loss: 10185.48, base loss: 14331.46
[INFO 2017-06-28 13:07:37,918 main.py:51] epoch 1859, training loss: 8951.96, average training loss: 10185.27, base loss: 14332.17
[INFO 2017-06-28 13:07:38,865 main.py:51] epoch 1860, training loss: 10191.34, average training loss: 10185.18, base loss: 14331.21
[INFO 2017-06-28 13:07:39,781 main.py:51] epoch 1861, training loss: 11387.85, average training loss: 10187.31, base loss: 14336.41
[INFO 2017-06-28 13:07:40,743 main.py:51] epoch 1862, training loss: 11425.64, average training loss: 10186.80, base loss: 14335.99
[INFO 2017-06-28 13:07:41,674 main.py:51] epoch 1863, training loss: 9188.08, average training loss: 10185.95, base loss: 14336.06
[INFO 2017-06-28 13:07:42,522 main.py:51] epoch 1864, training loss: 9081.14, average training loss: 10183.46, base loss: 14333.14
[INFO 2017-06-28 13:07:43,435 main.py:51] epoch 1865, training loss: 10574.68, average training loss: 10184.28, base loss: 14333.90
[INFO 2017-06-28 13:07:44,353 main.py:51] epoch 1866, training loss: 11410.90, average training loss: 10186.19, base loss: 14336.02
[INFO 2017-06-28 13:07:45,272 main.py:51] epoch 1867, training loss: 10377.94, average training loss: 10186.46, base loss: 14337.83
[INFO 2017-06-28 13:07:46,154 main.py:51] epoch 1868, training loss: 9528.98, average training loss: 10185.86, base loss: 14337.70
[INFO 2017-06-28 13:07:47,055 main.py:51] epoch 1869, training loss: 10994.68, average training loss: 10188.21, base loss: 14341.93
[INFO 2017-06-28 13:07:47,982 main.py:51] epoch 1870, training loss: 8715.30, average training loss: 10186.53, base loss: 14341.09
[INFO 2017-06-28 13:07:48,831 main.py:51] epoch 1871, training loss: 10612.25, average training loss: 10188.33, base loss: 14344.60
[INFO 2017-06-28 13:07:49,661 main.py:51] epoch 1872, training loss: 10696.44, average training loss: 10188.37, base loss: 14346.92
[INFO 2017-06-28 13:07:50,533 main.py:51] epoch 1873, training loss: 8803.62, average training loss: 10186.12, base loss: 14344.79
[INFO 2017-06-28 13:07:51,520 main.py:51] epoch 1874, training loss: 9792.42, average training loss: 10184.84, base loss: 14344.01
[INFO 2017-06-28 13:07:52,382 main.py:51] epoch 1875, training loss: 10273.69, average training loss: 10184.19, base loss: 14343.94
[INFO 2017-06-28 13:07:53,309 main.py:51] epoch 1876, training loss: 9671.14, average training loss: 10182.33, base loss: 14342.06
[INFO 2017-06-28 13:07:54,183 main.py:51] epoch 1877, training loss: 9641.65, average training loss: 10180.54, base loss: 14340.51
[INFO 2017-06-28 13:07:55,027 main.py:51] epoch 1878, training loss: 9306.00, average training loss: 10177.83, base loss: 14337.21
[INFO 2017-06-28 13:07:55,899 main.py:51] epoch 1879, training loss: 10606.37, average training loss: 10178.56, base loss: 14339.42
[INFO 2017-06-28 13:07:56,830 main.py:51] epoch 1880, training loss: 10922.85, average training loss: 10179.10, base loss: 14341.34
[INFO 2017-06-28 13:07:57,738 main.py:51] epoch 1881, training loss: 10594.20, average training loss: 10178.73, base loss: 14340.94
[INFO 2017-06-28 13:07:58,645 main.py:51] epoch 1882, training loss: 10511.42, average training loss: 10178.29, base loss: 14341.23
[INFO 2017-06-28 13:07:59,560 main.py:51] epoch 1883, training loss: 9839.34, average training loss: 10176.96, base loss: 14340.00
[INFO 2017-06-28 13:08:00,402 main.py:51] epoch 1884, training loss: 10129.88, average training loss: 10177.75, base loss: 14341.10
[INFO 2017-06-28 13:08:01,359 main.py:51] epoch 1885, training loss: 9388.29, average training loss: 10177.53, base loss: 14341.71
[INFO 2017-06-28 13:08:02,259 main.py:51] epoch 1886, training loss: 10168.49, average training loss: 10176.14, base loss: 14339.59
[INFO 2017-06-28 13:08:03,153 main.py:51] epoch 1887, training loss: 8690.88, average training loss: 10173.60, base loss: 14338.35
[INFO 2017-06-28 13:08:04,028 main.py:51] epoch 1888, training loss: 10193.36, average training loss: 10172.84, base loss: 14337.53
[INFO 2017-06-28 13:08:04,917 main.py:51] epoch 1889, training loss: 10573.93, average training loss: 10172.31, base loss: 14336.11
[INFO 2017-06-28 13:08:05,747 main.py:51] epoch 1890, training loss: 12627.71, average training loss: 10174.72, base loss: 14339.57
[INFO 2017-06-28 13:08:06,650 main.py:51] epoch 1891, training loss: 9899.61, average training loss: 10171.17, base loss: 14336.65
[INFO 2017-06-28 13:08:07,577 main.py:51] epoch 1892, training loss: 9219.54, average training loss: 10171.28, base loss: 14336.82
[INFO 2017-06-28 13:08:08,443 main.py:51] epoch 1893, training loss: 10992.38, average training loss: 10171.67, base loss: 14339.35
[INFO 2017-06-28 13:08:09,296 main.py:51] epoch 1894, training loss: 9877.16, average training loss: 10172.02, base loss: 14339.53
[INFO 2017-06-28 13:08:10,195 main.py:51] epoch 1895, training loss: 10458.72, average training loss: 10170.45, base loss: 14339.41
[INFO 2017-06-28 13:08:11,017 main.py:51] epoch 1896, training loss: 9258.37, average training loss: 10168.96, base loss: 14338.75
[INFO 2017-06-28 13:08:11,875 main.py:51] epoch 1897, training loss: 9776.52, average training loss: 10168.52, base loss: 14339.57
[INFO 2017-06-28 13:08:12,779 main.py:51] epoch 1898, training loss: 8927.97, average training loss: 10166.02, base loss: 14335.97
[INFO 2017-06-28 13:08:13,642 main.py:51] epoch 1899, training loss: 9147.83, average training loss: 10165.07, base loss: 14334.24
[INFO 2017-06-28 13:08:13,644 main.py:53] epoch 1899, testing
[INFO 2017-06-28 13:08:17,323 main.py:105] average testing loss: 11200.47, base loss: 15231.25
[INFO 2017-06-28 13:08:17,323 main.py:106] improve_loss: 4030.78, improve_percent: 0.26
[INFO 2017-06-28 13:08:17,324 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:08:18,004 main.py:51] epoch 1900, training loss: 10741.83, average training loss: 10164.60, base loss: 14334.67
[INFO 2017-06-28 13:08:18,655 main.py:51] epoch 1901, training loss: 11321.94, average training loss: 10165.62, base loss: 14337.30
[INFO 2017-06-28 13:08:19,291 main.py:51] epoch 1902, training loss: 10804.26, average training loss: 10166.60, base loss: 14340.00
[INFO 2017-06-28 13:08:19,933 main.py:51] epoch 1903, training loss: 9750.53, average training loss: 10166.15, base loss: 14338.85
[INFO 2017-06-28 13:08:20,603 main.py:51] epoch 1904, training loss: 8791.18, average training loss: 10165.05, base loss: 14336.07
[INFO 2017-06-28 13:08:21,255 main.py:51] epoch 1905, training loss: 10011.47, average training loss: 10165.51, base loss: 14339.65
[INFO 2017-06-28 13:08:21,909 main.py:51] epoch 1906, training loss: 10556.85, average training loss: 10167.30, base loss: 14342.91
[INFO 2017-06-28 13:08:22,577 main.py:51] epoch 1907, training loss: 12011.75, average training loss: 10169.81, base loss: 14346.79
[INFO 2017-06-28 13:08:23,240 main.py:51] epoch 1908, training loss: 10967.63, average training loss: 10170.03, base loss: 14350.82
[INFO 2017-06-28 13:08:23,883 main.py:51] epoch 1909, training loss: 9771.48, average training loss: 10170.51, base loss: 14350.82
[INFO 2017-06-28 13:08:24,533 main.py:51] epoch 1910, training loss: 11882.49, average training loss: 10169.23, base loss: 14349.83
[INFO 2017-06-28 13:08:25,194 main.py:51] epoch 1911, training loss: 9810.37, average training loss: 10169.12, base loss: 14349.46
[INFO 2017-06-28 13:08:25,843 main.py:51] epoch 1912, training loss: 9753.09, average training loss: 10166.66, base loss: 14346.77
[INFO 2017-06-28 13:08:26,514 main.py:51] epoch 1913, training loss: 9342.87, average training loss: 10164.76, base loss: 14345.69
[INFO 2017-06-28 13:08:27,195 main.py:51] epoch 1914, training loss: 10993.69, average training loss: 10165.08, base loss: 14346.37
[INFO 2017-06-28 13:08:27,887 main.py:51] epoch 1915, training loss: 11120.50, average training loss: 10164.05, base loss: 14343.48
[INFO 2017-06-28 13:08:28,548 main.py:51] epoch 1916, training loss: 9129.13, average training loss: 10162.45, base loss: 14341.37
[INFO 2017-06-28 13:08:29,229 main.py:51] epoch 1917, training loss: 8988.04, average training loss: 10161.47, base loss: 14341.10
[INFO 2017-06-28 13:08:29,875 main.py:51] epoch 1918, training loss: 10044.23, average training loss: 10161.15, base loss: 14339.49
[INFO 2017-06-28 13:08:30,542 main.py:51] epoch 1919, training loss: 14816.31, average training loss: 10165.49, base loss: 14345.91
[INFO 2017-06-28 13:08:31,212 main.py:51] epoch 1920, training loss: 9429.15, average training loss: 10164.00, base loss: 14345.22
[INFO 2017-06-28 13:08:31,881 main.py:51] epoch 1921, training loss: 10744.61, average training loss: 10164.07, base loss: 14346.29
[INFO 2017-06-28 13:08:32,573 main.py:51] epoch 1922, training loss: 9442.00, average training loss: 10161.69, base loss: 14342.31
[INFO 2017-06-28 13:08:33,253 main.py:51] epoch 1923, training loss: 12666.51, average training loss: 10164.17, base loss: 14345.34
[INFO 2017-06-28 13:08:33,899 main.py:51] epoch 1924, training loss: 11622.81, average training loss: 10165.17, base loss: 14346.28
[INFO 2017-06-28 13:08:34,546 main.py:51] epoch 1925, training loss: 9618.53, average training loss: 10165.42, base loss: 14348.01
[INFO 2017-06-28 13:08:35,199 main.py:51] epoch 1926, training loss: 9727.16, average training loss: 10164.79, base loss: 14348.70
[INFO 2017-06-28 13:08:35,840 main.py:51] epoch 1927, training loss: 10060.45, average training loss: 10166.27, base loss: 14351.93
[INFO 2017-06-28 13:08:36,490 main.py:51] epoch 1928, training loss: 10501.11, average training loss: 10166.09, base loss: 14351.85
[INFO 2017-06-28 13:08:37,140 main.py:51] epoch 1929, training loss: 9602.11, average training loss: 10166.00, base loss: 14352.43
[INFO 2017-06-28 13:08:37,805 main.py:51] epoch 1930, training loss: 10079.78, average training loss: 10164.38, base loss: 14350.13
[INFO 2017-06-28 13:08:38,485 main.py:51] epoch 1931, training loss: 10211.93, average training loss: 10162.44, base loss: 14348.21
[INFO 2017-06-28 13:08:39,143 main.py:51] epoch 1932, training loss: 9762.44, average training loss: 10162.87, base loss: 14348.71
[INFO 2017-06-28 13:08:39,835 main.py:51] epoch 1933, training loss: 9738.43, average training loss: 10162.16, base loss: 14348.40
[INFO 2017-06-28 13:08:40,491 main.py:51] epoch 1934, training loss: 9374.17, average training loss: 10158.76, base loss: 14343.66
[INFO 2017-06-28 13:08:41,161 main.py:51] epoch 1935, training loss: 10257.20, average training loss: 10159.28, base loss: 14342.83
[INFO 2017-06-28 13:08:41,822 main.py:51] epoch 1936, training loss: 10507.69, average training loss: 10158.51, base loss: 14342.01
[INFO 2017-06-28 13:08:42,483 main.py:51] epoch 1937, training loss: 9896.41, average training loss: 10156.85, base loss: 14340.80
[INFO 2017-06-28 13:08:43,131 main.py:51] epoch 1938, training loss: 11096.24, average training loss: 10157.32, base loss: 14342.14
[INFO 2017-06-28 13:08:43,784 main.py:51] epoch 1939, training loss: 10351.32, average training loss: 10156.23, base loss: 14341.44
[INFO 2017-06-28 13:08:44,428 main.py:51] epoch 1940, training loss: 8968.78, average training loss: 10154.80, base loss: 14340.68
[INFO 2017-06-28 13:08:45,130 main.py:51] epoch 1941, training loss: 8790.83, average training loss: 10153.33, base loss: 14339.35
[INFO 2017-06-28 13:08:45,783 main.py:51] epoch 1942, training loss: 9546.98, average training loss: 10151.34, base loss: 14336.56
[INFO 2017-06-28 13:08:46,456 main.py:51] epoch 1943, training loss: 11419.36, average training loss: 10150.76, base loss: 14338.58
[INFO 2017-06-28 13:08:47,109 main.py:51] epoch 1944, training loss: 12152.37, average training loss: 10153.61, base loss: 14343.15
[INFO 2017-06-28 13:08:47,766 main.py:51] epoch 1945, training loss: 11312.34, average training loss: 10155.90, base loss: 14347.10
[INFO 2017-06-28 13:08:48,419 main.py:51] epoch 1946, training loss: 10986.63, average training loss: 10156.98, base loss: 14348.86
[INFO 2017-06-28 13:08:49,063 main.py:51] epoch 1947, training loss: 10050.10, average training loss: 10157.43, base loss: 14349.83
[INFO 2017-06-28 13:08:49,725 main.py:51] epoch 1948, training loss: 9673.99, average training loss: 10156.59, base loss: 14348.59
[INFO 2017-06-28 13:08:50,397 main.py:51] epoch 1949, training loss: 9184.73, average training loss: 10156.80, base loss: 14350.27
[INFO 2017-06-28 13:08:51,073 main.py:51] epoch 1950, training loss: 8731.79, average training loss: 10157.00, base loss: 14352.31
[INFO 2017-06-28 13:08:51,746 main.py:51] epoch 1951, training loss: 12882.91, average training loss: 10159.14, base loss: 14355.76
[INFO 2017-06-28 13:08:52,418 main.py:51] epoch 1952, training loss: 9426.23, average training loss: 10158.94, base loss: 14356.53
[INFO 2017-06-28 13:08:53,102 main.py:51] epoch 1953, training loss: 10178.98, average training loss: 10158.24, base loss: 14355.86
[INFO 2017-06-28 13:08:53,759 main.py:51] epoch 1954, training loss: 11629.75, average training loss: 10159.01, base loss: 14357.40
[INFO 2017-06-28 13:08:54,438 main.py:51] epoch 1955, training loss: 10413.38, average training loss: 10158.50, base loss: 14356.08
[INFO 2017-06-28 13:08:55,110 main.py:51] epoch 1956, training loss: 9032.53, average training loss: 10157.21, base loss: 14353.95
[INFO 2017-06-28 13:08:55,770 main.py:51] epoch 1957, training loss: 9342.83, average training loss: 10157.60, base loss: 14355.41
[INFO 2017-06-28 13:08:56,446 main.py:51] epoch 1958, training loss: 10716.09, average training loss: 10158.07, base loss: 14356.63
[INFO 2017-06-28 13:08:57,102 main.py:51] epoch 1959, training loss: 9689.04, average training loss: 10157.52, base loss: 14355.48
[INFO 2017-06-28 13:08:57,777 main.py:51] epoch 1960, training loss: 12009.98, average training loss: 10160.63, base loss: 14358.87
[INFO 2017-06-28 13:08:58,450 main.py:51] epoch 1961, training loss: 9498.66, average training loss: 10158.35, base loss: 14355.90
[INFO 2017-06-28 13:08:59,118 main.py:51] epoch 1962, training loss: 8756.74, average training loss: 10153.34, base loss: 14349.32
[INFO 2017-06-28 13:08:59,771 main.py:51] epoch 1963, training loss: 8561.30, average training loss: 10151.85, base loss: 14347.07
[INFO 2017-06-28 13:09:00,417 main.py:51] epoch 1964, training loss: 10561.45, average training loss: 10151.05, base loss: 14346.38
[INFO 2017-06-28 13:09:01,074 main.py:51] epoch 1965, training loss: 9642.95, average training loss: 10151.73, base loss: 14347.90
[INFO 2017-06-28 13:09:01,732 main.py:51] epoch 1966, training loss: 11434.50, average training loss: 10152.53, base loss: 14348.46
[INFO 2017-06-28 13:09:02,407 main.py:51] epoch 1967, training loss: 9964.05, average training loss: 10152.20, base loss: 14350.04
[INFO 2017-06-28 13:09:03,068 main.py:51] epoch 1968, training loss: 9524.56, average training loss: 10150.49, base loss: 14349.55
[INFO 2017-06-28 13:09:03,747 main.py:51] epoch 1969, training loss: 9454.60, average training loss: 10150.04, base loss: 14350.57
[INFO 2017-06-28 13:09:04,427 main.py:51] epoch 1970, training loss: 9006.75, average training loss: 10148.84, base loss: 14349.35
[INFO 2017-06-28 13:09:05,091 main.py:51] epoch 1971, training loss: 12248.38, average training loss: 10150.97, base loss: 14352.46
[INFO 2017-06-28 13:09:05,758 main.py:51] epoch 1972, training loss: 10466.27, average training loss: 10149.88, base loss: 14352.43
[INFO 2017-06-28 13:09:06,427 main.py:51] epoch 1973, training loss: 9466.35, average training loss: 10147.81, base loss: 14350.42
[INFO 2017-06-28 13:09:07,085 main.py:51] epoch 1974, training loss: 9255.08, average training loss: 10147.16, base loss: 14349.87
[INFO 2017-06-28 13:09:07,737 main.py:51] epoch 1975, training loss: 12661.25, average training loss: 10149.74, base loss: 14354.67
[INFO 2017-06-28 13:09:08,392 main.py:51] epoch 1976, training loss: 10162.85, average training loss: 10150.62, base loss: 14356.87
[INFO 2017-06-28 13:09:09,048 main.py:51] epoch 1977, training loss: 11062.47, average training loss: 10152.12, base loss: 14361.16
[INFO 2017-06-28 13:09:09,695 main.py:51] epoch 1978, training loss: 8968.06, average training loss: 10151.56, base loss: 14361.27
[INFO 2017-06-28 13:09:10,362 main.py:51] epoch 1979, training loss: 11464.41, average training loss: 10153.26, base loss: 14362.79
[INFO 2017-06-28 13:09:11,043 main.py:51] epoch 1980, training loss: 10568.79, average training loss: 10153.78, base loss: 14365.53
[INFO 2017-06-28 13:09:11,707 main.py:51] epoch 1981, training loss: 10068.60, average training loss: 10154.08, base loss: 14366.06
[INFO 2017-06-28 13:09:12,367 main.py:51] epoch 1982, training loss: 9657.17, average training loss: 10154.68, base loss: 14367.60
[INFO 2017-06-28 13:09:13,034 main.py:51] epoch 1983, training loss: 9978.21, average training loss: 10154.18, base loss: 14368.53
[INFO 2017-06-28 13:09:13,704 main.py:51] epoch 1984, training loss: 9791.11, average training loss: 10153.22, base loss: 14366.63
[INFO 2017-06-28 13:09:14,369 main.py:51] epoch 1985, training loss: 9110.16, average training loss: 10152.70, base loss: 14367.17
[INFO 2017-06-28 13:09:15,027 main.py:51] epoch 1986, training loss: 10245.48, average training loss: 10153.03, base loss: 14368.63
[INFO 2017-06-28 13:09:15,677 main.py:51] epoch 1987, training loss: 9570.54, average training loss: 10151.30, base loss: 14368.14
[INFO 2017-06-28 13:09:16,338 main.py:51] epoch 1988, training loss: 9449.52, average training loss: 10151.26, base loss: 14367.86
[INFO 2017-06-28 13:09:17,030 main.py:51] epoch 1989, training loss: 9832.40, average training loss: 10151.41, base loss: 14370.14
[INFO 2017-06-28 13:09:17,685 main.py:51] epoch 1990, training loss: 9520.82, average training loss: 10149.13, base loss: 14367.32
[INFO 2017-06-28 13:09:18,375 main.py:51] epoch 1991, training loss: 9650.26, average training loss: 10145.22, base loss: 14361.93
[INFO 2017-06-28 13:09:19,054 main.py:51] epoch 1992, training loss: 10183.28, average training loss: 10145.27, base loss: 14363.41
[INFO 2017-06-28 13:09:19,732 main.py:51] epoch 1993, training loss: 12736.57, average training loss: 10149.09, base loss: 14370.12
[INFO 2017-06-28 13:09:20,397 main.py:51] epoch 1994, training loss: 9562.42, average training loss: 10147.52, base loss: 14368.52
[INFO 2017-06-28 13:09:21,066 main.py:51] epoch 1995, training loss: 8803.43, average training loss: 10146.19, base loss: 14366.39
[INFO 2017-06-28 13:09:21,750 main.py:51] epoch 1996, training loss: 9700.71, average training loss: 10146.56, base loss: 14367.45
[INFO 2017-06-28 13:09:22,405 main.py:51] epoch 1997, training loss: 11915.54, average training loss: 10147.78, base loss: 14371.54
[INFO 2017-06-28 13:09:23,054 main.py:51] epoch 1998, training loss: 10697.49, average training loss: 10146.31, base loss: 14369.09
[INFO 2017-06-28 13:09:23,730 main.py:51] epoch 1999, training loss: 9563.04, average training loss: 10145.54, base loss: 14367.56
[INFO 2017-06-28 13:09:23,730 main.py:53] epoch 1999, testing
[INFO 2017-06-28 13:09:26,389 main.py:105] average testing loss: 10874.48, base loss: 14453.35
[INFO 2017-06-28 13:09:26,389 main.py:106] improve_loss: 3578.87, improve_percent: 0.25
[INFO 2017-06-28 13:09:26,390 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:09:27,029 main.py:51] epoch 2000, training loss: 9515.16, average training loss: 10145.67, base loss: 14368.82
[INFO 2017-06-28 13:09:27,708 main.py:51] epoch 2001, training loss: 11585.36, average training loss: 10148.05, base loss: 14372.69
[INFO 2017-06-28 13:09:28,361 main.py:51] epoch 2002, training loss: 8895.47, average training loss: 10147.68, base loss: 14374.80
[INFO 2017-06-28 13:09:29,045 main.py:51] epoch 2003, training loss: 8516.86, average training loss: 10146.96, base loss: 14373.23
[INFO 2017-06-28 13:09:29,724 main.py:51] epoch 2004, training loss: 9216.48, average training loss: 10145.16, base loss: 14370.92
[INFO 2017-06-28 13:09:30,370 main.py:51] epoch 2005, training loss: 9452.84, average training loss: 10142.56, base loss: 14369.01
[INFO 2017-06-28 13:09:31,027 main.py:51] epoch 2006, training loss: 8532.03, average training loss: 10139.57, base loss: 14363.59
[INFO 2017-06-28 13:09:31,670 main.py:51] epoch 2007, training loss: 10684.13, average training loss: 10139.05, base loss: 14363.69
[INFO 2017-06-28 13:09:32,323 main.py:51] epoch 2008, training loss: 10291.57, average training loss: 10139.55, base loss: 14363.92
[INFO 2017-06-28 13:09:32,970 main.py:51] epoch 2009, training loss: 10405.19, average training loss: 10140.81, base loss: 14365.91
[INFO 2017-06-28 13:09:33,624 main.py:51] epoch 2010, training loss: 11515.23, average training loss: 10141.84, base loss: 14367.67
[INFO 2017-06-28 13:09:34,306 main.py:51] epoch 2011, training loss: 8342.75, average training loss: 10139.68, base loss: 14364.78
[INFO 2017-06-28 13:09:34,999 main.py:51] epoch 2012, training loss: 9527.40, average training loss: 10139.15, base loss: 14365.74
[INFO 2017-06-28 13:09:35,658 main.py:51] epoch 2013, training loss: 10054.45, average training loss: 10138.74, base loss: 14366.20
[INFO 2017-06-28 13:09:36,348 main.py:51] epoch 2014, training loss: 9135.53, average training loss: 10136.74, base loss: 14364.93
[INFO 2017-06-28 13:09:37,016 main.py:51] epoch 2015, training loss: 8720.87, average training loss: 10136.45, base loss: 14364.39
[INFO 2017-06-28 13:09:37,693 main.py:51] epoch 2016, training loss: 10216.30, average training loss: 10134.16, base loss: 14361.28
[INFO 2017-06-28 13:09:38,353 main.py:51] epoch 2017, training loss: 11176.50, average training loss: 10135.21, base loss: 14362.60
[INFO 2017-06-28 13:09:39,010 main.py:51] epoch 2018, training loss: 9581.91, average training loss: 10133.54, base loss: 14361.46
[INFO 2017-06-28 13:09:39,684 main.py:51] epoch 2019, training loss: 9499.88, average training loss: 10134.36, base loss: 14362.47
[INFO 2017-06-28 13:09:40,342 main.py:51] epoch 2020, training loss: 9505.06, average training loss: 10132.28, base loss: 14360.68
[INFO 2017-06-28 13:09:41,002 main.py:51] epoch 2021, training loss: 10601.97, average training loss: 10131.76, base loss: 14359.44
[INFO 2017-06-28 13:09:41,655 main.py:51] epoch 2022, training loss: 10224.58, average training loss: 10132.44, base loss: 14361.58
[INFO 2017-06-28 13:09:42,333 main.py:51] epoch 2023, training loss: 10584.55, average training loss: 10131.94, base loss: 14362.90
[INFO 2017-06-28 13:09:43,019 main.py:51] epoch 2024, training loss: 9497.05, average training loss: 10131.52, base loss: 14362.15
[INFO 2017-06-28 13:09:43,676 main.py:51] epoch 2025, training loss: 10097.35, average training loss: 10129.77, base loss: 14359.51
[INFO 2017-06-28 13:09:44,323 main.py:51] epoch 2026, training loss: 8614.95, average training loss: 10128.29, base loss: 14356.87
[INFO 2017-06-28 13:09:44,972 main.py:51] epoch 2027, training loss: 8568.27, average training loss: 10126.94, base loss: 14355.63
[INFO 2017-06-28 13:09:45,638 main.py:51] epoch 2028, training loss: 8980.57, average training loss: 10125.86, base loss: 14354.84
[INFO 2017-06-28 13:09:46,283 main.py:51] epoch 2029, training loss: 8821.69, average training loss: 10124.84, base loss: 14354.25
[INFO 2017-06-28 13:09:46,943 main.py:51] epoch 2030, training loss: 10077.00, average training loss: 10124.94, base loss: 14353.62
[INFO 2017-06-28 13:09:47,596 main.py:51] epoch 2031, training loss: 11195.14, average training loss: 10126.47, base loss: 14355.10
[INFO 2017-06-28 13:09:48,264 main.py:51] epoch 2032, training loss: 9262.62, average training loss: 10125.42, base loss: 14354.94
[INFO 2017-06-28 13:09:48,935 main.py:51] epoch 2033, training loss: 9813.79, average training loss: 10124.66, base loss: 14355.91
[INFO 2017-06-28 13:09:49,611 main.py:51] epoch 2034, training loss: 9521.84, average training loss: 10122.37, base loss: 14353.76
[INFO 2017-06-28 13:09:50,268 main.py:51] epoch 2035, training loss: 9949.05, average training loss: 10121.00, base loss: 14350.47
[INFO 2017-06-28 13:09:50,944 main.py:51] epoch 2036, training loss: 10642.81, average training loss: 10122.73, base loss: 14353.80
[INFO 2017-06-28 13:09:51,608 main.py:51] epoch 2037, training loss: 10115.24, average training loss: 10122.41, base loss: 14353.79
[INFO 2017-06-28 13:09:52,262 main.py:51] epoch 2038, training loss: 10033.52, average training loss: 10122.49, base loss: 14353.28
[INFO 2017-06-28 13:09:52,909 main.py:51] epoch 2039, training loss: 11522.61, average training loss: 10122.03, base loss: 14352.34
[INFO 2017-06-28 13:09:53,603 main.py:51] epoch 2040, training loss: 11839.22, average training loss: 10124.28, base loss: 14357.47
[INFO 2017-06-28 13:09:54,266 main.py:51] epoch 2041, training loss: 10771.08, average training loss: 10123.48, base loss: 14357.48
[INFO 2017-06-28 13:09:54,916 main.py:51] epoch 2042, training loss: 8840.19, average training loss: 10121.01, base loss: 14354.37
[INFO 2017-06-28 13:09:55,561 main.py:51] epoch 2043, training loss: 11243.34, average training loss: 10120.53, base loss: 14354.74
[INFO 2017-06-28 13:09:56,222 main.py:51] epoch 2044, training loss: 9417.48, average training loss: 10119.94, base loss: 14355.69
[INFO 2017-06-28 13:09:56,857 main.py:51] epoch 2045, training loss: 9526.27, average training loss: 10119.50, base loss: 14355.92
[INFO 2017-06-28 13:09:57,502 main.py:51] epoch 2046, training loss: 8822.36, average training loss: 10117.66, base loss: 14352.59
[INFO 2017-06-28 13:09:58,194 main.py:51] epoch 2047, training loss: 9339.26, average training loss: 10115.31, base loss: 14350.58
[INFO 2017-06-28 13:09:58,859 main.py:51] epoch 2048, training loss: 11063.15, average training loss: 10115.31, base loss: 14350.43
[INFO 2017-06-28 13:09:59,503 main.py:51] epoch 2049, training loss: 9508.06, average training loss: 10114.72, base loss: 14350.29
[INFO 2017-06-28 13:10:00,178 main.py:51] epoch 2050, training loss: 9643.38, average training loss: 10113.53, base loss: 14347.58
[INFO 2017-06-28 13:10:00,825 main.py:51] epoch 2051, training loss: 10194.94, average training loss: 10113.38, base loss: 14348.94
[INFO 2017-06-28 13:10:01,492 main.py:51] epoch 2052, training loss: 10249.30, average training loss: 10113.28, base loss: 14348.45
[INFO 2017-06-28 13:10:02,135 main.py:51] epoch 2053, training loss: 9413.76, average training loss: 10113.14, base loss: 14348.74
[INFO 2017-06-28 13:10:02,796 main.py:51] epoch 2054, training loss: 9736.85, average training loss: 10111.62, base loss: 14346.71
[INFO 2017-06-28 13:10:03,452 main.py:51] epoch 2055, training loss: 9728.58, average training loss: 10112.25, base loss: 14348.04
[INFO 2017-06-28 13:10:04,112 main.py:51] epoch 2056, training loss: 9216.42, average training loss: 10109.51, base loss: 14345.33
[INFO 2017-06-28 13:10:04,785 main.py:51] epoch 2057, training loss: 12032.62, average training loss: 10111.65, base loss: 14347.94
[INFO 2017-06-28 13:10:05,472 main.py:51] epoch 2058, training loss: 9330.37, average training loss: 10110.96, base loss: 14347.75
[INFO 2017-06-28 13:10:06,146 main.py:51] epoch 2059, training loss: 10478.00, average training loss: 10111.82, base loss: 14349.66
[INFO 2017-06-28 13:10:06,856 main.py:51] epoch 2060, training loss: 10594.05, average training loss: 10112.29, base loss: 14350.03
[INFO 2017-06-28 13:10:07,509 main.py:51] epoch 2061, training loss: 10282.94, average training loss: 10111.11, base loss: 14346.84
[INFO 2017-06-28 13:10:08,188 main.py:51] epoch 2062, training loss: 8388.83, average training loss: 10109.68, base loss: 14344.68
[INFO 2017-06-28 13:10:08,851 main.py:51] epoch 2063, training loss: 10316.50, average training loss: 10108.62, base loss: 14344.55
[INFO 2017-06-28 13:10:09,540 main.py:51] epoch 2064, training loss: 10890.99, average training loss: 10109.08, base loss: 14345.88
[INFO 2017-06-28 13:10:10,223 main.py:51] epoch 2065, training loss: 10029.37, average training loss: 10108.85, base loss: 14346.24
[INFO 2017-06-28 13:10:10,877 main.py:51] epoch 2066, training loss: 9593.15, average training loss: 10107.97, base loss: 14345.88
[INFO 2017-06-28 13:10:11,531 main.py:51] epoch 2067, training loss: 8586.73, average training loss: 10106.57, base loss: 14343.39
[INFO 2017-06-28 13:10:12,196 main.py:51] epoch 2068, training loss: 8649.89, average training loss: 10106.40, base loss: 14343.72
[INFO 2017-06-28 13:10:12,851 main.py:51] epoch 2069, training loss: 10429.07, average training loss: 10107.52, base loss: 14345.58
[INFO 2017-06-28 13:10:13,510 main.py:51] epoch 2070, training loss: 9322.36, average training loss: 10106.93, base loss: 14346.19
[INFO 2017-06-28 13:10:14,172 main.py:51] epoch 2071, training loss: 9604.39, average training loss: 10106.92, base loss: 14346.95
[INFO 2017-06-28 13:10:14,833 main.py:51] epoch 2072, training loss: 9662.50, average training loss: 10106.67, base loss: 14346.87
[INFO 2017-06-28 13:10:15,513 main.py:51] epoch 2073, training loss: 10850.45, average training loss: 10108.47, base loss: 14349.60
[INFO 2017-06-28 13:10:16,185 main.py:51] epoch 2074, training loss: 10400.30, average training loss: 10109.17, base loss: 14351.52
[INFO 2017-06-28 13:10:16,846 main.py:51] epoch 2075, training loss: 10152.46, average training loss: 10108.65, base loss: 14349.04
[INFO 2017-06-28 13:10:17,518 main.py:51] epoch 2076, training loss: 9929.16, average training loss: 10108.57, base loss: 14349.55
[INFO 2017-06-28 13:10:18,200 main.py:51] epoch 2077, training loss: 9838.84, average training loss: 10108.49, base loss: 14349.79
[INFO 2017-06-28 13:10:18,876 main.py:51] epoch 2078, training loss: 9363.18, average training loss: 10107.70, base loss: 14349.18
[INFO 2017-06-28 13:10:19,572 main.py:51] epoch 2079, training loss: 10320.43, average training loss: 10108.47, base loss: 14348.97
[INFO 2017-06-28 13:10:20,242 main.py:51] epoch 2080, training loss: 10083.42, average training loss: 10108.12, base loss: 14348.84
[INFO 2017-06-28 13:10:20,934 main.py:51] epoch 2081, training loss: 12223.29, average training loss: 10109.67, base loss: 14350.16
[INFO 2017-06-28 13:10:21,600 main.py:51] epoch 2082, training loss: 10635.94, average training loss: 10108.39, base loss: 14350.77
[INFO 2017-06-28 13:10:22,276 main.py:51] epoch 2083, training loss: 10014.53, average training loss: 10108.19, base loss: 14350.52
[INFO 2017-06-28 13:10:22,930 main.py:51] epoch 2084, training loss: 9044.21, average training loss: 10107.11, base loss: 14349.90
[INFO 2017-06-28 13:10:23,609 main.py:51] epoch 2085, training loss: 9358.72, average training loss: 10107.41, base loss: 14351.36
[INFO 2017-06-28 13:10:24,293 main.py:51] epoch 2086, training loss: 9595.29, average training loss: 10107.68, base loss: 14351.22
[INFO 2017-06-28 13:10:24,961 main.py:51] epoch 2087, training loss: 10191.83, average training loss: 10107.14, base loss: 14350.69
[INFO 2017-06-28 13:10:25,613 main.py:51] epoch 2088, training loss: 8474.94, average training loss: 10107.26, base loss: 14352.49
[INFO 2017-06-28 13:10:26,282 main.py:51] epoch 2089, training loss: 10611.31, average training loss: 10105.69, base loss: 14350.51
[INFO 2017-06-28 13:10:26,959 main.py:51] epoch 2090, training loss: 9768.36, average training loss: 10105.07, base loss: 14351.37
[INFO 2017-06-28 13:10:27,603 main.py:51] epoch 2091, training loss: 9568.56, average training loss: 10104.07, base loss: 14349.13
[INFO 2017-06-28 13:10:28,244 main.py:51] epoch 2092, training loss: 10065.49, average training loss: 10102.91, base loss: 14347.54
[INFO 2017-06-28 13:10:28,919 main.py:51] epoch 2093, training loss: 9218.05, average training loss: 10101.38, base loss: 14345.45
[INFO 2017-06-28 13:10:29,583 main.py:51] epoch 2094, training loss: 9968.24, average training loss: 10099.08, base loss: 14341.82
[INFO 2017-06-28 13:10:30,265 main.py:51] epoch 2095, training loss: 12085.55, average training loss: 10101.34, base loss: 14346.77
[INFO 2017-06-28 13:10:30,929 main.py:51] epoch 2096, training loss: 9749.12, average training loss: 10100.78, base loss: 14346.30
[INFO 2017-06-28 13:10:31,583 main.py:51] epoch 2097, training loss: 10259.84, average training loss: 10101.82, base loss: 14347.95
[INFO 2017-06-28 13:10:32,249 main.py:51] epoch 2098, training loss: 9076.90, average training loss: 10100.51, base loss: 14347.02
[INFO 2017-06-28 13:10:32,920 main.py:51] epoch 2099, training loss: 9535.34, average training loss: 10100.63, base loss: 14348.30
[INFO 2017-06-28 13:10:32,921 main.py:53] epoch 2099, testing
[INFO 2017-06-28 13:10:35,509 main.py:105] average testing loss: 11082.96, base loss: 15151.81
[INFO 2017-06-28 13:10:35,509 main.py:106] improve_loss: 4068.85, improve_percent: 0.27
[INFO 2017-06-28 13:10:35,510 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:10:36,175 main.py:51] epoch 2100, training loss: 10939.38, average training loss: 10102.27, base loss: 14351.22
[INFO 2017-06-28 13:10:36,821 main.py:51] epoch 2101, training loss: 9719.35, average training loss: 10102.11, base loss: 14351.91
[INFO 2017-06-28 13:10:37,476 main.py:51] epoch 2102, training loss: 9741.04, average training loss: 10100.70, base loss: 14349.27
[INFO 2017-06-28 13:10:38,154 main.py:51] epoch 2103, training loss: 9073.85, average training loss: 10097.42, base loss: 14345.70
[INFO 2017-06-28 13:10:38,793 main.py:51] epoch 2104, training loss: 9179.74, average training loss: 10095.92, base loss: 14343.38
[INFO 2017-06-28 13:10:39,452 main.py:51] epoch 2105, training loss: 9743.55, average training loss: 10095.83, base loss: 14343.96
[INFO 2017-06-28 13:10:40,109 main.py:51] epoch 2106, training loss: 10032.97, average training loss: 10096.54, base loss: 14346.23
[INFO 2017-06-28 13:10:40,787 main.py:51] epoch 2107, training loss: 9642.40, average training loss: 10096.54, base loss: 14346.57
[INFO 2017-06-28 13:10:41,453 main.py:51] epoch 2108, training loss: 9412.95, average training loss: 10094.37, base loss: 14343.80
[INFO 2017-06-28 13:10:42,141 main.py:51] epoch 2109, training loss: 9188.79, average training loss: 10094.75, base loss: 14344.65
[INFO 2017-06-28 13:10:42,795 main.py:51] epoch 2110, training loss: 10506.08, average training loss: 10095.12, base loss: 14346.95
[INFO 2017-06-28 13:10:43,482 main.py:51] epoch 2111, training loss: 10619.75, average training loss: 10095.02, base loss: 14347.92
[INFO 2017-06-28 13:10:44,142 main.py:51] epoch 2112, training loss: 10816.73, average training loss: 10095.97, base loss: 14349.86
[INFO 2017-06-28 13:10:44,795 main.py:51] epoch 2113, training loss: 8495.29, average training loss: 10093.94, base loss: 14346.22
[INFO 2017-06-28 13:10:45,453 main.py:51] epoch 2114, training loss: 9430.07, average training loss: 10092.90, base loss: 14343.60
[INFO 2017-06-28 13:10:46,096 main.py:51] epoch 2115, training loss: 8581.58, average training loss: 10090.06, base loss: 14340.06
[INFO 2017-06-28 13:10:46,780 main.py:51] epoch 2116, training loss: 10672.44, average training loss: 10090.82, base loss: 14341.18
[INFO 2017-06-28 13:10:47,424 main.py:51] epoch 2117, training loss: 9679.07, average training loss: 10090.37, base loss: 14342.10
[INFO 2017-06-28 13:10:48,097 main.py:51] epoch 2118, training loss: 9816.62, average training loss: 10090.84, base loss: 14343.47
[INFO 2017-06-28 13:10:48,772 main.py:51] epoch 2119, training loss: 9634.78, average training loss: 10091.00, base loss: 14345.02
[INFO 2017-06-28 13:10:49,432 main.py:51] epoch 2120, training loss: 10742.38, average training loss: 10088.98, base loss: 14344.40
[INFO 2017-06-28 13:10:50,084 main.py:51] epoch 2121, training loss: 9623.41, average training loss: 10087.92, base loss: 14342.34
[INFO 2017-06-28 13:10:50,764 main.py:51] epoch 2122, training loss: 9417.99, average training loss: 10088.77, base loss: 14343.71
[INFO 2017-06-28 13:10:51,438 main.py:51] epoch 2123, training loss: 11618.21, average training loss: 10090.03, base loss: 14345.25
[INFO 2017-06-28 13:10:52,103 main.py:51] epoch 2124, training loss: 9832.78, average training loss: 10089.10, base loss: 14345.09
[INFO 2017-06-28 13:10:52,767 main.py:51] epoch 2125, training loss: 10009.56, average training loss: 10088.77, base loss: 14344.66
[INFO 2017-06-28 13:10:53,424 main.py:51] epoch 2126, training loss: 10178.52, average training loss: 10089.17, base loss: 14345.94
[INFO 2017-06-28 13:10:54,089 main.py:51] epoch 2127, training loss: 9305.45, average training loss: 10087.66, base loss: 14345.91
[INFO 2017-06-28 13:10:54,741 main.py:51] epoch 2128, training loss: 10928.49, average training loss: 10089.03, base loss: 14348.16
[INFO 2017-06-28 13:10:55,390 main.py:51] epoch 2129, training loss: 10428.18, average training loss: 10089.72, base loss: 14349.90
[INFO 2017-06-28 13:10:56,043 main.py:51] epoch 2130, training loss: 11718.23, average training loss: 10091.37, base loss: 14351.42
[INFO 2017-06-28 13:10:56,712 main.py:51] epoch 2131, training loss: 8583.13, average training loss: 10090.12, base loss: 14350.42
[INFO 2017-06-28 13:10:57,401 main.py:51] epoch 2132, training loss: 11343.17, average training loss: 10091.54, base loss: 14352.88
[INFO 2017-06-28 13:10:58,060 main.py:51] epoch 2133, training loss: 10675.01, average training loss: 10092.16, base loss: 14352.47
[INFO 2017-06-28 13:10:58,720 main.py:51] epoch 2134, training loss: 10605.02, average training loss: 10093.22, base loss: 14354.36
[INFO 2017-06-28 13:10:59,384 main.py:51] epoch 2135, training loss: 12600.66, average training loss: 10095.41, base loss: 14357.85
[INFO 2017-06-28 13:11:00,042 main.py:51] epoch 2136, training loss: 10458.76, average training loss: 10096.78, base loss: 14361.44
[INFO 2017-06-28 13:11:00,701 main.py:51] epoch 2137, training loss: 10807.53, average training loss: 10097.80, base loss: 14362.62
[INFO 2017-06-28 13:11:01,366 main.py:51] epoch 2138, training loss: 9195.52, average training loss: 10097.40, base loss: 14362.75
[INFO 2017-06-28 13:11:02,012 main.py:51] epoch 2139, training loss: 9982.38, average training loss: 10095.86, base loss: 14360.14
[INFO 2017-06-28 13:11:02,708 main.py:51] epoch 2140, training loss: 10422.40, average training loss: 10094.60, base loss: 14359.73
[INFO 2017-06-28 13:11:03,364 main.py:51] epoch 2141, training loss: 10667.26, average training loss: 10096.47, base loss: 14362.87
[INFO 2017-06-28 13:11:04,023 main.py:51] epoch 2142, training loss: 10879.44, average training loss: 10096.97, base loss: 14363.43
[INFO 2017-06-28 13:11:04,674 main.py:51] epoch 2143, training loss: 9825.15, average training loss: 10098.67, base loss: 14366.77
[INFO 2017-06-28 13:11:05,351 main.py:51] epoch 2144, training loss: 9005.14, average training loss: 10098.09, base loss: 14366.26
[INFO 2017-06-28 13:11:06,033 main.py:51] epoch 2145, training loss: 10632.96, average training loss: 10095.62, base loss: 14362.70
[INFO 2017-06-28 13:11:06,697 main.py:51] epoch 2146, training loss: 9194.47, average training loss: 10095.09, base loss: 14361.61
[INFO 2017-06-28 13:11:07,368 main.py:51] epoch 2147, training loss: 8937.16, average training loss: 10093.52, base loss: 14360.48
[INFO 2017-06-28 13:11:08,012 main.py:51] epoch 2148, training loss: 9713.85, average training loss: 10093.20, base loss: 14361.21
[INFO 2017-06-28 13:11:08,664 main.py:51] epoch 2149, training loss: 10454.83, average training loss: 10094.46, base loss: 14363.84
[INFO 2017-06-28 13:11:09,332 main.py:51] epoch 2150, training loss: 9511.89, average training loss: 10095.01, base loss: 14364.87
[INFO 2017-06-28 13:11:10,007 main.py:51] epoch 2151, training loss: 10042.55, average training loss: 10095.01, base loss: 14366.19
[INFO 2017-06-28 13:11:10,658 main.py:51] epoch 2152, training loss: 10676.84, average training loss: 10096.45, base loss: 14368.06
[INFO 2017-06-28 13:11:11,317 main.py:51] epoch 2153, training loss: 10272.75, average training loss: 10095.80, base loss: 14367.35
[INFO 2017-06-28 13:11:11,972 main.py:51] epoch 2154, training loss: 8572.18, average training loss: 10092.92, base loss: 14364.37
[INFO 2017-06-28 13:11:12,617 main.py:51] epoch 2155, training loss: 9529.83, average training loss: 10091.49, base loss: 14364.22
[INFO 2017-06-28 13:11:13,282 main.py:51] epoch 2156, training loss: 9691.87, average training loss: 10091.82, base loss: 14365.04
[INFO 2017-06-28 13:11:13,940 main.py:51] epoch 2157, training loss: 9344.80, average training loss: 10092.04, base loss: 14365.53
[INFO 2017-06-28 13:11:14,611 main.py:51] epoch 2158, training loss: 11656.15, average training loss: 10090.75, base loss: 14361.79
[INFO 2017-06-28 13:11:15,265 main.py:51] epoch 2159, training loss: 11707.77, average training loss: 10091.13, base loss: 14363.57
[INFO 2017-06-28 13:11:15,913 main.py:51] epoch 2160, training loss: 10165.56, average training loss: 10090.13, base loss: 14363.35
[INFO 2017-06-28 13:11:16,587 main.py:51] epoch 2161, training loss: 9486.52, average training loss: 10090.82, base loss: 14365.34
[INFO 2017-06-28 13:11:17,272 main.py:51] epoch 2162, training loss: 10927.58, average training loss: 10092.04, base loss: 14366.43
[INFO 2017-06-28 13:11:17,929 main.py:51] epoch 2163, training loss: 10591.20, average training loss: 10092.24, base loss: 14367.51
[INFO 2017-06-28 13:11:18,572 main.py:51] epoch 2164, training loss: 9338.83, average training loss: 10091.98, base loss: 14367.51
[INFO 2017-06-28 13:11:19,234 main.py:51] epoch 2165, training loss: 11003.65, average training loss: 10093.02, base loss: 14367.76
[INFO 2017-06-28 13:11:19,896 main.py:51] epoch 2166, training loss: 9606.01, average training loss: 10093.08, base loss: 14367.77
[INFO 2017-06-28 13:11:20,564 main.py:51] epoch 2167, training loss: 11013.07, average training loss: 10093.36, base loss: 14368.29
[INFO 2017-06-28 13:11:21,217 main.py:51] epoch 2168, training loss: 11969.82, average training loss: 10095.42, base loss: 14371.41
[INFO 2017-06-28 13:11:21,877 main.py:51] epoch 2169, training loss: 11205.72, average training loss: 10097.46, base loss: 14374.95
[INFO 2017-06-28 13:11:22,543 main.py:51] epoch 2170, training loss: 11294.38, average training loss: 10098.56, base loss: 14375.95
[INFO 2017-06-28 13:11:23,214 main.py:51] epoch 2171, training loss: 9801.82, average training loss: 10096.01, base loss: 14374.26
[INFO 2017-06-28 13:11:23,880 main.py:51] epoch 2172, training loss: 12075.84, average training loss: 10098.29, base loss: 14377.97
[INFO 2017-06-28 13:11:24,533 main.py:51] epoch 2173, training loss: 8856.80, average training loss: 10097.07, base loss: 14376.39
[INFO 2017-06-28 13:11:25,177 main.py:51] epoch 2174, training loss: 10394.51, average training loss: 10098.47, base loss: 14379.98
[INFO 2017-06-28 13:11:25,853 main.py:51] epoch 2175, training loss: 9053.44, average training loss: 10098.69, base loss: 14381.81
[INFO 2017-06-28 13:11:26,539 main.py:51] epoch 2176, training loss: 10327.54, average training loss: 10100.88, base loss: 14384.50
[INFO 2017-06-28 13:11:27,229 main.py:51] epoch 2177, training loss: 9453.75, average training loss: 10097.86, base loss: 14381.76
[INFO 2017-06-28 13:11:27,875 main.py:51] epoch 2178, training loss: 11177.22, average training loss: 10099.12, base loss: 14384.60
[INFO 2017-06-28 13:11:28,536 main.py:51] epoch 2179, training loss: 8934.24, average training loss: 10098.36, base loss: 14383.15
[INFO 2017-06-28 13:11:29,190 main.py:51] epoch 2180, training loss: 9769.56, average training loss: 10097.15, base loss: 14383.70
[INFO 2017-06-28 13:11:29,857 main.py:51] epoch 2181, training loss: 9214.46, average training loss: 10095.53, base loss: 14381.92
[INFO 2017-06-28 13:11:30,527 main.py:51] epoch 2182, training loss: 10133.96, average training loss: 10095.27, base loss: 14381.76
[INFO 2017-06-28 13:11:31,175 main.py:51] epoch 2183, training loss: 11088.50, average training loss: 10094.08, base loss: 14381.20
[INFO 2017-06-28 13:11:31,852 main.py:51] epoch 2184, training loss: 9162.19, average training loss: 10092.72, base loss: 14379.62
[INFO 2017-06-28 13:11:32,520 main.py:51] epoch 2185, training loss: 9773.13, average training loss: 10093.38, base loss: 14380.34
[INFO 2017-06-28 13:11:33,184 main.py:51] epoch 2186, training loss: 10777.69, average training loss: 10093.58, base loss: 14381.72
[INFO 2017-06-28 13:11:33,885 main.py:51] epoch 2187, training loss: 10347.70, average training loss: 10094.17, base loss: 14384.24
[INFO 2017-06-28 13:11:34,560 main.py:51] epoch 2188, training loss: 10553.36, average training loss: 10094.90, base loss: 14386.42
[INFO 2017-06-28 13:11:35,212 main.py:51] epoch 2189, training loss: 8920.65, average training loss: 10093.85, base loss: 14384.41
[INFO 2017-06-28 13:11:35,857 main.py:51] epoch 2190, training loss: 9962.12, average training loss: 10091.74, base loss: 14381.95
[INFO 2017-06-28 13:11:36,505 main.py:51] epoch 2191, training loss: 9012.21, average training loss: 10091.20, base loss: 14381.66
[INFO 2017-06-28 13:11:37,161 main.py:51] epoch 2192, training loss: 9032.45, average training loss: 10089.48, base loss: 14379.92
[INFO 2017-06-28 13:11:37,844 main.py:51] epoch 2193, training loss: 10336.50, average training loss: 10089.29, base loss: 14379.85
[INFO 2017-06-28 13:11:38,514 main.py:51] epoch 2194, training loss: 10071.96, average training loss: 10087.29, base loss: 14378.72
[INFO 2017-06-28 13:11:39,172 main.py:51] epoch 2195, training loss: 10333.56, average training loss: 10086.52, base loss: 14378.14
[INFO 2017-06-28 13:11:39,826 main.py:51] epoch 2196, training loss: 11048.32, average training loss: 10088.55, base loss: 14381.69
[INFO 2017-06-28 13:11:40,505 main.py:51] epoch 2197, training loss: 8603.41, average training loss: 10087.92, base loss: 14380.08
[INFO 2017-06-28 13:11:41,170 main.py:51] epoch 2198, training loss: 10467.55, average training loss: 10087.69, base loss: 14380.43
[INFO 2017-06-28 13:11:41,828 main.py:51] epoch 2199, training loss: 9616.92, average training loss: 10088.50, base loss: 14379.97
[INFO 2017-06-28 13:11:41,828 main.py:53] epoch 2199, testing
[INFO 2017-06-28 13:11:44,451 main.py:105] average testing loss: 11193.57, base loss: 15491.11
[INFO 2017-06-28 13:11:44,452 main.py:106] improve_loss: 4297.54, improve_percent: 0.28
[INFO 2017-06-28 13:11:44,452 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:11:45,115 main.py:51] epoch 2200, training loss: 10133.71, average training loss: 10087.93, base loss: 14379.54
[INFO 2017-06-28 13:11:45,769 main.py:51] epoch 2201, training loss: 10568.82, average training loss: 10088.25, base loss: 14379.92
[INFO 2017-06-28 13:11:46,430 main.py:51] epoch 2202, training loss: 8509.10, average training loss: 10086.90, base loss: 14379.36
[INFO 2017-06-28 13:11:47,083 main.py:51] epoch 2203, training loss: 10072.48, average training loss: 10087.99, base loss: 14379.96
[INFO 2017-06-28 13:11:47,738 main.py:51] epoch 2204, training loss: 10438.32, average training loss: 10087.38, base loss: 14379.89
[INFO 2017-06-28 13:11:48,418 main.py:51] epoch 2205, training loss: 8904.61, average training loss: 10085.13, base loss: 14376.51
[INFO 2017-06-28 13:11:49,059 main.py:51] epoch 2206, training loss: 10043.23, average training loss: 10084.62, base loss: 14375.56
[INFO 2017-06-28 13:11:49,732 main.py:51] epoch 2207, training loss: 11051.50, average training loss: 10086.66, base loss: 14379.19
[INFO 2017-06-28 13:11:50,420 main.py:51] epoch 2208, training loss: 8864.74, average training loss: 10086.00, base loss: 14379.26
[INFO 2017-06-28 13:11:51,085 main.py:51] epoch 2209, training loss: 11435.06, average training loss: 10085.43, base loss: 14378.06
[INFO 2017-06-28 13:11:51,751 main.py:51] epoch 2210, training loss: 11234.72, average training loss: 10086.31, base loss: 14379.25
[INFO 2017-06-28 13:11:52,420 main.py:51] epoch 2211, training loss: 9432.92, average training loss: 10085.44, base loss: 14379.29
[INFO 2017-06-28 13:11:53,071 main.py:51] epoch 2212, training loss: 8988.74, average training loss: 10083.72, base loss: 14376.38
[INFO 2017-06-28 13:11:53,744 main.py:51] epoch 2213, training loss: 9192.99, average training loss: 10083.28, base loss: 14374.38
[INFO 2017-06-28 13:11:54,424 main.py:51] epoch 2214, training loss: 9117.70, average training loss: 10083.22, base loss: 14373.47
[INFO 2017-06-28 13:11:55,086 main.py:51] epoch 2215, training loss: 9252.79, average training loss: 10083.20, base loss: 14371.98
[INFO 2017-06-28 13:11:55,754 main.py:51] epoch 2216, training loss: 10681.11, average training loss: 10084.25, base loss: 14374.85
[INFO 2017-06-28 13:11:56,441 main.py:51] epoch 2217, training loss: 10044.55, average training loss: 10084.76, base loss: 14375.36
[INFO 2017-06-28 13:11:57,082 main.py:51] epoch 2218, training loss: 9849.28, average training loss: 10083.67, base loss: 14374.66
[INFO 2017-06-28 13:11:57,748 main.py:51] epoch 2219, training loss: 11827.20, average training loss: 10085.96, base loss: 14379.54
[INFO 2017-06-28 13:11:58,423 main.py:51] epoch 2220, training loss: 11230.96, average training loss: 10085.70, base loss: 14379.53
[INFO 2017-06-28 13:11:59,101 main.py:51] epoch 2221, training loss: 10191.67, average training loss: 10084.85, base loss: 14379.53
[INFO 2017-06-28 13:11:59,774 main.py:51] epoch 2222, training loss: 10811.73, average training loss: 10083.81, base loss: 14378.34
[INFO 2017-06-28 13:12:00,409 main.py:51] epoch 2223, training loss: 9617.64, average training loss: 10084.18, base loss: 14379.02
[INFO 2017-06-28 13:12:01,075 main.py:51] epoch 2224, training loss: 10980.83, average training loss: 10084.64, base loss: 14380.51
[INFO 2017-06-28 13:12:01,745 main.py:51] epoch 2225, training loss: 10874.44, average training loss: 10085.73, base loss: 14383.07
[INFO 2017-06-28 13:12:02,405 main.py:51] epoch 2226, training loss: 8991.86, average training loss: 10085.44, base loss: 14382.50
[INFO 2017-06-28 13:12:03,063 main.py:51] epoch 2227, training loss: 10444.80, average training loss: 10086.56, base loss: 14385.33
[INFO 2017-06-28 13:12:03,717 main.py:51] epoch 2228, training loss: 10219.23, average training loss: 10087.27, base loss: 14386.33
[INFO 2017-06-28 13:12:04,391 main.py:51] epoch 2229, training loss: 9894.78, average training loss: 10087.18, base loss: 14386.93
[INFO 2017-06-28 13:12:05,050 main.py:51] epoch 2230, training loss: 9695.35, average training loss: 10087.91, base loss: 14389.07
[INFO 2017-06-28 13:12:05,708 main.py:51] epoch 2231, training loss: 8928.79, average training loss: 10087.10, base loss: 14388.28
[INFO 2017-06-28 13:12:06,362 main.py:51] epoch 2232, training loss: 10205.74, average training loss: 10086.85, base loss: 14388.57
[INFO 2017-06-28 13:12:07,016 main.py:51] epoch 2233, training loss: 12087.84, average training loss: 10088.47, base loss: 14391.38
[INFO 2017-06-28 13:12:07,688 main.py:51] epoch 2234, training loss: 11091.54, average training loss: 10090.57, base loss: 14395.02
[INFO 2017-06-28 13:12:08,350 main.py:51] epoch 2235, training loss: 9515.40, average training loss: 10090.21, base loss: 14394.39
[INFO 2017-06-28 13:12:08,988 main.py:51] epoch 2236, training loss: 9042.10, average training loss: 10089.66, base loss: 14395.41
[INFO 2017-06-28 13:12:09,624 main.py:51] epoch 2237, training loss: 11143.04, average training loss: 10089.33, base loss: 14394.04
[INFO 2017-06-28 13:12:10,274 main.py:51] epoch 2238, training loss: 9445.85, average training loss: 10085.43, base loss: 14390.33
[INFO 2017-06-28 13:12:10,943 main.py:51] epoch 2239, training loss: 9843.56, average training loss: 10085.26, base loss: 14390.91
[INFO 2017-06-28 13:12:11,586 main.py:51] epoch 2240, training loss: 10166.51, average training loss: 10086.03, base loss: 14392.61
[INFO 2017-06-28 13:12:12,283 main.py:51] epoch 2241, training loss: 10777.62, average training loss: 10085.34, base loss: 14391.16
[INFO 2017-06-28 13:12:12,941 main.py:51] epoch 2242, training loss: 8262.65, average training loss: 10082.20, base loss: 14387.25
[INFO 2017-06-28 13:12:13,626 main.py:51] epoch 2243, training loss: 11838.27, average training loss: 10083.31, base loss: 14387.70
[INFO 2017-06-28 13:12:14,289 main.py:51] epoch 2244, training loss: 8991.02, average training loss: 10081.88, base loss: 14384.46
[INFO 2017-06-28 13:12:14,943 main.py:51] epoch 2245, training loss: 10049.03, average training loss: 10081.74, base loss: 14384.97
[INFO 2017-06-28 13:12:15,607 main.py:51] epoch 2246, training loss: 9204.24, average training loss: 10081.32, base loss: 14384.65
[INFO 2017-06-28 13:12:16,248 main.py:51] epoch 2247, training loss: 10101.44, average training loss: 10081.85, base loss: 14384.06
[INFO 2017-06-28 13:12:16,914 main.py:51] epoch 2248, training loss: 12591.01, average training loss: 10083.61, base loss: 14386.63
[INFO 2017-06-28 13:12:17,577 main.py:51] epoch 2249, training loss: 10786.69, average training loss: 10085.27, base loss: 14387.94
[INFO 2017-06-28 13:12:18,230 main.py:51] epoch 2250, training loss: 9995.41, average training loss: 10084.82, base loss: 14387.01
[INFO 2017-06-28 13:12:18,896 main.py:51] epoch 2251, training loss: 10313.71, average training loss: 10085.40, base loss: 14388.49
[INFO 2017-06-28 13:12:19,572 main.py:51] epoch 2252, training loss: 10735.86, average training loss: 10085.28, base loss: 14390.10
[INFO 2017-06-28 13:12:20,247 main.py:51] epoch 2253, training loss: 11388.91, average training loss: 10086.87, base loss: 14392.37
[INFO 2017-06-28 13:12:20,898 main.py:51] epoch 2254, training loss: 9156.80, average training loss: 10086.09, base loss: 14391.60
[INFO 2017-06-28 13:12:21,559 main.py:51] epoch 2255, training loss: 9064.09, average training loss: 10085.05, base loss: 14388.83
[INFO 2017-06-28 13:12:22,237 main.py:51] epoch 2256, training loss: 12005.02, average training loss: 10085.88, base loss: 14388.92
[INFO 2017-06-28 13:12:22,897 main.py:51] epoch 2257, training loss: 13713.16, average training loss: 10089.53, base loss: 14393.38
[INFO 2017-06-28 13:12:23,593 main.py:51] epoch 2258, training loss: 11075.92, average training loss: 10090.64, base loss: 14394.77
[INFO 2017-06-28 13:12:24,280 main.py:51] epoch 2259, training loss: 10478.84, average training loss: 10090.04, base loss: 14395.15
[INFO 2017-06-28 13:12:24,933 main.py:51] epoch 2260, training loss: 8757.40, average training loss: 10089.15, base loss: 14394.98
[INFO 2017-06-28 13:12:25,597 main.py:51] epoch 2261, training loss: 10858.92, average training loss: 10090.85, base loss: 14396.41
[INFO 2017-06-28 13:12:26,285 main.py:51] epoch 2262, training loss: 10501.44, average training loss: 10091.31, base loss: 14396.92
[INFO 2017-06-28 13:12:26,951 main.py:51] epoch 2263, training loss: 8600.50, average training loss: 10090.12, base loss: 14395.02
[INFO 2017-06-28 13:12:27,617 main.py:51] epoch 2264, training loss: 11077.42, average training loss: 10091.51, base loss: 14397.63
[INFO 2017-06-28 13:12:28,297 main.py:51] epoch 2265, training loss: 10094.39, average training loss: 10091.89, base loss: 14398.98
[INFO 2017-06-28 13:12:28,962 main.py:51] epoch 2266, training loss: 11621.78, average training loss: 10093.58, base loss: 14400.88
[INFO 2017-06-28 13:12:29,608 main.py:51] epoch 2267, training loss: 9355.41, average training loss: 10092.89, base loss: 14400.90
[INFO 2017-06-28 13:12:30,257 main.py:51] epoch 2268, training loss: 9377.38, average training loss: 10092.07, base loss: 14400.20
[INFO 2017-06-28 13:12:30,905 main.py:51] epoch 2269, training loss: 10038.34, average training loss: 10091.73, base loss: 14400.30
[INFO 2017-06-28 13:12:31,574 main.py:51] epoch 2270, training loss: 10949.26, average training loss: 10092.20, base loss: 14401.39
[INFO 2017-06-28 13:12:32,252 main.py:51] epoch 2271, training loss: 10815.90, average training loss: 10092.24, base loss: 14401.85
[INFO 2017-06-28 13:12:32,910 main.py:51] epoch 2272, training loss: 9672.75, average training loss: 10091.93, base loss: 14402.22
[INFO 2017-06-28 13:12:33,548 main.py:51] epoch 2273, training loss: 10245.07, average training loss: 10092.33, base loss: 14404.14
[INFO 2017-06-28 13:12:34,203 main.py:51] epoch 2274, training loss: 10585.66, average training loss: 10093.36, base loss: 14405.99
[INFO 2017-06-28 13:12:34,845 main.py:51] epoch 2275, training loss: 10394.51, average training loss: 10091.13, base loss: 14403.82
[INFO 2017-06-28 13:12:35,497 main.py:51] epoch 2276, training loss: 10590.37, average training loss: 10092.29, base loss: 14407.44
[INFO 2017-06-28 13:12:36,157 main.py:51] epoch 2277, training loss: 10271.54, average training loss: 10093.00, base loss: 14407.93
[INFO 2017-06-28 13:12:36,813 main.py:51] epoch 2278, training loss: 9561.21, average training loss: 10092.75, base loss: 14407.33
[INFO 2017-06-28 13:12:37,481 main.py:51] epoch 2279, training loss: 8955.22, average training loss: 10092.01, base loss: 14406.66
[INFO 2017-06-28 13:12:38,144 main.py:51] epoch 2280, training loss: 9446.96, average training loss: 10090.98, base loss: 14404.97
[INFO 2017-06-28 13:12:38,812 main.py:51] epoch 2281, training loss: 8259.62, average training loss: 10089.30, base loss: 14401.93
[INFO 2017-06-28 13:12:39,470 main.py:51] epoch 2282, training loss: 8786.78, average training loss: 10089.71, base loss: 14403.06
[INFO 2017-06-28 13:12:40,111 main.py:51] epoch 2283, training loss: 14275.54, average training loss: 10094.39, base loss: 14408.83
[INFO 2017-06-28 13:12:40,772 main.py:51] epoch 2284, training loss: 9747.11, average training loss: 10092.92, base loss: 14408.03
[INFO 2017-06-28 13:12:41,417 main.py:51] epoch 2285, training loss: 11628.59, average training loss: 10095.18, base loss: 14410.51
[INFO 2017-06-28 13:12:42,077 main.py:51] epoch 2286, training loss: 10045.45, average training loss: 10095.35, base loss: 14411.26
[INFO 2017-06-28 13:12:42,739 main.py:51] epoch 2287, training loss: 9849.17, average training loss: 10094.95, base loss: 14408.90
[INFO 2017-06-28 13:12:43,394 main.py:51] epoch 2288, training loss: 9837.02, average training loss: 10094.52, base loss: 14408.25
[INFO 2017-06-28 13:12:44,063 main.py:51] epoch 2289, training loss: 9104.03, average training loss: 10093.96, base loss: 14408.28
[INFO 2017-06-28 13:12:44,725 main.py:51] epoch 2290, training loss: 10875.58, average training loss: 10093.77, base loss: 14407.40
[INFO 2017-06-28 13:12:45,395 main.py:51] epoch 2291, training loss: 8914.51, average training loss: 10091.58, base loss: 14405.03
[INFO 2017-06-28 13:12:46,063 main.py:51] epoch 2292, training loss: 9909.89, average training loss: 10091.62, base loss: 14406.00
[INFO 2017-06-28 13:12:46,730 main.py:51] epoch 2293, training loss: 10665.58, average training loss: 10090.77, base loss: 14404.94
[INFO 2017-06-28 13:12:47,386 main.py:51] epoch 2294, training loss: 9793.87, average training loss: 10089.86, base loss: 14403.82
[INFO 2017-06-28 13:12:48,058 main.py:51] epoch 2295, training loss: 8989.54, average training loss: 10089.43, base loss: 14401.96
[INFO 2017-06-28 13:12:48,735 main.py:51] epoch 2296, training loss: 9940.45, average training loss: 10089.44, base loss: 14402.69
[INFO 2017-06-28 13:12:49,373 main.py:51] epoch 2297, training loss: 8995.03, average training loss: 10089.94, base loss: 14404.59
[INFO 2017-06-28 13:12:50,022 main.py:51] epoch 2298, training loss: 9418.85, average training loss: 10089.85, base loss: 14403.99
[INFO 2017-06-28 13:12:50,675 main.py:51] epoch 2299, training loss: 9020.08, average training loss: 10088.40, base loss: 14403.12
[INFO 2017-06-28 13:12:50,675 main.py:53] epoch 2299, testing
[INFO 2017-06-28 13:12:53,258 main.py:105] average testing loss: 10852.84, base loss: 14924.11
[INFO 2017-06-28 13:12:53,258 main.py:106] improve_loss: 4071.27, improve_percent: 0.27
[INFO 2017-06-28 13:12:53,258 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:12:53,934 main.py:51] epoch 2300, training loss: 9175.42, average training loss: 10085.61, base loss: 14399.43
[INFO 2017-06-28 13:12:54,610 main.py:51] epoch 2301, training loss: 11978.36, average training loss: 10086.57, base loss: 14400.89
[INFO 2017-06-28 13:12:55,266 main.py:51] epoch 2302, training loss: 10268.73, average training loss: 10087.45, base loss: 14400.79
[INFO 2017-06-28 13:12:55,932 main.py:51] epoch 2303, training loss: 10077.69, average training loss: 10087.79, base loss: 14402.49
[INFO 2017-06-28 13:12:56,597 main.py:51] epoch 2304, training loss: 10903.95, average training loss: 10086.32, base loss: 14401.63
[INFO 2017-06-28 13:12:57,288 main.py:51] epoch 2305, training loss: 10186.88, average training loss: 10086.55, base loss: 14400.26
[INFO 2017-06-28 13:12:57,945 main.py:51] epoch 2306, training loss: 8839.26, average training loss: 10085.70, base loss: 14399.81
[INFO 2017-06-28 13:12:58,609 main.py:51] epoch 2307, training loss: 8648.45, average training loss: 10083.89, base loss: 14396.89
[INFO 2017-06-28 13:12:59,249 main.py:51] epoch 2308, training loss: 9155.05, average training loss: 10080.75, base loss: 14392.22
[INFO 2017-06-28 13:12:59,913 main.py:51] epoch 2309, training loss: 10059.81, average training loss: 10080.91, base loss: 14391.93
[INFO 2017-06-28 13:13:00,590 main.py:51] epoch 2310, training loss: 9552.29, average training loss: 10080.26, base loss: 14391.94
[INFO 2017-06-28 13:13:01,245 main.py:51] epoch 2311, training loss: 10210.42, average training loss: 10080.27, base loss: 14393.27
[INFO 2017-06-28 13:13:01,891 main.py:51] epoch 2312, training loss: 11002.61, average training loss: 10081.66, base loss: 14395.64
[INFO 2017-06-28 13:13:02,549 main.py:51] epoch 2313, training loss: 10296.66, average training loss: 10082.62, base loss: 14395.37
[INFO 2017-06-28 13:13:03,234 main.py:51] epoch 2314, training loss: 11175.12, average training loss: 10083.38, base loss: 14396.12
[INFO 2017-06-28 13:13:03,894 main.py:51] epoch 2315, training loss: 9744.09, average training loss: 10082.88, base loss: 14394.54
[INFO 2017-06-28 13:13:04,550 main.py:51] epoch 2316, training loss: 9259.38, average training loss: 10083.42, base loss: 14394.55
[INFO 2017-06-28 13:13:05,219 main.py:51] epoch 2317, training loss: 10388.83, average training loss: 10084.52, base loss: 14396.83
[INFO 2017-06-28 13:13:05,894 main.py:51] epoch 2318, training loss: 8902.78, average training loss: 10083.48, base loss: 14396.86
[INFO 2017-06-28 13:13:06,559 main.py:51] epoch 2319, training loss: 10085.29, average training loss: 10083.70, base loss: 14396.01
[INFO 2017-06-28 13:13:07,206 main.py:51] epoch 2320, training loss: 10103.23, average training loss: 10083.76, base loss: 14395.47
[INFO 2017-06-28 13:13:07,862 main.py:51] epoch 2321, training loss: 9874.73, average training loss: 10083.10, base loss: 14395.43
[INFO 2017-06-28 13:13:08,537 main.py:51] epoch 2322, training loss: 11066.64, average training loss: 10084.44, base loss: 14398.50
[INFO 2017-06-28 13:13:09,184 main.py:51] epoch 2323, training loss: 10330.88, average training loss: 10085.11, base loss: 14399.53
[INFO 2017-06-28 13:13:09,846 main.py:51] epoch 2324, training loss: 9687.32, average training loss: 10084.47, base loss: 14398.38
[INFO 2017-06-28 13:13:10,496 main.py:51] epoch 2325, training loss: 9706.29, average training loss: 10084.74, base loss: 14398.90
[INFO 2017-06-28 13:13:11,161 main.py:51] epoch 2326, training loss: 10345.83, average training loss: 10084.29, base loss: 14399.26
[INFO 2017-06-28 13:13:11,834 main.py:51] epoch 2327, training loss: 9664.19, average training loss: 10082.14, base loss: 14397.23
[INFO 2017-06-28 13:13:12,518 main.py:51] epoch 2328, training loss: 9616.09, average training loss: 10080.79, base loss: 14397.61
[INFO 2017-06-28 13:13:13,162 main.py:51] epoch 2329, training loss: 8641.04, average training loss: 10080.21, base loss: 14397.82
[INFO 2017-06-28 13:13:13,819 main.py:51] epoch 2330, training loss: 8890.86, average training loss: 10080.19, base loss: 14397.26
[INFO 2017-06-28 13:13:14,497 main.py:51] epoch 2331, training loss: 10186.22, average training loss: 10079.25, base loss: 14396.26
[INFO 2017-06-28 13:13:15,135 main.py:51] epoch 2332, training loss: 10243.83, average training loss: 10078.64, base loss: 14394.98
[INFO 2017-06-28 13:13:15,798 main.py:51] epoch 2333, training loss: 13748.60, average training loss: 10081.58, base loss: 14399.83
[INFO 2017-06-28 13:13:16,462 main.py:51] epoch 2334, training loss: 10086.98, average training loss: 10081.07, base loss: 14399.53
[INFO 2017-06-28 13:13:17,106 main.py:51] epoch 2335, training loss: 9700.29, average training loss: 10082.35, base loss: 14400.47
[INFO 2017-06-28 13:13:17,757 main.py:51] epoch 2336, training loss: 9399.88, average training loss: 10079.96, base loss: 14397.02
[INFO 2017-06-28 13:13:18,420 main.py:51] epoch 2337, training loss: 10121.22, average training loss: 10079.84, base loss: 14397.48
[INFO 2017-06-28 13:13:19,078 main.py:51] epoch 2338, training loss: 10870.24, average training loss: 10080.01, base loss: 14397.05
[INFO 2017-06-28 13:13:19,756 main.py:51] epoch 2339, training loss: 9726.96, average training loss: 10079.63, base loss: 14398.29
[INFO 2017-06-28 13:13:20,401 main.py:51] epoch 2340, training loss: 11450.65, average training loss: 10081.74, base loss: 14402.24
[INFO 2017-06-28 13:13:21,058 main.py:51] epoch 2341, training loss: 9951.24, average training loss: 10082.52, base loss: 14404.69
[INFO 2017-06-28 13:13:21,713 main.py:51] epoch 2342, training loss: 10869.58, average training loss: 10081.89, base loss: 14403.73
[INFO 2017-06-28 13:13:22,376 main.py:51] epoch 2343, training loss: 9511.78, average training loss: 10080.32, base loss: 14402.04
[INFO 2017-06-28 13:13:23,039 main.py:51] epoch 2344, training loss: 10180.02, average training loss: 10080.97, base loss: 14403.57
[INFO 2017-06-28 13:13:23,676 main.py:51] epoch 2345, training loss: 10892.48, average training loss: 10081.28, base loss: 14405.53
[INFO 2017-06-28 13:13:24,338 main.py:51] epoch 2346, training loss: 9393.20, average training loss: 10077.97, base loss: 14400.46
[INFO 2017-06-28 13:13:24,998 main.py:51] epoch 2347, training loss: 9346.70, average training loss: 10076.60, base loss: 14397.86
[INFO 2017-06-28 13:13:25,635 main.py:51] epoch 2348, training loss: 10229.95, average training loss: 10077.41, base loss: 14398.86
[INFO 2017-06-28 13:13:26,306 main.py:51] epoch 2349, training loss: 10424.74, average training loss: 10079.65, base loss: 14401.93
[INFO 2017-06-28 13:13:26,971 main.py:51] epoch 2350, training loss: 11887.64, average training loss: 10081.74, base loss: 14406.67
[INFO 2017-06-28 13:13:27,629 main.py:51] epoch 2351, training loss: 9524.97, average training loss: 10081.90, base loss: 14407.41
[INFO 2017-06-28 13:13:28,277 main.py:51] epoch 2352, training loss: 10227.99, average training loss: 10083.94, base loss: 14410.50
[INFO 2017-06-28 13:13:28,937 main.py:51] epoch 2353, training loss: 10259.87, average training loss: 10084.27, base loss: 14412.90
[INFO 2017-06-28 13:13:29,614 main.py:51] epoch 2354, training loss: 10291.44, average training loss: 10085.94, base loss: 14413.11
[INFO 2017-06-28 13:13:30,280 main.py:51] epoch 2355, training loss: 8964.87, average training loss: 10084.60, base loss: 14411.54
[INFO 2017-06-28 13:13:30,944 main.py:51] epoch 2356, training loss: 9140.79, average training loss: 10083.29, base loss: 14409.02
[INFO 2017-06-28 13:13:31,606 main.py:51] epoch 2357, training loss: 9000.37, average training loss: 10082.16, base loss: 14408.91
[INFO 2017-06-28 13:13:32,287 main.py:51] epoch 2358, training loss: 8708.33, average training loss: 10081.38, base loss: 14406.87
[INFO 2017-06-28 13:13:32,968 main.py:51] epoch 2359, training loss: 9579.68, average training loss: 10081.29, base loss: 14407.78
[INFO 2017-06-28 13:13:33,625 main.py:51] epoch 2360, training loss: 10268.51, average training loss: 10082.34, base loss: 14410.15
[INFO 2017-06-28 13:13:34,284 main.py:51] epoch 2361, training loss: 10045.64, average training loss: 10082.09, base loss: 14409.92
[INFO 2017-06-28 13:13:34,933 main.py:51] epoch 2362, training loss: 10247.64, average training loss: 10082.01, base loss: 14410.36
[INFO 2017-06-28 13:13:35,610 main.py:51] epoch 2363, training loss: 10058.69, average training loss: 10080.73, base loss: 14408.88
[INFO 2017-06-28 13:13:36,277 main.py:51] epoch 2364, training loss: 11803.92, average training loss: 10082.80, base loss: 14411.61
[INFO 2017-06-28 13:13:36,955 main.py:51] epoch 2365, training loss: 10083.78, average training loss: 10081.89, base loss: 14409.30
[INFO 2017-06-28 13:13:37,604 main.py:51] epoch 2366, training loss: 9386.49, average training loss: 10081.90, base loss: 14410.00
[INFO 2017-06-28 13:13:38,267 main.py:51] epoch 2367, training loss: 9805.61, average training loss: 10082.19, base loss: 14409.95
[INFO 2017-06-28 13:13:38,941 main.py:51] epoch 2368, training loss: 11713.72, average training loss: 10085.19, base loss: 14412.89
[INFO 2017-06-28 13:13:39,609 main.py:51] epoch 2369, training loss: 9996.36, average training loss: 10085.67, base loss: 14413.35
[INFO 2017-06-28 13:13:40,267 main.py:51] epoch 2370, training loss: 10501.70, average training loss: 10084.84, base loss: 14412.29
[INFO 2017-06-28 13:13:40,950 main.py:51] epoch 2371, training loss: 9475.80, average training loss: 10084.78, base loss: 14410.52
[INFO 2017-06-28 13:13:41,600 main.py:51] epoch 2372, training loss: 9981.74, average training loss: 10084.22, base loss: 14411.03
[INFO 2017-06-28 13:13:42,284 main.py:51] epoch 2373, training loss: 9798.09, average training loss: 10083.22, base loss: 14408.35
[INFO 2017-06-28 13:13:42,924 main.py:51] epoch 2374, training loss: 11290.23, average training loss: 10084.92, base loss: 14412.29
[INFO 2017-06-28 13:13:43,608 main.py:51] epoch 2375, training loss: 10516.44, average training loss: 10083.83, base loss: 14411.24
[INFO 2017-06-28 13:13:44,243 main.py:51] epoch 2376, training loss: 10589.80, average training loss: 10084.78, base loss: 14412.30
[INFO 2017-06-28 13:13:44,897 main.py:51] epoch 2377, training loss: 10562.95, average training loss: 10085.44, base loss: 14413.11
[INFO 2017-06-28 13:13:45,555 main.py:51] epoch 2378, training loss: 8623.10, average training loss: 10084.05, base loss: 14411.23
[INFO 2017-06-28 13:13:46,210 main.py:51] epoch 2379, training loss: 10211.83, average training loss: 10084.74, base loss: 14413.38
[INFO 2017-06-28 13:13:46,862 main.py:51] epoch 2380, training loss: 9424.54, average training loss: 10084.63, base loss: 14412.67
[INFO 2017-06-28 13:13:47,517 main.py:51] epoch 2381, training loss: 9455.83, average training loss: 10083.88, base loss: 14412.86
[INFO 2017-06-28 13:13:48,163 main.py:51] epoch 2382, training loss: 8829.40, average training loss: 10084.09, base loss: 14413.06
[INFO 2017-06-28 13:13:48,838 main.py:51] epoch 2383, training loss: 9940.55, average training loss: 10084.61, base loss: 14413.96
[INFO 2017-06-28 13:13:49,528 main.py:51] epoch 2384, training loss: 8954.74, average training loss: 10083.09, base loss: 14412.77
[INFO 2017-06-28 13:13:50,218 main.py:51] epoch 2385, training loss: 9973.60, average training loss: 10082.74, base loss: 14412.65
[INFO 2017-06-28 13:13:50,849 main.py:51] epoch 2386, training loss: 8994.20, average training loss: 10080.94, base loss: 14411.06
[INFO 2017-06-28 13:13:51,510 main.py:51] epoch 2387, training loss: 8999.51, average training loss: 10079.48, base loss: 14409.76
[INFO 2017-06-28 13:13:52,182 main.py:51] epoch 2388, training loss: 10041.36, average training loss: 10077.65, base loss: 14408.40
[INFO 2017-06-28 13:13:52,841 main.py:51] epoch 2389, training loss: 10837.90, average training loss: 10078.76, base loss: 14411.05
[INFO 2017-06-28 13:13:53,490 main.py:51] epoch 2390, training loss: 10184.14, average training loss: 10078.40, base loss: 14410.71
[INFO 2017-06-28 13:13:54,148 main.py:51] epoch 2391, training loss: 8968.10, average training loss: 10075.45, base loss: 14408.93
[INFO 2017-06-28 13:13:54,788 main.py:51] epoch 2392, training loss: 10050.58, average training loss: 10075.82, base loss: 14409.86
[INFO 2017-06-28 13:13:55,439 main.py:51] epoch 2393, training loss: 10820.56, average training loss: 10073.46, base loss: 14406.78
[INFO 2017-06-28 13:13:56,121 main.py:51] epoch 2394, training loss: 10474.59, average training loss: 10074.51, base loss: 14409.14
[INFO 2017-06-28 13:13:56,788 main.py:51] epoch 2395, training loss: 10814.70, average training loss: 10076.65, base loss: 14413.23
[INFO 2017-06-28 13:13:57,445 main.py:51] epoch 2396, training loss: 11383.33, average training loss: 10078.20, base loss: 14414.55
[INFO 2017-06-28 13:13:58,098 main.py:51] epoch 2397, training loss: 9973.49, average training loss: 10077.85, base loss: 14413.13
[INFO 2017-06-28 13:13:58,777 main.py:51] epoch 2398, training loss: 10277.82, average training loss: 10077.35, base loss: 14413.77
[INFO 2017-06-28 13:13:59,449 main.py:51] epoch 2399, training loss: 9964.73, average training loss: 10078.38, base loss: 14415.50
[INFO 2017-06-28 13:13:59,449 main.py:53] epoch 2399, testing
[INFO 2017-06-28 13:14:02,162 main.py:105] average testing loss: 10921.42, base loss: 14892.34
[INFO 2017-06-28 13:14:02,162 main.py:106] improve_loss: 3970.92, improve_percent: 0.27
[INFO 2017-06-28 13:14:02,163 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:14:02,845 main.py:51] epoch 2400, training loss: 9742.28, average training loss: 10077.66, base loss: 14416.14
[INFO 2017-06-28 13:14:03,512 main.py:51] epoch 2401, training loss: 8725.25, average training loss: 10078.20, base loss: 14417.44
[INFO 2017-06-28 13:14:04,184 main.py:51] epoch 2402, training loss: 10795.55, average training loss: 10079.54, base loss: 14420.28
[INFO 2017-06-28 13:14:04,821 main.py:51] epoch 2403, training loss: 11356.71, average training loss: 10081.78, base loss: 14424.52
[INFO 2017-06-28 13:14:05,477 main.py:51] epoch 2404, training loss: 9572.84, average training loss: 10082.02, base loss: 14425.36
[INFO 2017-06-28 13:14:06,114 main.py:51] epoch 2405, training loss: 10268.00, average training loss: 10082.64, base loss: 14424.90
[INFO 2017-06-28 13:14:06,767 main.py:51] epoch 2406, training loss: 9698.12, average training loss: 10081.85, base loss: 14424.54
[INFO 2017-06-28 13:14:07,423 main.py:51] epoch 2407, training loss: 9033.99, average training loss: 10080.90, base loss: 14423.46
[INFO 2017-06-28 13:14:08,066 main.py:51] epoch 2408, training loss: 8942.40, average training loss: 10079.66, base loss: 14422.41
[INFO 2017-06-28 13:14:08,706 main.py:51] epoch 2409, training loss: 10176.14, average training loss: 10078.14, base loss: 14421.44
[INFO 2017-06-28 13:14:09,345 main.py:51] epoch 2410, training loss: 8684.54, average training loss: 10077.21, base loss: 14421.43
[INFO 2017-06-28 13:14:09,998 main.py:51] epoch 2411, training loss: 9865.90, average training loss: 10077.04, base loss: 14421.83
[INFO 2017-06-28 13:14:10,669 main.py:51] epoch 2412, training loss: 10846.49, average training loss: 10078.35, base loss: 14424.79
[INFO 2017-06-28 13:14:11,321 main.py:51] epoch 2413, training loss: 9868.26, average training loss: 10078.08, base loss: 14425.73
[INFO 2017-06-28 13:14:11,962 main.py:51] epoch 2414, training loss: 9131.95, average training loss: 10077.40, base loss: 14423.98
[INFO 2017-06-28 13:14:12,610 main.py:51] epoch 2415, training loss: 10358.86, average training loss: 10077.07, base loss: 14423.84
[INFO 2017-06-28 13:14:13,266 main.py:51] epoch 2416, training loss: 9608.44, average training loss: 10076.74, base loss: 14424.63
[INFO 2017-06-28 13:14:13,925 main.py:51] epoch 2417, training loss: 8547.69, average training loss: 10075.73, base loss: 14423.43
[INFO 2017-06-28 13:14:14,590 main.py:51] epoch 2418, training loss: 11549.58, average training loss: 10077.15, base loss: 14426.82
[INFO 2017-06-28 13:14:15,239 main.py:51] epoch 2419, training loss: 10468.73, average training loss: 10076.62, base loss: 14428.72
[INFO 2017-06-28 13:14:15,901 main.py:51] epoch 2420, training loss: 10793.26, average training loss: 10077.05, base loss: 14428.93
[INFO 2017-06-28 13:14:16,562 main.py:51] epoch 2421, training loss: 9087.10, average training loss: 10076.80, base loss: 14430.08
[INFO 2017-06-28 13:14:17,222 main.py:51] epoch 2422, training loss: 9871.44, average training loss: 10075.85, base loss: 14429.13
[INFO 2017-06-28 13:14:17,869 main.py:51] epoch 2423, training loss: 9448.57, average training loss: 10074.38, base loss: 14428.58
[INFO 2017-06-28 13:14:18,510 main.py:51] epoch 2424, training loss: 11090.03, average training loss: 10076.11, base loss: 14430.62
[INFO 2017-06-28 13:14:19,186 main.py:51] epoch 2425, training loss: 10572.87, average training loss: 10076.58, base loss: 14430.81
[INFO 2017-06-28 13:14:19,841 main.py:51] epoch 2426, training loss: 9041.92, average training loss: 10075.89, base loss: 14429.62
[INFO 2017-06-28 13:14:20,489 main.py:51] epoch 2427, training loss: 11030.49, average training loss: 10078.21, base loss: 14434.43
[INFO 2017-06-28 13:14:21,132 main.py:51] epoch 2428, training loss: 10623.50, average training loss: 10077.59, base loss: 14432.45
[INFO 2017-06-28 13:14:21,767 main.py:51] epoch 2429, training loss: 9413.40, average training loss: 10077.08, base loss: 14431.87
[INFO 2017-06-28 13:14:22,414 main.py:51] epoch 2430, training loss: 10843.03, average training loss: 10079.02, base loss: 14435.05
[INFO 2017-06-28 13:14:23,079 main.py:51] epoch 2431, training loss: 10306.87, average training loss: 10079.87, base loss: 14436.23
[INFO 2017-06-28 13:14:23,742 main.py:51] epoch 2432, training loss: 10450.96, average training loss: 10078.55, base loss: 14434.24
[INFO 2017-06-28 13:14:24,405 main.py:51] epoch 2433, training loss: 11080.73, average training loss: 10078.96, base loss: 14435.55
[INFO 2017-06-28 13:14:25,082 main.py:51] epoch 2434, training loss: 10526.08, average training loss: 10078.42, base loss: 14435.34
[INFO 2017-06-28 13:14:25,740 main.py:51] epoch 2435, training loss: 11909.24, average training loss: 10079.61, base loss: 14438.15
[INFO 2017-06-28 13:14:26,400 main.py:51] epoch 2436, training loss: 10476.94, average training loss: 10078.07, base loss: 14434.20
[INFO 2017-06-28 13:14:27,049 main.py:51] epoch 2437, training loss: 10434.47, average training loss: 10078.87, base loss: 14435.82
[INFO 2017-06-28 13:14:27,714 main.py:51] epoch 2438, training loss: 9001.82, average training loss: 10076.70, base loss: 14434.36
[INFO 2017-06-28 13:14:28,378 main.py:51] epoch 2439, training loss: 9370.12, average training loss: 10075.18, base loss: 14432.17
[INFO 2017-06-28 13:14:29,025 main.py:51] epoch 2440, training loss: 9218.24, average training loss: 10073.85, base loss: 14430.46
[INFO 2017-06-28 13:14:29,684 main.py:51] epoch 2441, training loss: 9797.66, average training loss: 10074.26, base loss: 14430.89
[INFO 2017-06-28 13:14:30,337 main.py:51] epoch 2442, training loss: 10272.54, average training loss: 10073.31, base loss: 14429.26
[INFO 2017-06-28 13:14:30,986 main.py:51] epoch 2443, training loss: 9162.14, average training loss: 10071.87, base loss: 14426.80
[INFO 2017-06-28 13:14:31,648 main.py:51] epoch 2444, training loss: 11993.40, average training loss: 10072.93, base loss: 14429.18
[INFO 2017-06-28 13:14:32,289 main.py:51] epoch 2445, training loss: 9011.33, average training loss: 10071.74, base loss: 14427.46
[INFO 2017-06-28 13:14:32,926 main.py:51] epoch 2446, training loss: 10172.05, average training loss: 10070.34, base loss: 14426.34
[INFO 2017-06-28 13:14:33,561 main.py:51] epoch 2447, training loss: 10957.01, average training loss: 10071.02, base loss: 14427.65
[INFO 2017-06-28 13:14:34,221 main.py:51] epoch 2448, training loss: 9168.02, average training loss: 10070.01, base loss: 14425.13
[INFO 2017-06-28 13:14:34,881 main.py:51] epoch 2449, training loss: 10980.36, average training loss: 10071.11, base loss: 14425.89
[INFO 2017-06-28 13:14:35,542 main.py:51] epoch 2450, training loss: 9497.70, average training loss: 10071.15, base loss: 14425.40
[INFO 2017-06-28 13:14:36,166 main.py:51] epoch 2451, training loss: 8115.51, average training loss: 10070.15, base loss: 14423.05
[INFO 2017-06-28 13:14:36,812 main.py:51] epoch 2452, training loss: 10466.84, average training loss: 10071.43, base loss: 14423.32
[INFO 2017-06-28 13:14:37,463 main.py:51] epoch 2453, training loss: 10398.28, average training loss: 10071.60, base loss: 14423.22
[INFO 2017-06-28 13:14:38,122 main.py:51] epoch 2454, training loss: 10764.48, average training loss: 10071.36, base loss: 14423.16
[INFO 2017-06-28 13:14:38,765 main.py:51] epoch 2455, training loss: 9348.30, average training loss: 10071.40, base loss: 14424.28
[INFO 2017-06-28 13:14:39,433 main.py:51] epoch 2456, training loss: 8798.10, average training loss: 10068.22, base loss: 14418.94
[INFO 2017-06-28 13:14:40,085 main.py:51] epoch 2457, training loss: 11209.43, average training loss: 10068.79, base loss: 14417.02
[INFO 2017-06-28 13:14:40,758 main.py:51] epoch 2458, training loss: 9973.21, average training loss: 10069.70, base loss: 14417.31
[INFO 2017-06-28 13:14:41,409 main.py:51] epoch 2459, training loss: 10673.90, average training loss: 10069.06, base loss: 14416.41
[INFO 2017-06-28 13:14:42,061 main.py:51] epoch 2460, training loss: 8987.43, average training loss: 10067.38, base loss: 14415.80
[INFO 2017-06-28 13:14:42,720 main.py:51] epoch 2461, training loss: 10198.97, average training loss: 10067.88, base loss: 14417.84
[INFO 2017-06-28 13:14:43,385 main.py:51] epoch 2462, training loss: 10916.76, average training loss: 10068.45, base loss: 14420.47
[INFO 2017-06-28 13:14:44,050 main.py:51] epoch 2463, training loss: 9849.51, average training loss: 10067.92, base loss: 14420.60
[INFO 2017-06-28 13:14:44,695 main.py:51] epoch 2464, training loss: 9708.49, average training loss: 10067.60, base loss: 14419.34
[INFO 2017-06-28 13:14:45,351 main.py:51] epoch 2465, training loss: 9531.90, average training loss: 10067.58, base loss: 14419.42
[INFO 2017-06-28 13:14:46,027 main.py:51] epoch 2466, training loss: 10141.69, average training loss: 10067.07, base loss: 14417.58
[INFO 2017-06-28 13:14:46,692 main.py:51] epoch 2467, training loss: 8349.67, average training loss: 10063.19, base loss: 14412.60
[INFO 2017-06-28 13:14:47,338 main.py:51] epoch 2468, training loss: 8944.67, average training loss: 10062.40, base loss: 14412.60
[INFO 2017-06-28 13:14:47,986 main.py:51] epoch 2469, training loss: 11461.35, average training loss: 10063.70, base loss: 14415.26
[INFO 2017-06-28 13:14:48,655 main.py:51] epoch 2470, training loss: 9431.29, average training loss: 10060.53, base loss: 14409.76
[INFO 2017-06-28 13:14:49,302 main.py:51] epoch 2471, training loss: 9746.70, average training loss: 10059.61, base loss: 14408.95
[INFO 2017-06-28 13:14:49,948 main.py:51] epoch 2472, training loss: 9985.08, average training loss: 10061.22, base loss: 14412.29
[INFO 2017-06-28 13:14:50,604 main.py:51] epoch 2473, training loss: 9672.50, average training loss: 10059.66, base loss: 14410.44
[INFO 2017-06-28 13:14:51,277 main.py:51] epoch 2474, training loss: 9772.94, average training loss: 10058.81, base loss: 14408.50
[INFO 2017-06-28 13:14:51,943 main.py:51] epoch 2475, training loss: 11600.06, average training loss: 10060.05, base loss: 14410.27
[INFO 2017-06-28 13:14:52,604 main.py:51] epoch 2476, training loss: 10811.92, average training loss: 10059.94, base loss: 14410.97
[INFO 2017-06-28 13:14:53,294 main.py:51] epoch 2477, training loss: 10866.07, average training loss: 10058.53, base loss: 14409.90
[INFO 2017-06-28 13:14:53,934 main.py:51] epoch 2478, training loss: 10505.48, average training loss: 10057.26, base loss: 14408.57
[INFO 2017-06-28 13:14:54,595 main.py:51] epoch 2479, training loss: 9511.06, average training loss: 10056.82, base loss: 14407.99
[INFO 2017-06-28 13:14:55,266 main.py:51] epoch 2480, training loss: 9453.97, average training loss: 10055.06, base loss: 14404.19
[INFO 2017-06-28 13:14:55,939 main.py:51] epoch 2481, training loss: 9411.17, average training loss: 10054.51, base loss: 14403.50
[INFO 2017-06-28 13:14:56,611 main.py:51] epoch 2482, training loss: 10633.89, average training loss: 10054.43, base loss: 14405.40
[INFO 2017-06-28 13:14:57,265 main.py:51] epoch 2483, training loss: 11194.89, average training loss: 10056.06, base loss: 14408.03
[INFO 2017-06-28 13:14:57,910 main.py:51] epoch 2484, training loss: 10317.48, average training loss: 10056.90, base loss: 14409.01
[INFO 2017-06-28 13:14:58,585 main.py:51] epoch 2485, training loss: 9673.43, average training loss: 10056.38, base loss: 14408.33
[INFO 2017-06-28 13:14:59,233 main.py:51] epoch 2486, training loss: 9258.35, average training loss: 10056.03, base loss: 14408.62
[INFO 2017-06-28 13:14:59,872 main.py:51] epoch 2487, training loss: 9521.87, average training loss: 10056.65, base loss: 14410.22
[INFO 2017-06-28 13:15:00,529 main.py:51] epoch 2488, training loss: 10029.59, average training loss: 10056.66, base loss: 14412.08
[INFO 2017-06-28 13:15:01,193 main.py:51] epoch 2489, training loss: 10221.97, average training loss: 10057.08, base loss: 14413.01
[INFO 2017-06-28 13:15:01,866 main.py:51] epoch 2490, training loss: 11125.90, average training loss: 10058.52, base loss: 14414.29
[INFO 2017-06-28 13:15:02,519 main.py:51] epoch 2491, training loss: 11075.47, average training loss: 10060.52, base loss: 14418.30
[INFO 2017-06-28 13:15:03,170 main.py:51] epoch 2492, training loss: 9479.53, average training loss: 10059.99, base loss: 14417.06
[INFO 2017-06-28 13:15:03,820 main.py:51] epoch 2493, training loss: 9104.49, average training loss: 10058.24, base loss: 14413.95
[INFO 2017-06-28 13:15:04,477 main.py:51] epoch 2494, training loss: 9358.11, average training loss: 10057.71, base loss: 14414.62
[INFO 2017-06-28 13:15:05,131 main.py:51] epoch 2495, training loss: 9945.29, average training loss: 10057.43, base loss: 14415.88
[INFO 2017-06-28 13:15:05,804 main.py:51] epoch 2496, training loss: 10910.86, average training loss: 10057.92, base loss: 14415.45
[INFO 2017-06-28 13:15:06,468 main.py:51] epoch 2497, training loss: 10118.44, average training loss: 10059.39, base loss: 14417.82
[INFO 2017-06-28 13:15:07,122 main.py:51] epoch 2498, training loss: 10316.81, average training loss: 10059.83, base loss: 14418.68
[INFO 2017-06-28 13:15:07,760 main.py:51] epoch 2499, training loss: 9993.12, average training loss: 10059.29, base loss: 14418.93
[INFO 2017-06-28 13:15:07,760 main.py:53] epoch 2499, testing
[INFO 2017-06-28 13:15:10,332 main.py:105] average testing loss: 10945.71, base loss: 15251.68
[INFO 2017-06-28 13:15:10,332 main.py:106] improve_loss: 4305.97, improve_percent: 0.28
[INFO 2017-06-28 13:15:10,332 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 13:15:10,369 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:15:11,033 main.py:51] epoch 2500, training loss: 11478.88, average training loss: 10060.00, base loss: 14420.74
[INFO 2017-06-28 13:15:11,685 main.py:51] epoch 2501, training loss: 8996.64, average training loss: 10056.60, base loss: 14417.07
[INFO 2017-06-28 13:15:12,335 main.py:51] epoch 2502, training loss: 10108.68, average training loss: 10057.67, base loss: 14417.24
[INFO 2017-06-28 13:15:13,013 main.py:51] epoch 2503, training loss: 10225.31, average training loss: 10057.76, base loss: 14417.57
[INFO 2017-06-28 13:15:13,674 main.py:51] epoch 2504, training loss: 9066.37, average training loss: 10056.05, base loss: 14416.20
[INFO 2017-06-28 13:15:14,331 main.py:51] epoch 2505, training loss: 8751.61, average training loss: 10054.56, base loss: 14414.74
[INFO 2017-06-28 13:15:14,976 main.py:51] epoch 2506, training loss: 9744.85, average training loss: 10053.25, base loss: 14412.45
[INFO 2017-06-28 13:15:15,648 main.py:51] epoch 2507, training loss: 9629.84, average training loss: 10052.71, base loss: 14411.67
[INFO 2017-06-28 13:15:16,336 main.py:51] epoch 2508, training loss: 11750.50, average training loss: 10054.65, base loss: 14413.41
[INFO 2017-06-28 13:15:16,991 main.py:51] epoch 2509, training loss: 9093.00, average training loss: 10054.23, base loss: 14413.35
[INFO 2017-06-28 13:15:17,653 main.py:51] epoch 2510, training loss: 9132.81, average training loss: 10052.82, base loss: 14410.61
[INFO 2017-06-28 13:15:18,299 main.py:51] epoch 2511, training loss: 13154.12, average training loss: 10054.25, base loss: 14413.24
[INFO 2017-06-28 13:15:18,955 main.py:51] epoch 2512, training loss: 9810.62, average training loss: 10054.17, base loss: 14413.04
[INFO 2017-06-28 13:15:19,631 main.py:51] epoch 2513, training loss: 8592.70, average training loss: 10052.51, base loss: 14409.71
[INFO 2017-06-28 13:15:20,284 main.py:51] epoch 2514, training loss: 10950.36, average training loss: 10051.90, base loss: 14410.20
[INFO 2017-06-28 13:15:20,936 main.py:51] epoch 2515, training loss: 9014.35, average training loss: 10051.09, base loss: 14409.04
[INFO 2017-06-28 13:15:21,610 main.py:51] epoch 2516, training loss: 10091.83, average training loss: 10051.03, base loss: 14409.32
[INFO 2017-06-28 13:15:22,270 main.py:51] epoch 2517, training loss: 8647.31, average training loss: 10049.43, base loss: 14408.08
[INFO 2017-06-28 13:15:22,933 main.py:51] epoch 2518, training loss: 10442.59, average training loss: 10050.55, base loss: 14411.70
[INFO 2017-06-28 13:15:23,630 main.py:51] epoch 2519, training loss: 10100.75, average training loss: 10050.83, base loss: 14411.67
[INFO 2017-06-28 13:15:24,293 main.py:51] epoch 2520, training loss: 8485.11, average training loss: 10050.20, base loss: 14412.51
[INFO 2017-06-28 13:15:24,942 main.py:51] epoch 2521, training loss: 9788.48, average training loss: 10050.29, base loss: 14413.78
[INFO 2017-06-28 13:15:25,595 main.py:51] epoch 2522, training loss: 9960.96, average training loss: 10049.97, base loss: 14412.57
[INFO 2017-06-28 13:15:26,255 main.py:51] epoch 2523, training loss: 9632.73, average training loss: 10047.91, base loss: 14410.16
[INFO 2017-06-28 13:15:26,912 main.py:51] epoch 2524, training loss: 10049.79, average training loss: 10047.08, base loss: 14409.57
[INFO 2017-06-28 13:15:27,565 main.py:51] epoch 2525, training loss: 10329.09, average training loss: 10047.41, base loss: 14411.70
[INFO 2017-06-28 13:15:28,226 main.py:51] epoch 2526, training loss: 9457.37, average training loss: 10047.05, base loss: 14410.87
[INFO 2017-06-28 13:15:28,864 main.py:51] epoch 2527, training loss: 11507.62, average training loss: 10049.01, base loss: 14412.99
[INFO 2017-06-28 13:15:29,509 main.py:51] epoch 2528, training loss: 10688.55, average training loss: 10050.14, base loss: 14415.54
[INFO 2017-06-28 13:15:30,164 main.py:51] epoch 2529, training loss: 12053.56, average training loss: 10051.77, base loss: 14417.69
[INFO 2017-06-28 13:15:30,805 main.py:51] epoch 2530, training loss: 9194.11, average training loss: 10049.89, base loss: 14414.82
[INFO 2017-06-28 13:15:31,448 main.py:51] epoch 2531, training loss: 9980.57, average training loss: 10050.84, base loss: 14416.55
[INFO 2017-06-28 13:15:32,128 main.py:51] epoch 2532, training loss: 9437.02, average training loss: 10049.91, base loss: 14416.38
[INFO 2017-06-28 13:15:32,791 main.py:51] epoch 2533, training loss: 10529.65, average training loss: 10051.51, base loss: 14419.22
[INFO 2017-06-28 13:15:33,460 main.py:51] epoch 2534, training loss: 10367.81, average training loss: 10051.50, base loss: 14418.49
[INFO 2017-06-28 13:15:34,143 main.py:51] epoch 2535, training loss: 10161.77, average training loss: 10050.56, base loss: 14417.34
[INFO 2017-06-28 13:15:34,786 main.py:51] epoch 2536, training loss: 14379.53, average training loss: 10054.20, base loss: 14421.17
[INFO 2017-06-28 13:15:35,444 main.py:51] epoch 2537, training loss: 10036.46, average training loss: 10053.96, base loss: 14421.08
[INFO 2017-06-28 13:15:36,080 main.py:51] epoch 2538, training loss: 9526.11, average training loss: 10053.66, base loss: 14421.52
[INFO 2017-06-28 13:15:36,718 main.py:51] epoch 2539, training loss: 9843.25, average training loss: 10053.70, base loss: 14421.73
[INFO 2017-06-28 13:15:37,377 main.py:51] epoch 2540, training loss: 9186.42, average training loss: 10052.14, base loss: 14419.83
[INFO 2017-06-28 13:15:38,033 main.py:51] epoch 2541, training loss: 9595.18, average training loss: 10051.69, base loss: 14418.70
[INFO 2017-06-28 13:15:38,674 main.py:51] epoch 2542, training loss: 9422.40, average training loss: 10051.01, base loss: 14417.17
[INFO 2017-06-28 13:15:39,321 main.py:51] epoch 2543, training loss: 10944.94, average training loss: 10052.74, base loss: 14418.63
[INFO 2017-06-28 13:15:39,961 main.py:51] epoch 2544, training loss: 9800.18, average training loss: 10050.84, base loss: 14415.65
[INFO 2017-06-28 13:15:40,624 main.py:51] epoch 2545, training loss: 10521.23, average training loss: 10051.29, base loss: 14416.11
[INFO 2017-06-28 13:15:41,272 main.py:51] epoch 2546, training loss: 10219.15, average training loss: 10050.69, base loss: 14416.25
[INFO 2017-06-28 13:15:41,915 main.py:51] epoch 2547, training loss: 9428.76, average training loss: 10049.76, base loss: 14414.50
[INFO 2017-06-28 13:15:42,573 main.py:51] epoch 2548, training loss: 9085.91, average training loss: 10049.60, base loss: 14416.19
[INFO 2017-06-28 13:15:43,216 main.py:51] epoch 2549, training loss: 9196.00, average training loss: 10049.35, base loss: 14415.44
[INFO 2017-06-28 13:15:43,879 main.py:51] epoch 2550, training loss: 9298.05, average training loss: 10046.72, base loss: 14412.30
[INFO 2017-06-28 13:15:44,537 main.py:51] epoch 2551, training loss: 10001.72, average training loss: 10047.42, base loss: 14413.59
[INFO 2017-06-28 13:15:45,197 main.py:51] epoch 2552, training loss: 11461.90, average training loss: 10048.91, base loss: 14416.03
[INFO 2017-06-28 13:15:45,833 main.py:51] epoch 2553, training loss: 10019.63, average training loss: 10049.08, base loss: 14416.93
[INFO 2017-06-28 13:15:46,489 main.py:51] epoch 2554, training loss: 10892.70, average training loss: 10049.91, base loss: 14417.90
[INFO 2017-06-28 13:15:47,157 main.py:51] epoch 2555, training loss: 9945.96, average training loss: 10049.52, base loss: 14416.39
[INFO 2017-06-28 13:15:47,804 main.py:51] epoch 2556, training loss: 9006.27, average training loss: 10048.91, base loss: 14414.37
[INFO 2017-06-28 13:15:48,447 main.py:51] epoch 2557, training loss: 10995.10, average training loss: 10050.76, base loss: 14416.04
[INFO 2017-06-28 13:15:49,103 main.py:51] epoch 2558, training loss: 9446.87, average training loss: 10050.33, base loss: 14416.42
[INFO 2017-06-28 13:15:49,759 main.py:51] epoch 2559, training loss: 9851.17, average training loss: 10051.36, base loss: 14419.24
[INFO 2017-06-28 13:15:50,424 main.py:51] epoch 2560, training loss: 10652.19, average training loss: 10052.33, base loss: 14420.90
[INFO 2017-06-28 13:15:51,089 main.py:51] epoch 2561, training loss: 11178.29, average training loss: 10054.43, base loss: 14424.53
[INFO 2017-06-28 13:15:51,745 main.py:51] epoch 2562, training loss: 10581.73, average training loss: 10054.94, base loss: 14426.17
[INFO 2017-06-28 13:15:52,427 main.py:51] epoch 2563, training loss: 9743.29, average training loss: 10054.34, base loss: 14425.56
[INFO 2017-06-28 13:15:53,064 main.py:51] epoch 2564, training loss: 8907.21, average training loss: 10054.12, base loss: 14425.78
[INFO 2017-06-28 13:15:53,726 main.py:51] epoch 2565, training loss: 11293.71, average training loss: 10055.54, base loss: 14427.50
[INFO 2017-06-28 13:15:54,421 main.py:51] epoch 2566, training loss: 11182.41, average training loss: 10058.25, base loss: 14430.68
[INFO 2017-06-28 13:15:55,093 main.py:51] epoch 2567, training loss: 9900.93, average training loss: 10057.16, base loss: 14430.62
[INFO 2017-06-28 13:15:55,745 main.py:51] epoch 2568, training loss: 11165.89, average training loss: 10059.29, base loss: 14433.98
[INFO 2017-06-28 13:15:56,396 main.py:51] epoch 2569, training loss: 10694.84, average training loss: 10059.93, base loss: 14434.34
[INFO 2017-06-28 13:15:57,047 main.py:51] epoch 2570, training loss: 8982.68, average training loss: 10059.93, base loss: 14434.57
[INFO 2017-06-28 13:15:57,703 main.py:51] epoch 2571, training loss: 8871.97, average training loss: 10057.67, base loss: 14432.03
[INFO 2017-06-28 13:15:58,383 main.py:51] epoch 2572, training loss: 10720.20, average training loss: 10058.42, base loss: 14431.23
[INFO 2017-06-28 13:15:59,048 main.py:51] epoch 2573, training loss: 9968.89, average training loss: 10058.94, base loss: 14432.16
[INFO 2017-06-28 13:15:59,722 main.py:51] epoch 2574, training loss: 10880.43, average training loss: 10059.68, base loss: 14432.19
[INFO 2017-06-28 13:16:00,388 main.py:51] epoch 2575, training loss: 11707.51, average training loss: 10062.10, base loss: 14436.15
[INFO 2017-06-28 13:16:01,044 main.py:51] epoch 2576, training loss: 9475.56, average training loss: 10062.58, base loss: 14437.04
[INFO 2017-06-28 13:16:01,711 main.py:51] epoch 2577, training loss: 9806.23, average training loss: 10061.10, base loss: 14435.13
[INFO 2017-06-28 13:16:02,370 main.py:51] epoch 2578, training loss: 10132.14, average training loss: 10061.10, base loss: 14433.83
[INFO 2017-06-28 13:16:03,016 main.py:51] epoch 2579, training loss: 10129.70, average training loss: 10059.23, base loss: 14431.10
[INFO 2017-06-28 13:16:03,654 main.py:51] epoch 2580, training loss: 10542.16, average training loss: 10058.80, base loss: 14427.72
[INFO 2017-06-28 13:16:04,326 main.py:51] epoch 2581, training loss: 9707.52, average training loss: 10058.04, base loss: 14425.39
[INFO 2017-06-28 13:16:04,972 main.py:51] epoch 2582, training loss: 9228.68, average training loss: 10058.47, base loss: 14427.43
[INFO 2017-06-28 13:16:05,627 main.py:51] epoch 2583, training loss: 8591.02, average training loss: 10057.35, base loss: 14424.78
[INFO 2017-06-28 13:16:06,301 main.py:51] epoch 2584, training loss: 10752.85, average training loss: 10058.49, base loss: 14425.20
[INFO 2017-06-28 13:16:06,959 main.py:51] epoch 2585, training loss: 11322.89, average training loss: 10060.06, base loss: 14427.08
[INFO 2017-06-28 13:16:07,609 main.py:51] epoch 2586, training loss: 10061.43, average training loss: 10059.16, base loss: 14427.08
[INFO 2017-06-28 13:16:08,256 main.py:51] epoch 2587, training loss: 11655.15, average training loss: 10061.81, base loss: 14431.26
[INFO 2017-06-28 13:16:08,919 main.py:51] epoch 2588, training loss: 10661.60, average training loss: 10063.88, base loss: 14434.20
[INFO 2017-06-28 13:16:09,586 main.py:51] epoch 2589, training loss: 10958.39, average training loss: 10065.47, base loss: 14436.25
[INFO 2017-06-28 13:16:10,247 main.py:51] epoch 2590, training loss: 9565.57, average training loss: 10066.19, base loss: 14437.14
[INFO 2017-06-28 13:16:10,934 main.py:51] epoch 2591, training loss: 9547.88, average training loss: 10065.45, base loss: 14434.66
[INFO 2017-06-28 13:16:11,604 main.py:51] epoch 2592, training loss: 10127.75, average training loss: 10064.59, base loss: 14433.33
[INFO 2017-06-28 13:16:12,256 main.py:51] epoch 2593, training loss: 9349.26, average training loss: 10063.82, base loss: 14433.17
[INFO 2017-06-28 13:16:12,918 main.py:51] epoch 2594, training loss: 9394.88, average training loss: 10063.05, base loss: 14431.75
[INFO 2017-06-28 13:16:13,569 main.py:51] epoch 2595, training loss: 10848.60, average training loss: 10064.11, base loss: 14434.60
[INFO 2017-06-28 13:16:14,231 main.py:51] epoch 2596, training loss: 10879.34, average training loss: 10066.69, base loss: 14438.19
[INFO 2017-06-28 13:16:14,906 main.py:51] epoch 2597, training loss: 9299.33, average training loss: 10066.27, base loss: 14438.29
[INFO 2017-06-28 13:16:15,565 main.py:51] epoch 2598, training loss: 12282.85, average training loss: 10067.72, base loss: 14440.79
[INFO 2017-06-28 13:16:16,231 main.py:51] epoch 2599, training loss: 8779.00, average training loss: 10066.97, base loss: 14440.36
[INFO 2017-06-28 13:16:16,232 main.py:53] epoch 2599, testing
[INFO 2017-06-28 13:16:18,751 main.py:105] average testing loss: 11930.74, base loss: 16308.78
[INFO 2017-06-28 13:16:18,751 main.py:106] improve_loss: 4378.03, improve_percent: 0.27
[INFO 2017-06-28 13:16:18,751 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:16:19,414 main.py:51] epoch 2600, training loss: 10354.93, average training loss: 10067.16, base loss: 14443.03
[INFO 2017-06-28 13:16:20,067 main.py:51] epoch 2601, training loss: 10374.44, average training loss: 10066.91, base loss: 14443.68
[INFO 2017-06-28 13:16:20,726 main.py:51] epoch 2602, training loss: 10043.11, average training loss: 10065.70, base loss: 14442.33
[INFO 2017-06-28 13:16:21,378 main.py:51] epoch 2603, training loss: 9900.94, average training loss: 10065.76, base loss: 14443.20
[INFO 2017-06-28 13:16:22,058 main.py:51] epoch 2604, training loss: 10598.54, average training loss: 10066.35, base loss: 14445.11
[INFO 2017-06-28 13:16:22,714 main.py:51] epoch 2605, training loss: 9767.56, average training loss: 10066.13, base loss: 14442.78
[INFO 2017-06-28 13:16:23,370 main.py:51] epoch 2606, training loss: 8639.08, average training loss: 10064.99, base loss: 14441.10
[INFO 2017-06-28 13:16:24,022 main.py:51] epoch 2607, training loss: 8555.00, average training loss: 10064.05, base loss: 14439.54
[INFO 2017-06-28 13:16:24,697 main.py:51] epoch 2608, training loss: 9213.98, average training loss: 10063.00, base loss: 14438.47
[INFO 2017-06-28 13:16:25,354 main.py:51] epoch 2609, training loss: 11978.86, average training loss: 10064.88, base loss: 14440.72
[INFO 2017-06-28 13:16:26,012 main.py:51] epoch 2610, training loss: 8630.99, average training loss: 10061.35, base loss: 14438.00
[INFO 2017-06-28 13:16:26,678 main.py:51] epoch 2611, training loss: 10539.17, average training loss: 10061.11, base loss: 14436.68
[INFO 2017-06-28 13:16:27,305 main.py:51] epoch 2612, training loss: 9276.40, average training loss: 10060.88, base loss: 14436.51
[INFO 2017-06-28 13:16:27,974 main.py:51] epoch 2613, training loss: 10541.41, average training loss: 10061.06, base loss: 14437.04
[INFO 2017-06-28 13:16:28,591 main.py:51] epoch 2614, training loss: 9135.09, average training loss: 10060.72, base loss: 14437.72
[INFO 2017-06-28 13:16:29,251 main.py:51] epoch 2615, training loss: 10710.22, average training loss: 10060.33, base loss: 14437.99
[INFO 2017-06-28 13:16:29,902 main.py:51] epoch 2616, training loss: 9449.33, average training loss: 10059.23, base loss: 14437.53
[INFO 2017-06-28 13:16:30,567 main.py:51] epoch 2617, training loss: 8825.13, average training loss: 10059.00, base loss: 14439.23
[INFO 2017-06-28 13:16:31,239 main.py:51] epoch 2618, training loss: 11010.56, average training loss: 10060.03, base loss: 14439.07
[INFO 2017-06-28 13:16:31,876 main.py:51] epoch 2619, training loss: 9934.18, average training loss: 10059.23, base loss: 14438.83
[INFO 2017-06-28 13:16:32,518 main.py:51] epoch 2620, training loss: 10404.97, average training loss: 10059.53, base loss: 14439.43
[INFO 2017-06-28 13:16:33,169 main.py:51] epoch 2621, training loss: 8606.01, average training loss: 10054.20, base loss: 14431.99
[INFO 2017-06-28 13:16:33,808 main.py:51] epoch 2622, training loss: 10011.62, average training loss: 10053.29, base loss: 14433.15
[INFO 2017-06-28 13:16:34,459 main.py:51] epoch 2623, training loss: 11045.27, average training loss: 10054.30, base loss: 14434.97
[INFO 2017-06-28 13:16:35,129 main.py:51] epoch 2624, training loss: 9965.03, average training loss: 10054.63, base loss: 14436.55
[INFO 2017-06-28 13:16:35,796 main.py:51] epoch 2625, training loss: 9590.49, average training loss: 10054.32, base loss: 14435.72
[INFO 2017-06-28 13:16:36,446 main.py:51] epoch 2626, training loss: 9522.71, average training loss: 10053.18, base loss: 14434.48
[INFO 2017-06-28 13:16:37,081 main.py:51] epoch 2627, training loss: 10359.86, average training loss: 10054.04, base loss: 14433.55
[INFO 2017-06-28 13:16:37,742 main.py:51] epoch 2628, training loss: 9769.88, average training loss: 10053.31, base loss: 14434.16
[INFO 2017-06-28 13:16:38,414 main.py:51] epoch 2629, training loss: 9820.39, average training loss: 10054.00, base loss: 14434.56
[INFO 2017-06-28 13:16:39,069 main.py:51] epoch 2630, training loss: 10115.01, average training loss: 10054.51, base loss: 14436.57
[INFO 2017-06-28 13:16:39,750 main.py:51] epoch 2631, training loss: 8840.88, average training loss: 10052.40, base loss: 14433.47
[INFO 2017-06-28 13:16:40,400 main.py:51] epoch 2632, training loss: 11109.43, average training loss: 10052.96, base loss: 14433.93
[INFO 2017-06-28 13:16:41,058 main.py:51] epoch 2633, training loss: 10500.40, average training loss: 10052.21, base loss: 14433.37
[INFO 2017-06-28 13:16:41,737 main.py:51] epoch 2634, training loss: 10621.82, average training loss: 10050.15, base loss: 14429.34
[INFO 2017-06-28 13:16:42,404 main.py:51] epoch 2635, training loss: 10288.16, average training loss: 10051.19, base loss: 14431.75
[INFO 2017-06-28 13:16:43,059 main.py:51] epoch 2636, training loss: 9944.56, average training loss: 10051.06, base loss: 14431.25
[INFO 2017-06-28 13:16:43,730 main.py:51] epoch 2637, training loss: 10253.42, average training loss: 10051.72, base loss: 14431.23
[INFO 2017-06-28 13:16:44,387 main.py:51] epoch 2638, training loss: 9513.50, average training loss: 10050.43, base loss: 14429.07
[INFO 2017-06-28 13:16:45,033 main.py:51] epoch 2639, training loss: 10711.09, average training loss: 10051.54, base loss: 14431.68
[INFO 2017-06-28 13:16:45,677 main.py:51] epoch 2640, training loss: 10169.63, average training loss: 10051.53, base loss: 14430.71
[INFO 2017-06-28 13:16:46,347 main.py:51] epoch 2641, training loss: 9802.50, average training loss: 10051.16, base loss: 14429.65
[INFO 2017-06-28 13:16:46,987 main.py:51] epoch 2642, training loss: 10577.85, average training loss: 10051.16, base loss: 14430.64
[INFO 2017-06-28 13:16:47,676 main.py:51] epoch 2643, training loss: 8572.44, average training loss: 10051.19, base loss: 14429.94
[INFO 2017-06-28 13:16:48,348 main.py:51] epoch 2644, training loss: 9743.48, average training loss: 10050.66, base loss: 14430.19
[INFO 2017-06-28 13:16:49,008 main.py:51] epoch 2645, training loss: 9724.40, average training loss: 10050.09, base loss: 14428.84
[INFO 2017-06-28 13:16:49,653 main.py:51] epoch 2646, training loss: 9769.68, average training loss: 10050.58, base loss: 14428.95
[INFO 2017-06-28 13:16:50,329 main.py:51] epoch 2647, training loss: 9395.10, average training loss: 10048.91, base loss: 14427.80
[INFO 2017-06-28 13:16:50,980 main.py:51] epoch 2648, training loss: 9646.92, average training loss: 10048.31, base loss: 14428.62
[INFO 2017-06-28 13:16:51,650 main.py:51] epoch 2649, training loss: 8627.71, average training loss: 10047.40, base loss: 14427.08
[INFO 2017-06-28 13:16:52,303 main.py:51] epoch 2650, training loss: 11507.32, average training loss: 10050.77, base loss: 14431.26
[INFO 2017-06-28 13:16:52,947 main.py:51] epoch 2651, training loss: 11981.86, average training loss: 10052.71, base loss: 14433.87
[INFO 2017-06-28 13:16:53,602 main.py:51] epoch 2652, training loss: 8569.68, average training loss: 10051.34, base loss: 14431.53
[INFO 2017-06-28 13:16:54,263 main.py:51] epoch 2653, training loss: 10272.48, average training loss: 10053.51, base loss: 14436.00
[INFO 2017-06-28 13:16:54,945 main.py:51] epoch 2654, training loss: 9671.00, average training loss: 10054.17, base loss: 14437.03
[INFO 2017-06-28 13:16:55,621 main.py:51] epoch 2655, training loss: 9546.76, average training loss: 10052.84, base loss: 14434.51
[INFO 2017-06-28 13:16:56,268 main.py:51] epoch 2656, training loss: 9714.33, average training loss: 10053.12, base loss: 14433.63
[INFO 2017-06-28 13:16:56,919 main.py:51] epoch 2657, training loss: 8559.02, average training loss: 10051.53, base loss: 14430.79
[INFO 2017-06-28 13:16:57,568 main.py:51] epoch 2658, training loss: 10755.24, average training loss: 10052.90, base loss: 14432.31
[INFO 2017-06-28 13:16:58,203 main.py:51] epoch 2659, training loss: 8525.27, average training loss: 10051.74, base loss: 14431.99
[INFO 2017-06-28 13:16:58,885 main.py:51] epoch 2660, training loss: 10330.85, average training loss: 10053.04, base loss: 14433.47
[INFO 2017-06-28 13:16:59,560 main.py:51] epoch 2661, training loss: 9955.88, average training loss: 10052.33, base loss: 14431.68
[INFO 2017-06-28 13:17:00,217 main.py:51] epoch 2662, training loss: 10755.65, average training loss: 10052.87, base loss: 14433.38
[INFO 2017-06-28 13:17:00,864 main.py:51] epoch 2663, training loss: 9966.33, average training loss: 10053.28, base loss: 14432.28
[INFO 2017-06-28 13:17:01,518 main.py:51] epoch 2664, training loss: 9707.30, average training loss: 10052.60, base loss: 14431.45
[INFO 2017-06-28 13:17:02,188 main.py:51] epoch 2665, training loss: 10782.52, average training loss: 10053.54, base loss: 14432.29
[INFO 2017-06-28 13:17:02,851 main.py:51] epoch 2666, training loss: 9343.49, average training loss: 10053.13, base loss: 14432.54
[INFO 2017-06-28 13:17:03,512 main.py:51] epoch 2667, training loss: 9287.82, average training loss: 10053.12, base loss: 14432.79
[INFO 2017-06-28 13:17:04,168 main.py:51] epoch 2668, training loss: 8994.27, average training loss: 10052.99, base loss: 14431.98
[INFO 2017-06-28 13:17:04,821 main.py:51] epoch 2669, training loss: 8706.52, average training loss: 10051.93, base loss: 14430.63
[INFO 2017-06-28 13:17:05,499 main.py:51] epoch 2670, training loss: 11492.04, average training loss: 10052.83, base loss: 14431.51
[INFO 2017-06-28 13:17:06,175 main.py:51] epoch 2671, training loss: 10071.68, average training loss: 10052.98, base loss: 14431.92
[INFO 2017-06-28 13:17:06,839 main.py:51] epoch 2672, training loss: 9620.98, average training loss: 10052.51, base loss: 14432.37
[INFO 2017-06-28 13:17:07,495 main.py:51] epoch 2673, training loss: 11216.42, average training loss: 10052.82, base loss: 14431.26
[INFO 2017-06-28 13:17:08,162 main.py:51] epoch 2674, training loss: 9217.88, average training loss: 10054.08, base loss: 14434.38
[INFO 2017-06-28 13:17:08,823 main.py:51] epoch 2675, training loss: 9332.72, average training loss: 10053.94, base loss: 14434.15
[INFO 2017-06-28 13:17:09,469 main.py:51] epoch 2676, training loss: 10467.01, average training loss: 10054.27, base loss: 14433.55
[INFO 2017-06-28 13:17:10,149 main.py:51] epoch 2677, training loss: 10076.07, average training loss: 10055.36, base loss: 14436.91
[INFO 2017-06-28 13:17:10,811 main.py:51] epoch 2678, training loss: 9650.90, average training loss: 10055.42, base loss: 14437.80
[INFO 2017-06-28 13:17:11,484 main.py:51] epoch 2679, training loss: 9855.86, average training loss: 10055.94, base loss: 14437.85
[INFO 2017-06-28 13:17:12,136 main.py:51] epoch 2680, training loss: 9365.29, average training loss: 10056.85, base loss: 14439.34
[INFO 2017-06-28 13:17:12,819 main.py:51] epoch 2681, training loss: 10083.19, average training loss: 10056.17, base loss: 14439.11
[INFO 2017-06-28 13:17:13,478 main.py:51] epoch 2682, training loss: 9384.18, average training loss: 10055.97, base loss: 14439.21
[INFO 2017-06-28 13:17:14,144 main.py:51] epoch 2683, training loss: 10447.07, average training loss: 10055.89, base loss: 14439.76
[INFO 2017-06-28 13:17:14,832 main.py:51] epoch 2684, training loss: 8804.71, average training loss: 10054.31, base loss: 14438.34
[INFO 2017-06-28 13:17:15,479 main.py:51] epoch 2685, training loss: 8102.55, average training loss: 10053.11, base loss: 14437.74
[INFO 2017-06-28 13:17:16,132 main.py:51] epoch 2686, training loss: 9337.75, average training loss: 10052.35, base loss: 14438.27
[INFO 2017-06-28 13:17:16,769 main.py:51] epoch 2687, training loss: 10103.56, average training loss: 10051.73, base loss: 14436.53
[INFO 2017-06-28 13:17:17,431 main.py:51] epoch 2688, training loss: 8825.92, average training loss: 10051.06, base loss: 14434.89
[INFO 2017-06-28 13:17:18,073 main.py:51] epoch 2689, training loss: 9920.07, average training loss: 10052.01, base loss: 14436.20
[INFO 2017-06-28 13:17:18,741 main.py:51] epoch 2690, training loss: 8883.51, average training loss: 10048.48, base loss: 14431.52
[INFO 2017-06-28 13:17:19,379 main.py:51] epoch 2691, training loss: 11359.93, average training loss: 10047.94, base loss: 14431.02
[INFO 2017-06-28 13:17:20,024 main.py:51] epoch 2692, training loss: 9490.29, average training loss: 10046.91, base loss: 14430.14
[INFO 2017-06-28 13:17:20,688 main.py:51] epoch 2693, training loss: 10547.46, average training loss: 10044.75, base loss: 14428.82
[INFO 2017-06-28 13:17:21,356 main.py:51] epoch 2694, training loss: 9600.97, average training loss: 10045.23, base loss: 14430.26
[INFO 2017-06-28 13:17:22,014 main.py:51] epoch 2695, training loss: 8639.95, average training loss: 10043.51, base loss: 14428.03
[INFO 2017-06-28 13:17:22,684 main.py:51] epoch 2696, training loss: 9856.67, average training loss: 10043.90, base loss: 14430.58
[INFO 2017-06-28 13:17:23,344 main.py:51] epoch 2697, training loss: 8825.74, average training loss: 10042.38, base loss: 14429.82
[INFO 2017-06-28 13:17:24,006 main.py:51] epoch 2698, training loss: 11421.91, average training loss: 10044.37, base loss: 14433.11
[INFO 2017-06-28 13:17:24,677 main.py:51] epoch 2699, training loss: 9299.58, average training loss: 10043.88, base loss: 14432.86
[INFO 2017-06-28 13:17:24,677 main.py:53] epoch 2699, testing
[INFO 2017-06-28 13:17:27,265 main.py:105] average testing loss: 10537.62, base loss: 14298.82
[INFO 2017-06-28 13:17:27,265 main.py:106] improve_loss: 3761.20, improve_percent: 0.26
[INFO 2017-06-28 13:17:27,265 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:17:27,949 main.py:51] epoch 2700, training loss: 10695.87, average training loss: 10045.18, base loss: 14434.33
[INFO 2017-06-28 13:17:28,598 main.py:51] epoch 2701, training loss: 8819.48, average training loss: 10043.85, base loss: 14431.79
[INFO 2017-06-28 13:17:29,263 main.py:51] epoch 2702, training loss: 10830.41, average training loss: 10044.31, base loss: 14435.30
[INFO 2017-06-28 13:17:29,916 main.py:51] epoch 2703, training loss: 10453.08, average training loss: 10043.31, base loss: 14433.75
[INFO 2017-06-28 13:17:30,577 main.py:51] epoch 2704, training loss: 10853.85, average training loss: 10044.61, base loss: 14437.28
[INFO 2017-06-28 13:17:31,256 main.py:51] epoch 2705, training loss: 11530.85, average training loss: 10046.27, base loss: 14438.99
[INFO 2017-06-28 13:17:31,922 main.py:51] epoch 2706, training loss: 10897.31, average training loss: 10048.49, base loss: 14443.83
[INFO 2017-06-28 13:17:32,586 main.py:51] epoch 2707, training loss: 9604.76, average training loss: 10048.25, base loss: 14445.60
[INFO 2017-06-28 13:17:33,234 main.py:51] epoch 2708, training loss: 9449.09, average training loss: 10048.53, base loss: 14446.97
[INFO 2017-06-28 13:17:33,866 main.py:51] epoch 2709, training loss: 9604.19, average training loss: 10047.42, base loss: 14446.68
[INFO 2017-06-28 13:17:34,514 main.py:51] epoch 2710, training loss: 9548.67, average training loss: 10047.15, base loss: 14447.03
[INFO 2017-06-28 13:17:35,163 main.py:51] epoch 2711, training loss: 9762.41, average training loss: 10047.94, base loss: 14448.25
[INFO 2017-06-28 13:17:35,815 main.py:51] epoch 2712, training loss: 9903.12, average training loss: 10046.00, base loss: 14445.57
[INFO 2017-06-28 13:17:36,472 main.py:51] epoch 2713, training loss: 9811.51, average training loss: 10046.28, base loss: 14446.76
[INFO 2017-06-28 13:17:37,120 main.py:51] epoch 2714, training loss: 9896.13, average training loss: 10045.42, base loss: 14444.52
[INFO 2017-06-28 13:17:37,772 main.py:51] epoch 2715, training loss: 9566.01, average training loss: 10043.45, base loss: 14442.45
[INFO 2017-06-28 13:17:38,412 main.py:51] epoch 2716, training loss: 10482.50, average training loss: 10042.30, base loss: 14442.04
[INFO 2017-06-28 13:17:39,073 main.py:51] epoch 2717, training loss: 10608.14, average training loss: 10042.88, base loss: 14443.04
[INFO 2017-06-28 13:17:39,750 main.py:51] epoch 2718, training loss: 9200.46, average training loss: 10042.88, base loss: 14443.28
[INFO 2017-06-28 13:17:40,407 main.py:51] epoch 2719, training loss: 9835.17, average training loss: 10041.91, base loss: 14439.57
[INFO 2017-06-28 13:17:41,067 main.py:51] epoch 2720, training loss: 10421.86, average training loss: 10043.22, base loss: 14442.22
[INFO 2017-06-28 13:17:41,718 main.py:51] epoch 2721, training loss: 11600.50, average training loss: 10046.52, base loss: 14446.92
[INFO 2017-06-28 13:17:42,358 main.py:51] epoch 2722, training loss: 9466.51, average training loss: 10046.34, base loss: 14446.38
[INFO 2017-06-28 13:17:43,020 main.py:51] epoch 2723, training loss: 8585.31, average training loss: 10044.29, base loss: 14444.09
[INFO 2017-06-28 13:17:43,669 main.py:51] epoch 2724, training loss: 10834.45, average training loss: 10045.90, base loss: 14447.51
[INFO 2017-06-28 13:17:44,322 main.py:51] epoch 2725, training loss: 10487.70, average training loss: 10046.41, base loss: 14449.92
[INFO 2017-06-28 13:17:44,982 main.py:51] epoch 2726, training loss: 9633.28, average training loss: 10046.12, base loss: 14449.30
[INFO 2017-06-28 13:17:45,644 main.py:51] epoch 2727, training loss: 10576.08, average training loss: 10047.21, base loss: 14451.65
[INFO 2017-06-28 13:17:46,303 main.py:51] epoch 2728, training loss: 8540.82, average training loss: 10044.63, base loss: 14447.63
[INFO 2017-06-28 13:17:46,942 main.py:51] epoch 2729, training loss: 9608.18, average training loss: 10044.62, base loss: 14448.14
[INFO 2017-06-28 13:17:47,602 main.py:51] epoch 2730, training loss: 10695.73, average training loss: 10043.99, base loss: 14448.16
[INFO 2017-06-28 13:17:48,283 main.py:51] epoch 2731, training loss: 10317.64, average training loss: 10042.52, base loss: 14446.47
[INFO 2017-06-28 13:17:48,930 main.py:51] epoch 2732, training loss: 9603.92, average training loss: 10041.84, base loss: 14445.00
[INFO 2017-06-28 13:17:49,587 main.py:51] epoch 2733, training loss: 8852.22, average training loss: 10040.28, base loss: 14443.17
[INFO 2017-06-28 13:17:50,246 main.py:51] epoch 2734, training loss: 8126.31, average training loss: 10039.07, base loss: 14442.72
[INFO 2017-06-28 13:17:50,908 main.py:51] epoch 2735, training loss: 10392.75, average training loss: 10041.05, base loss: 14443.56
[INFO 2017-06-28 13:17:51,566 main.py:51] epoch 2736, training loss: 10437.03, average training loss: 10042.48, base loss: 14445.06
[INFO 2017-06-28 13:17:52,236 main.py:51] epoch 2737, training loss: 8714.55, average training loss: 10041.21, base loss: 14443.93
[INFO 2017-06-28 13:17:52,873 main.py:51] epoch 2738, training loss: 10376.89, average training loss: 10041.69, base loss: 14444.22
[INFO 2017-06-28 13:17:53,533 main.py:51] epoch 2739, training loss: 11032.17, average training loss: 10042.00, base loss: 14446.44
[INFO 2017-06-28 13:17:54,187 main.py:51] epoch 2740, training loss: 9969.24, average training loss: 10042.65, base loss: 14446.67
[INFO 2017-06-28 13:17:54,821 main.py:51] epoch 2741, training loss: 8964.13, average training loss: 10041.38, base loss: 14446.60
[INFO 2017-06-28 13:17:55,485 main.py:51] epoch 2742, training loss: 11105.05, average training loss: 10041.31, base loss: 14446.55
[INFO 2017-06-28 13:17:56,140 main.py:51] epoch 2743, training loss: 10388.12, average training loss: 10042.35, base loss: 14446.76
[INFO 2017-06-28 13:17:56,797 main.py:51] epoch 2744, training loss: 10021.32, average training loss: 10041.65, base loss: 14445.28
[INFO 2017-06-28 13:17:57,443 main.py:51] epoch 2745, training loss: 10240.90, average training loss: 10042.58, base loss: 14447.14
[INFO 2017-06-28 13:17:58,115 main.py:51] epoch 2746, training loss: 9193.98, average training loss: 10042.43, base loss: 14446.53
[INFO 2017-06-28 13:17:58,775 main.py:51] epoch 2747, training loss: 10077.26, average training loss: 10042.62, base loss: 14447.02
[INFO 2017-06-28 13:17:59,428 main.py:51] epoch 2748, training loss: 10053.51, average training loss: 10043.18, base loss: 14448.95
[INFO 2017-06-28 13:18:00,079 main.py:51] epoch 2749, training loss: 9258.84, average training loss: 10042.95, base loss: 14448.55
[INFO 2017-06-28 13:18:00,757 main.py:51] epoch 2750, training loss: 10723.12, average training loss: 10043.61, base loss: 14450.08
[INFO 2017-06-28 13:18:01,408 main.py:51] epoch 2751, training loss: 9280.48, average training loss: 10042.27, base loss: 14449.10
[INFO 2017-06-28 13:18:02,055 main.py:51] epoch 2752, training loss: 8528.34, average training loss: 10040.87, base loss: 14447.17
[INFO 2017-06-28 13:18:02,719 main.py:51] epoch 2753, training loss: 10708.74, average training loss: 10042.03, base loss: 14448.71
[INFO 2017-06-28 13:18:03,365 main.py:51] epoch 2754, training loss: 9053.21, average training loss: 10041.70, base loss: 14449.74
[INFO 2017-06-28 13:18:04,017 main.py:51] epoch 2755, training loss: 9763.15, average training loss: 10041.52, base loss: 14448.80
[INFO 2017-06-28 13:18:04,665 main.py:51] epoch 2756, training loss: 10455.39, average training loss: 10041.96, base loss: 14450.22
[INFO 2017-06-28 13:18:05,338 main.py:51] epoch 2757, training loss: 10037.29, average training loss: 10042.33, base loss: 14452.68
[INFO 2017-06-28 13:18:06,021 main.py:51] epoch 2758, training loss: 9813.18, average training loss: 10042.73, base loss: 14452.68
[INFO 2017-06-28 13:18:06,682 main.py:51] epoch 2759, training loss: 10006.56, average training loss: 10043.80, base loss: 14453.38
[INFO 2017-06-28 13:18:07,339 main.py:51] epoch 2760, training loss: 7991.96, average training loss: 10040.13, base loss: 14448.80
[INFO 2017-06-28 13:18:07,982 main.py:51] epoch 2761, training loss: 11473.73, average training loss: 10041.47, base loss: 14451.91
[INFO 2017-06-28 13:18:08,672 main.py:51] epoch 2762, training loss: 10084.69, average training loss: 10042.26, base loss: 14451.92
[INFO 2017-06-28 13:18:09,329 main.py:51] epoch 2763, training loss: 9730.17, average training loss: 10042.09, base loss: 14453.42
[INFO 2017-06-28 13:18:09,990 main.py:51] epoch 2764, training loss: 9275.62, average training loss: 10041.37, base loss: 14451.35
[INFO 2017-06-28 13:18:10,673 main.py:51] epoch 2765, training loss: 9345.13, average training loss: 10040.45, base loss: 14449.31
[INFO 2017-06-28 13:18:11,352 main.py:51] epoch 2766, training loss: 9112.28, average training loss: 10039.18, base loss: 14447.75
[INFO 2017-06-28 13:18:12,010 main.py:51] epoch 2767, training loss: 10439.89, average training loss: 10040.05, base loss: 14448.14
[INFO 2017-06-28 13:18:12,662 main.py:51] epoch 2768, training loss: 9309.51, average training loss: 10039.21, base loss: 14446.35
[INFO 2017-06-28 13:18:13,327 main.py:51] epoch 2769, training loss: 10586.81, average training loss: 10039.25, base loss: 14445.73
[INFO 2017-06-28 13:18:14,004 main.py:51] epoch 2770, training loss: 9693.05, average training loss: 10039.76, base loss: 14446.96
[INFO 2017-06-28 13:18:14,682 main.py:51] epoch 2771, training loss: 9953.53, average training loss: 10040.28, base loss: 14449.45
[INFO 2017-06-28 13:18:15,332 main.py:51] epoch 2772, training loss: 10347.71, average training loss: 10040.06, base loss: 14449.22
[INFO 2017-06-28 13:18:15,991 main.py:51] epoch 2773, training loss: 8313.91, average training loss: 10039.30, base loss: 14447.50
[INFO 2017-06-28 13:18:16,641 main.py:51] epoch 2774, training loss: 10975.46, average training loss: 10038.90, base loss: 14448.27
[INFO 2017-06-28 13:18:17,322 main.py:51] epoch 2775, training loss: 9805.19, average training loss: 10038.43, base loss: 14448.11
[INFO 2017-06-28 13:18:17,978 main.py:51] epoch 2776, training loss: 10627.30, average training loss: 10039.86, base loss: 14450.03
[INFO 2017-06-28 13:18:18,637 main.py:51] epoch 2777, training loss: 8924.46, average training loss: 10038.27, base loss: 14447.73
[INFO 2017-06-28 13:18:19,297 main.py:51] epoch 2778, training loss: 9973.24, average training loss: 10035.73, base loss: 14444.87
[INFO 2017-06-28 13:18:19,975 main.py:51] epoch 2779, training loss: 10229.95, average training loss: 10036.85, base loss: 14446.08
[INFO 2017-06-28 13:18:20,644 main.py:51] epoch 2780, training loss: 11088.83, average training loss: 10036.34, base loss: 14446.10
[INFO 2017-06-28 13:18:21,319 main.py:51] epoch 2781, training loss: 10029.06, average training loss: 10036.48, base loss: 14445.81
[INFO 2017-06-28 13:18:21,990 main.py:51] epoch 2782, training loss: 9631.53, average training loss: 10036.66, base loss: 14444.76
[INFO 2017-06-28 13:18:22,639 main.py:51] epoch 2783, training loss: 10725.93, average training loss: 10036.84, base loss: 14445.19
[INFO 2017-06-28 13:18:23,292 main.py:51] epoch 2784, training loss: 11241.79, average training loss: 10038.49, base loss: 14447.60
[INFO 2017-06-28 13:18:23,960 main.py:51] epoch 2785, training loss: 11236.78, average training loss: 10039.57, base loss: 14448.45
[INFO 2017-06-28 13:18:24,633 main.py:51] epoch 2786, training loss: 9588.64, average training loss: 10039.85, base loss: 14450.37
[INFO 2017-06-28 13:18:25,311 main.py:51] epoch 2787, training loss: 9425.18, average training loss: 10037.93, base loss: 14446.14
[INFO 2017-06-28 13:18:25,964 main.py:51] epoch 2788, training loss: 9257.93, average training loss: 10037.51, base loss: 14445.45
[INFO 2017-06-28 13:18:26,632 main.py:51] epoch 2789, training loss: 10863.18, average training loss: 10037.29, base loss: 14445.23
[INFO 2017-06-28 13:18:27,284 main.py:51] epoch 2790, training loss: 9096.20, average training loss: 10037.55, base loss: 14446.61
[INFO 2017-06-28 13:18:27,936 main.py:51] epoch 2791, training loss: 8925.90, average training loss: 10036.60, base loss: 14443.44
[INFO 2017-06-28 13:18:28,594 main.py:51] epoch 2792, training loss: 8249.93, average training loss: 10034.77, base loss: 14439.11
[INFO 2017-06-28 13:18:29,257 main.py:51] epoch 2793, training loss: 10274.98, average training loss: 10035.72, base loss: 14440.01
[INFO 2017-06-28 13:18:29,940 main.py:51] epoch 2794, training loss: 12349.37, average training loss: 10037.16, base loss: 14440.79
[INFO 2017-06-28 13:18:30,579 main.py:51] epoch 2795, training loss: 11608.62, average training loss: 10040.20, base loss: 14445.64
[INFO 2017-06-28 13:18:31,227 main.py:51] epoch 2796, training loss: 8528.50, average training loss: 10038.71, base loss: 14443.45
[INFO 2017-06-28 13:18:31,874 main.py:51] epoch 2797, training loss: 8454.44, average training loss: 10036.62, base loss: 14441.95
[INFO 2017-06-28 13:18:32,576 main.py:51] epoch 2798, training loss: 9674.39, average training loss: 10034.85, base loss: 14439.06
[INFO 2017-06-28 13:18:33,224 main.py:51] epoch 2799, training loss: 9030.52, average training loss: 10033.78, base loss: 14437.79
[INFO 2017-06-28 13:18:33,225 main.py:53] epoch 2799, testing
[INFO 2017-06-28 13:18:35,825 main.py:105] average testing loss: 10771.06, base loss: 14907.01
[INFO 2017-06-28 13:18:35,826 main.py:106] improve_loss: 4135.95, improve_percent: 0.28
[INFO 2017-06-28 13:18:35,826 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:18:36,483 main.py:51] epoch 2800, training loss: 10266.58, average training loss: 10033.47, base loss: 14436.23
[INFO 2017-06-28 13:18:37,136 main.py:51] epoch 2801, training loss: 9897.67, average training loss: 10034.09, base loss: 14436.70
[INFO 2017-06-28 13:18:37,794 main.py:51] epoch 2802, training loss: 9952.09, average training loss: 10033.55, base loss: 14436.68
[INFO 2017-06-28 13:18:38,472 main.py:51] epoch 2803, training loss: 8251.85, average training loss: 10030.01, base loss: 14431.76
[INFO 2017-06-28 13:18:39,167 main.py:51] epoch 2804, training loss: 9756.58, average training loss: 10028.80, base loss: 14432.43
[INFO 2017-06-28 13:18:39,832 main.py:51] epoch 2805, training loss: 10114.91, average training loss: 10030.00, base loss: 14434.59
[INFO 2017-06-28 13:18:40,482 main.py:51] epoch 2806, training loss: 9979.60, average training loss: 10028.02, base loss: 14431.17
[INFO 2017-06-28 13:18:41,139 main.py:51] epoch 2807, training loss: 9574.86, average training loss: 10027.76, base loss: 14431.48
[INFO 2017-06-28 13:18:41,816 main.py:51] epoch 2808, training loss: 10696.94, average training loss: 10026.61, base loss: 14428.56
[INFO 2017-06-28 13:18:42,481 main.py:51] epoch 2809, training loss: 11171.56, average training loss: 10029.25, base loss: 14432.07
[INFO 2017-06-28 13:18:43,140 main.py:51] epoch 2810, training loss: 10579.17, average training loss: 10028.80, base loss: 14432.42
[INFO 2017-06-28 13:18:43,793 main.py:51] epoch 2811, training loss: 10424.98, average training loss: 10029.57, base loss: 14433.24
[INFO 2017-06-28 13:18:44,477 main.py:51] epoch 2812, training loss: 8918.77, average training loss: 10028.37, base loss: 14430.35
[INFO 2017-06-28 13:18:45,141 main.py:51] epoch 2813, training loss: 9802.12, average training loss: 10028.05, base loss: 14428.24
[INFO 2017-06-28 13:18:45,801 main.py:51] epoch 2814, training loss: 8504.92, average training loss: 10026.89, base loss: 14425.53
[INFO 2017-06-28 13:18:46,465 main.py:51] epoch 2815, training loss: 9348.32, average training loss: 10026.64, base loss: 14426.32
[INFO 2017-06-28 13:18:47,130 main.py:51] epoch 2816, training loss: 9045.61, average training loss: 10025.95, base loss: 14425.36
[INFO 2017-06-28 13:18:47,793 main.py:51] epoch 2817, training loss: 9949.58, average training loss: 10025.83, base loss: 14424.38
[INFO 2017-06-28 13:18:48,456 main.py:51] epoch 2818, training loss: 9204.37, average training loss: 10025.85, base loss: 14425.06
[INFO 2017-06-28 13:18:49,119 main.py:51] epoch 2819, training loss: 8979.19, average training loss: 10023.75, base loss: 14421.73
[INFO 2017-06-28 13:18:49,765 main.py:51] epoch 2820, training loss: 9057.01, average training loss: 10023.61, base loss: 14421.68
[INFO 2017-06-28 13:18:50,434 main.py:51] epoch 2821, training loss: 10788.47, average training loss: 10025.00, base loss: 14422.78
[INFO 2017-06-28 13:18:51,089 main.py:51] epoch 2822, training loss: 10094.27, average training loss: 10024.47, base loss: 14422.84
[INFO 2017-06-28 13:18:51,768 main.py:51] epoch 2823, training loss: 9318.39, average training loss: 10023.74, base loss: 14422.18
[INFO 2017-06-28 13:18:52,407 main.py:51] epoch 2824, training loss: 11532.11, average training loss: 10025.32, base loss: 14425.61
[INFO 2017-06-28 13:18:53,089 main.py:51] epoch 2825, training loss: 8707.99, average training loss: 10024.24, base loss: 14424.25
[INFO 2017-06-28 13:18:53,752 main.py:51] epoch 2826, training loss: 9557.47, average training loss: 10024.80, base loss: 14425.48
[INFO 2017-06-28 13:18:54,409 main.py:51] epoch 2827, training loss: 10504.19, average training loss: 10025.74, base loss: 14425.90
[INFO 2017-06-28 13:18:55,072 main.py:51] epoch 2828, training loss: 9846.69, average training loss: 10025.20, base loss: 14428.12
[INFO 2017-06-28 13:18:55,731 main.py:51] epoch 2829, training loss: 10242.68, average training loss: 10025.12, base loss: 14428.88
[INFO 2017-06-28 13:18:56,428 main.py:51] epoch 2830, training loss: 9981.46, average training loss: 10023.59, base loss: 14427.41
[INFO 2017-06-28 13:18:57,076 main.py:51] epoch 2831, training loss: 9404.29, average training loss: 10023.76, base loss: 14429.80
[INFO 2017-06-28 13:18:57,749 main.py:51] epoch 2832, training loss: 9950.84, average training loss: 10024.48, base loss: 14431.23
[INFO 2017-06-28 13:18:58,409 main.py:51] epoch 2833, training loss: 9260.74, average training loss: 10025.26, base loss: 14433.38
[INFO 2017-06-28 13:18:59,073 main.py:51] epoch 2834, training loss: 10956.58, average training loss: 10027.24, base loss: 14436.36
[INFO 2017-06-28 13:18:59,750 main.py:51] epoch 2835, training loss: 9856.64, average training loss: 10026.72, base loss: 14436.05
[INFO 2017-06-28 13:19:00,398 main.py:51] epoch 2836, training loss: 10967.62, average training loss: 10027.11, base loss: 14436.82
[INFO 2017-06-28 13:19:01,049 main.py:51] epoch 2837, training loss: 10229.47, average training loss: 10027.02, base loss: 14436.81
[INFO 2017-06-28 13:19:01,705 main.py:51] epoch 2838, training loss: 11709.59, average training loss: 10028.45, base loss: 14439.96
[INFO 2017-06-28 13:19:02,354 main.py:51] epoch 2839, training loss: 10110.09, average training loss: 10027.63, base loss: 14439.43
[INFO 2017-06-28 13:19:03,037 main.py:51] epoch 2840, training loss: 8753.98, average training loss: 10025.93, base loss: 14438.85
[INFO 2017-06-28 13:19:03,698 main.py:51] epoch 2841, training loss: 9476.97, average training loss: 10024.63, base loss: 14437.57
[INFO 2017-06-28 13:19:04,379 main.py:51] epoch 2842, training loss: 9960.32, average training loss: 10026.24, base loss: 14438.96
[INFO 2017-06-28 13:19:05,040 main.py:51] epoch 2843, training loss: 8990.40, average training loss: 10026.97, base loss: 14439.12
[INFO 2017-06-28 13:19:05,714 main.py:51] epoch 2844, training loss: 9915.43, average training loss: 10027.09, base loss: 14439.91
[INFO 2017-06-28 13:19:06,371 main.py:51] epoch 2845, training loss: 9327.29, average training loss: 10025.80, base loss: 14436.78
[INFO 2017-06-28 13:19:07,025 main.py:51] epoch 2846, training loss: 10278.71, average training loss: 10026.72, base loss: 14438.71
[INFO 2017-06-28 13:19:07,691 main.py:51] epoch 2847, training loss: 10471.61, average training loss: 10027.85, base loss: 14440.60
[INFO 2017-06-28 13:19:08,347 main.py:51] epoch 2848, training loss: 10421.57, average training loss: 10028.35, base loss: 14440.00
[INFO 2017-06-28 13:19:08,987 main.py:51] epoch 2849, training loss: 10177.60, average training loss: 10028.65, base loss: 14440.32
[INFO 2017-06-28 13:19:09,663 main.py:51] epoch 2850, training loss: 10816.05, average training loss: 10030.33, base loss: 14442.19
[INFO 2017-06-28 13:19:10,319 main.py:51] epoch 2851, training loss: 9928.65, average training loss: 10030.24, base loss: 14442.08
[INFO 2017-06-28 13:19:10,999 main.py:51] epoch 2852, training loss: 10025.19, average training loss: 10030.91, base loss: 14440.79
[INFO 2017-06-28 13:19:11,675 main.py:51] epoch 2853, training loss: 9720.87, average training loss: 10030.51, base loss: 14440.90
[INFO 2017-06-28 13:19:12,328 main.py:51] epoch 2854, training loss: 10730.45, average training loss: 10032.55, base loss: 14443.33
[INFO 2017-06-28 13:19:12,974 main.py:51] epoch 2855, training loss: 10031.16, average training loss: 10033.31, base loss: 14444.06
[INFO 2017-06-28 13:19:13,627 main.py:51] epoch 2856, training loss: 11777.65, average training loss: 10035.70, base loss: 14448.26
[INFO 2017-06-28 13:19:14,290 main.py:51] epoch 2857, training loss: 8439.57, average training loss: 10033.88, base loss: 14447.20
[INFO 2017-06-28 13:19:14,950 main.py:51] epoch 2858, training loss: 9786.11, average training loss: 10033.21, base loss: 14447.70
[INFO 2017-06-28 13:19:15,610 main.py:51] epoch 2859, training loss: 9096.64, average training loss: 10033.35, base loss: 14447.30
[INFO 2017-06-28 13:19:16,267 main.py:51] epoch 2860, training loss: 9545.12, average training loss: 10032.71, base loss: 14448.33
[INFO 2017-06-28 13:19:16,922 main.py:51] epoch 2861, training loss: 9404.43, average training loss: 10030.72, base loss: 14445.15
[INFO 2017-06-28 13:19:17,570 main.py:51] epoch 2862, training loss: 8533.26, average training loss: 10027.83, base loss: 14441.23
[INFO 2017-06-28 13:19:18,249 main.py:51] epoch 2863, training loss: 9856.89, average training loss: 10028.50, base loss: 14442.05
[INFO 2017-06-28 13:19:18,928 main.py:51] epoch 2864, training loss: 9086.73, average training loss: 10028.51, base loss: 14442.74
[INFO 2017-06-28 13:19:19,612 main.py:51] epoch 2865, training loss: 10525.15, average training loss: 10028.46, base loss: 14442.52
[INFO 2017-06-28 13:19:20,266 main.py:51] epoch 2866, training loss: 8320.09, average training loss: 10025.36, base loss: 14439.48
[INFO 2017-06-28 13:19:20,933 main.py:51] epoch 2867, training loss: 9859.73, average training loss: 10024.85, base loss: 14437.48
[INFO 2017-06-28 13:19:21,598 main.py:51] epoch 2868, training loss: 8595.44, average training loss: 10023.91, base loss: 14437.50
[INFO 2017-06-28 13:19:22,260 main.py:51] epoch 2869, training loss: 9467.67, average training loss: 10022.39, base loss: 14436.32
[INFO 2017-06-28 13:19:22,932 main.py:51] epoch 2870, training loss: 9947.15, average training loss: 10023.62, base loss: 14438.27
[INFO 2017-06-28 13:19:23,627 main.py:51] epoch 2871, training loss: 10103.26, average training loss: 10023.11, base loss: 14437.62
[INFO 2017-06-28 13:19:24,294 main.py:51] epoch 2872, training loss: 10579.31, average training loss: 10022.99, base loss: 14437.25
[INFO 2017-06-28 13:19:24,971 main.py:51] epoch 2873, training loss: 9163.73, average training loss: 10023.35, base loss: 14437.98
[INFO 2017-06-28 13:19:25,626 main.py:51] epoch 2874, training loss: 9207.27, average training loss: 10022.77, base loss: 14436.42
[INFO 2017-06-28 13:19:26,269 main.py:51] epoch 2875, training loss: 8087.16, average training loss: 10020.58, base loss: 14434.06
[INFO 2017-06-28 13:19:26,901 main.py:51] epoch 2876, training loss: 9758.13, average training loss: 10020.67, base loss: 14434.29
[INFO 2017-06-28 13:19:27,583 main.py:51] epoch 2877, training loss: 9592.42, average training loss: 10020.62, base loss: 14434.98
[INFO 2017-06-28 13:19:28,253 main.py:51] epoch 2878, training loss: 9289.83, average training loss: 10020.60, base loss: 14434.72
[INFO 2017-06-28 13:19:28,955 main.py:51] epoch 2879, training loss: 9274.39, average training loss: 10019.27, base loss: 14432.25
[INFO 2017-06-28 13:19:29,624 main.py:51] epoch 2880, training loss: 11230.88, average training loss: 10019.58, base loss: 14431.70
[INFO 2017-06-28 13:19:30,286 main.py:51] epoch 2881, training loss: 8417.58, average training loss: 10017.40, base loss: 14428.83
[INFO 2017-06-28 13:19:30,965 main.py:51] epoch 2882, training loss: 9678.89, average training loss: 10016.57, base loss: 14428.86
[INFO 2017-06-28 13:19:31,628 main.py:51] epoch 2883, training loss: 8953.57, average training loss: 10015.68, base loss: 14428.35
[INFO 2017-06-28 13:19:32,278 main.py:51] epoch 2884, training loss: 8742.88, average training loss: 10014.30, base loss: 14427.35
[INFO 2017-06-28 13:19:32,919 main.py:51] epoch 2885, training loss: 9777.91, average training loss: 10014.69, base loss: 14428.95
[INFO 2017-06-28 13:19:33,607 main.py:51] epoch 2886, training loss: 10082.83, average training loss: 10014.60, base loss: 14430.93
[INFO 2017-06-28 13:19:34,267 main.py:51] epoch 2887, training loss: 9116.08, average training loss: 10015.02, base loss: 14432.04
[INFO 2017-06-28 13:19:34,944 main.py:51] epoch 2888, training loss: 9193.51, average training loss: 10014.03, base loss: 14430.55
[INFO 2017-06-28 13:19:35,619 main.py:51] epoch 2889, training loss: 7875.01, average training loss: 10011.33, base loss: 14426.77
[INFO 2017-06-28 13:19:36,286 main.py:51] epoch 2890, training loss: 9569.85, average training loss: 10008.27, base loss: 14424.20
[INFO 2017-06-28 13:19:36,945 main.py:51] epoch 2891, training loss: 9774.44, average training loss: 10008.14, base loss: 14424.25
[INFO 2017-06-28 13:19:37,595 main.py:51] epoch 2892, training loss: 9479.53, average training loss: 10008.40, base loss: 14425.18
[INFO 2017-06-28 13:19:38,260 main.py:51] epoch 2893, training loss: 9024.28, average training loss: 10006.44, base loss: 14422.56
[INFO 2017-06-28 13:19:38,911 main.py:51] epoch 2894, training loss: 10149.37, average training loss: 10006.71, base loss: 14423.22
[INFO 2017-06-28 13:19:39,578 main.py:51] epoch 2895, training loss: 10230.54, average training loss: 10006.48, base loss: 14422.16
[INFO 2017-06-28 13:19:40,250 main.py:51] epoch 2896, training loss: 10970.18, average training loss: 10008.19, base loss: 14423.85
[INFO 2017-06-28 13:19:40,912 main.py:51] epoch 2897, training loss: 10761.03, average training loss: 10009.18, base loss: 14425.10
[INFO 2017-06-28 13:19:41,575 main.py:51] epoch 2898, training loss: 10506.53, average training loss: 10010.75, base loss: 14427.30
[INFO 2017-06-28 13:19:42,235 main.py:51] epoch 2899, training loss: 9064.47, average training loss: 10010.67, base loss: 14427.45
[INFO 2017-06-28 13:19:42,235 main.py:53] epoch 2899, testing
[INFO 2017-06-28 13:19:44,841 main.py:105] average testing loss: 11489.36, base loss: 15861.79
[INFO 2017-06-28 13:19:44,841 main.py:106] improve_loss: 4372.43, improve_percent: 0.28
[INFO 2017-06-28 13:19:44,842 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:19:45,483 main.py:51] epoch 2900, training loss: 7927.36, average training loss: 10007.86, base loss: 14423.26
[INFO 2017-06-28 13:19:46,144 main.py:51] epoch 2901, training loss: 8373.25, average training loss: 10004.91, base loss: 14418.77
[INFO 2017-06-28 13:19:46,805 main.py:51] epoch 2902, training loss: 10610.96, average training loss: 10004.71, base loss: 14419.53
[INFO 2017-06-28 13:19:47,454 main.py:51] epoch 2903, training loss: 9152.56, average training loss: 10004.12, base loss: 14419.36
[INFO 2017-06-28 13:19:48,113 main.py:51] epoch 2904, training loss: 10680.67, average training loss: 10006.01, base loss: 14424.28
[INFO 2017-06-28 13:19:48,786 main.py:51] epoch 2905, training loss: 10174.35, average training loss: 10006.17, base loss: 14425.46
[INFO 2017-06-28 13:19:49,453 main.py:51] epoch 2906, training loss: 10043.29, average training loss: 10005.65, base loss: 14424.58
[INFO 2017-06-28 13:19:50,101 main.py:51] epoch 2907, training loss: 11022.90, average training loss: 10004.67, base loss: 14423.72
[INFO 2017-06-28 13:19:50,756 main.py:51] epoch 2908, training loss: 8494.22, average training loss: 10002.19, base loss: 14418.27
[INFO 2017-06-28 13:19:51,422 main.py:51] epoch 2909, training loss: 8890.41, average training loss: 10001.31, base loss: 14417.37
[INFO 2017-06-28 13:19:52,075 main.py:51] epoch 2910, training loss: 9342.98, average training loss: 9998.77, base loss: 14414.25
[INFO 2017-06-28 13:19:52,731 main.py:51] epoch 2911, training loss: 8754.21, average training loss: 9997.72, base loss: 14413.07
[INFO 2017-06-28 13:19:53,383 main.py:51] epoch 2912, training loss: 8648.03, average training loss: 9996.61, base loss: 14409.91
[INFO 2017-06-28 13:19:54,060 main.py:51] epoch 2913, training loss: 9759.97, average training loss: 9997.03, base loss: 14411.56
[INFO 2017-06-28 13:19:54,710 main.py:51] epoch 2914, training loss: 9479.17, average training loss: 9995.51, base loss: 14411.44
[INFO 2017-06-28 13:19:55,384 main.py:51] epoch 2915, training loss: 10948.13, average training loss: 9995.34, base loss: 14412.11
[INFO 2017-06-28 13:19:56,026 main.py:51] epoch 2916, training loss: 9566.61, average training loss: 9995.78, base loss: 14413.52
[INFO 2017-06-28 13:19:56,674 main.py:51] epoch 2917, training loss: 10552.44, average training loss: 9997.34, base loss: 14415.29
[INFO 2017-06-28 13:19:57,324 main.py:51] epoch 2918, training loss: 11769.39, average training loss: 9999.07, base loss: 14418.28
[INFO 2017-06-28 13:19:57,965 main.py:51] epoch 2919, training loss: 9860.19, average training loss: 9994.11, base loss: 14413.39
[INFO 2017-06-28 13:19:58,610 main.py:51] epoch 2920, training loss: 11342.37, average training loss: 9996.03, base loss: 14416.61
[INFO 2017-06-28 13:19:59,288 main.py:51] epoch 2921, training loss: 8859.03, average training loss: 9994.14, base loss: 14413.82
[INFO 2017-06-28 13:19:59,962 main.py:51] epoch 2922, training loss: 10981.68, average training loss: 9995.68, base loss: 14418.13
[INFO 2017-06-28 13:20:00,643 main.py:51] epoch 2923, training loss: 8999.94, average training loss: 9992.01, base loss: 14413.69
[INFO 2017-06-28 13:20:01,312 main.py:51] epoch 2924, training loss: 9985.26, average training loss: 9990.38, base loss: 14413.00
[INFO 2017-06-28 13:20:01,970 main.py:51] epoch 2925, training loss: 9742.34, average training loss: 9990.50, base loss: 14412.44
[INFO 2017-06-28 13:20:02,628 main.py:51] epoch 2926, training loss: 9445.41, average training loss: 9990.22, base loss: 14412.56
[INFO 2017-06-28 13:20:03,268 main.py:51] epoch 2927, training loss: 9809.00, average training loss: 9989.97, base loss: 14411.27
[INFO 2017-06-28 13:20:03,916 main.py:51] epoch 2928, training loss: 11410.33, average training loss: 9990.88, base loss: 14412.24
[INFO 2017-06-28 13:20:04,578 main.py:51] epoch 2929, training loss: 9663.72, average training loss: 9990.94, base loss: 14412.14
[INFO 2017-06-28 13:20:05,225 main.py:51] epoch 2930, training loss: 8970.28, average training loss: 9989.83, base loss: 14410.22
[INFO 2017-06-28 13:20:05,876 main.py:51] epoch 2931, training loss: 10054.03, average training loss: 9989.67, base loss: 14410.88
[INFO 2017-06-28 13:20:06,529 main.py:51] epoch 2932, training loss: 8769.67, average training loss: 9988.68, base loss: 14409.66
[INFO 2017-06-28 13:20:07,179 main.py:51] epoch 2933, training loss: 10925.67, average training loss: 9989.86, base loss: 14410.25
[INFO 2017-06-28 13:20:07,829 main.py:51] epoch 2934, training loss: 11275.59, average training loss: 9991.77, base loss: 14412.20
[INFO 2017-06-28 13:20:08,487 main.py:51] epoch 2935, training loss: 9149.74, average training loss: 9990.66, base loss: 14411.92
[INFO 2017-06-28 13:20:09,148 main.py:51] epoch 2936, training loss: 9948.60, average training loss: 9990.10, base loss: 14411.66
[INFO 2017-06-28 13:20:09,819 main.py:51] epoch 2937, training loss: 9765.27, average training loss: 9989.97, base loss: 14413.27
[INFO 2017-06-28 13:20:10,468 main.py:51] epoch 2938, training loss: 10001.64, average training loss: 9988.87, base loss: 14412.44
[INFO 2017-06-28 13:20:11,111 main.py:51] epoch 2939, training loss: 9495.45, average training loss: 9988.02, base loss: 14411.09
[INFO 2017-06-28 13:20:11,776 main.py:51] epoch 2940, training loss: 11748.49, average training loss: 9990.80, base loss: 14416.48
[INFO 2017-06-28 13:20:12,435 main.py:51] epoch 2941, training loss: 9275.27, average training loss: 9991.28, base loss: 14418.38
[INFO 2017-06-28 13:20:13,116 main.py:51] epoch 2942, training loss: 8807.77, average training loss: 9990.54, base loss: 14416.81
[INFO 2017-06-28 13:20:13,771 main.py:51] epoch 2943, training loss: 10512.62, average training loss: 9989.64, base loss: 14413.79
[INFO 2017-06-28 13:20:14,470 main.py:51] epoch 2944, training loss: 8760.15, average training loss: 9986.24, base loss: 14408.73
[INFO 2017-06-28 13:20:15,147 main.py:51] epoch 2945, training loss: 9852.80, average training loss: 9984.78, base loss: 14407.51
[INFO 2017-06-28 13:20:15,820 main.py:51] epoch 2946, training loss: 9644.59, average training loss: 9983.44, base loss: 14405.59
[INFO 2017-06-28 13:20:16,504 main.py:51] epoch 2947, training loss: 9396.30, average training loss: 9982.79, base loss: 14405.60
[INFO 2017-06-28 13:20:17,164 main.py:51] epoch 2948, training loss: 9249.61, average training loss: 9982.36, base loss: 14405.04
[INFO 2017-06-28 13:20:17,828 main.py:51] epoch 2949, training loss: 10280.54, average training loss: 9983.46, base loss: 14406.79
[INFO 2017-06-28 13:20:18,475 main.py:51] epoch 2950, training loss: 10960.39, average training loss: 9985.69, base loss: 14407.46
[INFO 2017-06-28 13:20:19,144 main.py:51] epoch 2951, training loss: 8905.84, average training loss: 9981.71, base loss: 14401.46
[INFO 2017-06-28 13:20:19,823 main.py:51] epoch 2952, training loss: 9215.69, average training loss: 9981.50, base loss: 14399.69
[INFO 2017-06-28 13:20:20,502 main.py:51] epoch 2953, training loss: 9957.38, average training loss: 9981.28, base loss: 14399.96
[INFO 2017-06-28 13:20:21,165 main.py:51] epoch 2954, training loss: 9245.73, average training loss: 9978.89, base loss: 14396.27
[INFO 2017-06-28 13:20:21,843 main.py:51] epoch 2955, training loss: 11372.05, average training loss: 9979.85, base loss: 14397.67
[INFO 2017-06-28 13:20:22,508 main.py:51] epoch 2956, training loss: 9839.90, average training loss: 9980.66, base loss: 14398.74
[INFO 2017-06-28 13:20:23,174 main.py:51] epoch 2957, training loss: 10222.88, average training loss: 9981.54, base loss: 14400.84
[INFO 2017-06-28 13:20:23,843 main.py:51] epoch 2958, training loss: 12131.62, average training loss: 9982.96, base loss: 14403.15
[INFO 2017-06-28 13:20:24,506 main.py:51] epoch 2959, training loss: 10032.92, average training loss: 9983.30, base loss: 14403.67
[INFO 2017-06-28 13:20:25,191 main.py:51] epoch 2960, training loss: 9885.78, average training loss: 9981.18, base loss: 14400.86
[INFO 2017-06-28 13:20:25,867 main.py:51] epoch 2961, training loss: 9763.68, average training loss: 9981.44, base loss: 14402.56
[INFO 2017-06-28 13:20:26,521 main.py:51] epoch 2962, training loss: 10434.62, average training loss: 9983.12, base loss: 14406.94
[INFO 2017-06-28 13:20:27,172 main.py:51] epoch 2963, training loss: 9776.65, average training loss: 9984.33, base loss: 14408.74
[INFO 2017-06-28 13:20:27,846 main.py:51] epoch 2964, training loss: 9198.94, average training loss: 9982.97, base loss: 14406.48
[INFO 2017-06-28 13:20:28,493 main.py:51] epoch 2965, training loss: 10145.26, average training loss: 9983.47, base loss: 14407.75
[INFO 2017-06-28 13:20:29,149 main.py:51] epoch 2966, training loss: 8810.43, average training loss: 9980.85, base loss: 14403.29
[INFO 2017-06-28 13:20:29,810 main.py:51] epoch 2967, training loss: 9607.03, average training loss: 9980.49, base loss: 14403.14
[INFO 2017-06-28 13:20:30,585 main.py:51] epoch 2968, training loss: 9088.44, average training loss: 9980.06, base loss: 14402.37
[INFO 2017-06-28 13:20:31,242 main.py:51] epoch 2969, training loss: 9493.79, average training loss: 9980.10, base loss: 14401.79
[INFO 2017-06-28 13:20:31,940 main.py:51] epoch 2970, training loss: 10360.26, average training loss: 9981.45, base loss: 14403.88
[INFO 2017-06-28 13:20:32,655 main.py:51] epoch 2971, training loss: 8865.35, average training loss: 9978.07, base loss: 14399.50
[INFO 2017-06-28 13:20:33,354 main.py:51] epoch 2972, training loss: 9431.43, average training loss: 9977.03, base loss: 14396.77
[INFO 2017-06-28 13:20:34,007 main.py:51] epoch 2973, training loss: 9023.69, average training loss: 9976.59, base loss: 14396.22
[INFO 2017-06-28 13:20:34,666 main.py:51] epoch 2974, training loss: 8852.24, average training loss: 9976.19, base loss: 14396.30
[INFO 2017-06-28 13:20:35,317 main.py:51] epoch 2975, training loss: 10695.81, average training loss: 9974.22, base loss: 14393.67
[INFO 2017-06-28 13:20:35,982 main.py:51] epoch 2976, training loss: 10988.09, average training loss: 9975.05, base loss: 14394.66
[INFO 2017-06-28 13:20:36,635 main.py:51] epoch 2977, training loss: 10646.18, average training loss: 9974.63, base loss: 14393.44
[INFO 2017-06-28 13:20:37,302 main.py:51] epoch 2978, training loss: 9557.03, average training loss: 9975.22, base loss: 14394.28
[INFO 2017-06-28 13:20:37,973 main.py:51] epoch 2979, training loss: 10492.53, average training loss: 9974.25, base loss: 14394.22
[INFO 2017-06-28 13:20:38,661 main.py:51] epoch 2980, training loss: 9725.31, average training loss: 9973.40, base loss: 14391.15
[INFO 2017-06-28 13:20:39,373 main.py:51] epoch 2981, training loss: 10241.27, average training loss: 9973.58, base loss: 14391.17
[INFO 2017-06-28 13:20:40,035 main.py:51] epoch 2982, training loss: 10450.68, average training loss: 9974.37, base loss: 14392.74
[INFO 2017-06-28 13:20:40,721 main.py:51] epoch 2983, training loss: 9475.57, average training loss: 9973.87, base loss: 14391.27
[INFO 2017-06-28 13:20:41,429 main.py:51] epoch 2984, training loss: 10239.38, average training loss: 9974.31, base loss: 14393.59
[INFO 2017-06-28 13:20:42,121 main.py:51] epoch 2985, training loss: 9499.23, average training loss: 9974.70, base loss: 14393.55
[INFO 2017-06-28 13:20:42,803 main.py:51] epoch 2986, training loss: 9390.50, average training loss: 9973.85, base loss: 14391.62
[INFO 2017-06-28 13:20:43,491 main.py:51] epoch 2987, training loss: 10725.83, average training loss: 9975.00, base loss: 14391.95
[INFO 2017-06-28 13:20:44,188 main.py:51] epoch 2988, training loss: 9966.51, average training loss: 9975.52, base loss: 14393.32
[INFO 2017-06-28 13:20:44,850 main.py:51] epoch 2989, training loss: 9180.27, average training loss: 9974.87, base loss: 14391.83
[INFO 2017-06-28 13:20:45,583 main.py:51] epoch 2990, training loss: 9459.17, average training loss: 9974.81, base loss: 14391.17
[INFO 2017-06-28 13:20:46,273 main.py:51] epoch 2991, training loss: 11416.05, average training loss: 9976.57, base loss: 14395.66
[INFO 2017-06-28 13:20:46,982 main.py:51] epoch 2992, training loss: 9660.44, average training loss: 9976.05, base loss: 14393.98
[INFO 2017-06-28 13:20:47,736 main.py:51] epoch 2993, training loss: 9509.27, average training loss: 9972.82, base loss: 14388.83
[INFO 2017-06-28 13:20:48,451 main.py:51] epoch 2994, training loss: 9618.71, average training loss: 9972.88, base loss: 14388.38
[INFO 2017-06-28 13:20:49,201 main.py:51] epoch 2995, training loss: 10247.76, average training loss: 9974.32, base loss: 14388.92
[INFO 2017-06-28 13:20:49,847 main.py:51] epoch 2996, training loss: 11018.19, average training loss: 9975.64, base loss: 14391.14
[INFO 2017-06-28 13:20:50,506 main.py:51] epoch 2997, training loss: 9088.57, average training loss: 9972.81, base loss: 14384.22
[INFO 2017-06-28 13:20:51,221 main.py:51] epoch 2998, training loss: 10243.28, average training loss: 9972.36, base loss: 14383.98
[INFO 2017-06-28 13:20:51,903 main.py:51] epoch 2999, training loss: 10011.17, average training loss: 9972.81, base loss: 14385.90
[INFO 2017-06-28 13:20:51,903 main.py:53] epoch 2999, testing
[INFO 2017-06-28 13:20:54,601 main.py:105] average testing loss: 11347.72, base loss: 15600.60
[INFO 2017-06-28 13:20:54,601 main.py:106] improve_loss: 4252.88, improve_percent: 0.27
[INFO 2017-06-28 13:20:54,602 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:20:55,348 main.py:51] epoch 3000, training loss: 8012.78, average training loss: 9971.31, base loss: 14383.29
[INFO 2017-06-28 13:20:55,987 main.py:51] epoch 3001, training loss: 9206.50, average training loss: 9968.93, base loss: 14380.85
[INFO 2017-06-28 13:20:56,650 main.py:51] epoch 3002, training loss: 9955.09, average training loss: 9969.99, base loss: 14381.57
[INFO 2017-06-28 13:20:57,313 main.py:51] epoch 3003, training loss: 8739.34, average training loss: 9970.21, base loss: 14382.24
[INFO 2017-06-28 13:20:58,015 main.py:51] epoch 3004, training loss: 10330.30, average training loss: 9971.32, base loss: 14383.96
[INFO 2017-06-28 13:20:58,687 main.py:51] epoch 3005, training loss: 8914.75, average training loss: 9970.78, base loss: 14383.39
[INFO 2017-06-28 13:20:59,417 main.py:51] epoch 3006, training loss: 9622.55, average training loss: 9971.88, base loss: 14386.60
[INFO 2017-06-28 13:21:00,166 main.py:51] epoch 3007, training loss: 9625.90, average training loss: 9970.82, base loss: 14385.11
[INFO 2017-06-28 13:21:00,860 main.py:51] epoch 3008, training loss: 8201.73, average training loss: 9968.73, base loss: 14382.42
[INFO 2017-06-28 13:21:01,570 main.py:51] epoch 3009, training loss: 9665.29, average training loss: 9967.99, base loss: 14382.44
[INFO 2017-06-28 13:21:02,245 main.py:51] epoch 3010, training loss: 10620.16, average training loss: 9967.09, base loss: 14382.16
[INFO 2017-06-28 13:21:02,989 main.py:51] epoch 3011, training loss: 8285.35, average training loss: 9967.03, base loss: 14380.47
[INFO 2017-06-28 13:21:03,737 main.py:51] epoch 3012, training loss: 9975.51, average training loss: 9967.48, base loss: 14380.18
[INFO 2017-06-28 13:21:04,583 main.py:51] epoch 3013, training loss: 12072.63, average training loss: 9969.50, base loss: 14384.33
[INFO 2017-06-28 13:21:05,262 main.py:51] epoch 3014, training loss: 10761.63, average training loss: 9971.13, base loss: 14387.10
[INFO 2017-06-28 13:21:05,946 main.py:51] epoch 3015, training loss: 10637.96, average training loss: 9973.04, base loss: 14390.74
[INFO 2017-06-28 13:21:06,624 main.py:51] epoch 3016, training loss: 9059.15, average training loss: 9971.89, base loss: 14389.26
[INFO 2017-06-28 13:21:07,304 main.py:51] epoch 3017, training loss: 10556.65, average training loss: 9971.27, base loss: 14388.57
[INFO 2017-06-28 13:21:07,970 main.py:51] epoch 3018, training loss: 10829.40, average training loss: 9972.51, base loss: 14390.31
[INFO 2017-06-28 13:21:08,675 main.py:51] epoch 3019, training loss: 9728.68, average training loss: 9972.74, base loss: 14390.89
[INFO 2017-06-28 13:21:09,365 main.py:51] epoch 3020, training loss: 9609.39, average training loss: 9972.85, base loss: 14391.80
[INFO 2017-06-28 13:21:10,042 main.py:51] epoch 3021, training loss: 9671.61, average training loss: 9971.92, base loss: 14391.44
[INFO 2017-06-28 13:21:10,711 main.py:51] epoch 3022, training loss: 9195.89, average training loss: 9970.89, base loss: 14389.77
[INFO 2017-06-28 13:21:11,435 main.py:51] epoch 3023, training loss: 10278.73, average training loss: 9970.58, base loss: 14389.42
[INFO 2017-06-28 13:21:12,118 main.py:51] epoch 3024, training loss: 10333.47, average training loss: 9971.42, base loss: 14392.02
[INFO 2017-06-28 13:21:12,773 main.py:51] epoch 3025, training loss: 9283.91, average training loss: 9970.61, base loss: 14390.82
[INFO 2017-06-28 13:21:13,494 main.py:51] epoch 3026, training loss: 8849.72, average training loss: 9970.84, base loss: 14391.07
[INFO 2017-06-28 13:21:14,177 main.py:51] epoch 3027, training loss: 9993.32, average training loss: 9972.27, base loss: 14392.46
[INFO 2017-06-28 13:21:14,901 main.py:51] epoch 3028, training loss: 10596.52, average training loss: 9973.88, base loss: 14395.76
[INFO 2017-06-28 13:21:15,552 main.py:51] epoch 3029, training loss: 9225.91, average training loss: 9974.29, base loss: 14396.13
[INFO 2017-06-28 13:21:16,241 main.py:51] epoch 3030, training loss: 9928.22, average training loss: 9974.14, base loss: 14396.71
[INFO 2017-06-28 13:21:16,968 main.py:51] epoch 3031, training loss: 9094.30, average training loss: 9972.04, base loss: 14394.60
[INFO 2017-06-28 13:21:17,660 main.py:51] epoch 3032, training loss: 9941.41, average training loss: 9972.72, base loss: 14395.66
[INFO 2017-06-28 13:21:18,367 main.py:51] epoch 3033, training loss: 10516.84, average training loss: 9973.42, base loss: 14395.41
[INFO 2017-06-28 13:21:19,076 main.py:51] epoch 3034, training loss: 8100.11, average training loss: 9972.00, base loss: 14393.00
[INFO 2017-06-28 13:21:19,812 main.py:51] epoch 3035, training loss: 9833.79, average training loss: 9971.88, base loss: 14394.22
[INFO 2017-06-28 13:21:20,514 main.py:51] epoch 3036, training loss: 9475.52, average training loss: 9970.71, base loss: 14391.94
[INFO 2017-06-28 13:21:21,262 main.py:51] epoch 3037, training loss: 12288.45, average training loss: 9972.89, base loss: 14394.94
[INFO 2017-06-28 13:21:21,926 main.py:51] epoch 3038, training loss: 10791.44, average training loss: 9973.65, base loss: 14396.73
[INFO 2017-06-28 13:21:22,608 main.py:51] epoch 3039, training loss: 10443.52, average training loss: 9972.57, base loss: 14396.69
[INFO 2017-06-28 13:21:23,280 main.py:51] epoch 3040, training loss: 10758.40, average training loss: 9971.49, base loss: 14394.03
[INFO 2017-06-28 13:21:23,946 main.py:51] epoch 3041, training loss: 10304.81, average training loss: 9971.02, base loss: 14392.69
[INFO 2017-06-28 13:21:24,633 main.py:51] epoch 3042, training loss: 9510.17, average training loss: 9971.69, base loss: 14394.04
[INFO 2017-06-28 13:21:25,292 main.py:51] epoch 3043, training loss: 9774.18, average training loss: 9970.22, base loss: 14391.01
[INFO 2017-06-28 13:21:25,958 main.py:51] epoch 3044, training loss: 10319.38, average training loss: 9971.12, base loss: 14391.39
[INFO 2017-06-28 13:21:26,636 main.py:51] epoch 3045, training loss: 8920.63, average training loss: 9970.52, base loss: 14390.71
[INFO 2017-06-28 13:21:27,316 main.py:51] epoch 3046, training loss: 9546.49, average training loss: 9971.24, base loss: 14394.31
[INFO 2017-06-28 13:21:27,987 main.py:51] epoch 3047, training loss: 10897.14, average training loss: 9972.80, base loss: 14396.27
[INFO 2017-06-28 13:21:28,666 main.py:51] epoch 3048, training loss: 9892.33, average training loss: 9971.63, base loss: 14394.27
[INFO 2017-06-28 13:21:29,321 main.py:51] epoch 3049, training loss: 9088.49, average training loss: 9971.21, base loss: 14392.79
[INFO 2017-06-28 13:21:29,987 main.py:51] epoch 3050, training loss: 10478.28, average training loss: 9972.04, base loss: 14395.93
[INFO 2017-06-28 13:21:30,628 main.py:51] epoch 3051, training loss: 10151.15, average training loss: 9972.00, base loss: 14395.70
[INFO 2017-06-28 13:21:31,291 main.py:51] epoch 3052, training loss: 10004.93, average training loss: 9971.75, base loss: 14397.56
[INFO 2017-06-28 13:21:31,954 main.py:51] epoch 3053, training loss: 9764.80, average training loss: 9972.11, base loss: 14398.58
[INFO 2017-06-28 13:21:32,629 main.py:51] epoch 3054, training loss: 11044.19, average training loss: 9973.41, base loss: 14400.80
[INFO 2017-06-28 13:21:33,292 main.py:51] epoch 3055, training loss: 9614.00, average training loss: 9973.30, base loss: 14400.91
[INFO 2017-06-28 13:21:33,976 main.py:51] epoch 3056, training loss: 10390.64, average training loss: 9974.47, base loss: 14403.00
[INFO 2017-06-28 13:21:34,631 main.py:51] epoch 3057, training loss: 9927.08, average training loss: 9972.37, base loss: 14400.13
[INFO 2017-06-28 13:21:35,305 main.py:51] epoch 3058, training loss: 8789.66, average training loss: 9971.83, base loss: 14397.35
[INFO 2017-06-28 13:21:35,963 main.py:51] epoch 3059, training loss: 10809.19, average training loss: 9972.16, base loss: 14398.85
[INFO 2017-06-28 13:21:36,619 main.py:51] epoch 3060, training loss: 10733.99, average training loss: 9972.30, base loss: 14399.65
[INFO 2017-06-28 13:21:37,273 main.py:51] epoch 3061, training loss: 9869.02, average training loss: 9971.88, base loss: 14401.29
[INFO 2017-06-28 13:21:37,930 main.py:51] epoch 3062, training loss: 9306.69, average training loss: 9972.80, base loss: 14403.11
[INFO 2017-06-28 13:21:38,606 main.py:51] epoch 3063, training loss: 9654.03, average training loss: 9972.14, base loss: 14402.94
[INFO 2017-06-28 13:21:39,279 main.py:51] epoch 3064, training loss: 8750.11, average training loss: 9970.00, base loss: 14401.12
[INFO 2017-06-28 13:21:39,945 main.py:51] epoch 3065, training loss: 10841.86, average training loss: 9970.81, base loss: 14402.02
[INFO 2017-06-28 13:21:40,601 main.py:51] epoch 3066, training loss: 10656.13, average training loss: 9971.87, base loss: 14402.96
[INFO 2017-06-28 13:21:41,272 main.py:51] epoch 3067, training loss: 10498.18, average training loss: 9973.78, base loss: 14405.55
[INFO 2017-06-28 13:21:41,932 main.py:51] epoch 3068, training loss: 9723.11, average training loss: 9974.86, base loss: 14407.59
[INFO 2017-06-28 13:21:42,598 main.py:51] epoch 3069, training loss: 9907.78, average training loss: 9974.34, base loss: 14406.67
[INFO 2017-06-28 13:21:43,267 main.py:51] epoch 3070, training loss: 9327.85, average training loss: 9974.34, base loss: 14407.17
[INFO 2017-06-28 13:21:43,924 main.py:51] epoch 3071, training loss: 10839.61, average training loss: 9975.58, base loss: 14409.25
[INFO 2017-06-28 13:21:44,594 main.py:51] epoch 3072, training loss: 10334.09, average training loss: 9976.25, base loss: 14409.88
[INFO 2017-06-28 13:21:45,257 main.py:51] epoch 3073, training loss: 8162.00, average training loss: 9973.56, base loss: 14407.45
[INFO 2017-06-28 13:21:45,940 main.py:51] epoch 3074, training loss: 9297.54, average training loss: 9972.46, base loss: 14405.87
[INFO 2017-06-28 13:21:46,595 main.py:51] epoch 3075, training loss: 11167.34, average training loss: 9973.47, base loss: 14408.74
[INFO 2017-06-28 13:21:47,261 main.py:51] epoch 3076, training loss: 8898.54, average training loss: 9972.44, base loss: 14406.89
[INFO 2017-06-28 13:21:47,926 main.py:51] epoch 3077, training loss: 10216.20, average training loss: 9972.82, base loss: 14409.14
[INFO 2017-06-28 13:21:48,597 main.py:51] epoch 3078, training loss: 9436.18, average training loss: 9972.89, base loss: 14409.09
[INFO 2017-06-28 13:21:49,259 main.py:51] epoch 3079, training loss: 10019.71, average training loss: 9972.59, base loss: 14410.47
[INFO 2017-06-28 13:21:49,929 main.py:51] epoch 3080, training loss: 10720.48, average training loss: 9973.23, base loss: 14411.34
[INFO 2017-06-28 13:21:50,595 main.py:51] epoch 3081, training loss: 10956.25, average training loss: 9971.96, base loss: 14409.70
[INFO 2017-06-28 13:21:51,256 main.py:51] epoch 3082, training loss: 10237.29, average training loss: 9971.56, base loss: 14408.15
[INFO 2017-06-28 13:21:51,941 main.py:51] epoch 3083, training loss: 10754.01, average training loss: 9972.30, base loss: 14409.47
[INFO 2017-06-28 13:21:52,635 main.py:51] epoch 3084, training loss: 10834.14, average training loss: 9974.09, base loss: 14410.91
[INFO 2017-06-28 13:21:53,284 main.py:51] epoch 3085, training loss: 10042.41, average training loss: 9974.78, base loss: 14410.96
[INFO 2017-06-28 13:21:53,948 main.py:51] epoch 3086, training loss: 9823.60, average training loss: 9975.00, base loss: 14411.30
[INFO 2017-06-28 13:21:54,730 main.py:51] epoch 3087, training loss: 10087.15, average training loss: 9974.90, base loss: 14411.37
[INFO 2017-06-28 13:21:55,476 main.py:51] epoch 3088, training loss: 9276.07, average training loss: 9975.70, base loss: 14412.81
[INFO 2017-06-28 13:21:56,229 main.py:51] epoch 3089, training loss: 9031.02, average training loss: 9974.12, base loss: 14411.32
[INFO 2017-06-28 13:21:57,020 main.py:51] epoch 3090, training loss: 9985.15, average training loss: 9974.34, base loss: 14411.14
[INFO 2017-06-28 13:21:57,881 main.py:51] epoch 3091, training loss: 11023.27, average training loss: 9975.79, base loss: 14414.80
[INFO 2017-06-28 13:21:58,850 main.py:51] epoch 3092, training loss: 9182.88, average training loss: 9974.91, base loss: 14412.90
[INFO 2017-06-28 13:21:59,772 main.py:51] epoch 3093, training loss: 11016.25, average training loss: 9976.71, base loss: 14415.87
[INFO 2017-06-28 13:22:00,747 main.py:51] epoch 3094, training loss: 9848.57, average training loss: 9976.59, base loss: 14416.72
[INFO 2017-06-28 13:22:01,671 main.py:51] epoch 3095, training loss: 10640.63, average training loss: 9975.14, base loss: 14415.17
[INFO 2017-06-28 13:22:02,713 main.py:51] epoch 3096, training loss: 10512.94, average training loss: 9975.91, base loss: 14416.45
[INFO 2017-06-28 13:22:03,693 main.py:51] epoch 3097, training loss: 10554.38, average training loss: 9976.20, base loss: 14417.65
[INFO 2017-06-28 13:22:04,627 main.py:51] epoch 3098, training loss: 9570.54, average training loss: 9976.70, base loss: 14417.96
[INFO 2017-06-28 13:22:05,577 main.py:51] epoch 3099, training loss: 8946.58, average training loss: 9976.11, base loss: 14417.94
[INFO 2017-06-28 13:22:05,578 main.py:53] epoch 3099, testing
[INFO 2017-06-28 13:22:09,285 main.py:105] average testing loss: 10840.33, base loss: 14951.71
[INFO 2017-06-28 13:22:09,285 main.py:106] improve_loss: 4111.37, improve_percent: 0.27
[INFO 2017-06-28 13:22:09,286 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:22:10,241 main.py:51] epoch 3100, training loss: 9896.35, average training loss: 9975.06, base loss: 14416.15
[INFO 2017-06-28 13:22:11,271 main.py:51] epoch 3101, training loss: 9693.24, average training loss: 9975.04, base loss: 14414.82
[INFO 2017-06-28 13:22:12,208 main.py:51] epoch 3102, training loss: 9653.00, average training loss: 9974.95, base loss: 14415.50
[INFO 2017-06-28 13:22:13,146 main.py:51] epoch 3103, training loss: 8840.40, average training loss: 9974.72, base loss: 14414.65
[INFO 2017-06-28 13:22:14,142 main.py:51] epoch 3104, training loss: 10259.81, average training loss: 9975.80, base loss: 14416.77
[INFO 2017-06-28 13:22:15,230 main.py:51] epoch 3105, training loss: 9954.35, average training loss: 9976.01, base loss: 14417.60
[INFO 2017-06-28 13:22:16,273 main.py:51] epoch 3106, training loss: 9428.91, average training loss: 9975.40, base loss: 14416.40
[INFO 2017-06-28 13:22:17,199 main.py:51] epoch 3107, training loss: 9128.57, average training loss: 9974.89, base loss: 14415.81
[INFO 2017-06-28 13:22:18,206 main.py:51] epoch 3108, training loss: 11101.25, average training loss: 9976.58, base loss: 14418.16
[INFO 2017-06-28 13:22:19,126 main.py:51] epoch 3109, training loss: 8467.62, average training loss: 9975.86, base loss: 14417.35
[INFO 2017-06-28 13:22:20,058 main.py:51] epoch 3110, training loss: 10600.35, average training loss: 9975.95, base loss: 14417.76
[INFO 2017-06-28 13:22:20,940 main.py:51] epoch 3111, training loss: 8394.15, average training loss: 9973.72, base loss: 14414.73
[INFO 2017-06-28 13:22:21,855 main.py:51] epoch 3112, training loss: 9149.26, average training loss: 9972.06, base loss: 14412.04
[INFO 2017-06-28 13:22:22,827 main.py:51] epoch 3113, training loss: 8914.42, average training loss: 9972.48, base loss: 14413.91
[INFO 2017-06-28 13:22:23,812 main.py:51] epoch 3114, training loss: 8820.95, average training loss: 9971.87, base loss: 14414.10
[INFO 2017-06-28 13:22:24,763 main.py:51] epoch 3115, training loss: 10772.88, average training loss: 9974.06, base loss: 14417.49
[INFO 2017-06-28 13:22:25,682 main.py:51] epoch 3116, training loss: 8082.49, average training loss: 9971.47, base loss: 14413.22
[INFO 2017-06-28 13:22:26,640 main.py:51] epoch 3117, training loss: 10478.55, average training loss: 9972.27, base loss: 14414.31
[INFO 2017-06-28 13:22:27,604 main.py:51] epoch 3118, training loss: 9678.36, average training loss: 9972.13, base loss: 14414.83
[INFO 2017-06-28 13:22:28,541 main.py:51] epoch 3119, training loss: 8977.33, average training loss: 9971.47, base loss: 14412.98
[INFO 2017-06-28 13:22:29,439 main.py:51] epoch 3120, training loss: 8915.33, average training loss: 9969.65, base loss: 14409.80
[INFO 2017-06-28 13:22:30,387 main.py:51] epoch 3121, training loss: 10351.57, average training loss: 9970.37, base loss: 14412.40
[INFO 2017-06-28 13:22:31,399 main.py:51] epoch 3122, training loss: 9134.60, average training loss: 9970.09, base loss: 14412.69
[INFO 2017-06-28 13:22:32,326 main.py:51] epoch 3123, training loss: 9435.45, average training loss: 9967.91, base loss: 14411.36
[INFO 2017-06-28 13:22:33,387 main.py:51] epoch 3124, training loss: 8339.16, average training loss: 9966.41, base loss: 14408.68
[INFO 2017-06-28 13:22:34,291 main.py:51] epoch 3125, training loss: 9628.44, average training loss: 9966.03, base loss: 14408.38
[INFO 2017-06-28 13:22:35,380 main.py:51] epoch 3126, training loss: 9627.80, average training loss: 9965.48, base loss: 14408.36
[INFO 2017-06-28 13:22:36,343 main.py:51] epoch 3127, training loss: 9202.22, average training loss: 9965.38, base loss: 14408.17
[INFO 2017-06-28 13:22:37,306 main.py:51] epoch 3128, training loss: 10848.08, average training loss: 9965.30, base loss: 14407.35
[INFO 2017-06-28 13:22:38,237 main.py:51] epoch 3129, training loss: 10968.37, average training loss: 9965.84, base loss: 14408.53
[INFO 2017-06-28 13:22:39,204 main.py:51] epoch 3130, training loss: 9763.55, average training loss: 9963.88, base loss: 14408.10
[INFO 2017-06-28 13:22:40,174 main.py:51] epoch 3131, training loss: 8916.51, average training loss: 9964.22, base loss: 14409.37
[INFO 2017-06-28 13:22:41,157 main.py:51] epoch 3132, training loss: 9163.67, average training loss: 9962.04, base loss: 14406.69
[INFO 2017-06-28 13:22:42,109 main.py:51] epoch 3133, training loss: 10420.87, average training loss: 9961.78, base loss: 14407.22
[INFO 2017-06-28 13:22:43,058 main.py:51] epoch 3134, training loss: 10440.29, average training loss: 9961.62, base loss: 14408.07
[INFO 2017-06-28 13:22:43,935 main.py:51] epoch 3135, training loss: 8930.75, average training loss: 9957.95, base loss: 14403.38
[INFO 2017-06-28 13:22:44,977 main.py:51] epoch 3136, training loss: 9618.96, average training loss: 9957.11, base loss: 14401.45
[INFO 2017-06-28 13:22:45,887 main.py:51] epoch 3137, training loss: 10737.25, average training loss: 9957.04, base loss: 14402.57
[INFO 2017-06-28 13:22:46,840 main.py:51] epoch 3138, training loss: 10482.95, average training loss: 9958.33, base loss: 14403.20
[INFO 2017-06-28 13:22:47,854 main.py:51] epoch 3139, training loss: 9618.94, average training loss: 9957.96, base loss: 14404.21
[INFO 2017-06-28 13:22:48,834 main.py:51] epoch 3140, training loss: 9200.91, average training loss: 9956.74, base loss: 14403.41
[INFO 2017-06-28 13:22:49,796 main.py:51] epoch 3141, training loss: 9014.52, average training loss: 9955.09, base loss: 14399.85
[INFO 2017-06-28 13:22:50,913 main.py:51] epoch 3142, training loss: 11904.48, average training loss: 9956.11, base loss: 14401.62
[INFO 2017-06-28 13:22:51,890 main.py:51] epoch 3143, training loss: 9351.02, average training loss: 9955.64, base loss: 14401.66
[INFO 2017-06-28 13:22:52,842 main.py:51] epoch 3144, training loss: 9002.25, average training loss: 9955.64, base loss: 14402.43
[INFO 2017-06-28 13:22:53,817 main.py:51] epoch 3145, training loss: 9359.11, average training loss: 9954.36, base loss: 14401.22
[INFO 2017-06-28 13:22:54,848 main.py:51] epoch 3146, training loss: 10697.64, average training loss: 9955.87, base loss: 14404.65
[INFO 2017-06-28 13:22:55,868 main.py:51] epoch 3147, training loss: 9085.86, average training loss: 9956.01, base loss: 14405.69
[INFO 2017-06-28 13:22:56,798 main.py:51] epoch 3148, training loss: 8995.08, average training loss: 9955.30, base loss: 14404.91
[INFO 2017-06-28 13:22:57,702 main.py:51] epoch 3149, training loss: 9708.38, average training loss: 9954.55, base loss: 14404.35
[INFO 2017-06-28 13:22:58,678 main.py:51] epoch 3150, training loss: 9563.91, average training loss: 9954.60, base loss: 14404.71
[INFO 2017-06-28 13:22:59,647 main.py:51] epoch 3151, training loss: 9256.90, average training loss: 9953.82, base loss: 14404.07
[INFO 2017-06-28 13:23:00,601 main.py:51] epoch 3152, training loss: 10894.60, average training loss: 9954.03, base loss: 14404.02
[INFO 2017-06-28 13:23:01,476 main.py:51] epoch 3153, training loss: 10994.89, average training loss: 9954.76, base loss: 14404.69
[INFO 2017-06-28 13:23:02,441 main.py:51] epoch 3154, training loss: 10234.52, average training loss: 9956.42, base loss: 14407.23
[INFO 2017-06-28 13:23:03,342 main.py:51] epoch 3155, training loss: 10413.83, average training loss: 9957.30, base loss: 14406.74
[INFO 2017-06-28 13:23:04,271 main.py:51] epoch 3156, training loss: 8692.09, average training loss: 9956.30, base loss: 14405.76
[INFO 2017-06-28 13:23:05,174 main.py:51] epoch 3157, training loss: 8838.83, average training loss: 9955.80, base loss: 14405.27
[INFO 2017-06-28 13:23:06,161 main.py:51] epoch 3158, training loss: 9373.09, average training loss: 9953.51, base loss: 14405.10
[INFO 2017-06-28 13:23:07,102 main.py:51] epoch 3159, training loss: 11409.98, average training loss: 9953.22, base loss: 14405.01
[INFO 2017-06-28 13:23:08,083 main.py:51] epoch 3160, training loss: 10213.04, average training loss: 9953.26, base loss: 14403.87
[INFO 2017-06-28 13:23:09,048 main.py:51] epoch 3161, training loss: 8231.94, average training loss: 9952.01, base loss: 14402.61
[INFO 2017-06-28 13:23:09,995 main.py:51] epoch 3162, training loss: 9914.22, average training loss: 9950.99, base loss: 14401.74
[INFO 2017-06-28 13:23:10,968 main.py:51] epoch 3163, training loss: 9933.04, average training loss: 9950.34, base loss: 14401.48
[INFO 2017-06-28 13:23:11,905 main.py:51] epoch 3164, training loss: 11134.52, average training loss: 9952.13, base loss: 14404.06
[INFO 2017-06-28 13:23:12,852 main.py:51] epoch 3165, training loss: 9586.16, average training loss: 9950.71, base loss: 14403.80
[INFO 2017-06-28 13:23:13,885 main.py:51] epoch 3166, training loss: 9906.16, average training loss: 9951.01, base loss: 14404.18
[INFO 2017-06-28 13:23:14,849 main.py:51] epoch 3167, training loss: 10841.25, average training loss: 9950.84, base loss: 14403.31
[INFO 2017-06-28 13:23:15,768 main.py:51] epoch 3168, training loss: 9398.33, average training loss: 9948.27, base loss: 14400.95
[INFO 2017-06-28 13:23:16,684 main.py:51] epoch 3169, training loss: 9479.04, average training loss: 9946.54, base loss: 14399.84
[INFO 2017-06-28 13:23:17,655 main.py:51] epoch 3170, training loss: 12944.00, average training loss: 9948.19, base loss: 14401.50
[INFO 2017-06-28 13:23:18,588 main.py:51] epoch 3171, training loss: 9767.81, average training loss: 9948.16, base loss: 14401.79
[INFO 2017-06-28 13:23:19,476 main.py:51] epoch 3172, training loss: 10284.52, average training loss: 9946.37, base loss: 14398.01
[INFO 2017-06-28 13:23:20,377 main.py:51] epoch 3173, training loss: 9429.55, average training loss: 9946.94, base loss: 14399.29
[INFO 2017-06-28 13:23:21,226 main.py:51] epoch 3174, training loss: 10043.42, average training loss: 9946.59, base loss: 14399.51
[INFO 2017-06-28 13:23:22,181 main.py:51] epoch 3175, training loss: 10373.98, average training loss: 9947.91, base loss: 14400.99
[INFO 2017-06-28 13:23:23,079 main.py:51] epoch 3176, training loss: 9763.23, average training loss: 9947.35, base loss: 14400.33
[INFO 2017-06-28 13:23:23,992 main.py:51] epoch 3177, training loss: 10978.99, average training loss: 9948.87, base loss: 14402.52
[INFO 2017-06-28 13:23:25,016 main.py:51] epoch 3178, training loss: 9934.41, average training loss: 9947.63, base loss: 14400.77
[INFO 2017-06-28 13:23:25,938 main.py:51] epoch 3179, training loss: 10403.71, average training loss: 9949.10, base loss: 14404.81
[INFO 2017-06-28 13:23:26,968 main.py:51] epoch 3180, training loss: 10662.66, average training loss: 9949.99, base loss: 14405.26
[INFO 2017-06-28 13:23:27,836 main.py:51] epoch 3181, training loss: 10627.01, average training loss: 9951.40, base loss: 14409.10
[INFO 2017-06-28 13:23:28,874 main.py:51] epoch 3182, training loss: 9420.02, average training loss: 9950.69, base loss: 14408.77
[INFO 2017-06-28 13:23:29,825 main.py:51] epoch 3183, training loss: 9036.38, average training loss: 9948.64, base loss: 14404.91
[INFO 2017-06-28 13:23:30,504 main.py:51] epoch 3184, training loss: 8844.49, average training loss: 9948.32, base loss: 14404.23
[INFO 2017-06-28 13:23:31,170 main.py:51] epoch 3185, training loss: 8810.10, average training loss: 9947.36, base loss: 14402.04
[INFO 2017-06-28 13:23:31,837 main.py:51] epoch 3186, training loss: 9583.89, average training loss: 9946.16, base loss: 14400.57
[INFO 2017-06-28 13:23:32,499 main.py:51] epoch 3187, training loss: 9011.22, average training loss: 9944.83, base loss: 14397.71
[INFO 2017-06-28 13:23:33,175 main.py:51] epoch 3188, training loss: 10320.03, average training loss: 9944.59, base loss: 14397.00
[INFO 2017-06-28 13:23:33,820 main.py:51] epoch 3189, training loss: 9845.27, average training loss: 9945.52, base loss: 14399.02
[INFO 2017-06-28 13:23:34,503 main.py:51] epoch 3190, training loss: 10075.36, average training loss: 9945.63, base loss: 14399.10
[INFO 2017-06-28 13:23:35,156 main.py:51] epoch 3191, training loss: 8732.03, average training loss: 9945.35, base loss: 14398.44
[INFO 2017-06-28 13:23:35,823 main.py:51] epoch 3192, training loss: 8815.37, average training loss: 9945.13, base loss: 14398.61
[INFO 2017-06-28 13:23:36,506 main.py:51] epoch 3193, training loss: 10588.67, average training loss: 9945.39, base loss: 14399.23
[INFO 2017-06-28 13:23:37,167 main.py:51] epoch 3194, training loss: 9504.71, average training loss: 9944.82, base loss: 14399.91
[INFO 2017-06-28 13:23:37,859 main.py:51] epoch 3195, training loss: 9891.80, average training loss: 9944.38, base loss: 14399.24
[INFO 2017-06-28 13:23:38,522 main.py:51] epoch 3196, training loss: 8183.58, average training loss: 9941.51, base loss: 14393.88
[INFO 2017-06-28 13:23:39,181 main.py:51] epoch 3197, training loss: 9355.27, average training loss: 9942.27, base loss: 14397.87
[INFO 2017-06-28 13:23:39,855 main.py:51] epoch 3198, training loss: 10232.47, average training loss: 9942.03, base loss: 14397.08
[INFO 2017-06-28 13:23:40,532 main.py:51] epoch 3199, training loss: 9526.81, average training loss: 9941.94, base loss: 14398.27
[INFO 2017-06-28 13:23:40,533 main.py:53] epoch 3199, testing
[INFO 2017-06-28 13:23:43,118 main.py:105] average testing loss: 11383.24, base loss: 15894.89
[INFO 2017-06-28 13:23:43,118 main.py:106] improve_loss: 4511.65, improve_percent: 0.28
[INFO 2017-06-28 13:23:43,118 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 13:23:43,155 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:23:43,797 main.py:51] epoch 3200, training loss: 8378.81, average training loss: 9940.18, base loss: 14395.23
[INFO 2017-06-28 13:23:44,458 main.py:51] epoch 3201, training loss: 9497.61, average training loss: 9939.11, base loss: 14392.60
[INFO 2017-06-28 13:23:45,101 main.py:51] epoch 3202, training loss: 10664.66, average training loss: 9941.27, base loss: 14395.92
[INFO 2017-06-28 13:23:45,754 main.py:51] epoch 3203, training loss: 8661.11, average training loss: 9939.86, base loss: 14394.18
[INFO 2017-06-28 13:23:46,398 main.py:51] epoch 3204, training loss: 9789.46, average training loss: 9939.21, base loss: 14394.36
[INFO 2017-06-28 13:23:47,063 main.py:51] epoch 3205, training loss: 10709.23, average training loss: 9941.01, base loss: 14398.05
[INFO 2017-06-28 13:23:47,722 main.py:51] epoch 3206, training loss: 10498.21, average training loss: 9941.47, base loss: 14399.78
[INFO 2017-06-28 13:23:48,402 main.py:51] epoch 3207, training loss: 10651.87, average training loss: 9941.07, base loss: 14401.01
[INFO 2017-06-28 13:23:49,069 main.py:51] epoch 3208, training loss: 9777.44, average training loss: 9941.98, base loss: 14402.75
[INFO 2017-06-28 13:23:49,752 main.py:51] epoch 3209, training loss: 8742.11, average training loss: 9939.29, base loss: 14398.56
[INFO 2017-06-28 13:23:50,410 main.py:51] epoch 3210, training loss: 9925.07, average training loss: 9937.98, base loss: 14396.71
[INFO 2017-06-28 13:23:51,095 main.py:51] epoch 3211, training loss: 11883.73, average training loss: 9940.43, base loss: 14402.21
[INFO 2017-06-28 13:23:51,773 main.py:51] epoch 3212, training loss: 8518.16, average training loss: 9939.96, base loss: 14403.10
[INFO 2017-06-28 13:23:52,440 main.py:51] epoch 3213, training loss: 9928.59, average training loss: 9940.70, base loss: 14406.08
[INFO 2017-06-28 13:23:53,101 main.py:51] epoch 3214, training loss: 8996.26, average training loss: 9940.57, base loss: 14406.58
[INFO 2017-06-28 13:23:53,751 main.py:51] epoch 3215, training loss: 9399.65, average training loss: 9940.72, base loss: 14408.32
[INFO 2017-06-28 13:23:54,415 main.py:51] epoch 3216, training loss: 9336.55, average training loss: 9939.38, base loss: 14406.69
[INFO 2017-06-28 13:23:55,116 main.py:51] epoch 3217, training loss: 11263.81, average training loss: 9940.60, base loss: 14407.95
[INFO 2017-06-28 13:23:55,804 main.py:51] epoch 3218, training loss: 9360.06, average training loss: 9940.11, base loss: 14407.08
[INFO 2017-06-28 13:23:56,477 main.py:51] epoch 3219, training loss: 10990.93, average training loss: 9939.27, base loss: 14406.04
[INFO 2017-06-28 13:23:57,138 main.py:51] epoch 3220, training loss: 9427.61, average training loss: 9937.47, base loss: 14403.52
[INFO 2017-06-28 13:23:57,819 main.py:51] epoch 3221, training loss: 9659.75, average training loss: 9936.93, base loss: 14403.77
[INFO 2017-06-28 13:23:58,479 main.py:51] epoch 3222, training loss: 12514.91, average training loss: 9938.64, base loss: 14406.71
[INFO 2017-06-28 13:23:59,170 main.py:51] epoch 3223, training loss: 8761.03, average training loss: 9937.78, base loss: 14405.54
[INFO 2017-06-28 13:23:59,833 main.py:51] epoch 3224, training loss: 9464.53, average training loss: 9936.26, base loss: 14402.54
[INFO 2017-06-28 13:24:00,488 main.py:51] epoch 3225, training loss: 9350.24, average training loss: 9934.74, base loss: 14399.53
[INFO 2017-06-28 13:24:01,137 main.py:51] epoch 3226, training loss: 10463.72, average training loss: 9936.21, base loss: 14401.97
[INFO 2017-06-28 13:24:01,794 main.py:51] epoch 3227, training loss: 10846.30, average training loss: 9936.61, base loss: 14402.89
[INFO 2017-06-28 13:24:02,456 main.py:51] epoch 3228, training loss: 10294.72, average training loss: 9936.69, base loss: 14402.28
[INFO 2017-06-28 13:24:03,112 main.py:51] epoch 3229, training loss: 9196.00, average training loss: 9935.99, base loss: 14400.81
[INFO 2017-06-28 13:24:03,764 main.py:51] epoch 3230, training loss: 10108.79, average training loss: 9936.40, base loss: 14402.52
[INFO 2017-06-28 13:24:04,435 main.py:51] epoch 3231, training loss: 10643.94, average training loss: 9938.12, base loss: 14406.27
[INFO 2017-06-28 13:24:05,069 main.py:51] epoch 3232, training loss: 9764.62, average training loss: 9937.68, base loss: 14406.33
[INFO 2017-06-28 13:24:05,733 main.py:51] epoch 3233, training loss: 8983.89, average training loss: 9934.57, base loss: 14402.04
[INFO 2017-06-28 13:24:06,400 main.py:51] epoch 3234, training loss: 9416.57, average training loss: 9932.90, base loss: 14399.94
[INFO 2017-06-28 13:24:07,065 main.py:51] epoch 3235, training loss: 9260.22, average training loss: 9932.64, base loss: 14399.50
[INFO 2017-06-28 13:24:07,718 main.py:51] epoch 3236, training loss: 9526.44, average training loss: 9933.13, base loss: 14399.98
[INFO 2017-06-28 13:24:08,381 main.py:51] epoch 3237, training loss: 11045.64, average training loss: 9933.03, base loss: 14399.77
[INFO 2017-06-28 13:24:09,052 main.py:51] epoch 3238, training loss: 11579.09, average training loss: 9935.16, base loss: 14402.28
[INFO 2017-06-28 13:24:09,725 main.py:51] epoch 3239, training loss: 11508.80, average training loss: 9936.83, base loss: 14405.15
[INFO 2017-06-28 13:24:10,387 main.py:51] epoch 3240, training loss: 9715.08, average training loss: 9936.38, base loss: 14405.41
[INFO 2017-06-28 13:24:11,066 main.py:51] epoch 3241, training loss: 9180.25, average training loss: 9934.78, base loss: 14404.33
[INFO 2017-06-28 13:24:11,733 main.py:51] epoch 3242, training loss: 11000.09, average training loss: 9937.52, base loss: 14409.45
[INFO 2017-06-28 13:24:12,383 main.py:51] epoch 3243, training loss: 8945.15, average training loss: 9934.62, base loss: 14406.35
[INFO 2017-06-28 13:24:13,042 main.py:51] epoch 3244, training loss: 9495.59, average training loss: 9935.13, base loss: 14408.01
[INFO 2017-06-28 13:24:13,703 main.py:51] epoch 3245, training loss: 9504.29, average training loss: 9934.58, base loss: 14406.84
[INFO 2017-06-28 13:24:14,368 main.py:51] epoch 3246, training loss: 9193.85, average training loss: 9934.57, base loss: 14407.81
[INFO 2017-06-28 13:24:15,027 main.py:51] epoch 3247, training loss: 10793.15, average training loss: 9935.27, base loss: 14409.99
[INFO 2017-06-28 13:24:15,684 main.py:51] epoch 3248, training loss: 9052.14, average training loss: 9931.73, base loss: 14404.48
[INFO 2017-06-28 13:24:16,351 main.py:51] epoch 3249, training loss: 10468.91, average training loss: 9931.41, base loss: 14405.77
[INFO 2017-06-28 13:24:16,994 main.py:51] epoch 3250, training loss: 11041.94, average training loss: 9932.46, base loss: 14407.79
[INFO 2017-06-28 13:24:17,699 main.py:51] epoch 3251, training loss: 9693.47, average training loss: 9931.84, base loss: 14406.57
[INFO 2017-06-28 13:24:18,448 main.py:51] epoch 3252, training loss: 9919.17, average training loss: 9931.02, base loss: 14404.56
[INFO 2017-06-28 13:24:19,107 main.py:51] epoch 3253, training loss: 9045.43, average training loss: 9928.68, base loss: 14400.93
[INFO 2017-06-28 13:24:19,798 main.py:51] epoch 3254, training loss: 10567.28, average training loss: 9930.09, base loss: 14404.71
[INFO 2017-06-28 13:24:20,453 main.py:51] epoch 3255, training loss: 9892.46, average training loss: 9930.91, base loss: 14407.27
[INFO 2017-06-28 13:24:21,108 main.py:51] epoch 3256, training loss: 11361.78, average training loss: 9930.27, base loss: 14406.47
[INFO 2017-06-28 13:24:21,746 main.py:51] epoch 3257, training loss: 11707.69, average training loss: 9928.27, base loss: 14403.22
[INFO 2017-06-28 13:24:22,411 main.py:51] epoch 3258, training loss: 9863.35, average training loss: 9927.05, base loss: 14401.51
[INFO 2017-06-28 13:24:23,090 main.py:51] epoch 3259, training loss: 9950.17, average training loss: 9926.52, base loss: 14401.12
[INFO 2017-06-28 13:24:23,733 main.py:51] epoch 3260, training loss: 8953.99, average training loss: 9926.72, base loss: 14400.40
[INFO 2017-06-28 13:24:24,391 main.py:51] epoch 3261, training loss: 10207.07, average training loss: 9926.07, base loss: 14400.30
[INFO 2017-06-28 13:24:25,048 main.py:51] epoch 3262, training loss: 11620.63, average training loss: 9927.19, base loss: 14402.35
[INFO 2017-06-28 13:24:25,737 main.py:51] epoch 3263, training loss: 10168.59, average training loss: 9928.76, base loss: 14404.53
[INFO 2017-06-28 13:24:26,420 main.py:51] epoch 3264, training loss: 10416.77, average training loss: 9928.10, base loss: 14403.12
[INFO 2017-06-28 13:24:27,087 main.py:51] epoch 3265, training loss: 8307.92, average training loss: 9926.31, base loss: 14400.35
[INFO 2017-06-28 13:24:27,760 main.py:51] epoch 3266, training loss: 10757.68, average training loss: 9925.45, base loss: 14398.88
[INFO 2017-06-28 13:24:28,446 main.py:51] epoch 3267, training loss: 10517.65, average training loss: 9926.61, base loss: 14400.86
[INFO 2017-06-28 13:24:29,103 main.py:51] epoch 3268, training loss: 8576.33, average training loss: 9925.81, base loss: 14399.30
[INFO 2017-06-28 13:24:29,771 main.py:51] epoch 3269, training loss: 10156.84, average training loss: 9925.92, base loss: 14399.18
[INFO 2017-06-28 13:24:30,487 main.py:51] epoch 3270, training loss: 10796.22, average training loss: 9925.77, base loss: 14399.05
[INFO 2017-06-28 13:24:31,136 main.py:51] epoch 3271, training loss: 10368.69, average training loss: 9925.32, base loss: 14398.18
[INFO 2017-06-28 13:24:31,816 main.py:51] epoch 3272, training loss: 9683.24, average training loss: 9925.34, base loss: 14397.74
[INFO 2017-06-28 13:24:32,510 main.py:51] epoch 3273, training loss: 12154.72, average training loss: 9927.24, base loss: 14399.70
[INFO 2017-06-28 13:24:33,194 main.py:51] epoch 3274, training loss: 9476.86, average training loss: 9926.14, base loss: 14399.19
[INFO 2017-06-28 13:24:33,853 main.py:51] epoch 3275, training loss: 10573.12, average training loss: 9926.31, base loss: 14400.02
[INFO 2017-06-28 13:24:34,508 main.py:51] epoch 3276, training loss: 9392.88, average training loss: 9925.12, base loss: 14398.85
[INFO 2017-06-28 13:24:35,151 main.py:51] epoch 3277, training loss: 10303.66, average training loss: 9925.15, base loss: 14399.43
[INFO 2017-06-28 13:24:35,790 main.py:51] epoch 3278, training loss: 12344.09, average training loss: 9927.93, base loss: 14405.00
[INFO 2017-06-28 13:24:36,438 main.py:51] epoch 3279, training loss: 9822.58, average training loss: 9928.80, base loss: 14405.64
[INFO 2017-06-28 13:24:37,080 main.py:51] epoch 3280, training loss: 10552.44, average training loss: 9929.90, base loss: 14406.91
[INFO 2017-06-28 13:24:37,734 main.py:51] epoch 3281, training loss: 10743.68, average training loss: 9932.39, base loss: 14410.55
[INFO 2017-06-28 13:24:38,418 main.py:51] epoch 3282, training loss: 9521.32, average training loss: 9933.12, base loss: 14410.99
[INFO 2017-06-28 13:24:39,113 main.py:51] epoch 3283, training loss: 9408.43, average training loss: 9928.26, base loss: 14404.61
[INFO 2017-06-28 13:24:39,766 main.py:51] epoch 3284, training loss: 9155.42, average training loss: 9927.66, base loss: 14402.00
[INFO 2017-06-28 13:24:40,433 main.py:51] epoch 3285, training loss: 10214.99, average training loss: 9926.25, base loss: 14400.49
[INFO 2017-06-28 13:24:41,098 main.py:51] epoch 3286, training loss: 9308.98, average training loss: 9925.51, base loss: 14399.53
[INFO 2017-06-28 13:24:41,764 main.py:51] epoch 3287, training loss: 9984.26, average training loss: 9925.65, base loss: 14401.56
[INFO 2017-06-28 13:24:42,434 main.py:51] epoch 3288, training loss: 10110.96, average training loss: 9925.92, base loss: 14402.79
[INFO 2017-06-28 13:24:43,100 main.py:51] epoch 3289, training loss: 10326.07, average training loss: 9927.15, base loss: 14404.29
[INFO 2017-06-28 13:24:43,762 main.py:51] epoch 3290, training loss: 10518.10, average training loss: 9926.79, base loss: 14405.69
[INFO 2017-06-28 13:24:44,432 main.py:51] epoch 3291, training loss: 9271.40, average training loss: 9927.15, base loss: 14406.40
[INFO 2017-06-28 13:24:45,070 main.py:51] epoch 3292, training loss: 8310.95, average training loss: 9925.55, base loss: 14402.82
[INFO 2017-06-28 13:24:45,729 main.py:51] epoch 3293, training loss: 9579.90, average training loss: 9924.46, base loss: 14401.45
[INFO 2017-06-28 13:24:46,397 main.py:51] epoch 3294, training loss: 9893.03, average training loss: 9924.56, base loss: 14402.36
[INFO 2017-06-28 13:24:47,043 main.py:51] epoch 3295, training loss: 10208.07, average training loss: 9925.78, base loss: 14404.65
[INFO 2017-06-28 13:24:47,678 main.py:51] epoch 3296, training loss: 10818.70, average training loss: 9926.66, base loss: 14405.90
[INFO 2017-06-28 13:24:48,325 main.py:51] epoch 3297, training loss: 10452.97, average training loss: 9928.11, base loss: 14407.34
[INFO 2017-06-28 13:24:49,011 main.py:51] epoch 3298, training loss: 9306.54, average training loss: 9928.00, base loss: 14407.79
[INFO 2017-06-28 13:24:49,677 main.py:51] epoch 3299, training loss: 9063.05, average training loss: 9928.04, base loss: 14408.64
[INFO 2017-06-28 13:24:49,677 main.py:53] epoch 3299, testing
[INFO 2017-06-28 13:24:52,305 main.py:105] average testing loss: 11216.49, base loss: 15343.02
[INFO 2017-06-28 13:24:52,305 main.py:106] improve_loss: 4126.52, improve_percent: 0.27
[INFO 2017-06-28 13:24:52,306 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:24:52,991 main.py:51] epoch 3300, training loss: 10621.58, average training loss: 9929.49, base loss: 14411.22
[INFO 2017-06-28 13:24:53,702 main.py:51] epoch 3301, training loss: 9808.28, average training loss: 9927.32, base loss: 14408.85
[INFO 2017-06-28 13:24:54,381 main.py:51] epoch 3302, training loss: 8799.99, average training loss: 9925.85, base loss: 14408.36
[INFO 2017-06-28 13:24:55,046 main.py:51] epoch 3303, training loss: 10010.07, average training loss: 9925.78, base loss: 14408.40
[INFO 2017-06-28 13:24:55,696 main.py:51] epoch 3304, training loss: 8178.33, average training loss: 9923.06, base loss: 14403.47
[INFO 2017-06-28 13:24:56,363 main.py:51] epoch 3305, training loss: 10805.78, average training loss: 9923.68, base loss: 14405.75
[INFO 2017-06-28 13:24:57,037 main.py:51] epoch 3306, training loss: 10663.14, average training loss: 9925.50, base loss: 14407.75
[INFO 2017-06-28 13:24:57,683 main.py:51] epoch 3307, training loss: 9725.42, average training loss: 9926.58, base loss: 14410.42
[INFO 2017-06-28 13:24:58,385 main.py:51] epoch 3308, training loss: 11127.87, average training loss: 9928.55, base loss: 14413.41
[INFO 2017-06-28 13:24:59,047 main.py:51] epoch 3309, training loss: 9932.96, average training loss: 9928.42, base loss: 14413.88
[INFO 2017-06-28 13:24:59,717 main.py:51] epoch 3310, training loss: 9670.75, average training loss: 9928.54, base loss: 14412.69
[INFO 2017-06-28 13:25:00,370 main.py:51] epoch 3311, training loss: 8629.92, average training loss: 9926.96, base loss: 14411.40
[INFO 2017-06-28 13:25:01,045 main.py:51] epoch 3312, training loss: 8425.98, average training loss: 9924.39, base loss: 14407.00
[INFO 2017-06-28 13:25:01,709 main.py:51] epoch 3313, training loss: 9404.80, average training loss: 9923.49, base loss: 14407.04
[INFO 2017-06-28 13:25:02,364 main.py:51] epoch 3314, training loss: 9515.42, average training loss: 9921.83, base loss: 14405.89
[INFO 2017-06-28 13:25:03,029 main.py:51] epoch 3315, training loss: 8934.68, average training loss: 9921.03, base loss: 14405.65
[INFO 2017-06-28 13:25:03,735 main.py:51] epoch 3316, training loss: 10348.59, average training loss: 9922.11, base loss: 14407.57
[INFO 2017-06-28 13:25:04,425 main.py:51] epoch 3317, training loss: 10607.55, average training loss: 9922.33, base loss: 14407.48
[INFO 2017-06-28 13:25:05,094 main.py:51] epoch 3318, training loss: 9788.06, average training loss: 9923.22, base loss: 14407.83
[INFO 2017-06-28 13:25:05,757 main.py:51] epoch 3319, training loss: 8514.34, average training loss: 9921.65, base loss: 14407.32
[INFO 2017-06-28 13:25:06,444 main.py:51] epoch 3320, training loss: 10452.47, average training loss: 9922.00, base loss: 14408.13
[INFO 2017-06-28 13:25:07,100 main.py:51] epoch 3321, training loss: 9747.59, average training loss: 9921.87, base loss: 14408.35
[INFO 2017-06-28 13:25:07,778 main.py:51] epoch 3322, training loss: 11157.70, average training loss: 9921.96, base loss: 14409.03
[INFO 2017-06-28 13:25:08,439 main.py:51] epoch 3323, training loss: 8631.14, average training loss: 9920.26, base loss: 14406.04
[INFO 2017-06-28 13:25:09,105 main.py:51] epoch 3324, training loss: 10809.54, average training loss: 9921.38, base loss: 14408.30
[INFO 2017-06-28 13:25:09,762 main.py:51] epoch 3325, training loss: 9515.48, average training loss: 9921.19, base loss: 14407.23
[INFO 2017-06-28 13:25:10,413 main.py:51] epoch 3326, training loss: 9963.99, average training loss: 9920.81, base loss: 14406.55
[INFO 2017-06-28 13:25:11,053 main.py:51] epoch 3327, training loss: 11068.06, average training loss: 9922.21, base loss: 14408.30
[INFO 2017-06-28 13:25:11,690 main.py:51] epoch 3328, training loss: 12227.38, average training loss: 9924.83, base loss: 14411.57
[INFO 2017-06-28 13:25:12,345 main.py:51] epoch 3329, training loss: 9127.92, average training loss: 9925.31, base loss: 14412.31
[INFO 2017-06-28 13:25:13,005 main.py:51] epoch 3330, training loss: 10292.97, average training loss: 9926.71, base loss: 14414.04
[INFO 2017-06-28 13:25:13,684 main.py:51] epoch 3331, training loss: 9536.45, average training loss: 9926.06, base loss: 14413.68
[INFO 2017-06-28 13:25:14,323 main.py:51] epoch 3332, training loss: 9924.32, average training loss: 9925.75, base loss: 14414.92
[INFO 2017-06-28 13:25:14,983 main.py:51] epoch 3333, training loss: 10431.30, average training loss: 9922.43, base loss: 14410.37
[INFO 2017-06-28 13:25:15,639 main.py:51] epoch 3334, training loss: 9866.22, average training loss: 9922.21, base loss: 14408.90
[INFO 2017-06-28 13:25:16,303 main.py:51] epoch 3335, training loss: 10046.94, average training loss: 9922.55, base loss: 14411.23
[INFO 2017-06-28 13:25:16,955 main.py:51] epoch 3336, training loss: 8962.92, average training loss: 9922.12, base loss: 14412.00
[INFO 2017-06-28 13:25:17,620 main.py:51] epoch 3337, training loss: 9816.76, average training loss: 9921.81, base loss: 14411.96
[INFO 2017-06-28 13:25:18,275 main.py:51] epoch 3338, training loss: 9555.91, average training loss: 9920.50, base loss: 14411.47
[INFO 2017-06-28 13:25:18,930 main.py:51] epoch 3339, training loss: 9482.12, average training loss: 9920.25, base loss: 14410.36
[INFO 2017-06-28 13:25:19,592 main.py:51] epoch 3340, training loss: 11522.54, average training loss: 9920.33, base loss: 14409.42
[INFO 2017-06-28 13:25:20,246 main.py:51] epoch 3341, training loss: 10337.84, average training loss: 9920.71, base loss: 14410.75
[INFO 2017-06-28 13:25:20,899 main.py:51] epoch 3342, training loss: 10994.51, average training loss: 9920.84, base loss: 14410.29
[INFO 2017-06-28 13:25:21,590 main.py:51] epoch 3343, training loss: 8847.26, average training loss: 9920.17, base loss: 14408.16
[INFO 2017-06-28 13:25:22,248 main.py:51] epoch 3344, training loss: 9520.59, average training loss: 9919.51, base loss: 14408.04
[INFO 2017-06-28 13:25:22,901 main.py:51] epoch 3345, training loss: 10204.03, average training loss: 9918.82, base loss: 14406.81
[INFO 2017-06-28 13:25:23,560 main.py:51] epoch 3346, training loss: 10329.42, average training loss: 9919.76, base loss: 14407.95
[INFO 2017-06-28 13:25:24,247 main.py:51] epoch 3347, training loss: 9927.27, average training loss: 9920.34, base loss: 14408.93
[INFO 2017-06-28 13:25:24,886 main.py:51] epoch 3348, training loss: 9545.26, average training loss: 9919.66, base loss: 14407.63
[INFO 2017-06-28 13:25:25,550 main.py:51] epoch 3349, training loss: 9507.26, average training loss: 9918.74, base loss: 14406.22
[INFO 2017-06-28 13:25:26,206 main.py:51] epoch 3350, training loss: 8891.01, average training loss: 9915.74, base loss: 14400.47
[INFO 2017-06-28 13:25:26,875 main.py:51] epoch 3351, training loss: 10732.95, average training loss: 9916.95, base loss: 14402.40
[INFO 2017-06-28 13:25:27,554 main.py:51] epoch 3352, training loss: 10245.90, average training loss: 9916.97, base loss: 14402.72
[INFO 2017-06-28 13:25:28,199 main.py:51] epoch 3353, training loss: 10206.94, average training loss: 9916.92, base loss: 14401.85
[INFO 2017-06-28 13:25:28,853 main.py:51] epoch 3354, training loss: 9478.26, average training loss: 9916.10, base loss: 14401.05
[INFO 2017-06-28 13:25:29,520 main.py:51] epoch 3355, training loss: 9389.57, average training loss: 9916.53, base loss: 14401.32
[INFO 2017-06-28 13:25:30,179 main.py:51] epoch 3356, training loss: 11653.68, average training loss: 9919.04, base loss: 14406.84
[INFO 2017-06-28 13:25:30,845 main.py:51] epoch 3357, training loss: 9641.88, average training loss: 9919.68, base loss: 14407.02
[INFO 2017-06-28 13:25:31,505 main.py:51] epoch 3358, training loss: 10144.64, average training loss: 9921.12, base loss: 14410.65
[INFO 2017-06-28 13:25:32,164 main.py:51] epoch 3359, training loss: 9516.01, average training loss: 9921.05, base loss: 14409.91
[INFO 2017-06-28 13:25:32,820 main.py:51] epoch 3360, training loss: 8702.07, average training loss: 9919.49, base loss: 14407.36
[INFO 2017-06-28 13:25:33,471 main.py:51] epoch 3361, training loss: 9618.35, average training loss: 9919.06, base loss: 14407.84
[INFO 2017-06-28 13:25:34,135 main.py:51] epoch 3362, training loss: 11838.86, average training loss: 9920.65, base loss: 14410.80
[INFO 2017-06-28 13:25:34,782 main.py:51] epoch 3363, training loss: 9372.19, average training loss: 9919.96, base loss: 14410.26
[INFO 2017-06-28 13:25:35,423 main.py:51] epoch 3364, training loss: 9041.72, average training loss: 9917.20, base loss: 14407.98
[INFO 2017-06-28 13:25:36,071 main.py:51] epoch 3365, training loss: 9468.38, average training loss: 9916.59, base loss: 14406.98
[INFO 2017-06-28 13:25:36,713 main.py:51] epoch 3366, training loss: 9031.84, average training loss: 9916.23, base loss: 14405.71
[INFO 2017-06-28 13:25:37,376 main.py:51] epoch 3367, training loss: 10644.96, average training loss: 9917.07, base loss: 14407.80
[INFO 2017-06-28 13:25:38,029 main.py:51] epoch 3368, training loss: 8990.41, average training loss: 9914.35, base loss: 14405.49
[INFO 2017-06-28 13:25:38,696 main.py:51] epoch 3369, training loss: 10715.72, average training loss: 9915.07, base loss: 14405.87
[INFO 2017-06-28 13:25:39,361 main.py:51] epoch 3370, training loss: 11221.79, average training loss: 9915.79, base loss: 14406.84
[INFO 2017-06-28 13:25:40,004 main.py:51] epoch 3371, training loss: 8272.19, average training loss: 9914.58, base loss: 14407.30
[INFO 2017-06-28 13:25:40,662 main.py:51] epoch 3372, training loss: 10096.61, average training loss: 9914.70, base loss: 14407.19
[INFO 2017-06-28 13:25:41,326 main.py:51] epoch 3373, training loss: 9433.70, average training loss: 9914.33, base loss: 14406.77
[INFO 2017-06-28 13:25:41,999 main.py:51] epoch 3374, training loss: 8500.44, average training loss: 9911.55, base loss: 14402.17
[INFO 2017-06-28 13:25:42,703 main.py:51] epoch 3375, training loss: 10090.77, average training loss: 9911.12, base loss: 14402.97
[INFO 2017-06-28 13:25:43,368 main.py:51] epoch 3376, training loss: 10420.18, average training loss: 9910.95, base loss: 14403.41
[INFO 2017-06-28 13:25:44,020 main.py:51] epoch 3377, training loss: 9613.77, average training loss: 9910.00, base loss: 14403.00
[INFO 2017-06-28 13:25:44,712 main.py:51] epoch 3378, training loss: 9553.48, average training loss: 9910.93, base loss: 14405.59
[INFO 2017-06-28 13:25:45,367 main.py:51] epoch 3379, training loss: 10472.35, average training loss: 9911.19, base loss: 14405.41
[INFO 2017-06-28 13:25:46,010 main.py:51] epoch 3380, training loss: 10164.08, average training loss: 9911.93, base loss: 14407.51
[INFO 2017-06-28 13:25:46,689 main.py:51] epoch 3381, training loss: 8792.47, average training loss: 9911.27, base loss: 14407.89
[INFO 2017-06-28 13:25:47,331 main.py:51] epoch 3382, training loss: 9999.94, average training loss: 9912.44, base loss: 14409.07
[INFO 2017-06-28 13:25:47,984 main.py:51] epoch 3383, training loss: 10235.02, average training loss: 9912.73, base loss: 14409.90
[INFO 2017-06-28 13:25:48,673 main.py:51] epoch 3384, training loss: 9026.96, average training loss: 9912.80, base loss: 14410.14
[INFO 2017-06-28 13:25:49,455 main.py:51] epoch 3385, training loss: 11352.20, average training loss: 9914.18, base loss: 14414.19
[INFO 2017-06-28 13:25:50,179 main.py:51] epoch 3386, training loss: 10520.92, average training loss: 9915.71, base loss: 14415.12
[INFO 2017-06-28 13:25:50,851 main.py:51] epoch 3387, training loss: 10318.27, average training loss: 9917.03, base loss: 14416.84
[INFO 2017-06-28 13:25:51,517 main.py:51] epoch 3388, training loss: 9708.76, average training loss: 9916.70, base loss: 14417.60
[INFO 2017-06-28 13:25:52,167 main.py:51] epoch 3389, training loss: 8890.29, average training loss: 9914.75, base loss: 14415.02
[INFO 2017-06-28 13:25:52,832 main.py:51] epoch 3390, training loss: 9027.15, average training loss: 9913.59, base loss: 14413.74
[INFO 2017-06-28 13:25:53,488 main.py:51] epoch 3391, training loss: 9890.93, average training loss: 9914.51, base loss: 14414.86
[INFO 2017-06-28 13:25:54,169 main.py:51] epoch 3392, training loss: 10106.30, average training loss: 9914.57, base loss: 14415.68
[INFO 2017-06-28 13:25:54,816 main.py:51] epoch 3393, training loss: 10853.31, average training loss: 9914.60, base loss: 14416.18
[INFO 2017-06-28 13:25:55,475 main.py:51] epoch 3394, training loss: 11295.95, average training loss: 9915.42, base loss: 14415.90
[INFO 2017-06-28 13:25:56,176 main.py:51] epoch 3395, training loss: 9646.90, average training loss: 9914.26, base loss: 14414.85
[INFO 2017-06-28 13:25:56,814 main.py:51] epoch 3396, training loss: 10485.37, average training loss: 9913.36, base loss: 14414.69
[INFO 2017-06-28 13:25:57,501 main.py:51] epoch 3397, training loss: 9029.43, average training loss: 9912.41, base loss: 14414.43
[INFO 2017-06-28 13:25:58,244 main.py:51] epoch 3398, training loss: 7959.33, average training loss: 9910.10, base loss: 14409.55
[INFO 2017-06-28 13:25:58,917 main.py:51] epoch 3399, training loss: 9927.70, average training loss: 9910.06, base loss: 14410.01
[INFO 2017-06-28 13:25:58,917 main.py:53] epoch 3399, testing
[INFO 2017-06-28 13:26:01,528 main.py:105] average testing loss: 11015.32, base loss: 15045.91
[INFO 2017-06-28 13:26:01,528 main.py:106] improve_loss: 4030.59, improve_percent: 0.27
[INFO 2017-06-28 13:26:01,529 main.py:76] current best improved percent: 0.28
[INFO 2017-06-28 13:26:02,196 main.py:51] epoch 3400, training loss: 9547.26, average training loss: 9909.86, base loss: 14409.11
[INFO 2017-06-28 13:26:02,857 main.py:51] epoch 3401, training loss: 9322.26, average training loss: 9910.46, base loss: 14409.32
[INFO 2017-06-28 13:26:03,519 main.py:51] epoch 3402, training loss: 10633.67, average training loss: 9910.30, base loss: 14409.53
[INFO 2017-06-28 13:26:04,230 main.py:51] epoch 3403, training loss: 9510.20, average training loss: 9908.45, base loss: 14405.89
[INFO 2017-06-28 13:26:04,894 main.py:51] epoch 3404, training loss: 9013.90, average training loss: 9907.89, base loss: 14405.29
[INFO 2017-06-28 13:26:05,559 main.py:51] epoch 3405, training loss: 8827.25, average training loss: 9906.45, base loss: 14404.81
[INFO 2017-06-28 13:26:06,285 main.py:51] epoch 3406, training loss: 8668.05, average training loss: 9905.42, base loss: 14403.42
[INFO 2017-06-28 13:26:06,953 main.py:51] epoch 3407, training loss: 9720.13, average training loss: 9906.11, base loss: 14405.50
[INFO 2017-06-28 13:26:07,625 main.py:51] epoch 3408, training loss: 9428.94, average training loss: 9906.60, base loss: 14406.16
[INFO 2017-06-28 13:26:08,266 main.py:51] epoch 3409, training loss: 10351.31, average training loss: 9906.77, base loss: 14406.94
[INFO 2017-06-28 13:26:08,921 main.py:51] epoch 3410, training loss: 10629.33, average training loss: 9908.72, base loss: 14409.13
[INFO 2017-06-28 13:26:09,576 main.py:51] epoch 3411, training loss: 9824.47, average training loss: 9908.67, base loss: 14409.70
[INFO 2017-06-28 13:26:10,248 main.py:51] epoch 3412, training loss: 11055.95, average training loss: 9908.88, base loss: 14410.25
[INFO 2017-06-28 13:26:10,894 main.py:51] epoch 3413, training loss: 8663.12, average training loss: 9907.68, base loss: 14408.84
[INFO 2017-06-28 13:26:11,540 main.py:51] epoch 3414, training loss: 10198.40, average training loss: 9908.74, base loss: 14412.13
[INFO 2017-06-28 13:26:12,185 main.py:51] epoch 3415, training loss: 9206.78, average training loss: 9907.59, base loss: 14410.96
[INFO 2017-06-28 13:26:12,846 main.py:51] epoch 3416, training loss: 9570.08, average training loss: 9907.55, base loss: 14410.51
[INFO 2017-06-28 13:26:13,501 main.py:51] epoch 3417, training loss: 10379.55, average training loss: 9909.39, base loss: 14412.43
[INFO 2017-06-28 13:26:14,154 main.py:51] epoch 3418, training loss: 9310.39, average training loss: 9907.15, base loss: 14408.42
[INFO 2017-06-28 13:26:14,801 main.py:51] epoch 3419, training loss: 10689.59, average training loss: 9907.37, base loss: 14407.11
[INFO 2017-06-28 13:26:15,446 main.py:51] epoch 3420, training loss: 9163.83, average training loss: 9905.74, base loss: 14406.15
[INFO 2017-06-28 13:26:16,096 main.py:51] epoch 3421, training loss: 9122.07, average training loss: 9905.77, base loss: 14406.21
[INFO 2017-06-28 13:26:16,732 main.py:51] epoch 3422, training loss: 8967.67, average training loss: 9904.87, base loss: 14404.61
[INFO 2017-06-28 13:26:17,391 main.py:51] epoch 3423, training loss: 10919.93, average training loss: 9906.34, base loss: 14405.95
[INFO 2017-06-28 13:26:18,079 main.py:51] epoch 3424, training loss: 8345.17, average training loss: 9903.60, base loss: 14403.76
[INFO 2017-06-28 13:26:18,766 main.py:51] epoch 3425, training loss: 10950.61, average training loss: 9903.97, base loss: 14404.95
[INFO 2017-06-28 13:26:19,429 main.py:51] epoch 3426, training loss: 9401.06, average training loss: 9904.33, base loss: 14406.05
[INFO 2017-06-28 13:26:20,109 main.py:51] epoch 3427, training loss: 10540.34, average training loss: 9903.84, base loss: 14405.92
[INFO 2017-06-28 13:26:20,767 main.py:51] epoch 3428, training loss: 8607.41, average training loss: 9901.83, base loss: 14404.36
[INFO 2017-06-28 13:26:21,428 main.py:51] epoch 3429, training loss: 9407.14, average training loss: 9901.82, base loss: 14403.62
[INFO 2017-06-28 13:26:22,103 main.py:51] epoch 3430, training loss: 8761.38, average training loss: 9899.74, base loss: 14401.13
[INFO 2017-06-28 13:26:22,770 main.py:51] epoch 3431, training loss: 9513.60, average training loss: 9898.95, base loss: 14400.40
[INFO 2017-06-28 13:26:23,426 main.py:51] epoch 3432, training loss: 9233.88, average training loss: 9897.73, base loss: 14399.14
[INFO 2017-06-28 13:26:24,082 main.py:51] epoch 3433, training loss: 9134.55, average training loss: 9895.78, base loss: 14397.43
[INFO 2017-06-28 13:26:24,738 main.py:51] epoch 3434, training loss: 10911.93, average training loss: 9896.17, base loss: 14396.74
[INFO 2017-06-28 13:26:25,388 main.py:51] epoch 3435, training loss: 10906.73, average training loss: 9895.17, base loss: 14394.46
[INFO 2017-06-28 13:26:26,033 main.py:51] epoch 3436, training loss: 7952.50, average training loss: 9892.64, base loss: 14391.06
[INFO 2017-06-28 13:26:26,703 main.py:51] epoch 3437, training loss: 10031.67, average training loss: 9892.24, base loss: 14390.55
[INFO 2017-06-28 13:26:27,354 main.py:51] epoch 3438, training loss: 8736.82, average training loss: 9891.97, base loss: 14390.01
[INFO 2017-06-28 13:26:28,031 main.py:51] epoch 3439, training loss: 8851.08, average training loss: 9891.45, base loss: 14390.07
[INFO 2017-06-28 13:26:28,693 main.py:51] epoch 3440, training loss: 9542.73, average training loss: 9891.78, base loss: 14390.46
[INFO 2017-06-28 13:26:29,340 main.py:51] epoch 3441, training loss: 7898.06, average training loss: 9889.88, base loss: 14387.85
[INFO 2017-06-28 13:26:30,000 main.py:51] epoch 3442, training loss: 10100.66, average training loss: 9889.71, base loss: 14388.94
[INFO 2017-06-28 13:26:30,658 main.py:51] epoch 3443, training loss: 8878.92, average training loss: 9889.42, base loss: 14388.84
[INFO 2017-06-28 13:26:31,325 main.py:51] epoch 3444, training loss: 9137.05, average training loss: 9886.57, base loss: 14385.31
[INFO 2017-06-28 13:26:31,981 main.py:51] epoch 3445, training loss: 10973.96, average training loss: 9888.53, base loss: 14388.45
[INFO 2017-06-28 13:26:32,646 main.py:51] epoch 3446, training loss: 9839.35, average training loss: 9888.20, base loss: 14388.87
[INFO 2017-06-28 13:26:33,292 main.py:51] epoch 3447, training loss: 9574.23, average training loss: 9886.82, base loss: 14387.26
[INFO 2017-06-28 13:26:33,946 main.py:51] epoch 3448, training loss: 10378.40, average training loss: 9888.03, base loss: 14388.62
[INFO 2017-06-28 13:26:34,597 main.py:51] epoch 3449, training loss: 8215.53, average training loss: 9885.26, base loss: 14385.02
[INFO 2017-06-28 13:26:35,247 main.py:51] epoch 3450, training loss: 8588.01, average training loss: 9884.35, base loss: 14384.43
[INFO 2017-06-28 13:26:35,895 main.py:51] epoch 3451, training loss: 9179.12, average training loss: 9885.41, base loss: 14386.88
[INFO 2017-06-28 13:26:36,566 main.py:51] epoch 3452, training loss: 10912.64, average training loss: 9885.86, base loss: 14388.89
[INFO 2017-06-28 13:26:37,234 main.py:51] epoch 3453, training loss: 10890.73, average training loss: 9886.35, base loss: 14389.41
[INFO 2017-06-28 13:26:37,884 main.py:51] epoch 3454, training loss: 9914.53, average training loss: 9885.50, base loss: 14389.37
[INFO 2017-06-28 13:26:38,528 main.py:51] epoch 3455, training loss: 10707.98, average training loss: 9886.86, base loss: 14391.65
[INFO 2017-06-28 13:26:39,190 main.py:51] epoch 3456, training loss: 11052.33, average training loss: 9889.12, base loss: 14394.84
[INFO 2017-06-28 13:26:39,834 main.py:51] epoch 3457, training loss: 9899.35, average training loss: 9887.81, base loss: 14396.31
[INFO 2017-06-28 13:26:40,488 main.py:51] epoch 3458, training loss: 9229.65, average training loss: 9887.06, base loss: 14394.99
[INFO 2017-06-28 13:26:41,147 main.py:51] epoch 3459, training loss: 10217.87, average training loss: 9886.61, base loss: 14394.62
[INFO 2017-06-28 13:26:41,822 main.py:51] epoch 3460, training loss: 11807.33, average training loss: 9889.43, base loss: 14397.64
[INFO 2017-06-28 13:26:42,481 main.py:51] epoch 3461, training loss: 9286.14, average training loss: 9888.51, base loss: 14396.34
[INFO 2017-06-28 13:26:43,147 main.py:51] epoch 3462, training loss: 10060.26, average training loss: 9887.66, base loss: 14394.33
[INFO 2017-06-28 13:26:43,822 main.py:51] epoch 3463, training loss: 11110.12, average training loss: 9888.92, base loss: 14396.12
[INFO 2017-06-28 13:26:44,482 main.py:51] epoch 3464, training loss: 8588.06, average training loss: 9887.80, base loss: 14395.27
[INFO 2017-06-28 13:26:45,163 main.py:51] epoch 3465, training loss: 10365.14, average training loss: 9888.63, base loss: 14397.60
[INFO 2017-06-28 13:26:45,826 main.py:51] epoch 3466, training loss: 9453.44, average training loss: 9887.94, base loss: 14397.63
[INFO 2017-06-28 13:26:46,476 main.py:51] epoch 3467, training loss: 9558.26, average training loss: 9889.15, base loss: 14400.06
[INFO 2017-06-28 13:26:47,158 main.py:51] epoch 3468, training loss: 9557.03, average training loss: 9889.76, base loss: 14400.87
[INFO 2017-06-28 13:26:47,822 main.py:51] epoch 3469, training loss: 9709.37, average training loss: 9888.01, base loss: 14398.74
[INFO 2017-06-28 13:26:48,528 main.py:51] epoch 3470, training loss: 8542.01, average training loss: 9887.12, base loss: 14399.61
[INFO 2017-06-28 13:26:49,186 main.py:51] epoch 3471, training loss: 10502.84, average training loss: 9887.88, base loss: 14398.86
[INFO 2017-06-28 13:26:49,821 main.py:51] epoch 3472, training loss: 9446.92, average training loss: 9887.34, base loss: 14397.00
[INFO 2017-06-28 13:26:50,493 main.py:51] epoch 3473, training loss: 10586.96, average training loss: 9888.26, base loss: 14398.41
[INFO 2017-06-28 13:26:51,154 main.py:51] epoch 3474, training loss: 10404.94, average training loss: 9888.89, base loss: 14400.94
[INFO 2017-06-28 13:26:51,830 main.py:51] epoch 3475, training loss: 9981.79, average training loss: 9887.27, base loss: 14400.60
[INFO 2017-06-28 13:26:52,506 main.py:51] epoch 3476, training loss: 10722.30, average training loss: 9887.18, base loss: 14400.95
[INFO 2017-06-28 13:26:53,179 main.py:51] epoch 3477, training loss: 9456.17, average training loss: 9885.77, base loss: 14399.02
[INFO 2017-06-28 13:26:53,852 main.py:51] epoch 3478, training loss: 10276.35, average training loss: 9885.54, base loss: 14397.58
[INFO 2017-06-28 13:26:54,504 main.py:51] epoch 3479, training loss: 12062.84, average training loss: 9888.09, base loss: 14400.57
[INFO 2017-06-28 13:26:55,157 main.py:51] epoch 3480, training loss: 9513.53, average training loss: 9888.15, base loss: 14401.33
[INFO 2017-06-28 13:26:55,807 main.py:51] epoch 3481, training loss: 9151.55, average training loss: 9887.89, base loss: 14401.09
[INFO 2017-06-28 13:26:56,456 main.py:51] epoch 3482, training loss: 8859.92, average training loss: 9886.12, base loss: 14398.86
[INFO 2017-06-28 13:26:57,117 main.py:51] epoch 3483, training loss: 8512.12, average training loss: 9883.44, base loss: 14394.94
[INFO 2017-06-28 13:26:57,792 main.py:51] epoch 3484, training loss: 9068.19, average training loss: 9882.19, base loss: 14392.81
[INFO 2017-06-28 13:26:58,453 main.py:51] epoch 3485, training loss: 9806.96, average training loss: 9882.32, base loss: 14392.58
[INFO 2017-06-28 13:26:59,102 main.py:51] epoch 3486, training loss: 8884.75, average training loss: 9881.95, base loss: 14391.79
[INFO 2017-06-28 13:26:59,743 main.py:51] epoch 3487, training loss: 9402.79, average training loss: 9881.83, base loss: 14391.39
[INFO 2017-06-28 13:27:00,401 main.py:51] epoch 3488, training loss: 9302.96, average training loss: 9881.10, base loss: 14389.98
[INFO 2017-06-28 13:27:01,039 main.py:51] epoch 3489, training loss: 10625.43, average training loss: 9881.50, base loss: 14388.82
[INFO 2017-06-28 13:27:01,688 main.py:51] epoch 3490, training loss: 11103.80, average training loss: 9881.48, base loss: 14391.86
[INFO 2017-06-28 13:27:02,382 main.py:51] epoch 3491, training loss: 10959.38, average training loss: 9881.37, base loss: 14391.37
[INFO 2017-06-28 13:27:03,057 main.py:51] epoch 3492, training loss: 9392.42, average training loss: 9881.28, base loss: 14391.97
[INFO 2017-06-28 13:27:03,709 main.py:51] epoch 3493, training loss: 10044.31, average training loss: 9882.22, base loss: 14392.70
[INFO 2017-06-28 13:27:04,384 main.py:51] epoch 3494, training loss: 10608.69, average training loss: 9883.47, base loss: 14393.62
[INFO 2017-06-28 13:27:05,038 main.py:51] epoch 3495, training loss: 8844.34, average training loss: 9882.37, base loss: 14391.82
[INFO 2017-06-28 13:27:05,708 main.py:51] epoch 3496, training loss: 9297.32, average training loss: 9880.75, base loss: 14390.83
[INFO 2017-06-28 13:27:06,362 main.py:51] epoch 3497, training loss: 11107.08, average training loss: 9881.74, base loss: 14393.71
[INFO 2017-06-28 13:27:07,021 main.py:51] epoch 3498, training loss: 8745.44, average training loss: 9880.17, base loss: 14391.39
[INFO 2017-06-28 13:27:07,688 main.py:51] epoch 3499, training loss: 8988.89, average training loss: 9879.17, base loss: 14388.86
[INFO 2017-06-28 13:27:07,689 main.py:53] epoch 3499, testing
[INFO 2017-06-28 13:27:10,275 main.py:105] average testing loss: 10719.50, base loss: 15204.07
[INFO 2017-06-28 13:27:10,275 main.py:106] improve_loss: 4484.57, improve_percent: 0.29
[INFO 2017-06-28 13:27:10,275 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 13:27:10,312 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:27:10,993 main.py:51] epoch 3500, training loss: 9878.14, average training loss: 9877.57, base loss: 14386.82
[INFO 2017-06-28 13:27:11,661 main.py:51] epoch 3501, training loss: 10031.60, average training loss: 9878.60, base loss: 14388.57
[INFO 2017-06-28 13:27:12,380 main.py:51] epoch 3502, training loss: 9788.28, average training loss: 9878.28, base loss: 14388.94
[INFO 2017-06-28 13:27:13,038 main.py:51] epoch 3503, training loss: 9455.57, average training loss: 9877.51, base loss: 14388.09
[INFO 2017-06-28 13:27:13,720 main.py:51] epoch 3504, training loss: 10243.45, average training loss: 9878.69, base loss: 14389.27
[INFO 2017-06-28 13:27:14,404 main.py:51] epoch 3505, training loss: 8487.33, average training loss: 9878.42, base loss: 14389.37
[INFO 2017-06-28 13:27:15,074 main.py:51] epoch 3506, training loss: 9274.22, average training loss: 9877.95, base loss: 14389.21
[INFO 2017-06-28 13:27:15,754 main.py:51] epoch 3507, training loss: 9239.62, average training loss: 9877.56, base loss: 14389.77
[INFO 2017-06-28 13:27:16,451 main.py:51] epoch 3508, training loss: 10114.73, average training loss: 9875.93, base loss: 14388.32
[INFO 2017-06-28 13:27:17,123 main.py:51] epoch 3509, training loss: 10596.57, average training loss: 9877.43, base loss: 14390.51
[INFO 2017-06-28 13:27:17,797 main.py:51] epoch 3510, training loss: 10128.60, average training loss: 9878.43, base loss: 14391.24
[INFO 2017-06-28 13:27:18,447 main.py:51] epoch 3511, training loss: 9140.33, average training loss: 9874.41, base loss: 14386.93
[INFO 2017-06-28 13:27:19,114 main.py:51] epoch 3512, training loss: 8451.49, average training loss: 9873.05, base loss: 14384.33
[INFO 2017-06-28 13:27:19,793 main.py:51] epoch 3513, training loss: 9134.50, average training loss: 9873.60, base loss: 14384.32
[INFO 2017-06-28 13:27:20,454 main.py:51] epoch 3514, training loss: 9293.12, average training loss: 9871.94, base loss: 14381.89
[INFO 2017-06-28 13:27:21,145 main.py:51] epoch 3515, training loss: 10426.14, average training loss: 9873.35, base loss: 14383.30
[INFO 2017-06-28 13:27:21,803 main.py:51] epoch 3516, training loss: 9371.50, average training loss: 9872.63, base loss: 14382.81
[INFO 2017-06-28 13:27:22,477 main.py:51] epoch 3517, training loss: 9875.88, average training loss: 9873.86, base loss: 14383.36
[INFO 2017-06-28 13:27:23,152 main.py:51] epoch 3518, training loss: 9251.96, average training loss: 9872.67, base loss: 14380.86
[INFO 2017-06-28 13:27:23,825 main.py:51] epoch 3519, training loss: 8841.99, average training loss: 9871.41, base loss: 14379.44
[INFO 2017-06-28 13:27:24,479 main.py:51] epoch 3520, training loss: 8806.03, average training loss: 9871.73, base loss: 14379.33
[INFO 2017-06-28 13:27:25,146 main.py:51] epoch 3521, training loss: 10354.97, average training loss: 9872.30, base loss: 14379.96
[INFO 2017-06-28 13:27:25,793 main.py:51] epoch 3522, training loss: 9605.02, average training loss: 9871.94, base loss: 14380.02
[INFO 2017-06-28 13:27:26,449 main.py:51] epoch 3523, training loss: 10210.00, average training loss: 9872.52, base loss: 14381.02
[INFO 2017-06-28 13:27:27,095 main.py:51] epoch 3524, training loss: 11516.75, average training loss: 9873.98, base loss: 14382.38
[INFO 2017-06-28 13:27:27,747 main.py:51] epoch 3525, training loss: 8210.83, average training loss: 9871.87, base loss: 14378.42
[INFO 2017-06-28 13:27:28,391 main.py:51] epoch 3526, training loss: 9657.94, average training loss: 9872.07, base loss: 14380.01
[INFO 2017-06-28 13:27:29,052 main.py:51] epoch 3527, training loss: 11830.21, average training loss: 9872.39, base loss: 14381.35
[INFO 2017-06-28 13:27:29,705 main.py:51] epoch 3528, training loss: 10617.57, average training loss: 9872.32, base loss: 14381.94
[INFO 2017-06-28 13:27:30,349 main.py:51] epoch 3529, training loss: 10154.11, average training loss: 9870.42, base loss: 14379.77
[INFO 2017-06-28 13:27:31,004 main.py:51] epoch 3530, training loss: 10538.42, average training loss: 9871.76, base loss: 14381.02
[INFO 2017-06-28 13:27:31,668 main.py:51] epoch 3531, training loss: 10143.41, average training loss: 9871.93, base loss: 14382.66
[INFO 2017-06-28 13:27:32,322 main.py:51] epoch 3532, training loss: 8906.35, average training loss: 9871.40, base loss: 14383.42
[INFO 2017-06-28 13:27:32,981 main.py:51] epoch 3533, training loss: 8729.51, average training loss: 9869.60, base loss: 14381.18
[INFO 2017-06-28 13:27:33,647 main.py:51] epoch 3534, training loss: 8599.29, average training loss: 9867.83, base loss: 14378.54
[INFO 2017-06-28 13:27:34,327 main.py:51] epoch 3535, training loss: 7779.26, average training loss: 9865.44, base loss: 14374.22
[INFO 2017-06-28 13:27:34,994 main.py:51] epoch 3536, training loss: 10486.77, average training loss: 9861.55, base loss: 14370.78
[INFO 2017-06-28 13:27:35,701 main.py:51] epoch 3537, training loss: 9294.60, average training loss: 9860.81, base loss: 14369.41
[INFO 2017-06-28 13:27:36,346 main.py:51] epoch 3538, training loss: 10162.48, average training loss: 9861.45, base loss: 14371.11
[INFO 2017-06-28 13:27:37,003 main.py:51] epoch 3539, training loss: 9297.06, average training loss: 9860.90, base loss: 14370.43
[INFO 2017-06-28 13:27:37,673 main.py:51] epoch 3540, training loss: 9123.15, average training loss: 9860.84, base loss: 14369.90
[INFO 2017-06-28 13:27:38,338 main.py:51] epoch 3541, training loss: 8772.67, average training loss: 9860.01, base loss: 14368.15
[INFO 2017-06-28 13:27:39,031 main.py:51] epoch 3542, training loss: 9293.75, average training loss: 9859.89, base loss: 14369.23
[INFO 2017-06-28 13:27:39,706 main.py:51] epoch 3543, training loss: 8469.49, average training loss: 9857.41, base loss: 14367.02
[INFO 2017-06-28 13:27:40,386 main.py:51] epoch 3544, training loss: 10300.59, average training loss: 9857.91, base loss: 14368.59
[INFO 2017-06-28 13:27:41,059 main.py:51] epoch 3545, training loss: 8878.28, average training loss: 9856.27, base loss: 14365.73
[INFO 2017-06-28 13:27:41,720 main.py:51] epoch 3546, training loss: 8633.87, average training loss: 9854.68, base loss: 14362.48
[INFO 2017-06-28 13:27:42,382 main.py:51] epoch 3547, training loss: 10884.96, average training loss: 9856.14, base loss: 14365.08
[INFO 2017-06-28 13:27:43,051 main.py:51] epoch 3548, training loss: 10541.19, average training loss: 9857.59, base loss: 14366.70
[INFO 2017-06-28 13:27:43,711 main.py:51] epoch 3549, training loss: 10638.64, average training loss: 9859.04, base loss: 14369.62
[INFO 2017-06-28 13:27:44,370 main.py:51] epoch 3550, training loss: 9650.08, average training loss: 9859.39, base loss: 14368.70
[INFO 2017-06-28 13:27:45,040 main.py:51] epoch 3551, training loss: 9790.01, average training loss: 9859.18, base loss: 14367.72
[INFO 2017-06-28 13:27:45,698 main.py:51] epoch 3552, training loss: 11199.37, average training loss: 9858.91, base loss: 14366.91
[INFO 2017-06-28 13:27:46,349 main.py:51] epoch 3553, training loss: 11016.21, average training loss: 9859.91, base loss: 14368.16
[INFO 2017-06-28 13:27:47,034 main.py:51] epoch 3554, training loss: 10300.75, average training loss: 9859.32, base loss: 14368.68
[INFO 2017-06-28 13:27:47,686 main.py:51] epoch 3555, training loss: 8488.95, average training loss: 9857.86, base loss: 14368.02
[INFO 2017-06-28 13:27:48,344 main.py:51] epoch 3556, training loss: 10395.66, average training loss: 9859.25, base loss: 14369.39
[INFO 2017-06-28 13:27:48,992 main.py:51] epoch 3557, training loss: 10408.52, average training loss: 9858.66, base loss: 14369.93
[INFO 2017-06-28 13:27:49,642 main.py:51] epoch 3558, training loss: 9335.51, average training loss: 9858.55, base loss: 14369.46
[INFO 2017-06-28 13:27:50,293 main.py:51] epoch 3559, training loss: 10154.30, average training loss: 9858.86, base loss: 14369.53
[INFO 2017-06-28 13:27:50,946 main.py:51] epoch 3560, training loss: 11087.56, average training loss: 9859.29, base loss: 14369.37
[INFO 2017-06-28 13:27:51,594 main.py:51] epoch 3561, training loss: 9436.50, average training loss: 9857.55, base loss: 14367.39
[INFO 2017-06-28 13:27:52,260 main.py:51] epoch 3562, training loss: 9364.17, average training loss: 9856.33, base loss: 14365.81
[INFO 2017-06-28 13:27:52,914 main.py:51] epoch 3563, training loss: 8898.45, average training loss: 9855.49, base loss: 14365.62
[INFO 2017-06-28 13:27:53,575 main.py:51] epoch 3564, training loss: 10658.62, average training loss: 9857.24, base loss: 14367.55
[INFO 2017-06-28 13:27:54,229 main.py:51] epoch 3565, training loss: 8931.74, average training loss: 9854.88, base loss: 14364.05
[INFO 2017-06-28 13:27:54,885 main.py:51] epoch 3566, training loss: 9749.77, average training loss: 9853.44, base loss: 14362.57
[INFO 2017-06-28 13:27:55,546 main.py:51] epoch 3567, training loss: 10518.53, average training loss: 9854.06, base loss: 14364.43
[INFO 2017-06-28 13:27:56,181 main.py:51] epoch 3568, training loss: 9393.15, average training loss: 9852.29, base loss: 14360.94
[INFO 2017-06-28 13:27:56,854 main.py:51] epoch 3569, training loss: 8685.65, average training loss: 9850.28, base loss: 14358.37
[INFO 2017-06-28 13:27:57,506 main.py:51] epoch 3570, training loss: 9370.08, average training loss: 9850.67, base loss: 14359.23
[INFO 2017-06-28 13:27:58,151 main.py:51] epoch 3571, training loss: 8399.04, average training loss: 9850.19, base loss: 14356.25
[INFO 2017-06-28 13:27:58,808 main.py:51] epoch 3572, training loss: 8762.04, average training loss: 9848.24, base loss: 14356.59
[INFO 2017-06-28 13:27:59,478 main.py:51] epoch 3573, training loss: 10378.71, average training loss: 9848.65, base loss: 14356.93
[INFO 2017-06-28 13:28:00,124 main.py:51] epoch 3574, training loss: 9880.40, average training loss: 9847.65, base loss: 14356.21
[INFO 2017-06-28 13:28:00,763 main.py:51] epoch 3575, training loss: 11686.69, average training loss: 9847.63, base loss: 14356.65
[INFO 2017-06-28 13:28:01,408 main.py:51] epoch 3576, training loss: 9089.57, average training loss: 9847.24, base loss: 14356.52
[INFO 2017-06-28 13:28:02,047 main.py:51] epoch 3577, training loss: 9639.13, average training loss: 9847.07, base loss: 14356.62
[INFO 2017-06-28 13:28:02,699 main.py:51] epoch 3578, training loss: 9692.15, average training loss: 9846.63, base loss: 14356.20
[INFO 2017-06-28 13:28:03,373 main.py:51] epoch 3579, training loss: 9908.49, average training loss: 9846.41, base loss: 14356.93
[INFO 2017-06-28 13:28:04,028 main.py:51] epoch 3580, training loss: 9473.65, average training loss: 9845.34, base loss: 14357.55
[INFO 2017-06-28 13:28:04,689 main.py:51] epoch 3581, training loss: 9881.55, average training loss: 9845.52, base loss: 14358.87
[INFO 2017-06-28 13:28:05,366 main.py:51] epoch 3582, training loss: 9774.88, average training loss: 9846.06, base loss: 14359.01
[INFO 2017-06-28 13:28:06,023 main.py:51] epoch 3583, training loss: 9792.37, average training loss: 9847.26, base loss: 14361.15
[INFO 2017-06-28 13:28:06,684 main.py:51] epoch 3584, training loss: 8708.84, average training loss: 9845.22, base loss: 14359.36
[INFO 2017-06-28 13:28:07,319 main.py:51] epoch 3585, training loss: 10465.43, average training loss: 9844.36, base loss: 14358.79
[INFO 2017-06-28 13:28:07,985 main.py:51] epoch 3586, training loss: 9640.63, average training loss: 9843.94, base loss: 14358.37
[INFO 2017-06-28 13:28:08,678 main.py:51] epoch 3587, training loss: 9313.46, average training loss: 9841.60, base loss: 14354.40
[INFO 2017-06-28 13:28:09,321 main.py:51] epoch 3588, training loss: 9602.03, average training loss: 9840.54, base loss: 14353.82
[INFO 2017-06-28 13:28:09,982 main.py:51] epoch 3589, training loss: 8819.66, average training loss: 9838.40, base loss: 14350.89
[INFO 2017-06-28 13:28:10,652 main.py:51] epoch 3590, training loss: 10142.33, average training loss: 9838.98, base loss: 14351.60
[INFO 2017-06-28 13:28:11,313 main.py:51] epoch 3591, training loss: 9956.27, average training loss: 9839.39, base loss: 14353.22
[INFO 2017-06-28 13:28:11,973 main.py:51] epoch 3592, training loss: 9405.51, average training loss: 9838.66, base loss: 14352.30
[INFO 2017-06-28 13:28:12,645 main.py:51] epoch 3593, training loss: 10415.96, average training loss: 9839.73, base loss: 14355.15
[INFO 2017-06-28 13:28:13,318 main.py:51] epoch 3594, training loss: 8731.42, average training loss: 9839.07, base loss: 14354.13
[INFO 2017-06-28 13:28:13,997 main.py:51] epoch 3595, training loss: 10732.58, average training loss: 9838.95, base loss: 14354.86
[INFO 2017-06-28 13:28:14,645 main.py:51] epoch 3596, training loss: 10153.64, average training loss: 9838.23, base loss: 14354.95
[INFO 2017-06-28 13:28:15,326 main.py:51] epoch 3597, training loss: 9369.61, average training loss: 9838.30, base loss: 14356.06
[INFO 2017-06-28 13:28:16,006 main.py:51] epoch 3598, training loss: 10202.55, average training loss: 9836.22, base loss: 14354.99
[INFO 2017-06-28 13:28:16,676 main.py:51] epoch 3599, training loss: 8719.30, average training loss: 9836.16, base loss: 14355.72
[INFO 2017-06-28 13:28:16,677 main.py:53] epoch 3599, testing
[INFO 2017-06-28 13:28:19,333 main.py:105] average testing loss: 11047.01, base loss: 15271.71
[INFO 2017-06-28 13:28:19,333 main.py:106] improve_loss: 4224.70, improve_percent: 0.28
[INFO 2017-06-28 13:28:19,334 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:28:19,998 main.py:51] epoch 3600, training loss: 11255.59, average training loss: 9837.06, base loss: 14356.57
[INFO 2017-06-28 13:28:20,657 main.py:51] epoch 3601, training loss: 9024.52, average training loss: 9835.71, base loss: 14355.57
[INFO 2017-06-28 13:28:21,299 main.py:51] epoch 3602, training loss: 8828.84, average training loss: 9834.49, base loss: 14354.24
[INFO 2017-06-28 13:28:21,965 main.py:51] epoch 3603, training loss: 10058.29, average training loss: 9834.65, base loss: 14355.25
[INFO 2017-06-28 13:28:22,605 main.py:51] epoch 3604, training loss: 9057.58, average training loss: 9833.11, base loss: 14352.34
[INFO 2017-06-28 13:28:23,259 main.py:51] epoch 3605, training loss: 9388.21, average training loss: 9832.73, base loss: 14353.10
[INFO 2017-06-28 13:28:23,928 main.py:51] epoch 3606, training loss: 10458.69, average training loss: 9834.55, base loss: 14354.70
[INFO 2017-06-28 13:28:24,589 main.py:51] epoch 3607, training loss: 10762.58, average training loss: 9836.76, base loss: 14357.99
[INFO 2017-06-28 13:28:25,231 main.py:51] epoch 3608, training loss: 8852.36, average training loss: 9836.40, base loss: 14358.15
[INFO 2017-06-28 13:28:25,878 main.py:51] epoch 3609, training loss: 10449.88, average training loss: 9834.87, base loss: 14356.76
[INFO 2017-06-28 13:28:26,552 main.py:51] epoch 3610, training loss: 10104.76, average training loss: 9836.34, base loss: 14358.85
[INFO 2017-06-28 13:28:27,212 main.py:51] epoch 3611, training loss: 11309.88, average training loss: 9837.11, base loss: 14361.88
[INFO 2017-06-28 13:28:27,892 main.py:51] epoch 3612, training loss: 9603.08, average training loss: 9837.44, base loss: 14362.39
[INFO 2017-06-28 13:28:28,541 main.py:51] epoch 3613, training loss: 9870.01, average training loss: 9836.77, base loss: 14361.93
[INFO 2017-06-28 13:28:29,231 main.py:51] epoch 3614, training loss: 11599.56, average training loss: 9839.23, base loss: 14364.16
[INFO 2017-06-28 13:28:29,879 main.py:51] epoch 3615, training loss: 9720.55, average training loss: 9838.24, base loss: 14362.71
[INFO 2017-06-28 13:28:30,535 main.py:51] epoch 3616, training loss: 9304.11, average training loss: 9838.10, base loss: 14362.90
[INFO 2017-06-28 13:28:31,210 main.py:51] epoch 3617, training loss: 11493.16, average training loss: 9840.76, base loss: 14365.40
[INFO 2017-06-28 13:28:31,877 main.py:51] epoch 3618, training loss: 8837.66, average training loss: 9838.59, base loss: 14363.35
[INFO 2017-06-28 13:28:32,548 main.py:51] epoch 3619, training loss: 9019.35, average training loss: 9837.68, base loss: 14361.94
[INFO 2017-06-28 13:28:33,241 main.py:51] epoch 3620, training loss: 9669.35, average training loss: 9836.94, base loss: 14362.32
[INFO 2017-06-28 13:28:33,908 main.py:51] epoch 3621, training loss: 8183.09, average training loss: 9836.52, base loss: 14362.39
[INFO 2017-06-28 13:28:34,567 main.py:51] epoch 3622, training loss: 9278.83, average training loss: 9835.79, base loss: 14360.35
[INFO 2017-06-28 13:28:35,222 main.py:51] epoch 3623, training loss: 9777.77, average training loss: 9834.52, base loss: 14358.75
[INFO 2017-06-28 13:28:35,848 main.py:51] epoch 3624, training loss: 9817.83, average training loss: 9834.37, base loss: 14357.04
[INFO 2017-06-28 13:28:36,508 main.py:51] epoch 3625, training loss: 10140.49, average training loss: 9834.92, base loss: 14358.39
[INFO 2017-06-28 13:28:37,171 main.py:51] epoch 3626, training loss: 10161.48, average training loss: 9835.56, base loss: 14359.52
[INFO 2017-06-28 13:28:37,806 main.py:51] epoch 3627, training loss: 9046.57, average training loss: 9834.25, base loss: 14359.31
[INFO 2017-06-28 13:28:38,475 main.py:51] epoch 3628, training loss: 9505.50, average training loss: 9833.98, base loss: 14359.18
[INFO 2017-06-28 13:28:39,126 main.py:51] epoch 3629, training loss: 9591.65, average training loss: 9833.75, base loss: 14359.99
[INFO 2017-06-28 13:28:39,782 main.py:51] epoch 3630, training loss: 8894.37, average training loss: 9832.53, base loss: 14357.55
[INFO 2017-06-28 13:28:40,433 main.py:51] epoch 3631, training loss: 11184.12, average training loss: 9834.88, base loss: 14361.05
[INFO 2017-06-28 13:28:41,101 main.py:51] epoch 3632, training loss: 9478.43, average training loss: 9833.24, base loss: 14359.58
[INFO 2017-06-28 13:28:41,759 main.py:51] epoch 3633, training loss: 9938.18, average training loss: 9832.68, base loss: 14358.09
[INFO 2017-06-28 13:28:42,425 main.py:51] epoch 3634, training loss: 10083.24, average training loss: 9832.14, base loss: 14357.88
[INFO 2017-06-28 13:28:43,079 main.py:51] epoch 3635, training loss: 8961.59, average training loss: 9830.82, base loss: 14355.38
[INFO 2017-06-28 13:28:43,754 main.py:51] epoch 3636, training loss: 8123.56, average training loss: 9829.00, base loss: 14353.36
[INFO 2017-06-28 13:28:44,429 main.py:51] epoch 3637, training loss: 9762.03, average training loss: 9828.50, base loss: 14352.71
[INFO 2017-06-28 13:28:45,122 main.py:51] epoch 3638, training loss: 9812.03, average training loss: 9828.80, base loss: 14354.00
[INFO 2017-06-28 13:28:45,836 main.py:51] epoch 3639, training loss: 9692.47, average training loss: 9827.78, base loss: 14354.21
[INFO 2017-06-28 13:28:46,509 main.py:51] epoch 3640, training loss: 11820.81, average training loss: 9829.44, base loss: 14355.63
[INFO 2017-06-28 13:28:47,178 main.py:51] epoch 3641, training loss: 10169.21, average training loss: 9829.80, base loss: 14356.46
[INFO 2017-06-28 13:28:47,831 main.py:51] epoch 3642, training loss: 9125.13, average training loss: 9828.35, base loss: 14353.74
[INFO 2017-06-28 13:28:48,487 main.py:51] epoch 3643, training loss: 9632.76, average training loss: 9829.41, base loss: 14356.50
[INFO 2017-06-28 13:28:49,145 main.py:51] epoch 3644, training loss: 10133.58, average training loss: 9829.80, base loss: 14359.14
[INFO 2017-06-28 13:28:49,803 main.py:51] epoch 3645, training loss: 10450.30, average training loss: 9830.53, base loss: 14361.07
[INFO 2017-06-28 13:28:50,471 main.py:51] epoch 3646, training loss: 9065.21, average training loss: 9829.82, base loss: 14359.92
[INFO 2017-06-28 13:28:51,118 main.py:51] epoch 3647, training loss: 9425.87, average training loss: 9829.85, base loss: 14360.80
[INFO 2017-06-28 13:28:51,785 main.py:51] epoch 3648, training loss: 8893.02, average training loss: 9829.10, base loss: 14359.69
[INFO 2017-06-28 13:28:52,449 main.py:51] epoch 3649, training loss: 8909.64, average training loss: 9829.38, base loss: 14360.24
[INFO 2017-06-28 13:28:53,093 main.py:51] epoch 3650, training loss: 11212.65, average training loss: 9829.09, base loss: 14360.52
[INFO 2017-06-28 13:28:53,740 main.py:51] epoch 3651, training loss: 9211.94, average training loss: 9826.32, base loss: 14357.26
[INFO 2017-06-28 13:28:54,426 main.py:51] epoch 3652, training loss: 9166.00, average training loss: 9826.91, base loss: 14359.42
[INFO 2017-06-28 13:28:55,090 main.py:51] epoch 3653, training loss: 8776.18, average training loss: 9825.42, base loss: 14357.14
[INFO 2017-06-28 13:28:55,761 main.py:51] epoch 3654, training loss: 9830.54, average training loss: 9825.58, base loss: 14357.21
[INFO 2017-06-28 13:28:56,453 main.py:51] epoch 3655, training loss: 9720.17, average training loss: 9825.75, base loss: 14359.07
[INFO 2017-06-28 13:28:57,126 main.py:51] epoch 3656, training loss: 9559.87, average training loss: 9825.59, base loss: 14359.51
[INFO 2017-06-28 13:28:57,796 main.py:51] epoch 3657, training loss: 8314.47, average training loss: 9825.35, base loss: 14360.07
[INFO 2017-06-28 13:28:58,439 main.py:51] epoch 3658, training loss: 10512.05, average training loss: 9825.11, base loss: 14359.79
[INFO 2017-06-28 13:28:59,133 main.py:51] epoch 3659, training loss: 9845.40, average training loss: 9826.43, base loss: 14361.08
[INFO 2017-06-28 13:28:59,844 main.py:51] epoch 3660, training loss: 9665.89, average training loss: 9825.76, base loss: 14360.55
[INFO 2017-06-28 13:29:00,492 main.py:51] epoch 3661, training loss: 8806.23, average training loss: 9824.61, base loss: 14358.66
[INFO 2017-06-28 13:29:01,162 main.py:51] epoch 3662, training loss: 9402.47, average training loss: 9823.26, base loss: 14357.62
[INFO 2017-06-28 13:29:01,836 main.py:51] epoch 3663, training loss: 8843.03, average training loss: 9822.14, base loss: 14356.16
[INFO 2017-06-28 13:29:02,500 main.py:51] epoch 3664, training loss: 10027.58, average training loss: 9822.46, base loss: 14355.43
[INFO 2017-06-28 13:29:03,157 main.py:51] epoch 3665, training loss: 10099.12, average training loss: 9821.77, base loss: 14355.00
[INFO 2017-06-28 13:29:03,830 main.py:51] epoch 3666, training loss: 9918.63, average training loss: 9822.35, base loss: 14355.45
[INFO 2017-06-28 13:29:04,482 main.py:51] epoch 3667, training loss: 9724.38, average training loss: 9822.78, base loss: 14356.53
[INFO 2017-06-28 13:29:05,147 main.py:51] epoch 3668, training loss: 9344.46, average training loss: 9823.13, base loss: 14358.57
[INFO 2017-06-28 13:29:05,807 main.py:51] epoch 3669, training loss: 11593.99, average training loss: 9826.02, base loss: 14362.83
[INFO 2017-06-28 13:29:06,491 main.py:51] epoch 3670, training loss: 9075.74, average training loss: 9823.61, base loss: 14361.84
[INFO 2017-06-28 13:29:07,147 main.py:51] epoch 3671, training loss: 10564.31, average training loss: 9824.10, base loss: 14360.37
[INFO 2017-06-28 13:29:07,795 main.py:51] epoch 3672, training loss: 8915.60, average training loss: 9823.39, base loss: 14359.46
[INFO 2017-06-28 13:29:08,454 main.py:51] epoch 3673, training loss: 9312.05, average training loss: 9821.49, base loss: 14356.85
[INFO 2017-06-28 13:29:09,121 main.py:51] epoch 3674, training loss: 8966.26, average training loss: 9821.24, base loss: 14355.74
[INFO 2017-06-28 13:29:09,793 main.py:51] epoch 3675, training loss: 9441.60, average training loss: 9821.35, base loss: 14357.12
[INFO 2017-06-28 13:29:10,451 main.py:51] epoch 3676, training loss: 9052.47, average training loss: 9819.93, base loss: 14355.95
[INFO 2017-06-28 13:29:11,111 main.py:51] epoch 3677, training loss: 9374.62, average training loss: 9819.23, base loss: 14354.61
[INFO 2017-06-28 13:29:11,773 main.py:51] epoch 3678, training loss: 10499.03, average training loss: 9820.08, base loss: 14354.86
[INFO 2017-06-28 13:29:12,452 main.py:51] epoch 3679, training loss: 11630.70, average training loss: 9821.85, base loss: 14359.87
[INFO 2017-06-28 13:29:13,118 main.py:51] epoch 3680, training loss: 9568.29, average training loss: 9822.06, base loss: 14361.57
[INFO 2017-06-28 13:29:13,761 main.py:51] epoch 3681, training loss: 9471.62, average training loss: 9821.44, base loss: 14361.06
[INFO 2017-06-28 13:29:14,412 main.py:51] epoch 3682, training loss: 10918.21, average training loss: 9822.98, base loss: 14362.51
[INFO 2017-06-28 13:29:15,063 main.py:51] epoch 3683, training loss: 9239.77, average training loss: 9821.77, base loss: 14360.34
[INFO 2017-06-28 13:29:15,725 main.py:51] epoch 3684, training loss: 9325.70, average training loss: 9822.29, base loss: 14359.52
[INFO 2017-06-28 13:29:16,384 main.py:51] epoch 3685, training loss: 8908.32, average training loss: 9823.10, base loss: 14360.54
[INFO 2017-06-28 13:29:17,030 main.py:51] epoch 3686, training loss: 9637.94, average training loss: 9823.40, base loss: 14359.68
[INFO 2017-06-28 13:29:17,680 main.py:51] epoch 3687, training loss: 10251.59, average training loss: 9823.55, base loss: 14361.10
[INFO 2017-06-28 13:29:18,329 main.py:51] epoch 3688, training loss: 12436.52, average training loss: 9827.16, base loss: 14369.49
[INFO 2017-06-28 13:29:18,993 main.py:51] epoch 3689, training loss: 8323.97, average training loss: 9825.56, base loss: 14367.88
[INFO 2017-06-28 13:29:19,672 main.py:51] epoch 3690, training loss: 11339.37, average training loss: 9828.02, base loss: 14371.43
[INFO 2017-06-28 13:29:20,335 main.py:51] epoch 3691, training loss: 9880.52, average training loss: 9826.54, base loss: 14367.86
[INFO 2017-06-28 13:29:21,011 main.py:51] epoch 3692, training loss: 10126.55, average training loss: 9827.17, base loss: 14368.68
[INFO 2017-06-28 13:29:21,688 main.py:51] epoch 3693, training loss: 9699.38, average training loss: 9826.32, base loss: 14367.44
[INFO 2017-06-28 13:29:22,337 main.py:51] epoch 3694, training loss: 9300.21, average training loss: 9826.02, base loss: 14367.23
[INFO 2017-06-28 13:29:23,006 main.py:51] epoch 3695, training loss: 11655.00, average training loss: 9829.04, base loss: 14371.90
[INFO 2017-06-28 13:29:23,683 main.py:51] epoch 3696, training loss: 9307.63, average training loss: 9828.49, base loss: 14369.57
[INFO 2017-06-28 13:29:24,339 main.py:51] epoch 3697, training loss: 8712.66, average training loss: 9828.38, base loss: 14369.14
[INFO 2017-06-28 13:29:24,997 main.py:51] epoch 3698, training loss: 12594.80, average training loss: 9829.55, base loss: 14370.60
[INFO 2017-06-28 13:29:25,638 main.py:51] epoch 3699, training loss: 10727.18, average training loss: 9830.98, base loss: 14372.38
[INFO 2017-06-28 13:29:25,638 main.py:53] epoch 3699, testing
[INFO 2017-06-28 13:29:28,239 main.py:105] average testing loss: 10043.42, base loss: 14152.64
[INFO 2017-06-28 13:29:28,239 main.py:106] improve_loss: 4109.23, improve_percent: 0.29
[INFO 2017-06-28 13:29:28,240 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:29:28,903 main.py:51] epoch 3700, training loss: 9353.33, average training loss: 9829.63, base loss: 14371.48
[INFO 2017-06-28 13:29:29,560 main.py:51] epoch 3701, training loss: 10824.71, average training loss: 9831.64, base loss: 14374.91
[INFO 2017-06-28 13:29:30,216 main.py:51] epoch 3702, training loss: 11644.14, average training loss: 9832.45, base loss: 14375.22
[INFO 2017-06-28 13:29:30,889 main.py:51] epoch 3703, training loss: 9563.01, average training loss: 9831.56, base loss: 14375.36
[INFO 2017-06-28 13:29:31,560 main.py:51] epoch 3704, training loss: 8945.99, average training loss: 9829.66, base loss: 14370.89
[INFO 2017-06-28 13:29:32,234 main.py:51] epoch 3705, training loss: 9138.92, average training loss: 9827.26, base loss: 14368.81
[INFO 2017-06-28 13:29:32,914 main.py:51] epoch 3706, training loss: 9785.19, average training loss: 9826.15, base loss: 14368.12
[INFO 2017-06-28 13:29:33,574 main.py:51] epoch 3707, training loss: 9903.99, average training loss: 9826.45, base loss: 14366.37
[INFO 2017-06-28 13:29:34,233 main.py:51] epoch 3708, training loss: 11650.49, average training loss: 9828.65, base loss: 14368.38
[INFO 2017-06-28 13:29:34,879 main.py:51] epoch 3709, training loss: 9721.37, average training loss: 9828.77, base loss: 14368.61
[INFO 2017-06-28 13:29:35,551 main.py:51] epoch 3710, training loss: 9872.66, average training loss: 9829.09, base loss: 14367.26
[INFO 2017-06-28 13:29:36,234 main.py:51] epoch 3711, training loss: 10504.96, average training loss: 9829.84, base loss: 14366.78
[INFO 2017-06-28 13:29:36,886 main.py:51] epoch 3712, training loss: 9474.13, average training loss: 9829.41, base loss: 14368.06
[INFO 2017-06-28 13:29:37,526 main.py:51] epoch 3713, training loss: 8640.29, average training loss: 9828.24, base loss: 14365.64
[INFO 2017-06-28 13:29:38,197 main.py:51] epoch 3714, training loss: 10076.93, average training loss: 9828.42, base loss: 14365.30
[INFO 2017-06-28 13:29:38,865 main.py:51] epoch 3715, training loss: 9441.42, average training loss: 9828.29, base loss: 14365.29
[INFO 2017-06-28 13:29:39,518 main.py:51] epoch 3716, training loss: 8419.90, average training loss: 9826.23, base loss: 14362.55
[INFO 2017-06-28 13:29:40,181 main.py:51] epoch 3717, training loss: 8703.13, average training loss: 9824.32, base loss: 14359.54
[INFO 2017-06-28 13:29:40,853 main.py:51] epoch 3718, training loss: 10035.18, average training loss: 9825.16, base loss: 14360.69
[INFO 2017-06-28 13:29:41,501 main.py:51] epoch 3719, training loss: 9060.64, average training loss: 9824.38, base loss: 14362.08
[INFO 2017-06-28 13:29:42,179 main.py:51] epoch 3720, training loss: 9123.54, average training loss: 9823.09, base loss: 14359.87
[INFO 2017-06-28 13:29:42,850 main.py:51] epoch 3721, training loss: 8585.63, average training loss: 9820.07, base loss: 14356.16
[INFO 2017-06-28 13:29:43,522 main.py:51] epoch 3722, training loss: 10648.41, average training loss: 9821.25, base loss: 14356.81
[INFO 2017-06-28 13:29:44,160 main.py:51] epoch 3723, training loss: 8817.54, average training loss: 9821.49, base loss: 14356.60
[INFO 2017-06-28 13:29:44,837 main.py:51] epoch 3724, training loss: 9212.69, average training loss: 9819.86, base loss: 14354.50
[INFO 2017-06-28 13:29:45,493 main.py:51] epoch 3725, training loss: 9350.16, average training loss: 9818.73, base loss: 14353.78
[INFO 2017-06-28 13:29:46,149 main.py:51] epoch 3726, training loss: 10676.54, average training loss: 9819.77, base loss: 14357.46
[INFO 2017-06-28 13:29:46,790 main.py:51] epoch 3727, training loss: 9868.47, average training loss: 9819.06, base loss: 14357.07
[INFO 2017-06-28 13:29:47,446 main.py:51] epoch 3728, training loss: 11381.31, average training loss: 9821.90, base loss: 14362.29
[INFO 2017-06-28 13:29:48,086 main.py:51] epoch 3729, training loss: 11108.48, average training loss: 9823.40, base loss: 14364.53
[INFO 2017-06-28 13:29:48,769 main.py:51] epoch 3730, training loss: 9466.64, average training loss: 9822.17, base loss: 14363.36
[INFO 2017-06-28 13:29:49,433 main.py:51] epoch 3731, training loss: 9768.87, average training loss: 9821.63, base loss: 14364.05
[INFO 2017-06-28 13:29:50,103 main.py:51] epoch 3732, training loss: 10451.34, average training loss: 9822.47, base loss: 14365.97
[INFO 2017-06-28 13:29:50,774 main.py:51] epoch 3733, training loss: 10413.34, average training loss: 9824.03, base loss: 14366.84
[INFO 2017-06-28 13:29:51,454 main.py:51] epoch 3734, training loss: 10362.45, average training loss: 9826.27, base loss: 14369.57
[INFO 2017-06-28 13:29:52,112 main.py:51] epoch 3735, training loss: 8674.57, average training loss: 9824.55, base loss: 14368.14
[INFO 2017-06-28 13:29:52,794 main.py:51] epoch 3736, training loss: 10823.80, average training loss: 9824.94, base loss: 14368.87
[INFO 2017-06-28 13:29:53,465 main.py:51] epoch 3737, training loss: 10298.56, average training loss: 9826.52, base loss: 14370.46
[INFO 2017-06-28 13:29:54,149 main.py:51] epoch 3738, training loss: 8415.50, average training loss: 9824.56, base loss: 14368.68
[INFO 2017-06-28 13:29:54,812 main.py:51] epoch 3739, training loss: 10643.33, average training loss: 9824.17, base loss: 14367.02
[INFO 2017-06-28 13:29:55,467 main.py:51] epoch 3740, training loss: 8226.36, average training loss: 9822.43, base loss: 14365.75
[INFO 2017-06-28 13:29:56,123 main.py:51] epoch 3741, training loss: 9733.65, average training loss: 9823.20, base loss: 14366.35
[INFO 2017-06-28 13:29:56,797 main.py:51] epoch 3742, training loss: 10385.91, average training loss: 9822.48, base loss: 14365.25
[INFO 2017-06-28 13:29:57,454 main.py:51] epoch 3743, training loss: 11532.22, average training loss: 9823.62, base loss: 14367.21
[INFO 2017-06-28 13:29:58,096 main.py:51] epoch 3744, training loss: 10181.40, average training loss: 9823.78, base loss: 14366.08
[INFO 2017-06-28 13:29:58,774 main.py:51] epoch 3745, training loss: 9999.97, average training loss: 9823.54, base loss: 14366.02
[INFO 2017-06-28 13:29:59,447 main.py:51] epoch 3746, training loss: 9668.88, average training loss: 9824.02, base loss: 14368.50
[INFO 2017-06-28 13:30:00,121 main.py:51] epoch 3747, training loss: 10219.67, average training loss: 9824.16, base loss: 14369.17
[INFO 2017-06-28 13:30:00,776 main.py:51] epoch 3748, training loss: 9907.53, average training loss: 9824.01, base loss: 14369.15
[INFO 2017-06-28 13:30:01,447 main.py:51] epoch 3749, training loss: 8592.12, average training loss: 9823.35, base loss: 14368.27
[INFO 2017-06-28 13:30:02,105 main.py:51] epoch 3750, training loss: 9089.63, average training loss: 9821.71, base loss: 14366.17
[INFO 2017-06-28 13:30:02,766 main.py:51] epoch 3751, training loss: 9390.13, average training loss: 9821.82, base loss: 14365.71
[INFO 2017-06-28 13:30:03,434 main.py:51] epoch 3752, training loss: 10254.04, average training loss: 9823.55, base loss: 14366.93
[INFO 2017-06-28 13:30:04,082 main.py:51] epoch 3753, training loss: 11331.64, average training loss: 9824.17, base loss: 14368.67
[INFO 2017-06-28 13:30:04,726 main.py:51] epoch 3754, training loss: 9235.53, average training loss: 9824.35, base loss: 14368.26
[INFO 2017-06-28 13:30:05,401 main.py:51] epoch 3755, training loss: 9183.30, average training loss: 9823.77, base loss: 14369.02
[INFO 2017-06-28 13:30:06,075 main.py:51] epoch 3756, training loss: 11961.88, average training loss: 9825.28, base loss: 14370.83
[INFO 2017-06-28 13:30:06,736 main.py:51] epoch 3757, training loss: 8618.47, average training loss: 9823.86, base loss: 14367.47
[INFO 2017-06-28 13:30:07,413 main.py:51] epoch 3758, training loss: 9350.67, average training loss: 9823.40, base loss: 14367.51
[INFO 2017-06-28 13:30:08,066 main.py:51] epoch 3759, training loss: 9012.62, average training loss: 9822.41, base loss: 14367.25
[INFO 2017-06-28 13:30:08,710 main.py:51] epoch 3760, training loss: 10090.59, average training loss: 9824.50, base loss: 14371.96
[INFO 2017-06-28 13:30:09,361 main.py:51] epoch 3761, training loss: 9865.69, average training loss: 9822.90, base loss: 14370.32
[INFO 2017-06-28 13:30:10,004 main.py:51] epoch 3762, training loss: 9634.34, average training loss: 9822.45, base loss: 14370.16
[INFO 2017-06-28 13:30:10,664 main.py:51] epoch 3763, training loss: 9355.55, average training loss: 9822.07, base loss: 14367.86
[INFO 2017-06-28 13:30:11,342 main.py:51] epoch 3764, training loss: 9767.00, average training loss: 9822.56, base loss: 14370.99
[INFO 2017-06-28 13:30:11,991 main.py:51] epoch 3765, training loss: 10051.16, average training loss: 9823.27, base loss: 14373.46
[INFO 2017-06-28 13:30:12,663 main.py:51] epoch 3766, training loss: 8942.61, average training loss: 9823.10, base loss: 14373.22
[INFO 2017-06-28 13:30:13,329 main.py:51] epoch 3767, training loss: 10849.92, average training loss: 9823.51, base loss: 14375.58
[INFO 2017-06-28 13:30:13,982 main.py:51] epoch 3768, training loss: 8734.39, average training loss: 9822.93, base loss: 14375.91
[INFO 2017-06-28 13:30:14,644 main.py:51] epoch 3769, training loss: 9838.38, average training loss: 9822.19, base loss: 14373.93
[INFO 2017-06-28 13:30:15,324 main.py:51] epoch 3770, training loss: 10100.05, average training loss: 9822.59, base loss: 14374.20
[INFO 2017-06-28 13:30:15,973 main.py:51] epoch 3771, training loss: 10932.01, average training loss: 9823.57, base loss: 14374.60
[INFO 2017-06-28 13:30:16,635 main.py:51] epoch 3772, training loss: 8358.73, average training loss: 9821.58, base loss: 14371.07
[INFO 2017-06-28 13:30:17,317 main.py:51] epoch 3773, training loss: 9949.70, average training loss: 9823.22, base loss: 14374.88
[INFO 2017-06-28 13:30:17,981 main.py:51] epoch 3774, training loss: 9366.59, average training loss: 9821.61, base loss: 14373.16
[INFO 2017-06-28 13:30:18,629 main.py:51] epoch 3775, training loss: 9379.06, average training loss: 9821.18, base loss: 14370.63
[INFO 2017-06-28 13:30:19,319 main.py:51] epoch 3776, training loss: 10821.44, average training loss: 9821.38, base loss: 14370.22
[INFO 2017-06-28 13:30:19,950 main.py:51] epoch 3777, training loss: 12982.85, average training loss: 9825.44, base loss: 14376.40
[INFO 2017-06-28 13:30:20,633 main.py:51] epoch 3778, training loss: 10376.11, average training loss: 9825.84, base loss: 14377.21
[INFO 2017-06-28 13:30:21,285 main.py:51] epoch 3779, training loss: 9758.43, average training loss: 9825.37, base loss: 14377.83
[INFO 2017-06-28 13:30:21,948 main.py:51] epoch 3780, training loss: 9669.39, average training loss: 9823.95, base loss: 14376.77
[INFO 2017-06-28 13:30:22,607 main.py:51] epoch 3781, training loss: 9157.96, average training loss: 9823.08, base loss: 14376.73
[INFO 2017-06-28 13:30:23,264 main.py:51] epoch 3782, training loss: 11167.74, average training loss: 9824.61, base loss: 14379.37
[INFO 2017-06-28 13:30:23,956 main.py:51] epoch 3783, training loss: 9938.07, average training loss: 9823.82, base loss: 14379.42
[INFO 2017-06-28 13:30:24,619 main.py:51] epoch 3784, training loss: 8928.45, average training loss: 9821.51, base loss: 14376.15
[INFO 2017-06-28 13:30:25,256 main.py:51] epoch 3785, training loss: 8842.48, average training loss: 9819.12, base loss: 14372.67
[INFO 2017-06-28 13:30:25,900 main.py:51] epoch 3786, training loss: 11209.35, average training loss: 9820.74, base loss: 14373.93
[INFO 2017-06-28 13:30:26,559 main.py:51] epoch 3787, training loss: 8466.25, average training loss: 9819.78, base loss: 14372.95
[INFO 2017-06-28 13:30:27,233 main.py:51] epoch 3788, training loss: 8226.17, average training loss: 9818.75, base loss: 14371.64
[INFO 2017-06-28 13:30:27,889 main.py:51] epoch 3789, training loss: 10461.99, average training loss: 9818.35, base loss: 14371.45
[INFO 2017-06-28 13:30:28,536 main.py:51] epoch 3790, training loss: 8944.84, average training loss: 9818.19, base loss: 14371.14
[INFO 2017-06-28 13:30:29,211 main.py:51] epoch 3791, training loss: 9821.17, average training loss: 9819.09, base loss: 14375.05
[INFO 2017-06-28 13:30:29,851 main.py:51] epoch 3792, training loss: 9506.81, average training loss: 9820.35, base loss: 14376.88
[INFO 2017-06-28 13:30:30,514 main.py:51] epoch 3793, training loss: 8841.07, average training loss: 9818.91, base loss: 14373.86
[INFO 2017-06-28 13:30:31,179 main.py:51] epoch 3794, training loss: 9952.01, average training loss: 9816.52, base loss: 14369.30
[INFO 2017-06-28 13:30:31,864 main.py:51] epoch 3795, training loss: 9787.34, average training loss: 9814.69, base loss: 14366.49
[INFO 2017-06-28 13:30:32,502 main.py:51] epoch 3796, training loss: 10880.82, average training loss: 9817.05, base loss: 14370.84
[INFO 2017-06-28 13:30:33,174 main.py:51] epoch 3797, training loss: 10591.63, average training loss: 9819.18, base loss: 14372.18
[INFO 2017-06-28 13:30:33,830 main.py:51] epoch 3798, training loss: 9606.24, average training loss: 9819.12, base loss: 14372.40
[INFO 2017-06-28 13:30:34,487 main.py:51] epoch 3799, training loss: 10936.58, average training loss: 9821.02, base loss: 14375.88
[INFO 2017-06-28 13:30:34,487 main.py:53] epoch 3799, testing
[INFO 2017-06-28 13:30:37,091 main.py:105] average testing loss: 10534.41, base loss: 14678.20
[INFO 2017-06-28 13:30:37,092 main.py:106] improve_loss: 4143.78, improve_percent: 0.28
[INFO 2017-06-28 13:30:37,092 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:30:37,737 main.py:51] epoch 3800, training loss: 11601.96, average training loss: 9822.36, base loss: 14379.14
[INFO 2017-06-28 13:30:38,378 main.py:51] epoch 3801, training loss: 9968.38, average training loss: 9822.43, base loss: 14379.29
[INFO 2017-06-28 13:30:39,044 main.py:51] epoch 3802, training loss: 8412.09, average training loss: 9820.89, base loss: 14375.84
[INFO 2017-06-28 13:30:39,692 main.py:51] epoch 3803, training loss: 10428.72, average training loss: 9823.06, base loss: 14379.56
[INFO 2017-06-28 13:30:40,372 main.py:51] epoch 3804, training loss: 10363.34, average training loss: 9823.67, base loss: 14380.44
[INFO 2017-06-28 13:30:41,043 main.py:51] epoch 3805, training loss: 9838.88, average training loss: 9823.40, base loss: 14378.90
[INFO 2017-06-28 13:30:41,702 main.py:51] epoch 3806, training loss: 9037.91, average training loss: 9822.45, base loss: 14377.70
[INFO 2017-06-28 13:30:42,351 main.py:51] epoch 3807, training loss: 10823.85, average training loss: 9823.70, base loss: 14378.44
[INFO 2017-06-28 13:30:43,019 main.py:51] epoch 3808, training loss: 10672.48, average training loss: 9823.68, base loss: 14379.73
[INFO 2017-06-28 13:30:43,690 main.py:51] epoch 3809, training loss: 9828.33, average training loss: 9822.33, base loss: 14377.71
[INFO 2017-06-28 13:30:44,371 main.py:51] epoch 3810, training loss: 9619.69, average training loss: 9821.38, base loss: 14375.70
[INFO 2017-06-28 13:30:45,034 main.py:51] epoch 3811, training loss: 10503.08, average training loss: 9821.45, base loss: 14377.06
[INFO 2017-06-28 13:30:45,704 main.py:51] epoch 3812, training loss: 9463.86, average training loss: 9822.00, base loss: 14377.84
[INFO 2017-06-28 13:30:46,348 main.py:51] epoch 3813, training loss: 8997.24, average training loss: 9821.19, base loss: 14378.24
[INFO 2017-06-28 13:30:47,005 main.py:51] epoch 3814, training loss: 8536.05, average training loss: 9821.22, base loss: 14379.95
[INFO 2017-06-28 13:30:47,671 main.py:51] epoch 3815, training loss: 9618.84, average training loss: 9821.50, base loss: 14380.33
[INFO 2017-06-28 13:30:48,324 main.py:51] epoch 3816, training loss: 8856.85, average training loss: 9821.31, base loss: 14379.74
[INFO 2017-06-28 13:30:48,990 main.py:51] epoch 3817, training loss: 9381.98, average training loss: 9820.74, base loss: 14378.96
[INFO 2017-06-28 13:30:49,648 main.py:51] epoch 3818, training loss: 11428.91, average training loss: 9822.96, base loss: 14381.12
[INFO 2017-06-28 13:30:50,301 main.py:51] epoch 3819, training loss: 10224.83, average training loss: 9824.21, base loss: 14383.18
[INFO 2017-06-28 13:30:50,941 main.py:51] epoch 3820, training loss: 8940.84, average training loss: 9824.09, base loss: 14382.38
[INFO 2017-06-28 13:30:51,589 main.py:51] epoch 3821, training loss: 9901.74, average training loss: 9823.21, base loss: 14382.35
[INFO 2017-06-28 13:30:52,231 main.py:51] epoch 3822, training loss: 10750.83, average training loss: 9823.86, base loss: 14382.98
[INFO 2017-06-28 13:30:52,895 main.py:51] epoch 3823, training loss: 9164.54, average training loss: 9823.71, base loss: 14384.67
[INFO 2017-06-28 13:30:53,552 main.py:51] epoch 3824, training loss: 10096.47, average training loss: 9822.27, base loss: 14382.62
[INFO 2017-06-28 13:30:54,199 main.py:51] epoch 3825, training loss: 10058.28, average training loss: 9823.62, base loss: 14383.68
[INFO 2017-06-28 13:30:54,862 main.py:51] epoch 3826, training loss: 8609.70, average training loss: 9822.68, base loss: 14381.56
[INFO 2017-06-28 13:30:55,521 main.py:51] epoch 3827, training loss: 9971.88, average training loss: 9822.14, base loss: 14381.74
[INFO 2017-06-28 13:30:56,199 main.py:51] epoch 3828, training loss: 9723.21, average training loss: 9822.02, base loss: 14380.73
[INFO 2017-06-28 13:30:56,857 main.py:51] epoch 3829, training loss: 10114.33, average training loss: 9821.89, base loss: 14381.59
[INFO 2017-06-28 13:30:57,510 main.py:51] epoch 3830, training loss: 10989.80, average training loss: 9822.90, base loss: 14383.71
[INFO 2017-06-28 13:30:58,171 main.py:51] epoch 3831, training loss: 9601.51, average training loss: 9823.10, base loss: 14383.21
[INFO 2017-06-28 13:30:58,831 main.py:51] epoch 3832, training loss: 8889.09, average training loss: 9822.04, base loss: 14381.15
[INFO 2017-06-28 13:30:59,496 main.py:51] epoch 3833, training loss: 10765.79, average training loss: 9823.54, base loss: 14382.35
[INFO 2017-06-28 13:31:00,166 main.py:51] epoch 3834, training loss: 10427.83, average training loss: 9823.01, base loss: 14382.46
[INFO 2017-06-28 13:31:00,840 main.py:51] epoch 3835, training loss: 10037.96, average training loss: 9823.19, base loss: 14381.96
[INFO 2017-06-28 13:31:01,493 main.py:51] epoch 3836, training loss: 9113.15, average training loss: 9821.34, base loss: 14378.91
[INFO 2017-06-28 13:31:02,142 main.py:51] epoch 3837, training loss: 10512.33, average training loss: 9821.62, base loss: 14380.79
[INFO 2017-06-28 13:31:02,807 main.py:51] epoch 3838, training loss: 11271.81, average training loss: 9821.18, base loss: 14378.91
[INFO 2017-06-28 13:31:03,474 main.py:51] epoch 3839, training loss: 10567.53, average training loss: 9821.64, base loss: 14379.27
[INFO 2017-06-28 13:31:04,127 main.py:51] epoch 3840, training loss: 9290.57, average training loss: 9822.18, base loss: 14380.70
[INFO 2017-06-28 13:31:04,762 main.py:51] epoch 3841, training loss: 10532.04, average training loss: 9823.23, base loss: 14381.87
[INFO 2017-06-28 13:31:05,415 main.py:51] epoch 3842, training loss: 9931.14, average training loss: 9823.20, base loss: 14382.73
[INFO 2017-06-28 13:31:06,051 main.py:51] epoch 3843, training loss: 9544.94, average training loss: 9823.76, base loss: 14385.20
[INFO 2017-06-28 13:31:06,713 main.py:51] epoch 3844, training loss: 10006.06, average training loss: 9823.85, base loss: 14384.97
[INFO 2017-06-28 13:31:07,368 main.py:51] epoch 3845, training loss: 10059.93, average training loss: 9824.58, base loss: 14387.52
[INFO 2017-06-28 13:31:08,032 main.py:51] epoch 3846, training loss: 9693.89, average training loss: 9824.00, base loss: 14387.13
[INFO 2017-06-28 13:31:08,695 main.py:51] epoch 3847, training loss: 10760.81, average training loss: 9824.29, base loss: 14386.89
[INFO 2017-06-28 13:31:09,354 main.py:51] epoch 3848, training loss: 9338.50, average training loss: 9823.20, base loss: 14387.49
[INFO 2017-06-28 13:31:10,010 main.py:51] epoch 3849, training loss: 10058.03, average training loss: 9823.08, base loss: 14386.67
[INFO 2017-06-28 13:31:10,682 main.py:51] epoch 3850, training loss: 12045.94, average training loss: 9824.31, base loss: 14389.02
[INFO 2017-06-28 13:31:11,327 main.py:51] epoch 3851, training loss: 9531.62, average training loss: 9823.92, base loss: 14388.39
[INFO 2017-06-28 13:31:11,977 main.py:51] epoch 3852, training loss: 9768.38, average training loss: 9823.66, base loss: 14390.12
[INFO 2017-06-28 13:31:12,637 main.py:51] epoch 3853, training loss: 10769.22, average training loss: 9824.71, base loss: 14392.30
[INFO 2017-06-28 13:31:13,298 main.py:51] epoch 3854, training loss: 10074.30, average training loss: 9824.05, base loss: 14391.61
[INFO 2017-06-28 13:31:13,953 main.py:51] epoch 3855, training loss: 9557.83, average training loss: 9823.58, base loss: 14391.99
[INFO 2017-06-28 13:31:14,620 main.py:51] epoch 3856, training loss: 10750.88, average training loss: 9822.55, base loss: 14389.80
[INFO 2017-06-28 13:31:15,267 main.py:51] epoch 3857, training loss: 9522.08, average training loss: 9823.63, base loss: 14390.46
[INFO 2017-06-28 13:31:15,914 main.py:51] epoch 3858, training loss: 9830.02, average training loss: 9823.68, base loss: 14389.87
[INFO 2017-06-28 13:31:16,578 main.py:51] epoch 3859, training loss: 8998.83, average training loss: 9823.58, base loss: 14391.19
[INFO 2017-06-28 13:31:17,235 main.py:51] epoch 3860, training loss: 9261.74, average training loss: 9823.30, base loss: 14390.43
[INFO 2017-06-28 13:31:17,893 main.py:51] epoch 3861, training loss: 10528.70, average training loss: 9824.42, base loss: 14392.75
[INFO 2017-06-28 13:31:18,558 main.py:51] epoch 3862, training loss: 10098.02, average training loss: 9825.99, base loss: 14394.81
[INFO 2017-06-28 13:31:19,234 main.py:51] epoch 3863, training loss: 9696.47, average training loss: 9825.83, base loss: 14393.90
[INFO 2017-06-28 13:31:19,893 main.py:51] epoch 3864, training loss: 9910.22, average training loss: 9826.65, base loss: 14393.22
[INFO 2017-06-28 13:31:20,548 main.py:51] epoch 3865, training loss: 8885.86, average training loss: 9825.01, base loss: 14392.15
[INFO 2017-06-28 13:31:21,196 main.py:51] epoch 3866, training loss: 8811.24, average training loss: 9825.50, base loss: 14391.56
[INFO 2017-06-28 13:31:21,854 main.py:51] epoch 3867, training loss: 9751.23, average training loss: 9825.39, base loss: 14391.93
[INFO 2017-06-28 13:31:22,511 main.py:51] epoch 3868, training loss: 9851.69, average training loss: 9826.65, base loss: 14392.59
[INFO 2017-06-28 13:31:23,161 main.py:51] epoch 3869, training loss: 11993.37, average training loss: 9829.17, base loss: 14396.11
[INFO 2017-06-28 13:31:23,814 main.py:51] epoch 3870, training loss: 8997.14, average training loss: 9828.22, base loss: 14395.08
[INFO 2017-06-28 13:31:24,478 main.py:51] epoch 3871, training loss: 10170.20, average training loss: 9828.29, base loss: 14394.81
[INFO 2017-06-28 13:31:25,127 main.py:51] epoch 3872, training loss: 10549.57, average training loss: 9828.26, base loss: 14395.20
[INFO 2017-06-28 13:31:25,773 main.py:51] epoch 3873, training loss: 9525.41, average training loss: 9828.62, base loss: 14396.71
[INFO 2017-06-28 13:31:26,451 main.py:51] epoch 3874, training loss: 9483.88, average training loss: 9828.90, base loss: 14397.11
[INFO 2017-06-28 13:31:27,107 main.py:51] epoch 3875, training loss: 9599.40, average training loss: 9830.41, base loss: 14399.99
[INFO 2017-06-28 13:31:27,782 main.py:51] epoch 3876, training loss: 9618.87, average training loss: 9830.27, base loss: 14399.08
[INFO 2017-06-28 13:31:28,440 main.py:51] epoch 3877, training loss: 9496.37, average training loss: 9830.18, base loss: 14397.51
[INFO 2017-06-28 13:31:29,117 main.py:51] epoch 3878, training loss: 9245.35, average training loss: 9830.13, base loss: 14398.90
[INFO 2017-06-28 13:31:29,770 main.py:51] epoch 3879, training loss: 10025.38, average training loss: 9830.88, base loss: 14400.51
[INFO 2017-06-28 13:31:30,429 main.py:51] epoch 3880, training loss: 10184.60, average training loss: 9829.84, base loss: 14399.24
[INFO 2017-06-28 13:31:31,084 main.py:51] epoch 3881, training loss: 8995.75, average training loss: 9830.41, base loss: 14400.23
[INFO 2017-06-28 13:31:31,751 main.py:51] epoch 3882, training loss: 10447.62, average training loss: 9831.18, base loss: 14401.32
[INFO 2017-06-28 13:31:32,406 main.py:51] epoch 3883, training loss: 11329.84, average training loss: 9833.56, base loss: 14404.87
[INFO 2017-06-28 13:31:33,058 main.py:51] epoch 3884, training loss: 10791.95, average training loss: 9835.61, base loss: 14408.96
[INFO 2017-06-28 13:31:33,709 main.py:51] epoch 3885, training loss: 11241.74, average training loss: 9837.07, base loss: 14409.61
[INFO 2017-06-28 13:31:34,411 main.py:51] epoch 3886, training loss: 8853.92, average training loss: 9835.84, base loss: 14406.85
[INFO 2017-06-28 13:31:35,077 main.py:51] epoch 3887, training loss: 9477.32, average training loss: 9836.21, base loss: 14405.54
[INFO 2017-06-28 13:31:35,715 main.py:51] epoch 3888, training loss: 12076.91, average training loss: 9839.09, base loss: 14409.32
[INFO 2017-06-28 13:31:36,376 main.py:51] epoch 3889, training loss: 9005.37, average training loss: 9840.22, base loss: 14411.60
[INFO 2017-06-28 13:31:37,032 main.py:51] epoch 3890, training loss: 8102.93, average training loss: 9838.75, base loss: 14409.15
[INFO 2017-06-28 13:31:37,678 main.py:51] epoch 3891, training loss: 10339.86, average training loss: 9839.32, base loss: 14410.79
[INFO 2017-06-28 13:31:38,361 main.py:51] epoch 3892, training loss: 8151.83, average training loss: 9837.99, base loss: 14408.73
[INFO 2017-06-28 13:31:39,030 main.py:51] epoch 3893, training loss: 9295.06, average training loss: 9838.26, base loss: 14409.13
[INFO 2017-06-28 13:31:39,693 main.py:51] epoch 3894, training loss: 10103.59, average training loss: 9838.21, base loss: 14410.01
[INFO 2017-06-28 13:31:40,350 main.py:51] epoch 3895, training loss: 9600.25, average training loss: 9837.58, base loss: 14410.18
[INFO 2017-06-28 13:31:40,991 main.py:51] epoch 3896, training loss: 9625.28, average training loss: 9836.24, base loss: 14409.81
[INFO 2017-06-28 13:31:41,627 main.py:51] epoch 3897, training loss: 9838.70, average training loss: 9835.32, base loss: 14409.47
[INFO 2017-06-28 13:31:42,302 main.py:51] epoch 3898, training loss: 9189.00, average training loss: 9834.00, base loss: 14407.67
[INFO 2017-06-28 13:31:42,971 main.py:51] epoch 3899, training loss: 9716.76, average training loss: 9834.65, base loss: 14409.24
[INFO 2017-06-28 13:31:42,971 main.py:53] epoch 3899, testing
[INFO 2017-06-28 13:31:45,541 main.py:105] average testing loss: 10887.16, base loss: 15289.58
[INFO 2017-06-28 13:31:45,541 main.py:106] improve_loss: 4402.42, improve_percent: 0.29
[INFO 2017-06-28 13:31:45,542 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:31:46,219 main.py:51] epoch 3900, training loss: 9367.88, average training loss: 9836.09, base loss: 14412.61
[INFO 2017-06-28 13:31:46,887 main.py:51] epoch 3901, training loss: 11145.61, average training loss: 9838.86, base loss: 14416.58
[INFO 2017-06-28 13:31:47,520 main.py:51] epoch 3902, training loss: 9328.56, average training loss: 9837.58, base loss: 14415.58
[INFO 2017-06-28 13:31:48,168 main.py:51] epoch 3903, training loss: 10078.40, average training loss: 9838.51, base loss: 14417.59
[INFO 2017-06-28 13:31:48,831 main.py:51] epoch 3904, training loss: 9184.63, average training loss: 9837.01, base loss: 14414.98
[INFO 2017-06-28 13:31:49,483 main.py:51] epoch 3905, training loss: 10529.42, average training loss: 9837.37, base loss: 14414.33
[INFO 2017-06-28 13:31:50,121 main.py:51] epoch 3906, training loss: 10122.41, average training loss: 9837.45, base loss: 14415.20
[INFO 2017-06-28 13:31:50,777 main.py:51] epoch 3907, training loss: 8577.82, average training loss: 9835.00, base loss: 14411.36
[INFO 2017-06-28 13:31:51,450 main.py:51] epoch 3908, training loss: 10130.08, average training loss: 9836.64, base loss: 14413.26
[INFO 2017-06-28 13:31:52,131 main.py:51] epoch 3909, training loss: 9391.56, average training loss: 9837.14, base loss: 14415.35
[INFO 2017-06-28 13:31:52,794 main.py:51] epoch 3910, training loss: 9412.22, average training loss: 9837.21, base loss: 14415.55
[INFO 2017-06-28 13:31:53,465 main.py:51] epoch 3911, training loss: 9454.75, average training loss: 9837.91, base loss: 14417.98
[INFO 2017-06-28 13:31:54,122 main.py:51] epoch 3912, training loss: 9550.92, average training loss: 9838.81, base loss: 14420.32
[INFO 2017-06-28 13:31:54,769 main.py:51] epoch 3913, training loss: 10133.29, average training loss: 9839.18, base loss: 14418.40
[INFO 2017-06-28 13:31:55,420 main.py:51] epoch 3914, training loss: 9898.15, average training loss: 9839.60, base loss: 14418.83
[INFO 2017-06-28 13:31:56,079 main.py:51] epoch 3915, training loss: 9450.28, average training loss: 9838.11, base loss: 14416.96
[INFO 2017-06-28 13:31:56,740 main.py:51] epoch 3916, training loss: 9390.61, average training loss: 9837.93, base loss: 14417.25
[INFO 2017-06-28 13:31:57,395 main.py:51] epoch 3917, training loss: 10270.01, average training loss: 9837.65, base loss: 14417.03
[INFO 2017-06-28 13:31:58,062 main.py:51] epoch 3918, training loss: 8908.46, average training loss: 9834.79, base loss: 14415.63
[INFO 2017-06-28 13:31:58,717 main.py:51] epoch 3919, training loss: 10002.36, average training loss: 9834.93, base loss: 14415.16
[INFO 2017-06-28 13:31:59,361 main.py:51] epoch 3920, training loss: 8672.83, average training loss: 9832.26, base loss: 14411.47
[INFO 2017-06-28 13:32:00,003 main.py:51] epoch 3921, training loss: 9459.42, average training loss: 9832.86, base loss: 14412.97
[INFO 2017-06-28 13:32:00,665 main.py:51] epoch 3922, training loss: 9127.22, average training loss: 9831.00, base loss: 14408.87
[INFO 2017-06-28 13:32:01,327 main.py:51] epoch 3923, training loss: 10415.02, average training loss: 9832.42, base loss: 14410.72
[INFO 2017-06-28 13:32:01,989 main.py:51] epoch 3924, training loss: 10487.44, average training loss: 9832.92, base loss: 14411.67
[INFO 2017-06-28 13:32:02,658 main.py:51] epoch 3925, training loss: 9297.96, average training loss: 9832.48, base loss: 14411.26
[INFO 2017-06-28 13:32:03,340 main.py:51] epoch 3926, training loss: 10523.42, average training loss: 9833.56, base loss: 14412.29
[INFO 2017-06-28 13:32:03,988 main.py:51] epoch 3927, training loss: 9071.86, average training loss: 9832.82, base loss: 14411.32
[INFO 2017-06-28 13:32:04,661 main.py:51] epoch 3928, training loss: 9312.34, average training loss: 9830.72, base loss: 14408.88
[INFO 2017-06-28 13:32:05,317 main.py:51] epoch 3929, training loss: 11018.19, average training loss: 9832.07, base loss: 14412.53
[INFO 2017-06-28 13:32:05,965 main.py:51] epoch 3930, training loss: 8701.98, average training loss: 9831.81, base loss: 14411.46
[INFO 2017-06-28 13:32:06,625 main.py:51] epoch 3931, training loss: 9117.98, average training loss: 9830.87, base loss: 14409.25
[INFO 2017-06-28 13:32:07,286 main.py:51] epoch 3932, training loss: 9326.03, average training loss: 9831.43, base loss: 14410.93
[INFO 2017-06-28 13:32:07,925 main.py:51] epoch 3933, training loss: 9147.29, average training loss: 9829.65, base loss: 14410.29
[INFO 2017-06-28 13:32:08,600 main.py:51] epoch 3934, training loss: 11156.57, average training loss: 9829.53, base loss: 14409.48
[INFO 2017-06-28 13:32:09,277 main.py:51] epoch 3935, training loss: 8614.63, average training loss: 9828.99, base loss: 14409.19
[INFO 2017-06-28 13:32:09,989 main.py:51] epoch 3936, training loss: 10381.30, average training loss: 9829.43, base loss: 14409.81
[INFO 2017-06-28 13:32:10,644 main.py:51] epoch 3937, training loss: 9146.62, average training loss: 9828.81, base loss: 14407.98
[INFO 2017-06-28 13:32:11,293 main.py:51] epoch 3938, training loss: 8355.10, average training loss: 9827.16, base loss: 14404.93
[INFO 2017-06-28 13:32:11,927 main.py:51] epoch 3939, training loss: 10487.48, average training loss: 9828.15, base loss: 14406.15
[INFO 2017-06-28 13:32:12,593 main.py:51] epoch 3940, training loss: 9322.96, average training loss: 9825.73, base loss: 14402.32
[INFO 2017-06-28 13:32:13,236 main.py:51] epoch 3941, training loss: 9590.77, average training loss: 9826.04, base loss: 14402.16
[INFO 2017-06-28 13:32:13,884 main.py:51] epoch 3942, training loss: 10105.44, average training loss: 9827.34, base loss: 14404.63
[INFO 2017-06-28 13:32:14,570 main.py:51] epoch 3943, training loss: 9073.99, average training loss: 9825.90, base loss: 14403.25
[INFO 2017-06-28 13:32:15,233 main.py:51] epoch 3944, training loss: 11123.42, average training loss: 9828.27, base loss: 14406.54
[INFO 2017-06-28 13:32:15,920 main.py:51] epoch 3945, training loss: 8894.15, average training loss: 9827.31, base loss: 14405.73
[INFO 2017-06-28 13:32:16,572 main.py:51] epoch 3946, training loss: 10817.92, average training loss: 9828.48, base loss: 14406.00
[INFO 2017-06-28 13:32:17,239 main.py:51] epoch 3947, training loss: 9429.00, average training loss: 9828.51, base loss: 14406.38
[INFO 2017-06-28 13:32:17,887 main.py:51] epoch 3948, training loss: 8183.24, average training loss: 9827.45, base loss: 14405.14
[INFO 2017-06-28 13:32:18,549 main.py:51] epoch 3949, training loss: 10126.85, average training loss: 9827.29, base loss: 14404.98
[INFO 2017-06-28 13:32:19,194 main.py:51] epoch 3950, training loss: 9711.45, average training loss: 9826.04, base loss: 14405.44
[INFO 2017-06-28 13:32:19,852 main.py:51] epoch 3951, training loss: 9235.51, average training loss: 9826.37, base loss: 14406.54
[INFO 2017-06-28 13:32:20,511 main.py:51] epoch 3952, training loss: 11162.78, average training loss: 9828.32, base loss: 14411.17
[INFO 2017-06-28 13:32:21,178 main.py:51] epoch 3953, training loss: 8488.36, average training loss: 9826.85, base loss: 14408.56
[INFO 2017-06-28 13:32:21,835 main.py:51] epoch 3954, training loss: 10136.60, average training loss: 9827.74, base loss: 14410.32
[INFO 2017-06-28 13:32:22,499 main.py:51] epoch 3955, training loss: 9662.52, average training loss: 9826.03, base loss: 14408.68
[INFO 2017-06-28 13:32:23,145 main.py:51] epoch 3956, training loss: 8774.72, average training loss: 9824.97, base loss: 14408.53
[INFO 2017-06-28 13:32:23,815 main.py:51] epoch 3957, training loss: 10539.17, average training loss: 9825.28, base loss: 14408.97
[INFO 2017-06-28 13:32:24,472 main.py:51] epoch 3958, training loss: 9409.86, average training loss: 9822.56, base loss: 14406.12
[INFO 2017-06-28 13:32:25,118 main.py:51] epoch 3959, training loss: 9456.06, average training loss: 9821.99, base loss: 14406.38
[INFO 2017-06-28 13:32:25,768 main.py:51] epoch 3960, training loss: 9730.73, average training loss: 9821.83, base loss: 14407.21
[INFO 2017-06-28 13:32:26,437 main.py:51] epoch 3961, training loss: 9256.42, average training loss: 9821.32, base loss: 14405.57
[INFO 2017-06-28 13:32:27,113 main.py:51] epoch 3962, training loss: 9066.67, average training loss: 9819.96, base loss: 14402.13
[INFO 2017-06-28 13:32:27,765 main.py:51] epoch 3963, training loss: 9871.00, average training loss: 9820.05, base loss: 14403.42
[INFO 2017-06-28 13:32:28,421 main.py:51] epoch 3964, training loss: 10244.54, average training loss: 9821.10, base loss: 14406.14
[INFO 2017-06-28 13:32:29,075 main.py:51] epoch 3965, training loss: 10480.60, average training loss: 9821.43, base loss: 14405.44
[INFO 2017-06-28 13:32:29,744 main.py:51] epoch 3966, training loss: 9477.38, average training loss: 9822.10, base loss: 14406.01
[INFO 2017-06-28 13:32:30,419 main.py:51] epoch 3967, training loss: 9634.32, average training loss: 9822.13, base loss: 14405.88
[INFO 2017-06-28 13:32:31,070 main.py:51] epoch 3968, training loss: 9806.21, average training loss: 9822.84, base loss: 14407.49
[INFO 2017-06-28 13:32:31,725 main.py:51] epoch 3969, training loss: 10582.47, average training loss: 9823.93, base loss: 14410.36
[INFO 2017-06-28 13:32:32,404 main.py:51] epoch 3970, training loss: 8913.49, average training loss: 9822.49, base loss: 14408.92
[INFO 2017-06-28 13:32:33,071 main.py:51] epoch 3971, training loss: 9700.07, average training loss: 9823.32, base loss: 14409.20
[INFO 2017-06-28 13:32:33,755 main.py:51] epoch 3972, training loss: 9937.12, average training loss: 9823.83, base loss: 14410.37
[INFO 2017-06-28 13:32:34,415 main.py:51] epoch 3973, training loss: 11403.79, average training loss: 9826.21, base loss: 14413.46
[INFO 2017-06-28 13:32:35,049 main.py:51] epoch 3974, training loss: 10758.52, average training loss: 9828.11, base loss: 14416.98
[INFO 2017-06-28 13:32:35,725 main.py:51] epoch 3975, training loss: 9392.54, average training loss: 9826.81, base loss: 14414.81
[INFO 2017-06-28 13:32:36,360 main.py:51] epoch 3976, training loss: 9238.51, average training loss: 9825.06, base loss: 14412.67
[INFO 2017-06-28 13:32:37,017 main.py:51] epoch 3977, training loss: 10492.49, average training loss: 9824.91, base loss: 14412.93
[INFO 2017-06-28 13:32:37,654 main.py:51] epoch 3978, training loss: 9458.98, average training loss: 9824.81, base loss: 14413.74
[INFO 2017-06-28 13:32:38,315 main.py:51] epoch 3979, training loss: 7827.10, average training loss: 9822.14, base loss: 14408.97
[INFO 2017-06-28 13:32:38,968 main.py:51] epoch 3980, training loss: 10291.76, average training loss: 9822.71, base loss: 14412.14
[INFO 2017-06-28 13:32:39,623 main.py:51] epoch 3981, training loss: 9351.78, average training loss: 9821.82, base loss: 14412.12
[INFO 2017-06-28 13:32:40,270 main.py:51] epoch 3982, training loss: 8682.15, average training loss: 9820.05, base loss: 14409.64
[INFO 2017-06-28 13:32:40,944 main.py:51] epoch 3983, training loss: 9700.18, average training loss: 9820.27, base loss: 14411.02
[INFO 2017-06-28 13:32:41,599 main.py:51] epoch 3984, training loss: 8862.76, average training loss: 9818.90, base loss: 14408.45
[INFO 2017-06-28 13:32:42,255 main.py:51] epoch 3985, training loss: 9668.50, average training loss: 9819.07, base loss: 14409.79
[INFO 2017-06-28 13:32:42,910 main.py:51] epoch 3986, training loss: 9911.49, average training loss: 9819.59, base loss: 14411.18
[INFO 2017-06-28 13:32:43,580 main.py:51] epoch 3987, training loss: 10105.51, average training loss: 9818.97, base loss: 14410.76
[INFO 2017-06-28 13:32:44,263 main.py:51] epoch 3988, training loss: 10055.98, average training loss: 9819.06, base loss: 14411.79
[INFO 2017-06-28 13:32:44,904 main.py:51] epoch 3989, training loss: 8975.16, average training loss: 9818.85, base loss: 14412.13
[INFO 2017-06-28 13:32:45,567 main.py:51] epoch 3990, training loss: 10416.81, average training loss: 9819.81, base loss: 14414.73
[INFO 2017-06-28 13:32:46,221 main.py:51] epoch 3991, training loss: 9640.40, average training loss: 9818.03, base loss: 14411.23
[INFO 2017-06-28 13:32:46,862 main.py:51] epoch 3992, training loss: 9265.69, average training loss: 9817.64, base loss: 14412.40
[INFO 2017-06-28 13:32:47,514 main.py:51] epoch 3993, training loss: 8496.88, average training loss: 9816.63, base loss: 14412.18
[INFO 2017-06-28 13:32:48,176 main.py:51] epoch 3994, training loss: 10314.30, average training loss: 9817.32, base loss: 14413.79
[INFO 2017-06-28 13:32:48,835 main.py:51] epoch 3995, training loss: 11204.13, average training loss: 9818.28, base loss: 14416.10
[INFO 2017-06-28 13:32:49,486 main.py:51] epoch 3996, training loss: 9933.89, average training loss: 9817.19, base loss: 14414.82
[INFO 2017-06-28 13:32:50,142 main.py:51] epoch 3997, training loss: 10248.55, average training loss: 9818.35, base loss: 14417.66
[INFO 2017-06-28 13:32:50,795 main.py:51] epoch 3998, training loss: 9154.82, average training loss: 9817.27, base loss: 14416.44
[INFO 2017-06-28 13:32:51,450 main.py:51] epoch 3999, training loss: 10458.90, average training loss: 9817.71, base loss: 14416.05
[INFO 2017-06-28 13:32:51,450 main.py:53] epoch 3999, testing
[INFO 2017-06-28 13:32:54,017 main.py:105] average testing loss: 10873.66, base loss: 15119.33
[INFO 2017-06-28 13:32:54,017 main.py:106] improve_loss: 4245.67, improve_percent: 0.28
[INFO 2017-06-28 13:32:54,018 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:32:54,682 main.py:51] epoch 4000, training loss: 10042.80, average training loss: 9819.74, base loss: 14419.03
[INFO 2017-06-28 13:32:55,330 main.py:51] epoch 4001, training loss: 10061.06, average training loss: 9820.60, base loss: 14419.71
[INFO 2017-06-28 13:32:56,018 main.py:51] epoch 4002, training loss: 9405.02, average training loss: 9820.05, base loss: 14418.98
[INFO 2017-06-28 13:32:56,670 main.py:51] epoch 4003, training loss: 10995.79, average training loss: 9822.31, base loss: 14423.15
[INFO 2017-06-28 13:32:57,332 main.py:51] epoch 4004, training loss: 9593.44, average training loss: 9821.57, base loss: 14422.25
[INFO 2017-06-28 13:32:58,016 main.py:51] epoch 4005, training loss: 11379.35, average training loss: 9824.03, base loss: 14425.89
[INFO 2017-06-28 13:32:58,678 main.py:51] epoch 4006, training loss: 11460.81, average training loss: 9825.87, base loss: 14427.99
[INFO 2017-06-28 13:32:59,314 main.py:51] epoch 4007, training loss: 10028.64, average training loss: 9826.27, base loss: 14427.86
[INFO 2017-06-28 13:32:59,952 main.py:51] epoch 4008, training loss: 9662.79, average training loss: 9827.73, base loss: 14431.01
[INFO 2017-06-28 13:33:00,615 main.py:51] epoch 4009, training loss: 10114.61, average training loss: 9828.18, base loss: 14430.77
[INFO 2017-06-28 13:33:01,279 main.py:51] epoch 4010, training loss: 11092.83, average training loss: 9828.66, base loss: 14430.34
[INFO 2017-06-28 13:33:01,925 main.py:51] epoch 4011, training loss: 10186.33, average training loss: 9830.56, base loss: 14433.81
[INFO 2017-06-28 13:33:02,602 main.py:51] epoch 4012, training loss: 10448.41, average training loss: 9831.03, base loss: 14434.85
[INFO 2017-06-28 13:33:03,270 main.py:51] epoch 4013, training loss: 11073.50, average training loss: 9830.03, base loss: 14432.91
[INFO 2017-06-28 13:33:03,949 main.py:51] epoch 4014, training loss: 10158.00, average training loss: 9829.43, base loss: 14431.65
[INFO 2017-06-28 13:33:04,591 main.py:51] epoch 4015, training loss: 9483.54, average training loss: 9828.27, base loss: 14429.99
[INFO 2017-06-28 13:33:05,229 main.py:51] epoch 4016, training loss: 9817.75, average training loss: 9829.03, base loss: 14432.11
[INFO 2017-06-28 13:33:05,895 main.py:51] epoch 4017, training loss: 10091.51, average training loss: 9828.57, base loss: 14432.31
[INFO 2017-06-28 13:33:06,548 main.py:51] epoch 4018, training loss: 9048.26, average training loss: 9826.79, base loss: 14429.67
[INFO 2017-06-28 13:33:07,219 main.py:51] epoch 4019, training loss: 9342.61, average training loss: 9826.40, base loss: 14430.29
[INFO 2017-06-28 13:33:07,890 main.py:51] epoch 4020, training loss: 10187.28, average training loss: 9826.98, base loss: 14431.34
[INFO 2017-06-28 13:33:08,543 main.py:51] epoch 4021, training loss: 9486.22, average training loss: 9826.79, base loss: 14432.04
[INFO 2017-06-28 13:33:09,203 main.py:51] epoch 4022, training loss: 10451.27, average training loss: 9828.05, base loss: 14436.03
[INFO 2017-06-28 13:33:09,873 main.py:51] epoch 4023, training loss: 8830.48, average training loss: 9826.60, base loss: 14433.22
[INFO 2017-06-28 13:33:10,512 main.py:51] epoch 4024, training loss: 10596.54, average training loss: 9826.86, base loss: 14434.59
[INFO 2017-06-28 13:33:11,159 main.py:51] epoch 4025, training loss: 9817.01, average training loss: 9827.40, base loss: 14436.44
[INFO 2017-06-28 13:33:11,804 main.py:51] epoch 4026, training loss: 10371.04, average training loss: 9828.92, base loss: 14441.08
[INFO 2017-06-28 13:33:12,444 main.py:51] epoch 4027, training loss: 9750.31, average training loss: 9828.67, base loss: 14441.76
[INFO 2017-06-28 13:33:13,128 main.py:51] epoch 4028, training loss: 12171.19, average training loss: 9830.25, base loss: 14443.48
[INFO 2017-06-28 13:33:13,797 main.py:51] epoch 4029, training loss: 9834.30, average training loss: 9830.86, base loss: 14445.15
[INFO 2017-06-28 13:33:14,456 main.py:51] epoch 4030, training loss: 9905.96, average training loss: 9830.83, base loss: 14444.99
[INFO 2017-06-28 13:33:15,105 main.py:51] epoch 4031, training loss: 9479.99, average training loss: 9831.22, base loss: 14446.67
[INFO 2017-06-28 13:33:15,764 main.py:51] epoch 4032, training loss: 9283.29, average training loss: 9830.56, base loss: 14445.26
[INFO 2017-06-28 13:33:16,432 main.py:51] epoch 4033, training loss: 8469.69, average training loss: 9828.52, base loss: 14443.80
[INFO 2017-06-28 13:33:17,081 main.py:51] epoch 4034, training loss: 11632.74, average training loss: 9832.05, base loss: 14448.32
[INFO 2017-06-28 13:33:17,760 main.py:51] epoch 4035, training loss: 8109.34, average training loss: 9830.32, base loss: 14446.87
[INFO 2017-06-28 13:33:18,432 main.py:51] epoch 4036, training loss: 8926.84, average training loss: 9829.77, base loss: 14446.94
[INFO 2017-06-28 13:33:19,082 main.py:51] epoch 4037, training loss: 9812.77, average training loss: 9827.30, base loss: 14444.07
[INFO 2017-06-28 13:33:19,717 main.py:51] epoch 4038, training loss: 9772.48, average training loss: 9826.28, base loss: 14441.98
[INFO 2017-06-28 13:33:20,378 main.py:51] epoch 4039, training loss: 10482.24, average training loss: 9826.32, base loss: 14442.23
[INFO 2017-06-28 13:33:21,024 main.py:51] epoch 4040, training loss: 9652.71, average training loss: 9825.21, base loss: 14440.81
[INFO 2017-06-28 13:33:21,692 main.py:51] epoch 4041, training loss: 8367.44, average training loss: 9823.28, base loss: 14438.36
[INFO 2017-06-28 13:33:22,351 main.py:51] epoch 4042, training loss: 9412.74, average training loss: 9823.18, base loss: 14436.89
[INFO 2017-06-28 13:33:23,004 main.py:51] epoch 4043, training loss: 9628.88, average training loss: 9823.03, base loss: 14437.97
[INFO 2017-06-28 13:33:23,673 main.py:51] epoch 4044, training loss: 10413.35, average training loss: 9823.13, base loss: 14437.81
[INFO 2017-06-28 13:33:24,319 main.py:51] epoch 4045, training loss: 8816.20, average training loss: 9823.02, base loss: 14437.36
[INFO 2017-06-28 13:33:24,993 main.py:51] epoch 4046, training loss: 10999.78, average training loss: 9824.48, base loss: 14438.46
[INFO 2017-06-28 13:33:25,640 main.py:51] epoch 4047, training loss: 10347.24, average training loss: 9823.93, base loss: 14438.50
[INFO 2017-06-28 13:33:26,289 main.py:51] epoch 4048, training loss: 9936.73, average training loss: 9823.97, base loss: 14437.25
[INFO 2017-06-28 13:33:26,954 main.py:51] epoch 4049, training loss: 9162.92, average training loss: 9824.04, base loss: 14438.97
[INFO 2017-06-28 13:33:27,600 main.py:51] epoch 4050, training loss: 10403.86, average training loss: 9823.97, base loss: 14436.83
[INFO 2017-06-28 13:33:28,285 main.py:51] epoch 4051, training loss: 8708.02, average training loss: 9822.53, base loss: 14436.68
[INFO 2017-06-28 13:33:28,931 main.py:51] epoch 4052, training loss: 10790.97, average training loss: 9823.31, base loss: 14437.66
[INFO 2017-06-28 13:33:29,574 main.py:51] epoch 4053, training loss: 9646.41, average training loss: 9823.19, base loss: 14437.68
[INFO 2017-06-28 13:33:30,253 main.py:51] epoch 4054, training loss: 8339.10, average training loss: 9820.49, base loss: 14434.64
[INFO 2017-06-28 13:33:30,896 main.py:51] epoch 4055, training loss: 10042.07, average training loss: 9820.92, base loss: 14434.57
[INFO 2017-06-28 13:33:31,572 main.py:51] epoch 4056, training loss: 9930.14, average training loss: 9820.46, base loss: 14434.38
[INFO 2017-06-28 13:33:32,216 main.py:51] epoch 4057, training loss: 8128.37, average training loss: 9818.66, base loss: 14432.43
[INFO 2017-06-28 13:33:32,858 main.py:51] epoch 4058, training loss: 9071.91, average training loss: 9818.94, base loss: 14435.84
[INFO 2017-06-28 13:33:33,537 main.py:51] epoch 4059, training loss: 9933.80, average training loss: 9818.07, base loss: 14434.72
[INFO 2017-06-28 13:33:34,198 main.py:51] epoch 4060, training loss: 10042.57, average training loss: 9817.37, base loss: 14435.71
[INFO 2017-06-28 13:33:34,858 main.py:51] epoch 4061, training loss: 9699.21, average training loss: 9817.20, base loss: 14436.61
[INFO 2017-06-28 13:33:35,530 main.py:51] epoch 4062, training loss: 9256.92, average training loss: 9817.15, base loss: 14437.45
[INFO 2017-06-28 13:33:36,185 main.py:51] epoch 4063, training loss: 10020.42, average training loss: 9817.52, base loss: 14437.01
[INFO 2017-06-28 13:33:36,845 main.py:51] epoch 4064, training loss: 8992.33, average training loss: 9817.76, base loss: 14436.50
[INFO 2017-06-28 13:33:37,495 main.py:51] epoch 4065, training loss: 9587.89, average training loss: 9816.51, base loss: 14435.15
[INFO 2017-06-28 13:33:38,156 main.py:51] epoch 4066, training loss: 10919.03, average training loss: 9816.77, base loss: 14435.07
[INFO 2017-06-28 13:33:38,821 main.py:51] epoch 4067, training loss: 8501.45, average training loss: 9814.78, base loss: 14433.40
[INFO 2017-06-28 13:33:39,508 main.py:51] epoch 4068, training loss: 8101.26, average training loss: 9813.15, base loss: 14431.06
[INFO 2017-06-28 13:33:40,163 main.py:51] epoch 4069, training loss: 9285.27, average training loss: 9812.53, base loss: 14429.86
[INFO 2017-06-28 13:33:40,813 main.py:51] epoch 4070, training loss: 10728.81, average training loss: 9813.93, base loss: 14431.23
[INFO 2017-06-28 13:33:41,450 main.py:51] epoch 4071, training loss: 9320.72, average training loss: 9812.41, base loss: 14429.21
[INFO 2017-06-28 13:33:42,091 main.py:51] epoch 4072, training loss: 10946.28, average training loss: 9813.03, base loss: 14429.65
[INFO 2017-06-28 13:33:42,761 main.py:51] epoch 4073, training loss: 9938.66, average training loss: 9814.80, base loss: 14431.91
[INFO 2017-06-28 13:33:43,434 main.py:51] epoch 4074, training loss: 10497.22, average training loss: 9816.00, base loss: 14432.95
[INFO 2017-06-28 13:33:44,091 main.py:51] epoch 4075, training loss: 10540.45, average training loss: 9815.37, base loss: 14432.18
[INFO 2017-06-28 13:33:44,730 main.py:51] epoch 4076, training loss: 9630.35, average training loss: 9816.11, base loss: 14433.48
[INFO 2017-06-28 13:33:45,381 main.py:51] epoch 4077, training loss: 10224.26, average training loss: 9816.11, base loss: 14432.74
[INFO 2017-06-28 13:33:46,048 main.py:51] epoch 4078, training loss: 8554.40, average training loss: 9815.23, base loss: 14431.14
[INFO 2017-06-28 13:33:46,697 main.py:51] epoch 4079, training loss: 8936.44, average training loss: 9814.15, base loss: 14429.31
[INFO 2017-06-28 13:33:47,355 main.py:51] epoch 4080, training loss: 9836.20, average training loss: 9813.27, base loss: 14429.49
[INFO 2017-06-28 13:33:48,022 main.py:51] epoch 4081, training loss: 9682.63, average training loss: 9811.99, base loss: 14428.68
[INFO 2017-06-28 13:33:48,657 main.py:51] epoch 4082, training loss: 7815.92, average training loss: 9809.57, base loss: 14425.31
[INFO 2017-06-28 13:33:49,319 main.py:51] epoch 4083, training loss: 9934.91, average training loss: 9808.75, base loss: 14423.49
[INFO 2017-06-28 13:33:50,007 main.py:51] epoch 4084, training loss: 8579.13, average training loss: 9806.50, base loss: 14421.81
[INFO 2017-06-28 13:33:50,670 main.py:51] epoch 4085, training loss: 9422.05, average training loss: 9805.88, base loss: 14421.10
[INFO 2017-06-28 13:33:51,335 main.py:51] epoch 4086, training loss: 9853.91, average training loss: 9805.91, base loss: 14421.39
[INFO 2017-06-28 13:33:51,993 main.py:51] epoch 4087, training loss: 9815.74, average training loss: 9805.63, base loss: 14421.91
[INFO 2017-06-28 13:33:52,659 main.py:51] epoch 4088, training loss: 10575.81, average training loss: 9806.93, base loss: 14423.99
[INFO 2017-06-28 13:33:53,327 main.py:51] epoch 4089, training loss: 9152.38, average training loss: 9807.06, base loss: 14423.41
[INFO 2017-06-28 13:33:53,985 main.py:51] epoch 4090, training loss: 9059.40, average training loss: 9806.13, base loss: 14421.92
[INFO 2017-06-28 13:33:54,638 main.py:51] epoch 4091, training loss: 8860.62, average training loss: 9803.97, base loss: 14418.87
[INFO 2017-06-28 13:33:55,297 main.py:51] epoch 4092, training loss: 8133.81, average training loss: 9802.92, base loss: 14417.77
[INFO 2017-06-28 13:33:55,981 main.py:51] epoch 4093, training loss: 8859.81, average training loss: 9800.76, base loss: 14414.99
[INFO 2017-06-28 13:33:56,648 main.py:51] epoch 4094, training loss: 9409.29, average training loss: 9800.32, base loss: 14414.60
[INFO 2017-06-28 13:33:57,345 main.py:51] epoch 4095, training loss: 7976.20, average training loss: 9797.66, base loss: 14410.11
[INFO 2017-06-28 13:33:58,006 main.py:51] epoch 4096, training loss: 10317.70, average training loss: 9797.46, base loss: 14409.33
[INFO 2017-06-28 13:33:58,667 main.py:51] epoch 4097, training loss: 10139.16, average training loss: 9797.05, base loss: 14408.32
[INFO 2017-06-28 13:33:59,316 main.py:51] epoch 4098, training loss: 9319.02, average training loss: 9796.80, base loss: 14408.62
[INFO 2017-06-28 13:33:59,982 main.py:51] epoch 4099, training loss: 9502.26, average training loss: 9797.35, base loss: 14409.07
[INFO 2017-06-28 13:33:59,983 main.py:53] epoch 4099, testing
[INFO 2017-06-28 13:34:02,649 main.py:105] average testing loss: 10679.24, base loss: 14912.47
[INFO 2017-06-28 13:34:02,649 main.py:106] improve_loss: 4233.23, improve_percent: 0.28
[INFO 2017-06-28 13:34:02,650 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:34:03,332 main.py:51] epoch 4100, training loss: 9987.66, average training loss: 9797.44, base loss: 14409.63
[INFO 2017-06-28 13:34:04,020 main.py:51] epoch 4101, training loss: 8652.10, average training loss: 9796.40, base loss: 14409.37
[INFO 2017-06-28 13:34:04,682 main.py:51] epoch 4102, training loss: 8389.44, average training loss: 9795.14, base loss: 14407.86
[INFO 2017-06-28 13:34:05,320 main.py:51] epoch 4103, training loss: 10131.49, average training loss: 9796.43, base loss: 14409.56
[INFO 2017-06-28 13:34:05,982 main.py:51] epoch 4104, training loss: 9235.58, average training loss: 9795.41, base loss: 14407.86
[INFO 2017-06-28 13:34:06,627 main.py:51] epoch 4105, training loss: 10454.34, average training loss: 9795.91, base loss: 14408.59
[INFO 2017-06-28 13:34:07,278 main.py:51] epoch 4106, training loss: 9435.42, average training loss: 9795.91, base loss: 14409.09
[INFO 2017-06-28 13:34:07,929 main.py:51] epoch 4107, training loss: 9254.55, average training loss: 9796.04, base loss: 14410.21
[INFO 2017-06-28 13:34:08,566 main.py:51] epoch 4108, training loss: 9595.57, average training loss: 9794.53, base loss: 14408.99
[INFO 2017-06-28 13:34:09,214 main.py:51] epoch 4109, training loss: 8793.93, average training loss: 9794.86, base loss: 14409.76
[INFO 2017-06-28 13:34:09,852 main.py:51] epoch 4110, training loss: 10367.53, average training loss: 9794.63, base loss: 14409.20
[INFO 2017-06-28 13:34:10,515 main.py:51] epoch 4111, training loss: 10924.71, average training loss: 9797.16, base loss: 14413.02
[INFO 2017-06-28 13:34:11,175 main.py:51] epoch 4112, training loss: 9753.14, average training loss: 9797.76, base loss: 14414.86
[INFO 2017-06-28 13:34:11,840 main.py:51] epoch 4113, training loss: 9604.64, average training loss: 9798.45, base loss: 14416.06
[INFO 2017-06-28 13:34:12,544 main.py:51] epoch 4114, training loss: 9562.88, average training loss: 9799.19, base loss: 14418.22
[INFO 2017-06-28 13:34:13,228 main.py:51] epoch 4115, training loss: 8216.59, average training loss: 9796.64, base loss: 14413.85
[INFO 2017-06-28 13:34:13,897 main.py:51] epoch 4116, training loss: 8940.94, average training loss: 9797.49, base loss: 14415.90
[INFO 2017-06-28 13:34:14,564 main.py:51] epoch 4117, training loss: 10226.85, average training loss: 9797.24, base loss: 14414.91
[INFO 2017-06-28 13:34:15,227 main.py:51] epoch 4118, training loss: 9587.89, average training loss: 9797.15, base loss: 14413.83
[INFO 2017-06-28 13:34:15,889 main.py:51] epoch 4119, training loss: 10276.07, average training loss: 9798.45, base loss: 14416.80
[INFO 2017-06-28 13:34:16,570 main.py:51] epoch 4120, training loss: 11551.21, average training loss: 9801.09, base loss: 14421.61
[INFO 2017-06-28 13:34:17,219 main.py:51] epoch 4121, training loss: 10810.13, average training loss: 9801.55, base loss: 14422.20
[INFO 2017-06-28 13:34:17,863 main.py:51] epoch 4122, training loss: 9236.70, average training loss: 9801.65, base loss: 14421.23
[INFO 2017-06-28 13:34:18,516 main.py:51] epoch 4123, training loss: 8684.59, average training loss: 9800.90, base loss: 14419.55
[INFO 2017-06-28 13:34:19,188 main.py:51] epoch 4124, training loss: 9992.24, average training loss: 9802.55, base loss: 14422.83
[INFO 2017-06-28 13:34:19,832 main.py:51] epoch 4125, training loss: 10754.56, average training loss: 9803.68, base loss: 14424.01
[INFO 2017-06-28 13:34:20,471 main.py:51] epoch 4126, training loss: 9887.50, average training loss: 9803.94, base loss: 14423.62
[INFO 2017-06-28 13:34:21,135 main.py:51] epoch 4127, training loss: 9430.02, average training loss: 9804.16, base loss: 14423.86
[INFO 2017-06-28 13:34:21,797 main.py:51] epoch 4128, training loss: 8828.39, average training loss: 9802.14, base loss: 14422.70
[INFO 2017-06-28 13:34:22,446 main.py:51] epoch 4129, training loss: 9716.09, average training loss: 9800.89, base loss: 14421.06
[INFO 2017-06-28 13:34:23,087 main.py:51] epoch 4130, training loss: 10003.06, average training loss: 9801.13, base loss: 14420.65
[INFO 2017-06-28 13:34:23,755 main.py:51] epoch 4131, training loss: 11292.24, average training loss: 9803.51, base loss: 14423.80
[INFO 2017-06-28 13:34:24,435 main.py:51] epoch 4132, training loss: 8635.29, average training loss: 9802.98, base loss: 14422.21
[INFO 2017-06-28 13:34:25,106 main.py:51] epoch 4133, training loss: 9575.63, average training loss: 9802.13, base loss: 14422.95
[INFO 2017-06-28 13:34:25,768 main.py:51] epoch 4134, training loss: 8886.21, average training loss: 9800.58, base loss: 14419.66
[INFO 2017-06-28 13:34:26,412 main.py:51] epoch 4135, training loss: 9025.31, average training loss: 9800.67, base loss: 14420.45
[INFO 2017-06-28 13:34:27,061 main.py:51] epoch 4136, training loss: 11011.19, average training loss: 9802.07, base loss: 14423.93
[INFO 2017-06-28 13:34:27,728 main.py:51] epoch 4137, training loss: 8578.02, average training loss: 9799.91, base loss: 14418.88
[INFO 2017-06-28 13:34:28,378 main.py:51] epoch 4138, training loss: 8876.16, average training loss: 9798.30, base loss: 14417.55
[INFO 2017-06-28 13:34:29,049 main.py:51] epoch 4139, training loss: 9875.26, average training loss: 9798.56, base loss: 14417.14
[INFO 2017-06-28 13:34:29,708 main.py:51] epoch 4140, training loss: 9356.75, average training loss: 9798.71, base loss: 14417.05
[INFO 2017-06-28 13:34:30,362 main.py:51] epoch 4141, training loss: 9750.64, average training loss: 9799.45, base loss: 14420.35
[INFO 2017-06-28 13:34:31,022 main.py:51] epoch 4142, training loss: 8967.73, average training loss: 9796.51, base loss: 14416.67
[INFO 2017-06-28 13:34:31,694 main.py:51] epoch 4143, training loss: 10128.53, average training loss: 9797.29, base loss: 14416.19
[INFO 2017-06-28 13:34:32,339 main.py:51] epoch 4144, training loss: 9376.02, average training loss: 9797.66, base loss: 14417.65
[INFO 2017-06-28 13:34:33,005 main.py:51] epoch 4145, training loss: 9044.72, average training loss: 9797.35, base loss: 14417.47
[INFO 2017-06-28 13:34:33,647 main.py:51] epoch 4146, training loss: 9142.79, average training loss: 9795.79, base loss: 14416.03
[INFO 2017-06-28 13:34:34,292 main.py:51] epoch 4147, training loss: 8673.92, average training loss: 9795.38, base loss: 14415.61
[INFO 2017-06-28 13:34:34,945 main.py:51] epoch 4148, training loss: 9282.27, average training loss: 9795.67, base loss: 14415.67
[INFO 2017-06-28 13:34:35,599 main.py:51] epoch 4149, training loss: 9075.86, average training loss: 9795.04, base loss: 14415.08
[INFO 2017-06-28 13:34:36,253 main.py:51] epoch 4150, training loss: 9357.20, average training loss: 9794.83, base loss: 14415.00
[INFO 2017-06-28 13:34:36,923 main.py:51] epoch 4151, training loss: 10486.08, average training loss: 9796.06, base loss: 14416.23
[INFO 2017-06-28 13:34:37,564 main.py:51] epoch 4152, training loss: 9859.52, average training loss: 9795.02, base loss: 14415.15
[INFO 2017-06-28 13:34:38,223 main.py:51] epoch 4153, training loss: 8996.20, average training loss: 9793.02, base loss: 14412.45
[INFO 2017-06-28 13:34:38,913 main.py:51] epoch 4154, training loss: 11288.10, average training loss: 9794.08, base loss: 14414.21
[INFO 2017-06-28 13:34:39,589 main.py:51] epoch 4155, training loss: 10588.75, average training loss: 9794.25, base loss: 14415.63
[INFO 2017-06-28 13:34:40,241 main.py:51] epoch 4156, training loss: 10365.05, average training loss: 9795.93, base loss: 14418.23
[INFO 2017-06-28 13:34:40,904 main.py:51] epoch 4157, training loss: 10299.94, average training loss: 9797.39, base loss: 14420.80
[INFO 2017-06-28 13:34:41,562 main.py:51] epoch 4158, training loss: 9681.19, average training loss: 9797.70, base loss: 14421.87
[INFO 2017-06-28 13:34:42,230 main.py:51] epoch 4159, training loss: 9423.93, average training loss: 9795.71, base loss: 14417.36
[INFO 2017-06-28 13:34:42,909 main.py:51] epoch 4160, training loss: 9846.50, average training loss: 9795.34, base loss: 14417.88
[INFO 2017-06-28 13:34:43,565 main.py:51] epoch 4161, training loss: 10439.96, average training loss: 9797.55, base loss: 14420.89
[INFO 2017-06-28 13:34:44,248 main.py:51] epoch 4162, training loss: 9395.28, average training loss: 9797.03, base loss: 14420.16
[INFO 2017-06-28 13:34:44,893 main.py:51] epoch 4163, training loss: 8673.26, average training loss: 9795.77, base loss: 14418.25
[INFO 2017-06-28 13:34:45,576 main.py:51] epoch 4164, training loss: 9197.06, average training loss: 9793.83, base loss: 14415.83
[INFO 2017-06-28 13:34:46,218 main.py:51] epoch 4165, training loss: 10738.54, average training loss: 9794.99, base loss: 14416.51
[INFO 2017-06-28 13:34:46,871 main.py:51] epoch 4166, training loss: 7982.17, average training loss: 9793.06, base loss: 14414.60
[INFO 2017-06-28 13:34:47,526 main.py:51] epoch 4167, training loss: 11095.55, average training loss: 9793.32, base loss: 14416.30
[INFO 2017-06-28 13:34:48,154 main.py:51] epoch 4168, training loss: 11281.08, average training loss: 9795.20, base loss: 14419.21
[INFO 2017-06-28 13:34:48,817 main.py:51] epoch 4169, training loss: 9834.78, average training loss: 9795.56, base loss: 14419.63
[INFO 2017-06-28 13:34:49,478 main.py:51] epoch 4170, training loss: 9021.83, average training loss: 9791.63, base loss: 14414.32
[INFO 2017-06-28 13:34:50,144 main.py:51] epoch 4171, training loss: 9627.72, average training loss: 9791.49, base loss: 14413.77
[INFO 2017-06-28 13:34:50,797 main.py:51] epoch 4172, training loss: 9044.00, average training loss: 9790.25, base loss: 14411.07
[INFO 2017-06-28 13:34:51,457 main.py:51] epoch 4173, training loss: 9695.19, average training loss: 9790.52, base loss: 14411.22
[INFO 2017-06-28 13:34:52,131 main.py:51] epoch 4174, training loss: 9946.22, average training loss: 9790.42, base loss: 14410.21
[INFO 2017-06-28 13:34:52,786 main.py:51] epoch 4175, training loss: 9800.87, average training loss: 9789.85, base loss: 14411.70
[INFO 2017-06-28 13:34:53,444 main.py:51] epoch 4176, training loss: 10383.96, average training loss: 9790.47, base loss: 14412.82
[INFO 2017-06-28 13:34:54,087 main.py:51] epoch 4177, training loss: 8665.94, average training loss: 9788.16, base loss: 14409.49
[INFO 2017-06-28 13:34:54,765 main.py:51] epoch 4178, training loss: 7784.07, average training loss: 9786.01, base loss: 14406.46
[INFO 2017-06-28 13:34:55,435 main.py:51] epoch 4179, training loss: 10244.93, average training loss: 9785.85, base loss: 14404.91
[INFO 2017-06-28 13:34:56,119 main.py:51] epoch 4180, training loss: 9773.24, average training loss: 9784.96, base loss: 14405.55
[INFO 2017-06-28 13:34:56,781 main.py:51] epoch 4181, training loss: 8661.11, average training loss: 9782.99, base loss: 14401.55
[INFO 2017-06-28 13:34:57,457 main.py:51] epoch 4182, training loss: 8726.80, average training loss: 9782.30, base loss: 14400.31
[INFO 2017-06-28 13:34:58,111 main.py:51] epoch 4183, training loss: 8252.45, average training loss: 9781.51, base loss: 14399.99
[INFO 2017-06-28 13:34:58,783 main.py:51] epoch 4184, training loss: 9352.44, average training loss: 9782.02, base loss: 14403.82
[INFO 2017-06-28 13:34:59,446 main.py:51] epoch 4185, training loss: 8346.45, average training loss: 9781.56, base loss: 14404.97
[INFO 2017-06-28 13:35:00,104 main.py:51] epoch 4186, training loss: 10113.95, average training loss: 9782.09, base loss: 14405.94
[INFO 2017-06-28 13:35:00,768 main.py:51] epoch 4187, training loss: 11033.43, average training loss: 9784.11, base loss: 14411.19
[INFO 2017-06-28 13:35:01,429 main.py:51] epoch 4188, training loss: 10181.62, average training loss: 9783.97, base loss: 14411.24
[INFO 2017-06-28 13:35:02,106 main.py:51] epoch 4189, training loss: 9996.00, average training loss: 9784.12, base loss: 14410.94
[INFO 2017-06-28 13:35:02,780 main.py:51] epoch 4190, training loss: 8595.50, average training loss: 9782.64, base loss: 14410.33
[INFO 2017-06-28 13:35:03,430 main.py:51] epoch 4191, training loss: 10242.07, average training loss: 9784.15, base loss: 14413.40
[INFO 2017-06-28 13:35:04,103 main.py:51] epoch 4192, training loss: 9387.65, average training loss: 9784.73, base loss: 14414.65
[INFO 2017-06-28 13:35:04,783 main.py:51] epoch 4193, training loss: 8435.54, average training loss: 9782.57, base loss: 14412.15
[INFO 2017-06-28 13:35:05,451 main.py:51] epoch 4194, training loss: 10833.99, average training loss: 9783.90, base loss: 14411.57
[INFO 2017-06-28 13:35:06,093 main.py:51] epoch 4195, training loss: 9984.68, average training loss: 9783.99, base loss: 14411.77
[INFO 2017-06-28 13:35:06,732 main.py:51] epoch 4196, training loss: 11248.05, average training loss: 9787.06, base loss: 14417.11
[INFO 2017-06-28 13:35:07,390 main.py:51] epoch 4197, training loss: 10564.37, average training loss: 9788.27, base loss: 14418.10
[INFO 2017-06-28 13:35:08,043 main.py:51] epoch 4198, training loss: 8918.68, average training loss: 9786.95, base loss: 14417.05
[INFO 2017-06-28 13:35:08,683 main.py:51] epoch 4199, training loss: 9176.51, average training loss: 9786.60, base loss: 14417.82
[INFO 2017-06-28 13:35:08,684 main.py:53] epoch 4199, testing
[INFO 2017-06-28 13:35:11,276 main.py:105] average testing loss: 10467.18, base loss: 14646.47
[INFO 2017-06-28 13:35:11,276 main.py:106] improve_loss: 4179.29, improve_percent: 0.29
[INFO 2017-06-28 13:35:11,276 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:35:11,940 main.py:51] epoch 4200, training loss: 9365.43, average training loss: 9787.59, base loss: 14418.92
[INFO 2017-06-28 13:35:12,605 main.py:51] epoch 4201, training loss: 8422.95, average training loss: 9786.52, base loss: 14419.47
[INFO 2017-06-28 13:35:13,278 main.py:51] epoch 4202, training loss: 8437.49, average training loss: 9784.29, base loss: 14417.67
[INFO 2017-06-28 13:35:13,967 main.py:51] epoch 4203, training loss: 9391.46, average training loss: 9785.02, base loss: 14419.21
[INFO 2017-06-28 13:35:14,629 main.py:51] epoch 4204, training loss: 9225.72, average training loss: 9784.46, base loss: 14417.09
[INFO 2017-06-28 13:35:15,287 main.py:51] epoch 4205, training loss: 11835.18, average training loss: 9785.58, base loss: 14417.88
[INFO 2017-06-28 13:35:15,942 main.py:51] epoch 4206, training loss: 11423.56, average training loss: 9786.51, base loss: 14418.80
[INFO 2017-06-28 13:35:16,617 main.py:51] epoch 4207, training loss: 8431.12, average training loss: 9784.29, base loss: 14414.25
[INFO 2017-06-28 13:35:17,269 main.py:51] epoch 4208, training loss: 9843.37, average training loss: 9784.35, base loss: 14414.34
[INFO 2017-06-28 13:35:17,905 main.py:51] epoch 4209, training loss: 11075.52, average training loss: 9786.69, base loss: 14418.30
[INFO 2017-06-28 13:35:18,553 main.py:51] epoch 4210, training loss: 11057.43, average training loss: 9787.82, base loss: 14420.25
[INFO 2017-06-28 13:35:19,206 main.py:51] epoch 4211, training loss: 8808.16, average training loss: 9784.74, base loss: 14414.59
[INFO 2017-06-28 13:35:19,859 main.py:51] epoch 4212, training loss: 10549.67, average training loss: 9786.77, base loss: 14416.27
[INFO 2017-06-28 13:35:20,510 main.py:51] epoch 4213, training loss: 12368.34, average training loss: 9789.21, base loss: 14416.57
[INFO 2017-06-28 13:35:21,181 main.py:51] epoch 4214, training loss: 8668.55, average training loss: 9788.89, base loss: 14415.67
[INFO 2017-06-28 13:35:21,822 main.py:51] epoch 4215, training loss: 9937.40, average training loss: 9789.42, base loss: 14417.38
[INFO 2017-06-28 13:35:22,492 main.py:51] epoch 4216, training loss: 10317.20, average training loss: 9790.40, base loss: 14417.61
[INFO 2017-06-28 13:35:23,171 main.py:51] epoch 4217, training loss: 10439.05, average training loss: 9789.58, base loss: 14418.41
[INFO 2017-06-28 13:35:23,850 main.py:51] epoch 4218, training loss: 10283.23, average training loss: 9790.50, base loss: 14419.72
[INFO 2017-06-28 13:35:24,499 main.py:51] epoch 4219, training loss: 10691.83, average training loss: 9790.20, base loss: 14420.33
[INFO 2017-06-28 13:35:25,206 main.py:51] epoch 4220, training loss: 8855.28, average training loss: 9789.63, base loss: 14419.28
[INFO 2017-06-28 13:35:25,861 main.py:51] epoch 4221, training loss: 8519.49, average training loss: 9788.49, base loss: 14417.45
[INFO 2017-06-28 13:35:26,497 main.py:51] epoch 4222, training loss: 9524.10, average training loss: 9785.50, base loss: 14414.80
[INFO 2017-06-28 13:35:27,180 main.py:51] epoch 4223, training loss: 9131.97, average training loss: 9785.87, base loss: 14415.31
[INFO 2017-06-28 13:35:27,849 main.py:51] epoch 4224, training loss: 9344.41, average training loss: 9785.75, base loss: 14416.54
[INFO 2017-06-28 13:35:28,522 main.py:51] epoch 4225, training loss: 9913.84, average training loss: 9786.31, base loss: 14418.31
[INFO 2017-06-28 13:35:29,191 main.py:51] epoch 4226, training loss: 10004.01, average training loss: 9785.85, base loss: 14417.61
[INFO 2017-06-28 13:35:29,862 main.py:51] epoch 4227, training loss: 8196.00, average training loss: 9783.20, base loss: 14412.76
[INFO 2017-06-28 13:35:30,564 main.py:51] epoch 4228, training loss: 9061.74, average training loss: 9781.97, base loss: 14412.33
[INFO 2017-06-28 13:35:31,228 main.py:51] epoch 4229, training loss: 9328.47, average training loss: 9782.10, base loss: 14413.69
[INFO 2017-06-28 13:35:31,879 main.py:51] epoch 4230, training loss: 9016.32, average training loss: 9781.01, base loss: 14411.73
[INFO 2017-06-28 13:35:32,544 main.py:51] epoch 4231, training loss: 10547.85, average training loss: 9780.92, base loss: 14412.47
[INFO 2017-06-28 13:35:33,209 main.py:51] epoch 4232, training loss: 10399.49, average training loss: 9781.55, base loss: 14413.73
[INFO 2017-06-28 13:35:33,854 main.py:51] epoch 4233, training loss: 9627.48, average training loss: 9782.19, base loss: 14414.59
[INFO 2017-06-28 13:35:34,480 main.py:51] epoch 4234, training loss: 9642.73, average training loss: 9782.42, base loss: 14415.14
[INFO 2017-06-28 13:35:35,156 main.py:51] epoch 4235, training loss: 9855.85, average training loss: 9783.02, base loss: 14416.38
[INFO 2017-06-28 13:35:35,800 main.py:51] epoch 4236, training loss: 10578.75, average training loss: 9784.07, base loss: 14417.37
[INFO 2017-06-28 13:35:36,458 main.py:51] epoch 4237, training loss: 9960.62, average training loss: 9782.98, base loss: 14416.03
[INFO 2017-06-28 13:35:37,104 main.py:51] epoch 4238, training loss: 10180.24, average training loss: 9781.58, base loss: 14411.93
[INFO 2017-06-28 13:35:37,749 main.py:51] epoch 4239, training loss: 10453.59, average training loss: 9780.53, base loss: 14411.30
[INFO 2017-06-28 13:35:38,397 main.py:51] epoch 4240, training loss: 10393.66, average training loss: 9781.21, base loss: 14412.13
[INFO 2017-06-28 13:35:39,093 main.py:51] epoch 4241, training loss: 8831.16, average training loss: 9780.86, base loss: 14411.49
[INFO 2017-06-28 13:35:39,757 main.py:51] epoch 4242, training loss: 9692.78, average training loss: 9779.55, base loss: 14409.12
[INFO 2017-06-28 13:35:40,435 main.py:51] epoch 4243, training loss: 10916.20, average training loss: 9781.52, base loss: 14413.23
[INFO 2017-06-28 13:35:41,121 main.py:51] epoch 4244, training loss: 9429.66, average training loss: 9781.46, base loss: 14412.31
[INFO 2017-06-28 13:35:41,769 main.py:51] epoch 4245, training loss: 9169.76, average training loss: 9781.12, base loss: 14411.91
[INFO 2017-06-28 13:35:42,430 main.py:51] epoch 4246, training loss: 8819.60, average training loss: 9780.75, base loss: 14411.12
[INFO 2017-06-28 13:35:43,083 main.py:51] epoch 4247, training loss: 10335.61, average training loss: 9780.29, base loss: 14411.06
[INFO 2017-06-28 13:35:43,732 main.py:51] epoch 4248, training loss: 9448.87, average training loss: 9780.69, base loss: 14411.99
[INFO 2017-06-28 13:35:44,394 main.py:51] epoch 4249, training loss: 9741.88, average training loss: 9779.96, base loss: 14410.45
[INFO 2017-06-28 13:35:45,043 main.py:51] epoch 4250, training loss: 10629.46, average training loss: 9779.55, base loss: 14410.98
[INFO 2017-06-28 13:35:45,708 main.py:51] epoch 4251, training loss: 9056.39, average training loss: 9778.91, base loss: 14410.95
[INFO 2017-06-28 13:35:46,370 main.py:51] epoch 4252, training loss: 8954.97, average training loss: 9777.95, base loss: 14410.26
[INFO 2017-06-28 13:35:47,028 main.py:51] epoch 4253, training loss: 8796.29, average training loss: 9777.70, base loss: 14411.20
[INFO 2017-06-28 13:35:47,700 main.py:51] epoch 4254, training loss: 9809.18, average training loss: 9776.94, base loss: 14408.41
[INFO 2017-06-28 13:35:48,364 main.py:51] epoch 4255, training loss: 10083.73, average training loss: 9777.13, base loss: 14409.07
[INFO 2017-06-28 13:35:49,037 main.py:51] epoch 4256, training loss: 8499.82, average training loss: 9774.27, base loss: 14405.48
[INFO 2017-06-28 13:35:49,676 main.py:51] epoch 4257, training loss: 10964.48, average training loss: 9773.52, base loss: 14404.18
[INFO 2017-06-28 13:35:50,324 main.py:51] epoch 4258, training loss: 9484.91, average training loss: 9773.15, base loss: 14403.85
[INFO 2017-06-28 13:35:50,979 main.py:51] epoch 4259, training loss: 8970.30, average training loss: 9772.17, base loss: 14403.28
[INFO 2017-06-28 13:35:51,619 main.py:51] epoch 4260, training loss: 9206.78, average training loss: 9772.42, base loss: 14406.14
[INFO 2017-06-28 13:35:52,262 main.py:51] epoch 4261, training loss: 11996.14, average training loss: 9774.21, base loss: 14409.16
[INFO 2017-06-28 13:35:52,901 main.py:51] epoch 4262, training loss: 10216.67, average training loss: 9772.80, base loss: 14408.11
[INFO 2017-06-28 13:35:53,550 main.py:51] epoch 4263, training loss: 8729.61, average training loss: 9771.37, base loss: 14405.94
[INFO 2017-06-28 13:35:54,216 main.py:51] epoch 4264, training loss: 8576.86, average training loss: 9769.53, base loss: 14404.08
[INFO 2017-06-28 13:35:54,872 main.py:51] epoch 4265, training loss: 9510.90, average training loss: 9770.73, base loss: 14406.24
[INFO 2017-06-28 13:35:55,533 main.py:51] epoch 4266, training loss: 9848.01, average training loss: 9769.82, base loss: 14405.16
[INFO 2017-06-28 13:35:56,171 main.py:51] epoch 4267, training loss: 8534.49, average training loss: 9767.84, base loss: 14402.27
[INFO 2017-06-28 13:35:56,826 main.py:51] epoch 4268, training loss: 10242.55, average training loss: 9769.50, base loss: 14405.41
[INFO 2017-06-28 13:35:57,495 main.py:51] epoch 4269, training loss: 9298.69, average training loss: 9768.64, base loss: 14404.98
[INFO 2017-06-28 13:35:58,153 main.py:51] epoch 4270, training loss: 9470.82, average training loss: 9767.32, base loss: 14403.43
[INFO 2017-06-28 13:35:58,822 main.py:51] epoch 4271, training loss: 8944.76, average training loss: 9765.89, base loss: 14403.38
[INFO 2017-06-28 13:35:59,464 main.py:51] epoch 4272, training loss: 11311.93, average training loss: 9767.52, base loss: 14406.00
[INFO 2017-06-28 13:36:00,146 main.py:51] epoch 4273, training loss: 10010.99, average training loss: 9765.38, base loss: 14404.15
[INFO 2017-06-28 13:36:00,795 main.py:51] epoch 4274, training loss: 11750.60, average training loss: 9767.65, base loss: 14404.66
[INFO 2017-06-28 13:36:01,461 main.py:51] epoch 4275, training loss: 10585.91, average training loss: 9767.67, base loss: 14404.04
[INFO 2017-06-28 13:36:02,117 main.py:51] epoch 4276, training loss: 10431.87, average training loss: 9768.70, base loss: 14404.73
[INFO 2017-06-28 13:36:02,789 main.py:51] epoch 4277, training loss: 8258.22, average training loss: 9766.66, base loss: 14403.04
[INFO 2017-06-28 13:36:03,449 main.py:51] epoch 4278, training loss: 9202.99, average training loss: 9763.52, base loss: 14398.17
[INFO 2017-06-28 13:36:04,106 main.py:51] epoch 4279, training loss: 8837.59, average training loss: 9762.53, base loss: 14396.33
[INFO 2017-06-28 13:36:04,775 main.py:51] epoch 4280, training loss: 9884.57, average training loss: 9761.87, base loss: 14395.77
[INFO 2017-06-28 13:36:05,436 main.py:51] epoch 4281, training loss: 9071.02, average training loss: 9760.19, base loss: 14394.43
[INFO 2017-06-28 13:36:06,075 main.py:51] epoch 4282, training loss: 9167.44, average training loss: 9759.84, base loss: 14395.01
[INFO 2017-06-28 13:36:06,734 main.py:51] epoch 4283, training loss: 9944.20, average training loss: 9760.37, base loss: 14396.71
[INFO 2017-06-28 13:36:07,385 main.py:51] epoch 4284, training loss: 9970.49, average training loss: 9761.19, base loss: 14398.71
[INFO 2017-06-28 13:36:08,060 main.py:51] epoch 4285, training loss: 8943.16, average training loss: 9759.92, base loss: 14397.02
[INFO 2017-06-28 13:36:08,731 main.py:51] epoch 4286, training loss: 8241.37, average training loss: 9758.85, base loss: 14395.45
[INFO 2017-06-28 13:36:09,393 main.py:51] epoch 4287, training loss: 11726.51, average training loss: 9760.59, base loss: 14397.62
[INFO 2017-06-28 13:36:10,056 main.py:51] epoch 4288, training loss: 9147.52, average training loss: 9759.63, base loss: 14395.22
[INFO 2017-06-28 13:36:10,705 main.py:51] epoch 4289, training loss: 8778.98, average training loss: 9758.08, base loss: 14393.04
[INFO 2017-06-28 13:36:11,373 main.py:51] epoch 4290, training loss: 9809.83, average training loss: 9757.37, base loss: 14392.51
[INFO 2017-06-28 13:36:12,038 main.py:51] epoch 4291, training loss: 8279.41, average training loss: 9756.38, base loss: 14390.79
[INFO 2017-06-28 13:36:12,730 main.py:51] epoch 4292, training loss: 9643.73, average training loss: 9757.71, base loss: 14393.64
[INFO 2017-06-28 13:36:13,384 main.py:51] epoch 4293, training loss: 9223.96, average training loss: 9757.36, base loss: 14393.68
[INFO 2017-06-28 13:36:14,052 main.py:51] epoch 4294, training loss: 10979.64, average training loss: 9758.45, base loss: 14395.16
[INFO 2017-06-28 13:36:14,715 main.py:51] epoch 4295, training loss: 8122.03, average training loss: 9756.36, base loss: 14392.66
[INFO 2017-06-28 13:36:15,402 main.py:51] epoch 4296, training loss: 9975.91, average training loss: 9755.52, base loss: 14390.75
[INFO 2017-06-28 13:36:16,063 main.py:51] epoch 4297, training loss: 9241.37, average training loss: 9754.30, base loss: 14389.38
[INFO 2017-06-28 13:36:16,730 main.py:51] epoch 4298, training loss: 8250.72, average training loss: 9753.25, base loss: 14387.54
[INFO 2017-06-28 13:36:17,393 main.py:51] epoch 4299, training loss: 10241.24, average training loss: 9754.43, base loss: 14389.38
[INFO 2017-06-28 13:36:17,393 main.py:53] epoch 4299, testing
[INFO 2017-06-28 13:36:20,015 main.py:105] average testing loss: 11493.30, base loss: 16164.08
[INFO 2017-06-28 13:36:20,015 main.py:106] improve_loss: 4670.79, improve_percent: 0.29
[INFO 2017-06-28 13:36:20,015 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:36:20,654 main.py:51] epoch 4300, training loss: 10945.70, average training loss: 9754.75, base loss: 14389.53
[INFO 2017-06-28 13:36:21,336 main.py:51] epoch 4301, training loss: 9389.88, average training loss: 9754.33, base loss: 14389.39
[INFO 2017-06-28 13:36:22,009 main.py:51] epoch 4302, training loss: 9727.12, average training loss: 9755.26, base loss: 14391.74
[INFO 2017-06-28 13:36:22,694 main.py:51] epoch 4303, training loss: 8911.40, average training loss: 9754.16, base loss: 14387.47
[INFO 2017-06-28 13:36:23,350 main.py:51] epoch 4304, training loss: 10802.10, average training loss: 9756.78, base loss: 14390.97
[INFO 2017-06-28 13:36:24,009 main.py:51] epoch 4305, training loss: 9960.62, average training loss: 9755.94, base loss: 14389.18
[INFO 2017-06-28 13:36:24,680 main.py:51] epoch 4306, training loss: 9098.75, average training loss: 9754.38, base loss: 14387.38
[INFO 2017-06-28 13:36:25,343 main.py:51] epoch 4307, training loss: 9084.47, average training loss: 9753.73, base loss: 14386.14
[INFO 2017-06-28 13:36:26,007 main.py:51] epoch 4308, training loss: 10249.97, average training loss: 9752.86, base loss: 14384.74
[INFO 2017-06-28 13:36:26,683 main.py:51] epoch 4309, training loss: 9320.24, average training loss: 9752.24, base loss: 14383.26
[INFO 2017-06-28 13:36:27,343 main.py:51] epoch 4310, training loss: 9649.95, average training loss: 9752.22, base loss: 14384.74
[INFO 2017-06-28 13:36:28,010 main.py:51] epoch 4311, training loss: 8899.54, average training loss: 9752.49, base loss: 14384.04
[INFO 2017-06-28 13:36:28,675 main.py:51] epoch 4312, training loss: 9205.84, average training loss: 9753.27, base loss: 14385.51
[INFO 2017-06-28 13:36:29,329 main.py:51] epoch 4313, training loss: 10097.23, average training loss: 9753.96, base loss: 14385.82
[INFO 2017-06-28 13:36:29,989 main.py:51] epoch 4314, training loss: 8162.96, average training loss: 9752.61, base loss: 14382.59
[INFO 2017-06-28 13:36:30,631 main.py:51] epoch 4315, training loss: 9329.29, average training loss: 9753.01, base loss: 14384.65
[INFO 2017-06-28 13:36:31,302 main.py:51] epoch 4316, training loss: 8498.10, average training loss: 9751.16, base loss: 14382.92
[INFO 2017-06-28 13:36:31,959 main.py:51] epoch 4317, training loss: 10176.40, average training loss: 9750.73, base loss: 14383.31
[INFO 2017-06-28 13:36:32,616 main.py:51] epoch 4318, training loss: 11360.10, average training loss: 9752.30, base loss: 14386.04
[INFO 2017-06-28 13:36:33,286 main.py:51] epoch 4319, training loss: 9889.75, average training loss: 9753.67, base loss: 14387.64
[INFO 2017-06-28 13:36:33,938 main.py:51] epoch 4320, training loss: 9367.40, average training loss: 9752.59, base loss: 14387.31
[INFO 2017-06-28 13:36:34,636 main.py:51] epoch 4321, training loss: 10725.55, average training loss: 9753.57, base loss: 14390.29
[INFO 2017-06-28 13:36:35,296 main.py:51] epoch 4322, training loss: 9305.16, average training loss: 9751.71, base loss: 14385.89
[INFO 2017-06-28 13:36:35,957 main.py:51] epoch 4323, training loss: 10009.26, average training loss: 9753.09, base loss: 14387.83
[INFO 2017-06-28 13:36:36,615 main.py:51] epoch 4324, training loss: 9217.62, average training loss: 9751.50, base loss: 14386.39
[INFO 2017-06-28 13:36:37,282 main.py:51] epoch 4325, training loss: 10215.98, average training loss: 9752.20, base loss: 14386.78
[INFO 2017-06-28 13:36:37,955 main.py:51] epoch 4326, training loss: 10149.88, average training loss: 9752.39, base loss: 14388.71
[INFO 2017-06-28 13:36:38,624 main.py:51] epoch 4327, training loss: 8518.94, average training loss: 9749.84, base loss: 14385.64
[INFO 2017-06-28 13:36:39,290 main.py:51] epoch 4328, training loss: 8936.80, average training loss: 9746.55, base loss: 14380.43
[INFO 2017-06-28 13:36:39,960 main.py:51] epoch 4329, training loss: 8902.23, average training loss: 9746.32, base loss: 14379.72
[INFO 2017-06-28 13:36:40,625 main.py:51] epoch 4330, training loss: 8633.89, average training loss: 9744.66, base loss: 14377.81
[INFO 2017-06-28 13:36:41,294 main.py:51] epoch 4331, training loss: 10380.27, average training loss: 9745.51, base loss: 14380.13
[INFO 2017-06-28 13:36:41,949 main.py:51] epoch 4332, training loss: 9443.64, average training loss: 9745.02, base loss: 14377.63
[INFO 2017-06-28 13:36:42,609 main.py:51] epoch 4333, training loss: 9502.02, average training loss: 9744.10, base loss: 14376.15
[INFO 2017-06-28 13:36:43,296 main.py:51] epoch 4334, training loss: 8645.10, average training loss: 9742.87, base loss: 14375.87
[INFO 2017-06-28 13:36:43,971 main.py:51] epoch 4335, training loss: 8679.51, average training loss: 9741.51, base loss: 14374.37
[INFO 2017-06-28 13:36:44,627 main.py:51] epoch 4336, training loss: 9236.93, average training loss: 9741.78, base loss: 14375.10
[INFO 2017-06-28 13:36:45,275 main.py:51] epoch 4337, training loss: 8319.69, average training loss: 9740.28, base loss: 14373.04
[INFO 2017-06-28 13:36:45,944 main.py:51] epoch 4338, training loss: 10939.61, average training loss: 9741.67, base loss: 14374.92
[INFO 2017-06-28 13:36:46,591 main.py:51] epoch 4339, training loss: 10740.14, average training loss: 9742.93, base loss: 14377.60
[INFO 2017-06-28 13:36:47,247 main.py:51] epoch 4340, training loss: 10647.00, average training loss: 9742.05, base loss: 14377.62
[INFO 2017-06-28 13:36:47,895 main.py:51] epoch 4341, training loss: 10083.08, average training loss: 9741.80, base loss: 14376.16
[INFO 2017-06-28 13:36:48,593 main.py:51] epoch 4342, training loss: 10291.70, average training loss: 9741.09, base loss: 14377.43
[INFO 2017-06-28 13:36:49,294 main.py:51] epoch 4343, training loss: 9686.41, average training loss: 9741.93, base loss: 14380.60
[INFO 2017-06-28 13:36:49,970 main.py:51] epoch 4344, training loss: 9098.57, average training loss: 9741.51, base loss: 14380.86
[INFO 2017-06-28 13:36:50,628 main.py:51] epoch 4345, training loss: 10401.45, average training loss: 9741.71, base loss: 14380.99
[INFO 2017-06-28 13:36:51,304 main.py:51] epoch 4346, training loss: 10438.30, average training loss: 9741.82, base loss: 14382.85
[INFO 2017-06-28 13:36:51,969 main.py:51] epoch 4347, training loss: 10853.74, average training loss: 9742.74, base loss: 14383.52
[INFO 2017-06-28 13:36:52,614 main.py:51] epoch 4348, training loss: 10155.32, average training loss: 9743.35, base loss: 14384.40
[INFO 2017-06-28 13:36:53,284 main.py:51] epoch 4349, training loss: 11440.74, average training loss: 9745.29, base loss: 14389.16
[INFO 2017-06-28 13:36:53,972 main.py:51] epoch 4350, training loss: 10237.47, average training loss: 9746.63, base loss: 14392.41
[INFO 2017-06-28 13:36:54,649 main.py:51] epoch 4351, training loss: 8956.10, average training loss: 9744.86, base loss: 14390.16
[INFO 2017-06-28 13:36:55,307 main.py:51] epoch 4352, training loss: 9315.37, average training loss: 9743.92, base loss: 14388.30
[INFO 2017-06-28 13:36:55,960 main.py:51] epoch 4353, training loss: 9761.67, average training loss: 9743.48, base loss: 14386.64
[INFO 2017-06-28 13:36:56,644 main.py:51] epoch 4354, training loss: 8641.51, average training loss: 9742.64, base loss: 14387.14
[INFO 2017-06-28 13:36:57,305 main.py:51] epoch 4355, training loss: 11981.81, average training loss: 9745.24, base loss: 14392.13
[INFO 2017-06-28 13:36:57,957 main.py:51] epoch 4356, training loss: 9365.48, average training loss: 9742.95, base loss: 14388.60
[INFO 2017-06-28 13:36:58,610 main.py:51] epoch 4357, training loss: 10130.71, average training loss: 9743.44, base loss: 14390.32
[INFO 2017-06-28 13:36:59,273 main.py:51] epoch 4358, training loss: 9729.49, average training loss: 9743.02, base loss: 14388.84
[INFO 2017-06-28 13:36:59,929 main.py:51] epoch 4359, training loss: 9798.12, average training loss: 9743.30, base loss: 14389.40
[INFO 2017-06-28 13:37:00,593 main.py:51] epoch 4360, training loss: 8930.75, average training loss: 9743.53, base loss: 14390.36
[INFO 2017-06-28 13:37:01,266 main.py:51] epoch 4361, training loss: 8753.18, average training loss: 9742.67, base loss: 14388.05
[INFO 2017-06-28 13:37:01,931 main.py:51] epoch 4362, training loss: 9083.88, average training loss: 9739.91, base loss: 14382.21
[INFO 2017-06-28 13:37:02,602 main.py:51] epoch 4363, training loss: 8944.03, average training loss: 9739.48, base loss: 14381.27
[INFO 2017-06-28 13:37:03,261 main.py:51] epoch 4364, training loss: 10932.10, average training loss: 9741.37, base loss: 14382.65
[INFO 2017-06-28 13:37:03,916 main.py:51] epoch 4365, training loss: 9514.44, average training loss: 9741.42, base loss: 14382.62
[INFO 2017-06-28 13:37:04,553 main.py:51] epoch 4366, training loss: 10189.75, average training loss: 9742.58, base loss: 14384.73
[INFO 2017-06-28 13:37:05,226 main.py:51] epoch 4367, training loss: 10632.44, average training loss: 9742.56, base loss: 14384.77
[INFO 2017-06-28 13:37:05,889 main.py:51] epoch 4368, training loss: 8590.70, average training loss: 9742.17, base loss: 14384.59
[INFO 2017-06-28 13:37:06,535 main.py:51] epoch 4369, training loss: 9875.88, average training loss: 9741.33, base loss: 14382.92
[INFO 2017-06-28 13:37:07,208 main.py:51] epoch 4370, training loss: 11201.44, average training loss: 9741.30, base loss: 14383.80
[INFO 2017-06-28 13:37:07,867 main.py:51] epoch 4371, training loss: 10229.84, average training loss: 9743.26, base loss: 14387.51
[INFO 2017-06-28 13:37:08,524 main.py:51] epoch 4372, training loss: 9954.54, average training loss: 9743.12, base loss: 14388.60
[INFO 2017-06-28 13:37:09,201 main.py:51] epoch 4373, training loss: 11646.53, average training loss: 9745.33, base loss: 14392.18
[INFO 2017-06-28 13:37:09,871 main.py:51] epoch 4374, training loss: 8876.13, average training loss: 9745.71, base loss: 14393.47
[INFO 2017-06-28 13:37:10,517 main.py:51] epoch 4375, training loss: 9723.54, average training loss: 9745.34, base loss: 14390.71
[INFO 2017-06-28 13:37:11,165 main.py:51] epoch 4376, training loss: 10086.54, average training loss: 9745.01, base loss: 14390.31
[INFO 2017-06-28 13:37:11,802 main.py:51] epoch 4377, training loss: 9853.85, average training loss: 9745.25, base loss: 14390.51
[INFO 2017-06-28 13:37:12,457 main.py:51] epoch 4378, training loss: 9801.58, average training loss: 9745.50, base loss: 14390.95
[INFO 2017-06-28 13:37:13,104 main.py:51] epoch 4379, training loss: 9488.23, average training loss: 9744.51, base loss: 14390.41
[INFO 2017-06-28 13:37:13,753 main.py:51] epoch 4380, training loss: 9975.92, average training loss: 9744.32, base loss: 14391.25
[INFO 2017-06-28 13:37:14,414 main.py:51] epoch 4381, training loss: 9847.51, average training loss: 9745.38, base loss: 14391.53
[INFO 2017-06-28 13:37:15,059 main.py:51] epoch 4382, training loss: 10188.43, average training loss: 9745.57, base loss: 14391.85
[INFO 2017-06-28 13:37:15,740 main.py:51] epoch 4383, training loss: 10069.72, average training loss: 9745.40, base loss: 14392.33
[INFO 2017-06-28 13:37:16,388 main.py:51] epoch 4384, training loss: 10178.25, average training loss: 9746.55, base loss: 14394.00
[INFO 2017-06-28 13:37:17,067 main.py:51] epoch 4385, training loss: 8559.80, average training loss: 9743.76, base loss: 14388.69
[INFO 2017-06-28 13:37:17,730 main.py:51] epoch 4386, training loss: 9792.89, average training loss: 9743.03, base loss: 14388.87
[INFO 2017-06-28 13:37:18,366 main.py:51] epoch 4387, training loss: 10079.52, average training loss: 9742.79, base loss: 14388.10
[INFO 2017-06-28 13:37:19,035 main.py:51] epoch 4388, training loss: 10037.37, average training loss: 9743.12, base loss: 14386.80
[INFO 2017-06-28 13:37:19,686 main.py:51] epoch 4389, training loss: 9867.13, average training loss: 9744.10, base loss: 14387.88
[INFO 2017-06-28 13:37:20,333 main.py:51] epoch 4390, training loss: 10286.13, average training loss: 9745.36, base loss: 14391.02
[INFO 2017-06-28 13:37:21,023 main.py:51] epoch 4391, training loss: 8595.37, average training loss: 9744.06, base loss: 14388.76
[INFO 2017-06-28 13:37:21,683 main.py:51] epoch 4392, training loss: 9781.16, average training loss: 9743.74, base loss: 14387.27
[INFO 2017-06-28 13:37:22,316 main.py:51] epoch 4393, training loss: 9930.39, average training loss: 9742.82, base loss: 14385.08
[INFO 2017-06-28 13:37:22,954 main.py:51] epoch 4394, training loss: 9262.55, average training loss: 9740.78, base loss: 14382.74
[INFO 2017-06-28 13:37:23,604 main.py:51] epoch 4395, training loss: 9665.98, average training loss: 9740.80, base loss: 14381.61
[INFO 2017-06-28 13:37:24,297 main.py:51] epoch 4396, training loss: 10487.44, average training loss: 9740.80, base loss: 14382.91
[INFO 2017-06-28 13:37:24,962 main.py:51] epoch 4397, training loss: 10002.38, average training loss: 9741.78, base loss: 14385.98
[INFO 2017-06-28 13:37:25,637 main.py:51] epoch 4398, training loss: 11061.25, average training loss: 9744.88, base loss: 14390.45
[INFO 2017-06-28 13:37:26,320 main.py:51] epoch 4399, training loss: 10073.18, average training loss: 9745.02, base loss: 14391.66
[INFO 2017-06-28 13:37:26,320 main.py:53] epoch 4399, testing
[INFO 2017-06-28 13:37:28,923 main.py:105] average testing loss: 11209.87, base loss: 15674.61
[INFO 2017-06-28 13:37:28,923 main.py:106] improve_loss: 4464.74, improve_percent: 0.28
[INFO 2017-06-28 13:37:28,924 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:37:29,577 main.py:51] epoch 4400, training loss: 8169.79, average training loss: 9743.65, base loss: 14390.09
[INFO 2017-06-28 13:37:30,259 main.py:51] epoch 4401, training loss: 9496.34, average training loss: 9743.82, base loss: 14390.97
[INFO 2017-06-28 13:37:30,937 main.py:51] epoch 4402, training loss: 10030.08, average training loss: 9743.22, base loss: 14390.94
[INFO 2017-06-28 13:37:31,595 main.py:51] epoch 4403, training loss: 8971.34, average training loss: 9742.68, base loss: 14391.00
[INFO 2017-06-28 13:37:32,253 main.py:51] epoch 4404, training loss: 11730.60, average training loss: 9745.39, base loss: 14394.74
[INFO 2017-06-28 13:37:32,923 main.py:51] epoch 4405, training loss: 8995.01, average training loss: 9745.56, base loss: 14394.49
[INFO 2017-06-28 13:37:33,584 main.py:51] epoch 4406, training loss: 8907.24, average training loss: 9745.80, base loss: 14396.55
[INFO 2017-06-28 13:37:34,246 main.py:51] epoch 4407, training loss: 10514.01, average training loss: 9746.59, base loss: 14396.84
[INFO 2017-06-28 13:37:34,911 main.py:51] epoch 4408, training loss: 10039.62, average training loss: 9747.21, base loss: 14398.92
[INFO 2017-06-28 13:37:35,570 main.py:51] epoch 4409, training loss: 9615.49, average training loss: 9746.47, base loss: 14396.73
[INFO 2017-06-28 13:37:36,235 main.py:51] epoch 4410, training loss: 10379.91, average training loss: 9746.22, base loss: 14396.46
[INFO 2017-06-28 13:37:36,900 main.py:51] epoch 4411, training loss: 9125.34, average training loss: 9745.52, base loss: 14393.49
[INFO 2017-06-28 13:37:37,564 main.py:51] epoch 4412, training loss: 8634.63, average training loss: 9743.10, base loss: 14389.68
[INFO 2017-06-28 13:37:38,212 main.py:51] epoch 4413, training loss: 10676.92, average training loss: 9745.11, base loss: 14392.49
[INFO 2017-06-28 13:37:38,878 main.py:51] epoch 4414, training loss: 9744.44, average training loss: 9744.66, base loss: 14392.14
[INFO 2017-06-28 13:37:39,556 main.py:51] epoch 4415, training loss: 8552.37, average training loss: 9744.01, base loss: 14391.01
[INFO 2017-06-28 13:37:40,194 main.py:51] epoch 4416, training loss: 8664.89, average training loss: 9743.10, base loss: 14389.05
[INFO 2017-06-28 13:37:40,866 main.py:51] epoch 4417, training loss: 10911.65, average training loss: 9743.63, base loss: 14390.19
[INFO 2017-06-28 13:37:41,508 main.py:51] epoch 4418, training loss: 9548.94, average training loss: 9743.87, base loss: 14391.16
[INFO 2017-06-28 13:37:42,162 main.py:51] epoch 4419, training loss: 10490.74, average training loss: 9743.67, base loss: 14392.79
[INFO 2017-06-28 13:37:42,811 main.py:51] epoch 4420, training loss: 8813.94, average training loss: 9743.32, base loss: 14392.71
[INFO 2017-06-28 13:37:43,448 main.py:51] epoch 4421, training loss: 9774.64, average training loss: 9743.97, base loss: 14394.10
[INFO 2017-06-28 13:37:44,125 main.py:51] epoch 4422, training loss: 9129.90, average training loss: 9744.14, base loss: 14394.20
[INFO 2017-06-28 13:37:44,786 main.py:51] epoch 4423, training loss: 10134.58, average training loss: 9743.35, base loss: 14393.57
[INFO 2017-06-28 13:37:45,449 main.py:51] epoch 4424, training loss: 9450.75, average training loss: 9744.46, base loss: 14394.74
[INFO 2017-06-28 13:37:46,122 main.py:51] epoch 4425, training loss: 9039.63, average training loss: 9742.55, base loss: 14391.50
[INFO 2017-06-28 13:37:46,781 main.py:51] epoch 4426, training loss: 9939.32, average training loss: 9743.08, base loss: 14393.00
[INFO 2017-06-28 13:37:47,450 main.py:51] epoch 4427, training loss: 9213.33, average training loss: 9741.76, base loss: 14391.95
[INFO 2017-06-28 13:37:48,092 main.py:51] epoch 4428, training loss: 9945.96, average training loss: 9743.10, base loss: 14395.07
[INFO 2017-06-28 13:37:48,778 main.py:51] epoch 4429, training loss: 10258.80, average training loss: 9743.95, base loss: 14397.03
[INFO 2017-06-28 13:37:49,448 main.py:51] epoch 4430, training loss: 9514.00, average training loss: 9744.70, base loss: 14399.08
[INFO 2017-06-28 13:37:50,131 main.py:51] epoch 4431, training loss: 9391.00, average training loss: 9744.58, base loss: 14398.89
[INFO 2017-06-28 13:37:50,779 main.py:51] epoch 4432, training loss: 11141.81, average training loss: 9746.49, base loss: 14401.59
[INFO 2017-06-28 13:37:51,437 main.py:51] epoch 4433, training loss: 9103.02, average training loss: 9746.45, base loss: 14401.56
[INFO 2017-06-28 13:37:52,099 main.py:51] epoch 4434, training loss: 9641.68, average training loss: 9745.18, base loss: 14400.38
[INFO 2017-06-28 13:37:52,753 main.py:51] epoch 4435, training loss: 9030.69, average training loss: 9743.31, base loss: 14396.61
[INFO 2017-06-28 13:37:53,411 main.py:51] epoch 4436, training loss: 10521.11, average training loss: 9745.88, base loss: 14402.29
[INFO 2017-06-28 13:37:54,063 main.py:51] epoch 4437, training loss: 9675.41, average training loss: 9745.52, base loss: 14401.48
[INFO 2017-06-28 13:37:54,706 main.py:51] epoch 4438, training loss: 8604.44, average training loss: 9745.39, base loss: 14401.26
[INFO 2017-06-28 13:37:55,385 main.py:51] epoch 4439, training loss: 9167.58, average training loss: 9745.70, base loss: 14401.54
[INFO 2017-06-28 13:37:56,054 main.py:51] epoch 4440, training loss: 9743.86, average training loss: 9745.91, base loss: 14402.72
[INFO 2017-06-28 13:37:56,702 main.py:51] epoch 4441, training loss: 9082.56, average training loss: 9747.09, base loss: 14404.75
[INFO 2017-06-28 13:37:57,356 main.py:51] epoch 4442, training loss: 10294.01, average training loss: 9747.28, base loss: 14403.89
[INFO 2017-06-28 13:37:58,022 main.py:51] epoch 4443, training loss: 9544.70, average training loss: 9747.95, base loss: 14405.49
[INFO 2017-06-28 13:37:58,661 main.py:51] epoch 4444, training loss: 10458.67, average training loss: 9749.27, base loss: 14406.95
[INFO 2017-06-28 13:37:59,310 main.py:51] epoch 4445, training loss: 9945.20, average training loss: 9748.24, base loss: 14405.92
[INFO 2017-06-28 13:37:59,980 main.py:51] epoch 4446, training loss: 10046.03, average training loss: 9748.45, base loss: 14405.96
[INFO 2017-06-28 13:38:00,640 main.py:51] epoch 4447, training loss: 9661.79, average training loss: 9748.54, base loss: 14406.53
[INFO 2017-06-28 13:38:01,295 main.py:51] epoch 4448, training loss: 9863.08, average training loss: 9748.02, base loss: 14405.68
[INFO 2017-06-28 13:38:01,947 main.py:51] epoch 4449, training loss: 9258.72, average training loss: 9749.06, base loss: 14406.38
[INFO 2017-06-28 13:38:02,615 main.py:51] epoch 4450, training loss: 10204.90, average training loss: 9750.68, base loss: 14409.41
[INFO 2017-06-28 13:38:03,313 main.py:51] epoch 4451, training loss: 9790.55, average training loss: 9751.29, base loss: 14411.30
[INFO 2017-06-28 13:38:03,958 main.py:51] epoch 4452, training loss: 9468.60, average training loss: 9749.85, base loss: 14409.84
[INFO 2017-06-28 13:38:04,641 main.py:51] epoch 4453, training loss: 9143.70, average training loss: 9748.10, base loss: 14407.45
[INFO 2017-06-28 13:38:05,296 main.py:51] epoch 4454, training loss: 9555.02, average training loss: 9747.74, base loss: 14406.66
[INFO 2017-06-28 13:38:05,940 main.py:51] epoch 4455, training loss: 10126.89, average training loss: 9747.16, base loss: 14404.92
[INFO 2017-06-28 13:38:06,591 main.py:51] epoch 4456, training loss: 10209.24, average training loss: 9746.32, base loss: 14405.34
[INFO 2017-06-28 13:38:07,247 main.py:51] epoch 4457, training loss: 10164.22, average training loss: 9746.58, base loss: 14404.79
[INFO 2017-06-28 13:38:07,901 main.py:51] epoch 4458, training loss: 10596.49, average training loss: 9747.95, base loss: 14406.92
[INFO 2017-06-28 13:38:08,561 main.py:51] epoch 4459, training loss: 10466.42, average training loss: 9748.20, base loss: 14406.64
[INFO 2017-06-28 13:38:09,219 main.py:51] epoch 4460, training loss: 10792.25, average training loss: 9747.18, base loss: 14404.52
[INFO 2017-06-28 13:38:09,863 main.py:51] epoch 4461, training loss: 8847.37, average training loss: 9746.74, base loss: 14403.05
[INFO 2017-06-28 13:38:10,514 main.py:51] epoch 4462, training loss: 8621.60, average training loss: 9745.31, base loss: 14401.62
[INFO 2017-06-28 13:38:11,196 main.py:51] epoch 4463, training loss: 10683.27, average training loss: 9744.88, base loss: 14401.01
[INFO 2017-06-28 13:38:11,848 main.py:51] epoch 4464, training loss: 9064.97, average training loss: 9745.36, base loss: 14401.71
[INFO 2017-06-28 13:38:12,512 main.py:51] epoch 4465, training loss: 9467.62, average training loss: 9744.46, base loss: 14400.38
[INFO 2017-06-28 13:38:13,163 main.py:51] epoch 4466, training loss: 8177.38, average training loss: 9743.18, base loss: 14399.20
[INFO 2017-06-28 13:38:13,812 main.py:51] epoch 4467, training loss: 10092.44, average training loss: 9743.72, base loss: 14400.23
[INFO 2017-06-28 13:38:14,461 main.py:51] epoch 4468, training loss: 9923.56, average training loss: 9744.08, base loss: 14400.04
[INFO 2017-06-28 13:38:15,145 main.py:51] epoch 4469, training loss: 9478.59, average training loss: 9743.85, base loss: 14400.91
[INFO 2017-06-28 13:38:15,801 main.py:51] epoch 4470, training loss: 10714.20, average training loss: 9746.02, base loss: 14403.76
[INFO 2017-06-28 13:38:16,466 main.py:51] epoch 4471, training loss: 9229.86, average training loss: 9744.75, base loss: 14403.92
[INFO 2017-06-28 13:38:17,145 main.py:51] epoch 4472, training loss: 9095.75, average training loss: 9744.40, base loss: 14404.08
[INFO 2017-06-28 13:38:17,809 main.py:51] epoch 4473, training loss: 9105.46, average training loss: 9742.92, base loss: 14402.03
[INFO 2017-06-28 13:38:18,454 main.py:51] epoch 4474, training loss: 10492.39, average training loss: 9743.01, base loss: 14400.86
[INFO 2017-06-28 13:38:19,104 main.py:51] epoch 4475, training loss: 8413.48, average training loss: 9741.44, base loss: 14397.17
[INFO 2017-06-28 13:38:19,752 main.py:51] epoch 4476, training loss: 10527.35, average training loss: 9741.24, base loss: 14397.98
[INFO 2017-06-28 13:38:20,401 main.py:51] epoch 4477, training loss: 9798.38, average training loss: 9741.58, base loss: 14398.50
[INFO 2017-06-28 13:38:21,071 main.py:51] epoch 4478, training loss: 10131.82, average training loss: 9741.44, base loss: 14399.90
[INFO 2017-06-28 13:38:21,738 main.py:51] epoch 4479, training loss: 8296.55, average training loss: 9737.67, base loss: 14396.07
[INFO 2017-06-28 13:38:22,395 main.py:51] epoch 4480, training loss: 9894.19, average training loss: 9738.05, base loss: 14397.23
[INFO 2017-06-28 13:38:23,043 main.py:51] epoch 4481, training loss: 9465.64, average training loss: 9738.37, base loss: 14398.12
[INFO 2017-06-28 13:38:23,708 main.py:51] epoch 4482, training loss: 10364.05, average training loss: 9739.87, base loss: 14400.38
[INFO 2017-06-28 13:38:24,367 main.py:51] epoch 4483, training loss: 9768.50, average training loss: 9741.13, base loss: 14401.24
[INFO 2017-06-28 13:38:25,027 main.py:51] epoch 4484, training loss: 11203.06, average training loss: 9743.26, base loss: 14405.57
[INFO 2017-06-28 13:38:25,676 main.py:51] epoch 4485, training loss: 8911.39, average training loss: 9742.37, base loss: 14405.83
[INFO 2017-06-28 13:38:26,330 main.py:51] epoch 4486, training loss: 9392.25, average training loss: 9742.88, base loss: 14407.62
[INFO 2017-06-28 13:38:26,973 main.py:51] epoch 4487, training loss: 10509.82, average training loss: 9743.98, base loss: 14408.67
[INFO 2017-06-28 13:38:27,635 main.py:51] epoch 4488, training loss: 9054.10, average training loss: 9743.73, base loss: 14408.66
[INFO 2017-06-28 13:38:28,296 main.py:51] epoch 4489, training loss: 10714.73, average training loss: 9743.82, base loss: 14408.24
[INFO 2017-06-28 13:38:28,951 main.py:51] epoch 4490, training loss: 10696.44, average training loss: 9743.42, base loss: 14406.24
[INFO 2017-06-28 13:38:29,611 main.py:51] epoch 4491, training loss: 9059.34, average training loss: 9741.52, base loss: 14403.29
[INFO 2017-06-28 13:38:30,274 main.py:51] epoch 4492, training loss: 10282.42, average training loss: 9742.41, base loss: 14404.61
[INFO 2017-06-28 13:38:30,936 main.py:51] epoch 4493, training loss: 11185.61, average training loss: 9743.55, base loss: 14406.24
[INFO 2017-06-28 13:38:31,583 main.py:51] epoch 4494, training loss: 9257.53, average training loss: 9742.20, base loss: 14404.31
[INFO 2017-06-28 13:38:32,259 main.py:51] epoch 4495, training loss: 10394.80, average training loss: 9743.75, base loss: 14406.72
[INFO 2017-06-28 13:38:32,933 main.py:51] epoch 4496, training loss: 9802.76, average training loss: 9744.25, base loss: 14407.66
[INFO 2017-06-28 13:38:33,596 main.py:51] epoch 4497, training loss: 9267.78, average training loss: 9742.41, base loss: 14405.32
[INFO 2017-06-28 13:38:34,251 main.py:51] epoch 4498, training loss: 10992.76, average training loss: 9744.66, base loss: 14408.56
[INFO 2017-06-28 13:38:34,905 main.py:51] epoch 4499, training loss: 9183.61, average training loss: 9744.85, base loss: 14409.93
[INFO 2017-06-28 13:38:34,905 main.py:53] epoch 4499, testing
[INFO 2017-06-28 13:38:37,477 main.py:105] average testing loss: 10932.31, base loss: 15463.40
[INFO 2017-06-28 13:38:37,477 main.py:106] improve_loss: 4531.09, improve_percent: 0.29
[INFO 2017-06-28 13:38:37,478 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:38:38,138 main.py:51] epoch 4500, training loss: 9137.02, average training loss: 9744.11, base loss: 14408.28
[INFO 2017-06-28 13:38:38,791 main.py:51] epoch 4501, training loss: 9036.48, average training loss: 9743.12, base loss: 14405.50
[INFO 2017-06-28 13:38:39,468 main.py:51] epoch 4502, training loss: 8834.72, average training loss: 9742.16, base loss: 14404.21
[INFO 2017-06-28 13:38:40,116 main.py:51] epoch 4503, training loss: 8988.25, average training loss: 9741.70, base loss: 14404.58
[INFO 2017-06-28 13:38:40,780 main.py:51] epoch 4504, training loss: 8753.01, average training loss: 9740.21, base loss: 14402.67
[INFO 2017-06-28 13:38:41,433 main.py:51] epoch 4505, training loss: 8605.78, average training loss: 9740.33, base loss: 14402.77
[INFO 2017-06-28 13:38:42,098 main.py:51] epoch 4506, training loss: 10238.11, average training loss: 9741.29, base loss: 14404.65
[INFO 2017-06-28 13:38:42,753 main.py:51] epoch 4507, training loss: 10714.25, average training loss: 9742.76, base loss: 14406.56
[INFO 2017-06-28 13:38:43,408 main.py:51] epoch 4508, training loss: 9978.58, average training loss: 9742.63, base loss: 14407.50
[INFO 2017-06-28 13:38:44,059 main.py:51] epoch 4509, training loss: 9576.08, average training loss: 9741.61, base loss: 14406.55
[INFO 2017-06-28 13:38:44,755 main.py:51] epoch 4510, training loss: 10875.97, average training loss: 9742.35, base loss: 14407.99
[INFO 2017-06-28 13:38:45,443 main.py:51] epoch 4511, training loss: 9247.59, average training loss: 9742.46, base loss: 14406.98
[INFO 2017-06-28 13:38:46,106 main.py:51] epoch 4512, training loss: 11945.49, average training loss: 9745.96, base loss: 14412.99
[INFO 2017-06-28 13:38:46,757 main.py:51] epoch 4513, training loss: 12534.64, average training loss: 9749.36, base loss: 14419.26
[INFO 2017-06-28 13:38:47,402 main.py:51] epoch 4514, training loss: 9505.97, average training loss: 9749.57, base loss: 14419.00
[INFO 2017-06-28 13:38:48,049 main.py:51] epoch 4515, training loss: 9691.89, average training loss: 9748.83, base loss: 14418.53
[INFO 2017-06-28 13:38:48,711 main.py:51] epoch 4516, training loss: 9607.60, average training loss: 9749.07, base loss: 14419.30
[INFO 2017-06-28 13:38:49,376 main.py:51] epoch 4517, training loss: 8371.85, average training loss: 9747.57, base loss: 14419.22
[INFO 2017-06-28 13:38:50,030 main.py:51] epoch 4518, training loss: 11010.43, average training loss: 9749.33, base loss: 14423.03
[INFO 2017-06-28 13:38:50,671 main.py:51] epoch 4519, training loss: 8956.91, average training loss: 9749.44, base loss: 14423.34
[INFO 2017-06-28 13:38:51,338 main.py:51] epoch 4520, training loss: 10234.73, average training loss: 9750.87, base loss: 14426.25
[INFO 2017-06-28 13:38:52,015 main.py:51] epoch 4521, training loss: 10490.20, average training loss: 9751.00, base loss: 14427.02
[INFO 2017-06-28 13:38:52,675 main.py:51] epoch 4522, training loss: 10232.92, average training loss: 9751.63, base loss: 14428.35
[INFO 2017-06-28 13:38:53,337 main.py:51] epoch 4523, training loss: 8949.40, average training loss: 9750.37, base loss: 14427.14
[INFO 2017-06-28 13:38:54,003 main.py:51] epoch 4524, training loss: 9115.65, average training loss: 9747.97, base loss: 14425.84
[INFO 2017-06-28 13:38:54,677 main.py:51] epoch 4525, training loss: 9666.98, average training loss: 9749.43, base loss: 14428.00
[INFO 2017-06-28 13:38:55,352 main.py:51] epoch 4526, training loss: 10710.41, average training loss: 9750.48, base loss: 14428.32
[INFO 2017-06-28 13:38:56,019 main.py:51] epoch 4527, training loss: 12112.55, average training loss: 9750.76, base loss: 14426.58
[INFO 2017-06-28 13:38:56,675 main.py:51] epoch 4528, training loss: 10109.35, average training loss: 9750.25, base loss: 14424.98
[INFO 2017-06-28 13:38:57,337 main.py:51] epoch 4529, training loss: 9663.13, average training loss: 9749.76, base loss: 14424.51
[INFO 2017-06-28 13:38:58,001 main.py:51] epoch 4530, training loss: 8899.04, average training loss: 9748.12, base loss: 14424.22
[INFO 2017-06-28 13:38:58,674 main.py:51] epoch 4531, training loss: 9984.88, average training loss: 9747.96, base loss: 14424.12
[INFO 2017-06-28 13:38:59,362 main.py:51] epoch 4532, training loss: 9487.12, average training loss: 9748.55, base loss: 14424.78
[INFO 2017-06-28 13:39:00,012 main.py:51] epoch 4533, training loss: 10432.78, average training loss: 9750.25, base loss: 14427.15
[INFO 2017-06-28 13:39:00,680 main.py:51] epoch 4534, training loss: 9526.35, average training loss: 9751.18, base loss: 14429.19
[INFO 2017-06-28 13:39:01,355 main.py:51] epoch 4535, training loss: 9481.67, average training loss: 9752.88, base loss: 14432.24
[INFO 2017-06-28 13:39:02,000 main.py:51] epoch 4536, training loss: 9469.21, average training loss: 9751.86, base loss: 14431.44
[INFO 2017-06-28 13:39:02,655 main.py:51] epoch 4537, training loss: 9883.25, average training loss: 9752.45, base loss: 14431.90
[INFO 2017-06-28 13:39:03,324 main.py:51] epoch 4538, training loss: 10927.17, average training loss: 9753.21, base loss: 14431.16
[INFO 2017-06-28 13:39:03,977 main.py:51] epoch 4539, training loss: 9354.18, average training loss: 9753.27, base loss: 14432.73
[INFO 2017-06-28 13:39:04,668 main.py:51] epoch 4540, training loss: 8739.48, average training loss: 9752.89, base loss: 14432.41
[INFO 2017-06-28 13:39:05,340 main.py:51] epoch 4541, training loss: 9898.02, average training loss: 9754.01, base loss: 14435.44
[INFO 2017-06-28 13:39:05,997 main.py:51] epoch 4542, training loss: 9974.88, average training loss: 9754.69, base loss: 14435.34
[INFO 2017-06-28 13:39:06,631 main.py:51] epoch 4543, training loss: 9074.90, average training loss: 9755.30, base loss: 14436.08
[INFO 2017-06-28 13:39:07,325 main.py:51] epoch 4544, training loss: 9901.16, average training loss: 9754.90, base loss: 14435.33
[INFO 2017-06-28 13:39:07,994 main.py:51] epoch 4545, training loss: 8901.32, average training loss: 9754.92, base loss: 14435.83
[INFO 2017-06-28 13:39:08,684 main.py:51] epoch 4546, training loss: 10820.69, average training loss: 9757.11, base loss: 14439.21
[INFO 2017-06-28 13:39:09,354 main.py:51] epoch 4547, training loss: 7990.18, average training loss: 9754.21, base loss: 14435.26
[INFO 2017-06-28 13:39:10,026 main.py:51] epoch 4548, training loss: 11265.58, average training loss: 9754.94, base loss: 14436.73
[INFO 2017-06-28 13:39:10,680 main.py:51] epoch 4549, training loss: 10520.45, average training loss: 9754.82, base loss: 14437.15
[INFO 2017-06-28 13:39:11,351 main.py:51] epoch 4550, training loss: 9283.98, average training loss: 9754.45, base loss: 14438.08
[INFO 2017-06-28 13:39:12,009 main.py:51] epoch 4551, training loss: 9803.35, average training loss: 9754.47, base loss: 14438.53
[INFO 2017-06-28 13:39:12,701 main.py:51] epoch 4552, training loss: 12924.41, average training loss: 9756.19, base loss: 14442.08
[INFO 2017-06-28 13:39:13,380 main.py:51] epoch 4553, training loss: 8241.39, average training loss: 9753.42, base loss: 14438.18
[INFO 2017-06-28 13:39:14,040 main.py:51] epoch 4554, training loss: 10239.13, average training loss: 9753.36, base loss: 14437.51
[INFO 2017-06-28 13:39:14,701 main.py:51] epoch 4555, training loss: 9783.68, average training loss: 9754.65, base loss: 14438.99
[INFO 2017-06-28 13:39:15,341 main.py:51] epoch 4556, training loss: 10187.06, average training loss: 9754.44, base loss: 14439.70
[INFO 2017-06-28 13:39:16,040 main.py:51] epoch 4557, training loss: 9895.27, average training loss: 9753.93, base loss: 14439.21
[INFO 2017-06-28 13:39:16,723 main.py:51] epoch 4558, training loss: 9940.53, average training loss: 9754.53, base loss: 14441.80
[INFO 2017-06-28 13:39:17,422 main.py:51] epoch 4559, training loss: 10449.95, average training loss: 9754.83, base loss: 14441.86
[INFO 2017-06-28 13:39:18,075 main.py:51] epoch 4560, training loss: 11657.36, average training loss: 9755.40, base loss: 14443.75
[INFO 2017-06-28 13:39:18,723 main.py:51] epoch 4561, training loss: 11506.51, average training loss: 9757.47, base loss: 14448.02
[INFO 2017-06-28 13:39:19,369 main.py:51] epoch 4562, training loss: 9342.74, average training loss: 9757.45, base loss: 14447.66
[INFO 2017-06-28 13:39:20,061 main.py:51] epoch 4563, training loss: 9440.26, average training loss: 9757.99, base loss: 14448.65
[INFO 2017-06-28 13:39:20,717 main.py:51] epoch 4564, training loss: 10702.60, average training loss: 9758.03, base loss: 14449.07
[INFO 2017-06-28 13:39:21,369 main.py:51] epoch 4565, training loss: 10698.15, average training loss: 9759.80, base loss: 14453.99
[INFO 2017-06-28 13:39:22,030 main.py:51] epoch 4566, training loss: 11121.86, average training loss: 9761.17, base loss: 14454.34
[INFO 2017-06-28 13:39:22,690 main.py:51] epoch 4567, training loss: 10291.45, average training loss: 9760.95, base loss: 14454.46
[INFO 2017-06-28 13:39:23,347 main.py:51] epoch 4568, training loss: 9970.19, average training loss: 9761.52, base loss: 14455.23
[INFO 2017-06-28 13:39:24,024 main.py:51] epoch 4569, training loss: 10847.31, average training loss: 9763.68, base loss: 14460.09
[INFO 2017-06-28 13:39:24,699 main.py:51] epoch 4570, training loss: 9647.22, average training loss: 9763.96, base loss: 14460.18
[INFO 2017-06-28 13:39:25,365 main.py:51] epoch 4571, training loss: 9492.03, average training loss: 9765.05, base loss: 14463.66
[INFO 2017-06-28 13:39:26,033 main.py:51] epoch 4572, training loss: 9681.94, average training loss: 9765.97, base loss: 14464.55
[INFO 2017-06-28 13:39:26,676 main.py:51] epoch 4573, training loss: 10567.37, average training loss: 9766.16, base loss: 14464.63
[INFO 2017-06-28 13:39:27,356 main.py:51] epoch 4574, training loss: 9962.24, average training loss: 9766.24, base loss: 14465.34
[INFO 2017-06-28 13:39:28,009 main.py:51] epoch 4575, training loss: 10087.91, average training loss: 9764.65, base loss: 14463.31
[INFO 2017-06-28 13:39:28,669 main.py:51] epoch 4576, training loss: 9426.75, average training loss: 9764.98, base loss: 14464.66
[INFO 2017-06-28 13:39:29,332 main.py:51] epoch 4577, training loss: 10181.00, average training loss: 9765.53, base loss: 14466.02
[INFO 2017-06-28 13:39:30,005 main.py:51] epoch 4578, training loss: 9598.76, average training loss: 9765.43, base loss: 14466.16
[INFO 2017-06-28 13:39:30,685 main.py:51] epoch 4579, training loss: 9923.66, average training loss: 9765.45, base loss: 14465.93
[INFO 2017-06-28 13:39:31,352 main.py:51] epoch 4580, training loss: 9550.88, average training loss: 9765.52, base loss: 14465.40
[INFO 2017-06-28 13:39:32,008 main.py:51] epoch 4581, training loss: 8805.05, average training loss: 9764.45, base loss: 14464.79
[INFO 2017-06-28 13:39:32,681 main.py:51] epoch 4582, training loss: 9465.59, average training loss: 9764.14, base loss: 14465.23
[INFO 2017-06-28 13:39:33,363 main.py:51] epoch 4583, training loss: 9009.47, average training loss: 9763.36, base loss: 14464.36
[INFO 2017-06-28 13:39:34,013 main.py:51] epoch 4584, training loss: 8178.64, average training loss: 9762.83, base loss: 14464.09
[INFO 2017-06-28 13:39:34,686 main.py:51] epoch 4585, training loss: 10416.56, average training loss: 9762.78, base loss: 14463.03
[INFO 2017-06-28 13:39:35,360 main.py:51] epoch 4586, training loss: 9132.32, average training loss: 9762.27, base loss: 14461.56
[INFO 2017-06-28 13:39:36,013 main.py:51] epoch 4587, training loss: 9253.13, average training loss: 9762.21, base loss: 14461.85
[INFO 2017-06-28 13:39:36,683 main.py:51] epoch 4588, training loss: 8490.84, average training loss: 9761.10, base loss: 14459.17
[INFO 2017-06-28 13:39:37,333 main.py:51] epoch 4589, training loss: 9201.63, average training loss: 9761.48, base loss: 14460.96
[INFO 2017-06-28 13:39:38,003 main.py:51] epoch 4590, training loss: 8392.42, average training loss: 9759.73, base loss: 14458.57
[INFO 2017-06-28 13:39:38,659 main.py:51] epoch 4591, training loss: 8809.35, average training loss: 9758.58, base loss: 14457.43
[INFO 2017-06-28 13:39:39,321 main.py:51] epoch 4592, training loss: 7952.89, average training loss: 9757.13, base loss: 14455.17
[INFO 2017-06-28 13:39:39,991 main.py:51] epoch 4593, training loss: 10065.23, average training loss: 9756.78, base loss: 14452.53
[INFO 2017-06-28 13:39:40,630 main.py:51] epoch 4594, training loss: 9632.12, average training loss: 9757.68, base loss: 14455.40
[INFO 2017-06-28 13:39:41,286 main.py:51] epoch 4595, training loss: 9811.26, average training loss: 9756.76, base loss: 14454.28
[INFO 2017-06-28 13:39:41,954 main.py:51] epoch 4596, training loss: 9325.39, average training loss: 9755.93, base loss: 14453.11
[INFO 2017-06-28 13:39:42,598 main.py:51] epoch 4597, training loss: 8383.93, average training loss: 9754.94, base loss: 14450.95
[INFO 2017-06-28 13:39:43,280 main.py:51] epoch 4598, training loss: 9982.15, average training loss: 9754.72, base loss: 14450.06
[INFO 2017-06-28 13:39:43,938 main.py:51] epoch 4599, training loss: 8879.45, average training loss: 9754.88, base loss: 14450.40
[INFO 2017-06-28 13:39:43,938 main.py:53] epoch 4599, testing
[INFO 2017-06-28 13:39:46,491 main.py:105] average testing loss: 11436.92, base loss: 16035.04
[INFO 2017-06-28 13:39:46,491 main.py:106] improve_loss: 4598.12, improve_percent: 0.29
[INFO 2017-06-28 13:39:46,492 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:39:47,144 main.py:51] epoch 4600, training loss: 8620.33, average training loss: 9752.25, base loss: 14445.87
[INFO 2017-06-28 13:39:47,803 main.py:51] epoch 4601, training loss: 8351.34, average training loss: 9751.58, base loss: 14444.94
[INFO 2017-06-28 13:39:48,439 main.py:51] epoch 4602, training loss: 10223.67, average training loss: 9752.97, base loss: 14445.93
[INFO 2017-06-28 13:39:49,106 main.py:51] epoch 4603, training loss: 9830.55, average training loss: 9752.74, base loss: 14443.83
[INFO 2017-06-28 13:39:49,753 main.py:51] epoch 4604, training loss: 10301.92, average training loss: 9753.99, base loss: 14445.50
[INFO 2017-06-28 13:39:50,424 main.py:51] epoch 4605, training loss: 11255.17, average training loss: 9755.85, base loss: 14447.62
[INFO 2017-06-28 13:39:51,114 main.py:51] epoch 4606, training loss: 10834.97, average training loss: 9756.23, base loss: 14449.65
[INFO 2017-06-28 13:39:51,756 main.py:51] epoch 4607, training loss: 10103.96, average training loss: 9755.57, base loss: 14449.06
[INFO 2017-06-28 13:39:52,419 main.py:51] epoch 4608, training loss: 8923.86, average training loss: 9755.64, base loss: 14447.92
[INFO 2017-06-28 13:39:53,070 main.py:51] epoch 4609, training loss: 10161.98, average training loss: 9755.35, base loss: 14447.71
[INFO 2017-06-28 13:39:53,719 main.py:51] epoch 4610, training loss: 9732.03, average training loss: 9754.98, base loss: 14447.30
[INFO 2017-06-28 13:39:54,413 main.py:51] epoch 4611, training loss: 9351.77, average training loss: 9753.02, base loss: 14443.48
[INFO 2017-06-28 13:39:55,051 main.py:51] epoch 4612, training loss: 10218.68, average training loss: 9753.64, base loss: 14443.24
[INFO 2017-06-28 13:39:55,701 main.py:51] epoch 4613, training loss: 8953.19, average training loss: 9752.72, base loss: 14442.79
[INFO 2017-06-28 13:39:56,351 main.py:51] epoch 4614, training loss: 10446.12, average training loss: 9751.57, base loss: 14442.82
[INFO 2017-06-28 13:39:57,002 main.py:51] epoch 4615, training loss: 11919.02, average training loss: 9753.77, base loss: 14446.18
[INFO 2017-06-28 13:39:57,677 main.py:51] epoch 4616, training loss: 9236.62, average training loss: 9753.70, base loss: 14446.43
[INFO 2017-06-28 13:39:58,321 main.py:51] epoch 4617, training loss: 9181.55, average training loss: 9751.39, base loss: 14444.42
[INFO 2017-06-28 13:39:58,989 main.py:51] epoch 4618, training loss: 8552.73, average training loss: 9751.10, base loss: 14444.07
[INFO 2017-06-28 13:39:59,657 main.py:51] epoch 4619, training loss: 8484.15, average training loss: 9750.57, base loss: 14443.84
[INFO 2017-06-28 13:40:00,299 main.py:51] epoch 4620, training loss: 9574.44, average training loss: 9750.47, base loss: 14442.43
[INFO 2017-06-28 13:40:00,961 main.py:51] epoch 4621, training loss: 11135.11, average training loss: 9753.43, base loss: 14445.94
[INFO 2017-06-28 13:40:01,641 main.py:51] epoch 4622, training loss: 9514.07, average training loss: 9753.66, base loss: 14446.93
[INFO 2017-06-28 13:40:02,294 main.py:51] epoch 4623, training loss: 10417.49, average training loss: 9754.30, base loss: 14448.02
[INFO 2017-06-28 13:40:02,937 main.py:51] epoch 4624, training loss: 10333.91, average training loss: 9754.82, base loss: 14449.86
[INFO 2017-06-28 13:40:03,594 main.py:51] epoch 4625, training loss: 10263.26, average training loss: 9754.94, base loss: 14450.15
[INFO 2017-06-28 13:40:04,261 main.py:51] epoch 4626, training loss: 8462.06, average training loss: 9753.24, base loss: 14446.98
[INFO 2017-06-28 13:40:04,913 main.py:51] epoch 4627, training loss: 10555.50, average training loss: 9754.75, base loss: 14449.53
[INFO 2017-06-28 13:40:05,580 main.py:51] epoch 4628, training loss: 9087.39, average training loss: 9754.33, base loss: 14448.08
[INFO 2017-06-28 13:40:06,263 main.py:51] epoch 4629, training loss: 9741.29, average training loss: 9754.48, base loss: 14449.20
[INFO 2017-06-28 13:40:06,924 main.py:51] epoch 4630, training loss: 9776.68, average training loss: 9755.36, base loss: 14452.28
[INFO 2017-06-28 13:40:07,579 main.py:51] epoch 4631, training loss: 9653.18, average training loss: 9753.83, base loss: 14451.26
[INFO 2017-06-28 13:40:08,254 main.py:51] epoch 4632, training loss: 7430.19, average training loss: 9751.78, base loss: 14447.83
[INFO 2017-06-28 13:40:08,906 main.py:51] epoch 4633, training loss: 9962.23, average training loss: 9751.81, base loss: 14448.44
[INFO 2017-06-28 13:40:09,574 main.py:51] epoch 4634, training loss: 9275.88, average training loss: 9751.00, base loss: 14447.04
[INFO 2017-06-28 13:40:10,232 main.py:51] epoch 4635, training loss: 9714.03, average training loss: 9751.75, base loss: 14448.66
[INFO 2017-06-28 13:40:10,896 main.py:51] epoch 4636, training loss: 10193.06, average training loss: 9753.82, base loss: 14453.30
[INFO 2017-06-28 13:40:11,562 main.py:51] epoch 4637, training loss: 9190.67, average training loss: 9753.25, base loss: 14452.86
[INFO 2017-06-28 13:40:12,219 main.py:51] epoch 4638, training loss: 8828.66, average training loss: 9752.27, base loss: 14450.27
[INFO 2017-06-28 13:40:12,862 main.py:51] epoch 4639, training loss: 9215.52, average training loss: 9751.79, base loss: 14448.67
[INFO 2017-06-28 13:40:13,534 main.py:51] epoch 4640, training loss: 8540.56, average training loss: 9748.51, base loss: 14445.13
[INFO 2017-06-28 13:40:14,181 main.py:51] epoch 4641, training loss: 9298.87, average training loss: 9747.64, base loss: 14444.72
[INFO 2017-06-28 13:40:14,837 main.py:51] epoch 4642, training loss: 9885.71, average training loss: 9748.40, base loss: 14445.57
[INFO 2017-06-28 13:40:15,483 main.py:51] epoch 4643, training loss: 9134.07, average training loss: 9747.90, base loss: 14443.62
[INFO 2017-06-28 13:40:16,156 main.py:51] epoch 4644, training loss: 9314.48, average training loss: 9747.08, base loss: 14439.89
[INFO 2017-06-28 13:40:16,826 main.py:51] epoch 4645, training loss: 9732.75, average training loss: 9746.37, base loss: 14438.70
[INFO 2017-06-28 13:40:17,505 main.py:51] epoch 4646, training loss: 10026.22, average training loss: 9747.33, base loss: 14440.57
[INFO 2017-06-28 13:40:18,158 main.py:51] epoch 4647, training loss: 12017.92, average training loss: 9749.92, base loss: 14444.92
[INFO 2017-06-28 13:40:18,814 main.py:51] epoch 4648, training loss: 9615.58, average training loss: 9750.64, base loss: 14446.27
[INFO 2017-06-28 13:40:19,476 main.py:51] epoch 4649, training loss: 9286.68, average training loss: 9751.02, base loss: 14448.47
[INFO 2017-06-28 13:40:20,137 main.py:51] epoch 4650, training loss: 9001.34, average training loss: 9748.81, base loss: 14445.54
[INFO 2017-06-28 13:40:20,780 main.py:51] epoch 4651, training loss: 9142.35, average training loss: 9748.74, base loss: 14446.96
[INFO 2017-06-28 13:40:21,437 main.py:51] epoch 4652, training loss: 9753.71, average training loss: 9749.32, base loss: 14448.30
[INFO 2017-06-28 13:40:22,112 main.py:51] epoch 4653, training loss: 11002.17, average training loss: 9751.55, base loss: 14452.12
[INFO 2017-06-28 13:40:22,772 main.py:51] epoch 4654, training loss: 9076.35, average training loss: 9750.80, base loss: 14451.73
[INFO 2017-06-28 13:40:23,430 main.py:51] epoch 4655, training loss: 9456.85, average training loss: 9750.53, base loss: 14450.85
[INFO 2017-06-28 13:40:24,093 main.py:51] epoch 4656, training loss: 8619.33, average training loss: 9749.59, base loss: 14450.55
[INFO 2017-06-28 13:40:24,740 main.py:51] epoch 4657, training loss: 10143.15, average training loss: 9751.42, base loss: 14451.97
[INFO 2017-06-28 13:40:25,402 main.py:51] epoch 4658, training loss: 10326.84, average training loss: 9751.24, base loss: 14452.16
[INFO 2017-06-28 13:40:26,050 main.py:51] epoch 4659, training loss: 10685.41, average training loss: 9752.08, base loss: 14452.25
[INFO 2017-06-28 13:40:26,701 main.py:51] epoch 4660, training loss: 11378.00, average training loss: 9753.79, base loss: 14455.45
[INFO 2017-06-28 13:40:27,358 main.py:51] epoch 4661, training loss: 8569.02, average training loss: 9753.55, base loss: 14457.22
[INFO 2017-06-28 13:40:28,008 main.py:51] epoch 4662, training loss: 10556.37, average training loss: 9754.71, base loss: 14458.66
[INFO 2017-06-28 13:40:28,652 main.py:51] epoch 4663, training loss: 9933.44, average training loss: 9755.80, base loss: 14459.88
[INFO 2017-06-28 13:40:29,294 main.py:51] epoch 4664, training loss: 9133.54, average training loss: 9754.90, base loss: 14458.89
[INFO 2017-06-28 13:40:29,932 main.py:51] epoch 4665, training loss: 11062.98, average training loss: 9755.87, base loss: 14460.91
[INFO 2017-06-28 13:40:30,589 main.py:51] epoch 4666, training loss: 12049.48, average training loss: 9758.00, base loss: 14464.79
[INFO 2017-06-28 13:40:31,251 main.py:51] epoch 4667, training loss: 8407.98, average training loss: 9756.68, base loss: 14462.01
[INFO 2017-06-28 13:40:31,904 main.py:51] epoch 4668, training loss: 8593.48, average training loss: 9755.93, base loss: 14460.04
[INFO 2017-06-28 13:40:32,578 main.py:51] epoch 4669, training loss: 10231.03, average training loss: 9754.57, base loss: 14458.25
[INFO 2017-06-28 13:40:33,267 main.py:51] epoch 4670, training loss: 8205.39, average training loss: 9753.70, base loss: 14456.06
[INFO 2017-06-28 13:40:33,910 main.py:51] epoch 4671, training loss: 7792.51, average training loss: 9750.92, base loss: 14454.57
[INFO 2017-06-28 13:40:34,550 main.py:51] epoch 4672, training loss: 9417.66, average training loss: 9751.43, base loss: 14456.09
[INFO 2017-06-28 13:40:35,202 main.py:51] epoch 4673, training loss: 9076.18, average training loss: 9751.19, base loss: 14456.35
[INFO 2017-06-28 13:40:35,884 main.py:51] epoch 4674, training loss: 10046.02, average training loss: 9752.27, base loss: 14458.76
[INFO 2017-06-28 13:40:36,534 main.py:51] epoch 4675, training loss: 8811.59, average training loss: 9751.64, base loss: 14456.50
[INFO 2017-06-28 13:40:37,179 main.py:51] epoch 4676, training loss: 10069.30, average training loss: 9752.66, base loss: 14457.98
[INFO 2017-06-28 13:40:37,855 main.py:51] epoch 4677, training loss: 12657.13, average training loss: 9755.94, base loss: 14463.49
[INFO 2017-06-28 13:40:38,533 main.py:51] epoch 4678, training loss: 10941.30, average training loss: 9756.38, base loss: 14465.42
[INFO 2017-06-28 13:40:39,179 main.py:51] epoch 4679, training loss: 9289.51, average training loss: 9754.04, base loss: 14460.62
[INFO 2017-06-28 13:40:39,836 main.py:51] epoch 4680, training loss: 8827.92, average training loss: 9753.30, base loss: 14459.00
[INFO 2017-06-28 13:40:40,491 main.py:51] epoch 4681, training loss: 9364.24, average training loss: 9753.19, base loss: 14459.27
[INFO 2017-06-28 13:40:41,147 main.py:51] epoch 4682, training loss: 8906.94, average training loss: 9751.18, base loss: 14458.59
[INFO 2017-06-28 13:40:41,802 main.py:51] epoch 4683, training loss: 8585.25, average training loss: 9750.53, base loss: 14459.01
[INFO 2017-06-28 13:40:42,469 main.py:51] epoch 4684, training loss: 9147.55, average training loss: 9750.35, base loss: 14461.06
[INFO 2017-06-28 13:40:43,125 main.py:51] epoch 4685, training loss: 9958.50, average training loss: 9751.40, base loss: 14462.54
[INFO 2017-06-28 13:40:43,786 main.py:51] epoch 4686, training loss: 9282.60, average training loss: 9751.04, base loss: 14462.28
[INFO 2017-06-28 13:40:44,445 main.py:51] epoch 4687, training loss: 9584.75, average training loss: 9750.38, base loss: 14461.41
[INFO 2017-06-28 13:40:45,115 main.py:51] epoch 4688, training loss: 9920.11, average training loss: 9747.86, base loss: 14457.22
[INFO 2017-06-28 13:40:45,798 main.py:51] epoch 4689, training loss: 8939.18, average training loss: 9748.48, base loss: 14459.11
[INFO 2017-06-28 13:40:46,467 main.py:51] epoch 4690, training loss: 7941.03, average training loss: 9745.08, base loss: 14454.15
[INFO 2017-06-28 13:40:47,143 main.py:51] epoch 4691, training loss: 8081.13, average training loss: 9743.28, base loss: 14452.31
[INFO 2017-06-28 13:40:47,814 main.py:51] epoch 4692, training loss: 9947.25, average training loss: 9743.10, base loss: 14453.70
[INFO 2017-06-28 13:40:48,481 main.py:51] epoch 4693, training loss: 9306.79, average training loss: 9742.71, base loss: 14453.78
[INFO 2017-06-28 13:40:49,124 main.py:51] epoch 4694, training loss: 8265.71, average training loss: 9741.67, base loss: 14451.45
[INFO 2017-06-28 13:40:49,768 main.py:51] epoch 4695, training loss: 9426.08, average training loss: 9739.44, base loss: 14448.76
[INFO 2017-06-28 13:40:50,422 main.py:51] epoch 4696, training loss: 9396.22, average training loss: 9739.53, base loss: 14449.46
[INFO 2017-06-28 13:40:51,075 main.py:51] epoch 4697, training loss: 9852.65, average training loss: 9740.67, base loss: 14450.67
[INFO 2017-06-28 13:40:51,738 main.py:51] epoch 4698, training loss: 8451.85, average training loss: 9736.53, base loss: 14444.52
[INFO 2017-06-28 13:40:52,395 main.py:51] epoch 4699, training loss: 10775.39, average training loss: 9736.58, base loss: 14444.76
[INFO 2017-06-28 13:40:52,395 main.py:53] epoch 4699, testing
[INFO 2017-06-28 13:40:54,993 main.py:105] average testing loss: 10557.48, base loss: 14886.83
[INFO 2017-06-28 13:40:54,993 main.py:106] improve_loss: 4329.35, improve_percent: 0.29
[INFO 2017-06-28 13:40:54,993 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:40:55,648 main.py:51] epoch 4700, training loss: 10578.41, average training loss: 9737.80, base loss: 14446.21
[INFO 2017-06-28 13:40:56,311 main.py:51] epoch 4701, training loss: 10063.90, average training loss: 9737.04, base loss: 14445.73
[INFO 2017-06-28 13:40:56,955 main.py:51] epoch 4702, training loss: 9545.35, average training loss: 9734.94, base loss: 14443.07
[INFO 2017-06-28 13:40:57,627 main.py:51] epoch 4703, training loss: 8979.22, average training loss: 9734.36, base loss: 14443.53
[INFO 2017-06-28 13:40:58,323 main.py:51] epoch 4704, training loss: 10270.64, average training loss: 9735.68, base loss: 14445.89
[INFO 2017-06-28 13:40:58,971 main.py:51] epoch 4705, training loss: 10372.00, average training loss: 9736.92, base loss: 14447.11
[INFO 2017-06-28 13:40:59,651 main.py:51] epoch 4706, training loss: 10329.57, average training loss: 9737.46, base loss: 14446.57
[INFO 2017-06-28 13:41:00,292 main.py:51] epoch 4707, training loss: 8898.54, average training loss: 9736.45, base loss: 14445.24
[INFO 2017-06-28 13:41:00,940 main.py:51] epoch 4708, training loss: 8541.06, average training loss: 9733.34, base loss: 14441.26
[INFO 2017-06-28 13:41:01,592 main.py:51] epoch 4709, training loss: 9166.38, average training loss: 9732.79, base loss: 14441.10
[INFO 2017-06-28 13:41:02,238 main.py:51] epoch 4710, training loss: 9537.98, average training loss: 9732.46, base loss: 14442.09
[INFO 2017-06-28 13:41:02,905 main.py:51] epoch 4711, training loss: 8485.32, average training loss: 9730.44, base loss: 14439.45
[INFO 2017-06-28 13:41:03,561 main.py:51] epoch 4712, training loss: 9581.35, average training loss: 9730.54, base loss: 14439.22
[INFO 2017-06-28 13:41:04,226 main.py:51] epoch 4713, training loss: 9637.88, average training loss: 9731.54, base loss: 14441.28
[INFO 2017-06-28 13:41:04,881 main.py:51] epoch 4714, training loss: 10622.26, average training loss: 9732.09, base loss: 14443.52
[INFO 2017-06-28 13:41:05,540 main.py:51] epoch 4715, training loss: 8805.36, average training loss: 9731.45, base loss: 14441.65
[INFO 2017-06-28 13:41:06,201 main.py:51] epoch 4716, training loss: 9666.89, average training loss: 9732.70, base loss: 14442.45
[INFO 2017-06-28 13:41:06,858 main.py:51] epoch 4717, training loss: 9703.90, average training loss: 9733.70, base loss: 14443.07
[INFO 2017-06-28 13:41:07,528 main.py:51] epoch 4718, training loss: 8529.79, average training loss: 9732.19, base loss: 14440.88
[INFO 2017-06-28 13:41:08,178 main.py:51] epoch 4719, training loss: 9066.08, average training loss: 9732.20, base loss: 14441.23
[INFO 2017-06-28 13:41:08,837 main.py:51] epoch 4720, training loss: 9473.80, average training loss: 9732.55, base loss: 14440.57
[INFO 2017-06-28 13:41:09,501 main.py:51] epoch 4721, training loss: 9040.08, average training loss: 9733.00, base loss: 14441.40
[INFO 2017-06-28 13:41:10,146 main.py:51] epoch 4722, training loss: 9139.10, average training loss: 9731.49, base loss: 14439.77
[INFO 2017-06-28 13:41:10,790 main.py:51] epoch 4723, training loss: 11265.88, average training loss: 9733.94, base loss: 14444.20
[INFO 2017-06-28 13:41:11,446 main.py:51] epoch 4724, training loss: 9828.10, average training loss: 9734.56, base loss: 14444.06
[INFO 2017-06-28 13:41:12,079 main.py:51] epoch 4725, training loss: 9536.25, average training loss: 9734.74, base loss: 14444.10
[INFO 2017-06-28 13:41:12,732 main.py:51] epoch 4726, training loss: 9686.63, average training loss: 9733.75, base loss: 14441.27
[INFO 2017-06-28 13:41:13,386 main.py:51] epoch 4727, training loss: 10226.52, average training loss: 9734.11, base loss: 14443.11
[INFO 2017-06-28 13:41:14,052 main.py:51] epoch 4728, training loss: 8967.44, average training loss: 9731.70, base loss: 14438.47
[INFO 2017-06-28 13:41:14,706 main.py:51] epoch 4729, training loss: 7814.13, average training loss: 9728.40, base loss: 14433.48
[INFO 2017-06-28 13:41:15,371 main.py:51] epoch 4730, training loss: 9127.91, average training loss: 9728.06, base loss: 14432.28
[INFO 2017-06-28 13:41:16,035 main.py:51] epoch 4731, training loss: 10075.22, average training loss: 9728.37, base loss: 14433.06
[INFO 2017-06-28 13:41:16,700 main.py:51] epoch 4732, training loss: 8623.89, average training loss: 9726.54, base loss: 14430.12
[INFO 2017-06-28 13:41:17,349 main.py:51] epoch 4733, training loss: 10993.46, average training loss: 9727.12, base loss: 14432.97
[INFO 2017-06-28 13:41:18,021 main.py:51] epoch 4734, training loss: 8935.73, average training loss: 9725.70, base loss: 14430.44
[INFO 2017-06-28 13:41:18,682 main.py:51] epoch 4735, training loss: 9908.37, average training loss: 9726.93, base loss: 14433.70
[INFO 2017-06-28 13:41:19,341 main.py:51] epoch 4736, training loss: 10688.36, average training loss: 9726.79, base loss: 14434.09
[INFO 2017-06-28 13:41:20,000 main.py:51] epoch 4737, training loss: 9295.46, average training loss: 9725.79, base loss: 14433.00
[INFO 2017-06-28 13:41:20,671 main.py:51] epoch 4738, training loss: 9532.61, average training loss: 9726.91, base loss: 14435.59
[INFO 2017-06-28 13:41:21,331 main.py:51] epoch 4739, training loss: 9734.71, average training loss: 9726.00, base loss: 14433.99
[INFO 2017-06-28 13:41:21,995 main.py:51] epoch 4740, training loss: 10856.63, average training loss: 9728.63, base loss: 14438.17
[INFO 2017-06-28 13:41:22,655 main.py:51] epoch 4741, training loss: 8726.40, average training loss: 9727.62, base loss: 14437.65
[INFO 2017-06-28 13:41:23,323 main.py:51] epoch 4742, training loss: 9173.57, average training loss: 9726.41, base loss: 14436.10
[INFO 2017-06-28 13:41:23,980 main.py:51] epoch 4743, training loss: 10220.29, average training loss: 9725.10, base loss: 14434.60
[INFO 2017-06-28 13:41:24,613 main.py:51] epoch 4744, training loss: 8876.67, average training loss: 9723.79, base loss: 14433.38
[INFO 2017-06-28 13:41:25,279 main.py:51] epoch 4745, training loss: 10773.64, average training loss: 9724.57, base loss: 14434.44
[INFO 2017-06-28 13:41:25,916 main.py:51] epoch 4746, training loss: 9784.27, average training loss: 9724.68, base loss: 14433.51
[INFO 2017-06-28 13:41:26,586 main.py:51] epoch 4747, training loss: 9697.78, average training loss: 9724.16, base loss: 14433.11
[INFO 2017-06-28 13:41:27,236 main.py:51] epoch 4748, training loss: 9736.09, average training loss: 9723.99, base loss: 14433.65
[INFO 2017-06-28 13:41:27,903 main.py:51] epoch 4749, training loss: 8995.29, average training loss: 9724.39, base loss: 14433.88
[INFO 2017-06-28 13:41:28,543 main.py:51] epoch 4750, training loss: 10391.73, average training loss: 9725.69, base loss: 14436.02
[INFO 2017-06-28 13:41:29,206 main.py:51] epoch 4751, training loss: 10728.90, average training loss: 9727.03, base loss: 14438.75
[INFO 2017-06-28 13:41:29,862 main.py:51] epoch 4752, training loss: 9018.96, average training loss: 9725.80, base loss: 14436.88
[INFO 2017-06-28 13:41:30,513 main.py:51] epoch 4753, training loss: 8467.79, average training loss: 9722.93, base loss: 14432.92
[INFO 2017-06-28 13:41:31,159 main.py:51] epoch 4754, training loss: 9457.00, average training loss: 9723.16, base loss: 14432.52
[INFO 2017-06-28 13:41:31,801 main.py:51] epoch 4755, training loss: 9399.27, average training loss: 9723.37, base loss: 14432.41
[INFO 2017-06-28 13:41:32,467 main.py:51] epoch 4756, training loss: 10061.58, average training loss: 9721.47, base loss: 14428.82
[INFO 2017-06-28 13:41:33,115 main.py:51] epoch 4757, training loss: 10027.83, average training loss: 9722.88, base loss: 14430.80
[INFO 2017-06-28 13:41:33,779 main.py:51] epoch 4758, training loss: 8813.59, average training loss: 9722.34, base loss: 14430.19
[INFO 2017-06-28 13:41:34,447 main.py:51] epoch 4759, training loss: 9879.21, average training loss: 9723.21, base loss: 14431.83
[INFO 2017-06-28 13:41:35,097 main.py:51] epoch 4760, training loss: 9485.73, average training loss: 9722.61, base loss: 14430.10
[INFO 2017-06-28 13:41:35,740 main.py:51] epoch 4761, training loss: 9285.26, average training loss: 9722.03, base loss: 14428.81
[INFO 2017-06-28 13:41:36,411 main.py:51] epoch 4762, training loss: 12068.74, average training loss: 9724.46, base loss: 14432.70
[INFO 2017-06-28 13:41:37,090 main.py:51] epoch 4763, training loss: 8877.75, average training loss: 9723.98, base loss: 14433.70
[INFO 2017-06-28 13:41:37,757 main.py:51] epoch 4764, training loss: 8670.89, average training loss: 9722.89, base loss: 14431.16
[INFO 2017-06-28 13:41:38,407 main.py:51] epoch 4765, training loss: 8572.36, average training loss: 9721.41, base loss: 14427.99
[INFO 2017-06-28 13:41:39,084 main.py:51] epoch 4766, training loss: 9788.93, average training loss: 9722.25, base loss: 14429.57
[INFO 2017-06-28 13:41:39,735 main.py:51] epoch 4767, training loss: 9359.45, average training loss: 9720.76, base loss: 14427.46
[INFO 2017-06-28 13:41:40,402 main.py:51] epoch 4768, training loss: 8766.94, average training loss: 9720.80, base loss: 14426.75
[INFO 2017-06-28 13:41:41,048 main.py:51] epoch 4769, training loss: 10471.58, average training loss: 9721.43, base loss: 14427.79
[INFO 2017-06-28 13:41:41,705 main.py:51] epoch 4770, training loss: 10062.83, average training loss: 9721.39, base loss: 14428.45
[INFO 2017-06-28 13:41:42,348 main.py:51] epoch 4771, training loss: 11238.14, average training loss: 9721.70, base loss: 14429.21
[INFO 2017-06-28 13:41:42,998 main.py:51] epoch 4772, training loss: 10004.38, average training loss: 9723.34, base loss: 14432.01
[INFO 2017-06-28 13:41:43,638 main.py:51] epoch 4773, training loss: 9012.98, average training loss: 9722.41, base loss: 14430.95
[INFO 2017-06-28 13:41:44,302 main.py:51] epoch 4774, training loss: 9143.39, average training loss: 9722.18, base loss: 14430.44
[INFO 2017-06-28 13:41:44,989 main.py:51] epoch 4775, training loss: 9772.95, average training loss: 9722.58, base loss: 14433.37
[INFO 2017-06-28 13:41:45,649 main.py:51] epoch 4776, training loss: 8442.20, average training loss: 9720.20, base loss: 14431.26
[INFO 2017-06-28 13:41:46,316 main.py:51] epoch 4777, training loss: 10700.32, average training loss: 9717.92, base loss: 14427.69
[INFO 2017-06-28 13:41:46,983 main.py:51] epoch 4778, training loss: 9700.31, average training loss: 9717.24, base loss: 14427.73
[INFO 2017-06-28 13:41:47,638 main.py:51] epoch 4779, training loss: 9091.65, average training loss: 9716.57, base loss: 14426.32
[INFO 2017-06-28 13:41:48,307 main.py:51] epoch 4780, training loss: 8873.06, average training loss: 9715.78, base loss: 14425.93
[INFO 2017-06-28 13:41:48,980 main.py:51] epoch 4781, training loss: 10156.19, average training loss: 9716.77, base loss: 14426.58
[INFO 2017-06-28 13:41:49,640 main.py:51] epoch 4782, training loss: 10187.12, average training loss: 9715.79, base loss: 14425.70
[INFO 2017-06-28 13:41:50,294 main.py:51] epoch 4783, training loss: 10179.33, average training loss: 9716.04, base loss: 14425.04
[INFO 2017-06-28 13:41:50,935 main.py:51] epoch 4784, training loss: 8967.77, average training loss: 9716.07, base loss: 14425.38
[INFO 2017-06-28 13:41:51,613 main.py:51] epoch 4785, training loss: 9002.04, average training loss: 9716.23, base loss: 14425.41
[INFO 2017-06-28 13:41:52,287 main.py:51] epoch 4786, training loss: 10430.44, average training loss: 9715.46, base loss: 14426.32
[INFO 2017-06-28 13:41:52,936 main.py:51] epoch 4787, training loss: 9614.12, average training loss: 9716.60, base loss: 14428.48
[INFO 2017-06-28 13:41:53,597 main.py:51] epoch 4788, training loss: 11124.88, average training loss: 9719.50, base loss: 14431.62
[INFO 2017-06-28 13:41:54,251 main.py:51] epoch 4789, training loss: 9200.23, average training loss: 9718.24, base loss: 14428.79
[INFO 2017-06-28 13:41:54,911 main.py:51] epoch 4790, training loss: 8576.03, average training loss: 9717.87, base loss: 14429.01
[INFO 2017-06-28 13:41:55,584 main.py:51] epoch 4791, training loss: 9409.46, average training loss: 9717.46, base loss: 14426.77
[INFO 2017-06-28 13:41:56,246 main.py:51] epoch 4792, training loss: 9235.89, average training loss: 9717.19, base loss: 14428.22
[INFO 2017-06-28 13:41:56,915 main.py:51] epoch 4793, training loss: 8467.89, average training loss: 9716.82, base loss: 14429.55
[INFO 2017-06-28 13:41:57,573 main.py:51] epoch 4794, training loss: 9242.96, average training loss: 9716.11, base loss: 14428.58
[INFO 2017-06-28 13:41:58,230 main.py:51] epoch 4795, training loss: 8303.60, average training loss: 9714.62, base loss: 14426.72
[INFO 2017-06-28 13:41:58,874 main.py:51] epoch 4796, training loss: 8877.83, average training loss: 9712.62, base loss: 14423.80
[INFO 2017-06-28 13:41:59,530 main.py:51] epoch 4797, training loss: 10679.76, average training loss: 9712.71, base loss: 14426.62
[INFO 2017-06-28 13:42:00,210 main.py:51] epoch 4798, training loss: 9062.59, average training loss: 9712.16, base loss: 14425.56
[INFO 2017-06-28 13:42:00,861 main.py:51] epoch 4799, training loss: 8993.60, average training loss: 9710.22, base loss: 14421.37
[INFO 2017-06-28 13:42:00,861 main.py:53] epoch 4799, testing
[INFO 2017-06-28 13:42:03,476 main.py:105] average testing loss: 11130.03, base loss: 15508.54
[INFO 2017-06-28 13:42:03,476 main.py:106] improve_loss: 4378.52, improve_percent: 0.28
[INFO 2017-06-28 13:42:03,477 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 13:42:04,134 main.py:51] epoch 4800, training loss: 8991.93, average training loss: 9707.61, base loss: 14416.28
[INFO 2017-06-28 13:42:04,774 main.py:51] epoch 4801, training loss: 9612.34, average training loss: 9707.26, base loss: 14416.74
[INFO 2017-06-28 13:42:05,429 main.py:51] epoch 4802, training loss: 8878.70, average training loss: 9707.72, base loss: 14418.70
[INFO 2017-06-28 13:42:06,101 main.py:51] epoch 4803, training loss: 9002.07, average training loss: 9706.30, base loss: 14416.37
[INFO 2017-06-28 13:42:06,756 main.py:51] epoch 4804, training loss: 10657.28, average training loss: 9706.59, base loss: 14416.59
[INFO 2017-06-28 13:42:07,421 main.py:51] epoch 4805, training loss: 10165.64, average training loss: 9706.92, base loss: 14419.02
[INFO 2017-06-28 13:42:08,088 main.py:51] epoch 4806, training loss: 9304.72, average training loss: 9707.18, base loss: 14420.22
[INFO 2017-06-28 13:42:08,742 main.py:51] epoch 4807, training loss: 9989.50, average training loss: 9706.35, base loss: 14420.09
[INFO 2017-06-28 13:42:09,395 main.py:51] epoch 4808, training loss: 10110.21, average training loss: 9705.79, base loss: 14419.38
[INFO 2017-06-28 13:42:10,051 main.py:51] epoch 4809, training loss: 9654.94, average training loss: 9705.61, base loss: 14420.02
[INFO 2017-06-28 13:42:10,710 main.py:51] epoch 4810, training loss: 9971.08, average training loss: 9705.96, base loss: 14418.27
[INFO 2017-06-28 13:42:11,387 main.py:51] epoch 4811, training loss: 9134.70, average training loss: 9704.60, base loss: 14416.44
[INFO 2017-06-28 13:42:12,052 main.py:51] epoch 4812, training loss: 9760.32, average training loss: 9704.89, base loss: 14417.75
[INFO 2017-06-28 13:42:12,703 main.py:51] epoch 4813, training loss: 10094.19, average training loss: 9705.99, base loss: 14419.32
[INFO 2017-06-28 13:42:13,353 main.py:51] epoch 4814, training loss: 10493.49, average training loss: 9707.95, base loss: 14421.02
[INFO 2017-06-28 13:42:14,017 main.py:51] epoch 4815, training loss: 8966.53, average training loss: 9707.29, base loss: 14420.14
[INFO 2017-06-28 13:42:14,655 main.py:51] epoch 4816, training loss: 9768.51, average training loss: 9708.21, base loss: 14422.04
[INFO 2017-06-28 13:42:15,336 main.py:51] epoch 4817, training loss: 9820.38, average training loss: 9708.64, base loss: 14423.20
[INFO 2017-06-28 13:42:15,965 main.py:51] epoch 4818, training loss: 8630.50, average training loss: 9705.85, base loss: 14419.90
[INFO 2017-06-28 13:42:16,627 main.py:51] epoch 4819, training loss: 8552.26, average training loss: 9704.17, base loss: 14418.10
[INFO 2017-06-28 13:42:17,268 main.py:51] epoch 4820, training loss: 10072.54, average training loss: 9705.31, base loss: 14421.55
[INFO 2017-06-28 13:42:17,929 main.py:51] epoch 4821, training loss: 8635.58, average training loss: 9704.04, base loss: 14420.00
[INFO 2017-06-28 13:42:18,592 main.py:51] epoch 4822, training loss: 9036.56, average training loss: 9702.32, base loss: 14418.76
[INFO 2017-06-28 13:42:19,263 main.py:51] epoch 4823, training loss: 8080.28, average training loss: 9701.24, base loss: 14415.70
[INFO 2017-06-28 13:42:19,939 main.py:51] epoch 4824, training loss: 11683.40, average training loss: 9702.83, base loss: 14415.76
[INFO 2017-06-28 13:42:20,594 main.py:51] epoch 4825, training loss: 10664.08, average training loss: 9703.43, base loss: 14416.77
[INFO 2017-06-28 13:42:21,269 main.py:51] epoch 4826, training loss: 10071.60, average training loss: 9704.90, base loss: 14421.42
[INFO 2017-06-28 13:42:21,931 main.py:51] epoch 4827, training loss: 10516.26, average training loss: 9705.44, base loss: 14422.57
[INFO 2017-06-28 13:42:22,603 main.py:51] epoch 4828, training loss: 9841.99, average training loss: 9705.56, base loss: 14423.29
[INFO 2017-06-28 13:42:23,254 main.py:51] epoch 4829, training loss: 9732.73, average training loss: 9705.18, base loss: 14420.77
[INFO 2017-06-28 13:42:23,916 main.py:51] epoch 4830, training loss: 9636.06, average training loss: 9703.82, base loss: 14419.29
[INFO 2017-06-28 13:42:24,576 main.py:51] epoch 4831, training loss: 9365.79, average training loss: 9703.59, base loss: 14420.45
[INFO 2017-06-28 13:42:25,221 main.py:51] epoch 4832, training loss: 9779.31, average training loss: 9704.48, base loss: 14422.11
[INFO 2017-06-28 13:42:25,867 main.py:51] epoch 4833, training loss: 9194.54, average training loss: 9702.91, base loss: 14419.61
[INFO 2017-06-28 13:42:26,518 main.py:51] epoch 4834, training loss: 10691.03, average training loss: 9703.17, base loss: 14419.78
[INFO 2017-06-28 13:42:27,178 main.py:51] epoch 4835, training loss: 8434.07, average training loss: 9701.57, base loss: 14418.15
[INFO 2017-06-28 13:42:27,846 main.py:51] epoch 4836, training loss: 9648.76, average training loss: 9702.10, base loss: 14420.15
[INFO 2017-06-28 13:42:28,510 main.py:51] epoch 4837, training loss: 9238.27, average training loss: 9700.83, base loss: 14418.30
[INFO 2017-06-28 13:42:29,164 main.py:51] epoch 4838, training loss: 11030.30, average training loss: 9700.59, base loss: 14418.32
[INFO 2017-06-28 13:42:29,835 main.py:51] epoch 4839, training loss: 11636.35, average training loss: 9701.65, base loss: 14419.33
[INFO 2017-06-28 13:42:30,476 main.py:51] epoch 4840, training loss: 8481.94, average training loss: 9700.85, base loss: 14417.99
[INFO 2017-06-28 13:42:31,134 main.py:51] epoch 4841, training loss: 9940.77, average training loss: 9700.25, base loss: 14417.13
[INFO 2017-06-28 13:42:31,782 main.py:51] epoch 4842, training loss: 9563.97, average training loss: 9699.89, base loss: 14417.35
[INFO 2017-06-28 13:42:32,446 main.py:51] epoch 4843, training loss: 10498.09, average training loss: 9700.84, base loss: 14417.89
[INFO 2017-06-28 13:42:33,106 main.py:51] epoch 4844, training loss: 9855.26, average training loss: 9700.69, base loss: 14418.83
[INFO 2017-06-28 13:42:33,769 main.py:51] epoch 4845, training loss: 11132.91, average training loss: 9701.76, base loss: 14420.15
[INFO 2017-06-28 13:42:34,418 main.py:51] epoch 4846, training loss: 11524.64, average training loss: 9703.59, base loss: 14422.86
[INFO 2017-06-28 13:42:35,074 main.py:51] epoch 4847, training loss: 10590.61, average training loss: 9703.42, base loss: 14423.95
[INFO 2017-06-28 13:42:35,725 main.py:51] epoch 4848, training loss: 8566.76, average training loss: 9702.65, base loss: 14422.12
[INFO 2017-06-28 13:42:36,365 main.py:51] epoch 4849, training loss: 11190.30, average training loss: 9703.78, base loss: 14423.67
[INFO 2017-06-28 13:42:37,018 main.py:51] epoch 4850, training loss: 10712.34, average training loss: 9702.45, base loss: 14422.04
[INFO 2017-06-28 13:42:37,668 main.py:51] epoch 4851, training loss: 8781.55, average training loss: 9701.70, base loss: 14420.35
[INFO 2017-06-28 13:42:38,315 main.py:51] epoch 4852, training loss: 11388.70, average training loss: 9703.32, base loss: 14422.13
[INFO 2017-06-28 13:42:38,979 main.py:51] epoch 4853, training loss: 10772.41, average training loss: 9703.32, base loss: 14422.07
[INFO 2017-06-28 13:42:39,634 main.py:51] epoch 4854, training loss: 10232.27, average training loss: 9703.48, base loss: 14421.69
[INFO 2017-06-28 13:42:40,294 main.py:51] epoch 4855, training loss: 10801.54, average training loss: 9704.73, base loss: 14422.88
[INFO 2017-06-28 13:42:40,931 main.py:51] epoch 4856, training loss: 8843.19, average training loss: 9702.82, base loss: 14420.64
[INFO 2017-06-28 13:42:41,589 main.py:51] epoch 4857, training loss: 9213.66, average training loss: 9702.51, base loss: 14420.51
[INFO 2017-06-28 13:42:42,257 main.py:51] epoch 4858, training loss: 10705.11, average training loss: 9703.38, base loss: 14423.82
[INFO 2017-06-28 13:42:42,896 main.py:51] epoch 4859, training loss: 9000.90, average training loss: 9703.39, base loss: 14424.12
[INFO 2017-06-28 13:42:43,571 main.py:51] epoch 4860, training loss: 8682.14, average training loss: 9702.81, base loss: 14424.39
[INFO 2017-06-28 13:42:44,234 main.py:51] epoch 4861, training loss: 11503.12, average training loss: 9703.78, base loss: 14425.95
[INFO 2017-06-28 13:42:44,893 main.py:51] epoch 4862, training loss: 9750.81, average training loss: 9703.43, base loss: 14425.11
[INFO 2017-06-28 13:42:45,527 main.py:51] epoch 4863, training loss: 10907.69, average training loss: 9704.64, base loss: 14426.51
[INFO 2017-06-28 13:42:46,177 main.py:51] epoch 4864, training loss: 9146.24, average training loss: 9703.88, base loss: 14426.98
[INFO 2017-06-28 13:42:46,837 main.py:51] epoch 4865, training loss: 7952.27, average training loss: 9702.95, base loss: 14425.14
[INFO 2017-06-28 13:42:47,511 main.py:51] epoch 4866, training loss: 9716.04, average training loss: 9703.85, base loss: 14427.47
[INFO 2017-06-28 13:42:48,177 main.py:51] epoch 4867, training loss: 9750.09, average training loss: 9703.85, base loss: 14428.92
[INFO 2017-06-28 13:42:48,846 main.py:51] epoch 4868, training loss: 9447.50, average training loss: 9703.45, base loss: 14429.98
[INFO 2017-06-28 13:42:49,498 main.py:51] epoch 4869, training loss: 9951.84, average training loss: 9701.41, base loss: 14427.73
[INFO 2017-06-28 13:42:50,148 main.py:51] epoch 4870, training loss: 8288.51, average training loss: 9700.70, base loss: 14425.73
[INFO 2017-06-28 13:42:50,789 main.py:51] epoch 4871, training loss: 9901.68, average training loss: 9700.43, base loss: 14424.85
[INFO 2017-06-28 13:42:51,436 main.py:51] epoch 4872, training loss: 8876.90, average training loss: 9698.76, base loss: 14421.93
[INFO 2017-06-28 13:42:52,091 main.py:51] epoch 4873, training loss: 9219.72, average training loss: 9698.45, base loss: 14420.25
[INFO 2017-06-28 13:42:52,785 main.py:51] epoch 4874, training loss: 9642.91, average training loss: 9698.61, base loss: 14420.30
[INFO 2017-06-28 13:42:53,456 main.py:51] epoch 4875, training loss: 9544.55, average training loss: 9698.55, base loss: 14419.25
[INFO 2017-06-28 13:42:54,121 main.py:51] epoch 4876, training loss: 9659.34, average training loss: 9698.59, base loss: 14420.94
[INFO 2017-06-28 13:42:54,778 main.py:51] epoch 4877, training loss: 9077.01, average training loss: 9698.18, base loss: 14421.74
[INFO 2017-06-28 13:42:55,447 main.py:51] epoch 4878, training loss: 9217.91, average training loss: 9698.15, base loss: 14421.86
[INFO 2017-06-28 13:42:56,087 main.py:51] epoch 4879, training loss: 9978.42, average training loss: 9698.10, base loss: 14420.59
[INFO 2017-06-28 13:42:56,728 main.py:51] epoch 4880, training loss: 9643.61, average training loss: 9697.56, base loss: 14421.22
[INFO 2017-06-28 13:42:57,375 main.py:51] epoch 4881, training loss: 10500.50, average training loss: 9699.06, base loss: 14422.82
[INFO 2017-06-28 13:42:58,037 main.py:51] epoch 4882, training loss: 10642.23, average training loss: 9699.26, base loss: 14421.99
[INFO 2017-06-28 13:42:58,710 main.py:51] epoch 4883, training loss: 9113.36, average training loss: 9697.04, base loss: 14417.31
[INFO 2017-06-28 13:42:59,330 main.py:51] epoch 4884, training loss: 9539.50, average training loss: 9695.79, base loss: 14414.15
[INFO 2017-06-28 13:43:00,014 main.py:51] epoch 4885, training loss: 9746.66, average training loss: 9694.30, base loss: 14412.70
[INFO 2017-06-28 13:43:00,680 main.py:51] epoch 4886, training loss: 9344.04, average training loss: 9694.79, base loss: 14414.57
[INFO 2017-06-28 13:43:01,344 main.py:51] epoch 4887, training loss: 9842.68, average training loss: 9695.15, base loss: 14416.55
[INFO 2017-06-28 13:43:02,004 main.py:51] epoch 4888, training loss: 8900.01, average training loss: 9691.97, base loss: 14412.13
[INFO 2017-06-28 13:43:02,660 main.py:51] epoch 4889, training loss: 12148.47, average training loss: 9695.12, base loss: 14416.06
[INFO 2017-06-28 13:43:03,314 main.py:51] epoch 4890, training loss: 10080.69, average training loss: 9697.09, base loss: 14419.37
[INFO 2017-06-28 13:43:03,970 main.py:51] epoch 4891, training loss: 10095.22, average training loss: 9696.85, base loss: 14418.42
[INFO 2017-06-28 13:43:04,627 main.py:51] epoch 4892, training loss: 10785.02, average training loss: 9699.48, base loss: 14422.85
[INFO 2017-06-28 13:43:05,290 main.py:51] epoch 4893, training loss: 9309.95, average training loss: 9699.50, base loss: 14422.40
[INFO 2017-06-28 13:43:05,939 main.py:51] epoch 4894, training loss: 9616.41, average training loss: 9699.01, base loss: 14420.89
[INFO 2017-06-28 13:43:06,638 main.py:51] epoch 4895, training loss: 8626.93, average training loss: 9698.04, base loss: 14419.41
[INFO 2017-06-28 13:43:07,313 main.py:51] epoch 4896, training loss: 9374.22, average training loss: 9697.79, base loss: 14419.51
[INFO 2017-06-28 13:43:07,966 main.py:51] epoch 4897, training loss: 10589.19, average training loss: 9698.54, base loss: 14420.75
[INFO 2017-06-28 13:43:08,630 main.py:51] epoch 4898, training loss: 8631.54, average training loss: 9697.98, base loss: 14419.88
[INFO 2017-06-28 13:43:09,289 main.py:51] epoch 4899, training loss: 10080.58, average training loss: 9698.34, base loss: 14420.86
[INFO 2017-06-28 13:43:09,289 main.py:53] epoch 4899, testing
[INFO 2017-06-28 13:43:11,897 main.py:105] average testing loss: 10521.71, base loss: 14955.94
[INFO 2017-06-28 13:43:11,897 main.py:106] improve_loss: 4434.24, improve_percent: 0.30
[INFO 2017-06-28 13:43:11,897 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 13:43:11,933 main.py:76] current best improved percent: 0.30
[INFO 2017-06-28 13:43:12,596 main.py:51] epoch 4900, training loss: 10388.39, average training loss: 9699.36, base loss: 14421.26
[INFO 2017-06-28 13:43:13,253 main.py:51] epoch 4901, training loss: 10004.60, average training loss: 9698.22, base loss: 14420.87
[INFO 2017-06-28 13:43:13,901 main.py:51] epoch 4902, training loss: 9337.29, average training loss: 9698.23, base loss: 14420.65
[INFO 2017-06-28 13:43:14,570 main.py:51] epoch 4903, training loss: 9084.24, average training loss: 9697.24, base loss: 14419.56
[INFO 2017-06-28 13:43:15,214 main.py:51] epoch 4904, training loss: 12578.05, average training loss: 9700.63, base loss: 14425.79
[INFO 2017-06-28 13:43:15,860 main.py:51] epoch 4905, training loss: 9667.53, average training loss: 9699.77, base loss: 14423.55
[INFO 2017-06-28 13:43:16,515 main.py:51] epoch 4906, training loss: 9522.65, average training loss: 9699.17, base loss: 14421.53
[INFO 2017-06-28 13:43:17,164 main.py:51] epoch 4907, training loss: 9329.74, average training loss: 9699.92, base loss: 14424.34
[INFO 2017-06-28 13:43:17,831 main.py:51] epoch 4908, training loss: 9807.21, average training loss: 9699.60, base loss: 14425.08
[INFO 2017-06-28 13:43:18,523 main.py:51] epoch 4909, training loss: 10605.59, average training loss: 9700.81, base loss: 14425.32
[INFO 2017-06-28 13:43:19,197 main.py:51] epoch 4910, training loss: 9043.99, average training loss: 9700.44, base loss: 14424.46
[INFO 2017-06-28 13:43:19,871 main.py:51] epoch 4911, training loss: 11565.73, average training loss: 9702.55, base loss: 14426.89
[INFO 2017-06-28 13:43:20,539 main.py:51] epoch 4912, training loss: 9892.05, average training loss: 9702.90, base loss: 14427.75
[INFO 2017-06-28 13:43:21,193 main.py:51] epoch 4913, training loss: 11089.14, average training loss: 9703.85, base loss: 14431.83
[INFO 2017-06-28 13:43:21,837 main.py:51] epoch 4914, training loss: 9765.41, average training loss: 9703.72, base loss: 14431.26
[INFO 2017-06-28 13:43:22,499 main.py:51] epoch 4915, training loss: 8747.10, average training loss: 9703.02, base loss: 14429.95
[INFO 2017-06-28 13:43:23,175 main.py:51] epoch 4916, training loss: 10253.73, average training loss: 9703.88, base loss: 14430.70
[INFO 2017-06-28 13:43:23,850 main.py:51] epoch 4917, training loss: 10776.90, average training loss: 9704.39, base loss: 14429.70
[INFO 2017-06-28 13:43:24,533 main.py:51] epoch 4918, training loss: 10966.83, average training loss: 9706.44, base loss: 14431.24
[INFO 2017-06-28 13:43:25,217 main.py:51] epoch 4919, training loss: 9873.61, average training loss: 9706.32, base loss: 14430.63
[INFO 2017-06-28 13:43:25,869 main.py:51] epoch 4920, training loss: 9280.07, average training loss: 9706.92, base loss: 14431.14
[INFO 2017-06-28 13:43:26,510 main.py:51] epoch 4921, training loss: 10880.77, average training loss: 9708.34, base loss: 14433.15
[INFO 2017-06-28 13:43:27,163 main.py:51] epoch 4922, training loss: 8455.06, average training loss: 9707.67, base loss: 14432.48
[INFO 2017-06-28 13:43:27,828 main.py:51] epoch 4923, training loss: 10850.52, average training loss: 9708.11, base loss: 14433.52
[INFO 2017-06-28 13:43:28,487 main.py:51] epoch 4924, training loss: 11244.57, average training loss: 9708.86, base loss: 14434.55
[INFO 2017-06-28 13:43:29,143 main.py:51] epoch 4925, training loss: 9329.25, average training loss: 9708.90, base loss: 14436.13
[INFO 2017-06-28 13:43:29,783 main.py:51] epoch 4926, training loss: 9885.99, average training loss: 9708.26, base loss: 14436.80
[INFO 2017-06-28 13:43:30,424 main.py:51] epoch 4927, training loss: 9269.15, average training loss: 9708.46, base loss: 14437.52
[INFO 2017-06-28 13:43:31,089 main.py:51] epoch 4928, training loss: 9459.25, average training loss: 9708.60, base loss: 14439.77
[INFO 2017-06-28 13:43:31,754 main.py:51] epoch 4929, training loss: 8743.63, average training loss: 9706.33, base loss: 14435.97
[INFO 2017-06-28 13:43:32,414 main.py:51] epoch 4930, training loss: 8742.00, average training loss: 9706.37, base loss: 14437.12
[INFO 2017-06-28 13:43:33,066 main.py:51] epoch 4931, training loss: 9619.78, average training loss: 9706.87, base loss: 14439.59
[INFO 2017-06-28 13:43:33,725 main.py:51] epoch 4932, training loss: 9447.16, average training loss: 9706.99, base loss: 14439.60
[INFO 2017-06-28 13:43:34,391 main.py:51] epoch 4933, training loss: 9957.29, average training loss: 9707.80, base loss: 14440.42
[INFO 2017-06-28 13:43:35,061 main.py:51] epoch 4934, training loss: 10707.73, average training loss: 9707.35, base loss: 14441.66
[INFO 2017-06-28 13:43:35,703 main.py:51] epoch 4935, training loss: 9410.18, average training loss: 9708.15, base loss: 14442.15
[INFO 2017-06-28 13:43:36,378 main.py:51] epoch 4936, training loss: 9812.27, average training loss: 9707.58, base loss: 14442.27
[INFO 2017-06-28 13:43:37,026 main.py:51] epoch 4937, training loss: 8328.16, average training loss: 9706.76, base loss: 14442.12
[INFO 2017-06-28 13:43:37,685 main.py:51] epoch 4938, training loss: 8790.35, average training loss: 9707.20, base loss: 14442.68
[INFO 2017-06-28 13:43:38,336 main.py:51] epoch 4939, training loss: 9588.89, average training loss: 9706.30, base loss: 14443.99
[INFO 2017-06-28 13:43:38,991 main.py:51] epoch 4940, training loss: 9209.08, average training loss: 9706.18, base loss: 14443.94
[INFO 2017-06-28 13:43:39,654 main.py:51] epoch 4941, training loss: 9384.55, average training loss: 9705.98, base loss: 14443.72
[INFO 2017-06-28 13:43:40,304 main.py:51] epoch 4942, training loss: 9919.31, average training loss: 9705.79, base loss: 14445.86
[INFO 2017-06-28 13:43:40,958 main.py:51] epoch 4943, training loss: 9322.28, average training loss: 9706.04, base loss: 14446.94
[INFO 2017-06-28 13:43:41,605 main.py:51] epoch 4944, training loss: 9909.61, average training loss: 9704.83, base loss: 14447.21
[INFO 2017-06-28 13:43:42,276 main.py:51] epoch 4945, training loss: 8500.48, average training loss: 9704.43, base loss: 14446.91
[INFO 2017-06-28 13:43:42,945 main.py:51] epoch 4946, training loss: 9981.96, average training loss: 9703.60, base loss: 14448.61
[INFO 2017-06-28 13:43:43,615 main.py:51] epoch 4947, training loss: 10516.30, average training loss: 9704.68, base loss: 14449.95
[INFO 2017-06-28 13:43:44,274 main.py:51] epoch 4948, training loss: 8497.10, average training loss: 9705.00, base loss: 14451.61
[INFO 2017-06-28 13:43:44,948 main.py:51] epoch 4949, training loss: 8361.19, average training loss: 9703.23, base loss: 14448.54
[INFO 2017-06-28 13:43:45,621 main.py:51] epoch 4950, training loss: 9339.85, average training loss: 9702.86, base loss: 14447.94
[INFO 2017-06-28 13:43:46,289 main.py:51] epoch 4951, training loss: 8281.93, average training loss: 9701.91, base loss: 14446.69
[INFO 2017-06-28 13:43:46,941 main.py:51] epoch 4952, training loss: 9215.05, average training loss: 9699.96, base loss: 14444.32
[INFO 2017-06-28 13:43:47,617 main.py:51] epoch 4953, training loss: 9574.14, average training loss: 9701.04, base loss: 14445.90
[INFO 2017-06-28 13:43:48,268 main.py:51] epoch 4954, training loss: 9989.18, average training loss: 9700.90, base loss: 14447.20
[INFO 2017-06-28 13:43:48,913 main.py:51] epoch 4955, training loss: 9883.13, average training loss: 9701.12, base loss: 14448.06
[INFO 2017-06-28 13:43:49,595 main.py:51] epoch 4956, training loss: 9514.83, average training loss: 9701.86, base loss: 14449.74
[INFO 2017-06-28 13:43:50,257 main.py:51] epoch 4957, training loss: 8898.06, average training loss: 9700.22, base loss: 14446.37
[INFO 2017-06-28 13:43:50,913 main.py:51] epoch 4958, training loss: 11166.02, average training loss: 9701.97, base loss: 14448.53
[INFO 2017-06-28 13:43:51,566 main.py:51] epoch 4959, training loss: 10037.01, average training loss: 9702.55, base loss: 14449.64
[INFO 2017-06-28 13:43:52,216 main.py:51] epoch 4960, training loss: 9840.20, average training loss: 9702.66, base loss: 14449.35
[INFO 2017-06-28 13:43:52,857 main.py:51] epoch 4961, training loss: 9445.09, average training loss: 9702.85, base loss: 14450.05
[INFO 2017-06-28 13:43:53,507 main.py:51] epoch 4962, training loss: 10393.22, average training loss: 9704.18, base loss: 14453.19
[INFO 2017-06-28 13:43:54,156 main.py:51] epoch 4963, training loss: 9453.77, average training loss: 9703.76, base loss: 14451.68
[INFO 2017-06-28 13:43:54,815 main.py:51] epoch 4964, training loss: 9626.64, average training loss: 9703.14, base loss: 14450.60
[INFO 2017-06-28 13:43:55,468 main.py:51] epoch 4965, training loss: 9613.47, average training loss: 9702.28, base loss: 14449.97
[INFO 2017-06-28 13:43:56,127 main.py:51] epoch 4966, training loss: 8399.98, average training loss: 9701.20, base loss: 14450.49
[INFO 2017-06-28 13:43:56,808 main.py:51] epoch 4967, training loss: 9776.10, average training loss: 9701.34, base loss: 14451.14
[INFO 2017-06-28 13:43:57,485 main.py:51] epoch 4968, training loss: 9115.34, average training loss: 9700.65, base loss: 14450.44
[INFO 2017-06-28 13:43:58,140 main.py:51] epoch 4969, training loss: 8639.30, average training loss: 9698.71, base loss: 14446.18
[INFO 2017-06-28 13:43:58,826 main.py:51] epoch 4970, training loss: 9602.21, average training loss: 9699.39, base loss: 14445.82
[INFO 2017-06-28 13:43:59,484 main.py:51] epoch 4971, training loss: 9789.07, average training loss: 9699.48, base loss: 14447.27
[INFO 2017-06-28 13:44:00,124 main.py:51] epoch 4972, training loss: 8944.05, average training loss: 9698.49, base loss: 14445.95
[INFO 2017-06-28 13:44:00,785 main.py:51] epoch 4973, training loss: 10742.25, average training loss: 9697.83, base loss: 14445.24
[INFO 2017-06-28 13:44:01,425 main.py:51] epoch 4974, training loss: 9795.71, average training loss: 9696.87, base loss: 14444.25
[INFO 2017-06-28 13:44:02,072 main.py:51] epoch 4975, training loss: 9432.16, average training loss: 9696.91, base loss: 14444.42
[INFO 2017-06-28 13:44:02,753 main.py:51] epoch 4976, training loss: 9983.30, average training loss: 9697.65, base loss: 14445.24
[INFO 2017-06-28 13:44:03,415 main.py:51] epoch 4977, training loss: 11243.44, average training loss: 9698.40, base loss: 14447.15
[INFO 2017-06-28 13:44:04,062 main.py:51] epoch 4978, training loss: 8914.05, average training loss: 9697.86, base loss: 14445.14
[INFO 2017-06-28 13:44:04,723 main.py:51] epoch 4979, training loss: 9442.91, average training loss: 9699.47, base loss: 14448.64
[INFO 2017-06-28 13:44:05,395 main.py:51] epoch 4980, training loss: 8741.52, average training loss: 9697.92, base loss: 14445.56
[INFO 2017-06-28 13:44:06,060 main.py:51] epoch 4981, training loss: 9248.31, average training loss: 9697.82, base loss: 14443.55
[INFO 2017-06-28 13:44:06,728 main.py:51] epoch 4982, training loss: 9148.01, average training loss: 9698.28, base loss: 14444.31
[INFO 2017-06-28 13:44:07,401 main.py:51] epoch 4983, training loss: 10345.85, average training loss: 9698.93, base loss: 14444.64
[INFO 2017-06-28 13:44:08,076 main.py:51] epoch 4984, training loss: 8076.45, average training loss: 9698.14, base loss: 14441.92
[INFO 2017-06-28 13:44:08,753 main.py:51] epoch 4985, training loss: 9469.59, average training loss: 9697.95, base loss: 14439.91
[INFO 2017-06-28 13:44:09,411 main.py:51] epoch 4986, training loss: 12151.61, average training loss: 9700.19, base loss: 14441.89
[INFO 2017-06-28 13:44:10,062 main.py:51] epoch 4987, training loss: 8676.69, average training loss: 9698.76, base loss: 14440.54
[INFO 2017-06-28 13:44:10,733 main.py:51] epoch 4988, training loss: 9150.11, average training loss: 9697.85, base loss: 14437.36
[INFO 2017-06-28 13:44:11,395 main.py:51] epoch 4989, training loss: 9881.38, average training loss: 9698.76, base loss: 14438.89
[INFO 2017-06-28 13:44:12,055 main.py:51] epoch 4990, training loss: 9753.31, average training loss: 9698.09, base loss: 14438.18
[INFO 2017-06-28 13:44:12,708 main.py:51] epoch 4991, training loss: 9180.06, average training loss: 9697.63, base loss: 14439.17
[INFO 2017-06-28 13:44:13,362 main.py:51] epoch 4992, training loss: 9533.36, average training loss: 9697.90, base loss: 14440.00
[INFO 2017-06-28 13:44:14,025 main.py:51] epoch 4993, training loss: 10552.00, average training loss: 9699.96, base loss: 14442.50
[INFO 2017-06-28 13:44:14,683 main.py:51] epoch 4994, training loss: 8736.85, average training loss: 9698.38, base loss: 14440.48
[INFO 2017-06-28 13:44:15,349 main.py:51] epoch 4995, training loss: 9182.89, average training loss: 9696.36, base loss: 14436.81
[INFO 2017-06-28 13:44:16,019 main.py:51] epoch 4996, training loss: 9831.51, average training loss: 9696.25, base loss: 14436.66
[INFO 2017-06-28 13:44:16,683 main.py:51] epoch 4997, training loss: 9315.62, average training loss: 9695.32, base loss: 14436.70
[INFO 2017-06-28 13:44:17,377 main.py:51] epoch 4998, training loss: 9701.58, average training loss: 9695.87, base loss: 14438.38
[INFO 2017-06-28 13:44:18,030 main.py:51] epoch 4999, training loss: 10529.96, average training loss: 9695.94, base loss: 14438.69
[INFO 2017-06-28 13:44:18,030 main.py:53] epoch 4999, testing
[INFO 2017-06-28 13:44:20,593 main.py:105] average testing loss: 9655.96, base loss: 13906.29
[INFO 2017-06-28 13:44:20,593 main.py:106] improve_loss: 4250.33, improve_percent: 0.31
[INFO 2017-06-28 13:44:20,594 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 13:44:20,630 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:44:21,272 main.py:51] epoch 5000, training loss: 11263.91, average training loss: 9697.16, base loss: 14440.99
[INFO 2017-06-28 13:44:21,938 main.py:51] epoch 5001, training loss: 8632.76, average training loss: 9695.73, base loss: 14439.09
[INFO 2017-06-28 13:44:22,590 main.py:51] epoch 5002, training loss: 9880.77, average training loss: 9696.21, base loss: 14440.63
[INFO 2017-06-28 13:44:23,261 main.py:51] epoch 5003, training loss: 10455.63, average training loss: 9695.67, base loss: 14440.35
[INFO 2017-06-28 13:44:23,918 main.py:51] epoch 5004, training loss: 8615.43, average training loss: 9694.69, base loss: 14438.59
[INFO 2017-06-28 13:44:24,574 main.py:51] epoch 5005, training loss: 9680.52, average training loss: 9692.99, base loss: 14436.48
[INFO 2017-06-28 13:44:25,233 main.py:51] epoch 5006, training loss: 9376.56, average training loss: 9690.91, base loss: 14433.89
[INFO 2017-06-28 13:44:25,903 main.py:51] epoch 5007, training loss: 9338.21, average training loss: 9690.22, base loss: 14434.34
[INFO 2017-06-28 13:44:26,536 main.py:51] epoch 5008, training loss: 8079.65, average training loss: 9688.63, base loss: 14433.04
[INFO 2017-06-28 13:44:27,189 main.py:51] epoch 5009, training loss: 9503.08, average training loss: 9688.02, base loss: 14433.46
[INFO 2017-06-28 13:44:27,851 main.py:51] epoch 5010, training loss: 9846.92, average training loss: 9686.78, base loss: 14433.16
[INFO 2017-06-28 13:44:28,521 main.py:51] epoch 5011, training loss: 9776.73, average training loss: 9686.37, base loss: 14433.68
[INFO 2017-06-28 13:44:29,191 main.py:51] epoch 5012, training loss: 8343.59, average training loss: 9684.26, base loss: 14431.13
[INFO 2017-06-28 13:44:29,866 main.py:51] epoch 5013, training loss: 9908.63, average training loss: 9683.10, base loss: 14430.21
[INFO 2017-06-28 13:44:30,508 main.py:51] epoch 5014, training loss: 9309.72, average training loss: 9682.25, base loss: 14428.79
[INFO 2017-06-28 13:44:31,182 main.py:51] epoch 5015, training loss: 8487.43, average training loss: 9681.25, base loss: 14428.54
[INFO 2017-06-28 13:44:31,844 main.py:51] epoch 5016, training loss: 9205.20, average training loss: 9680.64, base loss: 14428.38
[INFO 2017-06-28 13:44:32,484 main.py:51] epoch 5017, training loss: 8851.91, average training loss: 9679.40, base loss: 14427.12
[INFO 2017-06-28 13:44:33,144 main.py:51] epoch 5018, training loss: 9258.92, average training loss: 9679.61, base loss: 14428.04
[INFO 2017-06-28 13:44:33,794 main.py:51] epoch 5019, training loss: 9315.63, average training loss: 9679.58, base loss: 14426.88
[INFO 2017-06-28 13:44:34,449 main.py:51] epoch 5020, training loss: 9841.57, average training loss: 9679.24, base loss: 14427.13
[INFO 2017-06-28 13:44:35,126 main.py:51] epoch 5021, training loss: 9479.63, average training loss: 9679.23, base loss: 14427.15
[INFO 2017-06-28 13:44:35,791 main.py:51] epoch 5022, training loss: 9533.74, average training loss: 9678.31, base loss: 14423.34
[INFO 2017-06-28 13:44:36,449 main.py:51] epoch 5023, training loss: 8680.87, average training loss: 9678.16, base loss: 14422.88
[INFO 2017-06-28 13:44:37,115 main.py:51] epoch 5024, training loss: 9282.92, average training loss: 9676.85, base loss: 14420.77
[INFO 2017-06-28 13:44:37,773 main.py:51] epoch 5025, training loss: 9420.06, average training loss: 9676.45, base loss: 14420.87
[INFO 2017-06-28 13:44:38,439 main.py:51] epoch 5026, training loss: 10359.16, average training loss: 9676.44, base loss: 14419.47
[INFO 2017-06-28 13:44:39,098 main.py:51] epoch 5027, training loss: 9539.00, average training loss: 9676.23, base loss: 14420.21
[INFO 2017-06-28 13:44:39,769 main.py:51] epoch 5028, training loss: 9115.08, average training loss: 9673.17, base loss: 14417.09
[INFO 2017-06-28 13:44:40,421 main.py:51] epoch 5029, training loss: 9566.63, average training loss: 9672.91, base loss: 14417.09
[INFO 2017-06-28 13:44:41,079 main.py:51] epoch 5030, training loss: 10103.45, average training loss: 9673.10, base loss: 14417.02
[INFO 2017-06-28 13:44:41,734 main.py:51] epoch 5031, training loss: 9074.05, average training loss: 9672.70, base loss: 14414.77
[INFO 2017-06-28 13:44:42,392 main.py:51] epoch 5032, training loss: 8423.29, average training loss: 9671.84, base loss: 14413.43
[INFO 2017-06-28 13:44:43,056 main.py:51] epoch 5033, training loss: 10931.88, average training loss: 9674.30, base loss: 14416.20
[INFO 2017-06-28 13:44:43,748 main.py:51] epoch 5034, training loss: 11942.59, average training loss: 9674.61, base loss: 14417.12
[INFO 2017-06-28 13:44:44,420 main.py:51] epoch 5035, training loss: 8745.40, average training loss: 9675.25, base loss: 14417.47
[INFO 2017-06-28 13:44:45,073 main.py:51] epoch 5036, training loss: 10756.00, average training loss: 9677.08, base loss: 14419.18
[INFO 2017-06-28 13:44:45,719 main.py:51] epoch 5037, training loss: 8549.71, average training loss: 9675.81, base loss: 14416.56
[INFO 2017-06-28 13:44:46,385 main.py:51] epoch 5038, training loss: 10080.79, average training loss: 9676.12, base loss: 14416.22
[INFO 2017-06-28 13:44:47,050 main.py:51] epoch 5039, training loss: 11439.45, average training loss: 9677.08, base loss: 14418.51
[INFO 2017-06-28 13:44:47,703 main.py:51] epoch 5040, training loss: 12286.35, average training loss: 9679.71, base loss: 14421.67
[INFO 2017-06-28 13:44:48,381 main.py:51] epoch 5041, training loss: 10073.53, average training loss: 9681.42, base loss: 14424.47
[INFO 2017-06-28 13:44:49,027 main.py:51] epoch 5042, training loss: 8926.71, average training loss: 9680.93, base loss: 14425.19
[INFO 2017-06-28 13:44:49,701 main.py:51] epoch 5043, training loss: 9602.22, average training loss: 9680.91, base loss: 14424.74
[INFO 2017-06-28 13:44:50,351 main.py:51] epoch 5044, training loss: 10521.89, average training loss: 9681.01, base loss: 14424.91
[INFO 2017-06-28 13:44:51,009 main.py:51] epoch 5045, training loss: 9763.42, average training loss: 9681.96, base loss: 14426.06
[INFO 2017-06-28 13:44:51,655 main.py:51] epoch 5046, training loss: 9756.80, average training loss: 9680.72, base loss: 14424.82
[INFO 2017-06-28 13:44:52,340 main.py:51] epoch 5047, training loss: 9561.67, average training loss: 9679.93, base loss: 14424.19
[INFO 2017-06-28 13:44:52,983 main.py:51] epoch 5048, training loss: 10017.32, average training loss: 9680.01, base loss: 14425.46
[INFO 2017-06-28 13:44:53,651 main.py:51] epoch 5049, training loss: 9226.59, average training loss: 9680.08, base loss: 14425.40
[INFO 2017-06-28 13:44:54,316 main.py:51] epoch 5050, training loss: 9695.61, average training loss: 9679.37, base loss: 14426.35
[INFO 2017-06-28 13:44:54,978 main.py:51] epoch 5051, training loss: 10799.37, average training loss: 9681.46, base loss: 14428.15
[INFO 2017-06-28 13:44:55,650 main.py:51] epoch 5052, training loss: 9562.22, average training loss: 9680.23, base loss: 14427.23
[INFO 2017-06-28 13:44:56,340 main.py:51] epoch 5053, training loss: 10337.12, average training loss: 9680.92, base loss: 14428.26
[INFO 2017-06-28 13:44:57,024 main.py:51] epoch 5054, training loss: 9310.60, average training loss: 9681.89, base loss: 14428.36
[INFO 2017-06-28 13:44:57,682 main.py:51] epoch 5055, training loss: 10149.96, average training loss: 9682.00, base loss: 14429.60
[INFO 2017-06-28 13:44:58,339 main.py:51] epoch 5056, training loss: 10434.38, average training loss: 9682.51, base loss: 14429.63
[INFO 2017-06-28 13:44:58,983 main.py:51] epoch 5057, training loss: 12034.38, average training loss: 9686.41, base loss: 14435.08
[INFO 2017-06-28 13:44:59,631 main.py:51] epoch 5058, training loss: 9656.71, average training loss: 9687.00, base loss: 14434.80
[INFO 2017-06-28 13:45:00,309 main.py:51] epoch 5059, training loss: 9461.93, average training loss: 9686.52, base loss: 14433.54
[INFO 2017-06-28 13:45:00,955 main.py:51] epoch 5060, training loss: 8302.21, average training loss: 9684.78, base loss: 14430.24
[INFO 2017-06-28 13:45:01,614 main.py:51] epoch 5061, training loss: 9586.56, average training loss: 9684.67, base loss: 14430.75
[INFO 2017-06-28 13:45:02,277 main.py:51] epoch 5062, training loss: 9830.74, average training loss: 9685.25, base loss: 14431.84
[INFO 2017-06-28 13:45:02,922 main.py:51] epoch 5063, training loss: 10213.76, average training loss: 9685.44, base loss: 14433.49
[INFO 2017-06-28 13:45:03,561 main.py:51] epoch 5064, training loss: 10480.89, average training loss: 9686.93, base loss: 14436.62
[INFO 2017-06-28 13:45:04,244 main.py:51] epoch 5065, training loss: 12377.84, average training loss: 9689.72, base loss: 14440.16
[INFO 2017-06-28 13:45:04,916 main.py:51] epoch 5066, training loss: 8237.58, average training loss: 9687.04, base loss: 14437.55
[INFO 2017-06-28 13:45:05,563 main.py:51] epoch 5067, training loss: 9566.60, average training loss: 9688.10, base loss: 14439.44
[INFO 2017-06-28 13:45:06,228 main.py:51] epoch 5068, training loss: 10160.29, average training loss: 9690.16, base loss: 14443.10
[INFO 2017-06-28 13:45:06,878 main.py:51] epoch 5069, training loss: 8768.70, average training loss: 9689.64, base loss: 14442.92
[INFO 2017-06-28 13:45:07,530 main.py:51] epoch 5070, training loss: 9558.81, average training loss: 9688.47, base loss: 14440.51
[INFO 2017-06-28 13:45:08,190 main.py:51] epoch 5071, training loss: 9253.46, average training loss: 9688.41, base loss: 14439.76
[INFO 2017-06-28 13:45:08,841 main.py:51] epoch 5072, training loss: 8248.62, average training loss: 9685.71, base loss: 14436.68
[INFO 2017-06-28 13:45:09,509 main.py:51] epoch 5073, training loss: 9596.73, average training loss: 9685.37, base loss: 14436.57
[INFO 2017-06-28 13:45:10,172 main.py:51] epoch 5074, training loss: 8518.25, average training loss: 9683.39, base loss: 14434.09
[INFO 2017-06-28 13:45:10,818 main.py:51] epoch 5075, training loss: 8834.55, average training loss: 9681.68, base loss: 14431.18
[INFO 2017-06-28 13:45:11,468 main.py:51] epoch 5076, training loss: 9797.48, average training loss: 9681.85, base loss: 14432.95
[INFO 2017-06-28 13:45:12,141 main.py:51] epoch 5077, training loss: 9380.43, average training loss: 9681.00, base loss: 14430.83
[INFO 2017-06-28 13:45:12,820 main.py:51] epoch 5078, training loss: 9595.51, average training loss: 9682.05, base loss: 14433.49
[INFO 2017-06-28 13:45:13,475 main.py:51] epoch 5079, training loss: 10053.64, average training loss: 9683.16, base loss: 14436.10
[INFO 2017-06-28 13:45:14,119 main.py:51] epoch 5080, training loss: 9572.28, average training loss: 9682.90, base loss: 14434.35
[INFO 2017-06-28 13:45:14,787 main.py:51] epoch 5081, training loss: 9802.23, average training loss: 9683.02, base loss: 14434.73
[INFO 2017-06-28 13:45:15,422 main.py:51] epoch 5082, training loss: 8247.02, average training loss: 9683.45, base loss: 14435.70
[INFO 2017-06-28 13:45:16,088 main.py:51] epoch 5083, training loss: 9902.37, average training loss: 9683.42, base loss: 14436.54
[INFO 2017-06-28 13:45:16,768 main.py:51] epoch 5084, training loss: 9848.23, average training loss: 9684.69, base loss: 14438.11
[INFO 2017-06-28 13:45:17,444 main.py:51] epoch 5085, training loss: 10511.73, average training loss: 9685.78, base loss: 14439.15
[INFO 2017-06-28 13:45:18,114 main.py:51] epoch 5086, training loss: 9038.22, average training loss: 9684.96, base loss: 14439.07
[INFO 2017-06-28 13:45:18,783 main.py:51] epoch 5087, training loss: 9165.16, average training loss: 9684.31, base loss: 14437.21
[INFO 2017-06-28 13:45:19,431 main.py:51] epoch 5088, training loss: 9916.55, average training loss: 9683.65, base loss: 14435.79
[INFO 2017-06-28 13:45:20,073 main.py:51] epoch 5089, training loss: 9470.24, average training loss: 9683.97, base loss: 14437.62
[INFO 2017-06-28 13:45:20,735 main.py:51] epoch 5090, training loss: 8649.59, average training loss: 9683.56, base loss: 14438.44
[INFO 2017-06-28 13:45:21,403 main.py:51] epoch 5091, training loss: 11620.91, average training loss: 9686.32, base loss: 14442.08
[INFO 2017-06-28 13:45:22,058 main.py:51] epoch 5092, training loss: 9250.16, average training loss: 9687.44, base loss: 14445.64
[INFO 2017-06-28 13:45:22,717 main.py:51] epoch 5093, training loss: 9502.38, average training loss: 9688.08, base loss: 14446.83
[INFO 2017-06-28 13:45:23,369 main.py:51] epoch 5094, training loss: 11930.99, average training loss: 9690.60, base loss: 14449.54
[INFO 2017-06-28 13:45:24,034 main.py:51] epoch 5095, training loss: 9056.47, average training loss: 9691.68, base loss: 14450.37
[INFO 2017-06-28 13:45:24,677 main.py:51] epoch 5096, training loss: 10062.81, average training loss: 9691.42, base loss: 14450.10
[INFO 2017-06-28 13:45:25,326 main.py:51] epoch 5097, training loss: 8235.99, average training loss: 9689.52, base loss: 14447.40
[INFO 2017-06-28 13:45:25,967 main.py:51] epoch 5098, training loss: 11410.43, average training loss: 9691.61, base loss: 14449.67
[INFO 2017-06-28 13:45:26,607 main.py:51] epoch 5099, training loss: 8496.66, average training loss: 9690.61, base loss: 14448.09
[INFO 2017-06-28 13:45:26,607 main.py:53] epoch 5099, testing
[INFO 2017-06-28 13:45:29,207 main.py:105] average testing loss: 10783.57, base loss: 15170.98
[INFO 2017-06-28 13:45:29,207 main.py:106] improve_loss: 4387.41, improve_percent: 0.29
[INFO 2017-06-28 13:45:29,208 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:45:29,874 main.py:51] epoch 5100, training loss: 10497.99, average training loss: 9691.12, base loss: 14448.89
[INFO 2017-06-28 13:45:30,517 main.py:51] epoch 5101, training loss: 9143.33, average training loss: 9691.61, base loss: 14450.13
[INFO 2017-06-28 13:45:31,179 main.py:51] epoch 5102, training loss: 9057.17, average training loss: 9692.28, base loss: 14450.11
[INFO 2017-06-28 13:45:31,829 main.py:51] epoch 5103, training loss: 11137.69, average training loss: 9693.28, base loss: 14451.13
[INFO 2017-06-28 13:45:32,473 main.py:51] epoch 5104, training loss: 9636.84, average training loss: 9693.68, base loss: 14451.54
[INFO 2017-06-28 13:45:33,125 main.py:51] epoch 5105, training loss: 9378.04, average training loss: 9692.61, base loss: 14450.22
[INFO 2017-06-28 13:45:33,780 main.py:51] epoch 5106, training loss: 9185.76, average training loss: 9692.36, base loss: 14450.27
[INFO 2017-06-28 13:45:34,444 main.py:51] epoch 5107, training loss: 9517.50, average training loss: 9692.62, base loss: 14450.26
[INFO 2017-06-28 13:45:35,106 main.py:51] epoch 5108, training loss: 10210.12, average training loss: 9693.24, base loss: 14450.92
[INFO 2017-06-28 13:45:35,769 main.py:51] epoch 5109, training loss: 10303.05, average training loss: 9694.74, base loss: 14453.04
[INFO 2017-06-28 13:45:36,424 main.py:51] epoch 5110, training loss: 10017.48, average training loss: 9694.39, base loss: 14452.03
[INFO 2017-06-28 13:45:37,074 main.py:51] epoch 5111, training loss: 8910.89, average training loss: 9692.38, base loss: 14448.63
[INFO 2017-06-28 13:45:37,740 main.py:51] epoch 5112, training loss: 10171.11, average training loss: 9692.80, base loss: 14448.51
[INFO 2017-06-28 13:45:38,415 main.py:51] epoch 5113, training loss: 8646.55, average training loss: 9691.84, base loss: 14446.73
[INFO 2017-06-28 13:45:39,100 main.py:51] epoch 5114, training loss: 8648.61, average training loss: 9690.93, base loss: 14443.98
[INFO 2017-06-28 13:45:39,766 main.py:51] epoch 5115, training loss: 8892.74, average training loss: 9691.60, base loss: 14444.89
[INFO 2017-06-28 13:45:40,433 main.py:51] epoch 5116, training loss: 8422.74, average training loss: 9691.08, base loss: 14443.79
[INFO 2017-06-28 13:45:41,090 main.py:51] epoch 5117, training loss: 8314.60, average training loss: 9689.17, base loss: 14441.20
[INFO 2017-06-28 13:45:41,771 main.py:51] epoch 5118, training loss: 9522.07, average training loss: 9689.11, base loss: 14443.55
[INFO 2017-06-28 13:45:42,444 main.py:51] epoch 5119, training loss: 9482.27, average training loss: 9688.31, base loss: 14441.20
[INFO 2017-06-28 13:45:43,118 main.py:51] epoch 5120, training loss: 9217.29, average training loss: 9685.98, base loss: 14436.59
[INFO 2017-06-28 13:45:43,762 main.py:51] epoch 5121, training loss: 9299.92, average training loss: 9684.47, base loss: 14434.47
[INFO 2017-06-28 13:45:44,417 main.py:51] epoch 5122, training loss: 8833.02, average training loss: 9684.06, base loss: 14435.28
[INFO 2017-06-28 13:45:45,071 main.py:51] epoch 5123, training loss: 9160.35, average training loss: 9684.54, base loss: 14436.95
[INFO 2017-06-28 13:45:45,735 main.py:51] epoch 5124, training loss: 10632.20, average training loss: 9685.18, base loss: 14438.57
[INFO 2017-06-28 13:45:46,384 main.py:51] epoch 5125, training loss: 10263.79, average training loss: 9684.69, base loss: 14439.43
[INFO 2017-06-28 13:45:47,036 main.py:51] epoch 5126, training loss: 9303.88, average training loss: 9684.11, base loss: 14439.83
[INFO 2017-06-28 13:45:47,699 main.py:51] epoch 5127, training loss: 10301.82, average training loss: 9684.98, base loss: 14441.22
[INFO 2017-06-28 13:45:48,355 main.py:51] epoch 5128, training loss: 9712.06, average training loss: 9685.86, base loss: 14442.13
[INFO 2017-06-28 13:45:49,021 main.py:51] epoch 5129, training loss: 8675.46, average training loss: 9684.82, base loss: 14439.77
[INFO 2017-06-28 13:45:49,671 main.py:51] epoch 5130, training loss: 9306.79, average training loss: 9684.12, base loss: 14438.45
[INFO 2017-06-28 13:45:50,324 main.py:51] epoch 5131, training loss: 9635.83, average training loss: 9682.47, base loss: 14435.02
[INFO 2017-06-28 13:45:50,972 main.py:51] epoch 5132, training loss: 10896.80, average training loss: 9684.73, base loss: 14439.70
[INFO 2017-06-28 13:45:51,640 main.py:51] epoch 5133, training loss: 9627.34, average training loss: 9684.78, base loss: 14438.93
[INFO 2017-06-28 13:45:52,300 main.py:51] epoch 5134, training loss: 9841.67, average training loss: 9685.74, base loss: 14440.22
[INFO 2017-06-28 13:45:52,949 main.py:51] epoch 5135, training loss: 9372.16, average training loss: 9686.08, base loss: 14440.95
[INFO 2017-06-28 13:45:53,606 main.py:51] epoch 5136, training loss: 8695.99, average training loss: 9683.77, base loss: 14436.33
[INFO 2017-06-28 13:45:54,258 main.py:51] epoch 5137, training loss: 9372.57, average training loss: 9684.56, base loss: 14440.17
[INFO 2017-06-28 13:45:54,905 main.py:51] epoch 5138, training loss: 8716.08, average training loss: 9684.40, base loss: 14440.87
[INFO 2017-06-28 13:45:55,567 main.py:51] epoch 5139, training loss: 11527.56, average training loss: 9686.06, base loss: 14444.87
[INFO 2017-06-28 13:45:56,261 main.py:51] epoch 5140, training loss: 9330.45, average training loss: 9686.03, base loss: 14442.99
[INFO 2017-06-28 13:45:56,929 main.py:51] epoch 5141, training loss: 8995.56, average training loss: 9685.27, base loss: 14441.37
[INFO 2017-06-28 13:45:57,583 main.py:51] epoch 5142, training loss: 8088.78, average training loss: 9684.40, base loss: 14440.10
[INFO 2017-06-28 13:45:58,246 main.py:51] epoch 5143, training loss: 8898.34, average training loss: 9683.16, base loss: 14440.01
[INFO 2017-06-28 13:45:58,909 main.py:51] epoch 5144, training loss: 8403.18, average training loss: 9682.19, base loss: 14438.61
[INFO 2017-06-28 13:45:59,559 main.py:51] epoch 5145, training loss: 9965.85, average training loss: 9683.11, base loss: 14439.99
[INFO 2017-06-28 13:46:00,197 main.py:51] epoch 5146, training loss: 12195.73, average training loss: 9686.17, base loss: 14443.95
[INFO 2017-06-28 13:46:00,858 main.py:51] epoch 5147, training loss: 10067.63, average training loss: 9687.56, base loss: 14445.97
[INFO 2017-06-28 13:46:01,512 main.py:51] epoch 5148, training loss: 8931.27, average training loss: 9687.21, base loss: 14445.09
[INFO 2017-06-28 13:46:02,147 main.py:51] epoch 5149, training loss: 9746.41, average training loss: 9687.88, base loss: 14445.34
[INFO 2017-06-28 13:46:02,796 main.py:51] epoch 5150, training loss: 9037.53, average training loss: 9687.56, base loss: 14446.30
[INFO 2017-06-28 13:46:03,454 main.py:51] epoch 5151, training loss: 9353.19, average training loss: 9686.43, base loss: 14444.97
[INFO 2017-06-28 13:46:04,129 main.py:51] epoch 5152, training loss: 10026.48, average training loss: 9686.59, base loss: 14445.41
[INFO 2017-06-28 13:46:04,765 main.py:51] epoch 5153, training loss: 9053.71, average training loss: 9686.65, base loss: 14445.84
[INFO 2017-06-28 13:46:05,405 main.py:51] epoch 5154, training loss: 9919.61, average training loss: 9685.28, base loss: 14444.02
[INFO 2017-06-28 13:46:06,060 main.py:51] epoch 5155, training loss: 9240.64, average training loss: 9683.93, base loss: 14443.21
[INFO 2017-06-28 13:46:06,723 main.py:51] epoch 5156, training loss: 8160.06, average training loss: 9681.73, base loss: 14440.62
[INFO 2017-06-28 13:46:07,396 main.py:51] epoch 5157, training loss: 10442.63, average training loss: 9681.87, base loss: 14441.21
[INFO 2017-06-28 13:46:08,046 main.py:51] epoch 5158, training loss: 11203.30, average training loss: 9683.39, base loss: 14442.84
[INFO 2017-06-28 13:46:08,695 main.py:51] epoch 5159, training loss: 10276.40, average training loss: 9684.25, base loss: 14443.00
[INFO 2017-06-28 13:46:09,359 main.py:51] epoch 5160, training loss: 9981.57, average training loss: 9684.38, base loss: 14442.20
[INFO 2017-06-28 13:46:10,018 main.py:51] epoch 5161, training loss: 8624.84, average training loss: 9682.57, base loss: 14440.56
[INFO 2017-06-28 13:46:10,650 main.py:51] epoch 5162, training loss: 8499.28, average training loss: 9681.67, base loss: 14439.88
[INFO 2017-06-28 13:46:11,307 main.py:51] epoch 5163, training loss: 8365.21, average training loss: 9681.36, base loss: 14438.57
[INFO 2017-06-28 13:46:11,958 main.py:51] epoch 5164, training loss: 9817.15, average training loss: 9681.98, base loss: 14439.99
[INFO 2017-06-28 13:46:12,603 main.py:51] epoch 5165, training loss: 8693.56, average training loss: 9679.94, base loss: 14437.50
[INFO 2017-06-28 13:46:13,227 main.py:51] epoch 5166, training loss: 10162.44, average training loss: 9682.12, base loss: 14440.42
[INFO 2017-06-28 13:46:13,883 main.py:51] epoch 5167, training loss: 9780.47, average training loss: 9680.80, base loss: 14439.91
[INFO 2017-06-28 13:46:14,527 main.py:51] epoch 5168, training loss: 8821.76, average training loss: 9678.34, base loss: 14435.82
[INFO 2017-06-28 13:46:15,183 main.py:51] epoch 5169, training loss: 9093.50, average training loss: 9677.60, base loss: 14433.54
[INFO 2017-06-28 13:46:15,824 main.py:51] epoch 5170, training loss: 10167.56, average training loss: 9678.75, base loss: 14435.31
[INFO 2017-06-28 13:46:16,526 main.py:51] epoch 5171, training loss: 10546.54, average training loss: 9679.67, base loss: 14437.48
[INFO 2017-06-28 13:46:17,181 main.py:51] epoch 5172, training loss: 9549.82, average training loss: 9680.17, base loss: 14441.64
[INFO 2017-06-28 13:46:17,834 main.py:51] epoch 5173, training loss: 8704.51, average training loss: 9679.18, base loss: 14441.55
[INFO 2017-06-28 13:46:18,486 main.py:51] epoch 5174, training loss: 10214.45, average training loss: 9679.45, base loss: 14442.18
[INFO 2017-06-28 13:46:19,130 main.py:51] epoch 5175, training loss: 9991.36, average training loss: 9679.64, base loss: 14441.51
[INFO 2017-06-28 13:46:19,775 main.py:51] epoch 5176, training loss: 10902.40, average training loss: 9680.16, base loss: 14442.21
[INFO 2017-06-28 13:46:20,426 main.py:51] epoch 5177, training loss: 9390.97, average training loss: 9680.88, base loss: 14445.21
[INFO 2017-06-28 13:46:21,097 main.py:51] epoch 5178, training loss: 10459.76, average training loss: 9683.56, base loss: 14448.23
[INFO 2017-06-28 13:46:21,796 main.py:51] epoch 5179, training loss: 10127.35, average training loss: 9683.44, base loss: 14449.35
[INFO 2017-06-28 13:46:22,461 main.py:51] epoch 5180, training loss: 8698.86, average training loss: 9682.37, base loss: 14447.12
[INFO 2017-06-28 13:46:23,104 main.py:51] epoch 5181, training loss: 9619.59, average training loss: 9683.33, base loss: 14448.06
[INFO 2017-06-28 13:46:23,765 main.py:51] epoch 5182, training loss: 8568.09, average training loss: 9683.17, base loss: 14448.11
[INFO 2017-06-28 13:46:24,422 main.py:51] epoch 5183, training loss: 8979.54, average training loss: 9683.90, base loss: 14448.29
[INFO 2017-06-28 13:46:25,094 main.py:51] epoch 5184, training loss: 7837.53, average training loss: 9682.38, base loss: 14443.32
[INFO 2017-06-28 13:46:25,727 main.py:51] epoch 5185, training loss: 9427.90, average training loss: 9683.46, base loss: 14444.27
[INFO 2017-06-28 13:46:26,382 main.py:51] epoch 5186, training loss: 8987.51, average training loss: 9682.34, base loss: 14441.36
[INFO 2017-06-28 13:46:27,067 main.py:51] epoch 5187, training loss: 10160.90, average training loss: 9681.46, base loss: 14439.84
[INFO 2017-06-28 13:46:27,717 main.py:51] epoch 5188, training loss: 10877.72, average training loss: 9682.16, base loss: 14441.00
[INFO 2017-06-28 13:46:28,370 main.py:51] epoch 5189, training loss: 9841.37, average training loss: 9682.00, base loss: 14441.97
[INFO 2017-06-28 13:46:29,048 main.py:51] epoch 5190, training loss: 8228.58, average training loss: 9681.64, base loss: 14440.18
[INFO 2017-06-28 13:46:29,704 main.py:51] epoch 5191, training loss: 9891.02, average training loss: 9681.29, base loss: 14437.65
[INFO 2017-06-28 13:46:30,352 main.py:51] epoch 5192, training loss: 10673.51, average training loss: 9682.57, base loss: 14438.25
[INFO 2017-06-28 13:46:31,018 main.py:51] epoch 5193, training loss: 9843.93, average training loss: 9683.98, base loss: 14439.25
[INFO 2017-06-28 13:46:31,665 main.py:51] epoch 5194, training loss: 9513.45, average training loss: 9682.66, base loss: 14438.72
[INFO 2017-06-28 13:46:32,306 main.py:51] epoch 5195, training loss: 9108.35, average training loss: 9681.78, base loss: 14438.86
[INFO 2017-06-28 13:46:32,977 main.py:51] epoch 5196, training loss: 10634.22, average training loss: 9681.17, base loss: 14436.75
[INFO 2017-06-28 13:46:33,636 main.py:51] epoch 5197, training loss: 11095.73, average training loss: 9681.70, base loss: 14437.81
[INFO 2017-06-28 13:46:34,276 main.py:51] epoch 5198, training loss: 10956.27, average training loss: 9683.74, base loss: 14439.80
[INFO 2017-06-28 13:46:34,931 main.py:51] epoch 5199, training loss: 8844.99, average training loss: 9683.41, base loss: 14439.17
[INFO 2017-06-28 13:46:34,931 main.py:53] epoch 5199, testing
[INFO 2017-06-28 13:46:37,531 main.py:105] average testing loss: 10795.94, base loss: 15462.93
[INFO 2017-06-28 13:46:37,531 main.py:106] improve_loss: 4666.99, improve_percent: 0.30
[INFO 2017-06-28 13:46:37,532 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:46:38,181 main.py:51] epoch 5200, training loss: 8923.69, average training loss: 9682.97, base loss: 14438.78
[INFO 2017-06-28 13:46:38,841 main.py:51] epoch 5201, training loss: 10399.35, average training loss: 9684.94, base loss: 14441.34
[INFO 2017-06-28 13:46:39,485 main.py:51] epoch 5202, training loss: 9699.30, average training loss: 9686.20, base loss: 14442.34
[INFO 2017-06-28 13:46:40,135 main.py:51] epoch 5203, training loss: 8621.43, average training loss: 9685.43, base loss: 14441.65
[INFO 2017-06-28 13:46:40,774 main.py:51] epoch 5204, training loss: 9958.76, average training loss: 9686.17, base loss: 14444.00
[INFO 2017-06-28 13:46:41,437 main.py:51] epoch 5205, training loss: 9219.10, average training loss: 9683.55, base loss: 14438.95
[INFO 2017-06-28 13:46:42,089 main.py:51] epoch 5206, training loss: 10119.50, average training loss: 9682.25, base loss: 14438.14
[INFO 2017-06-28 13:46:42,748 main.py:51] epoch 5207, training loss: 9048.33, average training loss: 9682.86, base loss: 14439.54
[INFO 2017-06-28 13:46:43,405 main.py:51] epoch 5208, training loss: 9624.89, average training loss: 9682.65, base loss: 14439.82
[INFO 2017-06-28 13:46:44,060 main.py:51] epoch 5209, training loss: 9377.48, average training loss: 9680.95, base loss: 14437.55
[INFO 2017-06-28 13:46:44,708 main.py:51] epoch 5210, training loss: 10200.68, average training loss: 9680.09, base loss: 14435.19
[INFO 2017-06-28 13:46:45,356 main.py:51] epoch 5211, training loss: 9888.10, average training loss: 9681.17, base loss: 14435.94
[INFO 2017-06-28 13:46:46,004 main.py:51] epoch 5212, training loss: 8013.50, average training loss: 9678.63, base loss: 14432.56
[INFO 2017-06-28 13:46:46,664 main.py:51] epoch 5213, training loss: 9716.91, average training loss: 9675.98, base loss: 14431.33
[INFO 2017-06-28 13:46:47,334 main.py:51] epoch 5214, training loss: 11654.63, average training loss: 9678.97, base loss: 14437.30
[INFO 2017-06-28 13:46:47,997 main.py:51] epoch 5215, training loss: 9858.68, average training loss: 9678.89, base loss: 14435.13
[INFO 2017-06-28 13:46:48,665 main.py:51] epoch 5216, training loss: 10169.34, average training loss: 9678.74, base loss: 14436.37
[INFO 2017-06-28 13:46:49,310 main.py:51] epoch 5217, training loss: 10910.16, average training loss: 9679.21, base loss: 14436.90
[INFO 2017-06-28 13:46:49,957 main.py:51] epoch 5218, training loss: 10386.13, average training loss: 9679.32, base loss: 14435.41
[INFO 2017-06-28 13:46:50,600 main.py:51] epoch 5219, training loss: 9353.38, average training loss: 9677.98, base loss: 14434.58
[INFO 2017-06-28 13:46:51,243 main.py:51] epoch 5220, training loss: 9396.24, average training loss: 9678.52, base loss: 14435.42
[INFO 2017-06-28 13:46:51,903 main.py:51] epoch 5221, training loss: 9573.67, average training loss: 9679.57, base loss: 14436.65
[INFO 2017-06-28 13:46:52,541 main.py:51] epoch 5222, training loss: 9927.78, average training loss: 9679.98, base loss: 14433.63
[INFO 2017-06-28 13:46:53,207 main.py:51] epoch 5223, training loss: 9250.48, average training loss: 9680.10, base loss: 14434.70
[INFO 2017-06-28 13:46:53,892 main.py:51] epoch 5224, training loss: 8798.05, average training loss: 9679.55, base loss: 14434.10
[INFO 2017-06-28 13:46:54,543 main.py:51] epoch 5225, training loss: 9297.21, average training loss: 9678.93, base loss: 14432.09
[INFO 2017-06-28 13:46:55,191 main.py:51] epoch 5226, training loss: 9089.29, average training loss: 9678.02, base loss: 14431.08
[INFO 2017-06-28 13:46:55,869 main.py:51] epoch 5227, training loss: 8846.49, average training loss: 9678.67, base loss: 14431.49
[INFO 2017-06-28 13:46:56,504 main.py:51] epoch 5228, training loss: 9566.36, average training loss: 9679.17, base loss: 14431.64
[INFO 2017-06-28 13:46:57,162 main.py:51] epoch 5229, training loss: 10290.59, average training loss: 9680.13, base loss: 14432.50
[INFO 2017-06-28 13:46:57,826 main.py:51] epoch 5230, training loss: 9263.52, average training loss: 9680.38, base loss: 14433.00
[INFO 2017-06-28 13:46:58,461 main.py:51] epoch 5231, training loss: 8314.71, average training loss: 9678.15, base loss: 14428.75
[INFO 2017-06-28 13:46:59,131 main.py:51] epoch 5232, training loss: 11406.49, average training loss: 9679.16, base loss: 14428.73
[INFO 2017-06-28 13:46:59,806 main.py:51] epoch 5233, training loss: 9329.15, average training loss: 9678.86, base loss: 14428.95
[INFO 2017-06-28 13:47:00,459 main.py:51] epoch 5234, training loss: 9797.59, average training loss: 9679.01, base loss: 14429.72
[INFO 2017-06-28 13:47:01,125 main.py:51] epoch 5235, training loss: 9177.68, average training loss: 9678.33, base loss: 14428.03
[INFO 2017-06-28 13:47:01,782 main.py:51] epoch 5236, training loss: 10763.24, average training loss: 9678.52, base loss: 14427.54
[INFO 2017-06-28 13:47:02,444 main.py:51] epoch 5237, training loss: 10484.17, average training loss: 9679.04, base loss: 14429.42
[INFO 2017-06-28 13:47:03,096 main.py:51] epoch 5238, training loss: 9064.02, average training loss: 9677.93, base loss: 14429.93
[INFO 2017-06-28 13:47:03,744 main.py:51] epoch 5239, training loss: 9167.19, average training loss: 9676.64, base loss: 14427.63
[INFO 2017-06-28 13:47:04,406 main.py:51] epoch 5240, training loss: 9633.81, average training loss: 9675.88, base loss: 14427.12
[INFO 2017-06-28 13:47:05,066 main.py:51] epoch 5241, training loss: 10314.55, average training loss: 9677.36, base loss: 14428.84
[INFO 2017-06-28 13:47:05,716 main.py:51] epoch 5242, training loss: 10240.26, average training loss: 9677.91, base loss: 14430.36
[INFO 2017-06-28 13:47:06,386 main.py:51] epoch 5243, training loss: 9171.18, average training loss: 9676.17, base loss: 14427.78
[INFO 2017-06-28 13:47:07,036 main.py:51] epoch 5244, training loss: 9185.94, average training loss: 9675.92, base loss: 14429.62
[INFO 2017-06-28 13:47:07,692 main.py:51] epoch 5245, training loss: 9584.58, average training loss: 9676.34, base loss: 14431.17
[INFO 2017-06-28 13:47:08,346 main.py:51] epoch 5246, training loss: 9181.96, average training loss: 9676.70, base loss: 14430.76
[INFO 2017-06-28 13:47:09,009 main.py:51] epoch 5247, training loss: 8878.20, average training loss: 9675.24, base loss: 14427.52
[INFO 2017-06-28 13:47:09,676 main.py:51] epoch 5248, training loss: 9683.60, average training loss: 9675.48, base loss: 14426.31
[INFO 2017-06-28 13:47:10,316 main.py:51] epoch 5249, training loss: 10000.51, average training loss: 9675.73, base loss: 14426.66
[INFO 2017-06-28 13:47:10,960 main.py:51] epoch 5250, training loss: 8576.99, average training loss: 9673.68, base loss: 14423.87
[INFO 2017-06-28 13:47:11,613 main.py:51] epoch 5251, training loss: 8867.76, average training loss: 9673.49, base loss: 14423.62
[INFO 2017-06-28 13:47:12,281 main.py:51] epoch 5252, training loss: 9324.25, average training loss: 9673.86, base loss: 14423.27
[INFO 2017-06-28 13:47:12,940 main.py:51] epoch 5253, training loss: 9200.77, average training loss: 9674.27, base loss: 14423.21
[INFO 2017-06-28 13:47:13,595 main.py:51] epoch 5254, training loss: 9879.02, average training loss: 9674.34, base loss: 14423.72
[INFO 2017-06-28 13:47:14,233 main.py:51] epoch 5255, training loss: 10186.49, average training loss: 9674.44, base loss: 14424.49
[INFO 2017-06-28 13:47:14,873 main.py:51] epoch 5256, training loss: 9260.01, average training loss: 9675.20, base loss: 14425.55
[INFO 2017-06-28 13:47:15,511 main.py:51] epoch 5257, training loss: 9139.51, average training loss: 9673.38, base loss: 14422.87
[INFO 2017-06-28 13:47:16,178 main.py:51] epoch 5258, training loss: 10009.80, average training loss: 9673.90, base loss: 14423.05
[INFO 2017-06-28 13:47:16,825 main.py:51] epoch 5259, training loss: 9578.69, average training loss: 9674.51, base loss: 14422.76
[INFO 2017-06-28 13:47:17,470 main.py:51] epoch 5260, training loss: 9077.09, average training loss: 9674.38, base loss: 14420.81
[INFO 2017-06-28 13:47:18,125 main.py:51] epoch 5261, training loss: 8887.05, average training loss: 9671.27, base loss: 14416.50
[INFO 2017-06-28 13:47:18,797 main.py:51] epoch 5262, training loss: 9653.03, average training loss: 9670.71, base loss: 14416.36
[INFO 2017-06-28 13:47:19,447 main.py:51] epoch 5263, training loss: 9865.42, average training loss: 9671.84, base loss: 14418.66
[INFO 2017-06-28 13:47:20,096 main.py:51] epoch 5264, training loss: 10116.43, average training loss: 9673.38, base loss: 14421.05
[INFO 2017-06-28 13:47:20,749 main.py:51] epoch 5265, training loss: 8985.80, average training loss: 9672.86, base loss: 14420.68
[INFO 2017-06-28 13:47:21,400 main.py:51] epoch 5266, training loss: 9296.95, average training loss: 9672.31, base loss: 14420.57
[INFO 2017-06-28 13:47:22,054 main.py:51] epoch 5267, training loss: 8872.43, average training loss: 9672.64, base loss: 14419.72
[INFO 2017-06-28 13:47:22,704 main.py:51] epoch 5268, training loss: 8327.55, average training loss: 9670.73, base loss: 14416.67
[INFO 2017-06-28 13:47:23,366 main.py:51] epoch 5269, training loss: 11001.71, average training loss: 9672.43, base loss: 14419.10
[INFO 2017-06-28 13:47:24,034 main.py:51] epoch 5270, training loss: 9853.87, average training loss: 9672.81, base loss: 14420.02
[INFO 2017-06-28 13:47:24,719 main.py:51] epoch 5271, training loss: 11068.67, average training loss: 9674.94, base loss: 14422.29
[INFO 2017-06-28 13:47:25,401 main.py:51] epoch 5272, training loss: 9981.71, average training loss: 9673.61, base loss: 14421.41
[INFO 2017-06-28 13:47:26,039 main.py:51] epoch 5273, training loss: 8417.53, average training loss: 9672.01, base loss: 14419.07
[INFO 2017-06-28 13:47:26,688 main.py:51] epoch 5274, training loss: 9024.91, average training loss: 9669.29, base loss: 14416.89
[INFO 2017-06-28 13:47:27,342 main.py:51] epoch 5275, training loss: 9046.33, average training loss: 9667.75, base loss: 14415.75
[INFO 2017-06-28 13:47:28,002 main.py:51] epoch 5276, training loss: 9648.98, average training loss: 9666.97, base loss: 14415.45
[INFO 2017-06-28 13:47:28,658 main.py:51] epoch 5277, training loss: 9335.59, average training loss: 9668.04, base loss: 14416.25
[INFO 2017-06-28 13:47:29,310 main.py:51] epoch 5278, training loss: 8911.49, average training loss: 9667.75, base loss: 14415.95
[INFO 2017-06-28 13:47:29,950 main.py:51] epoch 5279, training loss: 9778.43, average training loss: 9668.69, base loss: 14418.68
[INFO 2017-06-28 13:47:30,617 main.py:51] epoch 5280, training loss: 10419.95, average training loss: 9669.23, base loss: 14420.29
[INFO 2017-06-28 13:47:31,293 main.py:51] epoch 5281, training loss: 8080.76, average training loss: 9668.24, base loss: 14418.53
[INFO 2017-06-28 13:47:31,944 main.py:51] epoch 5282, training loss: 11248.76, average training loss: 9670.32, base loss: 14423.76
[INFO 2017-06-28 13:47:32,605 main.py:51] epoch 5283, training loss: 8806.39, average training loss: 9669.18, base loss: 14421.28
[INFO 2017-06-28 13:47:33,264 main.py:51] epoch 5284, training loss: 10098.74, average training loss: 9669.31, base loss: 14422.68
[INFO 2017-06-28 13:47:33,925 main.py:51] epoch 5285, training loss: 8721.87, average training loss: 9669.09, base loss: 14422.17
[INFO 2017-06-28 13:47:34,594 main.py:51] epoch 5286, training loss: 9390.95, average training loss: 9670.24, base loss: 14424.99
[INFO 2017-06-28 13:47:35,246 main.py:51] epoch 5287, training loss: 8930.13, average training loss: 9667.44, base loss: 14420.81
[INFO 2017-06-28 13:47:35,898 main.py:51] epoch 5288, training loss: 8729.05, average training loss: 9667.02, base loss: 14420.92
[INFO 2017-06-28 13:47:36,563 main.py:51] epoch 5289, training loss: 10564.73, average training loss: 9668.81, base loss: 14424.46
[INFO 2017-06-28 13:47:37,229 main.py:51] epoch 5290, training loss: 10154.85, average training loss: 9669.15, base loss: 14424.60
[INFO 2017-06-28 13:47:37,875 main.py:51] epoch 5291, training loss: 9764.51, average training loss: 9670.64, base loss: 14427.46
[INFO 2017-06-28 13:47:38,539 main.py:51] epoch 5292, training loss: 9384.51, average training loss: 9670.38, base loss: 14426.63
[INFO 2017-06-28 13:47:39,185 main.py:51] epoch 5293, training loss: 9692.93, average training loss: 9670.85, base loss: 14428.40
[INFO 2017-06-28 13:47:39,835 main.py:51] epoch 5294, training loss: 8615.17, average training loss: 9668.48, base loss: 14425.54
[INFO 2017-06-28 13:47:40,490 main.py:51] epoch 5295, training loss: 9290.56, average training loss: 9669.65, base loss: 14427.67
[INFO 2017-06-28 13:47:41,133 main.py:51] epoch 5296, training loss: 9470.98, average training loss: 9669.15, base loss: 14429.07
[INFO 2017-06-28 13:47:41,799 main.py:51] epoch 5297, training loss: 9910.78, average training loss: 9669.82, base loss: 14430.96
[INFO 2017-06-28 13:47:42,469 main.py:51] epoch 5298, training loss: 9764.38, average training loss: 9671.33, base loss: 14433.05
[INFO 2017-06-28 13:47:43,139 main.py:51] epoch 5299, training loss: 9500.66, average training loss: 9670.59, base loss: 14432.49
[INFO 2017-06-28 13:47:43,140 main.py:53] epoch 5299, testing
[INFO 2017-06-28 13:47:45,737 main.py:105] average testing loss: 10458.79, base loss: 14906.55
[INFO 2017-06-28 13:47:45,737 main.py:106] improve_loss: 4447.76, improve_percent: 0.30
[INFO 2017-06-28 13:47:45,737 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:47:46,406 main.py:51] epoch 5300, training loss: 8611.27, average training loss: 9668.26, base loss: 14427.22
[INFO 2017-06-28 13:47:47,063 main.py:51] epoch 5301, training loss: 9136.24, average training loss: 9668.00, base loss: 14425.92
[INFO 2017-06-28 13:47:47,711 main.py:51] epoch 5302, training loss: 9710.37, average training loss: 9667.99, base loss: 14425.00
[INFO 2017-06-28 13:47:48,357 main.py:51] epoch 5303, training loss: 9654.69, average training loss: 9668.73, base loss: 14429.01
[INFO 2017-06-28 13:47:49,021 main.py:51] epoch 5304, training loss: 9542.70, average training loss: 9667.47, base loss: 14427.75
[INFO 2017-06-28 13:47:49,669 main.py:51] epoch 5305, training loss: 10197.03, average training loss: 9667.71, base loss: 14429.06
[INFO 2017-06-28 13:47:50,368 main.py:51] epoch 5306, training loss: 9479.41, average training loss: 9668.09, base loss: 14430.34
[INFO 2017-06-28 13:47:51,011 main.py:51] epoch 5307, training loss: 9392.40, average training loss: 9668.39, base loss: 14430.46
[INFO 2017-06-28 13:47:51,688 main.py:51] epoch 5308, training loss: 10195.97, average training loss: 9668.34, base loss: 14430.98
[INFO 2017-06-28 13:47:52,337 main.py:51] epoch 5309, training loss: 9568.30, average training loss: 9668.59, base loss: 14432.37
[INFO 2017-06-28 13:47:53,000 main.py:51] epoch 5310, training loss: 8675.17, average training loss: 9667.61, base loss: 14431.07
[INFO 2017-06-28 13:47:53,667 main.py:51] epoch 5311, training loss: 9451.69, average training loss: 9668.17, base loss: 14431.79
[INFO 2017-06-28 13:47:54,308 main.py:51] epoch 5312, training loss: 9491.92, average training loss: 9668.45, base loss: 14433.58
[INFO 2017-06-28 13:47:54,957 main.py:51] epoch 5313, training loss: 9806.84, average training loss: 9668.16, base loss: 14434.10
[INFO 2017-06-28 13:47:55,605 main.py:51] epoch 5314, training loss: 10165.08, average training loss: 9670.16, base loss: 14437.40
[INFO 2017-06-28 13:47:56,268 main.py:51] epoch 5315, training loss: 8621.32, average training loss: 9669.46, base loss: 14434.45
[INFO 2017-06-28 13:47:56,914 main.py:51] epoch 5316, training loss: 11188.28, average training loss: 9672.15, base loss: 14439.07
[INFO 2017-06-28 13:47:57,555 main.py:51] epoch 5317, training loss: 9035.34, average training loss: 9671.01, base loss: 14435.32
[INFO 2017-06-28 13:47:58,222 main.py:51] epoch 5318, training loss: 9956.84, average training loss: 9669.60, base loss: 14433.15
[INFO 2017-06-28 13:47:58,878 main.py:51] epoch 5319, training loss: 8984.03, average training loss: 9668.70, base loss: 14431.16
[INFO 2017-06-28 13:47:59,532 main.py:51] epoch 5320, training loss: 9554.89, average training loss: 9668.88, base loss: 14431.21
[INFO 2017-06-28 13:48:00,185 main.py:51] epoch 5321, training loss: 9476.67, average training loss: 9667.63, base loss: 14427.94
[INFO 2017-06-28 13:48:00,835 main.py:51] epoch 5322, training loss: 9001.03, average training loss: 9667.33, base loss: 14429.04
[INFO 2017-06-28 13:48:01,506 main.py:51] epoch 5323, training loss: 9928.21, average training loss: 9667.25, base loss: 14429.86
[INFO 2017-06-28 13:48:02,171 main.py:51] epoch 5324, training loss: 10704.21, average training loss: 9668.74, base loss: 14430.67
[INFO 2017-06-28 13:48:02,823 main.py:51] epoch 5325, training loss: 9684.51, average training loss: 9668.20, base loss: 14431.56
[INFO 2017-06-28 13:48:03,471 main.py:51] epoch 5326, training loss: 8625.20, average training loss: 9666.68, base loss: 14427.92
[INFO 2017-06-28 13:48:04,138 main.py:51] epoch 5327, training loss: 9277.07, average training loss: 9667.44, base loss: 14428.25
[INFO 2017-06-28 13:48:04,831 main.py:51] epoch 5328, training loss: 10990.42, average training loss: 9669.49, base loss: 14431.72
[INFO 2017-06-28 13:48:05,493 main.py:51] epoch 5329, training loss: 9482.70, average training loss: 9670.07, base loss: 14433.30
[INFO 2017-06-28 13:48:06,153 main.py:51] epoch 5330, training loss: 9508.09, average training loss: 9670.95, base loss: 14434.57
[INFO 2017-06-28 13:48:06,803 main.py:51] epoch 5331, training loss: 11683.54, average training loss: 9672.25, base loss: 14436.60
[INFO 2017-06-28 13:48:07,438 main.py:51] epoch 5332, training loss: 10454.76, average training loss: 9673.26, base loss: 14438.43
[INFO 2017-06-28 13:48:08,082 main.py:51] epoch 5333, training loss: 8550.18, average training loss: 9672.31, base loss: 14436.56
[INFO 2017-06-28 13:48:08,738 main.py:51] epoch 5334, training loss: 9006.63, average training loss: 9672.67, base loss: 14437.32
[INFO 2017-06-28 13:48:09,369 main.py:51] epoch 5335, training loss: 9452.07, average training loss: 9673.44, base loss: 14438.64
[INFO 2017-06-28 13:48:10,014 main.py:51] epoch 5336, training loss: 10229.22, average training loss: 9674.44, base loss: 14438.24
[INFO 2017-06-28 13:48:10,694 main.py:51] epoch 5337, training loss: 8779.52, average training loss: 9674.90, base loss: 14438.17
[INFO 2017-06-28 13:48:11,327 main.py:51] epoch 5338, training loss: 8658.01, average training loss: 9672.61, base loss: 14434.68
[INFO 2017-06-28 13:48:11,984 main.py:51] epoch 5339, training loss: 8233.44, average training loss: 9670.11, base loss: 14430.54
[INFO 2017-06-28 13:48:12,649 main.py:51] epoch 5340, training loss: 10037.70, average training loss: 9669.50, base loss: 14429.48
[INFO 2017-06-28 13:48:13,308 main.py:51] epoch 5341, training loss: 9256.92, average training loss: 9668.67, base loss: 14429.12
[INFO 2017-06-28 13:48:13,948 main.py:51] epoch 5342, training loss: 10217.22, average training loss: 9668.60, base loss: 14428.90
[INFO 2017-06-28 13:48:14,591 main.py:51] epoch 5343, training loss: 9070.98, average training loss: 9667.98, base loss: 14426.90
[INFO 2017-06-28 13:48:15,254 main.py:51] epoch 5344, training loss: 8907.54, average training loss: 9667.79, base loss: 14426.46
[INFO 2017-06-28 13:48:15,906 main.py:51] epoch 5345, training loss: 7949.93, average training loss: 9665.34, base loss: 14423.16
[INFO 2017-06-28 13:48:16,611 main.py:51] epoch 5346, training loss: 8353.21, average training loss: 9663.25, base loss: 14420.47
[INFO 2017-06-28 13:48:17,258 main.py:51] epoch 5347, training loss: 10573.58, average training loss: 9662.97, base loss: 14421.52
[INFO 2017-06-28 13:48:17,914 main.py:51] epoch 5348, training loss: 9880.13, average training loss: 9662.70, base loss: 14421.28
[INFO 2017-06-28 13:48:18,595 main.py:51] epoch 5349, training loss: 10251.57, average training loss: 9661.51, base loss: 14419.17
[INFO 2017-06-28 13:48:19,245 main.py:51] epoch 5350, training loss: 8774.45, average training loss: 9660.05, base loss: 14417.20
[INFO 2017-06-28 13:48:19,903 main.py:51] epoch 5351, training loss: 10519.92, average training loss: 9661.61, base loss: 14419.33
[INFO 2017-06-28 13:48:20,527 main.py:51] epoch 5352, training loss: 9209.05, average training loss: 9661.50, base loss: 14420.02
[INFO 2017-06-28 13:48:21,196 main.py:51] epoch 5353, training loss: 10734.61, average training loss: 9662.48, base loss: 14422.54
[INFO 2017-06-28 13:48:21,861 main.py:51] epoch 5354, training loss: 10203.27, average training loss: 9664.04, base loss: 14423.34
[INFO 2017-06-28 13:48:22,518 main.py:51] epoch 5355, training loss: 11749.59, average training loss: 9663.81, base loss: 14420.48
[INFO 2017-06-28 13:48:23,174 main.py:51] epoch 5356, training loss: 8886.13, average training loss: 9663.33, base loss: 14419.97
[INFO 2017-06-28 13:48:23,831 main.py:51] epoch 5357, training loss: 9055.25, average training loss: 9662.25, base loss: 14418.76
[INFO 2017-06-28 13:48:24,484 main.py:51] epoch 5358, training loss: 8439.46, average training loss: 9660.96, base loss: 14416.67
[INFO 2017-06-28 13:48:25,129 main.py:51] epoch 5359, training loss: 8421.40, average training loss: 9659.58, base loss: 14414.82
[INFO 2017-06-28 13:48:25,795 main.py:51] epoch 5360, training loss: 9415.22, average training loss: 9660.07, base loss: 14415.22
[INFO 2017-06-28 13:48:26,458 main.py:51] epoch 5361, training loss: 9434.74, average training loss: 9660.75, base loss: 14417.18
[INFO 2017-06-28 13:48:27,099 main.py:51] epoch 5362, training loss: 9346.13, average training loss: 9661.01, base loss: 14419.65
[INFO 2017-06-28 13:48:27,742 main.py:51] epoch 5363, training loss: 8976.39, average training loss: 9661.05, base loss: 14419.06
[INFO 2017-06-28 13:48:28,396 main.py:51] epoch 5364, training loss: 9826.76, average training loss: 9659.94, base loss: 14417.94
[INFO 2017-06-28 13:48:29,037 main.py:51] epoch 5365, training loss: 9544.14, average training loss: 9659.97, base loss: 14418.94
[INFO 2017-06-28 13:48:29,707 main.py:51] epoch 5366, training loss: 9664.00, average training loss: 9659.44, base loss: 14418.93
[INFO 2017-06-28 13:48:30,396 main.py:51] epoch 5367, training loss: 10352.36, average training loss: 9659.16, base loss: 14417.02
[INFO 2017-06-28 13:48:31,064 main.py:51] epoch 5368, training loss: 8883.10, average training loss: 9659.46, base loss: 14417.82
[INFO 2017-06-28 13:48:31,732 main.py:51] epoch 5369, training loss: 9776.09, average training loss: 9659.36, base loss: 14419.79
[INFO 2017-06-28 13:48:32,399 main.py:51] epoch 5370, training loss: 9208.95, average training loss: 9657.36, base loss: 14417.24
[INFO 2017-06-28 13:48:33,056 main.py:51] epoch 5371, training loss: 10756.96, average training loss: 9657.89, base loss: 14417.98
[INFO 2017-06-28 13:48:33,697 main.py:51] epoch 5372, training loss: 9424.20, average training loss: 9657.36, base loss: 14416.63
[INFO 2017-06-28 13:48:34,373 main.py:51] epoch 5373, training loss: 9593.66, average training loss: 9655.31, base loss: 14414.83
[INFO 2017-06-28 13:48:35,037 main.py:51] epoch 5374, training loss: 9043.56, average training loss: 9655.48, base loss: 14413.68
[INFO 2017-06-28 13:48:35,703 main.py:51] epoch 5375, training loss: 9936.10, average training loss: 9655.69, base loss: 14414.94
[INFO 2017-06-28 13:48:36,367 main.py:51] epoch 5376, training loss: 9841.24, average training loss: 9655.44, base loss: 14414.60
[INFO 2017-06-28 13:48:37,033 main.py:51] epoch 5377, training loss: 9605.84, average training loss: 9655.19, base loss: 14414.18
[INFO 2017-06-28 13:48:37,688 main.py:51] epoch 5378, training loss: 8367.73, average training loss: 9653.76, base loss: 14410.59
[INFO 2017-06-28 13:48:38,344 main.py:51] epoch 5379, training loss: 10123.89, average training loss: 9654.40, base loss: 14410.58
[INFO 2017-06-28 13:48:38,989 main.py:51] epoch 5380, training loss: 10219.13, average training loss: 9654.64, base loss: 14408.75
[INFO 2017-06-28 13:48:39,639 main.py:51] epoch 5381, training loss: 11102.88, average training loss: 9655.90, base loss: 14411.77
[INFO 2017-06-28 13:48:40,299 main.py:51] epoch 5382, training loss: 9218.63, average training loss: 9654.93, base loss: 14411.59
[INFO 2017-06-28 13:48:40,969 main.py:51] epoch 5383, training loss: 9440.06, average training loss: 9654.30, base loss: 14409.02
[INFO 2017-06-28 13:48:41,619 main.py:51] epoch 5384, training loss: 9422.71, average training loss: 9653.54, base loss: 14408.74
[INFO 2017-06-28 13:48:42,267 main.py:51] epoch 5385, training loss: 10285.62, average training loss: 9655.27, base loss: 14410.49
[INFO 2017-06-28 13:48:42,923 main.py:51] epoch 5386, training loss: 8644.97, average training loss: 9654.12, base loss: 14410.45
[INFO 2017-06-28 13:48:43,588 main.py:51] epoch 5387, training loss: 10232.79, average training loss: 9654.27, base loss: 14411.37
[INFO 2017-06-28 13:48:44,253 main.py:51] epoch 5388, training loss: 8800.25, average training loss: 9653.03, base loss: 14410.50
[INFO 2017-06-28 13:48:44,907 main.py:51] epoch 5389, training loss: 9361.97, average training loss: 9652.53, base loss: 14410.56
[INFO 2017-06-28 13:48:45,561 main.py:51] epoch 5390, training loss: 9134.70, average training loss: 9651.38, base loss: 14407.79
[INFO 2017-06-28 13:48:46,220 main.py:51] epoch 5391, training loss: 10227.03, average training loss: 9653.01, base loss: 14410.49
[INFO 2017-06-28 13:48:46,870 main.py:51] epoch 5392, training loss: 9723.05, average training loss: 9652.95, base loss: 14411.08
[INFO 2017-06-28 13:48:47,512 main.py:51] epoch 5393, training loss: 8658.14, average training loss: 9651.68, base loss: 14410.59
[INFO 2017-06-28 13:48:48,158 main.py:51] epoch 5394, training loss: 8581.08, average training loss: 9651.00, base loss: 14410.45
[INFO 2017-06-28 13:48:48,821 main.py:51] epoch 5395, training loss: 10395.54, average training loss: 9651.73, base loss: 14412.53
[INFO 2017-06-28 13:48:49,469 main.py:51] epoch 5396, training loss: 9122.81, average training loss: 9650.36, base loss: 14410.89
[INFO 2017-06-28 13:48:50,141 main.py:51] epoch 5397, training loss: 10806.39, average training loss: 9651.17, base loss: 14410.62
[INFO 2017-06-28 13:48:50,832 main.py:51] epoch 5398, training loss: 9709.71, average training loss: 9649.81, base loss: 14409.56
[INFO 2017-06-28 13:48:51,495 main.py:51] epoch 5399, training loss: 10981.19, average training loss: 9650.72, base loss: 14411.85
[INFO 2017-06-28 13:48:51,495 main.py:53] epoch 5399, testing
[INFO 2017-06-28 13:48:54,066 main.py:105] average testing loss: 10362.83, base loss: 14491.66
[INFO 2017-06-28 13:48:54,066 main.py:106] improve_loss: 4128.83, improve_percent: 0.28
[INFO 2017-06-28 13:48:54,067 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:48:54,730 main.py:51] epoch 5400, training loss: 10436.61, average training loss: 9652.99, base loss: 14414.51
[INFO 2017-06-28 13:48:55,386 main.py:51] epoch 5401, training loss: 10654.22, average training loss: 9654.15, base loss: 14415.54
[INFO 2017-06-28 13:48:56,082 main.py:51] epoch 5402, training loss: 9508.99, average training loss: 9653.63, base loss: 14414.00
[INFO 2017-06-28 13:48:56,741 main.py:51] epoch 5403, training loss: 8890.94, average training loss: 9653.55, base loss: 14412.82
[INFO 2017-06-28 13:48:57,414 main.py:51] epoch 5404, training loss: 9570.14, average training loss: 9651.39, base loss: 14409.97
[INFO 2017-06-28 13:48:58,060 main.py:51] epoch 5405, training loss: 10772.86, average training loss: 9653.16, base loss: 14413.47
[INFO 2017-06-28 13:48:58,694 main.py:51] epoch 5406, training loss: 9385.12, average training loss: 9653.64, base loss: 14412.28
[INFO 2017-06-28 13:48:59,349 main.py:51] epoch 5407, training loss: 8614.25, average training loss: 9651.74, base loss: 14408.60
[INFO 2017-06-28 13:48:59,997 main.py:51] epoch 5408, training loss: 8508.28, average training loss: 9650.21, base loss: 14406.13
[INFO 2017-06-28 13:49:00,690 main.py:51] epoch 5409, training loss: 10748.08, average training loss: 9651.34, base loss: 14409.18
[INFO 2017-06-28 13:49:01,348 main.py:51] epoch 5410, training loss: 8601.73, average training loss: 9649.56, base loss: 14407.27
[INFO 2017-06-28 13:49:02,009 main.py:51] epoch 5411, training loss: 10527.64, average training loss: 9650.97, base loss: 14410.37
[INFO 2017-06-28 13:49:02,655 main.py:51] epoch 5412, training loss: 10613.13, average training loss: 9652.95, base loss: 14415.57
[INFO 2017-06-28 13:49:03,305 main.py:51] epoch 5413, training loss: 9248.10, average training loss: 9651.52, base loss: 14413.02
[INFO 2017-06-28 13:49:03,966 main.py:51] epoch 5414, training loss: 9909.93, average training loss: 9651.68, base loss: 14413.50
[INFO 2017-06-28 13:49:04,596 main.py:51] epoch 5415, training loss: 10227.61, average training loss: 9653.36, base loss: 14413.97
[INFO 2017-06-28 13:49:05,252 main.py:51] epoch 5416, training loss: 10404.83, average training loss: 9655.10, base loss: 14416.90
[INFO 2017-06-28 13:49:05,892 main.py:51] epoch 5417, training loss: 9369.62, average training loss: 9653.56, base loss: 14413.93
[INFO 2017-06-28 13:49:06,542 main.py:51] epoch 5418, training loss: 9384.76, average training loss: 9653.39, base loss: 14412.93
[INFO 2017-06-28 13:49:07,206 main.py:51] epoch 5419, training loss: 10875.27, average training loss: 9653.78, base loss: 14413.61
[INFO 2017-06-28 13:49:07,868 main.py:51] epoch 5420, training loss: 9428.30, average training loss: 9654.39, base loss: 14414.28
[INFO 2017-06-28 13:49:08,521 main.py:51] epoch 5421, training loss: 7998.00, average training loss: 9652.61, base loss: 14410.24
[INFO 2017-06-28 13:49:09,191 main.py:51] epoch 5422, training loss: 7580.69, average training loss: 9651.06, base loss: 14408.35
[INFO 2017-06-28 13:49:09,871 main.py:51] epoch 5423, training loss: 9533.54, average training loss: 9650.46, base loss: 14406.86
[INFO 2017-06-28 13:49:10,531 main.py:51] epoch 5424, training loss: 9540.33, average training loss: 9650.55, base loss: 14407.32
[INFO 2017-06-28 13:49:11,199 main.py:51] epoch 5425, training loss: 9796.16, average training loss: 9651.31, base loss: 14409.20
[INFO 2017-06-28 13:49:11,851 main.py:51] epoch 5426, training loss: 9607.10, average training loss: 9650.98, base loss: 14408.26
[INFO 2017-06-28 13:49:12,504 main.py:51] epoch 5427, training loss: 9063.41, average training loss: 9650.83, base loss: 14405.78
[INFO 2017-06-28 13:49:13,159 main.py:51] epoch 5428, training loss: 9056.11, average training loss: 9649.94, base loss: 14404.34
[INFO 2017-06-28 13:49:13,804 main.py:51] epoch 5429, training loss: 9664.00, average training loss: 9649.34, base loss: 14403.59
[INFO 2017-06-28 13:49:14,444 main.py:51] epoch 5430, training loss: 9584.65, average training loss: 9649.41, base loss: 14401.54
[INFO 2017-06-28 13:49:15,110 main.py:51] epoch 5431, training loss: 11524.59, average training loss: 9651.55, base loss: 14404.09
[INFO 2017-06-28 13:49:15,756 main.py:51] epoch 5432, training loss: 9074.21, average training loss: 9649.48, base loss: 14401.28
[INFO 2017-06-28 13:49:16,414 main.py:51] epoch 5433, training loss: 10325.22, average training loss: 9650.70, base loss: 14403.57
[INFO 2017-06-28 13:49:17,095 main.py:51] epoch 5434, training loss: 9592.90, average training loss: 9650.65, base loss: 14403.47
[INFO 2017-06-28 13:49:17,746 main.py:51] epoch 5435, training loss: 8788.81, average training loss: 9650.41, base loss: 14403.68
[INFO 2017-06-28 13:49:18,409 main.py:51] epoch 5436, training loss: 9177.93, average training loss: 9649.07, base loss: 14400.59
[INFO 2017-06-28 13:49:19,068 main.py:51] epoch 5437, training loss: 9085.00, average training loss: 9648.48, base loss: 14400.27
[INFO 2017-06-28 13:49:19,709 main.py:51] epoch 5438, training loss: 9808.18, average training loss: 9649.68, base loss: 14402.24
[INFO 2017-06-28 13:49:20,366 main.py:51] epoch 5439, training loss: 8610.65, average training loss: 9649.12, base loss: 14401.26
[INFO 2017-06-28 13:49:21,036 main.py:51] epoch 5440, training loss: 8724.70, average training loss: 9648.10, base loss: 14398.88
[INFO 2017-06-28 13:49:21,686 main.py:51] epoch 5441, training loss: 10353.29, average training loss: 9649.38, base loss: 14401.96
[INFO 2017-06-28 13:49:22,332 main.py:51] epoch 5442, training loss: 9694.58, average training loss: 9648.78, base loss: 14401.26
[INFO 2017-06-28 13:49:22,969 main.py:51] epoch 5443, training loss: 11019.80, average training loss: 9650.25, base loss: 14404.07
[INFO 2017-06-28 13:49:23,624 main.py:51] epoch 5444, training loss: 10641.65, average training loss: 9650.43, base loss: 14404.66
[INFO 2017-06-28 13:49:24,294 main.py:51] epoch 5445, training loss: 9821.26, average training loss: 9650.31, base loss: 14405.36
[INFO 2017-06-28 13:49:24,958 main.py:51] epoch 5446, training loss: 11422.38, average training loss: 9651.69, base loss: 14407.88
[INFO 2017-06-28 13:49:25,621 main.py:51] epoch 5447, training loss: 9819.16, average training loss: 9651.84, base loss: 14407.66
[INFO 2017-06-28 13:49:26,279 main.py:51] epoch 5448, training loss: 9680.29, average training loss: 9651.66, base loss: 14408.49
[INFO 2017-06-28 13:49:26,912 main.py:51] epoch 5449, training loss: 8496.85, average training loss: 9650.90, base loss: 14407.95
[INFO 2017-06-28 13:49:27,579 main.py:51] epoch 5450, training loss: 10678.00, average training loss: 9651.37, base loss: 14408.29
[INFO 2017-06-28 13:49:28,233 main.py:51] epoch 5451, training loss: 10054.76, average training loss: 9651.64, base loss: 14408.11
[INFO 2017-06-28 13:49:28,891 main.py:51] epoch 5452, training loss: 10005.74, average training loss: 9652.17, base loss: 14408.34
[INFO 2017-06-28 13:49:29,562 main.py:51] epoch 5453, training loss: 10550.56, average training loss: 9653.58, base loss: 14411.35
[INFO 2017-06-28 13:49:30,224 main.py:51] epoch 5454, training loss: 9346.67, average training loss: 9653.37, base loss: 14410.06
[INFO 2017-06-28 13:49:30,889 main.py:51] epoch 5455, training loss: 8301.76, average training loss: 9651.55, base loss: 14406.98
[INFO 2017-06-28 13:49:31,528 main.py:51] epoch 5456, training loss: 10000.49, average training loss: 9651.34, base loss: 14405.77
[INFO 2017-06-28 13:49:32,186 main.py:51] epoch 5457, training loss: 9894.07, average training loss: 9651.07, base loss: 14404.89
[INFO 2017-06-28 13:49:32,840 main.py:51] epoch 5458, training loss: 10809.42, average training loss: 9651.28, base loss: 14406.40
[INFO 2017-06-28 13:49:33,479 main.py:51] epoch 5459, training loss: 11062.27, average training loss: 9651.88, base loss: 14406.82
[INFO 2017-06-28 13:49:34,125 main.py:51] epoch 5460, training loss: 10942.90, average training loss: 9652.03, base loss: 14408.23
[INFO 2017-06-28 13:49:34,777 main.py:51] epoch 5461, training loss: 10004.03, average training loss: 9653.18, base loss: 14410.58
[INFO 2017-06-28 13:49:35,439 main.py:51] epoch 5462, training loss: 9616.84, average training loss: 9654.18, base loss: 14411.23
[INFO 2017-06-28 13:49:36,093 main.py:51] epoch 5463, training loss: 9888.85, average training loss: 9653.39, base loss: 14410.81
[INFO 2017-06-28 13:49:36,742 main.py:51] epoch 5464, training loss: 8748.80, average training loss: 9653.07, base loss: 14409.96
[INFO 2017-06-28 13:49:37,411 main.py:51] epoch 5465, training loss: 9203.95, average training loss: 9652.81, base loss: 14408.90
[INFO 2017-06-28 13:49:38,088 main.py:51] epoch 5466, training loss: 9782.90, average training loss: 9654.41, base loss: 14411.93
[INFO 2017-06-28 13:49:38,741 main.py:51] epoch 5467, training loss: 10205.92, average training loss: 9654.52, base loss: 14411.62
[INFO 2017-06-28 13:49:39,397 main.py:51] epoch 5468, training loss: 9179.12, average training loss: 9653.78, base loss: 14410.17
[INFO 2017-06-28 13:49:40,079 main.py:51] epoch 5469, training loss: 9415.42, average training loss: 9653.72, base loss: 14408.90
[INFO 2017-06-28 13:49:40,749 main.py:51] epoch 5470, training loss: 9435.38, average training loss: 9652.44, base loss: 14406.12
[INFO 2017-06-28 13:49:41,400 main.py:51] epoch 5471, training loss: 9193.77, average training loss: 9652.40, base loss: 14406.78
[INFO 2017-06-28 13:49:42,046 main.py:51] epoch 5472, training loss: 9121.17, average training loss: 9652.43, base loss: 14407.45
[INFO 2017-06-28 13:49:42,692 main.py:51] epoch 5473, training loss: 8930.46, average training loss: 9652.25, base loss: 14408.23
[INFO 2017-06-28 13:49:43,340 main.py:51] epoch 5474, training loss: 9574.90, average training loss: 9651.33, base loss: 14406.76
[INFO 2017-06-28 13:49:43,989 main.py:51] epoch 5475, training loss: 8928.30, average training loss: 9651.85, base loss: 14406.52
[INFO 2017-06-28 13:49:44,639 main.py:51] epoch 5476, training loss: 9899.74, average training loss: 9651.22, base loss: 14405.02
[INFO 2017-06-28 13:49:45,290 main.py:51] epoch 5477, training loss: 9750.44, average training loss: 9651.17, base loss: 14405.79
[INFO 2017-06-28 13:49:45,968 main.py:51] epoch 5478, training loss: 10803.31, average training loss: 9651.85, base loss: 14406.45
[INFO 2017-06-28 13:49:46,621 main.py:51] epoch 5479, training loss: 9362.69, average training loss: 9652.91, base loss: 14407.22
[INFO 2017-06-28 13:49:47,284 main.py:51] epoch 5480, training loss: 9623.53, average training loss: 9652.64, base loss: 14406.66
[INFO 2017-06-28 13:49:47,946 main.py:51] epoch 5481, training loss: 10067.61, average training loss: 9653.24, base loss: 14407.79
[INFO 2017-06-28 13:49:48,610 main.py:51] epoch 5482, training loss: 9629.63, average training loss: 9652.51, base loss: 14406.34
[INFO 2017-06-28 13:49:49,283 main.py:51] epoch 5483, training loss: 9490.83, average training loss: 9652.23, base loss: 14407.79
[INFO 2017-06-28 13:49:49,944 main.py:51] epoch 5484, training loss: 8033.82, average training loss: 9649.06, base loss: 14401.66
[INFO 2017-06-28 13:49:50,579 main.py:51] epoch 5485, training loss: 8254.93, average training loss: 9648.41, base loss: 14399.97
[INFO 2017-06-28 13:49:51,244 main.py:51] epoch 5486, training loss: 8381.62, average training loss: 9647.39, base loss: 14398.50
[INFO 2017-06-28 13:49:51,906 main.py:51] epoch 5487, training loss: 9955.56, average training loss: 9646.84, base loss: 14398.51
[INFO 2017-06-28 13:49:52,545 main.py:51] epoch 5488, training loss: 9496.31, average training loss: 9647.28, base loss: 14399.00
[INFO 2017-06-28 13:49:53,221 main.py:51] epoch 5489, training loss: 9396.37, average training loss: 9645.96, base loss: 14399.54
[INFO 2017-06-28 13:49:53,869 main.py:51] epoch 5490, training loss: 9241.31, average training loss: 9644.51, base loss: 14397.91
[INFO 2017-06-28 13:49:54,528 main.py:51] epoch 5491, training loss: 9205.51, average training loss: 9644.66, base loss: 14398.25
[INFO 2017-06-28 13:49:55,173 main.py:51] epoch 5492, training loss: 9483.26, average training loss: 9643.86, base loss: 14396.74
[INFO 2017-06-28 13:49:55,814 main.py:51] epoch 5493, training loss: 10239.66, average training loss: 9642.91, base loss: 14395.56
[INFO 2017-06-28 13:49:56,454 main.py:51] epoch 5494, training loss: 9323.21, average training loss: 9642.98, base loss: 14396.19
[INFO 2017-06-28 13:49:57,099 main.py:51] epoch 5495, training loss: 12056.36, average training loss: 9644.64, base loss: 14397.76
[INFO 2017-06-28 13:49:57,767 main.py:51] epoch 5496, training loss: 10251.16, average training loss: 9645.09, base loss: 14397.37
[INFO 2017-06-28 13:49:58,421 main.py:51] epoch 5497, training loss: 9340.43, average training loss: 9645.16, base loss: 14397.61
[INFO 2017-06-28 13:49:59,061 main.py:51] epoch 5498, training loss: 8267.44, average training loss: 9642.43, base loss: 14394.32
[INFO 2017-06-28 13:49:59,736 main.py:51] epoch 5499, training loss: 9627.87, average training loss: 9642.88, base loss: 14395.86
[INFO 2017-06-28 13:49:59,736 main.py:53] epoch 5499, testing
[INFO 2017-06-28 13:50:02,270 main.py:105] average testing loss: 10971.59, base loss: 15522.98
[INFO 2017-06-28 13:50:02,270 main.py:106] improve_loss: 4551.40, improve_percent: 0.29
[INFO 2017-06-28 13:50:02,270 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:50:02,909 main.py:51] epoch 5500, training loss: 10176.98, average training loss: 9643.92, base loss: 14397.17
[INFO 2017-06-28 13:50:03,590 main.py:51] epoch 5501, training loss: 8806.02, average training loss: 9643.69, base loss: 14397.49
[INFO 2017-06-28 13:50:04,253 main.py:51] epoch 5502, training loss: 8935.35, average training loss: 9643.79, base loss: 14398.52
[INFO 2017-06-28 13:50:04,891 main.py:51] epoch 5503, training loss: 10264.16, average training loss: 9645.06, base loss: 14399.59
[INFO 2017-06-28 13:50:05,540 main.py:51] epoch 5504, training loss: 8482.39, average training loss: 9644.79, base loss: 14399.13
[INFO 2017-06-28 13:50:06,189 main.py:51] epoch 5505, training loss: 9781.08, average training loss: 9645.97, base loss: 14400.87
[INFO 2017-06-28 13:50:06,845 main.py:51] epoch 5506, training loss: 9041.67, average training loss: 9644.77, base loss: 14399.08
[INFO 2017-06-28 13:50:07,503 main.py:51] epoch 5507, training loss: 11282.03, average training loss: 9645.34, base loss: 14400.33
[INFO 2017-06-28 13:50:08,153 main.py:51] epoch 5508, training loss: 9089.98, average training loss: 9644.45, base loss: 14398.54
[INFO 2017-06-28 13:50:08,804 main.py:51] epoch 5509, training loss: 8775.37, average training loss: 9643.65, base loss: 14396.87
[INFO 2017-06-28 13:50:09,457 main.py:51] epoch 5510, training loss: 10557.10, average training loss: 9643.33, base loss: 14398.62
[INFO 2017-06-28 13:50:10,110 main.py:51] epoch 5511, training loss: 9954.97, average training loss: 9644.04, base loss: 14401.25
[INFO 2017-06-28 13:50:10,768 main.py:51] epoch 5512, training loss: 9188.19, average training loss: 9641.28, base loss: 14397.65
[INFO 2017-06-28 13:50:11,423 main.py:51] epoch 5513, training loss: 10292.87, average training loss: 9639.04, base loss: 14393.63
[INFO 2017-06-28 13:50:12,081 main.py:51] epoch 5514, training loss: 7817.97, average training loss: 9637.35, base loss: 14391.28
[INFO 2017-06-28 13:50:12,730 main.py:51] epoch 5515, training loss: 9606.72, average training loss: 9637.27, base loss: 14391.71
[INFO 2017-06-28 13:50:13,365 main.py:51] epoch 5516, training loss: 8842.63, average training loss: 9636.50, base loss: 14388.53
[INFO 2017-06-28 13:50:14,012 main.py:51] epoch 5517, training loss: 9297.87, average training loss: 9637.43, base loss: 14388.44
[INFO 2017-06-28 13:50:14,671 main.py:51] epoch 5518, training loss: 9461.67, average training loss: 9635.88, base loss: 14385.95
[INFO 2017-06-28 13:50:15,325 main.py:51] epoch 5519, training loss: 10149.15, average training loss: 9637.07, base loss: 14387.94
[INFO 2017-06-28 13:50:15,980 main.py:51] epoch 5520, training loss: 9573.29, average training loss: 9636.41, base loss: 14387.61
[INFO 2017-06-28 13:50:16,653 main.py:51] epoch 5521, training loss: 9979.40, average training loss: 9635.90, base loss: 14386.26
[INFO 2017-06-28 13:50:17,303 main.py:51] epoch 5522, training loss: 10258.10, average training loss: 9635.92, base loss: 14386.08
[INFO 2017-06-28 13:50:17,963 main.py:51] epoch 5523, training loss: 10014.27, average training loss: 9636.99, base loss: 14388.54
[INFO 2017-06-28 13:50:18,629 main.py:51] epoch 5524, training loss: 9600.24, average training loss: 9637.47, base loss: 14388.14
[INFO 2017-06-28 13:50:19,303 main.py:51] epoch 5525, training loss: 9961.47, average training loss: 9637.77, base loss: 14388.84
[INFO 2017-06-28 13:50:19,942 main.py:51] epoch 5526, training loss: 8421.37, average training loss: 9635.48, base loss: 14386.00
[INFO 2017-06-28 13:50:20,600 main.py:51] epoch 5527, training loss: 9252.87, average training loss: 9632.62, base loss: 14384.83
[INFO 2017-06-28 13:50:21,255 main.py:51] epoch 5528, training loss: 8329.09, average training loss: 9630.84, base loss: 14382.96
[INFO 2017-06-28 13:50:21,929 main.py:51] epoch 5529, training loss: 9758.58, average training loss: 9630.93, base loss: 14383.40
[INFO 2017-06-28 13:50:22,592 main.py:51] epoch 5530, training loss: 7531.66, average training loss: 9629.57, base loss: 14380.80
[INFO 2017-06-28 13:50:23,237 main.py:51] epoch 5531, training loss: 9879.95, average training loss: 9629.46, base loss: 14379.65
[INFO 2017-06-28 13:50:23,901 main.py:51] epoch 5532, training loss: 11771.06, average training loss: 9631.75, base loss: 14383.68
[INFO 2017-06-28 13:50:24,558 main.py:51] epoch 5533, training loss: 8947.26, average training loss: 9630.26, base loss: 14381.47
[INFO 2017-06-28 13:50:25,225 main.py:51] epoch 5534, training loss: 9618.17, average training loss: 9630.35, base loss: 14381.99
[INFO 2017-06-28 13:50:25,898 main.py:51] epoch 5535, training loss: 9965.63, average training loss: 9630.84, base loss: 14382.72
[INFO 2017-06-28 13:50:26,553 main.py:51] epoch 5536, training loss: 11129.63, average training loss: 9632.50, base loss: 14381.97
[INFO 2017-06-28 13:50:27,218 main.py:51] epoch 5537, training loss: 9791.17, average training loss: 9632.40, base loss: 14381.33
[INFO 2017-06-28 13:50:27,874 main.py:51] epoch 5538, training loss: 9715.24, average training loss: 9631.19, base loss: 14379.37
[INFO 2017-06-28 13:50:28,530 main.py:51] epoch 5539, training loss: 10088.54, average training loss: 9631.93, base loss: 14380.20
[INFO 2017-06-28 13:50:29,194 main.py:51] epoch 5540, training loss: 9184.55, average training loss: 9632.37, base loss: 14379.92
[INFO 2017-06-28 13:50:29,867 main.py:51] epoch 5541, training loss: 10230.76, average training loss: 9632.70, base loss: 14380.21
[INFO 2017-06-28 13:50:30,499 main.py:51] epoch 5542, training loss: 8499.59, average training loss: 9631.23, base loss: 14377.74
[INFO 2017-06-28 13:50:31,154 main.py:51] epoch 5543, training loss: 9153.88, average training loss: 9631.31, base loss: 14378.19
[INFO 2017-06-28 13:50:31,810 main.py:51] epoch 5544, training loss: 10167.00, average training loss: 9631.57, base loss: 14380.14
[INFO 2017-06-28 13:50:32,453 main.py:51] epoch 5545, training loss: 11445.44, average training loss: 9634.12, base loss: 14385.01
[INFO 2017-06-28 13:50:33,094 main.py:51] epoch 5546, training loss: 8612.01, average training loss: 9631.91, base loss: 14382.76
[INFO 2017-06-28 13:50:33,747 main.py:51] epoch 5547, training loss: 10301.05, average training loss: 9634.22, base loss: 14385.86
[INFO 2017-06-28 13:50:34,390 main.py:51] epoch 5548, training loss: 8358.30, average training loss: 9631.31, base loss: 14381.36
[INFO 2017-06-28 13:50:35,048 main.py:51] epoch 5549, training loss: 8901.68, average training loss: 9629.69, base loss: 14378.44
[INFO 2017-06-28 13:50:35,721 main.py:51] epoch 5550, training loss: 9242.06, average training loss: 9629.65, base loss: 14378.69
[INFO 2017-06-28 13:50:36,369 main.py:51] epoch 5551, training loss: 10119.39, average training loss: 9629.97, base loss: 14379.24
[INFO 2017-06-28 13:50:37,012 main.py:51] epoch 5552, training loss: 11061.91, average training loss: 9628.11, base loss: 14375.74
[INFO 2017-06-28 13:50:37,680 main.py:51] epoch 5553, training loss: 8067.89, average training loss: 9627.93, base loss: 14375.77
[INFO 2017-06-28 13:50:38,335 main.py:51] epoch 5554, training loss: 9971.34, average training loss: 9627.66, base loss: 14376.25
[INFO 2017-06-28 13:50:38,995 main.py:51] epoch 5555, training loss: 9327.58, average training loss: 9627.21, base loss: 14376.23
[INFO 2017-06-28 13:50:39,660 main.py:51] epoch 5556, training loss: 9546.61, average training loss: 9626.57, base loss: 14375.71
[INFO 2017-06-28 13:50:40,331 main.py:51] epoch 5557, training loss: 9491.58, average training loss: 9626.16, base loss: 14374.98
[INFO 2017-06-28 13:50:40,991 main.py:51] epoch 5558, training loss: 9415.24, average training loss: 9625.64, base loss: 14374.46
[INFO 2017-06-28 13:50:41,672 main.py:51] epoch 5559, training loss: 10843.52, average training loss: 9626.03, base loss: 14375.19
[INFO 2017-06-28 13:50:42,337 main.py:51] epoch 5560, training loss: 8250.71, average training loss: 9622.63, base loss: 14371.14
[INFO 2017-06-28 13:50:42,994 main.py:51] epoch 5561, training loss: 9411.25, average training loss: 9620.53, base loss: 14365.98
[INFO 2017-06-28 13:50:43,674 main.py:51] epoch 5562, training loss: 9770.11, average training loss: 9620.96, base loss: 14367.49
[INFO 2017-06-28 13:50:44,340 main.py:51] epoch 5563, training loss: 9490.51, average training loss: 9621.01, base loss: 14367.61
[INFO 2017-06-28 13:50:44,993 main.py:51] epoch 5564, training loss: 9148.32, average training loss: 9619.45, base loss: 14365.17
[INFO 2017-06-28 13:50:45,642 main.py:51] epoch 5565, training loss: 10053.94, average training loss: 9618.81, base loss: 14360.98
[INFO 2017-06-28 13:50:46,291 main.py:51] epoch 5566, training loss: 9667.64, average training loss: 9617.36, base loss: 14359.61
[INFO 2017-06-28 13:50:46,960 main.py:51] epoch 5567, training loss: 9276.70, average training loss: 9616.34, base loss: 14357.80
[INFO 2017-06-28 13:50:47,621 main.py:51] epoch 5568, training loss: 9258.82, average training loss: 9615.63, base loss: 14357.39
[INFO 2017-06-28 13:50:48,276 main.py:51] epoch 5569, training loss: 8983.72, average training loss: 9613.77, base loss: 14353.99
[INFO 2017-06-28 13:50:48,924 main.py:51] epoch 5570, training loss: 9479.62, average training loss: 9613.60, base loss: 14353.94
[INFO 2017-06-28 13:50:49,568 main.py:51] epoch 5571, training loss: 9968.08, average training loss: 9614.07, base loss: 14354.55
[INFO 2017-06-28 13:50:50,249 main.py:51] epoch 5572, training loss: 10174.29, average training loss: 9614.57, base loss: 14354.84
[INFO 2017-06-28 13:50:50,873 main.py:51] epoch 5573, training loss: 8726.33, average training loss: 9612.73, base loss: 14353.25
[INFO 2017-06-28 13:50:51,534 main.py:51] epoch 5574, training loss: 9318.69, average training loss: 9612.08, base loss: 14352.26
[INFO 2017-06-28 13:50:52,197 main.py:51] epoch 5575, training loss: 9886.22, average training loss: 9611.88, base loss: 14352.22
[INFO 2017-06-28 13:50:52,840 main.py:51] epoch 5576, training loss: 9749.80, average training loss: 9612.20, base loss: 14352.40
[INFO 2017-06-28 13:50:53,513 main.py:51] epoch 5577, training loss: 9212.23, average training loss: 9611.23, base loss: 14350.53
[INFO 2017-06-28 13:50:54,188 main.py:51] epoch 5578, training loss: 9061.66, average training loss: 9610.70, base loss: 14350.20
[INFO 2017-06-28 13:50:54,838 main.py:51] epoch 5579, training loss: 9042.76, average training loss: 9609.82, base loss: 14348.87
[INFO 2017-06-28 13:50:55,490 main.py:51] epoch 5580, training loss: 10246.78, average training loss: 9610.51, base loss: 14349.48
[INFO 2017-06-28 13:50:56,138 main.py:51] epoch 5581, training loss: 9141.73, average training loss: 9610.85, base loss: 14350.37
[INFO 2017-06-28 13:50:56,803 main.py:51] epoch 5582, training loss: 11059.36, average training loss: 9612.44, base loss: 14352.51
[INFO 2017-06-28 13:50:57,486 main.py:51] epoch 5583, training loss: 9673.03, average training loss: 9613.11, base loss: 14354.47
[INFO 2017-06-28 13:50:58,176 main.py:51] epoch 5584, training loss: 8145.75, average training loss: 9613.07, base loss: 14354.34
[INFO 2017-06-28 13:50:58,825 main.py:51] epoch 5585, training loss: 8845.25, average training loss: 9611.50, base loss: 14353.97
[INFO 2017-06-28 13:50:59,460 main.py:51] epoch 5586, training loss: 8260.89, average training loss: 9610.63, base loss: 14351.79
[INFO 2017-06-28 13:51:00,108 main.py:51] epoch 5587, training loss: 9488.96, average training loss: 9610.87, base loss: 14352.36
[INFO 2017-06-28 13:51:00,770 main.py:51] epoch 5588, training loss: 10878.87, average training loss: 9613.26, base loss: 14357.68
[INFO 2017-06-28 13:51:01,446 main.py:51] epoch 5589, training loss: 9560.13, average training loss: 9613.61, base loss: 14357.96
[INFO 2017-06-28 13:51:02,102 main.py:51] epoch 5590, training loss: 8504.56, average training loss: 9613.73, base loss: 14359.40
[INFO 2017-06-28 13:51:02,761 main.py:51] epoch 5591, training loss: 9283.32, average training loss: 9614.20, base loss: 14359.44
[INFO 2017-06-28 13:51:03,410 main.py:51] epoch 5592, training loss: 10287.59, average training loss: 9616.53, base loss: 14364.76
[INFO 2017-06-28 13:51:04,053 main.py:51] epoch 5593, training loss: 8961.97, average training loss: 9615.43, base loss: 14364.24
[INFO 2017-06-28 13:51:04,735 main.py:51] epoch 5594, training loss: 9766.98, average training loss: 9615.57, base loss: 14364.00
[INFO 2017-06-28 13:51:05,440 main.py:51] epoch 5595, training loss: 10638.37, average training loss: 9616.39, base loss: 14364.18
[INFO 2017-06-28 13:51:06,100 main.py:51] epoch 5596, training loss: 10519.70, average training loss: 9617.59, base loss: 14365.06
[INFO 2017-06-28 13:51:06,746 main.py:51] epoch 5597, training loss: 11295.89, average training loss: 9620.50, base loss: 14369.58
[INFO 2017-06-28 13:51:07,405 main.py:51] epoch 5598, training loss: 11560.15, average training loss: 9622.08, base loss: 14371.60
[INFO 2017-06-28 13:51:08,059 main.py:51] epoch 5599, training loss: 8680.30, average training loss: 9621.88, base loss: 14372.14
[INFO 2017-06-28 13:51:08,059 main.py:53] epoch 5599, testing
[INFO 2017-06-28 13:51:10,665 main.py:105] average testing loss: 10731.39, base loss: 15032.46
[INFO 2017-06-28 13:51:10,665 main.py:106] improve_loss: 4301.07, improve_percent: 0.29
[INFO 2017-06-28 13:51:10,666 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:51:11,331 main.py:51] epoch 5600, training loss: 10673.50, average training loss: 9623.93, base loss: 14376.67
[INFO 2017-06-28 13:51:11,986 main.py:51] epoch 5601, training loss: 9576.74, average training loss: 9625.16, base loss: 14378.95
[INFO 2017-06-28 13:51:12,639 main.py:51] epoch 5602, training loss: 9952.42, average training loss: 9624.89, base loss: 14380.52
[INFO 2017-06-28 13:51:13,289 main.py:51] epoch 5603, training loss: 9611.29, average training loss: 9624.67, base loss: 14380.36
[INFO 2017-06-28 13:51:13,943 main.py:51] epoch 5604, training loss: 8408.81, average training loss: 9622.77, base loss: 14379.26
[INFO 2017-06-28 13:51:14,610 main.py:51] epoch 5605, training loss: 10219.01, average training loss: 9621.74, base loss: 14378.18
[INFO 2017-06-28 13:51:15,272 main.py:51] epoch 5606, training loss: 10030.09, average training loss: 9620.93, base loss: 14377.52
[INFO 2017-06-28 13:51:15,938 main.py:51] epoch 5607, training loss: 8802.20, average training loss: 9619.63, base loss: 14376.91
[INFO 2017-06-28 13:51:16,568 main.py:51] epoch 5608, training loss: 8620.70, average training loss: 9619.33, base loss: 14378.61
[INFO 2017-06-28 13:51:17,216 main.py:51] epoch 5609, training loss: 9998.15, average training loss: 9619.16, base loss: 14377.67
[INFO 2017-06-28 13:51:17,845 main.py:51] epoch 5610, training loss: 9463.49, average training loss: 9618.89, base loss: 14376.49
[INFO 2017-06-28 13:51:18,497 main.py:51] epoch 5611, training loss: 9639.71, average training loss: 9619.18, base loss: 14377.53
[INFO 2017-06-28 13:51:19,162 main.py:51] epoch 5612, training loss: 9791.57, average training loss: 9618.76, base loss: 14378.81
[INFO 2017-06-28 13:51:19,818 main.py:51] epoch 5613, training loss: 9555.28, average training loss: 9619.36, base loss: 14380.53
[INFO 2017-06-28 13:51:20,472 main.py:51] epoch 5614, training loss: 9667.10, average training loss: 9618.58, base loss: 14378.82
[INFO 2017-06-28 13:51:21,153 main.py:51] epoch 5615, training loss: 9625.86, average training loss: 9616.29, base loss: 14375.88
[INFO 2017-06-28 13:51:21,797 main.py:51] epoch 5616, training loss: 10755.22, average training loss: 9617.80, base loss: 14377.52
[INFO 2017-06-28 13:51:22,444 main.py:51] epoch 5617, training loss: 8137.44, average training loss: 9616.76, base loss: 14375.44
[INFO 2017-06-28 13:51:23,094 main.py:51] epoch 5618, training loss: 10606.55, average training loss: 9618.81, base loss: 14378.69
[INFO 2017-06-28 13:51:23,743 main.py:51] epoch 5619, training loss: 9506.60, average training loss: 9619.84, base loss: 14378.76
[INFO 2017-06-28 13:51:24,409 main.py:51] epoch 5620, training loss: 10836.58, average training loss: 9621.10, base loss: 14381.54
[INFO 2017-06-28 13:51:25,067 main.py:51] epoch 5621, training loss: 10244.35, average training loss: 9620.21, base loss: 14380.51
[INFO 2017-06-28 13:51:25,699 main.py:51] epoch 5622, training loss: 9058.72, average training loss: 9619.75, base loss: 14377.93
[INFO 2017-06-28 13:51:26,338 main.py:51] epoch 5623, training loss: 10065.70, average training loss: 9619.40, base loss: 14375.56
[INFO 2017-06-28 13:51:26,985 main.py:51] epoch 5624, training loss: 9899.83, average training loss: 9618.97, base loss: 14374.98
[INFO 2017-06-28 13:51:27,619 main.py:51] epoch 5625, training loss: 9907.48, average training loss: 9618.61, base loss: 14374.35
[INFO 2017-06-28 13:51:28,271 main.py:51] epoch 5626, training loss: 10009.00, average training loss: 9620.16, base loss: 14378.10
[INFO 2017-06-28 13:51:28,940 main.py:51] epoch 5627, training loss: 10309.16, average training loss: 9619.91, base loss: 14377.07
[INFO 2017-06-28 13:51:29,612 main.py:51] epoch 5628, training loss: 11419.40, average training loss: 9622.24, base loss: 14380.57
[INFO 2017-06-28 13:51:30,262 main.py:51] epoch 5629, training loss: 8632.89, average training loss: 9621.13, base loss: 14378.89
[INFO 2017-06-28 13:51:30,921 main.py:51] epoch 5630, training loss: 10548.61, average training loss: 9621.91, base loss: 14379.00
[INFO 2017-06-28 13:51:31,587 main.py:51] epoch 5631, training loss: 9713.62, average training loss: 9621.97, base loss: 14377.83
[INFO 2017-06-28 13:51:32,241 main.py:51] epoch 5632, training loss: 9387.96, average training loss: 9623.93, base loss: 14381.12
[INFO 2017-06-28 13:51:32,917 main.py:51] epoch 5633, training loss: 8986.94, average training loss: 9622.95, base loss: 14378.99
[INFO 2017-06-28 13:51:33,559 main.py:51] epoch 5634, training loss: 10601.83, average training loss: 9624.28, base loss: 14380.79
[INFO 2017-06-28 13:51:34,240 main.py:51] epoch 5635, training loss: 9801.65, average training loss: 9624.36, base loss: 14381.34
[INFO 2017-06-28 13:51:34,902 main.py:51] epoch 5636, training loss: 8984.93, average training loss: 9623.16, base loss: 14377.32
[INFO 2017-06-28 13:51:35,571 main.py:51] epoch 5637, training loss: 11218.61, average training loss: 9625.18, base loss: 14380.27
[INFO 2017-06-28 13:51:36,231 main.py:51] epoch 5638, training loss: 9931.64, average training loss: 9626.29, base loss: 14382.63
[INFO 2017-06-28 13:51:36,907 main.py:51] epoch 5639, training loss: 10092.24, average training loss: 9627.16, base loss: 14383.13
[INFO 2017-06-28 13:51:37,590 main.py:51] epoch 5640, training loss: 9828.48, average training loss: 9628.45, base loss: 14385.37
[INFO 2017-06-28 13:51:38,240 main.py:51] epoch 5641, training loss: 8941.90, average training loss: 9628.09, base loss: 14385.71
[INFO 2017-06-28 13:51:38,881 main.py:51] epoch 5642, training loss: 9429.31, average training loss: 9627.64, base loss: 14385.78
[INFO 2017-06-28 13:51:39,556 main.py:51] epoch 5643, training loss: 10225.05, average training loss: 9628.73, base loss: 14389.08
[INFO 2017-06-28 13:51:40,222 main.py:51] epoch 5644, training loss: 8897.03, average training loss: 9628.31, base loss: 14389.39
[INFO 2017-06-28 13:51:40,878 main.py:51] epoch 5645, training loss: 11069.92, average training loss: 9629.65, base loss: 14392.58
[INFO 2017-06-28 13:51:41,521 main.py:51] epoch 5646, training loss: 9909.34, average training loss: 9629.53, base loss: 14393.09
[INFO 2017-06-28 13:51:42,176 main.py:51] epoch 5647, training loss: 8656.33, average training loss: 9626.17, base loss: 14387.23
[INFO 2017-06-28 13:51:42,840 main.py:51] epoch 5648, training loss: 9569.26, average training loss: 9626.12, base loss: 14386.39
[INFO 2017-06-28 13:51:43,481 main.py:51] epoch 5649, training loss: 8859.94, average training loss: 9625.70, base loss: 14384.53
[INFO 2017-06-28 13:51:44,146 main.py:51] epoch 5650, training loss: 8566.12, average training loss: 9625.26, base loss: 14384.30
[INFO 2017-06-28 13:51:44,814 main.py:51] epoch 5651, training loss: 10845.48, average training loss: 9626.96, base loss: 14384.91
[INFO 2017-06-28 13:51:45,481 main.py:51] epoch 5652, training loss: 9251.78, average training loss: 9626.46, base loss: 14383.25
[INFO 2017-06-28 13:51:46,144 main.py:51] epoch 5653, training loss: 9373.05, average training loss: 9624.83, base loss: 14380.03
[INFO 2017-06-28 13:51:46,793 main.py:51] epoch 5654, training loss: 8150.71, average training loss: 9623.91, base loss: 14378.13
[INFO 2017-06-28 13:51:47,458 main.py:51] epoch 5655, training loss: 10715.61, average training loss: 9625.17, base loss: 14380.15
[INFO 2017-06-28 13:51:48,106 main.py:51] epoch 5656, training loss: 9587.97, average training loss: 9626.14, base loss: 14381.02
[INFO 2017-06-28 13:51:48,759 main.py:51] epoch 5657, training loss: 10383.32, average training loss: 9626.38, base loss: 14382.47
[INFO 2017-06-28 13:51:49,412 main.py:51] epoch 5658, training loss: 8647.66, average training loss: 9624.70, base loss: 14379.69
[INFO 2017-06-28 13:51:50,043 main.py:51] epoch 5659, training loss: 8505.96, average training loss: 9622.52, base loss: 14377.49
[INFO 2017-06-28 13:51:50,696 main.py:51] epoch 5660, training loss: 9163.95, average training loss: 9620.30, base loss: 14373.92
[INFO 2017-06-28 13:51:51,341 main.py:51] epoch 5661, training loss: 9153.13, average training loss: 9620.89, base loss: 14373.89
[INFO 2017-06-28 13:51:51,992 main.py:51] epoch 5662, training loss: 10343.43, average training loss: 9620.67, base loss: 14373.57
[INFO 2017-06-28 13:51:52,656 main.py:51] epoch 5663, training loss: 9137.74, average training loss: 9619.88, base loss: 14373.56
[INFO 2017-06-28 13:51:53,313 main.py:51] epoch 5664, training loss: 8801.15, average training loss: 9619.55, base loss: 14373.94
[INFO 2017-06-28 13:51:53,978 main.py:51] epoch 5665, training loss: 9164.00, average training loss: 9617.65, base loss: 14371.01
[INFO 2017-06-28 13:51:54,630 main.py:51] epoch 5666, training loss: 8997.84, average training loss: 9614.60, base loss: 14366.03
[INFO 2017-06-28 13:51:55,293 main.py:51] epoch 5667, training loss: 10242.57, average training loss: 9616.43, base loss: 14368.59
[INFO 2017-06-28 13:51:55,952 main.py:51] epoch 5668, training loss: 10188.78, average training loss: 9618.03, base loss: 14371.39
[INFO 2017-06-28 13:51:56,588 main.py:51] epoch 5669, training loss: 10668.09, average training loss: 9618.46, base loss: 14371.58
[INFO 2017-06-28 13:51:57,230 main.py:51] epoch 5670, training loss: 8890.96, average training loss: 9619.15, base loss: 14373.27
[INFO 2017-06-28 13:51:57,887 main.py:51] epoch 5671, training loss: 8590.38, average training loss: 9619.95, base loss: 14373.26
[INFO 2017-06-28 13:51:58,536 main.py:51] epoch 5672, training loss: 9629.48, average training loss: 9620.16, base loss: 14372.34
[INFO 2017-06-28 13:51:59,200 main.py:51] epoch 5673, training loss: 10443.58, average training loss: 9621.52, base loss: 14375.08
[INFO 2017-06-28 13:51:59,858 main.py:51] epoch 5674, training loss: 10030.88, average training loss: 9621.51, base loss: 14375.72
[INFO 2017-06-28 13:52:00,511 main.py:51] epoch 5675, training loss: 10643.96, average training loss: 9623.34, base loss: 14380.96
[INFO 2017-06-28 13:52:01,167 main.py:51] epoch 5676, training loss: 11182.91, average training loss: 9624.46, base loss: 14382.48
[INFO 2017-06-28 13:52:01,818 main.py:51] epoch 5677, training loss: 9085.47, average training loss: 9620.88, base loss: 14377.51
[INFO 2017-06-28 13:52:02,461 main.py:51] epoch 5678, training loss: 9009.18, average training loss: 9618.95, base loss: 14375.29
[INFO 2017-06-28 13:52:03,116 main.py:51] epoch 5679, training loss: 9639.58, average training loss: 9619.30, base loss: 14375.83
[INFO 2017-06-28 13:52:03,768 main.py:51] epoch 5680, training loss: 10135.82, average training loss: 9620.61, base loss: 14376.93
[INFO 2017-06-28 13:52:04,412 main.py:51] epoch 5681, training loss: 10221.96, average training loss: 9621.47, base loss: 14376.57
[INFO 2017-06-28 13:52:05,061 main.py:51] epoch 5682, training loss: 9844.35, average training loss: 9622.40, base loss: 14376.01
[INFO 2017-06-28 13:52:05,725 main.py:51] epoch 5683, training loss: 9335.00, average training loss: 9623.15, base loss: 14376.25
[INFO 2017-06-28 13:52:06,391 main.py:51] epoch 5684, training loss: 8995.63, average training loss: 9623.00, base loss: 14375.16
[INFO 2017-06-28 13:52:07,062 main.py:51] epoch 5685, training loss: 8777.54, average training loss: 9621.82, base loss: 14374.49
[INFO 2017-06-28 13:52:07,710 main.py:51] epoch 5686, training loss: 10737.91, average training loss: 9623.28, base loss: 14378.11
[INFO 2017-06-28 13:52:08,356 main.py:51] epoch 5687, training loss: 10647.17, average training loss: 9624.34, base loss: 14379.35
[INFO 2017-06-28 13:52:09,008 main.py:51] epoch 5688, training loss: 9992.12, average training loss: 9624.41, base loss: 14377.77
[INFO 2017-06-28 13:52:09,657 main.py:51] epoch 5689, training loss: 10094.25, average training loss: 9625.57, base loss: 14378.21
[INFO 2017-06-28 13:52:10,300 main.py:51] epoch 5690, training loss: 10266.85, average training loss: 9627.89, base loss: 14382.61
[INFO 2017-06-28 13:52:10,982 main.py:51] epoch 5691, training loss: 9356.26, average training loss: 9629.17, base loss: 14384.83
[INFO 2017-06-28 13:52:11,640 main.py:51] epoch 5692, training loss: 9504.75, average training loss: 9628.73, base loss: 14382.88
[INFO 2017-06-28 13:52:12,303 main.py:51] epoch 5693, training loss: 8862.40, average training loss: 9628.28, base loss: 14383.48
[INFO 2017-06-28 13:52:12,954 main.py:51] epoch 5694, training loss: 11956.95, average training loss: 9631.97, base loss: 14389.69
[INFO 2017-06-28 13:52:13,607 main.py:51] epoch 5695, training loss: 10895.58, average training loss: 9633.44, base loss: 14391.80
[INFO 2017-06-28 13:52:14,278 main.py:51] epoch 5696, training loss: 9860.74, average training loss: 9633.91, base loss: 14393.11
[INFO 2017-06-28 13:52:14,930 main.py:51] epoch 5697, training loss: 9718.06, average training loss: 9633.77, base loss: 14392.91
[INFO 2017-06-28 13:52:15,576 main.py:51] epoch 5698, training loss: 10180.49, average training loss: 9635.50, base loss: 14395.82
[INFO 2017-06-28 13:52:16,243 main.py:51] epoch 5699, training loss: 9652.74, average training loss: 9634.38, base loss: 14393.24
[INFO 2017-06-28 13:52:16,243 main.py:53] epoch 5699, testing
[INFO 2017-06-28 13:52:18,851 main.py:105] average testing loss: 10266.43, base loss: 14614.08
[INFO 2017-06-28 13:52:18,851 main.py:106] improve_loss: 4347.65, improve_percent: 0.30
[INFO 2017-06-28 13:52:18,852 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:52:19,525 main.py:51] epoch 5700, training loss: 9175.58, average training loss: 9632.97, base loss: 14391.67
[INFO 2017-06-28 13:52:20,170 main.py:51] epoch 5701, training loss: 10950.37, average training loss: 9633.86, base loss: 14393.60
[INFO 2017-06-28 13:52:20,834 main.py:51] epoch 5702, training loss: 9976.38, average training loss: 9634.29, base loss: 14393.90
[INFO 2017-06-28 13:52:21,482 main.py:51] epoch 5703, training loss: 10435.31, average training loss: 9635.75, base loss: 14394.51
[INFO 2017-06-28 13:52:22,137 main.py:51] epoch 5704, training loss: 10057.01, average training loss: 9635.53, base loss: 14393.35
[INFO 2017-06-28 13:52:22,822 main.py:51] epoch 5705, training loss: 10087.23, average training loss: 9635.25, base loss: 14393.37
[INFO 2017-06-28 13:52:23,467 main.py:51] epoch 5706, training loss: 9821.47, average training loss: 9634.74, base loss: 14392.90
[INFO 2017-06-28 13:52:24,122 main.py:51] epoch 5707, training loss: 10039.09, average training loss: 9635.88, base loss: 14393.94
[INFO 2017-06-28 13:52:24,804 main.py:51] epoch 5708, training loss: 8926.27, average training loss: 9636.27, base loss: 14396.30
[INFO 2017-06-28 13:52:25,462 main.py:51] epoch 5709, training loss: 10745.50, average training loss: 9637.85, base loss: 14397.64
[INFO 2017-06-28 13:52:26,116 main.py:51] epoch 5710, training loss: 10191.05, average training loss: 9638.50, base loss: 14399.46
[INFO 2017-06-28 13:52:26,766 main.py:51] epoch 5711, training loss: 10886.18, average training loss: 9640.90, base loss: 14404.13
[INFO 2017-06-28 13:52:27,437 main.py:51] epoch 5712, training loss: 9024.69, average training loss: 9640.34, base loss: 14402.50
[INFO 2017-06-28 13:52:28,091 main.py:51] epoch 5713, training loss: 9913.50, average training loss: 9640.62, base loss: 14403.13
[INFO 2017-06-28 13:52:28,750 main.py:51] epoch 5714, training loss: 8727.37, average training loss: 9638.72, base loss: 14398.43
[INFO 2017-06-28 13:52:29,408 main.py:51] epoch 5715, training loss: 10117.85, average training loss: 9640.04, base loss: 14401.80
[INFO 2017-06-28 13:52:30,063 main.py:51] epoch 5716, training loss: 9343.75, average training loss: 9639.71, base loss: 14401.25
[INFO 2017-06-28 13:52:30,706 main.py:51] epoch 5717, training loss: 9248.66, average training loss: 9639.26, base loss: 14400.66
[INFO 2017-06-28 13:52:31,367 main.py:51] epoch 5718, training loss: 10051.68, average training loss: 9640.78, base loss: 14402.77
[INFO 2017-06-28 13:52:32,029 main.py:51] epoch 5719, training loss: 8969.37, average training loss: 9640.68, base loss: 14401.62
[INFO 2017-06-28 13:52:32,685 main.py:51] epoch 5720, training loss: 10797.71, average training loss: 9642.01, base loss: 14405.06
[INFO 2017-06-28 13:52:33,332 main.py:51] epoch 5721, training loss: 8926.53, average training loss: 9641.89, base loss: 14403.71
[INFO 2017-06-28 13:52:33,973 main.py:51] epoch 5722, training loss: 9344.47, average training loss: 9642.10, base loss: 14405.22
[INFO 2017-06-28 13:52:34,605 main.py:51] epoch 5723, training loss: 10845.76, average training loss: 9641.68, base loss: 14405.05
[INFO 2017-06-28 13:52:35,238 main.py:51] epoch 5724, training loss: 10090.50, average training loss: 9641.94, base loss: 14406.99
[INFO 2017-06-28 13:52:35,892 main.py:51] epoch 5725, training loss: 9859.92, average training loss: 9642.27, base loss: 14407.33
[INFO 2017-06-28 13:52:36,558 main.py:51] epoch 5726, training loss: 9420.87, average training loss: 9642.00, base loss: 14407.18
[INFO 2017-06-28 13:52:37,235 main.py:51] epoch 5727, training loss: 10303.55, average training loss: 9642.08, base loss: 14406.34
[INFO 2017-06-28 13:52:37,881 main.py:51] epoch 5728, training loss: 9779.07, average training loss: 9642.89, base loss: 14408.02
[INFO 2017-06-28 13:52:38,545 main.py:51] epoch 5729, training loss: 9491.53, average training loss: 9644.57, base loss: 14411.01
[INFO 2017-06-28 13:52:39,197 main.py:51] epoch 5730, training loss: 10521.12, average training loss: 9645.96, base loss: 14415.08
[INFO 2017-06-28 13:52:39,878 main.py:51] epoch 5731, training loss: 9607.42, average training loss: 9645.49, base loss: 14412.69
[INFO 2017-06-28 13:52:40,516 main.py:51] epoch 5732, training loss: 9806.89, average training loss: 9646.67, base loss: 14414.07
[INFO 2017-06-28 13:52:41,184 main.py:51] epoch 5733, training loss: 9545.75, average training loss: 9645.23, base loss: 14411.19
[INFO 2017-06-28 13:52:41,863 main.py:51] epoch 5734, training loss: 8804.59, average training loss: 9645.10, base loss: 14411.37
[INFO 2017-06-28 13:52:42,512 main.py:51] epoch 5735, training loss: 9561.00, average training loss: 9644.75, base loss: 14410.53
[INFO 2017-06-28 13:52:43,182 main.py:51] epoch 5736, training loss: 10805.43, average training loss: 9644.86, base loss: 14409.25
[INFO 2017-06-28 13:52:43,831 main.py:51] epoch 5737, training loss: 10342.98, average training loss: 9645.91, base loss: 14411.13
[INFO 2017-06-28 13:52:44,477 main.py:51] epoch 5738, training loss: 9727.25, average training loss: 9646.11, base loss: 14410.21
[INFO 2017-06-28 13:52:45,141 main.py:51] epoch 5739, training loss: 9488.09, average training loss: 9645.86, base loss: 14409.47
[INFO 2017-06-28 13:52:45,797 main.py:51] epoch 5740, training loss: 9928.64, average training loss: 9644.93, base loss: 14409.41
[INFO 2017-06-28 13:52:46,449 main.py:51] epoch 5741, training loss: 10023.97, average training loss: 9646.23, base loss: 14411.36
[INFO 2017-06-28 13:52:47,097 main.py:51] epoch 5742, training loss: 9847.78, average training loss: 9646.90, base loss: 14412.61
[INFO 2017-06-28 13:52:47,739 main.py:51] epoch 5743, training loss: 8927.27, average training loss: 9645.61, base loss: 14410.73
[INFO 2017-06-28 13:52:48,416 main.py:51] epoch 5744, training loss: 9207.78, average training loss: 9645.94, base loss: 14412.24
[INFO 2017-06-28 13:52:49,060 main.py:51] epoch 5745, training loss: 9576.20, average training loss: 9644.74, base loss: 14409.25
[INFO 2017-06-28 13:52:49,722 main.py:51] epoch 5746, training loss: 9922.18, average training loss: 9644.88, base loss: 14409.46
[INFO 2017-06-28 13:52:50,411 main.py:51] epoch 5747, training loss: 9266.19, average training loss: 9644.45, base loss: 14408.44
[INFO 2017-06-28 13:52:51,089 main.py:51] epoch 5748, training loss: 8909.96, average training loss: 9643.63, base loss: 14406.76
[INFO 2017-06-28 13:52:51,734 main.py:51] epoch 5749, training loss: 11436.27, average training loss: 9646.07, base loss: 14410.88
[INFO 2017-06-28 13:52:52,378 main.py:51] epoch 5750, training loss: 9371.52, average training loss: 9645.05, base loss: 14408.57
[INFO 2017-06-28 13:52:53,037 main.py:51] epoch 5751, training loss: 10362.18, average training loss: 9644.68, base loss: 14407.82
[INFO 2017-06-28 13:52:53,706 main.py:51] epoch 5752, training loss: 9800.09, average training loss: 9645.46, base loss: 14409.52
[INFO 2017-06-28 13:52:54,347 main.py:51] epoch 5753, training loss: 10017.98, average training loss: 9647.01, base loss: 14413.15
[INFO 2017-06-28 13:52:54,985 main.py:51] epoch 5754, training loss: 10096.20, average training loss: 9647.65, base loss: 14415.01
[INFO 2017-06-28 13:52:55,630 main.py:51] epoch 5755, training loss: 9403.54, average training loss: 9647.65, base loss: 14414.30
[INFO 2017-06-28 13:52:56,299 main.py:51] epoch 5756, training loss: 9983.73, average training loss: 9647.58, base loss: 14414.74
[INFO 2017-06-28 13:52:56,953 main.py:51] epoch 5757, training loss: 9838.79, average training loss: 9647.39, base loss: 14416.42
[INFO 2017-06-28 13:52:57,613 main.py:51] epoch 5758, training loss: 10353.59, average training loss: 9648.93, base loss: 14419.03
[INFO 2017-06-28 13:52:58,273 main.py:51] epoch 5759, training loss: 8873.93, average training loss: 9647.92, base loss: 14417.48
[INFO 2017-06-28 13:52:58,909 main.py:51] epoch 5760, training loss: 8741.07, average training loss: 9647.18, base loss: 14415.79
[INFO 2017-06-28 13:52:59,571 main.py:51] epoch 5761, training loss: 7887.51, average training loss: 9645.78, base loss: 14413.76
[INFO 2017-06-28 13:53:00,228 main.py:51] epoch 5762, training loss: 9632.66, average training loss: 9643.34, base loss: 14411.41
[INFO 2017-06-28 13:53:00,852 main.py:51] epoch 5763, training loss: 9078.71, average training loss: 9643.54, base loss: 14412.57
[INFO 2017-06-28 13:53:01,524 main.py:51] epoch 5764, training loss: 10283.66, average training loss: 9645.16, base loss: 14415.57
[INFO 2017-06-28 13:53:02,179 main.py:51] epoch 5765, training loss: 11641.81, average training loss: 9648.23, base loss: 14419.88
[INFO 2017-06-28 13:53:02,862 main.py:51] epoch 5766, training loss: 9715.92, average training loss: 9648.15, base loss: 14420.61
[INFO 2017-06-28 13:53:03,511 main.py:51] epoch 5767, training loss: 9466.39, average training loss: 9648.26, base loss: 14421.09
[INFO 2017-06-28 13:53:04,178 main.py:51] epoch 5768, training loss: 8393.91, average training loss: 9647.89, base loss: 14421.15
[INFO 2017-06-28 13:53:04,845 main.py:51] epoch 5769, training loss: 10313.40, average training loss: 9647.73, base loss: 14422.44
[INFO 2017-06-28 13:53:05,484 main.py:51] epoch 5770, training loss: 8530.28, average training loss: 9646.20, base loss: 14420.44
[INFO 2017-06-28 13:53:06,133 main.py:51] epoch 5771, training loss: 10640.49, average training loss: 9645.60, base loss: 14420.60
[INFO 2017-06-28 13:53:06,765 main.py:51] epoch 5772, training loss: 9034.72, average training loss: 9644.63, base loss: 14419.94
[INFO 2017-06-28 13:53:07,435 main.py:51] epoch 5773, training loss: 9673.05, average training loss: 9645.29, base loss: 14420.13
[INFO 2017-06-28 13:53:08,089 main.py:51] epoch 5774, training loss: 9002.06, average training loss: 9645.15, base loss: 14419.86
[INFO 2017-06-28 13:53:08,782 main.py:51] epoch 5775, training loss: 9707.41, average training loss: 9645.08, base loss: 14420.00
[INFO 2017-06-28 13:53:09,429 main.py:51] epoch 5776, training loss: 9534.72, average training loss: 9646.18, base loss: 14421.72
[INFO 2017-06-28 13:53:10,077 main.py:51] epoch 5777, training loss: 8631.24, average training loss: 9644.11, base loss: 14419.73
[INFO 2017-06-28 13:53:10,745 main.py:51] epoch 5778, training loss: 10288.95, average training loss: 9644.69, base loss: 14420.31
[INFO 2017-06-28 13:53:11,401 main.py:51] epoch 5779, training loss: 9937.62, average training loss: 9645.54, base loss: 14422.06
[INFO 2017-06-28 13:53:12,085 main.py:51] epoch 5780, training loss: 9398.42, average training loss: 9646.07, base loss: 14422.91
[INFO 2017-06-28 13:53:12,732 main.py:51] epoch 5781, training loss: 9664.85, average training loss: 9645.57, base loss: 14424.47
[INFO 2017-06-28 13:53:13,395 main.py:51] epoch 5782, training loss: 9503.92, average training loss: 9644.89, base loss: 14424.44
[INFO 2017-06-28 13:53:14,026 main.py:51] epoch 5783, training loss: 10887.79, average training loss: 9645.60, base loss: 14427.42
[INFO 2017-06-28 13:53:14,682 main.py:51] epoch 5784, training loss: 8889.15, average training loss: 9645.52, base loss: 14426.27
[INFO 2017-06-28 13:53:15,336 main.py:51] epoch 5785, training loss: 8656.41, average training loss: 9645.18, base loss: 14426.46
[INFO 2017-06-28 13:53:16,004 main.py:51] epoch 5786, training loss: 10304.90, average training loss: 9645.05, base loss: 14423.24
[INFO 2017-06-28 13:53:16,648 main.py:51] epoch 5787, training loss: 11064.52, average training loss: 9646.50, base loss: 14425.71
[INFO 2017-06-28 13:53:17,301 main.py:51] epoch 5788, training loss: 8460.42, average training loss: 9643.84, base loss: 14423.76
[INFO 2017-06-28 13:53:17,948 main.py:51] epoch 5789, training loss: 8019.71, average training loss: 9642.66, base loss: 14422.36
[INFO 2017-06-28 13:53:18,609 main.py:51] epoch 5790, training loss: 9499.95, average training loss: 9643.58, base loss: 14424.11
[INFO 2017-06-28 13:53:19,253 main.py:51] epoch 5791, training loss: 8887.77, average training loss: 9643.06, base loss: 14424.86
[INFO 2017-06-28 13:53:19,904 main.py:51] epoch 5792, training loss: 10030.57, average training loss: 9643.85, base loss: 14425.30
[INFO 2017-06-28 13:53:20,553 main.py:51] epoch 5793, training loss: 8822.83, average training loss: 9644.21, base loss: 14425.03
[INFO 2017-06-28 13:53:21,194 main.py:51] epoch 5794, training loss: 8825.23, average training loss: 9643.79, base loss: 14425.62
[INFO 2017-06-28 13:53:21,883 main.py:51] epoch 5795, training loss: 10554.34, average training loss: 9646.04, base loss: 14428.15
[INFO 2017-06-28 13:53:22,535 main.py:51] epoch 5796, training loss: 10145.70, average training loss: 9647.31, base loss: 14429.27
[INFO 2017-06-28 13:53:23,184 main.py:51] epoch 5797, training loss: 7995.41, average training loss: 9644.62, base loss: 14424.55
[INFO 2017-06-28 13:53:23,850 main.py:51] epoch 5798, training loss: 10235.83, average training loss: 9645.80, base loss: 14425.64
[INFO 2017-06-28 13:53:24,498 main.py:51] epoch 5799, training loss: 9158.74, average training loss: 9645.96, base loss: 14426.35
[INFO 2017-06-28 13:53:24,498 main.py:53] epoch 5799, testing
[INFO 2017-06-28 13:53:27,077 main.py:105] average testing loss: 9990.40, base loss: 14356.48
[INFO 2017-06-28 13:53:27,077 main.py:106] improve_loss: 4366.08, improve_percent: 0.30
[INFO 2017-06-28 13:53:27,077 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:53:27,753 main.py:51] epoch 5800, training loss: 9913.86, average training loss: 9646.88, base loss: 14429.10
[INFO 2017-06-28 13:53:28,418 main.py:51] epoch 5801, training loss: 9037.67, average training loss: 9646.31, base loss: 14426.44
[INFO 2017-06-28 13:53:29,088 main.py:51] epoch 5802, training loss: 10936.62, average training loss: 9648.37, base loss: 14430.49
[INFO 2017-06-28 13:53:29,735 main.py:51] epoch 5803, training loss: 9421.56, average training loss: 9648.79, base loss: 14430.72
[INFO 2017-06-28 13:53:30,394 main.py:51] epoch 5804, training loss: 9549.49, average training loss: 9647.68, base loss: 14428.36
[INFO 2017-06-28 13:53:31,055 main.py:51] epoch 5805, training loss: 8602.86, average training loss: 9646.12, base loss: 14425.06
[INFO 2017-06-28 13:53:31,690 main.py:51] epoch 5806, training loss: 11196.68, average training loss: 9648.01, base loss: 14427.21
[INFO 2017-06-28 13:53:32,353 main.py:51] epoch 5807, training loss: 9829.91, average training loss: 9647.85, base loss: 14427.08
[INFO 2017-06-28 13:53:32,997 main.py:51] epoch 5808, training loss: 10013.39, average training loss: 9647.75, base loss: 14428.75
[INFO 2017-06-28 13:53:33,636 main.py:51] epoch 5809, training loss: 8514.72, average training loss: 9646.61, base loss: 14427.21
[INFO 2017-06-28 13:53:34,287 main.py:51] epoch 5810, training loss: 9602.64, average training loss: 9646.24, base loss: 14428.54
[INFO 2017-06-28 13:53:34,975 main.py:51] epoch 5811, training loss: 9153.07, average training loss: 9646.26, base loss: 14428.15
[INFO 2017-06-28 13:53:35,658 main.py:51] epoch 5812, training loss: 10161.98, average training loss: 9646.66, base loss: 14428.54
[INFO 2017-06-28 13:53:36,311 main.py:51] epoch 5813, training loss: 9004.59, average training loss: 9645.57, base loss: 14426.82
[INFO 2017-06-28 13:53:36,972 main.py:51] epoch 5814, training loss: 8570.96, average training loss: 9643.65, base loss: 14424.03
[INFO 2017-06-28 13:53:37,620 main.py:51] epoch 5815, training loss: 9733.84, average training loss: 9644.42, base loss: 14425.55
[INFO 2017-06-28 13:53:38,279 main.py:51] epoch 5816, training loss: 8998.81, average training loss: 9643.65, base loss: 14424.25
[INFO 2017-06-28 13:53:38,927 main.py:51] epoch 5817, training loss: 8675.59, average training loss: 9642.50, base loss: 14422.60
[INFO 2017-06-28 13:53:39,600 main.py:51] epoch 5818, training loss: 8484.48, average training loss: 9642.36, base loss: 14421.73
[INFO 2017-06-28 13:53:40,242 main.py:51] epoch 5819, training loss: 9125.64, average training loss: 9642.93, base loss: 14422.17
[INFO 2017-06-28 13:53:40,923 main.py:51] epoch 5820, training loss: 10064.40, average training loss: 9642.92, base loss: 14421.33
[INFO 2017-06-28 13:53:41,607 main.py:51] epoch 5821, training loss: 8597.04, average training loss: 9642.88, base loss: 14420.92
[INFO 2017-06-28 13:53:42,255 main.py:51] epoch 5822, training loss: 10005.10, average training loss: 9643.85, base loss: 14421.09
[INFO 2017-06-28 13:53:42,908 main.py:51] epoch 5823, training loss: 9543.94, average training loss: 9645.32, base loss: 14423.16
[INFO 2017-06-28 13:53:43,594 main.py:51] epoch 5824, training loss: 9149.47, average training loss: 9642.78, base loss: 14421.06
[INFO 2017-06-28 13:53:44,250 main.py:51] epoch 5825, training loss: 10628.36, average training loss: 9642.75, base loss: 14422.32
[INFO 2017-06-28 13:53:44,913 main.py:51] epoch 5826, training loss: 9564.98, average training loss: 9642.24, base loss: 14419.37
[INFO 2017-06-28 13:53:45,585 main.py:51] epoch 5827, training loss: 9459.79, average training loss: 9641.18, base loss: 14418.31
[INFO 2017-06-28 13:53:46,256 main.py:51] epoch 5828, training loss: 9025.05, average training loss: 9640.37, base loss: 14415.83
[INFO 2017-06-28 13:53:46,929 main.py:51] epoch 5829, training loss: 8845.65, average training loss: 9639.48, base loss: 14416.51
[INFO 2017-06-28 13:53:47,580 main.py:51] epoch 5830, training loss: 9101.21, average training loss: 9638.95, base loss: 14414.47
[INFO 2017-06-28 13:53:48,228 main.py:51] epoch 5831, training loss: 8235.90, average training loss: 9637.82, base loss: 14412.23
[INFO 2017-06-28 13:53:48,869 main.py:51] epoch 5832, training loss: 9122.42, average training loss: 9637.16, base loss: 14411.67
[INFO 2017-06-28 13:53:49,523 main.py:51] epoch 5833, training loss: 8696.17, average training loss: 9636.66, base loss: 14411.91
[INFO 2017-06-28 13:53:50,198 main.py:51] epoch 5834, training loss: 8798.34, average training loss: 9634.77, base loss: 14408.28
[INFO 2017-06-28 13:53:50,840 main.py:51] epoch 5835, training loss: 9631.21, average training loss: 9635.96, base loss: 14409.89
[INFO 2017-06-28 13:53:51,498 main.py:51] epoch 5836, training loss: 9823.34, average training loss: 9636.14, base loss: 14408.56
[INFO 2017-06-28 13:53:52,137 main.py:51] epoch 5837, training loss: 9638.95, average training loss: 9636.54, base loss: 14408.78
[INFO 2017-06-28 13:53:52,790 main.py:51] epoch 5838, training loss: 8388.03, average training loss: 9633.90, base loss: 14405.72
[INFO 2017-06-28 13:53:53,442 main.py:51] epoch 5839, training loss: 8793.87, average training loss: 9631.06, base loss: 14402.06
[INFO 2017-06-28 13:53:54,100 main.py:51] epoch 5840, training loss: 9301.18, average training loss: 9631.87, base loss: 14403.79
[INFO 2017-06-28 13:53:54,734 main.py:51] epoch 5841, training loss: 9580.49, average training loss: 9631.51, base loss: 14404.09
[INFO 2017-06-28 13:53:55,375 main.py:51] epoch 5842, training loss: 8897.84, average training loss: 9630.85, base loss: 14402.53
[INFO 2017-06-28 13:53:56,033 main.py:51] epoch 5843, training loss: 8979.37, average training loss: 9629.33, base loss: 14400.15
[INFO 2017-06-28 13:53:56,692 main.py:51] epoch 5844, training loss: 9998.80, average training loss: 9629.47, base loss: 14400.76
[INFO 2017-06-28 13:53:57,351 main.py:51] epoch 5845, training loss: 10520.72, average training loss: 9628.86, base loss: 14401.12
[INFO 2017-06-28 13:53:57,994 main.py:51] epoch 5846, training loss: 9823.31, average training loss: 9627.16, base loss: 14398.28
[INFO 2017-06-28 13:53:58,640 main.py:51] epoch 5847, training loss: 8796.99, average training loss: 9625.37, base loss: 14394.88
[INFO 2017-06-28 13:53:59,290 main.py:51] epoch 5848, training loss: 9096.64, average training loss: 9625.90, base loss: 14395.56
[INFO 2017-06-28 13:53:59,934 main.py:51] epoch 5849, training loss: 9393.75, average training loss: 9624.10, base loss: 14394.13
[INFO 2017-06-28 13:54:00,587 main.py:51] epoch 5850, training loss: 8913.60, average training loss: 9622.30, base loss: 14390.89
[INFO 2017-06-28 13:54:01,251 main.py:51] epoch 5851, training loss: 9515.60, average training loss: 9623.03, base loss: 14393.36
[INFO 2017-06-28 13:54:01,922 main.py:51] epoch 5852, training loss: 10236.72, average training loss: 9621.88, base loss: 14392.81
[INFO 2017-06-28 13:54:02,569 main.py:51] epoch 5853, training loss: 9917.27, average training loss: 9621.03, base loss: 14391.84
[INFO 2017-06-28 13:54:03,220 main.py:51] epoch 5854, training loss: 9817.29, average training loss: 9620.61, base loss: 14391.15
[INFO 2017-06-28 13:54:03,865 main.py:51] epoch 5855, training loss: 9510.30, average training loss: 9619.32, base loss: 14389.04
[INFO 2017-06-28 13:54:04,510 main.py:51] epoch 5856, training loss: 9972.51, average training loss: 9620.45, base loss: 14391.72
[INFO 2017-06-28 13:54:05,147 main.py:51] epoch 5857, training loss: 10948.40, average training loss: 9622.18, base loss: 14393.06
[INFO 2017-06-28 13:54:05,790 main.py:51] epoch 5858, training loss: 8396.43, average training loss: 9619.88, base loss: 14388.58
[INFO 2017-06-28 13:54:06,446 main.py:51] epoch 5859, training loss: 10898.60, average training loss: 9621.77, base loss: 14390.84
[INFO 2017-06-28 13:54:07,109 main.py:51] epoch 5860, training loss: 8911.80, average training loss: 9622.00, base loss: 14390.51
[INFO 2017-06-28 13:54:07,764 main.py:51] epoch 5861, training loss: 10758.54, average training loss: 9621.26, base loss: 14389.19
[INFO 2017-06-28 13:54:08,429 main.py:51] epoch 5862, training loss: 9100.70, average training loss: 9620.61, base loss: 14389.08
[INFO 2017-06-28 13:54:09,089 main.py:51] epoch 5863, training loss: 8298.38, average training loss: 9618.00, base loss: 14386.33
[INFO 2017-06-28 13:54:09,735 main.py:51] epoch 5864, training loss: 8061.30, average training loss: 9616.91, base loss: 14385.15
[INFO 2017-06-28 13:54:10,400 main.py:51] epoch 5865, training loss: 9903.95, average training loss: 9618.87, base loss: 14388.52
[INFO 2017-06-28 13:54:11,056 main.py:51] epoch 5866, training loss: 10151.42, average training loss: 9619.30, base loss: 14390.18
[INFO 2017-06-28 13:54:11,721 main.py:51] epoch 5867, training loss: 11182.93, average training loss: 9620.73, base loss: 14390.31
[INFO 2017-06-28 13:54:12,356 main.py:51] epoch 5868, training loss: 8830.24, average training loss: 9620.12, base loss: 14387.48
[INFO 2017-06-28 13:54:13,004 main.py:51] epoch 5869, training loss: 8456.38, average training loss: 9618.62, base loss: 14384.56
[INFO 2017-06-28 13:54:13,655 main.py:51] epoch 5870, training loss: 9608.35, average training loss: 9619.94, base loss: 14386.52
[INFO 2017-06-28 13:54:14,311 main.py:51] epoch 5871, training loss: 9482.69, average training loss: 9619.52, base loss: 14386.13
[INFO 2017-06-28 13:54:14,966 main.py:51] epoch 5872, training loss: 9129.62, average training loss: 9619.78, base loss: 14386.59
[INFO 2017-06-28 13:54:15,625 main.py:51] epoch 5873, training loss: 10574.43, average training loss: 9621.13, base loss: 14390.17
[INFO 2017-06-28 13:54:16,306 main.py:51] epoch 5874, training loss: 10416.56, average training loss: 9621.90, base loss: 14392.62
[INFO 2017-06-28 13:54:16,966 main.py:51] epoch 5875, training loss: 9346.90, average training loss: 9621.71, base loss: 14392.59
[INFO 2017-06-28 13:54:17,609 main.py:51] epoch 5876, training loss: 8396.08, average training loss: 9620.44, base loss: 14391.52
[INFO 2017-06-28 13:54:18,285 main.py:51] epoch 5877, training loss: 10310.26, average training loss: 9621.68, base loss: 14392.08
[INFO 2017-06-28 13:54:18,944 main.py:51] epoch 5878, training loss: 9589.59, average training loss: 9622.05, base loss: 14392.68
[INFO 2017-06-28 13:54:19,598 main.py:51] epoch 5879, training loss: 9752.71, average training loss: 9621.82, base loss: 14392.96
[INFO 2017-06-28 13:54:20,240 main.py:51] epoch 5880, training loss: 9998.60, average training loss: 9622.18, base loss: 14391.71
[INFO 2017-06-28 13:54:20,901 main.py:51] epoch 5881, training loss: 8996.22, average training loss: 9620.67, base loss: 14389.22
[INFO 2017-06-28 13:54:21,556 main.py:51] epoch 5882, training loss: 10905.52, average training loss: 9620.94, base loss: 14390.74
[INFO 2017-06-28 13:54:22,227 main.py:51] epoch 5883, training loss: 9578.80, average training loss: 9621.40, base loss: 14393.15
[INFO 2017-06-28 13:54:22,866 main.py:51] epoch 5884, training loss: 9003.74, average training loss: 9620.87, base loss: 14393.02
[INFO 2017-06-28 13:54:23,539 main.py:51] epoch 5885, training loss: 10338.15, average training loss: 9621.46, base loss: 14394.23
[INFO 2017-06-28 13:54:24,186 main.py:51] epoch 5886, training loss: 9176.50, average training loss: 9621.29, base loss: 14394.91
[INFO 2017-06-28 13:54:24,846 main.py:51] epoch 5887, training loss: 9945.51, average training loss: 9621.39, base loss: 14395.42
[INFO 2017-06-28 13:54:25,500 main.py:51] epoch 5888, training loss: 9131.03, average training loss: 9621.62, base loss: 14395.93
[INFO 2017-06-28 13:54:26,143 main.py:51] epoch 5889, training loss: 9071.13, average training loss: 9618.55, base loss: 14391.78
[INFO 2017-06-28 13:54:26,792 main.py:51] epoch 5890, training loss: 10415.16, average training loss: 9618.88, base loss: 14391.24
[INFO 2017-06-28 13:54:27,445 main.py:51] epoch 5891, training loss: 10235.97, average training loss: 9619.02, base loss: 14392.50
[INFO 2017-06-28 13:54:28,088 main.py:51] epoch 5892, training loss: 9363.38, average training loss: 9617.60, base loss: 14389.54
[INFO 2017-06-28 13:54:28,736 main.py:51] epoch 5893, training loss: 9048.36, average training loss: 9617.34, base loss: 14388.47
[INFO 2017-06-28 13:54:29,390 main.py:51] epoch 5894, training loss: 9703.67, average training loss: 9617.43, base loss: 14389.68
[INFO 2017-06-28 13:54:30,047 main.py:51] epoch 5895, training loss: 10201.67, average training loss: 9619.00, base loss: 14391.28
[INFO 2017-06-28 13:54:30,713 main.py:51] epoch 5896, training loss: 10096.08, average training loss: 9619.72, base loss: 14392.45
[INFO 2017-06-28 13:54:31,363 main.py:51] epoch 5897, training loss: 10252.41, average training loss: 9619.39, base loss: 14391.71
[INFO 2017-06-28 13:54:32,010 main.py:51] epoch 5898, training loss: 9645.68, average training loss: 9620.40, base loss: 14392.60
[INFO 2017-06-28 13:54:32,658 main.py:51] epoch 5899, training loss: 8306.15, average training loss: 9618.63, base loss: 14389.70
[INFO 2017-06-28 13:54:32,659 main.py:53] epoch 5899, testing
[INFO 2017-06-28 13:54:35,197 main.py:105] average testing loss: 11214.98, base loss: 16037.99
[INFO 2017-06-28 13:54:35,198 main.py:106] improve_loss: 4823.01, improve_percent: 0.30
[INFO 2017-06-28 13:54:35,198 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:54:35,824 main.py:51] epoch 5900, training loss: 9120.67, average training loss: 9617.36, base loss: 14388.97
[INFO 2017-06-28 13:54:36,481 main.py:51] epoch 5901, training loss: 10465.21, average training loss: 9617.82, base loss: 14389.15
[INFO 2017-06-28 13:54:37,148 main.py:51] epoch 5902, training loss: 8963.88, average training loss: 9617.44, base loss: 14389.84
[INFO 2017-06-28 13:54:37,793 main.py:51] epoch 5903, training loss: 8122.56, average training loss: 9616.48, base loss: 14386.95
[INFO 2017-06-28 13:54:38,475 main.py:51] epoch 5904, training loss: 9017.76, average training loss: 9612.92, base loss: 14380.72
[INFO 2017-06-28 13:54:39,115 main.py:51] epoch 5905, training loss: 10227.21, average training loss: 9613.48, base loss: 14383.93
[INFO 2017-06-28 13:54:39,751 main.py:51] epoch 5906, training loss: 9617.85, average training loss: 9613.58, base loss: 14383.96
[INFO 2017-06-28 13:54:40,411 main.py:51] epoch 5907, training loss: 9222.82, average training loss: 9613.47, base loss: 14382.97
[INFO 2017-06-28 13:54:41,067 main.py:51] epoch 5908, training loss: 8937.40, average training loss: 9612.60, base loss: 14382.06
[INFO 2017-06-28 13:54:41,708 main.py:51] epoch 5909, training loss: 11763.08, average training loss: 9613.76, base loss: 14385.85
[INFO 2017-06-28 13:54:42,367 main.py:51] epoch 5910, training loss: 10116.36, average training loss: 9614.83, base loss: 14386.89
[INFO 2017-06-28 13:54:43,016 main.py:51] epoch 5911, training loss: 9487.69, average training loss: 9612.75, base loss: 14384.72
[INFO 2017-06-28 13:54:43,660 main.py:51] epoch 5912, training loss: 9052.43, average training loss: 9611.91, base loss: 14383.70
[INFO 2017-06-28 13:54:44,319 main.py:51] epoch 5913, training loss: 9727.59, average training loss: 9610.55, base loss: 14380.17
[INFO 2017-06-28 13:54:44,975 main.py:51] epoch 5914, training loss: 10004.57, average training loss: 9610.79, base loss: 14380.54
[INFO 2017-06-28 13:54:45,636 main.py:51] epoch 5915, training loss: 8077.94, average training loss: 9610.12, base loss: 14380.70
[INFO 2017-06-28 13:54:46,275 main.py:51] epoch 5916, training loss: 9465.08, average training loss: 9609.33, base loss: 14380.38
[INFO 2017-06-28 13:54:46,940 main.py:51] epoch 5917, training loss: 8652.30, average training loss: 9607.21, base loss: 14379.18
[INFO 2017-06-28 13:54:47,579 main.py:51] epoch 5918, training loss: 9156.70, average training loss: 9605.40, base loss: 14376.82
[INFO 2017-06-28 13:54:48,241 main.py:51] epoch 5919, training loss: 10508.65, average training loss: 9606.03, base loss: 14376.29
[INFO 2017-06-28 13:54:48,886 main.py:51] epoch 5920, training loss: 11192.61, average training loss: 9607.95, base loss: 14379.06
[INFO 2017-06-28 13:54:49,548 main.py:51] epoch 5921, training loss: 9971.83, average training loss: 9607.04, base loss: 14377.74
[INFO 2017-06-28 13:54:50,210 main.py:51] epoch 5922, training loss: 8589.47, average training loss: 9607.17, base loss: 14378.45
[INFO 2017-06-28 13:54:50,866 main.py:51] epoch 5923, training loss: 9586.94, average training loss: 9605.91, base loss: 14378.70
[INFO 2017-06-28 13:54:51,506 main.py:51] epoch 5924, training loss: 8288.84, average training loss: 9602.95, base loss: 14375.00
[INFO 2017-06-28 13:54:52,153 main.py:51] epoch 5925, training loss: 8900.41, average training loss: 9602.52, base loss: 14372.19
[INFO 2017-06-28 13:54:52,798 main.py:51] epoch 5926, training loss: 9106.65, average training loss: 9601.74, base loss: 14369.49
[INFO 2017-06-28 13:54:53,446 main.py:51] epoch 5927, training loss: 8797.48, average training loss: 9601.27, base loss: 14369.32
[INFO 2017-06-28 13:54:54,098 main.py:51] epoch 5928, training loss: 9598.58, average training loss: 9601.41, base loss: 14369.01
[INFO 2017-06-28 13:54:54,762 main.py:51] epoch 5929, training loss: 7520.04, average training loss: 9600.19, base loss: 14366.61
[INFO 2017-06-28 13:54:55,423 main.py:51] epoch 5930, training loss: 10754.09, average training loss: 9602.20, base loss: 14368.90
[INFO 2017-06-28 13:54:56,082 main.py:51] epoch 5931, training loss: 10616.07, average training loss: 9603.20, base loss: 14368.59
[INFO 2017-06-28 13:54:56,734 main.py:51] epoch 5932, training loss: 8589.44, average training loss: 9602.34, base loss: 14366.64
[INFO 2017-06-28 13:54:57,390 main.py:51] epoch 5933, training loss: 10018.43, average training loss: 9602.40, base loss: 14366.59
[INFO 2017-06-28 13:54:58,035 main.py:51] epoch 5934, training loss: 9188.10, average training loss: 9600.88, base loss: 14363.56
[INFO 2017-06-28 13:54:58,684 main.py:51] epoch 5935, training loss: 8685.27, average training loss: 9600.15, base loss: 14363.60
[INFO 2017-06-28 13:54:59,330 main.py:51] epoch 5936, training loss: 9331.00, average training loss: 9599.67, base loss: 14362.63
[INFO 2017-06-28 13:54:59,970 main.py:51] epoch 5937, training loss: 9444.70, average training loss: 9600.79, base loss: 14365.07
[INFO 2017-06-28 13:55:00,620 main.py:51] epoch 5938, training loss: 9539.02, average training loss: 9601.54, base loss: 14367.43
[INFO 2017-06-28 13:55:01,285 main.py:51] epoch 5939, training loss: 10965.27, average training loss: 9602.92, base loss: 14369.16
[INFO 2017-06-28 13:55:01,955 main.py:51] epoch 5940, training loss: 9358.40, average training loss: 9603.06, base loss: 14369.13
[INFO 2017-06-28 13:55:02,623 main.py:51] epoch 5941, training loss: 9630.28, average training loss: 9603.31, base loss: 14369.32
[INFO 2017-06-28 13:55:03,271 main.py:51] epoch 5942, training loss: 9489.12, average training loss: 9602.88, base loss: 14366.68
[INFO 2017-06-28 13:55:03,929 main.py:51] epoch 5943, training loss: 9685.11, average training loss: 9603.24, base loss: 14366.13
[INFO 2017-06-28 13:55:04,610 main.py:51] epoch 5944, training loss: 9375.58, average training loss: 9602.71, base loss: 14364.01
[INFO 2017-06-28 13:55:05,257 main.py:51] epoch 5945, training loss: 9787.93, average training loss: 9604.00, base loss: 14365.45
[INFO 2017-06-28 13:55:05,915 main.py:51] epoch 5946, training loss: 9809.57, average training loss: 9603.82, base loss: 14363.79
[INFO 2017-06-28 13:55:06,602 main.py:51] epoch 5947, training loss: 9449.46, average training loss: 9602.76, base loss: 14363.51
[INFO 2017-06-28 13:55:07,259 main.py:51] epoch 5948, training loss: 10064.94, average training loss: 9604.32, base loss: 14365.49
[INFO 2017-06-28 13:55:07,918 main.py:51] epoch 5949, training loss: 10324.67, average training loss: 9606.29, base loss: 14368.76
[INFO 2017-06-28 13:55:08,566 main.py:51] epoch 5950, training loss: 8872.72, average training loss: 9605.82, base loss: 14367.73
[INFO 2017-06-28 13:55:09,214 main.py:51] epoch 5951, training loss: 9340.26, average training loss: 9606.88, base loss: 14369.36
[INFO 2017-06-28 13:55:09,887 main.py:51] epoch 5952, training loss: 8247.14, average training loss: 9605.91, base loss: 14366.39
[INFO 2017-06-28 13:55:10,542 main.py:51] epoch 5953, training loss: 9268.65, average training loss: 9605.61, base loss: 14367.26
[INFO 2017-06-28 13:55:11,207 main.py:51] epoch 5954, training loss: 9384.53, average training loss: 9605.00, base loss: 14364.88
[INFO 2017-06-28 13:55:11,840 main.py:51] epoch 5955, training loss: 9065.74, average training loss: 9604.18, base loss: 14363.73
[INFO 2017-06-28 13:55:12,489 main.py:51] epoch 5956, training loss: 9099.77, average training loss: 9603.77, base loss: 14361.19
[INFO 2017-06-28 13:55:13,142 main.py:51] epoch 5957, training loss: 8326.87, average training loss: 9603.20, base loss: 14361.77
[INFO 2017-06-28 13:55:13,779 main.py:51] epoch 5958, training loss: 10038.53, average training loss: 9602.07, base loss: 14359.11
[INFO 2017-06-28 13:55:14,427 main.py:51] epoch 5959, training loss: 9556.17, average training loss: 9601.59, base loss: 14357.66
[INFO 2017-06-28 13:55:15,112 main.py:51] epoch 5960, training loss: 9202.05, average training loss: 9600.95, base loss: 14357.71
[INFO 2017-06-28 13:55:15,793 main.py:51] epoch 5961, training loss: 8385.47, average training loss: 9599.89, base loss: 14355.16
[INFO 2017-06-28 13:55:16,457 main.py:51] epoch 5962, training loss: 9484.76, average training loss: 9598.98, base loss: 14353.15
[INFO 2017-06-28 13:55:17,124 main.py:51] epoch 5963, training loss: 10186.99, average training loss: 9599.72, base loss: 14353.87
[INFO 2017-06-28 13:55:17,784 main.py:51] epoch 5964, training loss: 9855.51, average training loss: 9599.95, base loss: 14353.20
[INFO 2017-06-28 13:55:18,439 main.py:51] epoch 5965, training loss: 8410.88, average training loss: 9598.74, base loss: 14350.45
[INFO 2017-06-28 13:55:19,082 main.py:51] epoch 5966, training loss: 11150.46, average training loss: 9601.49, base loss: 14353.91
[INFO 2017-06-28 13:55:19,727 main.py:51] epoch 5967, training loss: 8983.55, average training loss: 9600.70, base loss: 14352.59
[INFO 2017-06-28 13:55:20,383 main.py:51] epoch 5968, training loss: 9830.03, average training loss: 9601.42, base loss: 14353.22
[INFO 2017-06-28 13:55:21,031 main.py:51] epoch 5969, training loss: 9549.59, average training loss: 9602.33, base loss: 14355.01
[INFO 2017-06-28 13:55:21,688 main.py:51] epoch 5970, training loss: 8528.06, average training loss: 9601.25, base loss: 14352.63
[INFO 2017-06-28 13:55:22,342 main.py:51] epoch 5971, training loss: 8667.65, average training loss: 9600.13, base loss: 14350.61
[INFO 2017-06-28 13:55:23,009 main.py:51] epoch 5972, training loss: 9682.26, average training loss: 9600.87, base loss: 14353.19
[INFO 2017-06-28 13:55:23,670 main.py:51] epoch 5973, training loss: 11193.77, average training loss: 9601.32, base loss: 14353.44
[INFO 2017-06-28 13:55:24,308 main.py:51] epoch 5974, training loss: 10216.46, average training loss: 9601.74, base loss: 14353.42
[INFO 2017-06-28 13:55:24,969 main.py:51] epoch 5975, training loss: 8454.00, average training loss: 9600.76, base loss: 14350.31
[INFO 2017-06-28 13:55:25,633 main.py:51] epoch 5976, training loss: 9753.45, average training loss: 9600.53, base loss: 14350.87
[INFO 2017-06-28 13:55:26,291 main.py:51] epoch 5977, training loss: 10580.63, average training loss: 9599.87, base loss: 14348.74
[INFO 2017-06-28 13:55:26,982 main.py:51] epoch 5978, training loss: 9551.54, average training loss: 9600.51, base loss: 14349.75
[INFO 2017-06-28 13:55:27,636 main.py:51] epoch 5979, training loss: 10111.51, average training loss: 9601.18, base loss: 14349.86
[INFO 2017-06-28 13:55:28,295 main.py:51] epoch 5980, training loss: 9502.09, average training loss: 9601.94, base loss: 14350.88
[INFO 2017-06-28 13:55:28,953 main.py:51] epoch 5981, training loss: 9969.72, average training loss: 9602.66, base loss: 14353.28
[INFO 2017-06-28 13:55:29,624 main.py:51] epoch 5982, training loss: 9062.32, average training loss: 9602.57, base loss: 14353.48
[INFO 2017-06-28 13:55:30,300 main.py:51] epoch 5983, training loss: 7478.65, average training loss: 9599.70, base loss: 14349.19
[INFO 2017-06-28 13:55:30,969 main.py:51] epoch 5984, training loss: 11124.43, average training loss: 9602.75, base loss: 14356.62
[INFO 2017-06-28 13:55:31,625 main.py:51] epoch 5985, training loss: 8937.18, average training loss: 9602.22, base loss: 14357.38
[INFO 2017-06-28 13:55:32,297 main.py:51] epoch 5986, training loss: 8439.20, average training loss: 9598.51, base loss: 14352.95
[INFO 2017-06-28 13:55:32,948 main.py:51] epoch 5987, training loss: 10231.29, average training loss: 9600.06, base loss: 14355.01
[INFO 2017-06-28 13:55:33,640 main.py:51] epoch 5988, training loss: 10088.13, average training loss: 9601.00, base loss: 14356.90
[INFO 2017-06-28 13:55:34,302 main.py:51] epoch 5989, training loss: 7578.26, average training loss: 9598.70, base loss: 14352.43
[INFO 2017-06-28 13:55:34,944 main.py:51] epoch 5990, training loss: 11333.96, average training loss: 9600.28, base loss: 14356.38
[INFO 2017-06-28 13:55:35,603 main.py:51] epoch 5991, training loss: 9959.24, average training loss: 9601.06, base loss: 14356.01
[INFO 2017-06-28 13:55:36,248 main.py:51] epoch 5992, training loss: 11220.99, average training loss: 9602.75, base loss: 14355.98
[INFO 2017-06-28 13:55:36,906 main.py:51] epoch 5993, training loss: 9051.00, average training loss: 9601.24, base loss: 14352.86
[INFO 2017-06-28 13:55:37,596 main.py:51] epoch 5994, training loss: 10915.44, average training loss: 9603.42, base loss: 14356.38
[INFO 2017-06-28 13:55:38,236 main.py:51] epoch 5995, training loss: 9448.26, average training loss: 9603.69, base loss: 14358.49
[INFO 2017-06-28 13:55:38,899 main.py:51] epoch 5996, training loss: 9480.22, average training loss: 9603.34, base loss: 14358.72
[INFO 2017-06-28 13:55:39,543 main.py:51] epoch 5997, training loss: 9201.36, average training loss: 9603.22, base loss: 14356.91
[INFO 2017-06-28 13:55:40,216 main.py:51] epoch 5998, training loss: 10685.86, average training loss: 9604.21, base loss: 14358.17
[INFO 2017-06-28 13:55:40,872 main.py:51] epoch 5999, training loss: 8879.67, average training loss: 9602.56, base loss: 14356.24
[INFO 2017-06-28 13:55:40,872 main.py:53] epoch 5999, testing
[INFO 2017-06-28 13:55:43,520 main.py:105] average testing loss: 10407.86, base loss: 14423.30
[INFO 2017-06-28 13:55:43,520 main.py:106] improve_loss: 4015.44, improve_percent: 0.28
[INFO 2017-06-28 13:55:43,521 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:55:44,192 main.py:51] epoch 6000, training loss: 10140.01, average training loss: 9601.43, base loss: 14355.87
[INFO 2017-06-28 13:55:44,861 main.py:51] epoch 6001, training loss: 10713.05, average training loss: 9603.51, base loss: 14360.58
[INFO 2017-06-28 13:55:45,504 main.py:51] epoch 6002, training loss: 9468.85, average training loss: 9603.10, base loss: 14360.84
[INFO 2017-06-28 13:55:46,136 main.py:51] epoch 6003, training loss: 10750.11, average training loss: 9603.40, base loss: 14361.66
[INFO 2017-06-28 13:55:46,778 main.py:51] epoch 6004, training loss: 9445.39, average training loss: 9604.23, base loss: 14362.95
[INFO 2017-06-28 13:55:47,429 main.py:51] epoch 6005, training loss: 9828.15, average training loss: 9604.37, base loss: 14362.01
[INFO 2017-06-28 13:55:48,088 main.py:51] epoch 6006, training loss: 9309.86, average training loss: 9604.31, base loss: 14362.67
[INFO 2017-06-28 13:55:48,773 main.py:51] epoch 6007, training loss: 8837.72, average training loss: 9603.81, base loss: 14361.65
[INFO 2017-06-28 13:55:49,440 main.py:51] epoch 6008, training loss: 9012.96, average training loss: 9604.74, base loss: 14362.26
[INFO 2017-06-28 13:55:50,093 main.py:51] epoch 6009, training loss: 9311.21, average training loss: 9604.55, base loss: 14362.77
[INFO 2017-06-28 13:55:50,729 main.py:51] epoch 6010, training loss: 8607.45, average training loss: 9603.31, base loss: 14361.28
[INFO 2017-06-28 13:55:51,380 main.py:51] epoch 6011, training loss: 10402.05, average training loss: 9603.93, base loss: 14360.98
[INFO 2017-06-28 13:55:52,027 main.py:51] epoch 6012, training loss: 9597.70, average training loss: 9605.19, base loss: 14362.91
[INFO 2017-06-28 13:55:52,685 main.py:51] epoch 6013, training loss: 10375.99, average training loss: 9605.65, base loss: 14363.84
[INFO 2017-06-28 13:55:53,341 main.py:51] epoch 6014, training loss: 9418.32, average training loss: 9605.76, base loss: 14364.10
[INFO 2017-06-28 13:55:54,018 main.py:51] epoch 6015, training loss: 8526.45, average training loss: 9605.80, base loss: 14363.73
[INFO 2017-06-28 13:55:54,648 main.py:51] epoch 6016, training loss: 9898.05, average training loss: 9606.49, base loss: 14364.73
[INFO 2017-06-28 13:55:55,314 main.py:51] epoch 6017, training loss: 9753.34, average training loss: 9607.40, base loss: 14363.99
[INFO 2017-06-28 13:55:55,989 main.py:51] epoch 6018, training loss: 8527.25, average training loss: 9606.66, base loss: 14362.00
[INFO 2017-06-28 13:55:56,645 main.py:51] epoch 6019, training loss: 10222.08, average training loss: 9607.57, base loss: 14364.10
[INFO 2017-06-28 13:55:57,295 main.py:51] epoch 6020, training loss: 10642.22, average training loss: 9608.37, base loss: 14362.57
[INFO 2017-06-28 13:55:57,941 main.py:51] epoch 6021, training loss: 9286.60, average training loss: 9608.18, base loss: 14363.15
[INFO 2017-06-28 13:55:58,626 main.py:51] epoch 6022, training loss: 10104.06, average training loss: 9608.75, base loss: 14363.95
[INFO 2017-06-28 13:55:59,283 main.py:51] epoch 6023, training loss: 10200.07, average training loss: 9610.27, base loss: 14366.84
[INFO 2017-06-28 13:55:59,941 main.py:51] epoch 6024, training loss: 8875.49, average training loss: 9609.86, base loss: 14364.91
[INFO 2017-06-28 13:56:00,601 main.py:51] epoch 6025, training loss: 8898.89, average training loss: 9609.34, base loss: 14362.50
[INFO 2017-06-28 13:56:01,255 main.py:51] epoch 6026, training loss: 10418.52, average training loss: 9609.40, base loss: 14362.92
[INFO 2017-06-28 13:56:01,904 main.py:51] epoch 6027, training loss: 9400.17, average training loss: 9609.26, base loss: 14360.24
[INFO 2017-06-28 13:56:02,566 main.py:51] epoch 6028, training loss: 7848.87, average training loss: 9607.99, base loss: 14357.47
[INFO 2017-06-28 13:56:03,213 main.py:51] epoch 6029, training loss: 8693.99, average training loss: 9607.12, base loss: 14355.58
[INFO 2017-06-28 13:56:03,863 main.py:51] epoch 6030, training loss: 10757.44, average training loss: 9607.78, base loss: 14358.10
[INFO 2017-06-28 13:56:04,538 main.py:51] epoch 6031, training loss: 8879.96, average training loss: 9607.58, base loss: 14358.49
[INFO 2017-06-28 13:56:05,196 main.py:51] epoch 6032, training loss: 9688.16, average training loss: 9608.85, base loss: 14360.52
[INFO 2017-06-28 13:56:05,848 main.py:51] epoch 6033, training loss: 11068.32, average training loss: 9608.98, base loss: 14360.92
[INFO 2017-06-28 13:56:06,492 main.py:51] epoch 6034, training loss: 8505.95, average training loss: 9605.55, base loss: 14355.60
[INFO 2017-06-28 13:56:07,155 main.py:51] epoch 6035, training loss: 10129.62, average training loss: 9606.93, base loss: 14358.26
[INFO 2017-06-28 13:56:07,818 main.py:51] epoch 6036, training loss: 10747.74, average training loss: 9606.92, base loss: 14359.63
[INFO 2017-06-28 13:56:08,461 main.py:51] epoch 6037, training loss: 8892.76, average training loss: 9607.27, base loss: 14360.90
[INFO 2017-06-28 13:56:09,117 main.py:51] epoch 6038, training loss: 8787.08, average training loss: 9605.97, base loss: 14360.30
[INFO 2017-06-28 13:56:09,763 main.py:51] epoch 6039, training loss: 9457.10, average training loss: 9603.99, base loss: 14356.04
[INFO 2017-06-28 13:56:10,426 main.py:51] epoch 6040, training loss: 9652.69, average training loss: 9601.36, base loss: 14353.28
[INFO 2017-06-28 13:56:11,083 main.py:51] epoch 6041, training loss: 9675.45, average training loss: 9600.96, base loss: 14351.38
[INFO 2017-06-28 13:56:11,739 main.py:51] epoch 6042, training loss: 11256.32, average training loss: 9603.29, base loss: 14354.74
[INFO 2017-06-28 13:56:12,371 main.py:51] epoch 6043, training loss: 9711.75, average training loss: 9603.40, base loss: 14355.47
[INFO 2017-06-28 13:56:13,044 main.py:51] epoch 6044, training loss: 8376.89, average training loss: 9601.25, base loss: 14352.56
[INFO 2017-06-28 13:56:13,692 main.py:51] epoch 6045, training loss: 8689.87, average training loss: 9600.18, base loss: 14352.52
[INFO 2017-06-28 13:56:14,338 main.py:51] epoch 6046, training loss: 10417.60, average training loss: 9600.84, base loss: 14353.07
[INFO 2017-06-28 13:56:15,008 main.py:51] epoch 6047, training loss: 9471.28, average training loss: 9600.75, base loss: 14353.13
[INFO 2017-06-28 13:56:15,671 main.py:51] epoch 6048, training loss: 9312.92, average training loss: 9600.04, base loss: 14351.79
[INFO 2017-06-28 13:56:16,306 main.py:51] epoch 6049, training loss: 9386.84, average training loss: 9600.20, base loss: 14351.57
[INFO 2017-06-28 13:56:16,978 main.py:51] epoch 6050, training loss: 8328.24, average training loss: 9598.84, base loss: 14348.89
[INFO 2017-06-28 13:56:17,625 main.py:51] epoch 6051, training loss: 9759.63, average training loss: 9597.80, base loss: 14346.85
[INFO 2017-06-28 13:56:18,282 main.py:51] epoch 6052, training loss: 9428.91, average training loss: 9597.66, base loss: 14345.42
[INFO 2017-06-28 13:56:18,952 main.py:51] epoch 6053, training loss: 10642.90, average training loss: 9597.97, base loss: 14345.74
[INFO 2017-06-28 13:56:19,630 main.py:51] epoch 6054, training loss: 8944.65, average training loss: 9597.60, base loss: 14344.94
[INFO 2017-06-28 13:56:20,287 main.py:51] epoch 6055, training loss: 9027.54, average training loss: 9596.48, base loss: 14343.81
[INFO 2017-06-28 13:56:20,938 main.py:51] epoch 6056, training loss: 9336.47, average training loss: 9595.38, base loss: 14342.88
[INFO 2017-06-28 13:56:21,594 main.py:51] epoch 6057, training loss: 9192.77, average training loss: 9592.54, base loss: 14340.15
[INFO 2017-06-28 13:56:22,243 main.py:51] epoch 6058, training loss: 9119.80, average training loss: 9592.00, base loss: 14338.64
[INFO 2017-06-28 13:56:22,910 main.py:51] epoch 6059, training loss: 10761.70, average training loss: 9593.30, base loss: 14339.58
[INFO 2017-06-28 13:56:23,566 main.py:51] epoch 6060, training loss: 8990.60, average training loss: 9593.99, base loss: 14339.21
[INFO 2017-06-28 13:56:24,209 main.py:51] epoch 6061, training loss: 10185.50, average training loss: 9594.59, base loss: 14337.91
[INFO 2017-06-28 13:56:24,860 main.py:51] epoch 6062, training loss: 10273.62, average training loss: 9595.03, base loss: 14339.20
[INFO 2017-06-28 13:56:25,490 main.py:51] epoch 6063, training loss: 8960.66, average training loss: 9593.78, base loss: 14336.45
[INFO 2017-06-28 13:56:26,145 main.py:51] epoch 6064, training loss: 10075.14, average training loss: 9593.38, base loss: 14335.54
[INFO 2017-06-28 13:56:26,788 main.py:51] epoch 6065, training loss: 11085.72, average training loss: 9592.08, base loss: 14334.75
[INFO 2017-06-28 13:56:27,447 main.py:51] epoch 6066, training loss: 10217.07, average training loss: 9594.06, base loss: 14338.06
[INFO 2017-06-28 13:56:28,097 main.py:51] epoch 6067, training loss: 8798.53, average training loss: 9593.30, base loss: 14335.79
[INFO 2017-06-28 13:56:28,766 main.py:51] epoch 6068, training loss: 10237.40, average training loss: 9593.37, base loss: 14334.56
[INFO 2017-06-28 13:56:29,429 main.py:51] epoch 6069, training loss: 10225.42, average training loss: 9594.83, base loss: 14337.08
[INFO 2017-06-28 13:56:30,082 main.py:51] epoch 6070, training loss: 10971.04, average training loss: 9596.24, base loss: 14339.63
[INFO 2017-06-28 13:56:30,730 main.py:51] epoch 6071, training loss: 9257.45, average training loss: 9596.25, base loss: 14340.53
[INFO 2017-06-28 13:56:31,392 main.py:51] epoch 6072, training loss: 8800.77, average training loss: 9596.80, base loss: 14341.89
[INFO 2017-06-28 13:56:32,039 main.py:51] epoch 6073, training loss: 10785.12, average training loss: 9597.99, base loss: 14344.43
[INFO 2017-06-28 13:56:32,688 main.py:51] epoch 6074, training loss: 8984.30, average training loss: 9598.45, base loss: 14346.28
[INFO 2017-06-28 13:56:33,339 main.py:51] epoch 6075, training loss: 10751.44, average training loss: 9600.37, base loss: 14348.78
[INFO 2017-06-28 13:56:33,986 main.py:51] epoch 6076, training loss: 10919.77, average training loss: 9601.49, base loss: 14349.30
[INFO 2017-06-28 13:56:34,652 main.py:51] epoch 6077, training loss: 8865.69, average training loss: 9600.98, base loss: 14349.04
[INFO 2017-06-28 13:56:35,287 main.py:51] epoch 6078, training loss: 8729.45, average training loss: 9600.11, base loss: 14348.08
[INFO 2017-06-28 13:56:35,945 main.py:51] epoch 6079, training loss: 10180.67, average training loss: 9600.24, base loss: 14346.42
[INFO 2017-06-28 13:56:36,601 main.py:51] epoch 6080, training loss: 9037.54, average training loss: 9599.70, base loss: 14345.58
[INFO 2017-06-28 13:56:37,263 main.py:51] epoch 6081, training loss: 10237.08, average training loss: 9600.14, base loss: 14347.25
[INFO 2017-06-28 13:56:37,927 main.py:51] epoch 6082, training loss: 10168.71, average training loss: 9602.06, base loss: 14350.28
[INFO 2017-06-28 13:56:38,587 main.py:51] epoch 6083, training loss: 8714.67, average training loss: 9600.87, base loss: 14347.43
[INFO 2017-06-28 13:56:39,255 main.py:51] epoch 6084, training loss: 8980.48, average training loss: 9600.00, base loss: 14345.69
[INFO 2017-06-28 13:56:39,928 main.py:51] epoch 6085, training loss: 10254.80, average training loss: 9599.75, base loss: 14346.31
[INFO 2017-06-28 13:56:40,579 main.py:51] epoch 6086, training loss: 9704.32, average training loss: 9600.41, base loss: 14346.30
[INFO 2017-06-28 13:56:41,246 main.py:51] epoch 6087, training loss: 9105.46, average training loss: 9600.35, base loss: 14346.13
[INFO 2017-06-28 13:56:41,878 main.py:51] epoch 6088, training loss: 10616.02, average training loss: 9601.05, base loss: 14346.66
[INFO 2017-06-28 13:56:42,549 main.py:51] epoch 6089, training loss: 8704.38, average training loss: 9600.29, base loss: 14345.77
[INFO 2017-06-28 13:56:43,199 main.py:51] epoch 6090, training loss: 9510.59, average training loss: 9601.15, base loss: 14346.76
[INFO 2017-06-28 13:56:43,853 main.py:51] epoch 6091, training loss: 9853.76, average training loss: 9599.38, base loss: 14343.99
[INFO 2017-06-28 13:56:44,521 main.py:51] epoch 6092, training loss: 9604.34, average training loss: 9599.73, base loss: 14343.22
[INFO 2017-06-28 13:56:45,178 main.py:51] epoch 6093, training loss: 9660.39, average training loss: 9599.89, base loss: 14345.07
[INFO 2017-06-28 13:56:45,825 main.py:51] epoch 6094, training loss: 9715.16, average training loss: 9597.68, base loss: 14342.75
[INFO 2017-06-28 13:56:46,470 main.py:51] epoch 6095, training loss: 9081.81, average training loss: 9597.70, base loss: 14342.80
[INFO 2017-06-28 13:56:47,142 main.py:51] epoch 6096, training loss: 8943.37, average training loss: 9596.58, base loss: 14341.18
[INFO 2017-06-28 13:56:47,786 main.py:51] epoch 6097, training loss: 9580.43, average training loss: 9597.93, base loss: 14344.81
[INFO 2017-06-28 13:56:48,446 main.py:51] epoch 6098, training loss: 11395.17, average training loss: 9597.91, base loss: 14344.80
[INFO 2017-06-28 13:56:49,094 main.py:51] epoch 6099, training loss: 9873.29, average training loss: 9599.29, base loss: 14346.09
[INFO 2017-06-28 13:56:49,094 main.py:53] epoch 6099, testing
[INFO 2017-06-28 13:56:51,689 main.py:105] average testing loss: 10738.14, base loss: 15374.28
[INFO 2017-06-28 13:56:51,689 main.py:106] improve_loss: 4636.14, improve_percent: 0.30
[INFO 2017-06-28 13:56:51,690 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:56:52,373 main.py:51] epoch 6100, training loss: 8081.22, average training loss: 9596.87, base loss: 14342.62
[INFO 2017-06-28 13:56:53,040 main.py:51] epoch 6101, training loss: 9259.94, average training loss: 9596.99, base loss: 14341.48
[INFO 2017-06-28 13:56:53,680 main.py:51] epoch 6102, training loss: 9229.12, average training loss: 9597.16, base loss: 14342.46
[INFO 2017-06-28 13:56:54,327 main.py:51] epoch 6103, training loss: 9489.22, average training loss: 9595.51, base loss: 14341.45
[INFO 2017-06-28 13:56:54,976 main.py:51] epoch 6104, training loss: 9423.43, average training loss: 9595.30, base loss: 14341.44
[INFO 2017-06-28 13:56:55,620 main.py:51] epoch 6105, training loss: 9445.25, average training loss: 9595.37, base loss: 14341.88
[INFO 2017-06-28 13:56:56,265 main.py:51] epoch 6106, training loss: 9342.35, average training loss: 9595.52, base loss: 14342.61
[INFO 2017-06-28 13:56:56,918 main.py:51] epoch 6107, training loss: 10532.04, average training loss: 9596.54, base loss: 14344.65
[INFO 2017-06-28 13:56:57,609 main.py:51] epoch 6108, training loss: 10037.28, average training loss: 9596.36, base loss: 14343.59
[INFO 2017-06-28 13:56:58,251 main.py:51] epoch 6109, training loss: 8055.56, average training loss: 9594.12, base loss: 14341.12
[INFO 2017-06-28 13:56:58,904 main.py:51] epoch 6110, training loss: 9683.45, average training loss: 9593.78, base loss: 14341.99
[INFO 2017-06-28 13:56:59,560 main.py:51] epoch 6111, training loss: 8997.14, average training loss: 9593.87, base loss: 14342.97
[INFO 2017-06-28 13:57:00,198 main.py:51] epoch 6112, training loss: 9375.12, average training loss: 9593.07, base loss: 14342.05
[INFO 2017-06-28 13:57:00,877 main.py:51] epoch 6113, training loss: 10269.01, average training loss: 9594.70, base loss: 14344.37
[INFO 2017-06-28 13:57:01,534 main.py:51] epoch 6114, training loss: 10299.54, average training loss: 9596.35, base loss: 14346.99
[INFO 2017-06-28 13:57:02,206 main.py:51] epoch 6115, training loss: 9749.48, average training loss: 9597.20, base loss: 14350.49
[INFO 2017-06-28 13:57:02,908 main.py:51] epoch 6116, training loss: 9120.37, average training loss: 9597.90, base loss: 14351.79
[INFO 2017-06-28 13:57:03,565 main.py:51] epoch 6117, training loss: 9391.35, average training loss: 9598.98, base loss: 14353.19
[INFO 2017-06-28 13:57:04,222 main.py:51] epoch 6118, training loss: 9616.91, average training loss: 9599.07, base loss: 14352.05
[INFO 2017-06-28 13:57:04,880 main.py:51] epoch 6119, training loss: 9880.52, average training loss: 9599.47, base loss: 14354.20
[INFO 2017-06-28 13:57:05,525 main.py:51] epoch 6120, training loss: 11508.56, average training loss: 9601.76, base loss: 14357.44
[INFO 2017-06-28 13:57:06,171 main.py:51] epoch 6121, training loss: 9311.79, average training loss: 9601.77, base loss: 14357.13
[INFO 2017-06-28 13:57:06,835 main.py:51] epoch 6122, training loss: 9632.10, average training loss: 9602.57, base loss: 14358.30
[INFO 2017-06-28 13:57:07,496 main.py:51] epoch 6123, training loss: 10364.45, average training loss: 9603.78, base loss: 14358.05
[INFO 2017-06-28 13:57:08,156 main.py:51] epoch 6124, training loss: 9239.30, average training loss: 9602.38, base loss: 14355.73
[INFO 2017-06-28 13:57:08,826 main.py:51] epoch 6125, training loss: 9238.87, average training loss: 9601.36, base loss: 14352.66
[INFO 2017-06-28 13:57:09,464 main.py:51] epoch 6126, training loss: 9143.48, average training loss: 9601.20, base loss: 14352.11
[INFO 2017-06-28 13:57:10,110 main.py:51] epoch 6127, training loss: 8654.41, average training loss: 9599.55, base loss: 14349.91
[INFO 2017-06-28 13:57:10,755 main.py:51] epoch 6128, training loss: 8907.71, average training loss: 9598.75, base loss: 14348.96
[INFO 2017-06-28 13:57:11,410 main.py:51] epoch 6129, training loss: 9584.90, average training loss: 9599.66, base loss: 14350.38
[INFO 2017-06-28 13:57:12,052 main.py:51] epoch 6130, training loss: 8692.19, average training loss: 9599.04, base loss: 14350.01
[INFO 2017-06-28 13:57:12,696 main.py:51] epoch 6131, training loss: 8219.52, average training loss: 9597.63, base loss: 14348.66
[INFO 2017-06-28 13:57:13,344 main.py:51] epoch 6132, training loss: 8020.11, average training loss: 9594.75, base loss: 14343.79
[INFO 2017-06-28 13:57:13,985 main.py:51] epoch 6133, training loss: 9025.29, average training loss: 9594.15, base loss: 14342.57
[INFO 2017-06-28 13:57:14,647 main.py:51] epoch 6134, training loss: 9265.99, average training loss: 9593.57, base loss: 14341.50
[INFO 2017-06-28 13:57:15,312 main.py:51] epoch 6135, training loss: 10129.94, average training loss: 9594.33, base loss: 14342.81
[INFO 2017-06-28 13:57:15,949 main.py:51] epoch 6136, training loss: 10560.08, average training loss: 9596.19, base loss: 14345.78
[INFO 2017-06-28 13:57:16,613 main.py:51] epoch 6137, training loss: 9752.76, average training loss: 9596.57, base loss: 14345.19
[INFO 2017-06-28 13:57:17,258 main.py:51] epoch 6138, training loss: 11405.76, average training loss: 9599.26, base loss: 14348.80
[INFO 2017-06-28 13:57:17,930 main.py:51] epoch 6139, training loss: 10048.48, average training loss: 9597.78, base loss: 14346.86
[INFO 2017-06-28 13:57:18,577 main.py:51] epoch 6140, training loss: 10601.06, average training loss: 9599.05, base loss: 14349.26
[INFO 2017-06-28 13:57:19,234 main.py:51] epoch 6141, training loss: 10226.39, average training loss: 9600.28, base loss: 14351.17
[INFO 2017-06-28 13:57:19,887 main.py:51] epoch 6142, training loss: 9652.19, average training loss: 9601.85, base loss: 14353.50
[INFO 2017-06-28 13:57:20,535 main.py:51] epoch 6143, training loss: 9078.67, average training loss: 9602.03, base loss: 14353.83
[INFO 2017-06-28 13:57:21,182 main.py:51] epoch 6144, training loss: 8677.73, average training loss: 9602.30, base loss: 14353.77
[INFO 2017-06-28 13:57:21,830 main.py:51] epoch 6145, training loss: 9864.13, average training loss: 9602.20, base loss: 14354.57
[INFO 2017-06-28 13:57:22,508 main.py:51] epoch 6146, training loss: 11041.16, average training loss: 9601.05, base loss: 14352.04
[INFO 2017-06-28 13:57:23,162 main.py:51] epoch 6147, training loss: 9230.82, average training loss: 9600.21, base loss: 14351.25
[INFO 2017-06-28 13:57:23,826 main.py:51] epoch 6148, training loss: 8988.04, average training loss: 9600.27, base loss: 14352.26
[INFO 2017-06-28 13:57:24,485 main.py:51] epoch 6149, training loss: 9839.03, average training loss: 9600.36, base loss: 14352.33
[INFO 2017-06-28 13:57:25,126 main.py:51] epoch 6150, training loss: 10565.65, average training loss: 9601.89, base loss: 14353.26
[INFO 2017-06-28 13:57:25,781 main.py:51] epoch 6151, training loss: 8605.15, average training loss: 9601.14, base loss: 14351.43
[INFO 2017-06-28 13:57:26,434 main.py:51] epoch 6152, training loss: 8859.86, average training loss: 9599.97, base loss: 14350.87
[INFO 2017-06-28 13:57:27,096 main.py:51] epoch 6153, training loss: 8728.15, average training loss: 9599.65, base loss: 14350.65
[INFO 2017-06-28 13:57:27,742 main.py:51] epoch 6154, training loss: 8825.74, average training loss: 9598.55, base loss: 14349.45
[INFO 2017-06-28 13:57:28,411 main.py:51] epoch 6155, training loss: 9848.03, average training loss: 9599.16, base loss: 14349.54
[INFO 2017-06-28 13:57:29,052 main.py:51] epoch 6156, training loss: 9603.58, average training loss: 9600.60, base loss: 14352.42
[INFO 2017-06-28 13:57:29,734 main.py:51] epoch 6157, training loss: 8644.59, average training loss: 9598.81, base loss: 14349.38
[INFO 2017-06-28 13:57:30,420 main.py:51] epoch 6158, training loss: 9945.58, average training loss: 9597.55, base loss: 14348.50
[INFO 2017-06-28 13:57:31,094 main.py:51] epoch 6159, training loss: 7655.39, average training loss: 9594.93, base loss: 14346.80
[INFO 2017-06-28 13:57:31,747 main.py:51] epoch 6160, training loss: 9573.33, average training loss: 9594.52, base loss: 14347.49
[INFO 2017-06-28 13:57:32,387 main.py:51] epoch 6161, training loss: 9053.72, average training loss: 9594.95, base loss: 14347.69
[INFO 2017-06-28 13:57:33,025 main.py:51] epoch 6162, training loss: 10600.14, average training loss: 9597.05, base loss: 14351.65
[INFO 2017-06-28 13:57:33,699 main.py:51] epoch 6163, training loss: 12009.32, average training loss: 9600.69, base loss: 14357.34
[INFO 2017-06-28 13:57:34,368 main.py:51] epoch 6164, training loss: 9188.75, average training loss: 9600.06, base loss: 14356.99
[INFO 2017-06-28 13:57:35,038 main.py:51] epoch 6165, training loss: 9293.12, average training loss: 9600.66, base loss: 14359.20
[INFO 2017-06-28 13:57:35,675 main.py:51] epoch 6166, training loss: 8214.08, average training loss: 9598.72, base loss: 14356.79
[INFO 2017-06-28 13:57:36,306 main.py:51] epoch 6167, training loss: 11605.32, average training loss: 9600.54, base loss: 14358.41
[INFO 2017-06-28 13:57:36,963 main.py:51] epoch 6168, training loss: 9362.25, average training loss: 9601.08, base loss: 14358.81
[INFO 2017-06-28 13:57:37,623 main.py:51] epoch 6169, training loss: 9001.06, average training loss: 9600.99, base loss: 14358.07
[INFO 2017-06-28 13:57:38,280 main.py:51] epoch 6170, training loss: 10613.21, average training loss: 9601.43, base loss: 14359.21
[INFO 2017-06-28 13:57:38,922 main.py:51] epoch 6171, training loss: 8984.39, average training loss: 9599.87, base loss: 14356.67
[INFO 2017-06-28 13:57:39,575 main.py:51] epoch 6172, training loss: 9930.73, average training loss: 9600.25, base loss: 14355.21
[INFO 2017-06-28 13:57:40,245 main.py:51] epoch 6173, training loss: 9208.30, average training loss: 9600.76, base loss: 14354.51
[INFO 2017-06-28 13:57:40,901 main.py:51] epoch 6174, training loss: 9861.08, average training loss: 9600.40, base loss: 14354.06
[INFO 2017-06-28 13:57:41,545 main.py:51] epoch 6175, training loss: 9604.91, average training loss: 9600.02, base loss: 14352.53
[INFO 2017-06-28 13:57:42,197 main.py:51] epoch 6176, training loss: 10356.17, average training loss: 9599.47, base loss: 14352.41
[INFO 2017-06-28 13:57:42,842 main.py:51] epoch 6177, training loss: 9918.68, average training loss: 9600.00, base loss: 14351.55
[INFO 2017-06-28 13:57:43,505 main.py:51] epoch 6178, training loss: 10057.66, average training loss: 9599.60, base loss: 14352.81
[INFO 2017-06-28 13:57:44,145 main.py:51] epoch 6179, training loss: 10334.75, average training loss: 9599.80, base loss: 14351.88
[INFO 2017-06-28 13:57:44,813 main.py:51] epoch 6180, training loss: 8843.56, average training loss: 9599.95, base loss: 14351.34
[INFO 2017-06-28 13:57:45,476 main.py:51] epoch 6181, training loss: 9512.84, average training loss: 9599.84, base loss: 14351.85
[INFO 2017-06-28 13:57:46,142 main.py:51] epoch 6182, training loss: 8687.88, average training loss: 9599.96, base loss: 14350.48
[INFO 2017-06-28 13:57:46,796 main.py:51] epoch 6183, training loss: 11180.05, average training loss: 9602.16, base loss: 14353.55
[INFO 2017-06-28 13:57:47,440 main.py:51] epoch 6184, training loss: 10869.27, average training loss: 9605.19, base loss: 14359.33
[INFO 2017-06-28 13:57:48,108 main.py:51] epoch 6185, training loss: 9648.75, average training loss: 9605.41, base loss: 14358.54
[INFO 2017-06-28 13:57:48,749 main.py:51] epoch 6186, training loss: 8893.05, average training loss: 9605.32, base loss: 14359.68
[INFO 2017-06-28 13:57:49,416 main.py:51] epoch 6187, training loss: 8795.55, average training loss: 9603.96, base loss: 14357.08
[INFO 2017-06-28 13:57:50,069 main.py:51] epoch 6188, training loss: 10201.91, average training loss: 9603.28, base loss: 14356.00
[INFO 2017-06-28 13:57:50,745 main.py:51] epoch 6189, training loss: 10393.50, average training loss: 9603.83, base loss: 14356.03
[INFO 2017-06-28 13:57:51,401 main.py:51] epoch 6190, training loss: 9834.58, average training loss: 9605.44, base loss: 14358.83
[INFO 2017-06-28 13:57:52,066 main.py:51] epoch 6191, training loss: 7479.27, average training loss: 9603.03, base loss: 14356.78
[INFO 2017-06-28 13:57:52,707 main.py:51] epoch 6192, training loss: 9178.74, average training loss: 9601.53, base loss: 14355.31
[INFO 2017-06-28 13:57:53,352 main.py:51] epoch 6193, training loss: 7965.48, average training loss: 9599.65, base loss: 14353.81
[INFO 2017-06-28 13:57:54,021 main.py:51] epoch 6194, training loss: 10213.47, average training loss: 9600.35, base loss: 14355.20
[INFO 2017-06-28 13:57:54,659 main.py:51] epoch 6195, training loss: 9838.67, average training loss: 9601.08, base loss: 14356.87
[INFO 2017-06-28 13:57:55,322 main.py:51] epoch 6196, training loss: 10091.10, average training loss: 9600.54, base loss: 14357.46
[INFO 2017-06-28 13:57:55,981 main.py:51] epoch 6197, training loss: 9197.11, average training loss: 9598.64, base loss: 14354.89
[INFO 2017-06-28 13:57:56,662 main.py:51] epoch 6198, training loss: 8470.98, average training loss: 9596.16, base loss: 14350.76
[INFO 2017-06-28 13:57:57,302 main.py:51] epoch 6199, training loss: 8318.20, average training loss: 9595.63, base loss: 14349.39
[INFO 2017-06-28 13:57:57,302 main.py:53] epoch 6199, testing
[INFO 2017-06-28 13:57:59,878 main.py:105] average testing loss: 11715.29, base loss: 16416.19
[INFO 2017-06-28 13:57:59,878 main.py:106] improve_loss: 4700.90, improve_percent: 0.29
[INFO 2017-06-28 13:57:59,879 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:58:00,539 main.py:51] epoch 6200, training loss: 8817.55, average training loss: 9595.52, base loss: 14350.70
[INFO 2017-06-28 13:58:01,206 main.py:51] epoch 6201, training loss: 10324.07, average training loss: 9595.45, base loss: 14350.74
[INFO 2017-06-28 13:58:01,861 main.py:51] epoch 6202, training loss: 9950.47, average training loss: 9595.70, base loss: 14351.07
[INFO 2017-06-28 13:58:02,510 main.py:51] epoch 6203, training loss: 10039.25, average training loss: 9597.12, base loss: 14353.68
[INFO 2017-06-28 13:58:03,169 main.py:51] epoch 6204, training loss: 9056.02, average training loss: 9596.21, base loss: 14352.35
[INFO 2017-06-28 13:58:03,809 main.py:51] epoch 6205, training loss: 9074.15, average training loss: 9596.07, base loss: 14353.44
[INFO 2017-06-28 13:58:04,443 main.py:51] epoch 6206, training loss: 11162.12, average training loss: 9597.11, base loss: 14355.92
[INFO 2017-06-28 13:58:05,117 main.py:51] epoch 6207, training loss: 8950.25, average training loss: 9597.01, base loss: 14355.58
[INFO 2017-06-28 13:58:05,782 main.py:51] epoch 6208, training loss: 10505.33, average training loss: 9597.89, base loss: 14358.27
[INFO 2017-06-28 13:58:06,452 main.py:51] epoch 6209, training loss: 9544.89, average training loss: 9598.06, base loss: 14357.28
[INFO 2017-06-28 13:58:07,128 main.py:51] epoch 6210, training loss: 9762.60, average training loss: 9597.62, base loss: 14357.44
[INFO 2017-06-28 13:58:07,794 main.py:51] epoch 6211, training loss: 10401.58, average training loss: 9598.14, base loss: 14359.14
[INFO 2017-06-28 13:58:08,451 main.py:51] epoch 6212, training loss: 8542.47, average training loss: 9598.67, base loss: 14360.27
[INFO 2017-06-28 13:58:09,093 main.py:51] epoch 6213, training loss: 8304.10, average training loss: 9597.25, base loss: 14358.23
[INFO 2017-06-28 13:58:09,740 main.py:51] epoch 6214, training loss: 8940.96, average training loss: 9594.54, base loss: 14354.73
[INFO 2017-06-28 13:58:10,417 main.py:51] epoch 6215, training loss: 10100.70, average training loss: 9594.78, base loss: 14356.57
[INFO 2017-06-28 13:58:11,095 main.py:51] epoch 6216, training loss: 9705.22, average training loss: 9594.32, base loss: 14355.60
[INFO 2017-06-28 13:58:11,753 main.py:51] epoch 6217, training loss: 8909.11, average training loss: 9592.32, base loss: 14352.81
[INFO 2017-06-28 13:58:12,390 main.py:51] epoch 6218, training loss: 9909.36, average training loss: 9591.84, base loss: 14354.06
[INFO 2017-06-28 13:58:13,059 main.py:51] epoch 6219, training loss: 9226.13, average training loss: 9591.71, base loss: 14351.66
[INFO 2017-06-28 13:58:13,718 main.py:51] epoch 6220, training loss: 10203.25, average training loss: 9592.52, base loss: 14352.88
[INFO 2017-06-28 13:58:14,378 main.py:51] epoch 6221, training loss: 11551.59, average training loss: 9594.50, base loss: 14356.40
[INFO 2017-06-28 13:58:15,008 main.py:51] epoch 6222, training loss: 10763.22, average training loss: 9595.33, base loss: 14359.57
[INFO 2017-06-28 13:58:15,664 main.py:51] epoch 6223, training loss: 8178.82, average training loss: 9594.26, base loss: 14357.04
[INFO 2017-06-28 13:58:16,313 main.py:51] epoch 6224, training loss: 9512.90, average training loss: 9594.98, base loss: 14357.49
[INFO 2017-06-28 13:58:16,987 main.py:51] epoch 6225, training loss: 9715.92, average training loss: 9595.39, base loss: 14360.36
[INFO 2017-06-28 13:58:17,631 main.py:51] epoch 6226, training loss: 9768.83, average training loss: 9596.07, base loss: 14363.23
[INFO 2017-06-28 13:58:18,284 main.py:51] epoch 6227, training loss: 8925.52, average training loss: 9596.15, base loss: 14363.95
[INFO 2017-06-28 13:58:18,926 main.py:51] epoch 6228, training loss: 10507.29, average training loss: 9597.09, base loss: 14364.92
[INFO 2017-06-28 13:58:19,585 main.py:51] epoch 6229, training loss: 8601.85, average training loss: 9595.41, base loss: 14361.80
[INFO 2017-06-28 13:58:20,258 main.py:51] epoch 6230, training loss: 9978.97, average training loss: 9596.12, base loss: 14362.49
[INFO 2017-06-28 13:58:20,905 main.py:51] epoch 6231, training loss: 9154.10, average training loss: 9596.96, base loss: 14364.52
[INFO 2017-06-28 13:58:21,556 main.py:51] epoch 6232, training loss: 9384.76, average training loss: 9594.94, base loss: 14362.09
[INFO 2017-06-28 13:58:22,192 main.py:51] epoch 6233, training loss: 10410.19, average training loss: 9596.02, base loss: 14364.32
[INFO 2017-06-28 13:58:22,843 main.py:51] epoch 6234, training loss: 10277.06, average training loss: 9596.50, base loss: 14365.41
[INFO 2017-06-28 13:58:23,478 main.py:51] epoch 6235, training loss: 9291.25, average training loss: 9596.61, base loss: 14367.09
[INFO 2017-06-28 13:58:24,163 main.py:51] epoch 6236, training loss: 10039.88, average training loss: 9595.89, base loss: 14367.67
[INFO 2017-06-28 13:58:24,809 main.py:51] epoch 6237, training loss: 8818.80, average training loss: 9594.22, base loss: 14365.20
[INFO 2017-06-28 13:58:25,481 main.py:51] epoch 6238, training loss: 9825.82, average training loss: 9594.99, base loss: 14366.33
[INFO 2017-06-28 13:58:26,133 main.py:51] epoch 6239, training loss: 11150.07, average training loss: 9596.97, base loss: 14369.46
[INFO 2017-06-28 13:58:26,782 main.py:51] epoch 6240, training loss: 8846.10, average training loss: 9596.18, base loss: 14367.26
[INFO 2017-06-28 13:58:27,444 main.py:51] epoch 6241, training loss: 8813.61, average training loss: 9594.68, base loss: 14365.95
[INFO 2017-06-28 13:58:28,119 main.py:51] epoch 6242, training loss: 9823.76, average training loss: 9594.26, base loss: 14366.01
[INFO 2017-06-28 13:58:28,779 main.py:51] epoch 6243, training loss: 9200.06, average training loss: 9594.29, base loss: 14366.09
[INFO 2017-06-28 13:58:29,427 main.py:51] epoch 6244, training loss: 8439.37, average training loss: 9593.55, base loss: 14364.11
[INFO 2017-06-28 13:58:30,095 main.py:51] epoch 6245, training loss: 9121.23, average training loss: 9593.08, base loss: 14363.82
[INFO 2017-06-28 13:58:30,743 main.py:51] epoch 6246, training loss: 9794.74, average training loss: 9593.69, base loss: 14366.15
[INFO 2017-06-28 13:58:31,407 main.py:51] epoch 6247, training loss: 9117.12, average training loss: 9593.93, base loss: 14366.94
[INFO 2017-06-28 13:58:32,050 main.py:51] epoch 6248, training loss: 8441.16, average training loss: 9592.69, base loss: 14366.50
[INFO 2017-06-28 13:58:32,714 main.py:51] epoch 6249, training loss: 9380.21, average training loss: 9592.07, base loss: 14366.57
[INFO 2017-06-28 13:58:33,366 main.py:51] epoch 6250, training loss: 9394.87, average training loss: 9592.89, base loss: 14367.38
[INFO 2017-06-28 13:58:34,035 main.py:51] epoch 6251, training loss: 9535.51, average training loss: 9593.56, base loss: 14368.45
[INFO 2017-06-28 13:58:34,687 main.py:51] epoch 6252, training loss: 9349.74, average training loss: 9593.58, base loss: 14369.70
[INFO 2017-06-28 13:58:35,371 main.py:51] epoch 6253, training loss: 9096.47, average training loss: 9593.48, base loss: 14369.80
[INFO 2017-06-28 13:58:36,017 main.py:51] epoch 6254, training loss: 10426.56, average training loss: 9594.03, base loss: 14371.82
[INFO 2017-06-28 13:58:36,658 main.py:51] epoch 6255, training loss: 8960.08, average training loss: 9592.80, base loss: 14368.78
[INFO 2017-06-28 13:58:37,304 main.py:51] epoch 6256, training loss: 9371.46, average training loss: 9592.91, base loss: 14370.29
[INFO 2017-06-28 13:58:37,946 main.py:51] epoch 6257, training loss: 11382.93, average training loss: 9595.15, base loss: 14373.57
[INFO 2017-06-28 13:58:38,595 main.py:51] epoch 6258, training loss: 8858.24, average training loss: 9594.00, base loss: 14374.07
[INFO 2017-06-28 13:58:39,257 main.py:51] epoch 6259, training loss: 9045.41, average training loss: 9593.47, base loss: 14374.57
[INFO 2017-06-28 13:58:39,914 main.py:51] epoch 6260, training loss: 9079.26, average training loss: 9593.47, base loss: 14373.03
[INFO 2017-06-28 13:58:40,583 main.py:51] epoch 6261, training loss: 8837.67, average training loss: 9593.42, base loss: 14373.15
[INFO 2017-06-28 13:58:41,248 main.py:51] epoch 6262, training loss: 10375.56, average training loss: 9594.14, base loss: 14373.16
[INFO 2017-06-28 13:58:41,911 main.py:51] epoch 6263, training loss: 11041.44, average training loss: 9595.32, base loss: 14375.29
[INFO 2017-06-28 13:58:42,569 main.py:51] epoch 6264, training loss: 9053.77, average training loss: 9594.26, base loss: 14373.83
[INFO 2017-06-28 13:58:43,231 main.py:51] epoch 6265, training loss: 10407.46, average training loss: 9595.68, base loss: 14375.90
[INFO 2017-06-28 13:58:43,887 main.py:51] epoch 6266, training loss: 9378.15, average training loss: 9595.76, base loss: 14376.56
[INFO 2017-06-28 13:58:44,547 main.py:51] epoch 6267, training loss: 8828.67, average training loss: 9595.72, base loss: 14377.59
[INFO 2017-06-28 13:58:45,196 main.py:51] epoch 6268, training loss: 9161.10, average training loss: 9596.55, base loss: 14377.10
[INFO 2017-06-28 13:58:45,852 main.py:51] epoch 6269, training loss: 9859.59, average training loss: 9595.41, base loss: 14375.45
[INFO 2017-06-28 13:58:46,473 main.py:51] epoch 6270, training loss: 10651.01, average training loss: 9596.21, base loss: 14376.92
[INFO 2017-06-28 13:58:47,124 main.py:51] epoch 6271, training loss: 8992.95, average training loss: 9594.13, base loss: 14373.91
[INFO 2017-06-28 13:58:47,845 main.py:51] epoch 6272, training loss: 9541.38, average training loss: 9593.69, base loss: 14373.51
[INFO 2017-06-28 13:58:48,496 main.py:51] epoch 6273, training loss: 8770.76, average training loss: 9594.04, base loss: 14373.80
[INFO 2017-06-28 13:58:49,150 main.py:51] epoch 6274, training loss: 10961.35, average training loss: 9595.98, base loss: 14375.89
[INFO 2017-06-28 13:58:49,802 main.py:51] epoch 6275, training loss: 9082.90, average training loss: 9596.02, base loss: 14375.10
[INFO 2017-06-28 13:58:50,458 main.py:51] epoch 6276, training loss: 10175.40, average training loss: 9596.54, base loss: 14375.62
[INFO 2017-06-28 13:58:51,102 main.py:51] epoch 6277, training loss: 8908.98, average training loss: 9596.12, base loss: 14375.36
[INFO 2017-06-28 13:58:51,734 main.py:51] epoch 6278, training loss: 8829.81, average training loss: 9596.03, base loss: 14374.90
[INFO 2017-06-28 13:58:52,389 main.py:51] epoch 6279, training loss: 10104.84, average training loss: 9596.36, base loss: 14375.80
[INFO 2017-06-28 13:58:53,028 main.py:51] epoch 6280, training loss: 9674.17, average training loss: 9595.61, base loss: 14375.69
[INFO 2017-06-28 13:58:53,674 main.py:51] epoch 6281, training loss: 8988.50, average training loss: 9596.52, base loss: 14376.77
[INFO 2017-06-28 13:58:54,332 main.py:51] epoch 6282, training loss: 9254.72, average training loss: 9594.53, base loss: 14373.17
[INFO 2017-06-28 13:58:55,002 main.py:51] epoch 6283, training loss: 10099.41, average training loss: 9595.82, base loss: 14375.36
[INFO 2017-06-28 13:58:55,655 main.py:51] epoch 6284, training loss: 10316.19, average training loss: 9596.04, base loss: 14374.53
[INFO 2017-06-28 13:58:56,321 main.py:51] epoch 6285, training loss: 10942.22, average training loss: 9598.26, base loss: 14379.25
[INFO 2017-06-28 13:58:56,982 main.py:51] epoch 6286, training loss: 10702.84, average training loss: 9599.57, base loss: 14381.45
[INFO 2017-06-28 13:58:57,646 main.py:51] epoch 6287, training loss: 9245.24, average training loss: 9599.89, base loss: 14385.00
[INFO 2017-06-28 13:58:58,321 main.py:51] epoch 6288, training loss: 9192.54, average training loss: 9600.35, base loss: 14385.39
[INFO 2017-06-28 13:58:58,959 main.py:51] epoch 6289, training loss: 8821.18, average training loss: 9598.61, base loss: 14383.23
[INFO 2017-06-28 13:58:59,604 main.py:51] epoch 6290, training loss: 8563.27, average training loss: 9597.01, base loss: 14380.31
[INFO 2017-06-28 13:59:00,280 main.py:51] epoch 6291, training loss: 9790.55, average training loss: 9597.04, base loss: 14382.35
[INFO 2017-06-28 13:59:00,912 main.py:51] epoch 6292, training loss: 8140.28, average training loss: 9595.80, base loss: 14381.41
[INFO 2017-06-28 13:59:01,592 main.py:51] epoch 6293, training loss: 8484.63, average training loss: 9594.59, base loss: 14379.06
[INFO 2017-06-28 13:59:02,240 main.py:51] epoch 6294, training loss: 8689.56, average training loss: 9594.66, base loss: 14379.09
[INFO 2017-06-28 13:59:02,891 main.py:51] epoch 6295, training loss: 11418.07, average training loss: 9596.79, base loss: 14381.69
[INFO 2017-06-28 13:59:03,547 main.py:51] epoch 6296, training loss: 9480.79, average training loss: 9596.80, base loss: 14380.76
[INFO 2017-06-28 13:59:04,209 main.py:51] epoch 6297, training loss: 8558.13, average training loss: 9595.45, base loss: 14376.45
[INFO 2017-06-28 13:59:04,851 main.py:51] epoch 6298, training loss: 9290.24, average training loss: 9594.97, base loss: 14376.12
[INFO 2017-06-28 13:59:05,497 main.py:51] epoch 6299, training loss: 9277.51, average training loss: 9594.75, base loss: 14374.79
[INFO 2017-06-28 13:59:05,497 main.py:53] epoch 6299, testing
[INFO 2017-06-28 13:59:08,052 main.py:105] average testing loss: 10623.23, base loss: 15241.44
[INFO 2017-06-28 13:59:08,052 main.py:106] improve_loss: 4618.21, improve_percent: 0.30
[INFO 2017-06-28 13:59:08,053 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 13:59:08,692 main.py:51] epoch 6300, training loss: 8280.73, average training loss: 9594.42, base loss: 14374.90
[INFO 2017-06-28 13:59:09,336 main.py:51] epoch 6301, training loss: 10177.04, average training loss: 9595.46, base loss: 14376.93
[INFO 2017-06-28 13:59:10,007 main.py:51] epoch 6302, training loss: 10059.07, average training loss: 9595.81, base loss: 14377.76
[INFO 2017-06-28 13:59:10,678 main.py:51] epoch 6303, training loss: 9557.69, average training loss: 9595.71, base loss: 14376.92
[INFO 2017-06-28 13:59:11,323 main.py:51] epoch 6304, training loss: 9434.79, average training loss: 9595.60, base loss: 14377.92
[INFO 2017-06-28 13:59:11,960 main.py:51] epoch 6305, training loss: 8946.98, average training loss: 9594.35, base loss: 14376.52
[INFO 2017-06-28 13:59:12,652 main.py:51] epoch 6306, training loss: 9735.11, average training loss: 9594.61, base loss: 14376.07
[INFO 2017-06-28 13:59:13,326 main.py:51] epoch 6307, training loss: 10056.86, average training loss: 9595.27, base loss: 14378.27
[INFO 2017-06-28 13:59:13,976 main.py:51] epoch 6308, training loss: 10653.30, average training loss: 9595.73, base loss: 14379.39
[INFO 2017-06-28 13:59:14,610 main.py:51] epoch 6309, training loss: 9186.26, average training loss: 9595.35, base loss: 14379.08
[INFO 2017-06-28 13:59:15,272 main.py:51] epoch 6310, training loss: 8894.52, average training loss: 9595.57, base loss: 14377.89
[INFO 2017-06-28 13:59:15,911 main.py:51] epoch 6311, training loss: 9377.56, average training loss: 9595.49, base loss: 14378.93
[INFO 2017-06-28 13:59:16,551 main.py:51] epoch 6312, training loss: 10724.99, average training loss: 9596.73, base loss: 14380.41
[INFO 2017-06-28 13:59:17,196 main.py:51] epoch 6313, training loss: 8823.98, average training loss: 9595.74, base loss: 14378.42
[INFO 2017-06-28 13:59:17,865 main.py:51] epoch 6314, training loss: 9847.54, average training loss: 9595.43, base loss: 14378.74
[INFO 2017-06-28 13:59:18,527 main.py:51] epoch 6315, training loss: 9909.71, average training loss: 9596.72, base loss: 14381.66
[INFO 2017-06-28 13:59:19,169 main.py:51] epoch 6316, training loss: 7802.86, average training loss: 9593.33, base loss: 14376.18
[INFO 2017-06-28 13:59:19,822 main.py:51] epoch 6317, training loss: 9664.29, average training loss: 9593.96, base loss: 14376.95
[INFO 2017-06-28 13:59:20,512 main.py:51] epoch 6318, training loss: 8884.99, average training loss: 9592.89, base loss: 14376.70
[INFO 2017-06-28 13:59:21,189 main.py:51] epoch 6319, training loss: 8740.42, average training loss: 9592.64, base loss: 14376.21
[INFO 2017-06-28 13:59:21,835 main.py:51] epoch 6320, training loss: 9825.63, average training loss: 9592.91, base loss: 14376.49
[INFO 2017-06-28 13:59:22,468 main.py:51] epoch 6321, training loss: 8945.82, average training loss: 9592.38, base loss: 14375.63
[INFO 2017-06-28 13:59:23,112 main.py:51] epoch 6322, training loss: 9562.63, average training loss: 9592.94, base loss: 14376.72
[INFO 2017-06-28 13:59:23,775 main.py:51] epoch 6323, training loss: 10069.34, average training loss: 9593.09, base loss: 14376.44
[INFO 2017-06-28 13:59:24,418 main.py:51] epoch 6324, training loss: 8942.74, average training loss: 9591.32, base loss: 14375.00
[INFO 2017-06-28 13:59:25,070 main.py:51] epoch 6325, training loss: 7527.73, average training loss: 9589.17, base loss: 14372.01
[INFO 2017-06-28 13:59:25,722 main.py:51] epoch 6326, training loss: 9127.32, average training loss: 9589.67, base loss: 14373.36
[INFO 2017-06-28 13:59:26,399 main.py:51] epoch 6327, training loss: 9474.24, average training loss: 9589.87, base loss: 14374.44
[INFO 2017-06-28 13:59:27,082 main.py:51] epoch 6328, training loss: 9362.11, average training loss: 9588.24, base loss: 14372.77
[INFO 2017-06-28 13:59:27,738 main.py:51] epoch 6329, training loss: 8536.65, average training loss: 9587.29, base loss: 14370.61
[INFO 2017-06-28 13:59:28,395 main.py:51] epoch 6330, training loss: 8472.50, average training loss: 9586.26, base loss: 14368.82
[INFO 2017-06-28 13:59:29,064 main.py:51] epoch 6331, training loss: 9408.67, average training loss: 9583.98, base loss: 14364.56
[INFO 2017-06-28 13:59:29,740 main.py:51] epoch 6332, training loss: 11173.42, average training loss: 9584.70, base loss: 14365.04
[INFO 2017-06-28 13:59:30,407 main.py:51] epoch 6333, training loss: 9315.61, average training loss: 9585.47, base loss: 14367.46
[INFO 2017-06-28 13:59:31,060 main.py:51] epoch 6334, training loss: 10636.62, average training loss: 9587.10, base loss: 14370.17
[INFO 2017-06-28 13:59:31,714 main.py:51] epoch 6335, training loss: 9787.10, average training loss: 9587.43, base loss: 14370.20
[INFO 2017-06-28 13:59:32,403 main.py:51] epoch 6336, training loss: 8107.11, average training loss: 9585.31, base loss: 14369.18
[INFO 2017-06-28 13:59:33,050 main.py:51] epoch 6337, training loss: 9910.72, average training loss: 9586.44, base loss: 14371.83
[INFO 2017-06-28 13:59:33,690 main.py:51] epoch 6338, training loss: 9590.19, average training loss: 9587.37, base loss: 14373.67
[INFO 2017-06-28 13:59:34,353 main.py:51] epoch 6339, training loss: 9519.10, average training loss: 9588.66, base loss: 14377.01
[INFO 2017-06-28 13:59:35,010 main.py:51] epoch 6340, training loss: 9630.43, average training loss: 9588.25, base loss: 14376.96
[INFO 2017-06-28 13:59:35,656 main.py:51] epoch 6341, training loss: 8632.20, average training loss: 9587.63, base loss: 14376.55
[INFO 2017-06-28 13:59:36,320 main.py:51] epoch 6342, training loss: 9146.60, average training loss: 9586.56, base loss: 14374.41
[INFO 2017-06-28 13:59:36,967 main.py:51] epoch 6343, training loss: 9984.55, average training loss: 9587.47, base loss: 14375.84
[INFO 2017-06-28 13:59:37,612 main.py:51] epoch 6344, training loss: 9633.48, average training loss: 9588.19, base loss: 14376.23
[INFO 2017-06-28 13:59:38,290 main.py:51] epoch 6345, training loss: 9513.61, average training loss: 9589.76, base loss: 14379.30
[INFO 2017-06-28 13:59:38,942 main.py:51] epoch 6346, training loss: 8048.98, average training loss: 9589.45, base loss: 14378.79
[INFO 2017-06-28 13:59:39,589 main.py:51] epoch 6347, training loss: 9093.91, average training loss: 9587.97, base loss: 14375.96
[INFO 2017-06-28 13:59:40,236 main.py:51] epoch 6348, training loss: 9104.14, average training loss: 9587.20, base loss: 14373.94
[INFO 2017-06-28 13:59:40,885 main.py:51] epoch 6349, training loss: 11504.37, average training loss: 9588.45, base loss: 14374.69
[INFO 2017-06-28 13:59:41,555 main.py:51] epoch 6350, training loss: 9017.46, average training loss: 9588.69, base loss: 14374.54
[INFO 2017-06-28 13:59:42,232 main.py:51] epoch 6351, training loss: 10715.68, average training loss: 9588.89, base loss: 14375.22
[INFO 2017-06-28 13:59:42,884 main.py:51] epoch 6352, training loss: 9279.30, average training loss: 9588.96, base loss: 14376.02
[INFO 2017-06-28 13:59:43,566 main.py:51] epoch 6353, training loss: 10711.23, average training loss: 9588.94, base loss: 14375.39
[INFO 2017-06-28 13:59:44,206 main.py:51] epoch 6354, training loss: 9734.54, average training loss: 9588.47, base loss: 14376.23
[INFO 2017-06-28 13:59:44,862 main.py:51] epoch 6355, training loss: 9127.76, average training loss: 9585.85, base loss: 14374.03
[INFO 2017-06-28 13:59:45,533 main.py:51] epoch 6356, training loss: 8673.11, average training loss: 9585.63, base loss: 14372.48
[INFO 2017-06-28 13:59:46,184 main.py:51] epoch 6357, training loss: 10387.02, average training loss: 9586.97, base loss: 14374.36
[INFO 2017-06-28 13:59:46,840 main.py:51] epoch 6358, training loss: 9433.18, average training loss: 9587.96, base loss: 14377.82
[INFO 2017-06-28 13:59:47,515 main.py:51] epoch 6359, training loss: 10641.29, average training loss: 9590.18, base loss: 14380.84
[INFO 2017-06-28 13:59:48,184 main.py:51] epoch 6360, training loss: 10000.61, average training loss: 9590.76, base loss: 14382.64
[INFO 2017-06-28 13:59:48,848 main.py:51] epoch 6361, training loss: 9125.31, average training loss: 9590.45, base loss: 14382.41
[INFO 2017-06-28 13:59:49,518 main.py:51] epoch 6362, training loss: 8753.58, average training loss: 9589.86, base loss: 14381.13
[INFO 2017-06-28 13:59:50,180 main.py:51] epoch 6363, training loss: 8388.63, average training loss: 9589.27, base loss: 14380.60
[INFO 2017-06-28 13:59:50,837 main.py:51] epoch 6364, training loss: 9843.49, average training loss: 9589.29, base loss: 14381.06
[INFO 2017-06-28 13:59:51,492 main.py:51] epoch 6365, training loss: 8532.80, average training loss: 9588.28, base loss: 14379.61
[INFO 2017-06-28 13:59:52,151 main.py:51] epoch 6366, training loss: 9005.82, average training loss: 9587.62, base loss: 14379.05
[INFO 2017-06-28 13:59:52,808 main.py:51] epoch 6367, training loss: 11111.67, average training loss: 9588.38, base loss: 14381.30
[INFO 2017-06-28 13:59:53,490 main.py:51] epoch 6368, training loss: 8041.27, average training loss: 9587.54, base loss: 14378.62
[INFO 2017-06-28 13:59:54,139 main.py:51] epoch 6369, training loss: 10153.84, average training loss: 9587.92, base loss: 14379.36
[INFO 2017-06-28 13:59:54,793 main.py:51] epoch 6370, training loss: 8754.65, average training loss: 9587.46, base loss: 14377.74
[INFO 2017-06-28 13:59:55,464 main.py:51] epoch 6371, training loss: 8685.52, average training loss: 9585.39, base loss: 14374.55
[INFO 2017-06-28 13:59:56,104 main.py:51] epoch 6372, training loss: 9459.06, average training loss: 9585.43, base loss: 14373.95
[INFO 2017-06-28 13:59:56,750 main.py:51] epoch 6373, training loss: 10416.55, average training loss: 9586.25, base loss: 14373.85
[INFO 2017-06-28 13:59:57,420 main.py:51] epoch 6374, training loss: 10479.38, average training loss: 9587.68, base loss: 14378.85
[INFO 2017-06-28 13:59:58,093 main.py:51] epoch 6375, training loss: 9135.34, average training loss: 9586.88, base loss: 14378.33
[INFO 2017-06-28 13:59:58,738 main.py:51] epoch 6376, training loss: 10124.37, average training loss: 9587.17, base loss: 14377.76
[INFO 2017-06-28 13:59:59,388 main.py:51] epoch 6377, training loss: 9275.00, average training loss: 9586.84, base loss: 14376.87
[INFO 2017-06-28 14:00:00,046 main.py:51] epoch 6378, training loss: 10536.10, average training loss: 9589.00, base loss: 14381.85
[INFO 2017-06-28 14:00:00,691 main.py:51] epoch 6379, training loss: 9243.37, average training loss: 9588.12, base loss: 14382.11
[INFO 2017-06-28 14:00:01,360 main.py:51] epoch 6380, training loss: 9132.44, average training loss: 9587.04, base loss: 14381.72
[INFO 2017-06-28 14:00:02,005 main.py:51] epoch 6381, training loss: 9564.04, average training loss: 9585.50, base loss: 14379.18
[INFO 2017-06-28 14:00:02,692 main.py:51] epoch 6382, training loss: 8759.71, average training loss: 9585.04, base loss: 14378.39
[INFO 2017-06-28 14:00:03,342 main.py:51] epoch 6383, training loss: 9773.77, average training loss: 9585.37, base loss: 14379.60
[INFO 2017-06-28 14:00:04,005 main.py:51] epoch 6384, training loss: 9767.54, average training loss: 9585.72, base loss: 14379.82
[INFO 2017-06-28 14:00:04,659 main.py:51] epoch 6385, training loss: 10178.24, average training loss: 9585.61, base loss: 14382.16
[INFO 2017-06-28 14:00:05,316 main.py:51] epoch 6386, training loss: 8680.40, average training loss: 9585.65, base loss: 14382.59
[INFO 2017-06-28 14:00:05,970 main.py:51] epoch 6387, training loss: 8222.21, average training loss: 9583.64, base loss: 14380.43
[INFO 2017-06-28 14:00:06,615 main.py:51] epoch 6388, training loss: 9053.57, average training loss: 9583.89, base loss: 14379.57
[INFO 2017-06-28 14:00:07,299 main.py:51] epoch 6389, training loss: 9213.88, average training loss: 9583.74, base loss: 14378.36
[INFO 2017-06-28 14:00:07,951 main.py:51] epoch 6390, training loss: 9340.68, average training loss: 9583.95, base loss: 14379.57
[INFO 2017-06-28 14:00:08,632 main.py:51] epoch 6391, training loss: 9803.41, average training loss: 9583.52, base loss: 14379.23
[INFO 2017-06-28 14:00:09,280 main.py:51] epoch 6392, training loss: 9806.14, average training loss: 9583.61, base loss: 14379.89
[INFO 2017-06-28 14:00:09,932 main.py:51] epoch 6393, training loss: 11071.30, average training loss: 9586.02, base loss: 14382.85
[INFO 2017-06-28 14:00:10,574 main.py:51] epoch 6394, training loss: 9862.53, average training loss: 9587.30, base loss: 14384.17
[INFO 2017-06-28 14:00:11,263 main.py:51] epoch 6395, training loss: 8859.21, average training loss: 9585.76, base loss: 14382.70
[INFO 2017-06-28 14:00:11,929 main.py:51] epoch 6396, training loss: 8863.45, average training loss: 9585.51, base loss: 14381.53
[INFO 2017-06-28 14:00:12,576 main.py:51] epoch 6397, training loss: 8448.09, average training loss: 9583.15, base loss: 14377.83
[INFO 2017-06-28 14:00:13,218 main.py:51] epoch 6398, training loss: 10145.41, average training loss: 9583.58, base loss: 14378.20
[INFO 2017-06-28 14:00:13,862 main.py:51] epoch 6399, training loss: 8678.09, average training loss: 9581.28, base loss: 14372.92
[INFO 2017-06-28 14:00:13,862 main.py:53] epoch 6399, testing
[INFO 2017-06-28 14:00:16,460 main.py:105] average testing loss: 11151.33, base loss: 15492.89
[INFO 2017-06-28 14:00:16,460 main.py:106] improve_loss: 4341.56, improve_percent: 0.28
[INFO 2017-06-28 14:00:16,461 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:00:17,117 main.py:51] epoch 6400, training loss: 8460.41, average training loss: 9579.30, base loss: 14370.02
[INFO 2017-06-28 14:00:17,762 main.py:51] epoch 6401, training loss: 8454.66, average training loss: 9577.10, base loss: 14369.63
[INFO 2017-06-28 14:00:18,418 main.py:51] epoch 6402, training loss: 9178.70, average training loss: 9576.77, base loss: 14369.08
[INFO 2017-06-28 14:00:19,080 main.py:51] epoch 6403, training loss: 9176.82, average training loss: 9577.06, base loss: 14371.10
[INFO 2017-06-28 14:00:19,749 main.py:51] epoch 6404, training loss: 9939.62, average training loss: 9577.43, base loss: 14371.59
[INFO 2017-06-28 14:00:20,413 main.py:51] epoch 6405, training loss: 10319.34, average training loss: 9576.98, base loss: 14369.75
[INFO 2017-06-28 14:00:21,093 main.py:51] epoch 6406, training loss: 9034.70, average training loss: 9576.63, base loss: 14369.80
[INFO 2017-06-28 14:00:21,766 main.py:51] epoch 6407, training loss: 9792.88, average training loss: 9577.80, base loss: 14371.99
[INFO 2017-06-28 14:00:22,417 main.py:51] epoch 6408, training loss: 10656.84, average training loss: 9579.95, base loss: 14374.05
[INFO 2017-06-28 14:00:23,081 main.py:51] epoch 6409, training loss: 9609.71, average training loss: 9578.81, base loss: 14371.23
[INFO 2017-06-28 14:00:23,706 main.py:51] epoch 6410, training loss: 9699.58, average training loss: 9579.91, base loss: 14371.95
[INFO 2017-06-28 14:00:24,382 main.py:51] epoch 6411, training loss: 9822.89, average training loss: 9579.21, base loss: 14370.88
[INFO 2017-06-28 14:00:25,049 main.py:51] epoch 6412, training loss: 10287.97, average training loss: 9578.88, base loss: 14369.64
[INFO 2017-06-28 14:00:25,710 main.py:51] epoch 6413, training loss: 9293.71, average training loss: 9578.93, base loss: 14370.33
[INFO 2017-06-28 14:00:26,352 main.py:51] epoch 6414, training loss: 11053.99, average training loss: 9580.07, base loss: 14371.26
[INFO 2017-06-28 14:00:27,010 main.py:51] epoch 6415, training loss: 9544.88, average training loss: 9579.39, base loss: 14373.67
[INFO 2017-06-28 14:00:27,672 main.py:51] epoch 6416, training loss: 9448.33, average training loss: 9578.43, base loss: 14374.09
[INFO 2017-06-28 14:00:28,348 main.py:51] epoch 6417, training loss: 10296.18, average training loss: 9579.36, base loss: 14376.41
[INFO 2017-06-28 14:00:29,024 main.py:51] epoch 6418, training loss: 9382.15, average training loss: 9579.36, base loss: 14375.66
[INFO 2017-06-28 14:00:29,669 main.py:51] epoch 6419, training loss: 8789.89, average training loss: 9577.27, base loss: 14371.65
[INFO 2017-06-28 14:00:30,329 main.py:51] epoch 6420, training loss: 10431.71, average training loss: 9578.27, base loss: 14372.75
[INFO 2017-06-28 14:00:30,984 main.py:51] epoch 6421, training loss: 8785.56, average training loss: 9579.06, base loss: 14374.42
[INFO 2017-06-28 14:00:31,636 main.py:51] epoch 6422, training loss: 9905.07, average training loss: 9581.39, base loss: 14378.30
[INFO 2017-06-28 14:00:32,280 main.py:51] epoch 6423, training loss: 10059.24, average training loss: 9581.91, base loss: 14379.68
[INFO 2017-06-28 14:00:32,944 main.py:51] epoch 6424, training loss: 10814.30, average training loss: 9583.19, base loss: 14381.43
[INFO 2017-06-28 14:00:33,601 main.py:51] epoch 6425, training loss: 9574.05, average training loss: 9582.96, base loss: 14381.19
[INFO 2017-06-28 14:00:34,241 main.py:51] epoch 6426, training loss: 9404.07, average training loss: 9582.76, base loss: 14380.27
[INFO 2017-06-28 14:00:34,889 main.py:51] epoch 6427, training loss: 9683.26, average training loss: 9583.38, base loss: 14382.02
[INFO 2017-06-28 14:00:35,553 main.py:51] epoch 6428, training loss: 9330.44, average training loss: 9583.65, base loss: 14382.38
[INFO 2017-06-28 14:00:36,203 main.py:51] epoch 6429, training loss: 8689.17, average training loss: 9582.68, base loss: 14380.15
[INFO 2017-06-28 14:00:36,883 main.py:51] epoch 6430, training loss: 8502.72, average training loss: 9581.60, base loss: 14380.89
[INFO 2017-06-28 14:00:37,547 main.py:51] epoch 6431, training loss: 9855.39, average training loss: 9579.93, base loss: 14380.05
[INFO 2017-06-28 14:00:38,216 main.py:51] epoch 6432, training loss: 8453.34, average training loss: 9579.31, base loss: 14379.15
[INFO 2017-06-28 14:00:38,878 main.py:51] epoch 6433, training loss: 8365.54, average training loss: 9577.35, base loss: 14375.57
[INFO 2017-06-28 14:00:39,513 main.py:51] epoch 6434, training loss: 9464.99, average training loss: 9577.22, base loss: 14375.64
[INFO 2017-06-28 14:00:40,167 main.py:51] epoch 6435, training loss: 9308.05, average training loss: 9577.74, base loss: 14377.02
[INFO 2017-06-28 14:00:40,814 main.py:51] epoch 6436, training loss: 9033.76, average training loss: 9577.60, base loss: 14377.93
[INFO 2017-06-28 14:00:41,492 main.py:51] epoch 6437, training loss: 9743.53, average training loss: 9578.25, base loss: 14379.68
[INFO 2017-06-28 14:00:42,149 main.py:51] epoch 6438, training loss: 9846.93, average training loss: 9578.29, base loss: 14379.35
[INFO 2017-06-28 14:00:42,823 main.py:51] epoch 6439, training loss: 10660.19, average training loss: 9580.34, base loss: 14383.34
[INFO 2017-06-28 14:00:43,475 main.py:51] epoch 6440, training loss: 9283.86, average training loss: 9580.90, base loss: 14384.36
[INFO 2017-06-28 14:00:44,123 main.py:51] epoch 6441, training loss: 8664.46, average training loss: 9579.21, base loss: 14381.36
[INFO 2017-06-28 14:00:44,795 main.py:51] epoch 6442, training loss: 9215.65, average training loss: 9578.73, base loss: 14380.94
[INFO 2017-06-28 14:00:45,440 main.py:51] epoch 6443, training loss: 8642.34, average training loss: 9576.36, base loss: 14375.36
[INFO 2017-06-28 14:00:46,098 main.py:51] epoch 6444, training loss: 9218.61, average training loss: 9574.93, base loss: 14372.16
[INFO 2017-06-28 14:00:46,728 main.py:51] epoch 6445, training loss: 10382.67, average training loss: 9575.49, base loss: 14372.82
[INFO 2017-06-28 14:00:47,387 main.py:51] epoch 6446, training loss: 8892.90, average training loss: 9572.97, base loss: 14368.75
[INFO 2017-06-28 14:00:48,034 main.py:51] epoch 6447, training loss: 9715.38, average training loss: 9572.86, base loss: 14368.38
[INFO 2017-06-28 14:00:48,703 main.py:51] epoch 6448, training loss: 8222.04, average training loss: 9571.40, base loss: 14363.99
[INFO 2017-06-28 14:00:49,384 main.py:51] epoch 6449, training loss: 10490.31, average training loss: 9573.40, base loss: 14367.35
[INFO 2017-06-28 14:00:50,033 main.py:51] epoch 6450, training loss: 9878.61, average training loss: 9572.60, base loss: 14366.21
[INFO 2017-06-28 14:00:50,686 main.py:51] epoch 6451, training loss: 10278.59, average training loss: 9572.82, base loss: 14366.23
[INFO 2017-06-28 14:00:51,356 main.py:51] epoch 6452, training loss: 8601.76, average training loss: 9571.42, base loss: 14364.93
[INFO 2017-06-28 14:00:52,010 main.py:51] epoch 6453, training loss: 10205.65, average training loss: 9571.07, base loss: 14362.60
[INFO 2017-06-28 14:00:52,678 main.py:51] epoch 6454, training loss: 9418.88, average training loss: 9571.14, base loss: 14363.14
[INFO 2017-06-28 14:00:53,356 main.py:51] epoch 6455, training loss: 10163.41, average training loss: 9573.01, base loss: 14366.12
[INFO 2017-06-28 14:00:54,014 main.py:51] epoch 6456, training loss: 9292.39, average training loss: 9572.30, base loss: 14365.56
[INFO 2017-06-28 14:00:54,681 main.py:51] epoch 6457, training loss: 9908.51, average training loss: 9572.31, base loss: 14365.99
[INFO 2017-06-28 14:00:55,348 main.py:51] epoch 6458, training loss: 8645.46, average training loss: 9570.15, base loss: 14362.89
[INFO 2017-06-28 14:00:56,009 main.py:51] epoch 6459, training loss: 10515.49, average training loss: 9569.60, base loss: 14364.42
[INFO 2017-06-28 14:00:56,671 main.py:51] epoch 6460, training loss: 9249.34, average training loss: 9567.91, base loss: 14362.63
[INFO 2017-06-28 14:00:57,326 main.py:51] epoch 6461, training loss: 8435.31, average training loss: 9566.34, base loss: 14360.64
[INFO 2017-06-28 14:00:57,966 main.py:51] epoch 6462, training loss: 10606.31, average training loss: 9567.33, base loss: 14364.01
[INFO 2017-06-28 14:00:58,618 main.py:51] epoch 6463, training loss: 8584.11, average training loss: 9566.02, base loss: 14362.02
[INFO 2017-06-28 14:00:59,257 main.py:51] epoch 6464, training loss: 8839.99, average training loss: 9566.12, base loss: 14362.40
[INFO 2017-06-28 14:00:59,919 main.py:51] epoch 6465, training loss: 10353.69, average training loss: 9567.26, base loss: 14365.45
[INFO 2017-06-28 14:01:00,609 main.py:51] epoch 6466, training loss: 8711.78, average training loss: 9566.19, base loss: 14363.11
[INFO 2017-06-28 14:01:01,266 main.py:51] epoch 6467, training loss: 8988.15, average training loss: 9564.98, base loss: 14361.38
[INFO 2017-06-28 14:01:01,922 main.py:51] epoch 6468, training loss: 9295.82, average training loss: 9565.09, base loss: 14363.13
[INFO 2017-06-28 14:01:02,578 main.py:51] epoch 6469, training loss: 9166.40, average training loss: 9564.84, base loss: 14363.37
[INFO 2017-06-28 14:01:03,226 main.py:51] epoch 6470, training loss: 11288.21, average training loss: 9566.70, base loss: 14367.25
[INFO 2017-06-28 14:01:03,879 main.py:51] epoch 6471, training loss: 9428.21, average training loss: 9566.93, base loss: 14368.35
[INFO 2017-06-28 14:01:04,548 main.py:51] epoch 6472, training loss: 8924.35, average training loss: 9566.73, base loss: 14368.24
[INFO 2017-06-28 14:01:05,205 main.py:51] epoch 6473, training loss: 9379.11, average training loss: 9567.18, base loss: 14367.67
[INFO 2017-06-28 14:01:05,861 main.py:51] epoch 6474, training loss: 10025.71, average training loss: 9567.63, base loss: 14370.17
[INFO 2017-06-28 14:01:06,515 main.py:51] epoch 6475, training loss: 9354.56, average training loss: 9568.06, base loss: 14373.58
[INFO 2017-06-28 14:01:07,163 main.py:51] epoch 6476, training loss: 8859.96, average training loss: 9567.02, base loss: 14371.49
[INFO 2017-06-28 14:01:07,827 main.py:51] epoch 6477, training loss: 10042.94, average training loss: 9567.31, base loss: 14373.71
[INFO 2017-06-28 14:01:08,486 main.py:51] epoch 6478, training loss: 10408.64, average training loss: 9566.92, base loss: 14373.59
[INFO 2017-06-28 14:01:09,135 main.py:51] epoch 6479, training loss: 9551.97, average training loss: 9567.11, base loss: 14374.07
[INFO 2017-06-28 14:01:09,776 main.py:51] epoch 6480, training loss: 9425.73, average training loss: 9566.91, base loss: 14374.00
[INFO 2017-06-28 14:01:10,422 main.py:51] epoch 6481, training loss: 9241.85, average training loss: 9566.08, base loss: 14372.01
[INFO 2017-06-28 14:01:11,081 main.py:51] epoch 6482, training loss: 8934.75, average training loss: 9565.39, base loss: 14372.60
[INFO 2017-06-28 14:01:11,765 main.py:51] epoch 6483, training loss: 9545.16, average training loss: 9565.44, base loss: 14371.52
[INFO 2017-06-28 14:01:12,418 main.py:51] epoch 6484, training loss: 8714.95, average training loss: 9566.12, base loss: 14373.13
[INFO 2017-06-28 14:01:13,068 main.py:51] epoch 6485, training loss: 8562.10, average training loss: 9566.43, base loss: 14372.92
[INFO 2017-06-28 14:01:13,727 main.py:51] epoch 6486, training loss: 9302.21, average training loss: 9567.35, base loss: 14374.32
[INFO 2017-06-28 14:01:14,403 main.py:51] epoch 6487, training loss: 10193.10, average training loss: 9567.59, base loss: 14373.78
[INFO 2017-06-28 14:01:15,085 main.py:51] epoch 6488, training loss: 8449.04, average training loss: 9566.54, base loss: 14372.42
[INFO 2017-06-28 14:01:15,729 main.py:51] epoch 6489, training loss: 8439.38, average training loss: 9565.59, base loss: 14370.33
[INFO 2017-06-28 14:01:16,395 main.py:51] epoch 6490, training loss: 10664.91, average training loss: 9567.01, base loss: 14372.38
[INFO 2017-06-28 14:01:17,065 main.py:51] epoch 6491, training loss: 8763.02, average training loss: 9566.57, base loss: 14372.92
[INFO 2017-06-28 14:01:17,724 main.py:51] epoch 6492, training loss: 9466.15, average training loss: 9566.55, base loss: 14373.34
[INFO 2017-06-28 14:01:18,381 main.py:51] epoch 6493, training loss: 11138.23, average training loss: 9567.45, base loss: 14374.84
[INFO 2017-06-28 14:01:19,054 main.py:51] epoch 6494, training loss: 9282.59, average training loss: 9567.41, base loss: 14374.81
[INFO 2017-06-28 14:01:19,707 main.py:51] epoch 6495, training loss: 9683.18, average training loss: 9565.03, base loss: 14372.45
[INFO 2017-06-28 14:01:20,351 main.py:51] epoch 6496, training loss: 9746.81, average training loss: 9564.53, base loss: 14373.54
[INFO 2017-06-28 14:01:21,006 main.py:51] epoch 6497, training loss: 10828.42, average training loss: 9566.02, base loss: 14373.58
[INFO 2017-06-28 14:01:21,678 main.py:51] epoch 6498, training loss: 9492.01, average training loss: 9567.24, base loss: 14376.13
[INFO 2017-06-28 14:01:22,371 main.py:51] epoch 6499, training loss: 10078.61, average training loss: 9567.69, base loss: 14377.21
[INFO 2017-06-28 14:01:22,371 main.py:53] epoch 6499, testing
[INFO 2017-06-28 14:01:25,014 main.py:105] average testing loss: 10577.51, base loss: 14890.91
[INFO 2017-06-28 14:01:25,014 main.py:106] improve_loss: 4313.40, improve_percent: 0.29
[INFO 2017-06-28 14:01:25,015 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:01:25,665 main.py:51] epoch 6500, training loss: 8343.86, average training loss: 9565.86, base loss: 14375.21
[INFO 2017-06-28 14:01:26,318 main.py:51] epoch 6501, training loss: 12110.93, average training loss: 9569.16, base loss: 14381.78
[INFO 2017-06-28 14:01:26,962 main.py:51] epoch 6502, training loss: 9564.86, average training loss: 9569.79, base loss: 14382.53
[INFO 2017-06-28 14:01:27,621 main.py:51] epoch 6503, training loss: 9147.70, average training loss: 9568.68, base loss: 14379.62
[INFO 2017-06-28 14:01:28,274 main.py:51] epoch 6504, training loss: 9114.98, average training loss: 9569.31, base loss: 14381.34
[INFO 2017-06-28 14:01:28,933 main.py:51] epoch 6505, training loss: 9846.49, average training loss: 9569.38, base loss: 14382.66
[INFO 2017-06-28 14:01:29,563 main.py:51] epoch 6506, training loss: 9481.59, average training loss: 9569.82, base loss: 14384.00
[INFO 2017-06-28 14:01:30,238 main.py:51] epoch 6507, training loss: 10523.90, average training loss: 9569.06, base loss: 14383.07
[INFO 2017-06-28 14:01:30,879 main.py:51] epoch 6508, training loss: 10373.79, average training loss: 9570.34, base loss: 14385.18
[INFO 2017-06-28 14:01:31,510 main.py:51] epoch 6509, training loss: 10802.89, average training loss: 9572.37, base loss: 14389.53
[INFO 2017-06-28 14:01:32,190 main.py:51] epoch 6510, training loss: 9332.34, average training loss: 9571.14, base loss: 14386.81
[INFO 2017-06-28 14:01:32,819 main.py:51] epoch 6511, training loss: 8747.31, average training loss: 9569.94, base loss: 14385.01
[INFO 2017-06-28 14:01:33,484 main.py:51] epoch 6512, training loss: 10602.81, average training loss: 9571.35, base loss: 14387.10
[INFO 2017-06-28 14:01:34,149 main.py:51] epoch 6513, training loss: 8630.54, average training loss: 9569.69, base loss: 14385.30
[INFO 2017-06-28 14:01:34,802 main.py:51] epoch 6514, training loss: 9758.12, average training loss: 9571.63, base loss: 14390.24
[INFO 2017-06-28 14:01:35,468 main.py:51] epoch 6515, training loss: 9238.91, average training loss: 9571.26, base loss: 14389.93
[INFO 2017-06-28 14:01:36,120 main.py:51] epoch 6516, training loss: 9506.82, average training loss: 9571.93, base loss: 14392.36
[INFO 2017-06-28 14:01:36,784 main.py:51] epoch 6517, training loss: 9777.44, average training loss: 9572.40, base loss: 14394.06
[INFO 2017-06-28 14:01:37,459 main.py:51] epoch 6518, training loss: 10209.99, average training loss: 9573.15, base loss: 14394.57
[INFO 2017-06-28 14:01:38,106 main.py:51] epoch 6519, training loss: 8784.05, average training loss: 9571.79, base loss: 14392.66
[INFO 2017-06-28 14:01:38,772 main.py:51] epoch 6520, training loss: 9566.09, average training loss: 9571.78, base loss: 14392.34
[INFO 2017-06-28 14:01:39,419 main.py:51] epoch 6521, training loss: 8912.46, average training loss: 9570.71, base loss: 14390.30
[INFO 2017-06-28 14:01:40,087 main.py:51] epoch 6522, training loss: 10180.63, average training loss: 9570.64, base loss: 14390.34
[INFO 2017-06-28 14:01:40,758 main.py:51] epoch 6523, training loss: 8794.83, average training loss: 9569.42, base loss: 14387.72
[INFO 2017-06-28 14:01:41,404 main.py:51] epoch 6524, training loss: 9667.53, average training loss: 9569.48, base loss: 14389.24
[INFO 2017-06-28 14:01:42,056 main.py:51] epoch 6525, training loss: 9190.38, average training loss: 9568.71, base loss: 14388.78
[INFO 2017-06-28 14:01:42,736 main.py:51] epoch 6526, training loss: 9374.37, average training loss: 9569.67, base loss: 14390.70
[INFO 2017-06-28 14:01:43,410 main.py:51] epoch 6527, training loss: 9179.50, average training loss: 9569.59, base loss: 14390.58
[INFO 2017-06-28 14:01:44,071 main.py:51] epoch 6528, training loss: 9140.24, average training loss: 9570.40, base loss: 14391.41
[INFO 2017-06-28 14:01:44,732 main.py:51] epoch 6529, training loss: 10498.99, average training loss: 9571.14, base loss: 14393.40
[INFO 2017-06-28 14:01:45,380 main.py:51] epoch 6530, training loss: 10052.46, average training loss: 9573.67, base loss: 14397.34
[INFO 2017-06-28 14:01:46,053 main.py:51] epoch 6531, training loss: 9504.88, average training loss: 9573.29, base loss: 14398.64
[INFO 2017-06-28 14:01:46,749 main.py:51] epoch 6532, training loss: 9135.95, average training loss: 9570.66, base loss: 14393.74
[INFO 2017-06-28 14:01:47,408 main.py:51] epoch 6533, training loss: 10224.04, average training loss: 9571.93, base loss: 14395.52
[INFO 2017-06-28 14:01:48,064 main.py:51] epoch 6534, training loss: 8920.96, average training loss: 9571.23, base loss: 14394.35
[INFO 2017-06-28 14:01:48,705 main.py:51] epoch 6535, training loss: 9177.84, average training loss: 9570.45, base loss: 14393.43
[INFO 2017-06-28 14:01:49,346 main.py:51] epoch 6536, training loss: 8386.21, average training loss: 9567.70, base loss: 14391.33
[INFO 2017-06-28 14:01:50,010 main.py:51] epoch 6537, training loss: 9225.65, average training loss: 9567.14, base loss: 14391.38
[INFO 2017-06-28 14:01:50,654 main.py:51] epoch 6538, training loss: 11122.19, average training loss: 9568.54, base loss: 14393.89
[INFO 2017-06-28 14:01:51,315 main.py:51] epoch 6539, training loss: 7770.56, average training loss: 9566.23, base loss: 14390.44
[INFO 2017-06-28 14:01:51,985 main.py:51] epoch 6540, training loss: 10930.00, average training loss: 9567.97, base loss: 14394.02
[INFO 2017-06-28 14:01:52,646 main.py:51] epoch 6541, training loss: 10168.94, average training loss: 9567.91, base loss: 14395.04
[INFO 2017-06-28 14:01:53,304 main.py:51] epoch 6542, training loss: 10404.48, average training loss: 9569.82, base loss: 14398.31
[INFO 2017-06-28 14:01:53,960 main.py:51] epoch 6543, training loss: 9543.41, average training loss: 9570.20, base loss: 14399.84
[INFO 2017-06-28 14:01:54,605 main.py:51] epoch 6544, training loss: 10727.21, average training loss: 9570.77, base loss: 14400.82
[INFO 2017-06-28 14:01:55,261 main.py:51] epoch 6545, training loss: 10105.39, average training loss: 9569.43, base loss: 14398.55
[INFO 2017-06-28 14:01:55,922 main.py:51] epoch 6546, training loss: 8453.31, average training loss: 9569.27, base loss: 14399.05
[INFO 2017-06-28 14:01:56,587 main.py:51] epoch 6547, training loss: 10204.40, average training loss: 9569.17, base loss: 14399.86
[INFO 2017-06-28 14:01:57,219 main.py:51] epoch 6548, training loss: 9980.09, average training loss: 9570.79, base loss: 14400.26
[INFO 2017-06-28 14:01:57,857 main.py:51] epoch 6549, training loss: 10695.39, average training loss: 9572.59, base loss: 14402.09
[INFO 2017-06-28 14:01:58,517 main.py:51] epoch 6550, training loss: 11521.61, average training loss: 9574.86, base loss: 14404.86
[INFO 2017-06-28 14:01:59,199 main.py:51] epoch 6551, training loss: 10227.01, average training loss: 9574.97, base loss: 14404.98
[INFO 2017-06-28 14:01:59,825 main.py:51] epoch 6552, training loss: 9971.23, average training loss: 9573.88, base loss: 14403.64
[INFO 2017-06-28 14:02:00,469 main.py:51] epoch 6553, training loss: 9378.71, average training loss: 9575.19, base loss: 14406.27
[INFO 2017-06-28 14:02:01,145 main.py:51] epoch 6554, training loss: 7937.29, average training loss: 9573.16, base loss: 14402.87
[INFO 2017-06-28 14:02:01,797 main.py:51] epoch 6555, training loss: 8075.96, average training loss: 9571.91, base loss: 14400.78
[INFO 2017-06-28 14:02:02,446 main.py:51] epoch 6556, training loss: 9442.82, average training loss: 9571.80, base loss: 14400.75
[INFO 2017-06-28 14:02:03,094 main.py:51] epoch 6557, training loss: 9463.08, average training loss: 9571.77, base loss: 14398.79
[INFO 2017-06-28 14:02:03,750 main.py:51] epoch 6558, training loss: 9243.15, average training loss: 9571.60, base loss: 14396.94
[INFO 2017-06-28 14:02:04,407 main.py:51] epoch 6559, training loss: 9791.75, average training loss: 9570.55, base loss: 14394.87
[INFO 2017-06-28 14:02:05,063 main.py:51] epoch 6560, training loss: 9186.00, average training loss: 9571.49, base loss: 14396.90
[INFO 2017-06-28 14:02:05,711 main.py:51] epoch 6561, training loss: 9332.56, average training loss: 9571.41, base loss: 14398.40
[INFO 2017-06-28 14:02:06,368 main.py:51] epoch 6562, training loss: 10435.19, average training loss: 9572.07, base loss: 14397.13
[INFO 2017-06-28 14:02:07,021 main.py:51] epoch 6563, training loss: 9404.25, average training loss: 9571.99, base loss: 14396.00
[INFO 2017-06-28 14:02:07,686 main.py:51] epoch 6564, training loss: 9913.96, average training loss: 9572.75, base loss: 14398.75
[INFO 2017-06-28 14:02:08,336 main.py:51] epoch 6565, training loss: 9108.48, average training loss: 9571.81, base loss: 14399.18
[INFO 2017-06-28 14:02:09,001 main.py:51] epoch 6566, training loss: 11236.73, average training loss: 9573.38, base loss: 14402.97
[INFO 2017-06-28 14:02:09,670 main.py:51] epoch 6567, training loss: 10205.72, average training loss: 9574.30, base loss: 14403.74
[INFO 2017-06-28 14:02:10,338 main.py:51] epoch 6568, training loss: 9538.78, average training loss: 9574.58, base loss: 14404.77
[INFO 2017-06-28 14:02:10,976 main.py:51] epoch 6569, training loss: 9257.26, average training loss: 9574.86, base loss: 14405.11
[INFO 2017-06-28 14:02:11,615 main.py:51] epoch 6570, training loss: 9876.70, average training loss: 9575.25, base loss: 14405.86
[INFO 2017-06-28 14:02:12,253 main.py:51] epoch 6571, training loss: 9110.54, average training loss: 9574.40, base loss: 14405.19
[INFO 2017-06-28 14:02:12,916 main.py:51] epoch 6572, training loss: 9236.28, average training loss: 9573.46, base loss: 14402.39
[INFO 2017-06-28 14:02:13,571 main.py:51] epoch 6573, training loss: 9973.21, average training loss: 9574.71, base loss: 14405.22
[INFO 2017-06-28 14:02:14,229 main.py:51] epoch 6574, training loss: 9946.52, average training loss: 9575.33, base loss: 14404.87
[INFO 2017-06-28 14:02:14,875 main.py:51] epoch 6575, training loss: 7668.63, average training loss: 9573.12, base loss: 14400.95
[INFO 2017-06-28 14:02:15,518 main.py:51] epoch 6576, training loss: 10120.71, average training loss: 9573.49, base loss: 14402.52
[INFO 2017-06-28 14:02:16,166 main.py:51] epoch 6577, training loss: 9585.54, average training loss: 9573.86, base loss: 14401.08
[INFO 2017-06-28 14:02:16,818 main.py:51] epoch 6578, training loss: 10491.05, average training loss: 9575.29, base loss: 14402.90
[INFO 2017-06-28 14:02:17,465 main.py:51] epoch 6579, training loss: 9683.90, average training loss: 9575.93, base loss: 14404.65
[INFO 2017-06-28 14:02:18,128 main.py:51] epoch 6580, training loss: 10508.15, average training loss: 9576.19, base loss: 14406.62
[INFO 2017-06-28 14:02:18,803 main.py:51] epoch 6581, training loss: 9125.81, average training loss: 9576.18, base loss: 14406.80
[INFO 2017-06-28 14:02:19,454 main.py:51] epoch 6582, training loss: 9132.15, average training loss: 9574.25, base loss: 14403.26
[INFO 2017-06-28 14:02:20,109 main.py:51] epoch 6583, training loss: 10096.26, average training loss: 9574.67, base loss: 14403.60
[INFO 2017-06-28 14:02:20,747 main.py:51] epoch 6584, training loss: 9162.99, average training loss: 9575.69, base loss: 14405.14
[INFO 2017-06-28 14:02:21,399 main.py:51] epoch 6585, training loss: 8097.75, average training loss: 9574.94, base loss: 14403.69
[INFO 2017-06-28 14:02:22,062 main.py:51] epoch 6586, training loss: 10233.05, average training loss: 9576.91, base loss: 14408.73
[INFO 2017-06-28 14:02:22,725 main.py:51] epoch 6587, training loss: 9386.27, average training loss: 9576.81, base loss: 14408.53
[INFO 2017-06-28 14:02:23,376 main.py:51] epoch 6588, training loss: 9733.10, average training loss: 9575.67, base loss: 14405.56
[INFO 2017-06-28 14:02:24,038 main.py:51] epoch 6589, training loss: 10365.79, average training loss: 9576.47, base loss: 14405.14
[INFO 2017-06-28 14:02:24,687 main.py:51] epoch 6590, training loss: 9151.64, average training loss: 9577.12, base loss: 14404.62
[INFO 2017-06-28 14:02:25,349 main.py:51] epoch 6591, training loss: 10683.91, average training loss: 9578.52, base loss: 14406.39
[INFO 2017-06-28 14:02:26,007 main.py:51] epoch 6592, training loss: 9522.09, average training loss: 9577.75, base loss: 14404.11
[INFO 2017-06-28 14:02:26,680 main.py:51] epoch 6593, training loss: 8540.85, average training loss: 9577.33, base loss: 14402.81
[INFO 2017-06-28 14:02:27,321 main.py:51] epoch 6594, training loss: 10256.03, average training loss: 9577.82, base loss: 14403.73
[INFO 2017-06-28 14:02:27,967 main.py:51] epoch 6595, training loss: 10003.63, average training loss: 9577.19, base loss: 14402.32
[INFO 2017-06-28 14:02:28,617 main.py:51] epoch 6596, training loss: 9018.37, average training loss: 9575.69, base loss: 14399.96
[INFO 2017-06-28 14:02:29,282 main.py:51] epoch 6597, training loss: 11454.86, average training loss: 9575.84, base loss: 14400.58
[INFO 2017-06-28 14:02:29,942 main.py:51] epoch 6598, training loss: 10146.21, average training loss: 9574.43, base loss: 14398.72
[INFO 2017-06-28 14:02:30,606 main.py:51] epoch 6599, training loss: 9347.85, average training loss: 9575.10, base loss: 14398.78
[INFO 2017-06-28 14:02:30,606 main.py:53] epoch 6599, testing
[INFO 2017-06-28 14:02:33,218 main.py:105] average testing loss: 10222.27, base loss: 14225.93
[INFO 2017-06-28 14:02:33,218 main.py:106] improve_loss: 4003.66, improve_percent: 0.28
[INFO 2017-06-28 14:02:33,218 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:02:33,861 main.py:51] epoch 6600, training loss: 9141.15, average training loss: 9573.57, base loss: 14395.22
[INFO 2017-06-28 14:02:34,509 main.py:51] epoch 6601, training loss: 9781.80, average training loss: 9573.77, base loss: 14394.71
[INFO 2017-06-28 14:02:35,160 main.py:51] epoch 6602, training loss: 9156.18, average training loss: 9572.97, base loss: 14392.57
[INFO 2017-06-28 14:02:35,827 main.py:51] epoch 6603, training loss: 8656.05, average training loss: 9572.02, base loss: 14390.62
[INFO 2017-06-28 14:02:36,516 main.py:51] epoch 6604, training loss: 11752.99, average training loss: 9575.36, base loss: 14394.75
[INFO 2017-06-28 14:02:37,163 main.py:51] epoch 6605, training loss: 8666.35, average training loss: 9573.81, base loss: 14392.82
[INFO 2017-06-28 14:02:37,822 main.py:51] epoch 6606, training loss: 9666.48, average training loss: 9573.45, base loss: 14391.87
[INFO 2017-06-28 14:02:38,481 main.py:51] epoch 6607, training loss: 9481.85, average training loss: 9574.13, base loss: 14392.00
[INFO 2017-06-28 14:02:39,142 main.py:51] epoch 6608, training loss: 8472.28, average training loss: 9573.98, base loss: 14391.52
[INFO 2017-06-28 14:02:39,787 main.py:51] epoch 6609, training loss: 8923.47, average training loss: 9572.90, base loss: 14390.62
[INFO 2017-06-28 14:02:40,445 main.py:51] epoch 6610, training loss: 8382.62, average training loss: 9571.82, base loss: 14389.18
[INFO 2017-06-28 14:02:41,081 main.py:51] epoch 6611, training loss: 9482.80, average training loss: 9571.67, base loss: 14389.70
[INFO 2017-06-28 14:02:41,744 main.py:51] epoch 6612, training loss: 8195.92, average training loss: 9570.07, base loss: 14387.33
[INFO 2017-06-28 14:02:42,410 main.py:51] epoch 6613, training loss: 9866.05, average training loss: 9570.38, base loss: 14386.94
[INFO 2017-06-28 14:02:43,051 main.py:51] epoch 6614, training loss: 9715.28, average training loss: 9570.43, base loss: 14388.07
[INFO 2017-06-28 14:02:43,712 main.py:51] epoch 6615, training loss: 9521.16, average training loss: 9570.32, base loss: 14387.81
[INFO 2017-06-28 14:02:44,373 main.py:51] epoch 6616, training loss: 9274.21, average training loss: 9568.84, base loss: 14385.34
[INFO 2017-06-28 14:02:45,036 main.py:51] epoch 6617, training loss: 8653.69, average training loss: 9569.36, base loss: 14385.06
[INFO 2017-06-28 14:02:45,682 main.py:51] epoch 6618, training loss: 9896.59, average training loss: 9568.65, base loss: 14383.56
[INFO 2017-06-28 14:02:46,321 main.py:51] epoch 6619, training loss: 8408.87, average training loss: 9567.55, base loss: 14382.71
[INFO 2017-06-28 14:02:46,973 main.py:51] epoch 6620, training loss: 9776.45, average training loss: 9566.49, base loss: 14381.46
[INFO 2017-06-28 14:02:47,620 main.py:51] epoch 6621, training loss: 9686.75, average training loss: 9565.93, base loss: 14381.65
[INFO 2017-06-28 14:02:48,300 main.py:51] epoch 6622, training loss: 9600.45, average training loss: 9566.48, base loss: 14383.51
[INFO 2017-06-28 14:02:48,952 main.py:51] epoch 6623, training loss: 8887.40, average training loss: 9565.30, base loss: 14383.23
[INFO 2017-06-28 14:02:49,621 main.py:51] epoch 6624, training loss: 8826.46, average training loss: 9564.22, base loss: 14381.17
[INFO 2017-06-28 14:02:50,261 main.py:51] epoch 6625, training loss: 8290.38, average training loss: 9562.61, base loss: 14379.03
[INFO 2017-06-28 14:02:50,919 main.py:51] epoch 6626, training loss: 10511.33, average training loss: 9563.11, base loss: 14380.38
[INFO 2017-06-28 14:02:51,571 main.py:51] epoch 6627, training loss: 9648.36, average training loss: 9562.45, base loss: 14380.31
[INFO 2017-06-28 14:02:52,231 main.py:51] epoch 6628, training loss: 9576.48, average training loss: 9560.61, base loss: 14376.73
[INFO 2017-06-28 14:02:52,874 main.py:51] epoch 6629, training loss: 7947.37, average training loss: 9559.92, base loss: 14375.53
[INFO 2017-06-28 14:02:53,520 main.py:51] epoch 6630, training loss: 9146.17, average training loss: 9558.52, base loss: 14373.74
[INFO 2017-06-28 14:02:54,187 main.py:51] epoch 6631, training loss: 9429.68, average training loss: 9558.23, base loss: 14373.13
[INFO 2017-06-28 14:02:54,823 main.py:51] epoch 6632, training loss: 9058.66, average training loss: 9557.91, base loss: 14373.39
[INFO 2017-06-28 14:02:55,490 main.py:51] epoch 6633, training loss: 9561.44, average training loss: 9558.48, base loss: 14374.48
[INFO 2017-06-28 14:02:56,151 main.py:51] epoch 6634, training loss: 10172.97, average training loss: 9558.05, base loss: 14374.96
[INFO 2017-06-28 14:02:56,810 main.py:51] epoch 6635, training loss: 9301.90, average training loss: 9557.55, base loss: 14374.94
[INFO 2017-06-28 14:02:57,460 main.py:51] epoch 6636, training loss: 9517.14, average training loss: 9558.08, base loss: 14376.95
[INFO 2017-06-28 14:02:58,132 main.py:51] epoch 6637, training loss: 9122.73, average training loss: 9555.99, base loss: 14374.54
[INFO 2017-06-28 14:02:58,802 main.py:51] epoch 6638, training loss: 8749.96, average training loss: 9554.81, base loss: 14373.74
[INFO 2017-06-28 14:02:59,489 main.py:51] epoch 6639, training loss: 9313.29, average training loss: 9554.03, base loss: 14373.32
[INFO 2017-06-28 14:03:00,144 main.py:51] epoch 6640, training loss: 9246.29, average training loss: 9553.44, base loss: 14372.96
[INFO 2017-06-28 14:03:00,804 main.py:51] epoch 6641, training loss: 9090.62, average training loss: 9553.59, base loss: 14372.45
[INFO 2017-06-28 14:03:01,452 main.py:51] epoch 6642, training loss: 9184.16, average training loss: 9553.35, base loss: 14372.59
[INFO 2017-06-28 14:03:02,106 main.py:51] epoch 6643, training loss: 8977.08, average training loss: 9552.10, base loss: 14369.17
[INFO 2017-06-28 14:03:02,763 main.py:51] epoch 6644, training loss: 8991.92, average training loss: 9552.20, base loss: 14368.85
[INFO 2017-06-28 14:03:03,417 main.py:51] epoch 6645, training loss: 8952.95, average training loss: 9550.08, base loss: 14366.13
[INFO 2017-06-28 14:03:04,115 main.py:51] epoch 6646, training loss: 8043.29, average training loss: 9548.21, base loss: 14362.75
[INFO 2017-06-28 14:03:04,761 main.py:51] epoch 6647, training loss: 8430.07, average training loss: 9547.99, base loss: 14364.28
[INFO 2017-06-28 14:03:05,450 main.py:51] epoch 6648, training loss: 9691.67, average training loss: 9548.11, base loss: 14366.36
[INFO 2017-06-28 14:03:06,101 main.py:51] epoch 6649, training loss: 9862.70, average training loss: 9549.11, base loss: 14367.99
[INFO 2017-06-28 14:03:06,773 main.py:51] epoch 6650, training loss: 10136.54, average training loss: 9550.68, base loss: 14370.46
[INFO 2017-06-28 14:03:07,418 main.py:51] epoch 6651, training loss: 8826.66, average training loss: 9548.66, base loss: 14368.89
[INFO 2017-06-28 14:03:08,057 main.py:51] epoch 6652, training loss: 8942.25, average training loss: 9548.35, base loss: 14367.59
[INFO 2017-06-28 14:03:08,726 main.py:51] epoch 6653, training loss: 9793.85, average training loss: 9548.77, base loss: 14367.82
[INFO 2017-06-28 14:03:09,386 main.py:51] epoch 6654, training loss: 9264.66, average training loss: 9549.89, base loss: 14369.99
[INFO 2017-06-28 14:03:10,046 main.py:51] epoch 6655, training loss: 9982.70, average training loss: 9549.15, base loss: 14369.52
[INFO 2017-06-28 14:03:10,710 main.py:51] epoch 6656, training loss: 8491.60, average training loss: 9548.06, base loss: 14367.83
[INFO 2017-06-28 14:03:11,358 main.py:51] epoch 6657, training loss: 10282.25, average training loss: 9547.96, base loss: 14366.76
[INFO 2017-06-28 14:03:11,999 main.py:51] epoch 6658, training loss: 10931.04, average training loss: 9550.24, base loss: 14371.92
[INFO 2017-06-28 14:03:12,650 main.py:51] epoch 6659, training loss: 9384.83, average training loss: 9551.12, base loss: 14374.46
[INFO 2017-06-28 14:03:13,323 main.py:51] epoch 6660, training loss: 9430.45, average training loss: 9551.39, base loss: 14374.77
[INFO 2017-06-28 14:03:13,970 main.py:51] epoch 6661, training loss: 10815.01, average training loss: 9553.05, base loss: 14376.70
[INFO 2017-06-28 14:03:14,628 main.py:51] epoch 6662, training loss: 10108.21, average training loss: 9552.81, base loss: 14376.39
[INFO 2017-06-28 14:03:15,288 main.py:51] epoch 6663, training loss: 9605.06, average training loss: 9553.28, base loss: 14377.10
[INFO 2017-06-28 14:03:15,961 main.py:51] epoch 6664, training loss: 9044.99, average training loss: 9553.52, base loss: 14378.03
[INFO 2017-06-28 14:03:16,613 main.py:51] epoch 6665, training loss: 9513.44, average training loss: 9553.87, base loss: 14379.17
[INFO 2017-06-28 14:03:17,268 main.py:51] epoch 6666, training loss: 10229.36, average training loss: 9555.10, base loss: 14380.46
[INFO 2017-06-28 14:03:17,937 main.py:51] epoch 6667, training loss: 9292.55, average training loss: 9554.15, base loss: 14380.24
[INFO 2017-06-28 14:03:18,594 main.py:51] epoch 6668, training loss: 8442.69, average training loss: 9552.41, base loss: 14378.92
[INFO 2017-06-28 14:03:19,226 main.py:51] epoch 6669, training loss: 11328.25, average training loss: 9553.07, base loss: 14379.62
[INFO 2017-06-28 14:03:19,877 main.py:51] epoch 6670, training loss: 10032.18, average training loss: 9554.21, base loss: 14381.57
[INFO 2017-06-28 14:03:20,540 main.py:51] epoch 6671, training loss: 9215.34, average training loss: 9554.84, base loss: 14381.70
[INFO 2017-06-28 14:03:21,201 main.py:51] epoch 6672, training loss: 9592.42, average training loss: 9554.80, base loss: 14381.50
[INFO 2017-06-28 14:03:21,871 main.py:51] epoch 6673, training loss: 10582.54, average training loss: 9554.94, base loss: 14381.82
[INFO 2017-06-28 14:03:22,542 main.py:51] epoch 6674, training loss: 10218.72, average training loss: 9555.12, base loss: 14381.34
[INFO 2017-06-28 14:03:23,207 main.py:51] epoch 6675, training loss: 8018.83, average training loss: 9552.50, base loss: 14376.49
[INFO 2017-06-28 14:03:23,862 main.py:51] epoch 6676, training loss: 10146.02, average training loss: 9551.46, base loss: 14375.77
[INFO 2017-06-28 14:03:24,547 main.py:51] epoch 6677, training loss: 10648.00, average training loss: 9553.03, base loss: 14376.92
[INFO 2017-06-28 14:03:25,210 main.py:51] epoch 6678, training loss: 8987.55, average training loss: 9553.00, base loss: 14377.25
[INFO 2017-06-28 14:03:25,866 main.py:51] epoch 6679, training loss: 9520.22, average training loss: 9552.88, base loss: 14376.73
[INFO 2017-06-28 14:03:26,496 main.py:51] epoch 6680, training loss: 11097.63, average training loss: 9553.85, base loss: 14379.01
[INFO 2017-06-28 14:03:27,191 main.py:51] epoch 6681, training loss: 8523.94, average training loss: 9552.15, base loss: 14378.38
[INFO 2017-06-28 14:03:27,866 main.py:51] epoch 6682, training loss: 10906.32, average training loss: 9553.21, base loss: 14381.78
[INFO 2017-06-28 14:03:28,516 main.py:51] epoch 6683, training loss: 8141.86, average training loss: 9552.02, base loss: 14380.45
[INFO 2017-06-28 14:03:29,179 main.py:51] epoch 6684, training loss: 7801.10, average training loss: 9550.82, base loss: 14378.28
[INFO 2017-06-28 14:03:29,850 main.py:51] epoch 6685, training loss: 8787.40, average training loss: 9550.83, base loss: 14377.48
[INFO 2017-06-28 14:03:30,514 main.py:51] epoch 6686, training loss: 10267.04, average training loss: 9550.36, base loss: 14375.94
[INFO 2017-06-28 14:03:31,165 main.py:51] epoch 6687, training loss: 8720.72, average training loss: 9548.44, base loss: 14373.61
[INFO 2017-06-28 14:03:31,807 main.py:51] epoch 6688, training loss: 9615.06, average training loss: 9548.06, base loss: 14373.93
[INFO 2017-06-28 14:03:32,462 main.py:51] epoch 6689, training loss: 9905.36, average training loss: 9547.87, base loss: 14374.45
[INFO 2017-06-28 14:03:33,099 main.py:51] epoch 6690, training loss: 8643.88, average training loss: 9546.25, base loss: 14371.75
[INFO 2017-06-28 14:03:33,760 main.py:51] epoch 6691, training loss: 9569.35, average training loss: 9546.46, base loss: 14371.00
[INFO 2017-06-28 14:03:34,409 main.py:51] epoch 6692, training loss: 9915.89, average training loss: 9546.87, base loss: 14372.29
[INFO 2017-06-28 14:03:35,067 main.py:51] epoch 6693, training loss: 10176.83, average training loss: 9548.18, base loss: 14373.57
[INFO 2017-06-28 14:03:35,714 main.py:51] epoch 6694, training loss: 8812.01, average training loss: 9545.04, base loss: 14368.22
[INFO 2017-06-28 14:03:36,371 main.py:51] epoch 6695, training loss: 8634.45, average training loss: 9542.78, base loss: 14364.16
[INFO 2017-06-28 14:03:37,015 main.py:51] epoch 6696, training loss: 10081.59, average training loss: 9543.00, base loss: 14363.84
[INFO 2017-06-28 14:03:37,675 main.py:51] epoch 6697, training loss: 9936.59, average training loss: 9543.22, base loss: 14367.36
[INFO 2017-06-28 14:03:38,318 main.py:51] epoch 6698, training loss: 8389.25, average training loss: 9541.43, base loss: 14364.46
[INFO 2017-06-28 14:03:38,974 main.py:51] epoch 6699, training loss: 9890.73, average training loss: 9541.66, base loss: 14365.25
[INFO 2017-06-28 14:03:38,974 main.py:53] epoch 6699, testing
[INFO 2017-06-28 14:03:41,525 main.py:105] average testing loss: 10735.36, base loss: 15211.78
[INFO 2017-06-28 14:03:41,525 main.py:106] improve_loss: 4476.42, improve_percent: 0.29
[INFO 2017-06-28 14:03:41,525 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:03:42,170 main.py:51] epoch 6700, training loss: 9427.69, average training loss: 9541.92, base loss: 14364.59
[INFO 2017-06-28 14:03:42,820 main.py:51] epoch 6701, training loss: 9930.39, average training loss: 9540.90, base loss: 14362.54
[INFO 2017-06-28 14:03:43,485 main.py:51] epoch 6702, training loss: 10206.16, average training loss: 9541.13, base loss: 14362.63
[INFO 2017-06-28 14:03:44,153 main.py:51] epoch 6703, training loss: 10090.69, average training loss: 9540.78, base loss: 14362.13
[INFO 2017-06-28 14:03:44,808 main.py:51] epoch 6704, training loss: 12298.81, average training loss: 9543.02, base loss: 14365.17
[INFO 2017-06-28 14:03:45,457 main.py:51] epoch 6705, training loss: 8768.20, average training loss: 9541.70, base loss: 14362.81
[INFO 2017-06-28 14:03:46,107 main.py:51] epoch 6706, training loss: 10068.59, average training loss: 9541.95, base loss: 14362.72
[INFO 2017-06-28 14:03:46,772 main.py:51] epoch 6707, training loss: 10142.03, average training loss: 9542.05, base loss: 14363.85
[INFO 2017-06-28 14:03:47,410 main.py:51] epoch 6708, training loss: 9410.94, average training loss: 9542.54, base loss: 14363.07
[INFO 2017-06-28 14:03:48,077 main.py:51] epoch 6709, training loss: 7671.25, average training loss: 9539.47, base loss: 14358.37
[INFO 2017-06-28 14:03:48,746 main.py:51] epoch 6710, training loss: 9448.13, average training loss: 9538.72, base loss: 14357.55
[INFO 2017-06-28 14:03:49,382 main.py:51] epoch 6711, training loss: 9306.31, average training loss: 9537.14, base loss: 14354.68
[INFO 2017-06-28 14:03:50,056 main.py:51] epoch 6712, training loss: 10543.15, average training loss: 9538.66, base loss: 14356.67
[INFO 2017-06-28 14:03:50,717 main.py:51] epoch 6713, training loss: 9686.67, average training loss: 9538.43, base loss: 14357.58
[INFO 2017-06-28 14:03:51,365 main.py:51] epoch 6714, training loss: 9201.68, average training loss: 9538.91, base loss: 14359.66
[INFO 2017-06-28 14:03:52,016 main.py:51] epoch 6715, training loss: 9348.85, average training loss: 9538.14, base loss: 14359.27
[INFO 2017-06-28 14:03:52,671 main.py:51] epoch 6716, training loss: 8638.97, average training loss: 9537.43, base loss: 14359.16
[INFO 2017-06-28 14:03:53,337 main.py:51] epoch 6717, training loss: 9813.54, average training loss: 9538.00, base loss: 14361.66
[INFO 2017-06-28 14:03:54,022 main.py:51] epoch 6718, training loss: 8806.69, average training loss: 9536.75, base loss: 14360.02
[INFO 2017-06-28 14:03:54,690 main.py:51] epoch 6719, training loss: 9601.82, average training loss: 9537.39, base loss: 14360.35
[INFO 2017-06-28 14:03:55,373 main.py:51] epoch 6720, training loss: 10225.41, average training loss: 9536.81, base loss: 14359.25
[INFO 2017-06-28 14:03:56,043 main.py:51] epoch 6721, training loss: 9561.53, average training loss: 9537.45, base loss: 14361.85
[INFO 2017-06-28 14:03:56,716 main.py:51] epoch 6722, training loss: 8855.87, average training loss: 9536.96, base loss: 14360.64
[INFO 2017-06-28 14:03:57,383 main.py:51] epoch 6723, training loss: 9913.53, average training loss: 9536.03, base loss: 14358.92
[INFO 2017-06-28 14:03:58,016 main.py:51] epoch 6724, training loss: 9558.20, average training loss: 9535.50, base loss: 14356.87
[INFO 2017-06-28 14:03:58,686 main.py:51] epoch 6725, training loss: 10206.96, average training loss: 9535.84, base loss: 14355.89
[INFO 2017-06-28 14:03:59,325 main.py:51] epoch 6726, training loss: 9764.49, average training loss: 9536.19, base loss: 14356.37
[INFO 2017-06-28 14:03:59,976 main.py:51] epoch 6727, training loss: 9906.97, average training loss: 9535.79, base loss: 14355.71
[INFO 2017-06-28 14:04:00,630 main.py:51] epoch 6728, training loss: 8327.66, average training loss: 9534.34, base loss: 14354.35
[INFO 2017-06-28 14:04:01,282 main.py:51] epoch 6729, training loss: 8562.55, average training loss: 9533.41, base loss: 14352.44
[INFO 2017-06-28 14:04:01,923 main.py:51] epoch 6730, training loss: 10176.79, average training loss: 9533.07, base loss: 14350.52
[INFO 2017-06-28 14:04:02,581 main.py:51] epoch 6731, training loss: 9515.55, average training loss: 9532.97, base loss: 14350.94
[INFO 2017-06-28 14:04:03,275 main.py:51] epoch 6732, training loss: 10470.21, average training loss: 9533.64, base loss: 14351.80
[INFO 2017-06-28 14:04:03,934 main.py:51] epoch 6733, training loss: 8421.29, average training loss: 9532.51, base loss: 14349.86
[INFO 2017-06-28 14:04:04,591 main.py:51] epoch 6734, training loss: 8815.46, average training loss: 9532.52, base loss: 14349.75
[INFO 2017-06-28 14:04:05,235 main.py:51] epoch 6735, training loss: 9187.89, average training loss: 9532.15, base loss: 14348.79
[INFO 2017-06-28 14:04:05,875 main.py:51] epoch 6736, training loss: 10243.90, average training loss: 9531.59, base loss: 14349.69
[INFO 2017-06-28 14:04:06,519 main.py:51] epoch 6737, training loss: 8305.96, average training loss: 9529.55, base loss: 14346.11
[INFO 2017-06-28 14:04:07,184 main.py:51] epoch 6738, training loss: 8503.38, average training loss: 9528.33, base loss: 14344.82
[INFO 2017-06-28 14:04:07,840 main.py:51] epoch 6739, training loss: 9219.16, average training loss: 9528.06, base loss: 14345.00
[INFO 2017-06-28 14:04:08,504 main.py:51] epoch 6740, training loss: 8697.59, average training loss: 9526.83, base loss: 14341.71
[INFO 2017-06-28 14:04:09,176 main.py:51] epoch 6741, training loss: 9042.48, average training loss: 9525.85, base loss: 14339.02
[INFO 2017-06-28 14:04:09,825 main.py:51] epoch 6742, training loss: 9969.41, average training loss: 9525.97, base loss: 14337.71
[INFO 2017-06-28 14:04:10,495 main.py:51] epoch 6743, training loss: 10192.99, average training loss: 9527.23, base loss: 14339.19
[INFO 2017-06-28 14:04:11,145 main.py:51] epoch 6744, training loss: 9325.95, average training loss: 9527.35, base loss: 14339.65
[INFO 2017-06-28 14:04:11,799 main.py:51] epoch 6745, training loss: 8561.15, average training loss: 9526.34, base loss: 14340.22
[INFO 2017-06-28 14:04:12,470 main.py:51] epoch 6746, training loss: 9150.35, average training loss: 9525.57, base loss: 14337.88
[INFO 2017-06-28 14:04:13,133 main.py:51] epoch 6747, training loss: 9952.32, average training loss: 9526.25, base loss: 14339.89
[INFO 2017-06-28 14:04:13,790 main.py:51] epoch 6748, training loss: 8579.62, average training loss: 9525.92, base loss: 14340.11
[INFO 2017-06-28 14:04:14,423 main.py:51] epoch 6749, training loss: 8254.80, average training loss: 9522.74, base loss: 14335.13
[INFO 2017-06-28 14:04:15,072 main.py:51] epoch 6750, training loss: 10740.69, average training loss: 9524.11, base loss: 14338.01
[INFO 2017-06-28 14:04:15,731 main.py:51] epoch 6751, training loss: 9975.08, average training loss: 9523.72, base loss: 14336.82
[INFO 2017-06-28 14:04:16,398 main.py:51] epoch 6752, training loss: 10721.17, average training loss: 9524.64, base loss: 14339.60
[INFO 2017-06-28 14:04:17,057 main.py:51] epoch 6753, training loss: 10706.74, average training loss: 9525.33, base loss: 14340.86
[INFO 2017-06-28 14:04:17,710 main.py:51] epoch 6754, training loss: 8849.37, average training loss: 9524.08, base loss: 14339.08
[INFO 2017-06-28 14:04:18,378 main.py:51] epoch 6755, training loss: 8613.22, average training loss: 9523.29, base loss: 14338.46
[INFO 2017-06-28 14:04:19,043 main.py:51] epoch 6756, training loss: 9085.47, average training loss: 9522.40, base loss: 14337.84
[INFO 2017-06-28 14:04:19,722 main.py:51] epoch 6757, training loss: 8323.23, average training loss: 9520.88, base loss: 14333.69
[INFO 2017-06-28 14:04:20,382 main.py:51] epoch 6758, training loss: 8326.94, average training loss: 9518.85, base loss: 14330.72
[INFO 2017-06-28 14:04:21,045 main.py:51] epoch 6759, training loss: 9705.53, average training loss: 9519.69, base loss: 14332.43
[INFO 2017-06-28 14:04:21,711 main.py:51] epoch 6760, training loss: 9265.31, average training loss: 9520.21, base loss: 14332.44
[INFO 2017-06-28 14:04:22,400 main.py:51] epoch 6761, training loss: 9112.11, average training loss: 9521.43, base loss: 14335.48
[INFO 2017-06-28 14:04:23,064 main.py:51] epoch 6762, training loss: 8681.05, average training loss: 9520.48, base loss: 14332.78
[INFO 2017-06-28 14:04:23,711 main.py:51] epoch 6763, training loss: 8498.88, average training loss: 9519.90, base loss: 14331.18
[INFO 2017-06-28 14:04:24,362 main.py:51] epoch 6764, training loss: 9300.72, average training loss: 9518.92, base loss: 14331.39
[INFO 2017-06-28 14:04:25,019 main.py:51] epoch 6765, training loss: 9990.23, average training loss: 9517.27, base loss: 14329.44
[INFO 2017-06-28 14:04:25,661 main.py:51] epoch 6766, training loss: 10238.04, average training loss: 9517.79, base loss: 14327.82
[INFO 2017-06-28 14:04:26,323 main.py:51] epoch 6767, training loss: 8887.42, average training loss: 9517.21, base loss: 14327.08
[INFO 2017-06-28 14:04:26,979 main.py:51] epoch 6768, training loss: 9822.34, average training loss: 9518.64, base loss: 14330.86
[INFO 2017-06-28 14:04:27,636 main.py:51] epoch 6769, training loss: 10463.42, average training loss: 9518.79, base loss: 14329.67
[INFO 2017-06-28 14:04:28,312 main.py:51] epoch 6770, training loss: 9061.32, average training loss: 9519.32, base loss: 14331.29
[INFO 2017-06-28 14:04:28,958 main.py:51] epoch 6771, training loss: 9860.92, average training loss: 9518.54, base loss: 14330.52
[INFO 2017-06-28 14:04:29,640 main.py:51] epoch 6772, training loss: 10030.73, average training loss: 9519.54, base loss: 14332.38
[INFO 2017-06-28 14:04:30,296 main.py:51] epoch 6773, training loss: 8870.78, average training loss: 9518.74, base loss: 14332.44
[INFO 2017-06-28 14:04:30,943 main.py:51] epoch 6774, training loss: 9507.87, average training loss: 9519.24, base loss: 14334.03
[INFO 2017-06-28 14:04:31,611 main.py:51] epoch 6775, training loss: 10274.65, average training loss: 9519.81, base loss: 14335.63
[INFO 2017-06-28 14:04:32,285 main.py:51] epoch 6776, training loss: 11607.56, average training loss: 9521.88, base loss: 14337.69
[INFO 2017-06-28 14:04:32,934 main.py:51] epoch 6777, training loss: 11491.51, average training loss: 9524.74, base loss: 14341.09
[INFO 2017-06-28 14:04:33,566 main.py:51] epoch 6778, training loss: 9126.77, average training loss: 9523.58, base loss: 14339.99
[INFO 2017-06-28 14:04:34,208 main.py:51] epoch 6779, training loss: 11253.61, average training loss: 9524.90, base loss: 14341.82
[INFO 2017-06-28 14:04:34,860 main.py:51] epoch 6780, training loss: 9228.60, average training loss: 9524.73, base loss: 14339.80
[INFO 2017-06-28 14:04:35,525 main.py:51] epoch 6781, training loss: 9114.71, average training loss: 9524.18, base loss: 14337.85
[INFO 2017-06-28 14:04:36,173 main.py:51] epoch 6782, training loss: 10492.65, average training loss: 9525.16, base loss: 14339.02
[INFO 2017-06-28 14:04:36,832 main.py:51] epoch 6783, training loss: 9406.04, average training loss: 9523.68, base loss: 14337.25
[INFO 2017-06-28 14:04:37,491 main.py:51] epoch 6784, training loss: 8888.38, average training loss: 9523.68, base loss: 14338.80
[INFO 2017-06-28 14:04:38,143 main.py:51] epoch 6785, training loss: 10201.37, average training loss: 9525.23, base loss: 14341.97
[INFO 2017-06-28 14:04:38,812 main.py:51] epoch 6786, training loss: 8632.81, average training loss: 9523.55, base loss: 14341.07
[INFO 2017-06-28 14:04:39,459 main.py:51] epoch 6787, training loss: 9062.56, average training loss: 9521.55, base loss: 14337.41
[INFO 2017-06-28 14:04:40,111 main.py:51] epoch 6788, training loss: 8267.09, average training loss: 9521.36, base loss: 14336.60
[INFO 2017-06-28 14:04:40,764 main.py:51] epoch 6789, training loss: 10913.55, average training loss: 9524.25, base loss: 14342.24
[INFO 2017-06-28 14:04:41,410 main.py:51] epoch 6790, training loss: 9455.85, average training loss: 9524.21, base loss: 14340.56
[INFO 2017-06-28 14:04:42,086 main.py:51] epoch 6791, training loss: 9821.05, average training loss: 9525.14, base loss: 14340.88
[INFO 2017-06-28 14:04:42,724 main.py:51] epoch 6792, training loss: 9329.80, average training loss: 9524.44, base loss: 14340.62
[INFO 2017-06-28 14:04:43,388 main.py:51] epoch 6793, training loss: 10210.20, average training loss: 9525.83, base loss: 14343.74
[INFO 2017-06-28 14:04:44,056 main.py:51] epoch 6794, training loss: 9737.58, average training loss: 9526.74, base loss: 14345.09
[INFO 2017-06-28 14:04:44,702 main.py:51] epoch 6795, training loss: 9447.13, average training loss: 9525.63, base loss: 14344.44
[INFO 2017-06-28 14:04:45,360 main.py:51] epoch 6796, training loss: 9610.89, average training loss: 9525.10, base loss: 14343.22
[INFO 2017-06-28 14:04:46,030 main.py:51] epoch 6797, training loss: 9483.89, average training loss: 9526.59, base loss: 14345.86
[INFO 2017-06-28 14:04:46,679 main.py:51] epoch 6798, training loss: 10339.78, average training loss: 9526.69, base loss: 14347.07
[INFO 2017-06-28 14:04:47,313 main.py:51] epoch 6799, training loss: 10859.99, average training loss: 9528.39, base loss: 14349.16
[INFO 2017-06-28 14:04:47,313 main.py:53] epoch 6799, testing
[INFO 2017-06-28 14:04:49,896 main.py:105] average testing loss: 10892.31, base loss: 15489.29
[INFO 2017-06-28 14:04:49,897 main.py:106] improve_loss: 4596.97, improve_percent: 0.30
[INFO 2017-06-28 14:04:49,897 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:04:50,542 main.py:51] epoch 6800, training loss: 9442.91, average training loss: 9527.92, base loss: 14347.96
[INFO 2017-06-28 14:04:51,193 main.py:51] epoch 6801, training loss: 8697.30, average training loss: 9527.58, base loss: 14348.29
[INFO 2017-06-28 14:04:51,852 main.py:51] epoch 6802, training loss: 9260.48, average training loss: 9525.91, base loss: 14343.60
[INFO 2017-06-28 14:04:52,514 main.py:51] epoch 6803, training loss: 9077.78, average training loss: 9525.56, base loss: 14343.82
[INFO 2017-06-28 14:04:53,164 main.py:51] epoch 6804, training loss: 8575.40, average training loss: 9524.59, base loss: 14342.59
[INFO 2017-06-28 14:04:53,829 main.py:51] epoch 6805, training loss: 9004.35, average training loss: 9524.99, base loss: 14343.11
[INFO 2017-06-28 14:04:54,485 main.py:51] epoch 6806, training loss: 9607.99, average training loss: 9523.40, base loss: 14340.48
[INFO 2017-06-28 14:04:55,153 main.py:51] epoch 6807, training loss: 8031.06, average training loss: 9521.60, base loss: 14338.22
[INFO 2017-06-28 14:04:55,826 main.py:51] epoch 6808, training loss: 10129.82, average training loss: 9521.72, base loss: 14335.95
[INFO 2017-06-28 14:04:56,487 main.py:51] epoch 6809, training loss: 7872.56, average training loss: 9521.08, base loss: 14333.95
[INFO 2017-06-28 14:04:57,143 main.py:51] epoch 6810, training loss: 8643.55, average training loss: 9520.12, base loss: 14333.44
[INFO 2017-06-28 14:04:57,791 main.py:51] epoch 6811, training loss: 8944.58, average training loss: 9519.91, base loss: 14331.43
[INFO 2017-06-28 14:04:58,443 main.py:51] epoch 6812, training loss: 9923.27, average training loss: 9519.67, base loss: 14331.95
[INFO 2017-06-28 14:04:59,094 main.py:51] epoch 6813, training loss: 8940.66, average training loss: 9519.61, base loss: 14331.45
[INFO 2017-06-28 14:04:59,738 main.py:51] epoch 6814, training loss: 9456.00, average training loss: 9520.49, base loss: 14333.38
[INFO 2017-06-28 14:05:00,392 main.py:51] epoch 6815, training loss: 9422.24, average training loss: 9520.18, base loss: 14332.02
[INFO 2017-06-28 14:05:01,057 main.py:51] epoch 6816, training loss: 9158.45, average training loss: 9520.34, base loss: 14332.66
[INFO 2017-06-28 14:05:01,730 main.py:51] epoch 6817, training loss: 8964.51, average training loss: 9520.63, base loss: 14333.20
[INFO 2017-06-28 14:05:02,383 main.py:51] epoch 6818, training loss: 9027.07, average training loss: 9521.17, base loss: 14333.98
[INFO 2017-06-28 14:05:03,029 main.py:51] epoch 6819, training loss: 9900.15, average training loss: 9521.94, base loss: 14336.20
[INFO 2017-06-28 14:05:03,700 main.py:51] epoch 6820, training loss: 10517.27, average training loss: 9522.40, base loss: 14337.91
[INFO 2017-06-28 14:05:04,361 main.py:51] epoch 6821, training loss: 9771.06, average training loss: 9523.57, base loss: 14338.15
[INFO 2017-06-28 14:05:05,009 main.py:51] epoch 6822, training loss: 8961.90, average training loss: 9522.53, base loss: 14337.54
[INFO 2017-06-28 14:05:05,658 main.py:51] epoch 6823, training loss: 9959.57, average training loss: 9522.94, base loss: 14337.55
[INFO 2017-06-28 14:05:06,305 main.py:51] epoch 6824, training loss: 11034.51, average training loss: 9524.83, base loss: 14339.46
[INFO 2017-06-28 14:05:06,971 main.py:51] epoch 6825, training loss: 8608.77, average training loss: 9522.81, base loss: 14336.41
[INFO 2017-06-28 14:05:07,629 main.py:51] epoch 6826, training loss: 10254.44, average training loss: 9523.50, base loss: 14339.67
[INFO 2017-06-28 14:05:08,272 main.py:51] epoch 6827, training loss: 10687.27, average training loss: 9524.73, base loss: 14341.30
[INFO 2017-06-28 14:05:08,932 main.py:51] epoch 6828, training loss: 7870.93, average training loss: 9523.57, base loss: 14339.88
[INFO 2017-06-28 14:05:09,575 main.py:51] epoch 6829, training loss: 9486.22, average training loss: 9524.21, base loss: 14340.09
[INFO 2017-06-28 14:05:10,232 main.py:51] epoch 6830, training loss: 9369.27, average training loss: 9524.48, base loss: 14342.21
[INFO 2017-06-28 14:05:10,882 main.py:51] epoch 6831, training loss: 9199.95, average training loss: 9525.44, base loss: 14343.65
[INFO 2017-06-28 14:05:11,544 main.py:51] epoch 6832, training loss: 10856.37, average training loss: 9527.18, base loss: 14345.53
[INFO 2017-06-28 14:05:12,195 main.py:51] epoch 6833, training loss: 9135.36, average training loss: 9527.62, base loss: 14346.06
[INFO 2017-06-28 14:05:12,843 main.py:51] epoch 6834, training loss: 9202.85, average training loss: 9528.02, base loss: 14346.07
[INFO 2017-06-28 14:05:13,497 main.py:51] epoch 6835, training loss: 9356.14, average training loss: 9527.75, base loss: 14346.55
[INFO 2017-06-28 14:05:14,130 main.py:51] epoch 6836, training loss: 9503.43, average training loss: 9527.43, base loss: 14347.31
[INFO 2017-06-28 14:05:14,792 main.py:51] epoch 6837, training loss: 9118.03, average training loss: 9526.91, base loss: 14344.99
[INFO 2017-06-28 14:05:15,447 main.py:51] epoch 6838, training loss: 8789.14, average training loss: 9527.31, base loss: 14345.47
[INFO 2017-06-28 14:05:16,094 main.py:51] epoch 6839, training loss: 11075.68, average training loss: 9529.59, base loss: 14349.52
[INFO 2017-06-28 14:05:16,749 main.py:51] epoch 6840, training loss: 8933.49, average training loss: 9529.22, base loss: 14348.78
[INFO 2017-06-28 14:05:17,401 main.py:51] epoch 6841, training loss: 9549.29, average training loss: 9529.19, base loss: 14348.75
[INFO 2017-06-28 14:05:18,063 main.py:51] epoch 6842, training loss: 10009.98, average training loss: 9530.30, base loss: 14350.93
[INFO 2017-06-28 14:05:18,730 main.py:51] epoch 6843, training loss: 10957.52, average training loss: 9532.28, base loss: 14355.80
[INFO 2017-06-28 14:05:19,396 main.py:51] epoch 6844, training loss: 9148.26, average training loss: 9531.43, base loss: 14353.80
[INFO 2017-06-28 14:05:20,044 main.py:51] epoch 6845, training loss: 9478.29, average training loss: 9530.39, base loss: 14351.51
[INFO 2017-06-28 14:05:20,695 main.py:51] epoch 6846, training loss: 9644.98, average training loss: 9530.21, base loss: 14350.39
[INFO 2017-06-28 14:05:21,341 main.py:51] epoch 6847, training loss: 9450.90, average training loss: 9530.86, base loss: 14351.46
[INFO 2017-06-28 14:05:21,992 main.py:51] epoch 6848, training loss: 9174.04, average training loss: 9530.94, base loss: 14351.38
[INFO 2017-06-28 14:05:22,669 main.py:51] epoch 6849, training loss: 8238.28, average training loss: 9529.79, base loss: 14348.91
[INFO 2017-06-28 14:05:23,329 main.py:51] epoch 6850, training loss: 9670.59, average training loss: 9530.54, base loss: 14350.78
[INFO 2017-06-28 14:05:23,994 main.py:51] epoch 6851, training loss: 9516.21, average training loss: 9530.54, base loss: 14349.34
[INFO 2017-06-28 14:05:24,640 main.py:51] epoch 6852, training loss: 9774.41, average training loss: 9530.08, base loss: 14348.27
[INFO 2017-06-28 14:05:25,299 main.py:51] epoch 6853, training loss: 9922.05, average training loss: 9530.09, base loss: 14348.34
[INFO 2017-06-28 14:05:25,931 main.py:51] epoch 6854, training loss: 8236.10, average training loss: 9528.50, base loss: 14346.83
[INFO 2017-06-28 14:05:26,579 main.py:51] epoch 6855, training loss: 8486.47, average training loss: 9527.48, base loss: 14346.07
[INFO 2017-06-28 14:05:27,236 main.py:51] epoch 6856, training loss: 10488.46, average training loss: 9528.00, base loss: 14345.39
[INFO 2017-06-28 14:05:27,925 main.py:51] epoch 6857, training loss: 10132.00, average training loss: 9527.18, base loss: 14345.17
[INFO 2017-06-28 14:05:28,591 main.py:51] epoch 6858, training loss: 10381.77, average training loss: 9529.17, base loss: 14347.62
[INFO 2017-06-28 14:05:29,243 main.py:51] epoch 6859, training loss: 9814.32, average training loss: 9528.08, base loss: 14347.38
[INFO 2017-06-28 14:05:29,916 main.py:51] epoch 6860, training loss: 9820.35, average training loss: 9528.99, base loss: 14349.08
[INFO 2017-06-28 14:05:30,571 main.py:51] epoch 6861, training loss: 8706.54, average training loss: 9526.94, base loss: 14346.09
[INFO 2017-06-28 14:05:31,257 main.py:51] epoch 6862, training loss: 10557.56, average training loss: 9528.39, base loss: 14349.86
[INFO 2017-06-28 14:05:31,923 main.py:51] epoch 6863, training loss: 9618.47, average training loss: 9529.71, base loss: 14351.49
[INFO 2017-06-28 14:05:32,573 main.py:51] epoch 6864, training loss: 8801.79, average training loss: 9530.46, base loss: 14352.35
[INFO 2017-06-28 14:05:33,269 main.py:51] epoch 6865, training loss: 9065.55, average training loss: 9529.62, base loss: 14350.15
[INFO 2017-06-28 14:05:33,910 main.py:51] epoch 6866, training loss: 9472.46, average training loss: 9528.94, base loss: 14348.72
[INFO 2017-06-28 14:05:34,560 main.py:51] epoch 6867, training loss: 9248.67, average training loss: 9527.00, base loss: 14347.18
[INFO 2017-06-28 14:05:35,238 main.py:51] epoch 6868, training loss: 9021.84, average training loss: 9527.20, base loss: 14349.28
[INFO 2017-06-28 14:05:35,901 main.py:51] epoch 6869, training loss: 8851.55, average training loss: 9527.59, base loss: 14350.16
[INFO 2017-06-28 14:05:36,570 main.py:51] epoch 6870, training loss: 8715.25, average training loss: 9526.70, base loss: 14349.51
[INFO 2017-06-28 14:05:37,233 main.py:51] epoch 6871, training loss: 9590.11, average training loss: 9526.80, base loss: 14351.51
[INFO 2017-06-28 14:05:37,914 main.py:51] epoch 6872, training loss: 9484.99, average training loss: 9527.16, base loss: 14352.07
[INFO 2017-06-28 14:05:38,558 main.py:51] epoch 6873, training loss: 9682.66, average training loss: 9526.27, base loss: 14349.76
[INFO 2017-06-28 14:05:39,202 main.py:51] epoch 6874, training loss: 8267.68, average training loss: 9524.12, base loss: 14345.89
[INFO 2017-06-28 14:05:39,850 main.py:51] epoch 6875, training loss: 10006.53, average training loss: 9524.78, base loss: 14346.24
[INFO 2017-06-28 14:05:40,501 main.py:51] epoch 6876, training loss: 8664.66, average training loss: 9525.05, base loss: 14345.22
[INFO 2017-06-28 14:05:41,154 main.py:51] epoch 6877, training loss: 8207.09, average training loss: 9522.94, base loss: 14342.63
[INFO 2017-06-28 14:05:41,790 main.py:51] epoch 6878, training loss: 9228.96, average training loss: 9522.58, base loss: 14341.56
[INFO 2017-06-28 14:05:42,466 main.py:51] epoch 6879, training loss: 9131.47, average training loss: 9521.96, base loss: 14340.99
[INFO 2017-06-28 14:05:43,148 main.py:51] epoch 6880, training loss: 9301.10, average training loss: 9521.26, base loss: 14340.92
[INFO 2017-06-28 14:05:43,805 main.py:51] epoch 6881, training loss: 8867.89, average training loss: 9521.14, base loss: 14340.87
[INFO 2017-06-28 14:05:44,461 main.py:51] epoch 6882, training loss: 10597.95, average training loss: 9520.83, base loss: 14340.15
[INFO 2017-06-28 14:05:45,112 main.py:51] epoch 6883, training loss: 9401.23, average training loss: 9520.65, base loss: 14339.93
[INFO 2017-06-28 14:05:45,755 main.py:51] epoch 6884, training loss: 9522.05, average training loss: 9521.17, base loss: 14341.72
[INFO 2017-06-28 14:05:46,433 main.py:51] epoch 6885, training loss: 9688.25, average training loss: 9520.52, base loss: 14339.51
[INFO 2017-06-28 14:05:47,103 main.py:51] epoch 6886, training loss: 7831.87, average training loss: 9519.18, base loss: 14336.20
[INFO 2017-06-28 14:05:47,786 main.py:51] epoch 6887, training loss: 8685.64, average training loss: 9517.92, base loss: 14333.63
[INFO 2017-06-28 14:05:48,426 main.py:51] epoch 6888, training loss: 10317.08, average training loss: 9519.10, base loss: 14336.60
[INFO 2017-06-28 14:05:49,079 main.py:51] epoch 6889, training loss: 10238.65, average training loss: 9520.27, base loss: 14339.19
[INFO 2017-06-28 14:05:49,709 main.py:51] epoch 6890, training loss: 9953.16, average training loss: 9519.81, base loss: 14339.49
[INFO 2017-06-28 14:05:50,354 main.py:51] epoch 6891, training loss: 8921.06, average training loss: 9518.49, base loss: 14338.27
[INFO 2017-06-28 14:05:51,021 main.py:51] epoch 6892, training loss: 9662.58, average training loss: 9518.79, base loss: 14339.12
[INFO 2017-06-28 14:05:51,670 main.py:51] epoch 6893, training loss: 9159.90, average training loss: 9518.90, base loss: 14339.32
[INFO 2017-06-28 14:05:52,331 main.py:51] epoch 6894, training loss: 10490.55, average training loss: 9519.69, base loss: 14341.27
[INFO 2017-06-28 14:05:52,981 main.py:51] epoch 6895, training loss: 9383.75, average training loss: 9518.87, base loss: 14340.44
[INFO 2017-06-28 14:05:53,630 main.py:51] epoch 6896, training loss: 9938.81, average training loss: 9518.71, base loss: 14339.89
[INFO 2017-06-28 14:05:54,275 main.py:51] epoch 6897, training loss: 9004.10, average training loss: 9517.47, base loss: 14337.75
[INFO 2017-06-28 14:05:54,930 main.py:51] epoch 6898, training loss: 10161.28, average training loss: 9517.98, base loss: 14339.93
[INFO 2017-06-28 14:05:55,585 main.py:51] epoch 6899, training loss: 9801.33, average training loss: 9519.48, base loss: 14344.18
[INFO 2017-06-28 14:05:55,585 main.py:53] epoch 6899, testing
[INFO 2017-06-28 14:05:58,157 main.py:105] average testing loss: 11289.77, base loss: 15804.97
[INFO 2017-06-28 14:05:58,157 main.py:106] improve_loss: 4515.20, improve_percent: 0.29
[INFO 2017-06-28 14:05:58,158 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:05:58,821 main.py:51] epoch 6900, training loss: 8642.14, average training loss: 9519.00, base loss: 14342.29
[INFO 2017-06-28 14:05:59,526 main.py:51] epoch 6901, training loss: 9513.06, average training loss: 9518.05, base loss: 14342.20
[INFO 2017-06-28 14:06:00,182 main.py:51] epoch 6902, training loss: 9779.65, average training loss: 9518.86, base loss: 14342.67
[INFO 2017-06-28 14:06:00,838 main.py:51] epoch 6903, training loss: 11120.84, average training loss: 9521.86, base loss: 14347.56
[INFO 2017-06-28 14:06:01,506 main.py:51] epoch 6904, training loss: 9663.05, average training loss: 9522.51, base loss: 14347.16
[INFO 2017-06-28 14:06:02,172 main.py:51] epoch 6905, training loss: 8745.14, average training loss: 9521.02, base loss: 14343.83
[INFO 2017-06-28 14:06:02,816 main.py:51] epoch 6906, training loss: 9056.60, average training loss: 9520.46, base loss: 14343.10
[INFO 2017-06-28 14:06:03,500 main.py:51] epoch 6907, training loss: 10332.19, average training loss: 9521.57, base loss: 14346.34
[INFO 2017-06-28 14:06:04,168 main.py:51] epoch 6908, training loss: 9117.20, average training loss: 9521.75, base loss: 14347.40
[INFO 2017-06-28 14:06:04,829 main.py:51] epoch 6909, training loss: 9364.53, average training loss: 9519.35, base loss: 14343.27
[INFO 2017-06-28 14:06:05,457 main.py:51] epoch 6910, training loss: 9085.74, average training loss: 9518.32, base loss: 14342.00
[INFO 2017-06-28 14:06:06,130 main.py:51] epoch 6911, training loss: 10172.93, average training loss: 9519.01, base loss: 14341.12
[INFO 2017-06-28 14:06:06,767 main.py:51] epoch 6912, training loss: 9656.18, average training loss: 9519.61, base loss: 14342.55
[INFO 2017-06-28 14:06:07,413 main.py:51] epoch 6913, training loss: 10564.53, average training loss: 9520.45, base loss: 14344.78
[INFO 2017-06-28 14:06:08,055 main.py:51] epoch 6914, training loss: 9301.68, average training loss: 9519.75, base loss: 14344.40
[INFO 2017-06-28 14:06:08,710 main.py:51] epoch 6915, training loss: 9458.14, average training loss: 9521.13, base loss: 14346.18
[INFO 2017-06-28 14:06:09,360 main.py:51] epoch 6916, training loss: 10147.84, average training loss: 9521.81, base loss: 14345.93
[INFO 2017-06-28 14:06:10,008 main.py:51] epoch 6917, training loss: 10675.03, average training loss: 9523.83, base loss: 14349.19
[INFO 2017-06-28 14:06:10,663 main.py:51] epoch 6918, training loss: 11134.19, average training loss: 9525.81, base loss: 14351.30
[INFO 2017-06-28 14:06:11,313 main.py:51] epoch 6919, training loss: 8106.46, average training loss: 9523.41, base loss: 14349.76
[INFO 2017-06-28 14:06:11,979 main.py:51] epoch 6920, training loss: 10451.97, average training loss: 9522.67, base loss: 14348.12
[INFO 2017-06-28 14:06:12,637 main.py:51] epoch 6921, training loss: 9198.59, average training loss: 9521.89, base loss: 14346.45
[INFO 2017-06-28 14:06:13,304 main.py:51] epoch 6922, training loss: 9161.47, average training loss: 9522.46, base loss: 14347.18
[INFO 2017-06-28 14:06:13,995 main.py:51] epoch 6923, training loss: 10521.78, average training loss: 9523.40, base loss: 14348.25
[INFO 2017-06-28 14:06:14,664 main.py:51] epoch 6924, training loss: 9847.84, average training loss: 9524.96, base loss: 14349.96
[INFO 2017-06-28 14:06:15,370 main.py:51] epoch 6925, training loss: 8695.57, average training loss: 9524.75, base loss: 14350.78
[INFO 2017-06-28 14:06:16,021 main.py:51] epoch 6926, training loss: 8851.63, average training loss: 9524.50, base loss: 14350.80
[INFO 2017-06-28 14:06:16,663 main.py:51] epoch 6927, training loss: 8815.51, average training loss: 9524.52, base loss: 14351.55
[INFO 2017-06-28 14:06:17,344 main.py:51] epoch 6928, training loss: 10113.88, average training loss: 9525.03, base loss: 14351.51
[INFO 2017-06-28 14:06:18,008 main.py:51] epoch 6929, training loss: 9800.41, average training loss: 9527.31, base loss: 14355.36
[INFO 2017-06-28 14:06:18,657 main.py:51] epoch 6930, training loss: 9053.91, average training loss: 9525.61, base loss: 14354.24
[INFO 2017-06-28 14:06:19,311 main.py:51] epoch 6931, training loss: 8558.19, average training loss: 9523.55, base loss: 14352.36
[INFO 2017-06-28 14:06:19,985 main.py:51] epoch 6932, training loss: 9179.75, average training loss: 9524.14, base loss: 14353.70
[INFO 2017-06-28 14:06:20,639 main.py:51] epoch 6933, training loss: 9651.49, average training loss: 9523.78, base loss: 14352.48
[INFO 2017-06-28 14:06:21,279 main.py:51] epoch 6934, training loss: 9858.27, average training loss: 9524.45, base loss: 14354.41
[INFO 2017-06-28 14:06:21,938 main.py:51] epoch 6935, training loss: 10193.87, average training loss: 9525.96, base loss: 14356.06
[INFO 2017-06-28 14:06:22,597 main.py:51] epoch 6936, training loss: 9081.24, average training loss: 9525.71, base loss: 14355.08
[INFO 2017-06-28 14:06:23,259 main.py:51] epoch 6937, training loss: 9125.37, average training loss: 9525.39, base loss: 14353.99
[INFO 2017-06-28 14:06:23,919 main.py:51] epoch 6938, training loss: 10663.21, average training loss: 9526.51, base loss: 14355.73
[INFO 2017-06-28 14:06:24,587 main.py:51] epoch 6939, training loss: 8134.95, average training loss: 9523.68, base loss: 14350.89
[INFO 2017-06-28 14:06:25,258 main.py:51] epoch 6940, training loss: 9884.44, average training loss: 9524.21, base loss: 14351.93
[INFO 2017-06-28 14:06:25,908 main.py:51] epoch 6941, training loss: 9806.96, average training loss: 9524.38, base loss: 14352.49
[INFO 2017-06-28 14:06:26,587 main.py:51] epoch 6942, training loss: 11038.55, average training loss: 9525.93, base loss: 14355.77
[INFO 2017-06-28 14:06:27,231 main.py:51] epoch 6943, training loss: 7572.35, average training loss: 9523.82, base loss: 14354.52
[INFO 2017-06-28 14:06:27,895 main.py:51] epoch 6944, training loss: 9216.97, average training loss: 9523.66, base loss: 14353.98
[INFO 2017-06-28 14:06:28,555 main.py:51] epoch 6945, training loss: 9640.94, average training loss: 9523.51, base loss: 14354.42
[INFO 2017-06-28 14:06:29,209 main.py:51] epoch 6946, training loss: 8503.12, average training loss: 9522.21, base loss: 14353.43
[INFO 2017-06-28 14:06:29,922 main.py:51] epoch 6947, training loss: 8991.09, average training loss: 9521.75, base loss: 14351.84
[INFO 2017-06-28 14:06:30,580 main.py:51] epoch 6948, training loss: 8689.06, average training loss: 9520.37, base loss: 14349.93
[INFO 2017-06-28 14:06:31,264 main.py:51] epoch 6949, training loss: 8286.76, average training loss: 9518.34, base loss: 14347.48
[INFO 2017-06-28 14:06:31,961 main.py:51] epoch 6950, training loss: 8173.34, average training loss: 9517.64, base loss: 14347.98
[INFO 2017-06-28 14:06:32,616 main.py:51] epoch 6951, training loss: 9123.12, average training loss: 9517.42, base loss: 14347.28
[INFO 2017-06-28 14:06:33,284 main.py:51] epoch 6952, training loss: 8597.22, average training loss: 9517.77, base loss: 14349.13
[INFO 2017-06-28 14:06:33,922 main.py:51] epoch 6953, training loss: 8849.57, average training loss: 9517.35, base loss: 14347.06
[INFO 2017-06-28 14:06:34,585 main.py:51] epoch 6954, training loss: 9543.94, average training loss: 9517.51, base loss: 14346.70
[INFO 2017-06-28 14:06:35,261 main.py:51] epoch 6955, training loss: 8781.97, average training loss: 9517.23, base loss: 14346.79
[INFO 2017-06-28 14:06:35,919 main.py:51] epoch 6956, training loss: 9976.40, average training loss: 9518.10, base loss: 14348.72
[INFO 2017-06-28 14:06:36,618 main.py:51] epoch 6957, training loss: 8951.41, average training loss: 9518.73, base loss: 14348.28
[INFO 2017-06-28 14:06:37,273 main.py:51] epoch 6958, training loss: 7432.66, average training loss: 9516.12, base loss: 14345.24
[INFO 2017-06-28 14:06:37,934 main.py:51] epoch 6959, training loss: 9584.30, average training loss: 9516.15, base loss: 14345.14
[INFO 2017-06-28 14:06:38,611 main.py:51] epoch 6960, training loss: 9228.74, average training loss: 9516.18, base loss: 14346.14
[INFO 2017-06-28 14:06:39,270 main.py:51] epoch 6961, training loss: 9840.13, average training loss: 9517.63, base loss: 14348.53
[INFO 2017-06-28 14:06:39,935 main.py:51] epoch 6962, training loss: 9765.44, average training loss: 9517.91, base loss: 14349.41
[INFO 2017-06-28 14:06:40,575 main.py:51] epoch 6963, training loss: 9810.24, average training loss: 9517.54, base loss: 14349.98
[INFO 2017-06-28 14:06:41,243 main.py:51] epoch 6964, training loss: 9705.78, average training loss: 9517.39, base loss: 14351.38
[INFO 2017-06-28 14:06:41,919 main.py:51] epoch 6965, training loss: 8691.95, average training loss: 9517.67, base loss: 14353.27
[INFO 2017-06-28 14:06:42,589 main.py:51] epoch 6966, training loss: 9805.12, average training loss: 9516.32, base loss: 14351.56
[INFO 2017-06-28 14:06:43,245 main.py:51] epoch 6967, training loss: 11230.31, average training loss: 9518.57, base loss: 14355.33
[INFO 2017-06-28 14:06:43,915 main.py:51] epoch 6968, training loss: 9327.71, average training loss: 9518.07, base loss: 14354.43
[INFO 2017-06-28 14:06:44,565 main.py:51] epoch 6969, training loss: 8900.46, average training loss: 9517.42, base loss: 14356.19
[INFO 2017-06-28 14:06:45,246 main.py:51] epoch 6970, training loss: 8879.34, average training loss: 9517.77, base loss: 14358.28
[INFO 2017-06-28 14:06:45,887 main.py:51] epoch 6971, training loss: 8473.87, average training loss: 9517.57, base loss: 14357.39
[INFO 2017-06-28 14:06:46,550 main.py:51] epoch 6972, training loss: 8875.74, average training loss: 9516.77, base loss: 14354.75
[INFO 2017-06-28 14:06:47,219 main.py:51] epoch 6973, training loss: 10166.54, average training loss: 9515.74, base loss: 14353.69
[INFO 2017-06-28 14:06:47,889 main.py:51] epoch 6974, training loss: 8041.85, average training loss: 9513.57, base loss: 14350.75
[INFO 2017-06-28 14:06:48,535 main.py:51] epoch 6975, training loss: 8441.27, average training loss: 9513.55, base loss: 14353.41
[INFO 2017-06-28 14:06:49,211 main.py:51] epoch 6976, training loss: 9374.13, average training loss: 9513.17, base loss: 14353.36
[INFO 2017-06-28 14:06:49,845 main.py:51] epoch 6977, training loss: 9508.40, average training loss: 9512.10, base loss: 14351.81
[INFO 2017-06-28 14:06:50,515 main.py:51] epoch 6978, training loss: 9689.85, average training loss: 9512.24, base loss: 14353.16
[INFO 2017-06-28 14:06:51,175 main.py:51] epoch 6979, training loss: 10602.64, average training loss: 9512.73, base loss: 14354.90
[INFO 2017-06-28 14:06:51,822 main.py:51] epoch 6980, training loss: 9191.19, average training loss: 9512.42, base loss: 14353.54
[INFO 2017-06-28 14:06:52,478 main.py:51] epoch 6981, training loss: 9983.75, average training loss: 9512.43, base loss: 14353.11
[INFO 2017-06-28 14:06:53,144 main.py:51] epoch 6982, training loss: 9328.03, average training loss: 9512.70, base loss: 14352.42
[INFO 2017-06-28 14:06:53,785 main.py:51] epoch 6983, training loss: 10181.45, average training loss: 9515.40, base loss: 14356.40
[INFO 2017-06-28 14:06:54,438 main.py:51] epoch 6984, training loss: 9590.61, average training loss: 9513.87, base loss: 14352.23
[INFO 2017-06-28 14:06:55,102 main.py:51] epoch 6985, training loss: 10925.56, average training loss: 9515.86, base loss: 14355.42
[INFO 2017-06-28 14:06:55,786 main.py:51] epoch 6986, training loss: 9331.75, average training loss: 9516.75, base loss: 14356.52
[INFO 2017-06-28 14:06:56,442 main.py:51] epoch 6987, training loss: 9791.09, average training loss: 9516.31, base loss: 14355.34
[INFO 2017-06-28 14:06:57,084 main.py:51] epoch 6988, training loss: 8979.45, average training loss: 9515.20, base loss: 14355.06
[INFO 2017-06-28 14:06:57,759 main.py:51] epoch 6989, training loss: 8156.69, average training loss: 9515.78, base loss: 14355.72
[INFO 2017-06-28 14:06:58,419 main.py:51] epoch 6990, training loss: 8135.37, average training loss: 9512.58, base loss: 14348.20
[INFO 2017-06-28 14:06:59,073 main.py:51] epoch 6991, training loss: 9291.07, average training loss: 9511.91, base loss: 14348.25
[INFO 2017-06-28 14:06:59,727 main.py:51] epoch 6992, training loss: 8975.84, average training loss: 9509.67, base loss: 14347.86
[INFO 2017-06-28 14:07:00,368 main.py:51] epoch 6993, training loss: 8551.46, average training loss: 9509.17, base loss: 14348.17
[INFO 2017-06-28 14:07:01,016 main.py:51] epoch 6994, training loss: 10692.15, average training loss: 9508.94, base loss: 14348.50
[INFO 2017-06-28 14:07:01,681 main.py:51] epoch 6995, training loss: 9695.76, average training loss: 9509.19, base loss: 14348.74
[INFO 2017-06-28 14:07:02,364 main.py:51] epoch 6996, training loss: 9454.03, average training loss: 9509.17, base loss: 14348.11
[INFO 2017-06-28 14:07:03,035 main.py:51] epoch 6997, training loss: 8523.41, average training loss: 9508.49, base loss: 14348.10
[INFO 2017-06-28 14:07:03,702 main.py:51] epoch 6998, training loss: 11477.21, average training loss: 9509.28, base loss: 14351.24
[INFO 2017-06-28 14:07:04,399 main.py:51] epoch 6999, training loss: 9051.33, average training loss: 9509.45, base loss: 14352.90
[INFO 2017-06-28 14:07:04,399 main.py:53] epoch 6999, testing
[INFO 2017-06-28 14:07:06,957 main.py:105] average testing loss: 10681.95, base loss: 15231.78
[INFO 2017-06-28 14:07:06,957 main.py:106] improve_loss: 4549.84, improve_percent: 0.30
[INFO 2017-06-28 14:07:06,958 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:07:07,611 main.py:51] epoch 7000, training loss: 7983.16, average training loss: 9507.29, base loss: 14348.05
[INFO 2017-06-28 14:07:08,265 main.py:51] epoch 7001, training loss: 10271.36, average training loss: 9506.85, base loss: 14346.08
[INFO 2017-06-28 14:07:08,927 main.py:51] epoch 7002, training loss: 9114.88, average training loss: 9506.50, base loss: 14345.37
[INFO 2017-06-28 14:07:09,591 main.py:51] epoch 7003, training loss: 8718.70, average training loss: 9504.47, base loss: 14340.73
[INFO 2017-06-28 14:07:10,264 main.py:51] epoch 7004, training loss: 10507.12, average training loss: 9505.53, base loss: 14343.37
[INFO 2017-06-28 14:07:10,918 main.py:51] epoch 7005, training loss: 8987.12, average training loss: 9504.69, base loss: 14343.75
[INFO 2017-06-28 14:07:11,582 main.py:51] epoch 7006, training loss: 10817.00, average training loss: 9506.19, base loss: 14345.42
[INFO 2017-06-28 14:07:12,267 main.py:51] epoch 7007, training loss: 10062.19, average training loss: 9507.42, base loss: 14348.01
[INFO 2017-06-28 14:07:12,921 main.py:51] epoch 7008, training loss: 9762.67, average training loss: 9508.17, base loss: 14349.38
[INFO 2017-06-28 14:07:13,570 main.py:51] epoch 7009, training loss: 9796.01, average training loss: 9508.65, base loss: 14349.69
[INFO 2017-06-28 14:07:14,251 main.py:51] epoch 7010, training loss: 8689.92, average training loss: 9508.74, base loss: 14347.50
[INFO 2017-06-28 14:07:14,887 main.py:51] epoch 7011, training loss: 9645.55, average training loss: 9507.98, base loss: 14348.54
[INFO 2017-06-28 14:07:15,551 main.py:51] epoch 7012, training loss: 8849.95, average training loss: 9507.23, base loss: 14346.78
[INFO 2017-06-28 14:07:16,209 main.py:51] epoch 7013, training loss: 9570.22, average training loss: 9506.43, base loss: 14344.24
[INFO 2017-06-28 14:07:16,860 main.py:51] epoch 7014, training loss: 8527.74, average training loss: 9505.54, base loss: 14343.17
[INFO 2017-06-28 14:07:17,514 main.py:51] epoch 7015, training loss: 10128.92, average training loss: 9507.14, base loss: 14343.44
[INFO 2017-06-28 14:07:18,138 main.py:51] epoch 7016, training loss: 9047.02, average training loss: 9506.29, base loss: 14341.95
[INFO 2017-06-28 14:07:18,797 main.py:51] epoch 7017, training loss: 9038.95, average training loss: 9505.57, base loss: 14341.79
[INFO 2017-06-28 14:07:19,459 main.py:51] epoch 7018, training loss: 8957.94, average training loss: 9506.00, base loss: 14343.78
[INFO 2017-06-28 14:07:20,116 main.py:51] epoch 7019, training loss: 9694.32, average training loss: 9505.48, base loss: 14342.50
[INFO 2017-06-28 14:07:20,759 main.py:51] epoch 7020, training loss: 9646.14, average training loss: 9504.48, base loss: 14342.42
[INFO 2017-06-28 14:07:21,425 main.py:51] epoch 7021, training loss: 10543.28, average training loss: 9505.74, base loss: 14342.15
[INFO 2017-06-28 14:07:22,116 main.py:51] epoch 7022, training loss: 8554.50, average training loss: 9504.19, base loss: 14340.96
[INFO 2017-06-28 14:07:22,767 main.py:51] epoch 7023, training loss: 9449.13, average training loss: 9503.44, base loss: 14339.93
[INFO 2017-06-28 14:07:23,418 main.py:51] epoch 7024, training loss: 8897.96, average training loss: 9503.46, base loss: 14341.49
[INFO 2017-06-28 14:07:24,086 main.py:51] epoch 7025, training loss: 9950.29, average training loss: 9504.51, base loss: 14344.69
[INFO 2017-06-28 14:07:24,756 main.py:51] epoch 7026, training loss: 9117.50, average training loss: 9503.21, base loss: 14343.71
[INFO 2017-06-28 14:07:25,403 main.py:51] epoch 7027, training loss: 10060.30, average training loss: 9503.87, base loss: 14346.63
[INFO 2017-06-28 14:07:26,073 main.py:51] epoch 7028, training loss: 8255.07, average training loss: 9504.27, base loss: 14347.37
[INFO 2017-06-28 14:07:26,713 main.py:51] epoch 7029, training loss: 10061.43, average training loss: 9505.64, base loss: 14349.07
[INFO 2017-06-28 14:07:27,382 main.py:51] epoch 7030, training loss: 9895.59, average training loss: 9504.78, base loss: 14347.33
[INFO 2017-06-28 14:07:28,046 main.py:51] epoch 7031, training loss: 10067.98, average training loss: 9505.97, base loss: 14349.65
[INFO 2017-06-28 14:07:28,712 main.py:51] epoch 7032, training loss: 9022.67, average training loss: 9505.30, base loss: 14348.24
[INFO 2017-06-28 14:07:29,388 main.py:51] epoch 7033, training loss: 10408.70, average training loss: 9504.64, base loss: 14348.42
[INFO 2017-06-28 14:07:30,031 main.py:51] epoch 7034, training loss: 9868.34, average training loss: 9506.01, base loss: 14351.91
[INFO 2017-06-28 14:07:30,682 main.py:51] epoch 7035, training loss: 8478.40, average training loss: 9504.35, base loss: 14348.58
[INFO 2017-06-28 14:07:31,317 main.py:51] epoch 7036, training loss: 8270.49, average training loss: 9501.88, base loss: 14344.66
[INFO 2017-06-28 14:07:31,995 main.py:51] epoch 7037, training loss: 8064.64, average training loss: 9501.05, base loss: 14344.45
[INFO 2017-06-28 14:07:32,644 main.py:51] epoch 7038, training loss: 8352.15, average training loss: 9500.61, base loss: 14342.96
[INFO 2017-06-28 14:07:33,326 main.py:51] epoch 7039, training loss: 9197.69, average training loss: 9500.35, base loss: 14343.96
[INFO 2017-06-28 14:07:33,984 main.py:51] epoch 7040, training loss: 9194.73, average training loss: 9499.90, base loss: 14343.96
[INFO 2017-06-28 14:07:34,654 main.py:51] epoch 7041, training loss: 10038.10, average training loss: 9500.26, base loss: 14346.48
[INFO 2017-06-28 14:07:35,334 main.py:51] epoch 7042, training loss: 7968.08, average training loss: 9496.97, base loss: 14342.79
[INFO 2017-06-28 14:07:35,979 main.py:51] epoch 7043, training loss: 8659.04, average training loss: 9495.92, base loss: 14342.21
[INFO 2017-06-28 14:07:36,621 main.py:51] epoch 7044, training loss: 10498.74, average training loss: 9498.04, base loss: 14346.23
[INFO 2017-06-28 14:07:37,291 main.py:51] epoch 7045, training loss: 9067.60, average training loss: 9498.42, base loss: 14346.30
[INFO 2017-06-28 14:07:37,952 main.py:51] epoch 7046, training loss: 8620.09, average training loss: 9496.62, base loss: 14343.82
[INFO 2017-06-28 14:07:38,602 main.py:51] epoch 7047, training loss: 8528.52, average training loss: 9495.68, base loss: 14341.21
[INFO 2017-06-28 14:07:39,276 main.py:51] epoch 7048, training loss: 10935.78, average training loss: 9497.30, base loss: 14343.06
[INFO 2017-06-28 14:07:39,901 main.py:51] epoch 7049, training loss: 8940.87, average training loss: 9496.85, base loss: 14342.05
[INFO 2017-06-28 14:07:40,549 main.py:51] epoch 7050, training loss: 8801.50, average training loss: 9497.33, base loss: 14343.30
[INFO 2017-06-28 14:07:41,206 main.py:51] epoch 7051, training loss: 9192.89, average training loss: 9496.76, base loss: 14342.56
[INFO 2017-06-28 14:07:41,867 main.py:51] epoch 7052, training loss: 9072.78, average training loss: 9496.40, base loss: 14342.73
[INFO 2017-06-28 14:07:42,511 main.py:51] epoch 7053, training loss: 9785.61, average training loss: 9495.55, base loss: 14341.87
[INFO 2017-06-28 14:07:43,153 main.py:51] epoch 7054, training loss: 9229.43, average training loss: 9495.83, base loss: 14343.11
[INFO 2017-06-28 14:07:43,845 main.py:51] epoch 7055, training loss: 9532.32, average training loss: 9496.34, base loss: 14342.92
[INFO 2017-06-28 14:07:44,493 main.py:51] epoch 7056, training loss: 8320.45, average training loss: 9495.32, base loss: 14341.58
[INFO 2017-06-28 14:07:45,173 main.py:51] epoch 7057, training loss: 10227.96, average training loss: 9496.36, base loss: 14343.52
[INFO 2017-06-28 14:07:45,843 main.py:51] epoch 7058, training loss: 9208.92, average training loss: 9496.45, base loss: 14345.35
[INFO 2017-06-28 14:07:46,495 main.py:51] epoch 7059, training loss: 10790.81, average training loss: 9496.47, base loss: 14345.57
[INFO 2017-06-28 14:07:47,136 main.py:51] epoch 7060, training loss: 9785.80, average training loss: 9497.27, base loss: 14347.59
[INFO 2017-06-28 14:07:47,784 main.py:51] epoch 7061, training loss: 8811.29, average training loss: 9495.90, base loss: 14346.05
[INFO 2017-06-28 14:07:48,443 main.py:51] epoch 7062, training loss: 11217.64, average training loss: 9496.84, base loss: 14346.13
[INFO 2017-06-28 14:07:49,119 main.py:51] epoch 7063, training loss: 11695.89, average training loss: 9499.57, base loss: 14351.59
[INFO 2017-06-28 14:07:49,765 main.py:51] epoch 7064, training loss: 9046.12, average training loss: 9498.55, base loss: 14349.21
[INFO 2017-06-28 14:07:50,440 main.py:51] epoch 7065, training loss: 11134.06, average training loss: 9498.59, base loss: 14348.41
[INFO 2017-06-28 14:07:51,085 main.py:51] epoch 7066, training loss: 10086.21, average training loss: 9498.46, base loss: 14348.30
[INFO 2017-06-28 14:07:51,740 main.py:51] epoch 7067, training loss: 9226.29, average training loss: 9498.89, base loss: 14348.41
[INFO 2017-06-28 14:07:52,398 main.py:51] epoch 7068, training loss: 9216.94, average training loss: 9497.87, base loss: 14347.91
[INFO 2017-06-28 14:07:53,040 main.py:51] epoch 7069, training loss: 9802.03, average training loss: 9497.45, base loss: 14347.61
[INFO 2017-06-28 14:07:53,695 main.py:51] epoch 7070, training loss: 9403.91, average training loss: 9495.88, base loss: 14345.87
[INFO 2017-06-28 14:07:54,356 main.py:51] epoch 7071, training loss: 9523.72, average training loss: 9496.15, base loss: 14346.54
[INFO 2017-06-28 14:07:55,013 main.py:51] epoch 7072, training loss: 8352.10, average training loss: 9495.70, base loss: 14346.18
[INFO 2017-06-28 14:07:55,692 main.py:51] epoch 7073, training loss: 8651.07, average training loss: 9493.56, base loss: 14343.81
[INFO 2017-06-28 14:07:56,344 main.py:51] epoch 7074, training loss: 9411.06, average training loss: 9493.99, base loss: 14343.91
[INFO 2017-06-28 14:07:57,008 main.py:51] epoch 7075, training loss: 9489.77, average training loss: 9492.73, base loss: 14342.51
[INFO 2017-06-28 14:07:57,636 main.py:51] epoch 7076, training loss: 10511.94, average training loss: 9492.32, base loss: 14341.10
[INFO 2017-06-28 14:07:58,307 main.py:51] epoch 7077, training loss: 8490.47, average training loss: 9491.95, base loss: 14340.62
[INFO 2017-06-28 14:07:58,969 main.py:51] epoch 7078, training loss: 8764.24, average training loss: 9491.98, base loss: 14340.09
[INFO 2017-06-28 14:07:59,613 main.py:51] epoch 7079, training loss: 9264.75, average training loss: 9491.06, base loss: 14338.97
[INFO 2017-06-28 14:08:00,292 main.py:51] epoch 7080, training loss: 8586.80, average training loss: 9490.61, base loss: 14338.72
[INFO 2017-06-28 14:08:00,929 main.py:51] epoch 7081, training loss: 9332.01, average training loss: 9489.71, base loss: 14336.58
[INFO 2017-06-28 14:08:01,582 main.py:51] epoch 7082, training loss: 9563.24, average training loss: 9489.10, base loss: 14335.92
[INFO 2017-06-28 14:08:02,239 main.py:51] epoch 7083, training loss: 8237.50, average training loss: 9488.63, base loss: 14336.71
[INFO 2017-06-28 14:08:02,891 main.py:51] epoch 7084, training loss: 9482.73, average training loss: 9489.13, base loss: 14337.45
[INFO 2017-06-28 14:08:03,560 main.py:51] epoch 7085, training loss: 8615.74, average training loss: 9487.49, base loss: 14334.65
[INFO 2017-06-28 14:08:04,224 main.py:51] epoch 7086, training loss: 11416.03, average training loss: 9489.20, base loss: 14338.52
[INFO 2017-06-28 14:08:04,890 main.py:51] epoch 7087, training loss: 8707.87, average training loss: 9488.80, base loss: 14339.24
[INFO 2017-06-28 14:08:05,559 main.py:51] epoch 7088, training loss: 9822.65, average training loss: 9488.01, base loss: 14338.65
[INFO 2017-06-28 14:08:06,210 main.py:51] epoch 7089, training loss: 8958.73, average training loss: 9488.26, base loss: 14339.00
[INFO 2017-06-28 14:08:06,871 main.py:51] epoch 7090, training loss: 9102.56, average training loss: 9487.86, base loss: 14339.59
[INFO 2017-06-28 14:08:07,528 main.py:51] epoch 7091, training loss: 8030.23, average training loss: 9486.03, base loss: 14337.19
[INFO 2017-06-28 14:08:08,192 main.py:51] epoch 7092, training loss: 9226.90, average training loss: 9485.66, base loss: 14336.76
[INFO 2017-06-28 14:08:08,835 main.py:51] epoch 7093, training loss: 10271.50, average training loss: 9486.27, base loss: 14335.92
[INFO 2017-06-28 14:08:09,489 main.py:51] epoch 7094, training loss: 9963.46, average training loss: 9486.51, base loss: 14335.70
[INFO 2017-06-28 14:08:10,144 main.py:51] epoch 7095, training loss: 7603.09, average training loss: 9485.04, base loss: 14333.82
[INFO 2017-06-28 14:08:10,800 main.py:51] epoch 7096, training loss: 8438.73, average training loss: 9484.53, base loss: 14333.60
[INFO 2017-06-28 14:08:11,470 main.py:51] epoch 7097, training loss: 9307.12, average training loss: 9484.26, base loss: 14332.47
[INFO 2017-06-28 14:08:12,115 main.py:51] epoch 7098, training loss: 8991.00, average training loss: 9481.85, base loss: 14330.37
[INFO 2017-06-28 14:08:12,789 main.py:51] epoch 7099, training loss: 9295.41, average training loss: 9481.28, base loss: 14330.46
[INFO 2017-06-28 14:08:12,790 main.py:53] epoch 7099, testing
[INFO 2017-06-28 14:08:15,388 main.py:105] average testing loss: 10568.75, base loss: 15138.06
[INFO 2017-06-28 14:08:15,388 main.py:106] improve_loss: 4569.30, improve_percent: 0.30
[INFO 2017-06-28 14:08:15,388 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:08:16,046 main.py:51] epoch 7100, training loss: 10350.08, average training loss: 9483.54, base loss: 14333.73
[INFO 2017-06-28 14:08:16,699 main.py:51] epoch 7101, training loss: 9138.27, average training loss: 9483.42, base loss: 14334.59
[INFO 2017-06-28 14:08:17,379 main.py:51] epoch 7102, training loss: 9355.24, average training loss: 9483.55, base loss: 14334.93
[INFO 2017-06-28 14:08:18,021 main.py:51] epoch 7103, training loss: 8358.35, average training loss: 9482.42, base loss: 14333.15
[INFO 2017-06-28 14:08:18,668 main.py:51] epoch 7104, training loss: 9608.11, average training loss: 9482.60, base loss: 14333.51
[INFO 2017-06-28 14:08:19,313 main.py:51] epoch 7105, training loss: 9314.56, average training loss: 9482.47, base loss: 14333.92
[INFO 2017-06-28 14:08:19,982 main.py:51] epoch 7106, training loss: 9513.69, average training loss: 9482.64, base loss: 14334.16
[INFO 2017-06-28 14:08:20,634 main.py:51] epoch 7107, training loss: 9508.30, average training loss: 9481.62, base loss: 14333.08
[INFO 2017-06-28 14:08:21,292 main.py:51] epoch 7108, training loss: 9434.22, average training loss: 9481.02, base loss: 14333.95
[INFO 2017-06-28 14:08:21,943 main.py:51] epoch 7109, training loss: 9279.44, average training loss: 9482.24, base loss: 14334.89
[INFO 2017-06-28 14:08:22,592 main.py:51] epoch 7110, training loss: 10543.70, average training loss: 9483.10, base loss: 14334.92
[INFO 2017-06-28 14:08:23,240 main.py:51] epoch 7111, training loss: 8506.45, average training loss: 9482.61, base loss: 14333.74
[INFO 2017-06-28 14:08:23,876 main.py:51] epoch 7112, training loss: 10018.55, average training loss: 9483.25, base loss: 14334.55
[INFO 2017-06-28 14:08:24,528 main.py:51] epoch 7113, training loss: 12049.76, average training loss: 9485.03, base loss: 14337.13
[INFO 2017-06-28 14:08:25,195 main.py:51] epoch 7114, training loss: 8735.36, average training loss: 9483.47, base loss: 14335.89
[INFO 2017-06-28 14:08:25,846 main.py:51] epoch 7115, training loss: 10024.67, average training loss: 9483.75, base loss: 14334.75
[INFO 2017-06-28 14:08:26,521 main.py:51] epoch 7116, training loss: 9013.91, average training loss: 9483.64, base loss: 14335.39
[INFO 2017-06-28 14:08:27,165 main.py:51] epoch 7117, training loss: 9916.49, average training loss: 9484.16, base loss: 14336.40
[INFO 2017-06-28 14:08:27,816 main.py:51] epoch 7118, training loss: 9235.57, average training loss: 9483.78, base loss: 14335.11
[INFO 2017-06-28 14:08:28,476 main.py:51] epoch 7119, training loss: 8919.88, average training loss: 9482.82, base loss: 14333.60
[INFO 2017-06-28 14:08:29,126 main.py:51] epoch 7120, training loss: 10782.98, average training loss: 9482.10, base loss: 14332.53
[INFO 2017-06-28 14:08:29,794 main.py:51] epoch 7121, training loss: 9751.82, average training loss: 9482.54, base loss: 14333.93
[INFO 2017-06-28 14:08:30,458 main.py:51] epoch 7122, training loss: 9556.75, average training loss: 9482.46, base loss: 14334.17
[INFO 2017-06-28 14:08:31,136 main.py:51] epoch 7123, training loss: 12222.05, average training loss: 9484.32, base loss: 14338.02
[INFO 2017-06-28 14:08:31,832 main.py:51] epoch 7124, training loss: 8684.08, average training loss: 9483.76, base loss: 14336.24
[INFO 2017-06-28 14:08:32,475 main.py:51] epoch 7125, training loss: 9360.27, average training loss: 9483.89, base loss: 14337.54
[INFO 2017-06-28 14:08:33,134 main.py:51] epoch 7126, training loss: 9449.86, average training loss: 9484.19, base loss: 14338.07
[INFO 2017-06-28 14:08:33,798 main.py:51] epoch 7127, training loss: 9666.57, average training loss: 9485.20, base loss: 14338.91
[INFO 2017-06-28 14:08:34,459 main.py:51] epoch 7128, training loss: 9358.71, average training loss: 9485.65, base loss: 14339.22
[INFO 2017-06-28 14:08:35,101 main.py:51] epoch 7129, training loss: 9131.91, average training loss: 9485.20, base loss: 14339.21
[INFO 2017-06-28 14:08:35,747 main.py:51] epoch 7130, training loss: 10040.23, average training loss: 9486.55, base loss: 14341.35
[INFO 2017-06-28 14:08:36,398 main.py:51] epoch 7131, training loss: 9304.07, average training loss: 9487.63, base loss: 14344.37
[INFO 2017-06-28 14:08:37,064 main.py:51] epoch 7132, training loss: 8560.55, average training loss: 9488.17, base loss: 14346.37
[INFO 2017-06-28 14:08:37,733 main.py:51] epoch 7133, training loss: 9923.29, average training loss: 9489.07, base loss: 14347.66
[INFO 2017-06-28 14:08:38,383 main.py:51] epoch 7134, training loss: 9281.55, average training loss: 9489.09, base loss: 14349.52
[INFO 2017-06-28 14:08:39,047 main.py:51] epoch 7135, training loss: 10954.64, average training loss: 9489.91, base loss: 14349.59
[INFO 2017-06-28 14:08:39,695 main.py:51] epoch 7136, training loss: 9463.52, average training loss: 9488.82, base loss: 14349.76
[INFO 2017-06-28 14:08:40,386 main.py:51] epoch 7137, training loss: 10388.18, average training loss: 9489.45, base loss: 14351.95
[INFO 2017-06-28 14:08:41,023 main.py:51] epoch 7138, training loss: 8690.69, average training loss: 9486.74, base loss: 14347.78
[INFO 2017-06-28 14:08:41,685 main.py:51] epoch 7139, training loss: 9044.54, average training loss: 9485.73, base loss: 14345.36
[INFO 2017-06-28 14:08:42,348 main.py:51] epoch 7140, training loss: 9029.83, average training loss: 9484.16, base loss: 14343.61
[INFO 2017-06-28 14:08:43,009 main.py:51] epoch 7141, training loss: 10155.75, average training loss: 9484.09, base loss: 14342.03
[INFO 2017-06-28 14:08:43,671 main.py:51] epoch 7142, training loss: 10050.12, average training loss: 9484.49, base loss: 14341.73
[INFO 2017-06-28 14:08:44,325 main.py:51] epoch 7143, training loss: 9820.53, average training loss: 9485.23, base loss: 14342.77
[INFO 2017-06-28 14:08:44,960 main.py:51] epoch 7144, training loss: 9345.78, average training loss: 9485.90, base loss: 14342.98
[INFO 2017-06-28 14:08:45,621 main.py:51] epoch 7145, training loss: 8987.71, average training loss: 9485.02, base loss: 14340.77
[INFO 2017-06-28 14:08:46,286 main.py:51] epoch 7146, training loss: 8794.90, average training loss: 9482.78, base loss: 14337.70
[INFO 2017-06-28 14:08:46,940 main.py:51] epoch 7147, training loss: 8659.21, average training loss: 9482.20, base loss: 14336.19
[INFO 2017-06-28 14:08:47,608 main.py:51] epoch 7148, training loss: 9132.85, average training loss: 9482.35, base loss: 14336.22
[INFO 2017-06-28 14:08:48,280 main.py:51] epoch 7149, training loss: 8426.62, average training loss: 9480.94, base loss: 14333.98
[INFO 2017-06-28 14:08:48,931 main.py:51] epoch 7150, training loss: 8955.71, average training loss: 9479.33, base loss: 14332.32
[INFO 2017-06-28 14:08:49,595 main.py:51] epoch 7151, training loss: 9314.98, average training loss: 9480.04, base loss: 14333.82
[INFO 2017-06-28 14:08:50,253 main.py:51] epoch 7152, training loss: 9062.92, average training loss: 9480.24, base loss: 14333.91
[INFO 2017-06-28 14:08:50,898 main.py:51] epoch 7153, training loss: 8495.36, average training loss: 9480.01, base loss: 14332.92
[INFO 2017-06-28 14:08:51,551 main.py:51] epoch 7154, training loss: 9911.09, average training loss: 9481.09, base loss: 14334.43
[INFO 2017-06-28 14:08:52,192 main.py:51] epoch 7155, training loss: 10593.97, average training loss: 9481.84, base loss: 14336.07
[INFO 2017-06-28 14:08:52,830 main.py:51] epoch 7156, training loss: 9332.42, average training loss: 9481.57, base loss: 14334.53
[INFO 2017-06-28 14:08:53,476 main.py:51] epoch 7157, training loss: 9595.51, average training loss: 9482.52, base loss: 14335.87
[INFO 2017-06-28 14:08:54,122 main.py:51] epoch 7158, training loss: 10035.91, average training loss: 9482.61, base loss: 14335.59
[INFO 2017-06-28 14:08:54,788 main.py:51] epoch 7159, training loss: 10657.96, average training loss: 9485.61, base loss: 14339.95
[INFO 2017-06-28 14:08:55,445 main.py:51] epoch 7160, training loss: 8457.49, average training loss: 9484.50, base loss: 14337.22
[INFO 2017-06-28 14:08:56,092 main.py:51] epoch 7161, training loss: 9062.85, average training loss: 9484.50, base loss: 14336.79
[INFO 2017-06-28 14:08:56,770 main.py:51] epoch 7162, training loss: 9346.95, average training loss: 9483.25, base loss: 14332.93
[INFO 2017-06-28 14:08:57,422 main.py:51] epoch 7163, training loss: 8937.97, average training loss: 9480.18, base loss: 14329.04
[INFO 2017-06-28 14:08:58,072 main.py:51] epoch 7164, training loss: 8968.26, average training loss: 9479.96, base loss: 14328.01
[INFO 2017-06-28 14:08:58,721 main.py:51] epoch 7165, training loss: 8629.44, average training loss: 9479.30, base loss: 14326.87
[INFO 2017-06-28 14:08:59,373 main.py:51] epoch 7166, training loss: 9406.49, average training loss: 9480.49, base loss: 14329.08
[INFO 2017-06-28 14:09:00,070 main.py:51] epoch 7167, training loss: 9358.15, average training loss: 9478.24, base loss: 14326.34
[INFO 2017-06-28 14:09:00,711 main.py:51] epoch 7168, training loss: 9211.27, average training loss: 9478.09, base loss: 14327.43
[INFO 2017-06-28 14:09:01,372 main.py:51] epoch 7169, training loss: 9580.93, average training loss: 9478.67, base loss: 14329.88
[INFO 2017-06-28 14:09:02,031 main.py:51] epoch 7170, training loss: 9061.56, average training loss: 9477.12, base loss: 14329.59
[INFO 2017-06-28 14:09:02,682 main.py:51] epoch 7171, training loss: 9563.75, average training loss: 9477.70, base loss: 14331.60
[INFO 2017-06-28 14:09:03,325 main.py:51] epoch 7172, training loss: 9141.10, average training loss: 9476.91, base loss: 14331.60
[INFO 2017-06-28 14:09:03,987 main.py:51] epoch 7173, training loss: 9524.70, average training loss: 9477.22, base loss: 14333.56
[INFO 2017-06-28 14:09:04,633 main.py:51] epoch 7174, training loss: 9341.88, average training loss: 9476.71, base loss: 14333.06
[INFO 2017-06-28 14:09:05,302 main.py:51] epoch 7175, training loss: 8490.91, average training loss: 9475.59, base loss: 14331.83
[INFO 2017-06-28 14:09:05,961 main.py:51] epoch 7176, training loss: 9853.41, average training loss: 9475.09, base loss: 14330.95
[INFO 2017-06-28 14:09:06,611 main.py:51] epoch 7177, training loss: 9623.28, average training loss: 9474.79, base loss: 14330.13
[INFO 2017-06-28 14:09:07,268 main.py:51] epoch 7178, training loss: 10714.22, average training loss: 9475.45, base loss: 14331.01
[INFO 2017-06-28 14:09:07,915 main.py:51] epoch 7179, training loss: 10213.40, average training loss: 9475.33, base loss: 14331.13
[INFO 2017-06-28 14:09:08,581 main.py:51] epoch 7180, training loss: 8932.36, average training loss: 9475.42, base loss: 14329.87
[INFO 2017-06-28 14:09:09,224 main.py:51] epoch 7181, training loss: 8534.82, average training loss: 9474.44, base loss: 14329.10
[INFO 2017-06-28 14:09:09,885 main.py:51] epoch 7182, training loss: 10846.93, average training loss: 9476.60, base loss: 14332.82
[INFO 2017-06-28 14:09:10,550 main.py:51] epoch 7183, training loss: 10016.52, average training loss: 9475.43, base loss: 14331.63
[INFO 2017-06-28 14:09:11,215 main.py:51] epoch 7184, training loss: 10651.47, average training loss: 9475.22, base loss: 14329.86
[INFO 2017-06-28 14:09:11,872 main.py:51] epoch 7185, training loss: 9543.16, average training loss: 9475.11, base loss: 14330.49
[INFO 2017-06-28 14:09:12,541 main.py:51] epoch 7186, training loss: 9934.55, average training loss: 9476.15, base loss: 14331.57
[INFO 2017-06-28 14:09:13,205 main.py:51] epoch 7187, training loss: 8710.35, average training loss: 9476.07, base loss: 14331.23
[INFO 2017-06-28 14:09:13,853 main.py:51] epoch 7188, training loss: 10023.16, average training loss: 9475.89, base loss: 14331.41
[INFO 2017-06-28 14:09:14,534 main.py:51] epoch 7189, training loss: 8501.57, average training loss: 9474.00, base loss: 14329.37
[INFO 2017-06-28 14:09:15,204 main.py:51] epoch 7190, training loss: 8757.48, average training loss: 9472.92, base loss: 14327.42
[INFO 2017-06-28 14:09:15,863 main.py:51] epoch 7191, training loss: 10339.13, average training loss: 9475.78, base loss: 14331.42
[INFO 2017-06-28 14:09:16,535 main.py:51] epoch 7192, training loss: 10471.76, average training loss: 9477.07, base loss: 14334.31
[INFO 2017-06-28 14:09:17,217 main.py:51] epoch 7193, training loss: 8338.41, average training loss: 9477.45, base loss: 14335.05
[INFO 2017-06-28 14:09:17,913 main.py:51] epoch 7194, training loss: 9119.20, average training loss: 9476.35, base loss: 14332.74
[INFO 2017-06-28 14:09:18,565 main.py:51] epoch 7195, training loss: 9873.46, average training loss: 9476.39, base loss: 14331.07
[INFO 2017-06-28 14:09:19,210 main.py:51] epoch 7196, training loss: 10918.38, average training loss: 9477.21, base loss: 14332.37
[INFO 2017-06-28 14:09:19,861 main.py:51] epoch 7197, training loss: 10121.38, average training loss: 9478.14, base loss: 14332.19
[INFO 2017-06-28 14:09:20,507 main.py:51] epoch 7198, training loss: 8771.76, average training loss: 9478.44, base loss: 14332.76
[INFO 2017-06-28 14:09:21,160 main.py:51] epoch 7199, training loss: 7854.75, average training loss: 9477.97, base loss: 14333.50
[INFO 2017-06-28 14:09:21,160 main.py:53] epoch 7199, testing
[INFO 2017-06-28 14:09:23,766 main.py:105] average testing loss: 10557.54, base loss: 15165.41
[INFO 2017-06-28 14:09:23,766 main.py:106] improve_loss: 4607.87, improve_percent: 0.30
[INFO 2017-06-28 14:09:23,767 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:09:24,448 main.py:51] epoch 7200, training loss: 9771.38, average training loss: 9478.93, base loss: 14334.53
[INFO 2017-06-28 14:09:25,103 main.py:51] epoch 7201, training loss: 8350.14, average training loss: 9476.95, base loss: 14331.65
[INFO 2017-06-28 14:09:25,765 main.py:51] epoch 7202, training loss: 10749.95, average training loss: 9477.75, base loss: 14332.86
[INFO 2017-06-28 14:09:26,413 main.py:51] epoch 7203, training loss: 9851.40, average training loss: 9477.57, base loss: 14331.68
[INFO 2017-06-28 14:09:27,054 main.py:51] epoch 7204, training loss: 8719.87, average training loss: 9477.23, base loss: 14330.37
[INFO 2017-06-28 14:09:27,701 main.py:51] epoch 7205, training loss: 9172.30, average training loss: 9477.33, base loss: 14329.73
[INFO 2017-06-28 14:09:28,359 main.py:51] epoch 7206, training loss: 8993.78, average training loss: 9475.16, base loss: 14324.96
[INFO 2017-06-28 14:09:29,032 main.py:51] epoch 7207, training loss: 10028.11, average training loss: 9476.24, base loss: 14326.72
[INFO 2017-06-28 14:09:29,681 main.py:51] epoch 7208, training loss: 10088.60, average training loss: 9475.82, base loss: 14324.25
[INFO 2017-06-28 14:09:30,338 main.py:51] epoch 7209, training loss: 10064.14, average training loss: 9476.34, base loss: 14326.34
[INFO 2017-06-28 14:09:30,989 main.py:51] epoch 7210, training loss: 9193.52, average training loss: 9475.77, base loss: 14325.38
[INFO 2017-06-28 14:09:31,683 main.py:51] epoch 7211, training loss: 8457.84, average training loss: 9473.83, base loss: 14322.71
[INFO 2017-06-28 14:09:32,330 main.py:51] epoch 7212, training loss: 10261.58, average training loss: 9475.55, base loss: 14324.12
[INFO 2017-06-28 14:09:32,993 main.py:51] epoch 7213, training loss: 9239.57, average training loss: 9476.48, base loss: 14326.26
[INFO 2017-06-28 14:09:33,662 main.py:51] epoch 7214, training loss: 8647.47, average training loss: 9476.19, base loss: 14325.03
[INFO 2017-06-28 14:09:34,333 main.py:51] epoch 7215, training loss: 11871.80, average training loss: 9477.96, base loss: 14327.86
[INFO 2017-06-28 14:09:34,992 main.py:51] epoch 7216, training loss: 8552.22, average training loss: 9476.81, base loss: 14326.25
[INFO 2017-06-28 14:09:35,627 main.py:51] epoch 7217, training loss: 8609.95, average training loss: 9476.51, base loss: 14325.75
[INFO 2017-06-28 14:09:36,284 main.py:51] epoch 7218, training loss: 8919.29, average training loss: 9475.52, base loss: 14324.50
[INFO 2017-06-28 14:09:36,939 main.py:51] epoch 7219, training loss: 8547.56, average training loss: 9474.84, base loss: 14323.83
[INFO 2017-06-28 14:09:37,596 main.py:51] epoch 7220, training loss: 9262.81, average training loss: 9473.90, base loss: 14322.68
[INFO 2017-06-28 14:09:38,263 main.py:51] epoch 7221, training loss: 9446.19, average training loss: 9471.79, base loss: 14319.12
[INFO 2017-06-28 14:09:38,930 main.py:51] epoch 7222, training loss: 8658.40, average training loss: 9469.69, base loss: 14316.81
[INFO 2017-06-28 14:09:39,561 main.py:51] epoch 7223, training loss: 8742.36, average training loss: 9470.25, base loss: 14317.93
[INFO 2017-06-28 14:09:40,208 main.py:51] epoch 7224, training loss: 9074.65, average training loss: 9469.81, base loss: 14318.15
[INFO 2017-06-28 14:09:40,882 main.py:51] epoch 7225, training loss: 8931.81, average training loss: 9469.03, base loss: 14315.80
[INFO 2017-06-28 14:09:41,544 main.py:51] epoch 7226, training loss: 9082.66, average training loss: 9468.34, base loss: 14313.36
[INFO 2017-06-28 14:09:42,213 main.py:51] epoch 7227, training loss: 9073.13, average training loss: 9468.49, base loss: 14314.09
[INFO 2017-06-28 14:09:42,859 main.py:51] epoch 7228, training loss: 9033.32, average training loss: 9467.02, base loss: 14312.25
[INFO 2017-06-28 14:09:43,530 main.py:51] epoch 7229, training loss: 9212.36, average training loss: 9467.63, base loss: 14313.70
[INFO 2017-06-28 14:09:44,194 main.py:51] epoch 7230, training loss: 9108.97, average training loss: 9466.76, base loss: 14312.35
[INFO 2017-06-28 14:09:44,856 main.py:51] epoch 7231, training loss: 10238.93, average training loss: 9467.84, base loss: 14312.49
[INFO 2017-06-28 14:09:45,523 main.py:51] epoch 7232, training loss: 10957.17, average training loss: 9469.41, base loss: 14316.18
[INFO 2017-06-28 14:09:46,184 main.py:51] epoch 7233, training loss: 10915.11, average training loss: 9469.92, base loss: 14317.11
[INFO 2017-06-28 14:09:46,844 main.py:51] epoch 7234, training loss: 8540.85, average training loss: 9468.18, base loss: 14314.19
[INFO 2017-06-28 14:09:47,505 main.py:51] epoch 7235, training loss: 10299.02, average training loss: 9469.19, base loss: 14314.76
[INFO 2017-06-28 14:09:48,179 main.py:51] epoch 7236, training loss: 9784.10, average training loss: 9468.94, base loss: 14314.50
[INFO 2017-06-28 14:09:48,840 main.py:51] epoch 7237, training loss: 8355.29, average training loss: 9468.47, base loss: 14312.87
[INFO 2017-06-28 14:09:49,498 main.py:51] epoch 7238, training loss: 8279.33, average training loss: 9466.93, base loss: 14311.01
[INFO 2017-06-28 14:09:50,169 main.py:51] epoch 7239, training loss: 10394.58, average training loss: 9466.17, base loss: 14310.28
[INFO 2017-06-28 14:09:50,843 main.py:51] epoch 7240, training loss: 9744.67, average training loss: 9467.07, base loss: 14312.39
[INFO 2017-06-28 14:09:51,500 main.py:51] epoch 7241, training loss: 9364.03, average training loss: 9467.62, base loss: 14312.27
[INFO 2017-06-28 14:09:52,173 main.py:51] epoch 7242, training loss: 9209.71, average training loss: 9467.00, base loss: 14310.78
[INFO 2017-06-28 14:09:52,832 main.py:51] epoch 7243, training loss: 9298.59, average training loss: 9467.10, base loss: 14310.42
[INFO 2017-06-28 14:09:53,503 main.py:51] epoch 7244, training loss: 8925.44, average training loss: 9467.59, base loss: 14311.09
[INFO 2017-06-28 14:09:54,158 main.py:51] epoch 7245, training loss: 10835.80, average training loss: 9469.30, base loss: 14313.14
[INFO 2017-06-28 14:09:54,835 main.py:51] epoch 7246, training loss: 8589.96, average training loss: 9468.10, base loss: 14310.53
[INFO 2017-06-28 14:09:55,502 main.py:51] epoch 7247, training loss: 9166.15, average training loss: 9468.15, base loss: 14310.63
[INFO 2017-06-28 14:09:56,167 main.py:51] epoch 7248, training loss: 9639.03, average training loss: 9469.35, base loss: 14312.30
[INFO 2017-06-28 14:09:56,824 main.py:51] epoch 7249, training loss: 9913.21, average training loss: 9469.88, base loss: 14312.17
[INFO 2017-06-28 14:09:57,486 main.py:51] epoch 7250, training loss: 9196.95, average training loss: 9469.68, base loss: 14312.23
[INFO 2017-06-28 14:09:58,142 main.py:51] epoch 7251, training loss: 9476.43, average training loss: 9469.62, base loss: 14310.86
[INFO 2017-06-28 14:09:58,796 main.py:51] epoch 7252, training loss: 9057.82, average training loss: 9469.33, base loss: 14310.60
[INFO 2017-06-28 14:09:59,445 main.py:51] epoch 7253, training loss: 10155.40, average training loss: 9470.39, base loss: 14311.47
[INFO 2017-06-28 14:10:00,115 main.py:51] epoch 7254, training loss: 10216.72, average training loss: 9470.18, base loss: 14311.07
[INFO 2017-06-28 14:10:00,792 main.py:51] epoch 7255, training loss: 8981.74, average training loss: 9470.20, base loss: 14310.50
[INFO 2017-06-28 14:10:01,455 main.py:51] epoch 7256, training loss: 9724.00, average training loss: 9470.55, base loss: 14312.62
[INFO 2017-06-28 14:10:02,103 main.py:51] epoch 7257, training loss: 9044.63, average training loss: 9468.22, base loss: 14311.18
[INFO 2017-06-28 14:10:02,796 main.py:51] epoch 7258, training loss: 10210.83, average training loss: 9469.57, base loss: 14312.65
[INFO 2017-06-28 14:10:03,463 main.py:51] epoch 7259, training loss: 9355.25, average training loss: 9469.88, base loss: 14312.72
[INFO 2017-06-28 14:10:04,144 main.py:51] epoch 7260, training loss: 8948.41, average training loss: 9469.75, base loss: 14313.94
[INFO 2017-06-28 14:10:04,816 main.py:51] epoch 7261, training loss: 8603.35, average training loss: 9469.51, base loss: 14313.38
[INFO 2017-06-28 14:10:05,481 main.py:51] epoch 7262, training loss: 9385.04, average training loss: 9468.52, base loss: 14311.79
[INFO 2017-06-28 14:10:06,146 main.py:51] epoch 7263, training loss: 8498.43, average training loss: 9465.98, base loss: 14307.10
[INFO 2017-06-28 14:10:06,793 main.py:51] epoch 7264, training loss: 9439.78, average training loss: 9466.36, base loss: 14306.80
[INFO 2017-06-28 14:10:07,472 main.py:51] epoch 7265, training loss: 9653.85, average training loss: 9465.61, base loss: 14305.94
[INFO 2017-06-28 14:10:08,127 main.py:51] epoch 7266, training loss: 9474.97, average training loss: 9465.71, base loss: 14304.83
[INFO 2017-06-28 14:10:08,786 main.py:51] epoch 7267, training loss: 9415.43, average training loss: 9466.29, base loss: 14306.56
[INFO 2017-06-28 14:10:09,470 main.py:51] epoch 7268, training loss: 8673.24, average training loss: 9465.81, base loss: 14307.91
[INFO 2017-06-28 14:10:10,113 main.py:51] epoch 7269, training loss: 9426.97, average training loss: 9465.37, base loss: 14308.40
[INFO 2017-06-28 14:10:10,799 main.py:51] epoch 7270, training loss: 8994.03, average training loss: 9463.72, base loss: 14305.51
[INFO 2017-06-28 14:10:11,470 main.py:51] epoch 7271, training loss: 9789.64, average training loss: 9464.51, base loss: 14306.88
[INFO 2017-06-28 14:10:12,114 main.py:51] epoch 7272, training loss: 9073.00, average training loss: 9464.05, base loss: 14307.50
[INFO 2017-06-28 14:10:12,777 main.py:51] epoch 7273, training loss: 9950.79, average training loss: 9465.23, base loss: 14309.59
[INFO 2017-06-28 14:10:13,409 main.py:51] epoch 7274, training loss: 9440.46, average training loss: 9463.70, base loss: 14309.62
[INFO 2017-06-28 14:10:14,063 main.py:51] epoch 7275, training loss: 10296.12, average training loss: 9464.92, base loss: 14312.79
[INFO 2017-06-28 14:10:14,711 main.py:51] epoch 7276, training loss: 8354.51, average training loss: 9463.10, base loss: 14309.25
[INFO 2017-06-28 14:10:15,354 main.py:51] epoch 7277, training loss: 9497.76, average training loss: 9463.69, base loss: 14311.72
[INFO 2017-06-28 14:10:16,008 main.py:51] epoch 7278, training loss: 10438.31, average training loss: 9465.29, base loss: 14314.79
[INFO 2017-06-28 14:10:16,682 main.py:51] epoch 7279, training loss: 9842.66, average training loss: 9465.03, base loss: 14314.38
[INFO 2017-06-28 14:10:17,342 main.py:51] epoch 7280, training loss: 8525.41, average training loss: 9463.88, base loss: 14310.77
[INFO 2017-06-28 14:10:18,008 main.py:51] epoch 7281, training loss: 9517.78, average training loss: 9464.41, base loss: 14312.67
[INFO 2017-06-28 14:10:18,665 main.py:51] epoch 7282, training loss: 9014.14, average training loss: 9464.17, base loss: 14311.08
[INFO 2017-06-28 14:10:19,316 main.py:51] epoch 7283, training loss: 9300.72, average training loss: 9463.37, base loss: 14310.86
[INFO 2017-06-28 14:10:19,964 main.py:51] epoch 7284, training loss: 10992.01, average training loss: 9464.05, base loss: 14312.94
[INFO 2017-06-28 14:10:20,605 main.py:51] epoch 7285, training loss: 10267.92, average training loss: 9463.38, base loss: 14310.63
[INFO 2017-06-28 14:10:21,240 main.py:51] epoch 7286, training loss: 9033.74, average training loss: 9461.71, base loss: 14307.71
[INFO 2017-06-28 14:10:21,904 main.py:51] epoch 7287, training loss: 9431.49, average training loss: 9461.89, base loss: 14306.79
[INFO 2017-06-28 14:10:22,561 main.py:51] epoch 7288, training loss: 9552.81, average training loss: 9462.25, base loss: 14307.97
[INFO 2017-06-28 14:10:23,224 main.py:51] epoch 7289, training loss: 8393.13, average training loss: 9461.82, base loss: 14307.55
[INFO 2017-06-28 14:10:23,896 main.py:51] epoch 7290, training loss: 10020.70, average training loss: 9463.28, base loss: 14310.30
[INFO 2017-06-28 14:10:24,555 main.py:51] epoch 7291, training loss: 9657.75, average training loss: 9463.15, base loss: 14308.28
[INFO 2017-06-28 14:10:25,234 main.py:51] epoch 7292, training loss: 9126.81, average training loss: 9464.14, base loss: 14309.72
[INFO 2017-06-28 14:10:25,895 main.py:51] epoch 7293, training loss: 9732.35, average training loss: 9465.38, base loss: 14310.31
[INFO 2017-06-28 14:10:26,548 main.py:51] epoch 7294, training loss: 8381.51, average training loss: 9465.08, base loss: 14310.11
[INFO 2017-06-28 14:10:27,214 main.py:51] epoch 7295, training loss: 9389.23, average training loss: 9463.05, base loss: 14308.13
[INFO 2017-06-28 14:10:27,889 main.py:51] epoch 7296, training loss: 9672.15, average training loss: 9463.24, base loss: 14308.09
[INFO 2017-06-28 14:10:28,511 main.py:51] epoch 7297, training loss: 10544.98, average training loss: 9465.22, base loss: 14312.66
[INFO 2017-06-28 14:10:29,186 main.py:51] epoch 7298, training loss: 10884.15, average training loss: 9466.82, base loss: 14315.78
[INFO 2017-06-28 14:10:29,856 main.py:51] epoch 7299, training loss: 9088.80, average training loss: 9466.63, base loss: 14316.69
[INFO 2017-06-28 14:10:29,856 main.py:53] epoch 7299, testing
[INFO 2017-06-28 14:10:32,412 main.py:105] average testing loss: 11524.63, base loss: 16181.02
[INFO 2017-06-28 14:10:32,412 main.py:106] improve_loss: 4656.39, improve_percent: 0.29
[INFO 2017-06-28 14:10:32,413 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:10:33,068 main.py:51] epoch 7300, training loss: 10667.44, average training loss: 9469.02, base loss: 14321.23
[INFO 2017-06-28 14:10:33,698 main.py:51] epoch 7301, training loss: 8825.46, average training loss: 9467.67, base loss: 14319.98
[INFO 2017-06-28 14:10:34,325 main.py:51] epoch 7302, training loss: 9443.97, average training loss: 9467.05, base loss: 14319.32
[INFO 2017-06-28 14:10:34,986 main.py:51] epoch 7303, training loss: 9540.64, average training loss: 9467.03, base loss: 14319.85
[INFO 2017-06-28 14:10:35,658 main.py:51] epoch 7304, training loss: 11030.03, average training loss: 9468.63, base loss: 14320.65
[INFO 2017-06-28 14:10:36,310 main.py:51] epoch 7305, training loss: 9301.29, average training loss: 9468.98, base loss: 14320.44
[INFO 2017-06-28 14:10:36,959 main.py:51] epoch 7306, training loss: 7680.67, average training loss: 9466.93, base loss: 14317.64
[INFO 2017-06-28 14:10:37,614 main.py:51] epoch 7307, training loss: 9635.95, average training loss: 9466.51, base loss: 14316.86
[INFO 2017-06-28 14:10:38,255 main.py:51] epoch 7308, training loss: 9476.44, average training loss: 9465.33, base loss: 14314.26
[INFO 2017-06-28 14:10:38,924 main.py:51] epoch 7309, training loss: 11401.57, average training loss: 9467.55, base loss: 14317.21
[INFO 2017-06-28 14:10:39,571 main.py:51] epoch 7310, training loss: 9910.56, average training loss: 9468.56, base loss: 14321.23
[INFO 2017-06-28 14:10:40,250 main.py:51] epoch 7311, training loss: 9388.84, average training loss: 9468.57, base loss: 14319.56
[INFO 2017-06-28 14:10:40,930 main.py:51] epoch 7312, training loss: 9296.90, average training loss: 9467.14, base loss: 14317.85
[INFO 2017-06-28 14:10:41,580 main.py:51] epoch 7313, training loss: 8522.26, average training loss: 9466.84, base loss: 14318.36
[INFO 2017-06-28 14:10:42,264 main.py:51] epoch 7314, training loss: 9164.86, average training loss: 9466.16, base loss: 14317.62
[INFO 2017-06-28 14:10:42,928 main.py:51] epoch 7315, training loss: 10166.02, average training loss: 9466.42, base loss: 14316.77
[INFO 2017-06-28 14:10:43,591 main.py:51] epoch 7316, training loss: 9023.51, average training loss: 9467.64, base loss: 14318.86
[INFO 2017-06-28 14:10:44,258 main.py:51] epoch 7317, training loss: 9188.46, average training loss: 9467.16, base loss: 14319.08
[INFO 2017-06-28 14:10:44,925 main.py:51] epoch 7318, training loss: 10519.30, average training loss: 9468.80, base loss: 14321.46
[INFO 2017-06-28 14:10:45,580 main.py:51] epoch 7319, training loss: 10047.76, average training loss: 9470.10, base loss: 14325.73
[INFO 2017-06-28 14:10:46,251 main.py:51] epoch 7320, training loss: 10275.03, average training loss: 9470.55, base loss: 14327.33
[INFO 2017-06-28 14:10:46,894 main.py:51] epoch 7321, training loss: 9602.65, average training loss: 9471.21, base loss: 14327.53
[INFO 2017-06-28 14:10:47,559 main.py:51] epoch 7322, training loss: 9873.92, average training loss: 9471.52, base loss: 14328.15
[INFO 2017-06-28 14:10:48,220 main.py:51] epoch 7323, training loss: 10831.88, average training loss: 9472.28, base loss: 14332.33
[INFO 2017-06-28 14:10:48,878 main.py:51] epoch 7324, training loss: 9991.24, average training loss: 9473.33, base loss: 14333.43
[INFO 2017-06-28 14:10:49,540 main.py:51] epoch 7325, training loss: 8828.13, average training loss: 9474.63, base loss: 14335.14
[INFO 2017-06-28 14:10:50,216 main.py:51] epoch 7326, training loss: 9277.19, average training loss: 9474.78, base loss: 14335.13
[INFO 2017-06-28 14:10:50,850 main.py:51] epoch 7327, training loss: 8946.93, average training loss: 9474.25, base loss: 14334.16
[INFO 2017-06-28 14:10:51,490 main.py:51] epoch 7328, training loss: 9316.23, average training loss: 9474.21, base loss: 14334.07
[INFO 2017-06-28 14:10:52,138 main.py:51] epoch 7329, training loss: 11001.96, average training loss: 9476.67, base loss: 14338.98
[INFO 2017-06-28 14:10:52,789 main.py:51] epoch 7330, training loss: 8603.88, average training loss: 9476.81, base loss: 14339.69
[INFO 2017-06-28 14:10:53,439 main.py:51] epoch 7331, training loss: 10192.81, average training loss: 9477.59, base loss: 14341.58
[INFO 2017-06-28 14:10:54,124 main.py:51] epoch 7332, training loss: 9081.34, average training loss: 9475.50, base loss: 14339.90
[INFO 2017-06-28 14:10:54,782 main.py:51] epoch 7333, training loss: 9939.62, average training loss: 9476.12, base loss: 14341.00
[INFO 2017-06-28 14:10:55,443 main.py:51] epoch 7334, training loss: 8948.28, average training loss: 9474.43, base loss: 14337.59
[INFO 2017-06-28 14:10:56,107 main.py:51] epoch 7335, training loss: 8397.33, average training loss: 9473.04, base loss: 14335.42
[INFO 2017-06-28 14:10:56,763 main.py:51] epoch 7336, training loss: 10301.02, average training loss: 9475.24, base loss: 14337.40
[INFO 2017-06-28 14:10:57,452 main.py:51] epoch 7337, training loss: 8922.40, average training loss: 9474.25, base loss: 14335.22
[INFO 2017-06-28 14:10:58,109 main.py:51] epoch 7338, training loss: 9827.52, average training loss: 9474.49, base loss: 14335.42
[INFO 2017-06-28 14:10:58,769 main.py:51] epoch 7339, training loss: 9736.02, average training loss: 9474.70, base loss: 14333.83
[INFO 2017-06-28 14:10:59,428 main.py:51] epoch 7340, training loss: 9156.32, average training loss: 9474.23, base loss: 14331.06
[INFO 2017-06-28 14:11:00,071 main.py:51] epoch 7341, training loss: 9498.48, average training loss: 9475.10, base loss: 14331.90
[INFO 2017-06-28 14:11:00,724 main.py:51] epoch 7342, training loss: 9335.88, average training loss: 9475.28, base loss: 14331.90
[INFO 2017-06-28 14:11:01,356 main.py:51] epoch 7343, training loss: 9494.50, average training loss: 9474.79, base loss: 14331.18
[INFO 2017-06-28 14:11:02,002 main.py:51] epoch 7344, training loss: 8523.09, average training loss: 9473.68, base loss: 14329.56
[INFO 2017-06-28 14:11:02,673 main.py:51] epoch 7345, training loss: 10842.56, average training loss: 9475.01, base loss: 14330.92
[INFO 2017-06-28 14:11:03,362 main.py:51] epoch 7346, training loss: 9216.41, average training loss: 9476.18, base loss: 14332.71
[INFO 2017-06-28 14:11:04,008 main.py:51] epoch 7347, training loss: 10451.92, average training loss: 9477.54, base loss: 14335.67
[INFO 2017-06-28 14:11:04,663 main.py:51] epoch 7348, training loss: 10050.20, average training loss: 9478.48, base loss: 14339.05
[INFO 2017-06-28 14:11:05,305 main.py:51] epoch 7349, training loss: 9167.80, average training loss: 9476.15, base loss: 14336.75
[INFO 2017-06-28 14:11:05,970 main.py:51] epoch 7350, training loss: 8997.82, average training loss: 9476.13, base loss: 14337.34
[INFO 2017-06-28 14:11:06,621 main.py:51] epoch 7351, training loss: 9605.45, average training loss: 9475.02, base loss: 14336.42
[INFO 2017-06-28 14:11:07,275 main.py:51] epoch 7352, training loss: 9197.16, average training loss: 9474.94, base loss: 14335.03
[INFO 2017-06-28 14:11:07,917 main.py:51] epoch 7353, training loss: 8643.01, average training loss: 9472.87, base loss: 14330.82
[INFO 2017-06-28 14:11:08,566 main.py:51] epoch 7354, training loss: 10136.99, average training loss: 9473.27, base loss: 14331.22
[INFO 2017-06-28 14:11:09,226 main.py:51] epoch 7355, training loss: 8726.58, average training loss: 9472.87, base loss: 14331.27
[INFO 2017-06-28 14:11:09,878 main.py:51] epoch 7356, training loss: 10250.76, average training loss: 9474.45, base loss: 14332.81
[INFO 2017-06-28 14:11:10,555 main.py:51] epoch 7357, training loss: 8667.66, average training loss: 9472.73, base loss: 14329.61
[INFO 2017-06-28 14:11:11,225 main.py:51] epoch 7358, training loss: 9279.50, average training loss: 9472.57, base loss: 14328.50
[INFO 2017-06-28 14:11:11,894 main.py:51] epoch 7359, training loss: 9760.37, average training loss: 9471.69, base loss: 14327.53
[INFO 2017-06-28 14:11:12,536 main.py:51] epoch 7360, training loss: 8875.17, average training loss: 9470.57, base loss: 14325.63
[INFO 2017-06-28 14:11:13,192 main.py:51] epoch 7361, training loss: 9495.07, average training loss: 9470.94, base loss: 14326.89
[INFO 2017-06-28 14:11:13,837 main.py:51] epoch 7362, training loss: 8956.80, average training loss: 9471.14, base loss: 14327.13
[INFO 2017-06-28 14:11:14,507 main.py:51] epoch 7363, training loss: 10017.31, average training loss: 9472.77, base loss: 14329.69
[INFO 2017-06-28 14:11:15,171 main.py:51] epoch 7364, training loss: 9495.46, average training loss: 9472.42, base loss: 14329.46
[INFO 2017-06-28 14:11:15,829 main.py:51] epoch 7365, training loss: 9523.75, average training loss: 9473.41, base loss: 14331.02
[INFO 2017-06-28 14:11:16,496 main.py:51] epoch 7366, training loss: 9800.50, average training loss: 9474.21, base loss: 14330.85
[INFO 2017-06-28 14:11:17,186 main.py:51] epoch 7367, training loss: 10044.41, average training loss: 9473.14, base loss: 14330.23
[INFO 2017-06-28 14:11:17,833 main.py:51] epoch 7368, training loss: 8844.45, average training loss: 9473.94, base loss: 14331.82
[INFO 2017-06-28 14:11:18,502 main.py:51] epoch 7369, training loss: 9844.32, average training loss: 9473.63, base loss: 14330.79
[INFO 2017-06-28 14:11:19,162 main.py:51] epoch 7370, training loss: 10185.15, average training loss: 9475.06, base loss: 14333.12
[INFO 2017-06-28 14:11:19,808 main.py:51] epoch 7371, training loss: 9532.66, average training loss: 9475.91, base loss: 14332.67
[INFO 2017-06-28 14:11:20,447 main.py:51] epoch 7372, training loss: 9776.32, average training loss: 9476.23, base loss: 14333.83
[INFO 2017-06-28 14:11:21,135 main.py:51] epoch 7373, training loss: 9698.40, average training loss: 9475.51, base loss: 14333.42
[INFO 2017-06-28 14:11:21,808 main.py:51] epoch 7374, training loss: 10215.62, average training loss: 9475.25, base loss: 14331.08
[INFO 2017-06-28 14:11:22,462 main.py:51] epoch 7375, training loss: 10587.93, average training loss: 9476.70, base loss: 14333.19
[INFO 2017-06-28 14:11:23,121 main.py:51] epoch 7376, training loss: 8890.00, average training loss: 9475.46, base loss: 14332.15
[INFO 2017-06-28 14:11:23,784 main.py:51] epoch 7377, training loss: 9092.30, average training loss: 9475.28, base loss: 14332.24
[INFO 2017-06-28 14:11:24,449 main.py:51] epoch 7378, training loss: 8889.21, average training loss: 9473.63, base loss: 14330.19
[INFO 2017-06-28 14:11:25,118 main.py:51] epoch 7379, training loss: 10120.59, average training loss: 9474.51, base loss: 14330.13
[INFO 2017-06-28 14:11:25,770 main.py:51] epoch 7380, training loss: 10307.31, average training loss: 9475.69, base loss: 14332.50
[INFO 2017-06-28 14:11:26,445 main.py:51] epoch 7381, training loss: 10155.73, average training loss: 9476.28, base loss: 14332.03
[INFO 2017-06-28 14:11:27,118 main.py:51] epoch 7382, training loss: 10083.85, average training loss: 9477.60, base loss: 14333.05
[INFO 2017-06-28 14:11:27,790 main.py:51] epoch 7383, training loss: 9325.88, average training loss: 9477.15, base loss: 14332.62
[INFO 2017-06-28 14:11:28,461 main.py:51] epoch 7384, training loss: 10232.33, average training loss: 9477.62, base loss: 14333.95
[INFO 2017-06-28 14:11:29,134 main.py:51] epoch 7385, training loss: 8601.86, average training loss: 9476.04, base loss: 14329.62
[INFO 2017-06-28 14:11:29,791 main.py:51] epoch 7386, training loss: 8934.95, average training loss: 9476.30, base loss: 14329.20
[INFO 2017-06-28 14:11:30,460 main.py:51] epoch 7387, training loss: 10266.68, average training loss: 9478.34, base loss: 14332.89
[INFO 2017-06-28 14:11:31,114 main.py:51] epoch 7388, training loss: 9084.19, average training loss: 9478.37, base loss: 14332.95
[INFO 2017-06-28 14:11:31,747 main.py:51] epoch 7389, training loss: 8316.64, average training loss: 9477.48, base loss: 14331.13
[INFO 2017-06-28 14:11:32,409 main.py:51] epoch 7390, training loss: 9020.20, average training loss: 9477.16, base loss: 14330.07
[INFO 2017-06-28 14:11:33,059 main.py:51] epoch 7391, training loss: 9309.86, average training loss: 9476.66, base loss: 14329.19
[INFO 2017-06-28 14:11:33,744 main.py:51] epoch 7392, training loss: 9055.84, average training loss: 9475.91, base loss: 14327.39
[INFO 2017-06-28 14:11:34,410 main.py:51] epoch 7393, training loss: 10794.41, average training loss: 9475.63, base loss: 14326.03
[INFO 2017-06-28 14:11:35,068 main.py:51] epoch 7394, training loss: 9289.47, average training loss: 9475.06, base loss: 14326.60
[INFO 2017-06-28 14:11:35,740 main.py:51] epoch 7395, training loss: 9450.99, average training loss: 9475.65, base loss: 14325.68
[INFO 2017-06-28 14:11:36,411 main.py:51] epoch 7396, training loss: 9436.81, average training loss: 9476.23, base loss: 14327.45
[INFO 2017-06-28 14:11:37,043 main.py:51] epoch 7397, training loss: 9207.58, average training loss: 9476.99, base loss: 14329.80
[INFO 2017-06-28 14:11:37,705 main.py:51] epoch 7398, training loss: 9191.79, average training loss: 9476.03, base loss: 14329.37
[INFO 2017-06-28 14:11:38,353 main.py:51] epoch 7399, training loss: 9251.21, average training loss: 9476.61, base loss: 14329.86
[INFO 2017-06-28 14:11:38,354 main.py:53] epoch 7399, testing
[INFO 2017-06-28 14:11:40,956 main.py:105] average testing loss: 10374.26, base loss: 14849.42
[INFO 2017-06-28 14:11:40,956 main.py:106] improve_loss: 4475.16, improve_percent: 0.30
[INFO 2017-06-28 14:11:40,957 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:11:41,612 main.py:51] epoch 7400, training loss: 9688.36, average training loss: 9477.83, base loss: 14331.80
[INFO 2017-06-28 14:11:42,275 main.py:51] epoch 7401, training loss: 10222.91, average training loss: 9479.60, base loss: 14333.18
[INFO 2017-06-28 14:11:42,935 main.py:51] epoch 7402, training loss: 11329.79, average training loss: 9481.75, base loss: 14338.12
[INFO 2017-06-28 14:11:43,603 main.py:51] epoch 7403, training loss: 9686.59, average training loss: 9482.26, base loss: 14339.06
[INFO 2017-06-28 14:11:44,296 main.py:51] epoch 7404, training loss: 11337.99, average training loss: 9483.66, base loss: 14341.54
[INFO 2017-06-28 14:11:44,985 main.py:51] epoch 7405, training loss: 8533.88, average training loss: 9481.88, base loss: 14338.98
[INFO 2017-06-28 14:11:45,676 main.py:51] epoch 7406, training loss: 9470.80, average training loss: 9482.31, base loss: 14339.88
[INFO 2017-06-28 14:11:46,340 main.py:51] epoch 7407, training loss: 10183.48, average training loss: 9482.70, base loss: 14341.01
[INFO 2017-06-28 14:11:47,026 main.py:51] epoch 7408, training loss: 8925.34, average training loss: 9480.97, base loss: 14338.40
[INFO 2017-06-28 14:11:47,691 main.py:51] epoch 7409, training loss: 9393.60, average training loss: 9480.75, base loss: 14338.92
[INFO 2017-06-28 14:11:48,360 main.py:51] epoch 7410, training loss: 8929.29, average training loss: 9479.98, base loss: 14338.80
[INFO 2017-06-28 14:11:49,026 main.py:51] epoch 7411, training loss: 10895.02, average training loss: 9481.06, base loss: 14341.90
[INFO 2017-06-28 14:11:49,689 main.py:51] epoch 7412, training loss: 10549.35, average training loss: 9481.32, base loss: 14342.16
[INFO 2017-06-28 14:11:50,361 main.py:51] epoch 7413, training loss: 9671.69, average training loss: 9481.70, base loss: 14341.57
[INFO 2017-06-28 14:11:50,992 main.py:51] epoch 7414, training loss: 9769.42, average training loss: 9480.41, base loss: 14339.59
[INFO 2017-06-28 14:11:51,638 main.py:51] epoch 7415, training loss: 9281.28, average training loss: 9480.15, base loss: 14338.06
[INFO 2017-06-28 14:11:52,294 main.py:51] epoch 7416, training loss: 9884.89, average training loss: 9480.58, base loss: 14337.41
[INFO 2017-06-28 14:11:52,970 main.py:51] epoch 7417, training loss: 10007.18, average training loss: 9480.30, base loss: 14337.94
[INFO 2017-06-28 14:11:53,621 main.py:51] epoch 7418, training loss: 10153.46, average training loss: 9481.07, base loss: 14340.76
[INFO 2017-06-28 14:11:54,268 main.py:51] epoch 7419, training loss: 9982.09, average training loss: 9482.26, base loss: 14342.27
[INFO 2017-06-28 14:11:54,927 main.py:51] epoch 7420, training loss: 9562.83, average training loss: 9481.39, base loss: 14341.28
[INFO 2017-06-28 14:11:55,583 main.py:51] epoch 7421, training loss: 10052.67, average training loss: 9482.66, base loss: 14344.19
[INFO 2017-06-28 14:11:56,242 main.py:51] epoch 7422, training loss: 9881.28, average training loss: 9482.63, base loss: 14344.32
[INFO 2017-06-28 14:11:56,922 main.py:51] epoch 7423, training loss: 8785.86, average training loss: 9481.36, base loss: 14342.84
[INFO 2017-06-28 14:11:57,590 main.py:51] epoch 7424, training loss: 9395.17, average training loss: 9479.94, base loss: 14340.43
[INFO 2017-06-28 14:11:58,250 main.py:51] epoch 7425, training loss: 9880.87, average training loss: 9480.25, base loss: 14339.75
[INFO 2017-06-28 14:11:58,927 main.py:51] epoch 7426, training loss: 9906.36, average training loss: 9480.75, base loss: 14341.31
[INFO 2017-06-28 14:11:59,608 main.py:51] epoch 7427, training loss: 8955.57, average training loss: 9480.02, base loss: 14340.81
[INFO 2017-06-28 14:12:00,260 main.py:51] epoch 7428, training loss: 9139.90, average training loss: 9479.83, base loss: 14341.04
[INFO 2017-06-28 14:12:00,927 main.py:51] epoch 7429, training loss: 9397.37, average training loss: 9480.54, base loss: 14343.79
[INFO 2017-06-28 14:12:01,593 main.py:51] epoch 7430, training loss: 10076.34, average training loss: 9482.11, base loss: 14343.59
[INFO 2017-06-28 14:12:02,276 main.py:51] epoch 7431, training loss: 9465.38, average training loss: 9481.72, base loss: 14343.80
[INFO 2017-06-28 14:12:02,944 main.py:51] epoch 7432, training loss: 10042.60, average training loss: 9483.31, base loss: 14345.47
[INFO 2017-06-28 14:12:03,590 main.py:51] epoch 7433, training loss: 8849.66, average training loss: 9483.80, base loss: 14346.16
[INFO 2017-06-28 14:12:04,264 main.py:51] epoch 7434, training loss: 10157.10, average training loss: 9484.49, base loss: 14347.47
[INFO 2017-06-28 14:12:04,908 main.py:51] epoch 7435, training loss: 9639.05, average training loss: 9484.82, base loss: 14347.63
[INFO 2017-06-28 14:12:05,571 main.py:51] epoch 7436, training loss: 9522.07, average training loss: 9485.31, base loss: 14347.37
[INFO 2017-06-28 14:12:06,234 main.py:51] epoch 7437, training loss: 9082.79, average training loss: 9484.65, base loss: 14346.32
[INFO 2017-06-28 14:12:06,891 main.py:51] epoch 7438, training loss: 8890.66, average training loss: 9483.69, base loss: 14345.46
[INFO 2017-06-28 14:12:07,574 main.py:51] epoch 7439, training loss: 9504.65, average training loss: 9482.54, base loss: 14344.07
[INFO 2017-06-28 14:12:08,230 main.py:51] epoch 7440, training loss: 9500.93, average training loss: 9482.75, base loss: 14346.75
[INFO 2017-06-28 14:12:08,895 main.py:51] epoch 7441, training loss: 9192.78, average training loss: 9483.28, base loss: 14347.23
[INFO 2017-06-28 14:12:09,574 main.py:51] epoch 7442, training loss: 9502.07, average training loss: 9483.57, base loss: 14348.18
[INFO 2017-06-28 14:12:10,244 main.py:51] epoch 7443, training loss: 9533.53, average training loss: 9484.46, base loss: 14349.92
[INFO 2017-06-28 14:12:10,904 main.py:51] epoch 7444, training loss: 10797.76, average training loss: 9486.04, base loss: 14352.86
[INFO 2017-06-28 14:12:11,556 main.py:51] epoch 7445, training loss: 10208.69, average training loss: 9485.86, base loss: 14352.00
[INFO 2017-06-28 14:12:12,242 main.py:51] epoch 7446, training loss: 9498.50, average training loss: 9486.47, base loss: 14352.32
[INFO 2017-06-28 14:12:12,901 main.py:51] epoch 7447, training loss: 8418.52, average training loss: 9485.17, base loss: 14350.43
[INFO 2017-06-28 14:12:13,591 main.py:51] epoch 7448, training loss: 9634.16, average training loss: 9486.58, base loss: 14355.57
[INFO 2017-06-28 14:12:14,255 main.py:51] epoch 7449, training loss: 9055.58, average training loss: 9485.15, base loss: 14353.34
[INFO 2017-06-28 14:12:14,914 main.py:51] epoch 7450, training loss: 8480.52, average training loss: 9483.75, base loss: 14351.39
[INFO 2017-06-28 14:12:15,565 main.py:51] epoch 7451, training loss: 9947.66, average training loss: 9483.42, base loss: 14350.82
[INFO 2017-06-28 14:12:16,232 main.py:51] epoch 7452, training loss: 11934.02, average training loss: 9486.75, base loss: 14356.51
[INFO 2017-06-28 14:12:16,878 main.py:51] epoch 7453, training loss: 10230.24, average training loss: 9486.78, base loss: 14359.18
[INFO 2017-06-28 14:12:17,534 main.py:51] epoch 7454, training loss: 10210.45, average training loss: 9487.57, base loss: 14361.18
[INFO 2017-06-28 14:12:18,200 main.py:51] epoch 7455, training loss: 9195.11, average training loss: 9486.60, base loss: 14360.81
[INFO 2017-06-28 14:12:18,862 main.py:51] epoch 7456, training loss: 10353.80, average training loss: 9487.66, base loss: 14362.36
[INFO 2017-06-28 14:12:19,522 main.py:51] epoch 7457, training loss: 10287.16, average training loss: 9488.04, base loss: 14361.37
[INFO 2017-06-28 14:12:20,166 main.py:51] epoch 7458, training loss: 8392.52, average training loss: 9487.79, base loss: 14360.88
[INFO 2017-06-28 14:12:20,829 main.py:51] epoch 7459, training loss: 7985.09, average training loss: 9485.26, base loss: 14356.19
[INFO 2017-06-28 14:12:21,474 main.py:51] epoch 7460, training loss: 9867.26, average training loss: 9485.88, base loss: 14355.90
[INFO 2017-06-28 14:12:22,136 main.py:51] epoch 7461, training loss: 8727.07, average training loss: 9486.17, base loss: 14356.96
[INFO 2017-06-28 14:12:22,790 main.py:51] epoch 7462, training loss: 9878.70, average training loss: 9485.44, base loss: 14355.13
[INFO 2017-06-28 14:12:23,443 main.py:51] epoch 7463, training loss: 8120.83, average training loss: 9484.98, base loss: 14354.76
[INFO 2017-06-28 14:12:24,100 main.py:51] epoch 7464, training loss: 9575.93, average training loss: 9485.71, base loss: 14357.28
[INFO 2017-06-28 14:12:24,754 main.py:51] epoch 7465, training loss: 8256.32, average training loss: 9483.61, base loss: 14353.15
[INFO 2017-06-28 14:12:25,412 main.py:51] epoch 7466, training loss: 9806.49, average training loss: 9484.71, base loss: 14356.23
[INFO 2017-06-28 14:12:26,061 main.py:51] epoch 7467, training loss: 10046.40, average training loss: 9485.77, base loss: 14358.85
[INFO 2017-06-28 14:12:26,721 main.py:51] epoch 7468, training loss: 10723.37, average training loss: 9487.20, base loss: 14360.15
[INFO 2017-06-28 14:12:27,403 main.py:51] epoch 7469, training loss: 8008.07, average training loss: 9486.04, base loss: 14357.56
[INFO 2017-06-28 14:12:28,071 main.py:51] epoch 7470, training loss: 10196.31, average training loss: 9484.95, base loss: 14355.86
[INFO 2017-06-28 14:12:28,747 main.py:51] epoch 7471, training loss: 10180.23, average training loss: 9485.70, base loss: 14355.88
[INFO 2017-06-28 14:12:29,416 main.py:51] epoch 7472, training loss: 9447.20, average training loss: 9486.22, base loss: 14356.94
[INFO 2017-06-28 14:12:30,088 main.py:51] epoch 7473, training loss: 10406.54, average training loss: 9487.25, base loss: 14358.41
[INFO 2017-06-28 14:12:30,770 main.py:51] epoch 7474, training loss: 9349.83, average training loss: 9486.57, base loss: 14356.46
[INFO 2017-06-28 14:12:31,429 main.py:51] epoch 7475, training loss: 9454.20, average training loss: 9486.67, base loss: 14356.46
[INFO 2017-06-28 14:12:32,081 main.py:51] epoch 7476, training loss: 9191.14, average training loss: 9487.00, base loss: 14358.13
[INFO 2017-06-28 14:12:32,733 main.py:51] epoch 7477, training loss: 8906.62, average training loss: 9485.87, base loss: 14355.11
[INFO 2017-06-28 14:12:33,391 main.py:51] epoch 7478, training loss: 10350.42, average training loss: 9485.81, base loss: 14354.49
[INFO 2017-06-28 14:12:34,051 main.py:51] epoch 7479, training loss: 9720.07, average training loss: 9485.98, base loss: 14355.18
[INFO 2017-06-28 14:12:34,715 main.py:51] epoch 7480, training loss: 8613.81, average training loss: 9485.16, base loss: 14354.61
[INFO 2017-06-28 14:12:35,390 main.py:51] epoch 7481, training loss: 8294.59, average training loss: 9484.22, base loss: 14352.40
[INFO 2017-06-28 14:12:36,043 main.py:51] epoch 7482, training loss: 9161.47, average training loss: 9484.44, base loss: 14351.63
[INFO 2017-06-28 14:12:36,707 main.py:51] epoch 7483, training loss: 9419.08, average training loss: 9484.32, base loss: 14352.39
[INFO 2017-06-28 14:12:37,374 main.py:51] epoch 7484, training loss: 8228.13, average training loss: 9483.83, base loss: 14352.38
[INFO 2017-06-28 14:12:38,039 main.py:51] epoch 7485, training loss: 9412.75, average training loss: 9484.68, base loss: 14354.48
[INFO 2017-06-28 14:12:38,717 main.py:51] epoch 7486, training loss: 11173.10, average training loss: 9486.55, base loss: 14357.24
[INFO 2017-06-28 14:12:39,377 main.py:51] epoch 7487, training loss: 8947.99, average training loss: 9485.31, base loss: 14357.12
[INFO 2017-06-28 14:12:40,032 main.py:51] epoch 7488, training loss: 8642.34, average training loss: 9485.50, base loss: 14356.97
[INFO 2017-06-28 14:12:40,705 main.py:51] epoch 7489, training loss: 10342.73, average training loss: 9487.40, base loss: 14359.90
[INFO 2017-06-28 14:12:41,386 main.py:51] epoch 7490, training loss: 9166.53, average training loss: 9485.91, base loss: 14358.37
[INFO 2017-06-28 14:12:42,053 main.py:51] epoch 7491, training loss: 9275.05, average training loss: 9486.42, base loss: 14356.92
[INFO 2017-06-28 14:12:42,762 main.py:51] epoch 7492, training loss: 7955.66, average training loss: 9484.91, base loss: 14354.70
[INFO 2017-06-28 14:12:43,413 main.py:51] epoch 7493, training loss: 9389.79, average training loss: 9483.16, base loss: 14352.64
[INFO 2017-06-28 14:12:44,088 main.py:51] epoch 7494, training loss: 9449.13, average training loss: 9483.32, base loss: 14353.00
[INFO 2017-06-28 14:12:44,774 main.py:51] epoch 7495, training loss: 9050.19, average training loss: 9482.69, base loss: 14350.51
[INFO 2017-06-28 14:12:45,434 main.py:51] epoch 7496, training loss: 8605.59, average training loss: 9481.55, base loss: 14348.31
[INFO 2017-06-28 14:12:46,108 main.py:51] epoch 7497, training loss: 7177.00, average training loss: 9477.90, base loss: 14344.06
[INFO 2017-06-28 14:12:46,761 main.py:51] epoch 7498, training loss: 10249.29, average training loss: 9478.66, base loss: 14345.70
[INFO 2017-06-28 14:12:47,448 main.py:51] epoch 7499, training loss: 9413.02, average training loss: 9477.99, base loss: 14343.51
[INFO 2017-06-28 14:12:47,448 main.py:53] epoch 7499, testing
[INFO 2017-06-28 14:12:50,070 main.py:105] average testing loss: 11165.63, base loss: 16106.15
[INFO 2017-06-28 14:12:50,070 main.py:106] improve_loss: 4940.52, improve_percent: 0.31
[INFO 2017-06-28 14:12:50,071 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:12:50,123 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:12:50,831 main.py:51] epoch 7500, training loss: 8801.18, average training loss: 9478.45, base loss: 14344.74
[INFO 2017-06-28 14:12:51,476 main.py:51] epoch 7501, training loss: 9391.80, average training loss: 9475.73, base loss: 14339.68
[INFO 2017-06-28 14:12:52,133 main.py:51] epoch 7502, training loss: 8636.34, average training loss: 9474.80, base loss: 14337.51
[INFO 2017-06-28 14:12:52,808 main.py:51] epoch 7503, training loss: 8933.38, average training loss: 9474.59, base loss: 14338.13
[INFO 2017-06-28 14:12:53,465 main.py:51] epoch 7504, training loss: 10511.84, average training loss: 9475.98, base loss: 14339.73
[INFO 2017-06-28 14:12:54,124 main.py:51] epoch 7505, training loss: 9156.99, average training loss: 9475.29, base loss: 14337.66
[INFO 2017-06-28 14:12:54,789 main.py:51] epoch 7506, training loss: 9791.27, average training loss: 9475.60, base loss: 14337.88
[INFO 2017-06-28 14:12:55,450 main.py:51] epoch 7507, training loss: 10068.61, average training loss: 9475.15, base loss: 14337.05
[INFO 2017-06-28 14:12:56,118 main.py:51] epoch 7508, training loss: 9100.56, average training loss: 9473.87, base loss: 14334.12
[INFO 2017-06-28 14:12:56,763 main.py:51] epoch 7509, training loss: 9900.56, average training loss: 9472.97, base loss: 14332.40
[INFO 2017-06-28 14:12:57,437 main.py:51] epoch 7510, training loss: 9902.90, average training loss: 9473.54, base loss: 14333.21
[INFO 2017-06-28 14:12:58,105 main.py:51] epoch 7511, training loss: 11155.69, average training loss: 9475.95, base loss: 14335.96
[INFO 2017-06-28 14:12:58,774 main.py:51] epoch 7512, training loss: 9832.21, average training loss: 9475.18, base loss: 14336.75
[INFO 2017-06-28 14:12:59,418 main.py:51] epoch 7513, training loss: 8138.68, average training loss: 9474.69, base loss: 14336.00
[INFO 2017-06-28 14:13:00,084 main.py:51] epoch 7514, training loss: 10592.08, average training loss: 9475.52, base loss: 14334.41
[INFO 2017-06-28 14:13:00,757 main.py:51] epoch 7515, training loss: 9062.76, average training loss: 9475.35, base loss: 14334.13
[INFO 2017-06-28 14:13:01,445 main.py:51] epoch 7516, training loss: 9497.32, average training loss: 9475.34, base loss: 14333.74
[INFO 2017-06-28 14:13:02,132 main.py:51] epoch 7517, training loss: 12971.08, average training loss: 9478.53, base loss: 14337.84
[INFO 2017-06-28 14:13:02,794 main.py:51] epoch 7518, training loss: 9858.18, average training loss: 9478.18, base loss: 14337.14
[INFO 2017-06-28 14:13:03,459 main.py:51] epoch 7519, training loss: 9204.06, average training loss: 9478.60, base loss: 14338.93
[INFO 2017-06-28 14:13:04,113 main.py:51] epoch 7520, training loss: 9789.31, average training loss: 9478.82, base loss: 14338.52
[INFO 2017-06-28 14:13:04,769 main.py:51] epoch 7521, training loss: 10325.72, average training loss: 9480.24, base loss: 14340.49
[INFO 2017-06-28 14:13:05,430 main.py:51] epoch 7522, training loss: 8717.08, average training loss: 9478.77, base loss: 14338.22
[INFO 2017-06-28 14:13:06,090 main.py:51] epoch 7523, training loss: 9435.74, average training loss: 9479.41, base loss: 14340.35
[INFO 2017-06-28 14:13:06,749 main.py:51] epoch 7524, training loss: 10888.06, average training loss: 9480.63, base loss: 14340.56
[INFO 2017-06-28 14:13:07,414 main.py:51] epoch 7525, training loss: 9831.75, average training loss: 9481.27, base loss: 14340.85
[INFO 2017-06-28 14:13:08,096 main.py:51] epoch 7526, training loss: 8763.40, average training loss: 9480.66, base loss: 14340.32
[INFO 2017-06-28 14:13:08,735 main.py:51] epoch 7527, training loss: 9792.66, average training loss: 9481.28, base loss: 14340.59
[INFO 2017-06-28 14:13:09,428 main.py:51] epoch 7528, training loss: 9729.71, average training loss: 9481.87, base loss: 14342.84
[INFO 2017-06-28 14:13:10,088 main.py:51] epoch 7529, training loss: 8321.46, average training loss: 9479.69, base loss: 14338.87
[INFO 2017-06-28 14:13:10,751 main.py:51] epoch 7530, training loss: 10701.09, average training loss: 9480.34, base loss: 14340.10
[INFO 2017-06-28 14:13:11,411 main.py:51] epoch 7531, training loss: 9689.72, average training loss: 9480.52, base loss: 14338.88
[INFO 2017-06-28 14:13:12,090 main.py:51] epoch 7532, training loss: 9667.42, average training loss: 9481.05, base loss: 14338.30
[INFO 2017-06-28 14:13:12,755 main.py:51] epoch 7533, training loss: 8726.45, average training loss: 9479.56, base loss: 14336.59
[INFO 2017-06-28 14:13:13,429 main.py:51] epoch 7534, training loss: 9653.25, average training loss: 9480.29, base loss: 14338.46
[INFO 2017-06-28 14:13:14,086 main.py:51] epoch 7535, training loss: 9226.75, average training loss: 9480.34, base loss: 14339.00
[INFO 2017-06-28 14:13:14,754 main.py:51] epoch 7536, training loss: 10340.88, average training loss: 9482.29, base loss: 14342.35
[INFO 2017-06-28 14:13:15,415 main.py:51] epoch 7537, training loss: 8822.13, average training loss: 9481.89, base loss: 14341.55
[INFO 2017-06-28 14:13:16,085 main.py:51] epoch 7538, training loss: 8771.36, average training loss: 9479.54, base loss: 14340.07
[INFO 2017-06-28 14:13:16,766 main.py:51] epoch 7539, training loss: 9543.53, average training loss: 9481.31, base loss: 14342.42
[INFO 2017-06-28 14:13:17,423 main.py:51] epoch 7540, training loss: 10019.29, average training loss: 9480.40, base loss: 14339.94
[INFO 2017-06-28 14:13:18,078 main.py:51] epoch 7541, training loss: 9903.83, average training loss: 9480.14, base loss: 14337.36
[INFO 2017-06-28 14:13:18,736 main.py:51] epoch 7542, training loss: 10254.39, average training loss: 9479.99, base loss: 14337.63
[INFO 2017-06-28 14:13:19,404 main.py:51] epoch 7543, training loss: 9733.97, average training loss: 9480.18, base loss: 14337.81
[INFO 2017-06-28 14:13:20,062 main.py:51] epoch 7544, training loss: 9627.94, average training loss: 9479.08, base loss: 14335.29
[INFO 2017-06-28 14:13:20,710 main.py:51] epoch 7545, training loss: 9175.10, average training loss: 9478.15, base loss: 14334.19
[INFO 2017-06-28 14:13:21,372 main.py:51] epoch 7546, training loss: 8790.91, average training loss: 9478.48, base loss: 14333.80
[INFO 2017-06-28 14:13:22,043 main.py:51] epoch 7547, training loss: 8625.96, average training loss: 9476.91, base loss: 14330.44
[INFO 2017-06-28 14:13:22,713 main.py:51] epoch 7548, training loss: 9201.27, average training loss: 9476.13, base loss: 14331.85
[INFO 2017-06-28 14:13:23,347 main.py:51] epoch 7549, training loss: 9490.79, average training loss: 9474.92, base loss: 14331.60
[INFO 2017-06-28 14:13:24,025 main.py:51] epoch 7550, training loss: 9187.06, average training loss: 9472.59, base loss: 14329.56
[INFO 2017-06-28 14:13:24,679 main.py:51] epoch 7551, training loss: 10917.67, average training loss: 9473.28, base loss: 14331.29
[INFO 2017-06-28 14:13:25,332 main.py:51] epoch 7552, training loss: 10903.18, average training loss: 9474.21, base loss: 14332.90
[INFO 2017-06-28 14:13:25,993 main.py:51] epoch 7553, training loss: 10021.85, average training loss: 9474.85, base loss: 14334.15
[INFO 2017-06-28 14:13:26,669 main.py:51] epoch 7554, training loss: 10227.82, average training loss: 9477.14, base loss: 14338.03
[INFO 2017-06-28 14:13:27,309 main.py:51] epoch 7555, training loss: 7652.73, average training loss: 9476.72, base loss: 14338.15
[INFO 2017-06-28 14:13:27,966 main.py:51] epoch 7556, training loss: 8896.68, average training loss: 9476.17, base loss: 14338.23
[INFO 2017-06-28 14:13:28,632 main.py:51] epoch 7557, training loss: 9089.69, average training loss: 9475.80, base loss: 14339.68
[INFO 2017-06-28 14:13:29,292 main.py:51] epoch 7558, training loss: 9897.66, average training loss: 9476.46, base loss: 14340.43
[INFO 2017-06-28 14:13:29,971 main.py:51] epoch 7559, training loss: 9657.71, average training loss: 9476.32, base loss: 14341.15
[INFO 2017-06-28 14:13:30,679 main.py:51] epoch 7560, training loss: 9732.49, average training loss: 9476.87, base loss: 14339.85
[INFO 2017-06-28 14:13:31,352 main.py:51] epoch 7561, training loss: 10635.11, average training loss: 9478.17, base loss: 14340.19
[INFO 2017-06-28 14:13:32,012 main.py:51] epoch 7562, training loss: 9494.63, average training loss: 9477.23, base loss: 14340.60
[INFO 2017-06-28 14:13:32,674 main.py:51] epoch 7563, training loss: 9732.23, average training loss: 9477.56, base loss: 14342.05
[INFO 2017-06-28 14:13:33,336 main.py:51] epoch 7564, training loss: 8633.28, average training loss: 9476.28, base loss: 14339.66
[INFO 2017-06-28 14:13:33,995 main.py:51] epoch 7565, training loss: 11633.90, average training loss: 9478.80, base loss: 14343.51
[INFO 2017-06-28 14:13:34,692 main.py:51] epoch 7566, training loss: 9285.44, average training loss: 9476.85, base loss: 14340.58
[INFO 2017-06-28 14:13:35,333 main.py:51] epoch 7567, training loss: 9193.25, average training loss: 9475.84, base loss: 14339.44
[INFO 2017-06-28 14:13:35,990 main.py:51] epoch 7568, training loss: 9741.13, average training loss: 9476.04, base loss: 14339.66
[INFO 2017-06-28 14:13:36,660 main.py:51] epoch 7569, training loss: 10752.32, average training loss: 9477.54, base loss: 14342.97
[INFO 2017-06-28 14:13:37,330 main.py:51] epoch 7570, training loss: 8584.59, average training loss: 9476.24, base loss: 14342.33
[INFO 2017-06-28 14:13:37,992 main.py:51] epoch 7571, training loss: 8205.63, average training loss: 9475.34, base loss: 14339.07
[INFO 2017-06-28 14:13:38,645 main.py:51] epoch 7572, training loss: 8553.93, average training loss: 9474.66, base loss: 14339.29
[INFO 2017-06-28 14:13:39,314 main.py:51] epoch 7573, training loss: 9906.63, average training loss: 9474.59, base loss: 14340.29
[INFO 2017-06-28 14:13:39,972 main.py:51] epoch 7574, training loss: 8371.10, average training loss: 9473.01, base loss: 14337.73
[INFO 2017-06-28 14:13:40,647 main.py:51] epoch 7575, training loss: 9669.52, average training loss: 9475.02, base loss: 14342.33
[INFO 2017-06-28 14:13:41,319 main.py:51] epoch 7576, training loss: 9647.56, average training loss: 9474.54, base loss: 14340.00
[INFO 2017-06-28 14:13:41,996 main.py:51] epoch 7577, training loss: 9591.14, average training loss: 9474.55, base loss: 14341.48
[INFO 2017-06-28 14:13:42,666 main.py:51] epoch 7578, training loss: 9787.49, average training loss: 9473.84, base loss: 14340.83
[INFO 2017-06-28 14:13:43,325 main.py:51] epoch 7579, training loss: 8900.05, average training loss: 9473.06, base loss: 14338.73
[INFO 2017-06-28 14:13:43,985 main.py:51] epoch 7580, training loss: 9378.55, average training loss: 9471.93, base loss: 14336.70
[INFO 2017-06-28 14:13:44,639 main.py:51] epoch 7581, training loss: 9507.66, average training loss: 9472.31, base loss: 14336.72
[INFO 2017-06-28 14:13:45,302 main.py:51] epoch 7582, training loss: 9383.04, average training loss: 9472.56, base loss: 14339.27
[INFO 2017-06-28 14:13:45,949 main.py:51] epoch 7583, training loss: 11141.11, average training loss: 9473.61, base loss: 14340.99
[INFO 2017-06-28 14:13:46,623 main.py:51] epoch 7584, training loss: 9391.84, average training loss: 9473.84, base loss: 14340.69
[INFO 2017-06-28 14:13:47,302 main.py:51] epoch 7585, training loss: 9757.82, average training loss: 9475.50, base loss: 14343.04
[INFO 2017-06-28 14:13:47,964 main.py:51] epoch 7586, training loss: 9514.16, average training loss: 9474.78, base loss: 14340.99
[INFO 2017-06-28 14:13:48,597 main.py:51] epoch 7587, training loss: 11164.77, average training loss: 9476.56, base loss: 14344.29
[INFO 2017-06-28 14:13:49,252 main.py:51] epoch 7588, training loss: 9678.45, average training loss: 9476.50, base loss: 14343.01
[INFO 2017-06-28 14:13:49,924 main.py:51] epoch 7589, training loss: 8488.64, average training loss: 9474.63, base loss: 14341.31
[INFO 2017-06-28 14:13:50,599 main.py:51] epoch 7590, training loss: 10414.04, average training loss: 9475.89, base loss: 14345.01
[INFO 2017-06-28 14:13:51,263 main.py:51] epoch 7591, training loss: 9595.55, average training loss: 9474.80, base loss: 14344.44
[INFO 2017-06-28 14:13:51,920 main.py:51] epoch 7592, training loss: 8746.76, average training loss: 9474.02, base loss: 14342.93
[INFO 2017-06-28 14:13:52,596 main.py:51] epoch 7593, training loss: 8641.83, average training loss: 9474.13, base loss: 14342.98
[INFO 2017-06-28 14:13:53,255 main.py:51] epoch 7594, training loss: 10362.86, average training loss: 9474.23, base loss: 14344.65
[INFO 2017-06-28 14:13:53,902 main.py:51] epoch 7595, training loss: 8966.94, average training loss: 9473.20, base loss: 14343.16
[INFO 2017-06-28 14:13:54,562 main.py:51] epoch 7596, training loss: 9778.12, average training loss: 9473.95, base loss: 14346.03
[INFO 2017-06-28 14:13:55,233 main.py:51] epoch 7597, training loss: 9068.13, average training loss: 9471.57, base loss: 14342.52
[INFO 2017-06-28 14:13:55,894 main.py:51] epoch 7598, training loss: 8991.33, average training loss: 9470.41, base loss: 14340.62
[INFO 2017-06-28 14:13:56,558 main.py:51] epoch 7599, training loss: 9255.66, average training loss: 9470.32, base loss: 14340.08
[INFO 2017-06-28 14:13:56,558 main.py:53] epoch 7599, testing
[INFO 2017-06-28 14:13:59,135 main.py:105] average testing loss: 11051.02, base loss: 15727.32
[INFO 2017-06-28 14:13:59,135 main.py:106] improve_loss: 4676.29, improve_percent: 0.30
[INFO 2017-06-28 14:13:59,136 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:13:59,793 main.py:51] epoch 7600, training loss: 8405.32, average training loss: 9469.59, base loss: 14339.43
[INFO 2017-06-28 14:14:00,438 main.py:51] epoch 7601, training loss: 9888.75, average training loss: 9469.69, base loss: 14340.69
[INFO 2017-06-28 14:14:01,101 main.py:51] epoch 7602, training loss: 8571.92, average training loss: 9469.11, base loss: 14339.75
[INFO 2017-06-28 14:14:01,750 main.py:51] epoch 7603, training loss: 8956.36, average training loss: 9469.41, base loss: 14342.08
[INFO 2017-06-28 14:14:02,405 main.py:51] epoch 7604, training loss: 8719.35, average training loss: 9466.37, base loss: 14336.35
[INFO 2017-06-28 14:14:03,090 main.py:51] epoch 7605, training loss: 10533.61, average training loss: 9468.24, base loss: 14339.59
[INFO 2017-06-28 14:14:03,759 main.py:51] epoch 7606, training loss: 10395.90, average training loss: 9468.97, base loss: 14341.24
[INFO 2017-06-28 14:14:04,415 main.py:51] epoch 7607, training loss: 9326.45, average training loss: 9468.82, base loss: 14341.52
[INFO 2017-06-28 14:14:05,055 main.py:51] epoch 7608, training loss: 9584.38, average training loss: 9469.93, base loss: 14343.00
[INFO 2017-06-28 14:14:05,707 main.py:51] epoch 7609, training loss: 11421.86, average training loss: 9472.43, base loss: 14346.10
[INFO 2017-06-28 14:14:06,379 main.py:51] epoch 7610, training loss: 10845.63, average training loss: 9474.89, base loss: 14350.20
[INFO 2017-06-28 14:14:07,040 main.py:51] epoch 7611, training loss: 9788.11, average training loss: 9475.19, base loss: 14350.19
[INFO 2017-06-28 14:14:07,711 main.py:51] epoch 7612, training loss: 9122.87, average training loss: 9476.12, base loss: 14350.48
[INFO 2017-06-28 14:14:08,374 main.py:51] epoch 7613, training loss: 8709.04, average training loss: 9474.96, base loss: 14350.38
[INFO 2017-06-28 14:14:09,024 main.py:51] epoch 7614, training loss: 9535.85, average training loss: 9474.79, base loss: 14349.77
[INFO 2017-06-28 14:14:09,679 main.py:51] epoch 7615, training loss: 9448.15, average training loss: 9474.71, base loss: 14349.22
[INFO 2017-06-28 14:14:10,333 main.py:51] epoch 7616, training loss: 10662.05, average training loss: 9476.10, base loss: 14352.00
[INFO 2017-06-28 14:14:10,974 main.py:51] epoch 7617, training loss: 9127.44, average training loss: 9476.57, base loss: 14352.95
[INFO 2017-06-28 14:14:11,637 main.py:51] epoch 7618, training loss: 9687.63, average training loss: 9476.36, base loss: 14354.89
[INFO 2017-06-28 14:14:12,298 main.py:51] epoch 7619, training loss: 9094.56, average training loss: 9477.05, base loss: 14354.92
[INFO 2017-06-28 14:14:12,975 main.py:51] epoch 7620, training loss: 9241.87, average training loss: 9476.52, base loss: 14354.19
[INFO 2017-06-28 14:14:13,652 main.py:51] epoch 7621, training loss: 10023.14, average training loss: 9476.85, base loss: 14355.42
[INFO 2017-06-28 14:14:14,311 main.py:51] epoch 7622, training loss: 10270.15, average training loss: 9477.52, base loss: 14357.37
[INFO 2017-06-28 14:14:14,964 main.py:51] epoch 7623, training loss: 9048.44, average training loss: 9477.68, base loss: 14357.81
[INFO 2017-06-28 14:14:15,624 main.py:51] epoch 7624, training loss: 10021.31, average training loss: 9478.88, base loss: 14360.51
[INFO 2017-06-28 14:14:16,266 main.py:51] epoch 7625, training loss: 9733.42, average training loss: 9480.32, base loss: 14362.66
[INFO 2017-06-28 14:14:16,914 main.py:51] epoch 7626, training loss: 9226.25, average training loss: 9479.04, base loss: 14359.31
[INFO 2017-06-28 14:14:17,541 main.py:51] epoch 7627, training loss: 9352.81, average training loss: 9478.74, base loss: 14358.91
[INFO 2017-06-28 14:14:18,200 main.py:51] epoch 7628, training loss: 10348.08, average training loss: 9479.51, base loss: 14361.26
[INFO 2017-06-28 14:14:18,872 main.py:51] epoch 7629, training loss: 10110.18, average training loss: 9481.67, base loss: 14365.19
[INFO 2017-06-28 14:14:19,525 main.py:51] epoch 7630, training loss: 8878.09, average training loss: 9481.41, base loss: 14363.89
[INFO 2017-06-28 14:14:20,174 main.py:51] epoch 7631, training loss: 8424.79, average training loss: 9480.40, base loss: 14362.23
[INFO 2017-06-28 14:14:20,834 main.py:51] epoch 7632, training loss: 10032.20, average training loss: 9481.38, base loss: 14363.53
[INFO 2017-06-28 14:14:21,477 main.py:51] epoch 7633, training loss: 9394.78, average training loss: 9481.21, base loss: 14364.18
[INFO 2017-06-28 14:14:22,179 main.py:51] epoch 7634, training loss: 8288.00, average training loss: 9479.32, base loss: 14360.28
[INFO 2017-06-28 14:14:22,857 main.py:51] epoch 7635, training loss: 9443.98, average training loss: 9479.47, base loss: 14360.12
[INFO 2017-06-28 14:14:23,532 main.py:51] epoch 7636, training loss: 11297.62, average training loss: 9481.25, base loss: 14362.90
[INFO 2017-06-28 14:14:24,176 main.py:51] epoch 7637, training loss: 9476.79, average training loss: 9481.60, base loss: 14364.08
[INFO 2017-06-28 14:14:24,847 main.py:51] epoch 7638, training loss: 10589.04, average training loss: 9483.44, base loss: 14366.01
[INFO 2017-06-28 14:14:25,483 main.py:51] epoch 7639, training loss: 9755.73, average training loss: 9483.88, base loss: 14368.19
[INFO 2017-06-28 14:14:26,148 main.py:51] epoch 7640, training loss: 9003.66, average training loss: 9483.64, base loss: 14366.37
[INFO 2017-06-28 14:14:26,818 main.py:51] epoch 7641, training loss: 10915.72, average training loss: 9485.46, base loss: 14369.39
[INFO 2017-06-28 14:14:27,472 main.py:51] epoch 7642, training loss: 9138.47, average training loss: 9485.42, base loss: 14367.70
[INFO 2017-06-28 14:14:28,115 main.py:51] epoch 7643, training loss: 8943.90, average training loss: 9485.39, base loss: 14368.49
[INFO 2017-06-28 14:14:28,771 main.py:51] epoch 7644, training loss: 7754.12, average training loss: 9484.15, base loss: 14367.13
[INFO 2017-06-28 14:14:29,427 main.py:51] epoch 7645, training loss: 8775.89, average training loss: 9483.97, base loss: 14366.27
[INFO 2017-06-28 14:14:30,075 main.py:51] epoch 7646, training loss: 11045.31, average training loss: 9486.97, base loss: 14371.74
[INFO 2017-06-28 14:14:30,716 main.py:51] epoch 7647, training loss: 10182.69, average training loss: 9488.73, base loss: 14373.63
[INFO 2017-06-28 14:14:31,375 main.py:51] epoch 7648, training loss: 9853.32, average training loss: 9488.89, base loss: 14372.92
[INFO 2017-06-28 14:14:32,049 main.py:51] epoch 7649, training loss: 9452.00, average training loss: 9488.48, base loss: 14372.59
[INFO 2017-06-28 14:14:32,707 main.py:51] epoch 7650, training loss: 9036.04, average training loss: 9487.38, base loss: 14371.50
[INFO 2017-06-28 14:14:33,366 main.py:51] epoch 7651, training loss: 9572.31, average training loss: 9488.12, base loss: 14372.80
[INFO 2017-06-28 14:14:34,012 main.py:51] epoch 7652, training loss: 10393.04, average training loss: 9489.57, base loss: 14375.11
[INFO 2017-06-28 14:14:34,652 main.py:51] epoch 7653, training loss: 10469.00, average training loss: 9490.25, base loss: 14377.77
[INFO 2017-06-28 14:14:35,288 main.py:51] epoch 7654, training loss: 9039.58, average training loss: 9490.02, base loss: 14376.90
[INFO 2017-06-28 14:14:35,925 main.py:51] epoch 7655, training loss: 9438.45, average training loss: 9489.48, base loss: 14375.71
[INFO 2017-06-28 14:14:36,595 main.py:51] epoch 7656, training loss: 9545.23, average training loss: 9490.53, base loss: 14378.52
[INFO 2017-06-28 14:14:37,253 main.py:51] epoch 7657, training loss: 9303.26, average training loss: 9489.55, base loss: 14377.99
[INFO 2017-06-28 14:14:37,919 main.py:51] epoch 7658, training loss: 8084.06, average training loss: 9486.71, base loss: 14373.13
[INFO 2017-06-28 14:14:38,589 main.py:51] epoch 7659, training loss: 9622.90, average training loss: 9486.94, base loss: 14373.94
[INFO 2017-06-28 14:14:39,251 main.py:51] epoch 7660, training loss: 10845.38, average training loss: 9488.36, base loss: 14375.21
[INFO 2017-06-28 14:14:39,893 main.py:51] epoch 7661, training loss: 10339.46, average training loss: 9487.88, base loss: 14375.85
[INFO 2017-06-28 14:14:40,542 main.py:51] epoch 7662, training loss: 10485.80, average training loss: 9488.26, base loss: 14376.32
[INFO 2017-06-28 14:14:41,196 main.py:51] epoch 7663, training loss: 9667.92, average training loss: 9488.32, base loss: 14376.53
[INFO 2017-06-28 14:14:41,848 main.py:51] epoch 7664, training loss: 8975.36, average training loss: 9488.25, base loss: 14376.73
[INFO 2017-06-28 14:14:42,488 main.py:51] epoch 7665, training loss: 9140.25, average training loss: 9487.88, base loss: 14375.67
[INFO 2017-06-28 14:14:43,151 main.py:51] epoch 7666, training loss: 8717.34, average training loss: 9486.37, base loss: 14374.08
[INFO 2017-06-28 14:14:43,830 main.py:51] epoch 7667, training loss: 10181.01, average training loss: 9487.26, base loss: 14374.90
[INFO 2017-06-28 14:14:44,491 main.py:51] epoch 7668, training loss: 9108.80, average training loss: 9487.92, base loss: 14375.11
[INFO 2017-06-28 14:14:45,154 main.py:51] epoch 7669, training loss: 10558.40, average training loss: 9487.15, base loss: 14374.95
[INFO 2017-06-28 14:14:45,804 main.py:51] epoch 7670, training loss: 8618.70, average training loss: 9485.74, base loss: 14372.53
[INFO 2017-06-28 14:14:46,446 main.py:51] epoch 7671, training loss: 8873.43, average training loss: 9485.40, base loss: 14373.68
[INFO 2017-06-28 14:14:47,106 main.py:51] epoch 7672, training loss: 9530.46, average training loss: 9485.34, base loss: 14373.90
[INFO 2017-06-28 14:14:47,767 main.py:51] epoch 7673, training loss: 7965.27, average training loss: 9482.72, base loss: 14369.39
[INFO 2017-06-28 14:14:48,435 main.py:51] epoch 7674, training loss: 10496.93, average training loss: 9483.00, base loss: 14369.72
[INFO 2017-06-28 14:14:49,068 main.py:51] epoch 7675, training loss: 9383.55, average training loss: 9484.36, base loss: 14372.05
[INFO 2017-06-28 14:14:49,720 main.py:51] epoch 7676, training loss: 8920.16, average training loss: 9483.14, base loss: 14370.49
[INFO 2017-06-28 14:14:50,383 main.py:51] epoch 7677, training loss: 9821.41, average training loss: 9482.31, base loss: 14370.00
[INFO 2017-06-28 14:14:51,041 main.py:51] epoch 7678, training loss: 8690.89, average training loss: 9482.01, base loss: 14368.93
[INFO 2017-06-28 14:14:51,714 main.py:51] epoch 7679, training loss: 9948.29, average training loss: 9482.44, base loss: 14368.89
[INFO 2017-06-28 14:14:52,365 main.py:51] epoch 7680, training loss: 7947.41, average training loss: 9479.29, base loss: 14363.38
[INFO 2017-06-28 14:14:53,050 main.py:51] epoch 7681, training loss: 9477.62, average training loss: 9480.24, base loss: 14363.59
[INFO 2017-06-28 14:14:53,694 main.py:51] epoch 7682, training loss: 8716.38, average training loss: 9478.05, base loss: 14359.57
[INFO 2017-06-28 14:14:54,361 main.py:51] epoch 7683, training loss: 9458.00, average training loss: 9479.37, base loss: 14362.85
[INFO 2017-06-28 14:14:55,012 main.py:51] epoch 7684, training loss: 8720.05, average training loss: 9480.29, base loss: 14365.35
[INFO 2017-06-28 14:14:55,665 main.py:51] epoch 7685, training loss: 8197.48, average training loss: 9479.70, base loss: 14364.49
[INFO 2017-06-28 14:14:56,309 main.py:51] epoch 7686, training loss: 10553.57, average training loss: 9479.99, base loss: 14363.94
[INFO 2017-06-28 14:14:56,960 main.py:51] epoch 7687, training loss: 11087.90, average training loss: 9482.35, base loss: 14368.10
[INFO 2017-06-28 14:14:57,625 main.py:51] epoch 7688, training loss: 11078.07, average training loss: 9483.82, base loss: 14370.78
[INFO 2017-06-28 14:14:58,272 main.py:51] epoch 7689, training loss: 9794.60, average training loss: 9483.71, base loss: 14370.63
[INFO 2017-06-28 14:14:58,933 main.py:51] epoch 7690, training loss: 9329.24, average training loss: 9484.39, base loss: 14372.57
[INFO 2017-06-28 14:14:59,606 main.py:51] epoch 7691, training loss: 8352.67, average training loss: 9483.17, base loss: 14372.77
[INFO 2017-06-28 14:15:00,277 main.py:51] epoch 7692, training loss: 10523.15, average training loss: 9483.78, base loss: 14373.96
[INFO 2017-06-28 14:15:00,955 main.py:51] epoch 7693, training loss: 8887.52, average training loss: 9482.49, base loss: 14372.98
[INFO 2017-06-28 14:15:01,636 main.py:51] epoch 7694, training loss: 9409.07, average training loss: 9483.09, base loss: 14374.86
[INFO 2017-06-28 14:15:02,282 main.py:51] epoch 7695, training loss: 8407.02, average training loss: 9482.86, base loss: 14373.86
[INFO 2017-06-28 14:15:02,947 main.py:51] epoch 7696, training loss: 8896.90, average training loss: 9481.68, base loss: 14372.67
[INFO 2017-06-28 14:15:03,599 main.py:51] epoch 7697, training loss: 10183.79, average training loss: 9481.92, base loss: 14372.06
[INFO 2017-06-28 14:15:04,253 main.py:51] epoch 7698, training loss: 9524.43, average training loss: 9483.06, base loss: 14374.13
[INFO 2017-06-28 14:15:04,944 main.py:51] epoch 7699, training loss: 7183.92, average training loss: 9480.35, base loss: 14370.15
[INFO 2017-06-28 14:15:04,944 main.py:53] epoch 7699, testing
[INFO 2017-06-28 14:15:07,528 main.py:105] average testing loss: 10739.79, base loss: 15547.09
[INFO 2017-06-28 14:15:07,528 main.py:106] improve_loss: 4807.30, improve_percent: 0.31
[INFO 2017-06-28 14:15:07,529 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:15:07,566 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:15:08,224 main.py:51] epoch 7700, training loss: 9085.70, average training loss: 9480.01, base loss: 14371.56
[INFO 2017-06-28 14:15:08,875 main.py:51] epoch 7701, training loss: 8652.59, average training loss: 9478.73, base loss: 14369.59
[INFO 2017-06-28 14:15:09,539 main.py:51] epoch 7702, training loss: 10395.10, average training loss: 9478.92, base loss: 14370.85
[INFO 2017-06-28 14:15:10,181 main.py:51] epoch 7703, training loss: 9720.85, average training loss: 9478.55, base loss: 14370.72
[INFO 2017-06-28 14:15:10,830 main.py:51] epoch 7704, training loss: 8159.64, average training loss: 9474.41, base loss: 14366.38
[INFO 2017-06-28 14:15:11,467 main.py:51] epoch 7705, training loss: 8663.29, average training loss: 9474.31, base loss: 14365.82
[INFO 2017-06-28 14:15:12,139 main.py:51] epoch 7706, training loss: 9340.22, average training loss: 9473.58, base loss: 14365.49
[INFO 2017-06-28 14:15:12,790 main.py:51] epoch 7707, training loss: 10195.70, average training loss: 9473.63, base loss: 14366.29
[INFO 2017-06-28 14:15:13,441 main.py:51] epoch 7708, training loss: 9540.50, average training loss: 9473.76, base loss: 14367.01
[INFO 2017-06-28 14:15:14,102 main.py:51] epoch 7709, training loss: 9756.79, average training loss: 9475.85, base loss: 14370.76
[INFO 2017-06-28 14:15:14,763 main.py:51] epoch 7710, training loss: 7854.90, average training loss: 9474.25, base loss: 14366.76
[INFO 2017-06-28 14:15:15,419 main.py:51] epoch 7711, training loss: 9998.12, average training loss: 9474.95, base loss: 14367.28
[INFO 2017-06-28 14:15:16,066 main.py:51] epoch 7712, training loss: 9054.45, average training loss: 9473.46, base loss: 14366.76
[INFO 2017-06-28 14:15:16,731 main.py:51] epoch 7713, training loss: 9315.25, average training loss: 9473.09, base loss: 14364.97
[INFO 2017-06-28 14:15:17,399 main.py:51] epoch 7714, training loss: 9881.59, average training loss: 9473.77, base loss: 14364.97
[INFO 2017-06-28 14:15:18,056 main.py:51] epoch 7715, training loss: 9411.86, average training loss: 9473.83, base loss: 14363.76
[INFO 2017-06-28 14:15:18,754 main.py:51] epoch 7716, training loss: 8496.51, average training loss: 9473.69, base loss: 14364.17
[INFO 2017-06-28 14:15:19,512 main.py:51] epoch 7717, training loss: 8367.82, average training loss: 9472.24, base loss: 14360.93
[INFO 2017-06-28 14:15:20,278 main.py:51] epoch 7718, training loss: 9500.20, average training loss: 9472.93, base loss: 14361.82
[INFO 2017-06-28 14:15:20,984 main.py:51] epoch 7719, training loss: 8618.27, average training loss: 9471.95, base loss: 14360.68
[INFO 2017-06-28 14:15:21,854 main.py:51] epoch 7720, training loss: 10653.23, average training loss: 9472.38, base loss: 14361.42
[INFO 2017-06-28 14:15:22,870 main.py:51] epoch 7721, training loss: 9634.29, average training loss: 9472.45, base loss: 14361.75
[INFO 2017-06-28 14:15:23,924 main.py:51] epoch 7722, training loss: 9914.82, average training loss: 9473.51, base loss: 14363.39
[INFO 2017-06-28 14:15:24,972 main.py:51] epoch 7723, training loss: 8647.19, average training loss: 9472.24, base loss: 14361.62
[INFO 2017-06-28 14:15:26,061 main.py:51] epoch 7724, training loss: 8736.54, average training loss: 9471.42, base loss: 14361.22
[INFO 2017-06-28 14:15:27,144 main.py:51] epoch 7725, training loss: 11444.01, average training loss: 9472.66, base loss: 14364.84
[INFO 2017-06-28 14:15:28,253 main.py:51] epoch 7726, training loss: 9498.19, average training loss: 9472.39, base loss: 14365.63
[INFO 2017-06-28 14:15:29,331 main.py:51] epoch 7727, training loss: 9889.54, average training loss: 9472.38, base loss: 14366.35
[INFO 2017-06-28 14:15:30,396 main.py:51] epoch 7728, training loss: 8888.87, average training loss: 9472.94, base loss: 14365.65
[INFO 2017-06-28 14:15:31,512 main.py:51] epoch 7729, training loss: 9497.11, average training loss: 9473.87, base loss: 14367.45
[INFO 2017-06-28 14:15:32,629 main.py:51] epoch 7730, training loss: 10941.05, average training loss: 9474.64, base loss: 14368.20
[INFO 2017-06-28 14:15:33,745 main.py:51] epoch 7731, training loss: 10122.23, average training loss: 9475.24, base loss: 14367.82
[INFO 2017-06-28 14:15:34,838 main.py:51] epoch 7732, training loss: 8545.71, average training loss: 9473.32, base loss: 14366.68
[INFO 2017-06-28 14:15:35,915 main.py:51] epoch 7733, training loss: 9311.35, average training loss: 9474.21, base loss: 14368.20
[INFO 2017-06-28 14:15:37,041 main.py:51] epoch 7734, training loss: 9936.53, average training loss: 9475.33, base loss: 14371.33
[INFO 2017-06-28 14:15:38,137 main.py:51] epoch 7735, training loss: 8913.50, average training loss: 9475.06, base loss: 14371.24
[INFO 2017-06-28 14:15:39,257 main.py:51] epoch 7736, training loss: 8045.23, average training loss: 9472.86, base loss: 14367.92
[INFO 2017-06-28 14:15:40,347 main.py:51] epoch 7737, training loss: 8842.10, average training loss: 9473.39, base loss: 14369.06
[INFO 2017-06-28 14:15:41,445 main.py:51] epoch 7738, training loss: 10529.89, average training loss: 9475.42, base loss: 14372.99
[INFO 2017-06-28 14:15:42,510 main.py:51] epoch 7739, training loss: 8517.03, average training loss: 9474.72, base loss: 14371.63
[INFO 2017-06-28 14:15:43,628 main.py:51] epoch 7740, training loss: 9551.59, average training loss: 9475.57, base loss: 14374.24
[INFO 2017-06-28 14:15:44,708 main.py:51] epoch 7741, training loss: 9948.37, average training loss: 9476.48, base loss: 14376.60
[INFO 2017-06-28 14:15:45,796 main.py:51] epoch 7742, training loss: 9773.48, average training loss: 9476.28, base loss: 14377.52
[INFO 2017-06-28 14:15:46,831 main.py:51] epoch 7743, training loss: 7527.54, average training loss: 9473.62, base loss: 14373.46
[INFO 2017-06-28 14:15:47,914 main.py:51] epoch 7744, training loss: 9574.55, average training loss: 9473.86, base loss: 14373.41
[INFO 2017-06-28 14:15:48,968 main.py:51] epoch 7745, training loss: 10129.10, average training loss: 9475.43, base loss: 14375.38
[INFO 2017-06-28 14:15:50,059 main.py:51] epoch 7746, training loss: 10197.04, average training loss: 9476.48, base loss: 14378.33
[INFO 2017-06-28 14:15:51,146 main.py:51] epoch 7747, training loss: 9057.28, average training loss: 9475.58, base loss: 14374.82
[INFO 2017-06-28 14:15:52,263 main.py:51] epoch 7748, training loss: 9724.80, average training loss: 9476.73, base loss: 14376.69
[INFO 2017-06-28 14:15:53,347 main.py:51] epoch 7749, training loss: 10516.08, average training loss: 9478.99, base loss: 14380.84
[INFO 2017-06-28 14:15:54,456 main.py:51] epoch 7750, training loss: 9432.23, average training loss: 9477.68, base loss: 14379.37
[INFO 2017-06-28 14:15:55,546 main.py:51] epoch 7751, training loss: 8556.17, average training loss: 9476.26, base loss: 14377.61
[INFO 2017-06-28 14:15:56,656 main.py:51] epoch 7752, training loss: 9607.66, average training loss: 9475.15, base loss: 14374.71
[INFO 2017-06-28 14:15:57,753 main.py:51] epoch 7753, training loss: 10052.35, average training loss: 9474.49, base loss: 14373.21
[INFO 2017-06-28 14:15:58,852 main.py:51] epoch 7754, training loss: 10697.13, average training loss: 9476.34, base loss: 14376.29
[INFO 2017-06-28 14:15:59,885 main.py:51] epoch 7755, training loss: 9998.33, average training loss: 9477.73, base loss: 14377.81
[INFO 2017-06-28 14:16:00,973 main.py:51] epoch 7756, training loss: 10148.52, average training loss: 9478.79, base loss: 14379.56
[INFO 2017-06-28 14:16:02,052 main.py:51] epoch 7757, training loss: 8435.87, average training loss: 9478.90, base loss: 14380.18
[INFO 2017-06-28 14:16:03,140 main.py:51] epoch 7758, training loss: 9217.99, average training loss: 9479.79, base loss: 14382.64
[INFO 2017-06-28 14:16:04,246 main.py:51] epoch 7759, training loss: 9834.97, average training loss: 9479.92, base loss: 14382.99
[INFO 2017-06-28 14:16:05,372 main.py:51] epoch 7760, training loss: 10534.31, average training loss: 9481.19, base loss: 14385.13
[INFO 2017-06-28 14:16:06,463 main.py:51] epoch 7761, training loss: 9954.15, average training loss: 9482.03, base loss: 14385.15
[INFO 2017-06-28 14:16:07,579 main.py:51] epoch 7762, training loss: 8178.83, average training loss: 9481.53, base loss: 14385.96
[INFO 2017-06-28 14:16:08,673 main.py:51] epoch 7763, training loss: 10111.91, average training loss: 9483.15, base loss: 14388.67
[INFO 2017-06-28 14:16:09,760 main.py:51] epoch 7764, training loss: 9650.49, average training loss: 9483.50, base loss: 14387.97
[INFO 2017-06-28 14:16:10,865 main.py:51] epoch 7765, training loss: 10010.92, average training loss: 9483.52, base loss: 14387.24
[INFO 2017-06-28 14:16:11,983 main.py:51] epoch 7766, training loss: 10373.23, average training loss: 9483.65, base loss: 14387.79
[INFO 2017-06-28 14:16:13,080 main.py:51] epoch 7767, training loss: 9327.76, average training loss: 9484.09, base loss: 14387.84
[INFO 2017-06-28 14:16:14,184 main.py:51] epoch 7768, training loss: 9494.85, average training loss: 9483.76, base loss: 14386.48
[INFO 2017-06-28 14:16:15,277 main.py:51] epoch 7769, training loss: 8110.80, average training loss: 9481.41, base loss: 14383.99
[INFO 2017-06-28 14:16:16,378 main.py:51] epoch 7770, training loss: 10098.38, average training loss: 9482.45, base loss: 14385.62
[INFO 2017-06-28 14:16:17,474 main.py:51] epoch 7771, training loss: 10664.87, average training loss: 9483.25, base loss: 14385.70
[INFO 2017-06-28 14:16:18,516 main.py:51] epoch 7772, training loss: 10771.41, average training loss: 9483.99, base loss: 14386.37
[INFO 2017-06-28 14:16:19,613 main.py:51] epoch 7773, training loss: 8660.08, average training loss: 9483.78, base loss: 14385.14
[INFO 2017-06-28 14:16:20,644 main.py:51] epoch 7774, training loss: 9659.79, average training loss: 9483.93, base loss: 14385.04
[INFO 2017-06-28 14:16:21,740 main.py:51] epoch 7775, training loss: 9252.40, average training loss: 9482.91, base loss: 14382.82
[INFO 2017-06-28 14:16:22,817 main.py:51] epoch 7776, training loss: 8549.01, average training loss: 9479.85, base loss: 14379.94
[INFO 2017-06-28 14:16:23,899 main.py:51] epoch 7777, training loss: 10117.81, average training loss: 9478.48, base loss: 14377.89
[INFO 2017-06-28 14:16:25,013 main.py:51] epoch 7778, training loss: 8623.98, average training loss: 9477.98, base loss: 14375.96
[INFO 2017-06-28 14:16:26,162 main.py:51] epoch 7779, training loss: 8883.25, average training loss: 9475.61, base loss: 14371.64
[INFO 2017-06-28 14:16:27,289 main.py:51] epoch 7780, training loss: 10031.21, average training loss: 9476.41, base loss: 14373.20
[INFO 2017-06-28 14:16:28,404 main.py:51] epoch 7781, training loss: 11759.63, average training loss: 9479.05, base loss: 14376.90
[INFO 2017-06-28 14:16:29,511 main.py:51] epoch 7782, training loss: 10615.89, average training loss: 9479.18, base loss: 14377.14
[INFO 2017-06-28 14:16:30,594 main.py:51] epoch 7783, training loss: 8920.19, average training loss: 9478.69, base loss: 14374.99
[INFO 2017-06-28 14:16:31,644 main.py:51] epoch 7784, training loss: 11541.31, average training loss: 9481.34, base loss: 14378.86
[INFO 2017-06-28 14:16:32,712 main.py:51] epoch 7785, training loss: 8122.53, average training loss: 9479.27, base loss: 14376.07
[INFO 2017-06-28 14:16:33,777 main.py:51] epoch 7786, training loss: 10997.58, average training loss: 9481.63, base loss: 14379.88
[INFO 2017-06-28 14:16:34,844 main.py:51] epoch 7787, training loss: 8743.12, average training loss: 9481.31, base loss: 14379.29
[INFO 2017-06-28 14:16:35,831 main.py:51] epoch 7788, training loss: 10232.05, average training loss: 9483.28, base loss: 14381.94
[INFO 2017-06-28 14:16:36,819 main.py:51] epoch 7789, training loss: 8037.46, average training loss: 9480.40, base loss: 14376.83
[INFO 2017-06-28 14:16:37,822 main.py:51] epoch 7790, training loss: 9889.70, average training loss: 9480.83, base loss: 14378.53
[INFO 2017-06-28 14:16:38,936 main.py:51] epoch 7791, training loss: 9404.00, average training loss: 9480.42, base loss: 14378.28
[INFO 2017-06-28 14:16:40,040 main.py:51] epoch 7792, training loss: 10056.97, average training loss: 9481.14, base loss: 14377.57
[INFO 2017-06-28 14:16:41,189 main.py:51] epoch 7793, training loss: 10948.47, average training loss: 9481.88, base loss: 14378.12
[INFO 2017-06-28 14:16:42,278 main.py:51] epoch 7794, training loss: 10372.64, average training loss: 9482.52, base loss: 14380.21
[INFO 2017-06-28 14:16:43,413 main.py:51] epoch 7795, training loss: 9637.69, average training loss: 9482.71, base loss: 14380.68
[INFO 2017-06-28 14:16:44,511 main.py:51] epoch 7796, training loss: 9580.45, average training loss: 9482.68, base loss: 14381.04
[INFO 2017-06-28 14:16:45,644 main.py:51] epoch 7797, training loss: 10433.53, average training loss: 9483.63, base loss: 14383.49
[INFO 2017-06-28 14:16:46,760 main.py:51] epoch 7798, training loss: 8838.70, average training loss: 9482.13, base loss: 14381.94
[INFO 2017-06-28 14:16:47,863 main.py:51] epoch 7799, training loss: 9420.86, average training loss: 9480.69, base loss: 14380.05
[INFO 2017-06-28 14:16:47,863 main.py:53] epoch 7799, testing
[INFO 2017-06-28 14:16:51,693 main.py:105] average testing loss: 10231.88, base loss: 14416.31
[INFO 2017-06-28 14:16:51,693 main.py:106] improve_loss: 4184.43, improve_percent: 0.29
[INFO 2017-06-28 14:16:51,693 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:16:52,743 main.py:51] epoch 7800, training loss: 9109.28, average training loss: 9480.35, base loss: 14380.88
[INFO 2017-06-28 14:16:53,794 main.py:51] epoch 7801, training loss: 8108.60, average training loss: 9479.76, base loss: 14379.51
[INFO 2017-06-28 14:16:54,860 main.py:51] epoch 7802, training loss: 9593.71, average training loss: 9480.10, base loss: 14381.12
[INFO 2017-06-28 14:16:55,957 main.py:51] epoch 7803, training loss: 9609.50, average training loss: 9480.63, base loss: 14382.32
[INFO 2017-06-28 14:16:57,020 main.py:51] epoch 7804, training loss: 9240.35, average training loss: 9481.29, base loss: 14383.70
[INFO 2017-06-28 14:16:58,104 main.py:51] epoch 7805, training loss: 9798.38, average training loss: 9482.09, base loss: 14385.97
[INFO 2017-06-28 14:16:59,194 main.py:51] epoch 7806, training loss: 8205.30, average training loss: 9480.69, base loss: 14385.25
[INFO 2017-06-28 14:17:00,261 main.py:51] epoch 7807, training loss: 8853.68, average training loss: 9481.51, base loss: 14386.86
[INFO 2017-06-28 14:17:01,291 main.py:51] epoch 7808, training loss: 9857.15, average training loss: 9481.24, base loss: 14386.24
[INFO 2017-06-28 14:17:02,390 main.py:51] epoch 7809, training loss: 9228.86, average training loss: 9482.59, base loss: 14389.48
[INFO 2017-06-28 14:17:03,473 main.py:51] epoch 7810, training loss: 9247.23, average training loss: 9483.20, base loss: 14388.47
[INFO 2017-06-28 14:17:04,577 main.py:51] epoch 7811, training loss: 9292.01, average training loss: 9483.54, base loss: 14390.42
[INFO 2017-06-28 14:17:05,695 main.py:51] epoch 7812, training loss: 9699.59, average training loss: 9483.32, base loss: 14389.29
[INFO 2017-06-28 14:17:06,817 main.py:51] epoch 7813, training loss: 10445.80, average training loss: 9484.82, base loss: 14392.43
[INFO 2017-06-28 14:17:07,922 main.py:51] epoch 7814, training loss: 8149.92, average training loss: 9483.52, base loss: 14389.83
[INFO 2017-06-28 14:17:09,010 main.py:51] epoch 7815, training loss: 10233.99, average training loss: 9484.33, base loss: 14390.95
[INFO 2017-06-28 14:17:10,154 main.py:51] epoch 7816, training loss: 9897.04, average training loss: 9485.07, base loss: 14391.55
[INFO 2017-06-28 14:17:11,265 main.py:51] epoch 7817, training loss: 9873.48, average training loss: 9485.98, base loss: 14394.45
[INFO 2017-06-28 14:17:12,377 main.py:51] epoch 7818, training loss: 9876.91, average training loss: 9486.83, base loss: 14396.11
[INFO 2017-06-28 14:17:13,470 main.py:51] epoch 7819, training loss: 8669.95, average training loss: 9485.60, base loss: 14394.13
[INFO 2017-06-28 14:17:14,556 main.py:51] epoch 7820, training loss: 9200.80, average training loss: 9484.28, base loss: 14391.94
[INFO 2017-06-28 14:17:15,652 main.py:51] epoch 7821, training loss: 8915.64, average training loss: 9483.43, base loss: 14393.12
[INFO 2017-06-28 14:17:16,727 main.py:51] epoch 7822, training loss: 8626.26, average training loss: 9483.09, base loss: 14393.12
[INFO 2017-06-28 14:17:17,827 main.py:51] epoch 7823, training loss: 9104.21, average training loss: 9482.23, base loss: 14393.48
[INFO 2017-06-28 14:17:18,887 main.py:51] epoch 7824, training loss: 8549.41, average training loss: 9479.75, base loss: 14390.64
[INFO 2017-06-28 14:17:19,986 main.py:51] epoch 7825, training loss: 9555.28, average training loss: 9480.70, base loss: 14391.67
[INFO 2017-06-28 14:17:21,021 main.py:51] epoch 7826, training loss: 8180.39, average training loss: 9478.62, base loss: 14385.89
[INFO 2017-06-28 14:17:22,074 main.py:51] epoch 7827, training loss: 10185.68, average training loss: 9478.12, base loss: 14386.50
[INFO 2017-06-28 14:17:23,131 main.py:51] epoch 7828, training loss: 10310.10, average training loss: 9480.56, base loss: 14389.31
[INFO 2017-06-28 14:17:24,188 main.py:51] epoch 7829, training loss: 9934.34, average training loss: 9481.01, base loss: 14390.40
[INFO 2017-06-28 14:17:25,284 main.py:51] epoch 7830, training loss: 11142.10, average training loss: 9482.78, base loss: 14392.25
[INFO 2017-06-28 14:17:26,392 main.py:51] epoch 7831, training loss: 8737.57, average training loss: 9482.32, base loss: 14391.72
[INFO 2017-06-28 14:17:27,495 main.py:51] epoch 7832, training loss: 9647.10, average training loss: 9481.11, base loss: 14389.17
[INFO 2017-06-28 14:17:28,586 main.py:51] epoch 7833, training loss: 9426.85, average training loss: 9481.40, base loss: 14389.08
[INFO 2017-06-28 14:17:29,655 main.py:51] epoch 7834, training loss: 8818.44, average training loss: 9481.02, base loss: 14388.75
[INFO 2017-06-28 14:17:30,751 main.py:51] epoch 7835, training loss: 8085.65, average training loss: 9479.75, base loss: 14386.66
[INFO 2017-06-28 14:17:31,837 main.py:51] epoch 7836, training loss: 9463.61, average training loss: 9479.71, base loss: 14386.92
[INFO 2017-06-28 14:17:32,926 main.py:51] epoch 7837, training loss: 9502.32, average training loss: 9480.09, base loss: 14388.96
[INFO 2017-06-28 14:17:34,029 main.py:51] epoch 7838, training loss: 9107.11, average training loss: 9480.41, base loss: 14389.97
[INFO 2017-06-28 14:17:35,148 main.py:51] epoch 7839, training loss: 9066.81, average training loss: 9478.40, base loss: 14386.70
[INFO 2017-06-28 14:17:36,256 main.py:51] epoch 7840, training loss: 8582.49, average training loss: 9478.05, base loss: 14385.92
[INFO 2017-06-28 14:17:37,376 main.py:51] epoch 7841, training loss: 11422.25, average training loss: 9479.92, base loss: 14388.03
[INFO 2017-06-28 14:17:38,470 main.py:51] epoch 7842, training loss: 8851.57, average training loss: 9478.76, base loss: 14385.13
[INFO 2017-06-28 14:17:39,564 main.py:51] epoch 7843, training loss: 8551.14, average training loss: 9476.36, base loss: 14380.61
[INFO 2017-06-28 14:17:40,644 main.py:51] epoch 7844, training loss: 10984.23, average training loss: 9478.19, base loss: 14383.23
[INFO 2017-06-28 14:17:41,713 main.py:51] epoch 7845, training loss: 9973.36, average training loss: 9478.69, base loss: 14384.39
[INFO 2017-06-28 14:17:42,750 main.py:51] epoch 7846, training loss: 10225.27, average training loss: 9479.27, base loss: 14386.35
[INFO 2017-06-28 14:17:43,833 main.py:51] epoch 7847, training loss: 9026.44, average training loss: 9478.84, base loss: 14386.35
[INFO 2017-06-28 14:17:44,925 main.py:51] epoch 7848, training loss: 8919.26, average training loss: 9478.59, base loss: 14385.66
[INFO 2017-06-28 14:17:46,014 main.py:51] epoch 7849, training loss: 8713.96, average training loss: 9479.06, base loss: 14386.72
[INFO 2017-06-28 14:17:47,126 main.py:51] epoch 7850, training loss: 9261.02, average training loss: 9478.65, base loss: 14386.40
[INFO 2017-06-28 14:17:48,208 main.py:51] epoch 7851, training loss: 9010.17, average training loss: 9478.15, base loss: 14387.79
[INFO 2017-06-28 14:17:49,253 main.py:51] epoch 7852, training loss: 9780.82, average training loss: 9478.15, base loss: 14387.03
[INFO 2017-06-28 14:17:50,226 main.py:51] epoch 7853, training loss: 9026.16, average training loss: 9477.26, base loss: 14385.17
[INFO 2017-06-28 14:17:51,182 main.py:51] epoch 7854, training loss: 9499.33, average training loss: 9478.52, base loss: 14386.34
[INFO 2017-06-28 14:17:52,223 main.py:51] epoch 7855, training loss: 9068.35, average training loss: 9479.10, base loss: 14387.02
[INFO 2017-06-28 14:17:53,284 main.py:51] epoch 7856, training loss: 8759.56, average training loss: 9477.38, base loss: 14385.56
[INFO 2017-06-28 14:17:54,329 main.py:51] epoch 7857, training loss: 8336.17, average training loss: 9475.58, base loss: 14384.15
[INFO 2017-06-28 14:17:55,406 main.py:51] epoch 7858, training loss: 8332.46, average training loss: 9473.53, base loss: 14381.65
[INFO 2017-06-28 14:17:56,509 main.py:51] epoch 7859, training loss: 9561.54, average training loss: 9473.28, base loss: 14380.30
[INFO 2017-06-28 14:17:57,536 main.py:51] epoch 7860, training loss: 9381.33, average training loss: 9472.84, base loss: 14380.03
[INFO 2017-06-28 14:17:58,635 main.py:51] epoch 7861, training loss: 9723.76, average training loss: 9473.86, base loss: 14381.87
[INFO 2017-06-28 14:17:59,738 main.py:51] epoch 7862, training loss: 9366.07, average training loss: 9472.66, base loss: 14379.19
[INFO 2017-06-28 14:18:00,863 main.py:51] epoch 7863, training loss: 10764.22, average training loss: 9473.81, base loss: 14381.14
[INFO 2017-06-28 14:18:01,931 main.py:51] epoch 7864, training loss: 9568.66, average training loss: 9474.58, base loss: 14382.17
[INFO 2017-06-28 14:18:03,060 main.py:51] epoch 7865, training loss: 8396.41, average training loss: 9473.91, base loss: 14381.54
[INFO 2017-06-28 14:18:04,218 main.py:51] epoch 7866, training loss: 8980.31, average training loss: 9473.42, base loss: 14380.27
[INFO 2017-06-28 14:18:05,316 main.py:51] epoch 7867, training loss: 10500.88, average training loss: 9474.67, base loss: 14383.00
[INFO 2017-06-28 14:18:06,423 main.py:51] epoch 7868, training loss: 9077.80, average training loss: 9474.72, base loss: 14382.05
[INFO 2017-06-28 14:18:07,522 main.py:51] epoch 7869, training loss: 8627.57, average training loss: 9474.50, base loss: 14382.42
[INFO 2017-06-28 14:18:08,614 main.py:51] epoch 7870, training loss: 8805.12, average training loss: 9474.59, base loss: 14382.39
[INFO 2017-06-28 14:18:09,649 main.py:51] epoch 7871, training loss: 8293.18, average training loss: 9473.29, base loss: 14378.88
[INFO 2017-06-28 14:18:10,725 main.py:51] epoch 7872, training loss: 8680.05, average training loss: 9472.49, base loss: 14378.35
[INFO 2017-06-28 14:18:11,834 main.py:51] epoch 7873, training loss: 8885.13, average training loss: 9471.69, base loss: 14376.59
[INFO 2017-06-28 14:18:12,926 main.py:51] epoch 7874, training loss: 8953.57, average training loss: 9472.38, base loss: 14377.48
[INFO 2017-06-28 14:18:14,034 main.py:51] epoch 7875, training loss: 9928.16, average training loss: 9472.30, base loss: 14378.03
[INFO 2017-06-28 14:18:15,135 main.py:51] epoch 7876, training loss: 9014.72, average training loss: 9472.65, base loss: 14378.36
[INFO 2017-06-28 14:18:16,236 main.py:51] epoch 7877, training loss: 9666.84, average training loss: 9474.11, base loss: 14379.91
[INFO 2017-06-28 14:18:17,291 main.py:51] epoch 7878, training loss: 7797.47, average training loss: 9472.68, base loss: 14377.70
[INFO 2017-06-28 14:18:18,411 main.py:51] epoch 7879, training loss: 9152.93, average training loss: 9472.70, base loss: 14378.48
[INFO 2017-06-28 14:18:19,531 main.py:51] epoch 7880, training loss: 9447.51, average training loss: 9472.84, base loss: 14378.69
[INFO 2017-06-28 14:18:20,592 main.py:51] epoch 7881, training loss: 8955.00, average training loss: 9472.93, base loss: 14378.77
[INFO 2017-06-28 14:18:21,681 main.py:51] epoch 7882, training loss: 8538.21, average training loss: 9470.87, base loss: 14375.36
[INFO 2017-06-28 14:18:22,737 main.py:51] epoch 7883, training loss: 9732.89, average training loss: 9471.20, base loss: 14375.79
[INFO 2017-06-28 14:18:23,813 main.py:51] epoch 7884, training loss: 10194.69, average training loss: 9471.88, base loss: 14376.63
[INFO 2017-06-28 14:18:24,942 main.py:51] epoch 7885, training loss: 9938.16, average training loss: 9472.13, base loss: 14377.41
[INFO 2017-06-28 14:18:26,051 main.py:51] epoch 7886, training loss: 9816.83, average training loss: 9474.11, base loss: 14381.11
[INFO 2017-06-28 14:18:27,178 main.py:51] epoch 7887, training loss: 8803.67, average training loss: 9474.23, base loss: 14382.77
[INFO 2017-06-28 14:18:28,275 main.py:51] epoch 7888, training loss: 7990.96, average training loss: 9471.90, base loss: 14378.34
[INFO 2017-06-28 14:18:29,364 main.py:51] epoch 7889, training loss: 11155.26, average training loss: 9472.82, base loss: 14380.88
[INFO 2017-06-28 14:18:30,431 main.py:51] epoch 7890, training loss: 9067.73, average training loss: 9471.93, base loss: 14378.67
[INFO 2017-06-28 14:18:31,487 main.py:51] epoch 7891, training loss: 9709.41, average training loss: 9472.72, base loss: 14377.65
[INFO 2017-06-28 14:18:32,590 main.py:51] epoch 7892, training loss: 9284.28, average training loss: 9472.34, base loss: 14378.21
[INFO 2017-06-28 14:18:33,691 main.py:51] epoch 7893, training loss: 10775.92, average training loss: 9473.96, base loss: 14381.59
[INFO 2017-06-28 14:18:34,770 main.py:51] epoch 7894, training loss: 9823.06, average training loss: 9473.29, base loss: 14380.08
[INFO 2017-06-28 14:18:35,878 main.py:51] epoch 7895, training loss: 9473.02, average training loss: 9473.38, base loss: 14380.17
[INFO 2017-06-28 14:18:36,956 main.py:51] epoch 7896, training loss: 10457.89, average training loss: 9473.90, base loss: 14382.13
[INFO 2017-06-28 14:18:38,022 main.py:51] epoch 7897, training loss: 9212.86, average training loss: 9474.11, base loss: 14383.95
[INFO 2017-06-28 14:18:39,128 main.py:51] epoch 7898, training loss: 8980.29, average training loss: 9472.93, base loss: 14381.12
[INFO 2017-06-28 14:18:40,224 main.py:51] epoch 7899, training loss: 11211.98, average training loss: 9474.34, base loss: 14381.56
[INFO 2017-06-28 14:18:40,224 main.py:53] epoch 7899, testing
[INFO 2017-06-28 14:18:44,014 main.py:105] average testing loss: 10364.05, base loss: 14809.80
[INFO 2017-06-28 14:18:44,015 main.py:106] improve_loss: 4445.75, improve_percent: 0.30
[INFO 2017-06-28 14:18:44,015 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:18:45,117 main.py:51] epoch 7900, training loss: 8967.52, average training loss: 9474.66, base loss: 14380.94
[INFO 2017-06-28 14:18:46,184 main.py:51] epoch 7901, training loss: 8649.12, average training loss: 9473.80, base loss: 14378.83
[INFO 2017-06-28 14:18:47,269 main.py:51] epoch 7902, training loss: 9007.37, average training loss: 9473.03, base loss: 14375.84
[INFO 2017-06-28 14:18:48,345 main.py:51] epoch 7903, training loss: 8532.85, average training loss: 9470.44, base loss: 14372.03
[INFO 2017-06-28 14:18:49,430 main.py:51] epoch 7904, training loss: 8259.96, average training loss: 9469.04, base loss: 14371.34
[INFO 2017-06-28 14:18:50,519 main.py:51] epoch 7905, training loss: 9725.66, average training loss: 9470.02, base loss: 14373.32
[INFO 2017-06-28 14:18:51,618 main.py:51] epoch 7906, training loss: 8990.40, average training loss: 9469.95, base loss: 14375.33
[INFO 2017-06-28 14:18:52,658 main.py:51] epoch 7907, training loss: 8823.06, average training loss: 9468.44, base loss: 14372.49
[INFO 2017-06-28 14:18:53,705 main.py:51] epoch 7908, training loss: 9539.33, average training loss: 9468.86, base loss: 14372.25
[INFO 2017-06-28 14:18:54,778 main.py:51] epoch 7909, training loss: 8544.59, average training loss: 9468.04, base loss: 14370.49
[INFO 2017-06-28 14:18:55,841 main.py:51] epoch 7910, training loss: 9848.18, average training loss: 9468.81, base loss: 14371.50
[INFO 2017-06-28 14:18:56,888 main.py:51] epoch 7911, training loss: 9947.68, average training loss: 9468.58, base loss: 14373.75
[INFO 2017-06-28 14:18:57,976 main.py:51] epoch 7912, training loss: 9793.18, average training loss: 9468.72, base loss: 14373.48
[INFO 2017-06-28 14:18:59,069 main.py:51] epoch 7913, training loss: 9645.87, average training loss: 9467.80, base loss: 14371.82
[INFO 2017-06-28 14:19:00,089 main.py:51] epoch 7914, training loss: 10215.52, average training loss: 9468.71, base loss: 14372.40
[INFO 2017-06-28 14:19:01,139 main.py:51] epoch 7915, training loss: 9714.62, average training loss: 9468.97, base loss: 14373.04
[INFO 2017-06-28 14:19:02,221 main.py:51] epoch 7916, training loss: 11196.95, average training loss: 9470.02, base loss: 14374.95
[INFO 2017-06-28 14:19:03,267 main.py:51] epoch 7917, training loss: 9962.51, average training loss: 9469.31, base loss: 14373.85
[INFO 2017-06-28 14:19:04,239 main.py:51] epoch 7918, training loss: 8569.90, average training loss: 9466.74, base loss: 14371.03
[INFO 2017-06-28 14:19:05,252 main.py:51] epoch 7919, training loss: 8927.60, average training loss: 9467.56, base loss: 14373.17
[INFO 2017-06-28 14:19:06,324 main.py:51] epoch 7920, training loss: 9503.68, average training loss: 9466.62, base loss: 14372.34
[INFO 2017-06-28 14:19:07,246 main.py:51] epoch 7921, training loss: 9571.58, average training loss: 9466.99, base loss: 14373.99
[INFO 2017-06-28 14:19:07,906 main.py:51] epoch 7922, training loss: 10042.53, average training loss: 9467.87, base loss: 14376.09
[INFO 2017-06-28 14:19:08,563 main.py:51] epoch 7923, training loss: 8959.80, average training loss: 9466.31, base loss: 14374.18
[INFO 2017-06-28 14:19:09,215 main.py:51] epoch 7924, training loss: 9690.63, average training loss: 9466.15, base loss: 14374.20
[INFO 2017-06-28 14:19:09,872 main.py:51] epoch 7925, training loss: 8735.16, average training loss: 9466.19, base loss: 14375.61
[INFO 2017-06-28 14:19:10,561 main.py:51] epoch 7926, training loss: 9811.40, average training loss: 9467.15, base loss: 14377.57
[INFO 2017-06-28 14:19:11,235 main.py:51] epoch 7927, training loss: 8925.49, average training loss: 9467.26, base loss: 14376.84
[INFO 2017-06-28 14:19:11,920 main.py:51] epoch 7928, training loss: 8393.62, average training loss: 9465.54, base loss: 14374.84
[INFO 2017-06-28 14:19:12,600 main.py:51] epoch 7929, training loss: 9296.86, average training loss: 9465.04, base loss: 14373.66
[INFO 2017-06-28 14:19:13,278 main.py:51] epoch 7930, training loss: 9885.51, average training loss: 9465.87, base loss: 14375.52
[INFO 2017-06-28 14:19:13,934 main.py:51] epoch 7931, training loss: 10126.66, average training loss: 9467.44, base loss: 14377.68
[INFO 2017-06-28 14:19:14,609 main.py:51] epoch 7932, training loss: 8498.83, average training loss: 9466.75, base loss: 14377.08
[INFO 2017-06-28 14:19:15,279 main.py:51] epoch 7933, training loss: 8738.77, average training loss: 9465.84, base loss: 14376.07
[INFO 2017-06-28 14:19:15,937 main.py:51] epoch 7934, training loss: 7940.48, average training loss: 9463.92, base loss: 14372.63
[INFO 2017-06-28 14:19:16,620 main.py:51] epoch 7935, training loss: 8999.83, average training loss: 9462.73, base loss: 14370.40
[INFO 2017-06-28 14:19:17,294 main.py:51] epoch 7936, training loss: 9835.32, average training loss: 9463.48, base loss: 14371.15
[INFO 2017-06-28 14:19:17,961 main.py:51] epoch 7937, training loss: 10095.75, average training loss: 9464.45, base loss: 14372.42
[INFO 2017-06-28 14:19:18,627 main.py:51] epoch 7938, training loss: 9305.81, average training loss: 9463.10, base loss: 14369.89
[INFO 2017-06-28 14:19:19,285 main.py:51] epoch 7939, training loss: 9907.96, average training loss: 9464.87, base loss: 14371.79
[INFO 2017-06-28 14:19:19,941 main.py:51] epoch 7940, training loss: 9916.98, average training loss: 9464.90, base loss: 14372.61
[INFO 2017-06-28 14:19:20,608 main.py:51] epoch 7941, training loss: 9190.00, average training loss: 9464.29, base loss: 14371.32
[INFO 2017-06-28 14:19:21,244 main.py:51] epoch 7942, training loss: 12447.34, average training loss: 9465.69, base loss: 14372.38
[INFO 2017-06-28 14:19:21,905 main.py:51] epoch 7943, training loss: 10039.99, average training loss: 9468.16, base loss: 14375.59
[INFO 2017-06-28 14:19:22,577 main.py:51] epoch 7944, training loss: 10133.33, average training loss: 9469.08, base loss: 14377.48
[INFO 2017-06-28 14:19:23,365 main.py:51] epoch 7945, training loss: 9533.06, average training loss: 9468.97, base loss: 14377.90
[INFO 2017-06-28 14:19:24,140 main.py:51] epoch 7946, training loss: 8393.88, average training loss: 9468.86, base loss: 14377.44
[INFO 2017-06-28 14:19:24,919 main.py:51] epoch 7947, training loss: 8781.03, average training loss: 9468.65, base loss: 14375.34
[INFO 2017-06-28 14:19:25,720 main.py:51] epoch 7948, training loss: 9109.91, average training loss: 9469.07, base loss: 14375.24
[INFO 2017-06-28 14:19:26,761 main.py:51] epoch 7949, training loss: 9592.22, average training loss: 9470.38, base loss: 14377.23
[INFO 2017-06-28 14:19:27,815 main.py:51] epoch 7950, training loss: 10010.88, average training loss: 9472.22, base loss: 14379.00
[INFO 2017-06-28 14:19:28,857 main.py:51] epoch 7951, training loss: 9609.62, average training loss: 9472.70, base loss: 14380.97
[INFO 2017-06-28 14:19:29,893 main.py:51] epoch 7952, training loss: 9718.10, average training loss: 9473.82, base loss: 14382.99
[INFO 2017-06-28 14:19:30,907 main.py:51] epoch 7953, training loss: 9207.45, average training loss: 9474.18, base loss: 14385.06
[INFO 2017-06-28 14:19:31,942 main.py:51] epoch 7954, training loss: 11423.73, average training loss: 9476.06, base loss: 14389.52
[INFO 2017-06-28 14:19:32,999 main.py:51] epoch 7955, training loss: 9623.31, average training loss: 9476.90, base loss: 14390.30
[INFO 2017-06-28 14:19:34,017 main.py:51] epoch 7956, training loss: 9212.75, average training loss: 9476.14, base loss: 14389.36
[INFO 2017-06-28 14:19:35,085 main.py:51] epoch 7957, training loss: 8878.56, average training loss: 9476.07, base loss: 14390.02
[INFO 2017-06-28 14:19:36,170 main.py:51] epoch 7958, training loss: 9243.73, average training loss: 9477.88, base loss: 14393.42
[INFO 2017-06-28 14:19:37,145 main.py:51] epoch 7959, training loss: 9346.66, average training loss: 9477.64, base loss: 14394.03
[INFO 2017-06-28 14:19:38,185 main.py:51] epoch 7960, training loss: 10795.60, average training loss: 9479.21, base loss: 14394.44
[INFO 2017-06-28 14:19:39,227 main.py:51] epoch 7961, training loss: 10945.37, average training loss: 9480.31, base loss: 14396.05
[INFO 2017-06-28 14:19:40,340 main.py:51] epoch 7962, training loss: 10219.40, average training loss: 9480.76, base loss: 14396.72
[INFO 2017-06-28 14:19:41,422 main.py:51] epoch 7963, training loss: 10069.44, average training loss: 9481.02, base loss: 14396.15
[INFO 2017-06-28 14:19:42,455 main.py:51] epoch 7964, training loss: 9015.62, average training loss: 9480.33, base loss: 14394.77
[INFO 2017-06-28 14:19:43,528 main.py:51] epoch 7965, training loss: 9302.06, average training loss: 9480.94, base loss: 14396.05
[INFO 2017-06-28 14:19:44,586 main.py:51] epoch 7966, training loss: 9886.23, average training loss: 9481.02, base loss: 14396.05
[INFO 2017-06-28 14:19:45,655 main.py:51] epoch 7967, training loss: 9420.67, average training loss: 9479.22, base loss: 14394.33
[INFO 2017-06-28 14:19:46,736 main.py:51] epoch 7968, training loss: 9910.72, average training loss: 9479.80, base loss: 14395.18
[INFO 2017-06-28 14:19:47,818 main.py:51] epoch 7969, training loss: 8886.80, average training loss: 9479.78, base loss: 14393.07
[INFO 2017-06-28 14:19:48,877 main.py:51] epoch 7970, training loss: 9734.78, average training loss: 9480.64, base loss: 14394.56
[INFO 2017-06-28 14:19:49,895 main.py:51] epoch 7971, training loss: 9175.00, average training loss: 9481.34, base loss: 14396.53
[INFO 2017-06-28 14:19:50,988 main.py:51] epoch 7972, training loss: 10233.77, average training loss: 9482.70, base loss: 14399.23
[INFO 2017-06-28 14:19:52,048 main.py:51] epoch 7973, training loss: 9428.24, average training loss: 9481.96, base loss: 14398.58
[INFO 2017-06-28 14:19:53,084 main.py:51] epoch 7974, training loss: 10638.26, average training loss: 9484.56, base loss: 14402.46
[INFO 2017-06-28 14:19:54,148 main.py:51] epoch 7975, training loss: 8783.60, average training loss: 9484.90, base loss: 14402.80
[INFO 2017-06-28 14:19:55,184 main.py:51] epoch 7976, training loss: 10883.85, average training loss: 9486.41, base loss: 14404.58
[INFO 2017-06-28 14:19:56,239 main.py:51] epoch 7977, training loss: 8450.06, average training loss: 9485.35, base loss: 14402.93
[INFO 2017-06-28 14:19:57,310 main.py:51] epoch 7978, training loss: 9494.59, average training loss: 9485.16, base loss: 14403.43
[INFO 2017-06-28 14:19:58,353 main.py:51] epoch 7979, training loss: 9400.04, average training loss: 9483.95, base loss: 14403.71
[INFO 2017-06-28 14:19:59,372 main.py:51] epoch 7980, training loss: 10520.47, average training loss: 9485.28, base loss: 14406.94
[INFO 2017-06-28 14:20:00,447 main.py:51] epoch 7981, training loss: 9454.34, average training loss: 9484.75, base loss: 14406.39
[INFO 2017-06-28 14:20:01,479 main.py:51] epoch 7982, training loss: 9848.07, average training loss: 9485.27, base loss: 14407.50
[INFO 2017-06-28 14:20:02,515 main.py:51] epoch 7983, training loss: 10539.62, average training loss: 9485.63, base loss: 14408.71
[INFO 2017-06-28 14:20:03,524 main.py:51] epoch 7984, training loss: 8135.91, average training loss: 9484.18, base loss: 14406.38
[INFO 2017-06-28 14:20:04,567 main.py:51] epoch 7985, training loss: 8006.46, average training loss: 9481.26, base loss: 14401.20
[INFO 2017-06-28 14:20:05,651 main.py:51] epoch 7986, training loss: 9824.24, average training loss: 9481.75, base loss: 14400.68
[INFO 2017-06-28 14:20:06,656 main.py:51] epoch 7987, training loss: 9339.06, average training loss: 9481.30, base loss: 14399.36
[INFO 2017-06-28 14:20:07,717 main.py:51] epoch 7988, training loss: 8872.31, average training loss: 9481.19, base loss: 14398.42
[INFO 2017-06-28 14:20:08,805 main.py:51] epoch 7989, training loss: 10740.78, average training loss: 9483.77, base loss: 14403.97
[INFO 2017-06-28 14:20:09,818 main.py:51] epoch 7990, training loss: 10214.89, average training loss: 9485.85, base loss: 14407.35
[INFO 2017-06-28 14:20:10,846 main.py:51] epoch 7991, training loss: 9499.89, average training loss: 9486.06, base loss: 14408.03
[INFO 2017-06-28 14:20:11,915 main.py:51] epoch 7992, training loss: 10253.85, average training loss: 9487.34, base loss: 14409.25
[INFO 2017-06-28 14:20:12,986 main.py:51] epoch 7993, training loss: 9044.19, average training loss: 9487.83, base loss: 14409.97
[INFO 2017-06-28 14:20:14,069 main.py:51] epoch 7994, training loss: 11422.37, average training loss: 9488.56, base loss: 14409.94
[INFO 2017-06-28 14:20:15,097 main.py:51] epoch 7995, training loss: 8691.43, average training loss: 9487.56, base loss: 14406.84
[INFO 2017-06-28 14:20:16,188 main.py:51] epoch 7996, training loss: 10676.11, average training loss: 9488.78, base loss: 14408.85
[INFO 2017-06-28 14:20:17,204 main.py:51] epoch 7997, training loss: 9008.53, average training loss: 9489.27, base loss: 14408.44
[INFO 2017-06-28 14:20:18,286 main.py:51] epoch 7998, training loss: 8984.68, average training loss: 9486.77, base loss: 14403.16
[INFO 2017-06-28 14:20:19,395 main.py:51] epoch 7999, training loss: 8596.72, average training loss: 9486.32, base loss: 14401.40
[INFO 2017-06-28 14:20:19,395 main.py:53] epoch 7999, testing
[INFO 2017-06-28 14:20:23,058 main.py:105] average testing loss: 10800.01, base loss: 15548.57
[INFO 2017-06-28 14:20:23,058 main.py:106] improve_loss: 4748.56, improve_percent: 0.31
[INFO 2017-06-28 14:20:23,059 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:20:24,139 main.py:51] epoch 8000, training loss: 9258.36, average training loss: 9487.59, base loss: 14405.01
[INFO 2017-06-28 14:20:25,158 main.py:51] epoch 8001, training loss: 8269.22, average training loss: 9485.59, base loss: 14402.72
[INFO 2017-06-28 14:20:26,240 main.py:51] epoch 8002, training loss: 10143.70, average training loss: 9486.62, base loss: 14403.24
[INFO 2017-06-28 14:20:27,275 main.py:51] epoch 8003, training loss: 11966.14, average training loss: 9489.87, base loss: 14408.63
[INFO 2017-06-28 14:20:28,362 main.py:51] epoch 8004, training loss: 8633.20, average training loss: 9488.00, base loss: 14405.94
[INFO 2017-06-28 14:20:29,470 main.py:51] epoch 8005, training loss: 9935.38, average training loss: 9488.94, base loss: 14407.13
[INFO 2017-06-28 14:20:30,496 main.py:51] epoch 8006, training loss: 9636.81, average training loss: 9487.76, base loss: 14406.26
[INFO 2017-06-28 14:20:31,576 main.py:51] epoch 8007, training loss: 9887.94, average training loss: 9487.59, base loss: 14405.15
[INFO 2017-06-28 14:20:32,601 main.py:51] epoch 8008, training loss: 9643.21, average training loss: 9487.47, base loss: 14403.46
[INFO 2017-06-28 14:20:33,686 main.py:51] epoch 8009, training loss: 9421.40, average training loss: 9487.09, base loss: 14402.82
[INFO 2017-06-28 14:20:34,786 main.py:51] epoch 8010, training loss: 10621.71, average training loss: 9489.03, base loss: 14407.14
[INFO 2017-06-28 14:20:35,779 main.py:51] epoch 8011, training loss: 9510.03, average training loss: 9488.89, base loss: 14406.45
[INFO 2017-06-28 14:20:36,831 main.py:51] epoch 8012, training loss: 9853.35, average training loss: 9489.89, base loss: 14410.16
[INFO 2017-06-28 14:20:37,892 main.py:51] epoch 8013, training loss: 9603.04, average training loss: 9489.93, base loss: 14410.02
[INFO 2017-06-28 14:20:38,905 main.py:51] epoch 8014, training loss: 9209.35, average training loss: 9490.61, base loss: 14411.05
[INFO 2017-06-28 14:20:39,929 main.py:51] epoch 8015, training loss: 10050.32, average training loss: 9490.53, base loss: 14412.07
[INFO 2017-06-28 14:20:41,014 main.py:51] epoch 8016, training loss: 9172.76, average training loss: 9490.66, base loss: 14412.34
[INFO 2017-06-28 14:20:42,018 main.py:51] epoch 8017, training loss: 9522.01, average training loss: 9491.14, base loss: 14415.29
[INFO 2017-06-28 14:20:42,985 main.py:51] epoch 8018, training loss: 9641.64, average training loss: 9491.82, base loss: 14414.85
[INFO 2017-06-28 14:20:43,925 main.py:51] epoch 8019, training loss: 8460.20, average training loss: 9490.59, base loss: 14413.04
[INFO 2017-06-28 14:20:44,874 main.py:51] epoch 8020, training loss: 9016.19, average training loss: 9489.96, base loss: 14412.60
[INFO 2017-06-28 14:20:45,942 main.py:51] epoch 8021, training loss: 9855.87, average training loss: 9489.27, base loss: 14412.07
[INFO 2017-06-28 14:20:47,022 main.py:51] epoch 8022, training loss: 9928.97, average training loss: 9490.65, base loss: 14413.67
[INFO 2017-06-28 14:20:48,072 main.py:51] epoch 8023, training loss: 9001.69, average training loss: 9490.20, base loss: 14412.72
[INFO 2017-06-28 14:20:49,109 main.py:51] epoch 8024, training loss: 9160.45, average training loss: 9490.46, base loss: 14412.40
[INFO 2017-06-28 14:20:50,184 main.py:51] epoch 8025, training loss: 9908.43, average training loss: 9490.42, base loss: 14411.31
[INFO 2017-06-28 14:20:51,190 main.py:51] epoch 8026, training loss: 9494.01, average training loss: 9490.80, base loss: 14411.10
[INFO 2017-06-28 14:20:52,234 main.py:51] epoch 8027, training loss: 10892.62, average training loss: 9491.63, base loss: 14411.38
[INFO 2017-06-28 14:20:53,262 main.py:51] epoch 8028, training loss: 11578.22, average training loss: 9494.95, base loss: 14417.19
[INFO 2017-06-28 14:20:54,338 main.py:51] epoch 8029, training loss: 9777.37, average training loss: 9494.67, base loss: 14418.12
[INFO 2017-06-28 14:20:55,379 main.py:51] epoch 8030, training loss: 9581.15, average training loss: 9494.35, base loss: 14416.97
[INFO 2017-06-28 14:20:56,403 main.py:51] epoch 8031, training loss: 8555.68, average training loss: 9492.84, base loss: 14415.37
[INFO 2017-06-28 14:20:57,477 main.py:51] epoch 8032, training loss: 9334.63, average training loss: 9493.15, base loss: 14416.77
[INFO 2017-06-28 14:20:58,555 main.py:51] epoch 8033, training loss: 9348.59, average training loss: 9492.09, base loss: 14415.61
[INFO 2017-06-28 14:20:59,591 main.py:51] epoch 8034, training loss: 9505.32, average training loss: 9491.73, base loss: 14414.53
[INFO 2017-06-28 14:21:00,589 main.py:51] epoch 8035, training loss: 9049.42, average training loss: 9492.30, base loss: 14414.94
[INFO 2017-06-28 14:21:01,654 main.py:51] epoch 8036, training loss: 9843.51, average training loss: 9493.87, base loss: 14418.36
[INFO 2017-06-28 14:21:02,754 main.py:51] epoch 8037, training loss: 10154.63, average training loss: 9495.96, base loss: 14419.89
[INFO 2017-06-28 14:21:03,889 main.py:51] epoch 8038, training loss: 8467.16, average training loss: 9496.08, base loss: 14420.97
[INFO 2017-06-28 14:21:05,008 main.py:51] epoch 8039, training loss: 10994.04, average training loss: 9497.87, base loss: 14423.48
[INFO 2017-06-28 14:21:06,121 main.py:51] epoch 8040, training loss: 8441.75, average training loss: 9497.12, base loss: 14421.67
[INFO 2017-06-28 14:21:07,167 main.py:51] epoch 8041, training loss: 11202.27, average training loss: 9498.29, base loss: 14421.97
[INFO 2017-06-28 14:21:08,187 main.py:51] epoch 8042, training loss: 9088.24, average training loss: 9499.41, base loss: 14423.21
[INFO 2017-06-28 14:21:09,220 main.py:51] epoch 8043, training loss: 9263.53, average training loss: 9500.01, base loss: 14423.70
[INFO 2017-06-28 14:21:10,261 main.py:51] epoch 8044, training loss: 8779.21, average training loss: 9498.29, base loss: 14421.25
[INFO 2017-06-28 14:21:11,329 main.py:51] epoch 8045, training loss: 8699.31, average training loss: 9497.92, base loss: 14420.76
[INFO 2017-06-28 14:21:12,428 main.py:51] epoch 8046, training loss: 8767.55, average training loss: 9498.07, base loss: 14421.45
[INFO 2017-06-28 14:21:13,498 main.py:51] epoch 8047, training loss: 7827.12, average training loss: 9497.37, base loss: 14420.70
[INFO 2017-06-28 14:21:14,494 main.py:51] epoch 8048, training loss: 9371.76, average training loss: 9495.80, base loss: 14420.14
[INFO 2017-06-28 14:21:15,582 main.py:51] epoch 8049, training loss: 9603.28, average training loss: 9496.47, base loss: 14422.16
[INFO 2017-06-28 14:21:16,689 main.py:51] epoch 8050, training loss: 8584.66, average training loss: 9496.25, base loss: 14421.67
[INFO 2017-06-28 14:21:17,749 main.py:51] epoch 8051, training loss: 8045.44, average training loss: 9495.10, base loss: 14419.27
[INFO 2017-06-28 14:21:18,784 main.py:51] epoch 8052, training loss: 9383.80, average training loss: 9495.41, base loss: 14418.59
[INFO 2017-06-28 14:21:19,829 main.py:51] epoch 8053, training loss: 9929.71, average training loss: 9495.56, base loss: 14418.08
[INFO 2017-06-28 14:21:20,901 main.py:51] epoch 8054, training loss: 8880.14, average training loss: 9495.21, base loss: 14418.04
[INFO 2017-06-28 14:21:21,961 main.py:51] epoch 8055, training loss: 9996.78, average training loss: 9495.67, base loss: 14419.77
[INFO 2017-06-28 14:21:22,981 main.py:51] epoch 8056, training loss: 9998.06, average training loss: 9497.35, base loss: 14422.73
[INFO 2017-06-28 14:21:24,023 main.py:51] epoch 8057, training loss: 8957.28, average training loss: 9496.08, base loss: 14419.02
[INFO 2017-06-28 14:21:25,112 main.py:51] epoch 8058, training loss: 8207.31, average training loss: 9495.08, base loss: 14416.44
[INFO 2017-06-28 14:21:26,180 main.py:51] epoch 8059, training loss: 9251.17, average training loss: 9493.54, base loss: 14415.16
[INFO 2017-06-28 14:21:27,294 main.py:51] epoch 8060, training loss: 8888.51, average training loss: 9492.64, base loss: 14414.76
[INFO 2017-06-28 14:21:28,328 main.py:51] epoch 8061, training loss: 9215.99, average training loss: 9493.05, base loss: 14415.89
[INFO 2017-06-28 14:21:29,413 main.py:51] epoch 8062, training loss: 8920.07, average training loss: 9490.75, base loss: 14413.00
[INFO 2017-06-28 14:21:30,458 main.py:51] epoch 8063, training loss: 9258.26, average training loss: 9488.31, base loss: 14408.69
[INFO 2017-06-28 14:21:31,518 main.py:51] epoch 8064, training loss: 10695.32, average training loss: 9489.96, base loss: 14411.60
[INFO 2017-06-28 14:21:32,604 main.py:51] epoch 8065, training loss: 9947.34, average training loss: 9488.77, base loss: 14409.53
[INFO 2017-06-28 14:21:33,640 main.py:51] epoch 8066, training loss: 9414.80, average training loss: 9488.10, base loss: 14408.77
[INFO 2017-06-28 14:21:34,691 main.py:51] epoch 8067, training loss: 8395.82, average training loss: 9487.27, base loss: 14408.67
[INFO 2017-06-28 14:21:35,730 main.py:51] epoch 8068, training loss: 10176.84, average training loss: 9488.23, base loss: 14409.48
[INFO 2017-06-28 14:21:36,824 main.py:51] epoch 8069, training loss: 9099.81, average training loss: 9487.53, base loss: 14408.07
[INFO 2017-06-28 14:21:37,892 main.py:51] epoch 8070, training loss: 10154.17, average training loss: 9488.28, base loss: 14408.53
[INFO 2017-06-28 14:21:38,936 main.py:51] epoch 8071, training loss: 9285.56, average training loss: 9488.04, base loss: 14409.08
[INFO 2017-06-28 14:21:39,994 main.py:51] epoch 8072, training loss: 10584.44, average training loss: 9490.27, base loss: 14412.29
[INFO 2017-06-28 14:21:41,029 main.py:51] epoch 8073, training loss: 9784.36, average training loss: 9491.41, base loss: 14413.11
[INFO 2017-06-28 14:21:42,094 main.py:51] epoch 8074, training loss: 8284.33, average training loss: 9490.28, base loss: 14409.58
[INFO 2017-06-28 14:21:43,201 main.py:51] epoch 8075, training loss: 9098.62, average training loss: 9489.89, base loss: 14409.38
[INFO 2017-06-28 14:21:44,217 main.py:51] epoch 8076, training loss: 9401.35, average training loss: 9488.78, base loss: 14408.64
[INFO 2017-06-28 14:21:45,291 main.py:51] epoch 8077, training loss: 9924.80, average training loss: 9490.21, base loss: 14411.38
[INFO 2017-06-28 14:21:46,365 main.py:51] epoch 8078, training loss: 9158.34, average training loss: 9490.61, base loss: 14412.07
[INFO 2017-06-28 14:21:47,486 main.py:51] epoch 8079, training loss: 9290.92, average training loss: 9490.63, base loss: 14412.14
[INFO 2017-06-28 14:21:48,611 main.py:51] epoch 8080, training loss: 10284.09, average training loss: 9492.33, base loss: 14414.19
[INFO 2017-06-28 14:21:49,691 main.py:51] epoch 8081, training loss: 9277.68, average training loss: 9492.28, base loss: 14414.11
[INFO 2017-06-28 14:21:50,822 main.py:51] epoch 8082, training loss: 10758.27, average training loss: 9493.47, base loss: 14415.82
[INFO 2017-06-28 14:21:51,804 main.py:51] epoch 8083, training loss: 9606.42, average training loss: 9494.84, base loss: 14416.53
[INFO 2017-06-28 14:21:52,847 main.py:51] epoch 8084, training loss: 9517.97, average training loss: 9494.88, base loss: 14416.79
[INFO 2017-06-28 14:21:53,937 main.py:51] epoch 8085, training loss: 9369.60, average training loss: 9495.63, base loss: 14415.95
[INFO 2017-06-28 14:21:54,961 main.py:51] epoch 8086, training loss: 8992.75, average training loss: 9493.21, base loss: 14412.13
[INFO 2017-06-28 14:21:55,995 main.py:51] epoch 8087, training loss: 9926.32, average training loss: 9494.42, base loss: 14412.70
[INFO 2017-06-28 14:21:57,050 main.py:51] epoch 8088, training loss: 9047.45, average training loss: 9493.65, base loss: 14410.52
[INFO 2017-06-28 14:21:58,130 main.py:51] epoch 8089, training loss: 9729.79, average training loss: 9494.42, base loss: 14411.19
[INFO 2017-06-28 14:21:59,234 main.py:51] epoch 8090, training loss: 9395.96, average training loss: 9494.71, base loss: 14410.05
[INFO 2017-06-28 14:22:00,259 main.py:51] epoch 8091, training loss: 8487.75, average training loss: 9495.17, base loss: 14411.60
[INFO 2017-06-28 14:22:01,254 main.py:51] epoch 8092, training loss: 10801.12, average training loss: 9496.75, base loss: 14414.09
[INFO 2017-06-28 14:22:02,249 main.py:51] epoch 8093, training loss: 9811.77, average training loss: 9496.29, base loss: 14413.47
[INFO 2017-06-28 14:22:03,246 main.py:51] epoch 8094, training loss: 9922.31, average training loss: 9496.24, base loss: 14414.77
[INFO 2017-06-28 14:22:04,329 main.py:51] epoch 8095, training loss: 10333.17, average training loss: 9498.97, base loss: 14419.56
[INFO 2017-06-28 14:22:05,435 main.py:51] epoch 8096, training loss: 9912.35, average training loss: 9500.45, base loss: 14421.22
[INFO 2017-06-28 14:22:06,572 main.py:51] epoch 8097, training loss: 9289.04, average training loss: 9500.43, base loss: 14420.88
[INFO 2017-06-28 14:22:07,647 main.py:51] epoch 8098, training loss: 9092.33, average training loss: 9500.53, base loss: 14420.41
[INFO 2017-06-28 14:22:08,741 main.py:51] epoch 8099, training loss: 10239.64, average training loss: 9501.48, base loss: 14421.74
[INFO 2017-06-28 14:22:08,741 main.py:53] epoch 8099, testing
[INFO 2017-06-28 14:22:12,545 main.py:105] average testing loss: 10827.58, base loss: 15343.24
[INFO 2017-06-28 14:22:12,545 main.py:106] improve_loss: 4515.66, improve_percent: 0.29
[INFO 2017-06-28 14:22:12,546 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:22:13,529 main.py:51] epoch 8100, training loss: 11037.51, average training loss: 9502.16, base loss: 14422.86
[INFO 2017-06-28 14:22:14,573 main.py:51] epoch 8101, training loss: 8152.89, average training loss: 9501.18, base loss: 14420.89
[INFO 2017-06-28 14:22:15,661 main.py:51] epoch 8102, training loss: 8625.63, average training loss: 9500.45, base loss: 14419.34
[INFO 2017-06-28 14:22:16,677 main.py:51] epoch 8103, training loss: 9289.68, average training loss: 9501.38, base loss: 14421.75
[INFO 2017-06-28 14:22:17,729 main.py:51] epoch 8104, training loss: 8621.72, average training loss: 9500.39, base loss: 14421.18
[INFO 2017-06-28 14:22:18,814 main.py:51] epoch 8105, training loss: 9389.18, average training loss: 9500.47, base loss: 14421.17
[INFO 2017-06-28 14:22:19,849 main.py:51] epoch 8106, training loss: 10400.94, average training loss: 9501.35, base loss: 14421.59
[INFO 2017-06-28 14:22:20,891 main.py:51] epoch 8107, training loss: 9406.85, average training loss: 9501.25, base loss: 14422.24
[INFO 2017-06-28 14:22:21,928 main.py:51] epoch 8108, training loss: 9596.88, average training loss: 9501.42, base loss: 14421.28
[INFO 2017-06-28 14:22:22,998 main.py:51] epoch 8109, training loss: 9584.40, average training loss: 9501.72, base loss: 14422.51
[INFO 2017-06-28 14:22:24,037 main.py:51] epoch 8110, training loss: 10658.36, average training loss: 9501.84, base loss: 14422.75
[INFO 2017-06-28 14:22:25,109 main.py:51] epoch 8111, training loss: 8884.91, average training loss: 9502.21, base loss: 14422.89
[INFO 2017-06-28 14:22:26,205 main.py:51] epoch 8112, training loss: 10634.53, average training loss: 9502.83, base loss: 14423.55
[INFO 2017-06-28 14:22:27,213 main.py:51] epoch 8113, training loss: 9397.20, average training loss: 9500.18, base loss: 14420.69
[INFO 2017-06-28 14:22:28,281 main.py:51] epoch 8114, training loss: 10195.40, average training loss: 9501.64, base loss: 14422.56
[INFO 2017-06-28 14:22:29,414 main.py:51] epoch 8115, training loss: 10626.46, average training loss: 9502.24, base loss: 14424.20
[INFO 2017-06-28 14:22:30,420 main.py:51] epoch 8116, training loss: 9533.90, average training loss: 9502.76, base loss: 14424.12
[INFO 2017-06-28 14:22:31,502 main.py:51] epoch 8117, training loss: 10539.23, average training loss: 9503.38, base loss: 14424.81
[INFO 2017-06-28 14:22:32,531 main.py:51] epoch 8118, training loss: 9351.49, average training loss: 9503.50, base loss: 14425.40
[INFO 2017-06-28 14:22:33,604 main.py:51] epoch 8119, training loss: 9180.39, average training loss: 9503.76, base loss: 14425.93
[INFO 2017-06-28 14:22:34,735 main.py:51] epoch 8120, training loss: 9648.90, average training loss: 9502.62, base loss: 14424.34
[INFO 2017-06-28 14:22:35,781 main.py:51] epoch 8121, training loss: 10540.27, average training loss: 9503.41, base loss: 14423.93
[INFO 2017-06-28 14:22:36,879 main.py:51] epoch 8122, training loss: 9256.27, average training loss: 9503.11, base loss: 14423.14
[INFO 2017-06-28 14:22:37,925 main.py:51] epoch 8123, training loss: 9719.04, average training loss: 9500.61, base loss: 14420.29
[INFO 2017-06-28 14:22:38,963 main.py:51] epoch 8124, training loss: 8899.39, average training loss: 9500.82, base loss: 14421.48
[INFO 2017-06-28 14:22:40,055 main.py:51] epoch 8125, training loss: 7746.53, average training loss: 9499.21, base loss: 14418.96
[INFO 2017-06-28 14:22:41,148 main.py:51] epoch 8126, training loss: 10435.96, average training loss: 9500.20, base loss: 14420.57
[INFO 2017-06-28 14:22:42,255 main.py:51] epoch 8127, training loss: 10210.09, average training loss: 9500.74, base loss: 14422.87
[INFO 2017-06-28 14:22:43,285 main.py:51] epoch 8128, training loss: 8772.54, average training loss: 9500.15, base loss: 14423.03
[INFO 2017-06-28 14:22:44,311 main.py:51] epoch 8129, training loss: 8773.59, average training loss: 9499.80, base loss: 14423.56
[INFO 2017-06-28 14:22:45,385 main.py:51] epoch 8130, training loss: 7994.93, average training loss: 9497.75, base loss: 14420.70
[INFO 2017-06-28 14:22:46,496 main.py:51] epoch 8131, training loss: 9281.50, average training loss: 9497.73, base loss: 14421.37
[INFO 2017-06-28 14:22:47,595 main.py:51] epoch 8132, training loss: 9438.54, average training loss: 9498.61, base loss: 14422.42
[INFO 2017-06-28 14:22:48,626 main.py:51] epoch 8133, training loss: 8372.66, average training loss: 9497.06, base loss: 14419.20
[INFO 2017-06-28 14:22:49,666 main.py:51] epoch 8134, training loss: 9008.04, average training loss: 9496.78, base loss: 14417.32
[INFO 2017-06-28 14:22:50,715 main.py:51] epoch 8135, training loss: 9503.61, average training loss: 9495.33, base loss: 14415.50
[INFO 2017-06-28 14:22:51,785 main.py:51] epoch 8136, training loss: 12276.86, average training loss: 9498.14, base loss: 14418.80
[INFO 2017-06-28 14:22:52,856 main.py:51] epoch 8137, training loss: 9698.45, average training loss: 9497.45, base loss: 14416.80
[INFO 2017-06-28 14:22:53,935 main.py:51] epoch 8138, training loss: 9297.02, average training loss: 9498.06, base loss: 14419.26
[INFO 2017-06-28 14:22:55,026 main.py:51] epoch 8139, training loss: 10395.92, average training loss: 9499.41, base loss: 14422.61
[INFO 2017-06-28 14:22:56,063 main.py:51] epoch 8140, training loss: 9522.18, average training loss: 9499.90, base loss: 14425.06
[INFO 2017-06-28 14:22:57,125 main.py:51] epoch 8141, training loss: 9269.13, average training loss: 9499.02, base loss: 14425.22
[INFO 2017-06-28 14:22:58,147 main.py:51] epoch 8142, training loss: 8890.68, average training loss: 9497.86, base loss: 14423.98
[INFO 2017-06-28 14:22:59,246 main.py:51] epoch 8143, training loss: 9828.88, average training loss: 9497.87, base loss: 14423.51
[INFO 2017-06-28 14:23:00,346 main.py:51] epoch 8144, training loss: 10258.35, average training loss: 9498.78, base loss: 14425.64
[INFO 2017-06-28 14:23:01,383 main.py:51] epoch 8145, training loss: 9847.36, average training loss: 9499.64, base loss: 14428.77
[INFO 2017-06-28 14:23:02,427 main.py:51] epoch 8146, training loss: 9951.06, average training loss: 9500.80, base loss: 14430.88
[INFO 2017-06-28 14:23:03,459 main.py:51] epoch 8147, training loss: 10030.51, average training loss: 9502.17, base loss: 14432.07
[INFO 2017-06-28 14:23:04,514 main.py:51] epoch 8148, training loss: 8095.62, average training loss: 9501.13, base loss: 14430.22
[INFO 2017-06-28 14:23:05,623 main.py:51] epoch 8149, training loss: 9648.48, average training loss: 9502.35, base loss: 14432.17
[INFO 2017-06-28 14:23:06,663 main.py:51] epoch 8150, training loss: 9023.71, average training loss: 9502.42, base loss: 14433.73
[INFO 2017-06-28 14:23:07,783 main.py:51] epoch 8151, training loss: 9806.05, average training loss: 9502.91, base loss: 14435.57
[INFO 2017-06-28 14:23:08,815 main.py:51] epoch 8152, training loss: 9684.37, average training loss: 9503.53, base loss: 14436.66
[INFO 2017-06-28 14:23:09,888 main.py:51] epoch 8153, training loss: 9361.79, average training loss: 9504.40, base loss: 14439.35
[INFO 2017-06-28 14:23:10,925 main.py:51] epoch 8154, training loss: 12047.71, average training loss: 9506.53, base loss: 14441.62
[INFO 2017-06-28 14:23:11,971 main.py:51] epoch 8155, training loss: 9016.81, average training loss: 9504.96, base loss: 14438.71
[INFO 2017-06-28 14:23:13,069 main.py:51] epoch 8156, training loss: 8538.64, average training loss: 9504.16, base loss: 14438.41
[INFO 2017-06-28 14:23:14,124 main.py:51] epoch 8157, training loss: 10367.55, average training loss: 9504.94, base loss: 14440.11
[INFO 2017-06-28 14:23:15,143 main.py:51] epoch 8158, training loss: 8723.77, average training loss: 9503.62, base loss: 14436.81
[INFO 2017-06-28 14:23:16,223 main.py:51] epoch 8159, training loss: 9594.64, average training loss: 9502.56, base loss: 14435.94
[INFO 2017-06-28 14:23:17,308 main.py:51] epoch 8160, training loss: 9698.52, average training loss: 9503.80, base loss: 14437.33
[INFO 2017-06-28 14:23:18,417 main.py:51] epoch 8161, training loss: 9123.94, average training loss: 9503.86, base loss: 14438.68
[INFO 2017-06-28 14:23:19,351 main.py:51] epoch 8162, training loss: 9410.78, average training loss: 9503.93, base loss: 14439.22
[INFO 2017-06-28 14:23:20,289 main.py:51] epoch 8163, training loss: 8175.78, average training loss: 9503.16, base loss: 14437.98
[INFO 2017-06-28 14:23:21,288 main.py:51] epoch 8164, training loss: 9571.66, average training loss: 9503.77, base loss: 14439.75
[INFO 2017-06-28 14:23:22,396 main.py:51] epoch 8165, training loss: 8566.57, average training loss: 9503.70, base loss: 14439.81
[INFO 2017-06-28 14:23:23,539 main.py:51] epoch 8166, training loss: 9309.00, average training loss: 9503.61, base loss: 14438.11
[INFO 2017-06-28 14:23:24,600 main.py:51] epoch 8167, training loss: 8992.57, average training loss: 9503.24, base loss: 14437.04
[INFO 2017-06-28 14:23:25,634 main.py:51] epoch 8168, training loss: 9673.46, average training loss: 9503.70, base loss: 14436.08
[INFO 2017-06-28 14:23:26,739 main.py:51] epoch 8169, training loss: 8949.27, average training loss: 9503.07, base loss: 14434.75
[INFO 2017-06-28 14:23:27,835 main.py:51] epoch 8170, training loss: 9274.62, average training loss: 9503.29, base loss: 14434.46
[INFO 2017-06-28 14:23:28,865 main.py:51] epoch 8171, training loss: 8758.78, average training loss: 9502.48, base loss: 14433.04
[INFO 2017-06-28 14:23:29,925 main.py:51] epoch 8172, training loss: 8959.27, average training loss: 9502.30, base loss: 14431.53
[INFO 2017-06-28 14:23:30,970 main.py:51] epoch 8173, training loss: 9722.87, average training loss: 9502.50, base loss: 14430.94
[INFO 2017-06-28 14:23:32,076 main.py:51] epoch 8174, training loss: 8937.28, average training loss: 9502.09, base loss: 14430.54
[INFO 2017-06-28 14:23:33,192 main.py:51] epoch 8175, training loss: 9580.59, average training loss: 9503.18, base loss: 14432.23
[INFO 2017-06-28 14:23:34,234 main.py:51] epoch 8176, training loss: 10160.87, average training loss: 9503.49, base loss: 14432.07
[INFO 2017-06-28 14:23:35,316 main.py:51] epoch 8177, training loss: 8190.61, average training loss: 9502.06, base loss: 14430.78
[INFO 2017-06-28 14:23:36,344 main.py:51] epoch 8178, training loss: 9280.13, average training loss: 9500.62, base loss: 14428.17
[INFO 2017-06-28 14:23:37,410 main.py:51] epoch 8179, training loss: 11738.56, average training loss: 9502.15, base loss: 14431.21
[INFO 2017-06-28 14:23:38,515 main.py:51] epoch 8180, training loss: 7688.82, average training loss: 9500.90, base loss: 14429.64
[INFO 2017-06-28 14:23:39,555 main.py:51] epoch 8181, training loss: 9317.71, average training loss: 9501.69, base loss: 14430.87
[INFO 2017-06-28 14:23:40,588 main.py:51] epoch 8182, training loss: 9461.04, average training loss: 9500.30, base loss: 14429.57
[INFO 2017-06-28 14:23:41,647 main.py:51] epoch 8183, training loss: 9769.34, average training loss: 9500.05, base loss: 14430.13
[INFO 2017-06-28 14:23:42,741 main.py:51] epoch 8184, training loss: 8831.38, average training loss: 9498.23, base loss: 14428.69
[INFO 2017-06-28 14:23:43,864 main.py:51] epoch 8185, training loss: 8258.48, average training loss: 9496.95, base loss: 14427.89
[INFO 2017-06-28 14:23:44,898 main.py:51] epoch 8186, training loss: 8846.96, average training loss: 9495.86, base loss: 14427.20
[INFO 2017-06-28 14:23:45,914 main.py:51] epoch 8187, training loss: 9653.81, average training loss: 9496.80, base loss: 14428.78
[INFO 2017-06-28 14:23:46,959 main.py:51] epoch 8188, training loss: 8962.84, average training loss: 9495.74, base loss: 14428.19
[INFO 2017-06-28 14:23:48,044 main.py:51] epoch 8189, training loss: 9254.63, average training loss: 9496.50, base loss: 14429.54
[INFO 2017-06-28 14:23:49,156 main.py:51] epoch 8190, training loss: 12062.66, average training loss: 9499.80, base loss: 14435.69
[INFO 2017-06-28 14:23:50,207 main.py:51] epoch 8191, training loss: 8700.37, average training loss: 9498.16, base loss: 14433.20
[INFO 2017-06-28 14:23:51,234 main.py:51] epoch 8192, training loss: 10504.89, average training loss: 9498.20, base loss: 14431.63
[INFO 2017-06-28 14:23:52,295 main.py:51] epoch 8193, training loss: 8473.09, average training loss: 9498.33, base loss: 14431.94
[INFO 2017-06-28 14:23:53,393 main.py:51] epoch 8194, training loss: 10372.42, average training loss: 9499.59, base loss: 14433.38
[INFO 2017-06-28 14:23:54,475 main.py:51] epoch 8195, training loss: 8301.40, average training loss: 9498.01, base loss: 14431.01
[INFO 2017-06-28 14:23:55,482 main.py:51] epoch 8196, training loss: 9755.79, average training loss: 9496.85, base loss: 14429.70
[INFO 2017-06-28 14:23:56,548 main.py:51] epoch 8197, training loss: 10117.93, average training loss: 9496.85, base loss: 14431.85
[INFO 2017-06-28 14:23:57,634 main.py:51] epoch 8198, training loss: 9660.11, average training loss: 9497.74, base loss: 14433.93
[INFO 2017-06-28 14:23:58,688 main.py:51] epoch 8199, training loss: 9772.70, average training loss: 9499.65, base loss: 14436.24
[INFO 2017-06-28 14:23:58,688 main.py:53] epoch 8199, testing
[INFO 2017-06-28 14:24:02,350 main.py:105] average testing loss: 10510.19, base loss: 14858.41
[INFO 2017-06-28 14:24:02,350 main.py:106] improve_loss: 4348.22, improve_percent: 0.29
[INFO 2017-06-28 14:24:02,351 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:24:03,407 main.py:51] epoch 8200, training loss: 9031.54, average training loss: 9498.91, base loss: 14434.23
[INFO 2017-06-28 14:24:04,495 main.py:51] epoch 8201, training loss: 10842.20, average training loss: 9501.41, base loss: 14436.69
[INFO 2017-06-28 14:24:05,612 main.py:51] epoch 8202, training loss: 10381.31, average training loss: 9501.04, base loss: 14437.23
[INFO 2017-06-28 14:24:06,679 main.py:51] epoch 8203, training loss: 8965.33, average training loss: 9500.15, base loss: 14436.16
[INFO 2017-06-28 14:24:07,748 main.py:51] epoch 8204, training loss: 8487.29, average training loss: 9499.92, base loss: 14437.16
[INFO 2017-06-28 14:24:08,758 main.py:51] epoch 8205, training loss: 8563.41, average training loss: 9499.31, base loss: 14436.47
[INFO 2017-06-28 14:24:09,840 main.py:51] epoch 8206, training loss: 10110.43, average training loss: 9500.43, base loss: 14439.56
[INFO 2017-06-28 14:24:10,962 main.py:51] epoch 8207, training loss: 8630.21, average training loss: 9499.03, base loss: 14438.92
[INFO 2017-06-28 14:24:12,076 main.py:51] epoch 8208, training loss: 9185.02, average training loss: 9498.12, base loss: 14437.57
[INFO 2017-06-28 14:24:13,220 main.py:51] epoch 8209, training loss: 9139.41, average training loss: 9497.20, base loss: 14435.45
[INFO 2017-06-28 14:24:14,208 main.py:51] epoch 8210, training loss: 9817.33, average training loss: 9497.82, base loss: 14436.90
[INFO 2017-06-28 14:24:15,231 main.py:51] epoch 8211, training loss: 10198.58, average training loss: 9499.56, base loss: 14439.47
[INFO 2017-06-28 14:24:16,288 main.py:51] epoch 8212, training loss: 10317.94, average training loss: 9499.62, base loss: 14440.20
[INFO 2017-06-28 14:24:17,325 main.py:51] epoch 8213, training loss: 10842.66, average training loss: 9501.22, base loss: 14443.12
[INFO 2017-06-28 14:24:18,410 main.py:51] epoch 8214, training loss: 8961.14, average training loss: 9501.54, base loss: 14444.32
[INFO 2017-06-28 14:24:19,543 main.py:51] epoch 8215, training loss: 9575.76, average training loss: 9499.24, base loss: 14439.90
[INFO 2017-06-28 14:24:20,608 main.py:51] epoch 8216, training loss: 10247.34, average training loss: 9500.94, base loss: 14443.42
[INFO 2017-06-28 14:24:21,638 main.py:51] epoch 8217, training loss: 9407.03, average training loss: 9501.73, base loss: 14445.43
[INFO 2017-06-28 14:24:22,706 main.py:51] epoch 8218, training loss: 9769.50, average training loss: 9502.58, base loss: 14446.27
[INFO 2017-06-28 14:24:23,840 main.py:51] epoch 8219, training loss: 11166.65, average training loss: 9505.20, base loss: 14451.99
[INFO 2017-06-28 14:24:24,927 main.py:51] epoch 8220, training loss: 8843.51, average training loss: 9504.78, base loss: 14450.58
[INFO 2017-06-28 14:24:26,021 main.py:51] epoch 8221, training loss: 9174.26, average training loss: 9504.51, base loss: 14451.15
[INFO 2017-06-28 14:24:27,007 main.py:51] epoch 8222, training loss: 9624.02, average training loss: 9505.48, base loss: 14452.38
[INFO 2017-06-28 14:24:28,076 main.py:51] epoch 8223, training loss: 9723.86, average training loss: 9506.46, base loss: 14455.31
[INFO 2017-06-28 14:24:29,157 main.py:51] epoch 8224, training loss: 8849.10, average training loss: 9506.23, base loss: 14453.97
[INFO 2017-06-28 14:24:30,159 main.py:51] epoch 8225, training loss: 10515.32, average training loss: 9507.82, base loss: 14456.67
[INFO 2017-06-28 14:24:31,328 main.py:51] epoch 8226, training loss: 8485.64, average training loss: 9507.22, base loss: 14456.12
[INFO 2017-06-28 14:24:32,393 main.py:51] epoch 8227, training loss: 8174.99, average training loss: 9506.32, base loss: 14454.93
[INFO 2017-06-28 14:24:33,452 main.py:51] epoch 8228, training loss: 9135.79, average training loss: 9506.42, base loss: 14455.36
[INFO 2017-06-28 14:24:34,538 main.py:51] epoch 8229, training loss: 8441.02, average training loss: 9505.65, base loss: 14453.64
[INFO 2017-06-28 14:24:35,568 main.py:51] epoch 8230, training loss: 9181.99, average training loss: 9505.73, base loss: 14454.41
[INFO 2017-06-28 14:24:36,655 main.py:51] epoch 8231, training loss: 8103.47, average training loss: 9503.59, base loss: 14451.63
[INFO 2017-06-28 14:24:37,667 main.py:51] epoch 8232, training loss: 8855.71, average training loss: 9501.49, base loss: 14448.02
[INFO 2017-06-28 14:24:38,619 main.py:51] epoch 8233, training loss: 10724.21, average training loss: 9501.30, base loss: 14447.09
[INFO 2017-06-28 14:24:39,552 main.py:51] epoch 8234, training loss: 10430.61, average training loss: 9503.19, base loss: 14450.32
[INFO 2017-06-28 14:24:40,626 main.py:51] epoch 8235, training loss: 9639.63, average training loss: 9502.53, base loss: 14450.19
[INFO 2017-06-28 14:24:41,647 main.py:51] epoch 8236, training loss: 9088.81, average training loss: 9501.83, base loss: 14449.92
[INFO 2017-06-28 14:24:42,681 main.py:51] epoch 8237, training loss: 8540.97, average training loss: 9502.02, base loss: 14451.31
[INFO 2017-06-28 14:24:43,773 main.py:51] epoch 8238, training loss: 8518.51, average training loss: 9502.26, base loss: 14452.48
[INFO 2017-06-28 14:24:44,831 main.py:51] epoch 8239, training loss: 9852.77, average training loss: 9501.72, base loss: 14451.72
[INFO 2017-06-28 14:24:46,037 main.py:51] epoch 8240, training loss: 8892.95, average training loss: 9500.86, base loss: 14449.26
[INFO 2017-06-28 14:24:47,139 main.py:51] epoch 8241, training loss: 9904.85, average training loss: 9501.41, base loss: 14452.24
[INFO 2017-06-28 14:24:48,205 main.py:51] epoch 8242, training loss: 10096.06, average training loss: 9502.29, base loss: 14453.55
[INFO 2017-06-28 14:24:49,248 main.py:51] epoch 8243, training loss: 9641.30, average training loss: 9502.63, base loss: 14454.43
[INFO 2017-06-28 14:24:50,405 main.py:51] epoch 8244, training loss: 9701.28, average training loss: 9503.41, base loss: 14456.75
[INFO 2017-06-28 14:24:51,526 main.py:51] epoch 8245, training loss: 9369.40, average training loss: 9501.94, base loss: 14454.99
[INFO 2017-06-28 14:24:52,519 main.py:51] epoch 8246, training loss: 9900.19, average training loss: 9503.25, base loss: 14458.05
[INFO 2017-06-28 14:24:53,551 main.py:51] epoch 8247, training loss: 8815.86, average training loss: 9502.90, base loss: 14456.54
[INFO 2017-06-28 14:24:54,585 main.py:51] epoch 8248, training loss: 10288.33, average training loss: 9503.55, base loss: 14456.92
[INFO 2017-06-28 14:24:55,672 main.py:51] epoch 8249, training loss: 9087.35, average training loss: 9502.73, base loss: 14455.95
[INFO 2017-06-28 14:24:56,743 main.py:51] epoch 8250, training loss: 10168.63, average training loss: 9503.70, base loss: 14459.58
[INFO 2017-06-28 14:24:57,772 main.py:51] epoch 8251, training loss: 9276.56, average training loss: 9503.50, base loss: 14460.54
[INFO 2017-06-28 14:24:58,821 main.py:51] epoch 8252, training loss: 10144.63, average training loss: 9504.59, base loss: 14462.07
[INFO 2017-06-28 14:25:00,007 main.py:51] epoch 8253, training loss: 8260.73, average training loss: 9502.69, base loss: 14459.41
[INFO 2017-06-28 14:25:01,086 main.py:51] epoch 8254, training loss: 9282.88, average training loss: 9501.76, base loss: 14457.84
[INFO 2017-06-28 14:25:02,127 main.py:51] epoch 8255, training loss: 9313.72, average training loss: 9502.09, base loss: 14459.42
[INFO 2017-06-28 14:25:03,190 main.py:51] epoch 8256, training loss: 9261.37, average training loss: 9501.63, base loss: 14456.14
[INFO 2017-06-28 14:25:04,230 main.py:51] epoch 8257, training loss: 10664.72, average training loss: 9503.25, base loss: 14456.91
[INFO 2017-06-28 14:25:05,348 main.py:51] epoch 8258, training loss: 8984.61, average training loss: 9502.02, base loss: 14456.53
[INFO 2017-06-28 14:25:06,386 main.py:51] epoch 8259, training loss: 8518.44, average training loss: 9501.18, base loss: 14455.60
[INFO 2017-06-28 14:25:07,408 main.py:51] epoch 8260, training loss: 8758.56, average training loss: 9500.99, base loss: 14455.87
[INFO 2017-06-28 14:25:08,422 main.py:51] epoch 8261, training loss: 9703.48, average training loss: 9502.09, base loss: 14458.68
[INFO 2017-06-28 14:25:09,521 main.py:51] epoch 8262, training loss: 9497.03, average training loss: 9502.21, base loss: 14459.58
[INFO 2017-06-28 14:25:10,583 main.py:51] epoch 8263, training loss: 9506.85, average training loss: 9503.21, base loss: 14461.37
[INFO 2017-06-28 14:25:11,594 main.py:51] epoch 8264, training loss: 10746.53, average training loss: 9504.52, base loss: 14464.38
[INFO 2017-06-28 14:25:12,651 main.py:51] epoch 8265, training loss: 8434.09, average training loss: 9503.30, base loss: 14461.10
[INFO 2017-06-28 14:25:13,746 main.py:51] epoch 8266, training loss: 8600.75, average training loss: 9502.43, base loss: 14459.88
[INFO 2017-06-28 14:25:14,880 main.py:51] epoch 8267, training loss: 8879.32, average training loss: 9501.89, base loss: 14457.78
[INFO 2017-06-28 14:25:15,880 main.py:51] epoch 8268, training loss: 9060.82, average training loss: 9502.28, base loss: 14458.01
[INFO 2017-06-28 14:25:16,922 main.py:51] epoch 8269, training loss: 9098.11, average training loss: 9501.95, base loss: 14456.24
[INFO 2017-06-28 14:25:17,981 main.py:51] epoch 8270, training loss: 9025.70, average training loss: 9501.98, base loss: 14455.59
[INFO 2017-06-28 14:25:19,009 main.py:51] epoch 8271, training loss: 10905.17, average training loss: 9503.10, base loss: 14456.38
[INFO 2017-06-28 14:25:20,060 main.py:51] epoch 8272, training loss: 9761.55, average training loss: 9503.79, base loss: 14455.76
[INFO 2017-06-28 14:25:21,081 main.py:51] epoch 8273, training loss: 8271.57, average training loss: 9502.11, base loss: 14454.00
[INFO 2017-06-28 14:25:22,151 main.py:51] epoch 8274, training loss: 9351.51, average training loss: 9502.02, base loss: 14453.35
[INFO 2017-06-28 14:25:23,202 main.py:51] epoch 8275, training loss: 8970.85, average training loss: 9500.69, base loss: 14449.90
[INFO 2017-06-28 14:25:24,292 main.py:51] epoch 8276, training loss: 10133.92, average training loss: 9502.47, base loss: 14452.30
[INFO 2017-06-28 14:25:25,379 main.py:51] epoch 8277, training loss: 9446.95, average training loss: 9502.42, base loss: 14450.78
[INFO 2017-06-28 14:25:26,390 main.py:51] epoch 8278, training loss: 9110.33, average training loss: 9501.09, base loss: 14447.89
[INFO 2017-06-28 14:25:27,440 main.py:51] epoch 8279, training loss: 9533.05, average training loss: 9500.78, base loss: 14447.64
[INFO 2017-06-28 14:25:28,489 main.py:51] epoch 8280, training loss: 9959.04, average training loss: 9502.22, base loss: 14450.58
[INFO 2017-06-28 14:25:29,578 main.py:51] epoch 8281, training loss: 10500.92, average training loss: 9503.20, base loss: 14451.77
[INFO 2017-06-28 14:25:30,695 main.py:51] epoch 8282, training loss: 9959.59, average training loss: 9504.15, base loss: 14454.08
[INFO 2017-06-28 14:25:31,783 main.py:51] epoch 8283, training loss: 9190.30, average training loss: 9504.03, base loss: 14454.20
[INFO 2017-06-28 14:25:32,921 main.py:51] epoch 8284, training loss: 9509.80, average training loss: 9502.55, base loss: 14451.71
[INFO 2017-06-28 14:25:34,037 main.py:51] epoch 8285, training loss: 9278.55, average training loss: 9501.56, base loss: 14450.83
[INFO 2017-06-28 14:25:35,172 main.py:51] epoch 8286, training loss: 8694.49, average training loss: 9501.22, base loss: 14450.46
[INFO 2017-06-28 14:25:36,186 main.py:51] epoch 8287, training loss: 8776.33, average training loss: 9500.57, base loss: 14448.73
[INFO 2017-06-28 14:25:37,259 main.py:51] epoch 8288, training loss: 11400.73, average training loss: 9502.42, base loss: 14448.93
[INFO 2017-06-28 14:25:38,311 main.py:51] epoch 8289, training loss: 9556.17, average training loss: 9503.58, base loss: 14449.85
[INFO 2017-06-28 14:25:39,358 main.py:51] epoch 8290, training loss: 9171.74, average training loss: 9502.73, base loss: 14447.37
[INFO 2017-06-28 14:25:40,420 main.py:51] epoch 8291, training loss: 9925.34, average training loss: 9503.00, base loss: 14447.88
[INFO 2017-06-28 14:25:41,454 main.py:51] epoch 8292, training loss: 10004.54, average training loss: 9503.88, base loss: 14448.89
[INFO 2017-06-28 14:25:42,546 main.py:51] epoch 8293, training loss: 11795.73, average training loss: 9505.94, base loss: 14453.36
[INFO 2017-06-28 14:25:43,675 main.py:51] epoch 8294, training loss: 10279.89, average training loss: 9507.84, base loss: 14455.19
[INFO 2017-06-28 14:25:44,707 main.py:51] epoch 8295, training loss: 8770.12, average training loss: 9507.22, base loss: 14452.92
[INFO 2017-06-28 14:25:45,816 main.py:51] epoch 8296, training loss: 9255.58, average training loss: 9506.80, base loss: 14452.97
[INFO 2017-06-28 14:25:46,855 main.py:51] epoch 8297, training loss: 10817.84, average training loss: 9507.08, base loss: 14453.14
[INFO 2017-06-28 14:25:47,900 main.py:51] epoch 8298, training loss: 9379.56, average training loss: 9505.57, base loss: 14449.51
[INFO 2017-06-28 14:25:48,952 main.py:51] epoch 8299, training loss: 9005.69, average training loss: 9505.49, base loss: 14447.72
[INFO 2017-06-28 14:25:48,952 main.py:53] epoch 8299, testing
[INFO 2017-06-28 14:25:52,734 main.py:105] average testing loss: 10988.33, base loss: 15883.47
[INFO 2017-06-28 14:25:52,734 main.py:106] improve_loss: 4895.15, improve_percent: 0.31
[INFO 2017-06-28 14:25:52,735 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:25:53,806 main.py:51] epoch 8300, training loss: 8767.80, average training loss: 9503.59, base loss: 14444.84
[INFO 2017-06-28 14:25:54,917 main.py:51] epoch 8301, training loss: 9007.07, average training loss: 9503.77, base loss: 14444.93
[INFO 2017-06-28 14:25:56,002 main.py:51] epoch 8302, training loss: 11408.14, average training loss: 9505.73, base loss: 14447.34
[INFO 2017-06-28 14:25:56,950 main.py:51] epoch 8303, training loss: 9285.38, average training loss: 9505.48, base loss: 14446.24
[INFO 2017-06-28 14:25:57,907 main.py:51] epoch 8304, training loss: 11120.49, average training loss: 9505.57, base loss: 14446.82
[INFO 2017-06-28 14:25:58,873 main.py:51] epoch 8305, training loss: 8589.95, average training loss: 9504.86, base loss: 14445.76
[INFO 2017-06-28 14:25:59,899 main.py:51] epoch 8306, training loss: 8552.84, average training loss: 9505.73, base loss: 14446.37
[INFO 2017-06-28 14:26:00,970 main.py:51] epoch 8307, training loss: 9720.17, average training loss: 9505.81, base loss: 14446.57
[INFO 2017-06-28 14:26:01,997 main.py:51] epoch 8308, training loss: 8788.96, average training loss: 9505.13, base loss: 14446.27
[INFO 2017-06-28 14:26:03,045 main.py:51] epoch 8309, training loss: 8410.71, average training loss: 9502.14, base loss: 14442.48
[INFO 2017-06-28 14:26:04,185 main.py:51] epoch 8310, training loss: 8972.05, average training loss: 9501.20, base loss: 14441.09
[INFO 2017-06-28 14:26:05,176 main.py:51] epoch 8311, training loss: 9361.29, average training loss: 9501.17, base loss: 14442.62
[INFO 2017-06-28 14:26:06,254 main.py:51] epoch 8312, training loss: 10713.91, average training loss: 9502.59, base loss: 14444.77
[INFO 2017-06-28 14:26:07,412 main.py:51] epoch 8313, training loss: 9878.12, average training loss: 9503.94, base loss: 14446.62
[INFO 2017-06-28 14:26:08,495 main.py:51] epoch 8314, training loss: 9390.42, average training loss: 9504.17, base loss: 14446.36
[INFO 2017-06-28 14:26:09,611 main.py:51] epoch 8315, training loss: 9384.87, average training loss: 9503.39, base loss: 14446.53
[INFO 2017-06-28 14:26:10,639 main.py:51] epoch 8316, training loss: 9769.71, average training loss: 9504.13, base loss: 14447.70
[INFO 2017-06-28 14:26:11,673 main.py:51] epoch 8317, training loss: 9323.12, average training loss: 9504.27, base loss: 14448.44
[INFO 2017-06-28 14:26:12,730 main.py:51] epoch 8318, training loss: 7759.88, average training loss: 9501.51, base loss: 14443.43
[INFO 2017-06-28 14:26:13,797 main.py:51] epoch 8319, training loss: 8404.04, average training loss: 9499.86, base loss: 14440.11
[INFO 2017-06-28 14:26:14,855 main.py:51] epoch 8320, training loss: 10612.77, average training loss: 9500.20, base loss: 14440.42
[INFO 2017-06-28 14:26:15,926 main.py:51] epoch 8321, training loss: 8785.46, average training loss: 9499.38, base loss: 14440.30
[INFO 2017-06-28 14:26:16,973 main.py:51] epoch 8322, training loss: 9634.15, average training loss: 9499.15, base loss: 14438.74
[INFO 2017-06-28 14:26:18,058 main.py:51] epoch 8323, training loss: 9307.55, average training loss: 9497.62, base loss: 14434.58
[INFO 2017-06-28 14:26:19,096 main.py:51] epoch 8324, training loss: 9536.29, average training loss: 9497.17, base loss: 14435.44
[INFO 2017-06-28 14:26:20,243 main.py:51] epoch 8325, training loss: 10930.33, average training loss: 9499.27, base loss: 14437.15
[INFO 2017-06-28 14:26:21,285 main.py:51] epoch 8326, training loss: 8038.96, average training loss: 9498.03, base loss: 14435.28
[INFO 2017-06-28 14:26:22,331 main.py:51] epoch 8327, training loss: 9486.15, average training loss: 9498.57, base loss: 14436.89
[INFO 2017-06-28 14:26:23,347 main.py:51] epoch 8328, training loss: 8409.28, average training loss: 9497.66, base loss: 14435.81
[INFO 2017-06-28 14:26:24,373 main.py:51] epoch 8329, training loss: 8777.90, average training loss: 9495.44, base loss: 14432.02
[INFO 2017-06-28 14:26:25,425 main.py:51] epoch 8330, training loss: 9163.33, average training loss: 9496.00, base loss: 14431.82
[INFO 2017-06-28 14:26:26,456 main.py:51] epoch 8331, training loss: 9570.56, average training loss: 9495.38, base loss: 14430.59
[INFO 2017-06-28 14:26:27,541 main.py:51] epoch 8332, training loss: 10542.90, average training loss: 9496.84, base loss: 14432.69
[INFO 2017-06-28 14:26:28,639 main.py:51] epoch 8333, training loss: 10648.23, average training loss: 9497.55, base loss: 14433.17
[INFO 2017-06-28 14:26:29,633 main.py:51] epoch 8334, training loss: 10329.05, average training loss: 9498.93, base loss: 14436.91
[INFO 2017-06-28 14:26:30,669 main.py:51] epoch 8335, training loss: 8982.35, average training loss: 9499.51, base loss: 14437.75
[INFO 2017-06-28 14:26:31,742 main.py:51] epoch 8336, training loss: 9371.94, average training loss: 9498.58, base loss: 14437.36
[INFO 2017-06-28 14:26:32,775 main.py:51] epoch 8337, training loss: 9768.38, average training loss: 9499.43, base loss: 14439.79
[INFO 2017-06-28 14:26:33,847 main.py:51] epoch 8338, training loss: 8717.70, average training loss: 9498.32, base loss: 14438.16
[INFO 2017-06-28 14:26:34,912 main.py:51] epoch 8339, training loss: 8245.13, average training loss: 9496.83, base loss: 14437.33
[INFO 2017-06-28 14:26:35,974 main.py:51] epoch 8340, training loss: 8010.95, average training loss: 9495.68, base loss: 14436.72
[INFO 2017-06-28 14:26:37,047 main.py:51] epoch 8341, training loss: 10274.48, average training loss: 9496.46, base loss: 14439.24
[INFO 2017-06-28 14:26:38,111 main.py:51] epoch 8342, training loss: 8857.43, average training loss: 9495.98, base loss: 14439.38
[INFO 2017-06-28 14:26:39,158 main.py:51] epoch 8343, training loss: 9811.33, average training loss: 9496.30, base loss: 14441.41
[INFO 2017-06-28 14:26:40,201 main.py:51] epoch 8344, training loss: 9922.03, average training loss: 9497.70, base loss: 14443.50
[INFO 2017-06-28 14:26:41,264 main.py:51] epoch 8345, training loss: 9352.39, average training loss: 9496.21, base loss: 14441.46
[INFO 2017-06-28 14:26:42,319 main.py:51] epoch 8346, training loss: 9130.69, average training loss: 9496.12, base loss: 14440.59
[INFO 2017-06-28 14:26:43,359 main.py:51] epoch 8347, training loss: 11116.48, average training loss: 9496.78, base loss: 14441.90
[INFO 2017-06-28 14:26:44,453 main.py:51] epoch 8348, training loss: 10322.69, average training loss: 9497.06, base loss: 14442.63
[INFO 2017-06-28 14:26:45,452 main.py:51] epoch 8349, training loss: 9539.21, average training loss: 9497.43, base loss: 14443.10
[INFO 2017-06-28 14:26:46,498 main.py:51] epoch 8350, training loss: 9895.41, average training loss: 9498.33, base loss: 14445.02
[INFO 2017-06-28 14:26:47,544 main.py:51] epoch 8351, training loss: 8254.86, average training loss: 9496.97, base loss: 14442.38
[INFO 2017-06-28 14:26:48,559 main.py:51] epoch 8352, training loss: 8664.08, average training loss: 9496.44, base loss: 14442.21
[INFO 2017-06-28 14:26:49,613 main.py:51] epoch 8353, training loss: 10708.80, average training loss: 9498.51, base loss: 14447.56
[INFO 2017-06-28 14:26:50,654 main.py:51] epoch 8354, training loss: 9207.77, average training loss: 9497.58, base loss: 14446.23
[INFO 2017-06-28 14:26:51,744 main.py:51] epoch 8355, training loss: 9793.15, average training loss: 9498.64, base loss: 14447.10
[INFO 2017-06-28 14:26:52,846 main.py:51] epoch 8356, training loss: 7771.74, average training loss: 9496.17, base loss: 14445.16
[INFO 2017-06-28 14:26:53,914 main.py:51] epoch 8357, training loss: 9902.90, average training loss: 9497.40, base loss: 14446.54
[INFO 2017-06-28 14:26:55,001 main.py:51] epoch 8358, training loss: 9818.58, average training loss: 9497.94, base loss: 14447.34
[INFO 2017-06-28 14:26:56,052 main.py:51] epoch 8359, training loss: 8358.12, average training loss: 9496.54, base loss: 14444.85
[INFO 2017-06-28 14:26:57,103 main.py:51] epoch 8360, training loss: 9164.42, average training loss: 9496.83, base loss: 14446.08
[INFO 2017-06-28 14:26:58,162 main.py:51] epoch 8361, training loss: 9869.81, average training loss: 9497.20, base loss: 14446.42
[INFO 2017-06-28 14:26:59,177 main.py:51] epoch 8362, training loss: 9553.50, average training loss: 9497.80, base loss: 14446.92
[INFO 2017-06-28 14:27:00,186 main.py:51] epoch 8363, training loss: 9243.06, average training loss: 9497.02, base loss: 14447.72
[INFO 2017-06-28 14:27:01,218 main.py:51] epoch 8364, training loss: 9161.54, average training loss: 9496.69, base loss: 14447.17
[INFO 2017-06-28 14:27:02,292 main.py:51] epoch 8365, training loss: 8445.92, average training loss: 9495.61, base loss: 14445.17
[INFO 2017-06-28 14:27:03,372 main.py:51] epoch 8366, training loss: 8324.28, average training loss: 9494.14, base loss: 14443.72
[INFO 2017-06-28 14:27:04,359 main.py:51] epoch 8367, training loss: 8141.21, average training loss: 9492.23, base loss: 14440.66
[INFO 2017-06-28 14:27:05,417 main.py:51] epoch 8368, training loss: 9459.46, average training loss: 9492.85, base loss: 14440.70
[INFO 2017-06-28 14:27:06,517 main.py:51] epoch 8369, training loss: 9045.15, average training loss: 9492.05, base loss: 14438.96
[INFO 2017-06-28 14:27:07,565 main.py:51] epoch 8370, training loss: 8549.64, average training loss: 9490.41, base loss: 14436.91
[INFO 2017-06-28 14:27:08,643 main.py:51] epoch 8371, training loss: 9745.48, average training loss: 9490.63, base loss: 14439.00
[INFO 2017-06-28 14:27:09,662 main.py:51] epoch 8372, training loss: 8599.91, average training loss: 9489.45, base loss: 14436.68
[INFO 2017-06-28 14:27:10,690 main.py:51] epoch 8373, training loss: 9227.54, average training loss: 9488.98, base loss: 14436.36
[INFO 2017-06-28 14:27:11,762 main.py:51] epoch 8374, training loss: 9026.12, average training loss: 9487.79, base loss: 14433.85
[INFO 2017-06-28 14:27:12,800 main.py:51] epoch 8375, training loss: 8663.58, average training loss: 9485.87, base loss: 14429.76
[INFO 2017-06-28 14:27:13,904 main.py:51] epoch 8376, training loss: 8925.15, average training loss: 9485.90, base loss: 14429.28
[INFO 2017-06-28 14:27:14,878 main.py:51] epoch 8377, training loss: 8550.43, average training loss: 9485.36, base loss: 14429.59
[INFO 2017-06-28 14:27:15,878 main.py:51] epoch 8378, training loss: 8426.33, average training loss: 9484.90, base loss: 14427.99
[INFO 2017-06-28 14:27:16,855 main.py:51] epoch 8379, training loss: 8036.79, average training loss: 9482.81, base loss: 14425.35
[INFO 2017-06-28 14:27:17,870 main.py:51] epoch 8380, training loss: 10836.20, average training loss: 9483.34, base loss: 14426.88
[INFO 2017-06-28 14:27:18,908 main.py:51] epoch 8381, training loss: 9656.27, average training loss: 9482.84, base loss: 14426.96
[INFO 2017-06-28 14:27:19,960 main.py:51] epoch 8382, training loss: 8905.27, average training loss: 9481.66, base loss: 14427.21
[INFO 2017-06-28 14:27:20,941 main.py:51] epoch 8383, training loss: 9412.73, average training loss: 9481.75, base loss: 14427.89
[INFO 2017-06-28 14:27:21,992 main.py:51] epoch 8384, training loss: 8559.11, average training loss: 9480.08, base loss: 14425.39
[INFO 2017-06-28 14:27:23,038 main.py:51] epoch 8385, training loss: 9931.57, average training loss: 9481.41, base loss: 14428.08
[INFO 2017-06-28 14:27:24,063 main.py:51] epoch 8386, training loss: 9181.34, average training loss: 9481.65, base loss: 14428.43
[INFO 2017-06-28 14:27:25,112 main.py:51] epoch 8387, training loss: 8526.02, average training loss: 9479.91, base loss: 14424.02
[INFO 2017-06-28 14:27:26,145 main.py:51] epoch 8388, training loss: 9715.86, average training loss: 9480.54, base loss: 14426.09
[INFO 2017-06-28 14:27:27,226 main.py:51] epoch 8389, training loss: 9907.17, average training loss: 9482.13, base loss: 14430.20
[INFO 2017-06-28 14:27:28,275 main.py:51] epoch 8390, training loss: 9702.41, average training loss: 9482.82, base loss: 14432.84
[INFO 2017-06-28 14:27:29,261 main.py:51] epoch 8391, training loss: 8973.82, average training loss: 9482.48, base loss: 14430.92
[INFO 2017-06-28 14:27:30,323 main.py:51] epoch 8392, training loss: 8614.74, average training loss: 9482.04, base loss: 14430.58
[INFO 2017-06-28 14:27:31,358 main.py:51] epoch 8393, training loss: 8825.10, average training loss: 9480.07, base loss: 14428.77
[INFO 2017-06-28 14:27:32,359 main.py:51] epoch 8394, training loss: 10442.67, average training loss: 9481.22, base loss: 14430.21
[INFO 2017-06-28 14:27:33,437 main.py:51] epoch 8395, training loss: 8468.46, average training loss: 9480.24, base loss: 14430.23
[INFO 2017-06-28 14:27:34,448 main.py:51] epoch 8396, training loss: 8430.93, average training loss: 9479.23, base loss: 14427.36
[INFO 2017-06-28 14:27:35,538 main.py:51] epoch 8397, training loss: 8875.62, average training loss: 9478.90, base loss: 14426.50
[INFO 2017-06-28 14:27:36,666 main.py:51] epoch 8398, training loss: 9028.13, average training loss: 9478.74, base loss: 14425.57
[INFO 2017-06-28 14:27:37,643 main.py:51] epoch 8399, training loss: 9351.48, average training loss: 9478.84, base loss: 14427.67
[INFO 2017-06-28 14:27:37,643 main.py:53] epoch 8399, testing
[INFO 2017-06-28 14:27:41,344 main.py:105] average testing loss: 10995.31, base loss: 15383.21
[INFO 2017-06-28 14:27:41,344 main.py:106] improve_loss: 4387.89, improve_percent: 0.29
[INFO 2017-06-28 14:27:41,345 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:27:42,371 main.py:51] epoch 8400, training loss: 10143.58, average training loss: 9479.29, base loss: 14429.34
[INFO 2017-06-28 14:27:43,403 main.py:51] epoch 8401, training loss: 9593.85, average training loss: 9478.67, base loss: 14427.60
[INFO 2017-06-28 14:27:44,501 main.py:51] epoch 8402, training loss: 8127.35, average training loss: 9475.46, base loss: 14422.23
[INFO 2017-06-28 14:27:45,503 main.py:51] epoch 8403, training loss: 9142.41, average training loss: 9474.92, base loss: 14422.86
[INFO 2017-06-28 14:27:46,557 main.py:51] epoch 8404, training loss: 9873.21, average training loss: 9473.45, base loss: 14421.32
[INFO 2017-06-28 14:27:47,602 main.py:51] epoch 8405, training loss: 7969.39, average training loss: 9472.89, base loss: 14420.62
[INFO 2017-06-28 14:27:48,701 main.py:51] epoch 8406, training loss: 8958.51, average training loss: 9472.38, base loss: 14419.23
[INFO 2017-06-28 14:27:49,792 main.py:51] epoch 8407, training loss: 8527.69, average training loss: 9470.72, base loss: 14417.29
[INFO 2017-06-28 14:27:50,914 main.py:51] epoch 8408, training loss: 10814.52, average training loss: 9472.61, base loss: 14420.30
[INFO 2017-06-28 14:27:52,021 main.py:51] epoch 8409, training loss: 8102.86, average training loss: 9471.32, base loss: 14419.35
[INFO 2017-06-28 14:27:53,076 main.py:51] epoch 8410, training loss: 9888.68, average training loss: 9472.28, base loss: 14420.20
[INFO 2017-06-28 14:27:54,168 main.py:51] epoch 8411, training loss: 9717.97, average training loss: 9471.10, base loss: 14417.31
[INFO 2017-06-28 14:27:55,173 main.py:51] epoch 8412, training loss: 8669.21, average training loss: 9469.22, base loss: 14414.68
[INFO 2017-06-28 14:27:56,266 main.py:51] epoch 8413, training loss: 10501.19, average training loss: 9470.05, base loss: 14416.83
[INFO 2017-06-28 14:27:57,330 main.py:51] epoch 8414, training loss: 9321.77, average training loss: 9469.60, base loss: 14415.96
[INFO 2017-06-28 14:27:58,375 main.py:51] epoch 8415, training loss: 10629.44, average training loss: 9470.95, base loss: 14417.81
[INFO 2017-06-28 14:27:59,399 main.py:51] epoch 8416, training loss: 8898.24, average training loss: 9469.97, base loss: 14416.47
[INFO 2017-06-28 14:28:00,478 main.py:51] epoch 8417, training loss: 9269.45, average training loss: 9469.23, base loss: 14414.82
[INFO 2017-06-28 14:28:01,584 main.py:51] epoch 8418, training loss: 7357.42, average training loss: 9466.43, base loss: 14410.32
[INFO 2017-06-28 14:28:02,666 main.py:51] epoch 8419, training loss: 10828.55, average training loss: 9467.28, base loss: 14410.54
[INFO 2017-06-28 14:28:03,657 main.py:51] epoch 8420, training loss: 9053.82, average training loss: 9466.77, base loss: 14411.19
[INFO 2017-06-28 14:28:04,718 main.py:51] epoch 8421, training loss: 9275.84, average training loss: 9465.99, base loss: 14409.98
[INFO 2017-06-28 14:28:05,783 main.py:51] epoch 8422, training loss: 8645.26, average training loss: 9464.76, base loss: 14408.78
[INFO 2017-06-28 14:28:06,886 main.py:51] epoch 8423, training loss: 10131.01, average training loss: 9466.10, base loss: 14411.82
[INFO 2017-06-28 14:28:07,975 main.py:51] epoch 8424, training loss: 8666.71, average training loss: 9465.37, base loss: 14411.76
[INFO 2017-06-28 14:28:09,029 main.py:51] epoch 8425, training loss: 9079.93, average training loss: 9464.57, base loss: 14411.75
[INFO 2017-06-28 14:28:10,061 main.py:51] epoch 8426, training loss: 8745.76, average training loss: 9463.41, base loss: 14409.98
[INFO 2017-06-28 14:28:11,144 main.py:51] epoch 8427, training loss: 9682.72, average training loss: 9464.14, base loss: 14411.84
[INFO 2017-06-28 14:28:12,209 main.py:51] epoch 8428, training loss: 9727.72, average training loss: 9464.73, base loss: 14413.31
[INFO 2017-06-28 14:28:13,322 main.py:51] epoch 8429, training loss: 10621.00, average training loss: 9465.95, base loss: 14414.28
[INFO 2017-06-28 14:28:14,385 main.py:51] epoch 8430, training loss: 9075.51, average training loss: 9464.95, base loss: 14412.69
[INFO 2017-06-28 14:28:15,446 main.py:51] epoch 8431, training loss: 8944.62, average training loss: 9464.43, base loss: 14411.64
[INFO 2017-06-28 14:28:16,478 main.py:51] epoch 8432, training loss: 8391.92, average training loss: 9462.78, base loss: 14408.19
[INFO 2017-06-28 14:28:17,560 main.py:51] epoch 8433, training loss: 8869.74, average training loss: 9462.80, base loss: 14409.06
[INFO 2017-06-28 14:28:18,653 main.py:51] epoch 8434, training loss: 9044.99, average training loss: 9461.69, base loss: 14406.75
[INFO 2017-06-28 14:28:19,641 main.py:51] epoch 8435, training loss: 9103.47, average training loss: 9461.15, base loss: 14405.73
[INFO 2017-06-28 14:28:20,662 main.py:51] epoch 8436, training loss: 8593.72, average training loss: 9460.22, base loss: 14404.73
[INFO 2017-06-28 14:28:21,729 main.py:51] epoch 8437, training loss: 9078.50, average training loss: 9460.22, base loss: 14403.89
[INFO 2017-06-28 14:28:22,810 main.py:51] epoch 8438, training loss: 9039.99, average training loss: 9460.37, base loss: 14403.79
[INFO 2017-06-28 14:28:23,912 main.py:51] epoch 8439, training loss: 9382.43, average training loss: 9460.24, base loss: 14402.99
[INFO 2017-06-28 14:28:24,984 main.py:51] epoch 8440, training loss: 9989.83, average training loss: 9460.73, base loss: 14402.40
[INFO 2017-06-28 14:28:26,006 main.py:51] epoch 8441, training loss: 10814.40, average training loss: 9462.36, base loss: 14404.51
[INFO 2017-06-28 14:28:27,049 main.py:51] epoch 8442, training loss: 9671.83, average training loss: 9462.52, base loss: 14404.31
[INFO 2017-06-28 14:28:28,139 main.py:51] epoch 8443, training loss: 8780.23, average training loss: 9461.77, base loss: 14403.11
[INFO 2017-06-28 14:28:29,226 main.py:51] epoch 8444, training loss: 9684.33, average training loss: 9460.66, base loss: 14400.95
[INFO 2017-06-28 14:28:30,228 main.py:51] epoch 8445, training loss: 10177.69, average training loss: 9460.63, base loss: 14402.61
[INFO 2017-06-28 14:28:31,284 main.py:51] epoch 8446, training loss: 9863.72, average training loss: 9460.99, base loss: 14403.83
[INFO 2017-06-28 14:28:32,372 main.py:51] epoch 8447, training loss: 9796.96, average training loss: 9462.37, base loss: 14406.41
[INFO 2017-06-28 14:28:33,305 main.py:51] epoch 8448, training loss: 11176.51, average training loss: 9463.91, base loss: 14406.85
[INFO 2017-06-28 14:28:34,225 main.py:51] epoch 8449, training loss: 9959.47, average training loss: 9464.82, base loss: 14409.34
[INFO 2017-06-28 14:28:35,110 main.py:51] epoch 8450, training loss: 9041.03, average training loss: 9465.38, base loss: 14410.83
[INFO 2017-06-28 14:28:36,056 main.py:51] epoch 8451, training loss: 9222.76, average training loss: 9464.65, base loss: 14410.83
[INFO 2017-06-28 14:28:37,119 main.py:51] epoch 8452, training loss: 8224.92, average training loss: 9460.94, base loss: 14405.44
[INFO 2017-06-28 14:28:38,241 main.py:51] epoch 8453, training loss: 8913.56, average training loss: 9459.63, base loss: 14404.72
[INFO 2017-06-28 14:28:39,243 main.py:51] epoch 8454, training loss: 9148.79, average training loss: 9458.57, base loss: 14400.62
[INFO 2017-06-28 14:28:40,305 main.py:51] epoch 8455, training loss: 9394.39, average training loss: 9458.76, base loss: 14400.58
[INFO 2017-06-28 14:28:41,380 main.py:51] epoch 8456, training loss: 9090.83, average training loss: 9457.50, base loss: 14398.55
[INFO 2017-06-28 14:28:42,459 main.py:51] epoch 8457, training loss: 9588.05, average training loss: 9456.80, base loss: 14398.65
[INFO 2017-06-28 14:28:43,548 main.py:51] epoch 8458, training loss: 9648.20, average training loss: 9458.06, base loss: 14400.06
[INFO 2017-06-28 14:28:44,584 main.py:51] epoch 8459, training loss: 8568.71, average training loss: 9458.64, base loss: 14401.41
[INFO 2017-06-28 14:28:45,627 main.py:51] epoch 8460, training loss: 10726.88, average training loss: 9459.50, base loss: 14403.49
[INFO 2017-06-28 14:28:46,714 main.py:51] epoch 8461, training loss: 9095.10, average training loss: 9459.87, base loss: 14403.36
[INFO 2017-06-28 14:28:47,791 main.py:51] epoch 8462, training loss: 10107.55, average training loss: 9460.10, base loss: 14401.79
[INFO 2017-06-28 14:28:48,857 main.py:51] epoch 8463, training loss: 7891.54, average training loss: 9459.87, base loss: 14400.27
[INFO 2017-06-28 14:28:49,890 main.py:51] epoch 8464, training loss: 9937.27, average training loss: 9460.23, base loss: 14398.45
[INFO 2017-06-28 14:28:50,970 main.py:51] epoch 8465, training loss: 9391.67, average training loss: 9461.37, base loss: 14400.94
[INFO 2017-06-28 14:28:52,093 main.py:51] epoch 8466, training loss: 8112.35, average training loss: 9459.67, base loss: 14396.73
[INFO 2017-06-28 14:28:53,122 main.py:51] epoch 8467, training loss: 9788.91, average training loss: 9459.41, base loss: 14395.31
[INFO 2017-06-28 14:28:54,181 main.py:51] epoch 8468, training loss: 9133.23, average training loss: 9457.82, base loss: 14392.35
[INFO 2017-06-28 14:28:55,230 main.py:51] epoch 8469, training loss: 9812.60, average training loss: 9459.63, base loss: 14394.94
[INFO 2017-06-28 14:28:56,291 main.py:51] epoch 8470, training loss: 9056.93, average training loss: 9458.49, base loss: 14391.91
[INFO 2017-06-28 14:28:57,394 main.py:51] epoch 8471, training loss: 8347.07, average training loss: 9456.66, base loss: 14389.61
[INFO 2017-06-28 14:28:58,438 main.py:51] epoch 8472, training loss: 8970.16, average training loss: 9456.18, base loss: 14388.49
[INFO 2017-06-28 14:28:59,483 main.py:51] epoch 8473, training loss: 8687.00, average training loss: 9454.46, base loss: 14385.97
[INFO 2017-06-28 14:29:00,521 main.py:51] epoch 8474, training loss: 8629.57, average training loss: 9453.74, base loss: 14385.02
[INFO 2017-06-28 14:29:01,576 main.py:51] epoch 8475, training loss: 9362.22, average training loss: 9453.65, base loss: 14383.90
[INFO 2017-06-28 14:29:02,657 main.py:51] epoch 8476, training loss: 9752.61, average training loss: 9454.21, base loss: 14384.15
[INFO 2017-06-28 14:29:03,682 main.py:51] epoch 8477, training loss: 8544.12, average training loss: 9453.85, base loss: 14383.65
[INFO 2017-06-28 14:29:04,757 main.py:51] epoch 8478, training loss: 9027.12, average training loss: 9452.52, base loss: 14381.61
[INFO 2017-06-28 14:29:05,818 main.py:51] epoch 8479, training loss: 8132.21, average training loss: 9450.93, base loss: 14378.93
[INFO 2017-06-28 14:29:06,856 main.py:51] epoch 8480, training loss: 8492.22, average training loss: 9450.81, base loss: 14377.42
[INFO 2017-06-28 14:29:07,897 main.py:51] epoch 8481, training loss: 10937.06, average training loss: 9453.46, base loss: 14381.94
[INFO 2017-06-28 14:29:08,966 main.py:51] epoch 8482, training loss: 9910.73, average training loss: 9454.20, base loss: 14384.43
[INFO 2017-06-28 14:29:10,020 main.py:51] epoch 8483, training loss: 8795.98, average training loss: 9453.58, base loss: 14382.83
[INFO 2017-06-28 14:29:11,048 main.py:51] epoch 8484, training loss: 9719.21, average training loss: 9455.07, base loss: 14385.99
[INFO 2017-06-28 14:29:12,142 main.py:51] epoch 8485, training loss: 10018.92, average training loss: 9455.68, base loss: 14386.03
[INFO 2017-06-28 14:29:13,181 main.py:51] epoch 8486, training loss: 9619.63, average training loss: 9454.13, base loss: 14384.74
[INFO 2017-06-28 14:29:14,249 main.py:51] epoch 8487, training loss: 8989.65, average training loss: 9454.17, base loss: 14385.90
[INFO 2017-06-28 14:29:15,337 main.py:51] epoch 8488, training loss: 9963.85, average training loss: 9455.49, base loss: 14388.84
[INFO 2017-06-28 14:29:16,432 main.py:51] epoch 8489, training loss: 9886.68, average training loss: 9455.03, base loss: 14386.64
[INFO 2017-06-28 14:29:17,548 main.py:51] epoch 8490, training loss: 10019.24, average training loss: 9455.89, base loss: 14387.58
[INFO 2017-06-28 14:29:18,497 main.py:51] epoch 8491, training loss: 9516.13, average training loss: 9456.13, base loss: 14389.11
[INFO 2017-06-28 14:29:19,560 main.py:51] epoch 8492, training loss: 11056.28, average training loss: 9459.23, base loss: 14393.73
[INFO 2017-06-28 14:29:20,666 main.py:51] epoch 8493, training loss: 9470.97, average training loss: 9459.31, base loss: 14394.24
[INFO 2017-06-28 14:29:21,706 main.py:51] epoch 8494, training loss: 9054.55, average training loss: 9458.91, base loss: 14392.91
[INFO 2017-06-28 14:29:22,793 main.py:51] epoch 8495, training loss: 9429.50, average training loss: 9459.29, base loss: 14394.91
[INFO 2017-06-28 14:29:23,783 main.py:51] epoch 8496, training loss: 10508.86, average training loss: 9461.20, base loss: 14398.59
[INFO 2017-06-28 14:29:24,816 main.py:51] epoch 8497, training loss: 9913.71, average training loss: 9463.93, base loss: 14403.26
[INFO 2017-06-28 14:29:25,873 main.py:51] epoch 8498, training loss: 9894.55, average training loss: 9463.58, base loss: 14402.99
[INFO 2017-06-28 14:29:26,912 main.py:51] epoch 8499, training loss: 8518.24, average training loss: 9462.68, base loss: 14401.46
[INFO 2017-06-28 14:29:26,912 main.py:53] epoch 8499, testing
[INFO 2017-06-28 14:29:30,721 main.py:105] average testing loss: 10619.11, base loss: 14791.15
[INFO 2017-06-28 14:29:30,722 main.py:106] improve_loss: 4172.04, improve_percent: 0.28
[INFO 2017-06-28 14:29:30,722 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:29:31,808 main.py:51] epoch 8500, training loss: 9152.38, average training loss: 9463.03, base loss: 14401.13
[INFO 2017-06-28 14:29:32,880 main.py:51] epoch 8501, training loss: 9474.10, average training loss: 9463.12, base loss: 14402.09
[INFO 2017-06-28 14:29:33,950 main.py:51] epoch 8502, training loss: 8577.37, average training loss: 9463.06, base loss: 14403.42
[INFO 2017-06-28 14:29:34,977 main.py:51] epoch 8503, training loss: 10700.24, average training loss: 9464.82, base loss: 14407.11
[INFO 2017-06-28 14:29:36,055 main.py:51] epoch 8504, training loss: 8880.08, average training loss: 9463.19, base loss: 14403.71
[INFO 2017-06-28 14:29:37,138 main.py:51] epoch 8505, training loss: 9927.83, average training loss: 9463.96, base loss: 14405.19
[INFO 2017-06-28 14:29:38,236 main.py:51] epoch 8506, training loss: 9321.32, average training loss: 9463.49, base loss: 14404.19
[INFO 2017-06-28 14:29:39,269 main.py:51] epoch 8507, training loss: 9466.04, average training loss: 9462.89, base loss: 14404.28
[INFO 2017-06-28 14:29:40,304 main.py:51] epoch 8508, training loss: 8551.59, average training loss: 9462.34, base loss: 14404.15
[INFO 2017-06-28 14:29:41,381 main.py:51] epoch 8509, training loss: 10441.16, average training loss: 9462.88, base loss: 14405.12
[INFO 2017-06-28 14:29:42,516 main.py:51] epoch 8510, training loss: 11050.21, average training loss: 9464.03, base loss: 14406.73
[INFO 2017-06-28 14:29:43,602 main.py:51] epoch 8511, training loss: 9968.16, average training loss: 9462.84, base loss: 14406.79
[INFO 2017-06-28 14:29:44,706 main.py:51] epoch 8512, training loss: 10609.17, average training loss: 9463.62, base loss: 14407.04
[INFO 2017-06-28 14:29:45,688 main.py:51] epoch 8513, training loss: 8645.42, average training loss: 9464.13, base loss: 14408.53
[INFO 2017-06-28 14:29:46,734 main.py:51] epoch 8514, training loss: 9969.38, average training loss: 9463.50, base loss: 14408.34
[INFO 2017-06-28 14:29:47,817 main.py:51] epoch 8515, training loss: 11961.05, average training loss: 9466.40, base loss: 14410.82
[INFO 2017-06-28 14:29:48,802 main.py:51] epoch 8516, training loss: 10987.00, average training loss: 9467.89, base loss: 14413.55
[INFO 2017-06-28 14:29:49,874 main.py:51] epoch 8517, training loss: 9807.53, average training loss: 9464.73, base loss: 14408.86
[INFO 2017-06-28 14:29:50,937 main.py:51] epoch 8518, training loss: 9450.82, average training loss: 9464.32, base loss: 14408.73
[INFO 2017-06-28 14:29:51,897 main.py:51] epoch 8519, training loss: 9608.90, average training loss: 9464.73, base loss: 14408.69
[INFO 2017-06-28 14:29:52,884 main.py:51] epoch 8520, training loss: 9150.40, average training loss: 9464.09, base loss: 14408.37
[INFO 2017-06-28 14:29:53,841 main.py:51] epoch 8521, training loss: 10212.42, average training loss: 9463.97, base loss: 14409.40
[INFO 2017-06-28 14:29:54,904 main.py:51] epoch 8522, training loss: 9520.73, average training loss: 9464.78, base loss: 14410.39
[INFO 2017-06-28 14:29:55,974 main.py:51] epoch 8523, training loss: 9300.24, average training loss: 9464.64, base loss: 14409.76
[INFO 2017-06-28 14:29:56,970 main.py:51] epoch 8524, training loss: 9111.40, average training loss: 9462.87, base loss: 14408.41
[INFO 2017-06-28 14:29:58,042 main.py:51] epoch 8525, training loss: 9373.00, average training loss: 9462.41, base loss: 14407.89
[INFO 2017-06-28 14:29:59,099 main.py:51] epoch 8526, training loss: 9501.84, average training loss: 9463.14, base loss: 14410.29
[INFO 2017-06-28 14:30:00,176 main.py:51] epoch 8527, training loss: 8422.35, average training loss: 9461.77, base loss: 14408.47
[INFO 2017-06-28 14:30:01,268 main.py:51] epoch 8528, training loss: 10146.26, average training loss: 9462.19, base loss: 14407.47
[INFO 2017-06-28 14:30:02,359 main.py:51] epoch 8529, training loss: 8851.00, average training loss: 9462.72, base loss: 14408.01
[INFO 2017-06-28 14:30:03,445 main.py:51] epoch 8530, training loss: 9331.49, average training loss: 9461.35, base loss: 14405.79
[INFO 2017-06-28 14:30:04,463 main.py:51] epoch 8531, training loss: 8122.00, average training loss: 9459.78, base loss: 14402.97
[INFO 2017-06-28 14:30:05,517 main.py:51] epoch 8532, training loss: 10932.25, average training loss: 9461.05, base loss: 14405.76
[INFO 2017-06-28 14:30:06,601 main.py:51] epoch 8533, training loss: 9634.83, average training loss: 9461.96, base loss: 14407.30
[INFO 2017-06-28 14:30:07,696 main.py:51] epoch 8534, training loss: 8113.57, average training loss: 9460.42, base loss: 14404.42
[INFO 2017-06-28 14:30:08,805 main.py:51] epoch 8535, training loss: 9718.04, average training loss: 9460.91, base loss: 14404.94
[INFO 2017-06-28 14:30:09,867 main.py:51] epoch 8536, training loss: 9580.21, average training loss: 9460.15, base loss: 14404.13
[INFO 2017-06-28 14:30:10,945 main.py:51] epoch 8537, training loss: 9436.29, average training loss: 9460.76, base loss: 14405.95
[INFO 2017-06-28 14:30:11,931 main.py:51] epoch 8538, training loss: 9767.82, average training loss: 9461.76, base loss: 14406.43
[INFO 2017-06-28 14:30:12,991 main.py:51] epoch 8539, training loss: 9872.54, average training loss: 9462.09, base loss: 14406.86
[INFO 2017-06-28 14:30:14,066 main.py:51] epoch 8540, training loss: 8908.96, average training loss: 9460.98, base loss: 14407.02
[INFO 2017-06-28 14:30:15,139 main.py:51] epoch 8541, training loss: 9007.72, average training loss: 9460.08, base loss: 14407.37
[INFO 2017-06-28 14:30:16,195 main.py:51] epoch 8542, training loss: 8540.18, average training loss: 9458.37, base loss: 14405.95
[INFO 2017-06-28 14:30:17,217 main.py:51] epoch 8543, training loss: 10047.20, average training loss: 9458.68, base loss: 14404.56
[INFO 2017-06-28 14:30:18,286 main.py:51] epoch 8544, training loss: 9229.38, average training loss: 9458.28, base loss: 14404.96
[INFO 2017-06-28 14:30:19,380 main.py:51] epoch 8545, training loss: 11784.92, average training loss: 9460.89, base loss: 14408.12
[INFO 2017-06-28 14:30:20,461 main.py:51] epoch 8546, training loss: 8647.31, average training loss: 9460.75, base loss: 14407.93
[INFO 2017-06-28 14:30:21,561 main.py:51] epoch 8547, training loss: 9300.57, average training loss: 9461.42, base loss: 14410.09
[INFO 2017-06-28 14:30:22,604 main.py:51] epoch 8548, training loss: 8434.65, average training loss: 9460.66, base loss: 14409.06
[INFO 2017-06-28 14:30:23,634 main.py:51] epoch 8549, training loss: 9955.53, average training loss: 9461.12, base loss: 14409.10
[INFO 2017-06-28 14:30:24,697 main.py:51] epoch 8550, training loss: 8130.98, average training loss: 9460.06, base loss: 14405.77
[INFO 2017-06-28 14:30:25,774 main.py:51] epoch 8551, training loss: 9516.89, average training loss: 9458.66, base loss: 14403.77
[INFO 2017-06-28 14:30:26,883 main.py:51] epoch 8552, training loss: 9370.87, average training loss: 9457.13, base loss: 14402.10
[INFO 2017-06-28 14:30:27,927 main.py:51] epoch 8553, training loss: 9563.11, average training loss: 9456.67, base loss: 14401.92
[INFO 2017-06-28 14:30:28,952 main.py:51] epoch 8554, training loss: 9720.38, average training loss: 9456.16, base loss: 14400.96
[INFO 2017-06-28 14:30:30,028 main.py:51] epoch 8555, training loss: 8454.82, average training loss: 9456.97, base loss: 14401.50
[INFO 2017-06-28 14:30:31,032 main.py:51] epoch 8556, training loss: 10977.10, average training loss: 9459.05, base loss: 14403.33
[INFO 2017-06-28 14:30:32,076 main.py:51] epoch 8557, training loss: 8284.87, average training loss: 9458.24, base loss: 14401.49
[INFO 2017-06-28 14:30:33,133 main.py:51] epoch 8558, training loss: 12034.41, average training loss: 9460.38, base loss: 14406.66
[INFO 2017-06-28 14:30:34,247 main.py:51] epoch 8559, training loss: 9655.27, average training loss: 9460.38, base loss: 14407.39
[INFO 2017-06-28 14:30:35,339 main.py:51] epoch 8560, training loss: 8881.60, average training loss: 9459.53, base loss: 14406.42
[INFO 2017-06-28 14:30:36,364 main.py:51] epoch 8561, training loss: 8592.22, average training loss: 9457.48, base loss: 14404.87
[INFO 2017-06-28 14:30:37,371 main.py:51] epoch 8562, training loss: 8987.96, average training loss: 9456.98, base loss: 14403.83
[INFO 2017-06-28 14:30:38,442 main.py:51] epoch 8563, training loss: 10795.33, average training loss: 9458.04, base loss: 14406.56
[INFO 2017-06-28 14:30:39,532 main.py:51] epoch 8564, training loss: 9040.58, average training loss: 9458.45, base loss: 14407.40
[INFO 2017-06-28 14:30:40,587 main.py:51] epoch 8565, training loss: 8120.29, average training loss: 9454.93, base loss: 14401.96
[INFO 2017-06-28 14:30:41,650 main.py:51] epoch 8566, training loss: 9244.11, average training loss: 9454.89, base loss: 14401.59
[INFO 2017-06-28 14:30:42,713 main.py:51] epoch 8567, training loss: 9520.78, average training loss: 9455.22, base loss: 14401.71
[INFO 2017-06-28 14:30:43,735 main.py:51] epoch 8568, training loss: 9644.12, average training loss: 9455.12, base loss: 14401.49
[INFO 2017-06-28 14:30:44,812 main.py:51] epoch 8569, training loss: 10067.04, average training loss: 9454.44, base loss: 14398.82
[INFO 2017-06-28 14:30:45,921 main.py:51] epoch 8570, training loss: 8675.72, average training loss: 9454.53, base loss: 14398.66
[INFO 2017-06-28 14:30:47,006 main.py:51] epoch 8571, training loss: 9418.69, average training loss: 9455.74, base loss: 14401.69
[INFO 2017-06-28 14:30:48,138 main.py:51] epoch 8572, training loss: 8959.82, average training loss: 9456.15, base loss: 14402.01
[INFO 2017-06-28 14:30:49,191 main.py:51] epoch 8573, training loss: 8469.74, average training loss: 9454.71, base loss: 14398.00
[INFO 2017-06-28 14:30:50,289 main.py:51] epoch 8574, training loss: 8816.26, average training loss: 9455.16, base loss: 14400.38
[INFO 2017-06-28 14:30:51,317 main.py:51] epoch 8575, training loss: 9196.96, average training loss: 9454.68, base loss: 14398.08
[INFO 2017-06-28 14:30:52,361 main.py:51] epoch 8576, training loss: 8624.25, average training loss: 9453.66, base loss: 14397.51
[INFO 2017-06-28 14:30:53,454 main.py:51] epoch 8577, training loss: 8846.94, average training loss: 9452.92, base loss: 14396.76
[INFO 2017-06-28 14:30:54,469 main.py:51] epoch 8578, training loss: 10500.58, average training loss: 9453.63, base loss: 14397.54
[INFO 2017-06-28 14:30:55,480 main.py:51] epoch 8579, training loss: 9420.67, average training loss: 9454.15, base loss: 14397.82
[INFO 2017-06-28 14:30:56,527 main.py:51] epoch 8580, training loss: 9171.61, average training loss: 9453.94, base loss: 14397.38
[INFO 2017-06-28 14:30:57,588 main.py:51] epoch 8581, training loss: 8554.04, average training loss: 9452.99, base loss: 14395.49
[INFO 2017-06-28 14:30:58,637 main.py:51] epoch 8582, training loss: 9218.61, average training loss: 9452.82, base loss: 14394.29
[INFO 2017-06-28 14:30:59,687 main.py:51] epoch 8583, training loss: 9480.53, average training loss: 9451.16, base loss: 14392.78
[INFO 2017-06-28 14:31:00,772 main.py:51] epoch 8584, training loss: 9832.98, average training loss: 9451.60, base loss: 14394.20
[INFO 2017-06-28 14:31:01,890 main.py:51] epoch 8585, training loss: 9697.89, average training loss: 9451.54, base loss: 14395.31
[INFO 2017-06-28 14:31:02,929 main.py:51] epoch 8586, training loss: 10552.52, average training loss: 9452.58, base loss: 14397.51
[INFO 2017-06-28 14:31:03,983 main.py:51] epoch 8587, training loss: 8244.58, average training loss: 9449.66, base loss: 14391.69
[INFO 2017-06-28 14:31:05,005 main.py:51] epoch 8588, training loss: 9054.14, average training loss: 9449.04, base loss: 14392.18
[INFO 2017-06-28 14:31:06,093 main.py:51] epoch 8589, training loss: 7963.72, average training loss: 9448.51, base loss: 14390.94
[INFO 2017-06-28 14:31:07,135 main.py:51] epoch 8590, training loss: 8920.96, average training loss: 9447.02, base loss: 14388.84
[INFO 2017-06-28 14:31:08,195 main.py:51] epoch 8591, training loss: 8929.62, average training loss: 9446.35, base loss: 14388.31
[INFO 2017-06-28 14:31:09,275 main.py:51] epoch 8592, training loss: 9964.00, average training loss: 9447.57, base loss: 14390.20
[INFO 2017-06-28 14:31:10,215 main.py:51] epoch 8593, training loss: 9192.85, average training loss: 9448.12, base loss: 14390.67
[INFO 2017-06-28 14:31:11,190 main.py:51] epoch 8594, training loss: 10188.87, average training loss: 9447.95, base loss: 14388.05
[INFO 2017-06-28 14:31:12,128 main.py:51] epoch 8595, training loss: 8682.91, average training loss: 9447.66, base loss: 14388.69
[INFO 2017-06-28 14:31:13,151 main.py:51] epoch 8596, training loss: 8201.44, average training loss: 9446.09, base loss: 14385.25
[INFO 2017-06-28 14:31:14,238 main.py:51] epoch 8597, training loss: 8889.97, average training loss: 9445.91, base loss: 14385.54
[INFO 2017-06-28 14:31:15,322 main.py:51] epoch 8598, training loss: 9956.17, average training loss: 9446.87, base loss: 14388.03
[INFO 2017-06-28 14:31:16,335 main.py:51] epoch 8599, training loss: 9123.55, average training loss: 9446.74, base loss: 14389.07
[INFO 2017-06-28 14:31:16,335 main.py:53] epoch 8599, testing
[INFO 2017-06-28 14:31:20,083 main.py:105] average testing loss: 11283.74, base loss: 15727.05
[INFO 2017-06-28 14:31:20,083 main.py:106] improve_loss: 4443.30, improve_percent: 0.28
[INFO 2017-06-28 14:31:20,084 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:31:21,101 main.py:51] epoch 8600, training loss: 8325.12, average training loss: 9446.66, base loss: 14388.70
[INFO 2017-06-28 14:31:22,111 main.py:51] epoch 8601, training loss: 8307.27, average training loss: 9445.08, base loss: 14385.10
[INFO 2017-06-28 14:31:23,158 main.py:51] epoch 8602, training loss: 9943.68, average training loss: 9446.45, base loss: 14388.57
[INFO 2017-06-28 14:31:24,186 main.py:51] epoch 8603, training loss: 9627.48, average training loss: 9447.12, base loss: 14388.55
[INFO 2017-06-28 14:31:25,210 main.py:51] epoch 8604, training loss: 9098.01, average training loss: 9447.50, base loss: 14390.30
[INFO 2017-06-28 14:31:26,234 main.py:51] epoch 8605, training loss: 7860.19, average training loss: 9444.83, base loss: 14386.24
[INFO 2017-06-28 14:31:27,291 main.py:51] epoch 8606, training loss: 8551.93, average training loss: 9442.98, base loss: 14382.98
[INFO 2017-06-28 14:31:28,404 main.py:51] epoch 8607, training loss: 8295.81, average training loss: 9441.95, base loss: 14380.96
[INFO 2017-06-28 14:31:29,452 main.py:51] epoch 8608, training loss: 9106.62, average training loss: 9441.48, base loss: 14379.78
[INFO 2017-06-28 14:31:30,497 main.py:51] epoch 8609, training loss: 9639.14, average training loss: 9439.69, base loss: 14378.18
[INFO 2017-06-28 14:31:31,549 main.py:51] epoch 8610, training loss: 9610.25, average training loss: 9438.46, base loss: 14376.97
[INFO 2017-06-28 14:31:32,560 main.py:51] epoch 8611, training loss: 9121.71, average training loss: 9437.79, base loss: 14376.37
[INFO 2017-06-28 14:31:33,607 main.py:51] epoch 8612, training loss: 9265.89, average training loss: 9437.94, base loss: 14376.26
[INFO 2017-06-28 14:31:34,709 main.py:51] epoch 8613, training loss: 9118.55, average training loss: 9438.34, base loss: 14376.09
[INFO 2017-06-28 14:31:35,754 main.py:51] epoch 8614, training loss: 9412.88, average training loss: 9438.22, base loss: 14375.90
[INFO 2017-06-28 14:31:36,789 main.py:51] epoch 8615, training loss: 9582.60, average training loss: 9438.36, base loss: 14376.57
[INFO 2017-06-28 14:31:37,865 main.py:51] epoch 8616, training loss: 9409.85, average training loss: 9437.10, base loss: 14374.81
[INFO 2017-06-28 14:31:38,946 main.py:51] epoch 8617, training loss: 9257.67, average training loss: 9437.23, base loss: 14374.57
[INFO 2017-06-28 14:31:40,064 main.py:51] epoch 8618, training loss: 9709.98, average training loss: 9437.26, base loss: 14374.15
[INFO 2017-06-28 14:31:41,075 main.py:51] epoch 8619, training loss: 8134.67, average training loss: 9436.30, base loss: 14373.83
[INFO 2017-06-28 14:31:42,149 main.py:51] epoch 8620, training loss: 9895.14, average training loss: 9436.95, base loss: 14376.63
[INFO 2017-06-28 14:31:43,205 main.py:51] epoch 8621, training loss: 8154.48, average training loss: 9435.08, base loss: 14371.15
[INFO 2017-06-28 14:31:44,242 main.py:51] epoch 8622, training loss: 8658.82, average training loss: 9433.47, base loss: 14369.43
[INFO 2017-06-28 14:31:45,373 main.py:51] epoch 8623, training loss: 10402.82, average training loss: 9434.82, base loss: 14372.82
[INFO 2017-06-28 14:31:46,368 main.py:51] epoch 8624, training loss: 9051.79, average training loss: 9433.85, base loss: 14372.03
[INFO 2017-06-28 14:31:47,456 main.py:51] epoch 8625, training loss: 8350.37, average training loss: 9432.47, base loss: 14370.63
[INFO 2017-06-28 14:31:48,535 main.py:51] epoch 8626, training loss: 8838.67, average training loss: 9432.08, base loss: 14371.30
[INFO 2017-06-28 14:31:49,576 main.py:51] epoch 8627, training loss: 8911.48, average training loss: 9431.64, base loss: 14369.94
[INFO 2017-06-28 14:31:50,645 main.py:51] epoch 8628, training loss: 9778.16, average training loss: 9431.07, base loss: 14369.04
[INFO 2017-06-28 14:31:51,691 main.py:51] epoch 8629, training loss: 9170.22, average training loss: 9430.13, base loss: 14367.95
[INFO 2017-06-28 14:31:52,721 main.py:51] epoch 8630, training loss: 9831.07, average training loss: 9431.09, base loss: 14369.29
[INFO 2017-06-28 14:31:53,817 main.py:51] epoch 8631, training loss: 9513.93, average training loss: 9432.17, base loss: 14371.73
[INFO 2017-06-28 14:31:54,835 main.py:51] epoch 8632, training loss: 10831.85, average training loss: 9432.97, base loss: 14372.65
[INFO 2017-06-28 14:31:55,889 main.py:51] epoch 8633, training loss: 8261.78, average training loss: 9431.84, base loss: 14370.31
[INFO 2017-06-28 14:31:56,956 main.py:51] epoch 8634, training loss: 9844.62, average training loss: 9433.40, base loss: 14372.35
[INFO 2017-06-28 14:31:58,029 main.py:51] epoch 8635, training loss: 9893.00, average training loss: 9433.85, base loss: 14373.56
[INFO 2017-06-28 14:31:59,137 main.py:51] epoch 8636, training loss: 8497.64, average training loss: 9431.05, base loss: 14369.39
[INFO 2017-06-28 14:32:00,210 main.py:51] epoch 8637, training loss: 8996.47, average training loss: 9430.57, base loss: 14368.09
[INFO 2017-06-28 14:32:01,335 main.py:51] epoch 8638, training loss: 9688.36, average training loss: 9429.67, base loss: 14367.44
[INFO 2017-06-28 14:32:02,424 main.py:51] epoch 8639, training loss: 10650.04, average training loss: 9430.56, base loss: 14367.41
[INFO 2017-06-28 14:32:03,489 main.py:51] epoch 8640, training loss: 9129.33, average training loss: 9430.69, base loss: 14368.87
[INFO 2017-06-28 14:32:04,519 main.py:51] epoch 8641, training loss: 10470.76, average training loss: 9430.24, base loss: 14368.98
[INFO 2017-06-28 14:32:05,580 main.py:51] epoch 8642, training loss: 8452.45, average training loss: 9429.56, base loss: 14368.97
[INFO 2017-06-28 14:32:06,686 main.py:51] epoch 8643, training loss: 9116.51, average training loss: 9429.73, base loss: 14368.35
[INFO 2017-06-28 14:32:07,726 main.py:51] epoch 8644, training loss: 9340.30, average training loss: 9431.31, base loss: 14370.35
[INFO 2017-06-28 14:32:08,749 main.py:51] epoch 8645, training loss: 8119.09, average training loss: 9430.66, base loss: 14369.60
[INFO 2017-06-28 14:32:09,851 main.py:51] epoch 8646, training loss: 9197.35, average training loss: 9428.81, base loss: 14366.39
[INFO 2017-06-28 14:32:10,968 main.py:51] epoch 8647, training loss: 8715.27, average training loss: 9427.34, base loss: 14364.11
[INFO 2017-06-28 14:32:12,087 main.py:51] epoch 8648, training loss: 8960.39, average training loss: 9426.45, base loss: 14361.69
[INFO 2017-06-28 14:32:13,120 main.py:51] epoch 8649, training loss: 9950.48, average training loss: 9426.95, base loss: 14363.09
[INFO 2017-06-28 14:32:14,177 main.py:51] epoch 8650, training loss: 9937.25, average training loss: 9427.85, base loss: 14363.25
[INFO 2017-06-28 14:32:15,240 main.py:51] epoch 8651, training loss: 11278.45, average training loss: 9429.55, base loss: 14364.84
[INFO 2017-06-28 14:32:16,297 main.py:51] epoch 8652, training loss: 9017.39, average training loss: 9428.18, base loss: 14362.88
[INFO 2017-06-28 14:32:17,370 main.py:51] epoch 8653, training loss: 8087.92, average training loss: 9425.80, base loss: 14358.09
[INFO 2017-06-28 14:32:18,470 main.py:51] epoch 8654, training loss: 9274.45, average training loss: 9426.03, base loss: 14358.57
[INFO 2017-06-28 14:32:19,564 main.py:51] epoch 8655, training loss: 10164.94, average training loss: 9426.76, base loss: 14359.96
[INFO 2017-06-28 14:32:20,589 main.py:51] epoch 8656, training loss: 9522.04, average training loss: 9426.74, base loss: 14359.94
[INFO 2017-06-28 14:32:21,645 main.py:51] epoch 8657, training loss: 10367.05, average training loss: 9427.80, base loss: 14362.12
[INFO 2017-06-28 14:32:22,736 main.py:51] epoch 8658, training loss: 8986.05, average training loss: 9428.70, base loss: 14363.51
[INFO 2017-06-28 14:32:23,800 main.py:51] epoch 8659, training loss: 9507.81, average training loss: 9428.59, base loss: 14362.33
[INFO 2017-06-28 14:32:24,891 main.py:51] epoch 8660, training loss: 7948.84, average training loss: 9425.69, base loss: 14357.34
[INFO 2017-06-28 14:32:25,881 main.py:51] epoch 8661, training loss: 10027.21, average training loss: 9425.38, base loss: 14357.09
[INFO 2017-06-28 14:32:26,939 main.py:51] epoch 8662, training loss: 7936.14, average training loss: 9422.83, base loss: 14353.18
[INFO 2017-06-28 14:32:28,014 main.py:51] epoch 8663, training loss: 10055.81, average training loss: 9423.22, base loss: 14355.21
[INFO 2017-06-28 14:32:28,978 main.py:51] epoch 8664, training loss: 9766.28, average training loss: 9424.01, base loss: 14357.43
[INFO 2017-06-28 14:32:29,916 main.py:51] epoch 8665, training loss: 9281.01, average training loss: 9424.15, base loss: 14359.11
[INFO 2017-06-28 14:32:30,833 main.py:51] epoch 8666, training loss: 7449.93, average training loss: 9422.88, base loss: 14357.18
[INFO 2017-06-28 14:32:31,865 main.py:51] epoch 8667, training loss: 9553.83, average training loss: 9422.25, base loss: 14356.90
[INFO 2017-06-28 14:32:32,958 main.py:51] epoch 8668, training loss: 10205.50, average training loss: 9423.35, base loss: 14359.20
[INFO 2017-06-28 14:32:34,016 main.py:51] epoch 8669, training loss: 9841.57, average training loss: 9422.63, base loss: 14358.52
[INFO 2017-06-28 14:32:35,031 main.py:51] epoch 8670, training loss: 8689.59, average training loss: 9422.70, base loss: 14359.44
[INFO 2017-06-28 14:32:36,075 main.py:51] epoch 8671, training loss: 10218.23, average training loss: 9424.05, base loss: 14361.96
[INFO 2017-06-28 14:32:37,144 main.py:51] epoch 8672, training loss: 9093.70, average training loss: 9423.61, base loss: 14362.79
[INFO 2017-06-28 14:32:38,249 main.py:51] epoch 8673, training loss: 10148.45, average training loss: 9425.80, base loss: 14367.17
[INFO 2017-06-28 14:32:39,378 main.py:51] epoch 8674, training loss: 10211.29, average training loss: 9425.51, base loss: 14366.44
[INFO 2017-06-28 14:32:40,414 main.py:51] epoch 8675, training loss: 9394.70, average training loss: 9425.52, base loss: 14366.35
[INFO 2017-06-28 14:32:41,466 main.py:51] epoch 8676, training loss: 9624.07, average training loss: 9426.22, base loss: 14367.80
[INFO 2017-06-28 14:32:42,517 main.py:51] epoch 8677, training loss: 9290.85, average training loss: 9425.69, base loss: 14366.62
[INFO 2017-06-28 14:32:43,550 main.py:51] epoch 8678, training loss: 8782.89, average training loss: 9425.79, base loss: 14366.46
[INFO 2017-06-28 14:32:44,639 main.py:51] epoch 8679, training loss: 9893.27, average training loss: 9425.73, base loss: 14367.09
[INFO 2017-06-28 14:32:45,646 main.py:51] epoch 8680, training loss: 9120.90, average training loss: 9426.90, base loss: 14369.91
[INFO 2017-06-28 14:32:46,712 main.py:51] epoch 8681, training loss: 10038.41, average training loss: 9427.47, base loss: 14371.98
[INFO 2017-06-28 14:32:47,800 main.py:51] epoch 8682, training loss: 8942.15, average training loss: 9427.69, base loss: 14372.38
[INFO 2017-06-28 14:32:48,882 main.py:51] epoch 8683, training loss: 8599.21, average training loss: 9426.83, base loss: 14370.48
[INFO 2017-06-28 14:32:49,957 main.py:51] epoch 8684, training loss: 8341.99, average training loss: 9426.45, base loss: 14369.10
[INFO 2017-06-28 14:32:50,969 main.py:51] epoch 8685, training loss: 9848.97, average training loss: 9428.11, base loss: 14371.68
[INFO 2017-06-28 14:32:52,057 main.py:51] epoch 8686, training loss: 10324.34, average training loss: 9427.88, base loss: 14373.12
[INFO 2017-06-28 14:32:53,070 main.py:51] epoch 8687, training loss: 8723.22, average training loss: 9425.51, base loss: 14367.85
[INFO 2017-06-28 14:32:54,121 main.py:51] epoch 8688, training loss: 10203.45, average training loss: 9424.64, base loss: 14368.23
[INFO 2017-06-28 14:32:55,196 main.py:51] epoch 8689, training loss: 9969.50, average training loss: 9424.81, base loss: 14368.82
[INFO 2017-06-28 14:32:56,187 main.py:51] epoch 8690, training loss: 8791.28, average training loss: 9424.27, base loss: 14367.06
[INFO 2017-06-28 14:32:57,205 main.py:51] epoch 8691, training loss: 9604.38, average training loss: 9425.53, base loss: 14368.25
[INFO 2017-06-28 14:32:58,295 main.py:51] epoch 8692, training loss: 9280.53, average training loss: 9424.28, base loss: 14366.64
[INFO 2017-06-28 14:32:59,377 main.py:51] epoch 8693, training loss: 9417.74, average training loss: 9424.81, base loss: 14367.00
[INFO 2017-06-28 14:33:00,428 main.py:51] epoch 8694, training loss: 8999.97, average training loss: 9424.40, base loss: 14365.54
[INFO 2017-06-28 14:33:01,461 main.py:51] epoch 8695, training loss: 10177.03, average training loss: 9426.17, base loss: 14370.55
[INFO 2017-06-28 14:33:02,513 main.py:51] epoch 8696, training loss: 10520.92, average training loss: 9427.80, base loss: 14372.26
[INFO 2017-06-28 14:33:03,598 main.py:51] epoch 8697, training loss: 9442.25, average training loss: 9427.06, base loss: 14371.02
[INFO 2017-06-28 14:33:04,616 main.py:51] epoch 8698, training loss: 8395.45, average training loss: 9425.93, base loss: 14369.45
[INFO 2017-06-28 14:33:05,682 main.py:51] epoch 8699, training loss: 9548.60, average training loss: 9428.29, base loss: 14373.09
[INFO 2017-06-28 14:33:05,682 main.py:53] epoch 8699, testing
[INFO 2017-06-28 14:33:09,299 main.py:105] average testing loss: 10487.25, base loss: 14472.05
[INFO 2017-06-28 14:33:09,299 main.py:106] improve_loss: 3984.80, improve_percent: 0.28
[INFO 2017-06-28 14:33:09,300 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:33:10,281 main.py:51] epoch 8700, training loss: 9404.53, average training loss: 9428.61, base loss: 14373.17
[INFO 2017-06-28 14:33:11,344 main.py:51] epoch 8701, training loss: 9333.38, average training loss: 9429.29, base loss: 14373.07
[INFO 2017-06-28 14:33:12,429 main.py:51] epoch 8702, training loss: 8652.14, average training loss: 9427.55, base loss: 14370.09
[INFO 2017-06-28 14:33:13,510 main.py:51] epoch 8703, training loss: 8402.88, average training loss: 9426.23, base loss: 14368.25
[INFO 2017-06-28 14:33:14,562 main.py:51] epoch 8704, training loss: 9835.36, average training loss: 9427.91, base loss: 14369.83
[INFO 2017-06-28 14:33:15,590 main.py:51] epoch 8705, training loss: 9244.76, average training loss: 9428.49, base loss: 14371.73
[INFO 2017-06-28 14:33:16,639 main.py:51] epoch 8706, training loss: 8541.68, average training loss: 9427.69, base loss: 14371.49
[INFO 2017-06-28 14:33:17,696 main.py:51] epoch 8707, training loss: 10312.37, average training loss: 9427.81, base loss: 14371.97
[INFO 2017-06-28 14:33:18,734 main.py:51] epoch 8708, training loss: 10553.92, average training loss: 9428.82, base loss: 14373.64
[INFO 2017-06-28 14:33:19,782 main.py:51] epoch 8709, training loss: 8897.78, average training loss: 9427.96, base loss: 14371.48
[INFO 2017-06-28 14:33:20,814 main.py:51] epoch 8710, training loss: 9661.98, average training loss: 9429.77, base loss: 14374.83
[INFO 2017-06-28 14:33:21,850 main.py:51] epoch 8711, training loss: 9792.98, average training loss: 9429.56, base loss: 14375.58
[INFO 2017-06-28 14:33:22,892 main.py:51] epoch 8712, training loss: 9822.56, average training loss: 9430.33, base loss: 14375.54
[INFO 2017-06-28 14:33:23,918 main.py:51] epoch 8713, training loss: 7464.37, average training loss: 9428.48, base loss: 14372.13
[INFO 2017-06-28 14:33:25,000 main.py:51] epoch 8714, training loss: 9913.57, average training loss: 9428.51, base loss: 14372.54
[INFO 2017-06-28 14:33:26,080 main.py:51] epoch 8715, training loss: 9810.68, average training loss: 9428.91, base loss: 14374.04
[INFO 2017-06-28 14:33:27,132 main.py:51] epoch 8716, training loss: 9479.87, average training loss: 9429.89, base loss: 14375.21
[INFO 2017-06-28 14:33:28,161 main.py:51] epoch 8717, training loss: 8884.02, average training loss: 9430.41, base loss: 14376.64
[INFO 2017-06-28 14:33:29,236 main.py:51] epoch 8718, training loss: 9161.91, average training loss: 9430.07, base loss: 14377.38
[INFO 2017-06-28 14:33:30,215 main.py:51] epoch 8719, training loss: 9362.53, average training loss: 9430.82, base loss: 14379.41
[INFO 2017-06-28 14:33:31,236 main.py:51] epoch 8720, training loss: 8804.87, average training loss: 9428.97, base loss: 14375.75
[INFO 2017-06-28 14:33:32,305 main.py:51] epoch 8721, training loss: 9778.72, average training loss: 9429.11, base loss: 14376.43
[INFO 2017-06-28 14:33:33,336 main.py:51] epoch 8722, training loss: 8641.37, average training loss: 9427.84, base loss: 14374.59
[INFO 2017-06-28 14:33:34,375 main.py:51] epoch 8723, training loss: 8863.25, average training loss: 9428.06, base loss: 14373.86
[INFO 2017-06-28 14:33:35,405 main.py:51] epoch 8724, training loss: 9467.96, average training loss: 9428.79, base loss: 14374.92
[INFO 2017-06-28 14:33:36,475 main.py:51] epoch 8725, training loss: 10687.29, average training loss: 9428.03, base loss: 14372.13
[INFO 2017-06-28 14:33:37,590 main.py:51] epoch 8726, training loss: 9758.40, average training loss: 9428.29, base loss: 14372.81
[INFO 2017-06-28 14:33:38,663 main.py:51] epoch 8727, training loss: 9794.06, average training loss: 9428.19, base loss: 14372.06
[INFO 2017-06-28 14:33:39,783 main.py:51] epoch 8728, training loss: 8742.72, average training loss: 9428.05, base loss: 14373.12
[INFO 2017-06-28 14:33:40,767 main.py:51] epoch 8729, training loss: 8968.38, average training loss: 9427.52, base loss: 14372.32
[INFO 2017-06-28 14:33:41,837 main.py:51] epoch 8730, training loss: 8818.14, average training loss: 9425.40, base loss: 14370.45
[INFO 2017-06-28 14:33:42,914 main.py:51] epoch 8731, training loss: 9110.03, average training loss: 9424.38, base loss: 14371.39
[INFO 2017-06-28 14:33:43,932 main.py:51] epoch 8732, training loss: 8721.04, average training loss: 9424.56, base loss: 14371.00
[INFO 2017-06-28 14:33:44,961 main.py:51] epoch 8733, training loss: 9701.73, average training loss: 9424.95, base loss: 14371.81
[INFO 2017-06-28 14:33:46,001 main.py:51] epoch 8734, training loss: 9183.00, average training loss: 9424.20, base loss: 14369.69
[INFO 2017-06-28 14:33:46,945 main.py:51] epoch 8735, training loss: 8990.43, average training loss: 9424.27, base loss: 14368.59
[INFO 2017-06-28 14:33:47,875 main.py:51] epoch 8736, training loss: 9992.35, average training loss: 9426.22, base loss: 14370.88
[INFO 2017-06-28 14:33:48,843 main.py:51] epoch 8737, training loss: 8046.13, average training loss: 9425.43, base loss: 14369.05
[INFO 2017-06-28 14:33:49,856 main.py:51] epoch 8738, training loss: 8970.67, average training loss: 9423.87, base loss: 14366.69
[INFO 2017-06-28 14:33:50,898 main.py:51] epoch 8739, training loss: 8906.67, average training loss: 9424.26, base loss: 14368.87
[INFO 2017-06-28 14:33:51,951 main.py:51] epoch 8740, training loss: 9173.06, average training loss: 9423.88, base loss: 14367.03
[INFO 2017-06-28 14:33:53,021 main.py:51] epoch 8741, training loss: 8844.92, average training loss: 9422.77, base loss: 14364.89
[INFO 2017-06-28 14:33:54,128 main.py:51] epoch 8742, training loss: 8741.97, average training loss: 9421.74, base loss: 14365.05
[INFO 2017-06-28 14:33:55,143 main.py:51] epoch 8743, training loss: 8834.12, average training loss: 9423.05, base loss: 14366.64
[INFO 2017-06-28 14:33:56,218 main.py:51] epoch 8744, training loss: 9468.95, average training loss: 9422.94, base loss: 14368.05
[INFO 2017-06-28 14:33:57,230 main.py:51] epoch 8745, training loss: 9289.75, average training loss: 9422.10, base loss: 14367.56
[INFO 2017-06-28 14:33:58,301 main.py:51] epoch 8746, training loss: 10622.62, average training loss: 9422.53, base loss: 14368.63
[INFO 2017-06-28 14:33:59,348 main.py:51] epoch 8747, training loss: 9226.47, average training loss: 9422.70, base loss: 14371.13
[INFO 2017-06-28 14:34:00,392 main.py:51] epoch 8748, training loss: 8415.26, average training loss: 9421.39, base loss: 14368.29
[INFO 2017-06-28 14:34:01,485 main.py:51] epoch 8749, training loss: 10239.34, average training loss: 9421.11, base loss: 14366.27
[INFO 2017-06-28 14:34:02,497 main.py:51] epoch 8750, training loss: 8455.36, average training loss: 9420.14, base loss: 14365.63
[INFO 2017-06-28 14:34:03,520 main.py:51] epoch 8751, training loss: 9961.82, average training loss: 9421.54, base loss: 14368.37
[INFO 2017-06-28 14:34:04,610 main.py:51] epoch 8752, training loss: 8326.61, average training loss: 9420.26, base loss: 14367.01
[INFO 2017-06-28 14:34:05,676 main.py:51] epoch 8753, training loss: 9391.87, average training loss: 9419.60, base loss: 14365.37
[INFO 2017-06-28 14:34:06,758 main.py:51] epoch 8754, training loss: 9237.56, average training loss: 9418.14, base loss: 14363.12
[INFO 2017-06-28 14:34:07,786 main.py:51] epoch 8755, training loss: 9718.77, average training loss: 9417.86, base loss: 14363.44
[INFO 2017-06-28 14:34:08,856 main.py:51] epoch 8756, training loss: 9125.06, average training loss: 9416.84, base loss: 14360.66
[INFO 2017-06-28 14:34:09,956 main.py:51] epoch 8757, training loss: 10256.09, average training loss: 9418.66, base loss: 14363.21
[INFO 2017-06-28 14:34:10,995 main.py:51] epoch 8758, training loss: 9125.96, average training loss: 9418.57, base loss: 14362.62
[INFO 2017-06-28 14:34:12,026 main.py:51] epoch 8759, training loss: 8623.69, average training loss: 9417.35, base loss: 14360.46
[INFO 2017-06-28 14:34:13,072 main.py:51] epoch 8760, training loss: 8732.86, average training loss: 9415.55, base loss: 14357.79
[INFO 2017-06-28 14:34:14,154 main.py:51] epoch 8761, training loss: 10165.40, average training loss: 9415.76, base loss: 14358.46
[INFO 2017-06-28 14:34:15,239 main.py:51] epoch 8762, training loss: 9399.39, average training loss: 9416.98, base loss: 14360.43
[INFO 2017-06-28 14:34:16,324 main.py:51] epoch 8763, training loss: 9922.92, average training loss: 9416.80, base loss: 14359.28
[INFO 2017-06-28 14:34:17,411 main.py:51] epoch 8764, training loss: 9920.04, average training loss: 9417.06, base loss: 14359.92
[INFO 2017-06-28 14:34:18,420 main.py:51] epoch 8765, training loss: 9040.32, average training loss: 9416.09, base loss: 14358.76
[INFO 2017-06-28 14:34:19,494 main.py:51] epoch 8766, training loss: 9152.64, average training loss: 9414.87, base loss: 14358.96
[INFO 2017-06-28 14:34:20,565 main.py:51] epoch 8767, training loss: 12488.38, average training loss: 9418.03, base loss: 14364.05
[INFO 2017-06-28 14:34:21,557 main.py:51] epoch 8768, training loss: 9655.73, average training loss: 9418.20, base loss: 14362.92
[INFO 2017-06-28 14:34:22,632 main.py:51] epoch 8769, training loss: 10778.12, average training loss: 9420.86, base loss: 14365.98
[INFO 2017-06-28 14:34:23,726 main.py:51] epoch 8770, training loss: 8375.36, average training loss: 9419.14, base loss: 14362.54
[INFO 2017-06-28 14:34:24,700 main.py:51] epoch 8771, training loss: 10622.69, average training loss: 9419.10, base loss: 14362.51
[INFO 2017-06-28 14:34:25,731 main.py:51] epoch 8772, training loss: 8528.81, average training loss: 9416.85, base loss: 14359.72
[INFO 2017-06-28 14:34:26,824 main.py:51] epoch 8773, training loss: 10997.62, average training loss: 9419.19, base loss: 14363.38
[INFO 2017-06-28 14:34:27,892 main.py:51] epoch 8774, training loss: 9242.31, average training loss: 9418.77, base loss: 14363.50
[INFO 2017-06-28 14:34:28,947 main.py:51] epoch 8775, training loss: 10705.66, average training loss: 9420.23, base loss: 14366.31
[INFO 2017-06-28 14:34:29,968 main.py:51] epoch 8776, training loss: 10795.23, average training loss: 9422.47, base loss: 14368.44
[INFO 2017-06-28 14:34:31,047 main.py:51] epoch 8777, training loss: 10016.68, average training loss: 9422.37, base loss: 14370.27
[INFO 2017-06-28 14:34:32,118 main.py:51] epoch 8778, training loss: 9348.83, average training loss: 9423.10, base loss: 14371.86
[INFO 2017-06-28 14:34:33,192 main.py:51] epoch 8779, training loss: 8784.20, average training loss: 9423.00, base loss: 14372.18
[INFO 2017-06-28 14:34:34,294 main.py:51] epoch 8780, training loss: 9788.44, average training loss: 9422.76, base loss: 14371.06
[INFO 2017-06-28 14:34:35,273 main.py:51] epoch 8781, training loss: 8171.56, average training loss: 9419.17, base loss: 14365.82
[INFO 2017-06-28 14:34:36,335 main.py:51] epoch 8782, training loss: 10922.70, average training loss: 9419.47, base loss: 14366.75
[INFO 2017-06-28 14:34:37,429 main.py:51] epoch 8783, training loss: 10254.88, average training loss: 9420.81, base loss: 14368.26
[INFO 2017-06-28 14:34:38,465 main.py:51] epoch 8784, training loss: 9165.38, average training loss: 9418.43, base loss: 14363.61
[INFO 2017-06-28 14:34:39,546 main.py:51] epoch 8785, training loss: 8868.40, average training loss: 9419.18, base loss: 14363.08
[INFO 2017-06-28 14:34:40,600 main.py:51] epoch 8786, training loss: 9241.80, average training loss: 9417.42, base loss: 14361.12
[INFO 2017-06-28 14:34:41,664 main.py:51] epoch 8787, training loss: 8956.95, average training loss: 9417.64, base loss: 14361.99
[INFO 2017-06-28 14:34:42,745 main.py:51] epoch 8788, training loss: 7706.16, average training loss: 9415.11, base loss: 14358.72
[INFO 2017-06-28 14:34:43,793 main.py:51] epoch 8789, training loss: 9409.54, average training loss: 9416.48, base loss: 14362.33
[INFO 2017-06-28 14:34:44,781 main.py:51] epoch 8790, training loss: 10291.96, average training loss: 9416.89, base loss: 14363.29
[INFO 2017-06-28 14:34:45,838 main.py:51] epoch 8791, training loss: 9204.79, average training loss: 9416.69, base loss: 14363.01
[INFO 2017-06-28 14:34:46,947 main.py:51] epoch 8792, training loss: 9930.37, average training loss: 9416.56, base loss: 14364.14
[INFO 2017-06-28 14:34:48,059 main.py:51] epoch 8793, training loss: 10307.74, average training loss: 9415.92, base loss: 14363.37
[INFO 2017-06-28 14:34:49,164 main.py:51] epoch 8794, training loss: 8566.95, average training loss: 9414.11, base loss: 14360.00
[INFO 2017-06-28 14:34:50,285 main.py:51] epoch 8795, training loss: 9585.61, average training loss: 9414.06, base loss: 14360.13
[INFO 2017-06-28 14:34:51,400 main.py:51] epoch 8796, training loss: 9920.46, average training loss: 9414.40, base loss: 14361.51
[INFO 2017-06-28 14:34:52,472 main.py:51] epoch 8797, training loss: 10077.96, average training loss: 9414.05, base loss: 14359.72
[INFO 2017-06-28 14:34:53,497 main.py:51] epoch 8798, training loss: 8513.84, average training loss: 9413.72, base loss: 14358.29
[INFO 2017-06-28 14:34:54,563 main.py:51] epoch 8799, training loss: 8998.22, average training loss: 9413.30, base loss: 14357.43
[INFO 2017-06-28 14:34:54,563 main.py:53] epoch 8799, testing
[INFO 2017-06-28 14:34:58,359 main.py:105] average testing loss: 10311.37, base loss: 14441.12
[INFO 2017-06-28 14:34:58,359 main.py:106] improve_loss: 4129.74, improve_percent: 0.29
[INFO 2017-06-28 14:34:58,359 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:34:59,460 main.py:51] epoch 8800, training loss: 10034.69, average training loss: 9414.22, base loss: 14357.48
[INFO 2017-06-28 14:35:00,559 main.py:51] epoch 8801, training loss: 8327.25, average training loss: 9414.44, base loss: 14358.85
[INFO 2017-06-28 14:35:01,633 main.py:51] epoch 8802, training loss: 9598.00, average training loss: 9414.45, base loss: 14358.79
[INFO 2017-06-28 14:35:02,618 main.py:51] epoch 8803, training loss: 10041.43, average training loss: 9414.88, base loss: 14358.65
[INFO 2017-06-28 14:35:03,693 main.py:51] epoch 8804, training loss: 9842.21, average training loss: 9415.48, base loss: 14358.48
[INFO 2017-06-28 14:35:04,746 main.py:51] epoch 8805, training loss: 8674.57, average training loss: 9414.36, base loss: 14355.32
[INFO 2017-06-28 14:35:05,703 main.py:51] epoch 8806, training loss: 9030.65, average training loss: 9415.18, base loss: 14356.84
[INFO 2017-06-28 14:35:06,669 main.py:51] epoch 8807, training loss: 8865.91, average training loss: 9415.19, base loss: 14355.22
[INFO 2017-06-28 14:35:07,668 main.py:51] epoch 8808, training loss: 10411.73, average training loss: 9415.75, base loss: 14355.91
[INFO 2017-06-28 14:35:08,773 main.py:51] epoch 8809, training loss: 8727.28, average training loss: 9415.25, base loss: 14354.99
[INFO 2017-06-28 14:35:09,862 main.py:51] epoch 8810, training loss: 9241.84, average training loss: 9415.24, base loss: 14355.99
[INFO 2017-06-28 14:35:10,963 main.py:51] epoch 8811, training loss: 9954.92, average training loss: 9415.90, base loss: 14357.79
[INFO 2017-06-28 14:35:12,093 main.py:51] epoch 8812, training loss: 10368.19, average training loss: 9416.57, base loss: 14358.21
[INFO 2017-06-28 14:35:13,111 main.py:51] epoch 8813, training loss: 9306.39, average training loss: 9415.43, base loss: 14355.95
[INFO 2017-06-28 14:35:14,167 main.py:51] epoch 8814, training loss: 9610.85, average training loss: 9416.90, base loss: 14358.51
[INFO 2017-06-28 14:35:15,221 main.py:51] epoch 8815, training loss: 9610.99, average training loss: 9416.27, base loss: 14357.85
[INFO 2017-06-28 14:35:16,295 main.py:51] epoch 8816, training loss: 9148.36, average training loss: 9415.52, base loss: 14358.11
[INFO 2017-06-28 14:35:17,331 main.py:51] epoch 8817, training loss: 8879.36, average training loss: 9414.53, base loss: 14356.28
[INFO 2017-06-28 14:35:18,386 main.py:51] epoch 8818, training loss: 9210.26, average training loss: 9413.86, base loss: 14357.23
[INFO 2017-06-28 14:35:19,513 main.py:51] epoch 8819, training loss: 8262.81, average training loss: 9413.46, base loss: 14356.88
[INFO 2017-06-28 14:35:20,562 main.py:51] epoch 8820, training loss: 9071.26, average training loss: 9413.33, base loss: 14356.06
[INFO 2017-06-28 14:35:21,747 main.py:51] epoch 8821, training loss: 7806.88, average training loss: 9412.22, base loss: 14353.59
[INFO 2017-06-28 14:35:22,784 main.py:51] epoch 8822, training loss: 8934.49, average training loss: 9412.53, base loss: 14352.18
[INFO 2017-06-28 14:35:23,919 main.py:51] epoch 8823, training loss: 10498.27, average training loss: 9413.92, base loss: 14354.27
[INFO 2017-06-28 14:35:24,926 main.py:51] epoch 8824, training loss: 8569.35, average training loss: 9413.94, base loss: 14355.12
[INFO 2017-06-28 14:35:25,982 main.py:51] epoch 8825, training loss: 11330.74, average training loss: 9415.71, base loss: 14358.65
[INFO 2017-06-28 14:35:27,068 main.py:51] epoch 8826, training loss: 10099.89, average training loss: 9417.63, base loss: 14362.13
[INFO 2017-06-28 14:35:28,152 main.py:51] epoch 8827, training loss: 8457.21, average training loss: 9415.91, base loss: 14358.31
[INFO 2017-06-28 14:35:29,270 main.py:51] epoch 8828, training loss: 10305.03, average training loss: 9415.90, base loss: 14358.21
[INFO 2017-06-28 14:35:30,313 main.py:51] epoch 8829, training loss: 10245.44, average training loss: 9416.21, base loss: 14357.92
[INFO 2017-06-28 14:35:31,360 main.py:51] epoch 8830, training loss: 10021.15, average training loss: 9415.09, base loss: 14356.56
[INFO 2017-06-28 14:35:32,366 main.py:51] epoch 8831, training loss: 8243.82, average training loss: 9414.60, base loss: 14355.40
[INFO 2017-06-28 14:35:33,468 main.py:51] epoch 8832, training loss: 10263.41, average training loss: 9415.21, base loss: 14358.56
[INFO 2017-06-28 14:35:34,561 main.py:51] epoch 8833, training loss: 9925.52, average training loss: 9415.71, base loss: 14360.61
[INFO 2017-06-28 14:35:35,529 main.py:51] epoch 8834, training loss: 10337.78, average training loss: 9417.23, base loss: 14365.05
[INFO 2017-06-28 14:35:36,584 main.py:51] epoch 8835, training loss: 9239.42, average training loss: 9418.39, base loss: 14365.87
[INFO 2017-06-28 14:35:37,617 main.py:51] epoch 8836, training loss: 10567.57, average training loss: 9419.49, base loss: 14366.89
[INFO 2017-06-28 14:35:38,689 main.py:51] epoch 8837, training loss: 9693.88, average training loss: 9419.68, base loss: 14366.56
[INFO 2017-06-28 14:35:39,818 main.py:51] epoch 8838, training loss: 10900.59, average training loss: 9421.47, base loss: 14368.51
[INFO 2017-06-28 14:35:40,875 main.py:51] epoch 8839, training loss: 9092.66, average training loss: 9421.50, base loss: 14369.35
[INFO 2017-06-28 14:35:41,929 main.py:51] epoch 8840, training loss: 9279.84, average training loss: 9422.20, base loss: 14371.15
[INFO 2017-06-28 14:35:42,936 main.py:51] epoch 8841, training loss: 9629.69, average training loss: 9420.40, base loss: 14368.25
[INFO 2017-06-28 14:35:44,001 main.py:51] epoch 8842, training loss: 9444.69, average training loss: 9421.00, base loss: 14370.19
[INFO 2017-06-28 14:35:45,092 main.py:51] epoch 8843, training loss: 10278.50, average training loss: 9422.73, base loss: 14371.92
[INFO 2017-06-28 14:35:46,120 main.py:51] epoch 8844, training loss: 9675.19, average training loss: 9421.42, base loss: 14370.90
[INFO 2017-06-28 14:35:47,182 main.py:51] epoch 8845, training loss: 10003.05, average training loss: 9421.45, base loss: 14370.86
[INFO 2017-06-28 14:35:48,237 main.py:51] epoch 8846, training loss: 9203.15, average training loss: 9420.42, base loss: 14369.09
[INFO 2017-06-28 14:35:49,248 main.py:51] epoch 8847, training loss: 10320.42, average training loss: 9421.72, base loss: 14372.55
[INFO 2017-06-28 14:35:50,299 main.py:51] epoch 8848, training loss: 11176.83, average training loss: 9423.98, base loss: 14376.20
[INFO 2017-06-28 14:35:51,338 main.py:51] epoch 8849, training loss: 11641.91, average training loss: 9426.90, base loss: 14381.38
[INFO 2017-06-28 14:35:52,424 main.py:51] epoch 8850, training loss: 9822.39, average training loss: 9427.46, base loss: 14383.77
[INFO 2017-06-28 14:35:53,514 main.py:51] epoch 8851, training loss: 8577.28, average training loss: 9427.03, base loss: 14382.24
[INFO 2017-06-28 14:35:54,528 main.py:51] epoch 8852, training loss: 9027.69, average training loss: 9426.28, base loss: 14380.16
[INFO 2017-06-28 14:35:55,560 main.py:51] epoch 8853, training loss: 9649.20, average training loss: 9426.90, base loss: 14381.45
[INFO 2017-06-28 14:35:56,598 main.py:51] epoch 8854, training loss: 9927.64, average training loss: 9427.33, base loss: 14383.08
[INFO 2017-06-28 14:35:57,619 main.py:51] epoch 8855, training loss: 9195.58, average training loss: 9427.46, base loss: 14384.74
[INFO 2017-06-28 14:35:58,675 main.py:51] epoch 8856, training loss: 10580.77, average training loss: 9429.28, base loss: 14387.79
[INFO 2017-06-28 14:35:59,718 main.py:51] epoch 8857, training loss: 8316.71, average training loss: 9429.26, base loss: 14387.67
[INFO 2017-06-28 14:36:00,819 main.py:51] epoch 8858, training loss: 9987.99, average training loss: 9430.91, base loss: 14390.20
[INFO 2017-06-28 14:36:01,901 main.py:51] epoch 8859, training loss: 10175.74, average training loss: 9431.53, base loss: 14391.70
[INFO 2017-06-28 14:36:02,995 main.py:51] epoch 8860, training loss: 9607.57, average training loss: 9431.76, base loss: 14392.24
[INFO 2017-06-28 14:36:04,105 main.py:51] epoch 8861, training loss: 8458.79, average training loss: 9430.49, base loss: 14390.53
[INFO 2017-06-28 14:36:05,163 main.py:51] epoch 8862, training loss: 9622.35, average training loss: 9430.75, base loss: 14390.05
[INFO 2017-06-28 14:36:06,272 main.py:51] epoch 8863, training loss: 8856.75, average training loss: 9428.84, base loss: 14386.38
[INFO 2017-06-28 14:36:07,292 main.py:51] epoch 8864, training loss: 10358.45, average training loss: 9429.63, base loss: 14387.10
[INFO 2017-06-28 14:36:08,329 main.py:51] epoch 8865, training loss: 9936.20, average training loss: 9431.17, base loss: 14389.28
[INFO 2017-06-28 14:36:09,428 main.py:51] epoch 8866, training loss: 9213.07, average training loss: 9431.40, base loss: 14389.52
[INFO 2017-06-28 14:36:10,468 main.py:51] epoch 8867, training loss: 9501.97, average training loss: 9430.40, base loss: 14388.17
[INFO 2017-06-28 14:36:11,583 main.py:51] epoch 8868, training loss: 9163.49, average training loss: 9430.49, base loss: 14389.54
[INFO 2017-06-28 14:36:12,654 main.py:51] epoch 8869, training loss: 8910.29, average training loss: 9430.77, base loss: 14389.67
[INFO 2017-06-28 14:36:13,713 main.py:51] epoch 8870, training loss: 9838.79, average training loss: 9431.80, base loss: 14392.32
[INFO 2017-06-28 14:36:14,758 main.py:51] epoch 8871, training loss: 11788.82, average training loss: 9435.30, base loss: 14398.82
[INFO 2017-06-28 14:36:15,797 main.py:51] epoch 8872, training loss: 8652.70, average training loss: 9435.27, base loss: 14398.63
[INFO 2017-06-28 14:36:16,919 main.py:51] epoch 8873, training loss: 9283.41, average training loss: 9435.67, base loss: 14400.05
[INFO 2017-06-28 14:36:17,911 main.py:51] epoch 8874, training loss: 10307.99, average training loss: 9437.03, base loss: 14402.57
[INFO 2017-06-28 14:36:18,975 main.py:51] epoch 8875, training loss: 10714.51, average training loss: 9437.81, base loss: 14403.55
[INFO 2017-06-28 14:36:20,097 main.py:51] epoch 8876, training loss: 9614.08, average training loss: 9438.41, base loss: 14404.35
[INFO 2017-06-28 14:36:21,136 main.py:51] epoch 8877, training loss: 8692.08, average training loss: 9437.44, base loss: 14404.85
[INFO 2017-06-28 14:36:22,143 main.py:51] epoch 8878, training loss: 10146.64, average training loss: 9439.79, base loss: 14408.21
[INFO 2017-06-28 14:36:23,176 main.py:51] epoch 8879, training loss: 9663.87, average training loss: 9440.30, base loss: 14409.67
[INFO 2017-06-28 14:36:24,072 main.py:51] epoch 8880, training loss: 10549.37, average training loss: 9441.40, base loss: 14412.48
[INFO 2017-06-28 14:36:25,007 main.py:51] epoch 8881, training loss: 9037.96, average training loss: 9441.48, base loss: 14414.26
[INFO 2017-06-28 14:36:25,997 main.py:51] epoch 8882, training loss: 9911.57, average training loss: 9442.85, base loss: 14416.56
[INFO 2017-06-28 14:36:27,080 main.py:51] epoch 8883, training loss: 9431.55, average training loss: 9442.55, base loss: 14416.51
[INFO 2017-06-28 14:36:28,166 main.py:51] epoch 8884, training loss: 9975.46, average training loss: 9442.33, base loss: 14415.85
[INFO 2017-06-28 14:36:29,185 main.py:51] epoch 8885, training loss: 8660.63, average training loss: 9441.06, base loss: 14414.29
[INFO 2017-06-28 14:36:30,244 main.py:51] epoch 8886, training loss: 8760.70, average training loss: 9440.00, base loss: 14412.47
[INFO 2017-06-28 14:36:31,326 main.py:51] epoch 8887, training loss: 7959.67, average training loss: 9439.16, base loss: 14409.41
[INFO 2017-06-28 14:36:32,421 main.py:51] epoch 8888, training loss: 11905.84, average training loss: 9443.07, base loss: 14416.59
[INFO 2017-06-28 14:36:33,498 main.py:51] epoch 8889, training loss: 8375.06, average training loss: 9440.29, base loss: 14410.63
[INFO 2017-06-28 14:36:34,505 main.py:51] epoch 8890, training loss: 9384.80, average training loss: 9440.61, base loss: 14412.51
[INFO 2017-06-28 14:36:35,564 main.py:51] epoch 8891, training loss: 10723.93, average training loss: 9441.62, base loss: 14415.29
[INFO 2017-06-28 14:36:36,652 main.py:51] epoch 8892, training loss: 11178.29, average training loss: 9443.52, base loss: 14417.85
[INFO 2017-06-28 14:36:37,681 main.py:51] epoch 8893, training loss: 7876.27, average training loss: 9440.62, base loss: 14414.48
[INFO 2017-06-28 14:36:38,723 main.py:51] epoch 8894, training loss: 9909.65, average training loss: 9440.70, base loss: 14414.02
[INFO 2017-06-28 14:36:39,754 main.py:51] epoch 8895, training loss: 9284.74, average training loss: 9440.52, base loss: 14415.55
[INFO 2017-06-28 14:36:40,815 main.py:51] epoch 8896, training loss: 9091.23, average training loss: 9439.15, base loss: 14412.41
[INFO 2017-06-28 14:36:41,860 main.py:51] epoch 8897, training loss: 8067.78, average training loss: 9438.00, base loss: 14409.66
[INFO 2017-06-28 14:36:42,890 main.py:51] epoch 8898, training loss: 11212.46, average training loss: 9440.24, base loss: 14412.96
[INFO 2017-06-28 14:36:43,873 main.py:51] epoch 8899, training loss: 9651.03, average training loss: 9438.67, base loss: 14410.09
[INFO 2017-06-28 14:36:43,873 main.py:53] epoch 8899, testing
[INFO 2017-06-28 14:36:47,633 main.py:105] average testing loss: 10557.04, base loss: 15199.56
[INFO 2017-06-28 14:36:47,633 main.py:106] improve_loss: 4642.52, improve_percent: 0.31
[INFO 2017-06-28 14:36:47,634 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:36:48,705 main.py:51] epoch 8900, training loss: 10544.35, average training loss: 9440.25, base loss: 14415.44
[INFO 2017-06-28 14:36:49,769 main.py:51] epoch 8901, training loss: 9750.35, average training loss: 9441.35, base loss: 14415.89
[INFO 2017-06-28 14:36:50,856 main.py:51] epoch 8902, training loss: 9157.40, average training loss: 9441.50, base loss: 14418.45
[INFO 2017-06-28 14:36:51,922 main.py:51] epoch 8903, training loss: 8698.73, average training loss: 9441.67, base loss: 14419.61
[INFO 2017-06-28 14:36:52,961 main.py:51] epoch 8904, training loss: 9503.69, average training loss: 9442.91, base loss: 14422.66
[INFO 2017-06-28 14:36:54,032 main.py:51] epoch 8905, training loss: 10096.92, average training loss: 9443.28, base loss: 14424.52
[INFO 2017-06-28 14:36:55,128 main.py:51] epoch 8906, training loss: 8841.97, average training loss: 9443.14, base loss: 14423.98
[INFO 2017-06-28 14:36:56,146 main.py:51] epoch 8907, training loss: 9812.79, average training loss: 9444.13, base loss: 14425.24
[INFO 2017-06-28 14:36:57,168 main.py:51] epoch 8908, training loss: 9339.58, average training loss: 9443.93, base loss: 14425.60
[INFO 2017-06-28 14:36:58,214 main.py:51] epoch 8909, training loss: 11077.00, average training loss: 9446.46, base loss: 14429.83
[INFO 2017-06-28 14:36:59,298 main.py:51] epoch 8910, training loss: 8896.66, average training loss: 9445.51, base loss: 14429.12
[INFO 2017-06-28 14:37:00,400 main.py:51] epoch 8911, training loss: 9579.88, average training loss: 9445.14, base loss: 14428.27
[INFO 2017-06-28 14:37:01,389 main.py:51] epoch 8912, training loss: 8437.12, average training loss: 9443.78, base loss: 14426.93
[INFO 2017-06-28 14:37:02,405 main.py:51] epoch 8913, training loss: 9429.75, average training loss: 9443.57, base loss: 14428.17
[INFO 2017-06-28 14:37:03,456 main.py:51] epoch 8914, training loss: 9687.80, average training loss: 9443.04, base loss: 14427.84
[INFO 2017-06-28 14:37:04,532 main.py:51] epoch 8915, training loss: 9679.55, average training loss: 9443.00, base loss: 14428.74
[INFO 2017-06-28 14:37:05,675 main.py:51] epoch 8916, training loss: 9632.12, average training loss: 9441.44, base loss: 14425.86
[INFO 2017-06-28 14:37:06,777 main.py:51] epoch 8917, training loss: 8161.90, average training loss: 9439.64, base loss: 14421.93
[INFO 2017-06-28 14:37:07,907 main.py:51] epoch 8918, training loss: 9898.01, average training loss: 9440.97, base loss: 14424.92
[INFO 2017-06-28 14:37:08,950 main.py:51] epoch 8919, training loss: 8464.17, average training loss: 9440.50, base loss: 14423.57
[INFO 2017-06-28 14:37:10,066 main.py:51] epoch 8920, training loss: 10262.55, average training loss: 9441.26, base loss: 14423.85
[INFO 2017-06-28 14:37:11,114 main.py:51] epoch 8921, training loss: 9654.85, average training loss: 9441.34, base loss: 14424.36
[INFO 2017-06-28 14:37:12,182 main.py:51] epoch 8922, training loss: 8949.11, average training loss: 9440.25, base loss: 14421.61
[INFO 2017-06-28 14:37:13,213 main.py:51] epoch 8923, training loss: 9233.01, average training loss: 9440.52, base loss: 14420.81
[INFO 2017-06-28 14:37:14,293 main.py:51] epoch 8924, training loss: 8123.02, average training loss: 9438.96, base loss: 14417.87
[INFO 2017-06-28 14:37:15,404 main.py:51] epoch 8925, training loss: 9578.48, average training loss: 9439.80, base loss: 14418.80
[INFO 2017-06-28 14:37:16,473 main.py:51] epoch 8926, training loss: 9135.52, average training loss: 9439.12, base loss: 14416.28
[INFO 2017-06-28 14:37:17,606 main.py:51] epoch 8927, training loss: 8661.80, average training loss: 9438.86, base loss: 14415.83
[INFO 2017-06-28 14:37:18,618 main.py:51] epoch 8928, training loss: 9173.83, average training loss: 9439.64, base loss: 14417.43
[INFO 2017-06-28 14:37:19,675 main.py:51] epoch 8929, training loss: 9157.24, average training loss: 9439.50, base loss: 14417.19
[INFO 2017-06-28 14:37:20,739 main.py:51] epoch 8930, training loss: 9835.15, average training loss: 9439.45, base loss: 14415.40
[INFO 2017-06-28 14:37:21,720 main.py:51] epoch 8931, training loss: 10948.28, average training loss: 9440.27, base loss: 14417.60
[INFO 2017-06-28 14:37:22,749 main.py:51] epoch 8932, training loss: 8617.60, average training loss: 9440.39, base loss: 14417.84
[INFO 2017-06-28 14:37:23,841 main.py:51] epoch 8933, training loss: 8110.82, average training loss: 9439.76, base loss: 14416.27
[INFO 2017-06-28 14:37:24,881 main.py:51] epoch 8934, training loss: 9168.42, average training loss: 9440.99, base loss: 14418.25
[INFO 2017-06-28 14:37:25,979 main.py:51] epoch 8935, training loss: 8829.81, average training loss: 9440.82, base loss: 14418.58
[INFO 2017-06-28 14:37:27,021 main.py:51] epoch 8936, training loss: 8969.03, average training loss: 9439.96, base loss: 14417.38
[INFO 2017-06-28 14:37:28,039 main.py:51] epoch 8937, training loss: 9945.24, average training loss: 9439.80, base loss: 14418.31
[INFO 2017-06-28 14:37:29,126 main.py:51] epoch 8938, training loss: 9802.23, average training loss: 9440.30, base loss: 14418.28
[INFO 2017-06-28 14:37:30,218 main.py:51] epoch 8939, training loss: 9232.77, average training loss: 9439.63, base loss: 14418.27
[INFO 2017-06-28 14:37:31,303 main.py:51] epoch 8940, training loss: 10851.69, average training loss: 9440.56, base loss: 14420.90
[INFO 2017-06-28 14:37:32,335 main.py:51] epoch 8941, training loss: 8831.31, average training loss: 9440.20, base loss: 14420.45
[INFO 2017-06-28 14:37:33,381 main.py:51] epoch 8942, training loss: 9314.08, average training loss: 9437.07, base loss: 14415.85
[INFO 2017-06-28 14:37:34,428 main.py:51] epoch 8943, training loss: 9743.31, average training loss: 9436.77, base loss: 14415.55
[INFO 2017-06-28 14:37:35,519 main.py:51] epoch 8944, training loss: 8746.12, average training loss: 9435.38, base loss: 14413.52
[INFO 2017-06-28 14:37:36,663 main.py:51] epoch 8945, training loss: 8610.17, average training loss: 9434.46, base loss: 14410.74
[INFO 2017-06-28 14:37:37,649 main.py:51] epoch 8946, training loss: 8817.14, average training loss: 9434.89, base loss: 14410.55
[INFO 2017-06-28 14:37:38,676 main.py:51] epoch 8947, training loss: 8613.63, average training loss: 9434.72, base loss: 14411.38
[INFO 2017-06-28 14:37:39,719 main.py:51] epoch 8948, training loss: 9609.77, average training loss: 9435.22, base loss: 14411.77
[INFO 2017-06-28 14:37:40,790 main.py:51] epoch 8949, training loss: 10629.80, average training loss: 9436.26, base loss: 14412.66
[INFO 2017-06-28 14:37:41,784 main.py:51] epoch 8950, training loss: 9313.57, average training loss: 9435.56, base loss: 14412.15
[INFO 2017-06-28 14:37:42,716 main.py:51] epoch 8951, training loss: 9883.85, average training loss: 9435.83, base loss: 14412.18
[INFO 2017-06-28 14:37:43,659 main.py:51] epoch 8952, training loss: 8734.90, average training loss: 9434.85, base loss: 14411.14
[INFO 2017-06-28 14:37:44,774 main.py:51] epoch 8953, training loss: 8631.40, average training loss: 9434.27, base loss: 14409.08
[INFO 2017-06-28 14:37:45,778 main.py:51] epoch 8954, training loss: 8874.90, average training loss: 9431.72, base loss: 14404.65
[INFO 2017-06-28 14:37:46,883 main.py:51] epoch 8955, training loss: 8241.84, average training loss: 9430.34, base loss: 14403.60
[INFO 2017-06-28 14:37:47,917 main.py:51] epoch 8956, training loss: 10038.01, average training loss: 9431.17, base loss: 14403.90
[INFO 2017-06-28 14:37:48,999 main.py:51] epoch 8957, training loss: 9080.43, average training loss: 9431.37, base loss: 14405.21
[INFO 2017-06-28 14:37:50,062 main.py:51] epoch 8958, training loss: 10878.72, average training loss: 9433.00, base loss: 14407.42
[INFO 2017-06-28 14:37:51,127 main.py:51] epoch 8959, training loss: 9401.51, average training loss: 9433.06, base loss: 14407.18
[INFO 2017-06-28 14:37:52,215 main.py:51] epoch 8960, training loss: 8658.47, average training loss: 9430.92, base loss: 14404.16
[INFO 2017-06-28 14:37:53,207 main.py:51] epoch 8961, training loss: 8942.93, average training loss: 9428.92, base loss: 14403.07
[INFO 2017-06-28 14:37:54,292 main.py:51] epoch 8962, training loss: 8325.88, average training loss: 9427.03, base loss: 14400.59
[INFO 2017-06-28 14:37:55,330 main.py:51] epoch 8963, training loss: 8952.25, average training loss: 9425.91, base loss: 14399.77
[INFO 2017-06-28 14:37:56,321 main.py:51] epoch 8964, training loss: 8923.28, average training loss: 9425.82, base loss: 14399.96
[INFO 2017-06-28 14:37:57,384 main.py:51] epoch 8965, training loss: 8357.36, average training loss: 9424.87, base loss: 14398.19
[INFO 2017-06-28 14:37:58,476 main.py:51] epoch 8966, training loss: 9336.88, average training loss: 9424.32, base loss: 14396.79
[INFO 2017-06-28 14:37:59,525 main.py:51] epoch 8967, training loss: 8721.80, average training loss: 9423.62, base loss: 14394.47
[INFO 2017-06-28 14:38:00,615 main.py:51] epoch 8968, training loss: 9374.22, average training loss: 9423.09, base loss: 14394.66
[INFO 2017-06-28 14:38:01,612 main.py:51] epoch 8969, training loss: 10021.89, average training loss: 9424.22, base loss: 14397.25
[INFO 2017-06-28 14:38:02,695 main.py:51] epoch 8970, training loss: 9447.54, average training loss: 9423.94, base loss: 14397.42
[INFO 2017-06-28 14:38:03,764 main.py:51] epoch 8971, training loss: 9283.97, average training loss: 9424.04, base loss: 14397.58
[INFO 2017-06-28 14:38:04,889 main.py:51] epoch 8972, training loss: 8231.89, average training loss: 9422.04, base loss: 14395.13
[INFO 2017-06-28 14:38:06,017 main.py:51] epoch 8973, training loss: 9896.21, average training loss: 9422.51, base loss: 14395.81
[INFO 2017-06-28 14:38:07,038 main.py:51] epoch 8974, training loss: 10252.67, average training loss: 9422.12, base loss: 14395.77
[INFO 2017-06-28 14:38:08,099 main.py:51] epoch 8975, training loss: 10277.45, average training loss: 9423.62, base loss: 14398.02
[INFO 2017-06-28 14:38:09,120 main.py:51] epoch 8976, training loss: 9983.18, average training loss: 9422.72, base loss: 14396.44
[INFO 2017-06-28 14:38:10,208 main.py:51] epoch 8977, training loss: 10323.01, average training loss: 9424.59, base loss: 14401.02
[INFO 2017-06-28 14:38:11,300 main.py:51] epoch 8978, training loss: 9694.46, average training loss: 9424.79, base loss: 14400.57
[INFO 2017-06-28 14:38:12,402 main.py:51] epoch 8979, training loss: 8308.28, average training loss: 9423.70, base loss: 14397.07
[INFO 2017-06-28 14:38:13,441 main.py:51] epoch 8980, training loss: 9500.43, average training loss: 9422.68, base loss: 14397.12
[INFO 2017-06-28 14:38:14,448 main.py:51] epoch 8981, training loss: 10002.67, average training loss: 9423.23, base loss: 14398.20
[INFO 2017-06-28 14:38:15,538 main.py:51] epoch 8982, training loss: 8514.17, average training loss: 9421.89, base loss: 14396.96
[INFO 2017-06-28 14:38:16,600 main.py:51] epoch 8983, training loss: 8928.83, average training loss: 9420.28, base loss: 14395.09
[INFO 2017-06-28 14:38:17,650 main.py:51] epoch 8984, training loss: 8853.50, average training loss: 9421.00, base loss: 14396.33
[INFO 2017-06-28 14:38:18,689 main.py:51] epoch 8985, training loss: 9557.15, average training loss: 9422.55, base loss: 14399.58
[INFO 2017-06-28 14:38:19,722 main.py:51] epoch 8986, training loss: 10648.26, average training loss: 9423.37, base loss: 14402.54
[INFO 2017-06-28 14:38:20,820 main.py:51] epoch 8987, training loss: 8281.35, average training loss: 9422.32, base loss: 14402.54
[INFO 2017-06-28 14:38:21,929 main.py:51] epoch 8988, training loss: 9601.04, average training loss: 9423.05, base loss: 14403.19
[INFO 2017-06-28 14:38:23,039 main.py:51] epoch 8989, training loss: 8181.91, average training loss: 9420.49, base loss: 14399.44
[INFO 2017-06-28 14:38:24,178 main.py:51] epoch 8990, training loss: 9143.29, average training loss: 9419.42, base loss: 14398.69
[INFO 2017-06-28 14:38:25,206 main.py:51] epoch 8991, training loss: 9570.43, average training loss: 9419.49, base loss: 14399.41
[INFO 2017-06-28 14:38:26,236 main.py:51] epoch 8992, training loss: 8833.44, average training loss: 9418.07, base loss: 14395.93
[INFO 2017-06-28 14:38:27,278 main.py:51] epoch 8993, training loss: 8456.59, average training loss: 9417.48, base loss: 14393.87
[INFO 2017-06-28 14:38:28,317 main.py:51] epoch 8994, training loss: 9322.16, average training loss: 9415.38, base loss: 14391.99
[INFO 2017-06-28 14:38:29,382 main.py:51] epoch 8995, training loss: 8475.05, average training loss: 9415.16, base loss: 14392.90
[INFO 2017-06-28 14:38:30,410 main.py:51] epoch 8996, training loss: 8989.82, average training loss: 9413.48, base loss: 14390.06
[INFO 2017-06-28 14:38:31,459 main.py:51] epoch 8997, training loss: 9995.61, average training loss: 9414.46, base loss: 14392.95
[INFO 2017-06-28 14:38:32,505 main.py:51] epoch 8998, training loss: 9045.12, average training loss: 9414.52, base loss: 14393.43
[INFO 2017-06-28 14:38:33,542 main.py:51] epoch 8999, training loss: 9642.34, average training loss: 9415.57, base loss: 14397.39
[INFO 2017-06-28 14:38:33,542 main.py:53] epoch 8999, testing
[INFO 2017-06-28 14:38:37,210 main.py:105] average testing loss: 10542.21, base loss: 15382.79
[INFO 2017-06-28 14:38:37,210 main.py:106] improve_loss: 4840.58, improve_percent: 0.31
[INFO 2017-06-28 14:38:37,210 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 14:38:37,247 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:38:38,273 main.py:51] epoch 9000, training loss: 12447.05, average training loss: 9418.76, base loss: 14402.33
[INFO 2017-06-28 14:38:39,318 main.py:51] epoch 9001, training loss: 8487.50, average training loss: 9418.98, base loss: 14402.46
[INFO 2017-06-28 14:38:40,403 main.py:51] epoch 9002, training loss: 9980.78, average training loss: 9418.81, base loss: 14401.21
[INFO 2017-06-28 14:38:41,409 main.py:51] epoch 9003, training loss: 9062.29, average training loss: 9415.91, base loss: 14397.05
[INFO 2017-06-28 14:38:42,470 main.py:51] epoch 9004, training loss: 8022.89, average training loss: 9415.30, base loss: 14396.12
[INFO 2017-06-28 14:38:43,529 main.py:51] epoch 9005, training loss: 10461.65, average training loss: 9415.82, base loss: 14395.95
[INFO 2017-06-28 14:38:44,572 main.py:51] epoch 9006, training loss: 9538.83, average training loss: 9415.73, base loss: 14395.51
[INFO 2017-06-28 14:38:45,624 main.py:51] epoch 9007, training loss: 8858.93, average training loss: 9414.70, base loss: 14394.15
[INFO 2017-06-28 14:38:46,664 main.py:51] epoch 9008, training loss: 9111.83, average training loss: 9414.17, base loss: 14394.86
[INFO 2017-06-28 14:38:47,713 main.py:51] epoch 9009, training loss: 9543.70, average training loss: 9414.29, base loss: 14394.12
[INFO 2017-06-28 14:38:48,854 main.py:51] epoch 9010, training loss: 9145.85, average training loss: 9412.81, base loss: 14391.98
[INFO 2017-06-28 14:38:49,835 main.py:51] epoch 9011, training loss: 9244.73, average training loss: 9412.55, base loss: 14391.24
[INFO 2017-06-28 14:38:50,870 main.py:51] epoch 9012, training loss: 9116.10, average training loss: 9411.81, base loss: 14387.31
[INFO 2017-06-28 14:38:51,903 main.py:51] epoch 9013, training loss: 9725.67, average training loss: 9411.93, base loss: 14387.72
[INFO 2017-06-28 14:38:52,914 main.py:51] epoch 9014, training loss: 7798.55, average training loss: 9410.52, base loss: 14384.21
[INFO 2017-06-28 14:38:53,964 main.py:51] epoch 9015, training loss: 8122.92, average training loss: 9408.59, base loss: 14382.52
[INFO 2017-06-28 14:38:54,998 main.py:51] epoch 9016, training loss: 9126.33, average training loss: 9408.55, base loss: 14382.32
[INFO 2017-06-28 14:38:55,994 main.py:51] epoch 9017, training loss: 9251.35, average training loss: 9408.28, base loss: 14380.46
[INFO 2017-06-28 14:38:57,015 main.py:51] epoch 9018, training loss: 10209.07, average training loss: 9408.84, base loss: 14382.23
[INFO 2017-06-28 14:38:58,060 main.py:51] epoch 9019, training loss: 10125.85, average training loss: 9410.51, base loss: 14385.61
[INFO 2017-06-28 14:38:59,068 main.py:51] epoch 9020, training loss: 8631.02, average training loss: 9410.13, base loss: 14385.65
[INFO 2017-06-28 14:39:00,015 main.py:51] epoch 9021, training loss: 9548.86, average training loss: 9409.82, base loss: 14384.90
[INFO 2017-06-28 14:39:00,918 main.py:51] epoch 9022, training loss: 10052.47, average training loss: 9409.94, base loss: 14385.84
[INFO 2017-06-28 14:39:01,829 main.py:51] epoch 9023, training loss: 9426.13, average training loss: 9410.37, base loss: 14387.96
[INFO 2017-06-28 14:39:02,822 main.py:51] epoch 9024, training loss: 9339.68, average training loss: 9410.55, base loss: 14388.75
[INFO 2017-06-28 14:39:03,870 main.py:51] epoch 9025, training loss: 8432.46, average training loss: 9409.07, base loss: 14388.13
[INFO 2017-06-28 14:39:04,914 main.py:51] epoch 9026, training loss: 8963.04, average training loss: 9408.54, base loss: 14386.82
[INFO 2017-06-28 14:39:06,015 main.py:51] epoch 9027, training loss: 9073.30, average training loss: 9406.72, base loss: 14383.97
[INFO 2017-06-28 14:39:07,093 main.py:51] epoch 9028, training loss: 9410.09, average training loss: 9404.55, base loss: 14380.52
[INFO 2017-06-28 14:39:08,114 main.py:51] epoch 9029, training loss: 10628.83, average training loss: 9405.40, base loss: 14379.98
[INFO 2017-06-28 14:39:09,158 main.py:51] epoch 9030, training loss: 9561.52, average training loss: 9405.38, base loss: 14381.46
[INFO 2017-06-28 14:39:10,204 main.py:51] epoch 9031, training loss: 9455.17, average training loss: 9406.28, base loss: 14380.77
[INFO 2017-06-28 14:39:11,270 main.py:51] epoch 9032, training loss: 9976.39, average training loss: 9406.92, base loss: 14381.27
[INFO 2017-06-28 14:39:12,389 main.py:51] epoch 9033, training loss: 9169.20, average training loss: 9406.74, base loss: 14379.16
[INFO 2017-06-28 14:39:13,409 main.py:51] epoch 9034, training loss: 9556.97, average training loss: 9406.80, base loss: 14379.97
[INFO 2017-06-28 14:39:14,507 main.py:51] epoch 9035, training loss: 8214.59, average training loss: 9405.96, base loss: 14378.78
[INFO 2017-06-28 14:39:15,522 main.py:51] epoch 9036, training loss: 9251.73, average training loss: 9405.37, base loss: 14377.31
[INFO 2017-06-28 14:39:16,563 main.py:51] epoch 9037, training loss: 10711.50, average training loss: 9405.93, base loss: 14380.36
[INFO 2017-06-28 14:39:17,589 main.py:51] epoch 9038, training loss: 8831.91, average training loss: 9406.29, base loss: 14380.67
[INFO 2017-06-28 14:39:18,616 main.py:51] epoch 9039, training loss: 7836.10, average training loss: 9403.13, base loss: 14376.01
[INFO 2017-06-28 14:39:19,610 main.py:51] epoch 9040, training loss: 8566.35, average training loss: 9403.26, base loss: 14376.31
[INFO 2017-06-28 14:39:20,622 main.py:51] epoch 9041, training loss: 8397.55, average training loss: 9400.45, base loss: 14371.64
[INFO 2017-06-28 14:39:21,688 main.py:51] epoch 9042, training loss: 8886.00, average training loss: 9400.25, base loss: 14370.98
[INFO 2017-06-28 14:39:22,772 main.py:51] epoch 9043, training loss: 9270.08, average training loss: 9400.26, base loss: 14371.00
[INFO 2017-06-28 14:39:23,897 main.py:51] epoch 9044, training loss: 10765.80, average training loss: 9402.24, base loss: 14373.29
[INFO 2017-06-28 14:39:24,908 main.py:51] epoch 9045, training loss: 9762.72, average training loss: 9403.31, base loss: 14374.41
[INFO 2017-06-28 14:39:25,963 main.py:51] epoch 9046, training loss: 10866.17, average training loss: 9405.41, base loss: 14375.52
[INFO 2017-06-28 14:39:26,995 main.py:51] epoch 9047, training loss: 9749.06, average training loss: 9407.33, base loss: 14378.46
[INFO 2017-06-28 14:39:28,068 main.py:51] epoch 9048, training loss: 9628.18, average training loss: 9407.58, base loss: 14379.63
[INFO 2017-06-28 14:39:29,141 main.py:51] epoch 9049, training loss: 9797.71, average training loss: 9407.78, base loss: 14378.78
[INFO 2017-06-28 14:39:30,180 main.py:51] epoch 9050, training loss: 10402.70, average training loss: 9409.60, base loss: 14381.84
[INFO 2017-06-28 14:39:31,242 main.py:51] epoch 9051, training loss: 9444.92, average training loss: 9411.00, base loss: 14385.45
[INFO 2017-06-28 14:39:32,266 main.py:51] epoch 9052, training loss: 9959.28, average training loss: 9411.57, base loss: 14387.33
[INFO 2017-06-28 14:39:33,259 main.py:51] epoch 9053, training loss: 10000.06, average training loss: 9411.64, base loss: 14388.04
[INFO 2017-06-28 14:39:34,287 main.py:51] epoch 9054, training loss: 8024.00, average training loss: 9410.79, base loss: 14386.93
[INFO 2017-06-28 14:39:35,330 main.py:51] epoch 9055, training loss: 8788.15, average training loss: 9409.58, base loss: 14385.85
[INFO 2017-06-28 14:39:36,390 main.py:51] epoch 9056, training loss: 10248.25, average training loss: 9409.83, base loss: 14385.63
[INFO 2017-06-28 14:39:37,483 main.py:51] epoch 9057, training loss: 9034.47, average training loss: 9409.90, base loss: 14386.41
[INFO 2017-06-28 14:39:38,506 main.py:51] epoch 9058, training loss: 8688.22, average training loss: 9410.39, base loss: 14387.17
[INFO 2017-06-28 14:39:39,562 main.py:51] epoch 9059, training loss: 8015.42, average training loss: 9409.15, base loss: 14386.46
[INFO 2017-06-28 14:39:40,578 main.py:51] epoch 9060, training loss: 10677.44, average training loss: 9410.94, base loss: 14388.07
[INFO 2017-06-28 14:39:41,626 main.py:51] epoch 9061, training loss: 8499.91, average training loss: 9410.22, base loss: 14387.28
[INFO 2017-06-28 14:39:42,684 main.py:51] epoch 9062, training loss: 9881.51, average training loss: 9411.18, base loss: 14388.46
[INFO 2017-06-28 14:39:43,702 main.py:51] epoch 9063, training loss: 9899.40, average training loss: 9411.83, base loss: 14388.73
[INFO 2017-06-28 14:39:44,785 main.py:51] epoch 9064, training loss: 10660.61, average training loss: 9411.79, base loss: 14389.41
[INFO 2017-06-28 14:39:45,863 main.py:51] epoch 9065, training loss: 9265.24, average training loss: 9411.11, base loss: 14388.03
[INFO 2017-06-28 14:39:46,889 main.py:51] epoch 9066, training loss: 10448.59, average training loss: 9412.14, base loss: 14388.24
[INFO 2017-06-28 14:39:47,917 main.py:51] epoch 9067, training loss: 10790.27, average training loss: 9414.54, base loss: 14390.97
[INFO 2017-06-28 14:39:49,006 main.py:51] epoch 9068, training loss: 9245.69, average training loss: 9413.61, base loss: 14389.38
[INFO 2017-06-28 14:39:50,050 main.py:51] epoch 9069, training loss: 10067.40, average training loss: 9414.57, base loss: 14390.00
[INFO 2017-06-28 14:39:51,095 main.py:51] epoch 9070, training loss: 8760.51, average training loss: 9413.18, base loss: 14388.55
[INFO 2017-06-28 14:39:52,129 main.py:51] epoch 9071, training loss: 9029.32, average training loss: 9412.92, base loss: 14386.69
[INFO 2017-06-28 14:39:53,202 main.py:51] epoch 9072, training loss: 8677.63, average training loss: 9411.02, base loss: 14382.94
[INFO 2017-06-28 14:39:54,328 main.py:51] epoch 9073, training loss: 8818.42, average training loss: 9410.05, base loss: 14381.85
[INFO 2017-06-28 14:39:55,302 main.py:51] epoch 9074, training loss: 8365.78, average training loss: 9410.13, base loss: 14382.91
[INFO 2017-06-28 14:39:56,384 main.py:51] epoch 9075, training loss: 9446.64, average training loss: 9410.48, base loss: 14384.59
[INFO 2017-06-28 14:39:57,508 main.py:51] epoch 9076, training loss: 8663.38, average training loss: 9409.74, base loss: 14384.13
[INFO 2017-06-28 14:39:58,499 main.py:51] epoch 9077, training loss: 9346.08, average training loss: 9409.16, base loss: 14382.41
[INFO 2017-06-28 14:39:59,545 main.py:51] epoch 9078, training loss: 9448.27, average training loss: 9409.45, base loss: 14383.10
[INFO 2017-06-28 14:40:00,632 main.py:51] epoch 9079, training loss: 9218.81, average training loss: 9409.38, base loss: 14383.04
[INFO 2017-06-28 14:40:01,720 main.py:51] epoch 9080, training loss: 8682.09, average training loss: 9407.78, base loss: 14381.32
[INFO 2017-06-28 14:40:02,795 main.py:51] epoch 9081, training loss: 8616.35, average training loss: 9407.12, base loss: 14380.53
[INFO 2017-06-28 14:40:03,818 main.py:51] epoch 9082, training loss: 10579.28, average training loss: 9406.94, base loss: 14380.73
[INFO 2017-06-28 14:40:04,896 main.py:51] epoch 9083, training loss: 8882.91, average training loss: 9406.22, base loss: 14380.79
[INFO 2017-06-28 14:40:05,989 main.py:51] epoch 9084, training loss: 9365.66, average training loss: 9406.06, base loss: 14380.35
[INFO 2017-06-28 14:40:06,999 main.py:51] epoch 9085, training loss: 8298.94, average training loss: 9404.99, base loss: 14381.02
[INFO 2017-06-28 14:40:08,031 main.py:51] epoch 9086, training loss: 10641.32, average training loss: 9406.64, base loss: 14384.49
[INFO 2017-06-28 14:40:09,065 main.py:51] epoch 9087, training loss: 9097.41, average training loss: 9405.81, base loss: 14383.04
[INFO 2017-06-28 14:40:10,095 main.py:51] epoch 9088, training loss: 10478.45, average training loss: 9407.24, base loss: 14385.41
[INFO 2017-06-28 14:40:11,123 main.py:51] epoch 9089, training loss: 8960.69, average training loss: 9406.47, base loss: 14384.32
[INFO 2017-06-28 14:40:12,142 main.py:51] epoch 9090, training loss: 8097.83, average training loss: 9405.18, base loss: 14382.70
[INFO 2017-06-28 14:40:13,232 main.py:51] epoch 9091, training loss: 8480.85, average training loss: 9405.17, base loss: 14383.86
[INFO 2017-06-28 14:40:14,364 main.py:51] epoch 9092, training loss: 10454.08, average training loss: 9404.82, base loss: 14382.91
[INFO 2017-06-28 14:40:15,408 main.py:51] epoch 9093, training loss: 9125.68, average training loss: 9404.14, base loss: 14383.38
[INFO 2017-06-28 14:40:16,452 main.py:51] epoch 9094, training loss: 9033.57, average training loss: 9403.25, base loss: 14381.23
[INFO 2017-06-28 14:40:17,492 main.py:51] epoch 9095, training loss: 10304.63, average training loss: 9403.22, base loss: 14381.01
[INFO 2017-06-28 14:40:18,569 main.py:51] epoch 9096, training loss: 9686.91, average training loss: 9402.99, base loss: 14381.96
[INFO 2017-06-28 14:40:19,505 main.py:51] epoch 9097, training loss: 10213.76, average training loss: 9403.92, base loss: 14383.30
[INFO 2017-06-28 14:40:20,396 main.py:51] epoch 9098, training loss: 9623.44, average training loss: 9404.45, base loss: 14384.39
[INFO 2017-06-28 14:40:21,298 main.py:51] epoch 9099, training loss: 10236.40, average training loss: 9404.45, base loss: 14385.39
[INFO 2017-06-28 14:40:21,298 main.py:53] epoch 9099, testing
[INFO 2017-06-28 14:40:25,091 main.py:105] average testing loss: 10488.79, base loss: 15057.64
[INFO 2017-06-28 14:40:25,091 main.py:106] improve_loss: 4568.85, improve_percent: 0.30
[INFO 2017-06-28 14:40:25,092 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:40:26,126 main.py:51] epoch 9100, training loss: 9326.77, average training loss: 9402.73, base loss: 14384.45
[INFO 2017-06-28 14:40:27,195 main.py:51] epoch 9101, training loss: 9954.59, average training loss: 9404.54, base loss: 14388.72
[INFO 2017-06-28 14:40:28,200 main.py:51] epoch 9102, training loss: 8753.91, average training loss: 9404.66, base loss: 14388.89
[INFO 2017-06-28 14:40:29,266 main.py:51] epoch 9103, training loss: 9980.10, average training loss: 9405.36, base loss: 14390.68
[INFO 2017-06-28 14:40:30,362 main.py:51] epoch 9104, training loss: 9662.27, average training loss: 9406.40, base loss: 14390.81
[INFO 2017-06-28 14:40:31,397 main.py:51] epoch 9105, training loss: 9432.73, average training loss: 9406.44, base loss: 14390.54
[INFO 2017-06-28 14:40:32,422 main.py:51] epoch 9106, training loss: 9891.63, average training loss: 9405.93, base loss: 14391.63
[INFO 2017-06-28 14:40:33,466 main.py:51] epoch 9107, training loss: 9433.78, average training loss: 9405.96, base loss: 14391.00
[INFO 2017-06-28 14:40:34,529 main.py:51] epoch 9108, training loss: 7668.92, average training loss: 9404.03, base loss: 14388.01
[INFO 2017-06-28 14:40:35,559 main.py:51] epoch 9109, training loss: 8523.15, average training loss: 9402.97, base loss: 14386.97
[INFO 2017-06-28 14:40:36,594 main.py:51] epoch 9110, training loss: 8389.58, average training loss: 9400.70, base loss: 14385.98
[INFO 2017-06-28 14:40:37,622 main.py:51] epoch 9111, training loss: 8952.75, average training loss: 9400.77, base loss: 14385.74
[INFO 2017-06-28 14:40:38,658 main.py:51] epoch 9112, training loss: 9335.10, average training loss: 9399.47, base loss: 14385.08
[INFO 2017-06-28 14:40:39,741 main.py:51] epoch 9113, training loss: 8116.44, average training loss: 9398.19, base loss: 14382.94
[INFO 2017-06-28 14:40:40,864 main.py:51] epoch 9114, training loss: 8083.92, average training loss: 9396.08, base loss: 14379.66
[INFO 2017-06-28 14:40:41,954 main.py:51] epoch 9115, training loss: 8039.77, average training loss: 9393.49, base loss: 14376.79
[INFO 2017-06-28 14:40:43,108 main.py:51] epoch 9116, training loss: 9315.29, average training loss: 9393.27, base loss: 14377.55
[INFO 2017-06-28 14:40:44,151 main.py:51] epoch 9117, training loss: 9141.99, average training loss: 9391.87, base loss: 14375.39
[INFO 2017-06-28 14:40:45,214 main.py:51] epoch 9118, training loss: 9704.20, average training loss: 9392.23, base loss: 14376.06
[INFO 2017-06-28 14:40:46,239 main.py:51] epoch 9119, training loss: 8799.81, average training loss: 9391.84, base loss: 14375.07
[INFO 2017-06-28 14:40:47,331 main.py:51] epoch 9120, training loss: 10330.94, average training loss: 9392.53, base loss: 14377.87
[INFO 2017-06-28 14:40:48,432 main.py:51] epoch 9121, training loss: 10233.58, average training loss: 9392.22, base loss: 14379.89
[INFO 2017-06-28 14:40:49,462 main.py:51] epoch 9122, training loss: 9509.49, average training loss: 9392.47, base loss: 14381.75
[INFO 2017-06-28 14:40:50,518 main.py:51] epoch 9123, training loss: 9217.08, average training loss: 9391.97, base loss: 14380.54
[INFO 2017-06-28 14:40:51,563 main.py:51] epoch 9124, training loss: 10261.26, average training loss: 9393.33, base loss: 14382.44
[INFO 2017-06-28 14:40:52,641 main.py:51] epoch 9125, training loss: 9929.14, average training loss: 9395.52, base loss: 14387.28
[INFO 2017-06-28 14:40:53,724 main.py:51] epoch 9126, training loss: 9790.81, average training loss: 9394.87, base loss: 14386.35
[INFO 2017-06-28 14:40:54,811 main.py:51] epoch 9127, training loss: 8865.29, average training loss: 9393.53, base loss: 14384.64
[INFO 2017-06-28 14:40:55,889 main.py:51] epoch 9128, training loss: 9719.54, average training loss: 9394.47, base loss: 14385.32
[INFO 2017-06-28 14:40:56,980 main.py:51] epoch 9129, training loss: 9231.29, average training loss: 9394.93, base loss: 14385.97
[INFO 2017-06-28 14:40:58,058 main.py:51] epoch 9130, training loss: 10351.00, average training loss: 9397.29, base loss: 14389.02
[INFO 2017-06-28 14:40:59,095 main.py:51] epoch 9131, training loss: 9128.43, average training loss: 9397.13, base loss: 14387.40
[INFO 2017-06-28 14:41:00,157 main.py:51] epoch 9132, training loss: 9525.87, average training loss: 9397.22, base loss: 14386.33
[INFO 2017-06-28 14:41:01,211 main.py:51] epoch 9133, training loss: 10046.02, average training loss: 9398.89, base loss: 14390.10
[INFO 2017-06-28 14:41:02,285 main.py:51] epoch 9134, training loss: 10017.39, average training loss: 9399.90, base loss: 14390.86
[INFO 2017-06-28 14:41:03,400 main.py:51] epoch 9135, training loss: 7949.54, average training loss: 9398.35, base loss: 14389.13
[INFO 2017-06-28 14:41:04,415 main.py:51] epoch 9136, training loss: 10179.80, average training loss: 9396.25, base loss: 14387.62
[INFO 2017-06-28 14:41:05,507 main.py:51] epoch 9137, training loss: 10682.54, average training loss: 9397.24, base loss: 14388.50
[INFO 2017-06-28 14:41:06,533 main.py:51] epoch 9138, training loss: 9210.72, average training loss: 9397.15, base loss: 14387.68
[INFO 2017-06-28 14:41:07,583 main.py:51] epoch 9139, training loss: 10071.99, average training loss: 9396.83, base loss: 14387.37
[INFO 2017-06-28 14:41:08,626 main.py:51] epoch 9140, training loss: 9460.47, average training loss: 9396.76, base loss: 14387.09
[INFO 2017-06-28 14:41:09,655 main.py:51] epoch 9141, training loss: 9091.53, average training loss: 9396.59, base loss: 14387.44
[INFO 2017-06-28 14:41:10,675 main.py:51] epoch 9142, training loss: 8330.40, average training loss: 9396.03, base loss: 14386.60
[INFO 2017-06-28 14:41:11,704 main.py:51] epoch 9143, training loss: 8878.41, average training loss: 9395.08, base loss: 14385.72
[INFO 2017-06-28 14:41:12,771 main.py:51] epoch 9144, training loss: 8922.82, average training loss: 9393.74, base loss: 14384.70
[INFO 2017-06-28 14:41:13,849 main.py:51] epoch 9145, training loss: 10293.51, average training loss: 9394.19, base loss: 14382.49
[INFO 2017-06-28 14:41:14,836 main.py:51] epoch 9146, training loss: 11678.95, average training loss: 9395.91, base loss: 14385.71
[INFO 2017-06-28 14:41:15,855 main.py:51] epoch 9147, training loss: 9680.20, average training loss: 9395.56, base loss: 14387.31
[INFO 2017-06-28 14:41:16,915 main.py:51] epoch 9148, training loss: 10392.79, average training loss: 9397.86, base loss: 14391.09
[INFO 2017-06-28 14:41:17,937 main.py:51] epoch 9149, training loss: 7621.13, average training loss: 9395.83, base loss: 14388.44
[INFO 2017-06-28 14:41:19,025 main.py:51] epoch 9150, training loss: 9621.52, average training loss: 9396.43, base loss: 14386.87
[INFO 2017-06-28 14:41:20,114 main.py:51] epoch 9151, training loss: 8548.42, average training loss: 9395.17, base loss: 14384.71
[INFO 2017-06-28 14:41:21,101 main.py:51] epoch 9152, training loss: 8885.57, average training loss: 9394.38, base loss: 14382.78
[INFO 2017-06-28 14:41:22,178 main.py:51] epoch 9153, training loss: 8967.09, average training loss: 9393.98, base loss: 14381.85
[INFO 2017-06-28 14:41:23,254 main.py:51] epoch 9154, training loss: 9098.22, average training loss: 9391.03, base loss: 14378.14
[INFO 2017-06-28 14:41:24,241 main.py:51] epoch 9155, training loss: 8579.80, average training loss: 9390.59, base loss: 14378.07
[INFO 2017-06-28 14:41:25,250 main.py:51] epoch 9156, training loss: 8940.87, average training loss: 9391.00, base loss: 14377.84
[INFO 2017-06-28 14:41:26,318 main.py:51] epoch 9157, training loss: 9790.81, average training loss: 9390.42, base loss: 14376.73
[INFO 2017-06-28 14:41:27,338 main.py:51] epoch 9158, training loss: 8952.42, average training loss: 9390.65, base loss: 14379.12
[INFO 2017-06-28 14:41:28,381 main.py:51] epoch 9159, training loss: 8727.84, average training loss: 9389.78, base loss: 14377.57
[INFO 2017-06-28 14:41:29,451 main.py:51] epoch 9160, training loss: 7889.77, average training loss: 9387.97, base loss: 14377.04
[INFO 2017-06-28 14:41:30,502 main.py:51] epoch 9161, training loss: 10974.84, average training loss: 9389.82, base loss: 14378.84
[INFO 2017-06-28 14:41:31,561 main.py:51] epoch 9162, training loss: 10398.26, average training loss: 9390.81, base loss: 14379.94
[INFO 2017-06-28 14:41:32,596 main.py:51] epoch 9163, training loss: 10842.76, average training loss: 9393.48, base loss: 14383.70
[INFO 2017-06-28 14:41:33,604 main.py:51] epoch 9164, training loss: 8512.02, average training loss: 9392.42, base loss: 14380.62
[INFO 2017-06-28 14:41:34,730 main.py:51] epoch 9165, training loss: 9140.74, average training loss: 9392.99, base loss: 14380.77
[INFO 2017-06-28 14:41:35,728 main.py:51] epoch 9166, training loss: 9338.68, average training loss: 9393.02, base loss: 14381.63
[INFO 2017-06-28 14:41:36,792 main.py:51] epoch 9167, training loss: 9706.98, average training loss: 9393.74, base loss: 14384.04
[INFO 2017-06-28 14:41:37,734 main.py:51] epoch 9168, training loss: 9299.83, average training loss: 9393.36, base loss: 14383.53
[INFO 2017-06-28 14:41:38,690 main.py:51] epoch 9169, training loss: 9344.96, average training loss: 9393.76, base loss: 14383.40
[INFO 2017-06-28 14:41:39,654 main.py:51] epoch 9170, training loss: 9904.60, average training loss: 9394.39, base loss: 14385.13
[INFO 2017-06-28 14:41:40,735 main.py:51] epoch 9171, training loss: 8361.18, average training loss: 9393.99, base loss: 14383.28
[INFO 2017-06-28 14:41:41,714 main.py:51] epoch 9172, training loss: 9840.90, average training loss: 9394.87, base loss: 14386.39
[INFO 2017-06-28 14:41:42,745 main.py:51] epoch 9173, training loss: 10027.01, average training loss: 9395.18, base loss: 14388.31
[INFO 2017-06-28 14:41:43,792 main.py:51] epoch 9174, training loss: 9243.83, average training loss: 9395.48, base loss: 14389.40
[INFO 2017-06-28 14:41:44,792 main.py:51] epoch 9175, training loss: 9500.53, average training loss: 9395.40, base loss: 14388.98
[INFO 2017-06-28 14:41:45,832 main.py:51] epoch 9176, training loss: 8205.65, average training loss: 9393.45, base loss: 14386.37
[INFO 2017-06-28 14:41:46,852 main.py:51] epoch 9177, training loss: 9084.88, average training loss: 9394.34, base loss: 14388.80
[INFO 2017-06-28 14:41:47,872 main.py:51] epoch 9178, training loss: 9745.62, average training loss: 9394.81, base loss: 14389.78
[INFO 2017-06-28 14:41:48,917 main.py:51] epoch 9179, training loss: 9061.85, average training loss: 9392.13, base loss: 14385.01
[INFO 2017-06-28 14:41:49,969 main.py:51] epoch 9180, training loss: 8903.99, average training loss: 9393.35, base loss: 14388.94
[INFO 2017-06-28 14:41:51,064 main.py:51] epoch 9181, training loss: 8564.91, average training loss: 9392.59, base loss: 14387.33
[INFO 2017-06-28 14:41:52,153 main.py:51] epoch 9182, training loss: 8728.52, average training loss: 9391.86, base loss: 14386.48
[INFO 2017-06-28 14:41:53,246 main.py:51] epoch 9183, training loss: 10207.80, average training loss: 9392.30, base loss: 14384.89
[INFO 2017-06-28 14:41:54,359 main.py:51] epoch 9184, training loss: 8095.91, average training loss: 9391.56, base loss: 14384.39
[INFO 2017-06-28 14:41:55,410 main.py:51] epoch 9185, training loss: 8170.12, average training loss: 9391.48, base loss: 14383.59
[INFO 2017-06-28 14:41:56,466 main.py:51] epoch 9186, training loss: 9885.65, average training loss: 9392.51, base loss: 14383.72
[INFO 2017-06-28 14:41:57,497 main.py:51] epoch 9187, training loss: 8410.57, average training loss: 9391.27, base loss: 14381.57
[INFO 2017-06-28 14:41:58,564 main.py:51] epoch 9188, training loss: 8929.44, average training loss: 9391.24, base loss: 14380.80
[INFO 2017-06-28 14:41:59,722 main.py:51] epoch 9189, training loss: 8526.36, average training loss: 9390.51, base loss: 14378.82
[INFO 2017-06-28 14:42:00,779 main.py:51] epoch 9190, training loss: 9450.79, average training loss: 9387.90, base loss: 14374.36
[INFO 2017-06-28 14:42:01,854 main.py:51] epoch 9191, training loss: 9107.40, average training loss: 9388.31, base loss: 14375.66
[INFO 2017-06-28 14:42:02,866 main.py:51] epoch 9192, training loss: 8279.92, average training loss: 9386.08, base loss: 14373.05
[INFO 2017-06-28 14:42:03,927 main.py:51] epoch 9193, training loss: 11842.39, average training loss: 9389.45, base loss: 14378.54
[INFO 2017-06-28 14:42:05,024 main.py:51] epoch 9194, training loss: 8827.14, average training loss: 9387.90, base loss: 14377.21
[INFO 2017-06-28 14:42:06,004 main.py:51] epoch 9195, training loss: 8826.80, average training loss: 9388.43, base loss: 14378.22
[INFO 2017-06-28 14:42:07,061 main.py:51] epoch 9196, training loss: 9545.32, average training loss: 9388.22, base loss: 14377.18
[INFO 2017-06-28 14:42:08,098 main.py:51] epoch 9197, training loss: 9311.01, average training loss: 9387.41, base loss: 14375.70
[INFO 2017-06-28 14:42:09,168 main.py:51] epoch 9198, training loss: 8672.52, average training loss: 9386.42, base loss: 14373.97
[INFO 2017-06-28 14:42:10,255 main.py:51] epoch 9199, training loss: 9109.22, average training loss: 9385.76, base loss: 14372.81
[INFO 2017-06-28 14:42:10,255 main.py:53] epoch 9199, testing
[INFO 2017-06-28 14:42:14,010 main.py:105] average testing loss: 10502.28, base loss: 15049.41
[INFO 2017-06-28 14:42:14,010 main.py:106] improve_loss: 4547.13, improve_percent: 0.30
[INFO 2017-06-28 14:42:14,011 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:42:15,094 main.py:51] epoch 9200, training loss: 9782.49, average training loss: 9386.51, base loss: 14375.03
[INFO 2017-06-28 14:42:16,095 main.py:51] epoch 9201, training loss: 8474.84, average training loss: 9384.14, base loss: 14372.87
[INFO 2017-06-28 14:42:17,109 main.py:51] epoch 9202, training loss: 10446.26, average training loss: 9384.21, base loss: 14372.48
[INFO 2017-06-28 14:42:18,188 main.py:51] epoch 9203, training loss: 10603.23, average training loss: 9385.85, base loss: 14373.86
[INFO 2017-06-28 14:42:19,193 main.py:51] epoch 9204, training loss: 9204.80, average training loss: 9386.57, base loss: 14375.13
[INFO 2017-06-28 14:42:20,249 main.py:51] epoch 9205, training loss: 10004.02, average training loss: 9388.01, base loss: 14377.88
[INFO 2017-06-28 14:42:21,291 main.py:51] epoch 9206, training loss: 10118.65, average training loss: 9388.01, base loss: 14377.79
[INFO 2017-06-28 14:42:22,378 main.py:51] epoch 9207, training loss: 7646.33, average training loss: 9387.03, base loss: 14376.05
[INFO 2017-06-28 14:42:23,484 main.py:51] epoch 9208, training loss: 9619.21, average training loss: 9387.46, base loss: 14376.67
[INFO 2017-06-28 14:42:24,487 main.py:51] epoch 9209, training loss: 8260.77, average training loss: 9386.59, base loss: 14376.12
[INFO 2017-06-28 14:42:25,548 main.py:51] epoch 9210, training loss: 9517.91, average training loss: 9386.29, base loss: 14374.68
[INFO 2017-06-28 14:42:26,621 main.py:51] epoch 9211, training loss: 8770.96, average training loss: 9384.86, base loss: 14372.34
[INFO 2017-06-28 14:42:27,630 main.py:51] epoch 9212, training loss: 8474.13, average training loss: 9383.01, base loss: 14370.75
[INFO 2017-06-28 14:42:28,676 main.py:51] epoch 9213, training loss: 9667.56, average training loss: 9381.84, base loss: 14368.39
[INFO 2017-06-28 14:42:29,706 main.py:51] epoch 9214, training loss: 9903.74, average training loss: 9382.78, base loss: 14370.01
[INFO 2017-06-28 14:42:30,791 main.py:51] epoch 9215, training loss: 10527.65, average training loss: 9383.73, base loss: 14373.15
[INFO 2017-06-28 14:42:31,882 main.py:51] epoch 9216, training loss: 8243.84, average training loss: 9381.73, base loss: 14369.71
[INFO 2017-06-28 14:42:32,908 main.py:51] epoch 9217, training loss: 9064.68, average training loss: 9381.39, base loss: 14367.93
[INFO 2017-06-28 14:42:33,939 main.py:51] epoch 9218, training loss: 9176.91, average training loss: 9380.80, base loss: 14367.81
[INFO 2017-06-28 14:42:35,022 main.py:51] epoch 9219, training loss: 9715.72, average training loss: 9379.34, base loss: 14365.29
[INFO 2017-06-28 14:42:36,022 main.py:51] epoch 9220, training loss: 9193.12, average training loss: 9379.69, base loss: 14366.26
[INFO 2017-06-28 14:42:37,059 main.py:51] epoch 9221, training loss: 9531.73, average training loss: 9380.05, base loss: 14365.66
[INFO 2017-06-28 14:42:38,087 main.py:51] epoch 9222, training loss: 10107.42, average training loss: 9380.54, base loss: 14365.67
[INFO 2017-06-28 14:42:39,167 main.py:51] epoch 9223, training loss: 9386.47, average training loss: 9380.20, base loss: 14363.82
[INFO 2017-06-28 14:42:40,244 main.py:51] epoch 9224, training loss: 9968.08, average training loss: 9381.32, base loss: 14366.64
[INFO 2017-06-28 14:42:41,293 main.py:51] epoch 9225, training loss: 9872.75, average training loss: 9380.67, base loss: 14367.14
[INFO 2017-06-28 14:42:42,339 main.py:51] epoch 9226, training loss: 10815.82, average training loss: 9383.00, base loss: 14371.19
[INFO 2017-06-28 14:42:43,344 main.py:51] epoch 9227, training loss: 9024.13, average training loss: 9383.85, base loss: 14372.95
[INFO 2017-06-28 14:42:44,357 main.py:51] epoch 9228, training loss: 8857.47, average training loss: 9383.58, base loss: 14374.53
[INFO 2017-06-28 14:42:45,460 main.py:51] epoch 9229, training loss: 9724.36, average training loss: 9384.86, base loss: 14378.71
[INFO 2017-06-28 14:42:46,512 main.py:51] epoch 9230, training loss: 10472.93, average training loss: 9386.15, base loss: 14380.05
[INFO 2017-06-28 14:42:47,557 main.py:51] epoch 9231, training loss: 9203.56, average training loss: 9387.25, base loss: 14382.93
[INFO 2017-06-28 14:42:48,588 main.py:51] epoch 9232, training loss: 8680.14, average training loss: 9387.07, base loss: 14382.07
[INFO 2017-06-28 14:42:49,662 main.py:51] epoch 9233, training loss: 9007.78, average training loss: 9385.36, base loss: 14378.49
[INFO 2017-06-28 14:42:50,718 main.py:51] epoch 9234, training loss: 8046.66, average training loss: 9382.97, base loss: 14374.65
[INFO 2017-06-28 14:42:51,706 main.py:51] epoch 9235, training loss: 8438.13, average training loss: 9381.77, base loss: 14371.87
[INFO 2017-06-28 14:42:52,681 main.py:51] epoch 9236, training loss: 8615.86, average training loss: 9381.30, base loss: 14370.38
[INFO 2017-06-28 14:42:53,684 main.py:51] epoch 9237, training loss: 9604.61, average training loss: 9382.36, base loss: 14371.84
[INFO 2017-06-28 14:42:54,735 main.py:51] epoch 9238, training loss: 10149.29, average training loss: 9383.99, base loss: 14374.32
[INFO 2017-06-28 14:42:55,705 main.py:51] epoch 9239, training loss: 7777.86, average training loss: 9381.92, base loss: 14369.84
[INFO 2017-06-28 14:42:56,648 main.py:51] epoch 9240, training loss: 9510.67, average training loss: 9382.54, base loss: 14371.04
[INFO 2017-06-28 14:42:57,623 main.py:51] epoch 9241, training loss: 11276.44, average training loss: 9383.91, base loss: 14370.59
[INFO 2017-06-28 14:42:58,684 main.py:51] epoch 9242, training loss: 9645.72, average training loss: 9383.46, base loss: 14370.53
[INFO 2017-06-28 14:42:59,793 main.py:51] epoch 9243, training loss: 9073.15, average training loss: 9382.89, base loss: 14367.97
[INFO 2017-06-28 14:43:00,831 main.py:51] epoch 9244, training loss: 9709.06, average training loss: 9382.90, base loss: 14366.61
[INFO 2017-06-28 14:43:01,848 main.py:51] epoch 9245, training loss: 8588.78, average training loss: 9382.12, base loss: 14365.07
[INFO 2017-06-28 14:43:02,895 main.py:51] epoch 9246, training loss: 9182.76, average training loss: 9381.40, base loss: 14364.41
[INFO 2017-06-28 14:43:03,971 main.py:51] epoch 9247, training loss: 9086.64, average training loss: 9381.67, base loss: 14364.41
[INFO 2017-06-28 14:43:05,088 main.py:51] epoch 9248, training loss: 9165.33, average training loss: 9380.55, base loss: 14363.93
[INFO 2017-06-28 14:43:06,096 main.py:51] epoch 9249, training loss: 9884.27, average training loss: 9381.34, base loss: 14364.52
[INFO 2017-06-28 14:43:07,184 main.py:51] epoch 9250, training loss: 9764.28, average training loss: 9380.94, base loss: 14362.43
[INFO 2017-06-28 14:43:08,279 main.py:51] epoch 9251, training loss: 8936.85, average training loss: 9380.60, base loss: 14361.90
[INFO 2017-06-28 14:43:09,315 main.py:51] epoch 9252, training loss: 9794.40, average training loss: 9380.25, base loss: 14360.74
[INFO 2017-06-28 14:43:10,358 main.py:51] epoch 9253, training loss: 9331.95, average training loss: 9381.32, base loss: 14362.93
[INFO 2017-06-28 14:43:11,468 main.py:51] epoch 9254, training loss: 10207.31, average training loss: 9382.25, base loss: 14365.96
[INFO 2017-06-28 14:43:12,458 main.py:51] epoch 9255, training loss: 8845.85, average training loss: 9381.78, base loss: 14365.38
[INFO 2017-06-28 14:43:13,492 main.py:51] epoch 9256, training loss: 8804.84, average training loss: 9381.32, base loss: 14364.89
[INFO 2017-06-28 14:43:14,561 main.py:51] epoch 9257, training loss: 8839.50, average training loss: 9379.50, base loss: 14364.31
[INFO 2017-06-28 14:43:15,580 main.py:51] epoch 9258, training loss: 9574.27, average training loss: 9380.09, base loss: 14362.74
[INFO 2017-06-28 14:43:16,637 main.py:51] epoch 9259, training loss: 10692.28, average training loss: 9382.26, base loss: 14366.30
[INFO 2017-06-28 14:43:17,659 main.py:51] epoch 9260, training loss: 9894.35, average training loss: 9383.40, base loss: 14367.52
[INFO 2017-06-28 14:43:18,721 main.py:51] epoch 9261, training loss: 9803.34, average training loss: 9383.49, base loss: 14366.58
[INFO 2017-06-28 14:43:19,803 main.py:51] epoch 9262, training loss: 10215.43, average training loss: 9384.21, base loss: 14368.73
[INFO 2017-06-28 14:43:20,816 main.py:51] epoch 9263, training loss: 9516.35, average training loss: 9384.22, base loss: 14369.47
[INFO 2017-06-28 14:43:21,875 main.py:51] epoch 9264, training loss: 9758.89, average training loss: 9383.24, base loss: 14368.59
[INFO 2017-06-28 14:43:22,984 main.py:51] epoch 9265, training loss: 10026.53, average training loss: 9384.83, base loss: 14372.00
[INFO 2017-06-28 14:43:24,013 main.py:51] epoch 9266, training loss: 10598.17, average training loss: 9386.82, base loss: 14375.02
[INFO 2017-06-28 14:43:25,063 main.py:51] epoch 9267, training loss: 8917.88, average training loss: 9386.86, base loss: 14376.57
[INFO 2017-06-28 14:43:26,089 main.py:51] epoch 9268, training loss: 8038.12, average training loss: 9385.84, base loss: 14374.25
[INFO 2017-06-28 14:43:27,183 main.py:51] epoch 9269, training loss: 8896.70, average training loss: 9385.64, base loss: 14374.33
[INFO 2017-06-28 14:43:28,232 main.py:51] epoch 9270, training loss: 10197.81, average training loss: 9386.81, base loss: 14375.57
[INFO 2017-06-28 14:43:29,246 main.py:51] epoch 9271, training loss: 11519.13, average training loss: 9387.43, base loss: 14376.60
[INFO 2017-06-28 14:43:30,312 main.py:51] epoch 9272, training loss: 10179.67, average training loss: 9387.84, base loss: 14377.86
[INFO 2017-06-28 14:43:31,438 main.py:51] epoch 9273, training loss: 8601.32, average training loss: 9388.17, base loss: 14378.23
[INFO 2017-06-28 14:43:32,457 main.py:51] epoch 9274, training loss: 10434.87, average training loss: 9389.26, base loss: 14378.53
[INFO 2017-06-28 14:43:33,492 main.py:51] epoch 9275, training loss: 7858.02, average training loss: 9388.14, base loss: 14377.33
[INFO 2017-06-28 14:43:34,561 main.py:51] epoch 9276, training loss: 8188.66, average training loss: 9386.20, base loss: 14375.30
[INFO 2017-06-28 14:43:35,588 main.py:51] epoch 9277, training loss: 8871.41, average training loss: 9385.62, base loss: 14374.07
[INFO 2017-06-28 14:43:36,615 main.py:51] epoch 9278, training loss: 8309.67, average training loss: 9384.82, base loss: 14373.43
[INFO 2017-06-28 14:43:37,670 main.py:51] epoch 9279, training loss: 8709.80, average training loss: 9384.00, base loss: 14371.64
[INFO 2017-06-28 14:43:38,703 main.py:51] epoch 9280, training loss: 9689.64, average training loss: 9383.73, base loss: 14371.90
[INFO 2017-06-28 14:43:39,748 main.py:51] epoch 9281, training loss: 10325.90, average training loss: 9383.55, base loss: 14372.92
[INFO 2017-06-28 14:43:40,802 main.py:51] epoch 9282, training loss: 8975.40, average training loss: 9382.57, base loss: 14372.27
[INFO 2017-06-28 14:43:41,867 main.py:51] epoch 9283, training loss: 10349.06, average training loss: 9383.73, base loss: 14374.33
[INFO 2017-06-28 14:43:42,980 main.py:51] epoch 9284, training loss: 8088.96, average training loss: 9382.31, base loss: 14371.92
[INFO 2017-06-28 14:43:44,021 main.py:51] epoch 9285, training loss: 10593.01, average training loss: 9383.62, base loss: 14373.68
[INFO 2017-06-28 14:43:45,035 main.py:51] epoch 9286, training loss: 8529.40, average training loss: 9383.46, base loss: 14374.12
[INFO 2017-06-28 14:43:46,114 main.py:51] epoch 9287, training loss: 9232.74, average training loss: 9383.91, base loss: 14375.90
[INFO 2017-06-28 14:43:47,143 main.py:51] epoch 9288, training loss: 11168.72, average training loss: 9383.68, base loss: 14377.73
[INFO 2017-06-28 14:43:48,140 main.py:51] epoch 9289, training loss: 9187.43, average training loss: 9383.31, base loss: 14377.21
[INFO 2017-06-28 14:43:49,176 main.py:51] epoch 9290, training loss: 9067.91, average training loss: 9383.21, base loss: 14378.45
[INFO 2017-06-28 14:43:50,267 main.py:51] epoch 9291, training loss: 9972.02, average training loss: 9383.26, base loss: 14378.77
[INFO 2017-06-28 14:43:51,385 main.py:51] epoch 9292, training loss: 9681.20, average training loss: 9382.93, base loss: 14379.22
[INFO 2017-06-28 14:43:52,375 main.py:51] epoch 9293, training loss: 9711.46, average training loss: 9380.85, base loss: 14375.77
[INFO 2017-06-28 14:43:53,410 main.py:51] epoch 9294, training loss: 8955.06, average training loss: 9379.52, base loss: 14375.80
[INFO 2017-06-28 14:43:54,449 main.py:51] epoch 9295, training loss: 10250.18, average training loss: 9381.00, base loss: 14377.98
[INFO 2017-06-28 14:43:55,503 main.py:51] epoch 9296, training loss: 9151.91, average training loss: 9380.90, base loss: 14377.78
[INFO 2017-06-28 14:43:56,567 main.py:51] epoch 9297, training loss: 8760.27, average training loss: 9378.84, base loss: 14374.67
[INFO 2017-06-28 14:43:57,572 main.py:51] epoch 9298, training loss: 9823.45, average training loss: 9379.29, base loss: 14376.79
[INFO 2017-06-28 14:43:58,661 main.py:51] epoch 9299, training loss: 9078.90, average training loss: 9379.36, base loss: 14378.41
[INFO 2017-06-28 14:43:58,661 main.py:53] epoch 9299, testing
[INFO 2017-06-28 14:44:02,382 main.py:105] average testing loss: 10665.98, base loss: 15253.49
[INFO 2017-06-28 14:44:02,382 main.py:106] improve_loss: 4587.52, improve_percent: 0.30
[INFO 2017-06-28 14:44:02,383 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:44:03,453 main.py:51] epoch 9300, training loss: 10456.09, average training loss: 9381.05, base loss: 14380.73
[INFO 2017-06-28 14:44:04,532 main.py:51] epoch 9301, training loss: 9374.29, average training loss: 9381.42, base loss: 14381.77
[INFO 2017-06-28 14:44:05,611 main.py:51] epoch 9302, training loss: 10068.88, average training loss: 9380.08, base loss: 14379.92
[INFO 2017-06-28 14:44:06,676 main.py:51] epoch 9303, training loss: 9226.12, average training loss: 9380.02, base loss: 14380.76
[INFO 2017-06-28 14:44:07,720 main.py:51] epoch 9304, training loss: 8936.09, average training loss: 9377.83, base loss: 14378.29
[INFO 2017-06-28 14:44:08,774 main.py:51] epoch 9305, training loss: 8353.64, average training loss: 9377.60, base loss: 14377.74
[INFO 2017-06-28 14:44:09,874 main.py:51] epoch 9306, training loss: 9519.14, average training loss: 9378.56, base loss: 14380.44
[INFO 2017-06-28 14:44:10,919 main.py:51] epoch 9307, training loss: 9913.09, average training loss: 9378.76, base loss: 14381.43
[INFO 2017-06-28 14:44:11,991 main.py:51] epoch 9308, training loss: 9702.73, average training loss: 9379.67, base loss: 14383.38
[INFO 2017-06-28 14:44:13,045 main.py:51] epoch 9309, training loss: 10068.09, average training loss: 9381.33, base loss: 14385.70
[INFO 2017-06-28 14:44:14,095 main.py:51] epoch 9310, training loss: 8063.22, average training loss: 9380.42, base loss: 14383.47
[INFO 2017-06-28 14:44:15,039 main.py:51] epoch 9311, training loss: 8519.93, average training loss: 9379.58, base loss: 14380.77
[INFO 2017-06-28 14:44:16,016 main.py:51] epoch 9312, training loss: 10075.07, average training loss: 9378.94, base loss: 14380.36
[INFO 2017-06-28 14:44:16,938 main.py:51] epoch 9313, training loss: 10366.56, average training loss: 9379.43, base loss: 14379.77
[INFO 2017-06-28 14:44:17,957 main.py:51] epoch 9314, training loss: 8815.66, average training loss: 9378.85, base loss: 14379.28
[INFO 2017-06-28 14:44:19,024 main.py:51] epoch 9315, training loss: 9332.15, average training loss: 9378.80, base loss: 14379.39
[INFO 2017-06-28 14:44:20,119 main.py:51] epoch 9316, training loss: 9760.57, average training loss: 9378.79, base loss: 14380.07
[INFO 2017-06-28 14:44:21,127 main.py:51] epoch 9317, training loss: 9251.78, average training loss: 9378.72, base loss: 14379.55
[INFO 2017-06-28 14:44:22,203 main.py:51] epoch 9318, training loss: 9052.66, average training loss: 9380.01, base loss: 14381.29
[INFO 2017-06-28 14:44:23,289 main.py:51] epoch 9319, training loss: 10597.99, average training loss: 9382.20, base loss: 14383.41
[INFO 2017-06-28 14:44:24,391 main.py:51] epoch 9320, training loss: 8795.49, average training loss: 9380.39, base loss: 14380.25
[INFO 2017-06-28 14:44:25,484 main.py:51] epoch 9321, training loss: 10442.14, average training loss: 9382.04, base loss: 14382.69
[INFO 2017-06-28 14:44:26,506 main.py:51] epoch 9322, training loss: 9012.14, average training loss: 9381.42, base loss: 14383.17
[INFO 2017-06-28 14:44:27,565 main.py:51] epoch 9323, training loss: 10197.24, average training loss: 9382.31, base loss: 14384.83
[INFO 2017-06-28 14:44:28,585 main.py:51] epoch 9324, training loss: 9110.94, average training loss: 9381.89, base loss: 14382.71
[INFO 2017-06-28 14:44:29,663 main.py:51] epoch 9325, training loss: 9904.23, average training loss: 9380.86, base loss: 14382.57
[INFO 2017-06-28 14:44:30,802 main.py:51] epoch 9326, training loss: 8513.11, average training loss: 9381.33, base loss: 14383.13
[INFO 2017-06-28 14:44:31,887 main.py:51] epoch 9327, training loss: 8787.86, average training loss: 9380.64, base loss: 14381.91
[INFO 2017-06-28 14:44:32,968 main.py:51] epoch 9328, training loss: 8097.27, average training loss: 9380.32, base loss: 14381.49
[INFO 2017-06-28 14:44:33,949 main.py:51] epoch 9329, training loss: 9304.22, average training loss: 9380.85, base loss: 14382.27
[INFO 2017-06-28 14:44:35,012 main.py:51] epoch 9330, training loss: 9513.55, average training loss: 9381.20, base loss: 14383.48
[INFO 2017-06-28 14:44:36,080 main.py:51] epoch 9331, training loss: 8930.96, average training loss: 9380.56, base loss: 14382.97
[INFO 2017-06-28 14:44:37,101 main.py:51] epoch 9332, training loss: 10293.01, average training loss: 9380.31, base loss: 14383.61
[INFO 2017-06-28 14:44:38,190 main.py:51] epoch 9333, training loss: 9588.48, average training loss: 9379.25, base loss: 14381.75
[INFO 2017-06-28 14:44:39,305 main.py:51] epoch 9334, training loss: 7985.36, average training loss: 9376.91, base loss: 14376.69
[INFO 2017-06-28 14:44:40,359 main.py:51] epoch 9335, training loss: 8774.54, average training loss: 9376.70, base loss: 14377.21
[INFO 2017-06-28 14:44:41,427 main.py:51] epoch 9336, training loss: 9530.23, average training loss: 9376.86, base loss: 14376.97
[INFO 2017-06-28 14:44:42,464 main.py:51] epoch 9337, training loss: 9379.06, average training loss: 9376.47, base loss: 14375.82
[INFO 2017-06-28 14:44:43,511 main.py:51] epoch 9338, training loss: 9421.85, average training loss: 9377.17, base loss: 14376.44
[INFO 2017-06-28 14:44:44,584 main.py:51] epoch 9339, training loss: 8879.51, average training loss: 9377.81, base loss: 14377.37
[INFO 2017-06-28 14:44:45,611 main.py:51] epoch 9340, training loss: 9179.54, average training loss: 9378.98, base loss: 14378.89
[INFO 2017-06-28 14:44:46,670 main.py:51] epoch 9341, training loss: 8761.33, average training loss: 9377.46, base loss: 14376.34
[INFO 2017-06-28 14:44:47,667 main.py:51] epoch 9342, training loss: 8958.60, average training loss: 9377.56, base loss: 14375.72
[INFO 2017-06-28 14:44:48,725 main.py:51] epoch 9343, training loss: 11252.13, average training loss: 9379.00, base loss: 14376.62
[INFO 2017-06-28 14:44:49,811 main.py:51] epoch 9344, training loss: 8974.44, average training loss: 9378.06, base loss: 14375.52
[INFO 2017-06-28 14:44:50,821 main.py:51] epoch 9345, training loss: 9464.08, average training loss: 9378.17, base loss: 14376.44
[INFO 2017-06-28 14:44:51,831 main.py:51] epoch 9346, training loss: 8699.45, average training loss: 9377.74, base loss: 14376.25
[INFO 2017-06-28 14:44:52,853 main.py:51] epoch 9347, training loss: 9843.21, average training loss: 9376.46, base loss: 14374.44
[INFO 2017-06-28 14:44:53,943 main.py:51] epoch 9348, training loss: 9100.92, average training loss: 9375.24, base loss: 14371.62
[INFO 2017-06-28 14:44:55,049 main.py:51] epoch 9349, training loss: 8436.22, average training loss: 9374.14, base loss: 14369.35
[INFO 2017-06-28 14:44:56,049 main.py:51] epoch 9350, training loss: 9562.30, average training loss: 9373.81, base loss: 14367.38
[INFO 2017-06-28 14:44:57,111 main.py:51] epoch 9351, training loss: 8784.33, average training loss: 9374.34, base loss: 14367.90
[INFO 2017-06-28 14:44:58,228 main.py:51] epoch 9352, training loss: 9741.72, average training loss: 9375.41, base loss: 14368.20
[INFO 2017-06-28 14:44:59,270 main.py:51] epoch 9353, training loss: 10008.61, average training loss: 9374.71, base loss: 14365.89
[INFO 2017-06-28 14:45:00,301 main.py:51] epoch 9354, training loss: 9365.62, average training loss: 9374.87, base loss: 14368.17
[INFO 2017-06-28 14:45:01,354 main.py:51] epoch 9355, training loss: 8936.17, average training loss: 9374.01, base loss: 14367.33
[INFO 2017-06-28 14:45:02,439 main.py:51] epoch 9356, training loss: 8905.64, average training loss: 9375.15, base loss: 14368.63
[INFO 2017-06-28 14:45:03,539 main.py:51] epoch 9357, training loss: 10866.17, average training loss: 9376.11, base loss: 14371.09
[INFO 2017-06-28 14:45:04,538 main.py:51] epoch 9358, training loss: 10149.80, average training loss: 9376.44, base loss: 14370.83
[INFO 2017-06-28 14:45:05,557 main.py:51] epoch 9359, training loss: 9661.11, average training loss: 9377.75, base loss: 14372.78
[INFO 2017-06-28 14:45:06,640 main.py:51] epoch 9360, training loss: 8694.18, average training loss: 9377.28, base loss: 14371.82
[INFO 2017-06-28 14:45:07,633 main.py:51] epoch 9361, training loss: 8244.68, average training loss: 9375.65, base loss: 14368.02
[INFO 2017-06-28 14:45:08,638 main.py:51] epoch 9362, training loss: 9122.66, average training loss: 9375.22, base loss: 14368.38
[INFO 2017-06-28 14:45:09,698 main.py:51] epoch 9363, training loss: 8486.87, average training loss: 9374.46, base loss: 14366.36
[INFO 2017-06-28 14:45:10,722 main.py:51] epoch 9364, training loss: 9210.78, average training loss: 9374.51, base loss: 14364.97
[INFO 2017-06-28 14:45:11,783 main.py:51] epoch 9365, training loss: 9686.53, average training loss: 9375.75, base loss: 14368.09
[INFO 2017-06-28 14:45:12,871 main.py:51] epoch 9366, training loss: 10680.40, average training loss: 9378.11, base loss: 14372.53
[INFO 2017-06-28 14:45:13,936 main.py:51] epoch 9367, training loss: 8918.12, average training loss: 9378.89, base loss: 14374.13
[INFO 2017-06-28 14:45:15,024 main.py:51] epoch 9368, training loss: 9624.60, average training loss: 9379.05, base loss: 14376.95
[INFO 2017-06-28 14:45:16,062 main.py:51] epoch 9369, training loss: 10366.13, average training loss: 9380.37, base loss: 14381.29
[INFO 2017-06-28 14:45:17,088 main.py:51] epoch 9370, training loss: 9190.02, average training loss: 9381.01, base loss: 14382.85
[INFO 2017-06-28 14:45:18,182 main.py:51] epoch 9371, training loss: 8513.89, average training loss: 9379.78, base loss: 14378.74
[INFO 2017-06-28 14:45:19,150 main.py:51] epoch 9372, training loss: 8413.07, average training loss: 9379.59, base loss: 14377.36
[INFO 2017-06-28 14:45:20,217 main.py:51] epoch 9373, training loss: 9351.34, average training loss: 9379.72, base loss: 14377.99
[INFO 2017-06-28 14:45:21,314 main.py:51] epoch 9374, training loss: 9604.13, average training loss: 9380.30, base loss: 14380.13
[INFO 2017-06-28 14:45:22,323 main.py:51] epoch 9375, training loss: 9841.19, average training loss: 9381.47, base loss: 14381.66
[INFO 2017-06-28 14:45:23,363 main.py:51] epoch 9376, training loss: 9172.39, average training loss: 9381.72, base loss: 14382.12
[INFO 2017-06-28 14:45:24,402 main.py:51] epoch 9377, training loss: 8428.87, average training loss: 9381.60, base loss: 14380.92
[INFO 2017-06-28 14:45:25,478 main.py:51] epoch 9378, training loss: 9199.47, average training loss: 9382.37, base loss: 14381.08
[INFO 2017-06-28 14:45:26,587 main.py:51] epoch 9379, training loss: 9990.53, average training loss: 9384.33, base loss: 14384.08
[INFO 2017-06-28 14:45:27,589 main.py:51] epoch 9380, training loss: 10840.06, average training loss: 9384.33, base loss: 14385.10
[INFO 2017-06-28 14:45:28,681 main.py:51] epoch 9381, training loss: 10446.67, average training loss: 9385.12, base loss: 14387.19
[INFO 2017-06-28 14:45:29,725 main.py:51] epoch 9382, training loss: 9232.15, average training loss: 9385.45, base loss: 14386.64
[INFO 2017-06-28 14:45:30,802 main.py:51] epoch 9383, training loss: 8976.34, average training loss: 9385.01, base loss: 14385.89
[INFO 2017-06-28 14:45:31,867 main.py:51] epoch 9384, training loss: 9239.70, average training loss: 9385.69, base loss: 14386.54
[INFO 2017-06-28 14:45:32,899 main.py:51] epoch 9385, training loss: 9002.11, average training loss: 9384.76, base loss: 14384.37
[INFO 2017-06-28 14:45:33,849 main.py:51] epoch 9386, training loss: 8689.37, average training loss: 9384.27, base loss: 14383.81
[INFO 2017-06-28 14:45:34,783 main.py:51] epoch 9387, training loss: 9001.40, average training loss: 9384.75, base loss: 14384.76
[INFO 2017-06-28 14:45:35,768 main.py:51] epoch 9388, training loss: 9656.75, average training loss: 9384.69, base loss: 14384.91
[INFO 2017-06-28 14:45:36,807 main.py:51] epoch 9389, training loss: 9291.51, average training loss: 9384.07, base loss: 14384.21
[INFO 2017-06-28 14:45:37,896 main.py:51] epoch 9390, training loss: 10943.82, average training loss: 9385.31, base loss: 14383.82
[INFO 2017-06-28 14:45:38,898 main.py:51] epoch 9391, training loss: 9000.04, average training loss: 9385.34, base loss: 14385.05
[INFO 2017-06-28 14:45:39,959 main.py:51] epoch 9392, training loss: 8070.44, average training loss: 9384.79, base loss: 14384.05
[INFO 2017-06-28 14:45:41,059 main.py:51] epoch 9393, training loss: 9071.56, average training loss: 9385.04, base loss: 14384.06
[INFO 2017-06-28 14:45:42,100 main.py:51] epoch 9394, training loss: 10165.15, average training loss: 9384.76, base loss: 14383.17
[INFO 2017-06-28 14:45:43,122 main.py:51] epoch 9395, training loss: 8527.76, average training loss: 9384.82, base loss: 14383.35
[INFO 2017-06-28 14:45:44,185 main.py:51] epoch 9396, training loss: 8538.88, average training loss: 9384.93, base loss: 14384.66
[INFO 2017-06-28 14:45:45,271 main.py:51] epoch 9397, training loss: 10576.71, average training loss: 9386.63, base loss: 14387.13
[INFO 2017-06-28 14:45:46,348 main.py:51] epoch 9398, training loss: 8558.53, average training loss: 9386.16, base loss: 14386.41
[INFO 2017-06-28 14:45:47,408 main.py:51] epoch 9399, training loss: 8811.17, average training loss: 9385.62, base loss: 14384.82
[INFO 2017-06-28 14:45:47,408 main.py:53] epoch 9399, testing
[INFO 2017-06-28 14:45:51,057 main.py:105] average testing loss: 10526.79, base loss: 14863.49
[INFO 2017-06-28 14:45:51,057 main.py:106] improve_loss: 4336.70, improve_percent: 0.29
[INFO 2017-06-28 14:45:51,058 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:45:52,089 main.py:51] epoch 9400, training loss: 8973.08, average training loss: 9384.45, base loss: 14383.55
[INFO 2017-06-28 14:45:53,163 main.py:51] epoch 9401, training loss: 8048.92, average training loss: 9382.91, base loss: 14381.61
[INFO 2017-06-28 14:45:54,265 main.py:51] epoch 9402, training loss: 9151.05, average training loss: 9383.93, base loss: 14382.28
[INFO 2017-06-28 14:45:55,258 main.py:51] epoch 9403, training loss: 8316.86, average training loss: 9383.10, base loss: 14377.92
[INFO 2017-06-28 14:45:56,302 main.py:51] epoch 9404, training loss: 10398.28, average training loss: 9383.63, base loss: 14377.91
[INFO 2017-06-28 14:45:57,400 main.py:51] epoch 9405, training loss: 9719.35, average training loss: 9385.38, base loss: 14381.41
[INFO 2017-06-28 14:45:58,482 main.py:51] epoch 9406, training loss: 9608.91, average training loss: 9386.03, base loss: 14384.10
[INFO 2017-06-28 14:45:59,560 main.py:51] epoch 9407, training loss: 8572.99, average training loss: 9386.07, base loss: 14384.06
[INFO 2017-06-28 14:46:00,591 main.py:51] epoch 9408, training loss: 7996.40, average training loss: 9383.26, base loss: 14380.07
[INFO 2017-06-28 14:46:01,639 main.py:51] epoch 9409, training loss: 10295.19, average training loss: 9385.45, base loss: 14383.61
[INFO 2017-06-28 14:46:02,771 main.py:51] epoch 9410, training loss: 8337.13, average training loss: 9383.90, base loss: 14381.40
[INFO 2017-06-28 14:46:03,791 main.py:51] epoch 9411, training loss: 9262.31, average training loss: 9383.44, base loss: 14380.82
[INFO 2017-06-28 14:46:04,838 main.py:51] epoch 9412, training loss: 8791.59, average training loss: 9383.56, base loss: 14380.65
[INFO 2017-06-28 14:46:05,891 main.py:51] epoch 9413, training loss: 9219.69, average training loss: 9382.28, base loss: 14377.44
[INFO 2017-06-28 14:46:06,960 main.py:51] epoch 9414, training loss: 11312.90, average training loss: 9384.27, base loss: 14380.96
[INFO 2017-06-28 14:46:08,085 main.py:51] epoch 9415, training loss: 9585.10, average training loss: 9383.23, base loss: 14378.95
[INFO 2017-06-28 14:46:09,090 main.py:51] epoch 9416, training loss: 10751.41, average training loss: 9385.08, base loss: 14382.03
[INFO 2017-06-28 14:46:10,137 main.py:51] epoch 9417, training loss: 10058.60, average training loss: 9385.87, base loss: 14383.05
[INFO 2017-06-28 14:46:11,215 main.py:51] epoch 9418, training loss: 10038.63, average training loss: 9388.55, base loss: 14387.01
[INFO 2017-06-28 14:46:12,313 main.py:51] epoch 9419, training loss: 9218.89, average training loss: 9386.94, base loss: 14385.21
[INFO 2017-06-28 14:46:13,394 main.py:51] epoch 9420, training loss: 8694.66, average training loss: 9386.58, base loss: 14384.25
[INFO 2017-06-28 14:46:14,408 main.py:51] epoch 9421, training loss: 9363.68, average training loss: 9386.67, base loss: 14384.91
[INFO 2017-06-28 14:46:15,479 main.py:51] epoch 9422, training loss: 8210.45, average training loss: 9386.24, base loss: 14385.18
[INFO 2017-06-28 14:46:16,585 main.py:51] epoch 9423, training loss: 10112.08, average training loss: 9386.22, base loss: 14384.89
[INFO 2017-06-28 14:46:17,682 main.py:51] epoch 9424, training loss: 9718.70, average training loss: 9387.27, base loss: 14386.20
[INFO 2017-06-28 14:46:18,799 main.py:51] epoch 9425, training loss: 8390.62, average training loss: 9386.58, base loss: 14385.34
[INFO 2017-06-28 14:46:19,807 main.py:51] epoch 9426, training loss: 10525.66, average training loss: 9388.36, base loss: 14389.32
[INFO 2017-06-28 14:46:20,852 main.py:51] epoch 9427, training loss: 8902.81, average training loss: 9387.58, base loss: 14387.47
[INFO 2017-06-28 14:46:21,939 main.py:51] epoch 9428, training loss: 10997.87, average training loss: 9388.85, base loss: 14388.49
[INFO 2017-06-28 14:46:22,956 main.py:51] epoch 9429, training loss: 8669.05, average training loss: 9386.90, base loss: 14386.92
[INFO 2017-06-28 14:46:23,980 main.py:51] epoch 9430, training loss: 11502.75, average training loss: 9389.33, base loss: 14390.78
[INFO 2017-06-28 14:46:25,070 main.py:51] epoch 9431, training loss: 7903.84, average training loss: 9388.29, base loss: 14388.66
[INFO 2017-06-28 14:46:26,170 main.py:51] epoch 9432, training loss: 11346.56, average training loss: 9391.24, base loss: 14395.90
[INFO 2017-06-28 14:46:27,293 main.py:51] epoch 9433, training loss: 10313.74, average training loss: 9392.68, base loss: 14396.13
[INFO 2017-06-28 14:46:28,332 main.py:51] epoch 9434, training loss: 10829.40, average training loss: 9394.47, base loss: 14401.62
[INFO 2017-06-28 14:46:29,369 main.py:51] epoch 9435, training loss: 9142.41, average training loss: 9394.51, base loss: 14402.96
[INFO 2017-06-28 14:46:30,402 main.py:51] epoch 9436, training loss: 9668.55, average training loss: 9395.58, base loss: 14404.51
[INFO 2017-06-28 14:46:31,488 main.py:51] epoch 9437, training loss: 8725.40, average training loss: 9395.23, base loss: 14404.64
[INFO 2017-06-28 14:46:32,553 main.py:51] epoch 9438, training loss: 10145.05, average training loss: 9396.33, base loss: 14406.06
[INFO 2017-06-28 14:46:33,602 main.py:51] epoch 9439, training loss: 8978.67, average training loss: 9395.93, base loss: 14405.43
[INFO 2017-06-28 14:46:34,682 main.py:51] epoch 9440, training loss: 9394.44, average training loss: 9395.34, base loss: 14404.79
[INFO 2017-06-28 14:46:35,707 main.py:51] epoch 9441, training loss: 8809.32, average training loss: 9393.33, base loss: 14402.90
[INFO 2017-06-28 14:46:36,769 main.py:51] epoch 9442, training loss: 9198.07, average training loss: 9392.86, base loss: 14400.87
[INFO 2017-06-28 14:46:37,847 main.py:51] epoch 9443, training loss: 8873.04, average training loss: 9392.95, base loss: 14401.51
[INFO 2017-06-28 14:46:38,891 main.py:51] epoch 9444, training loss: 10570.30, average training loss: 9393.84, base loss: 14406.03
[INFO 2017-06-28 14:46:39,968 main.py:51] epoch 9445, training loss: 8786.78, average training loss: 9392.44, base loss: 14403.59
[INFO 2017-06-28 14:46:40,992 main.py:51] epoch 9446, training loss: 8993.43, average training loss: 9391.57, base loss: 14402.70
[INFO 2017-06-28 14:46:42,055 main.py:51] epoch 9447, training loss: 9393.37, average training loss: 9391.17, base loss: 14402.81
[INFO 2017-06-28 14:46:43,122 main.py:51] epoch 9448, training loss: 9022.08, average training loss: 9389.02, base loss: 14400.71
[INFO 2017-06-28 14:46:44,148 main.py:51] epoch 9449, training loss: 8561.99, average training loss: 9387.62, base loss: 14397.41
[INFO 2017-06-28 14:46:45,199 main.py:51] epoch 9450, training loss: 10292.38, average training loss: 9388.87, base loss: 14398.28
[INFO 2017-06-28 14:46:46,212 main.py:51] epoch 9451, training loss: 8764.16, average training loss: 9388.41, base loss: 14396.75
[INFO 2017-06-28 14:46:47,290 main.py:51] epoch 9452, training loss: 8320.82, average training loss: 9388.51, base loss: 14395.73
[INFO 2017-06-28 14:46:48,388 main.py:51] epoch 9453, training loss: 9673.19, average training loss: 9389.27, base loss: 14393.66
[INFO 2017-06-28 14:46:49,475 main.py:51] epoch 9454, training loss: 9913.17, average training loss: 9390.03, base loss: 14396.77
[INFO 2017-06-28 14:46:50,590 main.py:51] epoch 9455, training loss: 8978.02, average training loss: 9389.61, base loss: 14396.71
[INFO 2017-06-28 14:46:51,641 main.py:51] epoch 9456, training loss: 8964.50, average training loss: 9389.49, base loss: 14397.19
[INFO 2017-06-28 14:46:52,555 main.py:51] epoch 9457, training loss: 9256.11, average training loss: 9389.16, base loss: 14397.52
[INFO 2017-06-28 14:46:53,479 main.py:51] epoch 9458, training loss: 8663.29, average training loss: 9388.17, base loss: 14395.93
[INFO 2017-06-28 14:46:54,451 main.py:51] epoch 9459, training loss: 9085.01, average training loss: 9388.69, base loss: 14396.71
[INFO 2017-06-28 14:46:55,523 main.py:51] epoch 9460, training loss: 8774.92, average training loss: 9386.74, base loss: 14393.93
[INFO 2017-06-28 14:46:56,626 main.py:51] epoch 9461, training loss: 10196.99, average training loss: 9387.84, base loss: 14396.32
[INFO 2017-06-28 14:46:57,618 main.py:51] epoch 9462, training loss: 9347.22, average training loss: 9387.08, base loss: 14397.95
[INFO 2017-06-28 14:46:58,653 main.py:51] epoch 9463, training loss: 10183.67, average training loss: 9389.37, base loss: 14402.31
[INFO 2017-06-28 14:46:59,725 main.py:51] epoch 9464, training loss: 8794.49, average training loss: 9388.23, base loss: 14400.91
[INFO 2017-06-28 14:47:00,794 main.py:51] epoch 9465, training loss: 8307.29, average training loss: 9387.14, base loss: 14397.88
[INFO 2017-06-28 14:47:01,843 main.py:51] epoch 9466, training loss: 9765.67, average training loss: 9388.80, base loss: 14400.76
[INFO 2017-06-28 14:47:02,857 main.py:51] epoch 9467, training loss: 8324.31, average training loss: 9387.33, base loss: 14399.06
[INFO 2017-06-28 14:47:03,926 main.py:51] epoch 9468, training loss: 8755.15, average training loss: 9386.95, base loss: 14398.40
[INFO 2017-06-28 14:47:05,022 main.py:51] epoch 9469, training loss: 9701.72, average training loss: 9386.84, base loss: 14400.33
[INFO 2017-06-28 14:47:06,046 main.py:51] epoch 9470, training loss: 9176.35, average training loss: 9386.96, base loss: 14402.03
[INFO 2017-06-28 14:47:07,093 main.py:51] epoch 9471, training loss: 9842.48, average training loss: 9388.46, base loss: 14404.74
[INFO 2017-06-28 14:47:08,185 main.py:51] epoch 9472, training loss: 9630.30, average training loss: 9389.12, base loss: 14406.80
[INFO 2017-06-28 14:47:09,194 main.py:51] epoch 9473, training loss: 9427.15, average training loss: 9389.86, base loss: 14408.98
[INFO 2017-06-28 14:47:10,228 main.py:51] epoch 9474, training loss: 8985.62, average training loss: 9390.21, base loss: 14410.68
[INFO 2017-06-28 14:47:11,298 main.py:51] epoch 9475, training loss: 8099.83, average training loss: 9388.95, base loss: 14408.38
[INFO 2017-06-28 14:47:12,375 main.py:51] epoch 9476, training loss: 8981.97, average training loss: 9388.18, base loss: 14407.47
[INFO 2017-06-28 14:47:13,389 main.py:51] epoch 9477, training loss: 9861.34, average training loss: 9389.50, base loss: 14408.89
[INFO 2017-06-28 14:47:14,413 main.py:51] epoch 9478, training loss: 8611.38, average training loss: 9389.08, base loss: 14409.33
[INFO 2017-06-28 14:47:15,501 main.py:51] epoch 9479, training loss: 10027.26, average training loss: 9390.98, base loss: 14413.45
[INFO 2017-06-28 14:47:16,625 main.py:51] epoch 9480, training loss: 8399.26, average training loss: 9390.88, base loss: 14414.83
[INFO 2017-06-28 14:47:17,696 main.py:51] epoch 9481, training loss: 11483.71, average training loss: 9391.43, base loss: 14417.04
[INFO 2017-06-28 14:47:18,735 main.py:51] epoch 9482, training loss: 11960.64, average training loss: 9393.48, base loss: 14418.57
[INFO 2017-06-28 14:47:19,778 main.py:51] epoch 9483, training loss: 9792.60, average training loss: 9394.48, base loss: 14420.55
[INFO 2017-06-28 14:47:20,848 main.py:51] epoch 9484, training loss: 10309.33, average training loss: 9395.07, base loss: 14420.26
[INFO 2017-06-28 14:47:21,930 main.py:51] epoch 9485, training loss: 8157.44, average training loss: 9393.21, base loss: 14418.66
[INFO 2017-06-28 14:47:22,933 main.py:51] epoch 9486, training loss: 10265.29, average training loss: 9393.85, base loss: 14418.83
[INFO 2017-06-28 14:47:24,027 main.py:51] epoch 9487, training loss: 7964.99, average training loss: 9392.83, base loss: 14415.13
[INFO 2017-06-28 14:47:25,126 main.py:51] epoch 9488, training loss: 9121.19, average training loss: 9391.98, base loss: 14414.14
[INFO 2017-06-28 14:47:26,216 main.py:51] epoch 9489, training loss: 9452.07, average training loss: 9391.55, base loss: 14415.65
[INFO 2017-06-28 14:47:27,338 main.py:51] epoch 9490, training loss: 9100.35, average training loss: 9390.63, base loss: 14414.22
[INFO 2017-06-28 14:47:28,316 main.py:51] epoch 9491, training loss: 9795.84, average training loss: 9390.91, base loss: 14416.38
[INFO 2017-06-28 14:47:29,375 main.py:51] epoch 9492, training loss: 9033.34, average training loss: 9388.89, base loss: 14413.92
[INFO 2017-06-28 14:47:30,433 main.py:51] epoch 9493, training loss: 9902.15, average training loss: 9389.32, base loss: 14414.21
[INFO 2017-06-28 14:47:31,484 main.py:51] epoch 9494, training loss: 9097.19, average training loss: 9389.36, base loss: 14414.81
[INFO 2017-06-28 14:47:32,532 main.py:51] epoch 9495, training loss: 8937.68, average training loss: 9388.87, base loss: 14413.28
[INFO 2017-06-28 14:47:33,578 main.py:51] epoch 9496, training loss: 8838.99, average training loss: 9387.20, base loss: 14409.56
[INFO 2017-06-28 14:47:34,662 main.py:51] epoch 9497, training loss: 10234.67, average training loss: 9387.52, base loss: 14410.41
[INFO 2017-06-28 14:47:35,793 main.py:51] epoch 9498, training loss: 8996.60, average training loss: 9386.62, base loss: 14406.91
[INFO 2017-06-28 14:47:36,852 main.py:51] epoch 9499, training loss: 9666.72, average training loss: 9387.77, base loss: 14409.79
[INFO 2017-06-28 14:47:36,852 main.py:53] epoch 9499, testing
[INFO 2017-06-28 14:47:40,604 main.py:105] average testing loss: 10517.46, base loss: 14962.16
[INFO 2017-06-28 14:47:40,604 main.py:106] improve_loss: 4444.69, improve_percent: 0.30
[INFO 2017-06-28 14:47:40,605 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:47:41,655 main.py:51] epoch 9500, training loss: 8409.28, average training loss: 9387.03, base loss: 14410.52
[INFO 2017-06-28 14:47:42,799 main.py:51] epoch 9501, training loss: 10005.73, average training loss: 9387.56, base loss: 14409.23
[INFO 2017-06-28 14:47:43,842 main.py:51] epoch 9502, training loss: 8881.02, average training loss: 9387.86, base loss: 14408.57
[INFO 2017-06-28 14:47:44,873 main.py:51] epoch 9503, training loss: 9006.81, average training loss: 9386.17, base loss: 14405.89
[INFO 2017-06-28 14:47:45,898 main.py:51] epoch 9504, training loss: 9024.80, average training loss: 9386.31, base loss: 14408.36
[INFO 2017-06-28 14:47:46,976 main.py:51] epoch 9505, training loss: 9038.17, average training loss: 9385.42, base loss: 14405.34
[INFO 2017-06-28 14:47:48,052 main.py:51] epoch 9506, training loss: 8921.60, average training loss: 9385.03, base loss: 14405.20
[INFO 2017-06-28 14:47:49,067 main.py:51] epoch 9507, training loss: 9606.67, average training loss: 9385.17, base loss: 14403.72
[INFO 2017-06-28 14:47:50,147 main.py:51] epoch 9508, training loss: 9085.96, average training loss: 9385.70, base loss: 14404.19
[INFO 2017-06-28 14:47:51,225 main.py:51] epoch 9509, training loss: 9415.15, average training loss: 9384.67, base loss: 14402.32
[INFO 2017-06-28 14:47:52,250 main.py:51] epoch 9510, training loss: 8346.08, average training loss: 9381.97, base loss: 14398.08
[INFO 2017-06-28 14:47:53,285 main.py:51] epoch 9511, training loss: 11508.52, average training loss: 9383.51, base loss: 14399.23
[INFO 2017-06-28 14:47:54,332 main.py:51] epoch 9512, training loss: 9221.69, average training loss: 9382.12, base loss: 14395.89
[INFO 2017-06-28 14:47:55,430 main.py:51] epoch 9513, training loss: 8773.04, average training loss: 9382.25, base loss: 14395.73
[INFO 2017-06-28 14:47:56,531 main.py:51] epoch 9514, training loss: 9010.72, average training loss: 9381.29, base loss: 14394.28
[INFO 2017-06-28 14:47:57,556 main.py:51] epoch 9515, training loss: 9860.39, average training loss: 9379.19, base loss: 14393.57
[INFO 2017-06-28 14:47:58,586 main.py:51] epoch 9516, training loss: 8270.78, average training loss: 9376.48, base loss: 14389.47
[INFO 2017-06-28 14:47:59,668 main.py:51] epoch 9517, training loss: 9997.26, average training loss: 9376.66, base loss: 14390.25
[INFO 2017-06-28 14:48:00,691 main.py:51] epoch 9518, training loss: 8257.56, average training loss: 9375.47, base loss: 14389.02
[INFO 2017-06-28 14:48:01,731 main.py:51] epoch 9519, training loss: 10113.38, average training loss: 9375.98, base loss: 14390.59
[INFO 2017-06-28 14:48:02,758 main.py:51] epoch 9520, training loss: 9523.84, average training loss: 9376.35, base loss: 14391.48
[INFO 2017-06-28 14:48:03,772 main.py:51] epoch 9521, training loss: 10760.33, average training loss: 9376.90, base loss: 14391.23
[INFO 2017-06-28 14:48:04,806 main.py:51] epoch 9522, training loss: 8909.54, average training loss: 9376.29, base loss: 14389.93
[INFO 2017-06-28 14:48:05,845 main.py:51] epoch 9523, training loss: 9723.38, average training loss: 9376.71, base loss: 14389.54
[INFO 2017-06-28 14:48:06,911 main.py:51] epoch 9524, training loss: 9999.36, average training loss: 9377.60, base loss: 14389.20
[INFO 2017-06-28 14:48:08,028 main.py:51] epoch 9525, training loss: 8895.03, average training loss: 9377.12, base loss: 14388.52
[INFO 2017-06-28 14:48:09,042 main.py:51] epoch 9526, training loss: 8258.72, average training loss: 9375.88, base loss: 14385.45
[INFO 2017-06-28 14:48:10,100 main.py:51] epoch 9527, training loss: 8357.73, average training loss: 9375.81, base loss: 14384.47
[INFO 2017-06-28 14:48:11,178 main.py:51] epoch 9528, training loss: 9217.55, average training loss: 9374.88, base loss: 14384.64
[INFO 2017-06-28 14:48:12,113 main.py:51] epoch 9529, training loss: 9303.92, average training loss: 9375.34, base loss: 14385.64
[INFO 2017-06-28 14:48:13,051 main.py:51] epoch 9530, training loss: 9250.00, average training loss: 9375.25, base loss: 14385.77
[INFO 2017-06-28 14:48:13,979 main.py:51] epoch 9531, training loss: 8854.72, average training loss: 9375.99, base loss: 14387.95
[INFO 2017-06-28 14:48:15,000 main.py:51] epoch 9532, training loss: 8291.78, average training loss: 9373.35, base loss: 14384.60
[INFO 2017-06-28 14:48:16,064 main.py:51] epoch 9533, training loss: 8829.54, average training loss: 9372.54, base loss: 14382.94
[INFO 2017-06-28 14:48:17,155 main.py:51] epoch 9534, training loss: 8679.83, average training loss: 9373.11, base loss: 14384.33
[INFO 2017-06-28 14:48:18,236 main.py:51] epoch 9535, training loss: 8239.88, average training loss: 9371.63, base loss: 14381.02
[INFO 2017-06-28 14:48:19,318 main.py:51] epoch 9536, training loss: 10511.98, average training loss: 9372.56, base loss: 14382.69
[INFO 2017-06-28 14:48:20,315 main.py:51] epoch 9537, training loss: 9421.67, average training loss: 9372.55, base loss: 14382.90
[INFO 2017-06-28 14:48:21,406 main.py:51] epoch 9538, training loss: 10299.65, average training loss: 9373.08, base loss: 14384.67
[INFO 2017-06-28 14:48:22,514 main.py:51] epoch 9539, training loss: 8278.16, average training loss: 9371.48, base loss: 14382.27
[INFO 2017-06-28 14:48:23,554 main.py:51] epoch 9540, training loss: 10306.38, average training loss: 9372.88, base loss: 14384.85
[INFO 2017-06-28 14:48:24,613 main.py:51] epoch 9541, training loss: 8776.64, average training loss: 9372.65, base loss: 14384.94
[INFO 2017-06-28 14:48:25,648 main.py:51] epoch 9542, training loss: 10341.73, average training loss: 9374.45, base loss: 14387.12
[INFO 2017-06-28 14:48:26,697 main.py:51] epoch 9543, training loss: 9777.26, average training loss: 9374.18, base loss: 14387.93
[INFO 2017-06-28 14:48:27,782 main.py:51] epoch 9544, training loss: 9657.19, average training loss: 9374.61, base loss: 14388.54
[INFO 2017-06-28 14:48:28,860 main.py:51] epoch 9545, training loss: 9689.08, average training loss: 9372.51, base loss: 14385.41
[INFO 2017-06-28 14:48:29,952 main.py:51] epoch 9546, training loss: 8818.73, average training loss: 9372.69, base loss: 14385.87
[INFO 2017-06-28 14:48:30,947 main.py:51] epoch 9547, training loss: 10039.08, average training loss: 9373.42, base loss: 14385.13
[INFO 2017-06-28 14:48:32,028 main.py:51] epoch 9548, training loss: 9246.40, average training loss: 9374.24, base loss: 14387.24
[INFO 2017-06-28 14:48:33,126 main.py:51] epoch 9549, training loss: 8760.69, average training loss: 9373.04, base loss: 14387.02
[INFO 2017-06-28 14:48:34,133 main.py:51] epoch 9550, training loss: 9091.39, average training loss: 9374.00, base loss: 14389.80
[INFO 2017-06-28 14:48:35,208 main.py:51] epoch 9551, training loss: 10445.58, average training loss: 9374.93, base loss: 14390.62
[INFO 2017-06-28 14:48:36,236 main.py:51] epoch 9552, training loss: 9417.10, average training loss: 9374.98, base loss: 14390.47
[INFO 2017-06-28 14:48:37,335 main.py:51] epoch 9553, training loss: 10550.42, average training loss: 9375.96, base loss: 14391.68
[INFO 2017-06-28 14:48:38,452 main.py:51] epoch 9554, training loss: 9542.21, average training loss: 9375.79, base loss: 14391.29
[INFO 2017-06-28 14:48:39,569 main.py:51] epoch 9555, training loss: 9992.73, average training loss: 9377.32, base loss: 14393.07
[INFO 2017-06-28 14:48:40,676 main.py:51] epoch 9556, training loss: 10723.53, average training loss: 9377.07, base loss: 14393.66
[INFO 2017-06-28 14:48:41,710 main.py:51] epoch 9557, training loss: 8946.06, average training loss: 9377.73, base loss: 14394.28
[INFO 2017-06-28 14:48:42,787 main.py:51] epoch 9558, training loss: 9729.85, average training loss: 9375.43, base loss: 14389.87
[INFO 2017-06-28 14:48:43,829 main.py:51] epoch 9559, training loss: 8492.21, average training loss: 9374.26, base loss: 14388.17
[INFO 2017-06-28 14:48:44,888 main.py:51] epoch 9560, training loss: 9416.64, average training loss: 9374.80, base loss: 14390.07
[INFO 2017-06-28 14:48:46,007 main.py:51] epoch 9561, training loss: 8553.46, average training loss: 9374.76, base loss: 14389.89
[INFO 2017-06-28 14:48:47,043 main.py:51] epoch 9562, training loss: 9119.88, average training loss: 9374.89, base loss: 14389.63
[INFO 2017-06-28 14:48:48,096 main.py:51] epoch 9563, training loss: 9225.96, average training loss: 9373.32, base loss: 14386.65
[INFO 2017-06-28 14:48:49,146 main.py:51] epoch 9564, training loss: 10334.57, average training loss: 9374.62, base loss: 14387.92
[INFO 2017-06-28 14:48:50,194 main.py:51] epoch 9565, training loss: 8872.42, average training loss: 9375.37, base loss: 14390.05
[INFO 2017-06-28 14:48:51,297 main.py:51] epoch 9566, training loss: 8948.15, average training loss: 9375.07, base loss: 14390.62
[INFO 2017-06-28 14:48:52,367 main.py:51] epoch 9567, training loss: 9083.77, average training loss: 9374.64, base loss: 14390.45
[INFO 2017-06-28 14:48:53,485 main.py:51] epoch 9568, training loss: 9762.41, average training loss: 9374.75, base loss: 14392.55
[INFO 2017-06-28 14:48:54,535 main.py:51] epoch 9569, training loss: 9647.87, average training loss: 9374.33, base loss: 14393.39
[INFO 2017-06-28 14:48:55,589 main.py:51] epoch 9570, training loss: 8927.88, average training loss: 9374.59, base loss: 14394.57
[INFO 2017-06-28 14:48:56,633 main.py:51] epoch 9571, training loss: 10485.89, average training loss: 9375.65, base loss: 14398.14
[INFO 2017-06-28 14:48:57,706 main.py:51] epoch 9572, training loss: 8520.45, average training loss: 9375.21, base loss: 14398.01
[INFO 2017-06-28 14:48:58,841 main.py:51] epoch 9573, training loss: 8153.58, average training loss: 9374.90, base loss: 14397.91
[INFO 2017-06-28 14:48:59,919 main.py:51] epoch 9574, training loss: 8657.08, average training loss: 9374.74, base loss: 14396.27
[INFO 2017-06-28 14:49:00,980 main.py:51] epoch 9575, training loss: 8163.38, average training loss: 9373.71, base loss: 14395.50
[INFO 2017-06-28 14:49:02,004 main.py:51] epoch 9576, training loss: 9002.66, average training loss: 9374.08, base loss: 14397.08
[INFO 2017-06-28 14:49:03,084 main.py:51] epoch 9577, training loss: 10154.60, average training loss: 9375.39, base loss: 14399.62
[INFO 2017-06-28 14:49:04,184 main.py:51] epoch 9578, training loss: 9083.06, average training loss: 9373.97, base loss: 14397.68
[INFO 2017-06-28 14:49:05,191 main.py:51] epoch 9579, training loss: 9565.40, average training loss: 9374.12, base loss: 14399.77
[INFO 2017-06-28 14:49:06,217 main.py:51] epoch 9580, training loss: 8034.54, average training loss: 9372.98, base loss: 14398.17
[INFO 2017-06-28 14:49:07,280 main.py:51] epoch 9581, training loss: 12408.21, average training loss: 9376.84, base loss: 14405.30
[INFO 2017-06-28 14:49:08,337 main.py:51] epoch 9582, training loss: 8607.27, average training loss: 9376.22, base loss: 14403.82
[INFO 2017-06-28 14:49:09,480 main.py:51] epoch 9583, training loss: 9790.13, average training loss: 9376.53, base loss: 14401.87
[INFO 2017-06-28 14:49:10,587 main.py:51] epoch 9584, training loss: 9965.10, average training loss: 9376.67, base loss: 14403.77
[INFO 2017-06-28 14:49:11,671 main.py:51] epoch 9585, training loss: 9271.77, average training loss: 9376.24, base loss: 14402.87
[INFO 2017-06-28 14:49:12,663 main.py:51] epoch 9586, training loss: 7931.21, average training loss: 9373.62, base loss: 14399.21
[INFO 2017-06-28 14:49:13,749 main.py:51] epoch 9587, training loss: 9874.79, average training loss: 9375.25, base loss: 14403.13
[INFO 2017-06-28 14:49:14,842 main.py:51] epoch 9588, training loss: 10050.02, average training loss: 9376.24, base loss: 14403.69
[INFO 2017-06-28 14:49:15,853 main.py:51] epoch 9589, training loss: 10739.92, average training loss: 9379.02, base loss: 14410.09
[INFO 2017-06-28 14:49:16,881 main.py:51] epoch 9590, training loss: 8438.07, average training loss: 9378.54, base loss: 14408.71
[INFO 2017-06-28 14:49:17,949 main.py:51] epoch 9591, training loss: 9220.35, average training loss: 9378.83, base loss: 14409.85
[INFO 2017-06-28 14:49:19,062 main.py:51] epoch 9592, training loss: 9174.33, average training loss: 9378.04, base loss: 14409.70
[INFO 2017-06-28 14:49:20,152 main.py:51] epoch 9593, training loss: 9239.06, average training loss: 9378.09, base loss: 14410.86
[INFO 2017-06-28 14:49:21,172 main.py:51] epoch 9594, training loss: 8960.25, average training loss: 9376.86, base loss: 14409.65
[INFO 2017-06-28 14:49:22,212 main.py:51] epoch 9595, training loss: 9507.81, average training loss: 9377.68, base loss: 14410.59
[INFO 2017-06-28 14:49:23,298 main.py:51] epoch 9596, training loss: 8412.16, average training loss: 9377.89, base loss: 14410.98
[INFO 2017-06-28 14:49:24,385 main.py:51] epoch 9597, training loss: 8874.88, average training loss: 9377.88, base loss: 14409.83
[INFO 2017-06-28 14:49:25,504 main.py:51] epoch 9598, training loss: 9364.84, average training loss: 9377.29, base loss: 14408.50
[INFO 2017-06-28 14:49:26,547 main.py:51] epoch 9599, training loss: 10543.73, average training loss: 9378.71, base loss: 14409.03
[INFO 2017-06-28 14:49:26,547 main.py:53] epoch 9599, testing
[INFO 2017-06-28 14:49:30,223 main.py:105] average testing loss: 10485.55, base loss: 14680.00
[INFO 2017-06-28 14:49:30,224 main.py:106] improve_loss: 4194.45, improve_percent: 0.29
[INFO 2017-06-28 14:49:30,224 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:49:31,204 main.py:51] epoch 9600, training loss: 8484.93, average training loss: 9378.87, base loss: 14409.47
[INFO 2017-06-28 14:49:32,135 main.py:51] epoch 9601, training loss: 8276.26, average training loss: 9378.83, base loss: 14409.40
[INFO 2017-06-28 14:49:33,152 main.py:51] epoch 9602, training loss: 8963.39, average training loss: 9377.85, base loss: 14406.07
[INFO 2017-06-28 14:49:34,214 main.py:51] epoch 9603, training loss: 9495.15, average training loss: 9377.72, base loss: 14406.44
[INFO 2017-06-28 14:49:35,300 main.py:51] epoch 9604, training loss: 9519.30, average training loss: 9378.14, base loss: 14406.24
[INFO 2017-06-28 14:49:36,408 main.py:51] epoch 9605, training loss: 9957.48, average training loss: 9380.24, base loss: 14409.76
[INFO 2017-06-28 14:49:37,525 main.py:51] epoch 9606, training loss: 8911.50, average training loss: 9380.60, base loss: 14410.72
[INFO 2017-06-28 14:49:38,518 main.py:51] epoch 9607, training loss: 10360.23, average training loss: 9382.66, base loss: 14414.16
[INFO 2017-06-28 14:49:39,553 main.py:51] epoch 9608, training loss: 9415.24, average training loss: 9382.97, base loss: 14414.27
[INFO 2017-06-28 14:49:40,635 main.py:51] epoch 9609, training loss: 9409.14, average training loss: 9382.74, base loss: 14413.30
[INFO 2017-06-28 14:49:41,673 main.py:51] epoch 9610, training loss: 8925.69, average training loss: 9382.06, base loss: 14413.25
[INFO 2017-06-28 14:49:42,754 main.py:51] epoch 9611, training loss: 8571.66, average training loss: 9381.51, base loss: 14413.37
[INFO 2017-06-28 14:49:43,767 main.py:51] epoch 9612, training loss: 9506.71, average training loss: 9381.75, base loss: 14416.04
[INFO 2017-06-28 14:49:44,835 main.py:51] epoch 9613, training loss: 9017.96, average training loss: 9381.65, base loss: 14414.94
[INFO 2017-06-28 14:49:45,930 main.py:51] epoch 9614, training loss: 8549.20, average training loss: 9380.79, base loss: 14412.95
[INFO 2017-06-28 14:49:46,994 main.py:51] epoch 9615, training loss: 8878.21, average training loss: 9380.08, base loss: 14412.73
[INFO 2017-06-28 14:49:48,061 main.py:51] epoch 9616, training loss: 9625.06, average training loss: 9380.30, base loss: 14413.29
[INFO 2017-06-28 14:49:49,073 main.py:51] epoch 9617, training loss: 9796.94, average training loss: 9380.84, base loss: 14415.36
[INFO 2017-06-28 14:49:50,114 main.py:51] epoch 9618, training loss: 9795.95, average training loss: 9380.92, base loss: 14414.30
[INFO 2017-06-28 14:49:51,237 main.py:51] epoch 9619, training loss: 8198.04, average training loss: 9380.98, base loss: 14414.12
[INFO 2017-06-28 14:49:52,246 main.py:51] epoch 9620, training loss: 11610.79, average training loss: 9382.70, base loss: 14414.28
[INFO 2017-06-28 14:49:53,267 main.py:51] epoch 9621, training loss: 9212.12, average training loss: 9383.76, base loss: 14416.52
[INFO 2017-06-28 14:49:54,338 main.py:51] epoch 9622, training loss: 10248.82, average training loss: 9385.35, base loss: 14418.40
[INFO 2017-06-28 14:49:55,420 main.py:51] epoch 9623, training loss: 9237.88, average training loss: 9384.18, base loss: 14416.56
[INFO 2017-06-28 14:49:56,447 main.py:51] epoch 9624, training loss: 7756.26, average training loss: 9382.89, base loss: 14414.83
[INFO 2017-06-28 14:49:57,517 main.py:51] epoch 9625, training loss: 9635.61, average training loss: 9384.17, base loss: 14417.58
[INFO 2017-06-28 14:49:58,582 main.py:51] epoch 9626, training loss: 9565.30, average training loss: 9384.90, base loss: 14417.85
[INFO 2017-06-28 14:49:59,686 main.py:51] epoch 9627, training loss: 9083.07, average training loss: 9385.07, base loss: 14419.33
[INFO 2017-06-28 14:50:00,756 main.py:51] epoch 9628, training loss: 8303.21, average training loss: 9383.60, base loss: 14417.36
[INFO 2017-06-28 14:50:01,829 main.py:51] epoch 9629, training loss: 8384.07, average training loss: 9382.81, base loss: 14415.95
[INFO 2017-06-28 14:50:02,883 main.py:51] epoch 9630, training loss: 8661.56, average training loss: 9381.64, base loss: 14415.17
[INFO 2017-06-28 14:50:03,929 main.py:51] epoch 9631, training loss: 9646.64, average training loss: 9381.77, base loss: 14416.13
[INFO 2017-06-28 14:50:05,004 main.py:51] epoch 9632, training loss: 9235.32, average training loss: 9380.18, base loss: 14413.76
[INFO 2017-06-28 14:50:06,096 main.py:51] epoch 9633, training loss: 9124.43, average training loss: 9381.04, base loss: 14416.40
[INFO 2017-06-28 14:50:07,226 main.py:51] epoch 9634, training loss: 9929.91, average training loss: 9381.12, base loss: 14417.46
[INFO 2017-06-28 14:50:08,214 main.py:51] epoch 9635, training loss: 8911.80, average training loss: 9380.14, base loss: 14414.89
[INFO 2017-06-28 14:50:09,305 main.py:51] epoch 9636, training loss: 9979.71, average training loss: 9381.63, base loss: 14416.84
[INFO 2017-06-28 14:50:10,344 main.py:51] epoch 9637, training loss: 9726.20, average training loss: 9382.36, base loss: 14417.32
[INFO 2017-06-28 14:50:11,424 main.py:51] epoch 9638, training loss: 8854.74, average training loss: 9381.52, base loss: 14416.76
[INFO 2017-06-28 14:50:12,568 main.py:51] epoch 9639, training loss: 8285.33, average training loss: 9379.16, base loss: 14412.95
[INFO 2017-06-28 14:50:13,605 main.py:51] epoch 9640, training loss: 7855.93, average training loss: 9377.88, base loss: 14411.06
[INFO 2017-06-28 14:50:14,680 main.py:51] epoch 9641, training loss: 9181.13, average training loss: 9376.59, base loss: 14408.59
[INFO 2017-06-28 14:50:15,701 main.py:51] epoch 9642, training loss: 10134.52, average training loss: 9378.28, base loss: 14410.26
[INFO 2017-06-28 14:50:16,783 main.py:51] epoch 9643, training loss: 9998.83, average training loss: 9379.16, base loss: 14412.23
[INFO 2017-06-28 14:50:17,891 main.py:51] epoch 9644, training loss: 9895.84, average training loss: 9379.71, base loss: 14414.57
[INFO 2017-06-28 14:50:18,928 main.py:51] epoch 9645, training loss: 10130.54, average training loss: 9381.73, base loss: 14418.06
[INFO 2017-06-28 14:50:19,989 main.py:51] epoch 9646, training loss: 8863.89, average training loss: 9381.39, base loss: 14416.90
[INFO 2017-06-28 14:50:21,029 main.py:51] epoch 9647, training loss: 9107.86, average training loss: 9381.78, base loss: 14416.89
[INFO 2017-06-28 14:50:22,103 main.py:51] epoch 9648, training loss: 10015.84, average training loss: 9382.84, base loss: 14419.27
[INFO 2017-06-28 14:50:23,165 main.py:51] epoch 9649, training loss: 9471.31, average training loss: 9382.36, base loss: 14418.02
[INFO 2017-06-28 14:50:24,204 main.py:51] epoch 9650, training loss: 8271.12, average training loss: 9380.69, base loss: 14416.19
[INFO 2017-06-28 14:50:25,325 main.py:51] epoch 9651, training loss: 10802.99, average training loss: 9380.22, base loss: 14416.41
[INFO 2017-06-28 14:50:26,349 main.py:51] epoch 9652, training loss: 10692.08, average training loss: 9381.89, base loss: 14418.28
[INFO 2017-06-28 14:50:27,408 main.py:51] epoch 9653, training loss: 8299.29, average training loss: 9382.10, base loss: 14419.29
[INFO 2017-06-28 14:50:28,507 main.py:51] epoch 9654, training loss: 11600.00, average training loss: 9384.43, base loss: 14423.99
[INFO 2017-06-28 14:50:29,579 main.py:51] epoch 9655, training loss: 9358.35, average training loss: 9383.62, base loss: 14422.96
[INFO 2017-06-28 14:50:30,675 main.py:51] epoch 9656, training loss: 8804.19, average training loss: 9382.91, base loss: 14421.28
[INFO 2017-06-28 14:50:31,684 main.py:51] epoch 9657, training loss: 9985.10, average training loss: 9382.52, base loss: 14419.95
[INFO 2017-06-28 14:50:32,768 main.py:51] epoch 9658, training loss: 9216.87, average training loss: 9382.75, base loss: 14419.48
[INFO 2017-06-28 14:50:33,822 main.py:51] epoch 9659, training loss: 10029.06, average training loss: 9383.28, base loss: 14419.98
[INFO 2017-06-28 14:50:34,823 main.py:51] epoch 9660, training loss: 8995.96, average training loss: 9384.32, base loss: 14425.93
[INFO 2017-06-28 14:50:35,904 main.py:51] epoch 9661, training loss: 9386.34, average training loss: 9383.68, base loss: 14424.76
[INFO 2017-06-28 14:50:37,008 main.py:51] epoch 9662, training loss: 9098.79, average training loss: 9384.85, base loss: 14426.41
[INFO 2017-06-28 14:50:38,033 main.py:51] epoch 9663, training loss: 9941.14, average training loss: 9384.73, base loss: 14425.81
[INFO 2017-06-28 14:50:39,079 main.py:51] epoch 9664, training loss: 8792.02, average training loss: 9383.76, base loss: 14422.16
[INFO 2017-06-28 14:50:40,140 main.py:51] epoch 9665, training loss: 7949.90, average training loss: 9382.43, base loss: 14419.14
[INFO 2017-06-28 14:50:41,202 main.py:51] epoch 9666, training loss: 8564.89, average training loss: 9383.54, base loss: 14420.39
[INFO 2017-06-28 14:50:42,251 main.py:51] epoch 9667, training loss: 9788.33, average training loss: 9383.77, base loss: 14420.66
[INFO 2017-06-28 14:50:43,270 main.py:51] epoch 9668, training loss: 9460.78, average training loss: 9383.03, base loss: 14420.03
[INFO 2017-06-28 14:50:44,309 main.py:51] epoch 9669, training loss: 8585.16, average training loss: 9381.77, base loss: 14417.39
[INFO 2017-06-28 14:50:45,411 main.py:51] epoch 9670, training loss: 10240.46, average training loss: 9383.32, base loss: 14419.34
[INFO 2017-06-28 14:50:46,436 main.py:51] epoch 9671, training loss: 9387.09, average training loss: 9382.49, base loss: 14418.38
[INFO 2017-06-28 14:50:47,459 main.py:51] epoch 9672, training loss: 8344.70, average training loss: 9381.74, base loss: 14416.74
[INFO 2017-06-28 14:50:48,513 main.py:51] epoch 9673, training loss: 10063.28, average training loss: 9381.66, base loss: 14417.95
[INFO 2017-06-28 14:50:49,460 main.py:51] epoch 9674, training loss: 9141.10, average training loss: 9380.59, base loss: 14416.85
[INFO 2017-06-28 14:50:50,441 main.py:51] epoch 9675, training loss: 7959.31, average training loss: 9379.15, base loss: 14413.78
[INFO 2017-06-28 14:50:51,406 main.py:51] epoch 9676, training loss: 9962.09, average training loss: 9379.49, base loss: 14413.31
[INFO 2017-06-28 14:50:52,456 main.py:51] epoch 9677, training loss: 8718.61, average training loss: 9378.92, base loss: 14412.70
[INFO 2017-06-28 14:50:53,509 main.py:51] epoch 9678, training loss: 8886.67, average training loss: 9379.02, base loss: 14412.28
[INFO 2017-06-28 14:50:54,543 main.py:51] epoch 9679, training loss: 9893.62, average training loss: 9379.02, base loss: 14411.91
[INFO 2017-06-28 14:50:55,610 main.py:51] epoch 9680, training loss: 9324.82, average training loss: 9379.23, base loss: 14411.78
[INFO 2017-06-28 14:50:56,732 main.py:51] epoch 9681, training loss: 8350.52, average training loss: 9377.54, base loss: 14407.75
[INFO 2017-06-28 14:50:57,754 main.py:51] epoch 9682, training loss: 9511.23, average training loss: 9378.11, base loss: 14409.10
[INFO 2017-06-28 14:50:58,782 main.py:51] epoch 9683, training loss: 8782.50, average training loss: 9378.29, base loss: 14408.21
[INFO 2017-06-28 14:50:59,842 main.py:51] epoch 9684, training loss: 10011.98, average training loss: 9379.96, base loss: 14412.22
[INFO 2017-06-28 14:51:00,933 main.py:51] epoch 9685, training loss: 8692.76, average training loss: 9378.81, base loss: 14411.06
[INFO 2017-06-28 14:51:02,043 main.py:51] epoch 9686, training loss: 10314.42, average training loss: 9378.80, base loss: 14411.60
[INFO 2017-06-28 14:51:03,077 main.py:51] epoch 9687, training loss: 8603.04, average training loss: 9378.68, base loss: 14411.18
[INFO 2017-06-28 14:51:04,130 main.py:51] epoch 9688, training loss: 9954.72, average training loss: 9378.43, base loss: 14409.50
[INFO 2017-06-28 14:51:05,166 main.py:51] epoch 9689, training loss: 9054.57, average training loss: 9377.51, base loss: 14408.41
[INFO 2017-06-28 14:51:06,240 main.py:51] epoch 9690, training loss: 9703.04, average training loss: 9378.42, base loss: 14409.82
[INFO 2017-06-28 14:51:07,345 main.py:51] epoch 9691, training loss: 9109.04, average training loss: 9377.93, base loss: 14409.09
[INFO 2017-06-28 14:51:08,376 main.py:51] epoch 9692, training loss: 9207.44, average training loss: 9377.86, base loss: 14407.33
[INFO 2017-06-28 14:51:09,383 main.py:51] epoch 9693, training loss: 8983.16, average training loss: 9377.42, base loss: 14407.57
[INFO 2017-06-28 14:51:10,422 main.py:51] epoch 9694, training loss: 10311.67, average training loss: 9378.73, base loss: 14409.61
[INFO 2017-06-28 14:51:11,440 main.py:51] epoch 9695, training loss: 8546.11, average training loss: 9377.10, base loss: 14406.58
[INFO 2017-06-28 14:51:12,480 main.py:51] epoch 9696, training loss: 8596.46, average training loss: 9375.18, base loss: 14404.32
[INFO 2017-06-28 14:51:13,526 main.py:51] epoch 9697, training loss: 9961.02, average training loss: 9375.70, base loss: 14405.57
[INFO 2017-06-28 14:51:14,575 main.py:51] epoch 9698, training loss: 8570.31, average training loss: 9375.87, base loss: 14406.05
[INFO 2017-06-28 14:51:15,662 main.py:51] epoch 9699, training loss: 10139.82, average training loss: 9376.46, base loss: 14408.05
[INFO 2017-06-28 14:51:15,662 main.py:53] epoch 9699, testing
[INFO 2017-06-28 14:51:19,483 main.py:105] average testing loss: 10784.26, base loss: 15298.83
[INFO 2017-06-28 14:51:19,483 main.py:106] improve_loss: 4514.56, improve_percent: 0.30
[INFO 2017-06-28 14:51:19,484 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:51:20,558 main.py:51] epoch 9700, training loss: 8873.88, average training loss: 9375.93, base loss: 14406.00
[INFO 2017-06-28 14:51:21,607 main.py:51] epoch 9701, training loss: 10061.58, average training loss: 9376.66, base loss: 14409.08
[INFO 2017-06-28 14:51:22,686 main.py:51] epoch 9702, training loss: 9584.85, average training loss: 9377.59, base loss: 14411.47
[INFO 2017-06-28 14:51:23,706 main.py:51] epoch 9703, training loss: 9952.46, average training loss: 9379.14, base loss: 14412.59
[INFO 2017-06-28 14:51:24,791 main.py:51] epoch 9704, training loss: 9502.20, average training loss: 9378.81, base loss: 14413.65
[INFO 2017-06-28 14:51:25,906 main.py:51] epoch 9705, training loss: 9035.80, average training loss: 9378.60, base loss: 14413.24
[INFO 2017-06-28 14:51:26,897 main.py:51] epoch 9706, training loss: 8931.94, average training loss: 9378.99, base loss: 14413.37
[INFO 2017-06-28 14:51:27,929 main.py:51] epoch 9707, training loss: 9551.99, average training loss: 9378.23, base loss: 14410.77
[INFO 2017-06-28 14:51:28,977 main.py:51] epoch 9708, training loss: 8595.98, average training loss: 9376.27, base loss: 14406.68
[INFO 2017-06-28 14:51:30,013 main.py:51] epoch 9709, training loss: 8201.08, average training loss: 9375.57, base loss: 14405.88
[INFO 2017-06-28 14:51:31,059 main.py:51] epoch 9710, training loss: 10116.35, average training loss: 9376.03, base loss: 14406.62
[INFO 2017-06-28 14:51:32,101 main.py:51] epoch 9711, training loss: 9889.65, average training loss: 9376.13, base loss: 14407.71
[INFO 2017-06-28 14:51:33,171 main.py:51] epoch 9712, training loss: 8138.75, average training loss: 9374.44, base loss: 14406.07
[INFO 2017-06-28 14:51:34,267 main.py:51] epoch 9713, training loss: 9166.55, average training loss: 9376.14, base loss: 14408.69
[INFO 2017-06-28 14:51:35,302 main.py:51] epoch 9714, training loss: 9214.17, average training loss: 9375.44, base loss: 14408.29
[INFO 2017-06-28 14:51:36,350 main.py:51] epoch 9715, training loss: 9349.77, average training loss: 9374.98, base loss: 14406.74
[INFO 2017-06-28 14:51:37,389 main.py:51] epoch 9716, training loss: 9296.02, average training loss: 9374.80, base loss: 14406.97
[INFO 2017-06-28 14:51:38,477 main.py:51] epoch 9717, training loss: 10155.85, average training loss: 9376.07, base loss: 14409.02
[INFO 2017-06-28 14:51:39,610 main.py:51] epoch 9718, training loss: 10646.08, average training loss: 9377.56, base loss: 14410.66
[INFO 2017-06-28 14:51:40,643 main.py:51] epoch 9719, training loss: 10480.89, average training loss: 9378.67, base loss: 14412.83
[INFO 2017-06-28 14:51:41,685 main.py:51] epoch 9720, training loss: 8998.47, average training loss: 9378.87, base loss: 14413.72
[INFO 2017-06-28 14:51:42,714 main.py:51] epoch 9721, training loss: 11156.95, average training loss: 9380.25, base loss: 14415.34
[INFO 2017-06-28 14:51:43,817 main.py:51] epoch 9722, training loss: 9109.57, average training loss: 9380.71, base loss: 14416.20
[INFO 2017-06-28 14:51:44,912 main.py:51] epoch 9723, training loss: 8786.53, average training loss: 9380.64, base loss: 14417.93
[INFO 2017-06-28 14:51:45,924 main.py:51] epoch 9724, training loss: 9910.04, average training loss: 9381.08, base loss: 14418.27
[INFO 2017-06-28 14:51:46,954 main.py:51] epoch 9725, training loss: 10398.30, average training loss: 9380.79, base loss: 14419.87
[INFO 2017-06-28 14:51:48,003 main.py:51] epoch 9726, training loss: 10180.61, average training loss: 9381.21, base loss: 14418.93
[INFO 2017-06-28 14:51:48,970 main.py:51] epoch 9727, training loss: 9026.19, average training loss: 9380.44, base loss: 14417.44
[INFO 2017-06-28 14:51:50,023 main.py:51] epoch 9728, training loss: 8811.45, average training loss: 9380.51, base loss: 14417.92
[INFO 2017-06-28 14:51:51,030 main.py:51] epoch 9729, training loss: 8921.19, average training loss: 9380.47, base loss: 14418.13
[INFO 2017-06-28 14:51:52,056 main.py:51] epoch 9730, training loss: 8665.35, average training loss: 9380.31, base loss: 14416.63
[INFO 2017-06-28 14:51:53,078 main.py:51] epoch 9731, training loss: 9496.79, average training loss: 9380.70, base loss: 14416.40
[INFO 2017-06-28 14:51:54,128 main.py:51] epoch 9732, training loss: 8959.45, average training loss: 9380.94, base loss: 14416.06
[INFO 2017-06-28 14:51:55,138 main.py:51] epoch 9733, training loss: 8966.11, average training loss: 9380.20, base loss: 14415.94
[INFO 2017-06-28 14:51:56,232 main.py:51] epoch 9734, training loss: 9355.13, average training loss: 9380.38, base loss: 14416.92
[INFO 2017-06-28 14:51:57,243 main.py:51] epoch 9735, training loss: 10227.30, average training loss: 9381.61, base loss: 14419.34
[INFO 2017-06-28 14:51:58,277 main.py:51] epoch 9736, training loss: 8230.56, average training loss: 9379.85, base loss: 14416.52
[INFO 2017-06-28 14:51:59,385 main.py:51] epoch 9737, training loss: 9437.85, average training loss: 9381.24, base loss: 14419.57
[INFO 2017-06-28 14:52:00,414 main.py:51] epoch 9738, training loss: 10082.04, average training loss: 9382.35, base loss: 14421.43
[INFO 2017-06-28 14:52:01,459 main.py:51] epoch 9739, training loss: 9370.02, average training loss: 9382.82, base loss: 14421.60
[INFO 2017-06-28 14:52:02,505 main.py:51] epoch 9740, training loss: 9250.35, average training loss: 9382.89, base loss: 14421.49
[INFO 2017-06-28 14:52:03,499 main.py:51] epoch 9741, training loss: 9983.64, average training loss: 9384.03, base loss: 14424.25
[INFO 2017-06-28 14:52:04,551 main.py:51] epoch 9742, training loss: 9138.09, average training loss: 9384.43, base loss: 14423.40
[INFO 2017-06-28 14:52:05,576 main.py:51] epoch 9743, training loss: 9070.00, average training loss: 9384.66, base loss: 14425.58
[INFO 2017-06-28 14:52:06,622 main.py:51] epoch 9744, training loss: 8761.16, average training loss: 9383.96, base loss: 14422.66
[INFO 2017-06-28 14:52:07,633 main.py:51] epoch 9745, training loss: 8907.22, average training loss: 9383.57, base loss: 14422.67
[INFO 2017-06-28 14:52:08,548 main.py:51] epoch 9746, training loss: 10075.18, average training loss: 9383.03, base loss: 14421.08
[INFO 2017-06-28 14:52:09,491 main.py:51] epoch 9747, training loss: 9787.41, average training loss: 9383.59, base loss: 14420.62
[INFO 2017-06-28 14:52:10,569 main.py:51] epoch 9748, training loss: 9073.40, average training loss: 9384.25, base loss: 14421.08
[INFO 2017-06-28 14:52:11,597 main.py:51] epoch 9749, training loss: 8616.73, average training loss: 9382.62, base loss: 14419.20
[INFO 2017-06-28 14:52:12,669 main.py:51] epoch 9750, training loss: 9011.51, average training loss: 9383.18, base loss: 14419.52
[INFO 2017-06-28 14:52:13,731 main.py:51] epoch 9751, training loss: 9298.25, average training loss: 9382.52, base loss: 14418.85
[INFO 2017-06-28 14:52:14,820 main.py:51] epoch 9752, training loss: 9300.42, average training loss: 9383.49, base loss: 14420.50
[INFO 2017-06-28 14:52:15,929 main.py:51] epoch 9753, training loss: 10185.06, average training loss: 9384.28, base loss: 14424.03
[INFO 2017-06-28 14:52:17,022 main.py:51] epoch 9754, training loss: 10014.54, average training loss: 9385.06, base loss: 14425.10
[INFO 2017-06-28 14:52:18,142 main.py:51] epoch 9755, training loss: 9038.78, average training loss: 9384.38, base loss: 14423.39
[INFO 2017-06-28 14:52:19,158 main.py:51] epoch 9756, training loss: 10626.06, average training loss: 9385.88, base loss: 14426.47
[INFO 2017-06-28 14:52:20,185 main.py:51] epoch 9757, training loss: 9439.18, average training loss: 9385.06, base loss: 14426.10
[INFO 2017-06-28 14:52:21,223 main.py:51] epoch 9758, training loss: 8426.83, average training loss: 9384.37, base loss: 14424.59
[INFO 2017-06-28 14:52:22,315 main.py:51] epoch 9759, training loss: 8799.78, average training loss: 9384.54, base loss: 14424.53
[INFO 2017-06-28 14:52:23,422 main.py:51] epoch 9760, training loss: 9103.64, average training loss: 9384.91, base loss: 14426.01
[INFO 2017-06-28 14:52:24,442 main.py:51] epoch 9761, training loss: 9492.14, average training loss: 9384.24, base loss: 14425.83
[INFO 2017-06-28 14:52:25,514 main.py:51] epoch 9762, training loss: 8302.65, average training loss: 9383.14, base loss: 14423.75
[INFO 2017-06-28 14:52:26,541 main.py:51] epoch 9763, training loss: 9985.27, average training loss: 9383.20, base loss: 14424.98
[INFO 2017-06-28 14:52:27,621 main.py:51] epoch 9764, training loss: 9513.76, average training loss: 9382.80, base loss: 14423.40
[INFO 2017-06-28 14:52:28,775 main.py:51] epoch 9765, training loss: 9304.98, average training loss: 9383.06, base loss: 14424.77
[INFO 2017-06-28 14:52:29,799 main.py:51] epoch 9766, training loss: 8085.07, average training loss: 9382.00, base loss: 14421.93
[INFO 2017-06-28 14:52:30,814 main.py:51] epoch 9767, training loss: 8941.30, average training loss: 9378.45, base loss: 14417.27
[INFO 2017-06-28 14:52:31,858 main.py:51] epoch 9768, training loss: 9000.68, average training loss: 9377.79, base loss: 14417.94
[INFO 2017-06-28 14:52:32,916 main.py:51] epoch 9769, training loss: 8588.39, average training loss: 9375.60, base loss: 14416.60
[INFO 2017-06-28 14:52:34,018 main.py:51] epoch 9770, training loss: 8948.90, average training loss: 9376.18, base loss: 14418.77
[INFO 2017-06-28 14:52:35,032 main.py:51] epoch 9771, training loss: 8231.26, average training loss: 9373.79, base loss: 14415.25
[INFO 2017-06-28 14:52:36,067 main.py:51] epoch 9772, training loss: 8304.94, average training loss: 9373.56, base loss: 14414.72
[INFO 2017-06-28 14:52:37,100 main.py:51] epoch 9773, training loss: 8832.80, average training loss: 9371.40, base loss: 14412.66
[INFO 2017-06-28 14:52:38,192 main.py:51] epoch 9774, training loss: 9521.71, average training loss: 9371.68, base loss: 14413.89
[INFO 2017-06-28 14:52:39,275 main.py:51] epoch 9775, training loss: 8708.10, average training loss: 9369.68, base loss: 14410.63
[INFO 2017-06-28 14:52:40,304 main.py:51] epoch 9776, training loss: 10728.26, average training loss: 9369.61, base loss: 14410.61
[INFO 2017-06-28 14:52:41,370 main.py:51] epoch 9777, training loss: 9390.87, average training loss: 9368.99, base loss: 14409.14
[INFO 2017-06-28 14:52:42,408 main.py:51] epoch 9778, training loss: 7378.19, average training loss: 9367.02, base loss: 14404.51
[INFO 2017-06-28 14:52:43,487 main.py:51] epoch 9779, training loss: 9727.69, average training loss: 9367.96, base loss: 14407.20
[INFO 2017-06-28 14:52:44,569 main.py:51] epoch 9780, training loss: 8864.90, average training loss: 9367.04, base loss: 14406.47
[INFO 2017-06-28 14:52:45,565 main.py:51] epoch 9781, training loss: 9382.70, average training loss: 9368.25, base loss: 14409.19
[INFO 2017-06-28 14:52:46,638 main.py:51] epoch 9782, training loss: 8766.34, average training loss: 9366.09, base loss: 14405.51
[INFO 2017-06-28 14:52:47,673 main.py:51] epoch 9783, training loss: 9988.08, average training loss: 9365.82, base loss: 14404.50
[INFO 2017-06-28 14:52:48,669 main.py:51] epoch 9784, training loss: 8015.45, average training loss: 9364.67, base loss: 14402.94
[INFO 2017-06-28 14:52:49,722 main.py:51] epoch 9785, training loss: 9207.57, average training loss: 9365.01, base loss: 14404.21
[INFO 2017-06-28 14:52:50,800 main.py:51] epoch 9786, training loss: 9473.63, average training loss: 9365.24, base loss: 14404.89
[INFO 2017-06-28 14:52:51,888 main.py:51] epoch 9787, training loss: 8884.26, average training loss: 9365.17, base loss: 14405.08
[INFO 2017-06-28 14:52:52,991 main.py:51] epoch 9788, training loss: 11033.38, average training loss: 9368.50, base loss: 14410.62
[INFO 2017-06-28 14:52:54,033 main.py:51] epoch 9789, training loss: 10811.25, average training loss: 9369.90, base loss: 14412.13
[INFO 2017-06-28 14:52:55,063 main.py:51] epoch 9790, training loss: 9217.58, average training loss: 9368.83, base loss: 14410.55
[INFO 2017-06-28 14:52:56,153 main.py:51] epoch 9791, training loss: 9749.60, average training loss: 9369.37, base loss: 14412.76
[INFO 2017-06-28 14:52:57,199 main.py:51] epoch 9792, training loss: 9655.44, average training loss: 9369.10, base loss: 14413.66
[INFO 2017-06-28 14:52:58,232 main.py:51] epoch 9793, training loss: 9844.72, average training loss: 9368.63, base loss: 14412.54
[INFO 2017-06-28 14:52:59,274 main.py:51] epoch 9794, training loss: 8290.38, average training loss: 9368.36, base loss: 14411.21
[INFO 2017-06-28 14:53:00,336 main.py:51] epoch 9795, training loss: 10447.86, average training loss: 9369.22, base loss: 14412.18
[INFO 2017-06-28 14:53:01,453 main.py:51] epoch 9796, training loss: 7408.15, average training loss: 9366.71, base loss: 14407.52
[INFO 2017-06-28 14:53:02,510 main.py:51] epoch 9797, training loss: 8534.43, average training loss: 9365.16, base loss: 14406.21
[INFO 2017-06-28 14:53:03,579 main.py:51] epoch 9798, training loss: 10865.93, average training loss: 9367.51, base loss: 14408.99
[INFO 2017-06-28 14:53:04,612 main.py:51] epoch 9799, training loss: 8421.14, average training loss: 9366.94, base loss: 14408.24
[INFO 2017-06-28 14:53:04,612 main.py:53] epoch 9799, testing
[INFO 2017-06-28 14:53:08,436 main.py:105] average testing loss: 10721.09, base loss: 15005.61
[INFO 2017-06-28 14:53:08,436 main.py:106] improve_loss: 4284.52, improve_percent: 0.29
[INFO 2017-06-28 14:53:08,437 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:53:09,538 main.py:51] epoch 9800, training loss: 9595.36, average training loss: 9366.50, base loss: 14408.30
[INFO 2017-06-28 14:53:10,649 main.py:51] epoch 9801, training loss: 10163.88, average training loss: 9368.34, base loss: 14410.63
[INFO 2017-06-28 14:53:11,700 main.py:51] epoch 9802, training loss: 9618.99, average training loss: 9368.36, base loss: 14410.36
[INFO 2017-06-28 14:53:12,744 main.py:51] epoch 9803, training loss: 9496.68, average training loss: 9367.81, base loss: 14408.35
[INFO 2017-06-28 14:53:13,755 main.py:51] epoch 9804, training loss: 8355.42, average training loss: 9366.32, base loss: 14406.48
[INFO 2017-06-28 14:53:14,777 main.py:51] epoch 9805, training loss: 10152.93, average training loss: 9367.80, base loss: 14409.29
[INFO 2017-06-28 14:53:15,923 main.py:51] epoch 9806, training loss: 9097.22, average training loss: 9367.87, base loss: 14407.51
[INFO 2017-06-28 14:53:16,937 main.py:51] epoch 9807, training loss: 10477.38, average training loss: 9369.48, base loss: 14411.19
[INFO 2017-06-28 14:53:18,042 main.py:51] epoch 9808, training loss: 9070.23, average training loss: 9368.14, base loss: 14411.38
[INFO 2017-06-28 14:53:19,039 main.py:51] epoch 9809, training loss: 8550.14, average training loss: 9367.96, base loss: 14409.99
[INFO 2017-06-28 14:53:20,123 main.py:51] epoch 9810, training loss: 9709.38, average training loss: 9368.43, base loss: 14411.27
[INFO 2017-06-28 14:53:21,214 main.py:51] epoch 9811, training loss: 9003.84, average training loss: 9367.48, base loss: 14409.69
[INFO 2017-06-28 14:53:22,298 main.py:51] epoch 9812, training loss: 8985.46, average training loss: 9366.10, base loss: 14408.90
[INFO 2017-06-28 14:53:23,436 main.py:51] epoch 9813, training loss: 11250.30, average training loss: 9368.04, base loss: 14411.95
[INFO 2017-06-28 14:53:24,500 main.py:51] epoch 9814, training loss: 8789.38, average training loss: 9367.22, base loss: 14410.87
[INFO 2017-06-28 14:53:25,439 main.py:51] epoch 9815, training loss: 8747.16, average training loss: 9366.35, base loss: 14409.70
[INFO 2017-06-28 14:53:26,406 main.py:51] epoch 9816, training loss: 10155.12, average training loss: 9367.36, base loss: 14410.99
[INFO 2017-06-28 14:53:27,392 main.py:51] epoch 9817, training loss: 9658.02, average training loss: 9368.14, base loss: 14411.38
[INFO 2017-06-28 14:53:28,526 main.py:51] epoch 9818, training loss: 9282.60, average training loss: 9368.21, base loss: 14411.19
[INFO 2017-06-28 14:53:29,597 main.py:51] epoch 9819, training loss: 9925.46, average training loss: 9369.87, base loss: 14413.80
[INFO 2017-06-28 14:53:30,690 main.py:51] epoch 9820, training loss: 9700.03, average training loss: 9370.50, base loss: 14415.10
[INFO 2017-06-28 14:53:31,689 main.py:51] epoch 9821, training loss: 9036.97, average training loss: 9371.73, base loss: 14417.12
[INFO 2017-06-28 14:53:32,747 main.py:51] epoch 9822, training loss: 9092.25, average training loss: 9371.89, base loss: 14419.24
[INFO 2017-06-28 14:53:33,836 main.py:51] epoch 9823, training loss: 8602.63, average training loss: 9370.00, base loss: 14415.44
[INFO 2017-06-28 14:53:34,905 main.py:51] epoch 9824, training loss: 10892.50, average training loss: 9372.32, base loss: 14417.28
[INFO 2017-06-28 14:53:36,003 main.py:51] epoch 9825, training loss: 7428.02, average training loss: 9368.42, base loss: 14410.43
[INFO 2017-06-28 14:53:37,038 main.py:51] epoch 9826, training loss: 8911.05, average training loss: 9367.23, base loss: 14409.64
[INFO 2017-06-28 14:53:38,066 main.py:51] epoch 9827, training loss: 10344.81, average training loss: 9369.12, base loss: 14412.08
[INFO 2017-06-28 14:53:39,169 main.py:51] epoch 9828, training loss: 8698.55, average training loss: 9367.51, base loss: 14410.71
[INFO 2017-06-28 14:53:40,232 main.py:51] epoch 9829, training loss: 9475.41, average training loss: 9366.74, base loss: 14410.12
[INFO 2017-06-28 14:53:41,362 main.py:51] epoch 9830, training loss: 8635.07, average training loss: 9365.35, base loss: 14407.64
[INFO 2017-06-28 14:53:42,442 main.py:51] epoch 9831, training loss: 8376.32, average training loss: 9365.49, base loss: 14407.90
[INFO 2017-06-28 14:53:43,564 main.py:51] epoch 9832, training loss: 9260.46, average training loss: 9364.48, base loss: 14404.81
[INFO 2017-06-28 14:53:44,595 main.py:51] epoch 9833, training loss: 8098.80, average training loss: 9362.66, base loss: 14400.50
[INFO 2017-06-28 14:53:45,611 main.py:51] epoch 9834, training loss: 9369.10, average training loss: 9361.69, base loss: 14398.77
[INFO 2017-06-28 14:53:46,663 main.py:51] epoch 9835, training loss: 9951.72, average training loss: 9362.40, base loss: 14401.28
[INFO 2017-06-28 14:53:47,764 main.py:51] epoch 9836, training loss: 9983.86, average training loss: 9361.82, base loss: 14400.79
[INFO 2017-06-28 14:53:48,874 main.py:51] epoch 9837, training loss: 9993.29, average training loss: 9362.11, base loss: 14402.82
[INFO 2017-06-28 14:53:49,847 main.py:51] epoch 9838, training loss: 8382.34, average training loss: 9359.60, base loss: 14399.61
[INFO 2017-06-28 14:53:50,873 main.py:51] epoch 9839, training loss: 8055.71, average training loss: 9358.56, base loss: 14398.49
[INFO 2017-06-28 14:53:51,951 main.py:51] epoch 9840, training loss: 8769.88, average training loss: 9358.05, base loss: 14396.07
[INFO 2017-06-28 14:53:53,075 main.py:51] epoch 9841, training loss: 8397.07, average training loss: 9356.82, base loss: 14395.04
[INFO 2017-06-28 14:53:54,128 main.py:51] epoch 9842, training loss: 11033.76, average training loss: 9358.41, base loss: 14397.02
[INFO 2017-06-28 14:53:55,111 main.py:51] epoch 9843, training loss: 9664.63, average training loss: 9357.79, base loss: 14396.76
[INFO 2017-06-28 14:53:56,157 main.py:51] epoch 9844, training loss: 9646.08, average training loss: 9357.76, base loss: 14397.47
[INFO 2017-06-28 14:53:57,242 main.py:51] epoch 9845, training loss: 9482.20, average training loss: 9357.24, base loss: 14395.88
[INFO 2017-06-28 14:53:58,269 main.py:51] epoch 9846, training loss: 9091.11, average training loss: 9357.13, base loss: 14396.08
[INFO 2017-06-28 14:53:59,315 main.py:51] epoch 9847, training loss: 9705.75, average training loss: 9356.52, base loss: 14394.13
[INFO 2017-06-28 14:54:00,391 main.py:51] epoch 9848, training loss: 8984.27, average training loss: 9354.32, base loss: 14391.97
[INFO 2017-06-28 14:54:01,434 main.py:51] epoch 9849, training loss: 10207.92, average training loss: 9352.89, base loss: 14388.22
[INFO 2017-06-28 14:54:02,461 main.py:51] epoch 9850, training loss: 8644.87, average training loss: 9351.71, base loss: 14385.35
[INFO 2017-06-28 14:54:03,482 main.py:51] epoch 9851, training loss: 9934.95, average training loss: 9353.07, base loss: 14388.44
[INFO 2017-06-28 14:54:04,562 main.py:51] epoch 9852, training loss: 10950.93, average training loss: 9354.99, base loss: 14392.83
[INFO 2017-06-28 14:54:05,658 main.py:51] epoch 9853, training loss: 8804.99, average training loss: 9354.15, base loss: 14391.54
[INFO 2017-06-28 14:54:06,750 main.py:51] epoch 9854, training loss: 9307.72, average training loss: 9353.53, base loss: 14390.97
[INFO 2017-06-28 14:54:07,836 main.py:51] epoch 9855, training loss: 8167.37, average training loss: 9352.50, base loss: 14387.97
[INFO 2017-06-28 14:54:08,894 main.py:51] epoch 9856, training loss: 11321.23, average training loss: 9353.24, base loss: 14388.02
[INFO 2017-06-28 14:54:09,915 main.py:51] epoch 9857, training loss: 9477.29, average training loss: 9354.40, base loss: 14388.73
[INFO 2017-06-28 14:54:10,981 main.py:51] epoch 9858, training loss: 10500.15, average training loss: 9354.91, base loss: 14389.67
[INFO 2017-06-28 14:54:12,086 main.py:51] epoch 9859, training loss: 9256.57, average training loss: 9353.99, base loss: 14387.25
[INFO 2017-06-28 14:54:13,181 main.py:51] epoch 9860, training loss: 9115.44, average training loss: 9353.50, base loss: 14386.64
[INFO 2017-06-28 14:54:14,278 main.py:51] epoch 9861, training loss: 9604.11, average training loss: 9354.65, base loss: 14389.68
[INFO 2017-06-28 14:54:15,350 main.py:51] epoch 9862, training loss: 8787.21, average training loss: 9353.81, base loss: 14388.76
[INFO 2017-06-28 14:54:16,354 main.py:51] epoch 9863, training loss: 9974.76, average training loss: 9354.93, base loss: 14392.48
[INFO 2017-06-28 14:54:17,413 main.py:51] epoch 9864, training loss: 9392.63, average training loss: 9353.96, base loss: 14391.57
[INFO 2017-06-28 14:54:18,474 main.py:51] epoch 9865, training loss: 9891.83, average training loss: 9353.92, base loss: 14392.28
[INFO 2017-06-28 14:54:19,476 main.py:51] epoch 9866, training loss: 8588.63, average training loss: 9353.30, base loss: 14391.41
[INFO 2017-06-28 14:54:20,508 main.py:51] epoch 9867, training loss: 10174.15, average training loss: 9353.97, base loss: 14391.38
[INFO 2017-06-28 14:54:21,567 main.py:51] epoch 9868, training loss: 8775.17, average training loss: 9353.58, base loss: 14390.26
[INFO 2017-06-28 14:54:22,622 main.py:51] epoch 9869, training loss: 11073.41, average training loss: 9355.74, base loss: 14394.42
[INFO 2017-06-28 14:54:23,722 main.py:51] epoch 9870, training loss: 8477.20, average training loss: 9354.38, base loss: 14392.50
[INFO 2017-06-28 14:54:24,766 main.py:51] epoch 9871, training loss: 9732.79, average training loss: 9352.32, base loss: 14388.01
[INFO 2017-06-28 14:54:25,847 main.py:51] epoch 9872, training loss: 8967.34, average training loss: 9352.64, base loss: 14388.05
[INFO 2017-06-28 14:54:26,876 main.py:51] epoch 9873, training loss: 10059.93, average training loss: 9353.42, base loss: 14388.62
[INFO 2017-06-28 14:54:27,937 main.py:51] epoch 9874, training loss: 9263.96, average training loss: 9352.37, base loss: 14387.50
[INFO 2017-06-28 14:54:29,037 main.py:51] epoch 9875, training loss: 10150.28, average training loss: 9351.81, base loss: 14387.95
[INFO 2017-06-28 14:54:30,047 main.py:51] epoch 9876, training loss: 10269.88, average training loss: 9352.46, base loss: 14389.99
[INFO 2017-06-28 14:54:31,108 main.py:51] epoch 9877, training loss: 9315.69, average training loss: 9353.09, base loss: 14389.04
[INFO 2017-06-28 14:54:32,202 main.py:51] epoch 9878, training loss: 8878.54, average training loss: 9351.82, base loss: 14386.63
[INFO 2017-06-28 14:54:33,288 main.py:51] epoch 9879, training loss: 9727.78, average training loss: 9351.88, base loss: 14386.11
[INFO 2017-06-28 14:54:34,405 main.py:51] epoch 9880, training loss: 8983.75, average training loss: 9350.32, base loss: 14382.60
[INFO 2017-06-28 14:54:35,459 main.py:51] epoch 9881, training loss: 8229.69, average training loss: 9349.51, base loss: 14379.95
[INFO 2017-06-28 14:54:36,478 main.py:51] epoch 9882, training loss: 10537.53, average training loss: 9350.13, base loss: 14382.33
[INFO 2017-06-28 14:54:37,517 main.py:51] epoch 9883, training loss: 9965.15, average training loss: 9350.67, base loss: 14383.05
[INFO 2017-06-28 14:54:38,589 main.py:51] epoch 9884, training loss: 8809.91, average training loss: 9349.50, base loss: 14380.02
[INFO 2017-06-28 14:54:39,689 main.py:51] epoch 9885, training loss: 10868.96, average training loss: 9351.71, base loss: 14384.72
[INFO 2017-06-28 14:54:40,718 main.py:51] epoch 9886, training loss: 10724.05, average training loss: 9353.67, base loss: 14386.64
[INFO 2017-06-28 14:54:41,754 main.py:51] epoch 9887, training loss: 10722.60, average training loss: 9356.44, base loss: 14392.21
[INFO 2017-06-28 14:54:42,843 main.py:51] epoch 9888, training loss: 8669.71, average training loss: 9353.20, base loss: 14385.96
[INFO 2017-06-28 14:54:43,796 main.py:51] epoch 9889, training loss: 8373.22, average training loss: 9353.20, base loss: 14387.50
[INFO 2017-06-28 14:54:44,736 main.py:51] epoch 9890, training loss: 9135.08, average training loss: 9352.95, base loss: 14384.88
[INFO 2017-06-28 14:54:45,693 main.py:51] epoch 9891, training loss: 9237.44, average training loss: 9351.46, base loss: 14383.59
[INFO 2017-06-28 14:54:46,648 main.py:51] epoch 9892, training loss: 10712.71, average training loss: 9351.00, base loss: 14382.22
[INFO 2017-06-28 14:54:47,688 main.py:51] epoch 9893, training loss: 8996.40, average training loss: 9352.12, base loss: 14382.69
[INFO 2017-06-28 14:54:48,770 main.py:51] epoch 9894, training loss: 9015.68, average training loss: 9351.22, base loss: 14381.21
[INFO 2017-06-28 14:54:49,848 main.py:51] epoch 9895, training loss: 9515.52, average training loss: 9351.45, base loss: 14379.98
[INFO 2017-06-28 14:54:50,889 main.py:51] epoch 9896, training loss: 8989.26, average training loss: 9351.35, base loss: 14379.93
[INFO 2017-06-28 14:54:51,908 main.py:51] epoch 9897, training loss: 8668.38, average training loss: 9351.95, base loss: 14381.12
[INFO 2017-06-28 14:54:52,987 main.py:51] epoch 9898, training loss: 9156.52, average training loss: 9349.90, base loss: 14379.57
[INFO 2017-06-28 14:54:54,093 main.py:51] epoch 9899, training loss: 8166.72, average training loss: 9348.41, base loss: 14377.67
[INFO 2017-06-28 14:54:54,093 main.py:53] epoch 9899, testing
[INFO 2017-06-28 14:54:57,752 main.py:105] average testing loss: 10908.49, base loss: 15461.97
[INFO 2017-06-28 14:54:57,752 main.py:106] improve_loss: 4553.48, improve_percent: 0.29
[INFO 2017-06-28 14:54:57,753 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:54:58,835 main.py:51] epoch 9900, training loss: 9708.97, average training loss: 9347.58, base loss: 14374.78
[INFO 2017-06-28 14:54:59,949 main.py:51] epoch 9901, training loss: 8586.42, average training loss: 9346.41, base loss: 14375.28
[INFO 2017-06-28 14:55:01,059 main.py:51] epoch 9902, training loss: 8741.18, average training loss: 9346.00, base loss: 14373.25
[INFO 2017-06-28 14:55:02,094 main.py:51] epoch 9903, training loss: 10405.48, average training loss: 9347.70, base loss: 14374.91
[INFO 2017-06-28 14:55:03,181 main.py:51] epoch 9904, training loss: 9899.50, average training loss: 9348.10, base loss: 14374.51
[INFO 2017-06-28 14:55:04,214 main.py:51] epoch 9905, training loss: 11574.41, average training loss: 9349.58, base loss: 14375.69
[INFO 2017-06-28 14:55:05,251 main.py:51] epoch 9906, training loss: 10169.40, average training loss: 9350.91, base loss: 14377.17
[INFO 2017-06-28 14:55:06,335 main.py:51] epoch 9907, training loss: 7873.31, average training loss: 9348.97, base loss: 14374.37
[INFO 2017-06-28 14:55:07,439 main.py:51] epoch 9908, training loss: 9968.62, average training loss: 9349.59, base loss: 14375.70
[INFO 2017-06-28 14:55:08,487 main.py:51] epoch 9909, training loss: 8308.84, average training loss: 9346.83, base loss: 14370.35
[INFO 2017-06-28 14:55:09,543 main.py:51] epoch 9910, training loss: 9703.59, average training loss: 9347.63, base loss: 14371.97
[INFO 2017-06-28 14:55:10,589 main.py:51] epoch 9911, training loss: 8512.70, average training loss: 9346.57, base loss: 14369.30
[INFO 2017-06-28 14:55:11,656 main.py:51] epoch 9912, training loss: 9555.51, average training loss: 9347.68, base loss: 14371.34
[INFO 2017-06-28 14:55:12,730 main.py:51] epoch 9913, training loss: 10751.69, average training loss: 9349.01, base loss: 14370.75
[INFO 2017-06-28 14:55:13,831 main.py:51] epoch 9914, training loss: 9305.65, average training loss: 9348.62, base loss: 14369.86
[INFO 2017-06-28 14:55:14,862 main.py:51] epoch 9915, training loss: 9920.29, average training loss: 9348.87, base loss: 14370.38
[INFO 2017-06-28 14:55:15,898 main.py:51] epoch 9916, training loss: 9003.84, average training loss: 9348.24, base loss: 14370.54
[INFO 2017-06-28 14:55:16,972 main.py:51] epoch 9917, training loss: 9505.51, average training loss: 9349.58, base loss: 14373.98
[INFO 2017-06-28 14:55:18,068 main.py:51] epoch 9918, training loss: 9969.10, average training loss: 9349.65, base loss: 14372.78
[INFO 2017-06-28 14:55:19,158 main.py:51] epoch 9919, training loss: 9756.26, average training loss: 9350.94, base loss: 14374.68
[INFO 2017-06-28 14:55:20,172 main.py:51] epoch 9920, training loss: 10101.85, average training loss: 9350.78, base loss: 14375.85
[INFO 2017-06-28 14:55:21,250 main.py:51] epoch 9921, training loss: 9946.67, average training loss: 9351.07, base loss: 14375.87
[INFO 2017-06-28 14:55:22,343 main.py:51] epoch 9922, training loss: 8285.92, average training loss: 9350.41, base loss: 14375.68
[INFO 2017-06-28 14:55:23,412 main.py:51] epoch 9923, training loss: 12643.73, average training loss: 9353.82, base loss: 14378.97
[INFO 2017-06-28 14:55:24,447 main.py:51] epoch 9924, training loss: 10792.82, average training loss: 9356.49, base loss: 14385.57
[INFO 2017-06-28 14:55:25,480 main.py:51] epoch 9925, training loss: 9828.00, average training loss: 9356.74, base loss: 14386.62
[INFO 2017-06-28 14:55:26,496 main.py:51] epoch 9926, training loss: 8490.25, average training loss: 9356.10, base loss: 14386.51
[INFO 2017-06-28 14:55:27,557 main.py:51] epoch 9927, training loss: 7883.36, average training loss: 9355.32, base loss: 14386.44
[INFO 2017-06-28 14:55:28,600 main.py:51] epoch 9928, training loss: 9081.32, average training loss: 9355.23, base loss: 14386.18
[INFO 2017-06-28 14:55:29,637 main.py:51] epoch 9929, training loss: 8558.28, average training loss: 9354.63, base loss: 14385.56
[INFO 2017-06-28 14:55:30,686 main.py:51] epoch 9930, training loss: 10170.21, average training loss: 9354.96, base loss: 14386.94
[INFO 2017-06-28 14:55:31,767 main.py:51] epoch 9931, training loss: 8762.83, average training loss: 9352.78, base loss: 14383.87
[INFO 2017-06-28 14:55:32,859 main.py:51] epoch 9932, training loss: 8544.53, average training loss: 9352.70, base loss: 14383.36
[INFO 2017-06-28 14:55:33,912 main.py:51] epoch 9933, training loss: 9703.35, average training loss: 9354.30, base loss: 14385.83
[INFO 2017-06-28 14:55:34,941 main.py:51] epoch 9934, training loss: 8803.35, average training loss: 9353.93, base loss: 14384.56
[INFO 2017-06-28 14:55:35,972 main.py:51] epoch 9935, training loss: 10237.76, average training loss: 9355.34, base loss: 14387.55
[INFO 2017-06-28 14:55:37,053 main.py:51] epoch 9936, training loss: 9677.14, average training loss: 9356.05, base loss: 14389.58
[INFO 2017-06-28 14:55:38,128 main.py:51] epoch 9937, training loss: 8256.25, average training loss: 9354.36, base loss: 14386.18
[INFO 2017-06-28 14:55:39,127 main.py:51] epoch 9938, training loss: 8728.79, average training loss: 9353.28, base loss: 14385.76
[INFO 2017-06-28 14:55:40,204 main.py:51] epoch 9939, training loss: 9084.80, average training loss: 9353.14, base loss: 14385.31
[INFO 2017-06-28 14:55:41,273 main.py:51] epoch 9940, training loss: 8020.39, average training loss: 9350.30, base loss: 14380.27
[INFO 2017-06-28 14:55:42,332 main.py:51] epoch 9941, training loss: 9648.29, average training loss: 9351.12, base loss: 14381.99
[INFO 2017-06-28 14:55:43,388 main.py:51] epoch 9942, training loss: 9093.81, average training loss: 9350.90, base loss: 14383.19
[INFO 2017-06-28 14:55:44,405 main.py:51] epoch 9943, training loss: 9037.65, average training loss: 9350.20, base loss: 14381.48
[INFO 2017-06-28 14:55:45,469 main.py:51] epoch 9944, training loss: 9166.00, average training loss: 9350.62, base loss: 14382.27
[INFO 2017-06-28 14:55:46,559 main.py:51] epoch 9945, training loss: 9417.46, average training loss: 9351.42, base loss: 14383.68
[INFO 2017-06-28 14:55:47,683 main.py:51] epoch 9946, training loss: 8722.43, average training loss: 9351.33, base loss: 14384.30
[INFO 2017-06-28 14:55:48,798 main.py:51] epoch 9947, training loss: 8605.64, average training loss: 9351.32, base loss: 14384.66
[INFO 2017-06-28 14:55:49,873 main.py:51] epoch 9948, training loss: 9336.95, average training loss: 9351.05, base loss: 14385.31
[INFO 2017-06-28 14:55:50,971 main.py:51] epoch 9949, training loss: 8513.81, average training loss: 9348.93, base loss: 14382.89
[INFO 2017-06-28 14:55:52,022 main.py:51] epoch 9950, training loss: 10886.05, average training loss: 9350.50, base loss: 14384.75
[INFO 2017-06-28 14:55:53,067 main.py:51] epoch 9951, training loss: 9890.37, average training loss: 9350.51, base loss: 14384.78
[INFO 2017-06-28 14:55:54,130 main.py:51] epoch 9952, training loss: 8637.57, average training loss: 9350.41, base loss: 14383.52
[INFO 2017-06-28 14:55:55,208 main.py:51] epoch 9953, training loss: 9530.04, average training loss: 9351.31, base loss: 14386.00
[INFO 2017-06-28 14:55:56,294 main.py:51] epoch 9954, training loss: 10343.27, average training loss: 9352.78, base loss: 14387.80
[INFO 2017-06-28 14:55:57,321 main.py:51] epoch 9955, training loss: 8537.13, average training loss: 9353.08, base loss: 14388.08
[INFO 2017-06-28 14:55:58,367 main.py:51] epoch 9956, training loss: 11137.30, average training loss: 9354.17, base loss: 14392.40
[INFO 2017-06-28 14:55:59,403 main.py:51] epoch 9957, training loss: 9096.88, average training loss: 9354.19, base loss: 14391.55
[INFO 2017-06-28 14:56:00,487 main.py:51] epoch 9958, training loss: 9376.92, average training loss: 9352.69, base loss: 14389.62
[INFO 2017-06-28 14:56:01,549 main.py:51] epoch 9959, training loss: 9174.88, average training loss: 9352.46, base loss: 14390.27
[INFO 2017-06-28 14:56:02,506 main.py:51] epoch 9960, training loss: 8576.73, average training loss: 9352.38, base loss: 14390.09
[INFO 2017-06-28 14:56:03,426 main.py:51] epoch 9961, training loss: 9167.09, average training loss: 9352.61, base loss: 14390.01
[INFO 2017-06-28 14:56:04,420 main.py:51] epoch 9962, training loss: 8869.74, average training loss: 9353.15, base loss: 14389.95
[INFO 2017-06-28 14:56:05,460 main.py:51] epoch 9963, training loss: 11294.38, average training loss: 9355.49, base loss: 14392.72
[INFO 2017-06-28 14:56:06,447 main.py:51] epoch 9964, training loss: 8742.00, average training loss: 9355.31, base loss: 14393.70
[INFO 2017-06-28 14:56:07,522 main.py:51] epoch 9965, training loss: 9542.60, average training loss: 9356.50, base loss: 14395.38
[INFO 2017-06-28 14:56:08,562 main.py:51] epoch 9966, training loss: 7660.24, average training loss: 9354.82, base loss: 14394.56
[INFO 2017-06-28 14:56:09,637 main.py:51] epoch 9967, training loss: 8256.89, average training loss: 9354.35, base loss: 14393.81
[INFO 2017-06-28 14:56:10,698 main.py:51] epoch 9968, training loss: 9749.80, average training loss: 9354.73, base loss: 14394.37
[INFO 2017-06-28 14:56:11,755 main.py:51] epoch 9969, training loss: 9027.45, average training loss: 9353.73, base loss: 14392.68
[INFO 2017-06-28 14:56:12,799 main.py:51] epoch 9970, training loss: 8808.14, average training loss: 9353.10, base loss: 14390.87
[INFO 2017-06-28 14:56:13,843 main.py:51] epoch 9971, training loss: 8953.79, average training loss: 9352.77, base loss: 14389.65
[INFO 2017-06-28 14:56:14,914 main.py:51] epoch 9972, training loss: 8336.02, average training loss: 9352.87, base loss: 14388.92
[INFO 2017-06-28 14:56:15,976 main.py:51] epoch 9973, training loss: 9160.69, average training loss: 9352.13, base loss: 14388.26
[INFO 2017-06-28 14:56:17,007 main.py:51] epoch 9974, training loss: 9789.29, average training loss: 9351.67, base loss: 14386.40
[INFO 2017-06-28 14:56:18,071 main.py:51] epoch 9975, training loss: 8524.20, average training loss: 9349.92, base loss: 14383.64
[INFO 2017-06-28 14:56:19,188 main.py:51] epoch 9976, training loss: 10446.61, average training loss: 9350.38, base loss: 14384.66
[INFO 2017-06-28 14:56:20,191 main.py:51] epoch 9977, training loss: 10531.35, average training loss: 9350.59, base loss: 14384.62
[INFO 2017-06-28 14:56:21,224 main.py:51] epoch 9978, training loss: 8860.86, average training loss: 9349.76, base loss: 14383.18
[INFO 2017-06-28 14:56:22,291 main.py:51] epoch 9979, training loss: 10154.70, average training loss: 9351.60, base loss: 14385.67
[INFO 2017-06-28 14:56:23,315 main.py:51] epoch 9980, training loss: 8566.36, average training loss: 9350.67, base loss: 14382.26
[INFO 2017-06-28 14:56:24,346 main.py:51] epoch 9981, training loss: 8631.07, average training loss: 9349.30, base loss: 14379.74
[INFO 2017-06-28 14:56:25,413 main.py:51] epoch 9982, training loss: 9187.33, average training loss: 9349.97, base loss: 14381.93
[INFO 2017-06-28 14:56:26,523 main.py:51] epoch 9983, training loss: 8769.82, average training loss: 9349.81, base loss: 14381.25
[INFO 2017-06-28 14:56:27,596 main.py:51] epoch 9984, training loss: 10517.54, average training loss: 9351.47, base loss: 14385.35
[INFO 2017-06-28 14:56:28,639 main.py:51] epoch 9985, training loss: 7741.37, average training loss: 9349.66, base loss: 14381.93
[INFO 2017-06-28 14:56:29,687 main.py:51] epoch 9986, training loss: 8297.67, average training loss: 9347.31, base loss: 14378.24
[INFO 2017-06-28 14:56:30,706 main.py:51] epoch 9987, training loss: 10251.29, average training loss: 9349.28, base loss: 14381.35
[INFO 2017-06-28 14:56:31,790 main.py:51] epoch 9988, training loss: 9771.05, average training loss: 9349.45, base loss: 14381.96
[INFO 2017-06-28 14:56:32,928 main.py:51] epoch 9989, training loss: 8249.73, average training loss: 9349.52, base loss: 14381.60
[INFO 2017-06-28 14:56:33,950 main.py:51] epoch 9990, training loss: 8673.83, average training loss: 9349.05, base loss: 14380.11
[INFO 2017-06-28 14:56:35,017 main.py:51] epoch 9991, training loss: 9463.20, average training loss: 9348.94, base loss: 14377.93
[INFO 2017-06-28 14:56:36,031 main.py:51] epoch 9992, training loss: 10083.19, average training loss: 9350.19, base loss: 14381.05
[INFO 2017-06-28 14:56:37,089 main.py:51] epoch 9993, training loss: 9192.31, average training loss: 9350.92, base loss: 14383.79
[INFO 2017-06-28 14:56:38,172 main.py:51] epoch 9994, training loss: 8692.13, average training loss: 9350.29, base loss: 14382.16
[INFO 2017-06-28 14:56:39,195 main.py:51] epoch 9995, training loss: 8896.73, average training loss: 9350.72, base loss: 14382.65
[INFO 2017-06-28 14:56:40,233 main.py:51] epoch 9996, training loss: 9075.37, average training loss: 9350.80, base loss: 14384.24
[INFO 2017-06-28 14:56:41,327 main.py:51] epoch 9997, training loss: 7808.47, average training loss: 9348.61, base loss: 14379.47
[INFO 2017-06-28 14:56:42,361 main.py:51] epoch 9998, training loss: 10266.24, average training loss: 9349.84, base loss: 14381.24
[INFO 2017-06-28 14:56:43,439 main.py:51] epoch 9999, training loss: 9362.94, average training loss: 9349.56, base loss: 14379.19
[INFO 2017-06-28 14:56:43,440 main.py:53] epoch 9999, testing
[INFO 2017-06-28 14:56:47,232 main.py:105] average testing loss: 10683.89, base loss: 14876.56
[INFO 2017-06-28 14:56:47,232 main.py:106] improve_loss: 4192.68, improve_percent: 0.28
[INFO 2017-06-28 14:56:47,233 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:56:48,284 main.py:51] epoch 10000, training loss: 8753.18, average training loss: 9345.86, base loss: 14372.14
[INFO 2017-06-28 14:56:49,318 main.py:51] epoch 10001, training loss: 10524.32, average training loss: 9347.90, base loss: 14374.03
[INFO 2017-06-28 14:56:50,381 main.py:51] epoch 10002, training loss: 8367.75, average training loss: 9346.29, base loss: 14372.88
[INFO 2017-06-28 14:56:51,414 main.py:51] epoch 10003, training loss: 9410.70, average training loss: 9346.63, base loss: 14374.07
[INFO 2017-06-28 14:56:52,501 main.py:51] epoch 10004, training loss: 9959.19, average training loss: 9348.57, base loss: 14376.59
[INFO 2017-06-28 14:56:53,597 main.py:51] epoch 10005, training loss: 8953.08, average training loss: 9347.06, base loss: 14375.17
[INFO 2017-06-28 14:56:54,605 main.py:51] epoch 10006, training loss: 8033.04, average training loss: 9345.56, base loss: 14372.74
[INFO 2017-06-28 14:56:55,651 main.py:51] epoch 10007, training loss: 8953.15, average training loss: 9345.65, base loss: 14372.69
[INFO 2017-06-28 14:56:56,738 main.py:51] epoch 10008, training loss: 8880.34, average training loss: 9345.42, base loss: 14371.49
[INFO 2017-06-28 14:56:57,815 main.py:51] epoch 10009, training loss: 10025.79, average training loss: 9345.90, base loss: 14372.29
[INFO 2017-06-28 14:56:58,891 main.py:51] epoch 10010, training loss: 9137.04, average training loss: 9345.89, base loss: 14372.21
[INFO 2017-06-28 14:56:59,894 main.py:51] epoch 10011, training loss: 9526.86, average training loss: 9346.17, base loss: 14373.66
[INFO 2017-06-28 14:57:00,971 main.py:51] epoch 10012, training loss: 9184.82, average training loss: 9346.24, base loss: 14374.99
[INFO 2017-06-28 14:57:02,064 main.py:51] epoch 10013, training loss: 8568.38, average training loss: 9345.09, base loss: 14373.58
[INFO 2017-06-28 14:57:03,066 main.py:51] epoch 10014, training loss: 9286.49, average training loss: 9346.57, base loss: 14378.88
[INFO 2017-06-28 14:57:04,072 main.py:51] epoch 10015, training loss: 9024.14, average training loss: 9347.48, base loss: 14379.23
[INFO 2017-06-28 14:57:05,157 main.py:51] epoch 10016, training loss: 10736.76, average training loss: 9349.09, base loss: 14381.04
[INFO 2017-06-28 14:57:06,176 main.py:51] epoch 10017, training loss: 10763.73, average training loss: 9350.60, base loss: 14384.10
[INFO 2017-06-28 14:57:07,222 main.py:51] epoch 10018, training loss: 9166.91, average training loss: 9349.56, base loss: 14383.13
[INFO 2017-06-28 14:57:08,258 main.py:51] epoch 10019, training loss: 9754.49, average training loss: 9349.18, base loss: 14382.54
[INFO 2017-06-28 14:57:09,339 main.py:51] epoch 10020, training loss: 9444.13, average training loss: 9350.00, base loss: 14384.18
[INFO 2017-06-28 14:57:10,425 main.py:51] epoch 10021, training loss: 11201.38, average training loss: 9351.65, base loss: 14387.53
[INFO 2017-06-28 14:57:11,423 main.py:51] epoch 10022, training loss: 9434.09, average training loss: 9351.03, base loss: 14387.33
[INFO 2017-06-28 14:57:12,492 main.py:51] epoch 10023, training loss: 9115.74, average training loss: 9350.72, base loss: 14385.38
[INFO 2017-06-28 14:57:13,574 main.py:51] epoch 10024, training loss: 9684.31, average training loss: 9351.07, base loss: 14386.83
[INFO 2017-06-28 14:57:14,642 main.py:51] epoch 10025, training loss: 8129.01, average training loss: 9350.76, base loss: 14384.22
[INFO 2017-06-28 14:57:15,718 main.py:51] epoch 10026, training loss: 9543.82, average training loss: 9351.34, base loss: 14385.48
[INFO 2017-06-28 14:57:16,714 main.py:51] epoch 10027, training loss: 10028.87, average training loss: 9352.30, base loss: 14387.92
[INFO 2017-06-28 14:57:17,782 main.py:51] epoch 10028, training loss: 8820.64, average training loss: 9351.71, base loss: 14386.25
[INFO 2017-06-28 14:57:18,860 main.py:51] epoch 10029, training loss: 8924.72, average training loss: 9350.01, base loss: 14384.57
[INFO 2017-06-28 14:57:19,834 main.py:51] epoch 10030, training loss: 9363.28, average training loss: 9349.81, base loss: 14383.32
[INFO 2017-06-28 14:57:20,771 main.py:51] epoch 10031, training loss: 9894.71, average training loss: 9350.25, base loss: 14385.49
[INFO 2017-06-28 14:57:21,719 main.py:51] epoch 10032, training loss: 9545.69, average training loss: 9349.82, base loss: 14385.77
[INFO 2017-06-28 14:57:22,653 main.py:51] epoch 10033, training loss: 9505.19, average training loss: 9350.15, base loss: 14387.66
[INFO 2017-06-28 14:57:23,650 main.py:51] epoch 10034, training loss: 10619.42, average training loss: 9351.21, base loss: 14388.86
[INFO 2017-06-28 14:57:24,699 main.py:51] epoch 10035, training loss: 8805.65, average training loss: 9351.81, base loss: 14390.76
[INFO 2017-06-28 14:57:25,735 main.py:51] epoch 10036, training loss: 9652.21, average training loss: 9352.21, base loss: 14391.70
[INFO 2017-06-28 14:57:26,745 main.py:51] epoch 10037, training loss: 7944.50, average training loss: 9349.44, base loss: 14386.10
[INFO 2017-06-28 14:57:27,785 main.py:51] epoch 10038, training loss: 10683.46, average training loss: 9351.29, base loss: 14389.54
[INFO 2017-06-28 14:57:28,825 main.py:51] epoch 10039, training loss: 7972.36, average training loss: 9351.43, base loss: 14388.59
[INFO 2017-06-28 14:57:29,874 main.py:51] epoch 10040, training loss: 9153.11, average training loss: 9352.01, base loss: 14390.00
[INFO 2017-06-28 14:57:31,022 main.py:51] epoch 10041, training loss: 9022.44, average training loss: 9352.64, base loss: 14392.50
[INFO 2017-06-28 14:57:32,026 main.py:51] epoch 10042, training loss: 9120.15, average training loss: 9352.87, base loss: 14392.37
[INFO 2017-06-28 14:57:33,046 main.py:51] epoch 10043, training loss: 9517.85, average training loss: 9353.12, base loss: 14391.39
[INFO 2017-06-28 14:57:34,128 main.py:51] epoch 10044, training loss: 9163.70, average training loss: 9351.52, base loss: 14388.47
[INFO 2017-06-28 14:57:35,140 main.py:51] epoch 10045, training loss: 9051.63, average training loss: 9350.81, base loss: 14387.82
[INFO 2017-06-28 14:57:36,170 main.py:51] epoch 10046, training loss: 10320.75, average training loss: 9350.26, base loss: 14388.58
[INFO 2017-06-28 14:57:37,210 main.py:51] epoch 10047, training loss: 10131.93, average training loss: 9350.64, base loss: 14390.47
[INFO 2017-06-28 14:57:38,274 main.py:51] epoch 10048, training loss: 8662.35, average training loss: 9349.68, base loss: 14387.20
[INFO 2017-06-28 14:57:39,376 main.py:51] epoch 10049, training loss: 9054.24, average training loss: 9348.94, base loss: 14387.37
[INFO 2017-06-28 14:57:40,436 main.py:51] epoch 10050, training loss: 9878.78, average training loss: 9348.41, base loss: 14386.38
[INFO 2017-06-28 14:57:41,466 main.py:51] epoch 10051, training loss: 8981.75, average training loss: 9347.95, base loss: 14383.70
[INFO 2017-06-28 14:57:42,526 main.py:51] epoch 10052, training loss: 8079.62, average training loss: 9346.07, base loss: 14381.08
[INFO 2017-06-28 14:57:43,571 main.py:51] epoch 10053, training loss: 9294.70, average training loss: 9345.36, base loss: 14380.80
[INFO 2017-06-28 14:57:44,611 main.py:51] epoch 10054, training loss: 9601.84, average training loss: 9346.94, base loss: 14383.07
[INFO 2017-06-28 14:57:45,637 main.py:51] epoch 10055, training loss: 10072.78, average training loss: 9348.23, base loss: 14383.20
[INFO 2017-06-28 14:57:46,649 main.py:51] epoch 10056, training loss: 7457.12, average training loss: 9345.43, base loss: 14378.98
[INFO 2017-06-28 14:57:47,651 main.py:51] epoch 10057, training loss: 11801.82, average training loss: 9348.20, base loss: 14384.29
[INFO 2017-06-28 14:57:48,725 main.py:51] epoch 10058, training loss: 8637.71, average training loss: 9348.15, base loss: 14385.32
[INFO 2017-06-28 14:57:49,764 main.py:51] epoch 10059, training loss: 8386.15, average training loss: 9348.52, base loss: 14384.77
[INFO 2017-06-28 14:57:50,799 main.py:51] epoch 10060, training loss: 10879.40, average training loss: 9348.72, base loss: 14386.33
[INFO 2017-06-28 14:57:51,845 main.py:51] epoch 10061, training loss: 8569.35, average training loss: 9348.79, base loss: 14385.73
[INFO 2017-06-28 14:57:52,834 main.py:51] epoch 10062, training loss: 8451.97, average training loss: 9347.36, base loss: 14383.03
[INFO 2017-06-28 14:57:53,871 main.py:51] epoch 10063, training loss: 8590.15, average training loss: 9346.05, base loss: 14382.49
[INFO 2017-06-28 14:57:54,920 main.py:51] epoch 10064, training loss: 10054.60, average training loss: 9345.45, base loss: 14381.27
[INFO 2017-06-28 14:57:55,995 main.py:51] epoch 10065, training loss: 9746.01, average training loss: 9345.93, base loss: 14380.98
[INFO 2017-06-28 14:57:57,065 main.py:51] epoch 10066, training loss: 8907.10, average training loss: 9344.39, base loss: 14380.05
[INFO 2017-06-28 14:57:58,070 main.py:51] epoch 10067, training loss: 8340.76, average training loss: 9341.94, base loss: 14377.66
[INFO 2017-06-28 14:57:59,094 main.py:51] epoch 10068, training loss: 9820.95, average training loss: 9342.51, base loss: 14380.19
[INFO 2017-06-28 14:58:00,116 main.py:51] epoch 10069, training loss: 9071.87, average training loss: 9341.52, base loss: 14379.73
[INFO 2017-06-28 14:58:01,168 main.py:51] epoch 10070, training loss: 9620.26, average training loss: 9342.38, base loss: 14380.77
[INFO 2017-06-28 14:58:02,233 main.py:51] epoch 10071, training loss: 9855.00, average training loss: 9343.20, base loss: 14382.44
[INFO 2017-06-28 14:58:03,322 main.py:51] epoch 10072, training loss: 9159.39, average training loss: 9343.69, base loss: 14384.76
[INFO 2017-06-28 14:58:04,364 main.py:51] epoch 10073, training loss: 8147.87, average training loss: 9343.01, base loss: 14383.75
[INFO 2017-06-28 14:58:05,404 main.py:51] epoch 10074, training loss: 8417.89, average training loss: 9343.07, base loss: 14385.68
[INFO 2017-06-28 14:58:06,433 main.py:51] epoch 10075, training loss: 9240.98, average training loss: 9342.86, base loss: 14384.55
[INFO 2017-06-28 14:58:07,489 main.py:51] epoch 10076, training loss: 9273.69, average training loss: 9343.47, base loss: 14384.08
[INFO 2017-06-28 14:58:08,586 main.py:51] epoch 10077, training loss: 8915.26, average training loss: 9343.04, base loss: 14384.56
[INFO 2017-06-28 14:58:09,646 main.py:51] epoch 10078, training loss: 9376.20, average training loss: 9342.97, base loss: 14383.58
[INFO 2017-06-28 14:58:10,696 main.py:51] epoch 10079, training loss: 7958.62, average training loss: 9341.71, base loss: 14382.79
[INFO 2017-06-28 14:58:11,749 main.py:51] epoch 10080, training loss: 10062.23, average training loss: 9343.09, base loss: 14385.96
[INFO 2017-06-28 14:58:12,846 main.py:51] epoch 10081, training loss: 9381.10, average training loss: 9343.85, base loss: 14385.81
[INFO 2017-06-28 14:58:13,947 main.py:51] epoch 10082, training loss: 9837.14, average training loss: 9343.11, base loss: 14386.07
[INFO 2017-06-28 14:58:14,941 main.py:51] epoch 10083, training loss: 10312.46, average training loss: 9344.54, base loss: 14388.20
[INFO 2017-06-28 14:58:15,977 main.py:51] epoch 10084, training loss: 9803.64, average training loss: 9344.98, base loss: 14389.20
[INFO 2017-06-28 14:58:17,017 main.py:51] epoch 10085, training loss: 9409.88, average training loss: 9346.09, base loss: 14390.46
[INFO 2017-06-28 14:58:18,029 main.py:51] epoch 10086, training loss: 9149.77, average training loss: 9344.60, base loss: 14386.71
[INFO 2017-06-28 14:58:19,069 main.py:51] epoch 10087, training loss: 8749.35, average training loss: 9344.25, base loss: 14387.54
[INFO 2017-06-28 14:58:20,106 main.py:51] epoch 10088, training loss: 9872.34, average training loss: 9343.64, base loss: 14386.78
[INFO 2017-06-28 14:58:21,162 main.py:51] epoch 10089, training loss: 9742.03, average training loss: 9344.43, base loss: 14388.51
[INFO 2017-06-28 14:58:22,247 main.py:51] epoch 10090, training loss: 10317.67, average training loss: 9346.65, base loss: 14392.33
[INFO 2017-06-28 14:58:23,263 main.py:51] epoch 10091, training loss: 10512.68, average training loss: 9348.68, base loss: 14394.06
[INFO 2017-06-28 14:58:24,314 main.py:51] epoch 10092, training loss: 10410.94, average training loss: 9348.63, base loss: 14395.76
[INFO 2017-06-28 14:58:25,405 main.py:51] epoch 10093, training loss: 9949.29, average training loss: 9349.46, base loss: 14396.86
[INFO 2017-06-28 14:58:26,414 main.py:51] epoch 10094, training loss: 8908.38, average training loss: 9349.33, base loss: 14396.02
[INFO 2017-06-28 14:58:27,458 main.py:51] epoch 10095, training loss: 9274.08, average training loss: 9348.30, base loss: 14394.59
[INFO 2017-06-28 14:58:28,554 main.py:51] epoch 10096, training loss: 9790.62, average training loss: 9348.41, base loss: 14393.83
[INFO 2017-06-28 14:58:29,632 main.py:51] epoch 10097, training loss: 8891.05, average training loss: 9347.08, base loss: 14393.12
[INFO 2017-06-28 14:58:30,739 main.py:51] epoch 10098, training loss: 7828.77, average training loss: 9345.29, base loss: 14390.88
[INFO 2017-06-28 14:58:31,722 main.py:51] epoch 10099, training loss: 9153.68, average training loss: 9344.21, base loss: 14389.55
[INFO 2017-06-28 14:58:31,722 main.py:53] epoch 10099, testing
[INFO 2017-06-28 14:58:35,341 main.py:105] average testing loss: 10651.85, base loss: 15166.41
[INFO 2017-06-28 14:58:35,341 main.py:106] improve_loss: 4514.56, improve_percent: 0.30
[INFO 2017-06-28 14:58:35,341 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 14:58:36,395 main.py:51] epoch 10100, training loss: 9781.97, average training loss: 9344.66, base loss: 14388.95
[INFO 2017-06-28 14:58:37,473 main.py:51] epoch 10101, training loss: 9597.68, average training loss: 9344.30, base loss: 14387.44
[INFO 2017-06-28 14:58:38,582 main.py:51] epoch 10102, training loss: 10395.13, average training loss: 9345.94, base loss: 14390.12
[INFO 2017-06-28 14:58:39,596 main.py:51] epoch 10103, training loss: 8326.69, average training loss: 9344.29, base loss: 14385.88
[INFO 2017-06-28 14:58:40,535 main.py:51] epoch 10104, training loss: 9707.40, average training loss: 9344.34, base loss: 14386.70
[INFO 2017-06-28 14:58:41,487 main.py:51] epoch 10105, training loss: 9553.60, average training loss: 9344.46, base loss: 14387.62
[INFO 2017-06-28 14:58:42,533 main.py:51] epoch 10106, training loss: 8320.24, average training loss: 9342.89, base loss: 14384.58
[INFO 2017-06-28 14:58:43,646 main.py:51] epoch 10107, training loss: 8323.62, average training loss: 9341.78, base loss: 14381.82
[INFO 2017-06-28 14:58:44,678 main.py:51] epoch 10108, training loss: 8104.46, average training loss: 9342.21, base loss: 14382.67
[INFO 2017-06-28 14:58:45,720 main.py:51] epoch 10109, training loss: 8877.35, average training loss: 9342.57, base loss: 14382.86
[INFO 2017-06-28 14:58:46,751 main.py:51] epoch 10110, training loss: 12051.38, average training loss: 9346.23, base loss: 14387.62
[INFO 2017-06-28 14:58:47,798 main.py:51] epoch 10111, training loss: 9752.56, average training loss: 9347.03, base loss: 14388.74
[INFO 2017-06-28 14:58:48,867 main.py:51] epoch 10112, training loss: 9747.73, average training loss: 9347.44, base loss: 14390.05
[INFO 2017-06-28 14:58:49,908 main.py:51] epoch 10113, training loss: 8424.58, average training loss: 9347.75, base loss: 14391.77
[INFO 2017-06-28 14:58:50,959 main.py:51] epoch 10114, training loss: 9334.42, average training loss: 9349.00, base loss: 14394.75
[INFO 2017-06-28 14:58:52,042 main.py:51] epoch 10115, training loss: 10040.32, average training loss: 9351.00, base loss: 14396.28
[INFO 2017-06-28 14:58:53,096 main.py:51] epoch 10116, training loss: 9242.72, average training loss: 9350.93, base loss: 14394.54
[INFO 2017-06-28 14:58:54,149 main.py:51] epoch 10117, training loss: 9519.21, average training loss: 9351.30, base loss: 14396.97
[INFO 2017-06-28 14:58:55,184 main.py:51] epoch 10118, training loss: 8324.03, average training loss: 9349.92, base loss: 14395.15
[INFO 2017-06-28 14:58:56,273 main.py:51] epoch 10119, training loss: 9612.79, average training loss: 9350.74, base loss: 14396.85
[INFO 2017-06-28 14:58:57,400 main.py:51] epoch 10120, training loss: 10292.90, average training loss: 9350.70, base loss: 14395.32
[INFO 2017-06-28 14:58:58,496 main.py:51] epoch 10121, training loss: 8980.39, average training loss: 9349.45, base loss: 14393.18
[INFO 2017-06-28 14:58:59,553 main.py:51] epoch 10122, training loss: 10280.69, average training loss: 9350.22, base loss: 14393.93
[INFO 2017-06-28 14:59:00,567 main.py:51] epoch 10123, training loss: 9872.30, average training loss: 9350.87, base loss: 14395.56
[INFO 2017-06-28 14:59:01,644 main.py:51] epoch 10124, training loss: 9031.17, average training loss: 9349.64, base loss: 14393.37
[INFO 2017-06-28 14:59:02,762 main.py:51] epoch 10125, training loss: 9037.48, average training loss: 9348.75, base loss: 14391.23
[INFO 2017-06-28 14:59:03,841 main.py:51] epoch 10126, training loss: 8985.99, average training loss: 9347.95, base loss: 14390.10
[INFO 2017-06-28 14:59:04,874 main.py:51] epoch 10127, training loss: 9208.08, average training loss: 9348.29, base loss: 14388.99
[INFO 2017-06-28 14:59:05,916 main.py:51] epoch 10128, training loss: 8115.68, average training loss: 9346.68, base loss: 14385.89
[INFO 2017-06-28 14:59:06,979 main.py:51] epoch 10129, training loss: 9143.04, average training loss: 9346.60, base loss: 14385.52
[INFO 2017-06-28 14:59:08,118 main.py:51] epoch 10130, training loss: 7843.07, average training loss: 9344.09, base loss: 14382.22
[INFO 2017-06-28 14:59:09,171 main.py:51] epoch 10131, training loss: 10382.59, average training loss: 9345.34, base loss: 14384.63
[INFO 2017-06-28 14:59:10,256 main.py:51] epoch 10132, training loss: 8278.54, average training loss: 9344.09, base loss: 14383.79
[INFO 2017-06-28 14:59:11,265 main.py:51] epoch 10133, training loss: 10325.11, average training loss: 9344.37, base loss: 14383.64
[INFO 2017-06-28 14:59:12,317 main.py:51] epoch 10134, training loss: 9489.55, average training loss: 9343.85, base loss: 14385.09
[INFO 2017-06-28 14:59:13,420 main.py:51] epoch 10135, training loss: 9397.78, average training loss: 9345.29, base loss: 14386.85
[INFO 2017-06-28 14:59:14,507 main.py:51] epoch 10136, training loss: 8825.35, average training loss: 9343.94, base loss: 14383.85
[INFO 2017-06-28 14:59:15,635 main.py:51] epoch 10137, training loss: 9750.42, average training loss: 9343.01, base loss: 14383.55
[INFO 2017-06-28 14:59:16,642 main.py:51] epoch 10138, training loss: 8080.44, average training loss: 9341.88, base loss: 14381.34
[INFO 2017-06-28 14:59:17,697 main.py:51] epoch 10139, training loss: 10526.72, average training loss: 9342.33, base loss: 14380.87
[INFO 2017-06-28 14:59:18,748 main.py:51] epoch 10140, training loss: 11015.90, average training loss: 9343.89, base loss: 14382.62
[INFO 2017-06-28 14:59:19,842 main.py:51] epoch 10141, training loss: 10079.52, average training loss: 9344.88, base loss: 14384.47
[INFO 2017-06-28 14:59:20,924 main.py:51] epoch 10142, training loss: 10674.79, average training loss: 9347.22, base loss: 14389.68
[INFO 2017-06-28 14:59:21,995 main.py:51] epoch 10143, training loss: 9414.94, average training loss: 9347.76, base loss: 14389.47
[INFO 2017-06-28 14:59:23,069 main.py:51] epoch 10144, training loss: 8546.90, average training loss: 9347.38, base loss: 14387.46
[INFO 2017-06-28 14:59:24,092 main.py:51] epoch 10145, training loss: 10196.87, average training loss: 9347.28, base loss: 14387.81
[INFO 2017-06-28 14:59:25,162 main.py:51] epoch 10146, training loss: 9117.36, average training loss: 9344.72, base loss: 14383.88
[INFO 2017-06-28 14:59:26,257 main.py:51] epoch 10147, training loss: 9664.16, average training loss: 9344.71, base loss: 14383.01
[INFO 2017-06-28 14:59:27,289 main.py:51] epoch 10148, training loss: 8123.51, average training loss: 9342.44, base loss: 14379.12
[INFO 2017-06-28 14:59:28,382 main.py:51] epoch 10149, training loss: 9091.81, average training loss: 9343.91, base loss: 14382.35
[INFO 2017-06-28 14:59:29,371 main.py:51] epoch 10150, training loss: 8657.80, average training loss: 9342.94, base loss: 14381.53
[INFO 2017-06-28 14:59:30,451 main.py:51] epoch 10151, training loss: 10109.52, average training loss: 9344.51, base loss: 14383.68
[INFO 2017-06-28 14:59:31,560 main.py:51] epoch 10152, training loss: 9210.05, average training loss: 9344.83, base loss: 14383.31
[INFO 2017-06-28 14:59:32,642 main.py:51] epoch 10153, training loss: 10877.27, average training loss: 9346.74, base loss: 14388.27
[INFO 2017-06-28 14:59:33,701 main.py:51] epoch 10154, training loss: 8845.95, average training loss: 9346.49, base loss: 14388.17
[INFO 2017-06-28 14:59:34,737 main.py:51] epoch 10155, training loss: 9939.29, average training loss: 9347.85, base loss: 14391.38
[INFO 2017-06-28 14:59:35,811 main.py:51] epoch 10156, training loss: 9400.75, average training loss: 9348.31, base loss: 14393.39
[INFO 2017-06-28 14:59:36,907 main.py:51] epoch 10157, training loss: 8267.78, average training loss: 9346.78, base loss: 14392.04
[INFO 2017-06-28 14:59:37,941 main.py:51] epoch 10158, training loss: 8636.84, average training loss: 9346.47, base loss: 14391.60
[INFO 2017-06-28 14:59:39,048 main.py:51] epoch 10159, training loss: 10922.21, average training loss: 9348.66, base loss: 14394.94
[INFO 2017-06-28 14:59:40,045 main.py:51] epoch 10160, training loss: 9428.54, average training loss: 9350.20, base loss: 14396.64
[INFO 2017-06-28 14:59:41,118 main.py:51] epoch 10161, training loss: 8790.10, average training loss: 9348.02, base loss: 14394.22
[INFO 2017-06-28 14:59:42,208 main.py:51] epoch 10162, training loss: 11125.05, average training loss: 9348.74, base loss: 14397.66
[INFO 2017-06-28 14:59:43,205 main.py:51] epoch 10163, training loss: 9398.69, average training loss: 9347.30, base loss: 14395.57
[INFO 2017-06-28 14:59:44,188 main.py:51] epoch 10164, training loss: 8097.23, average training loss: 9346.88, base loss: 14394.27
[INFO 2017-06-28 14:59:45,290 main.py:51] epoch 10165, training loss: 8636.25, average training loss: 9346.38, base loss: 14394.55
[INFO 2017-06-28 14:59:46,307 main.py:51] epoch 10166, training loss: 9574.60, average training loss: 9346.62, base loss: 14395.29
[INFO 2017-06-28 14:59:47,375 main.py:51] epoch 10167, training loss: 8802.58, average training loss: 9345.71, base loss: 14391.32
[INFO 2017-06-28 14:59:48,409 main.py:51] epoch 10168, training loss: 8765.91, average training loss: 9345.18, base loss: 14390.99
[INFO 2017-06-28 14:59:49,478 main.py:51] epoch 10169, training loss: 9113.85, average training loss: 9344.95, base loss: 14391.58
[INFO 2017-06-28 14:59:50,628 main.py:51] epoch 10170, training loss: 8384.49, average training loss: 9343.43, base loss: 14388.76
[INFO 2017-06-28 14:59:51,739 main.py:51] epoch 10171, training loss: 10309.57, average training loss: 9345.37, base loss: 14391.75
[INFO 2017-06-28 14:59:52,841 main.py:51] epoch 10172, training loss: 9927.30, average training loss: 9345.46, base loss: 14391.70
[INFO 2017-06-28 14:59:53,891 main.py:51] epoch 10173, training loss: 8834.61, average training loss: 9344.27, base loss: 14388.76
[INFO 2017-06-28 14:59:54,943 main.py:51] epoch 10174, training loss: 8472.71, average training loss: 9343.50, base loss: 14384.88
[INFO 2017-06-28 14:59:55,964 main.py:51] epoch 10175, training loss: 9216.51, average training loss: 9343.21, base loss: 14385.57
[INFO 2017-06-28 14:59:57,038 main.py:51] epoch 10176, training loss: 10751.92, average training loss: 9345.76, base loss: 14390.04
[INFO 2017-06-28 14:59:58,069 main.py:51] epoch 10177, training loss: 8141.73, average training loss: 9344.82, base loss: 14387.74
[INFO 2017-06-28 14:59:59,021 main.py:51] epoch 10178, training loss: 9888.08, average training loss: 9344.96, base loss: 14386.87
[INFO 2017-06-28 14:59:59,992 main.py:51] epoch 10179, training loss: 8647.71, average training loss: 9344.55, base loss: 14386.81
[INFO 2017-06-28 15:00:00,986 main.py:51] epoch 10180, training loss: 10727.36, average training loss: 9346.37, base loss: 14388.95
[INFO 2017-06-28 15:00:02,069 main.py:51] epoch 10181, training loss: 9453.45, average training loss: 9347.26, base loss: 14390.65
[INFO 2017-06-28 15:00:03,121 main.py:51] epoch 10182, training loss: 9672.54, average training loss: 9348.20, base loss: 14391.34
[INFO 2017-06-28 15:00:04,152 main.py:51] epoch 10183, training loss: 10105.74, average training loss: 9348.10, base loss: 14392.85
[INFO 2017-06-28 15:00:05,226 main.py:51] epoch 10184, training loss: 10569.65, average training loss: 9350.57, base loss: 14396.57
[INFO 2017-06-28 15:00:06,299 main.py:51] epoch 10185, training loss: 8824.62, average training loss: 9351.23, base loss: 14398.64
[INFO 2017-06-28 15:00:07,306 main.py:51] epoch 10186, training loss: 9134.58, average training loss: 9350.48, base loss: 14398.86
[INFO 2017-06-28 15:00:08,320 main.py:51] epoch 10187, training loss: 9070.03, average training loss: 9351.14, base loss: 14400.83
[INFO 2017-06-28 15:00:09,375 main.py:51] epoch 10188, training loss: 9398.80, average training loss: 9351.61, base loss: 14401.65
[INFO 2017-06-28 15:00:10,458 main.py:51] epoch 10189, training loss: 9074.71, average training loss: 9352.15, base loss: 14403.64
[INFO 2017-06-28 15:00:11,504 main.py:51] epoch 10190, training loss: 10356.96, average training loss: 9353.06, base loss: 14404.10
[INFO 2017-06-28 15:00:12,575 main.py:51] epoch 10191, training loss: 9789.43, average training loss: 9353.74, base loss: 14405.47
[INFO 2017-06-28 15:00:13,610 main.py:51] epoch 10192, training loss: 8625.11, average training loss: 9354.09, base loss: 14407.01
[INFO 2017-06-28 15:00:14,645 main.py:51] epoch 10193, training loss: 9206.07, average training loss: 9351.45, base loss: 14401.39
[INFO 2017-06-28 15:00:15,714 main.py:51] epoch 10194, training loss: 8740.86, average training loss: 9351.36, base loss: 14401.93
[INFO 2017-06-28 15:00:16,793 main.py:51] epoch 10195, training loss: 8846.15, average training loss: 9351.38, base loss: 14402.65
[INFO 2017-06-28 15:00:17,871 main.py:51] epoch 10196, training loss: 9712.49, average training loss: 9351.55, base loss: 14403.90
[INFO 2017-06-28 15:00:18,970 main.py:51] epoch 10197, training loss: 9000.11, average training loss: 9351.24, base loss: 14404.61
[INFO 2017-06-28 15:00:20,070 main.py:51] epoch 10198, training loss: 8925.30, average training loss: 9351.49, base loss: 14406.14
[INFO 2017-06-28 15:00:21,144 main.py:51] epoch 10199, training loss: 10007.03, average training loss: 9352.39, base loss: 14407.14
[INFO 2017-06-28 15:00:21,144 main.py:53] epoch 10199, testing
[INFO 2017-06-28 15:00:24,973 main.py:105] average testing loss: 10007.37, base loss: 14277.57
[INFO 2017-06-28 15:00:24,974 main.py:106] improve_loss: 4270.21, improve_percent: 0.30
[INFO 2017-06-28 15:00:24,974 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:00:26,054 main.py:51] epoch 10200, training loss: 9864.98, average training loss: 9352.47, base loss: 14406.39
[INFO 2017-06-28 15:00:27,162 main.py:51] epoch 10201, training loss: 9419.56, average training loss: 9353.42, base loss: 14407.77
[INFO 2017-06-28 15:00:28,260 main.py:51] epoch 10202, training loss: 8737.28, average training loss: 9351.71, base loss: 14405.06
[INFO 2017-06-28 15:00:29,264 main.py:51] epoch 10203, training loss: 9666.91, average training loss: 9350.77, base loss: 14405.90
[INFO 2017-06-28 15:00:30,327 main.py:51] epoch 10204, training loss: 10097.25, average training loss: 9351.66, base loss: 14407.29
[INFO 2017-06-28 15:00:31,358 main.py:51] epoch 10205, training loss: 10128.02, average training loss: 9351.79, base loss: 14406.42
[INFO 2017-06-28 15:00:32,405 main.py:51] epoch 10206, training loss: 10044.14, average training loss: 9351.71, base loss: 14406.79
[INFO 2017-06-28 15:00:33,464 main.py:51] epoch 10207, training loss: 8208.83, average training loss: 9352.28, base loss: 14407.05
[INFO 2017-06-28 15:00:34,485 main.py:51] epoch 10208, training loss: 9241.86, average training loss: 9351.90, base loss: 14406.28
[INFO 2017-06-28 15:00:35,491 main.py:51] epoch 10209, training loss: 8974.81, average training loss: 9352.61, base loss: 14407.34
[INFO 2017-06-28 15:00:36,557 main.py:51] epoch 10210, training loss: 9766.97, average training loss: 9352.86, base loss: 14410.25
[INFO 2017-06-28 15:00:37,615 main.py:51] epoch 10211, training loss: 9334.08, average training loss: 9353.43, base loss: 14411.05
[INFO 2017-06-28 15:00:38,643 main.py:51] epoch 10212, training loss: 8895.71, average training loss: 9353.85, base loss: 14411.28
[INFO 2017-06-28 15:00:39,649 main.py:51] epoch 10213, training loss: 9578.90, average training loss: 9353.76, base loss: 14410.64
[INFO 2017-06-28 15:00:40,645 main.py:51] epoch 10214, training loss: 10777.12, average training loss: 9354.63, base loss: 14412.13
[INFO 2017-06-28 15:00:41,745 main.py:51] epoch 10215, training loss: 10909.13, average training loss: 9355.01, base loss: 14412.45
[INFO 2017-06-28 15:00:42,743 main.py:51] epoch 10216, training loss: 11271.84, average training loss: 9358.04, base loss: 14416.36
[INFO 2017-06-28 15:00:43,819 main.py:51] epoch 10217, training loss: 8496.56, average training loss: 9357.47, base loss: 14415.95
[INFO 2017-06-28 15:00:44,941 main.py:51] epoch 10218, training loss: 9271.89, average training loss: 9357.57, base loss: 14416.26
[INFO 2017-06-28 15:00:45,975 main.py:51] epoch 10219, training loss: 9390.91, average training loss: 9357.24, base loss: 14414.12
[INFO 2017-06-28 15:00:47,074 main.py:51] epoch 10220, training loss: 8802.65, average training loss: 9356.85, base loss: 14414.48
[INFO 2017-06-28 15:00:48,114 main.py:51] epoch 10221, training loss: 10977.27, average training loss: 9358.30, base loss: 14416.69
[INFO 2017-06-28 15:00:49,163 main.py:51] epoch 10222, training loss: 8935.71, average training loss: 9357.13, base loss: 14415.30
[INFO 2017-06-28 15:00:50,270 main.py:51] epoch 10223, training loss: 8635.49, average training loss: 9356.38, base loss: 14414.84
[INFO 2017-06-28 15:00:51,282 main.py:51] epoch 10224, training loss: 9652.56, average training loss: 9356.06, base loss: 14415.02
[INFO 2017-06-28 15:00:52,349 main.py:51] epoch 10225, training loss: 9005.06, average training loss: 9355.19, base loss: 14413.55
[INFO 2017-06-28 15:00:53,365 main.py:51] epoch 10226, training loss: 8240.50, average training loss: 9352.62, base loss: 14408.60
[INFO 2017-06-28 15:00:54,382 main.py:51] epoch 10227, training loss: 8392.61, average training loss: 9351.99, base loss: 14406.42
[INFO 2017-06-28 15:00:55,484 main.py:51] epoch 10228, training loss: 9046.02, average training loss: 9352.17, base loss: 14405.21
[INFO 2017-06-28 15:00:56,501 main.py:51] epoch 10229, training loss: 8674.78, average training loss: 9351.12, base loss: 14402.71
[INFO 2017-06-28 15:00:57,538 main.py:51] epoch 10230, training loss: 9673.63, average training loss: 9350.33, base loss: 14402.87
[INFO 2017-06-28 15:00:58,552 main.py:51] epoch 10231, training loss: 8633.83, average training loss: 9349.76, base loss: 14401.36
[INFO 2017-06-28 15:00:59,580 main.py:51] epoch 10232, training loss: 8559.66, average training loss: 9349.64, base loss: 14401.82
[INFO 2017-06-28 15:01:00,668 main.py:51] epoch 10233, training loss: 10231.64, average training loss: 9350.86, base loss: 14404.70
[INFO 2017-06-28 15:01:01,743 main.py:51] epoch 10234, training loss: 8400.60, average training loss: 9351.21, base loss: 14405.56
[INFO 2017-06-28 15:01:02,776 main.py:51] epoch 10235, training loss: 9165.77, average training loss: 9351.94, base loss: 14408.52
[INFO 2017-06-28 15:01:03,813 main.py:51] epoch 10236, training loss: 8851.99, average training loss: 9352.18, base loss: 14409.00
[INFO 2017-06-28 15:01:04,883 main.py:51] epoch 10237, training loss: 8935.63, average training loss: 9351.51, base loss: 14408.18
[INFO 2017-06-28 15:01:05,948 main.py:51] epoch 10238, training loss: 11008.55, average training loss: 9352.37, base loss: 14407.07
[INFO 2017-06-28 15:01:07,032 main.py:51] epoch 10239, training loss: 10935.57, average training loss: 9355.52, base loss: 14414.19
[INFO 2017-06-28 15:01:08,056 main.py:51] epoch 10240, training loss: 8754.47, average training loss: 9354.77, base loss: 14413.11
[INFO 2017-06-28 15:01:09,111 main.py:51] epoch 10241, training loss: 8199.44, average training loss: 9351.69, base loss: 14409.73
[INFO 2017-06-28 15:01:10,196 main.py:51] epoch 10242, training loss: 10389.59, average training loss: 9352.44, base loss: 14410.57
[INFO 2017-06-28 15:01:11,198 main.py:51] epoch 10243, training loss: 9700.88, average training loss: 9353.06, base loss: 14414.35
[INFO 2017-06-28 15:01:12,260 main.py:51] epoch 10244, training loss: 9561.85, average training loss: 9352.92, base loss: 14414.23
[INFO 2017-06-28 15:01:13,295 main.py:51] epoch 10245, training loss: 11283.11, average training loss: 9355.61, base loss: 14419.38
[INFO 2017-06-28 15:01:14,363 main.py:51] epoch 10246, training loss: 10016.47, average training loss: 9356.44, base loss: 14419.74
[INFO 2017-06-28 15:01:15,425 main.py:51] epoch 10247, training loss: 10502.83, average training loss: 9357.86, base loss: 14423.47
[INFO 2017-06-28 15:01:16,369 main.py:51] epoch 10248, training loss: 10120.78, average training loss: 9358.82, base loss: 14426.24
[INFO 2017-06-28 15:01:17,315 main.py:51] epoch 10249, training loss: 9979.59, average training loss: 9358.91, base loss: 14427.33
[INFO 2017-06-28 15:01:18,266 main.py:51] epoch 10250, training loss: 9120.92, average training loss: 9358.27, base loss: 14425.77
[INFO 2017-06-28 15:01:19,223 main.py:51] epoch 10251, training loss: 8404.81, average training loss: 9357.74, base loss: 14425.24
[INFO 2017-06-28 15:01:20,269 main.py:51] epoch 10252, training loss: 8577.29, average training loss: 9356.52, base loss: 14422.77
[INFO 2017-06-28 15:01:21,326 main.py:51] epoch 10253, training loss: 7799.72, average training loss: 9354.99, base loss: 14420.46
[INFO 2017-06-28 15:01:22,321 main.py:51] epoch 10254, training loss: 8758.39, average training loss: 9353.54, base loss: 14416.48
[INFO 2017-06-28 15:01:23,322 main.py:51] epoch 10255, training loss: 10173.08, average training loss: 9354.86, base loss: 14417.28
[INFO 2017-06-28 15:01:24,417 main.py:51] epoch 10256, training loss: 9520.64, average training loss: 9355.58, base loss: 14418.92
[INFO 2017-06-28 15:01:25,436 main.py:51] epoch 10257, training loss: 9826.21, average training loss: 9356.57, base loss: 14420.01
[INFO 2017-06-28 15:01:26,476 main.py:51] epoch 10258, training loss: 9921.60, average training loss: 9356.91, base loss: 14422.88
[INFO 2017-06-28 15:01:27,509 main.py:51] epoch 10259, training loss: 8307.60, average training loss: 9354.53, base loss: 14418.84
[INFO 2017-06-28 15:01:28,584 main.py:51] epoch 10260, training loss: 10396.35, average training loss: 9355.03, base loss: 14421.39
[INFO 2017-06-28 15:01:29,646 main.py:51] epoch 10261, training loss: 9852.86, average training loss: 9355.08, base loss: 14423.23
[INFO 2017-06-28 15:01:30,735 main.py:51] epoch 10262, training loss: 9182.44, average training loss: 9354.05, base loss: 14420.46
[INFO 2017-06-28 15:01:31,739 main.py:51] epoch 10263, training loss: 9643.19, average training loss: 9354.18, base loss: 14420.90
[INFO 2017-06-28 15:01:32,760 main.py:51] epoch 10264, training loss: 8539.51, average training loss: 9352.96, base loss: 14417.57
[INFO 2017-06-28 15:01:33,832 main.py:51] epoch 10265, training loss: 8952.95, average training loss: 9351.88, base loss: 14416.33
[INFO 2017-06-28 15:01:34,907 main.py:51] epoch 10266, training loss: 8136.10, average training loss: 9349.42, base loss: 14412.43
[INFO 2017-06-28 15:01:35,978 main.py:51] epoch 10267, training loss: 9522.15, average training loss: 9350.02, base loss: 14412.74
[INFO 2017-06-28 15:01:37,075 main.py:51] epoch 10268, training loss: 8371.12, average training loss: 9350.36, base loss: 14413.35
[INFO 2017-06-28 15:01:38,084 main.py:51] epoch 10269, training loss: 9913.55, average training loss: 9351.37, base loss: 14413.76
[INFO 2017-06-28 15:01:39,158 main.py:51] epoch 10270, training loss: 10163.41, average training loss: 9351.34, base loss: 14415.23
[INFO 2017-06-28 15:01:40,249 main.py:51] epoch 10271, training loss: 9274.53, average training loss: 9349.10, base loss: 14412.73
[INFO 2017-06-28 15:01:41,343 main.py:51] epoch 10272, training loss: 9174.72, average training loss: 9348.09, base loss: 14411.51
[INFO 2017-06-28 15:01:42,451 main.py:51] epoch 10273, training loss: 10054.11, average training loss: 9349.54, base loss: 14412.90
[INFO 2017-06-28 15:01:43,454 main.py:51] epoch 10274, training loss: 10514.24, average training loss: 9349.62, base loss: 14414.93
[INFO 2017-06-28 15:01:44,489 main.py:51] epoch 10275, training loss: 10028.23, average training loss: 9351.79, base loss: 14419.27
[INFO 2017-06-28 15:01:45,530 main.py:51] epoch 10276, training loss: 10414.82, average training loss: 9354.02, base loss: 14421.99
[INFO 2017-06-28 15:01:46,596 main.py:51] epoch 10277, training loss: 8853.35, average training loss: 9354.00, base loss: 14421.07
[INFO 2017-06-28 15:01:47,674 main.py:51] epoch 10278, training loss: 9329.27, average training loss: 9355.02, base loss: 14422.83
[INFO 2017-06-28 15:01:48,717 main.py:51] epoch 10279, training loss: 9943.87, average training loss: 9356.25, base loss: 14425.54
[INFO 2017-06-28 15:01:49,735 main.py:51] epoch 10280, training loss: 8953.68, average training loss: 9355.52, base loss: 14422.45
[INFO 2017-06-28 15:01:50,816 main.py:51] epoch 10281, training loss: 9835.03, average training loss: 9355.03, base loss: 14420.81
[INFO 2017-06-28 15:01:51,895 main.py:51] epoch 10282, training loss: 9719.37, average training loss: 9355.77, base loss: 14420.14
[INFO 2017-06-28 15:01:52,999 main.py:51] epoch 10283, training loss: 9883.44, average training loss: 9355.31, base loss: 14417.32
[INFO 2017-06-28 15:01:54,004 main.py:51] epoch 10284, training loss: 8430.59, average training loss: 9355.65, base loss: 14417.42
[INFO 2017-06-28 15:01:55,074 main.py:51] epoch 10285, training loss: 8827.22, average training loss: 9353.88, base loss: 14413.96
[INFO 2017-06-28 15:01:56,152 main.py:51] epoch 10286, training loss: 9726.87, average training loss: 9355.08, base loss: 14413.93
[INFO 2017-06-28 15:01:57,259 main.py:51] epoch 10287, training loss: 8401.82, average training loss: 9354.25, base loss: 14411.74
[INFO 2017-06-28 15:01:58,353 main.py:51] epoch 10288, training loss: 9638.79, average training loss: 9352.72, base loss: 14409.56
[INFO 2017-06-28 15:01:59,399 main.py:51] epoch 10289, training loss: 8893.88, average training loss: 9352.42, base loss: 14409.94
[INFO 2017-06-28 15:02:00,451 main.py:51] epoch 10290, training loss: 10434.52, average training loss: 9353.79, base loss: 14411.96
[INFO 2017-06-28 15:02:01,506 main.py:51] epoch 10291, training loss: 8450.45, average training loss: 9352.27, base loss: 14409.29
[INFO 2017-06-28 15:02:02,571 main.py:51] epoch 10292, training loss: 10543.91, average training loss: 9353.13, base loss: 14410.63
[INFO 2017-06-28 15:02:03,628 main.py:51] epoch 10293, training loss: 7869.92, average training loss: 9351.29, base loss: 14407.52
[INFO 2017-06-28 15:02:04,644 main.py:51] epoch 10294, training loss: 9394.89, average training loss: 9351.73, base loss: 14406.94
[INFO 2017-06-28 15:02:05,725 main.py:51] epoch 10295, training loss: 9235.36, average training loss: 9350.72, base loss: 14406.37
[INFO 2017-06-28 15:02:06,825 main.py:51] epoch 10296, training loss: 9486.94, average training loss: 9351.05, base loss: 14407.03
[INFO 2017-06-28 15:02:07,864 main.py:51] epoch 10297, training loss: 10041.66, average training loss: 9352.33, base loss: 14409.69
[INFO 2017-06-28 15:02:08,903 main.py:51] epoch 10298, training loss: 9286.89, average training loss: 9351.80, base loss: 14407.60
[INFO 2017-06-28 15:02:09,966 main.py:51] epoch 10299, training loss: 11121.06, average training loss: 9353.84, base loss: 14412.49
[INFO 2017-06-28 15:02:09,966 main.py:53] epoch 10299, testing
[INFO 2017-06-28 15:02:13,770 main.py:105] average testing loss: 10121.20, base loss: 14659.34
[INFO 2017-06-28 15:02:13,770 main.py:106] improve_loss: 4538.15, improve_percent: 0.31
[INFO 2017-06-28 15:02:13,771 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:02:14,766 main.py:51] epoch 10300, training loss: 10689.83, average training loss: 9354.07, base loss: 14411.76
[INFO 2017-06-28 15:02:15,811 main.py:51] epoch 10301, training loss: 8746.30, average training loss: 9353.44, base loss: 14410.06
[INFO 2017-06-28 15:02:16,853 main.py:51] epoch 10302, training loss: 8600.73, average training loss: 9351.98, base loss: 14408.72
[INFO 2017-06-28 15:02:17,947 main.py:51] epoch 10303, training loss: 9571.10, average training loss: 9352.32, base loss: 14409.80
[INFO 2017-06-28 15:02:19,030 main.py:51] epoch 10304, training loss: 9553.53, average training loss: 9352.94, base loss: 14411.10
[INFO 2017-06-28 15:02:20,097 main.py:51] epoch 10305, training loss: 9010.26, average training loss: 9353.59, base loss: 14412.73
[INFO 2017-06-28 15:02:21,177 main.py:51] epoch 10306, training loss: 9096.91, average training loss: 9353.17, base loss: 14410.76
[INFO 2017-06-28 15:02:22,184 main.py:51] epoch 10307, training loss: 10451.61, average training loss: 9353.71, base loss: 14410.65
[INFO 2017-06-28 15:02:23,238 main.py:51] epoch 10308, training loss: 8629.94, average training loss: 9352.64, base loss: 14408.91
[INFO 2017-06-28 15:02:24,306 main.py:51] epoch 10309, training loss: 9788.61, average training loss: 9352.36, base loss: 14408.10
[INFO 2017-06-28 15:02:25,326 main.py:51] epoch 10310, training loss: 10486.40, average training loss: 9354.78, base loss: 14411.37
[INFO 2017-06-28 15:02:26,331 main.py:51] epoch 10311, training loss: 8784.24, average training loss: 9355.05, base loss: 14412.28
[INFO 2017-06-28 15:02:27,388 main.py:51] epoch 10312, training loss: 9790.31, average training loss: 9354.76, base loss: 14412.10
[INFO 2017-06-28 15:02:28,465 main.py:51] epoch 10313, training loss: 9106.88, average training loss: 9353.50, base loss: 14412.58
[INFO 2017-06-28 15:02:29,618 main.py:51] epoch 10314, training loss: 9799.34, average training loss: 9354.49, base loss: 14414.15
[INFO 2017-06-28 15:02:30,698 main.py:51] epoch 10315, training loss: 8976.12, average training loss: 9354.13, base loss: 14413.62
[INFO 2017-06-28 15:02:31,798 main.py:51] epoch 10316, training loss: 8426.47, average training loss: 9352.80, base loss: 14410.71
[INFO 2017-06-28 15:02:32,849 main.py:51] epoch 10317, training loss: 9029.47, average training loss: 9352.57, base loss: 14410.75
[INFO 2017-06-28 15:02:33,922 main.py:51] epoch 10318, training loss: 8174.16, average training loss: 9351.69, base loss: 14409.57
[INFO 2017-06-28 15:02:34,851 main.py:51] epoch 10319, training loss: 8333.70, average training loss: 9349.43, base loss: 14408.14
[INFO 2017-06-28 15:02:35,806 main.py:51] epoch 10320, training loss: 9357.09, average training loss: 9349.99, base loss: 14408.27
[INFO 2017-06-28 15:02:36,756 main.py:51] epoch 10321, training loss: 9443.15, average training loss: 9348.99, base loss: 14406.43
[INFO 2017-06-28 15:02:37,846 main.py:51] epoch 10322, training loss: 7967.29, average training loss: 9347.95, base loss: 14403.22
[INFO 2017-06-28 15:02:38,894 main.py:51] epoch 10323, training loss: 9643.12, average training loss: 9347.39, base loss: 14401.79
[INFO 2017-06-28 15:02:39,942 main.py:51] epoch 10324, training loss: 8588.13, average training loss: 9346.87, base loss: 14401.34
[INFO 2017-06-28 15:02:40,975 main.py:51] epoch 10325, training loss: 11129.12, average training loss: 9348.10, base loss: 14403.24
[INFO 2017-06-28 15:02:42,033 main.py:51] epoch 10326, training loss: 9324.36, average training loss: 9348.91, base loss: 14405.83
[INFO 2017-06-28 15:02:43,118 main.py:51] epoch 10327, training loss: 8690.26, average training loss: 9348.81, base loss: 14405.71
[INFO 2017-06-28 15:02:44,158 main.py:51] epoch 10328, training loss: 9509.59, average training loss: 9350.22, base loss: 14408.82
[INFO 2017-06-28 15:02:45,208 main.py:51] epoch 10329, training loss: 9430.09, average training loss: 9350.35, base loss: 14408.90
[INFO 2017-06-28 15:02:46,283 main.py:51] epoch 10330, training loss: 9015.04, average training loss: 9349.85, base loss: 14407.40
[INFO 2017-06-28 15:02:47,354 main.py:51] epoch 10331, training loss: 10141.97, average training loss: 9351.06, base loss: 14408.81
[INFO 2017-06-28 15:02:48,460 main.py:51] epoch 10332, training loss: 9336.98, average training loss: 9350.10, base loss: 14407.47
[INFO 2017-06-28 15:02:49,495 main.py:51] epoch 10333, training loss: 9825.06, average training loss: 9350.34, base loss: 14408.52
[INFO 2017-06-28 15:02:50,544 main.py:51] epoch 10334, training loss: 9138.77, average training loss: 9351.49, base loss: 14411.25
[INFO 2017-06-28 15:02:51,599 main.py:51] epoch 10335, training loss: 8645.12, average training loss: 9351.36, base loss: 14410.42
[INFO 2017-06-28 15:02:52,607 main.py:51] epoch 10336, training loss: 9144.92, average training loss: 9350.98, base loss: 14408.74
[INFO 2017-06-28 15:02:53,622 main.py:51] epoch 10337, training loss: 9027.24, average training loss: 9350.63, base loss: 14408.23
[INFO 2017-06-28 15:02:54,700 main.py:51] epoch 10338, training loss: 9291.10, average training loss: 9350.50, base loss: 14408.89
[INFO 2017-06-28 15:02:55,772 main.py:51] epoch 10339, training loss: 9662.10, average training loss: 9351.28, base loss: 14409.65
[INFO 2017-06-28 15:02:56,822 main.py:51] epoch 10340, training loss: 8225.69, average training loss: 9350.33, base loss: 14408.62
[INFO 2017-06-28 15:02:57,881 main.py:51] epoch 10341, training loss: 9233.24, average training loss: 9350.80, base loss: 14407.85
[INFO 2017-06-28 15:02:58,976 main.py:51] epoch 10342, training loss: 11133.13, average training loss: 9352.97, base loss: 14412.19
[INFO 2017-06-28 15:02:59,961 main.py:51] epoch 10343, training loss: 9030.93, average training loss: 9350.75, base loss: 14409.26
[INFO 2017-06-28 15:03:01,001 main.py:51] epoch 10344, training loss: 9653.51, average training loss: 9351.43, base loss: 14410.10
[INFO 2017-06-28 15:03:02,074 main.py:51] epoch 10345, training loss: 9347.06, average training loss: 9351.31, base loss: 14409.00
[INFO 2017-06-28 15:03:03,096 main.py:51] epoch 10346, training loss: 9003.87, average training loss: 9351.62, base loss: 14410.36
[INFO 2017-06-28 15:03:04,171 main.py:51] epoch 10347, training loss: 9820.23, average training loss: 9351.59, base loss: 14410.62
[INFO 2017-06-28 15:03:05,242 main.py:51] epoch 10348, training loss: 9172.04, average training loss: 9351.67, base loss: 14410.79
[INFO 2017-06-28 15:03:06,338 main.py:51] epoch 10349, training loss: 9234.37, average training loss: 9352.46, base loss: 14413.51
[INFO 2017-06-28 15:03:07,440 main.py:51] epoch 10350, training loss: 9521.18, average training loss: 9352.42, base loss: 14414.67
[INFO 2017-06-28 15:03:08,440 main.py:51] epoch 10351, training loss: 10250.11, average training loss: 9353.89, base loss: 14417.73
[INFO 2017-06-28 15:03:09,519 main.py:51] epoch 10352, training loss: 9078.51, average training loss: 9353.23, base loss: 14418.61
[INFO 2017-06-28 15:03:10,598 main.py:51] epoch 10353, training loss: 9068.62, average training loss: 9352.29, base loss: 14419.18
[INFO 2017-06-28 15:03:11,604 main.py:51] epoch 10354, training loss: 10019.57, average training loss: 9352.94, base loss: 14418.19
[INFO 2017-06-28 15:03:12,668 main.py:51] epoch 10355, training loss: 8237.00, average training loss: 9352.24, base loss: 14416.75
[INFO 2017-06-28 15:03:13,767 main.py:51] epoch 10356, training loss: 9962.62, average training loss: 9353.30, base loss: 14419.64
[INFO 2017-06-28 15:03:14,782 main.py:51] epoch 10357, training loss: 9340.63, average training loss: 9351.77, base loss: 14416.17
[INFO 2017-06-28 15:03:15,823 main.py:51] epoch 10358, training loss: 11600.93, average training loss: 9353.22, base loss: 14420.13
[INFO 2017-06-28 15:03:16,857 main.py:51] epoch 10359, training loss: 8900.81, average training loss: 9352.46, base loss: 14419.78
[INFO 2017-06-28 15:03:17,932 main.py:51] epoch 10360, training loss: 9887.46, average training loss: 9353.66, base loss: 14420.75
[INFO 2017-06-28 15:03:18,995 main.py:51] epoch 10361, training loss: 7925.38, average training loss: 9353.34, base loss: 14421.79
[INFO 2017-06-28 15:03:19,994 main.py:51] epoch 10362, training loss: 9398.87, average training loss: 9353.61, base loss: 14421.34
[INFO 2017-06-28 15:03:21,030 main.py:51] epoch 10363, training loss: 9928.32, average training loss: 9355.05, base loss: 14424.56
[INFO 2017-06-28 15:03:22,113 main.py:51] epoch 10364, training loss: 8358.95, average training loss: 9354.20, base loss: 14424.65
[INFO 2017-06-28 15:03:23,125 main.py:51] epoch 10365, training loss: 9654.36, average training loss: 9354.17, base loss: 14424.66
[INFO 2017-06-28 15:03:24,172 main.py:51] epoch 10366, training loss: 9841.88, average training loss: 9353.33, base loss: 14422.98
[INFO 2017-06-28 15:03:25,240 main.py:51] epoch 10367, training loss: 8931.88, average training loss: 9353.35, base loss: 14422.59
[INFO 2017-06-28 15:03:26,320 main.py:51] epoch 10368, training loss: 9590.04, average training loss: 9353.31, base loss: 14422.11
[INFO 2017-06-28 15:03:27,365 main.py:51] epoch 10369, training loss: 9779.93, average training loss: 9352.72, base loss: 14418.92
[INFO 2017-06-28 15:03:28,395 main.py:51] epoch 10370, training loss: 10346.73, average training loss: 9353.88, base loss: 14421.93
[INFO 2017-06-28 15:03:29,450 main.py:51] epoch 10371, training loss: 8417.41, average training loss: 9353.78, base loss: 14422.98
[INFO 2017-06-28 15:03:30,549 main.py:51] epoch 10372, training loss: 9291.21, average training loss: 9354.66, base loss: 14425.64
[INFO 2017-06-28 15:03:31,555 main.py:51] epoch 10373, training loss: 9469.56, average training loss: 9354.78, base loss: 14426.22
[INFO 2017-06-28 15:03:32,622 main.py:51] epoch 10374, training loss: 9331.44, average training loss: 9354.51, base loss: 14426.83
[INFO 2017-06-28 15:03:33,697 main.py:51] epoch 10375, training loss: 9533.10, average training loss: 9354.20, base loss: 14427.47
[INFO 2017-06-28 15:03:34,807 main.py:51] epoch 10376, training loss: 8044.57, average training loss: 9353.07, base loss: 14424.66
[INFO 2017-06-28 15:03:35,919 main.py:51] epoch 10377, training loss: 9986.67, average training loss: 9354.63, base loss: 14427.49
[INFO 2017-06-28 15:03:36,949 main.py:51] epoch 10378, training loss: 9922.53, average training loss: 9355.35, base loss: 14429.60
[INFO 2017-06-28 15:03:38,043 main.py:51] epoch 10379, training loss: 8420.19, average training loss: 9353.78, base loss: 14427.02
[INFO 2017-06-28 15:03:39,110 main.py:51] epoch 10380, training loss: 8720.92, average training loss: 9351.66, base loss: 14423.67
[INFO 2017-06-28 15:03:40,166 main.py:51] epoch 10381, training loss: 9842.17, average training loss: 9351.06, base loss: 14423.02
[INFO 2017-06-28 15:03:41,206 main.py:51] epoch 10382, training loss: 8548.27, average training loss: 9350.38, base loss: 14421.58
[INFO 2017-06-28 15:03:42,302 main.py:51] epoch 10383, training loss: 7728.10, average training loss: 9349.13, base loss: 14418.66
[INFO 2017-06-28 15:03:43,409 main.py:51] epoch 10384, training loss: 8623.57, average training loss: 9348.51, base loss: 14417.82
[INFO 2017-06-28 15:03:44,455 main.py:51] epoch 10385, training loss: 9957.09, average training loss: 9349.47, base loss: 14420.55
[INFO 2017-06-28 15:03:45,540 main.py:51] epoch 10386, training loss: 8761.37, average training loss: 9349.54, base loss: 14420.52
[INFO 2017-06-28 15:03:46,587 main.py:51] epoch 10387, training loss: 8682.64, average training loss: 9349.22, base loss: 14420.18
[INFO 2017-06-28 15:03:47,645 main.py:51] epoch 10388, training loss: 9685.44, average training loss: 9349.25, base loss: 14421.25
[INFO 2017-06-28 15:03:48,666 main.py:51] epoch 10389, training loss: 8504.72, average training loss: 9348.46, base loss: 14418.51
[INFO 2017-06-28 15:03:49,738 main.py:51] epoch 10390, training loss: 9277.91, average training loss: 9346.80, base loss: 14417.38
[INFO 2017-06-28 15:03:50,831 main.py:51] epoch 10391, training loss: 8163.52, average training loss: 9345.96, base loss: 14416.75
[INFO 2017-06-28 15:03:51,869 main.py:51] epoch 10392, training loss: 9618.37, average training loss: 9347.51, base loss: 14419.11
[INFO 2017-06-28 15:03:52,907 main.py:51] epoch 10393, training loss: 8750.58, average training loss: 9347.19, base loss: 14420.69
[INFO 2017-06-28 15:03:53,862 main.py:51] epoch 10394, training loss: 10121.10, average training loss: 9347.14, base loss: 14420.35
[INFO 2017-06-28 15:03:54,843 main.py:51] epoch 10395, training loss: 9113.89, average training loss: 9347.73, base loss: 14422.22
[INFO 2017-06-28 15:03:55,764 main.py:51] epoch 10396, training loss: 7959.86, average training loss: 9347.15, base loss: 14421.14
[INFO 2017-06-28 15:03:56,821 main.py:51] epoch 10397, training loss: 9257.35, average training loss: 9345.83, base loss: 14418.76
[INFO 2017-06-28 15:03:57,886 main.py:51] epoch 10398, training loss: 9703.33, average training loss: 9346.97, base loss: 14421.66
[INFO 2017-06-28 15:03:58,943 main.py:51] epoch 10399, training loss: 9866.22, average training loss: 9348.03, base loss: 14422.75
[INFO 2017-06-28 15:03:58,943 main.py:53] epoch 10399, testing
[INFO 2017-06-28 15:04:02,636 main.py:105] average testing loss: 10858.79, base loss: 15557.36
[INFO 2017-06-28 15:04:02,637 main.py:106] improve_loss: 4698.58, improve_percent: 0.30
[INFO 2017-06-28 15:04:02,637 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:04:03,639 main.py:51] epoch 10400, training loss: 9602.00, average training loss: 9348.66, base loss: 14422.50
[INFO 2017-06-28 15:04:04,726 main.py:51] epoch 10401, training loss: 8691.86, average training loss: 9349.30, base loss: 14423.37
[INFO 2017-06-28 15:04:05,834 main.py:51] epoch 10402, training loss: 9253.46, average training loss: 9349.40, base loss: 14423.85
[INFO 2017-06-28 15:04:06,928 main.py:51] epoch 10403, training loss: 8965.72, average training loss: 9350.05, base loss: 14425.75
[INFO 2017-06-28 15:04:08,064 main.py:51] epoch 10404, training loss: 9349.91, average training loss: 9349.00, base loss: 14423.78
[INFO 2017-06-28 15:04:09,081 main.py:51] epoch 10405, training loss: 9361.11, average training loss: 9348.65, base loss: 14422.80
[INFO 2017-06-28 15:04:10,170 main.py:51] epoch 10406, training loss: 9526.99, average training loss: 9348.56, base loss: 14422.68
[INFO 2017-06-28 15:04:11,251 main.py:51] epoch 10407, training loss: 9867.80, average training loss: 9349.86, base loss: 14423.69
[INFO 2017-06-28 15:04:12,367 main.py:51] epoch 10408, training loss: 9363.10, average training loss: 9351.23, base loss: 14426.52
[INFO 2017-06-28 15:04:13,375 main.py:51] epoch 10409, training loss: 7870.75, average training loss: 9348.80, base loss: 14421.58
[INFO 2017-06-28 15:04:14,451 main.py:51] epoch 10410, training loss: 10336.10, average training loss: 9350.80, base loss: 14425.87
[INFO 2017-06-28 15:04:15,512 main.py:51] epoch 10411, training loss: 9462.87, average training loss: 9351.00, base loss: 14427.28
[INFO 2017-06-28 15:04:16,586 main.py:51] epoch 10412, training loss: 10191.79, average training loss: 9352.40, base loss: 14429.31
[INFO 2017-06-28 15:04:17,687 main.py:51] epoch 10413, training loss: 8928.56, average training loss: 9352.11, base loss: 14430.59
[INFO 2017-06-28 15:04:18,732 main.py:51] epoch 10414, training loss: 8670.75, average training loss: 9349.47, base loss: 14427.29
[INFO 2017-06-28 15:04:19,834 main.py:51] epoch 10415, training loss: 9266.44, average training loss: 9349.15, base loss: 14426.79
[INFO 2017-06-28 15:04:20,854 main.py:51] epoch 10416, training loss: 9398.59, average training loss: 9347.80, base loss: 14424.22
[INFO 2017-06-28 15:04:21,927 main.py:51] epoch 10417, training loss: 8446.57, average training loss: 9346.18, base loss: 14421.92
[INFO 2017-06-28 15:04:23,032 main.py:51] epoch 10418, training loss: 9291.12, average training loss: 9345.44, base loss: 14421.76
[INFO 2017-06-28 15:04:24,130 main.py:51] epoch 10419, training loss: 9688.24, average training loss: 9345.91, base loss: 14424.52
[INFO 2017-06-28 15:04:25,270 main.py:51] epoch 10420, training loss: 9155.90, average training loss: 9346.37, base loss: 14425.10
[INFO 2017-06-28 15:04:26,326 main.py:51] epoch 10421, training loss: 9021.33, average training loss: 9346.02, base loss: 14424.52
[INFO 2017-06-28 15:04:27,393 main.py:51] epoch 10422, training loss: 9049.59, average training loss: 9346.86, base loss: 14423.90
[INFO 2017-06-28 15:04:28,441 main.py:51] epoch 10423, training loss: 8938.70, average training loss: 9345.69, base loss: 14422.08
[INFO 2017-06-28 15:04:29,513 main.py:51] epoch 10424, training loss: 9851.82, average training loss: 9345.82, base loss: 14422.59
[INFO 2017-06-28 15:04:30,621 main.py:51] epoch 10425, training loss: 8818.52, average training loss: 9346.25, base loss: 14423.70
[INFO 2017-06-28 15:04:31,710 main.py:51] epoch 10426, training loss: 10030.33, average training loss: 9345.76, base loss: 14422.45
[INFO 2017-06-28 15:04:32,830 main.py:51] epoch 10427, training loss: 8165.07, average training loss: 9345.02, base loss: 14420.06
[INFO 2017-06-28 15:04:33,888 main.py:51] epoch 10428, training loss: 9784.85, average training loss: 9343.81, base loss: 14418.96
[INFO 2017-06-28 15:04:34,973 main.py:51] epoch 10429, training loss: 11026.38, average training loss: 9346.16, base loss: 14422.75
[INFO 2017-06-28 15:04:36,006 main.py:51] epoch 10430, training loss: 9625.31, average training loss: 9344.29, base loss: 14421.11
[INFO 2017-06-28 15:04:37,078 main.py:51] epoch 10431, training loss: 10744.29, average training loss: 9347.13, base loss: 14424.66
[INFO 2017-06-28 15:04:38,173 main.py:51] epoch 10432, training loss: 8797.81, average training loss: 9344.58, base loss: 14418.67
[INFO 2017-06-28 15:04:39,185 main.py:51] epoch 10433, training loss: 8962.65, average training loss: 9343.23, base loss: 14418.89
[INFO 2017-06-28 15:04:40,259 main.py:51] epoch 10434, training loss: 8723.39, average training loss: 9341.12, base loss: 14413.50
[INFO 2017-06-28 15:04:41,320 main.py:51] epoch 10435, training loss: 9717.50, average training loss: 9341.69, base loss: 14413.27
[INFO 2017-06-28 15:04:42,320 main.py:51] epoch 10436, training loss: 8328.84, average training loss: 9340.36, base loss: 14411.24
[INFO 2017-06-28 15:04:43,353 main.py:51] epoch 10437, training loss: 9338.25, average training loss: 9340.97, base loss: 14410.98
[INFO 2017-06-28 15:04:44,420 main.py:51] epoch 10438, training loss: 9620.15, average training loss: 9340.44, base loss: 14411.24
[INFO 2017-06-28 15:04:45,455 main.py:51] epoch 10439, training loss: 7884.21, average training loss: 9339.35, base loss: 14408.90
[INFO 2017-06-28 15:04:46,521 main.py:51] epoch 10440, training loss: 9350.87, average training loss: 9339.30, base loss: 14407.37
[INFO 2017-06-28 15:04:47,613 main.py:51] epoch 10441, training loss: 11334.04, average training loss: 9341.83, base loss: 14410.85
[INFO 2017-06-28 15:04:48,641 main.py:51] epoch 10442, training loss: 8768.76, average training loss: 9341.40, base loss: 14412.12
[INFO 2017-06-28 15:04:49,689 main.py:51] epoch 10443, training loss: 9117.51, average training loss: 9341.64, base loss: 14412.05
[INFO 2017-06-28 15:04:50,710 main.py:51] epoch 10444, training loss: 8049.42, average training loss: 9339.12, base loss: 14406.03
[INFO 2017-06-28 15:04:51,801 main.py:51] epoch 10445, training loss: 8994.52, average training loss: 9339.33, base loss: 14405.09
[INFO 2017-06-28 15:04:52,884 main.py:51] epoch 10446, training loss: 8645.58, average training loss: 9338.98, base loss: 14403.68
[INFO 2017-06-28 15:04:53,891 main.py:51] epoch 10447, training loss: 8025.53, average training loss: 9337.62, base loss: 14400.81
[INFO 2017-06-28 15:04:54,965 main.py:51] epoch 10448, training loss: 8692.13, average training loss: 9337.29, base loss: 14400.92
[INFO 2017-06-28 15:04:56,077 main.py:51] epoch 10449, training loss: 8506.78, average training loss: 9337.23, base loss: 14402.87
[INFO 2017-06-28 15:04:57,118 main.py:51] epoch 10450, training loss: 9503.90, average training loss: 9336.44, base loss: 14402.84
[INFO 2017-06-28 15:04:58,166 main.py:51] epoch 10451, training loss: 8998.66, average training loss: 9336.68, base loss: 14403.35
[INFO 2017-06-28 15:04:59,190 main.py:51] epoch 10452, training loss: 9018.72, average training loss: 9337.37, base loss: 14404.75
[INFO 2017-06-28 15:05:00,274 main.py:51] epoch 10453, training loss: 8434.74, average training loss: 9336.14, base loss: 14403.82
[INFO 2017-06-28 15:05:01,357 main.py:51] epoch 10454, training loss: 9263.40, average training loss: 9335.49, base loss: 14403.38
[INFO 2017-06-28 15:05:02,388 main.py:51] epoch 10455, training loss: 9816.30, average training loss: 9336.32, base loss: 14405.78
[INFO 2017-06-28 15:05:03,422 main.py:51] epoch 10456, training loss: 9982.41, average training loss: 9337.34, base loss: 14406.97
[INFO 2017-06-28 15:05:04,538 main.py:51] epoch 10457, training loss: 9594.44, average training loss: 9337.68, base loss: 14407.73
[INFO 2017-06-28 15:05:05,612 main.py:51] epoch 10458, training loss: 8962.72, average training loss: 9337.98, base loss: 14408.44
[INFO 2017-06-28 15:05:06,658 main.py:51] epoch 10459, training loss: 9406.41, average training loss: 9338.30, base loss: 14409.75
[INFO 2017-06-28 15:05:07,699 main.py:51] epoch 10460, training loss: 8730.17, average training loss: 9338.26, base loss: 14409.82
[INFO 2017-06-28 15:05:08,781 main.py:51] epoch 10461, training loss: 8378.61, average training loss: 9336.44, base loss: 14407.19
[INFO 2017-06-28 15:05:09,814 main.py:51] epoch 10462, training loss: 9473.05, average training loss: 9336.56, base loss: 14407.98
[INFO 2017-06-28 15:05:10,823 main.py:51] epoch 10463, training loss: 8262.53, average training loss: 9334.64, base loss: 14405.19
[INFO 2017-06-28 15:05:11,779 main.py:51] epoch 10464, training loss: 9029.37, average training loss: 9334.88, base loss: 14406.61
[INFO 2017-06-28 15:05:12,742 main.py:51] epoch 10465, training loss: 9157.96, average training loss: 9335.73, base loss: 14408.76
[INFO 2017-06-28 15:05:13,691 main.py:51] epoch 10466, training loss: 8307.84, average training loss: 9334.27, base loss: 14406.60
[INFO 2017-06-28 15:05:14,718 main.py:51] epoch 10467, training loss: 10390.05, average training loss: 9336.34, base loss: 14409.10
[INFO 2017-06-28 15:05:15,787 main.py:51] epoch 10468, training loss: 9369.22, average training loss: 9336.95, base loss: 14410.75
[INFO 2017-06-28 15:05:16,757 main.py:51] epoch 10469, training loss: 9624.36, average training loss: 9336.87, base loss: 14410.02
[INFO 2017-06-28 15:05:17,784 main.py:51] epoch 10470, training loss: 9687.38, average training loss: 9337.38, base loss: 14410.26
[INFO 2017-06-28 15:05:18,879 main.py:51] epoch 10471, training loss: 9255.98, average training loss: 9336.80, base loss: 14408.68
[INFO 2017-06-28 15:05:19,966 main.py:51] epoch 10472, training loss: 7834.88, average training loss: 9335.00, base loss: 14404.80
[INFO 2017-06-28 15:05:20,996 main.py:51] epoch 10473, training loss: 9438.80, average training loss: 9335.01, base loss: 14405.04
[INFO 2017-06-28 15:05:22,015 main.py:51] epoch 10474, training loss: 10200.61, average training loss: 9336.23, base loss: 14406.78
[INFO 2017-06-28 15:05:23,079 main.py:51] epoch 10475, training loss: 10300.41, average training loss: 9338.43, base loss: 14410.37
[INFO 2017-06-28 15:05:24,165 main.py:51] epoch 10476, training loss: 9646.41, average training loss: 9339.09, base loss: 14410.93
[INFO 2017-06-28 15:05:25,171 main.py:51] epoch 10477, training loss: 9408.68, average training loss: 9338.64, base loss: 14410.15
[INFO 2017-06-28 15:05:26,238 main.py:51] epoch 10478, training loss: 9909.80, average training loss: 9339.94, base loss: 14411.70
[INFO 2017-06-28 15:05:27,315 main.py:51] epoch 10479, training loss: 9743.80, average training loss: 9339.66, base loss: 14411.57
[INFO 2017-06-28 15:05:28,342 main.py:51] epoch 10480, training loss: 8403.05, average training loss: 9339.66, base loss: 14409.66
[INFO 2017-06-28 15:05:29,407 main.py:51] epoch 10481, training loss: 8914.08, average training loss: 9337.09, base loss: 14405.12
[INFO 2017-06-28 15:05:30,450 main.py:51] epoch 10482, training loss: 8917.53, average training loss: 9334.05, base loss: 14400.06
[INFO 2017-06-28 15:05:31,522 main.py:51] epoch 10483, training loss: 9603.04, average training loss: 9333.86, base loss: 14399.42
[INFO 2017-06-28 15:05:32,607 main.py:51] epoch 10484, training loss: 10313.84, average training loss: 9333.86, base loss: 14400.33
[INFO 2017-06-28 15:05:33,677 main.py:51] epoch 10485, training loss: 9486.69, average training loss: 9335.19, base loss: 14401.30
[INFO 2017-06-28 15:05:34,746 main.py:51] epoch 10486, training loss: 8349.74, average training loss: 9333.28, base loss: 14397.66
[INFO 2017-06-28 15:05:35,838 main.py:51] epoch 10487, training loss: 8909.11, average training loss: 9334.22, base loss: 14399.65
[INFO 2017-06-28 15:05:36,912 main.py:51] epoch 10488, training loss: 9015.11, average training loss: 9334.11, base loss: 14398.27
[INFO 2017-06-28 15:05:37,929 main.py:51] epoch 10489, training loss: 10065.76, average training loss: 9334.73, base loss: 14397.81
[INFO 2017-06-28 15:05:38,978 main.py:51] epoch 10490, training loss: 10017.88, average training loss: 9335.65, base loss: 14399.71
[INFO 2017-06-28 15:05:40,008 main.py:51] epoch 10491, training loss: 9877.80, average training loss: 9335.73, base loss: 14398.35
[INFO 2017-06-28 15:05:41,097 main.py:51] epoch 10492, training loss: 10079.65, average training loss: 9336.77, base loss: 14398.80
[INFO 2017-06-28 15:05:42,192 main.py:51] epoch 10493, training loss: 9336.35, average training loss: 9336.21, base loss: 14399.45
[INFO 2017-06-28 15:05:43,263 main.py:51] epoch 10494, training loss: 8312.94, average training loss: 9335.42, base loss: 14398.36
[INFO 2017-06-28 15:05:44,363 main.py:51] epoch 10495, training loss: 9397.53, average training loss: 9335.88, base loss: 14399.89
[INFO 2017-06-28 15:05:45,378 main.py:51] epoch 10496, training loss: 8255.89, average training loss: 9335.30, base loss: 14398.65
[INFO 2017-06-28 15:05:46,422 main.py:51] epoch 10497, training loss: 9598.00, average training loss: 9334.66, base loss: 14398.22
[INFO 2017-06-28 15:05:47,492 main.py:51] epoch 10498, training loss: 9063.07, average training loss: 9334.73, base loss: 14398.83
[INFO 2017-06-28 15:05:48,601 main.py:51] epoch 10499, training loss: 9441.77, average training loss: 9334.51, base loss: 14397.71
[INFO 2017-06-28 15:05:48,601 main.py:53] epoch 10499, testing
[INFO 2017-06-28 15:05:52,362 main.py:105] average testing loss: 10749.62, base loss: 15586.83
[INFO 2017-06-28 15:05:52,362 main.py:106] improve_loss: 4837.21, improve_percent: 0.31
[INFO 2017-06-28 15:05:52,363 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:05:53,390 main.py:51] epoch 10500, training loss: 8720.92, average training loss: 9334.82, base loss: 14397.55
[INFO 2017-06-28 15:05:54,472 main.py:51] epoch 10501, training loss: 9888.65, average training loss: 9334.70, base loss: 14397.62
[INFO 2017-06-28 15:05:55,542 main.py:51] epoch 10502, training loss: 8329.11, average training loss: 9334.15, base loss: 14396.15
[INFO 2017-06-28 15:05:56,625 main.py:51] epoch 10503, training loss: 8612.27, average training loss: 9333.75, base loss: 14394.48
[INFO 2017-06-28 15:05:57,664 main.py:51] epoch 10504, training loss: 9337.18, average training loss: 9334.07, base loss: 14394.66
[INFO 2017-06-28 15:05:58,710 main.py:51] epoch 10505, training loss: 8773.91, average training loss: 9333.80, base loss: 14396.02
[INFO 2017-06-28 15:05:59,748 main.py:51] epoch 10506, training loss: 10171.87, average training loss: 9335.05, base loss: 14397.03
[INFO 2017-06-28 15:06:00,833 main.py:51] epoch 10507, training loss: 10192.02, average training loss: 9335.64, base loss: 14398.86
[INFO 2017-06-28 15:06:01,949 main.py:51] epoch 10508, training loss: 9017.62, average training loss: 9335.57, base loss: 14399.04
[INFO 2017-06-28 15:06:02,989 main.py:51] epoch 10509, training loss: 9927.87, average training loss: 9336.08, base loss: 14400.01
[INFO 2017-06-28 15:06:04,045 main.py:51] epoch 10510, training loss: 9172.27, average training loss: 9336.91, base loss: 14400.35
[INFO 2017-06-28 15:06:05,053 main.py:51] epoch 10511, training loss: 8735.17, average training loss: 9334.13, base loss: 14396.24
[INFO 2017-06-28 15:06:06,121 main.py:51] epoch 10512, training loss: 9830.39, average training loss: 9334.74, base loss: 14397.62
[INFO 2017-06-28 15:06:07,259 main.py:51] epoch 10513, training loss: 9079.38, average training loss: 9335.05, base loss: 14398.31
[INFO 2017-06-28 15:06:08,358 main.py:51] epoch 10514, training loss: 9707.69, average training loss: 9335.75, base loss: 14400.40
[INFO 2017-06-28 15:06:09,461 main.py:51] epoch 10515, training loss: 9240.89, average training loss: 9335.13, base loss: 14398.47
[INFO 2017-06-28 15:06:10,495 main.py:51] epoch 10516, training loss: 9242.10, average training loss: 9336.10, base loss: 14400.50
[INFO 2017-06-28 15:06:11,532 main.py:51] epoch 10517, training loss: 8601.57, average training loss: 9334.70, base loss: 14396.87
[INFO 2017-06-28 15:06:12,595 main.py:51] epoch 10518, training loss: 8510.33, average training loss: 9334.96, base loss: 14397.01
[INFO 2017-06-28 15:06:13,643 main.py:51] epoch 10519, training loss: 8841.81, average training loss: 9333.68, base loss: 14395.19
[INFO 2017-06-28 15:06:14,734 main.py:51] epoch 10520, training loss: 8395.59, average training loss: 9332.56, base loss: 14394.42
[INFO 2017-06-28 15:06:15,728 main.py:51] epoch 10521, training loss: 8685.14, average training loss: 9330.48, base loss: 14392.14
[INFO 2017-06-28 15:06:16,814 main.py:51] epoch 10522, training loss: 7947.59, average training loss: 9329.52, base loss: 14391.27
[INFO 2017-06-28 15:06:17,902 main.py:51] epoch 10523, training loss: 8078.96, average training loss: 9327.87, base loss: 14389.98
[INFO 2017-06-28 15:06:18,960 main.py:51] epoch 10524, training loss: 8913.92, average training loss: 9326.79, base loss: 14388.56
[INFO 2017-06-28 15:06:20,087 main.py:51] epoch 10525, training loss: 9583.46, average training loss: 9327.48, base loss: 14391.48
[INFO 2017-06-28 15:06:21,094 main.py:51] epoch 10526, training loss: 9487.45, average training loss: 9328.71, base loss: 14392.79
[INFO 2017-06-28 15:06:22,160 main.py:51] epoch 10527, training loss: 9928.67, average training loss: 9330.28, base loss: 14395.43
[INFO 2017-06-28 15:06:23,258 main.py:51] epoch 10528, training loss: 9469.95, average training loss: 9330.53, base loss: 14395.12
[INFO 2017-06-28 15:06:24,302 main.py:51] epoch 10529, training loss: 9005.45, average training loss: 9330.23, base loss: 14396.14
[INFO 2017-06-28 15:06:25,371 main.py:51] epoch 10530, training loss: 8768.37, average training loss: 9329.75, base loss: 14395.77
[INFO 2017-06-28 15:06:26,402 main.py:51] epoch 10531, training loss: 8285.32, average training loss: 9329.18, base loss: 14395.04
[INFO 2017-06-28 15:06:27,458 main.py:51] epoch 10532, training loss: 8705.95, average training loss: 9329.59, base loss: 14395.45
[INFO 2017-06-28 15:06:28,514 main.py:51] epoch 10533, training loss: 11007.04, average training loss: 9331.77, base loss: 14399.38
[INFO 2017-06-28 15:06:29,470 main.py:51] epoch 10534, training loss: 11830.67, average training loss: 9334.92, base loss: 14403.54
[INFO 2017-06-28 15:06:30,384 main.py:51] epoch 10535, training loss: 9606.37, average training loss: 9336.29, base loss: 14406.99
[INFO 2017-06-28 15:06:31,376 main.py:51] epoch 10536, training loss: 8847.77, average training loss: 9334.62, base loss: 14402.92
[INFO 2017-06-28 15:06:32,418 main.py:51] epoch 10537, training loss: 9841.31, average training loss: 9335.04, base loss: 14403.79
[INFO 2017-06-28 15:06:33,484 main.py:51] epoch 10538, training loss: 9769.91, average training loss: 9334.51, base loss: 14401.98
[INFO 2017-06-28 15:06:34,577 main.py:51] epoch 10539, training loss: 9729.11, average training loss: 9335.97, base loss: 14404.47
[INFO 2017-06-28 15:06:35,643 main.py:51] epoch 10540, training loss: 8872.65, average training loss: 9334.53, base loss: 14401.75
[INFO 2017-06-28 15:06:36,671 main.py:51] epoch 10541, training loss: 9243.63, average training loss: 9335.00, base loss: 14401.55
[INFO 2017-06-28 15:06:37,730 main.py:51] epoch 10542, training loss: 9398.11, average training loss: 9334.05, base loss: 14400.71
[INFO 2017-06-28 15:06:38,785 main.py:51] epoch 10543, training loss: 8626.29, average training loss: 9332.90, base loss: 14398.70
[INFO 2017-06-28 15:06:39,821 main.py:51] epoch 10544, training loss: 9092.56, average training loss: 9332.34, base loss: 14397.91
[INFO 2017-06-28 15:06:40,901 main.py:51] epoch 10545, training loss: 8896.43, average training loss: 9331.55, base loss: 14397.79
[INFO 2017-06-28 15:06:41,999 main.py:51] epoch 10546, training loss: 8646.05, average training loss: 9331.37, base loss: 14397.72
[INFO 2017-06-28 15:06:43,079 main.py:51] epoch 10547, training loss: 10060.43, average training loss: 9331.40, base loss: 14399.90
[INFO 2017-06-28 15:06:44,184 main.py:51] epoch 10548, training loss: 8533.89, average training loss: 9330.68, base loss: 14398.66
[INFO 2017-06-28 15:06:45,214 main.py:51] epoch 10549, training loss: 8843.28, average training loss: 9330.77, base loss: 14397.91
[INFO 2017-06-28 15:06:46,250 main.py:51] epoch 10550, training loss: 8867.95, average training loss: 9330.54, base loss: 14396.55
[INFO 2017-06-28 15:06:47,313 main.py:51] epoch 10551, training loss: 8722.25, average training loss: 9328.82, base loss: 14393.07
[INFO 2017-06-28 15:06:48,381 main.py:51] epoch 10552, training loss: 11033.89, average training loss: 9330.44, base loss: 14395.43
[INFO 2017-06-28 15:06:49,459 main.py:51] epoch 10553, training loss: 10687.15, average training loss: 9330.57, base loss: 14395.24
[INFO 2017-06-28 15:06:50,497 main.py:51] epoch 10554, training loss: 8848.65, average training loss: 9329.88, base loss: 14392.79
[INFO 2017-06-28 15:06:51,552 main.py:51] epoch 10555, training loss: 8694.13, average training loss: 9328.58, base loss: 14391.34
[INFO 2017-06-28 15:06:52,577 main.py:51] epoch 10556, training loss: 8913.20, average training loss: 9326.77, base loss: 14388.27
[INFO 2017-06-28 15:06:53,594 main.py:51] epoch 10557, training loss: 9314.60, average training loss: 9327.14, base loss: 14390.69
[INFO 2017-06-28 15:06:54,679 main.py:51] epoch 10558, training loss: 9532.79, average training loss: 9326.94, base loss: 14390.20
[INFO 2017-06-28 15:06:55,701 main.py:51] epoch 10559, training loss: 8833.81, average training loss: 9327.28, base loss: 14391.83
[INFO 2017-06-28 15:06:56,764 main.py:51] epoch 10560, training loss: 8876.24, average training loss: 9326.74, base loss: 14390.58
[INFO 2017-06-28 15:06:57,871 main.py:51] epoch 10561, training loss: 10611.08, average training loss: 9328.80, base loss: 14394.66
[INFO 2017-06-28 15:06:58,861 main.py:51] epoch 10562, training loss: 9154.62, average training loss: 9328.83, base loss: 14395.07
[INFO 2017-06-28 15:06:59,938 main.py:51] epoch 10563, training loss: 9673.97, average training loss: 9329.28, base loss: 14395.09
[INFO 2017-06-28 15:07:01,030 main.py:51] epoch 10564, training loss: 9541.11, average training loss: 9328.49, base loss: 14393.87
[INFO 2017-06-28 15:07:02,111 main.py:51] epoch 10565, training loss: 8288.19, average training loss: 9327.91, base loss: 14392.10
[INFO 2017-06-28 15:07:03,122 main.py:51] epoch 10566, training loss: 9413.62, average training loss: 9328.37, base loss: 14391.40
[INFO 2017-06-28 15:07:04,179 main.py:51] epoch 10567, training loss: 8416.79, average training loss: 9327.70, base loss: 14389.52
[INFO 2017-06-28 15:07:05,257 main.py:51] epoch 10568, training loss: 8785.49, average training loss: 9326.73, base loss: 14385.87
[INFO 2017-06-28 15:07:06,357 main.py:51] epoch 10569, training loss: 8336.70, average training loss: 9325.42, base loss: 14382.75
[INFO 2017-06-28 15:07:07,370 main.py:51] epoch 10570, training loss: 10231.43, average training loss: 9326.72, base loss: 14383.30
[INFO 2017-06-28 15:07:08,443 main.py:51] epoch 10571, training loss: 9844.15, average training loss: 9326.08, base loss: 14381.05
[INFO 2017-06-28 15:07:09,532 main.py:51] epoch 10572, training loss: 8987.37, average training loss: 9326.54, base loss: 14382.57
[INFO 2017-06-28 15:07:10,556 main.py:51] epoch 10573, training loss: 10424.67, average training loss: 9328.82, base loss: 14387.31
[INFO 2017-06-28 15:07:11,603 main.py:51] epoch 10574, training loss: 8987.38, average training loss: 9329.15, base loss: 14389.36
[INFO 2017-06-28 15:07:12,688 main.py:51] epoch 10575, training loss: 9261.78, average training loss: 9330.24, base loss: 14391.03
[INFO 2017-06-28 15:07:13,755 main.py:51] epoch 10576, training loss: 10349.13, average training loss: 9331.59, base loss: 14393.89
[INFO 2017-06-28 15:07:14,815 main.py:51] epoch 10577, training loss: 8435.48, average training loss: 9329.87, base loss: 14391.31
[INFO 2017-06-28 15:07:15,842 main.py:51] epoch 10578, training loss: 8950.80, average training loss: 9329.74, base loss: 14390.25
[INFO 2017-06-28 15:07:16,902 main.py:51] epoch 10579, training loss: 8790.73, average training loss: 9328.96, base loss: 14388.07
[INFO 2017-06-28 15:07:17,973 main.py:51] epoch 10580, training loss: 9596.62, average training loss: 9330.53, base loss: 14390.21
[INFO 2017-06-28 15:07:19,037 main.py:51] epoch 10581, training loss: 10259.70, average training loss: 9328.38, base loss: 14385.98
[INFO 2017-06-28 15:07:20,137 main.py:51] epoch 10582, training loss: 9469.19, average training loss: 9329.24, base loss: 14388.09
[INFO 2017-06-28 15:07:21,179 main.py:51] epoch 10583, training loss: 9428.19, average training loss: 9328.88, base loss: 14388.64
[INFO 2017-06-28 15:07:22,222 main.py:51] epoch 10584, training loss: 10096.34, average training loss: 9329.01, base loss: 14387.79
[INFO 2017-06-28 15:07:23,306 main.py:51] epoch 10585, training loss: 8013.78, average training loss: 9327.75, base loss: 14384.71
[INFO 2017-06-28 15:07:24,400 main.py:51] epoch 10586, training loss: 9699.45, average training loss: 9329.52, base loss: 14387.62
[INFO 2017-06-28 15:07:25,524 main.py:51] epoch 10587, training loss: 9553.74, average training loss: 9329.20, base loss: 14386.58
[INFO 2017-06-28 15:07:26,552 main.py:51] epoch 10588, training loss: 9038.17, average training loss: 9328.19, base loss: 14386.38
[INFO 2017-06-28 15:07:27,628 main.py:51] epoch 10589, training loss: 9138.50, average training loss: 9326.59, base loss: 14383.37
[INFO 2017-06-28 15:07:28,673 main.py:51] epoch 10590, training loss: 9714.50, average training loss: 9327.86, base loss: 14385.08
[INFO 2017-06-28 15:07:29,705 main.py:51] epoch 10591, training loss: 10947.67, average training loss: 9329.59, base loss: 14386.84
[INFO 2017-06-28 15:07:30,795 main.py:51] epoch 10592, training loss: 9375.92, average training loss: 9329.79, base loss: 14387.47
[INFO 2017-06-28 15:07:31,893 main.py:51] epoch 10593, training loss: 8441.45, average training loss: 9328.99, base loss: 14385.60
[INFO 2017-06-28 15:07:33,006 main.py:51] epoch 10594, training loss: 8951.98, average training loss: 9328.98, base loss: 14385.81
[INFO 2017-06-28 15:07:34,048 main.py:51] epoch 10595, training loss: 8872.89, average training loss: 9328.35, base loss: 14386.88
[INFO 2017-06-28 15:07:35,121 main.py:51] epoch 10596, training loss: 9544.70, average training loss: 9329.48, base loss: 14389.81
[INFO 2017-06-28 15:07:36,162 main.py:51] epoch 10597, training loss: 8886.23, average training loss: 9329.49, base loss: 14389.64
[INFO 2017-06-28 15:07:37,204 main.py:51] epoch 10598, training loss: 8995.27, average training loss: 9329.12, base loss: 14388.95
[INFO 2017-06-28 15:07:38,323 main.py:51] epoch 10599, training loss: 9739.85, average training loss: 9328.32, base loss: 14388.75
[INFO 2017-06-28 15:07:38,324 main.py:53] epoch 10599, testing
[INFO 2017-06-28 15:07:42,254 main.py:105] average testing loss: 10721.74, base loss: 15544.26
[INFO 2017-06-28 15:07:42,255 main.py:106] improve_loss: 4822.52, improve_percent: 0.31
[INFO 2017-06-28 15:07:42,255 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:07:43,345 main.py:51] epoch 10600, training loss: 9164.99, average training loss: 9329.00, base loss: 14389.92
[INFO 2017-06-28 15:07:44,411 main.py:51] epoch 10601, training loss: 10637.45, average training loss: 9331.36, base loss: 14393.71
[INFO 2017-06-28 15:07:45,501 main.py:51] epoch 10602, training loss: 8803.58, average training loss: 9331.20, base loss: 14393.44
[INFO 2017-06-28 15:07:46,597 main.py:51] epoch 10603, training loss: 9867.19, average training loss: 9331.57, base loss: 14395.28
[INFO 2017-06-28 15:07:47,605 main.py:51] epoch 10604, training loss: 10533.54, average training loss: 9332.59, base loss: 14398.60
[INFO 2017-06-28 15:07:48,564 main.py:51] epoch 10605, training loss: 9300.14, average training loss: 9331.93, base loss: 14397.29
[INFO 2017-06-28 15:07:49,543 main.py:51] epoch 10606, training loss: 9106.27, average training loss: 9332.13, base loss: 14397.09
[INFO 2017-06-28 15:07:50,562 main.py:51] epoch 10607, training loss: 9195.28, average training loss: 9330.96, base loss: 14395.50
[INFO 2017-06-28 15:07:51,577 main.py:51] epoch 10608, training loss: 8628.81, average training loss: 9330.17, base loss: 14394.47
[INFO 2017-06-28 15:07:52,623 main.py:51] epoch 10609, training loss: 9239.91, average training loss: 9330.00, base loss: 14394.47
[INFO 2017-06-28 15:07:53,669 main.py:51] epoch 10610, training loss: 9025.15, average training loss: 9330.10, base loss: 14392.77
[INFO 2017-06-28 15:07:54,719 main.py:51] epoch 10611, training loss: 10501.20, average training loss: 9332.03, base loss: 14395.19
[INFO 2017-06-28 15:07:55,742 main.py:51] epoch 10612, training loss: 10652.02, average training loss: 9333.18, base loss: 14394.31
[INFO 2017-06-28 15:07:56,793 main.py:51] epoch 10613, training loss: 9326.06, average training loss: 9333.49, base loss: 14395.69
[INFO 2017-06-28 15:07:57,862 main.py:51] epoch 10614, training loss: 8297.20, average training loss: 9333.24, base loss: 14394.74
[INFO 2017-06-28 15:07:58,863 main.py:51] epoch 10615, training loss: 9978.54, average training loss: 9334.34, base loss: 14394.77
[INFO 2017-06-28 15:07:59,942 main.py:51] epoch 10616, training loss: 9070.53, average training loss: 9333.78, base loss: 14393.56
[INFO 2017-06-28 15:08:01,046 main.py:51] epoch 10617, training loss: 8872.94, average training loss: 9332.86, base loss: 14392.64
[INFO 2017-06-28 15:08:02,037 main.py:51] epoch 10618, training loss: 9296.69, average training loss: 9332.36, base loss: 14392.74
[INFO 2017-06-28 15:08:03,117 main.py:51] epoch 10619, training loss: 9126.49, average training loss: 9333.29, base loss: 14395.67
[INFO 2017-06-28 15:08:04,204 main.py:51] epoch 10620, training loss: 9768.34, average training loss: 9331.44, base loss: 14394.60
[INFO 2017-06-28 15:08:05,258 main.py:51] epoch 10621, training loss: 9965.32, average training loss: 9332.20, base loss: 14396.19
[INFO 2017-06-28 15:08:06,401 main.py:51] epoch 10622, training loss: 9185.55, average training loss: 9331.13, base loss: 14394.75
[INFO 2017-06-28 15:08:07,438 main.py:51] epoch 10623, training loss: 9138.35, average training loss: 9331.03, base loss: 14392.90
[INFO 2017-06-28 15:08:08,528 main.py:51] epoch 10624, training loss: 9214.13, average training loss: 9332.49, base loss: 14394.22
[INFO 2017-06-28 15:08:09,568 main.py:51] epoch 10625, training loss: 7634.10, average training loss: 9330.49, base loss: 14390.62
[INFO 2017-06-28 15:08:10,614 main.py:51] epoch 10626, training loss: 9244.01, average training loss: 9330.17, base loss: 14390.20
[INFO 2017-06-28 15:08:11,698 main.py:51] epoch 10627, training loss: 9601.52, average training loss: 9330.69, base loss: 14390.33
[INFO 2017-06-28 15:08:12,802 main.py:51] epoch 10628, training loss: 8780.54, average training loss: 9331.16, base loss: 14391.01
[INFO 2017-06-28 15:08:13,899 main.py:51] epoch 10629, training loss: 9982.05, average training loss: 9332.76, base loss: 14392.62
[INFO 2017-06-28 15:08:14,953 main.py:51] epoch 10630, training loss: 9847.85, average training loss: 9333.95, base loss: 14396.40
[INFO 2017-06-28 15:08:16,052 main.py:51] epoch 10631, training loss: 9118.81, average training loss: 9333.42, base loss: 14395.35
[INFO 2017-06-28 15:08:17,095 main.py:51] epoch 10632, training loss: 9490.81, average training loss: 9333.68, base loss: 14396.43
[INFO 2017-06-28 15:08:18,125 main.py:51] epoch 10633, training loss: 8562.75, average training loss: 9333.12, base loss: 14394.13
[INFO 2017-06-28 15:08:19,212 main.py:51] epoch 10634, training loss: 8974.47, average training loss: 9332.16, base loss: 14392.33
[INFO 2017-06-28 15:08:20,299 main.py:51] epoch 10635, training loss: 10151.73, average training loss: 9333.40, base loss: 14394.83
[INFO 2017-06-28 15:08:21,419 main.py:51] epoch 10636, training loss: 9006.30, average training loss: 9332.43, base loss: 14394.19
[INFO 2017-06-28 15:08:22,433 main.py:51] epoch 10637, training loss: 10287.28, average training loss: 9332.99, base loss: 14397.01
[INFO 2017-06-28 15:08:23,490 main.py:51] epoch 10638, training loss: 11515.26, average training loss: 9335.65, base loss: 14400.35
[INFO 2017-06-28 15:08:24,561 main.py:51] epoch 10639, training loss: 10696.59, average training loss: 9338.06, base loss: 14404.41
[INFO 2017-06-28 15:08:25,639 main.py:51] epoch 10640, training loss: 8946.15, average training loss: 9339.15, base loss: 14405.64
[INFO 2017-06-28 15:08:26,790 main.py:51] epoch 10641, training loss: 10517.21, average training loss: 9340.49, base loss: 14407.52
[INFO 2017-06-28 15:08:27,832 main.py:51] epoch 10642, training loss: 7909.09, average training loss: 9338.26, base loss: 14404.78
[INFO 2017-06-28 15:08:28,919 main.py:51] epoch 10643, training loss: 10071.41, average training loss: 9338.33, base loss: 14404.54
[INFO 2017-06-28 15:08:29,992 main.py:51] epoch 10644, training loss: 9324.83, average training loss: 9337.76, base loss: 14402.99
[INFO 2017-06-28 15:08:31,074 main.py:51] epoch 10645, training loss: 10645.89, average training loss: 9338.28, base loss: 14403.87
[INFO 2017-06-28 15:08:32,083 main.py:51] epoch 10646, training loss: 9895.61, average training loss: 9339.31, base loss: 14406.98
[INFO 2017-06-28 15:08:33,138 main.py:51] epoch 10647, training loss: 9219.00, average training loss: 9339.42, base loss: 14408.36
[INFO 2017-06-28 15:08:34,230 main.py:51] epoch 10648, training loss: 10044.63, average training loss: 9339.45, base loss: 14411.11
[INFO 2017-06-28 15:08:35,295 main.py:51] epoch 10649, training loss: 9291.89, average training loss: 9339.27, base loss: 14412.03
[INFO 2017-06-28 15:08:36,397 main.py:51] epoch 10650, training loss: 9194.01, average training loss: 9340.19, base loss: 14413.80
[INFO 2017-06-28 15:08:37,426 main.py:51] epoch 10651, training loss: 9110.45, average training loss: 9338.50, base loss: 14411.81
[INFO 2017-06-28 15:08:38,505 main.py:51] epoch 10652, training loss: 10100.87, average training loss: 9337.91, base loss: 14412.92
[INFO 2017-06-28 15:08:39,549 main.py:51] epoch 10653, training loss: 8225.92, average training loss: 9337.83, base loss: 14411.89
[INFO 2017-06-28 15:08:40,607 main.py:51] epoch 10654, training loss: 9111.35, average training loss: 9335.35, base loss: 14407.66
[INFO 2017-06-28 15:08:41,639 main.py:51] epoch 10655, training loss: 9463.89, average training loss: 9335.45, base loss: 14408.10
[INFO 2017-06-28 15:08:42,696 main.py:51] epoch 10656, training loss: 8246.12, average training loss: 9334.89, base loss: 14408.30
[INFO 2017-06-28 15:08:43,759 main.py:51] epoch 10657, training loss: 9370.17, average training loss: 9334.28, base loss: 14407.59
[INFO 2017-06-28 15:08:44,824 main.py:51] epoch 10658, training loss: 8807.20, average training loss: 9333.87, base loss: 14408.58
[INFO 2017-06-28 15:08:45,938 main.py:51] epoch 10659, training loss: 9729.24, average training loss: 9333.57, base loss: 14408.26
[INFO 2017-06-28 15:08:47,038 main.py:51] epoch 10660, training loss: 10574.38, average training loss: 9335.15, base loss: 14408.55
[INFO 2017-06-28 15:08:48,029 main.py:51] epoch 10661, training loss: 10231.69, average training loss: 9335.99, base loss: 14409.51
[INFO 2017-06-28 15:08:49,064 main.py:51] epoch 10662, training loss: 9717.22, average training loss: 9336.61, base loss: 14409.68
[INFO 2017-06-28 15:08:50,106 main.py:51] epoch 10663, training loss: 9662.45, average training loss: 9336.33, base loss: 14409.03
[INFO 2017-06-28 15:08:51,186 main.py:51] epoch 10664, training loss: 8856.49, average training loss: 9336.40, base loss: 14409.94
[INFO 2017-06-28 15:08:52,334 main.py:51] epoch 10665, training loss: 10070.99, average training loss: 9338.52, base loss: 14412.78
[INFO 2017-06-28 15:08:53,426 main.py:51] epoch 10666, training loss: 8645.13, average training loss: 9338.60, base loss: 14412.79
[INFO 2017-06-28 15:08:54,519 main.py:51] epoch 10667, training loss: 8990.58, average training loss: 9337.80, base loss: 14412.72
[INFO 2017-06-28 15:08:55,537 main.py:51] epoch 10668, training loss: 11002.89, average training loss: 9339.34, base loss: 14413.91
[INFO 2017-06-28 15:08:56,558 main.py:51] epoch 10669, training loss: 9095.36, average training loss: 9339.85, base loss: 14414.71
[INFO 2017-06-28 15:08:57,590 main.py:51] epoch 10670, training loss: 7950.59, average training loss: 9337.56, base loss: 14409.71
[INFO 2017-06-28 15:08:58,709 main.py:51] epoch 10671, training loss: 9152.15, average training loss: 9337.33, base loss: 14409.39
[INFO 2017-06-28 15:08:59,807 main.py:51] epoch 10672, training loss: 9226.73, average training loss: 9338.21, base loss: 14410.45
[INFO 2017-06-28 15:09:00,912 main.py:51] epoch 10673, training loss: 9002.31, average training loss: 9337.15, base loss: 14405.64
[INFO 2017-06-28 15:09:02,010 main.py:51] epoch 10674, training loss: 10601.76, average training loss: 9338.61, base loss: 14407.23
[INFO 2017-06-28 15:09:03,055 main.py:51] epoch 10675, training loss: 9719.98, average training loss: 9340.37, base loss: 14409.77
[INFO 2017-06-28 15:09:04,127 main.py:51] epoch 10676, training loss: 8487.07, average training loss: 9338.90, base loss: 14407.55
[INFO 2017-06-28 15:09:05,151 main.py:51] epoch 10677, training loss: 9355.98, average training loss: 9339.53, base loss: 14408.35
[INFO 2017-06-28 15:09:06,117 main.py:51] epoch 10678, training loss: 10159.28, average training loss: 9340.81, base loss: 14411.01
[INFO 2017-06-28 15:09:07,021 main.py:51] epoch 10679, training loss: 7903.87, average training loss: 9338.82, base loss: 14408.61
[INFO 2017-06-28 15:09:07,936 main.py:51] epoch 10680, training loss: 8609.61, average training loss: 9338.10, base loss: 14408.08
[INFO 2017-06-28 15:09:08,915 main.py:51] epoch 10681, training loss: 8520.74, average training loss: 9338.27, base loss: 14408.60
[INFO 2017-06-28 15:09:09,972 main.py:51] epoch 10682, training loss: 9561.64, average training loss: 9338.32, base loss: 14407.80
[INFO 2017-06-28 15:09:10,999 main.py:51] epoch 10683, training loss: 9379.62, average training loss: 9338.92, base loss: 14409.22
[INFO 2017-06-28 15:09:12,065 main.py:51] epoch 10684, training loss: 8544.75, average training loss: 9337.45, base loss: 14406.05
[INFO 2017-06-28 15:09:13,134 main.py:51] epoch 10685, training loss: 9174.74, average training loss: 9337.93, base loss: 14407.31
[INFO 2017-06-28 15:09:14,162 main.py:51] epoch 10686, training loss: 7574.06, average training loss: 9335.19, base loss: 14401.74
[INFO 2017-06-28 15:09:15,183 main.py:51] epoch 10687, training loss: 9520.34, average training loss: 9336.11, base loss: 14404.72
[INFO 2017-06-28 15:09:16,252 main.py:51] epoch 10688, training loss: 8953.78, average training loss: 9335.11, base loss: 14403.34
[INFO 2017-06-28 15:09:17,284 main.py:51] epoch 10689, training loss: 8162.46, average training loss: 9334.22, base loss: 14401.00
[INFO 2017-06-28 15:09:18,340 main.py:51] epoch 10690, training loss: 9075.62, average training loss: 9333.59, base loss: 14401.09
[INFO 2017-06-28 15:09:19,372 main.py:51] epoch 10691, training loss: 9771.44, average training loss: 9334.25, base loss: 14402.08
[INFO 2017-06-28 15:09:20,452 main.py:51] epoch 10692, training loss: 8782.16, average training loss: 9333.83, base loss: 14403.57
[INFO 2017-06-28 15:09:21,553 main.py:51] epoch 10693, training loss: 9242.70, average training loss: 9334.09, base loss: 14403.54
[INFO 2017-06-28 15:09:22,614 main.py:51] epoch 10694, training loss: 7995.15, average training loss: 9331.77, base loss: 14400.55
[INFO 2017-06-28 15:09:23,721 main.py:51] epoch 10695, training loss: 8398.79, average training loss: 9331.62, base loss: 14399.81
[INFO 2017-06-28 15:09:24,819 main.py:51] epoch 10696, training loss: 8868.93, average training loss: 9331.89, base loss: 14400.71
[INFO 2017-06-28 15:09:25,868 main.py:51] epoch 10697, training loss: 12269.10, average training loss: 9334.20, base loss: 14403.02
[INFO 2017-06-28 15:09:26,870 main.py:51] epoch 10698, training loss: 8589.31, average training loss: 9334.22, base loss: 14402.97
[INFO 2017-06-28 15:09:27,957 main.py:51] epoch 10699, training loss: 8522.65, average training loss: 9332.60, base loss: 14399.89
[INFO 2017-06-28 15:09:27,957 main.py:53] epoch 10699, testing
[INFO 2017-06-28 15:09:31,626 main.py:105] average testing loss: 10554.63, base loss: 14775.78
[INFO 2017-06-28 15:09:31,626 main.py:106] improve_loss: 4221.15, improve_percent: 0.29
[INFO 2017-06-28 15:09:31,627 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:09:32,641 main.py:51] epoch 10700, training loss: 8787.93, average training loss: 9332.52, base loss: 14400.48
[INFO 2017-06-28 15:09:33,733 main.py:51] epoch 10701, training loss: 9712.78, average training loss: 9332.17, base loss: 14400.00
[INFO 2017-06-28 15:09:34,735 main.py:51] epoch 10702, training loss: 8817.91, average training loss: 9331.40, base loss: 14397.26
[INFO 2017-06-28 15:09:35,796 main.py:51] epoch 10703, training loss: 9260.22, average training loss: 9330.71, base loss: 14397.27
[INFO 2017-06-28 15:09:36,889 main.py:51] epoch 10704, training loss: 8326.83, average training loss: 9329.54, base loss: 14394.87
[INFO 2017-06-28 15:09:37,923 main.py:51] epoch 10705, training loss: 9520.53, average training loss: 9330.02, base loss: 14395.67
[INFO 2017-06-28 15:09:38,958 main.py:51] epoch 10706, training loss: 9101.34, average training loss: 9330.19, base loss: 14396.69
[INFO 2017-06-28 15:09:39,985 main.py:51] epoch 10707, training loss: 9557.39, average training loss: 9330.20, base loss: 14397.74
[INFO 2017-06-28 15:09:41,051 main.py:51] epoch 10708, training loss: 10162.40, average training loss: 9331.76, base loss: 14402.46
[INFO 2017-06-28 15:09:42,156 main.py:51] epoch 10709, training loss: 9564.96, average training loss: 9333.13, base loss: 14405.49
[INFO 2017-06-28 15:09:43,161 main.py:51] epoch 10710, training loss: 9962.79, average training loss: 9332.97, base loss: 14405.53
[INFO 2017-06-28 15:09:44,206 main.py:51] epoch 10711, training loss: 9999.98, average training loss: 9333.08, base loss: 14405.35
[INFO 2017-06-28 15:09:45,298 main.py:51] epoch 10712, training loss: 8800.52, average training loss: 9333.74, base loss: 14407.06
[INFO 2017-06-28 15:09:46,371 main.py:51] epoch 10713, training loss: 9459.70, average training loss: 9334.04, base loss: 14409.29
[INFO 2017-06-28 15:09:47,454 main.py:51] epoch 10714, training loss: 9355.42, average training loss: 9334.18, base loss: 14409.58
[INFO 2017-06-28 15:09:48,500 main.py:51] epoch 10715, training loss: 10066.37, average training loss: 9334.89, base loss: 14410.26
[INFO 2017-06-28 15:09:49,532 main.py:51] epoch 10716, training loss: 9508.27, average training loss: 9335.11, base loss: 14409.61
[INFO 2017-06-28 15:09:50,612 main.py:51] epoch 10717, training loss: 8920.28, average training loss: 9333.87, base loss: 14409.63
[INFO 2017-06-28 15:09:51,681 main.py:51] epoch 10718, training loss: 8074.60, average training loss: 9331.30, base loss: 14405.46
[INFO 2017-06-28 15:09:52,752 main.py:51] epoch 10719, training loss: 9920.01, average training loss: 9330.74, base loss: 14404.55
[INFO 2017-06-28 15:09:53,770 main.py:51] epoch 10720, training loss: 8357.49, average training loss: 9330.10, base loss: 14403.66
[INFO 2017-06-28 15:09:54,849 main.py:51] epoch 10721, training loss: 7937.83, average training loss: 9326.88, base loss: 14398.18
[INFO 2017-06-28 15:09:55,979 main.py:51] epoch 10722, training loss: 9489.21, average training loss: 9327.26, base loss: 14399.21
[INFO 2017-06-28 15:09:57,083 main.py:51] epoch 10723, training loss: 9309.19, average training loss: 9327.78, base loss: 14400.63
[INFO 2017-06-28 15:09:58,164 main.py:51] epoch 10724, training loss: 8330.16, average training loss: 9326.20, base loss: 14399.43
[INFO 2017-06-28 15:09:59,117 main.py:51] epoch 10725, training loss: 9053.23, average training loss: 9324.86, base loss: 14396.88
[INFO 2017-06-28 15:10:00,193 main.py:51] epoch 10726, training loss: 9743.06, average training loss: 9324.42, base loss: 14397.05
[INFO 2017-06-28 15:10:01,305 main.py:51] epoch 10727, training loss: 9835.34, average training loss: 9325.23, base loss: 14399.78
[INFO 2017-06-28 15:10:02,303 main.py:51] epoch 10728, training loss: 7721.35, average training loss: 9324.14, base loss: 14397.70
[INFO 2017-06-28 15:10:03,335 main.py:51] epoch 10729, training loss: 8638.25, average training loss: 9323.86, base loss: 14397.72
[INFO 2017-06-28 15:10:04,405 main.py:51] epoch 10730, training loss: 8799.28, average training loss: 9323.99, base loss: 14398.91
[INFO 2017-06-28 15:10:05,520 main.py:51] epoch 10731, training loss: 9468.68, average training loss: 9323.96, base loss: 14397.72
[INFO 2017-06-28 15:10:06,647 main.py:51] epoch 10732, training loss: 8437.95, average training loss: 9323.44, base loss: 14397.59
[INFO 2017-06-28 15:10:07,676 main.py:51] epoch 10733, training loss: 9182.45, average training loss: 9323.66, base loss: 14396.39
[INFO 2017-06-28 15:10:08,761 main.py:51] epoch 10734, training loss: 8338.76, average training loss: 9322.64, base loss: 14394.69
[INFO 2017-06-28 15:10:09,788 main.py:51] epoch 10735, training loss: 9974.78, average training loss: 9322.39, base loss: 14395.87
[INFO 2017-06-28 15:10:10,847 main.py:51] epoch 10736, training loss: 9090.36, average training loss: 9323.25, base loss: 14397.85
[INFO 2017-06-28 15:10:11,958 main.py:51] epoch 10737, training loss: 8384.06, average training loss: 9322.19, base loss: 14395.14
[INFO 2017-06-28 15:10:12,969 main.py:51] epoch 10738, training loss: 9809.32, average training loss: 9321.92, base loss: 14394.88
[INFO 2017-06-28 15:10:13,993 main.py:51] epoch 10739, training loss: 9796.33, average training loss: 9322.35, base loss: 14395.26
[INFO 2017-06-28 15:10:15,032 main.py:51] epoch 10740, training loss: 9218.79, average training loss: 9322.31, base loss: 14396.86
[INFO 2017-06-28 15:10:16,099 main.py:51] epoch 10741, training loss: 10105.32, average training loss: 9322.44, base loss: 14394.88
[INFO 2017-06-28 15:10:17,191 main.py:51] epoch 10742, training loss: 8873.60, average training loss: 9322.17, base loss: 14394.34
[INFO 2017-06-28 15:10:18,229 main.py:51] epoch 10743, training loss: 8405.23, average training loss: 9321.51, base loss: 14392.69
[INFO 2017-06-28 15:10:19,328 main.py:51] epoch 10744, training loss: 10176.38, average training loss: 9322.92, base loss: 14395.35
[INFO 2017-06-28 15:10:20,377 main.py:51] epoch 10745, training loss: 8619.42, average training loss: 9322.63, base loss: 14393.52
[INFO 2017-06-28 15:10:21,433 main.py:51] epoch 10746, training loss: 8797.00, average training loss: 9321.36, base loss: 14391.45
[INFO 2017-06-28 15:10:22,472 main.py:51] epoch 10747, training loss: 9381.84, average training loss: 9320.95, base loss: 14391.79
[INFO 2017-06-28 15:10:23,531 main.py:51] epoch 10748, training loss: 11228.07, average training loss: 9323.11, base loss: 14396.37
[INFO 2017-06-28 15:10:24,494 main.py:51] epoch 10749, training loss: 9820.99, average training loss: 9324.31, base loss: 14399.33
[INFO 2017-06-28 15:10:25,495 main.py:51] epoch 10750, training loss: 9137.92, average training loss: 9324.44, base loss: 14399.07
[INFO 2017-06-28 15:10:26,489 main.py:51] epoch 10751, training loss: 10035.53, average training loss: 9325.17, base loss: 14400.57
[INFO 2017-06-28 15:10:27,487 main.py:51] epoch 10752, training loss: 9562.38, average training loss: 9325.44, base loss: 14400.04
[INFO 2017-06-28 15:10:28,575 main.py:51] epoch 10753, training loss: 9017.45, average training loss: 9324.27, base loss: 14397.64
[INFO 2017-06-28 15:10:29,675 main.py:51] epoch 10754, training loss: 8636.37, average training loss: 9322.89, base loss: 14396.11
[INFO 2017-06-28 15:10:30,744 main.py:51] epoch 10755, training loss: 9004.39, average training loss: 9322.86, base loss: 14397.65
[INFO 2017-06-28 15:10:31,765 main.py:51] epoch 10756, training loss: 9811.69, average training loss: 9322.04, base loss: 14396.03
[INFO 2017-06-28 15:10:32,809 main.py:51] epoch 10757, training loss: 10217.42, average training loss: 9322.82, base loss: 14397.11
[INFO 2017-06-28 15:10:33,891 main.py:51] epoch 10758, training loss: 9766.33, average training loss: 9324.16, base loss: 14399.90
[INFO 2017-06-28 15:10:34,989 main.py:51] epoch 10759, training loss: 10075.76, average training loss: 9325.43, base loss: 14401.50
[INFO 2017-06-28 15:10:36,088 main.py:51] epoch 10760, training loss: 9587.52, average training loss: 9325.92, base loss: 14403.23
[INFO 2017-06-28 15:10:37,224 main.py:51] epoch 10761, training loss: 7926.98, average training loss: 9324.35, base loss: 14400.10
[INFO 2017-06-28 15:10:38,250 main.py:51] epoch 10762, training loss: 8393.65, average training loss: 9324.44, base loss: 14400.40
[INFO 2017-06-28 15:10:39,341 main.py:51] epoch 10763, training loss: 9635.39, average training loss: 9324.09, base loss: 14400.25
[INFO 2017-06-28 15:10:40,372 main.py:51] epoch 10764, training loss: 9036.72, average training loss: 9323.62, base loss: 14400.79
[INFO 2017-06-28 15:10:41,427 main.py:51] epoch 10765, training loss: 10688.14, average training loss: 9325.00, base loss: 14403.02
[INFO 2017-06-28 15:10:42,551 main.py:51] epoch 10766, training loss: 9564.73, average training loss: 9326.48, base loss: 14405.69
[INFO 2017-06-28 15:10:43,664 main.py:51] epoch 10767, training loss: 8958.64, average training loss: 9326.50, base loss: 14406.19
[INFO 2017-06-28 15:10:44,816 main.py:51] epoch 10768, training loss: 10467.22, average training loss: 9327.96, base loss: 14409.22
[INFO 2017-06-28 15:10:45,901 main.py:51] epoch 10769, training loss: 9571.36, average training loss: 9328.95, base loss: 14410.07
[INFO 2017-06-28 15:10:46,971 main.py:51] epoch 10770, training loss: 9075.06, average training loss: 9329.07, base loss: 14410.35
[INFO 2017-06-28 15:10:47,989 main.py:51] epoch 10771, training loss: 8654.77, average training loss: 9329.50, base loss: 14411.36
[INFO 2017-06-28 15:10:49,033 main.py:51] epoch 10772, training loss: 8958.35, average training loss: 9330.15, base loss: 14412.56
[INFO 2017-06-28 15:10:50,116 main.py:51] epoch 10773, training loss: 9195.94, average training loss: 9330.51, base loss: 14412.03
[INFO 2017-06-28 15:10:51,183 main.py:51] epoch 10774, training loss: 10099.39, average training loss: 9331.09, base loss: 14411.27
[INFO 2017-06-28 15:10:52,297 main.py:51] epoch 10775, training loss: 8681.10, average training loss: 9331.06, base loss: 14411.50
[INFO 2017-06-28 15:10:53,345 main.py:51] epoch 10776, training loss: 11178.87, average training loss: 9331.51, base loss: 14413.66
[INFO 2017-06-28 15:10:54,425 main.py:51] epoch 10777, training loss: 9168.82, average training loss: 9331.29, base loss: 14412.96
[INFO 2017-06-28 15:10:55,444 main.py:51] epoch 10778, training loss: 8677.67, average training loss: 9332.59, base loss: 14415.58
[INFO 2017-06-28 15:10:56,526 main.py:51] epoch 10779, training loss: 9110.34, average training loss: 9331.97, base loss: 14413.87
[INFO 2017-06-28 15:10:57,568 main.py:51] epoch 10780, training loss: 9363.73, average training loss: 9332.47, base loss: 14414.87
[INFO 2017-06-28 15:10:58,624 main.py:51] epoch 10781, training loss: 9097.16, average training loss: 9332.19, base loss: 14414.12
[INFO 2017-06-28 15:10:59,699 main.py:51] epoch 10782, training loss: 8685.58, average training loss: 9332.11, base loss: 14414.64
[INFO 2017-06-28 15:11:00,788 main.py:51] epoch 10783, training loss: 8511.28, average training loss: 9330.63, base loss: 14413.87
[INFO 2017-06-28 15:11:01,875 main.py:51] epoch 10784, training loss: 7903.90, average training loss: 9330.52, base loss: 14414.05
[INFO 2017-06-28 15:11:02,921 main.py:51] epoch 10785, training loss: 9204.65, average training loss: 9330.52, base loss: 14414.37
[INFO 2017-06-28 15:11:03,963 main.py:51] epoch 10786, training loss: 9980.44, average training loss: 9331.02, base loss: 14413.97
[INFO 2017-06-28 15:11:05,015 main.py:51] epoch 10787, training loss: 9175.78, average training loss: 9331.31, base loss: 14413.74
[INFO 2017-06-28 15:11:06,090 main.py:51] epoch 10788, training loss: 10808.52, average training loss: 9331.09, base loss: 14413.06
[INFO 2017-06-28 15:11:07,164 main.py:51] epoch 10789, training loss: 9244.99, average training loss: 9329.52, base loss: 14408.97
[INFO 2017-06-28 15:11:08,179 main.py:51] epoch 10790, training loss: 8462.93, average training loss: 9328.77, base loss: 14407.20
[INFO 2017-06-28 15:11:09,251 main.py:51] epoch 10791, training loss: 9592.22, average training loss: 9328.61, base loss: 14406.58
[INFO 2017-06-28 15:11:10,315 main.py:51] epoch 10792, training loss: 9245.73, average training loss: 9328.20, base loss: 14404.00
[INFO 2017-06-28 15:11:11,319 main.py:51] epoch 10793, training loss: 7863.26, average training loss: 9326.22, base loss: 14401.18
[INFO 2017-06-28 15:11:12,361 main.py:51] epoch 10794, training loss: 9657.82, average training loss: 9327.59, base loss: 14403.06
[INFO 2017-06-28 15:11:13,419 main.py:51] epoch 10795, training loss: 9445.77, average training loss: 9326.59, base loss: 14402.73
[INFO 2017-06-28 15:11:14,481 main.py:51] epoch 10796, training loss: 10126.58, average training loss: 9329.30, base loss: 14407.70
[INFO 2017-06-28 15:11:15,506 main.py:51] epoch 10797, training loss: 9305.24, average training loss: 9330.07, base loss: 14407.37
[INFO 2017-06-28 15:11:16,534 main.py:51] epoch 10798, training loss: 9538.67, average training loss: 9328.75, base loss: 14405.00
[INFO 2017-06-28 15:11:17,514 main.py:51] epoch 10799, training loss: 9128.00, average training loss: 9329.45, base loss: 14406.53
[INFO 2017-06-28 15:11:17,514 main.py:53] epoch 10799, testing
[INFO 2017-06-28 15:11:21,240 main.py:105] average testing loss: 10072.92, base loss: 14576.24
[INFO 2017-06-28 15:11:21,240 main.py:106] improve_loss: 4503.32, improve_percent: 0.31
[INFO 2017-06-28 15:11:21,241 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:11:22,224 main.py:51] epoch 10800, training loss: 9295.63, average training loss: 9329.15, base loss: 14406.33
[INFO 2017-06-28 15:11:23,248 main.py:51] epoch 10801, training loss: 10894.11, average training loss: 9329.88, base loss: 14408.16
[INFO 2017-06-28 15:11:24,301 main.py:51] epoch 10802, training loss: 8023.55, average training loss: 9328.29, base loss: 14406.63
[INFO 2017-06-28 15:11:25,375 main.py:51] epoch 10803, training loss: 9106.61, average training loss: 9327.90, base loss: 14408.29
[INFO 2017-06-28 15:11:26,488 main.py:51] epoch 10804, training loss: 8152.56, average training loss: 9327.70, base loss: 14409.17
[INFO 2017-06-28 15:11:27,546 main.py:51] epoch 10805, training loss: 9415.34, average training loss: 9326.96, base loss: 14409.23
[INFO 2017-06-28 15:11:28,604 main.py:51] epoch 10806, training loss: 8996.44, average training loss: 9326.86, base loss: 14409.92
[INFO 2017-06-28 15:11:29,644 main.py:51] epoch 10807, training loss: 10057.95, average training loss: 9326.44, base loss: 14409.81
[INFO 2017-06-28 15:11:30,711 main.py:51] epoch 10808, training loss: 9580.33, average training loss: 9326.95, base loss: 14411.28
[INFO 2017-06-28 15:11:31,743 main.py:51] epoch 10809, training loss: 8990.14, average training loss: 9327.39, base loss: 14412.55
[INFO 2017-06-28 15:11:32,782 main.py:51] epoch 10810, training loss: 8729.83, average training loss: 9326.41, base loss: 14411.19
[INFO 2017-06-28 15:11:33,875 main.py:51] epoch 10811, training loss: 10380.98, average training loss: 9327.79, base loss: 14415.02
[INFO 2017-06-28 15:11:34,888 main.py:51] epoch 10812, training loss: 9999.83, average training loss: 9328.80, base loss: 14415.52
[INFO 2017-06-28 15:11:35,941 main.py:51] epoch 10813, training loss: 9419.39, average training loss: 9326.97, base loss: 14412.26
[INFO 2017-06-28 15:11:37,020 main.py:51] epoch 10814, training loss: 8572.93, average training loss: 9326.75, base loss: 14411.87
[INFO 2017-06-28 15:11:38,071 main.py:51] epoch 10815, training loss: 8762.75, average training loss: 9326.77, base loss: 14412.77
[INFO 2017-06-28 15:11:39,113 main.py:51] epoch 10816, training loss: 9100.49, average training loss: 9325.71, base loss: 14409.99
[INFO 2017-06-28 15:11:40,158 main.py:51] epoch 10817, training loss: 8676.61, average training loss: 9324.73, base loss: 14409.70
[INFO 2017-06-28 15:11:41,221 main.py:51] epoch 10818, training loss: 8617.12, average training loss: 9324.07, base loss: 14408.03
[INFO 2017-06-28 15:11:42,123 main.py:51] epoch 10819, training loss: 9278.67, average training loss: 9323.42, base loss: 14406.44
[INFO 2017-06-28 15:11:43,066 main.py:51] epoch 10820, training loss: 9413.54, average training loss: 9323.13, base loss: 14406.17
[INFO 2017-06-28 15:11:44,035 main.py:51] epoch 10821, training loss: 8758.55, average training loss: 9322.86, base loss: 14406.58
[INFO 2017-06-28 15:11:45,147 main.py:51] epoch 10822, training loss: 9016.56, average training loss: 9322.78, base loss: 14405.92
[INFO 2017-06-28 15:11:46,172 main.py:51] epoch 10823, training loss: 9433.96, average training loss: 9323.61, base loss: 14407.56
[INFO 2017-06-28 15:11:47,218 main.py:51] epoch 10824, training loss: 8783.98, average training loss: 9321.50, base loss: 14405.20
[INFO 2017-06-28 15:11:48,246 main.py:51] epoch 10825, training loss: 9980.45, average training loss: 9324.06, base loss: 14410.45
[INFO 2017-06-28 15:11:49,226 main.py:51] epoch 10826, training loss: 8618.88, average training loss: 9323.76, base loss: 14410.04
[INFO 2017-06-28 15:11:50,313 main.py:51] epoch 10827, training loss: 8429.21, average training loss: 9321.85, base loss: 14407.27
[INFO 2017-06-28 15:11:51,314 main.py:51] epoch 10828, training loss: 9175.61, average training loss: 9322.32, base loss: 14408.74
[INFO 2017-06-28 15:11:52,358 main.py:51] epoch 10829, training loss: 8783.67, average training loss: 9321.63, base loss: 14408.21
[INFO 2017-06-28 15:11:53,406 main.py:51] epoch 10830, training loss: 9332.18, average training loss: 9322.33, base loss: 14409.97
[INFO 2017-06-28 15:11:54,419 main.py:51] epoch 10831, training loss: 9275.11, average training loss: 9323.23, base loss: 14412.65
[INFO 2017-06-28 15:11:55,490 main.py:51] epoch 10832, training loss: 9067.46, average training loss: 9323.04, base loss: 14411.34
[INFO 2017-06-28 15:11:56,538 main.py:51] epoch 10833, training loss: 8051.12, average training loss: 9322.99, base loss: 14411.74
[INFO 2017-06-28 15:11:57,513 main.py:51] epoch 10834, training loss: 8914.02, average training loss: 9322.53, base loss: 14411.22
[INFO 2017-06-28 15:11:58,581 main.py:51] epoch 10835, training loss: 9530.51, average training loss: 9322.11, base loss: 14410.96
[INFO 2017-06-28 15:11:59,636 main.py:51] epoch 10836, training loss: 8330.14, average training loss: 9320.46, base loss: 14408.15
[INFO 2017-06-28 15:12:00,646 main.py:51] epoch 10837, training loss: 9300.50, average training loss: 9319.77, base loss: 14406.10
[INFO 2017-06-28 15:12:01,699 main.py:51] epoch 10838, training loss: 9013.70, average training loss: 9320.40, base loss: 14408.25
[INFO 2017-06-28 15:12:02,744 main.py:51] epoch 10839, training loss: 8625.54, average training loss: 9320.97, base loss: 14409.05
[INFO 2017-06-28 15:12:03,821 main.py:51] epoch 10840, training loss: 9408.98, average training loss: 9321.61, base loss: 14410.50
[INFO 2017-06-28 15:12:04,934 main.py:51] epoch 10841, training loss: 9601.60, average training loss: 9322.81, base loss: 14411.88
[INFO 2017-06-28 15:12:05,987 main.py:51] epoch 10842, training loss: 8491.38, average training loss: 9320.27, base loss: 14408.57
[INFO 2017-06-28 15:12:07,046 main.py:51] epoch 10843, training loss: 10483.85, average training loss: 9321.09, base loss: 14409.94
[INFO 2017-06-28 15:12:08,090 main.py:51] epoch 10844, training loss: 10404.04, average training loss: 9321.84, base loss: 14409.08
[INFO 2017-06-28 15:12:09,149 main.py:51] epoch 10845, training loss: 10087.29, average training loss: 9322.45, base loss: 14410.28
[INFO 2017-06-28 15:12:10,200 main.py:51] epoch 10846, training loss: 9796.91, average training loss: 9323.16, base loss: 14409.23
[INFO 2017-06-28 15:12:11,220 main.py:51] epoch 10847, training loss: 8136.03, average training loss: 9321.59, base loss: 14405.15
[INFO 2017-06-28 15:12:12,276 main.py:51] epoch 10848, training loss: 9468.56, average training loss: 9322.07, base loss: 14405.35
[INFO 2017-06-28 15:12:13,386 main.py:51] epoch 10849, training loss: 9280.91, average training loss: 9321.14, base loss: 14405.92
[INFO 2017-06-28 15:12:14,371 main.py:51] epoch 10850, training loss: 9740.57, average training loss: 9322.24, base loss: 14406.88
[INFO 2017-06-28 15:12:15,469 main.py:51] epoch 10851, training loss: 9907.26, average training loss: 9322.21, base loss: 14404.99
[INFO 2017-06-28 15:12:16,578 main.py:51] epoch 10852, training loss: 9350.86, average training loss: 9320.61, base loss: 14403.98
[INFO 2017-06-28 15:12:17,574 main.py:51] epoch 10853, training loss: 8761.25, average training loss: 9320.57, base loss: 14405.25
[INFO 2017-06-28 15:12:18,648 main.py:51] epoch 10854, training loss: 8323.35, average training loss: 9319.58, base loss: 14403.34
[INFO 2017-06-28 15:12:19,687 main.py:51] epoch 10855, training loss: 8746.58, average training loss: 9320.16, base loss: 14405.19
[INFO 2017-06-28 15:12:20,747 main.py:51] epoch 10856, training loss: 9479.34, average training loss: 9318.32, base loss: 14403.60
[INFO 2017-06-28 15:12:21,853 main.py:51] epoch 10857, training loss: 10223.80, average training loss: 9319.07, base loss: 14405.15
[INFO 2017-06-28 15:12:22,835 main.py:51] epoch 10858, training loss: 8259.48, average training loss: 9316.83, base loss: 14401.28
[INFO 2017-06-28 15:12:23,840 main.py:51] epoch 10859, training loss: 10157.61, average training loss: 9317.73, base loss: 14402.47
[INFO 2017-06-28 15:12:24,867 main.py:51] epoch 10860, training loss: 9402.13, average training loss: 9318.01, base loss: 14403.21
[INFO 2017-06-28 15:12:25,936 main.py:51] epoch 10861, training loss: 9164.62, average training loss: 9317.57, base loss: 14400.86
[INFO 2017-06-28 15:12:27,017 main.py:51] epoch 10862, training loss: 10039.03, average training loss: 9318.83, base loss: 14403.16
[INFO 2017-06-28 15:12:28,121 main.py:51] epoch 10863, training loss: 10526.12, average training loss: 9319.38, base loss: 14403.88
[INFO 2017-06-28 15:12:29,151 main.py:51] epoch 10864, training loss: 7959.60, average training loss: 9317.94, base loss: 14401.66
[INFO 2017-06-28 15:12:30,296 main.py:51] epoch 10865, training loss: 9043.15, average training loss: 9317.10, base loss: 14401.55
[INFO 2017-06-28 15:12:31,294 main.py:51] epoch 10866, training loss: 8097.91, average training loss: 9316.61, base loss: 14400.71
[INFO 2017-06-28 15:12:32,375 main.py:51] epoch 10867, training loss: 8708.14, average training loss: 9315.14, base loss: 14399.22
[INFO 2017-06-28 15:12:33,457 main.py:51] epoch 10868, training loss: 9030.48, average training loss: 9315.39, base loss: 14400.41
[INFO 2017-06-28 15:12:34,473 main.py:51] epoch 10869, training loss: 9179.45, average training loss: 9313.50, base loss: 14396.16
[INFO 2017-06-28 15:12:35,444 main.py:51] epoch 10870, training loss: 8720.66, average training loss: 9313.74, base loss: 14396.10
[INFO 2017-06-28 15:12:36,481 main.py:51] epoch 10871, training loss: 7456.41, average training loss: 9311.47, base loss: 14393.62
[INFO 2017-06-28 15:12:37,520 main.py:51] epoch 10872, training loss: 8817.70, average training loss: 9311.32, base loss: 14394.04
[INFO 2017-06-28 15:12:38,599 main.py:51] epoch 10873, training loss: 9204.53, average training loss: 9310.46, base loss: 14393.67
[INFO 2017-06-28 15:12:39,656 main.py:51] epoch 10874, training loss: 9531.67, average training loss: 9310.73, base loss: 14395.16
[INFO 2017-06-28 15:12:40,699 main.py:51] epoch 10875, training loss: 8905.22, average training loss: 9309.49, base loss: 14393.93
[INFO 2017-06-28 15:12:41,789 main.py:51] epoch 10876, training loss: 8224.83, average training loss: 9307.44, base loss: 14390.68
[INFO 2017-06-28 15:12:42,895 main.py:51] epoch 10877, training loss: 8897.67, average training loss: 9307.02, base loss: 14392.30
[INFO 2017-06-28 15:12:43,906 main.py:51] epoch 10878, training loss: 9767.52, average training loss: 9307.91, base loss: 14393.89
[INFO 2017-06-28 15:12:44,969 main.py:51] epoch 10879, training loss: 9211.11, average training loss: 9307.39, base loss: 14393.36
[INFO 2017-06-28 15:12:46,063 main.py:51] epoch 10880, training loss: 7984.71, average training loss: 9306.40, base loss: 14391.34
[INFO 2017-06-28 15:12:47,076 main.py:51] epoch 10881, training loss: 8427.71, average training loss: 9306.59, base loss: 14393.38
[INFO 2017-06-28 15:12:48,111 main.py:51] epoch 10882, training loss: 8615.56, average training loss: 9304.67, base loss: 14389.50
[INFO 2017-06-28 15:12:49,174 main.py:51] epoch 10883, training loss: 8807.69, average training loss: 9303.51, base loss: 14387.71
[INFO 2017-06-28 15:12:50,225 main.py:51] epoch 10884, training loss: 8871.24, average training loss: 9303.58, base loss: 14389.60
[INFO 2017-06-28 15:12:51,306 main.py:51] epoch 10885, training loss: 10355.67, average training loss: 9303.06, base loss: 14388.47
[INFO 2017-06-28 15:12:52,305 main.py:51] epoch 10886, training loss: 9999.69, average training loss: 9302.34, base loss: 14388.21
[INFO 2017-06-28 15:12:53,389 main.py:51] epoch 10887, training loss: 9063.13, average training loss: 9300.68, base loss: 14385.50
[INFO 2017-06-28 15:12:54,462 main.py:51] epoch 10888, training loss: 9474.39, average training loss: 9301.48, base loss: 14387.25
[INFO 2017-06-28 15:12:55,479 main.py:51] epoch 10889, training loss: 9917.56, average training loss: 9303.03, base loss: 14388.73
[INFO 2017-06-28 15:12:56,535 main.py:51] epoch 10890, training loss: 10286.19, average training loss: 9304.18, base loss: 14392.48
[INFO 2017-06-28 15:12:57,646 main.py:51] epoch 10891, training loss: 10506.23, average training loss: 9305.45, base loss: 14391.91
[INFO 2017-06-28 15:12:58,740 main.py:51] epoch 10892, training loss: 7570.93, average training loss: 9302.31, base loss: 14387.30
[INFO 2017-06-28 15:12:59,881 main.py:51] epoch 10893, training loss: 10334.84, average training loss: 9303.64, base loss: 14391.83
[INFO 2017-06-28 15:13:00,920 main.py:51] epoch 10894, training loss: 9599.91, average training loss: 9304.23, base loss: 14393.81
[INFO 2017-06-28 15:13:01,848 main.py:51] epoch 10895, training loss: 8498.85, average training loss: 9303.21, base loss: 14393.11
[INFO 2017-06-28 15:13:02,819 main.py:51] epoch 10896, training loss: 8956.19, average training loss: 9303.18, base loss: 14394.17
[INFO 2017-06-28 15:13:03,751 main.py:51] epoch 10897, training loss: 10016.03, average training loss: 9304.53, base loss: 14396.59
[INFO 2017-06-28 15:13:04,797 main.py:51] epoch 10898, training loss: 8919.36, average training loss: 9304.29, base loss: 14396.55
[INFO 2017-06-28 15:13:05,844 main.py:51] epoch 10899, training loss: 10525.40, average training loss: 9306.65, base loss: 14399.95
[INFO 2017-06-28 15:13:05,844 main.py:53] epoch 10899, testing
[INFO 2017-06-28 15:13:09,588 main.py:105] average testing loss: 10392.39, base loss: 14877.09
[INFO 2017-06-28 15:13:09,588 main.py:106] improve_loss: 4484.69, improve_percent: 0.30
[INFO 2017-06-28 15:13:09,589 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:13:10,680 main.py:51] epoch 10900, training loss: 7970.85, average training loss: 9304.91, base loss: 14397.15
[INFO 2017-06-28 15:13:11,755 main.py:51] epoch 10901, training loss: 8060.25, average training loss: 9304.38, base loss: 14395.05
[INFO 2017-06-28 15:13:12,766 main.py:51] epoch 10902, training loss: 10096.13, average training loss: 9305.74, base loss: 14396.83
[INFO 2017-06-28 15:13:13,825 main.py:51] epoch 10903, training loss: 9868.08, average training loss: 9305.20, base loss: 14397.28
[INFO 2017-06-28 15:13:14,880 main.py:51] epoch 10904, training loss: 9286.66, average training loss: 9304.59, base loss: 14396.57
[INFO 2017-06-28 15:13:15,947 main.py:51] epoch 10905, training loss: 10064.07, average training loss: 9303.08, base loss: 14393.40
[INFO 2017-06-28 15:13:17,063 main.py:51] epoch 10906, training loss: 10322.92, average training loss: 9303.23, base loss: 14394.14
[INFO 2017-06-28 15:13:18,118 main.py:51] epoch 10907, training loss: 9432.00, average training loss: 9304.79, base loss: 14395.82
[INFO 2017-06-28 15:13:19,249 main.py:51] epoch 10908, training loss: 8844.03, average training loss: 9303.67, base loss: 14393.58
[INFO 2017-06-28 15:13:20,291 main.py:51] epoch 10909, training loss: 8704.13, average training loss: 9304.06, base loss: 14395.74
[INFO 2017-06-28 15:13:21,313 main.py:51] epoch 10910, training loss: 8449.81, average training loss: 9302.81, base loss: 14392.46
[INFO 2017-06-28 15:13:22,387 main.py:51] epoch 10911, training loss: 10489.50, average training loss: 9304.78, base loss: 14396.07
[INFO 2017-06-28 15:13:23,420 main.py:51] epoch 10912, training loss: 8810.66, average training loss: 9304.04, base loss: 14394.14
[INFO 2017-06-28 15:13:24,450 main.py:51] epoch 10913, training loss: 10644.21, average training loss: 9303.93, base loss: 14395.08
[INFO 2017-06-28 15:13:25,484 main.py:51] epoch 10914, training loss: 9919.37, average training loss: 9304.54, base loss: 14396.05
[INFO 2017-06-28 15:13:26,561 main.py:51] epoch 10915, training loss: 8784.16, average training loss: 9303.41, base loss: 14394.35
[INFO 2017-06-28 15:13:27,642 main.py:51] epoch 10916, training loss: 9989.78, average training loss: 9304.39, base loss: 14396.57
[INFO 2017-06-28 15:13:28,718 main.py:51] epoch 10917, training loss: 9376.46, average training loss: 9304.27, base loss: 14397.92
[INFO 2017-06-28 15:13:29,747 main.py:51] epoch 10918, training loss: 8392.47, average training loss: 9302.69, base loss: 14396.07
[INFO 2017-06-28 15:13:30,791 main.py:51] epoch 10919, training loss: 10275.81, average training loss: 9303.21, base loss: 14396.16
[INFO 2017-06-28 15:13:31,829 main.py:51] epoch 10920, training loss: 9747.29, average training loss: 9302.85, base loss: 14395.75
[INFO 2017-06-28 15:13:32,935 main.py:51] epoch 10921, training loss: 8340.91, average training loss: 9301.25, base loss: 14391.84
[INFO 2017-06-28 15:13:34,036 main.py:51] epoch 10922, training loss: 8971.84, average training loss: 9301.93, base loss: 14391.63
[INFO 2017-06-28 15:13:35,135 main.py:51] epoch 10923, training loss: 8820.53, average training loss: 9298.11, base loss: 14388.27
[INFO 2017-06-28 15:13:36,183 main.py:51] epoch 10924, training loss: 9743.96, average training loss: 9297.06, base loss: 14385.18
[INFO 2017-06-28 15:13:37,253 main.py:51] epoch 10925, training loss: 9361.76, average training loss: 9296.60, base loss: 14384.33
[INFO 2017-06-28 15:13:38,301 main.py:51] epoch 10926, training loss: 10152.56, average training loss: 9298.26, base loss: 14387.81
[INFO 2017-06-28 15:13:39,371 main.py:51] epoch 10927, training loss: 10704.68, average training loss: 9301.08, base loss: 14390.21
[INFO 2017-06-28 15:13:40,500 main.py:51] epoch 10928, training loss: 8377.95, average training loss: 9300.38, base loss: 14388.44
[INFO 2017-06-28 15:13:41,567 main.py:51] epoch 10929, training loss: 9016.22, average training loss: 9300.83, base loss: 14389.54
[INFO 2017-06-28 15:13:42,609 main.py:51] epoch 10930, training loss: 8387.26, average training loss: 9299.05, base loss: 14386.68
[INFO 2017-06-28 15:13:43,653 main.py:51] epoch 10931, training loss: 9659.01, average training loss: 9299.95, base loss: 14387.46
[INFO 2017-06-28 15:13:44,716 main.py:51] epoch 10932, training loss: 8235.77, average training loss: 9299.64, base loss: 14387.70
[INFO 2017-06-28 15:13:45,783 main.py:51] epoch 10933, training loss: 9192.71, average training loss: 9299.13, base loss: 14389.03
[INFO 2017-06-28 15:13:46,770 main.py:51] epoch 10934, training loss: 9942.18, average training loss: 9300.27, base loss: 14391.76
[INFO 2017-06-28 15:13:47,845 main.py:51] epoch 10935, training loss: 8732.62, average training loss: 9298.76, base loss: 14388.27
[INFO 2017-06-28 15:13:48,883 main.py:51] epoch 10936, training loss: 8985.12, average training loss: 9298.07, base loss: 14387.08
[INFO 2017-06-28 15:13:49,888 main.py:51] epoch 10937, training loss: 9299.46, average training loss: 9299.11, base loss: 14388.34
[INFO 2017-06-28 15:13:50,958 main.py:51] epoch 10938, training loss: 8752.12, average training loss: 9299.14, base loss: 14388.68
[INFO 2017-06-28 15:13:52,057 main.py:51] epoch 10939, training loss: 9687.47, average training loss: 9299.74, base loss: 14389.45
[INFO 2017-06-28 15:13:53,072 main.py:51] epoch 10940, training loss: 8343.01, average training loss: 9300.06, base loss: 14388.81
[INFO 2017-06-28 15:13:54,174 main.py:51] epoch 10941, training loss: 9834.00, average training loss: 9300.25, base loss: 14391.27
[INFO 2017-06-28 15:13:55,220 main.py:51] epoch 10942, training loss: 8878.39, average training loss: 9300.03, base loss: 14390.91
[INFO 2017-06-28 15:13:56,255 main.py:51] epoch 10943, training loss: 9741.71, average training loss: 9300.74, base loss: 14392.14
[INFO 2017-06-28 15:13:57,315 main.py:51] epoch 10944, training loss: 10868.51, average training loss: 9302.44, base loss: 14394.02
[INFO 2017-06-28 15:13:58,379 main.py:51] epoch 10945, training loss: 8897.99, average training loss: 9301.92, base loss: 14392.99
[INFO 2017-06-28 15:13:59,485 main.py:51] epoch 10946, training loss: 8362.94, average training loss: 9301.56, base loss: 14392.48
[INFO 2017-06-28 15:14:00,558 main.py:51] epoch 10947, training loss: 9922.11, average training loss: 9302.88, base loss: 14394.83
[INFO 2017-06-28 15:14:01,615 main.py:51] epoch 10948, training loss: 9180.00, average training loss: 9302.72, base loss: 14394.14
[INFO 2017-06-28 15:14:02,642 main.py:51] epoch 10949, training loss: 8998.60, average training loss: 9303.20, base loss: 14394.07
[INFO 2017-06-28 15:14:03,663 main.py:51] epoch 10950, training loss: 9318.07, average training loss: 9301.64, base loss: 14392.01
[INFO 2017-06-28 15:14:04,758 main.py:51] epoch 10951, training loss: 9583.07, average training loss: 9301.33, base loss: 14392.05
[INFO 2017-06-28 15:14:05,796 main.py:51] epoch 10952, training loss: 9543.02, average training loss: 9302.23, base loss: 14394.09
[INFO 2017-06-28 15:14:06,835 main.py:51] epoch 10953, training loss: 9141.75, average training loss: 9301.85, base loss: 14393.30
[INFO 2017-06-28 15:14:07,896 main.py:51] epoch 10954, training loss: 9985.11, average training loss: 9301.49, base loss: 14392.25
[INFO 2017-06-28 15:14:08,973 main.py:51] epoch 10955, training loss: 8218.91, average training loss: 9301.17, base loss: 14390.79
[INFO 2017-06-28 15:14:10,001 main.py:51] epoch 10956, training loss: 10043.56, average training loss: 9300.08, base loss: 14388.97
[INFO 2017-06-28 15:14:11,047 main.py:51] epoch 10957, training loss: 9115.76, average training loss: 9300.09, base loss: 14390.04
[INFO 2017-06-28 15:14:12,114 main.py:51] epoch 10958, training loss: 9487.42, average training loss: 9300.20, base loss: 14390.54
[INFO 2017-06-28 15:14:13,211 main.py:51] epoch 10959, training loss: 11192.63, average training loss: 9302.22, base loss: 14393.93
[INFO 2017-06-28 15:14:14,205 main.py:51] epoch 10960, training loss: 9791.19, average training loss: 9303.44, base loss: 14396.92
[INFO 2017-06-28 15:14:15,286 main.py:51] epoch 10961, training loss: 8920.19, average training loss: 9303.19, base loss: 14396.41
[INFO 2017-06-28 15:14:16,374 main.py:51] epoch 10962, training loss: 8603.80, average training loss: 9302.92, base loss: 14396.88
[INFO 2017-06-28 15:14:17,359 main.py:51] epoch 10963, training loss: 9753.51, average training loss: 9301.38, base loss: 14395.90
[INFO 2017-06-28 15:14:18,422 main.py:51] epoch 10964, training loss: 9205.33, average training loss: 9301.85, base loss: 14395.85
[INFO 2017-06-28 15:14:19,537 main.py:51] epoch 10965, training loss: 10064.61, average training loss: 9302.37, base loss: 14397.05
[INFO 2017-06-28 15:14:20,509 main.py:51] epoch 10966, training loss: 8977.19, average training loss: 9303.69, base loss: 14397.59
[INFO 2017-06-28 15:14:21,440 main.py:51] epoch 10967, training loss: 8143.95, average training loss: 9303.57, base loss: 14397.37
[INFO 2017-06-28 15:14:22,398 main.py:51] epoch 10968, training loss: 9909.32, average training loss: 9303.73, base loss: 14396.63
[INFO 2017-06-28 15:14:23,424 main.py:51] epoch 10969, training loss: 10194.48, average training loss: 9304.90, base loss: 14397.49
[INFO 2017-06-28 15:14:24,492 main.py:51] epoch 10970, training loss: 7819.76, average training loss: 9303.91, base loss: 14396.50
[INFO 2017-06-28 15:14:25,585 main.py:51] epoch 10971, training loss: 8891.89, average training loss: 9303.85, base loss: 14398.00
[INFO 2017-06-28 15:14:26,650 main.py:51] epoch 10972, training loss: 9848.28, average training loss: 9305.36, base loss: 14401.67
[INFO 2017-06-28 15:14:27,682 main.py:51] epoch 10973, training loss: 8296.44, average training loss: 9304.50, base loss: 14400.83
[INFO 2017-06-28 15:14:28,714 main.py:51] epoch 10974, training loss: 8756.19, average training loss: 9303.46, base loss: 14400.03
[INFO 2017-06-28 15:14:29,802 main.py:51] epoch 10975, training loss: 9450.00, average training loss: 9304.39, base loss: 14400.92
[INFO 2017-06-28 15:14:30,909 main.py:51] epoch 10976, training loss: 9535.95, average training loss: 9303.48, base loss: 14399.26
[INFO 2017-06-28 15:14:31,940 main.py:51] epoch 10977, training loss: 10899.45, average training loss: 9303.85, base loss: 14400.04
[INFO 2017-06-28 15:14:33,005 main.py:51] epoch 10978, training loss: 12470.10, average training loss: 9307.46, base loss: 14405.74
[INFO 2017-06-28 15:14:34,064 main.py:51] epoch 10979, training loss: 9601.22, average training loss: 9306.90, base loss: 14405.22
[INFO 2017-06-28 15:14:35,133 main.py:51] epoch 10980, training loss: 8588.39, average training loss: 9306.92, base loss: 14405.62
[INFO 2017-06-28 15:14:36,198 main.py:51] epoch 10981, training loss: 8836.52, average training loss: 9307.13, base loss: 14406.52
[INFO 2017-06-28 15:14:37,288 main.py:51] epoch 10982, training loss: 7739.80, average training loss: 9305.68, base loss: 14403.13
[INFO 2017-06-28 15:14:38,396 main.py:51] epoch 10983, training loss: 9189.82, average training loss: 9306.10, base loss: 14403.35
[INFO 2017-06-28 15:14:39,462 main.py:51] epoch 10984, training loss: 9487.79, average training loss: 9305.07, base loss: 14401.59
[INFO 2017-06-28 15:14:40,526 main.py:51] epoch 10985, training loss: 8499.99, average training loss: 9305.83, base loss: 14402.90
[INFO 2017-06-28 15:14:41,568 main.py:51] epoch 10986, training loss: 9432.67, average training loss: 9306.97, base loss: 14405.73
[INFO 2017-06-28 15:14:42,662 main.py:51] epoch 10987, training loss: 9523.40, average training loss: 9306.24, base loss: 14404.43
[INFO 2017-06-28 15:14:43,706 main.py:51] epoch 10988, training loss: 8377.21, average training loss: 9304.84, base loss: 14403.12
[INFO 2017-06-28 15:14:44,769 main.py:51] epoch 10989, training loss: 9027.89, average training loss: 9305.62, base loss: 14405.34
[INFO 2017-06-28 15:14:45,887 main.py:51] epoch 10990, training loss: 9401.46, average training loss: 9306.35, base loss: 14407.52
[INFO 2017-06-28 15:14:46,969 main.py:51] epoch 10991, training loss: 10170.80, average training loss: 9307.06, base loss: 14410.60
[INFO 2017-06-28 15:14:48,089 main.py:51] epoch 10992, training loss: 10577.12, average training loss: 9307.55, base loss: 14412.87
[INFO 2017-06-28 15:14:49,104 main.py:51] epoch 10993, training loss: 8385.59, average training loss: 9306.75, base loss: 14410.93
[INFO 2017-06-28 15:14:50,171 main.py:51] epoch 10994, training loss: 8358.90, average training loss: 9306.41, base loss: 14410.29
[INFO 2017-06-28 15:14:51,268 main.py:51] epoch 10995, training loss: 7976.68, average training loss: 9305.49, base loss: 14408.02
[INFO 2017-06-28 15:14:52,322 main.py:51] epoch 10996, training loss: 8827.32, average training loss: 9305.24, base loss: 14406.81
[INFO 2017-06-28 15:14:53,383 main.py:51] epoch 10997, training loss: 9836.66, average training loss: 9307.27, base loss: 14411.17
[INFO 2017-06-28 15:14:54,389 main.py:51] epoch 10998, training loss: 9534.70, average training loss: 9306.54, base loss: 14408.97
[INFO 2017-06-28 15:14:55,465 main.py:51] epoch 10999, training loss: 9189.41, average training loss: 9306.37, base loss: 14408.29
[INFO 2017-06-28 15:14:55,465 main.py:53] epoch 10999, testing
[INFO 2017-06-28 15:14:59,224 main.py:105] average testing loss: 11359.52, base loss: 16045.11
[INFO 2017-06-28 15:14:59,225 main.py:106] improve_loss: 4685.59, improve_percent: 0.29
[INFO 2017-06-28 15:14:59,225 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:15:00,269 main.py:51] epoch 11000, training loss: 8760.38, average training loss: 9306.37, base loss: 14408.74
[INFO 2017-06-28 15:15:01,364 main.py:51] epoch 11001, training loss: 8212.30, average training loss: 9304.06, base loss: 14406.91
[INFO 2017-06-28 15:15:02,360 main.py:51] epoch 11002, training loss: 8864.29, average training loss: 9304.56, base loss: 14408.60
[INFO 2017-06-28 15:15:03,433 main.py:51] epoch 11003, training loss: 9296.74, average training loss: 9304.44, base loss: 14409.02
[INFO 2017-06-28 15:15:04,493 main.py:51] epoch 11004, training loss: 9057.91, average training loss: 9303.54, base loss: 14407.26
[INFO 2017-06-28 15:15:05,563 main.py:51] epoch 11005, training loss: 8799.92, average training loss: 9303.39, base loss: 14407.00
[INFO 2017-06-28 15:15:06,576 main.py:51] epoch 11006, training loss: 7923.57, average training loss: 9303.28, base loss: 14407.00
[INFO 2017-06-28 15:15:07,620 main.py:51] epoch 11007, training loss: 9019.52, average training loss: 9303.35, base loss: 14407.91
[INFO 2017-06-28 15:15:08,693 main.py:51] epoch 11008, training loss: 9321.27, average training loss: 9303.79, base loss: 14408.56
[INFO 2017-06-28 15:15:09,761 main.py:51] epoch 11009, training loss: 9638.42, average training loss: 9303.40, base loss: 14408.46
[INFO 2017-06-28 15:15:10,820 main.py:51] epoch 11010, training loss: 7939.05, average training loss: 9302.20, base loss: 14406.34
[INFO 2017-06-28 15:15:11,893 main.py:51] epoch 11011, training loss: 8971.09, average training loss: 9301.65, base loss: 14405.00
[INFO 2017-06-28 15:15:12,873 main.py:51] epoch 11012, training loss: 8979.66, average training loss: 9301.44, base loss: 14404.79
[INFO 2017-06-28 15:15:13,948 main.py:51] epoch 11013, training loss: 9528.08, average training loss: 9302.40, base loss: 14407.49
[INFO 2017-06-28 15:15:15,065 main.py:51] epoch 11014, training loss: 8384.22, average training loss: 9301.50, base loss: 14404.96
[INFO 2017-06-28 15:15:16,162 main.py:51] epoch 11015, training loss: 9312.15, average training loss: 9301.79, base loss: 14405.35
[INFO 2017-06-28 15:15:17,264 main.py:51] epoch 11016, training loss: 8214.10, average training loss: 9299.26, base loss: 14402.64
[INFO 2017-06-28 15:15:18,289 main.py:51] epoch 11017, training loss: 7864.18, average training loss: 9296.37, base loss: 14397.92
[INFO 2017-06-28 15:15:19,326 main.py:51] epoch 11018, training loss: 8701.78, average training loss: 9295.90, base loss: 14396.40
[INFO 2017-06-28 15:15:20,353 main.py:51] epoch 11019, training loss: 11482.51, average training loss: 9297.63, base loss: 14399.92
[INFO 2017-06-28 15:15:21,433 main.py:51] epoch 11020, training loss: 9648.30, average training loss: 9297.83, base loss: 14398.60
[INFO 2017-06-28 15:15:22,546 main.py:51] epoch 11021, training loss: 7932.21, average training loss: 9294.56, base loss: 14394.61
[INFO 2017-06-28 15:15:23,560 main.py:51] epoch 11022, training loss: 8977.33, average training loss: 9294.11, base loss: 14393.22
[INFO 2017-06-28 15:15:24,599 main.py:51] epoch 11023, training loss: 10511.12, average training loss: 9295.50, base loss: 14394.42
[INFO 2017-06-28 15:15:25,709 main.py:51] epoch 11024, training loss: 7507.92, average training loss: 9293.33, base loss: 14390.33
[INFO 2017-06-28 15:15:26,690 main.py:51] epoch 11025, training loss: 8673.20, average training loss: 9293.87, base loss: 14391.37
[INFO 2017-06-28 15:15:27,751 main.py:51] epoch 11026, training loss: 8494.70, average training loss: 9292.82, base loss: 14389.87
[INFO 2017-06-28 15:15:28,799 main.py:51] epoch 11027, training loss: 9163.87, average training loss: 9291.96, base loss: 14387.91
[INFO 2017-06-28 15:15:29,805 main.py:51] epoch 11028, training loss: 9413.45, average training loss: 9292.55, base loss: 14389.05
[INFO 2017-06-28 15:15:30,797 main.py:51] epoch 11029, training loss: 8533.80, average training loss: 9292.16, base loss: 14388.18
[INFO 2017-06-28 15:15:31,877 main.py:51] epoch 11030, training loss: 10391.63, average training loss: 9293.19, base loss: 14391.75
[INFO 2017-06-28 15:15:32,912 main.py:51] epoch 11031, training loss: 8315.70, average training loss: 9291.61, base loss: 14389.10
[INFO 2017-06-28 15:15:33,975 main.py:51] epoch 11032, training loss: 8625.61, average training loss: 9290.69, base loss: 14388.15
[INFO 2017-06-28 15:15:35,022 main.py:51] epoch 11033, training loss: 8919.84, average training loss: 9290.10, base loss: 14387.22
[INFO 2017-06-28 15:15:36,102 main.py:51] epoch 11034, training loss: 8394.99, average training loss: 9287.88, base loss: 14384.60
[INFO 2017-06-28 15:15:37,226 main.py:51] epoch 11035, training loss: 8609.09, average training loss: 9287.68, base loss: 14383.55
[INFO 2017-06-28 15:15:38,198 main.py:51] epoch 11036, training loss: 9381.82, average training loss: 9287.41, base loss: 14382.86
[INFO 2017-06-28 15:15:39,091 main.py:51] epoch 11037, training loss: 9370.46, average training loss: 9288.84, base loss: 14386.38
[INFO 2017-06-28 15:15:40,096 main.py:51] epoch 11038, training loss: 10206.58, average training loss: 9288.36, base loss: 14386.05
[INFO 2017-06-28 15:15:41,153 main.py:51] epoch 11039, training loss: 9567.54, average training loss: 9289.95, base loss: 14390.61
[INFO 2017-06-28 15:15:42,240 main.py:51] epoch 11040, training loss: 8412.92, average training loss: 9289.21, base loss: 14389.43
[INFO 2017-06-28 15:15:43,284 main.py:51] epoch 11041, training loss: 11536.10, average training loss: 9291.73, base loss: 14394.18
[INFO 2017-06-28 15:15:44,320 main.py:51] epoch 11042, training loss: 8909.05, average training loss: 9291.52, base loss: 14394.57
[INFO 2017-06-28 15:15:45,385 main.py:51] epoch 11043, training loss: 9450.89, average training loss: 9291.45, base loss: 14395.25
[INFO 2017-06-28 15:15:46,479 main.py:51] epoch 11044, training loss: 8464.10, average training loss: 9290.75, base loss: 14394.74
[INFO 2017-06-28 15:15:47,540 main.py:51] epoch 11045, training loss: 8237.84, average training loss: 9289.94, base loss: 14392.37
[INFO 2017-06-28 15:15:48,572 main.py:51] epoch 11046, training loss: 9790.70, average training loss: 9289.41, base loss: 14391.90
[INFO 2017-06-28 15:15:49,644 main.py:51] epoch 11047, training loss: 11115.23, average training loss: 9290.39, base loss: 14392.30
[INFO 2017-06-28 15:15:50,749 main.py:51] epoch 11048, training loss: 7750.99, average training loss: 9289.48, base loss: 14391.19
[INFO 2017-06-28 15:15:51,849 main.py:51] epoch 11049, training loss: 8529.84, average training loss: 9288.95, base loss: 14390.39
[INFO 2017-06-28 15:15:52,971 main.py:51] epoch 11050, training loss: 9780.10, average training loss: 9288.85, base loss: 14390.87
[INFO 2017-06-28 15:15:54,017 main.py:51] epoch 11051, training loss: 9251.47, average training loss: 9289.12, base loss: 14391.91
[INFO 2017-06-28 15:15:55,037 main.py:51] epoch 11052, training loss: 9349.13, average training loss: 9290.39, base loss: 14394.62
[INFO 2017-06-28 15:15:56,090 main.py:51] epoch 11053, training loss: 7946.36, average training loss: 9289.05, base loss: 14391.02
[INFO 2017-06-28 15:15:57,120 main.py:51] epoch 11054, training loss: 9118.68, average training loss: 9288.56, base loss: 14389.29
[INFO 2017-06-28 15:15:58,201 main.py:51] epoch 11055, training loss: 8751.40, average training loss: 9287.24, base loss: 14389.04
[INFO 2017-06-28 15:15:59,230 main.py:51] epoch 11056, training loss: 8425.51, average training loss: 9288.21, base loss: 14390.60
[INFO 2017-06-28 15:16:00,276 main.py:51] epoch 11057, training loss: 10322.42, average training loss: 9286.73, base loss: 14386.52
[INFO 2017-06-28 15:16:01,412 main.py:51] epoch 11058, training loss: 8700.31, average training loss: 9286.79, base loss: 14385.14
[INFO 2017-06-28 15:16:02,399 main.py:51] epoch 11059, training loss: 9721.69, average training loss: 9288.13, base loss: 14387.59
[INFO 2017-06-28 15:16:03,446 main.py:51] epoch 11060, training loss: 9502.33, average training loss: 9286.75, base loss: 14384.34
[INFO 2017-06-28 15:16:04,505 main.py:51] epoch 11061, training loss: 9634.62, average training loss: 9287.82, base loss: 14387.01
[INFO 2017-06-28 15:16:05,559 main.py:51] epoch 11062, training loss: 9871.65, average training loss: 9289.24, base loss: 14389.88
[INFO 2017-06-28 15:16:06,587 main.py:51] epoch 11063, training loss: 10141.30, average training loss: 9290.79, base loss: 14392.45
[INFO 2017-06-28 15:16:07,654 main.py:51] epoch 11064, training loss: 8537.12, average training loss: 9289.27, base loss: 14389.43
[INFO 2017-06-28 15:16:08,710 main.py:51] epoch 11065, training loss: 8957.84, average training loss: 9288.48, base loss: 14390.71
[INFO 2017-06-28 15:16:09,813 main.py:51] epoch 11066, training loss: 8671.71, average training loss: 9288.25, base loss: 14389.49
[INFO 2017-06-28 15:16:10,834 main.py:51] epoch 11067, training loss: 9299.16, average training loss: 9289.20, base loss: 14391.01
[INFO 2017-06-28 15:16:11,897 main.py:51] epoch 11068, training loss: 11129.00, average training loss: 9290.51, base loss: 14393.49
[INFO 2017-06-28 15:16:12,931 main.py:51] epoch 11069, training loss: 9100.37, average training loss: 9290.54, base loss: 14394.46
[INFO 2017-06-28 15:16:14,020 main.py:51] epoch 11070, training loss: 9230.77, average training loss: 9290.15, base loss: 14394.54
[INFO 2017-06-28 15:16:15,163 main.py:51] epoch 11071, training loss: 8830.34, average training loss: 9289.13, base loss: 14393.03
[INFO 2017-06-28 15:16:16,277 main.py:51] epoch 11072, training loss: 10276.72, average training loss: 9290.24, base loss: 14395.87
[INFO 2017-06-28 15:16:17,397 main.py:51] epoch 11073, training loss: 8495.17, average training loss: 9290.59, base loss: 14397.67
[INFO 2017-06-28 15:16:18,468 main.py:51] epoch 11074, training loss: 9379.12, average training loss: 9291.55, base loss: 14400.03
[INFO 2017-06-28 15:16:19,553 main.py:51] epoch 11075, training loss: 9642.47, average training loss: 9291.95, base loss: 14401.37
[INFO 2017-06-28 15:16:20,552 main.py:51] epoch 11076, training loss: 8589.60, average training loss: 9291.27, base loss: 14401.10
[INFO 2017-06-28 15:16:21,616 main.py:51] epoch 11077, training loss: 10282.86, average training loss: 9292.64, base loss: 14403.16
[INFO 2017-06-28 15:16:22,697 main.py:51] epoch 11078, training loss: 9346.74, average training loss: 9292.61, base loss: 14403.60
[INFO 2017-06-28 15:16:23,747 main.py:51] epoch 11079, training loss: 8606.28, average training loss: 9293.26, base loss: 14404.57
[INFO 2017-06-28 15:16:24,809 main.py:51] epoch 11080, training loss: 8643.26, average training loss: 9291.84, base loss: 14402.06
[INFO 2017-06-28 15:16:25,820 main.py:51] epoch 11081, training loss: 8434.01, average training loss: 9290.89, base loss: 14401.48
[INFO 2017-06-28 15:16:26,907 main.py:51] epoch 11082, training loss: 9045.35, average training loss: 9290.10, base loss: 14397.62
[INFO 2017-06-28 15:16:28,034 main.py:51] epoch 11083, training loss: 8576.55, average training loss: 9288.36, base loss: 14395.80
[INFO 2017-06-28 15:16:29,036 main.py:51] epoch 11084, training loss: 9673.57, average training loss: 9288.23, base loss: 14396.12
[INFO 2017-06-28 15:16:30,055 main.py:51] epoch 11085, training loss: 8854.00, average training loss: 9287.68, base loss: 14395.59
[INFO 2017-06-28 15:16:31,140 main.py:51] epoch 11086, training loss: 10368.15, average training loss: 9288.89, base loss: 14398.32
[INFO 2017-06-28 15:16:32,200 main.py:51] epoch 11087, training loss: 8867.85, average training loss: 9289.01, base loss: 14398.61
[INFO 2017-06-28 15:16:33,309 main.py:51] epoch 11088, training loss: 10912.63, average training loss: 9290.05, base loss: 14401.18
[INFO 2017-06-28 15:16:34,354 main.py:51] epoch 11089, training loss: 10612.45, average training loss: 9290.92, base loss: 14401.17
[INFO 2017-06-28 15:16:35,387 main.py:51] epoch 11090, training loss: 9904.32, average training loss: 9290.51, base loss: 14400.83
[INFO 2017-06-28 15:16:36,459 main.py:51] epoch 11091, training loss: 9289.89, average training loss: 9289.29, base loss: 14399.52
[INFO 2017-06-28 15:16:37,556 main.py:51] epoch 11092, training loss: 9516.37, average training loss: 9288.39, base loss: 14396.47
[INFO 2017-06-28 15:16:38,667 main.py:51] epoch 11093, training loss: 9374.80, average training loss: 9287.82, base loss: 14395.10
[INFO 2017-06-28 15:16:39,777 main.py:51] epoch 11094, training loss: 10609.31, average training loss: 9289.52, base loss: 14398.04
[INFO 2017-06-28 15:16:40,881 main.py:51] epoch 11095, training loss: 9640.92, average training loss: 9289.89, base loss: 14398.16
[INFO 2017-06-28 15:16:41,892 main.py:51] epoch 11096, training loss: 10048.76, average training loss: 9290.14, base loss: 14400.28
[INFO 2017-06-28 15:16:42,936 main.py:51] epoch 11097, training loss: 9522.19, average training loss: 9290.78, base loss: 14400.56
[INFO 2017-06-28 15:16:43,964 main.py:51] epoch 11098, training loss: 8791.32, average training loss: 9291.74, base loss: 14401.22
[INFO 2017-06-28 15:16:45,024 main.py:51] epoch 11099, training loss: 9686.49, average training loss: 9292.27, base loss: 14401.27
[INFO 2017-06-28 15:16:45,025 main.py:53] epoch 11099, testing
[INFO 2017-06-28 15:16:48,802 main.py:105] average testing loss: 11034.95, base loss: 15840.55
[INFO 2017-06-28 15:16:48,802 main.py:106] improve_loss: 4805.60, improve_percent: 0.30
[INFO 2017-06-28 15:16:48,803 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:16:49,833 main.py:51] epoch 11100, training loss: 9192.01, average training loss: 9291.68, base loss: 14401.06
[INFO 2017-06-28 15:16:50,929 main.py:51] epoch 11101, training loss: 8604.83, average training loss: 9290.69, base loss: 14400.06
[INFO 2017-06-28 15:16:51,932 main.py:51] epoch 11102, training loss: 9192.10, average training loss: 9289.49, base loss: 14398.61
[INFO 2017-06-28 15:16:52,957 main.py:51] epoch 11103, training loss: 8991.00, average training loss: 9290.15, base loss: 14400.37
[INFO 2017-06-28 15:16:54,024 main.py:51] epoch 11104, training loss: 8111.60, average training loss: 9288.55, base loss: 14397.55
[INFO 2017-06-28 15:16:55,023 main.py:51] epoch 11105, training loss: 11021.33, average training loss: 9290.02, base loss: 14399.77
[INFO 2017-06-28 15:16:56,062 main.py:51] epoch 11106, training loss: 8482.82, average training loss: 9290.18, base loss: 14400.06
[INFO 2017-06-28 15:16:57,005 main.py:51] epoch 11107, training loss: 9600.25, average training loss: 9291.46, base loss: 14403.86
[INFO 2017-06-28 15:16:57,978 main.py:51] epoch 11108, training loss: 9678.70, average training loss: 9293.04, base loss: 14406.90
[INFO 2017-06-28 15:16:58,905 main.py:51] epoch 11109, training loss: 9200.42, average training loss: 9293.36, base loss: 14406.42
[INFO 2017-06-28 15:16:59,994 main.py:51] epoch 11110, training loss: 8556.52, average training loss: 9289.86, base loss: 14400.94
[INFO 2017-06-28 15:17:01,038 main.py:51] epoch 11111, training loss: 9567.16, average training loss: 9289.68, base loss: 14402.96
[INFO 2017-06-28 15:17:02,123 main.py:51] epoch 11112, training loss: 9803.23, average training loss: 9289.73, base loss: 14399.94
[INFO 2017-06-28 15:17:03,137 main.py:51] epoch 11113, training loss: 9243.57, average training loss: 9290.55, base loss: 14400.23
[INFO 2017-06-28 15:17:04,203 main.py:51] epoch 11114, training loss: 9329.06, average training loss: 9290.55, base loss: 14398.65
[INFO 2017-06-28 15:17:05,331 main.py:51] epoch 11115, training loss: 7569.43, average training loss: 9288.08, base loss: 14394.76
[INFO 2017-06-28 15:17:06,302 main.py:51] epoch 11116, training loss: 9747.44, average training loss: 9288.58, base loss: 14396.45
[INFO 2017-06-28 15:17:07,392 main.py:51] epoch 11117, training loss: 8232.37, average training loss: 9287.29, base loss: 14392.37
[INFO 2017-06-28 15:17:08,485 main.py:51] epoch 11118, training loss: 9986.50, average training loss: 9288.96, base loss: 14394.98
[INFO 2017-06-28 15:17:09,484 main.py:51] epoch 11119, training loss: 10673.55, average training loss: 9290.02, base loss: 14397.28
[INFO 2017-06-28 15:17:10,506 main.py:51] epoch 11120, training loss: 8879.71, average training loss: 9288.60, base loss: 14394.66
[INFO 2017-06-28 15:17:11,551 main.py:51] epoch 11121, training loss: 8295.57, average training loss: 9287.92, base loss: 14392.97
[INFO 2017-06-28 15:17:12,602 main.py:51] epoch 11122, training loss: 10447.31, average training loss: 9288.09, base loss: 14391.27
[INFO 2017-06-28 15:17:13,665 main.py:51] epoch 11123, training loss: 9457.03, average training loss: 9287.67, base loss: 14389.61
[INFO 2017-06-28 15:17:14,679 main.py:51] epoch 11124, training loss: 9104.36, average training loss: 9287.74, base loss: 14389.70
[INFO 2017-06-28 15:17:15,742 main.py:51] epoch 11125, training loss: 8879.61, average training loss: 9287.59, base loss: 14388.79
[INFO 2017-06-28 15:17:16,822 main.py:51] epoch 11126, training loss: 9292.28, average training loss: 9287.89, base loss: 14390.83
[INFO 2017-06-28 15:17:17,847 main.py:51] epoch 11127, training loss: 8843.26, average training loss: 9287.53, base loss: 14391.22
[INFO 2017-06-28 15:17:18,889 main.py:51] epoch 11128, training loss: 9530.65, average training loss: 9288.94, base loss: 14395.82
[INFO 2017-06-28 15:17:19,953 main.py:51] epoch 11129, training loss: 9378.65, average training loss: 9289.18, base loss: 14395.32
[INFO 2017-06-28 15:17:21,036 main.py:51] epoch 11130, training loss: 10300.04, average training loss: 9291.64, base loss: 14399.42
[INFO 2017-06-28 15:17:22,162 main.py:51] epoch 11131, training loss: 10436.90, average training loss: 9291.69, base loss: 14399.35
[INFO 2017-06-28 15:17:23,261 main.py:51] epoch 11132, training loss: 9064.17, average training loss: 9292.48, base loss: 14400.20
[INFO 2017-06-28 15:17:24,393 main.py:51] epoch 11133, training loss: 8381.94, average training loss: 9290.53, base loss: 14399.10
[INFO 2017-06-28 15:17:25,401 main.py:51] epoch 11134, training loss: 9179.11, average training loss: 9290.22, base loss: 14398.31
[INFO 2017-06-28 15:17:26,421 main.py:51] epoch 11135, training loss: 9492.49, average training loss: 9290.32, base loss: 14398.18
[INFO 2017-06-28 15:17:27,511 main.py:51] epoch 11136, training loss: 8767.74, average training loss: 9290.26, base loss: 14397.78
[INFO 2017-06-28 15:17:28,528 main.py:51] epoch 11137, training loss: 8548.60, average training loss: 9289.06, base loss: 14395.21
[INFO 2017-06-28 15:17:29,543 main.py:51] epoch 11138, training loss: 9441.48, average training loss: 9290.42, base loss: 14397.48
[INFO 2017-06-28 15:17:30,588 main.py:51] epoch 11139, training loss: 10712.20, average training loss: 9290.60, base loss: 14399.56
[INFO 2017-06-28 15:17:31,660 main.py:51] epoch 11140, training loss: 8668.88, average training loss: 9288.26, base loss: 14395.69
[INFO 2017-06-28 15:17:32,678 main.py:51] epoch 11141, training loss: 8494.72, average training loss: 9286.67, base loss: 14392.28
[INFO 2017-06-28 15:17:33,734 main.py:51] epoch 11142, training loss: 9007.86, average training loss: 9285.00, base loss: 14389.02
[INFO 2017-06-28 15:17:34,813 main.py:51] epoch 11143, training loss: 8849.95, average training loss: 9284.44, base loss: 14389.06
[INFO 2017-06-28 15:17:35,858 main.py:51] epoch 11144, training loss: 9269.44, average training loss: 9285.16, base loss: 14391.32
[INFO 2017-06-28 15:17:36,859 main.py:51] epoch 11145, training loss: 10250.10, average training loss: 9285.22, base loss: 14392.70
[INFO 2017-06-28 15:17:37,900 main.py:51] epoch 11146, training loss: 8482.68, average training loss: 9284.58, base loss: 14391.52
[INFO 2017-06-28 15:17:39,023 main.py:51] epoch 11147, training loss: 8712.92, average training loss: 9283.63, base loss: 14391.18
[INFO 2017-06-28 15:17:40,169 main.py:51] epoch 11148, training loss: 8383.01, average training loss: 9283.89, base loss: 14392.08
[INFO 2017-06-28 15:17:41,217 main.py:51] epoch 11149, training loss: 9736.45, average training loss: 9284.53, base loss: 14393.01
[INFO 2017-06-28 15:17:42,257 main.py:51] epoch 11150, training loss: 10516.24, average training loss: 9286.39, base loss: 14396.86
[INFO 2017-06-28 15:17:43,341 main.py:51] epoch 11151, training loss: 8375.00, average training loss: 9284.66, base loss: 14394.83
[INFO 2017-06-28 15:17:44,492 main.py:51] epoch 11152, training loss: 10740.39, average training loss: 9286.19, base loss: 14399.28
[INFO 2017-06-28 15:17:45,513 main.py:51] epoch 11153, training loss: 9235.09, average training loss: 9284.55, base loss: 14393.93
[INFO 2017-06-28 15:17:46,553 main.py:51] epoch 11154, training loss: 9589.33, average training loss: 9285.29, base loss: 14395.94
[INFO 2017-06-28 15:17:47,624 main.py:51] epoch 11155, training loss: 8151.62, average training loss: 9283.50, base loss: 14392.45
[INFO 2017-06-28 15:17:48,695 main.py:51] epoch 11156, training loss: 9108.87, average training loss: 9283.21, base loss: 14392.05
[INFO 2017-06-28 15:17:49,768 main.py:51] epoch 11157, training loss: 9293.05, average training loss: 9284.23, base loss: 14393.09
[INFO 2017-06-28 15:17:50,774 main.py:51] epoch 11158, training loss: 9726.89, average training loss: 9285.32, base loss: 14393.95
[INFO 2017-06-28 15:17:51,845 main.py:51] epoch 11159, training loss: 9084.43, average training loss: 9283.49, base loss: 14390.85
[INFO 2017-06-28 15:17:52,959 main.py:51] epoch 11160, training loss: 9331.82, average training loss: 9283.39, base loss: 14391.02
[INFO 2017-06-28 15:17:53,985 main.py:51] epoch 11161, training loss: 8274.40, average training loss: 9282.87, base loss: 14389.44
[INFO 2017-06-28 15:17:55,043 main.py:51] epoch 11162, training loss: 9710.27, average training loss: 9281.46, base loss: 14386.05
[INFO 2017-06-28 15:17:56,167 main.py:51] epoch 11163, training loss: 8699.59, average training loss: 9280.76, base loss: 14385.33
[INFO 2017-06-28 15:17:57,143 main.py:51] epoch 11164, training loss: 8934.03, average training loss: 9281.60, base loss: 14387.93
[INFO 2017-06-28 15:17:58,183 main.py:51] epoch 11165, training loss: 8105.67, average training loss: 9281.07, base loss: 14386.65
[INFO 2017-06-28 15:17:59,278 main.py:51] epoch 11166, training loss: 9390.11, average training loss: 9280.88, base loss: 14386.48
[INFO 2017-06-28 15:18:00,340 main.py:51] epoch 11167, training loss: 9886.93, average training loss: 9281.97, base loss: 14389.35
[INFO 2017-06-28 15:18:01,375 main.py:51] epoch 11168, training loss: 9472.10, average training loss: 9282.67, base loss: 14388.88
[INFO 2017-06-28 15:18:02,415 main.py:51] epoch 11169, training loss: 8383.81, average training loss: 9281.94, base loss: 14387.64
[INFO 2017-06-28 15:18:03,523 main.py:51] epoch 11170, training loss: 9717.80, average training loss: 9283.28, base loss: 14389.51
[INFO 2017-06-28 15:18:04,523 main.py:51] epoch 11171, training loss: 8782.01, average training loss: 9281.75, base loss: 14387.59
[INFO 2017-06-28 15:18:05,590 main.py:51] epoch 11172, training loss: 8951.75, average training loss: 9280.77, base loss: 14386.15
[INFO 2017-06-28 15:18:06,696 main.py:51] epoch 11173, training loss: 8316.73, average training loss: 9280.26, base loss: 14384.66
[INFO 2017-06-28 15:18:07,799 main.py:51] epoch 11174, training loss: 9989.51, average training loss: 9281.77, base loss: 14388.83
[INFO 2017-06-28 15:18:08,904 main.py:51] epoch 11175, training loss: 8411.03, average training loss: 9280.97, base loss: 14385.74
[INFO 2017-06-28 15:18:09,965 main.py:51] epoch 11176, training loss: 8478.24, average training loss: 9278.69, base loss: 14382.23
[INFO 2017-06-28 15:18:11,020 main.py:51] epoch 11177, training loss: 8885.14, average training loss: 9279.44, base loss: 14383.32
[INFO 2017-06-28 15:18:12,064 main.py:51] epoch 11178, training loss: 9113.85, average training loss: 9278.66, base loss: 14382.13
[INFO 2017-06-28 15:18:13,137 main.py:51] epoch 11179, training loss: 9379.41, average training loss: 9279.39, base loss: 14384.34
[INFO 2017-06-28 15:18:14,229 main.py:51] epoch 11180, training loss: 8015.99, average training loss: 9276.68, base loss: 14380.84
[INFO 2017-06-28 15:18:15,236 main.py:51] epoch 11181, training loss: 9743.26, average training loss: 9276.97, base loss: 14380.87
[INFO 2017-06-28 15:18:16,170 main.py:51] epoch 11182, training loss: 8401.71, average training loss: 9275.70, base loss: 14379.64
[INFO 2017-06-28 15:18:17,122 main.py:51] epoch 11183, training loss: 9862.46, average training loss: 9275.46, base loss: 14380.42
[INFO 2017-06-28 15:18:18,051 main.py:51] epoch 11184, training loss: 9420.42, average training loss: 9274.31, base loss: 14378.19
[INFO 2017-06-28 15:18:19,025 main.py:51] epoch 11185, training loss: 8097.42, average training loss: 9273.58, base loss: 14376.10
[INFO 2017-06-28 15:18:20,072 main.py:51] epoch 11186, training loss: 8855.61, average training loss: 9273.30, base loss: 14375.44
[INFO 2017-06-28 15:18:21,149 main.py:51] epoch 11187, training loss: 9274.33, average training loss: 9273.51, base loss: 14374.89
[INFO 2017-06-28 15:18:22,157 main.py:51] epoch 11188, training loss: 9754.37, average training loss: 9273.86, base loss: 14375.52
[INFO 2017-06-28 15:18:23,196 main.py:51] epoch 11189, training loss: 9497.28, average training loss: 9274.29, base loss: 14375.30
[INFO 2017-06-28 15:18:24,264 main.py:51] epoch 11190, training loss: 10021.11, average training loss: 9273.95, base loss: 14376.97
[INFO 2017-06-28 15:18:25,280 main.py:51] epoch 11191, training loss: 8772.71, average training loss: 9272.93, base loss: 14374.28
[INFO 2017-06-28 15:18:26,298 main.py:51] epoch 11192, training loss: 8354.15, average training loss: 9272.66, base loss: 14373.28
[INFO 2017-06-28 15:18:27,366 main.py:51] epoch 11193, training loss: 8813.39, average training loss: 9272.27, base loss: 14374.06
[INFO 2017-06-28 15:18:28,391 main.py:51] epoch 11194, training loss: 9390.77, average training loss: 9272.92, base loss: 14373.04
[INFO 2017-06-28 15:18:29,457 main.py:51] epoch 11195, training loss: 10394.66, average training loss: 9274.47, base loss: 14376.94
[INFO 2017-06-28 15:18:30,509 main.py:51] epoch 11196, training loss: 9184.12, average training loss: 9273.94, base loss: 14375.64
[INFO 2017-06-28 15:18:31,517 main.py:51] epoch 11197, training loss: 9908.59, average training loss: 9274.85, base loss: 14376.95
[INFO 2017-06-28 15:18:32,621 main.py:51] epoch 11198, training loss: 8056.04, average training loss: 9273.98, base loss: 14375.95
[INFO 2017-06-28 15:18:33,707 main.py:51] epoch 11199, training loss: 9791.17, average training loss: 9273.76, base loss: 14376.04
[INFO 2017-06-28 15:18:33,707 main.py:53] epoch 11199, testing
[INFO 2017-06-28 15:18:37,528 main.py:105] average testing loss: 10535.08, base loss: 15074.45
[INFO 2017-06-28 15:18:37,528 main.py:106] improve_loss: 4539.37, improve_percent: 0.30
[INFO 2017-06-28 15:18:37,529 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:18:38,595 main.py:51] epoch 11200, training loss: 9065.25, average training loss: 9272.96, base loss: 14375.96
[INFO 2017-06-28 15:18:39,604 main.py:51] epoch 11201, training loss: 9929.21, average training loss: 9273.47, base loss: 14376.60
[INFO 2017-06-28 15:18:40,666 main.py:51] epoch 11202, training loss: 8666.60, average training loss: 9273.40, base loss: 14377.13
[INFO 2017-06-28 15:18:41,708 main.py:51] epoch 11203, training loss: 8583.67, average training loss: 9272.32, base loss: 14375.42
[INFO 2017-06-28 15:18:42,788 main.py:51] epoch 11204, training loss: 10239.72, average training loss: 9272.46, base loss: 14374.06
[INFO 2017-06-28 15:18:43,914 main.py:51] epoch 11205, training loss: 8072.59, average training loss: 9270.41, base loss: 14371.80
[INFO 2017-06-28 15:18:44,917 main.py:51] epoch 11206, training loss: 9114.53, average training loss: 9269.48, base loss: 14368.82
[INFO 2017-06-28 15:18:45,950 main.py:51] epoch 11207, training loss: 9811.71, average training loss: 9271.08, base loss: 14372.33
[INFO 2017-06-28 15:18:46,998 main.py:51] epoch 11208, training loss: 8706.81, average training loss: 9270.54, base loss: 14371.16
[INFO 2017-06-28 15:18:48,060 main.py:51] epoch 11209, training loss: 10069.16, average training loss: 9271.64, base loss: 14373.09
[INFO 2017-06-28 15:18:49,100 main.py:51] epoch 11210, training loss: 9732.25, average training loss: 9271.60, base loss: 14372.54
[INFO 2017-06-28 15:18:50,158 main.py:51] epoch 11211, training loss: 9660.50, average training loss: 9271.93, base loss: 14374.04
[INFO 2017-06-28 15:18:51,225 main.py:51] epoch 11212, training loss: 8186.27, average training loss: 9271.22, base loss: 14373.79
[INFO 2017-06-28 15:18:52,282 main.py:51] epoch 11213, training loss: 9584.80, average training loss: 9271.23, base loss: 14374.47
[INFO 2017-06-28 15:18:53,339 main.py:51] epoch 11214, training loss: 8278.71, average training loss: 9268.73, base loss: 14369.73
[INFO 2017-06-28 15:18:54,398 main.py:51] epoch 11215, training loss: 8915.55, average training loss: 9266.73, base loss: 14366.51
[INFO 2017-06-28 15:18:55,440 main.py:51] epoch 11216, training loss: 8512.15, average training loss: 9263.97, base loss: 14362.21
[INFO 2017-06-28 15:18:56,516 main.py:51] epoch 11217, training loss: 10049.48, average training loss: 9265.53, base loss: 14364.95
[INFO 2017-06-28 15:18:57,603 main.py:51] epoch 11218, training loss: 8855.31, average training loss: 9265.11, base loss: 14364.76
[INFO 2017-06-28 15:18:58,713 main.py:51] epoch 11219, training loss: 8714.07, average training loss: 9264.43, base loss: 14364.22
[INFO 2017-06-28 15:18:59,818 main.py:51] epoch 11220, training loss: 8288.89, average training loss: 9263.92, base loss: 14361.64
[INFO 2017-06-28 15:19:00,845 main.py:51] epoch 11221, training loss: 8770.01, average training loss: 9261.71, base loss: 14358.48
[INFO 2017-06-28 15:19:01,941 main.py:51] epoch 11222, training loss: 8749.95, average training loss: 9261.53, base loss: 14358.30
[INFO 2017-06-28 15:19:02,967 main.py:51] epoch 11223, training loss: 9584.10, average training loss: 9262.48, base loss: 14360.24
[INFO 2017-06-28 15:19:04,052 main.py:51] epoch 11224, training loss: 8366.11, average training loss: 9261.19, base loss: 14358.00
[INFO 2017-06-28 15:19:05,237 main.py:51] epoch 11225, training loss: 7553.80, average training loss: 9259.74, base loss: 14354.47
[INFO 2017-06-28 15:19:06,291 main.py:51] epoch 11226, training loss: 8881.87, average training loss: 9260.38, base loss: 14356.20
[INFO 2017-06-28 15:19:07,340 main.py:51] epoch 11227, training loss: 8946.52, average training loss: 9260.93, base loss: 14358.29
[INFO 2017-06-28 15:19:08,391 main.py:51] epoch 11228, training loss: 8355.30, average training loss: 9260.24, base loss: 14356.98
[INFO 2017-06-28 15:19:09,445 main.py:51] epoch 11229, training loss: 10690.23, average training loss: 9262.26, base loss: 14359.43
[INFO 2017-06-28 15:19:10,531 main.py:51] epoch 11230, training loss: 10017.78, average training loss: 9262.60, base loss: 14359.37
[INFO 2017-06-28 15:19:11,561 main.py:51] epoch 11231, training loss: 8309.30, average training loss: 9262.28, base loss: 14359.48
[INFO 2017-06-28 15:19:12,578 main.py:51] epoch 11232, training loss: 8507.95, average training loss: 9262.23, base loss: 14359.79
[INFO 2017-06-28 15:19:13,649 main.py:51] epoch 11233, training loss: 10144.46, average training loss: 9262.14, base loss: 14361.56
[INFO 2017-06-28 15:19:14,657 main.py:51] epoch 11234, training loss: 10882.65, average training loss: 9264.62, base loss: 14365.03
[INFO 2017-06-28 15:19:15,739 main.py:51] epoch 11235, training loss: 8715.98, average training loss: 9264.17, base loss: 14363.05
[INFO 2017-06-28 15:19:16,789 main.py:51] epoch 11236, training loss: 8110.47, average training loss: 9263.43, base loss: 14361.86
[INFO 2017-06-28 15:19:17,854 main.py:51] epoch 11237, training loss: 9405.39, average training loss: 9263.90, base loss: 14362.36
[INFO 2017-06-28 15:19:18,953 main.py:51] epoch 11238, training loss: 8396.13, average training loss: 9261.29, base loss: 14360.51
[INFO 2017-06-28 15:19:20,032 main.py:51] epoch 11239, training loss: 10281.34, average training loss: 9260.63, base loss: 14357.70
[INFO 2017-06-28 15:19:21,091 main.py:51] epoch 11240, training loss: 9406.23, average training loss: 9261.28, base loss: 14359.80
[INFO 2017-06-28 15:19:22,116 main.py:51] epoch 11241, training loss: 9966.67, average training loss: 9263.05, base loss: 14362.14
[INFO 2017-06-28 15:19:23,207 main.py:51] epoch 11242, training loss: 10159.03, average training loss: 9262.82, base loss: 14361.70
[INFO 2017-06-28 15:19:24,313 main.py:51] epoch 11243, training loss: 9720.34, average training loss: 9262.84, base loss: 14362.09
[INFO 2017-06-28 15:19:25,390 main.py:51] epoch 11244, training loss: 9479.20, average training loss: 9262.76, base loss: 14362.50
[INFO 2017-06-28 15:19:26,478 main.py:51] epoch 11245, training loss: 8187.65, average training loss: 9259.66, base loss: 14357.93
[INFO 2017-06-28 15:19:27,507 main.py:51] epoch 11246, training loss: 8205.68, average training loss: 9257.85, base loss: 14356.30
[INFO 2017-06-28 15:19:28,552 main.py:51] epoch 11247, training loss: 9403.78, average training loss: 9256.75, base loss: 14354.36
[INFO 2017-06-28 15:19:29,619 main.py:51] epoch 11248, training loss: 8225.13, average training loss: 9254.86, base loss: 14350.59
[INFO 2017-06-28 15:19:30,710 main.py:51] epoch 11249, training loss: 8621.05, average training loss: 9253.50, base loss: 14348.53
[INFO 2017-06-28 15:19:31,842 main.py:51] epoch 11250, training loss: 8985.78, average training loss: 9253.36, base loss: 14348.74
[INFO 2017-06-28 15:19:32,851 main.py:51] epoch 11251, training loss: 9481.64, average training loss: 9254.44, base loss: 14350.45
[INFO 2017-06-28 15:19:33,943 main.py:51] epoch 11252, training loss: 7925.01, average training loss: 9253.79, base loss: 14350.80
[INFO 2017-06-28 15:19:34,953 main.py:51] epoch 11253, training loss: 8140.15, average training loss: 9254.13, base loss: 14352.16
[INFO 2017-06-28 15:19:35,922 main.py:51] epoch 11254, training loss: 9296.31, average training loss: 9254.67, base loss: 14354.06
[INFO 2017-06-28 15:19:36,851 main.py:51] epoch 11255, training loss: 10172.31, average training loss: 9254.67, base loss: 14356.24
[INFO 2017-06-28 15:19:37,800 main.py:51] epoch 11256, training loss: 8362.09, average training loss: 9253.51, base loss: 14354.86
[INFO 2017-06-28 15:19:38,863 main.py:51] epoch 11257, training loss: 9037.17, average training loss: 9252.72, base loss: 14353.70
[INFO 2017-06-28 15:19:39,968 main.py:51] epoch 11258, training loss: 9650.62, average training loss: 9252.45, base loss: 14352.05
[INFO 2017-06-28 15:19:41,017 main.py:51] epoch 11259, training loss: 11217.86, average training loss: 9255.36, base loss: 14356.68
[INFO 2017-06-28 15:19:42,086 main.py:51] epoch 11260, training loss: 8461.66, average training loss: 9253.42, base loss: 14353.46
[INFO 2017-06-28 15:19:42,928 main.py:51] epoch 11261, training loss: 8688.01, average training loss: 9252.26, base loss: 14350.40
[INFO 2017-06-28 15:19:43,603 main.py:51] epoch 11262, training loss: 8399.15, average training loss: 9251.47, base loss: 14350.14
[INFO 2017-06-28 15:19:44,273 main.py:51] epoch 11263, training loss: 8207.92, average training loss: 9250.04, base loss: 14347.95
[INFO 2017-06-28 15:19:44,930 main.py:51] epoch 11264, training loss: 8586.94, average training loss: 9250.09, base loss: 14349.23
[INFO 2017-06-28 15:19:45,606 main.py:51] epoch 11265, training loss: 8284.25, average training loss: 9249.42, base loss: 14347.91
[INFO 2017-06-28 15:19:46,269 main.py:51] epoch 11266, training loss: 9190.53, average training loss: 9250.47, base loss: 14352.01
[INFO 2017-06-28 15:19:46,925 main.py:51] epoch 11267, training loss: 8603.06, average training loss: 9249.55, base loss: 14350.47
[INFO 2017-06-28 15:19:47,587 main.py:51] epoch 11268, training loss: 9117.01, average training loss: 9250.30, base loss: 14352.95
[INFO 2017-06-28 15:19:48,239 main.py:51] epoch 11269, training loss: 9224.09, average training loss: 9249.61, base loss: 14354.15
[INFO 2017-06-28 15:19:48,911 main.py:51] epoch 11270, training loss: 10215.40, average training loss: 9249.66, base loss: 14355.70
[INFO 2017-06-28 15:19:49,566 main.py:51] epoch 11271, training loss: 8694.00, average training loss: 9249.08, base loss: 14355.01
[INFO 2017-06-28 15:19:50,207 main.py:51] epoch 11272, training loss: 10060.60, average training loss: 9249.97, base loss: 14354.12
[INFO 2017-06-28 15:19:50,856 main.py:51] epoch 11273, training loss: 9069.46, average training loss: 9248.98, base loss: 14351.75
[INFO 2017-06-28 15:19:51,515 main.py:51] epoch 11274, training loss: 8898.35, average training loss: 9247.37, base loss: 14348.83
[INFO 2017-06-28 15:19:52,154 main.py:51] epoch 11275, training loss: 9336.26, average training loss: 9246.67, base loss: 14347.10
[INFO 2017-06-28 15:19:52,808 main.py:51] epoch 11276, training loss: 11074.73, average training loss: 9247.33, base loss: 14349.93
[INFO 2017-06-28 15:19:53,466 main.py:51] epoch 11277, training loss: 9060.28, average training loss: 9247.54, base loss: 14351.32
[INFO 2017-06-28 15:19:54,116 main.py:51] epoch 11278, training loss: 8250.80, average training loss: 9246.46, base loss: 14350.09
[INFO 2017-06-28 15:19:54,778 main.py:51] epoch 11279, training loss: 9329.87, average training loss: 9245.85, base loss: 14348.27
[INFO 2017-06-28 15:19:55,421 main.py:51] epoch 11280, training loss: 8413.37, average training loss: 9245.31, base loss: 14348.13
[INFO 2017-06-28 15:19:56,093 main.py:51] epoch 11281, training loss: 8352.54, average training loss: 9243.83, base loss: 14346.01
[INFO 2017-06-28 15:19:56,742 main.py:51] epoch 11282, training loss: 9599.73, average training loss: 9243.71, base loss: 14346.63
[INFO 2017-06-28 15:19:57,406 main.py:51] epoch 11283, training loss: 9430.89, average training loss: 9243.25, base loss: 14346.90
[INFO 2017-06-28 15:19:58,058 main.py:51] epoch 11284, training loss: 8466.08, average training loss: 9243.29, base loss: 14348.45
[INFO 2017-06-28 15:19:58,713 main.py:51] epoch 11285, training loss: 8239.82, average training loss: 9242.70, base loss: 14348.12
[INFO 2017-06-28 15:19:59,394 main.py:51] epoch 11286, training loss: 9246.90, average training loss: 9242.22, base loss: 14348.79
[INFO 2017-06-28 15:20:00,047 main.py:51] epoch 11287, training loss: 9648.34, average training loss: 9243.47, base loss: 14351.68
[INFO 2017-06-28 15:20:00,714 main.py:51] epoch 11288, training loss: 9486.96, average training loss: 9243.32, base loss: 14350.57
[INFO 2017-06-28 15:20:01,363 main.py:51] epoch 11289, training loss: 9304.07, average training loss: 9243.73, base loss: 14350.08
[INFO 2017-06-28 15:20:02,016 main.py:51] epoch 11290, training loss: 8813.95, average training loss: 9242.11, base loss: 14346.16
[INFO 2017-06-28 15:20:02,701 main.py:51] epoch 11291, training loss: 9145.28, average training loss: 9242.80, base loss: 14347.49
[INFO 2017-06-28 15:20:03,378 main.py:51] epoch 11292, training loss: 9642.27, average training loss: 9241.90, base loss: 14346.36
[INFO 2017-06-28 15:20:04,032 main.py:51] epoch 11293, training loss: 9382.31, average training loss: 9243.41, base loss: 14348.97
[INFO 2017-06-28 15:20:04,731 main.py:51] epoch 11294, training loss: 10075.62, average training loss: 9244.09, base loss: 14350.69
[INFO 2017-06-28 15:20:05,414 main.py:51] epoch 11295, training loss: 9595.02, average training loss: 9244.45, base loss: 14352.68
[INFO 2017-06-28 15:20:06,107 main.py:51] epoch 11296, training loss: 8740.32, average training loss: 9243.71, base loss: 14352.46
[INFO 2017-06-28 15:20:06,798 main.py:51] epoch 11297, training loss: 8320.12, average training loss: 9241.98, base loss: 14349.51
[INFO 2017-06-28 15:20:07,462 main.py:51] epoch 11298, training loss: 9997.78, average training loss: 9242.69, base loss: 14352.86
[INFO 2017-06-28 15:20:08,142 main.py:51] epoch 11299, training loss: 9767.46, average training loss: 9241.34, base loss: 14348.57
[INFO 2017-06-28 15:20:08,142 main.py:53] epoch 11299, testing
[INFO 2017-06-28 15:20:10,713 main.py:105] average testing loss: 10454.94, base loss: 15002.29
[INFO 2017-06-28 15:20:10,713 main.py:106] improve_loss: 4547.35, improve_percent: 0.30
[INFO 2017-06-28 15:20:10,714 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:20:11,379 main.py:51] epoch 11300, training loss: 7876.58, average training loss: 9238.53, base loss: 14345.89
[INFO 2017-06-28 15:20:12,033 main.py:51] epoch 11301, training loss: 8359.68, average training loss: 9238.14, base loss: 14345.90
[INFO 2017-06-28 15:20:12,694 main.py:51] epoch 11302, training loss: 8804.89, average training loss: 9238.35, base loss: 14345.63
[INFO 2017-06-28 15:20:13,368 main.py:51] epoch 11303, training loss: 8534.08, average training loss: 9237.31, base loss: 14343.41
[INFO 2017-06-28 15:20:14,010 main.py:51] epoch 11304, training loss: 9634.20, average training loss: 9237.39, base loss: 14342.87
[INFO 2017-06-28 15:20:14,670 main.py:51] epoch 11305, training loss: 9661.39, average training loss: 9238.04, base loss: 14344.39
[INFO 2017-06-28 15:20:15,325 main.py:51] epoch 11306, training loss: 8152.34, average training loss: 9237.10, base loss: 14344.33
[INFO 2017-06-28 15:20:15,956 main.py:51] epoch 11307, training loss: 8666.19, average training loss: 9235.31, base loss: 14341.41
[INFO 2017-06-28 15:20:16,603 main.py:51] epoch 11308, training loss: 8275.14, average training loss: 9234.96, base loss: 14340.71
[INFO 2017-06-28 15:20:17,249 main.py:51] epoch 11309, training loss: 7976.37, average training loss: 9233.14, base loss: 14339.21
[INFO 2017-06-28 15:20:17,915 main.py:51] epoch 11310, training loss: 9542.64, average training loss: 9232.20, base loss: 14337.94
[INFO 2017-06-28 15:20:18,564 main.py:51] epoch 11311, training loss: 9738.04, average training loss: 9233.15, base loss: 14340.97
[INFO 2017-06-28 15:20:19,212 main.py:51] epoch 11312, training loss: 9372.80, average training loss: 9232.74, base loss: 14339.20
[INFO 2017-06-28 15:20:19,858 main.py:51] epoch 11313, training loss: 8481.81, average training loss: 9232.11, base loss: 14338.37
[INFO 2017-06-28 15:20:20,491 main.py:51] epoch 11314, training loss: 7581.04, average training loss: 9229.89, base loss: 14335.43
[INFO 2017-06-28 15:20:21,169 main.py:51] epoch 11315, training loss: 9892.68, average training loss: 9230.81, base loss: 14337.88
[INFO 2017-06-28 15:20:21,827 main.py:51] epoch 11316, training loss: 8715.29, average training loss: 9231.10, base loss: 14338.27
[INFO 2017-06-28 15:20:22,488 main.py:51] epoch 11317, training loss: 9621.63, average training loss: 9231.69, base loss: 14339.77
[INFO 2017-06-28 15:20:23,148 main.py:51] epoch 11318, training loss: 8724.16, average training loss: 9232.24, base loss: 14341.51
[INFO 2017-06-28 15:20:23,803 main.py:51] epoch 11319, training loss: 8716.45, average training loss: 9232.62, base loss: 14341.11
[INFO 2017-06-28 15:20:24,454 main.py:51] epoch 11320, training loss: 9038.19, average training loss: 9232.30, base loss: 14341.53
[INFO 2017-06-28 15:20:25,131 main.py:51] epoch 11321, training loss: 8322.83, average training loss: 9231.18, base loss: 14341.23
[INFO 2017-06-28 15:20:25,790 main.py:51] epoch 11322, training loss: 9213.85, average training loss: 9232.43, base loss: 14343.94
[INFO 2017-06-28 15:20:26,463 main.py:51] epoch 11323, training loss: 9456.49, average training loss: 9232.24, base loss: 14344.66
[INFO 2017-06-28 15:20:27,121 main.py:51] epoch 11324, training loss: 9374.87, average training loss: 9233.03, base loss: 14346.03
[INFO 2017-06-28 15:20:27,778 main.py:51] epoch 11325, training loss: 8489.59, average training loss: 9230.39, base loss: 14342.08
[INFO 2017-06-28 15:20:28,447 main.py:51] epoch 11326, training loss: 7683.18, average training loss: 9228.75, base loss: 14338.26
[INFO 2017-06-28 15:20:29,101 main.py:51] epoch 11327, training loss: 8906.18, average training loss: 9228.97, base loss: 14340.18
[INFO 2017-06-28 15:20:29,777 main.py:51] epoch 11328, training loss: 8520.38, average training loss: 9227.98, base loss: 14338.03
[INFO 2017-06-28 15:20:30,444 main.py:51] epoch 11329, training loss: 8951.85, average training loss: 9227.50, base loss: 14337.02
[INFO 2017-06-28 15:20:31,108 main.py:51] epoch 11330, training loss: 8476.65, average training loss: 9226.96, base loss: 14336.15
[INFO 2017-06-28 15:20:31,778 main.py:51] epoch 11331, training loss: 9474.19, average training loss: 9226.29, base loss: 14335.41
[INFO 2017-06-28 15:20:32,488 main.py:51] epoch 11332, training loss: 9048.83, average training loss: 9226.00, base loss: 14335.16
[INFO 2017-06-28 15:20:33,150 main.py:51] epoch 11333, training loss: 9021.09, average training loss: 9225.20, base loss: 14333.51
[INFO 2017-06-28 15:20:33,792 main.py:51] epoch 11334, training loss: 10791.95, average training loss: 9226.85, base loss: 14334.59
[INFO 2017-06-28 15:20:34,438 main.py:51] epoch 11335, training loss: 9899.97, average training loss: 9228.11, base loss: 14337.73
[INFO 2017-06-28 15:20:35,136 main.py:51] epoch 11336, training loss: 8587.21, average training loss: 9227.55, base loss: 14338.20
[INFO 2017-06-28 15:20:35,783 main.py:51] epoch 11337, training loss: 9402.42, average training loss: 9227.93, base loss: 14339.09
[INFO 2017-06-28 15:20:36,430 main.py:51] epoch 11338, training loss: 10841.80, average training loss: 9229.48, base loss: 14341.82
[INFO 2017-06-28 15:20:37,103 main.py:51] epoch 11339, training loss: 9259.07, average training loss: 9229.07, base loss: 14341.96
[INFO 2017-06-28 15:20:37,775 main.py:51] epoch 11340, training loss: 9526.63, average training loss: 9230.37, base loss: 14344.64
[INFO 2017-06-28 15:20:38,451 main.py:51] epoch 11341, training loss: 8924.52, average training loss: 9230.06, base loss: 14344.72
[INFO 2017-06-28 15:20:39,131 main.py:51] epoch 11342, training loss: 9776.11, average training loss: 9228.71, base loss: 14342.76
[INFO 2017-06-28 15:20:39,777 main.py:51] epoch 11343, training loss: 8671.98, average training loss: 9228.35, base loss: 14343.30
[INFO 2017-06-28 15:20:40,433 main.py:51] epoch 11344, training loss: 8811.49, average training loss: 9227.51, base loss: 14342.46
[INFO 2017-06-28 15:20:41,101 main.py:51] epoch 11345, training loss: 8713.77, average training loss: 9226.87, base loss: 14343.30
[INFO 2017-06-28 15:20:41,747 main.py:51] epoch 11346, training loss: 8837.21, average training loss: 9226.71, base loss: 14342.31
[INFO 2017-06-28 15:20:42,405 main.py:51] epoch 11347, training loss: 8232.98, average training loss: 9225.12, base loss: 14339.41
[INFO 2017-06-28 15:20:43,071 main.py:51] epoch 11348, training loss: 11151.75, average training loss: 9227.10, base loss: 14344.02
[INFO 2017-06-28 15:20:43,726 main.py:51] epoch 11349, training loss: 9218.08, average training loss: 9227.08, base loss: 14343.70
[INFO 2017-06-28 15:20:44,393 main.py:51] epoch 11350, training loss: 10545.46, average training loss: 9228.11, base loss: 14346.85
[INFO 2017-06-28 15:20:45,059 main.py:51] epoch 11351, training loss: 9486.27, average training loss: 9227.34, base loss: 14345.47
[INFO 2017-06-28 15:20:45,732 main.py:51] epoch 11352, training loss: 8416.27, average training loss: 9226.68, base loss: 14342.90
[INFO 2017-06-28 15:20:46,441 main.py:51] epoch 11353, training loss: 8489.84, average training loss: 9226.10, base loss: 14341.16
[INFO 2017-06-28 15:20:47,110 main.py:51] epoch 11354, training loss: 7831.15, average training loss: 9223.91, base loss: 14338.36
[INFO 2017-06-28 15:20:47,773 main.py:51] epoch 11355, training loss: 9193.91, average training loss: 9224.87, base loss: 14338.78
[INFO 2017-06-28 15:20:48,442 main.py:51] epoch 11356, training loss: 9016.40, average training loss: 9223.92, base loss: 14337.82
[INFO 2017-06-28 15:20:49,126 main.py:51] epoch 11357, training loss: 8839.62, average training loss: 9223.42, base loss: 14338.50
[INFO 2017-06-28 15:20:49,806 main.py:51] epoch 11358, training loss: 9362.82, average training loss: 9221.19, base loss: 14334.70
[INFO 2017-06-28 15:20:50,477 main.py:51] epoch 11359, training loss: 9839.97, average training loss: 9222.12, base loss: 14335.72
[INFO 2017-06-28 15:20:51,141 main.py:51] epoch 11360, training loss: 9573.26, average training loss: 9221.81, base loss: 14336.08
[INFO 2017-06-28 15:20:51,803 main.py:51] epoch 11361, training loss: 8606.21, average training loss: 9222.49, base loss: 14336.92
[INFO 2017-06-28 15:20:52,448 main.py:51] epoch 11362, training loss: 10214.51, average training loss: 9223.31, base loss: 14339.65
[INFO 2017-06-28 15:20:53,119 main.py:51] epoch 11363, training loss: 8168.78, average training loss: 9221.55, base loss: 14336.01
[INFO 2017-06-28 15:20:53,783 main.py:51] epoch 11364, training loss: 9149.60, average training loss: 9222.34, base loss: 14336.81
[INFO 2017-06-28 15:20:54,438 main.py:51] epoch 11365, training loss: 8893.12, average training loss: 9221.58, base loss: 14335.43
[INFO 2017-06-28 15:20:55,103 main.py:51] epoch 11366, training loss: 8751.14, average training loss: 9220.49, base loss: 14332.87
[INFO 2017-06-28 15:20:55,753 main.py:51] epoch 11367, training loss: 8386.32, average training loss: 9219.94, base loss: 14331.70
[INFO 2017-06-28 15:20:56,407 main.py:51] epoch 11368, training loss: 8759.78, average training loss: 9219.11, base loss: 14330.09
[INFO 2017-06-28 15:20:57,059 main.py:51] epoch 11369, training loss: 8979.76, average training loss: 9218.31, base loss: 14329.39
[INFO 2017-06-28 15:20:57,698 main.py:51] epoch 11370, training loss: 9157.42, average training loss: 9217.12, base loss: 14327.38
[INFO 2017-06-28 15:20:58,353 main.py:51] epoch 11371, training loss: 9743.91, average training loss: 9218.45, base loss: 14330.82
[INFO 2017-06-28 15:20:59,028 main.py:51] epoch 11372, training loss: 9789.16, average training loss: 9218.95, base loss: 14331.43
[INFO 2017-06-28 15:20:59,689 main.py:51] epoch 11373, training loss: 9990.90, average training loss: 9219.47, base loss: 14330.18
[INFO 2017-06-28 15:21:00,334 main.py:51] epoch 11374, training loss: 8925.40, average training loss: 9219.06, base loss: 14328.61
[INFO 2017-06-28 15:21:00,994 main.py:51] epoch 11375, training loss: 9283.34, average training loss: 9218.81, base loss: 14328.28
[INFO 2017-06-28 15:21:01,659 main.py:51] epoch 11376, training loss: 9250.60, average training loss: 9220.02, base loss: 14332.07
[INFO 2017-06-28 15:21:02,328 main.py:51] epoch 11377, training loss: 9277.38, average training loss: 9219.31, base loss: 14330.44
[INFO 2017-06-28 15:21:02,998 main.py:51] epoch 11378, training loss: 9888.51, average training loss: 9219.27, base loss: 14329.53
[INFO 2017-06-28 15:21:03,659 main.py:51] epoch 11379, training loss: 8511.03, average training loss: 9219.36, base loss: 14330.77
[INFO 2017-06-28 15:21:04,319 main.py:51] epoch 11380, training loss: 9438.37, average training loss: 9220.08, base loss: 14331.86
[INFO 2017-06-28 15:21:04,957 main.py:51] epoch 11381, training loss: 7885.93, average training loss: 9218.13, base loss: 14328.39
[INFO 2017-06-28 15:21:05,623 main.py:51] epoch 11382, training loss: 9288.30, average training loss: 9218.87, base loss: 14330.49
[INFO 2017-06-28 15:21:06,281 main.py:51] epoch 11383, training loss: 9671.53, average training loss: 9220.81, base loss: 14334.52
[INFO 2017-06-28 15:21:06,955 main.py:51] epoch 11384, training loss: 8914.90, average training loss: 9221.10, base loss: 14335.13
[INFO 2017-06-28 15:21:07,620 main.py:51] epoch 11385, training loss: 9572.11, average training loss: 9220.72, base loss: 14334.12
[INFO 2017-06-28 15:21:08,296 main.py:51] epoch 11386, training loss: 8832.97, average training loss: 9220.79, base loss: 14334.87
[INFO 2017-06-28 15:21:08,954 main.py:51] epoch 11387, training loss: 8735.85, average training loss: 9220.84, base loss: 14335.05
[INFO 2017-06-28 15:21:09,639 main.py:51] epoch 11388, training loss: 10011.29, average training loss: 9221.17, base loss: 14334.35
[INFO 2017-06-28 15:21:10,286 main.py:51] epoch 11389, training loss: 8931.71, average training loss: 9221.59, base loss: 14336.00
[INFO 2017-06-28 15:21:10,949 main.py:51] epoch 11390, training loss: 8431.19, average training loss: 9220.75, base loss: 14333.82
[INFO 2017-06-28 15:21:11,607 main.py:51] epoch 11391, training loss: 9275.31, average training loss: 9221.86, base loss: 14335.44
[INFO 2017-06-28 15:21:12,282 main.py:51] epoch 11392, training loss: 9859.27, average training loss: 9222.10, base loss: 14336.70
[INFO 2017-06-28 15:21:12,965 main.py:51] epoch 11393, training loss: 8914.03, average training loss: 9222.26, base loss: 14335.95
[INFO 2017-06-28 15:21:13,640 main.py:51] epoch 11394, training loss: 9449.28, average training loss: 9221.59, base loss: 14336.74
[INFO 2017-06-28 15:21:14,322 main.py:51] epoch 11395, training loss: 8632.85, average training loss: 9221.11, base loss: 14334.71
[INFO 2017-06-28 15:21:15,002 main.py:51] epoch 11396, training loss: 9344.32, average training loss: 9222.49, base loss: 14335.56
[INFO 2017-06-28 15:21:15,669 main.py:51] epoch 11397, training loss: 7863.77, average training loss: 9221.10, base loss: 14331.92
[INFO 2017-06-28 15:21:16,324 main.py:51] epoch 11398, training loss: 9449.06, average training loss: 9220.85, base loss: 14330.75
[INFO 2017-06-28 15:21:17,010 main.py:51] epoch 11399, training loss: 9782.77, average training loss: 9220.76, base loss: 14331.82
[INFO 2017-06-28 15:21:17,010 main.py:53] epoch 11399, testing
[INFO 2017-06-28 15:21:19,607 main.py:105] average testing loss: 11188.03, base loss: 16140.25
[INFO 2017-06-28 15:21:19,608 main.py:106] improve_loss: 4952.22, improve_percent: 0.31
[INFO 2017-06-28 15:21:19,608 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:21:20,271 main.py:51] epoch 11400, training loss: 8064.17, average training loss: 9219.23, base loss: 14330.55
[INFO 2017-06-28 15:21:20,947 main.py:51] epoch 11401, training loss: 8423.31, average training loss: 9218.96, base loss: 14331.33
[INFO 2017-06-28 15:21:21,600 main.py:51] epoch 11402, training loss: 7951.06, average training loss: 9217.65, base loss: 14329.20
[INFO 2017-06-28 15:21:22,246 main.py:51] epoch 11403, training loss: 8218.92, average training loss: 9216.91, base loss: 14328.90
[INFO 2017-06-28 15:21:22,895 main.py:51] epoch 11404, training loss: 10516.44, average training loss: 9218.07, base loss: 14331.65
[INFO 2017-06-28 15:21:23,560 main.py:51] epoch 11405, training loss: 9346.36, average training loss: 9218.06, base loss: 14331.67
[INFO 2017-06-28 15:21:24,229 main.py:51] epoch 11406, training loss: 8936.45, average training loss: 9217.47, base loss: 14331.24
[INFO 2017-06-28 15:21:24,882 main.py:51] epoch 11407, training loss: 7923.46, average training loss: 9215.52, base loss: 14329.59
[INFO 2017-06-28 15:21:25,533 main.py:51] epoch 11408, training loss: 8952.76, average training loss: 9215.11, base loss: 14328.46
[INFO 2017-06-28 15:21:26,179 main.py:51] epoch 11409, training loss: 8934.96, average training loss: 9216.18, base loss: 14330.90
[INFO 2017-06-28 15:21:26,821 main.py:51] epoch 11410, training loss: 8787.42, average training loss: 9214.63, base loss: 14327.49
[INFO 2017-06-28 15:21:27,465 main.py:51] epoch 11411, training loss: 8781.90, average training loss: 9213.95, base loss: 14325.87
[INFO 2017-06-28 15:21:28,122 main.py:51] epoch 11412, training loss: 9746.52, average training loss: 9213.50, base loss: 14325.74
[INFO 2017-06-28 15:21:28,763 main.py:51] epoch 11413, training loss: 9082.80, average training loss: 9213.66, base loss: 14325.99
[INFO 2017-06-28 15:21:29,418 main.py:51] epoch 11414, training loss: 8246.71, average training loss: 9213.23, base loss: 14325.50
[INFO 2017-06-28 15:21:30,090 main.py:51] epoch 11415, training loss: 8607.23, average training loss: 9212.57, base loss: 14324.48
[INFO 2017-06-28 15:21:30,746 main.py:51] epoch 11416, training loss: 11135.45, average training loss: 9214.31, base loss: 14327.52
[INFO 2017-06-28 15:21:31,400 main.py:51] epoch 11417, training loss: 8510.08, average training loss: 9214.37, base loss: 14327.69
[INFO 2017-06-28 15:21:32,099 main.py:51] epoch 11418, training loss: 8300.18, average training loss: 9213.38, base loss: 14325.35
[INFO 2017-06-28 15:21:32,762 main.py:51] epoch 11419, training loss: 8550.09, average training loss: 9212.25, base loss: 14323.50
[INFO 2017-06-28 15:21:33,450 main.py:51] epoch 11420, training loss: 9976.92, average training loss: 9213.07, base loss: 14326.01
[INFO 2017-06-28 15:21:34,087 main.py:51] epoch 11421, training loss: 10655.89, average training loss: 9214.70, base loss: 14328.24
[INFO 2017-06-28 15:21:34,766 main.py:51] epoch 11422, training loss: 8366.03, average training loss: 9214.02, base loss: 14327.46
[INFO 2017-06-28 15:21:35,417 main.py:51] epoch 11423, training loss: 8473.76, average training loss: 9213.55, base loss: 14325.60
[INFO 2017-06-28 15:21:36,071 main.py:51] epoch 11424, training loss: 7975.61, average training loss: 9211.68, base loss: 14323.48
[INFO 2017-06-28 15:21:36,745 main.py:51] epoch 11425, training loss: 8907.68, average training loss: 9211.77, base loss: 14322.95
[INFO 2017-06-28 15:21:37,401 main.py:51] epoch 11426, training loss: 8715.35, average training loss: 9210.45, base loss: 14320.37
[INFO 2017-06-28 15:21:38,055 main.py:51] epoch 11427, training loss: 7695.00, average training loss: 9209.98, base loss: 14320.51
[INFO 2017-06-28 15:21:38,714 main.py:51] epoch 11428, training loss: 8560.73, average training loss: 9208.76, base loss: 14316.03
[INFO 2017-06-28 15:21:39,363 main.py:51] epoch 11429, training loss: 10214.23, average training loss: 9207.94, base loss: 14313.65
[INFO 2017-06-28 15:21:40,022 main.py:51] epoch 11430, training loss: 10165.25, average training loss: 9208.48, base loss: 14314.29
[INFO 2017-06-28 15:21:40,681 main.py:51] epoch 11431, training loss: 9356.30, average training loss: 9207.10, base loss: 14312.86
[INFO 2017-06-28 15:21:41,338 main.py:51] epoch 11432, training loss: 8426.24, average training loss: 9206.72, base loss: 14313.40
[INFO 2017-06-28 15:21:41,969 main.py:51] epoch 11433, training loss: 9379.32, average training loss: 9207.14, base loss: 14313.19
[INFO 2017-06-28 15:21:42,648 main.py:51] epoch 11434, training loss: 9515.89, average training loss: 9207.93, base loss: 14315.54
[INFO 2017-06-28 15:21:43,307 main.py:51] epoch 11435, training loss: 9143.44, average training loss: 9207.36, base loss: 14315.21
[INFO 2017-06-28 15:21:43,946 main.py:51] epoch 11436, training loss: 9363.87, average training loss: 9208.39, base loss: 14315.97
[INFO 2017-06-28 15:21:44,614 main.py:51] epoch 11437, training loss: 8853.97, average training loss: 9207.91, base loss: 14314.92
[INFO 2017-06-28 15:21:45,296 main.py:51] epoch 11438, training loss: 9683.06, average training loss: 9207.97, base loss: 14315.31
[INFO 2017-06-28 15:21:45,960 main.py:51] epoch 11439, training loss: 9190.73, average training loss: 9209.28, base loss: 14318.35
[INFO 2017-06-28 15:21:46,624 main.py:51] epoch 11440, training loss: 9329.17, average training loss: 9209.26, base loss: 14318.90
[INFO 2017-06-28 15:21:47,289 main.py:51] epoch 11441, training loss: 9544.44, average training loss: 9207.47, base loss: 14316.86
[INFO 2017-06-28 15:21:47,976 main.py:51] epoch 11442, training loss: 9287.59, average training loss: 9207.99, base loss: 14317.20
[INFO 2017-06-28 15:21:48,631 main.py:51] epoch 11443, training loss: 10317.62, average training loss: 9209.19, base loss: 14319.86
[INFO 2017-06-28 15:21:49,277 main.py:51] epoch 11444, training loss: 9379.02, average training loss: 9210.52, base loss: 14322.69
[INFO 2017-06-28 15:21:49,938 main.py:51] epoch 11445, training loss: 8449.74, average training loss: 9209.97, base loss: 14322.90
[INFO 2017-06-28 15:21:50,587 main.py:51] epoch 11446, training loss: 8864.94, average training loss: 9210.19, base loss: 14324.25
[INFO 2017-06-28 15:21:51,272 main.py:51] epoch 11447, training loss: 9785.77, average training loss: 9211.95, base loss: 14327.02
[INFO 2017-06-28 15:21:51,928 main.py:51] epoch 11448, training loss: 10940.91, average training loss: 9214.20, base loss: 14329.67
[INFO 2017-06-28 15:21:52,601 main.py:51] epoch 11449, training loss: 9784.43, average training loss: 9215.48, base loss: 14331.41
[INFO 2017-06-28 15:21:53,272 main.py:51] epoch 11450, training loss: 9231.02, average training loss: 9215.21, base loss: 14332.57
[INFO 2017-06-28 15:21:53,964 main.py:51] epoch 11451, training loss: 8597.73, average training loss: 9214.80, base loss: 14332.48
[INFO 2017-06-28 15:21:54,612 main.py:51] epoch 11452, training loss: 7898.37, average training loss: 9213.68, base loss: 14331.30
[INFO 2017-06-28 15:21:55,310 main.py:51] epoch 11453, training loss: 9337.84, average training loss: 9214.59, base loss: 14332.93
[INFO 2017-06-28 15:21:55,969 main.py:51] epoch 11454, training loss: 8569.42, average training loss: 9213.89, base loss: 14331.73
[INFO 2017-06-28 15:21:56,620 main.py:51] epoch 11455, training loss: 9929.42, average training loss: 9214.01, base loss: 14331.52
[INFO 2017-06-28 15:21:57,304 main.py:51] epoch 11456, training loss: 10179.17, average training loss: 9214.20, base loss: 14333.22
[INFO 2017-06-28 15:21:57,961 main.py:51] epoch 11457, training loss: 8464.92, average training loss: 9213.07, base loss: 14331.58
[INFO 2017-06-28 15:21:58,625 main.py:51] epoch 11458, training loss: 9274.18, average training loss: 9213.39, base loss: 14333.40
[INFO 2017-06-28 15:21:59,288 main.py:51] epoch 11459, training loss: 9726.11, average training loss: 9213.70, base loss: 14332.73
[INFO 2017-06-28 15:21:59,932 main.py:51] epoch 11460, training loss: 11152.14, average training loss: 9216.13, base loss: 14336.14
[INFO 2017-06-28 15:22:00,603 main.py:51] epoch 11461, training loss: 10363.84, average training loss: 9218.11, base loss: 14338.51
[INFO 2017-06-28 15:22:01,262 main.py:51] epoch 11462, training loss: 9017.61, average training loss: 9217.66, base loss: 14336.30
[INFO 2017-06-28 15:22:01,917 main.py:51] epoch 11463, training loss: 9739.38, average training loss: 9219.13, base loss: 14338.36
[INFO 2017-06-28 15:22:02,569 main.py:51] epoch 11464, training loss: 10435.83, average training loss: 9220.54, base loss: 14340.79
[INFO 2017-06-28 15:22:03,222 main.py:51] epoch 11465, training loss: 8251.88, average training loss: 9219.63, base loss: 14339.88
[INFO 2017-06-28 15:22:03,871 main.py:51] epoch 11466, training loss: 10394.75, average training loss: 9221.72, base loss: 14342.29
[INFO 2017-06-28 15:22:04,541 main.py:51] epoch 11467, training loss: 9214.34, average training loss: 9220.55, base loss: 14341.27
[INFO 2017-06-28 15:22:05,205 main.py:51] epoch 11468, training loss: 11198.94, average training loss: 9222.37, base loss: 14343.33
[INFO 2017-06-28 15:22:05,841 main.py:51] epoch 11469, training loss: 7911.77, average training loss: 9220.66, base loss: 14339.41
[INFO 2017-06-28 15:22:06,500 main.py:51] epoch 11470, training loss: 10991.01, average training loss: 9221.97, base loss: 14341.75
[INFO 2017-06-28 15:22:07,150 main.py:51] epoch 11471, training loss: 8870.55, average training loss: 9221.58, base loss: 14341.39
[INFO 2017-06-28 15:22:07,795 main.py:51] epoch 11472, training loss: 9325.59, average training loss: 9223.07, base loss: 14345.00
[INFO 2017-06-28 15:22:08,432 main.py:51] epoch 11473, training loss: 8055.92, average training loss: 9221.69, base loss: 14342.73
[INFO 2017-06-28 15:22:09,109 main.py:51] epoch 11474, training loss: 9874.98, average training loss: 9221.36, base loss: 14341.40
[INFO 2017-06-28 15:22:09,754 main.py:51] epoch 11475, training loss: 10160.89, average training loss: 9221.22, base loss: 14340.10
[INFO 2017-06-28 15:22:10,442 main.py:51] epoch 11476, training loss: 9434.41, average training loss: 9221.01, base loss: 14341.15
[INFO 2017-06-28 15:22:11,113 main.py:51] epoch 11477, training loss: 9513.40, average training loss: 9221.12, base loss: 14341.61
[INFO 2017-06-28 15:22:11,791 main.py:51] epoch 11478, training loss: 8963.21, average training loss: 9220.17, base loss: 14339.80
[INFO 2017-06-28 15:22:12,438 main.py:51] epoch 11479, training loss: 9626.29, average training loss: 9220.05, base loss: 14338.13
[INFO 2017-06-28 15:22:13,103 main.py:51] epoch 11480, training loss: 9057.51, average training loss: 9220.71, base loss: 14340.80
[INFO 2017-06-28 15:22:13,744 main.py:51] epoch 11481, training loss: 9714.58, average training loss: 9221.51, base loss: 14341.66
[INFO 2017-06-28 15:22:14,399 main.py:51] epoch 11482, training loss: 9269.44, average training loss: 9221.86, base loss: 14344.37
[INFO 2017-06-28 15:22:15,070 main.py:51] epoch 11483, training loss: 9434.96, average training loss: 9221.69, base loss: 14344.28
[INFO 2017-06-28 15:22:15,727 main.py:51] epoch 11484, training loss: 10870.88, average training loss: 9222.25, base loss: 14345.98
[INFO 2017-06-28 15:22:16,380 main.py:51] epoch 11485, training loss: 8961.87, average training loss: 9221.72, base loss: 14346.28
[INFO 2017-06-28 15:22:17,035 main.py:51] epoch 11486, training loss: 9122.20, average training loss: 9222.50, base loss: 14347.83
[INFO 2017-06-28 15:22:17,689 main.py:51] epoch 11487, training loss: 9103.05, average training loss: 9222.69, base loss: 14348.61
[INFO 2017-06-28 15:22:18,338 main.py:51] epoch 11488, training loss: 9609.48, average training loss: 9223.28, base loss: 14350.55
[INFO 2017-06-28 15:22:19,006 main.py:51] epoch 11489, training loss: 9904.63, average training loss: 9223.12, base loss: 14352.09
[INFO 2017-06-28 15:22:19,655 main.py:51] epoch 11490, training loss: 8711.54, average training loss: 9221.82, base loss: 14350.04
[INFO 2017-06-28 15:22:20,352 main.py:51] epoch 11491, training loss: 7812.37, average training loss: 9219.75, base loss: 14346.10
[INFO 2017-06-28 15:22:21,016 main.py:51] epoch 11492, training loss: 7844.56, average training loss: 9217.52, base loss: 14343.40
[INFO 2017-06-28 15:22:21,696 main.py:51] epoch 11493, training loss: 8272.28, average training loss: 9216.45, base loss: 14340.52
[INFO 2017-06-28 15:22:22,353 main.py:51] epoch 11494, training loss: 9501.62, average training loss: 9217.64, base loss: 14342.76
[INFO 2017-06-28 15:22:23,007 main.py:51] epoch 11495, training loss: 9242.74, average training loss: 9217.49, base loss: 14341.98
[INFO 2017-06-28 15:22:23,652 main.py:51] epoch 11496, training loss: 10807.90, average training loss: 9220.04, base loss: 14346.91
[INFO 2017-06-28 15:22:24,326 main.py:51] epoch 11497, training loss: 10622.23, average training loss: 9221.06, base loss: 14349.45
[INFO 2017-06-28 15:22:24,995 main.py:51] epoch 11498, training loss: 9547.73, average training loss: 9221.55, base loss: 14350.68
[INFO 2017-06-28 15:22:25,672 main.py:51] epoch 11499, training loss: 9265.11, average training loss: 9221.37, base loss: 14351.31
[INFO 2017-06-28 15:22:25,673 main.py:53] epoch 11499, testing
[INFO 2017-06-28 15:22:28,287 main.py:105] average testing loss: 11152.73, base loss: 15877.51
[INFO 2017-06-28 15:22:28,287 main.py:106] improve_loss: 4724.79, improve_percent: 0.30
[INFO 2017-06-28 15:22:28,288 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:22:28,963 main.py:51] epoch 11500, training loss: 8300.60, average training loss: 9220.95, base loss: 14350.97
[INFO 2017-06-28 15:22:29,617 main.py:51] epoch 11501, training loss: 9875.58, average training loss: 9220.94, base loss: 14353.09
[INFO 2017-06-28 15:22:30,302 main.py:51] epoch 11502, training loss: 10521.62, average training loss: 9223.13, base loss: 14355.79
[INFO 2017-06-28 15:22:30,974 main.py:51] epoch 11503, training loss: 10408.17, average training loss: 9224.92, base loss: 14359.45
[INFO 2017-06-28 15:22:31,643 main.py:51] epoch 11504, training loss: 9029.94, average training loss: 9224.62, base loss: 14359.31
[INFO 2017-06-28 15:22:32,313 main.py:51] epoch 11505, training loss: 9844.43, average training loss: 9225.69, base loss: 14359.72
[INFO 2017-06-28 15:22:32,972 main.py:51] epoch 11506, training loss: 9607.89, average training loss: 9225.12, base loss: 14359.66
[INFO 2017-06-28 15:22:33,641 main.py:51] epoch 11507, training loss: 9514.32, average training loss: 9224.45, base loss: 14358.18
[INFO 2017-06-28 15:22:34,310 main.py:51] epoch 11508, training loss: 8903.46, average training loss: 9224.33, base loss: 14357.95
[INFO 2017-06-28 15:22:34,971 main.py:51] epoch 11509, training loss: 9385.34, average training loss: 9223.79, base loss: 14357.24
[INFO 2017-06-28 15:22:35,631 main.py:51] epoch 11510, training loss: 8743.54, average training loss: 9223.36, base loss: 14358.14
[INFO 2017-06-28 15:22:36,296 main.py:51] epoch 11511, training loss: 9203.98, average training loss: 9223.83, base loss: 14360.77
[INFO 2017-06-28 15:22:36,984 main.py:51] epoch 11512, training loss: 8275.78, average training loss: 9222.27, base loss: 14359.09
[INFO 2017-06-28 15:22:37,654 main.py:51] epoch 11513, training loss: 9004.71, average training loss: 9222.20, base loss: 14357.88
[INFO 2017-06-28 15:22:38,308 main.py:51] epoch 11514, training loss: 9365.47, average training loss: 9221.86, base loss: 14356.27
[INFO 2017-06-28 15:22:38,980 main.py:51] epoch 11515, training loss: 9126.08, average training loss: 9221.74, base loss: 14356.39
[INFO 2017-06-28 15:22:39,660 main.py:51] epoch 11516, training loss: 8468.61, average training loss: 9220.97, base loss: 14354.20
[INFO 2017-06-28 15:22:40,321 main.py:51] epoch 11517, training loss: 9510.63, average training loss: 9221.88, base loss: 14357.36
[INFO 2017-06-28 15:22:40,976 main.py:51] epoch 11518, training loss: 8402.73, average training loss: 9221.77, base loss: 14357.68
[INFO 2017-06-28 15:22:41,630 main.py:51] epoch 11519, training loss: 9710.75, average training loss: 9222.64, base loss: 14359.94
[INFO 2017-06-28 15:22:42,282 main.py:51] epoch 11520, training loss: 8562.03, average training loss: 9222.81, base loss: 14359.52
[INFO 2017-06-28 15:22:42,935 main.py:51] epoch 11521, training loss: 8993.56, average training loss: 9223.11, base loss: 14360.84
[INFO 2017-06-28 15:22:43,592 main.py:51] epoch 11522, training loss: 9603.28, average training loss: 9224.77, base loss: 14363.90
[INFO 2017-06-28 15:22:44,255 main.py:51] epoch 11523, training loss: 8866.20, average training loss: 9225.56, base loss: 14365.55
[INFO 2017-06-28 15:22:44,909 main.py:51] epoch 11524, training loss: 8932.36, average training loss: 9225.58, base loss: 14365.54
[INFO 2017-06-28 15:22:45,584 main.py:51] epoch 11525, training loss: 10136.87, average training loss: 9226.13, base loss: 14364.60
[INFO 2017-06-28 15:22:46,251 main.py:51] epoch 11526, training loss: 9274.32, average training loss: 9225.92, base loss: 14365.86
[INFO 2017-06-28 15:22:46,904 main.py:51] epoch 11527, training loss: 9583.52, average training loss: 9225.57, base loss: 14365.95
[INFO 2017-06-28 15:22:47,569 main.py:51] epoch 11528, training loss: 8441.17, average training loss: 9224.54, base loss: 14364.58
[INFO 2017-06-28 15:22:48,236 main.py:51] epoch 11529, training loss: 9424.17, average training loss: 9224.96, base loss: 14363.87
[INFO 2017-06-28 15:22:48,933 main.py:51] epoch 11530, training loss: 8400.88, average training loss: 9224.59, base loss: 14363.98
[INFO 2017-06-28 15:22:49,599 main.py:51] epoch 11531, training loss: 9391.57, average training loss: 9225.70, base loss: 14365.25
[INFO 2017-06-28 15:22:50,262 main.py:51] epoch 11532, training loss: 9139.95, average training loss: 9226.13, base loss: 14367.07
[INFO 2017-06-28 15:22:50,932 main.py:51] epoch 11533, training loss: 9091.37, average training loss: 9224.22, base loss: 14364.31
[INFO 2017-06-28 15:22:51,621 main.py:51] epoch 11534, training loss: 9441.21, average training loss: 9221.83, base loss: 14361.12
[INFO 2017-06-28 15:22:52,275 main.py:51] epoch 11535, training loss: 9493.66, average training loss: 9221.72, base loss: 14359.79
[INFO 2017-06-28 15:22:52,948 main.py:51] epoch 11536, training loss: 8813.91, average training loss: 9221.68, base loss: 14361.89
[INFO 2017-06-28 15:22:53,614 main.py:51] epoch 11537, training loss: 7828.80, average training loss: 9219.67, base loss: 14357.54
[INFO 2017-06-28 15:22:54,268 main.py:51] epoch 11538, training loss: 10270.78, average training loss: 9220.17, base loss: 14357.73
[INFO 2017-06-28 15:22:54,916 main.py:51] epoch 11539, training loss: 9711.64, average training loss: 9220.15, base loss: 14357.22
[INFO 2017-06-28 15:22:55,595 main.py:51] epoch 11540, training loss: 8676.96, average training loss: 9219.96, base loss: 14356.38
[INFO 2017-06-28 15:22:56,291 main.py:51] epoch 11541, training loss: 9084.35, average training loss: 9219.80, base loss: 14356.35
[INFO 2017-06-28 15:22:56,957 main.py:51] epoch 11542, training loss: 10461.31, average training loss: 9220.86, base loss: 14357.61
[INFO 2017-06-28 15:22:57,603 main.py:51] epoch 11543, training loss: 8727.59, average training loss: 9220.96, base loss: 14359.21
[INFO 2017-06-28 15:22:58,264 main.py:51] epoch 11544, training loss: 8835.38, average training loss: 9220.71, base loss: 14359.97
[INFO 2017-06-28 15:22:58,939 main.py:51] epoch 11545, training loss: 9789.64, average training loss: 9221.60, base loss: 14360.13
[INFO 2017-06-28 15:22:59,578 main.py:51] epoch 11546, training loss: 9637.53, average training loss: 9222.59, base loss: 14361.43
[INFO 2017-06-28 15:23:00,252 main.py:51] epoch 11547, training loss: 8494.46, average training loss: 9221.02, base loss: 14357.63
[INFO 2017-06-28 15:23:00,903 main.py:51] epoch 11548, training loss: 10840.62, average training loss: 9223.33, base loss: 14360.58
[INFO 2017-06-28 15:23:01,577 main.py:51] epoch 11549, training loss: 8366.49, average training loss: 9222.85, base loss: 14359.71
[INFO 2017-06-28 15:23:02,226 main.py:51] epoch 11550, training loss: 10069.58, average training loss: 9224.06, base loss: 14363.69
[INFO 2017-06-28 15:23:02,904 main.py:51] epoch 11551, training loss: 8298.26, average training loss: 9223.63, base loss: 14362.86
[INFO 2017-06-28 15:23:03,569 main.py:51] epoch 11552, training loss: 8713.76, average training loss: 9221.31, base loss: 14359.13
[INFO 2017-06-28 15:23:04,213 main.py:51] epoch 11553, training loss: 7910.70, average training loss: 9218.54, base loss: 14354.59
[INFO 2017-06-28 15:23:04,869 main.py:51] epoch 11554, training loss: 9614.92, average training loss: 9219.30, base loss: 14357.99
[INFO 2017-06-28 15:23:05,531 main.py:51] epoch 11555, training loss: 9803.21, average training loss: 9220.41, base loss: 14359.91
[INFO 2017-06-28 15:23:06,190 main.py:51] epoch 11556, training loss: 9856.97, average training loss: 9221.35, base loss: 14362.57
[INFO 2017-06-28 15:23:06,836 main.py:51] epoch 11557, training loss: 9395.37, average training loss: 9221.44, base loss: 14361.15
[INFO 2017-06-28 15:23:07,519 main.py:51] epoch 11558, training loss: 9379.02, average training loss: 9221.28, base loss: 14362.37
[INFO 2017-06-28 15:23:08,179 main.py:51] epoch 11559, training loss: 8937.24, average training loss: 9221.38, base loss: 14361.55
[INFO 2017-06-28 15:23:08,851 main.py:51] epoch 11560, training loss: 9129.82, average training loss: 9221.64, base loss: 14361.55
[INFO 2017-06-28 15:23:09,502 main.py:51] epoch 11561, training loss: 10050.92, average training loss: 9221.08, base loss: 14358.56
[INFO 2017-06-28 15:23:10,142 main.py:51] epoch 11562, training loss: 8935.15, average training loss: 9220.86, base loss: 14359.00
[INFO 2017-06-28 15:23:10,826 main.py:51] epoch 11563, training loss: 8921.42, average training loss: 9220.11, base loss: 14357.62
[INFO 2017-06-28 15:23:11,500 main.py:51] epoch 11564, training loss: 8531.39, average training loss: 9219.10, base loss: 14355.66
[INFO 2017-06-28 15:23:12,186 main.py:51] epoch 11565, training loss: 9138.15, average training loss: 9219.95, base loss: 14357.36
[INFO 2017-06-28 15:23:12,841 main.py:51] epoch 11566, training loss: 8721.59, average training loss: 9219.25, base loss: 14357.79
[INFO 2017-06-28 15:23:13,507 main.py:51] epoch 11567, training loss: 9049.37, average training loss: 9219.89, base loss: 14359.78
[INFO 2017-06-28 15:23:14,145 main.py:51] epoch 11568, training loss: 8431.42, average training loss: 9219.53, base loss: 14360.54
[INFO 2017-06-28 15:23:14,848 main.py:51] epoch 11569, training loss: 9486.31, average training loss: 9220.68, base loss: 14362.79
[INFO 2017-06-28 15:23:15,497 main.py:51] epoch 11570, training loss: 10250.57, average training loss: 9220.70, base loss: 14364.09
[INFO 2017-06-28 15:23:16,154 main.py:51] epoch 11571, training loss: 10037.79, average training loss: 9220.90, base loss: 14364.05
[INFO 2017-06-28 15:23:16,842 main.py:51] epoch 11572, training loss: 8893.40, average training loss: 9220.80, base loss: 14364.00
[INFO 2017-06-28 15:23:17,488 main.py:51] epoch 11573, training loss: 8815.94, average training loss: 9219.19, base loss: 14360.29
[INFO 2017-06-28 15:23:18,169 main.py:51] epoch 11574, training loss: 8609.20, average training loss: 9218.81, base loss: 14359.39
[INFO 2017-06-28 15:23:18,829 main.py:51] epoch 11575, training loss: 9378.71, average training loss: 9218.93, base loss: 14360.43
[INFO 2017-06-28 15:23:19,470 main.py:51] epoch 11576, training loss: 9627.88, average training loss: 9218.21, base loss: 14358.29
[INFO 2017-06-28 15:23:20,142 main.py:51] epoch 11577, training loss: 9558.47, average training loss: 9219.33, base loss: 14359.40
[INFO 2017-06-28 15:23:20,815 main.py:51] epoch 11578, training loss: 9334.16, average training loss: 9219.72, base loss: 14361.66
[INFO 2017-06-28 15:23:21,481 main.py:51] epoch 11579, training loss: 8999.68, average training loss: 9219.93, base loss: 14362.27
[INFO 2017-06-28 15:23:22,121 main.py:51] epoch 11580, training loss: 7973.82, average training loss: 9218.30, base loss: 14358.91
[INFO 2017-06-28 15:23:22,812 main.py:51] epoch 11581, training loss: 9329.80, average training loss: 9217.37, base loss: 14358.62
[INFO 2017-06-28 15:23:23,468 main.py:51] epoch 11582, training loss: 8843.39, average training loss: 9216.75, base loss: 14358.86
[INFO 2017-06-28 15:23:24,114 main.py:51] epoch 11583, training loss: 9695.83, average training loss: 9217.01, base loss: 14360.25
[INFO 2017-06-28 15:23:24,772 main.py:51] epoch 11584, training loss: 9828.53, average training loss: 9216.75, base loss: 14360.09
[INFO 2017-06-28 15:23:25,453 main.py:51] epoch 11585, training loss: 8969.50, average training loss: 9217.70, base loss: 14362.50
[INFO 2017-06-28 15:23:26,112 main.py:51] epoch 11586, training loss: 10922.20, average training loss: 9218.93, base loss: 14363.42
[INFO 2017-06-28 15:23:26,765 main.py:51] epoch 11587, training loss: 10541.68, average training loss: 9219.91, base loss: 14366.25
[INFO 2017-06-28 15:23:27,430 main.py:51] epoch 11588, training loss: 9805.94, average training loss: 9220.68, base loss: 14367.41
[INFO 2017-06-28 15:23:28,077 main.py:51] epoch 11589, training loss: 9777.10, average training loss: 9221.32, base loss: 14367.62
[INFO 2017-06-28 15:23:28,747 main.py:51] epoch 11590, training loss: 8303.31, average training loss: 9219.91, base loss: 14366.49
[INFO 2017-06-28 15:23:29,472 main.py:51] epoch 11591, training loss: 8767.30, average training loss: 9217.73, base loss: 14363.45
[INFO 2017-06-28 15:23:30,140 main.py:51] epoch 11592, training loss: 8925.27, average training loss: 9217.28, base loss: 14362.54
[INFO 2017-06-28 15:23:30,801 main.py:51] epoch 11593, training loss: 11255.20, average training loss: 9220.09, base loss: 14366.91
[INFO 2017-06-28 15:23:31,464 main.py:51] epoch 11594, training loss: 8859.36, average training loss: 9220.00, base loss: 14367.19
[INFO 2017-06-28 15:23:32,133 main.py:51] epoch 11595, training loss: 8105.79, average training loss: 9219.23, base loss: 14364.44
[INFO 2017-06-28 15:23:32,787 main.py:51] epoch 11596, training loss: 8456.28, average training loss: 9218.14, base loss: 14363.07
[INFO 2017-06-28 15:23:33,429 main.py:51] epoch 11597, training loss: 8994.96, average training loss: 9218.25, base loss: 14363.43
[INFO 2017-06-28 15:23:34,084 main.py:51] epoch 11598, training loss: 9139.02, average training loss: 9218.40, base loss: 14363.34
[INFO 2017-06-28 15:23:34,748 main.py:51] epoch 11599, training loss: 8905.23, average training loss: 9217.56, base loss: 14362.75
[INFO 2017-06-28 15:23:34,748 main.py:53] epoch 11599, testing
[INFO 2017-06-28 15:23:37,389 main.py:105] average testing loss: 10782.06, base loss: 15271.57
[INFO 2017-06-28 15:23:37,390 main.py:106] improve_loss: 4489.51, improve_percent: 0.29
[INFO 2017-06-28 15:23:37,390 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:23:38,034 main.py:51] epoch 11600, training loss: 9036.37, average training loss: 9217.43, base loss: 14362.65
[INFO 2017-06-28 15:23:38,674 main.py:51] epoch 11601, training loss: 8985.09, average training loss: 9215.78, base loss: 14361.41
[INFO 2017-06-28 15:23:39,322 main.py:51] epoch 11602, training loss: 10756.28, average training loss: 9217.73, base loss: 14364.01
[INFO 2017-06-28 15:23:40,001 main.py:51] epoch 11603, training loss: 8513.47, average training loss: 9216.38, base loss: 14361.73
[INFO 2017-06-28 15:23:40,640 main.py:51] epoch 11604, training loss: 8610.95, average training loss: 9214.46, base loss: 14357.83
[INFO 2017-06-28 15:23:41,286 main.py:51] epoch 11605, training loss: 8847.68, average training loss: 9214.00, base loss: 14358.04
[INFO 2017-06-28 15:23:41,930 main.py:51] epoch 11606, training loss: 8366.70, average training loss: 9213.26, base loss: 14357.71
[INFO 2017-06-28 15:23:42,578 main.py:51] epoch 11607, training loss: 9225.92, average training loss: 9213.29, base loss: 14356.98
[INFO 2017-06-28 15:23:43,259 main.py:51] epoch 11608, training loss: 9256.73, average training loss: 9213.92, base loss: 14358.63
[INFO 2017-06-28 15:23:43,920 main.py:51] epoch 11609, training loss: 8182.59, average training loss: 9212.87, base loss: 14357.14
[INFO 2017-06-28 15:23:44,558 main.py:51] epoch 11610, training loss: 8353.84, average training loss: 9212.19, base loss: 14356.96
[INFO 2017-06-28 15:23:45,221 main.py:51] epoch 11611, training loss: 9160.38, average training loss: 9210.85, base loss: 14354.31
[INFO 2017-06-28 15:23:45,873 main.py:51] epoch 11612, training loss: 8567.21, average training loss: 9208.77, base loss: 14351.15
[INFO 2017-06-28 15:23:46,543 main.py:51] epoch 11613, training loss: 9379.17, average training loss: 9208.82, base loss: 14351.10
[INFO 2017-06-28 15:23:47,203 main.py:51] epoch 11614, training loss: 9277.91, average training loss: 9209.80, base loss: 14353.94
[INFO 2017-06-28 15:23:47,835 main.py:51] epoch 11615, training loss: 9948.22, average training loss: 9209.77, base loss: 14354.88
[INFO 2017-06-28 15:23:48,489 main.py:51] epoch 11616, training loss: 9484.18, average training loss: 9210.19, base loss: 14356.12
[INFO 2017-06-28 15:23:49,179 main.py:51] epoch 11617, training loss: 8755.87, average training loss: 9210.07, base loss: 14355.90
[INFO 2017-06-28 15:23:49,846 main.py:51] epoch 11618, training loss: 8579.05, average training loss: 9209.35, base loss: 14354.15
[INFO 2017-06-28 15:23:50,506 main.py:51] epoch 11619, training loss: 8616.31, average training loss: 9208.84, base loss: 14352.48
[INFO 2017-06-28 15:23:51,164 main.py:51] epoch 11620, training loss: 9066.90, average training loss: 9208.14, base loss: 14350.02
[INFO 2017-06-28 15:23:51,824 main.py:51] epoch 11621, training loss: 7997.71, average training loss: 9206.17, base loss: 14347.95
[INFO 2017-06-28 15:23:52,490 main.py:51] epoch 11622, training loss: 9547.49, average training loss: 9206.53, base loss: 14348.07
[INFO 2017-06-28 15:23:53,164 main.py:51] epoch 11623, training loss: 8534.05, average training loss: 9205.93, base loss: 14347.34
[INFO 2017-06-28 15:23:53,819 main.py:51] epoch 11624, training loss: 10077.38, average training loss: 9206.79, base loss: 14349.10
[INFO 2017-06-28 15:23:54,479 main.py:51] epoch 11625, training loss: 8686.93, average training loss: 9207.85, base loss: 14351.12
[INFO 2017-06-28 15:23:55,131 main.py:51] epoch 11626, training loss: 9453.35, average training loss: 9208.05, base loss: 14351.34
[INFO 2017-06-28 15:23:55,793 main.py:51] epoch 11627, training loss: 8199.72, average training loss: 9206.65, base loss: 14349.86
[INFO 2017-06-28 15:23:56,459 main.py:51] epoch 11628, training loss: 8544.57, average training loss: 9206.42, base loss: 14349.88
[INFO 2017-06-28 15:23:57,107 main.py:51] epoch 11629, training loss: 10083.42, average training loss: 9206.52, base loss: 14350.24
[INFO 2017-06-28 15:23:57,790 main.py:51] epoch 11630, training loss: 10530.88, average training loss: 9207.20, base loss: 14349.63
[INFO 2017-06-28 15:23:58,441 main.py:51] epoch 11631, training loss: 8739.30, average training loss: 9206.82, base loss: 14348.52
[INFO 2017-06-28 15:23:59,091 main.py:51] epoch 11632, training loss: 8256.38, average training loss: 9205.59, base loss: 14346.29
[INFO 2017-06-28 15:23:59,738 main.py:51] epoch 11633, training loss: 9414.73, average training loss: 9206.44, base loss: 14347.78
[INFO 2017-06-28 15:24:00,384 main.py:51] epoch 11634, training loss: 9354.58, average training loss: 9206.82, base loss: 14350.34
[INFO 2017-06-28 15:24:01,029 main.py:51] epoch 11635, training loss: 8739.58, average training loss: 9205.41, base loss: 14349.22
[INFO 2017-06-28 15:24:01,669 main.py:51] epoch 11636, training loss: 8200.88, average training loss: 9204.60, base loss: 14347.99
[INFO 2017-06-28 15:24:02,311 main.py:51] epoch 11637, training loss: 8190.56, average training loss: 9202.51, base loss: 14344.38
[INFO 2017-06-28 15:24:02,966 main.py:51] epoch 11638, training loss: 8370.51, average training loss: 9199.36, base loss: 14339.02
[INFO 2017-06-28 15:24:03,617 main.py:51] epoch 11639, training loss: 8723.76, average training loss: 9197.39, base loss: 14335.87
[INFO 2017-06-28 15:24:04,276 main.py:51] epoch 11640, training loss: 10272.96, average training loss: 9198.71, base loss: 14339.76
[INFO 2017-06-28 15:24:04,939 main.py:51] epoch 11641, training loss: 10033.68, average training loss: 9198.23, base loss: 14338.53
[INFO 2017-06-28 15:24:05,580 main.py:51] epoch 11642, training loss: 8735.63, average training loss: 9199.06, base loss: 14339.49
[INFO 2017-06-28 15:24:06,209 main.py:51] epoch 11643, training loss: 9093.75, average training loss: 9198.08, base loss: 14339.60
[INFO 2017-06-28 15:24:06,876 main.py:51] epoch 11644, training loss: 9751.74, average training loss: 9198.51, base loss: 14339.62
[INFO 2017-06-28 15:24:07,547 main.py:51] epoch 11645, training loss: 8990.54, average training loss: 9196.85, base loss: 14336.08
[INFO 2017-06-28 15:24:08,192 main.py:51] epoch 11646, training loss: 8104.97, average training loss: 9195.06, base loss: 14331.99
[INFO 2017-06-28 15:24:08,840 main.py:51] epoch 11647, training loss: 9536.13, average training loss: 9195.38, base loss: 14332.00
[INFO 2017-06-28 15:24:09,488 main.py:51] epoch 11648, training loss: 9002.92, average training loss: 9194.34, base loss: 14328.68
[INFO 2017-06-28 15:24:10,132 main.py:51] epoch 11649, training loss: 9137.46, average training loss: 9194.18, base loss: 14328.61
[INFO 2017-06-28 15:24:10,796 main.py:51] epoch 11650, training loss: 10210.78, average training loss: 9195.20, base loss: 14329.27
[INFO 2017-06-28 15:24:11,421 main.py:51] epoch 11651, training loss: 8971.30, average training loss: 9195.06, base loss: 14329.05
[INFO 2017-06-28 15:24:12,069 main.py:51] epoch 11652, training loss: 8808.08, average training loss: 9193.77, base loss: 14326.62
[INFO 2017-06-28 15:24:12,733 main.py:51] epoch 11653, training loss: 9179.54, average training loss: 9194.72, base loss: 14328.68
[INFO 2017-06-28 15:24:13,433 main.py:51] epoch 11654, training loss: 8610.63, average training loss: 9194.22, base loss: 14326.94
[INFO 2017-06-28 15:24:14,104 main.py:51] epoch 11655, training loss: 10476.88, average training loss: 9195.23, base loss: 14327.89
[INFO 2017-06-28 15:24:14,755 main.py:51] epoch 11656, training loss: 8536.05, average training loss: 9195.52, base loss: 14326.91
[INFO 2017-06-28 15:24:15,401 main.py:51] epoch 11657, training loss: 10269.79, average training loss: 9196.42, base loss: 14327.92
[INFO 2017-06-28 15:24:16,055 main.py:51] epoch 11658, training loss: 10158.39, average training loss: 9197.77, base loss: 14328.48
[INFO 2017-06-28 15:24:16,726 main.py:51] epoch 11659, training loss: 9647.39, average training loss: 9197.69, base loss: 14330.34
[INFO 2017-06-28 15:24:17,394 main.py:51] epoch 11660, training loss: 9207.70, average training loss: 9196.32, base loss: 14327.67
[INFO 2017-06-28 15:24:18,049 main.py:51] epoch 11661, training loss: 9968.93, average training loss: 9196.06, base loss: 14327.73
[INFO 2017-06-28 15:24:18,695 main.py:51] epoch 11662, training loss: 8872.81, average training loss: 9195.22, base loss: 14327.92
[INFO 2017-06-28 15:24:19,341 main.py:51] epoch 11663, training loss: 9192.73, average training loss: 9194.75, base loss: 14326.19
[INFO 2017-06-28 15:24:19,995 main.py:51] epoch 11664, training loss: 8787.75, average training loss: 9194.68, base loss: 14325.91
[INFO 2017-06-28 15:24:20,667 main.py:51] epoch 11665, training loss: 9549.57, average training loss: 9194.16, base loss: 14324.89
[INFO 2017-06-28 15:24:21,316 main.py:51] epoch 11666, training loss: 9183.40, average training loss: 9194.70, base loss: 14326.78
[INFO 2017-06-28 15:24:21,988 main.py:51] epoch 11667, training loss: 9532.07, average training loss: 9195.24, base loss: 14327.14
[INFO 2017-06-28 15:24:22,661 main.py:51] epoch 11668, training loss: 9013.92, average training loss: 9193.25, base loss: 14325.73
[INFO 2017-06-28 15:24:23,344 main.py:51] epoch 11669, training loss: 9240.08, average training loss: 9193.39, base loss: 14325.25
[INFO 2017-06-28 15:24:24,012 main.py:51] epoch 11670, training loss: 9752.46, average training loss: 9195.20, base loss: 14328.64
[INFO 2017-06-28 15:24:24,697 main.py:51] epoch 11671, training loss: 9380.43, average training loss: 9195.42, base loss: 14328.79
[INFO 2017-06-28 15:24:25,378 main.py:51] epoch 11672, training loss: 8312.64, average training loss: 9194.51, base loss: 14327.69
[INFO 2017-06-28 15:24:26,032 main.py:51] epoch 11673, training loss: 9724.14, average training loss: 9195.23, base loss: 14330.28
[INFO 2017-06-28 15:24:26,689 main.py:51] epoch 11674, training loss: 9408.97, average training loss: 9194.04, base loss: 14328.85
[INFO 2017-06-28 15:24:27,350 main.py:51] epoch 11675, training loss: 9754.61, average training loss: 9194.07, base loss: 14328.56
[INFO 2017-06-28 15:24:28,001 main.py:51] epoch 11676, training loss: 9475.35, average training loss: 9195.06, base loss: 14330.90
[INFO 2017-06-28 15:24:28,689 main.py:51] epoch 11677, training loss: 9369.66, average training loss: 9195.07, base loss: 14332.17
[INFO 2017-06-28 15:24:29,376 main.py:51] epoch 11678, training loss: 8530.92, average training loss: 9193.45, base loss: 14330.09
[INFO 2017-06-28 15:24:30,032 main.py:51] epoch 11679, training loss: 8452.76, average training loss: 9194.00, base loss: 14332.37
[INFO 2017-06-28 15:24:30,685 main.py:51] epoch 11680, training loss: 8941.64, average training loss: 9194.33, base loss: 14332.72
[INFO 2017-06-28 15:24:31,324 main.py:51] epoch 11681, training loss: 10393.43, average training loss: 9196.20, base loss: 14337.20
[INFO 2017-06-28 15:24:31,962 main.py:51] epoch 11682, training loss: 9726.63, average training loss: 9196.37, base loss: 14339.05
[INFO 2017-06-28 15:24:32,609 main.py:51] epoch 11683, training loss: 8861.00, average training loss: 9195.85, base loss: 14338.96
[INFO 2017-06-28 15:24:33,263 main.py:51] epoch 11684, training loss: 9701.41, average training loss: 9197.00, base loss: 14339.57
[INFO 2017-06-28 15:24:33,925 main.py:51] epoch 11685, training loss: 9671.69, average training loss: 9197.50, base loss: 14339.23
[INFO 2017-06-28 15:24:34,587 main.py:51] epoch 11686, training loss: 8787.19, average training loss: 9198.71, base loss: 14342.49
[INFO 2017-06-28 15:24:35,259 main.py:51] epoch 11687, training loss: 8435.57, average training loss: 9197.63, base loss: 14341.14
[INFO 2017-06-28 15:24:35,924 main.py:51] epoch 11688, training loss: 8151.18, average training loss: 9196.83, base loss: 14340.93
[INFO 2017-06-28 15:24:36,591 main.py:51] epoch 11689, training loss: 7829.64, average training loss: 9196.49, base loss: 14341.49
[INFO 2017-06-28 15:24:37,253 main.py:51] epoch 11690, training loss: 8838.14, average training loss: 9196.26, base loss: 14340.55
[INFO 2017-06-28 15:24:37,930 main.py:51] epoch 11691, training loss: 8951.97, average training loss: 9195.44, base loss: 14339.47
[INFO 2017-06-28 15:24:38,586 main.py:51] epoch 11692, training loss: 9574.12, average training loss: 9196.23, base loss: 14339.53
[INFO 2017-06-28 15:24:39,255 main.py:51] epoch 11693, training loss: 8788.40, average training loss: 9195.77, base loss: 14338.37
[INFO 2017-06-28 15:24:39,915 main.py:51] epoch 11694, training loss: 10483.46, average training loss: 9198.26, base loss: 14343.68
[INFO 2017-06-28 15:24:40,556 main.py:51] epoch 11695, training loss: 8602.32, average training loss: 9198.47, base loss: 14343.76
[INFO 2017-06-28 15:24:41,227 main.py:51] epoch 11696, training loss: 9916.28, average training loss: 9199.51, base loss: 14345.71
[INFO 2017-06-28 15:24:41,885 main.py:51] epoch 11697, training loss: 7445.18, average training loss: 9194.69, base loss: 14338.10
[INFO 2017-06-28 15:24:42,568 main.py:51] epoch 11698, training loss: 8524.89, average training loss: 9194.62, base loss: 14337.55
[INFO 2017-06-28 15:24:43,233 main.py:51] epoch 11699, training loss: 8872.07, average training loss: 9194.97, base loss: 14337.80
[INFO 2017-06-28 15:24:43,234 main.py:53] epoch 11699, testing
[INFO 2017-06-28 15:24:45,830 main.py:105] average testing loss: 11066.11, base loss: 15795.17
[INFO 2017-06-28 15:24:45,830 main.py:106] improve_loss: 4729.05, improve_percent: 0.30
[INFO 2017-06-28 15:24:45,831 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:24:46,490 main.py:51] epoch 11700, training loss: 8983.58, average training loss: 9195.17, base loss: 14339.09
[INFO 2017-06-28 15:24:47,160 main.py:51] epoch 11701, training loss: 8952.38, average training loss: 9194.41, base loss: 14337.75
[INFO 2017-06-28 15:24:47,830 main.py:51] epoch 11702, training loss: 7862.22, average training loss: 9193.45, base loss: 14337.61
[INFO 2017-06-28 15:24:48,491 main.py:51] epoch 11703, training loss: 9432.45, average training loss: 9193.63, base loss: 14338.62
[INFO 2017-06-28 15:24:49,165 main.py:51] epoch 11704, training loss: 10787.51, average training loss: 9196.09, base loss: 14342.35
[INFO 2017-06-28 15:24:49,820 main.py:51] epoch 11705, training loss: 9056.37, average training loss: 9195.62, base loss: 14342.07
[INFO 2017-06-28 15:24:50,482 main.py:51] epoch 11706, training loss: 8588.79, average training loss: 9195.11, base loss: 14342.33
[INFO 2017-06-28 15:24:51,129 main.py:51] epoch 11707, training loss: 8850.12, average training loss: 9194.40, base loss: 14341.04
[INFO 2017-06-28 15:24:51,806 main.py:51] epoch 11708, training loss: 7882.26, average training loss: 9192.12, base loss: 14336.38
[INFO 2017-06-28 15:24:52,460 main.py:51] epoch 11709, training loss: 9308.46, average training loss: 9191.87, base loss: 14334.67
[INFO 2017-06-28 15:24:53,109 main.py:51] epoch 11710, training loss: 8845.41, average training loss: 9190.75, base loss: 14333.52
[INFO 2017-06-28 15:24:53,755 main.py:51] epoch 11711, training loss: 8860.37, average training loss: 9189.61, base loss: 14332.42
[INFO 2017-06-28 15:24:54,428 main.py:51] epoch 11712, training loss: 8921.22, average training loss: 9189.73, base loss: 14331.00
[INFO 2017-06-28 15:24:55,101 main.py:51] epoch 11713, training loss: 8842.04, average training loss: 9189.11, base loss: 14328.68
[INFO 2017-06-28 15:24:55,765 main.py:51] epoch 11714, training loss: 8032.58, average training loss: 9187.79, base loss: 14327.37
[INFO 2017-06-28 15:24:56,444 main.py:51] epoch 11715, training loss: 9502.57, average training loss: 9187.23, base loss: 14329.00
[INFO 2017-06-28 15:24:57,101 main.py:51] epoch 11716, training loss: 8528.61, average training loss: 9186.25, base loss: 14329.09
[INFO 2017-06-28 15:24:57,758 main.py:51] epoch 11717, training loss: 8877.87, average training loss: 9186.20, base loss: 14327.37
[INFO 2017-06-28 15:24:58,415 main.py:51] epoch 11718, training loss: 9183.36, average training loss: 9187.31, base loss: 14330.40
[INFO 2017-06-28 15:24:59,075 main.py:51] epoch 11719, training loss: 9891.76, average training loss: 9187.28, base loss: 14330.99
[INFO 2017-06-28 15:24:59,732 main.py:51] epoch 11720, training loss: 9579.09, average training loss: 9188.51, base loss: 14332.58
[INFO 2017-06-28 15:25:00,384 main.py:51] epoch 11721, training loss: 9163.50, average training loss: 9189.73, base loss: 14336.20
[INFO 2017-06-28 15:25:01,055 main.py:51] epoch 11722, training loss: 9453.38, average training loss: 9189.70, base loss: 14334.96
[INFO 2017-06-28 15:25:01,713 main.py:51] epoch 11723, training loss: 9554.66, average training loss: 9189.94, base loss: 14333.74
[INFO 2017-06-28 15:25:02,351 main.py:51] epoch 11724, training loss: 10103.03, average training loss: 9191.71, base loss: 14335.63
[INFO 2017-06-28 15:25:03,007 main.py:51] epoch 11725, training loss: 8240.37, average training loss: 9190.90, base loss: 14334.06
[INFO 2017-06-28 15:25:03,668 main.py:51] epoch 11726, training loss: 10239.13, average training loss: 9191.40, base loss: 14334.48
[INFO 2017-06-28 15:25:04,328 main.py:51] epoch 11727, training loss: 9323.50, average training loss: 9190.88, base loss: 14333.96
[INFO 2017-06-28 15:25:04,977 main.py:51] epoch 11728, training loss: 10640.95, average training loss: 9193.80, base loss: 14338.73
[INFO 2017-06-28 15:25:05,651 main.py:51] epoch 11729, training loss: 9999.55, average training loss: 9195.17, base loss: 14340.01
[INFO 2017-06-28 15:25:06,311 main.py:51] epoch 11730, training loss: 9236.88, average training loss: 9195.60, base loss: 14341.59
[INFO 2017-06-28 15:25:06,993 main.py:51] epoch 11731, training loss: 9551.67, average training loss: 9195.69, base loss: 14342.19
[INFO 2017-06-28 15:25:07,681 main.py:51] epoch 11732, training loss: 8173.98, average training loss: 9195.42, base loss: 14342.35
[INFO 2017-06-28 15:25:08,348 main.py:51] epoch 11733, training loss: 10230.61, average training loss: 9196.47, base loss: 14343.81
[INFO 2017-06-28 15:25:09,023 main.py:51] epoch 11734, training loss: 8212.30, average training loss: 9196.34, base loss: 14342.98
[INFO 2017-06-28 15:25:09,673 main.py:51] epoch 11735, training loss: 9468.58, average training loss: 9195.84, base loss: 14342.74
[INFO 2017-06-28 15:25:10,350 main.py:51] epoch 11736, training loss: 9862.14, average training loss: 9196.61, base loss: 14344.61
[INFO 2017-06-28 15:25:11,036 main.py:51] epoch 11737, training loss: 8962.44, average training loss: 9197.19, base loss: 14345.66
[INFO 2017-06-28 15:25:11,727 main.py:51] epoch 11738, training loss: 9217.62, average training loss: 9196.60, base loss: 14344.19
[INFO 2017-06-28 15:25:12,398 main.py:51] epoch 11739, training loss: 8121.96, average training loss: 9194.92, base loss: 14340.87
[INFO 2017-06-28 15:25:13,067 main.py:51] epoch 11740, training loss: 7800.29, average training loss: 9193.50, base loss: 14338.73
[INFO 2017-06-28 15:25:13,757 main.py:51] epoch 11741, training loss: 9243.19, average training loss: 9192.64, base loss: 14339.54
[INFO 2017-06-28 15:25:14,443 main.py:51] epoch 11742, training loss: 9442.22, average training loss: 9193.21, base loss: 14340.67
[INFO 2017-06-28 15:25:15,117 main.py:51] epoch 11743, training loss: 9783.28, average training loss: 9194.59, base loss: 14344.26
[INFO 2017-06-28 15:25:15,780 main.py:51] epoch 11744, training loss: 8726.60, average training loss: 9193.14, base loss: 14340.22
[INFO 2017-06-28 15:25:16,444 main.py:51] epoch 11745, training loss: 8862.05, average training loss: 9193.38, base loss: 14340.80
[INFO 2017-06-28 15:25:17,111 main.py:51] epoch 11746, training loss: 8894.34, average training loss: 9193.48, base loss: 14342.79
[INFO 2017-06-28 15:25:17,776 main.py:51] epoch 11747, training loss: 7443.21, average training loss: 9191.54, base loss: 14339.09
[INFO 2017-06-28 15:25:18,435 main.py:51] epoch 11748, training loss: 9289.22, average training loss: 9189.60, base loss: 14334.92
[INFO 2017-06-28 15:25:19,089 main.py:51] epoch 11749, training loss: 8526.52, average training loss: 9188.31, base loss: 14332.94
[INFO 2017-06-28 15:25:19,743 main.py:51] epoch 11750, training loss: 8869.25, average training loss: 9188.04, base loss: 14333.96
[INFO 2017-06-28 15:25:20,417 main.py:51] epoch 11751, training loss: 9541.75, average training loss: 9187.54, base loss: 14333.20
[INFO 2017-06-28 15:25:21,079 main.py:51] epoch 11752, training loss: 9067.98, average training loss: 9187.05, base loss: 14332.81
[INFO 2017-06-28 15:25:21,736 main.py:51] epoch 11753, training loss: 9030.54, average training loss: 9187.06, base loss: 14331.94
[INFO 2017-06-28 15:25:22,396 main.py:51] epoch 11754, training loss: 9220.86, average training loss: 9187.65, base loss: 14332.37
[INFO 2017-06-28 15:25:23,070 main.py:51] epoch 11755, training loss: 7993.64, average training loss: 9186.64, base loss: 14329.97
[INFO 2017-06-28 15:25:23,715 main.py:51] epoch 11756, training loss: 8627.21, average training loss: 9185.45, base loss: 14328.17
[INFO 2017-06-28 15:25:24,390 main.py:51] epoch 11757, training loss: 9282.73, average training loss: 9184.52, base loss: 14327.72
[INFO 2017-06-28 15:25:25,053 main.py:51] epoch 11758, training loss: 8474.29, average training loss: 9183.23, base loss: 14325.11
[INFO 2017-06-28 15:25:25,706 main.py:51] epoch 11759, training loss: 8741.03, average training loss: 9181.89, base loss: 14324.61
[INFO 2017-06-28 15:25:26,358 main.py:51] epoch 11760, training loss: 8622.34, average training loss: 9180.93, base loss: 14322.20
[INFO 2017-06-28 15:25:27,010 main.py:51] epoch 11761, training loss: 8608.98, average training loss: 9181.61, base loss: 14324.72
[INFO 2017-06-28 15:25:27,678 main.py:51] epoch 11762, training loss: 7222.22, average training loss: 9180.44, base loss: 14322.59
[INFO 2017-06-28 15:25:28,327 main.py:51] epoch 11763, training loss: 8537.84, average training loss: 9179.34, base loss: 14320.81
[INFO 2017-06-28 15:25:28,994 main.py:51] epoch 11764, training loss: 7734.85, average training loss: 9178.04, base loss: 14319.28
[INFO 2017-06-28 15:25:29,667 main.py:51] epoch 11765, training loss: 9371.97, average training loss: 9176.72, base loss: 14316.54
[INFO 2017-06-28 15:25:30,329 main.py:51] epoch 11766, training loss: 8775.71, average training loss: 9175.93, base loss: 14315.16
[INFO 2017-06-28 15:25:30,985 main.py:51] epoch 11767, training loss: 9070.01, average training loss: 9176.04, base loss: 14315.91
[INFO 2017-06-28 15:25:31,634 main.py:51] epoch 11768, training loss: 7974.15, average training loss: 9173.55, base loss: 14311.01
[INFO 2017-06-28 15:25:32,296 main.py:51] epoch 11769, training loss: 8930.24, average training loss: 9172.91, base loss: 14310.30
[INFO 2017-06-28 15:25:32,936 main.py:51] epoch 11770, training loss: 9976.35, average training loss: 9173.81, base loss: 14310.85
[INFO 2017-06-28 15:25:33,622 main.py:51] epoch 11771, training loss: 8051.04, average training loss: 9173.21, base loss: 14310.10
[INFO 2017-06-28 15:25:34,293 main.py:51] epoch 11772, training loss: 8578.31, average training loss: 9172.83, base loss: 14308.24
[INFO 2017-06-28 15:25:34,975 main.py:51] epoch 11773, training loss: 9348.67, average training loss: 9172.98, base loss: 14308.85
[INFO 2017-06-28 15:25:35,638 main.py:51] epoch 11774, training loss: 8380.55, average training loss: 9171.26, base loss: 14307.15
[INFO 2017-06-28 15:25:36,314 main.py:51] epoch 11775, training loss: 8248.70, average training loss: 9170.83, base loss: 14305.53
[INFO 2017-06-28 15:25:36,974 main.py:51] epoch 11776, training loss: 9824.84, average training loss: 9169.47, base loss: 14302.97
[INFO 2017-06-28 15:25:37,634 main.py:51] epoch 11777, training loss: 10169.68, average training loss: 9170.47, base loss: 14304.90
[INFO 2017-06-28 15:25:38,305 main.py:51] epoch 11778, training loss: 11140.53, average training loss: 9172.94, base loss: 14309.37
[INFO 2017-06-28 15:25:38,961 main.py:51] epoch 11779, training loss: 8438.53, average training loss: 9172.27, base loss: 14308.23
[INFO 2017-06-28 15:25:39,635 main.py:51] epoch 11780, training loss: 9650.58, average training loss: 9172.55, base loss: 14308.51
[INFO 2017-06-28 15:25:40,305 main.py:51] epoch 11781, training loss: 9617.12, average training loss: 9173.07, base loss: 14308.12
[INFO 2017-06-28 15:25:40,972 main.py:51] epoch 11782, training loss: 8618.20, average training loss: 9173.00, base loss: 14308.47
[INFO 2017-06-28 15:25:41,704 main.py:51] epoch 11783, training loss: 7982.76, average training loss: 9172.48, base loss: 14307.17
[INFO 2017-06-28 15:25:42,452 main.py:51] epoch 11784, training loss: 9113.79, average training loss: 9173.69, base loss: 14311.03
[INFO 2017-06-28 15:25:43,138 main.py:51] epoch 11785, training loss: 10334.78, average training loss: 9174.82, base loss: 14313.45
[INFO 2017-06-28 15:25:43,805 main.py:51] epoch 11786, training loss: 9165.83, average training loss: 9174.00, base loss: 14312.57
[INFO 2017-06-28 15:25:44,452 main.py:51] epoch 11787, training loss: 9555.96, average training loss: 9174.38, base loss: 14313.91
[INFO 2017-06-28 15:25:45,126 main.py:51] epoch 11788, training loss: 9220.95, average training loss: 9172.79, base loss: 14311.06
[INFO 2017-06-28 15:25:45,781 main.py:51] epoch 11789, training loss: 8559.98, average training loss: 9172.11, base loss: 14311.90
[INFO 2017-06-28 15:25:46,425 main.py:51] epoch 11790, training loss: 9186.69, average training loss: 9172.83, base loss: 14313.61
[INFO 2017-06-28 15:25:47,069 main.py:51] epoch 11791, training loss: 9117.33, average training loss: 9172.36, base loss: 14312.67
[INFO 2017-06-28 15:25:47,738 main.py:51] epoch 11792, training loss: 8494.70, average training loss: 9171.61, base loss: 14311.47
[INFO 2017-06-28 15:25:48,413 main.py:51] epoch 11793, training loss: 9653.35, average training loss: 9173.40, base loss: 14314.29
[INFO 2017-06-28 15:25:49,077 main.py:51] epoch 11794, training loss: 9626.73, average training loss: 9173.37, base loss: 14315.23
[INFO 2017-06-28 15:25:49,763 main.py:51] epoch 11795, training loss: 8956.75, average training loss: 9172.88, base loss: 14313.59
[INFO 2017-06-28 15:25:50,447 main.py:51] epoch 11796, training loss: 8844.78, average training loss: 9171.60, base loss: 14310.89
[INFO 2017-06-28 15:25:51,114 main.py:51] epoch 11797, training loss: 10165.03, average training loss: 9172.46, base loss: 14314.15
[INFO 2017-06-28 15:25:51,761 main.py:51] epoch 11798, training loss: 8107.84, average training loss: 9171.02, base loss: 14313.72
[INFO 2017-06-28 15:25:52,432 main.py:51] epoch 11799, training loss: 9830.51, average training loss: 9171.73, base loss: 14315.70
[INFO 2017-06-28 15:25:52,432 main.py:53] epoch 11799, testing
[INFO 2017-06-28 15:25:55,064 main.py:105] average testing loss: 10521.89, base loss: 14848.43
[INFO 2017-06-28 15:25:55,064 main.py:106] improve_loss: 4326.54, improve_percent: 0.29
[INFO 2017-06-28 15:25:55,065 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:25:55,720 main.py:51] epoch 11800, training loss: 9783.29, average training loss: 9172.21, base loss: 14315.08
[INFO 2017-06-28 15:25:56,383 main.py:51] epoch 11801, training loss: 8780.93, average training loss: 9170.10, base loss: 14312.14
[INFO 2017-06-28 15:25:57,051 main.py:51] epoch 11802, training loss: 8677.98, average training loss: 9170.76, base loss: 14314.81
[INFO 2017-06-28 15:25:57,715 main.py:51] epoch 11803, training loss: 11072.09, average training loss: 9172.72, base loss: 14317.74
[INFO 2017-06-28 15:25:58,378 main.py:51] epoch 11804, training loss: 8936.21, average training loss: 9173.50, base loss: 14319.25
[INFO 2017-06-28 15:25:59,052 main.py:51] epoch 11805, training loss: 8826.48, average training loss: 9172.92, base loss: 14319.08
[INFO 2017-06-28 15:25:59,729 main.py:51] epoch 11806, training loss: 8348.30, average training loss: 9172.27, base loss: 14319.04
[INFO 2017-06-28 15:26:00,438 main.py:51] epoch 11807, training loss: 8316.80, average training loss: 9170.53, base loss: 14315.76
[INFO 2017-06-28 15:26:01,115 main.py:51] epoch 11808, training loss: 8347.15, average training loss: 9169.29, base loss: 14311.64
[INFO 2017-06-28 15:26:01,782 main.py:51] epoch 11809, training loss: 9524.36, average training loss: 9169.83, base loss: 14312.72
[INFO 2017-06-28 15:26:02,454 main.py:51] epoch 11810, training loss: 8887.38, average training loss: 9169.99, base loss: 14311.49
[INFO 2017-06-28 15:26:03,096 main.py:51] epoch 11811, training loss: 8419.54, average training loss: 9168.02, base loss: 14306.77
[INFO 2017-06-28 15:26:03,742 main.py:51] epoch 11812, training loss: 10098.09, average training loss: 9168.12, base loss: 14307.90
[INFO 2017-06-28 15:26:04,434 main.py:51] epoch 11813, training loss: 9307.60, average training loss: 9168.01, base loss: 14308.30
[INFO 2017-06-28 15:26:05,094 main.py:51] epoch 11814, training loss: 10225.57, average training loss: 9169.66, base loss: 14311.31
[INFO 2017-06-28 15:26:05,765 main.py:51] epoch 11815, training loss: 9112.66, average training loss: 9170.01, base loss: 14311.02
[INFO 2017-06-28 15:26:06,504 main.py:51] epoch 11816, training loss: 8356.29, average training loss: 9169.27, base loss: 14310.53
[INFO 2017-06-28 15:26:07,178 main.py:51] epoch 11817, training loss: 9809.24, average training loss: 9170.40, base loss: 14311.83
[INFO 2017-06-28 15:26:07,847 main.py:51] epoch 11818, training loss: 7989.04, average training loss: 9169.77, base loss: 14310.12
[INFO 2017-06-28 15:26:08,529 main.py:51] epoch 11819, training loss: 10929.05, average training loss: 9171.42, base loss: 14313.71
[INFO 2017-06-28 15:26:09,238 main.py:51] epoch 11820, training loss: 10509.81, average training loss: 9172.52, base loss: 14315.77
[INFO 2017-06-28 15:26:09,974 main.py:51] epoch 11821, training loss: 9543.32, average training loss: 9173.30, base loss: 14316.09
[INFO 2017-06-28 15:26:10,626 main.py:51] epoch 11822, training loss: 9366.06, average training loss: 9173.65, base loss: 14317.12
[INFO 2017-06-28 15:26:11,308 main.py:51] epoch 11823, training loss: 8785.17, average training loss: 9173.01, base loss: 14315.32
[INFO 2017-06-28 15:26:11,968 main.py:51] epoch 11824, training loss: 9749.12, average training loss: 9173.97, base loss: 14317.48
[INFO 2017-06-28 15:26:12,624 main.py:51] epoch 11825, training loss: 9056.86, average training loss: 9173.05, base loss: 14314.79
[INFO 2017-06-28 15:26:13,286 main.py:51] epoch 11826, training loss: 10244.07, average training loss: 9174.67, base loss: 14318.07
[INFO 2017-06-28 15:26:13,935 main.py:51] epoch 11827, training loss: 8705.44, average training loss: 9174.95, base loss: 14318.26
[INFO 2017-06-28 15:26:14,609 main.py:51] epoch 11828, training loss: 8351.40, average training loss: 9174.12, base loss: 14316.93
[INFO 2017-06-28 15:26:15,275 main.py:51] epoch 11829, training loss: 7680.10, average training loss: 9173.02, base loss: 14314.58
[INFO 2017-06-28 15:26:15,969 main.py:51] epoch 11830, training loss: 8898.19, average training loss: 9172.59, base loss: 14313.83
[INFO 2017-06-28 15:26:16,615 main.py:51] epoch 11831, training loss: 8881.57, average training loss: 9172.19, base loss: 14312.88
[INFO 2017-06-28 15:26:17,306 main.py:51] epoch 11832, training loss: 10308.79, average training loss: 9173.43, base loss: 14317.29
[INFO 2017-06-28 15:26:17,969 main.py:51] epoch 11833, training loss: 9732.73, average training loss: 9175.12, base loss: 14321.36
[INFO 2017-06-28 15:26:18,616 main.py:51] epoch 11834, training loss: 8152.54, average training loss: 9174.35, base loss: 14318.93
[INFO 2017-06-28 15:26:19,285 main.py:51] epoch 11835, training loss: 9266.76, average training loss: 9174.09, base loss: 14318.16
[INFO 2017-06-28 15:26:19,989 main.py:51] epoch 11836, training loss: 8141.05, average training loss: 9173.90, base loss: 14317.74
[INFO 2017-06-28 15:26:20,666 main.py:51] epoch 11837, training loss: 9229.92, average training loss: 9173.83, base loss: 14317.72
[INFO 2017-06-28 15:26:21,330 main.py:51] epoch 11838, training loss: 9150.95, average training loss: 9173.97, base loss: 14317.01
[INFO 2017-06-28 15:26:21,992 main.py:51] epoch 11839, training loss: 9120.75, average training loss: 9174.46, base loss: 14317.77
[INFO 2017-06-28 15:26:22,650 main.py:51] epoch 11840, training loss: 8886.58, average training loss: 9173.94, base loss: 14317.80
[INFO 2017-06-28 15:26:23,307 main.py:51] epoch 11841, training loss: 8824.94, average training loss: 9173.16, base loss: 14316.99
[INFO 2017-06-28 15:26:23,978 main.py:51] epoch 11842, training loss: 9726.57, average training loss: 9174.40, base loss: 14318.79
[INFO 2017-06-28 15:26:24,626 main.py:51] epoch 11843, training loss: 8384.98, average training loss: 9172.30, base loss: 14315.55
[INFO 2017-06-28 15:26:25,279 main.py:51] epoch 11844, training loss: 8815.65, average training loss: 9170.71, base loss: 14314.21
[INFO 2017-06-28 15:26:25,948 main.py:51] epoch 11845, training loss: 7908.95, average training loss: 9168.53, base loss: 14311.51
[INFO 2017-06-28 15:26:26,614 main.py:51] epoch 11846, training loss: 8464.43, average training loss: 9167.20, base loss: 14311.40
[INFO 2017-06-28 15:26:27,282 main.py:51] epoch 11847, training loss: 8574.65, average training loss: 9167.64, base loss: 14313.81
[INFO 2017-06-28 15:26:27,936 main.py:51] epoch 11848, training loss: 10300.38, average training loss: 9168.47, base loss: 14315.95
[INFO 2017-06-28 15:26:28,590 main.py:51] epoch 11849, training loss: 7925.92, average training loss: 9167.12, base loss: 14312.86
[INFO 2017-06-28 15:26:29,251 main.py:51] epoch 11850, training loss: 9570.13, average training loss: 9166.95, base loss: 14313.37
[INFO 2017-06-28 15:26:29,925 main.py:51] epoch 11851, training loss: 8730.46, average training loss: 9165.77, base loss: 14312.56
[INFO 2017-06-28 15:26:30,600 main.py:51] epoch 11852, training loss: 8734.87, average training loss: 9165.15, base loss: 14310.01
[INFO 2017-06-28 15:26:31,268 main.py:51] epoch 11853, training loss: 9353.28, average training loss: 9165.75, base loss: 14310.36
[INFO 2017-06-28 15:26:31,950 main.py:51] epoch 11854, training loss: 8680.78, average training loss: 9166.10, base loss: 14311.78
[INFO 2017-06-28 15:26:32,597 main.py:51] epoch 11855, training loss: 8855.26, average training loss: 9166.21, base loss: 14311.53
[INFO 2017-06-28 15:26:33,253 main.py:51] epoch 11856, training loss: 9899.84, average training loss: 9166.63, base loss: 14311.02
[INFO 2017-06-28 15:26:33,907 main.py:51] epoch 11857, training loss: 8602.02, average training loss: 9165.01, base loss: 14310.39
[INFO 2017-06-28 15:26:34,550 main.py:51] epoch 11858, training loss: 8607.63, average training loss: 9165.36, base loss: 14311.10
[INFO 2017-06-28 15:26:35,199 main.py:51] epoch 11859, training loss: 7307.86, average training loss: 9162.51, base loss: 14307.10
[INFO 2017-06-28 15:26:35,830 main.py:51] epoch 11860, training loss: 8599.21, average training loss: 9161.71, base loss: 14306.06
[INFO 2017-06-28 15:26:36,493 main.py:51] epoch 11861, training loss: 10705.61, average training loss: 9163.25, base loss: 14307.71
[INFO 2017-06-28 15:26:37,196 main.py:51] epoch 11862, training loss: 10275.59, average training loss: 9163.48, base loss: 14309.67
[INFO 2017-06-28 15:26:37,853 main.py:51] epoch 11863, training loss: 8566.09, average training loss: 9161.52, base loss: 14306.65
[INFO 2017-06-28 15:26:38,528 main.py:51] epoch 11864, training loss: 9046.00, average training loss: 9162.61, base loss: 14309.23
[INFO 2017-06-28 15:26:39,183 main.py:51] epoch 11865, training loss: 9326.45, average training loss: 9162.89, base loss: 14309.18
[INFO 2017-06-28 15:26:39,846 main.py:51] epoch 11866, training loss: 9772.67, average training loss: 9164.57, base loss: 14310.83
[INFO 2017-06-28 15:26:40,495 main.py:51] epoch 11867, training loss: 9652.28, average training loss: 9165.51, base loss: 14311.93
[INFO 2017-06-28 15:26:41,194 main.py:51] epoch 11868, training loss: 8646.95, average training loss: 9165.13, base loss: 14310.72
[INFO 2017-06-28 15:26:41,856 main.py:51] epoch 11869, training loss: 9380.58, average training loss: 9165.33, base loss: 14312.12
[INFO 2017-06-28 15:26:42,567 main.py:51] epoch 11870, training loss: 8745.75, average training loss: 9165.35, base loss: 14312.91
[INFO 2017-06-28 15:26:43,221 main.py:51] epoch 11871, training loss: 8664.30, average training loss: 9166.56, base loss: 14314.85
[INFO 2017-06-28 15:26:43,924 main.py:51] epoch 11872, training loss: 9358.80, average training loss: 9167.10, base loss: 14315.64
[INFO 2017-06-28 15:26:44,597 main.py:51] epoch 11873, training loss: 10245.29, average training loss: 9168.14, base loss: 14316.09
[INFO 2017-06-28 15:26:45,267 main.py:51] epoch 11874, training loss: 9049.56, average training loss: 9167.66, base loss: 14314.28
[INFO 2017-06-28 15:26:45,949 main.py:51] epoch 11875, training loss: 8814.69, average training loss: 9167.57, base loss: 14312.64
[INFO 2017-06-28 15:26:46,604 main.py:51] epoch 11876, training loss: 8436.12, average training loss: 9167.78, base loss: 14313.32
[INFO 2017-06-28 15:26:47,280 main.py:51] epoch 11877, training loss: 8789.82, average training loss: 9167.68, base loss: 14312.20
[INFO 2017-06-28 15:26:47,989 main.py:51] epoch 11878, training loss: 8608.43, average training loss: 9166.52, base loss: 14311.05
[INFO 2017-06-28 15:26:48,659 main.py:51] epoch 11879, training loss: 8704.11, average training loss: 9166.01, base loss: 14308.91
[INFO 2017-06-28 15:26:49,306 main.py:51] epoch 11880, training loss: 8996.58, average training loss: 9167.02, base loss: 14311.64
[INFO 2017-06-28 15:26:49,959 main.py:51] epoch 11881, training loss: 8525.27, average training loss: 9167.12, base loss: 14311.14
[INFO 2017-06-28 15:26:50,611 main.py:51] epoch 11882, training loss: 10021.37, average training loss: 9168.52, base loss: 14312.09
[INFO 2017-06-28 15:26:51,280 main.py:51] epoch 11883, training loss: 8533.24, average training loss: 9168.25, base loss: 14311.35
[INFO 2017-06-28 15:26:51,939 main.py:51] epoch 11884, training loss: 8317.46, average training loss: 9167.70, base loss: 14310.79
[INFO 2017-06-28 15:26:52,655 main.py:51] epoch 11885, training loss: 9655.98, average training loss: 9167.00, base loss: 14309.25
[INFO 2017-06-28 15:26:53,310 main.py:51] epoch 11886, training loss: 10120.80, average training loss: 9167.12, base loss: 14308.84
[INFO 2017-06-28 15:26:53,969 main.py:51] epoch 11887, training loss: 9686.39, average training loss: 9167.74, base loss: 14308.82
[INFO 2017-06-28 15:26:54,629 main.py:51] epoch 11888, training loss: 9973.05, average training loss: 9168.24, base loss: 14308.73
[INFO 2017-06-28 15:26:55,305 main.py:51] epoch 11889, training loss: 8626.89, average training loss: 9166.95, base loss: 14306.68
[INFO 2017-06-28 15:26:55,962 main.py:51] epoch 11890, training loss: 8841.98, average training loss: 9165.50, base loss: 14303.88
[INFO 2017-06-28 15:26:56,641 main.py:51] epoch 11891, training loss: 9010.57, average training loss: 9164.01, base loss: 14302.14
[INFO 2017-06-28 15:26:57,318 main.py:51] epoch 11892, training loss: 9264.63, average training loss: 9165.70, base loss: 14306.69
[INFO 2017-06-28 15:26:57,976 main.py:51] epoch 11893, training loss: 8660.83, average training loss: 9164.03, base loss: 14302.24
[INFO 2017-06-28 15:26:58,631 main.py:51] epoch 11894, training loss: 9939.79, average training loss: 9164.37, base loss: 14301.84
[INFO 2017-06-28 15:26:59,302 main.py:51] epoch 11895, training loss: 8568.86, average training loss: 9164.44, base loss: 14301.56
[INFO 2017-06-28 15:26:59,940 main.py:51] epoch 11896, training loss: 8810.10, average training loss: 9164.29, base loss: 14300.99
[INFO 2017-06-28 15:27:00,580 main.py:51] epoch 11897, training loss: 9933.02, average training loss: 9164.21, base loss: 14301.86
[INFO 2017-06-28 15:27:01,241 main.py:51] epoch 11898, training loss: 9415.48, average training loss: 9164.71, base loss: 14301.50
[INFO 2017-06-28 15:27:01,909 main.py:51] epoch 11899, training loss: 8828.17, average training loss: 9163.01, base loss: 14299.79
[INFO 2017-06-28 15:27:01,909 main.py:53] epoch 11899, testing
[INFO 2017-06-28 15:27:04,468 main.py:105] average testing loss: 10465.43, base loss: 15146.78
[INFO 2017-06-28 15:27:04,468 main.py:106] improve_loss: 4681.35, improve_percent: 0.31
[INFO 2017-06-28 15:27:04,469 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:27:05,146 main.py:51] epoch 11900, training loss: 10608.81, average training loss: 9165.65, base loss: 14304.12
[INFO 2017-06-28 15:27:05,817 main.py:51] epoch 11901, training loss: 9662.68, average training loss: 9167.25, base loss: 14307.32
[INFO 2017-06-28 15:27:06,498 main.py:51] epoch 11902, training loss: 9513.99, average training loss: 9166.67, base loss: 14307.17
[INFO 2017-06-28 15:27:07,157 main.py:51] epoch 11903, training loss: 8468.69, average training loss: 9165.27, base loss: 14304.68
[INFO 2017-06-28 15:27:07,851 main.py:51] epoch 11904, training loss: 9113.83, average training loss: 9165.09, base loss: 14303.59
[INFO 2017-06-28 15:27:08,515 main.py:51] epoch 11905, training loss: 8755.49, average training loss: 9163.79, base loss: 14302.91
[INFO 2017-06-28 15:27:09,168 main.py:51] epoch 11906, training loss: 9731.67, average training loss: 9163.19, base loss: 14302.27
[INFO 2017-06-28 15:27:09,849 main.py:51] epoch 11907, training loss: 10034.91, average training loss: 9163.80, base loss: 14302.90
[INFO 2017-06-28 15:27:10,529 main.py:51] epoch 11908, training loss: 9504.53, average training loss: 9164.46, base loss: 14303.25
[INFO 2017-06-28 15:27:11,190 main.py:51] epoch 11909, training loss: 10198.67, average training loss: 9165.95, base loss: 14305.71
[INFO 2017-06-28 15:27:11,835 main.py:51] epoch 11910, training loss: 9141.39, average training loss: 9166.64, base loss: 14307.59
[INFO 2017-06-28 15:27:12,499 main.py:51] epoch 11911, training loss: 9310.34, average training loss: 9165.47, base loss: 14306.60
[INFO 2017-06-28 15:27:13,168 main.py:51] epoch 11912, training loss: 9856.95, average training loss: 9166.51, base loss: 14308.18
[INFO 2017-06-28 15:27:13,825 main.py:51] epoch 11913, training loss: 8274.70, average training loss: 9164.14, base loss: 14306.16
[INFO 2017-06-28 15:27:14,504 main.py:51] epoch 11914, training loss: 8609.32, average training loss: 9162.83, base loss: 14304.33
[INFO 2017-06-28 15:27:15,163 main.py:51] epoch 11915, training loss: 9070.00, average training loss: 9163.12, base loss: 14303.94
[INFO 2017-06-28 15:27:15,840 main.py:51] epoch 11916, training loss: 8925.39, average training loss: 9162.05, base loss: 14301.39
[INFO 2017-06-28 15:27:16,495 main.py:51] epoch 11917, training loss: 8951.27, average training loss: 9161.63, base loss: 14300.27
[INFO 2017-06-28 15:27:17,145 main.py:51] epoch 11918, training loss: 7994.81, average training loss: 9161.23, base loss: 14298.93
[INFO 2017-06-28 15:27:17,812 main.py:51] epoch 11919, training loss: 10843.18, average training loss: 9161.80, base loss: 14300.32
[INFO 2017-06-28 15:27:18,472 main.py:51] epoch 11920, training loss: 10930.08, average training loss: 9162.98, base loss: 14301.28
[INFO 2017-06-28 15:27:19,164 main.py:51] epoch 11921, training loss: 8459.25, average training loss: 9163.10, base loss: 14302.79
[INFO 2017-06-28 15:27:19,829 main.py:51] epoch 11922, training loss: 9662.59, average training loss: 9163.79, base loss: 14304.57
[INFO 2017-06-28 15:27:20,488 main.py:51] epoch 11923, training loss: 9857.76, average training loss: 9164.83, base loss: 14306.19
[INFO 2017-06-28 15:27:21,165 main.py:51] epoch 11924, training loss: 9193.72, average training loss: 9164.28, base loss: 14305.78
[INFO 2017-06-28 15:27:21,861 main.py:51] epoch 11925, training loss: 9603.56, average training loss: 9164.52, base loss: 14306.76
[INFO 2017-06-28 15:27:22,509 main.py:51] epoch 11926, training loss: 9438.93, average training loss: 9163.80, base loss: 14304.53
[INFO 2017-06-28 15:27:23,172 main.py:51] epoch 11927, training loss: 8447.19, average training loss: 9161.55, base loss: 14301.94
[INFO 2017-06-28 15:27:23,828 main.py:51] epoch 11928, training loss: 8947.86, average training loss: 9162.12, base loss: 14303.36
[INFO 2017-06-28 15:27:24,493 main.py:51] epoch 11929, training loss: 9474.90, average training loss: 9162.58, base loss: 14303.88
[INFO 2017-06-28 15:27:25,139 main.py:51] epoch 11930, training loss: 8906.53, average training loss: 9163.10, base loss: 14305.77
[INFO 2017-06-28 15:27:25,830 main.py:51] epoch 11931, training loss: 10151.63, average training loss: 9163.59, base loss: 14307.24
[INFO 2017-06-28 15:27:26,475 main.py:51] epoch 11932, training loss: 9482.72, average training loss: 9164.83, base loss: 14308.81
[INFO 2017-06-28 15:27:27,148 main.py:51] epoch 11933, training loss: 9220.65, average training loss: 9164.86, base loss: 14307.71
[INFO 2017-06-28 15:27:27,816 main.py:51] epoch 11934, training loss: 8736.32, average training loss: 9163.66, base loss: 14304.85
[INFO 2017-06-28 15:27:28,471 main.py:51] epoch 11935, training loss: 8828.09, average training loss: 9163.75, base loss: 14305.64
[INFO 2017-06-28 15:27:29,164 main.py:51] epoch 11936, training loss: 9192.10, average training loss: 9163.96, base loss: 14306.48
[INFO 2017-06-28 15:27:29,857 main.py:51] epoch 11937, training loss: 8762.15, average training loss: 9163.42, base loss: 14306.08
[INFO 2017-06-28 15:27:30,530 main.py:51] epoch 11938, training loss: 10543.08, average training loss: 9165.21, base loss: 14308.98
[INFO 2017-06-28 15:27:31,181 main.py:51] epoch 11939, training loss: 8356.95, average training loss: 9163.88, base loss: 14307.46
[INFO 2017-06-28 15:27:31,882 main.py:51] epoch 11940, training loss: 9016.73, average training loss: 9164.56, base loss: 14308.75
[INFO 2017-06-28 15:27:32,527 main.py:51] epoch 11941, training loss: 8672.52, average training loss: 9163.39, base loss: 14306.25
[INFO 2017-06-28 15:27:33,186 main.py:51] epoch 11942, training loss: 11883.58, average training loss: 9166.40, base loss: 14310.00
[INFO 2017-06-28 15:27:33,843 main.py:51] epoch 11943, training loss: 9373.40, average training loss: 9166.03, base loss: 14310.12
[INFO 2017-06-28 15:27:34,517 main.py:51] epoch 11944, training loss: 9365.30, average training loss: 9164.53, base loss: 14308.12
[INFO 2017-06-28 15:27:35,181 main.py:51] epoch 11945, training loss: 10613.08, average training loss: 9166.24, base loss: 14312.10
[INFO 2017-06-28 15:27:35,847 main.py:51] epoch 11946, training loss: 9948.70, average training loss: 9167.83, base loss: 14315.91
[INFO 2017-06-28 15:27:36,539 main.py:51] epoch 11947, training loss: 9059.71, average training loss: 9166.97, base loss: 14313.07
[INFO 2017-06-28 15:27:37,251 main.py:51] epoch 11948, training loss: 8747.04, average training loss: 9166.53, base loss: 14313.52
[INFO 2017-06-28 15:27:37,930 main.py:51] epoch 11949, training loss: 9451.07, average training loss: 9166.99, base loss: 14314.38
[INFO 2017-06-28 15:27:38,599 main.py:51] epoch 11950, training loss: 9032.84, average training loss: 9166.70, base loss: 14315.53
[INFO 2017-06-28 15:27:39,247 main.py:51] epoch 11951, training loss: 10280.86, average training loss: 9167.40, base loss: 14317.24
[INFO 2017-06-28 15:27:39,906 main.py:51] epoch 11952, training loss: 9336.21, average training loss: 9167.19, base loss: 14316.81
[INFO 2017-06-28 15:27:40,567 main.py:51] epoch 11953, training loss: 8440.50, average training loss: 9166.49, base loss: 14315.89
[INFO 2017-06-28 15:27:41,244 main.py:51] epoch 11954, training loss: 9497.64, average training loss: 9166.00, base loss: 14317.66
[INFO 2017-06-28 15:27:41,925 main.py:51] epoch 11955, training loss: 8190.61, average training loss: 9165.97, base loss: 14319.66
[INFO 2017-06-28 15:27:42,601 main.py:51] epoch 11956, training loss: 8804.73, average training loss: 9164.74, base loss: 14316.54
[INFO 2017-06-28 15:27:43,273 main.py:51] epoch 11957, training loss: 9937.33, average training loss: 9165.56, base loss: 14318.44
[INFO 2017-06-28 15:27:43,922 main.py:51] epoch 11958, training loss: 9165.89, average training loss: 9165.24, base loss: 14316.45
[INFO 2017-06-28 15:27:44,573 main.py:51] epoch 11959, training loss: 8901.40, average training loss: 9162.94, base loss: 14313.83
[INFO 2017-06-28 15:27:45,233 main.py:51] epoch 11960, training loss: 10897.88, average training loss: 9164.05, base loss: 14315.06
[INFO 2017-06-28 15:27:45,919 main.py:51] epoch 11961, training loss: 9048.76, average training loss: 9164.18, base loss: 14315.68
[INFO 2017-06-28 15:27:46,585 main.py:51] epoch 11962, training loss: 9444.14, average training loss: 9165.02, base loss: 14316.61
[INFO 2017-06-28 15:27:47,269 main.py:51] epoch 11963, training loss: 9429.64, average training loss: 9164.70, base loss: 14315.33
[INFO 2017-06-28 15:27:47,923 main.py:51] epoch 11964, training loss: 8151.45, average training loss: 9163.64, base loss: 14311.91
[INFO 2017-06-28 15:27:48,584 main.py:51] epoch 11965, training loss: 8676.76, average training loss: 9162.25, base loss: 14309.37
[INFO 2017-06-28 15:27:49,241 main.py:51] epoch 11966, training loss: 9513.55, average training loss: 9162.79, base loss: 14310.60
[INFO 2017-06-28 15:27:49,891 main.py:51] epoch 11967, training loss: 10912.72, average training loss: 9165.56, base loss: 14313.77
[INFO 2017-06-28 15:27:50,555 main.py:51] epoch 11968, training loss: 10320.01, average training loss: 9165.97, base loss: 14314.45
[INFO 2017-06-28 15:27:51,214 main.py:51] epoch 11969, training loss: 9266.32, average training loss: 9165.04, base loss: 14313.35
[INFO 2017-06-28 15:27:51,873 main.py:51] epoch 11970, training loss: 8920.04, average training loss: 9166.14, base loss: 14314.67
[INFO 2017-06-28 15:27:52,549 main.py:51] epoch 11971, training loss: 9320.67, average training loss: 9166.57, base loss: 14315.06
[INFO 2017-06-28 15:27:53,200 main.py:51] epoch 11972, training loss: 10121.24, average training loss: 9166.84, base loss: 14314.87
[INFO 2017-06-28 15:27:53,878 main.py:51] epoch 11973, training loss: 9711.58, average training loss: 9168.26, base loss: 14316.70
[INFO 2017-06-28 15:27:54,541 main.py:51] epoch 11974, training loss: 8747.97, average training loss: 9168.25, base loss: 14317.10
[INFO 2017-06-28 15:27:55,216 main.py:51] epoch 11975, training loss: 9151.05, average training loss: 9167.95, base loss: 14317.85
[INFO 2017-06-28 15:27:55,889 main.py:51] epoch 11976, training loss: 9012.59, average training loss: 9167.43, base loss: 14316.59
[INFO 2017-06-28 15:27:56,516 main.py:51] epoch 11977, training loss: 9166.65, average training loss: 9165.70, base loss: 14313.59
[INFO 2017-06-28 15:27:57,175 main.py:51] epoch 11978, training loss: 8837.80, average training loss: 9162.06, base loss: 14308.35
[INFO 2017-06-28 15:27:57,856 main.py:51] epoch 11979, training loss: 9031.16, average training loss: 9161.49, base loss: 14306.74
[INFO 2017-06-28 15:27:58,504 main.py:51] epoch 11980, training loss: 9509.79, average training loss: 9162.42, base loss: 14308.00
[INFO 2017-06-28 15:27:59,154 main.py:51] epoch 11981, training loss: 9674.38, average training loss: 9163.25, base loss: 14309.69
[INFO 2017-06-28 15:27:59,818 main.py:51] epoch 11982, training loss: 9578.40, average training loss: 9165.09, base loss: 14311.46
[INFO 2017-06-28 15:28:00,463 main.py:51] epoch 11983, training loss: 8366.32, average training loss: 9164.27, base loss: 14311.05
[INFO 2017-06-28 15:28:01,110 main.py:51] epoch 11984, training loss: 7950.65, average training loss: 9162.73, base loss: 14308.02
[INFO 2017-06-28 15:28:01,782 main.py:51] epoch 11985, training loss: 8835.02, average training loss: 9163.07, base loss: 14308.39
[INFO 2017-06-28 15:28:02,442 main.py:51] epoch 11986, training loss: 8970.21, average training loss: 9162.60, base loss: 14306.96
[INFO 2017-06-28 15:28:03,111 main.py:51] epoch 11987, training loss: 9637.04, average training loss: 9162.72, base loss: 14307.80
[INFO 2017-06-28 15:28:03,766 main.py:51] epoch 11988, training loss: 10448.45, average training loss: 9164.79, base loss: 14309.38
[INFO 2017-06-28 15:28:04,429 main.py:51] epoch 11989, training loss: 8756.90, average training loss: 9164.52, base loss: 14308.01
[INFO 2017-06-28 15:28:05,086 main.py:51] epoch 11990, training loss: 9198.11, average training loss: 9164.31, base loss: 14307.32
[INFO 2017-06-28 15:28:05,720 main.py:51] epoch 11991, training loss: 9381.49, average training loss: 9163.53, base loss: 14304.71
[INFO 2017-06-28 15:28:06,376 main.py:51] epoch 11992, training loss: 9504.50, average training loss: 9162.45, base loss: 14301.61
[INFO 2017-06-28 15:28:07,048 main.py:51] epoch 11993, training loss: 8017.56, average training loss: 9162.08, base loss: 14300.41
[INFO 2017-06-28 15:28:07,724 main.py:51] epoch 11994, training loss: 8872.74, average training loss: 9162.60, base loss: 14301.54
[INFO 2017-06-28 15:28:08,377 main.py:51] epoch 11995, training loss: 9571.93, average training loss: 9164.19, base loss: 14304.33
[INFO 2017-06-28 15:28:09,025 main.py:51] epoch 11996, training loss: 10352.38, average training loss: 9165.72, base loss: 14306.24
[INFO 2017-06-28 15:28:09,667 main.py:51] epoch 11997, training loss: 8608.65, average training loss: 9164.49, base loss: 14305.34
[INFO 2017-06-28 15:28:10,320 main.py:51] epoch 11998, training loss: 10081.32, average training loss: 9165.04, base loss: 14307.65
[INFO 2017-06-28 15:28:10,994 main.py:51] epoch 11999, training loss: 9721.58, average training loss: 9165.57, base loss: 14310.54
[INFO 2017-06-28 15:28:10,995 main.py:53] epoch 11999, testing
[INFO 2017-06-28 15:28:13,578 main.py:105] average testing loss: 10923.09, base loss: 15401.29
[INFO 2017-06-28 15:28:13,578 main.py:106] improve_loss: 4478.20, improve_percent: 0.29
[INFO 2017-06-28 15:28:13,579 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:28:14,235 main.py:51] epoch 12000, training loss: 8769.77, average training loss: 9165.58, base loss: 14310.91
[INFO 2017-06-28 15:28:14,886 main.py:51] epoch 12001, training loss: 8609.46, average training loss: 9165.98, base loss: 14311.36
[INFO 2017-06-28 15:28:15,534 main.py:51] epoch 12002, training loss: 9306.44, average training loss: 9166.42, base loss: 14311.19
[INFO 2017-06-28 15:28:16,207 main.py:51] epoch 12003, training loss: 8819.59, average training loss: 9165.94, base loss: 14309.56
[INFO 2017-06-28 15:28:16,867 main.py:51] epoch 12004, training loss: 9926.23, average training loss: 9166.81, base loss: 14311.10
[INFO 2017-06-28 15:28:17,517 main.py:51] epoch 12005, training loss: 8791.90, average training loss: 9166.80, base loss: 14311.47
[INFO 2017-06-28 15:28:18,185 main.py:51] epoch 12006, training loss: 9237.95, average training loss: 9168.12, base loss: 14312.03
[INFO 2017-06-28 15:28:18,834 main.py:51] epoch 12007, training loss: 8676.14, average training loss: 9167.77, base loss: 14311.60
[INFO 2017-06-28 15:28:19,505 main.py:51] epoch 12008, training loss: 9470.11, average training loss: 9167.92, base loss: 14312.87
[INFO 2017-06-28 15:28:20,176 main.py:51] epoch 12009, training loss: 8573.44, average training loss: 9166.86, base loss: 14310.60
[INFO 2017-06-28 15:28:20,829 main.py:51] epoch 12010, training loss: 8653.98, average training loss: 9167.57, base loss: 14313.52
[INFO 2017-06-28 15:28:21,465 main.py:51] epoch 12011, training loss: 8783.59, average training loss: 9167.38, base loss: 14312.48
[INFO 2017-06-28 15:28:22,102 main.py:51] epoch 12012, training loss: 8841.45, average training loss: 9167.25, base loss: 14313.39
[INFO 2017-06-28 15:28:22,766 main.py:51] epoch 12013, training loss: 9449.90, average training loss: 9167.17, base loss: 14314.44
[INFO 2017-06-28 15:28:23,434 main.py:51] epoch 12014, training loss: 8456.22, average training loss: 9167.24, base loss: 14314.73
[INFO 2017-06-28 15:28:24,089 main.py:51] epoch 12015, training loss: 8330.58, average training loss: 9166.26, base loss: 14313.17
[INFO 2017-06-28 15:28:24,740 main.py:51] epoch 12016, training loss: 8338.98, average training loss: 9166.38, base loss: 14312.98
[INFO 2017-06-28 15:28:25,442 main.py:51] epoch 12017, training loss: 10544.28, average training loss: 9169.06, base loss: 14316.73
[INFO 2017-06-28 15:28:26,111 main.py:51] epoch 12018, training loss: 9640.26, average training loss: 9170.00, base loss: 14318.83
[INFO 2017-06-28 15:28:26,772 main.py:51] epoch 12019, training loss: 9177.60, average training loss: 9167.70, base loss: 14314.83
[INFO 2017-06-28 15:28:27,481 main.py:51] epoch 12020, training loss: 9494.57, average training loss: 9167.54, base loss: 14315.12
[INFO 2017-06-28 15:28:28,190 main.py:51] epoch 12021, training loss: 8544.93, average training loss: 9168.16, base loss: 14315.57
[INFO 2017-06-28 15:28:28,844 main.py:51] epoch 12022, training loss: 8872.20, average training loss: 9168.05, base loss: 14315.76
[INFO 2017-06-28 15:28:29,575 main.py:51] epoch 12023, training loss: 8497.57, average training loss: 9166.04, base loss: 14314.61
[INFO 2017-06-28 15:28:30,281 main.py:51] epoch 12024, training loss: 8362.61, average training loss: 9166.89, base loss: 14315.77
[INFO 2017-06-28 15:28:31,011 main.py:51] epoch 12025, training loss: 8953.35, average training loss: 9167.17, base loss: 14316.69
[INFO 2017-06-28 15:28:31,712 main.py:51] epoch 12026, training loss: 9323.35, average training loss: 9168.00, base loss: 14318.20
[INFO 2017-06-28 15:28:32,372 main.py:51] epoch 12027, training loss: 8450.31, average training loss: 9167.29, base loss: 14318.66
[INFO 2017-06-28 15:28:33,026 main.py:51] epoch 12028, training loss: 10825.69, average training loss: 9168.70, base loss: 14320.84
[INFO 2017-06-28 15:28:33,680 main.py:51] epoch 12029, training loss: 9661.87, average training loss: 9169.83, base loss: 14322.35
[INFO 2017-06-28 15:28:34,346 main.py:51] epoch 12030, training loss: 9670.69, average training loss: 9169.11, base loss: 14320.29
[INFO 2017-06-28 15:28:35,002 main.py:51] epoch 12031, training loss: 8694.76, average training loss: 9169.48, base loss: 14320.82
[INFO 2017-06-28 15:28:35,666 main.py:51] epoch 12032, training loss: 8541.95, average training loss: 9169.40, base loss: 14319.67
[INFO 2017-06-28 15:28:36,324 main.py:51] epoch 12033, training loss: 9020.92, average training loss: 9169.50, base loss: 14319.93
[INFO 2017-06-28 15:28:36,993 main.py:51] epoch 12034, training loss: 10287.87, average training loss: 9171.40, base loss: 14322.80
[INFO 2017-06-28 15:28:37,710 main.py:51] epoch 12035, training loss: 9511.07, average training loss: 9172.30, base loss: 14325.03
[INFO 2017-06-28 15:28:38,380 main.py:51] epoch 12036, training loss: 10734.40, average training loss: 9173.65, base loss: 14326.67
[INFO 2017-06-28 15:28:39,050 main.py:51] epoch 12037, training loss: 7587.92, average training loss: 9171.87, base loss: 14323.02
[INFO 2017-06-28 15:28:39,703 main.py:51] epoch 12038, training loss: 7292.87, average training loss: 9168.95, base loss: 14317.73
[INFO 2017-06-28 15:28:40,369 main.py:51] epoch 12039, training loss: 8558.27, average training loss: 9167.94, base loss: 14315.50
[INFO 2017-06-28 15:28:41,014 main.py:51] epoch 12040, training loss: 9994.13, average training loss: 9169.53, base loss: 14319.31
[INFO 2017-06-28 15:28:41,662 main.py:51] epoch 12041, training loss: 8462.16, average training loss: 9166.45, base loss: 14313.96
[INFO 2017-06-28 15:28:42,341 main.py:51] epoch 12042, training loss: 9328.24, average training loss: 9166.87, base loss: 14315.75
[INFO 2017-06-28 15:28:43,024 main.py:51] epoch 12043, training loss: 10083.69, average training loss: 9167.50, base loss: 14316.61
[INFO 2017-06-28 15:28:43,674 main.py:51] epoch 12044, training loss: 8401.02, average training loss: 9167.44, base loss: 14318.00
[INFO 2017-06-28 15:28:44,358 main.py:51] epoch 12045, training loss: 9235.71, average training loss: 9168.44, base loss: 14321.19
[INFO 2017-06-28 15:28:45,033 main.py:51] epoch 12046, training loss: 9006.61, average training loss: 9167.65, base loss: 14320.32
[INFO 2017-06-28 15:28:45,717 main.py:51] epoch 12047, training loss: 10701.31, average training loss: 9167.24, base loss: 14319.42
[INFO 2017-06-28 15:28:46,398 main.py:51] epoch 12048, training loss: 9712.75, average training loss: 9169.20, base loss: 14322.19
[INFO 2017-06-28 15:28:47,046 main.py:51] epoch 12049, training loss: 9577.95, average training loss: 9170.25, base loss: 14323.39
[INFO 2017-06-28 15:28:47,686 main.py:51] epoch 12050, training loss: 10396.29, average training loss: 9170.87, base loss: 14324.42
[INFO 2017-06-28 15:28:48,323 main.py:51] epoch 12051, training loss: 8127.03, average training loss: 9169.74, base loss: 14323.60
[INFO 2017-06-28 15:28:48,981 main.py:51] epoch 12052, training loss: 7998.84, average training loss: 9168.39, base loss: 14321.41
[INFO 2017-06-28 15:28:49,636 main.py:51] epoch 12053, training loss: 9848.67, average training loss: 9170.29, base loss: 14326.31
[INFO 2017-06-28 15:28:50,370 main.py:51] epoch 12054, training loss: 10427.54, average training loss: 9171.60, base loss: 14330.78
[INFO 2017-06-28 15:28:51,047 main.py:51] epoch 12055, training loss: 8178.48, average training loss: 9171.03, base loss: 14328.04
[INFO 2017-06-28 15:28:51,708 main.py:51] epoch 12056, training loss: 9237.03, average training loss: 9171.84, base loss: 14331.22
[INFO 2017-06-28 15:28:52,348 main.py:51] epoch 12057, training loss: 10297.71, average training loss: 9171.82, base loss: 14331.55
[INFO 2017-06-28 15:28:53,031 main.py:51] epoch 12058, training loss: 8132.26, average training loss: 9171.25, base loss: 14331.09
[INFO 2017-06-28 15:28:53,695 main.py:51] epoch 12059, training loss: 9605.94, average training loss: 9171.13, base loss: 14329.62
[INFO 2017-06-28 15:28:54,357 main.py:51] epoch 12060, training loss: 8491.42, average training loss: 9170.12, base loss: 14328.73
[INFO 2017-06-28 15:28:55,013 main.py:51] epoch 12061, training loss: 9246.55, average training loss: 9169.73, base loss: 14328.14
[INFO 2017-06-28 15:28:55,681 main.py:51] epoch 12062, training loss: 8905.04, average training loss: 9168.77, base loss: 14327.67
[INFO 2017-06-28 15:28:56,321 main.py:51] epoch 12063, training loss: 8564.32, average training loss: 9167.19, base loss: 14324.45
[INFO 2017-06-28 15:28:57,004 main.py:51] epoch 12064, training loss: 9068.25, average training loss: 9167.72, base loss: 14325.90
[INFO 2017-06-28 15:28:57,667 main.py:51] epoch 12065, training loss: 9716.00, average training loss: 9168.48, base loss: 14326.15
[INFO 2017-06-28 15:28:58,330 main.py:51] epoch 12066, training loss: 8265.51, average training loss: 9168.07, base loss: 14325.74
[INFO 2017-06-28 15:28:58,978 main.py:51] epoch 12067, training loss: 10341.17, average training loss: 9169.12, base loss: 14327.46
[INFO 2017-06-28 15:28:59,655 main.py:51] epoch 12068, training loss: 8765.23, average training loss: 9166.75, base loss: 14322.41
[INFO 2017-06-28 15:29:00,324 main.py:51] epoch 12069, training loss: 8442.75, average training loss: 9166.09, base loss: 14320.37
[INFO 2017-06-28 15:29:00,980 main.py:51] epoch 12070, training loss: 8822.37, average training loss: 9165.69, base loss: 14320.32
[INFO 2017-06-28 15:29:01,627 main.py:51] epoch 12071, training loss: 9447.23, average training loss: 9166.30, base loss: 14322.00
[INFO 2017-06-28 15:29:02,284 main.py:51] epoch 12072, training loss: 8643.29, average training loss: 9164.67, base loss: 14316.72
[INFO 2017-06-28 15:29:02,930 main.py:51] epoch 12073, training loss: 9524.57, average training loss: 9165.70, base loss: 14317.67
[INFO 2017-06-28 15:29:03,590 main.py:51] epoch 12074, training loss: 8258.13, average training loss: 9164.58, base loss: 14314.76
[INFO 2017-06-28 15:29:04,250 main.py:51] epoch 12075, training loss: 10156.83, average training loss: 9165.09, base loss: 14314.64
[INFO 2017-06-28 15:29:04,932 main.py:51] epoch 12076, training loss: 9236.10, average training loss: 9165.74, base loss: 14315.74
[INFO 2017-06-28 15:29:05,600 main.py:51] epoch 12077, training loss: 8259.26, average training loss: 9163.71, base loss: 14311.77
[INFO 2017-06-28 15:29:06,254 main.py:51] epoch 12078, training loss: 9624.54, average training loss: 9163.99, base loss: 14312.15
[INFO 2017-06-28 15:29:06,919 main.py:51] epoch 12079, training loss: 10612.41, average training loss: 9166.00, base loss: 14315.81
[INFO 2017-06-28 15:29:07,577 main.py:51] epoch 12080, training loss: 9597.45, average training loss: 9166.95, base loss: 14317.02
[INFO 2017-06-28 15:29:08,247 main.py:51] epoch 12081, training loss: 8547.30, average training loss: 9167.07, base loss: 14316.94
[INFO 2017-06-28 15:29:08,910 main.py:51] epoch 12082, training loss: 9086.54, average training loss: 9167.11, base loss: 14319.11
[INFO 2017-06-28 15:29:09,567 main.py:51] epoch 12083, training loss: 10749.72, average training loss: 9169.28, base loss: 14321.58
[INFO 2017-06-28 15:29:10,230 main.py:51] epoch 12084, training loss: 9589.34, average training loss: 9169.20, base loss: 14321.11
[INFO 2017-06-28 15:29:10,884 main.py:51] epoch 12085, training loss: 9069.44, average training loss: 9169.41, base loss: 14321.96
[INFO 2017-06-28 15:29:11,541 main.py:51] epoch 12086, training loss: 9549.71, average training loss: 9168.59, base loss: 14319.95
[INFO 2017-06-28 15:29:12,200 main.py:51] epoch 12087, training loss: 9879.83, average training loss: 9169.61, base loss: 14320.59
[INFO 2017-06-28 15:29:12,858 main.py:51] epoch 12088, training loss: 9779.67, average training loss: 9168.47, base loss: 14317.86
[INFO 2017-06-28 15:29:13,521 main.py:51] epoch 12089, training loss: 9374.30, average training loss: 9167.23, base loss: 14317.17
[INFO 2017-06-28 15:29:14,183 main.py:51] epoch 12090, training loss: 10414.44, average training loss: 9167.74, base loss: 14317.71
[INFO 2017-06-28 15:29:14,829 main.py:51] epoch 12091, training loss: 10199.72, average training loss: 9168.65, base loss: 14318.70
[INFO 2017-06-28 15:29:15,499 main.py:51] epoch 12092, training loss: 9357.36, average training loss: 9168.49, base loss: 14318.80
[INFO 2017-06-28 15:29:16,160 main.py:51] epoch 12093, training loss: 8486.32, average training loss: 9167.61, base loss: 14316.56
[INFO 2017-06-28 15:29:16,850 main.py:51] epoch 12094, training loss: 8874.25, average training loss: 9165.87, base loss: 14315.37
[INFO 2017-06-28 15:29:17,525 main.py:51] epoch 12095, training loss: 11663.66, average training loss: 9167.89, base loss: 14320.06
[INFO 2017-06-28 15:29:18,175 main.py:51] epoch 12096, training loss: 9925.01, average training loss: 9167.77, base loss: 14319.31
[INFO 2017-06-28 15:29:18,854 main.py:51] epoch 12097, training loss: 8696.67, average training loss: 9166.94, base loss: 14318.09
[INFO 2017-06-28 15:29:19,518 main.py:51] epoch 12098, training loss: 9815.09, average training loss: 9167.97, base loss: 14319.98
[INFO 2017-06-28 15:29:20,158 main.py:51] epoch 12099, training loss: 8952.00, average training loss: 9167.23, base loss: 14318.85
[INFO 2017-06-28 15:29:20,158 main.py:53] epoch 12099, testing
[INFO 2017-06-28 15:29:22,744 main.py:105] average testing loss: 10283.24, base loss: 14407.46
[INFO 2017-06-28 15:29:22,744 main.py:106] improve_loss: 4124.22, improve_percent: 0.29
[INFO 2017-06-28 15:29:22,745 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:29:23,427 main.py:51] epoch 12100, training loss: 9550.39, average training loss: 9167.59, base loss: 14318.66
[INFO 2017-06-28 15:29:24,085 main.py:51] epoch 12101, training loss: 8283.16, average training loss: 9167.27, base loss: 14317.23
[INFO 2017-06-28 15:29:24,759 main.py:51] epoch 12102, training loss: 8751.08, average training loss: 9166.83, base loss: 14315.83
[INFO 2017-06-28 15:29:25,410 main.py:51] epoch 12103, training loss: 8522.10, average training loss: 9166.36, base loss: 14314.82
[INFO 2017-06-28 15:29:26,073 main.py:51] epoch 12104, training loss: 9313.24, average training loss: 9167.56, base loss: 14318.09
[INFO 2017-06-28 15:29:26,734 main.py:51] epoch 12105, training loss: 8844.53, average training loss: 9165.39, base loss: 14314.26
[INFO 2017-06-28 15:29:27,416 main.py:51] epoch 12106, training loss: 8252.40, average training loss: 9165.16, base loss: 14313.34
[INFO 2017-06-28 15:29:28,089 main.py:51] epoch 12107, training loss: 8344.58, average training loss: 9163.90, base loss: 14311.75
[INFO 2017-06-28 15:29:28,742 main.py:51] epoch 12108, training loss: 8566.92, average training loss: 9162.79, base loss: 14308.98
[INFO 2017-06-28 15:29:29,425 main.py:51] epoch 12109, training loss: 9732.80, average training loss: 9163.32, base loss: 14310.32
[INFO 2017-06-28 15:29:30,086 main.py:51] epoch 12110, training loss: 10874.13, average training loss: 9165.64, base loss: 14313.38
[INFO 2017-06-28 15:29:30,750 main.py:51] epoch 12111, training loss: 9370.98, average training loss: 9165.44, base loss: 14312.47
[INFO 2017-06-28 15:29:31,415 main.py:51] epoch 12112, training loss: 10219.74, average training loss: 9165.86, base loss: 14314.45
[INFO 2017-06-28 15:29:32,091 main.py:51] epoch 12113, training loss: 8910.03, average training loss: 9165.52, base loss: 14315.38
[INFO 2017-06-28 15:29:32,742 main.py:51] epoch 12114, training loss: 9107.32, average training loss: 9165.30, base loss: 14316.52
[INFO 2017-06-28 15:29:33,394 main.py:51] epoch 12115, training loss: 9045.14, average training loss: 9166.78, base loss: 14319.95
[INFO 2017-06-28 15:29:34,058 main.py:51] epoch 12116, training loss: 8242.51, average training loss: 9165.27, base loss: 14317.35
[INFO 2017-06-28 15:29:34,719 main.py:51] epoch 12117, training loss: 10441.94, average training loss: 9167.48, base loss: 14322.95
[INFO 2017-06-28 15:29:35,379 main.py:51] epoch 12118, training loss: 8783.83, average training loss: 9166.28, base loss: 14321.33
[INFO 2017-06-28 15:29:36,039 main.py:51] epoch 12119, training loss: 7797.77, average training loss: 9163.40, base loss: 14315.62
[INFO 2017-06-28 15:29:36,696 main.py:51] epoch 12120, training loss: 9587.73, average training loss: 9164.11, base loss: 14318.00
[INFO 2017-06-28 15:29:37,365 main.py:51] epoch 12121, training loss: 9857.15, average training loss: 9165.67, base loss: 14321.37
[INFO 2017-06-28 15:29:38,018 main.py:51] epoch 12122, training loss: 10506.66, average training loss: 9165.73, base loss: 14323.44
[INFO 2017-06-28 15:29:38,667 main.py:51] epoch 12123, training loss: 7642.25, average training loss: 9163.92, base loss: 14321.06
[INFO 2017-06-28 15:29:39,334 main.py:51] epoch 12124, training loss: 8317.79, average training loss: 9163.13, base loss: 14319.94
[INFO 2017-06-28 15:29:39,989 main.py:51] epoch 12125, training loss: 9738.80, average training loss: 9163.99, base loss: 14322.27
[INFO 2017-06-28 15:29:40,628 main.py:51] epoch 12126, training loss: 9582.58, average training loss: 9164.28, base loss: 14321.92
[INFO 2017-06-28 15:29:41,288 main.py:51] epoch 12127, training loss: 8395.30, average training loss: 9163.83, base loss: 14320.48
[INFO 2017-06-28 15:29:41,934 main.py:51] epoch 12128, training loss: 8466.27, average training loss: 9162.77, base loss: 14317.75
[INFO 2017-06-28 15:29:42,579 main.py:51] epoch 12129, training loss: 9590.44, average training loss: 9162.98, base loss: 14318.62
[INFO 2017-06-28 15:29:43,244 main.py:51] epoch 12130, training loss: 8697.74, average training loss: 9161.38, base loss: 14316.48
[INFO 2017-06-28 15:29:43,909 main.py:51] epoch 12131, training loss: 9779.03, average training loss: 9160.72, base loss: 14315.06
[INFO 2017-06-28 15:29:44,572 main.py:51] epoch 12132, training loss: 9305.01, average training loss: 9160.96, base loss: 14316.07
[INFO 2017-06-28 15:29:45,222 main.py:51] epoch 12133, training loss: 10859.00, average training loss: 9163.44, base loss: 14319.21
[INFO 2017-06-28 15:29:45,875 main.py:51] epoch 12134, training loss: 8765.64, average training loss: 9163.03, base loss: 14318.33
[INFO 2017-06-28 15:29:46,533 main.py:51] epoch 12135, training loss: 7983.07, average training loss: 9161.52, base loss: 14317.24
[INFO 2017-06-28 15:29:47,210 main.py:51] epoch 12136, training loss: 9000.65, average training loss: 9161.75, base loss: 14317.36
[INFO 2017-06-28 15:29:47,878 main.py:51] epoch 12137, training loss: 8957.84, average training loss: 9162.16, base loss: 14319.45
[INFO 2017-06-28 15:29:48,556 main.py:51] epoch 12138, training loss: 9196.36, average training loss: 9161.91, base loss: 14319.89
[INFO 2017-06-28 15:29:49,222 main.py:51] epoch 12139, training loss: 9050.13, average training loss: 9160.25, base loss: 14315.55
[INFO 2017-06-28 15:29:49,893 main.py:51] epoch 12140, training loss: 9253.83, average training loss: 9160.84, base loss: 14317.26
[INFO 2017-06-28 15:29:50,572 main.py:51] epoch 12141, training loss: 8762.14, average training loss: 9161.10, base loss: 14318.07
[INFO 2017-06-28 15:29:51,236 main.py:51] epoch 12142, training loss: 9186.48, average training loss: 9161.28, base loss: 14318.13
[INFO 2017-06-28 15:29:51,873 main.py:51] epoch 12143, training loss: 7817.20, average training loss: 9160.25, base loss: 14317.22
[INFO 2017-06-28 15:29:52,515 main.py:51] epoch 12144, training loss: 10247.61, average training loss: 9161.23, base loss: 14318.71
[INFO 2017-06-28 15:29:53,194 main.py:51] epoch 12145, training loss: 9623.67, average training loss: 9160.60, base loss: 14318.52
[INFO 2017-06-28 15:29:53,841 main.py:51] epoch 12146, training loss: 8558.44, average training loss: 9160.68, base loss: 14319.08
[INFO 2017-06-28 15:29:54,489 main.py:51] epoch 12147, training loss: 9297.83, average training loss: 9161.26, base loss: 14320.14
[INFO 2017-06-28 15:29:55,162 main.py:51] epoch 12148, training loss: 9608.73, average training loss: 9162.49, base loss: 14321.60
[INFO 2017-06-28 15:29:55,840 main.py:51] epoch 12149, training loss: 8621.57, average training loss: 9161.37, base loss: 14320.71
[INFO 2017-06-28 15:29:56,490 main.py:51] epoch 12150, training loss: 9704.25, average training loss: 9160.56, base loss: 14319.08
[INFO 2017-06-28 15:29:57,143 main.py:51] epoch 12151, training loss: 9445.17, average training loss: 9161.63, base loss: 14319.83
[INFO 2017-06-28 15:29:57,822 main.py:51] epoch 12152, training loss: 9147.31, average training loss: 9160.04, base loss: 14317.05
[INFO 2017-06-28 15:29:58,461 main.py:51] epoch 12153, training loss: 9800.95, average training loss: 9160.60, base loss: 14320.17
[INFO 2017-06-28 15:29:59,122 main.py:51] epoch 12154, training loss: 9764.17, average training loss: 9160.78, base loss: 14318.82
[INFO 2017-06-28 15:29:59,800 main.py:51] epoch 12155, training loss: 8991.95, average training loss: 9161.62, base loss: 14319.88
[INFO 2017-06-28 15:30:00,488 main.py:51] epoch 12156, training loss: 8499.89, average training loss: 9161.01, base loss: 14318.34
[INFO 2017-06-28 15:30:01,178 main.py:51] epoch 12157, training loss: 8702.93, average training loss: 9160.42, base loss: 14318.04
[INFO 2017-06-28 15:30:01,861 main.py:51] epoch 12158, training loss: 8699.29, average training loss: 9159.39, base loss: 14317.04
[INFO 2017-06-28 15:30:02,529 main.py:51] epoch 12159, training loss: 9061.22, average training loss: 9159.37, base loss: 14317.63
[INFO 2017-06-28 15:30:03,193 main.py:51] epoch 12160, training loss: 9086.07, average training loss: 9159.12, base loss: 14316.49
[INFO 2017-06-28 15:30:03,879 main.py:51] epoch 12161, training loss: 10418.83, average training loss: 9161.27, base loss: 14320.27
[INFO 2017-06-28 15:30:04,560 main.py:51] epoch 12162, training loss: 9944.00, average training loss: 9161.50, base loss: 14322.69
[INFO 2017-06-28 15:30:05,228 main.py:51] epoch 12163, training loss: 8472.08, average training loss: 9161.27, base loss: 14322.54
[INFO 2017-06-28 15:30:05,915 main.py:51] epoch 12164, training loss: 9065.52, average training loss: 9161.41, base loss: 14322.60
[INFO 2017-06-28 15:30:06,574 main.py:51] epoch 12165, training loss: 9843.12, average training loss: 9163.14, base loss: 14326.53
[INFO 2017-06-28 15:30:07,222 main.py:51] epoch 12166, training loss: 8901.02, average training loss: 9162.65, base loss: 14327.77
[INFO 2017-06-28 15:30:07,894 main.py:51] epoch 12167, training loss: 9290.40, average training loss: 9162.06, base loss: 14326.60
[INFO 2017-06-28 15:30:08,556 main.py:51] epoch 12168, training loss: 9309.92, average training loss: 9161.89, base loss: 14330.46
[INFO 2017-06-28 15:30:09,205 main.py:51] epoch 12169, training loss: 9030.88, average training loss: 9162.54, base loss: 14330.65
[INFO 2017-06-28 15:30:09,860 main.py:51] epoch 12170, training loss: 10514.27, average training loss: 9163.34, base loss: 14331.38
[INFO 2017-06-28 15:30:10,535 main.py:51] epoch 12171, training loss: 9655.35, average training loss: 9164.21, base loss: 14332.09
[INFO 2017-06-28 15:30:11,197 main.py:51] epoch 12172, training loss: 9903.77, average training loss: 9165.16, base loss: 14333.81
[INFO 2017-06-28 15:30:11,860 main.py:51] epoch 12173, training loss: 9400.00, average training loss: 9166.25, base loss: 14336.58
[INFO 2017-06-28 15:30:12,532 main.py:51] epoch 12174, training loss: 9817.86, average training loss: 9166.08, base loss: 14336.56
[INFO 2017-06-28 15:30:13,175 main.py:51] epoch 12175, training loss: 8535.26, average training loss: 9166.20, base loss: 14338.34
[INFO 2017-06-28 15:30:13,831 main.py:51] epoch 12176, training loss: 9018.89, average training loss: 9166.74, base loss: 14338.69
[INFO 2017-06-28 15:30:14,480 main.py:51] epoch 12177, training loss: 8820.89, average training loss: 9166.68, base loss: 14338.49
[INFO 2017-06-28 15:30:15,161 main.py:51] epoch 12178, training loss: 9253.06, average training loss: 9166.82, base loss: 14340.43
[INFO 2017-06-28 15:30:15,838 main.py:51] epoch 12179, training loss: 8679.03, average training loss: 9166.11, base loss: 14337.65
[INFO 2017-06-28 15:30:16,498 main.py:51] epoch 12180, training loss: 7784.21, average training loss: 9165.88, base loss: 14336.23
[INFO 2017-06-28 15:30:17,152 main.py:51] epoch 12181, training loss: 8461.75, average training loss: 9164.60, base loss: 14334.71
[INFO 2017-06-28 15:30:17,821 main.py:51] epoch 12182, training loss: 8278.63, average training loss: 9164.48, base loss: 14335.61
[INFO 2017-06-28 15:30:18,469 main.py:51] epoch 12183, training loss: 8640.65, average training loss: 9163.26, base loss: 14333.00
[INFO 2017-06-28 15:30:19,131 main.py:51] epoch 12184, training loss: 9498.41, average training loss: 9163.33, base loss: 14332.86
[INFO 2017-06-28 15:30:19,809 main.py:51] epoch 12185, training loss: 8424.30, average training loss: 9163.66, base loss: 14333.35
[INFO 2017-06-28 15:30:20,455 main.py:51] epoch 12186, training loss: 8477.74, average training loss: 9163.28, base loss: 14333.45
[INFO 2017-06-28 15:30:21,102 main.py:51] epoch 12187, training loss: 9929.88, average training loss: 9163.94, base loss: 14334.59
[INFO 2017-06-28 15:30:21,748 main.py:51] epoch 12188, training loss: 9252.34, average training loss: 9163.44, base loss: 14334.44
[INFO 2017-06-28 15:30:22,411 main.py:51] epoch 12189, training loss: 8505.87, average training loss: 9162.45, base loss: 14333.07
[INFO 2017-06-28 15:30:23,047 main.py:51] epoch 12190, training loss: 10023.85, average training loss: 9162.45, base loss: 14332.73
[INFO 2017-06-28 15:30:23,721 main.py:51] epoch 12191, training loss: 8783.46, average training loss: 9162.46, base loss: 14334.62
[INFO 2017-06-28 15:30:24,351 main.py:51] epoch 12192, training loss: 8880.11, average training loss: 9162.99, base loss: 14335.17
[INFO 2017-06-28 15:30:25,017 main.py:51] epoch 12193, training loss: 8123.17, average training loss: 9162.30, base loss: 14333.81
[INFO 2017-06-28 15:30:25,668 main.py:51] epoch 12194, training loss: 10212.36, average training loss: 9163.12, base loss: 14337.35
[INFO 2017-06-28 15:30:26,332 main.py:51] epoch 12195, training loss: 8772.26, average training loss: 9161.49, base loss: 14332.97
[INFO 2017-06-28 15:30:26,995 main.py:51] epoch 12196, training loss: 9374.89, average training loss: 9161.69, base loss: 14333.08
[INFO 2017-06-28 15:30:27,667 main.py:51] epoch 12197, training loss: 8959.21, average training loss: 9160.74, base loss: 14329.98
[INFO 2017-06-28 15:30:28,309 main.py:51] epoch 12198, training loss: 9093.38, average training loss: 9161.77, base loss: 14331.00
[INFO 2017-06-28 15:30:28,982 main.py:51] epoch 12199, training loss: 8599.82, average training loss: 9160.58, base loss: 14329.06
[INFO 2017-06-28 15:30:28,983 main.py:53] epoch 12199, testing
[INFO 2017-06-28 15:30:31,600 main.py:105] average testing loss: 10427.25, base loss: 14841.77
[INFO 2017-06-28 15:30:31,600 main.py:106] improve_loss: 4414.52, improve_percent: 0.30
[INFO 2017-06-28 15:30:31,600 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:30:32,269 main.py:51] epoch 12200, training loss: 8561.66, average training loss: 9160.08, base loss: 14326.95
[INFO 2017-06-28 15:30:32,936 main.py:51] epoch 12201, training loss: 8669.63, average training loss: 9158.82, base loss: 14324.74
[INFO 2017-06-28 15:30:33,605 main.py:51] epoch 12202, training loss: 8689.36, average training loss: 9158.84, base loss: 14323.93
[INFO 2017-06-28 15:30:34,266 main.py:51] epoch 12203, training loss: 8712.15, average training loss: 9158.97, base loss: 14323.18
[INFO 2017-06-28 15:30:34,904 main.py:51] epoch 12204, training loss: 8397.17, average training loss: 9157.13, base loss: 14321.03
[INFO 2017-06-28 15:30:35,545 main.py:51] epoch 12205, training loss: 8078.94, average training loss: 9157.13, base loss: 14322.18
[INFO 2017-06-28 15:30:36,191 main.py:51] epoch 12206, training loss: 8604.97, average training loss: 9156.62, base loss: 14322.05
[INFO 2017-06-28 15:30:36,835 main.py:51] epoch 12207, training loss: 9390.08, average training loss: 9156.20, base loss: 14321.39
[INFO 2017-06-28 15:30:37,479 main.py:51] epoch 12208, training loss: 10742.46, average training loss: 9158.24, base loss: 14324.25
[INFO 2017-06-28 15:30:38,149 main.py:51] epoch 12209, training loss: 9020.24, average training loss: 9157.19, base loss: 14323.40
[INFO 2017-06-28 15:30:38,797 main.py:51] epoch 12210, training loss: 10433.51, average training loss: 9157.89, base loss: 14322.88
[INFO 2017-06-28 15:30:39,457 main.py:51] epoch 12211, training loss: 8478.52, average training loss: 9156.71, base loss: 14320.82
[INFO 2017-06-28 15:30:40,122 main.py:51] epoch 12212, training loss: 7834.19, average training loss: 9156.36, base loss: 14319.81
[INFO 2017-06-28 15:30:40,788 main.py:51] epoch 12213, training loss: 9088.17, average training loss: 9155.86, base loss: 14318.12
[INFO 2017-06-28 15:30:41,493 main.py:51] epoch 12214, training loss: 8571.01, average training loss: 9156.15, base loss: 14318.92
[INFO 2017-06-28 15:30:42,202 main.py:51] epoch 12215, training loss: 9843.10, average training loss: 9157.08, base loss: 14320.03
[INFO 2017-06-28 15:30:42,869 main.py:51] epoch 12216, training loss: 8772.09, average training loss: 9157.34, base loss: 14321.46
[INFO 2017-06-28 15:30:43,502 main.py:51] epoch 12217, training loss: 9898.77, average training loss: 9157.19, base loss: 14320.64
[INFO 2017-06-28 15:30:44,199 main.py:51] epoch 12218, training loss: 11174.85, average training loss: 9159.51, base loss: 14323.80
[INFO 2017-06-28 15:30:44,841 main.py:51] epoch 12219, training loss: 7988.14, average training loss: 9158.78, base loss: 14321.71
[INFO 2017-06-28 15:30:45,506 main.py:51] epoch 12220, training loss: 10066.08, average training loss: 9160.56, base loss: 14325.00
[INFO 2017-06-28 15:30:46,141 main.py:51] epoch 12221, training loss: 7632.37, average training loss: 9159.42, base loss: 14323.03
[INFO 2017-06-28 15:30:46,822 main.py:51] epoch 12222, training loss: 10924.20, average training loss: 9161.60, base loss: 14325.36
[INFO 2017-06-28 15:30:47,472 main.py:51] epoch 12223, training loss: 9412.77, average training loss: 9161.42, base loss: 14324.62
[INFO 2017-06-28 15:30:48,238 main.py:51] epoch 12224, training loss: 9748.86, average training loss: 9162.81, base loss: 14324.58
[INFO 2017-06-28 15:30:49,008 main.py:51] epoch 12225, training loss: 9412.95, average training loss: 9164.67, base loss: 14328.79
[INFO 2017-06-28 15:30:49,755 main.py:51] epoch 12226, training loss: 8452.47, average training loss: 9164.24, base loss: 14328.66
[INFO 2017-06-28 15:30:50,461 main.py:51] epoch 12227, training loss: 8913.05, average training loss: 9164.20, base loss: 14328.65
[INFO 2017-06-28 15:30:51,224 main.py:51] epoch 12228, training loss: 9790.64, average training loss: 9165.64, base loss: 14331.83
[INFO 2017-06-28 15:30:51,889 main.py:51] epoch 12229, training loss: 9999.05, average training loss: 9164.95, base loss: 14331.86
[INFO 2017-06-28 15:30:52,545 main.py:51] epoch 12230, training loss: 9104.02, average training loss: 9164.03, base loss: 14329.93
[INFO 2017-06-28 15:30:53,203 main.py:51] epoch 12231, training loss: 8992.13, average training loss: 9164.72, base loss: 14330.43
[INFO 2017-06-28 15:30:53,839 main.py:51] epoch 12232, training loss: 9701.91, average training loss: 9165.91, base loss: 14331.88
[INFO 2017-06-28 15:30:54,515 main.py:51] epoch 12233, training loss: 9911.18, average training loss: 9165.68, base loss: 14330.86
[INFO 2017-06-28 15:30:55,174 main.py:51] epoch 12234, training loss: 10196.19, average training loss: 9164.99, base loss: 14329.35
[INFO 2017-06-28 15:30:55,844 main.py:51] epoch 12235, training loss: 8014.38, average training loss: 9164.29, base loss: 14327.85
[INFO 2017-06-28 15:30:56,509 main.py:51] epoch 12236, training loss: 9191.93, average training loss: 9165.37, base loss: 14330.80
[INFO 2017-06-28 15:30:57,171 main.py:51] epoch 12237, training loss: 8212.39, average training loss: 9164.18, base loss: 14329.15
[INFO 2017-06-28 15:30:57,848 main.py:51] epoch 12238, training loss: 9771.70, average training loss: 9165.55, base loss: 14331.54
[INFO 2017-06-28 15:30:58,542 main.py:51] epoch 12239, training loss: 8930.81, average training loss: 9164.20, base loss: 14329.95
[INFO 2017-06-28 15:30:59,206 main.py:51] epoch 12240, training loss: 9913.76, average training loss: 9164.71, base loss: 14330.61
[INFO 2017-06-28 15:30:59,873 main.py:51] epoch 12241, training loss: 8207.78, average training loss: 9162.95, base loss: 14329.02
[INFO 2017-06-28 15:31:00,527 main.py:51] epoch 12242, training loss: 8184.93, average training loss: 9160.98, base loss: 14325.30
[INFO 2017-06-28 15:31:01,181 main.py:51] epoch 12243, training loss: 11024.69, average training loss: 9162.28, base loss: 14327.50
[INFO 2017-06-28 15:31:01,833 main.py:51] epoch 12244, training loss: 8445.54, average training loss: 9161.25, base loss: 14326.46
[INFO 2017-06-28 15:31:02,504 main.py:51] epoch 12245, training loss: 8772.79, average training loss: 9161.83, base loss: 14326.87
[INFO 2017-06-28 15:31:03,214 main.py:51] epoch 12246, training loss: 9771.36, average training loss: 9163.40, base loss: 14327.17
[INFO 2017-06-28 15:31:03,905 main.py:51] epoch 12247, training loss: 9198.36, average training loss: 9163.19, base loss: 14326.76
[INFO 2017-06-28 15:31:04,566 main.py:51] epoch 12248, training loss: 10668.21, average training loss: 9165.64, base loss: 14330.97
[INFO 2017-06-28 15:31:05,245 main.py:51] epoch 12249, training loss: 9120.80, average training loss: 9166.14, base loss: 14331.89
[INFO 2017-06-28 15:31:05,899 main.py:51] epoch 12250, training loss: 8431.70, average training loss: 9165.58, base loss: 14331.69
[INFO 2017-06-28 15:31:06,553 main.py:51] epoch 12251, training loss: 8846.29, average training loss: 9164.95, base loss: 14330.16
[INFO 2017-06-28 15:31:07,218 main.py:51] epoch 12252, training loss: 9206.71, average training loss: 9166.23, base loss: 14332.57
[INFO 2017-06-28 15:31:07,884 main.py:51] epoch 12253, training loss: 10809.80, average training loss: 9168.90, base loss: 14337.13
[INFO 2017-06-28 15:31:08,544 main.py:51] epoch 12254, training loss: 7680.95, average training loss: 9167.28, base loss: 14333.74
[INFO 2017-06-28 15:31:09,231 main.py:51] epoch 12255, training loss: 9427.87, average training loss: 9166.54, base loss: 14332.37
[INFO 2017-06-28 15:31:09,894 main.py:51] epoch 12256, training loss: 10507.50, average training loss: 9168.68, base loss: 14335.86
[INFO 2017-06-28 15:31:10,539 main.py:51] epoch 12257, training loss: 9279.60, average training loss: 9168.93, base loss: 14335.04
[INFO 2017-06-28 15:31:11,202 main.py:51] epoch 12258, training loss: 11265.98, average training loss: 9170.54, base loss: 14337.63
[INFO 2017-06-28 15:31:11,852 main.py:51] epoch 12259, training loss: 9347.38, average training loss: 9168.67, base loss: 14334.08
[INFO 2017-06-28 15:31:12,509 main.py:51] epoch 12260, training loss: 8235.89, average training loss: 9168.45, base loss: 14332.49
[INFO 2017-06-28 15:31:13,184 main.py:51] epoch 12261, training loss: 8932.14, average training loss: 9168.69, base loss: 14333.63
[INFO 2017-06-28 15:31:13,880 main.py:51] epoch 12262, training loss: 8639.53, average training loss: 9168.93, base loss: 14332.31
[INFO 2017-06-28 15:31:14,531 main.py:51] epoch 12263, training loss: 8666.94, average training loss: 9169.39, base loss: 14332.83
[INFO 2017-06-28 15:31:15,184 main.py:51] epoch 12264, training loss: 10311.05, average training loss: 9171.11, base loss: 14335.79
[INFO 2017-06-28 15:31:15,844 main.py:51] epoch 12265, training loss: 9010.99, average training loss: 9171.84, base loss: 14336.63
[INFO 2017-06-28 15:31:16,515 main.py:51] epoch 12266, training loss: 8343.20, average training loss: 9170.99, base loss: 14333.59
[INFO 2017-06-28 15:31:17,163 main.py:51] epoch 12267, training loss: 8903.62, average training loss: 9171.29, base loss: 14334.15
[INFO 2017-06-28 15:31:17,818 main.py:51] epoch 12268, training loss: 9745.92, average training loss: 9171.92, base loss: 14334.72
[INFO 2017-06-28 15:31:18,475 main.py:51] epoch 12269, training loss: 9207.64, average training loss: 9171.91, base loss: 14334.26
[INFO 2017-06-28 15:31:19,121 main.py:51] epoch 12270, training loss: 8490.39, average training loss: 9170.18, base loss: 14329.82
[INFO 2017-06-28 15:31:19,791 main.py:51] epoch 12271, training loss: 9710.81, average training loss: 9171.20, base loss: 14332.05
[INFO 2017-06-28 15:31:20,490 main.py:51] epoch 12272, training loss: 8472.62, average training loss: 9169.61, base loss: 14331.15
[INFO 2017-06-28 15:31:21,147 main.py:51] epoch 12273, training loss: 9883.57, average training loss: 9170.42, base loss: 14334.87
[INFO 2017-06-28 15:31:21,811 main.py:51] epoch 12274, training loss: 9946.30, average training loss: 9171.47, base loss: 14336.96
[INFO 2017-06-28 15:31:22,504 main.py:51] epoch 12275, training loss: 7711.89, average training loss: 9169.85, base loss: 14333.96
[INFO 2017-06-28 15:31:23,168 main.py:51] epoch 12276, training loss: 9175.43, average training loss: 9167.95, base loss: 14330.30
[INFO 2017-06-28 15:31:23,824 main.py:51] epoch 12277, training loss: 8322.54, average training loss: 9167.21, base loss: 14328.59
[INFO 2017-06-28 15:31:24,481 main.py:51] epoch 12278, training loss: 10196.08, average training loss: 9169.16, base loss: 14330.42
[INFO 2017-06-28 15:31:25,180 main.py:51] epoch 12279, training loss: 8359.71, average training loss: 9168.19, base loss: 14330.10
[INFO 2017-06-28 15:31:25,852 main.py:51] epoch 12280, training loss: 8272.92, average training loss: 9168.04, base loss: 14330.87
[INFO 2017-06-28 15:31:26,511 main.py:51] epoch 12281, training loss: 9673.86, average training loss: 9169.37, base loss: 14333.02
[INFO 2017-06-28 15:31:27,166 main.py:51] epoch 12282, training loss: 8954.27, average training loss: 9168.72, base loss: 14332.16
[INFO 2017-06-28 15:31:27,855 main.py:51] epoch 12283, training loss: 8533.05, average training loss: 9167.82, base loss: 14331.01
[INFO 2017-06-28 15:31:28,542 main.py:51] epoch 12284, training loss: 9395.30, average training loss: 9168.75, base loss: 14332.12
[INFO 2017-06-28 15:31:29,243 main.py:51] epoch 12285, training loss: 9015.72, average training loss: 9169.53, base loss: 14333.84
[INFO 2017-06-28 15:31:29,914 main.py:51] epoch 12286, training loss: 9705.27, average training loss: 9169.99, base loss: 14334.18
[INFO 2017-06-28 15:31:30,599 main.py:51] epoch 12287, training loss: 9594.15, average training loss: 9169.93, base loss: 14333.42
[INFO 2017-06-28 15:31:31,263 main.py:51] epoch 12288, training loss: 8659.92, average training loss: 9169.11, base loss: 14332.79
[INFO 2017-06-28 15:31:31,936 main.py:51] epoch 12289, training loss: 8497.11, average training loss: 9168.30, base loss: 14332.54
[INFO 2017-06-28 15:31:32,590 main.py:51] epoch 12290, training loss: 9543.16, average training loss: 9169.03, base loss: 14336.32
[INFO 2017-06-28 15:31:33,253 main.py:51] epoch 12291, training loss: 8564.11, average training loss: 9168.45, base loss: 14334.87
[INFO 2017-06-28 15:31:33,927 main.py:51] epoch 12292, training loss: 8517.70, average training loss: 9167.32, base loss: 14332.54
[INFO 2017-06-28 15:31:34,575 main.py:51] epoch 12293, training loss: 8770.27, average training loss: 9166.71, base loss: 14331.99
[INFO 2017-06-28 15:31:35,227 main.py:51] epoch 12294, training loss: 10255.64, average training loss: 9166.89, base loss: 14331.50
[INFO 2017-06-28 15:31:35,913 main.py:51] epoch 12295, training loss: 8405.70, average training loss: 9165.70, base loss: 14329.44
[INFO 2017-06-28 15:31:36,573 main.py:51] epoch 12296, training loss: 8678.45, average training loss: 9165.64, base loss: 14328.95
[INFO 2017-06-28 15:31:37,229 main.py:51] epoch 12297, training loss: 10214.61, average training loss: 9167.53, base loss: 14333.55
[INFO 2017-06-28 15:31:37,905 main.py:51] epoch 12298, training loss: 9352.21, average training loss: 9166.89, base loss: 14331.63
[INFO 2017-06-28 15:31:38,570 main.py:51] epoch 12299, training loss: 10686.34, average training loss: 9167.81, base loss: 14333.28
[INFO 2017-06-28 15:31:38,570 main.py:53] epoch 12299, testing
[INFO 2017-06-28 15:31:41,294 main.py:105] average testing loss: 10613.17, base loss: 15147.09
[INFO 2017-06-28 15:31:41,294 main.py:106] improve_loss: 4533.92, improve_percent: 0.30
[INFO 2017-06-28 15:31:41,294 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:31:41,963 main.py:51] epoch 12300, training loss: 8772.75, average training loss: 9168.70, base loss: 14334.28
[INFO 2017-06-28 15:31:42,622 main.py:51] epoch 12301, training loss: 9648.83, average training loss: 9169.99, base loss: 14335.26
[INFO 2017-06-28 15:31:43,392 main.py:51] epoch 12302, training loss: 8387.38, average training loss: 9169.57, base loss: 14334.84
[INFO 2017-06-28 15:31:44,165 main.py:51] epoch 12303, training loss: 9204.60, average training loss: 9170.24, base loss: 14335.84
[INFO 2017-06-28 15:31:44,896 main.py:51] epoch 12304, training loss: 8884.11, average training loss: 9169.49, base loss: 14335.58
[INFO 2017-06-28 15:31:45,677 main.py:51] epoch 12305, training loss: 10507.73, average training loss: 9170.34, base loss: 14336.62
[INFO 2017-06-28 15:31:46,344 main.py:51] epoch 12306, training loss: 9944.20, average training loss: 9172.13, base loss: 14339.47
[INFO 2017-06-28 15:31:46,996 main.py:51] epoch 12307, training loss: 10232.26, average training loss: 9173.70, base loss: 14340.63
[INFO 2017-06-28 15:31:47,656 main.py:51] epoch 12308, training loss: 9991.12, average training loss: 9175.41, base loss: 14341.71
[INFO 2017-06-28 15:31:48,322 main.py:51] epoch 12309, training loss: 11078.63, average training loss: 9178.52, base loss: 14344.58
[INFO 2017-06-28 15:31:48,977 main.py:51] epoch 12310, training loss: 9107.53, average training loss: 9178.08, base loss: 14344.11
[INFO 2017-06-28 15:31:49,630 main.py:51] epoch 12311, training loss: 10721.28, average training loss: 9179.07, base loss: 14344.11
[INFO 2017-06-28 15:31:50,311 main.py:51] epoch 12312, training loss: 9212.89, average training loss: 9178.91, base loss: 14344.62
[INFO 2017-06-28 15:31:50,968 main.py:51] epoch 12313, training loss: 9499.64, average training loss: 9179.92, base loss: 14347.55
[INFO 2017-06-28 15:31:51,633 main.py:51] epoch 12314, training loss: 8303.24, average training loss: 9180.65, base loss: 14347.67
[INFO 2017-06-28 15:31:52,288 main.py:51] epoch 12315, training loss: 10072.92, average training loss: 9180.83, base loss: 14347.05
[INFO 2017-06-28 15:31:52,947 main.py:51] epoch 12316, training loss: 8781.53, average training loss: 9180.89, base loss: 14347.70
[INFO 2017-06-28 15:31:53,606 main.py:51] epoch 12317, training loss: 10985.84, average training loss: 9182.26, base loss: 14349.62
[INFO 2017-06-28 15:31:54,256 main.py:51] epoch 12318, training loss: 8675.78, average training loss: 9182.21, base loss: 14347.56
[INFO 2017-06-28 15:31:54,905 main.py:51] epoch 12319, training loss: 9831.85, average training loss: 9183.32, base loss: 14349.43
[INFO 2017-06-28 15:31:55,564 main.py:51] epoch 12320, training loss: 8585.69, average training loss: 9182.87, base loss: 14349.07
[INFO 2017-06-28 15:31:56,247 main.py:51] epoch 12321, training loss: 8117.02, average training loss: 9182.66, base loss: 14348.16
[INFO 2017-06-28 15:31:56,891 main.py:51] epoch 12322, training loss: 10871.67, average training loss: 9184.32, base loss: 14351.10
[INFO 2017-06-28 15:31:57,557 main.py:51] epoch 12323, training loss: 9913.38, average training loss: 9184.78, base loss: 14351.10
[INFO 2017-06-28 15:31:58,196 main.py:51] epoch 12324, training loss: 9760.86, average training loss: 9185.17, base loss: 14350.72
[INFO 2017-06-28 15:31:58,871 main.py:51] epoch 12325, training loss: 8780.92, average training loss: 9185.46, base loss: 14351.85
[INFO 2017-06-28 15:31:59,545 main.py:51] epoch 12326, training loss: 8757.02, average training loss: 9186.53, base loss: 14354.06
[INFO 2017-06-28 15:32:00,195 main.py:51] epoch 12327, training loss: 9316.40, average training loss: 9186.94, base loss: 14353.84
[INFO 2017-06-28 15:32:00,852 main.py:51] epoch 12328, training loss: 9258.78, average training loss: 9187.68, base loss: 14355.11
[INFO 2017-06-28 15:32:01,508 main.py:51] epoch 12329, training loss: 9056.39, average training loss: 9187.78, base loss: 14356.32
[INFO 2017-06-28 15:32:02,175 main.py:51] epoch 12330, training loss: 10277.19, average training loss: 9189.58, base loss: 14360.97
[INFO 2017-06-28 15:32:02,843 main.py:51] epoch 12331, training loss: 9635.49, average training loss: 9189.75, base loss: 14361.55
[INFO 2017-06-28 15:32:03,493 main.py:51] epoch 12332, training loss: 10136.26, average training loss: 9190.83, base loss: 14362.20
[INFO 2017-06-28 15:32:04,159 main.py:51] epoch 12333, training loss: 9530.82, average training loss: 9191.34, base loss: 14364.78
[INFO 2017-06-28 15:32:04,802 main.py:51] epoch 12334, training loss: 9320.69, average training loss: 9189.87, base loss: 14363.01
[INFO 2017-06-28 15:32:05,465 main.py:51] epoch 12335, training loss: 11328.62, average training loss: 9191.30, base loss: 14364.22
[INFO 2017-06-28 15:32:06,135 main.py:51] epoch 12336, training loss: 9530.06, average training loss: 9192.24, base loss: 14366.43
[INFO 2017-06-28 15:32:06,810 main.py:51] epoch 12337, training loss: 8224.53, average training loss: 9191.07, base loss: 14364.36
[INFO 2017-06-28 15:32:07,460 main.py:51] epoch 12338, training loss: 9790.69, average training loss: 9190.01, base loss: 14361.21
[INFO 2017-06-28 15:32:08,124 main.py:51] epoch 12339, training loss: 10644.17, average training loss: 9191.40, base loss: 14363.41
[INFO 2017-06-28 15:32:08,787 main.py:51] epoch 12340, training loss: 8977.16, average training loss: 9190.85, base loss: 14361.96
[INFO 2017-06-28 15:32:09,441 main.py:51] epoch 12341, training loss: 9296.39, average training loss: 9191.22, base loss: 14362.76
[INFO 2017-06-28 15:32:10,110 main.py:51] epoch 12342, training loss: 9453.51, average training loss: 9190.90, base loss: 14363.39
[INFO 2017-06-28 15:32:10,762 main.py:51] epoch 12343, training loss: 8986.79, average training loss: 9191.21, base loss: 14363.39
[INFO 2017-06-28 15:32:11,432 main.py:51] epoch 12344, training loss: 8842.75, average training loss: 9191.24, base loss: 14362.93
[INFO 2017-06-28 15:32:12,080 main.py:51] epoch 12345, training loss: 9727.71, average training loss: 9192.26, base loss: 14362.75
[INFO 2017-06-28 15:32:12,735 main.py:51] epoch 12346, training loss: 9711.96, average training loss: 9193.13, base loss: 14363.57
[INFO 2017-06-28 15:32:13,395 main.py:51] epoch 12347, training loss: 9339.17, average training loss: 9194.24, base loss: 14366.29
[INFO 2017-06-28 15:32:14,058 main.py:51] epoch 12348, training loss: 10109.74, average training loss: 9193.20, base loss: 14361.86
[INFO 2017-06-28 15:32:14,714 main.py:51] epoch 12349, training loss: 9872.35, average training loss: 9193.85, base loss: 14361.10
[INFO 2017-06-28 15:32:15,355 main.py:51] epoch 12350, training loss: 7725.12, average training loss: 9191.03, base loss: 14354.03
[INFO 2017-06-28 15:32:16,019 main.py:51] epoch 12351, training loss: 10640.96, average training loss: 9192.19, base loss: 14355.68
[INFO 2017-06-28 15:32:16,709 main.py:51] epoch 12352, training loss: 9709.16, average training loss: 9193.48, base loss: 14357.83
[INFO 2017-06-28 15:32:17,382 main.py:51] epoch 12353, training loss: 8423.85, average training loss: 9193.41, base loss: 14356.93
[INFO 2017-06-28 15:32:18,036 main.py:51] epoch 12354, training loss: 9704.76, average training loss: 9195.29, base loss: 14359.76
[INFO 2017-06-28 15:32:18,690 main.py:51] epoch 12355, training loss: 8723.90, average training loss: 9194.82, base loss: 14360.97
[INFO 2017-06-28 15:32:19,385 main.py:51] epoch 12356, training loss: 9727.95, average training loss: 9195.53, base loss: 14361.14
[INFO 2017-06-28 15:32:20,049 main.py:51] epoch 12357, training loss: 8453.11, average training loss: 9195.14, base loss: 14359.59
[INFO 2017-06-28 15:32:20,707 main.py:51] epoch 12358, training loss: 9206.46, average training loss: 9194.99, base loss: 14358.89
[INFO 2017-06-28 15:32:21,394 main.py:51] epoch 12359, training loss: 9824.10, average training loss: 9194.97, base loss: 14358.92
[INFO 2017-06-28 15:32:22,047 main.py:51] epoch 12360, training loss: 10204.34, average training loss: 9195.60, base loss: 14359.12
[INFO 2017-06-28 15:32:22,700 main.py:51] epoch 12361, training loss: 9682.00, average training loss: 9196.68, base loss: 14360.93
[INFO 2017-06-28 15:32:23,362 main.py:51] epoch 12362, training loss: 8644.60, average training loss: 9195.11, base loss: 14357.29
[INFO 2017-06-28 15:32:24,011 main.py:51] epoch 12363, training loss: 9349.71, average training loss: 9196.29, base loss: 14358.13
[INFO 2017-06-28 15:32:24,680 main.py:51] epoch 12364, training loss: 9802.29, average training loss: 9196.94, base loss: 14358.85
[INFO 2017-06-28 15:32:25,322 main.py:51] epoch 12365, training loss: 9178.11, average training loss: 9197.23, base loss: 14358.60
[INFO 2017-06-28 15:32:26,006 main.py:51] epoch 12366, training loss: 7857.13, average training loss: 9196.33, base loss: 14357.98
[INFO 2017-06-28 15:32:26,663 main.py:51] epoch 12367, training loss: 10150.13, average training loss: 9198.10, base loss: 14360.28
[INFO 2017-06-28 15:32:27,319 main.py:51] epoch 12368, training loss: 9619.54, average training loss: 9198.95, base loss: 14361.14
[INFO 2017-06-28 15:32:27,972 main.py:51] epoch 12369, training loss: 9621.75, average training loss: 9199.60, base loss: 14362.58
[INFO 2017-06-28 15:32:28,614 main.py:51] epoch 12370, training loss: 9873.24, average training loss: 9200.31, base loss: 14362.85
[INFO 2017-06-28 15:32:29,257 main.py:51] epoch 12371, training loss: 8774.46, average training loss: 9199.34, base loss: 14360.69
[INFO 2017-06-28 15:32:29,908 main.py:51] epoch 12372, training loss: 8381.43, average training loss: 9197.94, base loss: 14358.61
[INFO 2017-06-28 15:32:30,582 main.py:51] epoch 12373, training loss: 9245.29, average training loss: 9197.19, base loss: 14358.96
[INFO 2017-06-28 15:32:31,235 main.py:51] epoch 12374, training loss: 10637.44, average training loss: 9198.90, base loss: 14363.08
[INFO 2017-06-28 15:32:31,894 main.py:51] epoch 12375, training loss: 9231.63, average training loss: 9198.85, base loss: 14362.32
[INFO 2017-06-28 15:32:32,535 main.py:51] epoch 12376, training loss: 8843.28, average training loss: 9198.44, base loss: 14362.63
[INFO 2017-06-28 15:32:33,199 main.py:51] epoch 12377, training loss: 9245.82, average training loss: 9198.41, base loss: 14362.72
[INFO 2017-06-28 15:32:33,878 main.py:51] epoch 12378, training loss: 8561.96, average training loss: 9197.08, base loss: 14360.56
[INFO 2017-06-28 15:32:34,538 main.py:51] epoch 12379, training loss: 9481.60, average training loss: 9198.06, base loss: 14362.31
[INFO 2017-06-28 15:32:35,186 main.py:51] epoch 12380, training loss: 9885.82, average training loss: 9198.50, base loss: 14360.30
[INFO 2017-06-28 15:32:35,847 main.py:51] epoch 12381, training loss: 10895.65, average training loss: 9201.51, base loss: 14365.61
[INFO 2017-06-28 15:32:36,519 main.py:51] epoch 12382, training loss: 11630.42, average training loss: 9203.85, base loss: 14368.84
[INFO 2017-06-28 15:32:37,190 main.py:51] epoch 12383, training loss: 8474.34, average training loss: 9202.66, base loss: 14366.91
[INFO 2017-06-28 15:32:37,852 main.py:51] epoch 12384, training loss: 8303.57, average training loss: 9202.05, base loss: 14366.07
[INFO 2017-06-28 15:32:38,535 main.py:51] epoch 12385, training loss: 8828.47, average training loss: 9201.30, base loss: 14363.24
[INFO 2017-06-28 15:32:39,202 main.py:51] epoch 12386, training loss: 9590.63, average training loss: 9202.06, base loss: 14365.18
[INFO 2017-06-28 15:32:39,856 main.py:51] epoch 12387, training loss: 9439.06, average training loss: 9202.76, base loss: 14365.00
[INFO 2017-06-28 15:32:40,547 main.py:51] epoch 12388, training loss: 8349.92, average training loss: 9201.10, base loss: 14362.33
[INFO 2017-06-28 15:32:41,256 main.py:51] epoch 12389, training loss: 9825.90, average training loss: 9202.00, base loss: 14363.77
[INFO 2017-06-28 15:32:41,929 main.py:51] epoch 12390, training loss: 9673.90, average training loss: 9203.24, base loss: 14365.92
[INFO 2017-06-28 15:32:42,571 main.py:51] epoch 12391, training loss: 9940.94, average training loss: 9203.90, base loss: 14366.74
[INFO 2017-06-28 15:32:43,223 main.py:51] epoch 12392, training loss: 8891.69, average training loss: 9202.94, base loss: 14364.95
[INFO 2017-06-28 15:32:43,891 main.py:51] epoch 12393, training loss: 9243.72, average training loss: 9203.27, base loss: 14365.54
[INFO 2017-06-28 15:32:44,551 main.py:51] epoch 12394, training loss: 10107.15, average training loss: 9203.92, base loss: 14365.87
[INFO 2017-06-28 15:32:45,209 main.py:51] epoch 12395, training loss: 8793.35, average training loss: 9204.08, base loss: 14365.34
[INFO 2017-06-28 15:32:45,880 main.py:51] epoch 12396, training loss: 9350.53, average training loss: 9204.09, base loss: 14367.22
[INFO 2017-06-28 15:32:46,535 main.py:51] epoch 12397, training loss: 9525.19, average training loss: 9205.75, base loss: 14370.96
[INFO 2017-06-28 15:32:47,189 main.py:51] epoch 12398, training loss: 8286.21, average training loss: 9204.59, base loss: 14369.08
[INFO 2017-06-28 15:32:47,852 main.py:51] epoch 12399, training loss: 8702.36, average training loss: 9203.51, base loss: 14367.26
[INFO 2017-06-28 15:32:47,853 main.py:53] epoch 12399, testing
[INFO 2017-06-28 15:32:50,453 main.py:105] average testing loss: 10715.09, base loss: 15433.91
[INFO 2017-06-28 15:32:50,453 main.py:106] improve_loss: 4718.81, improve_percent: 0.31
[INFO 2017-06-28 15:32:50,453 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:32:51,123 main.py:51] epoch 12400, training loss: 9255.36, average training loss: 9204.70, base loss: 14368.58
[INFO 2017-06-28 15:32:51,771 main.py:51] epoch 12401, training loss: 9020.99, average training loss: 9205.30, base loss: 14367.16
[INFO 2017-06-28 15:32:52,446 main.py:51] epoch 12402, training loss: 8304.03, average training loss: 9205.65, base loss: 14368.65
[INFO 2017-06-28 15:32:53,085 main.py:51] epoch 12403, training loss: 8822.83, average training loss: 9206.26, base loss: 14368.83
[INFO 2017-06-28 15:32:53,742 main.py:51] epoch 12404, training loss: 8262.87, average training loss: 9204.00, base loss: 14364.87
[INFO 2017-06-28 15:32:54,400 main.py:51] epoch 12405, training loss: 9203.25, average training loss: 9203.86, base loss: 14365.65
[INFO 2017-06-28 15:32:55,075 main.py:51] epoch 12406, training loss: 11151.53, average training loss: 9206.07, base loss: 14367.67
[INFO 2017-06-28 15:32:55,714 main.py:51] epoch 12407, training loss: 10824.62, average training loss: 9208.97, base loss: 14371.68
[INFO 2017-06-28 15:32:56,379 main.py:51] epoch 12408, training loss: 8227.23, average training loss: 9208.25, base loss: 14370.86
[INFO 2017-06-28 15:32:57,033 main.py:51] epoch 12409, training loss: 9395.11, average training loss: 9208.71, base loss: 14370.83
[INFO 2017-06-28 15:32:57,689 main.py:51] epoch 12410, training loss: 9879.27, average training loss: 9209.80, base loss: 14372.59
[INFO 2017-06-28 15:32:58,347 main.py:51] epoch 12411, training loss: 8877.01, average training loss: 9209.90, base loss: 14372.92
[INFO 2017-06-28 15:32:59,023 main.py:51] epoch 12412, training loss: 8919.64, average training loss: 9209.07, base loss: 14371.98
[INFO 2017-06-28 15:32:59,684 main.py:51] epoch 12413, training loss: 9330.90, average training loss: 9209.32, base loss: 14373.05
[INFO 2017-06-28 15:33:00,338 main.py:51] epoch 12414, training loss: 9712.88, average training loss: 9210.78, base loss: 14376.32
[INFO 2017-06-28 15:33:00,971 main.py:51] epoch 12415, training loss: 9036.94, average training loss: 9211.21, base loss: 14377.96
[INFO 2017-06-28 15:33:01,613 main.py:51] epoch 12416, training loss: 9295.69, average training loss: 9209.37, base loss: 14376.18
[INFO 2017-06-28 15:33:02,288 main.py:51] epoch 12417, training loss: 10126.82, average training loss: 9210.99, base loss: 14380.57
[INFO 2017-06-28 15:33:02,942 main.py:51] epoch 12418, training loss: 8908.22, average training loss: 9211.60, base loss: 14380.39
[INFO 2017-06-28 15:33:03,609 main.py:51] epoch 12419, training loss: 8797.16, average training loss: 9211.85, base loss: 14380.23
[INFO 2017-06-28 15:33:04,262 main.py:51] epoch 12420, training loss: 8366.62, average training loss: 9210.24, base loss: 14376.50
[INFO 2017-06-28 15:33:04,898 main.py:51] epoch 12421, training loss: 9389.96, average training loss: 9208.97, base loss: 14375.47
[INFO 2017-06-28 15:33:05,578 main.py:51] epoch 12422, training loss: 9179.69, average training loss: 9209.78, base loss: 14377.82
[INFO 2017-06-28 15:33:06,250 main.py:51] epoch 12423, training loss: 9632.38, average training loss: 9210.94, base loss: 14380.84
[INFO 2017-06-28 15:33:06,930 main.py:51] epoch 12424, training loss: 9896.65, average training loss: 9212.86, base loss: 14382.52
[INFO 2017-06-28 15:33:07,600 main.py:51] epoch 12425, training loss: 9862.57, average training loss: 9213.82, base loss: 14384.38
[INFO 2017-06-28 15:33:08,268 main.py:51] epoch 12426, training loss: 8868.99, average training loss: 9213.97, base loss: 14383.29
[INFO 2017-06-28 15:33:08,920 main.py:51] epoch 12427, training loss: 8576.70, average training loss: 9214.85, base loss: 14385.20
[INFO 2017-06-28 15:33:09,607 main.py:51] epoch 12428, training loss: 10237.47, average training loss: 9216.53, base loss: 14390.09
[INFO 2017-06-28 15:33:10,275 main.py:51] epoch 12429, training loss: 8219.28, average training loss: 9214.53, base loss: 14386.06
[INFO 2017-06-28 15:33:10,940 main.py:51] epoch 12430, training loss: 10270.79, average training loss: 9214.64, base loss: 14386.67
[INFO 2017-06-28 15:33:11,608 main.py:51] epoch 12431, training loss: 9274.17, average training loss: 9214.56, base loss: 14387.25
[INFO 2017-06-28 15:33:12,258 main.py:51] epoch 12432, training loss: 9408.62, average training loss: 9215.54, base loss: 14389.72
[INFO 2017-06-28 15:33:12,901 main.py:51] epoch 12433, training loss: 8437.08, average training loss: 9214.60, base loss: 14388.44
[INFO 2017-06-28 15:33:13,545 main.py:51] epoch 12434, training loss: 9568.50, average training loss: 9214.65, base loss: 14387.78
[INFO 2017-06-28 15:33:14,214 main.py:51] epoch 12435, training loss: 8521.86, average training loss: 9214.03, base loss: 14386.85
[INFO 2017-06-28 15:33:14,882 main.py:51] epoch 12436, training loss: 9196.42, average training loss: 9213.86, base loss: 14387.63
[INFO 2017-06-28 15:33:15,541 main.py:51] epoch 12437, training loss: 8384.48, average training loss: 9213.39, base loss: 14388.13
[INFO 2017-06-28 15:33:16,184 main.py:51] epoch 12438, training loss: 8882.20, average training loss: 9212.59, base loss: 14387.02
[INFO 2017-06-28 15:33:16,831 main.py:51] epoch 12439, training loss: 9101.98, average training loss: 9212.50, base loss: 14385.36
[INFO 2017-06-28 15:33:17,509 main.py:51] epoch 12440, training loss: 9546.51, average training loss: 9212.72, base loss: 14386.19
[INFO 2017-06-28 15:33:18,177 main.py:51] epoch 12441, training loss: 9380.25, average training loss: 9212.56, base loss: 14385.15
[INFO 2017-06-28 15:33:18,837 main.py:51] epoch 12442, training loss: 8924.24, average training loss: 9212.19, base loss: 14384.64
[INFO 2017-06-28 15:33:19,540 main.py:51] epoch 12443, training loss: 8327.33, average training loss: 9210.20, base loss: 14381.91
[INFO 2017-06-28 15:33:20,200 main.py:51] epoch 12444, training loss: 10285.82, average training loss: 9211.11, base loss: 14382.14
[INFO 2017-06-28 15:33:20,860 main.py:51] epoch 12445, training loss: 9266.20, average training loss: 9211.93, base loss: 14383.31
[INFO 2017-06-28 15:33:21,506 main.py:51] epoch 12446, training loss: 8379.70, average training loss: 9211.44, base loss: 14381.69
[INFO 2017-06-28 15:33:22,152 main.py:51] epoch 12447, training loss: 8091.48, average training loss: 9209.75, base loss: 14379.77
[INFO 2017-06-28 15:33:22,818 main.py:51] epoch 12448, training loss: 10438.04, average training loss: 9209.24, base loss: 14379.64
[INFO 2017-06-28 15:33:23,481 main.py:51] epoch 12449, training loss: 11077.46, average training loss: 9210.54, base loss: 14379.61
[INFO 2017-06-28 15:33:24,134 main.py:51] epoch 12450, training loss: 9275.59, average training loss: 9210.58, base loss: 14376.93
[INFO 2017-06-28 15:33:24,784 main.py:51] epoch 12451, training loss: 9538.81, average training loss: 9211.52, base loss: 14377.84
[INFO 2017-06-28 15:33:25,437 main.py:51] epoch 12452, training loss: 9792.21, average training loss: 9213.42, base loss: 14381.07
[INFO 2017-06-28 15:33:26,085 main.py:51] epoch 12453, training loss: 9310.92, average training loss: 9213.39, base loss: 14380.29
[INFO 2017-06-28 15:33:26,722 main.py:51] epoch 12454, training loss: 8619.74, average training loss: 9213.44, base loss: 14380.50
[INFO 2017-06-28 15:33:27,393 main.py:51] epoch 12455, training loss: 8970.13, average training loss: 9212.48, base loss: 14378.64
[INFO 2017-06-28 15:33:28,077 main.py:51] epoch 12456, training loss: 8819.02, average training loss: 9211.12, base loss: 14375.63
[INFO 2017-06-28 15:33:28,729 main.py:51] epoch 12457, training loss: 8918.71, average training loss: 9211.57, base loss: 14375.28
[INFO 2017-06-28 15:33:29,380 main.py:51] epoch 12458, training loss: 9472.98, average training loss: 9211.77, base loss: 14375.16
[INFO 2017-06-28 15:33:30,036 main.py:51] epoch 12459, training loss: 8917.34, average training loss: 9210.96, base loss: 14375.93
[INFO 2017-06-28 15:33:30,723 main.py:51] epoch 12460, training loss: 9257.99, average training loss: 9209.07, base loss: 14373.65
[INFO 2017-06-28 15:33:31,389 main.py:51] epoch 12461, training loss: 9374.73, average training loss: 9208.08, base loss: 14372.66
[INFO 2017-06-28 15:33:32,031 main.py:51] epoch 12462, training loss: 8983.24, average training loss: 9208.05, base loss: 14373.08
[INFO 2017-06-28 15:33:32,697 main.py:51] epoch 12463, training loss: 9149.79, average training loss: 9207.46, base loss: 14371.89
[INFO 2017-06-28 15:33:33,359 main.py:51] epoch 12464, training loss: 8562.80, average training loss: 9205.58, base loss: 14370.37
[INFO 2017-06-28 15:33:34,086 main.py:51] epoch 12465, training loss: 9039.90, average training loss: 9206.37, base loss: 14371.72
[INFO 2017-06-28 15:33:34,835 main.py:51] epoch 12466, training loss: 8806.54, average training loss: 9204.78, base loss: 14369.12
[INFO 2017-06-28 15:33:35,546 main.py:51] epoch 12467, training loss: 9503.17, average training loss: 9205.07, base loss: 14369.52
[INFO 2017-06-28 15:33:36,267 main.py:51] epoch 12468, training loss: 11342.04, average training loss: 9205.21, base loss: 14369.29
[INFO 2017-06-28 15:33:36,947 main.py:51] epoch 12469, training loss: 9514.55, average training loss: 9206.82, base loss: 14371.89
[INFO 2017-06-28 15:33:37,602 main.py:51] epoch 12470, training loss: 10159.99, average training loss: 9205.99, base loss: 14370.75
[INFO 2017-06-28 15:33:38,298 main.py:51] epoch 12471, training loss: 9402.83, average training loss: 9206.52, base loss: 14371.53
[INFO 2017-06-28 15:33:38,942 main.py:51] epoch 12472, training loss: 9222.69, average training loss: 9206.42, base loss: 14370.56
[INFO 2017-06-28 15:33:39,620 main.py:51] epoch 12473, training loss: 9151.52, average training loss: 9207.51, base loss: 14372.65
[INFO 2017-06-28 15:33:40,302 main.py:51] epoch 12474, training loss: 8827.88, average training loss: 9206.46, base loss: 14370.72
[INFO 2017-06-28 15:33:40,955 main.py:51] epoch 12475, training loss: 8309.78, average training loss: 9204.61, base loss: 14368.01
[INFO 2017-06-28 15:33:41,635 main.py:51] epoch 12476, training loss: 8669.40, average training loss: 9203.85, base loss: 14366.11
[INFO 2017-06-28 15:33:42,281 main.py:51] epoch 12477, training loss: 7984.38, average training loss: 9202.32, base loss: 14364.78
[INFO 2017-06-28 15:33:42,931 main.py:51] epoch 12478, training loss: 9973.64, average training loss: 9203.33, base loss: 14366.76
[INFO 2017-06-28 15:33:43,581 main.py:51] epoch 12479, training loss: 9123.85, average training loss: 9202.83, base loss: 14366.39
[INFO 2017-06-28 15:33:44,234 main.py:51] epoch 12480, training loss: 8166.85, average training loss: 9201.94, base loss: 14364.44
[INFO 2017-06-28 15:33:44,889 main.py:51] epoch 12481, training loss: 9772.88, average training loss: 9201.99, base loss: 14364.47
[INFO 2017-06-28 15:33:45,555 main.py:51] epoch 12482, training loss: 7894.24, average training loss: 9200.62, base loss: 14360.16
[INFO 2017-06-28 15:33:46,235 main.py:51] epoch 12483, training loss: 10155.41, average training loss: 9201.34, base loss: 14363.72
[INFO 2017-06-28 15:33:46,897 main.py:51] epoch 12484, training loss: 8630.15, average training loss: 9199.10, base loss: 14359.65
[INFO 2017-06-28 15:33:47,562 main.py:51] epoch 12485, training loss: 7864.22, average training loss: 9198.00, base loss: 14357.78
[INFO 2017-06-28 15:33:48,213 main.py:51] epoch 12486, training loss: 8010.33, average training loss: 9196.89, base loss: 14356.61
[INFO 2017-06-28 15:33:48,869 main.py:51] epoch 12487, training loss: 9798.12, average training loss: 9197.58, base loss: 14357.23
[INFO 2017-06-28 15:33:49,538 main.py:51] epoch 12488, training loss: 8142.23, average training loss: 9196.12, base loss: 14355.29
[INFO 2017-06-28 15:33:50,214 main.py:51] epoch 12489, training loss: 9187.50, average training loss: 9195.40, base loss: 14353.92
[INFO 2017-06-28 15:33:50,866 main.py:51] epoch 12490, training loss: 9463.23, average training loss: 9196.15, base loss: 14355.20
[INFO 2017-06-28 15:33:51,526 main.py:51] epoch 12491, training loss: 9058.96, average training loss: 9197.40, base loss: 14357.62
[INFO 2017-06-28 15:33:52,180 main.py:51] epoch 12492, training loss: 9457.31, average training loss: 9199.01, base loss: 14360.50
[INFO 2017-06-28 15:33:52,846 main.py:51] epoch 12493, training loss: 8962.34, average training loss: 9199.70, base loss: 14361.51
[INFO 2017-06-28 15:33:53,486 main.py:51] epoch 12494, training loss: 8089.25, average training loss: 9198.29, base loss: 14358.37
[INFO 2017-06-28 15:33:54,146 main.py:51] epoch 12495, training loss: 8835.58, average training loss: 9197.88, base loss: 14358.16
[INFO 2017-06-28 15:33:54,796 main.py:51] epoch 12496, training loss: 9102.01, average training loss: 9196.18, base loss: 14355.54
[INFO 2017-06-28 15:33:55,455 main.py:51] epoch 12497, training loss: 8745.77, average training loss: 9194.30, base loss: 14350.80
[INFO 2017-06-28 15:33:56,140 main.py:51] epoch 12498, training loss: 9661.54, average training loss: 9194.41, base loss: 14350.62
[INFO 2017-06-28 15:33:56,793 main.py:51] epoch 12499, training loss: 7934.44, average training loss: 9193.08, base loss: 14347.89
[INFO 2017-06-28 15:33:56,794 main.py:53] epoch 12499, testing
[INFO 2017-06-28 15:33:59,437 main.py:105] average testing loss: 10089.79, base loss: 14434.57
[INFO 2017-06-28 15:33:59,437 main.py:106] improve_loss: 4344.78, improve_percent: 0.30
[INFO 2017-06-28 15:33:59,438 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:34:00,098 main.py:51] epoch 12500, training loss: 9549.21, average training loss: 9194.33, base loss: 14349.28
[INFO 2017-06-28 15:34:00,768 main.py:51] epoch 12501, training loss: 7900.30, average training loss: 9192.36, base loss: 14344.58
[INFO 2017-06-28 15:34:01,489 main.py:51] epoch 12502, training loss: 9138.00, average training loss: 9190.97, base loss: 14345.19
[INFO 2017-06-28 15:34:02,205 main.py:51] epoch 12503, training loss: 8830.24, average training loss: 9189.39, base loss: 14342.82
[INFO 2017-06-28 15:34:02,937 main.py:51] epoch 12504, training loss: 8397.22, average training loss: 9188.76, base loss: 14340.72
[INFO 2017-06-28 15:34:03,579 main.py:51] epoch 12505, training loss: 7819.53, average training loss: 9186.74, base loss: 14338.71
[INFO 2017-06-28 15:34:04,232 main.py:51] epoch 12506, training loss: 8919.40, average training loss: 9186.05, base loss: 14337.79
[INFO 2017-06-28 15:34:04,897 main.py:51] epoch 12507, training loss: 9107.47, average training loss: 9185.64, base loss: 14338.63
[INFO 2017-06-28 15:34:05,549 main.py:51] epoch 12508, training loss: 9799.00, average training loss: 9186.54, base loss: 14341.29
[INFO 2017-06-28 15:34:06,226 main.py:51] epoch 12509, training loss: 8427.25, average training loss: 9185.58, base loss: 14340.24
[INFO 2017-06-28 15:34:06,866 main.py:51] epoch 12510, training loss: 9971.38, average training loss: 9186.81, base loss: 14341.39
[INFO 2017-06-28 15:34:07,541 main.py:51] epoch 12511, training loss: 8662.52, average training loss: 9186.27, base loss: 14337.30
[INFO 2017-06-28 15:34:08,205 main.py:51] epoch 12512, training loss: 9315.96, average training loss: 9187.31, base loss: 14338.99
[INFO 2017-06-28 15:34:08,864 main.py:51] epoch 12513, training loss: 8474.58, average training loss: 9186.78, base loss: 14339.55
[INFO 2017-06-28 15:34:09,545 main.py:51] epoch 12514, training loss: 8044.10, average training loss: 9185.45, base loss: 14338.67
[INFO 2017-06-28 15:34:10,200 main.py:51] epoch 12515, training loss: 9644.69, average training loss: 9185.97, base loss: 14339.19
[INFO 2017-06-28 15:34:10,878 main.py:51] epoch 12516, training loss: 9307.03, average training loss: 9186.81, base loss: 14342.34
[INFO 2017-06-28 15:34:11,530 main.py:51] epoch 12517, training loss: 9204.28, average training loss: 9186.50, base loss: 14341.09
[INFO 2017-06-28 15:34:12,205 main.py:51] epoch 12518, training loss: 9764.53, average training loss: 9187.87, base loss: 14342.65
[INFO 2017-06-28 15:34:12,858 main.py:51] epoch 12519, training loss: 9191.31, average training loss: 9187.35, base loss: 14340.77
[INFO 2017-06-28 15:34:13,514 main.py:51] epoch 12520, training loss: 9617.58, average training loss: 9188.40, base loss: 14342.88
[INFO 2017-06-28 15:34:14,170 main.py:51] epoch 12521, training loss: 10344.14, average training loss: 9189.75, base loss: 14344.92
[INFO 2017-06-28 15:34:14,838 main.py:51] epoch 12522, training loss: 9971.17, average training loss: 9190.12, base loss: 14346.29
[INFO 2017-06-28 15:34:15,475 main.py:51] epoch 12523, training loss: 9452.95, average training loss: 9190.71, base loss: 14345.75
[INFO 2017-06-28 15:34:16,154 main.py:51] epoch 12524, training loss: 9267.74, average training loss: 9191.04, base loss: 14347.64
[INFO 2017-06-28 15:34:16,800 main.py:51] epoch 12525, training loss: 9633.28, average training loss: 9190.54, base loss: 14346.78
[INFO 2017-06-28 15:34:17,476 main.py:51] epoch 12526, training loss: 8570.97, average training loss: 9189.84, base loss: 14346.04
[INFO 2017-06-28 15:34:18,156 main.py:51] epoch 12527, training loss: 8834.56, average training loss: 9189.09, base loss: 14344.40
[INFO 2017-06-28 15:34:18,828 main.py:51] epoch 12528, training loss: 9632.81, average training loss: 9190.28, base loss: 14347.03
[INFO 2017-06-28 15:34:19,485 main.py:51] epoch 12529, training loss: 9432.18, average training loss: 9190.29, base loss: 14347.83
[INFO 2017-06-28 15:34:20,151 main.py:51] epoch 12530, training loss: 8905.17, average training loss: 9190.79, base loss: 14347.87
[INFO 2017-06-28 15:34:20,810 main.py:51] epoch 12531, training loss: 9742.39, average training loss: 9191.14, base loss: 14350.08
[INFO 2017-06-28 15:34:21,498 main.py:51] epoch 12532, training loss: 8319.06, average training loss: 9190.32, base loss: 14347.48
[INFO 2017-06-28 15:34:22,175 main.py:51] epoch 12533, training loss: 9198.31, average training loss: 9190.43, base loss: 14347.35
[INFO 2017-06-28 15:34:22,843 main.py:51] epoch 12534, training loss: 8712.18, average training loss: 9189.70, base loss: 14346.86
[INFO 2017-06-28 15:34:23,507 main.py:51] epoch 12535, training loss: 8081.99, average training loss: 9188.29, base loss: 14345.67
[INFO 2017-06-28 15:34:24,161 main.py:51] epoch 12536, training loss: 8518.36, average training loss: 9187.99, base loss: 14344.92
[INFO 2017-06-28 15:34:24,814 main.py:51] epoch 12537, training loss: 7804.67, average training loss: 9187.97, base loss: 14345.45
[INFO 2017-06-28 15:34:25,491 main.py:51] epoch 12538, training loss: 9057.39, average training loss: 9186.75, base loss: 14344.31
[INFO 2017-06-28 15:34:26,154 main.py:51] epoch 12539, training loss: 9043.33, average training loss: 9186.09, base loss: 14343.93
[INFO 2017-06-28 15:34:26,854 main.py:51] epoch 12540, training loss: 10494.78, average training loss: 9187.90, base loss: 14347.60
[INFO 2017-06-28 15:34:27,514 main.py:51] epoch 12541, training loss: 10098.30, average training loss: 9188.92, base loss: 14350.07
[INFO 2017-06-28 15:34:28,200 main.py:51] epoch 12542, training loss: 9425.16, average training loss: 9187.88, base loss: 14348.79
[INFO 2017-06-28 15:34:28,853 main.py:51] epoch 12543, training loss: 9881.45, average training loss: 9189.04, base loss: 14351.15
[INFO 2017-06-28 15:34:29,533 main.py:51] epoch 12544, training loss: 8780.90, average training loss: 9188.98, base loss: 14349.79
[INFO 2017-06-28 15:34:30,203 main.py:51] epoch 12545, training loss: 8193.76, average training loss: 9187.39, base loss: 14348.89
[INFO 2017-06-28 15:34:30,871 main.py:51] epoch 12546, training loss: 7842.29, average training loss: 9185.59, base loss: 14345.91
[INFO 2017-06-28 15:34:31,531 main.py:51] epoch 12547, training loss: 9673.62, average training loss: 9186.77, base loss: 14349.23
[INFO 2017-06-28 15:34:32,199 main.py:51] epoch 12548, training loss: 8818.21, average training loss: 9184.75, base loss: 14346.57
[INFO 2017-06-28 15:34:32,863 main.py:51] epoch 12549, training loss: 9210.97, average training loss: 9185.59, base loss: 14349.56
[INFO 2017-06-28 15:34:33,522 main.py:51] epoch 12550, training loss: 9180.70, average training loss: 9184.70, base loss: 14347.34
[INFO 2017-06-28 15:34:34,183 main.py:51] epoch 12551, training loss: 9593.07, average training loss: 9186.00, base loss: 14350.38
[INFO 2017-06-28 15:34:34,860 main.py:51] epoch 12552, training loss: 8503.77, average training loss: 9185.79, base loss: 14349.40
[INFO 2017-06-28 15:34:35,527 main.py:51] epoch 12553, training loss: 9912.05, average training loss: 9187.79, base loss: 14352.05
[INFO 2017-06-28 15:34:36,193 main.py:51] epoch 12554, training loss: 9951.53, average training loss: 9188.13, base loss: 14353.05
[INFO 2017-06-28 15:34:36,863 main.py:51] epoch 12555, training loss: 8713.37, average training loss: 9187.04, base loss: 14350.97
[INFO 2017-06-28 15:34:37,529 main.py:51] epoch 12556, training loss: 9461.52, average training loss: 9186.64, base loss: 14350.65
[INFO 2017-06-28 15:34:38,177 main.py:51] epoch 12557, training loss: 8884.08, average training loss: 9186.13, base loss: 14350.43
[INFO 2017-06-28 15:34:38,854 main.py:51] epoch 12558, training loss: 9065.25, average training loss: 9185.81, base loss: 14348.70
[INFO 2017-06-28 15:34:39,526 main.py:51] epoch 12559, training loss: 9362.59, average training loss: 9186.24, base loss: 14348.94
[INFO 2017-06-28 15:34:40,175 main.py:51] epoch 12560, training loss: 8878.57, average training loss: 9185.99, base loss: 14348.67
[INFO 2017-06-28 15:34:40,828 main.py:51] epoch 12561, training loss: 8830.74, average training loss: 9184.77, base loss: 14347.44
[INFO 2017-06-28 15:34:41,473 main.py:51] epoch 12562, training loss: 9225.51, average training loss: 9185.06, base loss: 14347.88
[INFO 2017-06-28 15:34:42,138 main.py:51] epoch 12563, training loss: 8377.46, average training loss: 9184.52, base loss: 14348.59
[INFO 2017-06-28 15:34:42,808 main.py:51] epoch 12564, training loss: 11787.86, average training loss: 9187.77, base loss: 14355.02
[INFO 2017-06-28 15:34:43,474 main.py:51] epoch 12565, training loss: 7865.28, average training loss: 9186.50, base loss: 14352.33
[INFO 2017-06-28 15:34:44,144 main.py:51] epoch 12566, training loss: 9699.24, average training loss: 9187.48, base loss: 14354.98
[INFO 2017-06-28 15:34:44,802 main.py:51] epoch 12567, training loss: 8921.02, average training loss: 9187.35, base loss: 14353.33
[INFO 2017-06-28 15:34:45,455 main.py:51] epoch 12568, training loss: 8655.00, average training loss: 9187.57, base loss: 14353.30
[INFO 2017-06-28 15:34:46,140 main.py:51] epoch 12569, training loss: 10196.18, average training loss: 9188.28, base loss: 14354.24
[INFO 2017-06-28 15:34:46,817 main.py:51] epoch 12570, training loss: 9608.69, average training loss: 9187.64, base loss: 14352.39
[INFO 2017-06-28 15:34:47,473 main.py:51] epoch 12571, training loss: 8446.48, average training loss: 9186.05, base loss: 14349.24
[INFO 2017-06-28 15:34:48,126 main.py:51] epoch 12572, training loss: 8001.69, average training loss: 9185.16, base loss: 14347.89
[INFO 2017-06-28 15:34:48,792 main.py:51] epoch 12573, training loss: 9222.05, average training loss: 9185.56, base loss: 14348.75
[INFO 2017-06-28 15:34:49,435 main.py:51] epoch 12574, training loss: 7838.07, average training loss: 9184.79, base loss: 14347.30
[INFO 2017-06-28 15:34:50,116 main.py:51] epoch 12575, training loss: 8524.67, average training loss: 9183.94, base loss: 14345.62
[INFO 2017-06-28 15:34:50,774 main.py:51] epoch 12576, training loss: 8708.75, average training loss: 9183.02, base loss: 14343.28
[INFO 2017-06-28 15:34:51,412 main.py:51] epoch 12577, training loss: 10495.89, average training loss: 9183.96, base loss: 14345.65
[INFO 2017-06-28 15:34:52,071 main.py:51] epoch 12578, training loss: 9532.34, average training loss: 9184.15, base loss: 14345.56
[INFO 2017-06-28 15:34:52,754 main.py:51] epoch 12579, training loss: 10308.11, average training loss: 9185.46, base loss: 14347.79
[INFO 2017-06-28 15:34:53,440 main.py:51] epoch 12580, training loss: 8104.89, average training loss: 9185.59, base loss: 14349.17
[INFO 2017-06-28 15:34:54,101 main.py:51] epoch 12581, training loss: 9438.80, average training loss: 9185.70, base loss: 14347.40
[INFO 2017-06-28 15:34:54,765 main.py:51] epoch 12582, training loss: 8626.72, average training loss: 9185.49, base loss: 14345.63
[INFO 2017-06-28 15:34:55,427 main.py:51] epoch 12583, training loss: 10698.50, average training loss: 9186.49, base loss: 14347.43
[INFO 2017-06-28 15:34:56,085 main.py:51] epoch 12584, training loss: 10637.14, average training loss: 9187.30, base loss: 14348.52
[INFO 2017-06-28 15:34:56,745 main.py:51] epoch 12585, training loss: 8969.37, average training loss: 9187.30, base loss: 14348.48
[INFO 2017-06-28 15:34:57,380 main.py:51] epoch 12586, training loss: 8182.10, average training loss: 9184.56, base loss: 14344.68
[INFO 2017-06-28 15:34:58,055 main.py:51] epoch 12587, training loss: 9149.04, average training loss: 9183.16, base loss: 14343.47
[INFO 2017-06-28 15:34:58,690 main.py:51] epoch 12588, training loss: 8396.14, average training loss: 9181.75, base loss: 14340.38
[INFO 2017-06-28 15:34:59,323 main.py:51] epoch 12589, training loss: 9734.58, average training loss: 9181.71, base loss: 14341.26
[INFO 2017-06-28 15:34:59,983 main.py:51] epoch 12590, training loss: 9722.59, average training loss: 9183.13, base loss: 14343.87
[INFO 2017-06-28 15:35:00,655 main.py:51] epoch 12591, training loss: 9187.98, average training loss: 9183.55, base loss: 14344.47
[INFO 2017-06-28 15:35:01,308 main.py:51] epoch 12592, training loss: 9331.48, average training loss: 9183.96, base loss: 14345.22
[INFO 2017-06-28 15:35:01,962 main.py:51] epoch 12593, training loss: 9377.36, average training loss: 9182.08, base loss: 14342.35
[INFO 2017-06-28 15:35:02,608 main.py:51] epoch 12594, training loss: 7213.32, average training loss: 9180.43, base loss: 14339.40
[INFO 2017-06-28 15:35:03,248 main.py:51] epoch 12595, training loss: 9237.20, average training loss: 9181.57, base loss: 14340.51
[INFO 2017-06-28 15:35:03,922 main.py:51] epoch 12596, training loss: 8347.17, average training loss: 9181.46, base loss: 14338.88
[INFO 2017-06-28 15:35:04,596 main.py:51] epoch 12597, training loss: 7943.76, average training loss: 9180.41, base loss: 14337.75
[INFO 2017-06-28 15:35:05,260 main.py:51] epoch 12598, training loss: 9184.84, average training loss: 9180.45, base loss: 14338.97
[INFO 2017-06-28 15:35:05,908 main.py:51] epoch 12599, training loss: 9137.85, average training loss: 9180.68, base loss: 14337.72
[INFO 2017-06-28 15:35:05,908 main.py:53] epoch 12599, testing
[INFO 2017-06-28 15:35:08,542 main.py:105] average testing loss: 10797.08, base loss: 15078.90
[INFO 2017-06-28 15:35:08,542 main.py:106] improve_loss: 4281.82, improve_percent: 0.28
[INFO 2017-06-28 15:35:08,543 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:35:09,200 main.py:51] epoch 12600, training loss: 8987.95, average training loss: 9180.64, base loss: 14338.58
[INFO 2017-06-28 15:35:09,876 main.py:51] epoch 12601, training loss: 8894.39, average training loss: 9180.54, base loss: 14337.91
[INFO 2017-06-28 15:35:10,523 main.py:51] epoch 12602, training loss: 8918.70, average training loss: 9178.71, base loss: 14336.07
[INFO 2017-06-28 15:35:11,185 main.py:51] epoch 12603, training loss: 8897.13, average training loss: 9179.09, base loss: 14337.02
[INFO 2017-06-28 15:35:11,850 main.py:51] epoch 12604, training loss: 8729.88, average training loss: 9179.21, base loss: 14337.48
[INFO 2017-06-28 15:35:12,510 main.py:51] epoch 12605, training loss: 7772.14, average training loss: 9178.13, base loss: 14333.85
[INFO 2017-06-28 15:35:13,151 main.py:51] epoch 12606, training loss: 9220.80, average training loss: 9178.99, base loss: 14335.31
[INFO 2017-06-28 15:35:13,813 main.py:51] epoch 12607, training loss: 8166.18, average training loss: 9177.93, base loss: 14334.47
[INFO 2017-06-28 15:35:14,489 main.py:51] epoch 12608, training loss: 9099.72, average training loss: 9177.77, base loss: 14333.45
[INFO 2017-06-28 15:35:15,144 main.py:51] epoch 12609, training loss: 8614.91, average training loss: 9178.20, base loss: 14334.88
[INFO 2017-06-28 15:35:15,794 main.py:51] epoch 12610, training loss: 8806.13, average training loss: 9178.66, base loss: 14335.93
[INFO 2017-06-28 15:35:16,451 main.py:51] epoch 12611, training loss: 8387.23, average training loss: 9177.88, base loss: 14334.67
[INFO 2017-06-28 15:35:17,103 main.py:51] epoch 12612, training loss: 8114.13, average training loss: 9177.43, base loss: 14335.30
[INFO 2017-06-28 15:35:17,764 main.py:51] epoch 12613, training loss: 8437.49, average training loss: 9176.49, base loss: 14335.41
[INFO 2017-06-28 15:35:18,443 main.py:51] epoch 12614, training loss: 10293.29, average training loss: 9177.50, base loss: 14335.43
[INFO 2017-06-28 15:35:19,116 main.py:51] epoch 12615, training loss: 7802.90, average training loss: 9175.36, base loss: 14331.98
[INFO 2017-06-28 15:35:19,764 main.py:51] epoch 12616, training loss: 10162.93, average training loss: 9176.04, base loss: 14332.34
[INFO 2017-06-28 15:35:20,408 main.py:51] epoch 12617, training loss: 8940.26, average training loss: 9176.22, base loss: 14332.21
[INFO 2017-06-28 15:35:21,120 main.py:51] epoch 12618, training loss: 8383.94, average training loss: 9176.03, base loss: 14331.93
[INFO 2017-06-28 15:35:21,853 main.py:51] epoch 12619, training loss: 8165.19, average training loss: 9175.58, base loss: 14332.17
[INFO 2017-06-28 15:35:22,540 main.py:51] epoch 12620, training loss: 9276.18, average training loss: 9175.78, base loss: 14333.73
[INFO 2017-06-28 15:35:23,185 main.py:51] epoch 12621, training loss: 8706.26, average training loss: 9176.49, base loss: 14334.29
[INFO 2017-06-28 15:35:23,837 main.py:51] epoch 12622, training loss: 8594.67, average training loss: 9175.54, base loss: 14333.07
[INFO 2017-06-28 15:35:24,485 main.py:51] epoch 12623, training loss: 9678.02, average training loss: 9176.68, base loss: 14334.93
[INFO 2017-06-28 15:35:25,126 main.py:51] epoch 12624, training loss: 10355.74, average training loss: 9176.96, base loss: 14334.77
[INFO 2017-06-28 15:35:25,795 main.py:51] epoch 12625, training loss: 8169.38, average training loss: 9176.44, base loss: 14334.24
[INFO 2017-06-28 15:35:26,422 main.py:51] epoch 12626, training loss: 8600.22, average training loss: 9175.59, base loss: 14333.94
[INFO 2017-06-28 15:35:27,093 main.py:51] epoch 12627, training loss: 9399.41, average training loss: 9176.79, base loss: 14335.17
[INFO 2017-06-28 15:35:27,787 main.py:51] epoch 12628, training loss: 7709.37, average training loss: 9175.96, base loss: 14332.50
[INFO 2017-06-28 15:35:28,453 main.py:51] epoch 12629, training loss: 9317.66, average training loss: 9175.19, base loss: 14331.15
[INFO 2017-06-28 15:35:29,111 main.py:51] epoch 12630, training loss: 8219.29, average training loss: 9172.88, base loss: 14327.28
[INFO 2017-06-28 15:35:29,773 main.py:51] epoch 12631, training loss: 11374.46, average training loss: 9175.51, base loss: 14330.28
[INFO 2017-06-28 15:35:30,450 main.py:51] epoch 12632, training loss: 9426.29, average training loss: 9176.68, base loss: 14331.31
[INFO 2017-06-28 15:35:31,126 main.py:51] epoch 12633, training loss: 10296.33, average training loss: 9177.57, base loss: 14333.60
[INFO 2017-06-28 15:35:31,790 main.py:51] epoch 12634, training loss: 8788.54, average training loss: 9177.00, base loss: 14331.30
[INFO 2017-06-28 15:35:32,451 main.py:51] epoch 12635, training loss: 9586.81, average training loss: 9177.85, base loss: 14333.28
[INFO 2017-06-28 15:35:33,114 main.py:51] epoch 12636, training loss: 9337.06, average training loss: 9178.98, base loss: 14334.70
[INFO 2017-06-28 15:35:33,770 main.py:51] epoch 12637, training loss: 8423.05, average training loss: 9179.22, base loss: 14334.74
[INFO 2017-06-28 15:35:34,438 main.py:51] epoch 12638, training loss: 8550.81, average training loss: 9179.40, base loss: 14335.12
[INFO 2017-06-28 15:35:35,103 main.py:51] epoch 12639, training loss: 8107.97, average training loss: 9178.78, base loss: 14334.35
[INFO 2017-06-28 15:35:35,761 main.py:51] epoch 12640, training loss: 8274.59, average training loss: 9176.78, base loss: 14330.61
[INFO 2017-06-28 15:35:36,413 main.py:51] epoch 12641, training loss: 7989.70, average training loss: 9174.74, base loss: 14327.92
[INFO 2017-06-28 15:35:37,050 main.py:51] epoch 12642, training loss: 9084.59, average training loss: 9175.09, base loss: 14330.23
[INFO 2017-06-28 15:35:37,728 main.py:51] epoch 12643, training loss: 10839.66, average training loss: 9176.83, base loss: 14331.43
[INFO 2017-06-28 15:35:38,360 main.py:51] epoch 12644, training loss: 8914.39, average training loss: 9176.00, base loss: 14331.26
[INFO 2017-06-28 15:35:39,025 main.py:51] epoch 12645, training loss: 9331.50, average training loss: 9176.34, base loss: 14331.80
[INFO 2017-06-28 15:35:39,698 main.py:51] epoch 12646, training loss: 7765.39, average training loss: 9176.00, base loss: 14332.42
[INFO 2017-06-28 15:35:40,335 main.py:51] epoch 12647, training loss: 9976.42, average training loss: 9176.44, base loss: 14332.98
[INFO 2017-06-28 15:35:40,956 main.py:51] epoch 12648, training loss: 8297.92, average training loss: 9175.73, base loss: 14331.58
[INFO 2017-06-28 15:35:41,611 main.py:51] epoch 12649, training loss: 9063.90, average training loss: 9175.66, base loss: 14329.78
[INFO 2017-06-28 15:35:42,274 main.py:51] epoch 12650, training loss: 8685.56, average training loss: 9174.13, base loss: 14328.74
[INFO 2017-06-28 15:35:42,944 main.py:51] epoch 12651, training loss: 9351.91, average training loss: 9174.51, base loss: 14329.21
[INFO 2017-06-28 15:35:43,602 main.py:51] epoch 12652, training loss: 8519.68, average training loss: 9174.23, base loss: 14330.29
[INFO 2017-06-28 15:35:44,266 main.py:51] epoch 12653, training loss: 9765.02, average training loss: 9174.81, base loss: 14332.57
[INFO 2017-06-28 15:35:44,923 main.py:51] epoch 12654, training loss: 9377.62, average training loss: 9175.58, base loss: 14336.15
[INFO 2017-06-28 15:35:45,585 main.py:51] epoch 12655, training loss: 9815.94, average training loss: 9174.92, base loss: 14336.40
[INFO 2017-06-28 15:35:46,248 main.py:51] epoch 12656, training loss: 9839.94, average training loss: 9176.22, base loss: 14338.71
[INFO 2017-06-28 15:35:46,892 main.py:51] epoch 12657, training loss: 9120.21, average training loss: 9175.07, base loss: 14338.22
[INFO 2017-06-28 15:35:47,547 main.py:51] epoch 12658, training loss: 9734.24, average training loss: 9174.65, base loss: 14338.35
[INFO 2017-06-28 15:35:48,206 main.py:51] epoch 12659, training loss: 9714.47, average training loss: 9174.71, base loss: 14337.27
[INFO 2017-06-28 15:35:48,849 main.py:51] epoch 12660, training loss: 9606.86, average training loss: 9175.11, base loss: 14339.54
[INFO 2017-06-28 15:35:49,520 main.py:51] epoch 12661, training loss: 10025.25, average training loss: 9175.17, base loss: 14339.10
[INFO 2017-06-28 15:35:50,172 main.py:51] epoch 12662, training loss: 8288.67, average training loss: 9174.59, base loss: 14338.09
[INFO 2017-06-28 15:35:50,831 main.py:51] epoch 12663, training loss: 9306.14, average training loss: 9174.70, base loss: 14338.82
[INFO 2017-06-28 15:35:51,499 main.py:51] epoch 12664, training loss: 8259.80, average training loss: 9174.17, base loss: 14338.17
[INFO 2017-06-28 15:35:52,184 main.py:51] epoch 12665, training loss: 8550.67, average training loss: 9173.17, base loss: 14337.51
[INFO 2017-06-28 15:35:52,832 main.py:51] epoch 12666, training loss: 10130.80, average training loss: 9174.12, base loss: 14339.38
[INFO 2017-06-28 15:35:53,482 main.py:51] epoch 12667, training loss: 9390.37, average training loss: 9173.98, base loss: 14340.09
[INFO 2017-06-28 15:35:54,151 main.py:51] epoch 12668, training loss: 9624.69, average training loss: 9174.59, base loss: 14339.99
[INFO 2017-06-28 15:35:54,831 main.py:51] epoch 12669, training loss: 8387.86, average training loss: 9173.74, base loss: 14339.73
[INFO 2017-06-28 15:35:55,513 main.py:51] epoch 12670, training loss: 10227.89, average training loss: 9174.21, base loss: 14342.14
[INFO 2017-06-28 15:35:56,170 main.py:51] epoch 12671, training loss: 8773.40, average training loss: 9173.60, base loss: 14340.23
[INFO 2017-06-28 15:35:56,814 main.py:51] epoch 12672, training loss: 9176.69, average training loss: 9174.47, base loss: 14341.67
[INFO 2017-06-28 15:35:57,457 main.py:51] epoch 12673, training loss: 9531.39, average training loss: 9174.28, base loss: 14341.07
[INFO 2017-06-28 15:35:58,134 main.py:51] epoch 12674, training loss: 9601.30, average training loss: 9174.47, base loss: 14340.40
[INFO 2017-06-28 15:35:58,794 main.py:51] epoch 12675, training loss: 8804.54, average training loss: 9173.52, base loss: 14339.02
[INFO 2017-06-28 15:35:59,463 main.py:51] epoch 12676, training loss: 8581.58, average training loss: 9172.62, base loss: 14336.99
[INFO 2017-06-28 15:36:00,117 main.py:51] epoch 12677, training loss: 9523.31, average training loss: 9172.78, base loss: 14336.76
[INFO 2017-06-28 15:36:00,776 main.py:51] epoch 12678, training loss: 9329.78, average training loss: 9173.58, base loss: 14337.34
[INFO 2017-06-28 15:36:01,427 main.py:51] epoch 12679, training loss: 9187.27, average training loss: 9174.31, base loss: 14339.16
[INFO 2017-06-28 15:36:02,103 main.py:51] epoch 12680, training loss: 8071.74, average training loss: 9173.44, base loss: 14337.19
[INFO 2017-06-28 15:36:02,756 main.py:51] epoch 12681, training loss: 8182.27, average training loss: 9171.23, base loss: 14333.13
[INFO 2017-06-28 15:36:03,403 main.py:51] epoch 12682, training loss: 9544.73, average training loss: 9171.05, base loss: 14332.97
[INFO 2017-06-28 15:36:04,106 main.py:51] epoch 12683, training loss: 8374.35, average training loss: 9170.56, base loss: 14331.20
[INFO 2017-06-28 15:36:04,754 main.py:51] epoch 12684, training loss: 10271.06, average training loss: 9171.13, base loss: 14333.25
[INFO 2017-06-28 15:36:05,409 main.py:51] epoch 12685, training loss: 10172.11, average training loss: 9171.63, base loss: 14333.88
[INFO 2017-06-28 15:36:06,077 main.py:51] epoch 12686, training loss: 9568.14, average training loss: 9172.41, base loss: 14334.32
[INFO 2017-06-28 15:36:06,739 main.py:51] epoch 12687, training loss: 10095.70, average training loss: 9174.07, base loss: 14337.03
[INFO 2017-06-28 15:36:07,381 main.py:51] epoch 12688, training loss: 10433.24, average training loss: 9176.35, base loss: 14339.41
[INFO 2017-06-28 15:36:08,044 main.py:51] epoch 12689, training loss: 8707.17, average training loss: 9177.23, base loss: 14340.42
[INFO 2017-06-28 15:36:08,703 main.py:51] epoch 12690, training loss: 8912.43, average training loss: 9177.31, base loss: 14340.81
[INFO 2017-06-28 15:36:09,361 main.py:51] epoch 12691, training loss: 10169.32, average training loss: 9178.52, base loss: 14343.30
[INFO 2017-06-28 15:36:10,008 main.py:51] epoch 12692, training loss: 9177.47, average training loss: 9178.13, base loss: 14342.36
[INFO 2017-06-28 15:36:10,678 main.py:51] epoch 12693, training loss: 10215.68, average training loss: 9179.55, base loss: 14344.33
[INFO 2017-06-28 15:36:11,342 main.py:51] epoch 12694, training loss: 8371.92, average training loss: 9177.44, base loss: 14339.48
[INFO 2017-06-28 15:36:11,985 main.py:51] epoch 12695, training loss: 9828.97, average training loss: 9178.67, base loss: 14341.78
[INFO 2017-06-28 15:36:12,650 main.py:51] epoch 12696, training loss: 7927.93, average training loss: 9176.68, base loss: 14338.11
[INFO 2017-06-28 15:36:13,304 main.py:51] epoch 12697, training loss: 8198.02, average training loss: 9177.43, base loss: 14339.77
[INFO 2017-06-28 15:36:13,969 main.py:51] epoch 12698, training loss: 8147.36, average training loss: 9177.06, base loss: 14339.23
[INFO 2017-06-28 15:36:14,625 main.py:51] epoch 12699, training loss: 9613.09, average training loss: 9177.80, base loss: 14341.85
[INFO 2017-06-28 15:36:14,625 main.py:53] epoch 12699, testing
[INFO 2017-06-28 15:36:17,302 main.py:105] average testing loss: 10661.07, base loss: 15461.30
[INFO 2017-06-28 15:36:17,302 main.py:106] improve_loss: 4800.23, improve_percent: 0.31
[INFO 2017-06-28 15:36:17,303 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:36:17,968 main.py:51] epoch 12700, training loss: 8811.12, average training loss: 9177.63, base loss: 14341.64
[INFO 2017-06-28 15:36:18,612 main.py:51] epoch 12701, training loss: 8344.40, average training loss: 9177.02, base loss: 14340.39
[INFO 2017-06-28 15:36:19,266 main.py:51] epoch 12702, training loss: 9151.41, average training loss: 9178.31, base loss: 14342.92
[INFO 2017-06-28 15:36:19,907 main.py:51] epoch 12703, training loss: 8359.06, average training loss: 9177.23, base loss: 14339.90
[INFO 2017-06-28 15:36:20,592 main.py:51] epoch 12704, training loss: 10102.60, average training loss: 9176.55, base loss: 14338.71
[INFO 2017-06-28 15:36:21,244 main.py:51] epoch 12705, training loss: 8294.76, average training loss: 9175.79, base loss: 14337.25
[INFO 2017-06-28 15:36:21,875 main.py:51] epoch 12706, training loss: 7734.57, average training loss: 9174.93, base loss: 14334.19
[INFO 2017-06-28 15:36:22,516 main.py:51] epoch 12707, training loss: 9211.19, average training loss: 9175.29, base loss: 14335.00
[INFO 2017-06-28 15:36:23,179 main.py:51] epoch 12708, training loss: 9282.12, average training loss: 9176.69, base loss: 14336.99
[INFO 2017-06-28 15:36:23,842 main.py:51] epoch 12709, training loss: 7988.11, average training loss: 9175.37, base loss: 14335.14
[INFO 2017-06-28 15:36:24,484 main.py:51] epoch 12710, training loss: 9638.06, average training loss: 9176.17, base loss: 14336.28
[INFO 2017-06-28 15:36:25,153 main.py:51] epoch 12711, training loss: 8078.70, average training loss: 9175.38, base loss: 14334.73
[INFO 2017-06-28 15:36:25,821 main.py:51] epoch 12712, training loss: 8737.92, average training loss: 9175.20, base loss: 14336.22
[INFO 2017-06-28 15:36:26,468 main.py:51] epoch 12713, training loss: 8814.65, average training loss: 9175.17, base loss: 14338.02
[INFO 2017-06-28 15:36:27,130 main.py:51] epoch 12714, training loss: 8360.88, average training loss: 9175.50, base loss: 14338.55
[INFO 2017-06-28 15:36:27,787 main.py:51] epoch 12715, training loss: 8603.55, average training loss: 9174.60, base loss: 14334.89
[INFO 2017-06-28 15:36:28,466 main.py:51] epoch 12716, training loss: 9126.42, average training loss: 9175.20, base loss: 14334.78
[INFO 2017-06-28 15:36:29,124 main.py:51] epoch 12717, training loss: 8539.30, average training loss: 9174.86, base loss: 14334.18
[INFO 2017-06-28 15:36:29,795 main.py:51] epoch 12718, training loss: 9110.99, average training loss: 9174.79, base loss: 14333.97
[INFO 2017-06-28 15:36:30,512 main.py:51] epoch 12719, training loss: 9640.80, average training loss: 9174.54, base loss: 14333.24
[INFO 2017-06-28 15:36:31,184 main.py:51] epoch 12720, training loss: 8193.89, average training loss: 9173.15, base loss: 14331.31
[INFO 2017-06-28 15:36:31,866 main.py:51] epoch 12721, training loss: 9353.02, average training loss: 9173.34, base loss: 14330.49
[INFO 2017-06-28 15:36:32,544 main.py:51] epoch 12722, training loss: 9257.75, average training loss: 9173.15, base loss: 14331.58
[INFO 2017-06-28 15:36:33,201 main.py:51] epoch 12723, training loss: 9367.61, average training loss: 9172.96, base loss: 14332.02
[INFO 2017-06-28 15:36:33,854 main.py:51] epoch 12724, training loss: 8871.28, average training loss: 9171.73, base loss: 14330.59
[INFO 2017-06-28 15:36:34,526 main.py:51] epoch 12725, training loss: 9815.76, average training loss: 9173.30, base loss: 14334.21
[INFO 2017-06-28 15:36:35,168 main.py:51] epoch 12726, training loss: 8804.67, average training loss: 9171.87, base loss: 14332.59
[INFO 2017-06-28 15:36:35,834 main.py:51] epoch 12727, training loss: 9064.90, average training loss: 9171.61, base loss: 14331.72
[INFO 2017-06-28 15:36:36,514 main.py:51] epoch 12728, training loss: 10654.97, average training loss: 9171.62, base loss: 14331.77
[INFO 2017-06-28 15:36:37,189 main.py:51] epoch 12729, training loss: 8550.64, average training loss: 9170.18, base loss: 14331.02
[INFO 2017-06-28 15:36:37,850 main.py:51] epoch 12730, training loss: 9812.78, average training loss: 9170.75, base loss: 14331.23
[INFO 2017-06-28 15:36:38,504 main.py:51] epoch 12731, training loss: 10353.66, average training loss: 9171.55, base loss: 14333.06
[INFO 2017-06-28 15:36:39,176 main.py:51] epoch 12732, training loss: 9947.97, average training loss: 9173.33, base loss: 14335.11
[INFO 2017-06-28 15:36:39,860 main.py:51] epoch 12733, training loss: 9595.49, average training loss: 9172.69, base loss: 14333.75
[INFO 2017-06-28 15:36:40,515 main.py:51] epoch 12734, training loss: 8285.32, average training loss: 9172.77, base loss: 14334.60
[INFO 2017-06-28 15:36:41,176 main.py:51] epoch 12735, training loss: 11444.22, average training loss: 9174.74, base loss: 14336.95
[INFO 2017-06-28 15:36:41,832 main.py:51] epoch 12736, training loss: 9015.82, average training loss: 9173.89, base loss: 14335.17
[INFO 2017-06-28 15:36:42,489 main.py:51] epoch 12737, training loss: 9205.84, average training loss: 9174.14, base loss: 14336.24
[INFO 2017-06-28 15:36:43,152 main.py:51] epoch 12738, training loss: 9313.41, average training loss: 9174.23, base loss: 14335.98
[INFO 2017-06-28 15:36:43,831 main.py:51] epoch 12739, training loss: 8758.08, average training loss: 9174.87, base loss: 14339.45
[INFO 2017-06-28 15:36:44,499 main.py:51] epoch 12740, training loss: 8511.30, average training loss: 9175.58, base loss: 14340.12
[INFO 2017-06-28 15:36:45,141 main.py:51] epoch 12741, training loss: 8407.20, average training loss: 9174.75, base loss: 14338.86
[INFO 2017-06-28 15:36:45,846 main.py:51] epoch 12742, training loss: 11235.81, average training loss: 9176.54, base loss: 14341.40
[INFO 2017-06-28 15:36:46,502 main.py:51] epoch 12743, training loss: 10125.95, average training loss: 9176.88, base loss: 14341.53
[INFO 2017-06-28 15:36:47,183 main.py:51] epoch 12744, training loss: 7335.64, average training loss: 9175.49, base loss: 14340.91
[INFO 2017-06-28 15:36:47,845 main.py:51] epoch 12745, training loss: 10730.72, average training loss: 9177.36, base loss: 14344.54
[INFO 2017-06-28 15:36:48,516 main.py:51] epoch 12746, training loss: 9723.62, average training loss: 9178.19, base loss: 14343.80
[INFO 2017-06-28 15:36:49,168 main.py:51] epoch 12747, training loss: 8574.65, average training loss: 9179.32, base loss: 14347.19
[INFO 2017-06-28 15:36:49,831 main.py:51] epoch 12748, training loss: 8672.48, average training loss: 9178.70, base loss: 14346.73
[INFO 2017-06-28 15:36:50,492 main.py:51] epoch 12749, training loss: 9834.42, average training loss: 9180.01, base loss: 14350.24
[INFO 2017-06-28 15:36:51,205 main.py:51] epoch 12750, training loss: 8257.17, average training loss: 9179.40, base loss: 14348.73
[INFO 2017-06-28 15:36:51,910 main.py:51] epoch 12751, training loss: 9432.98, average training loss: 9179.29, base loss: 14348.36
[INFO 2017-06-28 15:36:52,559 main.py:51] epoch 12752, training loss: 8163.15, average training loss: 9178.39, base loss: 14347.00
[INFO 2017-06-28 15:36:53,225 main.py:51] epoch 12753, training loss: 9245.80, average training loss: 9178.60, base loss: 14349.07
[INFO 2017-06-28 15:36:53,879 main.py:51] epoch 12754, training loss: 9259.10, average training loss: 9178.64, base loss: 14349.04
[INFO 2017-06-28 15:36:54,531 main.py:51] epoch 12755, training loss: 11771.02, average training loss: 9182.42, base loss: 14354.17
[INFO 2017-06-28 15:36:55,192 main.py:51] epoch 12756, training loss: 7813.67, average training loss: 9181.60, base loss: 14353.36
[INFO 2017-06-28 15:36:55,869 main.py:51] epoch 12757, training loss: 8691.45, average training loss: 9181.01, base loss: 14351.13
[INFO 2017-06-28 15:36:56,530 main.py:51] epoch 12758, training loss: 8923.35, average training loss: 9181.46, base loss: 14351.91
[INFO 2017-06-28 15:36:57,201 main.py:51] epoch 12759, training loss: 9097.37, average training loss: 9181.82, base loss: 14352.15
[INFO 2017-06-28 15:36:57,848 main.py:51] epoch 12760, training loss: 9292.23, average training loss: 9182.49, base loss: 14353.50
[INFO 2017-06-28 15:36:58,513 main.py:51] epoch 12761, training loss: 11747.42, average training loss: 9185.63, base loss: 14357.32
[INFO 2017-06-28 15:36:59,170 main.py:51] epoch 12762, training loss: 7981.87, average training loss: 9186.38, base loss: 14358.70
[INFO 2017-06-28 15:36:59,819 main.py:51] epoch 12763, training loss: 10090.89, average training loss: 9187.94, base loss: 14363.47
[INFO 2017-06-28 15:37:00,473 main.py:51] epoch 12764, training loss: 8341.53, average training loss: 9188.54, base loss: 14363.95
[INFO 2017-06-28 15:37:01,163 main.py:51] epoch 12765, training loss: 9417.61, average training loss: 9188.59, base loss: 14364.74
[INFO 2017-06-28 15:37:01,818 main.py:51] epoch 12766, training loss: 9120.05, average training loss: 9188.93, base loss: 14365.41
[INFO 2017-06-28 15:37:02,489 main.py:51] epoch 12767, training loss: 8683.02, average training loss: 9188.55, base loss: 14363.54
[INFO 2017-06-28 15:37:03,166 main.py:51] epoch 12768, training loss: 8640.21, average training loss: 9189.21, base loss: 14366.62
[INFO 2017-06-28 15:37:03,817 main.py:51] epoch 12769, training loss: 9161.42, average training loss: 9189.44, base loss: 14366.39
[INFO 2017-06-28 15:37:04,475 main.py:51] epoch 12770, training loss: 9679.15, average training loss: 9189.15, base loss: 14366.74
[INFO 2017-06-28 15:37:05,122 main.py:51] epoch 12771, training loss: 10397.40, average training loss: 9191.49, base loss: 14371.23
[INFO 2017-06-28 15:37:05,785 main.py:51] epoch 12772, training loss: 8502.57, average training loss: 9191.42, base loss: 14371.98
[INFO 2017-06-28 15:37:06,450 main.py:51] epoch 12773, training loss: 9552.18, average training loss: 9191.62, base loss: 14371.37
[INFO 2017-06-28 15:37:07,117 main.py:51] epoch 12774, training loss: 9621.03, average training loss: 9192.86, base loss: 14372.43
[INFO 2017-06-28 15:37:07,779 main.py:51] epoch 12775, training loss: 10607.80, average training loss: 9195.22, base loss: 14375.66
[INFO 2017-06-28 15:37:08,456 main.py:51] epoch 12776, training loss: 9369.80, average training loss: 9194.77, base loss: 14375.32
[INFO 2017-06-28 15:37:09,138 main.py:51] epoch 12777, training loss: 9580.92, average training loss: 9194.18, base loss: 14372.71
[INFO 2017-06-28 15:37:09,807 main.py:51] epoch 12778, training loss: 9453.93, average training loss: 9192.49, base loss: 14370.62
[INFO 2017-06-28 15:37:10,448 main.py:51] epoch 12779, training loss: 8519.74, average training loss: 9192.57, base loss: 14371.84
[INFO 2017-06-28 15:37:11,098 main.py:51] epoch 12780, training loss: 8902.70, average training loss: 9191.82, base loss: 14371.23
[INFO 2017-06-28 15:37:11,765 main.py:51] epoch 12781, training loss: 9437.90, average training loss: 9191.65, base loss: 14370.81
[INFO 2017-06-28 15:37:12,407 main.py:51] epoch 12782, training loss: 10016.36, average training loss: 9193.04, base loss: 14372.90
[INFO 2017-06-28 15:37:13,066 main.py:51] epoch 12783, training loss: 8025.34, average training loss: 9193.09, base loss: 14373.18
[INFO 2017-06-28 15:37:13,727 main.py:51] epoch 12784, training loss: 8539.70, average training loss: 9192.51, base loss: 14370.50
[INFO 2017-06-28 15:37:14,373 main.py:51] epoch 12785, training loss: 7349.36, average training loss: 9189.53, base loss: 14365.28
[INFO 2017-06-28 15:37:15,048 main.py:51] epoch 12786, training loss: 8792.50, average training loss: 9189.15, base loss: 14365.47
[INFO 2017-06-28 15:37:15,745 main.py:51] epoch 12787, training loss: 8724.92, average training loss: 9188.32, base loss: 14364.41
[INFO 2017-06-28 15:37:16,413 main.py:51] epoch 12788, training loss: 9694.16, average training loss: 9188.80, base loss: 14366.97
[INFO 2017-06-28 15:37:17,087 main.py:51] epoch 12789, training loss: 8615.49, average training loss: 9188.85, base loss: 14366.42
[INFO 2017-06-28 15:37:17,765 main.py:51] epoch 12790, training loss: 9747.03, average training loss: 9189.41, base loss: 14367.78
[INFO 2017-06-28 15:37:18,424 main.py:51] epoch 12791, training loss: 8228.67, average training loss: 9188.52, base loss: 14366.38
[INFO 2017-06-28 15:37:19,088 main.py:51] epoch 12792, training loss: 10326.27, average training loss: 9190.35, base loss: 14370.75
[INFO 2017-06-28 15:37:19,767 main.py:51] epoch 12793, training loss: 9656.02, average training loss: 9190.36, base loss: 14371.64
[INFO 2017-06-28 15:37:20,429 main.py:51] epoch 12794, training loss: 8703.62, average training loss: 9189.43, base loss: 14369.39
[INFO 2017-06-28 15:37:21,099 main.py:51] epoch 12795, training loss: 8830.89, average training loss: 9189.31, base loss: 14369.11
[INFO 2017-06-28 15:37:21,786 main.py:51] epoch 12796, training loss: 8777.13, average training loss: 9189.24, base loss: 14370.08
[INFO 2017-06-28 15:37:22,447 main.py:51] epoch 12797, training loss: 8582.46, average training loss: 9187.66, base loss: 14367.21
[INFO 2017-06-28 15:37:23,104 main.py:51] epoch 12798, training loss: 9073.20, average training loss: 9188.62, base loss: 14369.79
[INFO 2017-06-28 15:37:23,816 main.py:51] epoch 12799, training loss: 7925.86, average training loss: 9186.72, base loss: 14366.68
[INFO 2017-06-28 15:37:23,816 main.py:53] epoch 12799, testing
[INFO 2017-06-28 15:37:26,424 main.py:105] average testing loss: 10855.01, base loss: 15805.83
[INFO 2017-06-28 15:37:26,424 main.py:106] improve_loss: 4950.82, improve_percent: 0.31
[INFO 2017-06-28 15:37:26,424 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:37:27,078 main.py:51] epoch 12800, training loss: 8299.30, average training loss: 9185.23, base loss: 14365.67
[INFO 2017-06-28 15:37:27,731 main.py:51] epoch 12801, training loss: 7987.70, average training loss: 9184.44, base loss: 14363.42
[INFO 2017-06-28 15:37:28,384 main.py:51] epoch 12802, training loss: 8768.83, average training loss: 9184.53, base loss: 14361.84
[INFO 2017-06-28 15:37:29,070 main.py:51] epoch 12803, training loss: 8828.15, average training loss: 9182.29, base loss: 14357.06
[INFO 2017-06-28 15:37:29,756 main.py:51] epoch 12804, training loss: 8971.43, average training loss: 9182.32, base loss: 14355.94
[INFO 2017-06-28 15:37:30,449 main.py:51] epoch 12805, training loss: 8651.53, average training loss: 9182.15, base loss: 14354.36
[INFO 2017-06-28 15:37:31,111 main.py:51] epoch 12806, training loss: 9182.74, average training loss: 9182.98, base loss: 14356.39
[INFO 2017-06-28 15:37:31,772 main.py:51] epoch 12807, training loss: 9877.29, average training loss: 9184.54, base loss: 14359.17
[INFO 2017-06-28 15:37:32,440 main.py:51] epoch 12808, training loss: 9122.94, average training loss: 9185.32, base loss: 14361.33
[INFO 2017-06-28 15:37:33,079 main.py:51] epoch 12809, training loss: 8116.25, average training loss: 9183.91, base loss: 14359.96
[INFO 2017-06-28 15:37:33,741 main.py:51] epoch 12810, training loss: 8844.82, average training loss: 9183.87, base loss: 14361.28
[INFO 2017-06-28 15:37:34,413 main.py:51] epoch 12811, training loss: 9430.25, average training loss: 9184.88, base loss: 14363.02
[INFO 2017-06-28 15:37:35,063 main.py:51] epoch 12812, training loss: 10597.71, average training loss: 9185.38, base loss: 14363.89
[INFO 2017-06-28 15:37:35,739 main.py:51] epoch 12813, training loss: 8996.69, average training loss: 9185.07, base loss: 14364.34
[INFO 2017-06-28 15:37:36,387 main.py:51] epoch 12814, training loss: 9686.42, average training loss: 9184.53, base loss: 14364.00
[INFO 2017-06-28 15:37:37,042 main.py:51] epoch 12815, training loss: 8765.17, average training loss: 9184.18, base loss: 14363.59
[INFO 2017-06-28 15:37:37,694 main.py:51] epoch 12816, training loss: 8153.08, average training loss: 9183.98, base loss: 14363.21
[INFO 2017-06-28 15:37:38,372 main.py:51] epoch 12817, training loss: 11479.59, average training loss: 9185.65, base loss: 14364.98
[INFO 2017-06-28 15:37:39,049 main.py:51] epoch 12818, training loss: 9288.21, average training loss: 9186.95, base loss: 14368.81
[INFO 2017-06-28 15:37:39,754 main.py:51] epoch 12819, training loss: 10292.22, average training loss: 9186.31, base loss: 14367.19
[INFO 2017-06-28 15:37:40,441 main.py:51] epoch 12820, training loss: 8526.66, average training loss: 9184.33, base loss: 14363.77
[INFO 2017-06-28 15:37:41,132 main.py:51] epoch 12821, training loss: 8243.05, average training loss: 9183.03, base loss: 14361.44
[INFO 2017-06-28 15:37:41,819 main.py:51] epoch 12822, training loss: 9605.50, average training loss: 9183.27, base loss: 14362.49
[INFO 2017-06-28 15:37:42,496 main.py:51] epoch 12823, training loss: 8447.02, average training loss: 9182.93, base loss: 14363.40
[INFO 2017-06-28 15:37:43,188 main.py:51] epoch 12824, training loss: 9465.00, average training loss: 9182.64, base loss: 14363.40
[INFO 2017-06-28 15:37:43,852 main.py:51] epoch 12825, training loss: 10387.38, average training loss: 9183.97, base loss: 14365.69
[INFO 2017-06-28 15:37:44,532 main.py:51] epoch 12826, training loss: 8108.79, average training loss: 9181.84, base loss: 14362.30
[INFO 2017-06-28 15:37:45,189 main.py:51] epoch 12827, training loss: 8256.82, average training loss: 9181.39, base loss: 14361.18
[INFO 2017-06-28 15:37:45,871 main.py:51] epoch 12828, training loss: 8620.66, average training loss: 9181.66, base loss: 14361.41
[INFO 2017-06-28 15:37:46,543 main.py:51] epoch 12829, training loss: 9384.53, average training loss: 9183.36, base loss: 14363.85
[INFO 2017-06-28 15:37:47,191 main.py:51] epoch 12830, training loss: 8620.55, average training loss: 9183.09, base loss: 14363.25
[INFO 2017-06-28 15:37:47,846 main.py:51] epoch 12831, training loss: 9575.17, average training loss: 9183.78, base loss: 14364.22
[INFO 2017-06-28 15:37:48,497 main.py:51] epoch 12832, training loss: 9266.28, average training loss: 9182.74, base loss: 14361.41
[INFO 2017-06-28 15:37:49,136 main.py:51] epoch 12833, training loss: 7779.42, average training loss: 9180.78, base loss: 14357.43
[INFO 2017-06-28 15:37:49,773 main.py:51] epoch 12834, training loss: 8823.03, average training loss: 9181.46, base loss: 14357.92
[INFO 2017-06-28 15:37:50,420 main.py:51] epoch 12835, training loss: 10116.36, average training loss: 9182.30, base loss: 14359.37
[INFO 2017-06-28 15:37:51,073 main.py:51] epoch 12836, training loss: 9070.04, average training loss: 9183.23, base loss: 14360.04
[INFO 2017-06-28 15:37:51,734 main.py:51] epoch 12837, training loss: 9193.52, average training loss: 9183.20, base loss: 14360.57
[INFO 2017-06-28 15:37:52,400 main.py:51] epoch 12838, training loss: 10606.77, average training loss: 9184.65, base loss: 14363.66
[INFO 2017-06-28 15:37:53,061 main.py:51] epoch 12839, training loss: 9395.49, average training loss: 9184.93, base loss: 14362.03
[INFO 2017-06-28 15:37:53,727 main.py:51] epoch 12840, training loss: 9348.02, average training loss: 9185.39, base loss: 14363.09
[INFO 2017-06-28 15:37:54,402 main.py:51] epoch 12841, training loss: 9861.53, average training loss: 9186.43, base loss: 14365.76
[INFO 2017-06-28 15:37:55,083 main.py:51] epoch 12842, training loss: 10071.86, average training loss: 9186.77, base loss: 14366.99
[INFO 2017-06-28 15:37:55,789 main.py:51] epoch 12843, training loss: 9377.90, average training loss: 9187.76, base loss: 14370.06
[INFO 2017-06-28 15:37:56,459 main.py:51] epoch 12844, training loss: 9964.56, average training loss: 9188.91, base loss: 14371.83
[INFO 2017-06-28 15:37:57,144 main.py:51] epoch 12845, training loss: 11249.85, average training loss: 9192.25, base loss: 14376.81
[INFO 2017-06-28 15:37:57,799 main.py:51] epoch 12846, training loss: 11177.84, average training loss: 9194.97, base loss: 14381.76
[INFO 2017-06-28 15:37:58,427 main.py:51] epoch 12847, training loss: 9264.95, average training loss: 9195.66, base loss: 14382.44
[INFO 2017-06-28 15:37:59,076 main.py:51] epoch 12848, training loss: 9861.46, average training loss: 9195.22, base loss: 14382.06
[INFO 2017-06-28 15:37:59,740 main.py:51] epoch 12849, training loss: 8728.55, average training loss: 9196.02, base loss: 14383.79
[INFO 2017-06-28 15:38:00,386 main.py:51] epoch 12850, training loss: 9240.43, average training loss: 9195.69, base loss: 14382.49
[INFO 2017-06-28 15:38:01,036 main.py:51] epoch 12851, training loss: 9267.60, average training loss: 9196.23, base loss: 14383.23
[INFO 2017-06-28 15:38:01,699 main.py:51] epoch 12852, training loss: 9178.18, average training loss: 9196.67, base loss: 14384.10
[INFO 2017-06-28 15:38:02,358 main.py:51] epoch 12853, training loss: 8815.51, average training loss: 9196.13, base loss: 14382.94
[INFO 2017-06-28 15:38:03,046 main.py:51] epoch 12854, training loss: 8899.79, average training loss: 9196.35, base loss: 14383.31
[INFO 2017-06-28 15:38:03,695 main.py:51] epoch 12855, training loss: 8785.55, average training loss: 9196.28, base loss: 14382.98
[INFO 2017-06-28 15:38:04,340 main.py:51] epoch 12856, training loss: 9757.87, average training loss: 9196.14, base loss: 14383.25
[INFO 2017-06-28 15:38:05,025 main.py:51] epoch 12857, training loss: 9427.03, average training loss: 9196.97, base loss: 14382.87
[INFO 2017-06-28 15:38:05,682 main.py:51] epoch 12858, training loss: 8124.00, average training loss: 9196.48, base loss: 14382.34
[INFO 2017-06-28 15:38:06,341 main.py:51] epoch 12859, training loss: 9176.08, average training loss: 9198.35, base loss: 14385.74
[INFO 2017-06-28 15:38:07,015 main.py:51] epoch 12860, training loss: 10194.31, average training loss: 9199.95, base loss: 14387.42
[INFO 2017-06-28 15:38:07,659 main.py:51] epoch 12861, training loss: 9392.77, average training loss: 9198.63, base loss: 14387.28
[INFO 2017-06-28 15:38:08,338 main.py:51] epoch 12862, training loss: 9425.96, average training loss: 9197.78, base loss: 14384.63
[INFO 2017-06-28 15:38:08,986 main.py:51] epoch 12863, training loss: 10544.66, average training loss: 9199.76, base loss: 14387.43
[INFO 2017-06-28 15:38:09,641 main.py:51] epoch 12864, training loss: 8863.59, average training loss: 9199.58, base loss: 14387.82
[INFO 2017-06-28 15:38:10,304 main.py:51] epoch 12865, training loss: 9368.88, average training loss: 9199.62, base loss: 14386.68
[INFO 2017-06-28 15:38:10,970 main.py:51] epoch 12866, training loss: 10920.08, average training loss: 9200.77, base loss: 14390.29
[INFO 2017-06-28 15:38:11,680 main.py:51] epoch 12867, training loss: 9727.70, average training loss: 9200.85, base loss: 14391.62
[INFO 2017-06-28 15:38:12,348 main.py:51] epoch 12868, training loss: 8849.75, average training loss: 9201.05, base loss: 14391.31
[INFO 2017-06-28 15:38:13,053 main.py:51] epoch 12869, training loss: 9167.89, average training loss: 9200.84, base loss: 14390.04
[INFO 2017-06-28 15:38:13,753 main.py:51] epoch 12870, training loss: 8953.95, average training loss: 9201.04, base loss: 14390.43
[INFO 2017-06-28 15:38:14,471 main.py:51] epoch 12871, training loss: 9862.90, average training loss: 9202.24, base loss: 14391.94
[INFO 2017-06-28 15:38:15,161 main.py:51] epoch 12872, training loss: 9350.31, average training loss: 9202.23, base loss: 14392.34
[INFO 2017-06-28 15:38:15,839 main.py:51] epoch 12873, training loss: 7714.92, average training loss: 9199.70, base loss: 14389.04
[INFO 2017-06-28 15:38:16,554 main.py:51] epoch 12874, training loss: 9445.47, average training loss: 9200.10, base loss: 14389.30
[INFO 2017-06-28 15:38:17,250 main.py:51] epoch 12875, training loss: 8972.00, average training loss: 9200.26, base loss: 14390.44
[INFO 2017-06-28 15:38:17,917 main.py:51] epoch 12876, training loss: 8445.02, average training loss: 9200.27, base loss: 14389.42
[INFO 2017-06-28 15:38:18,592 main.py:51] epoch 12877, training loss: 9264.57, average training loss: 9200.74, base loss: 14390.09
[INFO 2017-06-28 15:38:19,276 main.py:51] epoch 12878, training loss: 9870.50, average training loss: 9202.00, base loss: 14391.07
[INFO 2017-06-28 15:38:20,003 main.py:51] epoch 12879, training loss: 9720.18, average training loss: 9203.02, base loss: 14393.80
[INFO 2017-06-28 15:38:20,691 main.py:51] epoch 12880, training loss: 8414.97, average training loss: 9202.44, base loss: 14392.60
[INFO 2017-06-28 15:38:21,369 main.py:51] epoch 12881, training loss: 8683.75, average training loss: 9202.60, base loss: 14392.95
[INFO 2017-06-28 15:38:22,065 main.py:51] epoch 12882, training loss: 9134.77, average training loss: 9201.71, base loss: 14393.10
[INFO 2017-06-28 15:38:22,730 main.py:51] epoch 12883, training loss: 8551.82, average training loss: 9201.73, base loss: 14392.58
[INFO 2017-06-28 15:38:23,399 main.py:51] epoch 12884, training loss: 9182.75, average training loss: 9202.59, base loss: 14392.77
[INFO 2017-06-28 15:38:24,058 main.py:51] epoch 12885, training loss: 9448.38, average training loss: 9202.38, base loss: 14393.41
[INFO 2017-06-28 15:38:24,724 main.py:51] epoch 12886, training loss: 8997.91, average training loss: 9201.26, base loss: 14392.21
[INFO 2017-06-28 15:38:25,370 main.py:51] epoch 12887, training loss: 7176.17, average training loss: 9198.75, base loss: 14389.62
[INFO 2017-06-28 15:38:26,019 main.py:51] epoch 12888, training loss: 9359.83, average training loss: 9198.14, base loss: 14389.06
[INFO 2017-06-28 15:38:26,666 main.py:51] epoch 12889, training loss: 9217.13, average training loss: 9198.73, base loss: 14391.73
[INFO 2017-06-28 15:38:27,346 main.py:51] epoch 12890, training loss: 10516.72, average training loss: 9200.40, base loss: 14394.74
[INFO 2017-06-28 15:38:28,007 main.py:51] epoch 12891, training loss: 10352.74, average training loss: 9201.75, base loss: 14395.48
[INFO 2017-06-28 15:38:28,680 main.py:51] epoch 12892, training loss: 8223.94, average training loss: 9200.71, base loss: 14393.17
[INFO 2017-06-28 15:38:29,328 main.py:51] epoch 12893, training loss: 10146.25, average training loss: 9202.19, base loss: 14395.80
[INFO 2017-06-28 15:38:29,968 main.py:51] epoch 12894, training loss: 10088.13, average training loss: 9202.34, base loss: 14395.90
[INFO 2017-06-28 15:38:30,634 main.py:51] epoch 12895, training loss: 7882.03, average training loss: 9201.65, base loss: 14395.05
[INFO 2017-06-28 15:38:31,277 main.py:51] epoch 12896, training loss: 8507.62, average training loss: 9201.35, base loss: 14393.67
[INFO 2017-06-28 15:38:31,922 main.py:51] epoch 12897, training loss: 8016.78, average training loss: 9199.43, base loss: 14389.86
[INFO 2017-06-28 15:38:32,584 main.py:51] epoch 12898, training loss: 9026.22, average training loss: 9199.04, base loss: 14390.34
[INFO 2017-06-28 15:38:33,257 main.py:51] epoch 12899, training loss: 9854.31, average training loss: 9200.07, base loss: 14392.98
[INFO 2017-06-28 15:38:33,257 main.py:53] epoch 12899, testing
[INFO 2017-06-28 15:38:35,881 main.py:105] average testing loss: 10435.84, base loss: 15056.04
[INFO 2017-06-28 15:38:35,881 main.py:106] improve_loss: 4620.21, improve_percent: 0.31
[INFO 2017-06-28 15:38:35,881 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:38:36,530 main.py:51] epoch 12900, training loss: 10026.74, average training loss: 9199.49, base loss: 14392.96
[INFO 2017-06-28 15:38:37,172 main.py:51] epoch 12901, training loss: 9987.37, average training loss: 9199.81, base loss: 14391.88
[INFO 2017-06-28 15:38:37,847 main.py:51] epoch 12902, training loss: 9883.90, average training loss: 9200.18, base loss: 14394.97
[INFO 2017-06-28 15:38:38,517 main.py:51] epoch 12903, training loss: 10306.35, average training loss: 9202.02, base loss: 14398.19
[INFO 2017-06-28 15:38:39,171 main.py:51] epoch 12904, training loss: 10484.59, average training loss: 9203.39, base loss: 14401.47
[INFO 2017-06-28 15:38:39,848 main.py:51] epoch 12905, training loss: 8850.42, average training loss: 9203.49, base loss: 14401.54
[INFO 2017-06-28 15:38:40,516 main.py:51] epoch 12906, training loss: 9570.14, average training loss: 9203.32, base loss: 14400.27
[INFO 2017-06-28 15:38:41,176 main.py:51] epoch 12907, training loss: 9724.76, average training loss: 9203.01, base loss: 14400.80
[INFO 2017-06-28 15:38:41,827 main.py:51] epoch 12908, training loss: 9955.09, average training loss: 9203.46, base loss: 14402.02
[INFO 2017-06-28 15:38:42,498 main.py:51] epoch 12909, training loss: 8717.56, average training loss: 9201.98, base loss: 14398.53
[INFO 2017-06-28 15:38:43,161 main.py:51] epoch 12910, training loss: 9677.33, average training loss: 9202.52, base loss: 14400.45
[INFO 2017-06-28 15:38:43,842 main.py:51] epoch 12911, training loss: 9261.24, average training loss: 9202.47, base loss: 14400.12
[INFO 2017-06-28 15:38:44,493 main.py:51] epoch 12912, training loss: 9057.73, average training loss: 9201.67, base loss: 14399.64
[INFO 2017-06-28 15:38:45,165 main.py:51] epoch 12913, training loss: 9182.43, average training loss: 9202.58, base loss: 14399.77
[INFO 2017-06-28 15:38:45,839 main.py:51] epoch 12914, training loss: 9488.62, average training loss: 9203.46, base loss: 14401.44
[INFO 2017-06-28 15:38:46,531 main.py:51] epoch 12915, training loss: 9909.72, average training loss: 9204.30, base loss: 14402.77
[INFO 2017-06-28 15:38:47,219 main.py:51] epoch 12916, training loss: 8706.07, average training loss: 9204.08, base loss: 14403.39
[INFO 2017-06-28 15:38:47,924 main.py:51] epoch 12917, training loss: 9131.95, average training loss: 9204.26, base loss: 14404.35
[INFO 2017-06-28 15:38:48,587 main.py:51] epoch 12918, training loss: 10246.87, average training loss: 9206.51, base loss: 14407.31
[INFO 2017-06-28 15:38:49,252 main.py:51] epoch 12919, training loss: 8975.76, average training loss: 9204.64, base loss: 14404.99
[INFO 2017-06-28 15:38:49,931 main.py:51] epoch 12920, training loss: 11077.42, average training loss: 9204.79, base loss: 14406.51
[INFO 2017-06-28 15:38:50,600 main.py:51] epoch 12921, training loss: 9061.84, average training loss: 9205.39, base loss: 14407.80
[INFO 2017-06-28 15:38:51,251 main.py:51] epoch 12922, training loss: 9163.00, average training loss: 9204.89, base loss: 14407.28
[INFO 2017-06-28 15:38:51,905 main.py:51] epoch 12923, training loss: 7889.82, average training loss: 9202.93, base loss: 14403.32
[INFO 2017-06-28 15:38:52,560 main.py:51] epoch 12924, training loss: 7602.18, average training loss: 9201.33, base loss: 14399.78
[INFO 2017-06-28 15:38:53,219 main.py:51] epoch 12925, training loss: 10439.62, average training loss: 9202.17, base loss: 14400.95
[INFO 2017-06-28 15:38:53,875 main.py:51] epoch 12926, training loss: 10365.29, average training loss: 9203.10, base loss: 14402.11
[INFO 2017-06-28 15:38:54,549 main.py:51] epoch 12927, training loss: 11059.46, average training loss: 9205.71, base loss: 14407.48
[INFO 2017-06-28 15:38:55,221 main.py:51] epoch 12928, training loss: 8768.91, average training loss: 9205.53, base loss: 14405.82
[INFO 2017-06-28 15:38:55,894 main.py:51] epoch 12929, training loss: 10287.24, average training loss: 9206.34, base loss: 14408.31
[INFO 2017-06-28 15:38:56,581 main.py:51] epoch 12930, training loss: 8774.59, average training loss: 9206.21, base loss: 14407.72
[INFO 2017-06-28 15:38:57,262 main.py:51] epoch 12931, training loss: 9311.01, average training loss: 9205.37, base loss: 14406.08
[INFO 2017-06-28 15:38:57,948 main.py:51] epoch 12932, training loss: 8982.16, average training loss: 9204.87, base loss: 14405.29
[INFO 2017-06-28 15:38:58,627 main.py:51] epoch 12933, training loss: 9157.57, average training loss: 9204.81, base loss: 14404.93
[INFO 2017-06-28 15:38:59,265 main.py:51] epoch 12934, training loss: 8449.70, average training loss: 9204.52, base loss: 14405.41
[INFO 2017-06-28 15:38:59,922 main.py:51] epoch 12935, training loss: 7957.61, average training loss: 9203.65, base loss: 14403.66
[INFO 2017-06-28 15:39:00,581 main.py:51] epoch 12936, training loss: 9399.65, average training loss: 9203.86, base loss: 14404.15
[INFO 2017-06-28 15:39:01,272 main.py:51] epoch 12937, training loss: 8500.98, average training loss: 9203.60, base loss: 14403.39
[INFO 2017-06-28 15:39:01,965 main.py:51] epoch 12938, training loss: 8743.85, average training loss: 9201.80, base loss: 14399.53
[INFO 2017-06-28 15:39:02,639 main.py:51] epoch 12939, training loss: 8224.17, average training loss: 9201.66, base loss: 14398.39
[INFO 2017-06-28 15:39:03,303 main.py:51] epoch 12940, training loss: 9117.61, average training loss: 9201.77, base loss: 14398.94
[INFO 2017-06-28 15:39:03,969 main.py:51] epoch 12941, training loss: 10079.34, average training loss: 9203.17, base loss: 14399.52
[INFO 2017-06-28 15:39:04,646 main.py:51] epoch 12942, training loss: 9152.69, average training loss: 9200.44, base loss: 14395.66
[INFO 2017-06-28 15:39:05,315 main.py:51] epoch 12943, training loss: 10032.32, average training loss: 9201.10, base loss: 14396.81
[INFO 2017-06-28 15:39:05,970 main.py:51] epoch 12944, training loss: 9078.04, average training loss: 9200.81, base loss: 14397.71
[INFO 2017-06-28 15:39:06,632 main.py:51] epoch 12945, training loss: 9162.85, average training loss: 9199.36, base loss: 14395.08
[INFO 2017-06-28 15:39:07,310 main.py:51] epoch 12946, training loss: 9223.55, average training loss: 9198.64, base loss: 14393.20
[INFO 2017-06-28 15:39:08,011 main.py:51] epoch 12947, training loss: 9563.14, average training loss: 9199.14, base loss: 14395.88
[INFO 2017-06-28 15:39:08,673 main.py:51] epoch 12948, training loss: 8973.88, average training loss: 9199.37, base loss: 14395.64
[INFO 2017-06-28 15:39:09,337 main.py:51] epoch 12949, training loss: 8587.17, average training loss: 9198.50, base loss: 14394.61
[INFO 2017-06-28 15:39:10,001 main.py:51] epoch 12950, training loss: 8417.68, average training loss: 9197.89, base loss: 14392.75
[INFO 2017-06-28 15:39:10,696 main.py:51] epoch 12951, training loss: 9037.92, average training loss: 9196.65, base loss: 14389.73
[INFO 2017-06-28 15:39:11,353 main.py:51] epoch 12952, training loss: 9002.84, average training loss: 9196.31, base loss: 14391.22
[INFO 2017-06-28 15:39:12,023 main.py:51] epoch 12953, training loss: 10872.99, average training loss: 9198.74, base loss: 14395.32
[INFO 2017-06-28 15:39:12,755 main.py:51] epoch 12954, training loss: 9250.25, average training loss: 9198.50, base loss: 14393.95
[INFO 2017-06-28 15:39:13,396 main.py:51] epoch 12955, training loss: 9109.54, average training loss: 9199.42, base loss: 14394.26
[INFO 2017-06-28 15:39:14,041 main.py:51] epoch 12956, training loss: 8674.93, average training loss: 9199.29, base loss: 14394.69
[INFO 2017-06-28 15:39:14,718 main.py:51] epoch 12957, training loss: 9696.97, average training loss: 9199.05, base loss: 14393.37
[INFO 2017-06-28 15:39:15,385 main.py:51] epoch 12958, training loss: 9560.93, average training loss: 9199.44, base loss: 14394.68
[INFO 2017-06-28 15:39:16,122 main.py:51] epoch 12959, training loss: 9362.73, average training loss: 9199.90, base loss: 14393.88
[INFO 2017-06-28 15:39:16,799 main.py:51] epoch 12960, training loss: 8123.26, average training loss: 9197.13, base loss: 14390.81
[INFO 2017-06-28 15:39:17,459 main.py:51] epoch 12961, training loss: 9270.98, average training loss: 9197.35, base loss: 14390.65
[INFO 2017-06-28 15:39:18,159 main.py:51] epoch 12962, training loss: 9031.10, average training loss: 9196.94, base loss: 14390.33
[INFO 2017-06-28 15:39:18,821 main.py:51] epoch 12963, training loss: 9275.53, average training loss: 9196.78, base loss: 14390.73
[INFO 2017-06-28 15:39:19,483 main.py:51] epoch 12964, training loss: 8811.11, average training loss: 9197.44, base loss: 14391.96
[INFO 2017-06-28 15:39:20,135 main.py:51] epoch 12965, training loss: 9006.91, average training loss: 9197.77, base loss: 14393.25
[INFO 2017-06-28 15:39:20,775 main.py:51] epoch 12966, training loss: 9453.88, average training loss: 9197.71, base loss: 14393.91
[INFO 2017-06-28 15:39:21,419 main.py:51] epoch 12967, training loss: 9470.76, average training loss: 9196.27, base loss: 14393.15
[INFO 2017-06-28 15:39:22,123 main.py:51] epoch 12968, training loss: 9241.36, average training loss: 9195.19, base loss: 14392.20
[INFO 2017-06-28 15:39:22,796 main.py:51] epoch 12969, training loss: 8482.22, average training loss: 9194.41, base loss: 14391.44
[INFO 2017-06-28 15:39:23,473 main.py:51] epoch 12970, training loss: 7989.49, average training loss: 9193.48, base loss: 14389.86
[INFO 2017-06-28 15:39:24,134 main.py:51] epoch 12971, training loss: 8375.79, average training loss: 9192.53, base loss: 14387.83
[INFO 2017-06-28 15:39:24,782 main.py:51] epoch 12972, training loss: 8439.76, average training loss: 9190.85, base loss: 14385.29
[INFO 2017-06-28 15:39:25,456 main.py:51] epoch 12973, training loss: 9048.53, average training loss: 9190.19, base loss: 14383.62
[INFO 2017-06-28 15:39:26,115 main.py:51] epoch 12974, training loss: 8845.04, average training loss: 9190.29, base loss: 14383.24
[INFO 2017-06-28 15:39:26,762 main.py:51] epoch 12975, training loss: 9070.93, average training loss: 9190.21, base loss: 14383.52
[INFO 2017-06-28 15:39:27,432 main.py:51] epoch 12976, training loss: 9590.90, average training loss: 9190.78, base loss: 14384.52
[INFO 2017-06-28 15:39:28,123 main.py:51] epoch 12977, training loss: 8945.97, average training loss: 9190.56, base loss: 14384.87
[INFO 2017-06-28 15:39:28,800 main.py:51] epoch 12978, training loss: 7692.87, average training loss: 9189.42, base loss: 14382.95
[INFO 2017-06-28 15:39:29,460 main.py:51] epoch 12979, training loss: 8229.50, average training loss: 9188.62, base loss: 14381.76
[INFO 2017-06-28 15:39:30,119 main.py:51] epoch 12980, training loss: 9602.85, average training loss: 9188.71, base loss: 14381.56
[INFO 2017-06-28 15:39:30,756 main.py:51] epoch 12981, training loss: 9588.99, average training loss: 9188.62, base loss: 14382.25
[INFO 2017-06-28 15:39:31,407 main.py:51] epoch 12982, training loss: 9161.03, average training loss: 9188.21, base loss: 14382.23
[INFO 2017-06-28 15:39:32,042 main.py:51] epoch 12983, training loss: 10007.85, average training loss: 9189.85, base loss: 14384.58
[INFO 2017-06-28 15:39:32,702 main.py:51] epoch 12984, training loss: 8140.18, average training loss: 9190.04, base loss: 14384.75
[INFO 2017-06-28 15:39:33,374 main.py:51] epoch 12985, training loss: 9575.86, average training loss: 9190.78, base loss: 14386.82
[INFO 2017-06-28 15:39:34,049 main.py:51] epoch 12986, training loss: 8443.74, average training loss: 9190.25, base loss: 14386.22
[INFO 2017-06-28 15:39:34,717 main.py:51] epoch 12987, training loss: 9083.97, average training loss: 9189.70, base loss: 14384.64
[INFO 2017-06-28 15:39:35,379 main.py:51] epoch 12988, training loss: 8885.61, average training loss: 9188.14, base loss: 14381.74
[INFO 2017-06-28 15:39:36,026 main.py:51] epoch 12989, training loss: 8215.65, average training loss: 9187.59, base loss: 14380.85
[INFO 2017-06-28 15:39:36,676 main.py:51] epoch 12990, training loss: 8160.71, average training loss: 9186.56, base loss: 14379.13
[INFO 2017-06-28 15:39:37,331 main.py:51] epoch 12991, training loss: 8181.83, average training loss: 9185.36, base loss: 14377.84
[INFO 2017-06-28 15:39:37,972 main.py:51] epoch 12992, training loss: 9805.31, average training loss: 9185.66, base loss: 14378.51
[INFO 2017-06-28 15:39:38,626 main.py:51] epoch 12993, training loss: 8626.59, average training loss: 9186.27, base loss: 14381.48
[INFO 2017-06-28 15:39:39,295 main.py:51] epoch 12994, training loss: 9165.08, average training loss: 9186.56, base loss: 14381.78
[INFO 2017-06-28 15:39:39,965 main.py:51] epoch 12995, training loss: 8449.89, average training loss: 9185.44, base loss: 14380.11
[INFO 2017-06-28 15:39:40,623 main.py:51] epoch 12996, training loss: 9575.93, average training loss: 9184.66, base loss: 14380.69
[INFO 2017-06-28 15:39:41,336 main.py:51] epoch 12997, training loss: 9380.72, average training loss: 9185.43, base loss: 14380.41
[INFO 2017-06-28 15:39:41,986 main.py:51] epoch 12998, training loss: 10385.20, average training loss: 9185.74, base loss: 14380.95
[INFO 2017-06-28 15:39:42,701 main.py:51] epoch 12999, training loss: 9510.44, average training loss: 9185.53, base loss: 14378.92
[INFO 2017-06-28 15:39:42,702 main.py:53] epoch 12999, testing
[INFO 2017-06-28 15:39:45,314 main.py:105] average testing loss: 10199.71, base loss: 14481.52
[INFO 2017-06-28 15:39:45,314 main.py:106] improve_loss: 4281.81, improve_percent: 0.30
[INFO 2017-06-28 15:39:45,315 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:39:45,961 main.py:51] epoch 13000, training loss: 8720.88, average training loss: 9185.48, base loss: 14377.55
[INFO 2017-06-28 15:39:46,615 main.py:51] epoch 13001, training loss: 8581.74, average training loss: 9185.45, base loss: 14377.29
[INFO 2017-06-28 15:39:47,293 main.py:51] epoch 13002, training loss: 8736.13, average training loss: 9184.88, base loss: 14376.66
[INFO 2017-06-28 15:39:47,953 main.py:51] epoch 13003, training loss: 8351.92, average training loss: 9184.41, base loss: 14376.36
[INFO 2017-06-28 15:39:48,600 main.py:51] epoch 13004, training loss: 8899.92, average training loss: 9183.39, base loss: 14375.90
[INFO 2017-06-28 15:39:49,254 main.py:51] epoch 13005, training loss: 7828.82, average training loss: 9182.42, base loss: 14373.30
[INFO 2017-06-28 15:39:49,907 main.py:51] epoch 13006, training loss: 10721.90, average training loss: 9183.91, base loss: 14376.90
[INFO 2017-06-28 15:39:50,579 main.py:51] epoch 13007, training loss: 8754.88, average training loss: 9183.99, base loss: 14377.19
[INFO 2017-06-28 15:39:51,231 main.py:51] epoch 13008, training loss: 8962.80, average training loss: 9183.48, base loss: 14375.95
[INFO 2017-06-28 15:39:51,894 main.py:51] epoch 13009, training loss: 9570.47, average training loss: 9184.47, base loss: 14377.66
[INFO 2017-06-28 15:39:52,560 main.py:51] epoch 13010, training loss: 9291.69, average training loss: 9185.11, base loss: 14378.17
[INFO 2017-06-28 15:39:53,236 main.py:51] epoch 13011, training loss: 8474.01, average training loss: 9184.80, base loss: 14376.75
[INFO 2017-06-28 15:39:53,893 main.py:51] epoch 13012, training loss: 8878.67, average training loss: 9184.84, base loss: 14375.03
[INFO 2017-06-28 15:39:54,586 main.py:51] epoch 13013, training loss: 11043.68, average training loss: 9186.43, base loss: 14374.28
[INFO 2017-06-28 15:39:55,236 main.py:51] epoch 13014, training loss: 10194.29, average training loss: 9188.17, base loss: 14377.82
[INFO 2017-06-28 15:39:55,899 main.py:51] epoch 13015, training loss: 8458.40, average training loss: 9188.30, base loss: 14379.93
[INFO 2017-06-28 15:39:56,564 main.py:51] epoch 13016, training loss: 9108.17, average training loss: 9189.07, base loss: 14381.45
[INFO 2017-06-28 15:39:57,214 main.py:51] epoch 13017, training loss: 8099.87, average training loss: 9186.62, base loss: 14376.74
[INFO 2017-06-28 15:39:57,927 main.py:51] epoch 13018, training loss: 8237.89, average training loss: 9185.22, base loss: 14373.11
[INFO 2017-06-28 15:39:58,690 main.py:51] epoch 13019, training loss: 9677.87, average training loss: 9185.72, base loss: 14374.22
[INFO 2017-06-28 15:39:59,418 main.py:51] epoch 13020, training loss: 8088.60, average training loss: 9184.32, base loss: 14373.01
[INFO 2017-06-28 15:40:00,146 main.py:51] epoch 13021, training loss: 9224.45, average training loss: 9185.00, base loss: 14374.10
[INFO 2017-06-28 15:40:00,902 main.py:51] epoch 13022, training loss: 8946.00, average training loss: 9185.07, base loss: 14373.60
[INFO 2017-06-28 15:40:01,586 main.py:51] epoch 13023, training loss: 9893.26, average training loss: 9186.47, base loss: 14374.97
[INFO 2017-06-28 15:40:02,247 main.py:51] epoch 13024, training loss: 9040.25, average training loss: 9187.14, base loss: 14376.41
[INFO 2017-06-28 15:40:02,903 main.py:51] epoch 13025, training loss: 8952.46, average training loss: 9187.14, base loss: 14376.82
[INFO 2017-06-28 15:40:03,554 main.py:51] epoch 13026, training loss: 9104.15, average training loss: 9186.92, base loss: 14376.70
[INFO 2017-06-28 15:40:04,207 main.py:51] epoch 13027, training loss: 8195.43, average training loss: 9186.67, base loss: 14376.69
[INFO 2017-06-28 15:40:04,852 main.py:51] epoch 13028, training loss: 7471.94, average training loss: 9183.31, base loss: 14371.70
[INFO 2017-06-28 15:40:05,497 main.py:51] epoch 13029, training loss: 8651.59, average training loss: 9182.30, base loss: 14370.22
[INFO 2017-06-28 15:40:06,169 main.py:51] epoch 13030, training loss: 8356.68, average training loss: 9180.99, base loss: 14367.90
[INFO 2017-06-28 15:40:06,822 main.py:51] epoch 13031, training loss: 9159.31, average training loss: 9181.45, base loss: 14369.21
[INFO 2017-06-28 15:40:07,480 main.py:51] epoch 13032, training loss: 8809.13, average training loss: 9181.72, base loss: 14369.93
[INFO 2017-06-28 15:40:08,154 main.py:51] epoch 13033, training loss: 9628.88, average training loss: 9182.33, base loss: 14370.20
[INFO 2017-06-28 15:40:08,822 main.py:51] epoch 13034, training loss: 9740.47, average training loss: 9181.78, base loss: 14370.41
[INFO 2017-06-28 15:40:09,479 main.py:51] epoch 13035, training loss: 9885.45, average training loss: 9182.16, base loss: 14370.42
[INFO 2017-06-28 15:40:10,172 main.py:51] epoch 13036, training loss: 9561.12, average training loss: 9180.98, base loss: 14369.65
[INFO 2017-06-28 15:40:10,827 main.py:51] epoch 13037, training loss: 10200.94, average training loss: 9183.60, base loss: 14373.76
[INFO 2017-06-28 15:40:11,509 main.py:51] epoch 13038, training loss: 10455.05, average training loss: 9186.76, base loss: 14379.30
[INFO 2017-06-28 15:40:12,275 main.py:51] epoch 13039, training loss: 9116.08, average training loss: 9187.32, base loss: 14379.66
[INFO 2017-06-28 15:40:13,029 main.py:51] epoch 13040, training loss: 10278.21, average training loss: 9187.60, base loss: 14378.54
[INFO 2017-06-28 15:40:13,767 main.py:51] epoch 13041, training loss: 9124.81, average training loss: 9188.26, base loss: 14380.06
[INFO 2017-06-28 15:40:14,538 main.py:51] epoch 13042, training loss: 9028.16, average training loss: 9187.96, base loss: 14379.23
[INFO 2017-06-28 15:40:15,199 main.py:51] epoch 13043, training loss: 10160.83, average training loss: 9188.04, base loss: 14380.53
[INFO 2017-06-28 15:40:15,857 main.py:51] epoch 13044, training loss: 9864.81, average training loss: 9189.50, base loss: 14382.16
[INFO 2017-06-28 15:40:16,552 main.py:51] epoch 13045, training loss: 10155.19, average training loss: 9190.42, base loss: 14382.23
[INFO 2017-06-28 15:40:17,195 main.py:51] epoch 13046, training loss: 7649.15, average training loss: 9189.07, base loss: 14379.65
[INFO 2017-06-28 15:40:17,845 main.py:51] epoch 13047, training loss: 8107.00, average training loss: 9186.47, base loss: 14376.08
[INFO 2017-06-28 15:40:18,507 main.py:51] epoch 13048, training loss: 8123.38, average training loss: 9184.88, base loss: 14373.22
[INFO 2017-06-28 15:40:19,194 main.py:51] epoch 13049, training loss: 8560.12, average training loss: 9183.86, base loss: 14371.47
[INFO 2017-06-28 15:40:19,875 main.py:51] epoch 13050, training loss: 9371.89, average training loss: 9182.84, base loss: 14370.24
[INFO 2017-06-28 15:40:20,548 main.py:51] epoch 13051, training loss: 8102.53, average training loss: 9182.82, base loss: 14369.73
[INFO 2017-06-28 15:40:21,211 main.py:51] epoch 13052, training loss: 10171.03, average training loss: 9184.99, base loss: 14371.83
[INFO 2017-06-28 15:40:21,872 main.py:51] epoch 13053, training loss: 8506.31, average training loss: 9183.65, base loss: 14369.08
[INFO 2017-06-28 15:40:22,545 main.py:51] epoch 13054, training loss: 9336.02, average training loss: 9182.55, base loss: 14367.98
[INFO 2017-06-28 15:40:23,217 main.py:51] epoch 13055, training loss: 8681.31, average training loss: 9183.06, base loss: 14369.87
[INFO 2017-06-28 15:40:23,874 main.py:51] epoch 13056, training loss: 8755.87, average training loss: 9182.58, base loss: 14367.85
[INFO 2017-06-28 15:40:24,528 main.py:51] epoch 13057, training loss: 9921.90, average training loss: 9182.20, base loss: 14369.45
[INFO 2017-06-28 15:40:25,187 main.py:51] epoch 13058, training loss: 9404.04, average training loss: 9183.47, base loss: 14371.40
[INFO 2017-06-28 15:40:25,843 main.py:51] epoch 13059, training loss: 9513.38, average training loss: 9183.38, base loss: 14372.38
[INFO 2017-06-28 15:40:26,519 main.py:51] epoch 13060, training loss: 9502.89, average training loss: 9184.39, base loss: 14374.17
[INFO 2017-06-28 15:40:27,174 main.py:51] epoch 13061, training loss: 8675.72, average training loss: 9183.82, base loss: 14373.36
[INFO 2017-06-28 15:40:27,845 main.py:51] epoch 13062, training loss: 9587.97, average training loss: 9184.50, base loss: 14372.78
[INFO 2017-06-28 15:40:28,490 main.py:51] epoch 13063, training loss: 9126.47, average training loss: 9185.06, base loss: 14373.77
[INFO 2017-06-28 15:40:29,157 main.py:51] epoch 13064, training loss: 8947.25, average training loss: 9184.94, base loss: 14373.84
[INFO 2017-06-28 15:40:29,808 main.py:51] epoch 13065, training loss: 9187.39, average training loss: 9184.42, base loss: 14374.82
[INFO 2017-06-28 15:40:30,470 main.py:51] epoch 13066, training loss: 9864.84, average training loss: 9186.01, base loss: 14378.18
[INFO 2017-06-28 15:40:31,126 main.py:51] epoch 13067, training loss: 8383.63, average training loss: 9184.06, base loss: 14374.50
[INFO 2017-06-28 15:40:31,802 main.py:51] epoch 13068, training loss: 10041.57, average training loss: 9185.33, base loss: 14376.60
[INFO 2017-06-28 15:40:32,519 main.py:51] epoch 13069, training loss: 10022.05, average training loss: 9186.91, base loss: 14379.31
[INFO 2017-06-28 15:40:33,163 main.py:51] epoch 13070, training loss: 8774.21, average training loss: 9186.86, base loss: 14378.33
[INFO 2017-06-28 15:40:33,836 main.py:51] epoch 13071, training loss: 8670.61, average training loss: 9186.09, base loss: 14377.40
[INFO 2017-06-28 15:40:34,476 main.py:51] epoch 13072, training loss: 9509.06, average training loss: 9186.95, base loss: 14381.11
[INFO 2017-06-28 15:40:35,145 main.py:51] epoch 13073, training loss: 8924.20, average training loss: 9186.35, base loss: 14379.85
[INFO 2017-06-28 15:40:35,808 main.py:51] epoch 13074, training loss: 8600.12, average training loss: 9186.70, base loss: 14379.01
[INFO 2017-06-28 15:40:36,486 main.py:51] epoch 13075, training loss: 8664.06, average training loss: 9185.20, base loss: 14378.96
[INFO 2017-06-28 15:40:37,147 main.py:51] epoch 13076, training loss: 9402.06, average training loss: 9185.37, base loss: 14379.45
[INFO 2017-06-28 15:40:37,803 main.py:51] epoch 13077, training loss: 8385.50, average training loss: 9185.49, base loss: 14380.37
[INFO 2017-06-28 15:40:38,442 main.py:51] epoch 13078, training loss: 9051.40, average training loss: 9184.92, base loss: 14379.92
[INFO 2017-06-28 15:40:39,115 main.py:51] epoch 13079, training loss: 10275.47, average training loss: 9184.58, base loss: 14378.47
[INFO 2017-06-28 15:40:39,788 main.py:51] epoch 13080, training loss: 9089.80, average training loss: 9184.08, base loss: 14377.53
[INFO 2017-06-28 15:40:40,456 main.py:51] epoch 13081, training loss: 9798.18, average training loss: 9185.33, base loss: 14379.46
[INFO 2017-06-28 15:40:41,122 main.py:51] epoch 13082, training loss: 9374.34, average training loss: 9185.62, base loss: 14379.54
[INFO 2017-06-28 15:40:41,794 main.py:51] epoch 13083, training loss: 9029.74, average training loss: 9183.90, base loss: 14377.05
[INFO 2017-06-28 15:40:42,466 main.py:51] epoch 13084, training loss: 9045.09, average training loss: 9183.35, base loss: 14375.04
[INFO 2017-06-28 15:40:43,151 main.py:51] epoch 13085, training loss: 9413.54, average training loss: 9183.70, base loss: 14374.20
[INFO 2017-06-28 15:40:43,805 main.py:51] epoch 13086, training loss: 8772.37, average training loss: 9182.92, base loss: 14373.53
[INFO 2017-06-28 15:40:44,459 main.py:51] epoch 13087, training loss: 9140.05, average training loss: 9182.18, base loss: 14374.47
[INFO 2017-06-28 15:40:45,129 main.py:51] epoch 13088, training loss: 9010.38, average training loss: 9181.41, base loss: 14374.12
[INFO 2017-06-28 15:40:45,769 main.py:51] epoch 13089, training loss: 9883.73, average training loss: 9181.92, base loss: 14375.46
[INFO 2017-06-28 15:40:46,448 main.py:51] epoch 13090, training loss: 8513.64, average training loss: 9180.02, base loss: 14373.49
[INFO 2017-06-28 15:40:47,129 main.py:51] epoch 13091, training loss: 9073.22, average training loss: 9178.89, base loss: 14371.23
[INFO 2017-06-28 15:40:47,805 main.py:51] epoch 13092, training loss: 9274.50, average training loss: 9178.81, base loss: 14371.38
[INFO 2017-06-28 15:40:48,475 main.py:51] epoch 13093, training loss: 9369.35, average training loss: 9179.69, base loss: 14373.83
[INFO 2017-06-28 15:40:49,148 main.py:51] epoch 13094, training loss: 9410.23, average training loss: 9180.23, base loss: 14374.12
[INFO 2017-06-28 15:40:49,801 main.py:51] epoch 13095, training loss: 9037.67, average training loss: 9177.60, base loss: 14369.02
[INFO 2017-06-28 15:40:50,479 main.py:51] epoch 13096, training loss: 9770.07, average training loss: 9177.45, base loss: 14368.54
[INFO 2017-06-28 15:40:51,149 main.py:51] epoch 13097, training loss: 9271.58, average training loss: 9178.02, base loss: 14369.11
[INFO 2017-06-28 15:40:51,831 main.py:51] epoch 13098, training loss: 10188.93, average training loss: 9178.40, base loss: 14369.10
[INFO 2017-06-28 15:40:52,489 main.py:51] epoch 13099, training loss: 8659.28, average training loss: 9178.10, base loss: 14370.38
[INFO 2017-06-28 15:40:52,490 main.py:53] epoch 13099, testing
[INFO 2017-06-28 15:40:55,088 main.py:105] average testing loss: 9916.45, base loss: 14150.05
[INFO 2017-06-28 15:40:55,088 main.py:106] improve_loss: 4233.60, improve_percent: 0.30
[INFO 2017-06-28 15:40:55,088 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:40:55,767 main.py:51] epoch 13100, training loss: 9057.80, average training loss: 9177.61, base loss: 14371.28
[INFO 2017-06-28 15:40:56,422 main.py:51] epoch 13101, training loss: 8812.86, average training loss: 9178.14, base loss: 14373.77
[INFO 2017-06-28 15:40:57,086 main.py:51] epoch 13102, training loss: 9179.66, average training loss: 9178.57, base loss: 14375.47
[INFO 2017-06-28 15:40:57,763 main.py:51] epoch 13103, training loss: 9873.21, average training loss: 9179.92, base loss: 14377.94
[INFO 2017-06-28 15:40:58,435 main.py:51] epoch 13104, training loss: 9798.99, average training loss: 9180.41, base loss: 14378.17
[INFO 2017-06-28 15:40:59,117 main.py:51] epoch 13105, training loss: 11229.17, average training loss: 9182.79, base loss: 14383.76
[INFO 2017-06-28 15:40:59,787 main.py:51] epoch 13106, training loss: 10584.13, average training loss: 9185.12, base loss: 14388.88
[INFO 2017-06-28 15:41:00,451 main.py:51] epoch 13107, training loss: 9857.33, average training loss: 9186.63, base loss: 14390.44
[INFO 2017-06-28 15:41:01,114 main.py:51] epoch 13108, training loss: 9239.39, average training loss: 9187.31, base loss: 14393.22
[INFO 2017-06-28 15:41:01,772 main.py:51] epoch 13109, training loss: 8503.04, average training loss: 9186.08, base loss: 14391.51
[INFO 2017-06-28 15:41:02,424 main.py:51] epoch 13110, training loss: 9699.50, average training loss: 9184.90, base loss: 14389.83
[INFO 2017-06-28 15:41:03,090 main.py:51] epoch 13111, training loss: 8118.37, average training loss: 9183.65, base loss: 14388.13
[INFO 2017-06-28 15:41:03,770 main.py:51] epoch 13112, training loss: 9657.73, average training loss: 9183.09, base loss: 14388.57
[INFO 2017-06-28 15:41:04,425 main.py:51] epoch 13113, training loss: 8802.76, average training loss: 9182.98, base loss: 14387.61
[INFO 2017-06-28 15:41:05,096 main.py:51] epoch 13114, training loss: 8305.46, average training loss: 9182.18, base loss: 14384.40
[INFO 2017-06-28 15:41:05,785 main.py:51] epoch 13115, training loss: 9829.28, average training loss: 9182.96, base loss: 14386.49
[INFO 2017-06-28 15:41:06,461 main.py:51] epoch 13116, training loss: 9400.25, average training loss: 9184.12, base loss: 14388.20
[INFO 2017-06-28 15:41:07,114 main.py:51] epoch 13117, training loss: 8769.37, average training loss: 9182.45, base loss: 14384.37
[INFO 2017-06-28 15:41:07,772 main.py:51] epoch 13118, training loss: 9404.70, average training loss: 9183.07, base loss: 14386.41
[INFO 2017-06-28 15:41:08,432 main.py:51] epoch 13119, training loss: 10686.67, average training loss: 9185.96, base loss: 14392.79
[INFO 2017-06-28 15:41:09,096 main.py:51] epoch 13120, training loss: 9029.35, average training loss: 9185.40, base loss: 14391.10
[INFO 2017-06-28 15:41:09,750 main.py:51] epoch 13121, training loss: 10237.54, average training loss: 9185.78, base loss: 14390.53
[INFO 2017-06-28 15:41:10,406 main.py:51] epoch 13122, training loss: 8192.71, average training loss: 9183.47, base loss: 14386.29
[INFO 2017-06-28 15:41:11,069 main.py:51] epoch 13123, training loss: 8828.78, average training loss: 9184.65, base loss: 14388.20
[INFO 2017-06-28 15:41:11,721 main.py:51] epoch 13124, training loss: 9022.09, average training loss: 9185.36, base loss: 14390.52
[INFO 2017-06-28 15:41:12,386 main.py:51] epoch 13125, training loss: 9183.99, average training loss: 9184.80, base loss: 14388.24
[INFO 2017-06-28 15:41:13,074 main.py:51] epoch 13126, training loss: 8774.85, average training loss: 9183.99, base loss: 14386.42
[INFO 2017-06-28 15:41:13,762 main.py:51] epoch 13127, training loss: 8381.01, average training loss: 9183.98, base loss: 14385.66
[INFO 2017-06-28 15:41:14,427 main.py:51] epoch 13128, training loss: 9354.59, average training loss: 9184.87, base loss: 14386.87
[INFO 2017-06-28 15:41:15,065 main.py:51] epoch 13129, training loss: 8532.32, average training loss: 9183.81, base loss: 14386.32
[INFO 2017-06-28 15:41:15,719 main.py:51] epoch 13130, training loss: 8565.95, average training loss: 9183.68, base loss: 14385.49
[INFO 2017-06-28 15:41:16,365 main.py:51] epoch 13131, training loss: 8389.81, average training loss: 9182.29, base loss: 14382.76
[INFO 2017-06-28 15:41:17,007 main.py:51] epoch 13132, training loss: 10489.64, average training loss: 9183.47, base loss: 14385.09
[INFO 2017-06-28 15:41:17,652 main.py:51] epoch 13133, training loss: 8962.27, average training loss: 9181.58, base loss: 14381.33
[INFO 2017-06-28 15:41:18,326 main.py:51] epoch 13134, training loss: 8992.83, average training loss: 9181.80, base loss: 14381.51
[INFO 2017-06-28 15:41:18,992 main.py:51] epoch 13135, training loss: 8240.29, average training loss: 9182.06, base loss: 14380.66
[INFO 2017-06-28 15:41:19,658 main.py:51] epoch 13136, training loss: 9339.71, average training loss: 9182.40, base loss: 14382.49
[INFO 2017-06-28 15:41:20,315 main.py:51] epoch 13137, training loss: 7567.08, average training loss: 9181.01, base loss: 14379.20
[INFO 2017-06-28 15:41:20,957 main.py:51] epoch 13138, training loss: 8609.40, average training loss: 9180.42, base loss: 14377.88
[INFO 2017-06-28 15:41:21,647 main.py:51] epoch 13139, training loss: 7854.45, average training loss: 9179.23, base loss: 14376.87
[INFO 2017-06-28 15:41:22,306 main.py:51] epoch 13140, training loss: 8612.17, average training loss: 9178.59, base loss: 14375.29
[INFO 2017-06-28 15:41:22,956 main.py:51] epoch 13141, training loss: 9683.47, average training loss: 9179.51, base loss: 14377.15
[INFO 2017-06-28 15:41:23,629 main.py:51] epoch 13142, training loss: 8442.13, average training loss: 9178.76, base loss: 14377.17
[INFO 2017-06-28 15:41:24,287 main.py:51] epoch 13143, training loss: 8834.13, average training loss: 9179.78, base loss: 14378.87
[INFO 2017-06-28 15:41:24,950 main.py:51] epoch 13144, training loss: 8802.38, average training loss: 9178.33, base loss: 14376.71
[INFO 2017-06-28 15:41:25,607 main.py:51] epoch 13145, training loss: 8169.39, average training loss: 9176.88, base loss: 14373.44
[INFO 2017-06-28 15:41:26,286 main.py:51] epoch 13146, training loss: 8679.21, average training loss: 9177.00, base loss: 14373.81
[INFO 2017-06-28 15:41:26,948 main.py:51] epoch 13147, training loss: 8940.71, average training loss: 9176.64, base loss: 14372.33
[INFO 2017-06-28 15:41:27,611 main.py:51] epoch 13148, training loss: 8113.69, average training loss: 9175.15, base loss: 14369.94
[INFO 2017-06-28 15:41:28,273 main.py:51] epoch 13149, training loss: 9271.06, average training loss: 9175.80, base loss: 14369.81
[INFO 2017-06-28 15:41:28,953 main.py:51] epoch 13150, training loss: 9057.68, average training loss: 9175.15, base loss: 14368.56
[INFO 2017-06-28 15:41:29,621 main.py:51] epoch 13151, training loss: 9635.37, average training loss: 9175.34, base loss: 14369.35
[INFO 2017-06-28 15:41:30,263 main.py:51] epoch 13152, training loss: 8765.91, average training loss: 9174.96, base loss: 14368.74
[INFO 2017-06-28 15:41:30,935 main.py:51] epoch 13153, training loss: 8884.33, average training loss: 9174.04, base loss: 14366.36
[INFO 2017-06-28 15:41:31,580 main.py:51] epoch 13154, training loss: 7832.87, average training loss: 9172.11, base loss: 14364.94
[INFO 2017-06-28 15:41:32,260 main.py:51] epoch 13155, training loss: 9538.38, average training loss: 9172.66, base loss: 14366.33
[INFO 2017-06-28 15:41:32,934 main.py:51] epoch 13156, training loss: 8963.42, average training loss: 9173.12, base loss: 14368.69
[INFO 2017-06-28 15:41:33,588 main.py:51] epoch 13157, training loss: 9003.36, average training loss: 9173.42, base loss: 14369.86
[INFO 2017-06-28 15:41:34,242 main.py:51] epoch 13158, training loss: 9382.58, average training loss: 9174.11, base loss: 14369.43
[INFO 2017-06-28 15:41:34,894 main.py:51] epoch 13159, training loss: 8990.68, average training loss: 9174.04, base loss: 14369.63
[INFO 2017-06-28 15:41:35,538 main.py:51] epoch 13160, training loss: 9812.29, average training loss: 9174.76, base loss: 14371.95
[INFO 2017-06-28 15:41:36,205 main.py:51] epoch 13161, training loss: 8468.10, average training loss: 9172.81, base loss: 14368.99
[INFO 2017-06-28 15:41:36,880 main.py:51] epoch 13162, training loss: 9102.15, average training loss: 9171.97, base loss: 14365.06
[INFO 2017-06-28 15:41:37,535 main.py:51] epoch 13163, training loss: 9548.07, average training loss: 9173.04, base loss: 14366.14
[INFO 2017-06-28 15:41:38,188 main.py:51] epoch 13164, training loss: 10170.46, average training loss: 9174.15, base loss: 14367.56
[INFO 2017-06-28 15:41:38,857 main.py:51] epoch 13165, training loss: 9068.84, average training loss: 9173.38, base loss: 14364.16
[INFO 2017-06-28 15:41:39,495 main.py:51] epoch 13166, training loss: 10500.84, average training loss: 9174.98, base loss: 14364.23
[INFO 2017-06-28 15:41:40,160 main.py:51] epoch 13167, training loss: 9903.76, average training loss: 9175.59, base loss: 14366.42
[INFO 2017-06-28 15:41:40,820 main.py:51] epoch 13168, training loss: 9303.09, average training loss: 9175.58, base loss: 14364.59
[INFO 2017-06-28 15:41:41,488 main.py:51] epoch 13169, training loss: 9950.94, average training loss: 9176.50, base loss: 14366.73
[INFO 2017-06-28 15:41:42,165 main.py:51] epoch 13170, training loss: 10703.91, average training loss: 9176.69, base loss: 14366.70
[INFO 2017-06-28 15:41:42,822 main.py:51] epoch 13171, training loss: 9333.25, average training loss: 9176.37, base loss: 14367.01
[INFO 2017-06-28 15:41:43,470 main.py:51] epoch 13172, training loss: 9646.47, average training loss: 9176.11, base loss: 14365.18
[INFO 2017-06-28 15:41:44,147 main.py:51] epoch 13173, training loss: 9298.14, average training loss: 9176.01, base loss: 14365.22
[INFO 2017-06-28 15:41:44,827 main.py:51] epoch 13174, training loss: 8254.36, average training loss: 9174.45, base loss: 14363.41
[INFO 2017-06-28 15:41:45,490 main.py:51] epoch 13175, training loss: 9025.93, average training loss: 9174.94, base loss: 14363.92
[INFO 2017-06-28 15:41:46,158 main.py:51] epoch 13176, training loss: 8150.87, average training loss: 9174.07, base loss: 14363.28
[INFO 2017-06-28 15:41:46,821 main.py:51] epoch 13177, training loss: 8917.89, average training loss: 9174.17, base loss: 14364.04
[INFO 2017-06-28 15:41:47,481 main.py:51] epoch 13178, training loss: 7740.04, average training loss: 9172.65, base loss: 14361.50
[INFO 2017-06-28 15:41:48,154 main.py:51] epoch 13179, training loss: 9430.20, average training loss: 9173.40, base loss: 14363.18
[INFO 2017-06-28 15:41:48,848 main.py:51] epoch 13180, training loss: 8184.38, average training loss: 9173.80, base loss: 14363.99
[INFO 2017-06-28 15:41:49,495 main.py:51] epoch 13181, training loss: 9769.37, average training loss: 9175.11, base loss: 14367.01
[INFO 2017-06-28 15:41:50,161 main.py:51] epoch 13182, training loss: 9682.91, average training loss: 9176.52, base loss: 14367.73
[INFO 2017-06-28 15:41:50,828 main.py:51] epoch 13183, training loss: 10259.45, average training loss: 9178.14, base loss: 14372.31
[INFO 2017-06-28 15:41:51,474 main.py:51] epoch 13184, training loss: 7540.46, average training loss: 9176.18, base loss: 14368.46
[INFO 2017-06-28 15:41:52,175 main.py:51] epoch 13185, training loss: 8627.63, average training loss: 9176.38, base loss: 14369.09
[INFO 2017-06-28 15:41:52,860 main.py:51] epoch 13186, training loss: 9441.17, average training loss: 9177.34, base loss: 14370.57
[INFO 2017-06-28 15:41:53,548 main.py:51] epoch 13187, training loss: 9200.65, average training loss: 9176.61, base loss: 14370.05
[INFO 2017-06-28 15:41:54,221 main.py:51] epoch 13188, training loss: 9748.61, average training loss: 9177.11, base loss: 14370.34
[INFO 2017-06-28 15:41:54,895 main.py:51] epoch 13189, training loss: 10980.34, average training loss: 9179.59, base loss: 14374.92
[INFO 2017-06-28 15:41:55,588 main.py:51] epoch 13190, training loss: 7768.17, average training loss: 9177.33, base loss: 14371.15
[INFO 2017-06-28 15:41:56,236 main.py:51] epoch 13191, training loss: 9312.11, average training loss: 9177.86, base loss: 14371.36
[INFO 2017-06-28 15:41:56,915 main.py:51] epoch 13192, training loss: 9132.16, average training loss: 9178.11, base loss: 14371.55
[INFO 2017-06-28 15:41:57,590 main.py:51] epoch 13193, training loss: 8522.83, average training loss: 9178.51, base loss: 14371.51
[INFO 2017-06-28 15:41:58,277 main.py:51] epoch 13194, training loss: 9923.40, average training loss: 9178.22, base loss: 14369.63
[INFO 2017-06-28 15:41:58,967 main.py:51] epoch 13195, training loss: 10398.88, average training loss: 9179.85, base loss: 14373.75
[INFO 2017-06-28 15:41:59,604 main.py:51] epoch 13196, training loss: 8238.98, average training loss: 9178.71, base loss: 14371.20
[INFO 2017-06-28 15:42:00,256 main.py:51] epoch 13197, training loss: 9032.07, average training loss: 9178.78, base loss: 14371.40
[INFO 2017-06-28 15:42:00,917 main.py:51] epoch 13198, training loss: 10263.70, average training loss: 9179.96, base loss: 14373.59
[INFO 2017-06-28 15:42:01,567 main.py:51] epoch 13199, training loss: 10179.83, average training loss: 9181.54, base loss: 14376.67
[INFO 2017-06-28 15:42:01,567 main.py:53] epoch 13199, testing
[INFO 2017-06-28 15:42:04,234 main.py:105] average testing loss: 10901.65, base loss: 15095.38
[INFO 2017-06-28 15:42:04,234 main.py:106] improve_loss: 4193.73, improve_percent: 0.28
[INFO 2017-06-28 15:42:04,235 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:42:04,879 main.py:51] epoch 13200, training loss: 8240.98, average training loss: 9181.21, base loss: 14376.28
[INFO 2017-06-28 15:42:05,528 main.py:51] epoch 13201, training loss: 9723.12, average training loss: 9182.27, base loss: 14378.34
[INFO 2017-06-28 15:42:06,188 main.py:51] epoch 13202, training loss: 9182.11, average training loss: 9182.76, base loss: 14380.45
[INFO 2017-06-28 15:42:06,850 main.py:51] epoch 13203, training loss: 9319.16, average training loss: 9183.37, base loss: 14381.78
[INFO 2017-06-28 15:42:07,508 main.py:51] epoch 13204, training loss: 8707.81, average training loss: 9183.68, base loss: 14381.76
[INFO 2017-06-28 15:42:08,181 main.py:51] epoch 13205, training loss: 10582.07, average training loss: 9186.18, base loss: 14384.50
[INFO 2017-06-28 15:42:08,849 main.py:51] epoch 13206, training loss: 9044.06, average training loss: 9186.62, base loss: 14384.69
[INFO 2017-06-28 15:42:09,534 main.py:51] epoch 13207, training loss: 10659.29, average training loss: 9187.89, base loss: 14386.45
[INFO 2017-06-28 15:42:10,178 main.py:51] epoch 13208, training loss: 8961.20, average training loss: 9186.11, base loss: 14385.16
[INFO 2017-06-28 15:42:10,857 main.py:51] epoch 13209, training loss: 8626.53, average training loss: 9185.71, base loss: 14384.24
[INFO 2017-06-28 15:42:11,513 main.py:51] epoch 13210, training loss: 8352.69, average training loss: 9183.63, base loss: 14381.00
[INFO 2017-06-28 15:42:12,164 main.py:51] epoch 13211, training loss: 9834.01, average training loss: 9184.99, base loss: 14382.75
[INFO 2017-06-28 15:42:12,811 main.py:51] epoch 13212, training loss: 9060.25, average training loss: 9186.22, base loss: 14384.27
[INFO 2017-06-28 15:42:13,483 main.py:51] epoch 13213, training loss: 8654.91, average training loss: 9185.78, base loss: 14383.72
[INFO 2017-06-28 15:42:14,145 main.py:51] epoch 13214, training loss: 9696.35, average training loss: 9186.91, base loss: 14385.55
[INFO 2017-06-28 15:42:14,794 main.py:51] epoch 13215, training loss: 9086.32, average training loss: 9186.15, base loss: 14384.50
[INFO 2017-06-28 15:42:15,473 main.py:51] epoch 13216, training loss: 9387.92, average training loss: 9186.77, base loss: 14385.31
[INFO 2017-06-28 15:42:16,132 main.py:51] epoch 13217, training loss: 11004.53, average training loss: 9187.87, base loss: 14387.90
[INFO 2017-06-28 15:42:16,799 main.py:51] epoch 13218, training loss: 8844.32, average training loss: 9185.54, base loss: 14383.99
[INFO 2017-06-28 15:42:17,461 main.py:51] epoch 13219, training loss: 8499.42, average training loss: 9186.05, base loss: 14385.40
[INFO 2017-06-28 15:42:18,126 main.py:51] epoch 13220, training loss: 10601.46, average training loss: 9186.59, base loss: 14387.99
[INFO 2017-06-28 15:42:18,781 main.py:51] epoch 13221, training loss: 8515.82, average training loss: 9187.47, base loss: 14389.91
[INFO 2017-06-28 15:42:19,437 main.py:51] epoch 13222, training loss: 11712.42, average training loss: 9188.26, base loss: 14392.70
[INFO 2017-06-28 15:42:20,103 main.py:51] epoch 13223, training loss: 8876.52, average training loss: 9187.72, base loss: 14392.80
[INFO 2017-06-28 15:42:20,755 main.py:51] epoch 13224, training loss: 8446.31, average training loss: 9186.42, base loss: 14392.62
[INFO 2017-06-28 15:42:21,436 main.py:51] epoch 13225, training loss: 9638.24, average training loss: 9186.65, base loss: 14393.29
[INFO 2017-06-28 15:42:22,102 main.py:51] epoch 13226, training loss: 10882.01, average training loss: 9189.08, base loss: 14395.75
[INFO 2017-06-28 15:42:22,759 main.py:51] epoch 13227, training loss: 9771.08, average training loss: 9189.93, base loss: 14396.44
[INFO 2017-06-28 15:42:23,415 main.py:51] epoch 13228, training loss: 8912.43, average training loss: 9189.06, base loss: 14394.54
[INFO 2017-06-28 15:42:24,096 main.py:51] epoch 13229, training loss: 8584.21, average training loss: 9187.64, base loss: 14390.88
[INFO 2017-06-28 15:42:24,755 main.py:51] epoch 13230, training loss: 8775.18, average training loss: 9187.31, base loss: 14390.82
[INFO 2017-06-28 15:42:25,429 main.py:51] epoch 13231, training loss: 10602.22, average training loss: 9188.92, base loss: 14394.46
[INFO 2017-06-28 15:42:26,078 main.py:51] epoch 13232, training loss: 10224.14, average training loss: 9189.44, base loss: 14396.78
[INFO 2017-06-28 15:42:26,727 main.py:51] epoch 13233, training loss: 9468.05, average training loss: 9189.00, base loss: 14395.17
[INFO 2017-06-28 15:42:27,373 main.py:51] epoch 13234, training loss: 9753.81, average training loss: 9188.56, base loss: 14393.64
[INFO 2017-06-28 15:42:28,021 main.py:51] epoch 13235, training loss: 8860.27, average training loss: 9189.41, base loss: 14395.84
[INFO 2017-06-28 15:42:28,681 main.py:51] epoch 13236, training loss: 8779.75, average training loss: 9188.99, base loss: 14395.24
[INFO 2017-06-28 15:42:29,346 main.py:51] epoch 13237, training loss: 9260.06, average training loss: 9190.04, base loss: 14397.93
[INFO 2017-06-28 15:42:30,017 main.py:51] epoch 13238, training loss: 10997.24, average training loss: 9191.27, base loss: 14400.04
[INFO 2017-06-28 15:42:30,659 main.py:51] epoch 13239, training loss: 9326.58, average training loss: 9191.66, base loss: 14401.45
[INFO 2017-06-28 15:42:31,338 main.py:51] epoch 13240, training loss: 10357.21, average training loss: 9192.11, base loss: 14402.77
[INFO 2017-06-28 15:42:31,992 main.py:51] epoch 13241, training loss: 9542.81, average training loss: 9193.44, base loss: 14404.51
[INFO 2017-06-28 15:42:32,637 main.py:51] epoch 13242, training loss: 8741.68, average training loss: 9194.00, base loss: 14405.89
[INFO 2017-06-28 15:42:33,296 main.py:51] epoch 13243, training loss: 8187.19, average training loss: 9191.16, base loss: 14401.13
[INFO 2017-06-28 15:42:33,951 main.py:51] epoch 13244, training loss: 11001.51, average training loss: 9193.72, base loss: 14404.40
[INFO 2017-06-28 15:42:34,641 main.py:51] epoch 13245, training loss: 8966.62, average training loss: 9193.91, base loss: 14404.30
[INFO 2017-06-28 15:42:35,289 main.py:51] epoch 13246, training loss: 9394.25, average training loss: 9193.53, base loss: 14405.67
[INFO 2017-06-28 15:42:35,929 main.py:51] epoch 13247, training loss: 7767.19, average training loss: 9192.10, base loss: 14403.30
[INFO 2017-06-28 15:42:36,593 main.py:51] epoch 13248, training loss: 9871.21, average training loss: 9191.30, base loss: 14401.94
[INFO 2017-06-28 15:42:37,224 main.py:51] epoch 13249, training loss: 8727.11, average training loss: 9190.91, base loss: 14402.15
[INFO 2017-06-28 15:42:37,885 main.py:51] epoch 13250, training loss: 8967.44, average training loss: 9191.45, base loss: 14402.77
[INFO 2017-06-28 15:42:38,548 main.py:51] epoch 13251, training loss: 10510.35, average training loss: 9193.11, base loss: 14406.08
[INFO 2017-06-28 15:42:39,202 main.py:51] epoch 13252, training loss: 8966.85, average training loss: 9192.87, base loss: 14405.01
[INFO 2017-06-28 15:42:39,855 main.py:51] epoch 13253, training loss: 8627.72, average training loss: 9190.69, base loss: 14400.65
[INFO 2017-06-28 15:42:40,521 main.py:51] epoch 13254, training loss: 9124.63, average training loss: 9192.13, base loss: 14403.50
[INFO 2017-06-28 15:42:41,150 main.py:51] epoch 13255, training loss: 8701.69, average training loss: 9191.41, base loss: 14400.99
[INFO 2017-06-28 15:42:41,794 main.py:51] epoch 13256, training loss: 8541.45, average training loss: 9189.44, base loss: 14397.27
[INFO 2017-06-28 15:42:42,456 main.py:51] epoch 13257, training loss: 8454.84, average training loss: 9188.62, base loss: 14396.87
[INFO 2017-06-28 15:42:43,115 main.py:51] epoch 13258, training loss: 8465.92, average training loss: 9185.82, base loss: 14393.02
[INFO 2017-06-28 15:42:43,748 main.py:51] epoch 13259, training loss: 12243.32, average training loss: 9188.71, base loss: 14399.14
[INFO 2017-06-28 15:42:44,421 main.py:51] epoch 13260, training loss: 8871.73, average training loss: 9189.35, base loss: 14401.48
[INFO 2017-06-28 15:42:45,081 main.py:51] epoch 13261, training loss: 9863.78, average training loss: 9190.28, base loss: 14403.24
[INFO 2017-06-28 15:42:45,734 main.py:51] epoch 13262, training loss: 9828.27, average training loss: 9191.47, base loss: 14406.11
[INFO 2017-06-28 15:42:46,400 main.py:51] epoch 13263, training loss: 8454.88, average training loss: 9191.26, base loss: 14406.38
[INFO 2017-06-28 15:42:47,065 main.py:51] epoch 13264, training loss: 8265.26, average training loss: 9189.21, base loss: 14403.79
[INFO 2017-06-28 15:42:47,729 main.py:51] epoch 13265, training loss: 9144.57, average training loss: 9189.34, base loss: 14404.37
[INFO 2017-06-28 15:42:48,387 main.py:51] epoch 13266, training loss: 9751.94, average training loss: 9190.75, base loss: 14405.69
[INFO 2017-06-28 15:42:49,022 main.py:51] epoch 13267, training loss: 11328.57, average training loss: 9193.18, base loss: 14408.86
[INFO 2017-06-28 15:42:49,651 main.py:51] epoch 13268, training loss: 11233.50, average training loss: 9194.66, base loss: 14411.13
[INFO 2017-06-28 15:42:50,323 main.py:51] epoch 13269, training loss: 8565.94, average training loss: 9194.02, base loss: 14409.62
[INFO 2017-06-28 15:42:50,982 main.py:51] epoch 13270, training loss: 11001.04, average training loss: 9196.53, base loss: 14414.92
[INFO 2017-06-28 15:42:51,671 main.py:51] epoch 13271, training loss: 9018.94, average training loss: 9195.84, base loss: 14411.44
[INFO 2017-06-28 15:42:52,333 main.py:51] epoch 13272, training loss: 9104.07, average training loss: 9196.47, base loss: 14412.05
[INFO 2017-06-28 15:42:52,983 main.py:51] epoch 13273, training loss: 9634.22, average training loss: 9196.22, base loss: 14410.40
[INFO 2017-06-28 15:42:53,656 main.py:51] epoch 13274, training loss: 9569.05, average training loss: 9195.85, base loss: 14407.90
[INFO 2017-06-28 15:42:54,336 main.py:51] epoch 13275, training loss: 10001.68, average training loss: 9198.14, base loss: 14412.63
[INFO 2017-06-28 15:42:54,994 main.py:51] epoch 13276, training loss: 8702.24, average training loss: 9197.66, base loss: 14412.35
[INFO 2017-06-28 15:42:55,629 main.py:51] epoch 13277, training loss: 8512.89, average training loss: 9197.85, base loss: 14413.74
[INFO 2017-06-28 15:42:56,273 main.py:51] epoch 13278, training loss: 8660.11, average training loss: 9196.32, base loss: 14413.00
[INFO 2017-06-28 15:42:56,929 main.py:51] epoch 13279, training loss: 9475.38, average training loss: 9197.43, base loss: 14411.47
[INFO 2017-06-28 15:42:57,587 main.py:51] epoch 13280, training loss: 9819.97, average training loss: 9198.98, base loss: 14414.34
[INFO 2017-06-28 15:42:58,233 main.py:51] epoch 13281, training loss: 8610.36, average training loss: 9197.92, base loss: 14412.32
[INFO 2017-06-28 15:42:58,902 main.py:51] epoch 13282, training loss: 8353.51, average training loss: 9197.32, base loss: 14411.56
[INFO 2017-06-28 15:42:59,549 main.py:51] epoch 13283, training loss: 9273.44, average training loss: 9198.06, base loss: 14411.94
[INFO 2017-06-28 15:43:00,215 main.py:51] epoch 13284, training loss: 8077.22, average training loss: 9196.74, base loss: 14408.85
[INFO 2017-06-28 15:43:00,874 main.py:51] epoch 13285, training loss: 8815.24, average training loss: 9196.54, base loss: 14408.79
[INFO 2017-06-28 15:43:01,562 main.py:51] epoch 13286, training loss: 9116.94, average training loss: 9195.95, base loss: 14408.57
[INFO 2017-06-28 15:43:02,202 main.py:51] epoch 13287, training loss: 10014.00, average training loss: 9196.37, base loss: 14409.45
[INFO 2017-06-28 15:43:02,852 main.py:51] epoch 13288, training loss: 9262.99, average training loss: 9196.97, base loss: 14409.88
[INFO 2017-06-28 15:43:03,516 main.py:51] epoch 13289, training loss: 8907.13, average training loss: 9197.38, base loss: 14411.12
[INFO 2017-06-28 15:43:04,186 main.py:51] epoch 13290, training loss: 8116.30, average training loss: 9195.96, base loss: 14407.93
[INFO 2017-06-28 15:43:04,866 main.py:51] epoch 13291, training loss: 8505.85, average training loss: 9195.90, base loss: 14409.53
[INFO 2017-06-28 15:43:05,528 main.py:51] epoch 13292, training loss: 10778.51, average training loss: 9198.16, base loss: 14414.40
[INFO 2017-06-28 15:43:06,199 main.py:51] epoch 13293, training loss: 9481.57, average training loss: 9198.87, base loss: 14415.99
[INFO 2017-06-28 15:43:06,866 main.py:51] epoch 13294, training loss: 9180.78, average training loss: 9197.79, base loss: 14415.89
[INFO 2017-06-28 15:43:07,532 main.py:51] epoch 13295, training loss: 9763.10, average training loss: 9199.15, base loss: 14416.87
[INFO 2017-06-28 15:43:08,191 main.py:51] epoch 13296, training loss: 7798.46, average training loss: 9198.27, base loss: 14414.48
[INFO 2017-06-28 15:43:08,856 main.py:51] epoch 13297, training loss: 9463.79, average training loss: 9197.52, base loss: 14411.11
[INFO 2017-06-28 15:43:09,541 main.py:51] epoch 13298, training loss: 8806.92, average training loss: 9196.98, base loss: 14409.98
[INFO 2017-06-28 15:43:10,201 main.py:51] epoch 13299, training loss: 8065.20, average training loss: 9194.35, base loss: 14405.79
[INFO 2017-06-28 15:43:10,202 main.py:53] epoch 13299, testing
[INFO 2017-06-28 15:43:12,815 main.py:105] average testing loss: 10059.01, base loss: 14541.53
[INFO 2017-06-28 15:43:12,815 main.py:106] improve_loss: 4482.53, improve_percent: 0.31
[INFO 2017-06-28 15:43:12,816 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:43:13,477 main.py:51] epoch 13300, training loss: 9446.93, average training loss: 9195.03, base loss: 14407.25
[INFO 2017-06-28 15:43:14,119 main.py:51] epoch 13301, training loss: 8674.69, average training loss: 9194.05, base loss: 14407.47
[INFO 2017-06-28 15:43:14,784 main.py:51] epoch 13302, training loss: 9516.80, average training loss: 9195.18, base loss: 14409.42
[INFO 2017-06-28 15:43:15,448 main.py:51] epoch 13303, training loss: 9602.58, average training loss: 9195.58, base loss: 14409.99
[INFO 2017-06-28 15:43:16,101 main.py:51] epoch 13304, training loss: 8314.33, average training loss: 9195.01, base loss: 14409.09
[INFO 2017-06-28 15:43:16,739 main.py:51] epoch 13305, training loss: 8296.75, average training loss: 9192.80, base loss: 14404.89
[INFO 2017-06-28 15:43:17,404 main.py:51] epoch 13306, training loss: 8305.70, average training loss: 9191.16, base loss: 14403.13
[INFO 2017-06-28 15:43:18,060 main.py:51] epoch 13307, training loss: 9369.53, average training loss: 9190.30, base loss: 14403.99
[INFO 2017-06-28 15:43:18,730 main.py:51] epoch 13308, training loss: 8498.74, average training loss: 9188.81, base loss: 14403.94
[INFO 2017-06-28 15:43:19,387 main.py:51] epoch 13309, training loss: 9553.01, average training loss: 9187.28, base loss: 14403.92
[INFO 2017-06-28 15:43:20,039 main.py:51] epoch 13310, training loss: 9269.53, average training loss: 9187.44, base loss: 14404.76
[INFO 2017-06-28 15:43:20,727 main.py:51] epoch 13311, training loss: 9275.68, average training loss: 9186.00, base loss: 14403.68
[INFO 2017-06-28 15:43:21,370 main.py:51] epoch 13312, training loss: 8808.08, average training loss: 9185.59, base loss: 14403.87
[INFO 2017-06-28 15:43:22,065 main.py:51] epoch 13313, training loss: 9413.24, average training loss: 9185.51, base loss: 14403.58
[INFO 2017-06-28 15:43:22,733 main.py:51] epoch 13314, training loss: 8774.78, average training loss: 9185.98, base loss: 14404.34
[INFO 2017-06-28 15:43:23,379 main.py:51] epoch 13315, training loss: 9680.35, average training loss: 9185.59, base loss: 14402.67
[INFO 2017-06-28 15:43:24,013 main.py:51] epoch 13316, training loss: 9284.50, average training loss: 9186.09, base loss: 14402.74
[INFO 2017-06-28 15:43:24,658 main.py:51] epoch 13317, training loss: 9221.45, average training loss: 9184.32, base loss: 14398.72
[INFO 2017-06-28 15:43:25,321 main.py:51] epoch 13318, training loss: 8125.59, average training loss: 9183.77, base loss: 14399.98
[INFO 2017-06-28 15:43:25,963 main.py:51] epoch 13319, training loss: 9919.85, average training loss: 9183.86, base loss: 14400.51
[INFO 2017-06-28 15:43:26,627 main.py:51] epoch 13320, training loss: 9297.20, average training loss: 9184.57, base loss: 14401.77
[INFO 2017-06-28 15:43:27,305 main.py:51] epoch 13321, training loss: 10362.93, average training loss: 9186.82, base loss: 14405.12
[INFO 2017-06-28 15:43:27,943 main.py:51] epoch 13322, training loss: 9682.84, average training loss: 9185.63, base loss: 14402.35
[INFO 2017-06-28 15:43:28,607 main.py:51] epoch 13323, training loss: 8844.61, average training loss: 9184.56, base loss: 14400.10
[INFO 2017-06-28 15:43:29,261 main.py:51] epoch 13324, training loss: 9043.91, average training loss: 9183.85, base loss: 14400.10
[INFO 2017-06-28 15:43:29,910 main.py:51] epoch 13325, training loss: 10303.28, average training loss: 9185.37, base loss: 14402.67
[INFO 2017-06-28 15:43:30,583 main.py:51] epoch 13326, training loss: 9429.25, average training loss: 9186.04, base loss: 14403.22
[INFO 2017-06-28 15:43:31,255 main.py:51] epoch 13327, training loss: 8969.96, average training loss: 9185.69, base loss: 14403.41
[INFO 2017-06-28 15:43:31,918 main.py:51] epoch 13328, training loss: 8730.10, average training loss: 9185.16, base loss: 14403.01
[INFO 2017-06-28 15:43:32,587 main.py:51] epoch 13329, training loss: 8614.01, average training loss: 9184.72, base loss: 14400.97
[INFO 2017-06-28 15:43:33,261 main.py:51] epoch 13330, training loss: 10877.14, average training loss: 9185.32, base loss: 14400.57
[INFO 2017-06-28 15:43:33,936 main.py:51] epoch 13331, training loss: 9042.34, average training loss: 9184.73, base loss: 14399.26
[INFO 2017-06-28 15:43:34,571 main.py:51] epoch 13332, training loss: 8938.07, average training loss: 9183.53, base loss: 14398.27
[INFO 2017-06-28 15:43:35,247 main.py:51] epoch 13333, training loss: 8971.05, average training loss: 9182.97, base loss: 14395.57
[INFO 2017-06-28 15:43:35,920 main.py:51] epoch 13334, training loss: 8066.39, average training loss: 9181.72, base loss: 14392.96
[INFO 2017-06-28 15:43:36,573 main.py:51] epoch 13335, training loss: 9259.85, average training loss: 9179.65, base loss: 14389.57
[INFO 2017-06-28 15:43:37,217 main.py:51] epoch 13336, training loss: 9443.15, average training loss: 9179.56, base loss: 14388.96
[INFO 2017-06-28 15:43:37,853 main.py:51] epoch 13337, training loss: 10265.30, average training loss: 9181.60, base loss: 14391.64
[INFO 2017-06-28 15:43:38,507 main.py:51] epoch 13338, training loss: 9740.58, average training loss: 9181.55, base loss: 14392.89
[INFO 2017-06-28 15:43:39,171 main.py:51] epoch 13339, training loss: 10523.50, average training loss: 9181.43, base loss: 14391.45
[INFO 2017-06-28 15:43:39,842 main.py:51] epoch 13340, training loss: 9529.08, average training loss: 9181.98, base loss: 14392.45
[INFO 2017-06-28 15:43:40,508 main.py:51] epoch 13341, training loss: 9467.39, average training loss: 9182.15, base loss: 14393.49
[INFO 2017-06-28 15:43:41,159 main.py:51] epoch 13342, training loss: 7646.63, average training loss: 9180.35, base loss: 14389.38
[INFO 2017-06-28 15:43:41,828 main.py:51] epoch 13343, training loss: 10272.49, average training loss: 9181.63, base loss: 14391.06
[INFO 2017-06-28 15:43:42,483 main.py:51] epoch 13344, training loss: 9818.72, average training loss: 9182.61, base loss: 14393.47
[INFO 2017-06-28 15:43:43,135 main.py:51] epoch 13345, training loss: 9909.51, average training loss: 9182.79, base loss: 14394.61
[INFO 2017-06-28 15:43:43,795 main.py:51] epoch 13346, training loss: 8236.83, average training loss: 9181.32, base loss: 14392.62
[INFO 2017-06-28 15:43:44,441 main.py:51] epoch 13347, training loss: 8391.35, average training loss: 9180.37, base loss: 14390.81
[INFO 2017-06-28 15:43:45,090 main.py:51] epoch 13348, training loss: 9483.75, average training loss: 9179.74, base loss: 14392.59
[INFO 2017-06-28 15:43:45,766 main.py:51] epoch 13349, training loss: 8938.17, average training loss: 9178.81, base loss: 14392.38
[INFO 2017-06-28 15:43:46,410 main.py:51] epoch 13350, training loss: 8188.39, average training loss: 9179.27, base loss: 14394.11
[INFO 2017-06-28 15:43:47,046 main.py:51] epoch 13351, training loss: 8812.12, average training loss: 9177.44, base loss: 14391.75
[INFO 2017-06-28 15:43:47,723 main.py:51] epoch 13352, training loss: 8893.13, average training loss: 9176.63, base loss: 14390.73
[INFO 2017-06-28 15:43:48,377 main.py:51] epoch 13353, training loss: 8298.59, average training loss: 9176.50, base loss: 14391.48
[INFO 2017-06-28 15:43:49,035 main.py:51] epoch 13354, training loss: 9513.52, average training loss: 9176.31, base loss: 14392.08
[INFO 2017-06-28 15:43:49,701 main.py:51] epoch 13355, training loss: 9709.19, average training loss: 9177.29, base loss: 14393.27
[INFO 2017-06-28 15:43:50,344 main.py:51] epoch 13356, training loss: 8664.82, average training loss: 9176.23, base loss: 14391.19
[INFO 2017-06-28 15:43:51,014 main.py:51] epoch 13357, training loss: 9298.31, average training loss: 9177.08, base loss: 14391.90
[INFO 2017-06-28 15:43:51,684 main.py:51] epoch 13358, training loss: 9025.76, average training loss: 9176.90, base loss: 14391.86
[INFO 2017-06-28 15:43:52,354 main.py:51] epoch 13359, training loss: 9917.42, average training loss: 9176.99, base loss: 14392.21
[INFO 2017-06-28 15:43:53,010 main.py:51] epoch 13360, training loss: 10073.06, average training loss: 9176.86, base loss: 14391.89
[INFO 2017-06-28 15:43:53,694 main.py:51] epoch 13361, training loss: 9833.94, average training loss: 9177.01, base loss: 14391.31
[INFO 2017-06-28 15:43:54,346 main.py:51] epoch 13362, training loss: 9209.64, average training loss: 9177.58, base loss: 14392.14
[INFO 2017-06-28 15:43:54,996 main.py:51] epoch 13363, training loss: 9396.16, average training loss: 9177.62, base loss: 14393.31
[INFO 2017-06-28 15:43:55,656 main.py:51] epoch 13364, training loss: 8942.21, average training loss: 9176.76, base loss: 14393.32
[INFO 2017-06-28 15:43:56,305 main.py:51] epoch 13365, training loss: 8950.05, average training loss: 9176.53, base loss: 14393.02
[INFO 2017-06-28 15:43:56,970 main.py:51] epoch 13366, training loss: 8713.74, average training loss: 9177.39, base loss: 14396.00
[INFO 2017-06-28 15:43:57,631 main.py:51] epoch 13367, training loss: 9671.12, average training loss: 9176.91, base loss: 14395.83
[INFO 2017-06-28 15:43:58,282 main.py:51] epoch 13368, training loss: 8380.99, average training loss: 9175.67, base loss: 14394.55
[INFO 2017-06-28 15:43:58,954 main.py:51] epoch 13369, training loss: 8682.40, average training loss: 9174.73, base loss: 14391.30
[INFO 2017-06-28 15:43:59,620 main.py:51] epoch 13370, training loss: 9238.21, average training loss: 9174.10, base loss: 14390.64
[INFO 2017-06-28 15:44:00,285 main.py:51] epoch 13371, training loss: 9832.10, average training loss: 9175.16, base loss: 14393.35
[INFO 2017-06-28 15:44:00,931 main.py:51] epoch 13372, training loss: 10136.32, average training loss: 9176.91, base loss: 14396.34
[INFO 2017-06-28 15:44:01,590 main.py:51] epoch 13373, training loss: 9449.92, average training loss: 9177.12, base loss: 14395.96
[INFO 2017-06-28 15:44:02,248 main.py:51] epoch 13374, training loss: 9030.69, average training loss: 9175.51, base loss: 14392.01
[INFO 2017-06-28 15:44:02,914 main.py:51] epoch 13375, training loss: 9109.40, average training loss: 9175.39, base loss: 14392.66
[INFO 2017-06-28 15:44:03,568 main.py:51] epoch 13376, training loss: 9342.77, average training loss: 9175.89, base loss: 14392.98
[INFO 2017-06-28 15:44:04,203 main.py:51] epoch 13377, training loss: 8372.86, average training loss: 9175.01, base loss: 14391.47
[INFO 2017-06-28 15:44:04,862 main.py:51] epoch 13378, training loss: 9385.48, average training loss: 9175.84, base loss: 14394.06
[INFO 2017-06-28 15:44:05,513 main.py:51] epoch 13379, training loss: 8394.50, average training loss: 9174.75, base loss: 14391.91
[INFO 2017-06-28 15:44:06,167 main.py:51] epoch 13380, training loss: 9497.22, average training loss: 9174.36, base loss: 14392.28
[INFO 2017-06-28 15:44:06,822 main.py:51] epoch 13381, training loss: 9143.19, average training loss: 9172.61, base loss: 14388.29
[INFO 2017-06-28 15:44:07,446 main.py:51] epoch 13382, training loss: 8856.02, average training loss: 9169.83, base loss: 14384.50
[INFO 2017-06-28 15:44:08,103 main.py:51] epoch 13383, training loss: 9647.20, average training loss: 9171.01, base loss: 14386.15
[INFO 2017-06-28 15:44:08,755 main.py:51] epoch 13384, training loss: 9399.24, average training loss: 9172.10, base loss: 14387.48
[INFO 2017-06-28 15:44:09,421 main.py:51] epoch 13385, training loss: 8594.42, average training loss: 9171.87, base loss: 14388.60
[INFO 2017-06-28 15:44:10,091 main.py:51] epoch 13386, training loss: 9534.29, average training loss: 9171.81, base loss: 14388.00
[INFO 2017-06-28 15:44:10,764 main.py:51] epoch 13387, training loss: 10310.52, average training loss: 9172.68, base loss: 14390.08
[INFO 2017-06-28 15:44:11,424 main.py:51] epoch 13388, training loss: 8172.84, average training loss: 9172.51, base loss: 14391.15
[INFO 2017-06-28 15:44:12,081 main.py:51] epoch 13389, training loss: 8999.75, average training loss: 9171.68, base loss: 14390.92
[INFO 2017-06-28 15:44:12,753 main.py:51] epoch 13390, training loss: 8408.24, average training loss: 9170.41, base loss: 14389.51
[INFO 2017-06-28 15:44:13,438 main.py:51] epoch 13391, training loss: 9226.10, average training loss: 9169.70, base loss: 14389.42
[INFO 2017-06-28 15:44:14,107 main.py:51] epoch 13392, training loss: 9485.21, average training loss: 9170.29, base loss: 14390.96
[INFO 2017-06-28 15:44:14,760 main.py:51] epoch 13393, training loss: 10013.68, average training loss: 9171.06, base loss: 14393.09
[INFO 2017-06-28 15:44:15,417 main.py:51] epoch 13394, training loss: 9496.90, average training loss: 9170.45, base loss: 14390.35
[INFO 2017-06-28 15:44:16,078 main.py:51] epoch 13395, training loss: 9500.37, average training loss: 9171.16, base loss: 14393.23
[INFO 2017-06-28 15:44:16,750 main.py:51] epoch 13396, training loss: 9282.92, average training loss: 9171.09, base loss: 14393.03
[INFO 2017-06-28 15:44:17,405 main.py:51] epoch 13397, training loss: 8634.90, average training loss: 9170.20, base loss: 14392.22
[INFO 2017-06-28 15:44:18,077 main.py:51] epoch 13398, training loss: 8359.24, average training loss: 9170.27, base loss: 14391.46
[INFO 2017-06-28 15:44:18,737 main.py:51] epoch 13399, training loss: 7508.34, average training loss: 9169.08, base loss: 14388.49
[INFO 2017-06-28 15:44:18,737 main.py:53] epoch 13399, testing
[INFO 2017-06-28 15:44:21,316 main.py:105] average testing loss: 10290.21, base loss: 14735.84
[INFO 2017-06-28 15:44:21,316 main.py:106] improve_loss: 4445.64, improve_percent: 0.30
[INFO 2017-06-28 15:44:21,317 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:44:21,978 main.py:51] epoch 13400, training loss: 9965.82, average training loss: 9169.79, base loss: 14390.24
[INFO 2017-06-28 15:44:22,637 main.py:51] epoch 13401, training loss: 9633.01, average training loss: 9170.40, base loss: 14393.59
[INFO 2017-06-28 15:44:23,303 main.py:51] epoch 13402, training loss: 9294.38, average training loss: 9171.39, base loss: 14395.28
[INFO 2017-06-28 15:44:23,972 main.py:51] epoch 13403, training loss: 9183.67, average training loss: 9171.75, base loss: 14397.36
[INFO 2017-06-28 15:44:24,630 main.py:51] epoch 13404, training loss: 9001.32, average training loss: 9172.49, base loss: 14398.66
[INFO 2017-06-28 15:44:25,273 main.py:51] epoch 13405, training loss: 8966.47, average training loss: 9172.26, base loss: 14397.53
[INFO 2017-06-28 15:44:25,947 main.py:51] epoch 13406, training loss: 8979.19, average training loss: 9170.08, base loss: 14395.12
[INFO 2017-06-28 15:44:26,609 main.py:51] epoch 13407, training loss: 9355.09, average training loss: 9168.61, base loss: 14393.81
[INFO 2017-06-28 15:44:27,263 main.py:51] epoch 13408, training loss: 10632.01, average training loss: 9171.02, base loss: 14397.52
[INFO 2017-06-28 15:44:27,935 main.py:51] epoch 13409, training loss: 8937.16, average training loss: 9170.56, base loss: 14396.74
[INFO 2017-06-28 15:44:28,585 main.py:51] epoch 13410, training loss: 9802.87, average training loss: 9170.48, base loss: 14395.87
[INFO 2017-06-28 15:44:29,238 main.py:51] epoch 13411, training loss: 7726.82, average training loss: 9169.33, base loss: 14393.48
[INFO 2017-06-28 15:44:29,868 main.py:51] epoch 13412, training loss: 11330.32, average training loss: 9171.75, base loss: 14396.31
[INFO 2017-06-28 15:44:30,523 main.py:51] epoch 13413, training loss: 9578.17, average training loss: 9171.99, base loss: 14395.54
[INFO 2017-06-28 15:44:31,190 main.py:51] epoch 13414, training loss: 9178.88, average training loss: 9171.46, base loss: 14394.31
[INFO 2017-06-28 15:44:31,849 main.py:51] epoch 13415, training loss: 9906.92, average training loss: 9172.33, base loss: 14394.38
[INFO 2017-06-28 15:44:32,488 main.py:51] epoch 13416, training loss: 7079.94, average training loss: 9170.11, base loss: 14390.40
[INFO 2017-06-28 15:44:33,142 main.py:51] epoch 13417, training loss: 10251.41, average training loss: 9170.24, base loss: 14388.90
[INFO 2017-06-28 15:44:33,780 main.py:51] epoch 13418, training loss: 7658.95, average training loss: 9168.99, base loss: 14386.71
[INFO 2017-06-28 15:44:34,430 main.py:51] epoch 13419, training loss: 10019.68, average training loss: 9170.21, base loss: 14389.62
[INFO 2017-06-28 15:44:35,084 main.py:51] epoch 13420, training loss: 9063.71, average training loss: 9170.91, base loss: 14390.13
[INFO 2017-06-28 15:44:35,742 main.py:51] epoch 13421, training loss: 9603.61, average training loss: 9171.12, base loss: 14390.29
[INFO 2017-06-28 15:44:36,409 main.py:51] epoch 13422, training loss: 8556.36, average training loss: 9170.50, base loss: 14388.39
[INFO 2017-06-28 15:44:37,080 main.py:51] epoch 13423, training loss: 9042.21, average training loss: 9169.91, base loss: 14386.52
[INFO 2017-06-28 15:44:37,728 main.py:51] epoch 13424, training loss: 9280.96, average training loss: 9169.29, base loss: 14386.23
[INFO 2017-06-28 15:44:38,384 main.py:51] epoch 13425, training loss: 9944.20, average training loss: 9169.37, base loss: 14387.30
[INFO 2017-06-28 15:44:39,025 main.py:51] epoch 13426, training loss: 8373.02, average training loss: 9168.88, base loss: 14387.27
[INFO 2017-06-28 15:44:39,680 main.py:51] epoch 13427, training loss: 9487.58, average training loss: 9169.79, base loss: 14388.37
[INFO 2017-06-28 15:44:40,338 main.py:51] epoch 13428, training loss: 9339.82, average training loss: 9168.89, base loss: 14385.54
[INFO 2017-06-28 15:44:41,001 main.py:51] epoch 13429, training loss: 8753.17, average training loss: 9169.42, base loss: 14387.65
[INFO 2017-06-28 15:44:41,678 main.py:51] epoch 13430, training loss: 8318.73, average training loss: 9167.47, base loss: 14385.67
[INFO 2017-06-28 15:44:42,317 main.py:51] epoch 13431, training loss: 9896.28, average training loss: 9168.10, base loss: 14387.40
[INFO 2017-06-28 15:44:42,977 main.py:51] epoch 13432, training loss: 8772.30, average training loss: 9167.46, base loss: 14385.40
[INFO 2017-06-28 15:44:43,615 main.py:51] epoch 13433, training loss: 9407.78, average training loss: 9168.43, base loss: 14387.18
[INFO 2017-06-28 15:44:44,258 main.py:51] epoch 13434, training loss: 8394.93, average training loss: 9167.26, base loss: 14386.16
[INFO 2017-06-28 15:44:44,918 main.py:51] epoch 13435, training loss: 9761.39, average training loss: 9168.50, base loss: 14388.64
[INFO 2017-06-28 15:44:45,574 main.py:51] epoch 13436, training loss: 8125.00, average training loss: 9167.42, base loss: 14387.41
[INFO 2017-06-28 15:44:46,231 main.py:51] epoch 13437, training loss: 9814.66, average training loss: 9168.85, base loss: 14390.95
[INFO 2017-06-28 15:44:46,897 main.py:51] epoch 13438, training loss: 9169.28, average training loss: 9169.14, base loss: 14391.42
[INFO 2017-06-28 15:44:47,546 main.py:51] epoch 13439, training loss: 8882.90, average training loss: 9168.92, base loss: 14393.14
[INFO 2017-06-28 15:44:48,190 main.py:51] epoch 13440, training loss: 7828.55, average training loss: 9167.20, base loss: 14391.04
[INFO 2017-06-28 15:44:48,858 main.py:51] epoch 13441, training loss: 9375.80, average training loss: 9167.20, base loss: 14392.00
[INFO 2017-06-28 15:44:49,536 main.py:51] epoch 13442, training loss: 9704.12, average training loss: 9167.98, base loss: 14392.66
[INFO 2017-06-28 15:44:50,210 main.py:51] epoch 13443, training loss: 10439.46, average training loss: 9170.09, base loss: 14395.54
[INFO 2017-06-28 15:44:50,885 main.py:51] epoch 13444, training loss: 8269.87, average training loss: 9168.08, base loss: 14392.57
[INFO 2017-06-28 15:44:51,549 main.py:51] epoch 13445, training loss: 8656.70, average training loss: 9167.47, base loss: 14390.62
[INFO 2017-06-28 15:44:52,203 main.py:51] epoch 13446, training loss: 9837.51, average training loss: 9168.92, base loss: 14393.39
[INFO 2017-06-28 15:44:52,857 main.py:51] epoch 13447, training loss: 9732.17, average training loss: 9170.56, base loss: 14394.67
[INFO 2017-06-28 15:44:53,518 main.py:51] epoch 13448, training loss: 9905.11, average training loss: 9170.03, base loss: 14394.90
[INFO 2017-06-28 15:44:54,204 main.py:51] epoch 13449, training loss: 9266.90, average training loss: 9168.22, base loss: 14392.76
[INFO 2017-06-28 15:44:54,843 main.py:51] epoch 13450, training loss: 8787.86, average training loss: 9167.73, base loss: 14392.06
[INFO 2017-06-28 15:44:55,509 main.py:51] epoch 13451, training loss: 8204.76, average training loss: 9166.40, base loss: 14389.73
[INFO 2017-06-28 15:44:56,162 main.py:51] epoch 13452, training loss: 9162.05, average training loss: 9165.77, base loss: 14388.34
[INFO 2017-06-28 15:44:56,829 main.py:51] epoch 13453, training loss: 7620.04, average training loss: 9164.08, base loss: 14386.00
[INFO 2017-06-28 15:44:57,487 main.py:51] epoch 13454, training loss: 8752.46, average training loss: 9164.21, base loss: 14385.09
[INFO 2017-06-28 15:44:58,143 main.py:51] epoch 13455, training loss: 9133.70, average training loss: 9164.37, base loss: 14385.26
[INFO 2017-06-28 15:44:58,827 main.py:51] epoch 13456, training loss: 9240.48, average training loss: 9164.80, base loss: 14386.73
[INFO 2017-06-28 15:44:59,484 main.py:51] epoch 13457, training loss: 9355.22, average training loss: 9165.23, base loss: 14388.68
[INFO 2017-06-28 15:45:00,171 main.py:51] epoch 13458, training loss: 10315.59, average training loss: 9166.08, base loss: 14389.89
[INFO 2017-06-28 15:45:00,832 main.py:51] epoch 13459, training loss: 8913.58, average training loss: 9166.07, base loss: 14387.90
[INFO 2017-06-28 15:45:01,520 main.py:51] epoch 13460, training loss: 9544.52, average training loss: 9166.36, base loss: 14388.09
[INFO 2017-06-28 15:45:02,178 main.py:51] epoch 13461, training loss: 9142.85, average training loss: 9166.13, base loss: 14388.38
[INFO 2017-06-28 15:45:02,832 main.py:51] epoch 13462, training loss: 9625.86, average training loss: 9166.77, base loss: 14389.05
[INFO 2017-06-28 15:45:03,492 main.py:51] epoch 13463, training loss: 10699.15, average training loss: 9168.32, base loss: 14391.64
[INFO 2017-06-28 15:45:04,166 main.py:51] epoch 13464, training loss: 9598.79, average training loss: 9169.35, base loss: 14392.35
[INFO 2017-06-28 15:45:04,829 main.py:51] epoch 13465, training loss: 9015.08, average training loss: 9169.33, base loss: 14392.69
[INFO 2017-06-28 15:45:05,499 main.py:51] epoch 13466, training loss: 10035.76, average training loss: 9170.56, base loss: 14397.95
[INFO 2017-06-28 15:45:06,175 main.py:51] epoch 13467, training loss: 9445.65, average training loss: 9170.50, base loss: 14397.20
[INFO 2017-06-28 15:45:06,829 main.py:51] epoch 13468, training loss: 9966.72, average training loss: 9169.13, base loss: 14395.58
[INFO 2017-06-28 15:45:07,501 main.py:51] epoch 13469, training loss: 9216.05, average training loss: 9168.83, base loss: 14396.12
[INFO 2017-06-28 15:45:08,160 main.py:51] epoch 13470, training loss: 8877.49, average training loss: 9167.54, base loss: 14394.94
[INFO 2017-06-28 15:45:08,816 main.py:51] epoch 13471, training loss: 9825.24, average training loss: 9167.97, base loss: 14396.46
[INFO 2017-06-28 15:45:09,476 main.py:51] epoch 13472, training loss: 12101.80, average training loss: 9170.85, base loss: 14399.51
[INFO 2017-06-28 15:45:10,132 main.py:51] epoch 13473, training loss: 9214.91, average training loss: 9170.91, base loss: 14398.17
[INFO 2017-06-28 15:45:10,789 main.py:51] epoch 13474, training loss: 8861.71, average training loss: 9170.94, base loss: 14399.17
[INFO 2017-06-28 15:45:11,435 main.py:51] epoch 13475, training loss: 10207.02, average training loss: 9172.84, base loss: 14404.31
[INFO 2017-06-28 15:45:12,104 main.py:51] epoch 13476, training loss: 10030.31, average training loss: 9174.20, base loss: 14406.59
[INFO 2017-06-28 15:45:12,764 main.py:51] epoch 13477, training loss: 8910.05, average training loss: 9175.13, base loss: 14407.90
[INFO 2017-06-28 15:45:13,399 main.py:51] epoch 13478, training loss: 10526.19, average training loss: 9175.68, base loss: 14409.04
[INFO 2017-06-28 15:45:14,060 main.py:51] epoch 13479, training loss: 8266.45, average training loss: 9174.82, base loss: 14406.99
[INFO 2017-06-28 15:45:14,700 main.py:51] epoch 13480, training loss: 9313.78, average training loss: 9175.97, base loss: 14409.49
[INFO 2017-06-28 15:45:15,381 main.py:51] epoch 13481, training loss: 8113.61, average training loss: 9174.31, base loss: 14406.65
[INFO 2017-06-28 15:45:16,035 main.py:51] epoch 13482, training loss: 8268.62, average training loss: 9174.68, base loss: 14407.84
[INFO 2017-06-28 15:45:16,707 main.py:51] epoch 13483, training loss: 8809.31, average training loss: 9173.34, base loss: 14402.57
[INFO 2017-06-28 15:45:17,366 main.py:51] epoch 13484, training loss: 8083.15, average training loss: 9172.79, base loss: 14401.43
[INFO 2017-06-28 15:45:18,048 main.py:51] epoch 13485, training loss: 8737.15, average training loss: 9173.66, base loss: 14403.12
[INFO 2017-06-28 15:45:18,717 main.py:51] epoch 13486, training loss: 12569.99, average training loss: 9178.22, base loss: 14408.05
[INFO 2017-06-28 15:45:19,375 main.py:51] epoch 13487, training loss: 9109.87, average training loss: 9177.54, base loss: 14407.33
[INFO 2017-06-28 15:45:20,030 main.py:51] epoch 13488, training loss: 8830.28, average training loss: 9178.22, base loss: 14408.18
[INFO 2017-06-28 15:45:20,690 main.py:51] epoch 13489, training loss: 10473.63, average training loss: 9179.51, base loss: 14409.86
[INFO 2017-06-28 15:45:21,334 main.py:51] epoch 13490, training loss: 8231.01, average training loss: 9178.28, base loss: 14408.01
[INFO 2017-06-28 15:45:22,005 main.py:51] epoch 13491, training loss: 8827.92, average training loss: 9178.05, base loss: 14409.45
[INFO 2017-06-28 15:45:22,674 main.py:51] epoch 13492, training loss: 8447.29, average training loss: 9177.04, base loss: 14408.67
[INFO 2017-06-28 15:45:23,348 main.py:51] epoch 13493, training loss: 11459.90, average training loss: 9179.53, base loss: 14412.38
[INFO 2017-06-28 15:45:24,001 main.py:51] epoch 13494, training loss: 8687.89, average training loss: 9180.13, base loss: 14413.02
[INFO 2017-06-28 15:45:24,677 main.py:51] epoch 13495, training loss: 9081.56, average training loss: 9180.38, base loss: 14412.93
[INFO 2017-06-28 15:45:25,337 main.py:51] epoch 13496, training loss: 9572.33, average training loss: 9180.85, base loss: 14412.59
[INFO 2017-06-28 15:45:25,999 main.py:51] epoch 13497, training loss: 7908.27, average training loss: 9180.01, base loss: 14411.95
[INFO 2017-06-28 15:45:26,675 main.py:51] epoch 13498, training loss: 10119.50, average training loss: 9180.47, base loss: 14414.32
[INFO 2017-06-28 15:45:27,310 main.py:51] epoch 13499, training loss: 8929.45, average training loss: 9181.46, base loss: 14415.38
[INFO 2017-06-28 15:45:27,310 main.py:53] epoch 13499, testing
[INFO 2017-06-28 15:45:29,937 main.py:105] average testing loss: 10391.25, base loss: 14529.16
[INFO 2017-06-28 15:45:29,937 main.py:106] improve_loss: 4137.91, improve_percent: 0.28
[INFO 2017-06-28 15:45:29,938 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:45:30,595 main.py:51] epoch 13500, training loss: 7925.41, average training loss: 9179.84, base loss: 14413.31
[INFO 2017-06-28 15:45:31,265 main.py:51] epoch 13501, training loss: 8949.02, average training loss: 9180.89, base loss: 14416.61
[INFO 2017-06-28 15:45:31,938 main.py:51] epoch 13502, training loss: 9914.47, average training loss: 9181.67, base loss: 14418.12
[INFO 2017-06-28 15:45:32,615 main.py:51] epoch 13503, training loss: 8958.74, average training loss: 9181.79, base loss: 14419.35
[INFO 2017-06-28 15:45:33,283 main.py:51] epoch 13504, training loss: 9483.69, average training loss: 9182.88, base loss: 14422.19
[INFO 2017-06-28 15:45:33,947 main.py:51] epoch 13505, training loss: 8190.65, average training loss: 9183.25, base loss: 14423.66
[INFO 2017-06-28 15:45:34,594 main.py:51] epoch 13506, training loss: 10059.40, average training loss: 9184.39, base loss: 14424.69
[INFO 2017-06-28 15:45:35,228 main.py:51] epoch 13507, training loss: 9685.12, average training loss: 9184.97, base loss: 14423.98
[INFO 2017-06-28 15:45:35,888 main.py:51] epoch 13508, training loss: 11593.81, average training loss: 9186.76, base loss: 14424.44
[INFO 2017-06-28 15:45:36,546 main.py:51] epoch 13509, training loss: 9394.78, average training loss: 9187.73, base loss: 14424.95
[INFO 2017-06-28 15:45:37,205 main.py:51] epoch 13510, training loss: 9203.75, average training loss: 9186.96, base loss: 14423.76
[INFO 2017-06-28 15:45:37,863 main.py:51] epoch 13511, training loss: 9328.81, average training loss: 9187.63, base loss: 14426.39
[INFO 2017-06-28 15:45:38,521 main.py:51] epoch 13512, training loss: 7962.44, average training loss: 9186.28, base loss: 14423.29
[INFO 2017-06-28 15:45:39,175 main.py:51] epoch 13513, training loss: 8722.56, average training loss: 9186.53, base loss: 14422.89
[INFO 2017-06-28 15:45:39,842 main.py:51] epoch 13514, training loss: 8484.92, average training loss: 9186.97, base loss: 14422.52
[INFO 2017-06-28 15:45:40,508 main.py:51] epoch 13515, training loss: 9595.51, average training loss: 9186.92, base loss: 14421.12
[INFO 2017-06-28 15:45:41,193 main.py:51] epoch 13516, training loss: 9806.67, average training loss: 9187.42, base loss: 14420.66
[INFO 2017-06-28 15:45:41,854 main.py:51] epoch 13517, training loss: 9289.31, average training loss: 9187.50, base loss: 14423.15
[INFO 2017-06-28 15:45:42,513 main.py:51] epoch 13518, training loss: 9706.35, average training loss: 9187.44, base loss: 14423.14
[INFO 2017-06-28 15:45:43,175 main.py:51] epoch 13519, training loss: 8925.94, average training loss: 9187.18, base loss: 14421.95
[INFO 2017-06-28 15:45:43,811 main.py:51] epoch 13520, training loss: 8133.45, average training loss: 9185.69, base loss: 14419.88
[INFO 2017-06-28 15:45:44,477 main.py:51] epoch 13521, training loss: 8530.81, average training loss: 9183.88, base loss: 14416.04
[INFO 2017-06-28 15:45:45,146 main.py:51] epoch 13522, training loss: 10183.07, average training loss: 9184.09, base loss: 14415.45
[INFO 2017-06-28 15:45:45,820 main.py:51] epoch 13523, training loss: 10760.85, average training loss: 9185.40, base loss: 14418.10
[INFO 2017-06-28 15:45:46,475 main.py:51] epoch 13524, training loss: 8107.40, average training loss: 9184.24, base loss: 14415.88
[INFO 2017-06-28 15:45:47,138 main.py:51] epoch 13525, training loss: 9058.80, average training loss: 9183.67, base loss: 14415.65
[INFO 2017-06-28 15:45:47,782 main.py:51] epoch 13526, training loss: 9456.35, average training loss: 9184.55, base loss: 14417.72
[INFO 2017-06-28 15:45:48,429 main.py:51] epoch 13527, training loss: 9173.81, average training loss: 9184.89, base loss: 14419.35
[INFO 2017-06-28 15:45:49,104 main.py:51] epoch 13528, training loss: 9103.28, average training loss: 9184.36, base loss: 14417.56
[INFO 2017-06-28 15:45:49,763 main.py:51] epoch 13529, training loss: 8004.53, average training loss: 9182.93, base loss: 14415.08
[INFO 2017-06-28 15:45:50,468 main.py:51] epoch 13530, training loss: 8987.61, average training loss: 9183.02, base loss: 14414.77
[INFO 2017-06-28 15:45:51,158 main.py:51] epoch 13531, training loss: 9486.29, average training loss: 9182.76, base loss: 14414.22
[INFO 2017-06-28 15:45:51,831 main.py:51] epoch 13532, training loss: 8012.42, average training loss: 9182.45, base loss: 14414.36
[INFO 2017-06-28 15:45:52,487 main.py:51] epoch 13533, training loss: 7773.70, average training loss: 9181.03, base loss: 14411.05
[INFO 2017-06-28 15:45:53,175 main.py:51] epoch 13534, training loss: 9135.70, average training loss: 9181.45, base loss: 14411.03
[INFO 2017-06-28 15:45:53,811 main.py:51] epoch 13535, training loss: 10329.27, average training loss: 9183.70, base loss: 14415.28
[INFO 2017-06-28 15:45:54,478 main.py:51] epoch 13536, training loss: 9981.18, average training loss: 9185.16, base loss: 14417.54
[INFO 2017-06-28 15:45:55,138 main.py:51] epoch 13537, training loss: 8435.25, average training loss: 9185.79, base loss: 14417.67
[INFO 2017-06-28 15:45:55,786 main.py:51] epoch 13538, training loss: 10211.18, average training loss: 9186.95, base loss: 14420.07
[INFO 2017-06-28 15:45:56,455 main.py:51] epoch 13539, training loss: 9660.09, average training loss: 9187.56, base loss: 14421.58
[INFO 2017-06-28 15:45:57,110 main.py:51] epoch 13540, training loss: 7419.92, average training loss: 9184.49, base loss: 14416.37
[INFO 2017-06-28 15:45:57,763 main.py:51] epoch 13541, training loss: 10111.84, average training loss: 9184.50, base loss: 14415.99
[INFO 2017-06-28 15:45:58,425 main.py:51] epoch 13542, training loss: 8295.17, average training loss: 9183.37, base loss: 14414.15
[INFO 2017-06-28 15:45:59,078 main.py:51] epoch 13543, training loss: 8621.90, average training loss: 9182.11, base loss: 14412.64
[INFO 2017-06-28 15:45:59,749 main.py:51] epoch 13544, training loss: 9039.01, average training loss: 9182.37, base loss: 14413.96
[INFO 2017-06-28 15:46:00,422 main.py:51] epoch 13545, training loss: 8213.32, average training loss: 9182.39, base loss: 14415.03
[INFO 2017-06-28 15:46:01,103 main.py:51] epoch 13546, training loss: 9820.53, average training loss: 9184.37, base loss: 14418.41
[INFO 2017-06-28 15:46:01,739 main.py:51] epoch 13547, training loss: 9766.87, average training loss: 9184.46, base loss: 14418.07
[INFO 2017-06-28 15:46:02,418 main.py:51] epoch 13548, training loss: 8401.35, average training loss: 9184.04, base loss: 14417.65
[INFO 2017-06-28 15:46:03,074 main.py:51] epoch 13549, training loss: 9028.92, average training loss: 9183.86, base loss: 14417.02
[INFO 2017-06-28 15:46:03,738 main.py:51] epoch 13550, training loss: 8459.68, average training loss: 9183.14, base loss: 14414.61
[INFO 2017-06-28 15:46:04,387 main.py:51] epoch 13551, training loss: 8676.19, average training loss: 9182.22, base loss: 14414.29
[INFO 2017-06-28 15:46:05,053 main.py:51] epoch 13552, training loss: 9862.83, average training loss: 9183.58, base loss: 14418.29
[INFO 2017-06-28 15:46:05,718 main.py:51] epoch 13553, training loss: 7643.85, average training loss: 9181.32, base loss: 14416.50
[INFO 2017-06-28 15:46:06,375 main.py:51] epoch 13554, training loss: 9140.19, average training loss: 9180.50, base loss: 14414.56
[INFO 2017-06-28 15:46:07,046 main.py:51] epoch 13555, training loss: 8576.02, average training loss: 9180.37, base loss: 14415.98
[INFO 2017-06-28 15:46:07,712 main.py:51] epoch 13556, training loss: 9167.49, average training loss: 9180.07, base loss: 14413.83
[INFO 2017-06-28 15:46:08,357 main.py:51] epoch 13557, training loss: 8593.05, average training loss: 9179.78, base loss: 14412.93
[INFO 2017-06-28 15:46:09,009 main.py:51] epoch 13558, training loss: 9694.61, average training loss: 9180.41, base loss: 14415.17
[INFO 2017-06-28 15:46:09,679 main.py:51] epoch 13559, training loss: 9119.69, average training loss: 9180.17, base loss: 14413.99
[INFO 2017-06-28 15:46:10,331 main.py:51] epoch 13560, training loss: 9454.91, average training loss: 9180.74, base loss: 14415.01
[INFO 2017-06-28 15:46:10,998 main.py:51] epoch 13561, training loss: 10192.09, average training loss: 9182.11, base loss: 14418.43
[INFO 2017-06-28 15:46:11,653 main.py:51] epoch 13562, training loss: 8417.60, average training loss: 9181.30, base loss: 14417.18
[INFO 2017-06-28 15:46:12,305 main.py:51] epoch 13563, training loss: 9498.21, average training loss: 9182.42, base loss: 14417.80
[INFO 2017-06-28 15:46:12,962 main.py:51] epoch 13564, training loss: 9326.59, average training loss: 9179.96, base loss: 14414.61
[INFO 2017-06-28 15:46:13,613 main.py:51] epoch 13565, training loss: 8845.37, average training loss: 9180.94, base loss: 14416.78
[INFO 2017-06-28 15:46:14,261 main.py:51] epoch 13566, training loss: 8767.04, average training loss: 9180.01, base loss: 14413.71
[INFO 2017-06-28 15:46:14,916 main.py:51] epoch 13567, training loss: 8262.53, average training loss: 9179.35, base loss: 14414.46
[INFO 2017-06-28 15:46:15,554 main.py:51] epoch 13568, training loss: 9084.53, average training loss: 9179.78, base loss: 14414.35
[INFO 2017-06-28 15:46:16,192 main.py:51] epoch 13569, training loss: 9249.62, average training loss: 9178.83, base loss: 14413.15
[INFO 2017-06-28 15:46:16,855 main.py:51] epoch 13570, training loss: 8779.52, average training loss: 9178.00, base loss: 14413.53
[INFO 2017-06-28 15:46:17,502 main.py:51] epoch 13571, training loss: 7362.88, average training loss: 9176.92, base loss: 14413.02
[INFO 2017-06-28 15:46:18,177 main.py:51] epoch 13572, training loss: 9341.02, average training loss: 9178.26, base loss: 14414.89
[INFO 2017-06-28 15:46:18,826 main.py:51] epoch 13573, training loss: 7925.63, average training loss: 9176.96, base loss: 14412.11
[INFO 2017-06-28 15:46:19,472 main.py:51] epoch 13574, training loss: 10486.53, average training loss: 9179.61, base loss: 14417.20
[INFO 2017-06-28 15:46:20,117 main.py:51] epoch 13575, training loss: 8793.73, average training loss: 9179.88, base loss: 14417.61
[INFO 2017-06-28 15:46:20,785 main.py:51] epoch 13576, training loss: 7845.92, average training loss: 9179.01, base loss: 14417.46
[INFO 2017-06-28 15:46:21,443 main.py:51] epoch 13577, training loss: 7819.59, average training loss: 9176.34, base loss: 14412.39
[INFO 2017-06-28 15:46:22,131 main.py:51] epoch 13578, training loss: 8997.61, average training loss: 9175.80, base loss: 14411.50
[INFO 2017-06-28 15:46:22,793 main.py:51] epoch 13579, training loss: 8401.81, average training loss: 9173.90, base loss: 14407.92
[INFO 2017-06-28 15:46:23,454 main.py:51] epoch 13580, training loss: 9834.60, average training loss: 9175.63, base loss: 14410.42
[INFO 2017-06-28 15:46:24,121 main.py:51] epoch 13581, training loss: 8685.80, average training loss: 9174.87, base loss: 14411.24
[INFO 2017-06-28 15:46:24,782 main.py:51] epoch 13582, training loss: 8489.99, average training loss: 9174.74, base loss: 14411.33
[INFO 2017-06-28 15:46:25,436 main.py:51] epoch 13583, training loss: 9188.84, average training loss: 9173.23, base loss: 14407.80
[INFO 2017-06-28 15:46:26,105 main.py:51] epoch 13584, training loss: 8355.55, average training loss: 9170.95, base loss: 14404.46
[INFO 2017-06-28 15:46:26,759 main.py:51] epoch 13585, training loss: 7827.31, average training loss: 9169.80, base loss: 14402.24
[INFO 2017-06-28 15:46:27,446 main.py:51] epoch 13586, training loss: 8523.84, average training loss: 9170.15, base loss: 14404.50
[INFO 2017-06-28 15:46:28,104 main.py:51] epoch 13587, training loss: 8495.50, average training loss: 9169.49, base loss: 14402.07
[INFO 2017-06-28 15:46:28,747 main.py:51] epoch 13588, training loss: 8460.22, average training loss: 9169.56, base loss: 14402.71
[INFO 2017-06-28 15:46:29,415 main.py:51] epoch 13589, training loss: 8603.48, average training loss: 9168.42, base loss: 14400.96
[INFO 2017-06-28 15:46:30,066 main.py:51] epoch 13590, training loss: 9262.85, average training loss: 9167.97, base loss: 14398.34
[INFO 2017-06-28 15:46:30,732 main.py:51] epoch 13591, training loss: 8918.34, average training loss: 9167.70, base loss: 14397.70
[INFO 2017-06-28 15:46:31,385 main.py:51] epoch 13592, training loss: 9074.07, average training loss: 9167.44, base loss: 14396.71
[INFO 2017-06-28 15:46:32,028 main.py:51] epoch 13593, training loss: 9653.78, average training loss: 9167.71, base loss: 14398.47
[INFO 2017-06-28 15:46:32,689 main.py:51] epoch 13594, training loss: 9639.10, average training loss: 9170.14, base loss: 14402.48
[INFO 2017-06-28 15:46:33,340 main.py:51] epoch 13595, training loss: 9866.90, average training loss: 9170.77, base loss: 14405.41
[INFO 2017-06-28 15:46:33,989 main.py:51] epoch 13596, training loss: 10209.50, average training loss: 9172.63, base loss: 14409.04
[INFO 2017-06-28 15:46:34,647 main.py:51] epoch 13597, training loss: 9415.88, average training loss: 9174.10, base loss: 14412.63
[INFO 2017-06-28 15:46:35,315 main.py:51] epoch 13598, training loss: 9008.38, average training loss: 9173.93, base loss: 14413.60
[INFO 2017-06-28 15:46:35,990 main.py:51] epoch 13599, training loss: 10004.49, average training loss: 9174.79, base loss: 14416.86
[INFO 2017-06-28 15:46:35,990 main.py:53] epoch 13599, testing
[INFO 2017-06-28 15:46:38,540 main.py:105] average testing loss: 10538.75, base loss: 14661.97
[INFO 2017-06-28 15:46:38,540 main.py:106] improve_loss: 4123.22, improve_percent: 0.28
[INFO 2017-06-28 15:46:38,541 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:46:39,194 main.py:51] epoch 13600, training loss: 9642.50, average training loss: 9175.45, base loss: 14418.26
[INFO 2017-06-28 15:46:39,860 main.py:51] epoch 13601, training loss: 8874.24, average training loss: 9175.43, base loss: 14419.14
[INFO 2017-06-28 15:46:40,625 main.py:51] epoch 13602, training loss: 9484.27, average training loss: 9175.99, base loss: 14420.77
[INFO 2017-06-28 15:46:41,378 main.py:51] epoch 13603, training loss: 8815.17, average training loss: 9175.91, base loss: 14419.45
[INFO 2017-06-28 15:46:42,143 main.py:51] epoch 13604, training loss: 9848.35, average training loss: 9177.03, base loss: 14422.58
[INFO 2017-06-28 15:46:42,878 main.py:51] epoch 13605, training loss: 10486.18, average training loss: 9179.75, base loss: 14429.65
[INFO 2017-06-28 15:46:43,593 main.py:51] epoch 13606, training loss: 10271.53, average training loss: 9180.80, base loss: 14430.53
[INFO 2017-06-28 15:46:44,240 main.py:51] epoch 13607, training loss: 9704.28, average training loss: 9182.33, base loss: 14433.49
[INFO 2017-06-28 15:46:44,923 main.py:51] epoch 13608, training loss: 9984.22, average training loss: 9183.22, base loss: 14435.52
[INFO 2017-06-28 15:46:45,599 main.py:51] epoch 13609, training loss: 8371.08, average training loss: 9182.97, base loss: 14434.48
[INFO 2017-06-28 15:46:46,265 main.py:51] epoch 13610, training loss: 9636.91, average training loss: 9183.81, base loss: 14434.29
[INFO 2017-06-28 15:46:46,916 main.py:51] epoch 13611, training loss: 9812.30, average training loss: 9185.23, base loss: 14438.81
[INFO 2017-06-28 15:46:47,581 main.py:51] epoch 13612, training loss: 10463.53, average training loss: 9187.58, base loss: 14441.87
[INFO 2017-06-28 15:46:48,253 main.py:51] epoch 13613, training loss: 9704.05, average training loss: 9188.85, base loss: 14442.87
[INFO 2017-06-28 15:46:48,961 main.py:51] epoch 13614, training loss: 8839.49, average training loss: 9187.39, base loss: 14443.98
[INFO 2017-06-28 15:46:49,619 main.py:51] epoch 13615, training loss: 8657.52, average training loss: 9188.25, base loss: 14445.53
[INFO 2017-06-28 15:46:50,289 main.py:51] epoch 13616, training loss: 9216.74, average training loss: 9187.30, base loss: 14445.69
[INFO 2017-06-28 15:46:50,960 main.py:51] epoch 13617, training loss: 8214.52, average training loss: 9186.58, base loss: 14444.11
[INFO 2017-06-28 15:46:51,627 main.py:51] epoch 13618, training loss: 7823.22, average training loss: 9186.01, base loss: 14444.99
[INFO 2017-06-28 15:46:52,278 main.py:51] epoch 13619, training loss: 10477.81, average training loss: 9188.33, base loss: 14447.42
[INFO 2017-06-28 15:46:52,937 main.py:51] epoch 13620, training loss: 9257.69, average training loss: 9188.31, base loss: 14447.26
[INFO 2017-06-28 15:46:53,590 main.py:51] epoch 13621, training loss: 8765.50, average training loss: 9188.37, base loss: 14448.23
[INFO 2017-06-28 15:46:54,251 main.py:51] epoch 13622, training loss: 8300.05, average training loss: 9188.07, base loss: 14447.90
[INFO 2017-06-28 15:46:54,916 main.py:51] epoch 13623, training loss: 8819.79, average training loss: 9187.22, base loss: 14447.59
[INFO 2017-06-28 15:46:55,595 main.py:51] epoch 13624, training loss: 9186.76, average training loss: 9186.05, base loss: 14446.64
[INFO 2017-06-28 15:46:56,277 main.py:51] epoch 13625, training loss: 9929.01, average training loss: 9187.81, base loss: 14449.10
[INFO 2017-06-28 15:46:56,925 main.py:51] epoch 13626, training loss: 8068.99, average training loss: 9187.27, base loss: 14446.16
[INFO 2017-06-28 15:46:57,579 main.py:51] epoch 13627, training loss: 9553.34, average training loss: 9187.43, base loss: 14446.63
[INFO 2017-06-28 15:46:58,229 main.py:51] epoch 13628, training loss: 9236.39, average training loss: 9188.96, base loss: 14450.53
[INFO 2017-06-28 15:46:58,883 main.py:51] epoch 13629, training loss: 7881.57, average training loss: 9187.52, base loss: 14448.68
[INFO 2017-06-28 15:46:59,537 main.py:51] epoch 13630, training loss: 7898.48, average training loss: 9187.20, base loss: 14448.91
[INFO 2017-06-28 15:47:00,198 main.py:51] epoch 13631, training loss: 9095.91, average training loss: 9184.92, base loss: 14447.28
[INFO 2017-06-28 15:47:00,872 main.py:51] epoch 13632, training loss: 9099.68, average training loss: 9184.59, base loss: 14447.94
[INFO 2017-06-28 15:47:01,524 main.py:51] epoch 13633, training loss: 8788.29, average training loss: 9183.09, base loss: 14445.00
[INFO 2017-06-28 15:47:02,200 main.py:51] epoch 13634, training loss: 9440.56, average training loss: 9183.74, base loss: 14446.85
[INFO 2017-06-28 15:47:02,853 main.py:51] epoch 13635, training loss: 8510.05, average training loss: 9182.66, base loss: 14443.88
[INFO 2017-06-28 15:47:03,534 main.py:51] epoch 13636, training loss: 8507.68, average training loss: 9181.83, base loss: 14443.50
[INFO 2017-06-28 15:47:04,196 main.py:51] epoch 13637, training loss: 8979.71, average training loss: 9182.39, base loss: 14444.56
[INFO 2017-06-28 15:47:04,861 main.py:51] epoch 13638, training loss: 8800.42, average training loss: 9182.64, base loss: 14445.10
[INFO 2017-06-28 15:47:05,530 main.py:51] epoch 13639, training loss: 8543.94, average training loss: 9183.07, base loss: 14445.09
[INFO 2017-06-28 15:47:06,181 main.py:51] epoch 13640, training loss: 8653.35, average training loss: 9183.45, base loss: 14443.81
[INFO 2017-06-28 15:47:06,896 main.py:51] epoch 13641, training loss: 9298.50, average training loss: 9184.76, base loss: 14446.53
[INFO 2017-06-28 15:47:07,528 main.py:51] epoch 13642, training loss: 9215.49, average training loss: 9184.89, base loss: 14445.81
[INFO 2017-06-28 15:47:08,195 main.py:51] epoch 13643, training loss: 11180.26, average training loss: 9185.23, base loss: 14446.67
[INFO 2017-06-28 15:47:08,879 main.py:51] epoch 13644, training loss: 10401.57, average training loss: 9186.72, base loss: 14449.96
[INFO 2017-06-28 15:47:09,542 main.py:51] epoch 13645, training loss: 10025.70, average training loss: 9187.41, base loss: 14451.30
[INFO 2017-06-28 15:47:10,197 main.py:51] epoch 13646, training loss: 9695.35, average training loss: 9189.34, base loss: 14454.33
[INFO 2017-06-28 15:47:10,844 main.py:51] epoch 13647, training loss: 8243.72, average training loss: 9187.61, base loss: 14452.10
[INFO 2017-06-28 15:47:11,498 main.py:51] epoch 13648, training loss: 10362.25, average training loss: 9189.68, base loss: 14453.87
[INFO 2017-06-28 15:47:12,157 main.py:51] epoch 13649, training loss: 8393.81, average training loss: 9189.01, base loss: 14453.66
[INFO 2017-06-28 15:47:12,824 main.py:51] epoch 13650, training loss: 9253.03, average training loss: 9189.57, base loss: 14454.60
[INFO 2017-06-28 15:47:13,485 main.py:51] epoch 13651, training loss: 8707.46, average training loss: 9188.93, base loss: 14453.88
[INFO 2017-06-28 15:47:14,145 main.py:51] epoch 13652, training loss: 8340.72, average training loss: 9188.75, base loss: 14452.50
[INFO 2017-06-28 15:47:14,794 main.py:51] epoch 13653, training loss: 8756.88, average training loss: 9187.74, base loss: 14450.24
[INFO 2017-06-28 15:47:15,436 main.py:51] epoch 13654, training loss: 9435.35, average training loss: 9187.80, base loss: 14449.83
[INFO 2017-06-28 15:47:16,118 main.py:51] epoch 13655, training loss: 8735.20, average training loss: 9186.72, base loss: 14447.16
[INFO 2017-06-28 15:47:16,782 main.py:51] epoch 13656, training loss: 8100.26, average training loss: 9184.98, base loss: 14445.00
[INFO 2017-06-28 15:47:17,468 main.py:51] epoch 13657, training loss: 8990.73, average training loss: 9184.85, base loss: 14444.03
[INFO 2017-06-28 15:47:18,155 main.py:51] epoch 13658, training loss: 7970.91, average training loss: 9183.09, base loss: 14441.12
[INFO 2017-06-28 15:47:18,810 main.py:51] epoch 13659, training loss: 8562.93, average training loss: 9181.93, base loss: 14439.20
[INFO 2017-06-28 15:47:19,470 main.py:51] epoch 13660, training loss: 9301.57, average training loss: 9181.63, base loss: 14437.18
[INFO 2017-06-28 15:47:20,150 main.py:51] epoch 13661, training loss: 9520.11, average training loss: 9181.12, base loss: 14436.42
[INFO 2017-06-28 15:47:20,809 main.py:51] epoch 13662, training loss: 10087.02, average training loss: 9182.92, base loss: 14440.13
[INFO 2017-06-28 15:47:21,475 main.py:51] epoch 13663, training loss: 9050.45, average training loss: 9182.67, base loss: 14440.13
[INFO 2017-06-28 15:47:22,145 main.py:51] epoch 13664, training loss: 7800.52, average training loss: 9182.21, base loss: 14440.40
[INFO 2017-06-28 15:47:22,795 main.py:51] epoch 13665, training loss: 10146.83, average training loss: 9183.80, base loss: 14441.79
[INFO 2017-06-28 15:47:23,469 main.py:51] epoch 13666, training loss: 8841.29, average training loss: 9182.51, base loss: 14439.14
[INFO 2017-06-28 15:47:24,130 main.py:51] epoch 13667, training loss: 10753.94, average training loss: 9183.88, base loss: 14440.47
[INFO 2017-06-28 15:47:24,795 main.py:51] epoch 13668, training loss: 9228.78, average training loss: 9183.48, base loss: 14440.00
[INFO 2017-06-28 15:47:25,447 main.py:51] epoch 13669, training loss: 8562.61, average training loss: 9183.66, base loss: 14440.22
[INFO 2017-06-28 15:47:26,105 main.py:51] epoch 13670, training loss: 10229.15, average training loss: 9183.66, base loss: 14438.44
[INFO 2017-06-28 15:47:26,784 main.py:51] epoch 13671, training loss: 8678.58, average training loss: 9183.56, base loss: 14438.08
[INFO 2017-06-28 15:47:27,443 main.py:51] epoch 13672, training loss: 9928.53, average training loss: 9184.31, base loss: 14439.31
[INFO 2017-06-28 15:47:28,126 main.py:51] epoch 13673, training loss: 8742.27, average training loss: 9183.53, base loss: 14439.13
[INFO 2017-06-28 15:47:28,794 main.py:51] epoch 13674, training loss: 9363.82, average training loss: 9183.29, base loss: 14439.63
[INFO 2017-06-28 15:47:29,456 main.py:51] epoch 13675, training loss: 9627.06, average training loss: 9184.11, base loss: 14442.26
[INFO 2017-06-28 15:47:30,113 main.py:51] epoch 13676, training loss: 10228.74, average training loss: 9185.76, base loss: 14444.41
[INFO 2017-06-28 15:47:30,779 main.py:51] epoch 13677, training loss: 9928.02, average training loss: 9186.16, base loss: 14445.17
[INFO 2017-06-28 15:47:31,449 main.py:51] epoch 13678, training loss: 8856.33, average training loss: 9185.69, base loss: 14445.25
[INFO 2017-06-28 15:47:32,100 main.py:51] epoch 13679, training loss: 8248.45, average training loss: 9184.75, base loss: 14441.97
[INFO 2017-06-28 15:47:32,760 main.py:51] epoch 13680, training loss: 9652.36, average training loss: 9186.33, base loss: 14445.36
[INFO 2017-06-28 15:47:33,440 main.py:51] epoch 13681, training loss: 8496.18, average training loss: 9186.64, base loss: 14444.62
[INFO 2017-06-28 15:47:34,102 main.py:51] epoch 13682, training loss: 8280.72, average training loss: 9185.38, base loss: 14441.75
[INFO 2017-06-28 15:47:34,762 main.py:51] epoch 13683, training loss: 9238.53, average training loss: 9186.24, base loss: 14445.10
[INFO 2017-06-28 15:47:35,426 main.py:51] epoch 13684, training loss: 8770.40, average training loss: 9184.74, base loss: 14443.45
[INFO 2017-06-28 15:47:36,067 main.py:51] epoch 13685, training loss: 10079.29, average training loss: 9184.65, base loss: 14444.03
[INFO 2017-06-28 15:47:36,725 main.py:51] epoch 13686, training loss: 9984.08, average training loss: 9185.07, base loss: 14445.56
[INFO 2017-06-28 15:47:37,368 main.py:51] epoch 13687, training loss: 9229.34, average training loss: 9184.20, base loss: 14445.47
[INFO 2017-06-28 15:47:38,044 main.py:51] epoch 13688, training loss: 8986.62, average training loss: 9182.75, base loss: 14443.25
[INFO 2017-06-28 15:47:38,714 main.py:51] epoch 13689, training loss: 8352.26, average training loss: 9182.40, base loss: 14442.61
[INFO 2017-06-28 15:47:39,372 main.py:51] epoch 13690, training loss: 8785.48, average training loss: 9182.27, base loss: 14442.85
[INFO 2017-06-28 15:47:40,030 main.py:51] epoch 13691, training loss: 9081.98, average training loss: 9181.19, base loss: 14440.88
[INFO 2017-06-28 15:47:40,682 main.py:51] epoch 13692, training loss: 8434.47, average training loss: 9180.44, base loss: 14441.90
[INFO 2017-06-28 15:47:41,321 main.py:51] epoch 13693, training loss: 9368.16, average training loss: 9179.59, base loss: 14440.15
[INFO 2017-06-28 15:47:41,998 main.py:51] epoch 13694, training loss: 8868.64, average training loss: 9180.09, base loss: 14441.40
[INFO 2017-06-28 15:47:42,664 main.py:51] epoch 13695, training loss: 8667.82, average training loss: 9178.93, base loss: 14439.87
[INFO 2017-06-28 15:47:43,310 main.py:51] epoch 13696, training loss: 8426.78, average training loss: 9179.43, base loss: 14441.84
[INFO 2017-06-28 15:47:43,966 main.py:51] epoch 13697, training loss: 8563.29, average training loss: 9179.79, base loss: 14442.06
[INFO 2017-06-28 15:47:44,630 main.py:51] epoch 13698, training loss: 8650.22, average training loss: 9180.30, base loss: 14443.75
[INFO 2017-06-28 15:47:45,281 main.py:51] epoch 13699, training loss: 8793.76, average training loss: 9179.48, base loss: 14442.41
[INFO 2017-06-28 15:47:45,281 main.py:53] epoch 13699, testing
[INFO 2017-06-28 15:47:47,908 main.py:105] average testing loss: 10156.25, base loss: 14549.95
[INFO 2017-06-28 15:47:47,908 main.py:106] improve_loss: 4393.70, improve_percent: 0.30
[INFO 2017-06-28 15:47:47,909 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:47:48,555 main.py:51] epoch 13700, training loss: 8866.44, average training loss: 9179.53, base loss: 14441.65
[INFO 2017-06-28 15:47:49,211 main.py:51] epoch 13701, training loss: 9509.54, average training loss: 9180.70, base loss: 14444.47
[INFO 2017-06-28 15:47:49,868 main.py:51] epoch 13702, training loss: 8725.27, average training loss: 9180.27, base loss: 14443.74
[INFO 2017-06-28 15:47:50,537 main.py:51] epoch 13703, training loss: 8671.12, average training loss: 9180.58, base loss: 14445.24
[INFO 2017-06-28 15:47:51,201 main.py:51] epoch 13704, training loss: 9094.37, average training loss: 9179.58, base loss: 14444.94
[INFO 2017-06-28 15:47:51,855 main.py:51] epoch 13705, training loss: 9403.51, average training loss: 9180.68, base loss: 14447.73
[INFO 2017-06-28 15:47:52,508 main.py:51] epoch 13706, training loss: 9222.77, average training loss: 9182.17, base loss: 14450.75
[INFO 2017-06-28 15:47:53,172 main.py:51] epoch 13707, training loss: 9245.53, average training loss: 9182.21, base loss: 14451.04
[INFO 2017-06-28 15:47:53,880 main.py:51] epoch 13708, training loss: 7982.05, average training loss: 9180.91, base loss: 14449.42
[INFO 2017-06-28 15:47:54,558 main.py:51] epoch 13709, training loss: 10049.05, average training loss: 9182.97, base loss: 14453.77
[INFO 2017-06-28 15:47:55,209 main.py:51] epoch 13710, training loss: 8894.33, average training loss: 9182.22, base loss: 14452.61
[INFO 2017-06-28 15:47:55,866 main.py:51] epoch 13711, training loss: 10000.33, average training loss: 9184.15, base loss: 14455.80
[INFO 2017-06-28 15:47:56,520 main.py:51] epoch 13712, training loss: 9534.03, average training loss: 9184.94, base loss: 14456.69
[INFO 2017-06-28 15:47:57,164 main.py:51] epoch 13713, training loss: 10181.93, average training loss: 9186.31, base loss: 14458.68
[INFO 2017-06-28 15:47:57,835 main.py:51] epoch 13714, training loss: 9341.11, average training loss: 9187.29, base loss: 14459.67
[INFO 2017-06-28 15:47:58,502 main.py:51] epoch 13715, training loss: 8027.58, average training loss: 9186.71, base loss: 14459.73
[INFO 2017-06-28 15:47:59,152 main.py:51] epoch 13716, training loss: 9187.07, average training loss: 9186.77, base loss: 14460.67
[INFO 2017-06-28 15:47:59,800 main.py:51] epoch 13717, training loss: 8174.19, average training loss: 9186.41, base loss: 14460.04
[INFO 2017-06-28 15:48:00,474 main.py:51] epoch 13718, training loss: 9410.10, average training loss: 9186.71, base loss: 14461.01
[INFO 2017-06-28 15:48:01,126 main.py:51] epoch 13719, training loss: 8141.69, average training loss: 9185.21, base loss: 14457.31
[INFO 2017-06-28 15:48:01,795 main.py:51] epoch 13720, training loss: 9998.34, average training loss: 9187.01, base loss: 14458.78
[INFO 2017-06-28 15:48:02,468 main.py:51] epoch 13721, training loss: 8811.26, average training loss: 9186.47, base loss: 14458.63
[INFO 2017-06-28 15:48:03,146 main.py:51] epoch 13722, training loss: 10427.22, average training loss: 9187.64, base loss: 14461.01
[INFO 2017-06-28 15:48:03,791 main.py:51] epoch 13723, training loss: 8405.11, average training loss: 9186.68, base loss: 14460.34
[INFO 2017-06-28 15:48:04,458 main.py:51] epoch 13724, training loss: 8371.99, average training loss: 9186.18, base loss: 14459.06
[INFO 2017-06-28 15:48:05,163 main.py:51] epoch 13725, training loss: 8015.19, average training loss: 9184.38, base loss: 14455.39
[INFO 2017-06-28 15:48:05,820 main.py:51] epoch 13726, training loss: 8539.48, average training loss: 9184.11, base loss: 14455.27
[INFO 2017-06-28 15:48:06,483 main.py:51] epoch 13727, training loss: 8879.07, average training loss: 9183.93, base loss: 14454.74
[INFO 2017-06-28 15:48:07,140 main.py:51] epoch 13728, training loss: 8667.92, average training loss: 9181.94, base loss: 14451.58
[INFO 2017-06-28 15:48:07,819 main.py:51] epoch 13729, training loss: 8539.26, average training loss: 9181.93, base loss: 14450.65
[INFO 2017-06-28 15:48:08,471 main.py:51] epoch 13730, training loss: 9233.14, average training loss: 9181.35, base loss: 14449.42
[INFO 2017-06-28 15:48:09,124 main.py:51] epoch 13731, training loss: 9062.75, average training loss: 9180.06, base loss: 14447.98
[INFO 2017-06-28 15:48:09,779 main.py:51] epoch 13732, training loss: 9078.36, average training loss: 9179.19, base loss: 14447.68
[INFO 2017-06-28 15:48:10,465 main.py:51] epoch 13733, training loss: 10467.66, average training loss: 9180.06, base loss: 14450.55
[INFO 2017-06-28 15:48:11,134 main.py:51] epoch 13734, training loss: 8882.84, average training loss: 9180.66, base loss: 14449.29
[INFO 2017-06-28 15:48:11,791 main.py:51] epoch 13735, training loss: 8716.11, average training loss: 9177.93, base loss: 14444.61
[INFO 2017-06-28 15:48:12,459 main.py:51] epoch 13736, training loss: 8910.12, average training loss: 9177.83, base loss: 14444.89
[INFO 2017-06-28 15:48:13,113 main.py:51] epoch 13737, training loss: 9097.48, average training loss: 9177.72, base loss: 14445.23
[INFO 2017-06-28 15:48:13,779 main.py:51] epoch 13738, training loss: 8235.10, average training loss: 9176.64, base loss: 14444.02
[INFO 2017-06-28 15:48:14,448 main.py:51] epoch 13739, training loss: 7773.53, average training loss: 9175.65, base loss: 14439.83
[INFO 2017-06-28 15:48:15,108 main.py:51] epoch 13740, training loss: 8376.30, average training loss: 9175.52, base loss: 14438.72
[INFO 2017-06-28 15:48:15,781 main.py:51] epoch 13741, training loss: 8413.90, average training loss: 9175.53, base loss: 14438.90
[INFO 2017-06-28 15:48:16,407 main.py:51] epoch 13742, training loss: 10105.06, average training loss: 9174.40, base loss: 14438.29
[INFO 2017-06-28 15:48:17,077 main.py:51] epoch 13743, training loss: 9463.38, average training loss: 9173.73, base loss: 14437.33
[INFO 2017-06-28 15:48:17,740 main.py:51] epoch 13744, training loss: 9002.58, average training loss: 9175.40, base loss: 14441.75
[INFO 2017-06-28 15:48:18,393 main.py:51] epoch 13745, training loss: 8726.27, average training loss: 9173.39, base loss: 14437.41
[INFO 2017-06-28 15:48:19,048 main.py:51] epoch 13746, training loss: 9467.60, average training loss: 9173.14, base loss: 14438.26
[INFO 2017-06-28 15:48:19,721 main.py:51] epoch 13747, training loss: 8731.37, average training loss: 9173.30, base loss: 14438.18
[INFO 2017-06-28 15:48:20,400 main.py:51] epoch 13748, training loss: 9407.54, average training loss: 9174.03, base loss: 14438.94
[INFO 2017-06-28 15:48:21,061 main.py:51] epoch 13749, training loss: 8970.74, average training loss: 9173.17, base loss: 14437.60
[INFO 2017-06-28 15:48:21,727 main.py:51] epoch 13750, training loss: 7736.41, average training loss: 9172.65, base loss: 14435.57
[INFO 2017-06-28 15:48:22,385 main.py:51] epoch 13751, training loss: 7948.04, average training loss: 9171.16, base loss: 14433.09
[INFO 2017-06-28 15:48:23,121 main.py:51] epoch 13752, training loss: 9351.86, average training loss: 9172.35, base loss: 14436.68
[INFO 2017-06-28 15:48:23,892 main.py:51] epoch 13753, training loss: 10088.00, average training loss: 9173.19, base loss: 14436.34
[INFO 2017-06-28 15:48:24,635 main.py:51] epoch 13754, training loss: 10326.83, average training loss: 9174.26, base loss: 14438.05
[INFO 2017-06-28 15:48:25,331 main.py:51] epoch 13755, training loss: 9182.08, average training loss: 9171.67, base loss: 14434.92
[INFO 2017-06-28 15:48:26,095 main.py:51] epoch 13756, training loss: 8691.32, average training loss: 9172.55, base loss: 14436.35
[INFO 2017-06-28 15:48:26,749 main.py:51] epoch 13757, training loss: 9480.82, average training loss: 9173.34, base loss: 14438.43
[INFO 2017-06-28 15:48:27,416 main.py:51] epoch 13758, training loss: 8654.79, average training loss: 9173.07, base loss: 14437.96
[INFO 2017-06-28 15:48:28,087 main.py:51] epoch 13759, training loss: 8848.37, average training loss: 9172.82, base loss: 14436.19
[INFO 2017-06-28 15:48:28,760 main.py:51] epoch 13760, training loss: 7439.15, average training loss: 9170.97, base loss: 14431.82
[INFO 2017-06-28 15:48:29,419 main.py:51] epoch 13761, training loss: 9107.81, average training loss: 9168.33, base loss: 14428.02
[INFO 2017-06-28 15:48:30,092 main.py:51] epoch 13762, training loss: 8843.47, average training loss: 9169.19, base loss: 14429.35
[INFO 2017-06-28 15:48:30,756 main.py:51] epoch 13763, training loss: 8984.72, average training loss: 9168.08, base loss: 14425.59
[INFO 2017-06-28 15:48:31,405 main.py:51] epoch 13764, training loss: 8407.99, average training loss: 9168.15, base loss: 14424.26
[INFO 2017-06-28 15:48:32,045 main.py:51] epoch 13765, training loss: 11182.63, average training loss: 9169.91, base loss: 14425.74
[INFO 2017-06-28 15:48:32,711 main.py:51] epoch 13766, training loss: 8780.27, average training loss: 9169.57, base loss: 14426.44
[INFO 2017-06-28 15:48:33,359 main.py:51] epoch 13767, training loss: 9715.64, average training loss: 9170.61, base loss: 14428.94
[INFO 2017-06-28 15:48:34,019 main.py:51] epoch 13768, training loss: 8671.14, average training loss: 9170.64, base loss: 14426.53
[INFO 2017-06-28 15:48:34,686 main.py:51] epoch 13769, training loss: 9318.10, average training loss: 9170.80, base loss: 14428.31
[INFO 2017-06-28 15:48:35,342 main.py:51] epoch 13770, training loss: 9766.87, average training loss: 9170.88, base loss: 14427.64
[INFO 2017-06-28 15:48:36,008 main.py:51] epoch 13771, training loss: 9846.06, average training loss: 9170.33, base loss: 14427.69
[INFO 2017-06-28 15:48:36,651 main.py:51] epoch 13772, training loss: 8629.39, average training loss: 9170.46, base loss: 14428.48
[INFO 2017-06-28 15:48:37,303 main.py:51] epoch 13773, training loss: 9557.13, average training loss: 9170.46, base loss: 14428.96
[INFO 2017-06-28 15:48:37,975 main.py:51] epoch 13774, training loss: 9315.40, average training loss: 9170.16, base loss: 14429.97
[INFO 2017-06-28 15:48:38,649 main.py:51] epoch 13775, training loss: 9548.72, average training loss: 9169.10, base loss: 14429.35
[INFO 2017-06-28 15:48:39,334 main.py:51] epoch 13776, training loss: 9001.24, average training loss: 9168.73, base loss: 14428.77
[INFO 2017-06-28 15:48:40,018 main.py:51] epoch 13777, training loss: 8782.00, average training loss: 9167.93, base loss: 14430.02
[INFO 2017-06-28 15:48:40,675 main.py:51] epoch 13778, training loss: 11239.33, average training loss: 9169.72, base loss: 14432.74
[INFO 2017-06-28 15:48:41,359 main.py:51] epoch 13779, training loss: 10647.57, average training loss: 9171.84, base loss: 14435.45
[INFO 2017-06-28 15:48:42,050 main.py:51] epoch 13780, training loss: 9087.43, average training loss: 9172.03, base loss: 14436.43
[INFO 2017-06-28 15:48:42,710 main.py:51] epoch 13781, training loss: 8915.05, average training loss: 9171.51, base loss: 14436.46
[INFO 2017-06-28 15:48:43,467 main.py:51] epoch 13782, training loss: 8415.01, average training loss: 9169.90, base loss: 14432.49
[INFO 2017-06-28 15:48:44,283 main.py:51] epoch 13783, training loss: 9654.70, average training loss: 9171.53, base loss: 14435.15
[INFO 2017-06-28 15:48:45,015 main.py:51] epoch 13784, training loss: 9481.30, average training loss: 9172.48, base loss: 14436.19
[INFO 2017-06-28 15:48:45,789 main.py:51] epoch 13785, training loss: 8461.12, average training loss: 9173.59, base loss: 14438.58
[INFO 2017-06-28 15:48:46,456 main.py:51] epoch 13786, training loss: 8475.34, average training loss: 9173.27, base loss: 14437.66
[INFO 2017-06-28 15:48:47,109 main.py:51] epoch 13787, training loss: 9737.18, average training loss: 9174.28, base loss: 14438.56
[INFO 2017-06-28 15:48:47,757 main.py:51] epoch 13788, training loss: 9254.35, average training loss: 9173.84, base loss: 14437.64
[INFO 2017-06-28 15:48:48,416 main.py:51] epoch 13789, training loss: 8979.97, average training loss: 9174.21, base loss: 14437.57
[INFO 2017-06-28 15:48:49,071 main.py:51] epoch 13790, training loss: 11977.06, average training loss: 9176.44, base loss: 14440.71
[INFO 2017-06-28 15:48:49,722 main.py:51] epoch 13791, training loss: 9508.32, average training loss: 9177.72, base loss: 14442.82
[INFO 2017-06-28 15:48:50,393 main.py:51] epoch 13792, training loss: 10106.86, average training loss: 9177.50, base loss: 14442.19
[INFO 2017-06-28 15:48:51,055 main.py:51] epoch 13793, training loss: 8736.86, average training loss: 9176.58, base loss: 14440.43
[INFO 2017-06-28 15:48:51,712 main.py:51] epoch 13794, training loss: 9326.82, average training loss: 9177.20, base loss: 14441.64
[INFO 2017-06-28 15:48:52,355 main.py:51] epoch 13795, training loss: 7887.36, average training loss: 9176.26, base loss: 14439.52
[INFO 2017-06-28 15:48:52,995 main.py:51] epoch 13796, training loss: 8744.19, average training loss: 9176.23, base loss: 14438.98
[INFO 2017-06-28 15:48:53,656 main.py:51] epoch 13797, training loss: 8438.31, average training loss: 9176.08, base loss: 14438.31
[INFO 2017-06-28 15:48:54,349 main.py:51] epoch 13798, training loss: 9001.57, average training loss: 9176.01, base loss: 14437.99
[INFO 2017-06-28 15:48:55,028 main.py:51] epoch 13799, training loss: 9907.50, average training loss: 9177.99, base loss: 14440.82
[INFO 2017-06-28 15:48:55,028 main.py:53] epoch 13799, testing
[INFO 2017-06-28 15:48:57,575 main.py:105] average testing loss: 10377.95, base loss: 14735.86
[INFO 2017-06-28 15:48:57,575 main.py:106] improve_loss: 4357.92, improve_percent: 0.30
[INFO 2017-06-28 15:48:57,576 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:48:58,242 main.py:51] epoch 13800, training loss: 8946.82, average training loss: 9178.64, base loss: 14441.98
[INFO 2017-06-28 15:48:58,904 main.py:51] epoch 13801, training loss: 10081.18, average training loss: 9180.73, base loss: 14445.88
[INFO 2017-06-28 15:48:59,564 main.py:51] epoch 13802, training loss: 9523.82, average training loss: 9181.49, base loss: 14447.09
[INFO 2017-06-28 15:49:00,219 main.py:51] epoch 13803, training loss: 8154.38, average training loss: 9180.81, base loss: 14448.16
[INFO 2017-06-28 15:49:00,863 main.py:51] epoch 13804, training loss: 8839.34, average training loss: 9180.68, base loss: 14448.67
[INFO 2017-06-28 15:49:01,534 main.py:51] epoch 13805, training loss: 9029.65, average training loss: 9181.06, base loss: 14450.49
[INFO 2017-06-28 15:49:02,208 main.py:51] epoch 13806, training loss: 8898.29, average training loss: 9180.77, base loss: 14449.11
[INFO 2017-06-28 15:49:02,858 main.py:51] epoch 13807, training loss: 8645.69, average training loss: 9179.54, base loss: 14445.62
[INFO 2017-06-28 15:49:03,533 main.py:51] epoch 13808, training loss: 9487.94, average training loss: 9179.91, base loss: 14446.63
[INFO 2017-06-28 15:49:04,204 main.py:51] epoch 13809, training loss: 8969.74, average training loss: 9180.76, base loss: 14446.94
[INFO 2017-06-28 15:49:04,868 main.py:51] epoch 13810, training loss: 9369.21, average training loss: 9181.29, base loss: 14448.27
[INFO 2017-06-28 15:49:05,528 main.py:51] epoch 13811, training loss: 8582.66, average training loss: 9180.44, base loss: 14446.22
[INFO 2017-06-28 15:49:06,168 main.py:51] epoch 13812, training loss: 8598.71, average training loss: 9178.44, base loss: 14443.21
[INFO 2017-06-28 15:49:06,838 main.py:51] epoch 13813, training loss: 8457.47, average training loss: 9177.90, base loss: 14442.57
[INFO 2017-06-28 15:49:07,486 main.py:51] epoch 13814, training loss: 9267.22, average training loss: 9177.48, base loss: 14442.12
[INFO 2017-06-28 15:49:08,166 main.py:51] epoch 13815, training loss: 9387.49, average training loss: 9178.10, base loss: 14444.10
[INFO 2017-06-28 15:49:08,816 main.py:51] epoch 13816, training loss: 9615.00, average training loss: 9179.57, base loss: 14446.35
[INFO 2017-06-28 15:49:09,482 main.py:51] epoch 13817, training loss: 8093.13, average training loss: 9176.18, base loss: 14440.81
[INFO 2017-06-28 15:49:10,185 main.py:51] epoch 13818, training loss: 9367.83, average training loss: 9176.26, base loss: 14439.62
[INFO 2017-06-28 15:49:10,847 main.py:51] epoch 13819, training loss: 9086.75, average training loss: 9175.05, base loss: 14436.73
[INFO 2017-06-28 15:49:11,524 main.py:51] epoch 13820, training loss: 8381.28, average training loss: 9174.91, base loss: 14436.87
[INFO 2017-06-28 15:49:12,187 main.py:51] epoch 13821, training loss: 8401.18, average training loss: 9175.07, base loss: 14438.33
[INFO 2017-06-28 15:49:12,867 main.py:51] epoch 13822, training loss: 8197.46, average training loss: 9173.66, base loss: 14435.90
[INFO 2017-06-28 15:49:13,522 main.py:51] epoch 13823, training loss: 8458.75, average training loss: 9173.67, base loss: 14435.93
[INFO 2017-06-28 15:49:14,168 main.py:51] epoch 13824, training loss: 8211.94, average training loss: 9172.42, base loss: 14433.50
[INFO 2017-06-28 15:49:14,828 main.py:51] epoch 13825, training loss: 9146.43, average training loss: 9171.18, base loss: 14432.24
[INFO 2017-06-28 15:49:15,497 main.py:51] epoch 13826, training loss: 8982.47, average training loss: 9172.05, base loss: 14433.03
[INFO 2017-06-28 15:49:16,174 main.py:51] epoch 13827, training loss: 9421.03, average training loss: 9173.21, base loss: 14435.00
[INFO 2017-06-28 15:49:16,850 main.py:51] epoch 13828, training loss: 8218.87, average training loss: 9172.81, base loss: 14433.90
[INFO 2017-06-28 15:49:17,506 main.py:51] epoch 13829, training loss: 9265.06, average training loss: 9172.69, base loss: 14433.90
[INFO 2017-06-28 15:49:18,184 main.py:51] epoch 13830, training loss: 9173.65, average training loss: 9173.25, base loss: 14435.40
[INFO 2017-06-28 15:49:18,872 main.py:51] epoch 13831, training loss: 8034.61, average training loss: 9171.70, base loss: 14432.95
[INFO 2017-06-28 15:49:19,529 main.py:51] epoch 13832, training loss: 9040.21, average training loss: 9171.48, base loss: 14433.31
[INFO 2017-06-28 15:49:20,171 main.py:51] epoch 13833, training loss: 9249.88, average training loss: 9172.95, base loss: 14435.79
[INFO 2017-06-28 15:49:20,824 main.py:51] epoch 13834, training loss: 9190.34, average training loss: 9173.32, base loss: 14437.39
[INFO 2017-06-28 15:49:21,472 main.py:51] epoch 13835, training loss: 9442.33, average training loss: 9172.64, base loss: 14435.57
[INFO 2017-06-28 15:49:22,144 main.py:51] epoch 13836, training loss: 9479.29, average training loss: 9173.05, base loss: 14438.17
[INFO 2017-06-28 15:49:22,778 main.py:51] epoch 13837, training loss: 10954.71, average training loss: 9174.81, base loss: 14440.55
[INFO 2017-06-28 15:49:23,420 main.py:51] epoch 13838, training loss: 8955.72, average training loss: 9173.16, base loss: 14436.70
[INFO 2017-06-28 15:49:24,102 main.py:51] epoch 13839, training loss: 9598.04, average training loss: 9173.36, base loss: 14438.68
[INFO 2017-06-28 15:49:24,754 main.py:51] epoch 13840, training loss: 10243.60, average training loss: 9174.26, base loss: 14439.57
[INFO 2017-06-28 15:49:25,404 main.py:51] epoch 13841, training loss: 9478.23, average training loss: 9173.88, base loss: 14437.91
[INFO 2017-06-28 15:49:26,063 main.py:51] epoch 13842, training loss: 9105.74, average training loss: 9172.91, base loss: 14435.85
[INFO 2017-06-28 15:49:26,727 main.py:51] epoch 13843, training loss: 10261.12, average training loss: 9173.79, base loss: 14436.29
[INFO 2017-06-28 15:49:27,407 main.py:51] epoch 13844, training loss: 8771.39, average training loss: 9172.60, base loss: 14434.33
[INFO 2017-06-28 15:49:28,089 main.py:51] epoch 13845, training loss: 9694.31, average training loss: 9171.04, base loss: 14431.77
[INFO 2017-06-28 15:49:28,748 main.py:51] epoch 13846, training loss: 8891.14, average training loss: 9168.76, base loss: 14428.80
[INFO 2017-06-28 15:49:29,404 main.py:51] epoch 13847, training loss: 8581.54, average training loss: 9168.07, base loss: 14428.16
[INFO 2017-06-28 15:49:30,074 main.py:51] epoch 13848, training loss: 9157.92, average training loss: 9167.37, base loss: 14425.40
[INFO 2017-06-28 15:49:30,750 main.py:51] epoch 13849, training loss: 10025.87, average training loss: 9168.67, base loss: 14427.62
[INFO 2017-06-28 15:49:31,451 main.py:51] epoch 13850, training loss: 8519.70, average training loss: 9167.95, base loss: 14426.56
[INFO 2017-06-28 15:49:32,209 main.py:51] epoch 13851, training loss: 8333.48, average training loss: 9167.01, base loss: 14425.28
[INFO 2017-06-28 15:49:32,988 main.py:51] epoch 13852, training loss: 7129.08, average training loss: 9164.96, base loss: 14421.26
[INFO 2017-06-28 15:49:33,689 main.py:51] epoch 13853, training loss: 8815.37, average training loss: 9164.96, base loss: 14420.16
[INFO 2017-06-28 15:49:34,453 main.py:51] epoch 13854, training loss: 8806.61, average training loss: 9164.87, base loss: 14420.16
[INFO 2017-06-28 15:49:35,088 main.py:51] epoch 13855, training loss: 10261.14, average training loss: 9166.35, base loss: 14422.83
[INFO 2017-06-28 15:49:35,744 main.py:51] epoch 13856, training loss: 9134.30, average training loss: 9165.72, base loss: 14423.02
[INFO 2017-06-28 15:49:36,392 main.py:51] epoch 13857, training loss: 8048.69, average training loss: 9164.35, base loss: 14420.74
[INFO 2017-06-28 15:49:37,087 main.py:51] epoch 13858, training loss: 9795.78, average training loss: 9166.02, base loss: 14421.87
[INFO 2017-06-28 15:49:37,738 main.py:51] epoch 13859, training loss: 9385.95, average training loss: 9166.23, base loss: 14421.45
[INFO 2017-06-28 15:49:38,377 main.py:51] epoch 13860, training loss: 9895.17, average training loss: 9165.93, base loss: 14421.93
[INFO 2017-06-28 15:49:39,017 main.py:51] epoch 13861, training loss: 9168.09, average training loss: 9165.70, base loss: 14421.12
[INFO 2017-06-28 15:49:39,685 main.py:51] epoch 13862, training loss: 7895.89, average training loss: 9164.17, base loss: 14418.97
[INFO 2017-06-28 15:49:40,344 main.py:51] epoch 13863, training loss: 8273.81, average training loss: 9161.90, base loss: 14415.03
[INFO 2017-06-28 15:49:40,998 main.py:51] epoch 13864, training loss: 9547.14, average training loss: 9162.59, base loss: 14415.68
[INFO 2017-06-28 15:49:41,671 main.py:51] epoch 13865, training loss: 8671.50, average training loss: 9161.89, base loss: 14416.26
[INFO 2017-06-28 15:49:42,322 main.py:51] epoch 13866, training loss: 9395.39, average training loss: 9160.36, base loss: 14414.22
[INFO 2017-06-28 15:49:42,968 main.py:51] epoch 13867, training loss: 10375.74, average training loss: 9161.01, base loss: 14414.49
[INFO 2017-06-28 15:49:43,641 main.py:51] epoch 13868, training loss: 9477.94, average training loss: 9161.64, base loss: 14416.77
[INFO 2017-06-28 15:49:44,280 main.py:51] epoch 13869, training loss: 8537.09, average training loss: 9161.01, base loss: 14416.92
[INFO 2017-06-28 15:49:44,944 main.py:51] epoch 13870, training loss: 8996.02, average training loss: 9161.05, base loss: 14415.16
[INFO 2017-06-28 15:49:45,606 main.py:51] epoch 13871, training loss: 9520.97, average training loss: 9160.71, base loss: 14415.50
[INFO 2017-06-28 15:49:46,260 main.py:51] epoch 13872, training loss: 8368.72, average training loss: 9159.73, base loss: 14413.02
[INFO 2017-06-28 15:49:46,908 main.py:51] epoch 13873, training loss: 9943.44, average training loss: 9161.96, base loss: 14419.09
[INFO 2017-06-28 15:49:47,582 main.py:51] epoch 13874, training loss: 8774.24, average training loss: 9161.28, base loss: 14419.33
[INFO 2017-06-28 15:49:48,259 main.py:51] epoch 13875, training loss: 8820.13, average training loss: 9161.13, base loss: 14419.34
[INFO 2017-06-28 15:49:48,927 main.py:51] epoch 13876, training loss: 9183.68, average training loss: 9161.87, base loss: 14420.37
[INFO 2017-06-28 15:49:49,583 main.py:51] epoch 13877, training loss: 9046.83, average training loss: 9161.65, base loss: 14420.86
[INFO 2017-06-28 15:49:50,255 main.py:51] epoch 13878, training loss: 8615.19, average training loss: 9160.40, base loss: 14419.50
[INFO 2017-06-28 15:49:50,923 main.py:51] epoch 13879, training loss: 9126.39, average training loss: 9159.80, base loss: 14418.85
[INFO 2017-06-28 15:49:51,587 main.py:51] epoch 13880, training loss: 10290.00, average training loss: 9161.68, base loss: 14421.71
[INFO 2017-06-28 15:49:52,250 main.py:51] epoch 13881, training loss: 8689.06, average training loss: 9161.68, base loss: 14421.82
[INFO 2017-06-28 15:49:52,914 main.py:51] epoch 13882, training loss: 9961.16, average training loss: 9162.51, base loss: 14423.77
[INFO 2017-06-28 15:49:53,592 main.py:51] epoch 13883, training loss: 8812.75, average training loss: 9162.77, base loss: 14425.74
[INFO 2017-06-28 15:49:54,289 main.py:51] epoch 13884, training loss: 9212.76, average training loss: 9162.80, base loss: 14426.31
[INFO 2017-06-28 15:49:55,042 main.py:51] epoch 13885, training loss: 8979.55, average training loss: 9162.33, base loss: 14425.45
[INFO 2017-06-28 15:49:55,798 main.py:51] epoch 13886, training loss: 7958.35, average training loss: 9161.29, base loss: 14423.09
[INFO 2017-06-28 15:49:56,535 main.py:51] epoch 13887, training loss: 9116.11, average training loss: 9163.23, base loss: 14424.20
[INFO 2017-06-28 15:49:57,303 main.py:51] epoch 13888, training loss: 10386.21, average training loss: 9164.26, base loss: 14425.36
[INFO 2017-06-28 15:49:58,471 main.py:51] epoch 13889, training loss: 9062.09, average training loss: 9164.11, base loss: 14423.76
[INFO 2017-06-28 15:49:59,406 main.py:51] epoch 13890, training loss: 11315.25, average training loss: 9164.90, base loss: 14424.19
[INFO 2017-06-28 15:50:00,518 main.py:51] epoch 13891, training loss: 9544.58, average training loss: 9164.10, base loss: 14424.49
[INFO 2017-06-28 15:50:01,723 main.py:51] epoch 13892, training loss: 10573.13, average training loss: 9166.44, base loss: 14427.76
[INFO 2017-06-28 15:50:02,921 main.py:51] epoch 13893, training loss: 9272.08, average training loss: 9165.57, base loss: 14428.06
[INFO 2017-06-28 15:50:04,120 main.py:51] epoch 13894, training loss: 7876.56, average training loss: 9163.36, base loss: 14424.41
[INFO 2017-06-28 15:50:05,341 main.py:51] epoch 13895, training loss: 9163.53, average training loss: 9164.64, base loss: 14425.90
[INFO 2017-06-28 15:50:06,535 main.py:51] epoch 13896, training loss: 9793.27, average training loss: 9165.93, base loss: 14428.04
[INFO 2017-06-28 15:50:07,740 main.py:51] epoch 13897, training loss: 7455.01, average training loss: 9165.36, base loss: 14426.41
[INFO 2017-06-28 15:50:08,970 main.py:51] epoch 13898, training loss: 7883.83, average training loss: 9164.22, base loss: 14424.26
[INFO 2017-06-28 15:50:10,186 main.py:51] epoch 13899, training loss: 8574.26, average training loss: 9162.94, base loss: 14421.46
[INFO 2017-06-28 15:50:10,187 main.py:53] epoch 13899, testing
[INFO 2017-06-28 15:50:14,274 main.py:105] average testing loss: 9987.09, base loss: 14246.92
[INFO 2017-06-28 15:50:14,274 main.py:106] improve_loss: 4259.84, improve_percent: 0.30
[INFO 2017-06-28 15:50:14,275 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:50:15,460 main.py:51] epoch 13900, training loss: 9917.96, average training loss: 9162.83, base loss: 14421.28
[INFO 2017-06-28 15:50:16,590 main.py:51] epoch 13901, training loss: 8923.73, average training loss: 9161.77, base loss: 14421.65
[INFO 2017-06-28 15:50:17,665 main.py:51] epoch 13902, training loss: 8323.38, average training loss: 9160.21, base loss: 14416.81
[INFO 2017-06-28 15:50:18,745 main.py:51] epoch 13903, training loss: 9153.38, average training loss: 9159.06, base loss: 14414.93
[INFO 2017-06-28 15:50:19,945 main.py:51] epoch 13904, training loss: 7856.87, average training loss: 9156.43, base loss: 14410.05
[INFO 2017-06-28 15:50:21,100 main.py:51] epoch 13905, training loss: 8704.14, average training loss: 9156.28, base loss: 14408.05
[INFO 2017-06-28 15:50:22,268 main.py:51] epoch 13906, training loss: 8396.97, average training loss: 9155.11, base loss: 14406.85
[INFO 2017-06-28 15:50:23,438 main.py:51] epoch 13907, training loss: 9111.77, average training loss: 9154.50, base loss: 14406.05
[INFO 2017-06-28 15:50:24,669 main.py:51] epoch 13908, training loss: 9488.04, average training loss: 9154.03, base loss: 14404.52
[INFO 2017-06-28 15:50:25,831 main.py:51] epoch 13909, training loss: 8698.36, average training loss: 9154.01, base loss: 14404.26
[INFO 2017-06-28 15:50:26,948 main.py:51] epoch 13910, training loss: 8343.40, average training loss: 9152.68, base loss: 14402.12
[INFO 2017-06-28 15:50:28,008 main.py:51] epoch 13911, training loss: 11092.05, average training loss: 9154.51, base loss: 14404.01
[INFO 2017-06-28 15:50:29,135 main.py:51] epoch 13912, training loss: 9117.77, average training loss: 9154.57, base loss: 14403.77
[INFO 2017-06-28 15:50:30,335 main.py:51] epoch 13913, training loss: 9179.30, average training loss: 9154.56, base loss: 14404.17
[INFO 2017-06-28 15:50:31,545 main.py:51] epoch 13914, training loss: 9960.35, average training loss: 9155.04, base loss: 14404.91
[INFO 2017-06-28 15:50:32,736 main.py:51] epoch 13915, training loss: 10011.00, average training loss: 9155.14, base loss: 14405.75
[INFO 2017-06-28 15:50:33,954 main.py:51] epoch 13916, training loss: 10280.82, average training loss: 9156.71, base loss: 14408.69
[INFO 2017-06-28 15:50:35,163 main.py:51] epoch 13917, training loss: 9079.25, average training loss: 9156.66, base loss: 14408.08
[INFO 2017-06-28 15:50:36,352 main.py:51] epoch 13918, training loss: 8620.90, average training loss: 9155.03, base loss: 14406.74
[INFO 2017-06-28 15:50:37,554 main.py:51] epoch 13919, training loss: 9276.23, average training loss: 9155.33, base loss: 14405.83
[INFO 2017-06-28 15:50:38,735 main.py:51] epoch 13920, training loss: 9632.11, average training loss: 9153.89, base loss: 14403.42
[INFO 2017-06-28 15:50:39,952 main.py:51] epoch 13921, training loss: 9856.15, average training loss: 9154.68, base loss: 14405.09
[INFO 2017-06-28 15:50:41,124 main.py:51] epoch 13922, training loss: 8201.74, average training loss: 9153.72, base loss: 14404.40
[INFO 2017-06-28 15:50:42,273 main.py:51] epoch 13923, training loss: 9516.66, average training loss: 9155.35, base loss: 14406.44
[INFO 2017-06-28 15:50:43,475 main.py:51] epoch 13924, training loss: 8599.31, average training loss: 9156.34, base loss: 14409.22
[INFO 2017-06-28 15:50:44,678 main.py:51] epoch 13925, training loss: 9823.36, average training loss: 9155.73, base loss: 14407.59
[INFO 2017-06-28 15:50:45,831 main.py:51] epoch 13926, training loss: 8506.46, average training loss: 9153.87, base loss: 14405.31
[INFO 2017-06-28 15:50:46,883 main.py:51] epoch 13927, training loss: 9315.92, average training loss: 9152.13, base loss: 14402.54
[INFO 2017-06-28 15:50:47,975 main.py:51] epoch 13928, training loss: 8829.56, average training loss: 9152.19, base loss: 14404.31
[INFO 2017-06-28 15:50:49,134 main.py:51] epoch 13929, training loss: 8949.61, average training loss: 9150.85, base loss: 14402.80
[INFO 2017-06-28 15:50:50,340 main.py:51] epoch 13930, training loss: 8431.45, average training loss: 9150.51, base loss: 14402.31
[INFO 2017-06-28 15:50:51,549 main.py:51] epoch 13931, training loss: 8734.14, average training loss: 9149.93, base loss: 14400.82
[INFO 2017-06-28 15:50:52,765 main.py:51] epoch 13932, training loss: 8789.55, average training loss: 9149.74, base loss: 14399.92
[INFO 2017-06-28 15:50:53,969 main.py:51] epoch 13933, training loss: 8514.50, average training loss: 9149.09, base loss: 14399.30
[INFO 2017-06-28 15:50:55,174 main.py:51] epoch 13934, training loss: 9569.20, average training loss: 9150.21, base loss: 14400.65
[INFO 2017-06-28 15:50:56,353 main.py:51] epoch 13935, training loss: 9249.60, average training loss: 9151.50, base loss: 14403.06
[INFO 2017-06-28 15:50:57,556 main.py:51] epoch 13936, training loss: 9331.37, average training loss: 9151.44, base loss: 14402.20
[INFO 2017-06-28 15:50:58,779 main.py:51] epoch 13937, training loss: 10753.06, average training loss: 9153.69, base loss: 14405.22
[INFO 2017-06-28 15:50:59,977 main.py:51] epoch 13938, training loss: 8205.66, average training loss: 9153.15, base loss: 14404.41
[INFO 2017-06-28 15:51:01,171 main.py:51] epoch 13939, training loss: 9077.29, average training loss: 9154.00, base loss: 14407.73
[INFO 2017-06-28 15:51:02,390 main.py:51] epoch 13940, training loss: 9221.04, average training loss: 9154.11, base loss: 14407.68
[INFO 2017-06-28 15:51:03,581 main.py:51] epoch 13941, training loss: 8710.10, average training loss: 9152.74, base loss: 14406.68
[INFO 2017-06-28 15:51:04,761 main.py:51] epoch 13942, training loss: 8716.46, average training loss: 9152.30, base loss: 14407.40
[INFO 2017-06-28 15:51:05,892 main.py:51] epoch 13943, training loss: 8151.66, average training loss: 9150.42, base loss: 14404.28
[INFO 2017-06-28 15:51:07,055 main.py:51] epoch 13944, training loss: 9102.63, average training loss: 9150.45, base loss: 14404.24
[INFO 2017-06-28 15:51:08,241 main.py:51] epoch 13945, training loss: 7587.28, average training loss: 9148.87, base loss: 14401.59
[INFO 2017-06-28 15:51:09,351 main.py:51] epoch 13946, training loss: 8712.10, average training loss: 9148.36, base loss: 14400.04
[INFO 2017-06-28 15:51:10,426 main.py:51] epoch 13947, training loss: 9189.01, average training loss: 9147.98, base loss: 14397.86
[INFO 2017-06-28 15:51:11,547 main.py:51] epoch 13948, training loss: 8338.58, average training loss: 9147.35, base loss: 14397.29
[INFO 2017-06-28 15:51:12,736 main.py:51] epoch 13949, training loss: 8406.92, average training loss: 9147.17, base loss: 14397.30
[INFO 2017-06-28 15:51:13,955 main.py:51] epoch 13950, training loss: 7766.08, average training loss: 9146.52, base loss: 14395.77
[INFO 2017-06-28 15:51:15,174 main.py:51] epoch 13951, training loss: 8742.72, average training loss: 9146.22, base loss: 14396.12
[INFO 2017-06-28 15:51:16,389 main.py:51] epoch 13952, training loss: 7782.79, average training loss: 9145.00, base loss: 14392.35
[INFO 2017-06-28 15:51:17,595 main.py:51] epoch 13953, training loss: 8280.26, average training loss: 9142.41, base loss: 14387.13
[INFO 2017-06-28 15:51:18,804 main.py:51] epoch 13954, training loss: 7894.76, average training loss: 9141.05, base loss: 14384.63
[INFO 2017-06-28 15:51:20,005 main.py:51] epoch 13955, training loss: 10431.36, average training loss: 9142.38, base loss: 14387.58
[INFO 2017-06-28 15:51:21,219 main.py:51] epoch 13956, training loss: 9834.83, average training loss: 9143.54, base loss: 14390.59
[INFO 2017-06-28 15:51:22,393 main.py:51] epoch 13957, training loss: 8854.37, average training loss: 9142.69, base loss: 14388.60
[INFO 2017-06-28 15:51:23,624 main.py:51] epoch 13958, training loss: 8969.39, average training loss: 9142.10, base loss: 14386.44
[INFO 2017-06-28 15:51:24,843 main.py:51] epoch 13959, training loss: 9080.07, average training loss: 9141.82, base loss: 14386.44
[INFO 2017-06-28 15:51:26,053 main.py:51] epoch 13960, training loss: 9044.60, average training loss: 9142.74, base loss: 14387.16
[INFO 2017-06-28 15:51:27,270 main.py:51] epoch 13961, training loss: 9997.89, average training loss: 9143.47, base loss: 14388.70
[INFO 2017-06-28 15:51:28,483 main.py:51] epoch 13962, training loss: 8061.96, average training loss: 9142.50, base loss: 14388.57
[INFO 2017-06-28 15:51:29,692 main.py:51] epoch 13963, training loss: 9506.13, average training loss: 9142.73, base loss: 14389.46
[INFO 2017-06-28 15:51:30,897 main.py:51] epoch 13964, training loss: 9363.41, average training loss: 9143.28, base loss: 14392.33
[INFO 2017-06-28 15:51:32,095 main.py:51] epoch 13965, training loss: 10735.96, average training loss: 9145.01, base loss: 14394.48
[INFO 2017-06-28 15:51:33,317 main.py:51] epoch 13966, training loss: 8870.56, average training loss: 9144.43, base loss: 14392.43
[INFO 2017-06-28 15:51:34,540 main.py:51] epoch 13967, training loss: 9951.18, average training loss: 9144.91, base loss: 14392.89
[INFO 2017-06-28 15:51:35,724 main.py:51] epoch 13968, training loss: 8028.01, average training loss: 9143.69, base loss: 14390.74
[INFO 2017-06-28 15:51:36,934 main.py:51] epoch 13969, training loss: 9944.77, average training loss: 9145.16, base loss: 14393.99
[INFO 2017-06-28 15:51:38,123 main.py:51] epoch 13970, training loss: 10012.24, average training loss: 9147.18, base loss: 14397.88
[INFO 2017-06-28 15:51:39,276 main.py:51] epoch 13971, training loss: 8829.07, average training loss: 9147.63, base loss: 14399.63
[INFO 2017-06-28 15:51:40,409 main.py:51] epoch 13972, training loss: 8892.62, average training loss: 9148.08, base loss: 14400.47
[INFO 2017-06-28 15:51:41,587 main.py:51] epoch 13973, training loss: 8292.50, average training loss: 9147.33, base loss: 14399.72
[INFO 2017-06-28 15:51:42,744 main.py:51] epoch 13974, training loss: 8630.93, average training loss: 9147.11, base loss: 14398.99
[INFO 2017-06-28 15:51:43,774 main.py:51] epoch 13975, training loss: 8770.58, average training loss: 9146.81, base loss: 14397.85
[INFO 2017-06-28 15:51:44,901 main.py:51] epoch 13976, training loss: 9439.53, average training loss: 9146.66, base loss: 14399.24
[INFO 2017-06-28 15:51:46,055 main.py:51] epoch 13977, training loss: 9925.25, average training loss: 9147.64, base loss: 14400.09
[INFO 2017-06-28 15:51:47,250 main.py:51] epoch 13978, training loss: 9936.46, average training loss: 9149.89, base loss: 14403.00
[INFO 2017-06-28 15:51:48,464 main.py:51] epoch 13979, training loss: 8924.91, average training loss: 9150.58, base loss: 14404.66
[INFO 2017-06-28 15:51:49,671 main.py:51] epoch 13980, training loss: 9074.17, average training loss: 9150.05, base loss: 14404.86
[INFO 2017-06-28 15:51:50,892 main.py:51] epoch 13981, training loss: 8322.23, average training loss: 9148.79, base loss: 14401.54
[INFO 2017-06-28 15:51:52,098 main.py:51] epoch 13982, training loss: 9939.83, average training loss: 9149.56, base loss: 14405.13
[INFO 2017-06-28 15:51:53,320 main.py:51] epoch 13983, training loss: 9908.96, average training loss: 9149.47, base loss: 14404.56
[INFO 2017-06-28 15:51:54,514 main.py:51] epoch 13984, training loss: 11843.39, average training loss: 9153.17, base loss: 14410.28
[INFO 2017-06-28 15:51:55,730 main.py:51] epoch 13985, training loss: 8288.19, average training loss: 9151.88, base loss: 14407.30
[INFO 2017-06-28 15:51:56,938 main.py:51] epoch 13986, training loss: 10640.30, average training loss: 9154.08, base loss: 14410.35
[INFO 2017-06-28 15:51:58,127 main.py:51] epoch 13987, training loss: 7756.30, average training loss: 9152.75, base loss: 14407.57
[INFO 2017-06-28 15:51:59,264 main.py:51] epoch 13988, training loss: 8086.99, average training loss: 9151.95, base loss: 14407.88
[INFO 2017-06-28 15:52:00,453 main.py:51] epoch 13989, training loss: 9556.05, average training loss: 9153.29, base loss: 14409.95
[INFO 2017-06-28 15:52:01,620 main.py:51] epoch 13990, training loss: 8851.24, average training loss: 9153.98, base loss: 14411.54
[INFO 2017-06-28 15:52:02,466 main.py:51] epoch 13991, training loss: 7968.95, average training loss: 9153.77, base loss: 14410.55
[INFO 2017-06-28 15:52:03,121 main.py:51] epoch 13992, training loss: 8336.64, average training loss: 9152.30, base loss: 14407.82
[INFO 2017-06-28 15:52:03,774 main.py:51] epoch 13993, training loss: 9258.96, average training loss: 9152.93, base loss: 14407.74
[INFO 2017-06-28 15:52:04,431 main.py:51] epoch 13994, training loss: 9136.34, average training loss: 9152.90, base loss: 14408.82
[INFO 2017-06-28 15:52:05,084 main.py:51] epoch 13995, training loss: 10226.59, average training loss: 9154.68, base loss: 14412.59
[INFO 2017-06-28 15:52:05,731 main.py:51] epoch 13996, training loss: 8476.52, average training loss: 9153.58, base loss: 14409.18
[INFO 2017-06-28 15:52:06,382 main.py:51] epoch 13997, training loss: 8982.24, average training loss: 9153.18, base loss: 14408.19
[INFO 2017-06-28 15:52:07,038 main.py:51] epoch 13998, training loss: 8679.22, average training loss: 9151.48, base loss: 14404.86
[INFO 2017-06-28 15:52:07,691 main.py:51] epoch 13999, training loss: 10877.55, average training loss: 9152.84, base loss: 14406.93
[INFO 2017-06-28 15:52:07,691 main.py:53] epoch 13999, testing
[INFO 2017-06-28 15:52:10,279 main.py:105] average testing loss: 10230.43, base loss: 14500.77
[INFO 2017-06-28 15:52:10,279 main.py:106] improve_loss: 4270.34, improve_percent: 0.29
[INFO 2017-06-28 15:52:10,280 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:52:10,964 main.py:51] epoch 14000, training loss: 8093.56, average training loss: 9152.22, base loss: 14407.11
[INFO 2017-06-28 15:52:11,649 main.py:51] epoch 14001, training loss: 9329.07, average training loss: 9152.96, base loss: 14408.66
[INFO 2017-06-28 15:52:12,323 main.py:51] epoch 14002, training loss: 8724.62, average training loss: 9152.95, base loss: 14409.34
[INFO 2017-06-28 15:52:12,973 main.py:51] epoch 14003, training loss: 9353.01, average training loss: 9153.95, base loss: 14411.94
[INFO 2017-06-28 15:52:13,631 main.py:51] epoch 14004, training loss: 9411.02, average training loss: 9154.47, base loss: 14412.24
[INFO 2017-06-28 15:52:14,302 main.py:51] epoch 14005, training loss: 8954.24, average training loss: 9155.59, base loss: 14415.61
[INFO 2017-06-28 15:52:14,981 main.py:51] epoch 14006, training loss: 9175.41, average training loss: 9154.04, base loss: 14412.88
[INFO 2017-06-28 15:52:15,642 main.py:51] epoch 14007, training loss: 8204.31, average training loss: 9153.49, base loss: 14412.28
[INFO 2017-06-28 15:52:16,306 main.py:51] epoch 14008, training loss: 8568.45, average training loss: 9153.10, base loss: 14411.31
[INFO 2017-06-28 15:52:16,962 main.py:51] epoch 14009, training loss: 8582.03, average training loss: 9152.11, base loss: 14410.04
[INFO 2017-06-28 15:52:17,646 main.py:51] epoch 14010, training loss: 9431.25, average training loss: 9152.25, base loss: 14410.35
[INFO 2017-06-28 15:52:18,319 main.py:51] epoch 14011, training loss: 9177.42, average training loss: 9152.95, base loss: 14413.41
[INFO 2017-06-28 15:52:18,994 main.py:51] epoch 14012, training loss: 8663.61, average training loss: 9152.74, base loss: 14414.02
[INFO 2017-06-28 15:52:19,696 main.py:51] epoch 14013, training loss: 8534.68, average training loss: 9150.23, base loss: 14412.15
[INFO 2017-06-28 15:52:20,373 main.py:51] epoch 14014, training loss: 8853.65, average training loss: 9148.89, base loss: 14409.16
[INFO 2017-06-28 15:52:21,034 main.py:51] epoch 14015, training loss: 8856.38, average training loss: 9149.29, base loss: 14409.67
[INFO 2017-06-28 15:52:21,694 main.py:51] epoch 14016, training loss: 10268.12, average training loss: 9150.45, base loss: 14411.61
[INFO 2017-06-28 15:52:22,360 main.py:51] epoch 14017, training loss: 9084.39, average training loss: 9151.43, base loss: 14414.25
[INFO 2017-06-28 15:52:23,046 main.py:51] epoch 14018, training loss: 9018.79, average training loss: 9152.21, base loss: 14417.36
[INFO 2017-06-28 15:52:23,720 main.py:51] epoch 14019, training loss: 11238.50, average training loss: 9153.77, base loss: 14420.30
[INFO 2017-06-28 15:52:24,394 main.py:51] epoch 14020, training loss: 9117.50, average training loss: 9154.80, base loss: 14423.51
[INFO 2017-06-28 15:52:25,068 main.py:51] epoch 14021, training loss: 9747.55, average training loss: 9155.33, base loss: 14423.23
[INFO 2017-06-28 15:52:25,745 main.py:51] epoch 14022, training loss: 10526.58, average training loss: 9156.91, base loss: 14428.00
[INFO 2017-06-28 15:52:26,402 main.py:51] epoch 14023, training loss: 8238.38, average training loss: 9155.25, base loss: 14425.90
[INFO 2017-06-28 15:52:27,073 main.py:51] epoch 14024, training loss: 8236.56, average training loss: 9154.45, base loss: 14424.78
[INFO 2017-06-28 15:52:27,746 main.py:51] epoch 14025, training loss: 8284.70, average training loss: 9153.78, base loss: 14424.46
[INFO 2017-06-28 15:52:28,422 main.py:51] epoch 14026, training loss: 7744.93, average training loss: 9152.42, base loss: 14422.26
[INFO 2017-06-28 15:52:29,061 main.py:51] epoch 14027, training loss: 8407.33, average training loss: 9152.63, base loss: 14421.29
[INFO 2017-06-28 15:52:29,708 main.py:51] epoch 14028, training loss: 9347.07, average training loss: 9154.51, base loss: 14424.11
[INFO 2017-06-28 15:52:30,384 main.py:51] epoch 14029, training loss: 10272.42, average training loss: 9156.13, base loss: 14427.51
[INFO 2017-06-28 15:52:31,054 main.py:51] epoch 14030, training loss: 9460.94, average training loss: 9157.23, base loss: 14430.71
[INFO 2017-06-28 15:52:31,736 main.py:51] epoch 14031, training loss: 8539.80, average training loss: 9156.61, base loss: 14430.67
[INFO 2017-06-28 15:52:32,378 main.py:51] epoch 14032, training loss: 9351.81, average training loss: 9157.16, base loss: 14431.35
[INFO 2017-06-28 15:52:33,036 main.py:51] epoch 14033, training loss: 8729.44, average training loss: 9156.26, base loss: 14430.70
[INFO 2017-06-28 15:52:33,699 main.py:51] epoch 14034, training loss: 9428.10, average training loss: 9155.94, base loss: 14429.75
[INFO 2017-06-28 15:52:34,363 main.py:51] epoch 14035, training loss: 7166.35, average training loss: 9153.22, base loss: 14426.83
[INFO 2017-06-28 15:52:35,039 main.py:51] epoch 14036, training loss: 8141.35, average training loss: 9151.80, base loss: 14424.70
[INFO 2017-06-28 15:52:35,739 main.py:51] epoch 14037, training loss: 9919.04, average training loss: 9151.52, base loss: 14425.67
[INFO 2017-06-28 15:52:36,403 main.py:51] epoch 14038, training loss: 8764.76, average training loss: 9149.83, base loss: 14423.73
[INFO 2017-06-28 15:52:37,046 main.py:51] epoch 14039, training loss: 9671.08, average training loss: 9150.39, base loss: 14425.40
[INFO 2017-06-28 15:52:37,707 main.py:51] epoch 14040, training loss: 8314.76, average training loss: 9148.42, base loss: 14422.19
[INFO 2017-06-28 15:52:38,364 main.py:51] epoch 14041, training loss: 8996.52, average training loss: 9148.30, base loss: 14421.99
[INFO 2017-06-28 15:52:39,030 main.py:51] epoch 14042, training loss: 8181.03, average training loss: 9147.45, base loss: 14420.81
[INFO 2017-06-28 15:52:39,672 main.py:51] epoch 14043, training loss: 9539.81, average training loss: 9146.83, base loss: 14419.17
[INFO 2017-06-28 15:52:40,336 main.py:51] epoch 14044, training loss: 9472.03, average training loss: 9146.43, base loss: 14417.63
[INFO 2017-06-28 15:52:41,011 main.py:51] epoch 14045, training loss: 9521.22, average training loss: 9145.80, base loss: 14417.86
[INFO 2017-06-28 15:52:41,652 main.py:51] epoch 14046, training loss: 8977.99, average training loss: 9147.13, base loss: 14420.91
[INFO 2017-06-28 15:52:42,301 main.py:51] epoch 14047, training loss: 8225.07, average training loss: 9147.25, base loss: 14421.15
[INFO 2017-06-28 15:52:42,971 main.py:51] epoch 14048, training loss: 10275.31, average training loss: 9149.40, base loss: 14424.81
[INFO 2017-06-28 15:52:43,613 main.py:51] epoch 14049, training loss: 8872.08, average training loss: 9149.71, base loss: 14425.81
[INFO 2017-06-28 15:52:44,272 main.py:51] epoch 14050, training loss: 8967.34, average training loss: 9149.31, base loss: 14425.84
[INFO 2017-06-28 15:52:44,921 main.py:51] epoch 14051, training loss: 7703.74, average training loss: 9148.91, base loss: 14424.86
[INFO 2017-06-28 15:52:45,611 main.py:51] epoch 14052, training loss: 8816.85, average training loss: 9147.55, base loss: 14425.25
[INFO 2017-06-28 15:52:46,272 main.py:51] epoch 14053, training loss: 10593.48, average training loss: 9149.64, base loss: 14429.18
[INFO 2017-06-28 15:52:46,931 main.py:51] epoch 14054, training loss: 8921.04, average training loss: 9149.23, base loss: 14426.94
[INFO 2017-06-28 15:52:47,580 main.py:51] epoch 14055, training loss: 9163.64, average training loss: 9149.71, base loss: 14427.45
[INFO 2017-06-28 15:52:48,252 main.py:51] epoch 14056, training loss: 9665.87, average training loss: 9150.62, base loss: 14428.41
[INFO 2017-06-28 15:52:48,919 main.py:51] epoch 14057, training loss: 8379.75, average training loss: 9149.08, base loss: 14425.01
[INFO 2017-06-28 15:52:49,580 main.py:51] epoch 14058, training loss: 9423.54, average training loss: 9149.10, base loss: 14426.57
[INFO 2017-06-28 15:52:50,210 main.py:51] epoch 14059, training loss: 8049.47, average training loss: 9147.63, base loss: 14423.73
[INFO 2017-06-28 15:52:50,870 main.py:51] epoch 14060, training loss: 9299.44, average training loss: 9147.43, base loss: 14425.51
[INFO 2017-06-28 15:52:51,515 main.py:51] epoch 14061, training loss: 7915.40, average training loss: 9146.67, base loss: 14424.36
[INFO 2017-06-28 15:52:52,185 main.py:51] epoch 14062, training loss: 8584.45, average training loss: 9145.66, base loss: 14424.02
[INFO 2017-06-28 15:52:52,839 main.py:51] epoch 14063, training loss: 10144.50, average training loss: 9146.68, base loss: 14424.63
[INFO 2017-06-28 15:52:53,491 main.py:51] epoch 14064, training loss: 8885.39, average training loss: 9146.62, base loss: 14424.24
[INFO 2017-06-28 15:52:54,148 main.py:51] epoch 14065, training loss: 8653.61, average training loss: 9146.09, base loss: 14423.19
[INFO 2017-06-28 15:52:54,805 main.py:51] epoch 14066, training loss: 8535.41, average training loss: 9144.76, base loss: 14419.94
[INFO 2017-06-28 15:52:55,462 main.py:51] epoch 14067, training loss: 9575.67, average training loss: 9145.95, base loss: 14422.62
[INFO 2017-06-28 15:52:56,121 main.py:51] epoch 14068, training loss: 8631.15, average training loss: 9144.54, base loss: 14421.38
[INFO 2017-06-28 15:52:56,773 main.py:51] epoch 14069, training loss: 9200.73, average training loss: 9143.72, base loss: 14420.64
[INFO 2017-06-28 15:52:57,442 main.py:51] epoch 14070, training loss: 10176.80, average training loss: 9145.12, base loss: 14423.79
[INFO 2017-06-28 15:52:58,093 main.py:51] epoch 14071, training loss: 10186.78, average training loss: 9146.64, base loss: 14426.42
[INFO 2017-06-28 15:52:58,782 main.py:51] epoch 14072, training loss: 8917.46, average training loss: 9146.05, base loss: 14424.57
[INFO 2017-06-28 15:52:59,433 main.py:51] epoch 14073, training loss: 10352.11, average training loss: 9147.47, base loss: 14426.61
[INFO 2017-06-28 15:53:00,104 main.py:51] epoch 14074, training loss: 8483.14, average training loss: 9147.36, base loss: 14426.99
[INFO 2017-06-28 15:53:00,757 main.py:51] epoch 14075, training loss: 8720.43, average training loss: 9147.41, base loss: 14425.14
[INFO 2017-06-28 15:53:01,433 main.py:51] epoch 14076, training loss: 9726.61, average training loss: 9147.74, base loss: 14425.35
[INFO 2017-06-28 15:53:02,112 main.py:51] epoch 14077, training loss: 9768.15, average training loss: 9149.12, base loss: 14427.77
[INFO 2017-06-28 15:53:02,786 main.py:51] epoch 14078, training loss: 8743.72, average training loss: 9148.81, base loss: 14426.83
[INFO 2017-06-28 15:53:03,470 main.py:51] epoch 14079, training loss: 8889.29, average training loss: 9147.43, base loss: 14423.32
[INFO 2017-06-28 15:53:04,139 main.py:51] epoch 14080, training loss: 8203.85, average training loss: 9146.54, base loss: 14422.42
[INFO 2017-06-28 15:53:04,841 main.py:51] epoch 14081, training loss: 9589.52, average training loss: 9146.33, base loss: 14422.33
[INFO 2017-06-28 15:53:05,497 main.py:51] epoch 14082, training loss: 8636.39, average training loss: 9145.59, base loss: 14420.96
[INFO 2017-06-28 15:53:06,169 main.py:51] epoch 14083, training loss: 9629.36, average training loss: 9146.19, base loss: 14421.55
[INFO 2017-06-28 15:53:06,819 main.py:51] epoch 14084, training loss: 8475.69, average training loss: 9145.62, base loss: 14422.71
[INFO 2017-06-28 15:53:07,473 main.py:51] epoch 14085, training loss: 8624.71, average training loss: 9144.83, base loss: 14423.40
[INFO 2017-06-28 15:53:08,124 main.py:51] epoch 14086, training loss: 9852.07, average training loss: 9145.91, base loss: 14424.69
[INFO 2017-06-28 15:53:08,785 main.py:51] epoch 14087, training loss: 9237.61, average training loss: 9146.01, base loss: 14425.05
[INFO 2017-06-28 15:53:09,460 main.py:51] epoch 14088, training loss: 9554.48, average training loss: 9146.56, base loss: 14425.47
[INFO 2017-06-28 15:53:10,127 main.py:51] epoch 14089, training loss: 8372.09, average training loss: 9145.04, base loss: 14422.26
[INFO 2017-06-28 15:53:10,802 main.py:51] epoch 14090, training loss: 8298.68, average training loss: 9144.83, base loss: 14421.23
[INFO 2017-06-28 15:53:11,445 main.py:51] epoch 14091, training loss: 10047.62, average training loss: 9145.80, base loss: 14423.24
[INFO 2017-06-28 15:53:12,089 main.py:51] epoch 14092, training loss: 8425.33, average training loss: 9144.95, base loss: 14421.77
[INFO 2017-06-28 15:53:12,749 main.py:51] epoch 14093, training loss: 9316.46, average training loss: 9144.90, base loss: 14421.23
[INFO 2017-06-28 15:53:13,433 main.py:51] epoch 14094, training loss: 9502.03, average training loss: 9144.99, base loss: 14420.31
[INFO 2017-06-28 15:53:14,087 main.py:51] epoch 14095, training loss: 8308.44, average training loss: 9144.26, base loss: 14418.42
[INFO 2017-06-28 15:53:14,764 main.py:51] epoch 14096, training loss: 9273.13, average training loss: 9143.77, base loss: 14417.51
[INFO 2017-06-28 15:53:15,420 main.py:51] epoch 14097, training loss: 9898.31, average training loss: 9144.39, base loss: 14418.80
[INFO 2017-06-28 15:53:16,084 main.py:51] epoch 14098, training loss: 9116.50, average training loss: 9143.32, base loss: 14417.70
[INFO 2017-06-28 15:53:16,747 main.py:51] epoch 14099, training loss: 9509.46, average training loss: 9144.17, base loss: 14417.71
[INFO 2017-06-28 15:53:16,747 main.py:53] epoch 14099, testing
[INFO 2017-06-28 15:53:19,400 main.py:105] average testing loss: 10754.76, base loss: 15384.39
[INFO 2017-06-28 15:53:19,400 main.py:106] improve_loss: 4629.63, improve_percent: 0.30
[INFO 2017-06-28 15:53:19,400 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 15:53:20,073 main.py:51] epoch 14100, training loss: 9065.16, average training loss: 9144.18, base loss: 14416.73
[INFO 2017-06-28 15:53:20,730 main.py:51] epoch 14101, training loss: 9345.02, average training loss: 9144.71, base loss: 14416.61
[INFO 2017-06-28 15:53:21,385 main.py:51] epoch 14102, training loss: 8710.47, average training loss: 9144.24, base loss: 14417.05
