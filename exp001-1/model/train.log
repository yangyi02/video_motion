[INFO 2017-06-26 19:11:58,665 main.py:123] Namespace(batch_size=32, display=False, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=1, num_inputs=1, save_dir='./model', test=False, test_dir='/home/yi/Downloads/robot-64', test_epoch=10, test_interval=100, train=True, train_dir='/home/yi/Downloads/robot-64', train_epoch=10000)
[INFO 2017-06-26 19:12:03,711 main.py:47] epoch 0, training loss: 33750.29, average training loss: 33750.29, base loss: 2700.27
[INFO 2017-06-26 19:12:03,964 main.py:47] epoch 1, training loss: 26415.12, average training loss: 30082.70, base loss: 2487.83
[INFO 2017-06-26 19:12:04,197 main.py:47] epoch 2, training loss: 23280.58, average training loss: 27815.33, base loss: 2394.61
[INFO 2017-06-26 19:12:04,426 main.py:47] epoch 3, training loss: 19789.93, average training loss: 25808.98, base loss: 2311.10
[INFO 2017-06-26 19:12:04,658 main.py:47] epoch 4, training loss: 17484.50, average training loss: 24144.08, base loss: 2315.96
[INFO 2017-06-26 19:12:04,881 main.py:47] epoch 5, training loss: 15224.40, average training loss: 22657.47, base loss: 2298.89
[INFO 2017-06-26 19:12:05,112 main.py:47] epoch 6, training loss: 13571.89, average training loss: 21359.53, base loss: 2311.81
[INFO 2017-06-26 19:12:05,353 main.py:47] epoch 7, training loss: 11283.78, average training loss: 20100.06, base loss: 2285.88
[INFO 2017-06-26 19:12:05,588 main.py:47] epoch 8, training loss: 10151.85, average training loss: 18994.70, base loss: 2323.14
[INFO 2017-06-26 19:12:05,828 main.py:47] epoch 9, training loss: 8631.98, average training loss: 17958.43, base loss: 2308.50
[INFO 2017-06-26 19:12:06,062 main.py:47] epoch 10, training loss: 7723.18, average training loss: 17027.95, base loss: 2296.94
[INFO 2017-06-26 19:12:06,297 main.py:47] epoch 11, training loss: 6802.52, average training loss: 16175.83, base loss: 2289.59
[INFO 2017-06-26 19:12:06,531 main.py:47] epoch 12, training loss: 6155.92, average training loss: 15405.07, base loss: 2297.05
[INFO 2017-06-26 19:12:06,766 main.py:47] epoch 13, training loss: 5540.62, average training loss: 14700.47, base loss: 2299.94
[INFO 2017-06-26 19:12:06,997 main.py:47] epoch 14, training loss: 5214.48, average training loss: 14068.07, base loss: 2330.19
[INFO 2017-06-26 19:12:07,241 main.py:47] epoch 15, training loss: 4186.21, average training loss: 13450.45, base loss: 2309.63
[INFO 2017-06-26 19:12:07,484 main.py:47] epoch 16, training loss: 4359.67, average training loss: 12915.70, base loss: 2335.87
[INFO 2017-06-26 19:12:07,740 main.py:47] epoch 17, training loss: 3752.18, average training loss: 12406.62, base loss: 2339.49
[INFO 2017-06-26 19:12:08,005 main.py:47] epoch 18, training loss: 3294.48, average training loss: 11927.03, base loss: 2328.26
[INFO 2017-06-26 19:12:08,275 main.py:47] epoch 19, training loss: 3239.99, average training loss: 11492.68, base loss: 2320.37
[INFO 2017-06-26 19:12:08,556 main.py:47] epoch 20, training loss: 3070.62, average training loss: 11091.63, base loss: 2317.35
[INFO 2017-06-26 19:12:08,841 main.py:47] epoch 21, training loss: 2859.32, average training loss: 10717.43, base loss: 2307.59
[INFO 2017-06-26 19:12:09,124 main.py:47] epoch 22, training loss: 3113.17, average training loss: 10386.81, base loss: 2320.21
[INFO 2017-06-26 19:12:09,409 main.py:47] epoch 23, training loss: 2768.04, average training loss: 10069.36, base loss: 2319.63
[INFO 2017-06-26 19:12:09,694 main.py:47] epoch 24, training loss: 2407.54, average training loss: 9762.89, base loss: 2308.68
[INFO 2017-06-26 19:12:09,974 main.py:47] epoch 25, training loss: 2570.34, average training loss: 9486.25, base loss: 2307.55
[INFO 2017-06-26 19:12:10,260 main.py:47] epoch 26, training loss: 2464.17, average training loss: 9226.18, base loss: 2302.12
[INFO 2017-06-26 19:12:10,546 main.py:47] epoch 27, training loss: 2862.95, average training loss: 8998.92, base loss: 2314.73
[INFO 2017-06-26 19:12:10,829 main.py:47] epoch 28, training loss: 2663.24, average training loss: 8780.45, base loss: 2319.84
[INFO 2017-06-26 19:12:11,111 main.py:47] epoch 29, training loss: 2694.43, average training loss: 8577.58, base loss: 2327.10
[INFO 2017-06-26 19:12:11,393 main.py:47] epoch 30, training loss: 2618.83, average training loss: 8385.36, base loss: 2331.82
[INFO 2017-06-26 19:12:11,677 main.py:47] epoch 31, training loss: 2505.30, average training loss: 8201.61, base loss: 2333.14
[INFO 2017-06-26 19:12:11,965 main.py:47] epoch 32, training loss: 2264.28, average training loss: 8021.69, base loss: 2327.68
[INFO 2017-06-26 19:12:12,251 main.py:47] epoch 33, training loss: 2504.64, average training loss: 7859.42, base loss: 2330.02
[INFO 2017-06-26 19:12:12,534 main.py:47] epoch 34, training loss: 2384.38, average training loss: 7702.99, base loss: 2329.05
[INFO 2017-06-26 19:12:12,819 main.py:47] epoch 35, training loss: 2392.14, average training loss: 7555.47, base loss: 2328.44
[INFO 2017-06-26 19:12:13,100 main.py:47] epoch 36, training loss: 2406.35, average training loss: 7416.31, base loss: 2328.57
[INFO 2017-06-26 19:12:13,381 main.py:47] epoch 37, training loss: 2222.90, average training loss: 7279.64, base loss: 2323.93
[INFO 2017-06-26 19:12:13,665 main.py:47] epoch 38, training loss: 2730.08, average training loss: 7162.98, base loss: 2332.61
[INFO 2017-06-26 19:12:13,954 main.py:47] epoch 39, training loss: 2822.36, average training loss: 7054.47, base loss: 2343.36
[INFO 2017-06-26 19:12:14,242 main.py:47] epoch 40, training loss: 2534.44, average training loss: 6944.22, base loss: 2346.61
[INFO 2017-06-26 19:12:14,527 main.py:47] epoch 41, training loss: 2294.31, average training loss: 6833.51, base loss: 2344.02
[INFO 2017-06-26 19:12:14,811 main.py:47] epoch 42, training loss: 2897.85, average training loss: 6741.98, base loss: 2355.76
[INFO 2017-06-26 19:12:15,090 main.py:47] epoch 43, training loss: 2259.40, average training loss: 6640.11, base loss: 2352.44
[INFO 2017-06-26 19:12:15,374 main.py:47] epoch 44, training loss: 2517.05, average training loss: 6548.48, base loss: 2355.25
[INFO 2017-06-26 19:12:15,659 main.py:47] epoch 45, training loss: 2213.57, average training loss: 6454.24, base loss: 2351.10
[INFO 2017-06-26 19:12:16,058 main.py:47] epoch 46, training loss: 2708.04, average training loss: 6374.54, base loss: 2357.85
[INFO 2017-06-26 19:12:16,341 main.py:47] epoch 47, training loss: 2279.36, average training loss: 6289.22, base loss: 2355.19
[INFO 2017-06-26 19:12:16,625 main.py:47] epoch 48, training loss: 2601.15, average training loss: 6213.96, base loss: 2359.21
[INFO 2017-06-26 19:12:16,913 main.py:47] epoch 49, training loss: 2165.48, average training loss: 6132.99, base loss: 2354.70
[INFO 2017-06-26 19:12:17,200 main.py:47] epoch 50, training loss: 2755.03, average training loss: 6066.75, base loss: 2361.93
[INFO 2017-06-26 19:12:17,488 main.py:47] epoch 51, training loss: 2396.17, average training loss: 5996.16, base loss: 2362.11
[INFO 2017-06-26 19:12:17,772 main.py:47] epoch 52, training loss: 2442.76, average training loss: 5929.12, base loss: 2363.22
[INFO 2017-06-26 19:12:18,053 main.py:47] epoch 53, training loss: 2203.17, average training loss: 5860.12, base loss: 2359.70
[INFO 2017-06-26 19:12:18,334 main.py:47] epoch 54, training loss: 2172.09, average training loss: 5793.06, base loss: 2355.60
[INFO 2017-06-26 19:12:18,618 main.py:47] epoch 55, training loss: 2475.49, average training loss: 5733.82, base loss: 2357.28
[INFO 2017-06-26 19:12:18,906 main.py:47] epoch 56, training loss: 2000.41, average training loss: 5668.32, base loss: 2350.40
[INFO 2017-06-26 19:12:19,192 main.py:47] epoch 57, training loss: 2888.20, average training loss: 5620.39, base loss: 2359.30
[INFO 2017-06-26 19:12:19,476 main.py:47] epoch 58, training loss: 2408.48, average training loss: 5565.95, base loss: 2359.54
[INFO 2017-06-26 19:12:19,760 main.py:47] epoch 59, training loss: 2904.60, average training loss: 5521.59, base loss: 2368.22
[INFO 2017-06-26 19:12:20,042 main.py:47] epoch 60, training loss: 2254.28, average training loss: 5468.03, base loss: 2365.88
[INFO 2017-06-26 19:12:20,327 main.py:47] epoch 61, training loss: 2434.64, average training loss: 5419.11, base loss: 2366.62
[INFO 2017-06-26 19:12:20,614 main.py:47] epoch 62, training loss: 2281.56, average training loss: 5369.30, base loss: 2364.83
[INFO 2017-06-26 19:12:20,898 main.py:47] epoch 63, training loss: 2578.46, average training loss: 5325.70, base loss: 2367.84
[INFO 2017-06-26 19:12:21,185 main.py:47] epoch 64, training loss: 2407.65, average training loss: 5280.80, base loss: 2368.17
[INFO 2017-06-26 19:12:21,469 main.py:47] epoch 65, training loss: 2237.40, average training loss: 5234.69, base loss: 2365.80
[INFO 2017-06-26 19:12:21,754 main.py:47] epoch 66, training loss: 2613.74, average training loss: 5195.57, base loss: 2369.15
[INFO 2017-06-26 19:12:22,038 main.py:47] epoch 67, training loss: 3067.17, average training loss: 5164.27, base loss: 2379.19
[INFO 2017-06-26 19:12:22,322 main.py:47] epoch 68, training loss: 2450.18, average training loss: 5124.94, base loss: 2379.84
[INFO 2017-06-26 19:12:22,609 main.py:47] epoch 69, training loss: 2547.45, average training loss: 5088.12, base loss: 2382.01
[INFO 2017-06-26 19:12:22,894 main.py:47] epoch 70, training loss: 2989.54, average training loss: 5058.56, base loss: 2390.43
[INFO 2017-06-26 19:12:23,179 main.py:47] epoch 71, training loss: 2617.03, average training loss: 5024.65, base loss: 2393.25
[INFO 2017-06-26 19:12:23,461 main.py:47] epoch 72, training loss: 2126.65, average training loss: 4984.95, base loss: 2389.29
[INFO 2017-06-26 19:12:23,746 main.py:47] epoch 73, training loss: 2542.00, average training loss: 4951.94, base loss: 2391.15
[INFO 2017-06-26 19:12:24,028 main.py:47] epoch 74, training loss: 2291.51, average training loss: 4916.47, base loss: 2389.56
[INFO 2017-06-26 19:12:24,315 main.py:47] epoch 75, training loss: 2638.22, average training loss: 4886.49, base loss: 2392.63
[INFO 2017-06-26 19:12:24,602 main.py:47] epoch 76, training loss: 2255.00, average training loss: 4852.31, base loss: 2390.61
[INFO 2017-06-26 19:12:24,891 main.py:47] epoch 77, training loss: 2361.12, average training loss: 4820.38, base loss: 2390.00
[INFO 2017-06-26 19:12:25,176 main.py:47] epoch 78, training loss: 2343.45, average training loss: 4789.02, base loss: 2389.25
[INFO 2017-06-26 19:12:25,460 main.py:47] epoch 79, training loss: 2457.80, average training loss: 4759.88, base loss: 2390.07
[INFO 2017-06-26 19:12:25,762 main.py:47] epoch 80, training loss: 2383.73, average training loss: 4730.55, base loss: 2389.87
[INFO 2017-06-26 19:12:26,061 main.py:47] epoch 81, training loss: 2663.00, average training loss: 4705.33, base loss: 2393.07
[INFO 2017-06-26 19:12:26,350 main.py:47] epoch 82, training loss: 2020.82, average training loss: 4672.99, base loss: 2388.31
[INFO 2017-06-26 19:12:26,632 main.py:47] epoch 83, training loss: 2656.30, average training loss: 4648.98, base loss: 2391.33
[INFO 2017-06-26 19:12:26,920 main.py:47] epoch 84, training loss: 2154.10, average training loss: 4619.63, base loss: 2388.29
[INFO 2017-06-26 19:12:27,206 main.py:47] epoch 85, training loss: 2435.94, average training loss: 4594.24, base loss: 2388.60
[INFO 2017-06-26 19:12:27,498 main.py:47] epoch 86, training loss: 2299.40, average training loss: 4567.86, base loss: 2387.30
[INFO 2017-06-26 19:12:27,785 main.py:47] epoch 87, training loss: 2964.85, average training loss: 4549.64, base loss: 2393.78
[INFO 2017-06-26 19:12:28,072 main.py:47] epoch 88, training loss: 2467.57, average training loss: 4526.25, base loss: 2394.51
[INFO 2017-06-26 19:12:28,359 main.py:47] epoch 89, training loss: 2434.29, average training loss: 4503.01, base loss: 2394.90
[INFO 2017-06-26 19:12:28,645 main.py:47] epoch 90, training loss: 2480.12, average training loss: 4480.78, base loss: 2395.71
[INFO 2017-06-26 19:12:28,931 main.py:47] epoch 91, training loss: 2212.40, average training loss: 4456.12, base loss: 2393.48
[INFO 2017-06-26 19:12:29,215 main.py:47] epoch 92, training loss: 2310.91, average training loss: 4433.05, base loss: 2392.46
[INFO 2017-06-26 19:12:29,497 main.py:47] epoch 93, training loss: 2192.12, average training loss: 4409.21, base loss: 2390.32
[INFO 2017-06-26 19:12:29,778 main.py:47] epoch 94, training loss: 2438.65, average training loss: 4388.47, base loss: 2390.83
[INFO 2017-06-26 19:12:30,066 main.py:47] epoch 95, training loss: 2483.10, average training loss: 4368.62, base loss: 2391.70
[INFO 2017-06-26 19:12:30,354 main.py:47] epoch 96, training loss: 2158.21, average training loss: 4345.84, base loss: 2389.14
[INFO 2017-06-26 19:12:30,641 main.py:47] epoch 97, training loss: 2241.91, average training loss: 4324.37, base loss: 2387.52
[INFO 2017-06-26 19:12:30,926 main.py:47] epoch 98, training loss: 1992.35, average training loss: 4300.81, base loss: 2383.42
[INFO 2017-06-26 19:12:31,212 main.py:47] epoch 99, training loss: 2197.31, average training loss: 4279.78, base loss: 2381.39
[INFO 2017-06-26 19:12:31,212 main.py:49] epoch 99, testing
[INFO 2017-06-26 19:12:35,003 main.py:100] average testing loss: 2310.15, base loss: 2307.52
[INFO 2017-06-26 19:12:35,027 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:12:35,040 main.py:73] current best accuracy: 2310.15
[INFO 2017-06-26 19:12:35,324 main.py:47] epoch 100, training loss: 1864.11, average training loss: 4255.86, base loss: 2376.01
[INFO 2017-06-26 19:12:35,611 main.py:47] epoch 101, training loss: 2307.59, average training loss: 4236.76, base loss: 2375.25
[INFO 2017-06-26 19:12:35,894 main.py:47] epoch 102, training loss: 2525.17, average training loss: 4220.14, base loss: 2376.71
[INFO 2017-06-26 19:12:36,181 main.py:47] epoch 103, training loss: 2144.08, average training loss: 4200.18, base loss: 2374.34
[INFO 2017-06-26 19:12:36,464 main.py:47] epoch 104, training loss: 2532.06, average training loss: 4184.29, base loss: 2375.68
[INFO 2017-06-26 19:12:36,750 main.py:47] epoch 105, training loss: 2433.74, average training loss: 4167.78, base loss: 2376.21
[INFO 2017-06-26 19:12:37,034 main.py:47] epoch 106, training loss: 2465.67, average training loss: 4151.87, base loss: 2377.05
[INFO 2017-06-26 19:12:37,319 main.py:47] epoch 107, training loss: 2717.14, average training loss: 4138.58, base loss: 2380.23
[INFO 2017-06-26 19:12:37,605 main.py:47] epoch 108, training loss: 2386.88, average training loss: 4122.51, base loss: 2380.32
[INFO 2017-06-26 19:12:37,889 main.py:47] epoch 109, training loss: 2594.28, average training loss: 4108.62, base loss: 2382.14
[INFO 2017-06-26 19:12:38,170 main.py:47] epoch 110, training loss: 2565.26, average training loss: 4094.72, base loss: 2383.82
[INFO 2017-06-26 19:12:38,453 main.py:47] epoch 111, training loss: 2643.92, average training loss: 4081.76, base loss: 2386.17
[INFO 2017-06-26 19:12:38,737 main.py:47] epoch 112, training loss: 2496.97, average training loss: 4067.74, base loss: 2387.18
[INFO 2017-06-26 19:12:39,021 main.py:47] epoch 113, training loss: 2734.43, average training loss: 4056.04, base loss: 2390.27
[INFO 2017-06-26 19:12:39,305 main.py:47] epoch 114, training loss: 2312.37, average training loss: 4040.88, base loss: 2389.61
[INFO 2017-06-26 19:12:39,591 main.py:47] epoch 115, training loss: 2522.58, average training loss: 4027.79, base loss: 2390.78
[INFO 2017-06-26 19:12:39,876 main.py:47] epoch 116, training loss: 2285.60, average training loss: 4012.90, base loss: 2389.84
[INFO 2017-06-26 19:12:40,159 main.py:47] epoch 117, training loss: 2320.72, average training loss: 3998.56, base loss: 2389.22
[INFO 2017-06-26 19:12:40,443 main.py:47] epoch 118, training loss: 2321.98, average training loss: 3984.47, base loss: 2388.63
[INFO 2017-06-26 19:12:40,730 main.py:47] epoch 119, training loss: 1899.45, average training loss: 3967.10, base loss: 2384.54
[INFO 2017-06-26 19:12:41,011 main.py:47] epoch 120, training loss: 2563.08, average training loss: 3955.49, base loss: 2386.14
[INFO 2017-06-26 19:12:41,295 main.py:47] epoch 121, training loss: 2551.12, average training loss: 3943.98, base loss: 2387.64
[INFO 2017-06-26 19:12:41,577 main.py:47] epoch 122, training loss: 2704.78, average training loss: 3933.91, base loss: 2390.39
[INFO 2017-06-26 19:12:41,862 main.py:47] epoch 123, training loss: 1960.97, average training loss: 3918.00, base loss: 2386.83
[INFO 2017-06-26 19:12:42,145 main.py:47] epoch 124, training loss: 2426.24, average training loss: 3906.06, base loss: 2387.06
[INFO 2017-06-26 19:12:42,428 main.py:47] epoch 125, training loss: 2619.30, average training loss: 3895.85, base loss: 2389.01
[INFO 2017-06-26 19:12:42,714 main.py:47] epoch 126, training loss: 2557.80, average training loss: 3885.31, base loss: 2390.51
[INFO 2017-06-26 19:12:43,002 main.py:47] epoch 127, training loss: 2541.39, average training loss: 3874.81, base loss: 2391.75
[INFO 2017-06-26 19:12:43,289 main.py:47] epoch 128, training loss: 2016.27, average training loss: 3860.41, base loss: 2388.90
[INFO 2017-06-26 19:12:43,572 main.py:47] epoch 129, training loss: 2379.58, average training loss: 3849.02, base loss: 2388.85
[INFO 2017-06-26 19:12:43,861 main.py:47] epoch 130, training loss: 2777.90, average training loss: 3840.84, base loss: 2391.98
[INFO 2017-06-26 19:12:44,145 main.py:47] epoch 131, training loss: 2380.18, average training loss: 3829.77, base loss: 2391.96
[INFO 2017-06-26 19:12:44,429 main.py:47] epoch 132, training loss: 2142.54, average training loss: 3817.09, base loss: 2390.21
[INFO 2017-06-26 19:12:44,716 main.py:47] epoch 133, training loss: 2068.17, average training loss: 3804.04, base loss: 2387.84
[INFO 2017-06-26 19:12:45,000 main.py:47] epoch 134, training loss: 2344.24, average training loss: 3793.22, base loss: 2387.62
[INFO 2017-06-26 19:12:45,287 main.py:47] epoch 135, training loss: 2457.07, average training loss: 3783.40, base loss: 2388.26
[INFO 2017-06-26 19:12:45,572 main.py:47] epoch 136, training loss: 2193.29, average training loss: 3771.79, base loss: 2386.80
[INFO 2017-06-26 19:12:45,856 main.py:47] epoch 137, training loss: 2268.64, average training loss: 3760.90, base loss: 2385.90
[INFO 2017-06-26 19:12:46,142 main.py:47] epoch 138, training loss: 2744.90, average training loss: 3753.59, base loss: 2388.60
[INFO 2017-06-26 19:12:46,426 main.py:47] epoch 139, training loss: 1944.74, average training loss: 3740.67, base loss: 2385.44
[INFO 2017-06-26 19:12:46,711 main.py:47] epoch 140, training loss: 2052.87, average training loss: 3728.70, base loss: 2383.05
[INFO 2017-06-26 19:12:46,995 main.py:47] epoch 141, training loss: 2507.89, average training loss: 3720.10, base loss: 2384.10
[INFO 2017-06-26 19:12:47,281 main.py:47] epoch 142, training loss: 2109.49, average training loss: 3708.84, base loss: 2382.32
[INFO 2017-06-26 19:12:47,565 main.py:47] epoch 143, training loss: 2098.14, average training loss: 3697.65, base loss: 2380.43
[INFO 2017-06-26 19:12:47,849 main.py:47] epoch 144, training loss: 2516.10, average training loss: 3689.51, base loss: 2381.57
[INFO 2017-06-26 19:12:48,136 main.py:47] epoch 145, training loss: 2452.04, average training loss: 3681.03, base loss: 2382.24
[INFO 2017-06-26 19:12:48,418 main.py:47] epoch 146, training loss: 2587.14, average training loss: 3673.59, base loss: 2383.84
[INFO 2017-06-26 19:12:48,700 main.py:47] epoch 147, training loss: 2531.60, average training loss: 3665.87, base loss: 2384.87
[INFO 2017-06-26 19:12:48,982 main.py:47] epoch 148, training loss: 2367.21, average training loss: 3657.16, base loss: 2384.74
[INFO 2017-06-26 19:12:49,266 main.py:47] epoch 149, training loss: 2611.48, average training loss: 3650.19, base loss: 2386.25
[INFO 2017-06-26 19:12:49,552 main.py:47] epoch 150, training loss: 1966.83, average training loss: 3639.04, base loss: 2383.32
[INFO 2017-06-26 19:12:49,835 main.py:47] epoch 151, training loss: 2141.92, average training loss: 3629.19, base loss: 2381.87
[INFO 2017-06-26 19:12:50,122 main.py:47] epoch 152, training loss: 2570.44, average training loss: 3622.27, base loss: 2383.30
[INFO 2017-06-26 19:12:50,409 main.py:47] epoch 153, training loss: 2298.72, average training loss: 3613.67, base loss: 2382.84
[INFO 2017-06-26 19:12:50,692 main.py:47] epoch 154, training loss: 2250.69, average training loss: 3604.88, base loss: 2382.05
[INFO 2017-06-26 19:12:50,974 main.py:47] epoch 155, training loss: 2290.01, average training loss: 3596.45, base loss: 2381.55
[INFO 2017-06-26 19:12:51,255 main.py:47] epoch 156, training loss: 2435.00, average training loss: 3589.05, base loss: 2381.98
[INFO 2017-06-26 19:12:51,543 main.py:47] epoch 157, training loss: 1858.00, average training loss: 3578.10, base loss: 2378.50
[INFO 2017-06-26 19:12:51,827 main.py:47] epoch 158, training loss: 2467.04, average training loss: 3571.11, base loss: 2379.16
[INFO 2017-06-26 19:12:52,109 main.py:47] epoch 159, training loss: 2469.86, average training loss: 3564.23, base loss: 2379.83
[INFO 2017-06-26 19:12:52,394 main.py:47] epoch 160, training loss: 2335.09, average training loss: 3556.59, base loss: 2379.69
[INFO 2017-06-26 19:12:52,678 main.py:47] epoch 161, training loss: 2480.93, average training loss: 3549.95, base loss: 2380.49
[INFO 2017-06-26 19:12:52,963 main.py:47] epoch 162, training loss: 2366.59, average training loss: 3542.69, base loss: 2380.57
[INFO 2017-06-26 19:12:53,243 main.py:47] epoch 163, training loss: 2741.77, average training loss: 3537.81, base loss: 2382.99
[INFO 2017-06-26 19:12:53,528 main.py:47] epoch 164, training loss: 2386.21, average training loss: 3530.83, base loss: 2383.25
[INFO 2017-06-26 19:12:53,810 main.py:47] epoch 165, training loss: 2156.78, average training loss: 3522.55, base loss: 2381.90
[INFO 2017-06-26 19:12:54,096 main.py:47] epoch 166, training loss: 1839.66, average training loss: 3512.47, base loss: 2378.47
[INFO 2017-06-26 19:12:54,383 main.py:47] epoch 167, training loss: 2094.95, average training loss: 3504.04, base loss: 2376.79
[INFO 2017-06-26 19:12:54,670 main.py:47] epoch 168, training loss: 2585.68, average training loss: 3498.60, base loss: 2378.22
[INFO 2017-06-26 19:12:54,953 main.py:47] epoch 169, training loss: 2619.19, average training loss: 3493.43, base loss: 2379.89
[INFO 2017-06-26 19:12:55,234 main.py:47] epoch 170, training loss: 2695.99, average training loss: 3488.77, base loss: 2382.04
[INFO 2017-06-26 19:12:55,521 main.py:47] epoch 171, training loss: 2305.96, average training loss: 3481.89, base loss: 2381.63
[INFO 2017-06-26 19:12:55,804 main.py:47] epoch 172, training loss: 2431.10, average training loss: 3475.82, base loss: 2381.95
[INFO 2017-06-26 19:12:56,092 main.py:47] epoch 173, training loss: 2014.54, average training loss: 3467.42, base loss: 2379.88
[INFO 2017-06-26 19:12:56,375 main.py:47] epoch 174, training loss: 2399.95, average training loss: 3461.32, base loss: 2380.14
[INFO 2017-06-26 19:12:56,659 main.py:47] epoch 175, training loss: 2120.94, average training loss: 3453.70, base loss: 2378.76
[INFO 2017-06-26 19:12:56,940 main.py:47] epoch 176, training loss: 3289.22, average training loss: 3452.77, base loss: 2384.21
[INFO 2017-06-26 19:12:57,225 main.py:47] epoch 177, training loss: 2514.34, average training loss: 3447.50, base loss: 2385.07
[INFO 2017-06-26 19:12:57,508 main.py:47] epoch 178, training loss: 2283.81, average training loss: 3441.00, base loss: 2384.71
[INFO 2017-06-26 19:12:57,794 main.py:47] epoch 179, training loss: 2252.08, average training loss: 3434.39, base loss: 2384.24
[INFO 2017-06-26 19:12:58,079 main.py:47] epoch 180, training loss: 2338.99, average training loss: 3428.34, base loss: 2384.20
[INFO 2017-06-26 19:12:58,362 main.py:47] epoch 181, training loss: 1904.81, average training loss: 3419.97, base loss: 2381.62
[INFO 2017-06-26 19:12:58,645 main.py:47] epoch 182, training loss: 2397.50, average training loss: 3414.38, base loss: 2381.87
[INFO 2017-06-26 19:12:58,930 main.py:47] epoch 183, training loss: 2007.44, average training loss: 3406.74, base loss: 2380.01
[INFO 2017-06-26 19:12:59,216 main.py:47] epoch 184, training loss: 3050.80, average training loss: 3404.81, base loss: 2384.10
[INFO 2017-06-26 19:12:59,500 main.py:47] epoch 185, training loss: 2127.05, average training loss: 3397.94, base loss: 2382.76
[INFO 2017-06-26 19:12:59,785 main.py:47] epoch 186, training loss: 2534.61, average training loss: 3393.33, base loss: 2383.75
[INFO 2017-06-26 19:13:00,069 main.py:47] epoch 187, training loss: 2813.91, average training loss: 3390.25, base loss: 2386.39
[INFO 2017-06-26 19:13:00,351 main.py:47] epoch 188, training loss: 1952.63, average training loss: 3382.64, base loss: 2384.16
[INFO 2017-06-26 19:13:00,634 main.py:47] epoch 189, training loss: 2916.46, average training loss: 3380.19, base loss: 2387.44
[INFO 2017-06-26 19:13:00,921 main.py:47] epoch 190, training loss: 2352.07, average training loss: 3374.80, base loss: 2387.58
[INFO 2017-06-26 19:13:01,209 main.py:47] epoch 191, training loss: 2548.69, average training loss: 3370.50, base loss: 2388.52
[INFO 2017-06-26 19:13:01,524 main.py:47] epoch 192, training loss: 2557.08, average training loss: 3366.29, base loss: 2389.91
[INFO 2017-06-26 19:13:01,807 main.py:47] epoch 193, training loss: 2198.09, average training loss: 3360.26, base loss: 2388.81
[INFO 2017-06-26 19:13:02,090 main.py:47] epoch 194, training loss: 2490.62, average training loss: 3355.80, base loss: 2389.75
[INFO 2017-06-26 19:13:02,372 main.py:47] epoch 195, training loss: 2068.42, average training loss: 3349.24, base loss: 2388.30
[INFO 2017-06-26 19:13:02,655 main.py:47] epoch 196, training loss: 2402.10, average training loss: 3344.43, base loss: 2388.54
[INFO 2017-06-26 19:13:02,940 main.py:47] epoch 197, training loss: 2164.90, average training loss: 3338.47, base loss: 2387.57
[INFO 2017-06-26 19:13:03,221 main.py:47] epoch 198, training loss: 1892.26, average training loss: 3331.20, base loss: 2385.09
[INFO 2017-06-26 19:13:03,506 main.py:47] epoch 199, training loss: 2456.42, average training loss: 3326.83, base loss: 2385.59
[INFO 2017-06-26 19:13:03,506 main.py:49] epoch 199, testing
[INFO 2017-06-26 19:13:07,294 main.py:100] average testing loss: 2452.94, base loss: 2492.88
[INFO 2017-06-26 19:13:07,318 main.py:73] current best accuracy: 2310.15
[INFO 2017-06-26 19:13:07,602 main.py:47] epoch 200, training loss: 2560.21, average training loss: 3323.02, base loss: 2386.79
[INFO 2017-06-26 19:13:07,888 main.py:47] epoch 201, training loss: 1894.12, average training loss: 3315.94, base loss: 2384.38
[INFO 2017-06-26 19:13:08,171 main.py:47] epoch 202, training loss: 2543.55, average training loss: 3312.14, base loss: 2385.36
[INFO 2017-06-26 19:13:08,457 main.py:47] epoch 203, training loss: 2123.41, average training loss: 3306.31, base loss: 2384.04
[INFO 2017-06-26 19:13:08,743 main.py:47] epoch 204, training loss: 2233.67, average training loss: 3301.08, base loss: 2383.49
[INFO 2017-06-26 19:13:09,027 main.py:47] epoch 205, training loss: 2534.25, average training loss: 3297.35, base loss: 2384.41
[INFO 2017-06-26 19:13:09,311 main.py:47] epoch 206, training loss: 2677.69, average training loss: 3294.36, base loss: 2386.13
[INFO 2017-06-26 19:13:09,595 main.py:47] epoch 207, training loss: 2209.76, average training loss: 3289.15, base loss: 2385.50
[INFO 2017-06-26 19:13:09,878 main.py:47] epoch 208, training loss: 2327.89, average training loss: 3284.55, base loss: 2385.46
[INFO 2017-06-26 19:13:10,160 main.py:47] epoch 209, training loss: 2821.19, average training loss: 3282.34, base loss: 2387.68
[INFO 2017-06-26 19:13:10,445 main.py:47] epoch 210, training loss: 2273.56, average training loss: 3277.56, base loss: 2387.42
[INFO 2017-06-26 19:13:10,730 main.py:47] epoch 211, training loss: 2515.61, average training loss: 3273.97, base loss: 2388.32
[INFO 2017-06-26 19:13:11,011 main.py:47] epoch 212, training loss: 2521.76, average training loss: 3270.43, base loss: 2389.29
[INFO 2017-06-26 19:13:11,297 main.py:47] epoch 213, training loss: 2318.69, average training loss: 3265.99, base loss: 2389.33
[INFO 2017-06-26 19:13:11,582 main.py:47] epoch 214, training loss: 2470.70, average training loss: 3262.29, base loss: 2389.95
[INFO 2017-06-26 19:13:11,868 main.py:47] epoch 215, training loss: 2224.56, average training loss: 3257.48, base loss: 2389.22
[INFO 2017-06-26 19:13:12,153 main.py:47] epoch 216, training loss: 2785.96, average training loss: 3255.31, base loss: 2391.36
[INFO 2017-06-26 19:13:12,435 main.py:47] epoch 217, training loss: 2193.65, average training loss: 3250.44, base loss: 2390.60
[INFO 2017-06-26 19:13:12,716 main.py:47] epoch 218, training loss: 2205.66, average training loss: 3245.67, base loss: 2389.95
[INFO 2017-06-26 19:13:13,001 main.py:47] epoch 219, training loss: 2174.45, average training loss: 3240.80, base loss: 2389.22
[INFO 2017-06-26 19:13:13,288 main.py:47] epoch 220, training loss: 2146.11, average training loss: 3235.85, base loss: 2388.37
[INFO 2017-06-26 19:13:13,572 main.py:47] epoch 221, training loss: 2654.82, average training loss: 3233.23, base loss: 2389.71
[INFO 2017-06-26 19:13:13,857 main.py:47] epoch 222, training loss: 2373.81, average training loss: 3229.38, base loss: 2389.80
[INFO 2017-06-26 19:13:14,143 main.py:47] epoch 223, training loss: 2571.88, average training loss: 3226.44, base loss: 2390.83
[INFO 2017-06-26 19:13:14,430 main.py:47] epoch 224, training loss: 2099.30, average training loss: 3221.43, base loss: 2389.65
[INFO 2017-06-26 19:13:14,715 main.py:47] epoch 225, training loss: 2401.83, average training loss: 3217.81, base loss: 2390.13
[INFO 2017-06-26 19:13:15,003 main.py:47] epoch 226, training loss: 2465.26, average training loss: 3214.49, base loss: 2390.48
[INFO 2017-06-26 19:13:15,289 main.py:47] epoch 227, training loss: 1821.06, average training loss: 3208.38, base loss: 2388.05
[INFO 2017-06-26 19:13:15,575 main.py:47] epoch 228, training loss: 2725.10, average training loss: 3206.27, base loss: 2389.75
[INFO 2017-06-26 19:13:15,857 main.py:47] epoch 229, training loss: 2227.04, average training loss: 3202.01, base loss: 2389.16
[INFO 2017-06-26 19:13:16,138 main.py:47] epoch 230, training loss: 2355.11, average training loss: 3198.34, base loss: 2389.33
[INFO 2017-06-26 19:13:16,423 main.py:47] epoch 231, training loss: 2378.84, average training loss: 3194.81, base loss: 2389.48
[INFO 2017-06-26 19:13:16,711 main.py:47] epoch 232, training loss: 2332.11, average training loss: 3191.11, base loss: 2389.51
[INFO 2017-06-26 19:13:16,996 main.py:47] epoch 233, training loss: 2324.15, average training loss: 3187.40, base loss: 2389.51
[INFO 2017-06-26 19:13:17,284 main.py:47] epoch 234, training loss: 2047.16, average training loss: 3182.55, base loss: 2388.07
[INFO 2017-06-26 19:13:17,570 main.py:47] epoch 235, training loss: 1963.14, average training loss: 3177.39, base loss: 2386.56
[INFO 2017-06-26 19:13:17,854 main.py:47] epoch 236, training loss: 2174.80, average training loss: 3173.15, base loss: 2385.80
[INFO 2017-06-26 19:13:18,137 main.py:47] epoch 237, training loss: 1852.39, average training loss: 3167.61, base loss: 2383.60
[INFO 2017-06-26 19:13:18,421 main.py:47] epoch 238, training loss: 2481.19, average training loss: 3164.73, base loss: 2384.25
[INFO 2017-06-26 19:13:18,707 main.py:47] epoch 239, training loss: 1942.58, average training loss: 3159.64, base loss: 2382.51
[INFO 2017-06-26 19:13:18,992 main.py:47] epoch 240, training loss: 2242.52, average training loss: 3155.84, base loss: 2382.08
[INFO 2017-06-26 19:13:19,275 main.py:47] epoch 241, training loss: 2191.37, average training loss: 3151.85, base loss: 2381.50
[INFO 2017-06-26 19:13:19,560 main.py:47] epoch 242, training loss: 2385.77, average training loss: 3148.70, base loss: 2381.73
[INFO 2017-06-26 19:13:19,843 main.py:47] epoch 243, training loss: 2192.56, average training loss: 3144.78, base loss: 2381.13
[INFO 2017-06-26 19:13:20,129 main.py:47] epoch 244, training loss: 2110.23, average training loss: 3140.56, base loss: 2380.50
[INFO 2017-06-26 19:13:20,411 main.py:47] epoch 245, training loss: 2237.31, average training loss: 3136.88, base loss: 2379.92
[INFO 2017-06-26 19:13:20,696 main.py:47] epoch 246, training loss: 2114.81, average training loss: 3132.75, base loss: 2378.77
[INFO 2017-06-26 19:13:20,981 main.py:47] epoch 247, training loss: 2792.26, average training loss: 3131.37, base loss: 2380.99
[INFO 2017-06-26 19:13:21,266 main.py:47] epoch 248, training loss: 1833.23, average training loss: 3126.16, base loss: 2378.89
[INFO 2017-06-26 19:13:21,547 main.py:47] epoch 249, training loss: 2482.84, average training loss: 3123.59, base loss: 2379.50
[INFO 2017-06-26 19:13:21,834 main.py:47] epoch 250, training loss: 2345.06, average training loss: 3120.49, base loss: 2379.45
[INFO 2017-06-26 19:13:22,117 main.py:47] epoch 251, training loss: 2299.42, average training loss: 3117.23, base loss: 2379.35
[INFO 2017-06-26 19:13:22,404 main.py:47] epoch 252, training loss: 2376.28, average training loss: 3114.30, base loss: 2379.61
[INFO 2017-06-26 19:13:22,689 main.py:47] epoch 253, training loss: 2373.64, average training loss: 3111.38, base loss: 2379.88
[INFO 2017-06-26 19:13:22,972 main.py:47] epoch 254, training loss: 1987.60, average training loss: 3106.98, base loss: 2378.47
[INFO 2017-06-26 19:13:23,256 main.py:47] epoch 255, training loss: 2229.74, average training loss: 3103.55, base loss: 2378.09
[INFO 2017-06-26 19:13:23,543 main.py:47] epoch 256, training loss: 2301.29, average training loss: 3100.43, base loss: 2377.91
[INFO 2017-06-26 19:13:23,827 main.py:47] epoch 257, training loss: 2215.24, average training loss: 3097.00, base loss: 2377.31
[INFO 2017-06-26 19:13:24,112 main.py:47] epoch 258, training loss: 2159.29, average training loss: 3093.38, base loss: 2376.52
[INFO 2017-06-26 19:13:24,398 main.py:47] epoch 259, training loss: 2130.88, average training loss: 3089.67, base loss: 2375.96
[INFO 2017-06-26 19:13:24,679 main.py:47] epoch 260, training loss: 2265.60, average training loss: 3086.52, base loss: 2375.94
[INFO 2017-06-26 19:13:24,967 main.py:47] epoch 261, training loss: 2703.63, average training loss: 3085.06, base loss: 2377.42
[INFO 2017-06-26 19:13:25,251 main.py:47] epoch 262, training loss: 2495.00, average training loss: 3082.81, base loss: 2377.97
[INFO 2017-06-26 19:13:25,535 main.py:47] epoch 263, training loss: 2485.14, average training loss: 3080.55, base loss: 2378.60
[INFO 2017-06-26 19:13:25,819 main.py:47] epoch 264, training loss: 2279.17, average training loss: 3077.52, base loss: 2378.52
[INFO 2017-06-26 19:13:26,101 main.py:47] epoch 265, training loss: 2436.98, average training loss: 3075.12, base loss: 2379.05
[INFO 2017-06-26 19:13:26,388 main.py:47] epoch 266, training loss: 1931.48, average training loss: 3070.83, base loss: 2377.40
[INFO 2017-06-26 19:13:26,675 main.py:47] epoch 267, training loss: 2094.23, average training loss: 3067.19, base loss: 2376.68
[INFO 2017-06-26 19:13:26,959 main.py:47] epoch 268, training loss: 2452.09, average training loss: 3064.90, base loss: 2377.40
[INFO 2017-06-26 19:13:27,244 main.py:47] epoch 269, training loss: 1611.58, average training loss: 3059.52, base loss: 2374.45
[INFO 2017-06-26 19:13:27,530 main.py:47] epoch 270, training loss: 2580.20, average training loss: 3057.75, base loss: 2375.59
[INFO 2017-06-26 19:13:27,816 main.py:47] epoch 271, training loss: 2224.59, average training loss: 3054.69, base loss: 2375.25
[INFO 2017-06-26 19:13:28,100 main.py:47] epoch 272, training loss: 2452.20, average training loss: 3052.48, base loss: 2375.87
[INFO 2017-06-26 19:13:28,382 main.py:47] epoch 273, training loss: 2180.19, average training loss: 3049.30, base loss: 2375.47
[INFO 2017-06-26 19:13:28,670 main.py:47] epoch 274, training loss: 1899.74, average training loss: 3045.12, base loss: 2373.94
[INFO 2017-06-26 19:13:28,958 main.py:47] epoch 275, training loss: 2369.72, average training loss: 3042.67, base loss: 2374.01
[INFO 2017-06-26 19:13:29,245 main.py:47] epoch 276, training loss: 2496.21, average training loss: 3040.70, base loss: 2374.74
[INFO 2017-06-26 19:13:29,533 main.py:47] epoch 277, training loss: 2184.45, average training loss: 3037.62, base loss: 2374.35
[INFO 2017-06-26 19:13:29,821 main.py:47] epoch 278, training loss: 2501.43, average training loss: 3035.69, base loss: 2375.09
[INFO 2017-06-26 19:13:30,109 main.py:47] epoch 279, training loss: 2926.93, average training loss: 3035.31, base loss: 2377.48
[INFO 2017-06-26 19:13:30,393 main.py:47] epoch 280, training loss: 2334.26, average training loss: 3032.81, base loss: 2377.72
[INFO 2017-06-26 19:13:30,677 main.py:47] epoch 281, training loss: 2330.68, average training loss: 3030.32, base loss: 2377.70
[INFO 2017-06-26 19:13:30,964 main.py:47] epoch 282, training loss: 2580.77, average training loss: 3028.73, base loss: 2378.75
[INFO 2017-06-26 19:13:31,252 main.py:47] epoch 283, training loss: 2197.48, average training loss: 3025.81, base loss: 2378.17
[INFO 2017-06-26 19:13:31,540 main.py:47] epoch 284, training loss: 2150.78, average training loss: 3022.74, base loss: 2377.52
[INFO 2017-06-26 19:13:31,823 main.py:47] epoch 285, training loss: 2306.10, average training loss: 3020.23, base loss: 2377.54
[INFO 2017-06-26 19:13:32,107 main.py:47] epoch 286, training loss: 2453.94, average training loss: 3018.26, base loss: 2378.19
[INFO 2017-06-26 19:13:32,394 main.py:47] epoch 287, training loss: 1922.24, average training loss: 3014.45, base loss: 2376.76
[INFO 2017-06-26 19:13:32,680 main.py:47] epoch 288, training loss: 2251.42, average training loss: 3011.81, base loss: 2376.59
[INFO 2017-06-26 19:13:32,968 main.py:47] epoch 289, training loss: 2618.44, average training loss: 3010.45, base loss: 2377.90
[INFO 2017-06-26 19:13:33,250 main.py:47] epoch 290, training loss: 2120.82, average training loss: 3007.40, base loss: 2377.22
[INFO 2017-06-26 19:13:33,535 main.py:47] epoch 291, training loss: 2182.66, average training loss: 3004.57, base loss: 2376.79
[INFO 2017-06-26 19:13:33,820 main.py:47] epoch 292, training loss: 2247.15, average training loss: 3001.99, base loss: 2376.58
[INFO 2017-06-26 19:13:34,102 main.py:47] epoch 293, training loss: 2002.54, average training loss: 2998.59, base loss: 2375.46
[INFO 2017-06-26 19:13:34,389 main.py:47] epoch 294, training loss: 2263.08, average training loss: 2996.10, base loss: 2375.37
[INFO 2017-06-26 19:13:34,676 main.py:47] epoch 295, training loss: 2293.62, average training loss: 2993.72, base loss: 2375.38
[INFO 2017-06-26 19:13:34,962 main.py:47] epoch 296, training loss: 1692.61, average training loss: 2989.34, base loss: 2373.14
[INFO 2017-06-26 19:13:35,243 main.py:47] epoch 297, training loss: 2592.15, average training loss: 2988.01, base loss: 2374.13
[INFO 2017-06-26 19:13:35,527 main.py:47] epoch 298, training loss: 2311.92, average training loss: 2985.75, base loss: 2374.20
[INFO 2017-06-26 19:13:35,812 main.py:47] epoch 299, training loss: 2303.34, average training loss: 2983.47, base loss: 2374.25
[INFO 2017-06-26 19:13:35,812 main.py:49] epoch 299, testing
[INFO 2017-06-26 19:13:39,568 main.py:100] average testing loss: 2258.45, base loss: 2319.97
[INFO 2017-06-26 19:13:39,593 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:13:39,606 main.py:73] current best accuracy: 2258.45
[INFO 2017-06-26 19:13:39,891 main.py:47] epoch 300, training loss: 2218.71, average training loss: 2980.93, base loss: 2374.11
[INFO 2017-06-26 19:13:40,173 main.py:47] epoch 301, training loss: 2271.55, average training loss: 2978.58, base loss: 2374.10
[INFO 2017-06-26 19:13:40,462 main.py:47] epoch 302, training loss: 2367.44, average training loss: 2976.57, base loss: 2374.36
[INFO 2017-06-26 19:13:40,747 main.py:47] epoch 303, training loss: 2428.29, average training loss: 2974.76, base loss: 2374.66
[INFO 2017-06-26 19:13:41,033 main.py:47] epoch 304, training loss: 2017.72, average training loss: 2971.62, base loss: 2373.67
[INFO 2017-06-26 19:13:41,321 main.py:47] epoch 305, training loss: 2737.24, average training loss: 2970.86, base loss: 2375.07
[INFO 2017-06-26 19:13:41,606 main.py:47] epoch 306, training loss: 2739.38, average training loss: 2970.10, base loss: 2376.50
[INFO 2017-06-26 19:13:41,894 main.py:47] epoch 307, training loss: 2081.74, average training loss: 2967.22, base loss: 2375.71
[INFO 2017-06-26 19:13:42,180 main.py:47] epoch 308, training loss: 2638.23, average training loss: 2966.16, base loss: 2376.99
[INFO 2017-06-26 19:13:42,461 main.py:47] epoch 309, training loss: 2461.36, average training loss: 2964.53, base loss: 2377.63
[INFO 2017-06-26 19:13:42,745 main.py:47] epoch 310, training loss: 2573.42, average training loss: 2963.27, base loss: 2378.53
[INFO 2017-06-26 19:13:43,026 main.py:47] epoch 311, training loss: 2129.81, average training loss: 2960.60, base loss: 2377.84
[INFO 2017-06-26 19:13:43,311 main.py:47] epoch 312, training loss: 2685.05, average training loss: 2959.72, base loss: 2379.24
[INFO 2017-06-26 19:13:43,593 main.py:47] epoch 313, training loss: 2289.65, average training loss: 2957.58, base loss: 2379.23
[INFO 2017-06-26 19:13:43,877 main.py:47] epoch 314, training loss: 2571.46, average training loss: 2956.36, base loss: 2380.18
[INFO 2017-06-26 19:13:44,163 main.py:47] epoch 315, training loss: 2432.61, average training loss: 2954.70, base loss: 2380.63
[INFO 2017-06-26 19:13:44,450 main.py:47] epoch 316, training loss: 2107.97, average training loss: 2952.03, base loss: 2380.05
[INFO 2017-06-26 19:13:44,735 main.py:47] epoch 317, training loss: 2108.95, average training loss: 2949.38, base loss: 2379.30
[INFO 2017-06-26 19:13:45,017 main.py:47] epoch 318, training loss: 2249.92, average training loss: 2947.19, base loss: 2379.16
[INFO 2017-06-26 19:13:45,304 main.py:47] epoch 319, training loss: 2369.88, average training loss: 2945.38, base loss: 2379.38
[INFO 2017-06-26 19:13:45,586 main.py:47] epoch 320, training loss: 2514.94, average training loss: 2944.04, base loss: 2380.12
[INFO 2017-06-26 19:13:45,874 main.py:47] epoch 321, training loss: 2299.80, average training loss: 2942.04, base loss: 2380.23
[INFO 2017-06-26 19:13:46,162 main.py:47] epoch 322, training loss: 2220.85, average training loss: 2939.81, base loss: 2380.10
[INFO 2017-06-26 19:13:46,446 main.py:47] epoch 323, training loss: 2927.78, average training loss: 2939.77, base loss: 2381.90
[INFO 2017-06-26 19:13:46,734 main.py:47] epoch 324, training loss: 2088.36, average training loss: 2937.15, base loss: 2381.18
[INFO 2017-06-26 19:13:47,022 main.py:47] epoch 325, training loss: 2566.30, average training loss: 2936.01, base loss: 2382.06
[INFO 2017-06-26 19:13:47,307 main.py:47] epoch 326, training loss: 2207.39, average training loss: 2933.78, base loss: 2381.58
[INFO 2017-06-26 19:13:47,592 main.py:47] epoch 327, training loss: 1971.56, average training loss: 2930.85, base loss: 2380.52
[INFO 2017-06-26 19:13:47,877 main.py:47] epoch 328, training loss: 2182.97, average training loss: 2928.58, base loss: 2380.12
[INFO 2017-06-26 19:13:48,162 main.py:47] epoch 329, training loss: 2107.36, average training loss: 2926.09, base loss: 2379.38
[INFO 2017-06-26 19:13:48,449 main.py:47] epoch 330, training loss: 2348.86, average training loss: 2924.35, base loss: 2379.53
[INFO 2017-06-26 19:13:48,738 main.py:47] epoch 331, training loss: 1741.03, average training loss: 2920.78, base loss: 2377.65
[INFO 2017-06-26 19:13:49,024 main.py:47] epoch 332, training loss: 2206.95, average training loss: 2918.64, base loss: 2377.30
[INFO 2017-06-26 19:13:49,307 main.py:47] epoch 333, training loss: 2021.72, average training loss: 2915.95, base loss: 2376.41
[INFO 2017-06-26 19:13:49,593 main.py:47] epoch 334, training loss: 2167.17, average training loss: 2913.72, base loss: 2375.87
[INFO 2017-06-26 19:13:49,876 main.py:47] epoch 335, training loss: 2361.12, average training loss: 2912.07, base loss: 2376.07
[INFO 2017-06-26 19:13:50,161 main.py:47] epoch 336, training loss: 2688.22, average training loss: 2911.41, base loss: 2377.29
[INFO 2017-06-26 19:13:50,442 main.py:47] epoch 337, training loss: 1999.92, average training loss: 2908.71, base loss: 2376.29
[INFO 2017-06-26 19:13:50,726 main.py:47] epoch 338, training loss: 2966.47, average training loss: 2908.88, base loss: 2378.49
[INFO 2017-06-26 19:13:51,010 main.py:47] epoch 339, training loss: 1926.08, average training loss: 2905.99, base loss: 2377.11
[INFO 2017-06-26 19:13:51,296 main.py:47] epoch 340, training loss: 2282.85, average training loss: 2904.16, base loss: 2376.98
[INFO 2017-06-26 19:13:51,580 main.py:47] epoch 341, training loss: 2489.07, average training loss: 2902.95, base loss: 2377.35
[INFO 2017-06-26 19:13:51,865 main.py:47] epoch 342, training loss: 2292.38, average training loss: 2901.17, base loss: 2377.29
[INFO 2017-06-26 19:13:52,152 main.py:47] epoch 343, training loss: 2130.68, average training loss: 2898.93, base loss: 2376.72
[INFO 2017-06-26 19:13:52,439 main.py:47] epoch 344, training loss: 2485.81, average training loss: 2897.73, base loss: 2377.22
[INFO 2017-06-26 19:13:52,723 main.py:47] epoch 345, training loss: 1890.21, average training loss: 2894.82, base loss: 2375.89
[INFO 2017-06-26 19:13:53,010 main.py:47] epoch 346, training loss: 2275.81, average training loss: 2893.04, base loss: 2375.78
[INFO 2017-06-26 19:13:53,297 main.py:47] epoch 347, training loss: 1987.33, average training loss: 2890.43, base loss: 2374.74
[INFO 2017-06-26 19:13:53,581 main.py:47] epoch 348, training loss: 2182.94, average training loss: 2888.41, base loss: 2374.33
[INFO 2017-06-26 19:13:53,869 main.py:47] epoch 349, training loss: 2290.48, average training loss: 2886.70, base loss: 2374.34
[INFO 2017-06-26 19:13:54,156 main.py:47] epoch 350, training loss: 2100.65, average training loss: 2884.46, base loss: 2373.73
[INFO 2017-06-26 19:13:54,441 main.py:47] epoch 351, training loss: 2546.64, average training loss: 2883.50, base loss: 2374.44
[INFO 2017-06-26 19:13:54,722 main.py:47] epoch 352, training loss: 2060.20, average training loss: 2881.17, base loss: 2373.83
[INFO 2017-06-26 19:13:55,003 main.py:47] epoch 353, training loss: 2318.76, average training loss: 2879.58, base loss: 2373.90
[INFO 2017-06-26 19:13:55,289 main.py:47] epoch 354, training loss: 2508.56, average training loss: 2878.53, base loss: 2374.51
[INFO 2017-06-26 19:13:55,570 main.py:47] epoch 355, training loss: 2112.52, average training loss: 2876.38, base loss: 2374.01
[INFO 2017-06-26 19:13:55,851 main.py:47] epoch 356, training loss: 2063.96, average training loss: 2874.11, base loss: 2373.30
[INFO 2017-06-26 19:13:56,136 main.py:47] epoch 357, training loss: 2464.45, average training loss: 2872.96, base loss: 2373.79
[INFO 2017-06-26 19:13:56,423 main.py:47] epoch 358, training loss: 2411.84, average training loss: 2871.68, base loss: 2374.25
[INFO 2017-06-26 19:13:56,710 main.py:47] epoch 359, training loss: 2366.50, average training loss: 2870.27, base loss: 2374.27
[INFO 2017-06-26 19:13:56,992 main.py:47] epoch 360, training loss: 2831.17, average training loss: 2870.17, base loss: 2375.99
[INFO 2017-06-26 19:13:57,277 main.py:47] epoch 361, training loss: 1970.28, average training loss: 2867.68, base loss: 2375.07
[INFO 2017-06-26 19:13:57,562 main.py:47] epoch 362, training loss: 2092.71, average training loss: 2865.54, base loss: 2374.55
[INFO 2017-06-26 19:13:57,843 main.py:47] epoch 363, training loss: 2228.33, average training loss: 2863.79, base loss: 2374.39
[INFO 2017-06-26 19:13:58,127 main.py:47] epoch 364, training loss: 2623.30, average training loss: 2863.14, base loss: 2375.16
[INFO 2017-06-26 19:13:58,411 main.py:47] epoch 365, training loss: 1929.56, average training loss: 2860.58, base loss: 2374.17
[INFO 2017-06-26 19:13:58,698 main.py:47] epoch 366, training loss: 2052.98, average training loss: 2858.38, base loss: 2373.46
[INFO 2017-06-26 19:13:58,983 main.py:47] epoch 367, training loss: 2297.00, average training loss: 2856.86, base loss: 2373.50
[INFO 2017-06-26 19:13:59,268 main.py:47] epoch 368, training loss: 2195.63, average training loss: 2855.07, base loss: 2373.21
[INFO 2017-06-26 19:13:59,550 main.py:47] epoch 369, training loss: 2396.97, average training loss: 2853.83, base loss: 2373.50
[INFO 2017-06-26 19:13:59,835 main.py:47] epoch 370, training loss: 2216.20, average training loss: 2852.11, base loss: 2373.32
[INFO 2017-06-26 19:14:00,121 main.py:47] epoch 371, training loss: 2097.35, average training loss: 2850.08, base loss: 2372.62
[INFO 2017-06-26 19:14:00,408 main.py:47] epoch 372, training loss: 2504.69, average training loss: 2849.15, base loss: 2373.28
[INFO 2017-06-26 19:14:00,693 main.py:47] epoch 373, training loss: 2516.01, average training loss: 2848.26, base loss: 2373.91
[INFO 2017-06-26 19:14:00,977 main.py:47] epoch 374, training loss: 2639.13, average training loss: 2847.71, base loss: 2375.01
[INFO 2017-06-26 19:14:01,265 main.py:47] epoch 375, training loss: 2412.39, average training loss: 2846.55, base loss: 2375.38
[INFO 2017-06-26 19:14:01,550 main.py:47] epoch 376, training loss: 2214.70, average training loss: 2844.87, base loss: 2375.03
[INFO 2017-06-26 19:14:01,833 main.py:47] epoch 377, training loss: 2539.91, average training loss: 2844.07, base loss: 2375.75
[INFO 2017-06-26 19:14:02,121 main.py:47] epoch 378, training loss: 2517.17, average training loss: 2843.20, base loss: 2376.11
[INFO 2017-06-26 19:14:02,410 main.py:47] epoch 379, training loss: 2054.39, average training loss: 2841.13, base loss: 2375.35
[INFO 2017-06-26 19:14:02,694 main.py:47] epoch 380, training loss: 1923.37, average training loss: 2838.72, base loss: 2374.42
[INFO 2017-06-26 19:14:02,976 main.py:47] epoch 381, training loss: 2599.08, average training loss: 2838.09, base loss: 2375.24
[INFO 2017-06-26 19:14:03,262 main.py:47] epoch 382, training loss: 2272.05, average training loss: 2836.61, base loss: 2375.12
[INFO 2017-06-26 19:14:03,543 main.py:47] epoch 383, training loss: 1733.79, average training loss: 2833.74, base loss: 2373.50
[INFO 2017-06-26 19:14:03,829 main.py:47] epoch 384, training loss: 2162.33, average training loss: 2832.00, base loss: 2373.07
[INFO 2017-06-26 19:14:04,115 main.py:47] epoch 385, training loss: 2228.78, average training loss: 2830.44, base loss: 2372.86
[INFO 2017-06-26 19:14:04,401 main.py:47] epoch 386, training loss: 2162.97, average training loss: 2828.71, base loss: 2372.49
[INFO 2017-06-26 19:14:04,687 main.py:47] epoch 387, training loss: 2279.51, average training loss: 2827.29, base loss: 2372.37
[INFO 2017-06-26 19:14:04,972 main.py:47] epoch 388, training loss: 2324.50, average training loss: 2826.00, base loss: 2372.33
[INFO 2017-06-26 19:14:05,257 main.py:47] epoch 389, training loss: 2159.50, average training loss: 2824.29, base loss: 2371.98
[INFO 2017-06-26 19:14:05,542 main.py:47] epoch 390, training loss: 2411.82, average training loss: 2823.24, base loss: 2372.37
[INFO 2017-06-26 19:14:05,826 main.py:47] epoch 391, training loss: 2584.70, average training loss: 2822.63, base loss: 2373.23
[INFO 2017-06-26 19:14:06,108 main.py:47] epoch 392, training loss: 2092.20, average training loss: 2820.77, base loss: 2372.77
[INFO 2017-06-26 19:14:06,388 main.py:47] epoch 393, training loss: 2191.35, average training loss: 2819.17, base loss: 2372.45
[INFO 2017-06-26 19:14:06,674 main.py:47] epoch 394, training loss: 2520.30, average training loss: 2818.42, base loss: 2373.16
[INFO 2017-06-26 19:14:06,961 main.py:47] epoch 395, training loss: 2447.45, average training loss: 2817.48, base loss: 2373.78
[INFO 2017-06-26 19:14:07,248 main.py:47] epoch 396, training loss: 1981.03, average training loss: 2815.37, base loss: 2372.97
[INFO 2017-06-26 19:14:07,535 main.py:47] epoch 397, training loss: 2074.08, average training loss: 2813.51, base loss: 2372.47
[INFO 2017-06-26 19:14:07,823 main.py:47] epoch 398, training loss: 2219.12, average training loss: 2812.02, base loss: 2372.01
[INFO 2017-06-26 19:14:08,109 main.py:47] epoch 399, training loss: 2310.07, average training loss: 2810.77, base loss: 2372.05
[INFO 2017-06-26 19:14:08,109 main.py:49] epoch 399, testing
[INFO 2017-06-26 19:14:11,866 main.py:100] average testing loss: 2234.97, base loss: 2320.55
[INFO 2017-06-26 19:14:11,891 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:14:11,904 main.py:73] current best accuracy: 2234.97
[INFO 2017-06-26 19:14:12,188 main.py:47] epoch 400, training loss: 2276.07, average training loss: 2809.43, base loss: 2372.09
[INFO 2017-06-26 19:14:12,469 main.py:47] epoch 401, training loss: 2260.28, average training loss: 2808.07, base loss: 2372.02
[INFO 2017-06-26 19:14:12,754 main.py:47] epoch 402, training loss: 2132.76, average training loss: 2806.39, base loss: 2371.63
[INFO 2017-06-26 19:14:13,042 main.py:47] epoch 403, training loss: 2183.91, average training loss: 2804.85, base loss: 2371.29
[INFO 2017-06-26 19:14:13,330 main.py:47] epoch 404, training loss: 2143.72, average training loss: 2803.22, base loss: 2370.87
[INFO 2017-06-26 19:14:13,614 main.py:47] epoch 405, training loss: 2254.80, average training loss: 2801.87, base loss: 2370.76
[INFO 2017-06-26 19:14:13,901 main.py:47] epoch 406, training loss: 2570.60, average training loss: 2801.30, base loss: 2371.48
[INFO 2017-06-26 19:14:14,184 main.py:47] epoch 407, training loss: 2178.43, average training loss: 2799.77, base loss: 2371.24
[INFO 2017-06-26 19:14:14,470 main.py:47] epoch 408, training loss: 2135.91, average training loss: 2798.15, base loss: 2370.72
[INFO 2017-06-26 19:14:14,752 main.py:47] epoch 409, training loss: 2371.90, average training loss: 2797.11, base loss: 2371.01
[INFO 2017-06-26 19:14:15,037 main.py:47] epoch 410, training loss: 2131.26, average training loss: 2795.49, base loss: 2370.52
[INFO 2017-06-26 19:14:15,323 main.py:47] epoch 411, training loss: 2081.49, average training loss: 2793.76, base loss: 2370.01
[INFO 2017-06-26 19:14:15,605 main.py:47] epoch 412, training loss: 2539.90, average training loss: 2793.14, base loss: 2370.73
[INFO 2017-06-26 19:14:15,894 main.py:47] epoch 413, training loss: 2480.79, average training loss: 2792.39, base loss: 2371.22
[INFO 2017-06-26 19:14:16,187 main.py:47] epoch 414, training loss: 2357.13, average training loss: 2791.34, base loss: 2371.44
[INFO 2017-06-26 19:14:16,470 main.py:47] epoch 415, training loss: 2367.94, average training loss: 2790.32, base loss: 2371.58
[INFO 2017-06-26 19:14:16,757 main.py:47] epoch 416, training loss: 2008.85, average training loss: 2788.45, base loss: 2370.83
[INFO 2017-06-26 19:14:17,045 main.py:47] epoch 417, training loss: 1980.15, average training loss: 2786.51, base loss: 2370.17
[INFO 2017-06-26 19:14:17,326 main.py:47] epoch 418, training loss: 2134.23, average training loss: 2784.96, base loss: 2369.64
[INFO 2017-06-26 19:14:17,613 main.py:47] epoch 419, training loss: 1807.95, average training loss: 2782.63, base loss: 2368.33
[INFO 2017-06-26 19:14:17,896 main.py:47] epoch 420, training loss: 1931.22, average training loss: 2780.61, base loss: 2367.32
[INFO 2017-06-26 19:14:18,181 main.py:47] epoch 421, training loss: 2607.27, average training loss: 2780.20, base loss: 2368.05
[INFO 2017-06-26 19:14:18,463 main.py:47] epoch 422, training loss: 2167.73, average training loss: 2778.75, base loss: 2367.82
[INFO 2017-06-26 19:14:18,748 main.py:47] epoch 423, training loss: 2335.09, average training loss: 2777.70, base loss: 2367.89
[INFO 2017-06-26 19:14:19,035 main.py:47] epoch 424, training loss: 2384.01, average training loss: 2776.78, base loss: 2368.11
[INFO 2017-06-26 19:14:19,319 main.py:47] epoch 425, training loss: 2766.26, average training loss: 2776.75, base loss: 2369.25
[INFO 2017-06-26 19:14:19,606 main.py:47] epoch 426, training loss: 2131.43, average training loss: 2775.24, base loss: 2368.81
[INFO 2017-06-26 19:14:19,889 main.py:47] epoch 427, training loss: 2217.96, average training loss: 2773.94, base loss: 2368.68
[INFO 2017-06-26 19:14:20,171 main.py:47] epoch 428, training loss: 2200.97, average training loss: 2772.60, base loss: 2368.44
[INFO 2017-06-26 19:14:20,452 main.py:47] epoch 429, training loss: 2317.78, average training loss: 2771.54, base loss: 2368.63
[INFO 2017-06-26 19:14:20,737 main.py:47] epoch 430, training loss: 2191.83, average training loss: 2770.20, base loss: 2368.57
[INFO 2017-06-26 19:14:21,022 main.py:47] epoch 431, training loss: 2248.15, average training loss: 2768.99, base loss: 2368.61
[INFO 2017-06-26 19:14:21,306 main.py:47] epoch 432, training loss: 2295.74, average training loss: 2767.90, base loss: 2368.69
[INFO 2017-06-26 19:14:21,593 main.py:47] epoch 433, training loss: 1990.99, average training loss: 2766.11, base loss: 2367.73
[INFO 2017-06-26 19:14:21,877 main.py:47] epoch 434, training loss: 1960.45, average training loss: 2764.26, base loss: 2366.57
[INFO 2017-06-26 19:14:22,164 main.py:47] epoch 435, training loss: 2116.86, average training loss: 2762.77, base loss: 2366.21
[INFO 2017-06-26 19:14:22,452 main.py:47] epoch 436, training loss: 2489.14, average training loss: 2762.15, base loss: 2366.81
[INFO 2017-06-26 19:14:22,736 main.py:47] epoch 437, training loss: 2286.18, average training loss: 2761.06, base loss: 2366.86
[INFO 2017-06-26 19:14:23,024 main.py:47] epoch 438, training loss: 2573.28, average training loss: 2760.63, base loss: 2367.52
[INFO 2017-06-26 19:14:23,309 main.py:47] epoch 439, training loss: 2981.85, average training loss: 2761.13, base loss: 2369.20
[INFO 2017-06-26 19:14:23,593 main.py:47] epoch 440, training loss: 2536.02, average training loss: 2760.62, base loss: 2369.73
[INFO 2017-06-26 19:14:23,877 main.py:47] epoch 441, training loss: 2121.78, average training loss: 2759.18, base loss: 2369.22
[INFO 2017-06-26 19:14:24,163 main.py:47] epoch 442, training loss: 2497.44, average training loss: 2758.59, base loss: 2369.78
[INFO 2017-06-26 19:14:24,445 main.py:47] epoch 443, training loss: 2233.29, average training loss: 2757.40, base loss: 2369.71
[INFO 2017-06-26 19:14:24,730 main.py:47] epoch 444, training loss: 2074.31, average training loss: 2755.87, base loss: 2369.18
[INFO 2017-06-26 19:14:25,017 main.py:47] epoch 445, training loss: 2276.69, average training loss: 2754.79, base loss: 2369.24
[INFO 2017-06-26 19:14:25,304 main.py:47] epoch 446, training loss: 2209.71, average training loss: 2753.58, base loss: 2369.03
[INFO 2017-06-26 19:14:25,586 main.py:47] epoch 447, training loss: 2361.62, average training loss: 2752.70, base loss: 2369.34
[INFO 2017-06-26 19:14:25,874 main.py:47] epoch 448, training loss: 2218.84, average training loss: 2751.51, base loss: 2369.38
[INFO 2017-06-26 19:14:26,161 main.py:47] epoch 449, training loss: 2261.70, average training loss: 2750.42, base loss: 2369.24
[INFO 2017-06-26 19:14:26,452 main.py:47] epoch 450, training loss: 2260.84, average training loss: 2749.34, base loss: 2369.27
[INFO 2017-06-26 19:14:26,744 main.py:47] epoch 451, training loss: 2082.60, average training loss: 2747.86, base loss: 2368.79
[INFO 2017-06-26 19:14:27,032 main.py:47] epoch 452, training loss: 2294.12, average training loss: 2746.86, base loss: 2368.89
[INFO 2017-06-26 19:14:27,321 main.py:47] epoch 453, training loss: 2318.11, average training loss: 2745.92, base loss: 2369.14
[INFO 2017-06-26 19:14:27,608 main.py:47] epoch 454, training loss: 1885.19, average training loss: 2744.02, base loss: 2368.19
[INFO 2017-06-26 19:14:27,891 main.py:47] epoch 455, training loss: 2705.43, average training loss: 2743.94, base loss: 2369.32
[INFO 2017-06-26 19:14:28,178 main.py:47] epoch 456, training loss: 2011.11, average training loss: 2742.34, base loss: 2368.55
[INFO 2017-06-26 19:14:28,462 main.py:47] epoch 457, training loss: 2889.60, average training loss: 2742.66, base loss: 2370.19
[INFO 2017-06-26 19:14:28,744 main.py:47] epoch 458, training loss: 2289.99, average training loss: 2741.67, base loss: 2370.41
[INFO 2017-06-26 19:14:29,025 main.py:47] epoch 459, training loss: 2368.12, average training loss: 2740.86, base loss: 2370.72
[INFO 2017-06-26 19:14:29,309 main.py:47] epoch 460, training loss: 2109.31, average training loss: 2739.49, base loss: 2370.36
[INFO 2017-06-26 19:14:29,598 main.py:47] epoch 461, training loss: 2013.90, average training loss: 2737.92, base loss: 2369.82
[INFO 2017-06-26 19:14:29,882 main.py:47] epoch 462, training loss: 2624.20, average training loss: 2737.67, base loss: 2370.70
[INFO 2017-06-26 19:14:30,165 main.py:47] epoch 463, training loss: 2232.54, average training loss: 2736.58, base loss: 2370.42
[INFO 2017-06-26 19:14:30,450 main.py:47] epoch 464, training loss: 2083.67, average training loss: 2735.18, base loss: 2370.14
[INFO 2017-06-26 19:14:30,735 main.py:47] epoch 465, training loss: 2643.90, average training loss: 2734.98, base loss: 2371.20
[INFO 2017-06-26 19:14:31,024 main.py:47] epoch 466, training loss: 2160.94, average training loss: 2733.76, base loss: 2370.95
[INFO 2017-06-26 19:14:31,306 main.py:47] epoch 467, training loss: 2236.04, average training loss: 2732.69, base loss: 2370.86
[INFO 2017-06-26 19:14:31,593 main.py:47] epoch 468, training loss: 2890.53, average training loss: 2733.03, base loss: 2372.42
[INFO 2017-06-26 19:14:31,880 main.py:47] epoch 469, training loss: 1927.37, average training loss: 2731.31, base loss: 2371.81
[INFO 2017-06-26 19:14:32,162 main.py:47] epoch 470, training loss: 2103.69, average training loss: 2729.98, base loss: 2371.40
[INFO 2017-06-26 19:14:32,447 main.py:47] epoch 471, training loss: 1949.47, average training loss: 2728.33, base loss: 2370.74
[INFO 2017-06-26 19:14:32,733 main.py:47] epoch 472, training loss: 2391.73, average training loss: 2727.62, base loss: 2370.90
[INFO 2017-06-26 19:14:33,018 main.py:47] epoch 473, training loss: 2159.36, average training loss: 2726.42, base loss: 2370.68
[INFO 2017-06-26 19:14:33,304 main.py:47] epoch 474, training loss: 2041.73, average training loss: 2724.98, base loss: 2370.36
[INFO 2017-06-26 19:14:33,588 main.py:47] epoch 475, training loss: 2145.22, average training loss: 2723.76, base loss: 2370.02
[INFO 2017-06-26 19:14:33,873 main.py:47] epoch 476, training loss: 2054.31, average training loss: 2722.35, base loss: 2369.65
[INFO 2017-06-26 19:14:34,159 main.py:47] epoch 477, training loss: 2098.70, average training loss: 2721.05, base loss: 2369.33
[INFO 2017-06-26 19:14:34,444 main.py:47] epoch 478, training loss: 2000.17, average training loss: 2719.55, base loss: 2368.65
[INFO 2017-06-26 19:14:34,726 main.py:47] epoch 479, training loss: 2215.66, average training loss: 2718.50, base loss: 2368.67
[INFO 2017-06-26 19:14:35,011 main.py:47] epoch 480, training loss: 2530.16, average training loss: 2718.10, base loss: 2369.03
[INFO 2017-06-26 19:14:35,296 main.py:47] epoch 481, training loss: 2199.43, average training loss: 2717.03, base loss: 2368.99
[INFO 2017-06-26 19:14:35,581 main.py:47] epoch 482, training loss: 2378.03, average training loss: 2716.33, base loss: 2369.07
[INFO 2017-06-26 19:14:35,863 main.py:47] epoch 483, training loss: 2358.29, average training loss: 2715.59, base loss: 2369.18
[INFO 2017-06-26 19:14:36,145 main.py:47] epoch 484, training loss: 1901.34, average training loss: 2713.91, base loss: 2368.30
[INFO 2017-06-26 19:14:36,432 main.py:47] epoch 485, training loss: 2011.40, average training loss: 2712.46, base loss: 2367.71
[INFO 2017-06-26 19:14:36,720 main.py:47] epoch 486, training loss: 2259.14, average training loss: 2711.53, base loss: 2367.65
[INFO 2017-06-26 19:14:37,005 main.py:47] epoch 487, training loss: 2381.56, average training loss: 2710.85, base loss: 2367.82
[INFO 2017-06-26 19:14:37,292 main.py:47] epoch 488, training loss: 1947.17, average training loss: 2709.29, base loss: 2367.15
[INFO 2017-06-26 19:14:37,581 main.py:47] epoch 489, training loss: 2795.85, average training loss: 2709.47, base loss: 2368.27
[INFO 2017-06-26 19:14:37,870 main.py:47] epoch 490, training loss: 2260.67, average training loss: 2708.56, base loss: 2368.22
[INFO 2017-06-26 19:14:38,155 main.py:47] epoch 491, training loss: 2587.22, average training loss: 2708.31, base loss: 2368.89
[INFO 2017-06-26 19:14:38,441 main.py:47] epoch 492, training loss: 2512.87, average training loss: 2707.91, base loss: 2369.24
[INFO 2017-06-26 19:14:38,723 main.py:47] epoch 493, training loss: 2102.25, average training loss: 2706.69, base loss: 2368.86
[INFO 2017-06-26 19:14:39,007 main.py:47] epoch 494, training loss: 2012.97, average training loss: 2705.29, base loss: 2368.32
[INFO 2017-06-26 19:14:39,292 main.py:47] epoch 495, training loss: 2390.53, average training loss: 2704.65, base loss: 2368.48
[INFO 2017-06-26 19:14:39,576 main.py:47] epoch 496, training loss: 1841.56, average training loss: 2702.91, base loss: 2367.50
[INFO 2017-06-26 19:14:39,865 main.py:47] epoch 497, training loss: 2187.94, average training loss: 2701.88, base loss: 2367.36
[INFO 2017-06-26 19:14:40,153 main.py:47] epoch 498, training loss: 2874.21, average training loss: 2702.23, base loss: 2368.74
[INFO 2017-06-26 19:14:40,440 main.py:47] epoch 499, training loss: 2406.34, average training loss: 2701.63, base loss: 2368.90
[INFO 2017-06-26 19:14:40,440 main.py:49] epoch 499, testing
[INFO 2017-06-26 19:14:44,173 main.py:100] average testing loss: 2436.04, base loss: 2571.75
[INFO 2017-06-26 19:14:44,197 main.py:73] current best accuracy: 2234.97
[INFO 2017-06-26 19:14:44,478 main.py:47] epoch 500, training loss: 2101.79, average training loss: 2700.44, base loss: 2368.50
[INFO 2017-06-26 19:14:44,762 main.py:47] epoch 501, training loss: 2285.26, average training loss: 2699.61, base loss: 2368.58
[INFO 2017-06-26 19:14:45,048 main.py:47] epoch 502, training loss: 2420.02, average training loss: 2699.05, base loss: 2368.96
[INFO 2017-06-26 19:14:45,334 main.py:47] epoch 503, training loss: 2546.25, average training loss: 2698.75, base loss: 2369.50
[INFO 2017-06-26 19:14:45,622 main.py:47] epoch 504, training loss: 2798.85, average training loss: 2698.95, base loss: 2370.42
[INFO 2017-06-26 19:14:45,908 main.py:47] epoch 505, training loss: 2045.07, average training loss: 2697.66, base loss: 2369.96
[INFO 2017-06-26 19:14:46,192 main.py:47] epoch 506, training loss: 1978.66, average training loss: 2696.24, base loss: 2369.11
[INFO 2017-06-26 19:14:46,474 main.py:47] epoch 507, training loss: 2221.35, average training loss: 2695.30, base loss: 2369.11
[INFO 2017-06-26 19:14:46,759 main.py:47] epoch 508, training loss: 2140.69, average training loss: 2694.21, base loss: 2368.91
[INFO 2017-06-26 19:14:47,045 main.py:47] epoch 509, training loss: 2566.77, average training loss: 2693.96, base loss: 2369.38
[INFO 2017-06-26 19:14:47,330 main.py:47] epoch 510, training loss: 2561.45, average training loss: 2693.70, base loss: 2370.09
[INFO 2017-06-26 19:14:47,613 main.py:47] epoch 511, training loss: 3009.50, average training loss: 2694.32, base loss: 2371.72
[INFO 2017-06-26 19:14:47,896 main.py:47] epoch 512, training loss: 2715.71, average training loss: 2694.36, base loss: 2372.71
[INFO 2017-06-26 19:14:48,182 main.py:47] epoch 513, training loss: 1949.51, average training loss: 2692.91, base loss: 2371.98
[INFO 2017-06-26 19:14:48,464 main.py:47] epoch 514, training loss: 2469.69, average training loss: 2692.48, base loss: 2372.46
[INFO 2017-06-26 19:14:48,748 main.py:47] epoch 515, training loss: 2301.44, average training loss: 2691.72, base loss: 2372.56
[INFO 2017-06-26 19:14:49,030 main.py:47] epoch 516, training loss: 2120.88, average training loss: 2690.62, base loss: 2372.30
[INFO 2017-06-26 19:14:49,313 main.py:47] epoch 517, training loss: 2467.82, average training loss: 2690.19, base loss: 2372.84
[INFO 2017-06-26 19:14:49,597 main.py:47] epoch 518, training loss: 2342.07, average training loss: 2689.52, base loss: 2372.96
[INFO 2017-06-26 19:14:49,882 main.py:47] epoch 519, training loss: 2645.92, average training loss: 2689.43, base loss: 2373.83
[INFO 2017-06-26 19:14:50,165 main.py:47] epoch 520, training loss: 2133.35, average training loss: 2688.37, base loss: 2373.54
[INFO 2017-06-26 19:14:50,451 main.py:47] epoch 521, training loss: 2084.28, average training loss: 2687.21, base loss: 2373.04
[INFO 2017-06-26 19:14:50,732 main.py:47] epoch 522, training loss: 2564.28, average training loss: 2686.97, base loss: 2373.62
[INFO 2017-06-26 19:14:51,017 main.py:47] epoch 523, training loss: 2329.55, average training loss: 2686.29, base loss: 2373.67
[INFO 2017-06-26 19:14:51,299 main.py:47] epoch 524, training loss: 2200.96, average training loss: 2685.37, base loss: 2373.55
[INFO 2017-06-26 19:14:51,584 main.py:47] epoch 525, training loss: 2101.77, average training loss: 2684.26, base loss: 2373.20
[INFO 2017-06-26 19:14:51,873 main.py:47] epoch 526, training loss: 2019.84, average training loss: 2683.00, base loss: 2372.72
[INFO 2017-06-26 19:14:52,158 main.py:47] epoch 527, training loss: 2200.53, average training loss: 2682.08, base loss: 2372.39
[INFO 2017-06-26 19:14:52,444 main.py:47] epoch 528, training loss: 2465.21, average training loss: 2681.67, base loss: 2372.80
[INFO 2017-06-26 19:14:52,730 main.py:47] epoch 529, training loss: 2685.60, average training loss: 2681.68, base loss: 2373.73
[INFO 2017-06-26 19:14:53,017 main.py:47] epoch 530, training loss: 2284.58, average training loss: 2680.93, base loss: 2373.67
[INFO 2017-06-26 19:14:53,300 main.py:47] epoch 531, training loss: 2628.41, average training loss: 2680.83, base loss: 2374.39
[INFO 2017-06-26 19:14:53,586 main.py:47] epoch 532, training loss: 2037.97, average training loss: 2679.63, base loss: 2373.91
[INFO 2017-06-26 19:14:53,871 main.py:47] epoch 533, training loss: 3026.54, average training loss: 2680.28, base loss: 2375.49
[INFO 2017-06-26 19:14:54,152 main.py:47] epoch 534, training loss: 2566.32, average training loss: 2680.06, base loss: 2376.17
[INFO 2017-06-26 19:14:54,436 main.py:47] epoch 535, training loss: 2425.04, average training loss: 2679.59, base loss: 2376.33
[INFO 2017-06-26 19:14:54,723 main.py:47] epoch 536, training loss: 2267.42, average training loss: 2678.82, base loss: 2376.50
[INFO 2017-06-26 19:14:55,005 main.py:47] epoch 537, training loss: 2240.88, average training loss: 2678.01, base loss: 2376.49
[INFO 2017-06-26 19:14:55,290 main.py:47] epoch 538, training loss: 2552.71, average training loss: 2677.77, base loss: 2377.16
[INFO 2017-06-26 19:14:55,574 main.py:47] epoch 539, training loss: 2474.71, average training loss: 2677.40, base loss: 2377.77
[INFO 2017-06-26 19:14:55,862 main.py:47] epoch 540, training loss: 2315.42, average training loss: 2676.73, base loss: 2377.95
[INFO 2017-06-26 19:14:56,151 main.py:47] epoch 541, training loss: 2899.78, average training loss: 2677.14, base loss: 2379.34
[INFO 2017-06-26 19:14:56,440 main.py:47] epoch 542, training loss: 2272.87, average training loss: 2676.40, base loss: 2379.38
[INFO 2017-06-26 19:14:56,726 main.py:47] epoch 543, training loss: 2229.76, average training loss: 2675.58, base loss: 2379.21
[INFO 2017-06-26 19:14:57,010 main.py:47] epoch 544, training loss: 2358.21, average training loss: 2674.99, base loss: 2379.38
[INFO 2017-06-26 19:14:57,303 main.py:47] epoch 545, training loss: 2386.20, average training loss: 2674.46, base loss: 2379.58
[INFO 2017-06-26 19:14:57,591 main.py:47] epoch 546, training loss: 2244.84, average training loss: 2673.68, base loss: 2379.54
[INFO 2017-06-26 19:14:57,878 main.py:47] epoch 547, training loss: 2336.17, average training loss: 2673.06, base loss: 2379.31
[INFO 2017-06-26 19:14:58,168 main.py:47] epoch 548, training loss: 2072.66, average training loss: 2671.97, base loss: 2378.96
[INFO 2017-06-26 19:14:58,454 main.py:47] epoch 549, training loss: 1965.05, average training loss: 2670.68, base loss: 2378.33
[INFO 2017-06-26 19:14:58,739 main.py:47] epoch 550, training loss: 2243.02, average training loss: 2669.91, base loss: 2378.25
[INFO 2017-06-26 19:14:59,022 main.py:47] epoch 551, training loss: 2381.85, average training loss: 2669.39, base loss: 2378.40
[INFO 2017-06-26 19:14:59,307 main.py:47] epoch 552, training loss: 2567.97, average training loss: 2669.20, base loss: 2378.88
[INFO 2017-06-26 19:14:59,592 main.py:47] epoch 553, training loss: 1998.07, average training loss: 2667.99, base loss: 2378.28
[INFO 2017-06-26 19:14:59,879 main.py:47] epoch 554, training loss: 2136.59, average training loss: 2667.03, base loss: 2377.91
[INFO 2017-06-26 19:15:00,167 main.py:47] epoch 555, training loss: 2670.48, average training loss: 2667.04, base loss: 2378.65
[INFO 2017-06-26 19:15:00,454 main.py:47] epoch 556, training loss: 1753.26, average training loss: 2665.40, base loss: 2377.66
[INFO 2017-06-26 19:15:00,740 main.py:47] epoch 557, training loss: 2613.11, average training loss: 2665.31, base loss: 2378.20
[INFO 2017-06-26 19:15:01,023 main.py:47] epoch 558, training loss: 2048.29, average training loss: 2664.20, base loss: 2377.71
[INFO 2017-06-26 19:15:01,307 main.py:47] epoch 559, training loss: 2055.29, average training loss: 2663.11, base loss: 2377.25
[INFO 2017-06-26 19:15:01,592 main.py:47] epoch 560, training loss: 2165.85, average training loss: 2662.23, base loss: 2376.95
[INFO 2017-06-26 19:15:01,879 main.py:47] epoch 561, training loss: 2181.91, average training loss: 2661.37, base loss: 2376.77
[INFO 2017-06-26 19:15:02,182 main.py:47] epoch 562, training loss: 1853.53, average training loss: 2659.94, base loss: 2375.98
[INFO 2017-06-26 19:15:02,465 main.py:47] epoch 563, training loss: 2258.78, average training loss: 2659.23, base loss: 2376.04
[INFO 2017-06-26 19:15:02,753 main.py:47] epoch 564, training loss: 2244.93, average training loss: 2658.49, base loss: 2376.06
[INFO 2017-06-26 19:15:03,035 main.py:47] epoch 565, training loss: 2264.58, average training loss: 2657.80, base loss: 2376.06
[INFO 2017-06-26 19:15:03,321 main.py:47] epoch 566, training loss: 2212.48, average training loss: 2657.01, base loss: 2375.82
[INFO 2017-06-26 19:15:03,607 main.py:47] epoch 567, training loss: 2240.05, average training loss: 2656.28, base loss: 2375.74
[INFO 2017-06-26 19:15:03,890 main.py:47] epoch 568, training loss: 2808.94, average training loss: 2656.55, base loss: 2376.81
[INFO 2017-06-26 19:15:04,212 main.py:47] epoch 569, training loss: 2335.69, average training loss: 2655.98, base loss: 2376.91
[INFO 2017-06-26 19:15:04,523 main.py:47] epoch 570, training loss: 2362.17, average training loss: 2655.47, base loss: 2377.05
[INFO 2017-06-26 19:15:04,807 main.py:47] epoch 571, training loss: 2077.49, average training loss: 2654.46, base loss: 2376.85
[INFO 2017-06-26 19:15:05,096 main.py:47] epoch 572, training loss: 2606.96, average training loss: 2654.38, base loss: 2377.62
[INFO 2017-06-26 19:15:05,385 main.py:47] epoch 573, training loss: 1911.60, average training loss: 2653.08, base loss: 2376.94
[INFO 2017-06-26 19:15:05,674 main.py:47] epoch 574, training loss: 2213.95, average training loss: 2652.32, base loss: 2376.90
[INFO 2017-06-26 19:15:05,959 main.py:47] epoch 575, training loss: 2277.52, average training loss: 2651.67, base loss: 2376.85
[INFO 2017-06-26 19:15:06,247 main.py:47] epoch 576, training loss: 2263.75, average training loss: 2651.00, base loss: 2376.93
[INFO 2017-06-26 19:15:06,533 main.py:47] epoch 577, training loss: 2160.59, average training loss: 2650.15, base loss: 2376.76
[INFO 2017-06-26 19:15:06,822 main.py:47] epoch 578, training loss: 2279.23, average training loss: 2649.51, base loss: 2376.81
[INFO 2017-06-26 19:15:07,110 main.py:47] epoch 579, training loss: 1928.33, average training loss: 2648.26, base loss: 2376.03
[INFO 2017-06-26 19:15:07,394 main.py:47] epoch 580, training loss: 2018.49, average training loss: 2647.18, base loss: 2375.50
[INFO 2017-06-26 19:15:07,677 main.py:47] epoch 581, training loss: 2355.68, average training loss: 2646.68, base loss: 2375.60
[INFO 2017-06-26 19:15:07,963 main.py:47] epoch 582, training loss: 2178.31, average training loss: 2645.87, base loss: 2375.38
[INFO 2017-06-26 19:15:08,260 main.py:47] epoch 583, training loss: 2609.20, average training loss: 2645.81, base loss: 2375.98
[INFO 2017-06-26 19:15:08,552 main.py:47] epoch 584, training loss: 2505.59, average training loss: 2645.57, base loss: 2376.43
[INFO 2017-06-26 19:15:08,840 main.py:47] epoch 585, training loss: 2313.12, average training loss: 2645.01, base loss: 2376.37
[INFO 2017-06-26 19:15:09,129 main.py:47] epoch 586, training loss: 2406.45, average training loss: 2644.60, base loss: 2376.57
[INFO 2017-06-26 19:15:09,417 main.py:47] epoch 587, training loss: 2447.87, average training loss: 2644.26, base loss: 2376.75
[INFO 2017-06-26 19:15:09,703 main.py:47] epoch 588, training loss: 2431.45, average training loss: 2643.90, base loss: 2377.10
[INFO 2017-06-26 19:15:09,987 main.py:47] epoch 589, training loss: 1916.44, average training loss: 2642.67, base loss: 2376.46
[INFO 2017-06-26 19:15:10,275 main.py:47] epoch 590, training loss: 2379.57, average training loss: 2642.22, base loss: 2376.56
[INFO 2017-06-26 19:15:10,562 main.py:47] epoch 591, training loss: 2055.37, average training loss: 2641.23, base loss: 2376.20
[INFO 2017-06-26 19:15:10,849 main.py:47] epoch 592, training loss: 2326.03, average training loss: 2640.70, base loss: 2376.28
[INFO 2017-06-26 19:15:11,140 main.py:47] epoch 593, training loss: 2272.41, average training loss: 2640.08, base loss: 2376.26
[INFO 2017-06-26 19:15:11,429 main.py:47] epoch 594, training loss: 2376.29, average training loss: 2639.64, base loss: 2376.44
[INFO 2017-06-26 19:15:11,711 main.py:47] epoch 595, training loss: 2296.79, average training loss: 2639.06, base loss: 2376.40
[INFO 2017-06-26 19:15:12,060 main.py:47] epoch 596, training loss: 2283.21, average training loss: 2638.47, base loss: 2376.50
[INFO 2017-06-26 19:15:12,360 main.py:47] epoch 597, training loss: 2102.43, average training loss: 2637.57, base loss: 2376.16
[INFO 2017-06-26 19:15:12,650 main.py:47] epoch 598, training loss: 2300.33, average training loss: 2637.01, base loss: 2376.29
[INFO 2017-06-26 19:15:12,934 main.py:47] epoch 599, training loss: 2566.39, average training loss: 2636.89, base loss: 2376.84
[INFO 2017-06-26 19:15:12,934 main.py:49] epoch 599, testing
[INFO 2017-06-26 19:15:16,883 main.py:100] average testing loss: 2233.13, base loss: 2338.78
[INFO 2017-06-26 19:15:16,907 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:15:16,921 main.py:73] current best accuracy: 2233.13
[INFO 2017-06-26 19:15:17,205 main.py:47] epoch 600, training loss: 2218.04, average training loss: 2636.19, base loss: 2376.74
[INFO 2017-06-26 19:15:17,490 main.py:47] epoch 601, training loss: 1962.63, average training loss: 2635.07, base loss: 2376.33
[INFO 2017-06-26 19:15:17,773 main.py:47] epoch 602, training loss: 2262.72, average training loss: 2634.46, base loss: 2376.34
[INFO 2017-06-26 19:15:18,056 main.py:47] epoch 603, training loss: 1899.69, average training loss: 2633.24, base loss: 2375.74
[INFO 2017-06-26 19:15:18,343 main.py:47] epoch 604, training loss: 2389.02, average training loss: 2632.84, base loss: 2376.01
[INFO 2017-06-26 19:15:18,631 main.py:47] epoch 605, training loss: 2894.64, average training loss: 2633.27, base loss: 2377.07
[INFO 2017-06-26 19:15:18,916 main.py:47] epoch 606, training loss: 2163.98, average training loss: 2632.50, base loss: 2376.98
[INFO 2017-06-26 19:15:19,203 main.py:47] epoch 607, training loss: 1902.71, average training loss: 2631.30, base loss: 2376.35
[INFO 2017-06-26 19:15:19,547 main.py:47] epoch 608, training loss: 2510.26, average training loss: 2631.10, base loss: 2376.76
[INFO 2017-06-26 19:15:19,864 main.py:47] epoch 609, training loss: 2405.20, average training loss: 2630.73, base loss: 2377.01
[INFO 2017-06-26 19:15:20,153 main.py:47] epoch 610, training loss: 2670.75, average training loss: 2630.79, base loss: 2377.73
[INFO 2017-06-26 19:15:20,442 main.py:47] epoch 611, training loss: 2313.31, average training loss: 2630.27, base loss: 2377.68
[INFO 2017-06-26 19:15:20,727 main.py:47] epoch 612, training loss: 2719.86, average training loss: 2630.42, base loss: 2378.38
[INFO 2017-06-26 19:15:21,010 main.py:47] epoch 613, training loss: 1808.10, average training loss: 2629.08, base loss: 2377.58
[INFO 2017-06-26 19:15:21,299 main.py:47] epoch 614, training loss: 2183.01, average training loss: 2628.35, base loss: 2377.51
[INFO 2017-06-26 19:15:21,584 main.py:47] epoch 615, training loss: 2564.48, average training loss: 2628.25, base loss: 2378.10
[INFO 2017-06-26 19:15:21,871 main.py:47] epoch 616, training loss: 2293.70, average training loss: 2627.71, base loss: 2378.14
[INFO 2017-06-26 19:15:22,160 main.py:47] epoch 617, training loss: 2057.74, average training loss: 2626.79, base loss: 2377.64
[INFO 2017-06-26 19:15:22,448 main.py:47] epoch 618, training loss: 2339.44, average training loss: 2626.32, base loss: 2377.74
[INFO 2017-06-26 19:15:22,741 main.py:47] epoch 619, training loss: 2270.99, average training loss: 2625.75, base loss: 2377.71
[INFO 2017-06-26 19:15:23,026 main.py:47] epoch 620, training loss: 2334.56, average training loss: 2625.28, base loss: 2377.85
[INFO 2017-06-26 19:15:23,310 main.py:47] epoch 621, training loss: 2461.48, average training loss: 2625.02, base loss: 2378.36
[INFO 2017-06-26 19:15:23,597 main.py:47] epoch 622, training loss: 2526.41, average training loss: 2624.86, base loss: 2378.88
[INFO 2017-06-26 19:15:23,951 main.py:47] epoch 623, training loss: 2403.37, average training loss: 2624.50, base loss: 2379.15
[INFO 2017-06-26 19:15:24,247 main.py:47] epoch 624, training loss: 2097.86, average training loss: 2623.66, base loss: 2378.96
[INFO 2017-06-26 19:15:24,535 main.py:47] epoch 625, training loss: 2403.87, average training loss: 2623.31, base loss: 2379.37
[INFO 2017-06-26 19:15:24,891 main.py:47] epoch 626, training loss: 2611.48, average training loss: 2623.29, base loss: 2379.97
[INFO 2017-06-26 19:15:25,188 main.py:47] epoch 627, training loss: 2155.38, average training loss: 2622.55, base loss: 2379.68
[INFO 2017-06-26 19:15:25,471 main.py:47] epoch 628, training loss: 2225.44, average training loss: 2621.91, base loss: 2379.57
[INFO 2017-06-26 19:15:25,757 main.py:47] epoch 629, training loss: 2041.38, average training loss: 2620.99, base loss: 2379.27
[INFO 2017-06-26 19:15:26,039 main.py:47] epoch 630, training loss: 2355.33, average training loss: 2620.57, base loss: 2379.39
[INFO 2017-06-26 19:15:26,378 main.py:47] epoch 631, training loss: 2285.55, average training loss: 2620.04, base loss: 2379.42
[INFO 2017-06-26 19:15:26,684 main.py:47] epoch 632, training loss: 1731.60, average training loss: 2618.64, base loss: 2378.51
[INFO 2017-06-26 19:15:27,009 main.py:47] epoch 633, training loss: 2305.42, average training loss: 2618.14, base loss: 2378.55
[INFO 2017-06-26 19:15:27,316 main.py:47] epoch 634, training loss: 2990.72, average training loss: 2618.73, base loss: 2379.74
[INFO 2017-06-26 19:15:27,610 main.py:47] epoch 635, training loss: 2229.91, average training loss: 2618.12, base loss: 2379.65
[INFO 2017-06-26 19:15:27,986 main.py:47] epoch 636, training loss: 2296.99, average training loss: 2617.62, base loss: 2379.67
[INFO 2017-06-26 19:15:28,283 main.py:47] epoch 637, training loss: 2179.54, average training loss: 2616.93, base loss: 2379.49
[INFO 2017-06-26 19:15:28,568 main.py:47] epoch 638, training loss: 2260.27, average training loss: 2616.37, base loss: 2379.47
[INFO 2017-06-26 19:15:28,857 main.py:47] epoch 639, training loss: 2095.11, average training loss: 2615.56, base loss: 2379.20
[INFO 2017-06-26 19:15:29,142 main.py:47] epoch 640, training loss: 2756.44, average training loss: 2615.78, base loss: 2380.02
[INFO 2017-06-26 19:15:29,431 main.py:47] epoch 641, training loss: 2217.63, average training loss: 2615.16, base loss: 2379.99
[INFO 2017-06-26 19:15:29,715 main.py:47] epoch 642, training loss: 2564.28, average training loss: 2615.08, base loss: 2380.55
[INFO 2017-06-26 19:15:30,005 main.py:47] epoch 643, training loss: 2564.84, average training loss: 2615.00, base loss: 2381.05
[INFO 2017-06-26 19:15:30,295 main.py:47] epoch 644, training loss: 2489.14, average training loss: 2614.80, base loss: 2381.37
[INFO 2017-06-26 19:15:30,584 main.py:47] epoch 645, training loss: 2222.74, average training loss: 2614.20, base loss: 2381.26
[INFO 2017-06-26 19:15:30,874 main.py:47] epoch 646, training loss: 2311.92, average training loss: 2613.73, base loss: 2381.39
[INFO 2017-06-26 19:15:31,163 main.py:47] epoch 647, training loss: 2400.67, average training loss: 2613.40, base loss: 2381.54
[INFO 2017-06-26 19:15:31,447 main.py:47] epoch 648, training loss: 2038.81, average training loss: 2612.52, base loss: 2381.19
[INFO 2017-06-26 19:15:31,732 main.py:47] epoch 649, training loss: 2427.99, average training loss: 2612.23, base loss: 2381.42
[INFO 2017-06-26 19:15:32,017 main.py:47] epoch 650, training loss: 2105.83, average training loss: 2611.45, base loss: 2381.37
[INFO 2017-06-26 19:15:32,306 main.py:47] epoch 651, training loss: 2212.22, average training loss: 2610.84, base loss: 2381.19
[INFO 2017-06-26 19:15:32,594 main.py:47] epoch 652, training loss: 2466.05, average training loss: 2610.62, base loss: 2381.58
[INFO 2017-06-26 19:15:32,881 main.py:47] epoch 653, training loss: 2333.93, average training loss: 2610.20, base loss: 2381.66
[INFO 2017-06-26 19:15:33,170 main.py:47] epoch 654, training loss: 2289.75, average training loss: 2609.71, base loss: 2381.63
[INFO 2017-06-26 19:15:33,454 main.py:47] epoch 655, training loss: 2685.88, average training loss: 2609.82, base loss: 2382.43
[INFO 2017-06-26 19:15:33,742 main.py:47] epoch 656, training loss: 2871.26, average training loss: 2610.22, base loss: 2383.55
[INFO 2017-06-26 19:15:34,032 main.py:47] epoch 657, training loss: 2081.60, average training loss: 2609.42, base loss: 2383.09
[INFO 2017-06-26 19:15:34,320 main.py:47] epoch 658, training loss: 2102.30, average training loss: 2608.65, base loss: 2382.75
[INFO 2017-06-26 19:15:34,684 main.py:47] epoch 659, training loss: 2506.28, average training loss: 2608.49, base loss: 2383.13
[INFO 2017-06-26 19:15:35,018 main.py:47] epoch 660, training loss: 2551.81, average training loss: 2608.41, base loss: 2383.57
[INFO 2017-06-26 19:15:35,307 main.py:47] epoch 661, training loss: 2537.05, average training loss: 2608.30, base loss: 2384.00
[INFO 2017-06-26 19:15:35,592 main.py:47] epoch 662, training loss: 2185.02, average training loss: 2607.66, base loss: 2383.76
[INFO 2017-06-26 19:15:35,873 main.py:47] epoch 663, training loss: 2743.05, average training loss: 2607.87, base loss: 2384.63
[INFO 2017-06-26 19:15:36,161 main.py:47] epoch 664, training loss: 2421.39, average training loss: 2607.58, base loss: 2384.94
[INFO 2017-06-26 19:15:36,449 main.py:47] epoch 665, training loss: 2660.21, average training loss: 2607.66, base loss: 2385.68
[INFO 2017-06-26 19:15:36,741 main.py:47] epoch 666, training loss: 2138.35, average training loss: 2606.96, base loss: 2385.47
[INFO 2017-06-26 19:15:37,029 main.py:47] epoch 667, training loss: 2051.52, average training loss: 2606.13, base loss: 2385.04
[INFO 2017-06-26 19:15:37,318 main.py:47] epoch 668, training loss: 2286.47, average training loss: 2605.65, base loss: 2385.04
[INFO 2017-06-26 19:15:37,603 main.py:47] epoch 669, training loss: 1947.24, average training loss: 2604.67, base loss: 2384.49
[INFO 2017-06-26 19:15:37,894 main.py:47] epoch 670, training loss: 2624.36, average training loss: 2604.70, base loss: 2385.14
[INFO 2017-06-26 19:15:38,179 main.py:47] epoch 671, training loss: 2339.96, average training loss: 2604.30, base loss: 2385.28
[INFO 2017-06-26 19:15:38,462 main.py:47] epoch 672, training loss: 2281.27, average training loss: 2603.82, base loss: 2385.25
[INFO 2017-06-26 19:15:38,750 main.py:47] epoch 673, training loss: 2304.88, average training loss: 2603.38, base loss: 2385.47
[INFO 2017-06-26 19:15:39,038 main.py:47] epoch 674, training loss: 2309.09, average training loss: 2602.94, base loss: 2385.55
[INFO 2017-06-26 19:15:39,383 main.py:47] epoch 675, training loss: 2066.17, average training loss: 2602.15, base loss: 2385.06
[INFO 2017-06-26 19:15:39,684 main.py:47] epoch 676, training loss: 1787.19, average training loss: 2600.95, base loss: 2384.15
[INFO 2017-06-26 19:15:39,977 main.py:47] epoch 677, training loss: 1909.94, average training loss: 2599.93, base loss: 2383.53
[INFO 2017-06-26 19:15:40,270 main.py:47] epoch 678, training loss: 2580.93, average training loss: 2599.90, base loss: 2384.00
[INFO 2017-06-26 19:15:40,592 main.py:47] epoch 679, training loss: 2403.50, average training loss: 2599.61, base loss: 2384.29
[INFO 2017-06-26 19:15:40,889 main.py:47] epoch 680, training loss: 2078.13, average training loss: 2598.84, base loss: 2383.98
[INFO 2017-06-26 19:15:41,174 main.py:47] epoch 681, training loss: 2107.28, average training loss: 2598.12, base loss: 2383.74
[INFO 2017-06-26 19:15:41,462 main.py:47] epoch 682, training loss: 1892.18, average training loss: 2597.09, base loss: 2383.06
[INFO 2017-06-26 19:15:41,799 main.py:47] epoch 683, training loss: 2113.79, average training loss: 2596.38, base loss: 2382.82
[INFO 2017-06-26 19:15:42,112 main.py:47] epoch 684, training loss: 2387.80, average training loss: 2596.08, base loss: 2382.92
[INFO 2017-06-26 19:15:42,396 main.py:47] epoch 685, training loss: 2162.01, average training loss: 2595.45, base loss: 2382.73
[INFO 2017-06-26 19:15:42,679 main.py:47] epoch 686, training loss: 2437.93, average training loss: 2595.22, base loss: 2383.04
[INFO 2017-06-26 19:15:42,967 main.py:47] epoch 687, training loss: 2368.50, average training loss: 2594.89, base loss: 2383.23
[INFO 2017-06-26 19:15:43,254 main.py:47] epoch 688, training loss: 2657.01, average training loss: 2594.98, base loss: 2383.72
[INFO 2017-06-26 19:15:43,538 main.py:47] epoch 689, training loss: 2130.29, average training loss: 2594.30, base loss: 2383.58
[INFO 2017-06-26 19:15:43,819 main.py:47] epoch 690, training loss: 2167.71, average training loss: 2593.69, base loss: 2383.36
[INFO 2017-06-26 19:15:44,102 main.py:47] epoch 691, training loss: 2287.27, average training loss: 2593.24, base loss: 2383.45
[INFO 2017-06-26 19:15:44,391 main.py:47] epoch 692, training loss: 2236.19, average training loss: 2592.73, base loss: 2383.35
[INFO 2017-06-26 19:15:44,678 main.py:47] epoch 693, training loss: 2465.08, average training loss: 2592.54, base loss: 2383.69
[INFO 2017-06-26 19:15:44,967 main.py:47] epoch 694, training loss: 1990.03, average training loss: 2591.68, base loss: 2383.29
[INFO 2017-06-26 19:15:45,256 main.py:47] epoch 695, training loss: 2739.47, average training loss: 2591.89, base loss: 2384.00
[INFO 2017-06-26 19:15:45,544 main.py:47] epoch 696, training loss: 2390.76, average training loss: 2591.60, base loss: 2384.27
[INFO 2017-06-26 19:15:45,834 main.py:47] epoch 697, training loss: 2292.52, average training loss: 2591.17, base loss: 2384.26
[INFO 2017-06-26 19:15:46,118 main.py:47] epoch 698, training loss: 2071.18, average training loss: 2590.43, base loss: 2383.91
[INFO 2017-06-26 19:15:46,402 main.py:47] epoch 699, training loss: 1964.49, average training loss: 2589.54, base loss: 2383.40
[INFO 2017-06-26 19:15:46,402 main.py:49] epoch 699, testing
[INFO 2017-06-26 19:15:50,351 main.py:100] average testing loss: 2275.03, base loss: 2425.18
[INFO 2017-06-26 19:15:50,378 main.py:73] current best accuracy: 2233.13
[INFO 2017-06-26 19:15:50,730 main.py:47] epoch 700, training loss: 1820.72, average training loss: 2588.44, base loss: 2382.77
[INFO 2017-06-26 19:15:51,032 main.py:47] epoch 701, training loss: 2187.26, average training loss: 2587.87, base loss: 2382.60
[INFO 2017-06-26 19:15:51,322 main.py:47] epoch 702, training loss: 2541.64, average training loss: 2587.80, base loss: 2383.06
[INFO 2017-06-26 19:15:51,612 main.py:47] epoch 703, training loss: 2333.52, average training loss: 2587.44, base loss: 2383.26
[INFO 2017-06-26 19:15:51,900 main.py:47] epoch 704, training loss: 2323.37, average training loss: 2587.07, base loss: 2383.36
[INFO 2017-06-26 19:15:52,185 main.py:47] epoch 705, training loss: 2246.03, average training loss: 2586.58, base loss: 2383.31
[INFO 2017-06-26 19:15:52,468 main.py:47] epoch 706, training loss: 2065.03, average training loss: 2585.84, base loss: 2383.02
[INFO 2017-06-26 19:15:52,753 main.py:47] epoch 707, training loss: 2241.79, average training loss: 2585.36, base loss: 2383.07
[INFO 2017-06-26 19:15:53,040 main.py:47] epoch 708, training loss: 2100.25, average training loss: 2584.67, base loss: 2382.87
[INFO 2017-06-26 19:15:53,324 main.py:47] epoch 709, training loss: 1992.11, average training loss: 2583.84, base loss: 2382.31
[INFO 2017-06-26 19:15:53,612 main.py:47] epoch 710, training loss: 2350.16, average training loss: 2583.51, base loss: 2382.51
[INFO 2017-06-26 19:15:53,902 main.py:47] epoch 711, training loss: 2175.38, average training loss: 2582.94, base loss: 2382.40
[INFO 2017-06-26 19:15:54,190 main.py:47] epoch 712, training loss: 2083.60, average training loss: 2582.24, base loss: 2382.07
[INFO 2017-06-26 19:15:54,471 main.py:47] epoch 713, training loss: 2250.77, average training loss: 2581.77, base loss: 2382.19
[INFO 2017-06-26 19:15:54,756 main.py:47] epoch 714, training loss: 2121.72, average training loss: 2581.13, base loss: 2382.14
[INFO 2017-06-26 19:15:55,043 main.py:47] epoch 715, training loss: 2599.78, average training loss: 2581.16, base loss: 2382.67
[INFO 2017-06-26 19:15:55,333 main.py:47] epoch 716, training loss: 2368.34, average training loss: 2580.86, base loss: 2382.84
[INFO 2017-06-26 19:15:55,618 main.py:47] epoch 717, training loss: 2481.59, average training loss: 2580.72, base loss: 2383.40
[INFO 2017-06-26 19:15:55,908 main.py:47] epoch 718, training loss: 2577.73, average training loss: 2580.72, base loss: 2383.93
[INFO 2017-06-26 19:15:56,283 main.py:47] epoch 719, training loss: 2144.76, average training loss: 2580.11, base loss: 2383.73
[INFO 2017-06-26 19:15:56,582 main.py:47] epoch 720, training loss: 1996.91, average training loss: 2579.30, base loss: 2383.36
[INFO 2017-06-26 19:15:56,871 main.py:47] epoch 721, training loss: 2172.45, average training loss: 2578.74, base loss: 2383.29
[INFO 2017-06-26 19:15:57,159 main.py:47] epoch 722, training loss: 2302.66, average training loss: 2578.36, base loss: 2383.41
[INFO 2017-06-26 19:15:57,449 main.py:47] epoch 723, training loss: 2740.51, average training loss: 2578.58, base loss: 2384.24
[INFO 2017-06-26 19:15:57,738 main.py:47] epoch 724, training loss: 2112.96, average training loss: 2577.94, base loss: 2383.96
[INFO 2017-06-26 19:15:58,027 main.py:47] epoch 725, training loss: 2382.74, average training loss: 2577.67, base loss: 2384.26
[INFO 2017-06-26 19:15:58,316 main.py:47] epoch 726, training loss: 2191.83, average training loss: 2577.14, base loss: 2384.20
[INFO 2017-06-26 19:15:58,602 main.py:47] epoch 727, training loss: 2222.60, average training loss: 2576.65, base loss: 2384.23
[INFO 2017-06-26 19:15:58,892 main.py:47] epoch 728, training loss: 2539.85, average training loss: 2576.60, base loss: 2384.73
[INFO 2017-06-26 19:15:59,180 main.py:47] epoch 729, training loss: 2167.16, average training loss: 2576.04, base loss: 2384.52
[INFO 2017-06-26 19:15:59,466 main.py:47] epoch 730, training loss: 2735.09, average training loss: 2576.26, base loss: 2385.17
[INFO 2017-06-26 19:15:59,762 main.py:47] epoch 731, training loss: 1725.01, average training loss: 2575.10, base loss: 2384.43
[INFO 2017-06-26 19:16:00,051 main.py:47] epoch 732, training loss: 2045.43, average training loss: 2574.37, base loss: 2384.10
[INFO 2017-06-26 19:16:00,336 main.py:47] epoch 733, training loss: 2476.88, average training loss: 2574.24, base loss: 2384.35
[INFO 2017-06-26 19:16:00,709 main.py:47] epoch 734, training loss: 2276.51, average training loss: 2573.83, base loss: 2384.45
[INFO 2017-06-26 19:16:01,003 main.py:47] epoch 735, training loss: 2387.76, average training loss: 2573.58, base loss: 2384.54
[INFO 2017-06-26 19:16:01,292 main.py:47] epoch 736, training loss: 2203.35, average training loss: 2573.08, base loss: 2384.47
[INFO 2017-06-26 19:16:01,582 main.py:47] epoch 737, training loss: 1974.05, average training loss: 2572.27, base loss: 2384.10
[INFO 2017-06-26 19:16:01,872 main.py:47] epoch 738, training loss: 2396.61, average training loss: 2572.03, base loss: 2384.36
[INFO 2017-06-26 19:16:02,158 main.py:47] epoch 739, training loss: 2068.50, average training loss: 2571.35, base loss: 2384.17
[INFO 2017-06-26 19:16:02,503 main.py:47] epoch 740, training loss: 2650.08, average training loss: 2571.46, base loss: 2384.79
[INFO 2017-06-26 19:16:02,804 main.py:47] epoch 741, training loss: 1992.63, average training loss: 2570.68, base loss: 2384.17
[INFO 2017-06-26 19:16:03,091 main.py:47] epoch 742, training loss: 2834.80, average training loss: 2571.03, base loss: 2384.94
[INFO 2017-06-26 19:16:03,448 main.py:47] epoch 743, training loss: 2034.91, average training loss: 2570.31, base loss: 2384.59
[INFO 2017-06-26 19:16:03,748 main.py:47] epoch 744, training loss: 2558.05, average training loss: 2570.29, base loss: 2385.11
[INFO 2017-06-26 19:16:04,036 main.py:47] epoch 745, training loss: 2033.08, average training loss: 2569.57, base loss: 2384.67
[INFO 2017-06-26 19:16:04,323 main.py:47] epoch 746, training loss: 1792.38, average training loss: 2568.53, base loss: 2383.88
[INFO 2017-06-26 19:16:04,654 main.py:47] epoch 747, training loss: 2380.35, average training loss: 2568.28, base loss: 2384.03
[INFO 2017-06-26 19:16:04,966 main.py:47] epoch 748, training loss: 2640.90, average training loss: 2568.38, base loss: 2384.52
[INFO 2017-06-26 19:16:05,254 main.py:47] epoch 749, training loss: 1830.56, average training loss: 2567.40, base loss: 2383.85
[INFO 2017-06-26 19:16:05,596 main.py:47] epoch 750, training loss: 2175.90, average training loss: 2566.87, base loss: 2383.68
[INFO 2017-06-26 19:16:05,900 main.py:47] epoch 751, training loss: 2506.79, average training loss: 2566.79, base loss: 2383.98
[INFO 2017-06-26 19:16:06,185 main.py:47] epoch 752, training loss: 2321.98, average training loss: 2566.47, base loss: 2384.07
[INFO 2017-06-26 19:16:06,470 main.py:47] epoch 753, training loss: 2271.75, average training loss: 2566.08, base loss: 2384.08
[INFO 2017-06-26 19:16:06,754 main.py:47] epoch 754, training loss: 2311.93, average training loss: 2565.74, base loss: 2384.09
[INFO 2017-06-26 19:16:07,038 main.py:47] epoch 755, training loss: 2479.63, average training loss: 2565.63, base loss: 2384.38
[INFO 2017-06-26 19:16:07,325 main.py:47] epoch 756, training loss: 2215.02, average training loss: 2565.16, base loss: 2384.29
[INFO 2017-06-26 19:16:07,610 main.py:47] epoch 757, training loss: 2126.45, average training loss: 2564.59, base loss: 2384.03
[INFO 2017-06-26 19:16:07,898 main.py:47] epoch 758, training loss: 2054.87, average training loss: 2563.91, base loss: 2383.71
[INFO 2017-06-26 19:16:08,189 main.py:47] epoch 759, training loss: 2499.02, average training loss: 2563.83, base loss: 2384.02
[INFO 2017-06-26 19:16:08,475 main.py:47] epoch 760, training loss: 2069.32, average training loss: 2563.18, base loss: 2383.74
[INFO 2017-06-26 19:16:08,760 main.py:47] epoch 761, training loss: 2091.85, average training loss: 2562.56, base loss: 2383.50
[INFO 2017-06-26 19:16:09,049 main.py:47] epoch 762, training loss: 2265.73, average training loss: 2562.17, base loss: 2383.51
[INFO 2017-06-26 19:16:09,334 main.py:47] epoch 763, training loss: 2354.39, average training loss: 2561.90, base loss: 2383.73
[INFO 2017-06-26 19:16:09,621 main.py:47] epoch 764, training loss: 2092.21, average training loss: 2561.29, base loss: 2383.54
[INFO 2017-06-26 19:16:09,911 main.py:47] epoch 765, training loss: 1822.03, average training loss: 2560.32, base loss: 2382.98
[INFO 2017-06-26 19:16:10,194 main.py:47] epoch 766, training loss: 2076.70, average training loss: 2559.69, base loss: 2382.73
[INFO 2017-06-26 19:16:10,484 main.py:47] epoch 767, training loss: 1854.99, average training loss: 2558.77, base loss: 2382.14
[INFO 2017-06-26 19:16:10,773 main.py:47] epoch 768, training loss: 1946.63, average training loss: 2557.98, base loss: 2381.77
[INFO 2017-06-26 19:16:11,060 main.py:47] epoch 769, training loss: 2784.03, average training loss: 2558.27, base loss: 2382.52
[INFO 2017-06-26 19:16:11,356 main.py:47] epoch 770, training loss: 1896.16, average training loss: 2557.41, base loss: 2381.95
[INFO 2017-06-26 19:16:11,646 main.py:47] epoch 771, training loss: 2068.85, average training loss: 2556.78, base loss: 2381.76
[INFO 2017-06-26 19:16:11,942 main.py:47] epoch 772, training loss: 2211.62, average training loss: 2556.33, base loss: 2381.61
[INFO 2017-06-26 19:16:12,230 main.py:47] epoch 773, training loss: 2330.06, average training loss: 2556.04, base loss: 2381.69
[INFO 2017-06-26 19:16:12,517 main.py:47] epoch 774, training loss: 2121.96, average training loss: 2555.48, base loss: 2381.52
[INFO 2017-06-26 19:16:12,803 main.py:47] epoch 775, training loss: 2882.32, average training loss: 2555.90, base loss: 2382.48
[INFO 2017-06-26 19:16:13,096 main.py:47] epoch 776, training loss: 1938.52, average training loss: 2555.11, base loss: 2382.13
[INFO 2017-06-26 19:16:13,386 main.py:47] epoch 777, training loss: 3032.16, average training loss: 2555.72, base loss: 2383.16
[INFO 2017-06-26 19:16:13,673 main.py:47] epoch 778, training loss: 2215.44, average training loss: 2555.28, base loss: 2383.13
[INFO 2017-06-26 19:16:13,963 main.py:47] epoch 779, training loss: 1986.05, average training loss: 2554.55, base loss: 2382.69
[INFO 2017-06-26 19:16:14,256 main.py:47] epoch 780, training loss: 2062.48, average training loss: 2553.92, base loss: 2382.42
[INFO 2017-06-26 19:16:14,544 main.py:47] epoch 781, training loss: 1950.51, average training loss: 2553.15, base loss: 2382.06
[INFO 2017-06-26 19:16:14,833 main.py:47] epoch 782, training loss: 2173.45, average training loss: 2552.67, base loss: 2382.20
[INFO 2017-06-26 19:16:15,114 main.py:47] epoch 783, training loss: 2270.64, average training loss: 2552.31, base loss: 2382.08
[INFO 2017-06-26 19:16:15,399 main.py:47] epoch 784, training loss: 2382.23, average training loss: 2552.09, base loss: 2382.36
[INFO 2017-06-26 19:16:15,688 main.py:47] epoch 785, training loss: 2438.79, average training loss: 2551.95, base loss: 2382.51
[INFO 2017-06-26 19:16:15,975 main.py:47] epoch 786, training loss: 2368.97, average training loss: 2551.71, base loss: 2382.81
[INFO 2017-06-26 19:16:16,265 main.py:47] epoch 787, training loss: 1847.18, average training loss: 2550.82, base loss: 2382.39
[INFO 2017-06-26 19:16:16,554 main.py:47] epoch 788, training loss: 1765.76, average training loss: 2549.82, base loss: 2381.63
[INFO 2017-06-26 19:16:16,844 main.py:47] epoch 789, training loss: 2341.28, average training loss: 2549.56, base loss: 2381.79
[INFO 2017-06-26 19:16:17,130 main.py:47] epoch 790, training loss: 2244.58, average training loss: 2549.17, base loss: 2381.76
[INFO 2017-06-26 19:16:17,415 main.py:47] epoch 791, training loss: 2346.39, average training loss: 2548.92, base loss: 2381.93
[INFO 2017-06-26 19:16:17,704 main.py:47] epoch 792, training loss: 2108.31, average training loss: 2548.36, base loss: 2381.76
[INFO 2017-06-26 19:16:17,992 main.py:47] epoch 793, training loss: 2220.16, average training loss: 2547.95, base loss: 2381.76
[INFO 2017-06-26 19:16:18,279 main.py:47] epoch 794, training loss: 2287.96, average training loss: 2547.62, base loss: 2381.91
[INFO 2017-06-26 19:16:18,563 main.py:47] epoch 795, training loss: 2192.46, average training loss: 2547.18, base loss: 2381.84
[INFO 2017-06-26 19:16:18,853 main.py:47] epoch 796, training loss: 1978.28, average training loss: 2546.46, base loss: 2381.53
[INFO 2017-06-26 19:16:19,139 main.py:47] epoch 797, training loss: 2094.52, average training loss: 2545.90, base loss: 2381.32
[INFO 2017-06-26 19:16:19,425 main.py:47] epoch 798, training loss: 2039.60, average training loss: 2545.26, base loss: 2381.09
[INFO 2017-06-26 19:16:19,711 main.py:47] epoch 799, training loss: 2465.17, average training loss: 2545.16, base loss: 2381.52
[INFO 2017-06-26 19:16:19,711 main.py:49] epoch 799, testing
[INFO 2017-06-26 19:16:23,627 main.py:100] average testing loss: 2301.28, base loss: 2469.33
[INFO 2017-06-26 19:16:23,652 main.py:73] current best accuracy: 2233.13
[INFO 2017-06-26 19:16:23,939 main.py:47] epoch 800, training loss: 2146.72, average training loss: 2544.66, base loss: 2381.35
[INFO 2017-06-26 19:16:24,225 main.py:47] epoch 801, training loss: 2182.97, average training loss: 2544.21, base loss: 2381.21
[INFO 2017-06-26 19:16:24,515 main.py:47] epoch 802, training loss: 2009.15, average training loss: 2543.55, base loss: 2380.85
[INFO 2017-06-26 19:16:24,800 main.py:47] epoch 803, training loss: 2297.85, average training loss: 2543.24, base loss: 2380.89
[INFO 2017-06-26 19:16:25,090 main.py:47] epoch 804, training loss: 2336.13, average training loss: 2542.98, base loss: 2381.01
[INFO 2017-06-26 19:16:25,380 main.py:47] epoch 805, training loss: 2336.52, average training loss: 2542.73, base loss: 2381.21
[INFO 2017-06-26 19:16:25,669 main.py:47] epoch 806, training loss: 2572.85, average training loss: 2542.77, base loss: 2381.46
[INFO 2017-06-26 19:16:25,961 main.py:47] epoch 807, training loss: 2316.84, average training loss: 2542.49, base loss: 2381.54
[INFO 2017-06-26 19:16:26,334 main.py:47] epoch 808, training loss: 2510.03, average training loss: 2542.45, base loss: 2381.92
[INFO 2017-06-26 19:16:26,683 main.py:47] epoch 809, training loss: 2069.50, average training loss: 2541.86, base loss: 2381.63
[INFO 2017-06-26 19:16:27,020 main.py:47] epoch 810, training loss: 2555.13, average training loss: 2541.88, base loss: 2381.99
[INFO 2017-06-26 19:16:27,358 main.py:47] epoch 811, training loss: 2829.33, average training loss: 2542.23, base loss: 2382.85
[INFO 2017-06-26 19:16:27,684 main.py:47] epoch 812, training loss: 2620.10, average training loss: 2542.33, base loss: 2383.37
[INFO 2017-06-26 19:16:28,017 main.py:47] epoch 813, training loss: 1985.88, average training loss: 2541.64, base loss: 2382.93
[INFO 2017-06-26 19:16:28,327 main.py:47] epoch 814, training loss: 2390.73, average training loss: 2541.46, base loss: 2383.16
[INFO 2017-06-26 19:16:28,632 main.py:47] epoch 815, training loss: 2144.53, average training loss: 2540.97, base loss: 2382.94
[INFO 2017-06-26 19:16:28,981 main.py:47] epoch 816, training loss: 2411.55, average training loss: 2540.81, base loss: 2383.20
[INFO 2017-06-26 19:16:29,356 main.py:47] epoch 817, training loss: 2114.86, average training loss: 2540.29, base loss: 2383.09
[INFO 2017-06-26 19:16:29,665 main.py:47] epoch 818, training loss: 2657.28, average training loss: 2540.44, base loss: 2383.69
[INFO 2017-06-26 19:16:29,966 main.py:47] epoch 819, training loss: 2329.32, average training loss: 2540.18, base loss: 2383.86
[INFO 2017-06-26 19:16:30,251 main.py:47] epoch 820, training loss: 2026.64, average training loss: 2539.55, base loss: 2383.73
[INFO 2017-06-26 19:16:30,540 main.py:47] epoch 821, training loss: 2597.28, average training loss: 2539.62, base loss: 2384.12
[INFO 2017-06-26 19:16:30,825 main.py:47] epoch 822, training loss: 2421.38, average training loss: 2539.48, base loss: 2384.42
[INFO 2017-06-26 19:16:31,115 main.py:47] epoch 823, training loss: 2474.80, average training loss: 2539.40, base loss: 2384.46
[INFO 2017-06-26 19:16:31,404 main.py:47] epoch 824, training loss: 2053.47, average training loss: 2538.81, base loss: 2384.21
[INFO 2017-06-26 19:16:31,691 main.py:47] epoch 825, training loss: 2313.38, average training loss: 2538.54, base loss: 2384.39
[INFO 2017-06-26 19:16:31,978 main.py:47] epoch 826, training loss: 2522.80, average training loss: 2538.52, base loss: 2384.71
[INFO 2017-06-26 19:16:32,261 main.py:47] epoch 827, training loss: 2383.77, average training loss: 2538.33, base loss: 2384.98
[INFO 2017-06-26 19:16:32,548 main.py:47] epoch 828, training loss: 2606.48, average training loss: 2538.42, base loss: 2385.45
[INFO 2017-06-26 19:16:32,836 main.py:47] epoch 829, training loss: 2321.93, average training loss: 2538.16, base loss: 2385.45
[INFO 2017-06-26 19:16:33,122 main.py:47] epoch 830, training loss: 2267.00, average training loss: 2537.83, base loss: 2385.50
[INFO 2017-06-26 19:16:33,405 main.py:47] epoch 831, training loss: 2174.20, average training loss: 2537.39, base loss: 2385.34
[INFO 2017-06-26 19:16:33,694 main.py:47] epoch 832, training loss: 2307.71, average training loss: 2537.12, base loss: 2385.44
[INFO 2017-06-26 19:16:33,989 main.py:47] epoch 833, training loss: 2383.14, average training loss: 2536.93, base loss: 2385.69
[INFO 2017-06-26 19:16:34,279 main.py:47] epoch 834, training loss: 1928.80, average training loss: 2536.20, base loss: 2385.22
[INFO 2017-06-26 19:16:34,567 main.py:47] epoch 835, training loss: 2397.76, average training loss: 2536.04, base loss: 2385.53
[INFO 2017-06-26 19:16:34,855 main.py:47] epoch 836, training loss: 2211.36, average training loss: 2535.65, base loss: 2385.49
[INFO 2017-06-26 19:16:35,141 main.py:47] epoch 837, training loss: 2183.23, average training loss: 2535.23, base loss: 2385.38
[INFO 2017-06-26 19:16:35,430 main.py:47] epoch 838, training loss: 2454.49, average training loss: 2535.13, base loss: 2385.66
[INFO 2017-06-26 19:16:35,718 main.py:47] epoch 839, training loss: 2202.29, average training loss: 2534.74, base loss: 2385.69
[INFO 2017-06-26 19:16:36,003 main.py:47] epoch 840, training loss: 2352.81, average training loss: 2534.52, base loss: 2385.80
[INFO 2017-06-26 19:16:36,288 main.py:47] epoch 841, training loss: 2148.37, average training loss: 2534.06, base loss: 2385.68
[INFO 2017-06-26 19:16:36,569 main.py:47] epoch 842, training loss: 2109.02, average training loss: 2533.56, base loss: 2385.55
[INFO 2017-06-26 19:16:36,865 main.py:47] epoch 843, training loss: 2287.15, average training loss: 2533.27, base loss: 2385.49
[INFO 2017-06-26 19:16:37,149 main.py:47] epoch 844, training loss: 2301.26, average training loss: 2532.99, base loss: 2385.50
[INFO 2017-06-26 19:16:37,436 main.py:47] epoch 845, training loss: 2239.48, average training loss: 2532.64, base loss: 2385.54
[INFO 2017-06-26 19:16:37,736 main.py:47] epoch 846, training loss: 2231.34, average training loss: 2532.29, base loss: 2385.53
[INFO 2017-06-26 19:16:38,027 main.py:47] epoch 847, training loss: 2124.09, average training loss: 2531.81, base loss: 2385.33
[INFO 2017-06-26 19:16:38,316 main.py:47] epoch 848, training loss: 2448.86, average training loss: 2531.71, base loss: 2385.52
[INFO 2017-06-26 19:16:38,601 main.py:47] epoch 849, training loss: 2730.44, average training loss: 2531.94, base loss: 2386.19
[INFO 2017-06-26 19:16:38,885 main.py:47] epoch 850, training loss: 2351.71, average training loss: 2531.73, base loss: 2386.33
[INFO 2017-06-26 19:16:39,174 main.py:47] epoch 851, training loss: 1876.87, average training loss: 2530.96, base loss: 2385.82
[INFO 2017-06-26 19:16:39,477 main.py:47] epoch 852, training loss: 2023.68, average training loss: 2530.37, base loss: 2385.63
[INFO 2017-06-26 19:16:39,762 main.py:47] epoch 853, training loss: 2419.11, average training loss: 2530.24, base loss: 2385.80
[INFO 2017-06-26 19:16:40,060 main.py:47] epoch 854, training loss: 2181.27, average training loss: 2529.83, base loss: 2385.66
[INFO 2017-06-26 19:16:40,357 main.py:47] epoch 855, training loss: 2311.01, average training loss: 2529.57, base loss: 2385.74
[INFO 2017-06-26 19:16:40,641 main.py:47] epoch 856, training loss: 2408.66, average training loss: 2529.43, base loss: 2385.92
[INFO 2017-06-26 19:16:40,928 main.py:47] epoch 857, training loss: 2303.12, average training loss: 2529.17, base loss: 2385.98
[INFO 2017-06-26 19:16:41,214 main.py:47] epoch 858, training loss: 1810.01, average training loss: 2528.33, base loss: 2385.47
[INFO 2017-06-26 19:16:41,497 main.py:47] epoch 859, training loss: 2497.98, average training loss: 2528.30, base loss: 2385.80
[INFO 2017-06-26 19:16:41,784 main.py:47] epoch 860, training loss: 2081.41, average training loss: 2527.78, base loss: 2385.57
[INFO 2017-06-26 19:16:42,072 main.py:47] epoch 861, training loss: 2003.34, average training loss: 2527.17, base loss: 2385.33
[INFO 2017-06-26 19:16:42,357 main.py:47] epoch 862, training loss: 1778.22, average training loss: 2526.30, base loss: 2384.72
[INFO 2017-06-26 19:16:42,648 main.py:47] epoch 863, training loss: 2115.82, average training loss: 2525.83, base loss: 2384.50
[INFO 2017-06-26 19:16:42,951 main.py:47] epoch 864, training loss: 2193.57, average training loss: 2525.44, base loss: 2384.53
[INFO 2017-06-26 19:16:43,242 main.py:47] epoch 865, training loss: 2052.47, average training loss: 2524.90, base loss: 2384.36
[INFO 2017-06-26 19:16:43,529 main.py:47] epoch 866, training loss: 2236.75, average training loss: 2524.56, base loss: 2384.43
[INFO 2017-06-26 19:16:43,817 main.py:47] epoch 867, training loss: 2138.32, average training loss: 2524.12, base loss: 2384.26
[INFO 2017-06-26 19:16:44,110 main.py:47] epoch 868, training loss: 2159.14, average training loss: 2523.70, base loss: 2384.23
[INFO 2017-06-26 19:16:44,405 main.py:47] epoch 869, training loss: 2264.96, average training loss: 2523.40, base loss: 2384.43
[INFO 2017-06-26 19:16:44,694 main.py:47] epoch 870, training loss: 2513.31, average training loss: 2523.39, base loss: 2384.72
[INFO 2017-06-26 19:16:44,982 main.py:47] epoch 871, training loss: 2740.24, average training loss: 2523.64, base loss: 2385.30
[INFO 2017-06-26 19:16:45,267 main.py:47] epoch 872, training loss: 2082.44, average training loss: 2523.13, base loss: 2384.99
[INFO 2017-06-26 19:16:45,553 main.py:47] epoch 873, training loss: 1976.72, average training loss: 2522.51, base loss: 2384.58
[INFO 2017-06-26 19:16:45,842 main.py:47] epoch 874, training loss: 2438.93, average training loss: 2522.41, base loss: 2384.89
[INFO 2017-06-26 19:16:46,127 main.py:47] epoch 875, training loss: 1848.68, average training loss: 2521.64, base loss: 2384.38
[INFO 2017-06-26 19:16:46,417 main.py:47] epoch 876, training loss: 2623.18, average training loss: 2521.76, base loss: 2384.91
[INFO 2017-06-26 19:16:46,702 main.py:47] epoch 877, training loss: 2608.07, average training loss: 2521.86, base loss: 2385.37
[INFO 2017-06-26 19:16:46,987 main.py:47] epoch 878, training loss: 2303.39, average training loss: 2521.61, base loss: 2385.48
[INFO 2017-06-26 19:16:47,276 main.py:47] epoch 879, training loss: 2259.86, average training loss: 2521.31, base loss: 2385.49
[INFO 2017-06-26 19:16:47,588 main.py:47] epoch 880, training loss: 2242.02, average training loss: 2520.99, base loss: 2385.50
[INFO 2017-06-26 19:16:47,943 main.py:47] epoch 881, training loss: 2480.28, average training loss: 2520.95, base loss: 2385.76
[INFO 2017-06-26 19:16:48,282 main.py:47] epoch 882, training loss: 2343.48, average training loss: 2520.75, base loss: 2385.97
[INFO 2017-06-26 19:16:48,622 main.py:47] epoch 883, training loss: 2082.28, average training loss: 2520.25, base loss: 2385.80
[INFO 2017-06-26 19:16:48,961 main.py:47] epoch 884, training loss: 2361.52, average training loss: 2520.07, base loss: 2386.00
[INFO 2017-06-26 19:16:49,295 main.py:47] epoch 885, training loss: 2253.24, average training loss: 2519.77, base loss: 2386.11
[INFO 2017-06-26 19:16:49,625 main.py:47] epoch 886, training loss: 2622.89, average training loss: 2519.89, base loss: 2386.38
[INFO 2017-06-26 19:16:49,912 main.py:47] epoch 887, training loss: 2584.16, average training loss: 2519.96, base loss: 2386.54
[INFO 2017-06-26 19:16:50,202 main.py:47] epoch 888, training loss: 2271.15, average training loss: 2519.68, base loss: 2386.47
[INFO 2017-06-26 19:16:50,487 main.py:47] epoch 889, training loss: 2067.19, average training loss: 2519.17, base loss: 2386.25
[INFO 2017-06-26 19:16:50,772 main.py:47] epoch 890, training loss: 2023.08, average training loss: 2518.61, base loss: 2385.98
[INFO 2017-06-26 19:16:51,058 main.py:47] epoch 891, training loss: 1700.91, average training loss: 2517.70, base loss: 2385.33
[INFO 2017-06-26 19:16:51,349 main.py:47] epoch 892, training loss: 2245.31, average training loss: 2517.39, base loss: 2385.43
[INFO 2017-06-26 19:16:51,638 main.py:47] epoch 893, training loss: 1985.00, average training loss: 2516.80, base loss: 2385.07
[INFO 2017-06-26 19:16:51,929 main.py:47] epoch 894, training loss: 2039.27, average training loss: 2516.26, base loss: 2384.84
[INFO 2017-06-26 19:16:52,213 main.py:47] epoch 895, training loss: 2395.14, average training loss: 2516.13, base loss: 2384.95
[INFO 2017-06-26 19:16:52,496 main.py:47] epoch 896, training loss: 1955.42, average training loss: 2515.50, base loss: 2384.60
[INFO 2017-06-26 19:16:52,777 main.py:47] epoch 897, training loss: 2044.47, average training loss: 2514.98, base loss: 2384.35
[INFO 2017-06-26 19:16:53,064 main.py:47] epoch 898, training loss: 2048.91, average training loss: 2514.46, base loss: 2384.12
[INFO 2017-06-26 19:16:53,348 main.py:47] epoch 899, training loss: 1799.02, average training loss: 2513.67, base loss: 2383.57
[INFO 2017-06-26 19:16:53,349 main.py:49] epoch 899, testing
[INFO 2017-06-26 19:16:57,465 main.py:100] average testing loss: 2165.32, base loss: 2341.05
[INFO 2017-06-26 19:16:57,493 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:16:57,506 main.py:73] current best accuracy: 2165.32
[INFO 2017-06-26 19:16:57,793 main.py:47] epoch 900, training loss: 2169.91, average training loss: 2513.28, base loss: 2383.50
[INFO 2017-06-26 19:16:58,112 main.py:47] epoch 901, training loss: 2013.88, average training loss: 2512.73, base loss: 2383.34
[INFO 2017-06-26 19:16:58,405 main.py:47] epoch 902, training loss: 2598.23, average training loss: 2512.82, base loss: 2383.72
[INFO 2017-06-26 19:16:58,698 main.py:47] epoch 903, training loss: 2436.35, average training loss: 2512.74, base loss: 2383.91
[INFO 2017-06-26 19:16:58,988 main.py:47] epoch 904, training loss: 1957.32, average training loss: 2512.13, base loss: 2383.51
[INFO 2017-06-26 19:16:59,274 main.py:47] epoch 905, training loss: 1726.65, average training loss: 2511.26, base loss: 2382.82
[INFO 2017-06-26 19:16:59,572 main.py:47] epoch 906, training loss: 2215.18, average training loss: 2510.93, base loss: 2382.84
[INFO 2017-06-26 19:16:59,865 main.py:47] epoch 907, training loss: 1920.34, average training loss: 2510.28, base loss: 2382.41
[INFO 2017-06-26 19:17:00,155 main.py:47] epoch 908, training loss: 2297.26, average training loss: 2510.05, base loss: 2382.38
[INFO 2017-06-26 19:17:00,444 main.py:47] epoch 909, training loss: 2189.61, average training loss: 2509.70, base loss: 2382.27
[INFO 2017-06-26 19:17:00,729 main.py:47] epoch 910, training loss: 1903.31, average training loss: 2509.03, base loss: 2381.86
[INFO 2017-06-26 19:17:01,017 main.py:47] epoch 911, training loss: 2372.62, average training loss: 2508.88, base loss: 2382.00
[INFO 2017-06-26 19:17:01,307 main.py:47] epoch 912, training loss: 2130.08, average training loss: 2508.47, base loss: 2381.78
[INFO 2017-06-26 19:17:01,593 main.py:47] epoch 913, training loss: 2317.33, average training loss: 2508.26, base loss: 2381.85
[INFO 2017-06-26 19:17:01,882 main.py:47] epoch 914, training loss: 2432.68, average training loss: 2508.17, base loss: 2382.08
[INFO 2017-06-26 19:17:02,168 main.py:47] epoch 915, training loss: 2129.92, average training loss: 2507.76, base loss: 2381.93
[INFO 2017-06-26 19:17:02,457 main.py:47] epoch 916, training loss: 2300.07, average training loss: 2507.53, base loss: 2381.97
[INFO 2017-06-26 19:17:02,772 main.py:47] epoch 917, training loss: 2333.19, average training loss: 2507.34, base loss: 2382.10
[INFO 2017-06-26 19:17:03,115 main.py:47] epoch 918, training loss: 1986.59, average training loss: 2506.78, base loss: 2381.81
[INFO 2017-06-26 19:17:03,469 main.py:47] epoch 919, training loss: 2363.05, average training loss: 2506.62, base loss: 2381.91
[INFO 2017-06-26 19:17:03,804 main.py:47] epoch 920, training loss: 2267.43, average training loss: 2506.36, base loss: 2381.89
[INFO 2017-06-26 19:17:04,138 main.py:47] epoch 921, training loss: 2160.03, average training loss: 2505.99, base loss: 2381.84
[INFO 2017-06-26 19:17:04,476 main.py:47] epoch 922, training loss: 2544.02, average training loss: 2506.03, base loss: 2382.27
[INFO 2017-06-26 19:17:04,812 main.py:47] epoch 923, training loss: 2770.95, average training loss: 2506.31, base loss: 2382.96
[INFO 2017-06-26 19:17:05,113 main.py:47] epoch 924, training loss: 2054.37, average training loss: 2505.83, base loss: 2382.75
[INFO 2017-06-26 19:17:05,451 main.py:47] epoch 925, training loss: 2219.28, average training loss: 2505.52, base loss: 2382.72
[INFO 2017-06-26 19:17:05,792 main.py:47] epoch 926, training loss: 2635.51, average training loss: 2505.66, base loss: 2383.20
[INFO 2017-06-26 19:17:06,135 main.py:47] epoch 927, training loss: 1940.85, average training loss: 2505.05, base loss: 2382.81
[INFO 2017-06-26 19:17:06,455 main.py:47] epoch 928, training loss: 2385.02, average training loss: 2504.92, base loss: 2382.98
[INFO 2017-06-26 19:17:06,746 main.py:47] epoch 929, training loss: 1904.47, average training loss: 2504.27, base loss: 2382.63
[INFO 2017-06-26 19:17:07,045 main.py:47] epoch 930, training loss: 2263.15, average training loss: 2504.01, base loss: 2382.65
[INFO 2017-06-26 19:17:07,331 main.py:47] epoch 931, training loss: 2040.16, average training loss: 2503.52, base loss: 2382.44
[INFO 2017-06-26 19:17:07,613 main.py:47] epoch 932, training loss: 2221.31, average training loss: 2503.21, base loss: 2382.56
[INFO 2017-06-26 19:17:07,894 main.py:47] epoch 933, training loss: 2041.86, average training loss: 2502.72, base loss: 2382.42
[INFO 2017-06-26 19:17:08,184 main.py:47] epoch 934, training loss: 1901.03, average training loss: 2502.08, base loss: 2381.96
[INFO 2017-06-26 19:17:08,470 main.py:47] epoch 935, training loss: 2554.27, average training loss: 2502.13, base loss: 2382.35
[INFO 2017-06-26 19:17:08,759 main.py:47] epoch 936, training loss: 2041.99, average training loss: 2501.64, base loss: 2382.25
[INFO 2017-06-26 19:17:09,053 main.py:47] epoch 937, training loss: 2184.06, average training loss: 2501.30, base loss: 2382.11
[INFO 2017-06-26 19:17:09,335 main.py:47] epoch 938, training loss: 2057.77, average training loss: 2500.83, base loss: 2381.91
[INFO 2017-06-26 19:17:09,624 main.py:47] epoch 939, training loss: 2025.36, average training loss: 2500.32, base loss: 2381.69
[INFO 2017-06-26 19:17:09,916 main.py:47] epoch 940, training loss: 3187.30, average training loss: 2501.05, base loss: 2382.75
[INFO 2017-06-26 19:17:10,214 main.py:47] epoch 941, training loss: 2471.93, average training loss: 2501.02, base loss: 2383.15
[INFO 2017-06-26 19:17:10,498 main.py:47] epoch 942, training loss: 2086.96, average training loss: 2500.58, base loss: 2382.93
[INFO 2017-06-26 19:17:10,789 main.py:47] epoch 943, training loss: 1844.11, average training loss: 2499.89, base loss: 2382.47
[INFO 2017-06-26 19:17:11,075 main.py:47] epoch 944, training loss: 2421.30, average training loss: 2499.81, base loss: 2382.79
[INFO 2017-06-26 19:17:11,361 main.py:47] epoch 945, training loss: 2638.01, average training loss: 2499.95, base loss: 2383.19
[INFO 2017-06-26 19:17:11,648 main.py:47] epoch 946, training loss: 1853.31, average training loss: 2499.27, base loss: 2382.72
[INFO 2017-06-26 19:17:11,942 main.py:47] epoch 947, training loss: 2154.66, average training loss: 2498.91, base loss: 2382.64
[INFO 2017-06-26 19:17:12,234 main.py:47] epoch 948, training loss: 1871.16, average training loss: 2498.24, base loss: 2382.15
[INFO 2017-06-26 19:17:12,526 main.py:47] epoch 949, training loss: 2563.82, average training loss: 2498.31, base loss: 2382.52
[INFO 2017-06-26 19:17:12,819 main.py:47] epoch 950, training loss: 2573.51, average training loss: 2498.39, base loss: 2382.82
[INFO 2017-06-26 19:17:13,107 main.py:47] epoch 951, training loss: 2526.68, average training loss: 2498.42, base loss: 2383.33
[INFO 2017-06-26 19:17:13,397 main.py:47] epoch 952, training loss: 2016.07, average training loss: 2497.92, base loss: 2382.98
[INFO 2017-06-26 19:17:13,690 main.py:47] epoch 953, training loss: 2109.13, average training loss: 2497.51, base loss: 2382.79
[INFO 2017-06-26 19:17:13,980 main.py:47] epoch 954, training loss: 2721.22, average training loss: 2497.74, base loss: 2383.39
[INFO 2017-06-26 19:17:14,270 main.py:47] epoch 955, training loss: 1913.46, average training loss: 2497.13, base loss: 2383.10
[INFO 2017-06-26 19:17:14,556 main.py:47] epoch 956, training loss: 2149.31, average training loss: 2496.77, base loss: 2382.91
[INFO 2017-06-26 19:17:14,842 main.py:47] epoch 957, training loss: 2096.53, average training loss: 2496.35, base loss: 2382.85
[INFO 2017-06-26 19:17:15,131 main.py:47] epoch 958, training loss: 2230.23, average training loss: 2496.07, base loss: 2382.82
[INFO 2017-06-26 19:17:15,421 main.py:47] epoch 959, training loss: 2222.84, average training loss: 2495.79, base loss: 2382.76
[INFO 2017-06-26 19:17:15,707 main.py:47] epoch 960, training loss: 1861.78, average training loss: 2495.13, base loss: 2382.33
[INFO 2017-06-26 19:17:15,997 main.py:47] epoch 961, training loss: 2208.59, average training loss: 2494.83, base loss: 2382.33
[INFO 2017-06-26 19:17:16,319 main.py:47] epoch 962, training loss: 2370.39, average training loss: 2494.70, base loss: 2382.53
[INFO 2017-06-26 19:17:16,657 main.py:47] epoch 963, training loss: 2294.35, average training loss: 2494.49, base loss: 2382.63
[INFO 2017-06-26 19:17:16,993 main.py:47] epoch 964, training loss: 2173.31, average training loss: 2494.16, base loss: 2382.45
[INFO 2017-06-26 19:17:17,333 main.py:47] epoch 965, training loss: 2351.41, average training loss: 2494.01, base loss: 2382.64
[INFO 2017-06-26 19:17:17,694 main.py:47] epoch 966, training loss: 2496.20, average training loss: 2494.02, base loss: 2382.99
[INFO 2017-06-26 19:17:18,032 main.py:47] epoch 967, training loss: 2254.59, average training loss: 2493.77, base loss: 2383.02
[INFO 2017-06-26 19:17:18,365 main.py:47] epoch 968, training loss: 2076.25, average training loss: 2493.34, base loss: 2382.69
[INFO 2017-06-26 19:17:18,662 main.py:47] epoch 969, training loss: 2359.01, average training loss: 2493.20, base loss: 2382.81
[INFO 2017-06-26 19:17:19,001 main.py:47] epoch 970, training loss: 2340.53, average training loss: 2493.04, base loss: 2382.79
[INFO 2017-06-26 19:17:19,351 main.py:47] epoch 971, training loss: 2320.00, average training loss: 2492.86, base loss: 2382.90
[INFO 2017-06-26 19:17:19,713 main.py:47] epoch 972, training loss: 2175.57, average training loss: 2492.54, base loss: 2382.77
[INFO 2017-06-26 19:17:20,024 main.py:47] epoch 973, training loss: 1928.68, average training loss: 2491.96, base loss: 2382.41
[INFO 2017-06-26 19:17:20,310 main.py:47] epoch 974, training loss: 2224.52, average training loss: 2491.68, base loss: 2382.52
[INFO 2017-06-26 19:17:20,598 main.py:47] epoch 975, training loss: 2251.61, average training loss: 2491.44, base loss: 2382.58
[INFO 2017-06-26 19:17:20,888 main.py:47] epoch 976, training loss: 2127.20, average training loss: 2491.07, base loss: 2382.47
[INFO 2017-06-26 19:17:21,177 main.py:47] epoch 977, training loss: 2294.62, average training loss: 2490.86, base loss: 2382.60
[INFO 2017-06-26 19:17:21,462 main.py:47] epoch 978, training loss: 2388.87, average training loss: 2490.76, base loss: 2382.83
[INFO 2017-06-26 19:17:21,751 main.py:47] epoch 979, training loss: 2230.73, average training loss: 2490.49, base loss: 2382.86
[INFO 2017-06-26 19:17:22,040 main.py:47] epoch 980, training loss: 2045.32, average training loss: 2490.04, base loss: 2382.67
[INFO 2017-06-26 19:17:22,425 main.py:47] epoch 981, training loss: 2975.57, average training loss: 2490.54, base loss: 2383.49
[INFO 2017-06-26 19:17:22,728 main.py:47] epoch 982, training loss: 2366.96, average training loss: 2490.41, base loss: 2383.54
[INFO 2017-06-26 19:17:23,020 main.py:47] epoch 983, training loss: 1984.30, average training loss: 2489.90, base loss: 2383.23
[INFO 2017-06-26 19:17:23,310 main.py:47] epoch 984, training loss: 2240.48, average training loss: 2489.64, base loss: 2383.28
[INFO 2017-06-26 19:17:23,597 main.py:47] epoch 985, training loss: 1909.30, average training loss: 2489.05, base loss: 2382.93
[INFO 2017-06-26 19:17:23,928 main.py:47] epoch 986, training loss: 2128.46, average training loss: 2488.69, base loss: 2382.75
[INFO 2017-06-26 19:17:24,228 main.py:47] epoch 987, training loss: 2344.47, average training loss: 2488.54, base loss: 2382.95
[INFO 2017-06-26 19:17:24,515 main.py:47] epoch 988, training loss: 2051.77, average training loss: 2488.10, base loss: 2382.71
[INFO 2017-06-26 19:17:24,805 main.py:47] epoch 989, training loss: 2241.46, average training loss: 2487.85, base loss: 2382.68
[INFO 2017-06-26 19:17:25,103 main.py:47] epoch 990, training loss: 1892.02, average training loss: 2487.25, base loss: 2382.39
[INFO 2017-06-26 19:17:25,392 main.py:47] epoch 991, training loss: 2284.05, average training loss: 2487.05, base loss: 2382.49
[INFO 2017-06-26 19:17:25,683 main.py:47] epoch 992, training loss: 2043.44, average training loss: 2486.60, base loss: 2382.30
[INFO 2017-06-26 19:17:26,012 main.py:47] epoch 993, training loss: 2425.36, average training loss: 2486.54, base loss: 2382.44
[INFO 2017-06-26 19:17:26,326 main.py:47] epoch 994, training loss: 2673.23, average training loss: 2486.72, base loss: 2383.04
[INFO 2017-06-26 19:17:26,615 main.py:47] epoch 995, training loss: 2381.24, average training loss: 2486.62, base loss: 2383.26
[INFO 2017-06-26 19:17:26,903 main.py:47] epoch 996, training loss: 2405.15, average training loss: 2486.54, base loss: 2383.57
[INFO 2017-06-26 19:17:27,241 main.py:47] epoch 997, training loss: 2359.18, average training loss: 2486.41, base loss: 2383.79
[INFO 2017-06-26 19:17:27,553 main.py:47] epoch 998, training loss: 2148.28, average training loss: 2486.07, base loss: 2383.61
[INFO 2017-06-26 19:17:27,845 main.py:47] epoch 999, training loss: 2171.91, average training loss: 2485.76, base loss: 2383.46
[INFO 2017-06-26 19:17:27,845 main.py:49] epoch 999, testing
[INFO 2017-06-26 19:17:31,752 main.py:100] average testing loss: 2301.36, base loss: 2463.38
[INFO 2017-06-26 19:17:31,777 main.py:73] current best accuracy: 2165.32
[INFO 2017-06-26 19:17:32,067 main.py:47] epoch 1000, training loss: 2214.30, average training loss: 2454.22, base loss: 2383.13
[INFO 2017-06-26 19:17:32,392 main.py:47] epoch 1001, training loss: 2602.97, average training loss: 2430.41, base loss: 2383.74
[INFO 2017-06-26 19:17:32,684 main.py:47] epoch 1002, training loss: 2150.23, average training loss: 2409.28, base loss: 2383.86
[INFO 2017-06-26 19:17:32,976 main.py:47] epoch 1003, training loss: 1748.72, average training loss: 2391.24, base loss: 2383.64
[INFO 2017-06-26 19:17:33,263 main.py:47] epoch 1004, training loss: 2624.20, average training loss: 2376.38, base loss: 2384.07
[INFO 2017-06-26 19:17:33,552 main.py:47] epoch 1005, training loss: 2181.94, average training loss: 2363.33, base loss: 2384.24
[INFO 2017-06-26 19:17:33,837 main.py:47] epoch 1006, training loss: 1984.82, average training loss: 2351.75, base loss: 2383.93
[INFO 2017-06-26 19:17:34,122 main.py:47] epoch 1007, training loss: 1932.15, average training loss: 2342.40, base loss: 2383.89
[INFO 2017-06-26 19:17:34,405 main.py:47] epoch 1008, training loss: 2275.00, average training loss: 2334.52, base loss: 2383.68
[INFO 2017-06-26 19:17:34,692 main.py:47] epoch 1009, training loss: 1963.54, average training loss: 2327.85, base loss: 2383.56
[INFO 2017-06-26 19:17:34,975 main.py:47] epoch 1010, training loss: 1854.75, average training loss: 2321.98, base loss: 2383.28
[INFO 2017-06-26 19:17:35,353 main.py:47] epoch 1011, training loss: 2051.46, average training loss: 2317.23, base loss: 2383.27
[INFO 2017-06-26 19:17:35,653 main.py:47] epoch 1012, training loss: 2602.25, average training loss: 2313.68, base loss: 2383.62
[INFO 2017-06-26 19:17:35,944 main.py:47] epoch 1013, training loss: 2285.12, average training loss: 2310.42, base loss: 2383.75
[INFO 2017-06-26 19:17:36,230 main.py:47] epoch 1014, training loss: 2721.74, average training loss: 2307.93, base loss: 2383.94
[INFO 2017-06-26 19:17:36,526 main.py:47] epoch 1015, training loss: 2251.58, average training loss: 2305.99, base loss: 2384.30
[INFO 2017-06-26 19:17:36,900 main.py:47] epoch 1016, training loss: 3091.22, average training loss: 2304.73, base loss: 2384.89
[INFO 2017-06-26 19:17:37,200 main.py:47] epoch 1017, training loss: 2550.21, average training loss: 2303.52, base loss: 2385.24
[INFO 2017-06-26 19:17:37,485 main.py:47] epoch 1018, training loss: 2139.74, average training loss: 2302.37, base loss: 2385.39
[INFO 2017-06-26 19:17:37,834 main.py:47] epoch 1019, training loss: 2286.17, average training loss: 2301.42, base loss: 2385.66
[INFO 2017-06-26 19:17:38,151 main.py:47] epoch 1020, training loss: 2084.35, average training loss: 2300.43, base loss: 2385.67
[INFO 2017-06-26 19:17:38,442 main.py:47] epoch 1021, training loss: 2591.43, average training loss: 2300.16, base loss: 2386.25
[INFO 2017-06-26 19:17:38,807 main.py:47] epoch 1022, training loss: 2341.02, average training loss: 2299.39, base loss: 2386.11
[INFO 2017-06-26 19:17:39,132 main.py:47] epoch 1023, training loss: 1717.23, average training loss: 2298.34, base loss: 2385.66
[INFO 2017-06-26 19:17:39,425 main.py:47] epoch 1024, training loss: 1975.14, average training loss: 2297.91, base loss: 2385.66
[INFO 2017-06-26 19:17:39,717 main.py:47] epoch 1025, training loss: 2226.92, average training loss: 2297.56, base loss: 2385.76
[INFO 2017-06-26 19:17:40,004 main.py:47] epoch 1026, training loss: 2823.01, average training loss: 2297.92, base loss: 2386.60
[INFO 2017-06-26 19:17:40,288 main.py:47] epoch 1027, training loss: 2072.44, average training loss: 2297.13, base loss: 2386.20
[INFO 2017-06-26 19:17:40,574 main.py:47] epoch 1028, training loss: 2580.05, average training loss: 2297.05, base loss: 2386.56
[INFO 2017-06-26 19:17:40,862 main.py:47] epoch 1029, training loss: 2157.05, average training loss: 2296.51, base loss: 2386.28
[INFO 2017-06-26 19:17:41,148 main.py:47] epoch 1030, training loss: 2054.94, average training loss: 2295.95, base loss: 2386.01
[INFO 2017-06-26 19:17:41,432 main.py:47] epoch 1031, training loss: 2478.89, average training loss: 2295.92, base loss: 2386.30
[INFO 2017-06-26 19:17:41,717 main.py:47] epoch 1032, training loss: 2574.84, average training loss: 2296.23, base loss: 2387.03
[INFO 2017-06-26 19:17:42,002 main.py:47] epoch 1033, training loss: 1928.59, average training loss: 2295.65, base loss: 2386.69
[INFO 2017-06-26 19:17:42,287 main.py:47] epoch 1034, training loss: 1868.13, average training loss: 2295.14, base loss: 2386.31
[INFO 2017-06-26 19:17:42,644 main.py:47] epoch 1035, training loss: 2165.09, average training loss: 2294.91, base loss: 2386.28
[INFO 2017-06-26 19:17:42,943 main.py:47] epoch 1036, training loss: 2256.44, average training loss: 2294.76, base loss: 2386.41
[INFO 2017-06-26 19:17:43,229 main.py:47] epoch 1037, training loss: 2084.36, average training loss: 2294.62, base loss: 2386.37
[INFO 2017-06-26 19:17:43,572 main.py:47] epoch 1038, training loss: 2112.82, average training loss: 2294.01, base loss: 2386.03
[INFO 2017-06-26 19:17:43,895 main.py:47] epoch 1039, training loss: 2237.72, average training loss: 2293.42, base loss: 2385.61
[INFO 2017-06-26 19:17:44,191 main.py:47] epoch 1040, training loss: 2537.97, average training loss: 2293.42, base loss: 2385.93
[INFO 2017-06-26 19:17:44,493 main.py:47] epoch 1041, training loss: 2141.86, average training loss: 2293.27, base loss: 2385.93
[INFO 2017-06-26 19:17:44,783 main.py:47] epoch 1042, training loss: 2047.75, average training loss: 2292.42, base loss: 2385.25
[INFO 2017-06-26 19:17:45,094 main.py:47] epoch 1043, training loss: 2228.53, average training loss: 2292.39, base loss: 2385.29
[INFO 2017-06-26 19:17:45,383 main.py:47] epoch 1044, training loss: 2458.74, average training loss: 2292.33, base loss: 2385.44
[INFO 2017-06-26 19:17:45,674 main.py:47] epoch 1045, training loss: 2301.30, average training loss: 2292.42, base loss: 2385.74
[INFO 2017-06-26 19:17:45,959 main.py:47] epoch 1046, training loss: 2252.58, average training loss: 2291.96, base loss: 2385.48
[INFO 2017-06-26 19:17:46,243 main.py:47] epoch 1047, training loss: 2400.00, average training loss: 2292.09, base loss: 2385.77
[INFO 2017-06-26 19:17:46,526 main.py:47] epoch 1048, training loss: 2025.26, average training loss: 2291.51, base loss: 2385.43
[INFO 2017-06-26 19:17:46,811 main.py:47] epoch 1049, training loss: 1879.18, average training loss: 2291.22, base loss: 2385.39
[INFO 2017-06-26 19:17:47,100 main.py:47] epoch 1050, training loss: 2359.85, average training loss: 2290.83, base loss: 2385.23
[INFO 2017-06-26 19:17:47,384 main.py:47] epoch 1051, training loss: 2486.62, average training loss: 2290.92, base loss: 2385.52
[INFO 2017-06-26 19:17:47,673 main.py:47] epoch 1052, training loss: 2539.52, average training loss: 2291.02, base loss: 2385.88
[INFO 2017-06-26 19:17:47,961 main.py:47] epoch 1053, training loss: 2289.94, average training loss: 2291.10, base loss: 2386.14
[INFO 2017-06-26 19:17:48,245 main.py:47] epoch 1054, training loss: 2359.15, average training loss: 2291.29, base loss: 2386.59
[INFO 2017-06-26 19:17:48,529 main.py:47] epoch 1055, training loss: 2102.77, average training loss: 2290.92, base loss: 2386.41
[INFO 2017-06-26 19:17:48,813 main.py:47] epoch 1056, training loss: 2055.15, average training loss: 2290.97, base loss: 2386.66
[INFO 2017-06-26 19:17:49,096 main.py:47] epoch 1057, training loss: 2049.34, average training loss: 2290.13, base loss: 2386.01
[INFO 2017-06-26 19:17:49,422 main.py:47] epoch 1058, training loss: 2475.72, average training loss: 2290.20, base loss: 2386.17
[INFO 2017-06-26 19:17:49,734 main.py:47] epoch 1059, training loss: 2305.71, average training loss: 2289.60, base loss: 2385.77
[INFO 2017-06-26 19:17:50,025 main.py:47] epoch 1060, training loss: 1805.97, average training loss: 2289.15, base loss: 2385.52
[INFO 2017-06-26 19:17:50,314 main.py:47] epoch 1061, training loss: 2460.63, average training loss: 2289.18, base loss: 2385.79
[INFO 2017-06-26 19:17:50,685 main.py:47] epoch 1062, training loss: 2228.68, average training loss: 2289.13, base loss: 2385.92
[INFO 2017-06-26 19:17:50,980 main.py:47] epoch 1063, training loss: 2200.71, average training loss: 2288.75, base loss: 2385.61
[INFO 2017-06-26 19:17:51,267 main.py:47] epoch 1064, training loss: 2042.78, average training loss: 2288.38, base loss: 2385.31
[INFO 2017-06-26 19:17:51,555 main.py:47] epoch 1065, training loss: 2055.19, average training loss: 2288.20, base loss: 2385.25
[INFO 2017-06-26 19:17:51,916 main.py:47] epoch 1066, training loss: 2121.94, average training loss: 2287.71, base loss: 2384.90
[INFO 2017-06-26 19:17:52,220 main.py:47] epoch 1067, training loss: 1984.60, average training loss: 2286.63, base loss: 2383.96
[INFO 2017-06-26 19:17:52,507 main.py:47] epoch 1068, training loss: 2065.83, average training loss: 2286.24, base loss: 2383.74
[INFO 2017-06-26 19:17:52,793 main.py:47] epoch 1069, training loss: 2311.40, average training loss: 2286.01, base loss: 2383.70
[INFO 2017-06-26 19:17:53,080 main.py:47] epoch 1070, training loss: 2642.92, average training loss: 2285.66, base loss: 2383.53
[INFO 2017-06-26 19:17:53,370 main.py:47] epoch 1071, training loss: 2045.79, average training loss: 2285.09, base loss: 2383.13
[INFO 2017-06-26 19:17:53,652 main.py:47] epoch 1072, training loss: 2332.29, average training loss: 2285.29, base loss: 2383.51
[INFO 2017-06-26 19:17:53,942 main.py:47] epoch 1073, training loss: 1933.67, average training loss: 2284.69, base loss: 2383.00
[INFO 2017-06-26 19:17:54,224 main.py:47] epoch 1074, training loss: 2019.68, average training loss: 2284.41, base loss: 2382.93
[INFO 2017-06-26 19:17:54,513 main.py:47] epoch 1075, training loss: 2278.95, average training loss: 2284.05, base loss: 2382.81
[INFO 2017-06-26 19:17:54,796 main.py:47] epoch 1076, training loss: 1928.64, average training loss: 2283.73, base loss: 2382.64
[INFO 2017-06-26 19:17:55,128 main.py:47] epoch 1077, training loss: 1809.08, average training loss: 2283.18, base loss: 2382.14
[INFO 2017-06-26 19:17:55,447 main.py:47] epoch 1078, training loss: 2296.53, average training loss: 2283.13, base loss: 2382.33
[INFO 2017-06-26 19:17:55,742 main.py:47] epoch 1079, training loss: 2326.48, average training loss: 2283.00, base loss: 2382.44
[INFO 2017-06-26 19:17:56,098 main.py:47] epoch 1080, training loss: 2163.22, average training loss: 2282.78, base loss: 2382.32
[INFO 2017-06-26 19:17:56,401 main.py:47] epoch 1081, training loss: 2576.09, average training loss: 2282.69, base loss: 2382.44
[INFO 2017-06-26 19:17:56,692 main.py:47] epoch 1082, training loss: 2005.05, average training loss: 2282.67, base loss: 2382.61
[INFO 2017-06-26 19:17:57,028 main.py:47] epoch 1083, training loss: 2389.15, average training loss: 2282.41, base loss: 2382.62
[INFO 2017-06-26 19:17:57,352 main.py:47] epoch 1084, training loss: 2171.26, average training loss: 2282.42, base loss: 2382.75
[INFO 2017-06-26 19:17:57,672 main.py:47] epoch 1085, training loss: 2037.34, average training loss: 2282.03, base loss: 2382.60
[INFO 2017-06-26 19:17:58,043 main.py:47] epoch 1086, training loss: 2046.73, average training loss: 2281.77, base loss: 2382.32
[INFO 2017-06-26 19:17:58,344 main.py:47] epoch 1087, training loss: 2250.06, average training loss: 2281.06, base loss: 2381.75
[INFO 2017-06-26 19:17:58,694 main.py:47] epoch 1088, training loss: 2494.96, average training loss: 2281.09, base loss: 2382.06
[INFO 2017-06-26 19:17:58,992 main.py:47] epoch 1089, training loss: 2166.19, average training loss: 2280.82, base loss: 2381.90
[INFO 2017-06-26 19:17:59,282 main.py:47] epoch 1090, training loss: 2511.78, average training loss: 2280.85, base loss: 2382.14
[INFO 2017-06-26 19:17:59,631 main.py:47] epoch 1091, training loss: 2347.42, average training loss: 2280.98, base loss: 2382.39
[INFO 2017-06-26 19:17:59,943 main.py:47] epoch 1092, training loss: 2123.73, average training loss: 2280.80, base loss: 2382.42
[INFO 2017-06-26 19:18:00,250 main.py:47] epoch 1093, training loss: 1983.88, average training loss: 2280.59, base loss: 2382.39
[INFO 2017-06-26 19:18:00,563 main.py:47] epoch 1094, training loss: 2120.90, average training loss: 2280.27, base loss: 2382.13
[INFO 2017-06-26 19:18:00,861 main.py:47] epoch 1095, training loss: 2414.03, average training loss: 2280.20, base loss: 2382.21
[INFO 2017-06-26 19:18:01,147 main.py:47] epoch 1096, training loss: 2027.13, average training loss: 2280.07, base loss: 2382.29
[INFO 2017-06-26 19:18:01,434 main.py:47] epoch 1097, training loss: 2251.72, average training loss: 2280.08, base loss: 2382.46
[INFO 2017-06-26 19:18:01,724 main.py:47] epoch 1098, training loss: 1907.97, average training loss: 2280.00, base loss: 2382.50
[INFO 2017-06-26 19:18:02,013 main.py:47] epoch 1099, training loss: 2304.74, average training loss: 2280.10, base loss: 2382.73
[INFO 2017-06-26 19:18:02,013 main.py:49] epoch 1099, testing
[INFO 2017-06-26 19:18:05,793 main.py:100] average testing loss: 2234.05, base loss: 2375.76
[INFO 2017-06-26 19:18:05,819 main.py:73] current best accuracy: 2165.32
[INFO 2017-06-26 19:18:06,104 main.py:47] epoch 1100, training loss: 2121.67, average training loss: 2280.36, base loss: 2383.11
[INFO 2017-06-26 19:18:06,391 main.py:47] epoch 1101, training loss: 1839.68, average training loss: 2279.89, base loss: 2382.75
[INFO 2017-06-26 19:18:06,676 main.py:47] epoch 1102, training loss: 2024.64, average training loss: 2279.39, base loss: 2382.47
[INFO 2017-06-26 19:18:06,966 main.py:47] epoch 1103, training loss: 2202.91, average training loss: 2279.45, base loss: 2382.68
[INFO 2017-06-26 19:18:07,254 main.py:47] epoch 1104, training loss: 1662.25, average training loss: 2278.58, base loss: 2381.93
[INFO 2017-06-26 19:18:07,542 main.py:47] epoch 1105, training loss: 2241.49, average training loss: 2278.39, base loss: 2381.95
[INFO 2017-06-26 19:18:07,824 main.py:47] epoch 1106, training loss: 2539.62, average training loss: 2278.46, base loss: 2382.22
[INFO 2017-06-26 19:18:08,108 main.py:47] epoch 1107, training loss: 2493.89, average training loss: 2278.24, base loss: 2382.18
[INFO 2017-06-26 19:18:08,396 main.py:47] epoch 1108, training loss: 2092.35, average training loss: 2277.95, base loss: 2382.08
[INFO 2017-06-26 19:18:08,684 main.py:47] epoch 1109, training loss: 1925.59, average training loss: 2277.28, base loss: 2381.58
[INFO 2017-06-26 19:18:08,974 main.py:47] epoch 1110, training loss: 2378.52, average training loss: 2277.09, base loss: 2381.62
[INFO 2017-06-26 19:18:09,330 main.py:47] epoch 1111, training loss: 2294.14, average training loss: 2276.74, base loss: 2381.51
[INFO 2017-06-26 19:18:09,630 main.py:47] epoch 1112, training loss: 2081.22, average training loss: 2276.32, base loss: 2381.21
[INFO 2017-06-26 19:18:09,921 main.py:47] epoch 1113, training loss: 2218.76, average training loss: 2275.81, base loss: 2380.76
[INFO 2017-06-26 19:18:10,213 main.py:47] epoch 1114, training loss: 2161.93, average training loss: 2275.66, base loss: 2380.86
[INFO 2017-06-26 19:18:10,559 main.py:47] epoch 1115, training loss: 2395.53, average training loss: 2275.53, base loss: 2380.89
[INFO 2017-06-26 19:18:10,874 main.py:47] epoch 1116, training loss: 2384.49, average training loss: 2275.63, base loss: 2381.21
[INFO 2017-06-26 19:18:11,213 main.py:47] epoch 1117, training loss: 2303.10, average training loss: 2275.61, base loss: 2381.29
[INFO 2017-06-26 19:18:11,543 main.py:47] epoch 1118, training loss: 2138.72, average training loss: 2275.43, base loss: 2381.39
[INFO 2017-06-26 19:18:11,833 main.py:47] epoch 1119, training loss: 1855.80, average training loss: 2275.39, base loss: 2381.36
[INFO 2017-06-26 19:18:12,165 main.py:47] epoch 1120, training loss: 2414.27, average training loss: 2275.24, base loss: 2381.38
[INFO 2017-06-26 19:18:12,464 main.py:47] epoch 1121, training loss: 1773.12, average training loss: 2274.46, base loss: 2380.75
[INFO 2017-06-26 19:18:12,776 main.py:47] epoch 1122, training loss: 1774.84, average training loss: 2273.53, base loss: 2379.92
[INFO 2017-06-26 19:18:13,087 main.py:47] epoch 1123, training loss: 2181.37, average training loss: 2273.75, base loss: 2380.41
[INFO 2017-06-26 19:18:13,383 main.py:47] epoch 1124, training loss: 2239.39, average training loss: 2273.56, base loss: 2380.43
[INFO 2017-06-26 19:18:13,734 main.py:47] epoch 1125, training loss: 2288.67, average training loss: 2273.23, base loss: 2380.20
[INFO 2017-06-26 19:18:14,041 main.py:47] epoch 1126, training loss: 2440.55, average training loss: 2273.12, base loss: 2380.24
[INFO 2017-06-26 19:18:14,345 main.py:47] epoch 1127, training loss: 2079.32, average training loss: 2272.65, base loss: 2380.00
[INFO 2017-06-26 19:18:14,673 main.py:47] epoch 1128, training loss: 1581.90, average training loss: 2272.22, base loss: 2379.68
[INFO 2017-06-26 19:18:14,978 main.py:47] epoch 1129, training loss: 2034.68, average training loss: 2271.87, base loss: 2379.57
[INFO 2017-06-26 19:18:15,365 main.py:47] epoch 1130, training loss: 2164.34, average training loss: 2271.26, base loss: 2379.13
[INFO 2017-06-26 19:18:15,716 main.py:47] epoch 1131, training loss: 2327.01, average training loss: 2271.21, base loss: 2379.29
[INFO 2017-06-26 19:18:16,008 main.py:47] epoch 1132, training loss: 2225.58, average training loss: 2271.29, base loss: 2379.49
[INFO 2017-06-26 19:18:16,293 main.py:47] epoch 1133, training loss: 2576.05, average training loss: 2271.80, base loss: 2380.19
[INFO 2017-06-26 19:18:16,640 main.py:47] epoch 1134, training loss: 2148.05, average training loss: 2271.60, base loss: 2380.21
[INFO 2017-06-26 19:18:16,938 main.py:47] epoch 1135, training loss: 2046.67, average training loss: 2271.19, base loss: 2379.88
[INFO 2017-06-26 19:18:17,232 main.py:47] epoch 1136, training loss: 2557.62, average training loss: 2271.56, base loss: 2380.47
[INFO 2017-06-26 19:18:17,523 main.py:47] epoch 1137, training loss: 2081.28, average training loss: 2271.37, base loss: 2380.38
[INFO 2017-06-26 19:18:17,861 main.py:47] epoch 1138, training loss: 2027.12, average training loss: 2270.65, base loss: 2379.79
[INFO 2017-06-26 19:18:18,165 main.py:47] epoch 1139, training loss: 2487.77, average training loss: 2271.19, base loss: 2380.55
[INFO 2017-06-26 19:18:18,452 main.py:47] epoch 1140, training loss: 1940.22, average training loss: 2271.08, base loss: 2380.66
[INFO 2017-06-26 19:18:18,742 main.py:47] epoch 1141, training loss: 2488.30, average training loss: 2271.06, base loss: 2380.68
[INFO 2017-06-26 19:18:19,097 main.py:47] epoch 1142, training loss: 2260.06, average training loss: 2271.21, base loss: 2381.00
[INFO 2017-06-26 19:18:19,418 main.py:47] epoch 1143, training loss: 2357.35, average training loss: 2271.47, base loss: 2381.41
[INFO 2017-06-26 19:18:19,713 main.py:47] epoch 1144, training loss: 2020.03, average training loss: 2270.98, base loss: 2380.94
[INFO 2017-06-26 19:18:20,052 main.py:47] epoch 1145, training loss: 2096.75, average training loss: 2270.62, base loss: 2380.75
[INFO 2017-06-26 19:18:20,372 main.py:47] epoch 1146, training loss: 2049.70, average training loss: 2270.08, base loss: 2380.29
[INFO 2017-06-26 19:18:20,734 main.py:47] epoch 1147, training loss: 2384.66, average training loss: 2269.94, base loss: 2380.29
[INFO 2017-06-26 19:18:21,052 main.py:47] epoch 1148, training loss: 1805.91, average training loss: 2269.37, base loss: 2379.84
[INFO 2017-06-26 19:18:21,346 main.py:47] epoch 1149, training loss: 2227.39, average training loss: 2268.99, base loss: 2379.68
[INFO 2017-06-26 19:18:21,688 main.py:47] epoch 1150, training loss: 1855.87, average training loss: 2268.88, base loss: 2379.65
[INFO 2017-06-26 19:18:21,994 main.py:47] epoch 1151, training loss: 2045.27, average training loss: 2268.78, base loss: 2379.68
[INFO 2017-06-26 19:18:22,360 main.py:47] epoch 1152, training loss: 2137.37, average training loss: 2268.35, base loss: 2379.34
[INFO 2017-06-26 19:18:22,657 main.py:47] epoch 1153, training loss: 2198.59, average training loss: 2268.25, base loss: 2379.48
[INFO 2017-06-26 19:18:22,947 main.py:47] epoch 1154, training loss: 2326.06, average training loss: 2268.32, base loss: 2379.71
[INFO 2017-06-26 19:18:23,302 main.py:47] epoch 1155, training loss: 2119.10, average training loss: 2268.15, base loss: 2379.64
[INFO 2017-06-26 19:18:23,606 main.py:47] epoch 1156, training loss: 2216.40, average training loss: 2267.94, base loss: 2379.61
[INFO 2017-06-26 19:18:23,978 main.py:47] epoch 1157, training loss: 2364.97, average training loss: 2268.44, base loss: 2380.31
[INFO 2017-06-26 19:18:24,310 main.py:47] epoch 1158, training loss: 2081.04, average training loss: 2268.06, base loss: 2380.06
[INFO 2017-06-26 19:18:24,600 main.py:47] epoch 1159, training loss: 2173.90, average training loss: 2267.76, base loss: 2379.85
[INFO 2017-06-26 19:18:24,963 main.py:47] epoch 1160, training loss: 1893.14, average training loss: 2267.32, base loss: 2379.50
[INFO 2017-06-26 19:18:25,277 main.py:47] epoch 1161, training loss: 2238.40, average training loss: 2267.08, base loss: 2379.34
[INFO 2017-06-26 19:18:25,568 main.py:47] epoch 1162, training loss: 1829.08, average training loss: 2266.54, base loss: 2378.95
[INFO 2017-06-26 19:18:25,914 main.py:47] epoch 1163, training loss: 2072.05, average training loss: 2265.87, base loss: 2378.42
[INFO 2017-06-26 19:18:26,223 main.py:47] epoch 1164, training loss: 1847.84, average training loss: 2265.33, base loss: 2377.98
[INFO 2017-06-26 19:18:26,514 main.py:47] epoch 1165, training loss: 2503.41, average training loss: 2265.68, base loss: 2378.47
[INFO 2017-06-26 19:18:26,819 main.py:47] epoch 1166, training loss: 2756.51, average training loss: 2266.59, base loss: 2379.61
[INFO 2017-06-26 19:18:27,125 main.py:47] epoch 1167, training loss: 1998.87, average training loss: 2266.50, base loss: 2379.77
[INFO 2017-06-26 19:18:27,416 main.py:47] epoch 1168, training loss: 2036.81, average training loss: 2265.95, base loss: 2379.35
[INFO 2017-06-26 19:18:27,777 main.py:47] epoch 1169, training loss: 2069.11, average training loss: 2265.40, base loss: 2378.93
[INFO 2017-06-26 19:18:28,100 main.py:47] epoch 1170, training loss: 1932.39, average training loss: 2264.63, base loss: 2378.21
[INFO 2017-06-26 19:18:28,393 main.py:47] epoch 1171, training loss: 2013.26, average training loss: 2264.34, base loss: 2378.11
[INFO 2017-06-26 19:18:28,733 main.py:47] epoch 1172, training loss: 2023.83, average training loss: 2263.93, base loss: 2377.83
[INFO 2017-06-26 19:18:29,041 main.py:47] epoch 1173, training loss: 2062.41, average training loss: 2263.98, base loss: 2378.00
[INFO 2017-06-26 19:18:29,328 main.py:47] epoch 1174, training loss: 2691.83, average training loss: 2264.27, base loss: 2378.59
[INFO 2017-06-26 19:18:29,674 main.py:47] epoch 1175, training loss: 2315.03, average training loss: 2264.47, base loss: 2378.94
[INFO 2017-06-26 19:18:29,970 main.py:47] epoch 1176, training loss: 2175.83, average training loss: 2263.36, base loss: 2377.96
[INFO 2017-06-26 19:18:30,304 main.py:47] epoch 1177, training loss: 2012.57, average training loss: 2262.85, base loss: 2377.56
[INFO 2017-06-26 19:18:30,619 main.py:47] epoch 1178, training loss: 2181.62, average training loss: 2262.75, base loss: 2377.56
[INFO 2017-06-26 19:18:30,906 main.py:47] epoch 1179, training loss: 2455.05, average training loss: 2262.95, base loss: 2377.93
[INFO 2017-06-26 19:18:31,254 main.py:47] epoch 1180, training loss: 2203.77, average training loss: 2262.82, base loss: 2377.87
[INFO 2017-06-26 19:18:31,557 main.py:47] epoch 1181, training loss: 2180.45, average training loss: 2263.09, base loss: 2378.35
[INFO 2017-06-26 19:18:31,938 main.py:47] epoch 1182, training loss: 1942.16, average training loss: 2262.64, base loss: 2378.04
[INFO 2017-06-26 19:18:32,269 main.py:47] epoch 1183, training loss: 2275.44, average training loss: 2262.91, base loss: 2378.44
[INFO 2017-06-26 19:18:32,560 main.py:47] epoch 1184, training loss: 1831.62, average training loss: 2261.69, base loss: 2377.23
[INFO 2017-06-26 19:18:32,898 main.py:47] epoch 1185, training loss: 2468.26, average training loss: 2262.03, base loss: 2377.92
[INFO 2017-06-26 19:18:33,205 main.py:47] epoch 1186, training loss: 2819.84, average training loss: 2262.31, base loss: 2378.36
[INFO 2017-06-26 19:18:33,490 main.py:47] epoch 1187, training loss: 2359.32, average training loss: 2261.86, base loss: 2378.08
[INFO 2017-06-26 19:18:33,828 main.py:47] epoch 1188, training loss: 2203.02, average training loss: 2262.11, base loss: 2378.56
[INFO 2017-06-26 19:18:34,137 main.py:47] epoch 1189, training loss: 2058.12, average training loss: 2261.25, base loss: 2377.86
[INFO 2017-06-26 19:18:34,447 main.py:47] epoch 1190, training loss: 1934.41, average training loss: 2260.83, base loss: 2377.44
[INFO 2017-06-26 19:18:34,782 main.py:47] epoch 1191, training loss: 1855.69, average training loss: 2260.14, base loss: 2376.78
[INFO 2017-06-26 19:18:35,092 main.py:47] epoch 1192, training loss: 1891.47, average training loss: 2259.48, base loss: 2376.24
[INFO 2017-06-26 19:18:35,455 main.py:47] epoch 1193, training loss: 1881.03, average training loss: 2259.16, base loss: 2375.98
[INFO 2017-06-26 19:18:35,775 main.py:47] epoch 1194, training loss: 1868.08, average training loss: 2258.54, base loss: 2375.45
[INFO 2017-06-26 19:18:36,066 main.py:47] epoch 1195, training loss: 2375.78, average training loss: 2258.84, base loss: 2375.94
[INFO 2017-06-26 19:18:36,412 main.py:47] epoch 1196, training loss: 2208.62, average training loss: 2258.65, base loss: 2375.88
[INFO 2017-06-26 19:18:36,720 main.py:47] epoch 1197, training loss: 2061.39, average training loss: 2258.55, base loss: 2375.91
[INFO 2017-06-26 19:18:37,095 main.py:47] epoch 1198, training loss: 2243.82, average training loss: 2258.90, base loss: 2376.52
[INFO 2017-06-26 19:18:37,407 main.py:47] epoch 1199, training loss: 2405.13, average training loss: 2258.85, base loss: 2376.63
[INFO 2017-06-26 19:18:37,407 main.py:49] epoch 1199, testing
[INFO 2017-06-26 19:18:42,022 main.py:100] average testing loss: 2317.70, base loss: 2514.17
[INFO 2017-06-26 19:18:42,048 main.py:73] current best accuracy: 2165.32
[INFO 2017-06-26 19:18:42,421 main.py:47] epoch 1200, training loss: 2718.54, average training loss: 2259.01, base loss: 2376.95
[INFO 2017-06-26 19:18:42,726 main.py:47] epoch 1201, training loss: 2122.91, average training loss: 2259.23, base loss: 2377.27
[INFO 2017-06-26 19:18:43,017 main.py:47] epoch 1202, training loss: 1938.32, average training loss: 2258.63, base loss: 2376.76
[INFO 2017-06-26 19:18:43,311 main.py:47] epoch 1203, training loss: 2381.83, average training loss: 2258.89, base loss: 2377.23
[INFO 2017-06-26 19:18:43,665 main.py:47] epoch 1204, training loss: 1857.81, average training loss: 2258.51, base loss: 2376.96
[INFO 2017-06-26 19:18:43,969 main.py:47] epoch 1205, training loss: 2817.21, average training loss: 2258.79, base loss: 2377.46
[INFO 2017-06-26 19:18:44,260 main.py:47] epoch 1206, training loss: 2286.85, average training loss: 2258.40, base loss: 2377.10
[INFO 2017-06-26 19:18:44,552 main.py:47] epoch 1207, training loss: 2804.94, average training loss: 2259.00, base loss: 2377.87
[INFO 2017-06-26 19:18:44,846 main.py:47] epoch 1208, training loss: 2308.45, average training loss: 2258.98, base loss: 2378.07
[INFO 2017-06-26 19:18:45,137 main.py:47] epoch 1209, training loss: 2213.73, average training loss: 2258.37, base loss: 2377.69
[INFO 2017-06-26 19:18:45,425 main.py:47] epoch 1210, training loss: 2202.71, average training loss: 2258.30, base loss: 2377.74
[INFO 2017-06-26 19:18:45,709 main.py:47] epoch 1211, training loss: 1990.67, average training loss: 2257.78, base loss: 2377.25
[INFO 2017-06-26 19:18:45,999 main.py:47] epoch 1212, training loss: 2340.91, average training loss: 2257.60, base loss: 2377.17
[INFO 2017-06-26 19:18:46,294 main.py:47] epoch 1213, training loss: 2495.85, average training loss: 2257.77, base loss: 2377.35
[INFO 2017-06-26 19:18:46,584 main.py:47] epoch 1214, training loss: 2518.16, average training loss: 2257.82, base loss: 2377.66
[INFO 2017-06-26 19:18:46,874 main.py:47] epoch 1215, training loss: 2671.66, average training loss: 2258.27, base loss: 2378.25
[INFO 2017-06-26 19:18:47,167 main.py:47] epoch 1216, training loss: 2368.54, average training loss: 2257.85, base loss: 2377.95
[INFO 2017-06-26 19:18:47,456 main.py:47] epoch 1217, training loss: 2169.64, average training loss: 2257.83, base loss: 2378.04
[INFO 2017-06-26 19:18:47,742 main.py:47] epoch 1218, training loss: 2449.67, average training loss: 2258.07, base loss: 2378.53
[INFO 2017-06-26 19:18:48,033 main.py:47] epoch 1219, training loss: 2147.60, average training loss: 2258.04, base loss: 2378.58
[INFO 2017-06-26 19:18:48,322 main.py:47] epoch 1220, training loss: 1943.23, average training loss: 2257.84, base loss: 2378.55
[INFO 2017-06-26 19:18:48,610 main.py:47] epoch 1221, training loss: 2075.84, average training loss: 2257.26, base loss: 2378.10
[INFO 2017-06-26 19:18:48,898 main.py:47] epoch 1222, training loss: 1921.06, average training loss: 2256.81, base loss: 2377.81
[INFO 2017-06-26 19:18:49,179 main.py:47] epoch 1223, training loss: 2044.24, average training loss: 2256.28, base loss: 2377.49
[INFO 2017-06-26 19:18:49,466 main.py:47] epoch 1224, training loss: 2515.70, average training loss: 2256.70, base loss: 2378.08
[INFO 2017-06-26 19:18:49,755 main.py:47] epoch 1225, training loss: 1935.47, average training loss: 2256.23, base loss: 2377.58
[INFO 2017-06-26 19:18:50,044 main.py:47] epoch 1226, training loss: 2120.02, average training loss: 2255.89, base loss: 2377.42
[INFO 2017-06-26 19:18:50,329 main.py:47] epoch 1227, training loss: 2215.39, average training loss: 2256.28, base loss: 2377.97
[INFO 2017-06-26 19:18:50,610 main.py:47] epoch 1228, training loss: 2493.58, average training loss: 2256.05, base loss: 2377.87
[INFO 2017-06-26 19:18:50,895 main.py:47] epoch 1229, training loss: 1773.33, average training loss: 2255.59, base loss: 2377.62
[INFO 2017-06-26 19:18:51,182 main.py:47] epoch 1230, training loss: 2545.60, average training loss: 2255.78, base loss: 2377.99
[INFO 2017-06-26 19:18:51,466 main.py:47] epoch 1231, training loss: 2081.40, average training loss: 2255.49, base loss: 2377.80
[INFO 2017-06-26 19:18:51,754 main.py:47] epoch 1232, training loss: 2062.14, average training loss: 2255.22, base loss: 2377.67
[INFO 2017-06-26 19:18:52,036 main.py:47] epoch 1233, training loss: 2025.19, average training loss: 2254.92, base loss: 2377.37
[INFO 2017-06-26 19:18:52,323 main.py:47] epoch 1234, training loss: 1819.92, average training loss: 2254.69, base loss: 2377.31
[INFO 2017-06-26 19:18:52,611 main.py:47] epoch 1235, training loss: 2382.49, average training loss: 2255.11, base loss: 2377.87
[INFO 2017-06-26 19:18:52,898 main.py:47] epoch 1236, training loss: 2057.93, average training loss: 2254.99, base loss: 2377.87
[INFO 2017-06-26 19:18:53,185 main.py:47] epoch 1237, training loss: 2061.64, average training loss: 2255.20, base loss: 2378.12
[INFO 2017-06-26 19:18:53,471 main.py:47] epoch 1238, training loss: 1916.88, average training loss: 2254.64, base loss: 2377.61
[INFO 2017-06-26 19:18:53,755 main.py:47] epoch 1239, training loss: 2535.31, average training loss: 2255.23, base loss: 2378.40
[INFO 2017-06-26 19:18:54,044 main.py:47] epoch 1240, training loss: 2125.57, average training loss: 2255.11, base loss: 2378.43
[INFO 2017-06-26 19:18:54,329 main.py:47] epoch 1241, training loss: 2309.64, average training loss: 2255.23, base loss: 2378.84
[INFO 2017-06-26 19:18:54,618 main.py:47] epoch 1242, training loss: 2559.35, average training loss: 2255.41, base loss: 2379.23
[INFO 2017-06-26 19:18:54,906 main.py:47] epoch 1243, training loss: 2370.30, average training loss: 2255.58, base loss: 2379.62
[INFO 2017-06-26 19:18:55,194 main.py:47] epoch 1244, training loss: 1896.88, average training loss: 2255.37, base loss: 2379.30
[INFO 2017-06-26 19:18:55,481 main.py:47] epoch 1245, training loss: 2289.58, average training loss: 2255.42, base loss: 2379.51
[INFO 2017-06-26 19:18:55,768 main.py:47] epoch 1246, training loss: 1880.63, average training loss: 2255.19, base loss: 2379.44
[INFO 2017-06-26 19:18:56,053 main.py:47] epoch 1247, training loss: 2328.86, average training loss: 2254.73, base loss: 2379.10
[INFO 2017-06-26 19:18:56,342 main.py:47] epoch 1248, training loss: 2376.55, average training loss: 2255.27, base loss: 2379.84
[INFO 2017-06-26 19:18:56,625 main.py:47] epoch 1249, training loss: 2212.09, average training loss: 2255.00, base loss: 2379.68
[INFO 2017-06-26 19:18:56,910 main.py:47] epoch 1250, training loss: 2408.01, average training loss: 2255.06, base loss: 2380.02
[INFO 2017-06-26 19:18:57,195 main.py:47] epoch 1251, training loss: 2204.02, average training loss: 2254.97, base loss: 2380.09
[INFO 2017-06-26 19:18:57,481 main.py:47] epoch 1252, training loss: 2202.80, average training loss: 2254.79, base loss: 2380.12
[INFO 2017-06-26 19:18:57,766 main.py:47] epoch 1253, training loss: 2259.83, average training loss: 2254.68, base loss: 2380.06
[INFO 2017-06-26 19:18:58,055 main.py:47] epoch 1254, training loss: 2009.49, average training loss: 2254.70, base loss: 2380.13
[INFO 2017-06-26 19:18:58,340 main.py:47] epoch 1255, training loss: 2237.33, average training loss: 2254.71, base loss: 2380.26
[INFO 2017-06-26 19:18:58,627 main.py:47] epoch 1256, training loss: 2405.81, average training loss: 2254.81, base loss: 2380.44
[INFO 2017-06-26 19:18:58,914 main.py:47] epoch 1257, training loss: 2057.98, average training loss: 2254.65, base loss: 2380.51
[INFO 2017-06-26 19:18:59,201 main.py:47] epoch 1258, training loss: 2231.47, average training loss: 2254.73, base loss: 2380.77
[INFO 2017-06-26 19:18:59,490 main.py:47] epoch 1259, training loss: 2632.28, average training loss: 2255.23, base loss: 2381.42
[INFO 2017-06-26 19:18:59,777 main.py:47] epoch 1260, training loss: 2107.51, average training loss: 2255.07, base loss: 2381.40
[INFO 2017-06-26 19:19:00,059 main.py:47] epoch 1261, training loss: 1799.01, average training loss: 2254.17, base loss: 2380.56
[INFO 2017-06-26 19:19:00,342 main.py:47] epoch 1262, training loss: 1829.94, average training loss: 2253.50, base loss: 2380.03
[INFO 2017-06-26 19:19:00,629 main.py:47] epoch 1263, training loss: 2085.34, average training loss: 2253.10, base loss: 2379.68
[INFO 2017-06-26 19:19:00,916 main.py:47] epoch 1264, training loss: 2587.23, average training loss: 2253.41, base loss: 2380.05
[INFO 2017-06-26 19:19:01,201 main.py:47] epoch 1265, training loss: 2645.01, average training loss: 2253.62, base loss: 2380.34
[INFO 2017-06-26 19:19:01,490 main.py:47] epoch 1266, training loss: 2349.48, average training loss: 2254.03, base loss: 2380.84
[INFO 2017-06-26 19:19:01,777 main.py:47] epoch 1267, training loss: 2136.30, average training loss: 2254.08, base loss: 2380.95
[INFO 2017-06-26 19:19:02,068 main.py:47] epoch 1268, training loss: 2162.73, average training loss: 2253.79, base loss: 2380.79
[INFO 2017-06-26 19:19:02,356 main.py:47] epoch 1269, training loss: 2217.26, average training loss: 2254.39, base loss: 2381.58
[INFO 2017-06-26 19:19:02,637 main.py:47] epoch 1270, training loss: 2310.29, average training loss: 2254.12, base loss: 2381.50
[INFO 2017-06-26 19:19:02,921 main.py:47] epoch 1271, training loss: 1951.78, average training loss: 2253.85, base loss: 2381.36
[INFO 2017-06-26 19:19:03,205 main.py:47] epoch 1272, training loss: 2174.06, average training loss: 2253.57, base loss: 2381.11
[INFO 2017-06-26 19:19:03,490 main.py:47] epoch 1273, training loss: 2276.97, average training loss: 2253.67, base loss: 2381.23
[INFO 2017-06-26 19:19:03,779 main.py:47] epoch 1274, training loss: 1984.20, average training loss: 2253.75, base loss: 2381.39
[INFO 2017-06-26 19:19:04,064 main.py:47] epoch 1275, training loss: 2404.61, average training loss: 2253.79, base loss: 2381.46
[INFO 2017-06-26 19:19:04,353 main.py:47] epoch 1276, training loss: 2438.50, average training loss: 2253.73, base loss: 2381.53
[INFO 2017-06-26 19:19:04,638 main.py:47] epoch 1277, training loss: 2391.03, average training loss: 2253.94, base loss: 2381.79
[INFO 2017-06-26 19:19:04,924 main.py:47] epoch 1278, training loss: 2283.76, average training loss: 2253.72, base loss: 2381.67
[INFO 2017-06-26 19:19:05,205 main.py:47] epoch 1279, training loss: 2469.59, average training loss: 2253.26, base loss: 2381.33
[INFO 2017-06-26 19:19:05,492 main.py:47] epoch 1280, training loss: 2021.89, average training loss: 2252.95, base loss: 2381.15
[INFO 2017-06-26 19:19:05,780 main.py:47] epoch 1281, training loss: 1895.91, average training loss: 2252.52, base loss: 2380.79
[INFO 2017-06-26 19:19:06,065 main.py:47] epoch 1282, training loss: 1989.02, average training loss: 2251.92, base loss: 2380.30
[INFO 2017-06-26 19:19:06,354 main.py:47] epoch 1283, training loss: 2069.83, average training loss: 2251.80, base loss: 2380.28
[INFO 2017-06-26 19:19:06,638 main.py:47] epoch 1284, training loss: 2443.03, average training loss: 2252.09, base loss: 2380.74
[INFO 2017-06-26 19:19:06,927 main.py:47] epoch 1285, training loss: 1920.55, average training loss: 2251.70, base loss: 2380.33
[INFO 2017-06-26 19:19:07,214 main.py:47] epoch 1286, training loss: 1809.41, average training loss: 2251.06, base loss: 2379.69
[INFO 2017-06-26 19:19:07,497 main.py:47] epoch 1287, training loss: 1784.31, average training loss: 2250.92, base loss: 2379.71
[INFO 2017-06-26 19:19:07,783 main.py:47] epoch 1288, training loss: 1960.78, average training loss: 2250.63, base loss: 2379.41
[INFO 2017-06-26 19:19:08,071 main.py:47] epoch 1289, training loss: 2094.05, average training loss: 2250.11, base loss: 2378.91
[INFO 2017-06-26 19:19:08,359 main.py:47] epoch 1290, training loss: 2111.63, average training loss: 2250.10, base loss: 2379.00
[INFO 2017-06-26 19:19:08,648 main.py:47] epoch 1291, training loss: 2700.81, average training loss: 2250.61, base loss: 2379.65
[INFO 2017-06-26 19:19:08,936 main.py:47] epoch 1292, training loss: 2437.81, average training loss: 2250.80, base loss: 2379.88
[INFO 2017-06-26 19:19:09,220 main.py:47] epoch 1293, training loss: 2218.19, average training loss: 2251.02, base loss: 2380.31
[INFO 2017-06-26 19:19:09,503 main.py:47] epoch 1294, training loss: 1992.25, average training loss: 2250.75, base loss: 2380.09
[INFO 2017-06-26 19:19:09,791 main.py:47] epoch 1295, training loss: 1984.88, average training loss: 2250.44, base loss: 2379.73
[INFO 2017-06-26 19:19:10,075 main.py:47] epoch 1296, training loss: 2555.72, average training loss: 2251.30, base loss: 2380.69
[INFO 2017-06-26 19:19:10,366 main.py:47] epoch 1297, training loss: 2098.33, average training loss: 2250.81, base loss: 2380.23
[INFO 2017-06-26 19:19:10,651 main.py:47] epoch 1298, training loss: 2309.88, average training loss: 2250.81, base loss: 2380.33
[INFO 2017-06-26 19:19:10,936 main.py:47] epoch 1299, training loss: 2100.37, average training loss: 2250.61, base loss: 2380.26
[INFO 2017-06-26 19:19:10,936 main.py:49] epoch 1299, testing
[INFO 2017-06-26 19:19:14,658 main.py:100] average testing loss: 2180.54, base loss: 2367.85
[INFO 2017-06-26 19:19:14,683 main.py:73] current best accuracy: 2165.32
[INFO 2017-06-26 19:19:14,972 main.py:47] epoch 1300, training loss: 2424.70, average training loss: 2250.81, base loss: 2380.54
[INFO 2017-06-26 19:19:15,252 main.py:47] epoch 1301, training loss: 2051.85, average training loss: 2250.59, base loss: 2380.38
[INFO 2017-06-26 19:19:15,538 main.py:47] epoch 1302, training loss: 2461.51, average training loss: 2250.69, base loss: 2380.59
[INFO 2017-06-26 19:19:15,827 main.py:47] epoch 1303, training loss: 2522.98, average training loss: 2250.78, base loss: 2380.84
[INFO 2017-06-26 19:19:16,108 main.py:47] epoch 1304, training loss: 2013.46, average training loss: 2250.78, base loss: 2380.95
[INFO 2017-06-26 19:19:16,398 main.py:47] epoch 1305, training loss: 1988.42, average training loss: 2250.03, base loss: 2380.30
[INFO 2017-06-26 19:19:16,686 main.py:47] epoch 1306, training loss: 2271.07, average training loss: 2249.56, base loss: 2380.00
[INFO 2017-06-26 19:19:16,973 main.py:47] epoch 1307, training loss: 2636.48, average training loss: 2250.11, base loss: 2380.72
[INFO 2017-06-26 19:19:17,257 main.py:47] epoch 1308, training loss: 1959.41, average training loss: 2249.43, base loss: 2380.04
[INFO 2017-06-26 19:19:17,541 main.py:47] epoch 1309, training loss: 2326.97, average training loss: 2249.30, base loss: 2379.88
[INFO 2017-06-26 19:19:17,822 main.py:47] epoch 1310, training loss: 2123.00, average training loss: 2248.85, base loss: 2379.48
[INFO 2017-06-26 19:19:18,106 main.py:47] epoch 1311, training loss: 1966.51, average training loss: 2248.69, base loss: 2379.50
[INFO 2017-06-26 19:19:18,392 main.py:47] epoch 1312, training loss: 2843.01, average training loss: 2248.84, base loss: 2379.80
[INFO 2017-06-26 19:19:18,679 main.py:47] epoch 1313, training loss: 2297.11, average training loss: 2248.85, base loss: 2379.86
[INFO 2017-06-26 19:19:18,961 main.py:47] epoch 1314, training loss: 2428.74, average training loss: 2248.71, base loss: 2380.00
[INFO 2017-06-26 19:19:19,242 main.py:47] epoch 1315, training loss: 2580.33, average training loss: 2248.86, base loss: 2380.37
[INFO 2017-06-26 19:19:19,529 main.py:47] epoch 1316, training loss: 2350.80, average training loss: 2249.10, base loss: 2380.57
[INFO 2017-06-26 19:19:19,815 main.py:47] epoch 1317, training loss: 2587.59, average training loss: 2249.58, base loss: 2381.27
[INFO 2017-06-26 19:19:20,096 main.py:47] epoch 1318, training loss: 1916.49, average training loss: 2249.25, base loss: 2381.07
[INFO 2017-06-26 19:19:20,385 main.py:47] epoch 1319, training loss: 2064.63, average training loss: 2248.94, base loss: 2380.93
[INFO 2017-06-26 19:19:20,675 main.py:47] epoch 1320, training loss: 2295.19, average training loss: 2248.72, base loss: 2380.90
[INFO 2017-06-26 19:19:20,963 main.py:47] epoch 1321, training loss: 2187.65, average training loss: 2248.61, base loss: 2380.87
[INFO 2017-06-26 19:19:21,245 main.py:47] epoch 1322, training loss: 2365.35, average training loss: 2248.75, base loss: 2381.14
[INFO 2017-06-26 19:19:21,535 main.py:47] epoch 1323, training loss: 2186.10, average training loss: 2248.01, base loss: 2380.69
[INFO 2017-06-26 19:19:21,820 main.py:47] epoch 1324, training loss: 2052.51, average training loss: 2247.97, base loss: 2380.72
[INFO 2017-06-26 19:19:22,106 main.py:47] epoch 1325, training loss: 2497.30, average training loss: 2247.91, base loss: 2380.93
[INFO 2017-06-26 19:19:22,396 main.py:47] epoch 1326, training loss: 2002.35, average training loss: 2247.70, base loss: 2380.84
[INFO 2017-06-26 19:19:22,686 main.py:47] epoch 1327, training loss: 2261.68, average training loss: 2247.99, base loss: 2381.29
[INFO 2017-06-26 19:19:22,976 main.py:47] epoch 1328, training loss: 1999.04, average training loss: 2247.81, base loss: 2381.21
[INFO 2017-06-26 19:19:23,262 main.py:47] epoch 1329, training loss: 2296.92, average training loss: 2248.00, base loss: 2381.53
[INFO 2017-06-26 19:19:23,547 main.py:47] epoch 1330, training loss: 2241.05, average training loss: 2247.89, base loss: 2381.51
[INFO 2017-06-26 19:19:23,831 main.py:47] epoch 1331, training loss: 2574.90, average training loss: 2248.72, base loss: 2382.62
[INFO 2017-06-26 19:19:24,115 main.py:47] epoch 1332, training loss: 1896.65, average training loss: 2248.41, base loss: 2382.36
[INFO 2017-06-26 19:19:24,398 main.py:47] epoch 1333, training loss: 1739.70, average training loss: 2248.13, base loss: 2382.10
[INFO 2017-06-26 19:19:24,686 main.py:47] epoch 1334, training loss: 2115.86, average training loss: 2248.08, base loss: 2382.19
[INFO 2017-06-26 19:19:24,970 main.py:47] epoch 1335, training loss: 2575.49, average training loss: 2248.29, base loss: 2382.60
[INFO 2017-06-26 19:19:25,255 main.py:47] epoch 1336, training loss: 2102.56, average training loss: 2247.71, base loss: 2382.04
[INFO 2017-06-26 19:19:25,543 main.py:47] epoch 1337, training loss: 2007.29, average training loss: 2247.72, base loss: 2382.15
[INFO 2017-06-26 19:19:25,830 main.py:47] epoch 1338, training loss: 2425.19, average training loss: 2247.17, base loss: 2381.53
[INFO 2017-06-26 19:19:26,116 main.py:47] epoch 1339, training loss: 2089.16, average training loss: 2247.34, base loss: 2381.90
[INFO 2017-06-26 19:19:26,398 main.py:47] epoch 1340, training loss: 2252.55, average training loss: 2247.31, base loss: 2382.09
[INFO 2017-06-26 19:19:26,681 main.py:47] epoch 1341, training loss: 2063.92, average training loss: 2246.88, base loss: 2381.76
[INFO 2017-06-26 19:19:27,039 main.py:47] epoch 1342, training loss: 2336.25, average training loss: 2246.93, base loss: 2382.00
[INFO 2017-06-26 19:19:27,336 main.py:47] epoch 1343, training loss: 1991.60, average training loss: 2246.79, base loss: 2382.10
[INFO 2017-06-26 19:19:27,621 main.py:47] epoch 1344, training loss: 2278.85, average training loss: 2246.58, base loss: 2381.93
[INFO 2017-06-26 19:19:27,910 main.py:47] epoch 1345, training loss: 1861.30, average training loss: 2246.55, base loss: 2382.03
[INFO 2017-06-26 19:19:28,194 main.py:47] epoch 1346, training loss: 2263.44, average training loss: 2246.54, base loss: 2382.11
[INFO 2017-06-26 19:19:28,482 main.py:47] epoch 1347, training loss: 2378.45, average training loss: 2246.93, base loss: 2382.64
[INFO 2017-06-26 19:19:28,769 main.py:47] epoch 1348, training loss: 2385.75, average training loss: 2247.13, base loss: 2382.98
[INFO 2017-06-26 19:19:29,054 main.py:47] epoch 1349, training loss: 2335.39, average training loss: 2247.18, base loss: 2383.33
[INFO 2017-06-26 19:19:29,339 main.py:47] epoch 1350, training loss: 2167.91, average training loss: 2247.24, base loss: 2383.62
[INFO 2017-06-26 19:19:29,623 main.py:47] epoch 1351, training loss: 2163.66, average training loss: 2246.86, base loss: 2383.46
[INFO 2017-06-26 19:19:29,908 main.py:47] epoch 1352, training loss: 2085.47, average training loss: 2246.89, base loss: 2383.59
[INFO 2017-06-26 19:19:30,193 main.py:47] epoch 1353, training loss: 1909.30, average training loss: 2246.48, base loss: 2383.29
[INFO 2017-06-26 19:19:30,476 main.py:47] epoch 1354, training loss: 2062.43, average training loss: 2246.03, base loss: 2382.80
[INFO 2017-06-26 19:19:30,769 main.py:47] epoch 1355, training loss: 2172.52, average training loss: 2246.09, base loss: 2382.78
[INFO 2017-06-26 19:19:31,057 main.py:47] epoch 1356, training loss: 1703.36, average training loss: 2245.73, base loss: 2382.44
[INFO 2017-06-26 19:19:31,344 main.py:47] epoch 1357, training loss: 2674.46, average training loss: 2245.94, base loss: 2382.87
[INFO 2017-06-26 19:19:31,626 main.py:47] epoch 1358, training loss: 2158.55, average training loss: 2245.69, base loss: 2382.76
[INFO 2017-06-26 19:19:31,913 main.py:47] epoch 1359, training loss: 2507.57, average training loss: 2245.83, base loss: 2383.08
[INFO 2017-06-26 19:19:32,205 main.py:47] epoch 1360, training loss: 2281.21, average training loss: 2245.28, base loss: 2382.57
[INFO 2017-06-26 19:19:32,499 main.py:47] epoch 1361, training loss: 2252.56, average training loss: 2245.56, base loss: 2382.92
[INFO 2017-06-26 19:19:32,784 main.py:47] epoch 1362, training loss: 1959.90, average training loss: 2245.43, base loss: 2382.89
[INFO 2017-06-26 19:19:33,073 main.py:47] epoch 1363, training loss: 2273.17, average training loss: 2245.47, base loss: 2383.01
[INFO 2017-06-26 19:19:33,363 main.py:47] epoch 1364, training loss: 1963.43, average training loss: 2244.81, base loss: 2382.53
[INFO 2017-06-26 19:19:33,645 main.py:47] epoch 1365, training loss: 2375.27, average training loss: 2245.26, base loss: 2383.13
[INFO 2017-06-26 19:19:33,934 main.py:47] epoch 1366, training loss: 2169.26, average training loss: 2245.37, base loss: 2383.25
[INFO 2017-06-26 19:19:34,222 main.py:47] epoch 1367, training loss: 2495.41, average training loss: 2245.57, base loss: 2383.55
[INFO 2017-06-26 19:19:34,511 main.py:47] epoch 1368, training loss: 2087.70, average training loss: 2245.46, base loss: 2383.64
[INFO 2017-06-26 19:19:34,798 main.py:47] epoch 1369, training loss: 1916.57, average training loss: 2244.98, base loss: 2383.14
[INFO 2017-06-26 19:19:35,083 main.py:47] epoch 1370, training loss: 2456.14, average training loss: 2245.22, base loss: 2383.38
[INFO 2017-06-26 19:19:35,367 main.py:47] epoch 1371, training loss: 2857.86, average training loss: 2245.98, base loss: 2384.35
[INFO 2017-06-26 19:19:35,649 main.py:47] epoch 1372, training loss: 2439.30, average training loss: 2245.92, base loss: 2384.37
[INFO 2017-06-26 19:19:35,936 main.py:47] epoch 1373, training loss: 2209.40, average training loss: 2245.61, base loss: 2384.10
[INFO 2017-06-26 19:19:36,224 main.py:47] epoch 1374, training loss: 2031.60, average training loss: 2245.01, base loss: 2383.59
[INFO 2017-06-26 19:19:36,512 main.py:47] epoch 1375, training loss: 2135.35, average training loss: 2244.73, base loss: 2383.43
[INFO 2017-06-26 19:19:36,796 main.py:47] epoch 1376, training loss: 1870.27, average training loss: 2244.38, base loss: 2383.21
[INFO 2017-06-26 19:19:37,080 main.py:47] epoch 1377, training loss: 1916.08, average training loss: 2243.76, base loss: 2382.64
[INFO 2017-06-26 19:19:37,368 main.py:47] epoch 1378, training loss: 1832.31, average training loss: 2243.08, base loss: 2382.14
[INFO 2017-06-26 19:19:37,652 main.py:47] epoch 1379, training loss: 2133.84, average training loss: 2243.15, base loss: 2382.34
[INFO 2017-06-26 19:19:37,938 main.py:47] epoch 1380, training loss: 2519.69, average training loss: 2243.75, base loss: 2383.09
[INFO 2017-06-26 19:19:38,225 main.py:47] epoch 1381, training loss: 2412.33, average training loss: 2243.56, base loss: 2383.02
[INFO 2017-06-26 19:19:38,510 main.py:47] epoch 1382, training loss: 1913.52, average training loss: 2243.21, base loss: 2382.74
[INFO 2017-06-26 19:19:38,797 main.py:47] epoch 1383, training loss: 1890.67, average training loss: 2243.36, base loss: 2383.06
[INFO 2017-06-26 19:19:39,081 main.py:47] epoch 1384, training loss: 2403.20, average training loss: 2243.60, base loss: 2383.54
[INFO 2017-06-26 19:19:39,365 main.py:47] epoch 1385, training loss: 2591.97, average training loss: 2243.97, base loss: 2384.08
[INFO 2017-06-26 19:19:39,653 main.py:47] epoch 1386, training loss: 2151.69, average training loss: 2243.96, base loss: 2384.39
[INFO 2017-06-26 19:19:39,936 main.py:47] epoch 1387, training loss: 2110.50, average training loss: 2243.79, base loss: 2384.31
[INFO 2017-06-26 19:19:40,224 main.py:47] epoch 1388, training loss: 2517.02, average training loss: 2243.98, base loss: 2384.60
[INFO 2017-06-26 19:19:40,510 main.py:47] epoch 1389, training loss: 2021.41, average training loss: 2243.84, base loss: 2384.52
[INFO 2017-06-26 19:19:40,795 main.py:47] epoch 1390, training loss: 2042.15, average training loss: 2243.47, base loss: 2384.25
[INFO 2017-06-26 19:19:41,080 main.py:47] epoch 1391, training loss: 1933.89, average training loss: 2242.82, base loss: 2383.59
[INFO 2017-06-26 19:19:41,363 main.py:47] epoch 1392, training loss: 1899.39, average training loss: 2242.63, base loss: 2383.48
[INFO 2017-06-26 19:19:41,651 main.py:47] epoch 1393, training loss: 2393.13, average training loss: 2242.83, base loss: 2383.87
[INFO 2017-06-26 19:19:41,941 main.py:47] epoch 1394, training loss: 2216.10, average training loss: 2242.53, base loss: 2383.68
[INFO 2017-06-26 19:19:42,229 main.py:47] epoch 1395, training loss: 1865.19, average training loss: 2241.94, base loss: 2382.94
[INFO 2017-06-26 19:19:42,511 main.py:47] epoch 1396, training loss: 2085.71, average training loss: 2242.05, base loss: 2383.15
[INFO 2017-06-26 19:19:42,796 main.py:47] epoch 1397, training loss: 2168.13, average training loss: 2242.14, base loss: 2383.38
[INFO 2017-06-26 19:19:43,079 main.py:47] epoch 1398, training loss: 1792.66, average training loss: 2241.72, base loss: 2383.15
[INFO 2017-06-26 19:19:43,362 main.py:47] epoch 1399, training loss: 2821.49, average training loss: 2242.23, base loss: 2383.91
[INFO 2017-06-26 19:19:43,362 main.py:49] epoch 1399, testing
[INFO 2017-06-26 19:19:47,148 main.py:100] average testing loss: 2152.99, base loss: 2350.42
[INFO 2017-06-26 19:19:47,173 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:19:47,186 main.py:73] current best accuracy: 2152.99
[INFO 2017-06-26 19:19:47,472 main.py:47] epoch 1400, training loss: 1751.91, average training loss: 2241.70, base loss: 2383.41
[INFO 2017-06-26 19:19:47,753 main.py:47] epoch 1401, training loss: 2242.30, average training loss: 2241.68, base loss: 2383.45
[INFO 2017-06-26 19:19:48,034 main.py:47] epoch 1402, training loss: 2658.44, average training loss: 2242.21, base loss: 2384.18
[INFO 2017-06-26 19:19:48,316 main.py:47] epoch 1403, training loss: 2364.59, average training loss: 2242.39, base loss: 2384.57
[INFO 2017-06-26 19:19:48,603 main.py:47] epoch 1404, training loss: 1780.71, average training loss: 2242.03, base loss: 2384.19
[INFO 2017-06-26 19:19:48,889 main.py:47] epoch 1405, training loss: 2089.14, average training loss: 2241.86, base loss: 2384.14
[INFO 2017-06-26 19:19:49,170 main.py:47] epoch 1406, training loss: 2314.28, average training loss: 2241.61, base loss: 2384.00
[INFO 2017-06-26 19:19:49,458 main.py:47] epoch 1407, training loss: 1895.03, average training loss: 2241.32, base loss: 2383.70
[INFO 2017-06-26 19:19:49,746 main.py:47] epoch 1408, training loss: 1798.71, average training loss: 2240.99, base loss: 2383.41
[INFO 2017-06-26 19:19:50,030 main.py:47] epoch 1409, training loss: 2251.21, average training loss: 2240.86, base loss: 2383.39
[INFO 2017-06-26 19:19:50,314 main.py:47] epoch 1410, training loss: 2036.65, average training loss: 2240.77, base loss: 2383.44
[INFO 2017-06-26 19:19:50,597 main.py:47] epoch 1411, training loss: 2374.31, average training loss: 2241.06, base loss: 2383.79
[INFO 2017-06-26 19:19:50,885 main.py:47] epoch 1412, training loss: 1983.42, average training loss: 2240.51, base loss: 2383.27
[INFO 2017-06-26 19:19:51,170 main.py:47] epoch 1413, training loss: 2482.75, average training loss: 2240.51, base loss: 2383.33
[INFO 2017-06-26 19:19:51,454 main.py:47] epoch 1414, training loss: 2003.27, average training loss: 2240.15, base loss: 2382.95
[INFO 2017-06-26 19:19:51,736 main.py:47] epoch 1415, training loss: 2169.26, average training loss: 2239.96, base loss: 2382.96
[INFO 2017-06-26 19:19:52,026 main.py:47] epoch 1416, training loss: 2394.81, average training loss: 2240.34, base loss: 2383.60
[INFO 2017-06-26 19:19:52,307 main.py:47] epoch 1417, training loss: 2142.79, average training loss: 2240.50, base loss: 2383.84
[INFO 2017-06-26 19:19:52,592 main.py:47] epoch 1418, training loss: 2245.26, average training loss: 2240.62, base loss: 2384.18
[INFO 2017-06-26 19:19:52,880 main.py:47] epoch 1419, training loss: 2171.30, average training loss: 2240.98, base loss: 2384.77
[INFO 2017-06-26 19:19:53,164 main.py:47] epoch 1420, training loss: 2306.76, average training loss: 2241.35, base loss: 2385.40
[INFO 2017-06-26 19:19:53,449 main.py:47] epoch 1421, training loss: 2484.23, average training loss: 2241.23, base loss: 2385.54
[INFO 2017-06-26 19:19:53,733 main.py:47] epoch 1422, training loss: 1973.75, average training loss: 2241.04, base loss: 2385.38
[INFO 2017-06-26 19:19:54,018 main.py:47] epoch 1423, training loss: 1923.13, average training loss: 2240.63, base loss: 2384.90
[INFO 2017-06-26 19:19:54,306 main.py:47] epoch 1424, training loss: 2101.30, average training loss: 2240.34, base loss: 2384.60
[INFO 2017-06-26 19:19:54,589 main.py:47] epoch 1425, training loss: 1948.35, average training loss: 2239.52, base loss: 2383.86
[INFO 2017-06-26 19:19:54,873 main.py:47] epoch 1426, training loss: 2018.05, average training loss: 2239.41, base loss: 2383.83
[INFO 2017-06-26 19:19:55,155 main.py:47] epoch 1427, training loss: 1854.16, average training loss: 2239.05, base loss: 2383.44
[INFO 2017-06-26 19:19:55,443 main.py:47] epoch 1428, training loss: 2279.35, average training loss: 2239.13, base loss: 2383.73
[INFO 2017-06-26 19:19:55,724 main.py:47] epoch 1429, training loss: 2240.02, average training loss: 2239.05, base loss: 2383.63
[INFO 2017-06-26 19:19:56,011 main.py:47] epoch 1430, training loss: 2186.50, average training loss: 2239.04, base loss: 2383.63
[INFO 2017-06-26 19:19:56,298 main.py:47] epoch 1431, training loss: 2122.79, average training loss: 2238.92, base loss: 2383.52
[INFO 2017-06-26 19:19:56,584 main.py:47] epoch 1432, training loss: 2531.30, average training loss: 2239.15, base loss: 2383.89
[INFO 2017-06-26 19:19:56,870 main.py:47] epoch 1433, training loss: 2245.73, average training loss: 2239.41, base loss: 2384.35
[INFO 2017-06-26 19:19:57,155 main.py:47] epoch 1434, training loss: 1951.16, average training loss: 2239.40, base loss: 2384.58
[INFO 2017-06-26 19:19:57,436 main.py:47] epoch 1435, training loss: 2019.71, average training loss: 2239.30, base loss: 2384.61
[INFO 2017-06-26 19:19:57,720 main.py:47] epoch 1436, training loss: 2529.00, average training loss: 2239.34, base loss: 2384.63
[INFO 2017-06-26 19:19:58,008 main.py:47] epoch 1437, training loss: 2182.24, average training loss: 2239.24, base loss: 2384.49
[INFO 2017-06-26 19:19:58,292 main.py:47] epoch 1438, training loss: 2153.55, average training loss: 2238.82, base loss: 2384.10
[INFO 2017-06-26 19:19:58,577 main.py:47] epoch 1439, training loss: 2255.55, average training loss: 2238.09, base loss: 2383.34
[INFO 2017-06-26 19:19:58,865 main.py:47] epoch 1440, training loss: 1800.78, average training loss: 2237.36, base loss: 2382.54
[INFO 2017-06-26 19:19:59,149 main.py:47] epoch 1441, training loss: 2052.97, average training loss: 2237.29, base loss: 2382.56
[INFO 2017-06-26 19:19:59,434 main.py:47] epoch 1442, training loss: 1941.63, average training loss: 2236.73, base loss: 2382.03
[INFO 2017-06-26 19:19:59,722 main.py:47] epoch 1443, training loss: 2079.42, average training loss: 2236.58, base loss: 2381.84
[INFO 2017-06-26 19:20:00,010 main.py:47] epoch 1444, training loss: 2130.36, average training loss: 2236.63, base loss: 2381.96
[INFO 2017-06-26 19:20:00,294 main.py:47] epoch 1445, training loss: 2315.61, average training loss: 2236.67, base loss: 2382.10
[INFO 2017-06-26 19:20:00,579 main.py:47] epoch 1446, training loss: 2425.65, average training loss: 2236.89, base loss: 2382.44
[INFO 2017-06-26 19:20:00,867 main.py:47] epoch 1447, training loss: 2492.86, average training loss: 2237.02, base loss: 2382.59
[INFO 2017-06-26 19:20:01,153 main.py:47] epoch 1448, training loss: 1984.83, average training loss: 2236.79, base loss: 2382.30
[INFO 2017-06-26 19:20:01,437 main.py:47] epoch 1449, training loss: 2609.87, average training loss: 2237.13, base loss: 2382.88
[INFO 2017-06-26 19:20:01,725 main.py:47] epoch 1450, training loss: 2316.07, average training loss: 2237.19, base loss: 2382.94
[INFO 2017-06-26 19:20:02,007 main.py:47] epoch 1451, training loss: 2190.88, average training loss: 2237.30, base loss: 2383.11
[INFO 2017-06-26 19:20:02,288 main.py:47] epoch 1452, training loss: 2118.30, average training loss: 2237.12, base loss: 2383.03
[INFO 2017-06-26 19:20:02,572 main.py:47] epoch 1453, training loss: 2181.76, average training loss: 2236.98, base loss: 2382.89
[INFO 2017-06-26 19:20:02,854 main.py:47] epoch 1454, training loss: 2290.12, average training loss: 2237.39, base loss: 2383.43
[INFO 2017-06-26 19:20:03,140 main.py:47] epoch 1455, training loss: 2182.00, average training loss: 2236.87, base loss: 2382.94
[INFO 2017-06-26 19:20:03,427 main.py:47] epoch 1456, training loss: 2188.41, average training loss: 2237.04, base loss: 2383.28
[INFO 2017-06-26 19:20:03,712 main.py:47] epoch 1457, training loss: 2402.42, average training loss: 2236.56, base loss: 2382.76
[INFO 2017-06-26 19:20:04,000 main.py:47] epoch 1458, training loss: 2090.45, average training loss: 2236.36, base loss: 2382.69
[INFO 2017-06-26 19:20:04,285 main.py:47] epoch 1459, training loss: 2996.75, average training loss: 2236.99, base loss: 2383.50
[INFO 2017-06-26 19:20:04,573 main.py:47] epoch 1460, training loss: 2110.13, average training loss: 2236.99, base loss: 2383.58
[INFO 2017-06-26 19:20:04,861 main.py:47] epoch 1461, training loss: 2107.38, average training loss: 2237.08, base loss: 2383.65
[INFO 2017-06-26 19:20:05,143 main.py:47] epoch 1462, training loss: 2219.02, average training loss: 2236.67, base loss: 2383.31
[INFO 2017-06-26 19:20:05,431 main.py:47] epoch 1463, training loss: 2401.32, average training loss: 2236.84, base loss: 2383.79
[INFO 2017-06-26 19:20:05,715 main.py:47] epoch 1464, training loss: 2437.30, average training loss: 2237.20, base loss: 2384.21
[INFO 2017-06-26 19:20:05,999 main.py:47] epoch 1465, training loss: 2040.69, average training loss: 2236.59, base loss: 2383.56
[INFO 2017-06-26 19:20:06,284 main.py:47] epoch 1466, training loss: 2002.53, average training loss: 2236.44, base loss: 2383.60
[INFO 2017-06-26 19:20:06,569 main.py:47] epoch 1467, training loss: 2617.21, average training loss: 2236.82, base loss: 2384.09
[INFO 2017-06-26 19:20:06,858 main.py:47] epoch 1468, training loss: 2088.57, average training loss: 2236.01, base loss: 2383.33
[INFO 2017-06-26 19:20:07,142 main.py:47] epoch 1469, training loss: 2033.68, average training loss: 2236.12, base loss: 2383.52
[INFO 2017-06-26 19:20:07,426 main.py:47] epoch 1470, training loss: 2288.07, average training loss: 2236.31, base loss: 2383.86
[INFO 2017-06-26 19:20:07,707 main.py:47] epoch 1471, training loss: 2040.02, average training loss: 2236.40, base loss: 2384.05
[INFO 2017-06-26 19:20:07,989 main.py:47] epoch 1472, training loss: 2086.46, average training loss: 2236.09, base loss: 2383.88
[INFO 2017-06-26 19:20:08,273 main.py:47] epoch 1473, training loss: 1893.36, average training loss: 2235.82, base loss: 2383.74
[INFO 2017-06-26 19:20:08,561 main.py:47] epoch 1474, training loss: 2067.17, average training loss: 2235.85, base loss: 2383.74
[INFO 2017-06-26 19:20:08,845 main.py:47] epoch 1475, training loss: 1934.43, average training loss: 2235.64, base loss: 2383.62
[INFO 2017-06-26 19:20:09,127 main.py:47] epoch 1476, training loss: 1894.67, average training loss: 2235.48, base loss: 2383.48
[INFO 2017-06-26 19:20:09,411 main.py:47] epoch 1477, training loss: 1918.61, average training loss: 2235.30, base loss: 2383.47
[INFO 2017-06-26 19:20:09,700 main.py:47] epoch 1478, training loss: 2099.70, average training loss: 2235.40, base loss: 2383.73
[INFO 2017-06-26 19:20:09,987 main.py:47] epoch 1479, training loss: 2109.97, average training loss: 2235.29, base loss: 2383.73
[INFO 2017-06-26 19:20:10,276 main.py:47] epoch 1480, training loss: 2312.81, average training loss: 2235.08, base loss: 2383.66
[INFO 2017-06-26 19:20:10,563 main.py:47] epoch 1481, training loss: 2122.35, average training loss: 2235.00, base loss: 2383.71
[INFO 2017-06-26 19:20:10,844 main.py:47] epoch 1482, training loss: 2446.78, average training loss: 2235.07, base loss: 2384.07
[INFO 2017-06-26 19:20:11,133 main.py:47] epoch 1483, training loss: 2544.29, average training loss: 2235.25, base loss: 2384.36
[INFO 2017-06-26 19:20:11,417 main.py:47] epoch 1484, training loss: 2082.15, average training loss: 2235.43, base loss: 2384.80
[INFO 2017-06-26 19:20:11,702 main.py:47] epoch 1485, training loss: 1985.37, average training loss: 2235.41, base loss: 2384.82
[INFO 2017-06-26 19:20:12,004 main.py:47] epoch 1486, training loss: 1948.52, average training loss: 2235.10, base loss: 2384.66
[INFO 2017-06-26 19:20:12,289 main.py:47] epoch 1487, training loss: 2448.83, average training loss: 2235.17, base loss: 2385.00
[INFO 2017-06-26 19:20:12,574 main.py:47] epoch 1488, training loss: 2352.84, average training loss: 2235.57, base loss: 2385.35
[INFO 2017-06-26 19:20:12,862 main.py:47] epoch 1489, training loss: 2473.51, average training loss: 2235.25, base loss: 2385.07
[INFO 2017-06-26 19:20:13,146 main.py:47] epoch 1490, training loss: 1839.08, average training loss: 2234.83, base loss: 2384.75
[INFO 2017-06-26 19:20:13,433 main.py:47] epoch 1491, training loss: 1749.08, average training loss: 2233.99, base loss: 2384.03
[INFO 2017-06-26 19:20:13,718 main.py:47] epoch 1492, training loss: 2222.23, average training loss: 2233.70, base loss: 2384.00
[INFO 2017-06-26 19:20:14,006 main.py:47] epoch 1493, training loss: 1994.86, average training loss: 2233.59, base loss: 2383.82
[INFO 2017-06-26 19:20:14,290 main.py:47] epoch 1494, training loss: 2084.58, average training loss: 2233.66, base loss: 2384.01
[INFO 2017-06-26 19:20:14,576 main.py:47] epoch 1495, training loss: 1989.62, average training loss: 2233.26, base loss: 2383.68
[INFO 2017-06-26 19:20:14,859 main.py:47] epoch 1496, training loss: 1731.85, average training loss: 2233.15, base loss: 2383.68
[INFO 2017-06-26 19:20:15,139 main.py:47] epoch 1497, training loss: 1874.33, average training loss: 2232.84, base loss: 2383.39
[INFO 2017-06-26 19:20:15,420 main.py:47] epoch 1498, training loss: 1842.19, average training loss: 2231.81, base loss: 2382.37
[INFO 2017-06-26 19:20:15,707 main.py:47] epoch 1499, training loss: 1917.54, average training loss: 2231.32, base loss: 2381.99
[INFO 2017-06-26 19:20:15,707 main.py:49] epoch 1499, testing
[INFO 2017-06-26 19:20:19,412 main.py:100] average testing loss: 2172.04, base loss: 2343.45
[INFO 2017-06-26 19:20:19,438 main.py:73] current best accuracy: 2152.99
[INFO 2017-06-26 19:20:19,720 main.py:47] epoch 1500, training loss: 2190.03, average training loss: 2231.41, base loss: 2382.24
[INFO 2017-06-26 19:20:20,007 main.py:47] epoch 1501, training loss: 1995.35, average training loss: 2231.12, base loss: 2382.02
[INFO 2017-06-26 19:20:20,292 main.py:47] epoch 1502, training loss: 2095.56, average training loss: 2230.79, base loss: 2381.76
[INFO 2017-06-26 19:20:20,581 main.py:47] epoch 1503, training loss: 2362.65, average training loss: 2230.61, base loss: 2381.55
[INFO 2017-06-26 19:20:20,869 main.py:47] epoch 1504, training loss: 2190.14, average training loss: 2230.00, base loss: 2381.00
[INFO 2017-06-26 19:20:21,157 main.py:47] epoch 1505, training loss: 2338.52, average training loss: 2230.29, base loss: 2381.38
[INFO 2017-06-26 19:20:21,444 main.py:47] epoch 1506, training loss: 2052.31, average training loss: 2230.37, base loss: 2381.69
[INFO 2017-06-26 19:20:21,724 main.py:47] epoch 1507, training loss: 2025.93, average training loss: 2230.17, base loss: 2381.49
[INFO 2017-06-26 19:20:22,006 main.py:47] epoch 1508, training loss: 1981.61, average training loss: 2230.01, base loss: 2381.43
[INFO 2017-06-26 19:20:22,291 main.py:47] epoch 1509, training loss: 2065.29, average training loss: 2229.51, base loss: 2381.02
[INFO 2017-06-26 19:20:22,575 main.py:47] epoch 1510, training loss: 2115.79, average training loss: 2229.06, base loss: 2380.62
[INFO 2017-06-26 19:20:22,863 main.py:47] epoch 1511, training loss: 2287.09, average training loss: 2228.34, base loss: 2379.99
[INFO 2017-06-26 19:20:23,147 main.py:47] epoch 1512, training loss: 2159.49, average training loss: 2227.79, base loss: 2379.43
[INFO 2017-06-26 19:20:23,432 main.py:47] epoch 1513, training loss: 2477.68, average training loss: 2228.31, base loss: 2379.96
[INFO 2017-06-26 19:20:23,713 main.py:47] epoch 1514, training loss: 2272.70, average training loss: 2228.12, base loss: 2379.67
[INFO 2017-06-26 19:20:24,001 main.py:47] epoch 1515, training loss: 2014.87, average training loss: 2227.83, base loss: 2379.44
[INFO 2017-06-26 19:20:24,285 main.py:47] epoch 1516, training loss: 2561.72, average training loss: 2228.27, base loss: 2379.97
[INFO 2017-06-26 19:20:24,572 main.py:47] epoch 1517, training loss: 1921.24, average training loss: 2227.72, base loss: 2379.41
[INFO 2017-06-26 19:20:24,859 main.py:47] epoch 1518, training loss: 2579.41, average training loss: 2227.96, base loss: 2379.80
[INFO 2017-06-26 19:20:25,146 main.py:47] epoch 1519, training loss: 2797.24, average training loss: 2228.11, base loss: 2380.04
[INFO 2017-06-26 19:20:25,433 main.py:47] epoch 1520, training loss: 2297.42, average training loss: 2228.28, base loss: 2380.36
[INFO 2017-06-26 19:20:25,717 main.py:47] epoch 1521, training loss: 2722.61, average training loss: 2228.92, base loss: 2381.29
[INFO 2017-06-26 19:20:26,002 main.py:47] epoch 1522, training loss: 2277.54, average training loss: 2228.63, base loss: 2381.21
[INFO 2017-06-26 19:20:26,287 main.py:47] epoch 1523, training loss: 2053.69, average training loss: 2228.35, base loss: 2381.02
[INFO 2017-06-26 19:20:26,574 main.py:47] epoch 1524, training loss: 2134.16, average training loss: 2228.29, base loss: 2380.91
[INFO 2017-06-26 19:20:26,860 main.py:47] epoch 1525, training loss: 1885.46, average training loss: 2228.07, base loss: 2380.66
[INFO 2017-06-26 19:20:27,144 main.py:47] epoch 1526, training loss: 1988.58, average training loss: 2228.04, base loss: 2380.70
[INFO 2017-06-26 19:20:27,432 main.py:47] epoch 1527, training loss: 1899.24, average training loss: 2227.74, base loss: 2380.45
[INFO 2017-06-26 19:20:27,719 main.py:47] epoch 1528, training loss: 2327.57, average training loss: 2227.60, base loss: 2380.43
[INFO 2017-06-26 19:20:28,003 main.py:47] epoch 1529, training loss: 2458.64, average training loss: 2227.37, base loss: 2380.15
[INFO 2017-06-26 19:20:28,292 main.py:47] epoch 1530, training loss: 2158.33, average training loss: 2227.25, base loss: 2380.16
[INFO 2017-06-26 19:20:28,580 main.py:47] epoch 1531, training loss: 1717.76, average training loss: 2226.34, base loss: 2379.19
[INFO 2017-06-26 19:20:28,865 main.py:47] epoch 1532, training loss: 2133.53, average training loss: 2226.43, base loss: 2379.33
[INFO 2017-06-26 19:20:29,151 main.py:47] epoch 1533, training loss: 2324.59, average training loss: 2225.73, base loss: 2378.62
[INFO 2017-06-26 19:20:29,433 main.py:47] epoch 1534, training loss: 2208.34, average training loss: 2225.37, base loss: 2378.21
[INFO 2017-06-26 19:20:29,722 main.py:47] epoch 1535, training loss: 1918.77, average training loss: 2224.87, base loss: 2377.81
[INFO 2017-06-26 19:20:30,010 main.py:47] epoch 1536, training loss: 2260.78, average training loss: 2224.86, base loss: 2377.78
[INFO 2017-06-26 19:20:30,297 main.py:47] epoch 1537, training loss: 2043.04, average training loss: 2224.66, base loss: 2377.63
[INFO 2017-06-26 19:20:30,582 main.py:47] epoch 1538, training loss: 2633.83, average training loss: 2224.74, base loss: 2377.79
[INFO 2017-06-26 19:20:30,867 main.py:47] epoch 1539, training loss: 1955.81, average training loss: 2224.22, base loss: 2377.19
[INFO 2017-06-26 19:20:31,150 main.py:47] epoch 1540, training loss: 2143.54, average training loss: 2224.05, base loss: 2377.15
[INFO 2017-06-26 19:20:31,431 main.py:47] epoch 1541, training loss: 2539.67, average training loss: 2223.69, base loss: 2376.66
[INFO 2017-06-26 19:20:31,714 main.py:47] epoch 1542, training loss: 2358.05, average training loss: 2223.78, base loss: 2376.83
[INFO 2017-06-26 19:20:31,996 main.py:47] epoch 1543, training loss: 1848.18, average training loss: 2223.39, base loss: 2376.48
[INFO 2017-06-26 19:20:32,281 main.py:47] epoch 1544, training loss: 2102.59, average training loss: 2223.14, base loss: 2376.44
[INFO 2017-06-26 19:20:32,567 main.py:47] epoch 1545, training loss: 2512.49, average training loss: 2223.27, base loss: 2376.68
[INFO 2017-06-26 19:20:32,851 main.py:47] epoch 1546, training loss: 2232.15, average training loss: 2223.25, base loss: 2376.80
[INFO 2017-06-26 19:20:33,135 main.py:47] epoch 1547, training loss: 2674.35, average training loss: 2223.59, base loss: 2377.50
[INFO 2017-06-26 19:20:33,424 main.py:47] epoch 1548, training loss: 1991.05, average training loss: 2223.51, base loss: 2377.49
[INFO 2017-06-26 19:20:33,706 main.py:47] epoch 1549, training loss: 2538.65, average training loss: 2224.08, base loss: 2378.20
[INFO 2017-06-26 19:20:33,988 main.py:47] epoch 1550, training loss: 2012.15, average training loss: 2223.85, base loss: 2378.07
[INFO 2017-06-26 19:20:34,269 main.py:47] epoch 1551, training loss: 2214.50, average training loss: 2223.68, base loss: 2378.11
[INFO 2017-06-26 19:20:34,551 main.py:47] epoch 1552, training loss: 1990.48, average training loss: 2223.11, base loss: 2377.69
[INFO 2017-06-26 19:20:34,832 main.py:47] epoch 1553, training loss: 2335.82, average training loss: 2223.44, base loss: 2378.34
[INFO 2017-06-26 19:20:35,115 main.py:47] epoch 1554, training loss: 1857.66, average training loss: 2223.17, base loss: 2378.10
[INFO 2017-06-26 19:20:35,399 main.py:47] epoch 1555, training loss: 2256.98, average training loss: 2222.75, base loss: 2377.77
[INFO 2017-06-26 19:20:35,680 main.py:47] epoch 1556, training loss: 2082.37, average training loss: 2223.08, base loss: 2378.19
[INFO 2017-06-26 19:20:35,966 main.py:47] epoch 1557, training loss: 2242.86, average training loss: 2222.71, base loss: 2377.89
[INFO 2017-06-26 19:20:36,254 main.py:47] epoch 1558, training loss: 2085.56, average training loss: 2222.75, base loss: 2377.97
[INFO 2017-06-26 19:20:36,538 main.py:47] epoch 1559, training loss: 2161.51, average training loss: 2222.85, base loss: 2378.24
[INFO 2017-06-26 19:20:36,824 main.py:47] epoch 1560, training loss: 2443.77, average training loss: 2223.13, base loss: 2378.67
[INFO 2017-06-26 19:20:37,112 main.py:47] epoch 1561, training loss: 2426.61, average training loss: 2223.38, base loss: 2379.10
[INFO 2017-06-26 19:20:37,395 main.py:47] epoch 1562, training loss: 2050.29, average training loss: 2223.57, base loss: 2379.48
[INFO 2017-06-26 19:20:37,680 main.py:47] epoch 1563, training loss: 2089.36, average training loss: 2223.40, base loss: 2379.34
[INFO 2017-06-26 19:20:37,965 main.py:47] epoch 1564, training loss: 2261.41, average training loss: 2223.42, base loss: 2379.33
[INFO 2017-06-26 19:20:38,249 main.py:47] epoch 1565, training loss: 1965.33, average training loss: 2223.12, base loss: 2379.09
[INFO 2017-06-26 19:20:38,536 main.py:47] epoch 1566, training loss: 2236.87, average training loss: 2223.15, base loss: 2379.34
[INFO 2017-06-26 19:20:38,823 main.py:47] epoch 1567, training loss: 2220.22, average training loss: 2223.13, base loss: 2379.44
[INFO 2017-06-26 19:20:39,110 main.py:47] epoch 1568, training loss: 2559.74, average training loss: 2222.88, base loss: 2379.28
[INFO 2017-06-26 19:20:39,391 main.py:47] epoch 1569, training loss: 2106.05, average training loss: 2222.65, base loss: 2379.24
[INFO 2017-06-26 19:20:39,679 main.py:47] epoch 1570, training loss: 2295.32, average training loss: 2222.58, base loss: 2379.27
[INFO 2017-06-26 19:20:39,966 main.py:47] epoch 1571, training loss: 2134.07, average training loss: 2222.64, base loss: 2379.22
[INFO 2017-06-26 19:20:40,252 main.py:47] epoch 1572, training loss: 2094.63, average training loss: 2222.13, base loss: 2378.53
[INFO 2017-06-26 19:20:40,537 main.py:47] epoch 1573, training loss: 1971.92, average training loss: 2222.19, base loss: 2378.65
[INFO 2017-06-26 19:20:40,822 main.py:47] epoch 1574, training loss: 2372.12, average training loss: 2222.34, base loss: 2378.78
[INFO 2017-06-26 19:20:41,110 main.py:47] epoch 1575, training loss: 2292.14, average training loss: 2222.36, base loss: 2378.95
[INFO 2017-06-26 19:20:41,398 main.py:47] epoch 1576, training loss: 1907.63, average training loss: 2222.00, base loss: 2378.61
[INFO 2017-06-26 19:20:41,683 main.py:47] epoch 1577, training loss: 1936.34, average training loss: 2221.78, base loss: 2378.48
[INFO 2017-06-26 19:20:41,970 main.py:47] epoch 1578, training loss: 2170.65, average training loss: 2221.67, base loss: 2378.40
[INFO 2017-06-26 19:20:42,257 main.py:47] epoch 1579, training loss: 2176.47, average training loss: 2221.92, base loss: 2378.81
[INFO 2017-06-26 19:20:42,542 main.py:47] epoch 1580, training loss: 1936.67, average training loss: 2221.84, base loss: 2378.84
[INFO 2017-06-26 19:20:42,829 main.py:47] epoch 1581, training loss: 2240.99, average training loss: 2221.72, base loss: 2378.90
[INFO 2017-06-26 19:20:43,113 main.py:47] epoch 1582, training loss: 2140.35, average training loss: 2221.68, base loss: 2379.01
[INFO 2017-06-26 19:20:43,397 main.py:47] epoch 1583, training loss: 2294.04, average training loss: 2221.37, base loss: 2378.75
[INFO 2017-06-26 19:20:43,685 main.py:47] epoch 1584, training loss: 1907.14, average training loss: 2220.77, base loss: 2378.15
[INFO 2017-06-26 19:20:43,974 main.py:47] epoch 1585, training loss: 2500.80, average training loss: 2220.96, base loss: 2378.55
[INFO 2017-06-26 19:20:44,259 main.py:47] epoch 1586, training loss: 2530.43, average training loss: 2221.08, base loss: 2378.84
[INFO 2017-06-26 19:20:44,541 main.py:47] epoch 1587, training loss: 2530.12, average training loss: 2221.16, base loss: 2379.06
[INFO 2017-06-26 19:20:44,825 main.py:47] epoch 1588, training loss: 2183.91, average training loss: 2220.92, base loss: 2378.93
[INFO 2017-06-26 19:20:45,109 main.py:47] epoch 1589, training loss: 1992.74, average training loss: 2220.99, base loss: 2379.11
[INFO 2017-06-26 19:20:45,394 main.py:47] epoch 1590, training loss: 2295.16, average training loss: 2220.91, base loss: 2379.26
[INFO 2017-06-26 19:20:45,680 main.py:47] epoch 1591, training loss: 2262.44, average training loss: 2221.11, base loss: 2379.57
[INFO 2017-06-26 19:20:45,969 main.py:47] epoch 1592, training loss: 2109.43, average training loss: 2220.90, base loss: 2379.34
[INFO 2017-06-26 19:20:46,257 main.py:47] epoch 1593, training loss: 2282.93, average training loss: 2220.91, base loss: 2379.31
[INFO 2017-06-26 19:20:46,538 main.py:47] epoch 1594, training loss: 2083.77, average training loss: 2220.62, base loss: 2379.09
[INFO 2017-06-26 19:20:46,825 main.py:47] epoch 1595, training loss: 1823.46, average training loss: 2220.14, base loss: 2378.72
[INFO 2017-06-26 19:20:47,110 main.py:47] epoch 1596, training loss: 2198.64, average training loss: 2220.06, base loss: 2378.76
[INFO 2017-06-26 19:20:47,394 main.py:47] epoch 1597, training loss: 1810.46, average training loss: 2219.77, base loss: 2378.49
[INFO 2017-06-26 19:20:47,677 main.py:47] epoch 1598, training loss: 2344.52, average training loss: 2219.81, base loss: 2378.61
[INFO 2017-06-26 19:20:47,959 main.py:47] epoch 1599, training loss: 2382.89, average training loss: 2219.63, base loss: 2378.58
[INFO 2017-06-26 19:20:47,959 main.py:49] epoch 1599, testing
[INFO 2017-06-26 19:20:51,771 main.py:100] average testing loss: 2253.20, base loss: 2451.21
[INFO 2017-06-26 19:20:51,797 main.py:73] current best accuracy: 2152.99
[INFO 2017-06-26 19:20:52,085 main.py:47] epoch 1600, training loss: 2064.65, average training loss: 2219.47, base loss: 2378.50
[INFO 2017-06-26 19:20:52,370 main.py:47] epoch 1601, training loss: 2359.28, average training loss: 2219.87, base loss: 2378.88
[INFO 2017-06-26 19:20:52,657 main.py:47] epoch 1602, training loss: 2150.18, average training loss: 2219.76, base loss: 2378.96
[INFO 2017-06-26 19:20:52,940 main.py:47] epoch 1603, training loss: 2320.77, average training loss: 2220.18, base loss: 2379.43
[INFO 2017-06-26 19:20:53,228 main.py:47] epoch 1604, training loss: 2114.10, average training loss: 2219.90, base loss: 2379.20
[INFO 2017-06-26 19:20:53,515 main.py:47] epoch 1605, training loss: 1854.57, average training loss: 2218.86, base loss: 2378.18
[INFO 2017-06-26 19:20:53,804 main.py:47] epoch 1606, training loss: 2452.19, average training loss: 2219.15, base loss: 2378.43
[INFO 2017-06-26 19:20:54,091 main.py:47] epoch 1607, training loss: 2456.36, average training loss: 2219.71, base loss: 2379.17
[INFO 2017-06-26 19:20:54,372 main.py:47] epoch 1608, training loss: 2022.43, average training loss: 2219.22, base loss: 2378.81
[INFO 2017-06-26 19:20:54,653 main.py:47] epoch 1609, training loss: 2014.13, average training loss: 2218.83, base loss: 2378.52
[INFO 2017-06-26 19:20:54,937 main.py:47] epoch 1610, training loss: 1746.11, average training loss: 2217.90, base loss: 2377.47
[INFO 2017-06-26 19:20:55,220 main.py:47] epoch 1611, training loss: 1742.64, average training loss: 2217.33, base loss: 2376.98
[INFO 2017-06-26 19:20:55,500 main.py:47] epoch 1612, training loss: 2214.70, average training loss: 2216.83, base loss: 2376.50
[INFO 2017-06-26 19:20:55,784 main.py:47] epoch 1613, training loss: 1888.41, average training loss: 2216.91, base loss: 2376.63
[INFO 2017-06-26 19:20:56,067 main.py:47] epoch 1614, training loss: 2173.45, average training loss: 2216.90, base loss: 2376.57
[INFO 2017-06-26 19:20:56,352 main.py:47] epoch 1615, training loss: 2064.63, average training loss: 2216.40, base loss: 2376.10
[INFO 2017-06-26 19:20:56,632 main.py:47] epoch 1616, training loss: 2436.25, average training loss: 2216.54, base loss: 2376.29
[INFO 2017-06-26 19:20:56,918 main.py:47] epoch 1617, training loss: 1871.92, average training loss: 2216.35, base loss: 2376.22
[INFO 2017-06-26 19:20:57,198 main.py:47] epoch 1618, training loss: 2325.87, average training loss: 2216.34, base loss: 2376.26
[INFO 2017-06-26 19:20:57,480 main.py:47] epoch 1619, training loss: 1997.26, average training loss: 2216.07, base loss: 2376.06
[INFO 2017-06-26 19:20:57,762 main.py:47] epoch 1620, training loss: 2093.79, average training loss: 2215.83, base loss: 2375.85
[INFO 2017-06-26 19:20:58,044 main.py:47] epoch 1621, training loss: 1963.17, average training loss: 2215.33, base loss: 2375.22
[INFO 2017-06-26 19:20:58,327 main.py:47] epoch 1622, training loss: 2245.26, average training loss: 2215.05, base loss: 2374.92
[INFO 2017-06-26 19:20:58,609 main.py:47] epoch 1623, training loss: 2171.88, average training loss: 2214.81, base loss: 2374.67
[INFO 2017-06-26 19:20:58,893 main.py:47] epoch 1624, training loss: 1757.25, average training loss: 2214.47, base loss: 2374.31
[INFO 2017-06-26 19:20:59,178 main.py:47] epoch 1625, training loss: 2208.17, average training loss: 2214.28, base loss: 2374.18
[INFO 2017-06-26 19:20:59,465 main.py:47] epoch 1626, training loss: 2197.48, average training loss: 2213.86, base loss: 2373.77
[INFO 2017-06-26 19:20:59,752 main.py:47] epoch 1627, training loss: 1940.54, average training loss: 2213.65, base loss: 2373.64
[INFO 2017-06-26 19:21:00,037 main.py:47] epoch 1628, training loss: 1812.17, average training loss: 2213.24, base loss: 2373.33
[INFO 2017-06-26 19:21:00,324 main.py:47] epoch 1629, training loss: 2298.41, average training loss: 2213.49, base loss: 2373.65
[INFO 2017-06-26 19:21:00,609 main.py:47] epoch 1630, training loss: 2034.69, average training loss: 2213.17, base loss: 2373.39
[INFO 2017-06-26 19:21:00,896 main.py:47] epoch 1631, training loss: 2249.55, average training loss: 2213.14, base loss: 2373.39
[INFO 2017-06-26 19:21:01,177 main.py:47] epoch 1632, training loss: 2273.78, average training loss: 2213.68, base loss: 2374.17
[INFO 2017-06-26 19:21:01,461 main.py:47] epoch 1633, training loss: 1969.75, average training loss: 2213.34, base loss: 2373.93
[INFO 2017-06-26 19:21:01,745 main.py:47] epoch 1634, training loss: 2008.46, average training loss: 2212.36, base loss: 2372.94
[INFO 2017-06-26 19:21:02,029 main.py:47] epoch 1635, training loss: 2281.10, average training loss: 2212.41, base loss: 2373.08
[INFO 2017-06-26 19:21:02,313 main.py:47] epoch 1636, training loss: 2411.50, average training loss: 2212.53, base loss: 2373.42
[INFO 2017-06-26 19:21:02,593 main.py:47] epoch 1637, training loss: 2158.37, average training loss: 2212.51, base loss: 2373.47
[INFO 2017-06-26 19:21:02,880 main.py:47] epoch 1638, training loss: 2564.74, average training loss: 2212.81, base loss: 2373.96
[INFO 2017-06-26 19:21:03,163 main.py:47] epoch 1639, training loss: 2274.50, average training loss: 2212.99, base loss: 2374.17
[INFO 2017-06-26 19:21:03,448 main.py:47] epoch 1640, training loss: 2205.48, average training loss: 2212.44, base loss: 2373.58
[INFO 2017-06-26 19:21:03,733 main.py:47] epoch 1641, training loss: 1856.33, average training loss: 2212.08, base loss: 2373.12
[INFO 2017-06-26 19:21:04,021 main.py:47] epoch 1642, training loss: 1901.29, average training loss: 2211.41, base loss: 2372.38
[INFO 2017-06-26 19:21:04,304 main.py:47] epoch 1643, training loss: 2057.31, average training loss: 2210.91, base loss: 2372.01
[INFO 2017-06-26 19:21:04,584 main.py:47] epoch 1644, training loss: 1798.60, average training loss: 2210.22, base loss: 2371.37
[INFO 2017-06-26 19:21:04,868 main.py:47] epoch 1645, training loss: 2071.05, average training loss: 2210.06, base loss: 2371.30
[INFO 2017-06-26 19:21:05,152 main.py:47] epoch 1646, training loss: 1871.65, average training loss: 2209.62, base loss: 2370.93
[INFO 2017-06-26 19:21:05,436 main.py:47] epoch 1647, training loss: 2349.21, average training loss: 2209.57, base loss: 2370.98
[INFO 2017-06-26 19:21:05,720 main.py:47] epoch 1648, training loss: 2762.46, average training loss: 2210.30, base loss: 2371.82
[INFO 2017-06-26 19:21:06,008 main.py:47] epoch 1649, training loss: 2045.14, average training loss: 2209.91, base loss: 2371.53
[INFO 2017-06-26 19:21:06,295 main.py:47] epoch 1650, training loss: 2381.29, average training loss: 2210.19, base loss: 2371.67
[INFO 2017-06-26 19:21:06,578 main.py:47] epoch 1651, training loss: 2138.49, average training loss: 2210.12, base loss: 2371.75
[INFO 2017-06-26 19:21:06,862 main.py:47] epoch 1652, training loss: 2002.08, average training loss: 2209.65, base loss: 2371.32
[INFO 2017-06-26 19:21:07,147 main.py:47] epoch 1653, training loss: 2011.62, average training loss: 2209.33, base loss: 2371.08
[INFO 2017-06-26 19:21:07,430 main.py:47] epoch 1654, training loss: 2205.19, average training loss: 2209.24, base loss: 2371.17
[INFO 2017-06-26 19:21:07,717 main.py:47] epoch 1655, training loss: 2013.38, average training loss: 2208.57, base loss: 2370.50
[INFO 2017-06-26 19:21:08,002 main.py:47] epoch 1656, training loss: 1993.44, average training loss: 2207.69, base loss: 2369.37
[INFO 2017-06-26 19:21:08,287 main.py:47] epoch 1657, training loss: 1980.33, average training loss: 2207.59, base loss: 2369.56
[INFO 2017-06-26 19:21:08,571 main.py:47] epoch 1658, training loss: 1730.56, average training loss: 2207.22, base loss: 2369.25
[INFO 2017-06-26 19:21:08,854 main.py:47] epoch 1659, training loss: 1859.53, average training loss: 2206.57, base loss: 2368.67
[INFO 2017-06-26 19:21:09,134 main.py:47] epoch 1660, training loss: 2711.28, average training loss: 2206.73, base loss: 2369.00
[INFO 2017-06-26 19:21:09,415 main.py:47] epoch 1661, training loss: 2731.19, average training loss: 2206.93, base loss: 2369.35
[INFO 2017-06-26 19:21:09,702 main.py:47] epoch 1662, training loss: 2106.85, average training loss: 2206.85, base loss: 2369.37
[INFO 2017-06-26 19:21:09,987 main.py:47] epoch 1663, training loss: 2084.54, average training loss: 2206.19, base loss: 2368.70
[INFO 2017-06-26 19:21:10,272 main.py:47] epoch 1664, training loss: 1744.35, average training loss: 2205.51, base loss: 2367.98
[INFO 2017-06-26 19:21:10,556 main.py:47] epoch 1665, training loss: 2643.51, average training loss: 2205.50, base loss: 2368.09
[INFO 2017-06-26 19:21:10,840 main.py:47] epoch 1666, training loss: 2486.95, average training loss: 2205.85, base loss: 2368.69
[INFO 2017-06-26 19:21:11,124 main.py:47] epoch 1667, training loss: 2245.52, average training loss: 2206.04, base loss: 2369.08
[INFO 2017-06-26 19:21:11,411 main.py:47] epoch 1668, training loss: 2355.21, average training loss: 2206.11, base loss: 2369.31
[INFO 2017-06-26 19:21:11,694 main.py:47] epoch 1669, training loss: 2240.95, average training loss: 2206.40, base loss: 2369.95
[INFO 2017-06-26 19:21:11,981 main.py:47] epoch 1670, training loss: 2154.11, average training loss: 2205.93, base loss: 2369.48
[INFO 2017-06-26 19:21:12,264 main.py:47] epoch 1671, training loss: 2207.69, average training loss: 2205.80, base loss: 2369.28
[INFO 2017-06-26 19:21:12,548 main.py:47] epoch 1672, training loss: 2409.14, average training loss: 2205.93, base loss: 2369.48
[INFO 2017-06-26 19:21:12,834 main.py:47] epoch 1673, training loss: 1971.26, average training loss: 2205.59, base loss: 2369.05
[INFO 2017-06-26 19:21:13,120 main.py:47] epoch 1674, training loss: 1936.08, average training loss: 2205.22, base loss: 2368.65
[INFO 2017-06-26 19:21:13,401 main.py:47] epoch 1675, training loss: 1941.02, average training loss: 2205.10, base loss: 2368.62
[INFO 2017-06-26 19:21:13,688 main.py:47] epoch 1676, training loss: 2032.58, average training loss: 2205.34, base loss: 2369.04
[INFO 2017-06-26 19:21:13,969 main.py:47] epoch 1677, training loss: 2010.66, average training loss: 2205.44, base loss: 2369.28
[INFO 2017-06-26 19:21:14,252 main.py:47] epoch 1678, training loss: 2325.16, average training loss: 2205.19, base loss: 2369.04
[INFO 2017-06-26 19:21:14,540 main.py:47] epoch 1679, training loss: 2611.75, average training loss: 2205.39, base loss: 2369.32
[INFO 2017-06-26 19:21:14,827 main.py:47] epoch 1680, training loss: 2436.42, average training loss: 2205.75, base loss: 2369.91
[INFO 2017-06-26 19:21:15,114 main.py:47] epoch 1681, training loss: 1948.18, average training loss: 2205.59, base loss: 2369.70
[INFO 2017-06-26 19:21:15,402 main.py:47] epoch 1682, training loss: 2330.04, average training loss: 2206.03, base loss: 2370.24
[INFO 2017-06-26 19:21:15,684 main.py:47] epoch 1683, training loss: 1880.26, average training loss: 2205.80, base loss: 2369.93
[INFO 2017-06-26 19:21:15,968 main.py:47] epoch 1684, training loss: 2136.70, average training loss: 2205.55, base loss: 2369.72
[INFO 2017-06-26 19:21:16,257 main.py:47] epoch 1685, training loss: 2439.86, average training loss: 2205.82, base loss: 2370.10
[INFO 2017-06-26 19:21:16,540 main.py:47] epoch 1686, training loss: 1952.35, average training loss: 2205.34, base loss: 2369.65
[INFO 2017-06-26 19:21:16,824 main.py:47] epoch 1687, training loss: 2054.79, average training loss: 2205.03, base loss: 2369.27
[INFO 2017-06-26 19:21:17,110 main.py:47] epoch 1688, training loss: 1787.92, average training loss: 2204.16, base loss: 2368.37
[INFO 2017-06-26 19:21:17,392 main.py:47] epoch 1689, training loss: 2361.91, average training loss: 2204.39, base loss: 2368.65
[INFO 2017-06-26 19:21:17,680 main.py:47] epoch 1690, training loss: 1964.10, average training loss: 2204.18, base loss: 2368.54
[INFO 2017-06-26 19:21:17,969 main.py:47] epoch 1691, training loss: 1797.73, average training loss: 2203.69, base loss: 2368.00
[INFO 2017-06-26 19:21:18,254 main.py:47] epoch 1692, training loss: 2148.11, average training loss: 2203.61, base loss: 2368.05
[INFO 2017-06-26 19:21:18,539 main.py:47] epoch 1693, training loss: 1923.69, average training loss: 2203.07, base loss: 2367.48
[INFO 2017-06-26 19:21:18,820 main.py:47] epoch 1694, training loss: 2185.44, average training loss: 2203.26, base loss: 2367.82
[INFO 2017-06-26 19:21:19,104 main.py:47] epoch 1695, training loss: 2217.92, average training loss: 2202.74, base loss: 2367.36
[INFO 2017-06-26 19:21:19,389 main.py:47] epoch 1696, training loss: 1886.46, average training loss: 2202.24, base loss: 2366.82
[INFO 2017-06-26 19:21:19,677 main.py:47] epoch 1697, training loss: 1927.19, average training loss: 2201.87, base loss: 2366.50
[INFO 2017-06-26 19:21:19,958 main.py:47] epoch 1698, training loss: 1815.57, average training loss: 2201.61, base loss: 2366.28
[INFO 2017-06-26 19:21:20,241 main.py:47] epoch 1699, training loss: 2364.01, average training loss: 2202.01, base loss: 2366.90
[INFO 2017-06-26 19:21:20,241 main.py:49] epoch 1699, testing
[INFO 2017-06-26 19:21:23,965 main.py:100] average testing loss: 2049.08, base loss: 2248.27
[INFO 2017-06-26 19:21:23,991 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:21:24,005 main.py:73] current best accuracy: 2049.08
[INFO 2017-06-26 19:21:24,290 main.py:47] epoch 1700, training loss: 2295.85, average training loss: 2202.49, base loss: 2367.49
[INFO 2017-06-26 19:21:24,572 main.py:47] epoch 1701, training loss: 2537.16, average training loss: 2202.84, base loss: 2368.07
[INFO 2017-06-26 19:21:24,853 main.py:47] epoch 1702, training loss: 2064.22, average training loss: 2202.36, base loss: 2367.58
[INFO 2017-06-26 19:21:25,134 main.py:47] epoch 1703, training loss: 2022.32, average training loss: 2202.05, base loss: 2367.30
[INFO 2017-06-26 19:21:25,419 main.py:47] epoch 1704, training loss: 2146.29, average training loss: 2201.87, base loss: 2367.27
[INFO 2017-06-26 19:21:25,705 main.py:47] epoch 1705, training loss: 2219.56, average training loss: 2201.85, base loss: 2367.36
[INFO 2017-06-26 19:21:25,988 main.py:47] epoch 1706, training loss: 2192.63, average training loss: 2201.97, base loss: 2367.59
[INFO 2017-06-26 19:21:26,274 main.py:47] epoch 1707, training loss: 1948.47, average training loss: 2201.68, base loss: 2367.10
[INFO 2017-06-26 19:21:26,558 main.py:47] epoch 1708, training loss: 1856.96, average training loss: 2201.44, base loss: 2366.85
[INFO 2017-06-26 19:21:26,846 main.py:47] epoch 1709, training loss: 1960.10, average training loss: 2201.41, base loss: 2367.08
[INFO 2017-06-26 19:21:27,131 main.py:47] epoch 1710, training loss: 2396.56, average training loss: 2201.45, base loss: 2367.15
[INFO 2017-06-26 19:21:27,414 main.py:47] epoch 1711, training loss: 1855.00, average training loss: 2201.13, base loss: 2366.83
[INFO 2017-06-26 19:21:27,698 main.py:47] epoch 1712, training loss: 1891.25, average training loss: 2200.94, base loss: 2366.72
[INFO 2017-06-26 19:21:27,984 main.py:47] epoch 1713, training loss: 2373.98, average training loss: 2201.06, base loss: 2366.80
[INFO 2017-06-26 19:21:28,270 main.py:47] epoch 1714, training loss: 2234.19, average training loss: 2201.17, base loss: 2366.80
[INFO 2017-06-26 19:21:28,553 main.py:47] epoch 1715, training loss: 2416.38, average training loss: 2200.99, base loss: 2366.71
[INFO 2017-06-26 19:21:28,838 main.py:47] epoch 1716, training loss: 2067.71, average training loss: 2200.69, base loss: 2366.53
[INFO 2017-06-26 19:21:29,126 main.py:47] epoch 1717, training loss: 2099.82, average training loss: 2200.31, base loss: 2366.12
[INFO 2017-06-26 19:21:29,412 main.py:47] epoch 1718, training loss: 1935.24, average training loss: 2199.67, base loss: 2365.45
[INFO 2017-06-26 19:21:29,697 main.py:47] epoch 1719, training loss: 2469.95, average training loss: 2199.99, base loss: 2365.87
[INFO 2017-06-26 19:21:29,984 main.py:47] epoch 1720, training loss: 1739.58, average training loss: 2199.73, base loss: 2365.50
[INFO 2017-06-26 19:21:30,269 main.py:47] epoch 1721, training loss: 1802.20, average training loss: 2199.36, base loss: 2365.11
[INFO 2017-06-26 19:21:30,553 main.py:47] epoch 1722, training loss: 2071.41, average training loss: 2199.13, base loss: 2364.88
[INFO 2017-06-26 19:21:30,838 main.py:47] epoch 1723, training loss: 1823.03, average training loss: 2198.22, base loss: 2363.86
[INFO 2017-06-26 19:21:31,121 main.py:47] epoch 1724, training loss: 2251.04, average training loss: 2198.35, base loss: 2364.12
[INFO 2017-06-26 19:21:31,404 main.py:47] epoch 1725, training loss: 2227.31, average training loss: 2198.20, base loss: 2363.94
[INFO 2017-06-26 19:21:31,692 main.py:47] epoch 1726, training loss: 2138.66, average training loss: 2198.14, base loss: 2363.95
[INFO 2017-06-26 19:21:31,976 main.py:47] epoch 1727, training loss: 2420.40, average training loss: 2198.34, base loss: 2364.20
[INFO 2017-06-26 19:21:32,264 main.py:47] epoch 1728, training loss: 2031.09, average training loss: 2197.83, base loss: 2363.68
[INFO 2017-06-26 19:21:32,553 main.py:47] epoch 1729, training loss: 2071.99, average training loss: 2197.74, base loss: 2363.77
[INFO 2017-06-26 19:21:32,837 main.py:47] epoch 1730, training loss: 1796.98, average training loss: 2196.80, base loss: 2362.94
[INFO 2017-06-26 19:21:33,123 main.py:47] epoch 1731, training loss: 1942.34, average training loss: 2197.02, base loss: 2363.26
[INFO 2017-06-26 19:21:33,404 main.py:47] epoch 1732, training loss: 2197.02, average training loss: 2197.17, base loss: 2363.70
[INFO 2017-06-26 19:21:33,686 main.py:47] epoch 1733, training loss: 2101.90, average training loss: 2196.79, base loss: 2363.46
[INFO 2017-06-26 19:21:33,966 main.py:47] epoch 1734, training loss: 2002.94, average training loss: 2196.52, base loss: 2363.32
[INFO 2017-06-26 19:21:34,255 main.py:47] epoch 1735, training loss: 2468.04, average training loss: 2196.60, base loss: 2363.42
[INFO 2017-06-26 19:21:34,537 main.py:47] epoch 1736, training loss: 1933.10, average training loss: 2196.33, base loss: 2363.15
[INFO 2017-06-26 19:21:34,821 main.py:47] epoch 1737, training loss: 2225.78, average training loss: 2196.58, base loss: 2363.49
[INFO 2017-06-26 19:21:35,101 main.py:47] epoch 1738, training loss: 1991.33, average training loss: 2196.18, base loss: 2363.11
[INFO 2017-06-26 19:21:35,387 main.py:47] epoch 1739, training loss: 2326.16, average training loss: 2196.44, base loss: 2363.39
[INFO 2017-06-26 19:21:35,674 main.py:47] epoch 1740, training loss: 2239.49, average training loss: 2196.02, base loss: 2363.08
[INFO 2017-06-26 19:21:35,954 main.py:47] epoch 1741, training loss: 2229.56, average training loss: 2196.26, base loss: 2363.49
[INFO 2017-06-26 19:21:36,239 main.py:47] epoch 1742, training loss: 2015.87, average training loss: 2195.44, base loss: 2362.70
[INFO 2017-06-26 19:21:36,525 main.py:47] epoch 1743, training loss: 2092.04, average training loss: 2195.50, base loss: 2362.78
[INFO 2017-06-26 19:21:36,810 main.py:47] epoch 1744, training loss: 2428.52, average training loss: 2195.37, base loss: 2362.71
[INFO 2017-06-26 19:21:37,095 main.py:47] epoch 1745, training loss: 2275.90, average training loss: 2195.61, base loss: 2363.08
[INFO 2017-06-26 19:21:37,376 main.py:47] epoch 1746, training loss: 2386.19, average training loss: 2196.21, base loss: 2363.95
[INFO 2017-06-26 19:21:37,661 main.py:47] epoch 1747, training loss: 1926.03, average training loss: 2195.75, base loss: 2363.52
[INFO 2017-06-26 19:21:37,942 main.py:47] epoch 1748, training loss: 1939.41, average training loss: 2195.05, base loss: 2362.99
[INFO 2017-06-26 19:21:38,228 main.py:47] epoch 1749, training loss: 2488.94, average training loss: 2195.71, base loss: 2363.89
[INFO 2017-06-26 19:21:38,508 main.py:47] epoch 1750, training loss: 2146.25, average training loss: 2195.68, base loss: 2363.99
[INFO 2017-06-26 19:21:38,791 main.py:47] epoch 1751, training loss: 2134.00, average training loss: 2195.31, base loss: 2363.72
[INFO 2017-06-26 19:21:39,073 main.py:47] epoch 1752, training loss: 2557.13, average training loss: 2195.54, base loss: 2364.17
[INFO 2017-06-26 19:21:39,362 main.py:47] epoch 1753, training loss: 1997.42, average training loss: 2195.27, base loss: 2363.90
[INFO 2017-06-26 19:21:39,656 main.py:47] epoch 1754, training loss: 2002.42, average training loss: 2194.96, base loss: 2363.68
[INFO 2017-06-26 19:21:39,936 main.py:47] epoch 1755, training loss: 2143.58, average training loss: 2194.62, base loss: 2363.39
[INFO 2017-06-26 19:21:40,224 main.py:47] epoch 1756, training loss: 2092.51, average training loss: 2194.50, base loss: 2363.33
[INFO 2017-06-26 19:21:40,512 main.py:47] epoch 1757, training loss: 2081.66, average training loss: 2194.45, base loss: 2363.51
[INFO 2017-06-26 19:21:40,792 main.py:47] epoch 1758, training loss: 2255.49, average training loss: 2194.66, base loss: 2363.75
[INFO 2017-06-26 19:21:41,084 main.py:47] epoch 1759, training loss: 2143.36, average training loss: 2194.30, base loss: 2363.46
[INFO 2017-06-26 19:21:41,369 main.py:47] epoch 1760, training loss: 2577.28, average training loss: 2194.81, base loss: 2364.14
[INFO 2017-06-26 19:21:41,651 main.py:47] epoch 1761, training loss: 2082.96, average training loss: 2194.80, base loss: 2364.05
[INFO 2017-06-26 19:21:41,936 main.py:47] epoch 1762, training loss: 1643.76, average training loss: 2194.18, base loss: 2363.34
[INFO 2017-06-26 19:21:42,220 main.py:47] epoch 1763, training loss: 2136.80, average training loss: 2193.96, base loss: 2363.14
[INFO 2017-06-26 19:21:42,503 main.py:47] epoch 1764, training loss: 1946.48, average training loss: 2193.81, base loss: 2363.02
[INFO 2017-06-26 19:21:42,790 main.py:47] epoch 1765, training loss: 1916.66, average training loss: 2193.91, base loss: 2363.17
[INFO 2017-06-26 19:21:43,075 main.py:47] epoch 1766, training loss: 2018.12, average training loss: 2193.85, base loss: 2363.12
[INFO 2017-06-26 19:21:43,361 main.py:47] epoch 1767, training loss: 1960.15, average training loss: 2193.95, base loss: 2363.41
[INFO 2017-06-26 19:21:43,645 main.py:47] epoch 1768, training loss: 2128.34, average training loss: 2194.14, base loss: 2363.59
[INFO 2017-06-26 19:21:43,926 main.py:47] epoch 1769, training loss: 2695.36, average training loss: 2194.05, base loss: 2363.51
[INFO 2017-06-26 19:21:44,211 main.py:47] epoch 1770, training loss: 2174.81, average training loss: 2194.33, base loss: 2363.99
[INFO 2017-06-26 19:21:44,497 main.py:47] epoch 1771, training loss: 1857.29, average training loss: 2194.11, base loss: 2363.82
[INFO 2017-06-26 19:21:44,782 main.py:47] epoch 1772, training loss: 1712.63, average training loss: 2193.62, base loss: 2363.32
[INFO 2017-06-26 19:21:45,066 main.py:47] epoch 1773, training loss: 2408.27, average training loss: 2193.69, base loss: 2363.51
[INFO 2017-06-26 19:21:45,350 main.py:47] epoch 1774, training loss: 2008.85, average training loss: 2193.58, base loss: 2363.44
[INFO 2017-06-26 19:21:45,636 main.py:47] epoch 1775, training loss: 2079.86, average training loss: 2192.78, base loss: 2362.63
[INFO 2017-06-26 19:21:45,922 main.py:47] epoch 1776, training loss: 2134.90, average training loss: 2192.98, base loss: 2362.80
[INFO 2017-06-26 19:21:46,203 main.py:47] epoch 1777, training loss: 2292.24, average training loss: 2192.24, base loss: 2362.09
[INFO 2017-06-26 19:21:46,488 main.py:47] epoch 1778, training loss: 2235.95, average training loss: 2192.26, base loss: 2362.22
[INFO 2017-06-26 19:21:46,772 main.py:47] epoch 1779, training loss: 2113.03, average training loss: 2192.38, base loss: 2362.55
[INFO 2017-06-26 19:21:47,053 main.py:47] epoch 1780, training loss: 2248.69, average training loss: 2192.57, base loss: 2362.94
[INFO 2017-06-26 19:21:47,335 main.py:47] epoch 1781, training loss: 2283.75, average training loss: 2192.90, base loss: 2363.28
[INFO 2017-06-26 19:21:47,618 main.py:47] epoch 1782, training loss: 2033.88, average training loss: 2192.76, base loss: 2363.04
[INFO 2017-06-26 19:21:47,902 main.py:47] epoch 1783, training loss: 2629.14, average training loss: 2193.12, base loss: 2363.73
[INFO 2017-06-26 19:21:48,183 main.py:47] epoch 1784, training loss: 2792.49, average training loss: 2193.53, base loss: 2364.29
[INFO 2017-06-26 19:21:48,467 main.py:47] epoch 1785, training loss: 2501.33, average training loss: 2193.59, base loss: 2364.61
[INFO 2017-06-26 19:21:48,751 main.py:47] epoch 1786, training loss: 2294.68, average training loss: 2193.52, base loss: 2364.57
[INFO 2017-06-26 19:21:49,034 main.py:47] epoch 1787, training loss: 2309.54, average training loss: 2193.98, base loss: 2364.95
[INFO 2017-06-26 19:21:49,315 main.py:47] epoch 1788, training loss: 2052.49, average training loss: 2194.27, base loss: 2365.36
[INFO 2017-06-26 19:21:49,596 main.py:47] epoch 1789, training loss: 2100.93, average training loss: 2194.03, base loss: 2365.24
[INFO 2017-06-26 19:21:49,878 main.py:47] epoch 1790, training loss: 2419.65, average training loss: 2194.20, base loss: 2365.71
[INFO 2017-06-26 19:21:50,163 main.py:47] epoch 1791, training loss: 1967.01, average training loss: 2193.82, base loss: 2365.23
[INFO 2017-06-26 19:21:50,447 main.py:47] epoch 1792, training loss: 2448.80, average training loss: 2194.16, base loss: 2365.68
[INFO 2017-06-26 19:21:50,728 main.py:47] epoch 1793, training loss: 1907.44, average training loss: 2193.85, base loss: 2365.29
[INFO 2017-06-26 19:21:51,014 main.py:47] epoch 1794, training loss: 2456.05, average training loss: 2194.02, base loss: 2365.54
[INFO 2017-06-26 19:21:51,300 main.py:47] epoch 1795, training loss: 2536.65, average training loss: 2194.36, base loss: 2365.95
[INFO 2017-06-26 19:21:51,581 main.py:47] epoch 1796, training loss: 2308.25, average training loss: 2194.69, base loss: 2366.23
[INFO 2017-06-26 19:21:51,866 main.py:47] epoch 1797, training loss: 2253.16, average training loss: 2194.85, base loss: 2366.54
[INFO 2017-06-26 19:21:52,150 main.py:47] epoch 1798, training loss: 2225.09, average training loss: 2195.04, base loss: 2366.73
[INFO 2017-06-26 19:21:52,436 main.py:47] epoch 1799, training loss: 2483.22, average training loss: 2195.06, base loss: 2366.64
[INFO 2017-06-26 19:21:52,436 main.py:49] epoch 1799, testing
[INFO 2017-06-26 19:21:56,177 main.py:100] average testing loss: 2114.47, base loss: 2307.78
[INFO 2017-06-26 19:21:56,204 main.py:73] current best accuracy: 2049.08
[INFO 2017-06-26 19:21:56,493 main.py:47] epoch 1800, training loss: 1845.33, average training loss: 2194.75, base loss: 2366.27
[INFO 2017-06-26 19:21:56,779 main.py:47] epoch 1801, training loss: 2438.76, average training loss: 2195.01, base loss: 2366.60
[INFO 2017-06-26 19:21:57,063 main.py:47] epoch 1802, training loss: 2082.28, average training loss: 2195.08, base loss: 2366.81
[INFO 2017-06-26 19:21:57,349 main.py:47] epoch 1803, training loss: 2206.19, average training loss: 2194.99, base loss: 2366.74
[INFO 2017-06-26 19:21:57,633 main.py:47] epoch 1804, training loss: 2370.88, average training loss: 2195.03, base loss: 2366.92
[INFO 2017-06-26 19:21:57,917 main.py:47] epoch 1805, training loss: 2034.21, average training loss: 2194.72, base loss: 2366.65
[INFO 2017-06-26 19:21:58,201 main.py:47] epoch 1806, training loss: 2023.16, average training loss: 2194.17, base loss: 2366.27
[INFO 2017-06-26 19:21:58,488 main.py:47] epoch 1807, training loss: 1693.99, average training loss: 2193.55, base loss: 2365.68
[INFO 2017-06-26 19:21:58,774 main.py:47] epoch 1808, training loss: 2096.77, average training loss: 2193.14, base loss: 2365.28
[INFO 2017-06-26 19:21:59,060 main.py:47] epoch 1809, training loss: 2376.64, average training loss: 2193.45, base loss: 2365.82
[INFO 2017-06-26 19:21:59,341 main.py:47] epoch 1810, training loss: 2143.11, average training loss: 2193.03, base loss: 2365.48
[INFO 2017-06-26 19:21:59,625 main.py:47] epoch 1811, training loss: 2025.91, average training loss: 2192.23, base loss: 2364.65
[INFO 2017-06-26 19:21:59,909 main.py:47] epoch 1812, training loss: 2126.55, average training loss: 2191.74, base loss: 2364.18
[INFO 2017-06-26 19:22:00,192 main.py:47] epoch 1813, training loss: 2230.39, average training loss: 2191.98, base loss: 2364.38
[INFO 2017-06-26 19:22:00,477 main.py:47] epoch 1814, training loss: 2183.60, average training loss: 2191.77, base loss: 2364.37
[INFO 2017-06-26 19:22:00,762 main.py:47] epoch 1815, training loss: 2441.46, average training loss: 2192.07, base loss: 2364.89
[INFO 2017-06-26 19:22:01,050 main.py:47] epoch 1816, training loss: 2007.72, average training loss: 2191.67, base loss: 2364.54
[INFO 2017-06-26 19:22:01,335 main.py:47] epoch 1817, training loss: 2234.89, average training loss: 2191.79, base loss: 2364.72
[INFO 2017-06-26 19:22:01,618 main.py:47] epoch 1818, training loss: 1944.25, average training loss: 2191.07, base loss: 2363.89
[INFO 2017-06-26 19:22:01,902 main.py:47] epoch 1819, training loss: 2166.11, average training loss: 2190.91, base loss: 2363.78
[INFO 2017-06-26 19:22:02,191 main.py:47] epoch 1820, training loss: 2304.66, average training loss: 2191.19, base loss: 2363.93
[INFO 2017-06-26 19:22:02,475 main.py:47] epoch 1821, training loss: 2199.54, average training loss: 2190.79, base loss: 2363.71
[INFO 2017-06-26 19:22:02,763 main.py:47] epoch 1822, training loss: 2272.95, average training loss: 2190.64, base loss: 2363.61
[INFO 2017-06-26 19:22:03,047 main.py:47] epoch 1823, training loss: 2165.01, average training loss: 2190.33, base loss: 2363.61
[INFO 2017-06-26 19:22:03,334 main.py:47] epoch 1824, training loss: 1751.78, average training loss: 2190.03, base loss: 2363.35
[INFO 2017-06-26 19:22:03,620 main.py:47] epoch 1825, training loss: 2555.15, average training loss: 2190.27, base loss: 2363.68
[INFO 2017-06-26 19:22:03,905 main.py:47] epoch 1826, training loss: 2316.69, average training loss: 2190.07, base loss: 2363.47
[INFO 2017-06-26 19:22:04,187 main.py:47] epoch 1827, training loss: 2466.14, average training loss: 2190.15, base loss: 2363.51
[INFO 2017-06-26 19:22:04,469 main.py:47] epoch 1828, training loss: 2189.92, average training loss: 2189.73, base loss: 2363.17
[INFO 2017-06-26 19:22:04,755 main.py:47] epoch 1829, training loss: 2386.16, average training loss: 2189.80, base loss: 2363.42
[INFO 2017-06-26 19:22:05,039 main.py:47] epoch 1830, training loss: 2543.89, average training loss: 2190.07, base loss: 2363.85
[INFO 2017-06-26 19:22:05,325 main.py:47] epoch 1831, training loss: 2320.27, average training loss: 2190.22, base loss: 2364.14
[INFO 2017-06-26 19:22:05,610 main.py:47] epoch 1832, training loss: 2095.00, average training loss: 2190.01, base loss: 2363.89
[INFO 2017-06-26 19:22:05,893 main.py:47] epoch 1833, training loss: 2335.36, average training loss: 2189.96, base loss: 2363.88
[INFO 2017-06-26 19:22:06,177 main.py:47] epoch 1834, training loss: 2139.69, average training loss: 2190.17, base loss: 2364.28
[INFO 2017-06-26 19:22:06,460 main.py:47] epoch 1835, training loss: 2690.27, average training loss: 2190.46, base loss: 2364.64
[INFO 2017-06-26 19:22:06,745 main.py:47] epoch 1836, training loss: 2184.99, average training loss: 2190.44, base loss: 2364.75
[INFO 2017-06-26 19:22:07,033 main.py:47] epoch 1837, training loss: 2275.25, average training loss: 2190.53, base loss: 2365.04
[INFO 2017-06-26 19:22:07,321 main.py:47] epoch 1838, training loss: 2188.41, average training loss: 2190.26, base loss: 2364.89
[INFO 2017-06-26 19:22:07,610 main.py:47] epoch 1839, training loss: 2272.08, average training loss: 2190.33, base loss: 2364.98
[INFO 2017-06-26 19:22:07,898 main.py:47] epoch 1840, training loss: 2282.53, average training loss: 2190.26, base loss: 2364.97
[INFO 2017-06-26 19:22:08,186 main.py:47] epoch 1841, training loss: 2000.56, average training loss: 2190.11, base loss: 2364.97
[INFO 2017-06-26 19:22:08,474 main.py:47] epoch 1842, training loss: 2084.23, average training loss: 2190.09, base loss: 2364.93
[INFO 2017-06-26 19:22:08,762 main.py:47] epoch 1843, training loss: 2066.47, average training loss: 2189.87, base loss: 2364.95
[INFO 2017-06-26 19:22:09,049 main.py:47] epoch 1844, training loss: 2235.14, average training loss: 2189.80, base loss: 2364.92
[INFO 2017-06-26 19:22:09,332 main.py:47] epoch 1845, training loss: 2543.19, average training loss: 2190.11, base loss: 2365.36
[INFO 2017-06-26 19:22:09,616 main.py:47] epoch 1846, training loss: 1975.67, average training loss: 2189.85, base loss: 2365.19
[INFO 2017-06-26 19:22:09,904 main.py:47] epoch 1847, training loss: 2359.54, average training loss: 2190.09, base loss: 2365.46
[INFO 2017-06-26 19:22:10,190 main.py:47] epoch 1848, training loss: 2315.55, average training loss: 2189.95, base loss: 2365.38
[INFO 2017-06-26 19:22:10,473 main.py:47] epoch 1849, training loss: 2147.51, average training loss: 2189.37, base loss: 2364.73
[INFO 2017-06-26 19:22:10,761 main.py:47] epoch 1850, training loss: 2345.20, average training loss: 2189.36, base loss: 2364.84
[INFO 2017-06-26 19:22:11,044 main.py:47] epoch 1851, training loss: 1864.34, average training loss: 2189.35, base loss: 2364.91
[INFO 2017-06-26 19:22:11,328 main.py:47] epoch 1852, training loss: 1751.26, average training loss: 2189.08, base loss: 2364.68
[INFO 2017-06-26 19:22:11,614 main.py:47] epoch 1853, training loss: 2759.30, average training loss: 2189.42, base loss: 2365.24
[INFO 2017-06-26 19:22:11,902 main.py:47] epoch 1854, training loss: 2165.80, average training loss: 2189.40, base loss: 2365.26
[INFO 2017-06-26 19:22:12,189 main.py:47] epoch 1855, training loss: 1986.14, average training loss: 2189.08, base loss: 2364.96
[INFO 2017-06-26 19:22:12,475 main.py:47] epoch 1856, training loss: 2534.28, average training loss: 2189.20, base loss: 2365.25
[INFO 2017-06-26 19:22:12,764 main.py:47] epoch 1857, training loss: 2174.98, average training loss: 2189.08, base loss: 2365.12
[INFO 2017-06-26 19:22:13,048 main.py:47] epoch 1858, training loss: 1989.61, average training loss: 2189.26, base loss: 2365.45
[INFO 2017-06-26 19:22:13,337 main.py:47] epoch 1859, training loss: 2027.36, average training loss: 2188.78, base loss: 2365.06
[INFO 2017-06-26 19:22:13,621 main.py:47] epoch 1860, training loss: 2028.98, average training loss: 2188.73, base loss: 2365.10
[INFO 2017-06-26 19:22:13,908 main.py:47] epoch 1861, training loss: 2218.63, average training loss: 2188.95, base loss: 2365.43
[INFO 2017-06-26 19:22:14,197 main.py:47] epoch 1862, training loss: 2245.85, average training loss: 2189.42, base loss: 2366.13
[INFO 2017-06-26 19:22:14,478 main.py:47] epoch 1863, training loss: 2302.38, average training loss: 2189.60, base loss: 2366.47
[INFO 2017-06-26 19:22:14,764 main.py:47] epoch 1864, training loss: 1960.72, average training loss: 2189.37, base loss: 2366.25
[INFO 2017-06-26 19:22:15,053 main.py:47] epoch 1865, training loss: 1988.24, average training loss: 2189.30, base loss: 2366.22
[INFO 2017-06-26 19:22:15,340 main.py:47] epoch 1866, training loss: 1860.93, average training loss: 2188.93, base loss: 2365.72
[INFO 2017-06-26 19:22:15,628 main.py:47] epoch 1867, training loss: 2273.14, average training loss: 2189.06, base loss: 2365.91
[INFO 2017-06-26 19:22:15,911 main.py:47] epoch 1868, training loss: 2515.97, average training loss: 2189.42, base loss: 2366.36
[INFO 2017-06-26 19:22:16,199 main.py:47] epoch 1869, training loss: 2537.07, average training loss: 2189.69, base loss: 2366.45
[INFO 2017-06-26 19:22:16,486 main.py:47] epoch 1870, training loss: 2344.14, average training loss: 2189.52, base loss: 2366.38
[INFO 2017-06-26 19:22:16,772 main.py:47] epoch 1871, training loss: 2055.02, average training loss: 2188.84, base loss: 2365.74
[INFO 2017-06-26 19:22:17,059 main.py:47] epoch 1872, training loss: 1847.50, average training loss: 2188.60, base loss: 2365.78
[INFO 2017-06-26 19:22:17,343 main.py:47] epoch 1873, training loss: 2390.71, average training loss: 2189.02, base loss: 2366.42
[INFO 2017-06-26 19:22:17,627 main.py:47] epoch 1874, training loss: 2166.61, average training loss: 2188.74, base loss: 2366.15
[INFO 2017-06-26 19:22:17,915 main.py:47] epoch 1875, training loss: 2117.10, average training loss: 2189.01, base loss: 2366.57
[INFO 2017-06-26 19:22:18,199 main.py:47] epoch 1876, training loss: 2366.28, average training loss: 2188.76, base loss: 2366.31
[INFO 2017-06-26 19:22:18,486 main.py:47] epoch 1877, training loss: 2398.63, average training loss: 2188.55, base loss: 2366.13
[INFO 2017-06-26 19:22:18,770 main.py:47] epoch 1878, training loss: 1942.39, average training loss: 2188.19, base loss: 2365.74
[INFO 2017-06-26 19:22:19,059 main.py:47] epoch 1879, training loss: 2074.05, average training loss: 2188.00, base loss: 2365.64
[INFO 2017-06-26 19:22:19,346 main.py:47] epoch 1880, training loss: 2329.43, average training loss: 2188.09, base loss: 2365.77
[INFO 2017-06-26 19:22:19,633 main.py:47] epoch 1881, training loss: 2632.77, average training loss: 2188.24, base loss: 2366.04
[INFO 2017-06-26 19:22:19,918 main.py:47] epoch 1882, training loss: 2228.71, average training loss: 2188.13, base loss: 2366.10
[INFO 2017-06-26 19:22:20,202 main.py:47] epoch 1883, training loss: 2285.06, average training loss: 2188.33, base loss: 2366.45
[INFO 2017-06-26 19:22:20,487 main.py:47] epoch 1884, training loss: 2242.33, average training loss: 2188.21, base loss: 2366.37
[INFO 2017-06-26 19:22:20,775 main.py:47] epoch 1885, training loss: 2071.68, average training loss: 2188.03, base loss: 2366.06
[INFO 2017-06-26 19:22:21,056 main.py:47] epoch 1886, training loss: 2329.09, average training loss: 2187.73, base loss: 2366.16
[INFO 2017-06-26 19:22:21,340 main.py:47] epoch 1887, training loss: 2269.15, average training loss: 2187.42, base loss: 2366.12
[INFO 2017-06-26 19:22:21,625 main.py:47] epoch 1888, training loss: 2307.25, average training loss: 2187.45, base loss: 2366.09
[INFO 2017-06-26 19:22:21,913 main.py:47] epoch 1889, training loss: 2274.45, average training loss: 2187.66, base loss: 2366.48
[INFO 2017-06-26 19:22:22,203 main.py:47] epoch 1890, training loss: 1954.60, average training loss: 2187.59, base loss: 2366.34
[INFO 2017-06-26 19:22:22,487 main.py:47] epoch 1891, training loss: 2264.54, average training loss: 2188.16, base loss: 2367.06
[INFO 2017-06-26 19:22:22,772 main.py:47] epoch 1892, training loss: 2214.19, average training loss: 2188.13, base loss: 2366.96
[INFO 2017-06-26 19:22:23,057 main.py:47] epoch 1893, training loss: 2552.74, average training loss: 2188.69, base loss: 2367.65
[INFO 2017-06-26 19:22:23,338 main.py:47] epoch 1894, training loss: 2247.62, average training loss: 2188.90, base loss: 2368.07
[INFO 2017-06-26 19:22:23,625 main.py:47] epoch 1895, training loss: 2009.16, average training loss: 2188.52, base loss: 2367.82
[INFO 2017-06-26 19:22:23,913 main.py:47] epoch 1896, training loss: 2017.34, average training loss: 2188.58, base loss: 2367.99
[INFO 2017-06-26 19:22:24,201 main.py:47] epoch 1897, training loss: 1898.95, average training loss: 2188.43, base loss: 2367.93
[INFO 2017-06-26 19:22:24,489 main.py:47] epoch 1898, training loss: 2226.66, average training loss: 2188.61, base loss: 2368.12
[INFO 2017-06-26 19:22:24,776 main.py:47] epoch 1899, training loss: 1841.09, average training loss: 2188.65, base loss: 2368.29
[INFO 2017-06-26 19:22:24,776 main.py:49] epoch 1899, testing
[INFO 2017-06-26 19:22:28,520 main.py:100] average testing loss: 2035.53, base loss: 2200.36
[INFO 2017-06-26 19:22:28,547 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:22:28,560 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:22:28,844 main.py:47] epoch 1900, training loss: 1955.10, average training loss: 2188.44, base loss: 2368.16
[INFO 2017-06-26 19:22:29,129 main.py:47] epoch 1901, training loss: 2039.26, average training loss: 2188.46, base loss: 2368.04
[INFO 2017-06-26 19:22:29,417 main.py:47] epoch 1902, training loss: 1916.54, average training loss: 2187.78, base loss: 2367.38
[INFO 2017-06-26 19:22:29,704 main.py:47] epoch 1903, training loss: 2161.83, average training loss: 2187.51, base loss: 2367.20
[INFO 2017-06-26 19:22:29,984 main.py:47] epoch 1904, training loss: 2530.88, average training loss: 2188.08, base loss: 2367.94
[INFO 2017-06-26 19:22:30,270 main.py:47] epoch 1905, training loss: 2424.94, average training loss: 2188.78, base loss: 2368.88
[INFO 2017-06-26 19:22:30,557 main.py:47] epoch 1906, training loss: 1807.91, average training loss: 2188.37, base loss: 2368.48
[INFO 2017-06-26 19:22:30,841 main.py:47] epoch 1907, training loss: 2264.63, average training loss: 2188.72, base loss: 2369.05
[INFO 2017-06-26 19:22:31,129 main.py:47] epoch 1908, training loss: 2049.28, average training loss: 2188.47, base loss: 2368.71
[INFO 2017-06-26 19:22:31,417 main.py:47] epoch 1909, training loss: 2085.52, average training loss: 2188.36, base loss: 2368.70
[INFO 2017-06-26 19:22:31,703 main.py:47] epoch 1910, training loss: 2269.78, average training loss: 2188.73, base loss: 2369.28
[INFO 2017-06-26 19:22:31,988 main.py:47] epoch 1911, training loss: 2122.18, average training loss: 2188.48, base loss: 2369.11
[INFO 2017-06-26 19:22:32,272 main.py:47] epoch 1912, training loss: 1900.05, average training loss: 2188.25, base loss: 2368.96
[INFO 2017-06-26 19:22:32,560 main.py:47] epoch 1913, training loss: 2233.75, average training loss: 2188.17, base loss: 2368.93
[INFO 2017-06-26 19:22:32,845 main.py:47] epoch 1914, training loss: 1870.13, average training loss: 2187.60, base loss: 2368.28
[INFO 2017-06-26 19:22:33,125 main.py:47] epoch 1915, training loss: 2065.94, average training loss: 2187.54, base loss: 2368.34
[INFO 2017-06-26 19:22:33,409 main.py:47] epoch 1916, training loss: 2048.51, average training loss: 2187.29, base loss: 2368.14
[INFO 2017-06-26 19:22:33,693 main.py:47] epoch 1917, training loss: 2360.99, average training loss: 2187.32, base loss: 2368.26
[INFO 2017-06-26 19:22:33,977 main.py:47] epoch 1918, training loss: 1939.41, average training loss: 2187.27, base loss: 2368.28
[INFO 2017-06-26 19:22:34,260 main.py:47] epoch 1919, training loss: 2279.07, average training loss: 2187.18, base loss: 2368.38
[INFO 2017-06-26 19:22:34,541 main.py:47] epoch 1920, training loss: 2305.64, average training loss: 2187.22, base loss: 2368.61
[INFO 2017-06-26 19:22:34,828 main.py:47] epoch 1921, training loss: 2179.35, average training loss: 2187.24, base loss: 2368.73
[INFO 2017-06-26 19:22:35,110 main.py:47] epoch 1922, training loss: 2441.66, average training loss: 2187.14, base loss: 2368.78
[INFO 2017-06-26 19:22:35,395 main.py:47] epoch 1923, training loss: 2120.16, average training loss: 2186.49, base loss: 2368.07
[INFO 2017-06-26 19:22:35,678 main.py:47] epoch 1924, training loss: 1927.97, average training loss: 2186.36, base loss: 2368.03
[INFO 2017-06-26 19:22:35,960 main.py:47] epoch 1925, training loss: 2362.96, average training loss: 2186.51, base loss: 2368.19
[INFO 2017-06-26 19:22:36,239 main.py:47] epoch 1926, training loss: 1918.06, average training loss: 2185.79, base loss: 2367.41
[INFO 2017-06-26 19:22:36,525 main.py:47] epoch 1927, training loss: 2347.96, average training loss: 2186.20, base loss: 2368.12
[INFO 2017-06-26 19:22:36,812 main.py:47] epoch 1928, training loss: 2040.35, average training loss: 2185.85, base loss: 2367.84
[INFO 2017-06-26 19:22:37,099 main.py:47] epoch 1929, training loss: 2054.49, average training loss: 2186.00, base loss: 2368.14
[INFO 2017-06-26 19:22:37,386 main.py:47] epoch 1930, training loss: 2048.77, average training loss: 2185.79, base loss: 2367.92
[INFO 2017-06-26 19:22:37,672 main.py:47] epoch 1931, training loss: 2065.99, average training loss: 2185.81, base loss: 2367.89
[INFO 2017-06-26 19:22:37,953 main.py:47] epoch 1932, training loss: 2191.12, average training loss: 2185.78, base loss: 2367.96
[INFO 2017-06-26 19:22:38,233 main.py:47] epoch 1933, training loss: 2042.36, average training loss: 2185.78, base loss: 2367.98
[INFO 2017-06-26 19:22:38,522 main.py:47] epoch 1934, training loss: 2311.17, average training loss: 2186.19, base loss: 2368.60
[INFO 2017-06-26 19:22:38,804 main.py:47] epoch 1935, training loss: 2017.75, average training loss: 2185.66, base loss: 2368.11
[INFO 2017-06-26 19:22:39,085 main.py:47] epoch 1936, training loss: 2124.16, average training loss: 2185.74, base loss: 2368.22
[INFO 2017-06-26 19:22:39,371 main.py:47] epoch 1937, training loss: 2338.73, average training loss: 2185.89, base loss: 2368.61
[INFO 2017-06-26 19:22:39,655 main.py:47] epoch 1938, training loss: 2016.66, average training loss: 2185.85, base loss: 2368.63
[INFO 2017-06-26 19:22:39,943 main.py:47] epoch 1939, training loss: 1899.15, average training loss: 2185.73, base loss: 2368.40
[INFO 2017-06-26 19:22:40,230 main.py:47] epoch 1940, training loss: 2113.07, average training loss: 2184.65, base loss: 2367.28
[INFO 2017-06-26 19:22:40,518 main.py:47] epoch 1941, training loss: 2116.38, average training loss: 2184.30, base loss: 2366.66
[INFO 2017-06-26 19:22:40,806 main.py:47] epoch 1942, training loss: 1778.43, average training loss: 2183.99, base loss: 2366.40
[INFO 2017-06-26 19:22:41,089 main.py:47] epoch 1943, training loss: 2074.89, average training loss: 2184.22, base loss: 2366.85
[INFO 2017-06-26 19:22:41,376 main.py:47] epoch 1944, training loss: 1697.05, average training loss: 2183.49, base loss: 2366.06
[INFO 2017-06-26 19:22:41,664 main.py:47] epoch 1945, training loss: 2279.83, average training loss: 2183.14, base loss: 2365.82
[INFO 2017-06-26 19:22:41,947 main.py:47] epoch 1946, training loss: 2006.92, average training loss: 2183.29, base loss: 2366.11
[INFO 2017-06-26 19:22:42,235 main.py:47] epoch 1947, training loss: 1544.70, average training loss: 2182.68, base loss: 2365.41
[INFO 2017-06-26 19:22:42,544 main.py:47] epoch 1948, training loss: 2109.93, average training loss: 2182.92, base loss: 2365.83
[INFO 2017-06-26 19:22:42,828 main.py:47] epoch 1949, training loss: 2291.93, average training loss: 2182.65, base loss: 2365.54
[INFO 2017-06-26 19:22:43,109 main.py:47] epoch 1950, training loss: 2167.50, average training loss: 2182.24, base loss: 2365.27
[INFO 2017-06-26 19:22:43,396 main.py:47] epoch 1951, training loss: 1775.74, average training loss: 2181.49, base loss: 2364.37
[INFO 2017-06-26 19:22:43,683 main.py:47] epoch 1952, training loss: 2206.27, average training loss: 2181.68, base loss: 2364.71
[INFO 2017-06-26 19:22:43,967 main.py:47] epoch 1953, training loss: 2376.42, average training loss: 2181.95, base loss: 2365.19
[INFO 2017-06-26 19:22:44,254 main.py:47] epoch 1954, training loss: 2125.28, average training loss: 2181.35, base loss: 2364.62
[INFO 2017-06-26 19:22:44,540 main.py:47] epoch 1955, training loss: 2278.83, average training loss: 2181.72, base loss: 2364.93
[INFO 2017-06-26 19:22:44,824 main.py:47] epoch 1956, training loss: 2054.13, average training loss: 2181.62, base loss: 2365.09
[INFO 2017-06-26 19:22:45,110 main.py:47] epoch 1957, training loss: 2301.92, average training loss: 2181.83, base loss: 2365.28
[INFO 2017-06-26 19:22:45,394 main.py:47] epoch 1958, training loss: 2214.70, average training loss: 2181.81, base loss: 2365.29
[INFO 2017-06-26 19:22:45,676 main.py:47] epoch 1959, training loss: 2113.94, average training loss: 2181.70, base loss: 2365.36
[INFO 2017-06-26 19:22:45,959 main.py:47] epoch 1960, training loss: 1886.15, average training loss: 2181.73, base loss: 2365.52
[INFO 2017-06-26 19:22:46,247 main.py:47] epoch 1961, training loss: 2469.32, average training loss: 2181.99, base loss: 2365.84
[INFO 2017-06-26 19:22:46,534 main.py:47] epoch 1962, training loss: 2154.43, average training loss: 2181.77, base loss: 2365.69
[INFO 2017-06-26 19:22:46,819 main.py:47] epoch 1963, training loss: 2219.88, average training loss: 2181.70, base loss: 2365.64
[INFO 2017-06-26 19:22:47,104 main.py:47] epoch 1964, training loss: 2078.58, average training loss: 2181.60, base loss: 2365.62
[INFO 2017-06-26 19:22:47,390 main.py:47] epoch 1965, training loss: 2058.72, average training loss: 2181.31, base loss: 2365.23
[INFO 2017-06-26 19:22:47,674 main.py:47] epoch 1966, training loss: 1818.56, average training loss: 2180.63, base loss: 2364.57
[INFO 2017-06-26 19:22:47,969 main.py:47] epoch 1967, training loss: 2179.49, average training loss: 2180.56, base loss: 2364.70
[INFO 2017-06-26 19:22:48,250 main.py:47] epoch 1968, training loss: 1888.67, average training loss: 2180.37, base loss: 2364.67
[INFO 2017-06-26 19:22:48,539 main.py:47] epoch 1969, training loss: 2364.41, average training loss: 2180.37, base loss: 2364.79
[INFO 2017-06-26 19:22:48,821 main.py:47] epoch 1970, training loss: 2556.35, average training loss: 2180.59, base loss: 2365.33
[INFO 2017-06-26 19:22:49,103 main.py:47] epoch 1971, training loss: 1912.92, average training loss: 2180.18, base loss: 2364.93
[INFO 2017-06-26 19:22:49,386 main.py:47] epoch 1972, training loss: 2050.19, average training loss: 2180.06, base loss: 2364.91
[INFO 2017-06-26 19:22:49,670 main.py:47] epoch 1973, training loss: 2338.25, average training loss: 2180.47, base loss: 2365.48
[INFO 2017-06-26 19:22:49,957 main.py:47] epoch 1974, training loss: 1887.18, average training loss: 2180.13, base loss: 2365.13
[INFO 2017-06-26 19:22:50,246 main.py:47] epoch 1975, training loss: 2571.03, average training loss: 2180.45, base loss: 2365.51
[INFO 2017-06-26 19:22:50,533 main.py:47] epoch 1976, training loss: 2359.64, average training loss: 2180.68, base loss: 2365.91
[INFO 2017-06-26 19:22:50,816 main.py:47] epoch 1977, training loss: 2246.35, average training loss: 2180.63, base loss: 2366.02
[INFO 2017-06-26 19:22:51,101 main.py:47] epoch 1978, training loss: 1976.89, average training loss: 2180.22, base loss: 2365.52
[INFO 2017-06-26 19:22:51,387 main.py:47] epoch 1979, training loss: 2315.54, average training loss: 2180.31, base loss: 2365.73
[INFO 2017-06-26 19:22:51,675 main.py:47] epoch 1980, training loss: 2012.51, average training loss: 2180.27, base loss: 2365.76
[INFO 2017-06-26 19:22:51,959 main.py:47] epoch 1981, training loss: 2435.22, average training loss: 2179.73, base loss: 2365.37
[INFO 2017-06-26 19:22:52,244 main.py:47] epoch 1982, training loss: 2116.47, average training loss: 2179.48, base loss: 2365.33
[INFO 2017-06-26 19:22:52,528 main.py:47] epoch 1983, training loss: 1901.47, average training loss: 2179.40, base loss: 2365.20
[INFO 2017-06-26 19:22:52,811 main.py:47] epoch 1984, training loss: 1989.27, average training loss: 2179.15, base loss: 2364.72
[INFO 2017-06-26 19:22:53,096 main.py:47] epoch 1985, training loss: 2298.28, average training loss: 2179.54, base loss: 2365.25
[INFO 2017-06-26 19:22:53,382 main.py:47] epoch 1986, training loss: 2254.71, average training loss: 2179.66, base loss: 2365.46
[INFO 2017-06-26 19:22:53,668 main.py:47] epoch 1987, training loss: 1698.60, average training loss: 2179.02, base loss: 2364.63
[INFO 2017-06-26 19:22:53,956 main.py:47] epoch 1988, training loss: 2384.46, average training loss: 2179.35, base loss: 2365.15
[INFO 2017-06-26 19:22:54,241 main.py:47] epoch 1989, training loss: 1912.77, average training loss: 2179.02, base loss: 2364.84
[INFO 2017-06-26 19:22:54,526 main.py:47] epoch 1990, training loss: 1930.57, average training loss: 2179.06, base loss: 2364.85
[INFO 2017-06-26 19:22:54,806 main.py:47] epoch 1991, training loss: 2223.03, average training loss: 2179.00, base loss: 2364.88
[INFO 2017-06-26 19:22:55,094 main.py:47] epoch 1992, training loss: 1995.45, average training loss: 2178.95, base loss: 2364.89
[INFO 2017-06-26 19:22:55,377 main.py:47] epoch 1993, training loss: 2099.80, average training loss: 2178.63, base loss: 2364.68
[INFO 2017-06-26 19:22:55,662 main.py:47] epoch 1994, training loss: 2025.59, average training loss: 2177.98, base loss: 2363.88
[INFO 2017-06-26 19:22:55,950 main.py:47] epoch 1995, training loss: 2296.05, average training loss: 2177.89, base loss: 2363.79
[INFO 2017-06-26 19:22:56,237 main.py:47] epoch 1996, training loss: 2071.17, average training loss: 2177.56, base loss: 2363.40
[INFO 2017-06-26 19:22:56,522 main.py:47] epoch 1997, training loss: 2223.44, average training loss: 2177.42, base loss: 2363.13
[INFO 2017-06-26 19:22:56,805 main.py:47] epoch 1998, training loss: 2347.18, average training loss: 2177.62, base loss: 2363.49
[INFO 2017-06-26 19:22:57,090 main.py:47] epoch 1999, training loss: 2684.53, average training loss: 2178.14, base loss: 2364.21
[INFO 2017-06-26 19:22:57,090 main.py:49] epoch 1999, testing
[INFO 2017-06-26 19:23:00,809 main.py:100] average testing loss: 2213.41, base loss: 2395.84
[INFO 2017-06-26 19:23:00,836 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:23:01,120 main.py:47] epoch 2000, training loss: 2092.10, average training loss: 2178.01, base loss: 2364.27
[INFO 2017-06-26 19:23:01,410 main.py:47] epoch 2001, training loss: 2119.91, average training loss: 2177.53, base loss: 2363.62
[INFO 2017-06-26 19:23:01,694 main.py:47] epoch 2002, training loss: 2223.72, average training loss: 2177.60, base loss: 2363.67
[INFO 2017-06-26 19:23:01,978 main.py:47] epoch 2003, training loss: 2104.78, average training loss: 2177.96, base loss: 2364.17
[INFO 2017-06-26 19:23:02,262 main.py:47] epoch 2004, training loss: 2493.10, average training loss: 2177.83, base loss: 2364.14
[INFO 2017-06-26 19:23:02,547 main.py:47] epoch 2005, training loss: 2206.20, average training loss: 2177.85, base loss: 2364.20
[INFO 2017-06-26 19:23:02,832 main.py:47] epoch 2006, training loss: 2086.56, average training loss: 2177.95, base loss: 2364.42
[INFO 2017-06-26 19:23:03,117 main.py:47] epoch 2007, training loss: 2054.26, average training loss: 2178.08, base loss: 2364.65
[INFO 2017-06-26 19:23:03,397 main.py:47] epoch 2008, training loss: 2630.64, average training loss: 2178.43, base loss: 2365.19
[INFO 2017-06-26 19:23:03,684 main.py:47] epoch 2009, training loss: 2059.38, average training loss: 2178.53, base loss: 2365.45
[INFO 2017-06-26 19:23:03,967 main.py:47] epoch 2010, training loss: 2467.14, average training loss: 2179.14, base loss: 2366.17
[INFO 2017-06-26 19:23:04,251 main.py:47] epoch 2011, training loss: 2505.35, average training loss: 2179.59, base loss: 2366.76
[INFO 2017-06-26 19:23:04,534 main.py:47] epoch 2012, training loss: 2275.46, average training loss: 2179.27, base loss: 2366.59
[INFO 2017-06-26 19:23:04,818 main.py:47] epoch 2013, training loss: 2673.27, average training loss: 2179.66, base loss: 2367.29
[INFO 2017-06-26 19:23:05,103 main.py:47] epoch 2014, training loss: 2334.05, average training loss: 2179.27, base loss: 2367.02
[INFO 2017-06-26 19:23:05,384 main.py:47] epoch 2015, training loss: 2434.28, average training loss: 2179.45, base loss: 2367.46
[INFO 2017-06-26 19:23:05,668 main.py:47] epoch 2016, training loss: 1995.00, average training loss: 2178.35, base loss: 2366.15
[INFO 2017-06-26 19:23:05,953 main.py:47] epoch 2017, training loss: 2224.74, average training loss: 2178.03, base loss: 2365.91
[INFO 2017-06-26 19:23:06,238 main.py:47] epoch 2018, training loss: 2078.49, average training loss: 2177.97, base loss: 2365.94
[INFO 2017-06-26 19:23:06,522 main.py:47] epoch 2019, training loss: 2288.45, average training loss: 2177.97, base loss: 2366.04
[INFO 2017-06-26 19:23:06,806 main.py:47] epoch 2020, training loss: 2264.69, average training loss: 2178.15, base loss: 2366.30
[INFO 2017-06-26 19:23:07,094 main.py:47] epoch 2021, training loss: 1986.90, average training loss: 2177.55, base loss: 2365.91
[INFO 2017-06-26 19:23:07,378 main.py:47] epoch 2022, training loss: 2302.39, average training loss: 2177.51, base loss: 2365.96
[INFO 2017-06-26 19:23:07,658 main.py:47] epoch 2023, training loss: 1830.66, average training loss: 2177.62, base loss: 2366.00
[INFO 2017-06-26 19:23:07,946 main.py:47] epoch 2024, training loss: 2384.59, average training loss: 2178.03, base loss: 2366.53
[INFO 2017-06-26 19:23:08,231 main.py:47] epoch 2025, training loss: 2069.31, average training loss: 2177.87, base loss: 2366.54
[INFO 2017-06-26 19:23:08,516 main.py:47] epoch 2026, training loss: 1938.92, average training loss: 2176.99, base loss: 2365.65
[INFO 2017-06-26 19:23:08,801 main.py:47] epoch 2027, training loss: 1946.89, average training loss: 2176.86, base loss: 2365.60
[INFO 2017-06-26 19:23:09,087 main.py:47] epoch 2028, training loss: 2708.48, average training loss: 2176.99, base loss: 2365.55
[INFO 2017-06-26 19:23:09,372 main.py:47] epoch 2029, training loss: 2103.22, average training loss: 2176.94, base loss: 2365.69
[INFO 2017-06-26 19:23:09,652 main.py:47] epoch 2030, training loss: 2148.49, average training loss: 2177.03, base loss: 2365.70
[INFO 2017-06-26 19:23:09,933 main.py:47] epoch 2031, training loss: 2042.42, average training loss: 2176.59, base loss: 2365.29
[INFO 2017-06-26 19:23:10,218 main.py:47] epoch 2032, training loss: 2294.32, average training loss: 2176.31, base loss: 2365.05
[INFO 2017-06-26 19:23:10,501 main.py:47] epoch 2033, training loss: 2063.20, average training loss: 2176.45, base loss: 2365.20
[INFO 2017-06-26 19:23:10,788 main.py:47] epoch 2034, training loss: 2342.63, average training loss: 2176.92, base loss: 2365.87
[INFO 2017-06-26 19:23:11,073 main.py:47] epoch 2035, training loss: 1988.27, average training loss: 2176.75, base loss: 2365.70
[INFO 2017-06-26 19:23:11,360 main.py:47] epoch 2036, training loss: 1833.85, average training loss: 2176.32, base loss: 2365.29
[INFO 2017-06-26 19:23:11,648 main.py:47] epoch 2037, training loss: 2190.10, average training loss: 2176.43, base loss: 2365.60
[INFO 2017-06-26 19:23:11,936 main.py:47] epoch 2038, training loss: 2108.80, average training loss: 2176.43, base loss: 2365.54
[INFO 2017-06-26 19:23:12,218 main.py:47] epoch 2039, training loss: 1948.02, average training loss: 2176.14, base loss: 2365.47
[INFO 2017-06-26 19:23:12,505 main.py:47] epoch 2040, training loss: 1885.53, average training loss: 2175.48, base loss: 2364.63
[INFO 2017-06-26 19:23:12,792 main.py:47] epoch 2041, training loss: 2153.78, average training loss: 2175.50, base loss: 2364.77
[INFO 2017-06-26 19:23:13,075 main.py:47] epoch 2042, training loss: 2231.35, average training loss: 2175.68, base loss: 2365.10
[INFO 2017-06-26 19:23:13,362 main.py:47] epoch 2043, training loss: 2257.50, average training loss: 2175.71, base loss: 2365.30
[INFO 2017-06-26 19:23:13,646 main.py:47] epoch 2044, training loss: 2153.26, average training loss: 2175.40, base loss: 2365.10
[INFO 2017-06-26 19:23:13,933 main.py:47] epoch 2045, training loss: 2258.66, average training loss: 2175.36, base loss: 2365.15
[INFO 2017-06-26 19:23:14,216 main.py:47] epoch 2046, training loss: 2273.75, average training loss: 2175.38, base loss: 2365.22
[INFO 2017-06-26 19:23:14,500 main.py:47] epoch 2047, training loss: 2147.04, average training loss: 2175.13, base loss: 2365.07
[INFO 2017-06-26 19:23:14,782 main.py:47] epoch 2048, training loss: 2204.02, average training loss: 2175.31, base loss: 2365.28
[INFO 2017-06-26 19:23:15,065 main.py:47] epoch 2049, training loss: 2092.46, average training loss: 2175.52, base loss: 2365.45
[INFO 2017-06-26 19:23:15,352 main.py:47] epoch 2050, training loss: 2083.36, average training loss: 2175.24, base loss: 2365.12
[INFO 2017-06-26 19:23:15,637 main.py:47] epoch 2051, training loss: 1848.79, average training loss: 2174.61, base loss: 2364.53
[INFO 2017-06-26 19:23:15,922 main.py:47] epoch 2052, training loss: 2087.41, average training loss: 2174.15, base loss: 2363.95
[INFO 2017-06-26 19:23:16,210 main.py:47] epoch 2053, training loss: 2179.84, average training loss: 2174.04, base loss: 2363.92
[INFO 2017-06-26 19:23:16,498 main.py:47] epoch 2054, training loss: 2080.84, average training loss: 2173.77, base loss: 2363.62
[INFO 2017-06-26 19:23:16,781 main.py:47] epoch 2055, training loss: 1888.95, average training loss: 2173.55, base loss: 2363.39
[INFO 2017-06-26 19:23:17,065 main.py:47] epoch 2056, training loss: 2276.35, average training loss: 2173.77, base loss: 2363.71
[INFO 2017-06-26 19:23:17,348 main.py:47] epoch 2057, training loss: 2385.42, average training loss: 2174.11, base loss: 2364.25
[INFO 2017-06-26 19:23:17,632 main.py:47] epoch 2058, training loss: 2245.01, average training loss: 2173.88, base loss: 2364.19
[INFO 2017-06-26 19:23:17,918 main.py:47] epoch 2059, training loss: 2063.80, average training loss: 2173.64, base loss: 2364.07
[INFO 2017-06-26 19:23:18,213 main.py:47] epoch 2060, training loss: 2079.02, average training loss: 2173.91, base loss: 2364.20
[INFO 2017-06-26 19:23:18,498 main.py:47] epoch 2061, training loss: 2428.84, average training loss: 2173.88, base loss: 2364.24
[INFO 2017-06-26 19:23:18,784 main.py:47] epoch 2062, training loss: 2019.59, average training loss: 2173.67, base loss: 2363.95
[INFO 2017-06-26 19:23:19,069 main.py:47] epoch 2063, training loss: 2541.44, average training loss: 2174.01, base loss: 2364.54
[INFO 2017-06-26 19:23:19,357 main.py:47] epoch 2064, training loss: 2263.03, average training loss: 2174.23, base loss: 2365.21
[INFO 2017-06-26 19:23:19,642 main.py:47] epoch 2065, training loss: 2007.90, average training loss: 2174.18, base loss: 2364.96
[INFO 2017-06-26 19:23:19,926 main.py:47] epoch 2066, training loss: 2277.00, average training loss: 2174.34, base loss: 2365.16
[INFO 2017-06-26 19:23:20,212 main.py:47] epoch 2067, training loss: 1984.89, average training loss: 2174.34, base loss: 2365.20
[INFO 2017-06-26 19:23:20,493 main.py:47] epoch 2068, training loss: 2093.62, average training loss: 2174.36, base loss: 2365.32
[INFO 2017-06-26 19:23:20,772 main.py:47] epoch 2069, training loss: 1824.90, average training loss: 2173.88, base loss: 2364.80
[INFO 2017-06-26 19:23:21,056 main.py:47] epoch 2070, training loss: 2108.57, average training loss: 2173.34, base loss: 2364.33
[INFO 2017-06-26 19:23:21,340 main.py:47] epoch 2071, training loss: 2141.02, average training loss: 2173.44, base loss: 2364.50
[INFO 2017-06-26 19:23:21,625 main.py:47] epoch 2072, training loss: 2053.61, average training loss: 2173.16, base loss: 2364.28
[INFO 2017-06-26 19:23:21,906 main.py:47] epoch 2073, training loss: 2354.61, average training loss: 2173.58, base loss: 2364.88
[INFO 2017-06-26 19:23:22,188 main.py:47] epoch 2074, training loss: 2440.43, average training loss: 2174.00, base loss: 2365.32
[INFO 2017-06-26 19:23:22,473 main.py:47] epoch 2075, training loss: 2007.36, average training loss: 2173.73, base loss: 2365.06
[INFO 2017-06-26 19:23:22,755 main.py:47] epoch 2076, training loss: 2123.23, average training loss: 2173.93, base loss: 2365.22
[INFO 2017-06-26 19:23:23,039 main.py:47] epoch 2077, training loss: 2001.15, average training loss: 2174.12, base loss: 2365.42
[INFO 2017-06-26 19:23:23,325 main.py:47] epoch 2078, training loss: 1753.19, average training loss: 2173.57, base loss: 2364.79
[INFO 2017-06-26 19:23:23,611 main.py:47] epoch 2079, training loss: 2384.51, average training loss: 2173.63, base loss: 2364.84
[INFO 2017-06-26 19:23:23,895 main.py:47] epoch 2080, training loss: 1882.08, average training loss: 2173.35, base loss: 2364.67
[INFO 2017-06-26 19:23:24,181 main.py:47] epoch 2081, training loss: 2241.87, average training loss: 2173.02, base loss: 2364.43
[INFO 2017-06-26 19:23:24,464 main.py:47] epoch 2082, training loss: 2338.46, average training loss: 2173.35, base loss: 2364.84
[INFO 2017-06-26 19:23:24,745 main.py:47] epoch 2083, training loss: 2130.02, average training loss: 2173.09, base loss: 2364.64
[INFO 2017-06-26 19:23:25,026 main.py:47] epoch 2084, training loss: 2203.78, average training loss: 2173.12, base loss: 2364.86
[INFO 2017-06-26 19:23:25,307 main.py:47] epoch 2085, training loss: 1993.13, average training loss: 2173.08, base loss: 2364.75
[INFO 2017-06-26 19:23:25,592 main.py:47] epoch 2086, training loss: 2237.92, average training loss: 2173.27, base loss: 2365.07
[INFO 2017-06-26 19:23:25,878 main.py:47] epoch 2087, training loss: 2027.48, average training loss: 2173.05, base loss: 2365.07
[INFO 2017-06-26 19:23:26,163 main.py:47] epoch 2088, training loss: 2130.26, average training loss: 2172.68, base loss: 2364.69
[INFO 2017-06-26 19:23:26,445 main.py:47] epoch 2089, training loss: 1995.23, average training loss: 2172.51, base loss: 2364.67
[INFO 2017-06-26 19:23:26,731 main.py:47] epoch 2090, training loss: 2390.16, average training loss: 2172.39, base loss: 2364.62
[INFO 2017-06-26 19:23:27,013 main.py:47] epoch 2091, training loss: 2075.20, average training loss: 2172.12, base loss: 2364.42
[INFO 2017-06-26 19:23:27,302 main.py:47] epoch 2092, training loss: 2096.53, average training loss: 2172.09, base loss: 2364.44
[INFO 2017-06-26 19:23:27,583 main.py:47] epoch 2093, training loss: 1853.89, average training loss: 2171.96, base loss: 2364.33
[INFO 2017-06-26 19:23:27,868 main.py:47] epoch 2094, training loss: 1946.37, average training loss: 2171.79, base loss: 2364.18
[INFO 2017-06-26 19:23:28,153 main.py:47] epoch 2095, training loss: 2173.48, average training loss: 2171.55, base loss: 2363.99
[INFO 2017-06-26 19:23:28,441 main.py:47] epoch 2096, training loss: 2214.33, average training loss: 2171.73, base loss: 2364.26
[INFO 2017-06-26 19:23:28,725 main.py:47] epoch 2097, training loss: 2069.39, average training loss: 2171.55, base loss: 2364.17
[INFO 2017-06-26 19:23:29,009 main.py:47] epoch 2098, training loss: 2117.36, average training loss: 2171.76, base loss: 2364.54
[INFO 2017-06-26 19:23:29,292 main.py:47] epoch 2099, training loss: 2252.44, average training loss: 2171.71, base loss: 2364.66
[INFO 2017-06-26 19:23:29,293 main.py:49] epoch 2099, testing
[INFO 2017-06-26 19:23:33,052 main.py:100] average testing loss: 2095.61, base loss: 2292.48
[INFO 2017-06-26 19:23:33,076 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:23:33,358 main.py:47] epoch 2100, training loss: 2199.16, average training loss: 2171.79, base loss: 2364.94
[INFO 2017-06-26 19:23:33,641 main.py:47] epoch 2101, training loss: 2167.64, average training loss: 2172.11, base loss: 2365.30
[INFO 2017-06-26 19:23:33,929 main.py:47] epoch 2102, training loss: 2335.63, average training loss: 2172.42, base loss: 2365.73
[INFO 2017-06-26 19:23:34,216 main.py:47] epoch 2103, training loss: 2495.89, average training loss: 2172.72, base loss: 2366.11
[INFO 2017-06-26 19:23:34,497 main.py:47] epoch 2104, training loss: 2069.15, average training loss: 2173.12, base loss: 2366.59
[INFO 2017-06-26 19:23:34,781 main.py:47] epoch 2105, training loss: 2040.02, average training loss: 2172.92, base loss: 2366.48
[INFO 2017-06-26 19:23:35,069 main.py:47] epoch 2106, training loss: 2374.62, average training loss: 2172.76, base loss: 2366.27
[INFO 2017-06-26 19:23:35,353 main.py:47] epoch 2107, training loss: 2131.44, average training loss: 2172.40, base loss: 2365.92
[INFO 2017-06-26 19:23:35,636 main.py:47] epoch 2108, training loss: 2234.91, average training loss: 2172.54, base loss: 2366.02
[INFO 2017-06-26 19:23:35,921 main.py:47] epoch 2109, training loss: 2033.99, average training loss: 2172.65, base loss: 2366.28
[INFO 2017-06-26 19:23:36,204 main.py:47] epoch 2110, training loss: 1986.19, average training loss: 2172.25, base loss: 2365.72
[INFO 2017-06-26 19:23:36,488 main.py:47] epoch 2111, training loss: 2165.46, average training loss: 2172.13, base loss: 2365.67
[INFO 2017-06-26 19:23:36,771 main.py:47] epoch 2112, training loss: 2635.14, average training loss: 2172.68, base loss: 2366.42
[INFO 2017-06-26 19:23:37,055 main.py:47] epoch 2113, training loss: 2497.19, average training loss: 2172.96, base loss: 2366.96
[INFO 2017-06-26 19:23:37,338 main.py:47] epoch 2114, training loss: 2100.89, average training loss: 2172.90, base loss: 2366.66
[INFO 2017-06-26 19:23:37,621 main.py:47] epoch 2115, training loss: 2115.74, average training loss: 2172.62, base loss: 2366.47
[INFO 2017-06-26 19:23:37,905 main.py:47] epoch 2116, training loss: 2281.21, average training loss: 2172.51, base loss: 2366.53
[INFO 2017-06-26 19:23:38,191 main.py:47] epoch 2117, training loss: 2607.43, average training loss: 2172.82, base loss: 2367.11
[INFO 2017-06-26 19:23:38,478 main.py:47] epoch 2118, training loss: 1777.05, average training loss: 2172.46, base loss: 2366.67
[INFO 2017-06-26 19:23:38,763 main.py:47] epoch 2119, training loss: 1916.87, average training loss: 2172.52, base loss: 2367.18
[INFO 2017-06-26 19:23:39,050 main.py:47] epoch 2120, training loss: 1744.33, average training loss: 2171.85, base loss: 2366.30
[INFO 2017-06-26 19:23:39,338 main.py:47] epoch 2121, training loss: 2167.90, average training loss: 2172.24, base loss: 2366.85
[INFO 2017-06-26 19:23:39,628 main.py:47] epoch 2122, training loss: 2322.18, average training loss: 2172.79, base loss: 2367.30
[INFO 2017-06-26 19:23:39,914 main.py:47] epoch 2123, training loss: 2033.40, average training loss: 2172.64, base loss: 2367.17
[INFO 2017-06-26 19:23:40,198 main.py:47] epoch 2124, training loss: 1959.35, average training loss: 2172.36, base loss: 2366.88
[INFO 2017-06-26 19:23:40,484 main.py:47] epoch 2125, training loss: 2029.93, average training loss: 2172.10, base loss: 2366.73
[INFO 2017-06-26 19:23:40,772 main.py:47] epoch 2126, training loss: 1921.25, average training loss: 2171.58, base loss: 2366.19
[INFO 2017-06-26 19:23:41,058 main.py:47] epoch 2127, training loss: 2006.58, average training loss: 2171.51, base loss: 2366.19
[INFO 2017-06-26 19:23:41,343 main.py:47] epoch 2128, training loss: 2258.40, average training loss: 2172.19, base loss: 2367.04
[INFO 2017-06-26 19:23:41,626 main.py:47] epoch 2129, training loss: 2394.14, average training loss: 2172.55, base loss: 2367.44
[INFO 2017-06-26 19:23:41,912 main.py:47] epoch 2130, training loss: 2119.88, average training loss: 2172.50, base loss: 2367.34
[INFO 2017-06-26 19:23:42,200 main.py:47] epoch 2131, training loss: 1978.19, average training loss: 2172.15, base loss: 2366.87
[INFO 2017-06-26 19:23:42,487 main.py:47] epoch 2132, training loss: 2049.41, average training loss: 2171.98, base loss: 2366.57
[INFO 2017-06-26 19:23:42,769 main.py:47] epoch 2133, training loss: 2089.61, average training loss: 2171.49, base loss: 2366.05
[INFO 2017-06-26 19:23:43,050 main.py:47] epoch 2134, training loss: 2271.26, average training loss: 2171.61, base loss: 2366.15
[INFO 2017-06-26 19:23:43,330 main.py:47] epoch 2135, training loss: 2167.77, average training loss: 2171.74, base loss: 2366.34
[INFO 2017-06-26 19:23:43,614 main.py:47] epoch 2136, training loss: 2318.45, average training loss: 2171.50, base loss: 2366.03
[INFO 2017-06-26 19:23:43,898 main.py:47] epoch 2137, training loss: 1745.80, average training loss: 2171.16, base loss: 2365.75
[INFO 2017-06-26 19:23:44,182 main.py:47] epoch 2138, training loss: 2276.91, average training loss: 2171.41, base loss: 2366.06
[INFO 2017-06-26 19:23:44,471 main.py:47] epoch 2139, training loss: 1991.93, average training loss: 2170.91, base loss: 2365.53
[INFO 2017-06-26 19:23:44,751 main.py:47] epoch 2140, training loss: 2292.93, average training loss: 2171.27, base loss: 2365.96
[INFO 2017-06-26 19:23:45,035 main.py:47] epoch 2141, training loss: 1797.44, average training loss: 2170.58, base loss: 2365.35
[INFO 2017-06-26 19:23:45,315 main.py:47] epoch 2142, training loss: 2184.23, average training loss: 2170.50, base loss: 2365.37
[INFO 2017-06-26 19:23:45,597 main.py:47] epoch 2143, training loss: 1997.13, average training loss: 2170.14, base loss: 2364.92
[INFO 2017-06-26 19:23:45,878 main.py:47] epoch 2144, training loss: 2836.10, average training loss: 2170.96, base loss: 2365.91
[INFO 2017-06-26 19:23:46,162 main.py:47] epoch 2145, training loss: 2416.08, average training loss: 2171.28, base loss: 2366.17
[INFO 2017-06-26 19:23:46,447 main.py:47] epoch 2146, training loss: 1884.43, average training loss: 2171.11, base loss: 2366.03
[INFO 2017-06-26 19:23:46,731 main.py:47] epoch 2147, training loss: 1767.85, average training loss: 2170.49, base loss: 2365.39
[INFO 2017-06-26 19:23:47,015 main.py:47] epoch 2148, training loss: 1898.14, average training loss: 2170.59, base loss: 2365.63
[INFO 2017-06-26 19:23:47,299 main.py:47] epoch 2149, training loss: 1976.03, average training loss: 2170.33, base loss: 2365.41
[INFO 2017-06-26 19:23:47,585 main.py:47] epoch 2150, training loss: 1986.74, average training loss: 2170.47, base loss: 2365.66
[INFO 2017-06-26 19:23:47,865 main.py:47] epoch 2151, training loss: 2289.50, average training loss: 2170.71, base loss: 2365.96
[INFO 2017-06-26 19:23:48,153 main.py:47] epoch 2152, training loss: 2179.31, average training loss: 2170.75, base loss: 2366.11
[INFO 2017-06-26 19:23:48,435 main.py:47] epoch 2153, training loss: 2004.25, average training loss: 2170.56, base loss: 2365.89
[INFO 2017-06-26 19:23:48,721 main.py:47] epoch 2154, training loss: 1799.36, average training loss: 2170.03, base loss: 2365.51
[INFO 2017-06-26 19:23:49,005 main.py:47] epoch 2155, training loss: 2151.09, average training loss: 2170.06, base loss: 2365.66
[INFO 2017-06-26 19:23:49,290 main.py:47] epoch 2156, training loss: 2003.41, average training loss: 2169.85, base loss: 2365.32
[INFO 2017-06-26 19:23:49,571 main.py:47] epoch 2157, training loss: 2193.11, average training loss: 2169.68, base loss: 2365.20
[INFO 2017-06-26 19:23:49,855 main.py:47] epoch 2158, training loss: 2114.96, average training loss: 2169.71, base loss: 2365.24
[INFO 2017-06-26 19:23:50,142 main.py:47] epoch 2159, training loss: 1912.28, average training loss: 2169.45, base loss: 2365.05
[INFO 2017-06-26 19:23:50,429 main.py:47] epoch 2160, training loss: 1877.87, average training loss: 2169.43, base loss: 2365.11
[INFO 2017-06-26 19:23:50,713 main.py:47] epoch 2161, training loss: 2230.88, average training loss: 2169.43, base loss: 2365.33
[INFO 2017-06-26 19:23:51,000 main.py:47] epoch 2162, training loss: 2562.19, average training loss: 2170.16, base loss: 2366.28
[INFO 2017-06-26 19:23:51,284 main.py:47] epoch 2163, training loss: 2117.00, average training loss: 2170.21, base loss: 2366.29
[INFO 2017-06-26 19:23:51,570 main.py:47] epoch 2164, training loss: 1884.63, average training loss: 2170.24, base loss: 2366.32
[INFO 2017-06-26 19:23:51,851 main.py:47] epoch 2165, training loss: 2026.75, average training loss: 2169.77, base loss: 2365.94
[INFO 2017-06-26 19:23:52,135 main.py:47] epoch 2166, training loss: 2173.23, average training loss: 2169.18, base loss: 2365.44
[INFO 2017-06-26 19:23:52,420 main.py:47] epoch 2167, training loss: 2268.00, average training loss: 2169.45, base loss: 2365.74
[INFO 2017-06-26 19:23:52,705 main.py:47] epoch 2168, training loss: 2025.49, average training loss: 2169.44, base loss: 2365.62
[INFO 2017-06-26 19:23:52,987 main.py:47] epoch 2169, training loss: 2369.26, average training loss: 2169.74, base loss: 2365.93
[INFO 2017-06-26 19:23:53,269 main.py:47] epoch 2170, training loss: 2411.58, average training loss: 2170.22, base loss: 2366.60
[INFO 2017-06-26 19:23:53,553 main.py:47] epoch 2171, training loss: 2568.72, average training loss: 2170.77, base loss: 2367.22
[INFO 2017-06-26 19:23:53,835 main.py:47] epoch 2172, training loss: 1833.51, average training loss: 2170.58, base loss: 2367.02
[INFO 2017-06-26 19:23:54,119 main.py:47] epoch 2173, training loss: 1881.24, average training loss: 2170.40, base loss: 2366.83
[INFO 2017-06-26 19:23:54,405 main.py:47] epoch 2174, training loss: 2296.04, average training loss: 2170.01, base loss: 2366.38
[INFO 2017-06-26 19:23:54,689 main.py:47] epoch 2175, training loss: 2204.47, average training loss: 2169.90, base loss: 2366.17
[INFO 2017-06-26 19:23:54,976 main.py:47] epoch 2176, training loss: 2075.97, average training loss: 2169.80, base loss: 2366.10
[INFO 2017-06-26 19:23:55,264 main.py:47] epoch 2177, training loss: 1702.68, average training loss: 2169.49, base loss: 2365.71
[INFO 2017-06-26 19:23:55,548 main.py:47] epoch 2178, training loss: 2290.16, average training loss: 2169.60, base loss: 2365.89
[INFO 2017-06-26 19:23:55,831 main.py:47] epoch 2179, training loss: 2228.38, average training loss: 2169.37, base loss: 2365.74
[INFO 2017-06-26 19:23:56,115 main.py:47] epoch 2180, training loss: 2208.19, average training loss: 2169.37, base loss: 2365.86
[INFO 2017-06-26 19:23:56,402 main.py:47] epoch 2181, training loss: 2077.04, average training loss: 2169.27, base loss: 2365.81
[INFO 2017-06-26 19:23:56,689 main.py:47] epoch 2182, training loss: 2207.33, average training loss: 2169.54, base loss: 2366.10
[INFO 2017-06-26 19:23:56,977 main.py:47] epoch 2183, training loss: 2084.20, average training loss: 2169.34, base loss: 2365.94
[INFO 2017-06-26 19:23:57,261 main.py:47] epoch 2184, training loss: 2065.68, average training loss: 2169.58, base loss: 2366.40
[INFO 2017-06-26 19:23:57,548 main.py:47] epoch 2185, training loss: 2099.93, average training loss: 2169.21, base loss: 2365.91
[INFO 2017-06-26 19:23:57,831 main.py:47] epoch 2186, training loss: 2722.43, average training loss: 2169.11, base loss: 2365.96
[INFO 2017-06-26 19:23:58,117 main.py:47] epoch 2187, training loss: 2255.32, average training loss: 2169.01, base loss: 2365.80
[INFO 2017-06-26 19:23:58,402 main.py:47] epoch 2188, training loss: 2222.40, average training loss: 2169.03, base loss: 2365.71
[INFO 2017-06-26 19:23:58,689 main.py:47] epoch 2189, training loss: 2169.64, average training loss: 2169.14, base loss: 2365.68
[INFO 2017-06-26 19:23:58,975 main.py:47] epoch 2190, training loss: 2071.18, average training loss: 2169.28, base loss: 2365.90
[INFO 2017-06-26 19:23:59,258 main.py:47] epoch 2191, training loss: 2332.06, average training loss: 2169.75, base loss: 2366.58
[INFO 2017-06-26 19:23:59,544 main.py:47] epoch 2192, training loss: 2016.86, average training loss: 2169.88, base loss: 2366.62
[INFO 2017-06-26 19:23:59,830 main.py:47] epoch 2193, training loss: 1836.67, average training loss: 2169.83, base loss: 2366.75
[INFO 2017-06-26 19:24:00,114 main.py:47] epoch 2194, training loss: 2169.31, average training loss: 2170.13, base loss: 2367.03
[INFO 2017-06-26 19:24:00,400 main.py:47] epoch 2195, training loss: 1938.21, average training loss: 2169.70, base loss: 2366.55
[INFO 2017-06-26 19:24:00,684 main.py:47] epoch 2196, training loss: 1745.62, average training loss: 2169.23, base loss: 2366.07
[INFO 2017-06-26 19:24:00,968 main.py:47] epoch 2197, training loss: 2023.22, average training loss: 2169.20, base loss: 2366.02
[INFO 2017-06-26 19:24:01,251 main.py:47] epoch 2198, training loss: 1726.47, average training loss: 2168.68, base loss: 2365.42
[INFO 2017-06-26 19:24:01,534 main.py:47] epoch 2199, training loss: 2216.18, average training loss: 2168.49, base loss: 2365.29
[INFO 2017-06-26 19:24:01,534 main.py:49] epoch 2199, testing
[INFO 2017-06-26 19:24:05,235 main.py:100] average testing loss: 2046.21, base loss: 2247.05
[INFO 2017-06-26 19:24:05,261 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:24:05,545 main.py:47] epoch 2200, training loss: 1951.58, average training loss: 2167.72, base loss: 2364.36
[INFO 2017-06-26 19:24:05,825 main.py:47] epoch 2201, training loss: 2057.75, average training loss: 2167.66, base loss: 2364.44
[INFO 2017-06-26 19:24:06,112 main.py:47] epoch 2202, training loss: 2186.11, average training loss: 2167.91, base loss: 2364.74
[INFO 2017-06-26 19:24:06,399 main.py:47] epoch 2203, training loss: 1861.78, average training loss: 2167.39, base loss: 2364.13
[INFO 2017-06-26 19:24:06,685 main.py:47] epoch 2204, training loss: 2023.76, average training loss: 2167.55, base loss: 2364.28
[INFO 2017-06-26 19:24:06,973 main.py:47] epoch 2205, training loss: 2146.61, average training loss: 2166.88, base loss: 2363.68
[INFO 2017-06-26 19:24:07,258 main.py:47] epoch 2206, training loss: 2054.46, average training loss: 2166.65, base loss: 2363.60
[INFO 2017-06-26 19:24:07,538 main.py:47] epoch 2207, training loss: 2487.03, average training loss: 2166.33, base loss: 2363.28
[INFO 2017-06-26 19:24:07,822 main.py:47] epoch 2208, training loss: 2052.82, average training loss: 2166.07, base loss: 2363.11
[INFO 2017-06-26 19:24:08,106 main.py:47] epoch 2209, training loss: 2079.11, average training loss: 2165.94, base loss: 2363.05
[INFO 2017-06-26 19:24:08,394 main.py:47] epoch 2210, training loss: 2251.42, average training loss: 2165.99, base loss: 2363.09
[INFO 2017-06-26 19:24:08,679 main.py:47] epoch 2211, training loss: 2165.59, average training loss: 2166.16, base loss: 2363.48
[INFO 2017-06-26 19:24:08,963 main.py:47] epoch 2212, training loss: 2313.86, average training loss: 2166.14, base loss: 2363.65
[INFO 2017-06-26 19:24:09,246 main.py:47] epoch 2213, training loss: 2013.73, average training loss: 2165.65, base loss: 2363.48
[INFO 2017-06-26 19:24:09,541 main.py:47] epoch 2214, training loss: 2665.42, average training loss: 2165.80, base loss: 2363.58
[INFO 2017-06-26 19:24:09,823 main.py:47] epoch 2215, training loss: 2343.83, average training loss: 2165.47, base loss: 2363.46
[INFO 2017-06-26 19:24:10,108 main.py:47] epoch 2216, training loss: 1840.22, average training loss: 2164.95, base loss: 2362.75
[INFO 2017-06-26 19:24:10,393 main.py:47] epoch 2217, training loss: 2357.48, average training loss: 2165.13, base loss: 2362.99
[INFO 2017-06-26 19:24:10,674 main.py:47] epoch 2218, training loss: 1950.45, average training loss: 2164.63, base loss: 2362.41
[INFO 2017-06-26 19:24:10,957 main.py:47] epoch 2219, training loss: 2152.61, average training loss: 2164.64, base loss: 2362.48
[INFO 2017-06-26 19:24:11,244 main.py:47] epoch 2220, training loss: 2301.61, average training loss: 2165.00, base loss: 2362.86
[INFO 2017-06-26 19:24:11,532 main.py:47] epoch 2221, training loss: 1754.44, average training loss: 2164.68, base loss: 2362.56
[INFO 2017-06-26 19:24:11,816 main.py:47] epoch 2222, training loss: 1922.88, average training loss: 2164.68, base loss: 2362.55
[INFO 2017-06-26 19:24:12,096 main.py:47] epoch 2223, training loss: 2272.22, average training loss: 2164.91, base loss: 2362.70
[INFO 2017-06-26 19:24:12,377 main.py:47] epoch 2224, training loss: 2383.56, average training loss: 2164.77, base loss: 2362.58
[INFO 2017-06-26 19:24:12,661 main.py:47] epoch 2225, training loss: 2108.35, average training loss: 2164.95, base loss: 2362.95
[INFO 2017-06-26 19:24:12,947 main.py:47] epoch 2226, training loss: 2026.75, average training loss: 2164.85, base loss: 2362.91
[INFO 2017-06-26 19:24:13,228 main.py:47] epoch 2227, training loss: 2670.91, average training loss: 2165.31, base loss: 2363.49
[INFO 2017-06-26 19:24:13,510 main.py:47] epoch 2228, training loss: 1766.55, average training loss: 2164.58, base loss: 2362.78
[INFO 2017-06-26 19:24:13,797 main.py:47] epoch 2229, training loss: 2094.17, average training loss: 2164.90, base loss: 2363.26
[INFO 2017-06-26 19:24:14,082 main.py:47] epoch 2230, training loss: 2532.55, average training loss: 2164.89, base loss: 2363.34
[INFO 2017-06-26 19:24:14,365 main.py:47] epoch 2231, training loss: 2253.37, average training loss: 2165.06, base loss: 2363.67
[INFO 2017-06-26 19:24:14,650 main.py:47] epoch 2232, training loss: 2349.47, average training loss: 2165.35, base loss: 2364.18
[INFO 2017-06-26 19:24:14,931 main.py:47] epoch 2233, training loss: 2408.82, average training loss: 2165.73, base loss: 2364.71
[INFO 2017-06-26 19:24:15,213 main.py:47] epoch 2234, training loss: 2364.02, average training loss: 2166.28, base loss: 2365.46
[INFO 2017-06-26 19:24:15,494 main.py:47] epoch 2235, training loss: 2353.74, average training loss: 2166.25, base loss: 2365.61
[INFO 2017-06-26 19:24:15,787 main.py:47] epoch 2236, training loss: 2443.36, average training loss: 2166.63, base loss: 2366.01
[INFO 2017-06-26 19:24:16,071 main.py:47] epoch 2237, training loss: 2351.48, average training loss: 2166.92, base loss: 2366.42
[INFO 2017-06-26 19:24:16,357 main.py:47] epoch 2238, training loss: 2119.76, average training loss: 2167.13, base loss: 2366.86
[INFO 2017-06-26 19:24:16,638 main.py:47] epoch 2239, training loss: 2318.19, average training loss: 2166.91, base loss: 2366.62
[INFO 2017-06-26 19:24:16,926 main.py:47] epoch 2240, training loss: 2221.56, average training loss: 2167.00, base loss: 2366.72
[INFO 2017-06-26 19:24:17,213 main.py:47] epoch 2241, training loss: 1998.68, average training loss: 2166.69, base loss: 2366.15
[INFO 2017-06-26 19:24:17,495 main.py:47] epoch 2242, training loss: 2104.99, average training loss: 2166.24, base loss: 2365.72
[INFO 2017-06-26 19:24:17,781 main.py:47] epoch 2243, training loss: 2077.51, average training loss: 2165.95, base loss: 2365.33
[INFO 2017-06-26 19:24:18,068 main.py:47] epoch 2244, training loss: 2681.62, average training loss: 2166.73, base loss: 2366.33
[INFO 2017-06-26 19:24:18,356 main.py:47] epoch 2245, training loss: 1865.77, average training loss: 2166.31, base loss: 2366.01
[INFO 2017-06-26 19:24:18,640 main.py:47] epoch 2246, training loss: 2357.42, average training loss: 2166.78, base loss: 2366.57
[INFO 2017-06-26 19:24:18,928 main.py:47] epoch 2247, training loss: 2116.18, average training loss: 2166.57, base loss: 2366.22
[INFO 2017-06-26 19:24:19,216 main.py:47] epoch 2248, training loss: 2202.03, average training loss: 2166.40, base loss: 2366.05
[INFO 2017-06-26 19:24:19,501 main.py:47] epoch 2249, training loss: 2031.21, average training loss: 2166.22, base loss: 2365.96
[INFO 2017-06-26 19:24:19,786 main.py:47] epoch 2250, training loss: 1880.59, average training loss: 2165.69, base loss: 2365.29
[INFO 2017-06-26 19:24:20,069 main.py:47] epoch 2251, training loss: 2317.11, average training loss: 2165.80, base loss: 2365.36
[INFO 2017-06-26 19:24:20,352 main.py:47] epoch 2252, training loss: 1851.10, average training loss: 2165.45, base loss: 2364.92
[INFO 2017-06-26 19:24:20,637 main.py:47] epoch 2253, training loss: 2437.48, average training loss: 2165.63, base loss: 2365.23
[INFO 2017-06-26 19:24:20,923 main.py:47] epoch 2254, training loss: 2080.60, average training loss: 2165.70, base loss: 2365.35
[INFO 2017-06-26 19:24:21,211 main.py:47] epoch 2255, training loss: 2288.60, average training loss: 2165.75, base loss: 2365.43
[INFO 2017-06-26 19:24:21,495 main.py:47] epoch 2256, training loss: 2376.34, average training loss: 2165.72, base loss: 2365.65
[INFO 2017-06-26 19:24:21,780 main.py:47] epoch 2257, training loss: 2405.97, average training loss: 2166.07, base loss: 2366.07
[INFO 2017-06-26 19:24:22,066 main.py:47] epoch 2258, training loss: 2248.76, average training loss: 2166.09, base loss: 2366.14
[INFO 2017-06-26 19:24:22,350 main.py:47] epoch 2259, training loss: 2144.90, average training loss: 2165.60, base loss: 2365.77
[INFO 2017-06-26 19:24:22,634 main.py:47] epoch 2260, training loss: 2021.88, average training loss: 2165.51, base loss: 2365.69
[INFO 2017-06-26 19:24:22,921 main.py:47] epoch 2261, training loss: 2137.81, average training loss: 2165.85, base loss: 2366.05
[INFO 2017-06-26 19:24:23,205 main.py:47] epoch 2262, training loss: 2192.95, average training loss: 2166.22, base loss: 2366.50
[INFO 2017-06-26 19:24:23,490 main.py:47] epoch 2263, training loss: 2134.33, average training loss: 2166.26, base loss: 2366.79
[INFO 2017-06-26 19:24:23,774 main.py:47] epoch 2264, training loss: 2226.23, average training loss: 2165.90, base loss: 2366.49
[INFO 2017-06-26 19:24:24,058 main.py:47] epoch 2265, training loss: 2082.57, average training loss: 2165.34, base loss: 2366.12
[INFO 2017-06-26 19:24:24,342 main.py:47] epoch 2266, training loss: 2491.52, average training loss: 2165.48, base loss: 2366.54
[INFO 2017-06-26 19:24:24,626 main.py:47] epoch 2267, training loss: 2031.01, average training loss: 2165.38, base loss: 2366.62
[INFO 2017-06-26 19:24:24,911 main.py:47] epoch 2268, training loss: 2710.76, average training loss: 2165.93, base loss: 2367.32
[INFO 2017-06-26 19:24:25,196 main.py:47] epoch 2269, training loss: 1722.26, average training loss: 2165.43, base loss: 2366.67
[INFO 2017-06-26 19:24:25,479 main.py:47] epoch 2270, training loss: 2196.91, average training loss: 2165.32, base loss: 2366.49
[INFO 2017-06-26 19:24:25,762 main.py:47] epoch 2271, training loss: 2191.78, average training loss: 2165.56, base loss: 2366.77
[INFO 2017-06-26 19:24:26,045 main.py:47] epoch 2272, training loss: 2484.53, average training loss: 2165.87, base loss: 2367.26
[INFO 2017-06-26 19:24:26,328 main.py:47] epoch 2273, training loss: 2558.05, average training loss: 2166.15, base loss: 2367.77
[INFO 2017-06-26 19:24:26,616 main.py:47] epoch 2274, training loss: 2385.34, average training loss: 2166.55, base loss: 2368.23
[INFO 2017-06-26 19:24:26,904 main.py:47] epoch 2275, training loss: 1989.32, average training loss: 2166.13, base loss: 2367.85
[INFO 2017-06-26 19:24:27,189 main.py:47] epoch 2276, training loss: 1906.84, average training loss: 2165.60, base loss: 2367.42
[INFO 2017-06-26 19:24:27,473 main.py:47] epoch 2277, training loss: 1880.61, average training loss: 2165.09, base loss: 2366.88
[INFO 2017-06-26 19:24:27,761 main.py:47] epoch 2278, training loss: 1993.48, average training loss: 2164.80, base loss: 2366.61
[INFO 2017-06-26 19:24:28,045 main.py:47] epoch 2279, training loss: 2327.13, average training loss: 2164.66, base loss: 2366.50
[INFO 2017-06-26 19:24:28,332 main.py:47] epoch 2280, training loss: 1976.74, average training loss: 2164.61, base loss: 2366.42
[INFO 2017-06-26 19:24:28,614 main.py:47] epoch 2281, training loss: 1954.22, average training loss: 2164.67, base loss: 2366.60
[INFO 2017-06-26 19:24:28,902 main.py:47] epoch 2282, training loss: 2311.97, average training loss: 2165.00, base loss: 2366.92
[INFO 2017-06-26 19:24:29,191 main.py:47] epoch 2283, training loss: 2082.70, average training loss: 2165.01, base loss: 2367.00
[INFO 2017-06-26 19:24:29,473 main.py:47] epoch 2284, training loss: 2246.04, average training loss: 2164.81, base loss: 2366.83
[INFO 2017-06-26 19:24:29,757 main.py:47] epoch 2285, training loss: 2312.61, average training loss: 2165.20, base loss: 2367.43
[INFO 2017-06-26 19:24:30,044 main.py:47] epoch 2286, training loss: 2145.51, average training loss: 2165.54, base loss: 2367.78
[INFO 2017-06-26 19:24:30,333 main.py:47] epoch 2287, training loss: 1936.99, average training loss: 2165.69, base loss: 2367.90
[INFO 2017-06-26 19:24:30,616 main.py:47] epoch 2288, training loss: 1991.44, average training loss: 2165.72, base loss: 2368.13
[INFO 2017-06-26 19:24:30,903 main.py:47] epoch 2289, training loss: 2139.72, average training loss: 2165.77, base loss: 2368.24
[INFO 2017-06-26 19:24:31,184 main.py:47] epoch 2290, training loss: 2141.23, average training loss: 2165.80, base loss: 2368.43
[INFO 2017-06-26 19:24:31,469 main.py:47] epoch 2291, training loss: 1949.91, average training loss: 2165.05, base loss: 2367.59
[INFO 2017-06-26 19:24:31,752 main.py:47] epoch 2292, training loss: 2000.47, average training loss: 2164.61, base loss: 2367.34
[INFO 2017-06-26 19:24:32,040 main.py:47] epoch 2293, training loss: 1950.84, average training loss: 2164.34, base loss: 2367.13
[INFO 2017-06-26 19:24:32,325 main.py:47] epoch 2294, training loss: 2270.94, average training loss: 2164.62, base loss: 2367.38
[INFO 2017-06-26 19:24:32,614 main.py:47] epoch 2295, training loss: 2516.74, average training loss: 2165.15, base loss: 2368.13
[INFO 2017-06-26 19:24:32,901 main.py:47] epoch 2296, training loss: 1756.28, average training loss: 2164.35, base loss: 2367.39
[INFO 2017-06-26 19:24:33,188 main.py:47] epoch 2297, training loss: 2156.08, average training loss: 2164.41, base loss: 2367.44
[INFO 2017-06-26 19:24:33,475 main.py:47] epoch 2298, training loss: 2110.46, average training loss: 2164.21, base loss: 2367.31
[INFO 2017-06-26 19:24:33,761 main.py:47] epoch 2299, training loss: 2042.40, average training loss: 2164.15, base loss: 2367.18
[INFO 2017-06-26 19:24:33,761 main.py:49] epoch 2299, testing
[INFO 2017-06-26 19:24:37,580 main.py:100] average testing loss: 2183.44, base loss: 2423.12
[INFO 2017-06-26 19:24:37,605 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:24:37,893 main.py:47] epoch 2300, training loss: 2578.37, average training loss: 2164.31, base loss: 2367.38
[INFO 2017-06-26 19:24:38,177 main.py:47] epoch 2301, training loss: 2244.31, average training loss: 2164.50, base loss: 2367.62
[INFO 2017-06-26 19:24:38,462 main.py:47] epoch 2302, training loss: 2471.75, average training loss: 2164.51, base loss: 2367.70
[INFO 2017-06-26 19:24:38,748 main.py:47] epoch 2303, training loss: 2241.50, average training loss: 2164.23, base loss: 2367.55
[INFO 2017-06-26 19:24:39,034 main.py:47] epoch 2304, training loss: 2072.68, average training loss: 2164.29, base loss: 2367.61
[INFO 2017-06-26 19:24:39,319 main.py:47] epoch 2305, training loss: 2057.98, average training loss: 2164.36, base loss: 2367.78
[INFO 2017-06-26 19:24:39,604 main.py:47] epoch 2306, training loss: 2166.54, average training loss: 2164.25, base loss: 2367.61
[INFO 2017-06-26 19:24:39,887 main.py:47] epoch 2307, training loss: 1895.71, average training loss: 2163.51, base loss: 2366.79
[INFO 2017-06-26 19:24:40,174 main.py:47] epoch 2308, training loss: 2104.88, average training loss: 2163.66, base loss: 2367.03
[INFO 2017-06-26 19:24:40,462 main.py:47] epoch 2309, training loss: 2336.62, average training loss: 2163.67, base loss: 2367.22
[INFO 2017-06-26 19:24:40,746 main.py:47] epoch 2310, training loss: 2513.66, average training loss: 2164.06, base loss: 2367.65
[INFO 2017-06-26 19:24:41,032 main.py:47] epoch 2311, training loss: 2359.20, average training loss: 2164.45, base loss: 2368.02
[INFO 2017-06-26 19:24:41,315 main.py:47] epoch 2312, training loss: 2096.45, average training loss: 2163.70, base loss: 2367.30
[INFO 2017-06-26 19:24:41,602 main.py:47] epoch 2313, training loss: 1954.08, average training loss: 2163.36, base loss: 2367.11
[INFO 2017-06-26 19:24:41,883 main.py:47] epoch 2314, training loss: 2270.81, average training loss: 2163.20, base loss: 2366.86
[INFO 2017-06-26 19:24:42,169 main.py:47] epoch 2315, training loss: 1997.17, average training loss: 2162.62, base loss: 2366.11
[INFO 2017-06-26 19:24:42,453 main.py:47] epoch 2316, training loss: 2084.75, average training loss: 2162.35, base loss: 2365.88
[INFO 2017-06-26 19:24:42,736 main.py:47] epoch 2317, training loss: 2202.65, average training loss: 2161.97, base loss: 2365.46
[INFO 2017-06-26 19:24:43,020 main.py:47] epoch 2318, training loss: 2026.82, average training loss: 2162.08, base loss: 2365.53
[INFO 2017-06-26 19:24:43,326 main.py:47] epoch 2319, training loss: 1700.05, average training loss: 2161.72, base loss: 2365.03
[INFO 2017-06-26 19:24:43,619 main.py:47] epoch 2320, training loss: 1949.76, average training loss: 2161.37, base loss: 2364.65
[INFO 2017-06-26 19:24:43,925 main.py:47] epoch 2321, training loss: 2031.02, average training loss: 2161.21, base loss: 2364.56
[INFO 2017-06-26 19:24:44,210 main.py:47] epoch 2322, training loss: 2438.33, average training loss: 2161.29, base loss: 2364.59
[INFO 2017-06-26 19:24:44,497 main.py:47] epoch 2323, training loss: 2023.60, average training loss: 2161.12, base loss: 2364.36
[INFO 2017-06-26 19:24:44,780 main.py:47] epoch 2324, training loss: 1875.71, average training loss: 2160.95, base loss: 2364.24
[INFO 2017-06-26 19:24:45,066 main.py:47] epoch 2325, training loss: 2201.38, average training loss: 2160.65, base loss: 2363.71
[INFO 2017-06-26 19:24:45,347 main.py:47] epoch 2326, training loss: 1962.67, average training loss: 2160.61, base loss: 2363.77
[INFO 2017-06-26 19:24:45,632 main.py:47] epoch 2327, training loss: 2098.01, average training loss: 2160.45, base loss: 2363.51
[INFO 2017-06-26 19:24:45,915 main.py:47] epoch 2328, training loss: 2269.27, average training loss: 2160.72, base loss: 2363.91
[INFO 2017-06-26 19:24:46,200 main.py:47] epoch 2329, training loss: 2150.82, average training loss: 2160.57, base loss: 2363.83
[INFO 2017-06-26 19:24:46,484 main.py:47] epoch 2330, training loss: 1790.45, average training loss: 2160.12, base loss: 2363.34
[INFO 2017-06-26 19:24:46,768 main.py:47] epoch 2331, training loss: 1969.02, average training loss: 2159.52, base loss: 2362.52
[INFO 2017-06-26 19:24:47,052 main.py:47] epoch 2332, training loss: 1910.98, average training loss: 2159.53, base loss: 2362.59
[INFO 2017-06-26 19:24:47,333 main.py:47] epoch 2333, training loss: 2336.78, average training loss: 2160.13, base loss: 2363.49
[INFO 2017-06-26 19:24:47,618 main.py:47] epoch 2334, training loss: 2092.66, average training loss: 2160.10, base loss: 2363.53
[INFO 2017-06-26 19:24:47,902 main.py:47] epoch 2335, training loss: 2545.46, average training loss: 2160.07, base loss: 2363.47
[INFO 2017-06-26 19:24:48,186 main.py:47] epoch 2336, training loss: 2173.05, average training loss: 2160.14, base loss: 2363.50
[INFO 2017-06-26 19:24:48,473 main.py:47] epoch 2337, training loss: 1712.21, average training loss: 2159.85, base loss: 2363.23
[INFO 2017-06-26 19:24:48,761 main.py:47] epoch 2338, training loss: 2046.34, average training loss: 2159.47, base loss: 2362.99
[INFO 2017-06-26 19:24:49,045 main.py:47] epoch 2339, training loss: 2258.43, average training loss: 2159.64, base loss: 2363.31
[INFO 2017-06-26 19:24:49,331 main.py:47] epoch 2340, training loss: 2086.83, average training loss: 2159.47, base loss: 2363.22
[INFO 2017-06-26 19:24:49,619 main.py:47] epoch 2341, training loss: 2527.65, average training loss: 2159.94, base loss: 2363.81
[INFO 2017-06-26 19:24:49,907 main.py:47] epoch 2342, training loss: 2211.99, average training loss: 2159.81, base loss: 2363.72
[INFO 2017-06-26 19:24:50,191 main.py:47] epoch 2343, training loss: 2187.47, average training loss: 2160.01, base loss: 2363.94
[INFO 2017-06-26 19:24:50,478 main.py:47] epoch 2344, training loss: 1797.93, average training loss: 2159.53, base loss: 2363.57
[INFO 2017-06-26 19:24:50,762 main.py:47] epoch 2345, training loss: 2228.29, average training loss: 2159.89, base loss: 2363.89
[INFO 2017-06-26 19:24:51,046 main.py:47] epoch 2346, training loss: 2332.69, average training loss: 2159.96, base loss: 2363.88
[INFO 2017-06-26 19:24:51,334 main.py:47] epoch 2347, training loss: 2243.96, average training loss: 2159.83, base loss: 2363.82
[INFO 2017-06-26 19:24:51,620 main.py:47] epoch 2348, training loss: 1885.15, average training loss: 2159.33, base loss: 2363.28
[INFO 2017-06-26 19:24:51,901 main.py:47] epoch 2349, training loss: 2063.26, average training loss: 2159.06, base loss: 2362.90
[INFO 2017-06-26 19:24:52,184 main.py:47] epoch 2350, training loss: 2027.01, average training loss: 2158.92, base loss: 2362.74
[INFO 2017-06-26 19:24:52,469 main.py:47] epoch 2351, training loss: 2110.33, average training loss: 2158.86, base loss: 2362.59
[INFO 2017-06-26 19:24:52,757 main.py:47] epoch 2352, training loss: 2542.87, average training loss: 2159.32, base loss: 2363.24
[INFO 2017-06-26 19:24:53,040 main.py:47] epoch 2353, training loss: 1884.92, average training loss: 2159.30, base loss: 2363.31
[INFO 2017-06-26 19:24:53,325 main.py:47] epoch 2354, training loss: 2270.69, average training loss: 2159.50, base loss: 2363.80
[INFO 2017-06-26 19:24:53,608 main.py:47] epoch 2355, training loss: 2239.29, average training loss: 2159.57, base loss: 2364.00
[INFO 2017-06-26 19:24:53,890 main.py:47] epoch 2356, training loss: 2269.39, average training loss: 2160.14, base loss: 2364.73
[INFO 2017-06-26 19:24:54,170 main.py:47] epoch 2357, training loss: 2020.42, average training loss: 2159.48, base loss: 2364.14
[INFO 2017-06-26 19:24:54,454 main.py:47] epoch 2358, training loss: 2040.36, average training loss: 2159.36, base loss: 2363.96
[INFO 2017-06-26 19:24:54,733 main.py:47] epoch 2359, training loss: 2035.92, average training loss: 2158.89, base loss: 2363.52
[INFO 2017-06-26 19:24:55,016 main.py:47] epoch 2360, training loss: 1948.64, average training loss: 2158.56, base loss: 2363.24
[INFO 2017-06-26 19:24:55,300 main.py:47] epoch 2361, training loss: 2083.34, average training loss: 2158.39, base loss: 2363.02
[INFO 2017-06-26 19:24:55,581 main.py:47] epoch 2362, training loss: 2290.62, average training loss: 2158.72, base loss: 2363.42
[INFO 2017-06-26 19:24:55,865 main.py:47] epoch 2363, training loss: 2187.85, average training loss: 2158.64, base loss: 2363.25
[INFO 2017-06-26 19:24:56,149 main.py:47] epoch 2364, training loss: 2245.87, average training loss: 2158.92, base loss: 2363.52
[INFO 2017-06-26 19:24:56,430 main.py:47] epoch 2365, training loss: 2238.80, average training loss: 2158.78, base loss: 2363.45
[INFO 2017-06-26 19:24:56,711 main.py:47] epoch 2366, training loss: 1997.71, average training loss: 2158.61, base loss: 2363.42
[INFO 2017-06-26 19:24:56,991 main.py:47] epoch 2367, training loss: 1946.05, average training loss: 2158.06, base loss: 2363.06
[INFO 2017-06-26 19:24:57,274 main.py:47] epoch 2368, training loss: 2207.66, average training loss: 2158.18, base loss: 2363.12
[INFO 2017-06-26 19:24:57,556 main.py:47] epoch 2369, training loss: 2019.21, average training loss: 2158.28, base loss: 2363.47
[INFO 2017-06-26 19:24:57,841 main.py:47] epoch 2370, training loss: 2013.58, average training loss: 2157.84, base loss: 2363.12
[INFO 2017-06-26 19:24:58,120 main.py:47] epoch 2371, training loss: 2097.48, average training loss: 2157.08, base loss: 2362.36
[INFO 2017-06-26 19:24:58,402 main.py:47] epoch 2372, training loss: 1923.41, average training loss: 2156.57, base loss: 2361.93
[INFO 2017-06-26 19:24:58,686 main.py:47] epoch 2373, training loss: 2114.08, average training loss: 2156.47, base loss: 2361.87
[INFO 2017-06-26 19:24:58,973 main.py:47] epoch 2374, training loss: 2459.44, average training loss: 2156.90, base loss: 2362.23
[INFO 2017-06-26 19:24:59,255 main.py:47] epoch 2375, training loss: 2090.13, average training loss: 2156.85, base loss: 2362.07
[INFO 2017-06-26 19:24:59,537 main.py:47] epoch 2376, training loss: 2548.40, average training loss: 2157.53, base loss: 2362.86
[INFO 2017-06-26 19:24:59,821 main.py:47] epoch 2377, training loss: 2531.98, average training loss: 2158.15, base loss: 2363.64
[INFO 2017-06-26 19:25:00,105 main.py:47] epoch 2378, training loss: 1950.38, average training loss: 2158.26, base loss: 2363.74
[INFO 2017-06-26 19:25:00,391 main.py:47] epoch 2379, training loss: 2015.55, average training loss: 2158.15, base loss: 2363.67
[INFO 2017-06-26 19:25:00,678 main.py:47] epoch 2380, training loss: 1686.38, average training loss: 2157.31, base loss: 2362.74
[INFO 2017-06-26 19:25:00,961 main.py:47] epoch 2381, training loss: 2386.25, average training loss: 2157.29, base loss: 2362.78
[INFO 2017-06-26 19:25:01,249 main.py:47] epoch 2382, training loss: 2255.79, average training loss: 2157.63, base loss: 2363.24
[INFO 2017-06-26 19:25:01,533 main.py:47] epoch 2383, training loss: 2577.77, average training loss: 2158.32, base loss: 2364.05
[INFO 2017-06-26 19:25:01,817 main.py:47] epoch 2384, training loss: 2623.69, average training loss: 2158.54, base loss: 2364.16
[INFO 2017-06-26 19:25:02,101 main.py:47] epoch 2385, training loss: 2131.27, average training loss: 2158.08, base loss: 2363.70
[INFO 2017-06-26 19:25:02,385 main.py:47] epoch 2386, training loss: 2185.50, average training loss: 2158.11, base loss: 2363.65
[INFO 2017-06-26 19:25:02,667 main.py:47] epoch 2387, training loss: 1735.95, average training loss: 2157.74, base loss: 2363.28
[INFO 2017-06-26 19:25:02,951 main.py:47] epoch 2388, training loss: 1907.51, average training loss: 2157.13, base loss: 2362.70
[INFO 2017-06-26 19:25:03,232 main.py:47] epoch 2389, training loss: 2602.18, average training loss: 2157.71, base loss: 2363.51
[INFO 2017-06-26 19:25:03,512 main.py:47] epoch 2390, training loss: 2103.99, average training loss: 2157.77, base loss: 2363.47
[INFO 2017-06-26 19:25:03,798 main.py:47] epoch 2391, training loss: 1935.68, average training loss: 2157.77, base loss: 2363.51
[INFO 2017-06-26 19:25:04,082 main.py:47] epoch 2392, training loss: 2526.26, average training loss: 2158.40, base loss: 2364.32
[INFO 2017-06-26 19:25:04,367 main.py:47] epoch 2393, training loss: 2654.30, average training loss: 2158.66, base loss: 2364.69
[INFO 2017-06-26 19:25:04,654 main.py:47] epoch 2394, training loss: 2515.67, average training loss: 2158.96, base loss: 2365.02
[INFO 2017-06-26 19:25:04,937 main.py:47] epoch 2395, training loss: 2491.57, average training loss: 2159.58, base loss: 2366.06
[INFO 2017-06-26 19:25:05,221 main.py:47] epoch 2396, training loss: 2231.52, average training loss: 2159.73, base loss: 2366.31
[INFO 2017-06-26 19:25:05,507 main.py:47] epoch 2397, training loss: 2339.87, average training loss: 2159.90, base loss: 2366.43
[INFO 2017-06-26 19:25:05,793 main.py:47] epoch 2398, training loss: 2105.36, average training loss: 2160.21, base loss: 2366.81
[INFO 2017-06-26 19:25:06,074 main.py:47] epoch 2399, training loss: 2310.71, average training loss: 2159.70, base loss: 2366.27
[INFO 2017-06-26 19:25:06,074 main.py:49] epoch 2399, testing
[INFO 2017-06-26 19:25:09,792 main.py:100] average testing loss: 2209.52, base loss: 2403.34
[INFO 2017-06-26 19:25:09,818 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:25:10,103 main.py:47] epoch 2400, training loss: 2341.57, average training loss: 2160.29, base loss: 2367.11
[INFO 2017-06-26 19:25:10,391 main.py:47] epoch 2401, training loss: 2071.35, average training loss: 2160.12, base loss: 2367.10
[INFO 2017-06-26 19:25:10,675 main.py:47] epoch 2402, training loss: 2030.22, average training loss: 2159.49, base loss: 2366.38
[INFO 2017-06-26 19:25:10,965 main.py:47] epoch 2403, training loss: 2272.15, average training loss: 2159.40, base loss: 2366.36
[INFO 2017-06-26 19:25:11,249 main.py:47] epoch 2404, training loss: 2137.00, average training loss: 2159.76, base loss: 2366.97
[INFO 2017-06-26 19:25:11,530 main.py:47] epoch 2405, training loss: 1987.24, average training loss: 2159.66, base loss: 2366.89
[INFO 2017-06-26 19:25:11,811 main.py:47] epoch 2406, training loss: 2289.95, average training loss: 2159.63, base loss: 2366.86
[INFO 2017-06-26 19:25:12,091 main.py:47] epoch 2407, training loss: 2731.54, average training loss: 2160.47, base loss: 2367.94
[INFO 2017-06-26 19:25:12,378 main.py:47] epoch 2408, training loss: 2348.63, average training loss: 2161.02, base loss: 2368.54
[INFO 2017-06-26 19:25:12,666 main.py:47] epoch 2409, training loss: 2391.94, average training loss: 2161.16, base loss: 2368.72
[INFO 2017-06-26 19:25:12,949 main.py:47] epoch 2410, training loss: 2217.26, average training loss: 2161.34, base loss: 2369.04
[INFO 2017-06-26 19:25:13,229 main.py:47] epoch 2411, training loss: 2049.62, average training loss: 2161.02, base loss: 2368.68
[INFO 2017-06-26 19:25:13,513 main.py:47] epoch 2412, training loss: 2178.95, average training loss: 2161.21, base loss: 2368.84
[INFO 2017-06-26 19:25:13,800 main.py:47] epoch 2413, training loss: 2435.00, average training loss: 2161.16, base loss: 2369.01
[INFO 2017-06-26 19:25:14,088 main.py:47] epoch 2414, training loss: 2204.37, average training loss: 2161.36, base loss: 2369.47
[INFO 2017-06-26 19:25:14,373 main.py:47] epoch 2415, training loss: 2338.74, average training loss: 2161.53, base loss: 2369.67
[INFO 2017-06-26 19:25:14,652 main.py:47] epoch 2416, training loss: 1821.18, average training loss: 2160.96, base loss: 2368.89
[INFO 2017-06-26 19:25:14,937 main.py:47] epoch 2417, training loss: 2087.73, average training loss: 2160.90, base loss: 2368.80
[INFO 2017-06-26 19:25:15,222 main.py:47] epoch 2418, training loss: 2249.75, average training loss: 2160.91, base loss: 2368.72
[INFO 2017-06-26 19:25:15,506 main.py:47] epoch 2419, training loss: 2164.29, average training loss: 2160.90, base loss: 2368.73
[INFO 2017-06-26 19:25:15,789 main.py:47] epoch 2420, training loss: 2542.36, average training loss: 2161.14, base loss: 2368.99
[INFO 2017-06-26 19:25:16,076 main.py:47] epoch 2421, training loss: 2277.94, average training loss: 2160.93, base loss: 2368.70
[INFO 2017-06-26 19:25:16,360 main.py:47] epoch 2422, training loss: 1842.16, average training loss: 2160.80, base loss: 2368.61
[INFO 2017-06-26 19:25:16,644 main.py:47] epoch 2423, training loss: 1952.29, average training loss: 2160.83, base loss: 2368.91
[INFO 2017-06-26 19:25:16,928 main.py:47] epoch 2424, training loss: 1966.98, average training loss: 2160.69, base loss: 2368.76
[INFO 2017-06-26 19:25:17,215 main.py:47] epoch 2425, training loss: 2164.67, average training loss: 2160.91, base loss: 2369.07
[INFO 2017-06-26 19:25:17,498 main.py:47] epoch 2426, training loss: 2277.36, average training loss: 2161.17, base loss: 2369.55
[INFO 2017-06-26 19:25:17,787 main.py:47] epoch 2427, training loss: 1855.39, average training loss: 2161.17, base loss: 2369.70
[INFO 2017-06-26 19:25:18,075 main.py:47] epoch 2428, training loss: 1755.95, average training loss: 2160.65, base loss: 2369.02
[INFO 2017-06-26 19:25:18,357 main.py:47] epoch 2429, training loss: 2132.85, average training loss: 2160.54, base loss: 2369.05
[INFO 2017-06-26 19:25:18,643 main.py:47] epoch 2430, training loss: 2360.63, average training loss: 2160.72, base loss: 2369.46
[INFO 2017-06-26 19:25:18,927 main.py:47] epoch 2431, training loss: 2175.08, average training loss: 2160.77, base loss: 2369.59
[INFO 2017-06-26 19:25:19,210 main.py:47] epoch 2432, training loss: 1745.60, average training loss: 2159.98, base loss: 2368.75
[INFO 2017-06-26 19:25:19,495 main.py:47] epoch 2433, training loss: 2179.55, average training loss: 2159.92, base loss: 2368.73
[INFO 2017-06-26 19:25:19,782 main.py:47] epoch 2434, training loss: 2666.95, average training loss: 2160.63, base loss: 2369.68
[INFO 2017-06-26 19:25:20,064 main.py:47] epoch 2435, training loss: 2302.00, average training loss: 2160.91, base loss: 2369.96
[INFO 2017-06-26 19:25:20,349 main.py:47] epoch 2436, training loss: 2250.11, average training loss: 2160.63, base loss: 2369.90
[INFO 2017-06-26 19:25:20,632 main.py:47] epoch 2437, training loss: 2182.67, average training loss: 2160.64, base loss: 2369.85
[INFO 2017-06-26 19:25:20,915 main.py:47] epoch 2438, training loss: 2156.91, average training loss: 2160.64, base loss: 2369.95
[INFO 2017-06-26 19:25:21,196 main.py:47] epoch 2439, training loss: 2196.03, average training loss: 2160.58, base loss: 2370.02
[INFO 2017-06-26 19:25:21,477 main.py:47] epoch 2440, training loss: 1802.12, average training loss: 2160.58, base loss: 2370.21
[INFO 2017-06-26 19:25:21,761 main.py:47] epoch 2441, training loss: 2178.80, average training loss: 2160.71, base loss: 2370.55
[INFO 2017-06-26 19:25:22,041 main.py:47] epoch 2442, training loss: 1805.00, average training loss: 2160.57, base loss: 2370.50
[INFO 2017-06-26 19:25:22,324 main.py:47] epoch 2443, training loss: 2002.00, average training loss: 2160.49, base loss: 2370.58
[INFO 2017-06-26 19:25:22,607 main.py:47] epoch 2444, training loss: 2383.00, average training loss: 2160.74, base loss: 2371.00
[INFO 2017-06-26 19:25:22,892 main.py:47] epoch 2445, training loss: 1920.40, average training loss: 2160.35, base loss: 2370.55
[INFO 2017-06-26 19:25:23,173 main.py:47] epoch 2446, training loss: 2078.56, average training loss: 2160.00, base loss: 2370.22
[INFO 2017-06-26 19:25:23,457 main.py:47] epoch 2447, training loss: 2631.68, average training loss: 2160.14, base loss: 2370.49
[INFO 2017-06-26 19:25:23,741 main.py:47] epoch 2448, training loss: 2154.99, average training loss: 2160.31, base loss: 2370.84
[INFO 2017-06-26 19:25:24,026 main.py:47] epoch 2449, training loss: 2378.93, average training loss: 2160.08, base loss: 2370.64
[INFO 2017-06-26 19:25:24,308 main.py:47] epoch 2450, training loss: 2079.89, average training loss: 2159.84, base loss: 2370.64
[INFO 2017-06-26 19:25:24,592 main.py:47] epoch 2451, training loss: 1932.92, average training loss: 2159.59, base loss: 2370.52
[INFO 2017-06-26 19:25:24,876 main.py:47] epoch 2452, training loss: 2068.41, average training loss: 2159.54, base loss: 2370.48
[INFO 2017-06-26 19:25:25,160 main.py:47] epoch 2453, training loss: 1977.11, average training loss: 2159.33, base loss: 2370.24
[INFO 2017-06-26 19:25:25,441 main.py:47] epoch 2454, training loss: 2044.01, average training loss: 2159.09, base loss: 2370.02
[INFO 2017-06-26 19:25:25,722 main.py:47] epoch 2455, training loss: 1924.09, average training loss: 2158.83, base loss: 2369.73
[INFO 2017-06-26 19:25:26,005 main.py:47] epoch 2456, training loss: 2102.57, average training loss: 2158.74, base loss: 2369.67
[INFO 2017-06-26 19:25:26,290 main.py:47] epoch 2457, training loss: 2030.33, average training loss: 2158.37, base loss: 2369.30
[INFO 2017-06-26 19:25:26,575 main.py:47] epoch 2458, training loss: 2281.20, average training loss: 2158.56, base loss: 2369.58
[INFO 2017-06-26 19:25:26,860 main.py:47] epoch 2459, training loss: 2118.72, average training loss: 2157.68, base loss: 2368.66
[INFO 2017-06-26 19:25:27,144 main.py:47] epoch 2460, training loss: 2033.96, average training loss: 2157.61, base loss: 2368.58
[INFO 2017-06-26 19:25:27,431 main.py:47] epoch 2461, training loss: 2130.70, average training loss: 2157.63, base loss: 2368.66
[INFO 2017-06-26 19:25:27,714 main.py:47] epoch 2462, training loss: 2495.32, average training loss: 2157.91, base loss: 2368.97
[INFO 2017-06-26 19:25:27,998 main.py:47] epoch 2463, training loss: 2360.86, average training loss: 2157.87, base loss: 2368.88
[INFO 2017-06-26 19:25:28,285 main.py:47] epoch 2464, training loss: 2187.51, average training loss: 2157.62, base loss: 2368.59
[INFO 2017-06-26 19:25:28,571 main.py:47] epoch 2465, training loss: 2146.82, average training loss: 2157.72, base loss: 2368.80
[INFO 2017-06-26 19:25:28,853 main.py:47] epoch 2466, training loss: 2050.14, average training loss: 2157.77, base loss: 2368.92
[INFO 2017-06-26 19:25:29,140 main.py:47] epoch 2467, training loss: 2157.13, average training loss: 2157.31, base loss: 2368.62
[INFO 2017-06-26 19:25:29,423 main.py:47] epoch 2468, training loss: 2240.43, average training loss: 2157.46, base loss: 2368.78
[INFO 2017-06-26 19:25:29,703 main.py:47] epoch 2469, training loss: 2013.02, average training loss: 2157.44, base loss: 2368.62
[INFO 2017-06-26 19:25:29,987 main.py:47] epoch 2470, training loss: 2441.39, average training loss: 2157.59, base loss: 2369.01
[INFO 2017-06-26 19:25:30,269 main.py:47] epoch 2471, training loss: 2154.28, average training loss: 2157.71, base loss: 2369.27
[INFO 2017-06-26 19:25:30,553 main.py:47] epoch 2472, training loss: 2314.71, average training loss: 2157.94, base loss: 2369.72
[INFO 2017-06-26 19:25:30,833 main.py:47] epoch 2473, training loss: 1877.49, average training loss: 2157.92, base loss: 2369.73
[INFO 2017-06-26 19:25:31,117 main.py:47] epoch 2474, training loss: 2032.30, average training loss: 2157.89, base loss: 2369.79
[INFO 2017-06-26 19:25:31,402 main.py:47] epoch 2475, training loss: 2181.50, average training loss: 2158.13, base loss: 2370.19
[INFO 2017-06-26 19:25:31,690 main.py:47] epoch 2476, training loss: 1923.53, average training loss: 2158.16, base loss: 2370.15
[INFO 2017-06-26 19:25:31,979 main.py:47] epoch 2477, training loss: 1824.02, average training loss: 2158.07, base loss: 2369.88
[INFO 2017-06-26 19:25:32,263 main.py:47] epoch 2478, training loss: 2060.60, average training loss: 2158.03, base loss: 2369.79
[INFO 2017-06-26 19:25:32,548 main.py:47] epoch 2479, training loss: 1717.58, average training loss: 2157.64, base loss: 2369.35
[INFO 2017-06-26 19:25:32,833 main.py:47] epoch 2480, training loss: 2106.70, average training loss: 2157.43, base loss: 2369.09
[INFO 2017-06-26 19:25:33,114 main.py:47] epoch 2481, training loss: 2358.57, average training loss: 2157.67, base loss: 2369.07
[INFO 2017-06-26 19:25:33,397 main.py:47] epoch 2482, training loss: 1808.93, average training loss: 2157.03, base loss: 2368.22
[INFO 2017-06-26 19:25:33,681 main.py:47] epoch 2483, training loss: 2448.47, average training loss: 2156.93, base loss: 2368.12
[INFO 2017-06-26 19:25:33,962 main.py:47] epoch 2484, training loss: 2046.35, average training loss: 2156.90, base loss: 2367.88
[INFO 2017-06-26 19:25:34,243 main.py:47] epoch 2485, training loss: 2423.75, average training loss: 2157.33, base loss: 2368.44
[INFO 2017-06-26 19:25:34,529 main.py:47] epoch 2486, training loss: 1953.78, average training loss: 2157.34, base loss: 2368.39
[INFO 2017-06-26 19:25:34,815 main.py:47] epoch 2487, training loss: 2253.82, average training loss: 2157.14, base loss: 2368.02
[INFO 2017-06-26 19:25:35,097 main.py:47] epoch 2488, training loss: 1966.04, average training loss: 2156.76, base loss: 2367.71
[INFO 2017-06-26 19:25:35,379 main.py:47] epoch 2489, training loss: 2136.86, average training loss: 2156.42, base loss: 2367.49
[INFO 2017-06-26 19:25:35,666 main.py:47] epoch 2490, training loss: 2236.35, average training loss: 2156.82, base loss: 2367.90
[INFO 2017-06-26 19:25:35,953 main.py:47] epoch 2491, training loss: 1907.69, average training loss: 2156.98, base loss: 2368.05
[INFO 2017-06-26 19:25:36,234 main.py:47] epoch 2492, training loss: 2625.02, average training loss: 2157.38, base loss: 2368.48
[INFO 2017-06-26 19:25:36,517 main.py:47] epoch 2493, training loss: 2221.48, average training loss: 2157.61, base loss: 2368.84
[INFO 2017-06-26 19:25:36,798 main.py:47] epoch 2494, training loss: 2525.27, average training loss: 2158.05, base loss: 2369.38
[INFO 2017-06-26 19:25:37,078 main.py:47] epoch 2495, training loss: 2275.34, average training loss: 2158.33, base loss: 2369.62
[INFO 2017-06-26 19:25:37,363 main.py:47] epoch 2496, training loss: 2060.80, average training loss: 2158.66, base loss: 2370.03
[INFO 2017-06-26 19:25:37,647 main.py:47] epoch 2497, training loss: 2061.64, average training loss: 2158.85, base loss: 2370.24
[INFO 2017-06-26 19:25:37,934 main.py:47] epoch 2498, training loss: 1940.20, average training loss: 2158.95, base loss: 2370.35
[INFO 2017-06-26 19:25:38,219 main.py:47] epoch 2499, training loss: 2705.49, average training loss: 2159.74, base loss: 2371.25
[INFO 2017-06-26 19:25:38,219 main.py:49] epoch 2499, testing
[INFO 2017-06-26 19:25:41,953 main.py:100] average testing loss: 2354.81, base loss: 2627.23
[INFO 2017-06-26 19:25:41,979 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:25:42,260 main.py:47] epoch 2500, training loss: 2379.96, average training loss: 2159.93, base loss: 2371.58
[INFO 2017-06-26 19:25:42,543 main.py:47] epoch 2501, training loss: 2269.67, average training loss: 2160.20, base loss: 2371.90
[INFO 2017-06-26 19:25:42,827 main.py:47] epoch 2502, training loss: 2204.19, average training loss: 2160.31, base loss: 2372.06
[INFO 2017-06-26 19:25:43,110 main.py:47] epoch 2503, training loss: 1930.19, average training loss: 2159.88, base loss: 2371.77
[INFO 2017-06-26 19:25:43,394 main.py:47] epoch 2504, training loss: 2135.01, average training loss: 2159.82, base loss: 2371.92
[INFO 2017-06-26 19:25:43,678 main.py:47] epoch 2505, training loss: 2339.11, average training loss: 2159.82, base loss: 2371.96
[INFO 2017-06-26 19:25:43,962 main.py:47] epoch 2506, training loss: 1993.83, average training loss: 2159.76, base loss: 2372.04
[INFO 2017-06-26 19:25:44,247 main.py:47] epoch 2507, training loss: 2096.03, average training loss: 2159.83, base loss: 2372.13
[INFO 2017-06-26 19:25:44,528 main.py:47] epoch 2508, training loss: 2145.78, average training loss: 2160.00, base loss: 2372.33
[INFO 2017-06-26 19:25:44,809 main.py:47] epoch 2509, training loss: 1895.85, average training loss: 2159.83, base loss: 2372.16
[INFO 2017-06-26 19:25:45,097 main.py:47] epoch 2510, training loss: 1955.33, average training loss: 2159.67, base loss: 2372.01
[INFO 2017-06-26 19:25:45,385 main.py:47] epoch 2511, training loss: 1931.96, average training loss: 2159.31, base loss: 2371.47
[INFO 2017-06-26 19:25:45,667 main.py:47] epoch 2512, training loss: 2149.76, average training loss: 2159.30, base loss: 2371.53
[INFO 2017-06-26 19:25:45,956 main.py:47] epoch 2513, training loss: 1971.01, average training loss: 2158.80, base loss: 2371.13
[INFO 2017-06-26 19:25:46,243 main.py:47] epoch 2514, training loss: 2527.44, average training loss: 2159.05, base loss: 2371.66
[INFO 2017-06-26 19:25:46,526 main.py:47] epoch 2515, training loss: 2067.03, average training loss: 2159.10, base loss: 2371.82
[INFO 2017-06-26 19:25:46,813 main.py:47] epoch 2516, training loss: 2119.66, average training loss: 2158.66, base loss: 2371.45
[INFO 2017-06-26 19:25:47,099 main.py:47] epoch 2517, training loss: 2383.22, average training loss: 2159.12, base loss: 2372.07
[INFO 2017-06-26 19:25:47,384 main.py:47] epoch 2518, training loss: 2325.50, average training loss: 2158.87, base loss: 2371.90
[INFO 2017-06-26 19:25:47,668 main.py:47] epoch 2519, training loss: 2299.47, average training loss: 2158.37, base loss: 2371.41
[INFO 2017-06-26 19:25:47,952 main.py:47] epoch 2520, training loss: 2605.74, average training loss: 2158.68, base loss: 2371.79
[INFO 2017-06-26 19:25:48,231 main.py:47] epoch 2521, training loss: 2361.14, average training loss: 2158.32, base loss: 2371.23
[INFO 2017-06-26 19:25:48,512 main.py:47] epoch 2522, training loss: 2441.88, average training loss: 2158.48, base loss: 2371.40
[INFO 2017-06-26 19:25:48,799 main.py:47] epoch 2523, training loss: 2014.12, average training loss: 2158.44, base loss: 2371.44
[INFO 2017-06-26 19:25:49,083 main.py:47] epoch 2524, training loss: 2221.54, average training loss: 2158.53, base loss: 2371.88
[INFO 2017-06-26 19:25:49,367 main.py:47] epoch 2525, training loss: 1940.95, average training loss: 2158.59, base loss: 2372.17
[INFO 2017-06-26 19:25:49,652 main.py:47] epoch 2526, training loss: 2098.10, average training loss: 2158.69, base loss: 2372.28
[INFO 2017-06-26 19:25:49,940 main.py:47] epoch 2527, training loss: 2392.87, average training loss: 2159.19, base loss: 2373.00
[INFO 2017-06-26 19:25:50,224 main.py:47] epoch 2528, training loss: 2395.48, average training loss: 2159.26, base loss: 2373.20
[INFO 2017-06-26 19:25:50,512 main.py:47] epoch 2529, training loss: 1996.18, average training loss: 2158.79, base loss: 2372.82
[INFO 2017-06-26 19:25:50,800 main.py:47] epoch 2530, training loss: 2166.96, average training loss: 2158.80, base loss: 2372.86
[INFO 2017-06-26 19:25:51,087 main.py:47] epoch 2531, training loss: 2170.42, average training loss: 2159.26, base loss: 2373.43
[INFO 2017-06-26 19:25:51,372 main.py:47] epoch 2532, training loss: 2160.89, average training loss: 2159.28, base loss: 2373.44
[INFO 2017-06-26 19:25:51,660 main.py:47] epoch 2533, training loss: 1830.33, average training loss: 2158.79, base loss: 2373.01
[INFO 2017-06-26 19:25:51,946 main.py:47] epoch 2534, training loss: 2222.21, average training loss: 2158.80, base loss: 2373.05
[INFO 2017-06-26 19:25:52,230 main.py:47] epoch 2535, training loss: 2713.91, average training loss: 2159.60, base loss: 2373.90
[INFO 2017-06-26 19:25:52,511 main.py:47] epoch 2536, training loss: 1823.06, average training loss: 2159.16, base loss: 2373.53
[INFO 2017-06-26 19:25:52,792 main.py:47] epoch 2537, training loss: 2217.41, average training loss: 2159.33, base loss: 2373.75
[INFO 2017-06-26 19:25:53,073 main.py:47] epoch 2538, training loss: 1930.65, average training loss: 2158.63, base loss: 2373.05
[INFO 2017-06-26 19:25:53,358 main.py:47] epoch 2539, training loss: 2232.39, average training loss: 2158.91, base loss: 2373.50
[INFO 2017-06-26 19:25:53,646 main.py:47] epoch 2540, training loss: 2127.60, average training loss: 2158.89, base loss: 2373.46
[INFO 2017-06-26 19:25:53,931 main.py:47] epoch 2541, training loss: 2028.68, average training loss: 2158.38, base loss: 2373.26
[INFO 2017-06-26 19:25:54,216 main.py:47] epoch 2542, training loss: 2087.58, average training loss: 2158.11, base loss: 2373.07
[INFO 2017-06-26 19:25:54,497 main.py:47] epoch 2543, training loss: 2485.41, average training loss: 2158.75, base loss: 2373.85
[INFO 2017-06-26 19:25:54,781 main.py:47] epoch 2544, training loss: 2093.53, average training loss: 2158.74, base loss: 2373.78
[INFO 2017-06-26 19:25:55,062 main.py:47] epoch 2545, training loss: 1907.97, average training loss: 2158.13, base loss: 2373.23
[INFO 2017-06-26 19:25:55,348 main.py:47] epoch 2546, training loss: 2376.73, average training loss: 2158.28, base loss: 2373.42
[INFO 2017-06-26 19:25:55,635 main.py:47] epoch 2547, training loss: 2490.84, average training loss: 2158.09, base loss: 2373.14
[INFO 2017-06-26 19:25:55,917 main.py:47] epoch 2548, training loss: 2105.91, average training loss: 2158.21, base loss: 2373.33
[INFO 2017-06-26 19:25:56,198 main.py:47] epoch 2549, training loss: 1734.45, average training loss: 2157.41, base loss: 2372.44
[INFO 2017-06-26 19:25:56,478 main.py:47] epoch 2550, training loss: 2217.98, average training loss: 2157.61, base loss: 2372.95
[INFO 2017-06-26 19:25:56,758 main.py:47] epoch 2551, training loss: 1996.61, average training loss: 2157.39, base loss: 2372.72
[INFO 2017-06-26 19:25:57,039 main.py:47] epoch 2552, training loss: 1976.16, average training loss: 2157.38, base loss: 2372.72
[INFO 2017-06-26 19:25:57,323 main.py:47] epoch 2553, training loss: 2015.52, average training loss: 2157.06, base loss: 2372.24
[INFO 2017-06-26 19:25:57,605 main.py:47] epoch 2554, training loss: 2360.78, average training loss: 2157.56, base loss: 2372.94
[INFO 2017-06-26 19:25:57,886 main.py:47] epoch 2555, training loss: 1880.95, average training loss: 2157.19, base loss: 2372.44
[INFO 2017-06-26 19:25:58,173 main.py:47] epoch 2556, training loss: 2285.90, average training loss: 2157.39, base loss: 2372.67
[INFO 2017-06-26 19:25:58,457 main.py:47] epoch 2557, training loss: 1849.35, average training loss: 2157.00, base loss: 2372.29
[INFO 2017-06-26 19:25:58,744 main.py:47] epoch 2558, training loss: 2112.93, average training loss: 2157.02, base loss: 2372.49
[INFO 2017-06-26 19:25:59,028 main.py:47] epoch 2559, training loss: 2281.66, average training loss: 2157.14, base loss: 2372.77
[INFO 2017-06-26 19:25:59,311 main.py:47] epoch 2560, training loss: 1925.47, average training loss: 2156.62, base loss: 2372.34
[INFO 2017-06-26 19:25:59,597 main.py:47] epoch 2561, training loss: 2149.22, average training loss: 2156.35, base loss: 2371.86
[INFO 2017-06-26 19:25:59,884 main.py:47] epoch 2562, training loss: 1780.82, average training loss: 2156.08, base loss: 2371.48
[INFO 2017-06-26 19:26:00,172 main.py:47] epoch 2563, training loss: 1824.19, average training loss: 2155.81, base loss: 2371.16
[INFO 2017-06-26 19:26:00,459 main.py:47] epoch 2564, training loss: 2057.42, average training loss: 2155.61, base loss: 2371.14
[INFO 2017-06-26 19:26:00,747 main.py:47] epoch 2565, training loss: 1950.99, average training loss: 2155.59, base loss: 2371.22
[INFO 2017-06-26 19:26:01,031 main.py:47] epoch 2566, training loss: 2185.68, average training loss: 2155.54, base loss: 2371.23
[INFO 2017-06-26 19:26:01,317 main.py:47] epoch 2567, training loss: 1978.85, average training loss: 2155.30, base loss: 2371.05
[INFO 2017-06-26 19:26:01,600 main.py:47] epoch 2568, training loss: 1837.28, average training loss: 2154.58, base loss: 2370.30
[INFO 2017-06-26 19:26:01,884 main.py:47] epoch 2569, training loss: 1955.92, average training loss: 2154.43, base loss: 2370.08
[INFO 2017-06-26 19:26:02,168 main.py:47] epoch 2570, training loss: 1802.38, average training loss: 2153.94, base loss: 2369.73
[INFO 2017-06-26 19:26:02,453 main.py:47] epoch 2571, training loss: 2072.07, average training loss: 2153.87, base loss: 2369.84
[INFO 2017-06-26 19:26:02,736 main.py:47] epoch 2572, training loss: 2096.31, average training loss: 2153.88, base loss: 2370.05
[INFO 2017-06-26 19:26:03,016 main.py:47] epoch 2573, training loss: 2507.79, average training loss: 2154.41, base loss: 2370.80
[INFO 2017-06-26 19:26:03,302 main.py:47] epoch 2574, training loss: 2241.46, average training loss: 2154.28, base loss: 2370.91
[INFO 2017-06-26 19:26:03,586 main.py:47] epoch 2575, training loss: 2349.70, average training loss: 2154.34, base loss: 2371.13
[INFO 2017-06-26 19:26:03,870 main.py:47] epoch 2576, training loss: 1925.49, average training loss: 2154.36, base loss: 2371.28
[INFO 2017-06-26 19:26:04,157 main.py:47] epoch 2577, training loss: 2120.54, average training loss: 2154.54, base loss: 2371.54
[INFO 2017-06-26 19:26:04,438 main.py:47] epoch 2578, training loss: 2139.06, average training loss: 2154.51, base loss: 2371.43
[INFO 2017-06-26 19:26:04,722 main.py:47] epoch 2579, training loss: 2238.16, average training loss: 2154.57, base loss: 2371.60
[INFO 2017-06-26 19:26:05,011 main.py:47] epoch 2580, training loss: 2059.60, average training loss: 2154.69, base loss: 2371.82
[INFO 2017-06-26 19:26:05,295 main.py:47] epoch 2581, training loss: 2546.98, average training loss: 2155.00, base loss: 2372.21
[INFO 2017-06-26 19:26:05,579 main.py:47] epoch 2582, training loss: 2461.88, average training loss: 2155.32, base loss: 2372.46
[INFO 2017-06-26 19:26:05,863 main.py:47] epoch 2583, training loss: 2126.08, average training loss: 2155.15, base loss: 2372.47
[INFO 2017-06-26 19:26:06,147 main.py:47] epoch 2584, training loss: 1839.74, average training loss: 2155.09, base loss: 2372.42
[INFO 2017-06-26 19:26:06,428 main.py:47] epoch 2585, training loss: 2180.92, average training loss: 2154.77, base loss: 2372.22
[INFO 2017-06-26 19:26:06,713 main.py:47] epoch 2586, training loss: 1703.49, average training loss: 2153.94, base loss: 2371.27
[INFO 2017-06-26 19:26:06,997 main.py:47] epoch 2587, training loss: 2158.23, average training loss: 2153.57, base loss: 2370.94
[INFO 2017-06-26 19:26:07,280 main.py:47] epoch 2588, training loss: 1958.67, average training loss: 2153.34, base loss: 2370.69
[INFO 2017-06-26 19:26:07,560 main.py:47] epoch 2589, training loss: 2011.50, average training loss: 2153.36, base loss: 2370.79
[INFO 2017-06-26 19:26:07,841 main.py:47] epoch 2590, training loss: 1455.65, average training loss: 2152.52, base loss: 2369.68
[INFO 2017-06-26 19:26:08,125 main.py:47] epoch 2591, training loss: 2358.41, average training loss: 2152.62, base loss: 2369.87
[INFO 2017-06-26 19:26:08,409 main.py:47] epoch 2592, training loss: 2307.63, average training loss: 2152.82, base loss: 2370.32
[INFO 2017-06-26 19:26:08,691 main.py:47] epoch 2593, training loss: 2128.13, average training loss: 2152.66, base loss: 2370.36
[INFO 2017-06-26 19:26:08,974 main.py:47] epoch 2594, training loss: 2305.26, average training loss: 2152.88, base loss: 2370.66
[INFO 2017-06-26 19:26:09,259 main.py:47] epoch 2595, training loss: 2163.04, average training loss: 2153.22, base loss: 2370.75
[INFO 2017-06-26 19:26:09,542 main.py:47] epoch 2596, training loss: 2105.62, average training loss: 2153.13, base loss: 2370.57
[INFO 2017-06-26 19:26:09,828 main.py:47] epoch 2597, training loss: 2181.97, average training loss: 2153.50, base loss: 2371.08
[INFO 2017-06-26 19:26:10,111 main.py:47] epoch 2598, training loss: 2176.72, average training loss: 2153.33, base loss: 2371.10
[INFO 2017-06-26 19:26:10,395 main.py:47] epoch 2599, training loss: 2276.96, average training loss: 2153.23, base loss: 2371.13
[INFO 2017-06-26 19:26:10,396 main.py:49] epoch 2599, testing
[INFO 2017-06-26 19:26:14,110 main.py:100] average testing loss: 2102.29, base loss: 2351.58
[INFO 2017-06-26 19:26:14,136 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:26:14,418 main.py:47] epoch 2600, training loss: 2021.28, average training loss: 2153.18, base loss: 2371.13
[INFO 2017-06-26 19:26:14,701 main.py:47] epoch 2601, training loss: 2222.49, average training loss: 2153.05, base loss: 2371.17
[INFO 2017-06-26 19:26:14,982 main.py:47] epoch 2602, training loss: 2186.38, average training loss: 2153.08, base loss: 2371.12
[INFO 2017-06-26 19:26:15,270 main.py:47] epoch 2603, training loss: 2215.77, average training loss: 2152.98, base loss: 2371.24
[INFO 2017-06-26 19:26:15,556 main.py:47] epoch 2604, training loss: 2208.02, average training loss: 2153.07, base loss: 2371.51
[INFO 2017-06-26 19:26:15,841 main.py:47] epoch 2605, training loss: 2140.98, average training loss: 2153.36, base loss: 2371.87
[INFO 2017-06-26 19:26:16,125 main.py:47] epoch 2606, training loss: 1895.77, average training loss: 2152.80, base loss: 2371.42
[INFO 2017-06-26 19:26:16,410 main.py:47] epoch 2607, training loss: 2070.34, average training loss: 2152.42, base loss: 2370.95
[INFO 2017-06-26 19:26:16,694 main.py:47] epoch 2608, training loss: 2205.83, average training loss: 2152.60, base loss: 2371.27
[INFO 2017-06-26 19:26:16,982 main.py:47] epoch 2609, training loss: 2317.85, average training loss: 2152.90, base loss: 2371.52
[INFO 2017-06-26 19:26:17,267 main.py:47] epoch 2610, training loss: 1641.68, average training loss: 2152.80, base loss: 2371.45
[INFO 2017-06-26 19:26:17,554 main.py:47] epoch 2611, training loss: 2147.07, average training loss: 2153.20, base loss: 2372.07
[INFO 2017-06-26 19:26:17,841 main.py:47] epoch 2612, training loss: 2238.23, average training loss: 2153.23, base loss: 2372.26
[INFO 2017-06-26 19:26:18,129 main.py:47] epoch 2613, training loss: 2462.44, average training loss: 2153.80, base loss: 2372.95
[INFO 2017-06-26 19:26:18,416 main.py:47] epoch 2614, training loss: 1894.66, average training loss: 2153.52, base loss: 2372.81
[INFO 2017-06-26 19:26:18,705 main.py:47] epoch 2615, training loss: 2288.36, average training loss: 2153.75, base loss: 2373.05
[INFO 2017-06-26 19:26:18,993 main.py:47] epoch 2616, training loss: 2132.09, average training loss: 2153.44, base loss: 2372.91
[INFO 2017-06-26 19:26:19,280 main.py:47] epoch 2617, training loss: 1905.16, average training loss: 2153.47, base loss: 2373.10
[INFO 2017-06-26 19:26:19,563 main.py:47] epoch 2618, training loss: 2300.70, average training loss: 2153.45, base loss: 2373.23
[INFO 2017-06-26 19:26:19,847 main.py:47] epoch 2619, training loss: 2529.52, average training loss: 2153.98, base loss: 2373.92
[INFO 2017-06-26 19:26:20,130 main.py:47] epoch 2620, training loss: 1941.46, average training loss: 2153.83, base loss: 2373.81
[INFO 2017-06-26 19:26:20,416 main.py:47] epoch 2621, training loss: 1957.03, average training loss: 2153.82, base loss: 2373.81
[INFO 2017-06-26 19:26:20,701 main.py:47] epoch 2622, training loss: 2232.88, average training loss: 2153.81, base loss: 2373.87
[INFO 2017-06-26 19:26:20,984 main.py:47] epoch 2623, training loss: 1788.81, average training loss: 2153.43, base loss: 2373.57
[INFO 2017-06-26 19:26:21,268 main.py:47] epoch 2624, training loss: 2089.92, average training loss: 2153.76, base loss: 2374.12
[INFO 2017-06-26 19:26:21,549 main.py:47] epoch 2625, training loss: 2290.13, average training loss: 2153.84, base loss: 2374.19
[INFO 2017-06-26 19:26:21,836 main.py:47] epoch 2626, training loss: 2269.99, average training loss: 2153.91, base loss: 2374.35
[INFO 2017-06-26 19:26:22,119 main.py:47] epoch 2627, training loss: 1792.48, average training loss: 2153.77, base loss: 2374.12
[INFO 2017-06-26 19:26:22,400 main.py:47] epoch 2628, training loss: 2126.40, average training loss: 2154.08, base loss: 2374.48
[INFO 2017-06-26 19:26:22,689 main.py:47] epoch 2629, training loss: 2285.37, average training loss: 2154.07, base loss: 2374.34
[INFO 2017-06-26 19:26:22,976 main.py:47] epoch 2630, training loss: 2301.10, average training loss: 2154.33, base loss: 2374.69
[INFO 2017-06-26 19:26:23,258 main.py:47] epoch 2631, training loss: 1995.87, average training loss: 2154.08, base loss: 2374.46
[INFO 2017-06-26 19:26:23,541 main.py:47] epoch 2632, training loss: 2678.07, average training loss: 2154.48, base loss: 2374.78
[INFO 2017-06-26 19:26:23,825 main.py:47] epoch 2633, training loss: 2269.62, average training loss: 2154.78, base loss: 2375.10
[INFO 2017-06-26 19:26:24,113 main.py:47] epoch 2634, training loss: 2788.20, average training loss: 2155.56, base loss: 2375.96
[INFO 2017-06-26 19:26:24,396 main.py:47] epoch 2635, training loss: 1960.40, average training loss: 2155.24, base loss: 2375.70
[INFO 2017-06-26 19:26:24,685 main.py:47] epoch 2636, training loss: 1766.80, average training loss: 2154.60, base loss: 2374.84
[INFO 2017-06-26 19:26:24,968 main.py:47] epoch 2637, training loss: 1946.75, average training loss: 2154.39, base loss: 2374.52
[INFO 2017-06-26 19:26:25,251 main.py:47] epoch 2638, training loss: 2009.18, average training loss: 2153.83, base loss: 2374.00
[INFO 2017-06-26 19:26:25,532 main.py:47] epoch 2639, training loss: 2019.27, average training loss: 2153.58, base loss: 2373.91
[INFO 2017-06-26 19:26:25,813 main.py:47] epoch 2640, training loss: 1927.20, average training loss: 2153.30, base loss: 2373.77
[INFO 2017-06-26 19:26:26,094 main.py:47] epoch 2641, training loss: 2225.33, average training loss: 2153.67, base loss: 2374.28
[INFO 2017-06-26 19:26:26,378 main.py:47] epoch 2642, training loss: 1842.29, average training loss: 2153.61, base loss: 2374.35
[INFO 2017-06-26 19:26:26,666 main.py:47] epoch 2643, training loss: 2245.32, average training loss: 2153.80, base loss: 2374.53
[INFO 2017-06-26 19:26:26,950 main.py:47] epoch 2644, training loss: 2579.94, average training loss: 2154.58, base loss: 2375.51
[INFO 2017-06-26 19:26:27,234 main.py:47] epoch 2645, training loss: 2340.63, average training loss: 2154.85, base loss: 2375.94
[INFO 2017-06-26 19:26:27,518 main.py:47] epoch 2646, training loss: 2105.61, average training loss: 2155.08, base loss: 2376.16
[INFO 2017-06-26 19:26:27,800 main.py:47] epoch 2647, training loss: 2226.26, average training loss: 2154.96, base loss: 2375.85
[INFO 2017-06-26 19:26:28,080 main.py:47] epoch 2648, training loss: 1933.94, average training loss: 2154.13, base loss: 2375.05
[INFO 2017-06-26 19:26:28,364 main.py:47] epoch 2649, training loss: 2419.76, average training loss: 2154.50, base loss: 2375.56
[INFO 2017-06-26 19:26:28,652 main.py:47] epoch 2650, training loss: 2459.93, average training loss: 2154.58, base loss: 2375.83
[INFO 2017-06-26 19:26:28,933 main.py:47] epoch 2651, training loss: 2044.91, average training loss: 2154.49, base loss: 2375.82
[INFO 2017-06-26 19:26:29,214 main.py:47] epoch 2652, training loss: 2726.16, average training loss: 2155.21, base loss: 2376.57
[INFO 2017-06-26 19:26:29,499 main.py:47] epoch 2653, training loss: 2719.70, average training loss: 2155.92, base loss: 2377.54
[INFO 2017-06-26 19:26:29,780 main.py:47] epoch 2654, training loss: 1648.36, average training loss: 2155.36, base loss: 2377.06
[INFO 2017-06-26 19:26:30,065 main.py:47] epoch 2655, training loss: 1953.49, average training loss: 2155.30, base loss: 2377.04
[INFO 2017-06-26 19:26:30,346 main.py:47] epoch 2656, training loss: 2029.53, average training loss: 2155.34, base loss: 2377.32
[INFO 2017-06-26 19:26:30,632 main.py:47] epoch 2657, training loss: 2158.01, average training loss: 2155.52, base loss: 2377.33
[INFO 2017-06-26 19:26:30,914 main.py:47] epoch 2658, training loss: 1712.88, average training loss: 2155.50, base loss: 2377.39
[INFO 2017-06-26 19:26:31,194 main.py:47] epoch 2659, training loss: 2301.16, average training loss: 2155.94, base loss: 2378.03
[INFO 2017-06-26 19:26:31,479 main.py:47] epoch 2660, training loss: 2159.95, average training loss: 2155.39, base loss: 2377.34
[INFO 2017-06-26 19:26:31,766 main.py:47] epoch 2661, training loss: 2131.19, average training loss: 2154.79, base loss: 2376.68
[INFO 2017-06-26 19:26:32,053 main.py:47] epoch 2662, training loss: 2035.27, average training loss: 2154.72, base loss: 2376.63
[INFO 2017-06-26 19:26:32,338 main.py:47] epoch 2663, training loss: 2160.64, average training loss: 2154.80, base loss: 2376.82
[INFO 2017-06-26 19:26:32,625 main.py:47] epoch 2664, training loss: 2416.67, average training loss: 2155.47, base loss: 2377.58
[INFO 2017-06-26 19:26:32,914 main.py:47] epoch 2665, training loss: 1814.58, average training loss: 2154.64, base loss: 2376.54
[INFO 2017-06-26 19:26:33,197 main.py:47] epoch 2666, training loss: 2476.00, average training loss: 2154.63, base loss: 2376.52
[INFO 2017-06-26 19:26:33,486 main.py:47] epoch 2667, training loss: 1991.05, average training loss: 2154.37, base loss: 2376.21
[INFO 2017-06-26 19:26:33,770 main.py:47] epoch 2668, training loss: 2365.90, average training loss: 2154.38, base loss: 2376.27
[INFO 2017-06-26 19:26:34,054 main.py:47] epoch 2669, training loss: 1936.46, average training loss: 2154.08, base loss: 2375.86
[INFO 2017-06-26 19:26:34,339 main.py:47] epoch 2670, training loss: 2242.57, average training loss: 2154.17, base loss: 2376.19
[INFO 2017-06-26 19:26:34,619 main.py:47] epoch 2671, training loss: 2147.45, average training loss: 2154.11, base loss: 2376.22
[INFO 2017-06-26 19:26:34,903 main.py:47] epoch 2672, training loss: 2091.44, average training loss: 2153.79, base loss: 2375.91
[INFO 2017-06-26 19:26:35,188 main.py:47] epoch 2673, training loss: 2083.26, average training loss: 2153.90, base loss: 2376.20
[INFO 2017-06-26 19:26:35,476 main.py:47] epoch 2674, training loss: 1999.16, average training loss: 2153.97, base loss: 2376.28
[INFO 2017-06-26 19:26:35,757 main.py:47] epoch 2675, training loss: 2092.68, average training loss: 2154.12, base loss: 2376.74
[INFO 2017-06-26 19:26:36,040 main.py:47] epoch 2676, training loss: 2127.30, average training loss: 2154.21, base loss: 2376.88
[INFO 2017-06-26 19:26:36,324 main.py:47] epoch 2677, training loss: 2101.24, average training loss: 2154.30, base loss: 2377.01
[INFO 2017-06-26 19:26:36,604 main.py:47] epoch 2678, training loss: 2327.20, average training loss: 2154.30, base loss: 2377.17
[INFO 2017-06-26 19:26:36,885 main.py:47] epoch 2679, training loss: 2232.90, average training loss: 2153.93, base loss: 2376.84
[INFO 2017-06-26 19:26:37,166 main.py:47] epoch 2680, training loss: 1788.03, average training loss: 2153.28, base loss: 2376.10
[INFO 2017-06-26 19:26:37,452 main.py:47] epoch 2681, training loss: 2289.27, average training loss: 2153.62, base loss: 2376.73
[INFO 2017-06-26 19:26:37,733 main.py:47] epoch 2682, training loss: 1894.44, average training loss: 2153.18, base loss: 2376.47
[INFO 2017-06-26 19:26:38,015 main.py:47] epoch 2683, training loss: 2230.07, average training loss: 2153.53, base loss: 2377.13
[INFO 2017-06-26 19:26:38,297 main.py:47] epoch 2684, training loss: 2281.07, average training loss: 2153.68, base loss: 2377.34
[INFO 2017-06-26 19:26:38,585 main.py:47] epoch 2685, training loss: 2397.50, average training loss: 2153.63, base loss: 2377.52
[INFO 2017-06-26 19:26:38,868 main.py:47] epoch 2686, training loss: 1822.85, average training loss: 2153.51, base loss: 2377.50
[INFO 2017-06-26 19:26:39,156 main.py:47] epoch 2687, training loss: 2412.34, average training loss: 2153.86, base loss: 2378.11
[INFO 2017-06-26 19:26:39,438 main.py:47] epoch 2688, training loss: 2021.79, average training loss: 2154.10, base loss: 2378.42
[INFO 2017-06-26 19:26:39,721 main.py:47] epoch 2689, training loss: 2055.35, average training loss: 2153.79, base loss: 2378.10
[INFO 2017-06-26 19:26:40,006 main.py:47] epoch 2690, training loss: 2219.70, average training loss: 2154.05, base loss: 2378.55
[INFO 2017-06-26 19:26:40,291 main.py:47] epoch 2691, training loss: 1989.85, average training loss: 2154.24, base loss: 2378.89
[INFO 2017-06-26 19:26:40,575 main.py:47] epoch 2692, training loss: 2273.09, average training loss: 2154.36, base loss: 2378.95
[INFO 2017-06-26 19:26:40,862 main.py:47] epoch 2693, training loss: 1928.76, average training loss: 2154.37, base loss: 2379.08
[INFO 2017-06-26 19:26:41,150 main.py:47] epoch 2694, training loss: 1873.39, average training loss: 2154.06, base loss: 2378.64
[INFO 2017-06-26 19:26:41,436 main.py:47] epoch 2695, training loss: 2274.74, average training loss: 2154.11, base loss: 2378.90
[INFO 2017-06-26 19:26:41,717 main.py:47] epoch 2696, training loss: 1772.78, average training loss: 2154.00, base loss: 2378.82
[INFO 2017-06-26 19:26:41,998 main.py:47] epoch 2697, training loss: 1770.91, average training loss: 2153.84, base loss: 2378.68
[INFO 2017-06-26 19:26:42,281 main.py:47] epoch 2698, training loss: 2231.49, average training loss: 2154.26, base loss: 2379.21
[INFO 2017-06-26 19:26:42,567 main.py:47] epoch 2699, training loss: 2250.92, average training loss: 2154.15, base loss: 2379.09
[INFO 2017-06-26 19:26:42,568 main.py:49] epoch 2699, testing
[INFO 2017-06-26 19:26:46,261 main.py:100] average testing loss: 2162.42, base loss: 2390.77
[INFO 2017-06-26 19:26:46,287 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:26:46,574 main.py:47] epoch 2700, training loss: 2193.12, average training loss: 2154.04, base loss: 2378.98
[INFO 2017-06-26 19:26:46,859 main.py:47] epoch 2701, training loss: 2472.25, average training loss: 2153.98, base loss: 2378.89
[INFO 2017-06-26 19:26:47,143 main.py:47] epoch 2702, training loss: 2019.71, average training loss: 2153.93, base loss: 2378.90
[INFO 2017-06-26 19:26:47,432 main.py:47] epoch 2703, training loss: 1797.26, average training loss: 2153.71, base loss: 2378.61
[INFO 2017-06-26 19:26:47,713 main.py:47] epoch 2704, training loss: 1709.04, average training loss: 2153.27, base loss: 2378.01
[INFO 2017-06-26 19:26:47,993 main.py:47] epoch 2705, training loss: 2257.41, average training loss: 2153.31, base loss: 2378.00
[INFO 2017-06-26 19:26:48,273 main.py:47] epoch 2706, training loss: 1913.10, average training loss: 2153.03, base loss: 2377.83
[INFO 2017-06-26 19:26:48,559 main.py:47] epoch 2707, training loss: 1706.99, average training loss: 2152.79, base loss: 2377.75
[INFO 2017-06-26 19:26:48,843 main.py:47] epoch 2708, training loss: 2100.10, average training loss: 2153.03, base loss: 2378.08
[INFO 2017-06-26 19:26:49,127 main.py:47] epoch 2709, training loss: 2392.09, average training loss: 2153.46, base loss: 2378.60
[INFO 2017-06-26 19:26:49,411 main.py:47] epoch 2710, training loss: 1909.16, average training loss: 2152.98, base loss: 2378.13
[INFO 2017-06-26 19:26:49,692 main.py:47] epoch 2711, training loss: 2257.89, average training loss: 2153.38, base loss: 2378.71
[INFO 2017-06-26 19:26:49,976 main.py:47] epoch 2712, training loss: 2180.86, average training loss: 2153.67, base loss: 2379.20
[INFO 2017-06-26 19:26:50,258 main.py:47] epoch 2713, training loss: 1877.68, average training loss: 2153.17, base loss: 2378.74
[INFO 2017-06-26 19:26:50,542 main.py:47] epoch 2714, training loss: 2170.98, average training loss: 2153.11, base loss: 2378.89
[INFO 2017-06-26 19:26:50,827 main.py:47] epoch 2715, training loss: 1991.76, average training loss: 2152.68, base loss: 2378.38
[INFO 2017-06-26 19:26:51,116 main.py:47] epoch 2716, training loss: 2138.18, average training loss: 2152.75, base loss: 2378.44
[INFO 2017-06-26 19:26:51,405 main.py:47] epoch 2717, training loss: 2039.39, average training loss: 2152.69, base loss: 2378.42
[INFO 2017-06-26 19:26:51,693 main.py:47] epoch 2718, training loss: 1859.77, average training loss: 2152.62, base loss: 2378.38
[INFO 2017-06-26 19:26:51,981 main.py:47] epoch 2719, training loss: 2055.11, average training loss: 2152.20, base loss: 2377.97
[INFO 2017-06-26 19:26:52,262 main.py:47] epoch 2720, training loss: 2423.06, average training loss: 2152.89, base loss: 2378.88
[INFO 2017-06-26 19:26:52,547 main.py:47] epoch 2721, training loss: 1800.97, average training loss: 2152.89, base loss: 2378.98
[INFO 2017-06-26 19:26:52,829 main.py:47] epoch 2722, training loss: 2645.80, average training loss: 2153.46, base loss: 2379.65
[INFO 2017-06-26 19:26:53,114 main.py:47] epoch 2723, training loss: 2191.55, average training loss: 2153.83, base loss: 2380.16
[INFO 2017-06-26 19:26:53,403 main.py:47] epoch 2724, training loss: 2135.90, average training loss: 2153.71, base loss: 2380.10
[INFO 2017-06-26 19:26:53,690 main.py:47] epoch 2725, training loss: 2262.60, average training loss: 2153.75, base loss: 2380.17
[INFO 2017-06-26 19:26:53,979 main.py:47] epoch 2726, training loss: 1600.68, average training loss: 2153.21, base loss: 2379.50
[INFO 2017-06-26 19:26:54,263 main.py:47] epoch 2727, training loss: 2028.20, average training loss: 2152.82, base loss: 2379.17
[INFO 2017-06-26 19:26:54,547 main.py:47] epoch 2728, training loss: 2493.64, average training loss: 2153.28, base loss: 2379.70
[INFO 2017-06-26 19:26:54,830 main.py:47] epoch 2729, training loss: 2300.50, average training loss: 2153.51, base loss: 2379.95
[INFO 2017-06-26 19:26:55,117 main.py:47] epoch 2730, training loss: 2234.09, average training loss: 2153.95, base loss: 2380.45
[INFO 2017-06-26 19:26:55,407 main.py:47] epoch 2731, training loss: 2278.06, average training loss: 2154.28, base loss: 2380.81
[INFO 2017-06-26 19:26:55,691 main.py:47] epoch 2732, training loss: 2066.52, average training loss: 2154.15, base loss: 2380.48
[INFO 2017-06-26 19:26:55,975 main.py:47] epoch 2733, training loss: 2389.64, average training loss: 2154.44, base loss: 2381.01
[INFO 2017-06-26 19:26:56,259 main.py:47] epoch 2734, training loss: 1908.37, average training loss: 2154.35, base loss: 2380.71
[INFO 2017-06-26 19:26:56,546 main.py:47] epoch 2735, training loss: 1906.93, average training loss: 2153.78, base loss: 2380.18
[INFO 2017-06-26 19:26:56,829 main.py:47] epoch 2736, training loss: 2043.27, average training loss: 2153.89, base loss: 2380.39
[INFO 2017-06-26 19:26:57,118 main.py:47] epoch 2737, training loss: 1784.14, average training loss: 2153.45, base loss: 2379.90
[INFO 2017-06-26 19:26:57,402 main.py:47] epoch 2738, training loss: 2750.66, average training loss: 2154.21, base loss: 2380.84
[INFO 2017-06-26 19:26:57,692 main.py:47] epoch 2739, training loss: 1763.56, average training loss: 2153.65, base loss: 2380.18
[INFO 2017-06-26 19:26:57,978 main.py:47] epoch 2740, training loss: 1549.13, average training loss: 2152.96, base loss: 2379.36
[INFO 2017-06-26 19:26:58,262 main.py:47] epoch 2741, training loss: 2103.02, average training loss: 2152.83, base loss: 2379.42
[INFO 2017-06-26 19:26:58,549 main.py:47] epoch 2742, training loss: 2154.86, average training loss: 2152.97, base loss: 2379.71
[INFO 2017-06-26 19:26:58,836 main.py:47] epoch 2743, training loss: 2290.82, average training loss: 2153.17, base loss: 2380.11
[INFO 2017-06-26 19:26:59,124 main.py:47] epoch 2744, training loss: 2014.11, average training loss: 2152.76, base loss: 2379.45
[INFO 2017-06-26 19:26:59,409 main.py:47] epoch 2745, training loss: 2078.92, average training loss: 2152.56, base loss: 2379.29
[INFO 2017-06-26 19:26:59,692 main.py:47] epoch 2746, training loss: 2516.37, average training loss: 2152.69, base loss: 2379.36
[INFO 2017-06-26 19:26:59,974 main.py:47] epoch 2747, training loss: 2240.93, average training loss: 2153.00, base loss: 2379.76
[INFO 2017-06-26 19:27:00,262 main.py:47] epoch 2748, training loss: 2199.33, average training loss: 2153.26, base loss: 2380.09
[INFO 2017-06-26 19:27:00,546 main.py:47] epoch 2749, training loss: 2112.46, average training loss: 2152.89, base loss: 2379.68
[INFO 2017-06-26 19:27:00,833 main.py:47] epoch 2750, training loss: 1816.67, average training loss: 2152.56, base loss: 2379.15
[INFO 2017-06-26 19:27:01,116 main.py:47] epoch 2751, training loss: 2029.09, average training loss: 2152.45, base loss: 2379.13
[INFO 2017-06-26 19:27:01,400 main.py:47] epoch 2752, training loss: 1765.88, average training loss: 2151.66, base loss: 2378.28
[INFO 2017-06-26 19:27:01,689 main.py:47] epoch 2753, training loss: 2596.29, average training loss: 2152.26, base loss: 2379.14
[INFO 2017-06-26 19:27:01,974 main.py:47] epoch 2754, training loss: 1873.51, average training loss: 2152.13, base loss: 2379.20
[INFO 2017-06-26 19:27:02,258 main.py:47] epoch 2755, training loss: 2582.08, average training loss: 2152.57, base loss: 2379.56
[INFO 2017-06-26 19:27:02,540 main.py:47] epoch 2756, training loss: 2177.40, average training loss: 2152.65, base loss: 2379.60
[INFO 2017-06-26 19:27:02,823 main.py:47] epoch 2757, training loss: 2036.74, average training loss: 2152.61, base loss: 2379.61
[INFO 2017-06-26 19:27:03,107 main.py:47] epoch 2758, training loss: 2647.87, average training loss: 2153.00, base loss: 2380.22
[INFO 2017-06-26 19:27:03,393 main.py:47] epoch 2759, training loss: 2200.60, average training loss: 2153.06, base loss: 2380.35
[INFO 2017-06-26 19:27:03,679 main.py:47] epoch 2760, training loss: 1951.59, average training loss: 2152.43, base loss: 2379.74
[INFO 2017-06-26 19:27:03,961 main.py:47] epoch 2761, training loss: 2122.75, average training loss: 2152.47, base loss: 2379.95
[INFO 2017-06-26 19:27:04,245 main.py:47] epoch 2762, training loss: 2132.60, average training loss: 2152.96, base loss: 2380.67
[INFO 2017-06-26 19:27:04,529 main.py:47] epoch 2763, training loss: 2462.04, average training loss: 2153.29, base loss: 2381.25
[INFO 2017-06-26 19:27:04,815 main.py:47] epoch 2764, training loss: 1931.09, average training loss: 2153.27, base loss: 2381.28
[INFO 2017-06-26 19:27:05,097 main.py:47] epoch 2765, training loss: 2492.42, average training loss: 2153.85, base loss: 2381.98
[INFO 2017-06-26 19:27:05,381 main.py:47] epoch 2766, training loss: 2485.24, average training loss: 2154.32, base loss: 2382.78
[INFO 2017-06-26 19:27:05,665 main.py:47] epoch 2767, training loss: 2270.21, average training loss: 2154.63, base loss: 2383.06
[INFO 2017-06-26 19:27:05,950 main.py:47] epoch 2768, training loss: 1997.88, average training loss: 2154.49, base loss: 2383.03
[INFO 2017-06-26 19:27:06,234 main.py:47] epoch 2769, training loss: 2140.69, average training loss: 2153.94, base loss: 2382.45
[INFO 2017-06-26 19:27:06,518 main.py:47] epoch 2770, training loss: 2163.40, average training loss: 2153.93, base loss: 2382.55
[INFO 2017-06-26 19:27:06,802 main.py:47] epoch 2771, training loss: 2355.21, average training loss: 2154.43, base loss: 2383.07
[INFO 2017-06-26 19:27:07,086 main.py:47] epoch 2772, training loss: 2513.49, average training loss: 2155.23, base loss: 2384.11
[INFO 2017-06-26 19:27:07,369 main.py:47] epoch 2773, training loss: 1965.64, average training loss: 2154.78, base loss: 2383.68
[INFO 2017-06-26 19:27:07,656 main.py:47] epoch 2774, training loss: 1967.09, average training loss: 2154.74, base loss: 2383.74
[INFO 2017-06-26 19:27:07,944 main.py:47] epoch 2775, training loss: 2148.71, average training loss: 2154.81, base loss: 2383.87
[INFO 2017-06-26 19:27:08,230 main.py:47] epoch 2776, training loss: 1891.02, average training loss: 2154.57, base loss: 2383.74
[INFO 2017-06-26 19:27:08,513 main.py:47] epoch 2777, training loss: 2304.12, average training loss: 2154.58, base loss: 2383.77
[INFO 2017-06-26 19:27:08,797 main.py:47] epoch 2778, training loss: 2026.61, average training loss: 2154.37, base loss: 2383.50
[INFO 2017-06-26 19:27:09,079 main.py:47] epoch 2779, training loss: 2180.67, average training loss: 2154.44, base loss: 2383.61
[INFO 2017-06-26 19:27:09,365 main.py:47] epoch 2780, training loss: 2063.00, average training loss: 2154.25, base loss: 2383.35
[INFO 2017-06-26 19:27:09,651 main.py:47] epoch 2781, training loss: 2619.40, average training loss: 2154.59, base loss: 2383.79
[INFO 2017-06-26 19:27:09,934 main.py:47] epoch 2782, training loss: 1921.85, average training loss: 2154.48, base loss: 2383.79
[INFO 2017-06-26 19:27:10,220 main.py:47] epoch 2783, training loss: 2183.88, average training loss: 2154.03, base loss: 2383.20
[INFO 2017-06-26 19:27:10,507 main.py:47] epoch 2784, training loss: 2185.99, average training loss: 2153.42, base loss: 2382.46
[INFO 2017-06-26 19:27:10,790 main.py:47] epoch 2785, training loss: 2196.64, average training loss: 2153.12, base loss: 2382.14
[INFO 2017-06-26 19:27:11,073 main.py:47] epoch 2786, training loss: 2611.53, average training loss: 2153.44, base loss: 2382.44
[INFO 2017-06-26 19:27:11,362 main.py:47] epoch 2787, training loss: 2126.15, average training loss: 2153.25, base loss: 2382.35
[INFO 2017-06-26 19:27:11,645 main.py:47] epoch 2788, training loss: 1997.35, average training loss: 2153.20, base loss: 2382.28
[INFO 2017-06-26 19:27:11,925 main.py:47] epoch 2789, training loss: 2066.83, average training loss: 2153.16, base loss: 2382.17
[INFO 2017-06-26 19:27:12,209 main.py:47] epoch 2790, training loss: 2066.05, average training loss: 2152.81, base loss: 2381.63
[INFO 2017-06-26 19:27:12,492 main.py:47] epoch 2791, training loss: 2119.32, average training loss: 2152.96, base loss: 2381.96
[INFO 2017-06-26 19:27:12,775 main.py:47] epoch 2792, training loss: 2357.49, average training loss: 2152.87, base loss: 2381.94
[INFO 2017-06-26 19:27:13,058 main.py:47] epoch 2793, training loss: 2023.89, average training loss: 2152.99, base loss: 2382.20
[INFO 2017-06-26 19:27:13,344 main.py:47] epoch 2794, training loss: 1836.15, average training loss: 2152.37, base loss: 2381.57
[INFO 2017-06-26 19:27:13,630 main.py:47] epoch 2795, training loss: 2139.41, average training loss: 2151.97, base loss: 2381.13
[INFO 2017-06-26 19:27:13,910 main.py:47] epoch 2796, training loss: 2395.38, average training loss: 2152.06, base loss: 2381.39
[INFO 2017-06-26 19:27:14,191 main.py:47] epoch 2797, training loss: 2154.86, average training loss: 2151.96, base loss: 2381.27
[INFO 2017-06-26 19:27:14,474 main.py:47] epoch 2798, training loss: 2041.50, average training loss: 2151.78, base loss: 2381.14
[INFO 2017-06-26 19:27:14,753 main.py:47] epoch 2799, training loss: 2264.50, average training loss: 2151.56, base loss: 2381.04
[INFO 2017-06-26 19:27:14,753 main.py:49] epoch 2799, testing
[INFO 2017-06-26 19:27:18,517 main.py:100] average testing loss: 2056.71, base loss: 2285.60
[INFO 2017-06-26 19:27:18,541 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:27:18,829 main.py:47] epoch 2800, training loss: 2305.37, average training loss: 2152.02, base loss: 2381.85
[INFO 2017-06-26 19:27:19,113 main.py:47] epoch 2801, training loss: 2213.99, average training loss: 2151.79, base loss: 2381.68
[INFO 2017-06-26 19:27:19,399 main.py:47] epoch 2802, training loss: 1872.58, average training loss: 2151.58, base loss: 2381.56
[INFO 2017-06-26 19:27:19,683 main.py:47] epoch 2803, training loss: 1920.39, average training loss: 2151.30, base loss: 2381.40
[INFO 2017-06-26 19:27:19,963 main.py:47] epoch 2804, training loss: 1896.68, average training loss: 2150.82, base loss: 2380.88
[INFO 2017-06-26 19:27:20,247 main.py:47] epoch 2805, training loss: 1960.08, average training loss: 2150.75, base loss: 2380.79
[INFO 2017-06-26 19:27:20,527 main.py:47] epoch 2806, training loss: 2018.52, average training loss: 2150.74, base loss: 2380.81
[INFO 2017-06-26 19:27:20,812 main.py:47] epoch 2807, training loss: 1877.23, average training loss: 2150.93, base loss: 2381.16
[INFO 2017-06-26 19:27:21,100 main.py:47] epoch 2808, training loss: 2624.49, average training loss: 2151.46, base loss: 2381.85
[INFO 2017-06-26 19:27:21,386 main.py:47] epoch 2809, training loss: 2167.43, average training loss: 2151.25, base loss: 2381.46
[INFO 2017-06-26 19:27:21,667 main.py:47] epoch 2810, training loss: 2139.91, average training loss: 2151.24, base loss: 2381.51
[INFO 2017-06-26 19:27:21,951 main.py:47] epoch 2811, training loss: 2040.82, average training loss: 2151.26, base loss: 2381.56
[INFO 2017-06-26 19:27:22,234 main.py:47] epoch 2812, training loss: 2200.27, average training loss: 2151.33, base loss: 2381.70
[INFO 2017-06-26 19:27:22,515 main.py:47] epoch 2813, training loss: 1652.26, average training loss: 2150.75, base loss: 2381.33
[INFO 2017-06-26 19:27:22,799 main.py:47] epoch 2814, training loss: 2054.50, average training loss: 2150.62, base loss: 2381.19
[INFO 2017-06-26 19:27:23,079 main.py:47] epoch 2815, training loss: 2096.30, average training loss: 2150.28, base loss: 2380.84
[INFO 2017-06-26 19:27:23,366 main.py:47] epoch 2816, training loss: 2097.35, average training loss: 2150.37, base loss: 2381.01
[INFO 2017-06-26 19:27:23,648 main.py:47] epoch 2817, training loss: 2488.58, average training loss: 2150.62, base loss: 2381.20
[INFO 2017-06-26 19:27:23,931 main.py:47] epoch 2818, training loss: 1909.69, average training loss: 2150.59, base loss: 2381.43
[INFO 2017-06-26 19:27:24,212 main.py:47] epoch 2819, training loss: 2086.52, average training loss: 2150.51, base loss: 2381.42
[INFO 2017-06-26 19:27:24,491 main.py:47] epoch 2820, training loss: 1846.56, average training loss: 2150.05, base loss: 2381.06
[INFO 2017-06-26 19:27:24,779 main.py:47] epoch 2821, training loss: 2208.00, average training loss: 2150.06, base loss: 2381.07
[INFO 2017-06-26 19:27:25,063 main.py:47] epoch 2822, training loss: 2038.81, average training loss: 2149.82, base loss: 2380.76
[INFO 2017-06-26 19:27:25,344 main.py:47] epoch 2823, training loss: 2169.92, average training loss: 2149.83, base loss: 2380.87
[INFO 2017-06-26 19:27:25,626 main.py:47] epoch 2824, training loss: 2464.25, average training loss: 2150.54, base loss: 2381.86
[INFO 2017-06-26 19:27:25,905 main.py:47] epoch 2825, training loss: 1984.41, average training loss: 2149.97, base loss: 2381.06
[INFO 2017-06-26 19:27:26,192 main.py:47] epoch 2826, training loss: 2027.57, average training loss: 2149.68, base loss: 2380.97
[INFO 2017-06-26 19:27:26,477 main.py:47] epoch 2827, training loss: 2285.35, average training loss: 2149.50, base loss: 2380.96
[INFO 2017-06-26 19:27:26,763 main.py:47] epoch 2828, training loss: 1799.49, average training loss: 2149.11, base loss: 2380.39
[INFO 2017-06-26 19:27:27,046 main.py:47] epoch 2829, training loss: 1926.02, average training loss: 2148.65, base loss: 2379.90
[INFO 2017-06-26 19:27:27,325 main.py:47] epoch 2830, training loss: 2152.72, average training loss: 2148.26, base loss: 2379.55
[INFO 2017-06-26 19:27:27,608 main.py:47] epoch 2831, training loss: 2003.02, average training loss: 2147.94, base loss: 2379.27
[INFO 2017-06-26 19:27:27,892 main.py:47] epoch 2832, training loss: 2346.59, average training loss: 2148.19, base loss: 2379.69
[INFO 2017-06-26 19:27:28,173 main.py:47] epoch 2833, training loss: 1974.07, average training loss: 2147.83, base loss: 2379.41
[INFO 2017-06-26 19:27:28,456 main.py:47] epoch 2834, training loss: 2064.00, average training loss: 2147.76, base loss: 2379.22
[INFO 2017-06-26 19:27:28,740 main.py:47] epoch 2835, training loss: 1808.40, average training loss: 2146.87, base loss: 2378.22
[INFO 2017-06-26 19:27:29,024 main.py:47] epoch 2836, training loss: 2524.79, average training loss: 2147.21, base loss: 2378.63
[INFO 2017-06-26 19:27:29,309 main.py:47] epoch 2837, training loss: 2190.82, average training loss: 2147.13, base loss: 2378.51
[INFO 2017-06-26 19:27:29,589 main.py:47] epoch 2838, training loss: 2188.67, average training loss: 2147.13, base loss: 2378.47
[INFO 2017-06-26 19:27:29,872 main.py:47] epoch 2839, training loss: 2016.21, average training loss: 2146.87, base loss: 2378.22
[INFO 2017-06-26 19:27:30,157 main.py:47] epoch 2840, training loss: 1749.72, average training loss: 2146.34, base loss: 2377.80
[INFO 2017-06-26 19:27:30,444 main.py:47] epoch 2841, training loss: 2286.84, average training loss: 2146.63, base loss: 2378.06
[INFO 2017-06-26 19:27:30,744 main.py:47] epoch 2842, training loss: 2507.57, average training loss: 2147.05, base loss: 2378.64
[INFO 2017-06-26 19:27:31,035 main.py:47] epoch 2843, training loss: 1862.93, average training loss: 2146.85, base loss: 2378.35
[INFO 2017-06-26 19:27:31,319 main.py:47] epoch 2844, training loss: 1904.89, average training loss: 2146.52, base loss: 2378.02
[INFO 2017-06-26 19:27:31,606 main.py:47] epoch 2845, training loss: 2063.78, average training loss: 2146.04, base loss: 2377.53
[INFO 2017-06-26 19:27:31,893 main.py:47] epoch 2846, training loss: 1982.36, average training loss: 2146.04, base loss: 2377.46
[INFO 2017-06-26 19:27:32,176 main.py:47] epoch 2847, training loss: 2306.09, average training loss: 2145.99, base loss: 2377.56
[INFO 2017-06-26 19:27:32,463 main.py:47] epoch 2848, training loss: 1935.65, average training loss: 2145.61, base loss: 2377.27
[INFO 2017-06-26 19:27:32,748 main.py:47] epoch 2849, training loss: 1750.97, average training loss: 2145.21, base loss: 2377.02
[INFO 2017-06-26 19:27:33,032 main.py:47] epoch 2850, training loss: 2231.62, average training loss: 2145.10, base loss: 2376.93
[INFO 2017-06-26 19:27:33,318 main.py:47] epoch 2851, training loss: 2077.55, average training loss: 2145.31, base loss: 2377.21
[INFO 2017-06-26 19:27:33,605 main.py:47] epoch 2852, training loss: 2297.45, average training loss: 2145.86, base loss: 2377.87
[INFO 2017-06-26 19:27:33,891 main.py:47] epoch 2853, training loss: 2359.42, average training loss: 2145.46, base loss: 2377.53
[INFO 2017-06-26 19:27:34,179 main.py:47] epoch 2854, training loss: 1838.89, average training loss: 2145.13, base loss: 2377.40
[INFO 2017-06-26 19:27:34,464 main.py:47] epoch 2855, training loss: 1978.42, average training loss: 2145.13, base loss: 2377.16
[INFO 2017-06-26 19:27:34,749 main.py:47] epoch 2856, training loss: 2305.19, average training loss: 2144.90, base loss: 2377.03
[INFO 2017-06-26 19:27:35,037 main.py:47] epoch 2857, training loss: 2332.73, average training loss: 2145.05, base loss: 2377.44
[INFO 2017-06-26 19:27:35,323 main.py:47] epoch 2858, training loss: 2099.46, average training loss: 2145.16, base loss: 2377.56
[INFO 2017-06-26 19:27:35,606 main.py:47] epoch 2859, training loss: 1924.39, average training loss: 2145.06, base loss: 2377.31
[INFO 2017-06-26 19:27:35,893 main.py:47] epoch 2860, training loss: 2129.11, average training loss: 2145.16, base loss: 2377.47
[INFO 2017-06-26 19:27:36,179 main.py:47] epoch 2861, training loss: 2085.78, average training loss: 2145.03, base loss: 2377.18
[INFO 2017-06-26 19:27:36,465 main.py:47] epoch 2862, training loss: 1848.61, average training loss: 2144.63, base loss: 2376.69
[INFO 2017-06-26 19:27:36,750 main.py:47] epoch 2863, training loss: 1784.90, average training loss: 2144.11, base loss: 2376.12
[INFO 2017-06-26 19:27:37,033 main.py:47] epoch 2864, training loss: 2142.67, average training loss: 2144.30, base loss: 2376.29
[INFO 2017-06-26 19:27:37,320 main.py:47] epoch 2865, training loss: 2173.47, average training loss: 2144.48, base loss: 2376.42
[INFO 2017-06-26 19:27:37,607 main.py:47] epoch 2866, training loss: 2037.03, average training loss: 2144.66, base loss: 2376.92
[INFO 2017-06-26 19:27:37,894 main.py:47] epoch 2867, training loss: 2002.66, average training loss: 2144.39, base loss: 2376.66
[INFO 2017-06-26 19:27:38,177 main.py:47] epoch 2868, training loss: 2100.54, average training loss: 2143.97, base loss: 2376.19
[INFO 2017-06-26 19:27:38,461 main.py:47] epoch 2869, training loss: 2021.30, average training loss: 2143.46, base loss: 2375.85
[INFO 2017-06-26 19:27:38,744 main.py:47] epoch 2870, training loss: 2061.99, average training loss: 2143.17, base loss: 2375.68
[INFO 2017-06-26 19:27:39,027 main.py:47] epoch 2871, training loss: 1790.11, average training loss: 2142.91, base loss: 2375.51
[INFO 2017-06-26 19:27:39,313 main.py:47] epoch 2872, training loss: 2139.95, average training loss: 2143.20, base loss: 2375.78
[INFO 2017-06-26 19:27:39,599 main.py:47] epoch 2873, training loss: 2037.72, average training loss: 2142.85, base loss: 2375.34
[INFO 2017-06-26 19:27:39,882 main.py:47] epoch 2874, training loss: 2495.97, average training loss: 2143.18, base loss: 2375.87
[INFO 2017-06-26 19:27:40,168 main.py:47] epoch 2875, training loss: 1940.95, average training loss: 2143.00, base loss: 2375.74
[INFO 2017-06-26 19:27:40,448 main.py:47] epoch 2876, training loss: 2169.41, average training loss: 2142.80, base loss: 2375.44
[INFO 2017-06-26 19:27:40,732 main.py:47] epoch 2877, training loss: 1889.25, average training loss: 2142.30, base loss: 2374.99
[INFO 2017-06-26 19:27:41,018 main.py:47] epoch 2878, training loss: 2357.19, average training loss: 2142.71, base loss: 2375.67
[INFO 2017-06-26 19:27:41,298 main.py:47] epoch 2879, training loss: 1761.44, average training loss: 2142.40, base loss: 2375.27
[INFO 2017-06-26 19:27:41,582 main.py:47] epoch 2880, training loss: 2236.94, average training loss: 2142.31, base loss: 2375.29
[INFO 2017-06-26 19:27:41,865 main.py:47] epoch 2881, training loss: 2131.91, average training loss: 2141.80, base loss: 2374.75
[INFO 2017-06-26 19:27:42,146 main.py:47] epoch 2882, training loss: 2007.89, average training loss: 2141.58, base loss: 2374.30
[INFO 2017-06-26 19:27:42,434 main.py:47] epoch 2883, training loss: 2144.25, average training loss: 2141.44, base loss: 2374.03
[INFO 2017-06-26 19:27:42,717 main.py:47] epoch 2884, training loss: 2530.39, average training loss: 2141.73, base loss: 2374.34
[INFO 2017-06-26 19:27:42,998 main.py:47] epoch 2885, training loss: 2014.30, average training loss: 2141.67, base loss: 2374.35
[INFO 2017-06-26 19:27:43,281 main.py:47] epoch 2886, training loss: 2125.98, average training loss: 2141.47, base loss: 2374.02
[INFO 2017-06-26 19:27:43,564 main.py:47] epoch 2887, training loss: 2058.36, average training loss: 2141.26, base loss: 2373.81
[INFO 2017-06-26 19:27:43,848 main.py:47] epoch 2888, training loss: 2322.31, average training loss: 2141.27, base loss: 2374.06
[INFO 2017-06-26 19:27:44,131 main.py:47] epoch 2889, training loss: 2359.84, average training loss: 2141.36, base loss: 2374.14
[INFO 2017-06-26 19:27:44,416 main.py:47] epoch 2890, training loss: 2362.42, average training loss: 2141.77, base loss: 2374.73
[INFO 2017-06-26 19:27:44,699 main.py:47] epoch 2891, training loss: 2359.32, average training loss: 2141.86, base loss: 2374.90
[INFO 2017-06-26 19:27:44,983 main.py:47] epoch 2892, training loss: 2295.53, average training loss: 2141.94, base loss: 2375.13
[INFO 2017-06-26 19:27:45,264 main.py:47] epoch 2893, training loss: 2383.61, average training loss: 2141.77, base loss: 2375.20
[INFO 2017-06-26 19:27:45,548 main.py:47] epoch 2894, training loss: 1983.42, average training loss: 2141.51, base loss: 2374.81
[INFO 2017-06-26 19:27:45,836 main.py:47] epoch 2895, training loss: 1800.65, average training loss: 2141.30, base loss: 2374.56
[INFO 2017-06-26 19:27:46,120 main.py:47] epoch 2896, training loss: 1860.45, average training loss: 2141.14, base loss: 2374.17
[INFO 2017-06-26 19:27:46,404 main.py:47] epoch 2897, training loss: 1876.17, average training loss: 2141.12, base loss: 2374.21
[INFO 2017-06-26 19:27:46,684 main.py:47] epoch 2898, training loss: 2108.82, average training loss: 2141.00, base loss: 2374.19
[INFO 2017-06-26 19:27:46,965 main.py:47] epoch 2899, training loss: 2300.30, average training loss: 2141.46, base loss: 2374.66
[INFO 2017-06-26 19:27:46,965 main.py:49] epoch 2899, testing
[INFO 2017-06-26 19:27:50,671 main.py:100] average testing loss: 2083.42, base loss: 2331.40
[INFO 2017-06-26 19:27:50,696 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:27:50,981 main.py:47] epoch 2900, training loss: 2241.78, average training loss: 2141.75, base loss: 2374.92
[INFO 2017-06-26 19:27:51,265 main.py:47] epoch 2901, training loss: 1878.59, average training loss: 2141.59, base loss: 2374.85
[INFO 2017-06-26 19:27:51,548 main.py:47] epoch 2902, training loss: 2291.23, average training loss: 2141.96, base loss: 2375.26
[INFO 2017-06-26 19:27:51,832 main.py:47] epoch 2903, training loss: 2228.75, average training loss: 2142.03, base loss: 2375.31
[INFO 2017-06-26 19:27:52,117 main.py:47] epoch 2904, training loss: 2226.60, average training loss: 2141.73, base loss: 2375.04
[INFO 2017-06-26 19:27:52,404 main.py:47] epoch 2905, training loss: 2202.33, average training loss: 2141.50, base loss: 2374.77
[INFO 2017-06-26 19:27:52,691 main.py:47] epoch 2906, training loss: 2104.77, average training loss: 2141.80, base loss: 2375.10
[INFO 2017-06-26 19:27:52,975 main.py:47] epoch 2907, training loss: 2251.31, average training loss: 2141.79, base loss: 2375.05
[INFO 2017-06-26 19:27:53,260 main.py:47] epoch 2908, training loss: 1703.42, average training loss: 2141.44, base loss: 2374.85
[INFO 2017-06-26 19:27:53,543 main.py:47] epoch 2909, training loss: 2335.20, average training loss: 2141.69, base loss: 2375.28
[INFO 2017-06-26 19:27:53,827 main.py:47] epoch 2910, training loss: 1959.76, average training loss: 2141.38, base loss: 2374.79
[INFO 2017-06-26 19:27:54,111 main.py:47] epoch 2911, training loss: 1837.82, average training loss: 2141.10, base loss: 2374.57
[INFO 2017-06-26 19:27:54,398 main.py:47] epoch 2912, training loss: 2124.85, average training loss: 2141.32, base loss: 2374.95
[INFO 2017-06-26 19:27:54,682 main.py:47] epoch 2913, training loss: 2000.33, average training loss: 2141.09, base loss: 2374.70
[INFO 2017-06-26 19:27:54,962 main.py:47] epoch 2914, training loss: 1923.48, average training loss: 2141.14, base loss: 2374.80
[INFO 2017-06-26 19:27:55,243 main.py:47] epoch 2915, training loss: 2157.85, average training loss: 2141.23, base loss: 2374.91
[INFO 2017-06-26 19:27:55,527 main.py:47] epoch 2916, training loss: 1831.92, average training loss: 2141.02, base loss: 2374.74
[INFO 2017-06-26 19:27:55,812 main.py:47] epoch 2917, training loss: 1977.69, average training loss: 2140.63, base loss: 2374.30
[INFO 2017-06-26 19:27:56,094 main.py:47] epoch 2918, training loss: 1965.56, average training loss: 2140.66, base loss: 2374.43
[INFO 2017-06-26 19:27:56,379 main.py:47] epoch 2919, training loss: 2095.48, average training loss: 2140.48, base loss: 2374.20
[INFO 2017-06-26 19:27:56,666 main.py:47] epoch 2920, training loss: 1961.54, average training loss: 2140.13, base loss: 2373.86
[INFO 2017-06-26 19:27:56,950 main.py:47] epoch 2921, training loss: 2030.93, average training loss: 2139.98, base loss: 2373.72
[INFO 2017-06-26 19:27:57,235 main.py:47] epoch 2922, training loss: 2132.56, average training loss: 2139.67, base loss: 2373.26
[INFO 2017-06-26 19:27:57,522 main.py:47] epoch 2923, training loss: 1716.06, average training loss: 2139.27, base loss: 2372.70
[INFO 2017-06-26 19:27:57,809 main.py:47] epoch 2924, training loss: 1932.17, average training loss: 2139.28, base loss: 2372.63
[INFO 2017-06-26 19:27:58,096 main.py:47] epoch 2925, training loss: 1889.03, average training loss: 2138.80, base loss: 2372.21
[INFO 2017-06-26 19:27:58,380 main.py:47] epoch 2926, training loss: 2286.26, average training loss: 2139.17, base loss: 2372.71
[INFO 2017-06-26 19:27:58,665 main.py:47] epoch 2927, training loss: 1864.26, average training loss: 2138.69, base loss: 2371.98
[INFO 2017-06-26 19:27:58,951 main.py:47] epoch 2928, training loss: 2524.50, average training loss: 2139.17, base loss: 2372.47
[INFO 2017-06-26 19:27:59,233 main.py:47] epoch 2929, training loss: 2011.07, average training loss: 2139.13, base loss: 2372.42
[INFO 2017-06-26 19:27:59,517 main.py:47] epoch 2930, training loss: 1766.10, average training loss: 2138.84, base loss: 2372.17
[INFO 2017-06-26 19:27:59,801 main.py:47] epoch 2931, training loss: 2268.29, average training loss: 2139.05, base loss: 2372.49
[INFO 2017-06-26 19:28:00,085 main.py:47] epoch 2932, training loss: 2310.41, average training loss: 2139.17, base loss: 2372.33
[INFO 2017-06-26 19:28:00,370 main.py:47] epoch 2933, training loss: 1898.98, average training loss: 2139.02, base loss: 2372.13
[INFO 2017-06-26 19:28:00,652 main.py:47] epoch 2934, training loss: 2251.75, average training loss: 2138.96, base loss: 2372.07
[INFO 2017-06-26 19:28:00,939 main.py:47] epoch 2935, training loss: 2363.30, average training loss: 2139.31, base loss: 2372.54
[INFO 2017-06-26 19:28:01,225 main.py:47] epoch 2936, training loss: 2019.40, average training loss: 2139.20, base loss: 2372.40
[INFO 2017-06-26 19:28:01,512 main.py:47] epoch 2937, training loss: 2039.81, average training loss: 2138.90, base loss: 2372.08
[INFO 2017-06-26 19:28:01,799 main.py:47] epoch 2938, training loss: 2239.38, average training loss: 2139.13, base loss: 2372.47
[INFO 2017-06-26 19:28:02,082 main.py:47] epoch 2939, training loss: 2482.02, average training loss: 2139.71, base loss: 2373.24
[INFO 2017-06-26 19:28:02,366 main.py:47] epoch 2940, training loss: 1949.18, average training loss: 2139.55, base loss: 2373.09
[INFO 2017-06-26 19:28:02,646 main.py:47] epoch 2941, training loss: 1953.01, average training loss: 2139.38, base loss: 2373.16
[INFO 2017-06-26 19:28:02,926 main.py:47] epoch 2942, training loss: 2140.46, average training loss: 2139.74, base loss: 2373.52
[INFO 2017-06-26 19:28:03,209 main.py:47] epoch 2943, training loss: 2517.76, average training loss: 2140.19, base loss: 2374.02
[INFO 2017-06-26 19:28:03,494 main.py:47] epoch 2944, training loss: 2224.70, average training loss: 2140.72, base loss: 2374.61
[INFO 2017-06-26 19:28:03,776 main.py:47] epoch 2945, training loss: 2191.24, average training loss: 2140.63, base loss: 2374.54
[INFO 2017-06-26 19:28:04,064 main.py:47] epoch 2946, training loss: 2223.05, average training loss: 2140.84, base loss: 2374.74
[INFO 2017-06-26 19:28:04,349 main.py:47] epoch 2947, training loss: 1767.38, average training loss: 2141.07, base loss: 2375.11
[INFO 2017-06-26 19:28:04,633 main.py:47] epoch 2948, training loss: 2093.55, average training loss: 2141.05, base loss: 2375.17
[INFO 2017-06-26 19:28:04,919 main.py:47] epoch 2949, training loss: 1863.54, average training loss: 2140.62, base loss: 2374.79
[INFO 2017-06-26 19:28:05,201 main.py:47] epoch 2950, training loss: 1792.88, average training loss: 2140.25, base loss: 2374.38
[INFO 2017-06-26 19:28:05,487 main.py:47] epoch 2951, training loss: 2155.57, average training loss: 2140.63, base loss: 2374.83
[INFO 2017-06-26 19:28:05,767 main.py:47] epoch 2952, training loss: 2122.84, average training loss: 2140.54, base loss: 2374.81
[INFO 2017-06-26 19:28:06,049 main.py:47] epoch 2953, training loss: 2759.14, average training loss: 2140.93, base loss: 2375.27
[INFO 2017-06-26 19:28:06,332 main.py:47] epoch 2954, training loss: 1843.23, average training loss: 2140.64, base loss: 2374.92
[INFO 2017-06-26 19:28:06,617 main.py:47] epoch 2955, training loss: 2213.08, average training loss: 2140.58, base loss: 2374.99
[INFO 2017-06-26 19:28:06,906 main.py:47] epoch 2956, training loss: 2237.44, average training loss: 2140.76, base loss: 2375.22
[INFO 2017-06-26 19:28:07,190 main.py:47] epoch 2957, training loss: 2140.15, average training loss: 2140.60, base loss: 2374.96
[INFO 2017-06-26 19:28:07,474 main.py:47] epoch 2958, training loss: 2288.80, average training loss: 2140.67, base loss: 2375.25
[INFO 2017-06-26 19:28:07,754 main.py:47] epoch 2959, training loss: 2041.44, average training loss: 2140.60, base loss: 2375.08
[INFO 2017-06-26 19:28:08,034 main.py:47] epoch 2960, training loss: 2038.00, average training loss: 2140.75, base loss: 2375.07
[INFO 2017-06-26 19:28:08,319 main.py:47] epoch 2961, training loss: 2056.87, average training loss: 2140.34, base loss: 2374.78
[INFO 2017-06-26 19:28:08,599 main.py:47] epoch 2962, training loss: 2415.24, average training loss: 2140.60, base loss: 2375.18
[INFO 2017-06-26 19:28:08,882 main.py:47] epoch 2963, training loss: 2188.81, average training loss: 2140.57, base loss: 2375.25
[INFO 2017-06-26 19:28:09,169 main.py:47] epoch 2964, training loss: 2589.45, average training loss: 2141.08, base loss: 2376.04
[INFO 2017-06-26 19:28:09,452 main.py:47] epoch 2965, training loss: 2039.43, average training loss: 2141.06, base loss: 2376.14
[INFO 2017-06-26 19:28:09,736 main.py:47] epoch 2966, training loss: 2297.74, average training loss: 2141.54, base loss: 2376.66
[INFO 2017-06-26 19:28:10,020 main.py:47] epoch 2967, training loss: 2392.99, average training loss: 2141.75, base loss: 2376.99
[INFO 2017-06-26 19:28:10,303 main.py:47] epoch 2968, training loss: 2015.44, average training loss: 2141.88, base loss: 2377.18
[INFO 2017-06-26 19:28:10,587 main.py:47] epoch 2969, training loss: 2376.19, average training loss: 2141.89, base loss: 2377.12
[INFO 2017-06-26 19:28:10,868 main.py:47] epoch 2970, training loss: 1730.00, average training loss: 2141.07, base loss: 2376.21
[INFO 2017-06-26 19:28:11,149 main.py:47] epoch 2971, training loss: 2274.02, average training loss: 2141.43, base loss: 2376.79
[INFO 2017-06-26 19:28:11,435 main.py:47] epoch 2972, training loss: 2077.40, average training loss: 2141.45, base loss: 2377.06
[INFO 2017-06-26 19:28:11,716 main.py:47] epoch 2973, training loss: 2054.90, average training loss: 2141.17, base loss: 2376.88
[INFO 2017-06-26 19:28:12,004 main.py:47] epoch 2974, training loss: 2169.33, average training loss: 2141.45, base loss: 2377.16
[INFO 2017-06-26 19:28:12,286 main.py:47] epoch 2975, training loss: 2413.12, average training loss: 2141.30, base loss: 2377.06
[INFO 2017-06-26 19:28:12,574 main.py:47] epoch 2976, training loss: 2289.34, average training loss: 2141.23, base loss: 2377.03
[INFO 2017-06-26 19:28:12,861 main.py:47] epoch 2977, training loss: 2174.16, average training loss: 2141.15, base loss: 2376.89
[INFO 2017-06-26 19:28:13,142 main.py:47] epoch 2978, training loss: 2502.44, average training loss: 2141.68, base loss: 2377.72
[INFO 2017-06-26 19:28:13,430 main.py:47] epoch 2979, training loss: 2288.31, average training loss: 2141.65, base loss: 2377.66
[INFO 2017-06-26 19:28:13,714 main.py:47] epoch 2980, training loss: 2314.24, average training loss: 2141.95, base loss: 2378.07
[INFO 2017-06-26 19:28:13,995 main.py:47] epoch 2981, training loss: 2541.64, average training loss: 2142.06, base loss: 2378.21
[INFO 2017-06-26 19:28:14,282 main.py:47] epoch 2982, training loss: 1869.72, average training loss: 2141.81, base loss: 2377.98
[INFO 2017-06-26 19:28:14,562 main.py:47] epoch 2983, training loss: 2192.73, average training loss: 2142.10, base loss: 2378.45
[INFO 2017-06-26 19:28:14,842 main.py:47] epoch 2984, training loss: 2206.27, average training loss: 2142.32, base loss: 2379.11
[INFO 2017-06-26 19:28:15,129 main.py:47] epoch 2985, training loss: 1955.22, average training loss: 2141.98, base loss: 2378.67
[INFO 2017-06-26 19:28:15,418 main.py:47] epoch 2986, training loss: 1885.47, average training loss: 2141.61, base loss: 2378.25
[INFO 2017-06-26 19:28:15,698 main.py:47] epoch 2987, training loss: 2284.68, average training loss: 2142.19, base loss: 2379.14
[INFO 2017-06-26 19:28:15,983 main.py:47] epoch 2988, training loss: 2057.15, average training loss: 2141.87, base loss: 2378.81
[INFO 2017-06-26 19:28:16,263 main.py:47] epoch 2989, training loss: 2155.91, average training loss: 2142.11, base loss: 2379.14
[INFO 2017-06-26 19:28:16,550 main.py:47] epoch 2990, training loss: 2236.94, average training loss: 2142.42, base loss: 2379.68
[INFO 2017-06-26 19:28:16,838 main.py:47] epoch 2991, training loss: 2158.97, average training loss: 2142.35, base loss: 2379.57
[INFO 2017-06-26 19:28:17,127 main.py:47] epoch 2992, training loss: 1926.65, average training loss: 2142.28, base loss: 2379.47
[INFO 2017-06-26 19:28:17,414 main.py:47] epoch 2993, training loss: 2089.35, average training loss: 2142.27, base loss: 2379.51
[INFO 2017-06-26 19:28:17,702 main.py:47] epoch 2994, training loss: 2352.17, average training loss: 2142.60, base loss: 2380.02
[INFO 2017-06-26 19:28:17,985 main.py:47] epoch 2995, training loss: 2224.30, average training loss: 2142.53, base loss: 2379.91
[INFO 2017-06-26 19:28:18,269 main.py:47] epoch 2996, training loss: 1840.69, average training loss: 2142.30, base loss: 2379.71
[INFO 2017-06-26 19:28:18,549 main.py:47] epoch 2997, training loss: 2413.49, average training loss: 2142.49, base loss: 2380.17
[INFO 2017-06-26 19:28:18,829 main.py:47] epoch 2998, training loss: 1943.81, average training loss: 2142.08, base loss: 2379.89
[INFO 2017-06-26 19:28:19,117 main.py:47] epoch 2999, training loss: 1688.45, average training loss: 2141.09, base loss: 2378.75
[INFO 2017-06-26 19:28:19,117 main.py:49] epoch 2999, testing
[INFO 2017-06-26 19:28:22,816 main.py:100] average testing loss: 2173.64, base loss: 2485.64
[INFO 2017-06-26 19:28:22,841 main.py:73] current best accuracy: 2035.53
[INFO 2017-06-26 19:28:23,125 main.py:47] epoch 3000, training loss: 2070.72, average training loss: 2141.07, base loss: 2378.67
[INFO 2017-06-26 19:28:23,410 main.py:47] epoch 3001, training loss: 2046.94, average training loss: 2140.99, base loss: 2378.75
[INFO 2017-06-26 19:28:23,694 main.py:47] epoch 3002, training loss: 2192.59, average training loss: 2140.96, base loss: 2378.80
[INFO 2017-06-26 19:28:23,981 main.py:47] epoch 3003, training loss: 2382.15, average training loss: 2141.24, base loss: 2379.24
[INFO 2017-06-26 19:28:24,265 main.py:47] epoch 3004, training loss: 1837.28, average training loss: 2140.58, base loss: 2378.40
[INFO 2017-06-26 19:28:24,551 main.py:47] epoch 3005, training loss: 1933.08, average training loss: 2140.31, base loss: 2378.17
[INFO 2017-06-26 19:28:24,835 main.py:47] epoch 3006, training loss: 2065.68, average training loss: 2140.29, base loss: 2378.35
[INFO 2017-06-26 19:28:25,124 main.py:47] epoch 3007, training loss: 2227.09, average training loss: 2140.46, base loss: 2378.47
[INFO 2017-06-26 19:28:25,408 main.py:47] epoch 3008, training loss: 2524.80, average training loss: 2140.36, base loss: 2378.41
[INFO 2017-06-26 19:28:25,696 main.py:47] epoch 3009, training loss: 1984.05, average training loss: 2140.28, base loss: 2378.32
[INFO 2017-06-26 19:28:25,981 main.py:47] epoch 3010, training loss: 2256.67, average training loss: 2140.07, base loss: 2378.26
[INFO 2017-06-26 19:28:26,261 main.py:47] epoch 3011, training loss: 2228.57, average training loss: 2139.79, base loss: 2377.90
[INFO 2017-06-26 19:28:26,546 main.py:47] epoch 3012, training loss: 2058.09, average training loss: 2139.58, base loss: 2377.55
[INFO 2017-06-26 19:28:26,830 main.py:47] epoch 3013, training loss: 1922.56, average training loss: 2138.83, base loss: 2376.25
[INFO 2017-06-26 19:28:27,112 main.py:47] epoch 3014, training loss: 2022.83, average training loss: 2138.52, base loss: 2375.77
[INFO 2017-06-26 19:28:27,397 main.py:47] epoch 3015, training loss: 1844.36, average training loss: 2137.93, base loss: 2375.05
[INFO 2017-06-26 19:28:27,685 main.py:47] epoch 3016, training loss: 2054.11, average training loss: 2137.98, base loss: 2375.23
[INFO 2017-06-26 19:28:27,970 main.py:47] epoch 3017, training loss: 2166.86, average training loss: 2137.93, base loss: 2374.99
[INFO 2017-06-26 19:28:28,257 main.py:47] epoch 3018, training loss: 1731.58, average training loss: 2137.58, base loss: 2374.62
[INFO 2017-06-26 19:28:28,538 main.py:47] epoch 3019, training loss: 2166.99, average training loss: 2137.46, base loss: 2374.28
[INFO 2017-06-26 19:28:28,818 main.py:47] epoch 3020, training loss: 2093.41, average training loss: 2137.29, base loss: 2374.12
[INFO 2017-06-26 19:28:29,103 main.py:47] epoch 3021, training loss: 2151.38, average training loss: 2137.45, base loss: 2374.21
[INFO 2017-06-26 19:28:29,388 main.py:47] epoch 3022, training loss: 2145.97, average training loss: 2137.30, base loss: 2373.94
[INFO 2017-06-26 19:28:29,672 main.py:47] epoch 3023, training loss: 2019.73, average training loss: 2137.48, base loss: 2374.25
[INFO 2017-06-26 19:28:29,956 main.py:47] epoch 3024, training loss: 1624.73, average training loss: 2136.72, base loss: 2373.47
[INFO 2017-06-26 19:28:30,237 main.py:47] epoch 3025, training loss: 2248.99, average training loss: 2136.90, base loss: 2373.54
[INFO 2017-06-26 19:28:30,521 main.py:47] epoch 3026, training loss: 1899.42, average training loss: 2136.86, base loss: 2373.47
[INFO 2017-06-26 19:28:30,809 main.py:47] epoch 3027, training loss: 2766.55, average training loss: 2137.68, base loss: 2374.30
[INFO 2017-06-26 19:28:31,095 main.py:47] epoch 3028, training loss: 2402.21, average training loss: 2137.38, base loss: 2374.12
[INFO 2017-06-26 19:28:31,380 main.py:47] epoch 3029, training loss: 2237.37, average training loss: 2137.51, base loss: 2374.19
[INFO 2017-06-26 19:28:31,678 main.py:47] epoch 3030, training loss: 2118.42, average training loss: 2137.48, base loss: 2374.41
[INFO 2017-06-26 19:28:31,959 main.py:47] epoch 3031, training loss: 1883.38, average training loss: 2137.32, base loss: 2374.33
[INFO 2017-06-26 19:28:32,244 main.py:47] epoch 3032, training loss: 2186.57, average training loss: 2137.22, base loss: 2374.02
[INFO 2017-06-26 19:28:32,528 main.py:47] epoch 3033, training loss: 2082.96, average training loss: 2137.24, base loss: 2374.08
[INFO 2017-06-26 19:28:32,812 main.py:47] epoch 3034, training loss: 2119.04, average training loss: 2137.01, base loss: 2373.76
[INFO 2017-06-26 19:28:33,097 main.py:47] epoch 3035, training loss: 1820.12, average training loss: 2136.84, base loss: 2373.67
[INFO 2017-06-26 19:28:33,384 main.py:47] epoch 3036, training loss: 2932.42, average training loss: 2137.94, base loss: 2374.95
[INFO 2017-06-26 19:28:33,674 main.py:47] epoch 3037, training loss: 1841.54, average training loss: 2137.59, base loss: 2374.57
[INFO 2017-06-26 19:28:33,959 main.py:47] epoch 3038, training loss: 2676.69, average training loss: 2138.16, base loss: 2375.40
[INFO 2017-06-26 19:28:34,246 main.py:47] epoch 3039, training loss: 2189.96, average training loss: 2138.40, base loss: 2375.47
[INFO 2017-06-26 19:28:34,534 main.py:47] epoch 3040, training loss: 2080.67, average training loss: 2138.60, base loss: 2375.83
[INFO 2017-06-26 19:28:34,819 main.py:47] epoch 3041, training loss: 2300.76, average training loss: 2138.75, base loss: 2376.03
[INFO 2017-06-26 19:28:35,100 main.py:47] epoch 3042, training loss: 2514.24, average training loss: 2139.03, base loss: 2376.37
[INFO 2017-06-26 19:28:35,381 main.py:47] epoch 3043, training loss: 1890.32, average training loss: 2138.66, base loss: 2376.04
[INFO 2017-06-26 19:28:35,668 main.py:47] epoch 3044, training loss: 2093.26, average training loss: 2138.60, base loss: 2375.99
[INFO 2017-06-26 19:28:35,953 main.py:47] epoch 3045, training loss: 2287.64, average training loss: 2138.63, base loss: 2376.02
[INFO 2017-06-26 19:28:36,241 main.py:47] epoch 3046, training loss: 1880.85, average training loss: 2138.24, base loss: 2375.57
[INFO 2017-06-26 19:28:36,523 main.py:47] epoch 3047, training loss: 1975.76, average training loss: 2138.07, base loss: 2375.32
[INFO 2017-06-26 19:28:36,808 main.py:47] epoch 3048, training loss: 2047.08, average training loss: 2137.91, base loss: 2375.35
[INFO 2017-06-26 19:28:37,090 main.py:47] epoch 3049, training loss: 2136.83, average training loss: 2137.95, base loss: 2375.47
[INFO 2017-06-26 19:28:37,378 main.py:47] epoch 3050, training loss: 2045.71, average training loss: 2137.92, base loss: 2375.50
[INFO 2017-06-26 19:28:37,660 main.py:47] epoch 3051, training loss: 2188.29, average training loss: 2138.26, base loss: 2375.76
[INFO 2017-06-26 19:28:37,943 main.py:47] epoch 3052, training loss: 2233.13, average training loss: 2138.40, base loss: 2376.15
[INFO 2017-06-26 19:28:38,229 main.py:47] epoch 3053, training loss: 2249.68, average training loss: 2138.47, base loss: 2376.32
[INFO 2017-06-26 19:28:38,512 main.py:47] epoch 3054, training loss: 2081.74, average training loss: 2138.47, base loss: 2376.43
[INFO 2017-06-26 19:28:38,795 main.py:47] epoch 3055, training loss: 2218.04, average training loss: 2138.80, base loss: 2376.88
[INFO 2017-06-26 19:28:39,083 main.py:47] epoch 3056, training loss: 2200.77, average training loss: 2138.73, base loss: 2376.87
[INFO 2017-06-26 19:28:39,372 main.py:47] epoch 3057, training loss: 2026.42, average training loss: 2138.37, base loss: 2376.46
[INFO 2017-06-26 19:28:39,654 main.py:47] epoch 3058, training loss: 2496.24, average training loss: 2138.62, base loss: 2376.75
[INFO 2017-06-26 19:28:39,939 main.py:47] epoch 3059, training loss: 2394.96, average training loss: 2138.95, base loss: 2377.11
[INFO 2017-06-26 19:28:40,224 main.py:47] epoch 3060, training loss: 2046.60, average training loss: 2138.92, base loss: 2377.19
[INFO 2017-06-26 19:28:40,504 main.py:47] epoch 3061, training loss: 2357.67, average training loss: 2138.84, base loss: 2377.13
[INFO 2017-06-26 19:28:40,785 main.py:47] epoch 3062, training loss: 2269.17, average training loss: 2139.09, base loss: 2377.65
[INFO 2017-06-26 19:28:41,069 main.py:47] epoch 3063, training loss: 1908.01, average training loss: 2138.46, base loss: 2376.98
[INFO 2017-06-26 19:28:41,349 main.py:47] epoch 3064, training loss: 1913.17, average training loss: 2138.11, base loss: 2376.52
[INFO 2017-06-26 19:28:41,631 main.py:47] epoch 3065, training loss: 2088.15, average training loss: 2138.19, base loss: 2376.91
[INFO 2017-06-26 19:28:41,913 main.py:47] epoch 3066, training loss: 2411.82, average training loss: 2138.33, base loss: 2377.26
[INFO 2017-06-26 19:28:42,195 main.py:47] epoch 3067, training loss: 1955.98, average training loss: 2138.30, base loss: 2377.14
[INFO 2017-06-26 19:28:42,479 main.py:47] epoch 3068, training loss: 2593.16, average training loss: 2138.80, base loss: 2377.75
[INFO 2017-06-26 19:28:42,760 main.py:47] epoch 3069, training loss: 2097.21, average training loss: 2139.07, base loss: 2378.09
[INFO 2017-06-26 19:28:43,044 main.py:47] epoch 3070, training loss: 2060.37, average training loss: 2139.02, base loss: 2378.07
[INFO 2017-06-26 19:28:43,325 main.py:47] epoch 3071, training loss: 2004.45, average training loss: 2138.88, base loss: 2378.02
[INFO 2017-06-26 19:28:43,606 main.py:47] epoch 3072, training loss: 2112.75, average training loss: 2138.94, base loss: 2378.15
[INFO 2017-06-26 19:28:43,889 main.py:47] epoch 3073, training loss: 2110.04, average training loss: 2138.70, base loss: 2377.97
[INFO 2017-06-26 19:28:44,173 main.py:47] epoch 3074, training loss: 2236.35, average training loss: 2138.49, base loss: 2377.75
[INFO 2017-06-26 19:28:44,460 main.py:47] epoch 3075, training loss: 2195.14, average training loss: 2138.68, base loss: 2377.98
[INFO 2017-06-26 19:28:44,743 main.py:47] epoch 3076, training loss: 2058.08, average training loss: 2138.62, base loss: 2378.06
[INFO 2017-06-26 19:28:45,028 main.py:47] epoch 3077, training loss: 1826.17, average training loss: 2138.44, base loss: 2378.03
[INFO 2017-06-26 19:28:45,308 main.py:47] epoch 3078, training loss: 2171.76, average training loss: 2138.86, base loss: 2378.56
[INFO 2017-06-26 19:28:45,588 main.py:47] epoch 3079, training loss: 1800.92, average training loss: 2138.28, base loss: 2377.96
[INFO 2017-06-26 19:28:45,877 main.py:47] epoch 3080, training loss: 1894.71, average training loss: 2138.29, base loss: 2377.84
[INFO 2017-06-26 19:28:46,161 main.py:47] epoch 3081, training loss: 2565.84, average training loss: 2138.61, base loss: 2378.22
[INFO 2017-06-26 19:28:46,446 main.py:47] epoch 3082, training loss: 1973.81, average training loss: 2138.25, base loss: 2377.86
[INFO 2017-06-26 19:28:46,734 main.py:47] epoch 3083, training loss: 1889.24, average training loss: 2138.01, base loss: 2377.64
[INFO 2017-06-26 19:28:47,019 main.py:47] epoch 3084, training loss: 1799.40, average training loss: 2137.60, base loss: 2377.18
[INFO 2017-06-26 19:28:47,302 main.py:47] epoch 3085, training loss: 2465.39, average training loss: 2138.08, base loss: 2377.80
[INFO 2017-06-26 19:28:47,584 main.py:47] epoch 3086, training loss: 2166.42, average training loss: 2138.01, base loss: 2377.93
[INFO 2017-06-26 19:28:47,869 main.py:47] epoch 3087, training loss: 2178.64, average training loss: 2138.16, base loss: 2378.07
[INFO 2017-06-26 19:28:48,151 main.py:47] epoch 3088, training loss: 1926.26, average training loss: 2137.95, base loss: 2377.63
[INFO 2017-06-26 19:28:48,436 main.py:47] epoch 3089, training loss: 2323.65, average training loss: 2138.28, base loss: 2377.92
[INFO 2017-06-26 19:28:48,721 main.py:47] epoch 3090, training loss: 2323.95, average training loss: 2138.21, base loss: 2377.99
[INFO 2017-06-26 19:28:49,006 main.py:47] epoch 3091, training loss: 2369.69, average training loss: 2138.51, base loss: 2378.44
[INFO 2017-06-26 19:28:49,294 main.py:47] epoch 3092, training loss: 1947.38, average training loss: 2138.36, base loss: 2378.21
[INFO 2017-06-26 19:28:49,577 main.py:47] epoch 3093, training loss: 2206.40, average training loss: 2138.71, base loss: 2378.71
[INFO 2017-06-26 19:28:49,858 main.py:47] epoch 3094, training loss: 2298.96, average training loss: 2139.06, base loss: 2379.27
[INFO 2017-06-26 19:28:50,144 main.py:47] epoch 3095, training loss: 2260.13, average training loss: 2139.15, base loss: 2379.47
[INFO 2017-06-26 19:28:50,426 main.py:47] epoch 3096, training loss: 2226.51, average training loss: 2139.16, base loss: 2379.52
[INFO 2017-06-26 19:28:50,710 main.py:47] epoch 3097, training loss: 2055.23, average training loss: 2139.15, base loss: 2379.51
[INFO 2017-06-26 19:28:50,997 main.py:47] epoch 3098, training loss: 1985.36, average training loss: 2139.02, base loss: 2379.28
[INFO 2017-06-26 19:28:51,280 main.py:47] epoch 3099, training loss: 2249.39, average training loss: 2139.01, base loss: 2379.33
[INFO 2017-06-26 19:28:51,280 main.py:49] epoch 3099, testing
[INFO 2017-06-26 19:28:55,076 main.py:100] average testing loss: 2020.75, base loss: 2260.19
[INFO 2017-06-26 19:28:55,101 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:28:55,114 main.py:73] current best accuracy: 2020.75
[INFO 2017-06-26 19:28:55,399 main.py:47] epoch 3100, training loss: 2031.55, average training loss: 2138.85, base loss: 2379.09
[INFO 2017-06-26 19:28:55,683 main.py:47] epoch 3101, training loss: 2510.26, average training loss: 2139.19, base loss: 2379.55
[INFO 2017-06-26 19:28:55,966 main.py:47] epoch 3102, training loss: 2026.79, average training loss: 2138.88, base loss: 2379.15
[INFO 2017-06-26 19:28:56,247 main.py:47] epoch 3103, training loss: 2091.51, average training loss: 2138.48, base loss: 2378.88
[INFO 2017-06-26 19:28:56,530 main.py:47] epoch 3104, training loss: 2101.36, average training loss: 2138.51, base loss: 2379.09
[INFO 2017-06-26 19:28:56,815 main.py:47] epoch 3105, training loss: 2035.12, average training loss: 2138.50, base loss: 2379.09
[INFO 2017-06-26 19:28:57,099 main.py:47] epoch 3106, training loss: 2041.00, average training loss: 2138.17, base loss: 2379.01
[INFO 2017-06-26 19:28:57,379 main.py:47] epoch 3107, training loss: 2247.02, average training loss: 2138.29, base loss: 2379.26
[INFO 2017-06-26 19:28:57,663 main.py:47] epoch 3108, training loss: 1958.94, average training loss: 2138.01, base loss: 2379.00
[INFO 2017-06-26 19:28:57,950 main.py:47] epoch 3109, training loss: 1825.31, average training loss: 2137.80, base loss: 2378.66
[INFO 2017-06-26 19:28:58,229 main.py:47] epoch 3110, training loss: 2418.92, average training loss: 2138.23, base loss: 2379.42
[INFO 2017-06-26 19:28:58,511 main.py:47] epoch 3111, training loss: 1979.26, average training loss: 2138.05, base loss: 2379.04
[INFO 2017-06-26 19:28:58,794 main.py:47] epoch 3112, training loss: 2169.23, average training loss: 2137.58, base loss: 2378.65
[INFO 2017-06-26 19:28:59,078 main.py:47] epoch 3113, training loss: 1850.36, average training loss: 2136.93, base loss: 2377.79
[INFO 2017-06-26 19:28:59,358 main.py:47] epoch 3114, training loss: 1953.99, average training loss: 2136.79, base loss: 2377.75
[INFO 2017-06-26 19:28:59,645 main.py:47] epoch 3115, training loss: 2180.59, average training loss: 2136.85, base loss: 2377.84
[INFO 2017-06-26 19:28:59,929 main.py:47] epoch 3116, training loss: 1985.79, average training loss: 2136.56, base loss: 2377.53
[INFO 2017-06-26 19:29:00,210 main.py:47] epoch 3117, training loss: 2184.08, average training loss: 2136.13, base loss: 2376.92
[INFO 2017-06-26 19:29:00,496 main.py:47] epoch 3118, training loss: 2276.42, average training loss: 2136.63, base loss: 2377.49
[INFO 2017-06-26 19:29:00,776 main.py:47] epoch 3119, training loss: 2045.69, average training loss: 2136.76, base loss: 2377.38
[INFO 2017-06-26 19:29:01,058 main.py:47] epoch 3120, training loss: 2083.10, average training loss: 2137.10, base loss: 2378.01
[INFO 2017-06-26 19:29:01,339 main.py:47] epoch 3121, training loss: 2447.01, average training loss: 2137.38, base loss: 2378.30
[INFO 2017-06-26 19:29:01,624 main.py:47] epoch 3122, training loss: 2185.93, average training loss: 2137.24, base loss: 2378.66
[INFO 2017-06-26 19:29:01,906 main.py:47] epoch 3123, training loss: 1842.94, average training loss: 2137.05, base loss: 2378.40
[INFO 2017-06-26 19:29:02,189 main.py:47] epoch 3124, training loss: 2546.36, average training loss: 2137.64, base loss: 2379.09
[INFO 2017-06-26 19:29:02,477 main.py:47] epoch 3125, training loss: 1975.09, average training loss: 2137.59, base loss: 2379.16
[INFO 2017-06-26 19:29:02,758 main.py:47] epoch 3126, training loss: 1792.44, average training loss: 2137.46, base loss: 2378.97
[INFO 2017-06-26 19:29:03,041 main.py:47] epoch 3127, training loss: 2165.11, average training loss: 2137.62, base loss: 2379.28
[INFO 2017-06-26 19:29:03,325 main.py:47] epoch 3128, training loss: 2016.70, average training loss: 2137.37, base loss: 2378.91
[INFO 2017-06-26 19:29:03,608 main.py:47] epoch 3129, training loss: 1796.33, average training loss: 2136.78, base loss: 2378.29
[INFO 2017-06-26 19:29:03,894 main.py:47] epoch 3130, training loss: 1965.58, average training loss: 2136.62, base loss: 2378.17
[INFO 2017-06-26 19:29:04,177 main.py:47] epoch 3131, training loss: 2126.66, average training loss: 2136.77, base loss: 2378.47
[INFO 2017-06-26 19:29:04,457 main.py:47] epoch 3132, training loss: 2148.54, average training loss: 2136.87, base loss: 2378.79
[INFO 2017-06-26 19:29:04,743 main.py:47] epoch 3133, training loss: 2148.63, average training loss: 2136.93, base loss: 2378.96
[INFO 2017-06-26 19:29:05,026 main.py:47] epoch 3134, training loss: 2237.53, average training loss: 2136.89, base loss: 2379.01
[INFO 2017-06-26 19:29:05,313 main.py:47] epoch 3135, training loss: 1826.31, average training loss: 2136.55, base loss: 2378.64
[INFO 2017-06-26 19:29:05,600 main.py:47] epoch 3136, training loss: 2497.92, average training loss: 2136.73, base loss: 2379.05
[INFO 2017-06-26 19:29:05,885 main.py:47] epoch 3137, training loss: 2108.96, average training loss: 2137.10, base loss: 2379.66
[INFO 2017-06-26 19:29:06,173 main.py:47] epoch 3138, training loss: 1996.15, average training loss: 2136.81, base loss: 2379.49
[INFO 2017-06-26 19:29:06,459 main.py:47] epoch 3139, training loss: 1838.15, average training loss: 2136.66, base loss: 2379.39
[INFO 2017-06-26 19:29:06,742 main.py:47] epoch 3140, training loss: 2070.48, average training loss: 2136.44, base loss: 2379.08
[INFO 2017-06-26 19:29:07,025 main.py:47] epoch 3141, training loss: 1826.65, average training loss: 2136.47, base loss: 2379.10
[INFO 2017-06-26 19:29:07,308 main.py:47] epoch 3142, training loss: 2430.34, average training loss: 2136.71, base loss: 2379.30
[INFO 2017-06-26 19:29:07,592 main.py:47] epoch 3143, training loss: 1947.79, average training loss: 2136.66, base loss: 2379.42
[INFO 2017-06-26 19:29:07,872 main.py:47] epoch 3144, training loss: 2111.06, average training loss: 2135.94, base loss: 2378.91
[INFO 2017-06-26 19:29:08,155 main.py:47] epoch 3145, training loss: 2070.41, average training loss: 2135.59, base loss: 2378.67
[INFO 2017-06-26 19:29:08,443 main.py:47] epoch 3146, training loss: 1868.81, average training loss: 2135.58, base loss: 2378.64
[INFO 2017-06-26 19:29:08,726 main.py:47] epoch 3147, training loss: 2520.06, average training loss: 2136.33, base loss: 2379.58
[INFO 2017-06-26 19:29:09,009 main.py:47] epoch 3148, training loss: 2585.12, average training loss: 2137.02, base loss: 2380.45
[INFO 2017-06-26 19:29:09,296 main.py:47] epoch 3149, training loss: 2761.63, average training loss: 2137.80, base loss: 2381.34
[INFO 2017-06-26 19:29:09,579 main.py:47] epoch 3150, training loss: 2253.59, average training loss: 2138.07, base loss: 2381.78
[INFO 2017-06-26 19:29:09,866 main.py:47] epoch 3151, training loss: 1909.08, average training loss: 2137.69, base loss: 2381.26
[INFO 2017-06-26 19:29:10,150 main.py:47] epoch 3152, training loss: 2298.58, average training loss: 2137.81, base loss: 2381.44
[INFO 2017-06-26 19:29:10,437 main.py:47] epoch 3153, training loss: 2474.16, average training loss: 2138.28, base loss: 2381.91
[INFO 2017-06-26 19:29:10,721 main.py:47] epoch 3154, training loss: 2268.30, average training loss: 2138.75, base loss: 2382.36
[INFO 2017-06-26 19:29:11,007 main.py:47] epoch 3155, training loss: 2256.72, average training loss: 2138.85, base loss: 2382.61
[INFO 2017-06-26 19:29:11,294 main.py:47] epoch 3156, training loss: 1984.60, average training loss: 2138.83, base loss: 2382.78
[INFO 2017-06-26 19:29:11,580 main.py:47] epoch 3157, training loss: 2360.57, average training loss: 2139.00, base loss: 2383.04
[INFO 2017-06-26 19:29:11,862 main.py:47] epoch 3158, training loss: 2296.23, average training loss: 2139.18, base loss: 2383.32
[INFO 2017-06-26 19:29:12,145 main.py:47] epoch 3159, training loss: 2220.74, average training loss: 2139.49, base loss: 2383.63
[INFO 2017-06-26 19:29:12,433 main.py:47] epoch 3160, training loss: 2187.57, average training loss: 2139.80, base loss: 2384.12
[INFO 2017-06-26 19:29:12,720 main.py:47] epoch 3161, training loss: 1930.98, average training loss: 2139.50, base loss: 2383.86
[INFO 2017-06-26 19:29:13,003 main.py:47] epoch 3162, training loss: 2070.91, average training loss: 2139.01, base loss: 2383.33
[INFO 2017-06-26 19:29:13,286 main.py:47] epoch 3163, training loss: 2018.72, average training loss: 2138.91, base loss: 2383.24
[INFO 2017-06-26 19:29:13,575 main.py:47] epoch 3164, training loss: 1860.28, average training loss: 2138.89, base loss: 2383.37
[INFO 2017-06-26 19:29:13,859 main.py:47] epoch 3165, training loss: 2619.49, average training loss: 2139.48, base loss: 2384.02
[INFO 2017-06-26 19:29:14,143 main.py:47] epoch 3166, training loss: 2148.08, average training loss: 2139.45, base loss: 2384.01
[INFO 2017-06-26 19:29:14,423 main.py:47] epoch 3167, training loss: 1982.93, average training loss: 2139.17, base loss: 2383.64
[INFO 2017-06-26 19:29:14,704 main.py:47] epoch 3168, training loss: 1911.59, average training loss: 2139.06, base loss: 2383.59
[INFO 2017-06-26 19:29:14,983 main.py:47] epoch 3169, training loss: 2080.51, average training loss: 2138.77, base loss: 2383.50
[INFO 2017-06-26 19:29:15,263 main.py:47] epoch 3170, training loss: 2169.89, average training loss: 2138.53, base loss: 2383.27
[INFO 2017-06-26 19:29:15,545 main.py:47] epoch 3171, training loss: 1902.19, average training loss: 2137.86, base loss: 2382.59
[INFO 2017-06-26 19:29:15,825 main.py:47] epoch 3172, training loss: 2044.40, average training loss: 2138.07, base loss: 2382.97
[INFO 2017-06-26 19:29:16,108 main.py:47] epoch 3173, training loss: 1917.14, average training loss: 2138.11, base loss: 2383.18
[INFO 2017-06-26 19:29:16,389 main.py:47] epoch 3174, training loss: 2049.13, average training loss: 2137.86, base loss: 2382.89
[INFO 2017-06-26 19:29:16,672 main.py:47] epoch 3175, training loss: 2263.46, average training loss: 2137.92, base loss: 2383.26
[INFO 2017-06-26 19:29:16,955 main.py:47] epoch 3176, training loss: 1724.25, average training loss: 2137.57, base loss: 2382.90
[INFO 2017-06-26 19:29:17,239 main.py:47] epoch 3177, training loss: 2072.09, average training loss: 2137.94, base loss: 2383.60
[INFO 2017-06-26 19:29:17,533 main.py:47] epoch 3178, training loss: 1978.54, average training loss: 2137.62, base loss: 2383.44
[INFO 2017-06-26 19:29:17,820 main.py:47] epoch 3179, training loss: 2168.60, average training loss: 2137.56, base loss: 2383.43
[INFO 2017-06-26 19:29:18,104 main.py:47] epoch 3180, training loss: 1799.88, average training loss: 2137.16, base loss: 2383.03
[INFO 2017-06-26 19:29:18,388 main.py:47] epoch 3181, training loss: 1737.21, average training loss: 2136.82, base loss: 2382.69
[INFO 2017-06-26 19:29:18,674 main.py:47] epoch 3182, training loss: 2247.73, average training loss: 2136.86, base loss: 2382.77
[INFO 2017-06-26 19:29:18,962 main.py:47] epoch 3183, training loss: 2073.57, average training loss: 2136.85, base loss: 2382.80
[INFO 2017-06-26 19:29:19,246 main.py:47] epoch 3184, training loss: 1950.58, average training loss: 2136.73, base loss: 2382.50
[INFO 2017-06-26 19:29:19,530 main.py:47] epoch 3185, training loss: 2325.10, average training loss: 2136.96, base loss: 2382.76
[INFO 2017-06-26 19:29:19,813 main.py:47] epoch 3186, training loss: 1923.24, average training loss: 2136.16, base loss: 2381.73
[INFO 2017-06-26 19:29:20,099 main.py:47] epoch 3187, training loss: 2317.47, average training loss: 2136.22, base loss: 2381.93
[INFO 2017-06-26 19:29:20,385 main.py:47] epoch 3188, training loss: 1865.72, average training loss: 2135.86, base loss: 2381.65
[INFO 2017-06-26 19:29:20,669 main.py:47] epoch 3189, training loss: 2084.05, average training loss: 2135.78, base loss: 2381.74
[INFO 2017-06-26 19:29:20,955 main.py:47] epoch 3190, training loss: 2189.20, average training loss: 2135.89, base loss: 2381.97
[INFO 2017-06-26 19:29:21,238 main.py:47] epoch 3191, training loss: 2181.58, average training loss: 2135.74, base loss: 2381.83
[INFO 2017-06-26 19:29:21,521 main.py:47] epoch 3192, training loss: 2186.24, average training loss: 2135.91, base loss: 2382.11
[INFO 2017-06-26 19:29:21,805 main.py:47] epoch 3193, training loss: 2058.56, average training loss: 2136.14, base loss: 2382.35
[INFO 2017-06-26 19:29:22,089 main.py:47] epoch 3194, training loss: 1767.51, average training loss: 2135.73, base loss: 2381.90
[INFO 2017-06-26 19:29:22,370 main.py:47] epoch 3195, training loss: 1948.06, average training loss: 2135.74, base loss: 2382.04
[INFO 2017-06-26 19:29:22,657 main.py:47] epoch 3196, training loss: 1931.72, average training loss: 2135.93, base loss: 2382.48
[INFO 2017-06-26 19:29:22,941 main.py:47] epoch 3197, training loss: 2225.22, average training loss: 2136.13, base loss: 2382.79
[INFO 2017-06-26 19:29:23,225 main.py:47] epoch 3198, training loss: 1881.86, average training loss: 2136.29, base loss: 2382.92
[INFO 2017-06-26 19:29:23,511 main.py:47] epoch 3199, training loss: 2035.16, average training loss: 2136.11, base loss: 2382.76
[INFO 2017-06-26 19:29:23,511 main.py:49] epoch 3199, testing
[INFO 2017-06-26 19:29:27,252 main.py:100] average testing loss: 2079.58, base loss: 2320.90
[INFO 2017-06-26 19:29:27,277 main.py:73] current best accuracy: 2020.75
[INFO 2017-06-26 19:29:27,565 main.py:47] epoch 3200, training loss: 2187.33, average training loss: 2136.34, base loss: 2383.09
[INFO 2017-06-26 19:29:27,847 main.py:47] epoch 3201, training loss: 2641.91, average training loss: 2136.93, base loss: 2383.72
[INFO 2017-06-26 19:29:28,129 main.py:47] epoch 3202, training loss: 1943.35, average training loss: 2136.68, base loss: 2383.43
[INFO 2017-06-26 19:29:28,413 main.py:47] epoch 3203, training loss: 2102.07, average training loss: 2136.92, base loss: 2383.78
[INFO 2017-06-26 19:29:28,701 main.py:47] epoch 3204, training loss: 2385.92, average training loss: 2137.29, base loss: 2384.29
[INFO 2017-06-26 19:29:28,984 main.py:47] epoch 3205, training loss: 1820.31, average training loss: 2136.96, base loss: 2383.93
[INFO 2017-06-26 19:29:29,269 main.py:47] epoch 3206, training loss: 2227.54, average training loss: 2137.13, base loss: 2384.22
[INFO 2017-06-26 19:29:29,555 main.py:47] epoch 3207, training loss: 2145.10, average training loss: 2136.79, base loss: 2383.92
[INFO 2017-06-26 19:29:29,841 main.py:47] epoch 3208, training loss: 2284.31, average training loss: 2137.02, base loss: 2383.95
[INFO 2017-06-26 19:29:30,125 main.py:47] epoch 3209, training loss: 2148.40, average training loss: 2137.09, base loss: 2383.95
[INFO 2017-06-26 19:29:30,409 main.py:47] epoch 3210, training loss: 2431.31, average training loss: 2137.27, base loss: 2384.36
[INFO 2017-06-26 19:29:30,693 main.py:47] epoch 3211, training loss: 1994.30, average training loss: 2137.10, base loss: 2384.12
[INFO 2017-06-26 19:29:30,973 main.py:47] epoch 3212, training loss: 2188.46, average training loss: 2136.97, base loss: 2384.01
[INFO 2017-06-26 19:29:31,257 main.py:47] epoch 3213, training loss: 2308.91, average training loss: 2137.27, base loss: 2384.13
[INFO 2017-06-26 19:29:31,537 main.py:47] epoch 3214, training loss: 2065.20, average training loss: 2136.67, base loss: 2383.39
[INFO 2017-06-26 19:29:31,821 main.py:47] epoch 3215, training loss: 2075.08, average training loss: 2136.40, base loss: 2383.01
[INFO 2017-06-26 19:29:32,108 main.py:47] epoch 3216, training loss: 1981.18, average training loss: 2136.54, base loss: 2383.43
[INFO 2017-06-26 19:29:32,389 main.py:47] epoch 3217, training loss: 2405.63, average training loss: 2136.59, base loss: 2383.53
[INFO 2017-06-26 19:29:32,675 main.py:47] epoch 3218, training loss: 1876.31, average training loss: 2136.52, base loss: 2383.49
[INFO 2017-06-26 19:29:32,961 main.py:47] epoch 3219, training loss: 1715.26, average training loss: 2136.08, base loss: 2383.03
[INFO 2017-06-26 19:29:33,244 main.py:47] epoch 3220, training loss: 2357.75, average training loss: 2136.13, base loss: 2383.10
[INFO 2017-06-26 19:29:33,528 main.py:47] epoch 3221, training loss: 1983.25, average training loss: 2136.36, base loss: 2383.26
[INFO 2017-06-26 19:29:33,812 main.py:47] epoch 3222, training loss: 2158.11, average training loss: 2136.60, base loss: 2383.62
[INFO 2017-06-26 19:29:34,095 main.py:47] epoch 3223, training loss: 1845.62, average training loss: 2136.17, base loss: 2383.23
[INFO 2017-06-26 19:29:34,375 main.py:47] epoch 3224, training loss: 2128.96, average training loss: 2135.92, base loss: 2382.90
[INFO 2017-06-26 19:29:34,658 main.py:47] epoch 3225, training loss: 2001.70, average training loss: 2135.81, base loss: 2382.79
[INFO 2017-06-26 19:29:34,944 main.py:47] epoch 3226, training loss: 2377.58, average training loss: 2136.16, base loss: 2383.20
[INFO 2017-06-26 19:29:35,229 main.py:47] epoch 3227, training loss: 2053.36, average training loss: 2135.54, base loss: 2382.45
[INFO 2017-06-26 19:29:35,516 main.py:47] epoch 3228, training loss: 2023.61, average training loss: 2135.80, base loss: 2382.70
[INFO 2017-06-26 19:29:35,804 main.py:47] epoch 3229, training loss: 1803.86, average training loss: 2135.51, base loss: 2382.17
[INFO 2017-06-26 19:29:36,090 main.py:47] epoch 3230, training loss: 2319.11, average training loss: 2135.30, base loss: 2381.92
[INFO 2017-06-26 19:29:36,374 main.py:47] epoch 3231, training loss: 1976.06, average training loss: 2135.02, base loss: 2381.63
[INFO 2017-06-26 19:29:36,659 main.py:47] epoch 3232, training loss: 1674.01, average training loss: 2134.34, base loss: 2380.66
[INFO 2017-06-26 19:29:36,946 main.py:47] epoch 3233, training loss: 1880.22, average training loss: 2133.82, base loss: 2380.13
[INFO 2017-06-26 19:29:37,233 main.py:47] epoch 3234, training loss: 1805.63, average training loss: 2133.26, base loss: 2379.34
[INFO 2017-06-26 19:29:37,519 main.py:47] epoch 3235, training loss: 2261.14, average training loss: 2133.16, base loss: 2379.18
[INFO 2017-06-26 19:29:37,804 main.py:47] epoch 3236, training loss: 2097.13, average training loss: 2132.82, base loss: 2378.92
[INFO 2017-06-26 19:29:38,089 main.py:47] epoch 3237, training loss: 1779.36, average training loss: 2132.25, base loss: 2378.48
[INFO 2017-06-26 19:29:38,376 main.py:47] epoch 3238, training loss: 1872.57, average training loss: 2132.00, base loss: 2378.14
[INFO 2017-06-26 19:29:38,663 main.py:47] epoch 3239, training loss: 2291.83, average training loss: 2131.97, base loss: 2378.34
[INFO 2017-06-26 19:29:38,946 main.py:47] epoch 3240, training loss: 2229.25, average training loss: 2131.98, base loss: 2378.51
[INFO 2017-06-26 19:29:39,229 main.py:47] epoch 3241, training loss: 2228.41, average training loss: 2132.21, base loss: 2378.72
[INFO 2017-06-26 19:29:39,509 main.py:47] epoch 3242, training loss: 1937.43, average training loss: 2132.04, base loss: 2378.51
[INFO 2017-06-26 19:29:39,792 main.py:47] epoch 3243, training loss: 2123.91, average training loss: 2132.09, base loss: 2378.83
[INFO 2017-06-26 19:29:40,072 main.py:47] epoch 3244, training loss: 1894.91, average training loss: 2131.30, base loss: 2378.11
[INFO 2017-06-26 19:29:40,354 main.py:47] epoch 3245, training loss: 2319.72, average training loss: 2131.76, base loss: 2378.59
[INFO 2017-06-26 19:29:40,638 main.py:47] epoch 3246, training loss: 2475.46, average training loss: 2131.87, base loss: 2378.67
[INFO 2017-06-26 19:29:40,922 main.py:47] epoch 3247, training loss: 2179.30, average training loss: 2131.94, base loss: 2378.82
[INFO 2017-06-26 19:29:41,203 main.py:47] epoch 3248, training loss: 2149.14, average training loss: 2131.88, base loss: 2378.97
[INFO 2017-06-26 19:29:41,486 main.py:47] epoch 3249, training loss: 1955.56, average training loss: 2131.81, base loss: 2378.88
[INFO 2017-06-26 19:29:41,765 main.py:47] epoch 3250, training loss: 2059.70, average training loss: 2131.99, base loss: 2379.24
[INFO 2017-06-26 19:29:42,049 main.py:47] epoch 3251, training loss: 2443.90, average training loss: 2132.11, base loss: 2379.51
[INFO 2017-06-26 19:29:42,335 main.py:47] epoch 3252, training loss: 1758.62, average training loss: 2132.02, base loss: 2379.45
[INFO 2017-06-26 19:29:42,617 main.py:47] epoch 3253, training loss: 2052.15, average training loss: 2131.64, base loss: 2379.16
[INFO 2017-06-26 19:29:42,905 main.py:47] epoch 3254, training loss: 1899.11, average training loss: 2131.46, base loss: 2379.09
[INFO 2017-06-26 19:29:43,193 main.py:47] epoch 3255, training loss: 2248.06, average training loss: 2131.42, base loss: 2379.16
[INFO 2017-06-26 19:29:43,477 main.py:47] epoch 3256, training loss: 2170.49, average training loss: 2131.21, base loss: 2378.82
[INFO 2017-06-26 19:29:43,763 main.py:47] epoch 3257, training loss: 2141.15, average training loss: 2130.94, base loss: 2378.55
[INFO 2017-06-26 19:29:44,043 main.py:47] epoch 3258, training loss: 1913.64, average training loss: 2130.61, base loss: 2378.06
[INFO 2017-06-26 19:29:44,326 main.py:47] epoch 3259, training loss: 2084.32, average training loss: 2130.55, base loss: 2377.86
[INFO 2017-06-26 19:29:44,612 main.py:47] epoch 3260, training loss: 2051.92, average training loss: 2130.58, base loss: 2377.95
[INFO 2017-06-26 19:29:44,897 main.py:47] epoch 3261, training loss: 2122.14, average training loss: 2130.56, base loss: 2378.20
[INFO 2017-06-26 19:29:45,181 main.py:47] epoch 3262, training loss: 2178.47, average training loss: 2130.55, base loss: 2378.21
[INFO 2017-06-26 19:29:45,469 main.py:47] epoch 3263, training loss: 1849.04, average training loss: 2130.26, base loss: 2377.78
[INFO 2017-06-26 19:29:45,757 main.py:47] epoch 3264, training loss: 1754.49, average training loss: 2129.79, base loss: 2377.36
[INFO 2017-06-26 19:29:46,040 main.py:47] epoch 3265, training loss: 2373.77, average training loss: 2130.08, base loss: 2377.53
[INFO 2017-06-26 19:29:46,324 main.py:47] epoch 3266, training loss: 2010.13, average training loss: 2129.60, base loss: 2376.82
[INFO 2017-06-26 19:29:46,612 main.py:47] epoch 3267, training loss: 2082.94, average training loss: 2129.65, base loss: 2376.89
[INFO 2017-06-26 19:29:46,895 main.py:47] epoch 3268, training loss: 1782.97, average training loss: 2128.73, base loss: 2375.76
[INFO 2017-06-26 19:29:47,181 main.py:47] epoch 3269, training loss: 2081.70, average training loss: 2129.08, base loss: 2376.42
[INFO 2017-06-26 19:29:47,466 main.py:47] epoch 3270, training loss: 1889.22, average training loss: 2128.78, base loss: 2376.30
[INFO 2017-06-26 19:29:47,752 main.py:47] epoch 3271, training loss: 2375.15, average training loss: 2128.96, base loss: 2376.53
[INFO 2017-06-26 19:29:48,035 main.py:47] epoch 3272, training loss: 1863.93, average training loss: 2128.34, base loss: 2375.93
[INFO 2017-06-26 19:29:48,318 main.py:47] epoch 3273, training loss: 1991.59, average training loss: 2127.77, base loss: 2375.32
[INFO 2017-06-26 19:29:48,603 main.py:47] epoch 3274, training loss: 2500.02, average training loss: 2127.89, base loss: 2375.62
[INFO 2017-06-26 19:29:48,891 main.py:47] epoch 3275, training loss: 2107.62, average training loss: 2128.01, base loss: 2375.86
[INFO 2017-06-26 19:29:49,175 main.py:47] epoch 3276, training loss: 1735.71, average training loss: 2127.84, base loss: 2375.51
[INFO 2017-06-26 19:29:49,456 main.py:47] epoch 3277, training loss: 2164.25, average training loss: 2128.12, base loss: 2375.98
[INFO 2017-06-26 19:29:49,739 main.py:47] epoch 3278, training loss: 1926.71, average training loss: 2128.05, base loss: 2376.07
[INFO 2017-06-26 19:29:50,022 main.py:47] epoch 3279, training loss: 1923.63, average training loss: 2127.65, base loss: 2375.64
[INFO 2017-06-26 19:29:50,303 main.py:47] epoch 3280, training loss: 2321.78, average training loss: 2127.99, base loss: 2376.15
[INFO 2017-06-26 19:29:50,588 main.py:47] epoch 3281, training loss: 2195.19, average training loss: 2128.23, base loss: 2376.41
[INFO 2017-06-26 19:29:50,875 main.py:47] epoch 3282, training loss: 2143.23, average training loss: 2128.07, base loss: 2376.33
[INFO 2017-06-26 19:29:51,162 main.py:47] epoch 3283, training loss: 2206.60, average training loss: 2128.19, base loss: 2376.54
[INFO 2017-06-26 19:29:51,449 main.py:47] epoch 3284, training loss: 1970.14, average training loss: 2127.91, base loss: 2376.29
[INFO 2017-06-26 19:29:51,734 main.py:47] epoch 3285, training loss: 2312.23, average training loss: 2127.91, base loss: 2376.36
[INFO 2017-06-26 19:29:52,021 main.py:47] epoch 3286, training loss: 1979.07, average training loss: 2127.75, base loss: 2376.18
[INFO 2017-06-26 19:29:52,306 main.py:47] epoch 3287, training loss: 1585.27, average training loss: 2127.40, base loss: 2375.71
[INFO 2017-06-26 19:29:52,592 main.py:47] epoch 3288, training loss: 2674.33, average training loss: 2128.08, base loss: 2376.61
[INFO 2017-06-26 19:29:52,881 main.py:47] epoch 3289, training loss: 1923.23, average training loss: 2127.86, base loss: 2376.39
[INFO 2017-06-26 19:29:53,164 main.py:47] epoch 3290, training loss: 2055.06, average training loss: 2127.78, base loss: 2376.24
[INFO 2017-06-26 19:29:53,445 main.py:47] epoch 3291, training loss: 1793.06, average training loss: 2127.62, base loss: 2376.05
[INFO 2017-06-26 19:29:53,725 main.py:47] epoch 3292, training loss: 1983.06, average training loss: 2127.60, base loss: 2376.03
[INFO 2017-06-26 19:29:54,009 main.py:47] epoch 3293, training loss: 2286.05, average training loss: 2127.94, base loss: 2376.29
[INFO 2017-06-26 19:29:54,289 main.py:47] epoch 3294, training loss: 1914.75, average training loss: 2127.58, base loss: 2375.95
[INFO 2017-06-26 19:29:54,572 main.py:47] epoch 3295, training loss: 1911.65, average training loss: 2126.98, base loss: 2375.23
[INFO 2017-06-26 19:29:54,859 main.py:47] epoch 3296, training loss: 1782.40, average training loss: 2127.00, base loss: 2375.18
[INFO 2017-06-26 19:29:55,141 main.py:47] epoch 3297, training loss: 2091.11, average training loss: 2126.94, base loss: 2375.34
[INFO 2017-06-26 19:29:55,427 main.py:47] epoch 3298, training loss: 1808.80, average training loss: 2126.63, base loss: 2374.99
[INFO 2017-06-26 19:29:55,716 main.py:47] epoch 3299, training loss: 2110.20, average training loss: 2126.70, base loss: 2375.37
[INFO 2017-06-26 19:29:55,717 main.py:49] epoch 3299, testing
[INFO 2017-06-26 19:29:59,494 main.py:100] average testing loss: 2128.07, base loss: 2409.98
[INFO 2017-06-26 19:29:59,519 main.py:73] current best accuracy: 2020.75
[INFO 2017-06-26 19:29:59,807 main.py:47] epoch 3300, training loss: 2262.11, average training loss: 2126.39, base loss: 2375.27
[INFO 2017-06-26 19:30:00,086 main.py:47] epoch 3301, training loss: 2044.58, average training loss: 2126.19, base loss: 2375.19
[INFO 2017-06-26 19:30:00,367 main.py:47] epoch 3302, training loss: 2231.00, average training loss: 2125.95, base loss: 2375.05
[INFO 2017-06-26 19:30:00,647 main.py:47] epoch 3303, training loss: 2104.44, average training loss: 2125.81, base loss: 2374.81
[INFO 2017-06-26 19:30:00,927 main.py:47] epoch 3304, training loss: 2191.90, average training loss: 2125.93, base loss: 2374.99
[INFO 2017-06-26 19:30:01,214 main.py:47] epoch 3305, training loss: 1865.23, average training loss: 2125.74, base loss: 2374.84
[INFO 2017-06-26 19:30:01,498 main.py:47] epoch 3306, training loss: 2308.07, average training loss: 2125.88, base loss: 2375.06
[INFO 2017-06-26 19:30:01,778 main.py:47] epoch 3307, training loss: 2162.31, average training loss: 2126.14, base loss: 2375.51
[INFO 2017-06-26 19:30:02,066 main.py:47] epoch 3308, training loss: 1708.01, average training loss: 2125.75, base loss: 2374.96
[INFO 2017-06-26 19:30:02,347 main.py:47] epoch 3309, training loss: 2143.65, average training loss: 2125.55, base loss: 2374.95
[INFO 2017-06-26 19:30:02,630 main.py:47] epoch 3310, training loss: 1905.47, average training loss: 2124.95, base loss: 2374.52
[INFO 2017-06-26 19:30:02,913 main.py:47] epoch 3311, training loss: 1706.07, average training loss: 2124.29, base loss: 2373.93
[INFO 2017-06-26 19:30:03,199 main.py:47] epoch 3312, training loss: 2200.12, average training loss: 2124.40, base loss: 2374.16
[INFO 2017-06-26 19:30:03,480 main.py:47] epoch 3313, training loss: 1966.03, average training loss: 2124.41, base loss: 2374.14
[INFO 2017-06-26 19:30:03,764 main.py:47] epoch 3314, training loss: 1875.01, average training loss: 2124.01, base loss: 2373.86
[INFO 2017-06-26 19:30:04,049 main.py:47] epoch 3315, training loss: 2059.63, average training loss: 2124.07, base loss: 2374.05
[INFO 2017-06-26 19:30:04,334 main.py:47] epoch 3316, training loss: 2086.79, average training loss: 2124.08, base loss: 2374.35
[INFO 2017-06-26 19:30:04,614 main.py:47] epoch 3317, training loss: 2489.55, average training loss: 2124.36, base loss: 2374.81
[INFO 2017-06-26 19:30:04,894 main.py:47] epoch 3318, training loss: 1957.63, average training loss: 2124.29, base loss: 2374.77
[INFO 2017-06-26 19:30:05,178 main.py:47] epoch 3319, training loss: 1917.81, average training loss: 2124.51, base loss: 2374.86
[INFO 2017-06-26 19:30:05,460 main.py:47] epoch 3320, training loss: 2152.25, average training loss: 2124.71, base loss: 2374.96
[INFO 2017-06-26 19:30:05,740 main.py:47] epoch 3321, training loss: 1824.85, average training loss: 2124.51, base loss: 2374.65
[INFO 2017-06-26 19:30:06,025 main.py:47] epoch 3322, training loss: 2094.46, average training loss: 2124.16, base loss: 2374.40
[INFO 2017-06-26 19:30:06,305 main.py:47] epoch 3323, training loss: 1923.93, average training loss: 2124.06, base loss: 2374.21
[INFO 2017-06-26 19:30:06,585 main.py:47] epoch 3324, training loss: 1922.45, average training loss: 2124.11, base loss: 2374.06
[INFO 2017-06-26 19:30:06,866 main.py:47] epoch 3325, training loss: 2003.82, average training loss: 2123.91, base loss: 2373.88
[INFO 2017-06-26 19:30:07,149 main.py:47] epoch 3326, training loss: 1787.08, average training loss: 2123.74, base loss: 2373.59
[INFO 2017-06-26 19:30:07,429 main.py:47] epoch 3327, training loss: 2006.72, average training loss: 2123.65, base loss: 2373.48
[INFO 2017-06-26 19:30:07,714 main.py:47] epoch 3328, training loss: 2140.01, average training loss: 2123.52, base loss: 2373.23
[INFO 2017-06-26 19:30:07,997 main.py:47] epoch 3329, training loss: 2318.16, average training loss: 2123.69, base loss: 2373.36
[INFO 2017-06-26 19:30:08,281 main.py:47] epoch 3330, training loss: 2231.61, average training loss: 2124.13, base loss: 2373.90
[INFO 2017-06-26 19:30:08,564 main.py:47] epoch 3331, training loss: 2514.27, average training loss: 2124.67, base loss: 2374.64
[INFO 2017-06-26 19:30:08,848 main.py:47] epoch 3332, training loss: 1868.37, average training loss: 2124.63, base loss: 2374.62
[INFO 2017-06-26 19:30:09,131 main.py:47] epoch 3333, training loss: 1867.61, average training loss: 2124.16, base loss: 2373.93
[INFO 2017-06-26 19:30:09,415 main.py:47] epoch 3334, training loss: 2245.74, average training loss: 2124.31, base loss: 2374.18
[INFO 2017-06-26 19:30:09,698 main.py:47] epoch 3335, training loss: 1964.85, average training loss: 2123.73, base loss: 2373.60
[INFO 2017-06-26 19:30:09,984 main.py:47] epoch 3336, training loss: 2516.33, average training loss: 2124.08, base loss: 2374.29
[INFO 2017-06-26 19:30:10,267 main.py:47] epoch 3337, training loss: 2254.32, average training loss: 2124.62, base loss: 2374.93
[INFO 2017-06-26 19:30:10,551 main.py:47] epoch 3338, training loss: 2286.57, average training loss: 2124.86, base loss: 2375.23
[INFO 2017-06-26 19:30:10,840 main.py:47] epoch 3339, training loss: 2209.21, average training loss: 2124.81, base loss: 2375.21
[INFO 2017-06-26 19:30:11,123 main.py:47] epoch 3340, training loss: 2138.72, average training loss: 2124.86, base loss: 2375.30
[INFO 2017-06-26 19:30:11,406 main.py:47] epoch 3341, training loss: 2170.70, average training loss: 2124.50, base loss: 2374.93
[INFO 2017-06-26 19:30:11,691 main.py:47] epoch 3342, training loss: 1950.10, average training loss: 2124.24, base loss: 2374.83
[INFO 2017-06-26 19:30:11,974 main.py:47] epoch 3343, training loss: 2257.24, average training loss: 2124.31, base loss: 2374.89
[INFO 2017-06-26 19:30:12,258 main.py:47] epoch 3344, training loss: 2490.91, average training loss: 2125.00, base loss: 2375.50
[INFO 2017-06-26 19:30:12,539 main.py:47] epoch 3345, training loss: 1928.85, average training loss: 2124.71, base loss: 2375.44
[INFO 2017-06-26 19:30:12,824 main.py:47] epoch 3346, training loss: 2221.46, average training loss: 2124.59, base loss: 2375.61
[INFO 2017-06-26 19:30:13,108 main.py:47] epoch 3347, training loss: 1751.96, average training loss: 2124.10, base loss: 2375.18
[INFO 2017-06-26 19:30:13,392 main.py:47] epoch 3348, training loss: 2059.38, average training loss: 2124.28, base loss: 2375.57
[INFO 2017-06-26 19:30:13,677 main.py:47] epoch 3349, training loss: 2105.89, average training loss: 2124.32, base loss: 2375.59
[INFO 2017-06-26 19:30:13,965 main.py:47] epoch 3350, training loss: 1991.18, average training loss: 2124.28, base loss: 2375.64
[INFO 2017-06-26 19:30:14,249 main.py:47] epoch 3351, training loss: 2253.93, average training loss: 2124.43, base loss: 2375.97
[INFO 2017-06-26 19:30:14,535 main.py:47] epoch 3352, training loss: 2261.33, average training loss: 2124.14, base loss: 2375.73
[INFO 2017-06-26 19:30:14,822 main.py:47] epoch 3353, training loss: 1826.10, average training loss: 2124.09, base loss: 2375.74
[INFO 2017-06-26 19:30:15,108 main.py:47] epoch 3354, training loss: 1975.64, average training loss: 2123.79, base loss: 2375.22
[INFO 2017-06-26 19:30:15,392 main.py:47] epoch 3355, training loss: 2089.48, average training loss: 2123.64, base loss: 2375.08
[INFO 2017-06-26 19:30:15,675 main.py:47] epoch 3356, training loss: 1957.56, average training loss: 2123.33, base loss: 2374.77
[INFO 2017-06-26 19:30:15,961 main.py:47] epoch 3357, training loss: 2450.67, average training loss: 2123.76, base loss: 2375.17
[INFO 2017-06-26 19:30:16,248 main.py:47] epoch 3358, training loss: 2275.50, average training loss: 2123.99, base loss: 2375.57
[INFO 2017-06-26 19:30:16,534 main.py:47] epoch 3359, training loss: 1951.63, average training loss: 2123.91, base loss: 2375.51
[INFO 2017-06-26 19:30:16,817 main.py:47] epoch 3360, training loss: 2367.21, average training loss: 2124.33, base loss: 2376.03
[INFO 2017-06-26 19:30:17,100 main.py:47] epoch 3361, training loss: 1754.54, average training loss: 2124.00, base loss: 2375.94
[INFO 2017-06-26 19:30:17,380 main.py:47] epoch 3362, training loss: 2226.17, average training loss: 2123.94, base loss: 2375.91
[INFO 2017-06-26 19:30:17,664 main.py:47] epoch 3363, training loss: 1894.45, average training loss: 2123.64, base loss: 2375.80
[INFO 2017-06-26 19:30:17,950 main.py:47] epoch 3364, training loss: 2271.86, average training loss: 2123.67, base loss: 2375.84
[INFO 2017-06-26 19:30:18,230 main.py:47] epoch 3365, training loss: 2288.32, average training loss: 2123.72, base loss: 2375.86
[INFO 2017-06-26 19:30:18,513 main.py:47] epoch 3366, training loss: 2168.86, average training loss: 2123.89, base loss: 2376.07
[INFO 2017-06-26 19:30:18,794 main.py:47] epoch 3367, training loss: 2526.44, average training loss: 2124.47, base loss: 2376.75
[INFO 2017-06-26 19:30:19,077 main.py:47] epoch 3368, training loss: 1903.51, average training loss: 2124.17, base loss: 2376.53
[INFO 2017-06-26 19:30:19,361 main.py:47] epoch 3369, training loss: 2280.21, average training loss: 2124.43, base loss: 2376.85
[INFO 2017-06-26 19:30:19,642 main.py:47] epoch 3370, training loss: 2245.09, average training loss: 2124.66, base loss: 2377.03
[INFO 2017-06-26 19:30:19,927 main.py:47] epoch 3371, training loss: 2259.53, average training loss: 2124.82, base loss: 2377.20
[INFO 2017-06-26 19:30:20,212 main.py:47] epoch 3372, training loss: 1892.99, average training loss: 2124.79, base loss: 2377.19
[INFO 2017-06-26 19:30:20,492 main.py:47] epoch 3373, training loss: 1678.59, average training loss: 2124.35, base loss: 2376.83
[INFO 2017-06-26 19:30:20,775 main.py:47] epoch 3374, training loss: 2405.43, average training loss: 2124.30, base loss: 2376.93
[INFO 2017-06-26 19:30:21,055 main.py:47] epoch 3375, training loss: 1970.13, average training loss: 2124.18, base loss: 2376.98
[INFO 2017-06-26 19:30:21,344 main.py:47] epoch 3376, training loss: 1991.78, average training loss: 2123.62, base loss: 2376.31
[INFO 2017-06-26 19:30:21,628 main.py:47] epoch 3377, training loss: 1924.31, average training loss: 2123.02, base loss: 2375.82
[INFO 2017-06-26 19:30:21,913 main.py:47] epoch 3378, training loss: 1983.15, average training loss: 2123.05, base loss: 2376.00
[INFO 2017-06-26 19:30:22,196 main.py:47] epoch 3379, training loss: 2206.08, average training loss: 2123.24, base loss: 2376.16
[INFO 2017-06-26 19:30:22,484 main.py:47] epoch 3380, training loss: 2483.02, average training loss: 2124.04, base loss: 2377.19
[INFO 2017-06-26 19:30:22,771 main.py:47] epoch 3381, training loss: 2203.09, average training loss: 2123.85, base loss: 2377.13
[INFO 2017-06-26 19:30:23,058 main.py:47] epoch 3382, training loss: 2244.82, average training loss: 2123.84, base loss: 2377.23
[INFO 2017-06-26 19:30:23,342 main.py:47] epoch 3383, training loss: 2158.34, average training loss: 2123.42, base loss: 2376.92
[INFO 2017-06-26 19:30:23,625 main.py:47] epoch 3384, training loss: 2513.32, average training loss: 2123.31, base loss: 2377.00
[INFO 2017-06-26 19:30:23,909 main.py:47] epoch 3385, training loss: 2088.51, average training loss: 2123.27, base loss: 2377.00
[INFO 2017-06-26 19:30:24,200 main.py:47] epoch 3386, training loss: 1945.01, average training loss: 2123.03, base loss: 2376.65
[INFO 2017-06-26 19:30:24,489 main.py:47] epoch 3387, training loss: 1945.94, average training loss: 2123.24, base loss: 2376.88
[INFO 2017-06-26 19:30:24,781 main.py:47] epoch 3388, training loss: 2230.74, average training loss: 2123.56, base loss: 2377.29
[INFO 2017-06-26 19:30:25,069 main.py:47] epoch 3389, training loss: 1693.20, average training loss: 2122.65, base loss: 2376.28
[INFO 2017-06-26 19:30:25,359 main.py:47] epoch 3390, training loss: 2430.14, average training loss: 2122.98, base loss: 2376.71
[INFO 2017-06-26 19:30:25,645 main.py:47] epoch 3391, training loss: 2056.99, average training loss: 2123.10, base loss: 2377.01
[INFO 2017-06-26 19:30:25,930 main.py:47] epoch 3392, training loss: 2291.25, average training loss: 2122.87, base loss: 2376.79
[INFO 2017-06-26 19:30:26,220 main.py:47] epoch 3393, training loss: 2055.27, average training loss: 2122.27, base loss: 2376.20
[INFO 2017-06-26 19:30:26,501 main.py:47] epoch 3394, training loss: 2154.32, average training loss: 2121.90, base loss: 2375.85
[INFO 2017-06-26 19:30:26,787 main.py:47] epoch 3395, training loss: 2236.78, average training loss: 2121.65, base loss: 2375.45
[INFO 2017-06-26 19:30:27,074 main.py:47] epoch 3396, training loss: 2104.27, average training loss: 2121.52, base loss: 2375.35
[INFO 2017-06-26 19:30:27,359 main.py:47] epoch 3397, training loss: 1772.97, average training loss: 2120.96, base loss: 2374.87
[INFO 2017-06-26 19:30:27,643 main.py:47] epoch 3398, training loss: 2229.06, average training loss: 2121.08, base loss: 2374.94
[INFO 2017-06-26 19:30:27,925 main.py:47] epoch 3399, training loss: 2036.28, average training loss: 2120.81, base loss: 2374.54
[INFO 2017-06-26 19:30:27,925 main.py:49] epoch 3399, testing
[INFO 2017-06-26 19:30:31,652 main.py:100] average testing loss: 1992.18, base loss: 2265.47
[INFO 2017-06-26 19:30:31,678 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 19:30:31,692 main.py:73] current best accuracy: 1992.18
[INFO 2017-06-26 19:30:31,976 main.py:47] epoch 3400, training loss: 2243.28, average training loss: 2120.71, base loss: 2374.37
[INFO 2017-06-26 19:30:32,262 main.py:47] epoch 3401, training loss: 2127.45, average training loss: 2120.76, base loss: 2374.42
[INFO 2017-06-26 19:30:32,546 main.py:47] epoch 3402, training loss: 2014.37, average training loss: 2120.75, base loss: 2374.57
[INFO 2017-06-26 19:30:32,826 main.py:47] epoch 3403, training loss: 2430.48, average training loss: 2120.91, base loss: 2374.77
[INFO 2017-06-26 19:30:33,108 main.py:47] epoch 3404, training loss: 2117.45, average training loss: 2120.89, base loss: 2374.77
[INFO 2017-06-26 19:30:33,394 main.py:47] epoch 3405, training loss: 2549.95, average training loss: 2121.45, base loss: 2375.46
[INFO 2017-06-26 19:30:33,681 main.py:47] epoch 3406, training loss: 2324.39, average training loss: 2121.48, base loss: 2375.56
[INFO 2017-06-26 19:30:33,971 main.py:47] epoch 3407, training loss: 2124.94, average training loss: 2120.88, base loss: 2374.92
[INFO 2017-06-26 19:30:34,256 main.py:47] epoch 3408, training loss: 2071.36, average training loss: 2120.60, base loss: 2374.70
[INFO 2017-06-26 19:30:34,544 main.py:47] epoch 3409, training loss: 1859.45, average training loss: 2120.07, base loss: 2374.08
[INFO 2017-06-26 19:30:34,828 main.py:47] epoch 3410, training loss: 2435.42, average training loss: 2120.28, base loss: 2374.22
[INFO 2017-06-26 19:30:35,111 main.py:47] epoch 3411, training loss: 2154.60, average training loss: 2120.39, base loss: 2374.57
[INFO 2017-06-26 19:30:35,400 main.py:47] epoch 3412, training loss: 2379.35, average training loss: 2120.59, base loss: 2374.98
[INFO 2017-06-26 19:30:35,681 main.py:47] epoch 3413, training loss: 1888.61, average training loss: 2120.04, base loss: 2374.32
[INFO 2017-06-26 19:30:35,966 main.py:47] epoch 3414, training loss: 1892.66, average training loss: 2119.73, base loss: 2373.89
[INFO 2017-06-26 19:30:36,254 main.py:47] epoch 3415, training loss: 2074.97, average training loss: 2119.47, base loss: 2373.57
[INFO 2017-06-26 19:30:36,534 main.py:47] epoch 3416, training loss: 1941.48, average training loss: 2119.59, base loss: 2373.87
[INFO 2017-06-26 19:30:36,821 main.py:47] epoch 3417, training loss: 2004.05, average training loss: 2119.50, base loss: 2373.98
[INFO 2017-06-26 19:30:37,108 main.py:47] epoch 3418, training loss: 2270.32, average training loss: 2119.53, base loss: 2374.14
[INFO 2017-06-26 19:30:37,391 main.py:47] epoch 3419, training loss: 1956.08, average training loss: 2119.32, base loss: 2373.87
[INFO 2017-06-26 19:30:37,675 main.py:47] epoch 3420, training loss: 2508.65, average training loss: 2119.28, base loss: 2373.93
[INFO 2017-06-26 19:30:37,959 main.py:47] epoch 3421, training loss: 2542.65, average training loss: 2119.55, base loss: 2374.39
[INFO 2017-06-26 19:30:38,244 main.py:47] epoch 3422, training loss: 2063.05, average training loss: 2119.77, base loss: 2374.69
[INFO 2017-06-26 19:30:38,530 main.py:47] epoch 3423, training loss: 2313.17, average training loss: 2120.13, base loss: 2375.28
[INFO 2017-06-26 19:30:38,812 main.py:47] epoch 3424, training loss: 2323.25, average training loss: 2120.49, base loss: 2375.80
[INFO 2017-06-26 19:30:39,097 main.py:47] epoch 3425, training loss: 2011.58, average training loss: 2120.33, base loss: 2375.77
[INFO 2017-06-26 19:30:39,383 main.py:47] epoch 3426, training loss: 2473.51, average training loss: 2120.53, base loss: 2375.94
[INFO 2017-06-26 19:30:39,673 main.py:47] epoch 3427, training loss: 2031.55, average training loss: 2120.71, base loss: 2376.16
[INFO 2017-06-26 19:30:39,959 main.py:47] epoch 3428, training loss: 1972.31, average training loss: 2120.92, base loss: 2376.45
[INFO 2017-06-26 19:30:40,240 main.py:47] epoch 3429, training loss: 2127.79, average training loss: 2120.92, base loss: 2376.53
[INFO 2017-06-26 19:30:40,525 main.py:47] epoch 3430, training loss: 1923.42, average training loss: 2120.48, base loss: 2376.04
[INFO 2017-06-26 19:30:40,809 main.py:47] epoch 3431, training loss: 1884.52, average training loss: 2120.19, base loss: 2375.82
[INFO 2017-06-26 19:30:41,090 main.py:47] epoch 3432, training loss: 2172.96, average training loss: 2120.62, base loss: 2376.34
[INFO 2017-06-26 19:30:41,373 main.py:47] epoch 3433, training loss: 2215.86, average training loss: 2120.65, base loss: 2376.51
[INFO 2017-06-26 19:30:41,657 main.py:47] epoch 3434, training loss: 2113.73, average training loss: 2120.10, base loss: 2375.76
[INFO 2017-06-26 19:30:41,943 main.py:47] epoch 3435, training loss: 2216.90, average training loss: 2120.01, base loss: 2375.84
[INFO 2017-06-26 19:30:42,228 main.py:47] epoch 3436, training loss: 2419.43, average training loss: 2120.18, base loss: 2376.08
[INFO 2017-06-26 19:30:42,513 main.py:47] epoch 3437, training loss: 1843.58, average training loss: 2119.84, base loss: 2376.01
[INFO 2017-06-26 19:30:42,800 main.py:47] epoch 3438, training loss: 2081.55, average training loss: 2119.77, base loss: 2375.96
[INFO 2017-06-26 19:30:43,085 main.py:47] epoch 3439, training loss: 2191.73, average training loss: 2119.76, base loss: 2375.96
[INFO 2017-06-26 19:30:43,372 main.py:47] epoch 3440, training loss: 2047.55, average training loss: 2120.01, base loss: 2376.28
[INFO 2017-06-26 19:30:43,655 main.py:47] epoch 3441, training loss: 2152.14, average training loss: 2119.98, base loss: 2376.31
[INFO 2017-06-26 19:30:43,941 main.py:47] epoch 3442, training loss: 2372.73, average training loss: 2120.55, base loss: 2376.94
[INFO 2017-06-26 19:30:44,222 main.py:47] epoch 3443, training loss: 2120.58, average training loss: 2120.67, base loss: 2377.16
[INFO 2017-06-26 19:30:44,504 main.py:47] epoch 3444, training loss: 2322.77, average training loss: 2120.61, base loss: 2377.24
[INFO 2017-06-26 19:30:44,793 main.py:47] epoch 3445, training loss: 2325.99, average training loss: 2121.02, base loss: 2377.80
[INFO 2017-06-26 19:30:45,078 main.py:47] epoch 3446, training loss: 2295.82, average training loss: 2121.23, base loss: 2378.24
[INFO 2017-06-26 19:30:45,365 main.py:47] epoch 3447, training loss: 2511.02, average training loss: 2121.11, base loss: 2378.29
[INFO 2017-06-26 19:30:45,648 main.py:47] epoch 3448, training loss: 2063.86, average training loss: 2121.02, base loss: 2378.19
[INFO 2017-06-26 19:30:45,934 main.py:47] epoch 3449, training loss: 2297.94, average training loss: 2120.94, base loss: 2378.01
[INFO 2017-06-26 19:30:46,218 main.py:47] epoch 3450, training loss: 2046.43, average training loss: 2120.91, base loss: 2377.86
[INFO 2017-06-26 19:30:46,501 main.py:47] epoch 3451, training loss: 2055.86, average training loss: 2121.03, base loss: 2377.80
[INFO 2017-06-26 19:30:46,785 main.py:47] epoch 3452, training loss: 2085.48, average training loss: 2121.05, base loss: 2377.83
[INFO 2017-06-26 19:30:47,068 main.py:47] epoch 3453, training loss: 2372.45, average training loss: 2121.44, base loss: 2378.34
[INFO 2017-06-26 19:30:47,349 main.py:47] epoch 3454, training loss: 2058.84, average training loss: 2121.46, base loss: 2378.43
[INFO 2017-06-26 19:30:47,633 main.py:47] epoch 3455, training loss: 2053.26, average training loss: 2121.59, base loss: 2378.65
[INFO 2017-06-26 19:30:47,922 main.py:47] epoch 3456, training loss: 2145.68, average training loss: 2121.63, base loss: 2378.88
[INFO 2017-06-26 19:30:48,208 main.py:47] epoch 3457, training loss: 2153.23, average training loss: 2121.75, base loss: 2379.08
[INFO 2017-06-26 19:30:48,491 main.py:47] epoch 3458, training loss: 2071.44, average training loss: 2121.54, base loss: 2378.75
[INFO 2017-06-26 19:30:48,777 main.py:47] epoch 3459, training loss: 2279.51, average training loss: 2121.70, base loss: 2378.83
[INFO 2017-06-26 19:30:49,060 main.py:47] epoch 3460, training loss: 2034.52, average training loss: 2121.70, base loss: 2378.94
[INFO 2017-06-26 19:30:49,343 main.py:47] epoch 3461, training loss: 2069.84, average training loss: 2121.64, base loss: 2378.99
[INFO 2017-06-26 19:30:49,631 main.py:47] epoch 3462, training loss: 1997.06, average training loss: 2121.14, base loss: 2378.48
[INFO 2017-06-26 19:30:49,915 main.py:47] epoch 3463, training loss: 1899.88, average training loss: 2120.68, base loss: 2377.97
[INFO 2017-06-26 19:30:50,202 main.py:47] epoch 3464, training loss: 1854.03, average training loss: 2120.35, base loss: 2377.63
[INFO 2017-06-26 19:30:50,490 main.py:47] epoch 3465, training loss: 1772.71, average training loss: 2119.98, base loss: 2377.11
[INFO 2017-06-26 19:30:50,774 main.py:47] epoch 3466, training loss: 1881.10, average training loss: 2119.81, base loss: 2376.85
[INFO 2017-06-26 19:30:51,058 main.py:47] epoch 3467, training loss: 1992.46, average training loss: 2119.64, base loss: 2376.58
[INFO 2017-06-26 19:30:51,342 main.py:47] epoch 3468, training loss: 1950.34, average training loss: 2119.35, base loss: 2376.39
[INFO 2017-06-26 19:30:51,628 main.py:47] epoch 3469, training loss: 2179.98, average training loss: 2119.52, base loss: 2376.82
[INFO 2017-06-26 19:30:51,916 main.py:47] epoch 3470, training loss: 2132.72, average training loss: 2119.21, base loss: 2376.41
[INFO 2017-06-26 19:30:52,204 main.py:47] epoch 3471, training loss: 2218.47, average training loss: 2119.27, base loss: 2376.48
[INFO 2017-06-26 19:30:52,486 main.py:47] epoch 3472, training loss: 2083.12, average training loss: 2119.04, base loss: 2376.02
[INFO 2017-06-26 19:30:52,774 main.py:47] epoch 3473, training loss: 2600.60, average training loss: 2119.77, base loss: 2376.90
[INFO 2017-06-26 19:30:53,055 main.py:47] epoch 3474, training loss: 2220.92, average training loss: 2119.95, base loss: 2377.14
[INFO 2017-06-26 19:30:53,336 main.py:47] epoch 3475, training loss: 2352.45, average training loss: 2120.13, base loss: 2377.18
[INFO 2017-06-26 19:30:53,617 main.py:47] epoch 3476, training loss: 1977.98, average training loss: 2120.18, base loss: 2377.52
[INFO 2017-06-26 19:30:53,902 main.py:47] epoch 3477, training loss: 2173.50, average training loss: 2120.53, base loss: 2377.97
[INFO 2017-06-26 19:30:54,183 main.py:47] epoch 3478, training loss: 2017.15, average training loss: 2120.49, base loss: 2377.91
[INFO 2017-06-26 19:30:54,471 main.py:47] epoch 3479, training loss: 2028.03, average training loss: 2120.80, base loss: 2378.11
[INFO 2017-06-26 19:30:54,754 main.py:47] epoch 3480, training loss: 2043.76, average training loss: 2120.73, base loss: 2378.25
[INFO 2017-06-26 19:30:55,042 main.py:47] epoch 3481, training loss: 1562.15, average training loss: 2119.94, base loss: 2377.66
[INFO 2017-06-26 19:30:55,323 main.py:47] epoch 3482, training loss: 2344.12, average training loss: 2120.47, base loss: 2378.48
[INFO 2017-06-26 19:30:55,606 main.py:47] epoch 3483, training loss: 1894.88, average training loss: 2119.92, base loss: 2378.12
[INFO 2017-06-26 19:30:55,893 main.py:47] epoch 3484, training loss: 2346.48, average training loss: 2120.22, base loss: 2378.65
[INFO 2017-06-26 19:30:56,174 main.py:47] epoch 3485, training loss: 2300.84, average training loss: 2120.10, base loss: 2378.67
[INFO 2017-06-26 19:30:56,459 main.py:47] epoch 3486, training loss: 2235.43, average training loss: 2120.38, base loss: 2379.06
[INFO 2017-06-26 19:30:56,749 main.py:47] epoch 3487, training loss: 2185.42, average training loss: 2120.31, base loss: 2379.00
[INFO 2017-06-26 19:30:57,032 main.py:47] epoch 3488, training loss: 2004.90, average training loss: 2120.35, base loss: 2379.11
[INFO 2017-06-26 19:30:57,324 main.py:47] epoch 3489, training loss: 2104.58, average training loss: 2120.32, base loss: 2378.92
[INFO 2017-06-26 19:30:57,611 main.py:47] epoch 3490, training loss: 1970.71, average training loss: 2120.05, base loss: 2378.61
[INFO 2017-06-26 19:30:57,896 main.py:47] epoch 3491, training loss: 2684.76, average training loss: 2120.83, base loss: 2379.53
[INFO 2017-06-26 19:30:58,180 main.py:47] epoch 3492, training loss: 1699.58, average training loss: 2119.90, base loss: 2378.60
[INFO 2017-06-26 19:30:58,470 main.py:47] epoch 3493, training loss: 2662.45, average training loss: 2120.34, base loss: 2379.07
[INFO 2017-06-26 19:30:58,762 main.py:47] epoch 3494, training loss: 2070.53, average training loss: 2119.89, base loss: 2378.66
[INFO 2017-06-26 19:30:59,049 main.py:47] epoch 3495, training loss: 1893.70, average training loss: 2119.51, base loss: 2378.32
[INFO 2017-06-26 19:30:59,330 main.py:47] epoch 3496, training loss: 2323.07, average training loss: 2119.77, base loss: 2378.74
[INFO 2017-06-26 19:30:59,615 main.py:47] epoch 3497, training loss: 1937.30, average training loss: 2119.64, base loss: 2378.75
[INFO 2017-06-26 19:30:59,900 main.py:47] epoch 3498, training loss: 2206.75, average training loss: 2119.91, base loss: 2378.99
[INFO 2017-06-26 19:31:00,188 main.py:47] epoch 3499, training loss: 2387.59, average training loss: 2119.59, base loss: 2378.79
[INFO 2017-06-26 19:31:00,188 main.py:49] epoch 3499, testing
[INFO 2017-06-26 19:31:04,108 main.py:100] average testing loss: 2103.82, base loss: 2360.92
[INFO 2017-06-26 19:31:04,137 main.py:73] current best accuracy: 1992.18
[INFO 2017-06-26 19:31:04,425 main.py:47] epoch 3500, training loss: 1807.07, average training loss: 2119.02, base loss: 2377.96
[INFO 2017-06-26 19:31:04,711 main.py:47] epoch 3501, training loss: 1985.54, average training loss: 2118.74, base loss: 2377.68
[INFO 2017-06-26 19:31:04,998 main.py:47] epoch 3502, training loss: 2311.94, average training loss: 2118.84, base loss: 2378.01
[INFO 2017-06-26 19:31:05,285 main.py:47] epoch 3503, training loss: 2280.21, average training loss: 2119.19, base loss: 2378.47
[INFO 2017-06-26 19:31:05,570 main.py:47] epoch 3504, training loss: 2198.93, average training loss: 2119.26, base loss: 2378.59
[INFO 2017-06-26 19:31:05,861 main.py:47] epoch 3505, training loss: 1942.96, average training loss: 2118.86, base loss: 2378.18
[INFO 2017-06-26 19:31:06,151 main.py:47] epoch 3506, training loss: 2428.23, average training loss: 2119.30, base loss: 2378.65
[INFO 2017-06-26 19:31:06,437 main.py:47] epoch 3507, training loss: 2167.71, average training loss: 2119.37, base loss: 2378.51
[INFO 2017-06-26 19:31:06,727 main.py:47] epoch 3508, training loss: 2101.88, average training loss: 2119.32, base loss: 2378.41
[INFO 2017-06-26 19:31:07,069 main.py:47] epoch 3509, training loss: 2075.43, average training loss: 2119.50, base loss: 2378.60
[INFO 2017-06-26 19:31:07,412 main.py:47] epoch 3510, training loss: 2006.85, average training loss: 2119.55, base loss: 2378.67
[INFO 2017-06-26 19:31:07,745 main.py:47] epoch 3511, training loss: 2111.74, average training loss: 2119.73, base loss: 2378.99
[INFO 2017-06-26 19:31:08,093 main.py:47] epoch 3512, training loss: 2097.64, average training loss: 2119.68, base loss: 2379.01
[INFO 2017-06-26 19:31:08,417 main.py:47] epoch 3513, training loss: 2023.54, average training loss: 2119.73, base loss: 2379.15
[INFO 2017-06-26 19:31:08,762 main.py:47] epoch 3514, training loss: 2072.75, average training loss: 2119.28, base loss: 2378.57
[INFO 2017-06-26 19:31:09,080 main.py:47] epoch 3515, training loss: 1770.66, average training loss: 2118.98, base loss: 2378.17
[INFO 2017-06-26 19:31:09,379 main.py:47] epoch 3516, training loss: 2254.62, average training loss: 2119.12, base loss: 2378.40
[INFO 2017-06-26 19:31:09,723 main.py:47] epoch 3517, training loss: 2182.61, average training loss: 2118.92, base loss: 2378.35
[INFO 2017-06-26 19:31:10,082 main.py:47] epoch 3518, training loss: 2052.96, average training loss: 2118.65, base loss: 2377.98
[INFO 2017-06-26 19:31:10,428 main.py:47] epoch 3519, training loss: 2132.84, average training loss: 2118.48, base loss: 2377.77
[INFO 2017-06-26 19:31:10,732 main.py:47] epoch 3520, training loss: 2088.03, average training loss: 2117.96, base loss: 2377.22
[INFO 2017-06-26 19:31:11,018 main.py:47] epoch 3521, training loss: 2412.09, average training loss: 2118.01, base loss: 2377.52
[INFO 2017-06-26 19:31:11,308 main.py:47] epoch 3522, training loss: 2180.34, average training loss: 2117.75, base loss: 2377.24
[INFO 2017-06-26 19:31:11,591 main.py:47] epoch 3523, training loss: 1948.61, average training loss: 2117.69, base loss: 2377.29
[INFO 2017-06-26 19:31:11,879 main.py:47] epoch 3524, training loss: 1878.27, average training loss: 2117.34, base loss: 2376.69
[INFO 2017-06-26 19:31:12,163 main.py:47] epoch 3525, training loss: 1995.23, average training loss: 2117.40, base loss: 2376.50
[INFO 2017-06-26 19:31:12,452 main.py:47] epoch 3526, training loss: 1805.25, average training loss: 2117.10, base loss: 2376.34
[INFO 2017-06-26 19:31:12,741 main.py:47] epoch 3527, training loss: 2162.13, average training loss: 2116.87, base loss: 2376.32
[INFO 2017-06-26 19:31:13,077 main.py:47] epoch 3528, training loss: 2034.28, average training loss: 2116.51, base loss: 2375.79
[INFO 2017-06-26 19:31:13,388 main.py:47] epoch 3529, training loss: 1905.46, average training loss: 2116.42, base loss: 2375.78
[INFO 2017-06-26 19:31:13,674 main.py:47] epoch 3530, training loss: 1987.14, average training loss: 2116.24, base loss: 2375.61
[INFO 2017-06-26 19:31:13,966 main.py:47] epoch 3531, training loss: 2099.76, average training loss: 2116.17, base loss: 2375.53
[INFO 2017-06-26 19:31:14,257 main.py:47] epoch 3532, training loss: 2213.15, average training loss: 2116.22, base loss: 2375.74
[INFO 2017-06-26 19:31:14,544 main.py:47] epoch 3533, training loss: 1913.27, average training loss: 2116.31, base loss: 2375.89
[INFO 2017-06-26 19:31:14,914 main.py:47] epoch 3534, training loss: 2287.02, average training loss: 2116.37, base loss: 2375.94
[INFO 2017-06-26 19:31:15,244 main.py:47] epoch 3535, training loss: 2023.94, average training loss: 2115.68, base loss: 2375.28
[INFO 2017-06-26 19:31:15,536 main.py:47] epoch 3536, training loss: 2107.84, average training loss: 2115.97, base loss: 2375.49
[INFO 2017-06-26 19:31:15,882 main.py:47] epoch 3537, training loss: 1852.82, average training loss: 2115.60, base loss: 2375.16
[INFO 2017-06-26 19:31:16,182 main.py:47] epoch 3538, training loss: 1951.10, average training loss: 2115.62, base loss: 2375.31
[INFO 2017-06-26 19:31:16,471 main.py:47] epoch 3539, training loss: 1822.05, average training loss: 2115.21, base loss: 2374.89
[INFO 2017-06-26 19:31:16,804 main.py:47] epoch 3540, training loss: 2089.32, average training loss: 2115.17, base loss: 2374.94
[INFO 2017-06-26 19:31:17,157 main.py:47] epoch 3541, training loss: 1789.55, average training loss: 2114.93, base loss: 2374.48
[INFO 2017-06-26 19:31:17,450 main.py:47] epoch 3542, training loss: 2150.46, average training loss: 2115.00, base loss: 2374.50
[INFO 2017-06-26 19:31:17,781 main.py:47] epoch 3543, training loss: 2068.68, average training loss: 2114.58, base loss: 2374.05
[INFO 2017-06-26 19:31:18,122 main.py:47] epoch 3544, training loss: 1807.41, average training loss: 2114.29, base loss: 2373.64
[INFO 2017-06-26 19:31:18,412 main.py:47] epoch 3545, training loss: 2112.30, average training loss: 2114.50, base loss: 2373.90
[INFO 2017-06-26 19:31:18,713 main.py:47] epoch 3546, training loss: 2532.20, average training loss: 2114.65, base loss: 2374.01
[INFO 2017-06-26 19:31:19,058 main.py:47] epoch 3547, training loss: 2143.37, average training loss: 2114.31, base loss: 2373.80
[INFO 2017-06-26 19:31:19,356 main.py:47] epoch 3548, training loss: 1856.45, average training loss: 2114.06, base loss: 2373.52
[INFO 2017-06-26 19:31:19,647 main.py:47] epoch 3549, training loss: 2107.72, average training loss: 2114.43, base loss: 2374.15
[INFO 2017-06-26 19:31:19,936 main.py:47] epoch 3550, training loss: 2063.05, average training loss: 2114.27, base loss: 2373.83
[INFO 2017-06-26 19:31:20,226 main.py:47] epoch 3551, training loss: 2302.75, average training loss: 2114.58, base loss: 2374.00
[INFO 2017-06-26 19:31:20,586 main.py:47] epoch 3552, training loss: 1999.93, average training loss: 2114.60, base loss: 2374.00
[INFO 2017-06-26 19:31:20,882 main.py:47] epoch 3553, training loss: 2202.33, average training loss: 2114.79, base loss: 2374.27
[INFO 2017-06-26 19:31:21,168 main.py:47] epoch 3554, training loss: 1781.22, average training loss: 2114.21, base loss: 2373.59
[INFO 2017-06-26 19:31:21,547 main.py:47] epoch 3555, training loss: 1853.80, average training loss: 2114.18, base loss: 2373.62
[INFO 2017-06-26 19:31:21,844 main.py:47] epoch 3556, training loss: 2156.18, average training loss: 2114.05, base loss: 2373.67
[INFO 2017-06-26 19:31:22,133 main.py:47] epoch 3557, training loss: 2078.43, average training loss: 2114.28, base loss: 2373.87
[INFO 2017-06-26 19:31:22,425 main.py:47] epoch 3558, training loss: 1945.94, average training loss: 2114.12, base loss: 2373.78
[INFO 2017-06-26 19:31:22,794 main.py:47] epoch 3559, training loss: 2315.38, average training loss: 2114.15, base loss: 2373.73
[INFO 2017-06-26 19:31:23,089 main.py:47] epoch 3560, training loss: 1922.38, average training loss: 2114.15, base loss: 2373.59
[INFO 2017-06-26 19:31:23,383 main.py:47] epoch 3561, training loss: 2366.03, average training loss: 2114.36, base loss: 2374.20
[INFO 2017-06-26 19:31:23,716 main.py:47] epoch 3562, training loss: 1917.43, average training loss: 2114.50, base loss: 2374.51
[INFO 2017-06-26 19:31:24,039 main.py:47] epoch 3563, training loss: 2340.50, average training loss: 2115.02, base loss: 2375.27
[INFO 2017-06-26 19:31:24,330 main.py:47] epoch 3564, training loss: 2114.73, average training loss: 2115.07, base loss: 2375.35
[INFO 2017-06-26 19:31:24,616 main.py:47] epoch 3565, training loss: 2499.43, average training loss: 2115.62, base loss: 2375.95
[INFO 2017-06-26 19:31:24,944 main.py:47] epoch 3566, training loss: 2390.17, average training loss: 2115.83, base loss: 2376.17
[INFO 2017-06-26 19:31:25,251 main.py:47] epoch 3567, training loss: 1919.68, average training loss: 2115.77, base loss: 2375.95
[INFO 2017-06-26 19:31:25,540 main.py:47] epoch 3568, training loss: 2243.24, average training loss: 2116.17, base loss: 2376.22
[INFO 2017-06-26 19:31:25,907 main.py:47] epoch 3569, training loss: 2171.84, average training loss: 2116.39, base loss: 2376.52
[INFO 2017-06-26 19:31:26,240 main.py:47] epoch 3570, training loss: 2217.32, average training loss: 2116.81, base loss: 2376.84
[INFO 2017-06-26 19:31:26,537 main.py:47] epoch 3571, training loss: 1845.21, average training loss: 2116.58, base loss: 2376.61
[INFO 2017-06-26 19:31:26,916 main.py:47] epoch 3572, training loss: 2781.94, average training loss: 2117.26, base loss: 2377.36
[INFO 2017-06-26 19:31:27,224 main.py:47] epoch 3573, training loss: 2173.25, average training loss: 2116.93, base loss: 2376.91
[INFO 2017-06-26 19:31:27,571 main.py:47] epoch 3574, training loss: 1752.18, average training loss: 2116.44, base loss: 2376.16
[INFO 2017-06-26 19:31:27,881 main.py:47] epoch 3575, training loss: 2052.60, average training loss: 2116.14, base loss: 2375.86
[INFO 2017-06-26 19:31:28,172 main.py:47] epoch 3576, training loss: 2051.20, average training loss: 2116.27, base loss: 2375.93
[INFO 2017-06-26 19:31:28,463 main.py:47] epoch 3577, training loss: 1983.21, average training loss: 2116.13, base loss: 2375.62
[INFO 2017-06-26 19:31:28,748 main.py:47] epoch 3578, training loss: 2458.01, average training loss: 2116.45, base loss: 2376.23
[INFO 2017-06-26 19:31:29,033 main.py:47] epoch 3579, training loss: 1905.43, average training loss: 2116.12, base loss: 2375.91
[INFO 2017-06-26 19:31:29,316 main.py:47] epoch 3580, training loss: 1761.87, average training loss: 2115.82, base loss: 2375.59
[INFO 2017-06-26 19:31:29,598 main.py:47] epoch 3581, training loss: 2107.07, average training loss: 2115.38, base loss: 2375.09
[INFO 2017-06-26 19:31:29,944 main.py:47] epoch 3582, training loss: 2144.06, average training loss: 2115.06, base loss: 2374.94
[INFO 2017-06-26 19:31:30,245 main.py:47] epoch 3583, training loss: 1887.05, average training loss: 2114.82, base loss: 2374.65
[INFO 2017-06-26 19:31:30,537 main.py:47] epoch 3584, training loss: 2312.87, average training loss: 2115.30, base loss: 2375.22
[INFO 2017-06-26 19:31:30,827 main.py:47] epoch 3585, training loss: 2068.72, average training loss: 2115.18, base loss: 2374.89
[INFO 2017-06-26 19:31:31,118 main.py:47] epoch 3586, training loss: 2193.50, average training loss: 2115.67, base loss: 2375.41
[INFO 2017-06-26 19:31:31,403 main.py:47] epoch 3587, training loss: 1853.64, average training loss: 2115.37, base loss: 2375.22
[INFO 2017-06-26 19:31:31,710 main.py:47] epoch 3588, training loss: 1931.82, average training loss: 2115.34, base loss: 2375.26
[INFO 2017-06-26 19:31:32,015 main.py:47] epoch 3589, training loss: 2369.25, average training loss: 2115.70, base loss: 2375.69
[INFO 2017-06-26 19:31:32,308 main.py:47] epoch 3590, training loss: 2059.18, average training loss: 2116.30, base loss: 2376.39
[INFO 2017-06-26 19:31:32,617 main.py:47] epoch 3591, training loss: 2520.72, average training loss: 2116.47, base loss: 2376.55
[INFO 2017-06-26 19:31:32,957 main.py:47] epoch 3592, training loss: 2030.06, average training loss: 2116.19, base loss: 2376.21
[INFO 2017-06-26 19:31:33,248 main.py:47] epoch 3593, training loss: 2262.45, average training loss: 2116.32, base loss: 2376.58
[INFO 2017-06-26 19:31:33,548 main.py:47] epoch 3594, training loss: 1889.75, average training loss: 2115.91, base loss: 2376.28
[INFO 2017-06-26 19:31:33,838 main.py:47] epoch 3595, training loss: 1688.99, average training loss: 2115.43, base loss: 2376.05
[INFO 2017-06-26 19:31:34,123 main.py:47] epoch 3596, training loss: 2037.51, average training loss: 2115.37, base loss: 2375.85
[INFO 2017-06-26 19:31:34,423 main.py:47] epoch 3597, training loss: 2313.75, average training loss: 2115.50, base loss: 2376.02
[INFO 2017-06-26 19:31:34,710 main.py:47] epoch 3598, training loss: 2031.01, average training loss: 2115.35, base loss: 2375.78
[INFO 2017-06-26 19:31:34,998 main.py:47] epoch 3599, training loss: 1820.87, average training loss: 2114.90, base loss: 2375.16
[INFO 2017-06-26 19:31:34,999 main.py:49] epoch 3599, testing
[INFO 2017-06-26 19:31:38,670 main.py:100] average testing loss: 2162.24, base loss: 2445.63
[INFO 2017-06-26 19:31:38,696 main.py:73] current best accuracy: 1992.18
[INFO 2017-06-26 19:31:38,980 main.py:47] epoch 3600, training loss: 2626.00, average training loss: 2115.50, base loss: 2375.82
[INFO 2017-06-26 19:31:39,265 main.py:47] epoch 3601, training loss: 1976.13, average training loss: 2115.25, base loss: 2375.45
[INFO 2017-06-26 19:31:39,550 main.py:47] epoch 3602, training loss: 2111.77, average training loss: 2115.18, base loss: 2375.38
[INFO 2017-06-26 19:31:39,834 main.py:47] epoch 3603, training loss: 2045.48, average training loss: 2115.01, base loss: 2375.17
[INFO 2017-06-26 19:31:40,119 main.py:47] epoch 3604, training loss: 1869.86, average training loss: 2114.67, base loss: 2374.65
[INFO 2017-06-26 19:31:40,408 main.py:47] epoch 3605, training loss: 1878.46, average training loss: 2114.41, base loss: 2374.42
[INFO 2017-06-26 19:31:40,689 main.py:47] epoch 3606, training loss: 2359.62, average training loss: 2114.87, base loss: 2375.09
[INFO 2017-06-26 19:31:40,978 main.py:47] epoch 3607, training loss: 2118.92, average training loss: 2114.92, base loss: 2375.31
[INFO 2017-06-26 19:31:41,261 main.py:47] epoch 3608, training loss: 1913.19, average training loss: 2114.63, base loss: 2374.95
[INFO 2017-06-26 19:31:41,550 main.py:47] epoch 3609, training loss: 2278.17, average training loss: 2114.59, base loss: 2374.95
[INFO 2017-06-26 19:31:41,834 main.py:47] epoch 3610, training loss: 2396.28, average training loss: 2115.34, base loss: 2375.99
[INFO 2017-06-26 19:31:42,119 main.py:47] epoch 3611, training loss: 1897.87, average training loss: 2115.09, base loss: 2375.69
[INFO 2017-06-26 19:31:42,402 main.py:47] epoch 3612, training loss: 2190.46, average training loss: 2115.05, base loss: 2375.82
[INFO 2017-06-26 19:31:42,688 main.py:47] epoch 3613, training loss: 1865.03, average training loss: 2114.45, base loss: 2375.22
[INFO 2017-06-26 19:31:42,973 main.py:47] epoch 3614, training loss: 2043.89, average training loss: 2114.60, base loss: 2375.40
[INFO 2017-06-26 19:31:43,328 main.py:47] epoch 3615, training loss: 2182.31, average training loss: 2114.49, base loss: 2375.46
[INFO 2017-06-26 19:31:43,624 main.py:47] epoch 3616, training loss: 1898.32, average training loss: 2114.26, base loss: 2375.16
[INFO 2017-06-26 19:31:43,910 main.py:47] epoch 3617, training loss: 1844.72, average training loss: 2114.20, base loss: 2375.06
[INFO 2017-06-26 19:31:44,266 main.py:47] epoch 3618, training loss: 2070.35, average training loss: 2113.97, base loss: 2374.84
[INFO 2017-06-26 19:31:44,561 main.py:47] epoch 3619, training loss: 2042.37, average training loss: 2113.48, base loss: 2374.30
[INFO 2017-06-26 19:31:44,857 main.py:47] epoch 3620, training loss: 1780.01, average training loss: 2113.32, base loss: 2374.16
[INFO 2017-06-26 19:31:45,155 main.py:47] epoch 3621, training loss: 2020.70, average training loss: 2113.38, base loss: 2374.28
[INFO 2017-06-26 19:31:45,494 main.py:47] epoch 3622, training loss: 2169.81, average training loss: 2113.32, base loss: 2374.24
[INFO 2017-06-26 19:31:45,815 main.py:47] epoch 3623, training loss: 1632.41, average training loss: 2113.16, base loss: 2374.06
[INFO 2017-06-26 19:31:46,099 main.py:47] epoch 3624, training loss: 1890.53, average training loss: 2112.96, base loss: 2373.71
[INFO 2017-06-26 19:31:46,385 main.py:47] epoch 3625, training loss: 1939.26, average training loss: 2112.61, base loss: 2373.34
[INFO 2017-06-26 19:31:46,713 main.py:47] epoch 3626, training loss: 1982.07, average training loss: 2112.32, base loss: 2373.15
[INFO 2017-06-26 19:31:47,020 main.py:47] epoch 3627, training loss: 2295.86, average training loss: 2112.83, base loss: 2373.75
[INFO 2017-06-26 19:31:47,312 main.py:47] epoch 3628, training loss: 2100.50, average training loss: 2112.80, base loss: 2373.70
[INFO 2017-06-26 19:31:47,650 main.py:47] epoch 3629, training loss: 2244.55, average training loss: 2112.76, base loss: 2373.95
[INFO 2017-06-26 19:31:47,973 main.py:47] epoch 3630, training loss: 2302.51, average training loss: 2112.76, base loss: 2373.98
[INFO 2017-06-26 19:31:48,261 main.py:47] epoch 3631, training loss: 1874.77, average training loss: 2112.64, base loss: 2373.93
[INFO 2017-06-26 19:31:48,610 main.py:47] epoch 3632, training loss: 2320.64, average training loss: 2112.28, base loss: 2373.52
[INFO 2017-06-26 19:31:48,917 main.py:47] epoch 3633, training loss: 2279.52, average training loss: 2112.29, base loss: 2373.62
[INFO 2017-06-26 19:31:49,203 main.py:47] epoch 3634, training loss: 2293.96, average training loss: 2111.80, base loss: 2373.24
[INFO 2017-06-26 19:31:49,553 main.py:47] epoch 3635, training loss: 2332.74, average training loss: 2112.17, base loss: 2373.62
[INFO 2017-06-26 19:31:49,875 main.py:47] epoch 3636, training loss: 2162.50, average training loss: 2112.57, base loss: 2374.18
[INFO 2017-06-26 19:31:50,254 main.py:47] epoch 3637, training loss: 2020.63, average training loss: 2112.64, base loss: 2374.44
[INFO 2017-06-26 19:31:50,550 main.py:47] epoch 3638, training loss: 1939.79, average training loss: 2112.57, base loss: 2374.40
[INFO 2017-06-26 19:31:50,848 main.py:47] epoch 3639, training loss: 2080.84, average training loss: 2112.63, base loss: 2374.31
[INFO 2017-06-26 19:31:51,186 main.py:47] epoch 3640, training loss: 2274.18, average training loss: 2112.98, base loss: 2374.71
[INFO 2017-06-26 19:31:51,531 main.py:47] epoch 3641, training loss: 1856.89, average training loss: 2112.61, base loss: 2374.46
[INFO 2017-06-26 19:31:51,855 main.py:47] epoch 3642, training loss: 2039.03, average training loss: 2112.81, base loss: 2374.66
[INFO 2017-06-26 19:31:52,248 main.py:47] epoch 3643, training loss: 2436.80, average training loss: 2113.00, base loss: 2374.77
[INFO 2017-06-26 19:31:52,534 main.py:47] epoch 3644, training loss: 1679.98, average training loss: 2112.10, base loss: 2373.68
[INFO 2017-06-26 19:31:52,872 main.py:47] epoch 3645, training loss: 1652.71, average training loss: 2111.41, base loss: 2372.86
[INFO 2017-06-26 19:31:53,186 main.py:47] epoch 3646, training loss: 1908.89, average training loss: 2111.22, base loss: 2372.70
[INFO 2017-06-26 19:31:53,487 main.py:47] epoch 3647, training loss: 2297.73, average training loss: 2111.29, base loss: 2373.02
[INFO 2017-06-26 19:31:53,807 main.py:47] epoch 3648, training loss: 2035.18, average training loss: 2111.39, base loss: 2373.15
[INFO 2017-06-26 19:31:54,098 main.py:47] epoch 3649, training loss: 2470.06, average training loss: 2111.44, base loss: 2373.26
[INFO 2017-06-26 19:31:54,439 main.py:47] epoch 3650, training loss: 2128.23, average training loss: 2111.11, base loss: 2373.00
[INFO 2017-06-26 19:31:54,747 main.py:47] epoch 3651, training loss: 1700.36, average training loss: 2110.76, base loss: 2372.58
[INFO 2017-06-26 19:31:55,038 main.py:47] epoch 3652, training loss: 2603.54, average training loss: 2110.64, base loss: 2372.63
[INFO 2017-06-26 19:31:55,329 main.py:47] epoch 3653, training loss: 2064.86, average training loss: 2109.99, base loss: 2371.82
[INFO 2017-06-26 19:31:55,621 main.py:47] epoch 3654, training loss: 1990.07, average training loss: 2110.33, base loss: 2372.18
[INFO 2017-06-26 19:31:55,909 main.py:47] epoch 3655, training loss: 2198.63, average training loss: 2110.57, base loss: 2372.52
[INFO 2017-06-26 19:31:56,198 main.py:47] epoch 3656, training loss: 2269.61, average training loss: 2110.81, base loss: 2372.82
[INFO 2017-06-26 19:31:56,486 main.py:47] epoch 3657, training loss: 1779.97, average training loss: 2110.43, base loss: 2372.54
[INFO 2017-06-26 19:31:56,771 main.py:47] epoch 3658, training loss: 2436.38, average training loss: 2111.16, base loss: 2373.38
[INFO 2017-06-26 19:31:57,059 main.py:47] epoch 3659, training loss: 2115.70, average training loss: 2110.97, base loss: 2373.02
[INFO 2017-06-26 19:31:57,431 main.py:47] epoch 3660, training loss: 2171.82, average training loss: 2110.98, base loss: 2373.18
[INFO 2017-06-26 19:31:57,728 main.py:47] epoch 3661, training loss: 1981.76, average training loss: 2110.83, base loss: 2372.92
[INFO 2017-06-26 19:31:58,024 main.py:47] epoch 3662, training loss: 1883.56, average training loss: 2110.68, base loss: 2372.78
[INFO 2017-06-26 19:31:58,310 main.py:47] epoch 3663, training loss: 2388.96, average training loss: 2110.91, base loss: 2372.98
[INFO 2017-06-26 19:31:58,664 main.py:47] epoch 3664, training loss: 2197.87, average training loss: 2110.69, base loss: 2372.75
[INFO 2017-06-26 19:31:58,967 main.py:47] epoch 3665, training loss: 2167.80, average training loss: 2111.05, base loss: 2373.24
[INFO 2017-06-26 19:31:59,252 main.py:47] epoch 3666, training loss: 2232.34, average training loss: 2110.80, base loss: 2373.00
[INFO 2017-06-26 19:31:59,595 main.py:47] epoch 3667, training loss: 1942.17, average training loss: 2110.75, base loss: 2373.03
[INFO 2017-06-26 19:31:59,940 main.py:47] epoch 3668, training loss: 2124.76, average training loss: 2110.51, base loss: 2372.68
[INFO 2017-06-26 19:32:00,230 main.py:47] epoch 3669, training loss: 2063.28, average training loss: 2110.64, base loss: 2372.78
[INFO 2017-06-26 19:32:00,517 main.py:47] epoch 3670, training loss: 2350.86, average training loss: 2110.75, base loss: 2372.86
[INFO 2017-06-26 19:32:00,807 main.py:47] epoch 3671, training loss: 2030.95, average training loss: 2110.63, base loss: 2372.89
[INFO 2017-06-26 19:32:01,151 main.py:47] epoch 3672, training loss: 1966.26, average training loss: 2110.51, base loss: 2372.73
[INFO 2017-06-26 19:32:01,460 main.py:47] epoch 3673, training loss: 2123.60, average training loss: 2110.55, base loss: 2372.57
[INFO 2017-06-26 19:32:01,751 main.py:47] epoch 3674, training loss: 2333.38, average training loss: 2110.88, base loss: 2373.07
[INFO 2017-06-26 19:32:02,036 main.py:47] epoch 3675, training loss: 2187.75, average training loss: 2110.97, base loss: 2373.14
[INFO 2017-06-26 19:32:02,320 main.py:47] epoch 3676, training loss: 2241.28, average training loss: 2111.09, base loss: 2373.27
[INFO 2017-06-26 19:32:02,606 main.py:47] epoch 3677, training loss: 2063.98, average training loss: 2111.05, base loss: 2373.13
[INFO 2017-06-26 19:32:02,894 main.py:47] epoch 3678, training loss: 2092.22, average training loss: 2110.82, base loss: 2372.90
[INFO 2017-06-26 19:32:03,182 main.py:47] epoch 3679, training loss: 1996.68, average training loss: 2110.58, base loss: 2372.65
[INFO 2017-06-26 19:32:03,470 main.py:47] epoch 3680, training loss: 1888.58, average training loss: 2110.68, base loss: 2372.66
[INFO 2017-06-26 19:32:03,759 main.py:47] epoch 3681, training loss: 2103.49, average training loss: 2110.50, base loss: 2372.51
[INFO 2017-06-26 19:32:04,046 main.py:47] epoch 3682, training loss: 2609.65, average training loss: 2111.21, base loss: 2373.20
[INFO 2017-06-26 19:32:04,330 main.py:47] epoch 3683, training loss: 2188.52, average training loss: 2111.17, base loss: 2373.27
[INFO 2017-06-26 19:32:04,618 main.py:47] epoch 3684, training loss: 2391.82, average training loss: 2111.28, base loss: 2373.61
[INFO 2017-06-26 19:32:04,942 main.py:47] epoch 3685, training loss: 2030.52, average training loss: 2110.91, base loss: 2373.09
[INFO 2017-06-26 19:32:05,253 main.py:47] epoch 3686, training loss: 1988.70, average training loss: 2111.08, base loss: 2373.23
[INFO 2017-06-26 19:32:05,539 main.py:47] epoch 3687, training loss: 2499.28, average training loss: 2111.17, base loss: 2373.34
[INFO 2017-06-26 19:32:05,836 main.py:47] epoch 3688, training loss: 2052.68, average training loss: 2111.20, base loss: 2373.59
[INFO 2017-06-26 19:32:06,125 main.py:47] epoch 3689, training loss: 2257.62, average training loss: 2111.40, base loss: 2373.94
[INFO 2017-06-26 19:32:06,408 main.py:47] epoch 3690, training loss: 2049.77, average training loss: 2111.23, base loss: 2373.82
[INFO 2017-06-26 19:32:06,694 main.py:47] epoch 3691, training loss: 1737.77, average training loss: 2110.98, base loss: 2373.50
[INFO 2017-06-26 19:32:07,065 main.py:47] epoch 3692, training loss: 2510.96, average training loss: 2111.21, base loss: 2373.98
[INFO 2017-06-26 19:32:07,364 main.py:47] epoch 3693, training loss: 2189.34, average training loss: 2111.47, base loss: 2374.09
[INFO 2017-06-26 19:32:07,655 main.py:47] epoch 3694, training loss: 2303.18, average training loss: 2111.90, base loss: 2374.69
[INFO 2017-06-26 19:32:08,022 main.py:47] epoch 3695, training loss: 2123.58, average training loss: 2111.75, base loss: 2374.43
[INFO 2017-06-26 19:32:08,340 main.py:47] epoch 3696, training loss: 2001.87, average training loss: 2111.98, base loss: 2374.66
[INFO 2017-06-26 19:32:08,626 main.py:47] epoch 3697, training loss: 2013.57, average training loss: 2112.23, base loss: 2374.97
[INFO 2017-06-26 19:32:09,004 main.py:47] epoch 3698, training loss: 2555.72, average training loss: 2112.55, base loss: 2375.38
[INFO 2017-06-26 19:32:09,334 main.py:47] epoch 3699, training loss: 2021.06, average training loss: 2112.32, base loss: 2375.22
[INFO 2017-06-26 19:32:09,335 main.py:49] epoch 3699, testing
[INFO 2017-06-26 19:32:13,675 main.py:100] average testing loss: 2130.95, base loss: 2343.55
[INFO 2017-06-26 19:32:13,699 main.py:73] current best accuracy: 1992.18
[INFO 2017-06-26 19:32:13,986 main.py:47] epoch 3700, training loss: 2003.15, average training loss: 2112.13, base loss: 2374.99
[INFO 2017-06-26 19:32:14,276 main.py:47] epoch 3701, training loss: 1838.37, average training loss: 2111.50, base loss: 2374.33
[INFO 2017-06-26 19:32:14,560 main.py:47] epoch 3702, training loss: 2280.12, average training loss: 2111.76, base loss: 2374.74
[INFO 2017-06-26 19:32:14,843 main.py:47] epoch 3703, training loss: 1843.84, average training loss: 2111.80, base loss: 2374.90
[INFO 2017-06-26 19:32:15,126 main.py:47] epoch 3704, training loss: 2006.97, average training loss: 2112.10, base loss: 2375.35
[INFO 2017-06-26 19:32:15,416 main.py:47] epoch 3705, training loss: 1934.80, average training loss: 2111.78, base loss: 2375.05
[INFO 2017-06-26 19:32:15,701 main.py:47] epoch 3706, training loss: 2306.68, average training loss: 2112.17, base loss: 2375.63
[INFO 2017-06-26 19:32:15,984 main.py:47] epoch 3707, training loss: 2018.15, average training loss: 2112.48, base loss: 2376.09
[INFO 2017-06-26 19:32:16,268 main.py:47] epoch 3708, training loss: 2141.38, average training loss: 2112.52, base loss: 2376.30
[INFO 2017-06-26 19:32:16,593 main.py:47] epoch 3709, training loss: 2383.81, average training loss: 2112.52, base loss: 2376.35
[INFO 2017-06-26 19:32:16,917 main.py:47] epoch 3710, training loss: 2201.02, average training loss: 2112.81, base loss: 2376.74
[INFO 2017-06-26 19:32:17,201 main.py:47] epoch 3711, training loss: 2434.05, average training loss: 2112.98, base loss: 2376.94
[INFO 2017-06-26 19:32:17,494 main.py:47] epoch 3712, training loss: 2402.22, average training loss: 2113.21, base loss: 2377.33
[INFO 2017-06-26 19:32:17,786 main.py:47] epoch 3713, training loss: 2034.73, average training loss: 2113.36, base loss: 2377.56
[INFO 2017-06-26 19:32:18,159 main.py:47] epoch 3714, training loss: 2083.80, average training loss: 2113.28, base loss: 2377.39
[INFO 2017-06-26 19:32:18,455 main.py:47] epoch 3715, training loss: 1995.97, average training loss: 2113.28, base loss: 2377.53
[INFO 2017-06-26 19:32:18,785 main.py:47] epoch 3716, training loss: 2160.93, average training loss: 2113.30, base loss: 2377.49
[INFO 2017-06-26 19:32:19,099 main.py:47] epoch 3717, training loss: 2035.78, average training loss: 2113.30, base loss: 2377.38
[INFO 2017-06-26 19:32:19,393 main.py:47] epoch 3718, training loss: 1735.02, average training loss: 2113.17, base loss: 2377.08
[INFO 2017-06-26 19:32:19,717 main.py:47] epoch 3719, training loss: 2116.83, average training loss: 2113.24, base loss: 2377.30
[INFO 2017-06-26 19:32:20,029 main.py:47] epoch 3720, training loss: 1871.66, average training loss: 2112.68, base loss: 2376.71
[INFO 2017-06-26 19:32:20,315 main.py:47] epoch 3721, training loss: 1978.58, average training loss: 2112.86, base loss: 2376.93
[INFO 2017-06-26 19:32:20,612 main.py:47] epoch 3722, training loss: 2422.90, average training loss: 2112.64, base loss: 2376.67
[INFO 2017-06-26 19:32:20,899 main.py:47] epoch 3723, training loss: 2273.33, average training loss: 2112.72, base loss: 2376.74
[INFO 2017-06-26 19:32:21,189 main.py:47] epoch 3724, training loss: 2046.87, average training loss: 2112.63, base loss: 2376.63
[INFO 2017-06-26 19:32:21,474 main.py:47] epoch 3725, training loss: 1905.12, average training loss: 2112.27, base loss: 2376.29
[INFO 2017-06-26 19:32:21,816 main.py:47] epoch 3726, training loss: 1934.07, average training loss: 2112.61, base loss: 2376.87
[INFO 2017-06-26 19:32:22,120 main.py:47] epoch 3727, training loss: 2064.07, average training loss: 2112.64, base loss: 2377.01
[INFO 2017-06-26 19:32:22,448 main.py:47] epoch 3728, training loss: 2448.46, average training loss: 2112.60, base loss: 2377.18
[INFO 2017-06-26 19:32:22,778 main.py:47] epoch 3729, training loss: 2138.78, average training loss: 2112.44, base loss: 2377.10
[INFO 2017-06-26 19:32:23,066 main.py:47] epoch 3730, training loss: 2335.80, average training loss: 2112.54, base loss: 2377.06
[INFO 2017-06-26 19:32:23,414 main.py:47] epoch 3731, training loss: 2005.94, average training loss: 2112.27, base loss: 2376.72
[INFO 2017-06-26 19:32:23,739 main.py:47] epoch 3732, training loss: 1986.06, average training loss: 2112.19, base loss: 2376.77
[INFO 2017-06-26 19:32:24,067 main.py:47] epoch 3733, training loss: 2032.61, average training loss: 2111.83, base loss: 2376.14
[INFO 2017-06-26 19:32:24,398 main.py:47] epoch 3734, training loss: 1877.53, average training loss: 2111.80, base loss: 2376.35
[INFO 2017-06-26 19:32:24,685 main.py:47] epoch 3735, training loss: 2080.03, average training loss: 2111.97, base loss: 2376.60
[INFO 2017-06-26 19:32:25,032 main.py:47] epoch 3736, training loss: 2267.53, average training loss: 2112.20, base loss: 2376.94
[INFO 2017-06-26 19:32:25,341 main.py:47] epoch 3737, training loss: 2122.51, average training loss: 2112.53, base loss: 2377.32
[INFO 2017-06-26 19:32:25,631 main.py:47] epoch 3738, training loss: 2010.37, average training loss: 2111.79, base loss: 2376.29
[INFO 2017-06-26 19:32:26,001 main.py:47] epoch 3739, training loss: 2075.72, average training loss: 2112.11, base loss: 2376.86
[INFO 2017-06-26 19:32:26,293 main.py:47] epoch 3740, training loss: 1897.70, average training loss: 2112.45, base loss: 2377.33
[INFO 2017-06-26 19:32:26,592 main.py:47] epoch 3741, training loss: 1762.09, average training loss: 2112.11, base loss: 2377.01
[INFO 2017-06-26 19:32:26,884 main.py:47] epoch 3742, training loss: 2112.80, average training loss: 2112.07, base loss: 2376.94
[INFO 2017-06-26 19:32:27,169 main.py:47] epoch 3743, training loss: 2505.18, average training loss: 2112.29, base loss: 2377.11
[INFO 2017-06-26 19:32:27,452 main.py:47] epoch 3744, training loss: 2177.56, average training loss: 2112.45, base loss: 2377.54
[INFO 2017-06-26 19:32:27,747 main.py:47] epoch 3745, training loss: 1914.43, average training loss: 2112.28, base loss: 2377.53
[INFO 2017-06-26 19:32:28,035 main.py:47] epoch 3746, training loss: 1975.30, average training loss: 2111.74, base loss: 2377.02
[INFO 2017-06-26 19:32:28,326 main.py:47] epoch 3747, training loss: 2288.64, average training loss: 2111.79, base loss: 2377.24
[INFO 2017-06-26 19:32:28,609 main.py:47] epoch 3748, training loss: 2509.36, average training loss: 2112.10, base loss: 2377.64
[INFO 2017-06-26 19:32:28,894 main.py:47] epoch 3749, training loss: 2293.91, average training loss: 2112.28, base loss: 2377.93
[INFO 2017-06-26 19:32:29,178 main.py:47] epoch 3750, training loss: 2403.50, average training loss: 2112.87, base loss: 2378.90
[INFO 2017-06-26 19:32:29,467 main.py:47] epoch 3751, training loss: 2131.53, average training loss: 2112.97, base loss: 2379.07
[INFO 2017-06-26 19:32:29,753 main.py:47] epoch 3752, training loss: 2276.83, average training loss: 2113.48, base loss: 2379.67
[INFO 2017-06-26 19:32:30,036 main.py:47] epoch 3753, training loss: 2205.74, average training loss: 2113.09, base loss: 2379.20
[INFO 2017-06-26 19:32:30,325 main.py:47] epoch 3754, training loss: 1769.84, average training loss: 2112.99, base loss: 2378.83
[INFO 2017-06-26 19:32:30,605 main.py:47] epoch 3755, training loss: 2379.54, average training loss: 2112.79, base loss: 2378.68
[INFO 2017-06-26 19:32:30,894 main.py:47] epoch 3756, training loss: 2316.46, average training loss: 2112.92, base loss: 2379.14
[INFO 2017-06-26 19:32:31,176 main.py:47] epoch 3757, training loss: 1961.83, average training loss: 2112.85, base loss: 2378.97
[INFO 2017-06-26 19:32:31,460 main.py:47] epoch 3758, training loss: 1716.25, average training loss: 2111.92, base loss: 2377.82
[INFO 2017-06-26 19:32:31,744 main.py:47] epoch 3759, training loss: 2124.18, average training loss: 2111.84, base loss: 2377.81
[INFO 2017-06-26 19:32:32,027 main.py:47] epoch 3760, training loss: 2275.20, average training loss: 2112.17, base loss: 2378.22
[INFO 2017-06-26 19:32:32,311 main.py:47] epoch 3761, training loss: 2677.04, average training loss: 2112.72, base loss: 2378.98
[INFO 2017-06-26 19:32:32,595 main.py:47] epoch 3762, training loss: 2386.08, average training loss: 2112.97, base loss: 2379.32
[INFO 2017-06-26 19:32:32,882 main.py:47] epoch 3763, training loss: 2128.67, average training loss: 2112.64, base loss: 2378.95
[INFO 2017-06-26 19:32:33,168 main.py:47] epoch 3764, training loss: 1862.95, average training loss: 2112.57, base loss: 2378.93
[INFO 2017-06-26 19:32:33,456 main.py:47] epoch 3765, training loss: 2115.33, average training loss: 2112.19, base loss: 2378.50
[INFO 2017-06-26 19:32:33,738 main.py:47] epoch 3766, training loss: 2418.46, average training loss: 2112.13, base loss: 2378.27
[INFO 2017-06-26 19:32:34,023 main.py:47] epoch 3767, training loss: 1894.93, average training loss: 2111.75, base loss: 2377.76
[INFO 2017-06-26 19:32:34,305 main.py:47] epoch 3768, training loss: 2249.47, average training loss: 2112.00, base loss: 2378.15
[INFO 2017-06-26 19:32:34,586 main.py:47] epoch 3769, training loss: 2496.73, average training loss: 2112.36, base loss: 2378.82
[INFO 2017-06-26 19:32:34,875 main.py:47] epoch 3770, training loss: 1978.83, average training loss: 2112.18, base loss: 2378.63
[INFO 2017-06-26 19:32:35,159 main.py:47] epoch 3771, training loss: 2030.09, average training loss: 2111.85, base loss: 2378.44
[INFO 2017-06-26 19:32:35,447 main.py:47] epoch 3772, training loss: 2183.62, average training loss: 2111.52, base loss: 2378.19
[INFO 2017-06-26 19:32:35,728 main.py:47] epoch 3773, training loss: 1916.06, average training loss: 2111.47, base loss: 2378.10
[INFO 2017-06-26 19:32:36,013 main.py:47] epoch 3774, training loss: 2085.40, average training loss: 2111.59, base loss: 2378.16
[INFO 2017-06-26 19:32:36,294 main.py:47] epoch 3775, training loss: 1778.27, average training loss: 2111.22, base loss: 2377.69
[INFO 2017-06-26 19:32:36,578 main.py:47] epoch 3776, training loss: 2327.25, average training loss: 2111.66, base loss: 2378.24
[INFO 2017-06-26 19:32:36,862 main.py:47] epoch 3777, training loss: 2096.12, average training loss: 2111.45, base loss: 2378.15
[INFO 2017-06-26 19:32:37,148 main.py:47] epoch 3778, training loss: 1963.02, average training loss: 2111.38, base loss: 2378.07
[INFO 2017-06-26 19:32:37,429 main.py:47] epoch 3779, training loss: 2196.28, average training loss: 2111.40, base loss: 2378.07
[INFO 2017-06-26 19:32:37,710 main.py:47] epoch 3780, training loss: 1843.82, average training loss: 2111.18, base loss: 2377.81
[INFO 2017-06-26 19:32:37,990 main.py:47] epoch 3781, training loss: 2290.07, average training loss: 2110.85, base loss: 2377.59
[INFO 2017-06-26 19:32:38,271 main.py:47] epoch 3782, training loss: 2217.16, average training loss: 2111.15, base loss: 2377.85
[INFO 2017-06-26 19:32:38,553 main.py:47] epoch 3783, training loss: 1985.38, average training loss: 2110.95, base loss: 2377.62
[INFO 2017-06-26 19:32:38,843 main.py:47] epoch 3784, training loss: 2373.88, average training loss: 2111.14, base loss: 2377.90
[INFO 2017-06-26 19:32:39,131 main.py:47] epoch 3785, training loss: 1983.03, average training loss: 2110.92, base loss: 2377.57
[INFO 2017-06-26 19:32:39,415 main.py:47] epoch 3786, training loss: 2270.66, average training loss: 2110.58, base loss: 2377.33
[INFO 2017-06-26 19:32:39,705 main.py:47] epoch 3787, training loss: 1993.52, average training loss: 2110.45, base loss: 2377.14
[INFO 2017-06-26 19:32:39,989 main.py:47] epoch 3788, training loss: 1892.53, average training loss: 2110.34, base loss: 2377.20
[INFO 2017-06-26 19:32:40,276 main.py:47] epoch 3789, training loss: 2200.66, average training loss: 2110.48, base loss: 2377.59
[INFO 2017-06-26 19:32:40,562 main.py:47] epoch 3790, training loss: 2100.83, average training loss: 2110.51, base loss: 2377.86
[INFO 2017-06-26 19:32:40,849 main.py:47] epoch 3791, training loss: 2214.96, average training loss: 2110.61, base loss: 2377.85
[INFO 2017-06-26 19:32:41,135 main.py:47] epoch 3792, training loss: 1975.97, average training loss: 2110.23, base loss: 2377.31
[INFO 2017-06-26 19:32:41,420 main.py:47] epoch 3793, training loss: 2219.96, average training loss: 2110.42, base loss: 2377.54
[INFO 2017-06-26 19:32:41,701 main.py:47] epoch 3794, training loss: 1955.88, average training loss: 2110.54, base loss: 2377.77
[INFO 2017-06-26 19:32:41,981 main.py:47] epoch 3795, training loss: 2203.86, average training loss: 2110.61, base loss: 2378.05
[INFO 2017-06-26 19:32:42,269 main.py:47] epoch 3796, training loss: 2199.99, average training loss: 2110.41, base loss: 2378.09
[INFO 2017-06-26 19:32:42,556 main.py:47] epoch 3797, training loss: 2294.53, average training loss: 2110.55, base loss: 2378.50
[INFO 2017-06-26 19:32:42,841 main.py:47] epoch 3798, training loss: 1981.02, average training loss: 2110.49, base loss: 2378.47
[INFO 2017-06-26 19:32:43,124 main.py:47] epoch 3799, training loss: 2038.65, average training loss: 2110.26, base loss: 2378.11
[INFO 2017-06-26 19:32:43,124 main.py:49] epoch 3799, testing
[INFO 2017-06-26 19:32:46,863 main.py:100] average testing loss: 2142.85, base loss: 2415.90
[INFO 2017-06-26 19:32:46,888 main.py:73] current best accuracy: 1992.18
[INFO 2017-06-26 19:32:47,169 main.py:47] epoch 3800, training loss: 1951.07, average training loss: 2109.91, base loss: 2377.47
[INFO 2017-06-26 19:32:47,451 main.py:47] epoch 3801, training loss: 2573.56, average training loss: 2110.27, base loss: 2377.87
[INFO 2017-06-26 19:32:47,736 main.py:47] epoch 3802, training loss: 2066.82, average training loss: 2110.46, base loss: 2378.13
[INFO 2017-06-26 19:32:48,020 main.py:47] epoch 3803, training loss: 1850.30, average training loss: 2110.39, base loss: 2377.83
[INFO 2017-06-26 19:32:48,307 main.py:47] epoch 3804, training loss: 1903.58, average training loss: 2110.40, base loss: 2377.81
[INFO 2017-06-26 19:32:48,595 main.py:47] epoch 3805, training loss: 1839.48, average training loss: 2110.28, base loss: 2377.81
[INFO 2017-06-26 19:32:48,884 main.py:47] epoch 3806, training loss: 1724.54, average training loss: 2109.99, base loss: 2377.45
[INFO 2017-06-26 19:32:49,170 main.py:47] epoch 3807, training loss: 2207.71, average training loss: 2110.32, base loss: 2377.80
[INFO 2017-06-26 19:32:49,461 main.py:47] epoch 3808, training loss: 2455.47, average training loss: 2110.15, base loss: 2377.52
[INFO 2017-06-26 19:32:49,745 main.py:47] epoch 3809, training loss: 2002.99, average training loss: 2109.98, base loss: 2377.53
[INFO 2017-06-26 19:32:50,028 main.py:47] epoch 3810, training loss: 1954.40, average training loss: 2109.80, base loss: 2377.37
[INFO 2017-06-26 19:32:50,314 main.py:47] epoch 3811, training loss: 1812.70, average training loss: 2109.57, base loss: 2377.16
[INFO 2017-06-26 19:32:50,597 main.py:47] epoch 3812, training loss: 2167.34, average training loss: 2109.54, base loss: 2377.03
[INFO 2017-06-26 19:32:50,884 main.py:47] epoch 3813, training loss: 2210.33, average training loss: 2110.09, base loss: 2377.70
[INFO 2017-06-26 19:32:51,165 main.py:47] epoch 3814, training loss: 2323.95, average training loss: 2110.36, base loss: 2378.01
[INFO 2017-06-26 19:32:51,448 main.py:47] epoch 3815, training loss: 2114.21, average training loss: 2110.38, base loss: 2378.05
[INFO 2017-06-26 19:32:51,729 main.py:47] epoch 3816, training loss: 1800.35, average training loss: 2110.09, base loss: 2377.68
[INFO 2017-06-26 19:32:52,011 main.py:47] epoch 3817, training loss: 2141.85, average training loss: 2109.74, base loss: 2377.58
[INFO 2017-06-26 19:32:52,293 main.py:47] epoch 3818, training loss: 2112.17, average training loss: 2109.94, base loss: 2377.70
[INFO 2017-06-26 19:32:52,578 main.py:47] epoch 3819, training loss: 2191.12, average training loss: 2110.05, base loss: 2377.85
[INFO 2017-06-26 19:32:52,861 main.py:47] epoch 3820, training loss: 2471.38, average training loss: 2110.67, base loss: 2378.62
[INFO 2017-06-26 19:32:53,141 main.py:47] epoch 3821, training loss: 2071.26, average training loss: 2110.53, base loss: 2378.56
[INFO 2017-06-26 19:32:53,425 main.py:47] epoch 3822, training loss: 1936.39, average training loss: 2110.43, base loss: 2378.45
[INFO 2017-06-26 19:32:53,705 main.py:47] epoch 3823, training loss: 2233.83, average training loss: 2110.49, base loss: 2378.38
[INFO 2017-06-26 19:32:53,990 main.py:47] epoch 3824, training loss: 1739.02, average training loss: 2109.77, base loss: 2377.40
[INFO 2017-06-26 19:32:54,274 main.py:47] epoch 3825, training loss: 2072.79, average training loss: 2109.86, base loss: 2377.75
[INFO 2017-06-26 19:32:54,557 main.py:47] epoch 3826, training loss: 2206.41, average training loss: 2110.04, base loss: 2377.88
[INFO 2017-06-26 19:32:54,844 main.py:47] epoch 3827, training loss: 2434.95, average training loss: 2110.19, base loss: 2378.13
[INFO 2017-06-26 19:32:55,127 main.py:47] epoch 3828, training loss: 1914.28, average training loss: 2110.30, base loss: 2378.36
[INFO 2017-06-26 19:32:55,410 main.py:47] epoch 3829, training loss: 2188.50, average training loss: 2110.56, base loss: 2378.75
[INFO 2017-06-26 19:32:55,695 main.py:47] epoch 3830, training loss: 2092.08, average training loss: 2110.50, base loss: 2378.64
[INFO 2017-06-26 19:32:55,978 main.py:47] epoch 3831, training loss: 2270.71, average training loss: 2110.77, base loss: 2379.08
[INFO 2017-06-26 19:32:56,263 main.py:47] epoch 3832, training loss: 2355.21, average training loss: 2110.78, base loss: 2379.01
[INFO 2017-06-26 19:32:56,545 main.py:47] epoch 3833, training loss: 2514.97, average training loss: 2111.32, base loss: 2379.51
[INFO 2017-06-26 19:32:56,833 main.py:47] epoch 3834, training loss: 1993.91, average training loss: 2111.25, base loss: 2379.52
[INFO 2017-06-26 19:32:57,117 main.py:47] epoch 3835, training loss: 2200.25, average training loss: 2111.64, base loss: 2380.04
[INFO 2017-06-26 19:32:57,399 main.py:47] epoch 3836, training loss: 1969.45, average training loss: 2111.09, base loss: 2379.39
[INFO 2017-06-26 19:32:57,685 main.py:47] epoch 3837, training loss: 1849.77, average training loss: 2110.75, base loss: 2379.01
[INFO 2017-06-26 19:32:57,969 main.py:47] epoch 3838, training loss: 2021.16, average training loss: 2110.58, base loss: 2378.82
[INFO 2017-06-26 19:32:58,251 main.py:47] epoch 3839, training loss: 2102.35, average training loss: 2110.66, base loss: 2379.11
[INFO 2017-06-26 19:32:58,534 main.py:47] epoch 3840, training loss: 2021.50, average training loss: 2110.94, base loss: 2379.26
[INFO 2017-06-26 19:32:58,815 main.py:47] epoch 3841, training loss: 2186.01, average training loss: 2110.84, base loss: 2379.34
[INFO 2017-06-26 19:32:59,101 main.py:47] epoch 3842, training loss: 1999.27, average training loss: 2110.33, base loss: 2378.79
[INFO 2017-06-26 19:32:59,383 main.py:47] epoch 3843, training loss: 2273.44, average training loss: 2110.74, base loss: 2379.39
[INFO 2017-06-26 19:32:59,665 main.py:47] epoch 3844, training loss: 2050.19, average training loss: 2110.88, base loss: 2379.70
[INFO 2017-06-26 19:32:59,949 main.py:47] epoch 3845, training loss: 1824.39, average training loss: 2110.64, base loss: 2379.37
[INFO 2017-06-26 19:33:00,229 main.py:47] epoch 3846, training loss: 2263.19, average training loss: 2110.92, base loss: 2379.76
[INFO 2017-06-26 19:33:00,516 main.py:47] epoch 3847, training loss: 1941.75, average training loss: 2110.56, base loss: 2379.39
[INFO 2017-06-26 19:33:00,800 main.py:47] epoch 3848, training loss: 1990.11, average training loss: 2110.61, base loss: 2379.18
[INFO 2017-06-26 19:33:01,084 main.py:47] epoch 3849, training loss: 2363.45, average training loss: 2111.23, base loss: 2379.93
