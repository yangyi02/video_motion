[INFO 2017-06-26 17:46:02,236 main.py:123] Namespace(batch_size=32, display=False, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/robot-64', test_epoch=10, test_interval=100, train=True, train_dir='/home/yi/Downloads/robot-64', train_epoch=10000)
[INFO 2017-06-26 17:46:07,038 main.py:47] epoch 0, training loss: 99985.69, average training loss: 99985.69, base loss: 7956.09
[INFO 2017-06-26 17:46:07,344 main.py:47] epoch 1, training loss: 79265.39, average training loss: 89625.54, base loss: 8010.55
[INFO 2017-06-26 17:46:07,642 main.py:47] epoch 2, training loss: 69299.55, average training loss: 82850.21, base loss: 8489.91
[INFO 2017-06-26 17:46:07,938 main.py:47] epoch 3, training loss: 61581.82, average training loss: 77533.11, base loss: 8614.11
[INFO 2017-06-26 17:46:08,238 main.py:47] epoch 4, training loss: 51981.16, average training loss: 72422.72, base loss: 8712.91
[INFO 2017-06-26 17:46:08,532 main.py:47] epoch 5, training loss: 46726.23, average training loss: 68139.97, base loss: 8673.42
[INFO 2017-06-26 17:46:08,827 main.py:47] epoch 6, training loss: 40994.56, average training loss: 64262.06, base loss: 8774.43
[INFO 2017-06-26 17:46:09,124 main.py:47] epoch 7, training loss: 34335.60, average training loss: 60521.25, base loss: 8640.83
[INFO 2017-06-26 17:46:09,420 main.py:47] epoch 8, training loss: 30272.59, average training loss: 57160.29, base loss: 8533.93
[INFO 2017-06-26 17:46:09,716 main.py:47] epoch 9, training loss: 26308.50, average training loss: 54075.11, base loss: 8563.80
[INFO 2017-06-26 17:46:10,008 main.py:47] epoch 10, training loss: 22308.71, average training loss: 51187.25, base loss: 8549.59
[INFO 2017-06-26 17:46:10,300 main.py:47] epoch 11, training loss: 20781.67, average training loss: 48653.46, base loss: 8626.44
[INFO 2017-06-26 17:46:10,598 main.py:47] epoch 12, training loss: 18662.83, average training loss: 46346.49, base loss: 8653.00
[INFO 2017-06-26 17:46:10,891 main.py:47] epoch 13, training loss: 15975.73, average training loss: 44177.15, base loss: 8608.18
[INFO 2017-06-26 17:46:11,182 main.py:47] epoch 14, training loss: 17020.86, average training loss: 42366.73, base loss: 8774.05
[INFO 2017-06-26 17:46:11,476 main.py:47] epoch 15, training loss: 12847.82, average training loss: 40521.79, base loss: 8721.80
[INFO 2017-06-26 17:46:11,779 main.py:47] epoch 16, training loss: 13555.93, average training loss: 38935.57, base loss: 8755.83
[INFO 2017-06-26 17:46:12,075 main.py:47] epoch 17, training loss: 12285.47, average training loss: 37455.01, base loss: 8778.48
[INFO 2017-06-26 17:46:12,368 main.py:47] epoch 18, training loss: 11705.83, average training loss: 36099.79, base loss: 8788.35
[INFO 2017-06-26 17:46:12,663 main.py:47] epoch 19, training loss: 12134.26, average training loss: 34901.51, base loss: 8854.28
[INFO 2017-06-26 17:46:12,953 main.py:47] epoch 20, training loss: 9703.98, average training loss: 33701.63, base loss: 8809.27
[INFO 2017-06-26 17:46:13,243 main.py:47] epoch 21, training loss: 11622.18, average training loss: 32698.02, base loss: 8891.03
[INFO 2017-06-26 17:46:13,531 main.py:47] epoch 22, training loss: 10787.50, average training loss: 31745.39, base loss: 8935.23
[INFO 2017-06-26 17:46:13,821 main.py:47] epoch 23, training loss: 9262.09, average training loss: 30808.58, base loss: 8918.28
[INFO 2017-06-26 17:46:14,114 main.py:47] epoch 24, training loss: 9129.57, average training loss: 29941.42, base loss: 8900.87
[INFO 2017-06-26 17:46:14,401 main.py:47] epoch 25, training loss: 8223.83, average training loss: 29106.13, base loss: 8853.57
[INFO 2017-06-26 17:46:14,692 main.py:47] epoch 26, training loss: 9568.90, average training loss: 28382.53, base loss: 8867.45
[INFO 2017-06-26 17:46:14,982 main.py:47] epoch 27, training loss: 10853.79, average training loss: 27756.50, base loss: 8932.45
[INFO 2017-06-26 17:46:15,271 main.py:47] epoch 28, training loss: 9156.66, average training loss: 27115.13, base loss: 8929.63
[INFO 2017-06-26 17:46:15,562 main.py:47] epoch 29, training loss: 9577.37, average training loss: 26530.54, base loss: 8945.42
[INFO 2017-06-26 17:46:15,854 main.py:47] epoch 30, training loss: 8062.62, average training loss: 25934.80, base loss: 8910.94
[INFO 2017-06-26 17:46:16,146 main.py:47] epoch 31, training loss: 7973.36, average training loss: 25373.50, base loss: 8876.27
[INFO 2017-06-26 17:46:16,437 main.py:47] epoch 32, training loss: 9288.87, average training loss: 24886.09, base loss: 8885.24
[INFO 2017-06-26 17:46:16,728 main.py:47] epoch 33, training loss: 7376.53, average training loss: 24371.10, base loss: 8836.87
[INFO 2017-06-26 17:46:17,033 main.py:47] epoch 34, training loss: 8901.60, average training loss: 23929.12, base loss: 8837.83
[INFO 2017-06-26 17:46:17,330 main.py:47] epoch 35, training loss: 9008.75, average training loss: 23514.66, base loss: 8844.23
[INFO 2017-06-26 17:46:17,625 main.py:47] epoch 36, training loss: 10076.56, average training loss: 23151.47, base loss: 8880.60
[INFO 2017-06-26 17:46:17,916 main.py:47] epoch 37, training loss: 7698.23, average training loss: 22744.81, base loss: 8849.92
[INFO 2017-06-26 17:46:18,208 main.py:47] epoch 38, training loss: 8012.23, average training loss: 22367.05, base loss: 8828.38
[INFO 2017-06-26 17:46:18,502 main.py:47] epoch 39, training loss: 8598.96, average training loss: 22022.84, base loss: 8824.70
[INFO 2017-06-26 17:46:18,792 main.py:47] epoch 40, training loss: 8750.84, average training loss: 21699.14, base loss: 8824.39
[INFO 2017-06-26 17:46:19,083 main.py:47] epoch 41, training loss: 9875.08, average training loss: 21417.61, base loss: 8851.10
[INFO 2017-06-26 17:46:19,373 main.py:47] epoch 42, training loss: 9567.88, average training loss: 21142.04, base loss: 8870.50
[INFO 2017-06-26 17:46:19,662 main.py:47] epoch 43, training loss: 8245.70, average training loss: 20848.94, base loss: 8856.47
[INFO 2017-06-26 17:46:19,953 main.py:47] epoch 44, training loss: 8410.66, average training loss: 20572.53, base loss: 8846.99
[INFO 2017-06-26 17:46:20,248 main.py:47] epoch 45, training loss: 9840.91, average training loss: 20339.24, base loss: 8872.47
[INFO 2017-06-26 17:46:20,536 main.py:47] epoch 46, training loss: 8368.66, average training loss: 20084.54, base loss: 8861.63
[INFO 2017-06-26 17:46:20,835 main.py:47] epoch 47, training loss: 7925.61, average training loss: 19831.23, base loss: 8843.49
[INFO 2017-06-26 17:46:21,122 main.py:47] epoch 48, training loss: 9164.33, average training loss: 19613.54, base loss: 8852.78
[INFO 2017-06-26 17:46:21,414 main.py:47] epoch 49, training loss: 7245.98, average training loss: 19366.19, base loss: 8821.46
[INFO 2017-06-26 17:46:21,704 main.py:47] epoch 50, training loss: 8337.47, average training loss: 19149.94, base loss: 8815.10
[INFO 2017-06-26 17:46:22,016 main.py:47] epoch 51, training loss: 9200.77, average training loss: 18958.61, base loss: 8826.37
[INFO 2017-06-26 17:46:22,318 main.py:47] epoch 52, training loss: 8077.56, average training loss: 18753.31, base loss: 8814.56
[INFO 2017-06-26 17:46:22,612 main.py:47] epoch 53, training loss: 8029.84, average training loss: 18554.72, base loss: 8801.11
[INFO 2017-06-26 17:46:22,905 main.py:47] epoch 54, training loss: 8455.18, average training loss: 18371.10, base loss: 8797.77
[INFO 2017-06-26 17:46:23,196 main.py:47] epoch 55, training loss: 7926.92, average training loss: 18184.59, base loss: 8785.06
[INFO 2017-06-26 17:46:23,485 main.py:47] epoch 56, training loss: 9472.46, average training loss: 18031.75, base loss: 8800.77
[INFO 2017-06-26 17:46:23,772 main.py:47] epoch 57, training loss: 7501.55, average training loss: 17850.19, base loss: 8781.07
[INFO 2017-06-26 17:46:24,065 main.py:47] epoch 58, training loss: 8069.21, average training loss: 17684.41, base loss: 8771.13
[INFO 2017-06-26 17:46:24,356 main.py:47] epoch 59, training loss: 8150.03, average training loss: 17525.51, base loss: 8763.44
[INFO 2017-06-26 17:46:24,645 main.py:47] epoch 60, training loss: 9303.53, average training loss: 17390.72, base loss: 8777.08
[INFO 2017-06-26 17:46:24,937 main.py:47] epoch 61, training loss: 9352.32, average training loss: 17261.07, base loss: 8790.48
[INFO 2017-06-26 17:46:25,229 main.py:47] epoch 62, training loss: 9588.81, average training loss: 17139.29, base loss: 8807.44
[INFO 2017-06-26 17:46:25,519 main.py:47] epoch 63, training loss: 8368.61, average training loss: 17002.25, base loss: 8804.55
[INFO 2017-06-26 17:46:25,809 main.py:47] epoch 64, training loss: 9957.35, average training loss: 16893.86, base loss: 8826.79
[INFO 2017-06-26 17:46:26,097 main.py:47] epoch 65, training loss: 8809.62, average training loss: 16771.37, base loss: 8831.93
[INFO 2017-06-26 17:46:26,388 main.py:47] epoch 66, training loss: 8591.59, average training loss: 16649.29, base loss: 8833.84
[INFO 2017-06-26 17:46:26,682 main.py:47] epoch 67, training loss: 7269.86, average training loss: 16511.35, base loss: 8813.46
[INFO 2017-06-26 17:46:26,975 main.py:47] epoch 68, training loss: 8535.80, average training loss: 16395.77, base loss: 8812.49
[INFO 2017-06-26 17:46:27,274 main.py:47] epoch 69, training loss: 6878.72, average training loss: 16259.81, base loss: 8788.10
[INFO 2017-06-26 17:46:27,566 main.py:47] epoch 70, training loss: 10384.35, average training loss: 16177.06, base loss: 8815.70
[INFO 2017-06-26 17:46:27,859 main.py:47] epoch 71, training loss: 8567.69, average training loss: 16071.37, base loss: 8814.89
[INFO 2017-06-26 17:46:28,154 main.py:47] epoch 72, training loss: 9394.84, average training loss: 15979.91, base loss: 8827.44
[INFO 2017-06-26 17:46:28,446 main.py:47] epoch 73, training loss: 8970.96, average training loss: 15885.20, base loss: 8833.55
[INFO 2017-06-26 17:46:28,737 main.py:47] epoch 74, training loss: 9101.40, average training loss: 15794.74, base loss: 8839.81
[INFO 2017-06-26 17:46:29,028 main.py:47] epoch 75, training loss: 8188.54, average training loss: 15694.66, base loss: 8835.92
[INFO 2017-06-26 17:46:29,320 main.py:47] epoch 76, training loss: 8919.78, average training loss: 15606.68, base loss: 8841.86
[INFO 2017-06-26 17:46:29,611 main.py:47] epoch 77, training loss: 7778.43, average training loss: 15506.32, base loss: 8831.46
[INFO 2017-06-26 17:46:29,904 main.py:47] epoch 78, training loss: 9009.20, average training loss: 15424.07, base loss: 8839.34
[INFO 2017-06-26 17:46:30,247 main.py:47] epoch 79, training loss: 8347.90, average training loss: 15335.62, base loss: 8837.27
[INFO 2017-06-26 17:46:30,539 main.py:47] epoch 80, training loss: 9151.05, average training loss: 15259.27, base loss: 8845.40
[INFO 2017-06-26 17:46:30,833 main.py:47] epoch 81, training loss: 7596.16, average training loss: 15165.82, base loss: 8833.02
[INFO 2017-06-26 17:46:31,125 main.py:47] epoch 82, training loss: 10551.61, average training loss: 15110.22, base loss: 8860.17
[INFO 2017-06-26 17:46:31,418 main.py:47] epoch 83, training loss: 8853.33, average training loss: 15035.74, base loss: 8866.10
[INFO 2017-06-26 17:46:31,708 main.py:47] epoch 84, training loss: 8614.84, average training loss: 14960.20, base loss: 8869.75
[INFO 2017-06-26 17:46:32,003 main.py:47] epoch 85, training loss: 8840.89, average training loss: 14889.04, base loss: 8875.41
[INFO 2017-06-26 17:46:32,303 main.py:47] epoch 86, training loss: 7225.69, average training loss: 14800.96, base loss: 8858.42
[INFO 2017-06-26 17:46:32,596 main.py:47] epoch 87, training loss: 8624.31, average training loss: 14730.77, base loss: 8860.21
[INFO 2017-06-26 17:46:32,887 main.py:47] epoch 88, training loss: 10082.80, average training loss: 14678.54, base loss: 8880.94
[INFO 2017-06-26 17:46:33,183 main.py:47] epoch 89, training loss: 8651.32, average training loss: 14611.57, base loss: 8882.00
[INFO 2017-06-26 17:46:33,475 main.py:47] epoch 90, training loss: 8757.24, average training loss: 14547.24, base loss: 8886.09
[INFO 2017-06-26 17:46:33,766 main.py:47] epoch 91, training loss: 9211.37, average training loss: 14489.24, base loss: 8895.28
[INFO 2017-06-26 17:46:34,058 main.py:47] epoch 92, training loss: 8481.54, average training loss: 14424.64, base loss: 8895.15
[INFO 2017-06-26 17:46:34,348 main.py:47] epoch 93, training loss: 8929.99, average training loss: 14366.19, base loss: 8901.30
[INFO 2017-06-26 17:46:34,645 main.py:47] epoch 94, training loss: 7033.84, average training loss: 14289.01, base loss: 8883.32
[INFO 2017-06-26 17:46:34,938 main.py:47] epoch 95, training loss: 7874.74, average training loss: 14222.19, base loss: 8875.17
[INFO 2017-06-26 17:46:35,229 main.py:47] epoch 96, training loss: 8320.65, average training loss: 14161.35, base loss: 8873.21
[INFO 2017-06-26 17:46:35,523 main.py:47] epoch 97, training loss: 8174.68, average training loss: 14100.26, base loss: 8869.69
[INFO 2017-06-26 17:46:35,814 main.py:47] epoch 98, training loss: 8149.46, average training loss: 14040.15, base loss: 8867.09
[INFO 2017-06-26 17:46:36,106 main.py:47] epoch 99, training loss: 8354.95, average training loss: 13983.30, base loss: 8866.03
[INFO 2017-06-26 17:46:36,106 main.py:49] epoch 99, testing
[INFO 2017-06-26 17:46:40,145 main.py:100] average testing loss: 8359.44, base loss: 8816.55
[INFO 2017-06-26 17:46:40,168 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:46:40,174 main.py:73] current best accuracy: 8359.44
[INFO 2017-06-26 17:46:40,464 main.py:47] epoch 100, training loss: 7665.75, average training loss: 13920.75, base loss: 8856.66
[INFO 2017-06-26 17:46:40,759 main.py:47] epoch 101, training loss: 7778.70, average training loss: 13860.54, base loss: 8850.09
[INFO 2017-06-26 17:46:41,052 main.py:47] epoch 102, training loss: 8065.38, average training loss: 13804.27, base loss: 8844.83
[INFO 2017-06-26 17:46:41,347 main.py:47] epoch 103, training loss: 7714.94, average training loss: 13745.72, base loss: 8837.70
[INFO 2017-06-26 17:46:41,637 main.py:47] epoch 104, training loss: 10225.46, average training loss: 13712.19, base loss: 8858.40
[INFO 2017-06-26 17:46:41,934 main.py:47] epoch 105, training loss: 9301.21, average training loss: 13670.58, base loss: 8868.55
[INFO 2017-06-26 17:46:42,228 main.py:47] epoch 106, training loss: 8256.14, average training loss: 13619.98, base loss: 8866.76
[INFO 2017-06-26 17:46:42,525 main.py:47] epoch 107, training loss: 7911.55, average training loss: 13567.12, base loss: 8860.67
[INFO 2017-06-26 17:46:42,820 main.py:47] epoch 108, training loss: 9991.19, average training loss: 13534.32, base loss: 8878.12
[INFO 2017-06-26 17:46:43,115 main.py:47] epoch 109, training loss: 7451.56, average training loss: 13479.02, base loss: 8869.08
[INFO 2017-06-26 17:46:43,409 main.py:47] epoch 110, training loss: 7763.52, average training loss: 13427.53, base loss: 8862.50
[INFO 2017-06-26 17:46:43,701 main.py:47] epoch 111, training loss: 8756.97, average training loss: 13385.83, base loss: 8865.88
[INFO 2017-06-26 17:46:44,000 main.py:47] epoch 112, training loss: 9041.42, average training loss: 13347.38, base loss: 8871.87
[INFO 2017-06-26 17:46:44,307 main.py:47] epoch 113, training loss: 7608.82, average training loss: 13297.04, base loss: 8864.86
[INFO 2017-06-26 17:46:44,601 main.py:47] epoch 114, training loss: 7166.62, average training loss: 13243.73, base loss: 8851.51
[INFO 2017-06-26 17:46:44,898 main.py:47] epoch 115, training loss: 8564.45, average training loss: 13203.40, base loss: 8852.78
[INFO 2017-06-26 17:46:45,196 main.py:47] epoch 116, training loss: 9017.14, average training loss: 13167.62, base loss: 8859.39
[INFO 2017-06-26 17:46:45,500 main.py:47] epoch 117, training loss: 9272.61, average training loss: 13134.61, base loss: 8869.14
[INFO 2017-06-26 17:46:45,809 main.py:47] epoch 118, training loss: 7968.00, average training loss: 13091.19, base loss: 8865.41
[INFO 2017-06-26 17:46:46,130 main.py:47] epoch 119, training loss: 8843.60, average training loss: 13055.79, base loss: 8869.41
[INFO 2017-06-26 17:46:46,454 main.py:47] epoch 120, training loss: 8591.47, average training loss: 13018.90, base loss: 8871.82
[INFO 2017-06-26 17:46:46,777 main.py:47] epoch 121, training loss: 7233.08, average training loss: 12971.47, base loss: 8859.69
[INFO 2017-06-26 17:46:47,103 main.py:47] epoch 122, training loss: 8539.39, average training loss: 12935.44, base loss: 8861.14
[INFO 2017-06-26 17:46:47,432 main.py:47] epoch 123, training loss: 7938.54, average training loss: 12895.14, base loss: 8857.18
[INFO 2017-06-26 17:46:47,762 main.py:47] epoch 124, training loss: 9627.90, average training loss: 12869.00, base loss: 8868.65
[INFO 2017-06-26 17:46:48,091 main.py:47] epoch 125, training loss: 8505.63, average training loss: 12834.37, base loss: 8871.13
[INFO 2017-06-26 17:46:48,425 main.py:47] epoch 126, training loss: 8499.40, average training loss: 12800.24, base loss: 8872.32
[INFO 2017-06-26 17:46:48,753 main.py:47] epoch 127, training loss: 8602.90, average training loss: 12767.45, base loss: 8874.10
[INFO 2017-06-26 17:46:49,084 main.py:47] epoch 128, training loss: 8483.47, average training loss: 12734.24, base loss: 8876.25
[INFO 2017-06-26 17:46:49,413 main.py:47] epoch 129, training loss: 7798.96, average training loss: 12696.28, base loss: 8872.59
[INFO 2017-06-26 17:46:49,748 main.py:47] epoch 130, training loss: 8046.59, average training loss: 12660.78, base loss: 8870.59
[INFO 2017-06-26 17:46:50,083 main.py:47] epoch 131, training loss: 7861.97, average training loss: 12624.43, base loss: 8868.27
[INFO 2017-06-26 17:46:50,417 main.py:47] epoch 132, training loss: 8347.71, average training loss: 12592.27, base loss: 8869.56
[INFO 2017-06-26 17:46:50,752 main.py:47] epoch 133, training loss: 9012.64, average training loss: 12565.56, base loss: 8875.45
[INFO 2017-06-26 17:46:51,090 main.py:47] epoch 134, training loss: 7765.14, average training loss: 12530.00, base loss: 8871.73
[INFO 2017-06-26 17:46:51,493 main.py:47] epoch 135, training loss: 7436.47, average training loss: 12492.55, base loss: 8864.72
[INFO 2017-06-26 17:46:51,838 main.py:47] epoch 136, training loss: 8121.82, average training loss: 12460.64, base loss: 8864.17
[INFO 2017-06-26 17:46:52,189 main.py:47] epoch 137, training loss: 7967.27, average training loss: 12428.08, base loss: 8862.20
[INFO 2017-06-26 17:46:52,528 main.py:47] epoch 138, training loss: 7968.84, average training loss: 12396.00, base loss: 8860.29
[INFO 2017-06-26 17:46:52,864 main.py:47] epoch 139, training loss: 7331.74, average training loss: 12359.83, base loss: 8852.86
[INFO 2017-06-26 17:46:53,201 main.py:47] epoch 140, training loss: 7828.06, average training loss: 12327.69, base loss: 8850.08
[INFO 2017-06-26 17:46:53,540 main.py:47] epoch 141, training loss: 7456.65, average training loss: 12293.39, base loss: 8843.77
[INFO 2017-06-26 17:46:53,887 main.py:47] epoch 142, training loss: 7257.44, average training loss: 12258.17, base loss: 8836.76
[INFO 2017-06-26 17:46:54,241 main.py:47] epoch 143, training loss: 9244.98, average training loss: 12237.24, base loss: 8846.35
[INFO 2017-06-26 17:46:54,604 main.py:47] epoch 144, training loss: 6916.88, average training loss: 12200.55, base loss: 8835.19
[INFO 2017-06-26 17:46:54,958 main.py:47] epoch 145, training loss: 7861.06, average training loss: 12170.83, base loss: 8834.13
[INFO 2017-06-26 17:46:55,316 main.py:47] epoch 146, training loss: 8395.65, average training loss: 12145.15, base loss: 8835.49
[INFO 2017-06-26 17:46:55,670 main.py:47] epoch 147, training loss: 8134.36, average training loss: 12118.05, base loss: 8835.93
[INFO 2017-06-26 17:46:56,032 main.py:47] epoch 148, training loss: 8260.93, average training loss: 12092.16, base loss: 8837.29
[INFO 2017-06-26 17:46:56,393 main.py:47] epoch 149, training loss: 8488.39, average training loss: 12068.14, base loss: 8839.67
[INFO 2017-06-26 17:46:56,756 main.py:47] epoch 150, training loss: 8314.66, average training loss: 12043.28, base loss: 8840.29
[INFO 2017-06-26 17:46:57,117 main.py:47] epoch 151, training loss: 8184.12, average training loss: 12017.89, base loss: 8840.25
[INFO 2017-06-26 17:46:57,479 main.py:47] epoch 152, training loss: 7934.58, average training loss: 11991.20, base loss: 8837.63
[INFO 2017-06-26 17:46:57,841 main.py:47] epoch 153, training loss: 8164.81, average training loss: 11966.35, base loss: 8837.95
[INFO 2017-06-26 17:46:58,202 main.py:47] epoch 154, training loss: 7448.24, average training loss: 11937.21, base loss: 8832.00
[INFO 2017-06-26 17:46:58,565 main.py:47] epoch 155, training loss: 8565.33, average training loss: 11915.59, base loss: 8835.81
[INFO 2017-06-26 17:46:58,925 main.py:47] epoch 156, training loss: 7850.74, average training loss: 11889.70, base loss: 8832.98
[INFO 2017-06-26 17:46:59,289 main.py:47] epoch 157, training loss: 6983.31, average training loss: 11858.65, base loss: 8825.09
[INFO 2017-06-26 17:46:59,657 main.py:47] epoch 158, training loss: 8190.15, average training loss: 11835.58, base loss: 8824.69
[INFO 2017-06-26 17:47:00,019 main.py:47] epoch 159, training loss: 8227.03, average training loss: 11813.02, base loss: 8825.81
[INFO 2017-06-26 17:47:00,378 main.py:47] epoch 160, training loss: 8720.61, average training loss: 11793.81, base loss: 8831.09
[INFO 2017-06-26 17:47:00,741 main.py:47] epoch 161, training loss: 10086.97, average training loss: 11783.28, base loss: 8846.32
[INFO 2017-06-26 17:47:01,104 main.py:47] epoch 162, training loss: 8160.98, average training loss: 11761.06, base loss: 8845.90
[INFO 2017-06-26 17:47:01,467 main.py:47] epoch 163, training loss: 8374.18, average training loss: 11740.40, base loss: 8848.09
[INFO 2017-06-26 17:47:01,828 main.py:47] epoch 164, training loss: 7888.82, average training loss: 11717.06, base loss: 8846.13
[INFO 2017-06-26 17:47:02,188 main.py:47] epoch 165, training loss: 8186.02, average training loss: 11695.79, base loss: 8845.95
[INFO 2017-06-26 17:47:02,551 main.py:47] epoch 166, training loss: 7301.55, average training loss: 11669.48, base loss: 8839.27
[INFO 2017-06-26 17:47:02,912 main.py:47] epoch 167, training loss: 8933.43, average training loss: 11653.19, base loss: 8845.27
[INFO 2017-06-26 17:47:03,276 main.py:47] epoch 168, training loss: 8038.02, average training loss: 11631.80, base loss: 8844.78
[INFO 2017-06-26 17:47:03,639 main.py:47] epoch 169, training loss: 8718.49, average training loss: 11614.66, base loss: 8848.89
[INFO 2017-06-26 17:47:04,000 main.py:47] epoch 170, training loss: 7609.64, average training loss: 11591.24, base loss: 8845.66
[INFO 2017-06-26 17:47:04,360 main.py:47] epoch 171, training loss: 8901.05, average training loss: 11575.60, base loss: 8851.95
[INFO 2017-06-26 17:47:04,727 main.py:47] epoch 172, training loss: 10205.34, average training loss: 11567.68, base loss: 8866.38
[INFO 2017-06-26 17:47:05,111 main.py:47] epoch 173, training loss: 7626.53, average training loss: 11545.03, base loss: 8862.91
[INFO 2017-06-26 17:47:05,477 main.py:47] epoch 174, training loss: 8568.52, average training loss: 11528.02, base loss: 8865.07
[INFO 2017-06-26 17:47:05,843 main.py:47] epoch 175, training loss: 8949.03, average training loss: 11513.37, base loss: 8870.05
[INFO 2017-06-26 17:47:06,209 main.py:47] epoch 176, training loss: 7517.41, average training loss: 11490.79, base loss: 8866.08
[INFO 2017-06-26 17:47:06,573 main.py:47] epoch 177, training loss: 8504.21, average training loss: 11474.01, base loss: 8869.21
[INFO 2017-06-26 17:47:06,938 main.py:47] epoch 178, training loss: 7815.31, average training loss: 11453.57, base loss: 8867.02
[INFO 2017-06-26 17:47:07,305 main.py:47] epoch 179, training loss: 7498.61, average training loss: 11431.60, base loss: 8863.88
[INFO 2017-06-26 17:47:07,671 main.py:47] epoch 180, training loss: 7949.52, average training loss: 11412.36, base loss: 8863.83
[INFO 2017-06-26 17:47:08,033 main.py:47] epoch 181, training loss: 7083.44, average training loss: 11388.58, base loss: 8857.68
[INFO 2017-06-26 17:47:08,400 main.py:47] epoch 182, training loss: 7788.16, average training loss: 11368.90, base loss: 8855.31
[INFO 2017-06-26 17:47:08,766 main.py:47] epoch 183, training loss: 6770.30, average training loss: 11343.91, base loss: 8846.34
[INFO 2017-06-26 17:47:09,130 main.py:47] epoch 184, training loss: 8236.65, average training loss: 11327.11, base loss: 8846.54
[INFO 2017-06-26 17:47:09,497 main.py:47] epoch 185, training loss: 8796.89, average training loss: 11313.51, base loss: 8850.01
[INFO 2017-06-26 17:47:09,864 main.py:47] epoch 186, training loss: 7709.78, average training loss: 11294.24, base loss: 8848.05
[INFO 2017-06-26 17:47:10,231 main.py:47] epoch 187, training loss: 8485.38, average training loss: 11279.30, base loss: 8851.42
[INFO 2017-06-26 17:47:10,599 main.py:47] epoch 188, training loss: 8056.11, average training loss: 11262.25, base loss: 8851.08
[INFO 2017-06-26 17:47:10,965 main.py:47] epoch 189, training loss: 9057.50, average training loss: 11250.64, base loss: 8855.23
[INFO 2017-06-26 17:47:11,329 main.py:47] epoch 190, training loss: 8548.01, average training loss: 11236.49, base loss: 8859.04
[INFO 2017-06-26 17:47:11,691 main.py:47] epoch 191, training loss: 6787.99, average training loss: 11213.32, base loss: 8851.38
[INFO 2017-06-26 17:47:12,057 main.py:47] epoch 192, training loss: 8500.06, average training loss: 11199.26, base loss: 8855.37
[INFO 2017-06-26 17:47:12,425 main.py:47] epoch 193, training loss: 8289.25, average training loss: 11184.26, base loss: 8858.63
[INFO 2017-06-26 17:47:12,790 main.py:47] epoch 194, training loss: 7627.88, average training loss: 11166.03, base loss: 8855.90
[INFO 2017-06-26 17:47:13,155 main.py:47] epoch 195, training loss: 8249.72, average training loss: 11151.15, base loss: 8857.15
[INFO 2017-06-26 17:47:13,519 main.py:47] epoch 196, training loss: 7274.70, average training loss: 11131.47, base loss: 8853.08
[INFO 2017-06-26 17:47:13,884 main.py:47] epoch 197, training loss: 7819.07, average training loss: 11114.74, base loss: 8852.58
[INFO 2017-06-26 17:47:14,251 main.py:47] epoch 198, training loss: 7369.11, average training loss: 11095.92, base loss: 8848.35
[INFO 2017-06-26 17:47:14,615 main.py:47] epoch 199, training loss: 8207.94, average training loss: 11081.48, base loss: 8850.72
[INFO 2017-06-26 17:47:14,615 main.py:49] epoch 199, testing
[INFO 2017-06-26 17:47:18,764 main.py:100] average testing loss: 7763.25, base loss: 8493.42
[INFO 2017-06-26 17:47:18,789 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:47:18,798 main.py:73] current best accuracy: 7763.25
[INFO 2017-06-26 17:47:19,143 main.py:47] epoch 200, training loss: 7742.67, average training loss: 11064.87, base loss: 8851.25
[INFO 2017-06-26 17:47:19,498 main.py:47] epoch 201, training loss: 6932.82, average training loss: 11044.41, base loss: 8844.31
[INFO 2017-06-26 17:47:19,845 main.py:47] epoch 202, training loss: 8586.47, average training loss: 11032.30, base loss: 8847.04
[INFO 2017-06-26 17:47:20,205 main.py:47] epoch 203, training loss: 7372.24, average training loss: 11014.36, base loss: 8844.71
[INFO 2017-06-26 17:47:20,569 main.py:47] epoch 204, training loss: 8155.85, average training loss: 11000.42, base loss: 8845.66
[INFO 2017-06-26 17:47:20,932 main.py:47] epoch 205, training loss: 7814.02, average training loss: 10984.95, base loss: 8846.58
[INFO 2017-06-26 17:47:21,293 main.py:47] epoch 206, training loss: 8547.36, average training loss: 10973.17, base loss: 8849.99
[INFO 2017-06-26 17:47:21,654 main.py:47] epoch 207, training loss: 9207.99, average training loss: 10964.69, base loss: 8856.07
[INFO 2017-06-26 17:47:22,017 main.py:47] epoch 208, training loss: 7411.56, average training loss: 10947.69, base loss: 8852.04
[INFO 2017-06-26 17:47:22,379 main.py:47] epoch 209, training loss: 8302.41, average training loss: 10935.09, base loss: 8854.20
[INFO 2017-06-26 17:47:22,742 main.py:47] epoch 210, training loss: 7706.89, average training loss: 10919.79, base loss: 8852.29
[INFO 2017-06-26 17:47:23,106 main.py:47] epoch 211, training loss: 7409.69, average training loss: 10903.23, base loss: 8849.71
[INFO 2017-06-26 17:47:23,467 main.py:47] epoch 212, training loss: 7730.72, average training loss: 10888.34, base loss: 8848.36
[INFO 2017-06-26 17:47:23,832 main.py:47] epoch 213, training loss: 8341.77, average training loss: 10876.44, base loss: 8849.84
[INFO 2017-06-26 17:47:24,193 main.py:47] epoch 214, training loss: 8358.71, average training loss: 10864.73, base loss: 8851.35
[INFO 2017-06-26 17:47:24,555 main.py:47] epoch 215, training loss: 7810.08, average training loss: 10850.59, base loss: 8851.38
[INFO 2017-06-26 17:47:24,922 main.py:47] epoch 216, training loss: 8357.34, average training loss: 10839.10, base loss: 8854.12
[INFO 2017-06-26 17:47:25,299 main.py:47] epoch 217, training loss: 6989.74, average training loss: 10821.44, base loss: 8850.01
[INFO 2017-06-26 17:47:25,663 main.py:47] epoch 218, training loss: 7233.44, average training loss: 10805.06, base loss: 8844.97
[INFO 2017-06-26 17:47:26,024 main.py:47] epoch 219, training loss: 8459.03, average training loss: 10794.39, base loss: 8847.80
[INFO 2017-06-26 17:47:26,386 main.py:47] epoch 220, training loss: 9242.48, average training loss: 10787.37, base loss: 8852.28
[INFO 2017-06-26 17:47:26,747 main.py:47] epoch 221, training loss: 8696.61, average training loss: 10777.95, base loss: 8856.16
[INFO 2017-06-26 17:47:27,110 main.py:47] epoch 222, training loss: 7942.11, average training loss: 10765.24, base loss: 8855.69
[INFO 2017-06-26 17:47:27,470 main.py:47] epoch 223, training loss: 7880.43, average training loss: 10752.36, base loss: 8854.36
[INFO 2017-06-26 17:47:27,831 main.py:47] epoch 224, training loss: 7658.58, average training loss: 10738.61, base loss: 8852.49
[INFO 2017-06-26 17:47:28,196 main.py:47] epoch 225, training loss: 8068.87, average training loss: 10726.79, base loss: 8853.53
[INFO 2017-06-26 17:47:28,559 main.py:47] epoch 226, training loss: 7706.31, average training loss: 10713.49, base loss: 8852.51
[INFO 2017-06-26 17:47:28,921 main.py:47] epoch 227, training loss: 7423.60, average training loss: 10699.06, base loss: 8850.32
[INFO 2017-06-26 17:47:29,282 main.py:47] epoch 228, training loss: 7499.92, average training loss: 10685.09, base loss: 8848.15
[INFO 2017-06-26 17:47:29,644 main.py:47] epoch 229, training loss: 8895.12, average training loss: 10677.31, base loss: 8853.76
[INFO 2017-06-26 17:47:30,012 main.py:47] epoch 230, training loss: 8316.12, average training loss: 10667.08, base loss: 8854.80
[INFO 2017-06-26 17:47:30,381 main.py:47] epoch 231, training loss: 8182.59, average training loss: 10656.38, base loss: 8855.82
[INFO 2017-06-26 17:47:30,751 main.py:47] epoch 232, training loss: 8221.09, average training loss: 10645.92, base loss: 8857.04
[INFO 2017-06-26 17:47:31,122 main.py:47] epoch 233, training loss: 7481.04, average training loss: 10632.40, base loss: 8855.22
[INFO 2017-06-26 17:47:31,495 main.py:47] epoch 234, training loss: 7176.84, average training loss: 10617.69, base loss: 8851.43
[INFO 2017-06-26 17:47:31,864 main.py:47] epoch 235, training loss: 8517.11, average training loss: 10608.79, base loss: 8856.31
[INFO 2017-06-26 17:47:32,230 main.py:47] epoch 236, training loss: 7787.19, average training loss: 10596.89, base loss: 8856.70
[INFO 2017-06-26 17:47:32,596 main.py:47] epoch 237, training loss: 7296.68, average training loss: 10583.02, base loss: 8853.76
[INFO 2017-06-26 17:47:32,964 main.py:47] epoch 238, training loss: 7887.21, average training loss: 10571.74, base loss: 8852.62
[INFO 2017-06-26 17:47:33,328 main.py:47] epoch 239, training loss: 6368.63, average training loss: 10554.23, base loss: 8844.15
[INFO 2017-06-26 17:47:33,690 main.py:47] epoch 240, training loss: 7567.79, average training loss: 10541.84, base loss: 8842.40
[INFO 2017-06-26 17:47:34,055 main.py:47] epoch 241, training loss: 7329.91, average training loss: 10528.56, base loss: 8839.05
[INFO 2017-06-26 17:47:34,416 main.py:47] epoch 242, training loss: 8467.63, average training loss: 10520.08, base loss: 8841.98
[INFO 2017-06-26 17:47:34,896 main.py:47] epoch 243, training loss: 9064.86, average training loss: 10514.12, base loss: 8847.75
[INFO 2017-06-26 17:47:35,261 main.py:47] epoch 244, training loss: 8436.76, average training loss: 10505.64, base loss: 8851.33
[INFO 2017-06-26 17:47:35,633 main.py:47] epoch 245, training loss: 7056.37, average training loss: 10491.62, base loss: 8848.19
[INFO 2017-06-26 17:47:35,997 main.py:47] epoch 246, training loss: 7568.59, average training loss: 10479.78, base loss: 8846.72
[INFO 2017-06-26 17:47:36,362 main.py:47] epoch 247, training loss: 7797.70, average training loss: 10468.97, base loss: 8846.56
[INFO 2017-06-26 17:47:36,726 main.py:47] epoch 248, training loss: 7441.40, average training loss: 10456.81, base loss: 8844.82
[INFO 2017-06-26 17:47:37,090 main.py:47] epoch 249, training loss: 7965.71, average training loss: 10446.85, base loss: 8845.28
[INFO 2017-06-26 17:47:37,453 main.py:47] epoch 250, training loss: 6989.36, average training loss: 10433.07, base loss: 8840.30
[INFO 2017-06-26 17:47:37,815 main.py:47] epoch 251, training loss: 8186.34, average training loss: 10424.16, base loss: 8841.52
[INFO 2017-06-26 17:47:38,178 main.py:47] epoch 252, training loss: 7609.38, average training loss: 10413.03, base loss: 8838.75
[INFO 2017-06-26 17:47:38,543 main.py:47] epoch 253, training loss: 7469.48, average training loss: 10401.44, base loss: 8836.87
[INFO 2017-06-26 17:47:38,905 main.py:47] epoch 254, training loss: 8417.74, average training loss: 10393.66, base loss: 8841.36
[INFO 2017-06-26 17:47:39,267 main.py:47] epoch 255, training loss: 7848.83, average training loss: 10383.72, base loss: 8841.86
[INFO 2017-06-26 17:47:39,630 main.py:47] epoch 256, training loss: 7762.63, average training loss: 10373.52, base loss: 8840.12
[INFO 2017-06-26 17:47:39,993 main.py:47] epoch 257, training loss: 7274.31, average training loss: 10361.51, base loss: 8836.89
[INFO 2017-06-26 17:47:40,354 main.py:47] epoch 258, training loss: 7031.41, average training loss: 10348.65, base loss: 8832.54
[INFO 2017-06-26 17:47:40,719 main.py:47] epoch 259, training loss: 7879.18, average training loss: 10339.16, base loss: 8832.16
[INFO 2017-06-26 17:47:41,080 main.py:47] epoch 260, training loss: 8429.63, average training loss: 10331.84, base loss: 8835.09
[INFO 2017-06-26 17:47:41,441 main.py:47] epoch 261, training loss: 7205.60, average training loss: 10319.91, base loss: 8831.90
[INFO 2017-06-26 17:47:41,801 main.py:47] epoch 262, training loss: 7099.66, average training loss: 10307.66, base loss: 8826.82
[INFO 2017-06-26 17:47:42,163 main.py:47] epoch 263, training loss: 8187.63, average training loss: 10299.63, base loss: 8827.67
[INFO 2017-06-26 17:47:42,523 main.py:47] epoch 264, training loss: 7725.10, average training loss: 10289.92, base loss: 8826.56
[INFO 2017-06-26 17:47:42,886 main.py:47] epoch 265, training loss: 7712.46, average training loss: 10280.23, base loss: 8827.36
[INFO 2017-06-26 17:47:43,246 main.py:47] epoch 266, training loss: 6627.52, average training loss: 10266.55, base loss: 8821.65
[INFO 2017-06-26 17:47:43,607 main.py:47] epoch 267, training loss: 6045.30, average training loss: 10250.80, base loss: 8812.15
[INFO 2017-06-26 17:47:43,968 main.py:47] epoch 268, training loss: 9538.39, average training loss: 10248.15, base loss: 8819.86
[INFO 2017-06-26 17:47:44,332 main.py:47] epoch 269, training loss: 7419.92, average training loss: 10237.67, base loss: 8816.92
[INFO 2017-06-26 17:47:44,694 main.py:47] epoch 270, training loss: 8902.79, average training loss: 10232.75, base loss: 8821.69
[INFO 2017-06-26 17:47:45,055 main.py:47] epoch 271, training loss: 8162.79, average training loss: 10225.14, base loss: 8823.43
[INFO 2017-06-26 17:47:45,418 main.py:47] epoch 272, training loss: 7552.25, average training loss: 10215.35, base loss: 8823.44
[INFO 2017-06-26 17:47:45,782 main.py:47] epoch 273, training loss: 6647.18, average training loss: 10202.32, base loss: 8818.46
[INFO 2017-06-26 17:47:46,144 main.py:47] epoch 274, training loss: 8185.23, average training loss: 10194.99, base loss: 8820.05
[INFO 2017-06-26 17:47:46,506 main.py:47] epoch 275, training loss: 8864.19, average training loss: 10190.17, base loss: 8825.12
[INFO 2017-06-26 17:47:46,869 main.py:47] epoch 276, training loss: 7294.37, average training loss: 10179.71, base loss: 8822.11
[INFO 2017-06-26 17:47:47,228 main.py:47] epoch 277, training loss: 9226.70, average training loss: 10176.28, base loss: 8828.62
[INFO 2017-06-26 17:47:47,592 main.py:47] epoch 278, training loss: 7593.48, average training loss: 10167.03, base loss: 8827.56
[INFO 2017-06-26 17:47:47,954 main.py:47] epoch 279, training loss: 8205.32, average training loss: 10160.02, base loss: 8829.43
[INFO 2017-06-26 17:47:48,315 main.py:47] epoch 280, training loss: 8029.13, average training loss: 10152.44, base loss: 8831.09
[INFO 2017-06-26 17:47:48,723 main.py:47] epoch 281, training loss: 8223.71, average training loss: 10145.60, base loss: 8833.04
[INFO 2017-06-26 17:47:49,086 main.py:47] epoch 282, training loss: 6856.03, average training loss: 10133.97, base loss: 8828.74
[INFO 2017-06-26 17:47:49,451 main.py:47] epoch 283, training loss: 7917.69, average training loss: 10126.17, base loss: 8830.01
[INFO 2017-06-26 17:47:49,813 main.py:47] epoch 284, training loss: 7754.88, average training loss: 10117.85, base loss: 8830.64
[INFO 2017-06-26 17:47:50,175 main.py:47] epoch 285, training loss: 7313.75, average training loss: 10108.05, base loss: 8829.28
[INFO 2017-06-26 17:47:50,536 main.py:47] epoch 286, training loss: 7603.90, average training loss: 10099.32, base loss: 8828.38
[INFO 2017-06-26 17:47:50,898 main.py:47] epoch 287, training loss: 8560.22, average training loss: 10093.98, base loss: 8830.47
[INFO 2017-06-26 17:47:51,261 main.py:47] epoch 288, training loss: 7688.46, average training loss: 10085.65, base loss: 8829.83
[INFO 2017-06-26 17:47:51,621 main.py:47] epoch 289, training loss: 8328.61, average training loss: 10079.59, base loss: 8831.31
[INFO 2017-06-26 17:47:51,983 main.py:47] epoch 290, training loss: 6937.98, average training loss: 10068.80, base loss: 8826.80
[INFO 2017-06-26 17:47:52,347 main.py:47] epoch 291, training loss: 8356.16, average training loss: 10062.93, base loss: 8829.53
[INFO 2017-06-26 17:47:52,711 main.py:47] epoch 292, training loss: 6763.62, average training loss: 10051.67, base loss: 8825.46
[INFO 2017-06-26 17:47:53,071 main.py:47] epoch 293, training loss: 8125.07, average training loss: 10045.12, base loss: 8827.94
[INFO 2017-06-26 17:47:53,433 main.py:47] epoch 294, training loss: 8031.89, average training loss: 10038.29, base loss: 8829.33
[INFO 2017-06-26 17:47:53,794 main.py:47] epoch 295, training loss: 7477.12, average training loss: 10029.64, base loss: 8828.94
[INFO 2017-06-26 17:47:54,156 main.py:47] epoch 296, training loss: 6753.55, average training loss: 10018.61, base loss: 8823.90
[INFO 2017-06-26 17:47:54,518 main.py:47] epoch 297, training loss: 7755.95, average training loss: 10011.02, base loss: 8824.90
[INFO 2017-06-26 17:47:54,881 main.py:47] epoch 298, training loss: 7051.04, average training loss: 10001.12, base loss: 8821.84
[INFO 2017-06-26 17:47:55,244 main.py:47] epoch 299, training loss: 7318.81, average training loss: 9992.18, base loss: 8820.27
[INFO 2017-06-26 17:47:55,244 main.py:49] epoch 299, testing
[INFO 2017-06-26 17:47:59,451 main.py:100] average testing loss: 7381.74, base loss: 8468.58
[INFO 2017-06-26 17:47:59,474 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:47:59,481 main.py:73] current best accuracy: 7381.74
[INFO 2017-06-26 17:47:59,840 main.py:47] epoch 300, training loss: 7320.03, average training loss: 9983.30, base loss: 8818.16
[INFO 2017-06-26 17:48:00,204 main.py:47] epoch 301, training loss: 8265.30, average training loss: 9977.61, base loss: 8820.33
[INFO 2017-06-26 17:48:00,568 main.py:47] epoch 302, training loss: 7533.30, average training loss: 9969.54, base loss: 8819.66
[INFO 2017-06-26 17:48:00,935 main.py:47] epoch 303, training loss: 7567.32, average training loss: 9961.64, base loss: 8819.73
[INFO 2017-06-26 17:48:01,296 main.py:47] epoch 304, training loss: 7727.43, average training loss: 9954.32, base loss: 8820.94
[INFO 2017-06-26 17:48:01,661 main.py:47] epoch 305, training loss: 6919.77, average training loss: 9944.40, base loss: 8816.92
[INFO 2017-06-26 17:48:02,022 main.py:47] epoch 306, training loss: 7567.53, average training loss: 9936.66, base loss: 8817.07
[INFO 2017-06-26 17:48:02,384 main.py:47] epoch 307, training loss: 7633.88, average training loss: 9929.18, base loss: 8815.93
[INFO 2017-06-26 17:48:02,749 main.py:47] epoch 308, training loss: 7779.60, average training loss: 9922.23, base loss: 8815.63
[INFO 2017-06-26 17:48:03,112 main.py:47] epoch 309, training loss: 7690.28, average training loss: 9915.03, base loss: 8815.51
[INFO 2017-06-26 17:48:03,482 main.py:47] epoch 310, training loss: 8600.75, average training loss: 9910.80, base loss: 8818.23
[INFO 2017-06-26 17:48:03,846 main.py:47] epoch 311, training loss: 7990.91, average training loss: 9904.65, base loss: 8818.17
[INFO 2017-06-26 17:48:04,206 main.py:47] epoch 312, training loss: 8065.44, average training loss: 9898.77, base loss: 8820.03
[INFO 2017-06-26 17:48:04,569 main.py:47] epoch 313, training loss: 8810.33, average training loss: 9895.30, base loss: 8824.25
[INFO 2017-06-26 17:48:04,929 main.py:47] epoch 314, training loss: 7081.36, average training loss: 9886.37, base loss: 8821.24
[INFO 2017-06-26 17:48:05,292 main.py:47] epoch 315, training loss: 7798.13, average training loss: 9879.76, base loss: 8820.03
[INFO 2017-06-26 17:48:05,651 main.py:47] epoch 316, training loss: 7431.14, average training loss: 9872.04, base loss: 8819.10
[INFO 2017-06-26 17:48:06,015 main.py:47] epoch 317, training loss: 7556.53, average training loss: 9864.76, base loss: 8818.27
[INFO 2017-06-26 17:48:06,380 main.py:47] epoch 318, training loss: 8430.79, average training loss: 9860.26, base loss: 8821.36
[INFO 2017-06-26 17:48:06,744 main.py:47] epoch 319, training loss: 8305.85, average training loss: 9855.40, base loss: 8822.36
[INFO 2017-06-26 17:48:07,109 main.py:47] epoch 320, training loss: 7988.86, average training loss: 9849.59, base loss: 8824.47
[INFO 2017-06-26 17:48:07,471 main.py:47] epoch 321, training loss: 7674.89, average training loss: 9842.83, base loss: 8824.62
[INFO 2017-06-26 17:48:07,836 main.py:47] epoch 322, training loss: 8301.33, average training loss: 9838.06, base loss: 8827.53
[INFO 2017-06-26 17:48:08,200 main.py:47] epoch 323, training loss: 8645.13, average training loss: 9834.38, base loss: 8831.14
[INFO 2017-06-26 17:48:08,573 main.py:47] epoch 324, training loss: 8121.03, average training loss: 9829.11, base loss: 8832.57
[INFO 2017-06-26 17:48:08,940 main.py:47] epoch 325, training loss: 7294.90, average training loss: 9821.33, base loss: 8831.19
[INFO 2017-06-26 17:48:09,309 main.py:47] epoch 326, training loss: 6977.43, average training loss: 9812.64, base loss: 8828.19
[INFO 2017-06-26 17:48:09,672 main.py:47] epoch 327, training loss: 7141.57, average training loss: 9804.49, base loss: 8826.69
[INFO 2017-06-26 17:48:10,039 main.py:47] epoch 328, training loss: 8930.13, average training loss: 9801.84, base loss: 8832.48
[INFO 2017-06-26 17:48:10,402 main.py:47] epoch 329, training loss: 7960.13, average training loss: 9796.26, base loss: 8834.04
[INFO 2017-06-26 17:48:10,773 main.py:47] epoch 330, training loss: 7581.37, average training loss: 9789.56, base loss: 8833.00
[INFO 2017-06-26 17:48:11,134 main.py:47] epoch 331, training loss: 7896.08, average training loss: 9783.86, base loss: 8833.48
[INFO 2017-06-26 17:48:11,496 main.py:47] epoch 332, training loss: 7827.51, average training loss: 9777.99, base loss: 8834.91
[INFO 2017-06-26 17:48:11,860 main.py:47] epoch 333, training loss: 7835.40, average training loss: 9772.17, base loss: 8833.97
[INFO 2017-06-26 17:48:12,225 main.py:47] epoch 334, training loss: 7799.68, average training loss: 9766.28, base loss: 8832.29
[INFO 2017-06-26 17:48:12,586 main.py:47] epoch 335, training loss: 8279.88, average training loss: 9761.86, base loss: 8833.54
[INFO 2017-06-26 17:48:12,948 main.py:47] epoch 336, training loss: 7581.22, average training loss: 9755.39, base loss: 8833.25
[INFO 2017-06-26 17:48:13,312 main.py:47] epoch 337, training loss: 7412.15, average training loss: 9748.45, base loss: 8833.05
[INFO 2017-06-26 17:48:13,681 main.py:47] epoch 338, training loss: 6630.34, average training loss: 9739.26, base loss: 8829.64
[INFO 2017-06-26 17:48:14,049 main.py:47] epoch 339, training loss: 8016.88, average training loss: 9734.19, base loss: 8831.65
[INFO 2017-06-26 17:48:14,419 main.py:47] epoch 340, training loss: 7526.01, average training loss: 9727.72, base loss: 8831.37
[INFO 2017-06-26 17:48:14,784 main.py:47] epoch 341, training loss: 7071.29, average training loss: 9719.95, base loss: 8828.90
[INFO 2017-06-26 17:48:15,157 main.py:47] epoch 342, training loss: 7877.08, average training loss: 9714.58, base loss: 8829.87
[INFO 2017-06-26 17:48:15,528 main.py:47] epoch 343, training loss: 7320.31, average training loss: 9707.62, base loss: 8829.18
[INFO 2017-06-26 17:48:15,894 main.py:47] epoch 344, training loss: 7810.76, average training loss: 9702.12, base loss: 8829.36
[INFO 2017-06-26 17:48:16,263 main.py:47] epoch 345, training loss: 6455.39, average training loss: 9692.73, base loss: 8825.17
[INFO 2017-06-26 17:48:16,633 main.py:47] epoch 346, training loss: 8526.11, average training loss: 9689.37, base loss: 8828.19
[INFO 2017-06-26 17:48:16,996 main.py:47] epoch 347, training loss: 7382.89, average training loss: 9682.74, base loss: 8828.01
[INFO 2017-06-26 17:48:17,357 main.py:47] epoch 348, training loss: 7104.58, average training loss: 9675.36, base loss: 8826.05
[INFO 2017-06-26 17:48:17,717 main.py:47] epoch 349, training loss: 6720.13, average training loss: 9666.91, base loss: 8821.53
[INFO 2017-06-26 17:48:18,081 main.py:47] epoch 350, training loss: 7934.05, average training loss: 9661.98, base loss: 8822.23
[INFO 2017-06-26 17:48:18,443 main.py:47] epoch 351, training loss: 7041.21, average training loss: 9654.53, base loss: 8821.55
[INFO 2017-06-26 17:48:18,812 main.py:47] epoch 352, training loss: 7516.94, average training loss: 9648.48, base loss: 8820.82
[INFO 2017-06-26 17:48:19,175 main.py:47] epoch 353, training loss: 7424.11, average training loss: 9642.19, base loss: 8820.32
[INFO 2017-06-26 17:48:19,542 main.py:47] epoch 354, training loss: 7236.74, average training loss: 9635.42, base loss: 8818.89
[INFO 2017-06-26 17:48:19,906 main.py:47] epoch 355, training loss: 7990.21, average training loss: 9630.79, base loss: 8821.23
[INFO 2017-06-26 17:48:20,270 main.py:47] epoch 356, training loss: 8095.57, average training loss: 9626.49, base loss: 8823.59
[INFO 2017-06-26 17:48:20,636 main.py:47] epoch 357, training loss: 7360.49, average training loss: 9620.16, base loss: 8822.01
[INFO 2017-06-26 17:48:20,998 main.py:47] epoch 358, training loss: 7225.94, average training loss: 9613.50, base loss: 8820.67
[INFO 2017-06-26 17:48:21,364 main.py:47] epoch 359, training loss: 8882.74, average training loss: 9611.47, base loss: 8824.68
[INFO 2017-06-26 17:48:21,730 main.py:47] epoch 360, training loss: 7184.80, average training loss: 9604.74, base loss: 8823.09
[INFO 2017-06-26 17:48:22,094 main.py:47] epoch 361, training loss: 7150.11, average training loss: 9597.96, base loss: 8821.57
[INFO 2017-06-26 17:48:22,458 main.py:47] epoch 362, training loss: 9101.66, average training loss: 9596.60, base loss: 8825.47
[INFO 2017-06-26 17:48:22,824 main.py:47] epoch 363, training loss: 8734.19, average training loss: 9594.23, base loss: 8828.24
[INFO 2017-06-26 17:48:23,191 main.py:47] epoch 364, training loss: 8934.29, average training loss: 9592.42, base loss: 8832.05
[INFO 2017-06-26 17:48:23,556 main.py:47] epoch 365, training loss: 7647.78, average training loss: 9587.10, base loss: 8831.97
[INFO 2017-06-26 17:48:23,935 main.py:47] epoch 366, training loss: 7529.74, average training loss: 9581.50, base loss: 8831.44
[INFO 2017-06-26 17:48:24,304 main.py:47] epoch 367, training loss: 7883.31, average training loss: 9576.88, base loss: 8831.92
[INFO 2017-06-26 17:48:24,669 main.py:47] epoch 368, training loss: 8064.04, average training loss: 9572.78, base loss: 8832.90
[INFO 2017-06-26 17:48:25,034 main.py:47] epoch 369, training loss: 8459.71, average training loss: 9569.78, base loss: 8835.16
[INFO 2017-06-26 17:48:25,401 main.py:47] epoch 370, training loss: 7967.70, average training loss: 9565.46, base loss: 8837.10
[INFO 2017-06-26 17:48:25,768 main.py:47] epoch 371, training loss: 6883.29, average training loss: 9558.25, base loss: 8832.84
[INFO 2017-06-26 17:48:26,133 main.py:47] epoch 372, training loss: 7866.05, average training loss: 9553.71, base loss: 8833.94
[INFO 2017-06-26 17:48:26,498 main.py:47] epoch 373, training loss: 8328.30, average training loss: 9550.43, base loss: 8835.58
[INFO 2017-06-26 17:48:26,865 main.py:47] epoch 374, training loss: 7351.66, average training loss: 9544.57, base loss: 8834.91
[INFO 2017-06-26 17:48:27,230 main.py:47] epoch 375, training loss: 7577.89, average training loss: 9539.34, base loss: 8834.62
[INFO 2017-06-26 17:48:27,594 main.py:47] epoch 376, training loss: 7118.85, average training loss: 9532.92, base loss: 8832.85
[INFO 2017-06-26 17:48:27,959 main.py:47] epoch 377, training loss: 6395.78, average training loss: 9524.62, base loss: 8828.68
[INFO 2017-06-26 17:48:28,324 main.py:47] epoch 378, training loss: 7945.69, average training loss: 9520.45, base loss: 8829.29
[INFO 2017-06-26 17:48:28,686 main.py:47] epoch 379, training loss: 7441.43, average training loss: 9514.98, base loss: 8828.07
[INFO 2017-06-26 17:48:29,051 main.py:47] epoch 380, training loss: 7826.42, average training loss: 9510.55, base loss: 8828.76
[INFO 2017-06-26 17:48:29,414 main.py:47] epoch 381, training loss: 7448.55, average training loss: 9505.15, base loss: 8828.50
[INFO 2017-06-26 17:48:29,776 main.py:47] epoch 382, training loss: 8205.71, average training loss: 9501.76, base loss: 8829.80
[INFO 2017-06-26 17:48:30,142 main.py:47] epoch 383, training loss: 7447.23, average training loss: 9496.41, base loss: 8829.21
[INFO 2017-06-26 17:48:30,504 main.py:47] epoch 384, training loss: 7381.86, average training loss: 9490.92, base loss: 8827.97
[INFO 2017-06-26 17:48:30,866 main.py:47] epoch 385, training loss: 6940.93, average training loss: 9484.31, base loss: 8826.00
[INFO 2017-06-26 17:48:31,230 main.py:47] epoch 386, training loss: 7289.36, average training loss: 9478.64, base loss: 8824.99
[INFO 2017-06-26 17:48:31,591 main.py:47] epoch 387, training loss: 8276.33, average training loss: 9475.54, base loss: 8826.36
[INFO 2017-06-26 17:48:31,954 main.py:47] epoch 388, training loss: 7395.72, average training loss: 9470.20, base loss: 8826.16
[INFO 2017-06-26 17:48:32,319 main.py:47] epoch 389, training loss: 7407.41, average training loss: 9464.91, base loss: 8826.17
[INFO 2017-06-26 17:48:32,679 main.py:47] epoch 390, training loss: 6891.24, average training loss: 9458.32, base loss: 8824.39
[INFO 2017-06-26 17:48:33,044 main.py:47] epoch 391, training loss: 6976.88, average training loss: 9451.99, base loss: 8822.68
[INFO 2017-06-26 17:48:33,406 main.py:47] epoch 392, training loss: 7063.80, average training loss: 9445.92, base loss: 8821.10
[INFO 2017-06-26 17:48:33,769 main.py:47] epoch 393, training loss: 6607.20, average training loss: 9438.71, base loss: 8818.13
[INFO 2017-06-26 17:48:34,140 main.py:47] epoch 394, training loss: 7754.57, average training loss: 9434.45, base loss: 8819.57
[INFO 2017-06-26 17:48:34,516 main.py:47] epoch 395, training loss: 7983.56, average training loss: 9430.78, base loss: 8820.88
[INFO 2017-06-26 17:48:34,885 main.py:47] epoch 396, training loss: 6160.45, average training loss: 9422.55, base loss: 8816.38
[INFO 2017-06-26 17:48:35,250 main.py:47] epoch 397, training loss: 7664.89, average training loss: 9418.13, base loss: 8816.77
[INFO 2017-06-26 17:48:35,614 main.py:47] epoch 398, training loss: 7837.25, average training loss: 9414.17, base loss: 8818.03
[INFO 2017-06-26 17:48:35,974 main.py:47] epoch 399, training loss: 7887.73, average training loss: 9410.35, base loss: 8818.00
[INFO 2017-06-26 17:48:35,975 main.py:49] epoch 399, testing
[INFO 2017-06-26 17:48:40,182 main.py:100] average testing loss: 7449.39, base loss: 8592.93
[INFO 2017-06-26 17:48:40,206 main.py:73] current best accuracy: 7381.74
[INFO 2017-06-26 17:48:40,575 main.py:47] epoch 400, training loss: 7371.46, average training loss: 9405.27, base loss: 8816.61
[INFO 2017-06-26 17:48:40,937 main.py:47] epoch 401, training loss: 7417.17, average training loss: 9400.32, base loss: 8816.70
[INFO 2017-06-26 17:48:41,298 main.py:47] epoch 402, training loss: 8733.42, average training loss: 9398.67, base loss: 8820.08
[INFO 2017-06-26 17:48:41,663 main.py:47] epoch 403, training loss: 7222.20, average training loss: 9393.28, base loss: 8819.15
[INFO 2017-06-26 17:48:42,026 main.py:47] epoch 404, training loss: 8071.77, average training loss: 9390.02, base loss: 8821.51
[INFO 2017-06-26 17:48:42,389 main.py:47] epoch 405, training loss: 7110.40, average training loss: 9384.40, base loss: 8820.51
[INFO 2017-06-26 17:48:42,750 main.py:47] epoch 406, training loss: 7547.83, average training loss: 9379.89, base loss: 8821.06
[INFO 2017-06-26 17:48:43,114 main.py:47] epoch 407, training loss: 6647.09, average training loss: 9373.19, base loss: 8817.73
[INFO 2017-06-26 17:48:43,475 main.py:47] epoch 408, training loss: 7652.64, average training loss: 9368.98, base loss: 8819.21
[INFO 2017-06-26 17:48:43,838 main.py:47] epoch 409, training loss: 7095.48, average training loss: 9363.44, base loss: 8818.53
[INFO 2017-06-26 17:48:44,207 main.py:47] epoch 410, training loss: 8150.18, average training loss: 9360.49, base loss: 8820.68
[INFO 2017-06-26 17:48:44,572 main.py:47] epoch 411, training loss: 6717.73, average training loss: 9354.07, base loss: 8817.76
[INFO 2017-06-26 17:48:44,933 main.py:47] epoch 412, training loss: 7852.33, average training loss: 9350.44, base loss: 8818.77
[INFO 2017-06-26 17:48:45,295 main.py:47] epoch 413, training loss: 8065.69, average training loss: 9347.33, base loss: 8820.01
[INFO 2017-06-26 17:48:45,660 main.py:47] epoch 414, training loss: 7374.68, average training loss: 9342.58, base loss: 8819.72
[INFO 2017-06-26 17:48:46,023 main.py:47] epoch 415, training loss: 7309.23, average training loss: 9337.69, base loss: 8819.38
[INFO 2017-06-26 17:48:46,386 main.py:47] epoch 416, training loss: 8127.68, average training loss: 9334.79, base loss: 8819.83
[INFO 2017-06-26 17:48:46,749 main.py:47] epoch 417, training loss: 8504.15, average training loss: 9332.80, base loss: 8822.09
[INFO 2017-06-26 17:48:47,112 main.py:47] epoch 418, training loss: 6635.19, average training loss: 9326.37, base loss: 8818.81
[INFO 2017-06-26 17:48:47,474 main.py:47] epoch 419, training loss: 7152.31, average training loss: 9321.19, base loss: 8817.96
[INFO 2017-06-26 17:48:47,835 main.py:47] epoch 420, training loss: 6839.07, average training loss: 9315.29, base loss: 8816.51
[INFO 2017-06-26 17:48:48,197 main.py:47] epoch 421, training loss: 7916.73, average training loss: 9311.98, base loss: 8817.37
[INFO 2017-06-26 17:48:48,559 main.py:47] epoch 422, training loss: 7568.40, average training loss: 9307.86, base loss: 8818.62
[INFO 2017-06-26 17:48:48,922 main.py:47] epoch 423, training loss: 8111.70, average training loss: 9305.04, base loss: 8820.86
[INFO 2017-06-26 17:48:49,290 main.py:47] epoch 424, training loss: 8000.06, average training loss: 9301.97, base loss: 8821.63
[INFO 2017-06-26 17:48:49,652 main.py:47] epoch 425, training loss: 6596.84, average training loss: 9295.62, base loss: 8817.77
[INFO 2017-06-26 17:48:50,013 main.py:47] epoch 426, training loss: 7192.62, average training loss: 9290.69, base loss: 8815.84
[INFO 2017-06-26 17:48:50,374 main.py:47] epoch 427, training loss: 6677.62, average training loss: 9284.59, base loss: 8813.82
[INFO 2017-06-26 17:48:50,740 main.py:47] epoch 428, training loss: 7990.49, average training loss: 9281.57, base loss: 8815.41
[INFO 2017-06-26 17:48:51,102 main.py:47] epoch 429, training loss: 6898.69, average training loss: 9276.03, base loss: 8814.09
[INFO 2017-06-26 17:48:51,466 main.py:47] epoch 430, training loss: 6761.88, average training loss: 9270.19, base loss: 8811.41
[INFO 2017-06-26 17:48:51,831 main.py:47] epoch 431, training loss: 7596.27, average training loss: 9266.32, base loss: 8810.47
[INFO 2017-06-26 17:48:52,195 main.py:47] epoch 432, training loss: 7761.73, average training loss: 9262.84, base loss: 8811.27
[INFO 2017-06-26 17:48:52,563 main.py:47] epoch 433, training loss: 7575.85, average training loss: 9258.96, base loss: 8811.29
[INFO 2017-06-26 17:48:52,926 main.py:47] epoch 434, training loss: 8148.74, average training loss: 9256.40, base loss: 8813.51
[INFO 2017-06-26 17:48:53,289 main.py:47] epoch 435, training loss: 8194.44, average training loss: 9253.97, base loss: 8815.97
[INFO 2017-06-26 17:48:53,654 main.py:47] epoch 436, training loss: 7454.88, average training loss: 9249.85, base loss: 8815.89
[INFO 2017-06-26 17:48:54,018 main.py:47] epoch 437, training loss: 8087.64, average training loss: 9247.20, base loss: 8817.62
[INFO 2017-06-26 17:48:54,390 main.py:47] epoch 438, training loss: 8980.88, average training loss: 9246.59, base loss: 8822.21
[INFO 2017-06-26 17:48:54,757 main.py:47] epoch 439, training loss: 7348.58, average training loss: 9242.28, base loss: 8821.52
[INFO 2017-06-26 17:48:55,121 main.py:47] epoch 440, training loss: 7741.30, average training loss: 9238.88, base loss: 8821.99
[INFO 2017-06-26 17:48:55,488 main.py:47] epoch 441, training loss: 8115.76, average training loss: 9236.33, base loss: 8824.18
[INFO 2017-06-26 17:48:55,849 main.py:47] epoch 442, training loss: 6780.03, average training loss: 9230.79, base loss: 8821.79
[INFO 2017-06-26 17:48:56,210 main.py:47] epoch 443, training loss: 6992.89, average training loss: 9225.75, base loss: 8821.17
[INFO 2017-06-26 17:48:56,573 main.py:47] epoch 444, training loss: 7465.61, average training loss: 9221.79, base loss: 8820.98
[INFO 2017-06-26 17:48:56,937 main.py:47] epoch 445, training loss: 6983.33, average training loss: 9216.77, base loss: 8818.93
[INFO 2017-06-26 17:48:57,301 main.py:47] epoch 446, training loss: 7490.92, average training loss: 9212.91, base loss: 8818.81
[INFO 2017-06-26 17:48:57,665 main.py:47] epoch 447, training loss: 8596.12, average training loss: 9211.54, base loss: 8821.75
[INFO 2017-06-26 17:48:58,025 main.py:47] epoch 448, training loss: 6652.92, average training loss: 9205.84, base loss: 8818.26
[INFO 2017-06-26 17:48:58,390 main.py:47] epoch 449, training loss: 6798.64, average training loss: 9200.49, base loss: 8816.02
[INFO 2017-06-26 17:48:58,772 main.py:47] epoch 450, training loss: 7612.08, average training loss: 9196.97, base loss: 8816.32
[INFO 2017-06-26 17:48:59,138 main.py:47] epoch 451, training loss: 8062.82, average training loss: 9194.46, base loss: 8817.88
[INFO 2017-06-26 17:48:59,512 main.py:47] epoch 452, training loss: 7093.99, average training loss: 9189.82, base loss: 8816.52
[INFO 2017-06-26 17:48:59,880 main.py:47] epoch 453, training loss: 7897.05, average training loss: 9186.97, base loss: 8818.10
[INFO 2017-06-26 17:49:00,247 main.py:47] epoch 454, training loss: 7049.33, average training loss: 9182.28, base loss: 8816.80
[INFO 2017-06-26 17:49:00,629 main.py:47] epoch 455, training loss: 7262.81, average training loss: 9178.07, base loss: 8816.01
[INFO 2017-06-26 17:49:00,993 main.py:47] epoch 456, training loss: 7076.68, average training loss: 9173.47, base loss: 8815.00
[INFO 2017-06-26 17:49:01,363 main.py:47] epoch 457, training loss: 6722.75, average training loss: 9168.12, base loss: 8813.31
[INFO 2017-06-26 17:49:01,726 main.py:47] epoch 458, training loss: 7330.04, average training loss: 9164.11, base loss: 8812.85
[INFO 2017-06-26 17:49:02,089 main.py:47] epoch 459, training loss: 9236.12, average training loss: 9164.27, base loss: 8817.35
[INFO 2017-06-26 17:49:02,450 main.py:47] epoch 460, training loss: 8049.99, average training loss: 9161.85, base loss: 8819.29
[INFO 2017-06-26 17:49:02,816 main.py:47] epoch 461, training loss: 6516.11, average training loss: 9156.13, base loss: 8815.92
[INFO 2017-06-26 17:49:03,184 main.py:47] epoch 462, training loss: 7204.15, average training loss: 9151.91, base loss: 8814.78
[INFO 2017-06-26 17:49:03,551 main.py:47] epoch 463, training loss: 7310.14, average training loss: 9147.94, base loss: 8813.86
[INFO 2017-06-26 17:49:03,915 main.py:47] epoch 464, training loss: 7437.59, average training loss: 9144.26, base loss: 8813.97
[INFO 2017-06-26 17:49:04,278 main.py:47] epoch 465, training loss: 7116.24, average training loss: 9139.91, base loss: 8813.37
[INFO 2017-06-26 17:49:04,643 main.py:47] epoch 466, training loss: 8747.98, average training loss: 9139.07, base loss: 8816.59
[INFO 2017-06-26 17:49:05,006 main.py:47] epoch 467, training loss: 7401.71, average training loss: 9135.36, base loss: 8817.04
[INFO 2017-06-26 17:49:05,368 main.py:47] epoch 468, training loss: 7656.37, average training loss: 9132.20, base loss: 8817.32
[INFO 2017-06-26 17:49:05,731 main.py:47] epoch 469, training loss: 7393.40, average training loss: 9128.51, base loss: 8817.54
[INFO 2017-06-26 17:49:06,095 main.py:47] epoch 470, training loss: 8151.31, average training loss: 9126.43, base loss: 8819.38
[INFO 2017-06-26 17:49:06,458 main.py:47] epoch 471, training loss: 7343.87, average training loss: 9122.65, base loss: 8818.58
[INFO 2017-06-26 17:49:06,820 main.py:47] epoch 472, training loss: 7092.10, average training loss: 9118.36, base loss: 8817.25
[INFO 2017-06-26 17:49:07,185 main.py:47] epoch 473, training loss: 8268.31, average training loss: 9116.57, base loss: 8819.89
[INFO 2017-06-26 17:49:07,551 main.py:47] epoch 474, training loss: 6925.63, average training loss: 9111.96, base loss: 8819.22
[INFO 2017-06-26 17:49:07,911 main.py:47] epoch 475, training loss: 7651.31, average training loss: 9108.89, base loss: 8819.69
[INFO 2017-06-26 17:49:08,277 main.py:47] epoch 476, training loss: 8108.07, average training loss: 9106.79, base loss: 8821.95
[INFO 2017-06-26 17:49:08,638 main.py:47] epoch 477, training loss: 7438.17, average training loss: 9103.30, base loss: 8821.80
[INFO 2017-06-26 17:49:09,002 main.py:47] epoch 478, training loss: 7261.43, average training loss: 9099.45, base loss: 8821.65
[INFO 2017-06-26 17:49:09,383 main.py:47] epoch 479, training loss: 6229.62, average training loss: 9093.47, base loss: 8817.76
[INFO 2017-06-26 17:49:09,749 main.py:47] epoch 480, training loss: 6687.01, average training loss: 9088.47, base loss: 8816.11
[INFO 2017-06-26 17:49:10,114 main.py:47] epoch 481, training loss: 7389.45, average training loss: 9084.95, base loss: 8816.17
[INFO 2017-06-26 17:49:10,479 main.py:47] epoch 482, training loss: 6905.93, average training loss: 9080.43, base loss: 8814.53
[INFO 2017-06-26 17:49:10,843 main.py:47] epoch 483, training loss: 7458.48, average training loss: 9077.08, base loss: 8814.63
[INFO 2017-06-26 17:49:11,207 main.py:47] epoch 484, training loss: 8184.34, average training loss: 9075.24, base loss: 8816.34
[INFO 2017-06-26 17:49:11,569 main.py:47] epoch 485, training loss: 8009.64, average training loss: 9073.05, base loss: 8817.93
[INFO 2017-06-26 17:49:11,934 main.py:47] epoch 486, training loss: 7204.57, average training loss: 9069.21, base loss: 8817.42
[INFO 2017-06-26 17:49:12,297 main.py:47] epoch 487, training loss: 6546.23, average training loss: 9064.04, base loss: 8814.87
[INFO 2017-06-26 17:49:12,662 main.py:47] epoch 488, training loss: 8188.29, average training loss: 9062.25, base loss: 8816.98
[INFO 2017-06-26 17:49:13,027 main.py:47] epoch 489, training loss: 8555.58, average training loss: 9061.22, base loss: 8819.71
[INFO 2017-06-26 17:49:13,390 main.py:47] epoch 490, training loss: 7190.70, average training loss: 9057.41, base loss: 8819.56
[INFO 2017-06-26 17:49:13,753 main.py:47] epoch 491, training loss: 7518.13, average training loss: 9054.28, base loss: 8819.00
[INFO 2017-06-26 17:49:14,138 main.py:47] epoch 492, training loss: 7621.29, average training loss: 9051.37, base loss: 8820.09
[INFO 2017-06-26 17:49:14,532 main.py:47] epoch 493, training loss: 7635.89, average training loss: 9048.51, base loss: 8820.39
[INFO 2017-06-26 17:49:14,911 main.py:47] epoch 494, training loss: 7066.61, average training loss: 9044.50, base loss: 8819.72
[INFO 2017-06-26 17:49:15,304 main.py:47] epoch 495, training loss: 7869.91, average training loss: 9042.14, base loss: 8821.05
[INFO 2017-06-26 17:49:15,733 main.py:47] epoch 496, training loss: 7355.62, average training loss: 9038.74, base loss: 8821.09
[INFO 2017-06-26 17:49:16,121 main.py:47] epoch 497, training loss: 7039.96, average training loss: 9034.73, base loss: 8819.76
[INFO 2017-06-26 17:49:16,527 main.py:47] epoch 498, training loss: 8300.59, average training loss: 9033.26, base loss: 8821.64
[INFO 2017-06-26 17:49:16,908 main.py:47] epoch 499, training loss: 7746.85, average training loss: 9030.68, base loss: 8822.47
[INFO 2017-06-26 17:49:16,908 main.py:49] epoch 499, testing
[INFO 2017-06-26 17:49:21,133 main.py:100] average testing loss: 7087.62, base loss: 8421.28
[INFO 2017-06-26 17:49:21,155 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:49:21,162 main.py:73] current best accuracy: 7087.62
[INFO 2017-06-26 17:49:21,525 main.py:47] epoch 500, training loss: 7834.76, average training loss: 9028.30, base loss: 8822.93
[INFO 2017-06-26 17:49:21,890 main.py:47] epoch 501, training loss: 6600.04, average training loss: 9023.46, base loss: 8821.14
[INFO 2017-06-26 17:49:22,262 main.py:47] epoch 502, training loss: 7536.31, average training loss: 9020.50, base loss: 8821.17
[INFO 2017-06-26 17:49:22,633 main.py:47] epoch 503, training loss: 7498.85, average training loss: 9017.48, base loss: 8821.94
[INFO 2017-06-26 17:49:23,006 main.py:47] epoch 504, training loss: 8115.23, average training loss: 9015.70, base loss: 8823.76
[INFO 2017-06-26 17:49:23,408 main.py:47] epoch 505, training loss: 8489.18, average training loss: 9014.66, base loss: 8826.40
[INFO 2017-06-26 17:49:23,778 main.py:47] epoch 506, training loss: 7250.77, average training loss: 9011.18, base loss: 8826.69
[INFO 2017-06-26 17:49:24,142 main.py:47] epoch 507, training loss: 7777.77, average training loss: 9008.75, base loss: 8828.60
[INFO 2017-06-26 17:49:24,523 main.py:47] epoch 508, training loss: 7492.38, average training loss: 9005.77, base loss: 8827.75
[INFO 2017-06-26 17:49:24,885 main.py:47] epoch 509, training loss: 7016.47, average training loss: 9001.87, base loss: 8826.51
[INFO 2017-06-26 17:49:25,246 main.py:47] epoch 510, training loss: 7988.50, average training loss: 8999.89, base loss: 8828.00
[INFO 2017-06-26 17:49:25,645 main.py:47] epoch 511, training loss: 7673.42, average training loss: 8997.30, base loss: 8828.84
[INFO 2017-06-26 17:49:26,083 main.py:47] epoch 512, training loss: 7299.50, average training loss: 8993.99, base loss: 8828.12
[INFO 2017-06-26 17:49:26,468 main.py:47] epoch 513, training loss: 7743.50, average training loss: 8991.55, base loss: 8828.01
[INFO 2017-06-26 17:49:26,833 main.py:47] epoch 514, training loss: 6978.33, average training loss: 8987.65, base loss: 8826.43
[INFO 2017-06-26 17:49:27,199 main.py:47] epoch 515, training loss: 6016.45, average training loss: 8981.89, base loss: 8822.92
[INFO 2017-06-26 17:49:27,570 main.py:47] epoch 516, training loss: 7785.60, average training loss: 8979.57, base loss: 8823.83
[INFO 2017-06-26 17:49:27,932 main.py:47] epoch 517, training loss: 7064.37, average training loss: 8975.88, base loss: 8823.10
[INFO 2017-06-26 17:49:28,300 main.py:47] epoch 518, training loss: 6353.28, average training loss: 8970.82, base loss: 8820.35
[INFO 2017-06-26 17:49:28,713 main.py:47] epoch 519, training loss: 6976.15, average training loss: 8966.99, base loss: 8819.54
[INFO 2017-06-26 17:49:29,139 main.py:47] epoch 520, training loss: 8154.41, average training loss: 8965.43, base loss: 8822.02
[INFO 2017-06-26 17:49:29,515 main.py:47] epoch 521, training loss: 7416.89, average training loss: 8962.46, base loss: 8822.17
[INFO 2017-06-26 17:49:29,918 main.py:47] epoch 522, training loss: 7746.30, average training loss: 8960.14, base loss: 8823.09
[INFO 2017-06-26 17:49:30,284 main.py:47] epoch 523, training loss: 7500.73, average training loss: 8957.35, base loss: 8823.53
[INFO 2017-06-26 17:49:30,689 main.py:47] epoch 524, training loss: 7616.48, average training loss: 8954.80, base loss: 8823.46
[INFO 2017-06-26 17:49:31,083 main.py:47] epoch 525, training loss: 7587.02, average training loss: 8952.20, base loss: 8824.06
[INFO 2017-06-26 17:49:31,455 main.py:47] epoch 526, training loss: 7253.29, average training loss: 8948.97, base loss: 8823.06
[INFO 2017-06-26 17:49:31,824 main.py:47] epoch 527, training loss: 7506.85, average training loss: 8946.24, base loss: 8823.58
[INFO 2017-06-26 17:49:32,189 main.py:47] epoch 528, training loss: 6703.16, average training loss: 8942.00, base loss: 8821.90
[INFO 2017-06-26 17:49:32,551 main.py:47] epoch 529, training loss: 7500.19, average training loss: 8939.28, base loss: 8821.73
[INFO 2017-06-26 17:49:32,918 main.py:47] epoch 530, training loss: 7950.00, average training loss: 8937.42, base loss: 8823.22
[INFO 2017-06-26 17:49:33,346 main.py:47] epoch 531, training loss: 6925.68, average training loss: 8933.64, base loss: 8821.66
[INFO 2017-06-26 17:49:33,713 main.py:47] epoch 532, training loss: 7522.28, average training loss: 8930.99, base loss: 8822.34
[INFO 2017-06-26 17:49:34,104 main.py:47] epoch 533, training loss: 6973.73, average training loss: 8927.32, base loss: 8821.21
[INFO 2017-06-26 17:49:34,468 main.py:47] epoch 534, training loss: 6491.71, average training loss: 8922.77, base loss: 8819.16
[INFO 2017-06-26 17:49:34,833 main.py:47] epoch 535, training loss: 7339.17, average training loss: 8919.82, base loss: 8819.13
[INFO 2017-06-26 17:49:35,193 main.py:47] epoch 536, training loss: 8345.68, average training loss: 8918.75, base loss: 8821.06
[INFO 2017-06-26 17:49:35,558 main.py:47] epoch 537, training loss: 8501.37, average training loss: 8917.97, base loss: 8823.78
[INFO 2017-06-26 17:49:35,922 main.py:47] epoch 538, training loss: 7384.07, average training loss: 8915.12, base loss: 8822.96
[INFO 2017-06-26 17:49:36,286 main.py:47] epoch 539, training loss: 6896.67, average training loss: 8911.39, base loss: 8821.60
[INFO 2017-06-26 17:49:36,648 main.py:47] epoch 540, training loss: 7376.61, average training loss: 8908.55, base loss: 8820.80
[INFO 2017-06-26 17:49:37,012 main.py:47] epoch 541, training loss: 6996.55, average training loss: 8905.02, base loss: 8819.71
[INFO 2017-06-26 17:49:37,373 main.py:47] epoch 542, training loss: 8132.30, average training loss: 8903.60, base loss: 8820.67
[INFO 2017-06-26 17:49:37,736 main.py:47] epoch 543, training loss: 7282.70, average training loss: 8900.62, base loss: 8820.93
[INFO 2017-06-26 17:49:38,098 main.py:47] epoch 544, training loss: 8057.12, average training loss: 8899.07, base loss: 8822.81
[INFO 2017-06-26 17:49:38,460 main.py:47] epoch 545, training loss: 8108.06, average training loss: 8897.62, base loss: 8824.26
[INFO 2017-06-26 17:49:38,821 main.py:47] epoch 546, training loss: 6640.88, average training loss: 8893.50, base loss: 8822.07
[INFO 2017-06-26 17:49:39,188 main.py:47] epoch 547, training loss: 7443.80, average training loss: 8890.85, base loss: 8822.76
[INFO 2017-06-26 17:49:39,549 main.py:47] epoch 548, training loss: 7406.45, average training loss: 8888.15, base loss: 8823.09
[INFO 2017-06-26 17:49:39,910 main.py:47] epoch 549, training loss: 6877.45, average training loss: 8884.49, base loss: 8821.27
[INFO 2017-06-26 17:49:40,272 main.py:47] epoch 550, training loss: 7386.21, average training loss: 8881.77, base loss: 8821.50
[INFO 2017-06-26 17:49:40,633 main.py:47] epoch 551, training loss: 7570.74, average training loss: 8879.40, base loss: 8821.21
[INFO 2017-06-26 17:49:40,995 main.py:47] epoch 552, training loss: 7591.02, average training loss: 8877.07, base loss: 8822.31
[INFO 2017-06-26 17:49:41,360 main.py:47] epoch 553, training loss: 7427.91, average training loss: 8874.45, base loss: 8822.25
[INFO 2017-06-26 17:49:41,723 main.py:47] epoch 554, training loss: 7789.08, average training loss: 8872.50, base loss: 8822.60
[INFO 2017-06-26 17:49:42,085 main.py:47] epoch 555, training loss: 7359.92, average training loss: 8869.78, base loss: 8822.43
[INFO 2017-06-26 17:49:42,446 main.py:47] epoch 556, training loss: 8398.43, average training loss: 8868.93, base loss: 8824.81
[INFO 2017-06-26 17:49:42,809 main.py:47] epoch 557, training loss: 7975.92, average training loss: 8867.33, base loss: 8826.34
[INFO 2017-06-26 17:49:43,172 main.py:47] epoch 558, training loss: 7702.60, average training loss: 8865.25, base loss: 8826.35
[INFO 2017-06-26 17:49:43,534 main.py:47] epoch 559, training loss: 6721.27, average training loss: 8861.42, base loss: 8824.86
[INFO 2017-06-26 17:49:43,896 main.py:47] epoch 560, training loss: 7469.01, average training loss: 8858.94, base loss: 8824.81
[INFO 2017-06-26 17:49:44,261 main.py:47] epoch 561, training loss: 7564.28, average training loss: 8856.63, base loss: 8824.90
[INFO 2017-06-26 17:49:44,626 main.py:47] epoch 562, training loss: 8264.93, average training loss: 8855.58, base loss: 8826.78
[INFO 2017-06-26 17:49:44,990 main.py:47] epoch 563, training loss: 7706.44, average training loss: 8853.54, base loss: 8827.88
[INFO 2017-06-26 17:49:45,451 main.py:47] epoch 564, training loss: 7232.90, average training loss: 8850.68, base loss: 8826.92
[INFO 2017-06-26 17:49:45,829 main.py:47] epoch 565, training loss: 7791.00, average training loss: 8848.80, base loss: 8827.30
[INFO 2017-06-26 17:49:46,194 main.py:47] epoch 566, training loss: 7022.49, average training loss: 8845.58, base loss: 8827.16
[INFO 2017-06-26 17:49:46,562 main.py:47] epoch 567, training loss: 7701.03, average training loss: 8843.57, base loss: 8827.84
[INFO 2017-06-26 17:49:46,931 main.py:47] epoch 568, training loss: 7386.82, average training loss: 8841.01, base loss: 8828.38
[INFO 2017-06-26 17:49:47,294 main.py:47] epoch 569, training loss: 6733.20, average training loss: 8837.31, base loss: 8826.35
[INFO 2017-06-26 17:49:47,660 main.py:47] epoch 570, training loss: 7371.77, average training loss: 8834.74, base loss: 8826.37
[INFO 2017-06-26 17:49:48,025 main.py:47] epoch 571, training loss: 7287.88, average training loss: 8832.04, base loss: 8825.22
[INFO 2017-06-26 17:49:48,386 main.py:47] epoch 572, training loss: 6963.76, average training loss: 8828.78, base loss: 8823.64
[INFO 2017-06-26 17:49:48,746 main.py:47] epoch 573, training loss: 7615.96, average training loss: 8826.66, base loss: 8823.96
[INFO 2017-06-26 17:49:49,108 main.py:47] epoch 574, training loss: 6244.38, average training loss: 8822.17, base loss: 8822.08
[INFO 2017-06-26 17:49:49,477 main.py:47] epoch 575, training loss: 7740.40, average training loss: 8820.30, base loss: 8822.00
[INFO 2017-06-26 17:49:49,839 main.py:47] epoch 576, training loss: 7596.33, average training loss: 8818.17, base loss: 8821.88
[INFO 2017-06-26 17:49:50,209 main.py:47] epoch 577, training loss: 7936.12, average training loss: 8816.65, base loss: 8822.73
[INFO 2017-06-26 17:49:50,581 main.py:47] epoch 578, training loss: 6762.76, average training loss: 8813.10, base loss: 8820.85
[INFO 2017-06-26 17:49:50,946 main.py:47] epoch 579, training loss: 7005.82, average training loss: 8809.98, base loss: 8820.05
[INFO 2017-06-26 17:49:51,317 main.py:47] epoch 580, training loss: 7543.55, average training loss: 8807.81, base loss: 8820.49
[INFO 2017-06-26 17:49:51,683 main.py:47] epoch 581, training loss: 7052.50, average training loss: 8804.79, base loss: 8820.23
[INFO 2017-06-26 17:49:52,048 main.py:47] epoch 582, training loss: 8316.48, average training loss: 8803.95, base loss: 8822.06
[INFO 2017-06-26 17:49:52,416 main.py:47] epoch 583, training loss: 7775.68, average training loss: 8802.19, base loss: 8822.83
[INFO 2017-06-26 17:49:52,781 main.py:47] epoch 584, training loss: 7322.91, average training loss: 8799.66, base loss: 8822.26
[INFO 2017-06-26 17:49:53,146 main.py:47] epoch 585, training loss: 7344.32, average training loss: 8797.18, base loss: 8822.41
[INFO 2017-06-26 17:49:53,513 main.py:47] epoch 586, training loss: 6615.24, average training loss: 8793.46, base loss: 8820.31
[INFO 2017-06-26 17:49:53,879 main.py:47] epoch 587, training loss: 9148.80, average training loss: 8794.07, base loss: 8824.18
[INFO 2017-06-26 17:49:54,245 main.py:47] epoch 588, training loss: 7668.14, average training loss: 8792.15, base loss: 8824.41
[INFO 2017-06-26 17:49:54,609 main.py:47] epoch 589, training loss: 7582.25, average training loss: 8790.10, base loss: 8824.18
[INFO 2017-06-26 17:49:54,976 main.py:47] epoch 590, training loss: 8074.48, average training loss: 8788.89, base loss: 8826.24
[INFO 2017-06-26 17:49:55,343 main.py:47] epoch 591, training loss: 7298.02, average training loss: 8786.37, base loss: 8826.71
[INFO 2017-06-26 17:49:55,708 main.py:47] epoch 592, training loss: 7525.45, average training loss: 8784.25, base loss: 8826.66
[INFO 2017-06-26 17:49:56,077 main.py:47] epoch 593, training loss: 7463.52, average training loss: 8782.02, base loss: 8827.31
[INFO 2017-06-26 17:49:56,442 main.py:47] epoch 594, training loss: 7987.00, average training loss: 8780.69, base loss: 8828.20
[INFO 2017-06-26 17:49:56,807 main.py:47] epoch 595, training loss: 7316.29, average training loss: 8778.23, base loss: 8828.00
[INFO 2017-06-26 17:49:57,172 main.py:47] epoch 596, training loss: 6658.52, average training loss: 8774.68, base loss: 8826.20
[INFO 2017-06-26 17:49:57,538 main.py:47] epoch 597, training loss: 6163.17, average training loss: 8770.31, base loss: 8823.15
[INFO 2017-06-26 17:49:57,904 main.py:47] epoch 598, training loss: 7415.39, average training loss: 8768.05, base loss: 8822.19
[INFO 2017-06-26 17:49:58,269 main.py:47] epoch 599, training loss: 7924.73, average training loss: 8766.65, base loss: 8823.16
[INFO 2017-06-26 17:49:58,269 main.py:49] epoch 599, testing
[INFO 2017-06-26 17:50:02,435 main.py:100] average testing loss: 7130.29, base loss: 8447.83
[INFO 2017-06-26 17:50:02,459 main.py:73] current best accuracy: 7087.62
[INFO 2017-06-26 17:50:02,821 main.py:47] epoch 600, training loss: 7712.93, average training loss: 8764.89, base loss: 8823.69
[INFO 2017-06-26 17:50:03,187 main.py:47] epoch 601, training loss: 6985.24, average training loss: 8761.94, base loss: 8822.39
[INFO 2017-06-26 17:50:03,550 main.py:47] epoch 602, training loss: 7550.23, average training loss: 8759.93, base loss: 8823.32
[INFO 2017-06-26 17:50:03,913 main.py:47] epoch 603, training loss: 6233.47, average training loss: 8755.74, base loss: 8821.41
[INFO 2017-06-26 17:50:04,275 main.py:47] epoch 604, training loss: 7780.57, average training loss: 8754.13, base loss: 8822.22
[INFO 2017-06-26 17:50:04,635 main.py:47] epoch 605, training loss: 7786.03, average training loss: 8752.53, base loss: 8823.05
[INFO 2017-06-26 17:50:04,999 main.py:47] epoch 606, training loss: 7393.32, average training loss: 8750.30, base loss: 8822.76
[INFO 2017-06-26 17:50:05,366 main.py:47] epoch 607, training loss: 7704.20, average training loss: 8748.58, base loss: 8823.67
[INFO 2017-06-26 17:50:05,731 main.py:47] epoch 608, training loss: 8130.02, average training loss: 8747.56, base loss: 8825.21
[INFO 2017-06-26 17:50:06,097 main.py:47] epoch 609, training loss: 7806.56, average training loss: 8746.02, base loss: 8826.18
[INFO 2017-06-26 17:50:06,463 main.py:47] epoch 610, training loss: 7015.48, average training loss: 8743.18, base loss: 8825.92
[INFO 2017-06-26 17:50:06,830 main.py:47] epoch 611, training loss: 7434.99, average training loss: 8741.05, base loss: 8826.79
[INFO 2017-06-26 17:50:07,193 main.py:47] epoch 612, training loss: 6689.01, average training loss: 8737.70, base loss: 8825.84
[INFO 2017-06-26 17:50:07,562 main.py:47] epoch 613, training loss: 7282.62, average training loss: 8735.33, base loss: 8826.05
[INFO 2017-06-26 17:50:07,926 main.py:47] epoch 614, training loss: 6080.31, average training loss: 8731.01, base loss: 8823.13
[INFO 2017-06-26 17:50:08,294 main.py:47] epoch 615, training loss: 6760.96, average training loss: 8727.81, base loss: 8821.32
[INFO 2017-06-26 17:50:08,659 main.py:47] epoch 616, training loss: 7104.56, average training loss: 8725.18, base loss: 8820.68
[INFO 2017-06-26 17:50:09,021 main.py:47] epoch 617, training loss: 7041.71, average training loss: 8722.46, base loss: 8819.87
[INFO 2017-06-26 17:50:09,385 main.py:47] epoch 618, training loss: 7756.36, average training loss: 8720.90, base loss: 8820.47
[INFO 2017-06-26 17:50:09,747 main.py:47] epoch 619, training loss: 7472.94, average training loss: 8718.89, base loss: 8820.05
[INFO 2017-06-26 17:50:10,108 main.py:47] epoch 620, training loss: 7799.65, average training loss: 8717.41, base loss: 8821.75
[INFO 2017-06-26 17:50:10,472 main.py:47] epoch 621, training loss: 7604.72, average training loss: 8715.62, base loss: 8822.53
[INFO 2017-06-26 17:50:10,835 main.py:47] epoch 622, training loss: 6896.87, average training loss: 8712.70, base loss: 8821.76
[INFO 2017-06-26 17:50:11,199 main.py:47] epoch 623, training loss: 8796.92, average training loss: 8712.83, base loss: 8824.65
[INFO 2017-06-26 17:50:11,562 main.py:47] epoch 624, training loss: 7048.96, average training loss: 8710.17, base loss: 8823.93
[INFO 2017-06-26 17:50:11,924 main.py:47] epoch 625, training loss: 7100.74, average training loss: 8707.60, base loss: 8823.43
[INFO 2017-06-26 17:50:12,287 main.py:47] epoch 626, training loss: 7172.50, average training loss: 8705.15, base loss: 8823.61
[INFO 2017-06-26 17:50:12,648 main.py:47] epoch 627, training loss: 7540.93, average training loss: 8703.30, base loss: 8823.98
[INFO 2017-06-26 17:50:13,011 main.py:47] epoch 628, training loss: 8285.17, average training loss: 8702.63, base loss: 8826.32
[INFO 2017-06-26 17:50:13,374 main.py:47] epoch 629, training loss: 7614.05, average training loss: 8700.90, base loss: 8827.34
[INFO 2017-06-26 17:50:13,739 main.py:47] epoch 630, training loss: 8011.40, average training loss: 8699.81, base loss: 8829.38
[INFO 2017-06-26 17:50:14,101 main.py:47] epoch 631, training loss: 6651.25, average training loss: 8696.57, base loss: 8827.80
[INFO 2017-06-26 17:50:14,466 main.py:47] epoch 632, training loss: 6964.06, average training loss: 8693.83, base loss: 8826.30
[INFO 2017-06-26 17:50:14,830 main.py:47] epoch 633, training loss: 7908.05, average training loss: 8692.59, base loss: 8827.74
[INFO 2017-06-26 17:50:15,191 main.py:47] epoch 634, training loss: 7359.14, average training loss: 8690.49, base loss: 8827.98
[INFO 2017-06-26 17:50:15,554 main.py:47] epoch 635, training loss: 7455.88, average training loss: 8688.55, base loss: 8828.26
[INFO 2017-06-26 17:50:15,920 main.py:47] epoch 636, training loss: 7313.78, average training loss: 8686.39, base loss: 8828.55
[INFO 2017-06-26 17:50:16,282 main.py:47] epoch 637, training loss: 7593.21, average training loss: 8684.68, base loss: 8829.66
[INFO 2017-06-26 17:50:16,646 main.py:47] epoch 638, training loss: 7043.56, average training loss: 8682.11, base loss: 8829.33
[INFO 2017-06-26 17:50:17,010 main.py:47] epoch 639, training loss: 7857.19, average training loss: 8680.82, base loss: 8831.12
[INFO 2017-06-26 17:50:17,372 main.py:47] epoch 640, training loss: 7897.34, average training loss: 8679.60, base loss: 8832.44
[INFO 2017-06-26 17:50:17,732 main.py:47] epoch 641, training loss: 8414.70, average training loss: 8679.19, base loss: 8835.20
[INFO 2017-06-26 17:50:18,096 main.py:47] epoch 642, training loss: 7257.55, average training loss: 8676.98, base loss: 8833.63
[INFO 2017-06-26 17:50:18,458 main.py:47] epoch 643, training loss: 6901.30, average training loss: 8674.22, base loss: 8832.49
[INFO 2017-06-26 17:50:18,821 main.py:47] epoch 644, training loss: 6631.75, average training loss: 8671.05, base loss: 8830.14
[INFO 2017-06-26 17:50:19,185 main.py:47] epoch 645, training loss: 7341.50, average training loss: 8669.00, base loss: 8830.53
[INFO 2017-06-26 17:50:19,548 main.py:47] epoch 646, training loss: 7684.34, average training loss: 8667.47, base loss: 8831.12
[INFO 2017-06-26 17:50:19,910 main.py:47] epoch 647, training loss: 7171.12, average training loss: 8665.16, base loss: 8830.75
[INFO 2017-06-26 17:50:20,327 main.py:47] epoch 648, training loss: 6906.89, average training loss: 8662.46, base loss: 8829.97
[INFO 2017-06-26 17:50:20,697 main.py:47] epoch 649, training loss: 6676.29, average training loss: 8659.40, base loss: 8829.03
[INFO 2017-06-26 17:50:21,064 main.py:47] epoch 650, training loss: 7385.05, average training loss: 8657.44, base loss: 8828.87
[INFO 2017-06-26 17:50:21,429 main.py:47] epoch 651, training loss: 7427.18, average training loss: 8655.56, base loss: 8829.15
[INFO 2017-06-26 17:50:21,792 main.py:47] epoch 652, training loss: 6798.95, average training loss: 8652.71, base loss: 8828.28
[INFO 2017-06-26 17:50:22,154 main.py:47] epoch 653, training loss: 6963.71, average training loss: 8650.13, base loss: 8828.35
[INFO 2017-06-26 17:50:22,516 main.py:47] epoch 654, training loss: 8174.91, average training loss: 8649.40, base loss: 8829.96
[INFO 2017-06-26 17:50:22,878 main.py:47] epoch 655, training loss: 6979.41, average training loss: 8646.86, base loss: 8829.19
[INFO 2017-06-26 17:50:23,240 main.py:47] epoch 656, training loss: 7370.57, average training loss: 8644.92, base loss: 8828.92
[INFO 2017-06-26 17:50:23,603 main.py:47] epoch 657, training loss: 7882.45, average training loss: 8643.76, base loss: 8829.87
[INFO 2017-06-26 17:50:23,967 main.py:47] epoch 658, training loss: 7194.89, average training loss: 8641.56, base loss: 8829.69
[INFO 2017-06-26 17:50:24,332 main.py:47] epoch 659, training loss: 6399.84, average training loss: 8638.16, base loss: 8827.69
[INFO 2017-06-26 17:50:24,695 main.py:47] epoch 660, training loss: 6970.50, average training loss: 8635.64, base loss: 8827.39
[INFO 2017-06-26 17:50:25,061 main.py:47] epoch 661, training loss: 7928.33, average training loss: 8634.57, base loss: 8827.97
[INFO 2017-06-26 17:50:25,426 main.py:47] epoch 662, training loss: 7825.99, average training loss: 8633.35, base loss: 8829.06
[INFO 2017-06-26 17:50:25,789 main.py:47] epoch 663, training loss: 6796.98, average training loss: 8630.59, base loss: 8828.38
[INFO 2017-06-26 17:50:26,154 main.py:47] epoch 664, training loss: 6646.28, average training loss: 8627.60, base loss: 8826.28
[INFO 2017-06-26 17:50:26,518 main.py:47] epoch 665, training loss: 7848.00, average training loss: 8626.43, base loss: 8827.30
[INFO 2017-06-26 17:50:26,880 main.py:47] epoch 666, training loss: 7183.15, average training loss: 8624.27, base loss: 8826.98
[INFO 2017-06-26 17:50:27,242 main.py:47] epoch 667, training loss: 7042.53, average training loss: 8621.90, base loss: 8826.94
[INFO 2017-06-26 17:50:27,604 main.py:47] epoch 668, training loss: 6962.23, average training loss: 8619.42, base loss: 8826.79
[INFO 2017-06-26 17:50:27,965 main.py:47] epoch 669, training loss: 6918.63, average training loss: 8616.88, base loss: 8825.99
[INFO 2017-06-26 17:50:28,327 main.py:47] epoch 670, training loss: 7469.59, average training loss: 8615.17, base loss: 8826.49
[INFO 2017-06-26 17:50:28,699 main.py:47] epoch 671, training loss: 7049.40, average training loss: 8612.84, base loss: 8825.86
[INFO 2017-06-26 17:50:29,061 main.py:47] epoch 672, training loss: 6614.73, average training loss: 8609.87, base loss: 8824.52
[INFO 2017-06-26 17:50:29,427 main.py:47] epoch 673, training loss: 7513.67, average training loss: 8608.24, base loss: 8824.00
[INFO 2017-06-26 17:50:29,793 main.py:47] epoch 674, training loss: 8040.32, average training loss: 8607.40, base loss: 8825.08
[INFO 2017-06-26 17:50:30,156 main.py:47] epoch 675, training loss: 7534.38, average training loss: 8605.82, base loss: 8825.88
[INFO 2017-06-26 17:50:30,517 main.py:47] epoch 676, training loss: 7571.77, average training loss: 8604.29, base loss: 8826.93
[INFO 2017-06-26 17:50:30,879 main.py:47] epoch 677, training loss: 6762.26, average training loss: 8601.57, base loss: 8825.58
[INFO 2017-06-26 17:50:31,241 main.py:47] epoch 678, training loss: 7338.32, average training loss: 8599.71, base loss: 8825.83
[INFO 2017-06-26 17:50:31,605 main.py:47] epoch 679, training loss: 6868.97, average training loss: 8597.17, base loss: 8825.50
[INFO 2017-06-26 17:50:31,968 main.py:47] epoch 680, training loss: 7033.54, average training loss: 8594.87, base loss: 8824.55
[INFO 2017-06-26 17:50:32,329 main.py:47] epoch 681, training loss: 6517.21, average training loss: 8591.82, base loss: 8822.30
[INFO 2017-06-26 17:50:32,691 main.py:47] epoch 682, training loss: 7728.61, average training loss: 8590.56, base loss: 8823.18
[INFO 2017-06-26 17:50:33,053 main.py:47] epoch 683, training loss: 7692.95, average training loss: 8589.25, base loss: 8823.43
[INFO 2017-06-26 17:50:33,414 main.py:47] epoch 684, training loss: 8530.63, average training loss: 8589.16, base loss: 8825.55
[INFO 2017-06-26 17:50:33,777 main.py:47] epoch 685, training loss: 6840.66, average training loss: 8586.61, base loss: 8825.56
[INFO 2017-06-26 17:50:34,143 main.py:47] epoch 686, training loss: 7592.32, average training loss: 8585.17, base loss: 8826.17
[INFO 2017-06-26 17:50:34,505 main.py:47] epoch 687, training loss: 6551.62, average training loss: 8582.21, base loss: 8824.61
[INFO 2017-06-26 17:50:34,868 main.py:47] epoch 688, training loss: 7348.01, average training loss: 8580.42, base loss: 8824.44
[INFO 2017-06-26 17:50:35,231 main.py:47] epoch 689, training loss: 7905.80, average training loss: 8579.44, base loss: 8825.73
[INFO 2017-06-26 17:50:35,592 main.py:47] epoch 690, training loss: 6982.23, average training loss: 8577.13, base loss: 8825.02
[INFO 2017-06-26 17:50:35,954 main.py:47] epoch 691, training loss: 7752.32, average training loss: 8575.94, base loss: 8825.40
[INFO 2017-06-26 17:50:36,315 main.py:47] epoch 692, training loss: 7713.54, average training loss: 8574.69, base loss: 8826.05
[INFO 2017-06-26 17:50:36,677 main.py:47] epoch 693, training loss: 8445.87, average training loss: 8574.51, base loss: 8827.75
[INFO 2017-06-26 17:50:37,040 main.py:47] epoch 694, training loss: 6633.66, average training loss: 8571.71, base loss: 8826.91
[INFO 2017-06-26 17:50:37,400 main.py:47] epoch 695, training loss: 7303.09, average training loss: 8569.89, base loss: 8827.27
[INFO 2017-06-26 17:50:37,765 main.py:47] epoch 696, training loss: 6449.14, average training loss: 8566.85, base loss: 8825.11
[INFO 2017-06-26 17:50:38,130 main.py:47] epoch 697, training loss: 7576.75, average training loss: 8565.43, base loss: 8825.63
[INFO 2017-06-26 17:50:38,494 main.py:47] epoch 698, training loss: 6686.15, average training loss: 8562.74, base loss: 8824.46
[INFO 2017-06-26 17:50:38,928 main.py:47] epoch 699, training loss: 8210.49, average training loss: 8562.24, base loss: 8826.12
[INFO 2017-06-26 17:50:38,929 main.py:49] epoch 699, testing
[INFO 2017-06-26 17:50:43,106 main.py:100] average testing loss: 7122.29, base loss: 8493.91
[INFO 2017-06-26 17:50:43,130 main.py:73] current best accuracy: 7087.62
[INFO 2017-06-26 17:50:43,493 main.py:47] epoch 700, training loss: 7377.83, average training loss: 8560.55, base loss: 8826.09
[INFO 2017-06-26 17:50:43,868 main.py:47] epoch 701, training loss: 7078.69, average training loss: 8558.44, base loss: 8825.61
[INFO 2017-06-26 17:50:44,235 main.py:47] epoch 702, training loss: 6363.81, average training loss: 8555.32, base loss: 8823.64
[INFO 2017-06-26 17:50:44,602 main.py:47] epoch 703, training loss: 7556.64, average training loss: 8553.90, base loss: 8823.47
[INFO 2017-06-26 17:50:44,966 main.py:47] epoch 704, training loss: 6795.71, average training loss: 8551.40, base loss: 8822.80
[INFO 2017-06-26 17:50:45,332 main.py:47] epoch 705, training loss: 6656.59, average training loss: 8548.72, base loss: 8821.55
[INFO 2017-06-26 17:50:45,695 main.py:47] epoch 706, training loss: 7578.77, average training loss: 8547.35, base loss: 8822.41
[INFO 2017-06-26 17:50:46,054 main.py:47] epoch 707, training loss: 6690.68, average training loss: 8544.73, base loss: 8821.69
[INFO 2017-06-26 17:50:46,417 main.py:47] epoch 708, training loss: 7214.92, average training loss: 8542.85, base loss: 8821.32
[INFO 2017-06-26 17:50:46,782 main.py:47] epoch 709, training loss: 7047.35, average training loss: 8540.74, base loss: 8820.39
[INFO 2017-06-26 17:50:47,149 main.py:47] epoch 710, training loss: 6884.91, average training loss: 8538.42, base loss: 8820.26
[INFO 2017-06-26 17:50:47,514 main.py:47] epoch 711, training loss: 7212.48, average training loss: 8536.55, base loss: 8820.11
[INFO 2017-06-26 17:50:47,878 main.py:47] epoch 712, training loss: 6892.57, average training loss: 8534.25, base loss: 8818.79
[INFO 2017-06-26 17:50:48,242 main.py:47] epoch 713, training loss: 6663.47, average training loss: 8531.63, base loss: 8817.51
[INFO 2017-06-26 17:50:48,606 main.py:47] epoch 714, training loss: 7210.97, average training loss: 8529.78, base loss: 8817.41
[INFO 2017-06-26 17:50:48,975 main.py:47] epoch 715, training loss: 7727.26, average training loss: 8528.66, base loss: 8818.33
[INFO 2017-06-26 17:50:49,336 main.py:47] epoch 716, training loss: 7617.21, average training loss: 8527.39, base loss: 8819.09
[INFO 2017-06-26 17:50:49,698 main.py:47] epoch 717, training loss: 7084.33, average training loss: 8525.38, base loss: 8818.88
[INFO 2017-06-26 17:50:50,060 main.py:47] epoch 718, training loss: 6815.99, average training loss: 8523.00, base loss: 8817.99
[INFO 2017-06-26 17:50:50,423 main.py:47] epoch 719, training loss: 7711.66, average training loss: 8521.87, base loss: 8818.69
[INFO 2017-06-26 17:50:50,786 main.py:47] epoch 720, training loss: 7844.74, average training loss: 8520.93, base loss: 8818.83
[INFO 2017-06-26 17:50:51,147 main.py:47] epoch 721, training loss: 7476.26, average training loss: 8519.49, base loss: 8818.93
[INFO 2017-06-26 17:50:51,508 main.py:47] epoch 722, training loss: 7033.03, average training loss: 8517.43, base loss: 8818.88
[INFO 2017-06-26 17:50:51,870 main.py:47] epoch 723, training loss: 6633.00, average training loss: 8514.83, base loss: 8818.06
[INFO 2017-06-26 17:50:52,231 main.py:47] epoch 724, training loss: 7054.37, average training loss: 8512.81, base loss: 8817.59
[INFO 2017-06-26 17:50:52,595 main.py:47] epoch 725, training loss: 7246.06, average training loss: 8511.07, base loss: 8817.78
[INFO 2017-06-26 17:50:52,958 main.py:47] epoch 726, training loss: 8145.65, average training loss: 8510.57, base loss: 8819.55
[INFO 2017-06-26 17:50:53,320 main.py:47] epoch 727, training loss: 7535.10, average training loss: 8509.23, base loss: 8820.26
[INFO 2017-06-26 17:50:53,681 main.py:47] epoch 728, training loss: 7032.60, average training loss: 8507.20, base loss: 8820.18
[INFO 2017-06-26 17:50:54,055 main.py:47] epoch 729, training loss: 6896.44, average training loss: 8505.00, base loss: 8819.45
[INFO 2017-06-26 17:50:54,420 main.py:47] epoch 730, training loss: 6951.68, average training loss: 8502.87, base loss: 8818.77
[INFO 2017-06-26 17:50:54,791 main.py:47] epoch 731, training loss: 6661.20, average training loss: 8500.35, base loss: 8817.83
[INFO 2017-06-26 17:50:55,156 main.py:47] epoch 732, training loss: 7065.25, average training loss: 8498.40, base loss: 8816.76
[INFO 2017-06-26 17:50:55,523 main.py:47] epoch 733, training loss: 6905.40, average training loss: 8496.23, base loss: 8816.22
[INFO 2017-06-26 17:50:55,894 main.py:47] epoch 734, training loss: 6892.32, average training loss: 8494.04, base loss: 8815.94
[INFO 2017-06-26 17:50:56,259 main.py:47] epoch 735, training loss: 7453.90, average training loss: 8492.63, base loss: 8817.10
[INFO 2017-06-26 17:50:56,629 main.py:47] epoch 736, training loss: 7366.35, average training loss: 8491.10, base loss: 8817.27
[INFO 2017-06-26 17:50:56,992 main.py:47] epoch 737, training loss: 7361.79, average training loss: 8489.57, base loss: 8817.44
[INFO 2017-06-26 17:50:57,354 main.py:47] epoch 738, training loss: 7258.91, average training loss: 8487.91, base loss: 8817.60
[INFO 2017-06-26 17:50:57,715 main.py:47] epoch 739, training loss: 7581.24, average training loss: 8486.68, base loss: 8818.63
[INFO 2017-06-26 17:50:58,078 main.py:47] epoch 740, training loss: 7342.51, average training loss: 8485.14, base loss: 8818.94
[INFO 2017-06-26 17:50:58,441 main.py:47] epoch 741, training loss: 6696.26, average training loss: 8482.73, base loss: 8818.20
[INFO 2017-06-26 17:50:58,804 main.py:47] epoch 742, training loss: 6770.19, average training loss: 8480.42, base loss: 8816.93
[INFO 2017-06-26 17:50:59,176 main.py:47] epoch 743, training loss: 7428.96, average training loss: 8479.01, base loss: 8816.69
[INFO 2017-06-26 17:50:59,539 main.py:47] epoch 744, training loss: 7293.40, average training loss: 8477.42, base loss: 8817.23
[INFO 2017-06-26 17:50:59,902 main.py:47] epoch 745, training loss: 7552.43, average training loss: 8476.18, base loss: 8817.46
[INFO 2017-06-26 17:51:00,265 main.py:47] epoch 746, training loss: 7415.91, average training loss: 8474.76, base loss: 8817.84
[INFO 2017-06-26 17:51:00,629 main.py:47] epoch 747, training loss: 7628.80, average training loss: 8473.63, base loss: 8818.54
[INFO 2017-06-26 17:51:00,994 main.py:47] epoch 748, training loss: 7802.05, average training loss: 8472.73, base loss: 8819.49
[INFO 2017-06-26 17:51:01,360 main.py:47] epoch 749, training loss: 7858.36, average training loss: 8471.91, base loss: 8820.42
[INFO 2017-06-26 17:51:01,722 main.py:47] epoch 750, training loss: 6277.44, average training loss: 8468.99, base loss: 8818.93
[INFO 2017-06-26 17:51:02,083 main.py:47] epoch 751, training loss: 6794.74, average training loss: 8466.76, base loss: 8818.11
[INFO 2017-06-26 17:51:02,446 main.py:47] epoch 752, training loss: 7008.71, average training loss: 8464.83, base loss: 8818.24
[INFO 2017-06-26 17:51:02,809 main.py:47] epoch 753, training loss: 7096.72, average training loss: 8463.01, base loss: 8817.70
[INFO 2017-06-26 17:51:03,172 main.py:47] epoch 754, training loss: 8579.44, average training loss: 8463.17, base loss: 8820.13
[INFO 2017-06-26 17:51:03,534 main.py:47] epoch 755, training loss: 7549.97, average training loss: 8461.96, base loss: 8820.51
[INFO 2017-06-26 17:51:03,894 main.py:47] epoch 756, training loss: 7411.40, average training loss: 8460.57, base loss: 8820.27
[INFO 2017-06-26 17:51:04,257 main.py:47] epoch 757, training loss: 7198.09, average training loss: 8458.91, base loss: 8820.33
[INFO 2017-06-26 17:51:04,617 main.py:47] epoch 758, training loss: 7165.00, average training loss: 8457.20, base loss: 8820.33
[INFO 2017-06-26 17:51:04,980 main.py:47] epoch 759, training loss: 7927.98, average training loss: 8456.50, base loss: 8821.48
[INFO 2017-06-26 17:51:05,343 main.py:47] epoch 760, training loss: 6376.86, average training loss: 8453.77, base loss: 8820.03
[INFO 2017-06-26 17:51:05,706 main.py:47] epoch 761, training loss: 6387.53, average training loss: 8451.06, base loss: 8817.80
[INFO 2017-06-26 17:51:06,068 main.py:47] epoch 762, training loss: 6597.96, average training loss: 8448.63, base loss: 8816.37
[INFO 2017-06-26 17:51:06,430 main.py:47] epoch 763, training loss: 8177.63, average training loss: 8448.28, base loss: 8818.07
[INFO 2017-06-26 17:51:06,793 main.py:47] epoch 764, training loss: 7461.19, average training loss: 8446.99, base loss: 8818.48
[INFO 2017-06-26 17:51:07,155 main.py:47] epoch 765, training loss: 7137.70, average training loss: 8445.28, base loss: 8818.35
[INFO 2017-06-26 17:51:07,517 main.py:47] epoch 766, training loss: 7515.40, average training loss: 8444.06, base loss: 8818.50
[INFO 2017-06-26 17:51:07,879 main.py:47] epoch 767, training loss: 6712.23, average training loss: 8441.81, base loss: 8817.59
[INFO 2017-06-26 17:51:08,242 main.py:47] epoch 768, training loss: 7946.03, average training loss: 8441.16, base loss: 8818.45
[INFO 2017-06-26 17:51:08,604 main.py:47] epoch 769, training loss: 6868.39, average training loss: 8439.12, base loss: 8817.42
[INFO 2017-06-26 17:51:08,965 main.py:47] epoch 770, training loss: 6245.35, average training loss: 8436.28, base loss: 8815.73
[INFO 2017-06-26 17:51:09,338 main.py:47] epoch 771, training loss: 7774.37, average training loss: 8435.42, base loss: 8816.69
[INFO 2017-06-26 17:51:09,705 main.py:47] epoch 772, training loss: 6143.71, average training loss: 8432.45, base loss: 8814.72
[INFO 2017-06-26 17:51:10,073 main.py:47] epoch 773, training loss: 7275.07, average training loss: 8430.96, base loss: 8815.15
[INFO 2017-06-26 17:51:10,440 main.py:47] epoch 774, training loss: 7420.70, average training loss: 8429.66, base loss: 8814.82
[INFO 2017-06-26 17:51:10,806 main.py:47] epoch 775, training loss: 7374.71, average training loss: 8428.30, base loss: 8815.19
[INFO 2017-06-26 17:51:11,173 main.py:47] epoch 776, training loss: 7268.88, average training loss: 8426.80, base loss: 8815.68
[INFO 2017-06-26 17:51:11,539 main.py:47] epoch 777, training loss: 6945.33, average training loss: 8424.90, base loss: 8814.89
[INFO 2017-06-26 17:51:11,905 main.py:47] epoch 778, training loss: 7467.95, average training loss: 8423.67, base loss: 8815.38
[INFO 2017-06-26 17:51:12,270 main.py:47] epoch 779, training loss: 7446.88, average training loss: 8422.42, base loss: 8815.98
[INFO 2017-06-26 17:51:12,636 main.py:47] epoch 780, training loss: 6736.79, average training loss: 8420.26, base loss: 8815.08
[INFO 2017-06-26 17:51:13,002 main.py:47] epoch 781, training loss: 7358.66, average training loss: 8418.90, base loss: 8815.66
[INFO 2017-06-26 17:51:13,366 main.py:47] epoch 782, training loss: 7230.38, average training loss: 8417.39, base loss: 8815.42
[INFO 2017-06-26 17:51:13,731 main.py:47] epoch 783, training loss: 6185.59, average training loss: 8414.54, base loss: 8814.01
[INFO 2017-06-26 17:51:14,094 main.py:47] epoch 784, training loss: 6322.97, average training loss: 8411.87, base loss: 8812.37
[INFO 2017-06-26 17:51:14,458 main.py:47] epoch 785, training loss: 6550.06, average training loss: 8409.51, base loss: 8811.69
[INFO 2017-06-26 17:51:14,821 main.py:47] epoch 786, training loss: 6959.71, average training loss: 8407.66, base loss: 8811.55
[INFO 2017-06-26 17:51:15,183 main.py:47] epoch 787, training loss: 7279.54, average training loss: 8406.23, base loss: 8811.04
[INFO 2017-06-26 17:51:15,547 main.py:47] epoch 788, training loss: 6856.90, average training loss: 8404.27, base loss: 8810.20
[INFO 2017-06-26 17:51:15,910 main.py:47] epoch 789, training loss: 7534.22, average training loss: 8403.17, base loss: 8810.62
[INFO 2017-06-26 17:51:16,270 main.py:47] epoch 790, training loss: 7746.31, average training loss: 8402.34, base loss: 8811.33
[INFO 2017-06-26 17:51:16,632 main.py:47] epoch 791, training loss: 7037.30, average training loss: 8400.61, base loss: 8811.29
[INFO 2017-06-26 17:51:16,992 main.py:47] epoch 792, training loss: 6179.91, average training loss: 8397.81, base loss: 8809.60
[INFO 2017-06-26 17:51:17,353 main.py:47] epoch 793, training loss: 7605.52, average training loss: 8396.81, base loss: 8810.29
[INFO 2017-06-26 17:51:17,716 main.py:47] epoch 794, training loss: 8146.45, average training loss: 8396.50, base loss: 8811.86
[INFO 2017-06-26 17:51:18,080 main.py:47] epoch 795, training loss: 6507.66, average training loss: 8394.13, base loss: 8810.72
[INFO 2017-06-26 17:51:18,442 main.py:47] epoch 796, training loss: 6660.93, average training loss: 8391.95, base loss: 8809.73
[INFO 2017-06-26 17:51:18,806 main.py:47] epoch 797, training loss: 7228.12, average training loss: 8390.49, base loss: 8809.19
[INFO 2017-06-26 17:51:19,167 main.py:47] epoch 798, training loss: 8362.56, average training loss: 8390.46, base loss: 8811.32
[INFO 2017-06-26 17:51:19,536 main.py:47] epoch 799, training loss: 8382.19, average training loss: 8390.45, base loss: 8813.45
[INFO 2017-06-26 17:51:19,536 main.py:49] epoch 799, testing
[INFO 2017-06-26 17:51:23,709 main.py:100] average testing loss: 7016.53, base loss: 8665.70
[INFO 2017-06-26 17:51:23,735 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:51:23,741 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:51:24,104 main.py:47] epoch 800, training loss: 7715.68, average training loss: 8389.61, base loss: 8814.55
[INFO 2017-06-26 17:51:24,469 main.py:47] epoch 801, training loss: 7027.09, average training loss: 8387.91, base loss: 8814.62
[INFO 2017-06-26 17:51:24,830 main.py:47] epoch 802, training loss: 7577.43, average training loss: 8386.90, base loss: 8814.51
[INFO 2017-06-26 17:51:25,196 main.py:47] epoch 803, training loss: 7606.11, average training loss: 8385.93, base loss: 8814.90
[INFO 2017-06-26 17:51:25,559 main.py:47] epoch 804, training loss: 6625.99, average training loss: 8383.74, base loss: 8813.80
[INFO 2017-06-26 17:51:25,924 main.py:47] epoch 805, training loss: 8419.95, average training loss: 8383.79, base loss: 8815.42
[INFO 2017-06-26 17:51:26,287 main.py:47] epoch 806, training loss: 6764.72, average training loss: 8381.78, base loss: 8814.68
[INFO 2017-06-26 17:51:26,689 main.py:47] epoch 807, training loss: 6286.12, average training loss: 8379.19, base loss: 8813.44
[INFO 2017-06-26 17:51:27,051 main.py:47] epoch 808, training loss: 7236.13, average training loss: 8377.77, base loss: 8813.32
[INFO 2017-06-26 17:51:27,412 main.py:47] epoch 809, training loss: 7496.71, average training loss: 8376.68, base loss: 8813.87
[INFO 2017-06-26 17:51:27,775 main.py:47] epoch 810, training loss: 8205.45, average training loss: 8376.47, base loss: 8815.56
[INFO 2017-06-26 17:51:28,136 main.py:47] epoch 811, training loss: 7977.25, average training loss: 8375.98, base loss: 8816.98
[INFO 2017-06-26 17:51:28,499 main.py:47] epoch 812, training loss: 8345.96, average training loss: 8375.94, base loss: 8818.77
[INFO 2017-06-26 17:51:28,861 main.py:47] epoch 813, training loss: 8052.71, average training loss: 8375.55, base loss: 8819.65
[INFO 2017-06-26 17:51:29,223 main.py:47] epoch 814, training loss: 6837.44, average training loss: 8373.66, base loss: 8818.94
[INFO 2017-06-26 17:51:29,585 main.py:47] epoch 815, training loss: 6998.00, average training loss: 8371.97, base loss: 8819.51
[INFO 2017-06-26 17:51:29,951 main.py:47] epoch 816, training loss: 7458.69, average training loss: 8370.86, base loss: 8820.43
[INFO 2017-06-26 17:51:30,315 main.py:47] epoch 817, training loss: 7157.62, average training loss: 8369.37, base loss: 8820.46
[INFO 2017-06-26 17:51:30,680 main.py:47] epoch 818, training loss: 6463.80, average training loss: 8367.05, base loss: 8819.52
[INFO 2017-06-26 17:51:31,044 main.py:47] epoch 819, training loss: 7815.76, average training loss: 8366.37, base loss: 8820.55
[INFO 2017-06-26 17:51:31,408 main.py:47] epoch 820, training loss: 7524.85, average training loss: 8365.35, base loss: 8821.36
[INFO 2017-06-26 17:51:31,775 main.py:47] epoch 821, training loss: 7192.06, average training loss: 8363.92, base loss: 8821.91
[INFO 2017-06-26 17:51:32,137 main.py:47] epoch 822, training loss: 7303.52, average training loss: 8362.63, base loss: 8821.89
[INFO 2017-06-26 17:51:32,500 main.py:47] epoch 823, training loss: 6517.81, average training loss: 8360.39, base loss: 8820.73
[INFO 2017-06-26 17:51:32,860 main.py:47] epoch 824, training loss: 6125.03, average training loss: 8357.69, base loss: 8819.09
[INFO 2017-06-26 17:51:33,224 main.py:47] epoch 825, training loss: 6976.63, average training loss: 8356.01, base loss: 8819.35
[INFO 2017-06-26 17:51:33,584 main.py:47] epoch 826, training loss: 8779.63, average training loss: 8356.53, base loss: 8821.98
[INFO 2017-06-26 17:51:33,947 main.py:47] epoch 827, training loss: 7966.43, average training loss: 8356.05, base loss: 8822.86
[INFO 2017-06-26 17:51:34,311 main.py:47] epoch 828, training loss: 7633.90, average training loss: 8355.18, base loss: 8823.85
[INFO 2017-06-26 17:51:34,675 main.py:47] epoch 829, training loss: 7147.65, average training loss: 8353.73, base loss: 8823.73
[INFO 2017-06-26 17:51:35,046 main.py:47] epoch 830, training loss: 7354.32, average training loss: 8352.53, base loss: 8824.32
[INFO 2017-06-26 17:51:35,408 main.py:47] epoch 831, training loss: 7569.20, average training loss: 8351.58, base loss: 8825.06
[INFO 2017-06-26 17:51:35,770 main.py:47] epoch 832, training loss: 7131.81, average training loss: 8350.12, base loss: 8825.39
[INFO 2017-06-26 17:51:36,134 main.py:47] epoch 833, training loss: 7657.79, average training loss: 8349.29, base loss: 8825.88
[INFO 2017-06-26 17:51:36,494 main.py:47] epoch 834, training loss: 6862.37, average training loss: 8347.51, base loss: 8825.62
[INFO 2017-06-26 17:51:36,856 main.py:47] epoch 835, training loss: 7065.64, average training loss: 8345.98, base loss: 8825.02
[INFO 2017-06-26 17:51:37,219 main.py:47] epoch 836, training loss: 7723.44, average training loss: 8345.23, base loss: 8826.32
[INFO 2017-06-26 17:51:37,581 main.py:47] epoch 837, training loss: 7285.63, average training loss: 8343.97, base loss: 8826.84
[INFO 2017-06-26 17:51:37,943 main.py:47] epoch 838, training loss: 6383.59, average training loss: 8341.63, base loss: 8826.01
[INFO 2017-06-26 17:51:38,306 main.py:47] epoch 839, training loss: 6638.20, average training loss: 8339.60, base loss: 8825.18
[INFO 2017-06-26 17:51:38,668 main.py:47] epoch 840, training loss: 7617.97, average training loss: 8338.75, base loss: 8826.35
[INFO 2017-06-26 17:51:39,029 main.py:47] epoch 841, training loss: 8771.96, average training loss: 8339.26, base loss: 8828.66
[INFO 2017-06-26 17:51:39,392 main.py:47] epoch 842, training loss: 7224.73, average training loss: 8337.94, base loss: 8828.31
[INFO 2017-06-26 17:51:39,754 main.py:47] epoch 843, training loss: 7046.65, average training loss: 8336.41, base loss: 8828.59
[INFO 2017-06-26 17:51:40,120 main.py:47] epoch 844, training loss: 6611.15, average training loss: 8334.37, base loss: 8827.84
[INFO 2017-06-26 17:51:40,483 main.py:47] epoch 845, training loss: 6439.48, average training loss: 8332.13, base loss: 8826.90
[INFO 2017-06-26 17:51:40,843 main.py:47] epoch 846, training loss: 6853.76, average training loss: 8330.38, base loss: 8826.14
[INFO 2017-06-26 17:51:41,207 main.py:47] epoch 847, training loss: 7334.13, average training loss: 8329.21, base loss: 8826.36
[INFO 2017-06-26 17:51:41,569 main.py:47] epoch 848, training loss: 6496.42, average training loss: 8327.05, base loss: 8824.44
[INFO 2017-06-26 17:51:41,931 main.py:47] epoch 849, training loss: 8548.43, average training loss: 8327.31, base loss: 8826.53
[INFO 2017-06-26 17:51:42,292 main.py:47] epoch 850, training loss: 7579.88, average training loss: 8326.43, base loss: 8827.29
[INFO 2017-06-26 17:51:42,654 main.py:47] epoch 851, training loss: 7912.94, average training loss: 8325.94, base loss: 8828.35
[INFO 2017-06-26 17:51:43,015 main.py:47] epoch 852, training loss: 7642.88, average training loss: 8325.14, base loss: 8829.12
[INFO 2017-06-26 17:51:43,379 main.py:47] epoch 853, training loss: 7101.65, average training loss: 8323.71, base loss: 8829.11
[INFO 2017-06-26 17:51:43,741 main.py:47] epoch 854, training loss: 6849.71, average training loss: 8321.99, base loss: 8828.24
[INFO 2017-06-26 17:51:44,101 main.py:47] epoch 855, training loss: 7433.84, average training loss: 8320.95, base loss: 8828.51
[INFO 2017-06-26 17:51:44,462 main.py:47] epoch 856, training loss: 7209.51, average training loss: 8319.65, base loss: 8828.72
[INFO 2017-06-26 17:51:44,823 main.py:47] epoch 857, training loss: 7565.05, average training loss: 8318.77, base loss: 8829.11
[INFO 2017-06-26 17:51:45,186 main.py:47] epoch 858, training loss: 7048.30, average training loss: 8317.29, base loss: 8828.99
[INFO 2017-06-26 17:51:45,549 main.py:47] epoch 859, training loss: 7249.32, average training loss: 8316.05, base loss: 8828.94
[INFO 2017-06-26 17:51:45,911 main.py:47] epoch 860, training loss: 8453.10, average training loss: 8316.21, base loss: 8831.19
[INFO 2017-06-26 17:51:46,274 main.py:47] epoch 861, training loss: 6646.23, average training loss: 8314.27, base loss: 8830.16
[INFO 2017-06-26 17:51:46,636 main.py:47] epoch 862, training loss: 6915.83, average training loss: 8312.65, base loss: 8829.58
[INFO 2017-06-26 17:51:47,001 main.py:47] epoch 863, training loss: 7927.57, average training loss: 8312.21, base loss: 8830.74
[INFO 2017-06-26 17:51:47,362 main.py:47] epoch 864, training loss: 7242.16, average training loss: 8310.97, base loss: 8831.12
[INFO 2017-06-26 17:51:47,725 main.py:47] epoch 865, training loss: 7848.53, average training loss: 8310.44, base loss: 8832.84
[INFO 2017-06-26 17:51:48,089 main.py:47] epoch 866, training loss: 7516.83, average training loss: 8309.52, base loss: 8833.67
[INFO 2017-06-26 17:51:48,451 main.py:47] epoch 867, training loss: 7410.31, average training loss: 8308.49, base loss: 8833.97
[INFO 2017-06-26 17:51:48,812 main.py:47] epoch 868, training loss: 7485.63, average training loss: 8307.54, base loss: 8834.42
[INFO 2017-06-26 17:51:49,174 main.py:47] epoch 869, training loss: 7035.67, average training loss: 8306.08, base loss: 8834.24
[INFO 2017-06-26 17:51:49,537 main.py:47] epoch 870, training loss: 6934.98, average training loss: 8304.50, base loss: 8834.20
[INFO 2017-06-26 17:51:49,901 main.py:47] epoch 871, training loss: 6360.44, average training loss: 8302.27, base loss: 8832.97
[INFO 2017-06-26 17:51:50,267 main.py:47] epoch 872, training loss: 7542.66, average training loss: 8301.40, base loss: 8833.26
[INFO 2017-06-26 17:51:50,631 main.py:47] epoch 873, training loss: 7371.09, average training loss: 8300.34, base loss: 8833.35
[INFO 2017-06-26 17:51:50,994 main.py:47] epoch 874, training loss: 6624.91, average training loss: 8298.42, base loss: 8832.68
[INFO 2017-06-26 17:51:51,357 main.py:47] epoch 875, training loss: 7359.46, average training loss: 8297.35, base loss: 8832.88
[INFO 2017-06-26 17:51:51,722 main.py:47] epoch 876, training loss: 8130.48, average training loss: 8297.16, base loss: 8834.05
[INFO 2017-06-26 17:51:52,085 main.py:47] epoch 877, training loss: 7078.81, average training loss: 8295.77, base loss: 8833.71
[INFO 2017-06-26 17:51:52,449 main.py:47] epoch 878, training loss: 7012.67, average training loss: 8294.31, base loss: 8832.89
[INFO 2017-06-26 17:51:52,814 main.py:47] epoch 879, training loss: 6805.67, average training loss: 8292.62, base loss: 8832.72
[INFO 2017-06-26 17:51:53,179 main.py:47] epoch 880, training loss: 6332.46, average training loss: 8290.40, base loss: 8831.36
[INFO 2017-06-26 17:51:53,543 main.py:47] epoch 881, training loss: 8638.88, average training loss: 8290.79, base loss: 8833.69
[INFO 2017-06-26 17:51:53,906 main.py:47] epoch 882, training loss: 7255.69, average training loss: 8289.62, base loss: 8833.98
[INFO 2017-06-26 17:51:54,269 main.py:47] epoch 883, training loss: 7088.38, average training loss: 8288.26, base loss: 8833.60
[INFO 2017-06-26 17:51:54,634 main.py:47] epoch 884, training loss: 7215.74, average training loss: 8287.05, base loss: 8833.16
[INFO 2017-06-26 17:51:54,999 main.py:47] epoch 885, training loss: 6537.00, average training loss: 8285.07, base loss: 8831.99
[INFO 2017-06-26 17:51:55,370 main.py:47] epoch 886, training loss: 7566.97, average training loss: 8284.26, base loss: 8832.50
[INFO 2017-06-26 17:51:55,741 main.py:47] epoch 887, training loss: 8527.03, average training loss: 8284.54, base loss: 8834.82
[INFO 2017-06-26 17:51:56,107 main.py:47] epoch 888, training loss: 8074.73, average training loss: 8284.30, base loss: 8836.02
[INFO 2017-06-26 17:51:56,478 main.py:47] epoch 889, training loss: 7179.24, average training loss: 8283.06, base loss: 8836.32
[INFO 2017-06-26 17:51:56,845 main.py:47] epoch 890, training loss: 6813.83, average training loss: 8281.41, base loss: 8835.81
[INFO 2017-06-26 17:51:57,212 main.py:47] epoch 891, training loss: 7073.51, average training loss: 8280.06, base loss: 8835.31
[INFO 2017-06-26 17:51:57,577 main.py:47] epoch 892, training loss: 7454.26, average training loss: 8279.13, base loss: 8835.24
[INFO 2017-06-26 17:51:57,943 main.py:47] epoch 893, training loss: 7965.43, average training loss: 8278.78, base loss: 8836.24
[INFO 2017-06-26 17:51:58,307 main.py:47] epoch 894, training loss: 7466.66, average training loss: 8277.87, base loss: 8836.62
[INFO 2017-06-26 17:51:58,674 main.py:47] epoch 895, training loss: 7121.66, average training loss: 8276.58, base loss: 8836.32
[INFO 2017-06-26 17:51:59,036 main.py:47] epoch 896, training loss: 8196.12, average training loss: 8276.49, base loss: 8838.11
[INFO 2017-06-26 17:51:59,398 main.py:47] epoch 897, training loss: 6763.66, average training loss: 8274.81, base loss: 8837.62
[INFO 2017-06-26 17:51:59,762 main.py:47] epoch 898, training loss: 6840.02, average training loss: 8273.21, base loss: 8837.30
[INFO 2017-06-26 17:52:00,122 main.py:47] epoch 899, training loss: 7362.24, average training loss: 8272.20, base loss: 8837.71
[INFO 2017-06-26 17:52:00,122 main.py:49] epoch 899, testing
[INFO 2017-06-26 17:52:04,352 main.py:100] average testing loss: 7124.09, base loss: 8870.65
[INFO 2017-06-26 17:52:04,376 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:52:04,735 main.py:47] epoch 900, training loss: 7278.07, average training loss: 8271.10, base loss: 8838.00
[INFO 2017-06-26 17:52:05,098 main.py:47] epoch 901, training loss: 6657.32, average training loss: 8269.31, base loss: 8837.40
[INFO 2017-06-26 17:52:05,464 main.py:47] epoch 902, training loss: 7080.09, average training loss: 8267.99, base loss: 8837.85
[INFO 2017-06-26 17:52:05,827 main.py:47] epoch 903, training loss: 7372.98, average training loss: 8267.00, base loss: 8838.15
[INFO 2017-06-26 17:52:06,189 main.py:47] epoch 904, training loss: 7401.76, average training loss: 8266.05, base loss: 8838.76
[INFO 2017-06-26 17:52:06,555 main.py:47] epoch 905, training loss: 5965.12, average training loss: 8263.51, base loss: 8836.75
[INFO 2017-06-26 17:52:06,919 main.py:47] epoch 906, training loss: 7208.36, average training loss: 8262.34, base loss: 8837.43
[INFO 2017-06-26 17:52:07,281 main.py:47] epoch 907, training loss: 7347.64, average training loss: 8261.34, base loss: 8838.26
[INFO 2017-06-26 17:52:07,646 main.py:47] epoch 908, training loss: 7918.60, average training loss: 8260.96, base loss: 8839.47
[INFO 2017-06-26 17:52:08,009 main.py:47] epoch 909, training loss: 6376.98, average training loss: 8258.89, base loss: 8838.50
[INFO 2017-06-26 17:52:08,375 main.py:47] epoch 910, training loss: 7340.78, average training loss: 8257.88, base loss: 8838.73
[INFO 2017-06-26 17:52:08,738 main.py:47] epoch 911, training loss: 8111.73, average training loss: 8257.72, base loss: 8840.53
[INFO 2017-06-26 17:52:09,103 main.py:47] epoch 912, training loss: 6884.88, average training loss: 8256.22, base loss: 8840.24
[INFO 2017-06-26 17:52:09,464 main.py:47] epoch 913, training loss: 7585.55, average training loss: 8255.48, base loss: 8840.82
[INFO 2017-06-26 17:52:09,824 main.py:47] epoch 914, training loss: 7524.35, average training loss: 8254.68, base loss: 8841.07
[INFO 2017-06-26 17:52:10,184 main.py:47] epoch 915, training loss: 7578.84, average training loss: 8253.95, base loss: 8842.15
[INFO 2017-06-26 17:52:10,558 main.py:47] epoch 916, training loss: 7495.86, average training loss: 8253.12, base loss: 8843.26
[INFO 2017-06-26 17:52:10,920 main.py:47] epoch 917, training loss: 7204.82, average training loss: 8251.98, base loss: 8843.16
[INFO 2017-06-26 17:52:11,288 main.py:47] epoch 918, training loss: 6772.95, average training loss: 8250.37, base loss: 8842.15
[INFO 2017-06-26 17:52:11,649 main.py:47] epoch 919, training loss: 7345.13, average training loss: 8249.38, base loss: 8842.33
[INFO 2017-06-26 17:52:12,012 main.py:47] epoch 920, training loss: 6993.75, average training loss: 8248.02, base loss: 8842.19
[INFO 2017-06-26 17:52:12,375 main.py:47] epoch 921, training loss: 6944.57, average training loss: 8246.61, base loss: 8841.19
[INFO 2017-06-26 17:52:12,739 main.py:47] epoch 922, training loss: 6681.12, average training loss: 8244.91, base loss: 8840.11
[INFO 2017-06-26 17:52:13,104 main.py:47] epoch 923, training loss: 7111.46, average training loss: 8243.68, base loss: 8839.74
[INFO 2017-06-26 17:52:13,469 main.py:47] epoch 924, training loss: 6622.96, average training loss: 8241.93, base loss: 8839.03
[INFO 2017-06-26 17:52:13,833 main.py:47] epoch 925, training loss: 6597.66, average training loss: 8240.16, base loss: 8837.61
[INFO 2017-06-26 17:52:14,198 main.py:47] epoch 926, training loss: 7400.06, average training loss: 8239.25, base loss: 8837.58
[INFO 2017-06-26 17:52:14,563 main.py:47] epoch 927, training loss: 7258.64, average training loss: 8238.19, base loss: 8837.72
[INFO 2017-06-26 17:52:14,928 main.py:47] epoch 928, training loss: 5943.41, average training loss: 8235.72, base loss: 8836.05
[INFO 2017-06-26 17:52:15,292 main.py:47] epoch 929, training loss: 7176.77, average training loss: 8234.58, base loss: 8836.32
[INFO 2017-06-26 17:52:15,660 main.py:47] epoch 930, training loss: 8149.51, average training loss: 8234.49, base loss: 8837.91
[INFO 2017-06-26 17:52:16,023 main.py:47] epoch 931, training loss: 6948.99, average training loss: 8233.11, base loss: 8837.52
[INFO 2017-06-26 17:52:16,388 main.py:47] epoch 932, training loss: 6917.19, average training loss: 8231.70, base loss: 8837.26
[INFO 2017-06-26 17:52:16,750 main.py:47] epoch 933, training loss: 7378.30, average training loss: 8230.79, base loss: 8837.65
[INFO 2017-06-26 17:52:17,114 main.py:47] epoch 934, training loss: 7492.35, average training loss: 8230.00, base loss: 8837.63
[INFO 2017-06-26 17:52:17,479 main.py:47] epoch 935, training loss: 8819.59, average training loss: 8230.63, base loss: 8840.23
[INFO 2017-06-26 17:52:17,844 main.py:47] epoch 936, training loss: 7333.66, average training loss: 8229.67, base loss: 8840.19
[INFO 2017-06-26 17:52:18,208 main.py:47] epoch 937, training loss: 6687.30, average training loss: 8228.03, base loss: 8839.60
[INFO 2017-06-26 17:52:18,572 main.py:47] epoch 938, training loss: 7114.27, average training loss: 8226.84, base loss: 8839.33
[INFO 2017-06-26 17:52:18,938 main.py:47] epoch 939, training loss: 7097.41, average training loss: 8225.64, base loss: 8839.32
[INFO 2017-06-26 17:52:19,301 main.py:47] epoch 940, training loss: 7302.42, average training loss: 8224.66, base loss: 8839.52
[INFO 2017-06-26 17:52:19,665 main.py:47] epoch 941, training loss: 8052.06, average training loss: 8224.48, base loss: 8840.93
[INFO 2017-06-26 17:52:20,026 main.py:47] epoch 942, training loss: 6392.49, average training loss: 8222.53, base loss: 8839.62
[INFO 2017-06-26 17:52:20,388 main.py:47] epoch 943, training loss: 6431.42, average training loss: 8220.64, base loss: 8838.62
[INFO 2017-06-26 17:52:20,755 main.py:47] epoch 944, training loss: 8002.52, average training loss: 8220.41, base loss: 8839.93
[INFO 2017-06-26 17:52:21,114 main.py:47] epoch 945, training loss: 7703.07, average training loss: 8219.86, base loss: 8841.01
[INFO 2017-06-26 17:52:21,475 main.py:47] epoch 946, training loss: 6919.73, average training loss: 8218.49, base loss: 8840.31
[INFO 2017-06-26 17:52:21,834 main.py:47] epoch 947, training loss: 7785.74, average training loss: 8218.03, base loss: 8841.37
[INFO 2017-06-26 17:52:22,196 main.py:47] epoch 948, training loss: 6476.45, average training loss: 8216.19, base loss: 8839.55
[INFO 2017-06-26 17:52:22,558 main.py:47] epoch 949, training loss: 6804.91, average training loss: 8214.71, base loss: 8839.00
[INFO 2017-06-26 17:52:22,923 main.py:47] epoch 950, training loss: 6564.39, average training loss: 8212.97, base loss: 8838.05
[INFO 2017-06-26 17:52:23,289 main.py:47] epoch 951, training loss: 7024.68, average training loss: 8211.72, base loss: 8837.81
[INFO 2017-06-26 17:52:23,654 main.py:47] epoch 952, training loss: 7305.27, average training loss: 8210.77, base loss: 8837.69
[INFO 2017-06-26 17:52:24,018 main.py:47] epoch 953, training loss: 7416.27, average training loss: 8209.94, base loss: 8837.66
[INFO 2017-06-26 17:52:24,383 main.py:47] epoch 954, training loss: 6976.22, average training loss: 8208.65, base loss: 8836.79
[INFO 2017-06-26 17:52:24,747 main.py:47] epoch 955, training loss: 8060.17, average training loss: 8208.49, base loss: 8838.34
[INFO 2017-06-26 17:52:25,111 main.py:47] epoch 956, training loss: 6570.92, average training loss: 8206.78, base loss: 8837.20
[INFO 2017-06-26 17:52:25,476 main.py:47] epoch 957, training loss: 7253.93, average training loss: 8205.79, base loss: 8837.78
[INFO 2017-06-26 17:52:25,847 main.py:47] epoch 958, training loss: 7043.74, average training loss: 8204.58, base loss: 8838.02
[INFO 2017-06-26 17:52:26,213 main.py:47] epoch 959, training loss: 7161.32, average training loss: 8203.49, base loss: 8838.37
[INFO 2017-06-26 17:52:26,578 main.py:47] epoch 960, training loss: 7446.60, average training loss: 8202.70, base loss: 8838.74
[INFO 2017-06-26 17:52:26,942 main.py:47] epoch 961, training loss: 6760.79, average training loss: 8201.20, base loss: 8838.06
[INFO 2017-06-26 17:52:27,309 main.py:47] epoch 962, training loss: 8066.39, average training loss: 8201.06, base loss: 8839.40
[INFO 2017-06-26 17:52:27,674 main.py:47] epoch 963, training loss: 6167.55, average training loss: 8198.95, base loss: 8838.32
[INFO 2017-06-26 17:52:28,038 main.py:47] epoch 964, training loss: 6741.67, average training loss: 8197.44, base loss: 8837.85
[INFO 2017-06-26 17:52:28,399 main.py:47] epoch 965, training loss: 6912.00, average training loss: 8196.11, base loss: 8837.41
[INFO 2017-06-26 17:52:28,766 main.py:47] epoch 966, training loss: 6810.63, average training loss: 8194.68, base loss: 8837.14
[INFO 2017-06-26 17:52:29,130 main.py:47] epoch 967, training loss: 6158.23, average training loss: 8192.58, base loss: 8835.68
[INFO 2017-06-26 17:52:29,495 main.py:47] epoch 968, training loss: 8237.41, average training loss: 8192.62, base loss: 8836.75
[INFO 2017-06-26 17:52:29,859 main.py:47] epoch 969, training loss: 8333.68, average training loss: 8192.77, base loss: 8837.80
[INFO 2017-06-26 17:52:30,220 main.py:47] epoch 970, training loss: 7130.88, average training loss: 8191.67, base loss: 8837.80
[INFO 2017-06-26 17:52:30,581 main.py:47] epoch 971, training loss: 7255.54, average training loss: 8190.71, base loss: 8838.10
[INFO 2017-06-26 17:52:30,947 main.py:47] epoch 972, training loss: 8059.46, average training loss: 8190.58, base loss: 8839.40
[INFO 2017-06-26 17:52:31,315 main.py:47] epoch 973, training loss: 6460.78, average training loss: 8188.80, base loss: 8838.80
[INFO 2017-06-26 17:52:31,681 main.py:47] epoch 974, training loss: 6998.58, average training loss: 8187.58, base loss: 8838.51
[INFO 2017-06-26 17:52:32,044 main.py:47] epoch 975, training loss: 7486.22, average training loss: 8186.86, base loss: 8839.04
[INFO 2017-06-26 17:52:32,404 main.py:47] epoch 976, training loss: 6857.69, average training loss: 8185.50, base loss: 8838.31
[INFO 2017-06-26 17:52:32,767 main.py:47] epoch 977, training loss: 8015.30, average training loss: 8185.33, base loss: 8839.80
[INFO 2017-06-26 17:52:33,129 main.py:47] epoch 978, training loss: 7595.27, average training loss: 8184.72, base loss: 8840.06
[INFO 2017-06-26 17:52:33,490 main.py:47] epoch 979, training loss: 7169.07, average training loss: 8183.69, base loss: 8840.13
[INFO 2017-06-26 17:52:33,849 main.py:47] epoch 980, training loss: 6408.66, average training loss: 8181.88, base loss: 8839.06
[INFO 2017-06-26 17:52:34,209 main.py:47] epoch 981, training loss: 6677.03, average training loss: 8180.35, base loss: 8838.42
[INFO 2017-06-26 17:52:34,572 main.py:47] epoch 982, training loss: 7034.88, average training loss: 8179.18, base loss: 8838.50
[INFO 2017-06-26 17:52:34,933 main.py:47] epoch 983, training loss: 7985.54, average training loss: 8178.98, base loss: 8839.43
[INFO 2017-06-26 17:52:35,293 main.py:47] epoch 984, training loss: 7364.24, average training loss: 8178.16, base loss: 8839.09
[INFO 2017-06-26 17:52:35,658 main.py:47] epoch 985, training loss: 7222.24, average training loss: 8177.19, base loss: 8839.05
[INFO 2017-06-26 17:52:36,030 main.py:47] epoch 986, training loss: 7867.64, average training loss: 8176.87, base loss: 8840.05
[INFO 2017-06-26 17:52:36,401 main.py:47] epoch 987, training loss: 7458.66, average training loss: 8176.15, base loss: 8840.15
[INFO 2017-06-26 17:52:36,772 main.py:47] epoch 988, training loss: 6107.09, average training loss: 8174.05, base loss: 8838.63
[INFO 2017-06-26 17:52:37,142 main.py:47] epoch 989, training loss: 6740.81, average training loss: 8172.61, base loss: 8837.62
[INFO 2017-06-26 17:52:37,513 main.py:47] epoch 990, training loss: 6816.81, average training loss: 8171.24, base loss: 8836.85
[INFO 2017-06-26 17:52:37,878 main.py:47] epoch 991, training loss: 7478.98, average training loss: 8170.54, base loss: 8837.45
[INFO 2017-06-26 17:52:38,242 main.py:47] epoch 992, training loss: 7376.45, average training loss: 8169.74, base loss: 8837.83
[INFO 2017-06-26 17:52:38,607 main.py:47] epoch 993, training loss: 7431.35, average training loss: 8169.00, base loss: 8838.08
[INFO 2017-06-26 17:52:38,971 main.py:47] epoch 994, training loss: 8138.13, average training loss: 8168.97, base loss: 8838.97
[INFO 2017-06-26 17:52:39,336 main.py:47] epoch 995, training loss: 7085.87, average training loss: 8167.88, base loss: 8839.23
[INFO 2017-06-26 17:52:39,700 main.py:47] epoch 996, training loss: 8378.63, average training loss: 8168.09, base loss: 8840.96
[INFO 2017-06-26 17:52:40,064 main.py:47] epoch 997, training loss: 7779.15, average training loss: 8167.70, base loss: 8842.37
[INFO 2017-06-26 17:52:40,427 main.py:47] epoch 998, training loss: 6756.02, average training loss: 8166.29, base loss: 8842.08
[INFO 2017-06-26 17:52:40,793 main.py:47] epoch 999, training loss: 6795.26, average training loss: 8164.92, base loss: 8841.28
[INFO 2017-06-26 17:52:40,793 main.py:49] epoch 999, testing
[INFO 2017-06-26 17:52:44,984 main.py:100] average testing loss: 7164.45, base loss: 8846.79
[INFO 2017-06-26 17:52:45,009 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:52:45,374 main.py:47] epoch 1000, training loss: 6875.28, average training loss: 8071.81, base loss: 8842.20
[INFO 2017-06-26 17:52:45,739 main.py:47] epoch 1001, training loss: 6187.79, average training loss: 7998.73, base loss: 8841.62
[INFO 2017-06-26 17:52:46,102 main.py:47] epoch 1002, training loss: 6523.39, average training loss: 7935.95, base loss: 8839.94
[INFO 2017-06-26 17:52:46,475 main.py:47] epoch 1003, training loss: 8212.29, average training loss: 7882.58, base loss: 8841.11
[INFO 2017-06-26 17:52:46,837 main.py:47] epoch 1004, training loss: 6282.58, average training loss: 7836.88, base loss: 8839.25
[INFO 2017-06-26 17:52:47,200 main.py:47] epoch 1005, training loss: 8158.30, average training loss: 7798.32, base loss: 8840.82
[INFO 2017-06-26 17:52:47,563 main.py:47] epoch 1006, training loss: 7533.78, average training loss: 7764.86, base loss: 8840.44
[INFO 2017-06-26 17:52:47,922 main.py:47] epoch 1007, training loss: 8105.72, average training loss: 7738.63, base loss: 8842.99
[INFO 2017-06-26 17:52:48,282 main.py:47] epoch 1008, training loss: 6798.86, average training loss: 7715.15, base loss: 8843.72
[INFO 2017-06-26 17:52:48,643 main.py:47] epoch 1009, training loss: 7109.50, average training loss: 7695.95, base loss: 8843.85
[INFO 2017-06-26 17:52:49,005 main.py:47] epoch 1010, training loss: 6203.30, average training loss: 7679.85, base loss: 8843.51
[INFO 2017-06-26 17:52:49,365 main.py:47] epoch 1011, training loss: 7088.64, average training loss: 7666.15, base loss: 8842.60
[INFO 2017-06-26 17:52:49,728 main.py:47] epoch 1012, training loss: 6798.61, average training loss: 7654.29, base loss: 8841.71
[INFO 2017-06-26 17:52:50,088 main.py:47] epoch 1013, training loss: 7263.32, average training loss: 7645.58, base loss: 8842.76
[INFO 2017-06-26 17:52:50,450 main.py:47] epoch 1014, training loss: 8071.22, average training loss: 7636.63, base loss: 8841.93
[INFO 2017-06-26 17:52:50,812 main.py:47] epoch 1015, training loss: 6491.63, average training loss: 7630.27, base loss: 8841.88
[INFO 2017-06-26 17:52:51,173 main.py:47] epoch 1016, training loss: 7061.88, average training loss: 7623.78, base loss: 8841.66
[INFO 2017-06-26 17:52:51,543 main.py:47] epoch 1017, training loss: 6695.93, average training loss: 7618.19, base loss: 8840.73
[INFO 2017-06-26 17:52:51,907 main.py:47] epoch 1018, training loss: 6523.25, average training loss: 7613.01, base loss: 8840.41
[INFO 2017-06-26 17:52:52,269 main.py:47] epoch 1019, training loss: 6661.03, average training loss: 7607.53, base loss: 8838.67
[INFO 2017-06-26 17:52:52,631 main.py:47] epoch 1020, training loss: 9536.85, average training loss: 7607.37, base loss: 8842.39
[INFO 2017-06-26 17:52:52,992 main.py:47] epoch 1021, training loss: 7210.59, average training loss: 7602.95, base loss: 8840.43
[INFO 2017-06-26 17:52:53,353 main.py:47] epoch 1022, training loss: 7442.36, average training loss: 7599.61, base loss: 8839.58
[INFO 2017-06-26 17:52:53,715 main.py:47] epoch 1023, training loss: 7045.87, average training loss: 7597.39, base loss: 8839.94
[INFO 2017-06-26 17:52:54,079 main.py:47] epoch 1024, training loss: 7615.17, average training loss: 7595.88, base loss: 8841.11
[INFO 2017-06-26 17:52:54,441 main.py:47] epoch 1025, training loss: 6681.62, average training loss: 7594.34, base loss: 8841.70
[INFO 2017-06-26 17:52:54,803 main.py:47] epoch 1026, training loss: 7799.79, average training loss: 7592.57, base loss: 8841.58
[INFO 2017-06-26 17:52:55,165 main.py:47] epoch 1027, training loss: 8402.97, average training loss: 7590.12, base loss: 8841.23
[INFO 2017-06-26 17:52:55,527 main.py:47] epoch 1028, training loss: 6859.90, average training loss: 7587.82, base loss: 8840.50
[INFO 2017-06-26 17:52:55,891 main.py:47] epoch 1029, training loss: 6718.51, average training loss: 7584.96, base loss: 8839.15
[INFO 2017-06-26 17:52:56,254 main.py:47] epoch 1030, training loss: 7715.57, average training loss: 7584.61, base loss: 8840.53
[INFO 2017-06-26 17:52:56,625 main.py:47] epoch 1031, training loss: 6550.07, average training loss: 7583.19, base loss: 8840.50
[INFO 2017-06-26 17:52:56,995 main.py:47] epoch 1032, training loss: 6829.00, average training loss: 7580.73, base loss: 8839.75
[INFO 2017-06-26 17:52:57,362 main.py:47] epoch 1033, training loss: 7063.20, average training loss: 7580.42, base loss: 8841.28
[INFO 2017-06-26 17:52:57,726 main.py:47] epoch 1034, training loss: 6705.73, average training loss: 7578.22, base loss: 8840.69
[INFO 2017-06-26 17:52:58,089 main.py:47] epoch 1035, training loss: 7528.90, average training loss: 7576.74, base loss: 8840.41
[INFO 2017-06-26 17:52:58,452 main.py:47] epoch 1036, training loss: 7552.74, average training loss: 7574.22, base loss: 8839.51
[INFO 2017-06-26 17:52:58,816 main.py:47] epoch 1037, training loss: 6808.77, average training loss: 7573.33, base loss: 8840.15
[INFO 2017-06-26 17:52:59,179 main.py:47] epoch 1038, training loss: 6667.07, average training loss: 7571.98, base loss: 8840.43
[INFO 2017-06-26 17:52:59,542 main.py:47] epoch 1039, training loss: 8297.55, average training loss: 7571.68, base loss: 8842.50
[INFO 2017-06-26 17:52:59,904 main.py:47] epoch 1040, training loss: 8174.72, average training loss: 7571.11, base loss: 8844.07
[INFO 2017-06-26 17:53:00,268 main.py:47] epoch 1041, training loss: 7109.30, average training loss: 7568.34, base loss: 8842.99
[INFO 2017-06-26 17:53:00,636 main.py:47] epoch 1042, training loss: 6448.19, average training loss: 7565.22, base loss: 8841.22
[INFO 2017-06-26 17:53:01,001 main.py:47] epoch 1043, training loss: 7197.84, average training loss: 7564.17, base loss: 8842.27
[INFO 2017-06-26 17:53:01,362 main.py:47] epoch 1044, training loss: 7150.53, average training loss: 7562.91, base loss: 8842.56
[INFO 2017-06-26 17:53:01,728 main.py:47] epoch 1045, training loss: 6438.26, average training loss: 7559.51, base loss: 8840.22
[INFO 2017-06-26 17:53:02,088 main.py:47] epoch 1046, training loss: 6196.62, average training loss: 7557.34, base loss: 8839.21
[INFO 2017-06-26 17:53:02,449 main.py:47] epoch 1047, training loss: 7777.01, average training loss: 7557.19, base loss: 8840.26
[INFO 2017-06-26 17:53:02,812 main.py:47] epoch 1048, training loss: 8298.18, average training loss: 7556.32, base loss: 8841.15
[INFO 2017-06-26 17:53:03,175 main.py:47] epoch 1049, training loss: 7839.52, average training loss: 7556.92, base loss: 8843.59
[INFO 2017-06-26 17:53:03,538 main.py:47] epoch 1050, training loss: 6773.50, average training loss: 7555.35, base loss: 8843.45
[INFO 2017-06-26 17:53:03,901 main.py:47] epoch 1051, training loss: 6749.52, average training loss: 7552.90, base loss: 8841.78
[INFO 2017-06-26 17:53:04,265 main.py:47] epoch 1052, training loss: 7596.50, average training loss: 7552.42, base loss: 8842.85
[INFO 2017-06-26 17:53:04,629 main.py:47] epoch 1053, training loss: 6365.87, average training loss: 7550.76, base loss: 8842.57
[INFO 2017-06-26 17:53:04,989 main.py:47] epoch 1054, training loss: 8194.63, average training loss: 7550.50, base loss: 8844.42
[INFO 2017-06-26 17:53:05,349 main.py:47] epoch 1055, training loss: 7376.60, average training loss: 7549.94, base loss: 8845.40
[INFO 2017-06-26 17:53:05,710 main.py:47] epoch 1056, training loss: 6352.41, average training loss: 7546.82, base loss: 8843.29
[INFO 2017-06-26 17:53:06,070 main.py:47] epoch 1057, training loss: 7655.57, average training loss: 7546.98, base loss: 8845.18
[INFO 2017-06-26 17:53:06,432 main.py:47] epoch 1058, training loss: 6462.82, average training loss: 7545.37, base loss: 8844.20
[INFO 2017-06-26 17:53:06,791 main.py:47] epoch 1059, training loss: 7036.70, average training loss: 7544.26, base loss: 8844.61
[INFO 2017-06-26 17:53:07,152 main.py:47] epoch 1060, training loss: 6348.69, average training loss: 7541.30, base loss: 8843.06
[INFO 2017-06-26 17:53:07,510 main.py:47] epoch 1061, training loss: 7506.99, average training loss: 7539.46, base loss: 8843.18
[INFO 2017-06-26 17:53:07,874 main.py:47] epoch 1062, training loss: 7168.69, average training loss: 7537.04, base loss: 8842.38
[INFO 2017-06-26 17:53:08,236 main.py:47] epoch 1063, training loss: 8094.37, average training loss: 7536.76, base loss: 8843.77
[INFO 2017-06-26 17:53:08,597 main.py:47] epoch 1064, training loss: 6898.23, average training loss: 7533.71, base loss: 8842.40
[INFO 2017-06-26 17:53:08,960 main.py:47] epoch 1065, training loss: 6637.89, average training loss: 7531.53, base loss: 8841.25
[INFO 2017-06-26 17:53:09,320 main.py:47] epoch 1066, training loss: 6375.02, average training loss: 7529.32, base loss: 8839.81
[INFO 2017-06-26 17:53:09,681 main.py:47] epoch 1067, training loss: 7597.83, average training loss: 7529.65, base loss: 8842.41
[INFO 2017-06-26 17:53:10,040 main.py:47] epoch 1068, training loss: 7428.50, average training loss: 7528.54, base loss: 8842.96
[INFO 2017-06-26 17:53:10,404 main.py:47] epoch 1069, training loss: 7577.53, average training loss: 7529.24, base loss: 8845.50
[INFO 2017-06-26 17:53:10,767 main.py:47] epoch 1070, training loss: 6336.36, average training loss: 7525.19, base loss: 8842.66
[INFO 2017-06-26 17:53:11,131 main.py:47] epoch 1071, training loss: 7175.50, average training loss: 7523.80, base loss: 8842.76
[INFO 2017-06-26 17:53:11,494 main.py:47] epoch 1072, training loss: 7989.18, average training loss: 7522.39, base loss: 8842.75
[INFO 2017-06-26 17:53:11,857 main.py:47] epoch 1073, training loss: 7478.05, average training loss: 7520.90, base loss: 8842.89
[INFO 2017-06-26 17:53:12,218 main.py:47] epoch 1074, training loss: 6759.09, average training loss: 7518.56, base loss: 8841.77
[INFO 2017-06-26 17:53:12,581 main.py:47] epoch 1075, training loss: 7226.64, average training loss: 7517.59, base loss: 8842.03
[INFO 2017-06-26 17:53:12,944 main.py:47] epoch 1076, training loss: 7018.12, average training loss: 7515.69, base loss: 8841.45
[INFO 2017-06-26 17:53:13,306 main.py:47] epoch 1077, training loss: 6870.57, average training loss: 7514.78, base loss: 8842.11
[INFO 2017-06-26 17:53:13,667 main.py:47] epoch 1078, training loss: 6688.92, average training loss: 7512.46, base loss: 8841.12
[INFO 2017-06-26 17:53:14,029 main.py:47] epoch 1079, training loss: 7582.66, average training loss: 7511.70, base loss: 8842.05
[INFO 2017-06-26 17:53:14,391 main.py:47] epoch 1080, training loss: 7631.09, average training loss: 7510.18, base loss: 8842.09
[INFO 2017-06-26 17:53:14,754 main.py:47] epoch 1081, training loss: 6935.40, average training loss: 7509.52, base loss: 8842.60
[INFO 2017-06-26 17:53:15,115 main.py:47] epoch 1082, training loss: 7058.14, average training loss: 7506.02, base loss: 8840.00
[INFO 2017-06-26 17:53:15,480 main.py:47] epoch 1083, training loss: 6881.08, average training loss: 7504.05, base loss: 8839.69
[INFO 2017-06-26 17:53:15,844 main.py:47] epoch 1084, training loss: 7228.05, average training loss: 7502.67, base loss: 8839.58
[INFO 2017-06-26 17:53:16,205 main.py:47] epoch 1085, training loss: 6814.69, average training loss: 7500.64, base loss: 8838.73
[INFO 2017-06-26 17:53:16,568 main.py:47] epoch 1086, training loss: 7893.27, average training loss: 7501.31, base loss: 8841.08
[INFO 2017-06-26 17:53:16,936 main.py:47] epoch 1087, training loss: 7125.55, average training loss: 7499.81, base loss: 8840.97
[INFO 2017-06-26 17:53:17,298 main.py:47] epoch 1088, training loss: 6840.02, average training loss: 7496.57, base loss: 8839.11
[INFO 2017-06-26 17:53:17,660 main.py:47] epoch 1089, training loss: 7343.89, average training loss: 7495.26, base loss: 8839.13
[INFO 2017-06-26 17:53:18,022 main.py:47] epoch 1090, training loss: 7698.33, average training loss: 7494.20, base loss: 8839.66
[INFO 2017-06-26 17:53:18,383 main.py:47] epoch 1091, training loss: 6819.10, average training loss: 7491.81, base loss: 8838.44
[INFO 2017-06-26 17:53:18,745 main.py:47] epoch 1092, training loss: 7382.87, average training loss: 7490.71, base loss: 8838.92
[INFO 2017-06-26 17:53:19,106 main.py:47] epoch 1093, training loss: 7833.07, average training loss: 7489.61, base loss: 8839.64
[INFO 2017-06-26 17:53:19,467 main.py:47] epoch 1094, training loss: 7153.56, average training loss: 7489.73, base loss: 8841.38
[INFO 2017-06-26 17:53:19,827 main.py:47] epoch 1095, training loss: 5984.80, average training loss: 7487.84, base loss: 8840.58
[INFO 2017-06-26 17:53:20,189 main.py:47] epoch 1096, training loss: 6346.08, average training loss: 7485.87, base loss: 8839.50
[INFO 2017-06-26 17:53:20,551 main.py:47] epoch 1097, training loss: 7461.96, average training loss: 7485.15, base loss: 8840.30
[INFO 2017-06-26 17:53:20,912 main.py:47] epoch 1098, training loss: 7724.92, average training loss: 7484.73, base loss: 8841.31
[INFO 2017-06-26 17:53:21,273 main.py:47] epoch 1099, training loss: 6249.13, average training loss: 7482.62, base loss: 8839.24
[INFO 2017-06-26 17:53:21,274 main.py:49] epoch 1099, testing
[INFO 2017-06-26 17:53:25,528 main.py:100] average testing loss: 7131.66, base loss: 8873.63
[INFO 2017-06-26 17:53:25,551 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:53:25,912 main.py:47] epoch 1100, training loss: 7708.66, average training loss: 7482.67, base loss: 8841.64
[INFO 2017-06-26 17:53:26,275 main.py:47] epoch 1101, training loss: 6531.51, average training loss: 7481.42, base loss: 8841.76
[INFO 2017-06-26 17:53:26,637 main.py:47] epoch 1102, training loss: 7414.13, average training loss: 7480.77, base loss: 8842.62
[INFO 2017-06-26 17:53:26,999 main.py:47] epoch 1103, training loss: 6988.79, average training loss: 7480.04, base loss: 8843.49
[INFO 2017-06-26 17:53:27,359 main.py:47] epoch 1104, training loss: 6777.35, average training loss: 7476.59, base loss: 8841.00
[INFO 2017-06-26 17:53:27,725 main.py:47] epoch 1105, training loss: 7535.82, average training loss: 7474.83, base loss: 8839.94
[INFO 2017-06-26 17:53:28,087 main.py:47] epoch 1106, training loss: 6740.88, average training loss: 7473.31, base loss: 8839.33
[INFO 2017-06-26 17:53:28,448 main.py:47] epoch 1107, training loss: 7235.01, average training loss: 7472.64, base loss: 8840.32
[INFO 2017-06-26 17:53:28,810 main.py:47] epoch 1108, training loss: 6505.03, average training loss: 7469.15, base loss: 8838.02
[INFO 2017-06-26 17:53:29,172 main.py:47] epoch 1109, training loss: 7757.42, average training loss: 7469.46, base loss: 8839.44
[INFO 2017-06-26 17:53:29,539 main.py:47] epoch 1110, training loss: 7259.75, average training loss: 7468.95, base loss: 8840.24
[INFO 2017-06-26 17:53:29,900 main.py:47] epoch 1111, training loss: 6599.12, average training loss: 7466.79, base loss: 8838.51
[INFO 2017-06-26 17:53:30,261 main.py:47] epoch 1112, training loss: 7117.26, average training loss: 7464.87, base loss: 8837.52
[INFO 2017-06-26 17:53:30,623 main.py:47] epoch 1113, training loss: 6741.35, average training loss: 7464.00, base loss: 8838.23
[INFO 2017-06-26 17:53:30,985 main.py:47] epoch 1114, training loss: 6330.61, average training loss: 7463.17, base loss: 8839.03
[INFO 2017-06-26 17:53:31,348 main.py:47] epoch 1115, training loss: 7740.98, average training loss: 7462.34, base loss: 8839.42
[INFO 2017-06-26 17:53:31,712 main.py:47] epoch 1116, training loss: 5957.03, average training loss: 7459.28, base loss: 8836.97
[INFO 2017-06-26 17:53:32,075 main.py:47] epoch 1117, training loss: 6378.48, average training loss: 7456.39, base loss: 8835.38
[INFO 2017-06-26 17:53:32,437 main.py:47] epoch 1118, training loss: 6355.94, average training loss: 7454.78, base loss: 8834.52
[INFO 2017-06-26 17:53:32,798 main.py:47] epoch 1119, training loss: 6790.79, average training loss: 7452.72, base loss: 8834.21
[INFO 2017-06-26 17:53:33,159 main.py:47] epoch 1120, training loss: 6848.20, average training loss: 7450.98, base loss: 8833.79
[INFO 2017-06-26 17:53:33,523 main.py:47] epoch 1121, training loss: 9882.99, average training loss: 7453.63, base loss: 8838.41
[INFO 2017-06-26 17:53:33,886 main.py:47] epoch 1122, training loss: 7497.75, average training loss: 7452.59, base loss: 8838.92
[INFO 2017-06-26 17:53:34,249 main.py:47] epoch 1123, training loss: 7150.82, average training loss: 7451.80, base loss: 8839.93
[INFO 2017-06-26 17:53:34,620 main.py:47] epoch 1124, training loss: 7780.31, average training loss: 7449.95, base loss: 8839.49
[INFO 2017-06-26 17:53:34,981 main.py:47] epoch 1125, training loss: 7392.73, average training loss: 7448.84, base loss: 8838.62
[INFO 2017-06-26 17:53:35,348 main.py:47] epoch 1126, training loss: 7308.72, average training loss: 7447.65, base loss: 8838.44
[INFO 2017-06-26 17:53:35,709 main.py:47] epoch 1127, training loss: 7312.24, average training loss: 7446.36, base loss: 8838.55
[INFO 2017-06-26 17:53:36,074 main.py:47] epoch 1128, training loss: 6561.99, average training loss: 7444.44, base loss: 8837.55
[INFO 2017-06-26 17:53:36,440 main.py:47] epoch 1129, training loss: 7549.52, average training loss: 7444.19, base loss: 8838.40
[INFO 2017-06-26 17:53:36,801 main.py:47] epoch 1130, training loss: 7867.17, average training loss: 7444.01, base loss: 8839.35
[INFO 2017-06-26 17:53:37,164 main.py:47] epoch 1131, training loss: 6335.82, average training loss: 7442.48, base loss: 8838.71
[INFO 2017-06-26 17:53:37,523 main.py:47] epoch 1132, training loss: 7555.42, average training loss: 7441.69, base loss: 8838.89
[INFO 2017-06-26 17:53:37,884 main.py:47] epoch 1133, training loss: 6368.53, average training loss: 7439.05, base loss: 8836.91
[INFO 2017-06-26 17:53:38,248 main.py:47] epoch 1134, training loss: 7352.89, average training loss: 7438.63, base loss: 8837.75
[INFO 2017-06-26 17:53:38,607 main.py:47] epoch 1135, training loss: 6993.40, average training loss: 7438.19, base loss: 8838.47
[INFO 2017-06-26 17:53:38,969 main.py:47] epoch 1136, training loss: 7371.49, average training loss: 7437.44, base loss: 8838.88
[INFO 2017-06-26 17:53:39,332 main.py:47] epoch 1137, training loss: 6434.19, average training loss: 7435.91, base loss: 8837.78
[INFO 2017-06-26 17:53:39,695 main.py:47] epoch 1138, training loss: 8143.88, average training loss: 7436.08, base loss: 8839.47
[INFO 2017-06-26 17:53:40,061 main.py:47] epoch 1139, training loss: 6901.38, average training loss: 7435.65, base loss: 8839.85
[INFO 2017-06-26 17:53:40,429 main.py:47] epoch 1140, training loss: 6957.13, average training loss: 7434.78, base loss: 8840.34
[INFO 2017-06-26 17:53:40,791 main.py:47] epoch 1141, training loss: 7292.81, average training loss: 7434.62, base loss: 8840.93
[INFO 2017-06-26 17:53:41,154 main.py:47] epoch 1142, training loss: 7325.46, average training loss: 7434.69, base loss: 8842.09
[INFO 2017-06-26 17:53:41,515 main.py:47] epoch 1143, training loss: 6996.32, average training loss: 7432.44, base loss: 8840.70
[INFO 2017-06-26 17:53:41,876 main.py:47] epoch 1144, training loss: 8184.60, average training loss: 7433.71, base loss: 8843.55
[INFO 2017-06-26 17:53:42,238 main.py:47] epoch 1145, training loss: 6128.98, average training loss: 7431.97, base loss: 8842.02
[INFO 2017-06-26 17:53:42,601 main.py:47] epoch 1146, training loss: 7457.00, average training loss: 7431.03, base loss: 8842.34
[INFO 2017-06-26 17:53:42,962 main.py:47] epoch 1147, training loss: 6858.83, average training loss: 7429.76, base loss: 8841.65
[INFO 2017-06-26 17:53:43,325 main.py:47] epoch 1148, training loss: 6925.19, average training loss: 7428.42, base loss: 8840.87
[INFO 2017-06-26 17:53:43,686 main.py:47] epoch 1149, training loss: 7228.91, average training loss: 7427.16, base loss: 8840.64
[INFO 2017-06-26 17:53:44,047 main.py:47] epoch 1150, training loss: 6420.55, average training loss: 7425.27, base loss: 8839.20
[INFO 2017-06-26 17:53:44,408 main.py:47] epoch 1151, training loss: 6512.92, average training loss: 7423.60, base loss: 8838.38
[INFO 2017-06-26 17:53:44,777 main.py:47] epoch 1152, training loss: 6470.75, average training loss: 7422.13, base loss: 8838.21
[INFO 2017-06-26 17:53:45,140 main.py:47] epoch 1153, training loss: 7843.73, average training loss: 7421.81, base loss: 8838.79
[INFO 2017-06-26 17:53:45,500 main.py:47] epoch 1154, training loss: 6321.15, average training loss: 7420.69, base loss: 8838.38
[INFO 2017-06-26 17:53:45,872 main.py:47] epoch 1155, training loss: 7394.15, average training loss: 7419.52, base loss: 8837.77
[INFO 2017-06-26 17:53:46,237 main.py:47] epoch 1156, training loss: 8173.65, average training loss: 7419.84, base loss: 8839.27
[INFO 2017-06-26 17:53:46,597 main.py:47] epoch 1157, training loss: 6962.24, average training loss: 7419.82, base loss: 8840.25
[INFO 2017-06-26 17:53:46,962 main.py:47] epoch 1158, training loss: 7988.05, average training loss: 7419.61, base loss: 8841.24
[INFO 2017-06-26 17:53:47,323 main.py:47] epoch 1159, training loss: 6091.56, average training loss: 7417.48, base loss: 8839.57
[INFO 2017-06-26 17:53:47,685 main.py:47] epoch 1160, training loss: 7285.90, average training loss: 7416.04, base loss: 8838.67
[INFO 2017-06-26 17:53:48,048 main.py:47] epoch 1161, training loss: 7002.99, average training loss: 7412.96, base loss: 8835.94
[INFO 2017-06-26 17:53:48,410 main.py:47] epoch 1162, training loss: 7317.42, average training loss: 7412.12, base loss: 8836.02
[INFO 2017-06-26 17:53:48,774 main.py:47] epoch 1163, training loss: 7190.23, average training loss: 7410.93, base loss: 8835.73
[INFO 2017-06-26 17:53:49,136 main.py:47] epoch 1164, training loss: 7298.55, average training loss: 7410.34, base loss: 8836.37
[INFO 2017-06-26 17:53:49,499 main.py:47] epoch 1165, training loss: 8001.09, average training loss: 7410.16, base loss: 8837.38
[INFO 2017-06-26 17:53:49,860 main.py:47] epoch 1166, training loss: 7538.50, average training loss: 7410.39, base loss: 8838.84
[INFO 2017-06-26 17:53:50,222 main.py:47] epoch 1167, training loss: 6347.18, average training loss: 7407.81, base loss: 8836.91
[INFO 2017-06-26 17:53:50,586 main.py:47] epoch 1168, training loss: 7342.20, average training loss: 7407.11, base loss: 8837.36
[INFO 2017-06-26 17:53:50,952 main.py:47] epoch 1169, training loss: 6716.47, average training loss: 7405.11, base loss: 8836.18
[INFO 2017-06-26 17:53:51,314 main.py:47] epoch 1170, training loss: 7054.28, average training loss: 7404.56, base loss: 8836.84
[INFO 2017-06-26 17:53:51,675 main.py:47] epoch 1171, training loss: 6731.79, average training loss: 7402.39, base loss: 8835.19
[INFO 2017-06-26 17:53:52,036 main.py:47] epoch 1172, training loss: 7914.85, average training loss: 7400.10, base loss: 8833.75
[INFO 2017-06-26 17:53:52,397 main.py:47] epoch 1173, training loss: 7066.19, average training loss: 7399.54, base loss: 8834.54
[INFO 2017-06-26 17:53:52,758 main.py:47] epoch 1174, training loss: 6695.92, average training loss: 7397.66, base loss: 8833.07
[INFO 2017-06-26 17:53:53,119 main.py:47] epoch 1175, training loss: 6083.58, average training loss: 7394.80, base loss: 8830.46
[INFO 2017-06-26 17:53:53,482 main.py:47] epoch 1176, training loss: 6775.67, average training loss: 7394.06, base loss: 8830.98
[INFO 2017-06-26 17:53:53,843 main.py:47] epoch 1177, training loss: 6748.23, average training loss: 7392.30, base loss: 8830.20
[INFO 2017-06-26 17:53:54,207 main.py:47] epoch 1178, training loss: 7771.85, average training loss: 7392.26, base loss: 8830.98
[INFO 2017-06-26 17:53:54,569 main.py:47] epoch 1179, training loss: 6913.70, average training loss: 7391.67, base loss: 8831.06
[INFO 2017-06-26 17:53:54,930 main.py:47] epoch 1180, training loss: 6625.90, average training loss: 7390.35, base loss: 8830.08
[INFO 2017-06-26 17:53:55,292 main.py:47] epoch 1181, training loss: 6748.82, average training loss: 7390.01, base loss: 8830.99
[INFO 2017-06-26 17:53:55,654 main.py:47] epoch 1182, training loss: 7052.31, average training loss: 7389.28, base loss: 8831.38
[INFO 2017-06-26 17:53:56,017 main.py:47] epoch 1183, training loss: 7006.66, average training loss: 7389.51, base loss: 8832.33
[INFO 2017-06-26 17:53:56,382 main.py:47] epoch 1184, training loss: 7306.43, average training loss: 7388.58, base loss: 8832.86
[INFO 2017-06-26 17:53:56,742 main.py:47] epoch 1185, training loss: 8506.16, average training loss: 7388.29, base loss: 8833.95
[INFO 2017-06-26 17:53:57,104 main.py:47] epoch 1186, training loss: 7529.78, average training loss: 7388.11, base loss: 8835.07
[INFO 2017-06-26 17:53:57,466 main.py:47] epoch 1187, training loss: 7143.08, average training loss: 7386.77, base loss: 8834.46
[INFO 2017-06-26 17:53:57,827 main.py:47] epoch 1188, training loss: 7389.26, average training loss: 7386.10, base loss: 8834.90
[INFO 2017-06-26 17:53:58,190 main.py:47] epoch 1189, training loss: 6987.69, average training loss: 7384.03, base loss: 8834.31
[INFO 2017-06-26 17:53:58,553 main.py:47] epoch 1190, training loss: 7007.69, average training loss: 7382.49, base loss: 8833.18
[INFO 2017-06-26 17:53:58,914 main.py:47] epoch 1191, training loss: 7733.47, average training loss: 7383.44, base loss: 8835.16
[INFO 2017-06-26 17:53:59,274 main.py:47] epoch 1192, training loss: 7216.97, average training loss: 7382.16, base loss: 8834.95
[INFO 2017-06-26 17:53:59,635 main.py:47] epoch 1193, training loss: 6979.78, average training loss: 7380.85, base loss: 8833.95
[INFO 2017-06-26 17:54:00,006 main.py:47] epoch 1194, training loss: 6249.26, average training loss: 7379.47, base loss: 8833.22
[INFO 2017-06-26 17:54:00,367 main.py:47] epoch 1195, training loss: 7936.06, average training loss: 7379.15, base loss: 8834.08
[INFO 2017-06-26 17:54:00,729 main.py:47] epoch 1196, training loss: 7058.60, average training loss: 7378.94, base loss: 8834.30
[INFO 2017-06-26 17:54:01,088 main.py:47] epoch 1197, training loss: 6257.70, average training loss: 7377.38, base loss: 8833.19
[INFO 2017-06-26 17:54:01,449 main.py:47] epoch 1198, training loss: 6874.04, average training loss: 7376.88, base loss: 8833.87
[INFO 2017-06-26 17:54:01,809 main.py:47] epoch 1199, training loss: 8198.48, average training loss: 7376.87, base loss: 8834.84
[INFO 2017-06-26 17:54:01,809 main.py:49] epoch 1199, testing
[INFO 2017-06-26 17:54:05,995 main.py:100] average testing loss: 7189.88, base loss: 8954.13
[INFO 2017-06-26 17:54:06,021 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:54:06,383 main.py:47] epoch 1200, training loss: 7319.37, average training loss: 7376.45, base loss: 8834.67
[INFO 2017-06-26 17:54:06,745 main.py:47] epoch 1201, training loss: 7024.29, average training loss: 7376.54, base loss: 8836.06
[INFO 2017-06-26 17:54:07,106 main.py:47] epoch 1202, training loss: 8021.20, average training loss: 7375.97, base loss: 8836.82
[INFO 2017-06-26 17:54:07,468 main.py:47] epoch 1203, training loss: 7486.00, average training loss: 7376.09, base loss: 8838.18
[INFO 2017-06-26 17:54:07,829 main.py:47] epoch 1204, training loss: 7024.59, average training loss: 7374.96, base loss: 8837.83
[INFO 2017-06-26 17:54:08,190 main.py:47] epoch 1205, training loss: 7016.68, average training loss: 7374.16, base loss: 8837.48
[INFO 2017-06-26 17:54:08,552 main.py:47] epoch 1206, training loss: 7102.44, average training loss: 7372.72, base loss: 8836.77
[INFO 2017-06-26 17:54:08,913 main.py:47] epoch 1207, training loss: 8311.82, average training loss: 7371.82, base loss: 8837.46
[INFO 2017-06-26 17:54:09,273 main.py:47] epoch 1208, training loss: 7141.98, average training loss: 7371.55, base loss: 8838.02
[INFO 2017-06-26 17:54:09,634 main.py:47] epoch 1209, training loss: 7856.04, average training loss: 7371.10, base loss: 8838.06
[INFO 2017-06-26 17:54:10,001 main.py:47] epoch 1210, training loss: 7330.40, average training loss: 7370.73, base loss: 8838.27
[INFO 2017-06-26 17:54:10,360 main.py:47] epoch 1211, training loss: 7145.48, average training loss: 7370.46, base loss: 8838.35
[INFO 2017-06-26 17:54:10,720 main.py:47] epoch 1212, training loss: 5916.81, average training loss: 7368.65, base loss: 8836.69
[INFO 2017-06-26 17:54:11,081 main.py:47] epoch 1213, training loss: 7908.40, average training loss: 7368.21, base loss: 8837.23
[INFO 2017-06-26 17:54:11,441 main.py:47] epoch 1214, training loss: 7035.75, average training loss: 7366.89, base loss: 8836.35
[INFO 2017-06-26 17:54:11,802 main.py:47] epoch 1215, training loss: 7460.91, average training loss: 7366.54, base loss: 8836.83
[INFO 2017-06-26 17:54:12,163 main.py:47] epoch 1216, training loss: 6658.53, average training loss: 7364.84, base loss: 8835.94
[INFO 2017-06-26 17:54:12,525 main.py:47] epoch 1217, training loss: 7994.64, average training loss: 7365.85, base loss: 8838.17
[INFO 2017-06-26 17:54:12,887 main.py:47] epoch 1218, training loss: 7317.07, average training loss: 7365.93, base loss: 8839.23
[INFO 2017-06-26 17:54:13,250 main.py:47] epoch 1219, training loss: 7086.35, average training loss: 7364.56, base loss: 8838.63
[INFO 2017-06-26 17:54:13,614 main.py:47] epoch 1220, training loss: 7002.31, average training loss: 7362.32, base loss: 8836.77
[INFO 2017-06-26 17:54:13,976 main.py:47] epoch 1221, training loss: 7715.73, average training loss: 7361.34, base loss: 8836.79
[INFO 2017-06-26 17:54:14,339 main.py:47] epoch 1222, training loss: 6987.06, average training loss: 7360.38, base loss: 8836.71
[INFO 2017-06-26 17:54:14,703 main.py:47] epoch 1223, training loss: 6694.74, average training loss: 7359.20, base loss: 8836.17
[INFO 2017-06-26 17:54:15,072 main.py:47] epoch 1224, training loss: 8422.63, average training loss: 7359.96, base loss: 8838.30
[INFO 2017-06-26 17:54:15,432 main.py:47] epoch 1225, training loss: 7511.24, average training loss: 7359.40, base loss: 8838.46
[INFO 2017-06-26 17:54:15,792 main.py:47] epoch 1226, training loss: 7134.03, average training loss: 7358.83, base loss: 8838.73
[INFO 2017-06-26 17:54:16,153 main.py:47] epoch 1227, training loss: 7360.31, average training loss: 7358.77, base loss: 8839.48
[INFO 2017-06-26 17:54:16,519 main.py:47] epoch 1228, training loss: 7169.87, average training loss: 7358.44, base loss: 8840.07
[INFO 2017-06-26 17:54:16,885 main.py:47] epoch 1229, training loss: 6310.47, average training loss: 7355.85, base loss: 8837.47
[INFO 2017-06-26 17:54:17,249 main.py:47] epoch 1230, training loss: 7302.16, average training loss: 7354.84, base loss: 8837.59
[INFO 2017-06-26 17:54:17,612 main.py:47] epoch 1231, training loss: 7869.45, average training loss: 7354.53, base loss: 8838.50
[INFO 2017-06-26 17:54:17,976 main.py:47] epoch 1232, training loss: 7166.96, average training loss: 7353.47, base loss: 8837.93
[INFO 2017-06-26 17:54:18,341 main.py:47] epoch 1233, training loss: 6485.17, average training loss: 7352.48, base loss: 8838.01
[INFO 2017-06-26 17:54:18,705 main.py:47] epoch 1234, training loss: 7689.05, average training loss: 7352.99, base loss: 8839.90
[INFO 2017-06-26 17:54:19,068 main.py:47] epoch 1235, training loss: 6873.19, average training loss: 7351.35, base loss: 8838.47
[INFO 2017-06-26 17:54:19,432 main.py:47] epoch 1236, training loss: 6733.54, average training loss: 7350.29, base loss: 8838.17
[INFO 2017-06-26 17:54:19,798 main.py:47] epoch 1237, training loss: 6973.13, average training loss: 7349.97, base loss: 8839.17
[INFO 2017-06-26 17:54:20,166 main.py:47] epoch 1238, training loss: 7266.13, average training loss: 7349.35, base loss: 8839.68
[INFO 2017-06-26 17:54:20,536 main.py:47] epoch 1239, training loss: 7154.21, average training loss: 7350.13, base loss: 8841.12
[INFO 2017-06-26 17:54:20,907 main.py:47] epoch 1240, training loss: 6902.40, average training loss: 7349.47, base loss: 8841.05
[INFO 2017-06-26 17:54:21,274 main.py:47] epoch 1241, training loss: 6567.61, average training loss: 7348.70, base loss: 8841.27
[INFO 2017-06-26 17:54:21,640 main.py:47] epoch 1242, training loss: 7303.10, average training loss: 7347.54, base loss: 8840.46
[INFO 2017-06-26 17:54:22,001 main.py:47] epoch 1243, training loss: 6505.17, average training loss: 7344.98, base loss: 8838.08
[INFO 2017-06-26 17:54:22,363 main.py:47] epoch 1244, training loss: 6391.84, average training loss: 7342.94, base loss: 8836.28
[INFO 2017-06-26 17:54:22,728 main.py:47] epoch 1245, training loss: 8065.84, average training loss: 7343.95, base loss: 8838.11
[INFO 2017-06-26 17:54:23,093 main.py:47] epoch 1246, training loss: 7448.66, average training loss: 7343.83, base loss: 8839.19
[INFO 2017-06-26 17:54:23,456 main.py:47] epoch 1247, training loss: 6976.59, average training loss: 7343.00, base loss: 8839.40
[INFO 2017-06-26 17:54:23,821 main.py:47] epoch 1248, training loss: 7301.93, average training loss: 7342.86, base loss: 8840.51
[INFO 2017-06-26 17:54:24,182 main.py:47] epoch 1249, training loss: 7253.55, average training loss: 7342.15, base loss: 8840.76
[INFO 2017-06-26 17:54:24,543 main.py:47] epoch 1250, training loss: 6670.96, average training loss: 7341.83, base loss: 8841.08
[INFO 2017-06-26 17:54:24,905 main.py:47] epoch 1251, training loss: 6432.78, average training loss: 7340.08, base loss: 8839.45
[INFO 2017-06-26 17:54:25,275 main.py:47] epoch 1252, training loss: 7365.79, average training loss: 7339.84, base loss: 8840.52
[INFO 2017-06-26 17:54:25,637 main.py:47] epoch 1253, training loss: 6831.40, average training loss: 7339.20, base loss: 8840.49
[INFO 2017-06-26 17:54:26,002 main.py:47] epoch 1254, training loss: 7603.31, average training loss: 7338.38, base loss: 8839.83
[INFO 2017-06-26 17:54:26,368 main.py:47] epoch 1255, training loss: 7062.27, average training loss: 7337.60, base loss: 8839.70
[INFO 2017-06-26 17:54:26,731 main.py:47] epoch 1256, training loss: 6412.44, average training loss: 7336.25, base loss: 8839.45
[INFO 2017-06-26 17:54:27,093 main.py:47] epoch 1257, training loss: 7425.36, average training loss: 7336.40, base loss: 8840.63
[INFO 2017-06-26 17:54:27,457 main.py:47] epoch 1258, training loss: 6705.19, average training loss: 7336.07, base loss: 8841.41
[INFO 2017-06-26 17:54:27,818 main.py:47] epoch 1259, training loss: 6649.75, average training loss: 7334.84, base loss: 8840.78
[INFO 2017-06-26 17:54:28,180 main.py:47] epoch 1260, training loss: 6763.78, average training loss: 7333.18, base loss: 8839.14
[INFO 2017-06-26 17:54:28,542 main.py:47] epoch 1261, training loss: 7902.93, average training loss: 7333.87, base loss: 8840.81
[INFO 2017-06-26 17:54:28,905 main.py:47] epoch 1262, training loss: 6881.32, average training loss: 7333.66, base loss: 8842.15
[INFO 2017-06-26 17:54:29,267 main.py:47] epoch 1263, training loss: 5991.54, average training loss: 7331.46, base loss: 8840.19
[INFO 2017-06-26 17:54:29,630 main.py:47] epoch 1264, training loss: 7141.77, average training loss: 7330.88, base loss: 8840.04
[INFO 2017-06-26 17:54:29,992 main.py:47] epoch 1265, training loss: 6898.84, average training loss: 7330.06, base loss: 8839.74
[INFO 2017-06-26 17:54:30,359 main.py:47] epoch 1266, training loss: 7578.04, average training loss: 7331.01, base loss: 8841.73
[INFO 2017-06-26 17:54:30,724 main.py:47] epoch 1267, training loss: 7274.23, average training loss: 7332.24, base loss: 8844.64
[INFO 2017-06-26 17:54:31,085 main.py:47] epoch 1268, training loss: 6449.66, average training loss: 7329.15, base loss: 8841.57
[INFO 2017-06-26 17:54:31,449 main.py:47] epoch 1269, training loss: 7857.74, average training loss: 7329.59, base loss: 8844.07
[INFO 2017-06-26 17:54:31,815 main.py:47] epoch 1270, training loss: 6193.23, average training loss: 7326.88, base loss: 8841.26
[INFO 2017-06-26 17:54:32,177 main.py:47] epoch 1271, training loss: 7180.19, average training loss: 7325.90, base loss: 8840.80
[INFO 2017-06-26 17:54:32,541 main.py:47] epoch 1272, training loss: 6756.81, average training loss: 7325.10, base loss: 8840.00
[INFO 2017-06-26 17:54:32,906 main.py:47] epoch 1273, training loss: 7458.47, average training loss: 7325.92, base loss: 8842.07
[INFO 2017-06-26 17:54:33,269 main.py:47] epoch 1274, training loss: 6096.61, average training loss: 7323.83, base loss: 8839.98
[INFO 2017-06-26 17:54:33,631 main.py:47] epoch 1275, training loss: 7417.84, average training loss: 7322.38, base loss: 8838.79
[INFO 2017-06-26 17:54:33,992 main.py:47] epoch 1276, training loss: 6395.77, average training loss: 7321.48, base loss: 8838.81
[INFO 2017-06-26 17:54:34,354 main.py:47] epoch 1277, training loss: 6851.06, average training loss: 7319.11, base loss: 8836.73
[INFO 2017-06-26 17:54:34,715 main.py:47] epoch 1278, training loss: 7037.53, average training loss: 7318.55, base loss: 8837.12
[INFO 2017-06-26 17:54:35,076 main.py:47] epoch 1279, training loss: 6637.42, average training loss: 7316.98, base loss: 8836.00
[INFO 2017-06-26 17:54:35,448 main.py:47] epoch 1280, training loss: 7061.26, average training loss: 7316.01, base loss: 8835.27
[INFO 2017-06-26 17:54:35,811 main.py:47] epoch 1281, training loss: 6945.53, average training loss: 7314.74, base loss: 8834.52
[INFO 2017-06-26 17:54:36,180 main.py:47] epoch 1282, training loss: 7089.80, average training loss: 7314.97, base loss: 8836.01
[INFO 2017-06-26 17:54:36,546 main.py:47] epoch 1283, training loss: 6427.85, average training loss: 7313.48, base loss: 8834.69
[INFO 2017-06-26 17:54:36,910 main.py:47] epoch 1284, training loss: 7591.40, average training loss: 7313.32, base loss: 8835.03
[INFO 2017-06-26 17:54:37,274 main.py:47] epoch 1285, training loss: 6692.37, average training loss: 7312.70, base loss: 8834.79
[INFO 2017-06-26 17:54:37,635 main.py:47] epoch 1286, training loss: 5879.12, average training loss: 7310.97, base loss: 8833.07
[INFO 2017-06-26 17:54:37,999 main.py:47] epoch 1287, training loss: 6727.84, average training loss: 7309.14, base loss: 8831.96
[INFO 2017-06-26 17:54:38,360 main.py:47] epoch 1288, training loss: 7176.98, average training loss: 7308.63, base loss: 8832.75
[INFO 2017-06-26 17:54:38,721 main.py:47] epoch 1289, training loss: 6545.49, average training loss: 7306.84, base loss: 8830.88
[INFO 2017-06-26 17:54:39,083 main.py:47] epoch 1290, training loss: 7293.68, average training loss: 7307.20, base loss: 8832.36
[INFO 2017-06-26 17:54:39,447 main.py:47] epoch 1291, training loss: 6541.64, average training loss: 7305.38, base loss: 8830.93
[INFO 2017-06-26 17:54:39,810 main.py:47] epoch 1292, training loss: 7143.70, average training loss: 7305.77, base loss: 8832.28
[INFO 2017-06-26 17:54:40,172 main.py:47] epoch 1293, training loss: 7782.74, average training loss: 7305.42, base loss: 8832.85
[INFO 2017-06-26 17:54:40,534 main.py:47] epoch 1294, training loss: 7866.37, average training loss: 7305.26, base loss: 8833.49
[INFO 2017-06-26 17:54:40,907 main.py:47] epoch 1295, training loss: 7885.64, average training loss: 7305.67, base loss: 8834.85
[INFO 2017-06-26 17:54:41,270 main.py:47] epoch 1296, training loss: 6798.94, average training loss: 7305.71, base loss: 8836.22
[INFO 2017-06-26 17:54:41,632 main.py:47] epoch 1297, training loss: 5902.33, average training loss: 7303.86, base loss: 8833.70
[INFO 2017-06-26 17:54:41,993 main.py:47] epoch 1298, training loss: 7454.13, average training loss: 7304.26, base loss: 8835.68
[INFO 2017-06-26 17:54:42,356 main.py:47] epoch 1299, training loss: 7888.15, average training loss: 7304.83, base loss: 8837.20
[INFO 2017-06-26 17:54:42,356 main.py:49] epoch 1299, testing
[INFO 2017-06-26 17:54:46,523 main.py:100] average testing loss: 7100.28, base loss: 8956.36
[INFO 2017-06-26 17:54:46,548 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:54:46,910 main.py:47] epoch 1300, training loss: 7840.21, average training loss: 7305.35, base loss: 8838.74
[INFO 2017-06-26 17:54:47,273 main.py:47] epoch 1301, training loss: 7162.57, average training loss: 7304.25, base loss: 8838.12
[INFO 2017-06-26 17:54:47,635 main.py:47] epoch 1302, training loss: 7213.29, average training loss: 7303.93, base loss: 8839.05
[INFO 2017-06-26 17:54:47,997 main.py:47] epoch 1303, training loss: 6478.46, average training loss: 7302.84, base loss: 8838.43
[INFO 2017-06-26 17:54:48,358 main.py:47] epoch 1304, training loss: 7217.35, average training loss: 7302.33, base loss: 8838.47
[INFO 2017-06-26 17:54:48,720 main.py:47] epoch 1305, training loss: 7259.84, average training loss: 7302.67, base loss: 8839.97
[INFO 2017-06-26 17:54:49,081 main.py:47] epoch 1306, training loss: 6435.36, average training loss: 7301.54, base loss: 8839.20
[INFO 2017-06-26 17:54:49,442 main.py:47] epoch 1307, training loss: 6279.44, average training loss: 7300.18, base loss: 8838.59
[INFO 2017-06-26 17:54:49,802 main.py:47] epoch 1308, training loss: 6776.26, average training loss: 7299.18, base loss: 8838.66
[INFO 2017-06-26 17:54:50,162 main.py:47] epoch 1309, training loss: 7030.61, average training loss: 7298.52, base loss: 8838.98
[INFO 2017-06-26 17:54:50,530 main.py:47] epoch 1310, training loss: 7039.55, average training loss: 7296.96, base loss: 8838.21
[INFO 2017-06-26 17:54:50,893 main.py:47] epoch 1311, training loss: 6107.49, average training loss: 7295.07, base loss: 8836.67
[INFO 2017-06-26 17:54:51,255 main.py:47] epoch 1312, training loss: 7181.13, average training loss: 7294.19, base loss: 8836.47
[INFO 2017-06-26 17:54:51,616 main.py:47] epoch 1313, training loss: 7287.11, average training loss: 7292.67, base loss: 8835.68
[INFO 2017-06-26 17:54:51,980 main.py:47] epoch 1314, training loss: 8375.52, average training loss: 7293.96, base loss: 8838.41
[INFO 2017-06-26 17:54:52,344 main.py:47] epoch 1315, training loss: 7328.82, average training loss: 7293.49, base loss: 8839.84
[INFO 2017-06-26 17:54:52,710 main.py:47] epoch 1316, training loss: 6487.83, average training loss: 7292.55, base loss: 8839.12
[INFO 2017-06-26 17:54:53,078 main.py:47] epoch 1317, training loss: 7599.06, average training loss: 7292.59, base loss: 8840.44
[INFO 2017-06-26 17:54:53,441 main.py:47] epoch 1318, training loss: 6813.31, average training loss: 7290.97, base loss: 8839.53
[INFO 2017-06-26 17:54:53,807 main.py:47] epoch 1319, training loss: 6195.45, average training loss: 7288.86, base loss: 8837.86
[INFO 2017-06-26 17:54:54,205 main.py:47] epoch 1320, training loss: 6189.40, average training loss: 7287.06, base loss: 8835.91
[INFO 2017-06-26 17:54:54,570 main.py:47] epoch 1321, training loss: 7205.95, average training loss: 7286.59, base loss: 8836.17
[INFO 2017-06-26 17:54:54,937 main.py:47] epoch 1322, training loss: 8158.51, average training loss: 7286.45, base loss: 8837.11
[INFO 2017-06-26 17:54:55,299 main.py:47] epoch 1323, training loss: 7293.74, average training loss: 7285.10, base loss: 8836.05
[INFO 2017-06-26 17:54:55,662 main.py:47] epoch 1324, training loss: 6764.15, average training loss: 7283.74, base loss: 8835.24
[INFO 2017-06-26 17:54:56,024 main.py:47] epoch 1325, training loss: 6822.69, average training loss: 7283.27, base loss: 8835.28
[INFO 2017-06-26 17:54:56,386 main.py:47] epoch 1326, training loss: 6908.81, average training loss: 7283.20, base loss: 8836.14
[INFO 2017-06-26 17:54:56,750 main.py:47] epoch 1327, training loss: 7838.52, average training loss: 7283.90, base loss: 8837.16
[INFO 2017-06-26 17:54:57,112 main.py:47] epoch 1328, training loss: 7932.08, average training loss: 7282.90, base loss: 8836.10
[INFO 2017-06-26 17:54:57,477 main.py:47] epoch 1329, training loss: 6285.45, average training loss: 7281.23, base loss: 8834.33
[INFO 2017-06-26 17:54:57,841 main.py:47] epoch 1330, training loss: 7433.34, average training loss: 7281.08, base loss: 8834.99
[INFO 2017-06-26 17:54:58,211 main.py:47] epoch 1331, training loss: 6720.98, average training loss: 7279.90, base loss: 8834.57
[INFO 2017-06-26 17:54:58,606 main.py:47] epoch 1332, training loss: 7668.71, average training loss: 7279.74, base loss: 8835.26
[INFO 2017-06-26 17:54:58,998 main.py:47] epoch 1333, training loss: 6686.75, average training loss: 7278.60, base loss: 8834.80
[INFO 2017-06-26 17:54:59,393 main.py:47] epoch 1334, training loss: 7146.66, average training loss: 7277.94, base loss: 8835.98
[INFO 2017-06-26 17:54:59,779 main.py:47] epoch 1335, training loss: 6551.31, average training loss: 7276.21, base loss: 8834.46
[INFO 2017-06-26 17:55:00,150 main.py:47] epoch 1336, training loss: 7120.07, average training loss: 7275.75, base loss: 8834.77
[INFO 2017-06-26 17:55:00,564 main.py:47] epoch 1337, training loss: 7960.76, average training loss: 7276.30, base loss: 8836.55
[INFO 2017-06-26 17:55:00,930 main.py:47] epoch 1338, training loss: 7220.18, average training loss: 7276.89, base loss: 8838.02
[INFO 2017-06-26 17:55:01,293 main.py:47] epoch 1339, training loss: 6928.47, average training loss: 7275.80, base loss: 8837.40
[INFO 2017-06-26 17:55:01,655 main.py:47] epoch 1340, training loss: 7376.17, average training loss: 7275.65, base loss: 8838.19
[INFO 2017-06-26 17:55:02,018 main.py:47] epoch 1341, training loss: 7381.57, average training loss: 7275.96, base loss: 8839.24
[INFO 2017-06-26 17:55:02,380 main.py:47] epoch 1342, training loss: 7049.24, average training loss: 7275.14, base loss: 8839.23
[INFO 2017-06-26 17:55:02,744 main.py:47] epoch 1343, training loss: 6772.20, average training loss: 7274.59, base loss: 8838.89
[INFO 2017-06-26 17:55:03,105 main.py:47] epoch 1344, training loss: 6923.26, average training loss: 7273.70, base loss: 8838.37
[INFO 2017-06-26 17:55:03,467 main.py:47] epoch 1345, training loss: 7142.68, average training loss: 7274.39, base loss: 8839.40
[INFO 2017-06-26 17:55:03,829 main.py:47] epoch 1346, training loss: 8087.29, average training loss: 7273.95, base loss: 8839.40
[INFO 2017-06-26 17:55:04,192 main.py:47] epoch 1347, training loss: 7876.99, average training loss: 7274.44, base loss: 8840.61
[INFO 2017-06-26 17:55:04,554 main.py:47] epoch 1348, training loss: 6213.79, average training loss: 7273.55, base loss: 8839.90
[INFO 2017-06-26 17:55:04,916 main.py:47] epoch 1349, training loss: 7117.40, average training loss: 7273.95, base loss: 8840.90
[INFO 2017-06-26 17:55:05,279 main.py:47] epoch 1350, training loss: 7439.33, average training loss: 7273.45, base loss: 8841.40
[INFO 2017-06-26 17:55:05,640 main.py:47] epoch 1351, training loss: 7507.25, average training loss: 7273.92, base loss: 8842.16
[INFO 2017-06-26 17:55:06,002 main.py:47] epoch 1352, training loss: 6969.98, average training loss: 7273.37, base loss: 8842.09
[INFO 2017-06-26 17:55:06,365 main.py:47] epoch 1353, training loss: 7696.43, average training loss: 7273.65, base loss: 8843.13
[INFO 2017-06-26 17:55:06,725 main.py:47] epoch 1354, training loss: 6675.31, average training loss: 7273.08, base loss: 8842.33
[INFO 2017-06-26 17:55:07,086 main.py:47] epoch 1355, training loss: 6712.91, average training loss: 7271.81, base loss: 8840.89
[INFO 2017-06-26 17:55:07,448 main.py:47] epoch 1356, training loss: 6796.05, average training loss: 7270.51, base loss: 8839.90
[INFO 2017-06-26 17:55:07,810 main.py:47] epoch 1357, training loss: 6976.49, average training loss: 7270.12, base loss: 8839.82
[INFO 2017-06-26 17:55:08,171 main.py:47] epoch 1358, training loss: 7277.38, average training loss: 7270.18, base loss: 8840.57
[INFO 2017-06-26 17:55:08,536 main.py:47] epoch 1359, training loss: 6979.51, average training loss: 7268.27, base loss: 8838.84
[INFO 2017-06-26 17:55:08,899 main.py:47] epoch 1360, training loss: 6859.55, average training loss: 7267.95, base loss: 8839.24
[INFO 2017-06-26 17:55:09,301 main.py:47] epoch 1361, training loss: 7459.56, average training loss: 7268.26, base loss: 8840.28
[INFO 2017-06-26 17:55:09,669 main.py:47] epoch 1362, training loss: 6626.72, average training loss: 7265.78, base loss: 8838.16
[INFO 2017-06-26 17:55:10,039 main.py:47] epoch 1363, training loss: 6959.27, average training loss: 7264.01, base loss: 8837.26
[INFO 2017-06-26 17:55:10,401 main.py:47] epoch 1364, training loss: 7585.58, average training loss: 7262.66, base loss: 8836.50
[INFO 2017-06-26 17:55:10,762 main.py:47] epoch 1365, training loss: 7307.60, average training loss: 7262.32, base loss: 8837.34
[INFO 2017-06-26 17:55:11,123 main.py:47] epoch 1366, training loss: 6669.15, average training loss: 7261.46, base loss: 8836.87
[INFO 2017-06-26 17:55:11,484 main.py:47] epoch 1367, training loss: 7499.46, average training loss: 7261.07, base loss: 8837.05
[INFO 2017-06-26 17:55:11,845 main.py:47] epoch 1368, training loss: 6546.83, average training loss: 7259.56, base loss: 8835.80
[INFO 2017-06-26 17:55:12,206 main.py:47] epoch 1369, training loss: 5841.43, average training loss: 7256.94, base loss: 8832.75
[INFO 2017-06-26 17:55:12,566 main.py:47] epoch 1370, training loss: 7595.23, average training loss: 7256.57, base loss: 8833.61
[INFO 2017-06-26 17:55:12,928 main.py:47] epoch 1371, training loss: 7500.92, average training loss: 7257.18, base loss: 8835.38
[INFO 2017-06-26 17:55:13,290 main.py:47] epoch 1372, training loss: 6476.11, average training loss: 7255.79, base loss: 8834.25
[INFO 2017-06-26 17:55:13,650 main.py:47] epoch 1373, training loss: 6382.83, average training loss: 7253.85, base loss: 8832.48
[INFO 2017-06-26 17:55:14,043 main.py:47] epoch 1374, training loss: 6824.99, average training loss: 7253.32, base loss: 8832.12
[INFO 2017-06-26 17:55:14,435 main.py:47] epoch 1375, training loss: 6281.53, average training loss: 7252.02, base loss: 8830.96
[INFO 2017-06-26 17:55:14,812 main.py:47] epoch 1376, training loss: 6295.83, average training loss: 7251.20, base loss: 8830.24
[INFO 2017-06-26 17:55:15,175 main.py:47] epoch 1377, training loss: 6848.83, average training loss: 7251.65, base loss: 8831.68
[INFO 2017-06-26 17:55:15,541 main.py:47] epoch 1378, training loss: 6881.34, average training loss: 7250.59, base loss: 8830.88
[INFO 2017-06-26 17:55:15,903 main.py:47] epoch 1379, training loss: 6619.64, average training loss: 7249.77, base loss: 8830.83
[INFO 2017-06-26 17:55:16,265 main.py:47] epoch 1380, training loss: 7647.05, average training loss: 7249.59, base loss: 8831.24
[INFO 2017-06-26 17:55:16,629 main.py:47] epoch 1381, training loss: 6242.25, average training loss: 7248.38, base loss: 8829.88
[INFO 2017-06-26 17:55:17,011 main.py:47] epoch 1382, training loss: 7190.24, average training loss: 7247.37, base loss: 8829.55
[INFO 2017-06-26 17:55:17,389 main.py:47] epoch 1383, training loss: 7653.81, average training loss: 7247.57, base loss: 8830.39
[INFO 2017-06-26 17:55:17,752 main.py:47] epoch 1384, training loss: 8775.76, average training loss: 7248.97, base loss: 8832.86
[INFO 2017-06-26 17:55:18,115 main.py:47] epoch 1385, training loss: 7121.10, average training loss: 7249.15, base loss: 8833.90
[INFO 2017-06-26 17:55:18,477 main.py:47] epoch 1386, training loss: 6966.07, average training loss: 7248.82, base loss: 8834.27
[INFO 2017-06-26 17:55:18,861 main.py:47] epoch 1387, training loss: 7063.46, average training loss: 7247.61, base loss: 8833.87
[INFO 2017-06-26 17:55:19,224 main.py:47] epoch 1388, training loss: 7131.41, average training loss: 7247.35, base loss: 8834.39
[INFO 2017-06-26 17:55:19,587 main.py:47] epoch 1389, training loss: 6702.80, average training loss: 7246.64, base loss: 8833.97
[INFO 2017-06-26 17:55:19,984 main.py:47] epoch 1390, training loss: 6389.55, average training loss: 7246.14, base loss: 8833.50
[INFO 2017-06-26 17:55:20,356 main.py:47] epoch 1391, training loss: 6540.73, average training loss: 7245.70, base loss: 8833.89
[INFO 2017-06-26 17:55:20,733 main.py:47] epoch 1392, training loss: 8299.91, average training loss: 7246.94, base loss: 8836.19
[INFO 2017-06-26 17:55:21,095 main.py:47] epoch 1393, training loss: 7484.48, average training loss: 7247.82, base loss: 8837.58
[INFO 2017-06-26 17:55:21,455 main.py:47] epoch 1394, training loss: 6397.94, average training loss: 7246.46, base loss: 8836.03
[INFO 2017-06-26 17:55:21,816 main.py:47] epoch 1395, training loss: 7141.55, average training loss: 7245.62, base loss: 8835.74
[INFO 2017-06-26 17:55:22,176 main.py:47] epoch 1396, training loss: 6776.92, average training loss: 7246.24, base loss: 8837.38
[INFO 2017-06-26 17:55:22,536 main.py:47] epoch 1397, training loss: 6622.31, average training loss: 7245.19, base loss: 8836.80
[INFO 2017-06-26 17:55:22,899 main.py:47] epoch 1398, training loss: 7564.38, average training loss: 7244.92, base loss: 8836.76
[INFO 2017-06-26 17:55:23,262 main.py:47] epoch 1399, training loss: 7352.83, average training loss: 7244.39, base loss: 8836.07
[INFO 2017-06-26 17:55:23,262 main.py:49] epoch 1399, testing
[INFO 2017-06-26 17:55:27,530 main.py:100] average testing loss: 7357.01, base loss: 9208.99
[INFO 2017-06-26 17:55:27,554 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:55:27,916 main.py:47] epoch 1400, training loss: 7863.43, average training loss: 7244.88, base loss: 8837.82
[INFO 2017-06-26 17:55:28,278 main.py:47] epoch 1401, training loss: 5907.63, average training loss: 7243.37, base loss: 8836.55
[INFO 2017-06-26 17:55:28,640 main.py:47] epoch 1402, training loss: 6778.21, average training loss: 7241.41, base loss: 8834.65
[INFO 2017-06-26 17:55:29,001 main.py:47] epoch 1403, training loss: 7348.88, average training loss: 7241.54, base loss: 8835.98
[INFO 2017-06-26 17:55:29,360 main.py:47] epoch 1404, training loss: 8130.20, average training loss: 7241.60, base loss: 8836.74
[INFO 2017-06-26 17:55:29,721 main.py:47] epoch 1405, training loss: 7530.51, average training loss: 7242.02, base loss: 8837.69
[INFO 2017-06-26 17:55:30,080 main.py:47] epoch 1406, training loss: 7800.56, average training loss: 7242.27, base loss: 8838.88
[INFO 2017-06-26 17:55:30,439 main.py:47] epoch 1407, training loss: 7178.97, average training loss: 7242.80, base loss: 8839.79
[INFO 2017-06-26 17:55:30,803 main.py:47] epoch 1408, training loss: 7340.13, average training loss: 7242.49, base loss: 8838.90
[INFO 2017-06-26 17:55:31,218 main.py:47] epoch 1409, training loss: 7294.96, average training loss: 7242.69, base loss: 8839.48
[INFO 2017-06-26 17:55:31,587 main.py:47] epoch 1410, training loss: 6922.34, average training loss: 7241.46, base loss: 8838.67
[INFO 2017-06-26 17:55:31,955 main.py:47] epoch 1411, training loss: 7790.09, average training loss: 7242.53, base loss: 8840.95
[INFO 2017-06-26 17:55:32,316 main.py:47] epoch 1412, training loss: 6571.36, average training loss: 7241.25, base loss: 8839.99
[INFO 2017-06-26 17:55:32,678 main.py:47] epoch 1413, training loss: 6721.31, average training loss: 7239.91, base loss: 8839.15
[INFO 2017-06-26 17:55:33,039 main.py:47] epoch 1414, training loss: 7760.20, average training loss: 7240.29, base loss: 8840.32
[INFO 2017-06-26 17:55:33,401 main.py:47] epoch 1415, training loss: 7303.98, average training loss: 7240.29, base loss: 8840.73
[INFO 2017-06-26 17:55:33,764 main.py:47] epoch 1416, training loss: 6921.78, average training loss: 7239.08, base loss: 8840.07
[INFO 2017-06-26 17:55:34,123 main.py:47] epoch 1417, training loss: 7289.94, average training loss: 7237.87, base loss: 8839.32
[INFO 2017-06-26 17:55:34,484 main.py:47] epoch 1418, training loss: 5919.00, average training loss: 7237.15, base loss: 8838.94
[INFO 2017-06-26 17:55:34,847 main.py:47] epoch 1419, training loss: 6699.97, average training loss: 7236.70, base loss: 8838.94
[INFO 2017-06-26 17:55:35,209 main.py:47] epoch 1420, training loss: 7349.84, average training loss: 7237.21, base loss: 8839.65
[INFO 2017-06-26 17:55:35,571 main.py:47] epoch 1421, training loss: 6998.17, average training loss: 7236.29, base loss: 8839.07
[INFO 2017-06-26 17:55:35,932 main.py:47] epoch 1422, training loss: 6441.20, average training loss: 7235.17, base loss: 8837.71
[INFO 2017-06-26 17:55:36,293 main.py:47] epoch 1423, training loss: 7763.04, average training loss: 7234.82, base loss: 8837.91
[INFO 2017-06-26 17:55:36,656 main.py:47] epoch 1424, training loss: 6915.13, average training loss: 7233.73, base loss: 8837.87
[INFO 2017-06-26 17:55:37,031 main.py:47] epoch 1425, training loss: 6392.75, average training loss: 7233.53, base loss: 8838.61
[INFO 2017-06-26 17:55:37,395 main.py:47] epoch 1426, training loss: 7409.03, average training loss: 7233.74, base loss: 8839.94
[INFO 2017-06-26 17:55:37,756 main.py:47] epoch 1427, training loss: 7088.00, average training loss: 7234.15, base loss: 8841.72
[INFO 2017-06-26 17:55:38,118 main.py:47] epoch 1428, training loss: 7021.60, average training loss: 7233.19, base loss: 8840.88
[INFO 2017-06-26 17:55:38,479 main.py:47] epoch 1429, training loss: 7455.46, average training loss: 7233.74, base loss: 8841.66
[INFO 2017-06-26 17:55:38,838 main.py:47] epoch 1430, training loss: 7203.43, average training loss: 7234.18, base loss: 8842.92
[INFO 2017-06-26 17:55:39,198 main.py:47] epoch 1431, training loss: 6434.71, average training loss: 7233.02, base loss: 8842.59
[INFO 2017-06-26 17:55:39,557 main.py:47] epoch 1432, training loss: 6514.83, average training loss: 7231.78, base loss: 8841.46
[INFO 2017-06-26 17:55:39,918 main.py:47] epoch 1433, training loss: 6628.79, average training loss: 7230.83, base loss: 8841.06
[INFO 2017-06-26 17:55:40,278 main.py:47] epoch 1434, training loss: 7484.93, average training loss: 7230.16, base loss: 8840.64
[INFO 2017-06-26 17:55:40,638 main.py:47] epoch 1435, training loss: 6538.79, average training loss: 7228.51, base loss: 8838.66
[INFO 2017-06-26 17:55:41,000 main.py:47] epoch 1436, training loss: 7867.09, average training loss: 7228.92, base loss: 8839.90
[INFO 2017-06-26 17:55:41,365 main.py:47] epoch 1437, training loss: 7824.89, average training loss: 7228.66, base loss: 8840.19
[INFO 2017-06-26 17:55:41,743 main.py:47] epoch 1438, training loss: 7631.09, average training loss: 7227.31, base loss: 8838.40
[INFO 2017-06-26 17:55:42,112 main.py:47] epoch 1439, training loss: 7255.07, average training loss: 7227.22, base loss: 8838.64
[INFO 2017-06-26 17:55:42,475 main.py:47] epoch 1440, training loss: 6259.58, average training loss: 7225.73, base loss: 8836.95
[INFO 2017-06-26 17:55:42,845 main.py:47] epoch 1441, training loss: 6302.59, average training loss: 7223.92, base loss: 8834.69
[INFO 2017-06-26 17:55:43,204 main.py:47] epoch 1442, training loss: 7014.06, average training loss: 7224.15, base loss: 8836.32
[INFO 2017-06-26 17:55:43,567 main.py:47] epoch 1443, training loss: 7450.99, average training loss: 7224.61, base loss: 8837.17
[INFO 2017-06-26 17:55:43,933 main.py:47] epoch 1444, training loss: 6498.23, average training loss: 7223.65, base loss: 8836.69
[INFO 2017-06-26 17:55:44,293 main.py:47] epoch 1445, training loss: 6549.57, average training loss: 7223.21, base loss: 8836.39
[INFO 2017-06-26 17:55:44,653 main.py:47] epoch 1446, training loss: 5921.79, average training loss: 7221.64, base loss: 8834.62
[INFO 2017-06-26 17:55:45,014 main.py:47] epoch 1447, training loss: 7658.58, average training loss: 7220.70, base loss: 8834.00
[INFO 2017-06-26 17:55:45,373 main.py:47] epoch 1448, training loss: 7607.87, average training loss: 7221.66, base loss: 8836.02
[INFO 2017-06-26 17:55:45,735 main.py:47] epoch 1449, training loss: 7565.06, average training loss: 7222.43, base loss: 8837.56
[INFO 2017-06-26 17:55:46,096 main.py:47] epoch 1450, training loss: 7196.10, average training loss: 7222.01, base loss: 8837.45
[INFO 2017-06-26 17:55:46,465 main.py:47] epoch 1451, training loss: 6412.63, average training loss: 7220.36, base loss: 8835.85
[INFO 2017-06-26 17:55:46,831 main.py:47] epoch 1452, training loss: 6547.16, average training loss: 7219.81, base loss: 8836.09
[INFO 2017-06-26 17:55:47,199 main.py:47] epoch 1453, training loss: 6909.59, average training loss: 7218.83, base loss: 8834.88
[INFO 2017-06-26 17:55:47,567 main.py:47] epoch 1454, training loss: 7274.20, average training loss: 7219.05, base loss: 8836.32
[INFO 2017-06-26 17:55:47,935 main.py:47] epoch 1455, training loss: 8332.63, average training loss: 7220.12, base loss: 8838.59
[INFO 2017-06-26 17:55:48,301 main.py:47] epoch 1456, training loss: 6020.34, average training loss: 7219.06, base loss: 8837.28
[INFO 2017-06-26 17:55:48,665 main.py:47] epoch 1457, training loss: 7839.10, average training loss: 7220.18, base loss: 8839.23
[INFO 2017-06-26 17:55:49,028 main.py:47] epoch 1458, training loss: 7240.86, average training loss: 7220.09, base loss: 8839.46
[INFO 2017-06-26 17:55:49,390 main.py:47] epoch 1459, training loss: 6502.36, average training loss: 7217.36, base loss: 8836.21
[INFO 2017-06-26 17:55:49,754 main.py:47] epoch 1460, training loss: 7136.68, average training loss: 7216.44, base loss: 8835.80
[INFO 2017-06-26 17:55:50,116 main.py:47] epoch 1461, training loss: 6870.67, average training loss: 7216.80, base loss: 8837.42
[INFO 2017-06-26 17:55:50,480 main.py:47] epoch 1462, training loss: 6890.30, average training loss: 7216.48, base loss: 8837.27
[INFO 2017-06-26 17:55:50,841 main.py:47] epoch 1463, training loss: 6882.20, average training loss: 7216.06, base loss: 8837.37
[INFO 2017-06-26 17:55:51,200 main.py:47] epoch 1464, training loss: 7079.64, average training loss: 7215.70, base loss: 8837.60
[INFO 2017-06-26 17:55:51,578 main.py:47] epoch 1465, training loss: 6800.12, average training loss: 7215.38, base loss: 8837.71
[INFO 2017-06-26 17:55:51,940 main.py:47] epoch 1466, training loss: 7699.06, average training loss: 7214.33, base loss: 8837.34
[INFO 2017-06-26 17:55:52,301 main.py:47] epoch 1467, training loss: 6276.64, average training loss: 7213.21, base loss: 8836.09
[INFO 2017-06-26 17:55:52,661 main.py:47] epoch 1468, training loss: 6466.34, average training loss: 7212.02, base loss: 8835.27
[INFO 2017-06-26 17:55:53,023 main.py:47] epoch 1469, training loss: 7086.52, average training loss: 7211.71, base loss: 8835.03
[INFO 2017-06-26 17:55:53,385 main.py:47] epoch 1470, training loss: 6538.53, average training loss: 7210.10, base loss: 8833.26
[INFO 2017-06-26 17:55:53,747 main.py:47] epoch 1471, training loss: 7061.41, average training loss: 7209.82, base loss: 8833.83
[INFO 2017-06-26 17:55:54,109 main.py:47] epoch 1472, training loss: 6337.84, average training loss: 7209.06, base loss: 8833.46
[INFO 2017-06-26 17:55:54,471 main.py:47] epoch 1473, training loss: 7059.14, average training loss: 7207.85, base loss: 8832.84
[INFO 2017-06-26 17:55:54,833 main.py:47] epoch 1474, training loss: 7282.18, average training loss: 7208.21, base loss: 8833.47
[INFO 2017-06-26 17:55:55,198 main.py:47] epoch 1475, training loss: 7167.58, average training loss: 7207.73, base loss: 8832.80
[INFO 2017-06-26 17:55:55,567 main.py:47] epoch 1476, training loss: 6109.66, average training loss: 7205.73, base loss: 8830.92
[INFO 2017-06-26 17:55:55,944 main.py:47] epoch 1477, training loss: 6512.42, average training loss: 7204.80, base loss: 8830.55
[INFO 2017-06-26 17:55:56,308 main.py:47] epoch 1478, training loss: 6612.89, average training loss: 7204.15, base loss: 8830.12
[INFO 2017-06-26 17:55:56,683 main.py:47] epoch 1479, training loss: 6934.17, average training loss: 7204.86, base loss: 8831.94
[INFO 2017-06-26 17:55:57,051 main.py:47] epoch 1480, training loss: 7586.29, average training loss: 7205.76, base loss: 8833.24
[INFO 2017-06-26 17:55:57,416 main.py:47] epoch 1481, training loss: 7255.63, average training loss: 7205.62, base loss: 8833.07
[INFO 2017-06-26 17:55:57,782 main.py:47] epoch 1482, training loss: 7681.33, average training loss: 7206.40, base loss: 8834.55
[INFO 2017-06-26 17:55:58,149 main.py:47] epoch 1483, training loss: 5703.07, average training loss: 7204.64, base loss: 8832.07
[INFO 2017-06-26 17:55:58,511 main.py:47] epoch 1484, training loss: 7305.25, average training loss: 7203.76, base loss: 8831.08
[INFO 2017-06-26 17:55:58,880 main.py:47] epoch 1485, training loss: 7199.33, average training loss: 7202.95, base loss: 8830.26
[INFO 2017-06-26 17:55:59,264 main.py:47] epoch 1486, training loss: 6286.33, average training loss: 7202.04, base loss: 8829.69
[INFO 2017-06-26 17:55:59,627 main.py:47] epoch 1487, training loss: 6122.94, average training loss: 7201.61, base loss: 8829.67
[INFO 2017-06-26 17:55:59,990 main.py:47] epoch 1488, training loss: 7282.28, average training loss: 7200.71, base loss: 8828.91
[INFO 2017-06-26 17:56:00,354 main.py:47] epoch 1489, training loss: 8833.03, average training loss: 7200.98, base loss: 8830.23
[INFO 2017-06-26 17:56:00,716 main.py:47] epoch 1490, training loss: 6595.68, average training loss: 7200.39, base loss: 8829.72
[INFO 2017-06-26 17:56:01,077 main.py:47] epoch 1491, training loss: 6119.57, average training loss: 7198.99, base loss: 8828.76
[INFO 2017-06-26 17:56:01,450 main.py:47] epoch 1492, training loss: 6552.48, average training loss: 7197.92, base loss: 8827.47
[INFO 2017-06-26 17:56:01,814 main.py:47] epoch 1493, training loss: 7686.72, average training loss: 7197.97, base loss: 8827.87
[INFO 2017-06-26 17:56:02,183 main.py:47] epoch 1494, training loss: 6854.77, average training loss: 7197.76, base loss: 8828.05
[INFO 2017-06-26 17:56:02,553 main.py:47] epoch 1495, training loss: 8748.63, average training loss: 7198.64, base loss: 8829.47
[INFO 2017-06-26 17:56:02,919 main.py:47] epoch 1496, training loss: 6667.08, average training loss: 7197.95, base loss: 8829.12
[INFO 2017-06-26 17:56:03,281 main.py:47] epoch 1497, training loss: 6889.21, average training loss: 7197.80, base loss: 8829.31
[INFO 2017-06-26 17:56:03,644 main.py:47] epoch 1498, training loss: 5921.96, average training loss: 7195.42, base loss: 8826.68
[INFO 2017-06-26 17:56:04,008 main.py:47] epoch 1499, training loss: 7066.52, average training loss: 7194.74, base loss: 8826.28
[INFO 2017-06-26 17:56:04,008 main.py:49] epoch 1499, testing
[INFO 2017-06-26 17:56:08,335 main.py:100] average testing loss: 7218.20, base loss: 9225.86
[INFO 2017-06-26 17:56:08,362 main.py:73] current best accuracy: 7016.53
[INFO 2017-06-26 17:56:08,726 main.py:47] epoch 1500, training loss: 6511.58, average training loss: 7193.42, base loss: 8825.37
[INFO 2017-06-26 17:56:09,092 main.py:47] epoch 1501, training loss: 6646.72, average training loss: 7193.46, base loss: 8825.51
[INFO 2017-06-26 17:56:09,456 main.py:47] epoch 1502, training loss: 6609.52, average training loss: 7192.54, base loss: 8825.38
[INFO 2017-06-26 17:56:09,822 main.py:47] epoch 1503, training loss: 6769.58, average training loss: 7191.81, base loss: 8824.41
[INFO 2017-06-26 17:56:10,188 main.py:47] epoch 1504, training loss: 7834.57, average training loss: 7191.53, base loss: 8824.95
[INFO 2017-06-26 17:56:10,552 main.py:47] epoch 1505, training loss: 6494.76, average training loss: 7189.53, base loss: 8823.30
[INFO 2017-06-26 17:56:10,916 main.py:47] epoch 1506, training loss: 6876.26, average training loss: 7189.16, base loss: 8822.79
[INFO 2017-06-26 17:56:11,280 main.py:47] epoch 1507, training loss: 7511.32, average training loss: 7188.89, base loss: 8822.83
[INFO 2017-06-26 17:56:11,645 main.py:47] epoch 1508, training loss: 6122.82, average training loss: 7187.52, base loss: 8821.89
[INFO 2017-06-26 17:56:12,008 main.py:47] epoch 1509, training loss: 7126.68, average training loss: 7187.63, base loss: 8822.50
[INFO 2017-06-26 17:56:12,392 main.py:47] epoch 1510, training loss: 7113.62, average training loss: 7186.76, base loss: 8822.03
[INFO 2017-06-26 17:56:12,782 main.py:47] epoch 1511, training loss: 7515.17, average training loss: 7186.60, base loss: 8821.73
[INFO 2017-06-26 17:56:13,170 main.py:47] epoch 1512, training loss: 6631.71, average training loss: 7185.93, base loss: 8821.36
[INFO 2017-06-26 17:56:13,566 main.py:47] epoch 1513, training loss: 6734.86, average training loss: 7184.92, base loss: 8821.19
[INFO 2017-06-26 17:56:13,938 main.py:47] epoch 1514, training loss: 6613.47, average training loss: 7184.56, base loss: 8821.45
[INFO 2017-06-26 17:56:14,323 main.py:47] epoch 1515, training loss: 5880.14, average training loss: 7184.42, base loss: 8821.57
[INFO 2017-06-26 17:56:14,729 main.py:47] epoch 1516, training loss: 6814.84, average training loss: 7183.45, base loss: 8820.81
[INFO 2017-06-26 17:56:15,139 main.py:47] epoch 1517, training loss: 7063.21, average training loss: 7183.45, base loss: 8821.60
[INFO 2017-06-26 17:56:15,502 main.py:47] epoch 1518, training loss: 6150.53, average training loss: 7183.25, base loss: 8821.47
[INFO 2017-06-26 17:56:15,865 main.py:47] epoch 1519, training loss: 6044.35, average training loss: 7182.32, base loss: 8820.18
[INFO 2017-06-26 17:56:16,228 main.py:47] epoch 1520, training loss: 7100.40, average training loss: 7181.26, base loss: 8819.19
[INFO 2017-06-26 17:56:16,588 main.py:47] epoch 1521, training loss: 8162.67, average training loss: 7182.01, base loss: 8820.46
[INFO 2017-06-26 17:56:16,952 main.py:47] epoch 1522, training loss: 7361.75, average training loss: 7181.62, base loss: 8820.20
[INFO 2017-06-26 17:56:17,316 main.py:47] epoch 1523, training loss: 8017.62, average training loss: 7182.14, base loss: 8822.14
[INFO 2017-06-26 17:56:17,677 main.py:47] epoch 1524, training loss: 6745.29, average training loss: 7181.27, base loss: 8821.56
[INFO 2017-06-26 17:56:18,038 main.py:47] epoch 1525, training loss: 7548.70, average training loss: 7181.23, base loss: 8821.78
[INFO 2017-06-26 17:56:18,400 main.py:47] epoch 1526, training loss: 8522.40, average training loss: 7182.50, base loss: 8824.13
[INFO 2017-06-26 17:56:18,765 main.py:47] epoch 1527, training loss: 7180.79, average training loss: 7182.17, base loss: 8823.74
[INFO 2017-06-26 17:56:19,130 main.py:47] epoch 1528, training loss: 6992.66, average training loss: 7182.46, base loss: 8824.93
[INFO 2017-06-26 17:56:19,497 main.py:47] epoch 1529, training loss: 7538.55, average training loss: 7182.50, base loss: 8825.86
[INFO 2017-06-26 17:56:19,860 main.py:47] epoch 1530, training loss: 6928.57, average training loss: 7181.48, base loss: 8825.00
[INFO 2017-06-26 17:56:20,222 main.py:47] epoch 1531, training loss: 7567.78, average training loss: 7182.12, base loss: 8826.68
[INFO 2017-06-26 17:56:20,595 main.py:47] epoch 1532, training loss: 6375.33, average training loss: 7180.98, base loss: 8825.64
[INFO 2017-06-26 17:56:20,958 main.py:47] epoch 1533, training loss: 7491.61, average training loss: 7181.49, base loss: 8826.87
[INFO 2017-06-26 17:56:21,339 main.py:47] epoch 1534, training loss: 7287.79, average training loss: 7182.29, base loss: 8828.22
[INFO 2017-06-26 17:56:21,700 main.py:47] epoch 1535, training loss: 7879.35, average training loss: 7182.83, base loss: 8829.50
[INFO 2017-06-26 17:56:22,061 main.py:47] epoch 1536, training loss: 8259.87, average training loss: 7182.74, base loss: 8830.26
[INFO 2017-06-26 17:56:22,422 main.py:47] epoch 1537, training loss: 7392.86, average training loss: 7181.63, base loss: 8829.41
[INFO 2017-06-26 17:56:22,783 main.py:47] epoch 1538, training loss: 7699.44, average training loss: 7181.95, base loss: 8830.44
[INFO 2017-06-26 17:56:23,144 main.py:47] epoch 1539, training loss: 6721.52, average training loss: 7181.78, base loss: 8830.13
[INFO 2017-06-26 17:56:23,506 main.py:47] epoch 1540, training loss: 6732.34, average training loss: 7181.13, base loss: 8829.97
[INFO 2017-06-26 17:56:23,867 main.py:47] epoch 1541, training loss: 6952.65, average training loss: 7181.09, base loss: 8830.25
[INFO 2017-06-26 17:56:24,228 main.py:47] epoch 1542, training loss: 6703.04, average training loss: 7179.66, base loss: 8829.33
[INFO 2017-06-26 17:56:24,589 main.py:47] epoch 1543, training loss: 6986.79, average training loss: 7179.36, base loss: 8829.51
[INFO 2017-06-26 17:56:24,951 main.py:47] epoch 1544, training loss: 7943.41, average training loss: 7179.25, base loss: 8829.79
[INFO 2017-06-26 17:56:25,311 main.py:47] epoch 1545, training loss: 7354.42, average training loss: 7178.49, base loss: 8829.08
[INFO 2017-06-26 17:56:25,674 main.py:47] epoch 1546, training loss: 7145.81, average training loss: 7179.00, base loss: 8830.10
[INFO 2017-06-26 17:56:26,037 main.py:47] epoch 1547, training loss: 7369.51, average training loss: 7178.93, base loss: 8830.39
[INFO 2017-06-26 17:56:26,396 main.py:47] epoch 1548, training loss: 6668.36, average training loss: 7178.19, base loss: 8829.68
[INFO 2017-06-26 17:56:26,759 main.py:47] epoch 1549, training loss: 6676.55, average training loss: 7177.99, base loss: 8829.79
[INFO 2017-06-26 17:56:27,120 main.py:47] epoch 1550, training loss: 7095.44, average training loss: 7177.70, base loss: 8829.50
[INFO 2017-06-26 17:56:27,516 main.py:47] epoch 1551, training loss: 7240.41, average training loss: 7177.37, base loss: 8830.19
[INFO 2017-06-26 17:56:27,921 main.py:47] epoch 1552, training loss: 8036.20, average training loss: 7177.81, base loss: 8831.02
[INFO 2017-06-26 17:56:28,293 main.py:47] epoch 1553, training loss: 6763.42, average training loss: 7177.15, base loss: 8830.80
[INFO 2017-06-26 17:56:28,654 main.py:47] epoch 1554, training loss: 6767.41, average training loss: 7176.12, base loss: 8829.70
[INFO 2017-06-26 17:56:29,013 main.py:47] epoch 1555, training loss: 8026.14, average training loss: 7176.79, base loss: 8831.68
[INFO 2017-06-26 17:56:29,375 main.py:47] epoch 1556, training loss: 6620.30, average training loss: 7175.01, base loss: 8830.00
[INFO 2017-06-26 17:56:29,738 main.py:47] epoch 1557, training loss: 7417.18, average training loss: 7174.45, base loss: 8828.94
[INFO 2017-06-26 17:56:30,099 main.py:47] epoch 1558, training loss: 7725.38, average training loss: 7174.48, base loss: 8829.93
[INFO 2017-06-26 17:56:30,463 main.py:47] epoch 1559, training loss: 7234.02, average training loss: 7174.99, base loss: 8831.26
[INFO 2017-06-26 17:56:30,823 main.py:47] epoch 1560, training loss: 8416.46, average training loss: 7175.94, base loss: 8833.46
[INFO 2017-06-26 17:56:31,184 main.py:47] epoch 1561, training loss: 6681.57, average training loss: 7175.05, base loss: 8832.78
[INFO 2017-06-26 17:56:31,544 main.py:47] epoch 1562, training loss: 7836.14, average training loss: 7174.62, base loss: 8833.25
[INFO 2017-06-26 17:56:31,908 main.py:47] epoch 1563, training loss: 6485.96, average training loss: 7173.40, base loss: 8831.37
[INFO 2017-06-26 17:56:32,304 main.py:47] epoch 1564, training loss: 6247.35, average training loss: 7172.42, base loss: 8830.57
[INFO 2017-06-26 17:56:32,669 main.py:47] epoch 1565, training loss: 7932.48, average training loss: 7172.56, base loss: 8831.50
[INFO 2017-06-26 17:56:33,035 main.py:47] epoch 1566, training loss: 7211.07, average training loss: 7172.75, base loss: 8831.70
[INFO 2017-06-26 17:56:33,399 main.py:47] epoch 1567, training loss: 6875.68, average training loss: 7171.92, base loss: 8831.42
[INFO 2017-06-26 17:56:33,762 main.py:47] epoch 1568, training loss: 6541.18, average training loss: 7171.08, base loss: 8829.74
[INFO 2017-06-26 17:56:34,126 main.py:47] epoch 1569, training loss: 7601.66, average training loss: 7171.95, base loss: 8831.82
[INFO 2017-06-26 17:56:34,489 main.py:47] epoch 1570, training loss: 6966.04, average training loss: 7171.54, base loss: 8831.61
[INFO 2017-06-26 17:56:34,850 main.py:47] epoch 1571, training loss: 7474.37, average training loss: 7171.73, base loss: 8833.11
[INFO 2017-06-26 17:56:35,212 main.py:47] epoch 1572, training loss: 6821.04, average training loss: 7171.58, base loss: 8833.25
[INFO 2017-06-26 17:56:35,573 main.py:47] epoch 1573, training loss: 7445.90, average training loss: 7171.41, base loss: 8833.62
[INFO 2017-06-26 17:56:35,933 main.py:47] epoch 1574, training loss: 6656.25, average training loss: 7171.83, base loss: 8834.59
[INFO 2017-06-26 17:56:36,294 main.py:47] epoch 1575, training loss: 7150.19, average training loss: 7171.24, base loss: 8835.13
[INFO 2017-06-26 17:56:36,656 main.py:47] epoch 1576, training loss: 7358.27, average training loss: 7171.00, base loss: 8834.95
[INFO 2017-06-26 17:56:37,017 main.py:47] epoch 1577, training loss: 7659.59, average training loss: 7170.72, base loss: 8835.98
[INFO 2017-06-26 17:56:37,379 main.py:47] epoch 1578, training loss: 6501.45, average training loss: 7170.46, base loss: 8836.30
[INFO 2017-06-26 17:56:37,739 main.py:47] epoch 1579, training loss: 7093.35, average training loss: 7170.55, base loss: 8837.35
[INFO 2017-06-26 17:56:38,100 main.py:47] epoch 1580, training loss: 7338.76, average training loss: 7170.34, base loss: 8837.77
[INFO 2017-06-26 17:56:38,465 main.py:47] epoch 1581, training loss: 8214.54, average training loss: 7171.50, base loss: 8839.66
[INFO 2017-06-26 17:56:38,865 main.py:47] epoch 1582, training loss: 7186.53, average training loss: 7170.37, base loss: 8838.55
[INFO 2017-06-26 17:56:39,272 main.py:47] epoch 1583, training loss: 7678.61, average training loss: 7170.28, base loss: 8839.33
[INFO 2017-06-26 17:56:39,640 main.py:47] epoch 1584, training loss: 6550.44, average training loss: 7169.51, base loss: 8838.78
[INFO 2017-06-26 17:56:40,007 main.py:47] epoch 1585, training loss: 7997.55, average training loss: 7170.16, base loss: 8840.33
[INFO 2017-06-26 17:56:40,369 main.py:47] epoch 1586, training loss: 7168.88, average training loss: 7170.71, base loss: 8841.45
[INFO 2017-06-26 17:56:40,733 main.py:47] epoch 1587, training loss: 7482.01, average training loss: 7169.05, base loss: 8839.72
[INFO 2017-06-26 17:56:41,097 main.py:47] epoch 1588, training loss: 6293.37, average training loss: 7167.67, base loss: 8838.46
[INFO 2017-06-26 17:56:41,461 main.py:47] epoch 1589, training loss: 6739.11, average training loss: 7166.83, base loss: 8838.29
[INFO 2017-06-26 17:56:41,825 main.py:47] epoch 1590, training loss: 6703.69, average training loss: 7165.46, base loss: 8836.53
[INFO 2017-06-26 17:56:42,189 main.py:47] epoch 1591, training loss: 7687.14, average training loss: 7165.85, base loss: 8837.18
[INFO 2017-06-26 17:56:42,554 main.py:47] epoch 1592, training loss: 6931.91, average training loss: 7165.25, base loss: 8836.85
[INFO 2017-06-26 17:56:42,916 main.py:47] epoch 1593, training loss: 6820.45, average training loss: 7164.61, base loss: 8836.05
[INFO 2017-06-26 17:56:43,278 main.py:47] epoch 1594, training loss: 7282.74, average training loss: 7163.90, base loss: 8835.65
[INFO 2017-06-26 17:56:43,641 main.py:47] epoch 1595, training loss: 6240.58, average training loss: 7162.83, base loss: 8834.91
[INFO 2017-06-26 17:56:44,002 main.py:47] epoch 1596, training loss: 7163.87, average training loss: 7163.33, base loss: 8836.23
[INFO 2017-06-26 17:56:44,364 main.py:47] epoch 1597, training loss: 6908.53, average training loss: 7164.08, base loss: 8837.79
[INFO 2017-06-26 17:56:44,727 main.py:47] epoch 1598, training loss: 7298.99, average training loss: 7163.96, base loss: 8838.57
[INFO 2017-06-26 17:56:45,091 main.py:47] epoch 1599, training loss: 7324.54, average training loss: 7163.36, base loss: 8838.30
[INFO 2017-06-26 17:56:45,091 main.py:49] epoch 1599, testing
[INFO 2017-06-26 17:56:49,287 main.py:100] average testing loss: 6984.85, base loss: 8990.67
[INFO 2017-06-26 17:56:49,313 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:56:49,319 main.py:73] current best accuracy: 6984.85
[INFO 2017-06-26 17:56:49,691 main.py:47] epoch 1600, training loss: 6543.93, average training loss: 7162.19, base loss: 8837.27
[INFO 2017-06-26 17:56:50,054 main.py:47] epoch 1601, training loss: 7803.77, average training loss: 7163.01, base loss: 8839.25
[INFO 2017-06-26 17:56:50,421 main.py:47] epoch 1602, training loss: 7013.14, average training loss: 7162.48, base loss: 8838.99
[INFO 2017-06-26 17:56:50,788 main.py:47] epoch 1603, training loss: 6222.09, average training loss: 7162.46, base loss: 8838.93
[INFO 2017-06-26 17:56:51,150 main.py:47] epoch 1604, training loss: 7856.24, average training loss: 7162.54, base loss: 8839.66
[INFO 2017-06-26 17:56:51,515 main.py:47] epoch 1605, training loss: 8215.60, average training loss: 7162.97, base loss: 8840.70
[INFO 2017-06-26 17:56:51,879 main.py:47] epoch 1606, training loss: 7166.26, average training loss: 7162.74, base loss: 8841.13
[INFO 2017-06-26 17:56:52,240 main.py:47] epoch 1607, training loss: 8231.88, average training loss: 7163.27, base loss: 8842.29
[INFO 2017-06-26 17:56:52,602 main.py:47] epoch 1608, training loss: 8158.47, average training loss: 7163.30, base loss: 8843.45
[INFO 2017-06-26 17:56:52,965 main.py:47] epoch 1609, training loss: 6779.83, average training loss: 7162.27, base loss: 8842.41
[INFO 2017-06-26 17:56:53,330 main.py:47] epoch 1610, training loss: 6514.59, average training loss: 7161.77, base loss: 8841.20
[INFO 2017-06-26 17:56:53,692 main.py:47] epoch 1611, training loss: 8176.27, average training loss: 7162.51, base loss: 8841.83
[INFO 2017-06-26 17:56:54,059 main.py:47] epoch 1612, training loss: 7825.74, average training loss: 7163.65, base loss: 8842.78
[INFO 2017-06-26 17:56:54,457 main.py:47] epoch 1613, training loss: 7389.85, average training loss: 7163.76, base loss: 8842.97
[INFO 2017-06-26 17:56:54,822 main.py:47] epoch 1614, training loss: 6915.32, average training loss: 7164.59, base loss: 8844.75
[INFO 2017-06-26 17:56:55,181 main.py:47] epoch 1615, training loss: 8163.23, average training loss: 7165.99, base loss: 8847.53
[INFO 2017-06-26 17:56:55,542 main.py:47] epoch 1616, training loss: 6510.72, average training loss: 7165.40, base loss: 8847.87
[INFO 2017-06-26 17:56:55,906 main.py:47] epoch 1617, training loss: 6723.30, average training loss: 7165.08, base loss: 8848.15
[INFO 2017-06-26 17:56:56,265 main.py:47] epoch 1618, training loss: 7382.69, average training loss: 7164.71, base loss: 8848.32
[INFO 2017-06-26 17:56:56,631 main.py:47] epoch 1619, training loss: 6640.30, average training loss: 7163.88, base loss: 8848.40
[INFO 2017-06-26 17:56:56,997 main.py:47] epoch 1620, training loss: 6988.36, average training loss: 7163.06, base loss: 8847.51
[INFO 2017-06-26 17:56:57,368 main.py:47] epoch 1621, training loss: 7074.32, average training loss: 7162.53, base loss: 8847.28
[INFO 2017-06-26 17:56:57,736 main.py:47] epoch 1622, training loss: 6073.28, average training loss: 7161.71, base loss: 8845.92
[INFO 2017-06-26 17:56:58,102 main.py:47] epoch 1623, training loss: 6176.15, average training loss: 7159.09, base loss: 8843.28
[INFO 2017-06-26 17:56:58,465 main.py:47] epoch 1624, training loss: 6952.57, average training loss: 7158.99, base loss: 8843.70
[INFO 2017-06-26 17:56:58,834 main.py:47] epoch 1625, training loss: 6885.54, average training loss: 7158.78, base loss: 8843.90
[INFO 2017-06-26 17:56:59,194 main.py:47] epoch 1626, training loss: 7709.34, average training loss: 7159.31, base loss: 8844.81
[INFO 2017-06-26 17:56:59,563 main.py:47] epoch 1627, training loss: 6927.16, average training loss: 7158.70, base loss: 8844.01
[INFO 2017-06-26 17:56:59,925 main.py:47] epoch 1628, training loss: 7084.49, average training loss: 7157.50, base loss: 8842.23
[INFO 2017-06-26 17:57:00,289 main.py:47] epoch 1629, training loss: 6977.27, average training loss: 7156.86, base loss: 8841.13
[INFO 2017-06-26 17:57:00,651 main.py:47] epoch 1630, training loss: 6481.14, average training loss: 7155.33, base loss: 8839.68
[INFO 2017-06-26 17:57:01,015 main.py:47] epoch 1631, training loss: 6101.14, average training loss: 7154.78, base loss: 8839.38
[INFO 2017-06-26 17:57:01,398 main.py:47] epoch 1632, training loss: 6903.42, average training loss: 7154.72, base loss: 8840.00
[INFO 2017-06-26 17:57:01,763 main.py:47] epoch 1633, training loss: 7648.28, average training loss: 7154.46, base loss: 8840.19
[INFO 2017-06-26 17:57:02,127 main.py:47] epoch 1634, training loss: 7048.11, average training loss: 7154.15, base loss: 8840.11
[INFO 2017-06-26 17:57:02,488 main.py:47] epoch 1635, training loss: 7579.14, average training loss: 7154.27, base loss: 8840.56
[INFO 2017-06-26 17:57:02,848 main.py:47] epoch 1636, training loss: 6477.02, average training loss: 7153.44, base loss: 8839.77
[INFO 2017-06-26 17:57:03,208 main.py:47] epoch 1637, training loss: 6863.51, average training loss: 7152.71, base loss: 8838.87
[INFO 2017-06-26 17:57:03,570 main.py:47] epoch 1638, training loss: 6189.36, average training loss: 7151.85, base loss: 8837.86
[INFO 2017-06-26 17:57:03,934 main.py:47] epoch 1639, training loss: 7123.33, average training loss: 7151.12, base loss: 8836.95
[INFO 2017-06-26 17:57:04,295 main.py:47] epoch 1640, training loss: 6820.90, average training loss: 7150.04, base loss: 8836.20
[INFO 2017-06-26 17:57:04,657 main.py:47] epoch 1641, training loss: 6851.61, average training loss: 7148.48, base loss: 8834.51
[INFO 2017-06-26 17:57:05,024 main.py:47] epoch 1642, training loss: 7080.63, average training loss: 7148.30, base loss: 8835.91
[INFO 2017-06-26 17:57:05,390 main.py:47] epoch 1643, training loss: 6706.06, average training loss: 7148.11, base loss: 8835.65
[INFO 2017-06-26 17:57:05,759 main.py:47] epoch 1644, training loss: 6697.13, average training loss: 7148.17, base loss: 8836.71
[INFO 2017-06-26 17:57:06,128 main.py:47] epoch 1645, training loss: 7087.10, average training loss: 7147.92, base loss: 8836.18
[INFO 2017-06-26 17:57:06,501 main.py:47] epoch 1646, training loss: 6988.66, average training loss: 7147.22, base loss: 8835.48
[INFO 2017-06-26 17:57:06,862 main.py:47] epoch 1647, training loss: 6703.22, average training loss: 7146.76, base loss: 8834.90
[INFO 2017-06-26 17:57:07,226 main.py:47] epoch 1648, training loss: 6581.16, average training loss: 7146.43, base loss: 8834.98
[INFO 2017-06-26 17:57:07,591 main.py:47] epoch 1649, training loss: 5972.03, average training loss: 7145.73, base loss: 8834.27
[INFO 2017-06-26 17:57:07,954 main.py:47] epoch 1650, training loss: 6315.47, average training loss: 7144.66, base loss: 8833.28
[INFO 2017-06-26 17:57:08,319 main.py:47] epoch 1651, training loss: 6883.03, average training loss: 7144.11, base loss: 8832.92
[INFO 2017-06-26 17:57:08,684 main.py:47] epoch 1652, training loss: 6903.27, average training loss: 7144.22, base loss: 8833.42
[INFO 2017-06-26 17:57:09,046 main.py:47] epoch 1653, training loss: 6343.49, average training loss: 7143.60, base loss: 8832.34
[INFO 2017-06-26 17:57:09,411 main.py:47] epoch 1654, training loss: 7505.47, average training loss: 7142.93, base loss: 8832.06
[INFO 2017-06-26 17:57:09,775 main.py:47] epoch 1655, training loss: 7843.89, average training loss: 7143.79, base loss: 8833.77
[INFO 2017-06-26 17:57:10,139 main.py:47] epoch 1656, training loss: 7004.77, average training loss: 7143.43, base loss: 8834.09
[INFO 2017-06-26 17:57:10,501 main.py:47] epoch 1657, training loss: 6603.88, average training loss: 7142.15, base loss: 8832.50
[INFO 2017-06-26 17:57:10,864 main.py:47] epoch 1658, training loss: 6695.32, average training loss: 7141.65, base loss: 8831.61
[INFO 2017-06-26 17:57:11,237 main.py:47] epoch 1659, training loss: 7088.10, average training loss: 7142.34, base loss: 8832.61
[INFO 2017-06-26 17:57:11,599 main.py:47] epoch 1660, training loss: 6314.95, average training loss: 7141.68, base loss: 8831.39
[INFO 2017-06-26 17:57:11,963 main.py:47] epoch 1661, training loss: 7233.87, average training loss: 7140.99, base loss: 8831.84
[INFO 2017-06-26 17:57:12,331 main.py:47] epoch 1662, training loss: 7798.31, average training loss: 7140.96, base loss: 8832.43
[INFO 2017-06-26 17:57:12,693 main.py:47] epoch 1663, training loss: 7673.93, average training loss: 7141.83, base loss: 8834.04
[INFO 2017-06-26 17:57:13,081 main.py:47] epoch 1664, training loss: 6676.34, average training loss: 7141.86, base loss: 8834.71
[INFO 2017-06-26 17:57:13,473 main.py:47] epoch 1665, training loss: 6384.43, average training loss: 7140.40, base loss: 8832.96
[INFO 2017-06-26 17:57:13,865 main.py:47] epoch 1666, training loss: 6297.07, average training loss: 7139.52, base loss: 8832.46
[INFO 2017-06-26 17:57:14,244 main.py:47] epoch 1667, training loss: 6622.01, average training loss: 7139.09, base loss: 8832.18
[INFO 2017-06-26 17:57:14,616 main.py:47] epoch 1668, training loss: 6706.25, average training loss: 7138.84, base loss: 8832.08
[INFO 2017-06-26 17:57:15,001 main.py:47] epoch 1669, training loss: 7831.80, average training loss: 7139.75, base loss: 8833.89
[INFO 2017-06-26 17:57:15,405 main.py:47] epoch 1670, training loss: 6348.64, average training loss: 7138.63, base loss: 8832.68
[INFO 2017-06-26 17:57:15,793 main.py:47] epoch 1671, training loss: 6539.74, average training loss: 7138.12, base loss: 8832.46
[INFO 2017-06-26 17:57:16,162 main.py:47] epoch 1672, training loss: 7794.47, average training loss: 7139.30, base loss: 8834.81
[INFO 2017-06-26 17:57:16,524 main.py:47] epoch 1673, training loss: 6995.75, average training loss: 7138.78, base loss: 8835.28
[INFO 2017-06-26 17:57:16,888 main.py:47] epoch 1674, training loss: 7925.82, average training loss: 7138.67, base loss: 8835.97
[INFO 2017-06-26 17:57:17,252 main.py:47] epoch 1675, training loss: 6955.45, average training loss: 7138.09, base loss: 8835.43
[INFO 2017-06-26 17:57:17,614 main.py:47] epoch 1676, training loss: 6350.00, average training loss: 7136.87, base loss: 8833.30
[INFO 2017-06-26 17:57:17,975 main.py:47] epoch 1677, training loss: 6681.33, average training loss: 7136.79, base loss: 8834.26
[INFO 2017-06-26 17:57:18,338 main.py:47] epoch 1678, training loss: 6598.06, average training loss: 7136.05, base loss: 8833.38
[INFO 2017-06-26 17:57:18,698 main.py:47] epoch 1679, training loss: 6895.31, average training loss: 7136.07, base loss: 8833.52
[INFO 2017-06-26 17:57:19,061 main.py:47] epoch 1680, training loss: 7181.96, average training loss: 7136.22, base loss: 8834.67
[INFO 2017-06-26 17:57:19,423 main.py:47] epoch 1681, training loss: 7283.12, average training loss: 7136.99, base loss: 8836.62
[INFO 2017-06-26 17:57:19,808 main.py:47] epoch 1682, training loss: 8319.31, average training loss: 7137.58, base loss: 8837.91
[INFO 2017-06-26 17:57:20,172 main.py:47] epoch 1683, training loss: 6657.81, average training loss: 7136.54, base loss: 8836.95
[INFO 2017-06-26 17:57:20,548 main.py:47] epoch 1684, training loss: 7538.27, average training loss: 7135.55, base loss: 8835.53
[INFO 2017-06-26 17:57:20,913 main.py:47] epoch 1685, training loss: 7345.96, average training loss: 7136.06, base loss: 8836.42
[INFO 2017-06-26 17:57:21,274 main.py:47] epoch 1686, training loss: 7255.99, average training loss: 7135.72, base loss: 8836.34
[INFO 2017-06-26 17:57:21,635 main.py:47] epoch 1687, training loss: 6476.41, average training loss: 7135.64, base loss: 8836.40
[INFO 2017-06-26 17:57:21,995 main.py:47] epoch 1688, training loss: 6762.04, average training loss: 7135.06, base loss: 8836.21
[INFO 2017-06-26 17:57:22,355 main.py:47] epoch 1689, training loss: 6793.13, average training loss: 7133.95, base loss: 8835.04
[INFO 2017-06-26 17:57:22,716 main.py:47] epoch 1690, training loss: 7342.16, average training loss: 7134.31, base loss: 8835.88
[INFO 2017-06-26 17:57:23,076 main.py:47] epoch 1691, training loss: 6153.94, average training loss: 7132.71, base loss: 8834.69
[INFO 2017-06-26 17:57:23,438 main.py:47] epoch 1692, training loss: 7343.86, average training loss: 7132.34, base loss: 8834.18
[INFO 2017-06-26 17:57:23,798 main.py:47] epoch 1693, training loss: 7687.92, average training loss: 7131.58, base loss: 8834.07
[INFO 2017-06-26 17:57:24,157 main.py:47] epoch 1694, training loss: 6607.20, average training loss: 7131.55, base loss: 8834.09
[INFO 2017-06-26 17:57:24,518 main.py:47] epoch 1695, training loss: 7260.80, average training loss: 7131.51, base loss: 8834.30
[INFO 2017-06-26 17:57:24,879 main.py:47] epoch 1696, training loss: 7179.27, average training loss: 7132.24, base loss: 8835.64
[INFO 2017-06-26 17:57:25,240 main.py:47] epoch 1697, training loss: 6787.19, average training loss: 7131.45, base loss: 8835.25
[INFO 2017-06-26 17:57:25,600 main.py:47] epoch 1698, training loss: 7953.19, average training loss: 7132.72, base loss: 8837.23
[INFO 2017-06-26 17:57:25,993 main.py:47] epoch 1699, training loss: 7169.31, average training loss: 7131.68, base loss: 8836.55
[INFO 2017-06-26 17:57:25,994 main.py:49] epoch 1699, testing
[INFO 2017-06-26 17:57:30,423 main.py:100] average testing loss: 6972.66, base loss: 8970.84
[INFO 2017-06-26 17:57:30,455 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:57:30,461 main.py:73] current best accuracy: 6972.66
[INFO 2017-06-26 17:57:30,823 main.py:47] epoch 1700, training loss: 6549.28, average training loss: 7130.85, base loss: 8835.74
[INFO 2017-06-26 17:57:31,187 main.py:47] epoch 1701, training loss: 6732.98, average training loss: 7130.50, base loss: 8835.68
[INFO 2017-06-26 17:57:31,549 main.py:47] epoch 1702, training loss: 6844.02, average training loss: 7130.98, base loss: 8836.23
[INFO 2017-06-26 17:57:31,909 main.py:47] epoch 1703, training loss: 7431.76, average training loss: 7130.86, base loss: 8837.22
[INFO 2017-06-26 17:57:32,269 main.py:47] epoch 1704, training loss: 7653.72, average training loss: 7131.72, base loss: 8838.34
[INFO 2017-06-26 17:57:32,630 main.py:47] epoch 1705, training loss: 7419.46, average training loss: 7132.48, base loss: 8840.20
[INFO 2017-06-26 17:57:32,991 main.py:47] epoch 1706, training loss: 7124.94, average training loss: 7132.03, base loss: 8840.09
[INFO 2017-06-26 17:57:33,354 main.py:47] epoch 1707, training loss: 6480.36, average training loss: 7131.82, base loss: 8839.79
[INFO 2017-06-26 17:57:33,713 main.py:47] epoch 1708, training loss: 7603.20, average training loss: 7132.20, base loss: 8841.18
[INFO 2017-06-26 17:57:34,076 main.py:47] epoch 1709, training loss: 7018.43, average training loss: 7132.17, base loss: 8842.00
[INFO 2017-06-26 17:57:34,437 main.py:47] epoch 1710, training loss: 7089.79, average training loss: 7132.38, base loss: 8842.71
[INFO 2017-06-26 17:57:34,798 main.py:47] epoch 1711, training loss: 7532.40, average training loss: 7132.70, base loss: 8843.27
[INFO 2017-06-26 17:57:35,158 main.py:47] epoch 1712, training loss: 6303.07, average training loss: 7132.11, base loss: 8843.09
[INFO 2017-06-26 17:57:35,521 main.py:47] epoch 1713, training loss: 7299.16, average training loss: 7132.75, base loss: 8844.42
[INFO 2017-06-26 17:57:35,882 main.py:47] epoch 1714, training loss: 6651.45, average training loss: 7132.19, base loss: 8844.11
[INFO 2017-06-26 17:57:36,242 main.py:47] epoch 1715, training loss: 6800.25, average training loss: 7131.26, base loss: 8843.07
[INFO 2017-06-26 17:57:36,603 main.py:47] epoch 1716, training loss: 7387.12, average training loss: 7131.03, base loss: 8843.10
[INFO 2017-06-26 17:57:36,965 main.py:47] epoch 1717, training loss: 6695.44, average training loss: 7130.64, base loss: 8842.89
[INFO 2017-06-26 17:57:37,330 main.py:47] epoch 1718, training loss: 7294.43, average training loss: 7131.12, base loss: 8844.19
[INFO 2017-06-26 17:57:37,690 main.py:47] epoch 1719, training loss: 7573.09, average training loss: 7130.98, base loss: 8844.43
[INFO 2017-06-26 17:57:38,049 main.py:47] epoch 1720, training loss: 7136.92, average training loss: 7130.27, base loss: 8844.11
[INFO 2017-06-26 17:57:38,410 main.py:47] epoch 1721, training loss: 7562.16, average training loss: 7130.36, base loss: 8844.51
[INFO 2017-06-26 17:57:38,769 main.py:47] epoch 1722, training loss: 6946.80, average training loss: 7130.27, base loss: 8844.32
[INFO 2017-06-26 17:57:39,129 main.py:47] epoch 1723, training loss: 6606.94, average training loss: 7130.25, base loss: 8843.78
[INFO 2017-06-26 17:57:39,490 main.py:47] epoch 1724, training loss: 6307.46, average training loss: 7129.50, base loss: 8843.29
[INFO 2017-06-26 17:57:39,851 main.py:47] epoch 1725, training loss: 6861.85, average training loss: 7129.11, base loss: 8843.01
[INFO 2017-06-26 17:57:40,211 main.py:47] epoch 1726, training loss: 7636.03, average training loss: 7128.60, base loss: 8842.26
[INFO 2017-06-26 17:57:40,572 main.py:47] epoch 1727, training loss: 6332.06, average training loss: 7127.40, base loss: 8840.70
[INFO 2017-06-26 17:57:40,934 main.py:47] epoch 1728, training loss: 6827.34, average training loss: 7127.20, base loss: 8840.40
[INFO 2017-06-26 17:57:41,296 main.py:47] epoch 1729, training loss: 6820.73, average training loss: 7127.12, base loss: 8840.37
[INFO 2017-06-26 17:57:41,655 main.py:47] epoch 1730, training loss: 6666.69, average training loss: 7126.84, base loss: 8840.64
[INFO 2017-06-26 17:57:42,052 main.py:47] epoch 1731, training loss: 6489.61, average training loss: 7126.66, base loss: 8840.48
[INFO 2017-06-26 17:57:42,413 main.py:47] epoch 1732, training loss: 6093.53, average training loss: 7125.69, base loss: 8840.11
[INFO 2017-06-26 17:57:42,781 main.py:47] epoch 1733, training loss: 7520.69, average training loss: 7126.31, base loss: 8841.95
[INFO 2017-06-26 17:57:43,143 main.py:47] epoch 1734, training loss: 6734.34, average training loss: 7126.15, base loss: 8842.02
[INFO 2017-06-26 17:57:43,504 main.py:47] epoch 1735, training loss: 6920.90, average training loss: 7125.62, base loss: 8841.05
[INFO 2017-06-26 17:57:43,864 main.py:47] epoch 1736, training loss: 7038.08, average training loss: 7125.29, base loss: 8840.71
[INFO 2017-06-26 17:57:44,228 main.py:47] epoch 1737, training loss: 7510.43, average training loss: 7125.44, base loss: 8840.69
[INFO 2017-06-26 17:57:44,591 main.py:47] epoch 1738, training loss: 7064.79, average training loss: 7125.24, base loss: 8841.31
[INFO 2017-06-26 17:57:44,952 main.py:47] epoch 1739, training loss: 7244.24, average training loss: 7124.91, base loss: 8840.67
[INFO 2017-06-26 17:57:45,314 main.py:47] epoch 1740, training loss: 6419.29, average training loss: 7123.98, base loss: 8839.67
[INFO 2017-06-26 17:57:45,677 main.py:47] epoch 1741, training loss: 7296.69, average training loss: 7124.58, base loss: 8841.51
[INFO 2017-06-26 17:57:46,038 main.py:47] epoch 1742, training loss: 6584.56, average training loss: 7124.40, base loss: 8842.31
[INFO 2017-06-26 17:57:46,401 main.py:47] epoch 1743, training loss: 7041.80, average training loss: 7124.01, base loss: 8842.73
[INFO 2017-06-26 17:57:46,847 main.py:47] epoch 1744, training loss: 6840.91, average training loss: 7123.56, base loss: 8842.23
[INFO 2017-06-26 17:57:47,211 main.py:47] epoch 1745, training loss: 6371.06, average training loss: 7122.38, base loss: 8841.04
[INFO 2017-06-26 17:57:47,576 main.py:47] epoch 1746, training loss: 7424.88, average training loss: 7122.39, base loss: 8840.95
[INFO 2017-06-26 17:57:47,949 main.py:47] epoch 1747, training loss: 6317.85, average training loss: 7121.07, base loss: 8839.60
[INFO 2017-06-26 17:57:48,317 main.py:47] epoch 1748, training loss: 7124.19, average training loss: 7120.40, base loss: 8839.02
[INFO 2017-06-26 17:57:48,681 main.py:47] epoch 1749, training loss: 7554.27, average training loss: 7120.09, base loss: 8839.03
[INFO 2017-06-26 17:57:49,041 main.py:47] epoch 1750, training loss: 6629.22, average training loss: 7120.44, base loss: 8839.28
[INFO 2017-06-26 17:57:49,402 main.py:47] epoch 1751, training loss: 7225.50, average training loss: 7120.88, base loss: 8840.40
[INFO 2017-06-26 17:57:49,765 main.py:47] epoch 1752, training loss: 8148.82, average training loss: 7122.02, base loss: 8841.92
[INFO 2017-06-26 17:57:50,159 main.py:47] epoch 1753, training loss: 6756.63, average training loss: 7121.68, base loss: 8841.98
[INFO 2017-06-26 17:57:50,535 main.py:47] epoch 1754, training loss: 6908.58, average training loss: 7120.00, base loss: 8840.29
[INFO 2017-06-26 17:57:50,915 main.py:47] epoch 1755, training loss: 7711.06, average training loss: 7120.17, base loss: 8840.83
[INFO 2017-06-26 17:57:51,279 main.py:47] epoch 1756, training loss: 6576.15, average training loss: 7119.33, base loss: 8840.28
[INFO 2017-06-26 17:57:51,672 main.py:47] epoch 1757, training loss: 6322.30, average training loss: 7118.45, base loss: 8839.37
[INFO 2017-06-26 17:57:52,045 main.py:47] epoch 1758, training loss: 8461.64, average training loss: 7119.75, base loss: 8841.48
[INFO 2017-06-26 17:57:52,419 main.py:47] epoch 1759, training loss: 6734.68, average training loss: 7118.56, base loss: 8840.17
[INFO 2017-06-26 17:57:52,827 main.py:47] epoch 1760, training loss: 6463.33, average training loss: 7118.64, base loss: 8840.25
[INFO 2017-06-26 17:57:53,231 main.py:47] epoch 1761, training loss: 8543.29, average training loss: 7120.80, base loss: 8843.88
[INFO 2017-06-26 17:57:53,600 main.py:47] epoch 1762, training loss: 6711.91, average training loss: 7120.91, base loss: 8844.74
[INFO 2017-06-26 17:57:53,966 main.py:47] epoch 1763, training loss: 6627.23, average training loss: 7119.36, base loss: 8843.58
[INFO 2017-06-26 17:57:54,328 main.py:47] epoch 1764, training loss: 6954.25, average training loss: 7118.86, base loss: 8843.26
[INFO 2017-06-26 17:57:54,691 main.py:47] epoch 1765, training loss: 6563.70, average training loss: 7118.28, base loss: 8843.00
[INFO 2017-06-26 17:57:55,053 main.py:47] epoch 1766, training loss: 6512.62, average training loss: 7117.28, base loss: 8841.57
[INFO 2017-06-26 17:57:55,416 main.py:47] epoch 1767, training loss: 6274.17, average training loss: 7116.84, base loss: 8841.16
[INFO 2017-06-26 17:57:55,779 main.py:47] epoch 1768, training loss: 7042.77, average training loss: 7115.94, base loss: 8840.54
[INFO 2017-06-26 17:57:56,141 main.py:47] epoch 1769, training loss: 6575.83, average training loss: 7115.65, base loss: 8840.59
[INFO 2017-06-26 17:57:56,501 main.py:47] epoch 1770, training loss: 6811.14, average training loss: 7116.21, base loss: 8841.81
[INFO 2017-06-26 17:57:56,864 main.py:47] epoch 1771, training loss: 5910.93, average training loss: 7114.35, base loss: 8839.51
[INFO 2017-06-26 17:57:57,225 main.py:47] epoch 1772, training loss: 6855.37, average training loss: 7115.06, base loss: 8840.78
[INFO 2017-06-26 17:57:57,587 main.py:47] epoch 1773, training loss: 6122.67, average training loss: 7113.91, base loss: 8839.54
[INFO 2017-06-26 17:57:57,948 main.py:47] epoch 1774, training loss: 6975.50, average training loss: 7113.46, base loss: 8839.74
[INFO 2017-06-26 17:57:58,345 main.py:47] epoch 1775, training loss: 7147.28, average training loss: 7113.23, base loss: 8840.11
[INFO 2017-06-26 17:57:58,710 main.py:47] epoch 1776, training loss: 6358.40, average training loss: 7112.32, base loss: 8838.89
[INFO 2017-06-26 17:57:59,080 main.py:47] epoch 1777, training loss: 6089.29, average training loss: 7111.47, base loss: 8838.51
[INFO 2017-06-26 17:57:59,446 main.py:47] epoch 1778, training loss: 7007.21, average training loss: 7111.01, base loss: 8838.51
[INFO 2017-06-26 17:57:59,807 main.py:47] epoch 1779, training loss: 6994.18, average training loss: 7110.55, base loss: 8838.36
[INFO 2017-06-26 17:58:00,204 main.py:47] epoch 1780, training loss: 7090.66, average training loss: 7110.91, base loss: 8839.36
[INFO 2017-06-26 17:58:00,566 main.py:47] epoch 1781, training loss: 6747.15, average training loss: 7110.30, base loss: 8838.45
[INFO 2017-06-26 17:58:00,931 main.py:47] epoch 1782, training loss: 6476.43, average training loss: 7109.54, base loss: 8837.31
[INFO 2017-06-26 17:58:01,293 main.py:47] epoch 1783, training loss: 6597.72, average training loss: 7109.96, base loss: 8837.66
[INFO 2017-06-26 17:58:01,690 main.py:47] epoch 1784, training loss: 6216.88, average training loss: 7109.85, base loss: 8837.72
[INFO 2017-06-26 17:58:02,054 main.py:47] epoch 1785, training loss: 7425.69, average training loss: 7110.73, base loss: 8838.75
[INFO 2017-06-26 17:58:02,417 main.py:47] epoch 1786, training loss: 7435.52, average training loss: 7111.20, base loss: 8839.29
[INFO 2017-06-26 17:58:02,781 main.py:47] epoch 1787, training loss: 7034.85, average training loss: 7110.96, base loss: 8839.58
[INFO 2017-06-26 17:58:03,142 main.py:47] epoch 1788, training loss: 6785.67, average training loss: 7110.88, base loss: 8840.11
[INFO 2017-06-26 17:58:03,499 main.py:47] epoch 1789, training loss: 6460.80, average training loss: 7109.81, base loss: 8839.04
[INFO 2017-06-26 17:58:03,859 main.py:47] epoch 1790, training loss: 5992.33, average training loss: 7108.06, base loss: 8837.10
[INFO 2017-06-26 17:58:04,219 main.py:47] epoch 1791, training loss: 6246.87, average training loss: 7107.27, base loss: 8836.32
[INFO 2017-06-26 17:58:04,580 main.py:47] epoch 1792, training loss: 7186.11, average training loss: 7108.27, base loss: 8837.85
[INFO 2017-06-26 17:58:04,942 main.py:47] epoch 1793, training loss: 6297.30, average training loss: 7106.97, base loss: 8836.11
[INFO 2017-06-26 17:58:05,337 main.py:47] epoch 1794, training loss: 7546.11, average training loss: 7106.36, base loss: 8835.81
[INFO 2017-06-26 17:58:05,729 main.py:47] epoch 1795, training loss: 6419.94, average training loss: 7106.28, base loss: 8836.05
[INFO 2017-06-26 17:58:06,110 main.py:47] epoch 1796, training loss: 7494.07, average training loss: 7107.11, base loss: 8837.63
[INFO 2017-06-26 17:58:06,471 main.py:47] epoch 1797, training loss: 7548.10, average training loss: 7107.43, base loss: 8838.98
[INFO 2017-06-26 17:58:06,831 main.py:47] epoch 1798, training loss: 7355.62, average training loss: 7106.42, base loss: 8837.84
[INFO 2017-06-26 17:58:07,190 main.py:47] epoch 1799, training loss: 6787.70, average training loss: 7104.83, base loss: 8836.25
[INFO 2017-06-26 17:58:07,190 main.py:49] epoch 1799, testing
[INFO 2017-06-26 17:58:11,433 main.py:100] average testing loss: 7013.45, base loss: 8925.95
[INFO 2017-06-26 17:58:11,456 main.py:73] current best accuracy: 6972.66
[INFO 2017-06-26 17:58:11,815 main.py:47] epoch 1800, training loss: 6675.44, average training loss: 7103.79, base loss: 8834.63
[INFO 2017-06-26 17:58:12,176 main.py:47] epoch 1801, training loss: 7040.90, average training loss: 7103.80, base loss: 8834.49
[INFO 2017-06-26 17:58:12,538 main.py:47] epoch 1802, training loss: 6836.46, average training loss: 7103.06, base loss: 8834.24
[INFO 2017-06-26 17:58:12,899 main.py:47] epoch 1803, training loss: 6580.41, average training loss: 7102.04, base loss: 8833.13
[INFO 2017-06-26 17:58:13,258 main.py:47] epoch 1804, training loss: 6067.60, average training loss: 7101.48, base loss: 8832.65
[INFO 2017-06-26 17:58:13,618 main.py:47] epoch 1805, training loss: 6917.93, average training loss: 7099.98, base loss: 8830.73
[INFO 2017-06-26 17:58:13,979 main.py:47] epoch 1806, training loss: 7123.78, average training loss: 7100.33, base loss: 8831.54
[INFO 2017-06-26 17:58:14,343 main.py:47] epoch 1807, training loss: 5963.69, average training loss: 7100.01, base loss: 8830.83
[INFO 2017-06-26 17:58:14,702 main.py:47] epoch 1808, training loss: 7297.91, average training loss: 7100.07, base loss: 8831.32
[INFO 2017-06-26 17:58:15,062 main.py:47] epoch 1809, training loss: 6960.54, average training loss: 7099.54, base loss: 8830.51
[INFO 2017-06-26 17:58:15,422 main.py:47] epoch 1810, training loss: 7189.82, average training loss: 7098.52, base loss: 8829.55
[INFO 2017-06-26 17:58:15,784 main.py:47] epoch 1811, training loss: 7052.37, average training loss: 7097.60, base loss: 8828.73
[INFO 2017-06-26 17:58:16,143 main.py:47] epoch 1812, training loss: 7084.74, average training loss: 7096.34, base loss: 8827.56
[INFO 2017-06-26 17:58:16,504 main.py:47] epoch 1813, training loss: 7164.62, average training loss: 7095.45, base loss: 8827.39
[INFO 2017-06-26 17:58:16,864 main.py:47] epoch 1814, training loss: 6648.36, average training loss: 7095.26, base loss: 8827.07
[INFO 2017-06-26 17:58:17,224 main.py:47] epoch 1815, training loss: 6967.67, average training loss: 7095.23, base loss: 8826.81
[INFO 2017-06-26 17:58:17,585 main.py:47] epoch 1816, training loss: 7102.93, average training loss: 7094.87, base loss: 8826.85
[INFO 2017-06-26 17:58:17,946 main.py:47] epoch 1817, training loss: 7582.72, average training loss: 7095.30, base loss: 8827.47
[INFO 2017-06-26 17:58:18,307 main.py:47] epoch 1818, training loss: 6389.70, average training loss: 7095.22, base loss: 8827.27
[INFO 2017-06-26 17:58:18,666 main.py:47] epoch 1819, training loss: 7150.21, average training loss: 7094.56, base loss: 8826.71
[INFO 2017-06-26 17:58:19,034 main.py:47] epoch 1820, training loss: 7394.06, average training loss: 7094.43, base loss: 8826.95
[INFO 2017-06-26 17:58:19,398 main.py:47] epoch 1821, training loss: 7514.67, average training loss: 7094.75, base loss: 8826.87
[INFO 2017-06-26 17:58:19,757 main.py:47] epoch 1822, training loss: 9134.02, average training loss: 7096.58, base loss: 8829.46
[INFO 2017-06-26 17:58:20,119 main.py:47] epoch 1823, training loss: 7482.14, average training loss: 7097.54, base loss: 8831.10
[INFO 2017-06-26 17:58:20,480 main.py:47] epoch 1824, training loss: 6255.99, average training loss: 7097.68, base loss: 8831.40
[INFO 2017-06-26 17:58:20,841 main.py:47] epoch 1825, training loss: 5955.03, average training loss: 7096.65, base loss: 8829.50
[INFO 2017-06-26 17:58:21,200 main.py:47] epoch 1826, training loss: 7394.76, average training loss: 7095.27, base loss: 8828.22
[INFO 2017-06-26 17:58:21,561 main.py:47] epoch 1827, training loss: 7079.46, average training loss: 7094.38, base loss: 8827.75
[INFO 2017-06-26 17:58:22,107 main.py:47] epoch 1828, training loss: 6422.02, average training loss: 7093.17, base loss: 8825.96
[INFO 2017-06-26 17:58:22,478 main.py:47] epoch 1829, training loss: 7060.02, average training loss: 7093.08, base loss: 8826.03
[INFO 2017-06-26 17:58:22,842 main.py:47] epoch 1830, training loss: 6488.38, average training loss: 7092.22, base loss: 8824.90
[INFO 2017-06-26 17:58:23,209 main.py:47] epoch 1831, training loss: 6639.22, average training loss: 7091.29, base loss: 8823.91
[INFO 2017-06-26 17:58:23,569 main.py:47] epoch 1832, training loss: 5850.54, average training loss: 7090.01, base loss: 8822.02
[INFO 2017-06-26 17:58:23,929 main.py:47] epoch 1833, training loss: 6530.83, average training loss: 7088.88, base loss: 8821.34
[INFO 2017-06-26 17:58:24,290 main.py:47] epoch 1834, training loss: 7617.99, average training loss: 7089.63, base loss: 8822.88
[INFO 2017-06-26 17:58:24,657 main.py:47] epoch 1835, training loss: 7296.71, average training loss: 7089.87, base loss: 8824.03
[INFO 2017-06-26 17:58:25,020 main.py:47] epoch 1836, training loss: 6971.42, average training loss: 7089.11, base loss: 8822.84
[INFO 2017-06-26 17:58:25,382 main.py:47] epoch 1837, training loss: 6113.27, average training loss: 7087.94, base loss: 8821.12
[INFO 2017-06-26 17:58:25,742 main.py:47] epoch 1838, training loss: 7195.42, average training loss: 7088.75, base loss: 8822.16
[INFO 2017-06-26 17:58:26,103 main.py:47] epoch 1839, training loss: 7840.57, average training loss: 7089.95, base loss: 8824.15
[INFO 2017-06-26 17:58:26,462 main.py:47] epoch 1840, training loss: 6708.82, average training loss: 7089.05, base loss: 8822.79
[INFO 2017-06-26 17:58:26,822 main.py:47] epoch 1841, training loss: 7215.29, average training loss: 7087.49, base loss: 8820.66
[INFO 2017-06-26 17:58:27,184 main.py:47] epoch 1842, training loss: 6695.62, average training loss: 7086.96, base loss: 8820.61
[INFO 2017-06-26 17:58:27,543 main.py:47] epoch 1843, training loss: 7259.25, average training loss: 7087.17, base loss: 8821.12
[INFO 2017-06-26 17:58:27,903 main.py:47] epoch 1844, training loss: 7430.46, average training loss: 7087.99, base loss: 8822.68
[INFO 2017-06-26 17:58:28,264 main.py:47] epoch 1845, training loss: 7039.25, average training loss: 7088.59, base loss: 8822.94
[INFO 2017-06-26 17:58:28,624 main.py:47] epoch 1846, training loss: 7651.17, average training loss: 7089.39, base loss: 8824.90
[INFO 2017-06-26 17:58:28,985 main.py:47] epoch 1847, training loss: 6321.00, average training loss: 7088.38, base loss: 8823.65
[INFO 2017-06-26 17:58:29,344 main.py:47] epoch 1848, training loss: 6316.40, average training loss: 7088.20, base loss: 8824.48
[INFO 2017-06-26 17:58:29,708 main.py:47] epoch 1849, training loss: 7060.41, average training loss: 7086.71, base loss: 8822.80
[INFO 2017-06-26 17:58:30,073 main.py:47] epoch 1850, training loss: 6670.29, average training loss: 7085.80, base loss: 8821.71
[INFO 2017-06-26 17:58:30,433 main.py:47] epoch 1851, training loss: 6804.66, average training loss: 7084.69, base loss: 8820.85
[INFO 2017-06-26 17:58:30,798 main.py:47] epoch 1852, training loss: 6573.26, average training loss: 7083.62, base loss: 8819.80
[INFO 2017-06-26 17:58:31,160 main.py:47] epoch 1853, training loss: 6922.49, average training loss: 7083.44, base loss: 8819.73
[INFO 2017-06-26 17:58:31,521 main.py:47] epoch 1854, training loss: 6981.84, average training loss: 7083.57, base loss: 8820.35
[INFO 2017-06-26 17:58:31,890 main.py:47] epoch 1855, training loss: 6253.75, average training loss: 7082.39, base loss: 8818.83
[INFO 2017-06-26 17:58:32,251 main.py:47] epoch 1856, training loss: 6896.22, average training loss: 7082.08, base loss: 8818.03
[INFO 2017-06-26 17:58:32,612 main.py:47] epoch 1857, training loss: 6402.50, average training loss: 7080.92, base loss: 8816.76
[INFO 2017-06-26 17:58:32,972 main.py:47] epoch 1858, training loss: 6496.20, average training loss: 7080.37, base loss: 8816.26
[INFO 2017-06-26 17:58:33,334 main.py:47] epoch 1859, training loss: 6417.26, average training loss: 7079.53, base loss: 8816.02
[INFO 2017-06-26 17:58:33,727 main.py:47] epoch 1860, training loss: 7914.99, average training loss: 7079.00, base loss: 8815.19
[INFO 2017-06-26 17:58:34,103 main.py:47] epoch 1861, training loss: 7018.56, average training loss: 7079.37, base loss: 8816.19
[INFO 2017-06-26 17:58:34,473 main.py:47] epoch 1862, training loss: 6661.72, average training loss: 7079.11, base loss: 8815.86
[INFO 2017-06-26 17:58:34,834 main.py:47] epoch 1863, training loss: 7693.23, average training loss: 7078.88, base loss: 8815.82
[INFO 2017-06-26 17:58:35,228 main.py:47] epoch 1864, training loss: 6733.11, average training loss: 7078.37, base loss: 8815.38
[INFO 2017-06-26 17:58:35,602 main.py:47] epoch 1865, training loss: 7632.44, average training loss: 7078.15, base loss: 8815.01
[INFO 2017-06-26 17:58:35,967 main.py:47] epoch 1866, training loss: 6385.37, average training loss: 7077.02, base loss: 8813.74
[INFO 2017-06-26 17:58:36,335 main.py:47] epoch 1867, training loss: 7311.50, average training loss: 7076.92, base loss: 8814.03
[INFO 2017-06-26 17:58:36,695 main.py:47] epoch 1868, training loss: 7885.94, average training loss: 7077.32, base loss: 8814.79
[INFO 2017-06-26 17:58:37,088 main.py:47] epoch 1869, training loss: 7233.97, average training loss: 7077.52, base loss: 8815.09
[INFO 2017-06-26 17:58:37,467 main.py:47] epoch 1870, training loss: 6864.32, average training loss: 7077.45, base loss: 8814.79
[INFO 2017-06-26 17:58:37,838 main.py:47] epoch 1871, training loss: 7067.73, average training loss: 7078.16, base loss: 8815.70
[INFO 2017-06-26 17:58:38,211 main.py:47] epoch 1872, training loss: 7080.74, average training loss: 7077.70, base loss: 8815.85
[INFO 2017-06-26 17:58:38,587 main.py:47] epoch 1873, training loss: 6930.94, average training loss: 7077.26, base loss: 8815.96
[INFO 2017-06-26 17:58:38,949 main.py:47] epoch 1874, training loss: 7295.74, average training loss: 7077.93, base loss: 8817.21
[INFO 2017-06-26 17:58:39,311 main.py:47] epoch 1875, training loss: 8446.96, average training loss: 7079.02, base loss: 8818.73
[INFO 2017-06-26 17:58:39,673 main.py:47] epoch 1876, training loss: 7125.86, average training loss: 7078.01, base loss: 8818.02
[INFO 2017-06-26 17:58:40,035 main.py:47] epoch 1877, training loss: 7597.13, average training loss: 7078.53, base loss: 8819.62
[INFO 2017-06-26 17:58:40,398 main.py:47] epoch 1878, training loss: 7773.26, average training loss: 7079.29, base loss: 8821.80
[INFO 2017-06-26 17:58:40,757 main.py:47] epoch 1879, training loss: 7107.13, average training loss: 7079.59, base loss: 8821.83
[INFO 2017-06-26 17:58:41,118 main.py:47] epoch 1880, training loss: 7158.57, average training loss: 7080.42, base loss: 8823.01
[INFO 2017-06-26 17:58:41,481 main.py:47] epoch 1881, training loss: 6508.25, average training loss: 7078.29, base loss: 8820.12
[INFO 2017-06-26 17:58:41,842 main.py:47] epoch 1882, training loss: 6938.48, average training loss: 7077.97, base loss: 8819.66
[INFO 2017-06-26 17:58:42,218 main.py:47] epoch 1883, training loss: 6482.66, average training loss: 7077.36, base loss: 8819.20
[INFO 2017-06-26 17:58:42,577 main.py:47] epoch 1884, training loss: 7522.05, average training loss: 7077.67, base loss: 8820.26
[INFO 2017-06-26 17:58:42,941 main.py:47] epoch 1885, training loss: 7270.80, average training loss: 7078.40, base loss: 8821.30
[INFO 2017-06-26 17:58:43,306 main.py:47] epoch 1886, training loss: 6476.93, average training loss: 7077.31, base loss: 8820.77
[INFO 2017-06-26 17:58:43,665 main.py:47] epoch 1887, training loss: 6531.34, average training loss: 7075.32, base loss: 8817.99
[INFO 2017-06-26 17:58:44,026 main.py:47] epoch 1888, training loss: 6195.78, average training loss: 7073.44, base loss: 8815.69
[INFO 2017-06-26 17:58:44,388 main.py:47] epoch 1889, training loss: 7209.42, average training loss: 7073.47, base loss: 8815.52
[INFO 2017-06-26 17:58:44,747 main.py:47] epoch 1890, training loss: 7424.64, average training loss: 7074.08, base loss: 8816.77
[INFO 2017-06-26 17:58:45,108 main.py:47] epoch 1891, training loss: 6300.82, average training loss: 7073.31, base loss: 8816.21
[INFO 2017-06-26 17:58:45,468 main.py:47] epoch 1892, training loss: 7617.40, average training loss: 7073.47, base loss: 8817.18
[INFO 2017-06-26 17:58:45,826 main.py:47] epoch 1893, training loss: 6116.55, average training loss: 7071.62, base loss: 8815.36
[INFO 2017-06-26 17:58:46,190 main.py:47] epoch 1894, training loss: 6959.53, average training loss: 7071.11, base loss: 8814.72
[INFO 2017-06-26 17:58:46,551 main.py:47] epoch 1895, training loss: 6762.27, average training loss: 7070.75, base loss: 8814.34
[INFO 2017-06-26 17:58:46,911 main.py:47] epoch 1896, training loss: 7820.55, average training loss: 7070.38, base loss: 8813.62
[INFO 2017-06-26 17:58:47,276 main.py:47] epoch 1897, training loss: 7406.89, average training loss: 7071.02, base loss: 8815.11
[INFO 2017-06-26 17:58:47,635 main.py:47] epoch 1898, training loss: 6764.12, average training loss: 7070.95, base loss: 8814.81
[INFO 2017-06-26 17:58:47,995 main.py:47] epoch 1899, training loss: 7314.08, average training loss: 7070.90, base loss: 8814.71
[INFO 2017-06-26 17:58:47,996 main.py:49] epoch 1899, testing
[INFO 2017-06-26 17:58:52,190 main.py:100] average testing loss: 6964.30, base loss: 8758.71
[INFO 2017-06-26 17:58:52,214 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:58:52,221 main.py:73] current best accuracy: 6964.30
[INFO 2017-06-26 17:58:52,580 main.py:47] epoch 1900, training loss: 6838.79, average training loss: 7070.46, base loss: 8814.04
[INFO 2017-06-26 17:58:52,942 main.py:47] epoch 1901, training loss: 7574.18, average training loss: 7071.38, base loss: 8815.75
[INFO 2017-06-26 17:58:53,304 main.py:47] epoch 1902, training loss: 7116.65, average training loss: 7071.41, base loss: 8815.89
[INFO 2017-06-26 17:58:53,666 main.py:47] epoch 1903, training loss: 6968.72, average training loss: 7071.01, base loss: 8814.86
[INFO 2017-06-26 17:58:54,025 main.py:47] epoch 1904, training loss: 8718.34, average training loss: 7072.33, base loss: 8816.82
[INFO 2017-06-26 17:58:54,385 main.py:47] epoch 1905, training loss: 6792.08, average training loss: 7073.15, base loss: 8818.74
[INFO 2017-06-26 17:58:54,743 main.py:47] epoch 1906, training loss: 6863.52, average training loss: 7072.81, base loss: 8818.51
[INFO 2017-06-26 17:58:55,102 main.py:47] epoch 1907, training loss: 7219.55, average training loss: 7072.68, base loss: 8818.01
[INFO 2017-06-26 17:58:55,461 main.py:47] epoch 1908, training loss: 6187.59, average training loss: 7070.95, base loss: 8814.95
[INFO 2017-06-26 17:58:55,819 main.py:47] epoch 1909, training loss: 6489.84, average training loss: 7071.06, base loss: 8814.80
[INFO 2017-06-26 17:58:56,182 main.py:47] epoch 1910, training loss: 7613.76, average training loss: 7071.33, base loss: 8816.11
[INFO 2017-06-26 17:58:56,543 main.py:47] epoch 1911, training loss: 6397.51, average training loss: 7069.62, base loss: 8813.31
[INFO 2017-06-26 17:58:56,905 main.py:47] epoch 1912, training loss: 6871.68, average training loss: 7069.61, base loss: 8813.61
[INFO 2017-06-26 17:58:57,269 main.py:47] epoch 1913, training loss: 6584.61, average training loss: 7068.61, base loss: 8812.28
[INFO 2017-06-26 17:58:57,632 main.py:47] epoch 1914, training loss: 7205.53, average training loss: 7068.29, base loss: 8812.68
[INFO 2017-06-26 17:58:58,028 main.py:47] epoch 1915, training loss: 6476.83, average training loss: 7067.18, base loss: 8811.33
[INFO 2017-06-26 17:58:58,399 main.py:47] epoch 1916, training loss: 7509.73, average training loss: 7067.20, base loss: 8811.63
[INFO 2017-06-26 17:58:58,765 main.py:47] epoch 1917, training loss: 6497.89, average training loss: 7066.49, base loss: 8811.02
[INFO 2017-06-26 17:58:59,127 main.py:47] epoch 1918, training loss: 6931.64, average training loss: 7066.65, base loss: 8812.10
[INFO 2017-06-26 17:58:59,523 main.py:47] epoch 1919, training loss: 6823.25, average training loss: 7066.13, base loss: 8811.53
[INFO 2017-06-26 17:58:59,903 main.py:47] epoch 1920, training loss: 7114.00, average training loss: 7066.25, base loss: 8811.64
[INFO 2017-06-26 17:59:00,310 main.py:47] epoch 1921, training loss: 7049.00, average training loss: 7066.35, base loss: 8812.69
[INFO 2017-06-26 17:59:00,731 main.py:47] epoch 1922, training loss: 6983.31, average training loss: 7066.66, base loss: 8813.48
[INFO 2017-06-26 17:59:01,098 main.py:47] epoch 1923, training loss: 7344.22, average training loss: 7066.89, base loss: 8814.23
[INFO 2017-06-26 17:59:01,462 main.py:47] epoch 1924, training loss: 7190.99, average training loss: 7067.46, base loss: 8815.09
[INFO 2017-06-26 17:59:01,822 main.py:47] epoch 1925, training loss: 7376.86, average training loss: 7068.24, base loss: 8817.25
[INFO 2017-06-26 17:59:02,183 main.py:47] epoch 1926, training loss: 7524.83, average training loss: 7068.36, base loss: 8817.88
[INFO 2017-06-26 17:59:02,545 main.py:47] epoch 1927, training loss: 6274.08, average training loss: 7067.38, base loss: 8817.18
[INFO 2017-06-26 17:59:02,908 main.py:47] epoch 1928, training loss: 6765.84, average training loss: 7068.20, base loss: 8818.30
[INFO 2017-06-26 17:59:03,270 main.py:47] epoch 1929, training loss: 7115.35, average training loss: 7068.14, base loss: 8818.15
[INFO 2017-06-26 17:59:03,632 main.py:47] epoch 1930, training loss: 6213.29, average training loss: 7066.20, base loss: 8815.44
[INFO 2017-06-26 17:59:03,992 main.py:47] epoch 1931, training loss: 6700.41, average training loss: 7065.95, base loss: 8815.29
[INFO 2017-06-26 17:59:04,352 main.py:47] epoch 1932, training loss: 7027.42, average training loss: 7066.06, base loss: 8815.54
[INFO 2017-06-26 17:59:04,714 main.py:47] epoch 1933, training loss: 6456.67, average training loss: 7065.14, base loss: 8814.24
[INFO 2017-06-26 17:59:05,073 main.py:47] epoch 1934, training loss: 7607.07, average training loss: 7065.26, base loss: 8815.16
[INFO 2017-06-26 17:59:05,435 main.py:47] epoch 1935, training loss: 8496.81, average training loss: 7064.93, base loss: 8815.15
[INFO 2017-06-26 17:59:05,794 main.py:47] epoch 1936, training loss: 6087.06, average training loss: 7063.69, base loss: 8813.56
[INFO 2017-06-26 17:59:06,155 main.py:47] epoch 1937, training loss: 6798.94, average training loss: 7063.80, base loss: 8814.59
[INFO 2017-06-26 17:59:06,519 main.py:47] epoch 1938, training loss: 6846.38, average training loss: 7063.53, base loss: 8814.60
[INFO 2017-06-26 17:59:06,880 main.py:47] epoch 1939, training loss: 6211.05, average training loss: 7062.64, base loss: 8813.35
[INFO 2017-06-26 17:59:07,239 main.py:47] epoch 1940, training loss: 6763.96, average training loss: 7062.10, base loss: 8812.55
[INFO 2017-06-26 17:59:07,597 main.py:47] epoch 1941, training loss: 6173.37, average training loss: 7060.23, base loss: 8809.86
[INFO 2017-06-26 17:59:07,958 main.py:47] epoch 1942, training loss: 6433.40, average training loss: 7060.27, base loss: 8810.22
[INFO 2017-06-26 17:59:08,318 main.py:47] epoch 1943, training loss: 6959.49, average training loss: 7060.79, base loss: 8811.12
[INFO 2017-06-26 17:59:08,677 main.py:47] epoch 1944, training loss: 7677.53, average training loss: 7060.47, base loss: 8811.14
[INFO 2017-06-26 17:59:09,036 main.py:47] epoch 1945, training loss: 6836.18, average training loss: 7059.60, base loss: 8810.38
[INFO 2017-06-26 17:59:09,397 main.py:47] epoch 1946, training loss: 7068.18, average training loss: 7059.75, base loss: 8811.16
[INFO 2017-06-26 17:59:09,756 main.py:47] epoch 1947, training loss: 7525.53, average training loss: 7059.49, base loss: 8811.26
[INFO 2017-06-26 17:59:10,117 main.py:47] epoch 1948, training loss: 7845.41, average training loss: 7060.86, base loss: 8813.85
[INFO 2017-06-26 17:59:10,475 main.py:47] epoch 1949, training loss: 7636.98, average training loss: 7061.69, base loss: 8815.38
[INFO 2017-06-26 17:59:10,836 main.py:47] epoch 1950, training loss: 6372.33, average training loss: 7061.50, base loss: 8815.68
[INFO 2017-06-26 17:59:11,198 main.py:47] epoch 1951, training loss: 7090.16, average training loss: 7061.57, base loss: 8816.28
[INFO 2017-06-26 17:59:11,556 main.py:47] epoch 1952, training loss: 6955.48, average training loss: 7061.22, base loss: 8815.92
[INFO 2017-06-26 17:59:11,917 main.py:47] epoch 1953, training loss: 6447.51, average training loss: 7060.25, base loss: 8814.43
[INFO 2017-06-26 17:59:12,278 main.py:47] epoch 1954, training loss: 6575.37, average training loss: 7059.85, base loss: 8814.89
[INFO 2017-06-26 17:59:12,640 main.py:47] epoch 1955, training loss: 6337.76, average training loss: 7058.12, base loss: 8812.76
[INFO 2017-06-26 17:59:13,002 main.py:47] epoch 1956, training loss: 6436.15, average training loss: 7057.99, base loss: 8812.84
[INFO 2017-06-26 17:59:13,362 main.py:47] epoch 1957, training loss: 6720.30, average training loss: 7057.46, base loss: 8811.97
[INFO 2017-06-26 17:59:13,723 main.py:47] epoch 1958, training loss: 7695.33, average training loss: 7058.11, base loss: 8813.04
[INFO 2017-06-26 17:59:14,083 main.py:47] epoch 1959, training loss: 8887.29, average training loss: 7059.83, base loss: 8815.38
[INFO 2017-06-26 17:59:14,444 main.py:47] epoch 1960, training loss: 6550.43, average training loss: 7058.94, base loss: 8814.57
[INFO 2017-06-26 17:59:14,805 main.py:47] epoch 1961, training loss: 7816.20, average training loss: 7059.99, base loss: 8816.25
[INFO 2017-06-26 17:59:15,166 main.py:47] epoch 1962, training loss: 6900.19, average training loss: 7058.83, base loss: 8814.83
[INFO 2017-06-26 17:59:15,527 main.py:47] epoch 1963, training loss: 7240.73, average training loss: 7059.90, base loss: 8816.44
[INFO 2017-06-26 17:59:15,887 main.py:47] epoch 1964, training loss: 6624.19, average training loss: 7059.78, base loss: 8816.21
[INFO 2017-06-26 17:59:16,247 main.py:47] epoch 1965, training loss: 7125.94, average training loss: 7060.00, base loss: 8816.47
[INFO 2017-06-26 17:59:16,606 main.py:47] epoch 1966, training loss: 6610.56, average training loss: 7059.80, base loss: 8816.72
[INFO 2017-06-26 17:59:16,968 main.py:47] epoch 1967, training loss: 7663.84, average training loss: 7061.30, base loss: 8819.12
[INFO 2017-06-26 17:59:17,329 main.py:47] epoch 1968, training loss: 6651.14, average training loss: 7059.71, base loss: 8817.39
[INFO 2017-06-26 17:59:17,688 main.py:47] epoch 1969, training loss: 7176.49, average training loss: 7058.56, base loss: 8817.04
[INFO 2017-06-26 17:59:18,049 main.py:47] epoch 1970, training loss: 7349.20, average training loss: 7058.78, base loss: 8817.73
[INFO 2017-06-26 17:59:18,410 main.py:47] epoch 1971, training loss: 6276.01, average training loss: 7057.80, base loss: 8816.68
[INFO 2017-06-26 17:59:18,770 main.py:47] epoch 1972, training loss: 6857.33, average training loss: 7056.59, base loss: 8815.54
[INFO 2017-06-26 17:59:19,130 main.py:47] epoch 1973, training loss: 6491.64, average training loss: 7056.63, base loss: 8815.38
[INFO 2017-06-26 17:59:19,492 main.py:47] epoch 1974, training loss: 6952.85, average training loss: 7056.58, base loss: 8815.89
[INFO 2017-06-26 17:59:19,854 main.py:47] epoch 1975, training loss: 7558.52, average training loss: 7056.65, base loss: 8816.92
[INFO 2017-06-26 17:59:20,217 main.py:47] epoch 1976, training loss: 6552.38, average training loss: 7056.35, base loss: 8817.13
[INFO 2017-06-26 17:59:20,580 main.py:47] epoch 1977, training loss: 7658.68, average training loss: 7055.99, base loss: 8816.06
[INFO 2017-06-26 17:59:20,941 main.py:47] epoch 1978, training loss: 6889.60, average training loss: 7055.28, base loss: 8815.54
[INFO 2017-06-26 17:59:21,305 main.py:47] epoch 1979, training loss: 6582.67, average training loss: 7054.70, base loss: 8815.12
[INFO 2017-06-26 17:59:21,669 main.py:47] epoch 1980, training loss: 6560.96, average training loss: 7054.85, base loss: 8815.63
[INFO 2017-06-26 17:59:22,031 main.py:47] epoch 1981, training loss: 6375.32, average training loss: 7054.55, base loss: 8815.64
[INFO 2017-06-26 17:59:22,394 main.py:47] epoch 1982, training loss: 6753.38, average training loss: 7054.27, base loss: 8815.51
[INFO 2017-06-26 17:59:22,761 main.py:47] epoch 1983, training loss: 7450.45, average training loss: 7053.73, base loss: 8815.29
[INFO 2017-06-26 17:59:23,124 main.py:47] epoch 1984, training loss: 7018.44, average training loss: 7053.39, base loss: 8815.86
[INFO 2017-06-26 17:59:23,490 main.py:47] epoch 1985, training loss: 7140.87, average training loss: 7053.30, base loss: 8815.97
[INFO 2017-06-26 17:59:23,854 main.py:47] epoch 1986, training loss: 6836.37, average training loss: 7052.27, base loss: 8815.14
[INFO 2017-06-26 17:59:24,218 main.py:47] epoch 1987, training loss: 6999.67, average training loss: 7051.81, base loss: 8815.21
[INFO 2017-06-26 17:59:24,582 main.py:47] epoch 1988, training loss: 6293.79, average training loss: 7052.00, base loss: 8815.35
[INFO 2017-06-26 17:59:24,947 main.py:47] epoch 1989, training loss: 7067.95, average training loss: 7052.33, base loss: 8816.32
[INFO 2017-06-26 17:59:25,311 main.py:47] epoch 1990, training loss: 7034.55, average training loss: 7052.55, base loss: 8816.46
[INFO 2017-06-26 17:59:25,676 main.py:47] epoch 1991, training loss: 7210.44, average training loss: 7052.28, base loss: 8815.84
[INFO 2017-06-26 17:59:26,040 main.py:47] epoch 1992, training loss: 7022.11, average training loss: 7051.92, base loss: 8815.29
[INFO 2017-06-26 17:59:26,403 main.py:47] epoch 1993, training loss: 7508.56, average training loss: 7052.00, base loss: 8816.40
[INFO 2017-06-26 17:59:26,767 main.py:47] epoch 1994, training loss: 6525.90, average training loss: 7050.39, base loss: 8814.77
[INFO 2017-06-26 17:59:27,132 main.py:47] epoch 1995, training loss: 6662.73, average training loss: 7049.97, base loss: 8814.45
[INFO 2017-06-26 17:59:27,496 main.py:47] epoch 1996, training loss: 7586.64, average training loss: 7049.17, base loss: 8813.42
[INFO 2017-06-26 17:59:27,862 main.py:47] epoch 1997, training loss: 6680.51, average training loss: 7048.07, base loss: 8811.54
[INFO 2017-06-26 17:59:28,225 main.py:47] epoch 1998, training loss: 6555.08, average training loss: 7047.87, base loss: 8811.27
[INFO 2017-06-26 17:59:28,590 main.py:47] epoch 1999, training loss: 6352.52, average training loss: 7047.43, base loss: 8810.69
[INFO 2017-06-26 17:59:28,590 main.py:49] epoch 1999, testing
[INFO 2017-06-26 17:59:33,010 main.py:100] average testing loss: 7023.51, base loss: 9037.63
[INFO 2017-06-26 17:59:33,033 main.py:73] current best accuracy: 6964.30
[INFO 2017-06-26 17:59:33,396 main.py:47] epoch 2000, training loss: 6854.88, average training loss: 7047.41, base loss: 8810.27
[INFO 2017-06-26 17:59:33,757 main.py:47] epoch 2001, training loss: 6429.61, average training loss: 7047.65, base loss: 8810.66
[INFO 2017-06-26 17:59:34,117 main.py:47] epoch 2002, training loss: 6436.25, average training loss: 7047.56, base loss: 8811.36
[INFO 2017-06-26 17:59:34,480 main.py:47] epoch 2003, training loss: 6590.55, average training loss: 7045.94, base loss: 8809.51
[INFO 2017-06-26 17:59:34,850 main.py:47] epoch 2004, training loss: 6955.49, average training loss: 7046.62, base loss: 8811.01
[INFO 2017-06-26 17:59:35,208 main.py:47] epoch 2005, training loss: 7152.26, average training loss: 7045.61, base loss: 8810.10
[INFO 2017-06-26 17:59:35,571 main.py:47] epoch 2006, training loss: 6184.55, average training loss: 7044.26, base loss: 8808.71
[INFO 2017-06-26 17:59:35,934 main.py:47] epoch 2007, training loss: 7672.00, average training loss: 7043.83, base loss: 8808.42
[INFO 2017-06-26 17:59:36,294 main.py:47] epoch 2008, training loss: 6704.45, average training loss: 7043.73, base loss: 8808.59
[INFO 2017-06-26 17:59:36,656 main.py:47] epoch 2009, training loss: 7265.73, average training loss: 7043.89, base loss: 8808.52
[INFO 2017-06-26 17:59:37,016 main.py:47] epoch 2010, training loss: 6960.90, average training loss: 7044.65, base loss: 8809.60
[INFO 2017-06-26 17:59:37,377 main.py:47] epoch 2011, training loss: 7183.68, average training loss: 7044.74, base loss: 8810.34
[INFO 2017-06-26 17:59:37,738 main.py:47] epoch 2012, training loss: 6740.99, average training loss: 7044.68, base loss: 8810.77
[INFO 2017-06-26 17:59:38,099 main.py:47] epoch 2013, training loss: 6556.12, average training loss: 7043.98, base loss: 8809.85
[INFO 2017-06-26 17:59:38,461 main.py:47] epoch 2014, training loss: 7054.79, average training loss: 7042.96, base loss: 8808.92
[INFO 2017-06-26 17:59:38,822 main.py:47] epoch 2015, training loss: 6228.55, average training loss: 7042.70, base loss: 8809.05
[INFO 2017-06-26 17:59:39,184 main.py:47] epoch 2016, training loss: 6602.72, average training loss: 7042.24, base loss: 8808.45
[INFO 2017-06-26 17:59:39,548 main.py:47] epoch 2017, training loss: 7950.93, average training loss: 7043.49, base loss: 8810.96
[INFO 2017-06-26 17:59:39,907 main.py:47] epoch 2018, training loss: 6491.53, average training loss: 7043.46, base loss: 8810.60
[INFO 2017-06-26 17:59:40,269 main.py:47] epoch 2019, training loss: 7070.44, average training loss: 7043.87, base loss: 8811.51
[INFO 2017-06-26 17:59:40,631 main.py:47] epoch 2020, training loss: 6329.26, average training loss: 7040.66, base loss: 8807.87
[INFO 2017-06-26 17:59:40,992 main.py:47] epoch 2021, training loss: 6412.92, average training loss: 7039.87, base loss: 8806.91
[INFO 2017-06-26 17:59:41,352 main.py:47] epoch 2022, training loss: 6791.97, average training loss: 7039.22, base loss: 8806.41
[INFO 2017-06-26 17:59:41,712 main.py:47] epoch 2023, training loss: 7060.96, average training loss: 7039.23, base loss: 8806.70
[INFO 2017-06-26 17:59:42,072 main.py:47] epoch 2024, training loss: 6376.09, average training loss: 7037.99, base loss: 8805.25
[INFO 2017-06-26 17:59:42,432 main.py:47] epoch 2025, training loss: 7425.65, average training loss: 7038.74, base loss: 8806.54
[INFO 2017-06-26 17:59:42,793 main.py:47] epoch 2026, training loss: 6720.91, average training loss: 7037.66, base loss: 8806.63
[INFO 2017-06-26 17:59:43,154 main.py:47] epoch 2027, training loss: 7949.11, average training loss: 7037.20, base loss: 8806.22
[INFO 2017-06-26 17:59:43,517 main.py:47] epoch 2028, training loss: 6601.64, average training loss: 7036.94, base loss: 8806.26
[INFO 2017-06-26 17:59:43,878 main.py:47] epoch 2029, training loss: 7272.00, average training loss: 7037.50, base loss: 8807.12
[INFO 2017-06-26 17:59:44,240 main.py:47] epoch 2030, training loss: 7211.41, average training loss: 7036.99, base loss: 8806.99
[INFO 2017-06-26 17:59:44,600 main.py:47] epoch 2031, training loss: 6677.55, average training loss: 7037.12, base loss: 8807.14
[INFO 2017-06-26 17:59:44,961 main.py:47] epoch 2032, training loss: 6484.02, average training loss: 7036.78, base loss: 8807.06
[INFO 2017-06-26 17:59:45,324 main.py:47] epoch 2033, training loss: 7508.69, average training loss: 7037.22, base loss: 8806.96
[INFO 2017-06-26 17:59:45,682 main.py:47] epoch 2034, training loss: 8052.41, average training loss: 7038.57, base loss: 8809.28
[INFO 2017-06-26 17:59:46,044 main.py:47] epoch 2035, training loss: 6818.25, average training loss: 7037.86, base loss: 8809.09
[INFO 2017-06-26 17:59:46,406 main.py:47] epoch 2036, training loss: 6628.90, average training loss: 7036.93, base loss: 8808.29
[INFO 2017-06-26 17:59:46,765 main.py:47] epoch 2037, training loss: 7026.39, average training loss: 7037.15, base loss: 8809.15
[INFO 2017-06-26 17:59:47,127 main.py:47] epoch 2038, training loss: 7231.92, average training loss: 7037.72, base loss: 8809.83
[INFO 2017-06-26 17:59:47,486 main.py:47] epoch 2039, training loss: 6752.55, average training loss: 7036.17, base loss: 8808.14
[INFO 2017-06-26 17:59:47,849 main.py:47] epoch 2040, training loss: 6899.64, average training loss: 7034.90, base loss: 8806.37
[INFO 2017-06-26 17:59:48,209 main.py:47] epoch 2041, training loss: 6303.55, average training loss: 7034.09, base loss: 8805.13
[INFO 2017-06-26 17:59:48,571 main.py:47] epoch 2042, training loss: 6369.93, average training loss: 7034.01, base loss: 8805.25
[INFO 2017-06-26 17:59:48,932 main.py:47] epoch 2043, training loss: 8005.51, average training loss: 7034.82, base loss: 8806.09
[INFO 2017-06-26 17:59:49,294 main.py:47] epoch 2044, training loss: 7212.42, average training loss: 7034.88, base loss: 8806.26
[INFO 2017-06-26 17:59:49,651 main.py:47] epoch 2045, training loss: 6224.83, average training loss: 7034.67, base loss: 8806.36
[INFO 2017-06-26 17:59:50,014 main.py:47] epoch 2046, training loss: 6921.67, average training loss: 7035.39, base loss: 8807.76
[INFO 2017-06-26 17:59:50,376 main.py:47] epoch 2047, training loss: 7740.68, average training loss: 7035.36, base loss: 8808.42
[INFO 2017-06-26 17:59:50,739 main.py:47] epoch 2048, training loss: 6951.33, average training loss: 7034.01, base loss: 8807.26
[INFO 2017-06-26 17:59:51,104 main.py:47] epoch 2049, training loss: 6579.24, average training loss: 7032.75, base loss: 8805.96
[INFO 2017-06-26 17:59:51,469 main.py:47] epoch 2050, training loss: 7756.45, average training loss: 7033.73, base loss: 8808.03
[INFO 2017-06-26 17:59:51,832 main.py:47] epoch 2051, training loss: 7281.71, average training loss: 7034.26, base loss: 8809.67
[INFO 2017-06-26 17:59:52,195 main.py:47] epoch 2052, training loss: 6966.67, average training loss: 7033.64, base loss: 8809.57
[INFO 2017-06-26 17:59:52,559 main.py:47] epoch 2053, training loss: 7104.88, average training loss: 7034.37, base loss: 8810.48
[INFO 2017-06-26 17:59:52,922 main.py:47] epoch 2054, training loss: 6976.09, average training loss: 7033.16, base loss: 8808.77
[INFO 2017-06-26 17:59:53,287 main.py:47] epoch 2055, training loss: 6411.34, average training loss: 7032.19, base loss: 8807.63
[INFO 2017-06-26 17:59:53,666 main.py:47] epoch 2056, training loss: 7058.71, average training loss: 7032.90, base loss: 8809.16
[INFO 2017-06-26 17:59:54,027 main.py:47] epoch 2057, training loss: 6797.05, average training loss: 7032.04, base loss: 8808.30
[INFO 2017-06-26 17:59:54,388 main.py:47] epoch 2058, training loss: 6267.00, average training loss: 7031.84, base loss: 8809.05
[INFO 2017-06-26 17:59:54,747 main.py:47] epoch 2059, training loss: 7106.24, average training loss: 7031.91, base loss: 8809.68
[INFO 2017-06-26 17:59:55,111 main.py:47] epoch 2060, training loss: 6838.30, average training loss: 7032.40, base loss: 8810.56
[INFO 2017-06-26 17:59:55,470 main.py:47] epoch 2061, training loss: 6773.78, average training loss: 7031.67, base loss: 8809.69
[INFO 2017-06-26 17:59:55,832 main.py:47] epoch 2062, training loss: 6702.23, average training loss: 7031.20, base loss: 8809.43
[INFO 2017-06-26 17:59:56,193 main.py:47] epoch 2063, training loss: 6173.63, average training loss: 7029.28, base loss: 8806.94
[INFO 2017-06-26 17:59:56,554 main.py:47] epoch 2064, training loss: 7032.98, average training loss: 7029.42, base loss: 8807.20
[INFO 2017-06-26 17:59:56,916 main.py:47] epoch 2065, training loss: 6928.15, average training loss: 7029.71, base loss: 8807.68
[INFO 2017-06-26 17:59:57,275 main.py:47] epoch 2066, training loss: 7006.79, average training loss: 7030.34, base loss: 8808.87
[INFO 2017-06-26 17:59:57,637 main.py:47] epoch 2067, training loss: 6470.86, average training loss: 7029.21, base loss: 8807.63
[INFO 2017-06-26 17:59:57,999 main.py:47] epoch 2068, training loss: 7566.14, average training loss: 7029.35, base loss: 8808.61
[INFO 2017-06-26 17:59:58,359 main.py:47] epoch 2069, training loss: 6634.44, average training loss: 7028.41, base loss: 8807.57
[INFO 2017-06-26 17:59:58,720 main.py:47] epoch 2070, training loss: 7139.43, average training loss: 7029.21, base loss: 8809.08
[INFO 2017-06-26 17:59:59,079 main.py:47] epoch 2071, training loss: 7335.64, average training loss: 7029.37, base loss: 8810.05
[INFO 2017-06-26 17:59:59,440 main.py:47] epoch 2072, training loss: 6518.90, average training loss: 7027.90, base loss: 8808.75
[INFO 2017-06-26 17:59:59,799 main.py:47] epoch 2073, training loss: 6694.30, average training loss: 7027.11, base loss: 8807.89
[INFO 2017-06-26 18:00:00,161 main.py:47] epoch 2074, training loss: 6738.77, average training loss: 7027.09, base loss: 8808.14
[INFO 2017-06-26 18:00:00,520 main.py:47] epoch 2075, training loss: 6049.40, average training loss: 7025.92, base loss: 8806.98
[INFO 2017-06-26 18:00:00,882 main.py:47] epoch 2076, training loss: 6337.35, average training loss: 7025.24, base loss: 8805.98
[INFO 2017-06-26 18:00:01,247 main.py:47] epoch 2077, training loss: 6784.79, average training loss: 7025.15, base loss: 8805.72
[INFO 2017-06-26 18:00:01,612 main.py:47] epoch 2078, training loss: 7283.11, average training loss: 7025.74, base loss: 8806.64
[INFO 2017-06-26 18:00:01,979 main.py:47] epoch 2079, training loss: 7105.45, average training loss: 7025.27, base loss: 8805.94
[INFO 2017-06-26 18:00:02,344 main.py:47] epoch 2080, training loss: 6770.87, average training loss: 7024.41, base loss: 8805.32
[INFO 2017-06-26 18:00:02,708 main.py:47] epoch 2081, training loss: 7934.12, average training loss: 7025.41, base loss: 8807.29
[INFO 2017-06-26 18:00:03,073 main.py:47] epoch 2082, training loss: 6550.22, average training loss: 7024.90, base loss: 8807.27
[INFO 2017-06-26 18:00:03,439 main.py:47] epoch 2083, training loss: 8085.12, average training loss: 7026.10, base loss: 8808.49
[INFO 2017-06-26 18:00:03,804 main.py:47] epoch 2084, training loss: 7107.51, average training loss: 7025.98, base loss: 8808.94
[INFO 2017-06-26 18:00:04,165 main.py:47] epoch 2085, training loss: 7359.40, average training loss: 7026.53, base loss: 8809.51
[INFO 2017-06-26 18:00:04,524 main.py:47] epoch 2086, training loss: 6380.03, average training loss: 7025.01, base loss: 8807.70
[INFO 2017-06-26 18:00:04,890 main.py:47] epoch 2087, training loss: 7204.47, average training loss: 7025.09, base loss: 8807.95
[INFO 2017-06-26 18:00:05,254 main.py:47] epoch 2088, training loss: 7076.99, average training loss: 7025.33, base loss: 8808.17
[INFO 2017-06-26 18:00:05,615 main.py:47] epoch 2089, training loss: 6809.32, average training loss: 7024.79, base loss: 8807.37
[INFO 2017-06-26 18:00:05,976 main.py:47] epoch 2090, training loss: 6732.47, average training loss: 7023.83, base loss: 8805.99
[INFO 2017-06-26 18:00:06,339 main.py:47] epoch 2091, training loss: 6514.18, average training loss: 7023.52, base loss: 8805.82
[INFO 2017-06-26 18:00:06,700 main.py:47] epoch 2092, training loss: 6688.18, average training loss: 7022.83, base loss: 8804.54
[INFO 2017-06-26 18:00:07,062 main.py:47] epoch 2093, training loss: 6341.88, average training loss: 7021.34, base loss: 8802.76
[INFO 2017-06-26 18:00:07,424 main.py:47] epoch 2094, training loss: 6636.35, average training loss: 7020.82, base loss: 8802.00
[INFO 2017-06-26 18:00:07,787 main.py:47] epoch 2095, training loss: 6697.08, average training loss: 7021.53, base loss: 8803.09
[INFO 2017-06-26 18:00:08,147 main.py:47] epoch 2096, training loss: 7400.76, average training loss: 7022.59, base loss: 8805.20
[INFO 2017-06-26 18:00:08,509 main.py:47] epoch 2097, training loss: 7525.56, average training loss: 7022.65, base loss: 8805.79
[INFO 2017-06-26 18:00:08,870 main.py:47] epoch 2098, training loss: 7892.24, average training loss: 7022.82, base loss: 8806.31
[INFO 2017-06-26 18:00:09,233 main.py:47] epoch 2099, training loss: 6942.62, average training loss: 7023.51, base loss: 8808.90
[INFO 2017-06-26 18:00:09,233 main.py:49] epoch 2099, testing
[INFO 2017-06-26 18:00:13,424 main.py:100] average testing loss: 6904.51, base loss: 8896.60
[INFO 2017-06-26 18:00:13,449 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 18:00:13,456 main.py:73] current best accuracy: 6904.51
[INFO 2017-06-26 18:00:13,815 main.py:47] epoch 2100, training loss: 7300.42, average training loss: 7023.10, base loss: 8807.94
[INFO 2017-06-26 18:00:14,177 main.py:47] epoch 2101, training loss: 6916.10, average training loss: 7023.49, base loss: 8808.96
[INFO 2017-06-26 18:00:14,538 main.py:47] epoch 2102, training loss: 7206.82, average training loss: 7023.28, base loss: 8809.07
[INFO 2017-06-26 18:00:14,899 main.py:47] epoch 2103, training loss: 7576.77, average training loss: 7023.87, base loss: 8810.53
[INFO 2017-06-26 18:00:15,260 main.py:47] epoch 2104, training loss: 6740.05, average training loss: 7023.83, base loss: 8810.71
[INFO 2017-06-26 18:00:15,621 main.py:47] epoch 2105, training loss: 7337.10, average training loss: 7023.63, base loss: 8811.42
[INFO 2017-06-26 18:00:15,980 main.py:47] epoch 2106, training loss: 7080.04, average training loss: 7023.97, base loss: 8812.43
[INFO 2017-06-26 18:00:16,340 main.py:47] epoch 2107, training loss: 6960.39, average training loss: 7023.70, base loss: 8811.36
[INFO 2017-06-26 18:00:16,701 main.py:47] epoch 2108, training loss: 6060.65, average training loss: 7023.25, base loss: 8810.62
[INFO 2017-06-26 18:00:17,063 main.py:47] epoch 2109, training loss: 7172.43, average training loss: 7022.67, base loss: 8810.64
[INFO 2017-06-26 18:00:17,422 main.py:47] epoch 2110, training loss: 6924.81, average training loss: 7022.33, base loss: 8810.69
[INFO 2017-06-26 18:00:17,782 main.py:47] epoch 2111, training loss: 7291.41, average training loss: 7023.03, base loss: 8812.90
[INFO 2017-06-26 18:00:18,142 main.py:47] epoch 2112, training loss: 6483.74, average training loss: 7022.39, base loss: 8812.65
[INFO 2017-06-26 18:00:18,501 main.py:47] epoch 2113, training loss: 6764.24, average training loss: 7022.41, base loss: 8812.62
[INFO 2017-06-26 18:00:18,860 main.py:47] epoch 2114, training loss: 6284.20, average training loss: 7022.37, base loss: 8812.47
[INFO 2017-06-26 18:00:19,220 main.py:47] epoch 2115, training loss: 7083.17, average training loss: 7021.71, base loss: 8812.15
[INFO 2017-06-26 18:00:19,582 main.py:47] epoch 2116, training loss: 6368.83, average training loss: 7022.12, base loss: 8813.43
[INFO 2017-06-26 18:00:19,943 main.py:47] epoch 2117, training loss: 7762.59, average training loss: 7023.51, base loss: 8814.72
[INFO 2017-06-26 18:00:20,304 main.py:47] epoch 2118, training loss: 6896.45, average training loss: 7024.05, base loss: 8816.20
[INFO 2017-06-26 18:00:20,664 main.py:47] epoch 2119, training loss: 6453.00, average training loss: 7023.71, base loss: 8815.59
[INFO 2017-06-26 18:00:21,029 main.py:47] epoch 2120, training loss: 7165.66, average training loss: 7024.03, base loss: 8816.35
[INFO 2017-06-26 18:00:21,391 main.py:47] epoch 2121, training loss: 6829.41, average training loss: 7020.97, base loss: 8812.82
[INFO 2017-06-26 18:00:21,752 main.py:47] epoch 2122, training loss: 6751.60, average training loss: 7020.23, base loss: 8811.92
[INFO 2017-06-26 18:00:22,114 main.py:47] epoch 2123, training loss: 6639.03, average training loss: 7019.72, base loss: 8810.86
[INFO 2017-06-26 18:00:22,473 main.py:47] epoch 2124, training loss: 6915.23, average training loss: 7018.85, base loss: 8810.20
[INFO 2017-06-26 18:00:22,834 main.py:47] epoch 2125, training loss: 7354.66, average training loss: 7018.81, base loss: 8811.51
[INFO 2017-06-26 18:00:23,195 main.py:47] epoch 2126, training loss: 7147.65, average training loss: 7018.65, base loss: 8811.71
[INFO 2017-06-26 18:00:23,558 main.py:47] epoch 2127, training loss: 6905.09, average training loss: 7018.24, base loss: 8810.58
[INFO 2017-06-26 18:00:23,917 main.py:47] epoch 2128, training loss: 6082.47, average training loss: 7017.76, base loss: 8810.08
[INFO 2017-06-26 18:00:24,277 main.py:47] epoch 2129, training loss: 6467.78, average training loss: 7016.68, base loss: 8809.18
[INFO 2017-06-26 18:00:24,639 main.py:47] epoch 2130, training loss: 7708.13, average training loss: 7016.52, base loss: 8809.43
[INFO 2017-06-26 18:00:24,999 main.py:47] epoch 2131, training loss: 7990.12, average training loss: 7018.18, base loss: 8812.52
[INFO 2017-06-26 18:00:25,360 main.py:47] epoch 2132, training loss: 6282.40, average training loss: 7016.90, base loss: 8811.28
[INFO 2017-06-26 18:00:25,723 main.py:47] epoch 2133, training loss: 6654.02, average training loss: 7017.19, base loss: 8812.12
[INFO 2017-06-26 18:00:26,082 main.py:47] epoch 2134, training loss: 6571.45, average training loss: 7016.41, base loss: 8811.14
[INFO 2017-06-26 18:00:26,443 main.py:47] epoch 2135, training loss: 6655.53, average training loss: 7016.07, base loss: 8811.05
[INFO 2017-06-26 18:00:26,804 main.py:47] epoch 2136, training loss: 6344.79, average training loss: 7015.04, base loss: 8809.61
[INFO 2017-06-26 18:00:27,164 main.py:47] epoch 2137, training loss: 6786.68, average training loss: 7015.40, base loss: 8810.61
[INFO 2017-06-26 18:00:27,524 main.py:47] epoch 2138, training loss: 7430.68, average training loss: 7014.68, base loss: 8809.78
[INFO 2017-06-26 18:00:27,885 main.py:47] epoch 2139, training loss: 7296.35, average training loss: 7015.08, base loss: 8810.63
[INFO 2017-06-26 18:00:28,247 main.py:47] epoch 2140, training loss: 6567.12, average training loss: 7014.69, base loss: 8809.68
[INFO 2017-06-26 18:00:28,609 main.py:47] epoch 2141, training loss: 7351.94, average training loss: 7014.75, base loss: 8810.41
[INFO 2017-06-26 18:00:28,972 main.py:47] epoch 2142, training loss: 7118.07, average training loss: 7014.54, base loss: 8810.23
[INFO 2017-06-26 18:00:29,334 main.py:47] epoch 2143, training loss: 7243.94, average training loss: 7014.79, base loss: 8811.01
[INFO 2017-06-26 18:00:29,694 main.py:47] epoch 2144, training loss: 6385.80, average training loss: 7012.99, base loss: 8808.71
[INFO 2017-06-26 18:00:30,055 main.py:47] epoch 2145, training loss: 7173.16, average training loss: 7014.03, base loss: 8810.71
[INFO 2017-06-26 18:00:30,416 main.py:47] epoch 2146, training loss: 7387.54, average training loss: 7013.96, base loss: 8811.05
[INFO 2017-06-26 18:00:30,777 main.py:47] epoch 2147, training loss: 6200.09, average training loss: 7013.30, base loss: 8809.77
[INFO 2017-06-26 18:00:31,141 main.py:47] epoch 2148, training loss: 7125.35, average training loss: 7013.51, base loss: 8810.90
[INFO 2017-06-26 18:00:31,501 main.py:47] epoch 2149, training loss: 8350.96, average training loss: 7014.63, base loss: 8813.07
[INFO 2017-06-26 18:00:31,863 main.py:47] epoch 2150, training loss: 6298.12, average training loss: 7014.50, base loss: 8813.75
[INFO 2017-06-26 18:00:32,221 main.py:47] epoch 2151, training loss: 6313.99, average training loss: 7014.31, base loss: 8813.09
[INFO 2017-06-26 18:00:32,589 main.py:47] epoch 2152, training loss: 6734.94, average training loss: 7014.57, base loss: 8812.87
[INFO 2017-06-26 18:00:32,956 main.py:47] epoch 2153, training loss: 6558.30, average training loss: 7013.28, base loss: 8811.44
[INFO 2017-06-26 18:00:33,321 main.py:47] epoch 2154, training loss: 7389.94, average training loss: 7014.35, base loss: 8813.48
[INFO 2017-06-26 18:00:33,686 main.py:47] epoch 2155, training loss: 7737.24, average training loss: 7014.70, base loss: 8813.85
[INFO 2017-06-26 18:00:34,049 main.py:47] epoch 2156, training loss: 6851.73, average training loss: 7013.37, base loss: 8812.65
[INFO 2017-06-26 18:00:34,413 main.py:47] epoch 2157, training loss: 6655.13, average training loss: 7013.07, base loss: 8812.68
[INFO 2017-06-26 18:00:34,779 main.py:47] epoch 2158, training loss: 6163.24, average training loss: 7011.24, base loss: 8810.93
[INFO 2017-06-26 18:00:35,140 main.py:47] epoch 2159, training loss: 7011.82, average training loss: 7012.16, base loss: 8812.65
[INFO 2017-06-26 18:00:35,500 main.py:47] epoch 2160, training loss: 7187.39, average training loss: 7012.06, base loss: 8813.06
[INFO 2017-06-26 18:00:35,860 main.py:47] epoch 2161, training loss: 7361.92, average training loss: 7012.42, base loss: 8814.14
[INFO 2017-06-26 18:00:36,221 main.py:47] epoch 2162, training loss: 6280.81, average training loss: 7011.39, base loss: 8812.64
[INFO 2017-06-26 18:00:36,582 main.py:47] epoch 2163, training loss: 6147.23, average training loss: 7010.34, base loss: 8811.22
[INFO 2017-06-26 18:00:36,943 main.py:47] epoch 2164, training loss: 6645.42, average training loss: 7009.69, base loss: 8810.84
[INFO 2017-06-26 18:00:37,303 main.py:47] epoch 2165, training loss: 7073.68, average training loss: 7008.76, base loss: 8810.09
[INFO 2017-06-26 18:00:37,666 main.py:47] epoch 2166, training loss: 6766.77, average training loss: 7007.99, base loss: 8809.90
[INFO 2017-06-26 18:00:38,026 main.py:47] epoch 2167, training loss: 6312.95, average training loss: 7007.96, base loss: 8810.29
[INFO 2017-06-26 18:00:38,388 main.py:47] epoch 2168, training loss: 8429.10, average training loss: 7009.04, base loss: 8811.95
[INFO 2017-06-26 18:00:38,749 main.py:47] epoch 2169, training loss: 8375.17, average training loss: 7010.70, base loss: 8814.13
[INFO 2017-06-26 18:00:39,112 main.py:47] epoch 2170, training loss: 8309.67, average training loss: 7011.96, base loss: 8815.84
[INFO 2017-06-26 18:00:39,474 main.py:47] epoch 2171, training loss: 7118.24, average training loss: 7012.34, base loss: 8816.74
[INFO 2017-06-26 18:00:39,834 main.py:47] epoch 2172, training loss: 6631.51, average training loss: 7011.06, base loss: 8815.73
[INFO 2017-06-26 18:00:40,198 main.py:47] epoch 2173, training loss: 6457.87, average training loss: 7010.45, base loss: 8814.64
[INFO 2017-06-26 18:00:40,562 main.py:47] epoch 2174, training loss: 6712.38, average training loss: 7010.47, base loss: 8815.40
[INFO 2017-06-26 18:00:40,923 main.py:47] epoch 2175, training loss: 8152.67, average training loss: 7012.54, base loss: 8819.13
[INFO 2017-06-26 18:00:41,283 main.py:47] epoch 2176, training loss: 6910.77, average training loss: 7012.67, base loss: 8819.19
[INFO 2017-06-26 18:00:41,644 main.py:47] epoch 2177, training loss: 7236.34, average training loss: 7013.16, base loss: 8819.48
[INFO 2017-06-26 18:00:42,004 main.py:47] epoch 2178, training loss: 7903.25, average training loss: 7013.29, base loss: 8820.68
[INFO 2017-06-26 18:00:42,366 main.py:47] epoch 2179, training loss: 6523.83, average training loss: 7012.90, base loss: 8820.25
[INFO 2017-06-26 18:00:42,726 main.py:47] epoch 2180, training loss: 7001.50, average training loss: 7013.28, base loss: 8821.93
[INFO 2017-06-26 18:00:43,085 main.py:47] epoch 2181, training loss: 7051.47, average training loss: 7013.58, base loss: 8822.64
[INFO 2017-06-26 18:00:43,445 main.py:47] epoch 2182, training loss: 7417.39, average training loss: 7013.95, base loss: 8823.23
[INFO 2017-06-26 18:00:43,806 main.py:47] epoch 2183, training loss: 7544.61, average training loss: 7014.48, base loss: 8825.77
[INFO 2017-06-26 18:00:44,167 main.py:47] epoch 2184, training loss: 7840.27, average training loss: 7015.02, base loss: 8826.07
[INFO 2017-06-26 18:00:44,530 main.py:47] epoch 2185, training loss: 7775.00, average training loss: 7014.29, base loss: 8826.42
[INFO 2017-06-26 18:00:44,891 main.py:47] epoch 2186, training loss: 6089.99, average training loss: 7012.85, base loss: 8823.79
[INFO 2017-06-26 18:00:45,256 main.py:47] epoch 2187, training loss: 6920.55, average training loss: 7012.62, base loss: 8823.23
[INFO 2017-06-26 18:00:45,622 main.py:47] epoch 2188, training loss: 6213.27, average training loss: 7011.45, base loss: 8821.71
[INFO 2017-06-26 18:00:45,984 main.py:47] epoch 2189, training loss: 6642.90, average training loss: 7011.10, base loss: 8820.96
[INFO 2017-06-26 18:00:46,347 main.py:47] epoch 2190, training loss: 6975.10, average training loss: 7011.07, base loss: 8821.10
[INFO 2017-06-26 18:00:46,714 main.py:47] epoch 2191, training loss: 6658.31, average training loss: 7010.00, base loss: 8820.98
[INFO 2017-06-26 18:00:47,081 main.py:47] epoch 2192, training loss: 6628.34, average training loss: 7009.41, base loss: 8820.32
[INFO 2017-06-26 18:00:47,445 main.py:47] epoch 2193, training loss: 6465.40, average training loss: 7008.89, base loss: 8819.80
[INFO 2017-06-26 18:00:47,803 main.py:47] epoch 2194, training loss: 6539.75, average training loss: 7009.18, base loss: 8820.07
[INFO 2017-06-26 18:00:48,163 main.py:47] epoch 2195, training loss: 7015.04, average training loss: 7008.26, base loss: 8819.48
[INFO 2017-06-26 18:00:48,524 main.py:47] epoch 2196, training loss: 6490.83, average training loss: 7007.69, base loss: 8819.46
[INFO 2017-06-26 18:00:48,885 main.py:47] epoch 2197, training loss: 6737.42, average training loss: 7008.17, base loss: 8820.35
[INFO 2017-06-26 18:00:49,246 main.py:47] epoch 2198, training loss: 6741.65, average training loss: 7008.04, base loss: 8820.17
[INFO 2017-06-26 18:00:49,607 main.py:47] epoch 2199, training loss: 7009.70, average training loss: 7006.85, base loss: 8818.68
[INFO 2017-06-26 18:00:49,607 main.py:49] epoch 2199, testing
[INFO 2017-06-26 18:00:53,857 main.py:100] average testing loss: 6814.41, base loss: 8709.34
[INFO 2017-06-26 18:00:53,881 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 18:00:53,888 main.py:73] current best accuracy: 6814.41
[INFO 2017-06-26 18:00:54,245 main.py:47] epoch 2200, training loss: 8169.69, average training loss: 7007.70, base loss: 8820.34
[INFO 2017-06-26 18:00:54,608 main.py:47] epoch 2201, training loss: 8623.03, average training loss: 7009.30, base loss: 8822.01
[INFO 2017-06-26 18:00:54,971 main.py:47] epoch 2202, training loss: 6842.31, average training loss: 7008.12, base loss: 8820.47
[INFO 2017-06-26 18:00:55,331 main.py:47] epoch 2203, training loss: 7224.75, average training loss: 7007.86, base loss: 8819.89
[INFO 2017-06-26 18:00:55,691 main.py:47] epoch 2204, training loss: 6537.31, average training loss: 7007.38, base loss: 8819.13
[INFO 2017-06-26 18:00:56,052 main.py:47] epoch 2205, training loss: 6955.74, average training loss: 7007.31, base loss: 8819.69
[INFO 2017-06-26 18:00:56,414 main.py:47] epoch 2206, training loss: 6618.53, average training loss: 7006.83, base loss: 8819.55
[INFO 2017-06-26 18:00:56,774 main.py:47] epoch 2207, training loss: 7709.38, average training loss: 7006.23, base loss: 8818.75
[INFO 2017-06-26 18:00:57,137 main.py:47] epoch 2208, training loss: 7042.99, average training loss: 7006.13, base loss: 8818.91
[INFO 2017-06-26 18:00:57,496 main.py:47] epoch 2209, training loss: 6283.71, average training loss: 7004.56, base loss: 8817.39
[INFO 2017-06-26 18:00:57,855 main.py:47] epoch 2210, training loss: 6504.58, average training loss: 7003.73, base loss: 8816.96
[INFO 2017-06-26 18:00:58,213 main.py:47] epoch 2211, training loss: 8093.54, average training loss: 7004.68, base loss: 8819.09
[INFO 2017-06-26 18:00:58,574 main.py:47] epoch 2212, training loss: 7135.52, average training loss: 7005.90, base loss: 8821.86
[INFO 2017-06-26 18:00:58,934 main.py:47] epoch 2213, training loss: 7389.01, average training loss: 7005.38, base loss: 8821.43
[INFO 2017-06-26 18:00:59,294 main.py:47] epoch 2214, training loss: 7394.79, average training loss: 7005.74, base loss: 8822.38
[INFO 2017-06-26 18:00:59,655 main.py:47] epoch 2215, training loss: 7343.07, average training loss: 7005.62, base loss: 8823.08
[INFO 2017-06-26 18:01:00,016 main.py:47] epoch 2216, training loss: 7209.67, average training loss: 7006.17, base loss: 8823.46
[INFO 2017-06-26 18:01:00,378 main.py:47] epoch 2217, training loss: 7057.93, average training loss: 7005.23, base loss: 8822.42
[INFO 2017-06-26 18:01:00,740 main.py:47] epoch 2218, training loss: 6322.24, average training loss: 7004.24, base loss: 8821.84
[INFO 2017-06-26 18:01:01,100 main.py:47] epoch 2219, training loss: 6352.85, average training loss: 7003.51, base loss: 8821.11
[INFO 2017-06-26 18:01:01,459 main.py:47] epoch 2220, training loss: 6863.41, average training loss: 7003.37, base loss: 8821.78
[INFO 2017-06-26 18:01:01,820 main.py:47] epoch 2221, training loss: 7555.72, average training loss: 7003.21, base loss: 8822.20
[INFO 2017-06-26 18:01:02,179 main.py:47] epoch 2222, training loss: 6637.25, average training loss: 7002.86, base loss: 8822.52
[INFO 2017-06-26 18:01:02,540 main.py:47] epoch 2223, training loss: 6339.41, average training loss: 7002.50, base loss: 8822.69
[INFO 2017-06-26 18:01:02,902 main.py:47] epoch 2224, training loss: 6508.17, average training loss: 7000.59, base loss: 8820.24
[INFO 2017-06-26 18:01:03,264 main.py:47] epoch 2225, training loss: 6128.60, average training loss: 6999.20, base loss: 8818.53
[INFO 2017-06-26 18:01:03,624 main.py:47] epoch 2226, training loss: 7076.84, average training loss: 6999.15, base loss: 8818.70
[INFO 2017-06-26 18:01:03,984 main.py:47] epoch 2227, training loss: 6777.05, average training loss: 6998.56, base loss: 8818.41
[INFO 2017-06-26 18:01:04,345 main.py:47] epoch 2228, training loss: 6769.48, average training loss: 6998.16, base loss: 8818.44
[INFO 2017-06-26 18:01:04,706 main.py:47] epoch 2229, training loss: 6577.73, average training loss: 6998.43, base loss: 8819.19
[INFO 2017-06-26 18:01:05,067 main.py:47] epoch 2230, training loss: 7469.50, average training loss: 6998.60, base loss: 8819.55
[INFO 2017-06-26 18:01:05,430 main.py:47] epoch 2231, training loss: 7769.81, average training loss: 6998.50, base loss: 8820.14
[INFO 2017-06-26 18:01:05,796 main.py:47] epoch 2232, training loss: 7515.40, average training loss: 6998.85, base loss: 8821.52
[INFO 2017-06-26 18:01:06,159 main.py:47] epoch 2233, training loss: 6679.10, average training loss: 6999.04, base loss: 8821.83
[INFO 2017-06-26 18:01:06,522 main.py:47] epoch 2234, training loss: 6705.90, average training loss: 6998.06, base loss: 8820.73
[INFO 2017-06-26 18:01:06,885 main.py:47] epoch 2235, training loss: 7253.50, average training loss: 6998.44, base loss: 8821.41
[INFO 2017-06-26 18:01:07,250 main.py:47] epoch 2236, training loss: 6984.74, average training loss: 6998.69, base loss: 8821.15
[INFO 2017-06-26 18:01:07,610 main.py:47] epoch 2237, training loss: 6490.98, average training loss: 6998.21, base loss: 8820.36
[INFO 2017-06-26 18:01:07,973 main.py:47] epoch 2238, training loss: 6711.93, average training loss: 6997.65, base loss: 8819.57
[INFO 2017-06-26 18:01:08,347 main.py:47] epoch 2239, training loss: 6457.32, average training loss: 6996.96, base loss: 8819.67
[INFO 2017-06-26 18:01:08,712 main.py:47] epoch 2240, training loss: 5875.59, average training loss: 6995.93, base loss: 8818.53
[INFO 2017-06-26 18:01:09,074 main.py:47] epoch 2241, training loss: 6420.95, average training loss: 6995.78, base loss: 8818.56
[INFO 2017-06-26 18:01:09,434 main.py:47] epoch 2242, training loss: 7448.49, average training loss: 6995.93, base loss: 8819.19
[INFO 2017-06-26 18:01:09,793 main.py:47] epoch 2243, training loss: 7427.76, average training loss: 6996.85, base loss: 8820.64
[INFO 2017-06-26 18:01:10,153 main.py:47] epoch 2244, training loss: 7942.51, average training loss: 6998.40, base loss: 8822.89
[INFO 2017-06-26 18:01:10,515 main.py:47] epoch 2245, training loss: 7436.04, average training loss: 6997.77, base loss: 8822.43
[INFO 2017-06-26 18:01:10,877 main.py:47] epoch 2246, training loss: 6788.50, average training loss: 6997.11, base loss: 8821.10
[INFO 2017-06-26 18:01:11,239 main.py:47] epoch 2247, training loss: 7142.35, average training loss: 6997.28, base loss: 8820.74
[INFO 2017-06-26 18:01:11,601 main.py:47] epoch 2248, training loss: 7264.35, average training loss: 6997.24, base loss: 8820.81
[INFO 2017-06-26 18:01:11,961 main.py:47] epoch 2249, training loss: 7095.82, average training loss: 6997.08, base loss: 8820.93
[INFO 2017-06-26 18:01:12,321 main.py:47] epoch 2250, training loss: 8011.25, average training loss: 6998.42, base loss: 8823.47
[INFO 2017-06-26 18:01:12,682 main.py:47] epoch 2251, training loss: 6681.68, average training loss: 6998.67, base loss: 8824.52
[INFO 2017-06-26 18:01:13,045 main.py:47] epoch 2252, training loss: 6915.40, average training loss: 6998.22, base loss: 8824.10
[INFO 2017-06-26 18:01:13,403 main.py:47] epoch 2253, training loss: 6959.34, average training loss: 6998.35, base loss: 8825.56
[INFO 2017-06-26 18:01:13,762 main.py:47] epoch 2254, training loss: 7217.91, average training loss: 6997.96, base loss: 8825.56
[INFO 2017-06-26 18:01:14,124 main.py:47] epoch 2255, training loss: 6050.32, average training loss: 6996.95, base loss: 8824.46
[INFO 2017-06-26 18:01:14,483 main.py:47] epoch 2256, training loss: 6928.38, average training loss: 6997.47, base loss: 8825.59
[INFO 2017-06-26 18:01:14,842 main.py:47] epoch 2257, training loss: 6140.88, average training loss: 6996.18, base loss: 8824.03
[INFO 2017-06-26 18:01:15,203 main.py:47] epoch 2258, training loss: 6719.71, average training loss: 6996.20, base loss: 8824.30
[INFO 2017-06-26 18:01:15,564 main.py:47] epoch 2259, training loss: 5993.22, average training loss: 6995.54, base loss: 8824.04
[INFO 2017-06-26 18:01:15,927 main.py:47] epoch 2260, training loss: 7368.79, average training loss: 6996.15, base loss: 8825.96
[INFO 2017-06-26 18:01:16,289 main.py:47] epoch 2261, training loss: 6466.28, average training loss: 6994.71, base loss: 8824.76
[INFO 2017-06-26 18:01:16,652 main.py:47] epoch 2262, training loss: 7232.92, average training loss: 6995.06, base loss: 8825.41
[INFO 2017-06-26 18:01:17,013 main.py:47] epoch 2263, training loss: 6792.93, average training loss: 6995.86, base loss: 8827.23
[INFO 2017-06-26 18:01:17,374 main.py:47] epoch 2264, training loss: 7171.62, average training loss: 6995.89, base loss: 8828.41
[INFO 2017-06-26 18:01:17,734 main.py:47] epoch 2265, training loss: 7524.96, average training loss: 6996.52, base loss: 8829.08
[INFO 2017-06-26 18:01:18,096 main.py:47] epoch 2266, training loss: 7628.28, average training loss: 6996.57, base loss: 8829.97
[INFO 2017-06-26 18:01:18,460 main.py:47] epoch 2267, training loss: 6230.21, average training loss: 6995.52, base loss: 8828.65
[INFO 2017-06-26 18:01:18,820 main.py:47] epoch 2268, training loss: 7298.34, average training loss: 6996.37, base loss: 8830.23
[INFO 2017-06-26 18:01:19,181 main.py:47] epoch 2269, training loss: 6922.60, average training loss: 6995.44, base loss: 8828.35
[INFO 2017-06-26 18:01:19,543 main.py:47] epoch 2270, training loss: 7511.41, average training loss: 6996.76, base loss: 8831.42
[INFO 2017-06-26 18:01:19,903 main.py:47] epoch 2271, training loss: 7720.35, average training loss: 6997.30, base loss: 8832.57
[INFO 2017-06-26 18:01:20,264 main.py:47] epoch 2272, training loss: 6214.85, average training loss: 6996.75, base loss: 8832.34
[INFO 2017-06-26 18:01:20,627 main.py:47] epoch 2273, training loss: 6812.22, average training loss: 6996.11, base loss: 8831.27
[INFO 2017-06-26 18:01:20,988 main.py:47] epoch 2274, training loss: 6760.18, average training loss: 6996.77, base loss: 8832.41
[INFO 2017-06-26 18:01:21,347 main.py:47] epoch 2275, training loss: 8227.45, average training loss: 6997.58, base loss: 8834.43
[INFO 2017-06-26 18:01:21,708 main.py:47] epoch 2276, training loss: 6177.18, average training loss: 6997.36, base loss: 8834.42
[INFO 2017-06-26 18:01:22,069 main.py:47] epoch 2277, training loss: 7139.78, average training loss: 6997.65, base loss: 8835.00
[INFO 2017-06-26 18:01:22,431 main.py:47] epoch 2278, training loss: 6150.96, average training loss: 6996.76, base loss: 8833.68
[INFO 2017-06-26 18:01:22,792 main.py:47] epoch 2279, training loss: 6852.49, average training loss: 6996.98, base loss: 8834.08
[INFO 2017-06-26 18:01:23,155 main.py:47] epoch 2280, training loss: 6628.76, average training loss: 6996.55, base loss: 8833.89
[INFO 2017-06-26 18:01:23,515 main.py:47] epoch 2281, training loss: 6514.78, average training loss: 6996.12, base loss: 8833.32
[INFO 2017-06-26 18:01:23,876 main.py:47] epoch 2282, training loss: 7297.45, average training loss: 6996.32, base loss: 8833.84
[INFO 2017-06-26 18:01:24,238 main.py:47] epoch 2283, training loss: 6813.33, average training loss: 6996.71, base loss: 8834.84
[INFO 2017-06-26 18:01:24,598 main.py:47] epoch 2284, training loss: 7544.69, average training loss: 6996.66, base loss: 8835.57
[INFO 2017-06-26 18:01:24,960 main.py:47] epoch 2285, training loss: 6532.30, average training loss: 6996.50, base loss: 8835.73
[INFO 2017-06-26 18:01:25,321 main.py:47] epoch 2286, training loss: 7253.80, average training loss: 6997.88, base loss: 8838.47
[INFO 2017-06-26 18:01:25,686 main.py:47] epoch 2287, training loss: 7265.39, average training loss: 6998.41, base loss: 8839.44
[INFO 2017-06-26 18:01:26,050 main.py:47] epoch 2288, training loss: 7123.90, average training loss: 6998.36, base loss: 8838.95
[INFO 2017-06-26 18:01:26,412 main.py:47] epoch 2289, training loss: 7544.46, average training loss: 6999.36, base loss: 8841.52
[INFO 2017-06-26 18:01:26,773 main.py:47] epoch 2290, training loss: 6986.58, average training loss: 6999.05, base loss: 8841.09
[INFO 2017-06-26 18:01:27,133 main.py:47] epoch 2291, training loss: 6470.24, average training loss: 6998.98, base loss: 8841.35
[INFO 2017-06-26 18:01:27,494 main.py:47] epoch 2292, training loss: 7119.04, average training loss: 6998.96, base loss: 8841.97
[INFO 2017-06-26 18:01:27,852 main.py:47] epoch 2293, training loss: 6201.97, average training loss: 6997.38, base loss: 8840.08
[INFO 2017-06-26 18:01:28,214 main.py:47] epoch 2294, training loss: 6554.55, average training loss: 6996.06, base loss: 8838.86
[INFO 2017-06-26 18:01:28,574 main.py:47] epoch 2295, training loss: 6347.64, average training loss: 6994.53, base loss: 8836.94
[INFO 2017-06-26 18:01:28,934 main.py:47] epoch 2296, training loss: 7195.25, average training loss: 6994.92, base loss: 8837.48
[INFO 2017-06-26 18:01:29,296 main.py:47] epoch 2297, training loss: 6329.38, average training loss: 6995.35, base loss: 8839.05
[INFO 2017-06-26 18:01:29,662 main.py:47] epoch 2298, training loss: 6732.01, average training loss: 6994.63, base loss: 8837.89
[INFO 2017-06-26 18:01:30,022 main.py:47] epoch 2299, training loss: 6727.00, average training loss: 6993.47, base loss: 8836.73
[INFO 2017-06-26 18:01:30,022 main.py:49] epoch 2299, testing
[INFO 2017-06-26 18:01:34,204 main.py:100] average testing loss: 6785.60, base loss: 8733.63
[INFO 2017-06-26 18:01:34,230 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 18:01:34,236 main.py:73] current best accuracy: 6785.60
[INFO 2017-06-26 18:01:34,595 main.py:47] epoch 2300, training loss: 6167.24, average training loss: 6991.79, base loss: 8834.64
[INFO 2017-06-26 18:01:34,959 main.py:47] epoch 2301, training loss: 7350.68, average training loss: 6991.98, base loss: 8835.26
[INFO 2017-06-26 18:01:35,318 main.py:47] epoch 2302, training loss: 7562.34, average training loss: 6992.33, base loss: 8835.11
[INFO 2017-06-26 18:01:35,679 main.py:47] epoch 2303, training loss: 7405.87, average training loss: 6993.26, base loss: 8836.88
[INFO 2017-06-26 18:01:36,039 main.py:47] epoch 2304, training loss: 6726.06, average training loss: 6992.77, base loss: 8836.50
[INFO 2017-06-26 18:01:36,400 main.py:47] epoch 2305, training loss: 7662.17, average training loss: 6993.17, base loss: 8837.27
[INFO 2017-06-26 18:01:36,762 main.py:47] epoch 2306, training loss: 7088.19, average training loss: 6993.82, base loss: 8838.77
[INFO 2017-06-26 18:01:37,123 main.py:47] epoch 2307, training loss: 7753.81, average training loss: 6995.30, base loss: 8840.78
[INFO 2017-06-26 18:01:37,484 main.py:47] epoch 2308, training loss: 7190.64, average training loss: 6995.71, base loss: 8841.72
[INFO 2017-06-26 18:01:37,845 main.py:47] epoch 2309, training loss: 7328.82, average training loss: 6996.01, base loss: 8841.33
[INFO 2017-06-26 18:01:38,205 main.py:47] epoch 2310, training loss: 6928.01, average training loss: 6995.90, base loss: 8841.74
[INFO 2017-06-26 18:01:38,566 main.py:47] epoch 2311, training loss: 6656.56, average training loss: 6996.45, base loss: 8843.00
[INFO 2017-06-26 18:01:38,929 main.py:47] epoch 2312, training loss: 6670.19, average training loss: 6995.94, base loss: 8842.10
[INFO 2017-06-26 18:01:39,290 main.py:47] epoch 2313, training loss: 7560.67, average training loss: 6996.21, base loss: 8842.09
[INFO 2017-06-26 18:01:39,651 main.py:47] epoch 2314, training loss: 5704.69, average training loss: 6993.54, base loss: 8838.81
[INFO 2017-06-26 18:01:40,012 main.py:47] epoch 2315, training loss: 6334.13, average training loss: 6992.54, base loss: 8836.91
[INFO 2017-06-26 18:01:40,372 main.py:47] epoch 2316, training loss: 7266.88, average training loss: 6993.32, base loss: 8838.72
[INFO 2017-06-26 18:01:40,733 main.py:47] epoch 2317, training loss: 6628.04, average training loss: 6992.35, base loss: 8836.94
[INFO 2017-06-26 18:01:41,094 main.py:47] epoch 2318, training loss: 7289.87, average training loss: 6992.83, base loss: 8837.55
[INFO 2017-06-26 18:01:41,455 main.py:47] epoch 2319, training loss: 7398.54, average training loss: 6994.03, base loss: 8839.39
[INFO 2017-06-26 18:01:41,815 main.py:47] epoch 2320, training loss: 7616.88, average training loss: 6995.46, base loss: 8842.05
[INFO 2017-06-26 18:01:42,176 main.py:47] epoch 2321, training loss: 7638.48, average training loss: 6995.89, base loss: 8842.91
[INFO 2017-06-26 18:01:42,536 main.py:47] epoch 2322, training loss: 7122.34, average training loss: 6994.86, base loss: 8841.21
[INFO 2017-06-26 18:01:42,898 main.py:47] epoch 2323, training loss: 6860.62, average training loss: 6994.42, base loss: 8841.03
[INFO 2017-06-26 18:01:43,259 main.py:47] epoch 2324, training loss: 6025.71, average training loss: 6993.68, base loss: 8839.99
[INFO 2017-06-26 18:01:43,620 main.py:47] epoch 2325, training loss: 6929.73, average training loss: 6993.79, base loss: 8839.71
[INFO 2017-06-26 18:01:43,984 main.py:47] epoch 2326, training loss: 6469.62, average training loss: 6993.35, base loss: 8838.96
[INFO 2017-06-26 18:01:44,346 main.py:47] epoch 2327, training loss: 6085.78, average training loss: 6991.60, base loss: 8837.18
[INFO 2017-06-26 18:01:44,706 main.py:47] epoch 2328, training loss: 6656.16, average training loss: 6990.32, base loss: 8835.47
[INFO 2017-06-26 18:01:45,066 main.py:47] epoch 2329, training loss: 6727.83, average training loss: 6990.77, base loss: 8836.30
[INFO 2017-06-26 18:01:45,431 main.py:47] epoch 2330, training loss: 7106.36, average training loss: 6990.44, base loss: 8836.06
[INFO 2017-06-26 18:01:45,793 main.py:47] epoch 2331, training loss: 6677.35, average training loss: 6990.39, base loss: 8836.53
[INFO 2017-06-26 18:01:46,151 main.py:47] epoch 2332, training loss: 6974.62, average training loss: 6989.70, base loss: 8835.19
[INFO 2017-06-26 18:01:46,512 main.py:47] epoch 2333, training loss: 6845.79, average training loss: 6989.86, base loss: 8835.64
[INFO 2017-06-26 18:01:46,872 main.py:47] epoch 2334, training loss: 6556.10, average training loss: 6989.27, base loss: 8834.85
[INFO 2017-06-26 18:01:47,234 main.py:47] epoch 2335, training loss: 7499.13, average training loss: 6990.22, base loss: 8836.73
[INFO 2017-06-26 18:01:47,595 main.py:47] epoch 2336, training loss: 6995.87, average training loss: 6990.09, base loss: 8836.58
[INFO 2017-06-26 18:01:47,957 main.py:47] epoch 2337, training loss: 6470.92, average training loss: 6988.60, base loss: 8833.99
[INFO 2017-06-26 18:01:48,319 main.py:47] epoch 2338, training loss: 6623.26, average training loss: 6988.01, base loss: 8833.16
[INFO 2017-06-26 18:01:48,685 main.py:47] epoch 2339, training loss: 6711.01, average training loss: 6987.79, base loss: 8832.91
[INFO 2017-06-26 18:01:49,045 main.py:47] epoch 2340, training loss: 6547.34, average training loss: 6986.96, base loss: 8831.68
[INFO 2017-06-26 18:01:49,407 main.py:47] epoch 2341, training loss: 6976.93, average training loss: 6986.56, base loss: 8832.06
[INFO 2017-06-26 18:01:49,768 main.py:47] epoch 2342, training loss: 7453.78, average training loss: 6986.96, base loss: 8832.24
[INFO 2017-06-26 18:01:50,129 main.py:47] epoch 2343, training loss: 6276.81, average training loss: 6986.46, base loss: 8832.21
[INFO 2017-06-26 18:01:50,489 main.py:47] epoch 2344, training loss: 6778.19, average training loss: 6986.32, base loss: 8832.82
[INFO 2017-06-26 18:01:50,851 main.py:47] epoch 2345, training loss: 6531.09, average training loss: 6985.71, base loss: 8832.98
[INFO 2017-06-26 18:01:51,213 main.py:47] epoch 2346, training loss: 7188.38, average training loss: 6984.81, base loss: 8832.53
[INFO 2017-06-26 18:01:51,573 main.py:47] epoch 2347, training loss: 6427.63, average training loss: 6983.36, base loss: 8831.38
[INFO 2017-06-26 18:01:51,932 main.py:47] epoch 2348, training loss: 7909.63, average training loss: 6985.06, base loss: 8834.24
[INFO 2017-06-26 18:01:52,294 main.py:47] epoch 2349, training loss: 6624.57, average training loss: 6984.56, base loss: 8834.43
[INFO 2017-06-26 18:01:52,654 main.py:47] epoch 2350, training loss: 6720.40, average training loss: 6983.84, base loss: 8833.25
[INFO 2017-06-26 18:01:53,015 main.py:47] epoch 2351, training loss: 6602.19, average training loss: 6982.94, base loss: 8832.62
[INFO 2017-06-26 18:01:53,375 main.py:47] epoch 2352, training loss: 6863.76, average training loss: 6982.83, base loss: 8833.23
[INFO 2017-06-26 18:01:53,736 main.py:47] epoch 2353, training loss: 6973.32, average training loss: 6982.11, base loss: 8832.26
[INFO 2017-06-26 18:01:54,096 main.py:47] epoch 2354, training loss: 7777.88, average training loss: 6983.21, base loss: 8834.37
[INFO 2017-06-26 18:01:54,457 main.py:47] epoch 2355, training loss: 6823.71, average training loss: 6983.32, base loss: 8834.27
[INFO 2017-06-26 18:01:54,818 main.py:47] epoch 2356, training loss: 6472.95, average training loss: 6983.00, base loss: 8833.78
[INFO 2017-06-26 18:01:55,180 main.py:47] epoch 2357, training loss: 6021.88, average training loss: 6982.04, base loss: 8833.18
[INFO 2017-06-26 18:01:55,540 main.py:47] epoch 2358, training loss: 7081.36, average training loss: 6981.85, base loss: 8832.88
[INFO 2017-06-26 18:01:55,901 main.py:47] epoch 2359, training loss: 6721.58, average training loss: 6981.59, base loss: 8833.16
[INFO 2017-06-26 18:01:56,263 main.py:47] epoch 2360, training loss: 6540.94, average training loss: 6981.27, base loss: 8832.69
[INFO 2017-06-26 18:01:56,624 main.py:47] epoch 2361, training loss: 7764.71, average training loss: 6981.58, base loss: 8833.60
[INFO 2017-06-26 18:01:56,987 main.py:47] epoch 2362, training loss: 7100.08, average training loss: 6982.05, base loss: 8834.33
[INFO 2017-06-26 18:01:57,349 main.py:47] epoch 2363, training loss: 6924.87, average training loss: 6982.02, base loss: 8834.33
[INFO 2017-06-26 18:01:57,708 main.py:47] epoch 2364, training loss: 7419.96, average training loss: 6981.85, base loss: 8834.43
[INFO 2017-06-26 18:01:58,071 main.py:47] epoch 2365, training loss: 6540.07, average training loss: 6981.08, base loss: 8833.15
[INFO 2017-06-26 18:01:58,431 main.py:47] epoch 2366, training loss: 6602.47, average training loss: 6981.02, base loss: 8833.70
[INFO 2017-06-26 18:01:58,791 main.py:47] epoch 2367, training loss: 7172.95, average training loss: 6980.69, base loss: 8833.62
[INFO 2017-06-26 18:01:59,150 main.py:47] epoch 2368, training loss: 7378.01, average training loss: 6981.52, base loss: 8835.27
[INFO 2017-06-26 18:01:59,511 main.py:47] epoch 2369, training loss: 6559.63, average training loss: 6982.24, base loss: 8836.98
[INFO 2017-06-26 18:01:59,871 main.py:47] epoch 2370, training loss: 7279.31, average training loss: 6981.92, base loss: 8836.37
[INFO 2017-06-26 18:02:00,231 main.py:47] epoch 2371, training loss: 6935.84, average training loss: 6981.36, base loss: 8836.15
[INFO 2017-06-26 18:02:00,593 main.py:47] epoch 2372, training loss: 7856.86, average training loss: 6982.74, base loss: 8838.71
[INFO 2017-06-26 18:02:00,954 main.py:47] epoch 2373, training loss: 6103.15, average training loss: 6982.46, base loss: 8838.76
[INFO 2017-06-26 18:02:01,314 main.py:47] epoch 2374, training loss: 6524.54, average training loss: 6982.16, base loss: 8838.83
[INFO 2017-06-26 18:02:01,674 main.py:47] epoch 2375, training loss: 6174.32, average training loss: 6982.05, base loss: 8839.33
[INFO 2017-06-26 18:02:02,038 main.py:47] epoch 2376, training loss: 6530.22, average training loss: 6982.29, base loss: 8839.92
[INFO 2017-06-26 18:02:02,400 main.py:47] epoch 2377, training loss: 7213.11, average training loss: 6982.65, base loss: 8840.19
[INFO 2017-06-26 18:02:02,761 main.py:47] epoch 2378, training loss: 6258.71, average training loss: 6982.03, base loss: 8840.00
[INFO 2017-06-26 18:02:03,121 main.py:47] epoch 2379, training loss: 7274.51, average training loss: 6982.68, base loss: 8841.04
[INFO 2017-06-26 18:02:03,485 main.py:47] epoch 2380, training loss: 6902.10, average training loss: 6981.94, base loss: 8840.07
[INFO 2017-06-26 18:02:03,844 main.py:47] epoch 2381, training loss: 6497.08, average training loss: 6982.19, base loss: 8841.31
[INFO 2017-06-26 18:02:04,203 main.py:47] epoch 2382, training loss: 6574.76, average training loss: 6981.58, base loss: 8840.51
[INFO 2017-06-26 18:02:04,563 main.py:47] epoch 2383, training loss: 7508.84, average training loss: 6981.43, base loss: 8840.30
[INFO 2017-06-26 18:02:04,923 main.py:47] epoch 2384, training loss: 6278.49, average training loss: 6978.93, base loss: 8837.14
[INFO 2017-06-26 18:02:05,283 main.py:47] epoch 2385, training loss: 6849.50, average training loss: 6978.66, base loss: 8836.28
[INFO 2017-06-26 18:02:05,644 main.py:47] epoch 2386, training loss: 6653.34, average training loss: 6978.35, base loss: 8836.37
[INFO 2017-06-26 18:02:06,005 main.py:47] epoch 2387, training loss: 7070.94, average training loss: 6978.36, base loss: 8836.57
[INFO 2017-06-26 18:02:06,363 main.py:47] epoch 2388, training loss: 7546.69, average training loss: 6978.77, base loss: 8837.22
[INFO 2017-06-26 18:02:06,724 main.py:47] epoch 2389, training loss: 6969.17, average training loss: 6979.04, base loss: 8837.56
[INFO 2017-06-26 18:02:07,086 main.py:47] epoch 2390, training loss: 7245.29, average training loss: 6979.90, base loss: 8839.50
[INFO 2017-06-26 18:02:07,446 main.py:47] epoch 2391, training loss: 7718.76, average training loss: 6981.07, base loss: 8840.45
[INFO 2017-06-26 18:02:07,806 main.py:47] epoch 2392, training loss: 6921.19, average training loss: 6979.69, base loss: 8839.31
[INFO 2017-06-26 18:02:08,168 main.py:47] epoch 2393, training loss: 7169.38, average training loss: 6979.38, base loss: 8840.06
[INFO 2017-06-26 18:02:08,528 main.py:47] epoch 2394, training loss: 6492.40, average training loss: 6979.47, base loss: 8840.19
[INFO 2017-06-26 18:02:08,889 main.py:47] epoch 2395, training loss: 6193.26, average training loss: 6978.53, base loss: 8839.08
[INFO 2017-06-26 18:02:09,250 main.py:47] epoch 2396, training loss: 6383.16, average training loss: 6978.13, base loss: 8838.53
[INFO 2017-06-26 18:02:09,611 main.py:47] epoch 2397, training loss: 6615.92, average training loss: 6978.13, base loss: 8838.16
[INFO 2017-06-26 18:02:09,972 main.py:47] epoch 2398, training loss: 6544.00, average training loss: 6977.11, base loss: 8837.47
[INFO 2017-06-26 18:02:10,332 main.py:47] epoch 2399, training loss: 6947.87, average training loss: 6976.70, base loss: 8838.47
[INFO 2017-06-26 18:02:10,332 main.py:49] epoch 2399, testing
[INFO 2017-06-26 18:02:14,548 main.py:100] average testing loss: 7246.91, base loss: 9442.23
[INFO 2017-06-26 18:02:14,573 main.py:73] current best accuracy: 6785.60
[INFO 2017-06-26 18:02:14,934 main.py:47] epoch 2400, training loss: 6619.38, average training loss: 6975.46, base loss: 8836.99
[INFO 2017-06-26 18:02:15,295 main.py:47] epoch 2401, training loss: 7186.20, average training loss: 6976.73, base loss: 8838.59
[INFO 2017-06-26 18:02:15,654 main.py:47] epoch 2402, training loss: 6343.23, average training loss: 6976.30, base loss: 8838.22
[INFO 2017-06-26 18:02:16,014 main.py:47] epoch 2403, training loss: 6467.63, average training loss: 6975.42, base loss: 8836.91
[INFO 2017-06-26 18:02:16,376 main.py:47] epoch 2404, training loss: 6813.44, average training loss: 6974.10, base loss: 8835.16
[INFO 2017-06-26 18:02:16,735 main.py:47] epoch 2405, training loss: 6683.08, average training loss: 6973.25, base loss: 8834.68
[INFO 2017-06-26 18:02:17,097 main.py:47] epoch 2406, training loss: 6562.42, average training loss: 6972.02, base loss: 8832.90
[INFO 2017-06-26 18:02:17,457 main.py:47] epoch 2407, training loss: 6883.74, average training loss: 6971.72, base loss: 8833.09
[INFO 2017-06-26 18:02:17,817 main.py:47] epoch 2408, training loss: 6160.07, average training loss: 6970.54, base loss: 8832.04
[INFO 2017-06-26 18:02:18,175 main.py:47] epoch 2409, training loss: 6263.68, average training loss: 6969.51, base loss: 8830.84
[INFO 2017-06-26 18:02:18,537 main.py:47] epoch 2410, training loss: 6808.49, average training loss: 6969.40, base loss: 8830.56
[INFO 2017-06-26 18:02:18,898 main.py:47] epoch 2411, training loss: 7360.05, average training loss: 6968.97, base loss: 8829.28
[INFO 2017-06-26 18:02:19,257 main.py:47] epoch 2412, training loss: 6909.50, average training loss: 6969.30, base loss: 8829.85
[INFO 2017-06-26 18:02:19,625 main.py:47] epoch 2413, training loss: 6309.18, average training loss: 6968.89, base loss: 8828.88
[INFO 2017-06-26 18:02:19,987 main.py:47] epoch 2414, training loss: 6696.35, average training loss: 6967.83, base loss: 8828.06
[INFO 2017-06-26 18:02:20,346 main.py:47] epoch 2415, training loss: 6894.86, average training loss: 6967.42, base loss: 8827.96
[INFO 2017-06-26 18:02:20,706 main.py:47] epoch 2416, training loss: 7922.15, average training loss: 6968.42, base loss: 8830.27
[INFO 2017-06-26 18:02:21,067 main.py:47] epoch 2417, training loss: 7091.81, average training loss: 6968.22, base loss: 8830.32
[INFO 2017-06-26 18:02:21,428 main.py:47] epoch 2418, training loss: 6745.20, average training loss: 6969.05, base loss: 8832.15
[INFO 2017-06-26 18:02:21,788 main.py:47] epoch 2419, training loss: 7541.46, average training loss: 6969.89, base loss: 8833.53
[INFO 2017-06-26 18:02:22,149 main.py:47] epoch 2420, training loss: 7831.25, average training loss: 6970.37, base loss: 8834.27
[INFO 2017-06-26 18:02:22,511 main.py:47] epoch 2421, training loss: 6785.84, average training loss: 6970.16, base loss: 8833.25
[INFO 2017-06-26 18:02:22,875 main.py:47] epoch 2422, training loss: 6157.67, average training loss: 6969.87, base loss: 8833.32
[INFO 2017-06-26 18:02:23,241 main.py:47] epoch 2423, training loss: 6584.39, average training loss: 6968.70, base loss: 8831.22
[INFO 2017-06-26 18:02:23,604 main.py:47] epoch 2424, training loss: 6421.96, average training loss: 6968.20, base loss: 8830.04
[INFO 2017-06-26 18:02:23,971 main.py:47] epoch 2425, training loss: 7273.60, average training loss: 6969.08, base loss: 8830.84
[INFO 2017-06-26 18:02:24,336 main.py:47] epoch 2426, training loss: 6926.06, average training loss: 6968.60, base loss: 8830.32
[INFO 2017-06-26 18:02:24,696 main.py:47] epoch 2427, training loss: 7335.41, average training loss: 6968.85, base loss: 8830.29
[INFO 2017-06-26 18:02:25,059 main.py:47] epoch 2428, training loss: 7673.93, average training loss: 6969.50, base loss: 8831.37
[INFO 2017-06-26 18:02:25,428 main.py:47] epoch 2429, training loss: 6342.79, average training loss: 6968.39, base loss: 8830.58
[INFO 2017-06-26 18:02:25,792 main.py:47] epoch 2430, training loss: 6778.95, average training loss: 6967.96, base loss: 8830.38
[INFO 2017-06-26 18:02:26,155 main.py:47] epoch 2431, training loss: 8037.78, average training loss: 6969.57, base loss: 8832.76
[INFO 2017-06-26 18:02:26,515 main.py:47] epoch 2432, training loss: 7346.11, average training loss: 6970.40, base loss: 8833.65
[INFO 2017-06-26 18:02:26,877 main.py:47] epoch 2433, training loss: 7126.91, average training loss: 6970.90, base loss: 8834.45
[INFO 2017-06-26 18:02:27,242 main.py:47] epoch 2434, training loss: 6880.39, average training loss: 6970.29, base loss: 8833.29
[INFO 2017-06-26 18:02:27,606 main.py:47] epoch 2435, training loss: 7961.29, average training loss: 6971.71, base loss: 8834.85
[INFO 2017-06-26 18:02:27,970 main.py:47] epoch 2436, training loss: 6697.40, average training loss: 6970.54, base loss: 8833.40
[INFO 2017-06-26 18:02:28,330 main.py:47] epoch 2437, training loss: 7578.06, average training loss: 6970.30, base loss: 8833.54
[INFO 2017-06-26 18:02:28,694 main.py:47] epoch 2438, training loss: 6804.37, average training loss: 6969.47, base loss: 8833.30
[INFO 2017-06-26 18:02:29,081 main.py:47] epoch 2439, training loss: 7849.68, average training loss: 6970.06, base loss: 8835.10
[INFO 2017-06-26 18:02:29,443 main.py:47] epoch 2440, training loss: 7544.90, average training loss: 6971.35, base loss: 8837.22
[INFO 2017-06-26 18:02:29,803 main.py:47] epoch 2441, training loss: 7285.51, average training loss: 6972.33, base loss: 8839.38
[INFO 2017-06-26 18:02:30,165 main.py:47] epoch 2442, training loss: 6793.29, average training loss: 6972.11, base loss: 8838.98
[INFO 2017-06-26 18:02:30,526 main.py:47] epoch 2443, training loss: 6434.61, average training loss: 6971.10, base loss: 8837.93
[INFO 2017-06-26 18:02:30,891 main.py:47] epoch 2444, training loss: 6960.10, average training loss: 6971.56, base loss: 8838.29
[INFO 2017-06-26 18:02:31,253 main.py:47] epoch 2445, training loss: 7583.46, average training loss: 6972.59, base loss: 8840.43
[INFO 2017-06-26 18:02:31,617 main.py:47] epoch 2446, training loss: 7065.59, average training loss: 6973.74, base loss: 8842.54
[INFO 2017-06-26 18:02:31,978 main.py:47] epoch 2447, training loss: 6627.51, average training loss: 6972.70, base loss: 8841.55
[INFO 2017-06-26 18:02:32,341 main.py:47] epoch 2448, training loss: 6560.76, average training loss: 6971.66, base loss: 8840.69
[INFO 2017-06-26 18:02:32,703 main.py:47] epoch 2449, training loss: 6385.69, average training loss: 6970.48, base loss: 8839.14
[INFO 2017-06-26 18:02:33,064 main.py:47] epoch 2450, training loss: 6950.42, average training loss: 6970.23, base loss: 8839.25
[INFO 2017-06-26 18:02:33,426 main.py:47] epoch 2451, training loss: 7957.59, average training loss: 6971.78, base loss: 8841.77
[INFO 2017-06-26 18:02:33,788 main.py:47] epoch 2452, training loss: 6623.44, average training loss: 6971.85, base loss: 8841.78
[INFO 2017-06-26 18:02:34,150 main.py:47] epoch 2453, training loss: 6415.04, average training loss: 6971.36, base loss: 8841.50
[INFO 2017-06-26 18:02:34,512 main.py:47] epoch 2454, training loss: 5900.97, average training loss: 6969.99, base loss: 8839.02
[INFO 2017-06-26 18:02:34,877 main.py:47] epoch 2455, training loss: 7211.55, average training loss: 6968.86, base loss: 8837.80
[INFO 2017-06-26 18:02:35,295 main.py:47] epoch 2456, training loss: 7069.56, average training loss: 6969.91, base loss: 8840.30
[INFO 2017-06-26 18:02:35,679 main.py:47] epoch 2457, training loss: 6655.01, average training loss: 6968.73, base loss: 8839.32
[INFO 2017-06-26 18:02:36,070 main.py:47] epoch 2458, training loss: 7508.84, average training loss: 6969.00, base loss: 8841.03
[INFO 2017-06-26 18:02:36,464 main.py:47] epoch 2459, training loss: 7115.11, average training loss: 6969.61, base loss: 8842.36
[INFO 2017-06-26 18:02:36,826 main.py:47] epoch 2460, training loss: 8133.56, average training loss: 6970.61, base loss: 8843.84
[INFO 2017-06-26 18:02:37,216 main.py:47] epoch 2461, training loss: 6383.35, average training loss: 6970.12, base loss: 8842.82
[INFO 2017-06-26 18:02:37,608 main.py:47] epoch 2462, training loss: 6398.84, average training loss: 6969.63, base loss: 8842.79
[INFO 2017-06-26 18:02:37,986 main.py:47] epoch 2463, training loss: 7569.39, average training loss: 6970.32, base loss: 8843.96
[INFO 2017-06-26 18:02:38,348 main.py:47] epoch 2464, training loss: 6710.25, average training loss: 6969.95, base loss: 8843.14
[INFO 2017-06-26 18:02:38,706 main.py:47] epoch 2465, training loss: 8037.41, average training loss: 6971.18, base loss: 8845.44
[INFO 2017-06-26 18:02:39,067 main.py:47] epoch 2466, training loss: 6588.18, average training loss: 6970.07, base loss: 8843.87
[INFO 2017-06-26 18:02:39,430 main.py:47] epoch 2467, training loss: 7156.57, average training loss: 6970.95, base loss: 8845.16
[INFO 2017-06-26 18:02:39,790 main.py:47] epoch 2468, training loss: 6601.34, average training loss: 6971.09, base loss: 8845.82
[INFO 2017-06-26 18:02:40,153 main.py:47] epoch 2469, training loss: 6203.47, average training loss: 6970.20, base loss: 8845.22
[INFO 2017-06-26 18:02:40,515 main.py:47] epoch 2470, training loss: 7338.42, average training loss: 6971.00, base loss: 8846.80
[INFO 2017-06-26 18:02:40,875 main.py:47] epoch 2471, training loss: 7567.10, average training loss: 6971.51, base loss: 8847.22
[INFO 2017-06-26 18:02:41,237 main.py:47] epoch 2472, training loss: 6251.20, average training loss: 6971.42, base loss: 8847.48
[INFO 2017-06-26 18:02:41,599 main.py:47] epoch 2473, training loss: 6770.01, average training loss: 6971.13, base loss: 8846.65
[INFO 2017-06-26 18:02:41,958 main.py:47] epoch 2474, training loss: 7001.31, average training loss: 6970.85, base loss: 8846.86
[INFO 2017-06-26 18:02:42,318 main.py:47] epoch 2475, training loss: 5847.04, average training loss: 6969.53, base loss: 8845.88
[INFO 2017-06-26 18:02:42,677 main.py:47] epoch 2476, training loss: 7158.07, average training loss: 6970.58, base loss: 8846.79
[INFO 2017-06-26 18:02:43,036 main.py:47] epoch 2477, training loss: 7101.45, average training loss: 6971.17, base loss: 8847.52
[INFO 2017-06-26 18:02:43,401 main.py:47] epoch 2478, training loss: 6732.28, average training loss: 6971.29, base loss: 8847.97
[INFO 2017-06-26 18:02:43,761 main.py:47] epoch 2479, training loss: 7116.29, average training loss: 6971.47, base loss: 8848.45
[INFO 2017-06-26 18:02:44,120 main.py:47] epoch 2480, training loss: 6534.82, average training loss: 6970.42, base loss: 8847.40
[INFO 2017-06-26 18:02:44,481 main.py:47] epoch 2481, training loss: 6287.96, average training loss: 6969.45, base loss: 8846.65
[INFO 2017-06-26 18:02:44,840 main.py:47] epoch 2482, training loss: 7031.95, average training loss: 6968.80, base loss: 8846.77
[INFO 2017-06-26 18:02:45,198 main.py:47] epoch 2483, training loss: 7790.98, average training loss: 6970.89, base loss: 8850.88
[INFO 2017-06-26 18:02:45,559 main.py:47] epoch 2484, training loss: 6799.28, average training loss: 6970.39, base loss: 8851.04
[INFO 2017-06-26 18:02:45,918 main.py:47] epoch 2485, training loss: 6955.80, average training loss: 6970.14, base loss: 8851.20
[INFO 2017-06-26 18:02:46,278 main.py:47] epoch 2486, training loss: 6491.87, average training loss: 6970.35, base loss: 8851.41
[INFO 2017-06-26 18:02:46,638 main.py:47] epoch 2487, training loss: 6896.62, average training loss: 6971.12, base loss: 8852.79
[INFO 2017-06-26 18:02:46,998 main.py:47] epoch 2488, training loss: 6519.35, average training loss: 6970.36, base loss: 8851.95
[INFO 2017-06-26 18:02:47,356 main.py:47] epoch 2489, training loss: 5554.89, average training loss: 6967.08, base loss: 8847.45
[INFO 2017-06-26 18:02:47,716 main.py:47] epoch 2490, training loss: 6653.06, average training loss: 6967.14, base loss: 8847.46
[INFO 2017-06-26 18:02:48,078 main.py:47] epoch 2491, training loss: 6445.95, average training loss: 6967.46, base loss: 8847.53
[INFO 2017-06-26 18:02:48,438 main.py:47] epoch 2492, training loss: 6301.44, average training loss: 6967.21, base loss: 8847.57
[INFO 2017-06-26 18:02:48,800 main.py:47] epoch 2493, training loss: 6424.07, average training loss: 6965.95, base loss: 8846.27
[INFO 2017-06-26 18:02:49,160 main.py:47] epoch 2494, training loss: 6567.40, average training loss: 6965.66, base loss: 8845.96
[INFO 2017-06-26 18:02:49,518 main.py:47] epoch 2495, training loss: 7178.55, average training loss: 6964.09, base loss: 8844.41
[INFO 2017-06-26 18:02:49,879 main.py:47] epoch 2496, training loss: 8322.73, average training loss: 6965.75, base loss: 8847.18
[INFO 2017-06-26 18:02:50,238 main.py:47] epoch 2497, training loss: 6834.80, average training loss: 6965.69, base loss: 8847.25
[INFO 2017-06-26 18:02:50,598 main.py:47] epoch 2498, training loss: 7252.25, average training loss: 6967.02, base loss: 8849.48
[INFO 2017-06-26 18:02:50,957 main.py:47] epoch 2499, training loss: 6940.31, average training loss: 6966.90, base loss: 8848.93
[INFO 2017-06-26 18:02:50,957 main.py:49] epoch 2499, testing
[INFO 2017-06-26 18:02:55,124 main.py:100] average testing loss: 7032.45, base loss: 9111.89
[INFO 2017-06-26 18:02:55,148 main.py:73] current best accuracy: 6785.60
[INFO 2017-06-26 18:02:55,506 main.py:47] epoch 2500, training loss: 6971.13, average training loss: 6967.36, base loss: 8849.70
[INFO 2017-06-26 18:02:55,865 main.py:47] epoch 2501, training loss: 7467.44, average training loss: 6968.18, base loss: 8851.70
[INFO 2017-06-26 18:02:56,226 main.py:47] epoch 2502, training loss: 6461.16, average training loss: 6968.03, base loss: 8850.50
[INFO 2017-06-26 18:02:56,587 main.py:47] epoch 2503, training loss: 5917.41, average training loss: 6967.18, base loss: 8849.74
[INFO 2017-06-26 18:02:56,947 main.py:47] epoch 2504, training loss: 7367.76, average training loss: 6966.71, base loss: 8848.98
[INFO 2017-06-26 18:02:57,308 main.py:47] epoch 2505, training loss: 7142.38, average training loss: 6967.36, base loss: 8849.63
[INFO 2017-06-26 18:02:57,666 main.py:47] epoch 2506, training loss: 6719.93, average training loss: 6967.20, base loss: 8849.93
[INFO 2017-06-26 18:02:58,025 main.py:47] epoch 2507, training loss: 6307.12, average training loss: 6966.00, base loss: 8847.98
[INFO 2017-06-26 18:02:58,386 main.py:47] epoch 2508, training loss: 6340.28, average training loss: 6966.22, base loss: 8848.73
[INFO 2017-06-26 18:02:58,746 main.py:47] epoch 2509, training loss: 7676.58, average training loss: 6966.77, base loss: 8849.65
[INFO 2017-06-26 18:02:59,114 main.py:47] epoch 2510, training loss: 6433.52, average training loss: 6966.09, base loss: 8849.04
[INFO 2017-06-26 18:02:59,475 main.py:47] epoch 2511, training loss: 6925.46, average training loss: 6965.50, base loss: 8848.87
[INFO 2017-06-26 18:02:59,834 main.py:47] epoch 2512, training loss: 7691.17, average training loss: 6966.55, base loss: 8850.84
[INFO 2017-06-26 18:03:00,195 main.py:47] epoch 2513, training loss: 6864.04, average training loss: 6966.68, base loss: 8850.43
[INFO 2017-06-26 18:03:00,555 main.py:47] epoch 2514, training loss: 7015.72, average training loss: 6967.09, base loss: 8851.59
[INFO 2017-06-26 18:03:00,913 main.py:47] epoch 2515, training loss: 6329.17, average training loss: 6967.54, base loss: 8852.51
[INFO 2017-06-26 18:03:01,273 main.py:47] epoch 2516, training loss: 7030.62, average training loss: 6967.75, base loss: 8852.75
[INFO 2017-06-26 18:03:01,633 main.py:47] epoch 2517, training loss: 6488.72, average training loss: 6967.18, base loss: 8851.69
[INFO 2017-06-26 18:03:01,994 main.py:47] epoch 2518, training loss: 6787.85, average training loss: 6967.81, base loss: 8852.98
[INFO 2017-06-26 18:03:02,354 main.py:47] epoch 2519, training loss: 7988.81, average training loss: 6969.76, base loss: 8856.07
[INFO 2017-06-26 18:03:02,715 main.py:47] epoch 2520, training loss: 5901.39, average training loss: 6968.56, base loss: 8853.70
[INFO 2017-06-26 18:03:03,074 main.py:47] epoch 2521, training loss: 6815.89, average training loss: 6967.21, base loss: 8852.37
[INFO 2017-06-26 18:03:03,435 main.py:47] epoch 2522, training loss: 7519.13, average training loss: 6967.37, base loss: 8853.12
[INFO 2017-06-26 18:03:03,796 main.py:47] epoch 2523, training loss: 6623.26, average training loss: 6965.98, base loss: 8850.57
[INFO 2017-06-26 18:03:04,154 main.py:47] epoch 2524, training loss: 8386.63, average training loss: 6967.62, base loss: 8853.53
[INFO 2017-06-26 18:03:04,513 main.py:47] epoch 2525, training loss: 7115.04, average training loss: 6967.18, base loss: 8853.43
[INFO 2017-06-26 18:03:04,874 main.py:47] epoch 2526, training loss: 7328.31, average training loss: 6965.99, base loss: 8852.56
[INFO 2017-06-26 18:03:05,233 main.py:47] epoch 2527, training loss: 6664.80, average training loss: 6965.47, base loss: 8852.76
[INFO 2017-06-26 18:03:05,595 main.py:47] epoch 2528, training loss: 7138.49, average training loss: 6965.62, base loss: 8853.26
[INFO 2017-06-26 18:03:05,986 main.py:47] epoch 2529, training loss: 6380.67, average training loss: 6964.46, base loss: 8852.37
[INFO 2017-06-26 18:03:06,389 main.py:47] epoch 2530, training loss: 7850.23, average training loss: 6965.38, base loss: 8853.68
[INFO 2017-06-26 18:03:06,752 main.py:47] epoch 2531, training loss: 7319.66, average training loss: 6965.13, base loss: 8853.60
[INFO 2017-06-26 18:03:07,117 main.py:47] epoch 2532, training loss: 6974.78, average training loss: 6965.73, base loss: 8854.75
[INFO 2017-06-26 18:03:07,499 main.py:47] epoch 2533, training loss: 6487.51, average training loss: 6964.73, base loss: 8853.30
[INFO 2017-06-26 18:03:07,865 main.py:47] epoch 2534, training loss: 6115.94, average training loss: 6963.56, base loss: 8851.58
[INFO 2017-06-26 18:03:08,225 main.py:47] epoch 2535, training loss: 6822.58, average training loss: 6962.50, base loss: 8850.90
[INFO 2017-06-26 18:03:08,586 main.py:47] epoch 2536, training loss: 6220.30, average training loss: 6960.46, base loss: 8848.38
[INFO 2017-06-26 18:03:08,947 main.py:47] epoch 2537, training loss: 7193.94, average training loss: 6960.26, base loss: 8847.91
[INFO 2017-06-26 18:03:09,306 main.py:47] epoch 2538, training loss: 6670.38, average training loss: 6959.23, base loss: 8846.95
[INFO 2017-06-26 18:03:09,664 main.py:47] epoch 2539, training loss: 6505.93, average training loss: 6959.02, base loss: 8847.08
[INFO 2017-06-26 18:03:10,024 main.py:47] epoch 2540, training loss: 8804.29, average training loss: 6961.09, base loss: 8849.59
[INFO 2017-06-26 18:03:10,384 main.py:47] epoch 2541, training loss: 5686.96, average training loss: 6959.82, base loss: 8848.40
[INFO 2017-06-26 18:03:10,748 main.py:47] epoch 2542, training loss: 6119.40, average training loss: 6959.24, base loss: 8848.05
[INFO 2017-06-26 18:03:11,106 main.py:47] epoch 2543, training loss: 6927.98, average training loss: 6959.18, base loss: 8847.81
[INFO 2017-06-26 18:03:11,465 main.py:47] epoch 2544, training loss: 7085.43, average training loss: 6958.32, base loss: 8847.01
[INFO 2017-06-26 18:03:11,854 main.py:47] epoch 2545, training loss: 6532.69, average training loss: 6957.50, base loss: 8846.81
[INFO 2017-06-26 18:03:12,247 main.py:47] epoch 2546, training loss: 6745.00, average training loss: 6957.10, base loss: 8846.62
[INFO 2017-06-26 18:03:12,610 main.py:47] epoch 2547, training loss: 6400.02, average training loss: 6956.13, base loss: 8845.26
[INFO 2017-06-26 18:03:12,974 main.py:47] epoch 2548, training loss: 7729.84, average training loss: 6957.19, base loss: 8846.43
[INFO 2017-06-26 18:03:13,334 main.py:47] epoch 2549, training loss: 7073.23, average training loss: 6957.59, base loss: 8847.50
[INFO 2017-06-26 18:03:13,692 main.py:47] epoch 2550, training loss: 6724.12, average training loss: 6957.22, base loss: 8847.19
[INFO 2017-06-26 18:03:14,052 main.py:47] epoch 2551, training loss: 7060.67, average training loss: 6957.04, base loss: 8846.87
[INFO 2017-06-26 18:03:14,445 main.py:47] epoch 2552, training loss: 6916.18, average training loss: 6955.92, base loss: 8845.80
[INFO 2017-06-26 18:03:14,856 main.py:47] epoch 2553, training loss: 6743.81, average training loss: 6955.90, base loss: 8846.37
[INFO 2017-06-26 18:03:15,228 main.py:47] epoch 2554, training loss: 6769.34, average training loss: 6955.90, base loss: 8846.41
[INFO 2017-06-26 18:03:15,589 main.py:47] epoch 2555, training loss: 6406.92, average training loss: 6954.28, base loss: 8844.05
[INFO 2017-06-26 18:03:15,983 main.py:47] epoch 2556, training loss: 7580.80, average training loss: 6955.24, base loss: 8845.52
[INFO 2017-06-26 18:03:16,362 main.py:47] epoch 2557, training loss: 6953.05, average training loss: 6954.78, base loss: 8846.05
[INFO 2017-06-26 18:03:16,733 main.py:47] epoch 2558, training loss: 7170.39, average training loss: 6954.22, base loss: 8844.88
[INFO 2017-06-26 18:03:17,128 main.py:47] epoch 2559, training loss: 7001.11, average training loss: 6953.99, base loss: 8844.50
[INFO 2017-06-26 18:03:17,510 main.py:47] epoch 2560, training loss: 7502.18, average training loss: 6953.08, base loss: 8843.27
[INFO 2017-06-26 18:03:17,906 main.py:47] epoch 2561, training loss: 5973.41, average training loss: 6952.37, base loss: 8842.48
[INFO 2017-06-26 18:03:18,317 main.py:47] epoch 2562, training loss: 6397.65, average training loss: 6950.93, base loss: 8840.44
[INFO 2017-06-26 18:03:18,687 main.py:47] epoch 2563, training loss: 6318.81, average training loss: 6950.76, base loss: 8840.48
[INFO 2017-06-26 18:03:19,048 main.py:47] epoch 2564, training loss: 6714.00, average training loss: 6951.23, base loss: 8841.29
[INFO 2017-06-26 18:03:19,409 main.py:47] epoch 2565, training loss: 7559.92, average training loss: 6950.86, base loss: 8841.07
[INFO 2017-06-26 18:03:19,769 main.py:47] epoch 2566, training loss: 6328.38, average training loss: 6949.97, base loss: 8840.20
[INFO 2017-06-26 18:03:20,134 main.py:47] epoch 2567, training loss: 6566.88, average training loss: 6949.67, base loss: 8839.89
[INFO 2017-06-26 18:03:20,493 main.py:47] epoch 2568, training loss: 7462.14, average training loss: 6950.59, base loss: 8841.86
[INFO 2017-06-26 18:03:20,852 main.py:47] epoch 2569, training loss: 6915.34, average training loss: 6949.90, base loss: 8840.67
[INFO 2017-06-26 18:03:21,212 main.py:47] epoch 2570, training loss: 6844.21, average training loss: 6949.78, base loss: 8840.97
[INFO 2017-06-26 18:03:21,572 main.py:47] epoch 2571, training loss: 6237.29, average training loss: 6948.54, base loss: 8839.05
[INFO 2017-06-26 18:03:21,933 main.py:47] epoch 2572, training loss: 7274.71, average training loss: 6948.99, base loss: 8840.66
[INFO 2017-06-26 18:03:22,292 main.py:47] epoch 2573, training loss: 6218.93, average training loss: 6947.77, base loss: 8839.34
[INFO 2017-06-26 18:03:22,652 main.py:47] epoch 2574, training loss: 6696.24, average training loss: 6947.81, base loss: 8839.42
[INFO 2017-06-26 18:03:23,064 main.py:47] epoch 2575, training loss: 6705.00, average training loss: 6947.36, base loss: 8838.81
[INFO 2017-06-26 18:03:23,428 main.py:47] epoch 2576, training loss: 6151.91, average training loss: 6946.16, base loss: 8837.91
[INFO 2017-06-26 18:03:23,790 main.py:47] epoch 2577, training loss: 6541.30, average training loss: 6945.04, base loss: 8836.71
[INFO 2017-06-26 18:03:24,151 main.py:47] epoch 2578, training loss: 7105.75, average training loss: 6945.64, base loss: 8838.06
[INFO 2017-06-26 18:03:24,512 main.py:47] epoch 2579, training loss: 7023.89, average training loss: 6945.57, base loss: 8837.39
[INFO 2017-06-26 18:03:24,873 main.py:47] epoch 2580, training loss: 7570.33, average training loss: 6945.80, base loss: 8838.03
[INFO 2017-06-26 18:03:25,234 main.py:47] epoch 2581, training loss: 5763.52, average training loss: 6943.35, base loss: 8835.12
[INFO 2017-06-26 18:03:25,597 main.py:47] epoch 2582, training loss: 6350.07, average training loss: 6942.52, base loss: 8834.80
[INFO 2017-06-26 18:03:25,958 main.py:47] epoch 2583, training loss: 6037.28, average training loss: 6940.88, base loss: 8832.84
[INFO 2017-06-26 18:03:26,317 main.py:47] epoch 2584, training loss: 6494.85, average training loss: 6940.82, base loss: 8833.24
[INFO 2017-06-26 18:03:26,677 main.py:47] epoch 2585, training loss: 7085.81, average training loss: 6939.91, base loss: 8831.45
[INFO 2017-06-26 18:03:27,042 main.py:47] epoch 2586, training loss: 6282.52, average training loss: 6939.02, base loss: 8831.24
[INFO 2017-06-26 18:03:27,406 main.py:47] epoch 2587, training loss: 6016.77, average training loss: 6937.56, base loss: 8829.23
[INFO 2017-06-26 18:03:27,764 main.py:47] epoch 2588, training loss: 6380.83, average training loss: 6937.64, base loss: 8829.74
[INFO 2017-06-26 18:03:28,124 main.py:47] epoch 2589, training loss: 6927.69, average training loss: 6937.83, base loss: 8830.50
[INFO 2017-06-26 18:03:28,486 main.py:47] epoch 2590, training loss: 5924.58, average training loss: 6937.05, base loss: 8829.71
[INFO 2017-06-26 18:03:28,847 main.py:47] epoch 2591, training loss: 7442.77, average training loss: 6936.81, base loss: 8829.63
[INFO 2017-06-26 18:03:29,210 main.py:47] epoch 2592, training loss: 8037.54, average training loss: 6937.91, base loss: 8832.13
[INFO 2017-06-26 18:03:29,571 main.py:47] epoch 2593, training loss: 7297.89, average training loss: 6938.39, base loss: 8833.57
[INFO 2017-06-26 18:03:29,932 main.py:47] epoch 2594, training loss: 6692.94, average training loss: 6937.80, base loss: 8833.40
[INFO 2017-06-26 18:03:30,292 main.py:47] epoch 2595, training loss: 7073.15, average training loss: 6938.63, base loss: 8834.95
[INFO 2017-06-26 18:03:30,653 main.py:47] epoch 2596, training loss: 7994.92, average training loss: 6939.47, base loss: 8836.02
[INFO 2017-06-26 18:03:31,016 main.py:47] epoch 2597, training loss: 6994.28, average training loss: 6939.55, base loss: 8836.68
[INFO 2017-06-26 18:03:31,375 main.py:47] epoch 2598, training loss: 7142.18, average training loss: 6939.39, base loss: 8836.79
[INFO 2017-06-26 18:03:31,735 main.py:47] epoch 2599, training loss: 7130.20, average training loss: 6939.20, base loss: 8837.20
[INFO 2017-06-26 18:03:31,735 main.py:49] epoch 2599, testing
[INFO 2017-06-26 18:03:35,971 main.py:100] average testing loss: 6690.65, base loss: 8526.19
[INFO 2017-06-26 18:03:35,994 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 18:03:36,001 main.py:73] current best accuracy: 6690.65
[INFO 2017-06-26 18:03:36,358 main.py:47] epoch 2600, training loss: 6653.12, average training loss: 6939.31, base loss: 8837.50
[INFO 2017-06-26 18:03:36,717 main.py:47] epoch 2601, training loss: 7665.53, average training loss: 6939.17, base loss: 8837.91
[INFO 2017-06-26 18:03:37,077 main.py:47] epoch 2602, training loss: 7421.67, average training loss: 6939.58, base loss: 8838.69
[INFO 2017-06-26 18:03:37,436 main.py:47] epoch 2603, training loss: 6408.24, average training loss: 6939.77, base loss: 8839.58
[INFO 2017-06-26 18:03:37,795 main.py:47] epoch 2604, training loss: 6285.51, average training loss: 6938.20, base loss: 8837.62
[INFO 2017-06-26 18:03:38,153 main.py:47] epoch 2605, training loss: 6898.21, average training loss: 6936.88, base loss: 8836.16
[INFO 2017-06-26 18:03:38,512 main.py:47] epoch 2606, training loss: 6632.73, average training loss: 6936.34, base loss: 8835.54
[INFO 2017-06-26 18:03:38,871 main.py:47] epoch 2607, training loss: 7273.55, average training loss: 6935.39, base loss: 8834.14
[INFO 2017-06-26 18:03:39,230 main.py:47] epoch 2608, training loss: 6356.11, average training loss: 6933.58, base loss: 8831.01
[INFO 2017-06-26 18:03:39,589 main.py:47] epoch 2609, training loss: 6404.66, average training loss: 6933.21, base loss: 8830.52
[INFO 2017-06-26 18:03:39,950 main.py:47] epoch 2610, training loss: 6293.88, average training loss: 6932.99, base loss: 8831.08
[INFO 2017-06-26 18:03:40,309 main.py:47] epoch 2611, training loss: 6947.17, average training loss: 6931.76, base loss: 8830.43
[INFO 2017-06-26 18:03:40,668 main.py:47] epoch 2612, training loss: 7530.94, average training loss: 6931.46, base loss: 8830.72
[INFO 2017-06-26 18:03:41,029 main.py:47] epoch 2613, training loss: 6557.82, average training loss: 6930.63, base loss: 8830.26
[INFO 2017-06-26 18:03:41,389 main.py:47] epoch 2614, training loss: 7020.38, average training loss: 6930.74, base loss: 8830.54
[INFO 2017-06-26 18:03:41,748 main.py:47] epoch 2615, training loss: 7406.72, average training loss: 6929.98, base loss: 8829.63
[INFO 2017-06-26 18:03:42,108 main.py:47] epoch 2616, training loss: 7040.36, average training loss: 6930.51, base loss: 8830.06
[INFO 2017-06-26 18:03:42,468 main.py:47] epoch 2617, training loss: 6475.25, average training loss: 6930.26, base loss: 8829.73
[INFO 2017-06-26 18:03:42,827 main.py:47] epoch 2618, training loss: 7331.85, average training loss: 6930.21, base loss: 8829.73
[INFO 2017-06-26 18:03:43,187 main.py:47] epoch 2619, training loss: 7933.56, average training loss: 6931.50, base loss: 8831.36
[INFO 2017-06-26 18:03:43,548 main.py:47] epoch 2620, training loss: 6827.54, average training loss: 6931.34, base loss: 8831.35
[INFO 2017-06-26 18:03:43,907 main.py:47] epoch 2621, training loss: 7236.77, average training loss: 6931.51, base loss: 8831.38
[INFO 2017-06-26 18:03:44,266 main.py:47] epoch 2622, training loss: 6677.98, average training loss: 6932.11, base loss: 8833.46
[INFO 2017-06-26 18:03:44,626 main.py:47] epoch 2623, training loss: 6648.54, average training loss: 6932.58, base loss: 8834.01
[INFO 2017-06-26 18:03:44,987 main.py:47] epoch 2624, training loss: 6157.23, average training loss: 6931.79, base loss: 8832.68
[INFO 2017-06-26 18:03:45,347 main.py:47] epoch 2625, training loss: 6559.48, average training loss: 6931.46, base loss: 8832.04
[INFO 2017-06-26 18:03:45,707 main.py:47] epoch 2626, training loss: 7277.88, average training loss: 6931.03, base loss: 8831.34
[INFO 2017-06-26 18:03:46,067 main.py:47] epoch 2627, training loss: 6913.54, average training loss: 6931.02, base loss: 8832.00
[INFO 2017-06-26 18:03:46,429 main.py:47] epoch 2628, training loss: 7009.44, average training loss: 6930.94, base loss: 8832.56
[INFO 2017-06-26 18:03:46,790 main.py:47] epoch 2629, training loss: 6643.73, average training loss: 6930.61, base loss: 8832.84
[INFO 2017-06-26 18:03:47,153 main.py:47] epoch 2630, training loss: 6911.43, average training loss: 6931.04, base loss: 8833.36
[INFO 2017-06-26 18:03:47,514 main.py:47] epoch 2631, training loss: 6642.86, average training loss: 6931.58, base loss: 8833.94
[INFO 2017-06-26 18:03:47,875 main.py:47] epoch 2632, training loss: 6206.73, average training loss: 6930.88, base loss: 8833.55
[INFO 2017-06-26 18:03:48,234 main.py:47] epoch 2633, training loss: 6712.66, average training loss: 6929.95, base loss: 8832.19
[INFO 2017-06-26 18:03:48,595 main.py:47] epoch 2634, training loss: 7422.78, average training loss: 6930.32, base loss: 8832.96
[INFO 2017-06-26 18:03:48,954 main.py:47] epoch 2635, training loss: 7013.61, average training loss: 6929.76, base loss: 8832.62
[INFO 2017-06-26 18:03:49,313 main.py:47] epoch 2636, training loss: 7046.51, average training loss: 6930.33, base loss: 8833.48
[INFO 2017-06-26 18:03:49,675 main.py:47] epoch 2637, training loss: 5772.64, average training loss: 6929.24, base loss: 8832.36
[INFO 2017-06-26 18:03:50,038 main.py:47] epoch 2638, training loss: 6803.59, average training loss: 6929.85, base loss: 8833.26
[INFO 2017-06-26 18:03:50,399 main.py:47] epoch 2639, training loss: 6488.13, average training loss: 6929.21, base loss: 8832.58
[INFO 2017-06-26 18:03:50,762 main.py:47] epoch 2640, training loss: 6728.92, average training loss: 6929.12, base loss: 8832.31
[INFO 2017-06-26 18:03:51,122 main.py:47] epoch 2641, training loss: 6808.47, average training loss: 6929.08, base loss: 8832.10
[INFO 2017-06-26 18:03:51,480 main.py:47] epoch 2642, training loss: 6403.60, average training loss: 6928.40, base loss: 8831.00
[INFO 2017-06-26 18:03:51,841 main.py:47] epoch 2643, training loss: 6687.17, average training loss: 6928.38, base loss: 8831.46
[INFO 2017-06-26 18:03:52,201 main.py:47] epoch 2644, training loss: 7164.00, average training loss: 6928.85, base loss: 8832.52
[INFO 2017-06-26 18:03:52,561 main.py:47] epoch 2645, training loss: 7150.86, average training loss: 6928.91, base loss: 8833.53
[INFO 2017-06-26 18:03:52,920 main.py:47] epoch 2646, training loss: 7218.19, average training loss: 6929.14, base loss: 8834.22
[INFO 2017-06-26 18:03:53,280 main.py:47] epoch 2647, training loss: 6964.71, average training loss: 6929.41, base loss: 8835.16
[INFO 2017-06-26 18:03:53,642 main.py:47] epoch 2648, training loss: 7479.72, average training loss: 6930.30, base loss: 8836.57
[INFO 2017-06-26 18:03:54,001 main.py:47] epoch 2649, training loss: 5979.50, average training loss: 6930.31, base loss: 8836.79
[INFO 2017-06-26 18:03:54,360 main.py:47] epoch 2650, training loss: 6747.92, average training loss: 6930.74, base loss: 8838.41
[INFO 2017-06-26 18:03:54,720 main.py:47] epoch 2651, training loss: 7608.45, average training loss: 6931.47, base loss: 8839.71
[INFO 2017-06-26 18:03:55,081 main.py:47] epoch 2652, training loss: 7106.17, average training loss: 6931.67, base loss: 8841.02
[INFO 2017-06-26 18:03:55,441 main.py:47] epoch 2653, training loss: 6906.76, average training loss: 6932.24, base loss: 8842.21
[INFO 2017-06-26 18:03:55,801 main.py:47] epoch 2654, training loss: 7055.91, average training loss: 6931.79, base loss: 8841.42
[INFO 2017-06-26 18:03:56,160 main.py:47] epoch 2655, training loss: 6213.63, average training loss: 6930.16, base loss: 8839.25
[INFO 2017-06-26 18:03:56,519 main.py:47] epoch 2656, training loss: 6955.16, average training loss: 6930.11, base loss: 8839.29
[INFO 2017-06-26 18:03:56,879 main.py:47] epoch 2657, training loss: 5799.59, average training loss: 6929.30, base loss: 8838.19
[INFO 2017-06-26 18:03:57,239 main.py:47] epoch 2658, training loss: 7469.93, average training loss: 6930.08, base loss: 8840.82
[INFO 2017-06-26 18:03:57,599 main.py:47] epoch 2659, training loss: 6585.15, average training loss: 6929.57, base loss: 8840.83
[INFO 2017-06-26 18:03:57,958 main.py:47] epoch 2660, training loss: 6674.98, average training loss: 6929.93, base loss: 8842.05
[INFO 2017-06-26 18:03:58,317 main.py:47] epoch 2661, training loss: 6461.64, average training loss: 6929.16, base loss: 8840.12
[INFO 2017-06-26 18:03:58,677 main.py:47] epoch 2662, training loss: 6723.05, average training loss: 6928.09, base loss: 8838.73
[INFO 2017-06-26 18:03:59,038 main.py:47] epoch 2663, training loss: 7423.62, average training loss: 6927.84, base loss: 8837.92
[INFO 2017-06-26 18:03:59,398 main.py:47] epoch 2664, training loss: 6895.16, average training loss: 6928.05, base loss: 8838.60
[INFO 2017-06-26 18:03:59,756 main.py:47] epoch 2665, training loss: 6630.83, average training loss: 6928.30, base loss: 8839.63
[INFO 2017-06-26 18:04:00,113 main.py:47] epoch 2666, training loss: 6564.12, average training loss: 6928.57, base loss: 8840.03
[INFO 2017-06-26 18:04:00,473 main.py:47] epoch 2667, training loss: 6239.59, average training loss: 6928.19, base loss: 8839.72
[INFO 2017-06-26 18:04:00,833 main.py:47] epoch 2668, training loss: 7264.52, average training loss: 6928.74, base loss: 8840.18
[INFO 2017-06-26 18:04:01,194 main.py:47] epoch 2669, training loss: 6919.13, average training loss: 6927.83, base loss: 8838.82
[INFO 2017-06-26 18:04:01,552 main.py:47] epoch 2670, training loss: 6504.05, average training loss: 6927.99, base loss: 8839.48
[INFO 2017-06-26 18:04:01,912 main.py:47] epoch 2671, training loss: 7582.86, average training loss: 6929.03, base loss: 8841.37
[INFO 2017-06-26 18:04:02,273 main.py:47] epoch 2672, training loss: 6876.57, average training loss: 6928.11, base loss: 8839.96
[INFO 2017-06-26 18:04:02,633 main.py:47] epoch 2673, training loss: 7146.32, average training loss: 6928.26, base loss: 8840.09
[INFO 2017-06-26 18:04:02,993 main.py:47] epoch 2674, training loss: 6432.40, average training loss: 6926.77, base loss: 8837.79
[INFO 2017-06-26 18:04:03,354 main.py:47] epoch 2675, training loss: 6819.09, average training loss: 6926.63, base loss: 8837.76
[INFO 2017-06-26 18:04:03,715 main.py:47] epoch 2676, training loss: 6300.68, average training loss: 6926.58, base loss: 8838.29
[INFO 2017-06-26 18:04:04,076 main.py:47] epoch 2677, training loss: 6347.19, average training loss: 6926.25, base loss: 8837.96
[INFO 2017-06-26 18:04:04,434 main.py:47] epoch 2678, training loss: 7002.72, average training loss: 6926.65, base loss: 8838.49
[INFO 2017-06-26 18:04:04,794 main.py:47] epoch 2679, training loss: 6318.87, average training loss: 6926.08, base loss: 8837.83
[INFO 2017-06-26 18:04:05,151 main.py:47] epoch 2680, training loss: 7798.20, average training loss: 6926.69, base loss: 8838.61
[INFO 2017-06-26 18:04:05,511 main.py:47] epoch 2681, training loss: 6112.00, average training loss: 6925.52, base loss: 8836.97
[INFO 2017-06-26 18:04:05,871 main.py:47] epoch 2682, training loss: 7151.81, average training loss: 6924.35, base loss: 8835.66
[INFO 2017-06-26 18:04:06,231 main.py:47] epoch 2683, training loss: 7581.70, average training loss: 6925.28, base loss: 8837.77
[INFO 2017-06-26 18:04:06,590 main.py:47] epoch 2684, training loss: 6707.76, average training loss: 6924.45, base loss: 8837.24
[INFO 2017-06-26 18:04:06,951 main.py:47] epoch 2685, training loss: 8160.82, average training loss: 6925.26, base loss: 8838.71
[INFO 2017-06-26 18:04:07,310 main.py:47] epoch 2686, training loss: 6177.64, average training loss: 6924.18, base loss: 8837.08
[INFO 2017-06-26 18:04:07,670 main.py:47] epoch 2687, training loss: 6927.33, average training loss: 6924.64, base loss: 8838.25
[INFO 2017-06-26 18:04:08,030 main.py:47] epoch 2688, training loss: 7548.62, average training loss: 6925.42, base loss: 8839.42
[INFO 2017-06-26 18:04:08,392 main.py:47] epoch 2689, training loss: 7283.74, average training loss: 6925.91, base loss: 8840.32
[INFO 2017-06-26 18:04:08,750 main.py:47] epoch 2690, training loss: 7434.61, average training loss: 6926.01, base loss: 8841.08
[INFO 2017-06-26 18:04:09,110 main.py:47] epoch 2691, training loss: 6958.99, average training loss: 6926.81, base loss: 8841.88
[INFO 2017-06-26 18:04:09,468 main.py:47] epoch 2692, training loss: 6836.15, average training loss: 6926.30, base loss: 8842.45
[INFO 2017-06-26 18:04:09,832 main.py:47] epoch 2693, training loss: 6351.71, average training loss: 6924.97, base loss: 8840.18
[INFO 2017-06-26 18:04:10,189 main.py:47] epoch 2694, training loss: 6850.73, average training loss: 6925.21, base loss: 8840.62
[INFO 2017-06-26 18:04:10,549 main.py:47] epoch 2695, training loss: 6460.85, average training loss: 6924.41, base loss: 8839.69
[INFO 2017-06-26 18:04:10,909 main.py:47] epoch 2696, training loss: 6305.90, average training loss: 6923.54, base loss: 8839.45
[INFO 2017-06-26 18:04:11,269 main.py:47] epoch 2697, training loss: 7078.18, average training loss: 6923.83, base loss: 8840.04
[INFO 2017-06-26 18:04:11,629 main.py:47] epoch 2698, training loss: 6877.17, average training loss: 6922.75, base loss: 8838.97
[INFO 2017-06-26 18:04:11,992 main.py:47] epoch 2699, training loss: 6406.83, average training loss: 6921.99, base loss: 8838.15
[INFO 2017-06-26 18:04:11,992 main.py:49] epoch 2699, testing
[INFO 2017-06-26 18:04:16,145 main.py:100] average testing loss: 6709.69, base loss: 8718.23
[INFO 2017-06-26 18:04:16,170 main.py:73] current best accuracy: 6690.65
[INFO 2017-06-26 18:04:16,529 main.py:47] epoch 2700, training loss: 6602.88, average training loss: 6922.04, base loss: 8838.64
[INFO 2017-06-26 18:04:16,890 main.py:47] epoch 2701, training loss: 6273.92, average training loss: 6921.58, base loss: 8838.71
[INFO 2017-06-26 18:04:17,251 main.py:47] epoch 2702, training loss: 6321.82, average training loss: 6921.06, base loss: 8838.49
[INFO 2017-06-26 18:04:17,611 main.py:47] epoch 2703, training loss: 7362.53, average training loss: 6920.99, base loss: 8838.36
[INFO 2017-06-26 18:04:17,971 main.py:47] epoch 2704, training loss: 7282.70, average training loss: 6920.62, base loss: 8838.08
[INFO 2017-06-26 18:04:18,330 main.py:47] epoch 2705, training loss: 6549.96, average training loss: 6919.75, base loss: 8836.71
[INFO 2017-06-26 18:04:18,691 main.py:47] epoch 2706, training loss: 5987.40, average training loss: 6918.61, base loss: 8835.22
[INFO 2017-06-26 18:04:19,050 main.py:47] epoch 2707, training loss: 7034.06, average training loss: 6919.17, base loss: 8836.76
[INFO 2017-06-26 18:04:19,410 main.py:47] epoch 2708, training loss: 6309.63, average training loss: 6917.87, base loss: 8834.36
[INFO 2017-06-26 18:04:19,771 main.py:47] epoch 2709, training loss: 7677.74, average training loss: 6918.53, base loss: 8836.13
[INFO 2017-06-26 18:04:20,131 main.py:47] epoch 2710, training loss: 7508.51, average training loss: 6918.95, base loss: 8836.29
[INFO 2017-06-26 18:04:20,490 main.py:47] epoch 2711, training loss: 7309.62, average training loss: 6918.73, base loss: 8836.15
[INFO 2017-06-26 18:04:20,848 main.py:47] epoch 2712, training loss: 5921.79, average training loss: 6918.35, base loss: 8836.06
[INFO 2017-06-26 18:04:21,208 main.py:47] epoch 2713, training loss: 7148.59, average training loss: 6918.20, base loss: 8836.24
[INFO 2017-06-26 18:04:21,568 main.py:47] epoch 2714, training loss: 5976.77, average training loss: 6917.52, base loss: 8835.15
[INFO 2017-06-26 18:04:21,929 main.py:47] epoch 2715, training loss: 6338.09, average training loss: 6917.06, base loss: 8835.79
[INFO 2017-06-26 18:04:22,289 main.py:47] epoch 2716, training loss: 6829.06, average training loss: 6916.50, base loss: 8835.04
[INFO 2017-06-26 18:04:22,650 main.py:47] epoch 2717, training loss: 6721.71, average training loss: 6916.53, base loss: 8835.67
[INFO 2017-06-26 18:04:23,011 main.py:47] epoch 2718, training loss: 6761.23, average training loss: 6916.00, base loss: 8834.95
[INFO 2017-06-26 18:04:23,371 main.py:47] epoch 2719, training loss: 7471.40, average training loss: 6915.89, base loss: 8834.56
[INFO 2017-06-26 18:04:23,736 main.py:47] epoch 2720, training loss: 6857.21, average training loss: 6915.61, base loss: 8834.49
[INFO 2017-06-26 18:04:24,095 main.py:47] epoch 2721, training loss: 7686.31, average training loss: 6915.74, base loss: 8835.15
[INFO 2017-06-26 18:04:24,455 main.py:47] epoch 2722, training loss: 6084.13, average training loss: 6914.88, base loss: 8834.13
[INFO 2017-06-26 18:04:24,815 main.py:47] epoch 2723, training loss: 6536.49, average training loss: 6914.81, base loss: 8834.81
[INFO 2017-06-26 18:04:25,176 main.py:47] epoch 2724, training loss: 6579.59, average training loss: 6915.08, base loss: 8835.49
[INFO 2017-06-26 18:04:25,542 main.py:47] epoch 2725, training loss: 5919.75, average training loss: 6914.14, base loss: 8834.24
[INFO 2017-06-26 18:04:25,902 main.py:47] epoch 2726, training loss: 6741.33, average training loss: 6913.24, base loss: 8833.52
[INFO 2017-06-26 18:04:26,260 main.py:47] epoch 2727, training loss: 7203.90, average training loss: 6914.11, base loss: 8835.11
[INFO 2017-06-26 18:04:26,620 main.py:47] epoch 2728, training loss: 6450.29, average training loss: 6913.74, base loss: 8834.98
[INFO 2017-06-26 18:04:26,981 main.py:47] epoch 2729, training loss: 6734.51, average training loss: 6913.65, base loss: 8835.85
[INFO 2017-06-26 18:04:27,340 main.py:47] epoch 2730, training loss: 6652.62, average training loss: 6913.64, base loss: 8836.16
[INFO 2017-06-26 18:04:27,701 main.py:47] epoch 2731, training loss: 7219.12, average training loss: 6914.36, base loss: 8837.38
[INFO 2017-06-26 18:04:28,062 main.py:47] epoch 2732, training loss: 6881.11, average training loss: 6915.15, base loss: 8839.15
[INFO 2017-06-26 18:04:28,421 main.py:47] epoch 2733, training loss: 6808.51, average training loss: 6914.44, base loss: 8837.85
[INFO 2017-06-26 18:04:28,783 main.py:47] epoch 2734, training loss: 7139.38, average training loss: 6914.85, base loss: 8838.38
[INFO 2017-06-26 18:04:29,145 main.py:47] epoch 2735, training loss: 7537.27, average training loss: 6915.46, base loss: 8839.22
[INFO 2017-06-26 18:04:29,506 main.py:47] epoch 2736, training loss: 6414.29, average training loss: 6914.84, base loss: 8838.91
[INFO 2017-06-26 18:04:29,866 main.py:47] epoch 2737, training loss: 7172.73, average training loss: 6914.50, base loss: 8839.33
[INFO 2017-06-26 18:04:30,227 main.py:47] epoch 2738, training loss: 6673.14, average training loss: 6914.11, base loss: 8837.93
[INFO 2017-06-26 18:04:30,584 main.py:47] epoch 2739, training loss: 7510.15, average training loss: 6914.37, base loss: 8838.63
[INFO 2017-06-26 18:04:30,946 main.py:47] epoch 2740, training loss: 6909.73, average training loss: 6914.86, base loss: 8839.67
[INFO 2017-06-26 18:04:31,308 main.py:47] epoch 2741, training loss: 6690.41, average training loss: 6914.26, base loss: 8838.33
[INFO 2017-06-26 18:04:31,666 main.py:47] epoch 2742, training loss: 7183.75, average training loss: 6914.86, base loss: 8839.02
[INFO 2017-06-26 18:04:32,027 main.py:47] epoch 2743, training loss: 6827.80, average training loss: 6914.64, base loss: 8839.18
[INFO 2017-06-26 18:04:32,386 main.py:47] epoch 2744, training loss: 6484.86, average training loss: 6914.29, base loss: 8838.98
[INFO 2017-06-26 18:04:32,746 main.py:47] epoch 2745, training loss: 5884.67, average training loss: 6913.80, base loss: 8838.93
[INFO 2017-06-26 18:04:33,109 main.py:47] epoch 2746, training loss: 6522.85, average training loss: 6912.90, base loss: 8838.37
[INFO 2017-06-26 18:04:33,469 main.py:47] epoch 2747, training loss: 7118.45, average training loss: 6913.70, base loss: 8839.65
[INFO 2017-06-26 18:04:33,832 main.py:47] epoch 2748, training loss: 6367.96, average training loss: 6912.94, base loss: 8839.12
[INFO 2017-06-26 18:04:34,194 main.py:47] epoch 2749, training loss: 6023.82, average training loss: 6911.41, base loss: 8837.27
[INFO 2017-06-26 18:04:34,555 main.py:47] epoch 2750, training loss: 6880.68, average training loss: 6911.66, base loss: 8838.44
[INFO 2017-06-26 18:04:34,916 main.py:47] epoch 2751, training loss: 7274.24, average training loss: 6911.71, base loss: 8839.19
[INFO 2017-06-26 18:04:35,277 main.py:47] epoch 2752, training loss: 7606.02, average training loss: 6911.17, base loss: 8838.89
[INFO 2017-06-26 18:04:35,636 main.py:47] epoch 2753, training loss: 7109.85, average training loss: 6911.52, base loss: 8839.48
[INFO 2017-06-26 18:04:35,997 main.py:47] epoch 2754, training loss: 6196.34, average training loss: 6910.81, base loss: 8838.87
[INFO 2017-06-26 18:04:36,358 main.py:47] epoch 2755, training loss: 6565.60, average training loss: 6909.67, base loss: 8837.83
[INFO 2017-06-26 18:04:36,721 main.py:47] epoch 2756, training loss: 6296.89, average training loss: 6909.39, base loss: 8837.54
[INFO 2017-06-26 18:04:37,082 main.py:47] epoch 2757, training loss: 6860.37, average training loss: 6909.93, base loss: 8838.13
[INFO 2017-06-26 18:04:37,443 main.py:47] epoch 2758, training loss: 7045.03, average training loss: 6908.51, base loss: 8836.29
[INFO 2017-06-26 18:04:37,803 main.py:47] epoch 2759, training loss: 6084.02, average training loss: 6907.86, base loss: 8835.38
[INFO 2017-06-26 18:04:38,162 main.py:47] epoch 2760, training loss: 6652.91, average training loss: 6908.05, base loss: 8836.40
[INFO 2017-06-26 18:04:38,525 main.py:47] epoch 2761, training loss: 7255.62, average training loss: 6906.76, base loss: 8834.83
[INFO 2017-06-26 18:04:38,885 main.py:47] epoch 2762, training loss: 7187.78, average training loss: 6907.24, base loss: 8835.95
[INFO 2017-06-26 18:04:39,248 main.py:47] epoch 2763, training loss: 6281.78, average training loss: 6906.89, base loss: 8835.05
[INFO 2017-06-26 18:04:39,607 main.py:47] epoch 2764, training loss: 6772.38, average training loss: 6906.71, base loss: 8835.34
[INFO 2017-06-26 18:04:39,969 main.py:47] epoch 2765, training loss: 6872.43, average training loss: 6907.02, base loss: 8836.21
[INFO 2017-06-26 18:04:40,329 main.py:47] epoch 2766, training loss: 8354.24, average training loss: 6908.86, base loss: 8840.15
[INFO 2017-06-26 18:04:40,690 main.py:47] epoch 2767, training loss: 7128.20, average training loss: 6909.71, base loss: 8841.69
[INFO 2017-06-26 18:04:41,050 main.py:47] epoch 2768, training loss: 6851.59, average training loss: 6909.52, base loss: 8841.87
[INFO 2017-06-26 18:04:41,411 main.py:47] epoch 2769, training loss: 8376.32, average training loss: 6911.32, base loss: 8844.99
[INFO 2017-06-26 18:04:41,774 main.py:47] epoch 2770, training loss: 6801.98, average training loss: 6911.31, base loss: 8844.79
[INFO 2017-06-26 18:04:42,136 main.py:47] epoch 2771, training loss: 6917.09, average training loss: 6912.32, base loss: 8846.75
[INFO 2017-06-26 18:04:42,495 main.py:47] epoch 2772, training loss: 7215.89, average training loss: 6912.68, base loss: 8847.25
[INFO 2017-06-26 18:04:42,856 main.py:47] epoch 2773, training loss: 7443.86, average training loss: 6914.00, base loss: 8849.55
[INFO 2017-06-26 18:04:43,218 main.py:47] epoch 2774, training loss: 7132.89, average training loss: 6914.16, base loss: 8850.67
[INFO 2017-06-26 18:04:43,580 main.py:47] epoch 2775, training loss: 7073.53, average training loss: 6914.08, base loss: 8849.68
[INFO 2017-06-26 18:04:43,940 main.py:47] epoch 2776, training loss: 6744.58, average training loss: 6914.47, base loss: 8850.45
[INFO 2017-06-26 18:04:44,301 main.py:47] epoch 2777, training loss: 5994.10, average training loss: 6914.38, base loss: 8849.98
[INFO 2017-06-26 18:04:44,661 main.py:47] epoch 2778, training loss: 6094.47, average training loss: 6913.46, base loss: 8848.01
[INFO 2017-06-26 18:04:45,022 main.py:47] epoch 2779, training loss: 6591.16, average training loss: 6913.06, base loss: 8846.97
[INFO 2017-06-26 18:04:45,385 main.py:47] epoch 2780, training loss: 7740.56, average training loss: 6913.71, base loss: 8847.95
[INFO 2017-06-26 18:04:45,745 main.py:47] epoch 2781, training loss: 6892.92, average training loss: 6913.86, base loss: 8848.07
[INFO 2017-06-26 18:04:46,105 main.py:47] epoch 2782, training loss: 7069.62, average training loss: 6914.45, base loss: 8849.90
[INFO 2017-06-26 18:04:46,467 main.py:47] epoch 2783, training loss: 7687.12, average training loss: 6915.54, base loss: 8851.83
[INFO 2017-06-26 18:04:46,831 main.py:47] epoch 2784, training loss: 6644.40, average training loss: 6915.97, base loss: 8852.97
[INFO 2017-06-26 18:04:47,192 main.py:47] epoch 2785, training loss: 7276.88, average training loss: 6915.82, base loss: 8853.34
[INFO 2017-06-26 18:04:47,552 main.py:47] epoch 2786, training loss: 7248.41, average training loss: 6915.63, base loss: 8853.29
[INFO 2017-06-26 18:04:47,914 main.py:47] epoch 2787, training loss: 6134.80, average training loss: 6914.73, base loss: 8852.35
[INFO 2017-06-26 18:04:48,271 main.py:47] epoch 2788, training loss: 5958.94, average training loss: 6913.90, base loss: 8851.30
[INFO 2017-06-26 18:04:48,632 main.py:47] epoch 2789, training loss: 6888.57, average training loss: 6914.33, base loss: 8852.32
[INFO 2017-06-26 18:04:48,993 main.py:47] epoch 2790, training loss: 6400.58, average training loss: 6914.74, base loss: 8852.15
[INFO 2017-06-26 18:04:49,356 main.py:47] epoch 2791, training loss: 6603.49, average training loss: 6915.10, base loss: 8852.94
[INFO 2017-06-26 18:04:49,716 main.py:47] epoch 2792, training loss: 6597.87, average training loss: 6914.51, base loss: 8852.38
[INFO 2017-06-26 18:04:50,076 main.py:47] epoch 2793, training loss: 7467.88, average training loss: 6915.68, base loss: 8854.38
[INFO 2017-06-26 18:04:50,437 main.py:47] epoch 2794, training loss: 6419.88, average training loss: 6914.55, base loss: 8852.38
[INFO 2017-06-26 18:04:50,799 main.py:47] epoch 2795, training loss: 6369.38, average training loss: 6914.50, base loss: 8852.30
[INFO 2017-06-26 18:04:51,160 main.py:47] epoch 2796, training loss: 6915.78, average training loss: 6913.92, base loss: 8851.59
[INFO 2017-06-26 18:04:51,522 main.py:47] epoch 2797, training loss: 6838.79, average training loss: 6913.21, base loss: 8850.49
[INFO 2017-06-26 18:04:51,883 main.py:47] epoch 2798, training loss: 7598.51, average training loss: 6913.46, base loss: 8850.89
[INFO 2017-06-26 18:04:52,244 main.py:47] epoch 2799, training loss: 6635.36, average training loss: 6913.30, base loss: 8850.48
[INFO 2017-06-26 18:04:52,244 main.py:49] epoch 2799, testing
[INFO 2017-06-26 18:04:56,504 main.py:100] average testing loss: 7089.16, base loss: 9320.93
[INFO 2017-06-26 18:04:56,529 main.py:73] current best accuracy: 6690.65
[INFO 2017-06-26 18:04:56,891 main.py:47] epoch 2800, training loss: 7051.52, average training loss: 6913.68, base loss: 8851.68
[INFO 2017-06-26 18:04:57,252 main.py:47] epoch 2801, training loss: 6264.66, average training loss: 6912.90, base loss: 8850.88
[INFO 2017-06-26 18:04:57,613 main.py:47] epoch 2802, training loss: 7762.04, average training loss: 6913.83, base loss: 8852.47
[INFO 2017-06-26 18:04:57,975 main.py:47] epoch 2803, training loss: 6869.25, average training loss: 6914.12, base loss: 8852.94
[INFO 2017-06-26 18:04:58,337 main.py:47] epoch 2804, training loss: 6411.95, average training loss: 6914.46, base loss: 8853.32
[INFO 2017-06-26 18:04:58,726 main.py:47] epoch 2805, training loss: 6646.38, average training loss: 6914.19, base loss: 8853.47
[INFO 2017-06-26 18:04:59,091 main.py:47] epoch 2806, training loss: 7030.68, average training loss: 6914.10, base loss: 8853.59
[INFO 2017-06-26 18:04:59,451 main.py:47] epoch 2807, training loss: 6892.33, average training loss: 6915.03, base loss: 8855.13
[INFO 2017-06-26 18:04:59,813 main.py:47] epoch 2808, training loss: 6756.35, average training loss: 6914.48, base loss: 8854.68
[INFO 2017-06-26 18:05:00,175 main.py:47] epoch 2809, training loss: 6783.04, average training loss: 6914.31, base loss: 8855.15
[INFO 2017-06-26 18:05:00,536 main.py:47] epoch 2810, training loss: 7423.50, average training loss: 6914.54, base loss: 8855.52
[INFO 2017-06-26 18:05:00,900 main.py:47] epoch 2811, training loss: 7822.11, average training loss: 6915.31, base loss: 8856.70
[INFO 2017-06-26 18:05:01,261 main.py:47] epoch 2812, training loss: 7658.34, average training loss: 6915.88, base loss: 8857.82
[INFO 2017-06-26 18:05:01,622 main.py:47] epoch 2813, training loss: 6805.59, average training loss: 6915.53, base loss: 8857.73
[INFO 2017-06-26 18:05:01,981 main.py:47] epoch 2814, training loss: 6244.09, average training loss: 6915.12, base loss: 8857.24
[INFO 2017-06-26 18:05:02,342 main.py:47] epoch 2815, training loss: 6404.71, average training loss: 6914.56, base loss: 8856.48
[INFO 2017-06-26 18:05:02,703 main.py:47] epoch 2816, training loss: 6635.31, average training loss: 6914.09, base loss: 8855.28
[INFO 2017-06-26 18:05:03,063 main.py:47] epoch 2817, training loss: 6787.31, average training loss: 6913.29, base loss: 8853.90
[INFO 2017-06-26 18:05:03,423 main.py:47] epoch 2818, training loss: 6721.31, average training loss: 6913.63, base loss: 8854.63
[INFO 2017-06-26 18:05:03,783 main.py:47] epoch 2819, training loss: 7115.07, average training loss: 6913.59, base loss: 8854.44
[INFO 2017-06-26 18:05:04,143 main.py:47] epoch 2820, training loss: 6624.98, average training loss: 6912.82, base loss: 8853.03
[INFO 2017-06-26 18:05:04,503 main.py:47] epoch 2821, training loss: 6967.08, average training loss: 6912.27, base loss: 8852.54
[INFO 2017-06-26 18:05:04,863 main.py:47] epoch 2822, training loss: 7109.71, average training loss: 6910.25, base loss: 8850.38
[INFO 2017-06-26 18:05:05,225 main.py:47] epoch 2823, training loss: 7994.65, average training loss: 6910.76, base loss: 8851.06
[INFO 2017-06-26 18:05:05,585 main.py:47] epoch 2824, training loss: 7738.40, average training loss: 6912.25, base loss: 8853.50
[INFO 2017-06-26 18:05:05,943 main.py:47] epoch 2825, training loss: 6156.99, average training loss: 6912.45, base loss: 8853.93
[INFO 2017-06-26 18:05:06,304 main.py:47] epoch 2826, training loss: 6193.16, average training loss: 6911.25, base loss: 8852.52
[INFO 2017-06-26 18:05:06,666 main.py:47] epoch 2827, training loss: 6931.73, average training loss: 6911.10, base loss: 8852.37
[INFO 2017-06-26 18:05:07,027 main.py:47] epoch 2828, training loss: 6298.83, average training loss: 6910.97, base loss: 8852.73
[INFO 2017-06-26 18:05:07,389 main.py:47] epoch 2829, training loss: 6662.57, average training loss: 6910.58, base loss: 8852.58
[INFO 2017-06-26 18:05:07,751 main.py:47] epoch 2830, training loss: 8551.28, average training loss: 6912.64, base loss: 8855.92
[INFO 2017-06-26 18:05:08,113 main.py:47] epoch 2831, training loss: 5990.69, average training loss: 6911.99, base loss: 8855.07
[INFO 2017-06-26 18:05:08,474 main.py:47] epoch 2832, training loss: 6907.85, average training loss: 6913.05, base loss: 8857.06
[INFO 2017-06-26 18:05:08,832 main.py:47] epoch 2833, training loss: 6974.72, average training loss: 6913.49, base loss: 8857.72
[INFO 2017-06-26 18:05:09,194 main.py:47] epoch 2834, training loss: 6550.20, average training loss: 6912.43, base loss: 8855.90
[INFO 2017-06-26 18:05:09,556 main.py:47] epoch 2835, training loss: 7176.93, average training loss: 6912.31, base loss: 8856.17
[INFO 2017-06-26 18:05:09,919 main.py:47] epoch 2836, training loss: 7428.32, average training loss: 6912.76, base loss: 8856.90
[INFO 2017-06-26 18:05:10,281 main.py:47] epoch 2837, training loss: 6418.43, average training loss: 6913.07, base loss: 8857.99
[INFO 2017-06-26 18:05:10,643 main.py:47] epoch 2838, training loss: 6906.92, average training loss: 6912.78, base loss: 8858.29
[INFO 2017-06-26 18:05:11,009 main.py:47] epoch 2839, training loss: 7397.33, average training loss: 6912.34, base loss: 8857.05
[INFO 2017-06-26 18:05:11,370 main.py:47] epoch 2840, training loss: 6494.98, average training loss: 6912.12, base loss: 8856.71
[INFO 2017-06-26 18:05:11,731 main.py:47] epoch 2841, training loss: 6829.28, average training loss: 6911.74, base loss: 8857.29
[INFO 2017-06-26 18:05:12,093 main.py:47] epoch 2842, training loss: 6913.47, average training loss: 6911.95, base loss: 8858.30
[INFO 2017-06-26 18:05:12,455 main.py:47] epoch 2843, training loss: 6604.12, average training loss: 6911.30, base loss: 8857.01
[INFO 2017-06-26 18:05:12,817 main.py:47] epoch 2844, training loss: 6531.90, average training loss: 6910.40, base loss: 8855.72
[INFO 2017-06-26 18:05:13,179 main.py:47] epoch 2845, training loss: 7250.40, average training loss: 6910.61, base loss: 8856.59
[INFO 2017-06-26 18:05:13,539 main.py:47] epoch 2846, training loss: 6860.91, average training loss: 6909.82, base loss: 8855.56
[INFO 2017-06-26 18:05:13,902 main.py:47] epoch 2847, training loss: 6434.52, average training loss: 6909.93, base loss: 8856.31
[INFO 2017-06-26 18:05:14,262 main.py:47] epoch 2848, training loss: 6934.57, average training loss: 6910.55, base loss: 8857.24
[INFO 2017-06-26 18:05:14,625 main.py:47] epoch 2849, training loss: 7414.64, average training loss: 6910.91, base loss: 8858.95
[INFO 2017-06-26 18:05:14,985 main.py:47] epoch 2850, training loss: 7942.21, average training loss: 6912.18, base loss: 8861.31
[INFO 2017-06-26 18:05:15,346 main.py:47] epoch 2851, training loss: 7332.19, average training loss: 6912.71, base loss: 8861.99
[INFO 2017-06-26 18:05:15,706 main.py:47] epoch 2852, training loss: 6106.78, average training loss: 6912.24, base loss: 8861.45
[INFO 2017-06-26 18:05:16,067 main.py:47] epoch 2853, training loss: 6429.29, average training loss: 6911.75, base loss: 8861.06
[INFO 2017-06-26 18:05:16,428 main.py:47] epoch 2854, training loss: 6956.39, average training loss: 6911.72, base loss: 8861.42
[INFO 2017-06-26 18:05:16,790 main.py:47] epoch 2855, training loss: 6618.06, average training loss: 6912.09, base loss: 8862.79
[INFO 2017-06-26 18:05:17,153 main.py:47] epoch 2856, training loss: 6903.23, average training loss: 6912.09, base loss: 8863.02
[INFO 2017-06-26 18:05:17,515 main.py:47] epoch 2857, training loss: 6898.01, average training loss: 6912.59, base loss: 8863.89
[INFO 2017-06-26 18:05:17,876 main.py:47] epoch 2858, training loss: 6378.72, average training loss: 6912.47, base loss: 8864.33
[INFO 2017-06-26 18:05:18,237 main.py:47] epoch 2859, training loss: 7010.26, average training loss: 6913.06, base loss: 8864.52
[INFO 2017-06-26 18:05:18,601 main.py:47] epoch 2860, training loss: 7339.02, average training loss: 6912.49, base loss: 8864.19
[INFO 2017-06-26 18:05:18,961 main.py:47] epoch 2861, training loss: 7345.09, average training loss: 6912.81, base loss: 8864.99
[INFO 2017-06-26 18:05:19,324 main.py:47] epoch 2862, training loss: 6859.80, average training loss: 6913.01, base loss: 8865.78
[INFO 2017-06-26 18:05:19,683 main.py:47] epoch 2863, training loss: 6659.38, average training loss: 6911.98, base loss: 8864.20
[INFO 2017-06-26 18:05:20,046 main.py:47] epoch 2864, training loss: 6253.90, average training loss: 6911.50, base loss: 8863.63
[INFO 2017-06-26 18:05:20,406 main.py:47] epoch 2865, training loss: 6792.52, average training loss: 6910.66, base loss: 8862.48
[INFO 2017-06-26 18:05:20,767 main.py:47] epoch 2866, training loss: 5950.43, average training loss: 6910.22, base loss: 8861.95
[INFO 2017-06-26 18:05:21,131 main.py:47] epoch 2867, training loss: 7379.47, average training loss: 6910.29, base loss: 8861.80
[INFO 2017-06-26 18:05:21,493 main.py:47] epoch 2868, training loss: 6821.86, average training loss: 6909.23, base loss: 8860.78
[INFO 2017-06-26 18:05:21,855 main.py:47] epoch 2869, training loss: 6991.49, average training loss: 6908.99, base loss: 8860.69
[INFO 2017-06-26 18:05:22,216 main.py:47] epoch 2870, training loss: 7464.63, average training loss: 6909.59, base loss: 8861.52
[INFO 2017-06-26 18:05:22,578 main.py:47] epoch 2871, training loss: 6678.42, average training loss: 6909.20, base loss: 8861.26
[INFO 2017-06-26 18:05:22,939 main.py:47] epoch 2872, training loss: 6528.91, average training loss: 6908.64, base loss: 8860.92
[INFO 2017-06-26 18:05:23,302 main.py:47] epoch 2873, training loss: 6356.15, average training loss: 6908.07, base loss: 8860.11
[INFO 2017-06-26 18:05:23,664 main.py:47] epoch 2874, training loss: 7570.71, average training loss: 6908.34, base loss: 8860.79
[INFO 2017-06-26 18:05:24,027 main.py:47] epoch 2875, training loss: 5882.56, average training loss: 6905.78, base loss: 8857.92
[INFO 2017-06-26 18:05:24,389 main.py:47] epoch 2876, training loss: 6710.79, average training loss: 6905.37, base loss: 8857.21
[INFO 2017-06-26 18:05:24,754 main.py:47] epoch 2877, training loss: 7390.00, average training loss: 6905.16, base loss: 8856.50
[INFO 2017-06-26 18:05:25,114 main.py:47] epoch 2878, training loss: 6507.67, average training loss: 6903.89, base loss: 8854.79
[INFO 2017-06-26 18:05:25,471 main.py:47] epoch 2879, training loss: 6082.47, average training loss: 6902.87, base loss: 8853.87
[INFO 2017-06-26 18:05:25,832 main.py:47] epoch 2880, training loss: 6055.49, average training loss: 6901.77, base loss: 8852.94
[INFO 2017-06-26 18:05:26,197 main.py:47] epoch 2881, training loss: 6658.75, average training loss: 6901.92, base loss: 8853.84
[INFO 2017-06-26 18:05:26,559 main.py:47] epoch 2882, training loss: 6350.21, average training loss: 6901.33, base loss: 8853.09
[INFO 2017-06-26 18:05:26,918 main.py:47] epoch 2883, training loss: 6691.87, average training loss: 6901.54, base loss: 8853.78
[INFO 2017-06-26 18:05:27,279 main.py:47] epoch 2884, training loss: 7531.18, average training loss: 6901.55, base loss: 8854.15
[INFO 2017-06-26 18:05:27,641 main.py:47] epoch 2885, training loss: 6755.91, average training loss: 6901.03, base loss: 8854.34
[INFO 2017-06-26 18:05:28,001 main.py:47] epoch 2886, training loss: 7305.81, average training loss: 6901.86, base loss: 8855.27
[INFO 2017-06-26 18:05:28,361 main.py:47] epoch 2887, training loss: 5947.84, average training loss: 6901.28, base loss: 8854.60
[INFO 2017-06-26 18:05:28,723 main.py:47] epoch 2888, training loss: 7516.32, average training loss: 6902.60, base loss: 8856.81
[INFO 2017-06-26 18:05:29,083 main.py:47] epoch 2889, training loss: 6859.62, average training loss: 6902.25, base loss: 8857.08
[INFO 2017-06-26 18:05:29,445 main.py:47] epoch 2890, training loss: 6618.53, average training loss: 6901.44, base loss: 8856.41
[INFO 2017-06-26 18:05:29,804 main.py:47] epoch 2891, training loss: 6589.68, average training loss: 6901.73, base loss: 8857.13
[INFO 2017-06-26 18:05:30,165 main.py:47] epoch 2892, training loss: 6552.92, average training loss: 6900.67, base loss: 8856.13
[INFO 2017-06-26 18:05:30,525 main.py:47] epoch 2893, training loss: 6192.11, average training loss: 6900.74, base loss: 8855.98
[INFO 2017-06-26 18:05:30,886 main.py:47] epoch 2894, training loss: 7206.87, average training loss: 6900.99, base loss: 8856.54
[INFO 2017-06-26 18:05:31,243 main.py:47] epoch 2895, training loss: 6414.58, average training loss: 6900.64, base loss: 8856.56
[INFO 2017-06-26 18:05:31,603 main.py:47] epoch 2896, training loss: 7220.63, average training loss: 6900.04, base loss: 8855.95
[INFO 2017-06-26 18:05:31,962 main.py:47] epoch 2897, training loss: 6028.21, average training loss: 6898.66, base loss: 8853.75
[INFO 2017-06-26 18:05:32,322 main.py:47] epoch 2898, training loss: 5779.77, average training loss: 6897.68, base loss: 8852.06
[INFO 2017-06-26 18:05:32,684 main.py:47] epoch 2899, training loss: 6412.53, average training loss: 6896.78, base loss: 8851.74
[INFO 2017-06-26 18:05:32,684 main.py:49] epoch 2899, testing
[INFO 2017-06-26 18:05:36,900 main.py:100] average testing loss: 6947.88, base loss: 8955.96
[INFO 2017-06-26 18:05:36,926 main.py:73] current best accuracy: 6690.65
[INFO 2017-06-26 18:05:37,285 main.py:47] epoch 2900, training loss: 6384.92, average training loss: 6896.32, base loss: 8851.58
[INFO 2017-06-26 18:05:37,647 main.py:47] epoch 2901, training loss: 7218.68, average training loss: 6895.97, base loss: 8850.90
[INFO 2017-06-26 18:05:38,008 main.py:47] epoch 2902, training loss: 5866.07, average training loss: 6894.72, base loss: 8848.35
[INFO 2017-06-26 18:05:38,369 main.py:47] epoch 2903, training loss: 7096.65, average training loss: 6894.84, base loss: 8849.27
[INFO 2017-06-26 18:05:38,730 main.py:47] epoch 2904, training loss: 7470.25, average training loss: 6893.60, base loss: 8847.85
[INFO 2017-06-26 18:05:39,091 main.py:47] epoch 2905, training loss: 6502.04, average training loss: 6893.31, base loss: 8847.51
[INFO 2017-06-26 18:05:39,451 main.py:47] epoch 2906, training loss: 7146.17, average training loss: 6893.59, base loss: 8847.31
[INFO 2017-06-26 18:05:39,813 main.py:47] epoch 2907, training loss: 6969.17, average training loss: 6893.34, base loss: 8847.24
[INFO 2017-06-26 18:05:40,174 main.py:47] epoch 2908, training loss: 5688.07, average training loss: 6892.84, base loss: 8847.40
[INFO 2017-06-26 18:05:40,535 main.py:47] epoch 2909, training loss: 6542.91, average training loss: 6892.89, base loss: 8847.78
[INFO 2017-06-26 18:05:40,897 main.py:47] epoch 2910, training loss: 6267.14, average training loss: 6891.54, base loss: 8845.68
[INFO 2017-06-26 18:05:41,259 main.py:47] epoch 2911, training loss: 6661.78, average training loss: 6891.81, base loss: 8846.87
[INFO 2017-06-26 18:05:41,620 main.py:47] epoch 2912, training loss: 6131.61, average training loss: 6891.07, base loss: 8846.14
[INFO 2017-06-26 18:05:41,981 main.py:47] epoch 2913, training loss: 6205.68, average training loss: 6890.69, base loss: 8846.08
[INFO 2017-06-26 18:05:42,342 main.py:47] epoch 2914, training loss: 8190.87, average training loss: 6891.68, base loss: 8846.77
[INFO 2017-06-26 18:05:42,704 main.py:47] epoch 2915, training loss: 7229.46, average training loss: 6892.43, base loss: 8847.71
[INFO 2017-06-26 18:05:43,063 main.py:47] epoch 2916, training loss: 7485.21, average training loss: 6892.40, base loss: 8847.22
[INFO 2017-06-26 18:05:43,424 main.py:47] epoch 2917, training loss: 7236.40, average training loss: 6893.14, base loss: 8848.26
[INFO 2017-06-26 18:05:43,786 main.py:47] epoch 2918, training loss: 7311.78, average training loss: 6893.52, base loss: 8848.91
[INFO 2017-06-26 18:05:44,147 main.py:47] epoch 2919, training loss: 6510.76, average training loss: 6893.21, base loss: 8848.83
[INFO 2017-06-26 18:05:44,509 main.py:47] epoch 2920, training loss: 6523.05, average training loss: 6892.62, base loss: 8848.47
[INFO 2017-06-26 18:05:44,872 main.py:47] epoch 2921, training loss: 7109.62, average training loss: 6892.68, base loss: 8848.59
[INFO 2017-06-26 18:05:45,236 main.py:47] epoch 2922, training loss: 6861.73, average training loss: 6892.56, base loss: 8848.62
[INFO 2017-06-26 18:05:45,600 main.py:47] epoch 2923, training loss: 7439.95, average training loss: 6892.65, base loss: 8849.10
[INFO 2017-06-26 18:05:45,964 main.py:47] epoch 2924, training loss: 6902.58, average training loss: 6892.37, base loss: 8848.44
[INFO 2017-06-26 18:05:46,327 main.py:47] epoch 2925, training loss: 6349.80, average training loss: 6891.34, base loss: 8847.22
[INFO 2017-06-26 18:05:46,685 main.py:47] epoch 2926, training loss: 6250.53, average training loss: 6890.06, base loss: 8845.63
[INFO 2017-06-26 18:05:47,047 main.py:47] epoch 2927, training loss: 5880.44, average training loss: 6889.67, base loss: 8844.47
[INFO 2017-06-26 18:05:47,411 main.py:47] epoch 2928, training loss: 6912.55, average training loss: 6889.82, base loss: 8844.89
[INFO 2017-06-26 18:05:47,778 main.py:47] epoch 2929, training loss: 6555.87, average training loss: 6889.26, base loss: 8844.19
[INFO 2017-06-26 18:05:48,140 main.py:47] epoch 2930, training loss: 6857.48, average training loss: 6889.90, base loss: 8845.71
[INFO 2017-06-26 18:05:48,503 main.py:47] epoch 2931, training loss: 7355.72, average training loss: 6890.56, base loss: 8847.23
[INFO 2017-06-26 18:05:48,866 main.py:47] epoch 2932, training loss: 6593.82, average training loss: 6890.12, base loss: 8847.08
[INFO 2017-06-26 18:05:49,225 main.py:47] epoch 2933, training loss: 7127.86, average training loss: 6890.79, base loss: 8848.32
[INFO 2017-06-26 18:05:49,586 main.py:47] epoch 2934, training loss: 7236.78, average training loss: 6890.42, base loss: 8848.30
[INFO 2017-06-26 18:05:49,948 main.py:47] epoch 2935, training loss: 6276.24, average training loss: 6888.20, base loss: 8845.29
[INFO 2017-06-26 18:05:50,308 main.py:47] epoch 2936, training loss: 6181.51, average training loss: 6888.30, base loss: 8846.28
[INFO 2017-06-26 18:05:50,671 main.py:47] epoch 2937, training loss: 7505.96, average training loss: 6889.01, base loss: 8846.68
[INFO 2017-06-26 18:05:51,033 main.py:47] epoch 2938, training loss: 8223.11, average training loss: 6890.38, base loss: 8849.04
[INFO 2017-06-26 18:05:51,396 main.py:47] epoch 2939, training loss: 6966.58, average training loss: 6891.14, base loss: 8850.40
[INFO 2017-06-26 18:05:51,754 main.py:47] epoch 2940, training loss: 7591.27, average training loss: 6891.96, base loss: 8851.91
[INFO 2017-06-26 18:05:52,164 main.py:47] epoch 2941, training loss: 7209.22, average training loss: 6893.00, base loss: 8853.64
[INFO 2017-06-26 18:05:52,550 main.py:47] epoch 2942, training loss: 6633.14, average training loss: 6893.20, base loss: 8854.34
[INFO 2017-06-26 18:05:52,940 main.py:47] epoch 2943, training loss: 7193.45, average training loss: 6893.43, base loss: 8854.59
[INFO 2017-06-26 18:05:53,330 main.py:47] epoch 2944, training loss: 6979.43, average training loss: 6892.74, base loss: 8853.57
[INFO 2017-06-26 18:05:53,694 main.py:47] epoch 2945, training loss: 6618.43, average training loss: 6892.52, base loss: 8853.18
[INFO 2017-06-26 18:05:54,083 main.py:47] epoch 2946, training loss: 8122.23, average training loss: 6893.57, base loss: 8855.03
[INFO 2017-06-26 18:05:54,482 main.py:47] epoch 2947, training loss: 6733.95, average training loss: 6892.78, base loss: 8854.18
[INFO 2017-06-26 18:05:54,876 main.py:47] epoch 2948, training loss: 6763.43, average training loss: 6891.70, base loss: 8853.18
[INFO 2017-06-26 18:05:55,243 main.py:47] epoch 2949, training loss: 6722.70, average training loss: 6890.78, base loss: 8852.06
[INFO 2017-06-26 18:05:55,614 main.py:47] epoch 2950, training loss: 7180.47, average training loss: 6891.59, base loss: 8853.32
[INFO 2017-06-26 18:05:55,980 main.py:47] epoch 2951, training loss: 6701.89, average training loss: 6891.20, base loss: 8852.74
[INFO 2017-06-26 18:05:56,344 main.py:47] epoch 2952, training loss: 7257.33, average training loss: 6891.51, base loss: 8853.86
[INFO 2017-06-26 18:05:56,707 main.py:47] epoch 2953, training loss: 8090.62, average training loss: 6893.15, base loss: 8856.77
[INFO 2017-06-26 18:05:57,070 main.py:47] epoch 2954, training loss: 6777.72, average training loss: 6893.35, base loss: 8857.05
[INFO 2017-06-26 18:05:57,432 main.py:47] epoch 2955, training loss: 6147.64, average training loss: 6893.16, base loss: 8856.94
[INFO 2017-06-26 18:05:57,794 main.py:47] epoch 2956, training loss: 6239.59, average training loss: 6892.97, base loss: 8857.10
[INFO 2017-06-26 18:05:58,154 main.py:47] epoch 2957, training loss: 5672.81, average training loss: 6891.92, base loss: 8855.44
[INFO 2017-06-26 18:05:58,516 main.py:47] epoch 2958, training loss: 6605.10, average training loss: 6890.83, base loss: 8854.19
[INFO 2017-06-26 18:05:58,876 main.py:47] epoch 2959, training loss: 6679.44, average training loss: 6888.62, base loss: 8851.52
[INFO 2017-06-26 18:05:59,234 main.py:47] epoch 2960, training loss: 6569.81, average training loss: 6888.64, base loss: 8851.66
[INFO 2017-06-26 18:05:59,597 main.py:47] epoch 2961, training loss: 6875.67, average training loss: 6887.70, base loss: 8851.14
[INFO 2017-06-26 18:05:59,959 main.py:47] epoch 2962, training loss: 6869.47, average training loss: 6887.67, base loss: 8851.42
[INFO 2017-06-26 18:06:00,326 main.py:47] epoch 2963, training loss: 6792.04, average training loss: 6887.22, base loss: 8850.86
[INFO 2017-06-26 18:06:00,688 main.py:47] epoch 2964, training loss: 6422.83, average training loss: 6887.02, base loss: 8851.14
[INFO 2017-06-26 18:06:01,049 main.py:47] epoch 2965, training loss: 7041.04, average training loss: 6886.93, base loss: 8851.87
[INFO 2017-06-26 18:06:01,409 main.py:47] epoch 2966, training loss: 6219.55, average training loss: 6886.54, base loss: 8851.06
[INFO 2017-06-26 18:06:01,770 main.py:47] epoch 2967, training loss: 6799.90, average training loss: 6885.68, base loss: 8849.81
[INFO 2017-06-26 18:06:02,130 main.py:47] epoch 2968, training loss: 7900.09, average training loss: 6886.93, base loss: 8851.63
[INFO 2017-06-26 18:06:02,492 main.py:47] epoch 2969, training loss: 6950.73, average training loss: 6886.70, base loss: 8851.22
[INFO 2017-06-26 18:06:02,849 main.py:47] epoch 2970, training loss: 7760.02, average training loss: 6887.11, base loss: 8852.06
[INFO 2017-06-26 18:06:03,242 main.py:47] epoch 2971, training loss: 5899.51, average training loss: 6886.74, base loss: 8851.78
[INFO 2017-06-26 18:06:03,623 main.py:47] epoch 2972, training loss: 7417.77, average training loss: 6887.30, base loss: 8852.75
[INFO 2017-06-26 18:06:03,991 main.py:47] epoch 2973, training loss: 6945.30, average training loss: 6887.75, base loss: 8854.18
[INFO 2017-06-26 18:06:04,355 main.py:47] epoch 2974, training loss: 6740.36, average training loss: 6887.54, base loss: 8854.57
[INFO 2017-06-26 18:06:04,719 main.py:47] epoch 2975, training loss: 8010.63, average training loss: 6887.99, base loss: 8854.33
[INFO 2017-06-26 18:06:05,115 main.py:47] epoch 2976, training loss: 7400.16, average training loss: 6888.84, base loss: 8856.04
[INFO 2017-06-26 18:06:05,512 main.py:47] epoch 2977, training loss: 6595.56, average training loss: 6887.77, base loss: 8855.11
[INFO 2017-06-26 18:06:05,889 main.py:47] epoch 2978, training loss: 6571.34, average training loss: 6887.46, base loss: 8854.10
[INFO 2017-06-26 18:06:06,253 main.py:47] epoch 2979, training loss: 7294.89, average training loss: 6888.17, base loss: 8855.30
[INFO 2017-06-26 18:06:06,615 main.py:47] epoch 2980, training loss: 6874.62, average training loss: 6888.48, base loss: 8856.18
[INFO 2017-06-26 18:06:06,976 main.py:47] epoch 2981, training loss: 6913.16, average training loss: 6889.02, base loss: 8856.97
[INFO 2017-06-26 18:06:07,367 main.py:47] epoch 2982, training loss: 6844.45, average training loss: 6889.11, base loss: 8856.66
[INFO 2017-06-26 18:06:07,731 main.py:47] epoch 2983, training loss: 6135.88, average training loss: 6887.80, base loss: 8854.88
[INFO 2017-06-26 18:06:08,098 main.py:47] epoch 2984, training loss: 6381.60, average training loss: 6887.16, base loss: 8854.11
[INFO 2017-06-26 18:06:08,461 main.py:47] epoch 2985, training loss: 7009.74, average training loss: 6887.03, base loss: 8854.37
[INFO 2017-06-26 18:06:08,820 main.py:47] epoch 2986, training loss: 6587.82, average training loss: 6886.78, base loss: 8854.08
[INFO 2017-06-26 18:06:09,178 main.py:47] epoch 2987, training loss: 6532.61, average training loss: 6886.31, base loss: 8853.40
[INFO 2017-06-26 18:06:09,539 main.py:47] epoch 2988, training loss: 6531.64, average training loss: 6886.55, base loss: 8853.72
[INFO 2017-06-26 18:06:09,898 main.py:47] epoch 2989, training loss: 7069.98, average training loss: 6886.55, base loss: 8854.59
[INFO 2017-06-26 18:06:10,260 main.py:47] epoch 2990, training loss: 7097.58, average training loss: 6886.61, base loss: 8855.77
[INFO 2017-06-26 18:06:10,621 main.py:47] epoch 2991, training loss: 7384.91, average training loss: 6886.79, base loss: 8856.62
[INFO 2017-06-26 18:06:10,983 main.py:47] epoch 2992, training loss: 7196.83, average training loss: 6886.96, base loss: 8857.28
[INFO 2017-06-26 18:06:11,343 main.py:47] epoch 2993, training loss: 5992.78, average training loss: 6885.45, base loss: 8853.85
[INFO 2017-06-26 18:06:11,705 main.py:47] epoch 2994, training loss: 6758.05, average training loss: 6885.68, base loss: 8854.44
[INFO 2017-06-26 18:06:12,069 main.py:47] epoch 2995, training loss: 6382.02, average training loss: 6885.40, base loss: 8853.60
[INFO 2017-06-26 18:06:12,429 main.py:47] epoch 2996, training loss: 6302.60, average training loss: 6884.12, base loss: 8851.95
[INFO 2017-06-26 18:06:12,790 main.py:47] epoch 2997, training loss: 7427.86, average training loss: 6884.86, base loss: 8853.61
[INFO 2017-06-26 18:06:13,150 main.py:47] epoch 2998, training loss: 6542.69, average training loss: 6884.85, base loss: 8854.18
[INFO 2017-06-26 18:06:13,510 main.py:47] epoch 2999, training loss: 7108.93, average training loss: 6885.61, base loss: 8855.73
[INFO 2017-06-26 18:06:13,510 main.py:49] epoch 2999, testing
[INFO 2017-06-26 18:06:17,693 main.py:100] average testing loss: 6817.57, base loss: 8947.82
[INFO 2017-06-26 18:06:17,718 main.py:73] current best accuracy: 6690.65
[INFO 2017-06-26 18:06:18,077 main.py:47] epoch 3000, training loss: 6598.07, average training loss: 6885.35, base loss: 8856.16
[INFO 2017-06-26 18:06:18,635 main.py:47] epoch 3001, training loss: 5986.34, average training loss: 6884.91, base loss: 8855.66
[INFO 2017-06-26 18:06:19,009 main.py:47] epoch 3002, training loss: 6705.23, average training loss: 6885.18, base loss: 8855.82
[INFO 2017-06-26 18:06:19,368 main.py:47] epoch 3003, training loss: 6725.59, average training loss: 6885.31, base loss: 8855.91
[INFO 2017-06-26 18:06:19,728 main.py:47] epoch 3004, training loss: 7327.54, average training loss: 6885.68, base loss: 8856.60
[INFO 2017-06-26 18:06:20,087 main.py:47] epoch 3005, training loss: 7046.16, average training loss: 6885.58, base loss: 8857.17
[INFO 2017-06-26 18:06:20,446 main.py:47] epoch 3006, training loss: 7275.43, average training loss: 6886.67, base loss: 8858.74
[INFO 2017-06-26 18:06:20,805 main.py:47] epoch 3007, training loss: 7625.62, average training loss: 6886.62, base loss: 8858.48
[INFO 2017-06-26 18:06:21,164 main.py:47] epoch 3008, training loss: 6111.73, average training loss: 6886.03, base loss: 8857.64
[INFO 2017-06-26 18:06:21,524 main.py:47] epoch 3009, training loss: 6066.85, average training loss: 6884.83, base loss: 8856.65
[INFO 2017-06-26 18:06:21,884 main.py:47] epoch 3010, training loss: 7714.79, average training loss: 6885.58, base loss: 8857.55
[INFO 2017-06-26 18:06:22,243 main.py:47] epoch 3011, training loss: 6745.70, average training loss: 6885.15, base loss: 8856.79
[INFO 2017-06-26 18:06:22,601 main.py:47] epoch 3012, training loss: 6228.42, average training loss: 6884.63, base loss: 8855.92
[INFO 2017-06-26 18:06:22,961 main.py:47] epoch 3013, training loss: 6801.74, average training loss: 6884.88, base loss: 8856.23
[INFO 2017-06-26 18:06:23,318 main.py:47] epoch 3014, training loss: 6936.98, average training loss: 6884.76, base loss: 8855.89
[INFO 2017-06-26 18:06:23,676 main.py:47] epoch 3015, training loss: 6729.43, average training loss: 6885.26, base loss: 8856.03
[INFO 2017-06-26 18:06:24,036 main.py:47] epoch 3016, training loss: 7211.44, average training loss: 6885.87, base loss: 8857.25
[INFO 2017-06-26 18:06:24,395 main.py:47] epoch 3017, training loss: 7301.60, average training loss: 6885.22, base loss: 8855.42
[INFO 2017-06-26 18:06:24,758 main.py:47] epoch 3018, training loss: 6797.53, average training loss: 6885.53, base loss: 8855.99
[INFO 2017-06-26 18:06:25,118 main.py:47] epoch 3019, training loss: 6365.61, average training loss: 6884.82, base loss: 8854.42
[INFO 2017-06-26 18:06:25,477 main.py:47] epoch 3020, training loss: 7087.54, average training loss: 6885.58, base loss: 8855.39
[INFO 2017-06-26 18:06:25,839 main.py:47] epoch 3021, training loss: 6554.19, average training loss: 6885.72, base loss: 8856.28
[INFO 2017-06-26 18:06:26,202 main.py:47] epoch 3022, training loss: 6321.64, average training loss: 6885.25, base loss: 8855.41
[INFO 2017-06-26 18:06:26,563 main.py:47] epoch 3023, training loss: 7374.97, average training loss: 6885.57, base loss: 8855.54
[INFO 2017-06-26 18:06:26,925 main.py:47] epoch 3024, training loss: 6088.33, average training loss: 6885.28, base loss: 8854.59
[INFO 2017-06-26 18:06:27,288 main.py:47] epoch 3025, training loss: 6878.82, average training loss: 6884.73, base loss: 8853.63
[INFO 2017-06-26 18:06:27,654 main.py:47] epoch 3026, training loss: 6480.21, average training loss: 6884.49, base loss: 8853.01
[INFO 2017-06-26 18:06:28,015 main.py:47] epoch 3027, training loss: 7058.00, average training loss: 6883.60, base loss: 8852.49
[INFO 2017-06-26 18:06:28,410 main.py:47] epoch 3028, training loss: 7591.04, average training loss: 6884.59, base loss: 8854.42
[INFO 2017-06-26 18:06:28,774 main.py:47] epoch 3029, training loss: 6627.03, average training loss: 6883.94, base loss: 8854.42
[INFO 2017-06-26 18:06:29,152 main.py:47] epoch 3030, training loss: 6907.29, average training loss: 6883.64, base loss: 8854.02
[INFO 2017-06-26 18:06:29,513 main.py:47] epoch 3031, training loss: 7680.41, average training loss: 6884.64, base loss: 8856.53
[INFO 2017-06-26 18:06:29,873 main.py:47] epoch 3032, training loss: 7810.21, average training loss: 6885.97, base loss: 8858.54
[INFO 2017-06-26 18:06:30,233 main.py:47] epoch 3033, training loss: 6415.49, average training loss: 6884.88, base loss: 8857.92
[INFO 2017-06-26 18:06:30,630 main.py:47] epoch 3034, training loss: 6907.12, average training loss: 6883.73, base loss: 8855.76
[INFO 2017-06-26 18:06:30,995 main.py:47] epoch 3035, training loss: 5913.05, average training loss: 6882.82, base loss: 8854.67
[INFO 2017-06-26 18:06:31,357 main.py:47] epoch 3036, training loss: 8042.05, average training loss: 6884.24, base loss: 8856.10
[INFO 2017-06-26 18:06:31,720 main.py:47] epoch 3037, training loss: 6437.39, average training loss: 6883.65, base loss: 8855.18
[INFO 2017-06-26 18:06:32,080 main.py:47] epoch 3038, training loss: 7188.98, average training loss: 6883.61, base loss: 8855.99
[INFO 2017-06-26 18:06:32,443 main.py:47] epoch 3039, training loss: 6420.48, average training loss: 6883.27, base loss: 8855.23
[INFO 2017-06-26 18:06:32,803 main.py:47] epoch 3040, training loss: 6789.77, average training loss: 6883.16, base loss: 8855.17
[INFO 2017-06-26 18:06:33,164 main.py:47] epoch 3041, training loss: 7798.93, average training loss: 6884.66, base loss: 8858.09
[INFO 2017-06-26 18:06:33,524 main.py:47] epoch 3042, training loss: 7347.95, average training loss: 6885.64, base loss: 8859.75
[INFO 2017-06-26 18:06:33,885 main.py:47] epoch 3043, training loss: 6727.50, average training loss: 6884.36, base loss: 8858.20
[INFO 2017-06-26 18:06:34,245 main.py:47] epoch 3044, training loss: 7417.55, average training loss: 6884.56, base loss: 8859.14
[INFO 2017-06-26 18:06:34,607 main.py:47] epoch 3045, training loss: 6461.05, average training loss: 6884.80, base loss: 8859.46
[INFO 2017-06-26 18:06:34,968 main.py:47] epoch 3046, training loss: 6713.49, average training loss: 6884.59, base loss: 8859.35
[INFO 2017-06-26 18:06:35,329 main.py:47] epoch 3047, training loss: 7359.18, average training loss: 6884.21, base loss: 8858.97
[INFO 2017-06-26 18:06:35,690 main.py:47] epoch 3048, training loss: 6904.73, average training loss: 6884.16, base loss: 8859.03
[INFO 2017-06-26 18:06:36,052 main.py:47] epoch 3049, training loss: 6575.92, average training loss: 6884.16, base loss: 8858.48
[INFO 2017-06-26 18:06:36,424 main.py:47] epoch 3050, training loss: 6321.87, average training loss: 6882.73, base loss: 8856.23
[INFO 2017-06-26 18:06:36,784 main.py:47] epoch 3051, training loss: 7562.27, average training loss: 6883.01, base loss: 8857.10
[INFO 2017-06-26 18:06:37,144 main.py:47] epoch 3052, training loss: 6100.55, average training loss: 6882.14, base loss: 8855.70
[INFO 2017-06-26 18:06:37,503 main.py:47] epoch 3053, training loss: 6665.07, average training loss: 6881.70, base loss: 8855.64
[INFO 2017-06-26 18:06:37,864 main.py:47] epoch 3054, training loss: 6247.18, average training loss: 6880.97, base loss: 8854.71
[INFO 2017-06-26 18:06:38,224 main.py:47] epoch 3055, training loss: 5913.08, average training loss: 6880.47, base loss: 8854.37
[INFO 2017-06-26 18:06:38,584 main.py:47] epoch 3056, training loss: 7650.59, average training loss: 6881.07, base loss: 8855.22
[INFO 2017-06-26 18:06:38,945 main.py:47] epoch 3057, training loss: 6882.47, average training loss: 6881.15, base loss: 8855.54
[INFO 2017-06-26 18:06:39,306 main.py:47] epoch 3058, training loss: 7855.38, average training loss: 6882.74, base loss: 8857.82
[INFO 2017-06-26 18:06:39,666 main.py:47] epoch 3059, training loss: 6892.46, average training loss: 6882.53, base loss: 8857.77
[INFO 2017-06-26 18:06:40,026 main.py:47] epoch 3060, training loss: 6917.75, average training loss: 6882.61, base loss: 8858.06
[INFO 2017-06-26 18:06:40,387 main.py:47] epoch 3061, training loss: 8254.60, average training loss: 6884.09, base loss: 8859.75
[INFO 2017-06-26 18:06:40,747 main.py:47] epoch 3062, training loss: 6484.83, average training loss: 6883.87, base loss: 8859.76
[INFO 2017-06-26 18:06:41,110 main.py:47] epoch 3063, training loss: 7448.74, average training loss: 6885.14, base loss: 8861.90
[INFO 2017-06-26 18:06:41,470 main.py:47] epoch 3064, training loss: 6211.98, average training loss: 6884.32, base loss: 8860.23
[INFO 2017-06-26 18:06:41,832 main.py:47] epoch 3065, training loss: 6743.88, average training loss: 6884.14, base loss: 8860.06
[INFO 2017-06-26 18:06:42,193 main.py:47] epoch 3066, training loss: 6767.42, average training loss: 6883.90, base loss: 8859.35
[INFO 2017-06-26 18:06:42,554 main.py:47] epoch 3067, training loss: 6498.88, average training loss: 6883.93, base loss: 8859.00
[INFO 2017-06-26 18:06:42,914 main.py:47] epoch 3068, training loss: 7711.04, average training loss: 6884.07, base loss: 8858.84
[INFO 2017-06-26 18:06:43,275 main.py:47] epoch 3069, training loss: 6954.30, average training loss: 6884.39, base loss: 8859.10
[INFO 2017-06-26 18:06:43,637 main.py:47] epoch 3070, training loss: 7003.80, average training loss: 6884.26, base loss: 8858.66
[INFO 2017-06-26 18:06:43,994 main.py:47] epoch 3071, training loss: 6394.01, average training loss: 6883.31, base loss: 8857.38
[INFO 2017-06-26 18:06:44,356 main.py:47] epoch 3072, training loss: 6663.60, average training loss: 6883.46, base loss: 8857.08
[INFO 2017-06-26 18:06:44,716 main.py:47] epoch 3073, training loss: 7245.39, average training loss: 6884.01, base loss: 8857.87
[INFO 2017-06-26 18:06:45,077 main.py:47] epoch 3074, training loss: 7252.83, average training loss: 6884.52, base loss: 8858.89
[INFO 2017-06-26 18:06:45,437 main.py:47] epoch 3075, training loss: 7065.24, average training loss: 6885.54, base loss: 8860.53
[INFO 2017-06-26 18:06:45,797 main.py:47] epoch 3076, training loss: 5992.93, average training loss: 6885.20, base loss: 8860.17
[INFO 2017-06-26 18:06:46,156 main.py:47] epoch 3077, training loss: 7419.20, average training loss: 6885.83, base loss: 8861.07
[INFO 2017-06-26 18:06:46,517 main.py:47] epoch 3078, training loss: 7470.53, average training loss: 6886.02, base loss: 8861.52
[INFO 2017-06-26 18:06:46,877 main.py:47] epoch 3079, training loss: 6249.37, average training loss: 6885.16, base loss: 8860.72
[INFO 2017-06-26 18:06:47,237 main.py:47] epoch 3080, training loss: 7129.46, average training loss: 6885.52, base loss: 8860.90
[INFO 2017-06-26 18:06:47,598 main.py:47] epoch 3081, training loss: 6845.21, average training loss: 6884.43, base loss: 8859.57
[INFO 2017-06-26 18:06:47,957 main.py:47] epoch 3082, training loss: 6780.39, average training loss: 6884.66, base loss: 8859.83
[INFO 2017-06-26 18:06:48,318 main.py:47] epoch 3083, training loss: 6738.28, average training loss: 6883.31, base loss: 8857.90
[INFO 2017-06-26 18:06:48,679 main.py:47] epoch 3084, training loss: 6601.26, average training loss: 6882.81, base loss: 8856.75
[INFO 2017-06-26 18:06:49,042 main.py:47] epoch 3085, training loss: 6779.45, average training loss: 6882.23, base loss: 8856.45
[INFO 2017-06-26 18:06:49,402 main.py:47] epoch 3086, training loss: 6412.77, average training loss: 6882.26, base loss: 8857.26
[INFO 2017-06-26 18:06:49,764 main.py:47] epoch 3087, training loss: 6425.55, average training loss: 6881.48, base loss: 8856.64
[INFO 2017-06-26 18:06:50,124 main.py:47] epoch 3088, training loss: 6790.69, average training loss: 6881.20, base loss: 8856.59
[INFO 2017-06-26 18:06:50,485 main.py:47] epoch 3089, training loss: 6240.49, average training loss: 6880.63, base loss: 8856.69
[INFO 2017-06-26 18:06:50,844 main.py:47] epoch 3090, training loss: 8003.92, average training loss: 6881.90, base loss: 8858.92
[INFO 2017-06-26 18:06:51,206 main.py:47] epoch 3091, training loss: 7403.51, average training loss: 6882.79, base loss: 8859.57
[INFO 2017-06-26 18:06:51,568 main.py:47] epoch 3092, training loss: 6344.31, average training loss: 6882.44, base loss: 8859.98
[INFO 2017-06-26 18:06:51,929 main.py:47] epoch 3093, training loss: 6469.15, average training loss: 6882.57, base loss: 8860.24
[INFO 2017-06-26 18:06:52,291 main.py:47] epoch 3094, training loss: 6844.18, average training loss: 6882.78, base loss: 8860.97
[INFO 2017-06-26 18:06:52,651 main.py:47] epoch 3095, training loss: 6946.53, average training loss: 6883.03, base loss: 8861.78
[INFO 2017-06-26 18:06:53,012 main.py:47] epoch 3096, training loss: 6575.23, average training loss: 6882.20, base loss: 8860.74
[INFO 2017-06-26 18:06:53,373 main.py:47] epoch 3097, training loss: 6302.21, average training loss: 6880.98, base loss: 8858.46
[INFO 2017-06-26 18:06:53,735 main.py:47] epoch 3098, training loss: 7390.94, average training loss: 6880.48, base loss: 8857.56
[INFO 2017-06-26 18:06:54,097 main.py:47] epoch 3099, training loss: 6511.86, average training loss: 6880.05, base loss: 8856.51
[INFO 2017-06-26 18:06:54,097 main.py:49] epoch 3099, testing
[INFO 2017-06-26 18:06:58,348 main.py:100] average testing loss: 6619.67, base loss: 8548.20
[INFO 2017-06-26 18:06:58,374 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 18:06:58,380 main.py:73] current best accuracy: 6619.67
[INFO 2017-06-26 18:06:58,743 main.py:47] epoch 3100, training loss: 6172.63, average training loss: 6878.92, base loss: 8854.70
[INFO 2017-06-26 18:06:59,108 main.py:47] epoch 3101, training loss: 6137.92, average training loss: 6878.14, base loss: 8853.33
[INFO 2017-06-26 18:06:59,470 main.py:47] epoch 3102, training loss: 7258.06, average training loss: 6878.19, base loss: 8853.28
[INFO 2017-06-26 18:06:59,830 main.py:47] epoch 3103, training loss: 6577.43, average training loss: 6877.19, base loss: 8851.37
[INFO 2017-06-26 18:07:00,190 main.py:47] epoch 3104, training loss: 6769.91, average training loss: 6877.22, base loss: 8851.24
[INFO 2017-06-26 18:07:00,549 main.py:47] epoch 3105, training loss: 6270.44, average training loss: 6876.16, base loss: 8849.86
[INFO 2017-06-26 18:07:00,910 main.py:47] epoch 3106, training loss: 6539.14, average training loss: 6875.62, base loss: 8848.93
[INFO 2017-06-26 18:07:01,269 main.py:47] epoch 3107, training loss: 7937.34, average training loss: 6876.59, base loss: 8850.94
[INFO 2017-06-26 18:07:01,628 main.py:47] epoch 3108, training loss: 6832.28, average training loss: 6877.36, base loss: 8851.99
[INFO 2017-06-26 18:07:01,987 main.py:47] epoch 3109, training loss: 6474.45, average training loss: 6876.67, base loss: 8851.60
[INFO 2017-06-26 18:07:02,343 main.py:47] epoch 3110, training loss: 6853.71, average training loss: 6876.60, base loss: 8852.08
[INFO 2017-06-26 18:07:02,702 main.py:47] epoch 3111, training loss: 6609.25, average training loss: 6875.91, base loss: 8851.09
[INFO 2017-06-26 18:07:03,061 main.py:47] epoch 3112, training loss: 7281.87, average training loss: 6876.71, base loss: 8853.03
[INFO 2017-06-26 18:07:03,421 main.py:47] epoch 3113, training loss: 7337.21, average training loss: 6877.28, base loss: 8853.80
[INFO 2017-06-26 18:07:03,780 main.py:47] epoch 3114, training loss: 6074.99, average training loss: 6877.08, base loss: 8853.83
[INFO 2017-06-26 18:07:04,138 main.py:47] epoch 3115, training loss: 5867.58, average training loss: 6875.86, base loss: 8852.29
[INFO 2017-06-26 18:07:04,498 main.py:47] epoch 3116, training loss: 7106.98, average training loss: 6876.60, base loss: 8853.10
[INFO 2017-06-26 18:07:04,860 main.py:47] epoch 3117, training loss: 6949.85, average training loss: 6875.79, base loss: 8851.84
[INFO 2017-06-26 18:07:05,219 main.py:47] epoch 3118, training loss: 7119.33, average training loss: 6876.01, base loss: 8852.56
[INFO 2017-06-26 18:07:05,579 main.py:47] epoch 3119, training loss: 7767.10, average training loss: 6877.32, base loss: 8854.34
[INFO 2017-06-26 18:07:05,969 main.py:47] epoch 3120, training loss: 6990.01, average training loss: 6877.15, base loss: 8853.88
[INFO 2017-06-26 18:07:06,386 main.py:47] epoch 3121, training loss: 6768.88, average training loss: 6877.09, base loss: 8854.09
[INFO 2017-06-26 18:07:06,750 main.py:47] epoch 3122, training loss: 7072.07, average training loss: 6877.41, base loss: 8854.67
[INFO 2017-06-26 18:07:07,112 main.py:47] epoch 3123, training loss: 6986.29, average training loss: 6877.75, base loss: 8854.92
[INFO 2017-06-26 18:07:07,471 main.py:47] epoch 3124, training loss: 7031.08, average training loss: 6877.87, base loss: 8854.25
[INFO 2017-06-26 18:07:07,831 main.py:47] epoch 3125, training loss: 6525.23, average training loss: 6877.04, base loss: 8853.16
[INFO 2017-06-26 18:07:08,191 main.py:47] epoch 3126, training loss: 6939.57, average training loss: 6876.83, base loss: 8853.40
[INFO 2017-06-26 18:07:08,560 main.py:47] epoch 3127, training loss: 6080.69, average training loss: 6876.01, base loss: 8853.21
[INFO 2017-06-26 18:07:08,921 main.py:47] epoch 3128, training loss: 8231.92, average training loss: 6878.16, base loss: 8856.25
[INFO 2017-06-26 18:07:09,281 main.py:47] epoch 3129, training loss: 6196.19, average training loss: 6877.89, base loss: 8855.63
[INFO 2017-06-26 18:07:09,642 main.py:47] epoch 3130, training loss: 7256.36, average training loss: 6877.43, base loss: 8855.43
[INFO 2017-06-26 18:07:09,999 main.py:47] epoch 3131, training loss: 5824.95, average training loss: 6875.27, base loss: 8851.56
[INFO 2017-06-26 18:07:10,412 main.py:47] epoch 3132, training loss: 6941.17, average training loss: 6875.93, base loss: 8852.75
[INFO 2017-06-26 18:07:10,772 main.py:47] epoch 3133, training loss: 6485.08, average training loss: 6875.76, base loss: 8852.69
[INFO 2017-06-26 18:07:11,140 main.py:47] epoch 3134, training loss: 7167.72, average training loss: 6876.35, base loss: 8853.43
[INFO 2017-06-26 18:07:11,502 main.py:47] epoch 3135, training loss: 6244.18, average training loss: 6875.94, base loss: 8852.70
[INFO 2017-06-26 18:07:11,863 main.py:47] epoch 3136, training loss: 7044.99, average training loss: 6876.64, base loss: 8853.99
[INFO 2017-06-26 18:07:12,226 main.py:47] epoch 3137, training loss: 6811.06, average training loss: 6876.67, base loss: 8854.30
[INFO 2017-06-26 18:07:12,587 main.py:47] epoch 3138, training loss: 7461.01, average training loss: 6876.70, base loss: 8854.32
[INFO 2017-06-26 18:07:12,948 main.py:47] epoch 3139, training loss: 7389.13, average training loss: 6876.79, base loss: 8854.50
[INFO 2017-06-26 18:07:13,344 main.py:47] epoch 3140, training loss: 7746.43, average training loss: 6877.97, base loss: 8856.91
[INFO 2017-06-26 18:07:13,705 main.py:47] epoch 3141, training loss: 6852.19, average training loss: 6877.47, base loss: 8857.00
[INFO 2017-06-26 18:07:14,068 main.py:47] epoch 3142, training loss: 6355.58, average training loss: 6876.71, base loss: 8856.55
[INFO 2017-06-26 18:07:14,433 main.py:47] epoch 3143, training loss: 6471.33, average training loss: 6875.94, base loss: 8855.70
[INFO 2017-06-26 18:07:14,791 main.py:47] epoch 3144, training loss: 6697.31, average training loss: 6876.25, base loss: 8856.69
[INFO 2017-06-26 18:07:15,152 main.py:47] epoch 3145, training loss: 6824.02, average training loss: 6875.90, base loss: 8856.93
[INFO 2017-06-26 18:07:15,535 main.py:47] epoch 3146, training loss: 7966.12, average training loss: 6876.48, base loss: 8857.90
[INFO 2017-06-26 18:07:15,896 main.py:47] epoch 3147, training loss: 7515.09, average training loss: 6877.79, base loss: 8861.00
[INFO 2017-06-26 18:07:16,257 main.py:47] epoch 3148, training loss: 6884.44, average training loss: 6877.55, base loss: 8861.13
[INFO 2017-06-26 18:07:16,619 main.py:47] epoch 3149, training loss: 6323.92, average training loss: 6875.52, base loss: 8858.21
[INFO 2017-06-26 18:07:16,980 main.py:47] epoch 3150, training loss: 7240.03, average training loss: 6876.47, base loss: 8859.31
[INFO 2017-06-26 18:07:17,343 main.py:47] epoch 3151, training loss: 6518.37, average training loss: 6876.67, base loss: 8860.21
[INFO 2017-06-26 18:07:17,700 main.py:47] epoch 3152, training loss: 6741.58, average training loss: 6876.68, base loss: 8860.94
[INFO 2017-06-26 18:07:18,061 main.py:47] epoch 3153, training loss: 6708.64, average training loss: 6876.83, base loss: 8861.74
[INFO 2017-06-26 18:07:18,421 main.py:47] epoch 3154, training loss: 6317.74, average training loss: 6875.75, base loss: 8860.82
[INFO 2017-06-26 18:07:18,783 main.py:47] epoch 3155, training loss: 7289.64, average training loss: 6875.31, base loss: 8861.04
[INFO 2017-06-26 18:07:19,143 main.py:47] epoch 3156, training loss: 7624.87, average training loss: 6876.08, base loss: 8862.15
[INFO 2017-06-26 18:07:19,504 main.py:47] epoch 3157, training loss: 6558.88, average training loss: 6875.98, base loss: 8861.97
[INFO 2017-06-26 18:07:19,866 main.py:47] epoch 3158, training loss: 6590.17, average training loss: 6876.41, base loss: 8862.88
[INFO 2017-06-26 18:07:20,243 main.py:47] epoch 3159, training loss: 6538.07, average training loss: 6875.94, base loss: 8862.94
[INFO 2017-06-26 18:07:20,627 main.py:47] epoch 3160, training loss: 7275.58, average training loss: 6876.03, base loss: 8863.52
[INFO 2017-06-26 18:07:21,014 main.py:47] epoch 3161, training loss: 7077.24, average training loss: 6875.74, base loss: 8863.01
[INFO 2017-06-26 18:07:21,404 main.py:47] epoch 3162, training loss: 6090.41, average training loss: 6875.55, base loss: 8862.97
[INFO 2017-06-26 18:07:21,785 main.py:47] epoch 3163, training loss: 6792.59, average training loss: 6876.20, base loss: 8864.44
[INFO 2017-06-26 18:07:22,166 main.py:47] epoch 3164, training loss: 6496.42, average training loss: 6876.05, base loss: 8864.51
[INFO 2017-06-26 18:07:22,580 main.py:47] epoch 3165, training loss: 6511.96, average training loss: 6875.48, base loss: 8863.92
[INFO 2017-06-26 18:07:22,947 main.py:47] epoch 3166, training loss: 6977.78, average training loss: 6875.70, base loss: 8864.07
[INFO 2017-06-26 18:07:23,312 main.py:47] epoch 3167, training loss: 7762.01, average training loss: 6877.14, base loss: 8866.22
[INFO 2017-06-26 18:07:23,671 main.py:47] epoch 3168, training loss: 6127.19, average training loss: 6874.84, base loss: 8863.45
[INFO 2017-06-26 18:07:24,031 main.py:47] epoch 3169, training loss: 6339.77, average training loss: 6872.81, base loss: 8861.32
[INFO 2017-06-26 18:07:24,389 main.py:47] epoch 3170, training loss: 7161.55, average training loss: 6871.66, base loss: 8860.56
[INFO 2017-06-26 18:07:24,749 main.py:47] epoch 3171, training loss: 7330.15, average training loss: 6871.87, base loss: 8860.72
[INFO 2017-06-26 18:07:25,107 main.py:47] epoch 3172, training loss: 7257.09, average training loss: 6872.50, base loss: 8861.54
[INFO 2017-06-26 18:07:25,467 main.py:47] epoch 3173, training loss: 6826.68, average training loss: 6872.87, base loss: 8862.77
[INFO 2017-06-26 18:07:25,826 main.py:47] epoch 3174, training loss: 7091.25, average training loss: 6873.24, base loss: 8863.71
[INFO 2017-06-26 18:07:26,184 main.py:47] epoch 3175, training loss: 6344.47, average training loss: 6871.44, base loss: 8861.21
[INFO 2017-06-26 18:07:26,542 main.py:47] epoch 3176, training loss: 6501.65, average training loss: 6871.03, base loss: 8860.80
[INFO 2017-06-26 18:07:26,901 main.py:47] epoch 3177, training loss: 7466.39, average training loss: 6871.26, base loss: 8861.44
[INFO 2017-06-26 18:07:27,259 main.py:47] epoch 3178, training loss: 6378.86, average training loss: 6869.73, base loss: 8859.25
[INFO 2017-06-26 18:07:27,620 main.py:47] epoch 3179, training loss: 6401.08, average training loss: 6869.61, base loss: 8859.50
[INFO 2017-06-26 18:07:27,979 main.py:47] epoch 3180, training loss: 6827.28, average training loss: 6869.44, base loss: 8858.39
[INFO 2017-06-26 18:07:28,340 main.py:47] epoch 3181, training loss: 7253.91, average training loss: 6869.64, base loss: 8858.75
[INFO 2017-06-26 18:07:28,699 main.py:47] epoch 3182, training loss: 6912.20, average training loss: 6869.13, base loss: 8858.32
[INFO 2017-06-26 18:07:29,058 main.py:47] epoch 3183, training loss: 6533.13, average training loss: 6868.12, base loss: 8856.18
[INFO 2017-06-26 18:07:29,420 main.py:47] epoch 3184, training loss: 6857.45, average training loss: 6867.14, base loss: 8855.92
[INFO 2017-06-26 18:07:29,779 main.py:47] epoch 3185, training loss: 6514.41, average training loss: 6865.88, base loss: 8853.33
[INFO 2017-06-26 18:07:30,140 main.py:47] epoch 3186, training loss: 6847.73, average training loss: 6866.64, base loss: 8855.31
[INFO 2017-06-26 18:07:30,501 main.py:47] epoch 3187, training loss: 7159.86, average training loss: 6866.88, base loss: 8856.24
[INFO 2017-06-26 18:07:30,860 main.py:47] epoch 3188, training loss: 6274.25, average training loss: 6866.94, base loss: 8856.55
[INFO 2017-06-26 18:07:31,220 main.py:47] epoch 3189, training loss: 6324.95, average training loss: 6866.62, base loss: 8856.32
[INFO 2017-06-26 18:07:31,580 main.py:47] epoch 3190, training loss: 7971.31, average training loss: 6867.61, base loss: 8857.84
[INFO 2017-06-26 18:07:31,940 main.py:47] epoch 3191, training loss: 7564.38, average training loss: 6868.52, base loss: 8858.65
[INFO 2017-06-26 18:07:32,302 main.py:47] epoch 3192, training loss: 6452.10, average training loss: 6868.34, base loss: 8858.76
[INFO 2017-06-26 18:07:32,662 main.py:47] epoch 3193, training loss: 6525.10, average training loss: 6868.40, base loss: 8858.90
[INFO 2017-06-26 18:07:33,022 main.py:47] epoch 3194, training loss: 6721.97, average training loss: 6868.59, base loss: 8859.41
[INFO 2017-06-26 18:07:33,379 main.py:47] epoch 3195, training loss: 6276.87, average training loss: 6867.85, base loss: 8858.63
[INFO 2017-06-26 18:07:33,736 main.py:47] epoch 3196, training loss: 6600.13, average training loss: 6867.96, base loss: 8858.24
[INFO 2017-06-26 18:07:34,096 main.py:47] epoch 3197, training loss: 6844.67, average training loss: 6868.06, base loss: 8858.62
[INFO 2017-06-26 18:07:34,458 main.py:47] epoch 3198, training loss: 5880.63, average training loss: 6867.20, base loss: 8857.28
[INFO 2017-06-26 18:07:34,817 main.py:47] epoch 3199, training loss: 6812.84, average training loss: 6867.01, base loss: 8857.63
[INFO 2017-06-26 18:07:34,817 main.py:49] epoch 3199, testing
[INFO 2017-06-26 18:07:38,988 main.py:100] average testing loss: 6794.02, base loss: 8874.07
[INFO 2017-06-26 18:07:39,013 main.py:73] current best accuracy: 6619.67
[INFO 2017-06-26 18:07:39,373 main.py:47] epoch 3200, training loss: 6040.75, average training loss: 6864.88, base loss: 8855.00
[INFO 2017-06-26 18:07:39,731 main.py:47] epoch 3201, training loss: 6186.54, average training loss: 6862.44, base loss: 8852.59
[INFO 2017-06-26 18:07:40,091 main.py:47] epoch 3202, training loss: 6374.28, average training loss: 6861.97, base loss: 8852.27
[INFO 2017-06-26 18:07:40,451 main.py:47] epoch 3203, training loss: 6770.80, average training loss: 6861.52, base loss: 8851.73
[INFO 2017-06-26 18:07:40,812 main.py:47] epoch 3204, training loss: 6813.45, average training loss: 6861.80, base loss: 8852.52
[INFO 2017-06-26 18:07:41,172 main.py:47] epoch 3205, training loss: 7761.77, average training loss: 6862.60, base loss: 8853.49
[INFO 2017-06-26 18:07:41,532 main.py:47] epoch 3206, training loss: 6479.00, average training loss: 6862.46, base loss: 8853.68
[INFO 2017-06-26 18:07:41,891 main.py:47] epoch 3207, training loss: 7226.81, average training loss: 6861.98, base loss: 8852.78
[INFO 2017-06-26 18:07:42,251 main.py:47] epoch 3208, training loss: 6984.70, average training loss: 6861.92, base loss: 8853.24
[INFO 2017-06-26 18:07:42,611 main.py:47] epoch 3209, training loss: 6439.09, average training loss: 6862.08, base loss: 8853.48
[INFO 2017-06-26 18:07:42,972 main.py:47] epoch 3210, training loss: 7048.51, average training loss: 6862.62, base loss: 8854.44
[INFO 2017-06-26 18:07:43,333 main.py:47] epoch 3211, training loss: 7621.10, average training loss: 6862.15, base loss: 8854.66
[INFO 2017-06-26 18:07:43,693 main.py:47] epoch 3212, training loss: 6917.64, average training loss: 6861.93, base loss: 8853.81
[INFO 2017-06-26 18:07:44,050 main.py:47] epoch 3213, training loss: 7084.05, average training loss: 6861.63, base loss: 8853.58
[INFO 2017-06-26 18:07:44,410 main.py:47] epoch 3214, training loss: 7259.37, average training loss: 6861.49, base loss: 8853.89
[INFO 2017-06-26 18:07:44,769 main.py:47] epoch 3215, training loss: 7673.83, average training loss: 6861.82, base loss: 8854.05
[INFO 2017-06-26 18:07:45,128 main.py:47] epoch 3216, training loss: 6993.33, average training loss: 6861.60, base loss: 8853.80
[INFO 2017-06-26 18:07:45,489 main.py:47] epoch 3217, training loss: 6309.49, average training loss: 6860.86, base loss: 8852.64
[INFO 2017-06-26 18:07:45,850 main.py:47] epoch 3218, training loss: 6890.96, average training loss: 6861.42, base loss: 8853.20
[INFO 2017-06-26 18:07:46,210 main.py:47] epoch 3219, training loss: 7668.25, average training loss: 6862.74, base loss: 8854.89
[INFO 2017-06-26 18:07:46,570 main.py:47] epoch 3220, training loss: 6408.42, average training loss: 6862.28, base loss: 8854.87
[INFO 2017-06-26 18:07:46,931 main.py:47] epoch 3221, training loss: 6567.36, average training loss: 6861.30, base loss: 8852.85
[INFO 2017-06-26 18:07:47,289 main.py:47] epoch 3222, training loss: 6130.97, average training loss: 6860.79, base loss: 8851.98
[INFO 2017-06-26 18:07:47,650 main.py:47] epoch 3223, training loss: 6429.32, average training loss: 6860.88, base loss: 8851.96
[INFO 2017-06-26 18:07:48,009 main.py:47] epoch 3224, training loss: 6524.26, average training loss: 6860.90, base loss: 8852.39
[INFO 2017-06-26 18:07:48,370 main.py:47] epoch 3225, training loss: 7022.37, average training loss: 6861.79, base loss: 8854.21
[INFO 2017-06-26 18:07:48,731 main.py:47] epoch 3226, training loss: 6655.42, average training loss: 6861.37, base loss: 8854.51
[INFO 2017-06-26 18:07:49,088 main.py:47] epoch 3227, training loss: 6938.24, average training loss: 6861.53, base loss: 8855.33
[INFO 2017-06-26 18:07:49,450 main.py:47] epoch 3228, training loss: 6758.90, average training loss: 6861.52, base loss: 8854.82
[INFO 2017-06-26 18:07:49,809 main.py:47] epoch 3229, training loss: 6205.50, average training loss: 6861.15, base loss: 8854.71
[INFO 2017-06-26 18:07:50,169 main.py:47] epoch 3230, training loss: 6422.12, average training loss: 6860.10, base loss: 8852.45
[INFO 2017-06-26 18:07:50,528 main.py:47] epoch 3231, training loss: 6832.85, average training loss: 6859.16, base loss: 8851.18
[INFO 2017-06-26 18:07:50,887 main.py:47] epoch 3232, training loss: 7357.74, average training loss: 6859.01, base loss: 8851.32
[INFO 2017-06-26 18:07:51,247 main.py:47] epoch 3233, training loss: 5888.13, average training loss: 6858.21, base loss: 8849.47
[INFO 2017-06-26 18:07:51,608 main.py:47] epoch 3234, training loss: 7088.86, average training loss: 6858.60, base loss: 8849.76
[INFO 2017-06-26 18:07:51,969 main.py:47] epoch 3235, training loss: 7032.35, average training loss: 6858.38, base loss: 8849.89
[INFO 2017-06-26 18:07:52,331 main.py:47] epoch 3236, training loss: 7037.89, average training loss: 6858.43, base loss: 8850.47
[INFO 2017-06-26 18:07:52,692 main.py:47] epoch 3237, training loss: 7832.01, average training loss: 6859.77, base loss: 8852.11
[INFO 2017-06-26 18:07:53,052 main.py:47] epoch 3238, training loss: 6399.92, average training loss: 6859.46, base loss: 8852.59
[INFO 2017-06-26 18:07:53,449 main.py:47] epoch 3239, training loss: 6140.05, average training loss: 6859.14, base loss: 8852.13
[INFO 2017-06-26 18:07:53,819 main.py:47] epoch 3240, training loss: 6876.13, average training loss: 6860.14, base loss: 8854.20
[INFO 2017-06-26 18:07:54,190 main.py:47] epoch 3241, training loss: 7694.90, average training loss: 6861.42, base loss: 8856.17
[INFO 2017-06-26 18:07:54,552 main.py:47] epoch 3242, training loss: 6130.97, average training loss: 6860.10, base loss: 8854.61
[INFO 2017-06-26 18:07:54,945 main.py:47] epoch 3243, training loss: 6389.49, average training loss: 6859.06, base loss: 8853.42
[INFO 2017-06-26 18:07:55,343 main.py:47] epoch 3244, training loss: 6514.94, average training loss: 6857.63, base loss: 8851.47
[INFO 2017-06-26 18:07:55,706 main.py:47] epoch 3245, training loss: 6748.64, average training loss: 6856.94, base loss: 8850.58
[INFO 2017-06-26 18:07:56,069 main.py:47] epoch 3246, training loss: 6895.34, average training loss: 6857.05, base loss: 8851.64
[INFO 2017-06-26 18:07:56,428 main.py:47] epoch 3247, training loss: 6090.06, average training loss: 6856.00, base loss: 8851.02
[INFO 2017-06-26 18:07:56,824 main.py:47] epoch 3248, training loss: 6724.29, average training loss: 6855.46, base loss: 8850.38
[INFO 2017-06-26 18:07:57,190 main.py:47] epoch 3249, training loss: 6128.89, average training loss: 6854.49, base loss: 8848.76
[INFO 2017-06-26 18:07:57,551 main.py:47] epoch 3250, training loss: 6903.24, average training loss: 6853.38, base loss: 8847.05
[INFO 2017-06-26 18:07:57,914 main.py:47] epoch 3251, training loss: 8137.20, average training loss: 6854.84, base loss: 8849.23
[INFO 2017-06-26 18:07:58,310 main.py:47] epoch 3252, training loss: 7452.08, average training loss: 6855.38, base loss: 8850.27
[INFO 2017-06-26 18:07:58,686 main.py:47] epoch 3253, training loss: 7462.40, average training loss: 6855.88, base loss: 8850.14
[INFO 2017-06-26 18:07:59,094 main.py:47] epoch 3254, training loss: 6051.87, average training loss: 6854.71, base loss: 8848.93
[INFO 2017-06-26 18:07:59,474 main.py:47] epoch 3255, training loss: 6677.24, average training loss: 6855.34, base loss: 8850.38
[INFO 2017-06-26 18:07:59,835 main.py:47] epoch 3256, training loss: 7697.91, average training loss: 6856.11, base loss: 8851.64
[INFO 2017-06-26 18:08:00,196 main.py:47] epoch 3257, training loss: 7098.67, average training loss: 6857.07, base loss: 8852.81
[INFO 2017-06-26 18:08:00,556 main.py:47] epoch 3258, training loss: 6077.60, average training loss: 6856.43, base loss: 8851.86
[INFO 2017-06-26 18:08:00,919 main.py:47] epoch 3259, training loss: 5745.70, average training loss: 6856.18, base loss: 8851.12
[INFO 2017-06-26 18:08:01,280 main.py:47] epoch 3260, training loss: 6373.68, average training loss: 6855.18, base loss: 8849.88
[INFO 2017-06-26 18:08:01,640 main.py:47] epoch 3261, training loss: 6979.69, average training loss: 6855.70, base loss: 8850.80
[INFO 2017-06-26 18:08:02,001 main.py:47] epoch 3262, training loss: 6165.29, average training loss: 6854.63, base loss: 8849.07
[INFO 2017-06-26 18:08:02,362 main.py:47] epoch 3263, training loss: 6941.88, average training loss: 6854.78, base loss: 8849.49
[INFO 2017-06-26 18:08:02,723 main.py:47] epoch 3264, training loss: 5630.72, average training loss: 6853.24, base loss: 8846.97
[INFO 2017-06-26 18:08:03,084 main.py:47] epoch 3265, training loss: 7128.39, average training loss: 6852.84, base loss: 8846.58
[INFO 2017-06-26 18:08:03,445 main.py:47] epoch 3266, training loss: 6039.72, average training loss: 6851.25, base loss: 8843.88
[INFO 2017-06-26 18:08:03,815 main.py:47] epoch 3267, training loss: 7476.28, average training loss: 6852.50, base loss: 8846.51
[INFO 2017-06-26 18:08:04,175 main.py:47] epoch 3268, training loss: 7204.16, average training loss: 6852.40, base loss: 8847.03
[INFO 2017-06-26 18:08:04,537 main.py:47] epoch 3269, training loss: 6798.36, average training loss: 6852.28, base loss: 8846.98
[INFO 2017-06-26 18:08:04,899 main.py:47] epoch 3270, training loss: 6357.83, average training loss: 6851.13, base loss: 8844.85
[INFO 2017-06-26 18:08:05,262 main.py:47] epoch 3271, training loss: 6488.02, average training loss: 6849.89, base loss: 8843.48
[INFO 2017-06-26 18:08:05,624 main.py:47] epoch 3272, training loss: 7337.86, average training loss: 6851.02, base loss: 8845.19
[INFO 2017-06-26 18:08:05,986 main.py:47] epoch 3273, training loss: 6710.89, average training loss: 6850.91, base loss: 8845.51
[INFO 2017-06-26 18:08:06,347 main.py:47] epoch 3274, training loss: 8256.35, average training loss: 6852.41, base loss: 8848.11
[INFO 2017-06-26 18:08:06,708 main.py:47] epoch 3275, training loss: 7001.41, average training loss: 6851.19, base loss: 8846.14
[INFO 2017-06-26 18:08:07,073 main.py:47] epoch 3276, training loss: 5884.22, average training loss: 6850.89, base loss: 8845.62
[INFO 2017-06-26 18:08:07,433 main.py:47] epoch 3277, training loss: 6994.69, average training loss: 6850.75, base loss: 8845.86
[INFO 2017-06-26 18:08:07,794 main.py:47] epoch 3278, training loss: 6301.59, average training loss: 6850.90, base loss: 8846.19
[INFO 2017-06-26 18:08:08,155 main.py:47] epoch 3279, training loss: 5864.93, average training loss: 6849.91, base loss: 8844.52
[INFO 2017-06-26 18:08:08,515 main.py:47] epoch 3280, training loss: 7854.21, average training loss: 6851.14, base loss: 8846.23
[INFO 2017-06-26 18:08:08,875 main.py:47] epoch 3281, training loss: 7047.07, average training loss: 6851.67, base loss: 8847.46
[INFO 2017-06-26 18:08:09,239 main.py:47] epoch 3282, training loss: 7139.73, average training loss: 6851.51, base loss: 8846.69
[INFO 2017-06-26 18:08:09,600 main.py:47] epoch 3283, training loss: 6448.72, average training loss: 6851.15, base loss: 8846.25
[INFO 2017-06-26 18:08:09,962 main.py:47] epoch 3284, training loss: 7402.70, average training loss: 6851.00, base loss: 8846.26
[INFO 2017-06-26 18:08:10,322 main.py:47] epoch 3285, training loss: 6099.65, average training loss: 6850.57, base loss: 8845.19
[INFO 2017-06-26 18:08:10,684 main.py:47] epoch 3286, training loss: 6318.68, average training loss: 6849.64, base loss: 8843.27
[INFO 2017-06-26 18:08:11,046 main.py:47] epoch 3287, training loss: 6310.80, average training loss: 6848.68, base loss: 8841.41
[INFO 2017-06-26 18:08:11,439 main.py:47] epoch 3288, training loss: 6965.46, average training loss: 6848.52, base loss: 8841.35
[INFO 2017-06-26 18:08:11,823 main.py:47] epoch 3289, training loss: 7069.97, average training loss: 6848.05, base loss: 8840.68
[INFO 2017-06-26 18:08:12,187 main.py:47] epoch 3290, training loss: 6908.50, average training loss: 6847.97, base loss: 8841.14
[INFO 2017-06-26 18:08:12,550 main.py:47] epoch 3291, training loss: 6804.57, average training loss: 6848.30, base loss: 8840.73
[INFO 2017-06-26 18:08:12,913 main.py:47] epoch 3292, training loss: 6226.46, average training loss: 6847.41, base loss: 8839.11
[INFO 2017-06-26 18:08:13,275 main.py:47] epoch 3293, training loss: 6644.14, average training loss: 6847.85, base loss: 8840.12
[INFO 2017-06-26 18:08:13,635 main.py:47] epoch 3294, training loss: 7129.18, average training loss: 6848.43, base loss: 8840.83
[INFO 2017-06-26 18:08:13,995 main.py:47] epoch 3295, training loss: 7798.01, average training loss: 6849.88, base loss: 8842.91
[INFO 2017-06-26 18:08:14,355 main.py:47] epoch 3296, training loss: 6712.30, average training loss: 6849.40, base loss: 8842.53
[INFO 2017-06-26 18:08:14,717 main.py:47] epoch 3297, training loss: 8258.05, average training loss: 6851.32, base loss: 8845.93
[INFO 2017-06-26 18:08:15,079 main.py:47] epoch 3298, training loss: 6843.37, average training loss: 6851.44, base loss: 8846.51
[INFO 2017-06-26 18:08:15,438 main.py:47] epoch 3299, training loss: 6298.72, average training loss: 6851.01, base loss: 8845.79
[INFO 2017-06-26 18:08:15,439 main.py:49] epoch 3299, testing
[INFO 2017-06-26 18:08:19,583 main.py:100] average testing loss: 6662.29, base loss: 8544.18
[INFO 2017-06-26 18:08:19,609 main.py:73] current best accuracy: 6619.67
[INFO 2017-06-26 18:08:19,970 main.py:47] epoch 3300, training loss: 6947.47, average training loss: 6851.79, base loss: 8847.57
[INFO 2017-06-26 18:08:20,330 main.py:47] epoch 3301, training loss: 6462.31, average training loss: 6850.90, base loss: 8845.98
[INFO 2017-06-26 18:08:20,690 main.py:47] epoch 3302, training loss: 6947.23, average training loss: 6850.28, base loss: 8845.81
[INFO 2017-06-26 18:08:21,051 main.py:47] epoch 3303, training loss: 6089.03, average training loss: 6848.97, base loss: 8843.25
[INFO 2017-06-26 18:08:21,412 main.py:47] epoch 3304, training loss: 6553.67, average training loss: 6848.80, base loss: 8843.10
[INFO 2017-06-26 18:08:21,771 main.py:47] epoch 3305, training loss: 6749.08, average training loss: 6847.88, base loss: 8842.05
[INFO 2017-06-26 18:08:22,131 main.py:47] epoch 3306, training loss: 7066.24, average training loss: 6847.86, base loss: 8841.74
[INFO 2017-06-26 18:08:22,489 main.py:47] epoch 3307, training loss: 6939.27, average training loss: 6847.05, base loss: 8841.46
[INFO 2017-06-26 18:08:22,850 main.py:47] epoch 3308, training loss: 7357.70, average training loss: 6847.21, base loss: 8841.38
[INFO 2017-06-26 18:08:23,211 main.py:47] epoch 3309, training loss: 8319.91, average training loss: 6848.20, base loss: 8843.11
[INFO 2017-06-26 18:08:23,570 main.py:47] epoch 3310, training loss: 7272.65, average training loss: 6848.55, base loss: 8843.72
[INFO 2017-06-26 18:08:23,930 main.py:47] epoch 3311, training loss: 6960.08, average training loss: 6848.85, base loss: 8844.33
[INFO 2017-06-26 18:08:24,289 main.py:47] epoch 3312, training loss: 6317.26, average training loss: 6848.50, base loss: 8844.35
[INFO 2017-06-26 18:08:24,651 main.py:47] epoch 3313, training loss: 6602.87, average training loss: 6847.54, base loss: 8843.30
[INFO 2017-06-26 18:08:25,012 main.py:47] epoch 3314, training loss: 6156.56, average training loss: 6847.99, base loss: 8843.80
[INFO 2017-06-26 18:08:25,370 main.py:47] epoch 3315, training loss: 6933.54, average training loss: 6848.59, base loss: 8844.64
[INFO 2017-06-26 18:08:25,730 main.py:47] epoch 3316, training loss: 7173.03, average training loss: 6848.50, base loss: 8844.32
[INFO 2017-06-26 18:08:26,092 main.py:47] epoch 3317, training loss: 7741.19, average training loss: 6849.61, base loss: 8846.41
[INFO 2017-06-26 18:08:26,452 main.py:47] epoch 3318, training loss: 7417.74, average training loss: 6849.74, base loss: 8845.97
[INFO 2017-06-26 18:08:26,812 main.py:47] epoch 3319, training loss: 7151.53, average training loss: 6849.49, base loss: 8846.14
[INFO 2017-06-26 18:08:27,171 main.py:47] epoch 3320, training loss: 6778.81, average training loss: 6848.65, base loss: 8844.37
[INFO 2017-06-26 18:08:27,531 main.py:47] epoch 3321, training loss: 7552.59, average training loss: 6848.57, base loss: 8844.28
[INFO 2017-06-26 18:08:27,892 main.py:47] epoch 3322, training loss: 6578.92, average training loss: 6848.03, base loss: 8843.74
[INFO 2017-06-26 18:08:28,252 main.py:47] epoch 3323, training loss: 7036.76, average training loss: 6848.20, base loss: 8844.57
[INFO 2017-06-26 18:08:28,611 main.py:47] epoch 3324, training loss: 6519.21, average training loss: 6848.70, base loss: 8845.63
[INFO 2017-06-26 18:08:28,971 main.py:47] epoch 3325, training loss: 6859.01, average training loss: 6848.62, base loss: 8846.75
[INFO 2017-06-26 18:08:29,331 main.py:47] epoch 3326, training loss: 7149.99, average training loss: 6849.30, base loss: 8848.43
[INFO 2017-06-26 18:08:29,692 main.py:47] epoch 3327, training loss: 7834.77, average training loss: 6851.05, base loss: 8851.43
[INFO 2017-06-26 18:08:30,053 main.py:47] epoch 3328, training loss: 7451.26, average training loss: 6851.85, base loss: 8853.19
[INFO 2017-06-26 18:08:30,412 main.py:47] epoch 3329, training loss: 7628.70, average training loss: 6852.75, base loss: 8854.74
[INFO 2017-06-26 18:08:30,792 main.py:47] epoch 3330, training loss: 7022.60, average training loss: 6852.67, base loss: 8855.69
[INFO 2017-06-26 18:08:31,151 main.py:47] epoch 3331, training loss: 6299.72, average training loss: 6852.29, base loss: 8854.14
[INFO 2017-06-26 18:08:31,513 main.py:47] epoch 3332, training loss: 6954.33, average training loss: 6852.27, base loss: 8854.74
[INFO 2017-06-26 18:08:31,873 main.py:47] epoch 3333, training loss: 6881.10, average training loss: 6852.30, base loss: 8854.83
[INFO 2017-06-26 18:08:32,234 main.py:47] epoch 3334, training loss: 6779.57, average training loss: 6852.53, base loss: 8854.66
[INFO 2017-06-26 18:08:32,594 main.py:47] epoch 3335, training loss: 6415.95, average training loss: 6851.44, base loss: 8853.53
[INFO 2017-06-26 18:08:32,954 main.py:47] epoch 3336, training loss: 6833.21, average training loss: 6851.28, base loss: 8853.33
[INFO 2017-06-26 18:08:33,317 main.py:47] epoch 3337, training loss: 7025.86, average training loss: 6851.84, base loss: 8853.97
[INFO 2017-06-26 18:08:33,674 main.py:47] epoch 3338, training loss: 7543.30, average training loss: 6852.76, base loss: 8855.44
[INFO 2017-06-26 18:08:34,034 main.py:47] epoch 3339, training loss: 7811.83, average training loss: 6853.86, base loss: 8856.59
[INFO 2017-06-26 18:08:34,395 main.py:47] epoch 3340, training loss: 6933.16, average training loss: 6854.24, base loss: 8857.31
[INFO 2017-06-26 18:08:34,753 main.py:47] epoch 3341, training loss: 6279.10, average training loss: 6853.54, base loss: 8855.69
[INFO 2017-06-26 18:08:35,115 main.py:47] epoch 3342, training loss: 6453.55, average training loss: 6852.54, base loss: 8854.28
[INFO 2017-06-26 18:08:35,476 main.py:47] epoch 3343, training loss: 6711.30, average training loss: 6852.98, base loss: 8854.40
[INFO 2017-06-26 18:08:35,837 main.py:47] epoch 3344, training loss: 7460.39, average training loss: 6853.66, base loss: 8854.59
[INFO 2017-06-26 18:08:36,196 main.py:47] epoch 3345, training loss: 6970.31, average training loss: 6854.10, base loss: 8855.34
[INFO 2017-06-26 18:08:36,557 main.py:47] epoch 3346, training loss: 6148.89, average training loss: 6853.06, base loss: 8853.63
[INFO 2017-06-26 18:08:36,918 main.py:47] epoch 3347, training loss: 6878.25, average training loss: 6853.51, base loss: 8853.76
[INFO 2017-06-26 18:08:37,279 main.py:47] epoch 3348, training loss: 6964.62, average training loss: 6852.57, base loss: 8852.67
[INFO 2017-06-26 18:08:37,640 main.py:47] epoch 3349, training loss: 7952.35, average training loss: 6853.89, base loss: 8854.90
[INFO 2017-06-26 18:08:38,001 main.py:47] epoch 3350, training loss: 7124.95, average training loss: 6854.30, base loss: 8855.61
[INFO 2017-06-26 18:08:38,364 main.py:47] epoch 3351, training loss: 7441.01, average training loss: 6855.14, base loss: 8856.66
[INFO 2017-06-26 18:08:38,736 main.py:47] epoch 3352, training loss: 7799.51, average training loss: 6856.07, base loss: 8858.45
[INFO 2017-06-26 18:08:39,098 main.py:47] epoch 3353, training loss: 6290.14, average training loss: 6855.39, base loss: 8857.56
[INFO 2017-06-26 18:08:39,458 main.py:47] epoch 3354, training loss: 6556.58, average training loss: 6854.17, base loss: 8857.02
[INFO 2017-06-26 18:08:39,818 main.py:47] epoch 3355, training loss: 6830.00, average training loss: 6854.18, base loss: 8857.58
[INFO 2017-06-26 18:08:40,211 main.py:47] epoch 3356, training loss: 7856.33, average training loss: 6855.56, base loss: 8859.58
[INFO 2017-06-26 18:08:40,573 main.py:47] epoch 3357, training loss: 6300.66, average training loss: 6855.84, base loss: 8859.62
[INFO 2017-06-26 18:08:40,940 main.py:47] epoch 3358, training loss: 6903.53, average training loss: 6855.66, base loss: 8860.37
[INFO 2017-06-26 18:08:41,344 main.py:47] epoch 3359, training loss: 7214.88, average training loss: 6856.15, base loss: 8861.36
[INFO 2017-06-26 18:08:41,718 main.py:47] epoch 3360, training loss: 6898.11, average training loss: 6856.51, base loss: 8862.25
[INFO 2017-06-26 18:08:42,112 main.py:47] epoch 3361, training loss: 7034.15, average training loss: 6855.78, base loss: 8861.60
[INFO 2017-06-26 18:08:42,531 main.py:47] epoch 3362, training loss: 7215.08, average training loss: 6855.89, base loss: 8862.44
[INFO 2017-06-26 18:08:42,893 main.py:47] epoch 3363, training loss: 6961.79, average training loss: 6855.93, base loss: 8862.73
[INFO 2017-06-26 18:08:43,256 main.py:47] epoch 3364, training loss: 5945.88, average training loss: 6854.46, base loss: 8860.43
[INFO 2017-06-26 18:08:43,618 main.py:47] epoch 3365, training loss: 7210.94, average training loss: 6855.13, base loss: 8861.37
[INFO 2017-06-26 18:08:43,979 main.py:47] epoch 3366, training loss: 6154.43, average training loss: 6854.68, base loss: 8860.35
[INFO 2017-06-26 18:08:44,341 main.py:47] epoch 3367, training loss: 7252.19, average training loss: 6854.76, base loss: 8860.52
[INFO 2017-06-26 18:08:44,701 main.py:47] epoch 3368, training loss: 6289.91, average training loss: 6853.67, base loss: 8859.44
[INFO 2017-06-26 18:08:45,069 main.py:47] epoch 3369, training loss: 6263.85, average training loss: 6853.38, base loss: 8859.08
[INFO 2017-06-26 18:08:45,443 main.py:47] epoch 3370, training loss: 6570.99, average training loss: 6852.67, base loss: 8857.46
[INFO 2017-06-26 18:08:45,810 main.py:47] epoch 3371, training loss: 7426.47, average training loss: 6853.16, base loss: 8858.23
[INFO 2017-06-26 18:08:46,172 main.py:47] epoch 3372, training loss: 6464.28, average training loss: 6851.77, base loss: 8855.99
[INFO 2017-06-26 18:08:46,533 main.py:47] epoch 3373, training loss: 6129.50, average training loss: 6851.79, base loss: 8856.06
[INFO 2017-06-26 18:08:46,893 main.py:47] epoch 3374, training loss: 6057.33, average training loss: 6851.32, base loss: 8855.20
[INFO 2017-06-26 18:08:47,253 main.py:47] epoch 3375, training loss: 6547.84, average training loss: 6851.70, base loss: 8855.85
[INFO 2017-06-26 18:08:47,613 main.py:47] epoch 3376, training loss: 6372.69, average training loss: 6851.54, base loss: 8855.86
[INFO 2017-06-26 18:08:47,972 main.py:47] epoch 3377, training loss: 6183.33, average training loss: 6850.51, base loss: 8854.29
[INFO 2017-06-26 18:08:48,330 main.py:47] epoch 3378, training loss: 5856.88, average training loss: 6850.11, base loss: 8853.78
[INFO 2017-06-26 18:08:48,691 main.py:47] epoch 3379, training loss: 6235.55, average training loss: 6849.07, base loss: 8852.45
[INFO 2017-06-26 18:08:49,052 main.py:47] epoch 3380, training loss: 6598.57, average training loss: 6848.77, base loss: 8852.15
[INFO 2017-06-26 18:08:49,412 main.py:47] epoch 3381, training loss: 6335.02, average training loss: 6848.60, base loss: 8852.17
[INFO 2017-06-26 18:08:49,774 main.py:47] epoch 3382, training loss: 7169.32, average training loss: 6849.20, base loss: 8853.54
[INFO 2017-06-26 18:08:50,133 main.py:47] epoch 3383, training loss: 6359.09, average training loss: 6848.05, base loss: 8852.50
[INFO 2017-06-26 18:08:50,493 main.py:47] epoch 3384, training loss: 7410.46, average training loss: 6849.18, base loss: 8854.62
[INFO 2017-06-26 18:08:50,852 main.py:47] epoch 3385, training loss: 6630.92, average training loss: 6848.96, base loss: 8855.00
[INFO 2017-06-26 18:08:51,213 main.py:47] epoch 3386, training loss: 5949.85, average training loss: 6848.26, base loss: 8853.75
[INFO 2017-06-26 18:08:51,574 main.py:47] epoch 3387, training loss: 6606.76, average training loss: 6847.79, base loss: 8853.14
[INFO 2017-06-26 18:08:51,933 main.py:47] epoch 3388, training loss: 7451.85, average training loss: 6847.70, base loss: 8853.06
[INFO 2017-06-26 18:08:52,292 main.py:47] epoch 3389, training loss: 7881.85, average training loss: 6848.61, base loss: 8854.85
[INFO 2017-06-26 18:08:52,651 main.py:47] epoch 3390, training loss: 6870.98, average training loss: 6848.24, base loss: 8854.64
[INFO 2017-06-26 18:08:53,009 main.py:47] epoch 3391, training loss: 7009.83, average training loss: 6847.53, base loss: 8854.47
[INFO 2017-06-26 18:08:53,367 main.py:47] epoch 3392, training loss: 6012.53, average training loss: 6846.62, base loss: 8852.29
[INFO 2017-06-26 18:08:53,726 main.py:47] epoch 3393, training loss: 5904.40, average training loss: 6845.36, base loss: 8850.10
[INFO 2017-06-26 18:08:54,087 main.py:47] epoch 3394, training loss: 6850.09, average training loss: 6845.71, base loss: 8851.14
[INFO 2017-06-26 18:08:54,447 main.py:47] epoch 3395, training loss: 6967.34, average training loss: 6846.49, base loss: 8852.51
[INFO 2017-06-26 18:08:54,804 main.py:47] epoch 3396, training loss: 6721.11, average training loss: 6846.83, base loss: 8853.72
[INFO 2017-06-26 18:08:55,165 main.py:47] epoch 3397, training loss: 6809.26, average training loss: 6847.02, base loss: 8854.36
[INFO 2017-06-26 18:08:55,522 main.py:47] epoch 3398, training loss: 6552.00, average training loss: 6847.03, base loss: 8853.81
[INFO 2017-06-26 18:08:55,882 main.py:47] epoch 3399, training loss: 6021.86, average training loss: 6846.10, base loss: 8852.42
[INFO 2017-06-26 18:08:55,883 main.py:49] epoch 3399, testing
[INFO 2017-06-26 18:09:00,121 main.py:100] average testing loss: 6760.81, base loss: 8720.48
[INFO 2017-06-26 18:09:00,145 main.py:73] current best accuracy: 6619.67
[INFO 2017-06-26 18:09:00,504 main.py:47] epoch 3400, training loss: 6925.73, average training loss: 6846.41, base loss: 8853.06
[INFO 2017-06-26 18:09:00,863 main.py:47] epoch 3401, training loss: 7218.23, average training loss: 6846.44, base loss: 8853.55
[INFO 2017-06-26 18:09:01,222 main.py:47] epoch 3402, training loss: 7032.51, average training loss: 6847.13, base loss: 8855.15
[INFO 2017-06-26 18:09:01,583 main.py:47] epoch 3403, training loss: 6624.29, average training loss: 6847.29, base loss: 8855.19
[INFO 2017-06-26 18:09:01,941 main.py:47] epoch 3404, training loss: 6336.22, average training loss: 6846.81, base loss: 8854.57
[INFO 2017-06-26 18:09:02,302 main.py:47] epoch 3405, training loss: 6338.17, average training loss: 6846.46, base loss: 8853.79
[INFO 2017-06-26 18:09:02,663 main.py:47] epoch 3406, training loss: 6837.15, average training loss: 6846.74, base loss: 8854.01
[INFO 2017-06-26 18:09:03,021 main.py:47] epoch 3407, training loss: 6184.84, average training loss: 6846.04, base loss: 8853.82
[INFO 2017-06-26 18:09:03,379 main.py:47] epoch 3408, training loss: 6766.26, average training loss: 6846.65, base loss: 8855.47
[INFO 2017-06-26 18:09:03,737 main.py:47] epoch 3409, training loss: 6568.23, average training loss: 6846.95, base loss: 8856.22
[INFO 2017-06-26 18:09:04,096 main.py:47] epoch 3410, training loss: 7275.88, average training loss: 6847.42, base loss: 8856.90
[INFO 2017-06-26 18:09:04,455 main.py:47] epoch 3411, training loss: 7152.04, average training loss: 6847.21, base loss: 8857.83
[INFO 2017-06-26 18:09:04,815 main.py:47] epoch 3412, training loss: 7152.93, average training loss: 6847.45, base loss: 8858.27
[INFO 2017-06-26 18:09:05,173 main.py:47] epoch 3413, training loss: 6519.39, average training loss: 6847.66, base loss: 8859.39
[INFO 2017-06-26 18:09:05,534 main.py:47] epoch 3414, training loss: 6732.88, average training loss: 6847.70, base loss: 8858.67
[INFO 2017-06-26 18:09:05,893 main.py:47] epoch 3415, training loss: 7334.56, average training loss: 6848.14, base loss: 8859.33
[INFO 2017-06-26 18:09:06,250 main.py:47] epoch 3416, training loss: 6519.31, average training loss: 6846.74, base loss: 8856.93
[INFO 2017-06-26 18:09:06,610 main.py:47] epoch 3417, training loss: 6195.20, average training loss: 6845.84, base loss: 8855.79
[INFO 2017-06-26 18:09:06,968 main.py:47] epoch 3418, training loss: 7332.55, average training loss: 6846.43, base loss: 8856.87
[INFO 2017-06-26 18:09:07,326 main.py:47] epoch 3419, training loss: 7169.17, average training loss: 6846.05, base loss: 8856.46
[INFO 2017-06-26 18:09:07,683 main.py:47] epoch 3420, training loss: 6652.98, average training loss: 6844.88, base loss: 8855.63
[INFO 2017-06-26 18:09:08,042 main.py:47] epoch 3421, training loss: 5814.71, average training loss: 6843.91, base loss: 8855.38
[INFO 2017-06-26 18:09:08,400 main.py:47] epoch 3422, training loss: 7796.00, average training loss: 6845.54, base loss: 8857.56
[INFO 2017-06-26 18:09:08,760 main.py:47] epoch 3423, training loss: 6961.03, average training loss: 6845.92, base loss: 8858.55
[INFO 2017-06-26 18:09:09,120 main.py:47] epoch 3424, training loss: 7604.38, average training loss: 6847.10, base loss: 8859.98
[INFO 2017-06-26 18:09:09,475 main.py:47] epoch 3425, training loss: 6598.39, average training loss: 6846.43, base loss: 8859.67
[INFO 2017-06-26 18:09:09,834 main.py:47] epoch 3426, training loss: 6207.26, average training loss: 6845.71, base loss: 8858.81
[INFO 2017-06-26 18:09:10,193 main.py:47] epoch 3427, training loss: 6006.73, average training loss: 6844.38, base loss: 8856.97
[INFO 2017-06-26 18:09:10,550 main.py:47] epoch 3428, training loss: 7275.24, average training loss: 6843.98, base loss: 8857.21
[INFO 2017-06-26 18:09:10,910 main.py:47] epoch 3429, training loss: 7109.12, average training loss: 6844.75, base loss: 8859.14
[INFO 2017-06-26 18:09:11,269 main.py:47] epoch 3430, training loss: 6492.34, average training loss: 6844.46, base loss: 8858.77
[INFO 2017-06-26 18:09:11,631 main.py:47] epoch 3431, training loss: 6640.82, average training loss: 6843.06, base loss: 8857.40
[INFO 2017-06-26 18:09:11,990 main.py:47] epoch 3432, training loss: 6346.56, average training loss: 6842.06, base loss: 8856.32
[INFO 2017-06-26 18:09:12,349 main.py:47] epoch 3433, training loss: 6629.34, average training loss: 6841.57, base loss: 8855.79
[INFO 2017-06-26 18:09:12,708 main.py:47] epoch 3434, training loss: 7425.52, average training loss: 6842.11, base loss: 8857.31
[INFO 2017-06-26 18:09:13,068 main.py:47] epoch 3435, training loss: 5982.33, average training loss: 6840.13, base loss: 8855.64
[INFO 2017-06-26 18:09:13,429 main.py:47] epoch 3436, training loss: 6837.04, average training loss: 6840.27, base loss: 8856.17
[INFO 2017-06-26 18:09:13,796 main.py:47] epoch 3437, training loss: 7467.34, average training loss: 6840.16, base loss: 8855.53
[INFO 2017-06-26 18:09:14,157 main.py:47] epoch 3438, training loss: 6798.23, average training loss: 6840.16, base loss: 8855.17
[INFO 2017-06-26 18:09:14,518 main.py:47] epoch 3439, training loss: 6314.68, average training loss: 6838.62, base loss: 8852.63
[INFO 2017-06-26 18:09:14,878 main.py:47] epoch 3440, training loss: 6693.89, average training loss: 6837.77, base loss: 8851.70
[INFO 2017-06-26 18:09:15,239 main.py:47] epoch 3441, training loss: 6585.88, average training loss: 6837.07, base loss: 8850.73
[INFO 2017-06-26 18:09:15,599 main.py:47] epoch 3442, training loss: 7101.85, average training loss: 6837.38, base loss: 8851.36
[INFO 2017-06-26 18:09:15,960 main.py:47] epoch 3443, training loss: 7610.05, average training loss: 6838.55, base loss: 8853.29
[INFO 2017-06-26 18:09:16,321 main.py:47] epoch 3444, training loss: 6470.32, average training loss: 6838.06, base loss: 8853.23
[INFO 2017-06-26 18:09:16,680 main.py:47] epoch 3445, training loss: 6552.68, average training loss: 6837.03, base loss: 8851.85
[INFO 2017-06-26 18:09:17,042 main.py:47] epoch 3446, training loss: 6704.74, average training loss: 6836.67, base loss: 8851.71
[INFO 2017-06-26 18:09:17,403 main.py:47] epoch 3447, training loss: 6468.27, average training loss: 6836.51, base loss: 8851.39
[INFO 2017-06-26 18:09:17,765 main.py:47] epoch 3448, training loss: 6272.36, average training loss: 6836.22, base loss: 8851.48
[INFO 2017-06-26 18:09:18,127 main.py:47] epoch 3449, training loss: 6835.62, average training loss: 6836.67, base loss: 8852.38
[INFO 2017-06-26 18:09:18,490 main.py:47] epoch 3450, training loss: 7238.54, average training loss: 6836.96, base loss: 8852.87
[INFO 2017-06-26 18:09:18,851 main.py:47] epoch 3451, training loss: 6758.48, average training loss: 6835.76, base loss: 8851.50
[INFO 2017-06-26 18:09:19,215 main.py:47] epoch 3452, training loss: 6476.55, average training loss: 6835.62, base loss: 8851.49
[INFO 2017-06-26 18:09:19,577 main.py:47] epoch 3453, training loss: 7350.43, average training loss: 6836.55, base loss: 8853.41
[INFO 2017-06-26 18:09:19,941 main.py:47] epoch 3454, training loss: 6558.56, average training loss: 6837.21, base loss: 8854.99
[INFO 2017-06-26 18:09:20,304 main.py:47] epoch 3455, training loss: 6463.34, average training loss: 6836.46, base loss: 8853.98
[INFO 2017-06-26 18:09:20,666 main.py:47] epoch 3456, training loss: 6324.25, average training loss: 6835.72, base loss: 8852.29
[INFO 2017-06-26 18:09:21,027 main.py:47] epoch 3457, training loss: 7434.02, average training loss: 6836.50, base loss: 8852.88
[INFO 2017-06-26 18:09:21,386 main.py:47] epoch 3458, training loss: 6777.97, average training loss: 6835.76, base loss: 8851.80
[INFO 2017-06-26 18:09:21,744 main.py:47] epoch 3459, training loss: 6727.32, average training loss: 6835.38, base loss: 8851.34
[INFO 2017-06-26 18:09:22,103 main.py:47] epoch 3460, training loss: 6569.25, average training loss: 6833.81, base loss: 8848.86
[INFO 2017-06-26 18:09:22,463 main.py:47] epoch 3461, training loss: 6966.45, average training loss: 6834.40, base loss: 8850.86
[INFO 2017-06-26 18:09:22,823 main.py:47] epoch 3462, training loss: 7154.68, average training loss: 6835.15, base loss: 8852.05
[INFO 2017-06-26 18:09:23,184 main.py:47] epoch 3463, training loss: 5988.96, average training loss: 6833.57, base loss: 8849.62
[INFO 2017-06-26 18:09:23,544 main.py:47] epoch 3464, training loss: 6648.36, average training loss: 6833.51, base loss: 8850.64
[INFO 2017-06-26 18:09:23,970 main.py:47] epoch 3465, training loss: 6529.92, average training loss: 6832.00, base loss: 8848.02
[INFO 2017-06-26 18:09:24,339 main.py:47] epoch 3466, training loss: 6308.10, average training loss: 6831.72, base loss: 8847.33
[INFO 2017-06-26 18:09:24,700 main.py:47] epoch 3467, training loss: 6254.24, average training loss: 6830.82, base loss: 8846.91
[INFO 2017-06-26 18:09:25,058 main.py:47] epoch 3468, training loss: 6854.78, average training loss: 6831.07, base loss: 8847.04
[INFO 2017-06-26 18:09:25,418 main.py:47] epoch 3469, training loss: 6966.59, average training loss: 6831.84, base loss: 8847.97
[INFO 2017-06-26 18:09:25,777 main.py:47] epoch 3470, training loss: 6835.99, average training loss: 6831.33, base loss: 8847.77
[INFO 2017-06-26 18:09:26,140 main.py:47] epoch 3471, training loss: 7102.93, average training loss: 6830.87, base loss: 8847.51
[INFO 2017-06-26 18:09:26,504 main.py:47] epoch 3472, training loss: 7005.05, average training loss: 6831.62, base loss: 8848.17
[INFO 2017-06-26 18:09:26,864 main.py:47] epoch 3473, training loss: 7034.12, average training loss: 6831.89, base loss: 8847.93
[INFO 2017-06-26 18:09:27,257 main.py:47] epoch 3474, training loss: 7417.06, average training loss: 6832.30, base loss: 8848.18
[INFO 2017-06-26 18:09:27,625 main.py:47] epoch 3475, training loss: 6191.55, average training loss: 6832.65, base loss: 8848.99
[INFO 2017-06-26 18:09:27,991 main.py:47] epoch 3476, training loss: 6800.99, average training loss: 6832.29, base loss: 8849.01
[INFO 2017-06-26 18:09:28,352 main.py:47] epoch 3477, training loss: 6869.65, average training loss: 6832.06, base loss: 8848.84
[INFO 2017-06-26 18:09:28,711 main.py:47] epoch 3478, training loss: 6778.02, average training loss: 6832.10, base loss: 8848.53
[INFO 2017-06-26 18:09:29,073 main.py:47] epoch 3479, training loss: 6682.30, average training loss: 6831.67, base loss: 8847.82
[INFO 2017-06-26 18:09:29,433 main.py:47] epoch 3480, training loss: 6676.06, average training loss: 6831.81, base loss: 8848.52
[INFO 2017-06-26 18:09:29,816 main.py:47] epoch 3481, training loss: 6605.58, average training loss: 6832.13, base loss: 8849.44
[INFO 2017-06-26 18:09:30,177 main.py:47] epoch 3482, training loss: 6235.36, average training loss: 6831.33, base loss: 8847.72
[INFO 2017-06-26 18:09:30,537 main.py:47] epoch 3483, training loss: 6129.12, average training loss: 6829.67, base loss: 8844.59
[INFO 2017-06-26 18:09:30,897 main.py:47] epoch 3484, training loss: 6796.66, average training loss: 6829.67, base loss: 8844.38
[INFO 2017-06-26 18:09:31,258 main.py:47] epoch 3485, training loss: 7875.42, average training loss: 6830.59, base loss: 8846.22
[INFO 2017-06-26 18:09:31,617 main.py:47] epoch 3486, training loss: 6568.38, average training loss: 6830.66, base loss: 8846.37
[INFO 2017-06-26 18:09:31,980 main.py:47] epoch 3487, training loss: 6581.76, average training loss: 6830.35, base loss: 8846.06
[INFO 2017-06-26 18:09:32,341 main.py:47] epoch 3488, training loss: 7787.54, average training loss: 6831.62, base loss: 8848.41
[INFO 2017-06-26 18:09:32,704 main.py:47] epoch 3489, training loss: 6227.68, average training loss: 6832.29, base loss: 8848.92
[INFO 2017-06-26 18:09:33,064 main.py:47] epoch 3490, training loss: 7382.99, average training loss: 6833.02, base loss: 8850.57
[INFO 2017-06-26 18:09:33,434 main.py:47] epoch 3491, training loss: 6391.95, average training loss: 6832.97, base loss: 8851.21
[INFO 2017-06-26 18:09:33,794 main.py:47] epoch 3492, training loss: 6900.39, average training loss: 6833.57, base loss: 8852.16
[INFO 2017-06-26 18:09:34,152 main.py:47] epoch 3493, training loss: 6542.02, average training loss: 6833.68, base loss: 8853.07
[INFO 2017-06-26 18:09:34,513 main.py:47] epoch 3494, training loss: 7350.64, average training loss: 6834.47, base loss: 8854.19
[INFO 2017-06-26 18:09:34,872 main.py:47] epoch 3495, training loss: 6360.39, average training loss: 6833.65, base loss: 8853.16
[INFO 2017-06-26 18:09:35,233 main.py:47] epoch 3496, training loss: 6081.02, average training loss: 6831.41, base loss: 8849.73
[INFO 2017-06-26 18:09:35,592 main.py:47] epoch 3497, training loss: 6481.90, average training loss: 6831.05, base loss: 8849.97
[INFO 2017-06-26 18:09:35,952 main.py:47] epoch 3498, training loss: 5622.45, average training loss: 6829.42, base loss: 8848.00
[INFO 2017-06-26 18:09:36,312 main.py:47] epoch 3499, training loss: 6359.26, average training loss: 6828.84, base loss: 8847.73
[INFO 2017-06-26 18:09:36,312 main.py:49] epoch 3499, testing
[INFO 2017-06-26 18:09:40,467 main.py:100] average testing loss: 6765.14, base loss: 8859.59
[INFO 2017-06-26 18:09:40,493 main.py:73] current best accuracy: 6619.67
[INFO 2017-06-26 18:09:40,853 main.py:47] epoch 3500, training loss: 8054.75, average training loss: 6829.93, base loss: 8849.46
[INFO 2017-06-26 18:09:41,212 main.py:47] epoch 3501, training loss: 7137.06, average training loss: 6829.60, base loss: 8848.29
[INFO 2017-06-26 18:09:41,571 main.py:47] epoch 3502, training loss: 7701.98, average training loss: 6830.84, base loss: 8851.07
[INFO 2017-06-26 18:09:41,932 main.py:47] epoch 3503, training loss: 7182.39, average training loss: 6832.10, base loss: 8852.79
[INFO 2017-06-26 18:09:42,291 main.py:47] epoch 3504, training loss: 7416.54, average training loss: 6832.15, base loss: 8852.95
[INFO 2017-06-26 18:09:42,651 main.py:47] epoch 3505, training loss: 7533.77, average training loss: 6832.54, base loss: 8853.86
[INFO 2017-06-26 18:09:43,010 main.py:47] epoch 3506, training loss: 6559.87, average training loss: 6832.38, base loss: 8853.41
[INFO 2017-06-26 18:09:43,366 main.py:47] epoch 3507, training loss: 6532.03, average training loss: 6832.61, base loss: 8853.66
[INFO 2017-06-26 18:09:43,724 main.py:47] epoch 3508, training loss: 7616.02, average training loss: 6833.88, base loss: 8855.66
[INFO 2017-06-26 18:09:44,084 main.py:47] epoch 3509, training loss: 8494.96, average training loss: 6834.70, base loss: 8857.98
[INFO 2017-06-26 18:09:44,440 main.py:47] epoch 3510, training loss: 7934.04, average training loss: 6836.20, base loss: 8859.79
[INFO 2017-06-26 18:09:44,803 main.py:47] epoch 3511, training loss: 7131.11, average training loss: 6836.41, base loss: 8860.08
[INFO 2017-06-26 18:09:45,163 main.py:47] epoch 3512, training loss: 6058.77, average training loss: 6834.77, base loss: 8857.88
[INFO 2017-06-26 18:09:45,521 main.py:47] epoch 3513, training loss: 6151.91, average training loss: 6834.06, base loss: 8857.48
[INFO 2017-06-26 18:09:45,881 main.py:47] epoch 3514, training loss: 6700.89, average training loss: 6833.75, base loss: 8856.58
[INFO 2017-06-26 18:09:46,239 main.py:47] epoch 3515, training loss: 6211.95, average training loss: 6833.63, base loss: 8856.59
[INFO 2017-06-26 18:09:46,598 main.py:47] epoch 3516, training loss: 5929.09, average training loss: 6832.53, base loss: 8855.50
[INFO 2017-06-26 18:09:46,959 main.py:47] epoch 3517, training loss: 7453.33, average training loss: 6833.49, base loss: 8857.07
[INFO 2017-06-26 18:09:47,318 main.py:47] epoch 3518, training loss: 6394.08, average training loss: 6833.10, base loss: 8857.19
[INFO 2017-06-26 18:09:47,677 main.py:47] epoch 3519, training loss: 7969.76, average training loss: 6833.08, base loss: 8856.95
[INFO 2017-06-26 18:09:48,035 main.py:47] epoch 3520, training loss: 6501.45, average training loss: 6833.68, base loss: 8858.68
[INFO 2017-06-26 18:09:48,394 main.py:47] epoch 3521, training loss: 6183.61, average training loss: 6833.05, base loss: 8858.40
[INFO 2017-06-26 18:09:48,753 main.py:47] epoch 3522, training loss: 6737.53, average training loss: 6832.27, base loss: 8857.07
[INFO 2017-06-26 18:09:49,111 main.py:47] epoch 3523, training loss: 6186.22, average training loss: 6831.83, base loss: 8856.85
[INFO 2017-06-26 18:09:49,470 main.py:47] epoch 3524, training loss: 6962.14, average training loss: 6830.41, base loss: 8854.95
[INFO 2017-06-26 18:09:49,829 main.py:47] epoch 3525, training loss: 6384.35, average training loss: 6829.67, base loss: 8853.68
[INFO 2017-06-26 18:09:50,189 main.py:47] epoch 3526, training loss: 7010.42, average training loss: 6829.36, base loss: 8852.51
[INFO 2017-06-26 18:09:50,550 main.py:47] epoch 3527, training loss: 7158.58, average training loss: 6829.85, base loss: 8852.46
[INFO 2017-06-26 18:09:50,911 main.py:47] epoch 3528, training loss: 7228.23, average training loss: 6829.94, base loss: 8852.68
[INFO 2017-06-26 18:09:51,273 main.py:47] epoch 3529, training loss: 6182.03, average training loss: 6829.74, base loss: 8851.92
[INFO 2017-06-26 18:09:51,633 main.py:47] epoch 3530, training loss: 7267.14, average training loss: 6829.16, base loss: 8851.38
[INFO 2017-06-26 18:09:51,991 main.py:47] epoch 3531, training loss: 7027.58, average training loss: 6828.87, base loss: 8851.04
[INFO 2017-06-26 18:09:52,351 main.py:47] epoch 3532, training loss: 6221.98, average training loss: 6828.11, base loss: 8849.85
[INFO 2017-06-26 18:09:52,732 main.py:47] epoch 3533, training loss: 6344.23, average training loss: 6827.97, base loss: 8850.55
[INFO 2017-06-26 18:09:53,092 main.py:47] epoch 3534, training loss: 6228.96, average training loss: 6828.08, base loss: 8850.76
[INFO 2017-06-26 18:09:53,455 main.py:47] epoch 3535, training loss: 6448.58, average training loss: 6827.71, base loss: 8849.87
[INFO 2017-06-26 18:09:53,818 main.py:47] epoch 3536, training loss: 7477.75, average training loss: 6828.97, base loss: 8851.48
[INFO 2017-06-26 18:09:54,177 main.py:47] epoch 3537, training loss: 6689.87, average training loss: 6828.46, base loss: 8850.87
[INFO 2017-06-26 18:09:54,538 main.py:47] epoch 3538, training loss: 6507.93, average training loss: 6828.30, base loss: 8850.80
[INFO 2017-06-26 18:09:54,898 main.py:47] epoch 3539, training loss: 6981.26, average training loss: 6828.78, base loss: 8852.14
[INFO 2017-06-26 18:09:55,258 main.py:47] epoch 3540, training loss: 7426.12, average training loss: 6827.40, base loss: 8851.40
[INFO 2017-06-26 18:09:55,617 main.py:47] epoch 3541, training loss: 6431.13, average training loss: 6828.14, base loss: 8852.24
[INFO 2017-06-26 18:09:55,976 main.py:47] epoch 3542, training loss: 6484.31, average training loss: 6828.51, base loss: 8853.15
[INFO 2017-06-26 18:09:56,335 main.py:47] epoch 3543, training loss: 7420.83, average training loss: 6829.00, base loss: 8854.16
[INFO 2017-06-26 18:09:56,693 main.py:47] epoch 3544, training loss: 6884.56, average training loss: 6828.80, base loss: 8853.47
[INFO 2017-06-26 18:09:57,053 main.py:47] epoch 3545, training loss: 7695.43, average training loss: 6829.96, base loss: 8854.56
[INFO 2017-06-26 18:09:57,414 main.py:47] epoch 3546, training loss: 5946.33, average training loss: 6829.16, base loss: 8853.45
[INFO 2017-06-26 18:09:57,774 main.py:47] epoch 3547, training loss: 7017.15, average training loss: 6829.78, base loss: 8854.49
[INFO 2017-06-26 18:09:58,133 main.py:47] epoch 3548, training loss: 7234.69, average training loss: 6829.28, base loss: 8854.71
[INFO 2017-06-26 18:09:58,492 main.py:47] epoch 3549, training loss: 7022.26, average training loss: 6829.23, base loss: 8854.68
[INFO 2017-06-26 18:09:58,850 main.py:47] epoch 3550, training loss: 8189.66, average training loss: 6830.70, base loss: 8857.35
[INFO 2017-06-26 18:09:59,208 main.py:47] epoch 3551, training loss: 6225.25, average training loss: 6829.86, base loss: 8856.63
[INFO 2017-06-26 18:09:59,568 main.py:47] epoch 3552, training loss: 7989.12, average training loss: 6830.94, base loss: 8858.11
[INFO 2017-06-26 18:09:59,928 main.py:47] epoch 3553, training loss: 7222.90, average training loss: 6831.42, base loss: 8858.78
[INFO 2017-06-26 18:10:00,286 main.py:47] epoch 3554, training loss: 6905.34, average training loss: 6831.55, base loss: 8858.94
[INFO 2017-06-26 18:10:00,646 main.py:47] epoch 3555, training loss: 7682.38, average training loss: 6832.83, base loss: 8860.97
[INFO 2017-06-26 18:10:01,008 main.py:47] epoch 3556, training loss: 6411.93, average training loss: 6831.66, base loss: 8859.04
[INFO 2017-06-26 18:10:01,368 main.py:47] epoch 3557, training loss: 6243.58, average training loss: 6830.95, base loss: 8857.28
[INFO 2017-06-26 18:10:01,728 main.py:47] epoch 3558, training loss: 7016.33, average training loss: 6830.79, base loss: 8857.61
[INFO 2017-06-26 18:10:02,090 main.py:47] epoch 3559, training loss: 6689.43, average training loss: 6830.48, base loss: 8857.62
[INFO 2017-06-26 18:10:02,448 main.py:47] epoch 3560, training loss: 7447.21, average training loss: 6830.43, base loss: 8857.13
[INFO 2017-06-26 18:10:02,809 main.py:47] epoch 3561, training loss: 5958.23, average training loss: 6830.41, base loss: 8857.30
[INFO 2017-06-26 18:10:03,170 main.py:47] epoch 3562, training loss: 7064.71, average training loss: 6831.08, base loss: 8858.35
[INFO 2017-06-26 18:10:03,531 main.py:47] epoch 3563, training loss: 7243.81, average training loss: 6832.01, base loss: 8860.22
[INFO 2017-06-26 18:10:03,893 main.py:47] epoch 3564, training loss: 7149.29, average training loss: 6832.44, base loss: 8860.68
[INFO 2017-06-26 18:10:04,256 main.py:47] epoch 3565, training loss: 6003.11, average training loss: 6830.88, base loss: 8858.66
[INFO 2017-06-26 18:10:04,619 main.py:47] epoch 3566, training loss: 6305.83, average training loss: 6830.86, base loss: 8859.05
[INFO 2017-06-26 18:10:04,982 main.py:47] epoch 3567, training loss: 6152.97, average training loss: 6830.45, base loss: 8858.43
[INFO 2017-06-26 18:10:05,344 main.py:47] epoch 3568, training loss: 6517.21, average training loss: 6829.50, base loss: 8857.65
[INFO 2017-06-26 18:10:05,704 main.py:47] epoch 3569, training loss: 7154.76, average training loss: 6829.74, base loss: 8858.25
[INFO 2017-06-26 18:10:06,065 main.py:47] epoch 3570, training loss: 7309.46, average training loss: 6830.21, base loss: 8858.74
[INFO 2017-06-26 18:10:06,426 main.py:47] epoch 3571, training loss: 7765.34, average training loss: 6831.74, base loss: 8860.98
[INFO 2017-06-26 18:10:06,788 main.py:47] epoch 3572, training loss: 6083.48, average training loss: 6830.54, base loss: 8859.12
[INFO 2017-06-26 18:10:07,148 main.py:47] epoch 3573, training loss: 6643.68, average training loss: 6830.97, base loss: 8859.60
[INFO 2017-06-26 18:10:07,508 main.py:47] epoch 3574, training loss: 7512.97, average training loss: 6831.79, base loss: 8860.72
[INFO 2017-06-26 18:10:07,869 main.py:47] epoch 3575, training loss: 6171.14, average training loss: 6831.25, base loss: 8859.53
[INFO 2017-06-26 18:10:08,230 main.py:47] epoch 3576, training loss: 7079.55, average training loss: 6832.18, base loss: 8861.21
[INFO 2017-06-26 18:10:08,591 main.py:47] epoch 3577, training loss: 6185.28, average training loss: 6831.82, base loss: 8859.72
[INFO 2017-06-26 18:10:08,958 main.py:47] epoch 3578, training loss: 6570.92, average training loss: 6831.29, base loss: 8858.76
[INFO 2017-06-26 18:10:09,320 main.py:47] epoch 3579, training loss: 7518.61, average training loss: 6831.78, base loss: 8859.43
[INFO 2017-06-26 18:10:09,677 main.py:47] epoch 3580, training loss: 7286.60, average training loss: 6831.50, base loss: 8858.72
[INFO 2017-06-26 18:10:10,039 main.py:47] epoch 3581, training loss: 6844.01, average training loss: 6832.58, base loss: 8860.74
[INFO 2017-06-26 18:10:10,400 main.py:47] epoch 3582, training loss: 6709.63, average training loss: 6832.94, base loss: 8861.01
[INFO 2017-06-26 18:10:10,760 main.py:47] epoch 3583, training loss: 6629.09, average training loss: 6833.53, base loss: 8861.73
[INFO 2017-06-26 18:10:11,120 main.py:47] epoch 3584, training loss: 7206.53, average training loss: 6834.24, base loss: 8862.49
[INFO 2017-06-26 18:10:11,482 main.py:47] epoch 3585, training loss: 6785.26, average training loss: 6833.94, base loss: 8862.92
[INFO 2017-06-26 18:10:11,843 main.py:47] epoch 3586, training loss: 6488.57, average training loss: 6834.15, base loss: 8862.78
[INFO 2017-06-26 18:10:12,205 main.py:47] epoch 3587, training loss: 7164.34, average training loss: 6835.30, base loss: 8864.98
[INFO 2017-06-26 18:10:12,566 main.py:47] epoch 3588, training loss: 6660.00, average training loss: 6835.57, base loss: 8865.67
[INFO 2017-06-26 18:10:12,926 main.py:47] epoch 3589, training loss: 6288.45, average training loss: 6834.94, base loss: 8864.45
[INFO 2017-06-26 18:10:13,303 main.py:47] epoch 3590, training loss: 6939.75, average training loss: 6835.95, base loss: 8866.30
[INFO 2017-06-26 18:10:13,664 main.py:47] epoch 3591, training loss: 6479.94, average training loss: 6834.99, base loss: 8865.00
[INFO 2017-06-26 18:10:14,032 main.py:47] epoch 3592, training loss: 6707.99, average training loss: 6833.66, base loss: 8862.89
[INFO 2017-06-26 18:10:14,406 main.py:47] epoch 3593, training loss: 6089.06, average training loss: 6832.45, base loss: 8860.23
[INFO 2017-06-26 18:10:14,764 main.py:47] epoch 3594, training loss: 6742.21, average training loss: 6832.50, base loss: 8859.10
[INFO 2017-06-26 18:10:15,124 main.py:47] epoch 3595, training loss: 5853.60, average training loss: 6831.28, base loss: 8856.72
[INFO 2017-06-26 18:10:15,483 main.py:47] epoch 3596, training loss: 6986.67, average training loss: 6830.27, base loss: 8855.80
[INFO 2017-06-26 18:10:15,841 main.py:47] epoch 3597, training loss: 5734.17, average training loss: 6829.01, base loss: 8853.75
[INFO 2017-06-26 18:10:16,200 main.py:47] epoch 3598, training loss: 7258.29, average training loss: 6829.13, base loss: 8853.79
[INFO 2017-06-26 18:10:16,558 main.py:47] epoch 3599, training loss: 8398.66, average training loss: 6830.40, base loss: 8855.18
[INFO 2017-06-26 18:10:16,558 main.py:49] epoch 3599, testing
[INFO 2017-06-26 18:10:20,715 main.py:100] average testing loss: 6819.47, base loss: 8663.78
[INFO 2017-06-26 18:10:20,740 main.py:73] current best accuracy: 6619.67
[INFO 2017-06-26 18:10:21,099 main.py:47] epoch 3600, training loss: 7391.04, average training loss: 6831.13, base loss: 8856.14
[INFO 2017-06-26 18:10:21,457 main.py:47] epoch 3601, training loss: 7551.83, average training loss: 6831.02, base loss: 8855.79
[INFO 2017-06-26 18:10:21,815 main.py:47] epoch 3602, training loss: 8155.97, average training loss: 6831.75, base loss: 8857.10
[INFO 2017-06-26 18:10:22,175 main.py:47] epoch 3603, training loss: 5937.29, average training loss: 6831.28, base loss: 8856.16
[INFO 2017-06-26 18:10:22,535 main.py:47] epoch 3604, training loss: 6790.39, average training loss: 6831.79, base loss: 8857.13
[INFO 2017-06-26 18:10:22,899 main.py:47] epoch 3605, training loss: 7051.09, average training loss: 6831.94, base loss: 8857.67
[INFO 2017-06-26 18:10:23,261 main.py:47] epoch 3606, training loss: 6198.73, average training loss: 6831.51, base loss: 8857.50
[INFO 2017-06-26 18:10:23,623 main.py:47] epoch 3607, training loss: 6784.49, average training loss: 6831.02, base loss: 8857.30
[INFO 2017-06-26 18:10:23,984 main.py:47] epoch 3608, training loss: 6948.52, average training loss: 6831.61, base loss: 8858.91
[INFO 2017-06-26 18:10:24,342 main.py:47] epoch 3609, training loss: 7399.86, average training loss: 6832.61, base loss: 8860.85
[INFO 2017-06-26 18:10:24,703 main.py:47] epoch 3610, training loss: 6970.98, average training loss: 6833.28, base loss: 8862.15
[INFO 2017-06-26 18:10:25,064 main.py:47] epoch 3611, training loss: 6146.37, average training loss: 6832.48, base loss: 8860.64
[INFO 2017-06-26 18:10:25,424 main.py:47] epoch 3612, training loss: 7404.17, average training loss: 6832.36, base loss: 8860.37
[INFO 2017-06-26 18:10:25,783 main.py:47] epoch 3613, training loss: 6157.67, average training loss: 6831.95, base loss: 8860.13
[INFO 2017-06-26 18:10:26,143 main.py:47] epoch 3614, training loss: 7149.20, average training loss: 6832.08, base loss: 8860.66
[INFO 2017-06-26 18:10:26,502 main.py:47] epoch 3615, training loss: 7171.14, average training loss: 6831.85, base loss: 8860.40
[INFO 2017-06-26 18:10:26,860 main.py:47] epoch 3616, training loss: 7373.57, average training loss: 6832.18, base loss: 8860.93
[INFO 2017-06-26 18:10:27,218 main.py:47] epoch 3617, training loss: 6746.24, average training loss: 6832.45, base loss: 8861.47
[INFO 2017-06-26 18:10:27,575 main.py:47] epoch 3618, training loss: 6638.41, average training loss: 6831.76, base loss: 8860.86
[INFO 2017-06-26 18:10:27,932 main.py:47] epoch 3619, training loss: 6425.42, average training loss: 6830.25, base loss: 8858.89
[INFO 2017-06-26 18:10:28,290 main.py:47] epoch 3620, training loss: 6705.60, average training loss: 6830.13, base loss: 8858.92
[INFO 2017-06-26 18:10:28,649 main.py:47] epoch 3621, training loss: 7019.17, average training loss: 6829.91, base loss: 8858.72
[INFO 2017-06-26 18:10:29,009 main.py:47] epoch 3622, training loss: 6850.07, average training loss: 6830.08, base loss: 8858.59
[INFO 2017-06-26 18:10:29,368 main.py:47] epoch 3623, training loss: 7258.23, average training loss: 6830.69, base loss: 8859.48
[INFO 2017-06-26 18:10:29,727 main.py:47] epoch 3624, training loss: 6405.61, average training loss: 6830.94, base loss: 8859.96
[INFO 2017-06-26 18:10:30,087 main.py:47] epoch 3625, training loss: 6296.69, average training loss: 6830.68, base loss: 8859.99
[INFO 2017-06-26 18:10:30,447 main.py:47] epoch 3626, training loss: 6989.62, average training loss: 6830.39, base loss: 8860.40
[INFO 2017-06-26 18:10:30,805 main.py:47] epoch 3627, training loss: 6687.04, average training loss: 6830.16, base loss: 8859.93
[INFO 2017-06-26 18:10:31,165 main.py:47] epoch 3628, training loss: 6555.87, average training loss: 6829.71, base loss: 8859.75
[INFO 2017-06-26 18:10:31,526 main.py:47] epoch 3629, training loss: 6650.22, average training loss: 6829.72, base loss: 8859.53
[INFO 2017-06-26 18:10:31,884 main.py:47] epoch 3630, training loss: 6126.50, average training loss: 6828.93, base loss: 8858.42
[INFO 2017-06-26 18:10:32,244 main.py:47] epoch 3631, training loss: 6264.65, average training loss: 6828.55, base loss: 8858.48
[INFO 2017-06-26 18:10:32,603 main.py:47] epoch 3632, training loss: 6195.62, average training loss: 6828.54, base loss: 8858.33
[INFO 2017-06-26 18:10:32,963 main.py:47] epoch 3633, training loss: 6655.66, average training loss: 6828.49, base loss: 8858.09
[INFO 2017-06-26 18:10:33,322 main.py:47] epoch 3634, training loss: 7610.33, average training loss: 6828.67, base loss: 8858.41
[INFO 2017-06-26 18:10:33,683 main.py:47] epoch 3635, training loss: 6539.33, average training loss: 6828.20, base loss: 8857.75
[INFO 2017-06-26 18:10:34,043 main.py:47] epoch 3636, training loss: 7653.92, average training loss: 6828.81, base loss: 8858.59
[INFO 2017-06-26 18:10:34,402 main.py:47] epoch 3637, training loss: 6562.97, average training loss: 6829.60, base loss: 8859.85
[INFO 2017-06-26 18:10:34,764 main.py:47] epoch 3638, training loss: 6934.56, average training loss: 6829.73, base loss: 8860.96
[INFO 2017-06-26 18:10:35,125 main.py:47] epoch 3639, training loss: 7730.08, average training loss: 6830.97, base loss: 8863.11
[INFO 2017-06-26 18:10:35,487 main.py:47] epoch 3640, training loss: 6722.55, average training loss: 6830.96, base loss: 8862.91
[INFO 2017-06-26 18:10:35,846 main.py:47] epoch 3641, training loss: 7103.35, average training loss: 6831.26, base loss: 8863.62
[INFO 2017-06-26 18:10:36,205 main.py:47] epoch 3642, training loss: 7089.75, average training loss: 6831.94, base loss: 8864.97
[INFO 2017-06-26 18:10:36,564 main.py:47] epoch 3643, training loss: 6214.65, average training loss: 6831.47, base loss: 8864.42
[INFO 2017-06-26 18:10:36,923 main.py:47] epoch 3644, training loss: 6427.25, average training loss: 6830.73, base loss: 8864.03
[INFO 2017-06-26 18:10:37,281 main.py:47] epoch 3645, training loss: 6409.63, average training loss: 6829.99, base loss: 8862.66
[INFO 2017-06-26 18:10:37,639 main.py:47] epoch 3646, training loss: 6530.37, average training loss: 6829.31, base loss: 8861.97
[INFO 2017-06-26 18:10:37,999 main.py:47] epoch 3647, training loss: 7080.03, average training loss: 6829.42, base loss: 8862.55
[INFO 2017-06-26 18:10:38,359 main.py:47] epoch 3648, training loss: 6600.73, average training loss: 6828.54, base loss: 8861.17
[INFO 2017-06-26 18:10:38,717 main.py:47] epoch 3649, training loss: 5976.75, average training loss: 6828.54, base loss: 8861.40
[INFO 2017-06-26 18:10:39,105 main.py:47] epoch 3650, training loss: 7086.32, average training loss: 6828.88, base loss: 8861.25
[INFO 2017-06-26 18:10:39,465 main.py:47] epoch 3651, training loss: 7084.61, average training loss: 6828.35, base loss: 8860.38
[INFO 2017-06-26 18:10:39,827 main.py:47] epoch 3652, training loss: 5862.36, average training loss: 6827.11, base loss: 8857.45
[INFO 2017-06-26 18:10:40,187 main.py:47] epoch 3653, training loss: 6201.37, average training loss: 6826.40, base loss: 8856.71
[INFO 2017-06-26 18:10:40,546 main.py:47] epoch 3654, training loss: 7871.92, average training loss: 6827.22, base loss: 8858.03
[INFO 2017-06-26 18:10:40,906 main.py:47] epoch 3655, training loss: 6636.94, average training loss: 6827.64, base loss: 8858.76
[INFO 2017-06-26 18:10:41,265 main.py:47] epoch 3656, training loss: 6655.93, average training loss: 6827.34, base loss: 8858.01
[INFO 2017-06-26 18:10:41,624 main.py:47] epoch 3657, training loss: 7072.43, average training loss: 6828.62, base loss: 8860.83
[INFO 2017-06-26 18:10:41,988 main.py:47] epoch 3658, training loss: 6475.19, average training loss: 6827.62, base loss: 8859.14
[INFO 2017-06-26 18:10:42,346 main.py:47] epoch 3659, training loss: 6389.65, average training loss: 6827.43, base loss: 8858.88
[INFO 2017-06-26 18:10:42,707 main.py:47] epoch 3660, training loss: 7008.38, average training loss: 6827.76, base loss: 8860.02
[INFO 2017-06-26 18:10:43,067 main.py:47] epoch 3661, training loss: 6610.52, average training loss: 6827.91, base loss: 8861.12
[INFO 2017-06-26 18:10:43,424 main.py:47] epoch 3662, training loss: 7717.51, average training loss: 6828.90, base loss: 8862.90
[INFO 2017-06-26 18:10:43,783 main.py:47] epoch 3663, training loss: 6032.80, average training loss: 6827.51, base loss: 8860.90
[INFO 2017-06-26 18:10:44,141 main.py:47] epoch 3664, training loss: 6375.62, average training loss: 6826.99, base loss: 8860.78
[INFO 2017-06-26 18:10:44,502 main.py:47] epoch 3665, training loss: 6490.43, average training loss: 6826.85, base loss: 8860.23
[INFO 2017-06-26 18:10:44,860 main.py:47] epoch 3666, training loss: 6561.92, average training loss: 6826.85, base loss: 8860.09
[INFO 2017-06-26 18:10:45,220 main.py:47] epoch 3667, training loss: 6721.21, average training loss: 6827.33, base loss: 8860.49
[INFO 2017-06-26 18:10:45,580 main.py:47] epoch 3668, training loss: 6509.52, average training loss: 6826.58, base loss: 8859.65
[INFO 2017-06-26 18:10:45,941 main.py:47] epoch 3669, training loss: 6700.73, average training loss: 6826.36, base loss: 8859.30
[INFO 2017-06-26 18:10:46,300 main.py:47] epoch 3670, training loss: 7222.90, average training loss: 6827.08, base loss: 8860.12
[INFO 2017-06-26 18:10:46,661 main.py:47] epoch 3671, training loss: 7577.05, average training loss: 6827.07, base loss: 8859.86
[INFO 2017-06-26 18:10:47,021 main.py:47] epoch 3672, training loss: 6850.93, average training loss: 6827.05, base loss: 8860.34
[INFO 2017-06-26 18:10:47,380 main.py:47] epoch 3673, training loss: 7740.11, average training loss: 6827.64, base loss: 8861.05
[INFO 2017-06-26 18:10:47,737 main.py:47] epoch 3674, training loss: 6338.54, average training loss: 6827.55, base loss: 8861.31
[INFO 2017-06-26 18:10:48,097 main.py:47] epoch 3675, training loss: 6774.50, average training loss: 6827.50, base loss: 8860.91
[INFO 2017-06-26 18:10:48,459 main.py:47] epoch 3676, training loss: 7190.35, average training loss: 6828.39, base loss: 8862.67
[INFO 2017-06-26 18:10:48,818 main.py:47] epoch 3677, training loss: 6082.75, average training loss: 6828.13, base loss: 8861.56
[INFO 2017-06-26 18:10:49,178 main.py:47] epoch 3678, training loss: 7140.73, average training loss: 6828.27, base loss: 8862.69
[INFO 2017-06-26 18:10:49,536 main.py:47] epoch 3679, training loss: 6709.58, average training loss: 6828.66, base loss: 8863.23
[INFO 2017-06-26 18:10:49,897 main.py:47] epoch 3680, training loss: 6635.62, average training loss: 6827.49, base loss: 8861.08
[INFO 2017-06-26 18:10:50,256 main.py:47] epoch 3681, training loss: 7371.73, average training loss: 6828.75, base loss: 8863.58
[INFO 2017-06-26 18:10:50,616 main.py:47] epoch 3682, training loss: 6233.18, average training loss: 6827.83, base loss: 8862.31
[INFO 2017-06-26 18:10:50,976 main.py:47] epoch 3683, training loss: 6022.26, average training loss: 6826.28, base loss: 8860.02
[INFO 2017-06-26 18:10:51,336 main.py:47] epoch 3684, training loss: 7180.76, average training loss: 6826.75, base loss: 8861.74
[INFO 2017-06-26 18:10:51,696 main.py:47] epoch 3685, training loss: 6876.32, average training loss: 6825.46, base loss: 8859.47
[INFO 2017-06-26 18:10:52,055 main.py:47] epoch 3686, training loss: 6852.28, average training loss: 6826.14, base loss: 8861.30
[INFO 2017-06-26 18:10:52,414 main.py:47] epoch 3687, training loss: 6787.16, average training loss: 6826.00, base loss: 8860.81
[INFO 2017-06-26 18:10:52,772 main.py:47] epoch 3688, training loss: 7469.81, average training loss: 6825.92, base loss: 8860.51
[INFO 2017-06-26 18:10:53,131 main.py:47] epoch 3689, training loss: 6869.62, average training loss: 6825.51, base loss: 8860.71
[INFO 2017-06-26 18:10:53,492 main.py:47] epoch 3690, training loss: 6414.84, average training loss: 6824.49, base loss: 8858.76
[INFO 2017-06-26 18:10:53,853 main.py:47] epoch 3691, training loss: 6878.51, average training loss: 6824.40, base loss: 8858.97
[INFO 2017-06-26 18:10:54,212 main.py:47] epoch 3692, training loss: 6946.76, average training loss: 6824.52, base loss: 8859.04
[INFO 2017-06-26 18:10:54,569 main.py:47] epoch 3693, training loss: 6890.51, average training loss: 6825.05, base loss: 8860.77
[INFO 2017-06-26 18:10:54,930 main.py:47] epoch 3694, training loss: 6277.15, average training loss: 6824.48, base loss: 8860.23
[INFO 2017-06-26 18:10:55,291 main.py:47] epoch 3695, training loss: 7500.65, average training loss: 6825.52, base loss: 8861.81
[INFO 2017-06-26 18:10:55,649 main.py:47] epoch 3696, training loss: 6592.25, average training loss: 6825.81, base loss: 8861.78
[INFO 2017-06-26 18:10:56,009 main.py:47] epoch 3697, training loss: 6846.52, average training loss: 6825.58, base loss: 8860.88
[INFO 2017-06-26 18:10:56,372 main.py:47] epoch 3698, training loss: 6146.23, average training loss: 6824.84, base loss: 8859.88
[INFO 2017-06-26 18:10:56,735 main.py:47] epoch 3699, training loss: 7030.82, average training loss: 6825.47, base loss: 8860.37
[INFO 2017-06-26 18:10:56,735 main.py:49] epoch 3699, testing
[INFO 2017-06-26 18:11:00,990 main.py:100] average testing loss: 6364.27, base loss: 8363.28
[INFO 2017-06-26 18:11:01,014 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 18:11:01,021 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:11:01,380 main.py:47] epoch 3700, training loss: 6511.83, average training loss: 6825.38, base loss: 8859.61
[INFO 2017-06-26 18:11:01,739 main.py:47] epoch 3701, training loss: 6635.53, average training loss: 6825.74, base loss: 8859.84
[INFO 2017-06-26 18:11:02,098 main.py:47] epoch 3702, training loss: 6527.26, average training loss: 6825.94, base loss: 8860.31
[INFO 2017-06-26 18:11:02,456 main.py:47] epoch 3703, training loss: 6613.65, average training loss: 6825.20, base loss: 8859.88
[INFO 2017-06-26 18:11:02,816 main.py:47] epoch 3704, training loss: 6676.40, average training loss: 6824.59, base loss: 8859.46
[INFO 2017-06-26 18:11:03,176 main.py:47] epoch 3705, training loss: 5794.94, average training loss: 6823.83, base loss: 8858.05
[INFO 2017-06-26 18:11:03,536 main.py:47] epoch 3706, training loss: 6197.93, average training loss: 6824.04, base loss: 8858.57
[INFO 2017-06-26 18:11:03,895 main.py:47] epoch 3707, training loss: 6492.98, average training loss: 6823.50, base loss: 8857.21
[INFO 2017-06-26 18:11:04,253 main.py:47] epoch 3708, training loss: 6214.11, average training loss: 6823.41, base loss: 8857.64
[INFO 2017-06-26 18:11:04,612 main.py:47] epoch 3709, training loss: 6568.10, average training loss: 6822.30, base loss: 8855.79
[INFO 2017-06-26 18:11:04,970 main.py:47] epoch 3710, training loss: 6534.39, average training loss: 6821.32, base loss: 8854.71
[INFO 2017-06-26 18:11:05,331 main.py:47] epoch 3711, training loss: 7557.52, average training loss: 6821.57, base loss: 8855.45
[INFO 2017-06-26 18:11:05,690 main.py:47] epoch 3712, training loss: 7031.63, average training loss: 6822.68, base loss: 8857.67
[INFO 2017-06-26 18:11:06,049 main.py:47] epoch 3713, training loss: 6698.79, average training loss: 6822.23, base loss: 8856.65
[INFO 2017-06-26 18:11:06,410 main.py:47] epoch 3714, training loss: 6918.95, average training loss: 6823.17, base loss: 8858.16
[INFO 2017-06-26 18:11:06,768 main.py:47] epoch 3715, training loss: 6544.46, average training loss: 6823.38, base loss: 8857.46
[INFO 2017-06-26 18:11:07,130 main.py:47] epoch 3716, training loss: 8336.25, average training loss: 6824.89, base loss: 8859.70
[INFO 2017-06-26 18:11:07,494 main.py:47] epoch 3717, training loss: 6441.86, average training loss: 6824.61, base loss: 8859.07
[INFO 2017-06-26 18:11:07,856 main.py:47] epoch 3718, training loss: 6118.42, average training loss: 6823.97, base loss: 8857.63
[INFO 2017-06-26 18:11:08,219 main.py:47] epoch 3719, training loss: 6434.44, average training loss: 6822.93, base loss: 8856.64
[INFO 2017-06-26 18:11:08,581 main.py:47] epoch 3720, training loss: 6183.51, average training loss: 6822.25, base loss: 8856.43
[INFO 2017-06-26 18:11:08,943 main.py:47] epoch 3721, training loss: 6612.16, average training loss: 6821.18, base loss: 8855.11
[INFO 2017-06-26 18:11:09,303 main.py:47] epoch 3722, training loss: 6467.26, average training loss: 6821.56, base loss: 8856.01
[INFO 2017-06-26 18:11:09,668 main.py:47] epoch 3723, training loss: 6296.37, average training loss: 6821.32, base loss: 8855.10
[INFO 2017-06-26 18:11:10,028 main.py:47] epoch 3724, training loss: 7552.01, average training loss: 6822.30, base loss: 8856.14
[INFO 2017-06-26 18:11:10,386 main.py:47] epoch 3725, training loss: 6668.27, average training loss: 6823.04, base loss: 8857.44
[INFO 2017-06-26 18:11:10,746 main.py:47] epoch 3726, training loss: 6882.69, average training loss: 6823.19, base loss: 8857.41
[INFO 2017-06-26 18:11:11,106 main.py:47] epoch 3727, training loss: 6249.94, average training loss: 6822.23, base loss: 8855.59
[INFO 2017-06-26 18:11:11,465 main.py:47] epoch 3728, training loss: 6108.65, average training loss: 6821.89, base loss: 8855.20
[INFO 2017-06-26 18:11:11,826 main.py:47] epoch 3729, training loss: 6950.24, average training loss: 6822.11, base loss: 8855.25
[INFO 2017-06-26 18:11:12,186 main.py:47] epoch 3730, training loss: 6810.73, average training loss: 6822.26, base loss: 8854.45
[INFO 2017-06-26 18:11:12,544 main.py:47] epoch 3731, training loss: 7277.48, average training loss: 6822.32, base loss: 8854.58
[INFO 2017-06-26 18:11:12,904 main.py:47] epoch 3732, training loss: 6679.28, average training loss: 6822.12, base loss: 8853.97
[INFO 2017-06-26 18:11:13,264 main.py:47] epoch 3733, training loss: 7809.00, average training loss: 6823.12, base loss: 8854.69
[INFO 2017-06-26 18:11:13,623 main.py:47] epoch 3734, training loss: 8413.61, average training loss: 6824.40, base loss: 8856.53
[INFO 2017-06-26 18:11:13,983 main.py:47] epoch 3735, training loss: 7124.67, average training loss: 6823.98, base loss: 8856.73
[INFO 2017-06-26 18:11:14,343 main.py:47] epoch 3736, training loss: 6801.26, average training loss: 6824.37, base loss: 8857.00
[INFO 2017-06-26 18:11:14,702 main.py:47] epoch 3737, training loss: 7352.14, average training loss: 6824.55, base loss: 8857.28
[INFO 2017-06-26 18:11:15,061 main.py:47] epoch 3738, training loss: 6688.39, average training loss: 6824.56, base loss: 8857.66
[INFO 2017-06-26 18:11:15,420 main.py:47] epoch 3739, training loss: 7290.41, average training loss: 6824.34, base loss: 8858.28
[INFO 2017-06-26 18:11:15,780 main.py:47] epoch 3740, training loss: 5927.23, average training loss: 6823.36, base loss: 8856.73
[INFO 2017-06-26 18:11:16,139 main.py:47] epoch 3741, training loss: 6475.74, average training loss: 6823.15, base loss: 8855.58
[INFO 2017-06-26 18:11:16,498 main.py:47] epoch 3742, training loss: 7513.30, average training loss: 6823.48, base loss: 8856.53
[INFO 2017-06-26 18:11:16,857 main.py:47] epoch 3743, training loss: 5902.05, average training loss: 6822.55, base loss: 8855.30
[INFO 2017-06-26 18:11:17,215 main.py:47] epoch 3744, training loss: 6085.90, average training loss: 6822.15, base loss: 8854.68
[INFO 2017-06-26 18:11:17,574 main.py:47] epoch 3745, training loss: 6424.28, average training loss: 6822.69, base loss: 8855.54
[INFO 2017-06-26 18:11:17,934 main.py:47] epoch 3746, training loss: 7520.04, average training loss: 6823.69, base loss: 8857.10
[INFO 2017-06-26 18:11:18,294 main.py:47] epoch 3747, training loss: 6891.65, average training loss: 6823.46, base loss: 8857.18
[INFO 2017-06-26 18:11:18,655 main.py:47] epoch 3748, training loss: 6512.16, average training loss: 6823.61, base loss: 8857.24
[INFO 2017-06-26 18:11:19,014 main.py:47] epoch 3749, training loss: 6945.23, average training loss: 6824.53, base loss: 8858.88
[INFO 2017-06-26 18:11:19,371 main.py:47] epoch 3750, training loss: 7058.04, average training loss: 6824.71, base loss: 8859.15
[INFO 2017-06-26 18:11:19,731 main.py:47] epoch 3751, training loss: 6909.63, average training loss: 6824.34, base loss: 8858.13
[INFO 2017-06-26 18:11:20,091 main.py:47] epoch 3752, training loss: 6424.65, average training loss: 6823.16, base loss: 8856.44
[INFO 2017-06-26 18:11:20,454 main.py:47] epoch 3753, training loss: 6424.50, average training loss: 6822.47, base loss: 8855.58
[INFO 2017-06-26 18:11:20,816 main.py:47] epoch 3754, training loss: 5954.09, average training loss: 6822.23, base loss: 8855.23
[INFO 2017-06-26 18:11:21,182 main.py:47] epoch 3755, training loss: 6025.42, average training loss: 6821.69, base loss: 8854.06
[INFO 2017-06-26 18:11:21,543 main.py:47] epoch 3756, training loss: 6042.84, average training loss: 6821.44, base loss: 8853.50
[INFO 2017-06-26 18:11:21,904 main.py:47] epoch 3757, training loss: 6928.97, average training loss: 6821.51, base loss: 8853.83
[INFO 2017-06-26 18:11:22,269 main.py:47] epoch 3758, training loss: 6164.72, average training loss: 6820.63, base loss: 8852.32
[INFO 2017-06-26 18:11:22,632 main.py:47] epoch 3759, training loss: 7140.63, average training loss: 6821.68, base loss: 8854.56
[INFO 2017-06-26 18:11:22,993 main.py:47] epoch 3760, training loss: 7308.67, average training loss: 6822.34, base loss: 8855.16
[INFO 2017-06-26 18:11:23,351 main.py:47] epoch 3761, training loss: 7260.91, average training loss: 6822.34, base loss: 8855.40
[INFO 2017-06-26 18:11:23,723 main.py:47] epoch 3762, training loss: 7083.41, average training loss: 6822.24, base loss: 8854.61
[INFO 2017-06-26 18:11:24,089 main.py:47] epoch 3763, training loss: 5839.60, average training loss: 6821.80, base loss: 8854.22
[INFO 2017-06-26 18:11:24,453 main.py:47] epoch 3764, training loss: 6532.62, average training loss: 6821.56, base loss: 8853.66
[INFO 2017-06-26 18:11:24,814 main.py:47] epoch 3765, training loss: 6371.47, average training loss: 6821.06, base loss: 8852.95
[INFO 2017-06-26 18:11:25,178 main.py:47] epoch 3766, training loss: 6207.32, average training loss: 6818.91, base loss: 8849.41
[INFO 2017-06-26 18:11:25,541 main.py:47] epoch 3767, training loss: 6451.80, average training loss: 6818.23, base loss: 8848.81
[INFO 2017-06-26 18:11:25,906 main.py:47] epoch 3768, training loss: 6871.54, average training loss: 6818.25, base loss: 8848.94
[INFO 2017-06-26 18:11:26,267 main.py:47] epoch 3769, training loss: 8246.80, average training loss: 6818.12, base loss: 8848.42
[INFO 2017-06-26 18:11:26,627 main.py:47] epoch 3770, training loss: 6231.22, average training loss: 6817.55, base loss: 8848.52
[INFO 2017-06-26 18:11:26,988 main.py:47] epoch 3771, training loss: 7446.66, average training loss: 6818.08, base loss: 8848.88
[INFO 2017-06-26 18:11:27,349 main.py:47] epoch 3772, training loss: 6750.11, average training loss: 6817.62, base loss: 8848.76
[INFO 2017-06-26 18:11:27,710 main.py:47] epoch 3773, training loss: 7054.92, average training loss: 6817.23, base loss: 8848.34
[INFO 2017-06-26 18:11:28,069 main.py:47] epoch 3774, training loss: 7321.07, average training loss: 6817.42, base loss: 8848.50
[INFO 2017-06-26 18:11:28,439 main.py:47] epoch 3775, training loss: 6671.33, average training loss: 6817.01, base loss: 8848.45
[INFO 2017-06-26 18:11:28,834 main.py:47] epoch 3776, training loss: 7028.67, average training loss: 6817.30, base loss: 8849.11
[INFO 2017-06-26 18:11:29,226 main.py:47] epoch 3777, training loss: 6672.24, average training loss: 6817.98, base loss: 8850.39
[INFO 2017-06-26 18:11:29,605 main.py:47] epoch 3778, training loss: 7459.47, average training loss: 6819.34, base loss: 8853.04
[INFO 2017-06-26 18:11:29,981 main.py:47] epoch 3779, training loss: 7052.32, average training loss: 6819.80, base loss: 8854.11
[INFO 2017-06-26 18:11:30,364 main.py:47] epoch 3780, training loss: 6512.37, average training loss: 6818.57, base loss: 8853.14
[INFO 2017-06-26 18:11:30,772 main.py:47] epoch 3781, training loss: 6521.49, average training loss: 6818.20, base loss: 8853.17
[INFO 2017-06-26 18:11:31,151 main.py:47] epoch 3782, training loss: 5851.58, average training loss: 6816.98, base loss: 8851.17
[INFO 2017-06-26 18:11:31,514 main.py:47] epoch 3783, training loss: 7054.99, average training loss: 6816.35, base loss: 8851.00
[INFO 2017-06-26 18:11:31,875 main.py:47] epoch 3784, training loss: 6840.34, average training loss: 6816.55, base loss: 8851.19
[INFO 2017-06-26 18:11:32,236 main.py:47] epoch 3785, training loss: 7277.81, average training loss: 6816.55, base loss: 8851.15
[INFO 2017-06-26 18:11:32,595 main.py:47] epoch 3786, training loss: 6679.28, average training loss: 6815.98, base loss: 8850.63
[INFO 2017-06-26 18:11:32,953 main.py:47] epoch 3787, training loss: 6256.52, average training loss: 6816.10, base loss: 8851.10
[INFO 2017-06-26 18:11:33,313 main.py:47] epoch 3788, training loss: 7977.41, average training loss: 6818.12, base loss: 8854.16
[INFO 2017-06-26 18:11:33,673 main.py:47] epoch 3789, training loss: 6094.24, average training loss: 6817.33, base loss: 8852.95
[INFO 2017-06-26 18:11:34,032 main.py:47] epoch 3790, training loss: 6739.67, average training loss: 6817.66, base loss: 8854.08
[INFO 2017-06-26 18:11:34,391 main.py:47] epoch 3791, training loss: 6684.87, average training loss: 6817.75, base loss: 8854.18
[INFO 2017-06-26 18:11:34,750 main.py:47] epoch 3792, training loss: 6650.09, average training loss: 6817.80, base loss: 8854.60
[INFO 2017-06-26 18:11:35,109 main.py:47] epoch 3793, training loss: 7646.81, average training loss: 6817.98, base loss: 8854.69
[INFO 2017-06-26 18:11:35,469 main.py:47] epoch 3794, training loss: 7089.14, average training loss: 6818.65, base loss: 8856.44
[INFO 2017-06-26 18:11:35,827 main.py:47] epoch 3795, training loss: 5978.31, average training loss: 6818.26, base loss: 8855.49
[INFO 2017-06-26 18:11:36,184 main.py:47] epoch 3796, training loss: 7316.06, average training loss: 6818.66, base loss: 8856.37
[INFO 2017-06-26 18:11:36,598 main.py:47] epoch 3797, training loss: 6932.16, average training loss: 6818.75, base loss: 8857.18
[INFO 2017-06-26 18:11:36,976 main.py:47] epoch 3798, training loss: 7932.87, average training loss: 6819.08, base loss: 8857.83
[INFO 2017-06-26 18:11:37,337 main.py:47] epoch 3799, training loss: 8158.39, average training loss: 6820.61, base loss: 8860.70
[INFO 2017-06-26 18:11:37,337 main.py:49] epoch 3799, testing
[INFO 2017-06-26 18:11:41,727 main.py:100] average testing loss: 6927.76, base loss: 9026.29
[INFO 2017-06-26 18:11:41,753 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:11:42,108 main.py:47] epoch 3800, training loss: 6545.92, average training loss: 6820.10, base loss: 8860.17
[INFO 2017-06-26 18:11:42,467 main.py:47] epoch 3801, training loss: 6426.85, average training loss: 6820.26, base loss: 8861.04
[INFO 2017-06-26 18:11:42,827 main.py:47] epoch 3802, training loss: 6514.56, average training loss: 6819.02, base loss: 8859.20
[INFO 2017-06-26 18:11:43,185 main.py:47] epoch 3803, training loss: 7315.56, average training loss: 6819.46, base loss: 8861.14
[INFO 2017-06-26 18:11:43,545 main.py:47] epoch 3804, training loss: 6151.05, average training loss: 6819.20, base loss: 8861.83
[INFO 2017-06-26 18:11:43,904 main.py:47] epoch 3805, training loss: 6505.33, average training loss: 6819.06, base loss: 8862.58
[INFO 2017-06-26 18:11:44,263 main.py:47] epoch 3806, training loss: 6901.09, average training loss: 6818.93, base loss: 8862.49
[INFO 2017-06-26 18:11:44,623 main.py:47] epoch 3807, training loss: 7140.77, average training loss: 6819.18, base loss: 8863.42
[INFO 2017-06-26 18:11:44,983 main.py:47] epoch 3808, training loss: 7538.31, average training loss: 6819.96, base loss: 8864.98
[INFO 2017-06-26 18:11:45,342 main.py:47] epoch 3809, training loss: 6720.21, average training loss: 6819.90, base loss: 8864.75
[INFO 2017-06-26 18:11:45,701 main.py:47] epoch 3810, training loss: 6205.21, average training loss: 6818.68, base loss: 8863.31
[INFO 2017-06-26 18:11:46,061 main.py:47] epoch 3811, training loss: 6726.72, average training loss: 6817.58, base loss: 8862.10
[INFO 2017-06-26 18:11:46,421 main.py:47] epoch 3812, training loss: 6046.37, average training loss: 6815.97, base loss: 8859.83
[INFO 2017-06-26 18:11:46,780 main.py:47] epoch 3813, training loss: 6852.34, average training loss: 6816.02, base loss: 8859.29
[INFO 2017-06-26 18:11:47,135 main.py:47] epoch 3814, training loss: 6367.37, average training loss: 6816.14, base loss: 8859.83
[INFO 2017-06-26 18:11:47,494 main.py:47] epoch 3815, training loss: 6950.78, average training loss: 6816.69, base loss: 8860.50
[INFO 2017-06-26 18:11:47,853 main.py:47] epoch 3816, training loss: 6350.58, average training loss: 6816.40, base loss: 8860.15
[INFO 2017-06-26 18:11:48,212 main.py:47] epoch 3817, training loss: 7307.78, average training loss: 6816.92, base loss: 8861.55
[INFO 2017-06-26 18:11:48,570 main.py:47] epoch 3818, training loss: 6811.61, average training loss: 6817.01, base loss: 8861.94
[INFO 2017-06-26 18:11:48,929 main.py:47] epoch 3819, training loss: 6383.02, average training loss: 6816.28, base loss: 8860.87
[INFO 2017-06-26 18:11:49,286 main.py:47] epoch 3820, training loss: 6869.12, average training loss: 6816.53, base loss: 8861.45
[INFO 2017-06-26 18:11:49,645 main.py:47] epoch 3821, training loss: 6559.25, average training loss: 6816.12, base loss: 8861.12
[INFO 2017-06-26 18:11:50,004 main.py:47] epoch 3822, training loss: 6476.11, average training loss: 6815.48, base loss: 8860.05
[INFO 2017-06-26 18:11:50,363 main.py:47] epoch 3823, training loss: 6656.81, average training loss: 6814.15, base loss: 8858.40
[INFO 2017-06-26 18:11:50,723 main.py:47] epoch 3824, training loss: 6144.82, average training loss: 6812.55, base loss: 8856.48
[INFO 2017-06-26 18:11:51,083 main.py:47] epoch 3825, training loss: 6733.73, average training loss: 6813.13, base loss: 8857.85
[INFO 2017-06-26 18:11:51,443 main.py:47] epoch 3826, training loss: 6163.18, average training loss: 6813.10, base loss: 8857.57
[INFO 2017-06-26 18:11:51,803 main.py:47] epoch 3827, training loss: 6573.79, average training loss: 6812.74, base loss: 8857.42
[INFO 2017-06-26 18:11:52,162 main.py:47] epoch 3828, training loss: 6341.20, average training loss: 6812.78, base loss: 8857.21
[INFO 2017-06-26 18:11:52,521 main.py:47] epoch 3829, training loss: 6729.31, average training loss: 6812.85, base loss: 8857.42
[INFO 2017-06-26 18:11:52,911 main.py:47] epoch 3830, training loss: 6836.87, average training loss: 6811.14, base loss: 8854.70
[INFO 2017-06-26 18:11:53,273 main.py:47] epoch 3831, training loss: 6140.00, average training loss: 6811.29, base loss: 8854.90
[INFO 2017-06-26 18:11:53,636 main.py:47] epoch 3832, training loss: 6599.54, average training loss: 6810.98, base loss: 8853.74
[INFO 2017-06-26 18:11:53,997 main.py:47] epoch 3833, training loss: 6973.58, average training loss: 6810.98, base loss: 8853.91
[INFO 2017-06-26 18:11:54,357 main.py:47] epoch 3834, training loss: 6507.52, average training loss: 6810.93, base loss: 8853.68
[INFO 2017-06-26 18:11:54,716 main.py:47] epoch 3835, training loss: 6885.40, average training loss: 6810.64, base loss: 8852.69
[INFO 2017-06-26 18:11:55,075 main.py:47] epoch 3836, training loss: 5921.34, average training loss: 6809.14, base loss: 8850.85
[INFO 2017-06-26 18:11:55,434 main.py:47] epoch 3837, training loss: 7122.12, average training loss: 6809.84, base loss: 8851.69
[INFO 2017-06-26 18:11:55,794 main.py:47] epoch 3838, training loss: 7674.96, average training loss: 6810.61, base loss: 8851.98
[INFO 2017-06-26 18:11:56,150 main.py:47] epoch 3839, training loss: 5920.97, average training loss: 6809.13, base loss: 8851.10
[INFO 2017-06-26 18:11:56,510 main.py:47] epoch 3840, training loss: 7614.13, average training loss: 6810.25, base loss: 8853.34
[INFO 2017-06-26 18:11:56,869 main.py:47] epoch 3841, training loss: 7990.04, average training loss: 6811.41, base loss: 8854.17
[INFO 2017-06-26 18:11:57,227 main.py:47] epoch 3842, training loss: 8076.92, average training loss: 6812.57, base loss: 8855.29
[INFO 2017-06-26 18:11:57,586 main.py:47] epoch 3843, training loss: 8191.82, average training loss: 6814.16, base loss: 8857.48
[INFO 2017-06-26 18:11:57,945 main.py:47] epoch 3844, training loss: 6440.36, average training loss: 6814.07, base loss: 8857.47
[INFO 2017-06-26 18:11:58,305 main.py:47] epoch 3845, training loss: 6274.77, average training loss: 6813.09, base loss: 8855.59
[INFO 2017-06-26 18:11:58,665 main.py:47] epoch 3846, training loss: 7094.65, average training loss: 6813.33, base loss: 8856.43
[INFO 2017-06-26 18:11:59,024 main.py:47] epoch 3847, training loss: 6978.93, average training loss: 6813.87, base loss: 8857.07
[INFO 2017-06-26 18:11:59,384 main.py:47] epoch 3848, training loss: 7879.12, average training loss: 6814.82, base loss: 8858.44
[INFO 2017-06-26 18:11:59,744 main.py:47] epoch 3849, training loss: 6556.85, average training loss: 6813.96, base loss: 8856.43
[INFO 2017-06-26 18:12:00,136 main.py:47] epoch 3850, training loss: 6816.91, average training loss: 6812.83, base loss: 8853.88
[INFO 2017-06-26 18:12:00,516 main.py:47] epoch 3851, training loss: 6387.26, average training loss: 6811.89, base loss: 8852.18
[INFO 2017-06-26 18:12:00,879 main.py:47] epoch 3852, training loss: 6320.86, average training loss: 6812.10, base loss: 8852.49
[INFO 2017-06-26 18:12:01,241 main.py:47] epoch 3853, training loss: 6253.94, average training loss: 6811.93, base loss: 8852.71
[INFO 2017-06-26 18:12:01,601 main.py:47] epoch 3854, training loss: 6176.35, average training loss: 6811.15, base loss: 8851.56
[INFO 2017-06-26 18:12:01,960 main.py:47] epoch 3855, training loss: 8142.37, average training loss: 6812.67, base loss: 8854.27
[INFO 2017-06-26 18:12:02,319 main.py:47] epoch 3856, training loss: 6299.22, average training loss: 6812.07, base loss: 8853.82
[INFO 2017-06-26 18:12:02,680 main.py:47] epoch 3857, training loss: 6557.89, average training loss: 6811.73, base loss: 8853.27
[INFO 2017-06-26 18:12:03,042 main.py:47] epoch 3858, training loss: 6675.62, average training loss: 6812.03, base loss: 8853.19
[INFO 2017-06-26 18:12:03,403 main.py:47] epoch 3859, training loss: 8382.35, average training loss: 6813.40, base loss: 8855.57
[INFO 2017-06-26 18:12:03,763 main.py:47] epoch 3860, training loss: 7168.03, average training loss: 6813.23, base loss: 8855.48
[INFO 2017-06-26 18:12:04,120 main.py:47] epoch 3861, training loss: 6771.83, average training loss: 6812.65, base loss: 8854.88
[INFO 2017-06-26 18:12:04,479 main.py:47] epoch 3862, training loss: 6404.14, average training loss: 6812.20, base loss: 8854.27
[INFO 2017-06-26 18:12:04,843 main.py:47] epoch 3863, training loss: 6543.31, average training loss: 6812.08, base loss: 8854.90
[INFO 2017-06-26 18:12:05,204 main.py:47] epoch 3864, training loss: 7322.20, average training loss: 6813.15, base loss: 8856.90
[INFO 2017-06-26 18:12:05,566 main.py:47] epoch 3865, training loss: 6709.44, average training loss: 6813.07, base loss: 8856.74
[INFO 2017-06-26 18:12:05,925 main.py:47] epoch 3866, training loss: 6350.35, average training loss: 6813.47, base loss: 8857.10
[INFO 2017-06-26 18:12:06,284 main.py:47] epoch 3867, training loss: 6469.64, average training loss: 6812.56, base loss: 8856.40
[INFO 2017-06-26 18:12:06,645 main.py:47] epoch 3868, training loss: 6712.53, average training loss: 6812.45, base loss: 8855.92
[INFO 2017-06-26 18:12:07,020 main.py:47] epoch 3869, training loss: 7137.42, average training loss: 6812.59, base loss: 8857.12
[INFO 2017-06-26 18:12:07,380 main.py:47] epoch 3870, training loss: 7267.24, average training loss: 6812.40, base loss: 8857.67
[INFO 2017-06-26 18:12:07,738 main.py:47] epoch 3871, training loss: 6805.48, average training loss: 6812.52, base loss: 8857.55
[INFO 2017-06-26 18:12:08,097 main.py:47] epoch 3872, training loss: 6379.40, average training loss: 6812.37, base loss: 8856.68
[INFO 2017-06-26 18:12:08,455 main.py:47] epoch 3873, training loss: 6970.57, average training loss: 6812.99, base loss: 8857.22
[INFO 2017-06-26 18:12:08,814 main.py:47] epoch 3874, training loss: 7505.69, average training loss: 6812.92, base loss: 8856.35
[INFO 2017-06-26 18:12:09,170 main.py:47] epoch 3875, training loss: 7020.62, average training loss: 6814.06, base loss: 8857.19
[INFO 2017-06-26 18:12:09,529 main.py:47] epoch 3876, training loss: 6243.84, average training loss: 6813.59, base loss: 8856.26
[INFO 2017-06-26 18:12:09,888 main.py:47] epoch 3877, training loss: 6575.66, average training loss: 6812.78, base loss: 8855.71
[INFO 2017-06-26 18:12:10,249 main.py:47] epoch 3878, training loss: 6577.46, average training loss: 6812.85, base loss: 8856.65
[INFO 2017-06-26 18:12:10,605 main.py:47] epoch 3879, training loss: 6158.26, average training loss: 6812.93, base loss: 8857.23
[INFO 2017-06-26 18:12:10,964 main.py:47] epoch 3880, training loss: 6833.42, average training loss: 6813.70, base loss: 8858.68
[INFO 2017-06-26 18:12:11,325 main.py:47] epoch 3881, training loss: 6678.81, average training loss: 6813.72, base loss: 8858.87
[INFO 2017-06-26 18:12:11,684 main.py:47] epoch 3882, training loss: 7189.82, average training loss: 6814.56, base loss: 8860.85
[INFO 2017-06-26 18:12:12,044 main.py:47] epoch 3883, training loss: 6507.63, average training loss: 6814.38, base loss: 8860.78
[INFO 2017-06-26 18:12:12,402 main.py:47] epoch 3884, training loss: 6387.58, average training loss: 6813.24, base loss: 8859.29
[INFO 2017-06-26 18:12:12,793 main.py:47] epoch 3885, training loss: 6521.63, average training loss: 6813.00, base loss: 8858.79
[INFO 2017-06-26 18:12:13,157 main.py:47] epoch 3886, training loss: 7105.97, average training loss: 6812.80, base loss: 8858.37
[INFO 2017-06-26 18:12:13,533 main.py:47] epoch 3887, training loss: 6998.11, average training loss: 6813.85, base loss: 8859.67
[INFO 2017-06-26 18:12:13,928 main.py:47] epoch 3888, training loss: 6551.18, average training loss: 6812.89, base loss: 8858.44
[INFO 2017-06-26 18:12:14,329 main.py:47] epoch 3889, training loss: 6510.74, average training loss: 6812.54, base loss: 8857.60
[INFO 2017-06-26 18:12:14,735 main.py:47] epoch 3890, training loss: 7422.12, average training loss: 6813.34, base loss: 8858.54
[INFO 2017-06-26 18:12:15,115 main.py:47] epoch 3891, training loss: 6508.46, average training loss: 6813.26, base loss: 8858.23
[INFO 2017-06-26 18:12:15,479 main.py:47] epoch 3892, training loss: 6015.07, average training loss: 6812.72, base loss: 8857.03
[INFO 2017-06-26 18:12:15,874 main.py:47] epoch 3893, training loss: 7384.42, average training loss: 6813.91, base loss: 8859.45
[INFO 2017-06-26 18:12:16,251 main.py:47] epoch 3894, training loss: 7260.96, average training loss: 6813.97, base loss: 8860.09
[INFO 2017-06-26 18:12:16,613 main.py:47] epoch 3895, training loss: 6226.64, average training loss: 6813.78, base loss: 8860.25
[INFO 2017-06-26 18:12:16,975 main.py:47] epoch 3896, training loss: 7450.00, average training loss: 6814.01, base loss: 8861.08
[INFO 2017-06-26 18:12:17,337 main.py:47] epoch 3897, training loss: 6707.72, average training loss: 6814.69, base loss: 8861.75
[INFO 2017-06-26 18:12:17,694 main.py:47] epoch 3898, training loss: 7237.78, average training loss: 6816.15, base loss: 8865.24
[INFO 2017-06-26 18:12:18,054 main.py:47] epoch 3899, training loss: 6122.03, average training loss: 6815.86, base loss: 8863.68
[INFO 2017-06-26 18:12:18,054 main.py:49] epoch 3899, testing
[INFO 2017-06-26 18:12:22,230 main.py:100] average testing loss: 7237.59, base loss: 9526.33
[INFO 2017-06-26 18:12:22,256 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:12:22,614 main.py:47] epoch 3900, training loss: 6593.32, average training loss: 6816.07, base loss: 8864.09
[INFO 2017-06-26 18:12:22,973 main.py:47] epoch 3901, training loss: 7008.12, average training loss: 6815.85, base loss: 8864.12
[INFO 2017-06-26 18:12:23,333 main.py:47] epoch 3902, training loss: 6347.76, average training loss: 6816.34, base loss: 8865.58
[INFO 2017-06-26 18:12:23,692 main.py:47] epoch 3903, training loss: 6567.54, average training loss: 6815.81, base loss: 8865.08
[INFO 2017-06-26 18:12:24,053 main.py:47] epoch 3904, training loss: 6799.04, average training loss: 6815.14, base loss: 8864.00
[INFO 2017-06-26 18:12:24,410 main.py:47] epoch 3905, training loss: 5986.61, average training loss: 6814.62, base loss: 8863.26
[INFO 2017-06-26 18:12:24,802 main.py:47] epoch 3906, training loss: 6879.17, average training loss: 6814.35, base loss: 8863.38
[INFO 2017-06-26 18:12:25,187 main.py:47] epoch 3907, training loss: 6275.40, average training loss: 6813.66, base loss: 8862.33
[INFO 2017-06-26 18:12:25,552 main.py:47] epoch 3908, training loss: 6526.96, average training loss: 6814.50, base loss: 8863.84
[INFO 2017-06-26 18:12:25,918 main.py:47] epoch 3909, training loss: 6724.35, average training loss: 6814.68, base loss: 8864.54
[INFO 2017-06-26 18:12:26,324 main.py:47] epoch 3910, training loss: 6947.98, average training loss: 6815.36, base loss: 8865.80
[INFO 2017-06-26 18:12:26,716 main.py:47] epoch 3911, training loss: 5898.49, average training loss: 6814.60, base loss: 8864.31
[INFO 2017-06-26 18:12:27,084 main.py:47] epoch 3912, training loss: 7185.46, average training loss: 6815.65, base loss: 8865.42
[INFO 2017-06-26 18:12:27,483 main.py:47] epoch 3913, training loss: 6981.19, average training loss: 6816.43, base loss: 8866.57
[INFO 2017-06-26 18:12:27,867 main.py:47] epoch 3914, training loss: 7569.19, average training loss: 6815.81, base loss: 8866.08
[INFO 2017-06-26 18:12:28,230 main.py:47] epoch 3915, training loss: 6553.71, average training loss: 6815.13, base loss: 8865.20
[INFO 2017-06-26 18:12:28,594 main.py:47] epoch 3916, training loss: 6220.50, average training loss: 6813.86, base loss: 8863.82
[INFO 2017-06-26 18:12:28,952 main.py:47] epoch 3917, training loss: 6850.28, average training loss: 6813.48, base loss: 8863.63
[INFO 2017-06-26 18:12:29,313 main.py:47] epoch 3918, training loss: 7074.43, average training loss: 6813.24, base loss: 8863.74
[INFO 2017-06-26 18:12:29,708 main.py:47] epoch 3919, training loss: 6428.33, average training loss: 6813.16, base loss: 8863.35
[INFO 2017-06-26 18:12:30,078 main.py:47] epoch 3920, training loss: 6139.46, average training loss: 6812.78, base loss: 8862.65
[INFO 2017-06-26 18:12:30,439 main.py:47] epoch 3921, training loss: 7068.30, average training loss: 6812.73, base loss: 8863.09
[INFO 2017-06-26 18:12:30,835 main.py:47] epoch 3922, training loss: 7117.75, average training loss: 6812.99, base loss: 8864.76
[INFO 2017-06-26 18:12:31,209 main.py:47] epoch 3923, training loss: 6992.22, average training loss: 6812.54, base loss: 8864.58
[INFO 2017-06-26 18:12:31,571 main.py:47] epoch 3924, training loss: 7793.76, average training loss: 6813.43, base loss: 8866.93
[INFO 2017-06-26 18:12:31,930 main.py:47] epoch 3925, training loss: 6821.02, average training loss: 6813.90, base loss: 8867.65
[INFO 2017-06-26 18:12:32,290 main.py:47] epoch 3926, training loss: 6396.44, average training loss: 6814.05, base loss: 8868.23
[INFO 2017-06-26 18:12:32,651 main.py:47] epoch 3927, training loss: 6593.64, average training loss: 6814.76, base loss: 8869.77
[INFO 2017-06-26 18:12:33,010 main.py:47] epoch 3928, training loss: 6924.04, average training loss: 6814.78, base loss: 8869.19
[INFO 2017-06-26 18:12:33,385 main.py:47] epoch 3929, training loss: 6583.07, average training loss: 6814.80, base loss: 8869.43
[INFO 2017-06-26 18:12:33,747 main.py:47] epoch 3930, training loss: 8692.84, average training loss: 6816.64, base loss: 8872.60
[INFO 2017-06-26 18:12:34,108 main.py:47] epoch 3931, training loss: 7866.87, average training loss: 6817.15, base loss: 8873.06
[INFO 2017-06-26 18:12:34,487 main.py:47] epoch 3932, training loss: 6912.97, average training loss: 6817.47, base loss: 8873.64
[INFO 2017-06-26 18:12:34,847 main.py:47] epoch 3933, training loss: 6986.31, average training loss: 6817.33, base loss: 8873.94
[INFO 2017-06-26 18:12:35,206 main.py:47] epoch 3934, training loss: 6082.36, average training loss: 6816.17, base loss: 8871.57
[INFO 2017-06-26 18:12:35,566 main.py:47] epoch 3935, training loss: 7037.29, average training loss: 6816.93, base loss: 8872.76
[INFO 2017-06-26 18:12:35,924 main.py:47] epoch 3936, training loss: 6053.88, average training loss: 6816.81, base loss: 8872.27
[INFO 2017-06-26 18:12:36,284 main.py:47] epoch 3937, training loss: 6360.08, average training loss: 6815.66, base loss: 8870.83
[INFO 2017-06-26 18:12:36,642 main.py:47] epoch 3938, training loss: 6980.51, average training loss: 6814.42, base loss: 8868.82
[INFO 2017-06-26 18:12:37,000 main.py:47] epoch 3939, training loss: 6618.72, average training loss: 6814.07, base loss: 8869.03
[INFO 2017-06-26 18:12:37,360 main.py:47] epoch 3940, training loss: 6318.73, average training loss: 6812.80, base loss: 8867.63
[INFO 2017-06-26 18:12:37,756 main.py:47] epoch 3941, training loss: 7223.57, average training loss: 6812.81, base loss: 8867.84
[INFO 2017-06-26 18:12:38,142 main.py:47] epoch 3942, training loss: 7141.52, average training loss: 6813.32, base loss: 8869.07
[INFO 2017-06-26 18:12:38,550 main.py:47] epoch 3943, training loss: 6362.74, average training loss: 6812.49, base loss: 8868.57
[INFO 2017-06-26 18:12:38,984 main.py:47] epoch 3944, training loss: 7278.96, average training loss: 6812.79, base loss: 8869.45
[INFO 2017-06-26 18:12:39,348 main.py:47] epoch 3945, training loss: 6148.88, average training loss: 6812.32, base loss: 8868.51
[INFO 2017-06-26 18:12:39,718 main.py:47] epoch 3946, training loss: 6741.94, average training loss: 6810.94, base loss: 8866.81
[INFO 2017-06-26 18:12:40,081 main.py:47] epoch 3947, training loss: 7031.41, average training loss: 6811.24, base loss: 8867.15
[INFO 2017-06-26 18:12:40,442 main.py:47] epoch 3948, training loss: 5844.36, average training loss: 6810.32, base loss: 8865.80
[INFO 2017-06-26 18:12:40,803 main.py:47] epoch 3949, training loss: 6588.79, average training loss: 6810.18, base loss: 8865.39
[INFO 2017-06-26 18:12:41,164 main.py:47] epoch 3950, training loss: 7645.39, average training loss: 6810.65, base loss: 8865.55
[INFO 2017-06-26 18:12:41,524 main.py:47] epoch 3951, training loss: 6473.09, average training loss: 6810.42, base loss: 8865.03
[INFO 2017-06-26 18:12:41,882 main.py:47] epoch 3952, training loss: 7177.94, average training loss: 6810.34, base loss: 8865.24
[INFO 2017-06-26 18:12:42,243 main.py:47] epoch 3953, training loss: 7596.71, average training loss: 6809.85, base loss: 8865.17
[INFO 2017-06-26 18:12:42,603 main.py:47] epoch 3954, training loss: 6415.51, average training loss: 6809.48, base loss: 8864.36
[INFO 2017-06-26 18:12:42,964 main.py:47] epoch 3955, training loss: 6486.66, average training loss: 6809.82, base loss: 8864.29
[INFO 2017-06-26 18:12:43,325 main.py:47] epoch 3956, training loss: 6733.37, average training loss: 6810.32, base loss: 8865.54
[INFO 2017-06-26 18:12:43,701 main.py:47] epoch 3957, training loss: 6214.36, average training loss: 6810.86, base loss: 8867.26
[INFO 2017-06-26 18:12:44,064 main.py:47] epoch 3958, training loss: 6433.95, average training loss: 6810.69, base loss: 8866.63
[INFO 2017-06-26 18:12:44,428 main.py:47] epoch 3959, training loss: 7024.79, average training loss: 6811.03, base loss: 8866.93
[INFO 2017-06-26 18:12:44,790 main.py:47] epoch 3960, training loss: 5817.30, average training loss: 6810.28, base loss: 8865.01
[INFO 2017-06-26 18:12:45,149 main.py:47] epoch 3961, training loss: 6769.26, average training loss: 6810.17, base loss: 8864.62
[INFO 2017-06-26 18:12:45,511 main.py:47] epoch 3962, training loss: 6471.32, average training loss: 6809.77, base loss: 8864.01
[INFO 2017-06-26 18:12:45,870 main.py:47] epoch 3963, training loss: 5926.93, average training loss: 6808.91, base loss: 8862.90
[INFO 2017-06-26 18:12:46,231 main.py:47] epoch 3964, training loss: 6162.90, average training loss: 6808.65, base loss: 8862.29
[INFO 2017-06-26 18:12:46,589 main.py:47] epoch 3965, training loss: 7600.65, average training loss: 6809.21, base loss: 8863.17
[INFO 2017-06-26 18:12:46,946 main.py:47] epoch 3966, training loss: 6665.84, average training loss: 6809.66, base loss: 8864.17
[INFO 2017-06-26 18:12:47,305 main.py:47] epoch 3967, training loss: 6020.04, average training loss: 6808.88, base loss: 8863.26
[INFO 2017-06-26 18:12:47,664 main.py:47] epoch 3968, training loss: 6383.37, average training loss: 6807.36, base loss: 8861.57
[INFO 2017-06-26 18:12:48,022 main.py:47] epoch 3969, training loss: 7068.93, average training loss: 6807.48, base loss: 8861.89
[INFO 2017-06-26 18:12:48,381 main.py:47] epoch 3970, training loss: 7249.70, average training loss: 6806.97, base loss: 8861.20
[INFO 2017-06-26 18:12:48,739 main.py:47] epoch 3971, training loss: 6401.30, average training loss: 6807.47, base loss: 8862.19
[INFO 2017-06-26 18:12:49,098 main.py:47] epoch 3972, training loss: 6632.90, average training loss: 6806.68, base loss: 8860.83
[INFO 2017-06-26 18:12:49,456 main.py:47] epoch 3973, training loss: 6232.41, average training loss: 6805.97, base loss: 8859.19
[INFO 2017-06-26 18:12:49,814 main.py:47] epoch 3974, training loss: 5972.22, average training loss: 6805.20, base loss: 8857.42
[INFO 2017-06-26 18:12:50,172 main.py:47] epoch 3975, training loss: 6656.50, average training loss: 6803.85, base loss: 8855.56
[INFO 2017-06-26 18:12:50,532 main.py:47] epoch 3976, training loss: 7573.53, average training loss: 6804.02, base loss: 8856.27
[INFO 2017-06-26 18:12:50,890 main.py:47] epoch 3977, training loss: 7253.01, average training loss: 6804.68, base loss: 8857.42
[INFO 2017-06-26 18:12:51,249 main.py:47] epoch 3978, training loss: 6502.51, average training loss: 6804.61, base loss: 8858.67
[INFO 2017-06-26 18:12:51,607 main.py:47] epoch 3979, training loss: 6643.63, average training loss: 6803.96, base loss: 8857.90
[INFO 2017-06-26 18:12:51,966 main.py:47] epoch 3980, training loss: 7346.67, average training loss: 6804.43, base loss: 8858.24
[INFO 2017-06-26 18:12:52,323 main.py:47] epoch 3981, training loss: 5997.06, average training loss: 6803.52, base loss: 8857.59
[INFO 2017-06-26 18:12:52,681 main.py:47] epoch 3982, training loss: 7003.93, average training loss: 6803.68, base loss: 8858.58
[INFO 2017-06-26 18:12:53,040 main.py:47] epoch 3983, training loss: 6099.55, average training loss: 6803.64, base loss: 8858.66
[INFO 2017-06-26 18:12:53,399 main.py:47] epoch 3984, training loss: 6696.15, average training loss: 6803.95, base loss: 8859.05
[INFO 2017-06-26 18:12:53,758 main.py:47] epoch 3985, training loss: 6920.39, average training loss: 6803.86, base loss: 8859.28
[INFO 2017-06-26 18:12:54,117 main.py:47] epoch 3986, training loss: 7925.49, average training loss: 6805.20, base loss: 8861.14
[INFO 2017-06-26 18:12:54,479 main.py:47] epoch 3987, training loss: 6786.44, average training loss: 6805.46, base loss: 8861.66
[INFO 2017-06-26 18:12:54,837 main.py:47] epoch 3988, training loss: 7807.58, average training loss: 6806.73, base loss: 8864.24
[INFO 2017-06-26 18:12:55,197 main.py:47] epoch 3989, training loss: 6962.87, average training loss: 6806.62, base loss: 8863.62
[INFO 2017-06-26 18:12:55,556 main.py:47] epoch 3990, training loss: 6865.37, average training loss: 6806.39, base loss: 8863.29
[INFO 2017-06-26 18:12:55,914 main.py:47] epoch 3991, training loss: 6763.19, average training loss: 6805.77, base loss: 8862.31
[INFO 2017-06-26 18:12:56,272 main.py:47] epoch 3992, training loss: 6688.50, average training loss: 6805.26, base loss: 8861.73
[INFO 2017-06-26 18:12:56,631 main.py:47] epoch 3993, training loss: 7112.94, average training loss: 6806.38, base loss: 8864.76
[INFO 2017-06-26 18:12:56,988 main.py:47] epoch 3994, training loss: 6839.48, average training loss: 6806.46, base loss: 8865.06
[INFO 2017-06-26 18:12:57,346 main.py:47] epoch 3995, training loss: 6689.84, average training loss: 6806.77, base loss: 8865.54
[INFO 2017-06-26 18:12:57,705 main.py:47] epoch 3996, training loss: 6510.91, average training loss: 6806.98, base loss: 8866.21
[INFO 2017-06-26 18:12:58,064 main.py:47] epoch 3997, training loss: 6729.10, average training loss: 6806.28, base loss: 8864.51
[INFO 2017-06-26 18:12:58,422 main.py:47] epoch 3998, training loss: 6615.66, average training loss: 6806.35, base loss: 8863.90
[INFO 2017-06-26 18:12:58,783 main.py:47] epoch 3999, training loss: 7312.22, average training loss: 6806.56, base loss: 8865.05
[INFO 2017-06-26 18:12:58,783 main.py:49] epoch 3999, testing
[INFO 2017-06-26 18:13:02,986 main.py:100] average testing loss: 6793.56, base loss: 9028.69
[INFO 2017-06-26 18:13:03,012 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:13:03,372 main.py:47] epoch 4000, training loss: 7218.48, average training loss: 6807.18, base loss: 8865.32
[INFO 2017-06-26 18:13:03,732 main.py:47] epoch 4001, training loss: 7100.92, average training loss: 6808.29, base loss: 8867.13
[INFO 2017-06-26 18:13:04,091 main.py:47] epoch 4002, training loss: 6261.27, average training loss: 6807.85, base loss: 8866.37
[INFO 2017-06-26 18:13:04,450 main.py:47] epoch 4003, training loss: 6750.52, average training loss: 6807.87, base loss: 8866.80
[INFO 2017-06-26 18:13:04,809 main.py:47] epoch 4004, training loss: 6366.71, average training loss: 6806.91, base loss: 8865.98
[INFO 2017-06-26 18:13:05,169 main.py:47] epoch 4005, training loss: 8145.64, average training loss: 6808.01, base loss: 8867.60
[INFO 2017-06-26 18:13:05,528 main.py:47] epoch 4006, training loss: 6306.63, average training loss: 6807.04, base loss: 8866.57
[INFO 2017-06-26 18:13:05,886 main.py:47] epoch 4007, training loss: 6910.93, average training loss: 6806.33, base loss: 8865.32
[INFO 2017-06-26 18:13:06,244 main.py:47] epoch 4008, training loss: 5934.46, average training loss: 6806.15, base loss: 8864.95
[INFO 2017-06-26 18:13:06,604 main.py:47] epoch 4009, training loss: 7311.29, average training loss: 6807.40, base loss: 8867.08
[INFO 2017-06-26 18:13:06,965 main.py:47] epoch 4010, training loss: 7010.48, average training loss: 6806.69, base loss: 8866.45
[INFO 2017-06-26 18:13:07,331 main.py:47] epoch 4011, training loss: 6906.86, average training loss: 6806.85, base loss: 8867.68
[INFO 2017-06-26 18:13:07,694 main.py:47] epoch 4012, training loss: 6585.75, average training loss: 6807.21, base loss: 8868.47
[INFO 2017-06-26 18:13:08,054 main.py:47] epoch 4013, training loss: 6756.45, average training loss: 6807.16, base loss: 8869.13
[INFO 2017-06-26 18:13:08,413 main.py:47] epoch 4014, training loss: 6267.04, average training loss: 6806.49, base loss: 8868.48
[INFO 2017-06-26 18:13:08,772 main.py:47] epoch 4015, training loss: 6760.30, average training loss: 6806.53, base loss: 8869.40
[INFO 2017-06-26 18:13:09,131 main.py:47] epoch 4016, training loss: 6971.91, average training loss: 6806.29, base loss: 8869.04
[INFO 2017-06-26 18:13:09,489 main.py:47] epoch 4017, training loss: 7188.72, average training loss: 6806.17, base loss: 8869.86
[INFO 2017-06-26 18:13:09,849 main.py:47] epoch 4018, training loss: 6299.19, average training loss: 6805.67, base loss: 8869.28
[INFO 2017-06-26 18:13:10,208 main.py:47] epoch 4019, training loss: 6880.87, average training loss: 6806.19, base loss: 8870.46
[INFO 2017-06-26 18:13:10,567 main.py:47] epoch 4020, training loss: 6050.37, average training loss: 6805.15, base loss: 8869.49
[INFO 2017-06-26 18:13:10,928 main.py:47] epoch 4021, training loss: 6307.11, average training loss: 6804.91, base loss: 8869.20
[INFO 2017-06-26 18:13:11,287 main.py:47] epoch 4022, training loss: 6835.89, average training loss: 6805.42, base loss: 8870.13
[INFO 2017-06-26 18:13:11,648 main.py:47] epoch 4023, training loss: 7974.95, average training loss: 6806.02, base loss: 8871.40
[INFO 2017-06-26 18:13:12,008 main.py:47] epoch 4024, training loss: 6325.10, average training loss: 6806.26, base loss: 8872.20
[INFO 2017-06-26 18:13:12,396 main.py:47] epoch 4025, training loss: 5881.38, average training loss: 6805.26, base loss: 8871.36
[INFO 2017-06-26 18:13:12,773 main.py:47] epoch 4026, training loss: 6450.38, average training loss: 6805.23, base loss: 8870.93
[INFO 2017-06-26 18:13:13,143 main.py:47] epoch 4027, training loss: 6482.53, average training loss: 6804.65, base loss: 8869.90
[INFO 2017-06-26 18:13:13,503 main.py:47] epoch 4028, training loss: 6901.38, average training loss: 6803.96, base loss: 8869.05
[INFO 2017-06-26 18:13:13,860 main.py:47] epoch 4029, training loss: 6984.31, average training loss: 6804.32, base loss: 8869.43
[INFO 2017-06-26 18:13:14,219 main.py:47] epoch 4030, training loss: 6295.25, average training loss: 6803.71, base loss: 8869.41
[INFO 2017-06-26 18:13:14,577 main.py:47] epoch 4031, training loss: 6997.51, average training loss: 6803.03, base loss: 8868.27
[INFO 2017-06-26 18:13:14,937 main.py:47] epoch 4032, training loss: 7078.71, average training loss: 6802.30, base loss: 8867.36
[INFO 2017-06-26 18:13:15,296 main.py:47] epoch 4033, training loss: 6299.13, average training loss: 6802.18, base loss: 8868.02
[INFO 2017-06-26 18:13:15,656 main.py:47] epoch 4034, training loss: 7054.46, average training loss: 6802.33, base loss: 8868.73
[INFO 2017-06-26 18:13:16,014 main.py:47] epoch 4035, training loss: 7057.47, average training loss: 6803.47, base loss: 8870.67
[INFO 2017-06-26 18:13:16,372 main.py:47] epoch 4036, training loss: 7270.90, average training loss: 6802.70, base loss: 8870.23
[INFO 2017-06-26 18:13:16,730 main.py:47] epoch 4037, training loss: 6902.38, average training loss: 6803.16, base loss: 8871.47
[INFO 2017-06-26 18:13:17,088 main.py:47] epoch 4038, training loss: 5999.43, average training loss: 6801.97, base loss: 8869.29
[INFO 2017-06-26 18:13:17,448 main.py:47] epoch 4039, training loss: 5980.53, average training loss: 6801.53, base loss: 8868.75
[INFO 2017-06-26 18:13:17,816 main.py:47] epoch 4040, training loss: 6548.15, average training loss: 6801.29, base loss: 8869.43
[INFO 2017-06-26 18:13:18,173 main.py:47] epoch 4041, training loss: 7145.12, average training loss: 6800.64, base loss: 8868.48
[INFO 2017-06-26 18:13:18,545 main.py:47] epoch 4042, training loss: 6318.00, average training loss: 6799.61, base loss: 8867.08
[INFO 2017-06-26 18:13:18,917 main.py:47] epoch 4043, training loss: 6887.80, average training loss: 6799.77, base loss: 8868.08
[INFO 2017-06-26 18:13:19,284 main.py:47] epoch 4044, training loss: 6727.10, average training loss: 6799.08, base loss: 8867.07
[INFO 2017-06-26 18:13:19,645 main.py:47] epoch 4045, training loss: 7098.36, average training loss: 6799.72, base loss: 8868.55
[INFO 2017-06-26 18:13:20,040 main.py:47] epoch 4046, training loss: 7136.08, average training loss: 6800.14, base loss: 8869.64
[INFO 2017-06-26 18:13:20,405 main.py:47] epoch 4047, training loss: 7024.96, average training loss: 6799.80, base loss: 8869.29
[INFO 2017-06-26 18:13:20,766 main.py:47] epoch 4048, training loss: 6376.28, average training loss: 6799.28, base loss: 8868.35
[INFO 2017-06-26 18:13:21,160 main.py:47] epoch 4049, training loss: 6021.16, average training loss: 6798.72, base loss: 8867.79
[INFO 2017-06-26 18:13:21,579 main.py:47] epoch 4050, training loss: 6370.45, average training loss: 6798.77, base loss: 8867.86
[INFO 2017-06-26 18:13:21,959 main.py:47] epoch 4051, training loss: 6295.84, average training loss: 6797.50, base loss: 8865.90
[INFO 2017-06-26 18:13:22,353 main.py:47] epoch 4052, training loss: 6638.31, average training loss: 6798.04, base loss: 8866.42
[INFO 2017-06-26 18:13:22,734 main.py:47] epoch 4053, training loss: 6904.42, average training loss: 6798.28, base loss: 8866.60
[INFO 2017-06-26 18:13:23,102 main.py:47] epoch 4054, training loss: 6902.31, average training loss: 6798.94, base loss: 8867.73
[INFO 2017-06-26 18:13:23,476 main.py:47] epoch 4055, training loss: 6367.58, average training loss: 6799.39, base loss: 8868.46
[INFO 2017-06-26 18:13:23,837 main.py:47] epoch 4056, training loss: 6431.82, average training loss: 6798.17, base loss: 8867.17
[INFO 2017-06-26 18:13:24,197 main.py:47] epoch 4057, training loss: 6808.64, average training loss: 6798.10, base loss: 8866.85
[INFO 2017-06-26 18:13:24,557 main.py:47] epoch 4058, training loss: 6678.89, average training loss: 6796.92, base loss: 8865.10
[INFO 2017-06-26 18:13:24,940 main.py:47] epoch 4059, training loss: 6214.11, average training loss: 6796.24, base loss: 8863.70
[INFO 2017-06-26 18:13:25,302 main.py:47] epoch 4060, training loss: 6557.27, average training loss: 6795.88, base loss: 8863.22
[INFO 2017-06-26 18:13:25,663 main.py:47] epoch 4061, training loss: 6861.98, average training loss: 6794.49, base loss: 8861.63
[INFO 2017-06-26 18:13:26,057 main.py:47] epoch 4062, training loss: 6701.35, average training loss: 6794.71, base loss: 8862.23
[INFO 2017-06-26 18:13:26,499 main.py:47] epoch 4063, training loss: 6349.96, average training loss: 6793.61, base loss: 8861.25
[INFO 2017-06-26 18:13:26,862 main.py:47] epoch 4064, training loss: 7945.38, average training loss: 6795.34, base loss: 8863.80
[INFO 2017-06-26 18:13:27,224 main.py:47] epoch 4065, training loss: 6829.17, average training loss: 6795.43, base loss: 8864.40
[INFO 2017-06-26 18:13:27,585 main.py:47] epoch 4066, training loss: 7559.17, average training loss: 6796.22, base loss: 8866.63
[INFO 2017-06-26 18:13:27,947 main.py:47] epoch 4067, training loss: 6642.42, average training loss: 6796.36, base loss: 8866.99
[INFO 2017-06-26 18:13:28,309 main.py:47] epoch 4068, training loss: 6636.69, average training loss: 6795.29, base loss: 8865.91
[INFO 2017-06-26 18:13:28,667 main.py:47] epoch 4069, training loss: 6702.39, average training loss: 6795.04, base loss: 8866.14
[INFO 2017-06-26 18:13:29,027 main.py:47] epoch 4070, training loss: 6646.66, average training loss: 6794.68, base loss: 8865.99
[INFO 2017-06-26 18:13:29,471 main.py:47] epoch 4071, training loss: 6956.38, average training loss: 6795.24, base loss: 8866.84
[INFO 2017-06-26 18:13:29,833 main.py:47] epoch 4072, training loss: 6886.15, average training loss: 6795.46, base loss: 8868.43
[INFO 2017-06-26 18:13:30,191 main.py:47] epoch 4073, training loss: 7007.96, average training loss: 6795.23, base loss: 8868.35
[INFO 2017-06-26 18:13:30,553 main.py:47] epoch 4074, training loss: 6298.00, average training loss: 6794.27, base loss: 8867.34
[INFO 2017-06-26 18:13:30,912 main.py:47] epoch 4075, training loss: 6117.05, average training loss: 6793.32, base loss: 8866.17
[INFO 2017-06-26 18:13:31,274 main.py:47] epoch 4076, training loss: 5639.73, average training loss: 6792.97, base loss: 8865.66
[INFO 2017-06-26 18:13:31,634 main.py:47] epoch 4077, training loss: 7186.65, average training loss: 6792.74, base loss: 8866.21
[INFO 2017-06-26 18:13:31,994 main.py:47] epoch 4078, training loss: 7493.61, average training loss: 6792.76, base loss: 8865.65
[INFO 2017-06-26 18:13:32,355 main.py:47] epoch 4079, training loss: 6736.83, average training loss: 6793.25, base loss: 8866.55
[INFO 2017-06-26 18:13:32,716 main.py:47] epoch 4080, training loss: 7220.15, average training loss: 6793.34, base loss: 8867.28
[INFO 2017-06-26 18:13:33,077 main.py:47] epoch 4081, training loss: 6331.50, average training loss: 6792.82, base loss: 8866.55
[INFO 2017-06-26 18:13:33,436 main.py:47] epoch 4082, training loss: 7578.63, average training loss: 6793.62, base loss: 8867.82
[INFO 2017-06-26 18:13:33,796 main.py:47] epoch 4083, training loss: 6835.41, average training loss: 6793.72, base loss: 8868.63
[INFO 2017-06-26 18:13:34,155 main.py:47] epoch 4084, training loss: 6233.81, average training loss: 6793.35, base loss: 8868.19
[INFO 2017-06-26 18:13:34,514 main.py:47] epoch 4085, training loss: 7151.78, average training loss: 6793.72, base loss: 8869.61
[INFO 2017-06-26 18:13:34,875 main.py:47] epoch 4086, training loss: 7797.96, average training loss: 6795.11, base loss: 8871.71
[INFO 2017-06-26 18:13:35,235 main.py:47] epoch 4087, training loss: 6622.78, average training loss: 6795.31, base loss: 8872.52
[INFO 2017-06-26 18:13:35,594 main.py:47] epoch 4088, training loss: 6947.50, average training loss: 6795.46, base loss: 8872.13
[INFO 2017-06-26 18:13:35,953 main.py:47] epoch 4089, training loss: 6767.07, average training loss: 6795.99, base loss: 8873.41
[INFO 2017-06-26 18:13:36,312 main.py:47] epoch 4090, training loss: 7304.43, average training loss: 6795.29, base loss: 8872.54
[INFO 2017-06-26 18:13:36,674 main.py:47] epoch 4091, training loss: 6300.61, average training loss: 6794.19, base loss: 8871.83
[INFO 2017-06-26 18:13:37,035 main.py:47] epoch 4092, training loss: 6220.25, average training loss: 6794.06, base loss: 8870.99
[INFO 2017-06-26 18:13:37,396 main.py:47] epoch 4093, training loss: 6643.98, average training loss: 6794.24, base loss: 8871.01
[INFO 2017-06-26 18:13:37,755 main.py:47] epoch 4094, training loss: 6342.79, average training loss: 6793.74, base loss: 8870.22
[INFO 2017-06-26 18:13:38,117 main.py:47] epoch 4095, training loss: 7129.16, average training loss: 6793.92, base loss: 8870.45
[INFO 2017-06-26 18:13:38,477 main.py:47] epoch 4096, training loss: 7216.91, average training loss: 6794.56, base loss: 8871.03
[INFO 2017-06-26 18:13:38,836 main.py:47] epoch 4097, training loss: 6947.64, average training loss: 6795.21, base loss: 8872.88
[INFO 2017-06-26 18:13:39,196 main.py:47] epoch 4098, training loss: 6776.93, average training loss: 6794.59, base loss: 8872.73
[INFO 2017-06-26 18:13:39,556 main.py:47] epoch 4099, training loss: 6431.80, average training loss: 6794.51, base loss: 8872.91
[INFO 2017-06-26 18:13:39,556 main.py:49] epoch 4099, testing
[INFO 2017-06-26 18:13:43,713 main.py:100] average testing loss: 7060.04, base loss: 9244.41
[INFO 2017-06-26 18:13:43,738 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:13:44,097 main.py:47] epoch 4100, training loss: 7815.79, average training loss: 6796.16, base loss: 8876.04
[INFO 2017-06-26 18:13:44,458 main.py:47] epoch 4101, training loss: 6949.86, average training loss: 6796.97, base loss: 8876.58
[INFO 2017-06-26 18:13:44,818 main.py:47] epoch 4102, training loss: 6856.79, average training loss: 6796.57, base loss: 8876.11
[INFO 2017-06-26 18:13:45,179 main.py:47] epoch 4103, training loss: 6386.84, average training loss: 6796.38, base loss: 8876.15
[INFO 2017-06-26 18:13:45,539 main.py:47] epoch 4104, training loss: 7490.69, average training loss: 6797.10, base loss: 8876.94
[INFO 2017-06-26 18:13:45,899 main.py:47] epoch 4105, training loss: 8179.48, average training loss: 6799.01, base loss: 8879.35
[INFO 2017-06-26 18:13:46,258 main.py:47] epoch 4106, training loss: 7363.18, average training loss: 6799.83, base loss: 8881.10
[INFO 2017-06-26 18:13:46,618 main.py:47] epoch 4107, training loss: 7114.64, average training loss: 6799.01, base loss: 8880.35
[INFO 2017-06-26 18:13:46,977 main.py:47] epoch 4108, training loss: 7036.86, average training loss: 6799.21, base loss: 8880.69
[INFO 2017-06-26 18:13:47,336 main.py:47] epoch 4109, training loss: 6779.38, average training loss: 6799.52, base loss: 8879.60
[INFO 2017-06-26 18:13:47,693 main.py:47] epoch 4110, training loss: 7601.78, average training loss: 6800.27, base loss: 8879.75
[INFO 2017-06-26 18:13:48,053 main.py:47] epoch 4111, training loss: 7033.21, average training loss: 6800.69, base loss: 8880.12
[INFO 2017-06-26 18:13:48,412 main.py:47] epoch 4112, training loss: 6934.35, average training loss: 6800.34, base loss: 8878.64
[INFO 2017-06-26 18:13:48,770 main.py:47] epoch 4113, training loss: 6203.91, average training loss: 6799.21, base loss: 8877.37
[INFO 2017-06-26 18:13:49,131 main.py:47] epoch 4114, training loss: 6467.48, average training loss: 6799.60, base loss: 8877.93
[INFO 2017-06-26 18:13:49,490 main.py:47] epoch 4115, training loss: 7099.91, average training loss: 6800.83, base loss: 8880.19
[INFO 2017-06-26 18:13:49,850 main.py:47] epoch 4116, training loss: 6699.46, average training loss: 6800.43, base loss: 8879.89
[INFO 2017-06-26 18:13:50,209 main.py:47] epoch 4117, training loss: 7874.09, average training loss: 6801.35, base loss: 8881.90
[INFO 2017-06-26 18:13:50,568 main.py:47] epoch 4118, training loss: 7371.82, average training loss: 6801.60, base loss: 8881.65
[INFO 2017-06-26 18:13:50,928 main.py:47] epoch 4119, training loss: 6549.58, average training loss: 6800.39, base loss: 8879.83
[INFO 2017-06-26 18:13:51,288 main.py:47] epoch 4120, training loss: 7377.42, average training loss: 6800.77, base loss: 8880.45
[INFO 2017-06-26 18:13:51,647 main.py:47] epoch 4121, training loss: 6056.73, average training loss: 6800.06, base loss: 8879.66
[INFO 2017-06-26 18:13:52,008 main.py:47] epoch 4122, training loss: 6985.67, average training loss: 6799.97, base loss: 8879.18
[INFO 2017-06-26 18:13:52,368 main.py:47] epoch 4123, training loss: 5925.02, average training loss: 6798.91, base loss: 8878.32
[INFO 2017-06-26 18:13:52,725 main.py:47] epoch 4124, training loss: 7444.58, average training loss: 6799.33, base loss: 8878.91
[INFO 2017-06-26 18:13:53,087 main.py:47] epoch 4125, training loss: 7360.71, average training loss: 6800.16, base loss: 8880.65
[INFO 2017-06-26 18:13:53,447 main.py:47] epoch 4126, training loss: 6918.32, average training loss: 6800.14, base loss: 8880.87
[INFO 2017-06-26 18:13:53,808 main.py:47] epoch 4127, training loss: 6906.25, average training loss: 6800.97, base loss: 8881.26
[INFO 2017-06-26 18:13:54,167 main.py:47] epoch 4128, training loss: 6383.96, average training loss: 6799.12, base loss: 8878.47
[INFO 2017-06-26 18:13:54,526 main.py:47] epoch 4129, training loss: 6995.09, average training loss: 6799.92, base loss: 8880.08
[INFO 2017-06-26 18:13:54,883 main.py:47] epoch 4130, training loss: 6896.51, average training loss: 6799.56, base loss: 8879.24
[INFO 2017-06-26 18:13:55,243 main.py:47] epoch 4131, training loss: 7037.89, average training loss: 6800.77, base loss: 8881.48
[INFO 2017-06-26 18:13:55,604 main.py:47] epoch 4132, training loss: 6868.07, average training loss: 6800.70, base loss: 8881.27
[INFO 2017-06-26 18:13:55,962 main.py:47] epoch 4133, training loss: 6939.60, average training loss: 6801.15, base loss: 8881.94
[INFO 2017-06-26 18:13:56,321 main.py:47] epoch 4134, training loss: 6520.39, average training loss: 6800.50, base loss: 8881.52
[INFO 2017-06-26 18:13:56,679 main.py:47] epoch 4135, training loss: 6240.09, average training loss: 6800.50, base loss: 8881.66
[INFO 2017-06-26 18:13:57,038 main.py:47] epoch 4136, training loss: 7263.43, average training loss: 6800.72, base loss: 8882.34
[INFO 2017-06-26 18:13:57,399 main.py:47] epoch 4137, training loss: 6557.68, average training loss: 6800.47, base loss: 8881.72
[INFO 2017-06-26 18:13:57,755 main.py:47] epoch 4138, training loss: 7119.73, average training loss: 6800.12, base loss: 8881.73
[INFO 2017-06-26 18:13:58,115 main.py:47] epoch 4139, training loss: 6561.71, average training loss: 6799.30, base loss: 8880.63
[INFO 2017-06-26 18:13:58,474 main.py:47] epoch 4140, training loss: 6674.25, average training loss: 6798.22, base loss: 8878.81
[INFO 2017-06-26 18:13:58,831 main.py:47] epoch 4141, training loss: 6467.29, average training loss: 6797.84, base loss: 8877.81
[INFO 2017-06-26 18:13:59,191 main.py:47] epoch 4142, training loss: 7377.14, average training loss: 6798.86, base loss: 8879.09
[INFO 2017-06-26 18:13:59,550 main.py:47] epoch 4143, training loss: 6252.32, average training loss: 6798.64, base loss: 8878.79
[INFO 2017-06-26 18:13:59,909 main.py:47] epoch 4144, training loss: 7435.22, average training loss: 6799.38, base loss: 8880.07
[INFO 2017-06-26 18:14:00,266 main.py:47] epoch 4145, training loss: 7081.71, average training loss: 6799.64, base loss: 8880.34
[INFO 2017-06-26 18:14:00,625 main.py:47] epoch 4146, training loss: 6033.93, average training loss: 6797.71, base loss: 8876.96
[INFO 2017-06-26 18:14:00,984 main.py:47] epoch 4147, training loss: 7409.99, average training loss: 6797.60, base loss: 8876.70
[INFO 2017-06-26 18:14:01,343 main.py:47] epoch 4148, training loss: 6568.64, average training loss: 6797.28, base loss: 8875.95
[INFO 2017-06-26 18:14:01,701 main.py:47] epoch 4149, training loss: 7287.87, average training loss: 6798.25, base loss: 8877.30
[INFO 2017-06-26 18:14:02,060 main.py:47] epoch 4150, training loss: 6404.93, average training loss: 6797.41, base loss: 8876.45
[INFO 2017-06-26 18:14:02,418 main.py:47] epoch 4151, training loss: 7482.25, average training loss: 6798.38, base loss: 8878.44
[INFO 2017-06-26 18:14:02,779 main.py:47] epoch 4152, training loss: 7590.43, average training loss: 6799.23, base loss: 8879.03
[INFO 2017-06-26 18:14:03,139 main.py:47] epoch 4153, training loss: 5846.02, average training loss: 6798.36, base loss: 8876.94
[INFO 2017-06-26 18:14:03,497 main.py:47] epoch 4154, training loss: 6877.43, average training loss: 6798.92, base loss: 8877.25
[INFO 2017-06-26 18:14:03,855 main.py:47] epoch 4155, training loss: 6473.05, average training loss: 6798.11, base loss: 8875.87
[INFO 2017-06-26 18:14:04,215 main.py:47] epoch 4156, training loss: 6518.61, average training loss: 6797.00, base loss: 8875.29
[INFO 2017-06-26 18:14:04,579 main.py:47] epoch 4157, training loss: 6922.32, average training loss: 6797.36, base loss: 8875.88
[INFO 2017-06-26 18:14:04,940 main.py:47] epoch 4158, training loss: 6390.62, average training loss: 6797.16, base loss: 8875.45
[INFO 2017-06-26 18:14:05,300 main.py:47] epoch 4159, training loss: 5928.18, average training loss: 6796.55, base loss: 8873.75
[INFO 2017-06-26 18:14:05,659 main.py:47] epoch 4160, training loss: 6144.67, average training loss: 6795.42, base loss: 8872.36
[INFO 2017-06-26 18:14:06,019 main.py:47] epoch 4161, training loss: 8121.15, average training loss: 6796.47, base loss: 8873.47
[INFO 2017-06-26 18:14:06,381 main.py:47] epoch 4162, training loss: 6913.31, average training loss: 6797.29, base loss: 8874.84
[INFO 2017-06-26 18:14:06,740 main.py:47] epoch 4163, training loss: 7508.64, average training loss: 6798.01, base loss: 8875.78
[INFO 2017-06-26 18:14:07,099 main.py:47] epoch 4164, training loss: 7472.78, average training loss: 6798.98, base loss: 8876.99
[INFO 2017-06-26 18:14:07,458 main.py:47] epoch 4165, training loss: 7326.68, average training loss: 6799.80, base loss: 8877.38
[INFO 2017-06-26 18:14:07,816 main.py:47] epoch 4166, training loss: 6562.21, average training loss: 6799.38, base loss: 8876.58
[INFO 2017-06-26 18:14:08,176 main.py:47] epoch 4167, training loss: 6499.39, average training loss: 6798.12, base loss: 8875.02
[INFO 2017-06-26 18:14:08,535 main.py:47] epoch 4168, training loss: 7138.39, average training loss: 6799.13, base loss: 8876.25
[INFO 2017-06-26 18:14:08,894 main.py:47] epoch 4169, training loss: 6905.52, average training loss: 6799.70, base loss: 8876.34
[INFO 2017-06-26 18:14:09,254 main.py:47] epoch 4170, training loss: 5913.65, average training loss: 6798.45, base loss: 8874.09
[INFO 2017-06-26 18:14:09,613 main.py:47] epoch 4171, training loss: 6089.10, average training loss: 6797.21, base loss: 8873.18
[INFO 2017-06-26 18:14:09,973 main.py:47] epoch 4172, training loss: 6909.04, average training loss: 6796.86, base loss: 8871.95
[INFO 2017-06-26 18:14:10,331 main.py:47] epoch 4173, training loss: 6222.49, average training loss: 6796.25, base loss: 8871.01
[INFO 2017-06-26 18:14:10,691 main.py:47] epoch 4174, training loss: 6440.91, average training loss: 6795.60, base loss: 8869.68
[INFO 2017-06-26 18:14:11,050 main.py:47] epoch 4175, training loss: 6232.43, average training loss: 6795.49, base loss: 8869.56
[INFO 2017-06-26 18:14:11,408 main.py:47] epoch 4176, training loss: 6174.60, average training loss: 6795.17, base loss: 8869.09
[INFO 2017-06-26 18:14:11,767 main.py:47] epoch 4177, training loss: 7074.00, average training loss: 6794.77, base loss: 8869.16
[INFO 2017-06-26 18:14:12,126 main.py:47] epoch 4178, training loss: 6552.20, average training loss: 6794.95, base loss: 8868.74
[INFO 2017-06-26 18:14:12,486 main.py:47] epoch 4179, training loss: 6354.27, average training loss: 6794.90, base loss: 8869.24
[INFO 2017-06-26 18:14:12,843 main.py:47] epoch 4180, training loss: 7044.67, average training loss: 6795.12, base loss: 8869.79
[INFO 2017-06-26 18:14:13,203 main.py:47] epoch 4181, training loss: 6600.33, average training loss: 6794.46, base loss: 8868.36
[INFO 2017-06-26 18:14:13,560 main.py:47] epoch 4182, training loss: 6734.87, average training loss: 6794.29, base loss: 8868.56
[INFO 2017-06-26 18:14:13,919 main.py:47] epoch 4183, training loss: 6930.27, average training loss: 6794.68, base loss: 8868.92
[INFO 2017-06-26 18:14:14,278 main.py:47] epoch 4184, training loss: 6911.98, average training loss: 6794.74, base loss: 8868.55
[INFO 2017-06-26 18:14:14,637 main.py:47] epoch 4185, training loss: 6930.52, average training loss: 6795.15, base loss: 8869.17
[INFO 2017-06-26 18:14:14,994 main.py:47] epoch 4186, training loss: 6726.26, average training loss: 6795.03, base loss: 8869.36
[INFO 2017-06-26 18:14:15,351 main.py:47] epoch 4187, training loss: 7223.35, average training loss: 6795.10, base loss: 8869.39
[INFO 2017-06-26 18:14:15,709 main.py:47] epoch 4188, training loss: 6929.69, average training loss: 6795.75, base loss: 8870.52
[INFO 2017-06-26 18:14:16,070 main.py:47] epoch 4189, training loss: 6120.59, average training loss: 6795.55, base loss: 8870.48
[INFO 2017-06-26 18:14:16,430 main.py:47] epoch 4190, training loss: 6882.08, average training loss: 6794.46, base loss: 8869.40
[INFO 2017-06-26 18:14:16,790 main.py:47] epoch 4191, training loss: 6585.75, average training loss: 6793.48, base loss: 8867.60
[INFO 2017-06-26 18:14:17,150 main.py:47] epoch 4192, training loss: 6831.24, average training loss: 6793.86, base loss: 8867.99
[INFO 2017-06-26 18:14:17,510 main.py:47] epoch 4193, training loss: 8063.36, average training loss: 6795.40, base loss: 8870.33
[INFO 2017-06-26 18:14:17,870 main.py:47] epoch 4194, training loss: 6836.90, average training loss: 6795.51, base loss: 8870.80
[INFO 2017-06-26 18:14:18,230 main.py:47] epoch 4195, training loss: 7773.58, average training loss: 6797.01, base loss: 8872.83
[INFO 2017-06-26 18:14:18,588 main.py:47] epoch 4196, training loss: 7192.34, average training loss: 6797.60, base loss: 8874.34
[INFO 2017-06-26 18:14:18,948 main.py:47] epoch 4197, training loss: 6939.74, average training loss: 6797.70, base loss: 8874.51
[INFO 2017-06-26 18:14:19,360 main.py:47] epoch 4198, training loss: 6838.25, average training loss: 6798.65, base loss: 8876.21
[INFO 2017-06-26 18:14:19,721 main.py:47] epoch 4199, training loss: 6147.58, average training loss: 6797.99, base loss: 8874.56
[INFO 2017-06-26 18:14:19,721 main.py:49] epoch 4199, testing
[INFO 2017-06-26 18:14:23,873 main.py:100] average testing loss: 6970.64, base loss: 9066.38
[INFO 2017-06-26 18:14:23,896 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:14:24,256 main.py:47] epoch 4200, training loss: 7096.52, average training loss: 6799.04, base loss: 8875.67
[INFO 2017-06-26 18:14:24,616 main.py:47] epoch 4201, training loss: 7484.17, average training loss: 6800.34, base loss: 8877.68
[INFO 2017-06-26 18:14:24,977 main.py:47] epoch 4202, training loss: 6660.35, average training loss: 6800.63, base loss: 8877.55
[INFO 2017-06-26 18:14:25,374 main.py:47] epoch 4203, training loss: 6334.14, average training loss: 6800.19, base loss: 8877.35
[INFO 2017-06-26 18:14:25,734 main.py:47] epoch 4204, training loss: 6097.72, average training loss: 6799.47, base loss: 8876.33
[INFO 2017-06-26 18:14:26,103 main.py:47] epoch 4205, training loss: 6636.39, average training loss: 6798.35, base loss: 8874.84
[INFO 2017-06-26 18:14:26,465 main.py:47] epoch 4206, training loss: 6852.05, average training loss: 6798.72, base loss: 8875.28
[INFO 2017-06-26 18:14:26,824 main.py:47] epoch 4207, training loss: 6262.08, average training loss: 6797.76, base loss: 8874.47
[INFO 2017-06-26 18:14:27,186 main.py:47] epoch 4208, training loss: 5821.85, average training loss: 6796.59, base loss: 8872.90
[INFO 2017-06-26 18:14:27,548 main.py:47] epoch 4209, training loss: 6873.59, average training loss: 6797.03, base loss: 8873.88
[INFO 2017-06-26 18:14:27,913 main.py:47] epoch 4210, training loss: 6056.67, average training loss: 6796.04, base loss: 8871.89
[INFO 2017-06-26 18:14:28,272 main.py:47] epoch 4211, training loss: 6766.85, average training loss: 6795.18, base loss: 8869.88
[INFO 2017-06-26 18:14:28,637 main.py:47] epoch 4212, training loss: 6074.91, average training loss: 6794.34, base loss: 8868.06
[INFO 2017-06-26 18:14:28,998 main.py:47] epoch 4213, training loss: 5805.68, average training loss: 6793.06, base loss: 8866.37
[INFO 2017-06-26 18:14:29,356 main.py:47] epoch 4214, training loss: 6459.38, average training loss: 6792.26, base loss: 8865.32
[INFO 2017-06-26 18:14:29,716 main.py:47] epoch 4215, training loss: 6915.25, average training loss: 6791.50, base loss: 8864.63
[INFO 2017-06-26 18:14:30,077 main.py:47] epoch 4216, training loss: 6267.40, average training loss: 6790.78, base loss: 8863.76
[INFO 2017-06-26 18:14:30,438 main.py:47] epoch 4217, training loss: 7104.33, average training loss: 6791.57, base loss: 8865.25
[INFO 2017-06-26 18:14:30,800 main.py:47] epoch 4218, training loss: 6431.95, average training loss: 6791.11, base loss: 8865.04
[INFO 2017-06-26 18:14:31,161 main.py:47] epoch 4219, training loss: 7022.24, average training loss: 6790.47, base loss: 8864.69
[INFO 2017-06-26 18:14:31,524 main.py:47] epoch 4220, training loss: 6288.10, average training loss: 6790.35, base loss: 8864.07
[INFO 2017-06-26 18:14:31,916 main.py:47] epoch 4221, training loss: 6458.03, average training loss: 6790.24, base loss: 8864.55
[INFO 2017-06-26 18:14:32,312 main.py:47] epoch 4222, training loss: 7629.59, average training loss: 6791.74, base loss: 8865.19
[INFO 2017-06-26 18:14:32,673 main.py:47] epoch 4223, training loss: 5964.56, average training loss: 6791.27, base loss: 8864.32
[INFO 2017-06-26 18:14:33,034 main.py:47] epoch 4224, training loss: 6901.08, average training loss: 6791.65, base loss: 8864.99
[INFO 2017-06-26 18:14:33,395 main.py:47] epoch 4225, training loss: 6891.04, average training loss: 6791.52, base loss: 8864.43
[INFO 2017-06-26 18:14:33,756 main.py:47] epoch 4226, training loss: 7829.21, average training loss: 6792.69, base loss: 8865.56
[INFO 2017-06-26 18:14:34,115 main.py:47] epoch 4227, training loss: 6980.31, average training loss: 6792.73, base loss: 8865.06
[INFO 2017-06-26 18:14:34,474 main.py:47] epoch 4228, training loss: 5890.03, average training loss: 6791.86, base loss: 8863.83
[INFO 2017-06-26 18:14:34,832 main.py:47] epoch 4229, training loss: 6612.08, average training loss: 6792.27, base loss: 8864.24
[INFO 2017-06-26 18:14:35,192 main.py:47] epoch 4230, training loss: 6769.33, average training loss: 6792.62, base loss: 8865.56
[INFO 2017-06-26 18:14:35,551 main.py:47] epoch 4231, training loss: 6676.69, average training loss: 6792.46, base loss: 8865.14
[INFO 2017-06-26 18:14:35,910 main.py:47] epoch 4232, training loss: 6322.73, average training loss: 6791.43, base loss: 8863.28
[INFO 2017-06-26 18:14:36,268 main.py:47] epoch 4233, training loss: 7841.92, average training loss: 6793.38, base loss: 8866.34
[INFO 2017-06-26 18:14:36,625 main.py:47] epoch 4234, training loss: 6438.53, average training loss: 6792.73, base loss: 8865.61
[INFO 2017-06-26 18:14:36,982 main.py:47] epoch 4235, training loss: 6459.02, average training loss: 6792.16, base loss: 8865.09
[INFO 2017-06-26 18:14:37,342 main.py:47] epoch 4236, training loss: 6922.89, average training loss: 6792.04, base loss: 8865.20
[INFO 2017-06-26 18:14:37,700 main.py:47] epoch 4237, training loss: 6764.37, average training loss: 6790.97, base loss: 8863.79
[INFO 2017-06-26 18:14:38,061 main.py:47] epoch 4238, training loss: 6656.74, average training loss: 6791.23, base loss: 8864.04
[INFO 2017-06-26 18:14:38,421 main.py:47] epoch 4239, training loss: 6317.63, average training loss: 6791.41, base loss: 8864.52
[INFO 2017-06-26 18:14:38,780 main.py:47] epoch 4240, training loss: 6595.82, average training loss: 6791.13, base loss: 8863.91
[INFO 2017-06-26 18:14:39,177 main.py:47] epoch 4241, training loss: 6550.45, average training loss: 6789.98, base loss: 8862.81
[INFO 2017-06-26 18:14:39,569 main.py:47] epoch 4242, training loss: 7409.23, average training loss: 6791.26, base loss: 8864.70
[INFO 2017-06-26 18:14:39,935 main.py:47] epoch 4243, training loss: 6287.97, average training loss: 6791.16, base loss: 8864.79
[INFO 2017-06-26 18:14:40,332 main.py:47] epoch 4244, training loss: 5719.17, average training loss: 6790.36, base loss: 8864.21
[INFO 2017-06-26 18:14:40,713 main.py:47] epoch 4245, training loss: 7771.94, average training loss: 6791.39, base loss: 8865.72
[INFO 2017-06-26 18:14:41,119 main.py:47] epoch 4246, training loss: 5816.90, average training loss: 6790.31, base loss: 8863.84
[INFO 2017-06-26 18:14:41,533 main.py:47] epoch 4247, training loss: 6961.65, average training loss: 6791.18, base loss: 8864.44
[INFO 2017-06-26 18:14:41,930 main.py:47] epoch 4248, training loss: 6092.13, average training loss: 6790.55, base loss: 8863.26
[INFO 2017-06-26 18:14:42,311 main.py:47] epoch 4249, training loss: 5915.46, average training loss: 6790.34, base loss: 8863.08
[INFO 2017-06-26 18:14:42,714 main.py:47] epoch 4250, training loss: 6869.37, average training loss: 6790.30, base loss: 8863.03
[INFO 2017-06-26 18:14:43,091 main.py:47] epoch 4251, training loss: 6892.76, average training loss: 6789.06, base loss: 8860.84
[INFO 2017-06-26 18:14:43,457 main.py:47] epoch 4252, training loss: 6716.13, average training loss: 6788.32, base loss: 8860.16
[INFO 2017-06-26 18:14:43,814 main.py:47] epoch 4253, training loss: 7748.97, average training loss: 6788.61, base loss: 8861.03
[INFO 2017-06-26 18:14:44,175 main.py:47] epoch 4254, training loss: 7139.97, average training loss: 6789.70, base loss: 8862.04
[INFO 2017-06-26 18:14:44,535 main.py:47] epoch 4255, training loss: 7264.75, average training loss: 6790.28, base loss: 8862.72
[INFO 2017-06-26 18:14:44,895 main.py:47] epoch 4256, training loss: 6268.22, average training loss: 6788.85, base loss: 8859.87
[INFO 2017-06-26 18:14:45,252 main.py:47] epoch 4257, training loss: 8011.03, average training loss: 6789.77, base loss: 8861.89
[INFO 2017-06-26 18:14:45,611 main.py:47] epoch 4258, training loss: 6585.83, average training loss: 6790.27, base loss: 8862.83
[INFO 2017-06-26 18:14:45,971 main.py:47] epoch 4259, training loss: 6704.09, average training loss: 6791.23, base loss: 8863.82
[INFO 2017-06-26 18:14:46,329 main.py:47] epoch 4260, training loss: 7007.82, average training loss: 6791.87, base loss: 8864.53
[INFO 2017-06-26 18:14:46,686 main.py:47] epoch 4261, training loss: 6545.53, average training loss: 6791.43, base loss: 8863.87
[INFO 2017-06-26 18:14:47,044 main.py:47] epoch 4262, training loss: 6989.02, average training loss: 6792.26, base loss: 8865.48
[INFO 2017-06-26 18:14:47,402 main.py:47] epoch 4263, training loss: 6501.58, average training loss: 6791.82, base loss: 8865.24
[INFO 2017-06-26 18:14:47,760 main.py:47] epoch 4264, training loss: 6520.24, average training loss: 6792.71, base loss: 8866.79
[INFO 2017-06-26 18:14:48,118 main.py:47] epoch 4265, training loss: 6402.50, average training loss: 6791.98, base loss: 8865.52
[INFO 2017-06-26 18:14:48,477 main.py:47] epoch 4266, training loss: 6923.20, average training loss: 6792.86, base loss: 8867.14
[INFO 2017-06-26 18:14:48,834 main.py:47] epoch 4267, training loss: 5681.23, average training loss: 6791.07, base loss: 8863.85
[INFO 2017-06-26 18:14:49,193 main.py:47] epoch 4268, training loss: 7612.50, average training loss: 6791.48, base loss: 8864.41
[INFO 2017-06-26 18:14:49,551 main.py:47] epoch 4269, training loss: 7029.50, average training loss: 6791.71, base loss: 8865.19
[INFO 2017-06-26 18:14:49,910 main.py:47] epoch 4270, training loss: 6637.63, average training loss: 6791.99, base loss: 8865.61
[INFO 2017-06-26 18:14:50,269 main.py:47] epoch 4271, training loss: 6393.92, average training loss: 6791.89, base loss: 8866.02
[INFO 2017-06-26 18:14:50,625 main.py:47] epoch 4272, training loss: 7057.10, average training loss: 6791.61, base loss: 8865.98
[INFO 2017-06-26 18:14:50,983 main.py:47] epoch 4273, training loss: 7463.57, average training loss: 6792.37, base loss: 8867.34
[INFO 2017-06-26 18:14:51,342 main.py:47] epoch 4274, training loss: 7050.50, average training loss: 6791.16, base loss: 8865.72
[INFO 2017-06-26 18:14:51,700 main.py:47] epoch 4275, training loss: 7267.11, average training loss: 6791.43, base loss: 8865.95
[INFO 2017-06-26 18:14:52,061 main.py:47] epoch 4276, training loss: 6460.90, average training loss: 6792.00, base loss: 8866.59
[INFO 2017-06-26 18:14:52,421 main.py:47] epoch 4277, training loss: 7225.89, average training loss: 6792.23, base loss: 8867.30
[INFO 2017-06-26 18:14:52,797 main.py:47] epoch 4278, training loss: 7092.26, average training loss: 6793.02, base loss: 8868.72
[INFO 2017-06-26 18:14:53,164 main.py:47] epoch 4279, training loss: 6758.76, average training loss: 6793.92, base loss: 8871.16
[INFO 2017-06-26 18:14:53,525 main.py:47] epoch 4280, training loss: 6348.05, average training loss: 6792.41, base loss: 8869.42
[INFO 2017-06-26 18:14:53,885 main.py:47] epoch 4281, training loss: 6308.57, average training loss: 6791.67, base loss: 8868.23
[INFO 2017-06-26 18:14:54,245 main.py:47] epoch 4282, training loss: 7490.84, average training loss: 6792.02, base loss: 8869.24
[INFO 2017-06-26 18:14:54,605 main.py:47] epoch 4283, training loss: 6450.29, average training loss: 6792.03, base loss: 8869.61
[INFO 2017-06-26 18:14:54,964 main.py:47] epoch 4284, training loss: 7692.57, average training loss: 6792.32, base loss: 8869.39
[INFO 2017-06-26 18:14:55,322 main.py:47] epoch 4285, training loss: 6166.40, average training loss: 6792.38, base loss: 8870.54
[INFO 2017-06-26 18:14:55,681 main.py:47] epoch 4286, training loss: 6711.38, average training loss: 6792.77, base loss: 8871.41
[INFO 2017-06-26 18:14:56,040 main.py:47] epoch 4287, training loss: 6769.07, average training loss: 6793.23, base loss: 8873.24
[INFO 2017-06-26 18:14:56,400 main.py:47] epoch 4288, training loss: 6716.25, average training loss: 6792.98, base loss: 8873.42
[INFO 2017-06-26 18:14:56,759 main.py:47] epoch 4289, training loss: 6549.95, average training loss: 6792.46, base loss: 8872.46
[INFO 2017-06-26 18:14:57,123 main.py:47] epoch 4290, training loss: 6463.37, average training loss: 6792.02, base loss: 8871.88
[INFO 2017-06-26 18:14:57,484 main.py:47] epoch 4291, training loss: 6175.82, average training loss: 6791.39, base loss: 8872.38
[INFO 2017-06-26 18:14:57,842 main.py:47] epoch 4292, training loss: 6496.06, average training loss: 6791.66, base loss: 8873.08
[INFO 2017-06-26 18:14:58,200 main.py:47] epoch 4293, training loss: 7658.21, average training loss: 6792.67, base loss: 8873.49
[INFO 2017-06-26 18:14:58,559 main.py:47] epoch 4294, training loss: 7346.87, average training loss: 6792.89, base loss: 8874.55
[INFO 2017-06-26 18:14:58,917 main.py:47] epoch 4295, training loss: 6461.05, average training loss: 6791.55, base loss: 8872.80
[INFO 2017-06-26 18:14:59,277 main.py:47] epoch 4296, training loss: 6506.96, average training loss: 6791.35, base loss: 8872.32
[INFO 2017-06-26 18:14:59,635 main.py:47] epoch 4297, training loss: 6934.78, average training loss: 6790.03, base loss: 8869.70
[INFO 2017-06-26 18:14:59,994 main.py:47] epoch 4298, training loss: 6148.19, average training loss: 6789.33, base loss: 8868.38
[INFO 2017-06-26 18:15:00,352 main.py:47] epoch 4299, training loss: 7046.47, average training loss: 6790.08, base loss: 8869.92
[INFO 2017-06-26 18:15:00,352 main.py:49] epoch 4299, testing
[INFO 2017-06-26 18:15:04,492 main.py:100] average testing loss: 6912.49, base loss: 9042.87
[INFO 2017-06-26 18:15:04,517 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:15:04,876 main.py:47] epoch 4300, training loss: 7089.45, average training loss: 6790.22, base loss: 8870.33
[INFO 2017-06-26 18:15:05,237 main.py:47] epoch 4301, training loss: 7790.81, average training loss: 6791.55, base loss: 8873.70
[INFO 2017-06-26 18:15:05,596 main.py:47] epoch 4302, training loss: 5733.63, average training loss: 6790.34, base loss: 8872.41
[INFO 2017-06-26 18:15:05,954 main.py:47] epoch 4303, training loss: 7391.67, average training loss: 6791.64, base loss: 8874.38
[INFO 2017-06-26 18:15:06,312 main.py:47] epoch 4304, training loss: 7503.00, average training loss: 6792.59, base loss: 8875.47
[INFO 2017-06-26 18:15:06,671 main.py:47] epoch 4305, training loss: 6636.80, average training loss: 6792.48, base loss: 8875.46
[INFO 2017-06-26 18:15:07,030 main.py:47] epoch 4306, training loss: 7106.74, average training loss: 6792.52, base loss: 8875.44
[INFO 2017-06-26 18:15:07,387 main.py:47] epoch 4307, training loss: 6969.79, average training loss: 6792.55, base loss: 8874.96
[INFO 2017-06-26 18:15:07,746 main.py:47] epoch 4308, training loss: 6244.95, average training loss: 6791.43, base loss: 8873.33
[INFO 2017-06-26 18:15:08,105 main.py:47] epoch 4309, training loss: 6912.94, average training loss: 6790.03, base loss: 8871.98
[INFO 2017-06-26 18:15:08,462 main.py:47] epoch 4310, training loss: 6140.61, average training loss: 6788.89, base loss: 8869.98
[INFO 2017-06-26 18:15:08,822 main.py:47] epoch 4311, training loss: 6445.47, average training loss: 6788.38, base loss: 8869.29
[INFO 2017-06-26 18:15:09,180 main.py:47] epoch 4312, training loss: 6812.82, average training loss: 6788.88, base loss: 8869.49
[INFO 2017-06-26 18:15:09,598 main.py:47] epoch 4313, training loss: 6012.49, average training loss: 6788.28, base loss: 8868.83
[INFO 2017-06-26 18:15:09,959 main.py:47] epoch 4314, training loss: 6335.90, average training loss: 6788.46, base loss: 8869.11
[INFO 2017-06-26 18:15:10,318 main.py:47] epoch 4315, training loss: 7247.00, average training loss: 6788.78, base loss: 8869.89
[INFO 2017-06-26 18:15:10,710 main.py:47] epoch 4316, training loss: 6502.05, average training loss: 6788.11, base loss: 8868.76
[INFO 2017-06-26 18:15:11,087 main.py:47] epoch 4317, training loss: 6158.02, average training loss: 6786.52, base loss: 8866.76
[INFO 2017-06-26 18:15:11,449 main.py:47] epoch 4318, training loss: 6420.70, average training loss: 6785.53, base loss: 8866.35
[INFO 2017-06-26 18:15:11,821 main.py:47] epoch 4319, training loss: 7137.32, average training loss: 6785.51, base loss: 8866.12
[INFO 2017-06-26 18:15:12,183 main.py:47] epoch 4320, training loss: 5877.90, average training loss: 6784.61, base loss: 8864.55
[INFO 2017-06-26 18:15:12,543 main.py:47] epoch 4321, training loss: 6330.59, average training loss: 6783.39, base loss: 8863.40
[INFO 2017-06-26 18:15:12,905 main.py:47] epoch 4322, training loss: 6169.07, average training loss: 6782.98, base loss: 8863.06
[INFO 2017-06-26 18:15:13,300 main.py:47] epoch 4323, training loss: 6840.12, average training loss: 6782.78, base loss: 8862.42
[INFO 2017-06-26 18:15:13,710 main.py:47] epoch 4324, training loss: 7163.46, average training loss: 6783.43, base loss: 8863.48
[INFO 2017-06-26 18:15:14,078 main.py:47] epoch 4325, training loss: 6552.48, average training loss: 6783.12, base loss: 8863.51
[INFO 2017-06-26 18:15:14,438 main.py:47] epoch 4326, training loss: 7655.25, average training loss: 6783.63, base loss: 8863.88
[INFO 2017-06-26 18:15:14,797 main.py:47] epoch 4327, training loss: 6891.48, average training loss: 6782.68, base loss: 8862.01
[INFO 2017-06-26 18:15:15,156 main.py:47] epoch 4328, training loss: 6385.67, average training loss: 6781.62, base loss: 8860.83
[INFO 2017-06-26 18:15:15,514 main.py:47] epoch 4329, training loss: 6603.70, average training loss: 6780.59, base loss: 8858.92
[INFO 2017-06-26 18:15:15,871 main.py:47] epoch 4330, training loss: 5922.14, average training loss: 6779.49, base loss: 8855.94
[INFO 2017-06-26 18:15:16,230 main.py:47] epoch 4331, training loss: 6698.34, average training loss: 6779.89, base loss: 8857.33
[INFO 2017-06-26 18:15:16,589 main.py:47] epoch 4332, training loss: 7387.74, average training loss: 6780.32, base loss: 8858.02
[INFO 2017-06-26 18:15:16,945 main.py:47] epoch 4333, training loss: 7720.44, average training loss: 6781.16, base loss: 8859.78
[INFO 2017-06-26 18:15:17,318 main.py:47] epoch 4334, training loss: 7262.25, average training loss: 6781.65, base loss: 8860.98
[INFO 2017-06-26 18:15:17,677 main.py:47] epoch 4335, training loss: 6258.89, average training loss: 6781.49, base loss: 8860.94
[INFO 2017-06-26 18:15:18,039 main.py:47] epoch 4336, training loss: 6963.88, average training loss: 6781.62, base loss: 8861.34
[INFO 2017-06-26 18:15:18,415 main.py:47] epoch 4337, training loss: 7528.86, average training loss: 6782.12, base loss: 8863.47
[INFO 2017-06-26 18:15:18,776 main.py:47] epoch 4338, training loss: 6190.22, average training loss: 6780.77, base loss: 8861.95
[INFO 2017-06-26 18:15:19,168 main.py:47] epoch 4339, training loss: 5987.04, average training loss: 6778.94, base loss: 8859.56
[INFO 2017-06-26 18:15:19,542 main.py:47] epoch 4340, training loss: 6595.21, average training loss: 6778.61, base loss: 8859.54
[INFO 2017-06-26 18:15:19,902 main.py:47] epoch 4341, training loss: 6530.51, average training loss: 6778.86, base loss: 8860.67
[INFO 2017-06-26 18:15:20,297 main.py:47] epoch 4342, training loss: 7058.49, average training loss: 6779.46, base loss: 8862.31
[INFO 2017-06-26 18:15:20,665 main.py:47] epoch 4343, training loss: 7008.37, average training loss: 6779.76, base loss: 8862.99
[INFO 2017-06-26 18:15:21,030 main.py:47] epoch 4344, training loss: 7487.43, average training loss: 6779.79, base loss: 8863.48
[INFO 2017-06-26 18:15:21,417 main.py:47] epoch 4345, training loss: 6648.51, average training loss: 6779.46, base loss: 8863.12
[INFO 2017-06-26 18:15:21,776 main.py:47] epoch 4346, training loss: 6451.08, average training loss: 6779.77, base loss: 8863.92
[INFO 2017-06-26 18:15:22,152 main.py:47] epoch 4347, training loss: 6739.96, average training loss: 6779.63, base loss: 8863.92
[INFO 2017-06-26 18:15:22,512 main.py:47] epoch 4348, training loss: 6333.44, average training loss: 6779.00, base loss: 8862.80
[INFO 2017-06-26 18:15:22,871 main.py:47] epoch 4349, training loss: 6323.20, average training loss: 6777.37, base loss: 8860.07
[INFO 2017-06-26 18:15:23,228 main.py:47] epoch 4350, training loss: 6902.88, average training loss: 6777.15, base loss: 8860.12
[INFO 2017-06-26 18:15:23,587 main.py:47] epoch 4351, training loss: 6157.64, average training loss: 6775.86, base loss: 8858.70
[INFO 2017-06-26 18:15:23,942 main.py:47] epoch 4352, training loss: 6692.10, average training loss: 6774.76, base loss: 8856.30
[INFO 2017-06-26 18:15:24,301 main.py:47] epoch 4353, training loss: 6085.68, average training loss: 6774.55, base loss: 8856.49
[INFO 2017-06-26 18:15:24,658 main.py:47] epoch 4354, training loss: 7619.93, average training loss: 6775.61, base loss: 8857.79
[INFO 2017-06-26 18:15:25,016 main.py:47] epoch 4355, training loss: 6763.46, average training loss: 6775.55, base loss: 8857.83
[INFO 2017-06-26 18:15:25,374 main.py:47] epoch 4356, training loss: 6423.45, average training loss: 6774.12, base loss: 8855.79
[INFO 2017-06-26 18:15:25,733 main.py:47] epoch 4357, training loss: 8204.10, average training loss: 6776.02, base loss: 8859.06
[INFO 2017-06-26 18:15:26,092 main.py:47] epoch 4358, training loss: 6634.68, average training loss: 6775.75, base loss: 8858.00
[INFO 2017-06-26 18:15:26,467 main.py:47] epoch 4359, training loss: 5721.16, average training loss: 6774.26, base loss: 8855.59
[INFO 2017-06-26 18:15:26,828 main.py:47] epoch 4360, training loss: 7862.05, average training loss: 6775.22, base loss: 8857.19
[INFO 2017-06-26 18:15:27,189 main.py:47] epoch 4361, training loss: 6426.35, average training loss: 6774.61, base loss: 8856.13
[INFO 2017-06-26 18:15:27,583 main.py:47] epoch 4362, training loss: 6792.78, average training loss: 6774.19, base loss: 8855.04
[INFO 2017-06-26 18:15:27,962 main.py:47] epoch 4363, training loss: 6848.71, average training loss: 6774.08, base loss: 8855.24
[INFO 2017-06-26 18:15:28,325 main.py:47] epoch 4364, training loss: 6789.85, average training loss: 6774.92, base loss: 8857.01
[INFO 2017-06-26 18:15:28,690 main.py:47] epoch 4365, training loss: 5518.46, average training loss: 6773.23, base loss: 8854.36
[INFO 2017-06-26 18:15:29,048 main.py:47] epoch 4366, training loss: 6927.63, average training loss: 6774.00, base loss: 8855.90
[INFO 2017-06-26 18:15:29,403 main.py:47] epoch 4367, training loss: 6646.20, average training loss: 6773.40, base loss: 8855.74
[INFO 2017-06-26 18:15:29,764 main.py:47] epoch 4368, training loss: 7138.70, average training loss: 6774.24, base loss: 8857.09
[INFO 2017-06-26 18:15:30,124 main.py:47] epoch 4369, training loss: 6602.70, average training loss: 6774.58, base loss: 8857.81
[INFO 2017-06-26 18:15:30,482 main.py:47] epoch 4370, training loss: 6808.36, average training loss: 6774.82, base loss: 8858.59
[INFO 2017-06-26 18:15:30,842 main.py:47] epoch 4371, training loss: 7039.98, average training loss: 6774.43, base loss: 8858.41
[INFO 2017-06-26 18:15:31,200 main.py:47] epoch 4372, training loss: 6716.17, average training loss: 6774.69, base loss: 8858.13
[INFO 2017-06-26 18:15:31,561 main.py:47] epoch 4373, training loss: 7406.83, average training loss: 6775.96, base loss: 8859.83
[INFO 2017-06-26 18:15:31,922 main.py:47] epoch 4374, training loss: 6147.80, average training loss: 6776.05, base loss: 8859.98
[INFO 2017-06-26 18:15:32,280 main.py:47] epoch 4375, training loss: 6567.96, average training loss: 6776.07, base loss: 8860.23
[INFO 2017-06-26 18:15:32,642 main.py:47] epoch 4376, training loss: 6767.08, average training loss: 6776.47, base loss: 8861.41
[INFO 2017-06-26 18:15:33,001 main.py:47] epoch 4377, training loss: 7262.46, average training loss: 6777.55, base loss: 8863.68
[INFO 2017-06-26 18:15:33,361 main.py:47] epoch 4378, training loss: 7480.12, average training loss: 6779.17, base loss: 8866.23
[INFO 2017-06-26 18:15:33,719 main.py:47] epoch 4379, training loss: 6501.38, average training loss: 6779.44, base loss: 8866.95
[INFO 2017-06-26 18:15:34,079 main.py:47] epoch 4380, training loss: 6858.31, average training loss: 6779.70, base loss: 8867.08
[INFO 2017-06-26 18:15:34,439 main.py:47] epoch 4381, training loss: 6337.65, average training loss: 6779.70, base loss: 8866.39
[INFO 2017-06-26 18:15:34,801 main.py:47] epoch 4382, training loss: 6099.16, average training loss: 6778.63, base loss: 8865.02
[INFO 2017-06-26 18:15:35,160 main.py:47] epoch 4383, training loss: 6274.47, average training loss: 6778.54, base loss: 8864.57
[INFO 2017-06-26 18:15:35,520 main.py:47] epoch 4384, training loss: 7066.28, average training loss: 6778.20, base loss: 8864.45
[INFO 2017-06-26 18:15:35,883 main.py:47] epoch 4385, training loss: 6656.20, average training loss: 6778.23, base loss: 8864.51
[INFO 2017-06-26 18:15:36,243 main.py:47] epoch 4386, training loss: 6530.34, average training loss: 6778.81, base loss: 8866.06
[INFO 2017-06-26 18:15:36,605 main.py:47] epoch 4387, training loss: 7454.46, average training loss: 6779.65, base loss: 8867.26
[INFO 2017-06-26 18:15:36,966 main.py:47] epoch 4388, training loss: 7413.29, average training loss: 6779.61, base loss: 8867.63
[INFO 2017-06-26 18:15:37,337 main.py:47] epoch 4389, training loss: 7393.98, average training loss: 6779.13, base loss: 8867.24
[INFO 2017-06-26 18:15:37,698 main.py:47] epoch 4390, training loss: 7075.73, average training loss: 6779.33, base loss: 8867.79
[INFO 2017-06-26 18:15:38,056 main.py:47] epoch 4391, training loss: 6403.43, average training loss: 6778.73, base loss: 8866.97
[INFO 2017-06-26 18:15:38,419 main.py:47] epoch 4392, training loss: 7574.71, average training loss: 6780.29, base loss: 8870.46
[INFO 2017-06-26 18:15:38,778 main.py:47] epoch 4393, training loss: 6648.99, average training loss: 6781.03, base loss: 8870.89
[INFO 2017-06-26 18:15:39,136 main.py:47] epoch 4394, training loss: 6698.52, average training loss: 6780.88, base loss: 8870.59
[INFO 2017-06-26 18:15:39,493 main.py:47] epoch 4395, training loss: 6495.14, average training loss: 6780.41, base loss: 8869.90
[INFO 2017-06-26 18:15:39,853 main.py:47] epoch 4396, training loss: 6078.79, average training loss: 6779.77, base loss: 8868.18
[INFO 2017-06-26 18:15:40,211 main.py:47] epoch 4397, training loss: 6128.08, average training loss: 6779.08, base loss: 8867.09
[INFO 2017-06-26 18:15:40,568 main.py:47] epoch 4398, training loss: 6894.42, average training loss: 6779.43, base loss: 8868.15
[INFO 2017-06-26 18:15:40,928 main.py:47] epoch 4399, training loss: 7890.98, average training loss: 6781.30, base loss: 8871.09
[INFO 2017-06-26 18:15:40,928 main.py:49] epoch 4399, testing
[INFO 2017-06-26 18:15:45,165 main.py:100] average testing loss: 6773.37, base loss: 8800.14
[INFO 2017-06-26 18:15:45,189 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:15:45,549 main.py:47] epoch 4400, training loss: 8318.03, average training loss: 6782.69, base loss: 8873.03
[INFO 2017-06-26 18:15:45,908 main.py:47] epoch 4401, training loss: 6787.94, average training loss: 6782.26, base loss: 8872.33
[INFO 2017-06-26 18:15:46,267 main.py:47] epoch 4402, training loss: 6133.53, average training loss: 6781.36, base loss: 8871.09
[INFO 2017-06-26 18:15:46,625 main.py:47] epoch 4403, training loss: 6417.60, average training loss: 6781.15, base loss: 8870.77
[INFO 2017-06-26 18:15:46,983 main.py:47] epoch 4404, training loss: 7298.68, average training loss: 6782.12, base loss: 8872.60
[INFO 2017-06-26 18:15:47,342 main.py:47] epoch 4405, training loss: 7224.76, average training loss: 6783.00, base loss: 8874.35
[INFO 2017-06-26 18:15:47,700 main.py:47] epoch 4406, training loss: 6882.15, average training loss: 6783.05, base loss: 8874.57
[INFO 2017-06-26 18:15:48,059 main.py:47] epoch 4407, training loss: 6328.26, average training loss: 6783.19, base loss: 8874.49
[INFO 2017-06-26 18:15:48,418 main.py:47] epoch 4408, training loss: 7385.47, average training loss: 6783.81, base loss: 8874.90
[INFO 2017-06-26 18:15:48,775 main.py:47] epoch 4409, training loss: 6731.42, average training loss: 6783.97, base loss: 8875.53
[INFO 2017-06-26 18:15:49,134 main.py:47] epoch 4410, training loss: 6399.85, average training loss: 6783.10, base loss: 8874.44
[INFO 2017-06-26 18:15:49,493 main.py:47] epoch 4411, training loss: 7113.66, average training loss: 6783.06, base loss: 8874.04
[INFO 2017-06-26 18:15:49,850 main.py:47] epoch 4412, training loss: 6155.29, average training loss: 6782.06, base loss: 8872.98
[INFO 2017-06-26 18:15:50,209 main.py:47] epoch 4413, training loss: 6958.15, average training loss: 6782.50, base loss: 8873.52
[INFO 2017-06-26 18:15:50,569 main.py:47] epoch 4414, training loss: 6447.95, average training loss: 6782.21, base loss: 8873.50
[INFO 2017-06-26 18:15:50,928 main.py:47] epoch 4415, training loss: 6293.17, average training loss: 6781.17, base loss: 8872.05
[INFO 2017-06-26 18:15:51,288 main.py:47] epoch 4416, training loss: 7204.61, average training loss: 6781.86, base loss: 8872.95
[INFO 2017-06-26 18:15:51,647 main.py:47] epoch 4417, training loss: 6623.40, average training loss: 6782.29, base loss: 8873.50
[INFO 2017-06-26 18:15:52,006 main.py:47] epoch 4418, training loss: 7247.85, average training loss: 6782.20, base loss: 8873.36
[INFO 2017-06-26 18:15:52,365 main.py:47] epoch 4419, training loss: 7196.73, average training loss: 6782.23, base loss: 8873.44
[INFO 2017-06-26 18:15:52,725 main.py:47] epoch 4420, training loss: 6863.56, average training loss: 6782.44, base loss: 8873.80
[INFO 2017-06-26 18:15:53,083 main.py:47] epoch 4421, training loss: 6854.78, average training loss: 6783.48, base loss: 8875.56
[INFO 2017-06-26 18:15:53,443 main.py:47] epoch 4422, training loss: 7199.94, average training loss: 6782.88, base loss: 8874.25
[INFO 2017-06-26 18:15:53,800 main.py:47] epoch 4423, training loss: 5894.80, average training loss: 6781.82, base loss: 8873.53
[INFO 2017-06-26 18:15:54,158 main.py:47] epoch 4424, training loss: 5600.61, average training loss: 6779.81, base loss: 8871.44
[INFO 2017-06-26 18:15:54,518 main.py:47] epoch 4425, training loss: 7141.48, average training loss: 6780.36, base loss: 8872.36
[INFO 2017-06-26 18:15:54,877 main.py:47] epoch 4426, training loss: 7027.14, average training loss: 6781.18, base loss: 8874.31
[INFO 2017-06-26 18:15:55,237 main.py:47] epoch 4427, training loss: 6209.54, average training loss: 6781.38, base loss: 8874.34
[INFO 2017-06-26 18:15:55,597 main.py:47] epoch 4428, training loss: 6477.06, average training loss: 6780.58, base loss: 8872.91
[INFO 2017-06-26 18:15:55,956 main.py:47] epoch 4429, training loss: 6799.58, average training loss: 6780.27, base loss: 8871.42
[INFO 2017-06-26 18:15:56,316 main.py:47] epoch 4430, training loss: 7293.76, average training loss: 6781.07, base loss: 8872.40
[INFO 2017-06-26 18:15:56,675 main.py:47] epoch 4431, training loss: 6523.61, average training loss: 6780.96, base loss: 8871.75
[INFO 2017-06-26 18:15:57,035 main.py:47] epoch 4432, training loss: 6644.59, average training loss: 6781.25, base loss: 8872.48
[INFO 2017-06-26 18:15:57,399 main.py:47] epoch 4433, training loss: 5993.84, average training loss: 6780.62, base loss: 8871.44
[INFO 2017-06-26 18:15:57,759 main.py:47] epoch 4434, training loss: 7060.77, average training loss: 6780.25, base loss: 8870.73
[INFO 2017-06-26 18:15:58,120 main.py:47] epoch 4435, training loss: 6642.36, average training loss: 6780.91, base loss: 8872.06
[INFO 2017-06-26 18:15:58,482 main.py:47] epoch 4436, training loss: 6043.93, average training loss: 6780.12, base loss: 8870.57
[INFO 2017-06-26 18:15:58,844 main.py:47] epoch 4437, training loss: 6893.87, average training loss: 6779.55, base loss: 8870.50
[INFO 2017-06-26 18:15:59,205 main.py:47] epoch 4438, training loss: 6434.26, average training loss: 6779.18, base loss: 8870.45
[INFO 2017-06-26 18:15:59,564 main.py:47] epoch 4439, training loss: 6471.60, average training loss: 6779.34, base loss: 8871.10
[INFO 2017-06-26 18:15:59,927 main.py:47] epoch 4440, training loss: 7899.93, average training loss: 6780.55, base loss: 8873.34
[INFO 2017-06-26 18:16:00,286 main.py:47] epoch 4441, training loss: 6552.87, average training loss: 6780.51, base loss: 8873.37
[INFO 2017-06-26 18:16:00,650 main.py:47] epoch 4442, training loss: 7402.99, average training loss: 6780.81, base loss: 8874.21
[INFO 2017-06-26 18:16:01,015 main.py:47] epoch 4443, training loss: 6494.89, average training loss: 6779.70, base loss: 8872.23
[INFO 2017-06-26 18:16:01,377 main.py:47] epoch 4444, training loss: 7411.04, average training loss: 6780.64, base loss: 8872.89
[INFO 2017-06-26 18:16:01,739 main.py:47] epoch 4445, training loss: 6345.86, average training loss: 6780.43, base loss: 8873.28
[INFO 2017-06-26 18:16:02,099 main.py:47] epoch 4446, training loss: 6959.64, average training loss: 6780.69, base loss: 8874.01
[INFO 2017-06-26 18:16:02,457 main.py:47] epoch 4447, training loss: 6646.02, average training loss: 6780.87, base loss: 8874.49
[INFO 2017-06-26 18:16:02,816 main.py:47] epoch 4448, training loss: 6099.11, average training loss: 6780.69, base loss: 8874.39
[INFO 2017-06-26 18:16:03,176 main.py:47] epoch 4449, training loss: 6531.30, average training loss: 6780.39, base loss: 8874.36
[INFO 2017-06-26 18:16:03,535 main.py:47] epoch 4450, training loss: 7291.48, average training loss: 6780.44, base loss: 8875.18
[INFO 2017-06-26 18:16:03,894 main.py:47] epoch 4451, training loss: 6521.78, average training loss: 6780.20, base loss: 8874.33
[INFO 2017-06-26 18:16:04,256 main.py:47] epoch 4452, training loss: 6363.24, average training loss: 6780.09, base loss: 8874.81
[INFO 2017-06-26 18:16:04,618 main.py:47] epoch 4453, training loss: 6285.97, average training loss: 6779.03, base loss: 8873.20
[INFO 2017-06-26 18:16:04,980 main.py:47] epoch 4454, training loss: 6938.51, average training loss: 6779.41, base loss: 8873.74
[INFO 2017-06-26 18:16:05,340 main.py:47] epoch 4455, training loss: 6313.82, average training loss: 6779.26, base loss: 8873.16
[INFO 2017-06-26 18:16:05,703 main.py:47] epoch 4456, training loss: 6762.27, average training loss: 6779.70, base loss: 8874.39
[INFO 2017-06-26 18:16:06,065 main.py:47] epoch 4457, training loss: 6581.42, average training loss: 6778.84, base loss: 8873.81
[INFO 2017-06-26 18:16:06,428 main.py:47] epoch 4458, training loss: 6230.47, average training loss: 6778.30, base loss: 8872.37
[INFO 2017-06-26 18:16:06,789 main.py:47] epoch 4459, training loss: 6868.42, average training loss: 6778.44, base loss: 8872.16
[INFO 2017-06-26 18:16:07,149 main.py:47] epoch 4460, training loss: 7050.42, average training loss: 6778.92, base loss: 8872.63
[INFO 2017-06-26 18:16:07,518 main.py:47] epoch 4461, training loss: 6982.35, average training loss: 6778.93, base loss: 8872.50
[INFO 2017-06-26 18:16:07,879 main.py:47] epoch 4462, training loss: 7333.56, average training loss: 6779.11, base loss: 8873.21
[INFO 2017-06-26 18:16:08,242 main.py:47] epoch 4463, training loss: 7288.95, average training loss: 6780.41, base loss: 8875.27
[INFO 2017-06-26 18:16:08,607 main.py:47] epoch 4464, training loss: 6649.03, average training loss: 6780.41, base loss: 8874.39
[INFO 2017-06-26 18:16:08,966 main.py:47] epoch 4465, training loss: 6168.55, average training loss: 6780.05, base loss: 8873.38
[INFO 2017-06-26 18:16:09,329 main.py:47] epoch 4466, training loss: 6403.46, average training loss: 6780.15, base loss: 8873.87
[INFO 2017-06-26 18:16:09,692 main.py:47] epoch 4467, training loss: 6285.82, average training loss: 6780.18, base loss: 8873.51
[INFO 2017-06-26 18:16:10,054 main.py:47] epoch 4468, training loss: 6609.17, average training loss: 6779.93, base loss: 8873.29
[INFO 2017-06-26 18:16:10,416 main.py:47] epoch 4469, training loss: 7351.55, average training loss: 6780.32, base loss: 8874.25
[INFO 2017-06-26 18:16:10,779 main.py:47] epoch 4470, training loss: 6188.08, average training loss: 6779.67, base loss: 8872.87
[INFO 2017-06-26 18:16:11,138 main.py:47] epoch 4471, training loss: 7263.22, average training loss: 6779.83, base loss: 8873.05
[INFO 2017-06-26 18:16:11,498 main.py:47] epoch 4472, training loss: 7081.82, average training loss: 6779.91, base loss: 8873.50
[INFO 2017-06-26 18:16:11,859 main.py:47] epoch 4473, training loss: 7063.02, average training loss: 6779.94, base loss: 8874.03
[INFO 2017-06-26 18:16:12,218 main.py:47] epoch 4474, training loss: 6117.05, average training loss: 6778.64, base loss: 8872.28
[INFO 2017-06-26 18:16:12,578 main.py:47] epoch 4475, training loss: 6288.42, average training loss: 6778.73, base loss: 8871.79
[INFO 2017-06-26 18:16:12,937 main.py:47] epoch 4476, training loss: 6375.12, average training loss: 6778.31, base loss: 8871.47
[INFO 2017-06-26 18:16:13,297 main.py:47] epoch 4477, training loss: 7314.28, average training loss: 6778.75, base loss: 8872.19
[INFO 2017-06-26 18:16:13,658 main.py:47] epoch 4478, training loss: 6986.81, average training loss: 6778.96, base loss: 8872.91
[INFO 2017-06-26 18:16:14,049 main.py:47] epoch 4479, training loss: 6907.98, average training loss: 6779.19, base loss: 8873.13
[INFO 2017-06-26 18:16:14,415 main.py:47] epoch 4480, training loss: 7723.11, average training loss: 6780.23, base loss: 8874.26
[INFO 2017-06-26 18:16:14,775 main.py:47] epoch 4481, training loss: 6942.12, average training loss: 6780.57, base loss: 8874.48
[INFO 2017-06-26 18:16:15,134 main.py:47] epoch 4482, training loss: 6510.14, average training loss: 6780.84, base loss: 8875.81
[INFO 2017-06-26 18:16:15,494 main.py:47] epoch 4483, training loss: 7140.79, average training loss: 6781.86, base loss: 8877.82
[INFO 2017-06-26 18:16:15,857 main.py:47] epoch 4484, training loss: 6688.54, average training loss: 6781.75, base loss: 8878.11
[INFO 2017-06-26 18:16:16,217 main.py:47] epoch 4485, training loss: 6491.82, average training loss: 6780.36, base loss: 8875.67
[INFO 2017-06-26 18:16:16,577 main.py:47] epoch 4486, training loss: 6594.62, average training loss: 6780.39, base loss: 8875.84
[INFO 2017-06-26 18:16:16,937 main.py:47] epoch 4487, training loss: 5828.81, average training loss: 6779.64, base loss: 8875.23
[INFO 2017-06-26 18:16:17,297 main.py:47] epoch 4488, training loss: 6984.76, average training loss: 6778.83, base loss: 8873.22
[INFO 2017-06-26 18:16:17,659 main.py:47] epoch 4489, training loss: 7739.13, average training loss: 6780.35, base loss: 8875.58
[INFO 2017-06-26 18:16:18,021 main.py:47] epoch 4490, training loss: 6531.54, average training loss: 6779.49, base loss: 8874.26
[INFO 2017-06-26 18:16:18,380 main.py:47] epoch 4491, training loss: 7052.93, average training loss: 6780.16, base loss: 8875.18
[INFO 2017-06-26 18:16:18,740 main.py:47] epoch 4492, training loss: 6407.70, average training loss: 6779.66, base loss: 8873.83
[INFO 2017-06-26 18:16:19,098 main.py:47] epoch 4493, training loss: 7677.46, average training loss: 6780.80, base loss: 8875.33
[INFO 2017-06-26 18:16:19,456 main.py:47] epoch 4494, training loss: 6967.46, average training loss: 6780.42, base loss: 8875.10
[INFO 2017-06-26 18:16:19,815 main.py:47] epoch 4495, training loss: 6392.72, average training loss: 6780.45, base loss: 8875.37
[INFO 2017-06-26 18:16:20,174 main.py:47] epoch 4496, training loss: 6974.96, average training loss: 6781.34, base loss: 8876.73
[INFO 2017-06-26 18:16:20,534 main.py:47] epoch 4497, training loss: 7188.58, average training loss: 6782.05, base loss: 8877.40
[INFO 2017-06-26 18:16:20,892 main.py:47] epoch 4498, training loss: 6259.13, average training loss: 6782.68, base loss: 8878.38
[INFO 2017-06-26 18:16:21,250 main.py:47] epoch 4499, training loss: 6758.25, average training loss: 6783.08, base loss: 8879.31
[INFO 2017-06-26 18:16:21,250 main.py:49] epoch 4499, testing
[INFO 2017-06-26 18:16:25,400 main.py:100] average testing loss: 6428.65, base loss: 8341.53
[INFO 2017-06-26 18:16:25,425 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:16:25,785 main.py:47] epoch 4500, training loss: 6700.61, average training loss: 6781.73, base loss: 8877.28
[INFO 2017-06-26 18:16:26,144 main.py:47] epoch 4501, training loss: 6572.61, average training loss: 6781.17, base loss: 8877.03
[INFO 2017-06-26 18:16:26,504 main.py:47] epoch 4502, training loss: 7491.83, average training loss: 6780.96, base loss: 8876.54
[INFO 2017-06-26 18:16:26,862 main.py:47] epoch 4503, training loss: 6611.43, average training loss: 6780.38, base loss: 8875.81
[INFO 2017-06-26 18:16:27,252 main.py:47] epoch 4504, training loss: 7214.97, average training loss: 6780.18, base loss: 8875.88
[INFO 2017-06-26 18:16:27,672 main.py:47] epoch 4505, training loss: 6604.42, average training loss: 6779.25, base loss: 8874.02
[INFO 2017-06-26 18:16:28,033 main.py:47] epoch 4506, training loss: 6932.52, average training loss: 6779.63, base loss: 8874.94
[INFO 2017-06-26 18:16:28,392 main.py:47] epoch 4507, training loss: 6556.87, average training loss: 6779.65, base loss: 8875.19
[INFO 2017-06-26 18:16:28,750 main.py:47] epoch 4508, training loss: 6404.06, average training loss: 6778.44, base loss: 8874.32
[INFO 2017-06-26 18:16:29,141 main.py:47] epoch 4509, training loss: 6449.27, average training loss: 6776.39, base loss: 8870.67
[INFO 2017-06-26 18:16:29,517 main.py:47] epoch 4510, training loss: 8378.72, average training loss: 6776.84, base loss: 8871.26
[INFO 2017-06-26 18:16:29,876 main.py:47] epoch 4511, training loss: 7174.57, average training loss: 6776.88, base loss: 8872.02
[INFO 2017-06-26 18:16:30,234 main.py:47] epoch 4512, training loss: 7494.10, average training loss: 6778.32, base loss: 8874.36
[INFO 2017-06-26 18:16:30,595 main.py:47] epoch 4513, training loss: 7220.84, average training loss: 6779.39, base loss: 8876.81
[INFO 2017-06-26 18:16:30,954 main.py:47] epoch 4514, training loss: 5850.49, average training loss: 6778.54, base loss: 8876.13
[INFO 2017-06-26 18:16:31,314 main.py:47] epoch 4515, training loss: 6422.13, average training loss: 6778.75, base loss: 8875.77
[INFO 2017-06-26 18:16:31,673 main.py:47] epoch 4516, training loss: 7534.06, average training loss: 6780.35, base loss: 8877.88
[INFO 2017-06-26 18:16:32,032 main.py:47] epoch 4517, training loss: 6113.04, average training loss: 6779.01, base loss: 8875.88
[INFO 2017-06-26 18:16:32,391 main.py:47] epoch 4518, training loss: 6512.67, average training loss: 6779.13, base loss: 8875.59
[INFO 2017-06-26 18:16:32,749 main.py:47] epoch 4519, training loss: 6598.16, average training loss: 6777.76, base loss: 8873.78
[INFO 2017-06-26 18:16:33,108 main.py:47] epoch 4520, training loss: 6827.77, average training loss: 6778.08, base loss: 8874.43
[INFO 2017-06-26 18:16:33,468 main.py:47] epoch 4521, training loss: 7019.81, average training loss: 6778.92, base loss: 8875.02
[INFO 2017-06-26 18:16:33,825 main.py:47] epoch 4522, training loss: 6364.45, average training loss: 6778.55, base loss: 8874.47
[INFO 2017-06-26 18:16:34,184 main.py:47] epoch 4523, training loss: 6972.03, average training loss: 6779.33, base loss: 8875.40
[INFO 2017-06-26 18:16:34,542 main.py:47] epoch 4524, training loss: 6604.54, average training loss: 6778.97, base loss: 8874.82
[INFO 2017-06-26 18:16:34,902 main.py:47] epoch 4525, training loss: 6417.64, average training loss: 6779.01, base loss: 8875.22
[INFO 2017-06-26 18:16:35,261 main.py:47] epoch 4526, training loss: 6486.35, average training loss: 6778.48, base loss: 8875.23
[INFO 2017-06-26 18:16:35,619 main.py:47] epoch 4527, training loss: 7069.90, average training loss: 6778.40, base loss: 8875.53
[INFO 2017-06-26 18:16:35,978 main.py:47] epoch 4528, training loss: 7432.50, average training loss: 6778.60, base loss: 8875.37
[INFO 2017-06-26 18:16:36,337 main.py:47] epoch 4529, training loss: 6860.04, average training loss: 6779.28, base loss: 8876.09
[INFO 2017-06-26 18:16:36,697 main.py:47] epoch 4530, training loss: 6810.49, average training loss: 6778.82, base loss: 8875.32
[INFO 2017-06-26 18:16:37,054 main.py:47] epoch 4531, training loss: 7247.17, average training loss: 6779.04, base loss: 8875.13
[INFO 2017-06-26 18:16:37,413 main.py:47] epoch 4532, training loss: 7008.33, average training loss: 6779.83, base loss: 8876.65
[INFO 2017-06-26 18:16:37,770 main.py:47] epoch 4533, training loss: 6953.66, average training loss: 6780.44, base loss: 8877.55
[INFO 2017-06-26 18:16:38,131 main.py:47] epoch 4534, training loss: 6008.47, average training loss: 6780.22, base loss: 8877.54
[INFO 2017-06-26 18:16:38,488 main.py:47] epoch 4535, training loss: 6794.49, average training loss: 6780.56, base loss: 8877.61
[INFO 2017-06-26 18:16:38,847 main.py:47] epoch 4536, training loss: 7656.53, average training loss: 6780.74, base loss: 8877.84
[INFO 2017-06-26 18:16:39,206 main.py:47] epoch 4537, training loss: 5867.85, average training loss: 6779.92, base loss: 8876.98
[INFO 2017-06-26 18:16:39,563 main.py:47] epoch 4538, training loss: 7721.81, average training loss: 6781.13, base loss: 8878.58
[INFO 2017-06-26 18:16:39,921 main.py:47] epoch 4539, training loss: 6112.51, average training loss: 6780.26, base loss: 8877.53
[INFO 2017-06-26 18:16:40,280 main.py:47] epoch 4540, training loss: 6957.16, average training loss: 6779.79, base loss: 8876.70
[INFO 2017-06-26 18:16:40,639 main.py:47] epoch 4541, training loss: 6062.21, average training loss: 6779.43, base loss: 8876.43
[INFO 2017-06-26 18:16:41,000 main.py:47] epoch 4542, training loss: 6096.94, average training loss: 6779.04, base loss: 8874.90
[INFO 2017-06-26 18:16:41,358 main.py:47] epoch 4543, training loss: 6600.83, average training loss: 6778.22, base loss: 8873.78
[INFO 2017-06-26 18:16:41,750 main.py:47] epoch 4544, training loss: 7147.59, average training loss: 6778.48, base loss: 8874.24
[INFO 2017-06-26 18:16:42,138 main.py:47] epoch 4545, training loss: 7091.43, average training loss: 6777.88, base loss: 8873.90
[INFO 2017-06-26 18:16:42,512 main.py:47] epoch 4546, training loss: 7178.98, average training loss: 6779.11, base loss: 8876.17
[INFO 2017-06-26 18:16:42,873 main.py:47] epoch 4547, training loss: 6367.29, average training loss: 6778.46, base loss: 8875.48
[INFO 2017-06-26 18:16:43,231 main.py:47] epoch 4548, training loss: 6974.32, average training loss: 6778.20, base loss: 8875.03
[INFO 2017-06-26 18:16:43,589 main.py:47] epoch 4549, training loss: 6715.97, average training loss: 6777.89, base loss: 8875.36
[INFO 2017-06-26 18:16:43,949 main.py:47] epoch 4550, training loss: 6705.92, average training loss: 6776.41, base loss: 8873.09
[INFO 2017-06-26 18:16:44,309 main.py:47] epoch 4551, training loss: 7026.91, average training loss: 6777.21, base loss: 8873.58
[INFO 2017-06-26 18:16:44,665 main.py:47] epoch 4552, training loss: 6422.29, average training loss: 6775.64, base loss: 8871.79
[INFO 2017-06-26 18:16:45,022 main.py:47] epoch 4553, training loss: 6870.04, average training loss: 6775.29, base loss: 8871.24
[INFO 2017-06-26 18:16:45,384 main.py:47] epoch 4554, training loss: 6228.94, average training loss: 6774.62, base loss: 8871.63
[INFO 2017-06-26 18:16:45,742 main.py:47] epoch 4555, training loss: 6601.01, average training loss: 6773.53, base loss: 8869.82
[INFO 2017-06-26 18:16:46,102 main.py:47] epoch 4556, training loss: 6563.01, average training loss: 6773.68, base loss: 8870.31
[INFO 2017-06-26 18:16:46,461 main.py:47] epoch 4557, training loss: 6906.36, average training loss: 6774.35, base loss: 8872.13
[INFO 2017-06-26 18:16:46,820 main.py:47] epoch 4558, training loss: 6520.19, average training loss: 6773.85, base loss: 8871.92
[INFO 2017-06-26 18:16:47,213 main.py:47] epoch 4559, training loss: 5775.32, average training loss: 6772.94, base loss: 8870.29
[INFO 2017-06-26 18:16:47,601 main.py:47] epoch 4560, training loss: 6315.27, average training loss: 6771.81, base loss: 8869.54
[INFO 2017-06-26 18:16:47,960 main.py:47] epoch 4561, training loss: 7582.03, average training loss: 6773.43, base loss: 8872.26
[INFO 2017-06-26 18:16:48,319 main.py:47] epoch 4562, training loss: 6362.21, average training loss: 6772.73, base loss: 8871.23
[INFO 2017-06-26 18:16:48,675 main.py:47] epoch 4563, training loss: 6211.80, average training loss: 6771.69, base loss: 8869.33
[INFO 2017-06-26 18:16:49,049 main.py:47] epoch 4564, training loss: 7440.85, average training loss: 6771.99, base loss: 8870.67
[INFO 2017-06-26 18:16:49,409 main.py:47] epoch 4565, training loss: 6144.29, average training loss: 6772.13, base loss: 8871.28
[INFO 2017-06-26 18:16:49,771 main.py:47] epoch 4566, training loss: 6622.28, average training loss: 6772.44, base loss: 8871.49
[INFO 2017-06-26 18:16:50,128 main.py:47] epoch 4567, training loss: 6053.23, average training loss: 6772.34, base loss: 8871.56
[INFO 2017-06-26 18:16:50,487 main.py:47] epoch 4568, training loss: 8088.65, average training loss: 6773.92, base loss: 8874.33
[INFO 2017-06-26 18:16:50,847 main.py:47] epoch 4569, training loss: 6735.44, average training loss: 6773.50, base loss: 8874.35
[INFO 2017-06-26 18:16:51,205 main.py:47] epoch 4570, training loss: 6331.22, average training loss: 6772.52, base loss: 8872.81
[INFO 2017-06-26 18:16:51,564 main.py:47] epoch 4571, training loss: 6455.63, average training loss: 6771.21, base loss: 8870.69
[INFO 2017-06-26 18:16:51,922 main.py:47] epoch 4572, training loss: 6769.24, average training loss: 6771.89, base loss: 8872.18
[INFO 2017-06-26 18:16:52,286 main.py:47] epoch 4573, training loss: 6644.03, average training loss: 6771.89, base loss: 8872.48
[INFO 2017-06-26 18:16:52,646 main.py:47] epoch 4574, training loss: 7043.10, average training loss: 6771.42, base loss: 8872.17
[INFO 2017-06-26 18:16:53,004 main.py:47] epoch 4575, training loss: 7729.94, average training loss: 6772.98, base loss: 8874.79
[INFO 2017-06-26 18:16:53,364 main.py:47] epoch 4576, training loss: 6179.37, average training loss: 6772.08, base loss: 8873.90
[INFO 2017-06-26 18:16:53,723 main.py:47] epoch 4577, training loss: 6616.99, average training loss: 6772.52, base loss: 8874.91
[INFO 2017-06-26 18:16:54,080 main.py:47] epoch 4578, training loss: 5905.06, average training loss: 6771.85, base loss: 8874.41
[INFO 2017-06-26 18:16:54,439 main.py:47] epoch 4579, training loss: 6058.05, average training loss: 6770.39, base loss: 8872.93
[INFO 2017-06-26 18:16:54,796 main.py:47] epoch 4580, training loss: 6896.67, average training loss: 6770.00, base loss: 8872.76
[INFO 2017-06-26 18:16:55,154 main.py:47] epoch 4581, training loss: 6624.22, average training loss: 6769.78, base loss: 8871.31
[INFO 2017-06-26 18:16:55,512 main.py:47] epoch 4582, training loss: 6501.37, average training loss: 6769.57, base loss: 8871.20
[INFO 2017-06-26 18:16:55,870 main.py:47] epoch 4583, training loss: 6315.30, average training loss: 6769.26, base loss: 8870.78
[INFO 2017-06-26 18:16:56,230 main.py:47] epoch 4584, training loss: 6472.57, average training loss: 6768.52, base loss: 8870.05
[INFO 2017-06-26 18:16:56,590 main.py:47] epoch 4585, training loss: 6830.63, average training loss: 6768.57, base loss: 8870.05
[INFO 2017-06-26 18:16:56,948 main.py:47] epoch 4586, training loss: 6520.94, average training loss: 6768.60, base loss: 8870.27
[INFO 2017-06-26 18:16:57,304 main.py:47] epoch 4587, training loss: 5778.17, average training loss: 6767.21, base loss: 8867.83
[INFO 2017-06-26 18:16:57,664 main.py:47] epoch 4588, training loss: 6936.72, average training loss: 6767.49, base loss: 8868.47
[INFO 2017-06-26 18:16:58,022 main.py:47] epoch 4589, training loss: 7807.15, average training loss: 6769.01, base loss: 8871.09
[INFO 2017-06-26 18:16:58,382 main.py:47] epoch 4590, training loss: 6746.74, average training loss: 6768.82, base loss: 8870.35
[INFO 2017-06-26 18:16:58,741 main.py:47] epoch 4591, training loss: 6717.67, average training loss: 6769.05, base loss: 8871.16
[INFO 2017-06-26 18:16:59,101 main.py:47] epoch 4592, training loss: 5849.97, average training loss: 6768.20, base loss: 8869.37
[INFO 2017-06-26 18:16:59,461 main.py:47] epoch 4593, training loss: 6952.03, average training loss: 6769.06, base loss: 8871.33
[INFO 2017-06-26 18:16:59,815 main.py:47] epoch 4594, training loss: 7309.66, average training loss: 6769.63, base loss: 8873.29
[INFO 2017-06-26 18:17:00,189 main.py:47] epoch 4595, training loss: 6694.18, average training loss: 6770.47, base loss: 8874.56
[INFO 2017-06-26 18:17:00,552 main.py:47] epoch 4596, training loss: 7321.29, average training loss: 6770.80, base loss: 8875.73
[INFO 2017-06-26 18:17:00,928 main.py:47] epoch 4597, training loss: 7213.10, average training loss: 6772.28, base loss: 8878.15
[INFO 2017-06-26 18:17:01,288 main.py:47] epoch 4598, training loss: 6722.42, average training loss: 6771.75, base loss: 8877.71
[INFO 2017-06-26 18:17:01,647 main.py:47] epoch 4599, training loss: 6504.70, average training loss: 6769.85, base loss: 8875.49
[INFO 2017-06-26 18:17:01,647 main.py:49] epoch 4599, testing
[INFO 2017-06-26 18:17:05,993 main.py:100] average testing loss: 6694.55, base loss: 8794.54
[INFO 2017-06-26 18:17:06,017 main.py:73] current best accuracy: 6364.27
[INFO 2017-06-26 18:17:06,376 main.py:47] epoch 4600, training loss: 6742.64, average training loss: 6769.20, base loss: 8874.29
[INFO 2017-06-26 18:17:06,735 main.py:47] epoch 4601, training loss: 6278.12, average training loss: 6767.93, base loss: 8871.84
[INFO 2017-06-26 18:17:07,092 main.py:47] epoch 4602, training loss: 5948.27, average training loss: 6765.72, base loss: 8868.37
[INFO 2017-06-26 18:17:07,450 main.py:47] epoch 4603, training loss: 6855.35, average training loss: 6766.64, base loss: 8869.93
[INFO 2017-06-26 18:17:07,811 main.py:47] epoch 4604, training loss: 7544.06, average training loss: 6767.39, base loss: 8870.51
[INFO 2017-06-26 18:17:08,170 main.py:47] epoch 4605, training loss: 6639.09, average training loss: 6766.98, base loss: 8870.05
[INFO 2017-06-26 18:17:08,529 main.py:47] epoch 4606, training loss: 6778.82, average training loss: 6767.56, base loss: 8870.90
[INFO 2017-06-26 18:17:08,887 main.py:47] epoch 4607, training loss: 6955.50, average training loss: 6767.73, base loss: 8871.13
[INFO 2017-06-26 18:17:09,280 main.py:47] epoch 4608, training loss: 7483.13, average training loss: 6768.27, base loss: 8871.26
[INFO 2017-06-26 18:17:09,665 main.py:47] epoch 4609, training loss: 6719.38, average training loss: 6767.59, base loss: 8870.22
[INFO 2017-06-26 18:17:10,032 main.py:47] epoch 4610, training loss: 7176.34, average training loss: 6767.79, base loss: 8870.59
[INFO 2017-06-26 18:17:10,393 main.py:47] epoch 4611, training loss: 7932.28, average training loss: 6769.58, base loss: 8873.11
[INFO 2017-06-26 18:17:10,754 main.py:47] epoch 4612, training loss: 7049.02, average training loss: 6769.22, base loss: 8873.36
[INFO 2017-06-26 18:17:11,115 main.py:47] epoch 4613, training loss: 6556.19, average training loss: 6769.62, base loss: 8873.73
[INFO 2017-06-26 18:17:11,475 main.py:47] epoch 4614, training loss: 6473.53, average training loss: 6768.95, base loss: 8872.76
[INFO 2017-06-26 18:17:11,834 main.py:47] epoch 4615, training loss: 6248.82, average training loss: 6768.02, base loss: 8871.35
[INFO 2017-06-26 18:17:12,195 main.py:47] epoch 4616, training loss: 6739.52, average training loss: 6767.39, base loss: 8870.22
[INFO 2017-06-26 18:17:12,557 main.py:47] epoch 4617, training loss: 5615.85, average training loss: 6766.26, base loss: 8867.95
