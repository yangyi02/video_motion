[INFO 2017-06-26 17:28:01,087 main.py:123] Namespace(batch_size=32, display=False, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/robot-64', test_epoch=10, test_interval=100, train=True, train_dir='/home/yi/Downloads/robot-64', train_epoch=10000)
[INFO 2017-06-26 17:28:05,917 main.py:47] epoch 0, training loss: 107752.03, average training loss: 107752.03, base loss: 8960.31
[INFO 2017-06-26 17:28:06,298 main.py:47] epoch 1, training loss: 86278.96, average training loss: 97015.50, base loss: 8723.18
[INFO 2017-06-26 17:28:06,611 main.py:47] epoch 2, training loss: 72261.66, average training loss: 88764.22, base loss: 8373.82
[INFO 2017-06-26 17:28:06,900 main.py:47] epoch 3, training loss: 64219.04, average training loss: 82627.92, base loss: 8356.74
[INFO 2017-06-26 17:28:07,188 main.py:47] epoch 4, training loss: 56086.97, average training loss: 77319.73, base loss: 8490.55
[INFO 2017-06-26 17:28:07,472 main.py:47] epoch 5, training loss: 49518.86, average training loss: 72686.25, base loss: 8434.89
[INFO 2017-06-26 17:28:07,756 main.py:47] epoch 6, training loss: 41383.78, average training loss: 68214.47, base loss: 8381.22
[INFO 2017-06-26 17:28:08,041 main.py:47] epoch 7, training loss: 36818.66, average training loss: 64289.99, base loss: 8369.61
[INFO 2017-06-26 17:28:08,324 main.py:47] epoch 8, training loss: 33602.28, average training loss: 60880.25, base loss: 8372.76
[INFO 2017-06-26 17:28:08,609 main.py:47] epoch 9, training loss: 28351.83, average training loss: 57627.41, base loss: 8438.93
[INFO 2017-06-26 17:28:08,894 main.py:47] epoch 10, training loss: 24827.59, average training loss: 54645.61, base loss: 8413.54
[INFO 2017-06-26 17:28:09,179 main.py:47] epoch 11, training loss: 21735.71, average training loss: 51903.11, base loss: 8373.28
[INFO 2017-06-26 17:28:09,463 main.py:47] epoch 12, training loss: 19661.10, average training loss: 49422.96, base loss: 8352.59
[INFO 2017-06-26 17:28:09,751 main.py:47] epoch 13, training loss: 18197.08, average training loss: 47192.54, base loss: 8406.01
[INFO 2017-06-26 17:28:10,037 main.py:47] epoch 14, training loss: 16231.25, average training loss: 45128.45, base loss: 8426.97
[INFO 2017-06-26 17:28:10,323 main.py:47] epoch 15, training loss: 14031.81, average training loss: 43184.91, base loss: 8380.40
[INFO 2017-06-26 17:28:10,642 main.py:47] epoch 16, training loss: 13349.19, average training loss: 41429.87, base loss: 8393.21
[INFO 2017-06-26 17:28:10,933 main.py:47] epoch 17, training loss: 12816.81, average training loss: 39840.26, base loss: 8423.74
[INFO 2017-06-26 17:28:11,226 main.py:47] epoch 18, training loss: 10663.12, average training loss: 38304.62, base loss: 8365.23
[INFO 2017-06-26 17:28:11,516 main.py:47] epoch 19, training loss: 11707.59, average training loss: 36974.77, base loss: 8398.50
[INFO 2017-06-26 17:28:11,803 main.py:47] epoch 20, training loss: 9930.29, average training loss: 35686.93, base loss: 8362.19
[INFO 2017-06-26 17:28:12,085 main.py:47] epoch 21, training loss: 11579.72, average training loss: 34591.15, base loss: 8435.43
[INFO 2017-06-26 17:28:12,370 main.py:47] epoch 22, training loss: 9102.07, average training loss: 33482.93, base loss: 8408.13
[INFO 2017-06-26 17:28:12,658 main.py:47] epoch 23, training loss: 9911.69, average training loss: 32500.80, base loss: 8430.15
[INFO 2017-06-26 17:28:12,941 main.py:47] epoch 24, training loss: 8886.33, average training loss: 31556.22, base loss: 8416.02
[INFO 2017-06-26 17:28:13,224 main.py:47] epoch 25, training loss: 8786.18, average training loss: 30680.45, base loss: 8406.63
[INFO 2017-06-26 17:28:13,508 main.py:47] epoch 26, training loss: 8562.45, average training loss: 29861.26, base loss: 8392.42
[INFO 2017-06-26 17:28:13,793 main.py:47] epoch 27, training loss: 9608.00, average training loss: 29137.93, base loss: 8421.77
[INFO 2017-06-26 17:28:14,077 main.py:47] epoch 28, training loss: 9364.09, average training loss: 28456.07, base loss: 8443.40
[INFO 2017-06-26 17:28:14,366 main.py:47] epoch 29, training loss: 7584.36, average training loss: 27760.35, base loss: 8403.50
[INFO 2017-06-26 17:28:14,657 main.py:47] epoch 30, training loss: 10119.17, average training loss: 27191.28, base loss: 8458.19
[INFO 2017-06-26 17:28:14,950 main.py:47] epoch 31, training loss: 9426.43, average training loss: 26636.13, base loss: 8484.82
[INFO 2017-06-26 17:28:15,251 main.py:47] epoch 32, training loss: 8465.63, average training loss: 26085.51, base loss: 8478.23
[INFO 2017-06-26 17:28:15,564 main.py:47] epoch 33, training loss: 8669.71, average training loss: 25573.28, base loss: 8481.75
[INFO 2017-06-26 17:28:15,908 main.py:47] epoch 34, training loss: 8503.18, average training loss: 25085.56, base loss: 8480.94
[INFO 2017-06-26 17:28:16,263 main.py:47] epoch 35, training loss: 7370.17, average training loss: 24593.47, base loss: 8446.96
[INFO 2017-06-26 17:28:16,625 main.py:47] epoch 36, training loss: 8347.90, average training loss: 24154.40, base loss: 8443.31
[INFO 2017-06-26 17:28:16,981 main.py:47] epoch 37, training loss: 8716.29, average training loss: 23748.13, base loss: 8451.24
[INFO 2017-06-26 17:28:17,335 main.py:47] epoch 38, training loss: 8373.63, average training loss: 23353.91, base loss: 8449.09
[INFO 2017-06-26 17:28:17,689 main.py:47] epoch 39, training loss: 8300.83, average training loss: 22977.59, base loss: 8443.53
[INFO 2017-06-26 17:28:18,044 main.py:47] epoch 40, training loss: 8605.09, average training loss: 22627.04, base loss: 8446.88
[INFO 2017-06-26 17:28:18,399 main.py:47] epoch 41, training loss: 7900.36, average training loss: 22276.40, base loss: 8433.02
[INFO 2017-06-26 17:28:18,753 main.py:47] epoch 42, training loss: 9658.37, average training loss: 21982.96, base loss: 8463.89
[INFO 2017-06-26 17:28:19,105 main.py:47] epoch 43, training loss: 9011.46, average training loss: 21688.15, base loss: 8480.03
[INFO 2017-06-26 17:28:19,460 main.py:47] epoch 44, training loss: 10064.85, average training loss: 21429.86, base loss: 8517.74
[INFO 2017-06-26 17:28:19,814 main.py:47] epoch 45, training loss: 7843.72, average training loss: 21134.51, base loss: 8505.13
[INFO 2017-06-26 17:28:20,171 main.py:47] epoch 46, training loss: 8667.43, average training loss: 20869.25, base loss: 8508.51
[INFO 2017-06-26 17:28:20,524 main.py:47] epoch 47, training loss: 8442.47, average training loss: 20610.36, base loss: 8510.80
[INFO 2017-06-26 17:28:20,891 main.py:47] epoch 48, training loss: 9737.39, average training loss: 20388.46, base loss: 8540.11
[INFO 2017-06-26 17:28:21,245 main.py:47] epoch 49, training loss: 8536.72, average training loss: 20151.43, base loss: 8542.29
[INFO 2017-06-26 17:28:21,600 main.py:47] epoch 50, training loss: 8503.47, average training loss: 19923.03, base loss: 8542.00
[INFO 2017-06-26 17:28:21,953 main.py:47] epoch 51, training loss: 8708.24, average training loss: 19707.37, base loss: 8547.67
[INFO 2017-06-26 17:28:22,308 main.py:47] epoch 52, training loss: 9391.87, average training loss: 19512.73, base loss: 8569.38
[INFO 2017-06-26 17:28:22,662 main.py:47] epoch 53, training loss: 8724.47, average training loss: 19312.95, base loss: 8574.39
[INFO 2017-06-26 17:28:23,016 main.py:47] epoch 54, training loss: 7303.82, average training loss: 19094.60, base loss: 8552.22
[INFO 2017-06-26 17:28:23,370 main.py:47] epoch 55, training loss: 9173.03, average training loss: 18917.43, base loss: 8568.58
[INFO 2017-06-26 17:28:23,725 main.py:47] epoch 56, training loss: 8331.02, average training loss: 18731.71, base loss: 8565.97
[INFO 2017-06-26 17:28:24,079 main.py:47] epoch 57, training loss: 8017.65, average training loss: 18546.98, base loss: 8559.29
[INFO 2017-06-26 17:28:24,435 main.py:47] epoch 58, training loss: 8971.36, average training loss: 18384.68, base loss: 8570.87
[INFO 2017-06-26 17:28:24,789 main.py:47] epoch 59, training loss: 7984.48, average training loss: 18211.35, base loss: 8563.78
[INFO 2017-06-26 17:28:25,145 main.py:47] epoch 60, training loss: 8919.83, average training loss: 18059.03, base loss: 8572.99
[INFO 2017-06-26 17:28:25,499 main.py:47] epoch 61, training loss: 8125.72, average training loss: 17898.81, base loss: 8570.00
[INFO 2017-06-26 17:28:25,854 main.py:47] epoch 62, training loss: 8353.21, average training loss: 17747.29, base loss: 8570.33
[INFO 2017-06-26 17:28:26,215 main.py:47] epoch 63, training loss: 7483.35, average training loss: 17586.92, base loss: 8553.94
[INFO 2017-06-26 17:28:26,570 main.py:47] epoch 64, training loss: 7995.42, average training loss: 17439.36, base loss: 8548.24
[INFO 2017-06-26 17:28:26,928 main.py:47] epoch 65, training loss: 6701.00, average training loss: 17276.66, base loss: 8522.01
[INFO 2017-06-26 17:28:27,282 main.py:47] epoch 66, training loss: 8415.78, average training loss: 17144.40, base loss: 8523.99
[INFO 2017-06-26 17:28:27,637 main.py:47] epoch 67, training loss: 8521.53, average training loss: 17017.60, base loss: 8527.28
[INFO 2017-06-26 17:28:27,993 main.py:47] epoch 68, training loss: 8057.78, average training loss: 16887.74, base loss: 8523.43
[INFO 2017-06-26 17:28:28,349 main.py:47] epoch 69, training loss: 8854.38, average training loss: 16772.98, base loss: 8531.60
[INFO 2017-06-26 17:28:28,706 main.py:47] epoch 70, training loss: 9707.23, average training loss: 16673.46, base loss: 8551.79
[INFO 2017-06-26 17:28:29,062 main.py:47] epoch 71, training loss: 8451.42, average training loss: 16559.27, base loss: 8552.50
[INFO 2017-06-26 17:28:29,418 main.py:47] epoch 72, training loss: 8213.61, average training loss: 16444.94, base loss: 8551.31
[INFO 2017-06-26 17:28:29,775 main.py:47] epoch 73, training loss: 8945.92, average training loss: 16343.61, base loss: 8559.35
[INFO 2017-06-26 17:28:30,129 main.py:47] epoch 74, training loss: 7837.13, average training loss: 16230.19, base loss: 8553.49
[INFO 2017-06-26 17:28:30,484 main.py:47] epoch 75, training loss: 8101.14, average training loss: 16123.23, base loss: 8551.18
[INFO 2017-06-26 17:28:30,838 main.py:47] epoch 76, training loss: 8541.19, average training loss: 16024.76, base loss: 8554.25
[INFO 2017-06-26 17:28:31,222 main.py:47] epoch 77, training loss: 8375.43, average training loss: 15926.69, base loss: 8555.17
[INFO 2017-06-26 17:28:31,579 main.py:47] epoch 78, training loss: 8841.71, average training loss: 15837.01, base loss: 8563.20
[INFO 2017-06-26 17:28:31,942 main.py:47] epoch 79, training loss: 8725.67, average training loss: 15748.11, base loss: 8569.77
[INFO 2017-06-26 17:28:32,304 main.py:47] epoch 80, training loss: 8234.58, average training loss: 15655.35, base loss: 8570.04
[INFO 2017-06-26 17:28:32,658 main.py:47] epoch 81, training loss: 9158.12, average training loss: 15576.12, base loss: 8583.20
[INFO 2017-06-26 17:28:33,010 main.py:47] epoch 82, training loss: 8346.07, average training loss: 15489.01, base loss: 8584.71
[INFO 2017-06-26 17:28:33,363 main.py:47] epoch 83, training loss: 7919.01, average training loss: 15398.89, base loss: 8578.68
[INFO 2017-06-26 17:28:33,717 main.py:47] epoch 84, training loss: 9352.08, average training loss: 15327.75, base loss: 8592.17
[INFO 2017-06-26 17:28:34,072 main.py:47] epoch 85, training loss: 7306.86, average training loss: 15234.49, base loss: 8580.67
[INFO 2017-06-26 17:28:34,426 main.py:47] epoch 86, training loss: 7355.36, average training loss: 15143.92, base loss: 8569.14
[INFO 2017-06-26 17:28:34,781 main.py:47] epoch 87, training loss: 8088.81, average training loss: 15063.75, base loss: 8566.59
[INFO 2017-06-26 17:28:35,135 main.py:47] epoch 88, training loss: 7956.78, average training loss: 14983.90, base loss: 8562.24
[INFO 2017-06-26 17:28:35,489 main.py:47] epoch 89, training loss: 8095.53, average training loss: 14907.36, base loss: 8560.29
[INFO 2017-06-26 17:28:35,843 main.py:47] epoch 90, training loss: 8732.94, average training loss: 14839.51, base loss: 8566.59
[INFO 2017-06-26 17:28:36,211 main.py:47] epoch 91, training loss: 8037.25, average training loss: 14765.57, base loss: 8564.41
[INFO 2017-06-26 17:28:36,564 main.py:47] epoch 92, training loss: 8865.31, average training loss: 14702.13, base loss: 8570.57
[INFO 2017-06-26 17:28:36,918 main.py:47] epoch 93, training loss: 7429.50, average training loss: 14624.76, base loss: 8560.73
[INFO 2017-06-26 17:28:37,272 main.py:47] epoch 94, training loss: 8071.94, average training loss: 14555.78, base loss: 8559.45
[INFO 2017-06-26 17:28:37,628 main.py:47] epoch 95, training loss: 8896.68, average training loss: 14496.83, base loss: 8568.59
[INFO 2017-06-26 17:28:37,982 main.py:47] epoch 96, training loss: 9398.39, average training loss: 14444.27, base loss: 8583.08
[INFO 2017-06-26 17:28:38,335 main.py:47] epoch 97, training loss: 7557.52, average training loss: 14374.00, base loss: 8575.40
[INFO 2017-06-26 17:28:38,689 main.py:47] epoch 98, training loss: 7875.55, average training loss: 14308.36, base loss: 8571.52
[INFO 2017-06-26 17:28:39,044 main.py:47] epoch 99, training loss: 7766.15, average training loss: 14242.94, base loss: 8566.64
[INFO 2017-06-26 17:28:39,044 main.py:49] epoch 99, testing
[INFO 2017-06-26 17:28:43,204 main.py:100] average testing loss: 8461.03, base loss: 8874.01
[INFO 2017-06-26 17:28:43,227 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:28:43,233 main.py:73] current best accuracy: 8461.03
[INFO 2017-06-26 17:28:43,586 main.py:47] epoch 100, training loss: 7347.37, average training loss: 14174.66, base loss: 8558.03
[INFO 2017-06-26 17:28:43,940 main.py:47] epoch 101, training loss: 9801.59, average training loss: 14131.79, base loss: 8575.33
[INFO 2017-06-26 17:28:44,292 main.py:47] epoch 102, training loss: 7955.68, average training loss: 14071.83, base loss: 8570.96
[INFO 2017-06-26 17:28:44,649 main.py:47] epoch 103, training loss: 8152.37, average training loss: 14014.91, base loss: 8571.58
[INFO 2017-06-26 17:28:45,005 main.py:47] epoch 104, training loss: 8521.31, average training loss: 13962.59, base loss: 8574.21
[INFO 2017-06-26 17:28:45,378 main.py:47] epoch 105, training loss: 8079.37, average training loss: 13907.09, base loss: 8572.88
[INFO 2017-06-26 17:28:45,739 main.py:47] epoch 106, training loss: 7653.94, average training loss: 13848.65, base loss: 8566.81
[INFO 2017-06-26 17:28:46,098 main.py:47] epoch 107, training loss: 7905.19, average training loss: 13793.62, base loss: 8564.44
[INFO 2017-06-26 17:28:46,455 main.py:47] epoch 108, training loss: 7830.03, average training loss: 13738.90, base loss: 8560.47
[INFO 2017-06-26 17:28:46,813 main.py:47] epoch 109, training loss: 8313.42, average training loss: 13689.58, base loss: 8562.41
[INFO 2017-06-26 17:28:47,194 main.py:47] epoch 110, training loss: 7551.23, average training loss: 13634.28, base loss: 8555.91
[INFO 2017-06-26 17:28:47,557 main.py:47] epoch 111, training loss: 8795.29, average training loss: 13591.07, base loss: 8561.76
[INFO 2017-06-26 17:28:47,915 main.py:47] epoch 112, training loss: 7666.61, average training loss: 13538.65, base loss: 8557.74
[INFO 2017-06-26 17:28:48,271 main.py:47] epoch 113, training loss: 8374.31, average training loss: 13493.34, base loss: 8559.49
[INFO 2017-06-26 17:28:48,629 main.py:47] epoch 114, training loss: 7659.97, average training loss: 13442.62, base loss: 8553.27
[INFO 2017-06-26 17:28:48,986 main.py:47] epoch 115, training loss: 7157.22, average training loss: 13388.44, base loss: 8544.70
[INFO 2017-06-26 17:28:49,341 main.py:47] epoch 116, training loss: 8339.81, average training loss: 13345.28, base loss: 8547.25
[INFO 2017-06-26 17:28:49,698 main.py:47] epoch 117, training loss: 7435.11, average training loss: 13295.20, base loss: 8540.18
[INFO 2017-06-26 17:28:50,055 main.py:47] epoch 118, training loss: 7934.12, average training loss: 13250.15, base loss: 8538.32
[INFO 2017-06-26 17:28:50,412 main.py:47] epoch 119, training loss: 10456.52, average training loss: 13226.87, base loss: 8559.57
[INFO 2017-06-26 17:28:50,766 main.py:47] epoch 120, training loss: 7767.02, average training loss: 13181.74, base loss: 8555.20
[INFO 2017-06-26 17:28:51,123 main.py:47] epoch 121, training loss: 9595.21, average training loss: 13152.35, base loss: 8568.16
[INFO 2017-06-26 17:28:51,478 main.py:47] epoch 122, training loss: 7255.63, average training loss: 13104.41, base loss: 8559.72
[INFO 2017-06-26 17:28:51,832 main.py:47] epoch 123, training loss: 7471.79, average training loss: 13058.98, base loss: 8554.30
[INFO 2017-06-26 17:28:52,200 main.py:47] epoch 124, training loss: 6969.51, average training loss: 13010.27, base loss: 8543.42
[INFO 2017-06-26 17:28:52,556 main.py:47] epoch 125, training loss: 9103.22, average training loss: 12979.26, base loss: 8552.92
[INFO 2017-06-26 17:28:52,911 main.py:47] epoch 126, training loss: 9034.85, average training loss: 12948.20, base loss: 8562.41
[INFO 2017-06-26 17:28:53,266 main.py:47] epoch 127, training loss: 8016.21, average training loss: 12909.67, base loss: 8562.74
[INFO 2017-06-26 17:28:53,621 main.py:47] epoch 128, training loss: 8559.73, average training loss: 12875.95, base loss: 8566.23
[INFO 2017-06-26 17:28:53,977 main.py:47] epoch 129, training loss: 6996.92, average training loss: 12830.72, base loss: 8555.75
[INFO 2017-06-26 17:28:54,331 main.py:47] epoch 130, training loss: 8319.75, average training loss: 12796.29, base loss: 8557.45
[INFO 2017-06-26 17:28:54,685 main.py:47] epoch 131, training loss: 8395.25, average training loss: 12762.95, base loss: 8558.79
[INFO 2017-06-26 17:28:55,043 main.py:47] epoch 132, training loss: 8172.44, average training loss: 12728.43, base loss: 8559.48
[INFO 2017-06-26 17:28:55,398 main.py:47] epoch 133, training loss: 8228.47, average training loss: 12694.85, base loss: 8560.54
[INFO 2017-06-26 17:28:55,753 main.py:47] epoch 134, training loss: 6978.77, average training loss: 12652.51, base loss: 8551.37
[INFO 2017-06-26 17:28:56,107 main.py:47] epoch 135, training loss: 8622.39, average training loss: 12622.88, base loss: 8555.67
[INFO 2017-06-26 17:28:56,463 main.py:47] epoch 136, training loss: 7945.72, average training loss: 12588.74, base loss: 8556.01
[INFO 2017-06-26 17:28:56,818 main.py:47] epoch 137, training loss: 7110.39, average training loss: 12549.04, base loss: 8549.00
[INFO 2017-06-26 17:28:57,191 main.py:47] epoch 138, training loss: 7672.37, average training loss: 12513.95, base loss: 8545.83
[INFO 2017-06-26 17:28:57,550 main.py:47] epoch 139, training loss: 7537.70, average training loss: 12478.41, base loss: 8541.32
[INFO 2017-06-26 17:28:57,936 main.py:47] epoch 140, training loss: 7328.92, average training loss: 12441.89, base loss: 8536.83
[INFO 2017-06-26 17:28:58,337 main.py:47] epoch 141, training loss: 8399.38, average training loss: 12413.42, base loss: 8539.65
[INFO 2017-06-26 17:28:58,718 main.py:47] epoch 142, training loss: 7327.01, average training loss: 12377.85, base loss: 8534.68
[INFO 2017-06-26 17:28:59,076 main.py:47] epoch 143, training loss: 8856.53, average training loss: 12353.40, base loss: 8541.10
[INFO 2017-06-26 17:28:59,432 main.py:47] epoch 144, training loss: 8065.65, average training loss: 12323.83, base loss: 8542.54
[INFO 2017-06-26 17:28:59,795 main.py:47] epoch 145, training loss: 7984.52, average training loss: 12294.11, base loss: 8543.50
[INFO 2017-06-26 17:29:00,151 main.py:47] epoch 146, training loss: 7649.62, average training loss: 12262.51, base loss: 8540.79
[INFO 2017-06-26 17:29:00,510 main.py:47] epoch 147, training loss: 7857.10, average training loss: 12232.74, base loss: 8538.12
[INFO 2017-06-26 17:29:00,866 main.py:47] epoch 148, training loss: 8906.34, average training loss: 12210.42, base loss: 8544.94
[INFO 2017-06-26 17:29:01,222 main.py:47] epoch 149, training loss: 6694.08, average training loss: 12173.64, base loss: 8534.72
[INFO 2017-06-26 17:29:01,579 main.py:47] epoch 150, training loss: 7962.60, average training loss: 12145.76, base loss: 8535.27
[INFO 2017-06-26 17:29:01,934 main.py:47] epoch 151, training loss: 8761.84, average training loss: 12123.49, base loss: 8540.50
[INFO 2017-06-26 17:29:02,289 main.py:47] epoch 152, training loss: 7231.57, average training loss: 12091.52, base loss: 8535.33
[INFO 2017-06-26 17:29:02,646 main.py:47] epoch 153, training loss: 6837.68, average training loss: 12057.40, base loss: 8526.34
[INFO 2017-06-26 17:29:03,026 main.py:47] epoch 154, training loss: 8837.68, average training loss: 12036.63, base loss: 8531.94
[INFO 2017-06-26 17:29:03,392 main.py:47] epoch 155, training loss: 8723.60, average training loss: 12015.39, base loss: 8538.39
[INFO 2017-06-26 17:29:03,756 main.py:47] epoch 156, training loss: 7314.37, average training loss: 11985.45, base loss: 8534.44
[INFO 2017-06-26 17:29:04,116 main.py:47] epoch 157, training loss: 6681.71, average training loss: 11951.88, base loss: 8525.71
[INFO 2017-06-26 17:29:04,488 main.py:47] epoch 158, training loss: 7629.55, average training loss: 11924.70, base loss: 8524.78
[INFO 2017-06-26 17:29:04,871 main.py:47] epoch 159, training loss: 7019.96, average training loss: 11894.04, base loss: 8516.35
[INFO 2017-06-26 17:29:05,251 main.py:47] epoch 160, training loss: 7713.19, average training loss: 11868.08, base loss: 8515.12
[INFO 2017-06-26 17:29:05,637 main.py:47] epoch 161, training loss: 7831.11, average training loss: 11843.16, base loss: 8514.68
[INFO 2017-06-26 17:29:06,014 main.py:47] epoch 162, training loss: 8739.34, average training loss: 11824.11, base loss: 8520.50
[INFO 2017-06-26 17:29:06,382 main.py:47] epoch 163, training loss: 7346.72, average training loss: 11796.81, base loss: 8517.56
[INFO 2017-06-26 17:29:06,793 main.py:47] epoch 164, training loss: 7128.41, average training loss: 11768.52, base loss: 8511.66
[INFO 2017-06-26 17:29:07,150 main.py:47] epoch 165, training loss: 7383.65, average training loss: 11742.11, base loss: 8508.80
[INFO 2017-06-26 17:29:07,507 main.py:47] epoch 166, training loss: 8090.31, average training loss: 11720.24, base loss: 8510.27
[INFO 2017-06-26 17:29:07,874 main.py:47] epoch 167, training loss: 7790.16, average training loss: 11696.85, base loss: 8508.20
[INFO 2017-06-26 17:29:08,239 main.py:47] epoch 168, training loss: 7846.79, average training loss: 11674.06, base loss: 8507.61
[INFO 2017-06-26 17:29:08,604 main.py:47] epoch 169, training loss: 8361.09, average training loss: 11654.58, base loss: 8509.94
[INFO 2017-06-26 17:29:08,959 main.py:47] epoch 170, training loss: 8529.25, average training loss: 11636.30, base loss: 8513.70
[INFO 2017-06-26 17:29:09,313 main.py:47] epoch 171, training loss: 8937.40, average training loss: 11620.61, base loss: 8519.33
[INFO 2017-06-26 17:29:09,701 main.py:47] epoch 172, training loss: 7803.18, average training loss: 11598.54, base loss: 8519.05
[INFO 2017-06-26 17:29:10,072 main.py:47] epoch 173, training loss: 7737.97, average training loss: 11576.35, base loss: 8518.11
[INFO 2017-06-26 17:29:10,508 main.py:47] epoch 174, training loss: 6984.32, average training loss: 11550.11, base loss: 8511.29
[INFO 2017-06-26 17:29:10,931 main.py:47] epoch 175, training loss: 7194.54, average training loss: 11525.37, base loss: 8507.74
[INFO 2017-06-26 17:29:11,307 main.py:47] epoch 176, training loss: 7422.67, average training loss: 11502.19, base loss: 8506.56
[INFO 2017-06-26 17:29:11,748 main.py:47] epoch 177, training loss: 8558.57, average training loss: 11485.65, base loss: 8512.49
[INFO 2017-06-26 17:29:12,147 main.py:47] epoch 178, training loss: 6845.31, average training loss: 11459.73, base loss: 8504.85
[INFO 2017-06-26 17:29:12,550 main.py:47] epoch 179, training loss: 8053.03, average training loss: 11440.80, base loss: 8507.20
[INFO 2017-06-26 17:29:12,973 main.py:47] epoch 180, training loss: 7874.68, average training loss: 11421.10, base loss: 8506.31
[INFO 2017-06-26 17:29:13,365 main.py:47] epoch 181, training loss: 7445.17, average training loss: 11399.25, base loss: 8502.91
[INFO 2017-06-26 17:29:13,822 main.py:47] epoch 182, training loss: 6688.82, average training loss: 11373.51, base loss: 8496.03
[INFO 2017-06-26 17:29:14,203 main.py:47] epoch 183, training loss: 9446.19, average training loss: 11363.04, base loss: 8505.78
[INFO 2017-06-26 17:29:14,632 main.py:47] epoch 184, training loss: 8103.84, average training loss: 11345.42, base loss: 8508.07
[INFO 2017-06-26 17:29:14,991 main.py:47] epoch 185, training loss: 6573.45, average training loss: 11319.76, base loss: 8499.88
[INFO 2017-06-26 17:29:15,408 main.py:47] epoch 186, training loss: 8616.28, average training loss: 11305.31, base loss: 8505.11
[INFO 2017-06-26 17:29:15,768 main.py:47] epoch 187, training loss: 7572.71, average training loss: 11285.45, base loss: 8504.06
[INFO 2017-06-26 17:29:16,135 main.py:47] epoch 188, training loss: 9787.00, average training loss: 11277.52, base loss: 8516.02
[INFO 2017-06-26 17:29:16,559 main.py:47] epoch 189, training loss: 7598.13, average training loss: 11258.16, base loss: 8514.60
[INFO 2017-06-26 17:29:16,930 main.py:47] epoch 190, training loss: 8038.18, average training loss: 11241.30, base loss: 8515.70
[INFO 2017-06-26 17:29:17,324 main.py:47] epoch 191, training loss: 8025.78, average training loss: 11224.55, base loss: 8516.88
[INFO 2017-06-26 17:29:17,695 main.py:47] epoch 192, training loss: 8199.70, average training loss: 11208.88, base loss: 8518.14
[INFO 2017-06-26 17:29:18,097 main.py:47] epoch 193, training loss: 8148.25, average training loss: 11193.10, base loss: 8520.59
[INFO 2017-06-26 17:29:18,491 main.py:47] epoch 194, training loss: 6868.75, average training loss: 11170.93, base loss: 8514.44
[INFO 2017-06-26 17:29:18,925 main.py:47] epoch 195, training loss: 6897.46, average training loss: 11149.12, base loss: 8509.02
[INFO 2017-06-26 17:29:19,320 main.py:47] epoch 196, training loss: 8273.22, average training loss: 11134.53, base loss: 8511.08
[INFO 2017-06-26 17:29:19,767 main.py:47] epoch 197, training loss: 7486.47, average training loss: 11116.10, base loss: 8510.47
[INFO 2017-06-26 17:29:20,125 main.py:47] epoch 198, training loss: 8826.99, average training loss: 11104.60, base loss: 8516.33
[INFO 2017-06-26 17:29:20,646 main.py:47] epoch 199, training loss: 6662.60, average training loss: 11082.39, base loss: 8509.85
[INFO 2017-06-26 17:29:20,646 main.py:49] epoch 199, testing
[INFO 2017-06-26 17:29:25,388 main.py:100] average testing loss: 8130.12, base loss: 8967.88
[INFO 2017-06-26 17:29:25,413 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:29:25,421 main.py:73] current best accuracy: 8130.12
[INFO 2017-06-26 17:29:25,883 main.py:47] epoch 200, training loss: 7781.61, average training loss: 11065.97, base loss: 8510.80
[INFO 2017-06-26 17:29:26,247 main.py:47] epoch 201, training loss: 8147.36, average training loss: 11051.52, base loss: 8513.32
[INFO 2017-06-26 17:29:26,609 main.py:47] epoch 202, training loss: 8262.82, average training loss: 11037.78, base loss: 8515.48
[INFO 2017-06-26 17:29:26,972 main.py:47] epoch 203, training loss: 6037.79, average training loss: 11013.27, base loss: 8503.95
[INFO 2017-06-26 17:29:27,333 main.py:47] epoch 204, training loss: 7924.43, average training loss: 10998.20, base loss: 8504.28
[INFO 2017-06-26 17:29:27,691 main.py:47] epoch 205, training loss: 7327.47, average training loss: 10980.38, base loss: 8500.79
[INFO 2017-06-26 17:29:28,056 main.py:47] epoch 206, training loss: 8064.96, average training loss: 10966.30, base loss: 8501.92
[INFO 2017-06-26 17:29:28,414 main.py:47] epoch 207, training loss: 7796.56, average training loss: 10951.06, base loss: 8502.56
[INFO 2017-06-26 17:29:28,797 main.py:47] epoch 208, training loss: 7922.56, average training loss: 10936.57, base loss: 8501.92
[INFO 2017-06-26 17:29:29,162 main.py:47] epoch 209, training loss: 7132.45, average training loss: 10918.46, base loss: 8497.40
[INFO 2017-06-26 17:29:29,529 main.py:47] epoch 210, training loss: 8546.71, average training loss: 10907.22, base loss: 8500.89
[INFO 2017-06-26 17:29:29,911 main.py:47] epoch 211, training loss: 7188.77, average training loss: 10889.68, base loss: 8497.26
[INFO 2017-06-26 17:29:30,267 main.py:47] epoch 212, training loss: 7025.10, average training loss: 10871.53, base loss: 8493.11
[INFO 2017-06-26 17:29:30,629 main.py:47] epoch 213, training loss: 8122.15, average training loss: 10858.68, base loss: 8494.75
[INFO 2017-06-26 17:29:30,984 main.py:47] epoch 214, training loss: 8791.69, average training loss: 10849.07, base loss: 8500.77
[INFO 2017-06-26 17:29:31,341 main.py:47] epoch 215, training loss: 7727.59, average training loss: 10834.62, base loss: 8500.32
[INFO 2017-06-26 17:29:31,698 main.py:47] epoch 216, training loss: 7214.31, average training loss: 10817.94, base loss: 8497.81
[INFO 2017-06-26 17:29:32,055 main.py:47] epoch 217, training loss: 8291.11, average training loss: 10806.34, base loss: 8501.06
[INFO 2017-06-26 17:29:32,410 main.py:47] epoch 218, training loss: 7304.66, average training loss: 10790.36, base loss: 8499.17
[INFO 2017-06-26 17:29:32,766 main.py:47] epoch 219, training loss: 7610.34, average training loss: 10775.90, base loss: 8498.56
[INFO 2017-06-26 17:29:33,123 main.py:47] epoch 220, training loss: 9746.53, average training loss: 10771.24, base loss: 8507.54
[INFO 2017-06-26 17:29:33,479 main.py:47] epoch 221, training loss: 7360.62, average training loss: 10755.88, base loss: 8506.97
[INFO 2017-06-26 17:29:33,851 main.py:47] epoch 222, training loss: 7349.15, average training loss: 10740.60, base loss: 8504.13
[INFO 2017-06-26 17:29:34,214 main.py:47] epoch 223, training loss: 7460.61, average training loss: 10725.96, base loss: 8502.86
[INFO 2017-06-26 17:29:34,580 main.py:47] epoch 224, training loss: 6868.40, average training loss: 10708.82, base loss: 8497.47
[INFO 2017-06-26 17:29:34,978 main.py:47] epoch 225, training loss: 7444.72, average training loss: 10694.37, base loss: 8496.87
[INFO 2017-06-26 17:29:35,367 main.py:47] epoch 226, training loss: 6865.62, average training loss: 10677.51, base loss: 8492.76
[INFO 2017-06-26 17:29:35,746 main.py:47] epoch 227, training loss: 7814.33, average training loss: 10664.95, base loss: 8493.36
[INFO 2017-06-26 17:29:36,131 main.py:47] epoch 228, training loss: 7768.64, average training loss: 10652.30, base loss: 8494.55
[INFO 2017-06-26 17:29:36,488 main.py:47] epoch 229, training loss: 7179.85, average training loss: 10637.20, base loss: 8490.93
[INFO 2017-06-26 17:29:36,874 main.py:47] epoch 230, training loss: 7464.31, average training loss: 10623.47, base loss: 8490.46
[INFO 2017-06-26 17:29:37,247 main.py:47] epoch 231, training loss: 8201.17, average training loss: 10613.03, base loss: 8493.99
[INFO 2017-06-26 17:29:37,628 main.py:47] epoch 232, training loss: 7915.86, average training loss: 10601.45, base loss: 8496.52
[INFO 2017-06-26 17:29:38,010 main.py:47] epoch 233, training loss: 7120.20, average training loss: 10586.57, base loss: 8493.04
[INFO 2017-06-26 17:29:38,367 main.py:47] epoch 234, training loss: 7751.68, average training loss: 10574.51, base loss: 8494.94
[INFO 2017-06-26 17:29:38,723 main.py:47] epoch 235, training loss: 8698.54, average training loss: 10566.56, base loss: 8499.74
[INFO 2017-06-26 17:29:39,083 main.py:47] epoch 236, training loss: 8741.35, average training loss: 10558.86, base loss: 8505.76
[INFO 2017-06-26 17:29:39,470 main.py:47] epoch 237, training loss: 6984.64, average training loss: 10543.84, base loss: 8501.65
[INFO 2017-06-26 17:29:39,834 main.py:47] epoch 238, training loss: 7010.17, average training loss: 10529.06, base loss: 8499.31
[INFO 2017-06-26 17:29:40,211 main.py:47] epoch 239, training loss: 7609.61, average training loss: 10516.89, base loss: 8498.36
[INFO 2017-06-26 17:29:40,580 main.py:47] epoch 240, training loss: 7377.05, average training loss: 10503.86, base loss: 8497.09
[INFO 2017-06-26 17:29:40,979 main.py:47] epoch 241, training loss: 6878.01, average training loss: 10488.88, base loss: 8492.76
[INFO 2017-06-26 17:29:41,354 main.py:47] epoch 242, training loss: 7092.04, average training loss: 10474.90, base loss: 8488.87
[INFO 2017-06-26 17:29:41,727 main.py:47] epoch 243, training loss: 6954.82, average training loss: 10460.48, base loss: 8486.02
[INFO 2017-06-26 17:29:42,135 main.py:47] epoch 244, training loss: 7967.96, average training loss: 10450.30, base loss: 8488.09
[INFO 2017-06-26 17:29:42,495 main.py:47] epoch 245, training loss: 7735.87, average training loss: 10439.27, base loss: 8488.37
[INFO 2017-06-26 17:29:42,902 main.py:47] epoch 246, training loss: 7037.15, average training loss: 10425.49, base loss: 8485.65
[INFO 2017-06-26 17:29:43,332 main.py:47] epoch 247, training loss: 7925.61, average training loss: 10415.41, base loss: 8486.61
[INFO 2017-06-26 17:29:43,722 main.py:47] epoch 248, training loss: 7260.57, average training loss: 10402.74, base loss: 8485.24
[INFO 2017-06-26 17:29:44,134 main.py:47] epoch 249, training loss: 6900.42, average training loss: 10388.73, base loss: 8482.26
[INFO 2017-06-26 17:29:44,538 main.py:47] epoch 250, training loss: 7260.58, average training loss: 10376.27, base loss: 8480.69
[INFO 2017-06-26 17:29:44,936 main.py:47] epoch 251, training loss: 7576.81, average training loss: 10365.16, base loss: 8480.14
[INFO 2017-06-26 17:29:45,310 main.py:47] epoch 252, training loss: 7387.22, average training loss: 10353.39, base loss: 8478.89
[INFO 2017-06-26 17:29:45,676 main.py:47] epoch 253, training loss: 8318.76, average training loss: 10345.38, base loss: 8481.96
[INFO 2017-06-26 17:29:46,029 main.py:47] epoch 254, training loss: 6536.55, average training loss: 10330.45, base loss: 8477.12
[INFO 2017-06-26 17:29:46,419 main.py:47] epoch 255, training loss: 7663.34, average training loss: 10320.03, base loss: 8477.64
[INFO 2017-06-26 17:29:46,774 main.py:47] epoch 256, training loss: 7536.71, average training loss: 10309.20, base loss: 8476.20
[INFO 2017-06-26 17:29:47,181 main.py:47] epoch 257, training loss: 8153.87, average training loss: 10300.84, base loss: 8478.72
[INFO 2017-06-26 17:29:47,540 main.py:47] epoch 258, training loss: 8330.00, average training loss: 10293.23, base loss: 8482.11
[INFO 2017-06-26 17:29:47,970 main.py:47] epoch 259, training loss: 8229.82, average training loss: 10285.30, base loss: 8485.24
[INFO 2017-06-26 17:29:48,344 main.py:47] epoch 260, training loss: 8972.74, average training loss: 10280.27, base loss: 8491.04
[INFO 2017-06-26 17:29:48,700 main.py:47] epoch 261, training loss: 7661.55, average training loss: 10270.27, base loss: 8493.09
[INFO 2017-06-26 17:29:49,076 main.py:47] epoch 262, training loss: 7280.36, average training loss: 10258.90, base loss: 8490.61
[INFO 2017-06-26 17:29:49,430 main.py:47] epoch 263, training loss: 7994.61, average training loss: 10250.33, base loss: 8490.98
[INFO 2017-06-26 17:29:49,786 main.py:47] epoch 264, training loss: 8846.78, average training loss: 10245.03, base loss: 8497.20
[INFO 2017-06-26 17:29:50,138 main.py:47] epoch 265, training loss: 6969.54, average training loss: 10232.72, base loss: 8494.51
[INFO 2017-06-26 17:29:50,522 main.py:47] epoch 266, training loss: 7732.63, average training loss: 10223.35, base loss: 8496.29
[INFO 2017-06-26 17:29:50,903 main.py:47] epoch 267, training loss: 7391.84, average training loss: 10212.79, base loss: 8496.21
[INFO 2017-06-26 17:29:51,274 main.py:47] epoch 268, training loss: 8639.22, average training loss: 10206.94, base loss: 8501.77
[INFO 2017-06-26 17:29:51,641 main.py:47] epoch 269, training loss: 7129.14, average training loss: 10195.54, base loss: 8499.85
[INFO 2017-06-26 17:29:52,034 main.py:47] epoch 270, training loss: 7781.62, average training loss: 10186.63, base loss: 8500.07
[INFO 2017-06-26 17:29:52,407 main.py:47] epoch 271, training loss: 7993.11, average training loss: 10178.57, base loss: 8503.38
[INFO 2017-06-26 17:29:52,814 main.py:47] epoch 272, training loss: 7506.15, average training loss: 10168.78, base loss: 8503.79
[INFO 2017-06-26 17:29:53,213 main.py:47] epoch 273, training loss: 6483.97, average training loss: 10155.33, base loss: 8499.92
[INFO 2017-06-26 17:29:53,623 main.py:47] epoch 274, training loss: 8104.30, average training loss: 10147.87, base loss: 8502.31
[INFO 2017-06-26 17:29:54,017 main.py:47] epoch 275, training loss: 7527.70, average training loss: 10138.38, base loss: 8502.20
[INFO 2017-06-26 17:29:54,406 main.py:47] epoch 276, training loss: 7196.39, average training loss: 10127.76, base loss: 8500.04
[INFO 2017-06-26 17:29:54,825 main.py:47] epoch 277, training loss: 7811.45, average training loss: 10119.43, base loss: 8501.61
[INFO 2017-06-26 17:29:55,217 main.py:47] epoch 278, training loss: 7419.01, average training loss: 10109.75, base loss: 8501.19
[INFO 2017-06-26 17:29:55,705 main.py:47] epoch 279, training loss: 7344.98, average training loss: 10099.87, base loss: 8500.20
[INFO 2017-06-26 17:29:56,136 main.py:47] epoch 280, training loss: 7148.76, average training loss: 10089.37, base loss: 8498.67
[INFO 2017-06-26 17:29:56,505 main.py:47] epoch 281, training loss: 7257.21, average training loss: 10079.33, base loss: 8497.44
[INFO 2017-06-26 17:29:56,914 main.py:47] epoch 282, training loss: 7274.86, average training loss: 10069.42, base loss: 8496.58
[INFO 2017-06-26 17:29:57,316 main.py:47] epoch 283, training loss: 7386.91, average training loss: 10059.97, base loss: 8496.46
[INFO 2017-06-26 17:29:57,719 main.py:47] epoch 284, training loss: 7941.92, average training loss: 10052.54, base loss: 8499.69
[INFO 2017-06-26 17:29:58,110 main.py:47] epoch 285, training loss: 7243.04, average training loss: 10042.72, base loss: 8497.73
[INFO 2017-06-26 17:29:58,518 main.py:47] epoch 286, training loss: 7542.56, average training loss: 10034.01, base loss: 8498.68
[INFO 2017-06-26 17:29:58,909 main.py:47] epoch 287, training loss: 7132.91, average training loss: 10023.93, base loss: 8497.53
[INFO 2017-06-26 17:29:59,331 main.py:47] epoch 288, training loss: 7210.72, average training loss: 10014.20, base loss: 8495.78
[INFO 2017-06-26 17:29:59,763 main.py:47] epoch 289, training loss: 7945.05, average training loss: 10007.06, base loss: 8498.84
[INFO 2017-06-26 17:30:00,158 main.py:47] epoch 290, training loss: 6237.53, average training loss: 9994.11, base loss: 8492.43
[INFO 2017-06-26 17:30:00,560 main.py:47] epoch 291, training loss: 6588.03, average training loss: 9982.44, base loss: 8488.24
[INFO 2017-06-26 17:30:00,974 main.py:47] epoch 292, training loss: 6741.20, average training loss: 9971.38, base loss: 8484.82
[INFO 2017-06-26 17:30:01,402 main.py:47] epoch 293, training loss: 7920.48, average training loss: 9964.41, base loss: 8486.46
[INFO 2017-06-26 17:30:01,806 main.py:47] epoch 294, training loss: 7341.00, average training loss: 9955.51, base loss: 8485.49
[INFO 2017-06-26 17:30:02,164 main.py:47] epoch 295, training loss: 7873.41, average training loss: 9948.48, base loss: 8487.52
[INFO 2017-06-26 17:30:02,569 main.py:47] epoch 296, training loss: 7597.03, average training loss: 9940.56, base loss: 8486.60
[INFO 2017-06-26 17:30:02,940 main.py:47] epoch 297, training loss: 7613.66, average training loss: 9932.75, base loss: 8486.16
[INFO 2017-06-26 17:30:03,358 main.py:47] epoch 298, training loss: 7288.38, average training loss: 9923.91, base loss: 8485.20
[INFO 2017-06-26 17:30:03,727 main.py:47] epoch 299, training loss: 7385.34, average training loss: 9915.45, base loss: 8484.22
[INFO 2017-06-26 17:30:03,727 main.py:49] epoch 299, testing
[INFO 2017-06-26 17:30:08,363 main.py:100] average testing loss: 7629.80, base loss: 8677.64
[INFO 2017-06-26 17:30:08,387 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:30:08,394 main.py:73] current best accuracy: 7629.80
[INFO 2017-06-26 17:30:08,756 main.py:47] epoch 300, training loss: 6764.66, average training loss: 9904.98, base loss: 8480.72
[INFO 2017-06-26 17:30:09,117 main.py:47] epoch 301, training loss: 7816.76, average training loss: 9898.07, base loss: 8482.53
[INFO 2017-06-26 17:30:09,478 main.py:47] epoch 302, training loss: 7575.82, average training loss: 9890.40, base loss: 8483.40
[INFO 2017-06-26 17:30:09,834 main.py:47] epoch 303, training loss: 7075.33, average training loss: 9881.14, base loss: 8481.54
[INFO 2017-06-26 17:30:10,188 main.py:47] epoch 304, training loss: 7243.97, average training loss: 9872.49, base loss: 8480.53
[INFO 2017-06-26 17:30:10,566 main.py:47] epoch 305, training loss: 6331.67, average training loss: 9860.92, base loss: 8475.39
[INFO 2017-06-26 17:30:10,923 main.py:47] epoch 306, training loss: 7021.52, average training loss: 9851.67, base loss: 8473.21
[INFO 2017-06-26 17:30:11,278 main.py:47] epoch 307, training loss: 8007.34, average training loss: 9845.69, base loss: 8474.65
[INFO 2017-06-26 17:30:11,636 main.py:47] epoch 308, training loss: 8216.61, average training loss: 9840.41, base loss: 8477.98
[INFO 2017-06-26 17:30:11,991 main.py:47] epoch 309, training loss: 8153.35, average training loss: 9834.97, base loss: 8480.24
[INFO 2017-06-26 17:30:12,346 main.py:47] epoch 310, training loss: 7278.73, average training loss: 9826.75, base loss: 8479.40
[INFO 2017-06-26 17:30:12,702 main.py:47] epoch 311, training loss: 7308.58, average training loss: 9818.68, base loss: 8478.44
[INFO 2017-06-26 17:30:13,059 main.py:47] epoch 312, training loss: 7639.05, average training loss: 9811.72, base loss: 8479.90
[INFO 2017-06-26 17:30:13,414 main.py:47] epoch 313, training loss: 6895.34, average training loss: 9802.43, base loss: 8477.61
[INFO 2017-06-26 17:30:13,768 main.py:47] epoch 314, training loss: 8125.30, average training loss: 9797.11, base loss: 8479.06
[INFO 2017-06-26 17:30:14,123 main.py:47] epoch 315, training loss: 7886.31, average training loss: 9791.06, base loss: 8479.99
[INFO 2017-06-26 17:30:14,480 main.py:47] epoch 316, training loss: 6651.51, average training loss: 9781.16, base loss: 8476.53
[INFO 2017-06-26 17:30:14,835 main.py:47] epoch 317, training loss: 7985.13, average training loss: 9775.51, base loss: 8478.68
[INFO 2017-06-26 17:30:15,192 main.py:47] epoch 318, training loss: 7397.82, average training loss: 9768.05, base loss: 8477.97
[INFO 2017-06-26 17:30:15,573 main.py:47] epoch 319, training loss: 8287.59, average training loss: 9763.43, base loss: 8480.55
[INFO 2017-06-26 17:30:15,954 main.py:47] epoch 320, training loss: 6928.59, average training loss: 9754.60, base loss: 8477.85
[INFO 2017-06-26 17:30:16,338 main.py:47] epoch 321, training loss: 8534.00, average training loss: 9750.81, base loss: 8482.23
[INFO 2017-06-26 17:30:16,728 main.py:47] epoch 322, training loss: 8279.13, average training loss: 9746.25, base loss: 8486.30
[INFO 2017-06-26 17:30:17,109 main.py:47] epoch 323, training loss: 8338.95, average training loss: 9741.91, base loss: 8489.98
[INFO 2017-06-26 17:30:17,477 main.py:47] epoch 324, training loss: 7346.87, average training loss: 9734.54, base loss: 8489.36
[INFO 2017-06-26 17:30:17,853 main.py:47] epoch 325, training loss: 7287.32, average training loss: 9727.03, base loss: 8486.98
[INFO 2017-06-26 17:30:18,256 main.py:47] epoch 326, training loss: 7879.76, average training loss: 9721.38, base loss: 8488.13
[INFO 2017-06-26 17:30:18,629 main.py:47] epoch 327, training loss: 7443.08, average training loss: 9714.43, base loss: 8488.66
[INFO 2017-06-26 17:30:18,986 main.py:47] epoch 328, training loss: 7521.91, average training loss: 9707.77, base loss: 8488.06
[INFO 2017-06-26 17:30:19,344 main.py:47] epoch 329, training loss: 7341.31, average training loss: 9700.60, base loss: 8487.80
[INFO 2017-06-26 17:30:19,699 main.py:47] epoch 330, training loss: 8093.29, average training loss: 9695.74, base loss: 8489.71
[INFO 2017-06-26 17:30:20,053 main.py:47] epoch 331, training loss: 8480.85, average training loss: 9692.08, base loss: 8493.14
[INFO 2017-06-26 17:30:20,408 main.py:47] epoch 332, training loss: 7733.82, average training loss: 9686.20, base loss: 8494.54
[INFO 2017-06-26 17:30:20,795 main.py:47] epoch 333, training loss: 7667.79, average training loss: 9680.16, base loss: 8495.00
[INFO 2017-06-26 17:30:21,160 main.py:47] epoch 334, training loss: 7895.36, average training loss: 9674.83, base loss: 8496.38
[INFO 2017-06-26 17:30:21,513 main.py:47] epoch 335, training loss: 6629.20, average training loss: 9665.77, base loss: 8493.00
[INFO 2017-06-26 17:30:21,886 main.py:47] epoch 336, training loss: 6950.96, average training loss: 9657.71, base loss: 8491.09
[INFO 2017-06-26 17:30:22,272 main.py:47] epoch 337, training loss: 6610.52, average training loss: 9648.70, base loss: 8488.50
[INFO 2017-06-26 17:30:22,631 main.py:47] epoch 338, training loss: 8157.37, average training loss: 9644.30, base loss: 8491.61
[INFO 2017-06-26 17:30:23,025 main.py:47] epoch 339, training loss: 7837.72, average training loss: 9638.98, base loss: 8494.18
[INFO 2017-06-26 17:30:23,440 main.py:47] epoch 340, training loss: 7235.45, average training loss: 9631.94, base loss: 8493.33
[INFO 2017-06-26 17:30:23,828 main.py:47] epoch 341, training loss: 7288.50, average training loss: 9625.08, base loss: 8492.39
[INFO 2017-06-26 17:30:24,241 main.py:47] epoch 342, training loss: 8205.46, average training loss: 9620.94, base loss: 8494.83
[INFO 2017-06-26 17:30:24,630 main.py:47] epoch 343, training loss: 7326.52, average training loss: 9614.27, base loss: 8494.75
[INFO 2017-06-26 17:30:25,005 main.py:47] epoch 344, training loss: 6362.49, average training loss: 9604.85, base loss: 8490.50
[INFO 2017-06-26 17:30:25,365 main.py:47] epoch 345, training loss: 6399.31, average training loss: 9595.58, base loss: 8486.81
[INFO 2017-06-26 17:30:25,730 main.py:47] epoch 346, training loss: 7623.37, average training loss: 9589.90, base loss: 8486.57
[INFO 2017-06-26 17:30:26,088 main.py:47] epoch 347, training loss: 7270.34, average training loss: 9583.24, base loss: 8486.33
[INFO 2017-06-26 17:30:26,483 main.py:47] epoch 348, training loss: 6894.12, average training loss: 9575.53, base loss: 8484.88
[INFO 2017-06-26 17:30:26,873 main.py:47] epoch 349, training loss: 7532.74, average training loss: 9569.69, base loss: 8485.21
[INFO 2017-06-26 17:30:27,253 main.py:47] epoch 350, training loss: 7891.81, average training loss: 9564.91, base loss: 8486.59
[INFO 2017-06-26 17:30:27,679 main.py:47] epoch 351, training loss: 7311.70, average training loss: 9558.51, base loss: 8486.99
[INFO 2017-06-26 17:30:28,059 main.py:47] epoch 352, training loss: 6900.81, average training loss: 9550.98, base loss: 8485.17
[INFO 2017-06-26 17:30:28,463 main.py:47] epoch 353, training loss: 7401.65, average training loss: 9544.91, base loss: 8485.16
[INFO 2017-06-26 17:30:28,849 main.py:47] epoch 354, training loss: 8480.15, average training loss: 9541.91, base loss: 8488.36
[INFO 2017-06-26 17:30:29,209 main.py:47] epoch 355, training loss: 9216.30, average training loss: 9541.00, base loss: 8494.65
[INFO 2017-06-26 17:30:29,570 main.py:47] epoch 356, training loss: 7089.70, average training loss: 9534.13, base loss: 8492.46
[INFO 2017-06-26 17:30:29,925 main.py:47] epoch 357, training loss: 6926.06, average training loss: 9526.85, base loss: 8490.44
[INFO 2017-06-26 17:30:30,280 main.py:47] epoch 358, training loss: 7321.17, average training loss: 9520.70, base loss: 8490.23
[INFO 2017-06-26 17:30:30,632 main.py:47] epoch 359, training loss: 7840.73, average training loss: 9516.04, base loss: 8491.96
[INFO 2017-06-26 17:30:31,037 main.py:47] epoch 360, training loss: 7170.96, average training loss: 9509.54, base loss: 8490.48
[INFO 2017-06-26 17:30:31,414 main.py:47] epoch 361, training loss: 7633.47, average training loss: 9504.36, base loss: 8491.12
[INFO 2017-06-26 17:30:31,769 main.py:47] epoch 362, training loss: 6725.77, average training loss: 9496.70, base loss: 8488.12
[INFO 2017-06-26 17:30:32,124 main.py:47] epoch 363, training loss: 7943.01, average training loss: 9492.43, base loss: 8490.11
[INFO 2017-06-26 17:30:32,485 main.py:47] epoch 364, training loss: 7767.95, average training loss: 9487.71, base loss: 8491.04
[INFO 2017-06-26 17:30:32,889 main.py:47] epoch 365, training loss: 7995.65, average training loss: 9483.63, base loss: 8492.71
[INFO 2017-06-26 17:30:33,288 main.py:47] epoch 366, training loss: 6800.43, average training loss: 9476.32, base loss: 8489.85
[INFO 2017-06-26 17:30:33,677 main.py:47] epoch 367, training loss: 6599.78, average training loss: 9468.51, base loss: 8486.93
[INFO 2017-06-26 17:30:34,105 main.py:47] epoch 368, training loss: 7644.04, average training loss: 9463.56, base loss: 8487.93
[INFO 2017-06-26 17:30:34,493 main.py:47] epoch 369, training loss: 8661.72, average training loss: 9461.39, base loss: 8492.73
[INFO 2017-06-26 17:30:34,895 main.py:47] epoch 370, training loss: 7395.14, average training loss: 9455.82, base loss: 8492.61
[INFO 2017-06-26 17:30:35,254 main.py:47] epoch 371, training loss: 6650.02, average training loss: 9448.28, base loss: 8491.11
[INFO 2017-06-26 17:30:35,614 main.py:47] epoch 372, training loss: 7642.22, average training loss: 9443.44, base loss: 8491.02
[INFO 2017-06-26 17:30:35,969 main.py:47] epoch 373, training loss: 7358.08, average training loss: 9437.86, base loss: 8490.22
[INFO 2017-06-26 17:30:36,324 main.py:47] epoch 374, training loss: 7879.18, average training loss: 9433.71, base loss: 8491.55
[INFO 2017-06-26 17:30:36,692 main.py:47] epoch 375, training loss: 7178.05, average training loss: 9427.71, base loss: 8490.22
[INFO 2017-06-26 17:30:37,050 main.py:47] epoch 376, training loss: 7631.34, average training loss: 9422.94, base loss: 8491.17
[INFO 2017-06-26 17:30:37,440 main.py:47] epoch 377, training loss: 7223.20, average training loss: 9417.12, base loss: 8490.31
[INFO 2017-06-26 17:30:37,823 main.py:47] epoch 378, training loss: 6855.37, average training loss: 9410.36, base loss: 8488.90
[INFO 2017-06-26 17:30:38,180 main.py:47] epoch 379, training loss: 6633.38, average training loss: 9403.06, base loss: 8486.18
[INFO 2017-06-26 17:30:38,540 main.py:47] epoch 380, training loss: 7641.29, average training loss: 9398.43, base loss: 8487.06
[INFO 2017-06-26 17:30:38,893 main.py:47] epoch 381, training loss: 6879.47, average training loss: 9391.84, base loss: 8485.11
[INFO 2017-06-26 17:30:39,249 main.py:47] epoch 382, training loss: 7170.01, average training loss: 9386.04, base loss: 8484.10
[INFO 2017-06-26 17:30:39,634 main.py:47] epoch 383, training loss: 7289.88, average training loss: 9380.58, base loss: 8483.59
[INFO 2017-06-26 17:30:40,008 main.py:47] epoch 384, training loss: 7363.81, average training loss: 9375.34, base loss: 8484.04
[INFO 2017-06-26 17:30:40,382 main.py:47] epoch 385, training loss: 6383.43, average training loss: 9367.59, base loss: 8481.13
[INFO 2017-06-26 17:30:40,746 main.py:47] epoch 386, training loss: 7365.42, average training loss: 9362.42, base loss: 8480.62
[INFO 2017-06-26 17:30:41,103 main.py:47] epoch 387, training loss: 6864.01, average training loss: 9355.98, base loss: 8478.94
[INFO 2017-06-26 17:30:41,494 main.py:47] epoch 388, training loss: 8406.19, average training loss: 9353.54, base loss: 8482.10
[INFO 2017-06-26 17:30:41,879 main.py:47] epoch 389, training loss: 7170.73, average training loss: 9347.94, base loss: 8482.55
[INFO 2017-06-26 17:30:42,250 main.py:47] epoch 390, training loss: 7051.84, average training loss: 9342.07, base loss: 8481.69
[INFO 2017-06-26 17:30:42,615 main.py:47] epoch 391, training loss: 7365.09, average training loss: 9337.02, base loss: 8481.74
[INFO 2017-06-26 17:30:42,969 main.py:47] epoch 392, training loss: 8671.31, average training loss: 9335.33, base loss: 8486.12
[INFO 2017-06-26 17:30:43,358 main.py:47] epoch 393, training loss: 7662.92, average training loss: 9331.08, base loss: 8487.01
[INFO 2017-06-26 17:30:43,712 main.py:47] epoch 394, training loss: 7249.30, average training loss: 9325.81, base loss: 8485.48
[INFO 2017-06-26 17:30:44,106 main.py:47] epoch 395, training loss: 8813.30, average training loss: 9324.52, base loss: 8491.04
[INFO 2017-06-26 17:30:44,463 main.py:47] epoch 396, training loss: 6425.24, average training loss: 9317.22, base loss: 8488.59
[INFO 2017-06-26 17:30:44,836 main.py:47] epoch 397, training loss: 7478.40, average training loss: 9312.60, base loss: 8489.88
[INFO 2017-06-26 17:30:45,191 main.py:47] epoch 398, training loss: 6900.82, average training loss: 9306.55, base loss: 8488.38
[INFO 2017-06-26 17:30:45,545 main.py:47] epoch 399, training loss: 7868.41, average training loss: 9302.96, base loss: 8490.21
[INFO 2017-06-26 17:30:45,546 main.py:49] epoch 399, testing
[INFO 2017-06-26 17:30:49,731 main.py:100] average testing loss: 7415.83, base loss: 8379.13
[INFO 2017-06-26 17:30:49,755 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:30:49,762 main.py:73] current best accuracy: 7415.83
[INFO 2017-06-26 17:30:50,116 main.py:47] epoch 400, training loss: 7745.19, average training loss: 9299.07, base loss: 8491.08
[INFO 2017-06-26 17:30:50,472 main.py:47] epoch 401, training loss: 7767.27, average training loss: 9295.26, base loss: 8492.61
[INFO 2017-06-26 17:30:50,827 main.py:47] epoch 402, training loss: 7380.15, average training loss: 9290.51, base loss: 8492.86
[INFO 2017-06-26 17:30:51,262 main.py:47] epoch 403, training loss: 6755.03, average training loss: 9284.23, base loss: 8491.53
[INFO 2017-06-26 17:30:51,626 main.py:47] epoch 404, training loss: 7104.30, average training loss: 9278.85, base loss: 8491.39
[INFO 2017-06-26 17:30:51,997 main.py:47] epoch 405, training loss: 6759.97, average training loss: 9272.65, base loss: 8490.74
[INFO 2017-06-26 17:30:52,364 main.py:47] epoch 406, training loss: 8304.28, average training loss: 9270.27, base loss: 8493.27
[INFO 2017-06-26 17:30:52,736 main.py:47] epoch 407, training loss: 7785.16, average training loss: 9266.63, base loss: 8495.74
[INFO 2017-06-26 17:30:53,098 main.py:47] epoch 408, training loss: 6946.39, average training loss: 9260.95, base loss: 8494.66
[INFO 2017-06-26 17:30:53,462 main.py:47] epoch 409, training loss: 6968.35, average training loss: 9255.36, base loss: 8492.22
[INFO 2017-06-26 17:30:53,826 main.py:47] epoch 410, training loss: 7923.46, average training loss: 9252.12, base loss: 8495.15
[INFO 2017-06-26 17:30:54,184 main.py:47] epoch 411, training loss: 8133.30, average training loss: 9249.41, base loss: 8496.56
[INFO 2017-06-26 17:30:54,544 main.py:47] epoch 412, training loss: 6650.96, average training loss: 9243.11, base loss: 8494.32
[INFO 2017-06-26 17:30:54,899 main.py:47] epoch 413, training loss: 6492.44, average training loss: 9236.47, base loss: 8490.43
[INFO 2017-06-26 17:30:55,253 main.py:47] epoch 414, training loss: 7201.29, average training loss: 9231.57, base loss: 8489.37
[INFO 2017-06-26 17:30:55,607 main.py:47] epoch 415, training loss: 6950.93, average training loss: 9226.08, base loss: 8487.78
[INFO 2017-06-26 17:30:55,960 main.py:47] epoch 416, training loss: 7374.54, average training loss: 9221.64, base loss: 8488.66
[INFO 2017-06-26 17:30:56,316 main.py:47] epoch 417, training loss: 7157.92, average training loss: 9216.71, base loss: 8487.69
[INFO 2017-06-26 17:30:56,669 main.py:47] epoch 418, training loss: 7470.53, average training loss: 9212.54, base loss: 8487.60
[INFO 2017-06-26 17:30:57,043 main.py:47] epoch 419, training loss: 7078.27, average training loss: 9207.46, base loss: 8487.15
[INFO 2017-06-26 17:30:57,398 main.py:47] epoch 420, training loss: 7307.88, average training loss: 9202.95, base loss: 8487.03
[INFO 2017-06-26 17:30:57,757 main.py:47] epoch 421, training loss: 7308.66, average training loss: 9198.46, base loss: 8487.29
[INFO 2017-06-26 17:30:58,113 main.py:47] epoch 422, training loss: 7337.71, average training loss: 9194.06, base loss: 8487.03
[INFO 2017-06-26 17:30:58,469 main.py:47] epoch 423, training loss: 7001.50, average training loss: 9188.89, base loss: 8485.69
[INFO 2017-06-26 17:30:58,833 main.py:47] epoch 424, training loss: 6383.19, average training loss: 9182.29, base loss: 8483.41
[INFO 2017-06-26 17:30:59,196 main.py:47] epoch 425, training loss: 7932.21, average training loss: 9179.35, base loss: 8484.85
[INFO 2017-06-26 17:30:59,556 main.py:47] epoch 426, training loss: 7729.68, average training loss: 9175.96, base loss: 8486.79
[INFO 2017-06-26 17:30:59,917 main.py:47] epoch 427, training loss: 6918.37, average training loss: 9170.68, base loss: 8485.94
[INFO 2017-06-26 17:31:00,272 main.py:47] epoch 428, training loss: 7490.37, average training loss: 9166.76, base loss: 8486.29
[INFO 2017-06-26 17:31:00,626 main.py:47] epoch 429, training loss: 6883.35, average training loss: 9161.45, base loss: 8485.58
[INFO 2017-06-26 17:31:01,012 main.py:47] epoch 430, training loss: 6741.95, average training loss: 9155.84, base loss: 8484.60
[INFO 2017-06-26 17:31:01,399 main.py:47] epoch 431, training loss: 6821.19, average training loss: 9150.44, base loss: 8483.02
[INFO 2017-06-26 17:31:01,763 main.py:47] epoch 432, training loss: 7397.85, average training loss: 9146.39, base loss: 8482.83
[INFO 2017-06-26 17:31:02,129 main.py:47] epoch 433, training loss: 7182.76, average training loss: 9141.86, base loss: 8482.92
[INFO 2017-06-26 17:31:02,483 main.py:47] epoch 434, training loss: 6623.16, average training loss: 9136.07, base loss: 8480.85
[INFO 2017-06-26 17:31:02,837 main.py:47] epoch 435, training loss: 7201.00, average training loss: 9131.64, base loss: 8481.27
[INFO 2017-06-26 17:31:03,191 main.py:47] epoch 436, training loss: 6560.54, average training loss: 9125.75, base loss: 8479.04
[INFO 2017-06-26 17:31:03,548 main.py:47] epoch 437, training loss: 6293.59, average training loss: 9119.29, base loss: 8477.14
[INFO 2017-06-26 17:31:03,907 main.py:47] epoch 438, training loss: 7354.36, average training loss: 9115.27, base loss: 8476.97
[INFO 2017-06-26 17:31:04,283 main.py:47] epoch 439, training loss: 8276.28, average training loss: 9113.36, base loss: 8479.85
[INFO 2017-06-26 17:31:04,672 main.py:47] epoch 440, training loss: 6512.45, average training loss: 9107.46, base loss: 8478.74
[INFO 2017-06-26 17:31:05,062 main.py:47] epoch 441, training loss: 7351.92, average training loss: 9103.49, base loss: 8478.85
[INFO 2017-06-26 17:31:05,425 main.py:47] epoch 442, training loss: 7789.29, average training loss: 9100.52, base loss: 8479.67
[INFO 2017-06-26 17:31:05,796 main.py:47] epoch 443, training loss: 7574.98, average training loss: 9097.09, base loss: 8479.85
[INFO 2017-06-26 17:31:06,262 main.py:47] epoch 444, training loss: 7007.84, average training loss: 9092.39, base loss: 8479.60
[INFO 2017-06-26 17:31:06,627 main.py:47] epoch 445, training loss: 8412.94, average training loss: 9090.87, base loss: 8482.30
[INFO 2017-06-26 17:31:06,989 main.py:47] epoch 446, training loss: 6868.20, average training loss: 9085.90, base loss: 8480.78
[INFO 2017-06-26 17:31:07,349 main.py:47] epoch 447, training loss: 7137.04, average training loss: 9081.55, base loss: 8481.15
[INFO 2017-06-26 17:31:07,702 main.py:47] epoch 448, training loss: 8195.79, average training loss: 9079.57, base loss: 8484.48
[INFO 2017-06-26 17:31:08,057 main.py:47] epoch 449, training loss: 6681.05, average training loss: 9074.24, base loss: 8482.79
[INFO 2017-06-26 17:31:08,410 main.py:47] epoch 450, training loss: 6852.06, average training loss: 9069.32, base loss: 8481.31
[INFO 2017-06-26 17:31:08,765 main.py:47] epoch 451, training loss: 6066.35, average training loss: 9062.67, base loss: 8477.69
[INFO 2017-06-26 17:31:09,120 main.py:47] epoch 452, training loss: 5920.85, average training loss: 9055.74, base loss: 8474.05
[INFO 2017-06-26 17:31:09,472 main.py:47] epoch 453, training loss: 8192.16, average training loss: 9053.83, base loss: 8475.30
[INFO 2017-06-26 17:31:09,850 main.py:47] epoch 454, training loss: 7023.95, average training loss: 9049.37, base loss: 8473.99
[INFO 2017-06-26 17:31:10,235 main.py:47] epoch 455, training loss: 7475.64, average training loss: 9045.92, base loss: 8473.49
[INFO 2017-06-26 17:31:10,600 main.py:47] epoch 456, training loss: 7169.13, average training loss: 9041.82, base loss: 8472.66
[INFO 2017-06-26 17:31:10,956 main.py:47] epoch 457, training loss: 7786.20, average training loss: 9039.07, base loss: 8473.60
[INFO 2017-06-26 17:31:11,315 main.py:47] epoch 458, training loss: 7653.10, average training loss: 9036.05, base loss: 8473.65
[INFO 2017-06-26 17:31:11,669 main.py:47] epoch 459, training loss: 7385.03, average training loss: 9032.46, base loss: 8473.46
[INFO 2017-06-26 17:31:12,023 main.py:47] epoch 460, training loss: 6446.97, average training loss: 9026.86, base loss: 8470.96
[INFO 2017-06-26 17:31:12,410 main.py:47] epoch 461, training loss: 6969.82, average training loss: 9022.40, base loss: 8469.57
[INFO 2017-06-26 17:31:12,773 main.py:47] epoch 462, training loss: 7753.03, average training loss: 9019.66, base loss: 8470.67
[INFO 2017-06-26 17:31:13,129 main.py:47] epoch 463, training loss: 7919.56, average training loss: 9017.29, base loss: 8472.37
[INFO 2017-06-26 17:31:13,487 main.py:47] epoch 464, training loss: 6883.61, average training loss: 9012.70, base loss: 8472.16
[INFO 2017-06-26 17:31:13,872 main.py:47] epoch 465, training loss: 9099.48, average training loss: 9012.89, base loss: 8476.48
[INFO 2017-06-26 17:31:14,235 main.py:47] epoch 466, training loss: 7109.96, average training loss: 9008.81, base loss: 8476.45
[INFO 2017-06-26 17:31:14,591 main.py:47] epoch 467, training loss: 6470.51, average training loss: 9003.39, base loss: 8473.71
[INFO 2017-06-26 17:31:14,957 main.py:47] epoch 468, training loss: 6892.96, average training loss: 8998.89, base loss: 8473.00
[INFO 2017-06-26 17:31:15,312 main.py:47] epoch 469, training loss: 6847.61, average training loss: 8994.31, base loss: 8472.27
[INFO 2017-06-26 17:31:15,665 main.py:47] epoch 470, training loss: 7967.97, average training loss: 8992.13, base loss: 8473.94
[INFO 2017-06-26 17:31:16,020 main.py:47] epoch 471, training loss: 7474.63, average training loss: 8988.92, base loss: 8475.08
[INFO 2017-06-26 17:31:16,373 main.py:47] epoch 472, training loss: 6954.58, average training loss: 8984.62, base loss: 8474.12
[INFO 2017-06-26 17:31:16,727 main.py:47] epoch 473, training loss: 8382.75, average training loss: 8983.35, base loss: 8477.03
[INFO 2017-06-26 17:31:17,080 main.py:47] epoch 474, training loss: 8103.59, average training loss: 8981.50, base loss: 8479.77
[INFO 2017-06-26 17:31:17,432 main.py:47] epoch 475, training loss: 6931.42, average training loss: 8977.19, base loss: 8478.96
[INFO 2017-06-26 17:31:17,785 main.py:47] epoch 476, training loss: 6742.53, average training loss: 8972.50, base loss: 8477.72
[INFO 2017-06-26 17:31:18,137 main.py:47] epoch 477, training loss: 7175.65, average training loss: 8968.75, base loss: 8477.28
[INFO 2017-06-26 17:31:18,494 main.py:47] epoch 478, training loss: 7734.37, average training loss: 8966.17, base loss: 8478.13
[INFO 2017-06-26 17:31:18,850 main.py:47] epoch 479, training loss: 7768.60, average training loss: 8963.67, base loss: 8479.70
[INFO 2017-06-26 17:31:19,240 main.py:47] epoch 480, training loss: 7099.37, average training loss: 8959.80, base loss: 8479.45
[INFO 2017-06-26 17:31:19,609 main.py:47] epoch 481, training loss: 7832.20, average training loss: 8957.46, base loss: 8480.62
[INFO 2017-06-26 17:31:19,965 main.py:47] epoch 482, training loss: 6955.00, average training loss: 8953.31, base loss: 8479.05
[INFO 2017-06-26 17:31:20,370 main.py:47] epoch 483, training loss: 6576.77, average training loss: 8948.40, base loss: 8477.23
[INFO 2017-06-26 17:31:20,734 main.py:47] epoch 484, training loss: 7119.25, average training loss: 8944.63, base loss: 8477.23
[INFO 2017-06-26 17:31:21,155 main.py:47] epoch 485, training loss: 7181.18, average training loss: 8941.00, base loss: 8476.57
[INFO 2017-06-26 17:31:21,529 main.py:47] epoch 486, training loss: 7444.83, average training loss: 8937.93, base loss: 8476.49
[INFO 2017-06-26 17:31:21,925 main.py:47] epoch 487, training loss: 6895.23, average training loss: 8933.74, base loss: 8475.73
[INFO 2017-06-26 17:31:22,294 main.py:47] epoch 488, training loss: 8095.37, average training loss: 8932.03, base loss: 8477.76
[INFO 2017-06-26 17:31:22,653 main.py:47] epoch 489, training loss: 9281.34, average training loss: 8932.74, base loss: 8482.44
[INFO 2017-06-26 17:31:23,052 main.py:47] epoch 490, training loss: 6897.37, average training loss: 8928.60, base loss: 8480.47
[INFO 2017-06-26 17:31:23,408 main.py:47] epoch 491, training loss: 7244.94, average training loss: 8925.18, base loss: 8480.36
[INFO 2017-06-26 17:31:23,804 main.py:47] epoch 492, training loss: 7786.21, average training loss: 8922.87, base loss: 8481.84
[INFO 2017-06-26 17:31:24,184 main.py:47] epoch 493, training loss: 7239.18, average training loss: 8919.46, base loss: 8482.29
[INFO 2017-06-26 17:31:24,589 main.py:47] epoch 494, training loss: 6905.83, average training loss: 8915.39, base loss: 8481.46
[INFO 2017-06-26 17:31:24,957 main.py:47] epoch 495, training loss: 7598.03, average training loss: 8912.73, base loss: 8481.82
[INFO 2017-06-26 17:31:25,323 main.py:47] epoch 496, training loss: 6532.23, average training loss: 8907.94, base loss: 8479.98
[INFO 2017-06-26 17:31:25,680 main.py:47] epoch 497, training loss: 7492.75, average training loss: 8905.10, base loss: 8481.02
[INFO 2017-06-26 17:31:26,036 main.py:47] epoch 498, training loss: 8274.82, average training loss: 8903.84, base loss: 8483.95
[INFO 2017-06-26 17:31:26,392 main.py:47] epoch 499, training loss: 6534.95, average training loss: 8899.10, base loss: 8482.43
[INFO 2017-06-26 17:31:26,392 main.py:49] epoch 499, testing
[INFO 2017-06-26 17:31:30,783 main.py:100] average testing loss: 7537.63, base loss: 8843.03
[INFO 2017-06-26 17:31:30,807 main.py:73] current best accuracy: 7415.83
[INFO 2017-06-26 17:31:31,161 main.py:47] epoch 500, training loss: 7080.02, average training loss: 8895.47, base loss: 8482.11
[INFO 2017-06-26 17:31:31,513 main.py:47] epoch 501, training loss: 7551.25, average training loss: 8892.79, base loss: 8483.10
[INFO 2017-06-26 17:31:31,866 main.py:47] epoch 502, training loss: 7760.75, average training loss: 8890.54, base loss: 8485.16
[INFO 2017-06-26 17:31:32,217 main.py:47] epoch 503, training loss: 6869.77, average training loss: 8886.53, base loss: 8484.16
[INFO 2017-06-26 17:31:32,570 main.py:47] epoch 504, training loss: 6464.95, average training loss: 8881.74, base loss: 8482.12
[INFO 2017-06-26 17:31:32,924 main.py:47] epoch 505, training loss: 6753.64, average training loss: 8877.53, base loss: 8481.44
[INFO 2017-06-26 17:31:33,278 main.py:47] epoch 506, training loss: 7832.14, average training loss: 8875.47, base loss: 8482.55
[INFO 2017-06-26 17:31:33,633 main.py:47] epoch 507, training loss: 6346.30, average training loss: 8870.49, base loss: 8480.19
[INFO 2017-06-26 17:31:33,989 main.py:47] epoch 508, training loss: 7216.10, average training loss: 8867.24, base loss: 8480.32
[INFO 2017-06-26 17:31:34,345 main.py:47] epoch 509, training loss: 7116.51, average training loss: 8863.81, base loss: 8480.02
[INFO 2017-06-26 17:31:34,701 main.py:47] epoch 510, training loss: 7605.30, average training loss: 8861.34, base loss: 8481.08
[INFO 2017-06-26 17:31:35,094 main.py:47] epoch 511, training loss: 8330.31, average training loss: 8860.31, base loss: 8484.04
[INFO 2017-06-26 17:31:35,451 main.py:47] epoch 512, training loss: 7763.34, average training loss: 8858.17, base loss: 8484.72
[INFO 2017-06-26 17:31:35,809 main.py:47] epoch 513, training loss: 6995.51, average training loss: 8854.55, base loss: 8484.05
[INFO 2017-06-26 17:31:36,166 main.py:47] epoch 514, training loss: 9072.66, average training loss: 8854.97, base loss: 8488.67
[INFO 2017-06-26 17:31:36,522 main.py:47] epoch 515, training loss: 7467.41, average training loss: 8852.28, base loss: 8489.07
[INFO 2017-06-26 17:31:36,894 main.py:47] epoch 516, training loss: 7202.29, average training loss: 8849.09, base loss: 8489.29
[INFO 2017-06-26 17:31:37,249 main.py:47] epoch 517, training loss: 8316.26, average training loss: 8848.06, base loss: 8491.77
[INFO 2017-06-26 17:31:37,606 main.py:47] epoch 518, training loss: 7102.61, average training loss: 8844.70, base loss: 8491.75
[INFO 2017-06-26 17:31:37,962 main.py:47] epoch 519, training loss: 6854.61, average training loss: 8840.87, base loss: 8489.59
[INFO 2017-06-26 17:31:38,318 main.py:47] epoch 520, training loss: 7116.60, average training loss: 8837.56, base loss: 8488.98
[INFO 2017-06-26 17:31:38,678 main.py:47] epoch 521, training loss: 6811.97, average training loss: 8833.68, base loss: 8487.60
[INFO 2017-06-26 17:31:39,034 main.py:47] epoch 522, training loss: 7758.56, average training loss: 8831.62, base loss: 8488.53
[INFO 2017-06-26 17:31:39,391 main.py:47] epoch 523, training loss: 7367.79, average training loss: 8828.83, base loss: 8488.49
[INFO 2017-06-26 17:31:39,750 main.py:47] epoch 524, training loss: 7662.80, average training loss: 8826.61, base loss: 8489.65
[INFO 2017-06-26 17:31:40,135 main.py:47] epoch 525, training loss: 6138.94, average training loss: 8821.50, base loss: 8487.32
[INFO 2017-06-26 17:31:40,519 main.py:47] epoch 526, training loss: 6160.24, average training loss: 8816.45, base loss: 8484.70
[INFO 2017-06-26 17:31:40,902 main.py:47] epoch 527, training loss: 6918.18, average training loss: 8812.85, base loss: 8483.76
[INFO 2017-06-26 17:31:41,304 main.py:47] epoch 528, training loss: 6844.25, average training loss: 8809.13, base loss: 8482.99
[INFO 2017-06-26 17:31:41,679 main.py:47] epoch 529, training loss: 6774.55, average training loss: 8805.29, base loss: 8482.41
[INFO 2017-06-26 17:31:42,117 main.py:47] epoch 530, training loss: 6788.67, average training loss: 8801.50, base loss: 8481.63
[INFO 2017-06-26 17:31:42,481 main.py:47] epoch 531, training loss: 7620.21, average training loss: 8799.28, base loss: 8482.79
[INFO 2017-06-26 17:31:42,839 main.py:47] epoch 532, training loss: 7005.36, average training loss: 8795.91, base loss: 8483.37
[INFO 2017-06-26 17:31:43,195 main.py:47] epoch 533, training loss: 6257.55, average training loss: 8791.16, base loss: 8481.09
[INFO 2017-06-26 17:31:43,554 main.py:47] epoch 534, training loss: 6964.20, average training loss: 8787.74, base loss: 8480.77
[INFO 2017-06-26 17:31:43,913 main.py:47] epoch 535, training loss: 6537.26, average training loss: 8783.54, base loss: 8478.91
[INFO 2017-06-26 17:31:44,270 main.py:47] epoch 536, training loss: 6808.91, average training loss: 8779.87, base loss: 8477.59
[INFO 2017-06-26 17:31:44,629 main.py:47] epoch 537, training loss: 6376.01, average training loss: 8775.40, base loss: 8476.32
[INFO 2017-06-26 17:31:44,980 main.py:47] epoch 538, training loss: 7423.40, average training loss: 8772.89, base loss: 8477.23
[INFO 2017-06-26 17:31:45,334 main.py:47] epoch 539, training loss: 6708.83, average training loss: 8769.07, base loss: 8476.12
[INFO 2017-06-26 17:31:45,687 main.py:47] epoch 540, training loss: 7332.77, average training loss: 8766.41, base loss: 8476.36
[INFO 2017-06-26 17:31:46,043 main.py:47] epoch 541, training loss: 7070.98, average training loss: 8763.28, base loss: 8475.78
[INFO 2017-06-26 17:31:46,398 main.py:47] epoch 542, training loss: 9255.61, average training loss: 8764.19, base loss: 8479.92
[INFO 2017-06-26 17:31:46,750 main.py:47] epoch 543, training loss: 7470.94, average training loss: 8761.81, base loss: 8480.64
[INFO 2017-06-26 17:31:47,101 main.py:47] epoch 544, training loss: 6701.52, average training loss: 8758.03, base loss: 8479.19
[INFO 2017-06-26 17:31:47,455 main.py:47] epoch 545, training loss: 7459.54, average training loss: 8755.66, base loss: 8480.18
[INFO 2017-06-26 17:31:47,808 main.py:47] epoch 546, training loss: 6886.84, average training loss: 8752.24, base loss: 8479.26
[INFO 2017-06-26 17:31:48,214 main.py:47] epoch 547, training loss: 6819.97, average training loss: 8748.71, base loss: 8477.61
[INFO 2017-06-26 17:31:48,589 main.py:47] epoch 548, training loss: 7744.36, average training loss: 8746.88, base loss: 8478.86
[INFO 2017-06-26 17:31:48,949 main.py:47] epoch 549, training loss: 7124.53, average training loss: 8743.93, base loss: 8478.70
[INFO 2017-06-26 17:31:49,310 main.py:47] epoch 550, training loss: 6462.04, average training loss: 8739.79, base loss: 8477.09
[INFO 2017-06-26 17:31:49,665 main.py:47] epoch 551, training loss: 7479.32, average training loss: 8737.51, base loss: 8477.99
[INFO 2017-06-26 17:31:50,043 main.py:47] epoch 552, training loss: 6463.87, average training loss: 8733.40, base loss: 8477.01
[INFO 2017-06-26 17:31:50,397 main.py:47] epoch 553, training loss: 7724.07, average training loss: 8731.58, base loss: 8477.90
[INFO 2017-06-26 17:31:50,752 main.py:47] epoch 554, training loss: 6774.24, average training loss: 8728.05, base loss: 8477.07
[INFO 2017-06-26 17:31:51,107 main.py:47] epoch 555, training loss: 7585.27, average training loss: 8725.99, base loss: 8477.47
[INFO 2017-06-26 17:31:51,463 main.py:47] epoch 556, training loss: 6399.98, average training loss: 8721.82, base loss: 8475.44
[INFO 2017-06-26 17:31:51,817 main.py:47] epoch 557, training loss: 8078.07, average training loss: 8720.66, base loss: 8477.63
[INFO 2017-06-26 17:31:52,171 main.py:47] epoch 558, training loss: 6736.89, average training loss: 8717.11, base loss: 8476.23
[INFO 2017-06-26 17:31:52,524 main.py:47] epoch 559, training loss: 7284.33, average training loss: 8714.56, base loss: 8475.78
[INFO 2017-06-26 17:31:52,909 main.py:47] epoch 560, training loss: 6725.83, average training loss: 8711.01, base loss: 8474.56
[INFO 2017-06-26 17:31:53,295 main.py:47] epoch 561, training loss: 6583.24, average training loss: 8707.23, base loss: 8472.90
[INFO 2017-06-26 17:31:53,653 main.py:47] epoch 562, training loss: 7849.20, average training loss: 8705.70, base loss: 8474.79
[INFO 2017-06-26 17:31:54,027 main.py:47] epoch 563, training loss: 8105.67, average training loss: 8704.64, base loss: 8477.50
[INFO 2017-06-26 17:31:54,383 main.py:47] epoch 564, training loss: 6347.38, average training loss: 8700.47, base loss: 8476.28
[INFO 2017-06-26 17:31:54,786 main.py:47] epoch 565, training loss: 6862.53, average training loss: 8697.22, base loss: 8474.95
[INFO 2017-06-26 17:31:55,148 main.py:47] epoch 566, training loss: 7382.15, average training loss: 8694.90, base loss: 8475.48
[INFO 2017-06-26 17:31:55,508 main.py:47] epoch 567, training loss: 7098.92, average training loss: 8692.09, base loss: 8474.79
[INFO 2017-06-26 17:31:55,861 main.py:47] epoch 568, training loss: 6621.94, average training loss: 8688.45, base loss: 8473.36
[INFO 2017-06-26 17:31:56,215 main.py:47] epoch 569, training loss: 8258.26, average training loss: 8687.70, base loss: 8475.22
[INFO 2017-06-26 17:31:56,570 main.py:47] epoch 570, training loss: 6996.39, average training loss: 8684.73, base loss: 8475.33
[INFO 2017-06-26 17:31:56,923 main.py:47] epoch 571, training loss: 8141.76, average training loss: 8683.78, base loss: 8477.26
[INFO 2017-06-26 17:31:57,278 main.py:47] epoch 572, training loss: 7455.79, average training loss: 8681.64, base loss: 8478.04
[INFO 2017-06-26 17:31:57,630 main.py:47] epoch 573, training loss: 6232.67, average training loss: 8677.37, base loss: 8475.85
[INFO 2017-06-26 17:31:57,983 main.py:47] epoch 574, training loss: 7173.82, average training loss: 8674.76, base loss: 8475.22
[INFO 2017-06-26 17:31:58,335 main.py:47] epoch 575, training loss: 6775.41, average training loss: 8671.46, base loss: 8474.62
[INFO 2017-06-26 17:31:58,689 main.py:47] epoch 576, training loss: 6932.92, average training loss: 8668.45, base loss: 8474.35
[INFO 2017-06-26 17:31:59,086 main.py:47] epoch 577, training loss: 8327.66, average training loss: 8667.86, base loss: 8476.71
[INFO 2017-06-26 17:31:59,486 main.py:47] epoch 578, training loss: 7012.97, average training loss: 8665.00, base loss: 8477.08
[INFO 2017-06-26 17:31:59,866 main.py:47] epoch 579, training loss: 7192.89, average training loss: 8662.46, base loss: 8477.56
[INFO 2017-06-26 17:32:00,221 main.py:47] epoch 580, training loss: 7118.58, average training loss: 8659.81, base loss: 8477.49
[INFO 2017-06-26 17:32:00,575 main.py:47] epoch 581, training loss: 7438.34, average training loss: 8657.71, base loss: 8477.81
[INFO 2017-06-26 17:32:00,928 main.py:47] epoch 582, training loss: 7972.44, average training loss: 8656.53, base loss: 8479.70
[INFO 2017-06-26 17:32:01,284 main.py:47] epoch 583, training loss: 8016.89, average training loss: 8655.44, base loss: 8482.25
[INFO 2017-06-26 17:32:01,637 main.py:47] epoch 584, training loss: 7628.75, average training loss: 8653.68, base loss: 8482.66
[INFO 2017-06-26 17:32:01,990 main.py:47] epoch 585, training loss: 7184.72, average training loss: 8651.17, base loss: 8483.61
[INFO 2017-06-26 17:32:02,343 main.py:47] epoch 586, training loss: 7928.83, average training loss: 8649.94, base loss: 8485.56
[INFO 2017-06-26 17:32:02,733 main.py:47] epoch 587, training loss: 7138.79, average training loss: 8647.37, base loss: 8485.58
[INFO 2017-06-26 17:32:03,093 main.py:47] epoch 588, training loss: 7839.56, average training loss: 8646.00, base loss: 8487.40
[INFO 2017-06-26 17:32:03,460 main.py:47] epoch 589, training loss: 6267.57, average training loss: 8641.97, base loss: 8485.74
[INFO 2017-06-26 17:32:03,828 main.py:47] epoch 590, training loss: 6319.51, average training loss: 8638.04, base loss: 8483.75
[INFO 2017-06-26 17:32:04,191 main.py:47] epoch 591, training loss: 7079.31, average training loss: 8635.41, base loss: 8483.22
[INFO 2017-06-26 17:32:04,548 main.py:47] epoch 592, training loss: 8082.63, average training loss: 8634.48, base loss: 8485.14
[INFO 2017-06-26 17:32:04,936 main.py:47] epoch 593, training loss: 7152.58, average training loss: 8631.98, base loss: 8485.51
[INFO 2017-06-26 17:32:05,308 main.py:47] epoch 594, training loss: 6558.62, average training loss: 8628.50, base loss: 8484.02
[INFO 2017-06-26 17:32:05,663 main.py:47] epoch 595, training loss: 8725.54, average training loss: 8628.66, base loss: 8486.36
[INFO 2017-06-26 17:32:06,024 main.py:47] epoch 596, training loss: 7115.36, average training loss: 8626.13, base loss: 8485.62
[INFO 2017-06-26 17:32:06,492 main.py:47] epoch 597, training loss: 7034.40, average training loss: 8623.46, base loss: 8485.66
[INFO 2017-06-26 17:32:06,890 main.py:47] epoch 598, training loss: 6349.48, average training loss: 8619.67, base loss: 8484.05
[INFO 2017-06-26 17:32:07,256 main.py:47] epoch 599, training loss: 6640.60, average training loss: 8616.37, base loss: 8482.88
[INFO 2017-06-26 17:32:07,256 main.py:49] epoch 599, testing
[INFO 2017-06-26 17:32:12,108 main.py:100] average testing loss: 7489.44, base loss: 8936.00
[INFO 2017-06-26 17:32:12,132 main.py:73] current best accuracy: 7415.83
[INFO 2017-06-26 17:32:12,498 main.py:47] epoch 600, training loss: 6350.53, average training loss: 8612.60, base loss: 8481.14
[INFO 2017-06-26 17:32:12,852 main.py:47] epoch 601, training loss: 7357.69, average training loss: 8610.51, base loss: 8481.55
[INFO 2017-06-26 17:32:13,211 main.py:47] epoch 602, training loss: 7688.48, average training loss: 8608.99, base loss: 8482.06
[INFO 2017-06-26 17:32:13,610 main.py:47] epoch 603, training loss: 6850.76, average training loss: 8606.07, base loss: 8481.40
[INFO 2017-06-26 17:32:14,009 main.py:47] epoch 604, training loss: 7776.75, average training loss: 8604.70, base loss: 8482.25
[INFO 2017-06-26 17:32:14,372 main.py:47] epoch 605, training loss: 7297.52, average training loss: 8602.55, base loss: 8482.88
[INFO 2017-06-26 17:32:14,724 main.py:47] epoch 606, training loss: 8072.19, average training loss: 8601.67, base loss: 8485.26
[INFO 2017-06-26 17:32:15,077 main.py:47] epoch 607, training loss: 7021.66, average training loss: 8599.07, base loss: 8484.71
[INFO 2017-06-26 17:32:15,430 main.py:47] epoch 608, training loss: 7037.67, average training loss: 8596.51, base loss: 8484.77
[INFO 2017-06-26 17:32:15,785 main.py:47] epoch 609, training loss: 8194.33, average training loss: 8595.85, base loss: 8487.17
[INFO 2017-06-26 17:32:16,139 main.py:47] epoch 610, training loss: 8766.77, average training loss: 8596.13, base loss: 8490.60
[INFO 2017-06-26 17:32:16,514 main.py:47] epoch 611, training loss: 7638.09, average training loss: 8594.56, base loss: 8492.05
[INFO 2017-06-26 17:32:16,874 main.py:47] epoch 612, training loss: 7667.90, average training loss: 8593.05, base loss: 8492.86
[INFO 2017-06-26 17:32:17,238 main.py:47] epoch 613, training loss: 8032.39, average training loss: 8592.14, base loss: 8495.40
[INFO 2017-06-26 17:32:17,600 main.py:47] epoch 614, training loss: 7628.34, average training loss: 8590.57, base loss: 8496.60
[INFO 2017-06-26 17:32:17,956 main.py:47] epoch 615, training loss: 8362.21, average training loss: 8590.20, base loss: 8498.55
[INFO 2017-06-26 17:32:18,309 main.py:47] epoch 616, training loss: 6578.48, average training loss: 8586.94, base loss: 8497.46
[INFO 2017-06-26 17:32:18,700 main.py:47] epoch 617, training loss: 6706.39, average training loss: 8583.90, base loss: 8496.46
[INFO 2017-06-26 17:32:19,065 main.py:47] epoch 618, training loss: 6464.73, average training loss: 8580.48, base loss: 8495.22
[INFO 2017-06-26 17:32:19,426 main.py:47] epoch 619, training loss: 6554.94, average training loss: 8577.21, base loss: 8493.89
[INFO 2017-06-26 17:32:19,781 main.py:47] epoch 620, training loss: 6788.45, average training loss: 8574.33, base loss: 8492.70
[INFO 2017-06-26 17:32:20,135 main.py:47] epoch 621, training loss: 7502.24, average training loss: 8572.60, base loss: 8493.16
[INFO 2017-06-26 17:32:20,490 main.py:47] epoch 622, training loss: 8686.21, average training loss: 8572.79, base loss: 8495.79
[INFO 2017-06-26 17:32:20,876 main.py:47] epoch 623, training loss: 5926.58, average training loss: 8568.55, base loss: 8493.19
[INFO 2017-06-26 17:32:21,245 main.py:47] epoch 624, training loss: 6937.01, average training loss: 8565.94, base loss: 8492.13
[INFO 2017-06-26 17:32:21,616 main.py:47] epoch 625, training loss: 7464.73, average training loss: 8564.18, base loss: 8492.84
[INFO 2017-06-26 17:32:21,975 main.py:47] epoch 626, training loss: 6724.15, average training loss: 8561.24, base loss: 8491.92
[INFO 2017-06-26 17:32:22,327 main.py:47] epoch 627, training loss: 7447.11, average training loss: 8559.47, base loss: 8492.76
[INFO 2017-06-26 17:32:22,686 main.py:47] epoch 628, training loss: 7431.17, average training loss: 8557.67, base loss: 8492.84
[INFO 2017-06-26 17:32:23,046 main.py:47] epoch 629, training loss: 6743.07, average training loss: 8554.79, base loss: 8491.41
[INFO 2017-06-26 17:32:23,432 main.py:47] epoch 630, training loss: 6474.93, average training loss: 8551.50, base loss: 8489.54
[INFO 2017-06-26 17:32:23,806 main.py:47] epoch 631, training loss: 6871.90, average training loss: 8548.84, base loss: 8489.09
[INFO 2017-06-26 17:32:24,180 main.py:47] epoch 632, training loss: 7270.23, average training loss: 8546.82, base loss: 8489.33
[INFO 2017-06-26 17:32:24,534 main.py:47] epoch 633, training loss: 6728.44, average training loss: 8543.95, base loss: 8488.27
[INFO 2017-06-26 17:32:24,922 main.py:47] epoch 634, training loss: 7046.33, average training loss: 8541.59, base loss: 8488.20
[INFO 2017-06-26 17:32:25,310 main.py:47] epoch 635, training loss: 7558.78, average training loss: 8540.05, base loss: 8489.08
[INFO 2017-06-26 17:32:25,667 main.py:47] epoch 636, training loss: 7864.42, average training loss: 8538.99, base loss: 8490.52
[INFO 2017-06-26 17:32:26,024 main.py:47] epoch 637, training loss: 7033.51, average training loss: 8536.63, base loss: 8489.99
[INFO 2017-06-26 17:32:26,385 main.py:47] epoch 638, training loss: 6156.33, average training loss: 8532.90, base loss: 8488.36
[INFO 2017-06-26 17:32:26,736 main.py:47] epoch 639, training loss: 7380.22, average training loss: 8531.10, base loss: 8489.22
[INFO 2017-06-26 17:32:27,090 main.py:47] epoch 640, training loss: 7048.24, average training loss: 8528.79, base loss: 8488.41
[INFO 2017-06-26 17:32:27,446 main.py:47] epoch 641, training loss: 7182.49, average training loss: 8526.69, base loss: 8488.42
[INFO 2017-06-26 17:32:27,806 main.py:47] epoch 642, training loss: 6488.18, average training loss: 8523.52, base loss: 8486.61
[INFO 2017-06-26 17:32:28,159 main.py:47] epoch 643, training loss: 8611.38, average training loss: 8523.66, base loss: 8488.16
[INFO 2017-06-26 17:32:28,545 main.py:47] epoch 644, training loss: 7150.38, average training loss: 8521.53, base loss: 8487.35
[INFO 2017-06-26 17:32:28,902 main.py:47] epoch 645, training loss: 6275.95, average training loss: 8518.05, base loss: 8485.27
[INFO 2017-06-26 17:32:29,261 main.py:47] epoch 646, training loss: 8212.19, average training loss: 8517.58, base loss: 8487.11
[INFO 2017-06-26 17:32:29,620 main.py:47] epoch 647, training loss: 7878.83, average training loss: 8516.59, base loss: 8488.21
[INFO 2017-06-26 17:32:29,973 main.py:47] epoch 648, training loss: 7508.20, average training loss: 8515.04, base loss: 8488.60
[INFO 2017-06-26 17:32:30,326 main.py:47] epoch 649, training loss: 7369.76, average training loss: 8513.28, base loss: 8488.88
[INFO 2017-06-26 17:32:30,712 main.py:47] epoch 650, training loss: 7523.60, average training loss: 8511.76, base loss: 8490.11
[INFO 2017-06-26 17:32:31,067 main.py:47] epoch 651, training loss: 6734.12, average training loss: 8509.03, base loss: 8489.55
[INFO 2017-06-26 17:32:31,437 main.py:47] epoch 652, training loss: 7838.69, average training loss: 8508.00, base loss: 8491.84
[INFO 2017-06-26 17:32:31,790 main.py:47] epoch 653, training loss: 6889.09, average training loss: 8505.53, base loss: 8490.41
[INFO 2017-06-26 17:32:32,140 main.py:47] epoch 654, training loss: 6890.15, average training loss: 8503.06, base loss: 8489.49
[INFO 2017-06-26 17:32:32,494 main.py:47] epoch 655, training loss: 7023.31, average training loss: 8500.81, base loss: 8489.08
[INFO 2017-06-26 17:32:32,871 main.py:47] epoch 656, training loss: 8234.53, average training loss: 8500.40, base loss: 8489.72
[INFO 2017-06-26 17:32:33,233 main.py:47] epoch 657, training loss: 6850.69, average training loss: 8497.89, base loss: 8488.75
[INFO 2017-06-26 17:32:33,594 main.py:47] epoch 658, training loss: 7129.02, average training loss: 8495.82, base loss: 8488.83
[INFO 2017-06-26 17:32:33,948 main.py:47] epoch 659, training loss: 7416.86, average training loss: 8494.18, base loss: 8489.26
[INFO 2017-06-26 17:32:34,301 main.py:47] epoch 660, training loss: 7275.99, average training loss: 8492.34, base loss: 8488.90
[INFO 2017-06-26 17:32:34,655 main.py:47] epoch 661, training loss: 6015.63, average training loss: 8488.60, base loss: 8486.59
[INFO 2017-06-26 17:32:35,009 main.py:47] epoch 662, training loss: 7226.87, average training loss: 8486.70, base loss: 8486.20
[INFO 2017-06-26 17:32:35,363 main.py:47] epoch 663, training loss: 6829.46, average training loss: 8484.20, base loss: 8485.55
[INFO 2017-06-26 17:32:35,714 main.py:47] epoch 664, training loss: 5430.00, average training loss: 8479.61, base loss: 8482.12
[INFO 2017-06-26 17:32:36,068 main.py:47] epoch 665, training loss: 7191.39, average training loss: 8477.67, base loss: 8482.29
[INFO 2017-06-26 17:32:36,454 main.py:47] epoch 666, training loss: 7601.80, average training loss: 8476.36, base loss: 8483.31
[INFO 2017-06-26 17:32:36,831 main.py:47] epoch 667, training loss: 6459.14, average training loss: 8473.34, base loss: 8482.40
[INFO 2017-06-26 17:32:37,199 main.py:47] epoch 668, training loss: 7307.68, average training loss: 8471.60, base loss: 8483.21
[INFO 2017-06-26 17:32:37,553 main.py:47] epoch 669, training loss: 7233.02, average training loss: 8469.75, base loss: 8483.59
[INFO 2017-06-26 17:32:37,960 main.py:47] epoch 670, training loss: 7164.29, average training loss: 8467.80, base loss: 8484.00
[INFO 2017-06-26 17:32:38,366 main.py:47] epoch 671, training loss: 6709.91, average training loss: 8465.19, base loss: 8483.30
[INFO 2017-06-26 17:32:38,724 main.py:47] epoch 672, training loss: 6564.17, average training loss: 8462.36, base loss: 8482.32
[INFO 2017-06-26 17:32:39,118 main.py:47] epoch 673, training loss: 6452.95, average training loss: 8459.38, base loss: 8480.93
[INFO 2017-06-26 17:32:39,481 main.py:47] epoch 674, training loss: 8073.60, average training loss: 8458.81, base loss: 8482.90
[INFO 2017-06-26 17:32:39,845 main.py:47] epoch 675, training loss: 6722.61, average training loss: 8456.24, base loss: 8482.24
[INFO 2017-06-26 17:32:40,241 main.py:47] epoch 676, training loss: 6128.22, average training loss: 8452.80, base loss: 8480.95
[INFO 2017-06-26 17:32:40,598 main.py:47] epoch 677, training loss: 6767.84, average training loss: 8450.32, base loss: 8479.74
[INFO 2017-06-26 17:32:40,969 main.py:47] epoch 678, training loss: 8235.82, average training loss: 8450.00, base loss: 8481.94
[INFO 2017-06-26 17:32:41,321 main.py:47] epoch 679, training loss: 7255.50, average training loss: 8448.24, base loss: 8482.26
[INFO 2017-06-26 17:32:41,672 main.py:47] epoch 680, training loss: 6885.60, average training loss: 8445.95, base loss: 8482.01
[INFO 2017-06-26 17:32:42,028 main.py:47] epoch 681, training loss: 8566.09, average training loss: 8446.13, base loss: 8483.87
[INFO 2017-06-26 17:32:42,382 main.py:47] epoch 682, training loss: 7473.64, average training loss: 8444.70, base loss: 8484.49
[INFO 2017-06-26 17:32:42,738 main.py:47] epoch 683, training loss: 6500.85, average training loss: 8441.86, base loss: 8483.53
[INFO 2017-06-26 17:32:43,115 main.py:47] epoch 684, training loss: 7092.39, average training loss: 8439.89, base loss: 8483.91
[INFO 2017-06-26 17:32:43,474 main.py:47] epoch 685, training loss: 6634.13, average training loss: 8437.26, base loss: 8483.47
[INFO 2017-06-26 17:32:43,837 main.py:47] epoch 686, training loss: 7620.06, average training loss: 8436.07, base loss: 8484.41
[INFO 2017-06-26 17:32:44,246 main.py:47] epoch 687, training loss: 7322.62, average training loss: 8434.45, base loss: 8484.58
[INFO 2017-06-26 17:32:44,602 main.py:47] epoch 688, training loss: 6861.67, average training loss: 8432.17, base loss: 8484.20
[INFO 2017-06-26 17:32:44,973 main.py:47] epoch 689, training loss: 7220.66, average training loss: 8430.41, base loss: 8484.31
[INFO 2017-06-26 17:32:45,328 main.py:47] epoch 690, training loss: 8321.69, average training loss: 8430.25, base loss: 8487.13
[INFO 2017-06-26 17:32:45,682 main.py:47] epoch 691, training loss: 7337.19, average training loss: 8428.68, base loss: 8487.57
[INFO 2017-06-26 17:32:46,035 main.py:47] epoch 692, training loss: 6581.44, average training loss: 8426.01, base loss: 8487.05
[INFO 2017-06-26 17:32:46,389 main.py:47] epoch 693, training loss: 7627.48, average training loss: 8424.86, base loss: 8487.92
[INFO 2017-06-26 17:32:46,747 main.py:47] epoch 694, training loss: 7754.76, average training loss: 8423.89, base loss: 8488.81
[INFO 2017-06-26 17:32:47,100 main.py:47] epoch 695, training loss: 7882.74, average training loss: 8423.12, base loss: 8489.96
[INFO 2017-06-26 17:32:47,453 main.py:47] epoch 696, training loss: 7484.93, average training loss: 8421.77, base loss: 8490.43
[INFO 2017-06-26 17:32:47,806 main.py:47] epoch 697, training loss: 6946.18, average training loss: 8419.66, base loss: 8489.78
[INFO 2017-06-26 17:32:48,172 main.py:47] epoch 698, training loss: 8046.20, average training loss: 8419.12, base loss: 8491.97
[INFO 2017-06-26 17:32:48,527 main.py:47] epoch 699, training loss: 6774.05, average training loss: 8416.77, base loss: 8491.22
[INFO 2017-06-26 17:32:48,527 main.py:49] epoch 699, testing
[INFO 2017-06-26 17:32:52,719 main.py:100] average testing loss: 7088.64, base loss: 8510.95
[INFO 2017-06-26 17:32:52,742 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:32:52,749 main.py:73] current best accuracy: 7088.64
[INFO 2017-06-26 17:32:53,102 main.py:47] epoch 700, training loss: 6158.01, average training loss: 8413.55, base loss: 8489.47
[INFO 2017-06-26 17:32:53,462 main.py:47] epoch 701, training loss: 7419.09, average training loss: 8412.13, base loss: 8489.81
[INFO 2017-06-26 17:32:53,814 main.py:47] epoch 702, training loss: 6453.59, average training loss: 8409.35, base loss: 8488.25
[INFO 2017-06-26 17:32:54,167 main.py:47] epoch 703, training loss: 8366.91, average training loss: 8409.29, base loss: 8489.65
[INFO 2017-06-26 17:32:54,521 main.py:47] epoch 704, training loss: 7127.55, average training loss: 8407.47, base loss: 8488.79
[INFO 2017-06-26 17:32:54,875 main.py:47] epoch 705, training loss: 6452.30, average training loss: 8404.70, base loss: 8488.11
[INFO 2017-06-26 17:32:55,232 main.py:47] epoch 706, training loss: 7377.83, average training loss: 8403.25, base loss: 8488.96
[INFO 2017-06-26 17:32:55,588 main.py:47] epoch 707, training loss: 6978.98, average training loss: 8401.24, base loss: 8488.25
[INFO 2017-06-26 17:32:55,942 main.py:47] epoch 708, training loss: 6613.82, average training loss: 8398.72, base loss: 8487.61
[INFO 2017-06-26 17:32:56,295 main.py:47] epoch 709, training loss: 7027.82, average training loss: 8396.78, base loss: 8487.56
[INFO 2017-06-26 17:32:56,648 main.py:47] epoch 710, training loss: 7846.42, average training loss: 8396.01, base loss: 8489.31
[INFO 2017-06-26 17:32:57,003 main.py:47] epoch 711, training loss: 6906.95, average training loss: 8393.92, base loss: 8489.75
[INFO 2017-06-26 17:32:57,356 main.py:47] epoch 712, training loss: 7151.03, average training loss: 8392.18, base loss: 8489.28
[INFO 2017-06-26 17:32:57,714 main.py:47] epoch 713, training loss: 6653.44, average training loss: 8389.74, base loss: 8489.03
[INFO 2017-06-26 17:32:58,069 main.py:47] epoch 714, training loss: 7068.07, average training loss: 8387.89, base loss: 8488.79
[INFO 2017-06-26 17:32:58,431 main.py:47] epoch 715, training loss: 6885.06, average training loss: 8385.79, base loss: 8487.90
[INFO 2017-06-26 17:32:58,786 main.py:47] epoch 716, training loss: 7091.14, average training loss: 8383.99, base loss: 8487.64
[INFO 2017-06-26 17:32:59,141 main.py:47] epoch 717, training loss: 7623.40, average training loss: 8382.93, base loss: 8489.15
[INFO 2017-06-26 17:32:59,497 main.py:47] epoch 718, training loss: 6572.73, average training loss: 8380.41, base loss: 8487.91
[INFO 2017-06-26 17:32:59,853 main.py:47] epoch 719, training loss: 7682.67, average training loss: 8379.44, base loss: 8489.36
[INFO 2017-06-26 17:33:00,208 main.py:47] epoch 720, training loss: 6853.29, average training loss: 8377.32, base loss: 8488.86
[INFO 2017-06-26 17:33:00,562 main.py:47] epoch 721, training loss: 7339.26, average training loss: 8375.89, base loss: 8488.99
[INFO 2017-06-26 17:33:00,916 main.py:47] epoch 722, training loss: 8164.03, average training loss: 8375.59, base loss: 8490.90
[INFO 2017-06-26 17:33:01,272 main.py:47] epoch 723, training loss: 6697.52, average training loss: 8373.28, base loss: 8490.23
[INFO 2017-06-26 17:33:01,629 main.py:47] epoch 724, training loss: 6091.38, average training loss: 8370.13, base loss: 8488.21
[INFO 2017-06-26 17:33:01,982 main.py:47] epoch 725, training loss: 7460.62, average training loss: 8368.88, base loss: 8489.14
[INFO 2017-06-26 17:33:02,351 main.py:47] epoch 726, training loss: 7418.19, average training loss: 8367.57, base loss: 8489.26
[INFO 2017-06-26 17:33:02,731 main.py:47] epoch 727, training loss: 8667.59, average training loss: 8367.98, base loss: 8492.93
[INFO 2017-06-26 17:33:03,118 main.py:47] epoch 728, training loss: 7870.15, average training loss: 8367.30, base loss: 8494.88
[INFO 2017-06-26 17:33:03,508 main.py:47] epoch 729, training loss: 7242.91, average training loss: 8365.76, base loss: 8494.25
[INFO 2017-06-26 17:33:03,878 main.py:47] epoch 730, training loss: 7439.12, average training loss: 8364.49, base loss: 8495.20
[INFO 2017-06-26 17:33:04,250 main.py:47] epoch 731, training loss: 6538.14, average training loss: 8361.99, base loss: 8493.75
[INFO 2017-06-26 17:33:04,651 main.py:47] epoch 732, training loss: 6733.81, average training loss: 8359.77, base loss: 8493.41
[INFO 2017-06-26 17:33:05,013 main.py:47] epoch 733, training loss: 6668.97, average training loss: 8357.47, base loss: 8492.37
[INFO 2017-06-26 17:33:05,370 main.py:47] epoch 734, training loss: 6193.78, average training loss: 8354.53, base loss: 8490.70
[INFO 2017-06-26 17:33:05,724 main.py:47] epoch 735, training loss: 7319.53, average training loss: 8353.12, base loss: 8490.49
[INFO 2017-06-26 17:33:06,088 main.py:47] epoch 736, training loss: 7768.76, average training loss: 8352.33, base loss: 8491.49
[INFO 2017-06-26 17:33:06,441 main.py:47] epoch 737, training loss: 6855.72, average training loss: 8350.30, base loss: 8490.56
[INFO 2017-06-26 17:33:06,796 main.py:47] epoch 738, training loss: 6597.55, average training loss: 8347.93, base loss: 8489.23
[INFO 2017-06-26 17:33:07,149 main.py:47] epoch 739, training loss: 6616.36, average training loss: 8345.59, base loss: 8488.09
[INFO 2017-06-26 17:33:07,501 main.py:47] epoch 740, training loss: 6878.83, average training loss: 8343.61, base loss: 8487.35
[INFO 2017-06-26 17:33:07,854 main.py:47] epoch 741, training loss: 7381.06, average training loss: 8342.31, base loss: 8487.74
[INFO 2017-06-26 17:33:08,208 main.py:47] epoch 742, training loss: 9396.69, average training loss: 8343.73, base loss: 8491.07
[INFO 2017-06-26 17:33:08,561 main.py:47] epoch 743, training loss: 6941.40, average training loss: 8341.84, base loss: 8490.89
[INFO 2017-06-26 17:33:08,915 main.py:47] epoch 744, training loss: 6206.11, average training loss: 8338.98, base loss: 8488.93
[INFO 2017-06-26 17:33:09,268 main.py:47] epoch 745, training loss: 6501.42, average training loss: 8336.51, base loss: 8487.59
[INFO 2017-06-26 17:33:09,623 main.py:47] epoch 746, training loss: 6432.78, average training loss: 8333.97, base loss: 8486.38
[INFO 2017-06-26 17:33:09,978 main.py:47] epoch 747, training loss: 6628.24, average training loss: 8331.69, base loss: 8485.42
[INFO 2017-06-26 17:33:10,332 main.py:47] epoch 748, training loss: 7020.78, average training loss: 8329.94, base loss: 8485.40
[INFO 2017-06-26 17:33:10,697 main.py:47] epoch 749, training loss: 7669.47, average training loss: 8329.05, base loss: 8486.19
[INFO 2017-06-26 17:33:11,051 main.py:47] epoch 750, training loss: 7209.36, average training loss: 8327.56, base loss: 8486.31
[INFO 2017-06-26 17:33:11,408 main.py:47] epoch 751, training loss: 8305.39, average training loss: 8327.53, base loss: 8488.32
[INFO 2017-06-26 17:33:11,763 main.py:47] epoch 752, training loss: 6530.27, average training loss: 8325.15, base loss: 8486.93
[INFO 2017-06-26 17:33:12,119 main.py:47] epoch 753, training loss: 6942.35, average training loss: 8323.31, base loss: 8486.53
[INFO 2017-06-26 17:33:12,475 main.py:47] epoch 754, training loss: 6612.21, average training loss: 8321.05, base loss: 8485.69
[INFO 2017-06-26 17:33:12,832 main.py:47] epoch 755, training loss: 7142.75, average training loss: 8319.49, base loss: 8486.06
[INFO 2017-06-26 17:33:13,189 main.py:47] epoch 756, training loss: 6593.64, average training loss: 8317.21, base loss: 8484.52
[INFO 2017-06-26 17:33:13,545 main.py:47] epoch 757, training loss: 7365.60, average training loss: 8315.95, base loss: 8484.87
[INFO 2017-06-26 17:33:13,901 main.py:47] epoch 758, training loss: 7184.40, average training loss: 8314.46, base loss: 8485.37
[INFO 2017-06-26 17:33:14,259 main.py:47] epoch 759, training loss: 7048.41, average training loss: 8312.80, base loss: 8485.20
[INFO 2017-06-26 17:33:14,612 main.py:47] epoch 760, training loss: 7120.74, average training loss: 8311.23, base loss: 8484.76
[INFO 2017-06-26 17:33:14,968 main.py:47] epoch 761, training loss: 6134.14, average training loss: 8308.37, base loss: 8483.29
[INFO 2017-06-26 17:33:15,354 main.py:47] epoch 762, training loss: 6008.07, average training loss: 8305.36, base loss: 8481.32
[INFO 2017-06-26 17:33:15,742 main.py:47] epoch 763, training loss: 6696.73, average training loss: 8303.25, base loss: 8480.72
[INFO 2017-06-26 17:33:16,109 main.py:47] epoch 764, training loss: 7674.74, average training loss: 8302.43, base loss: 8481.51
[INFO 2017-06-26 17:33:16,466 main.py:47] epoch 765, training loss: 6157.45, average training loss: 8299.63, base loss: 8479.82
[INFO 2017-06-26 17:33:16,818 main.py:47] epoch 766, training loss: 6225.99, average training loss: 8296.93, base loss: 8478.08
[INFO 2017-06-26 17:33:17,203 main.py:47] epoch 767, training loss: 7164.61, average training loss: 8295.45, base loss: 8478.16
[INFO 2017-06-26 17:33:17,610 main.py:47] epoch 768, training loss: 6785.37, average training loss: 8293.49, base loss: 8477.55
[INFO 2017-06-26 17:33:17,973 main.py:47] epoch 769, training loss: 7428.47, average training loss: 8292.37, base loss: 8477.99
[INFO 2017-06-26 17:33:18,331 main.py:47] epoch 770, training loss: 6696.18, average training loss: 8290.30, base loss: 8477.12
[INFO 2017-06-26 17:33:18,688 main.py:47] epoch 771, training loss: 7035.84, average training loss: 8288.67, base loss: 8477.63
[INFO 2017-06-26 17:33:19,043 main.py:47] epoch 772, training loss: 6692.22, average training loss: 8286.61, base loss: 8477.09
[INFO 2017-06-26 17:33:19,396 main.py:47] epoch 773, training loss: 8166.81, average training loss: 8286.45, base loss: 8479.32
[INFO 2017-06-26 17:33:19,749 main.py:47] epoch 774, training loss: 6749.93, average training loss: 8284.47, base loss: 8478.05
[INFO 2017-06-26 17:33:20,103 main.py:47] epoch 775, training loss: 7242.86, average training loss: 8283.13, base loss: 8478.35
[INFO 2017-06-26 17:33:20,457 main.py:47] epoch 776, training loss: 6838.54, average training loss: 8281.27, base loss: 8478.06
[INFO 2017-06-26 17:33:20,848 main.py:47] epoch 777, training loss: 6827.36, average training loss: 8279.40, base loss: 8477.46
[INFO 2017-06-26 17:33:21,220 main.py:47] epoch 778, training loss: 7989.47, average training loss: 8279.03, base loss: 8478.52
[INFO 2017-06-26 17:33:21,581 main.py:47] epoch 779, training loss: 7229.52, average training loss: 8277.68, base loss: 8479.00
[INFO 2017-06-26 17:33:21,944 main.py:47] epoch 780, training loss: 7335.22, average training loss: 8276.47, base loss: 8478.84
[INFO 2017-06-26 17:33:22,317 main.py:47] epoch 781, training loss: 8449.80, average training loss: 8276.70, base loss: 8480.80
[INFO 2017-06-26 17:33:22,683 main.py:47] epoch 782, training loss: 7764.16, average training loss: 8276.04, base loss: 8482.31
[INFO 2017-06-26 17:33:23,037 main.py:47] epoch 783, training loss: 6768.08, average training loss: 8274.12, base loss: 8481.87
[INFO 2017-06-26 17:33:23,425 main.py:47] epoch 784, training loss: 7141.59, average training loss: 8272.67, base loss: 8481.93
[INFO 2017-06-26 17:33:23,790 main.py:47] epoch 785, training loss: 7178.44, average training loss: 8271.28, base loss: 8481.98
[INFO 2017-06-26 17:33:24,159 main.py:47] epoch 786, training loss: 7145.46, average training loss: 8269.85, base loss: 8481.93
[INFO 2017-06-26 17:33:24,513 main.py:47] epoch 787, training loss: 7620.02, average training loss: 8269.03, base loss: 8482.90
[INFO 2017-06-26 17:33:24,866 main.py:47] epoch 788, training loss: 7333.29, average training loss: 8267.84, base loss: 8483.45
[INFO 2017-06-26 17:33:25,221 main.py:47] epoch 789, training loss: 8235.04, average training loss: 8267.80, base loss: 8485.20
[INFO 2017-06-26 17:33:25,608 main.py:47] epoch 790, training loss: 7382.11, average training loss: 8266.68, base loss: 8485.11
[INFO 2017-06-26 17:33:25,978 main.py:47] epoch 791, training loss: 7118.31, average training loss: 8265.23, base loss: 8484.63
[INFO 2017-06-26 17:33:26,344 main.py:47] epoch 792, training loss: 7090.52, average training loss: 8263.75, base loss: 8484.05
[INFO 2017-06-26 17:33:26,754 main.py:47] epoch 793, training loss: 6699.98, average training loss: 8261.78, base loss: 8483.53
[INFO 2017-06-26 17:33:27,112 main.py:47] epoch 794, training loss: 6962.18, average training loss: 8260.14, base loss: 8483.36
[INFO 2017-06-26 17:33:27,477 main.py:47] epoch 795, training loss: 8015.41, average training loss: 8259.84, base loss: 8485.44
[INFO 2017-06-26 17:33:27,835 main.py:47] epoch 796, training loss: 6684.65, average training loss: 8257.86, base loss: 8485.24
[INFO 2017-06-26 17:33:28,193 main.py:47] epoch 797, training loss: 7190.47, average training loss: 8256.52, base loss: 8485.45
[INFO 2017-06-26 17:33:28,568 main.py:47] epoch 798, training loss: 7830.60, average training loss: 8255.99, base loss: 8486.66
[INFO 2017-06-26 17:33:28,920 main.py:47] epoch 799, training loss: 6767.38, average training loss: 8254.13, base loss: 8486.34
[INFO 2017-06-26 17:33:28,920 main.py:49] epoch 799, testing
[INFO 2017-06-26 17:33:33,220 main.py:100] average testing loss: 7212.24, base loss: 8731.86
[INFO 2017-06-26 17:33:33,246 main.py:73] current best accuracy: 7088.64
[INFO 2017-06-26 17:33:33,599 main.py:47] epoch 800, training loss: 7513.13, average training loss: 8253.20, base loss: 8487.77
[INFO 2017-06-26 17:33:33,956 main.py:47] epoch 801, training loss: 6677.36, average training loss: 8251.24, base loss: 8486.93
[INFO 2017-06-26 17:33:34,310 main.py:47] epoch 802, training loss: 6605.61, average training loss: 8249.19, base loss: 8485.70
[INFO 2017-06-26 17:33:34,664 main.py:47] epoch 803, training loss: 7176.34, average training loss: 8247.86, base loss: 8485.76
[INFO 2017-06-26 17:33:35,020 main.py:47] epoch 804, training loss: 7048.33, average training loss: 8246.37, base loss: 8486.38
[INFO 2017-06-26 17:33:35,374 main.py:47] epoch 805, training loss: 7555.98, average training loss: 8245.51, base loss: 8486.43
[INFO 2017-06-26 17:33:35,728 main.py:47] epoch 806, training loss: 6899.80, average training loss: 8243.84, base loss: 8486.02
[INFO 2017-06-26 17:33:36,083 main.py:47] epoch 807, training loss: 6535.95, average training loss: 8241.73, base loss: 8485.29
[INFO 2017-06-26 17:33:36,439 main.py:47] epoch 808, training loss: 6414.62, average training loss: 8239.47, base loss: 8484.27
[INFO 2017-06-26 17:33:36,791 main.py:47] epoch 809, training loss: 7758.36, average training loss: 8238.88, base loss: 8485.71
[INFO 2017-06-26 17:33:37,145 main.py:47] epoch 810, training loss: 7266.89, average training loss: 8237.68, base loss: 8486.16
[INFO 2017-06-26 17:33:37,501 main.py:47] epoch 811, training loss: 6655.30, average training loss: 8235.73, base loss: 8485.58
[INFO 2017-06-26 17:33:37,856 main.py:47] epoch 812, training loss: 6825.88, average training loss: 8233.99, base loss: 8485.27
[INFO 2017-06-26 17:33:38,209 main.py:47] epoch 813, training loss: 7147.50, average training loss: 8232.66, base loss: 8485.38
[INFO 2017-06-26 17:33:38,573 main.py:47] epoch 814, training loss: 6968.70, average training loss: 8231.11, base loss: 8485.62
[INFO 2017-06-26 17:33:38,925 main.py:47] epoch 815, training loss: 7759.36, average training loss: 8230.53, base loss: 8486.15
[INFO 2017-06-26 17:33:39,279 main.py:47] epoch 816, training loss: 7416.78, average training loss: 8229.53, base loss: 8486.49
[INFO 2017-06-26 17:33:39,633 main.py:47] epoch 817, training loss: 7233.27, average training loss: 8228.32, base loss: 8486.33
[INFO 2017-06-26 17:33:40,019 main.py:47] epoch 818, training loss: 7232.37, average training loss: 8227.10, base loss: 8486.62
[INFO 2017-06-26 17:33:40,414 main.py:47] epoch 819, training loss: 7366.55, average training loss: 8226.05, base loss: 8487.25
[INFO 2017-06-26 17:33:40,769 main.py:47] epoch 820, training loss: 8504.11, average training loss: 8226.39, base loss: 8489.94
[INFO 2017-06-26 17:33:41,125 main.py:47] epoch 821, training loss: 7944.65, average training loss: 8226.05, base loss: 8491.26
[INFO 2017-06-26 17:33:41,478 main.py:47] epoch 822, training loss: 7111.76, average training loss: 8224.69, base loss: 8491.65
[INFO 2017-06-26 17:33:41,833 main.py:47] epoch 823, training loss: 7627.14, average training loss: 8223.97, base loss: 8492.34
[INFO 2017-06-26 17:33:42,188 main.py:47] epoch 824, training loss: 6869.64, average training loss: 8222.33, base loss: 8491.61
[INFO 2017-06-26 17:33:42,540 main.py:47] epoch 825, training loss: 6641.89, average training loss: 8220.41, base loss: 8491.54
[INFO 2017-06-26 17:33:42,895 main.py:47] epoch 826, training loss: 6794.89, average training loss: 8218.69, base loss: 8491.33
[INFO 2017-06-26 17:33:43,250 main.py:47] epoch 827, training loss: 6395.76, average training loss: 8216.49, base loss: 8490.39
[INFO 2017-06-26 17:33:43,617 main.py:47] epoch 828, training loss: 6209.51, average training loss: 8214.07, base loss: 8488.90
[INFO 2017-06-26 17:33:43,967 main.py:47] epoch 829, training loss: 7597.65, average training loss: 8213.32, base loss: 8489.41
[INFO 2017-06-26 17:33:44,322 main.py:47] epoch 830, training loss: 7443.31, average training loss: 8212.40, base loss: 8489.82
[INFO 2017-06-26 17:33:44,673 main.py:47] epoch 831, training loss: 6913.68, average training loss: 8210.84, base loss: 8489.66
[INFO 2017-06-26 17:33:45,027 main.py:47] epoch 832, training loss: 7593.22, average training loss: 8210.09, base loss: 8490.53
[INFO 2017-06-26 17:33:45,382 main.py:47] epoch 833, training loss: 5828.67, average training loss: 8207.24, base loss: 8488.23
[INFO 2017-06-26 17:33:45,771 main.py:47] epoch 834, training loss: 6651.51, average training loss: 8205.38, base loss: 8488.03
[INFO 2017-06-26 17:33:46,130 main.py:47] epoch 835, training loss: 6676.12, average training loss: 8203.55, base loss: 8486.71
[INFO 2017-06-26 17:33:46,484 main.py:47] epoch 836, training loss: 6835.41, average training loss: 8201.91, base loss: 8486.55
[INFO 2017-06-26 17:33:46,854 main.py:47] epoch 837, training loss: 6145.93, average training loss: 8199.46, base loss: 8485.07
[INFO 2017-06-26 17:33:47,207 main.py:47] epoch 838, training loss: 7507.66, average training loss: 8198.63, base loss: 8485.65
[INFO 2017-06-26 17:33:47,560 main.py:47] epoch 839, training loss: 6872.44, average training loss: 8197.06, base loss: 8485.40
[INFO 2017-06-26 17:33:47,947 main.py:47] epoch 840, training loss: 6344.30, average training loss: 8194.85, base loss: 8483.80
[INFO 2017-06-26 17:33:48,320 main.py:47] epoch 841, training loss: 6923.32, average training loss: 8193.34, base loss: 8483.62
[INFO 2017-06-26 17:33:48,682 main.py:47] epoch 842, training loss: 8198.56, average training loss: 8193.35, base loss: 8485.31
[INFO 2017-06-26 17:33:49,043 main.py:47] epoch 843, training loss: 8107.46, average training loss: 8193.25, base loss: 8486.85
[INFO 2017-06-26 17:33:49,395 main.py:47] epoch 844, training loss: 6316.96, average training loss: 8191.03, base loss: 8485.48
[INFO 2017-06-26 17:33:49,749 main.py:47] epoch 845, training loss: 6368.80, average training loss: 8188.87, base loss: 8484.80
[INFO 2017-06-26 17:33:50,101 main.py:47] epoch 846, training loss: 7274.60, average training loss: 8187.79, base loss: 8485.21
[INFO 2017-06-26 17:33:50,454 main.py:47] epoch 847, training loss: 6258.41, average training loss: 8185.52, base loss: 8484.03
[INFO 2017-06-26 17:33:50,805 main.py:47] epoch 848, training loss: 6926.17, average training loss: 8184.03, base loss: 8483.86
[INFO 2017-06-26 17:33:51,160 main.py:47] epoch 849, training loss: 8119.79, average training loss: 8183.96, base loss: 8485.61
[INFO 2017-06-26 17:33:51,546 main.py:47] epoch 850, training loss: 7504.78, average training loss: 8183.16, base loss: 8485.95
[INFO 2017-06-26 17:33:51,946 main.py:47] epoch 851, training loss: 6951.33, average training loss: 8181.71, base loss: 8486.14
[INFO 2017-06-26 17:33:52,315 main.py:47] epoch 852, training loss: 8204.20, average training loss: 8181.74, base loss: 8487.66
[INFO 2017-06-26 17:33:52,710 main.py:47] epoch 853, training loss: 6358.04, average training loss: 8179.61, base loss: 8486.12
[INFO 2017-06-26 17:33:53,080 main.py:47] epoch 854, training loss: 7239.10, average training loss: 8178.51, base loss: 8486.70
[INFO 2017-06-26 17:33:53,434 main.py:47] epoch 855, training loss: 7406.42, average training loss: 8177.60, base loss: 8487.54
[INFO 2017-06-26 17:33:53,798 main.py:47] epoch 856, training loss: 7136.91, average training loss: 8176.39, base loss: 8488.24
[INFO 2017-06-26 17:33:54,154 main.py:47] epoch 857, training loss: 7461.80, average training loss: 8175.56, base loss: 8488.72
[INFO 2017-06-26 17:33:54,557 main.py:47] epoch 858, training loss: 7299.85, average training loss: 8174.54, base loss: 8488.93
[INFO 2017-06-26 17:33:54,909 main.py:47] epoch 859, training loss: 7461.88, average training loss: 8173.71, base loss: 8489.60
[INFO 2017-06-26 17:33:55,294 main.py:47] epoch 860, training loss: 7502.44, average training loss: 8172.93, base loss: 8490.82
[INFO 2017-06-26 17:33:55,660 main.py:47] epoch 861, training loss: 7034.26, average training loss: 8171.61, base loss: 8490.92
[INFO 2017-06-26 17:33:56,021 main.py:47] epoch 862, training loss: 6661.99, average training loss: 8169.86, base loss: 8489.90
[INFO 2017-06-26 17:33:56,376 main.py:47] epoch 863, training loss: 6592.14, average training loss: 8168.03, base loss: 8489.17
[INFO 2017-06-26 17:33:56,728 main.py:47] epoch 864, training loss: 7363.15, average training loss: 8167.10, base loss: 8489.50
[INFO 2017-06-26 17:33:57,113 main.py:47] epoch 865, training loss: 7040.10, average training loss: 8165.80, base loss: 8489.60
[INFO 2017-06-26 17:33:57,493 main.py:47] epoch 866, training loss: 6846.68, average training loss: 8164.28, base loss: 8489.40
[INFO 2017-06-26 17:33:57,851 main.py:47] epoch 867, training loss: 7466.16, average training loss: 8163.47, base loss: 8489.90
[INFO 2017-06-26 17:33:58,238 main.py:47] epoch 868, training loss: 6999.87, average training loss: 8162.14, base loss: 8489.71
[INFO 2017-06-26 17:33:58,610 main.py:47] epoch 869, training loss: 7613.92, average training loss: 8161.51, base loss: 8490.85
[INFO 2017-06-26 17:33:58,968 main.py:47] epoch 870, training loss: 8652.17, average training loss: 8162.07, base loss: 8493.52
[INFO 2017-06-26 17:33:59,323 main.py:47] epoch 871, training loss: 6837.85, average training loss: 8160.55, base loss: 8492.64
[INFO 2017-06-26 17:33:59,676 main.py:47] epoch 872, training loss: 7913.90, average training loss: 8160.27, base loss: 8493.61
[INFO 2017-06-26 17:34:00,029 main.py:47] epoch 873, training loss: 6911.32, average training loss: 8158.84, base loss: 8493.61
[INFO 2017-06-26 17:34:00,380 main.py:47] epoch 874, training loss: 7302.29, average training loss: 8157.86, base loss: 8494.64
[INFO 2017-06-26 17:34:00,733 main.py:47] epoch 875, training loss: 8126.15, average training loss: 8157.82, base loss: 8496.38
[INFO 2017-06-26 17:34:01,087 main.py:47] epoch 876, training loss: 7876.63, average training loss: 8157.50, base loss: 8497.39
[INFO 2017-06-26 17:34:01,437 main.py:47] epoch 877, training loss: 7071.78, average training loss: 8156.27, base loss: 8497.23
[INFO 2017-06-26 17:34:01,836 main.py:47] epoch 878, training loss: 6781.06, average training loss: 8154.70, base loss: 8496.39
[INFO 2017-06-26 17:34:02,191 main.py:47] epoch 879, training loss: 6284.19, average training loss: 8152.58, base loss: 8495.19
[INFO 2017-06-26 17:34:02,548 main.py:47] epoch 880, training loss: 5809.36, average training loss: 8149.92, base loss: 8492.93
[INFO 2017-06-26 17:34:02,909 main.py:47] epoch 881, training loss: 6511.28, average training loss: 8148.06, base loss: 8492.02
[INFO 2017-06-26 17:34:03,273 main.py:47] epoch 882, training loss: 7603.90, average training loss: 8147.44, base loss: 8492.82
[INFO 2017-06-26 17:34:03,627 main.py:47] epoch 883, training loss: 7044.38, average training loss: 8146.19, base loss: 8493.04
[INFO 2017-06-26 17:34:03,979 main.py:47] epoch 884, training loss: 6456.13, average training loss: 8144.28, base loss: 8491.91
[INFO 2017-06-26 17:34:04,336 main.py:47] epoch 885, training loss: 7085.75, average training loss: 8143.09, base loss: 8491.74
[INFO 2017-06-26 17:34:04,694 main.py:47] epoch 886, training loss: 6355.26, average training loss: 8141.07, base loss: 8490.52
[INFO 2017-06-26 17:34:05,081 main.py:47] epoch 887, training loss: 7226.74, average training loss: 8140.04, base loss: 8490.70
[INFO 2017-06-26 17:34:05,460 main.py:47] epoch 888, training loss: 7186.52, average training loss: 8138.97, base loss: 8490.51
[INFO 2017-06-26 17:34:05,831 main.py:47] epoch 889, training loss: 6951.35, average training loss: 8137.64, base loss: 8490.99
[INFO 2017-06-26 17:34:06,186 main.py:47] epoch 890, training loss: 7221.69, average training loss: 8136.61, base loss: 8491.23
[INFO 2017-06-26 17:34:06,542 main.py:47] epoch 891, training loss: 6858.92, average training loss: 8135.18, base loss: 8491.08
[INFO 2017-06-26 17:34:07,099 main.py:47] epoch 892, training loss: 6904.97, average training loss: 8133.80, base loss: 8491.12
[INFO 2017-06-26 17:34:07,466 main.py:47] epoch 893, training loss: 8050.02, average training loss: 8133.71, base loss: 8492.85
[INFO 2017-06-26 17:34:07,840 main.py:47] epoch 894, training loss: 7134.70, average training loss: 8132.59, base loss: 8492.83
[INFO 2017-06-26 17:34:08,240 main.py:47] epoch 895, training loss: 6880.89, average training loss: 8131.19, base loss: 8493.27
[INFO 2017-06-26 17:34:08,617 main.py:47] epoch 896, training loss: 6817.02, average training loss: 8129.73, base loss: 8492.94
[INFO 2017-06-26 17:34:08,996 main.py:47] epoch 897, training loss: 8265.60, average training loss: 8129.88, base loss: 8494.94
[INFO 2017-06-26 17:34:09,363 main.py:47] epoch 898, training loss: 6543.87, average training loss: 8128.12, base loss: 8494.55
[INFO 2017-06-26 17:34:09,723 main.py:47] epoch 899, training loss: 7259.96, average training loss: 8127.15, base loss: 8495.48
[INFO 2017-06-26 17:34:09,724 main.py:49] epoch 899, testing
[INFO 2017-06-26 17:34:14,025 main.py:100] average testing loss: 6945.46, base loss: 8218.63
[INFO 2017-06-26 17:34:14,048 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:34:14,055 main.py:73] current best accuracy: 6945.46
[INFO 2017-06-26 17:34:14,421 main.py:47] epoch 900, training loss: 7576.96, average training loss: 8126.54, base loss: 8496.54
[INFO 2017-06-26 17:34:14,773 main.py:47] epoch 901, training loss: 7744.63, average training loss: 8126.12, base loss: 8497.50
[INFO 2017-06-26 17:34:15,125 main.py:47] epoch 902, training loss: 7378.33, average training loss: 8125.29, base loss: 8498.65
[INFO 2017-06-26 17:34:15,477 main.py:47] epoch 903, training loss: 7377.87, average training loss: 8124.46, base loss: 8499.31
[INFO 2017-06-26 17:34:15,829 main.py:47] epoch 904, training loss: 7158.73, average training loss: 8123.39, base loss: 8499.91
[INFO 2017-06-26 17:34:16,181 main.py:47] epoch 905, training loss: 6367.47, average training loss: 8121.46, base loss: 8498.59
[INFO 2017-06-26 17:34:16,567 main.py:47] epoch 906, training loss: 6984.36, average training loss: 8120.20, base loss: 8498.44
[INFO 2017-06-26 17:34:16,930 main.py:47] epoch 907, training loss: 7971.97, average training loss: 8120.04, base loss: 8499.76
[INFO 2017-06-26 17:34:17,284 main.py:47] epoch 908, training loss: 6161.31, average training loss: 8117.88, base loss: 8498.42
[INFO 2017-06-26 17:34:17,638 main.py:47] epoch 909, training loss: 7968.61, average training loss: 8117.72, base loss: 8500.03
[INFO 2017-06-26 17:34:17,993 main.py:47] epoch 910, training loss: 6931.63, average training loss: 8116.42, base loss: 8499.53
[INFO 2017-06-26 17:34:18,348 main.py:47] epoch 911, training loss: 6817.49, average training loss: 8114.99, base loss: 8498.64
[INFO 2017-06-26 17:34:18,702 main.py:47] epoch 912, training loss: 6139.75, average training loss: 8112.83, base loss: 8497.14
[INFO 2017-06-26 17:34:19,053 main.py:47] epoch 913, training loss: 7021.90, average training loss: 8111.64, base loss: 8497.45
[INFO 2017-06-26 17:34:19,406 main.py:47] epoch 914, training loss: 7483.81, average training loss: 8110.95, base loss: 8498.44
[INFO 2017-06-26 17:34:19,758 main.py:47] epoch 915, training loss: 6754.21, average training loss: 8109.47, base loss: 8498.07
[INFO 2017-06-26 17:34:20,109 main.py:47] epoch 916, training loss: 6281.56, average training loss: 8107.48, base loss: 8497.14
[INFO 2017-06-26 17:34:20,463 main.py:47] epoch 917, training loss: 7612.66, average training loss: 8106.94, base loss: 8498.06
[INFO 2017-06-26 17:34:20,817 main.py:47] epoch 918, training loss: 6503.72, average training loss: 8105.19, base loss: 8496.82
[INFO 2017-06-26 17:34:21,170 main.py:47] epoch 919, training loss: 7163.55, average training loss: 8104.17, base loss: 8496.93
[INFO 2017-06-26 17:34:21,521 main.py:47] epoch 920, training loss: 6965.13, average training loss: 8102.93, base loss: 8497.06
[INFO 2017-06-26 17:34:21,875 main.py:47] epoch 921, training loss: 5786.54, average training loss: 8100.42, base loss: 8495.11
[INFO 2017-06-26 17:34:22,228 main.py:47] epoch 922, training loss: 7770.80, average training loss: 8100.06, base loss: 8495.95
[INFO 2017-06-26 17:34:22,580 main.py:47] epoch 923, training loss: 7476.57, average training loss: 8099.39, base loss: 8496.54
[INFO 2017-06-26 17:34:22,935 main.py:47] epoch 924, training loss: 6599.00, average training loss: 8097.77, base loss: 8496.03
[INFO 2017-06-26 17:34:23,315 main.py:47] epoch 925, training loss: 7292.51, average training loss: 8096.90, base loss: 8496.34
[INFO 2017-06-26 17:34:23,666 main.py:47] epoch 926, training loss: 6695.18, average training loss: 8095.38, base loss: 8495.55
[INFO 2017-06-26 17:34:24,040 main.py:47] epoch 927, training loss: 6926.32, average training loss: 8094.12, base loss: 8495.67
[INFO 2017-06-26 17:34:24,395 main.py:47] epoch 928, training loss: 6518.97, average training loss: 8092.43, base loss: 8494.87
[INFO 2017-06-26 17:34:24,748 main.py:47] epoch 929, training loss: 6973.42, average training loss: 8091.23, base loss: 8495.66
[INFO 2017-06-26 17:34:25,106 main.py:47] epoch 930, training loss: 7476.44, average training loss: 8090.57, base loss: 8496.53
[INFO 2017-06-26 17:34:25,462 main.py:47] epoch 931, training loss: 7336.44, average training loss: 8089.76, base loss: 8497.48
[INFO 2017-06-26 17:34:25,822 main.py:47] epoch 932, training loss: 7360.30, average training loss: 8088.97, base loss: 8498.59
[INFO 2017-06-26 17:34:26,177 main.py:47] epoch 933, training loss: 6706.87, average training loss: 8087.50, base loss: 8498.69
[INFO 2017-06-26 17:34:26,532 main.py:47] epoch 934, training loss: 6808.37, average training loss: 8086.13, base loss: 8498.32
[INFO 2017-06-26 17:34:26,890 main.py:47] epoch 935, training loss: 7198.27, average training loss: 8085.18, base loss: 8498.22
[INFO 2017-06-26 17:34:27,245 main.py:47] epoch 936, training loss: 7485.64, average training loss: 8084.54, base loss: 8499.04
[INFO 2017-06-26 17:34:27,598 main.py:47] epoch 937, training loss: 7269.69, average training loss: 8083.67, base loss: 8499.91
[INFO 2017-06-26 17:34:27,954 main.py:47] epoch 938, training loss: 6890.85, average training loss: 8082.40, base loss: 8499.90
[INFO 2017-06-26 17:34:28,309 main.py:47] epoch 939, training loss: 7908.84, average training loss: 8082.21, base loss: 8501.28
[INFO 2017-06-26 17:34:28,674 main.py:47] epoch 940, training loss: 7235.43, average training loss: 8081.32, base loss: 8501.54
[INFO 2017-06-26 17:34:29,037 main.py:47] epoch 941, training loss: 7250.43, average training loss: 8080.43, base loss: 8502.02
[INFO 2017-06-26 17:34:29,391 main.py:47] epoch 942, training loss: 7617.77, average training loss: 8079.94, base loss: 8502.46
[INFO 2017-06-26 17:34:29,754 main.py:47] epoch 943, training loss: 6999.54, average training loss: 8078.80, base loss: 8502.57
[INFO 2017-06-26 17:34:30,129 main.py:47] epoch 944, training loss: 6060.54, average training loss: 8076.66, base loss: 8501.05
[INFO 2017-06-26 17:34:30,512 main.py:47] epoch 945, training loss: 7746.21, average training loss: 8076.31, base loss: 8501.97
[INFO 2017-06-26 17:34:30,894 main.py:47] epoch 946, training loss: 8113.94, average training loss: 8076.35, base loss: 8503.36
[INFO 2017-06-26 17:34:31,272 main.py:47] epoch 947, training loss: 7041.64, average training loss: 8075.26, base loss: 8503.56
[INFO 2017-06-26 17:34:31,632 main.py:47] epoch 948, training loss: 7119.65, average training loss: 8074.25, base loss: 8503.63
[INFO 2017-06-26 17:34:32,015 main.py:47] epoch 949, training loss: 7064.46, average training loss: 8073.19, base loss: 8503.63
[INFO 2017-06-26 17:34:32,382 main.py:47] epoch 950, training loss: 7041.38, average training loss: 8072.11, base loss: 8503.31
[INFO 2017-06-26 17:34:32,739 main.py:47] epoch 951, training loss: 7414.56, average training loss: 8071.42, base loss: 8503.75
[INFO 2017-06-26 17:34:33,103 main.py:47] epoch 952, training loss: 7549.80, average training loss: 8070.87, base loss: 8504.41
[INFO 2017-06-26 17:34:33,459 main.py:47] epoch 953, training loss: 6323.68, average training loss: 8069.04, base loss: 8503.12
[INFO 2017-06-26 17:34:33,811 main.py:47] epoch 954, training loss: 8144.22, average training loss: 8069.12, base loss: 8504.83
[INFO 2017-06-26 17:34:34,166 main.py:47] epoch 955, training loss: 6873.25, average training loss: 8067.86, base loss: 8504.81
[INFO 2017-06-26 17:34:34,521 main.py:47] epoch 956, training loss: 7191.35, average training loss: 8066.95, base loss: 8505.03
[INFO 2017-06-26 17:34:34,874 main.py:47] epoch 957, training loss: 6733.14, average training loss: 8065.56, base loss: 8504.60
[INFO 2017-06-26 17:34:35,227 main.py:47] epoch 958, training loss: 6773.98, average training loss: 8064.21, base loss: 8504.23
[INFO 2017-06-26 17:34:35,580 main.py:47] epoch 959, training loss: 7126.37, average training loss: 8063.23, base loss: 8504.36
[INFO 2017-06-26 17:34:35,933 main.py:47] epoch 960, training loss: 7434.53, average training loss: 8062.58, base loss: 8505.09
[INFO 2017-06-26 17:34:36,285 main.py:47] epoch 961, training loss: 6994.13, average training loss: 8061.47, base loss: 8505.25
[INFO 2017-06-26 17:34:36,636 main.py:47] epoch 962, training loss: 7234.08, average training loss: 8060.61, base loss: 8505.19
[INFO 2017-06-26 17:34:36,986 main.py:47] epoch 963, training loss: 6742.30, average training loss: 8059.24, base loss: 8504.52
[INFO 2017-06-26 17:34:37,342 main.py:47] epoch 964, training loss: 7007.80, average training loss: 8058.15, base loss: 8504.55
[INFO 2017-06-26 17:34:37,693 main.py:47] epoch 965, training loss: 8171.60, average training loss: 8058.27, base loss: 8505.91
[INFO 2017-06-26 17:34:38,080 main.py:47] epoch 966, training loss: 7295.40, average training loss: 8057.48, base loss: 8506.42
[INFO 2017-06-26 17:34:38,454 main.py:47] epoch 967, training loss: 6259.37, average training loss: 8055.62, base loss: 8505.57
[INFO 2017-06-26 17:34:38,811 main.py:47] epoch 968, training loss: 6608.88, average training loss: 8054.13, base loss: 8505.07
[INFO 2017-06-26 17:34:39,167 main.py:47] epoch 969, training loss: 6847.91, average training loss: 8052.89, base loss: 8505.54
[INFO 2017-06-26 17:34:39,519 main.py:47] epoch 970, training loss: 6443.06, average training loss: 8051.23, base loss: 8504.53
[INFO 2017-06-26 17:34:39,872 main.py:47] epoch 971, training loss: 6322.10, average training loss: 8049.45, base loss: 8503.66
[INFO 2017-06-26 17:34:40,223 main.py:47] epoch 972, training loss: 6771.21, average training loss: 8048.14, base loss: 8503.28
[INFO 2017-06-26 17:34:40,590 main.py:47] epoch 973, training loss: 7141.94, average training loss: 8047.21, base loss: 8503.28
[INFO 2017-06-26 17:34:40,941 main.py:47] epoch 974, training loss: 6492.33, average training loss: 8045.61, base loss: 8502.72
[INFO 2017-06-26 17:34:41,310 main.py:47] epoch 975, training loss: 6305.59, average training loss: 8043.83, base loss: 8502.07
[INFO 2017-06-26 17:34:41,670 main.py:47] epoch 976, training loss: 7305.74, average training loss: 8043.07, base loss: 8502.29
[INFO 2017-06-26 17:34:42,023 main.py:47] epoch 977, training loss: 8094.11, average training loss: 8043.12, base loss: 8503.44
[INFO 2017-06-26 17:34:42,373 main.py:47] epoch 978, training loss: 6835.83, average training loss: 8041.89, base loss: 8502.71
[INFO 2017-06-26 17:34:42,727 main.py:47] epoch 979, training loss: 6624.19, average training loss: 8040.44, base loss: 8502.33
[INFO 2017-06-26 17:34:43,082 main.py:47] epoch 980, training loss: 7584.71, average training loss: 8039.98, base loss: 8503.19
[INFO 2017-06-26 17:34:43,435 main.py:47] epoch 981, training loss: 6742.45, average training loss: 8038.66, base loss: 8503.07
[INFO 2017-06-26 17:34:43,794 main.py:47] epoch 982, training loss: 7261.09, average training loss: 8037.87, base loss: 8503.50
[INFO 2017-06-26 17:34:44,151 main.py:47] epoch 983, training loss: 6304.44, average training loss: 8036.11, base loss: 8502.76
[INFO 2017-06-26 17:34:44,513 main.py:47] epoch 984, training loss: 7880.05, average training loss: 8035.95, base loss: 8504.01
[INFO 2017-06-26 17:34:44,869 main.py:47] epoch 985, training loss: 6288.64, average training loss: 8034.18, base loss: 8503.46
[INFO 2017-06-26 17:34:45,255 main.py:47] epoch 986, training loss: 8401.03, average training loss: 8034.55, base loss: 8505.47
[INFO 2017-06-26 17:34:45,622 main.py:47] epoch 987, training loss: 6929.41, average training loss: 8033.43, base loss: 8504.79
[INFO 2017-06-26 17:34:45,982 main.py:47] epoch 988, training loss: 6992.02, average training loss: 8032.38, base loss: 8505.15
[INFO 2017-06-26 17:34:46,335 main.py:47] epoch 989, training loss: 7164.48, average training loss: 8031.50, base loss: 8505.40
[INFO 2017-06-26 17:34:46,687 main.py:47] epoch 990, training loss: 7444.42, average training loss: 8030.91, base loss: 8506.34
[INFO 2017-06-26 17:34:47,037 main.py:47] epoch 991, training loss: 6889.57, average training loss: 8029.76, base loss: 8506.32
[INFO 2017-06-26 17:34:47,391 main.py:47] epoch 992, training loss: 7059.59, average training loss: 8028.78, base loss: 8506.59
[INFO 2017-06-26 17:34:47,742 main.py:47] epoch 993, training loss: 6465.89, average training loss: 8027.21, base loss: 8505.73
[INFO 2017-06-26 17:34:48,117 main.py:47] epoch 994, training loss: 6986.56, average training loss: 8026.16, base loss: 8505.70
[INFO 2017-06-26 17:34:48,484 main.py:47] epoch 995, training loss: 7577.89, average training loss: 8025.71, base loss: 8507.18
[INFO 2017-06-26 17:34:48,853 main.py:47] epoch 996, training loss: 7292.61, average training loss: 8024.98, base loss: 8507.75
[INFO 2017-06-26 17:34:49,215 main.py:47] epoch 997, training loss: 6490.56, average training loss: 8023.44, base loss: 8507.08
[INFO 2017-06-26 17:34:49,576 main.py:47] epoch 998, training loss: 6288.71, average training loss: 8021.70, base loss: 8505.58
[INFO 2017-06-26 17:34:49,936 main.py:47] epoch 999, training loss: 6623.28, average training loss: 8020.30, base loss: 8505.44
[INFO 2017-06-26 17:34:49,937 main.py:49] epoch 999, testing
[INFO 2017-06-26 17:34:54,335 main.py:100] average testing loss: 6952.24, base loss: 8376.17
[INFO 2017-06-26 17:34:54,359 main.py:73] current best accuracy: 6945.46
[INFO 2017-06-26 17:34:54,712 main.py:47] epoch 1000, training loss: 6431.40, average training loss: 7918.98, base loss: 8504.50
[INFO 2017-06-26 17:34:55,066 main.py:47] epoch 1001, training loss: 6035.22, average training loss: 7838.74, base loss: 8503.36
[INFO 2017-06-26 17:34:55,420 main.py:47] epoch 1002, training loss: 6833.26, average training loss: 7773.31, base loss: 8503.94
[INFO 2017-06-26 17:34:55,773 main.py:47] epoch 1003, training loss: 6623.52, average training loss: 7715.71, base loss: 8503.23
[INFO 2017-06-26 17:34:56,142 main.py:47] epoch 1004, training loss: 7311.42, average training loss: 7666.94, base loss: 8502.96
[INFO 2017-06-26 17:34:56,500 main.py:47] epoch 1005, training loss: 6658.55, average training loss: 7624.08, base loss: 8502.37
[INFO 2017-06-26 17:34:56,855 main.py:47] epoch 1006, training loss: 6713.47, average training loss: 7589.41, base loss: 8502.37
[INFO 2017-06-26 17:34:57,253 main.py:47] epoch 1007, training loss: 7519.22, average training loss: 7560.11, base loss: 8503.09
[INFO 2017-06-26 17:34:57,625 main.py:47] epoch 1008, training loss: 7174.19, average training loss: 7533.68, base loss: 8503.48
[INFO 2017-06-26 17:34:57,993 main.py:47] epoch 1009, training loss: 6697.61, average training loss: 7512.03, base loss: 8502.55
[INFO 2017-06-26 17:34:58,354 main.py:47] epoch 1010, training loss: 7383.65, average training loss: 7494.58, base loss: 8503.24
[INFO 2017-06-26 17:34:58,713 main.py:47] epoch 1011, training loss: 6940.69, average training loss: 7479.79, base loss: 8503.97
[INFO 2017-06-26 17:34:59,084 main.py:47] epoch 1012, training loss: 6768.22, average training loss: 7466.89, base loss: 8504.20
[INFO 2017-06-26 17:34:59,443 main.py:47] epoch 1013, training loss: 7420.73, average training loss: 7456.12, base loss: 8503.91
[INFO 2017-06-26 17:34:59,803 main.py:47] epoch 1014, training loss: 6090.44, average training loss: 7445.98, base loss: 8502.73
[INFO 2017-06-26 17:35:00,157 main.py:47] epoch 1015, training loss: 6233.31, average training loss: 7438.18, base loss: 8502.46
[INFO 2017-06-26 17:35:00,516 main.py:47] epoch 1016, training loss: 8943.74, average training loss: 7433.77, base loss: 8504.67
[INFO 2017-06-26 17:35:00,874 main.py:47] epoch 1017, training loss: 6919.50, average training loss: 7427.88, base loss: 8504.23
[INFO 2017-06-26 17:35:01,233 main.py:47] epoch 1018, training loss: 6439.85, average training loss: 7423.65, base loss: 8504.28
[INFO 2017-06-26 17:35:01,586 main.py:47] epoch 1019, training loss: 6414.14, average training loss: 7418.36, base loss: 8503.34
[INFO 2017-06-26 17:35:01,941 main.py:47] epoch 1020, training loss: 6352.68, average training loss: 7414.78, base loss: 8503.47
[INFO 2017-06-26 17:35:02,298 main.py:47] epoch 1021, training loss: 8042.46, average training loss: 7411.24, base loss: 8503.54
[INFO 2017-06-26 17:35:02,651 main.py:47] epoch 1022, training loss: 6178.63, average training loss: 7408.32, base loss: 8503.16
[INFO 2017-06-26 17:35:03,004 main.py:47] epoch 1023, training loss: 8157.54, average training loss: 7406.57, base loss: 8504.35
[INFO 2017-06-26 17:35:03,388 main.py:47] epoch 1024, training loss: 6546.68, average training loss: 7404.23, base loss: 8504.51
[INFO 2017-06-26 17:35:03,743 main.py:47] epoch 1025, training loss: 7055.15, average training loss: 7402.50, base loss: 8504.84
[INFO 2017-06-26 17:35:04,114 main.py:47] epoch 1026, training loss: 6873.40, average training loss: 7400.81, base loss: 8505.34
[INFO 2017-06-26 17:35:04,481 main.py:47] epoch 1027, training loss: 6702.19, average training loss: 7397.90, base loss: 8504.22
[INFO 2017-06-26 17:35:04,837 main.py:47] epoch 1028, training loss: 6719.27, average training loss: 7395.26, base loss: 8503.23
[INFO 2017-06-26 17:35:05,191 main.py:47] epoch 1029, training loss: 8011.33, average training loss: 7395.68, base loss: 8505.66
[INFO 2017-06-26 17:35:05,545 main.py:47] epoch 1030, training loss: 6792.32, average training loss: 7392.36, base loss: 8504.06
[INFO 2017-06-26 17:35:05,906 main.py:47] epoch 1031, training loss: 6291.86, average training loss: 7389.22, base loss: 8502.37
[INFO 2017-06-26 17:35:06,276 main.py:47] epoch 1032, training loss: 8247.72, average training loss: 7389.00, base loss: 8504.48
[INFO 2017-06-26 17:35:06,676 main.py:47] epoch 1033, training loss: 7466.64, average training loss: 7387.80, base loss: 8505.26
[INFO 2017-06-26 17:35:07,043 main.py:47] epoch 1034, training loss: 6524.94, average training loss: 7385.82, base loss: 8504.72
[INFO 2017-06-26 17:35:07,404 main.py:47] epoch 1035, training loss: 6237.50, average training loss: 7384.69, base loss: 8504.88
[INFO 2017-06-26 17:35:07,755 main.py:47] epoch 1036, training loss: 6504.42, average training loss: 7382.85, base loss: 8504.74
[INFO 2017-06-26 17:35:08,107 main.py:47] epoch 1037, training loss: 6182.58, average training loss: 7380.31, base loss: 8503.45
[INFO 2017-06-26 17:35:08,460 main.py:47] epoch 1038, training loss: 7503.52, average training loss: 7379.44, base loss: 8504.18
[INFO 2017-06-26 17:35:08,811 main.py:47] epoch 1039, training loss: 8051.37, average training loss: 7379.19, base loss: 8505.77
[INFO 2017-06-26 17:35:09,168 main.py:47] epoch 1040, training loss: 6949.33, average training loss: 7377.54, base loss: 8505.43
[INFO 2017-06-26 17:35:09,520 main.py:47] epoch 1041, training loss: 6434.17, average training loss: 7376.07, base loss: 8505.75
[INFO 2017-06-26 17:35:09,873 main.py:47] epoch 1042, training loss: 7157.77, average training loss: 7373.57, base loss: 8504.81
[INFO 2017-06-26 17:35:10,225 main.py:47] epoch 1043, training loss: 5608.33, average training loss: 7370.17, base loss: 8502.00
[INFO 2017-06-26 17:35:10,577 main.py:47] epoch 1044, training loss: 6478.18, average training loss: 7366.58, base loss: 8499.50
[INFO 2017-06-26 17:35:10,929 main.py:47] epoch 1045, training loss: 6572.02, average training loss: 7365.31, base loss: 8498.97
[INFO 2017-06-26 17:35:11,282 main.py:47] epoch 1046, training loss: 6082.89, average training loss: 7362.73, base loss: 8497.63
[INFO 2017-06-26 17:35:11,634 main.py:47] epoch 1047, training loss: 7500.92, average training loss: 7361.78, base loss: 8498.19
[INFO 2017-06-26 17:35:11,986 main.py:47] epoch 1048, training loss: 6081.86, average training loss: 7358.13, base loss: 8495.72
[INFO 2017-06-26 17:35:12,344 main.py:47] epoch 1049, training loss: 6439.94, average training loss: 7356.03, base loss: 8494.57
[INFO 2017-06-26 17:35:12,699 main.py:47] epoch 1050, training loss: 5999.88, average training loss: 7353.53, base loss: 8493.22
[INFO 2017-06-26 17:35:13,073 main.py:47] epoch 1051, training loss: 5973.58, average training loss: 7350.79, base loss: 8491.78
[INFO 2017-06-26 17:35:13,425 main.py:47] epoch 1052, training loss: 6126.88, average training loss: 7347.53, base loss: 8489.43
[INFO 2017-06-26 17:35:13,779 main.py:47] epoch 1053, training loss: 7694.70, average training loss: 7346.50, base loss: 8489.87
[INFO 2017-06-26 17:35:14,151 main.py:47] epoch 1054, training loss: 7303.30, average training loss: 7346.50, base loss: 8491.30
[INFO 2017-06-26 17:35:14,504 main.py:47] epoch 1055, training loss: 7662.38, average training loss: 7344.99, base loss: 8490.80
[INFO 2017-06-26 17:35:14,856 main.py:47] epoch 1056, training loss: 6497.04, average training loss: 7343.15, base loss: 8490.50
[INFO 2017-06-26 17:35:15,210 main.py:47] epoch 1057, training loss: 7925.69, average training loss: 7343.06, base loss: 8492.67
[INFO 2017-06-26 17:35:15,562 main.py:47] epoch 1058, training loss: 7307.06, average training loss: 7341.40, base loss: 8492.03
[INFO 2017-06-26 17:35:15,915 main.py:47] epoch 1059, training loss: 6701.87, average training loss: 7340.11, base loss: 8491.64
[INFO 2017-06-26 17:35:16,267 main.py:47] epoch 1060, training loss: 6764.65, average training loss: 7337.96, base loss: 8490.76
[INFO 2017-06-26 17:35:16,619 main.py:47] epoch 1061, training loss: 6710.28, average training loss: 7336.54, base loss: 8491.07
[INFO 2017-06-26 17:35:16,971 main.py:47] epoch 1062, training loss: 7784.90, average training loss: 7335.98, base loss: 8491.97
[INFO 2017-06-26 17:35:17,323 main.py:47] epoch 1063, training loss: 6485.04, average training loss: 7334.98, base loss: 8492.48
[INFO 2017-06-26 17:35:17,676 main.py:47] epoch 1064, training loss: 6853.58, average training loss: 7333.84, base loss: 8492.49
[INFO 2017-06-26 17:35:18,029 main.py:47] epoch 1065, training loss: 6605.28, average training loss: 7333.74, base loss: 8493.70
[INFO 2017-06-26 17:35:18,383 main.py:47] epoch 1066, training loss: 7785.27, average training loss: 7333.11, base loss: 8494.59
[INFO 2017-06-26 17:35:18,736 main.py:47] epoch 1067, training loss: 5969.52, average training loss: 7330.56, base loss: 8493.15
[INFO 2017-06-26 17:35:19,088 main.py:47] epoch 1068, training loss: 6457.07, average training loss: 7328.96, base loss: 8492.35
[INFO 2017-06-26 17:35:19,446 main.py:47] epoch 1069, training loss: 7423.95, average training loss: 7327.53, base loss: 8492.07
[INFO 2017-06-26 17:35:19,796 main.py:47] epoch 1070, training loss: 6801.03, average training loss: 7324.62, base loss: 8490.36
[INFO 2017-06-26 17:35:20,148 main.py:47] epoch 1071, training loss: 7646.54, average training loss: 7323.81, base loss: 8491.35
[INFO 2017-06-26 17:35:20,500 main.py:47] epoch 1072, training loss: 6515.32, average training loss: 7322.12, base loss: 8490.76
[INFO 2017-06-26 17:35:20,851 main.py:47] epoch 1073, training loss: 6885.67, average training loss: 7320.06, base loss: 8489.90
[INFO 2017-06-26 17:35:21,202 main.py:47] epoch 1074, training loss: 6915.67, average training loss: 7319.13, base loss: 8489.97
[INFO 2017-06-26 17:35:21,557 main.py:47] epoch 1075, training loss: 7834.99, average training loss: 7318.87, base loss: 8491.09
[INFO 2017-06-26 17:35:21,932 main.py:47] epoch 1076, training loss: 6302.83, average training loss: 7316.63, base loss: 8489.65
[INFO 2017-06-26 17:35:22,283 main.py:47] epoch 1077, training loss: 7616.94, average training loss: 7315.87, base loss: 8490.75
[INFO 2017-06-26 17:35:22,635 main.py:47] epoch 1078, training loss: 6950.11, average training loss: 7313.98, base loss: 8490.06
[INFO 2017-06-26 17:35:22,986 main.py:47] epoch 1079, training loss: 7558.27, average training loss: 7312.81, base loss: 8490.11
[INFO 2017-06-26 17:35:23,340 main.py:47] epoch 1080, training loss: 6661.37, average training loss: 7311.24, base loss: 8489.25
[INFO 2017-06-26 17:35:23,690 main.py:47] epoch 1081, training loss: 6093.35, average training loss: 7308.17, base loss: 8486.96
[INFO 2017-06-26 17:35:24,044 main.py:47] epoch 1082, training loss: 6882.96, average training loss: 7306.71, base loss: 8486.39
[INFO 2017-06-26 17:35:24,397 main.py:47] epoch 1083, training loss: 7271.91, average training loss: 7306.06, base loss: 8487.00
[INFO 2017-06-26 17:35:24,750 main.py:47] epoch 1084, training loss: 7123.29, average training loss: 7303.84, base loss: 8486.17
[INFO 2017-06-26 17:35:25,102 main.py:47] epoch 1085, training loss: 6997.27, average training loss: 7303.53, base loss: 8487.01
[INFO 2017-06-26 17:35:25,455 main.py:47] epoch 1086, training loss: 6724.76, average training loss: 7302.90, base loss: 8487.30
[INFO 2017-06-26 17:35:25,831 main.py:47] epoch 1087, training loss: 7817.15, average training loss: 7302.62, base loss: 8488.39
[INFO 2017-06-26 17:35:26,186 main.py:47] epoch 1088, training loss: 7205.78, average training loss: 7301.87, base loss: 8489.12
[INFO 2017-06-26 17:35:26,576 main.py:47] epoch 1089, training loss: 7233.25, average training loss: 7301.01, base loss: 8489.27
[INFO 2017-06-26 17:35:26,945 main.py:47] epoch 1090, training loss: 6745.11, average training loss: 7299.02, base loss: 8488.50
[INFO 2017-06-26 17:35:27,305 main.py:47] epoch 1091, training loss: 7140.26, average training loss: 7298.13, base loss: 8488.57
[INFO 2017-06-26 17:35:27,674 main.py:47] epoch 1092, training loss: 7405.16, average training loss: 7296.67, base loss: 8488.65
[INFO 2017-06-26 17:35:28,029 main.py:47] epoch 1093, training loss: 7345.04, average training loss: 7296.58, base loss: 8489.90
[INFO 2017-06-26 17:35:28,383 main.py:47] epoch 1094, training loss: 6901.51, average training loss: 7295.41, base loss: 8489.54
[INFO 2017-06-26 17:35:28,735 main.py:47] epoch 1095, training loss: 6427.73, average training loss: 7292.94, base loss: 8488.04
[INFO 2017-06-26 17:35:29,087 main.py:47] epoch 1096, training loss: 6087.60, average training loss: 7289.63, base loss: 8485.26
[INFO 2017-06-26 17:35:29,439 main.py:47] epoch 1097, training loss: 7540.88, average training loss: 7289.61, base loss: 8486.79
[INFO 2017-06-26 17:35:29,806 main.py:47] epoch 1098, training loss: 7171.50, average training loss: 7288.91, base loss: 8486.96
[INFO 2017-06-26 17:35:30,158 main.py:47] epoch 1099, training loss: 7064.55, average training loss: 7288.21, base loss: 8487.37
[INFO 2017-06-26 17:35:30,158 main.py:49] epoch 1099, testing
[INFO 2017-06-26 17:35:34,603 main.py:100] average testing loss: 7113.95, base loss: 8683.34
[INFO 2017-06-26 17:35:34,626 main.py:73] current best accuracy: 6945.46
[INFO 2017-06-26 17:35:34,981 main.py:47] epoch 1100, training loss: 6808.12, average training loss: 7287.67, base loss: 8487.79
[INFO 2017-06-26 17:35:35,334 main.py:47] epoch 1101, training loss: 7064.30, average training loss: 7284.93, base loss: 8485.67
[INFO 2017-06-26 17:35:35,686 main.py:47] epoch 1102, training loss: 7254.66, average training loss: 7284.23, base loss: 8486.18
[INFO 2017-06-26 17:35:36,067 main.py:47] epoch 1103, training loss: 6740.84, average training loss: 7282.82, base loss: 8485.45
[INFO 2017-06-26 17:35:36,453 main.py:47] epoch 1104, training loss: 7532.96, average training loss: 7281.83, base loss: 8485.65
[INFO 2017-06-26 17:35:36,825 main.py:47] epoch 1105, training loss: 7474.64, average training loss: 7281.23, base loss: 8486.42
[INFO 2017-06-26 17:35:37,218 main.py:47] epoch 1106, training loss: 7055.33, average training loss: 7280.63, base loss: 8486.89
[INFO 2017-06-26 17:35:37,598 main.py:47] epoch 1107, training loss: 7625.39, average training loss: 7280.35, base loss: 8487.48
[INFO 2017-06-26 17:35:37,962 main.py:47] epoch 1108, training loss: 7339.81, average training loss: 7279.86, base loss: 8488.70
[INFO 2017-06-26 17:35:38,324 main.py:47] epoch 1109, training loss: 5911.32, average training loss: 7277.46, base loss: 8486.99
[INFO 2017-06-26 17:35:38,680 main.py:47] epoch 1110, training loss: 7132.16, average training loss: 7277.04, base loss: 8487.90
[INFO 2017-06-26 17:35:39,039 main.py:47] epoch 1111, training loss: 6616.83, average training loss: 7274.86, base loss: 8486.88
[INFO 2017-06-26 17:35:39,401 main.py:47] epoch 1112, training loss: 6700.72, average training loss: 7273.89, base loss: 8487.16
[INFO 2017-06-26 17:35:39,772 main.py:47] epoch 1113, training loss: 7579.75, average training loss: 7273.10, base loss: 8487.61
[INFO 2017-06-26 17:35:40,142 main.py:47] epoch 1114, training loss: 6762.00, average training loss: 7272.20, base loss: 8488.15
[INFO 2017-06-26 17:35:40,496 main.py:47] epoch 1115, training loss: 6702.80, average training loss: 7271.75, base loss: 8488.94
[INFO 2017-06-26 17:35:40,864 main.py:47] epoch 1116, training loss: 6554.94, average training loss: 7269.96, base loss: 8487.57
[INFO 2017-06-26 17:35:41,220 main.py:47] epoch 1117, training loss: 7185.86, average training loss: 7269.71, base loss: 8488.27
[INFO 2017-06-26 17:35:41,571 main.py:47] epoch 1118, training loss: 7571.06, average training loss: 7269.35, base loss: 8489.75
[INFO 2017-06-26 17:35:41,931 main.py:47] epoch 1119, training loss: 6310.67, average training loss: 7265.20, base loss: 8486.10
[INFO 2017-06-26 17:35:42,283 main.py:47] epoch 1120, training loss: 7484.86, average training loss: 7264.92, base loss: 8487.42
[INFO 2017-06-26 17:35:42,633 main.py:47] epoch 1121, training loss: 7205.42, average training loss: 7262.53, base loss: 8485.93
[INFO 2017-06-26 17:35:42,985 main.py:47] epoch 1122, training loss: 7103.93, average training loss: 7262.38, base loss: 8486.81
[INFO 2017-06-26 17:35:43,370 main.py:47] epoch 1123, training loss: 7174.69, average training loss: 7262.08, base loss: 8487.34
[INFO 2017-06-26 17:35:43,821 main.py:47] epoch 1124, training loss: 6404.42, average training loss: 7261.52, base loss: 8487.67
[INFO 2017-06-26 17:35:44,190 main.py:47] epoch 1125, training loss: 8037.71, average training loss: 7260.45, base loss: 8487.62
[INFO 2017-06-26 17:35:44,570 main.py:47] epoch 1126, training loss: 5832.57, average training loss: 7257.25, base loss: 8484.57
[INFO 2017-06-26 17:35:44,926 main.py:47] epoch 1127, training loss: 6629.10, average training loss: 7255.86, base loss: 8484.07
[INFO 2017-06-26 17:35:45,295 main.py:47] epoch 1128, training loss: 6385.51, average training loss: 7253.69, base loss: 8482.56
[INFO 2017-06-26 17:35:45,656 main.py:47] epoch 1129, training loss: 6946.74, average training loss: 7253.64, base loss: 8483.38
[INFO 2017-06-26 17:35:46,031 main.py:47] epoch 1130, training loss: 6286.70, average training loss: 7251.60, base loss: 8482.14
[INFO 2017-06-26 17:35:46,399 main.py:47] epoch 1131, training loss: 6282.48, average training loss: 7249.49, base loss: 8480.84
[INFO 2017-06-26 17:35:46,772 main.py:47] epoch 1132, training loss: 6972.21, average training loss: 7248.29, base loss: 8480.69
[INFO 2017-06-26 17:35:47,125 main.py:47] epoch 1133, training loss: 7470.84, average training loss: 7247.53, base loss: 8481.11
[INFO 2017-06-26 17:35:47,488 main.py:47] epoch 1134, training loss: 6648.54, average training loss: 7247.20, base loss: 8481.96
[INFO 2017-06-26 17:35:47,852 main.py:47] epoch 1135, training loss: 6625.66, average training loss: 7245.21, base loss: 8481.27
[INFO 2017-06-26 17:35:48,207 main.py:47] epoch 1136, training loss: 6759.56, average training loss: 7244.02, base loss: 8480.95
[INFO 2017-06-26 17:35:48,570 main.py:47] epoch 1137, training loss: 6708.94, average training loss: 7243.62, base loss: 8481.63
[INFO 2017-06-26 17:35:48,922 main.py:47] epoch 1138, training loss: 7033.97, average training loss: 7242.98, base loss: 8482.28
[INFO 2017-06-26 17:35:49,274 main.py:47] epoch 1139, training loss: 7675.78, average training loss: 7243.12, base loss: 8484.08
[INFO 2017-06-26 17:35:49,626 main.py:47] epoch 1140, training loss: 5924.83, average training loss: 7241.71, base loss: 8482.98
[INFO 2017-06-26 17:35:49,980 main.py:47] epoch 1141, training loss: 6361.85, average training loss: 7239.68, base loss: 8481.59
[INFO 2017-06-26 17:35:50,351 main.py:47] epoch 1142, training loss: 7545.12, average training loss: 7239.90, base loss: 8483.32
[INFO 2017-06-26 17:35:50,718 main.py:47] epoch 1143, training loss: 7226.64, average training loss: 7238.27, base loss: 8482.64
[INFO 2017-06-26 17:35:51,108 main.py:47] epoch 1144, training loss: 6943.84, average training loss: 7237.14, base loss: 8482.52
[INFO 2017-06-26 17:35:51,468 main.py:47] epoch 1145, training loss: 6480.33, average training loss: 7235.64, base loss: 8482.01
[INFO 2017-06-26 17:35:51,823 main.py:47] epoch 1146, training loss: 6099.22, average training loss: 7234.09, base loss: 8481.03
[INFO 2017-06-26 17:35:52,182 main.py:47] epoch 1147, training loss: 6628.38, average training loss: 7232.86, base loss: 8481.51
[INFO 2017-06-26 17:35:52,541 main.py:47] epoch 1148, training loss: 7999.21, average training loss: 7231.95, base loss: 8482.05
[INFO 2017-06-26 17:35:52,928 main.py:47] epoch 1149, training loss: 6087.09, average training loss: 7231.35, base loss: 8482.30
[INFO 2017-06-26 17:35:53,298 main.py:47] epoch 1150, training loss: 6642.09, average training loss: 7230.03, base loss: 8481.99
[INFO 2017-06-26 17:35:53,660 main.py:47] epoch 1151, training loss: 6709.61, average training loss: 7227.97, base loss: 8480.97
[INFO 2017-06-26 17:35:54,052 main.py:47] epoch 1152, training loss: 6953.80, average training loss: 7227.70, base loss: 8481.42
[INFO 2017-06-26 17:35:54,418 main.py:47] epoch 1153, training loss: 6885.21, average training loss: 7227.74, base loss: 8482.81
[INFO 2017-06-26 17:35:54,787 main.py:47] epoch 1154, training loss: 6801.27, average training loss: 7225.71, base loss: 8481.83
[INFO 2017-06-26 17:35:55,147 main.py:47] epoch 1155, training loss: 6141.64, average training loss: 7223.12, base loss: 8479.19
[INFO 2017-06-26 17:35:55,499 main.py:47] epoch 1156, training loss: 6532.86, average training loss: 7222.34, base loss: 8479.27
[INFO 2017-06-26 17:35:55,850 main.py:47] epoch 1157, training loss: 7027.83, average training loss: 7222.69, base loss: 8480.46
[INFO 2017-06-26 17:35:56,203 main.py:47] epoch 1158, training loss: 7500.17, average training loss: 7222.56, base loss: 8481.60
[INFO 2017-06-26 17:35:56,556 main.py:47] epoch 1159, training loss: 7162.03, average training loss: 7222.70, base loss: 8483.35
[INFO 2017-06-26 17:35:56,906 main.py:47] epoch 1160, training loss: 6893.51, average training loss: 7221.88, base loss: 8483.37
[INFO 2017-06-26 17:35:57,296 main.py:47] epoch 1161, training loss: 7701.65, average training loss: 7221.75, base loss: 8484.14
[INFO 2017-06-26 17:35:57,661 main.py:47] epoch 1162, training loss: 7023.23, average training loss: 7220.04, base loss: 8483.26
[INFO 2017-06-26 17:35:58,046 main.py:47] epoch 1163, training loss: 6771.15, average training loss: 7219.46, base loss: 8483.71
[INFO 2017-06-26 17:35:58,417 main.py:47] epoch 1164, training loss: 7024.88, average training loss: 7219.36, base loss: 8485.30
[INFO 2017-06-26 17:35:58,780 main.py:47] epoch 1165, training loss: 6113.51, average training loss: 7218.09, base loss: 8484.99
[INFO 2017-06-26 17:35:59,166 main.py:47] epoch 1166, training loss: 6019.08, average training loss: 7216.02, base loss: 8483.17
[INFO 2017-06-26 17:35:59,521 main.py:47] epoch 1167, training loss: 6440.54, average training loss: 7214.67, base loss: 8483.22
[INFO 2017-06-26 17:35:59,875 main.py:47] epoch 1168, training loss: 7079.18, average training loss: 7213.90, base loss: 8483.47
[INFO 2017-06-26 17:36:00,236 main.py:47] epoch 1169, training loss: 7097.69, average training loss: 7212.64, base loss: 8483.63
[INFO 2017-06-26 17:36:00,589 main.py:47] epoch 1170, training loss: 6224.76, average training loss: 7210.33, base loss: 8481.91
[INFO 2017-06-26 17:36:00,976 main.py:47] epoch 1171, training loss: 6876.50, average training loss: 7208.27, base loss: 8480.78
[INFO 2017-06-26 17:36:01,330 main.py:47] epoch 1172, training loss: 7434.19, average training loss: 7207.90, base loss: 8481.39
[INFO 2017-06-26 17:36:01,689 main.py:47] epoch 1173, training loss: 7897.96, average training loss: 7208.06, base loss: 8483.30
[INFO 2017-06-26 17:36:02,041 main.py:47] epoch 1174, training loss: 6680.32, average training loss: 7207.76, base loss: 8484.04
[INFO 2017-06-26 17:36:02,393 main.py:47] epoch 1175, training loss: 6588.84, average training loss: 7207.15, base loss: 8484.30
[INFO 2017-06-26 17:36:02,778 main.py:47] epoch 1176, training loss: 7428.07, average training loss: 7207.16, base loss: 8485.48
[INFO 2017-06-26 17:36:03,146 main.py:47] epoch 1177, training loss: 6902.76, average training loss: 7205.50, base loss: 8484.44
[INFO 2017-06-26 17:36:03,503 main.py:47] epoch 1178, training loss: 6987.23, average training loss: 7205.64, base loss: 8486.05
[INFO 2017-06-26 17:36:03,857 main.py:47] epoch 1179, training loss: 6353.90, average training loss: 7203.94, base loss: 8485.15
[INFO 2017-06-26 17:36:04,246 main.py:47] epoch 1180, training loss: 7246.01, average training loss: 7203.32, base loss: 8485.96
[INFO 2017-06-26 17:36:04,607 main.py:47] epoch 1181, training loss: 6800.52, average training loss: 7202.67, base loss: 8486.55
[INFO 2017-06-26 17:36:04,963 main.py:47] epoch 1182, training loss: 5888.65, average training loss: 7201.87, base loss: 8486.48
[INFO 2017-06-26 17:36:05,338 main.py:47] epoch 1183, training loss: 6030.57, average training loss: 7198.45, base loss: 8483.54
[INFO 2017-06-26 17:36:05,692 main.py:47] epoch 1184, training loss: 6234.48, average training loss: 7196.59, base loss: 8482.51
[INFO 2017-06-26 17:36:06,054 main.py:47] epoch 1185, training loss: 7780.21, average training loss: 7197.79, base loss: 8484.94
[INFO 2017-06-26 17:36:06,421 main.py:47] epoch 1186, training loss: 6291.00, average training loss: 7195.47, base loss: 8483.19
[INFO 2017-06-26 17:36:06,772 main.py:47] epoch 1187, training loss: 6190.39, average training loss: 7194.08, base loss: 8482.22
[INFO 2017-06-26 17:36:07,125 main.py:47] epoch 1188, training loss: 6185.21, average training loss: 7190.48, base loss: 8479.11
[INFO 2017-06-26 17:36:07,511 main.py:47] epoch 1189, training loss: 7452.38, average training loss: 7190.34, base loss: 8480.18
[INFO 2017-06-26 17:36:07,906 main.py:47] epoch 1190, training loss: 7469.13, average training loss: 7189.77, base loss: 8480.59
[INFO 2017-06-26 17:36:08,260 main.py:47] epoch 1191, training loss: 7013.05, average training loss: 7188.76, base loss: 8480.65
[INFO 2017-06-26 17:36:08,614 main.py:47] epoch 1192, training loss: 7817.05, average training loss: 7188.37, base loss: 8481.55
[INFO 2017-06-26 17:36:08,974 main.py:47] epoch 1193, training loss: 7543.07, average training loss: 7187.77, base loss: 8481.51
[INFO 2017-06-26 17:36:09,360 main.py:47] epoch 1194, training loss: 6386.67, average training loss: 7187.29, base loss: 8482.01
[INFO 2017-06-26 17:36:09,715 main.py:47] epoch 1195, training loss: 7086.69, average training loss: 7187.47, base loss: 8483.18
[INFO 2017-06-26 17:36:10,077 main.py:47] epoch 1196, training loss: 8822.74, average training loss: 7188.02, base loss: 8484.95
[INFO 2017-06-26 17:36:10,431 main.py:47] epoch 1197, training loss: 7707.44, average training loss: 7188.25, base loss: 8486.06
[INFO 2017-06-26 17:36:10,822 main.py:47] epoch 1198, training loss: 6806.88, average training loss: 7186.23, base loss: 8484.83
[INFO 2017-06-26 17:36:11,176 main.py:47] epoch 1199, training loss: 6934.32, average training loss: 7186.50, base loss: 8486.02
[INFO 2017-06-26 17:36:11,176 main.py:49] epoch 1199, testing
[INFO 2017-06-26 17:36:15,804 main.py:100] average testing loss: 6799.82, base loss: 8266.38
[INFO 2017-06-26 17:36:15,828 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:36:15,834 main.py:73] current best accuracy: 6799.82
[INFO 2017-06-26 17:36:16,220 main.py:47] epoch 1200, training loss: 7256.32, average training loss: 7185.97, base loss: 8486.57
[INFO 2017-06-26 17:36:16,596 main.py:47] epoch 1201, training loss: 6958.68, average training loss: 7184.78, base loss: 8486.13
[INFO 2017-06-26 17:36:16,994 main.py:47] epoch 1202, training loss: 7180.19, average training loss: 7183.70, base loss: 8485.83
[INFO 2017-06-26 17:36:17,403 main.py:47] epoch 1203, training loss: 7357.55, average training loss: 7185.02, base loss: 8488.91
[INFO 2017-06-26 17:36:17,757 main.py:47] epoch 1204, training loss: 7561.29, average training loss: 7184.66, base loss: 8489.63
[INFO 2017-06-26 17:36:18,118 main.py:47] epoch 1205, training loss: 6128.09, average training loss: 7183.46, base loss: 8489.18
[INFO 2017-06-26 17:36:18,469 main.py:47] epoch 1206, training loss: 7969.22, average training loss: 7183.36, base loss: 8490.33
[INFO 2017-06-26 17:36:18,824 main.py:47] epoch 1207, training loss: 6762.06, average training loss: 7182.33, base loss: 8489.88
[INFO 2017-06-26 17:36:19,184 main.py:47] epoch 1208, training loss: 6506.27, average training loss: 7180.91, base loss: 8489.49
[INFO 2017-06-26 17:36:19,537 main.py:47] epoch 1209, training loss: 6509.57, average training loss: 7180.29, base loss: 8489.80
[INFO 2017-06-26 17:36:19,912 main.py:47] epoch 1210, training loss: 8241.15, average training loss: 7179.98, base loss: 8490.59
[INFO 2017-06-26 17:36:20,264 main.py:47] epoch 1211, training loss: 6546.22, average training loss: 7179.34, base loss: 8491.00
[INFO 2017-06-26 17:36:20,617 main.py:47] epoch 1212, training loss: 6804.09, average training loss: 7179.12, base loss: 8491.80
[INFO 2017-06-26 17:36:20,976 main.py:47] epoch 1213, training loss: 6803.59, average training loss: 7177.80, base loss: 8491.44
[INFO 2017-06-26 17:36:21,332 main.py:47] epoch 1214, training loss: 7249.08, average training loss: 7176.26, base loss: 8491.03
[INFO 2017-06-26 17:36:21,687 main.py:47] epoch 1215, training loss: 6801.42, average training loss: 7175.33, base loss: 8491.17
[INFO 2017-06-26 17:36:22,043 main.py:47] epoch 1216, training loss: 7322.84, average training loss: 7175.44, base loss: 8492.13
[INFO 2017-06-26 17:36:22,444 main.py:47] epoch 1217, training loss: 6283.10, average training loss: 7173.43, base loss: 8490.94
[INFO 2017-06-26 17:36:22,809 main.py:47] epoch 1218, training loss: 7217.13, average training loss: 7173.34, base loss: 8491.87
[INFO 2017-06-26 17:36:23,180 main.py:47] epoch 1219, training loss: 6798.45, average training loss: 7172.53, base loss: 8491.80
[INFO 2017-06-26 17:36:23,540 main.py:47] epoch 1220, training loss: 6810.20, average training loss: 7169.60, base loss: 8489.41
[INFO 2017-06-26 17:36:23,894 main.py:47] epoch 1221, training loss: 6552.23, average training loss: 7168.79, base loss: 8489.22
[INFO 2017-06-26 17:36:24,260 main.py:47] epoch 1222, training loss: 6216.89, average training loss: 7167.66, base loss: 8489.05
[INFO 2017-06-26 17:36:24,630 main.py:47] epoch 1223, training loss: 6470.54, average training loss: 7166.67, base loss: 8488.94
[INFO 2017-06-26 17:36:24,994 main.py:47] epoch 1224, training loss: 6650.49, average training loss: 7166.45, base loss: 8490.05
[INFO 2017-06-26 17:36:25,351 main.py:47] epoch 1225, training loss: 7101.68, average training loss: 7166.10, base loss: 8489.91
[INFO 2017-06-26 17:36:25,710 main.py:47] epoch 1226, training loss: 6764.91, average training loss: 7166.00, base loss: 8490.71
[INFO 2017-06-26 17:36:26,063 main.py:47] epoch 1227, training loss: 7566.66, average training loss: 7165.76, base loss: 8491.46
[INFO 2017-06-26 17:36:26,418 main.py:47] epoch 1228, training loss: 7370.02, average training loss: 7165.36, base loss: 8491.68
[INFO 2017-06-26 17:36:26,771 main.py:47] epoch 1229, training loss: 6097.42, average training loss: 7164.28, base loss: 8491.67
[INFO 2017-06-26 17:36:27,124 main.py:47] epoch 1230, training loss: 7227.22, average training loss: 7164.04, base loss: 8492.23
[INFO 2017-06-26 17:36:27,529 main.py:47] epoch 1231, training loss: 6568.46, average training loss: 7162.41, base loss: 8491.23
[INFO 2017-06-26 17:36:27,912 main.py:47] epoch 1232, training loss: 6177.45, average training loss: 7160.67, base loss: 8489.13
[INFO 2017-06-26 17:36:28,293 main.py:47] epoch 1233, training loss: 7163.21, average training loss: 7160.71, base loss: 8490.37
[INFO 2017-06-26 17:36:28,676 main.py:47] epoch 1234, training loss: 7739.78, average training loss: 7160.70, base loss: 8490.71
[INFO 2017-06-26 17:36:29,041 main.py:47] epoch 1235, training loss: 6551.61, average training loss: 7158.55, base loss: 8489.04
[INFO 2017-06-26 17:36:29,425 main.py:47] epoch 1236, training loss: 7657.05, average training loss: 7157.47, base loss: 8488.08
[INFO 2017-06-26 17:36:29,806 main.py:47] epoch 1237, training loss: 6874.42, average training loss: 7157.36, base loss: 8488.70
[INFO 2017-06-26 17:36:30,172 main.py:47] epoch 1238, training loss: 6893.25, average training loss: 7157.24, base loss: 8489.08
[INFO 2017-06-26 17:36:30,530 main.py:47] epoch 1239, training loss: 6696.24, average training loss: 7156.33, base loss: 8489.00
[INFO 2017-06-26 17:36:30,882 main.py:47] epoch 1240, training loss: 6612.41, average training loss: 7155.56, base loss: 8489.30
[INFO 2017-06-26 17:36:31,233 main.py:47] epoch 1241, training loss: 6042.43, average training loss: 7154.73, base loss: 8488.96
[INFO 2017-06-26 17:36:31,588 main.py:47] epoch 1242, training loss: 7201.39, average training loss: 7154.84, base loss: 8490.22
[INFO 2017-06-26 17:36:31,940 main.py:47] epoch 1243, training loss: 6326.03, average training loss: 7154.21, base loss: 8489.78
[INFO 2017-06-26 17:36:32,293 main.py:47] epoch 1244, training loss: 6201.24, average training loss: 7152.44, base loss: 8488.51
[INFO 2017-06-26 17:36:32,643 main.py:47] epoch 1245, training loss: 6775.42, average training loss: 7151.48, base loss: 8488.07
[INFO 2017-06-26 17:36:32,998 main.py:47] epoch 1246, training loss: 6253.44, average training loss: 7150.70, base loss: 8487.82
[INFO 2017-06-26 17:36:33,350 main.py:47] epoch 1247, training loss: 7131.68, average training loss: 7149.90, base loss: 8488.11
[INFO 2017-06-26 17:36:33,701 main.py:47] epoch 1248, training loss: 7213.15, average training loss: 7149.85, base loss: 8488.80
[INFO 2017-06-26 17:36:34,053 main.py:47] epoch 1249, training loss: 6956.08, average training loss: 7149.91, base loss: 8489.75
[INFO 2017-06-26 17:36:34,407 main.py:47] epoch 1250, training loss: 7062.45, average training loss: 7149.71, base loss: 8490.78
[INFO 2017-06-26 17:36:34,762 main.py:47] epoch 1251, training loss: 6551.07, average training loss: 7148.69, base loss: 8490.79
[INFO 2017-06-26 17:36:35,115 main.py:47] epoch 1252, training loss: 5913.60, average training loss: 7147.21, base loss: 8489.42
[INFO 2017-06-26 17:36:35,469 main.py:47] epoch 1253, training loss: 6482.91, average training loss: 7145.38, base loss: 8487.92
[INFO 2017-06-26 17:36:35,825 main.py:47] epoch 1254, training loss: 6061.20, average training loss: 7144.90, base loss: 8488.14
[INFO 2017-06-26 17:36:36,179 main.py:47] epoch 1255, training loss: 6967.02, average training loss: 7144.21, base loss: 8488.13
[INFO 2017-06-26 17:36:36,534 main.py:47] epoch 1256, training loss: 6321.79, average training loss: 7142.99, base loss: 8488.11
[INFO 2017-06-26 17:36:36,887 main.py:47] epoch 1257, training loss: 6795.21, average training loss: 7141.63, base loss: 8487.23
[INFO 2017-06-26 17:36:37,239 main.py:47] epoch 1258, training loss: 7649.55, average training loss: 7140.95, base loss: 8487.25
[INFO 2017-06-26 17:36:37,593 main.py:47] epoch 1259, training loss: 6955.84, average training loss: 7139.68, base loss: 8486.55
[INFO 2017-06-26 17:36:37,966 main.py:47] epoch 1260, training loss: 5821.24, average training loss: 7136.53, base loss: 8483.50
[INFO 2017-06-26 17:36:38,337 main.py:47] epoch 1261, training loss: 7126.92, average training loss: 7135.99, base loss: 8483.54
[INFO 2017-06-26 17:36:38,702 main.py:47] epoch 1262, training loss: 6786.20, average training loss: 7135.50, base loss: 8484.69
[INFO 2017-06-26 17:36:39,054 main.py:47] epoch 1263, training loss: 7159.23, average training loss: 7134.66, base loss: 8484.64
[INFO 2017-06-26 17:36:39,405 main.py:47] epoch 1264, training loss: 6328.63, average training loss: 7132.14, base loss: 8482.86
[INFO 2017-06-26 17:36:39,757 main.py:47] epoch 1265, training loss: 7365.64, average training loss: 7132.54, base loss: 8484.38
[INFO 2017-06-26 17:36:40,110 main.py:47] epoch 1266, training loss: 6820.96, average training loss: 7131.63, base loss: 8483.77
[INFO 2017-06-26 17:36:40,494 main.py:47] epoch 1267, training loss: 7564.25, average training loss: 7131.80, base loss: 8484.45
[INFO 2017-06-26 17:36:40,853 main.py:47] epoch 1268, training loss: 7044.12, average training loss: 7130.21, base loss: 8483.02
[INFO 2017-06-26 17:36:41,206 main.py:47] epoch 1269, training loss: 7421.20, average training loss: 7130.50, base loss: 8484.36
[INFO 2017-06-26 17:36:41,562 main.py:47] epoch 1270, training loss: 6784.15, average training loss: 7129.50, base loss: 8484.29
[INFO 2017-06-26 17:36:41,915 main.py:47] epoch 1271, training loss: 6933.12, average training loss: 7128.44, base loss: 8482.83
[INFO 2017-06-26 17:36:42,271 main.py:47] epoch 1272, training loss: 6920.85, average training loss: 7127.85, base loss: 8483.21
[INFO 2017-06-26 17:36:42,624 main.py:47] epoch 1273, training loss: 7029.26, average training loss: 7128.40, base loss: 8484.15
[INFO 2017-06-26 17:36:43,009 main.py:47] epoch 1274, training loss: 5788.26, average training loss: 7126.08, base loss: 8481.88
[INFO 2017-06-26 17:36:43,384 main.py:47] epoch 1275, training loss: 8241.01, average training loss: 7126.80, base loss: 8483.57
[INFO 2017-06-26 17:36:43,744 main.py:47] epoch 1276, training loss: 7027.41, average training loss: 7126.63, base loss: 8484.41
[INFO 2017-06-26 17:36:44,098 main.py:47] epoch 1277, training loss: 6704.55, average training loss: 7125.52, base loss: 8483.44
[INFO 2017-06-26 17:36:44,449 main.py:47] epoch 1278, training loss: 7012.74, average training loss: 7125.11, base loss: 8483.88
[INFO 2017-06-26 17:36:44,808 main.py:47] epoch 1279, training loss: 7810.61, average training loss: 7125.58, base loss: 8484.92
[INFO 2017-06-26 17:36:45,202 main.py:47] epoch 1280, training loss: 6085.70, average training loss: 7124.52, base loss: 8484.37
[INFO 2017-06-26 17:36:45,572 main.py:47] epoch 1281, training loss: 7335.06, average training loss: 7124.60, base loss: 8485.50
[INFO 2017-06-26 17:36:45,927 main.py:47] epoch 1282, training loss: 6865.63, average training loss: 7124.19, base loss: 8485.76
[INFO 2017-06-26 17:36:46,313 main.py:47] epoch 1283, training loss: 6048.72, average training loss: 7122.85, base loss: 8484.56
[INFO 2017-06-26 17:36:46,691 main.py:47] epoch 1284, training loss: 6395.01, average training loss: 7121.30, base loss: 8482.83
[INFO 2017-06-26 17:36:47,063 main.py:47] epoch 1285, training loss: 7585.54, average training loss: 7121.64, base loss: 8483.82
[INFO 2017-06-26 17:36:47,454 main.py:47] epoch 1286, training loss: 6574.37, average training loss: 7120.68, base loss: 8482.80
[INFO 2017-06-26 17:36:47,888 main.py:47] epoch 1287, training loss: 6836.82, average training loss: 7120.38, base loss: 8483.24
[INFO 2017-06-26 17:36:48,302 main.py:47] epoch 1288, training loss: 8173.69, average training loss: 7121.34, base loss: 8485.37
[INFO 2017-06-26 17:36:48,714 main.py:47] epoch 1289, training loss: 7067.66, average training loss: 7120.46, base loss: 8484.47
[INFO 2017-06-26 17:36:49,067 main.py:47] epoch 1290, training loss: 5866.27, average training loss: 7120.09, base loss: 8484.66
[INFO 2017-06-26 17:36:49,427 main.py:47] epoch 1291, training loss: 7236.28, average training loss: 7120.74, base loss: 8486.28
[INFO 2017-06-26 17:36:49,793 main.py:47] epoch 1292, training loss: 7055.27, average training loss: 7121.06, base loss: 8487.64
[INFO 2017-06-26 17:36:50,145 main.py:47] epoch 1293, training loss: 6502.03, average training loss: 7119.64, base loss: 8486.13
[INFO 2017-06-26 17:36:50,497 main.py:47] epoch 1294, training loss: 7383.76, average training loss: 7119.68, base loss: 8487.42
[INFO 2017-06-26 17:36:50,849 main.py:47] epoch 1295, training loss: 7085.19, average training loss: 7118.89, base loss: 8487.33
[INFO 2017-06-26 17:36:51,204 main.py:47] epoch 1296, training loss: 8483.40, average training loss: 7119.78, base loss: 8489.56
[INFO 2017-06-26 17:36:51,556 main.py:47] epoch 1297, training loss: 7211.47, average training loss: 7119.38, base loss: 8490.49
[INFO 2017-06-26 17:36:51,909 main.py:47] epoch 1298, training loss: 7352.31, average training loss: 7119.44, base loss: 8491.56
[INFO 2017-06-26 17:36:52,262 main.py:47] epoch 1299, training loss: 7027.60, average training loss: 7119.08, base loss: 8491.68
[INFO 2017-06-26 17:36:52,262 main.py:49] epoch 1299, testing
[INFO 2017-06-26 17:36:56,481 main.py:100] average testing loss: 6977.85, base loss: 8523.95
[INFO 2017-06-26 17:36:56,505 main.py:73] current best accuracy: 6799.82
[INFO 2017-06-26 17:36:56,856 main.py:47] epoch 1300, training loss: 7163.94, average training loss: 7119.48, base loss: 8493.25
[INFO 2017-06-26 17:36:57,206 main.py:47] epoch 1301, training loss: 7299.83, average training loss: 7118.96, base loss: 8493.10
[INFO 2017-06-26 17:36:57,558 main.py:47] epoch 1302, training loss: 7228.84, average training loss: 7118.62, base loss: 8492.74
[INFO 2017-06-26 17:36:57,909 main.py:47] epoch 1303, training loss: 7530.16, average training loss: 7119.07, base loss: 8494.31
[INFO 2017-06-26 17:36:58,260 main.py:47] epoch 1304, training loss: 7407.91, average training loss: 7119.24, base loss: 8495.46
[INFO 2017-06-26 17:36:58,613 main.py:47] epoch 1305, training loss: 6462.72, average training loss: 7119.37, base loss: 8496.45
[INFO 2017-06-26 17:36:58,966 main.py:47] epoch 1306, training loss: 6516.56, average training loss: 7118.86, base loss: 8496.20
[INFO 2017-06-26 17:36:59,316 main.py:47] epoch 1307, training loss: 7495.97, average training loss: 7118.35, base loss: 8496.91
[INFO 2017-06-26 17:36:59,667 main.py:47] epoch 1308, training loss: 6503.00, average training loss: 7116.64, base loss: 8495.21
[INFO 2017-06-26 17:37:00,019 main.py:47] epoch 1309, training loss: 6889.18, average training loss: 7115.37, base loss: 8494.65
[INFO 2017-06-26 17:37:00,377 main.py:47] epoch 1310, training loss: 7262.21, average training loss: 7115.36, base loss: 8495.31
[INFO 2017-06-26 17:37:00,729 main.py:47] epoch 1311, training loss: 6733.95, average training loss: 7114.78, base loss: 8495.59
[INFO 2017-06-26 17:37:01,082 main.py:47] epoch 1312, training loss: 7353.18, average training loss: 7114.50, base loss: 8495.87
[INFO 2017-06-26 17:37:01,435 main.py:47] epoch 1313, training loss: 6805.07, average training loss: 7114.41, base loss: 8496.58
[INFO 2017-06-26 17:37:01,787 main.py:47] epoch 1314, training loss: 7243.93, average training loss: 7113.52, base loss: 8496.68
[INFO 2017-06-26 17:37:02,141 main.py:47] epoch 1315, training loss: 7364.51, average training loss: 7113.00, base loss: 8497.22
[INFO 2017-06-26 17:37:02,525 main.py:47] epoch 1316, training loss: 6865.46, average training loss: 7113.22, base loss: 8498.80
[INFO 2017-06-26 17:37:02,889 main.py:47] epoch 1317, training loss: 7031.23, average training loss: 7112.26, base loss: 8498.52
[INFO 2017-06-26 17:37:03,245 main.py:47] epoch 1318, training loss: 6309.56, average training loss: 7111.17, base loss: 8497.52
[INFO 2017-06-26 17:37:03,595 main.py:47] epoch 1319, training loss: 7582.72, average training loss: 7110.47, base loss: 8498.09
[INFO 2017-06-26 17:37:03,947 main.py:47] epoch 1320, training loss: 6177.69, average training loss: 7109.72, base loss: 8498.28
[INFO 2017-06-26 17:37:04,298 main.py:47] epoch 1321, training loss: 6224.09, average training loss: 7107.41, base loss: 8496.07
[INFO 2017-06-26 17:37:04,648 main.py:47] epoch 1322, training loss: 7821.27, average training loss: 7106.95, base loss: 8495.82
[INFO 2017-06-26 17:37:04,997 main.py:47] epoch 1323, training loss: 5594.37, average training loss: 7104.21, base loss: 8492.72
[INFO 2017-06-26 17:37:05,370 main.py:47] epoch 1324, training loss: 6872.26, average training loss: 7103.73, base loss: 8492.92
[INFO 2017-06-26 17:37:05,725 main.py:47] epoch 1325, training loss: 6180.84, average training loss: 7102.63, base loss: 8492.66
[INFO 2017-06-26 17:37:06,083 main.py:47] epoch 1326, training loss: 5858.61, average training loss: 7100.60, base loss: 8490.79
[INFO 2017-06-26 17:37:06,438 main.py:47] epoch 1327, training loss: 6471.21, average training loss: 7099.63, base loss: 8489.89
[INFO 2017-06-26 17:37:06,789 main.py:47] epoch 1328, training loss: 7066.26, average training loss: 7099.18, base loss: 8489.99
[INFO 2017-06-26 17:37:07,140 main.py:47] epoch 1329, training loss: 7035.45, average training loss: 7098.87, base loss: 8490.37
[INFO 2017-06-26 17:37:07,492 main.py:47] epoch 1330, training loss: 6603.99, average training loss: 7097.38, base loss: 8489.45
[INFO 2017-06-26 17:37:07,844 main.py:47] epoch 1331, training loss: 8698.96, average training loss: 7097.60, base loss: 8491.11
[INFO 2017-06-26 17:37:08,200 main.py:47] epoch 1332, training loss: 7508.73, average training loss: 7097.37, base loss: 8492.01
[INFO 2017-06-26 17:37:08,552 main.py:47] epoch 1333, training loss: 7855.79, average training loss: 7097.56, base loss: 8493.14
[INFO 2017-06-26 17:37:08,902 main.py:47] epoch 1334, training loss: 7367.40, average training loss: 7097.03, base loss: 8493.79
[INFO 2017-06-26 17:37:09,254 main.py:47] epoch 1335, training loss: 6567.85, average training loss: 7096.97, base loss: 8494.36
[INFO 2017-06-26 17:37:09,607 main.py:47] epoch 1336, training loss: 7424.04, average training loss: 7097.45, base loss: 8495.31
[INFO 2017-06-26 17:37:09,960 main.py:47] epoch 1337, training loss: 7623.69, average training loss: 7098.46, base loss: 8497.24
[INFO 2017-06-26 17:37:10,310 main.py:47] epoch 1338, training loss: 7931.33, average training loss: 7098.23, base loss: 8497.68
[INFO 2017-06-26 17:37:10,672 main.py:47] epoch 1339, training loss: 7053.27, average training loss: 7097.45, base loss: 8496.99
[INFO 2017-06-26 17:37:11,026 main.py:47] epoch 1340, training loss: 6489.52, average training loss: 7096.70, base loss: 8497.59
[INFO 2017-06-26 17:37:11,378 main.py:47] epoch 1341, training loss: 6828.57, average training loss: 7096.24, base loss: 8497.68
[INFO 2017-06-26 17:37:11,733 main.py:47] epoch 1342, training loss: 8035.54, average training loss: 7096.07, base loss: 8498.50
[INFO 2017-06-26 17:37:12,083 main.py:47] epoch 1343, training loss: 6342.58, average training loss: 7095.09, base loss: 8497.56
[INFO 2017-06-26 17:37:12,433 main.py:47] epoch 1344, training loss: 6441.62, average training loss: 7095.17, base loss: 8497.90
[INFO 2017-06-26 17:37:12,786 main.py:47] epoch 1345, training loss: 7329.79, average training loss: 7096.10, base loss: 8499.41
[INFO 2017-06-26 17:37:13,144 main.py:47] epoch 1346, training loss: 8304.65, average training loss: 7096.78, base loss: 8501.24
[INFO 2017-06-26 17:37:13,506 main.py:47] epoch 1347, training loss: 6504.22, average training loss: 7096.01, base loss: 8501.04
[INFO 2017-06-26 17:37:13,862 main.py:47] epoch 1348, training loss: 6950.12, average training loss: 7096.07, base loss: 8501.63
[INFO 2017-06-26 17:37:14,217 main.py:47] epoch 1349, training loss: 6543.50, average training loss: 7095.08, base loss: 8500.50
[INFO 2017-06-26 17:37:14,571 main.py:47] epoch 1350, training loss: 6542.53, average training loss: 7093.73, base loss: 8499.35
[INFO 2017-06-26 17:37:14,928 main.py:47] epoch 1351, training loss: 6939.49, average training loss: 7093.36, base loss: 8499.51
[INFO 2017-06-26 17:37:15,314 main.py:47] epoch 1352, training loss: 6708.59, average training loss: 7093.17, base loss: 8499.93
[INFO 2017-06-26 17:37:15,708 main.py:47] epoch 1353, training loss: 7679.44, average training loss: 7093.45, base loss: 8501.04
[INFO 2017-06-26 17:37:16,067 main.py:47] epoch 1354, training loss: 6288.55, average training loss: 7091.25, base loss: 8499.01
[INFO 2017-06-26 17:37:16,422 main.py:47] epoch 1355, training loss: 6228.95, average training loss: 7088.27, base loss: 8495.84
[INFO 2017-06-26 17:37:16,774 main.py:47] epoch 1356, training loss: 6897.14, average training loss: 7088.07, base loss: 8496.54
[INFO 2017-06-26 17:37:17,127 main.py:47] epoch 1357, training loss: 7244.70, average training loss: 7088.39, base loss: 8498.38
[INFO 2017-06-26 17:37:17,511 main.py:47] epoch 1358, training loss: 7119.92, average training loss: 7088.19, base loss: 8498.85
[INFO 2017-06-26 17:37:17,917 main.py:47] epoch 1359, training loss: 7096.19, average training loss: 7087.45, base loss: 8498.05
[INFO 2017-06-26 17:37:18,272 main.py:47] epoch 1360, training loss: 6286.78, average training loss: 7086.56, base loss: 8497.70
[INFO 2017-06-26 17:37:18,626 main.py:47] epoch 1361, training loss: 6552.44, average training loss: 7085.48, base loss: 8497.39
[INFO 2017-06-26 17:37:18,978 main.py:47] epoch 1362, training loss: 6427.12, average training loss: 7085.18, base loss: 8497.70
[INFO 2017-06-26 17:37:19,330 main.py:47] epoch 1363, training loss: 7922.21, average training loss: 7085.16, base loss: 8498.38
[INFO 2017-06-26 17:37:19,715 main.py:47] epoch 1364, training loss: 7364.53, average training loss: 7084.76, base loss: 8498.14
[INFO 2017-06-26 17:37:20,087 main.py:47] epoch 1365, training loss: 7063.35, average training loss: 7083.83, base loss: 8497.31
[INFO 2017-06-26 17:37:20,442 main.py:47] epoch 1366, training loss: 7854.94, average training loss: 7084.88, base loss: 8500.07
[INFO 2017-06-26 17:37:20,810 main.py:47] epoch 1367, training loss: 6605.44, average training loss: 7084.89, base loss: 8500.81
[INFO 2017-06-26 17:37:21,170 main.py:47] epoch 1368, training loss: 6176.06, average training loss: 7083.42, base loss: 8499.38
[INFO 2017-06-26 17:37:21,528 main.py:47] epoch 1369, training loss: 7209.30, average training loss: 7081.97, base loss: 8498.08
[INFO 2017-06-26 17:37:21,883 main.py:47] epoch 1370, training loss: 7798.98, average training loss: 7082.37, base loss: 8499.43
[INFO 2017-06-26 17:37:22,235 main.py:47] epoch 1371, training loss: 6536.47, average training loss: 7082.26, base loss: 8499.94
[INFO 2017-06-26 17:37:22,588 main.py:47] epoch 1372, training loss: 7223.58, average training loss: 7081.84, base loss: 8500.59
[INFO 2017-06-26 17:37:22,948 main.py:47] epoch 1373, training loss: 7118.79, average training loss: 7081.60, base loss: 8500.68
[INFO 2017-06-26 17:37:23,305 main.py:47] epoch 1374, training loss: 6953.15, average training loss: 7080.67, base loss: 8499.93
[INFO 2017-06-26 17:37:23,659 main.py:47] epoch 1375, training loss: 7319.15, average training loss: 7080.81, base loss: 8501.38
[INFO 2017-06-26 17:37:24,008 main.py:47] epoch 1376, training loss: 7435.41, average training loss: 7080.62, base loss: 8501.82
[INFO 2017-06-26 17:37:24,358 main.py:47] epoch 1377, training loss: 6583.18, average training loss: 7079.98, base loss: 8502.08
[INFO 2017-06-26 17:37:24,709 main.py:47] epoch 1378, training loss: 7040.40, average training loss: 7080.16, base loss: 8502.83
[INFO 2017-06-26 17:37:25,061 main.py:47] epoch 1379, training loss: 7688.07, average training loss: 7081.22, base loss: 8504.63
[INFO 2017-06-26 17:37:25,413 main.py:47] epoch 1380, training loss: 6673.74, average training loss: 7080.25, base loss: 8504.33
[INFO 2017-06-26 17:37:25,777 main.py:47] epoch 1381, training loss: 6744.83, average training loss: 7080.11, base loss: 8504.83
[INFO 2017-06-26 17:37:26,128 main.py:47] epoch 1382, training loss: 8375.73, average training loss: 7081.32, base loss: 8507.19
[INFO 2017-06-26 17:37:26,481 main.py:47] epoch 1383, training loss: 6501.57, average training loss: 7080.53, base loss: 8506.92
[INFO 2017-06-26 17:37:26,833 main.py:47] epoch 1384, training loss: 7997.65, average training loss: 7081.17, base loss: 8508.31
[INFO 2017-06-26 17:37:27,187 main.py:47] epoch 1385, training loss: 6643.20, average training loss: 7081.43, base loss: 8508.85
[INFO 2017-06-26 17:37:27,571 main.py:47] epoch 1386, training loss: 6425.05, average training loss: 7080.49, base loss: 8507.66
[INFO 2017-06-26 17:37:27,947 main.py:47] epoch 1387, training loss: 7094.88, average training loss: 7080.72, base loss: 8508.74
[INFO 2017-06-26 17:37:28,319 main.py:47] epoch 1388, training loss: 7007.99, average training loss: 7079.32, base loss: 8507.95
[INFO 2017-06-26 17:37:28,694 main.py:47] epoch 1389, training loss: 7796.46, average training loss: 7079.94, base loss: 8509.51
[INFO 2017-06-26 17:37:29,045 main.py:47] epoch 1390, training loss: 6183.50, average training loss: 7079.08, base loss: 8509.08
[INFO 2017-06-26 17:37:29,395 main.py:47] epoch 1391, training loss: 7409.69, average training loss: 7079.12, base loss: 8510.36
[INFO 2017-06-26 17:37:29,748 main.py:47] epoch 1392, training loss: 7227.69, average training loss: 7077.68, base loss: 8509.41
[INFO 2017-06-26 17:37:30,099 main.py:47] epoch 1393, training loss: 6147.11, average training loss: 7076.16, base loss: 8507.70
[INFO 2017-06-26 17:37:30,450 main.py:47] epoch 1394, training loss: 7495.25, average training loss: 7076.41, base loss: 8509.10
[INFO 2017-06-26 17:37:30,811 main.py:47] epoch 1395, training loss: 7243.53, average training loss: 7074.84, base loss: 8507.13
[INFO 2017-06-26 17:37:31,163 main.py:47] epoch 1396, training loss: 6263.14, average training loss: 7074.67, base loss: 8507.06
[INFO 2017-06-26 17:37:31,516 main.py:47] epoch 1397, training loss: 8092.90, average training loss: 7075.29, base loss: 8507.79
[INFO 2017-06-26 17:37:31,869 main.py:47] epoch 1398, training loss: 7185.26, average training loss: 7075.57, base loss: 8508.59
[INFO 2017-06-26 17:37:32,219 main.py:47] epoch 1399, training loss: 6584.18, average training loss: 7074.29, base loss: 8507.56
[INFO 2017-06-26 17:37:32,219 main.py:49] epoch 1399, testing
[INFO 2017-06-26 17:37:36,400 main.py:100] average testing loss: 7043.71, base loss: 8700.78
[INFO 2017-06-26 17:37:36,425 main.py:73] current best accuracy: 6799.82
[INFO 2017-06-26 17:37:36,778 main.py:47] epoch 1400, training loss: 7289.81, average training loss: 7073.83, base loss: 8507.55
[INFO 2017-06-26 17:37:37,130 main.py:47] epoch 1401, training loss: 6774.14, average training loss: 7072.84, base loss: 8506.73
[INFO 2017-06-26 17:37:37,480 main.py:47] epoch 1402, training loss: 6001.19, average training loss: 7071.46, base loss: 8505.32
[INFO 2017-06-26 17:37:37,832 main.py:47] epoch 1403, training loss: 7853.38, average training loss: 7072.56, base loss: 8507.21
[INFO 2017-06-26 17:37:38,277 main.py:47] epoch 1404, training loss: 6443.11, average training loss: 7071.90, base loss: 8506.62
[INFO 2017-06-26 17:37:38,633 main.py:47] epoch 1405, training loss: 6328.04, average training loss: 7071.47, base loss: 8506.17
[INFO 2017-06-26 17:37:38,990 main.py:47] epoch 1406, training loss: 7242.88, average training loss: 7070.41, base loss: 8505.37
[INFO 2017-06-26 17:37:39,345 main.py:47] epoch 1407, training loss: 6696.98, average training loss: 7069.32, base loss: 8504.34
[INFO 2017-06-26 17:37:39,696 main.py:47] epoch 1408, training loss: 6364.75, average training loss: 7068.74, base loss: 8503.90
[INFO 2017-06-26 17:37:40,051 main.py:47] epoch 1409, training loss: 8276.82, average training loss: 7070.04, base loss: 8507.20
[INFO 2017-06-26 17:37:40,403 main.py:47] epoch 1410, training loss: 7215.17, average training loss: 7069.34, base loss: 8505.66
[INFO 2017-06-26 17:37:40,756 main.py:47] epoch 1411, training loss: 7611.37, average training loss: 7068.81, base loss: 8506.05
[INFO 2017-06-26 17:37:41,113 main.py:47] epoch 1412, training loss: 7101.79, average training loss: 7069.27, base loss: 8506.94
[INFO 2017-06-26 17:37:41,466 main.py:47] epoch 1413, training loss: 6822.27, average training loss: 7069.59, base loss: 8508.63
[INFO 2017-06-26 17:37:41,818 main.py:47] epoch 1414, training loss: 6073.77, average training loss: 7068.47, base loss: 8508.27
[INFO 2017-06-26 17:37:42,172 main.py:47] epoch 1415, training loss: 8267.98, average training loss: 7069.78, base loss: 8510.37
[INFO 2017-06-26 17:37:42,524 main.py:47] epoch 1416, training loss: 7248.57, average training loss: 7069.66, base loss: 8510.33
[INFO 2017-06-26 17:37:42,875 main.py:47] epoch 1417, training loss: 8147.76, average training loss: 7070.65, base loss: 8511.76
[INFO 2017-06-26 17:37:43,227 main.py:47] epoch 1418, training loss: 5892.25, average training loss: 7069.07, base loss: 8510.34
[INFO 2017-06-26 17:37:43,623 main.py:47] epoch 1419, training loss: 7009.08, average training loss: 7069.00, base loss: 8510.95
[INFO 2017-06-26 17:37:43,988 main.py:47] epoch 1420, training loss: 7903.25, average training loss: 7069.60, base loss: 8512.60
[INFO 2017-06-26 17:37:44,341 main.py:47] epoch 1421, training loss: 6548.79, average training loss: 7068.84, base loss: 8512.55
[INFO 2017-06-26 17:37:44,693 main.py:47] epoch 1422, training loss: 6631.19, average training loss: 7068.13, base loss: 8512.24
[INFO 2017-06-26 17:37:45,044 main.py:47] epoch 1423, training loss: 6495.10, average training loss: 7067.62, base loss: 8512.39
[INFO 2017-06-26 17:37:45,397 main.py:47] epoch 1424, training loss: 6489.28, average training loss: 7067.73, base loss: 8513.05
[INFO 2017-06-26 17:37:45,749 main.py:47] epoch 1425, training loss: 6167.71, average training loss: 7065.97, base loss: 8511.34
[INFO 2017-06-26 17:37:46,102 main.py:47] epoch 1426, training loss: 6997.06, average training loss: 7065.23, base loss: 8510.78
[INFO 2017-06-26 17:37:46,454 main.py:47] epoch 1427, training loss: 6536.53, average training loss: 7064.85, base loss: 8510.50
[INFO 2017-06-26 17:37:46,810 main.py:47] epoch 1428, training loss: 6879.44, average training loss: 7064.24, base loss: 8509.89
[INFO 2017-06-26 17:37:47,161 main.py:47] epoch 1429, training loss: 6715.24, average training loss: 7064.07, base loss: 8509.37
[INFO 2017-06-26 17:37:47,514 main.py:47] epoch 1430, training loss: 6253.07, average training loss: 7063.58, base loss: 8508.80
[INFO 2017-06-26 17:37:47,864 main.py:47] epoch 1431, training loss: 6804.44, average training loss: 7063.57, base loss: 8509.43
[INFO 2017-06-26 17:37:48,214 main.py:47] epoch 1432, training loss: 6586.03, average training loss: 7062.75, base loss: 8508.96
[INFO 2017-06-26 17:37:48,565 main.py:47] epoch 1433, training loss: 7524.14, average training loss: 7063.10, base loss: 8509.64
[INFO 2017-06-26 17:37:48,916 main.py:47] epoch 1434, training loss: 6577.85, average training loss: 7063.05, base loss: 8509.98
[INFO 2017-06-26 17:37:49,289 main.py:47] epoch 1435, training loss: 7870.60, average training loss: 7063.72, base loss: 8511.16
[INFO 2017-06-26 17:37:49,645 main.py:47] epoch 1436, training loss: 6625.27, average training loss: 7063.78, base loss: 8511.80
[INFO 2017-06-26 17:37:50,013 main.py:47] epoch 1437, training loss: 6425.07, average training loss: 7063.92, base loss: 8512.02
[INFO 2017-06-26 17:37:50,400 main.py:47] epoch 1438, training loss: 6967.34, average training loss: 7063.53, base loss: 8512.10
[INFO 2017-06-26 17:37:50,766 main.py:47] epoch 1439, training loss: 7156.79, average training loss: 7062.41, base loss: 8511.09
[INFO 2017-06-26 17:37:51,121 main.py:47] epoch 1440, training loss: 6964.46, average training loss: 7062.86, base loss: 8511.44
[INFO 2017-06-26 17:37:51,475 main.py:47] epoch 1441, training loss: 6610.23, average training loss: 7062.12, base loss: 8510.88
[INFO 2017-06-26 17:37:51,834 main.py:47] epoch 1442, training loss: 6868.35, average training loss: 7061.20, base loss: 8510.53
[INFO 2017-06-26 17:37:52,192 main.py:47] epoch 1443, training loss: 7061.27, average training loss: 7060.69, base loss: 8510.35
[INFO 2017-06-26 17:37:52,544 main.py:47] epoch 1444, training loss: 7700.22, average training loss: 7061.38, base loss: 8511.74
[INFO 2017-06-26 17:37:52,897 main.py:47] epoch 1445, training loss: 6653.52, average training loss: 7059.62, base loss: 8510.46
[INFO 2017-06-26 17:37:53,248 main.py:47] epoch 1446, training loss: 6314.66, average training loss: 7059.06, base loss: 8510.12
[INFO 2017-06-26 17:37:53,600 main.py:47] epoch 1447, training loss: 7688.34, average training loss: 7059.62, base loss: 8511.23
[INFO 2017-06-26 17:37:53,985 main.py:47] epoch 1448, training loss: 5909.59, average training loss: 7057.33, base loss: 8508.67
[INFO 2017-06-26 17:37:54,354 main.py:47] epoch 1449, training loss: 6330.84, average training loss: 7056.98, base loss: 8508.46
[INFO 2017-06-26 17:37:54,709 main.py:47] epoch 1450, training loss: 7282.62, average training loss: 7057.41, base loss: 8509.25
[INFO 2017-06-26 17:37:55,060 main.py:47] epoch 1451, training loss: 6402.34, average training loss: 7057.75, base loss: 8510.14
[INFO 2017-06-26 17:37:55,412 main.py:47] epoch 1452, training loss: 6228.34, average training loss: 7058.05, base loss: 8510.82
[INFO 2017-06-26 17:37:55,763 main.py:47] epoch 1453, training loss: 6646.40, average training loss: 7056.51, base loss: 8509.89
[INFO 2017-06-26 17:37:56,114 main.py:47] epoch 1454, training loss: 6692.55, average training loss: 7056.18, base loss: 8510.23
[INFO 2017-06-26 17:37:56,465 main.py:47] epoch 1455, training loss: 6707.36, average training loss: 7055.41, base loss: 8510.00
[INFO 2017-06-26 17:37:56,830 main.py:47] epoch 1456, training loss: 6731.17, average training loss: 7054.97, base loss: 8510.13
[INFO 2017-06-26 17:37:57,184 main.py:47] epoch 1457, training loss: 5925.02, average training loss: 7053.11, base loss: 8508.35
[INFO 2017-06-26 17:37:57,537 main.py:47] epoch 1458, training loss: 7199.70, average training loss: 7052.66, base loss: 8509.21
[INFO 2017-06-26 17:37:57,889 main.py:47] epoch 1459, training loss: 6772.93, average training loss: 7052.04, base loss: 8508.80
[INFO 2017-06-26 17:37:58,242 main.py:47] epoch 1460, training loss: 7104.22, average training loss: 7052.70, base loss: 8510.20
[INFO 2017-06-26 17:37:58,594 main.py:47] epoch 1461, training loss: 6706.88, average training loss: 7052.44, base loss: 8510.38
[INFO 2017-06-26 17:37:58,948 main.py:47] epoch 1462, training loss: 7321.15, average training loss: 7052.01, base loss: 8510.90
[INFO 2017-06-26 17:37:59,300 main.py:47] epoch 1463, training loss: 6217.70, average training loss: 7050.30, base loss: 8509.21
[INFO 2017-06-26 17:37:59,654 main.py:47] epoch 1464, training loss: 6450.10, average training loss: 7049.87, base loss: 8509.16
[INFO 2017-06-26 17:38:00,007 main.py:47] epoch 1465, training loss: 6614.84, average training loss: 7047.39, base loss: 8507.09
[INFO 2017-06-26 17:38:00,359 main.py:47] epoch 1466, training loss: 6207.63, average training loss: 7046.48, base loss: 8506.22
[INFO 2017-06-26 17:38:00,708 main.py:47] epoch 1467, training loss: 7640.49, average training loss: 7047.65, base loss: 8508.73
[INFO 2017-06-26 17:38:01,061 main.py:47] epoch 1468, training loss: 7002.18, average training loss: 7047.76, base loss: 8509.30
[INFO 2017-06-26 17:38:01,412 main.py:47] epoch 1469, training loss: 6970.90, average training loss: 7047.89, base loss: 8509.48
[INFO 2017-06-26 17:38:01,762 main.py:47] epoch 1470, training loss: 6363.51, average training loss: 7046.28, base loss: 8508.45
[INFO 2017-06-26 17:38:02,122 main.py:47] epoch 1471, training loss: 6804.20, average training loss: 7045.61, base loss: 8507.56
[INFO 2017-06-26 17:38:02,478 main.py:47] epoch 1472, training loss: 6515.66, average training loss: 7045.17, base loss: 8507.57
[INFO 2017-06-26 17:38:02,835 main.py:47] epoch 1473, training loss: 6501.42, average training loss: 7043.29, base loss: 8505.72
[INFO 2017-06-26 17:38:03,194 main.py:47] epoch 1474, training loss: 7296.76, average training loss: 7042.48, base loss: 8504.91
[INFO 2017-06-26 17:38:03,548 main.py:47] epoch 1475, training loss: 6721.63, average training loss: 7042.27, base loss: 8505.16
[INFO 2017-06-26 17:38:03,901 main.py:47] epoch 1476, training loss: 6170.21, average training loss: 7041.70, base loss: 8504.60
[INFO 2017-06-26 17:38:04,255 main.py:47] epoch 1477, training loss: 8626.22, average training loss: 7043.15, base loss: 8506.97
[INFO 2017-06-26 17:38:04,609 main.py:47] epoch 1478, training loss: 6214.81, average training loss: 7041.63, base loss: 8505.20
[INFO 2017-06-26 17:38:04,994 main.py:47] epoch 1479, training loss: 6899.30, average training loss: 7040.76, base loss: 8504.88
[INFO 2017-06-26 17:38:05,431 main.py:47] epoch 1480, training loss: 6804.88, average training loss: 7040.47, base loss: 8504.79
[INFO 2017-06-26 17:38:05,789 main.py:47] epoch 1481, training loss: 6943.55, average training loss: 7039.58, base loss: 8504.56
[INFO 2017-06-26 17:38:06,144 main.py:47] epoch 1482, training loss: 5973.50, average training loss: 7038.60, base loss: 8504.24
[INFO 2017-06-26 17:38:06,496 main.py:47] epoch 1483, training loss: 6947.85, average training loss: 7038.97, base loss: 8504.91
[INFO 2017-06-26 17:38:06,847 main.py:47] epoch 1484, training loss: 7105.64, average training loss: 7038.96, base loss: 8505.04
[INFO 2017-06-26 17:38:07,202 main.py:47] epoch 1485, training loss: 6855.89, average training loss: 7038.63, base loss: 8505.14
[INFO 2017-06-26 17:38:07,554 main.py:47] epoch 1486, training loss: 6676.00, average training loss: 7037.86, base loss: 8505.16
[INFO 2017-06-26 17:38:07,939 main.py:47] epoch 1487, training loss: 6457.19, average training loss: 7037.42, base loss: 8504.64
[INFO 2017-06-26 17:38:08,450 main.py:47] epoch 1488, training loss: 6989.72, average training loss: 7036.32, base loss: 8503.53
[INFO 2017-06-26 17:38:08,812 main.py:47] epoch 1489, training loss: 6561.26, average training loss: 7033.60, base loss: 8501.24
[INFO 2017-06-26 17:38:09,171 main.py:47] epoch 1490, training loss: 8096.95, average training loss: 7034.80, base loss: 8504.30
[INFO 2017-06-26 17:38:09,538 main.py:47] epoch 1491, training loss: 6646.04, average training loss: 7034.20, base loss: 8504.19
[INFO 2017-06-26 17:38:09,894 main.py:47] epoch 1492, training loss: 6446.92, average training loss: 7032.86, base loss: 8502.82
[INFO 2017-06-26 17:38:10,290 main.py:47] epoch 1493, training loss: 6117.89, average training loss: 7031.74, base loss: 8501.97
[INFO 2017-06-26 17:38:10,646 main.py:47] epoch 1494, training loss: 6560.38, average training loss: 7031.39, base loss: 8501.41
[INFO 2017-06-26 17:38:11,015 main.py:47] epoch 1495, training loss: 6929.81, average training loss: 7030.72, base loss: 8501.45
[INFO 2017-06-26 17:38:11,369 main.py:47] epoch 1496, training loss: 6549.83, average training loss: 7030.74, base loss: 8502.29
[INFO 2017-06-26 17:38:11,721 main.py:47] epoch 1497, training loss: 6710.38, average training loss: 7029.96, base loss: 8501.83
[INFO 2017-06-26 17:38:12,073 main.py:47] epoch 1498, training loss: 7172.65, average training loss: 7028.86, base loss: 8500.94
[INFO 2017-06-26 17:38:12,443 main.py:47] epoch 1499, training loss: 6470.53, average training loss: 7028.79, base loss: 8500.70
[INFO 2017-06-26 17:38:12,443 main.py:49] epoch 1499, testing
[INFO 2017-06-26 17:38:16,942 main.py:100] average testing loss: 6392.41, base loss: 7888.59
[INFO 2017-06-26 17:38:16,966 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 17:38:16,976 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:38:17,326 main.py:47] epoch 1500, training loss: 6840.41, average training loss: 7028.55, base loss: 8500.76
[INFO 2017-06-26 17:38:17,682 main.py:47] epoch 1501, training loss: 7066.17, average training loss: 7028.07, base loss: 8500.80
[INFO 2017-06-26 17:38:18,065 main.py:47] epoch 1502, training loss: 8403.72, average training loss: 7028.71, base loss: 8502.02
[INFO 2017-06-26 17:38:18,435 main.py:47] epoch 1503, training loss: 6642.28, average training loss: 7028.48, base loss: 8502.20
[INFO 2017-06-26 17:38:18,790 main.py:47] epoch 1504, training loss: 6629.85, average training loss: 7028.65, base loss: 8502.54
[INFO 2017-06-26 17:38:19,158 main.py:47] epoch 1505, training loss: 6219.35, average training loss: 7028.11, base loss: 8502.23
[INFO 2017-06-26 17:38:19,546 main.py:47] epoch 1506, training loss: 5966.62, average training loss: 7026.25, base loss: 8500.71
[INFO 2017-06-26 17:38:19,939 main.py:47] epoch 1507, training loss: 6557.94, average training loss: 7026.46, base loss: 8501.41
[INFO 2017-06-26 17:38:20,299 main.py:47] epoch 1508, training loss: 6909.43, average training loss: 7026.15, base loss: 8501.23
[INFO 2017-06-26 17:38:20,654 main.py:47] epoch 1509, training loss: 7463.45, average training loss: 7026.50, base loss: 8502.30
[INFO 2017-06-26 17:38:21,016 main.py:47] epoch 1510, training loss: 7050.48, average training loss: 7025.95, base loss: 8501.80
[INFO 2017-06-26 17:38:21,370 main.py:47] epoch 1511, training loss: 7572.00, average training loss: 7025.19, base loss: 8501.26
[INFO 2017-06-26 17:38:21,720 main.py:47] epoch 1512, training loss: 6803.04, average training loss: 7024.23, base loss: 8501.20
[INFO 2017-06-26 17:38:22,073 main.py:47] epoch 1513, training loss: 6707.76, average training loss: 7023.94, base loss: 8501.12
[INFO 2017-06-26 17:38:22,436 main.py:47] epoch 1514, training loss: 6827.77, average training loss: 7021.70, base loss: 8498.21
[INFO 2017-06-26 17:38:22,786 main.py:47] epoch 1515, training loss: 7142.65, average training loss: 7021.37, base loss: 8498.30
[INFO 2017-06-26 17:38:23,138 main.py:47] epoch 1516, training loss: 5920.44, average training loss: 7020.09, base loss: 8496.75
[INFO 2017-06-26 17:38:23,491 main.py:47] epoch 1517, training loss: 6352.05, average training loss: 7018.12, base loss: 8494.53
[INFO 2017-06-26 17:38:23,844 main.py:47] epoch 1518, training loss: 6036.43, average training loss: 7017.06, base loss: 8493.64
[INFO 2017-06-26 17:38:24,195 main.py:47] epoch 1519, training loss: 6295.47, average training loss: 7016.50, base loss: 8493.56
[INFO 2017-06-26 17:38:24,558 main.py:47] epoch 1520, training loss: 6373.30, average training loss: 7015.76, base loss: 8493.28
[INFO 2017-06-26 17:38:24,909 main.py:47] epoch 1521, training loss: 6613.19, average training loss: 7015.56, base loss: 8493.44
[INFO 2017-06-26 17:38:25,264 main.py:47] epoch 1522, training loss: 6296.39, average training loss: 7014.09, base loss: 8492.14
[INFO 2017-06-26 17:38:25,635 main.py:47] epoch 1523, training loss: 7567.97, average training loss: 7014.30, base loss: 8492.92
[INFO 2017-06-26 17:38:25,987 main.py:47] epoch 1524, training loss: 7358.12, average training loss: 7013.99, base loss: 8492.78
[INFO 2017-06-26 17:38:26,340 main.py:47] epoch 1525, training loss: 7061.93, average training loss: 7014.91, base loss: 8494.05
[INFO 2017-06-26 17:38:26,695 main.py:47] epoch 1526, training loss: 7830.75, average training loss: 7016.58, base loss: 8496.43
[INFO 2017-06-26 17:38:27,049 main.py:47] epoch 1527, training loss: 6813.35, average training loss: 7016.48, base loss: 8497.09
[INFO 2017-06-26 17:38:27,400 main.py:47] epoch 1528, training loss: 7463.54, average training loss: 7017.10, base loss: 8498.19
[INFO 2017-06-26 17:38:27,762 main.py:47] epoch 1529, training loss: 6586.63, average training loss: 7016.91, base loss: 8498.05
[INFO 2017-06-26 17:38:28,115 main.py:47] epoch 1530, training loss: 6427.34, average training loss: 7016.55, base loss: 8497.68
[INFO 2017-06-26 17:38:28,469 main.py:47] epoch 1531, training loss: 6129.75, average training loss: 7015.06, base loss: 8495.50
[INFO 2017-06-26 17:38:28,820 main.py:47] epoch 1532, training loss: 6824.87, average training loss: 7014.88, base loss: 8495.22
[INFO 2017-06-26 17:38:29,173 main.py:47] epoch 1533, training loss: 7130.12, average training loss: 7015.75, base loss: 8497.02
[INFO 2017-06-26 17:38:29,525 main.py:47] epoch 1534, training loss: 8253.00, average training loss: 7017.04, base loss: 8498.73
[INFO 2017-06-26 17:38:29,890 main.py:47] epoch 1535, training loss: 8296.93, average training loss: 7018.80, base loss: 8501.73
[INFO 2017-06-26 17:38:30,267 main.py:47] epoch 1536, training loss: 6899.84, average training loss: 7018.89, base loss: 8502.36
[INFO 2017-06-26 17:38:30,652 main.py:47] epoch 1537, training loss: 6817.75, average training loss: 7019.33, base loss: 8502.91
[INFO 2017-06-26 17:38:31,032 main.py:47] epoch 1538, training loss: 7279.64, average training loss: 7019.19, base loss: 8502.53
[INFO 2017-06-26 17:38:31,408 main.py:47] epoch 1539, training loss: 8073.45, average training loss: 7020.55, base loss: 8505.09
[INFO 2017-06-26 17:38:31,768 main.py:47] epoch 1540, training loss: 6344.18, average training loss: 7019.56, base loss: 8503.88
[INFO 2017-06-26 17:38:32,153 main.py:47] epoch 1541, training loss: 6657.41, average training loss: 7019.15, base loss: 8504.52
[INFO 2017-06-26 17:38:32,508 main.py:47] epoch 1542, training loss: 8216.82, average training loss: 7018.11, base loss: 8503.95
[INFO 2017-06-26 17:38:32,860 main.py:47] epoch 1543, training loss: 6843.00, average training loss: 7017.48, base loss: 8503.68
[INFO 2017-06-26 17:38:33,217 main.py:47] epoch 1544, training loss: 6883.50, average training loss: 7017.67, base loss: 8503.91
[INFO 2017-06-26 17:38:33,568 main.py:47] epoch 1545, training loss: 6345.95, average training loss: 7016.55, base loss: 8502.84
[INFO 2017-06-26 17:38:33,921 main.py:47] epoch 1546, training loss: 6913.78, average training loss: 7016.58, base loss: 8503.48
[INFO 2017-06-26 17:38:34,272 main.py:47] epoch 1547, training loss: 7213.76, average training loss: 7016.97, base loss: 8504.99
[INFO 2017-06-26 17:38:34,622 main.py:47] epoch 1548, training loss: 7175.31, average training loss: 7016.40, base loss: 8505.09
[INFO 2017-06-26 17:38:34,975 main.py:47] epoch 1549, training loss: 6768.01, average training loss: 7016.05, base loss: 8504.42
[INFO 2017-06-26 17:38:35,326 main.py:47] epoch 1550, training loss: 6556.66, average training loss: 7016.14, base loss: 8504.75
[INFO 2017-06-26 17:38:35,679 main.py:47] epoch 1551, training loss: 6991.21, average training loss: 7015.65, base loss: 8504.62
[INFO 2017-06-26 17:38:36,031 main.py:47] epoch 1552, training loss: 7038.15, average training loss: 7016.23, base loss: 8505.54
[INFO 2017-06-26 17:38:36,386 main.py:47] epoch 1553, training loss: 6424.00, average training loss: 7014.93, base loss: 8504.26
[INFO 2017-06-26 17:38:36,735 main.py:47] epoch 1554, training loss: 7857.60, average training loss: 7016.01, base loss: 8506.44
[INFO 2017-06-26 17:38:37,087 main.py:47] epoch 1555, training loss: 8152.24, average training loss: 7016.58, base loss: 8507.38
[INFO 2017-06-26 17:38:37,438 main.py:47] epoch 1556, training loss: 6811.45, average training loss: 7016.99, base loss: 8508.29
[INFO 2017-06-26 17:38:37,789 main.py:47] epoch 1557, training loss: 6798.69, average training loss: 7015.71, base loss: 8506.99
[INFO 2017-06-26 17:38:38,141 main.py:47] epoch 1558, training loss: 6460.79, average training loss: 7015.43, base loss: 8507.26
[INFO 2017-06-26 17:38:38,495 main.py:47] epoch 1559, training loss: 6984.83, average training loss: 7015.13, base loss: 8507.35
[INFO 2017-06-26 17:38:38,880 main.py:47] epoch 1560, training loss: 6013.90, average training loss: 7014.42, base loss: 8506.97
[INFO 2017-06-26 17:38:39,237 main.py:47] epoch 1561, training loss: 6756.87, average training loss: 7014.60, base loss: 8507.84
[INFO 2017-06-26 17:38:39,599 main.py:47] epoch 1562, training loss: 6744.51, average training loss: 7013.49, base loss: 8506.48
[INFO 2017-06-26 17:38:39,963 main.py:47] epoch 1563, training loss: 5857.66, average training loss: 7011.24, base loss: 8503.08
[INFO 2017-06-26 17:38:40,317 main.py:47] epoch 1564, training loss: 6496.71, average training loss: 7011.39, base loss: 8503.11
[INFO 2017-06-26 17:38:40,669 main.py:47] epoch 1565, training loss: 6651.52, average training loss: 7011.18, base loss: 8503.43
[INFO 2017-06-26 17:38:41,023 main.py:47] epoch 1566, training loss: 5957.34, average training loss: 7009.76, base loss: 8502.26
[INFO 2017-06-26 17:38:41,375 main.py:47] epoch 1567, training loss: 6537.69, average training loss: 7009.20, base loss: 8502.22
[INFO 2017-06-26 17:38:41,726 main.py:47] epoch 1568, training loss: 7531.01, average training loss: 7010.11, base loss: 8503.98
[INFO 2017-06-26 17:38:42,080 main.py:47] epoch 1569, training loss: 6847.72, average training loss: 7008.69, base loss: 8502.93
[INFO 2017-06-26 17:38:42,439 main.py:47] epoch 1570, training loss: 6238.35, average training loss: 7007.94, base loss: 8502.43
[INFO 2017-06-26 17:38:42,797 main.py:47] epoch 1571, training loss: 5486.81, average training loss: 7005.28, base loss: 8499.43
[INFO 2017-06-26 17:38:43,150 main.py:47] epoch 1572, training loss: 7107.62, average training loss: 7004.93, base loss: 8499.11
[INFO 2017-06-26 17:38:43,515 main.py:47] epoch 1573, training loss: 7188.68, average training loss: 7005.89, base loss: 8500.74
[INFO 2017-06-26 17:38:43,873 main.py:47] epoch 1574, training loss: 7153.02, average training loss: 7005.87, base loss: 8500.96
[INFO 2017-06-26 17:38:44,228 main.py:47] epoch 1575, training loss: 7198.58, average training loss: 7006.29, base loss: 8501.71
[INFO 2017-06-26 17:38:44,580 main.py:47] epoch 1576, training loss: 6458.33, average training loss: 7005.82, base loss: 8501.21
[INFO 2017-06-26 17:38:44,964 main.py:47] epoch 1577, training loss: 6401.91, average training loss: 7003.89, base loss: 8498.83
[INFO 2017-06-26 17:38:45,359 main.py:47] epoch 1578, training loss: 7634.49, average training loss: 7004.51, base loss: 8499.27
[INFO 2017-06-26 17:38:45,715 main.py:47] epoch 1579, training loss: 6398.15, average training loss: 7003.72, base loss: 8498.35
[INFO 2017-06-26 17:38:46,070 main.py:47] epoch 1580, training loss: 6483.44, average training loss: 7003.08, base loss: 8498.16
[INFO 2017-06-26 17:38:46,424 main.py:47] epoch 1581, training loss: 6020.77, average training loss: 7001.67, base loss: 8496.71
[INFO 2017-06-26 17:38:46,777 main.py:47] epoch 1582, training loss: 8038.37, average training loss: 7001.73, base loss: 8496.73
[INFO 2017-06-26 17:38:47,134 main.py:47] epoch 1583, training loss: 6515.41, average training loss: 7000.23, base loss: 8495.18
[INFO 2017-06-26 17:38:47,484 main.py:47] epoch 1584, training loss: 6001.59, average training loss: 6998.60, base loss: 8493.67
[INFO 2017-06-26 17:38:47,836 main.py:47] epoch 1585, training loss: 6827.74, average training loss: 6998.25, base loss: 8493.37
[INFO 2017-06-26 17:38:48,187 main.py:47] epoch 1586, training loss: 7256.53, average training loss: 6997.57, base loss: 8492.60
[INFO 2017-06-26 17:38:48,556 main.py:47] epoch 1587, training loss: 6516.03, average training loss: 6996.95, base loss: 8492.04
[INFO 2017-06-26 17:38:48,910 main.py:47] epoch 1588, training loss: 6759.12, average training loss: 6995.87, base loss: 8491.29
[INFO 2017-06-26 17:38:49,261 main.py:47] epoch 1589, training loss: 6877.94, average training loss: 6996.48, base loss: 8492.45
[INFO 2017-06-26 17:38:49,610 main.py:47] epoch 1590, training loss: 7342.28, average training loss: 6997.50, base loss: 8494.52
[INFO 2017-06-26 17:38:49,995 main.py:47] epoch 1591, training loss: 6931.37, average training loss: 6997.36, base loss: 8495.22
[INFO 2017-06-26 17:38:50,392 main.py:47] epoch 1592, training loss: 6318.59, average training loss: 6995.59, base loss: 8493.53
[INFO 2017-06-26 17:38:50,755 main.py:47] epoch 1593, training loss: 6761.03, average training loss: 6995.20, base loss: 8492.99
[INFO 2017-06-26 17:38:51,110 main.py:47] epoch 1594, training loss: 7774.88, average training loss: 6996.42, base loss: 8495.36
[INFO 2017-06-26 17:38:51,461 main.py:47] epoch 1595, training loss: 6851.25, average training loss: 6994.54, base loss: 8494.49
[INFO 2017-06-26 17:38:51,815 main.py:47] epoch 1596, training loss: 6020.75, average training loss: 6993.45, base loss: 8493.69
[INFO 2017-06-26 17:38:52,199 main.py:47] epoch 1597, training loss: 6827.81, average training loss: 6993.24, base loss: 8493.41
[INFO 2017-06-26 17:38:52,568 main.py:47] epoch 1598, training loss: 6813.28, average training loss: 6993.70, base loss: 8494.80
[INFO 2017-06-26 17:38:52,920 main.py:47] epoch 1599, training loss: 6379.40, average training loss: 6993.44, base loss: 8494.77
[INFO 2017-06-26 17:38:52,921 main.py:49] epoch 1599, testing
[INFO 2017-06-26 17:38:57,077 main.py:100] average testing loss: 7024.04, base loss: 8744.59
[INFO 2017-06-26 17:38:57,102 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:38:57,453 main.py:47] epoch 1600, training loss: 6563.21, average training loss: 6993.66, base loss: 8495.62
[INFO 2017-06-26 17:38:57,804 main.py:47] epoch 1601, training loss: 7149.59, average training loss: 6993.45, base loss: 8495.94
[INFO 2017-06-26 17:38:58,192 main.py:47] epoch 1602, training loss: 6802.10, average training loss: 6992.56, base loss: 8495.66
[INFO 2017-06-26 17:38:58,593 main.py:47] epoch 1603, training loss: 7231.09, average training loss: 6992.94, base loss: 8496.57
[INFO 2017-06-26 17:38:58,960 main.py:47] epoch 1604, training loss: 6820.50, average training loss: 6991.99, base loss: 8496.12
[INFO 2017-06-26 17:38:59,317 main.py:47] epoch 1605, training loss: 7064.14, average training loss: 6991.75, base loss: 8496.09
[INFO 2017-06-26 17:38:59,667 main.py:47] epoch 1606, training loss: 6857.26, average training loss: 6990.54, base loss: 8494.39
[INFO 2017-06-26 17:39:00,018 main.py:47] epoch 1607, training loss: 7177.31, average training loss: 6990.69, base loss: 8494.87
[INFO 2017-06-26 17:39:00,369 main.py:47] epoch 1608, training loss: 6542.57, average training loss: 6990.20, base loss: 8494.24
[INFO 2017-06-26 17:39:00,768 main.py:47] epoch 1609, training loss: 6285.28, average training loss: 6988.29, base loss: 8491.92
[INFO 2017-06-26 17:39:01,123 main.py:47] epoch 1610, training loss: 7247.75, average training loss: 6986.77, base loss: 8490.56
[INFO 2017-06-26 17:39:01,475 main.py:47] epoch 1611, training loss: 7080.50, average training loss: 6986.21, base loss: 8490.15
[INFO 2017-06-26 17:39:01,827 main.py:47] epoch 1612, training loss: 6081.90, average training loss: 6984.63, base loss: 8488.61
[INFO 2017-06-26 17:39:02,178 main.py:47] epoch 1613, training loss: 6438.29, average training loss: 6983.03, base loss: 8485.80
[INFO 2017-06-26 17:39:02,530 main.py:47] epoch 1614, training loss: 5987.25, average training loss: 6981.39, base loss: 8483.66
[INFO 2017-06-26 17:39:02,883 main.py:47] epoch 1615, training loss: 7363.40, average training loss: 6980.39, base loss: 8483.05
[INFO 2017-06-26 17:39:03,236 main.py:47] epoch 1616, training loss: 6824.25, average training loss: 6980.64, base loss: 8483.43
[INFO 2017-06-26 17:39:03,588 main.py:47] epoch 1617, training loss: 6853.76, average training loss: 6980.79, base loss: 8484.27
[INFO 2017-06-26 17:39:03,939 main.py:47] epoch 1618, training loss: 6636.14, average training loss: 6980.96, base loss: 8484.58
[INFO 2017-06-26 17:39:04,304 main.py:47] epoch 1619, training loss: 6217.87, average training loss: 6980.62, base loss: 8484.65
[INFO 2017-06-26 17:39:04,657 main.py:47] epoch 1620, training loss: 7004.09, average training loss: 6980.84, base loss: 8485.25
[INFO 2017-06-26 17:39:05,009 main.py:47] epoch 1621, training loss: 7842.08, average training loss: 6981.18, base loss: 8486.17
[INFO 2017-06-26 17:39:05,361 main.py:47] epoch 1622, training loss: 6622.58, average training loss: 6979.11, base loss: 8484.00
[INFO 2017-06-26 17:39:05,712 main.py:47] epoch 1623, training loss: 6202.83, average training loss: 6979.39, base loss: 8484.56
[INFO 2017-06-26 17:39:06,064 main.py:47] epoch 1624, training loss: 6512.13, average training loss: 6978.96, base loss: 8484.94
[INFO 2017-06-26 17:39:06,416 main.py:47] epoch 1625, training loss: 5997.73, average training loss: 6977.50, base loss: 8483.27
[INFO 2017-06-26 17:39:06,769 main.py:47] epoch 1626, training loss: 7208.47, average training loss: 6977.98, base loss: 8484.55
[INFO 2017-06-26 17:39:07,120 main.py:47] epoch 1627, training loss: 8326.02, average training loss: 6978.86, base loss: 8485.70
[INFO 2017-06-26 17:39:07,471 main.py:47] epoch 1628, training loss: 7416.02, average training loss: 6978.84, base loss: 8486.35
[INFO 2017-06-26 17:39:07,823 main.py:47] epoch 1629, training loss: 6875.49, average training loss: 6978.98, base loss: 8487.42
[INFO 2017-06-26 17:39:08,174 main.py:47] epoch 1630, training loss: 6650.32, average training loss: 6979.15, base loss: 8488.65
[INFO 2017-06-26 17:39:08,533 main.py:47] epoch 1631, training loss: 6763.67, average training loss: 6979.04, base loss: 8488.83
[INFO 2017-06-26 17:39:08,888 main.py:47] epoch 1632, training loss: 7033.82, average training loss: 6978.81, base loss: 8488.83
[INFO 2017-06-26 17:39:09,249 main.py:47] epoch 1633, training loss: 7026.43, average training loss: 6979.11, base loss: 8490.00
[INFO 2017-06-26 17:39:09,602 main.py:47] epoch 1634, training loss: 7817.36, average training loss: 6979.88, base loss: 8491.58
[INFO 2017-06-26 17:39:09,952 main.py:47] epoch 1635, training loss: 7360.33, average training loss: 6979.68, base loss: 8491.68
[INFO 2017-06-26 17:39:10,299 main.py:47] epoch 1636, training loss: 7136.94, average training loss: 6978.95, base loss: 8491.57
[INFO 2017-06-26 17:39:10,652 main.py:47] epoch 1637, training loss: 7239.45, average training loss: 6979.16, base loss: 8492.83
[INFO 2017-06-26 17:39:11,004 main.py:47] epoch 1638, training loss: 6152.13, average training loss: 6979.15, base loss: 8493.05
[INFO 2017-06-26 17:39:11,356 main.py:47] epoch 1639, training loss: 7902.55, average training loss: 6979.67, base loss: 8493.90
[INFO 2017-06-26 17:39:11,708 main.py:47] epoch 1640, training loss: 6388.35, average training loss: 6979.01, base loss: 8493.04
[INFO 2017-06-26 17:39:12,058 main.py:47] epoch 1641, training loss: 7471.74, average training loss: 6979.30, base loss: 8493.73
[INFO 2017-06-26 17:39:12,446 main.py:47] epoch 1642, training loss: 6682.61, average training loss: 6979.50, base loss: 8494.30
[INFO 2017-06-26 17:39:12,810 main.py:47] epoch 1643, training loss: 6274.54, average training loss: 6977.16, base loss: 8492.39
[INFO 2017-06-26 17:39:13,179 main.py:47] epoch 1644, training loss: 8194.03, average training loss: 6978.21, base loss: 8494.48
[INFO 2017-06-26 17:39:13,535 main.py:47] epoch 1645, training loss: 8428.61, average training loss: 6980.36, base loss: 8497.59
[INFO 2017-06-26 17:39:13,885 main.py:47] epoch 1646, training loss: 6102.46, average training loss: 6978.25, base loss: 8495.67
[INFO 2017-06-26 17:39:14,256 main.py:47] epoch 1647, training loss: 5850.10, average training loss: 6976.22, base loss: 8493.88
[INFO 2017-06-26 17:39:14,642 main.py:47] epoch 1648, training loss: 7109.42, average training loss: 6975.82, base loss: 8493.87
[INFO 2017-06-26 17:39:15,026 main.py:47] epoch 1649, training loss: 6461.19, average training loss: 6974.91, base loss: 8493.79
[INFO 2017-06-26 17:39:15,394 main.py:47] epoch 1650, training loss: 6366.78, average training loss: 6973.76, base loss: 8492.53
[INFO 2017-06-26 17:39:15,776 main.py:47] epoch 1651, training loss: 6861.87, average training loss: 6973.88, base loss: 8493.00
[INFO 2017-06-26 17:39:16,146 main.py:47] epoch 1652, training loss: 5864.31, average training loss: 6971.91, base loss: 8490.47
[INFO 2017-06-26 17:39:16,500 main.py:47] epoch 1653, training loss: 6402.20, average training loss: 6971.42, base loss: 8489.93
[INFO 2017-06-26 17:39:16,851 main.py:47] epoch 1654, training loss: 7368.32, average training loss: 6971.90, base loss: 8491.13
[INFO 2017-06-26 17:39:17,201 main.py:47] epoch 1655, training loss: 6850.66, average training loss: 6971.73, base loss: 8491.06
[INFO 2017-06-26 17:39:17,553 main.py:47] epoch 1656, training loss: 6990.49, average training loss: 6970.48, base loss: 8490.31
[INFO 2017-06-26 17:39:17,904 main.py:47] epoch 1657, training loss: 7618.53, average training loss: 6971.25, base loss: 8491.43
[INFO 2017-06-26 17:39:18,255 main.py:47] epoch 1658, training loss: 6855.44, average training loss: 6970.98, base loss: 8491.64
[INFO 2017-06-26 17:39:18,607 main.py:47] epoch 1659, training loss: 7601.29, average training loss: 6971.16, base loss: 8492.18
[INFO 2017-06-26 17:39:18,961 main.py:47] epoch 1660, training loss: 6928.54, average training loss: 6970.81, base loss: 8492.83
[INFO 2017-06-26 17:39:19,320 main.py:47] epoch 1661, training loss: 6746.07, average training loss: 6971.54, base loss: 8494.16
[INFO 2017-06-26 17:39:19,670 main.py:47] epoch 1662, training loss: 6709.23, average training loss: 6971.03, base loss: 8494.74
[INFO 2017-06-26 17:39:20,023 main.py:47] epoch 1663, training loss: 7034.24, average training loss: 6971.23, base loss: 8495.68
[INFO 2017-06-26 17:39:20,374 main.py:47] epoch 1664, training loss: 6106.95, average training loss: 6971.91, base loss: 8496.59
[INFO 2017-06-26 17:39:20,724 main.py:47] epoch 1665, training loss: 6386.53, average training loss: 6971.10, base loss: 8495.33
[INFO 2017-06-26 17:39:21,073 main.py:47] epoch 1666, training loss: 7299.32, average training loss: 6970.80, base loss: 8495.27
[INFO 2017-06-26 17:39:21,459 main.py:47] epoch 1667, training loss: 6447.67, average training loss: 6970.79, base loss: 8495.23
[INFO 2017-06-26 17:39:21,841 main.py:47] epoch 1668, training loss: 7508.74, average training loss: 6970.99, base loss: 8495.99
[INFO 2017-06-26 17:39:22,193 main.py:47] epoch 1669, training loss: 6519.13, average training loss: 6970.28, base loss: 8495.20
[INFO 2017-06-26 17:39:22,546 main.py:47] epoch 1670, training loss: 7839.86, average training loss: 6970.95, base loss: 8496.55
[INFO 2017-06-26 17:39:22,897 main.py:47] epoch 1671, training loss: 6386.65, average training loss: 6970.63, base loss: 8496.65
[INFO 2017-06-26 17:39:23,249 main.py:47] epoch 1672, training loss: 6814.04, average training loss: 6970.88, base loss: 8497.38
[INFO 2017-06-26 17:39:23,602 main.py:47] epoch 1673, training loss: 6715.66, average training loss: 6971.14, base loss: 8497.92
[INFO 2017-06-26 17:39:23,955 main.py:47] epoch 1674, training loss: 7518.16, average training loss: 6970.59, base loss: 8497.93
[INFO 2017-06-26 17:39:24,323 main.py:47] epoch 1675, training loss: 6701.61, average training loss: 6970.57, base loss: 8498.14
[INFO 2017-06-26 17:39:24,675 main.py:47] epoch 1676, training loss: 7449.19, average training loss: 6971.89, base loss: 8500.29
[INFO 2017-06-26 17:39:25,032 main.py:47] epoch 1677, training loss: 6819.38, average training loss: 6971.94, base loss: 8500.93
[INFO 2017-06-26 17:39:25,390 main.py:47] epoch 1678, training loss: 6842.50, average training loss: 6970.54, base loss: 8499.42
[INFO 2017-06-26 17:39:25,774 main.py:47] epoch 1679, training loss: 7228.31, average training loss: 6970.52, base loss: 8500.18
[INFO 2017-06-26 17:39:26,159 main.py:47] epoch 1680, training loss: 6386.80, average training loss: 6970.02, base loss: 8499.30
[INFO 2017-06-26 17:39:26,515 main.py:47] epoch 1681, training loss: 6944.08, average training loss: 6968.40, base loss: 8498.04
[INFO 2017-06-26 17:39:26,903 main.py:47] epoch 1682, training loss: 7055.71, average training loss: 6967.98, base loss: 8498.07
[INFO 2017-06-26 17:39:27,266 main.py:47] epoch 1683, training loss: 6226.85, average training loss: 6967.70, base loss: 8497.97
[INFO 2017-06-26 17:39:27,633 main.py:47] epoch 1684, training loss: 6172.08, average training loss: 6966.78, base loss: 8496.70
[INFO 2017-06-26 17:39:27,996 main.py:47] epoch 1685, training loss: 7724.99, average training loss: 6967.88, base loss: 8497.72
[INFO 2017-06-26 17:39:28,348 main.py:47] epoch 1686, training loss: 7266.18, average training loss: 6967.52, base loss: 8497.36
[INFO 2017-06-26 17:39:28,701 main.py:47] epoch 1687, training loss: 7783.75, average training loss: 6967.98, base loss: 8498.45
[INFO 2017-06-26 17:39:29,051 main.py:47] epoch 1688, training loss: 7593.04, average training loss: 6968.71, base loss: 8499.32
[INFO 2017-06-26 17:39:29,412 main.py:47] epoch 1689, training loss: 6727.23, average training loss: 6968.22, base loss: 8499.00
[INFO 2017-06-26 17:39:29,765 main.py:47] epoch 1690, training loss: 6525.90, average training loss: 6966.42, base loss: 8496.64
[INFO 2017-06-26 17:39:30,134 main.py:47] epoch 1691, training loss: 6534.23, average training loss: 6965.62, base loss: 8496.10
[INFO 2017-06-26 17:39:30,486 main.py:47] epoch 1692, training loss: 6687.41, average training loss: 6965.73, base loss: 8496.31
[INFO 2017-06-26 17:39:30,870 main.py:47] epoch 1693, training loss: 6914.71, average training loss: 6965.02, base loss: 8495.70
[INFO 2017-06-26 17:39:31,267 main.py:47] epoch 1694, training loss: 6205.26, average training loss: 6963.47, base loss: 8493.94
[INFO 2017-06-26 17:39:31,619 main.py:47] epoch 1695, training loss: 7074.30, average training loss: 6962.66, base loss: 8493.58
[INFO 2017-06-26 17:39:32,007 main.py:47] epoch 1696, training loss: 7375.60, average training loss: 6962.55, base loss: 8494.32
[INFO 2017-06-26 17:39:32,362 main.py:47] epoch 1697, training loss: 6979.57, average training loss: 6962.58, base loss: 8495.00
[INFO 2017-06-26 17:39:32,726 main.py:47] epoch 1698, training loss: 7086.21, average training loss: 6961.62, base loss: 8494.28
[INFO 2017-06-26 17:39:33,080 main.py:47] epoch 1699, training loss: 7402.63, average training loss: 6962.25, base loss: 8495.73
[INFO 2017-06-26 17:39:33,080 main.py:49] epoch 1699, testing
[INFO 2017-06-26 17:39:37,299 main.py:100] average testing loss: 6682.91, base loss: 8317.23
[INFO 2017-06-26 17:39:37,322 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:39:37,676 main.py:47] epoch 1700, training loss: 7779.97, average training loss: 6963.87, base loss: 8498.11
[INFO 2017-06-26 17:39:38,060 main.py:47] epoch 1701, training loss: 7188.39, average training loss: 6963.64, base loss: 8498.60
[INFO 2017-06-26 17:39:38,548 main.py:47] epoch 1702, training loss: 7570.19, average training loss: 6964.76, base loss: 8501.41
[INFO 2017-06-26 17:39:38,905 main.py:47] epoch 1703, training loss: 6811.39, average training loss: 6963.20, base loss: 8500.07
[INFO 2017-06-26 17:39:39,257 main.py:47] epoch 1704, training loss: 6018.68, average training loss: 6962.09, base loss: 8499.66
[INFO 2017-06-26 17:39:39,610 main.py:47] epoch 1705, training loss: 6329.80, average training loss: 6961.97, base loss: 8499.33
[INFO 2017-06-26 17:39:39,965 main.py:47] epoch 1706, training loss: 6556.12, average training loss: 6961.15, base loss: 8498.27
[INFO 2017-06-26 17:39:40,320 main.py:47] epoch 1707, training loss: 6199.68, average training loss: 6960.37, base loss: 8497.56
[INFO 2017-06-26 17:39:40,673 main.py:47] epoch 1708, training loss: 6598.60, average training loss: 6960.35, base loss: 8497.91
[INFO 2017-06-26 17:39:41,024 main.py:47] epoch 1709, training loss: 7098.49, average training loss: 6960.43, base loss: 8498.04
[INFO 2017-06-26 17:39:41,406 main.py:47] epoch 1710, training loss: 6858.14, average training loss: 6959.44, base loss: 8497.24
[INFO 2017-06-26 17:39:41,773 main.py:47] epoch 1711, training loss: 6884.33, average training loss: 6959.41, base loss: 8496.95
[INFO 2017-06-26 17:39:42,137 main.py:47] epoch 1712, training loss: 6627.93, average training loss: 6958.89, base loss: 8496.96
[INFO 2017-06-26 17:39:42,529 main.py:47] epoch 1713, training loss: 6301.78, average training loss: 6958.54, base loss: 8496.18
[INFO 2017-06-26 17:39:42,885 main.py:47] epoch 1714, training loss: 6305.93, average training loss: 6957.78, base loss: 8496.27
[INFO 2017-06-26 17:39:43,250 main.py:47] epoch 1715, training loss: 6851.65, average training loss: 6957.74, base loss: 8497.24
[INFO 2017-06-26 17:39:43,610 main.py:47] epoch 1716, training loss: 6710.65, average training loss: 6957.36, base loss: 8497.19
[INFO 2017-06-26 17:39:43,978 main.py:47] epoch 1717, training loss: 7329.52, average training loss: 6957.07, base loss: 8497.33
[INFO 2017-06-26 17:39:44,335 main.py:47] epoch 1718, training loss: 7071.46, average training loss: 6957.57, base loss: 8499.07
[INFO 2017-06-26 17:39:44,689 main.py:47] epoch 1719, training loss: 6743.93, average training loss: 6956.63, base loss: 8497.64
[INFO 2017-06-26 17:39:45,040 main.py:47] epoch 1720, training loss: 7147.98, average training loss: 6956.92, base loss: 8498.57
[INFO 2017-06-26 17:39:45,391 main.py:47] epoch 1721, training loss: 6347.43, average training loss: 6955.93, base loss: 8497.72
[INFO 2017-06-26 17:39:45,750 main.py:47] epoch 1722, training loss: 6712.74, average training loss: 6954.48, base loss: 8496.15
[INFO 2017-06-26 17:39:46,102 main.py:47] epoch 1723, training loss: 6442.92, average training loss: 6954.23, base loss: 8495.83
[INFO 2017-06-26 17:39:46,454 main.py:47] epoch 1724, training loss: 7146.54, average training loss: 6955.28, base loss: 8497.81
[INFO 2017-06-26 17:39:46,804 main.py:47] epoch 1725, training loss: 6728.78, average training loss: 6954.55, base loss: 8497.16
[INFO 2017-06-26 17:39:47,153 main.py:47] epoch 1726, training loss: 7241.80, average training loss: 6954.37, base loss: 8497.64
[INFO 2017-06-26 17:39:47,503 main.py:47] epoch 1727, training loss: 7808.53, average training loss: 6953.51, base loss: 8496.34
[INFO 2017-06-26 17:39:47,853 main.py:47] epoch 1728, training loss: 6423.96, average training loss: 6952.07, base loss: 8494.42
[INFO 2017-06-26 17:39:48,203 main.py:47] epoch 1729, training loss: 7641.65, average training loss: 6952.47, base loss: 8495.82
[INFO 2017-06-26 17:39:48,554 main.py:47] epoch 1730, training loss: 6338.80, average training loss: 6951.37, base loss: 8494.50
[INFO 2017-06-26 17:39:48,905 main.py:47] epoch 1731, training loss: 7338.54, average training loss: 6952.17, base loss: 8496.15
[INFO 2017-06-26 17:39:49,255 main.py:47] epoch 1732, training loss: 8001.01, average training loss: 6953.43, base loss: 8498.25
[INFO 2017-06-26 17:39:49,607 main.py:47] epoch 1733, training loss: 7285.22, average training loss: 6954.05, base loss: 8500.23
[INFO 2017-06-26 17:39:49,957 main.py:47] epoch 1734, training loss: 7172.79, average training loss: 6955.03, base loss: 8501.68
[INFO 2017-06-26 17:39:50,306 main.py:47] epoch 1735, training loss: 6600.81, average training loss: 6954.31, base loss: 8501.64
[INFO 2017-06-26 17:39:50,672 main.py:47] epoch 1736, training loss: 7163.99, average training loss: 6953.71, base loss: 8501.52
[INFO 2017-06-26 17:39:51,027 main.py:47] epoch 1737, training loss: 6175.32, average training loss: 6953.03, base loss: 8501.38
[INFO 2017-06-26 17:39:51,380 main.py:47] epoch 1738, training loss: 7097.24, average training loss: 6953.53, base loss: 8502.93
[INFO 2017-06-26 17:39:51,731 main.py:47] epoch 1739, training loss: 6703.42, average training loss: 6953.61, base loss: 8503.52
[INFO 2017-06-26 17:39:52,083 main.py:47] epoch 1740, training loss: 6282.74, average training loss: 6953.02, base loss: 8503.67
[INFO 2017-06-26 17:39:52,434 main.py:47] epoch 1741, training loss: 6926.88, average training loss: 6952.56, base loss: 8503.53
[INFO 2017-06-26 17:39:52,819 main.py:47] epoch 1742, training loss: 7017.31, average training loss: 6950.18, base loss: 8501.55
[INFO 2017-06-26 17:39:53,209 main.py:47] epoch 1743, training loss: 6807.51, average training loss: 6950.05, base loss: 8501.81
[INFO 2017-06-26 17:39:53,578 main.py:47] epoch 1744, training loss: 6197.48, average training loss: 6950.04, base loss: 8502.64
[INFO 2017-06-26 17:39:53,938 main.py:47] epoch 1745, training loss: 6793.58, average training loss: 6950.33, base loss: 8503.66
[INFO 2017-06-26 17:39:54,320 main.py:47] epoch 1746, training loss: 7145.22, average training loss: 6951.04, base loss: 8505.10
[INFO 2017-06-26 17:39:54,695 main.py:47] epoch 1747, training loss: 6619.43, average training loss: 6951.04, base loss: 8505.76
[INFO 2017-06-26 17:39:55,050 main.py:47] epoch 1748, training loss: 6571.80, average training loss: 6950.59, base loss: 8505.83
[INFO 2017-06-26 17:39:55,406 main.py:47] epoch 1749, training loss: 7233.60, average training loss: 6950.15, base loss: 8505.77
[INFO 2017-06-26 17:39:55,773 main.py:47] epoch 1750, training loss: 7164.82, average training loss: 6950.11, base loss: 8506.14
[INFO 2017-06-26 17:39:56,127 main.py:47] epoch 1751, training loss: 7591.65, average training loss: 6949.39, base loss: 8505.46
[INFO 2017-06-26 17:39:56,487 main.py:47] epoch 1752, training loss: 6243.92, average training loss: 6949.11, base loss: 8506.14
[INFO 2017-06-26 17:39:56,836 main.py:47] epoch 1753, training loss: 6853.20, average training loss: 6949.02, base loss: 8506.81
[INFO 2017-06-26 17:39:57,195 main.py:47] epoch 1754, training loss: 6664.87, average training loss: 6949.07, base loss: 8507.35
[INFO 2017-06-26 17:39:57,548 main.py:47] epoch 1755, training loss: 7459.19, average training loss: 6949.39, base loss: 8507.90
[INFO 2017-06-26 17:39:57,900 main.py:47] epoch 1756, training loss: 7527.48, average training loss: 6950.32, base loss: 8509.85
[INFO 2017-06-26 17:39:58,251 main.py:47] epoch 1757, training loss: 6491.06, average training loss: 6949.45, base loss: 8508.48
[INFO 2017-06-26 17:39:58,639 main.py:47] epoch 1758, training loss: 6454.74, average training loss: 6948.72, base loss: 8507.53
[INFO 2017-06-26 17:39:59,037 main.py:47] epoch 1759, training loss: 6612.88, average training loss: 6948.28, base loss: 8507.04
[INFO 2017-06-26 17:39:59,410 main.py:47] epoch 1760, training loss: 6668.92, average training loss: 6947.83, base loss: 8507.32
[INFO 2017-06-26 17:39:59,765 main.py:47] epoch 1761, training loss: 6422.37, average training loss: 6948.12, base loss: 8507.70
[INFO 2017-06-26 17:40:00,118 main.py:47] epoch 1762, training loss: 6026.02, average training loss: 6948.13, base loss: 8508.42
[INFO 2017-06-26 17:40:00,467 main.py:47] epoch 1763, training loss: 6054.88, average training loss: 6947.49, base loss: 8507.78
[INFO 2017-06-26 17:40:00,813 main.py:47] epoch 1764, training loss: 6985.39, average training loss: 6946.80, base loss: 8507.34
[INFO 2017-06-26 17:40:01,167 main.py:47] epoch 1765, training loss: 6754.52, average training loss: 6947.40, base loss: 8509.11
[INFO 2017-06-26 17:40:01,518 main.py:47] epoch 1766, training loss: 5367.90, average training loss: 6946.54, base loss: 8508.12
[INFO 2017-06-26 17:40:01,871 main.py:47] epoch 1767, training loss: 6754.83, average training loss: 6946.13, base loss: 8507.89
[INFO 2017-06-26 17:40:02,224 main.py:47] epoch 1768, training loss: 7636.53, average training loss: 6946.98, base loss: 8509.78
[INFO 2017-06-26 17:40:02,577 main.py:47] epoch 1769, training loss: 6783.55, average training loss: 6946.34, base loss: 8508.91
[INFO 2017-06-26 17:40:02,927 main.py:47] epoch 1770, training loss: 6787.70, average training loss: 6946.43, base loss: 8509.60
[INFO 2017-06-26 17:40:03,278 main.py:47] epoch 1771, training loss: 6164.69, average training loss: 6945.56, base loss: 8508.52
[INFO 2017-06-26 17:40:03,628 main.py:47] epoch 1772, training loss: 6497.73, average training loss: 6945.37, base loss: 8508.49
[INFO 2017-06-26 17:40:03,977 main.py:47] epoch 1773, training loss: 6285.87, average training loss: 6943.48, base loss: 8506.08
[INFO 2017-06-26 17:40:04,328 main.py:47] epoch 1774, training loss: 7688.83, average training loss: 6944.42, base loss: 8508.15
[INFO 2017-06-26 17:40:04,677 main.py:47] epoch 1775, training loss: 7115.33, average training loss: 6944.30, base loss: 8508.50
[INFO 2017-06-26 17:40:05,031 main.py:47] epoch 1776, training loss: 6667.54, average training loss: 6944.12, base loss: 8508.64
[INFO 2017-06-26 17:40:05,382 main.py:47] epoch 1777, training loss: 6621.00, average training loss: 6943.92, base loss: 8508.34
[INFO 2017-06-26 17:40:05,731 main.py:47] epoch 1778, training loss: 7197.93, average training loss: 6943.13, base loss: 8507.81
[INFO 2017-06-26 17:40:06,096 main.py:47] epoch 1779, training loss: 7838.63, average training loss: 6943.74, base loss: 8508.62
[INFO 2017-06-26 17:40:06,449 main.py:47] epoch 1780, training loss: 7113.75, average training loss: 6943.51, base loss: 8509.10
[INFO 2017-06-26 17:40:06,799 main.py:47] epoch 1781, training loss: 6982.89, average training loss: 6942.05, base loss: 8507.89
[INFO 2017-06-26 17:40:07,154 main.py:47] epoch 1782, training loss: 5895.94, average training loss: 6940.18, base loss: 8505.35
[INFO 2017-06-26 17:40:07,508 main.py:47] epoch 1783, training loss: 7042.03, average training loss: 6940.45, base loss: 8506.21
[INFO 2017-06-26 17:40:07,860 main.py:47] epoch 1784, training loss: 6478.80, average training loss: 6939.79, base loss: 8505.73
[INFO 2017-06-26 17:40:08,212 main.py:47] epoch 1785, training loss: 6005.28, average training loss: 6938.62, base loss: 8504.33
[INFO 2017-06-26 17:40:08,561 main.py:47] epoch 1786, training loss: 7176.44, average training loss: 6938.65, base loss: 8505.04
[INFO 2017-06-26 17:40:08,911 main.py:47] epoch 1787, training loss: 6088.27, average training loss: 6937.12, base loss: 8503.85
[INFO 2017-06-26 17:40:09,262 main.py:47] epoch 1788, training loss: 7115.98, average training loss: 6936.90, base loss: 8504.31
[INFO 2017-06-26 17:40:09,612 main.py:47] epoch 1789, training loss: 7310.98, average training loss: 6935.97, base loss: 8503.92
[INFO 2017-06-26 17:40:09,964 main.py:47] epoch 1790, training loss: 7519.29, average training loss: 6936.11, base loss: 8505.40
[INFO 2017-06-26 17:40:10,316 main.py:47] epoch 1791, training loss: 6479.58, average training loss: 6935.47, base loss: 8505.45
[INFO 2017-06-26 17:40:10,668 main.py:47] epoch 1792, training loss: 6510.51, average training loss: 6934.89, base loss: 8505.35
[INFO 2017-06-26 17:40:11,022 main.py:47] epoch 1793, training loss: 6299.31, average training loss: 6934.49, base loss: 8504.20
[INFO 2017-06-26 17:40:11,388 main.py:47] epoch 1794, training loss: 6541.51, average training loss: 6934.07, base loss: 8503.67
[INFO 2017-06-26 17:40:11,746 main.py:47] epoch 1795, training loss: 6344.46, average training loss: 6932.40, base loss: 8501.49
[INFO 2017-06-26 17:40:12,101 main.py:47] epoch 1796, training loss: 6816.34, average training loss: 6932.53, base loss: 8502.06
[INFO 2017-06-26 17:40:12,454 main.py:47] epoch 1797, training loss: 7474.13, average training loss: 6932.82, base loss: 8502.99
[INFO 2017-06-26 17:40:12,806 main.py:47] epoch 1798, training loss: 7367.60, average training loss: 6932.35, base loss: 8502.50
[INFO 2017-06-26 17:40:13,160 main.py:47] epoch 1799, training loss: 6694.17, average training loss: 6932.28, base loss: 8502.30
[INFO 2017-06-26 17:40:13,160 main.py:49] epoch 1799, testing
[INFO 2017-06-26 17:40:17,350 main.py:100] average testing loss: 6695.66, base loss: 8372.70
[INFO 2017-06-26 17:40:17,374 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:40:17,725 main.py:47] epoch 1800, training loss: 6075.35, average training loss: 6930.84, base loss: 8499.90
[INFO 2017-06-26 17:40:18,078 main.py:47] epoch 1801, training loss: 7950.90, average training loss: 6932.12, base loss: 8501.83
[INFO 2017-06-26 17:40:18,426 main.py:47] epoch 1802, training loss: 7647.31, average training loss: 6933.16, base loss: 8503.73
[INFO 2017-06-26 17:40:18,778 main.py:47] epoch 1803, training loss: 6337.95, average training loss: 6932.32, base loss: 8502.85
[INFO 2017-06-26 17:40:19,130 main.py:47] epoch 1804, training loss: 6262.55, average training loss: 6931.53, base loss: 8501.55
[INFO 2017-06-26 17:40:19,482 main.py:47] epoch 1805, training loss: 6608.00, average training loss: 6930.59, base loss: 8501.14
[INFO 2017-06-26 17:40:19,831 main.py:47] epoch 1806, training loss: 7378.26, average training loss: 6931.06, base loss: 8502.49
[INFO 2017-06-26 17:40:20,182 main.py:47] epoch 1807, training loss: 6547.62, average training loss: 6931.08, base loss: 8502.45
[INFO 2017-06-26 17:40:20,532 main.py:47] epoch 1808, training loss: 6919.21, average training loss: 6931.58, base loss: 8503.79
[INFO 2017-06-26 17:40:20,883 main.py:47] epoch 1809, training loss: 7158.39, average training loss: 6930.98, base loss: 8503.37
[INFO 2017-06-26 17:40:21,232 main.py:47] epoch 1810, training loss: 6205.03, average training loss: 6929.92, base loss: 8502.66
[INFO 2017-06-26 17:40:21,588 main.py:47] epoch 1811, training loss: 7321.10, average training loss: 6930.58, base loss: 8504.12
[INFO 2017-06-26 17:40:21,937 main.py:47] epoch 1812, training loss: 6965.16, average training loss: 6930.72, base loss: 8504.62
[INFO 2017-06-26 17:40:22,290 main.py:47] epoch 1813, training loss: 7516.59, average training loss: 6931.09, base loss: 8505.62
[INFO 2017-06-26 17:40:22,639 main.py:47] epoch 1814, training loss: 7384.18, average training loss: 6931.51, base loss: 8505.82
[INFO 2017-06-26 17:40:22,991 main.py:47] epoch 1815, training loss: 6580.23, average training loss: 6930.33, base loss: 8505.15
[INFO 2017-06-26 17:40:23,344 main.py:47] epoch 1816, training loss: 7324.45, average training loss: 6930.24, base loss: 8505.68
[INFO 2017-06-26 17:40:23,695 main.py:47] epoch 1817, training loss: 6922.58, average training loss: 6929.93, base loss: 8506.09
[INFO 2017-06-26 17:40:24,044 main.py:47] epoch 1818, training loss: 6918.83, average training loss: 6929.61, base loss: 8506.13
[INFO 2017-06-26 17:40:24,401 main.py:47] epoch 1819, training loss: 6628.72, average training loss: 6928.87, base loss: 8505.45
[INFO 2017-06-26 17:40:24,756 main.py:47] epoch 1820, training loss: 7193.79, average training loss: 6927.56, base loss: 8503.48
[INFO 2017-06-26 17:40:25,109 main.py:47] epoch 1821, training loss: 6353.84, average training loss: 6925.97, base loss: 8501.67
[INFO 2017-06-26 17:40:25,463 main.py:47] epoch 1822, training loss: 6696.38, average training loss: 6925.56, base loss: 8501.30
[INFO 2017-06-26 17:40:25,818 main.py:47] epoch 1823, training loss: 6782.54, average training loss: 6924.71, base loss: 8500.95
[INFO 2017-06-26 17:40:26,170 main.py:47] epoch 1824, training loss: 6743.44, average training loss: 6924.59, base loss: 8501.60
[INFO 2017-06-26 17:40:26,524 main.py:47] epoch 1825, training loss: 6576.70, average training loss: 6924.52, base loss: 8501.41
[INFO 2017-06-26 17:40:26,878 main.py:47] epoch 1826, training loss: 6618.15, average training loss: 6924.35, base loss: 8501.70
[INFO 2017-06-26 17:40:27,232 main.py:47] epoch 1827, training loss: 6379.33, average training loss: 6924.33, base loss: 8501.93
[INFO 2017-06-26 17:40:27,585 main.py:47] epoch 1828, training loss: 6225.86, average training loss: 6924.35, base loss: 8502.18
[INFO 2017-06-26 17:40:27,941 main.py:47] epoch 1829, training loss: 5814.81, average training loss: 6922.56, base loss: 8500.17
[INFO 2017-06-26 17:40:28,296 main.py:47] epoch 1830, training loss: 6816.00, average training loss: 6921.94, base loss: 8499.91
[INFO 2017-06-26 17:40:28,651 main.py:47] epoch 1831, training loss: 7126.58, average training loss: 6922.15, base loss: 8500.02
[INFO 2017-06-26 17:40:29,005 main.py:47] epoch 1832, training loss: 7019.65, average training loss: 6921.57, base loss: 8499.89
[INFO 2017-06-26 17:40:29,357 main.py:47] epoch 1833, training loss: 6867.44, average training loss: 6922.61, base loss: 8502.28
[INFO 2017-06-26 17:40:29,707 main.py:47] epoch 1834, training loss: 7837.42, average training loss: 6923.80, base loss: 8503.69
[INFO 2017-06-26 17:40:30,057 main.py:47] epoch 1835, training loss: 6467.58, average training loss: 6923.59, base loss: 8504.01
[INFO 2017-06-26 17:40:30,408 main.py:47] epoch 1836, training loss: 7593.16, average training loss: 6924.35, base loss: 8505.70
[INFO 2017-06-26 17:40:30,760 main.py:47] epoch 1837, training loss: 6552.92, average training loss: 6924.76, base loss: 8506.82
[INFO 2017-06-26 17:40:31,112 main.py:47] epoch 1838, training loss: 6460.74, average training loss: 6923.71, base loss: 8505.94
[INFO 2017-06-26 17:40:31,463 main.py:47] epoch 1839, training loss: 7477.33, average training loss: 6924.31, base loss: 8507.57
[INFO 2017-06-26 17:40:31,814 main.py:47] epoch 1840, training loss: 6759.89, average training loss: 6924.73, base loss: 8508.22
[INFO 2017-06-26 17:40:32,170 main.py:47] epoch 1841, training loss: 6870.33, average training loss: 6924.68, base loss: 8508.86
[INFO 2017-06-26 17:40:32,520 main.py:47] epoch 1842, training loss: 6658.62, average training loss: 6923.14, base loss: 8507.60
[INFO 2017-06-26 17:40:32,870 main.py:47] epoch 1843, training loss: 7367.31, average training loss: 6922.40, base loss: 8507.34
[INFO 2017-06-26 17:40:33,221 main.py:47] epoch 1844, training loss: 6432.76, average training loss: 6922.51, base loss: 8508.16
[INFO 2017-06-26 17:40:33,571 main.py:47] epoch 1845, training loss: 6911.63, average training loss: 6923.05, base loss: 8508.90
[INFO 2017-06-26 17:40:33,924 main.py:47] epoch 1846, training loss: 7249.23, average training loss: 6923.03, base loss: 8509.17
[INFO 2017-06-26 17:40:34,276 main.py:47] epoch 1847, training loss: 6413.64, average training loss: 6923.18, base loss: 8509.41
[INFO 2017-06-26 17:40:34,629 main.py:47] epoch 1848, training loss: 6513.97, average training loss: 6922.77, base loss: 8508.83
[INFO 2017-06-26 17:40:34,979 main.py:47] epoch 1849, training loss: 6711.92, average training loss: 6921.36, base loss: 8507.25
[INFO 2017-06-26 17:40:35,333 main.py:47] epoch 1850, training loss: 7313.02, average training loss: 6921.17, base loss: 8507.54
[INFO 2017-06-26 17:40:35,686 main.py:47] epoch 1851, training loss: 6397.84, average training loss: 6920.62, base loss: 8506.86
[INFO 2017-06-26 17:40:36,036 main.py:47] epoch 1852, training loss: 6159.34, average training loss: 6918.57, base loss: 8504.49
[INFO 2017-06-26 17:40:36,393 main.py:47] epoch 1853, training loss: 6552.79, average training loss: 6918.77, base loss: 8505.67
[INFO 2017-06-26 17:40:36,744 main.py:47] epoch 1854, training loss: 6999.83, average training loss: 6918.53, base loss: 8504.98
[INFO 2017-06-26 17:40:37,093 main.py:47] epoch 1855, training loss: 5904.56, average training loss: 6917.03, base loss: 8502.97
[INFO 2017-06-26 17:40:37,457 main.py:47] epoch 1856, training loss: 6257.57, average training loss: 6916.15, base loss: 8501.93
[INFO 2017-06-26 17:40:37,811 main.py:47] epoch 1857, training loss: 6493.65, average training loss: 6915.18, base loss: 8501.07
[INFO 2017-06-26 17:40:38,162 main.py:47] epoch 1858, training loss: 6084.52, average training loss: 6913.96, base loss: 8499.95
[INFO 2017-06-26 17:40:38,515 main.py:47] epoch 1859, training loss: 7238.74, average training loss: 6913.74, base loss: 8499.72
[INFO 2017-06-26 17:40:38,867 main.py:47] epoch 1860, training loss: 6236.65, average training loss: 6912.48, base loss: 8497.93
[INFO 2017-06-26 17:40:39,218 main.py:47] epoch 1861, training loss: 6282.67, average training loss: 6911.72, base loss: 8497.18
[INFO 2017-06-26 17:40:39,571 main.py:47] epoch 1862, training loss: 6043.81, average training loss: 6911.11, base loss: 8496.74
[INFO 2017-06-26 17:40:39,923 main.py:47] epoch 1863, training loss: 6446.67, average training loss: 6910.96, base loss: 8497.11
[INFO 2017-06-26 17:40:40,273 main.py:47] epoch 1864, training loss: 6356.29, average training loss: 6909.95, base loss: 8496.02
[INFO 2017-06-26 17:40:40,624 main.py:47] epoch 1865, training loss: 6211.33, average training loss: 6909.13, base loss: 8495.71
[INFO 2017-06-26 17:40:40,975 main.py:47] epoch 1866, training loss: 6226.52, average training loss: 6908.50, base loss: 8495.22
[INFO 2017-06-26 17:40:41,326 main.py:47] epoch 1867, training loss: 7629.54, average training loss: 6908.67, base loss: 8495.52
[INFO 2017-06-26 17:40:41,678 main.py:47] epoch 1868, training loss: 6813.32, average training loss: 6908.48, base loss: 8496.06
[INFO 2017-06-26 17:40:42,030 main.py:47] epoch 1869, training loss: 6348.30, average training loss: 6907.22, base loss: 8494.58
[INFO 2017-06-26 17:40:42,397 main.py:47] epoch 1870, training loss: 7417.38, average training loss: 6905.98, base loss: 8493.16
[INFO 2017-06-26 17:40:42,750 main.py:47] epoch 1871, training loss: 7376.18, average training loss: 6906.52, base loss: 8494.55
[INFO 2017-06-26 17:40:43,106 main.py:47] epoch 1872, training loss: 6145.41, average training loss: 6904.75, base loss: 8492.93
[INFO 2017-06-26 17:40:43,466 main.py:47] epoch 1873, training loss: 5828.84, average training loss: 6903.67, base loss: 8491.22
[INFO 2017-06-26 17:40:43,818 main.py:47] epoch 1874, training loss: 6132.38, average training loss: 6902.50, base loss: 8489.00
[INFO 2017-06-26 17:40:44,169 main.py:47] epoch 1875, training loss: 6414.33, average training loss: 6900.79, base loss: 8486.90
[INFO 2017-06-26 17:40:44,521 main.py:47] epoch 1876, training loss: 7321.43, average training loss: 6900.23, base loss: 8486.86
[INFO 2017-06-26 17:40:44,871 main.py:47] epoch 1877, training loss: 6418.58, average training loss: 6899.58, base loss: 8486.65
[INFO 2017-06-26 17:40:45,224 main.py:47] epoch 1878, training loss: 6624.06, average training loss: 6899.42, base loss: 8487.25
[INFO 2017-06-26 17:40:45,576 main.py:47] epoch 1879, training loss: 7090.88, average training loss: 6900.23, base loss: 8488.67
[INFO 2017-06-26 17:40:45,960 main.py:47] epoch 1880, training loss: 6459.39, average training loss: 6900.88, base loss: 8490.42
[INFO 2017-06-26 17:40:46,325 main.py:47] epoch 1881, training loss: 6391.09, average training loss: 6900.76, base loss: 8490.13
[INFO 2017-06-26 17:40:46,687 main.py:47] epoch 1882, training loss: 6669.92, average training loss: 6899.82, base loss: 8489.49
[INFO 2017-06-26 17:40:47,060 main.py:47] epoch 1883, training loss: 6801.69, average training loss: 6899.58, base loss: 8489.82
[INFO 2017-06-26 17:40:47,418 main.py:47] epoch 1884, training loss: 7220.24, average training loss: 6900.35, base loss: 8491.61
[INFO 2017-06-26 17:40:47,773 main.py:47] epoch 1885, training loss: 6576.85, average training loss: 6899.84, base loss: 8491.62
[INFO 2017-06-26 17:40:48,143 main.py:47] epoch 1886, training loss: 6727.03, average training loss: 6900.21, base loss: 8492.65
[INFO 2017-06-26 17:40:48,502 main.py:47] epoch 1887, training loss: 5728.69, average training loss: 6898.71, base loss: 8491.01
[INFO 2017-06-26 17:40:48,857 main.py:47] epoch 1888, training loss: 7535.44, average training loss: 6899.06, base loss: 8492.75
[INFO 2017-06-26 17:40:49,207 main.py:47] epoch 1889, training loss: 5904.17, average training loss: 6898.01, base loss: 8490.86
[INFO 2017-06-26 17:40:49,557 main.py:47] epoch 1890, training loss: 6025.47, average training loss: 6896.82, base loss: 8489.49
[INFO 2017-06-26 17:40:49,908 main.py:47] epoch 1891, training loss: 7084.75, average training loss: 6897.04, base loss: 8490.07
[INFO 2017-06-26 17:40:50,259 main.py:47] epoch 1892, training loss: 6851.69, average training loss: 6896.99, base loss: 8490.00
[INFO 2017-06-26 17:40:50,608 main.py:47] epoch 1893, training loss: 6793.14, average training loss: 6895.73, base loss: 8489.08
[INFO 2017-06-26 17:40:50,959 main.py:47] epoch 1894, training loss: 7371.40, average training loss: 6895.97, base loss: 8489.68
[INFO 2017-06-26 17:40:51,309 main.py:47] epoch 1895, training loss: 6231.15, average training loss: 6895.32, base loss: 8488.69
[INFO 2017-06-26 17:40:51,659 main.py:47] epoch 1896, training loss: 7481.91, average training loss: 6895.98, base loss: 8490.03
[INFO 2017-06-26 17:40:52,010 main.py:47] epoch 1897, training loss: 6895.48, average training loss: 6894.61, base loss: 8488.42
[INFO 2017-06-26 17:40:52,360 main.py:47] epoch 1898, training loss: 6185.13, average training loss: 6894.25, base loss: 8488.43
[INFO 2017-06-26 17:40:52,719 main.py:47] epoch 1899, training loss: 6291.98, average training loss: 6893.29, base loss: 8487.27
[INFO 2017-06-26 17:40:52,719 main.py:49] epoch 1899, testing
[INFO 2017-06-26 17:40:56,868 main.py:100] average testing loss: 6994.60, base loss: 8766.14
[INFO 2017-06-26 17:40:56,892 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:40:57,242 main.py:47] epoch 1900, training loss: 6270.13, average training loss: 6891.98, base loss: 8485.48
[INFO 2017-06-26 17:40:57,591 main.py:47] epoch 1901, training loss: 6768.75, average training loss: 6891.00, base loss: 8484.37
[INFO 2017-06-26 17:40:57,940 main.py:47] epoch 1902, training loss: 6006.21, average training loss: 6889.63, base loss: 8482.67
[INFO 2017-06-26 17:40:58,291 main.py:47] epoch 1903, training loss: 6234.08, average training loss: 6888.49, base loss: 8481.43
[INFO 2017-06-26 17:40:58,650 main.py:47] epoch 1904, training loss: 5821.96, average training loss: 6887.15, base loss: 8479.72
[INFO 2017-06-26 17:40:58,999 main.py:47] epoch 1905, training loss: 7891.53, average training loss: 6888.68, base loss: 8482.07
[INFO 2017-06-26 17:40:59,353 main.py:47] epoch 1906, training loss: 8142.30, average training loss: 6889.83, base loss: 8484.42
[INFO 2017-06-26 17:40:59,709 main.py:47] epoch 1907, training loss: 7156.82, average training loss: 6889.02, base loss: 8483.83
[INFO 2017-06-26 17:41:00,065 main.py:47] epoch 1908, training loss: 6937.63, average training loss: 6889.79, base loss: 8485.43
[INFO 2017-06-26 17:41:00,415 main.py:47] epoch 1909, training loss: 7595.13, average training loss: 6889.42, base loss: 8485.16
[INFO 2017-06-26 17:41:00,766 main.py:47] epoch 1910, training loss: 6967.14, average training loss: 6889.46, base loss: 8485.88
[INFO 2017-06-26 17:41:01,116 main.py:47] epoch 1911, training loss: 6328.04, average training loss: 6888.97, base loss: 8486.01
[INFO 2017-06-26 17:41:01,466 main.py:47] epoch 1912, training loss: 6937.92, average training loss: 6889.77, base loss: 8486.89
[INFO 2017-06-26 17:41:01,816 main.py:47] epoch 1913, training loss: 6895.16, average training loss: 6889.64, base loss: 8486.86
[INFO 2017-06-26 17:41:02,167 main.py:47] epoch 1914, training loss: 6447.72, average training loss: 6888.60, base loss: 8485.35
[INFO 2017-06-26 17:41:02,518 main.py:47] epoch 1915, training loss: 6850.67, average training loss: 6888.70, base loss: 8486.03
[INFO 2017-06-26 17:41:02,868 main.py:47] epoch 1916, training loss: 6367.55, average training loss: 6888.78, base loss: 8486.35
[INFO 2017-06-26 17:41:03,219 main.py:47] epoch 1917, training loss: 7046.63, average training loss: 6888.22, base loss: 8485.55
[INFO 2017-06-26 17:41:03,570 main.py:47] epoch 1918, training loss: 6989.43, average training loss: 6888.70, base loss: 8487.12
[INFO 2017-06-26 17:41:03,991 main.py:47] epoch 1919, training loss: 7367.87, average training loss: 6888.91, base loss: 8487.81
[INFO 2017-06-26 17:41:04,344 main.py:47] epoch 1920, training loss: 7286.61, average training loss: 6889.23, base loss: 8488.04
[INFO 2017-06-26 17:41:04,696 main.py:47] epoch 1921, training loss: 5944.01, average training loss: 6889.39, base loss: 8488.73
[INFO 2017-06-26 17:41:05,056 main.py:47] epoch 1922, training loss: 6405.25, average training loss: 6888.02, base loss: 8487.46
[INFO 2017-06-26 17:41:05,409 main.py:47] epoch 1923, training loss: 6705.06, average training loss: 6887.25, base loss: 8487.43
[INFO 2017-06-26 17:41:05,762 main.py:47] epoch 1924, training loss: 6552.03, average training loss: 6887.20, base loss: 8487.78
[INFO 2017-06-26 17:41:06,115 main.py:47] epoch 1925, training loss: 6217.11, average training loss: 6886.13, base loss: 8486.65
[INFO 2017-06-26 17:41:06,473 main.py:47] epoch 1926, training loss: 6632.52, average training loss: 6886.07, base loss: 8487.15
[INFO 2017-06-26 17:41:06,824 main.py:47] epoch 1927, training loss: 6391.26, average training loss: 6885.53, base loss: 8486.16
[INFO 2017-06-26 17:41:07,175 main.py:47] epoch 1928, training loss: 7203.21, average training loss: 6886.21, base loss: 8487.52
[INFO 2017-06-26 17:41:07,528 main.py:47] epoch 1929, training loss: 6033.58, average training loss: 6885.27, base loss: 8485.78
[INFO 2017-06-26 17:41:07,882 main.py:47] epoch 1930, training loss: 7716.75, average training loss: 6885.52, base loss: 8486.17
[INFO 2017-06-26 17:41:08,233 main.py:47] epoch 1931, training loss: 6939.98, average training loss: 6885.12, base loss: 8485.49
[INFO 2017-06-26 17:41:08,587 main.py:47] epoch 1932, training loss: 6603.46, average training loss: 6884.36, base loss: 8483.90
[INFO 2017-06-26 17:41:08,939 main.py:47] epoch 1933, training loss: 6982.22, average training loss: 6884.64, base loss: 8484.20
[INFO 2017-06-26 17:41:09,289 main.py:47] epoch 1934, training loss: 6401.77, average training loss: 6884.23, base loss: 8484.32
[INFO 2017-06-26 17:41:09,672 main.py:47] epoch 1935, training loss: 6586.64, average training loss: 6883.62, base loss: 8484.25
[INFO 2017-06-26 17:41:10,049 main.py:47] epoch 1936, training loss: 6390.62, average training loss: 6882.52, base loss: 8482.94
[INFO 2017-06-26 17:41:10,406 main.py:47] epoch 1937, training loss: 6981.82, average training loss: 6882.24, base loss: 8482.51
[INFO 2017-06-26 17:41:10,765 main.py:47] epoch 1938, training loss: 6443.63, average training loss: 6881.79, base loss: 8482.61
[INFO 2017-06-26 17:41:11,113 main.py:47] epoch 1939, training loss: 7576.28, average training loss: 6881.46, base loss: 8481.95
[INFO 2017-06-26 17:41:11,463 main.py:47] epoch 1940, training loss: 8317.20, average training loss: 6882.54, base loss: 8483.06
[INFO 2017-06-26 17:41:11,812 main.py:47] epoch 1941, training loss: 6659.26, average training loss: 6881.95, base loss: 8482.06
[INFO 2017-06-26 17:41:12,163 main.py:47] epoch 1942, training loss: 6750.56, average training loss: 6881.08, base loss: 8481.85
[INFO 2017-06-26 17:41:12,513 main.py:47] epoch 1943, training loss: 6974.62, average training loss: 6881.05, base loss: 8482.10
[INFO 2017-06-26 17:41:12,864 main.py:47] epoch 1944, training loss: 6793.55, average training loss: 6881.79, base loss: 8483.59
[INFO 2017-06-26 17:41:13,215 main.py:47] epoch 1945, training loss: 6896.88, average training loss: 6880.94, base loss: 8483.10
[INFO 2017-06-26 17:41:13,565 main.py:47] epoch 1946, training loss: 6373.45, average training loss: 6879.20, base loss: 8481.10
[INFO 2017-06-26 17:41:13,924 main.py:47] epoch 1947, training loss: 6150.66, average training loss: 6878.31, base loss: 8480.00
[INFO 2017-06-26 17:41:14,275 main.py:47] epoch 1948, training loss: 5844.24, average training loss: 6877.03, base loss: 8478.01
[INFO 2017-06-26 17:41:14,624 main.py:47] epoch 1949, training loss: 5962.86, average training loss: 6875.93, base loss: 8477.14
[INFO 2017-06-26 17:41:14,974 main.py:47] epoch 1950, training loss: 7054.70, average training loss: 6875.94, base loss: 8477.83
[INFO 2017-06-26 17:41:15,326 main.py:47] epoch 1951, training loss: 6267.81, average training loss: 6874.80, base loss: 8476.85
[INFO 2017-06-26 17:41:15,677 main.py:47] epoch 1952, training loss: 6378.33, average training loss: 6873.63, base loss: 8474.99
[INFO 2017-06-26 17:41:16,024 main.py:47] epoch 1953, training loss: 6527.38, average training loss: 6873.83, base loss: 8475.84
[INFO 2017-06-26 17:41:16,374 main.py:47] epoch 1954, training loss: 6494.66, average training loss: 6872.18, base loss: 8473.77
[INFO 2017-06-26 17:41:16,724 main.py:47] epoch 1955, training loss: 6568.75, average training loss: 6871.87, base loss: 8473.31
[INFO 2017-06-26 17:41:17,074 main.py:47] epoch 1956, training loss: 6948.89, average training loss: 6871.63, base loss: 8473.30
[INFO 2017-06-26 17:41:17,423 main.py:47] epoch 1957, training loss: 6188.84, average training loss: 6871.09, base loss: 8473.34
[INFO 2017-06-26 17:41:17,773 main.py:47] epoch 1958, training loss: 6867.24, average training loss: 6871.18, base loss: 8473.48
[INFO 2017-06-26 17:41:18,121 main.py:47] epoch 1959, training loss: 7469.86, average training loss: 6871.52, base loss: 8474.62
[INFO 2017-06-26 17:41:18,473 main.py:47] epoch 1960, training loss: 6701.00, average training loss: 6870.79, base loss: 8473.72
[INFO 2017-06-26 17:41:18,825 main.py:47] epoch 1961, training loss: 6958.84, average training loss: 6870.76, base loss: 8474.01
[INFO 2017-06-26 17:41:19,187 main.py:47] epoch 1962, training loss: 6971.89, average training loss: 6870.49, base loss: 8474.15
[INFO 2017-06-26 17:41:19,543 main.py:47] epoch 1963, training loss: 6552.47, average training loss: 6870.30, base loss: 8474.71
[INFO 2017-06-26 17:41:19,896 main.py:47] epoch 1964, training loss: 6628.14, average training loss: 6869.92, base loss: 8474.45
[INFO 2017-06-26 17:41:20,247 main.py:47] epoch 1965, training loss: 6903.41, average training loss: 6868.66, base loss: 8472.84
[INFO 2017-06-26 17:41:20,598 main.py:47] epoch 1966, training loss: 6140.66, average training loss: 6867.50, base loss: 8471.31
[INFO 2017-06-26 17:41:20,982 main.py:47] epoch 1967, training loss: 6519.97, average training loss: 6867.76, base loss: 8471.38
[INFO 2017-06-26 17:41:21,349 main.py:47] epoch 1968, training loss: 5984.56, average training loss: 6867.14, base loss: 8470.41
[INFO 2017-06-26 17:41:21,702 main.py:47] epoch 1969, training loss: 7912.57, average training loss: 6868.20, base loss: 8471.30
[INFO 2017-06-26 17:41:22,065 main.py:47] epoch 1970, training loss: 6997.01, average training loss: 6868.76, base loss: 8472.28
[INFO 2017-06-26 17:41:22,416 main.py:47] epoch 1971, training loss: 6785.82, average training loss: 6869.22, base loss: 8473.51
[INFO 2017-06-26 17:41:22,766 main.py:47] epoch 1972, training loss: 6151.06, average training loss: 6868.60, base loss: 8472.75
[INFO 2017-06-26 17:41:23,118 main.py:47] epoch 1973, training loss: 6404.80, average training loss: 6867.86, base loss: 8472.19
[INFO 2017-06-26 17:41:23,469 main.py:47] epoch 1974, training loss: 6830.29, average training loss: 6868.20, base loss: 8473.47
[INFO 2017-06-26 17:41:23,820 main.py:47] epoch 1975, training loss: 5991.58, average training loss: 6867.89, base loss: 8473.20
[INFO 2017-06-26 17:41:24,180 main.py:47] epoch 1976, training loss: 5964.72, average training loss: 6866.55, base loss: 8471.56
[INFO 2017-06-26 17:41:24,531 main.py:47] epoch 1977, training loss: 6025.78, average training loss: 6864.48, base loss: 8469.75
[INFO 2017-06-26 17:41:24,882 main.py:47] epoch 1978, training loss: 7371.80, average training loss: 6865.01, base loss: 8471.30
[INFO 2017-06-26 17:41:25,232 main.py:47] epoch 1979, training loss: 7325.32, average training loss: 6865.71, base loss: 8471.99
[INFO 2017-06-26 17:41:25,584 main.py:47] epoch 1980, training loss: 7206.57, average training loss: 6865.34, base loss: 8471.74
[INFO 2017-06-26 17:41:25,935 main.py:47] epoch 1981, training loss: 6811.57, average training loss: 6865.41, base loss: 8471.95
[INFO 2017-06-26 17:41:26,285 main.py:47] epoch 1982, training loss: 6863.51, average training loss: 6865.01, base loss: 8471.82
[INFO 2017-06-26 17:41:26,635 main.py:47] epoch 1983, training loss: 7607.71, average training loss: 6866.31, base loss: 8474.49
[INFO 2017-06-26 17:41:26,985 main.py:47] epoch 1984, training loss: 6569.36, average training loss: 6865.00, base loss: 8472.79
[INFO 2017-06-26 17:41:27,335 main.py:47] epoch 1985, training loss: 6577.75, average training loss: 6865.29, base loss: 8472.75
[INFO 2017-06-26 17:41:27,685 main.py:47] epoch 1986, training loss: 5748.47, average training loss: 6862.64, base loss: 8469.12
[INFO 2017-06-26 17:41:28,036 main.py:47] epoch 1987, training loss: 7275.34, average training loss: 6862.98, base loss: 8470.36
[INFO 2017-06-26 17:41:28,386 main.py:47] epoch 1988, training loss: 6087.03, average training loss: 6862.08, base loss: 8469.28
[INFO 2017-06-26 17:41:28,738 main.py:47] epoch 1989, training loss: 5742.76, average training loss: 6860.66, base loss: 8467.54
[INFO 2017-06-26 17:41:29,089 main.py:47] epoch 1990, training loss: 6099.72, average training loss: 6859.31, base loss: 8465.50
[INFO 2017-06-26 17:41:29,444 main.py:47] epoch 1991, training loss: 5814.09, average training loss: 6858.24, base loss: 8463.77
[INFO 2017-06-26 17:41:29,794 main.py:47] epoch 1992, training loss: 6772.47, average training loss: 6857.95, base loss: 8463.52
[INFO 2017-06-26 17:41:30,145 main.py:47] epoch 1993, training loss: 6270.46, average training loss: 6857.75, base loss: 8463.87
[INFO 2017-06-26 17:41:30,494 main.py:47] epoch 1994, training loss: 7611.90, average training loss: 6858.38, base loss: 8464.96
[INFO 2017-06-26 17:41:30,847 main.py:47] epoch 1995, training loss: 7226.37, average training loss: 6858.03, base loss: 8464.02
[INFO 2017-06-26 17:41:31,198 main.py:47] epoch 1996, training loss: 6383.83, average training loss: 6857.12, base loss: 8463.02
[INFO 2017-06-26 17:41:31,548 main.py:47] epoch 1997, training loss: 6144.94, average training loss: 6856.77, base loss: 8462.48
[INFO 2017-06-26 17:41:31,900 main.py:47] epoch 1998, training loss: 6477.88, average training loss: 6856.96, base loss: 8463.70
[INFO 2017-06-26 17:41:32,252 main.py:47] epoch 1999, training loss: 7355.38, average training loss: 6857.69, base loss: 8464.42
[INFO 2017-06-26 17:41:32,252 main.py:49] epoch 1999, testing
[INFO 2017-06-26 17:41:36,431 main.py:100] average testing loss: 6701.90, base loss: 8321.84
[INFO 2017-06-26 17:41:36,465 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:41:36,832 main.py:47] epoch 2000, training loss: 6755.10, average training loss: 6858.02, base loss: 8464.81
[INFO 2017-06-26 17:41:37,204 main.py:47] epoch 2001, training loss: 6751.92, average training loss: 6858.73, base loss: 8466.12
[INFO 2017-06-26 17:41:37,556 main.py:47] epoch 2002, training loss: 6823.02, average training loss: 6858.72, base loss: 8466.74
[INFO 2017-06-26 17:41:37,906 main.py:47] epoch 2003, training loss: 7043.21, average training loss: 6859.14, base loss: 8467.83
[INFO 2017-06-26 17:41:38,258 main.py:47] epoch 2004, training loss: 6823.84, average training loss: 6858.66, base loss: 8467.74
[INFO 2017-06-26 17:41:38,609 main.py:47] epoch 2005, training loss: 7259.96, average training loss: 6859.26, base loss: 8469.21
[INFO 2017-06-26 17:41:38,960 main.py:47] epoch 2006, training loss: 6070.86, average training loss: 6858.62, base loss: 8468.65
[INFO 2017-06-26 17:41:39,310 main.py:47] epoch 2007, training loss: 7216.19, average training loss: 6858.31, base loss: 8468.35
[INFO 2017-06-26 17:41:39,661 main.py:47] epoch 2008, training loss: 6890.38, average training loss: 6858.03, base loss: 8467.84
[INFO 2017-06-26 17:41:40,016 main.py:47] epoch 2009, training loss: 6377.13, average training loss: 6857.71, base loss: 8467.46
[INFO 2017-06-26 17:41:40,374 main.py:47] epoch 2010, training loss: 6024.03, average training loss: 6856.35, base loss: 8466.06
[INFO 2017-06-26 17:41:40,725 main.py:47] epoch 2011, training loss: 7122.85, average training loss: 6856.53, base loss: 8466.34
[INFO 2017-06-26 17:41:41,076 main.py:47] epoch 2012, training loss: 6421.23, average training loss: 6856.18, base loss: 8466.04
[INFO 2017-06-26 17:41:41,428 main.py:47] epoch 2013, training loss: 6846.31, average training loss: 6855.61, base loss: 8466.05
[INFO 2017-06-26 17:41:41,780 main.py:47] epoch 2014, training loss: 6909.20, average training loss: 6856.43, base loss: 8467.21
[INFO 2017-06-26 17:41:42,129 main.py:47] epoch 2015, training loss: 6257.31, average training loss: 6856.45, base loss: 8467.69
[INFO 2017-06-26 17:41:42,511 main.py:47] epoch 2016, training loss: 7120.82, average training loss: 6854.63, base loss: 8465.55
[INFO 2017-06-26 17:41:42,881 main.py:47] epoch 2017, training loss: 7182.65, average training loss: 6854.89, base loss: 8465.98
[INFO 2017-06-26 17:41:43,239 main.py:47] epoch 2018, training loss: 7536.56, average training loss: 6855.99, base loss: 8467.64
[INFO 2017-06-26 17:41:43,591 main.py:47] epoch 2019, training loss: 6608.44, average training loss: 6856.18, base loss: 8467.81
[INFO 2017-06-26 17:41:43,945 main.py:47] epoch 2020, training loss: 6321.59, average training loss: 6856.15, base loss: 8467.85
[INFO 2017-06-26 17:41:44,298 main.py:47] epoch 2021, training loss: 6795.79, average training loss: 6854.91, base loss: 8466.57
[INFO 2017-06-26 17:41:44,650 main.py:47] epoch 2022, training loss: 6678.63, average training loss: 6855.41, base loss: 8467.64
[INFO 2017-06-26 17:41:45,001 main.py:47] epoch 2023, training loss: 6994.20, average training loss: 6854.24, base loss: 8466.90
[INFO 2017-06-26 17:41:45,362 main.py:47] epoch 2024, training loss: 5746.73, average training loss: 6853.44, base loss: 8465.67
[INFO 2017-06-26 17:41:45,712 main.py:47] epoch 2025, training loss: 6583.41, average training loss: 6852.97, base loss: 8465.77
[INFO 2017-06-26 17:41:46,064 main.py:47] epoch 2026, training loss: 6211.07, average training loss: 6852.31, base loss: 8464.92
[INFO 2017-06-26 17:41:46,417 main.py:47] epoch 2027, training loss: 6590.74, average training loss: 6852.20, base loss: 8464.88
[INFO 2017-06-26 17:41:46,771 main.py:47] epoch 2028, training loss: 6110.51, average training loss: 6851.59, base loss: 8464.41
[INFO 2017-06-26 17:41:47,124 main.py:47] epoch 2029, training loss: 6663.70, average training loss: 6850.24, base loss: 8463.55
[INFO 2017-06-26 17:41:47,477 main.py:47] epoch 2030, training loss: 6237.89, average training loss: 6849.69, base loss: 8462.75
[INFO 2017-06-26 17:41:47,826 main.py:47] epoch 2031, training loss: 6938.79, average training loss: 6850.33, base loss: 8463.48
[INFO 2017-06-26 17:41:48,183 main.py:47] epoch 2032, training loss: 6464.02, average training loss: 6848.55, base loss: 8460.78
[INFO 2017-06-26 17:41:48,534 main.py:47] epoch 2033, training loss: 6418.35, average training loss: 6847.50, base loss: 8459.69
[INFO 2017-06-26 17:41:48,927 main.py:47] epoch 2034, training loss: 6650.35, average training loss: 6847.63, base loss: 8460.12
[INFO 2017-06-26 17:41:49,319 main.py:47] epoch 2035, training loss: 6016.09, average training loss: 6847.40, base loss: 8459.94
[INFO 2017-06-26 17:41:49,684 main.py:47] epoch 2036, training loss: 7146.53, average training loss: 6848.05, base loss: 8461.14
[INFO 2017-06-26 17:41:50,049 main.py:47] epoch 2037, training loss: 6163.16, average training loss: 6848.03, base loss: 8461.69
[INFO 2017-06-26 17:41:50,418 main.py:47] epoch 2038, training loss: 6883.30, average training loss: 6847.41, base loss: 8461.71
[INFO 2017-06-26 17:41:50,770 main.py:47] epoch 2039, training loss: 6645.70, average training loss: 6846.00, base loss: 8460.28
[INFO 2017-06-26 17:41:51,120 main.py:47] epoch 2040, training loss: 6603.46, average training loss: 6845.66, base loss: 8460.32
[INFO 2017-06-26 17:41:51,471 main.py:47] epoch 2041, training loss: 6342.14, average training loss: 6845.56, base loss: 8460.13
[INFO 2017-06-26 17:41:51,823 main.py:47] epoch 2042, training loss: 7783.82, average training loss: 6846.19, base loss: 8460.85
[INFO 2017-06-26 17:41:52,177 main.py:47] epoch 2043, training loss: 6821.85, average training loss: 6847.40, base loss: 8462.84
[INFO 2017-06-26 17:41:52,529 main.py:47] epoch 2044, training loss: 6125.24, average training loss: 6847.05, base loss: 8462.70
[INFO 2017-06-26 17:41:52,915 main.py:47] epoch 2045, training loss: 6466.14, average training loss: 6846.94, base loss: 8463.84
[INFO 2017-06-26 17:41:53,320 main.py:47] epoch 2046, training loss: 6607.07, average training loss: 6847.47, base loss: 8464.83
[INFO 2017-06-26 17:41:53,679 main.py:47] epoch 2047, training loss: 5886.75, average training loss: 6845.85, base loss: 8462.75
[INFO 2017-06-26 17:41:54,049 main.py:47] epoch 2048, training loss: 7024.40, average training loss: 6846.80, base loss: 8464.46
[INFO 2017-06-26 17:41:54,411 main.py:47] epoch 2049, training loss: 7493.71, average training loss: 6847.85, base loss: 8466.47
[INFO 2017-06-26 17:41:54,766 main.py:47] epoch 2050, training loss: 5982.36, average training loss: 6847.83, base loss: 8466.91
[INFO 2017-06-26 17:41:55,118 main.py:47] epoch 2051, training loss: 6522.99, average training loss: 6848.38, base loss: 8467.74
[INFO 2017-06-26 17:41:55,524 main.py:47] epoch 2052, training loss: 7425.61, average training loss: 6849.68, base loss: 8470.69
[INFO 2017-06-26 17:41:55,897 main.py:47] epoch 2053, training loss: 6369.88, average training loss: 6848.36, base loss: 8469.10
[INFO 2017-06-26 17:41:56,251 main.py:47] epoch 2054, training loss: 6186.41, average training loss: 6847.24, base loss: 8467.54
[INFO 2017-06-26 17:41:56,608 main.py:47] epoch 2055, training loss: 6641.82, average training loss: 6846.22, base loss: 8466.99
[INFO 2017-06-26 17:41:56,998 main.py:47] epoch 2056, training loss: 6376.63, average training loss: 6846.10, base loss: 8467.11
[INFO 2017-06-26 17:41:57,366 main.py:47] epoch 2057, training loss: 6947.06, average training loss: 6845.12, base loss: 8465.26
[INFO 2017-06-26 17:41:57,756 main.py:47] epoch 2058, training loss: 6799.60, average training loss: 6844.61, base loss: 8465.03
[INFO 2017-06-26 17:41:58,125 main.py:47] epoch 2059, training loss: 5824.95, average training loss: 6843.74, base loss: 8464.33
[INFO 2017-06-26 17:41:58,488 main.py:47] epoch 2060, training loss: 6409.14, average training loss: 6843.38, base loss: 8464.12
[INFO 2017-06-26 17:41:58,846 main.py:47] epoch 2061, training loss: 6668.02, average training loss: 6843.34, base loss: 8463.79
[INFO 2017-06-26 17:41:59,198 main.py:47] epoch 2062, training loss: 6448.01, average training loss: 6842.00, base loss: 8462.47
[INFO 2017-06-26 17:41:59,549 main.py:47] epoch 2063, training loss: 7079.66, average training loss: 6842.60, base loss: 8463.64
[INFO 2017-06-26 17:41:59,911 main.py:47] epoch 2064, training loss: 5983.20, average training loss: 6841.72, base loss: 8462.99
[INFO 2017-06-26 17:42:00,262 main.py:47] epoch 2065, training loss: 6330.62, average training loss: 6841.45, base loss: 8462.41
[INFO 2017-06-26 17:42:00,618 main.py:47] epoch 2066, training loss: 6986.47, average training loss: 6840.65, base loss: 8461.31
[INFO 2017-06-26 17:42:00,969 main.py:47] epoch 2067, training loss: 6625.25, average training loss: 6841.31, base loss: 8462.23
[INFO 2017-06-26 17:42:01,320 main.py:47] epoch 2068, training loss: 7409.00, average training loss: 6842.26, base loss: 8464.37
[INFO 2017-06-26 17:42:01,668 main.py:47] epoch 2069, training loss: 7673.10, average training loss: 6842.51, base loss: 8464.88
[INFO 2017-06-26 17:42:02,018 main.py:47] epoch 2070, training loss: 6931.79, average training loss: 6842.64, base loss: 8465.44
[INFO 2017-06-26 17:42:02,369 main.py:47] epoch 2071, training loss: 7236.78, average training loss: 6842.23, base loss: 8465.33
[INFO 2017-06-26 17:42:02,721 main.py:47] epoch 2072, training loss: 8880.88, average training loss: 6844.59, base loss: 8468.39
[INFO 2017-06-26 17:42:03,072 main.py:47] epoch 2073, training loss: 6305.35, average training loss: 6844.01, base loss: 8468.10
[INFO 2017-06-26 17:42:03,423 main.py:47] epoch 2074, training loss: 6242.95, average training loss: 6843.34, base loss: 8467.66
[INFO 2017-06-26 17:42:03,774 main.py:47] epoch 2075, training loss: 6003.92, average training loss: 6841.51, base loss: 8465.43
[INFO 2017-06-26 17:42:04,124 main.py:47] epoch 2076, training loss: 7767.59, average training loss: 6842.98, base loss: 8468.44
[INFO 2017-06-26 17:42:04,474 main.py:47] epoch 2077, training loss: 6727.63, average training loss: 6842.09, base loss: 8467.78
[INFO 2017-06-26 17:42:04,858 main.py:47] epoch 2078, training loss: 6512.38, average training loss: 6841.65, base loss: 8467.44
[INFO 2017-06-26 17:42:05,211 main.py:47] epoch 2079, training loss: 6122.08, average training loss: 6840.21, base loss: 8465.51
[INFO 2017-06-26 17:42:05,571 main.py:47] epoch 2080, training loss: 6784.38, average training loss: 6840.34, base loss: 8466.68
[INFO 2017-06-26 17:42:05,935 main.py:47] epoch 2081, training loss: 6125.58, average training loss: 6840.37, base loss: 8467.02
[INFO 2017-06-26 17:42:06,290 main.py:47] epoch 2082, training loss: 6496.06, average training loss: 6839.98, base loss: 8466.70
[INFO 2017-06-26 17:42:06,643 main.py:47] epoch 2083, training loss: 6600.58, average training loss: 6839.31, base loss: 8466.37
[INFO 2017-06-26 17:42:06,996 main.py:47] epoch 2084, training loss: 6489.22, average training loss: 6838.68, base loss: 8465.98
[INFO 2017-06-26 17:42:07,349 main.py:47] epoch 2085, training loss: 7029.79, average training loss: 6838.71, base loss: 8466.39
[INFO 2017-06-26 17:42:07,699 main.py:47] epoch 2086, training loss: 6414.78, average training loss: 6838.40, base loss: 8466.74
[INFO 2017-06-26 17:42:08,048 main.py:47] epoch 2087, training loss: 6754.70, average training loss: 6837.34, base loss: 8465.95
[INFO 2017-06-26 17:42:08,401 main.py:47] epoch 2088, training loss: 6560.95, average training loss: 6836.69, base loss: 8465.15
[INFO 2017-06-26 17:42:08,750 main.py:47] epoch 2089, training loss: 6099.04, average training loss: 6835.56, base loss: 8464.18
[INFO 2017-06-26 17:42:09,104 main.py:47] epoch 2090, training loss: 7366.88, average training loss: 6836.18, base loss: 8465.16
[INFO 2017-06-26 17:42:09,460 main.py:47] epoch 2091, training loss: 6387.81, average training loss: 6835.43, base loss: 8464.59
[INFO 2017-06-26 17:42:09,814 main.py:47] epoch 2092, training loss: 6366.99, average training loss: 6834.39, base loss: 8463.03
[INFO 2017-06-26 17:42:10,167 main.py:47] epoch 2093, training loss: 6318.24, average training loss: 6833.36, base loss: 8462.01
[INFO 2017-06-26 17:42:10,527 main.py:47] epoch 2094, training loss: 6786.86, average training loss: 6833.25, base loss: 8462.95
[INFO 2017-06-26 17:42:10,879 main.py:47] epoch 2095, training loss: 7396.21, average training loss: 6834.21, base loss: 8464.54
[INFO 2017-06-26 17:42:11,230 main.py:47] epoch 2096, training loss: 6637.76, average training loss: 6834.76, base loss: 8466.31
[INFO 2017-06-26 17:42:11,582 main.py:47] epoch 2097, training loss: 7468.51, average training loss: 6834.69, base loss: 8466.18
[INFO 2017-06-26 17:42:11,934 main.py:47] epoch 2098, training loss: 6119.81, average training loss: 6833.64, base loss: 8465.40
[INFO 2017-06-26 17:42:12,287 main.py:47] epoch 2099, training loss: 7451.85, average training loss: 6834.03, base loss: 8466.94
[INFO 2017-06-26 17:42:12,287 main.py:49] epoch 2099, testing
[INFO 2017-06-26 17:42:16,486 main.py:100] average testing loss: 6546.13, base loss: 8024.65
[INFO 2017-06-26 17:42:16,511 main.py:73] current best accuracy: 6392.41
[INFO 2017-06-26 17:42:16,861 main.py:47] epoch 2100, training loss: 7150.55, average training loss: 6834.37, base loss: 8468.36
[INFO 2017-06-26 17:42:17,211 main.py:47] epoch 2101, training loss: 6518.22, average training loss: 6833.82, base loss: 8468.39
[INFO 2017-06-26 17:42:17,563 main.py:47] epoch 2102, training loss: 6237.81, average training loss: 6832.81, base loss: 8466.96
[INFO 2017-06-26 17:42:17,913 main.py:47] epoch 2103, training loss: 6530.14, average training loss: 6832.60, base loss: 8467.27
[INFO 2017-06-26 17:42:18,264 main.py:47] epoch 2104, training loss: 6617.67, average training loss: 6831.68, base loss: 8466.84
