[INFO 2017-06-25 14:26:40,000 main.py:167] Namespace(batch_size=64, display=False, image_dir='/home/yi/Downloads/mpii-240-2', image_size=64, init_model_path='', input_video_path='video', learning_rate=0.001, method='unsupervised', motion_range=5, num_channel=3, num_inputs=4, output_flow_path='flow', output_flow_video_path='flow_video', save_dir='./model', test=False, test_epoch=100, test_interval=1000, test_video=False, train=True, train_epoch=100000)
[INFO 2017-06-25 14:26:45,380 main.py:50] epoch 0, training loss: 199294.56, average training loss: 199294.56, base loss: 25079.97
[INFO 2017-06-25 14:26:48,708 main.py:50] epoch 1, training loss: 179415.58, average training loss: 189355.07, base loss: 27222.74
[INFO 2017-06-25 14:26:52,045 main.py:50] epoch 2, training loss: 154133.64, average training loss: 177614.59, base loss: 27061.29
[INFO 2017-06-25 14:26:55,357 main.py:50] epoch 3, training loss: 125004.10, average training loss: 164461.97, base loss: 26599.01
[INFO 2017-06-25 14:26:58,683 main.py:50] epoch 4, training loss: 112779.62, average training loss: 154125.50, base loss: 26263.42
[INFO 2017-06-25 14:27:02,030 main.py:50] epoch 5, training loss: 88943.97, average training loss: 143261.91, base loss: 26158.04
[INFO 2017-06-25 14:27:05,385 main.py:50] epoch 6, training loss: 88541.60, average training loss: 135444.72, base loss: 25286.71
[INFO 2017-06-25 14:27:08,833 main.py:50] epoch 7, training loss: 76008.73, average training loss: 128015.23, base loss: 25177.34
[INFO 2017-06-25 14:27:12,764 main.py:50] epoch 8, training loss: 60404.62, average training loss: 120502.94, base loss: 24631.50
[INFO 2017-06-25 14:27:17,013 main.py:50] epoch 9, training loss: 59766.34, average training loss: 114429.28, base loss: 24824.87
[INFO 2017-06-25 14:27:21,290 main.py:50] epoch 10, training loss: 57009.25, average training loss: 109209.27, base loss: 24727.65
[INFO 2017-06-25 14:27:25,532 main.py:50] epoch 11, training loss: 54480.79, average training loss: 104648.57, base loss: 24756.02
[INFO 2017-06-25 14:27:29,785 main.py:50] epoch 12, training loss: 51976.80, average training loss: 100596.89, base loss: 24726.26
[INFO 2017-06-25 14:27:34,079 main.py:50] epoch 13, training loss: 44518.89, average training loss: 96591.32, base loss: 24660.76
[INFO 2017-06-25 14:27:38,370 main.py:50] epoch 14, training loss: 39683.94, average training loss: 92797.50, base loss: 24339.50
[INFO 2017-06-25 14:27:42,665 main.py:50] epoch 15, training loss: 38557.87, average training loss: 89407.52, base loss: 24248.45
[INFO 2017-06-25 14:27:46,911 main.py:50] epoch 16, training loss: 40615.89, average training loss: 86537.42, base loss: 24471.13
[INFO 2017-06-25 14:27:51,218 main.py:50] epoch 17, training loss: 33775.22, average training loss: 83606.19, base loss: 24192.82
[INFO 2017-06-25 14:27:55,470 main.py:50] epoch 18, training loss: 29794.30, average training loss: 80773.99, base loss: 23912.67
[INFO 2017-06-25 14:27:59,736 main.py:50] epoch 19, training loss: 32676.70, average training loss: 78369.12, base loss: 23820.12
[INFO 2017-06-25 14:28:04,020 main.py:50] epoch 20, training loss: 28356.25, average training loss: 75987.56, base loss: 23649.80
[INFO 2017-06-25 14:28:08,291 main.py:50] epoch 21, training loss: 30589.73, average training loss: 73924.02, base loss: 23645.42
[INFO 2017-06-25 14:28:12,563 main.py:50] epoch 22, training loss: 38371.82, average training loss: 72378.27, base loss: 24073.22
[INFO 2017-06-25 14:28:16,887 main.py:50] epoch 23, training loss: 27862.39, average training loss: 70523.44, base loss: 23985.57
[INFO 2017-06-25 14:28:21,174 main.py:50] epoch 24, training loss: 27378.84, average training loss: 68797.66, base loss: 23917.30
[INFO 2017-06-25 14:28:25,457 main.py:50] epoch 25, training loss: 26956.78, average training loss: 67188.39, base loss: 23896.16
[INFO 2017-06-25 14:28:29,742 main.py:50] epoch 26, training loss: 30226.25, average training loss: 65819.43, base loss: 24009.17
[INFO 2017-06-25 14:28:34,026 main.py:50] epoch 27, training loss: 25907.54, average training loss: 64394.00, base loss: 23994.56
[INFO 2017-06-25 14:28:38,384 main.py:50] epoch 28, training loss: 26402.44, average training loss: 63083.95, base loss: 24004.49
[INFO 2017-06-25 14:28:42,644 main.py:50] epoch 29, training loss: 27909.87, average training loss: 61911.48, base loss: 24068.18
[INFO 2017-06-25 14:28:46,943 main.py:50] epoch 30, training loss: 30945.97, average training loss: 60912.59, base loss: 24226.55
[INFO 2017-06-25 14:28:51,207 main.py:50] epoch 31, training loss: 21551.74, average training loss: 59682.56, base loss: 24079.16
[INFO 2017-06-25 14:28:55,493 main.py:50] epoch 32, training loss: 24077.96, average training loss: 58603.64, base loss: 24052.94
[INFO 2017-06-25 14:28:59,767 main.py:50] epoch 33, training loss: 18873.69, average training loss: 57435.11, base loss: 23851.63
[INFO 2017-06-25 14:29:04,037 main.py:50] epoch 34, training loss: 25307.90, average training loss: 56517.19, base loss: 23856.09
[INFO 2017-06-25 14:29:08,292 main.py:50] epoch 35, training loss: 21363.71, average training loss: 55540.70, base loss: 23753.14
[INFO 2017-06-25 14:29:12,571 main.py:50] epoch 36, training loss: 23211.24, average training loss: 54666.93, base loss: 23714.11
[INFO 2017-06-25 14:29:16,865 main.py:50] epoch 37, training loss: 24274.29, average training loss: 53867.13, base loss: 23712.28
[INFO 2017-06-25 14:29:21,179 main.py:50] epoch 38, training loss: 24547.60, average training loss: 53115.34, base loss: 23715.13
[INFO 2017-06-25 14:29:25,432 main.py:50] epoch 39, training loss: 22919.22, average training loss: 52360.44, base loss: 23680.57
[INFO 2017-06-25 14:29:29,716 main.py:50] epoch 40, training loss: 22313.29, average training loss: 51627.58, base loss: 23635.42
[INFO 2017-06-25 14:29:34,017 main.py:50] epoch 41, training loss: 29071.54, average training loss: 51090.54, base loss: 23760.90
[INFO 2017-06-25 14:29:38,330 main.py:50] epoch 42, training loss: 20415.84, average training loss: 50377.17, base loss: 23674.00
[INFO 2017-06-25 14:29:42,604 main.py:50] epoch 43, training loss: 25677.15, average training loss: 49815.81, base loss: 23714.78
[INFO 2017-06-25 14:29:46,864 main.py:50] epoch 44, training loss: 27573.52, average training loss: 49321.53, base loss: 23795.19
[INFO 2017-06-25 14:29:51,156 main.py:50] epoch 45, training loss: 21930.47, average training loss: 48726.08, base loss: 23749.60
[INFO 2017-06-25 14:29:55,450 main.py:50] epoch 46, training loss: 20800.66, average training loss: 48131.92, base loss: 23680.89
[INFO 2017-06-25 14:29:59,744 main.py:50] epoch 47, training loss: 20929.68, average training loss: 47565.20, base loss: 23620.97
[INFO 2017-06-25 14:30:04,037 main.py:50] epoch 48, training loss: 19936.83, average training loss: 47001.36, base loss: 23543.78
[INFO 2017-06-25 14:30:08,349 main.py:50] epoch 49, training loss: 24553.25, average training loss: 46552.40, base loss: 23559.40
[INFO 2017-06-25 14:30:12,622 main.py:50] epoch 50, training loss: 20999.37, average training loss: 46051.36, base loss: 23504.59
[INFO 2017-06-25 14:30:16,949 main.py:50] epoch 51, training loss: 22129.39, average training loss: 45591.32, base loss: 23473.68
[INFO 2017-06-25 14:30:21,206 main.py:50] epoch 52, training loss: 22082.54, average training loss: 45147.76, base loss: 23445.30
[INFO 2017-06-25 14:30:25,498 main.py:50] epoch 53, training loss: 23588.44, average training loss: 44748.51, base loss: 23446.54
[INFO 2017-06-25 14:30:29,796 main.py:50] epoch 54, training loss: 22181.34, average training loss: 44338.20, base loss: 23423.72
[INFO 2017-06-25 14:30:34,063 main.py:50] epoch 55, training loss: 24001.24, average training loss: 43975.04, base loss: 23434.93
[INFO 2017-06-25 14:30:38,341 main.py:50] epoch 56, training loss: 23807.74, average training loss: 43621.23, base loss: 23438.88
[INFO 2017-06-25 14:30:42,633 main.py:50] epoch 57, training loss: 21972.48, average training loss: 43247.97, base loss: 23410.50
[INFO 2017-06-25 14:30:46,902 main.py:50] epoch 58, training loss: 18608.57, average training loss: 42830.36, base loss: 23330.45
[INFO 2017-06-25 14:30:51,192 main.py:50] epoch 59, training loss: 27767.29, average training loss: 42579.30, base loss: 23406.24
[INFO 2017-06-25 14:30:55,465 main.py:50] epoch 60, training loss: 17713.23, average training loss: 42171.66, base loss: 23310.21
[INFO 2017-06-25 14:30:59,759 main.py:50] epoch 61, training loss: 20732.09, average training loss: 41825.86, base loss: 23269.04
[INFO 2017-06-25 14:31:04,035 main.py:50] epoch 62, training loss: 24385.53, average training loss: 41549.03, base loss: 23290.60
[INFO 2017-06-25 14:31:08,319 main.py:50] epoch 63, training loss: 21536.20, average training loss: 41236.33, base loss: 23263.22
[INFO 2017-06-25 14:31:12,625 main.py:50] epoch 64, training loss: 24287.51, average training loss: 40975.58, base loss: 23286.56
[INFO 2017-06-25 14:31:16,912 main.py:50] epoch 65, training loss: 23384.52, average training loss: 40709.05, base loss: 23281.98
[INFO 2017-06-25 14:31:21,219 main.py:50] epoch 66, training loss: 23322.48, average training loss: 40449.55, base loss: 23283.70
[INFO 2017-06-25 14:31:25,500 main.py:50] epoch 67, training loss: 23809.17, average training loss: 40204.84, base loss: 23295.64
[INFO 2017-06-25 14:31:29,797 main.py:50] epoch 68, training loss: 28291.63, average training loss: 40032.18, base loss: 23377.31
[INFO 2017-06-25 14:31:34,107 main.py:50] epoch 69, training loss: 22816.75, average training loss: 39786.25, base loss: 23371.18
[INFO 2017-06-25 14:31:38,410 main.py:50] epoch 70, training loss: 22795.98, average training loss: 39546.95, base loss: 23368.07
[INFO 2017-06-25 14:31:42,730 main.py:50] epoch 71, training loss: 21204.44, average training loss: 39292.19, base loss: 23340.29
[INFO 2017-06-25 14:31:47,022 main.py:50] epoch 72, training loss: 25329.46, average training loss: 39100.92, base loss: 23374.26
[INFO 2017-06-25 14:31:51,321 main.py:50] epoch 73, training loss: 25166.90, average training loss: 38912.62, base loss: 23403.19
[INFO 2017-06-25 14:31:55,594 main.py:50] epoch 74, training loss: 16692.99, average training loss: 38616.36, base loss: 23308.52
[INFO 2017-06-25 14:31:59,921 main.py:50] epoch 75, training loss: 25259.71, average training loss: 38440.62, base loss: 23340.94
[INFO 2017-06-25 14:32:04,221 main.py:50] epoch 76, training loss: 24668.53, average training loss: 38261.76, base loss: 23363.89
[INFO 2017-06-25 14:32:08,514 main.py:50] epoch 77, training loss: 21449.57, average training loss: 38046.22, base loss: 23346.79
[INFO 2017-06-25 14:32:12,796 main.py:50] epoch 78, training loss: 25920.05, average training loss: 37892.72, base loss: 23383.22
[INFO 2017-06-25 14:32:17,073 main.py:50] epoch 79, training loss: 25557.24, average training loss: 37738.53, base loss: 23413.28
[INFO 2017-06-25 14:32:21,393 main.py:50] epoch 80, training loss: 23437.16, average training loss: 37561.97, base loss: 23420.98
[INFO 2017-06-25 14:32:25,673 main.py:50] epoch 81, training loss: 21683.35, average training loss: 37368.33, base loss: 23402.49
[INFO 2017-06-25 14:32:30,003 main.py:50] epoch 82, training loss: 21763.48, average training loss: 37180.32, base loss: 23387.04
[INFO 2017-06-25 14:32:34,298 main.py:50] epoch 83, training loss: 17520.03, average training loss: 36946.27, base loss: 23318.43
[INFO 2017-06-25 14:32:38,610 main.py:50] epoch 84, training loss: 21657.80, average training loss: 36766.40, base loss: 23304.40
[INFO 2017-06-25 14:32:42,878 main.py:50] epoch 85, training loss: 20286.69, average training loss: 36574.78, base loss: 23276.12
[INFO 2017-06-25 14:32:47,154 main.py:50] epoch 86, training loss: 22736.50, average training loss: 36415.72, base loss: 23278.73
[INFO 2017-06-25 14:32:51,439 main.py:50] epoch 87, training loss: 23203.58, average training loss: 36265.58, base loss: 23284.51
[INFO 2017-06-25 14:32:55,715 main.py:50] epoch 88, training loss: 26147.41, average training loss: 36151.89, base loss: 23327.54
[INFO 2017-06-25 14:32:59,995 main.py:50] epoch 89, training loss: 19805.93, average training loss: 35970.27, base loss: 23293.41
[INFO 2017-06-25 14:33:04,275 main.py:50] epoch 90, training loss: 22038.24, average training loss: 35817.17, base loss: 23288.76
[INFO 2017-06-25 14:33:08,584 main.py:50] epoch 91, training loss: 28310.85, average training loss: 35735.58, base loss: 23356.33
[INFO 2017-06-25 14:33:12,890 main.py:50] epoch 92, training loss: 22898.80, average training loss: 35597.55, base loss: 23359.99
[INFO 2017-06-25 14:33:17,191 main.py:50] epoch 93, training loss: 18578.07, average training loss: 35416.49, base loss: 23312.99
[INFO 2017-06-25 14:33:21,474 main.py:50] epoch 94, training loss: 19381.01, average training loss: 35247.70, base loss: 23269.31
[INFO 2017-06-25 14:33:25,753 main.py:50] epoch 95, training loss: 24987.38, average training loss: 35140.82, base loss: 23296.89
[INFO 2017-06-25 14:33:30,047 main.py:50] epoch 96, training loss: 27589.25, average training loss: 35062.97, base loss: 23351.61
[INFO 2017-06-25 14:33:34,344 main.py:50] epoch 97, training loss: 21482.82, average training loss: 34924.39, base loss: 23340.72
[INFO 2017-06-25 14:33:38,597 main.py:50] epoch 98, training loss: 17285.13, average training loss: 34746.22, base loss: 23285.74
[INFO 2017-06-25 14:33:42,908 main.py:50] epoch 99, training loss: 24541.74, average training loss: 34644.18, base loss: 23306.66
[INFO 2017-06-25 14:33:47,188 main.py:50] epoch 100, training loss: 24471.84, average training loss: 34543.46, base loss: 23326.92
[INFO 2017-06-25 14:33:51,468 main.py:50] epoch 101, training loss: 20135.52, average training loss: 34402.20, base loss: 23302.43
[INFO 2017-06-25 14:33:55,759 main.py:50] epoch 102, training loss: 22134.02, average training loss: 34283.10, base loss: 23299.56
[INFO 2017-06-25 14:34:00,039 main.py:50] epoch 103, training loss: 21872.45, average training loss: 34163.76, base loss: 23295.02
[INFO 2017-06-25 14:34:04,346 main.py:50] epoch 104, training loss: 23355.56, average training loss: 34060.83, base loss: 23303.40
[INFO 2017-06-25 14:34:08,635 main.py:50] epoch 105, training loss: 23034.36, average training loss: 33956.80, base loss: 23313.21
[INFO 2017-06-25 14:34:12,930 main.py:50] epoch 106, training loss: 26477.92, average training loss: 33886.91, base loss: 23354.95
[INFO 2017-06-25 14:34:17,194 main.py:50] epoch 107, training loss: 25753.59, average training loss: 33811.60, base loss: 23384.86
[INFO 2017-06-25 14:34:21,473 main.py:50] epoch 108, training loss: 26524.72, average training loss: 33744.75, base loss: 23421.68
[INFO 2017-06-25 14:34:25,795 main.py:50] epoch 109, training loss: 22867.92, average training loss: 33645.87, base loss: 23422.28
[INFO 2017-06-25 14:34:30,079 main.py:50] epoch 110, training loss: 21411.53, average training loss: 33535.65, base loss: 23412.89
[INFO 2017-06-25 14:34:34,332 main.py:50] epoch 111, training loss: 20492.00, average training loss: 33419.19, base loss: 23393.23
[INFO 2017-06-25 14:34:38,606 main.py:50] epoch 112, training loss: 19662.83, average training loss: 33297.45, base loss: 23363.62
[INFO 2017-06-25 14:34:42,882 main.py:50] epoch 113, training loss: 25578.83, average training loss: 33229.74, base loss: 23389.71
[INFO 2017-06-25 14:34:47,164 main.py:50] epoch 114, training loss: 24243.34, average training loss: 33151.60, base loss: 23406.02
[INFO 2017-06-25 14:34:51,448 main.py:50] epoch 115, training loss: 19876.58, average training loss: 33037.16, base loss: 23383.79
[INFO 2017-06-25 14:34:55,703 main.py:50] epoch 116, training loss: 22100.42, average training loss: 32943.68, base loss: 23381.39
[INFO 2017-06-25 14:34:59,988 main.py:50] epoch 117, training loss: 21021.51, average training loss: 32842.65, base loss: 23373.12
[INFO 2017-06-25 14:35:04,244 main.py:50] epoch 118, training loss: 18739.12, average training loss: 32724.13, base loss: 23336.91
[INFO 2017-06-25 14:35:08,516 main.py:50] epoch 119, training loss: 23298.83, average training loss: 32645.59, base loss: 23334.93
[INFO 2017-06-25 14:35:12,788 main.py:50] epoch 120, training loss: 23120.60, average training loss: 32566.87, base loss: 23339.58
[INFO 2017-06-25 14:35:17,051 main.py:50] epoch 121, training loss: 16716.03, average training loss: 32436.94, base loss: 23286.76
[INFO 2017-06-25 14:35:21,335 main.py:50] epoch 122, training loss: 20283.14, average training loss: 32338.13, base loss: 23268.88
[INFO 2017-06-25 14:35:25,678 main.py:50] epoch 123, training loss: 21622.60, average training loss: 32251.72, base loss: 23262.51
[INFO 2017-06-25 14:35:29,948 main.py:50] epoch 124, training loss: 26342.75, average training loss: 32204.44, base loss: 23293.39
[INFO 2017-06-25 14:35:34,204 main.py:50] epoch 125, training loss: 21285.07, average training loss: 32117.78, base loss: 23286.04
[INFO 2017-06-25 14:35:38,494 main.py:50] epoch 126, training loss: 23857.48, average training loss: 32052.74, base loss: 23297.94
[INFO 2017-06-25 14:35:42,765 main.py:50] epoch 127, training loss: 27719.43, average training loss: 32018.89, base loss: 23343.38
[INFO 2017-06-25 14:35:47,046 main.py:50] epoch 128, training loss: 24335.12, average training loss: 31959.32, base loss: 23361.34
[INFO 2017-06-25 14:35:51,354 main.py:50] epoch 129, training loss: 25142.35, average training loss: 31906.88, base loss: 23384.11
[INFO 2017-06-25 14:35:55,628 main.py:50] epoch 130, training loss: 23421.17, average training loss: 31842.11, base loss: 23391.76
[INFO 2017-06-25 14:35:59,920 main.py:50] epoch 131, training loss: 27253.18, average training loss: 31807.34, base loss: 23429.59
[INFO 2017-06-25 14:36:04,183 main.py:50] epoch 132, training loss: 21500.29, average training loss: 31729.85, base loss: 23422.31
[INFO 2017-06-25 14:36:08,464 main.py:50] epoch 133, training loss: 35022.46, average training loss: 31754.42, base loss: 23531.39
[INFO 2017-06-25 14:36:12,733 main.py:50] epoch 134, training loss: 24892.40, average training loss: 31703.59, base loss: 23550.98
[INFO 2017-06-25 14:36:17,033 main.py:50] epoch 135, training loss: 24171.32, average training loss: 31648.20, base loss: 23566.35
[INFO 2017-06-25 14:36:21,336 main.py:50] epoch 136, training loss: 21693.51, average training loss: 31575.54, base loss: 23560.54
[INFO 2017-06-25 14:36:25,587 main.py:50] epoch 137, training loss: 20836.76, average training loss: 31497.73, base loss: 23550.16
[INFO 2017-06-25 14:36:29,865 main.py:50] epoch 138, training loss: 24251.55, average training loss: 31445.59, base loss: 23566.03
[INFO 2017-06-25 14:36:34,143 main.py:50] epoch 139, training loss: 23731.06, average training loss: 31390.49, base loss: 23575.81
[INFO 2017-06-25 14:36:38,431 main.py:50] epoch 140, training loss: 17921.16, average training loss: 31294.96, base loss: 23536.97
[INFO 2017-06-25 14:36:42,698 main.py:50] epoch 141, training loss: 18644.17, average training loss: 31205.87, base loss: 23500.50
[INFO 2017-06-25 14:36:46,966 main.py:50] epoch 142, training loss: 17610.57, average training loss: 31110.80, base loss: 23464.03
[INFO 2017-06-25 14:36:51,265 main.py:50] epoch 143, training loss: 20858.28, average training loss: 31039.60, base loss: 23451.41
[INFO 2017-06-25 14:36:55,609 main.py:50] epoch 144, training loss: 21082.35, average training loss: 30970.93, base loss: 23439.68
[INFO 2017-06-25 14:36:59,921 main.py:50] epoch 145, training loss: 23833.60, average training loss: 30922.05, base loss: 23452.72
[INFO 2017-06-25 14:37:04,216 main.py:50] epoch 146, training loss: 24347.29, average training loss: 30877.32, base loss: 23464.80
[INFO 2017-06-25 14:37:08,521 main.py:50] epoch 147, training loss: 21057.38, average training loss: 30810.97, base loss: 23453.81
[INFO 2017-06-25 14:37:12,824 main.py:50] epoch 148, training loss: 27614.82, average training loss: 30789.52, base loss: 23492.23
[INFO 2017-06-25 14:37:17,115 main.py:50] epoch 149, training loss: 22525.63, average training loss: 30734.43, base loss: 23490.51
[INFO 2017-06-25 14:37:21,432 main.py:50] epoch 150, training loss: 18819.07, average training loss: 30655.52, base loss: 23465.85
[INFO 2017-06-25 14:37:25,725 main.py:50] epoch 151, training loss: 22724.19, average training loss: 30603.34, base loss: 23469.77
[INFO 2017-06-25 14:37:30,023 main.py:50] epoch 152, training loss: 21072.73, average training loss: 30541.05, base loss: 23458.73
[INFO 2017-06-25 14:37:34,306 main.py:50] epoch 153, training loss: 24030.82, average training loss: 30498.77, base loss: 23473.79
[INFO 2017-06-25 14:37:38,602 main.py:50] epoch 154, training loss: 16139.71, average training loss: 30406.13, base loss: 23427.95
[INFO 2017-06-25 14:37:42,914 main.py:50] epoch 155, training loss: 22434.17, average training loss: 30355.03, base loss: 23432.78
[INFO 2017-06-25 14:37:47,250 main.py:50] epoch 156, training loss: 21554.82, average training loss: 30298.98, base loss: 23424.74
[INFO 2017-06-25 14:37:51,551 main.py:50] epoch 157, training loss: 21198.81, average training loss: 30241.38, base loss: 23418.73
[INFO 2017-06-25 14:37:55,842 main.py:50] epoch 158, training loss: 18585.76, average training loss: 30168.08, base loss: 23392.69
[INFO 2017-06-25 14:38:00,137 main.py:50] epoch 159, training loss: 18569.89, average training loss: 30095.59, base loss: 23371.79
[INFO 2017-06-25 14:38:04,452 main.py:50] epoch 160, training loss: 19391.41, average training loss: 30029.10, base loss: 23353.14
[INFO 2017-06-25 14:38:08,751 main.py:50] epoch 161, training loss: 22122.24, average training loss: 29980.29, base loss: 23353.35
[INFO 2017-06-25 14:38:13,016 main.py:50] epoch 162, training loss: 31567.22, average training loss: 29990.03, base loss: 23417.60
[INFO 2017-06-25 14:38:17,296 main.py:50] epoch 163, training loss: 21599.87, average training loss: 29938.87, base loss: 23414.75
[INFO 2017-06-25 14:38:21,561 main.py:50] epoch 164, training loss: 19925.37, average training loss: 29878.18, base loss: 23398.71
[INFO 2017-06-25 14:38:25,845 main.py:50] epoch 165, training loss: 20913.15, average training loss: 29824.18, base loss: 23394.07
[INFO 2017-06-25 14:38:30,112 main.py:50] epoch 166, training loss: 19137.54, average training loss: 29760.18, base loss: 23371.57
[INFO 2017-06-25 14:38:34,410 main.py:50] epoch 167, training loss: 21432.76, average training loss: 29710.62, base loss: 23372.21
[INFO 2017-06-25 14:38:38,738 main.py:50] epoch 168, training loss: 19066.27, average training loss: 29647.63, base loss: 23353.66
[INFO 2017-06-25 14:38:43,044 main.py:50] epoch 169, training loss: 24301.78, average training loss: 29616.19, base loss: 23365.53
[INFO 2017-06-25 14:38:47,371 main.py:50] epoch 170, training loss: 24910.74, average training loss: 29588.67, base loss: 23383.15
[INFO 2017-06-25 14:38:51,663 main.py:50] epoch 171, training loss: 21157.11, average training loss: 29539.65, base loss: 23377.65
[INFO 2017-06-25 14:38:55,979 main.py:50] epoch 172, training loss: 21936.60, average training loss: 29495.70, base loss: 23378.67
[INFO 2017-06-25 14:39:00,247 main.py:50] epoch 173, training loss: 22847.08, average training loss: 29457.49, base loss: 23384.66
[INFO 2017-06-25 14:39:04,538 main.py:50] epoch 174, training loss: 17672.52, average training loss: 29390.15, base loss: 23354.75
[INFO 2017-06-25 14:39:08,826 main.py:50] epoch 175, training loss: 29231.98, average training loss: 29389.25, base loss: 23402.27
[INFO 2017-06-25 14:39:13,150 main.py:50] epoch 176, training loss: 27504.79, average training loss: 29378.60, base loss: 23434.37
[INFO 2017-06-25 14:39:17,450 main.py:50] epoch 177, training loss: 17913.04, average training loss: 29314.19, base loss: 23409.35
[INFO 2017-06-25 14:39:21,733 main.py:50] epoch 178, training loss: 16510.00, average training loss: 29242.66, base loss: 23378.15
[INFO 2017-06-25 14:39:26,018 main.py:50] epoch 179, training loss: 24812.16, average training loss: 29218.04, base loss: 23394.37
[INFO 2017-06-25 14:39:30,319 main.py:50] epoch 180, training loss: 25973.60, average training loss: 29200.12, base loss: 23417.19
[INFO 2017-06-25 14:39:34,619 main.py:50] epoch 181, training loss: 22431.40, average training loss: 29162.93, base loss: 23421.64
[INFO 2017-06-25 14:39:38,904 main.py:50] epoch 182, training loss: 20187.11, average training loss: 29113.88, base loss: 23406.63
[INFO 2017-06-25 14:39:43,175 main.py:50] epoch 183, training loss: 14175.34, average training loss: 29032.69, base loss: 23359.40
[INFO 2017-06-25 14:39:47,442 main.py:50] epoch 184, training loss: 20916.41, average training loss: 28988.82, base loss: 23353.20
[INFO 2017-06-25 14:39:51,722 main.py:50] epoch 185, training loss: 21430.66, average training loss: 28948.18, base loss: 23354.62
[INFO 2017-06-25 14:39:56,022 main.py:50] epoch 186, training loss: 22983.17, average training loss: 28916.28, base loss: 23361.29
[INFO 2017-06-25 14:40:00,348 main.py:50] epoch 187, training loss: 20645.46, average training loss: 28872.29, base loss: 23355.56
[INFO 2017-06-25 14:40:04,613 main.py:50] epoch 188, training loss: 24275.52, average training loss: 28847.97, base loss: 23368.37
[INFO 2017-06-25 14:40:08,883 main.py:50] epoch 189, training loss: 21242.41, average training loss: 28807.94, base loss: 23360.01
[INFO 2017-06-25 14:40:13,168 main.py:50] epoch 190, training loss: 21319.70, average training loss: 28768.73, base loss: 23355.22
[INFO 2017-06-25 14:40:17,452 main.py:50] epoch 191, training loss: 22026.23, average training loss: 28733.62, base loss: 23353.53
[INFO 2017-06-25 14:40:21,740 main.py:50] epoch 192, training loss: 26842.25, average training loss: 28723.82, base loss: 23379.15
[INFO 2017-06-25 14:40:26,019 main.py:50] epoch 193, training loss: 24156.53, average training loss: 28700.27, base loss: 23393.01
[INFO 2017-06-25 14:40:30,293 main.py:50] epoch 194, training loss: 22026.66, average training loss: 28666.05, base loss: 23390.87
[INFO 2017-06-25 14:40:34,568 main.py:50] epoch 195, training loss: 19717.04, average training loss: 28620.39, base loss: 23377.09
[INFO 2017-06-25 14:40:38,829 main.py:50] epoch 196, training loss: 23679.38, average training loss: 28595.31, base loss: 23384.56
[INFO 2017-06-25 14:40:43,143 main.py:50] epoch 197, training loss: 21943.56, average training loss: 28561.72, base loss: 23384.29
[INFO 2017-06-25 14:40:47,445 main.py:50] epoch 198, training loss: 20339.28, average training loss: 28520.40, base loss: 23373.45
[INFO 2017-06-25 14:40:51,748 main.py:50] epoch 199, training loss: 22806.40, average training loss: 28491.83, base loss: 23381.70
[INFO 2017-06-25 14:40:56,030 main.py:50] epoch 200, training loss: 19036.52, average training loss: 28444.79, base loss: 23365.99
[INFO 2017-06-25 14:41:00,318 main.py:50] epoch 201, training loss: 26761.65, average training loss: 28436.45, base loss: 23391.31
[INFO 2017-06-25 14:41:04,615 main.py:50] epoch 202, training loss: 18891.93, average training loss: 28389.44, base loss: 23374.37
[INFO 2017-06-25 14:41:08,923 main.py:50] epoch 203, training loss: 22588.28, average training loss: 28361.00, base loss: 23378.91
[INFO 2017-06-25 14:41:13,243 main.py:50] epoch 204, training loss: 20647.72, average training loss: 28323.37, base loss: 23375.28
[INFO 2017-06-25 14:41:17,554 main.py:50] epoch 205, training loss: 22917.40, average training loss: 28297.13, base loss: 23381.65
[INFO 2017-06-25 14:41:21,856 main.py:50] epoch 206, training loss: 23642.77, average training loss: 28274.65, base loss: 23393.97
[INFO 2017-06-25 14:41:26,176 main.py:50] epoch 207, training loss: 21067.58, average training loss: 28240.00, base loss: 23391.07
[INFO 2017-06-25 14:41:30,488 main.py:50] epoch 208, training loss: 19209.82, average training loss: 28196.79, base loss: 23375.81
[INFO 2017-06-25 14:41:34,793 main.py:50] epoch 209, training loss: 23388.35, average training loss: 28173.89, base loss: 23385.76
[INFO 2017-06-25 14:41:39,098 main.py:50] epoch 210, training loss: 23979.36, average training loss: 28154.01, base loss: 23397.89
[INFO 2017-06-25 14:41:43,443 main.py:50] epoch 211, training loss: 23119.08, average training loss: 28130.26, base loss: 23406.49
[INFO 2017-06-25 14:41:47,737 main.py:50] epoch 212, training loss: 19024.10, average training loss: 28087.51, base loss: 23390.86
[INFO 2017-06-25 14:41:52,088 main.py:50] epoch 213, training loss: 19698.62, average training loss: 28048.31, base loss: 23381.42
[INFO 2017-06-25 14:41:56,421 main.py:50] epoch 214, training loss: 31100.34, average training loss: 28062.51, base loss: 23426.46
[INFO 2017-06-25 14:42:00,755 main.py:50] epoch 215, training loss: 19868.83, average training loss: 28024.57, base loss: 23416.62
[INFO 2017-06-25 14:42:05,058 main.py:50] epoch 216, training loss: 16493.90, average training loss: 27971.44, base loss: 23387.67
[INFO 2017-06-25 14:42:09,381 main.py:50] epoch 217, training loss: 20379.70, average training loss: 27936.61, base loss: 23382.86
[INFO 2017-06-25 14:42:13,693 main.py:50] epoch 218, training loss: 21294.01, average training loss: 27906.28, base loss: 23382.41
[INFO 2017-06-25 14:42:17,991 main.py:50] epoch 219, training loss: 22279.18, average training loss: 27880.70, base loss: 23386.24
[INFO 2017-06-25 14:42:22,297 main.py:50] epoch 220, training loss: 19530.86, average training loss: 27842.92, base loss: 23374.96
[INFO 2017-06-25 14:42:26,591 main.py:50] epoch 221, training loss: 18486.40, average training loss: 27800.77, base loss: 23361.18
[INFO 2017-06-25 14:42:30,894 main.py:50] epoch 222, training loss: 16979.91, average training loss: 27752.25, base loss: 23336.03
[INFO 2017-06-25 14:42:35,259 main.py:50] epoch 223, training loss: 18577.82, average training loss: 27711.29, base loss: 23319.65
[INFO 2017-06-25 14:42:39,548 main.py:50] epoch 224, training loss: 19076.68, average training loss: 27672.92, base loss: 23307.22
[INFO 2017-06-25 14:42:43,861 main.py:50] epoch 225, training loss: 20774.56, average training loss: 27642.39, base loss: 23304.26
[INFO 2017-06-25 14:42:48,185 main.py:50] epoch 226, training loss: 20797.98, average training loss: 27612.24, base loss: 23299.53
[INFO 2017-06-25 14:42:52,523 main.py:50] epoch 227, training loss: 18666.31, average training loss: 27573.01, base loss: 23284.61
[INFO 2017-06-25 14:42:56,869 main.py:50] epoch 228, training loss: 23884.65, average training loss: 27556.90, base loss: 23293.64
[INFO 2017-06-25 14:43:01,156 main.py:50] epoch 229, training loss: 25843.88, average training loss: 27549.45, base loss: 23314.30
[INFO 2017-06-25 14:43:05,451 main.py:50] epoch 230, training loss: 25771.85, average training loss: 27541.76, base loss: 23332.39
[INFO 2017-06-25 14:43:09,762 main.py:50] epoch 231, training loss: 21263.17, average training loss: 27514.69, base loss: 23333.06
[INFO 2017-06-25 14:43:14,065 main.py:50] epoch 232, training loss: 24976.72, average training loss: 27503.80, base loss: 23348.57
[INFO 2017-06-25 14:43:18,353 main.py:50] epoch 233, training loss: 18985.55, average training loss: 27467.40, base loss: 23336.54
[INFO 2017-06-25 14:43:22,643 main.py:50] epoch 234, training loss: 25179.75, average training loss: 27457.66, base loss: 23351.98
[INFO 2017-06-25 14:43:26,926 main.py:50] epoch 235, training loss: 23456.14, average training loss: 27440.71, base loss: 23362.96
[INFO 2017-06-25 14:43:31,233 main.py:50] epoch 236, training loss: 30244.04, average training loss: 27452.54, base loss: 23404.94
[INFO 2017-06-25 14:43:35,552 main.py:50] epoch 237, training loss: 25228.33, average training loss: 27443.19, base loss: 23423.11
[INFO 2017-06-25 14:43:39,839 main.py:50] epoch 238, training loss: 22933.06, average training loss: 27424.32, base loss: 23430.01
[INFO 2017-06-25 14:43:44,146 main.py:50] epoch 239, training loss: 23116.67, average training loss: 27406.37, base loss: 23441.59
[INFO 2017-06-25 14:43:48,422 main.py:50] epoch 240, training loss: 21124.70, average training loss: 27380.31, base loss: 23436.12
[INFO 2017-06-25 14:43:52,735 main.py:50] epoch 241, training loss: 18418.39, average training loss: 27343.27, base loss: 23417.12
[INFO 2017-06-25 14:43:57,015 main.py:50] epoch 242, training loss: 24890.23, average training loss: 27333.18, base loss: 23435.85
[INFO 2017-06-25 14:44:01,343 main.py:50] epoch 243, training loss: 15432.45, average training loss: 27284.41, base loss: 23408.16
[INFO 2017-06-25 14:44:05,630 main.py:50] epoch 244, training loss: 22400.25, average training loss: 27264.47, base loss: 23413.14
[INFO 2017-06-25 14:44:09,937 main.py:50] epoch 245, training loss: 21392.14, average training loss: 27240.60, base loss: 23411.10
[INFO 2017-06-25 14:44:14,219 main.py:50] epoch 246, training loss: 18104.82, average training loss: 27203.61, base loss: 23392.88
[INFO 2017-06-25 14:44:18,505 main.py:50] epoch 247, training loss: 19371.44, average training loss: 27172.03, base loss: 23379.48
[INFO 2017-06-25 14:44:22,795 main.py:50] epoch 248, training loss: 18829.52, average training loss: 27138.53, base loss: 23368.39
[INFO 2017-06-25 14:44:27,073 main.py:50] epoch 249, training loss: 24902.50, average training loss: 27129.58, base loss: 23383.43
[INFO 2017-06-25 14:44:31,358 main.py:50] epoch 250, training loss: 25395.85, average training loss: 27122.67, base loss: 23400.00
[INFO 2017-06-25 14:44:35,655 main.py:50] epoch 251, training loss: 26253.46, average training loss: 27119.23, base loss: 23421.32
[INFO 2017-06-25 14:44:39,968 main.py:50] epoch 252, training loss: 20092.14, average training loss: 27091.45, base loss: 23412.52
[INFO 2017-06-25 14:44:44,281 main.py:50] epoch 253, training loss: 15889.54, average training loss: 27047.35, base loss: 23388.11
[INFO 2017-06-25 14:44:48,572 main.py:50] epoch 254, training loss: 21478.38, average training loss: 27025.51, base loss: 23390.89
[INFO 2017-06-25 14:44:52,870 main.py:50] epoch 255, training loss: 22150.57, average training loss: 27006.47, base loss: 23395.13
[INFO 2017-06-25 14:44:57,188 main.py:50] epoch 256, training loss: 20545.74, average training loss: 26981.33, base loss: 23390.41
[INFO 2017-06-25 14:45:01,510 main.py:50] epoch 257, training loss: 24922.11, average training loss: 26973.35, base loss: 23406.89
[INFO 2017-06-25 14:45:05,807 main.py:50] epoch 258, training loss: 15802.76, average training loss: 26930.22, base loss: 23381.16
[INFO 2017-06-25 14:45:10,096 main.py:50] epoch 259, training loss: 23840.69, average training loss: 26918.33, base loss: 23390.28
[INFO 2017-06-25 14:45:14,401 main.py:50] epoch 260, training loss: 20352.43, average training loss: 26893.18, base loss: 23382.08
[INFO 2017-06-25 14:45:18,715 main.py:50] epoch 261, training loss: 20463.52, average training loss: 26868.64, base loss: 23378.69
[INFO 2017-06-25 14:45:23,004 main.py:50] epoch 262, training loss: 26128.34, average training loss: 26865.82, base loss: 23395.97
[INFO 2017-06-25 14:45:27,296 main.py:50] epoch 263, training loss: 24891.08, average training loss: 26858.34, base loss: 23411.60
[INFO 2017-06-25 14:45:31,644 main.py:50] epoch 264, training loss: 22788.63, average training loss: 26842.98, base loss: 23418.55
[INFO 2017-06-25 14:45:35,912 main.py:50] epoch 265, training loss: 20073.62, average training loss: 26817.54, base loss: 23407.58
[INFO 2017-06-25 14:45:40,209 main.py:50] epoch 266, training loss: 22379.17, average training loss: 26800.91, base loss: 23412.05
[INFO 2017-06-25 14:45:44,540 main.py:50] epoch 267, training loss: 25501.54, average training loss: 26796.06, base loss: 23431.46
[INFO 2017-06-25 14:45:48,820 main.py:50] epoch 268, training loss: 19260.70, average training loss: 26768.05, base loss: 23419.86
[INFO 2017-06-25 14:45:53,090 main.py:50] epoch 269, training loss: 16911.43, average training loss: 26731.55, base loss: 23399.16
[INFO 2017-06-25 14:45:57,369 main.py:50] epoch 270, training loss: 18446.97, average training loss: 26700.97, base loss: 23385.64
[INFO 2017-06-25 14:46:01,636 main.py:50] epoch 271, training loss: 26458.57, average training loss: 26700.08, base loss: 23402.72
[INFO 2017-06-25 14:46:05,927 main.py:50] epoch 272, training loss: 20734.32, average training loss: 26678.23, base loss: 23401.38
[INFO 2017-06-25 14:46:10,202 main.py:50] epoch 273, training loss: 28241.86, average training loss: 26683.94, base loss: 23424.48
[INFO 2017-06-25 14:46:14,494 main.py:50] epoch 274, training loss: 24814.50, average training loss: 26677.14, base loss: 23438.67
[INFO 2017-06-25 14:46:18,784 main.py:50] epoch 275, training loss: 26419.80, average training loss: 26676.21, base loss: 23456.65
[INFO 2017-06-25 14:46:23,063 main.py:50] epoch 276, training loss: 18011.26, average training loss: 26644.93, base loss: 23438.54
[INFO 2017-06-25 14:46:27,345 main.py:50] epoch 277, training loss: 24453.52, average training loss: 26637.04, base loss: 23450.83
[INFO 2017-06-25 14:46:31,612 main.py:50] epoch 278, training loss: 25800.39, average training loss: 26634.04, base loss: 23466.73
[INFO 2017-06-25 14:46:35,911 main.py:50] epoch 279, training loss: 29637.36, average training loss: 26644.77, base loss: 23500.18
[INFO 2017-06-25 14:46:40,190 main.py:50] epoch 280, training loss: 23009.81, average training loss: 26631.83, base loss: 23504.82
[INFO 2017-06-25 14:46:44,494 main.py:50] epoch 281, training loss: 28391.33, average training loss: 26638.07, base loss: 23528.54
[INFO 2017-06-25 14:46:48,761 main.py:50] epoch 282, training loss: 19608.05, average training loss: 26613.23, base loss: 23519.12
[INFO 2017-06-25 14:46:53,032 main.py:50] epoch 283, training loss: 19989.26, average training loss: 26589.91, base loss: 23512.55
[INFO 2017-06-25 14:46:57,309 main.py:50] epoch 284, training loss: 23345.31, average training loss: 26578.52, base loss: 23523.18
[INFO 2017-06-25 14:47:01,602 main.py:50] epoch 285, training loss: 23862.14, average training loss: 26569.03, base loss: 23530.31
[INFO 2017-06-25 14:47:05,872 main.py:50] epoch 286, training loss: 21805.08, average training loss: 26552.43, base loss: 23530.28
[INFO 2017-06-25 14:47:10,146 main.py:50] epoch 287, training loss: 20449.97, average training loss: 26531.24, base loss: 23522.55
[INFO 2017-06-25 14:47:14,427 main.py:50] epoch 288, training loss: 17264.99, average training loss: 26499.18, base loss: 23507.12
[INFO 2017-06-25 14:47:18,713 main.py:50] epoch 289, training loss: 21415.55, average training loss: 26481.65, base loss: 23504.90
[INFO 2017-06-25 14:47:23,001 main.py:50] epoch 290, training loss: 21350.46, average training loss: 26464.01, base loss: 23501.87
[INFO 2017-06-25 14:47:27,296 main.py:50] epoch 291, training loss: 18891.31, average training loss: 26438.08, base loss: 23490.17
[INFO 2017-06-25 14:47:31,601 main.py:50] epoch 292, training loss: 23474.46, average training loss: 26427.96, base loss: 23500.45
[INFO 2017-06-25 14:47:35,894 main.py:50] epoch 293, training loss: 18698.97, average training loss: 26401.68, base loss: 23491.36
[INFO 2017-06-25 14:47:40,205 main.py:50] epoch 294, training loss: 21083.18, average training loss: 26383.65, base loss: 23487.16
[INFO 2017-06-25 14:47:44,481 main.py:50] epoch 295, training loss: 24131.42, average training loss: 26376.04, base loss: 23496.01
[INFO 2017-06-25 14:47:48,801 main.py:50] epoch 296, training loss: 24432.30, average training loss: 26369.49, base loss: 23507.42
[INFO 2017-06-25 14:47:53,109 main.py:50] epoch 297, training loss: 20178.72, average training loss: 26348.72, base loss: 23501.24
[INFO 2017-06-25 14:47:57,428 main.py:50] epoch 298, training loss: 23286.69, average training loss: 26338.48, base loss: 23509.72
[INFO 2017-06-25 14:48:01,728 main.py:50] epoch 299, training loss: 20360.72, average training loss: 26318.55, base loss: 23507.11
[INFO 2017-06-25 14:48:05,991 main.py:50] epoch 300, training loss: 24431.78, average training loss: 26312.28, base loss: 23519.65
[INFO 2017-06-25 14:48:10,276 main.py:50] epoch 301, training loss: 23617.27, average training loss: 26303.36, base loss: 23526.54
[INFO 2017-06-25 14:48:14,540 main.py:50] epoch 302, training loss: 19654.96, average training loss: 26281.42, base loss: 23519.48
[INFO 2017-06-25 14:48:18,837 main.py:50] epoch 303, training loss: 19007.34, average training loss: 26257.49, base loss: 23508.06
[INFO 2017-06-25 14:48:23,099 main.py:50] epoch 304, training loss: 20506.71, average training loss: 26238.63, base loss: 23499.78
[INFO 2017-06-25 14:48:27,368 main.py:50] epoch 305, training loss: 29513.61, average training loss: 26249.34, base loss: 23528.69
[INFO 2017-06-25 14:48:31,665 main.py:50] epoch 306, training loss: 19179.48, average training loss: 26226.31, base loss: 23519.11
[INFO 2017-06-25 14:48:35,941 main.py:50] epoch 307, training loss: 18826.87, average training loss: 26202.28, base loss: 23508.27
[INFO 2017-06-25 14:48:40,241 main.py:50] epoch 308, training loss: 21724.40, average training loss: 26187.79, base loss: 23510.55
[INFO 2017-06-25 14:48:44,524 main.py:50] epoch 309, training loss: 19899.80, average training loss: 26167.51, base loss: 23507.81
[INFO 2017-06-25 14:48:48,834 main.py:50] epoch 310, training loss: 26098.98, average training loss: 26167.29, base loss: 23523.62
[INFO 2017-06-25 14:48:53,118 main.py:50] epoch 311, training loss: 21938.49, average training loss: 26153.73, base loss: 23527.61
[INFO 2017-06-25 14:48:57,395 main.py:50] epoch 312, training loss: 21620.13, average training loss: 26139.25, base loss: 23529.22
[INFO 2017-06-25 14:49:01,662 main.py:50] epoch 313, training loss: 24376.50, average training loss: 26133.64, base loss: 23539.74
[INFO 2017-06-25 14:49:05,926 main.py:50] epoch 314, training loss: 19437.28, average training loss: 26112.38, base loss: 23535.50
[INFO 2017-06-25 14:49:10,214 main.py:50] epoch 315, training loss: 19685.22, average training loss: 26092.04, base loss: 23530.49
[INFO 2017-06-25 14:49:14,526 main.py:50] epoch 316, training loss: 19799.83, average training loss: 26072.19, base loss: 23525.67
[INFO 2017-06-25 14:49:18,824 main.py:50] epoch 317, training loss: 21185.17, average training loss: 26056.82, base loss: 23524.41
[INFO 2017-06-25 14:49:23,128 main.py:50] epoch 318, training loss: 23002.43, average training loss: 26047.25, base loss: 23529.61
[INFO 2017-06-25 14:49:27,406 main.py:50] epoch 319, training loss: 18816.99, average training loss: 26024.65, base loss: 23519.08
[INFO 2017-06-25 14:49:31,657 main.py:50] epoch 320, training loss: 21383.91, average training loss: 26010.20, base loss: 23518.51
[INFO 2017-06-25 14:49:35,918 main.py:50] epoch 321, training loss: 16774.29, average training loss: 25981.51, base loss: 23501.24
[INFO 2017-06-25 14:49:40,182 main.py:50] epoch 322, training loss: 26254.07, average training loss: 25982.36, base loss: 23518.86
[INFO 2017-06-25 14:49:44,468 main.py:50] epoch 323, training loss: 15715.26, average training loss: 25950.67, base loss: 23497.78
[INFO 2017-06-25 14:49:48,737 main.py:50] epoch 324, training loss: 17427.93, average training loss: 25924.44, base loss: 23485.89
[INFO 2017-06-25 14:49:53,021 main.py:50] epoch 325, training loss: 24129.96, average training loss: 25918.94, base loss: 23497.20
[INFO 2017-06-25 14:49:57,295 main.py:50] epoch 326, training loss: 21353.06, average training loss: 25904.98, base loss: 23498.93
[INFO 2017-06-25 14:50:01,583 main.py:50] epoch 327, training loss: 20134.83, average training loss: 25887.38, base loss: 23493.85
[INFO 2017-06-25 14:50:05,871 main.py:50] epoch 328, training loss: 19770.64, average training loss: 25868.79, base loss: 23487.73
[INFO 2017-06-25 14:50:10,169 main.py:50] epoch 329, training loss: 21436.76, average training loss: 25855.36, base loss: 23487.40
[INFO 2017-06-25 14:50:14,454 main.py:50] epoch 330, training loss: 17830.66, average training loss: 25831.12, base loss: 23475.30
[INFO 2017-06-25 14:50:18,755 main.py:50] epoch 331, training loss: 18681.53, average training loss: 25809.58, base loss: 23469.49
[INFO 2017-06-25 14:50:23,032 main.py:50] epoch 332, training loss: 20335.02, average training loss: 25793.14, base loss: 23466.03
[INFO 2017-06-25 14:50:27,315 main.py:50] epoch 333, training loss: 17821.20, average training loss: 25769.27, base loss: 23450.53
[INFO 2017-06-25 14:50:31,617 main.py:50] epoch 334, training loss: 18473.26, average training loss: 25747.50, base loss: 23438.94
[INFO 2017-06-25 14:50:35,894 main.py:50] epoch 335, training loss: 20132.39, average training loss: 25730.78, base loss: 23433.10
[INFO 2017-06-25 14:50:40,171 main.py:50] epoch 336, training loss: 18783.74, average training loss: 25710.17, base loss: 23423.01
[INFO 2017-06-25 14:50:44,484 main.py:50] epoch 337, training loss: 27197.99, average training loss: 25714.57, base loss: 23442.81
[INFO 2017-06-25 14:50:48,782 main.py:50] epoch 338, training loss: 22879.44, average training loss: 25706.21, base loss: 23447.41
[INFO 2017-06-25 14:50:53,067 main.py:50] epoch 339, training loss: 27547.47, average training loss: 25711.62, base loss: 23468.09
[INFO 2017-06-25 14:50:57,381 main.py:50] epoch 340, training loss: 18558.51, average training loss: 25690.65, base loss: 23459.34
[INFO 2017-06-25 14:51:01,663 main.py:50] epoch 341, training loss: 22842.11, average training loss: 25682.32, base loss: 23468.38
[INFO 2017-06-25 14:51:05,978 main.py:50] epoch 342, training loss: 19776.87, average training loss: 25665.10, base loss: 23465.44
[INFO 2017-06-25 14:51:10,286 main.py:50] epoch 343, training loss: 15902.13, average training loss: 25636.72, base loss: 23443.50
[INFO 2017-06-25 14:51:14,581 main.py:50] epoch 344, training loss: 19241.45, average training loss: 25618.18, base loss: 23440.80
[INFO 2017-06-25 14:51:18,877 main.py:50] epoch 345, training loss: 23509.69, average training loss: 25612.09, base loss: 23445.73
[INFO 2017-06-25 14:51:23,190 main.py:50] epoch 346, training loss: 23681.71, average training loss: 25606.53, base loss: 23450.87
[INFO 2017-06-25 14:51:27,492 main.py:50] epoch 347, training loss: 22225.00, average training loss: 25596.81, base loss: 23455.50
[INFO 2017-06-25 14:51:31,781 main.py:50] epoch 348, training loss: 22375.65, average training loss: 25587.58, base loss: 23462.22
[INFO 2017-06-25 14:51:36,071 main.py:50] epoch 349, training loss: 19177.04, average training loss: 25569.26, base loss: 23458.59
[INFO 2017-06-25 14:51:40,385 main.py:50] epoch 350, training loss: 21044.69, average training loss: 25556.37, base loss: 23458.60
[INFO 2017-06-25 14:51:44,674 main.py:50] epoch 351, training loss: 19448.21, average training loss: 25539.02, base loss: 23453.84
[INFO 2017-06-25 14:51:48,979 main.py:50] epoch 352, training loss: 19955.37, average training loss: 25523.20, base loss: 23450.76
[INFO 2017-06-25 14:51:53,292 main.py:50] epoch 353, training loss: 19159.96, average training loss: 25505.23, base loss: 23444.06
[INFO 2017-06-25 14:51:57,618 main.py:50] epoch 354, training loss: 24742.07, average training loss: 25503.08, base loss: 23454.31
[INFO 2017-06-25 14:52:01,926 main.py:50] epoch 355, training loss: 18958.23, average training loss: 25484.69, base loss: 23452.16
[INFO 2017-06-25 14:52:06,238 main.py:50] epoch 356, training loss: 22775.47, average training loss: 25477.10, base loss: 23460.01
[INFO 2017-06-25 14:52:10,546 main.py:50] epoch 357, training loss: 21625.29, average training loss: 25466.35, base loss: 23460.38
[INFO 2017-06-25 14:52:14,854 main.py:50] epoch 358, training loss: 25335.16, average training loss: 25465.98, base loss: 23475.25
[INFO 2017-06-25 14:52:19,192 main.py:50] epoch 359, training loss: 22226.70, average training loss: 25456.98, base loss: 23480.44
[INFO 2017-06-25 14:52:23,522 main.py:50] epoch 360, training loss: 21427.26, average training loss: 25445.82, base loss: 23483.86
[INFO 2017-06-25 14:52:27,865 main.py:50] epoch 361, training loss: 23017.48, average training loss: 25439.11, base loss: 23489.64
[INFO 2017-06-25 14:52:32,208 main.py:50] epoch 362, training loss: 17109.80, average training loss: 25416.17, base loss: 23474.84
[INFO 2017-06-25 14:52:36,512 main.py:50] epoch 363, training loss: 18173.20, average training loss: 25396.27, base loss: 23466.05
[INFO 2017-06-25 14:52:40,849 main.py:50] epoch 364, training loss: 20128.87, average training loss: 25381.84, base loss: 23462.58
[INFO 2017-06-25 14:52:45,168 main.py:50] epoch 365, training loss: 21196.12, average training loss: 25370.40, base loss: 23465.40
[INFO 2017-06-25 14:52:49,474 main.py:50] epoch 366, training loss: 18886.33, average training loss: 25352.73, base loss: 23461.08
[INFO 2017-06-25 14:52:53,790 main.py:50] epoch 367, training loss: 17682.74, average training loss: 25331.89, base loss: 23453.70
[INFO 2017-06-25 14:52:58,093 main.py:50] epoch 368, training loss: 21847.07, average training loss: 25322.45, base loss: 23459.32
[INFO 2017-06-25 14:53:02,386 main.py:50] epoch 369, training loss: 20672.69, average training loss: 25309.88, base loss: 23456.44
[INFO 2017-06-25 14:53:06,714 main.py:50] epoch 370, training loss: 18369.45, average training loss: 25291.17, base loss: 23448.58
[INFO 2017-06-25 14:53:11,021 main.py:50] epoch 371, training loss: 21810.54, average training loss: 25281.81, base loss: 23453.98
[INFO 2017-06-25 14:53:15,348 main.py:50] epoch 372, training loss: 22789.62, average training loss: 25275.13, base loss: 23462.33
[INFO 2017-06-25 14:53:19,633 main.py:50] epoch 373, training loss: 16817.40, average training loss: 25252.52, base loss: 23449.56
[INFO 2017-06-25 14:53:23,936 main.py:50] epoch 374, training loss: 18402.25, average training loss: 25234.25, base loss: 23440.84
[INFO 2017-06-25 14:53:28,250 main.py:50] epoch 375, training loss: 18985.61, average training loss: 25217.63, base loss: 23432.44
[INFO 2017-06-25 14:53:32,553 main.py:50] epoch 376, training loss: 19333.34, average training loss: 25202.02, base loss: 23428.93
[INFO 2017-06-25 14:53:36,928 main.py:50] epoch 377, training loss: 22786.30, average training loss: 25195.63, base loss: 23433.52
[INFO 2017-06-25 14:53:41,257 main.py:50] epoch 378, training loss: 21455.43, average training loss: 25185.77, base loss: 23434.42
[INFO 2017-06-25 14:53:45,579 main.py:50] epoch 379, training loss: 21770.10, average training loss: 25176.78, base loss: 23437.96
[INFO 2017-06-25 14:53:49,882 main.py:50] epoch 380, training loss: 14669.08, average training loss: 25149.20, base loss: 23419.11
[INFO 2017-06-25 14:53:54,185 main.py:50] epoch 381, training loss: 19747.04, average training loss: 25135.06, base loss: 23417.20
[INFO 2017-06-25 14:53:58,512 main.py:50] epoch 382, training loss: 23175.63, average training loss: 25129.94, base loss: 23424.49
[INFO 2017-06-25 14:54:02,839 main.py:50] epoch 383, training loss: 22218.21, average training loss: 25122.36, base loss: 23428.42
[INFO 2017-06-25 14:54:07,157 main.py:50] epoch 384, training loss: 24917.23, average training loss: 25121.82, base loss: 23442.45
[INFO 2017-06-25 14:54:11,447 main.py:50] epoch 385, training loss: 22470.51, average training loss: 25114.96, base loss: 23446.49
[INFO 2017-06-25 14:54:15,774 main.py:50] epoch 386, training loss: 18746.24, average training loss: 25098.50, base loss: 23444.16
[INFO 2017-06-25 14:54:20,086 main.py:50] epoch 387, training loss: 15975.31, average training loss: 25074.99, base loss: 23429.83
[INFO 2017-06-25 14:54:24,408 main.py:50] epoch 388, training loss: 17140.10, average training loss: 25054.59, base loss: 23420.21
[INFO 2017-06-25 14:54:28,737 main.py:50] epoch 389, training loss: 16857.80, average training loss: 25033.57, base loss: 23408.82
[INFO 2017-06-25 14:54:33,040 main.py:50] epoch 390, training loss: 22090.75, average training loss: 25026.04, base loss: 23412.87
[INFO 2017-06-25 14:54:37,332 main.py:50] epoch 391, training loss: 20533.73, average training loss: 25014.58, base loss: 23415.25
[INFO 2017-06-25 14:54:41,638 main.py:50] epoch 392, training loss: 17858.95, average training loss: 24996.38, base loss: 23405.70
[INFO 2017-06-25 14:54:45,951 main.py:50] epoch 393, training loss: 25816.32, average training loss: 24998.46, base loss: 23422.43
[INFO 2017-06-25 14:54:50,229 main.py:50] epoch 394, training loss: 19407.04, average training loss: 24984.30, base loss: 23419.30
[INFO 2017-06-25 14:54:54,521 main.py:50] epoch 395, training loss: 25474.58, average training loss: 24985.54, base loss: 23429.54
[INFO 2017-06-25 14:54:58,850 main.py:50] epoch 396, training loss: 20376.46, average training loss: 24973.93, base loss: 23427.99
[INFO 2017-06-25 14:55:03,168 main.py:50] epoch 397, training loss: 20307.81, average training loss: 24962.21, base loss: 23422.33
[INFO 2017-06-25 14:55:07,502 main.py:50] epoch 398, training loss: 19077.88, average training loss: 24947.46, base loss: 23416.41
[INFO 2017-06-25 14:55:11,808 main.py:50] epoch 399, training loss: 18318.91, average training loss: 24930.89, base loss: 23410.35
[INFO 2017-06-25 14:55:16,125 main.py:50] epoch 400, training loss: 17750.59, average training loss: 24912.98, base loss: 23402.37
[INFO 2017-06-25 14:55:20,417 main.py:50] epoch 401, training loss: 19630.95, average training loss: 24899.84, base loss: 23398.92
[INFO 2017-06-25 14:55:24,761 main.py:50] epoch 402, training loss: 20639.78, average training loss: 24889.27, base loss: 23400.08
[INFO 2017-06-25 14:55:29,075 main.py:50] epoch 403, training loss: 19790.52, average training loss: 24876.65, base loss: 23399.83
[INFO 2017-06-25 14:55:33,409 main.py:50] epoch 404, training loss: 23661.26, average training loss: 24873.65, base loss: 23409.29
[INFO 2017-06-25 14:55:37,714 main.py:50] epoch 405, training loss: 18915.91, average training loss: 24858.97, base loss: 23402.86
[INFO 2017-06-25 14:55:42,024 main.py:50] epoch 406, training loss: 20076.70, average training loss: 24847.22, base loss: 23398.96
[INFO 2017-06-25 14:55:46,316 main.py:50] epoch 407, training loss: 21814.02, average training loss: 24839.79, base loss: 23402.70
[INFO 2017-06-25 14:55:50,631 main.py:50] epoch 408, training loss: 16256.15, average training loss: 24818.80, base loss: 23389.66
[INFO 2017-06-25 14:55:54,911 main.py:50] epoch 409, training loss: 21540.06, average training loss: 24810.81, base loss: 23392.76
[INFO 2017-06-25 14:55:59,209 main.py:50] epoch 410, training loss: 18989.46, average training loss: 24796.64, base loss: 23389.67
[INFO 2017-06-25 14:56:03,526 main.py:50] epoch 411, training loss: 28716.32, average training loss: 24806.16, base loss: 23412.10
[INFO 2017-06-25 14:56:07,831 main.py:50] epoch 412, training loss: 16343.19, average training loss: 24785.66, base loss: 23400.93
[INFO 2017-06-25 14:56:12,131 main.py:50] epoch 413, training loss: 18190.47, average training loss: 24769.73, base loss: 23392.51
[INFO 2017-06-25 14:56:16,434 main.py:50] epoch 414, training loss: 22410.10, average training loss: 24764.05, base loss: 23399.41
[INFO 2017-06-25 14:56:20,747 main.py:50] epoch 415, training loss: 21496.79, average training loss: 24756.19, base loss: 23401.52
[INFO 2017-06-25 14:56:25,048 main.py:50] epoch 416, training loss: 26363.85, average training loss: 24760.05, base loss: 23416.11
[INFO 2017-06-25 14:56:29,371 main.py:50] epoch 417, training loss: 20460.27, average training loss: 24749.76, base loss: 23415.84
[INFO 2017-06-25 14:56:33,688 main.py:50] epoch 418, training loss: 21085.37, average training loss: 24741.02, base loss: 23419.15
[INFO 2017-06-25 14:56:37,973 main.py:50] epoch 419, training loss: 18430.46, average training loss: 24725.99, base loss: 23413.52
[INFO 2017-06-25 14:56:42,268 main.py:50] epoch 420, training loss: 17439.75, average training loss: 24708.69, base loss: 23406.00
[INFO 2017-06-25 14:56:46,577 main.py:50] epoch 421, training loss: 21859.81, average training loss: 24701.93, base loss: 23410.70
[INFO 2017-06-25 14:56:50,893 main.py:50] epoch 422, training loss: 25782.21, average training loss: 24704.49, base loss: 23424.17
[INFO 2017-06-25 14:56:55,206 main.py:50] epoch 423, training loss: 19495.84, average training loss: 24692.20, base loss: 23423.31
[INFO 2017-06-25 14:56:59,518 main.py:50] epoch 424, training loss: 15267.88, average training loss: 24670.03, base loss: 23410.47
[INFO 2017-06-25 14:57:03,835 main.py:50] epoch 425, training loss: 25743.84, average training loss: 24672.55, base loss: 23423.85
[INFO 2017-06-25 14:57:08,155 main.py:50] epoch 426, training loss: 17275.26, average training loss: 24655.23, base loss: 23416.66
[INFO 2017-06-25 14:57:12,449 main.py:50] epoch 427, training loss: 21846.01, average training loss: 24648.66, base loss: 23422.82
[INFO 2017-06-25 14:57:16,731 main.py:50] epoch 428, training loss: 19656.29, average training loss: 24637.03, base loss: 23419.48
[INFO 2017-06-25 14:57:21,043 main.py:50] epoch 429, training loss: 16834.37, average training loss: 24618.88, base loss: 23408.69
[INFO 2017-06-25 14:57:25,359 main.py:50] epoch 430, training loss: 20354.07, average training loss: 24608.98, base loss: 23407.51
[INFO 2017-06-25 14:57:29,688 main.py:50] epoch 431, training loss: 21892.40, average training loss: 24602.70, base loss: 23411.15
[INFO 2017-06-25 14:57:33,998 main.py:50] epoch 432, training loss: 19573.51, average training loss: 24591.08, base loss: 23408.74
[INFO 2017-06-25 14:57:38,283 main.py:50] epoch 433, training loss: 20649.30, average training loss: 24582.00, base loss: 23410.61
[INFO 2017-06-25 14:57:42,584 main.py:50] epoch 434, training loss: 22240.03, average training loss: 24576.61, base loss: 23417.21
[INFO 2017-06-25 14:57:46,879 main.py:50] epoch 435, training loss: 16278.01, average training loss: 24557.58, base loss: 23406.38
[INFO 2017-06-25 14:57:51,192 main.py:50] epoch 436, training loss: 23569.91, average training loss: 24555.32, base loss: 23413.82
[INFO 2017-06-25 14:57:55,483 main.py:50] epoch 437, training loss: 21129.37, average training loss: 24547.50, base loss: 23416.17
[INFO 2017-06-25 14:57:59,762 main.py:50] epoch 438, training loss: 22501.19, average training loss: 24542.84, base loss: 23422.88
[INFO 2017-06-25 14:58:04,069 main.py:50] epoch 439, training loss: 24882.30, average training loss: 24543.61, base loss: 23434.96
[INFO 2017-06-25 14:58:08,391 main.py:50] epoch 440, training loss: 22433.55, average training loss: 24538.82, base loss: 23442.04
[INFO 2017-06-25 14:58:12,718 main.py:50] epoch 441, training loss: 20939.07, average training loss: 24530.68, base loss: 23444.78
[INFO 2017-06-25 14:58:16,996 main.py:50] epoch 442, training loss: 21075.58, average training loss: 24522.88, base loss: 23446.07
[INFO 2017-06-25 14:58:21,289 main.py:50] epoch 443, training loss: 22493.43, average training loss: 24518.31, base loss: 23449.96
[INFO 2017-06-25 14:58:25,597 main.py:50] epoch 444, training loss: 20113.80, average training loss: 24508.41, base loss: 23452.13
[INFO 2017-06-25 14:58:29,902 main.py:50] epoch 445, training loss: 17158.92, average training loss: 24491.93, base loss: 23443.52
[INFO 2017-06-25 14:58:34,207 main.py:50] epoch 446, training loss: 21057.64, average training loss: 24484.25, base loss: 23445.98
[INFO 2017-06-25 14:58:38,528 main.py:50] epoch 447, training loss: 18229.64, average training loss: 24470.29, base loss: 23438.60
[INFO 2017-06-25 14:58:42,818 main.py:50] epoch 448, training loss: 18857.42, average training loss: 24457.79, base loss: 23434.72
[INFO 2017-06-25 14:58:47,131 main.py:50] epoch 449, training loss: 18020.63, average training loss: 24443.48, base loss: 23429.40
[INFO 2017-06-25 14:58:51,461 main.py:50] epoch 450, training loss: 20019.75, average training loss: 24433.68, base loss: 23429.48
[INFO 2017-06-25 14:58:55,794 main.py:50] epoch 451, training loss: 16629.60, average training loss: 24416.41, base loss: 23420.67
[INFO 2017-06-25 14:59:00,104 main.py:50] epoch 452, training loss: 24798.34, average training loss: 24417.25, base loss: 23434.05
[INFO 2017-06-25 14:59:04,428 main.py:50] epoch 453, training loss: 16994.07, average training loss: 24400.90, base loss: 23425.15
[INFO 2017-06-25 14:59:08,736 main.py:50] epoch 454, training loss: 20706.99, average training loss: 24392.78, base loss: 23425.55
[INFO 2017-06-25 14:59:13,038 main.py:50] epoch 455, training loss: 22901.71, average training loss: 24389.51, base loss: 23435.67
[INFO 2017-06-25 14:59:17,340 main.py:50] epoch 456, training loss: 19397.46, average training loss: 24378.59, base loss: 23433.85
[INFO 2017-06-25 14:59:21,638 main.py:50] epoch 457, training loss: 18681.53, average training loss: 24366.15, base loss: 23428.07
[INFO 2017-06-25 14:59:25,918 main.py:50] epoch 458, training loss: 19213.74, average training loss: 24354.93, base loss: 23423.25
[INFO 2017-06-25 14:59:30,210 main.py:50] epoch 459, training loss: 23248.02, average training loss: 24352.52, base loss: 23430.36
[INFO 2017-06-25 14:59:34,515 main.py:50] epoch 460, training loss: 26167.40, average training loss: 24356.46, base loss: 23444.31
[INFO 2017-06-25 14:59:38,863 main.py:50] epoch 461, training loss: 24915.48, average training loss: 24357.67, base loss: 23455.39
[INFO 2017-06-25 14:59:43,186 main.py:50] epoch 462, training loss: 17303.25, average training loss: 24342.43, base loss: 23449.91
[INFO 2017-06-25 14:59:47,479 main.py:50] epoch 463, training loss: 21819.14, average training loss: 24336.99, base loss: 23454.62
[INFO 2017-06-25 14:59:51,771 main.py:50] epoch 464, training loss: 22073.89, average training loss: 24332.13, base loss: 23460.47
[INFO 2017-06-25 14:59:56,092 main.py:50] epoch 465, training loss: 15456.61, average training loss: 24313.08, base loss: 23449.93
[INFO 2017-06-25 15:00:00,390 main.py:50] epoch 466, training loss: 24048.53, average training loss: 24312.51, base loss: 23462.92
[INFO 2017-06-25 15:00:04,697 main.py:50] epoch 467, training loss: 18271.06, average training loss: 24299.60, base loss: 23458.26
[INFO 2017-06-25 15:00:09,042 main.py:50] epoch 468, training loss: 23604.74, average training loss: 24298.12, base loss: 23466.28
[INFO 2017-06-25 15:00:13,361 main.py:50] epoch 469, training loss: 19426.83, average training loss: 24287.76, base loss: 23465.48
[INFO 2017-06-25 15:00:17,658 main.py:50] epoch 470, training loss: 22926.20, average training loss: 24284.87, base loss: 23475.66
[INFO 2017-06-25 15:00:21,989 main.py:50] epoch 471, training loss: 24410.89, average training loss: 24285.13, base loss: 23483.81
[INFO 2017-06-25 15:00:26,281 main.py:50] epoch 472, training loss: 20197.05, average training loss: 24276.49, base loss: 23483.79
[INFO 2017-06-25 15:00:30,570 main.py:50] epoch 473, training loss: 22391.65, average training loss: 24272.51, base loss: 23488.48
[INFO 2017-06-25 15:00:34,854 main.py:50] epoch 474, training loss: 22708.33, average training loss: 24269.22, base loss: 23495.55
[INFO 2017-06-25 15:00:39,131 main.py:50] epoch 475, training loss: 19788.49, average training loss: 24259.81, base loss: 23494.33
[INFO 2017-06-25 15:00:43,424 main.py:50] epoch 476, training loss: 17845.14, average training loss: 24246.36, base loss: 23486.07
[INFO 2017-06-25 15:00:47,728 main.py:50] epoch 477, training loss: 18310.85, average training loss: 24233.94, base loss: 23477.16
[INFO 2017-06-25 15:00:52,034 main.py:50] epoch 478, training loss: 17520.88, average training loss: 24219.93, base loss: 23471.38
[INFO 2017-06-25 15:00:56,353 main.py:50] epoch 479, training loss: 20928.01, average training loss: 24213.07, base loss: 23475.60
[INFO 2017-06-25 15:01:00,639 main.py:50] epoch 480, training loss: 20353.72, average training loss: 24205.05, base loss: 23475.98
[INFO 2017-06-25 15:01:04,933 main.py:50] epoch 481, training loss: 20678.20, average training loss: 24197.73, base loss: 23476.81
[INFO 2017-06-25 15:01:09,255 main.py:50] epoch 482, training loss: 19541.04, average training loss: 24188.09, base loss: 23474.86
[INFO 2017-06-25 15:01:13,565 main.py:50] epoch 483, training loss: 21488.33, average training loss: 24182.51, base loss: 23477.05
[INFO 2017-06-25 15:01:17,888 main.py:50] epoch 484, training loss: 22211.98, average training loss: 24178.45, base loss: 23481.24
[INFO 2017-06-25 15:01:22,211 main.py:50] epoch 485, training loss: 20515.83, average training loss: 24170.91, base loss: 23480.59
[INFO 2017-06-25 15:01:26,506 main.py:50] epoch 486, training loss: 20415.13, average training loss: 24163.20, base loss: 23482.04
[INFO 2017-06-25 15:01:30,813 main.py:50] epoch 487, training loss: 20866.73, average training loss: 24156.44, base loss: 23483.74
[INFO 2017-06-25 15:01:35,086 main.py:50] epoch 488, training loss: 20524.87, average training loss: 24149.02, base loss: 23485.52
[INFO 2017-06-25 15:01:39,390 main.py:50] epoch 489, training loss: 17637.40, average training loss: 24135.73, base loss: 23477.49
[INFO 2017-06-25 15:01:43,695 main.py:50] epoch 490, training loss: 18389.40, average training loss: 24124.02, base loss: 23473.11
[INFO 2017-06-25 15:01:48,031 main.py:50] epoch 491, training loss: 21057.44, average training loss: 24117.79, base loss: 23472.59
[INFO 2017-06-25 15:01:52,340 main.py:50] epoch 492, training loss: 23372.59, average training loss: 24116.28, base loss: 23478.62
[INFO 2017-06-25 15:01:56,627 main.py:50] epoch 493, training loss: 20677.29, average training loss: 24109.32, base loss: 23478.92
[INFO 2017-06-25 15:02:00,917 main.py:50] epoch 494, training loss: 19422.12, average training loss: 24099.85, base loss: 23474.89
[INFO 2017-06-25 15:02:05,201 main.py:50] epoch 495, training loss: 18334.44, average training loss: 24088.23, base loss: 23470.49
[INFO 2017-06-25 15:02:09,502 main.py:50] epoch 496, training loss: 21267.59, average training loss: 24082.55, base loss: 23472.13
[INFO 2017-06-25 15:02:13,819 main.py:50] epoch 497, training loss: 24804.60, average training loss: 24084.00, base loss: 23482.14
[INFO 2017-06-25 15:02:18,129 main.py:50] epoch 498, training loss: 18742.25, average training loss: 24073.30, base loss: 23479.48
[INFO 2017-06-25 15:02:22,439 main.py:50] epoch 499, training loss: 22255.61, average training loss: 24069.66, base loss: 23486.88
[INFO 2017-06-25 15:02:26,746 main.py:50] epoch 500, training loss: 19772.36, average training loss: 24061.08, base loss: 23486.93
[INFO 2017-06-25 15:02:31,053 main.py:50] epoch 501, training loss: 17194.47, average training loss: 24047.40, base loss: 23479.61
[INFO 2017-06-25 15:02:35,346 main.py:50] epoch 502, training loss: 22830.54, average training loss: 24044.99, base loss: 23485.47
[INFO 2017-06-25 15:02:39,670 main.py:50] epoch 503, training loss: 21283.39, average training loss: 24039.51, base loss: 23488.38
[INFO 2017-06-25 15:02:43,986 main.py:50] epoch 504, training loss: 21407.39, average training loss: 24034.29, base loss: 23492.32
[INFO 2017-06-25 15:02:48,331 main.py:50] epoch 505, training loss: 21741.43, average training loss: 24029.76, base loss: 23494.84
[INFO 2017-06-25 15:02:52,638 main.py:50] epoch 506, training loss: 18212.84, average training loss: 24018.29, base loss: 23490.95
[INFO 2017-06-25 15:02:56,970 main.py:50] epoch 507, training loss: 24766.76, average training loss: 24019.76, base loss: 23502.68
[INFO 2017-06-25 15:03:01,297 main.py:50] epoch 508, training loss: 18851.04, average training loss: 24009.61, base loss: 23500.38
[INFO 2017-06-25 15:03:05,600 main.py:50] epoch 509, training loss: 17582.38, average training loss: 23997.01, base loss: 23496.81
[INFO 2017-06-25 15:03:09,919 main.py:50] epoch 510, training loss: 23213.32, average training loss: 23995.47, base loss: 23503.71
[INFO 2017-06-25 15:03:14,225 main.py:50] epoch 511, training loss: 17296.43, average training loss: 23982.39, base loss: 23495.46
[INFO 2017-06-25 15:03:18,522 main.py:50] epoch 512, training loss: 21644.37, average training loss: 23977.83, base loss: 23498.54
[INFO 2017-06-25 15:03:22,867 main.py:50] epoch 513, training loss: 17309.96, average training loss: 23964.86, base loss: 23492.00
[INFO 2017-06-25 15:03:27,176 main.py:50] epoch 514, training loss: 24203.87, average training loss: 23965.32, base loss: 23499.91
[INFO 2017-06-25 15:03:31,492 main.py:50] epoch 515, training loss: 19851.87, average training loss: 23957.35, base loss: 23499.57
[INFO 2017-06-25 15:03:35,785 main.py:50] epoch 516, training loss: 20152.78, average training loss: 23949.99, base loss: 23502.05
[INFO 2017-06-25 15:03:40,097 main.py:50] epoch 517, training loss: 24450.18, average training loss: 23950.96, base loss: 23512.60
[INFO 2017-06-25 15:03:44,402 main.py:50] epoch 518, training loss: 25104.77, average training loss: 23953.18, base loss: 23521.42
[INFO 2017-06-25 15:03:48,700 main.py:50] epoch 519, training loss: 23915.43, average training loss: 23953.11, base loss: 23526.84
[INFO 2017-06-25 15:03:53,020 main.py:50] epoch 520, training loss: 19168.57, average training loss: 23943.92, base loss: 23525.87
[INFO 2017-06-25 15:03:57,353 main.py:50] epoch 521, training loss: 18401.61, average training loss: 23933.31, base loss: 23522.81
[INFO 2017-06-25 15:04:01,671 main.py:50] epoch 522, training loss: 21761.32, average training loss: 23929.15, base loss: 23527.03
[INFO 2017-06-25 15:04:05,965 main.py:50] epoch 523, training loss: 15464.74, average training loss: 23913.00, base loss: 23517.88
[INFO 2017-06-25 15:04:10,277 main.py:50] epoch 524, training loss: 18760.07, average training loss: 23903.18, base loss: 23512.77
[INFO 2017-06-25 15:04:14,588 main.py:50] epoch 525, training loss: 18533.60, average training loss: 23892.98, base loss: 23508.13
[INFO 2017-06-25 15:04:18,900 main.py:50] epoch 526, training loss: 17726.36, average training loss: 23881.28, base loss: 23503.83
[INFO 2017-06-25 15:04:23,216 main.py:50] epoch 527, training loss: 18932.57, average training loss: 23871.90, base loss: 23502.14
[INFO 2017-06-25 15:04:27,512 main.py:50] epoch 528, training loss: 18743.52, average training loss: 23862.21, base loss: 23501.24
[INFO 2017-06-25 15:04:31,834 main.py:50] epoch 529, training loss: 20597.76, average training loss: 23856.05, base loss: 23499.73
[INFO 2017-06-25 15:04:36,141 main.py:50] epoch 530, training loss: 17424.83, average training loss: 23843.94, base loss: 23494.73
[INFO 2017-06-25 15:04:40,434 main.py:50] epoch 531, training loss: 21161.24, average training loss: 23838.89, base loss: 23497.18
[INFO 2017-06-25 15:04:44,721 main.py:50] epoch 532, training loss: 21787.26, average training loss: 23835.05, base loss: 23500.98
[INFO 2017-06-25 15:04:49,028 main.py:50] epoch 533, training loss: 20317.33, average training loss: 23828.46, base loss: 23501.81
[INFO 2017-06-25 15:04:53,329 main.py:50] epoch 534, training loss: 16992.77, average training loss: 23815.68, base loss: 23493.98
[INFO 2017-06-25 15:04:57,655 main.py:50] epoch 535, training loss: 19862.06, average training loss: 23808.30, base loss: 23491.41
[INFO 2017-06-25 15:05:01,982 main.py:50] epoch 536, training loss: 18751.86, average training loss: 23798.89, base loss: 23488.16
[INFO 2017-06-25 15:05:06,295 main.py:50] epoch 537, training loss: 23342.24, average training loss: 23798.04, base loss: 23494.68
[INFO 2017-06-25 15:05:10,593 main.py:50] epoch 538, training loss: 18135.28, average training loss: 23787.53, base loss: 23489.06
[INFO 2017-06-25 15:05:14,937 main.py:50] epoch 539, training loss: 17951.37, average training loss: 23776.73, base loss: 23485.66
[INFO 2017-06-25 15:05:19,242 main.py:50] epoch 540, training loss: 17046.20, average training loss: 23764.29, base loss: 23478.37
[INFO 2017-06-25 15:05:23,558 main.py:50] epoch 541, training loss: 21033.07, average training loss: 23759.25, base loss: 23481.43
[INFO 2017-06-25 15:05:27,869 main.py:50] epoch 542, training loss: 16573.41, average training loss: 23746.01, base loss: 23473.03
[INFO 2017-06-25 15:05:32,174 main.py:50] epoch 543, training loss: 21541.54, average training loss: 23741.96, base loss: 23477.68
[INFO 2017-06-25 15:05:36,483 main.py:50] epoch 544, training loss: 23380.36, average training loss: 23741.30, base loss: 23484.25
[INFO 2017-06-25 15:05:40,811 main.py:50] epoch 545, training loss: 23472.93, average training loss: 23740.81, base loss: 23490.32
[INFO 2017-06-25 15:05:45,100 main.py:50] epoch 546, training loss: 20119.14, average training loss: 23734.18, base loss: 23490.03
[INFO 2017-06-25 15:05:49,427 main.py:50] epoch 547, training loss: 17645.63, average training loss: 23723.07, base loss: 23486.11
[INFO 2017-06-25 15:05:53,751 main.py:50] epoch 548, training loss: 22595.83, average training loss: 23721.02, base loss: 23491.54
[INFO 2017-06-25 15:05:58,033 main.py:50] epoch 549, training loss: 20855.97, average training loss: 23715.81, base loss: 23494.26
[INFO 2017-06-25 15:06:02,321 main.py:50] epoch 550, training loss: 16233.77, average training loss: 23702.23, base loss: 23484.45
[INFO 2017-06-25 15:06:06,646 main.py:50] epoch 551, training loss: 22524.10, average training loss: 23700.10, base loss: 23490.09
[INFO 2017-06-25 15:06:10,957 main.py:50] epoch 552, training loss: 21789.71, average training loss: 23696.64, base loss: 23494.43
[INFO 2017-06-25 15:06:15,266 main.py:50] epoch 553, training loss: 19040.45, average training loss: 23688.24, base loss: 23493.24
[INFO 2017-06-25 15:06:19,594 main.py:50] epoch 554, training loss: 19891.02, average training loss: 23681.40, base loss: 23493.69
[INFO 2017-06-25 15:06:23,911 main.py:50] epoch 555, training loss: 18483.28, average training loss: 23672.05, base loss: 23487.08
[INFO 2017-06-25 15:06:28,198 main.py:50] epoch 556, training loss: 18032.21, average training loss: 23661.92, base loss: 23483.37
[INFO 2017-06-25 15:06:32,503 main.py:50] epoch 557, training loss: 23448.87, average training loss: 23661.54, base loss: 23488.80
[INFO 2017-06-25 15:06:36,828 main.py:50] epoch 558, training loss: 19700.89, average training loss: 23654.46, base loss: 23488.48
[INFO 2017-06-25 15:06:41,123 main.py:50] epoch 559, training loss: 19044.99, average training loss: 23646.22, base loss: 23486.84
[INFO 2017-06-25 15:06:45,451 main.py:50] epoch 560, training loss: 18517.71, average training loss: 23637.08, base loss: 23483.04
[INFO 2017-06-25 15:06:49,784 main.py:50] epoch 561, training loss: 21200.28, average training loss: 23632.75, base loss: 23488.40
[INFO 2017-06-25 15:06:54,099 main.py:50] epoch 562, training loss: 23486.18, average training loss: 23632.49, base loss: 23496.64
[INFO 2017-06-25 15:06:58,393 main.py:50] epoch 563, training loss: 22166.13, average training loss: 23629.89, base loss: 23500.26
[INFO 2017-06-25 15:07:02,731 main.py:50] epoch 564, training loss: 21195.94, average training loss: 23625.58, base loss: 23504.28
[INFO 2017-06-25 15:07:07,033 main.py:50] epoch 565, training loss: 27407.20, average training loss: 23632.26, base loss: 23522.82
[INFO 2017-06-25 15:07:11,321 main.py:50] epoch 566, training loss: 18141.47, average training loss: 23622.58, base loss: 23514.18
[INFO 2017-06-25 15:07:15,650 main.py:50] epoch 567, training loss: 18059.85, average training loss: 23612.78, base loss: 23506.72
[INFO 2017-06-25 15:07:19,985 main.py:50] epoch 568, training loss: 22267.93, average training loss: 23610.42, base loss: 23512.10
[INFO 2017-06-25 15:07:24,299 main.py:50] epoch 569, training loss: 19121.81, average training loss: 23602.54, base loss: 23511.03
[INFO 2017-06-25 15:07:28,590 main.py:50] epoch 570, training loss: 20115.43, average training loss: 23596.44, base loss: 23513.96
[INFO 2017-06-25 15:07:32,894 main.py:50] epoch 571, training loss: 18793.20, average training loss: 23588.04, base loss: 23513.10
[INFO 2017-06-25 15:07:37,196 main.py:50] epoch 572, training loss: 22068.30, average training loss: 23585.39, base loss: 23515.16
[INFO 2017-06-25 15:07:41,467 main.py:50] epoch 573, training loss: 22362.48, average training loss: 23583.26, base loss: 23520.73
[INFO 2017-06-25 15:07:45,774 main.py:50] epoch 574, training loss: 23551.11, average training loss: 23583.20, base loss: 23526.94
[INFO 2017-06-25 15:07:50,065 main.py:50] epoch 575, training loss: 17948.29, average training loss: 23573.42, base loss: 23522.18
[INFO 2017-06-25 15:07:54,380 main.py:50] epoch 576, training loss: 17614.88, average training loss: 23563.09, base loss: 23514.78
[INFO 2017-06-25 15:07:58,681 main.py:50] epoch 577, training loss: 17499.04, average training loss: 23552.60, base loss: 23508.93
[INFO 2017-06-25 15:08:02,982 main.py:50] epoch 578, training loss: 19703.27, average training loss: 23545.95, base loss: 23505.96
[INFO 2017-06-25 15:08:07,285 main.py:50] epoch 579, training loss: 18133.49, average training loss: 23536.62, base loss: 23505.07
[INFO 2017-06-25 15:08:11,585 main.py:50] epoch 580, training loss: 21106.37, average training loss: 23532.44, base loss: 23508.26
[INFO 2017-06-25 15:08:15,898 main.py:50] epoch 581, training loss: 21409.56, average training loss: 23528.79, base loss: 23513.57
[INFO 2017-06-25 15:08:20,213 main.py:50] epoch 582, training loss: 18687.38, average training loss: 23520.48, base loss: 23513.18
[INFO 2017-06-25 15:08:24,544 main.py:50] epoch 583, training loss: 23787.41, average training loss: 23520.94, base loss: 23523.80
[INFO 2017-06-25 15:08:28,876 main.py:50] epoch 584, training loss: 17128.11, average training loss: 23510.01, base loss: 23517.51
[INFO 2017-06-25 15:08:33,227 main.py:50] epoch 585, training loss: 16630.53, average training loss: 23498.27, base loss: 23509.62
[INFO 2017-06-25 15:08:37,526 main.py:50] epoch 586, training loss: 18675.62, average training loss: 23490.06, base loss: 23505.75
[INFO 2017-06-25 15:08:41,844 main.py:50] epoch 587, training loss: 17389.25, average training loss: 23479.68, base loss: 23499.41
[INFO 2017-06-25 15:08:46,177 main.py:50] epoch 588, training loss: 23565.85, average training loss: 23479.83, base loss: 23506.62
[INFO 2017-06-25 15:08:50,520 main.py:50] epoch 589, training loss: 21726.70, average training loss: 23476.86, base loss: 23506.63
[INFO 2017-06-25 15:08:54,809 main.py:50] epoch 590, training loss: 17288.76, average training loss: 23466.39, base loss: 23502.16
[INFO 2017-06-25 15:08:59,103 main.py:50] epoch 591, training loss: 27056.22, average training loss: 23472.45, base loss: 23517.34
[INFO 2017-06-25 15:09:03,415 main.py:50] epoch 592, training loss: 22247.50, average training loss: 23470.39, base loss: 23521.16
[INFO 2017-06-25 15:09:07,719 main.py:50] epoch 593, training loss: 21098.45, average training loss: 23466.39, base loss: 23525.37
[INFO 2017-06-25 15:09:12,023 main.py:50] epoch 594, training loss: 17791.96, average training loss: 23456.86, base loss: 23520.32
[INFO 2017-06-25 15:09:16,345 main.py:50] epoch 595, training loss: 24305.70, average training loss: 23458.28, base loss: 23528.89
[INFO 2017-06-25 15:09:20,702 main.py:50] epoch 596, training loss: 20649.40, average training loss: 23453.57, base loss: 23531.35
[INFO 2017-06-25 15:09:25,040 main.py:50] epoch 597, training loss: 17444.49, average training loss: 23443.53, base loss: 23526.47
[INFO 2017-06-25 15:09:29,369 main.py:50] epoch 598, training loss: 19253.16, average training loss: 23436.53, base loss: 23525.21
[INFO 2017-06-25 15:09:33,698 main.py:50] epoch 599, training loss: 25058.71, average training loss: 23439.23, base loss: 23533.60
[INFO 2017-06-25 15:09:38,030 main.py:50] epoch 600, training loss: 25161.53, average training loss: 23442.10, base loss: 23545.80
[INFO 2017-06-25 15:09:42,346 main.py:50] epoch 601, training loss: 24173.32, average training loss: 23443.31, base loss: 23554.60
[INFO 2017-06-25 15:09:46,646 main.py:50] epoch 602, training loss: 19677.89, average training loss: 23437.07, base loss: 23552.50
[INFO 2017-06-25 15:09:50,976 main.py:50] epoch 603, training loss: 20771.39, average training loss: 23432.66, base loss: 23555.52
[INFO 2017-06-25 15:09:55,273 main.py:50] epoch 604, training loss: 21376.94, average training loss: 23429.26, base loss: 23558.58
[INFO 2017-06-25 15:09:59,553 main.py:50] epoch 605, training loss: 19034.94, average training loss: 23422.01, base loss: 23556.65
[INFO 2017-06-25 15:10:03,855 main.py:50] epoch 606, training loss: 20372.63, average training loss: 23416.98, base loss: 23559.69
[INFO 2017-06-25 15:10:08,158 main.py:50] epoch 607, training loss: 19344.75, average training loss: 23410.29, base loss: 23560.29
[INFO 2017-06-25 15:10:12,481 main.py:50] epoch 608, training loss: 21303.27, average training loss: 23406.83, base loss: 23563.82
[INFO 2017-06-25 15:10:16,776 main.py:50] epoch 609, training loss: 22485.59, average training loss: 23405.32, base loss: 23571.55
[INFO 2017-06-25 15:10:21,073 main.py:50] epoch 610, training loss: 20573.68, average training loss: 23400.68, base loss: 23571.72
[INFO 2017-06-25 15:10:25,409 main.py:50] epoch 611, training loss: 19215.98, average training loss: 23393.84, base loss: 23568.80
[INFO 2017-06-25 15:10:29,719 main.py:50] epoch 612, training loss: 19487.99, average training loss: 23387.47, base loss: 23568.78
[INFO 2017-06-25 15:10:34,042 main.py:50] epoch 613, training loss: 20154.74, average training loss: 23382.21, base loss: 23568.10
[INFO 2017-06-25 15:10:38,344 main.py:50] epoch 614, training loss: 20146.01, average training loss: 23376.95, base loss: 23570.24
[INFO 2017-06-25 15:10:42,705 main.py:50] epoch 615, training loss: 16879.72, average training loss: 23366.40, base loss: 23563.59
[INFO 2017-06-25 15:10:47,052 main.py:50] epoch 616, training loss: 22425.98, average training loss: 23364.87, base loss: 23567.58
[INFO 2017-06-25 15:10:51,395 main.py:50] epoch 617, training loss: 21770.13, average training loss: 23362.29, base loss: 23569.27
[INFO 2017-06-25 15:10:55,718 main.py:50] epoch 618, training loss: 18667.97, average training loss: 23354.71, base loss: 23570.05
[INFO 2017-06-25 15:11:00,041 main.py:50] epoch 619, training loss: 19521.90, average training loss: 23348.53, base loss: 23567.93
[INFO 2017-06-25 15:11:04,338 main.py:50] epoch 620, training loss: 16463.69, average training loss: 23337.44, base loss: 23558.61
[INFO 2017-06-25 15:11:08,683 main.py:50] epoch 621, training loss: 25132.36, average training loss: 23340.33, base loss: 23566.17
[INFO 2017-06-25 15:11:12,987 main.py:50] epoch 622, training loss: 19675.38, average training loss: 23334.44, base loss: 23565.60
[INFO 2017-06-25 15:11:17,308 main.py:50] epoch 623, training loss: 22337.71, average training loss: 23332.85, base loss: 23571.69
[INFO 2017-06-25 15:11:21,641 main.py:50] epoch 624, training loss: 15540.89, average training loss: 23320.38, base loss: 23562.41
[INFO 2017-06-25 15:11:25,967 main.py:50] epoch 625, training loss: 26582.00, average training loss: 23325.59, base loss: 23574.30
[INFO 2017-06-25 15:11:30,284 main.py:50] epoch 626, training loss: 23090.27, average training loss: 23325.21, base loss: 23579.72
[INFO 2017-06-25 15:11:34,609 main.py:50] epoch 627, training loss: 22383.87, average training loss: 23323.72, base loss: 23583.93
[INFO 2017-06-25 15:11:38,934 main.py:50] epoch 628, training loss: 18720.58, average training loss: 23316.40, base loss: 23582.29
[INFO 2017-06-25 15:11:43,254 main.py:50] epoch 629, training loss: 20397.03, average training loss: 23311.76, base loss: 23584.18
[INFO 2017-06-25 15:11:47,553 main.py:50] epoch 630, training loss: 20511.86, average training loss: 23307.33, base loss: 23584.93
[INFO 2017-06-25 15:11:51,860 main.py:50] epoch 631, training loss: 17648.21, average training loss: 23298.37, base loss: 23582.39
[INFO 2017-06-25 15:11:56,158 main.py:50] epoch 632, training loss: 18856.64, average training loss: 23291.35, base loss: 23581.50
[INFO 2017-06-25 15:12:00,454 main.py:50] epoch 633, training loss: 24121.83, average training loss: 23292.66, base loss: 23588.63
[INFO 2017-06-25 15:12:04,774 main.py:50] epoch 634, training loss: 22176.45, average training loss: 23290.91, base loss: 23589.54
[INFO 2017-06-25 15:12:09,105 main.py:50] epoch 635, training loss: 20738.75, average training loss: 23286.89, base loss: 23593.95
[INFO 2017-06-25 15:12:13,423 main.py:50] epoch 636, training loss: 18459.26, average training loss: 23279.32, base loss: 23592.98
[INFO 2017-06-25 15:12:17,758 main.py:50] epoch 637, training loss: 21650.20, average training loss: 23276.76, base loss: 23596.78
[INFO 2017-06-25 15:12:22,037 main.py:50] epoch 638, training loss: 19320.97, average training loss: 23270.57, base loss: 23596.53
[INFO 2017-06-25 15:12:26,360 main.py:50] epoch 639, training loss: 18931.59, average training loss: 23263.79, base loss: 23593.63
[INFO 2017-06-25 15:12:30,653 main.py:50] epoch 640, training loss: 19551.26, average training loss: 23258.00, base loss: 23592.27
[INFO 2017-06-25 15:12:34,989 main.py:50] epoch 641, training loss: 19084.02, average training loss: 23251.50, base loss: 23590.39
[INFO 2017-06-25 15:12:39,328 main.py:50] epoch 642, training loss: 15438.54, average training loss: 23239.35, base loss: 23583.29
[INFO 2017-06-25 15:12:43,633 main.py:50] epoch 643, training loss: 19788.01, average training loss: 23233.99, base loss: 23581.25
[INFO 2017-06-25 15:12:47,925 main.py:50] epoch 644, training loss: 19950.57, average training loss: 23228.90, base loss: 23582.11
[INFO 2017-06-25 15:12:52,244 main.py:50] epoch 645, training loss: 18081.70, average training loss: 23220.93, base loss: 23578.48
[INFO 2017-06-25 15:12:56,553 main.py:50] epoch 646, training loss: 16781.41, average training loss: 23210.98, base loss: 23571.72
[INFO 2017-06-25 15:13:00,888 main.py:50] epoch 647, training loss: 17639.64, average training loss: 23202.38, base loss: 23567.85
[INFO 2017-06-25 15:13:05,172 main.py:50] epoch 648, training loss: 19766.44, average training loss: 23197.08, base loss: 23568.48
[INFO 2017-06-25 15:13:09,489 main.py:50] epoch 649, training loss: 18264.24, average training loss: 23189.50, base loss: 23566.67
[INFO 2017-06-25 15:13:13,793 main.py:50] epoch 650, training loss: 25852.78, average training loss: 23193.59, base loss: 23578.32
[INFO 2017-06-25 15:13:18,111 main.py:50] epoch 651, training loss: 17697.35, average training loss: 23185.16, base loss: 23572.92
[INFO 2017-06-25 15:13:22,406 main.py:50] epoch 652, training loss: 17754.26, average training loss: 23176.84, base loss: 23571.95
[INFO 2017-06-25 15:13:26,718 main.py:50] epoch 653, training loss: 19127.49, average training loss: 23170.65, base loss: 23569.67
[INFO 2017-06-25 15:13:31,016 main.py:50] epoch 654, training loss: 25379.85, average training loss: 23174.02, base loss: 23580.80
[INFO 2017-06-25 15:13:35,333 main.py:50] epoch 655, training loss: 19887.63, average training loss: 23169.01, base loss: 23582.56
[INFO 2017-06-25 15:13:39,638 main.py:50] epoch 656, training loss: 21490.61, average training loss: 23166.46, base loss: 23585.29
[INFO 2017-06-25 15:13:43,954 main.py:50] epoch 657, training loss: 22605.98, average training loss: 23165.61, base loss: 23589.36
[INFO 2017-06-25 15:13:48,293 main.py:50] epoch 658, training loss: 20172.70, average training loss: 23161.06, base loss: 23591.33
[INFO 2017-06-25 15:13:52,577 main.py:50] epoch 659, training loss: 25514.28, average training loss: 23164.63, base loss: 23603.55
[INFO 2017-06-25 15:13:56,898 main.py:50] epoch 660, training loss: 21780.09, average training loss: 23162.53, base loss: 23609.22
[INFO 2017-06-25 15:14:01,219 main.py:50] epoch 661, training loss: 16002.18, average training loss: 23151.72, base loss: 23602.85
[INFO 2017-06-25 15:14:05,497 main.py:50] epoch 662, training loss: 21579.45, average training loss: 23149.35, base loss: 23605.39
[INFO 2017-06-25 15:14:09,810 main.py:50] epoch 663, training loss: 21240.86, average training loss: 23146.47, base loss: 23605.13
[INFO 2017-06-25 15:14:14,112 main.py:50] epoch 664, training loss: 16589.79, average training loss: 23136.61, base loss: 23599.23
[INFO 2017-06-25 15:14:18,439 main.py:50] epoch 665, training loss: 15997.00, average training loss: 23125.89, base loss: 23593.59
[INFO 2017-06-25 15:14:22,738 main.py:50] epoch 666, training loss: 22137.60, average training loss: 23124.41, base loss: 23600.11
[INFO 2017-06-25 15:14:27,055 main.py:50] epoch 667, training loss: 17943.82, average training loss: 23116.66, base loss: 23599.45
[INFO 2017-06-25 15:14:31,382 main.py:50] epoch 668, training loss: 25029.42, average training loss: 23119.51, base loss: 23606.93
[INFO 2017-06-25 15:14:35,678 main.py:50] epoch 669, training loss: 27475.07, average training loss: 23126.02, base loss: 23617.94
[INFO 2017-06-25 15:14:39,965 main.py:50] epoch 670, training loss: 21402.64, average training loss: 23123.45, base loss: 23622.47
[INFO 2017-06-25 15:14:44,256 main.py:50] epoch 671, training loss: 17035.79, average training loss: 23114.39, base loss: 23616.96
[INFO 2017-06-25 15:14:48,589 main.py:50] epoch 672, training loss: 15784.93, average training loss: 23103.50, base loss: 23610.77
[INFO 2017-06-25 15:14:52,923 main.py:50] epoch 673, training loss: 22925.28, average training loss: 23103.23, base loss: 23618.72
[INFO 2017-06-25 15:14:57,250 main.py:50] epoch 674, training loss: 21425.44, average training loss: 23100.75, base loss: 23622.25
[INFO 2017-06-25 15:15:01,528 main.py:50] epoch 675, training loss: 13123.80, average training loss: 23085.99, base loss: 23612.07
[INFO 2017-06-25 15:15:05,843 main.py:50] epoch 676, training loss: 17469.33, average training loss: 23077.69, base loss: 23610.94
[INFO 2017-06-25 15:15:10,114 main.py:50] epoch 677, training loss: 24493.50, average training loss: 23079.78, base loss: 23617.52
[INFO 2017-06-25 15:15:14,427 main.py:50] epoch 678, training loss: 19438.71, average training loss: 23074.42, base loss: 23617.28
[INFO 2017-06-25 15:15:18,739 main.py:50] epoch 679, training loss: 16799.25, average training loss: 23065.19, base loss: 23611.13
[INFO 2017-06-25 15:15:23,011 main.py:50] epoch 680, training loss: 18900.04, average training loss: 23059.07, base loss: 23609.12
[INFO 2017-06-25 15:15:27,302 main.py:50] epoch 681, training loss: 19230.53, average training loss: 23053.46, base loss: 23607.63
[INFO 2017-06-25 15:15:31,575 main.py:50] epoch 682, training loss: 14946.99, average training loss: 23041.59, base loss: 23599.07
[INFO 2017-06-25 15:15:35,850 main.py:50] epoch 683, training loss: 16980.98, average training loss: 23032.73, base loss: 23594.63
[INFO 2017-06-25 15:15:40,147 main.py:50] epoch 684, training loss: 19960.59, average training loss: 23028.25, base loss: 23594.68
[INFO 2017-06-25 15:15:44,419 main.py:50] epoch 685, training loss: 17399.90, average training loss: 23020.04, base loss: 23589.56
[INFO 2017-06-25 15:15:48,697 main.py:50] epoch 686, training loss: 16866.99, average training loss: 23011.08, base loss: 23585.16
[INFO 2017-06-25 15:15:53,030 main.py:50] epoch 687, training loss: 24031.08, average training loss: 23012.57, base loss: 23591.88
[INFO 2017-06-25 15:15:57,317 main.py:50] epoch 688, training loss: 23327.66, average training loss: 23013.02, base loss: 23597.44
[INFO 2017-06-25 15:16:01,577 main.py:50] epoch 689, training loss: 14836.35, average training loss: 23001.17, base loss: 23588.97
[INFO 2017-06-25 15:16:05,868 main.py:50] epoch 690, training loss: 17353.33, average training loss: 22993.00, base loss: 23585.00
[INFO 2017-06-25 15:16:10,131 main.py:50] epoch 691, training loss: 16762.65, average training loss: 22984.00, base loss: 23579.43
[INFO 2017-06-25 15:16:14,400 main.py:50] epoch 692, training loss: 14543.02, average training loss: 22971.82, base loss: 23569.67
[INFO 2017-06-25 15:16:18,703 main.py:50] epoch 693, training loss: 29011.65, average training loss: 22980.52, base loss: 23583.95
[INFO 2017-06-25 15:16:22,976 main.py:50] epoch 694, training loss: 20442.70, average training loss: 22976.87, base loss: 23583.98
[INFO 2017-06-25 15:16:27,246 main.py:50] epoch 695, training loss: 16476.82, average training loss: 22967.53, base loss: 23576.50
[INFO 2017-06-25 15:16:31,527 main.py:50] epoch 696, training loss: 21361.79, average training loss: 22965.23, base loss: 23580.69
[INFO 2017-06-25 15:16:35,846 main.py:50] epoch 697, training loss: 14495.24, average training loss: 22953.09, base loss: 23570.48
[INFO 2017-06-25 15:16:40,096 main.py:50] epoch 698, training loss: 16579.14, average training loss: 22943.97, base loss: 23567.52
[INFO 2017-06-25 15:16:44,405 main.py:50] epoch 699, training loss: 18730.77, average training loss: 22937.95, base loss: 23568.13
[INFO 2017-06-25 15:16:48,691 main.py:50] epoch 700, training loss: 26509.72, average training loss: 22943.05, base loss: 23578.71
[INFO 2017-06-25 15:16:52,976 main.py:50] epoch 701, training loss: 19692.43, average training loss: 22938.42, base loss: 23578.28
[INFO 2017-06-25 15:16:57,257 main.py:50] epoch 702, training loss: 20704.40, average training loss: 22935.24, base loss: 23580.07
[INFO 2017-06-25 15:17:01,542 main.py:50] epoch 703, training loss: 20370.81, average training loss: 22931.60, base loss: 23582.41
[INFO 2017-06-25 15:17:05,844 main.py:50] epoch 704, training loss: 17946.55, average training loss: 22924.53, base loss: 23580.59
[INFO 2017-06-25 15:17:10,151 main.py:50] epoch 705, training loss: 16053.66, average training loss: 22914.79, base loss: 23574.23
[INFO 2017-06-25 15:17:14,468 main.py:50] epoch 706, training loss: 19115.14, average training loss: 22909.42, base loss: 23576.17
[INFO 2017-06-25 15:17:18,780 main.py:50] epoch 707, training loss: 22541.88, average training loss: 22908.90, base loss: 23580.74
[INFO 2017-06-25 15:17:23,080 main.py:50] epoch 708, training loss: 20826.57, average training loss: 22905.96, base loss: 23584.03
[INFO 2017-06-25 15:17:27,345 main.py:50] epoch 709, training loss: 21644.78, average training loss: 22904.19, base loss: 23585.24
[INFO 2017-06-25 15:17:31,644 main.py:50] epoch 710, training loss: 18785.95, average training loss: 22898.40, base loss: 23583.03
[INFO 2017-06-25 15:17:35,952 main.py:50] epoch 711, training loss: 17145.19, average training loss: 22890.32, base loss: 23580.39
[INFO 2017-06-25 15:17:40,240 main.py:50] epoch 712, training loss: 21004.44, average training loss: 22887.67, base loss: 23580.07
[INFO 2017-06-25 15:17:44,517 main.py:50] epoch 713, training loss: 15210.70, average training loss: 22876.92, base loss: 23571.88
[INFO 2017-06-25 15:17:48,835 main.py:50] epoch 714, training loss: 14618.89, average training loss: 22865.37, base loss: 23564.15
[INFO 2017-06-25 15:17:53,130 main.py:50] epoch 715, training loss: 24118.11, average training loss: 22867.12, base loss: 23569.51
[INFO 2017-06-25 15:17:57,395 main.py:50] epoch 716, training loss: 20522.05, average training loss: 22863.85, base loss: 23572.05
[INFO 2017-06-25 15:18:01,652 main.py:50] epoch 717, training loss: 19446.45, average training loss: 22859.09, base loss: 23572.10
[INFO 2017-06-25 15:18:05,960 main.py:50] epoch 718, training loss: 18912.97, average training loss: 22853.60, base loss: 23570.24
[INFO 2017-06-25 15:18:10,218 main.py:50] epoch 719, training loss: 24214.84, average training loss: 22855.49, base loss: 23578.78
[INFO 2017-06-25 15:18:14,507 main.py:50] epoch 720, training loss: 19657.89, average training loss: 22851.06, base loss: 23576.90
[INFO 2017-06-25 15:18:18,772 main.py:50] epoch 721, training loss: 25310.72, average training loss: 22854.46, base loss: 23585.62
[INFO 2017-06-25 15:18:23,051 main.py:50] epoch 722, training loss: 18876.18, average training loss: 22848.96, base loss: 23582.90
[INFO 2017-06-25 15:18:27,343 main.py:50] epoch 723, training loss: 18098.69, average training loss: 22842.40, base loss: 23581.33
[INFO 2017-06-25 15:18:31,633 main.py:50] epoch 724, training loss: 18729.54, average training loss: 22836.73, base loss: 23579.59
[INFO 2017-06-25 15:18:35,947 main.py:50] epoch 725, training loss: 22830.09, average training loss: 22836.72, base loss: 23582.63
[INFO 2017-06-25 15:18:40,248 main.py:50] epoch 726, training loss: 16491.47, average training loss: 22827.99, base loss: 23578.18
[INFO 2017-06-25 15:18:44,552 main.py:50] epoch 727, training loss: 18206.06, average training loss: 22821.64, base loss: 23575.94
[INFO 2017-06-25 15:18:48,818 main.py:50] epoch 728, training loss: 18482.83, average training loss: 22815.69, base loss: 23573.21
[INFO 2017-06-25 15:18:53,084 main.py:50] epoch 729, training loss: 15220.32, average training loss: 22805.28, base loss: 23565.28
[INFO 2017-06-25 15:18:57,385 main.py:50] epoch 730, training loss: 17689.35, average training loss: 22798.28, base loss: 23563.40
[INFO 2017-06-25 15:19:01,671 main.py:50] epoch 731, training loss: 17256.75, average training loss: 22790.71, base loss: 23560.29
[INFO 2017-06-25 15:19:05,957 main.py:50] epoch 732, training loss: 15601.93, average training loss: 22780.91, base loss: 23551.81
[INFO 2017-06-25 15:19:10,235 main.py:50] epoch 733, training loss: 21098.12, average training loss: 22778.61, base loss: 23556.43
[INFO 2017-06-25 15:19:14,530 main.py:50] epoch 734, training loss: 18492.55, average training loss: 22772.78, base loss: 23556.12
[INFO 2017-06-25 15:19:18,803 main.py:50] epoch 735, training loss: 23429.61, average training loss: 22773.68, base loss: 23561.91
[INFO 2017-06-25 15:19:23,094 main.py:50] epoch 736, training loss: 25725.77, average training loss: 22777.68, base loss: 23573.10
[INFO 2017-06-25 15:19:27,379 main.py:50] epoch 737, training loss: 18178.89, average training loss: 22771.45, base loss: 23569.44
[INFO 2017-06-25 15:19:31,677 main.py:50] epoch 738, training loss: 20828.99, average training loss: 22768.82, base loss: 23572.96
[INFO 2017-06-25 15:19:35,984 main.py:50] epoch 739, training loss: 23091.69, average training loss: 22769.26, base loss: 23578.86
[INFO 2017-06-25 15:19:40,255 main.py:50] epoch 740, training loss: 22339.80, average training loss: 22768.68, base loss: 23581.06
[INFO 2017-06-25 15:19:44,533 main.py:50] epoch 741, training loss: 18368.25, average training loss: 22762.75, base loss: 23578.78
[INFO 2017-06-25 15:19:48,825 main.py:50] epoch 742, training loss: 24021.98, average training loss: 22764.44, base loss: 23588.67
[INFO 2017-06-25 15:19:53,144 main.py:50] epoch 743, training loss: 17603.14, average training loss: 22757.50, base loss: 23588.48
[INFO 2017-06-25 15:19:57,456 main.py:50] epoch 744, training loss: 20431.89, average training loss: 22754.38, base loss: 23589.45
[INFO 2017-06-25 15:20:01,767 main.py:50] epoch 745, training loss: 23872.64, average training loss: 22755.88, base loss: 23597.00
[INFO 2017-06-25 15:20:06,026 main.py:50] epoch 746, training loss: 18873.76, average training loss: 22750.69, base loss: 23597.50
[INFO 2017-06-25 15:20:10,327 main.py:50] epoch 747, training loss: 27156.98, average training loss: 22756.58, base loss: 23610.92
[INFO 2017-06-25 15:20:14,633 main.py:50] epoch 748, training loss: 21790.13, average training loss: 22755.29, base loss: 23614.57
[INFO 2017-06-25 15:20:18,935 main.py:50] epoch 749, training loss: 17254.78, average training loss: 22747.95, base loss: 23609.69
[INFO 2017-06-25 15:20:23,214 main.py:50] epoch 750, training loss: 24327.56, average training loss: 22750.05, base loss: 23617.19
[INFO 2017-06-25 15:20:27,522 main.py:50] epoch 751, training loss: 17647.42, average training loss: 22743.27, base loss: 23614.17
[INFO 2017-06-25 15:20:31,838 main.py:50] epoch 752, training loss: 16974.65, average training loss: 22735.61, base loss: 23608.89
[INFO 2017-06-25 15:20:36,139 main.py:50] epoch 753, training loss: 20654.96, average training loss: 22732.85, base loss: 23611.28
[INFO 2017-06-25 15:20:40,419 main.py:50] epoch 754, training loss: 20732.98, average training loss: 22730.20, base loss: 23614.73
[INFO 2017-06-25 15:20:44,745 main.py:50] epoch 755, training loss: 18494.17, average training loss: 22724.60, base loss: 23615.56
[INFO 2017-06-25 15:20:49,033 main.py:50] epoch 756, training loss: 20093.51, average training loss: 22721.12, base loss: 23619.09
[INFO 2017-06-25 15:20:53,317 main.py:50] epoch 757, training loss: 25869.32, average training loss: 22725.27, base loss: 23629.22
[INFO 2017-06-25 15:20:57,610 main.py:50] epoch 758, training loss: 22504.31, average training loss: 22724.98, base loss: 23635.15
[INFO 2017-06-25 15:21:01,897 main.py:50] epoch 759, training loss: 17876.51, average training loss: 22718.60, base loss: 23633.39
[INFO 2017-06-25 15:21:06,159 main.py:50] epoch 760, training loss: 20301.11, average training loss: 22715.43, base loss: 23633.91
[INFO 2017-06-25 15:21:10,479 main.py:50] epoch 761, training loss: 18657.87, average training loss: 22710.10, base loss: 23634.51
[INFO 2017-06-25 15:21:14,796 main.py:50] epoch 762, training loss: 17618.99, average training loss: 22703.43, base loss: 23632.06
[INFO 2017-06-25 15:21:19,056 main.py:50] epoch 763, training loss: 15733.85, average training loss: 22694.31, base loss: 23625.24
[INFO 2017-06-25 15:21:23,348 main.py:50] epoch 764, training loss: 18846.71, average training loss: 22689.28, base loss: 23624.47
[INFO 2017-06-25 15:21:27,640 main.py:50] epoch 765, training loss: 16201.78, average training loss: 22680.81, base loss: 23617.71
[INFO 2017-06-25 15:21:31,951 main.py:50] epoch 766, training loss: 18600.57, average training loss: 22675.49, base loss: 23616.64
[INFO 2017-06-25 15:21:36,251 main.py:50] epoch 767, training loss: 18153.37, average training loss: 22669.60, base loss: 23613.77
[INFO 2017-06-25 15:21:40,554 main.py:50] epoch 768, training loss: 18503.67, average training loss: 22664.18, base loss: 23612.31
[INFO 2017-06-25 15:21:44,819 main.py:50] epoch 769, training loss: 15830.87, average training loss: 22655.31, base loss: 23606.31
[INFO 2017-06-25 15:21:49,067 main.py:50] epoch 770, training loss: 21399.61, average training loss: 22653.68, base loss: 23606.87
[INFO 2017-06-25 15:21:53,357 main.py:50] epoch 771, training loss: 16039.61, average training loss: 22645.11, base loss: 23601.75
[INFO 2017-06-25 15:21:57,630 main.py:50] epoch 772, training loss: 19042.14, average training loss: 22640.45, base loss: 23600.35
[INFO 2017-06-25 15:22:01,935 main.py:50] epoch 773, training loss: 20691.89, average training loss: 22637.93, base loss: 23599.01
[INFO 2017-06-25 15:22:06,253 main.py:50] epoch 774, training loss: 18672.26, average training loss: 22632.82, base loss: 23599.50
[INFO 2017-06-25 15:22:10,597 main.py:50] epoch 775, training loss: 24745.23, average training loss: 22635.54, base loss: 23607.74
[INFO 2017-06-25 15:22:14,911 main.py:50] epoch 776, training loss: 14543.41, average training loss: 22625.12, base loss: 23598.71
[INFO 2017-06-25 15:22:19,218 main.py:50] epoch 777, training loss: 17146.76, average training loss: 22618.08, base loss: 23595.11
[INFO 2017-06-25 15:22:23,520 main.py:50] epoch 778, training loss: 16837.64, average training loss: 22610.66, base loss: 23590.32
[INFO 2017-06-25 15:22:27,827 main.py:50] epoch 779, training loss: 23130.71, average training loss: 22611.33, base loss: 23595.42
[INFO 2017-06-25 15:22:32,151 main.py:50] epoch 780, training loss: 17871.25, average training loss: 22605.26, base loss: 23592.81
[INFO 2017-06-25 15:22:36,461 main.py:50] epoch 781, training loss: 19460.22, average training loss: 22601.24, base loss: 23593.49
[INFO 2017-06-25 15:22:40,765 main.py:50] epoch 782, training loss: 22411.78, average training loss: 22601.00, base loss: 23598.36
[INFO 2017-06-25 15:22:45,047 main.py:50] epoch 783, training loss: 20062.67, average training loss: 22597.76, base loss: 23599.30
[INFO 2017-06-25 15:22:49,351 main.py:50] epoch 784, training loss: 24824.98, average training loss: 22600.60, base loss: 23606.33
[INFO 2017-06-25 15:22:53,649 main.py:50] epoch 785, training loss: 20474.34, average training loss: 22597.89, base loss: 23605.63
[INFO 2017-06-25 15:22:57,945 main.py:50] epoch 786, training loss: 19647.91, average training loss: 22594.14, base loss: 23604.34
[INFO 2017-06-25 15:23:02,242 main.py:50] epoch 787, training loss: 18889.49, average training loss: 22589.44, base loss: 23603.84
[INFO 2017-06-25 15:23:06,575 main.py:50] epoch 788, training loss: 17284.39, average training loss: 22582.72, base loss: 23599.50
[INFO 2017-06-25 15:23:10,909 main.py:50] epoch 789, training loss: 23039.61, average training loss: 22583.30, base loss: 23604.93
[INFO 2017-06-25 15:23:15,178 main.py:50] epoch 790, training loss: 18983.15, average training loss: 22578.74, base loss: 23601.79
[INFO 2017-06-25 15:23:19,520 main.py:50] epoch 791, training loss: 25412.68, average training loss: 22582.32, base loss: 23612.46
[INFO 2017-06-25 15:23:23,835 main.py:50] epoch 792, training loss: 17504.12, average training loss: 22575.92, base loss: 23609.54
[INFO 2017-06-25 15:23:28,128 main.py:50] epoch 793, training loss: 17874.93, average training loss: 22570.00, base loss: 23606.66
[INFO 2017-06-25 15:23:32,433 main.py:50] epoch 794, training loss: 19009.94, average training loss: 22565.52, base loss: 23605.46
[INFO 2017-06-25 15:23:36,749 main.py:50] epoch 795, training loss: 20429.88, average training loss: 22562.84, base loss: 23605.60
[INFO 2017-06-25 15:23:41,018 main.py:50] epoch 796, training loss: 19935.61, average training loss: 22559.54, base loss: 23604.66
[INFO 2017-06-25 15:23:45,329 main.py:50] epoch 797, training loss: 16336.77, average training loss: 22551.74, base loss: 23596.46
[INFO 2017-06-25 15:23:49,635 main.py:50] epoch 798, training loss: 19440.08, average training loss: 22547.85, base loss: 23595.19
[INFO 2017-06-25 15:23:53,935 main.py:50] epoch 799, training loss: 25606.54, average training loss: 22551.67, base loss: 23602.73
[INFO 2017-06-25 15:23:58,224 main.py:50] epoch 800, training loss: 14698.75, average training loss: 22541.87, base loss: 23595.71
[INFO 2017-06-25 15:24:02,545 main.py:50] epoch 801, training loss: 20204.14, average training loss: 22538.95, base loss: 23597.90
[INFO 2017-06-25 15:24:06,837 main.py:50] epoch 802, training loss: 14578.16, average training loss: 22529.04, base loss: 23591.44
[INFO 2017-06-25 15:24:11,124 main.py:50] epoch 803, training loss: 20249.40, average training loss: 22526.20, base loss: 23592.39
[INFO 2017-06-25 15:24:15,424 main.py:50] epoch 804, training loss: 17243.75, average training loss: 22519.64, base loss: 23590.69
[INFO 2017-06-25 15:24:19,729 main.py:50] epoch 805, training loss: 19110.51, average training loss: 22515.41, base loss: 23590.00
[INFO 2017-06-25 15:24:24,005 main.py:50] epoch 806, training loss: 17556.41, average training loss: 22509.27, base loss: 23587.11
[INFO 2017-06-25 15:24:28,293 main.py:50] epoch 807, training loss: 18676.47, average training loss: 22504.52, base loss: 23587.07
[INFO 2017-06-25 15:24:32,564 main.py:50] epoch 808, training loss: 19858.82, average training loss: 22501.25, base loss: 23588.40
[INFO 2017-06-25 15:24:36,894 main.py:50] epoch 809, training loss: 17205.57, average training loss: 22494.72, base loss: 23584.07
[INFO 2017-06-25 15:24:41,187 main.py:50] epoch 810, training loss: 13674.40, average training loss: 22483.84, base loss: 23576.50
[INFO 2017-06-25 15:24:45,476 main.py:50] epoch 811, training loss: 19331.80, average training loss: 22479.96, base loss: 23577.80
[INFO 2017-06-25 15:24:49,758 main.py:50] epoch 812, training loss: 19158.17, average training loss: 22475.87, base loss: 23576.60
[INFO 2017-06-25 15:24:54,039 main.py:50] epoch 813, training loss: 18354.00, average training loss: 22470.81, base loss: 23574.70
[INFO 2017-06-25 15:24:58,342 main.py:50] epoch 814, training loss: 23072.62, average training loss: 22471.55, base loss: 23581.00
[INFO 2017-06-25 15:25:02,652 main.py:50] epoch 815, training loss: 17210.62, average training loss: 22465.10, base loss: 23579.41
[INFO 2017-06-25 15:25:06,941 main.py:50] epoch 816, training loss: 21105.96, average training loss: 22463.44, base loss: 23583.67
[INFO 2017-06-25 15:25:11,258 main.py:50] epoch 817, training loss: 19396.04, average training loss: 22459.69, base loss: 23584.95
[INFO 2017-06-25 15:25:15,544 main.py:50] epoch 818, training loss: 18864.36, average training loss: 22455.30, base loss: 23582.65
[INFO 2017-06-25 15:25:19,880 main.py:50] epoch 819, training loss: 18902.91, average training loss: 22450.96, base loss: 23581.77
[INFO 2017-06-25 15:25:24,157 main.py:50] epoch 820, training loss: 17656.21, average training loss: 22445.12, base loss: 23579.73
[INFO 2017-06-25 15:25:28,442 main.py:50] epoch 821, training loss: 21306.04, average training loss: 22443.74, base loss: 23581.23
[INFO 2017-06-25 15:25:32,729 main.py:50] epoch 822, training loss: 21766.89, average training loss: 22442.92, base loss: 23585.09
[INFO 2017-06-25 15:25:37,018 main.py:50] epoch 823, training loss: 21693.76, average training loss: 22442.01, base loss: 23589.82
[INFO 2017-06-25 15:25:41,282 main.py:50] epoch 824, training loss: 17283.67, average training loss: 22435.75, base loss: 23585.88
[INFO 2017-06-25 15:25:45,583 main.py:50] epoch 825, training loss: 20562.41, average training loss: 22433.49, base loss: 23588.00
[INFO 2017-06-25 15:25:49,881 main.py:50] epoch 826, training loss: 20630.91, average training loss: 22431.31, base loss: 23591.68
[INFO 2017-06-25 15:25:54,180 main.py:50] epoch 827, training loss: 16786.31, average training loss: 22424.49, base loss: 23588.76
[INFO 2017-06-25 15:25:58,514 main.py:50] epoch 828, training loss: 17713.10, average training loss: 22418.81, base loss: 23586.87
[INFO 2017-06-25 15:26:02,835 main.py:50] epoch 829, training loss: 15706.80, average training loss: 22410.72, base loss: 23581.99
[INFO 2017-06-25 15:26:07,159 main.py:50] epoch 830, training loss: 20483.38, average training loss: 22408.40, base loss: 23585.46
[INFO 2017-06-25 15:26:11,524 main.py:50] epoch 831, training loss: 17833.70, average training loss: 22402.90, base loss: 23583.45
[INFO 2017-06-25 15:26:15,860 main.py:50] epoch 832, training loss: 20967.73, average training loss: 22401.18, base loss: 23587.31
[INFO 2017-06-25 15:26:20,187 main.py:50] epoch 833, training loss: 23517.88, average training loss: 22402.52, base loss: 23591.46
[INFO 2017-06-25 15:26:24,529 main.py:50] epoch 834, training loss: 19397.61, average training loss: 22398.92, base loss: 23591.09
[INFO 2017-06-25 15:26:28,839 main.py:50] epoch 835, training loss: 21542.85, average training loss: 22397.89, base loss: 23594.07
[INFO 2017-06-25 15:26:33,137 main.py:50] epoch 836, training loss: 18289.41, average training loss: 22392.99, base loss: 23590.83
[INFO 2017-06-25 15:26:37,420 main.py:50] epoch 837, training loss: 22179.92, average training loss: 22392.73, base loss: 23595.34
[INFO 2017-06-25 15:26:41,729 main.py:50] epoch 838, training loss: 21669.76, average training loss: 22391.87, base loss: 23600.11
[INFO 2017-06-25 15:26:46,020 main.py:50] epoch 839, training loss: 16909.63, average training loss: 22385.34, base loss: 23597.23
[INFO 2017-06-25 15:26:50,317 main.py:50] epoch 840, training loss: 22467.05, average training loss: 22385.44, base loss: 23601.72
[INFO 2017-06-25 15:26:54,649 main.py:50] epoch 841, training loss: 18681.08, average training loss: 22381.04, base loss: 23600.42
[INFO 2017-06-25 15:26:58,922 main.py:50] epoch 842, training loss: 15877.89, average training loss: 22373.33, base loss: 23594.76
[INFO 2017-06-25 15:27:03,197 main.py:50] epoch 843, training loss: 17513.30, average training loss: 22367.57, base loss: 23593.48
[INFO 2017-06-25 15:27:07,491 main.py:50] epoch 844, training loss: 19449.39, average training loss: 22364.11, base loss: 23594.13
[INFO 2017-06-25 15:27:11,787 main.py:50] epoch 845, training loss: 16377.32, average training loss: 22357.04, base loss: 23588.71
[INFO 2017-06-25 15:27:16,091 main.py:50] epoch 846, training loss: 21355.38, average training loss: 22355.86, base loss: 23593.17
[INFO 2017-06-25 15:27:20,384 main.py:50] epoch 847, training loss: 21523.13, average training loss: 22354.87, base loss: 23598.18
[INFO 2017-06-25 15:27:24,695 main.py:50] epoch 848, training loss: 18485.56, average training loss: 22350.32, base loss: 23595.66
[INFO 2017-06-25 15:27:28,990 main.py:50] epoch 849, training loss: 20166.43, average training loss: 22347.75, base loss: 23595.58
[INFO 2017-06-25 15:27:33,287 main.py:50] epoch 850, training loss: 19450.56, average training loss: 22344.34, base loss: 23594.93
[INFO 2017-06-25 15:27:37,589 main.py:50] epoch 851, training loss: 18706.21, average training loss: 22340.07, base loss: 23592.34
[INFO 2017-06-25 15:27:41,878 main.py:50] epoch 852, training loss: 20883.24, average training loss: 22338.36, base loss: 23595.28
[INFO 2017-06-25 15:27:46,202 main.py:50] epoch 853, training loss: 21150.21, average training loss: 22336.97, base loss: 23597.64
[INFO 2017-06-25 15:27:50,486 main.py:50] epoch 854, training loss: 18622.12, average training loss: 22332.63, base loss: 23597.45
[INFO 2017-06-25 15:27:54,805 main.py:50] epoch 855, training loss: 18741.54, average training loss: 22328.43, base loss: 23596.26
[INFO 2017-06-25 15:27:59,092 main.py:50] epoch 856, training loss: 20347.46, average training loss: 22326.12, base loss: 23597.35
[INFO 2017-06-25 15:28:03,415 main.py:50] epoch 857, training loss: 23639.01, average training loss: 22327.65, base loss: 23603.94
[INFO 2017-06-25 15:28:07,721 main.py:50] epoch 858, training loss: 17928.03, average training loss: 22322.53, base loss: 23600.39
[INFO 2017-06-25 15:28:12,004 main.py:50] epoch 859, training loss: 13639.47, average training loss: 22312.43, base loss: 23591.08
[INFO 2017-06-25 15:28:16,322 main.py:50] epoch 860, training loss: 18477.76, average training loss: 22307.98, base loss: 23591.02
[INFO 2017-06-25 15:28:20,653 main.py:50] epoch 861, training loss: 20131.20, average training loss: 22305.45, base loss: 23594.11
[INFO 2017-06-25 15:28:24,963 main.py:50] epoch 862, training loss: 19604.80, average training loss: 22302.32, base loss: 23596.81
[INFO 2017-06-25 15:28:29,239 main.py:50] epoch 863, training loss: 14865.43, average training loss: 22293.72, base loss: 23593.09
[INFO 2017-06-25 15:28:33,534 main.py:50] epoch 864, training loss: 20415.28, average training loss: 22291.55, base loss: 23595.15
[INFO 2017-06-25 15:28:37,839 main.py:50] epoch 865, training loss: 16480.55, average training loss: 22284.84, base loss: 23591.42
[INFO 2017-06-25 15:28:42,132 main.py:50] epoch 866, training loss: 16042.76, average training loss: 22277.64, base loss: 23585.57
[INFO 2017-06-25 15:28:46,429 main.py:50] epoch 867, training loss: 15142.62, average training loss: 22269.42, base loss: 23578.64
[INFO 2017-06-25 15:28:50,728 main.py:50] epoch 868, training loss: 16964.90, average training loss: 22263.31, base loss: 23575.65
[INFO 2017-06-25 15:28:55,017 main.py:50] epoch 869, training loss: 16077.20, average training loss: 22256.20, base loss: 23569.63
[INFO 2017-06-25 15:28:59,311 main.py:50] epoch 870, training loss: 14587.35, average training loss: 22247.40, base loss: 23563.12
[INFO 2017-06-25 15:29:03,605 main.py:50] epoch 871, training loss: 19702.29, average training loss: 22244.48, base loss: 23564.45
[INFO 2017-06-25 15:29:07,902 main.py:50] epoch 872, training loss: 19894.02, average training loss: 22241.79, base loss: 23563.97
[INFO 2017-06-25 15:29:12,193 main.py:50] epoch 873, training loss: 20163.20, average training loss: 22239.41, base loss: 23564.76
[INFO 2017-06-25 15:29:16,472 main.py:50] epoch 874, training loss: 15734.39, average training loss: 22231.97, base loss: 23560.29
[INFO 2017-06-25 15:29:20,767 main.py:50] epoch 875, training loss: 23836.22, average training loss: 22233.80, base loss: 23565.37
[INFO 2017-06-25 15:29:25,068 main.py:50] epoch 876, training loss: 18017.96, average training loss: 22229.00, base loss: 23563.14
[INFO 2017-06-25 15:29:29,385 main.py:50] epoch 877, training loss: 22343.52, average training loss: 22229.13, base loss: 23567.14
[INFO 2017-06-25 15:29:33,699 main.py:50] epoch 878, training loss: 17463.95, average training loss: 22223.71, base loss: 23563.89
[INFO 2017-06-25 15:29:37,979 main.py:50] epoch 879, training loss: 19782.14, average training loss: 22220.93, base loss: 23561.95
[INFO 2017-06-25 15:29:42,287 main.py:50] epoch 880, training loss: 20637.74, average training loss: 22219.13, base loss: 23563.56
[INFO 2017-06-25 15:29:46,610 main.py:50] epoch 881, training loss: 18400.38, average training loss: 22214.81, base loss: 23562.94
[INFO 2017-06-25 15:29:50,912 main.py:50] epoch 882, training loss: 21201.24, average training loss: 22213.66, base loss: 23564.40
[INFO 2017-06-25 15:29:55,202 main.py:50] epoch 883, training loss: 19079.17, average training loss: 22210.11, base loss: 23564.20
[INFO 2017-06-25 15:29:59,518 main.py:50] epoch 884, training loss: 23878.52, average training loss: 22212.00, base loss: 23570.73
[INFO 2017-06-25 15:30:03,839 main.py:50] epoch 885, training loss: 14856.96, average training loss: 22203.70, base loss: 23564.95
[INFO 2017-06-25 15:30:08,132 main.py:50] epoch 886, training loss: 19967.51, average training loss: 22201.17, base loss: 23564.41
[INFO 2017-06-25 15:30:12,416 main.py:50] epoch 887, training loss: 24641.02, average training loss: 22203.92, base loss: 23570.52
[INFO 2017-06-25 15:30:16,689 main.py:50] epoch 888, training loss: 18876.80, average training loss: 22200.18, base loss: 23568.81
[INFO 2017-06-25 15:30:20,971 main.py:50] epoch 889, training loss: 18975.71, average training loss: 22196.56, base loss: 23569.37
[INFO 2017-06-25 15:30:25,336 main.py:50] epoch 890, training loss: 15801.47, average training loss: 22189.38, base loss: 23564.02
[INFO 2017-06-25 15:30:29,654 main.py:50] epoch 891, training loss: 17419.34, average training loss: 22184.03, base loss: 23561.72
[INFO 2017-06-25 15:30:33,942 main.py:50] epoch 892, training loss: 17564.99, average training loss: 22178.86, base loss: 23559.08
[INFO 2017-06-25 15:30:38,211 main.py:50] epoch 893, training loss: 15665.14, average training loss: 22171.57, base loss: 23554.08
[INFO 2017-06-25 15:30:42,513 main.py:50] epoch 894, training loss: 17114.45, average training loss: 22165.92, base loss: 23552.40
[INFO 2017-06-25 15:30:46,836 main.py:50] epoch 895, training loss: 19118.77, average training loss: 22162.52, base loss: 23551.86
[INFO 2017-06-25 15:30:51,146 main.py:50] epoch 896, training loss: 16372.21, average training loss: 22156.07, base loss: 23547.63
[INFO 2017-06-25 15:30:55,451 main.py:50] epoch 897, training loss: 19296.77, average training loss: 22152.88, base loss: 23547.67
[INFO 2017-06-25 15:30:59,728 main.py:50] epoch 898, training loss: 15187.26, average training loss: 22145.13, base loss: 23542.09
[INFO 2017-06-25 15:31:04,014 main.py:50] epoch 899, training loss: 25339.25, average training loss: 22148.68, base loss: 23550.86
[INFO 2017-06-25 15:31:08,373 main.py:50] epoch 900, training loss: 17242.41, average training loss: 22143.24, base loss: 23548.06
[INFO 2017-06-25 15:31:12,700 main.py:50] epoch 901, training loss: 15606.70, average training loss: 22135.99, base loss: 23541.74
[INFO 2017-06-25 15:31:16,992 main.py:50] epoch 902, training loss: 21751.55, average training loss: 22135.57, base loss: 23544.60
[INFO 2017-06-25 15:31:21,295 main.py:50] epoch 903, training loss: 21093.28, average training loss: 22134.41, base loss: 23547.06
[INFO 2017-06-25 15:31:25,573 main.py:50] epoch 904, training loss: 18992.80, average training loss: 22130.94, base loss: 23545.76
[INFO 2017-06-25 15:31:29,854 main.py:50] epoch 905, training loss: 19007.26, average training loss: 22127.49, base loss: 23546.06
[INFO 2017-06-25 15:31:34,154 main.py:50] epoch 906, training loss: 19607.90, average training loss: 22124.72, base loss: 23547.97
[INFO 2017-06-25 15:31:38,447 main.py:50] epoch 907, training loss: 16418.80, average training loss: 22118.43, base loss: 23543.17
[INFO 2017-06-25 15:31:42,777 main.py:50] epoch 908, training loss: 20712.38, average training loss: 22116.88, base loss: 23544.04
[INFO 2017-06-25 15:31:47,082 main.py:50] epoch 909, training loss: 21430.70, average training loss: 22116.13, base loss: 23546.68
[INFO 2017-06-25 15:31:51,370 main.py:50] epoch 910, training loss: 17294.09, average training loss: 22110.84, base loss: 23543.35
[INFO 2017-06-25 15:31:55,663 main.py:50] epoch 911, training loss: 20292.03, average training loss: 22108.84, base loss: 23545.70
[INFO 2017-06-25 15:31:59,960 main.py:50] epoch 912, training loss: 16944.92, average training loss: 22103.19, base loss: 23543.99
[INFO 2017-06-25 15:32:04,260 main.py:50] epoch 913, training loss: 15458.22, average training loss: 22095.92, base loss: 23538.50
[INFO 2017-06-25 15:32:08,559 main.py:50] epoch 914, training loss: 21404.06, average training loss: 22095.16, base loss: 23541.94
[INFO 2017-06-25 15:32:12,855 main.py:50] epoch 915, training loss: 20951.47, average training loss: 22093.91, base loss: 23545.08
[INFO 2017-06-25 15:32:17,155 main.py:50] epoch 916, training loss: 18057.93, average training loss: 22089.51, base loss: 23543.02
[INFO 2017-06-25 15:32:21,471 main.py:50] epoch 917, training loss: 22154.40, average training loss: 22089.58, base loss: 23547.80
[INFO 2017-06-25 15:32:25,752 main.py:50] epoch 918, training loss: 19686.04, average training loss: 22086.97, base loss: 23549.71
[INFO 2017-06-25 15:32:30,078 main.py:50] epoch 919, training loss: 18781.57, average training loss: 22083.37, base loss: 23547.59
[INFO 2017-06-25 15:32:34,347 main.py:50] epoch 920, training loss: 16760.90, average training loss: 22077.59, base loss: 23543.06
[INFO 2017-06-25 15:32:38,640 main.py:50] epoch 921, training loss: 17990.04, average training loss: 22073.16, base loss: 23541.22
[INFO 2017-06-25 15:32:42,930 main.py:50] epoch 922, training loss: 16033.33, average training loss: 22066.62, base loss: 23539.62
[INFO 2017-06-25 15:32:47,222 main.py:50] epoch 923, training loss: 15181.27, average training loss: 22059.17, base loss: 23534.03
[INFO 2017-06-25 15:32:51,545 main.py:50] epoch 924, training loss: 23542.43, average training loss: 22060.77, base loss: 23539.71
[INFO 2017-06-25 15:32:55,830 main.py:50] epoch 925, training loss: 19024.20, average training loss: 22057.49, base loss: 23539.04
[INFO 2017-06-25 15:33:00,116 main.py:50] epoch 926, training loss: 18717.20, average training loss: 22053.89, base loss: 23540.64
[INFO 2017-06-25 15:33:04,433 main.py:50] epoch 927, training loss: 23402.71, average training loss: 22055.34, base loss: 23548.12
[INFO 2017-06-25 15:33:08,724 main.py:50] epoch 928, training loss: 19665.77, average training loss: 22052.77, base loss: 23550.00
[INFO 2017-06-25 15:33:13,012 main.py:50] epoch 929, training loss: 20498.53, average training loss: 22051.10, base loss: 23552.93
[INFO 2017-06-25 15:33:17,303 main.py:50] epoch 930, training loss: 20121.65, average training loss: 22049.02, base loss: 23554.69
[INFO 2017-06-25 15:33:21,578 main.py:50] epoch 931, training loss: 20330.30, average training loss: 22047.18, base loss: 23555.68
[INFO 2017-06-25 15:33:25,876 main.py:50] epoch 932, training loss: 18735.05, average training loss: 22043.63, base loss: 23554.36
[INFO 2017-06-25 15:33:30,160 main.py:50] epoch 933, training loss: 17327.26, average training loss: 22038.58, base loss: 23550.37
[INFO 2017-06-25 15:33:34,475 main.py:50] epoch 934, training loss: 19310.26, average training loss: 22035.66, base loss: 23551.29
[INFO 2017-06-25 15:33:38,805 main.py:50] epoch 935, training loss: 21051.76, average training loss: 22034.61, base loss: 23552.91
[INFO 2017-06-25 15:33:43,120 main.py:50] epoch 936, training loss: 20717.71, average training loss: 22033.21, base loss: 23556.62
[INFO 2017-06-25 15:33:47,426 main.py:50] epoch 937, training loss: 20179.01, average training loss: 22031.23, base loss: 23558.73
[INFO 2017-06-25 15:33:51,720 main.py:50] epoch 938, training loss: 19027.11, average training loss: 22028.03, base loss: 23560.15
[INFO 2017-06-25 15:33:56,049 main.py:50] epoch 939, training loss: 19651.89, average training loss: 22025.50, base loss: 23560.75
[INFO 2017-06-25 15:34:00,331 main.py:50] epoch 940, training loss: 14183.38, average training loss: 22017.17, base loss: 23554.44
[INFO 2017-06-25 15:34:04,661 main.py:50] epoch 941, training loss: 22938.62, average training loss: 22018.15, base loss: 23557.37
[INFO 2017-06-25 15:34:08,969 main.py:50] epoch 942, training loss: 19020.30, average training loss: 22014.97, base loss: 23557.14
[INFO 2017-06-25 15:34:13,275 main.py:50] epoch 943, training loss: 19972.66, average training loss: 22012.80, base loss: 23559.44
[INFO 2017-06-25 15:34:17,553 main.py:50] epoch 944, training loss: 20979.30, average training loss: 22011.71, base loss: 23561.79
[INFO 2017-06-25 15:34:21,861 main.py:50] epoch 945, training loss: 23043.38, average training loss: 22012.80, base loss: 23566.79
[INFO 2017-06-25 15:34:26,155 main.py:50] epoch 946, training loss: 20678.64, average training loss: 22011.39, base loss: 23569.05
[INFO 2017-06-25 15:34:30,472 main.py:50] epoch 947, training loss: 18166.75, average training loss: 22007.34, base loss: 23570.61
[INFO 2017-06-25 15:34:34,775 main.py:50] epoch 948, training loss: 23796.11, average training loss: 22009.22, base loss: 23576.37
[INFO 2017-06-25 15:34:39,057 main.py:50] epoch 949, training loss: 23086.17, average training loss: 22010.35, base loss: 23582.92
[INFO 2017-06-25 15:34:43,358 main.py:50] epoch 950, training loss: 14222.37, average training loss: 22002.17, base loss: 23577.64
[INFO 2017-06-25 15:34:47,659 main.py:50] epoch 951, training loss: 17555.25, average training loss: 21997.49, base loss: 23576.19
[INFO 2017-06-25 15:34:51,958 main.py:50] epoch 952, training loss: 18378.83, average training loss: 21993.70, base loss: 23575.29
[INFO 2017-06-25 15:34:56,246 main.py:50] epoch 953, training loss: 15569.39, average training loss: 21986.96, base loss: 23571.49
[INFO 2017-06-25 15:35:00,531 main.py:50] epoch 954, training loss: 24000.94, average training loss: 21989.07, base loss: 23576.45
[INFO 2017-06-25 15:35:04,847 main.py:50] epoch 955, training loss: 20140.22, average training loss: 21987.14, base loss: 23575.60
[INFO 2017-06-25 15:35:09,179 main.py:50] epoch 956, training loss: 24166.05, average training loss: 21989.41, base loss: 23581.92
[INFO 2017-06-25 15:35:13,478 main.py:50] epoch 957, training loss: 17200.88, average training loss: 21984.42, base loss: 23581.47
[INFO 2017-06-25 15:35:17,749 main.py:50] epoch 958, training loss: 15923.89, average training loss: 21978.10, base loss: 23577.17
[INFO 2017-06-25 15:35:22,057 main.py:50] epoch 959, training loss: 19787.26, average training loss: 21975.81, base loss: 23576.69
[INFO 2017-06-25 15:35:26,332 main.py:50] epoch 960, training loss: 17238.30, average training loss: 21970.88, base loss: 23574.74
[INFO 2017-06-25 15:35:30,626 main.py:50] epoch 961, training loss: 19331.87, average training loss: 21968.14, base loss: 23574.29
[INFO 2017-06-25 15:35:34,938 main.py:50] epoch 962, training loss: 13409.70, average training loss: 21959.25, base loss: 23566.04
[INFO 2017-06-25 15:35:39,218 main.py:50] epoch 963, training loss: 20731.75, average training loss: 21957.98, base loss: 23566.32
[INFO 2017-06-25 15:35:43,495 main.py:50] epoch 964, training loss: 19756.21, average training loss: 21955.70, base loss: 23565.87
[INFO 2017-06-25 15:35:47,773 main.py:50] epoch 965, training loss: 20152.02, average training loss: 21953.83, base loss: 23568.28
[INFO 2017-06-25 15:35:52,044 main.py:50] epoch 966, training loss: 21389.33, average training loss: 21953.25, base loss: 23570.46
[INFO 2017-06-25 15:35:56,342 main.py:50] epoch 967, training loss: 17267.18, average training loss: 21948.41, base loss: 23568.07
[INFO 2017-06-25 15:36:00,642 main.py:50] epoch 968, training loss: 18521.67, average training loss: 21944.87, base loss: 23565.42
[INFO 2017-06-25 15:36:04,950 main.py:50] epoch 969, training loss: 22397.06, average training loss: 21945.34, base loss: 23569.54
[INFO 2017-06-25 15:36:09,259 main.py:50] epoch 970, training loss: 14945.58, average training loss: 21938.13, base loss: 23564.54
[INFO 2017-06-25 15:36:13,542 main.py:50] epoch 971, training loss: 18559.26, average training loss: 21934.65, base loss: 23563.55
[INFO 2017-06-25 15:36:17,842 main.py:50] epoch 972, training loss: 20384.68, average training loss: 21933.06, base loss: 23565.95
[INFO 2017-06-25 15:36:22,147 main.py:50] epoch 973, training loss: 19877.16, average training loss: 21930.95, base loss: 23567.58
[INFO 2017-06-25 15:36:26,480 main.py:50] epoch 974, training loss: 17716.48, average training loss: 21926.63, base loss: 23567.14
[INFO 2017-06-25 15:36:30,800 main.py:50] epoch 975, training loss: 18843.96, average training loss: 21923.47, base loss: 23566.70
[INFO 2017-06-25 15:36:35,098 main.py:50] epoch 976, training loss: 18250.70, average training loss: 21919.71, base loss: 23565.79
[INFO 2017-06-25 15:36:39,399 main.py:50] epoch 977, training loss: 22450.14, average training loss: 21920.25, base loss: 23571.06
[INFO 2017-06-25 15:36:43,690 main.py:50] epoch 978, training loss: 15823.66, average training loss: 21914.02, base loss: 23567.74
[INFO 2017-06-25 15:36:47,986 main.py:50] epoch 979, training loss: 23908.56, average training loss: 21916.06, base loss: 23574.12
[INFO 2017-06-25 15:36:52,272 main.py:50] epoch 980, training loss: 24069.02, average training loss: 21918.25, base loss: 23578.74
[INFO 2017-06-25 15:36:56,587 main.py:50] epoch 981, training loss: 19733.71, average training loss: 21916.03, base loss: 23579.34
[INFO 2017-06-25 15:37:00,879 main.py:50] epoch 982, training loss: 17783.00, average training loss: 21911.82, base loss: 23577.96
[INFO 2017-06-25 15:37:05,203 main.py:50] epoch 983, training loss: 19584.24, average training loss: 21909.46, base loss: 23578.05
[INFO 2017-06-25 15:37:09,498 main.py:50] epoch 984, training loss: 18194.08, average training loss: 21905.69, base loss: 23577.35
[INFO 2017-06-25 15:37:13,790 main.py:50] epoch 985, training loss: 23417.37, average training loss: 21907.22, base loss: 23582.09
[INFO 2017-06-25 15:37:18,074 main.py:50] epoch 986, training loss: 17472.57, average training loss: 21902.73, base loss: 23580.18
[INFO 2017-06-25 15:37:22,390 main.py:50] epoch 987, training loss: 22057.73, average training loss: 21902.88, base loss: 23584.27
[INFO 2017-06-25 15:37:26,675 main.py:50] epoch 988, training loss: 20172.23, average training loss: 21901.13, base loss: 23584.07
[INFO 2017-06-25 15:37:30,991 main.py:50] epoch 989, training loss: 19209.73, average training loss: 21898.42, base loss: 23584.81
[INFO 2017-06-25 15:37:35,292 main.py:50] epoch 990, training loss: 18689.71, average training loss: 21895.18, base loss: 23583.63
[INFO 2017-06-25 15:37:39,592 main.py:50] epoch 991, training loss: 21113.98, average training loss: 21894.39, base loss: 23586.77
[INFO 2017-06-25 15:37:43,872 main.py:50] epoch 992, training loss: 15908.85, average training loss: 21888.36, base loss: 23582.89
[INFO 2017-06-25 15:37:48,120 main.py:50] epoch 993, training loss: 14554.08, average training loss: 21880.98, base loss: 23578.36
[INFO 2017-06-25 15:37:52,381 main.py:50] epoch 994, training loss: 17672.05, average training loss: 21876.75, base loss: 23576.50
[INFO 2017-06-25 15:37:56,650 main.py:50] epoch 995, training loss: 18825.34, average training loss: 21873.69, base loss: 23573.94
[INFO 2017-06-25 15:38:00,938 main.py:50] epoch 996, training loss: 18948.97, average training loss: 21870.76, base loss: 23575.36
[INFO 2017-06-25 15:38:05,201 main.py:50] epoch 997, training loss: 19802.48, average training loss: 21868.68, base loss: 23575.31
[INFO 2017-06-25 15:38:09,478 main.py:50] epoch 998, training loss: 18481.47, average training loss: 21865.29, base loss: 23574.21
[INFO 2017-06-25 15:38:13,748 main.py:50] epoch 999, training loss: 18041.10, average training loss: 21861.47, base loss: 23572.05
[INFO 2017-06-25 15:38:13,748 main.py:52] epoch 999, testing
[INFO 2017-06-25 15:41:51,676 main.py:100] average testing loss: 19112.02
[INFO 2017-06-25 15:41:51,680 main.py:72] model save to ./model/final.pth
[INFO 2017-06-25 15:41:51,686 main.py:76] current best accuracy: 19112.02
[INFO 2017-06-25 15:41:55,957 main.py:50] epoch 1000, training loss: 16634.17, average training loss: 21678.81, base loss: 23567.72
[INFO 2017-06-25 15:42:00,224 main.py:50] epoch 1001, training loss: 19995.51, average training loss: 21519.39, base loss: 23565.21
[INFO 2017-06-25 15:42:04,491 main.py:50] epoch 1002, training loss: 20679.11, average training loss: 21385.93, base loss: 23563.94
[INFO 2017-06-25 15:42:08,810 main.py:50] epoch 1003, training loss: 17778.68, average training loss: 21278.71, base loss: 23561.56
[INFO 2017-06-25 15:42:13,141 main.py:50] epoch 1004, training loss: 17478.14, average training loss: 21183.41, base loss: 23557.82
[INFO 2017-06-25 15:42:17,498 main.py:50] epoch 1005, training loss: 17553.89, average training loss: 21112.02, base loss: 23553.28
[INFO 2017-06-25 15:42:21,826 main.py:50] epoch 1006, training loss: 23098.35, average training loss: 21046.57, base loss: 23561.98
[INFO 2017-06-25 15:42:26,133 main.py:50] epoch 1007, training loss: 22689.13, average training loss: 20993.25, base loss: 23566.80
[INFO 2017-06-25 15:42:30,438 main.py:50] epoch 1008, training loss: 18582.98, average training loss: 20951.43, base loss: 23570.09
[INFO 2017-06-25 15:42:34,783 main.py:50] epoch 1009, training loss: 18761.55, average training loss: 20910.43, base loss: 23567.96
[INFO 2017-06-25 15:42:39,096 main.py:50] epoch 1010, training loss: 18486.54, average training loss: 20871.91, base loss: 23566.17
[INFO 2017-06-25 15:42:43,403 main.py:50] epoch 1011, training loss: 21033.38, average training loss: 20838.46, base loss: 23566.03
[INFO 2017-06-25 15:42:47,727 main.py:50] epoch 1012, training loss: 16884.82, average training loss: 20803.37, base loss: 23562.24
[INFO 2017-06-25 15:42:52,076 main.py:50] epoch 1013, training loss: 18845.46, average training loss: 20777.69, base loss: 23562.03
[INFO 2017-06-25 15:42:56,367 main.py:50] epoch 1014, training loss: 20163.14, average training loss: 20758.17, base loss: 23566.69
[INFO 2017-06-25 15:43:00,683 main.py:50] epoch 1015, training loss: 23221.42, average training loss: 20742.84, base loss: 23571.40
[INFO 2017-06-25 15:43:04,963 main.py:50] epoch 1016, training loss: 21100.48, average training loss: 20723.32, base loss: 23569.89
[INFO 2017-06-25 15:43:09,252 main.py:50] epoch 1017, training loss: 20892.56, average training loss: 20710.44, base loss: 23573.88
[INFO 2017-06-25 15:43:13,540 main.py:50] epoch 1018, training loss: 20939.58, average training loss: 20701.58, base loss: 23582.58
[INFO 2017-06-25 15:43:17,846 main.py:50] epoch 1019, training loss: 21734.53, average training loss: 20690.64, base loss: 23587.18
[INFO 2017-06-25 15:43:22,132 main.py:50] epoch 1020, training loss: 15518.41, average training loss: 20677.80, base loss: 23586.81
[INFO 2017-06-25 15:43:26,402 main.py:50] epoch 1021, training loss: 22093.29, average training loss: 20669.31, base loss: 23588.85
[INFO 2017-06-25 15:43:30,672 main.py:50] epoch 1022, training loss: 19376.23, average training loss: 20650.31, base loss: 23580.07
[INFO 2017-06-25 15:43:34,949 main.py:50] epoch 1023, training loss: 21396.23, average training loss: 20643.84, base loss: 23585.56
[INFO 2017-06-25 15:43:39,231 main.py:50] epoch 1024, training loss: 14252.10, average training loss: 20630.72, base loss: 23579.38
[INFO 2017-06-25 15:43:43,497 main.py:50] epoch 1025, training loss: 23584.79, average training loss: 20627.35, base loss: 23584.54
[INFO 2017-06-25 15:43:47,772 main.py:50] epoch 1026, training loss: 20203.96, average training loss: 20617.32, base loss: 23582.97
[INFO 2017-06-25 15:43:52,052 main.py:50] epoch 1027, training loss: 21078.82, average training loss: 20612.49, base loss: 23584.59
[INFO 2017-06-25 15:43:56,311 main.py:50] epoch 1028, training loss: 14954.42, average training loss: 20601.05, base loss: 23578.42
[INFO 2017-06-25 15:44:00,597 main.py:50] epoch 1029, training loss: 19793.36, average training loss: 20592.93, base loss: 23576.98
[INFO 2017-06-25 15:44:04,846 main.py:50] epoch 1030, training loss: 18556.28, average training loss: 20580.54, base loss: 23570.29
[INFO 2017-06-25 15:44:09,127 main.py:50] epoch 1031, training loss: 19245.53, average training loss: 20578.23, base loss: 23575.97
[INFO 2017-06-25 15:44:13,404 main.py:50] epoch 1032, training loss: 18421.50, average training loss: 20572.58, base loss: 23575.81
[INFO 2017-06-25 15:44:17,649 main.py:50] epoch 1033, training loss: 17240.99, average training loss: 20570.94, base loss: 23578.48
[INFO 2017-06-25 15:44:21,920 main.py:50] epoch 1034, training loss: 19816.84, average training loss: 20565.45, base loss: 23578.74
[INFO 2017-06-25 15:44:26,192 main.py:50] epoch 1035, training loss: 17049.14, average training loss: 20561.14, base loss: 23579.61
[INFO 2017-06-25 15:44:30,463 main.py:50] epoch 1036, training loss: 14218.87, average training loss: 20552.15, base loss: 23575.45
[INFO 2017-06-25 15:44:34,735 main.py:50] epoch 1037, training loss: 17371.78, average training loss: 20545.24, base loss: 23573.56
[INFO 2017-06-25 15:44:39,003 main.py:50] epoch 1038, training loss: 19345.17, average training loss: 20540.04, base loss: 23574.33
[INFO 2017-06-25 15:44:43,282 main.py:50] epoch 1039, training loss: 22936.97, average training loss: 20540.06, base loss: 23578.57
[INFO 2017-06-25 15:44:47,536 main.py:50] epoch 1040, training loss: 18006.67, average training loss: 20535.75, base loss: 23578.29
[INFO 2017-06-25 15:44:51,775 main.py:50] epoch 1041, training loss: 23087.36, average training loss: 20529.77, base loss: 23577.60
[INFO 2017-06-25 15:44:56,054 main.py:50] epoch 1042, training loss: 23398.00, average training loss: 20532.75, base loss: 23587.21
[INFO 2017-06-25 15:45:00,318 main.py:50] epoch 1043, training loss: 19416.48, average training loss: 20526.49, base loss: 23585.99
[INFO 2017-06-25 15:45:04,594 main.py:50] epoch 1044, training loss: 19731.54, average training loss: 20518.65, base loss: 23583.10
[INFO 2017-06-25 15:45:08,857 main.py:50] epoch 1045, training loss: 19156.56, average training loss: 20515.87, base loss: 23584.73
[INFO 2017-06-25 15:45:13,137 main.py:50] epoch 1046, training loss: 22095.78, average training loss: 20517.17, base loss: 23592.80
[INFO 2017-06-25 15:45:17,401 main.py:50] epoch 1047, training loss: 18729.86, average training loss: 20514.97, base loss: 23594.50
[INFO 2017-06-25 15:45:21,703 main.py:50] epoch 1048, training loss: 22111.77, average training loss: 20517.14, base loss: 23602.88
[INFO 2017-06-25 15:45:25,965 main.py:50] epoch 1049, training loss: 22066.07, average training loss: 20514.66, base loss: 23605.66
[INFO 2017-06-25 15:45:30,246 main.py:50] epoch 1050, training loss: 17901.35, average training loss: 20511.56, base loss: 23606.08
[INFO 2017-06-25 15:45:34,534 main.py:50] epoch 1051, training loss: 21722.29, average training loss: 20511.15, base loss: 23611.49
[INFO 2017-06-25 15:45:38,790 main.py:50] epoch 1052, training loss: 21043.39, average training loss: 20510.11, base loss: 23615.61
[INFO 2017-06-25 15:45:43,061 main.py:50] epoch 1053, training loss: 14933.37, average training loss: 20501.46, base loss: 23611.63
[INFO 2017-06-25 15:45:47,340 main.py:50] epoch 1054, training loss: 16318.25, average training loss: 20495.60, base loss: 23608.70
[INFO 2017-06-25 15:45:51,597 main.py:50] epoch 1055, training loss: 16275.35, average training loss: 20487.87, base loss: 23603.93
[INFO 2017-06-25 15:45:55,880 main.py:50] epoch 1056, training loss: 16529.66, average training loss: 20480.59, base loss: 23601.83
[INFO 2017-06-25 15:46:00,170 main.py:50] epoch 1057, training loss: 18960.85, average training loss: 20477.58, base loss: 23604.56
[INFO 2017-06-25 15:46:04,444 main.py:50] epoch 1058, training loss: 21494.83, average training loss: 20480.47, base loss: 23613.71
[INFO 2017-06-25 15:46:08,730 main.py:50] epoch 1059, training loss: 17497.80, average training loss: 20470.20, base loss: 23607.76
[INFO 2017-06-25 15:46:13,009 main.py:50] epoch 1060, training loss: 19895.08, average training loss: 20472.38, base loss: 23615.90
[INFO 2017-06-25 15:46:17,275 main.py:50] epoch 1061, training loss: 18357.73, average training loss: 20470.00, base loss: 23616.54
[INFO 2017-06-25 15:46:21,555 main.py:50] epoch 1062, training loss: 17502.39, average training loss: 20463.12, base loss: 23613.04
[INFO 2017-06-25 15:46:25,836 main.py:50] epoch 1063, training loss: 15815.93, average training loss: 20457.40, base loss: 23610.34
[INFO 2017-06-25 15:46:30,087 main.py:50] epoch 1064, training loss: 22271.82, average training loss: 20455.38, base loss: 23612.48
[INFO 2017-06-25 15:46:34,367 main.py:50] epoch 1065, training loss: 23026.76, average training loss: 20455.03, base loss: 23619.56
[INFO 2017-06-25 15:46:38,624 main.py:50] epoch 1066, training loss: 15614.53, average training loss: 20447.32, base loss: 23616.60
[INFO 2017-06-25 15:46:42,907 main.py:50] epoch 1067, training loss: 17794.10, average training loss: 20441.30, base loss: 23616.23
[INFO 2017-06-25 15:46:47,203 main.py:50] epoch 1068, training loss: 21349.54, average training loss: 20434.36, base loss: 23612.75
[INFO 2017-06-25 15:46:51,466 main.py:50] epoch 1069, training loss: 18300.12, average training loss: 20429.85, base loss: 23611.64
[INFO 2017-06-25 15:46:55,727 main.py:50] epoch 1070, training loss: 19222.93, average training loss: 20426.27, base loss: 23612.46
[INFO 2017-06-25 15:46:59,968 main.py:50] epoch 1071, training loss: 16879.32, average training loss: 20421.95, base loss: 23611.70
[INFO 2017-06-25 15:47:04,246 main.py:50] epoch 1072, training loss: 17907.16, average training loss: 20414.52, base loss: 23608.38
[INFO 2017-06-25 15:47:08,525 main.py:50] epoch 1073, training loss: 21178.66, average training loss: 20410.54, base loss: 23610.36
[INFO 2017-06-25 15:47:12,840 main.py:50] epoch 1074, training loss: 18914.04, average training loss: 20412.76, base loss: 23617.18
[INFO 2017-06-25 15:47:17,126 main.py:50] epoch 1075, training loss: 21590.35, average training loss: 20409.09, base loss: 23619.02
[INFO 2017-06-25 15:47:21,402 main.py:50] epoch 1076, training loss: 17877.71, average training loss: 20402.30, base loss: 23616.67
[INFO 2017-06-25 15:47:25,666 main.py:50] epoch 1077, training loss: 17682.95, average training loss: 20398.53, base loss: 23618.58
[INFO 2017-06-25 15:47:29,952 main.py:50] epoch 1078, training loss: 22216.65, average training loss: 20394.83, base loss: 23618.83
[INFO 2017-06-25 15:47:34,229 main.py:50] epoch 1079, training loss: 20216.34, average training loss: 20389.49, base loss: 23620.60
[INFO 2017-06-25 15:47:38,494 main.py:50] epoch 1080, training loss: 18372.50, average training loss: 20384.42, base loss: 23619.15
[INFO 2017-06-25 15:47:42,784 main.py:50] epoch 1081, training loss: 15631.52, average training loss: 20378.37, base loss: 23618.00
[INFO 2017-06-25 15:47:47,088 main.py:50] epoch 1082, training loss: 12558.69, average training loss: 20369.17, base loss: 23609.58
[INFO 2017-06-25 15:47:51,375 main.py:50] epoch 1083, training loss: 24819.55, average training loss: 20376.46, base loss: 23620.51
[INFO 2017-06-25 15:47:55,659 main.py:50] epoch 1084, training loss: 17395.38, average training loss: 20372.20, base loss: 23618.71
[INFO 2017-06-25 15:47:59,934 main.py:50] epoch 1085, training loss: 19048.03, average training loss: 20370.96, base loss: 23621.29
[INFO 2017-06-25 15:48:04,208 main.py:50] epoch 1086, training loss: 21856.71, average training loss: 20370.08, base loss: 23626.95
[INFO 2017-06-25 15:48:08,485 main.py:50] epoch 1087, training loss: 20877.96, average training loss: 20367.76, base loss: 23627.45
[INFO 2017-06-25 15:48:12,754 main.py:50] epoch 1088, training loss: 23175.98, average training loss: 20364.79, base loss: 23629.00
[INFO 2017-06-25 15:48:17,043 main.py:50] epoch 1089, training loss: 25692.34, average training loss: 20370.67, base loss: 23641.30
[INFO 2017-06-25 15:48:21,349 main.py:50] epoch 1090, training loss: 23078.13, average training loss: 20371.71, base loss: 23646.79
[INFO 2017-06-25 15:48:25,610 main.py:50] epoch 1091, training loss: 18355.23, average training loss: 20361.76, base loss: 23640.75
[INFO 2017-06-25 15:48:29,876 main.py:50] epoch 1092, training loss: 15964.79, average training loss: 20354.82, base loss: 23636.23
[INFO 2017-06-25 15:48:34,133 main.py:50] epoch 1093, training loss: 19019.51, average training loss: 20355.26, base loss: 23642.37
[INFO 2017-06-25 15:48:38,408 main.py:50] epoch 1094, training loss: 21520.61, average training loss: 20357.40, base loss: 23649.09
[INFO 2017-06-25 15:48:42,681 main.py:50] epoch 1095, training loss: 21134.04, average training loss: 20353.55, base loss: 23648.50
[INFO 2017-06-25 15:48:46,975 main.py:50] epoch 1096, training loss: 17039.35, average training loss: 20343.00, base loss: 23641.02
[INFO 2017-06-25 15:48:51,233 main.py:50] epoch 1097, training loss: 18485.12, average training loss: 20340.00, base loss: 23641.58
[INFO 2017-06-25 15:48:55,494 main.py:50] epoch 1098, training loss: 20815.84, average training loss: 20343.53, base loss: 23648.57
[INFO 2017-06-25 15:48:59,804 main.py:50] epoch 1099, training loss: 21241.33, average training loss: 20340.23, base loss: 23649.11
[INFO 2017-06-25 15:49:04,064 main.py:50] epoch 1100, training loss: 16524.92, average training loss: 20332.29, base loss: 23643.68
[INFO 2017-06-25 15:49:08,327 main.py:50] epoch 1101, training loss: 17092.06, average training loss: 20329.24, base loss: 23644.90
[INFO 2017-06-25 15:49:12,607 main.py:50] epoch 1102, training loss: 19521.61, average training loss: 20326.63, base loss: 23645.52
[INFO 2017-06-25 15:49:16,892 main.py:50] epoch 1103, training loss: 17667.31, average training loss: 20322.43, base loss: 23643.39
[INFO 2017-06-25 15:49:21,211 main.py:50] epoch 1104, training loss: 18391.33, average training loss: 20317.46, base loss: 23644.08
[INFO 2017-06-25 15:49:25,463 main.py:50] epoch 1105, training loss: 14681.36, average training loss: 20309.11, base loss: 23638.48
[INFO 2017-06-25 15:49:29,733 main.py:50] epoch 1106, training loss: 18510.54, average training loss: 20301.14, base loss: 23633.84
[INFO 2017-06-25 15:49:34,007 main.py:50] epoch 1107, training loss: 18638.76, average training loss: 20294.03, base loss: 23630.68
[INFO 2017-06-25 15:49:38,298 main.py:50] epoch 1108, training loss: 21631.54, average training loss: 20289.13, base loss: 23630.45
[INFO 2017-06-25 15:49:42,587 main.py:50] epoch 1109, training loss: 15645.73, average training loss: 20281.91, base loss: 23626.04
[INFO 2017-06-25 15:49:46,871 main.py:50] epoch 1110, training loss: 23009.45, average training loss: 20283.51, base loss: 23632.08
[INFO 2017-06-25 15:49:51,145 main.py:50] epoch 1111, training loss: 17377.17, average training loss: 20280.39, base loss: 23631.32
[INFO 2017-06-25 15:49:55,414 main.py:50] epoch 1112, training loss: 14954.67, average training loss: 20275.69, base loss: 23629.09
[INFO 2017-06-25 15:49:59,701 main.py:50] epoch 1113, training loss: 19252.57, average training loss: 20269.36, base loss: 23627.78
[INFO 2017-06-25 15:50:03,969 main.py:50] epoch 1114, training loss: 15519.25, average training loss: 20260.64, base loss: 23621.97
[INFO 2017-06-25 15:50:08,252 main.py:50] epoch 1115, training loss: 17715.58, average training loss: 20258.47, base loss: 23624.51
[INFO 2017-06-25 15:50:12,540 main.py:50] epoch 1116, training loss: 18941.22, average training loss: 20255.32, base loss: 23625.01
[INFO 2017-06-25 15:50:16,802 main.py:50] epoch 1117, training loss: 18503.49, average training loss: 20252.80, base loss: 23626.35
[INFO 2017-06-25 15:50:21,103 main.py:50] epoch 1118, training loss: 25104.10, average training loss: 20259.16, base loss: 23638.54
[INFO 2017-06-25 15:50:25,404 main.py:50] epoch 1119, training loss: 17006.35, average training loss: 20252.87, base loss: 23635.48
[INFO 2017-06-25 15:50:29,673 main.py:50] epoch 1120, training loss: 16778.00, average training loss: 20246.53, base loss: 23631.17
[INFO 2017-06-25 15:50:33,943 main.py:50] epoch 1121, training loss: 18094.14, average training loss: 20247.91, base loss: 23634.76
[INFO 2017-06-25 15:50:38,195 main.py:50] epoch 1122, training loss: 18228.82, average training loss: 20245.85, base loss: 23637.15
[INFO 2017-06-25 15:50:42,458 main.py:50] epoch 1123, training loss: 19476.95, average training loss: 20243.71, base loss: 23638.10
[INFO 2017-06-25 15:50:46,715 main.py:50] epoch 1124, training loss: 18789.12, average training loss: 20236.15, base loss: 23633.20
[INFO 2017-06-25 15:50:50,980 main.py:50] epoch 1125, training loss: 21741.34, average training loss: 20236.61, base loss: 23637.36
[INFO 2017-06-25 15:50:55,255 main.py:50] epoch 1126, training loss: 22586.68, average training loss: 20235.34, base loss: 23640.67
[INFO 2017-06-25 15:50:59,522 main.py:50] epoch 1127, training loss: 16213.08, average training loss: 20223.83, base loss: 23632.89
[INFO 2017-06-25 15:51:03,795 main.py:50] epoch 1128, training loss: 17747.68, average training loss: 20217.24, base loss: 23628.54
[INFO 2017-06-25 15:51:08,069 main.py:50] epoch 1129, training loss: 18238.74, average training loss: 20210.34, base loss: 23626.26
[INFO 2017-06-25 15:51:12,345 main.py:50] epoch 1130, training loss: 19869.89, average training loss: 20206.79, base loss: 23628.31
[INFO 2017-06-25 15:51:16,619 main.py:50] epoch 1131, training loss: 15821.92, average training loss: 20195.36, base loss: 23616.79
[INFO 2017-06-25 15:51:20,888 main.py:50] epoch 1132, training loss: 21933.46, average training loss: 20195.79, base loss: 23621.76
[INFO 2017-06-25 15:51:25,170 main.py:50] epoch 1133, training loss: 19581.93, average training loss: 20180.35, base loss: 23610.78
[INFO 2017-06-25 15:51:29,429 main.py:50] epoch 1134, training loss: 16749.96, average training loss: 20172.21, base loss: 23605.65
[INFO 2017-06-25 15:51:33,714 main.py:50] epoch 1135, training loss: 20497.63, average training loss: 20168.53, base loss: 23607.25
[INFO 2017-06-25 15:51:38,010 main.py:50] epoch 1136, training loss: 16274.29, average training loss: 20163.11, base loss: 23604.30
[INFO 2017-06-25 15:51:42,301 main.py:50] epoch 1137, training loss: 15739.99, average training loss: 20158.02, base loss: 23601.26
[INFO 2017-06-25 15:51:46,566 main.py:50] epoch 1138, training loss: 15475.49, average training loss: 20149.24, base loss: 23595.45
[INFO 2017-06-25 15:51:50,853 main.py:50] epoch 1139, training loss: 18152.01, average training loss: 20143.66, base loss: 23592.24
[INFO 2017-06-25 15:51:55,134 main.py:50] epoch 1140, training loss: 29183.82, average training loss: 20154.93, base loss: 23609.74
[INFO 2017-06-25 15:51:59,422 main.py:50] epoch 1141, training loss: 17944.87, average training loss: 20154.23, base loss: 23616.24
[INFO 2017-06-25 15:52:03,698 main.py:50] epoch 1142, training loss: 19590.05, average training loss: 20156.21, base loss: 23622.51
[INFO 2017-06-25 15:52:07,958 main.py:50] epoch 1143, training loss: 24215.70, average training loss: 20159.56, base loss: 23630.09
[INFO 2017-06-25 15:52:12,216 main.py:50] epoch 1144, training loss: 17206.80, average training loss: 20155.69, base loss: 23630.16
[INFO 2017-06-25 15:52:16,493 main.py:50] epoch 1145, training loss: 17276.77, average training loss: 20149.13, base loss: 23626.22
[INFO 2017-06-25 15:52:20,755 main.py:50] epoch 1146, training loss: 15230.81, average training loss: 20140.01, base loss: 23619.21
[INFO 2017-06-25 15:52:25,034 main.py:50] epoch 1147, training loss: 16678.56, average training loss: 20135.64, base loss: 23617.68
[INFO 2017-06-25 15:52:29,297 main.py:50] epoch 1148, training loss: 16861.63, average training loss: 20124.88, base loss: 23610.83
[INFO 2017-06-25 15:52:33,568 main.py:50] epoch 1149, training loss: 18836.44, average training loss: 20121.19, base loss: 23611.41
[INFO 2017-06-25 15:52:37,831 main.py:50] epoch 1150, training loss: 15801.27, average training loss: 20118.18, base loss: 23610.46
[INFO 2017-06-25 15:52:42,113 main.py:50] epoch 1151, training loss: 19283.93, average training loss: 20114.73, base loss: 23608.80
[INFO 2017-06-25 15:52:46,374 main.py:50] epoch 1152, training loss: 15458.49, average training loss: 20109.12, base loss: 23606.39
[INFO 2017-06-25 15:52:50,651 main.py:50] epoch 1153, training loss: 15581.81, average training loss: 20100.67, base loss: 23599.28
[INFO 2017-06-25 15:52:54,923 main.py:50] epoch 1154, training loss: 18385.93, average training loss: 20102.92, base loss: 23606.73
[INFO 2017-06-25 15:52:59,183 main.py:50] epoch 1155, training loss: 20020.85, average training loss: 20100.50, base loss: 23608.25
[INFO 2017-06-25 15:53:03,451 main.py:50] epoch 1156, training loss: 16348.26, average training loss: 20095.30, base loss: 23605.62
[INFO 2017-06-25 15:53:07,747 main.py:50] epoch 1157, training loss: 17503.21, average training loss: 20091.60, base loss: 23605.46
[INFO 2017-06-25 15:53:11,998 main.py:50] epoch 1158, training loss: 14696.38, average training loss: 20087.71, base loss: 23603.88
[INFO 2017-06-25 15:53:16,251 main.py:50] epoch 1159, training loss: 19525.44, average training loss: 20088.67, base loss: 23608.26
[INFO 2017-06-25 15:53:20,517 main.py:50] epoch 1160, training loss: 21588.15, average training loss: 20090.87, base loss: 23614.34
[INFO 2017-06-25 15:53:24,787 main.py:50] epoch 1161, training loss: 20852.21, average training loss: 20089.60, base loss: 23617.10
[INFO 2017-06-25 15:53:29,064 main.py:50] epoch 1162, training loss: 17733.66, average training loss: 20075.76, base loss: 23604.59
[INFO 2017-06-25 15:53:33,333 main.py:50] epoch 1163, training loss: 21365.13, average training loss: 20075.53, base loss: 23607.22
[INFO 2017-06-25 15:53:37,637 main.py:50] epoch 1164, training loss: 19364.62, average training loss: 20074.97, base loss: 23611.82
[INFO 2017-06-25 15:53:41,907 main.py:50] epoch 1165, training loss: 15616.62, average training loss: 20069.67, base loss: 23608.50
[INFO 2017-06-25 15:53:46,190 main.py:50] epoch 1166, training loss: 18112.28, average training loss: 20068.64, base loss: 23610.07
[INFO 2017-06-25 15:53:50,480 main.py:50] epoch 1167, training loss: 16563.71, average training loss: 20063.78, base loss: 23609.64
[INFO 2017-06-25 15:53:54,763 main.py:50] epoch 1168, training loss: 18626.42, average training loss: 20063.34, base loss: 23612.90
[INFO 2017-06-25 15:53:59,017 main.py:50] epoch 1169, training loss: 15834.69, average training loss: 20054.87, base loss: 23605.51
[INFO 2017-06-25 15:54:03,296 main.py:50] epoch 1170, training loss: 18359.88, average training loss: 20048.32, base loss: 23602.43
[INFO 2017-06-25 15:54:07,553 main.py:50] epoch 1171, training loss: 17695.01, average training loss: 20044.86, base loss: 23603.14
[INFO 2017-06-25 15:54:11,825 main.py:50] epoch 1172, training loss: 16640.07, average training loss: 20039.56, base loss: 23600.27
[INFO 2017-06-25 15:54:16,085 main.py:50] epoch 1173, training loss: 13202.04, average training loss: 20029.91, base loss: 23592.42
[INFO 2017-06-25 15:54:20,373 main.py:50] epoch 1174, training loss: 23331.01, average training loss: 20035.57, base loss: 23602.56
[INFO 2017-06-25 15:54:24,639 main.py:50] epoch 1175, training loss: 16150.41, average training loss: 20022.49, base loss: 23589.45
[INFO 2017-06-25 15:54:28,922 main.py:50] epoch 1176, training loss: 20228.76, average training loss: 20015.21, base loss: 23584.07
[INFO 2017-06-25 15:54:33,204 main.py:50] epoch 1177, training loss: 16759.71, average training loss: 20014.06, base loss: 23587.62
[INFO 2017-06-25 15:54:37,475 main.py:50] epoch 1178, training loss: 19068.10, average training loss: 20016.62, base loss: 23592.08
[INFO 2017-06-25 15:54:41,756 main.py:50] epoch 1179, training loss: 16878.67, average training loss: 20008.69, base loss: 23587.55
[INFO 2017-06-25 15:54:46,022 main.py:50] epoch 1180, training loss: 16046.16, average training loss: 19998.76, base loss: 23578.78
[INFO 2017-06-25 15:54:50,301 main.py:50] epoch 1181, training loss: 20319.06, average training loss: 19996.65, base loss: 23578.38
[INFO 2017-06-25 15:54:54,572 main.py:50] epoch 1182, training loss: 14788.79, average training loss: 19991.25, base loss: 23574.45
[INFO 2017-06-25 15:54:58,850 main.py:50] epoch 1183, training loss: 20244.98, average training loss: 19997.32, base loss: 23585.26
[INFO 2017-06-25 15:55:03,128 main.py:50] epoch 1184, training loss: 17019.13, average training loss: 19993.42, base loss: 23585.44
[INFO 2017-06-25 15:55:07,410 main.py:50] epoch 1185, training loss: 17358.64, average training loss: 19989.35, base loss: 23583.32
[INFO 2017-06-25 15:55:11,680 main.py:50] epoch 1186, training loss: 18928.37, average training loss: 19985.29, base loss: 23582.48
[INFO 2017-06-25 15:55:15,971 main.py:50] epoch 1187, training loss: 16667.86, average training loss: 19981.32, base loss: 23580.49
[INFO 2017-06-25 15:55:20,256 main.py:50] epoch 1188, training loss: 21926.44, average training loss: 19978.97, base loss: 23581.59
[INFO 2017-06-25 15:55:24,540 main.py:50] epoch 1189, training loss: 14778.94, average training loss: 19972.50, base loss: 23578.78
[INFO 2017-06-25 15:55:28,838 main.py:50] epoch 1190, training loss: 19577.02, average training loss: 19970.76, base loss: 23579.52
[INFO 2017-06-25 15:55:33,113 main.py:50] epoch 1191, training loss: 14033.92, average training loss: 19962.77, base loss: 23574.28
[INFO 2017-06-25 15:55:37,367 main.py:50] epoch 1192, training loss: 20707.54, average training loss: 19956.63, base loss: 23572.44
[INFO 2017-06-25 15:55:41,673 main.py:50] epoch 1193, training loss: 18404.39, average training loss: 19950.88, base loss: 23569.20
[INFO 2017-06-25 15:55:45,922 main.py:50] epoch 1194, training loss: 20338.45, average training loss: 19949.19, base loss: 23571.97
[INFO 2017-06-25 15:55:50,193 main.py:50] epoch 1195, training loss: 17255.90, average training loss: 19946.73, base loss: 23572.01
[INFO 2017-06-25 15:55:54,501 main.py:50] epoch 1196, training loss: 18983.64, average training loss: 19942.04, base loss: 23570.58
[INFO 2017-06-25 15:55:58,796 main.py:50] epoch 1197, training loss: 19159.23, average training loss: 19939.25, base loss: 23570.93
[INFO 2017-06-25 15:56:03,095 main.py:50] epoch 1198, training loss: 16726.29, average training loss: 19935.64, base loss: 23572.32
[INFO 2017-06-25 15:56:07,377 main.py:50] epoch 1199, training loss: 22133.34, average training loss: 19934.97, base loss: 23574.79
[INFO 2017-06-25 15:56:11,695 main.py:50] epoch 1200, training loss: 20506.70, average training loss: 19936.44, base loss: 23579.36
[INFO 2017-06-25 15:56:15,996 main.py:50] epoch 1201, training loss: 17735.00, average training loss: 19927.41, base loss: 23574.00
[INFO 2017-06-25 15:56:20,288 main.py:50] epoch 1202, training loss: 17861.11, average training loss: 19926.38, base loss: 23576.44
[INFO 2017-06-25 15:56:24,578 main.py:50] epoch 1203, training loss: 17559.93, average training loss: 19921.35, base loss: 23576.00
[INFO 2017-06-25 15:56:28,881 main.py:50] epoch 1204, training loss: 24999.99, average training loss: 19925.70, base loss: 23584.06
[INFO 2017-06-25 15:56:33,195 main.py:50] epoch 1205, training loss: 17646.30, average training loss: 19920.43, base loss: 23582.27
[INFO 2017-06-25 15:56:37,489 main.py:50] epoch 1206, training loss: 18567.87, average training loss: 19915.36, base loss: 23579.85
[INFO 2017-06-25 15:56:41,768 main.py:50] epoch 1207, training loss: 18676.75, average training loss: 19912.97, base loss: 23582.23
[INFO 2017-06-25 15:56:46,037 main.py:50] epoch 1208, training loss: 18784.92, average training loss: 19912.54, base loss: 23585.49
[INFO 2017-06-25 15:56:50,349 main.py:50] epoch 1209, training loss: 21663.50, average training loss: 19910.82, base loss: 23587.32
[INFO 2017-06-25 15:56:54,642 main.py:50] epoch 1210, training loss: 26684.24, average training loss: 19913.52, base loss: 23594.88
[INFO 2017-06-25 15:56:58,933 main.py:50] epoch 1211, training loss: 21534.28, average training loss: 19911.94, base loss: 23597.27
[INFO 2017-06-25 15:57:03,237 main.py:50] epoch 1212, training loss: 21006.91, average training loss: 19913.92, base loss: 23604.51
[INFO 2017-06-25 15:57:07,504 main.py:50] epoch 1213, training loss: 14777.17, average training loss: 19909.00, base loss: 23599.80
[INFO 2017-06-25 15:57:11,773 main.py:50] epoch 1214, training loss: 17467.72, average training loss: 19895.36, base loss: 23589.36
[INFO 2017-06-25 15:57:16,060 main.py:50] epoch 1215, training loss: 17761.04, average training loss: 19893.26, base loss: 23590.58
[INFO 2017-06-25 15:57:20,368 main.py:50] epoch 1216, training loss: 18083.15, average training loss: 19894.85, base loss: 23596.07
[INFO 2017-06-25 15:57:24,683 main.py:50] epoch 1217, training loss: 20633.46, average training loss: 19895.10, base loss: 23600.58
[INFO 2017-06-25 15:57:28,960 main.py:50] epoch 1218, training loss: 16308.70, average training loss: 19890.11, base loss: 23597.43
[INFO 2017-06-25 15:57:33,243 main.py:50] epoch 1219, training loss: 17136.68, average training loss: 19884.97, base loss: 23595.88
[INFO 2017-06-25 15:57:37,534 main.py:50] epoch 1220, training loss: 17299.90, average training loss: 19882.74, base loss: 23596.60
[INFO 2017-06-25 15:57:41,823 main.py:50] epoch 1221, training loss: 15823.16, average training loss: 19880.08, base loss: 23596.03
[INFO 2017-06-25 15:57:46,106 main.py:50] epoch 1222, training loss: 15000.23, average training loss: 19878.10, base loss: 23596.64
[INFO 2017-06-25 15:57:50,382 main.py:50] epoch 1223, training loss: 13297.37, average training loss: 19872.82, base loss: 23593.94
[INFO 2017-06-25 15:57:54,663 main.py:50] epoch 1224, training loss: 18229.27, average training loss: 19871.97, base loss: 23596.29
[INFO 2017-06-25 15:57:58,952 main.py:50] epoch 1225, training loss: 19107.68, average training loss: 19870.30, base loss: 23599.11
[INFO 2017-06-25 15:58:03,256 main.py:50] epoch 1226, training loss: 16772.32, average training loss: 19866.28, base loss: 23596.67
[INFO 2017-06-25 15:58:07,527 main.py:50] epoch 1227, training loss: 15989.46, average training loss: 19863.60, base loss: 23594.99
[INFO 2017-06-25 15:58:11,800 main.py:50] epoch 1228, training loss: 16999.05, average training loss: 19856.72, base loss: 23591.32
[INFO 2017-06-25 15:58:16,098 main.py:50] epoch 1229, training loss: 21870.73, average training loss: 19852.74, base loss: 23589.63
[INFO 2017-06-25 15:58:20,356 main.py:50] epoch 1230, training loss: 20407.79, average training loss: 19847.38, base loss: 23587.15
[INFO 2017-06-25 15:58:24,616 main.py:50] epoch 1231, training loss: 18107.76, average training loss: 19844.22, base loss: 23586.48
[INFO 2017-06-25 15:58:28,900 main.py:50] epoch 1232, training loss: 21948.32, average training loss: 19841.19, base loss: 23586.42
[INFO 2017-06-25 15:58:33,157 main.py:50] epoch 1233, training loss: 20781.30, average training loss: 19842.99, base loss: 23591.02
[INFO 2017-06-25 15:58:37,437 main.py:50] epoch 1234, training loss: 17190.06, average training loss: 19835.00, base loss: 23585.49
[INFO 2017-06-25 15:58:41,707 main.py:50] epoch 1235, training loss: 21684.55, average training loss: 19833.23, base loss: 23585.06
[INFO 2017-06-25 15:58:45,989 main.py:50] epoch 1236, training loss: 14030.39, average training loss: 19817.02, base loss: 23569.16
[INFO 2017-06-25 15:58:50,259 main.py:50] epoch 1237, training loss: 14714.05, average training loss: 19806.50, base loss: 23560.08
[INFO 2017-06-25 15:58:54,518 main.py:50] epoch 1238, training loss: 20612.31, average training loss: 19804.18, base loss: 23561.37
[INFO 2017-06-25 15:58:58,831 main.py:50] epoch 1239, training loss: 22177.74, average training loss: 19803.24, base loss: 23560.92
[INFO 2017-06-25 15:59:03,137 main.py:50] epoch 1240, training loss: 19726.23, average training loss: 19801.84, base loss: 23564.26
[INFO 2017-06-25 15:59:07,400 main.py:50] epoch 1241, training loss: 15982.12, average training loss: 19799.41, base loss: 23566.16
[INFO 2017-06-25 15:59:11,681 main.py:50] epoch 1242, training loss: 23426.05, average training loss: 19797.94, base loss: 23569.36
[INFO 2017-06-25 15:59:15,953 main.py:50] epoch 1243, training loss: 18980.62, average training loss: 19801.49, base loss: 23576.02
[INFO 2017-06-25 15:59:20,253 main.py:50] epoch 1244, training loss: 17755.81, average training loss: 19796.85, base loss: 23572.02
[INFO 2017-06-25 15:59:24,549 main.py:50] epoch 1245, training loss: 20243.25, average training loss: 19795.70, base loss: 23572.66
[INFO 2017-06-25 15:59:28,836 main.py:50] epoch 1246, training loss: 17006.26, average training loss: 19794.60, base loss: 23575.85
[INFO 2017-06-25 15:59:33,087 main.py:50] epoch 1247, training loss: 19704.57, average training loss: 19794.93, base loss: 23580.41
[INFO 2017-06-25 15:59:37,361 main.py:50] epoch 1248, training loss: 23372.22, average training loss: 19799.47, base loss: 23588.12
[INFO 2017-06-25 15:59:41,639 main.py:50] epoch 1249, training loss: 20980.64, average training loss: 19795.55, base loss: 23588.82
[INFO 2017-06-25 15:59:45,922 main.py:50] epoch 1250, training loss: 18273.40, average training loss: 19788.43, base loss: 23584.90
[INFO 2017-06-25 15:59:50,181 main.py:50] epoch 1251, training loss: 19680.79, average training loss: 19781.86, base loss: 23581.72
[INFO 2017-06-25 15:59:54,454 main.py:50] epoch 1252, training loss: 17884.84, average training loss: 19779.65, base loss: 23584.05
[INFO 2017-06-25 15:59:58,744 main.py:50] epoch 1253, training loss: 19132.00, average training loss: 19782.89, base loss: 23592.38
[INFO 2017-06-25 16:00:03,052 main.py:50] epoch 1254, training loss: 20904.29, average training loss: 19782.32, base loss: 23592.78
[INFO 2017-06-25 16:00:07,328 main.py:50] epoch 1255, training loss: 19718.20, average training loss: 19779.89, base loss: 23594.31
[INFO 2017-06-25 16:00:11,604 main.py:50] epoch 1256, training loss: 18952.22, average training loss: 19778.29, base loss: 23595.26
[INFO 2017-06-25 16:00:15,887 main.py:50] epoch 1257, training loss: 17972.07, average training loss: 19771.34, base loss: 23588.82
[INFO 2017-06-25 16:00:20,181 main.py:50] epoch 1258, training loss: 19234.43, average training loss: 19774.77, base loss: 23597.42
[INFO 2017-06-25 16:00:24,452 main.py:50] epoch 1259, training loss: 17606.67, average training loss: 19768.54, base loss: 23593.43
[INFO 2017-06-25 16:00:28,723 main.py:50] epoch 1260, training loss: 16299.40, average training loss: 19764.49, base loss: 23591.51
[INFO 2017-06-25 16:00:32,986 main.py:50] epoch 1261, training loss: 18358.47, average training loss: 19762.38, base loss: 23590.71
[INFO 2017-06-25 16:00:37,276 main.py:50] epoch 1262, training loss: 23428.38, average training loss: 19759.68, base loss: 23592.80
[INFO 2017-06-25 16:00:41,537 main.py:50] epoch 1263, training loss: 24880.91, average training loss: 19759.67, base loss: 23594.30
[INFO 2017-06-25 16:00:45,825 main.py:50] epoch 1264, training loss: 24104.47, average training loss: 19760.99, base loss: 23598.77
[INFO 2017-06-25 16:00:50,095 main.py:50] epoch 1265, training loss: 24615.96, average training loss: 19765.53, base loss: 23606.49
[INFO 2017-06-25 16:00:54,378 main.py:50] epoch 1266, training loss: 17056.69, average training loss: 19760.21, base loss: 23602.20
[INFO 2017-06-25 16:00:58,655 main.py:50] epoch 1267, training loss: 21808.62, average training loss: 19756.51, base loss: 23600.07
[INFO 2017-06-25 16:01:02,932 main.py:50] epoch 1268, training loss: 19314.17, average training loss: 19756.57, base loss: 23603.89
[INFO 2017-06-25 16:01:07,227 main.py:50] epoch 1269, training loss: 24172.66, average training loss: 19763.83, base loss: 23614.71
[INFO 2017-06-25 16:01:11,492 main.py:50] epoch 1270, training loss: 20147.06, average training loss: 19765.53, base loss: 23620.01
[INFO 2017-06-25 16:01:15,779 main.py:50] epoch 1271, training loss: 18191.31, average training loss: 19757.26, base loss: 23614.30
[INFO 2017-06-25 16:01:20,049 main.py:50] epoch 1272, training loss: 20261.07, average training loss: 19756.79, base loss: 23616.58
[INFO 2017-06-25 16:01:24,328 main.py:50] epoch 1273, training loss: 21560.42, average training loss: 19750.11, base loss: 23614.75
[INFO 2017-06-25 16:01:28,599 main.py:50] epoch 1274, training loss: 24181.93, average training loss: 19749.48, base loss: 23617.41
[INFO 2017-06-25 16:01:32,874 main.py:50] epoch 1275, training loss: 19809.95, average training loss: 19742.87, base loss: 23613.39
[INFO 2017-06-25 16:01:37,157 main.py:50] epoch 1276, training loss: 17177.89, average training loss: 19742.03, base loss: 23616.66
[INFO 2017-06-25 16:01:41,404 main.py:50] epoch 1277, training loss: 22303.96, average training loss: 19739.88, base loss: 23616.92
[INFO 2017-06-25 16:01:45,691 main.py:50] epoch 1278, training loss: 19187.14, average training loss: 19733.27, base loss: 23610.56
[INFO 2017-06-25 16:01:49,982 main.py:50] epoch 1279, training loss: 21044.33, average training loss: 19724.68, base loss: 23604.96
[INFO 2017-06-25 16:01:54,257 main.py:50] epoch 1280, training loss: 17443.19, average training loss: 19719.11, base loss: 23603.24
[INFO 2017-06-25 16:01:58,505 main.py:50] epoch 1281, training loss: 22456.73, average training loss: 19713.17, base loss: 23600.12
[INFO 2017-06-25 16:02:02,767 main.py:50] epoch 1282, training loss: 19293.51, average training loss: 19712.86, base loss: 23600.76
[INFO 2017-06-25 16:02:07,022 main.py:50] epoch 1283, training loss: 13846.08, average training loss: 19706.72, base loss: 23594.58
[INFO 2017-06-25 16:02:11,314 main.py:50] epoch 1284, training loss: 20538.44, average training loss: 19703.91, base loss: 23593.93
[INFO 2017-06-25 16:02:15,605 main.py:50] epoch 1285, training loss: 22000.63, average training loss: 19702.05, base loss: 23595.29
[INFO 2017-06-25 16:02:19,871 main.py:50] epoch 1286, training loss: 17555.42, average training loss: 19697.80, base loss: 23593.72
[INFO 2017-06-25 16:02:24,147 main.py:50] epoch 1287, training loss: 21324.02, average training loss: 19698.67, base loss: 23600.03
[INFO 2017-06-25 16:02:28,414 main.py:50] epoch 1288, training loss: 19896.98, average training loss: 19701.31, base loss: 23606.48
[INFO 2017-06-25 16:02:32,677 main.py:50] epoch 1289, training loss: 18379.44, average training loss: 19698.27, base loss: 23606.70
[INFO 2017-06-25 16:02:36,975 main.py:50] epoch 1290, training loss: 22394.67, average training loss: 19699.31, base loss: 23611.89
[INFO 2017-06-25 16:02:41,283 main.py:50] epoch 1291, training loss: 22488.74, average training loss: 19702.91, base loss: 23618.24
[INFO 2017-06-25 16:02:45,549 main.py:50] epoch 1292, training loss: 18269.82, average training loss: 19697.71, base loss: 23616.19
[INFO 2017-06-25 16:02:49,793 main.py:50] epoch 1293, training loss: 19497.61, average training loss: 19698.50, base loss: 23618.62
[INFO 2017-06-25 16:02:54,073 main.py:50] epoch 1294, training loss: 18226.83, average training loss: 19695.65, base loss: 23618.30
[INFO 2017-06-25 16:02:58,355 main.py:50] epoch 1295, training loss: 18626.87, average training loss: 19690.14, base loss: 23614.54
[INFO 2017-06-25 16:03:02,639 main.py:50] epoch 1296, training loss: 26242.56, average training loss: 19691.95, base loss: 23621.13
[INFO 2017-06-25 16:03:06,907 main.py:50] epoch 1297, training loss: 16693.61, average training loss: 19688.47, base loss: 23621.15
[INFO 2017-06-25 16:03:11,182 main.py:50] epoch 1298, training loss: 16525.37, average training loss: 19681.71, base loss: 23616.24
[INFO 2017-06-25 16:03:15,465 main.py:50] epoch 1299, training loss: 18731.27, average training loss: 19680.08, base loss: 23617.01
[INFO 2017-06-25 16:03:19,747 main.py:50] epoch 1300, training loss: 16095.52, average training loss: 19671.74, base loss: 23609.35
[INFO 2017-06-25 16:03:24,026 main.py:50] epoch 1301, training loss: 17164.42, average training loss: 19665.29, base loss: 23605.05
[INFO 2017-06-25 16:03:28,307 main.py:50] epoch 1302, training loss: 19740.56, average training loss: 19665.37, base loss: 23608.10
[INFO 2017-06-25 16:03:32,587 main.py:50] epoch 1303, training loss: 15289.35, average training loss: 19661.66, base loss: 23607.11
[INFO 2017-06-25 16:03:36,895 main.py:50] epoch 1304, training loss: 16291.01, average training loss: 19657.44, base loss: 23606.54
[INFO 2017-06-25 16:03:41,171 main.py:50] epoch 1305, training loss: 16021.85, average training loss: 19643.95, base loss: 23595.07
[INFO 2017-06-25 16:03:45,448 main.py:50] epoch 1306, training loss: 18804.22, average training loss: 19643.57, base loss: 23598.35
[INFO 2017-06-25 16:03:49,738 main.py:50] epoch 1307, training loss: 17059.93, average training loss: 19641.81, base loss: 23599.17
[INFO 2017-06-25 16:03:54,012 main.py:50] epoch 1308, training loss: 12159.81, average training loss: 19632.24, base loss: 23591.22
[INFO 2017-06-25 16:03:58,280 main.py:50] epoch 1309, training loss: 17972.74, average training loss: 19630.32, base loss: 23590.57
[INFO 2017-06-25 16:04:02,558 main.py:50] epoch 1310, training loss: 21934.92, average training loss: 19626.15, base loss: 23588.66
[INFO 2017-06-25 16:04:06,822 main.py:50] epoch 1311, training loss: 20896.80, average training loss: 19625.11, base loss: 23590.89
[INFO 2017-06-25 16:04:11,089 main.py:50] epoch 1312, training loss: 20184.78, average training loss: 19623.67, base loss: 23593.01
[INFO 2017-06-25 16:04:15,373 main.py:50] epoch 1313, training loss: 17926.94, average training loss: 19617.22, base loss: 23589.91
[INFO 2017-06-25 16:04:19,642 main.py:50] epoch 1314, training loss: 18070.77, average training loss: 19615.86, base loss: 23590.86
[INFO 2017-06-25 16:04:23,894 main.py:50] epoch 1315, training loss: 19298.70, average training loss: 19615.47, base loss: 23592.57
[INFO 2017-06-25 16:04:28,218 main.py:50] epoch 1316, training loss: 21041.38, average training loss: 19616.71, base loss: 23596.76
[INFO 2017-06-25 16:04:32,479 main.py:50] epoch 1317, training loss: 18365.63, average training loss: 19613.89, base loss: 23596.96
[INFO 2017-06-25 16:04:36,758 main.py:50] epoch 1318, training loss: 17255.15, average training loss: 19608.15, base loss: 23594.78
[INFO 2017-06-25 16:04:41,031 main.py:50] epoch 1319, training loss: 15182.44, average training loss: 19604.51, base loss: 23593.10
[INFO 2017-06-25 16:04:45,317 main.py:50] epoch 1320, training loss: 18783.21, average training loss: 19601.91, base loss: 23593.09
[INFO 2017-06-25 16:04:49,618 main.py:50] epoch 1321, training loss: 18200.52, average training loss: 19603.34, base loss: 23599.33
[INFO 2017-06-25 16:04:53,861 main.py:50] epoch 1322, training loss: 21043.56, average training loss: 19598.13, base loss: 23597.02
[INFO 2017-06-25 16:04:58,132 main.py:50] epoch 1323, training loss: 17367.65, average training loss: 19599.78, base loss: 23602.74
[INFO 2017-06-25 16:05:02,421 main.py:50] epoch 1324, training loss: 18288.39, average training loss: 19600.64, base loss: 23605.08
[INFO 2017-06-25 16:05:06,740 main.py:50] epoch 1325, training loss: 16548.88, average training loss: 19593.06, base loss: 23599.14
[INFO 2017-06-25 16:05:11,023 main.py:50] epoch 1326, training loss: 18543.81, average training loss: 19590.25, base loss: 23599.06
[INFO 2017-06-25 16:05:15,304 main.py:50] epoch 1327, training loss: 20761.39, average training loss: 19590.88, base loss: 23600.99
[INFO 2017-06-25 16:05:19,585 main.py:50] epoch 1328, training loss: 14031.44, average training loss: 19585.14, base loss: 23595.23
[INFO 2017-06-25 16:05:23,858 main.py:50] epoch 1329, training loss: 20047.29, average training loss: 19583.75, base loss: 23597.49
[INFO 2017-06-25 16:05:28,146 main.py:50] epoch 1330, training loss: 19722.21, average training loss: 19585.64, base loss: 23602.18
[INFO 2017-06-25 16:05:32,411 main.py:50] epoch 1331, training loss: 19866.27, average training loss: 19586.82, base loss: 23605.36
[INFO 2017-06-25 16:05:36,716 main.py:50] epoch 1332, training loss: 18132.06, average training loss: 19584.62, base loss: 23605.34
[INFO 2017-06-25 16:05:41,004 main.py:50] epoch 1333, training loss: 16192.43, average training loss: 19582.99, base loss: 23608.00
[INFO 2017-06-25 16:05:45,277 main.py:50] epoch 1334, training loss: 17548.18, average training loss: 19582.07, base loss: 23612.68
[INFO 2017-06-25 16:05:49,564 main.py:50] epoch 1335, training loss: 14801.24, average training loss: 19576.74, base loss: 23609.66
[INFO 2017-06-25 16:05:53,862 main.py:50] epoch 1336, training loss: 17609.65, average training loss: 19575.56, base loss: 23614.14
[INFO 2017-06-25 16:05:58,150 main.py:50] epoch 1337, training loss: 17814.00, average training loss: 19566.18, base loss: 23607.32
[INFO 2017-06-25 16:06:02,443 main.py:50] epoch 1338, training loss: 18199.66, average training loss: 19561.50, base loss: 23603.92
[INFO 2017-06-25 16:06:06,763 main.py:50] epoch 1339, training loss: 18135.61, average training loss: 19552.09, base loss: 23596.75
[INFO 2017-06-25 16:06:11,045 main.py:50] epoch 1340, training loss: 17651.87, average training loss: 19551.18, base loss: 23600.79
[INFO 2017-06-25 16:06:15,349 main.py:50] epoch 1341, training loss: 22171.98, average training loss: 19550.51, base loss: 23601.54
[INFO 2017-06-25 16:06:19,657 main.py:50] epoch 1342, training loss: 15838.06, average training loss: 19546.57, base loss: 23599.21
[INFO 2017-06-25 16:06:23,945 main.py:50] epoch 1343, training loss: 21064.91, average training loss: 19551.73, base loss: 23607.75
[INFO 2017-06-25 16:06:28,239 main.py:50] epoch 1344, training loss: 18328.73, average training loss: 19550.82, base loss: 23606.32
[INFO 2017-06-25 16:06:32,535 main.py:50] epoch 1345, training loss: 20141.51, average training loss: 19547.45, base loss: 23607.12
[INFO 2017-06-25 16:06:36,851 main.py:50] epoch 1346, training loss: 15378.92, average training loss: 19539.15, base loss: 23600.74
[INFO 2017-06-25 16:06:41,159 main.py:50] epoch 1347, training loss: 19040.74, average training loss: 19535.97, base loss: 23598.51
[INFO 2017-06-25 16:06:45,453 main.py:50] epoch 1348, training loss: 17021.76, average training loss: 19530.61, base loss: 23595.32
[INFO 2017-06-25 16:06:49,751 main.py:50] epoch 1349, training loss: 24164.92, average training loss: 19535.60, base loss: 23602.77
[INFO 2017-06-25 16:06:54,048 main.py:50] epoch 1350, training loss: 18317.80, average training loss: 19532.87, base loss: 23603.04
[INFO 2017-06-25 16:06:58,320 main.py:50] epoch 1351, training loss: 18371.85, average training loss: 19531.80, base loss: 23604.56
[INFO 2017-06-25 16:07:02,579 main.py:50] epoch 1352, training loss: 15813.90, average training loss: 19527.65, base loss: 23600.43
[INFO 2017-06-25 16:07:06,884 main.py:50] epoch 1353, training loss: 19572.73, average training loss: 19528.07, base loss: 23604.64
[INFO 2017-06-25 16:07:11,176 main.py:50] epoch 1354, training loss: 20176.66, average training loss: 19523.50, base loss: 23602.26
[INFO 2017-06-25 16:07:15,454 main.py:50] epoch 1355, training loss: 19419.24, average training loss: 19523.96, base loss: 23603.32
[INFO 2017-06-25 16:07:19,756 main.py:50] epoch 1356, training loss: 16261.18, average training loss: 19517.45, base loss: 23596.50
[INFO 2017-06-25 16:07:24,019 main.py:50] epoch 1357, training loss: 19086.13, average training loss: 19514.91, base loss: 23598.10
[INFO 2017-06-25 16:07:28,355 main.py:50] epoch 1358, training loss: 20298.43, average training loss: 19509.87, base loss: 23593.44
[INFO 2017-06-25 16:07:32,622 main.py:50] epoch 1359, training loss: 17065.77, average training loss: 19504.71, base loss: 23588.89
[INFO 2017-06-25 16:07:36,930 main.py:50] epoch 1360, training loss: 19771.10, average training loss: 19503.06, base loss: 23587.73
[INFO 2017-06-25 16:07:41,226 main.py:50] epoch 1361, training loss: 21230.16, average training loss: 19501.27, base loss: 23588.73
[INFO 2017-06-25 16:07:45,517 main.py:50] epoch 1362, training loss: 18782.26, average training loss: 19502.94, base loss: 23594.25
[INFO 2017-06-25 16:07:49,827 main.py:50] epoch 1363, training loss: 24800.42, average training loss: 19509.57, base loss: 23605.53
[INFO 2017-06-25 16:07:54,093 main.py:50] epoch 1364, training loss: 15010.26, average training loss: 19504.45, base loss: 23600.06
[INFO 2017-06-25 16:07:58,398 main.py:50] epoch 1365, training loss: 19092.86, average training loss: 19502.35, base loss: 23599.85
[INFO 2017-06-25 16:08:02,688 main.py:50] epoch 1366, training loss: 17620.84, average training loss: 19501.08, base loss: 23600.19
[INFO 2017-06-25 16:08:06,968 main.py:50] epoch 1367, training loss: 22731.63, average training loss: 19506.13, base loss: 23607.65
[INFO 2017-06-25 16:08:11,255 main.py:50] epoch 1368, training loss: 14313.46, average training loss: 19498.60, base loss: 23599.00
[INFO 2017-06-25 16:08:15,549 main.py:50] epoch 1369, training loss: 17774.71, average training loss: 19495.70, base loss: 23599.72
[INFO 2017-06-25 16:08:19,823 main.py:50] epoch 1370, training loss: 17073.50, average training loss: 19494.40, base loss: 23602.48
[INFO 2017-06-25 16:08:24,093 main.py:50] epoch 1371, training loss: 16375.68, average training loss: 19488.97, base loss: 23599.18
[INFO 2017-06-25 16:08:28,380 main.py:50] epoch 1372, training loss: 14847.75, average training loss: 19481.03, base loss: 23591.79
[INFO 2017-06-25 16:08:32,648 main.py:50] epoch 1373, training loss: 17952.95, average training loss: 19482.16, base loss: 23596.69
[INFO 2017-06-25 16:08:36,938 main.py:50] epoch 1374, training loss: 18608.98, average training loss: 19482.37, base loss: 23601.02
[INFO 2017-06-25 16:08:41,226 main.py:50] epoch 1375, training loss: 15845.48, average training loss: 19479.23, base loss: 23602.27
[INFO 2017-06-25 16:08:45,535 main.py:50] epoch 1376, training loss: 16636.19, average training loss: 19476.53, base loss: 23602.26
[INFO 2017-06-25 16:08:49,811 main.py:50] epoch 1377, training loss: 21058.20, average training loss: 19474.80, base loss: 23603.84
[INFO 2017-06-25 16:08:54,089 main.py:50] epoch 1378, training loss: 20066.26, average training loss: 19473.41, base loss: 23606.09
[INFO 2017-06-25 16:08:58,414 main.py:50] epoch 1379, training loss: 17776.70, average training loss: 19469.42, base loss: 23604.42
[INFO 2017-06-25 16:09:02,708 main.py:50] epoch 1380, training loss: 19384.43, average training loss: 19474.13, base loss: 23614.79
[INFO 2017-06-25 16:09:06,977 main.py:50] epoch 1381, training loss: 17150.61, average training loss: 19471.54, base loss: 23612.33
[INFO 2017-06-25 16:09:11,281 main.py:50] epoch 1382, training loss: 20027.91, average training loss: 19468.39, base loss: 23613.44
[INFO 2017-06-25 16:09:15,552 main.py:50] epoch 1383, training loss: 18647.93, average training loss: 19464.82, base loss: 23612.93
[INFO 2017-06-25 16:09:19,860 main.py:50] epoch 1384, training loss: 19818.15, average training loss: 19459.72, base loss: 23607.57
[INFO 2017-06-25 16:09:24,123 main.py:50] epoch 1385, training loss: 18802.55, average training loss: 19456.05, base loss: 23608.65
[INFO 2017-06-25 16:09:28,398 main.py:50] epoch 1386, training loss: 22495.79, average training loss: 19459.80, base loss: 23613.84
[INFO 2017-06-25 16:09:32,695 main.py:50] epoch 1387, training loss: 16781.37, average training loss: 19460.61, base loss: 23617.12
[INFO 2017-06-25 16:09:36,992 main.py:50] epoch 1388, training loss: 16858.69, average training loss: 19460.33, base loss: 23618.39
[INFO 2017-06-25 16:09:41,290 main.py:50] epoch 1389, training loss: 16824.91, average training loss: 19460.29, base loss: 23620.39
[INFO 2017-06-25 16:09:45,600 main.py:50] epoch 1390, training loss: 16134.21, average training loss: 19454.34, base loss: 23616.40
[INFO 2017-06-25 16:09:49,874 main.py:50] epoch 1391, training loss: 20744.58, average training loss: 19454.55, base loss: 23619.29
[INFO 2017-06-25 16:09:54,154 main.py:50] epoch 1392, training loss: 15225.62, average training loss: 19451.92, base loss: 23620.35
[INFO 2017-06-25 16:09:58,452 main.py:50] epoch 1393, training loss: 19988.80, average training loss: 19446.09, base loss: 23613.35
[INFO 2017-06-25 16:10:02,729 main.py:50] epoch 1394, training loss: 19569.38, average training loss: 19446.25, base loss: 23614.96
[INFO 2017-06-25 16:10:07,029 main.py:50] epoch 1395, training loss: 16296.49, average training loss: 19437.07, base loss: 23607.83
[INFO 2017-06-25 16:10:11,320 main.py:50] epoch 1396, training loss: 21062.37, average training loss: 19437.76, base loss: 23612.01
[INFO 2017-06-25 16:10:15,596 main.py:50] epoch 1397, training loss: 19449.43, average training loss: 19436.90, base loss: 23615.98
[INFO 2017-06-25 16:10:19,867 main.py:50] epoch 1398, training loss: 18553.18, average training loss: 19436.38, base loss: 23616.74
[INFO 2017-06-25 16:10:24,152 main.py:50] epoch 1399, training loss: 15712.29, average training loss: 19433.77, base loss: 23614.94
[INFO 2017-06-25 16:10:28,430 main.py:50] epoch 1400, training loss: 19061.02, average training loss: 19435.08, base loss: 23621.36
[INFO 2017-06-25 16:10:32,740 main.py:50] epoch 1401, training loss: 20439.33, average training loss: 19435.89, base loss: 23625.46
[INFO 2017-06-25 16:10:37,026 main.py:50] epoch 1402, training loss: 18352.97, average training loss: 19433.60, base loss: 23623.87
[INFO 2017-06-25 16:10:41,311 main.py:50] epoch 1403, training loss: 21108.51, average training loss: 19434.92, base loss: 23626.13
[INFO 2017-06-25 16:10:45,606 main.py:50] epoch 1404, training loss: 21676.39, average training loss: 19432.93, base loss: 23626.84
[INFO 2017-06-25 16:10:49,897 main.py:50] epoch 1405, training loss: 21217.55, average training loss: 19435.24, base loss: 23632.85
[INFO 2017-06-25 16:10:54,187 main.py:50] epoch 1406, training loss: 19666.32, average training loss: 19434.83, base loss: 23633.67
[INFO 2017-06-25 16:10:58,471 main.py:50] epoch 1407, training loss: 18012.38, average training loss: 19431.02, base loss: 23631.72
[INFO 2017-06-25 16:11:02,785 main.py:50] epoch 1408, training loss: 23145.55, average training loss: 19437.91, base loss: 23641.88
[INFO 2017-06-25 16:11:07,066 main.py:50] epoch 1409, training loss: 22304.43, average training loss: 19438.68, base loss: 23644.71
[INFO 2017-06-25 16:11:11,365 main.py:50] epoch 1410, training loss: 20475.56, average training loss: 19440.16, base loss: 23649.82
[INFO 2017-06-25 16:11:15,648 main.py:50] epoch 1411, training loss: 16114.20, average training loss: 19427.56, base loss: 23637.95
[INFO 2017-06-25 16:11:19,910 main.py:50] epoch 1412, training loss: 20332.62, average training loss: 19431.55, base loss: 23645.29
[INFO 2017-06-25 16:11:24,185 main.py:50] epoch 1413, training loss: 22071.63, average training loss: 19435.43, base loss: 23655.00
[INFO 2017-06-25 16:11:28,485 main.py:50] epoch 1414, training loss: 22464.28, average training loss: 19435.49, base loss: 23657.36
[INFO 2017-06-25 16:11:32,766 main.py:50] epoch 1415, training loss: 24391.46, average training loss: 19438.38, base loss: 23662.36
[INFO 2017-06-25 16:11:37,052 main.py:50] epoch 1416, training loss: 19619.96, average training loss: 19431.64, base loss: 23656.99
[INFO 2017-06-25 16:11:41,339 main.py:50] epoch 1417, training loss: 21135.20, average training loss: 19432.31, base loss: 23659.16
[INFO 2017-06-25 16:11:45,644 main.py:50] epoch 1418, training loss: 18466.01, average training loss: 19429.69, base loss: 23658.52
[INFO 2017-06-25 16:11:49,917 main.py:50] epoch 1419, training loss: 15859.26, average training loss: 19427.12, base loss: 23659.24
[INFO 2017-06-25 16:11:54,174 main.py:50] epoch 1420, training loss: 22368.80, average training loss: 19432.05, base loss: 23666.89
[INFO 2017-06-25 16:11:58,471 main.py:50] epoch 1421, training loss: 17498.69, average training loss: 19427.69, base loss: 23663.36
[INFO 2017-06-25 16:12:02,763 main.py:50] epoch 1422, training loss: 18576.74, average training loss: 19420.48, base loss: 23657.92
[INFO 2017-06-25 16:12:07,046 main.py:50] epoch 1423, training loss: 18973.31, average training loss: 19419.96, base loss: 23657.12
[INFO 2017-06-25 16:12:11,371 main.py:50] epoch 1424, training loss: 18505.79, average training loss: 19423.20, base loss: 23662.62
[INFO 2017-06-25 16:12:15,693 main.py:50] epoch 1425, training loss: 20992.49, average training loss: 19418.45, base loss: 23660.24
[INFO 2017-06-25 16:12:19,988 main.py:50] epoch 1426, training loss: 18892.51, average training loss: 19420.06, base loss: 23663.40
[INFO 2017-06-25 16:12:24,274 main.py:50] epoch 1427, training loss: 20471.20, average training loss: 19418.69, base loss: 23662.99
[INFO 2017-06-25 16:12:28,581 main.py:50] epoch 1428, training loss: 18515.76, average training loss: 19417.55, base loss: 23666.03
[INFO 2017-06-25 16:12:32,869 main.py:50] epoch 1429, training loss: 17898.58, average training loss: 19418.61, base loss: 23670.50
[INFO 2017-06-25 16:12:37,183 main.py:50] epoch 1430, training loss: 17342.18, average training loss: 19415.60, base loss: 23668.90
[INFO 2017-06-25 16:12:41,470 main.py:50] epoch 1431, training loss: 23663.96, average training loss: 19417.37, base loss: 23674.96
[INFO 2017-06-25 16:12:45,735 main.py:50] epoch 1432, training loss: 21044.40, average training loss: 19418.84, base loss: 23680.78
[INFO 2017-06-25 16:12:49,996 main.py:50] epoch 1433, training loss: 21242.02, average training loss: 19419.44, base loss: 23683.79
[INFO 2017-06-25 16:12:54,271 main.py:50] epoch 1434, training loss: 16889.12, average training loss: 19414.09, base loss: 23678.24
[INFO 2017-06-25 16:12:58,579 main.py:50] epoch 1435, training loss: 19153.13, average training loss: 19416.96, base loss: 23682.53
[INFO 2017-06-25 16:13:02,866 main.py:50] epoch 1436, training loss: 20221.38, average training loss: 19413.61, base loss: 23680.10
[INFO 2017-06-25 16:13:07,149 main.py:50] epoch 1437, training loss: 19284.08, average training loss: 19411.77, base loss: 23679.76
[INFO 2017-06-25 16:13:11,432 main.py:50] epoch 1438, training loss: 17446.98, average training loss: 19406.71, base loss: 23677.38
[INFO 2017-06-25 16:13:15,707 main.py:50] epoch 1439, training loss: 24825.77, average training loss: 19406.66, base loss: 23679.41
[INFO 2017-06-25 16:13:19,987 main.py:50] epoch 1440, training loss: 22476.33, average training loss: 19406.70, base loss: 23679.60
[INFO 2017-06-25 16:13:24,277 main.py:50] epoch 1441, training loss: 17708.57, average training loss: 19403.47, base loss: 23678.19
[INFO 2017-06-25 16:13:28,574 main.py:50] epoch 1442, training loss: 16932.12, average training loss: 19399.33, base loss: 23673.82
[INFO 2017-06-25 16:13:32,872 main.py:50] epoch 1443, training loss: 19264.12, average training loss: 19396.10, base loss: 23674.41
[INFO 2017-06-25 16:13:37,132 main.py:50] epoch 1444, training loss: 16390.19, average training loss: 19392.37, base loss: 23670.38
[INFO 2017-06-25 16:13:41,378 main.py:50] epoch 1445, training loss: 18328.53, average training loss: 19393.54, base loss: 23672.40
[INFO 2017-06-25 16:13:45,643 main.py:50] epoch 1446, training loss: 20931.65, average training loss: 19393.42, base loss: 23674.16
[INFO 2017-06-25 16:13:49,934 main.py:50] epoch 1447, training loss: 14359.18, average training loss: 19389.55, base loss: 23671.37
[INFO 2017-06-25 16:13:54,200 main.py:50] epoch 1448, training loss: 19100.40, average training loss: 19389.79, base loss: 23675.25
[INFO 2017-06-25 16:13:58,467 main.py:50] epoch 1449, training loss: 16286.40, average training loss: 19388.05, base loss: 23675.49
[INFO 2017-06-25 16:14:02,777 main.py:50] epoch 1450, training loss: 20557.08, average training loss: 19388.59, base loss: 23678.55
[INFO 2017-06-25 16:14:07,037 main.py:50] epoch 1451, training loss: 23650.65, average training loss: 19395.61, base loss: 23685.96
[INFO 2017-06-25 16:14:11,326 main.py:50] epoch 1452, training loss: 22718.01, average training loss: 19393.53, base loss: 23684.94
[INFO 2017-06-25 16:14:15,650 main.py:50] epoch 1453, training loss: 17965.80, average training loss: 19394.50, base loss: 23688.63
[INFO 2017-06-25 16:14:19,966 main.py:50] epoch 1454, training loss: 19130.12, average training loss: 19392.93, base loss: 23687.99
[INFO 2017-06-25 16:14:24,250 main.py:50] epoch 1455, training loss: 18294.57, average training loss: 19388.32, base loss: 23682.61
[INFO 2017-06-25 16:14:28,531 main.py:50] epoch 1456, training loss: 16703.28, average training loss: 19385.63, base loss: 23683.29
[INFO 2017-06-25 16:14:32,810 main.py:50] epoch 1457, training loss: 17028.18, average training loss: 19383.97, base loss: 23684.48
[INFO 2017-06-25 16:14:37,077 main.py:50] epoch 1458, training loss: 16575.79, average training loss: 19381.33, base loss: 23684.58
[INFO 2017-06-25 16:14:41,398 main.py:50] epoch 1459, training loss: 16109.04, average training loss: 19374.20, base loss: 23676.50
[INFO 2017-06-25 16:14:45,673 main.py:50] epoch 1460, training loss: 20487.88, average training loss: 19368.52, base loss: 23672.90
[INFO 2017-06-25 16:14:49,985 main.py:50] epoch 1461, training loss: 20479.55, average training loss: 19364.08, base loss: 23669.76
[INFO 2017-06-25 16:14:54,282 main.py:50] epoch 1462, training loss: 20937.40, average training loss: 19367.71, base loss: 23675.16
[INFO 2017-06-25 16:14:58,571 main.py:50] epoch 1463, training loss: 14185.18, average training loss: 19360.08, base loss: 23667.08
[INFO 2017-06-25 16:15:02,861 main.py:50] epoch 1464, training loss: 19585.21, average training loss: 19357.59, base loss: 23665.54
[INFO 2017-06-25 16:15:07,133 main.py:50] epoch 1465, training loss: 21919.13, average training loss: 19364.05, base loss: 23674.25
[INFO 2017-06-25 16:15:11,408 main.py:50] epoch 1466, training loss: 20558.34, average training loss: 19360.56, base loss: 23670.29
[INFO 2017-06-25 16:15:15,701 main.py:50] epoch 1467, training loss: 17847.51, average training loss: 19360.14, base loss: 23671.02
[INFO 2017-06-25 16:15:19,981 main.py:50] epoch 1468, training loss: 19315.73, average training loss: 19355.85, base loss: 23668.46
[INFO 2017-06-25 16:15:24,247 main.py:50] epoch 1469, training loss: 17472.61, average training loss: 19353.90, base loss: 23669.07
[INFO 2017-06-25 16:15:28,546 main.py:50] epoch 1470, training loss: 20789.22, average training loss: 19351.76, base loss: 23666.93
[INFO 2017-06-25 16:15:32,831 main.py:50] epoch 1471, training loss: 20626.96, average training loss: 19347.98, base loss: 23665.68
[INFO 2017-06-25 16:15:37,118 main.py:50] epoch 1472, training loss: 19866.12, average training loss: 19347.65, base loss: 23665.28
[INFO 2017-06-25 16:15:41,400 main.py:50] epoch 1473, training loss: 18470.04, average training loss: 19343.72, base loss: 23663.42
[INFO 2017-06-25 16:15:45,699 main.py:50] epoch 1474, training loss: 19524.84, average training loss: 19340.54, base loss: 23663.62
[INFO 2017-06-25 16:15:49,990 main.py:50] epoch 1475, training loss: 17272.77, average training loss: 19338.02, base loss: 23661.90
[INFO 2017-06-25 16:15:54,282 main.py:50] epoch 1476, training loss: 19153.50, average training loss: 19339.33, base loss: 23666.96
[INFO 2017-06-25 16:15:58,594 main.py:50] epoch 1477, training loss: 19818.74, average training loss: 19340.84, base loss: 23672.26
[INFO 2017-06-25 16:16:02,886 main.py:50] epoch 1478, training loss: 14657.61, average training loss: 19337.98, base loss: 23671.83
[INFO 2017-06-25 16:16:07,203 main.py:50] epoch 1479, training loss: 18628.55, average training loss: 19335.68, base loss: 23669.45
[INFO 2017-06-25 16:16:11,506 main.py:50] epoch 1480, training loss: 15768.67, average training loss: 19331.09, base loss: 23665.46
[INFO 2017-06-25 16:16:15,830 main.py:50] epoch 1481, training loss: 17417.24, average training loss: 19327.83, base loss: 23664.21
[INFO 2017-06-25 16:16:20,098 main.py:50] epoch 1482, training loss: 14878.24, average training loss: 19323.17, base loss: 23662.65
[INFO 2017-06-25 16:16:24,415 main.py:50] epoch 1483, training loss: 17459.87, average training loss: 19319.14, base loss: 23661.76
[INFO 2017-06-25 16:16:28,739 main.py:50] epoch 1484, training loss: 16806.16, average training loss: 19313.74, base loss: 23657.21
[INFO 2017-06-25 16:16:33,023 main.py:50] epoch 1485, training loss: 18245.66, average training loss: 19311.46, base loss: 23658.58
[INFO 2017-06-25 16:16:37,335 main.py:50] epoch 1486, training loss: 25175.15, average training loss: 19316.22, base loss: 23666.43
[INFO 2017-06-25 16:16:41,640 main.py:50] epoch 1487, training loss: 13969.75, average training loss: 19309.33, base loss: 23658.99
[INFO 2017-06-25 16:16:45,927 main.py:50] epoch 1488, training loss: 19101.51, average training loss: 19307.90, base loss: 23657.95
[INFO 2017-06-25 16:16:50,234 main.py:50] epoch 1489, training loss: 17520.23, average training loss: 19307.79, base loss: 23658.76
[INFO 2017-06-25 16:16:54,530 main.py:50] epoch 1490, training loss: 17941.50, average training loss: 19307.34, base loss: 23659.61
[INFO 2017-06-25 16:16:58,799 main.py:50] epoch 1491, training loss: 19604.45, average training loss: 19305.89, base loss: 23661.38
[INFO 2017-06-25 16:17:03,081 main.py:50] epoch 1492, training loss: 19195.45, average training loss: 19301.71, base loss: 23660.05
[INFO 2017-06-25 16:17:07,370 main.py:50] epoch 1493, training loss: 21258.39, average training loss: 19302.29, base loss: 23664.69
[INFO 2017-06-25 16:17:11,657 main.py:50] epoch 1494, training loss: 16887.62, average training loss: 19299.76, base loss: 23665.82
[INFO 2017-06-25 16:17:15,918 main.py:50] epoch 1495, training loss: 18133.32, average training loss: 19299.55, base loss: 23667.20
[INFO 2017-06-25 16:17:20,216 main.py:50] epoch 1496, training loss: 18497.07, average training loss: 19296.78, base loss: 23667.38
[INFO 2017-06-25 16:17:24,487 main.py:50] epoch 1497, training loss: 14341.40, average training loss: 19286.32, base loss: 23655.66
[INFO 2017-06-25 16:17:28,739 main.py:50] epoch 1498, training loss: 16207.62, average training loss: 19283.79, base loss: 23654.35
[INFO 2017-06-25 16:17:33,023 main.py:50] epoch 1499, training loss: 16188.13, average training loss: 19277.72, base loss: 23648.33
[INFO 2017-06-25 16:17:37,297 main.py:50] epoch 1500, training loss: 18694.02, average training loss: 19276.64, base loss: 23648.92
[INFO 2017-06-25 16:17:41,551 main.py:50] epoch 1501, training loss: 13686.25, average training loss: 19273.13, base loss: 23645.47
[INFO 2017-06-25 16:17:45,817 main.py:50] epoch 1502, training loss: 18723.81, average training loss: 19269.03, base loss: 23641.50
[INFO 2017-06-25 16:17:50,062 main.py:50] epoch 1503, training loss: 13299.66, average training loss: 19261.04, base loss: 23631.74
[INFO 2017-06-25 16:17:54,346 main.py:50] epoch 1504, training loss: 27437.63, average training loss: 19267.07, base loss: 23638.96
[INFO 2017-06-25 16:17:58,622 main.py:50] epoch 1505, training loss: 14365.43, average training loss: 19259.70, base loss: 23632.17
[INFO 2017-06-25 16:18:02,875 main.py:50] epoch 1506, training loss: 23845.46, average training loss: 19265.33, base loss: 23641.17
[INFO 2017-06-25 16:18:07,156 main.py:50] epoch 1507, training loss: 21733.16, average training loss: 19262.30, base loss: 23638.14
[INFO 2017-06-25 16:18:11,458 main.py:50] epoch 1508, training loss: 20580.21, average training loss: 19264.02, base loss: 23641.89
[INFO 2017-06-25 16:18:15,709 main.py:50] epoch 1509, training loss: 12973.86, average training loss: 19259.42, base loss: 23636.17
[INFO 2017-06-25 16:18:19,960 main.py:50] epoch 1510, training loss: 17921.22, average training loss: 19254.12, base loss: 23632.23
[INFO 2017-06-25 16:18:24,271 main.py:50] epoch 1511, training loss: 15884.81, average training loss: 19252.71, base loss: 23634.18
[INFO 2017-06-25 16:18:28,541 main.py:50] epoch 1512, training loss: 17402.49, average training loss: 19248.47, base loss: 23632.16
[INFO 2017-06-25 16:18:32,821 main.py:50] epoch 1513, training loss: 15762.01, average training loss: 19246.92, base loss: 23630.29
[INFO 2017-06-25 16:18:37,133 main.py:50] epoch 1514, training loss: 23242.84, average training loss: 19245.96, base loss: 23632.90
[INFO 2017-06-25 16:18:41,423 main.py:50] epoch 1515, training loss: 16648.39, average training loss: 19242.76, base loss: 23631.27
[INFO 2017-06-25 16:18:45,721 main.py:50] epoch 1516, training loss: 19812.71, average training loss: 19242.42, base loss: 23632.04
[INFO 2017-06-25 16:18:50,004 main.py:50] epoch 1517, training loss: 21393.67, average training loss: 19239.36, base loss: 23628.15
[INFO 2017-06-25 16:18:54,308 main.py:50] epoch 1518, training loss: 14115.73, average training loss: 19228.37, base loss: 23617.34
[INFO 2017-06-25 16:18:58,623 main.py:50] epoch 1519, training loss: 20263.15, average training loss: 19224.72, base loss: 23618.25
[INFO 2017-06-25 16:19:02,899 main.py:50] epoch 1520, training loss: 21671.00, average training loss: 19227.22, base loss: 23622.68
[INFO 2017-06-25 16:19:07,156 main.py:50] epoch 1521, training loss: 16869.43, average training loss: 19225.69, base loss: 23621.91
[INFO 2017-06-25 16:19:11,436 main.py:50] epoch 1522, training loss: 17214.72, average training loss: 19221.14, base loss: 23618.69
[INFO 2017-06-25 16:19:15,704 main.py:50] epoch 1523, training loss: 23129.70, average training loss: 19228.81, base loss: 23628.82
[INFO 2017-06-25 16:19:20,005 main.py:50] epoch 1524, training loss: 19083.42, average training loss: 19229.13, base loss: 23633.37
[INFO 2017-06-25 16:19:24,314 main.py:50] epoch 1525, training loss: 19128.35, average training loss: 19229.73, base loss: 23637.53
[INFO 2017-06-25 16:19:28,604 main.py:50] epoch 1526, training loss: 12955.32, average training loss: 19224.96, base loss: 23632.28
[INFO 2017-06-25 16:19:32,910 main.py:50] epoch 1527, training loss: 18768.59, average training loss: 19224.79, base loss: 23635.65
[INFO 2017-06-25 16:19:37,188 main.py:50] epoch 1528, training loss: 19568.43, average training loss: 19225.62, base loss: 23637.33
[INFO 2017-06-25 16:19:41,457 main.py:50] epoch 1529, training loss: 17055.69, average training loss: 19222.07, base loss: 23635.90
[INFO 2017-06-25 16:19:45,728 main.py:50] epoch 1530, training loss: 24349.72, average training loss: 19229.00, base loss: 23645.98
[INFO 2017-06-25 16:19:50,012 main.py:50] epoch 1531, training loss: 15168.75, average training loss: 19223.01, base loss: 23640.26
[INFO 2017-06-25 16:19:54,277 main.py:50] epoch 1532, training loss: 17615.81, average training loss: 19218.84, base loss: 23637.86
[INFO 2017-06-25 16:19:58,548 main.py:50] epoch 1533, training loss: 16552.72, average training loss: 19215.07, base loss: 23634.56
[INFO 2017-06-25 16:20:02,810 main.py:50] epoch 1534, training loss: 17353.24, average training loss: 19215.43, base loss: 23636.31
[INFO 2017-06-25 16:20:07,088 main.py:50] epoch 1535, training loss: 14225.30, average training loss: 19209.79, base loss: 23633.47
[INFO 2017-06-25 16:20:11,373 main.py:50] epoch 1536, training loss: 19942.09, average training loss: 19210.98, base loss: 23637.75
[INFO 2017-06-25 16:20:15,658 main.py:50] epoch 1537, training loss: 16072.79, average training loss: 19203.72, base loss: 23631.34
[INFO 2017-06-25 16:20:19,939 main.py:50] epoch 1538, training loss: 15675.92, average training loss: 19201.26, base loss: 23630.93
[INFO 2017-06-25 16:20:24,221 main.py:50] epoch 1539, training loss: 18117.68, average training loss: 19201.42, base loss: 23632.44
[INFO 2017-06-25 16:20:28,500 main.py:50] epoch 1540, training loss: 19510.14, average training loss: 19203.89, base loss: 23636.61
[INFO 2017-06-25 16:20:32,762 main.py:50] epoch 1541, training loss: 13079.41, average training loss: 19195.93, base loss: 23627.78
[INFO 2017-06-25 16:20:37,060 main.py:50] epoch 1542, training loss: 21152.17, average training loss: 19200.51, base loss: 23636.62
[INFO 2017-06-25 16:20:41,333 main.py:50] epoch 1543, training loss: 18340.32, average training loss: 19197.31, base loss: 23635.11
[INFO 2017-06-25 16:20:45,598 main.py:50] epoch 1544, training loss: 19356.11, average training loss: 19193.29, base loss: 23631.87
[INFO 2017-06-25 16:20:49,891 main.py:50] epoch 1545, training loss: 19292.79, average training loss: 19189.11, base loss: 23628.43
[INFO 2017-06-25 16:20:54,170 main.py:50] epoch 1546, training loss: 19541.79, average training loss: 19188.53, base loss: 23629.24
[INFO 2017-06-25 16:20:58,465 main.py:50] epoch 1547, training loss: 20258.40, average training loss: 19191.14, base loss: 23633.61
[INFO 2017-06-25 16:21:02,748 main.py:50] epoch 1548, training loss: 16352.39, average training loss: 19184.90, base loss: 23628.99
[INFO 2017-06-25 16:21:07,025 main.py:50] epoch 1549, training loss: 23579.42, average training loss: 19187.62, base loss: 23632.19
[INFO 2017-06-25 16:21:11,331 main.py:50] epoch 1550, training loss: 17318.61, average training loss: 19188.71, base loss: 23635.79
[INFO 2017-06-25 16:21:15,596 main.py:50] epoch 1551, training loss: 20675.13, average training loss: 19186.86, base loss: 23636.78
[INFO 2017-06-25 16:21:19,872 main.py:50] epoch 1552, training loss: 17756.17, average training loss: 19182.82, base loss: 23633.21
[INFO 2017-06-25 16:21:24,150 main.py:50] epoch 1553, training loss: 16609.36, average training loss: 19180.39, base loss: 23631.04
[INFO 2017-06-25 16:21:28,425 main.py:50] epoch 1554, training loss: 18574.62, average training loss: 19179.08, base loss: 23631.38
[INFO 2017-06-25 16:21:32,678 main.py:50] epoch 1555, training loss: 16531.62, average training loss: 19177.12, base loss: 23633.27
[INFO 2017-06-25 16:21:36,956 main.py:50] epoch 1556, training loss: 18425.10, average training loss: 19177.52, base loss: 23634.71
[INFO 2017-06-25 16:21:41,236 main.py:50] epoch 1557, training loss: 17042.07, average training loss: 19171.11, base loss: 23630.25
[INFO 2017-06-25 16:21:45,500 main.py:50] epoch 1558, training loss: 16989.41, average training loss: 19168.40, base loss: 23628.92
[INFO 2017-06-25 16:21:49,772 main.py:50] epoch 1559, training loss: 19780.58, average training loss: 19169.13, base loss: 23631.29
[INFO 2017-06-25 16:21:54,047 main.py:50] epoch 1560, training loss: 17422.88, average training loss: 19168.04, base loss: 23632.75
[INFO 2017-06-25 16:21:58,364 main.py:50] epoch 1561, training loss: 22574.37, average training loss: 19169.41, base loss: 23634.52
[INFO 2017-06-25 16:22:02,642 main.py:50] epoch 1562, training loss: 16148.43, average training loss: 19162.08, base loss: 23627.06
[INFO 2017-06-25 16:22:06,938 main.py:50] epoch 1563, training loss: 17973.85, average training loss: 19157.88, base loss: 23625.18
[INFO 2017-06-25 16:22:11,225 main.py:50] epoch 1564, training loss: 14995.31, average training loss: 19151.68, base loss: 23618.28
[INFO 2017-06-25 16:22:15,535 main.py:50] epoch 1565, training loss: 22653.21, average training loss: 19146.93, base loss: 23613.44
[INFO 2017-06-25 16:22:19,818 main.py:50] epoch 1566, training loss: 19732.67, average training loss: 19148.52, base loss: 23618.47
[INFO 2017-06-25 16:22:24,097 main.py:50] epoch 1567, training loss: 18311.44, average training loss: 19148.77, base loss: 23620.48
[INFO 2017-06-25 16:22:28,369 main.py:50] epoch 1568, training loss: 15891.04, average training loss: 19142.40, base loss: 23614.92
[INFO 2017-06-25 16:22:32,627 main.py:50] epoch 1569, training loss: 17091.88, average training loss: 19140.37, base loss: 23614.16
[INFO 2017-06-25 16:22:36,924 main.py:50] epoch 1570, training loss: 15621.85, average training loss: 19135.87, base loss: 23608.42
[INFO 2017-06-25 16:22:41,235 main.py:50] epoch 1571, training loss: 20351.84, average training loss: 19137.43, base loss: 23611.23
[INFO 2017-06-25 16:22:45,527 main.py:50] epoch 1572, training loss: 20372.10, average training loss: 19135.73, base loss: 23612.35
[INFO 2017-06-25 16:22:49,815 main.py:50] epoch 1573, training loss: 15722.55, average training loss: 19129.09, base loss: 23604.05
[INFO 2017-06-25 16:22:54,081 main.py:50] epoch 1574, training loss: 16748.19, average training loss: 19122.29, base loss: 23599.10
[INFO 2017-06-25 16:22:58,366 main.py:50] epoch 1575, training loss: 20931.16, average training loss: 19125.27, base loss: 23602.51
[INFO 2017-06-25 16:23:02,655 main.py:50] epoch 1576, training loss: 18562.90, average training loss: 19126.22, base loss: 23606.75
[INFO 2017-06-25 16:23:06,913 main.py:50] epoch 1577, training loss: 18084.24, average training loss: 19126.81, base loss: 23611.22
[INFO 2017-06-25 16:23:11,177 main.py:50] epoch 1578, training loss: 15788.33, average training loss: 19122.89, base loss: 23609.67
[INFO 2017-06-25 16:23:15,442 main.py:50] epoch 1579, training loss: 20603.78, average training loss: 19125.36, base loss: 23612.14
[INFO 2017-06-25 16:23:19,716 main.py:50] epoch 1580, training loss: 16377.50, average training loss: 19120.63, base loss: 23607.43
[INFO 2017-06-25 16:23:24,011 main.py:50] epoch 1581, training loss: 18749.57, average training loss: 19117.97, base loss: 23604.01
[INFO 2017-06-25 16:23:28,288 main.py:50] epoch 1582, training loss: 19378.09, average training loss: 19118.66, base loss: 23605.32
[INFO 2017-06-25 16:23:32,563 main.py:50] epoch 1583, training loss: 21272.14, average training loss: 19116.15, base loss: 23602.55
[INFO 2017-06-25 16:23:36,829 main.py:50] epoch 1584, training loss: 14410.01, average training loss: 19113.43, base loss: 23599.34
[INFO 2017-06-25 16:23:41,092 main.py:50] epoch 1585, training loss: 15169.32, average training loss: 19111.97, base loss: 23599.69
[INFO 2017-06-25 16:23:45,367 main.py:50] epoch 1586, training loss: 15596.57, average training loss: 19108.89, base loss: 23598.45
[INFO 2017-06-25 16:23:49,657 main.py:50] epoch 1587, training loss: 15696.07, average training loss: 19107.20, base loss: 23597.63
[INFO 2017-06-25 16:23:53,934 main.py:50] epoch 1588, training loss: 16339.43, average training loss: 19099.97, base loss: 23590.98
[INFO 2017-06-25 16:23:58,207 main.py:50] epoch 1589, training loss: 17378.45, average training loss: 19095.62, base loss: 23589.50
[INFO 2017-06-25 16:24:02,512 main.py:50] epoch 1590, training loss: 20367.99, average training loss: 19098.70, base loss: 23596.13
[INFO 2017-06-25 16:24:06,778 main.py:50] epoch 1591, training loss: 15021.01, average training loss: 19086.67, base loss: 23582.33
[INFO 2017-06-25 16:24:11,096 main.py:50] epoch 1592, training loss: 15615.25, average training loss: 19080.03, base loss: 23576.12
[INFO 2017-06-25 16:24:15,399 main.py:50] epoch 1593, training loss: 18038.26, average training loss: 19076.97, base loss: 23572.85
[INFO 2017-06-25 16:24:19,682 main.py:50] epoch 1594, training loss: 18311.49, average training loss: 19077.49, base loss: 23576.15
[INFO 2017-06-25 16:24:23,975 main.py:50] epoch 1595, training loss: 17362.90, average training loss: 19070.55, base loss: 23569.87
[INFO 2017-06-25 16:24:28,263 main.py:50] epoch 1596, training loss: 14588.25, average training loss: 19064.49, base loss: 23563.65
[INFO 2017-06-25 16:24:32,524 main.py:50] epoch 1597, training loss: 18853.20, average training loss: 19065.90, base loss: 23566.97
[INFO 2017-06-25 16:24:36,789 main.py:50] epoch 1598, training loss: 22849.12, average training loss: 19069.49, base loss: 23571.69
[INFO 2017-06-25 16:24:41,062 main.py:50] epoch 1599, training loss: 17845.20, average training loss: 19062.28, base loss: 23565.19
[INFO 2017-06-25 16:24:45,357 main.py:50] epoch 1600, training loss: 18147.15, average training loss: 19055.27, base loss: 23558.87
[INFO 2017-06-25 16:24:49,651 main.py:50] epoch 1601, training loss: 18013.85, average training loss: 19049.11, base loss: 23552.33
[INFO 2017-06-25 16:24:53,905 main.py:50] epoch 1602, training loss: 15648.64, average training loss: 19045.08, base loss: 23551.26
[INFO 2017-06-25 16:24:58,176 main.py:50] epoch 1603, training loss: 18989.98, average training loss: 19043.30, base loss: 23547.73
[INFO 2017-06-25 16:25:02,437 main.py:50] epoch 1604, training loss: 17892.77, average training loss: 19039.81, base loss: 23545.44
[INFO 2017-06-25 16:25:06,725 main.py:50] epoch 1605, training loss: 18398.91, average training loss: 19039.18, base loss: 23546.11
[INFO 2017-06-25 16:25:11,004 main.py:50] epoch 1606, training loss: 18125.98, average training loss: 19036.93, base loss: 23544.35
[INFO 2017-06-25 16:25:15,279 main.py:50] epoch 1607, training loss: 15988.06, average training loss: 19033.57, base loss: 23540.77
[INFO 2017-06-25 16:25:19,554 main.py:50] epoch 1608, training loss: 17052.15, average training loss: 19029.32, base loss: 23538.22
[INFO 2017-06-25 16:25:23,846 main.py:50] epoch 1609, training loss: 21965.21, average training loss: 19028.80, base loss: 23538.05
[INFO 2017-06-25 16:25:28,115 main.py:50] epoch 1610, training loss: 16765.71, average training loss: 19024.99, base loss: 23535.90
[INFO 2017-06-25 16:25:32,381 main.py:50] epoch 1611, training loss: 17238.92, average training loss: 19023.02, base loss: 23538.11
[INFO 2017-06-25 16:25:36,667 main.py:50] epoch 1612, training loss: 14988.32, average training loss: 19018.52, base loss: 23532.66
[INFO 2017-06-25 16:25:40,950 main.py:50] epoch 1613, training loss: 23650.64, average training loss: 19022.01, base loss: 23537.85
[INFO 2017-06-25 16:25:45,258 main.py:50] epoch 1614, training loss: 17867.41, average training loss: 19019.73, base loss: 23535.67
[INFO 2017-06-25 16:25:49,521 main.py:50] epoch 1615, training loss: 15885.93, average training loss: 19018.74, base loss: 23535.06
[INFO 2017-06-25 16:25:53,812 main.py:50] epoch 1616, training loss: 16966.16, average training loss: 19013.28, base loss: 23528.91
[INFO 2017-06-25 16:25:58,093 main.py:50] epoch 1617, training loss: 20931.72, average training loss: 19012.44, base loss: 23531.63
[INFO 2017-06-25 16:26:02,404 main.py:50] epoch 1618, training loss: 24978.07, average training loss: 19018.75, base loss: 23538.61
[INFO 2017-06-25 16:26:06,652 main.py:50] epoch 1619, training loss: 20321.83, average training loss: 19019.55, base loss: 23541.83
[INFO 2017-06-25 16:26:10,940 main.py:50] epoch 1620, training loss: 19316.78, average training loss: 19022.41, base loss: 23549.25
[INFO 2017-06-25 16:26:15,236 main.py:50] epoch 1621, training loss: 20120.35, average training loss: 19017.39, base loss: 23545.41
[INFO 2017-06-25 16:26:19,511 main.py:50] epoch 1622, training loss: 18887.07, average training loss: 19016.61, base loss: 23545.01
[INFO 2017-06-25 16:26:23,763 main.py:50] epoch 1623, training loss: 19018.16, average training loss: 19013.29, base loss: 23543.02
[INFO 2017-06-25 16:26:28,042 main.py:50] epoch 1624, training loss: 24451.58, average training loss: 19022.20, base loss: 23556.47
[INFO 2017-06-25 16:26:32,341 main.py:50] epoch 1625, training loss: 21161.67, average training loss: 19016.78, base loss: 23551.92
[INFO 2017-06-25 16:26:36,627 main.py:50] epoch 1626, training loss: 20045.10, average training loss: 19013.73, base loss: 23549.82
[INFO 2017-06-25 16:26:40,910 main.py:50] epoch 1627, training loss: 19185.19, average training loss: 19010.53, base loss: 23546.12
[INFO 2017-06-25 16:26:45,196 main.py:50] epoch 1628, training loss: 14127.99, average training loss: 19005.94, base loss: 23542.02
[INFO 2017-06-25 16:26:49,489 main.py:50] epoch 1629, training loss: 17803.43, average training loss: 19003.35, base loss: 23540.78
[INFO 2017-06-25 16:26:53,797 main.py:50] epoch 1630, training loss: 21293.48, average training loss: 19004.13, base loss: 23543.01
[INFO 2017-06-25 16:26:58,087 main.py:50] epoch 1631, training loss: 18253.74, average training loss: 19004.73, base loss: 23543.90
[INFO 2017-06-25 16:27:02,371 main.py:50] epoch 1632, training loss: 17819.39, average training loss: 19003.70, base loss: 23543.53
[INFO 2017-06-25 16:27:06,646 main.py:50] epoch 1633, training loss: 21250.42, average training loss: 19000.82, base loss: 23543.11
[INFO 2017-06-25 16:27:10,920 main.py:50] epoch 1634, training loss: 17451.07, average training loss: 18996.10, base loss: 23541.65
[INFO 2017-06-25 16:27:15,199 main.py:50] epoch 1635, training loss: 19903.36, average training loss: 18995.26, base loss: 23538.24
[INFO 2017-06-25 16:27:19,486 main.py:50] epoch 1636, training loss: 18289.53, average training loss: 18995.09, base loss: 23536.07
[INFO 2017-06-25 16:27:23,753 main.py:50] epoch 1637, training loss: 18279.01, average training loss: 18991.72, base loss: 23534.14
[INFO 2017-06-25 16:27:28,020 main.py:50] epoch 1638, training loss: 20560.08, average training loss: 18992.96, base loss: 23535.57
[INFO 2017-06-25 16:27:32,303 main.py:50] epoch 1639, training loss: 18852.25, average training loss: 18992.88, base loss: 23537.56
[INFO 2017-06-25 16:27:36,577 main.py:50] epoch 1640, training loss: 17500.01, average training loss: 18990.83, base loss: 23538.36
[INFO 2017-06-25 16:27:40,852 main.py:50] epoch 1641, training loss: 19123.56, average training loss: 18990.87, base loss: 23539.77
[INFO 2017-06-25 16:27:45,136 main.py:50] epoch 1642, training loss: 19085.83, average training loss: 18994.52, base loss: 23542.75
[INFO 2017-06-25 16:27:49,429 main.py:50] epoch 1643, training loss: 19543.55, average training loss: 18994.27, base loss: 23545.56
[INFO 2017-06-25 16:27:53,728 main.py:50] epoch 1644, training loss: 13812.70, average training loss: 18988.14, base loss: 23540.95
[INFO 2017-06-25 16:27:57,976 main.py:50] epoch 1645, training loss: 20677.89, average training loss: 18990.73, base loss: 23547.11
[INFO 2017-06-25 16:28:02,248 main.py:50] epoch 1646, training loss: 15155.81, average training loss: 18989.11, base loss: 23548.54
[INFO 2017-06-25 16:28:06,509 main.py:50] epoch 1647, training loss: 20243.06, average training loss: 18991.71, base loss: 23552.47
[INFO 2017-06-25 16:28:10,768 main.py:50] epoch 1648, training loss: 17407.17, average training loss: 18989.35, base loss: 23549.22
[INFO 2017-06-25 16:28:15,025 main.py:50] epoch 1649, training loss: 14060.62, average training loss: 18985.15, base loss: 23546.58
[INFO 2017-06-25 16:28:19,289 main.py:50] epoch 1650, training loss: 20997.20, average training loss: 18980.29, base loss: 23541.38
[INFO 2017-06-25 16:28:23,556 main.py:50] epoch 1651, training loss: 23303.28, average training loss: 18985.90, base loss: 23551.20
[INFO 2017-06-25 16:28:27,833 main.py:50] epoch 1652, training loss: 17083.33, average training loss: 18985.23, base loss: 23549.34
[INFO 2017-06-25 16:28:32,129 main.py:50] epoch 1653, training loss: 16371.27, average training loss: 18982.47, base loss: 23548.88
[INFO 2017-06-25 16:28:36,382 main.py:50] epoch 1654, training loss: 16141.48, average training loss: 18973.23, base loss: 23538.18
[INFO 2017-06-25 16:28:40,666 main.py:50] epoch 1655, training loss: 21184.78, average training loss: 18974.53, base loss: 23539.67
[INFO 2017-06-25 16:28:44,948 main.py:50] epoch 1656, training loss: 15987.08, average training loss: 18969.03, base loss: 23534.21
[INFO 2017-06-25 16:28:49,217 main.py:50] epoch 1657, training loss: 18628.41, average training loss: 18965.05, base loss: 23530.87
[INFO 2017-06-25 16:28:53,472 main.py:50] epoch 1658, training loss: 19588.25, average training loss: 18964.46, base loss: 23528.47
[INFO 2017-06-25 16:28:57,740 main.py:50] epoch 1659, training loss: 14230.61, average training loss: 18953.18, base loss: 23514.93
[INFO 2017-06-25 16:29:02,033 main.py:50] epoch 1660, training loss: 16313.54, average training loss: 18947.71, base loss: 23509.38
[INFO 2017-06-25 16:29:06,332 main.py:50] epoch 1661, training loss: 16030.20, average training loss: 18947.74, base loss: 23511.25
[INFO 2017-06-25 16:29:10,605 main.py:50] epoch 1662, training loss: 16716.41, average training loss: 18942.88, base loss: 23506.14
[INFO 2017-06-25 16:29:14,893 main.py:50] epoch 1663, training loss: 18580.36, average training loss: 18940.22, base loss: 23507.07
[INFO 2017-06-25 16:29:19,145 main.py:50] epoch 1664, training loss: 22365.07, average training loss: 18945.99, base loss: 23515.18
[INFO 2017-06-25 16:29:23,419 main.py:50] epoch 1665, training loss: 17005.92, average training loss: 18947.00, base loss: 23516.98
[INFO 2017-06-25 16:29:27,680 main.py:50] epoch 1666, training loss: 19443.71, average training loss: 18944.31, base loss: 23512.39
[INFO 2017-06-25 16:29:31,967 main.py:50] epoch 1667, training loss: 17867.11, average training loss: 18944.23, base loss: 23512.81
[INFO 2017-06-25 16:29:36,226 main.py:50] epoch 1668, training loss: 19690.77, average training loss: 18938.89, base loss: 23508.10
[INFO 2017-06-25 16:29:40,480 main.py:50] epoch 1669, training loss: 21058.57, average training loss: 18932.48, base loss: 23504.20
[INFO 2017-06-25 16:29:44,774 main.py:50] epoch 1670, training loss: 19861.44, average training loss: 18930.93, base loss: 23503.47
[INFO 2017-06-25 16:29:49,070 main.py:50] epoch 1671, training loss: 18536.74, average training loss: 18932.44, base loss: 23507.61
[INFO 2017-06-25 16:29:53,375 main.py:50] epoch 1672, training loss: 20069.69, average training loss: 18936.72, base loss: 23515.17
[INFO 2017-06-25 16:29:57,653 main.py:50] epoch 1673, training loss: 18387.28, average training loss: 18932.18, base loss: 23511.53
[INFO 2017-06-25 16:30:01,921 main.py:50] epoch 1674, training loss: 19263.19, average training loss: 18930.02, base loss: 23511.25
[INFO 2017-06-25 16:30:06,205 main.py:50] epoch 1675, training loss: 21937.13, average training loss: 18938.83, base loss: 23524.59
[INFO 2017-06-25 16:30:10,476 main.py:50] epoch 1676, training loss: 17502.32, average training loss: 18938.87, base loss: 23523.61
[INFO 2017-06-25 16:30:14,766 main.py:50] epoch 1677, training loss: 19499.21, average training loss: 18933.87, base loss: 23520.87
[INFO 2017-06-25 16:30:19,062 main.py:50] epoch 1678, training loss: 19426.12, average training loss: 18933.86, base loss: 23522.44
[INFO 2017-06-25 16:30:23,343 main.py:50] epoch 1679, training loss: 14991.88, average training loss: 18932.05, base loss: 23523.27
[INFO 2017-06-25 16:30:27,583 main.py:50] epoch 1680, training loss: 21416.28, average training loss: 18934.57, base loss: 23530.35
[INFO 2017-06-25 16:30:31,833 main.py:50] epoch 1681, training loss: 18366.87, average training loss: 18933.70, base loss: 23531.92
[INFO 2017-06-25 16:30:36,106 main.py:50] epoch 1682, training loss: 17890.07, average training loss: 18936.65, base loss: 23538.47
[INFO 2017-06-25 16:30:40,365 main.py:50] epoch 1683, training loss: 17526.49, average training loss: 18937.19, base loss: 23541.71
[INFO 2017-06-25 16:30:44,627 main.py:50] epoch 1684, training loss: 17802.07, average training loss: 18935.03, base loss: 23541.81
[INFO 2017-06-25 16:30:48,914 main.py:50] epoch 1685, training loss: 19600.19, average training loss: 18937.24, base loss: 23545.54
[INFO 2017-06-25 16:30:53,207 main.py:50] epoch 1686, training loss: 15479.84, average training loss: 18935.85, base loss: 23545.23
[INFO 2017-06-25 16:30:57,510 main.py:50] epoch 1687, training loss: 16488.63, average training loss: 18928.31, base loss: 23536.86
[INFO 2017-06-25 16:31:01,793 main.py:50] epoch 1688, training loss: 14950.72, average training loss: 18919.93, base loss: 23527.96
[INFO 2017-06-25 16:31:06,057 main.py:50] epoch 1689, training loss: 17519.19, average training loss: 18922.61, base loss: 23531.45
[INFO 2017-06-25 16:31:10,319 main.py:50] epoch 1690, training loss: 19041.70, average training loss: 18924.30, base loss: 23534.23
[INFO 2017-06-25 16:31:14,590 main.py:50] epoch 1691, training loss: 16140.22, average training loss: 18923.68, base loss: 23533.48
[INFO 2017-06-25 16:31:18,877 main.py:50] epoch 1692, training loss: 16464.96, average training loss: 18925.60, base loss: 23537.88
[INFO 2017-06-25 16:31:23,151 main.py:50] epoch 1693, training loss: 18830.88, average training loss: 18915.42, base loss: 23528.13
[INFO 2017-06-25 16:31:27,399 main.py:50] epoch 1694, training loss: 17941.54, average training loss: 18912.92, base loss: 23526.89
[INFO 2017-06-25 16:31:31,672 main.py:50] epoch 1695, training loss: 18889.22, average training loss: 18915.33, base loss: 23534.33
[INFO 2017-06-25 16:31:35,924 main.py:50] epoch 1696, training loss: 16152.15, average training loss: 18910.12, base loss: 23528.65
[INFO 2017-06-25 16:31:40,254 main.py:50] epoch 1697, training loss: 17651.70, average training loss: 18913.28, base loss: 23537.00
[INFO 2017-06-25 16:31:44,527 main.py:50] epoch 1698, training loss: 20084.78, average training loss: 18916.78, base loss: 23539.80
[INFO 2017-06-25 16:31:48,802 main.py:50] epoch 1699, training loss: 18674.66, average training loss: 18916.73, base loss: 23539.70
[INFO 2017-06-25 16:31:53,067 main.py:50] epoch 1700, training loss: 16781.04, average training loss: 18907.00, base loss: 23527.40
[INFO 2017-06-25 16:31:57,314 main.py:50] epoch 1701, training loss: 18135.67, average training loss: 18905.44, base loss: 23526.80
[INFO 2017-06-25 16:32:01,615 main.py:50] epoch 1702, training loss: 15269.67, average training loss: 18900.01, base loss: 23523.44
[INFO 2017-06-25 16:32:05,899 main.py:50] epoch 1703, training loss: 15909.45, average training loss: 18895.54, base loss: 23519.25
[INFO 2017-06-25 16:32:10,209 main.py:50] epoch 1704, training loss: 21630.81, average training loss: 18899.23, base loss: 23525.70
[INFO 2017-06-25 16:32:14,474 main.py:50] epoch 1705, training loss: 18788.15, average training loss: 18901.96, base loss: 23529.71
[INFO 2017-06-25 16:32:18,763 main.py:50] epoch 1706, training loss: 18378.15, average training loss: 18901.23, base loss: 23528.28
[INFO 2017-06-25 16:32:23,056 main.py:50] epoch 1707, training loss: 18113.20, average training loss: 18896.80, base loss: 23523.12
[INFO 2017-06-25 16:32:27,363 main.py:50] epoch 1708, training loss: 16525.88, average training loss: 18892.50, base loss: 23519.05
[INFO 2017-06-25 16:32:31,655 main.py:50] epoch 1709, training loss: 15902.08, average training loss: 18886.75, base loss: 23516.05
[INFO 2017-06-25 16:32:35,960 main.py:50] epoch 1710, training loss: 17795.83, average training loss: 18885.76, base loss: 23516.86
[INFO 2017-06-25 16:32:40,260 main.py:50] epoch 1711, training loss: 15862.79, average training loss: 18884.48, base loss: 23517.24
[INFO 2017-06-25 16:32:44,535 main.py:50] epoch 1712, training loss: 16047.69, average training loss: 18879.53, base loss: 23511.91
[INFO 2017-06-25 16:32:48,816 main.py:50] epoch 1713, training loss: 18771.14, average training loss: 18883.09, base loss: 23519.39
[INFO 2017-06-25 16:32:53,094 main.py:50] epoch 1714, training loss: 15394.03, average training loss: 18883.86, base loss: 23520.88
[INFO 2017-06-25 16:32:57,405 main.py:50] epoch 1715, training loss: 21154.24, average training loss: 18880.90, base loss: 23522.37
[INFO 2017-06-25 16:33:01,696 main.py:50] epoch 1716, training loss: 18314.56, average training loss: 18878.69, base loss: 23520.99
[INFO 2017-06-25 16:33:05,989 main.py:50] epoch 1717, training loss: 22035.83, average training loss: 18881.28, base loss: 23523.44
[INFO 2017-06-25 16:33:10,295 main.py:50] epoch 1718, training loss: 19023.25, average training loss: 18881.39, base loss: 23525.42
[INFO 2017-06-25 16:33:14,555 main.py:50] epoch 1719, training loss: 17733.91, average training loss: 18874.91, base loss: 23521.28
[INFO 2017-06-25 16:33:18,852 main.py:50] epoch 1720, training loss: 21882.27, average training loss: 18877.13, base loss: 23528.70
[INFO 2017-06-25 16:33:23,108 main.py:50] epoch 1721, training loss: 18556.39, average training loss: 18870.38, base loss: 23524.01
[INFO 2017-06-25 16:33:27,374 main.py:50] epoch 1722, training loss: 17025.76, average training loss: 18868.53, base loss: 23525.48
[INFO 2017-06-25 16:33:31,670 main.py:50] epoch 1723, training loss: 18034.62, average training loss: 18868.46, base loss: 23526.86
[INFO 2017-06-25 16:33:35,964 main.py:50] epoch 1724, training loss: 17258.05, average training loss: 18866.99, base loss: 23528.35
[INFO 2017-06-25 16:33:40,266 main.py:50] epoch 1725, training loss: 20456.74, average training loss: 18864.62, base loss: 23528.43
[INFO 2017-06-25 16:33:44,529 main.py:50] epoch 1726, training loss: 16948.65, average training loss: 18865.08, base loss: 23528.61
[INFO 2017-06-25 16:33:48,831 main.py:50] epoch 1727, training loss: 20488.92, average training loss: 18867.36, base loss: 23533.22
[INFO 2017-06-25 16:33:53,126 main.py:50] epoch 1728, training loss: 17706.60, average training loss: 18866.58, base loss: 23533.55
[INFO 2017-06-25 16:33:57,379 main.py:50] epoch 1729, training loss: 14434.80, average training loss: 18865.80, base loss: 23533.90
[INFO 2017-06-25 16:34:01,662 main.py:50] epoch 1730, training loss: 20789.96, average training loss: 18868.90, base loss: 23536.64
[INFO 2017-06-25 16:34:05,947 main.py:50] epoch 1731, training loss: 18390.66, average training loss: 18870.03, base loss: 23538.91
[INFO 2017-06-25 16:34:10,226 main.py:50] epoch 1732, training loss: 21827.25, average training loss: 18876.26, base loss: 23548.46
[INFO 2017-06-25 16:34:14,517 main.py:50] epoch 1733, training loss: 14164.49, average training loss: 18869.32, base loss: 23540.35
[INFO 2017-06-25 16:34:18,785 main.py:50] epoch 1734, training loss: 21715.79, average training loss: 18872.55, base loss: 23544.49
[INFO 2017-06-25 16:34:23,061 main.py:50] epoch 1735, training loss: 22898.39, average training loss: 18872.02, base loss: 23546.14
[INFO 2017-06-25 16:34:27,338 main.py:50] epoch 1736, training loss: 21847.08, average training loss: 18868.14, base loss: 23542.35
[INFO 2017-06-25 16:34:31,607 main.py:50] epoch 1737, training loss: 17271.50, average training loss: 18867.23, base loss: 23544.64
[INFO 2017-06-25 16:34:35,895 main.py:50] epoch 1738, training loss: 20613.25, average training loss: 18867.01, base loss: 23543.97
[INFO 2017-06-25 16:34:40,188 main.py:50] epoch 1739, training loss: 21414.39, average training loss: 18865.34, base loss: 23543.49
[INFO 2017-06-25 16:34:44,457 main.py:50] epoch 1740, training loss: 18062.33, average training loss: 18861.06, base loss: 23542.38
[INFO 2017-06-25 16:34:48,726 main.py:50] epoch 1741, training loss: 22483.12, average training loss: 18865.17, base loss: 23552.09
[INFO 2017-06-25 16:34:53,018 main.py:50] epoch 1742, training loss: 16487.73, average training loss: 18857.64, base loss: 23541.51
[INFO 2017-06-25 16:34:57,302 main.py:50] epoch 1743, training loss: 18474.29, average training loss: 18858.51, base loss: 23540.60
[INFO 2017-06-25 16:35:01,576 main.py:50] epoch 1744, training loss: 18236.88, average training loss: 18856.32, base loss: 23539.06
[INFO 2017-06-25 16:35:05,860 main.py:50] epoch 1745, training loss: 19917.88, average training loss: 18852.36, base loss: 23534.72
[INFO 2017-06-25 16:35:10,133 main.py:50] epoch 1746, training loss: 18920.72, average training loss: 18852.41, base loss: 23535.53
[INFO 2017-06-25 16:35:14,395 main.py:50] epoch 1747, training loss: 17361.59, average training loss: 18842.61, base loss: 23524.14
[INFO 2017-06-25 16:35:18,685 main.py:50] epoch 1748, training loss: 15039.28, average training loss: 18835.86, base loss: 23516.56
[INFO 2017-06-25 16:35:22,989 main.py:50] epoch 1749, training loss: 15124.44, average training loss: 18833.73, base loss: 23515.96
[INFO 2017-06-25 16:35:27,286 main.py:50] epoch 1750, training loss: 15160.05, average training loss: 18824.56, base loss: 23504.49
[INFO 2017-06-25 16:35:31,546 main.py:50] epoch 1751, training loss: 17126.38, average training loss: 18824.04, base loss: 23505.73
[INFO 2017-06-25 16:35:35,815 main.py:50] epoch 1752, training loss: 17144.31, average training loss: 18824.21, base loss: 23507.22
[INFO 2017-06-25 16:35:40,106 main.py:50] epoch 1753, training loss: 14909.36, average training loss: 18818.47, base loss: 23499.91
[INFO 2017-06-25 16:35:44,359 main.py:50] epoch 1754, training loss: 14843.64, average training loss: 18812.58, base loss: 23493.36
[INFO 2017-06-25 16:35:48,639 main.py:50] epoch 1755, training loss: 17889.86, average training loss: 18811.97, base loss: 23492.89
[INFO 2017-06-25 16:35:52,918 main.py:50] epoch 1756, training loss: 19268.39, average training loss: 18811.15, base loss: 23490.91
[INFO 2017-06-25 16:35:57,242 main.py:50] epoch 1757, training loss: 17445.58, average training loss: 18802.72, base loss: 23481.47
[INFO 2017-06-25 16:36:01,525 main.py:50] epoch 1758, training loss: 16796.29, average training loss: 18797.02, base loss: 23475.47
[INFO 2017-06-25 16:36:05,812 main.py:50] epoch 1759, training loss: 20100.45, average training loss: 18799.24, base loss: 23477.34
[INFO 2017-06-25 16:36:10,120 main.py:50] epoch 1760, training loss: 16263.76, average training loss: 18795.20, base loss: 23476.04
[INFO 2017-06-25 16:36:14,397 main.py:50] epoch 1761, training loss: 19547.41, average training loss: 18796.09, base loss: 23475.62
[INFO 2017-06-25 16:36:18,687 main.py:50] epoch 1762, training loss: 19775.68, average training loss: 18798.25, base loss: 23479.06
[INFO 2017-06-25 16:36:22,971 main.py:50] epoch 1763, training loss: 17438.81, average training loss: 18799.95, base loss: 23482.63
[INFO 2017-06-25 16:36:27,250 main.py:50] epoch 1764, training loss: 16001.35, average training loss: 18797.11, base loss: 23482.36
[INFO 2017-06-25 16:36:31,549 main.py:50] epoch 1765, training loss: 18852.76, average training loss: 18799.76, base loss: 23486.46
[INFO 2017-06-25 16:36:35,824 main.py:50] epoch 1766, training loss: 15661.30, average training loss: 18796.82, base loss: 23484.28
[INFO 2017-06-25 16:36:40,121 main.py:50] epoch 1767, training loss: 22424.06, average training loss: 18801.09, base loss: 23489.03
[INFO 2017-06-25 16:36:44,394 main.py:50] epoch 1768, training loss: 15044.39, average training loss: 18797.63, base loss: 23484.92
[INFO 2017-06-25 16:36:48,663 main.py:50] epoch 1769, training loss: 17352.75, average training loss: 18799.15, base loss: 23489.01
[INFO 2017-06-25 16:36:52,935 main.py:50] epoch 1770, training loss: 15526.52, average training loss: 18793.28, base loss: 23484.71
[INFO 2017-06-25 16:36:57,229 main.py:50] epoch 1771, training loss: 27438.32, average training loss: 18804.68, base loss: 23497.26
[INFO 2017-06-25 16:37:01,480 main.py:50] epoch 1772, training loss: 15818.74, average training loss: 18801.46, base loss: 23495.79
[INFO 2017-06-25 16:37:05,789 main.py:50] epoch 1773, training loss: 19993.16, average training loss: 18800.76, base loss: 23500.69
[INFO 2017-06-25 16:37:10,092 main.py:50] epoch 1774, training loss: 17407.17, average training loss: 18799.49, base loss: 23499.22
[INFO 2017-06-25 16:37:14,374 main.py:50] epoch 1775, training loss: 19829.60, average training loss: 18794.58, base loss: 23494.54
[INFO 2017-06-25 16:37:18,645 main.py:50] epoch 1776, training loss: 16040.76, average training loss: 18796.07, base loss: 23497.68
[INFO 2017-06-25 16:37:22,909 main.py:50] epoch 1777, training loss: 13994.85, average training loss: 18792.92, base loss: 23495.74
[INFO 2017-06-25 16:37:27,168 main.py:50] epoch 1778, training loss: 15502.07, average training loss: 18791.59, base loss: 23496.01
[INFO 2017-06-25 16:37:31,424 main.py:50] epoch 1779, training loss: 22611.33, average training loss: 18791.07, base loss: 23497.24
[INFO 2017-06-25 16:37:35,729 main.py:50] epoch 1780, training loss: 17478.20, average training loss: 18790.67, base loss: 23498.61
[INFO 2017-06-25 16:37:40,000 main.py:50] epoch 1781, training loss: 17493.35, average training loss: 18788.71, base loss: 23496.79
[INFO 2017-06-25 16:37:44,259 main.py:50] epoch 1782, training loss: 14478.04, average training loss: 18780.77, base loss: 23487.76
[INFO 2017-06-25 16:37:48,512 main.py:50] epoch 1783, training loss: 19045.48, average training loss: 18779.76, base loss: 23488.43
[INFO 2017-06-25 16:37:52,795 main.py:50] epoch 1784, training loss: 17599.89, average training loss: 18772.53, base loss: 23480.58
[INFO 2017-06-25 16:37:57,059 main.py:50] epoch 1785, training loss: 19162.11, average training loss: 18771.22, base loss: 23482.74
[INFO 2017-06-25 16:38:01,319 main.py:50] epoch 1786, training loss: 20010.89, average training loss: 18771.58, base loss: 23485.36
[INFO 2017-06-25 16:38:05,590 main.py:50] epoch 1787, training loss: 19525.16, average training loss: 18772.22, base loss: 23487.23
[INFO 2017-06-25 16:38:09,861 main.py:50] epoch 1788, training loss: 17382.46, average training loss: 18772.32, base loss: 23490.20
[INFO 2017-06-25 16:38:14,138 main.py:50] epoch 1789, training loss: 19144.43, average training loss: 18768.42, base loss: 23487.02
[INFO 2017-06-25 16:38:18,405 main.py:50] epoch 1790, training loss: 20882.20, average training loss: 18770.32, base loss: 23492.03
[INFO 2017-06-25 16:38:22,652 main.py:50] epoch 1791, training loss: 17219.89, average training loss: 18762.13, base loss: 23482.75
[INFO 2017-06-25 16:38:26,915 main.py:50] epoch 1792, training loss: 18677.28, average training loss: 18763.30, base loss: 23484.93
[INFO 2017-06-25 16:38:31,189 main.py:50] epoch 1793, training loss: 19579.96, average training loss: 18765.00, base loss: 23491.21
[INFO 2017-06-25 16:38:35,458 main.py:50] epoch 1794, training loss: 15901.19, average training loss: 18761.90, base loss: 23489.58
[INFO 2017-06-25 16:38:39,727 main.py:50] epoch 1795, training loss: 15998.60, average training loss: 18757.46, base loss: 23487.25
[INFO 2017-06-25 16:38:44,023 main.py:50] epoch 1796, training loss: 16033.00, average training loss: 18753.56, base loss: 23486.34
[INFO 2017-06-25 16:38:48,282 main.py:50] epoch 1797, training loss: 17757.09, average training loss: 18754.98, base loss: 23489.97
[INFO 2017-06-25 16:38:52,545 main.py:50] epoch 1798, training loss: 17768.09, average training loss: 18753.31, base loss: 23489.72
[INFO 2017-06-25 16:38:56,808 main.py:50] epoch 1799, training loss: 16398.85, average training loss: 18744.10, base loss: 23481.99
[INFO 2017-06-25 16:39:01,077 main.py:50] epoch 1800, training loss: 21825.22, average training loss: 18751.23, base loss: 23492.10
[INFO 2017-06-25 16:39:05,372 main.py:50] epoch 1801, training loss: 18168.68, average training loss: 18749.19, base loss: 23488.05
[INFO 2017-06-25 16:39:09,633 main.py:50] epoch 1802, training loss: 21286.91, average training loss: 18755.90, base loss: 23496.36
[INFO 2017-06-25 16:39:13,909 main.py:50] epoch 1803, training loss: 14765.03, average training loss: 18750.42, base loss: 23491.49
[INFO 2017-06-25 16:39:18,192 main.py:50] epoch 1804, training loss: 20544.15, average training loss: 18753.72, base loss: 23494.28
[INFO 2017-06-25 16:39:22,475 main.py:50] epoch 1805, training loss: 15620.29, average training loss: 18750.23, base loss: 23492.09
[INFO 2017-06-25 16:39:26,776 main.py:50] epoch 1806, training loss: 16665.41, average training loss: 18749.34, base loss: 23492.64
[INFO 2017-06-25 16:39:31,093 main.py:50] epoch 1807, training loss: 17347.72, average training loss: 18748.01, base loss: 23489.76
[INFO 2017-06-25 16:39:35,383 main.py:50] epoch 1808, training loss: 20463.69, average training loss: 18748.61, base loss: 23492.25
[INFO 2017-06-25 16:39:39,653 main.py:50] epoch 1809, training loss: 16612.39, average training loss: 18748.02, base loss: 23494.49
[INFO 2017-06-25 16:39:43,957 main.py:50] epoch 1810, training loss: 20817.35, average training loss: 18755.16, base loss: 23501.81
[INFO 2017-06-25 16:39:48,246 main.py:50] epoch 1811, training loss: 16415.75, average training loss: 18752.25, base loss: 23499.53
[INFO 2017-06-25 16:39:52,562 main.py:50] epoch 1812, training loss: 16527.29, average training loss: 18749.62, base loss: 23499.24
[INFO 2017-06-25 16:39:56,842 main.py:50] epoch 1813, training loss: 19967.12, average training loss: 18751.23, base loss: 23502.74
[INFO 2017-06-25 16:40:01,118 main.py:50] epoch 1814, training loss: 17090.47, average training loss: 18745.25, base loss: 23496.23
[INFO 2017-06-25 16:40:05,397 main.py:50] epoch 1815, training loss: 14819.45, average training loss: 18742.86, base loss: 23492.67
[INFO 2017-06-25 16:40:09,641 main.py:50] epoch 1816, training loss: 21760.73, average training loss: 18743.51, base loss: 23492.37
[INFO 2017-06-25 16:40:13,940 main.py:50] epoch 1817, training loss: 23288.90, average training loss: 18747.40, base loss: 23498.03
[INFO 2017-06-25 16:40:18,256 main.py:50] epoch 1818, training loss: 15213.70, average training loss: 18743.75, base loss: 23493.63
[INFO 2017-06-25 16:40:22,524 main.py:50] epoch 1819, training loss: 16658.12, average training loss: 18741.51, base loss: 23491.22
[INFO 2017-06-25 16:40:26,811 main.py:50] epoch 1820, training loss: 18077.82, average training loss: 18741.93, base loss: 23493.94
[INFO 2017-06-25 16:40:31,084 main.py:50] epoch 1821, training loss: 22099.43, average training loss: 18742.72, base loss: 23497.66
[INFO 2017-06-25 16:40:35,354 main.py:50] epoch 1822, training loss: 18057.06, average training loss: 18739.01, base loss: 23493.81
[INFO 2017-06-25 16:40:39,605 main.py:50] epoch 1823, training loss: 20850.05, average training loss: 18738.17, base loss: 23492.93
[INFO 2017-06-25 16:40:43,875 main.py:50] epoch 1824, training loss: 19754.77, average training loss: 18740.64, base loss: 23497.80
[INFO 2017-06-25 16:40:48,132 main.py:50] epoch 1825, training loss: 17434.33, average training loss: 18737.51, base loss: 23493.81
[INFO 2017-06-25 16:40:52,413 main.py:50] epoch 1826, training loss: 15706.70, average training loss: 18732.59, base loss: 23485.84
[INFO 2017-06-25 16:40:56,696 main.py:50] epoch 1827, training loss: 17932.79, average training loss: 18733.73, base loss: 23490.95
[INFO 2017-06-25 16:41:00,982 main.py:50] epoch 1828, training loss: 21453.67, average training loss: 18737.48, base loss: 23495.74
[INFO 2017-06-25 16:41:05,242 main.py:50] epoch 1829, training loss: 15515.66, average training loss: 18737.28, base loss: 23496.03
[INFO 2017-06-25 16:41:09,513 main.py:50] epoch 1830, training loss: 17366.75, average training loss: 18734.17, base loss: 23491.79
[INFO 2017-06-25 16:41:13,792 main.py:50] epoch 1831, training loss: 16138.35, average training loss: 18732.47, base loss: 23490.18
[INFO 2017-06-25 16:41:18,065 main.py:50] epoch 1832, training loss: 20392.31, average training loss: 18731.90, base loss: 23489.64
[INFO 2017-06-25 16:41:22,349 main.py:50] epoch 1833, training loss: 19818.85, average training loss: 18728.20, base loss: 23490.60
[INFO 2017-06-25 16:41:26,658 main.py:50] epoch 1834, training loss: 19089.59, average training loss: 18727.89, base loss: 23493.03
[INFO 2017-06-25 16:41:30,925 main.py:50] epoch 1835, training loss: 16987.36, average training loss: 18723.33, base loss: 23487.84
[INFO 2017-06-25 16:41:35,197 main.py:50] epoch 1836, training loss: 14656.22, average training loss: 18719.70, base loss: 23484.41
[INFO 2017-06-25 16:41:39,495 main.py:50] epoch 1837, training loss: 19292.43, average training loss: 18716.81, base loss: 23482.03
[INFO 2017-06-25 16:41:43,767 main.py:50] epoch 1838, training loss: 18673.21, average training loss: 18713.82, base loss: 23480.30
[INFO 2017-06-25 16:41:48,035 main.py:50] epoch 1839, training loss: 17378.83, average training loss: 18714.29, base loss: 23480.35
[INFO 2017-06-25 16:41:52,324 main.py:50] epoch 1840, training loss: 16773.88, average training loss: 18708.59, base loss: 23474.27
[INFO 2017-06-25 16:41:56,599 main.py:50] epoch 1841, training loss: 16693.34, average training loss: 18706.61, base loss: 23473.65
[INFO 2017-06-25 16:42:00,855 main.py:50] epoch 1842, training loss: 15083.78, average training loss: 18705.81, base loss: 23475.32
[INFO 2017-06-25 16:42:05,105 main.py:50] epoch 1843, training loss: 18525.84, average training loss: 18706.82, base loss: 23475.41
[INFO 2017-06-25 16:42:09,405 main.py:50] epoch 1844, training loss: 19108.08, average training loss: 18706.48, base loss: 23474.55
[INFO 2017-06-25 16:42:13,662 main.py:50] epoch 1845, training loss: 19533.74, average training loss: 18709.64, base loss: 23479.33
[INFO 2017-06-25 16:42:17,929 main.py:50] epoch 1846, training loss: 18054.84, average training loss: 18706.34, base loss: 23474.45
[INFO 2017-06-25 16:42:22,197 main.py:50] epoch 1847, training loss: 18996.59, average training loss: 18703.81, base loss: 23474.16
[INFO 2017-06-25 16:42:26,476 main.py:50] epoch 1848, training loss: 15021.25, average training loss: 18700.35, base loss: 23472.09
[INFO 2017-06-25 16:42:30,739 main.py:50] epoch 1849, training loss: 16724.05, average training loss: 18696.91, base loss: 23471.08
[INFO 2017-06-25 16:42:35,014 main.py:50] epoch 1850, training loss: 16937.05, average training loss: 18694.39, base loss: 23470.75
[INFO 2017-06-25 16:42:39,305 main.py:50] epoch 1851, training loss: 20085.82, average training loss: 18695.77, base loss: 23476.93
[INFO 2017-06-25 16:42:43,585 main.py:50] epoch 1852, training loss: 20886.39, average training loss: 18695.77, base loss: 23478.50
[INFO 2017-06-25 16:42:47,841 main.py:50] epoch 1853, training loss: 23783.84, average training loss: 18698.41, base loss: 23482.94
[INFO 2017-06-25 16:42:52,141 main.py:50] epoch 1854, training loss: 14830.23, average training loss: 18694.62, base loss: 23476.80
[INFO 2017-06-25 16:42:56,387 main.py:50] epoch 1855, training loss: 19093.93, average training loss: 18694.97, base loss: 23478.31
[INFO 2017-06-25 16:43:00,633 main.py:50] epoch 1856, training loss: 16952.26, average training loss: 18691.57, base loss: 23477.41
[INFO 2017-06-25 16:43:04,909 main.py:50] epoch 1857, training loss: 21688.15, average training loss: 18689.62, base loss: 23475.95
[INFO 2017-06-25 16:43:09,177 main.py:50] epoch 1858, training loss: 16721.17, average training loss: 18688.42, base loss: 23477.41
[INFO 2017-06-25 16:43:13,462 main.py:50] epoch 1859, training loss: 20012.71, average training loss: 18694.79, base loss: 23487.66
[INFO 2017-06-25 16:43:17,722 main.py:50] epoch 1860, training loss: 19186.36, average training loss: 18695.50, base loss: 23487.97
[INFO 2017-06-25 16:43:22,005 main.py:50] epoch 1861, training loss: 17606.06, average training loss: 18692.97, base loss: 23484.61
[INFO 2017-06-25 16:43:26,284 main.py:50] epoch 1862, training loss: 21891.59, average training loss: 18695.26, base loss: 23486.57
[INFO 2017-06-25 16:43:30,585 main.py:50] epoch 1863, training loss: 19092.77, average training loss: 18699.49, base loss: 23491.07
[INFO 2017-06-25 16:43:34,875 main.py:50] epoch 1864, training loss: 14952.75, average training loss: 18694.02, base loss: 23484.09
[INFO 2017-06-25 16:43:39,127 main.py:50] epoch 1865, training loss: 19543.72, average training loss: 18697.09, base loss: 23489.77
[INFO 2017-06-25 16:43:43,392 main.py:50] epoch 1866, training loss: 18526.97, average training loss: 18699.57, base loss: 23495.62
[INFO 2017-06-25 16:43:47,650 main.py:50] epoch 1867, training loss: 14220.23, average training loss: 18698.65, base loss: 23495.15
[INFO 2017-06-25 16:43:51,919 main.py:50] epoch 1868, training loss: 16713.57, average training loss: 18698.40, base loss: 23496.36
[INFO 2017-06-25 16:43:56,229 main.py:50] epoch 1869, training loss: 22983.04, average training loss: 18705.30, base loss: 23507.32
[INFO 2017-06-25 16:44:00,484 main.py:50] epoch 1870, training loss: 18638.98, average training loss: 18709.36, base loss: 23514.92
[INFO 2017-06-25 16:44:04,776 main.py:50] epoch 1871, training loss: 16804.07, average training loss: 18706.46, base loss: 23511.40
[INFO 2017-06-25 16:44:09,051 main.py:50] epoch 1872, training loss: 17550.82, average training loss: 18704.11, base loss: 23510.50
[INFO 2017-06-25 16:44:13,330 main.py:50] epoch 1873, training loss: 16350.80, average training loss: 18700.30, base loss: 23505.68
[INFO 2017-06-25 16:44:17,607 main.py:50] epoch 1874, training loss: 21576.50, average training loss: 18706.14, base loss: 23512.94
[INFO 2017-06-25 16:44:21,905 main.py:50] epoch 1875, training loss: 19745.03, average training loss: 18702.05, base loss: 23511.43
[INFO 2017-06-25 16:44:26,173 main.py:50] epoch 1876, training loss: 19095.38, average training loss: 18703.13, base loss: 23514.63
[INFO 2017-06-25 16:44:30,451 main.py:50] epoch 1877, training loss: 15940.42, average training loss: 18696.73, base loss: 23507.29
[INFO 2017-06-25 16:44:34,714 main.py:50] epoch 1878, training loss: 15428.32, average training loss: 18694.69, base loss: 23506.88
[INFO 2017-06-25 16:44:38,989 main.py:50] epoch 1879, training loss: 20037.50, average training loss: 18694.95, base loss: 23510.41
[INFO 2017-06-25 16:44:43,278 main.py:50] epoch 1880, training loss: 16315.36, average training loss: 18690.62, base loss: 23507.69
[INFO 2017-06-25 16:44:47,558 main.py:50] epoch 1881, training loss: 16007.53, average training loss: 18688.23, base loss: 23505.00
[INFO 2017-06-25 16:44:51,822 main.py:50] epoch 1882, training loss: 15924.49, average training loss: 18682.95, base loss: 23501.48
[INFO 2017-06-25 16:44:56,112 main.py:50] epoch 1883, training loss: 19171.35, average training loss: 18683.05, base loss: 23504.29
[INFO 2017-06-25 16:45:00,367 main.py:50] epoch 1884, training loss: 17071.00, average training loss: 18676.24, base loss: 23496.21
[INFO 2017-06-25 16:45:04,639 main.py:50] epoch 1885, training loss: 16224.39, average training loss: 18677.61, base loss: 23498.41
[INFO 2017-06-25 16:45:08,944 main.py:50] epoch 1886, training loss: 21921.73, average training loss: 18679.56, base loss: 23502.01
[INFO 2017-06-25 16:45:13,229 main.py:50] epoch 1887, training loss: 20461.82, average training loss: 18675.38, base loss: 23498.97
[INFO 2017-06-25 16:45:17,508 main.py:50] epoch 1888, training loss: 16977.96, average training loss: 18673.48, base loss: 23497.69
[INFO 2017-06-25 16:45:21,810 main.py:50] epoch 1889, training loss: 18406.26, average training loss: 18672.91, base loss: 23497.55
[INFO 2017-06-25 16:45:26,103 main.py:50] epoch 1890, training loss: 18843.98, average training loss: 18675.96, base loss: 23503.38
[INFO 2017-06-25 16:45:30,361 main.py:50] epoch 1891, training loss: 13779.26, average training loss: 18672.32, base loss: 23499.34
[INFO 2017-06-25 16:45:34,605 main.py:50] epoch 1892, training loss: 15371.35, average training loss: 18670.12, base loss: 23499.38
[INFO 2017-06-25 16:45:38,913 main.py:50] epoch 1893, training loss: 24029.40, average training loss: 18678.49, base loss: 23506.85
[INFO 2017-06-25 16:45:43,190 main.py:50] epoch 1894, training loss: 16723.13, average training loss: 18678.09, base loss: 23507.59
[INFO 2017-06-25 16:45:47,504 main.py:50] epoch 1895, training loss: 18370.18, average training loss: 18677.35, base loss: 23509.08
[INFO 2017-06-25 16:45:51,754 main.py:50] epoch 1896, training loss: 16879.59, average training loss: 18677.85, base loss: 23511.78
[INFO 2017-06-25 16:45:56,025 main.py:50] epoch 1897, training loss: 16516.22, average training loss: 18675.07, base loss: 23508.81
[INFO 2017-06-25 16:46:00,312 main.py:50] epoch 1898, training loss: 16323.98, average training loss: 18676.21, base loss: 23511.47
[INFO 2017-06-25 16:46:04,571 main.py:50] epoch 1899, training loss: 17210.54, average training loss: 18668.08, base loss: 23502.29
[INFO 2017-06-25 16:46:08,876 main.py:50] epoch 1900, training loss: 17276.28, average training loss: 18668.11, base loss: 23503.40
[INFO 2017-06-25 16:46:13,150 main.py:50] epoch 1901, training loss: 18604.71, average training loss: 18671.11, base loss: 23509.14
[INFO 2017-06-25 16:46:17,418 main.py:50] epoch 1902, training loss: 17649.74, average training loss: 18667.01, base loss: 23505.61
[INFO 2017-06-25 16:46:21,706 main.py:50] epoch 1903, training loss: 18590.41, average training loss: 18664.51, base loss: 23504.42
[INFO 2017-06-25 16:46:25,984 main.py:50] epoch 1904, training loss: 19737.51, average training loss: 18665.25, base loss: 23507.77
[INFO 2017-06-25 16:46:30,265 main.py:50] epoch 1905, training loss: 19430.64, average training loss: 18665.68, base loss: 23509.75
[INFO 2017-06-25 16:46:34,528 main.py:50] epoch 1906, training loss: 16784.31, average training loss: 18662.85, base loss: 23505.35
[INFO 2017-06-25 16:46:38,812 main.py:50] epoch 1907, training loss: 20825.92, average training loss: 18667.26, base loss: 23512.69
[INFO 2017-06-25 16:46:43,081 main.py:50] epoch 1908, training loss: 15159.98, average training loss: 18661.71, base loss: 23508.90
[INFO 2017-06-25 16:46:47,370 main.py:50] epoch 1909, training loss: 22137.67, average training loss: 18662.41, base loss: 23510.38
[INFO 2017-06-25 16:46:51,640 main.py:50] epoch 1910, training loss: 17652.63, average training loss: 18662.77, base loss: 23510.72
[INFO 2017-06-25 16:46:55,902 main.py:50] epoch 1911, training loss: 18070.79, average training loss: 18660.55, base loss: 23506.11
[INFO 2017-06-25 16:47:00,189 main.py:50] epoch 1912, training loss: 17781.41, average training loss: 18661.39, base loss: 23508.36
[INFO 2017-06-25 16:47:04,482 main.py:50] epoch 1913, training loss: 19806.19, average training loss: 18665.74, base loss: 23512.96
[INFO 2017-06-25 16:47:08,767 main.py:50] epoch 1914, training loss: 17333.71, average training loss: 18661.67, base loss: 23509.28
[INFO 2017-06-25 16:47:13,044 main.py:50] epoch 1915, training loss: 14369.99, average training loss: 18655.08, base loss: 23502.30
[INFO 2017-06-25 16:47:17,347 main.py:50] epoch 1916, training loss: 14152.18, average training loss: 18651.18, base loss: 23499.08
[INFO 2017-06-25 16:47:21,620 main.py:50] epoch 1917, training loss: 20797.85, average training loss: 18649.82, base loss: 23497.89
[INFO 2017-06-25 16:47:25,910 main.py:50] epoch 1918, training loss: 18031.91, average training loss: 18648.17, base loss: 23494.51
[INFO 2017-06-25 16:47:30,218 main.py:50] epoch 1919, training loss: 17785.62, average training loss: 18647.17, base loss: 23494.88
[INFO 2017-06-25 16:47:34,487 main.py:50] epoch 1920, training loss: 15051.51, average training loss: 18645.46, base loss: 23494.67
[INFO 2017-06-25 16:47:38,786 main.py:50] epoch 1921, training loss: 16893.54, average training loss: 18644.37, base loss: 23494.68
[INFO 2017-06-25 16:47:43,056 main.py:50] epoch 1922, training loss: 19597.39, average training loss: 18647.93, base loss: 23498.33
[INFO 2017-06-25 16:47:47,322 main.py:50] epoch 1923, training loss: 20010.50, average training loss: 18652.76, base loss: 23505.70
[INFO 2017-06-25 16:47:51,614 main.py:50] epoch 1924, training loss: 21714.81, average training loss: 18650.93, base loss: 23504.02
[INFO 2017-06-25 16:47:55,911 main.py:50] epoch 1925, training loss: 19346.70, average training loss: 18651.25, base loss: 23506.58
[INFO 2017-06-25 16:48:00,203 main.py:50] epoch 1926, training loss: 18055.67, average training loss: 18650.59, base loss: 23505.25
[INFO 2017-06-25 16:48:04,500 main.py:50] epoch 1927, training loss: 17690.08, average training loss: 18644.88, base loss: 23497.51
[INFO 2017-06-25 16:48:08,753 main.py:50] epoch 1928, training loss: 16635.57, average training loss: 18641.85, base loss: 23493.86
[INFO 2017-06-25 16:48:13,038 main.py:50] epoch 1929, training loss: 15519.09, average training loss: 18636.87, base loss: 23487.69
[INFO 2017-06-25 16:48:17,327 main.py:50] epoch 1930, training loss: 15136.72, average training loss: 18631.89, base loss: 23481.86
[INFO 2017-06-25 16:48:21,607 main.py:50] epoch 1931, training loss: 18253.23, average training loss: 18629.81, base loss: 23482.43
[INFO 2017-06-25 16:48:25,884 main.py:50] epoch 1932, training loss: 16183.22, average training loss: 18627.26, base loss: 23481.65
[INFO 2017-06-25 16:48:30,174 main.py:50] epoch 1933, training loss: 19223.99, average training loss: 18629.15, base loss: 23485.81
[INFO 2017-06-25 16:48:34,453 main.py:50] epoch 1934, training loss: 18618.87, average training loss: 18628.46, base loss: 23483.19
[INFO 2017-06-25 16:48:38,727 main.py:50] epoch 1935, training loss: 24969.81, average training loss: 18632.38, base loss: 23490.73
[INFO 2017-06-25 16:48:43,003 main.py:50] epoch 1936, training loss: 16187.25, average training loss: 18627.85, base loss: 23484.70
[INFO 2017-06-25 16:48:47,268 main.py:50] epoch 1937, training loss: 17351.86, average training loss: 18625.02, base loss: 23478.85
[INFO 2017-06-25 16:48:51,559 main.py:50] epoch 1938, training loss: 14390.90, average training loss: 18620.39, base loss: 23472.36
[INFO 2017-06-25 16:48:55,840 main.py:50] epoch 1939, training loss: 18083.70, average training loss: 18618.82, base loss: 23470.21
[INFO 2017-06-25 16:49:00,122 main.py:50] epoch 1940, training loss: 14519.63, average training loss: 18619.15, base loss: 23472.38
[INFO 2017-06-25 16:49:04,434 main.py:50] epoch 1941, training loss: 19119.21, average training loss: 18615.33, base loss: 23470.68
[INFO 2017-06-25 16:49:08,722 main.py:50] epoch 1942, training loss: 14376.71, average training loss: 18610.69, base loss: 23465.12
[INFO 2017-06-25 16:49:12,983 main.py:50] epoch 1943, training loss: 18100.55, average training loss: 18608.82, base loss: 23462.26
[INFO 2017-06-25 16:49:17,261 main.py:50] epoch 1944, training loss: 16440.40, average training loss: 18604.28, base loss: 23457.60
[INFO 2017-06-25 16:49:21,542 main.py:50] epoch 1945, training loss: 23818.56, average training loss: 18605.06, base loss: 23459.80
[INFO 2017-06-25 16:49:25,814 main.py:50] epoch 1946, training loss: 16087.17, average training loss: 18600.46, base loss: 23454.16
[INFO 2017-06-25 16:49:30,079 main.py:50] epoch 1947, training loss: 15907.37, average training loss: 18598.20, base loss: 23449.35
[INFO 2017-06-25 16:49:34,357 main.py:50] epoch 1948, training loss: 20637.00, average training loss: 18595.05, base loss: 23446.47
[INFO 2017-06-25 16:49:38,667 main.py:50] epoch 1949, training loss: 19494.74, average training loss: 18591.45, base loss: 23441.50
[INFO 2017-06-25 16:49:42,944 main.py:50] epoch 1950, training loss: 16120.07, average training loss: 18593.35, base loss: 23443.38
[INFO 2017-06-25 16:49:47,245 main.py:50] epoch 1951, training loss: 24083.96, average training loss: 18599.88, base loss: 23452.27
[INFO 2017-06-25 16:49:51,504 main.py:50] epoch 1952, training loss: 16615.80, average training loss: 18598.12, base loss: 23450.68
[INFO 2017-06-25 16:49:55,778 main.py:50] epoch 1953, training loss: 18301.16, average training loss: 18600.85, base loss: 23455.29
[INFO 2017-06-25 16:50:00,089 main.py:50] epoch 1954, training loss: 18649.69, average training loss: 18595.50, base loss: 23449.88
[INFO 2017-06-25 16:50:04,375 main.py:50] epoch 1955, training loss: 18940.58, average training loss: 18594.30, base loss: 23451.59
[INFO 2017-06-25 16:50:08,668 main.py:50] epoch 1956, training loss: 16682.86, average training loss: 18586.82, base loss: 23444.32
[INFO 2017-06-25 16:50:12,955 main.py:50] epoch 1957, training loss: 18267.26, average training loss: 18587.88, base loss: 23443.71
[INFO 2017-06-25 16:50:17,242 main.py:50] epoch 1958, training loss: 21402.40, average training loss: 18593.36, base loss: 23452.86
[INFO 2017-06-25 16:50:21,541 main.py:50] epoch 1959, training loss: 17372.43, average training loss: 18590.95, base loss: 23451.92
[INFO 2017-06-25 16:50:25,837 main.py:50] epoch 1960, training loss: 18459.19, average training loss: 18592.17, base loss: 23453.65
[INFO 2017-06-25 16:50:30,124 main.py:50] epoch 1961, training loss: 18099.70, average training loss: 18590.93, base loss: 23453.19
[INFO 2017-06-25 16:50:34,427 main.py:50] epoch 1962, training loss: 21646.46, average training loss: 18599.17, base loss: 23462.71
[INFO 2017-06-25 16:50:38,708 main.py:50] epoch 1963, training loss: 16991.43, average training loss: 18595.43, base loss: 23462.18
[INFO 2017-06-25 16:50:42,988 main.py:50] epoch 1964, training loss: 18142.29, average training loss: 18593.82, base loss: 23463.03
[INFO 2017-06-25 16:50:47,292 main.py:50] epoch 1965, training loss: 17030.23, average training loss: 18590.69, base loss: 23458.34
[INFO 2017-06-25 16:50:51,564 main.py:50] epoch 1966, training loss: 17267.10, average training loss: 18586.57, base loss: 23454.68
[INFO 2017-06-25 16:50:55,837 main.py:50] epoch 1967, training loss: 16299.36, average training loss: 18585.60, base loss: 23454.49
[INFO 2017-06-25 16:51:00,097 main.py:50] epoch 1968, training loss: 23214.91, average training loss: 18590.30, base loss: 23460.83
[INFO 2017-06-25 16:51:04,403 main.py:50] epoch 1969, training loss: 18520.46, average training loss: 18586.42, base loss: 23458.48
[INFO 2017-06-25 16:51:08,702 main.py:50] epoch 1970, training loss: 17510.38, average training loss: 18588.99, base loss: 23462.25
[INFO 2017-06-25 16:51:13,024 main.py:50] epoch 1971, training loss: 15771.82, average training loss: 18586.20, base loss: 23459.79
[INFO 2017-06-25 16:51:17,313 main.py:50] epoch 1972, training loss: 16139.58, average training loss: 18581.95, base loss: 23453.08
[INFO 2017-06-25 16:51:21,615 main.py:50] epoch 1973, training loss: 14801.52, average training loss: 18576.88, base loss: 23445.51
[INFO 2017-06-25 16:51:25,904 main.py:50] epoch 1974, training loss: 16448.70, average training loss: 18575.61, base loss: 23443.82
[INFO 2017-06-25 16:51:30,180 main.py:50] epoch 1975, training loss: 22319.28, average training loss: 18579.09, base loss: 23447.63
[INFO 2017-06-25 16:51:34,465 main.py:50] epoch 1976, training loss: 17091.10, average training loss: 18577.93, base loss: 23446.28
[INFO 2017-06-25 16:51:38,759 main.py:50] epoch 1977, training loss: 20501.35, average training loss: 18575.98, base loss: 23441.35
[INFO 2017-06-25 16:51:43,026 main.py:50] epoch 1978, training loss: 17899.87, average training loss: 18578.05, base loss: 23443.59
[INFO 2017-06-25 16:51:47,313 main.py:50] epoch 1979, training loss: 21511.25, average training loss: 18575.66, base loss: 23440.45
[INFO 2017-06-25 16:51:51,569 main.py:50] epoch 1980, training loss: 20188.85, average training loss: 18571.78, base loss: 23437.35
[INFO 2017-06-25 16:51:55,852 main.py:50] epoch 1981, training loss: 19866.75, average training loss: 18571.91, base loss: 23439.66
[INFO 2017-06-25 16:52:00,134 main.py:50] epoch 1982, training loss: 16601.83, average training loss: 18570.73, base loss: 23438.01
[INFO 2017-06-25 16:52:04,417 main.py:50] epoch 1983, training loss: 18489.40, average training loss: 18569.63, base loss: 23436.82
[INFO 2017-06-25 16:52:08,674 main.py:50] epoch 1984, training loss: 20931.83, average training loss: 18572.37, base loss: 23440.56
[INFO 2017-06-25 16:52:12,956 main.py:50] epoch 1985, training loss: 18962.79, average training loss: 18567.92, base loss: 23437.59
[INFO 2017-06-25 16:52:17,226 main.py:50] epoch 1986, training loss: 21493.77, average training loss: 18571.94, base loss: 23444.08
[INFO 2017-06-25 16:52:21,521 main.py:50] epoch 1987, training loss: 16392.08, average training loss: 18566.27, base loss: 23436.64
[INFO 2017-06-25 16:52:25,805 main.py:50] epoch 1988, training loss: 16447.09, average training loss: 18562.55, base loss: 23433.94
[INFO 2017-06-25 16:52:30,090 main.py:50] epoch 1989, training loss: 15181.69, average training loss: 18558.52, base loss: 23426.74
[INFO 2017-06-25 16:52:34,354 main.py:50] epoch 1990, training loss: 14967.08, average training loss: 18554.80, base loss: 23423.79
[INFO 2017-06-25 16:52:38,652 main.py:50] epoch 1991, training loss: 20003.32, average training loss: 18553.69, base loss: 23423.15
[INFO 2017-06-25 16:52:42,946 main.py:50] epoch 1992, training loss: 18278.61, average training loss: 18556.05, base loss: 23426.78
[INFO 2017-06-25 16:52:47,240 main.py:50] epoch 1993, training loss: 19188.07, average training loss: 18560.69, base loss: 23431.91
[INFO 2017-06-25 16:52:51,520 main.py:50] epoch 1994, training loss: 22058.74, average training loss: 18565.08, base loss: 23438.27
[INFO 2017-06-25 16:52:55,770 main.py:50] epoch 1995, training loss: 18019.54, average training loss: 18564.27, base loss: 23440.01
[INFO 2017-06-25 16:53:00,039 main.py:50] epoch 1996, training loss: 18663.44, average training loss: 18563.98, base loss: 23438.87
[INFO 2017-06-25 16:53:04,352 main.py:50] epoch 1997, training loss: 19439.30, average training loss: 18563.62, base loss: 23441.96
[INFO 2017-06-25 16:53:08,628 main.py:50] epoch 1998, training loss: 20174.67, average training loss: 18565.31, base loss: 23446.28
[INFO 2017-06-25 16:53:12,906 main.py:50] epoch 1999, training loss: 21022.98, average training loss: 18568.30, base loss: 23452.32
[INFO 2017-06-25 16:53:12,907 main.py:52] epoch 1999, testing
[INFO 2017-06-25 16:56:50,371 main.py:100] average testing loss: 18980.49
[INFO 2017-06-25 16:56:50,374 main.py:72] model save to ./model/final.pth
[INFO 2017-06-25 16:56:50,381 main.py:76] current best accuracy: 18980.49
[INFO 2017-06-25 16:56:54,660 main.py:50] epoch 2000, training loss: 16659.35, average training loss: 18568.32, base loss: 23453.76
[INFO 2017-06-25 16:56:58,950 main.py:50] epoch 2001, training loss: 16618.13, average training loss: 18564.94, base loss: 23448.92
[INFO 2017-06-25 16:57:03,232 main.py:50] epoch 2002, training loss: 15038.12, average training loss: 18559.30, base loss: 23443.29
[INFO 2017-06-25 16:57:07,508 main.py:50] epoch 2003, training loss: 14950.89, average training loss: 18556.48, base loss: 23439.17
[INFO 2017-06-25 16:57:11,814 main.py:50] epoch 2004, training loss: 19744.72, average training loss: 18558.74, base loss: 23443.57
[INFO 2017-06-25 16:57:16,078 main.py:50] epoch 2005, training loss: 12891.78, average training loss: 18554.08, base loss: 23438.94
[INFO 2017-06-25 16:57:20,369 main.py:50] epoch 2006, training loss: 17615.24, average training loss: 18548.60, base loss: 23434.61
[INFO 2017-06-25 16:57:24,626 main.py:50] epoch 2007, training loss: 23347.67, average training loss: 18549.25, base loss: 23435.67
[INFO 2017-06-25 16:57:28,910 main.py:50] epoch 2008, training loss: 19254.51, average training loss: 18549.93, base loss: 23438.10
[INFO 2017-06-25 16:57:33,218 main.py:50] epoch 2009, training loss: 16503.44, average training loss: 18547.67, base loss: 23434.47
[INFO 2017-06-25 16:57:37,525 main.py:50] epoch 2010, training loss: 16677.11, average training loss: 18545.86, base loss: 23436.06
[INFO 2017-06-25 16:57:41,782 main.py:50] epoch 2011, training loss: 15139.41, average training loss: 18539.96, base loss: 23430.88
[INFO 2017-06-25 16:57:46,054 main.py:50] epoch 2012, training loss: 17552.19, average training loss: 18540.63, base loss: 23434.23
[INFO 2017-06-25 16:57:50,334 main.py:50] epoch 2013, training loss: 17191.39, average training loss: 18538.98, base loss: 23434.11
[INFO 2017-06-25 16:57:54,612 main.py:50] epoch 2014, training loss: 17276.58, average training loss: 18536.09, base loss: 23431.41
[INFO 2017-06-25 16:57:58,884 main.py:50] epoch 2015, training loss: 17887.73, average training loss: 18530.76, base loss: 23429.13
[INFO 2017-06-25 16:58:03,163 main.py:50] epoch 2016, training loss: 19814.77, average training loss: 18529.47, base loss: 23428.68
[INFO 2017-06-25 16:58:07,440 main.py:50] epoch 2017, training loss: 20575.48, average training loss: 18529.16, base loss: 23433.52
[INFO 2017-06-25 16:58:11,735 main.py:50] epoch 2018, training loss: 16865.00, average training loss: 18525.08, base loss: 23427.43
[INFO 2017-06-25 16:58:16,013 main.py:50] epoch 2019, training loss: 17527.21, average training loss: 18520.87, base loss: 23423.45
[INFO 2017-06-25 16:58:20,293 main.py:50] epoch 2020, training loss: 18178.24, average training loss: 18523.53, base loss: 23426.09
[INFO 2017-06-25 16:58:24,548 main.py:50] epoch 2021, training loss: 17952.46, average training loss: 18519.39, base loss: 23424.49
[INFO 2017-06-25 16:58:28,806 main.py:50] epoch 2022, training loss: 15966.46, average training loss: 18515.98, base loss: 23421.93
[INFO 2017-06-25 16:58:33,098 main.py:50] epoch 2023, training loss: 17703.91, average training loss: 18512.29, base loss: 23418.44
[INFO 2017-06-25 16:58:37,397 main.py:50] epoch 2024, training loss: 15243.91, average training loss: 18513.28, base loss: 23420.79
[INFO 2017-06-25 16:58:41,675 main.py:50] epoch 2025, training loss: 17832.49, average training loss: 18507.53, base loss: 23414.38
[INFO 2017-06-25 16:58:45,940 main.py:50] epoch 2026, training loss: 20803.30, average training loss: 18508.13, base loss: 23416.04
[INFO 2017-06-25 16:58:50,228 main.py:50] epoch 2027, training loss: 15736.22, average training loss: 18502.79, base loss: 23410.86
[INFO 2017-06-25 16:58:54,526 main.py:50] epoch 2028, training loss: 18277.98, average training loss: 18506.11, base loss: 23416.08
[INFO 2017-06-25 16:58:58,806 main.py:50] epoch 2029, training loss: 20505.21, average training loss: 18506.82, base loss: 23417.13
[INFO 2017-06-25 16:59:03,123 main.py:50] epoch 2030, training loss: 18992.47, average training loss: 18507.26, base loss: 23419.64
[INFO 2017-06-25 16:59:07,430 main.py:50] epoch 2031, training loss: 22010.87, average training loss: 18510.02, base loss: 23423.14
[INFO 2017-06-25 16:59:11,746 main.py:50] epoch 2032, training loss: 20987.79, average training loss: 18512.59, base loss: 23427.65
[INFO 2017-06-25 16:59:16,054 main.py:50] epoch 2033, training loss: 19683.07, average training loss: 18515.03, base loss: 23431.66
[INFO 2017-06-25 16:59:20,372 main.py:50] epoch 2034, training loss: 19638.96, average training loss: 18514.85, base loss: 23431.71
[INFO 2017-06-25 16:59:24,665 main.py:50] epoch 2035, training loss: 22860.92, average training loss: 18520.67, base loss: 23439.42
[INFO 2017-06-25 16:59:28,971 main.py:50] epoch 2036, training loss: 20136.19, average training loss: 18526.58, base loss: 23447.22
[INFO 2017-06-25 16:59:33,272 main.py:50] epoch 2037, training loss: 18384.42, average training loss: 18527.60, base loss: 23451.26
[INFO 2017-06-25 16:59:37,568 main.py:50] epoch 2038, training loss: 13749.45, average training loss: 18522.00, base loss: 23446.28
[INFO 2017-06-25 16:59:41,860 main.py:50] epoch 2039, training loss: 19816.16, average training loss: 18518.88, base loss: 23444.79
[INFO 2017-06-25 16:59:46,139 main.py:50] epoch 2040, training loss: 18895.27, average training loss: 18519.77, base loss: 23447.79
[INFO 2017-06-25 16:59:50,417 main.py:50] epoch 2041, training loss: 16795.30, average training loss: 18513.48, base loss: 23441.69
[INFO 2017-06-25 16:59:54,692 main.py:50] epoch 2042, training loss: 22892.32, average training loss: 18512.97, base loss: 23442.05
[INFO 2017-06-25 16:59:58,980 main.py:50] epoch 2043, training loss: 20859.41, average training loss: 18514.41, base loss: 23443.54
[INFO 2017-06-25 17:00:03,230 main.py:50] epoch 2044, training loss: 18136.65, average training loss: 18512.82, base loss: 23443.29
[INFO 2017-06-25 17:00:07,516 main.py:50] epoch 2045, training loss: 18006.93, average training loss: 18511.67, base loss: 23442.03
[INFO 2017-06-25 17:00:11,781 main.py:50] epoch 2046, training loss: 19758.44, average training loss: 18509.33, base loss: 23438.15
[INFO 2017-06-25 17:00:16,068 main.py:50] epoch 2047, training loss: 20618.35, average training loss: 18511.22, base loss: 23440.33
[INFO 2017-06-25 17:00:20,353 main.py:50] epoch 2048, training loss: 16169.65, average training loss: 18505.28, base loss: 23433.60
[INFO 2017-06-25 17:00:24,629 main.py:50] epoch 2049, training loss: 16494.12, average training loss: 18499.71, base loss: 23427.64
[INFO 2017-06-25 17:00:28,893 main.py:50] epoch 2050, training loss: 18028.47, average training loss: 18499.83, base loss: 23430.27
[INFO 2017-06-25 17:00:33,182 main.py:50] epoch 2051, training loss: 17734.42, average training loss: 18495.84, base loss: 23426.04
[INFO 2017-06-25 17:00:37,464 main.py:50] epoch 2052, training loss: 21259.88, average training loss: 18496.06, base loss: 23426.85
[INFO 2017-06-25 17:00:41,746 main.py:50] epoch 2053, training loss: 20716.89, average training loss: 18501.84, base loss: 23433.16
[INFO 2017-06-25 17:00:46,015 main.py:50] epoch 2054, training loss: 19845.91, average training loss: 18505.37, base loss: 23440.65
[INFO 2017-06-25 17:00:50,293 main.py:50] epoch 2055, training loss: 17800.89, average training loss: 18506.90, base loss: 23444.05
[INFO 2017-06-25 17:00:54,561 main.py:50] epoch 2056, training loss: 23638.55, average training loss: 18514.01, base loss: 23452.21
[INFO 2017-06-25 17:00:58,814 main.py:50] epoch 2057, training loss: 18918.14, average training loss: 18513.96, base loss: 23452.71
[INFO 2017-06-25 17:01:03,088 main.py:50] epoch 2058, training loss: 14367.31, average training loss: 18506.84, base loss: 23440.80
[INFO 2017-06-25 17:01:07,370 main.py:50] epoch 2059, training loss: 22228.95, average training loss: 18511.57, base loss: 23445.78
[INFO 2017-06-25 17:01:11,654 main.py:50] epoch 2060, training loss: 19263.83, average training loss: 18510.94, base loss: 23445.26
[INFO 2017-06-25 17:01:15,942 main.py:50] epoch 2061, training loss: 19486.25, average training loss: 18512.06, base loss: 23448.50
[INFO 2017-06-25 17:01:20,238 main.py:50] epoch 2062, training loss: 20093.42, average training loss: 18514.66, base loss: 23452.27
[INFO 2017-06-25 17:01:24,513 main.py:50] epoch 2063, training loss: 19919.48, average training loss: 18518.76, base loss: 23459.32
[INFO 2017-06-25 17:01:28,763 main.py:50] epoch 2064, training loss: 18691.41, average training loss: 18515.18, base loss: 23455.29
[INFO 2017-06-25 17:01:33,041 main.py:50] epoch 2065, training loss: 16287.55, average training loss: 18508.44, base loss: 23445.74
[INFO 2017-06-25 17:01:37,312 main.py:50] epoch 2066, training loss: 25181.11, average training loss: 18518.01, base loss: 23457.82
[INFO 2017-06-25 17:01:41,577 main.py:50] epoch 2067, training loss: 17178.71, average training loss: 18517.39, base loss: 23457.02
[INFO 2017-06-25 17:01:45,864 main.py:50] epoch 2068, training loss: 22917.19, average training loss: 18518.96, base loss: 23456.91
[INFO 2017-06-25 17:01:50,144 main.py:50] epoch 2069, training loss: 13685.09, average training loss: 18514.34, base loss: 23453.05
[INFO 2017-06-25 17:01:54,448 main.py:50] epoch 2070, training loss: 16975.84, average training loss: 18512.10, base loss: 23450.79
[INFO 2017-06-25 17:01:58,737 main.py:50] epoch 2071, training loss: 18229.07, average training loss: 18513.45, base loss: 23453.05
[INFO 2017-06-25 17:02:03,007 main.py:50] epoch 2072, training loss: 17416.25, average training loss: 18512.96, base loss: 23452.22
[INFO 2017-06-25 17:02:07,288 main.py:50] epoch 2073, training loss: 19260.28, average training loss: 18511.04, base loss: 23449.46
[INFO 2017-06-25 17:02:11,574 main.py:50] epoch 2074, training loss: 27156.57, average training loss: 18519.28, base loss: 23460.11
[INFO 2017-06-25 17:02:15,858 main.py:50] epoch 2075, training loss: 21898.54, average training loss: 18519.59, base loss: 23459.72
[INFO 2017-06-25 17:02:20,141 main.py:50] epoch 2076, training loss: 18655.35, average training loss: 18520.37, base loss: 23460.80
[INFO 2017-06-25 17:02:24,425 main.py:50] epoch 2077, training loss: 18704.44, average training loss: 18521.39, base loss: 23461.84
[INFO 2017-06-25 17:02:28,694 main.py:50] epoch 2078, training loss: 14890.06, average training loss: 18514.06, base loss: 23455.75
[INFO 2017-06-25 17:02:32,975 main.py:50] epoch 2079, training loss: 20060.45, average training loss: 18513.90, base loss: 23454.83
[INFO 2017-06-25 17:02:37,255 main.py:50] epoch 2080, training loss: 22984.72, average training loss: 18518.52, base loss: 23460.19
[INFO 2017-06-25 17:02:41,524 main.py:50] epoch 2081, training loss: 18417.40, average training loss: 18521.30, base loss: 23461.21
[INFO 2017-06-25 17:02:45,790 main.py:50] epoch 2082, training loss: 20850.38, average training loss: 18529.59, base loss: 23474.13
[INFO 2017-06-25 17:02:50,077 main.py:50] epoch 2083, training loss: 22341.48, average training loss: 18527.12, base loss: 23474.80
[INFO 2017-06-25 17:02:54,348 main.py:50] epoch 2084, training loss: 21413.75, average training loss: 18531.13, base loss: 23482.35
[INFO 2017-06-25 17:02:58,639 main.py:50] epoch 2085, training loss: 19297.79, average training loss: 18531.38, base loss: 23485.16
[INFO 2017-06-25 17:03:02,922 main.py:50] epoch 2086, training loss: 16032.65, average training loss: 18525.56, base loss: 23473.35
[INFO 2017-06-25 17:03:07,202 main.py:50] epoch 2087, training loss: 17093.01, average training loss: 18521.78, base loss: 23472.68
[INFO 2017-06-25 17:03:11,474 main.py:50] epoch 2088, training loss: 16433.24, average training loss: 18515.03, base loss: 23465.90
[INFO 2017-06-25 17:03:15,782 main.py:50] epoch 2089, training loss: 22039.91, average training loss: 18511.38, base loss: 23461.59
[INFO 2017-06-25 17:03:20,082 main.py:50] epoch 2090, training loss: 18263.92, average training loss: 18506.57, base loss: 23457.15
[INFO 2017-06-25 17:03:24,345 main.py:50] epoch 2091, training loss: 19604.41, average training loss: 18507.82, base loss: 23460.17
[INFO 2017-06-25 17:03:28,598 main.py:50] epoch 2092, training loss: 18159.34, average training loss: 18510.01, base loss: 23464.88
[INFO 2017-06-25 17:03:32,881 main.py:50] epoch 2093, training loss: 22766.83, average training loss: 18513.76, base loss: 23469.00
[INFO 2017-06-25 17:03:37,163 main.py:50] epoch 2094, training loss: 17499.33, average training loss: 18509.74, base loss: 23465.14
[INFO 2017-06-25 17:03:41,435 main.py:50] epoch 2095, training loss: 22488.42, average training loss: 18511.09, base loss: 23467.01
[INFO 2017-06-25 17:03:45,740 main.py:50] epoch 2096, training loss: 20767.04, average training loss: 18514.82, base loss: 23472.80
[INFO 2017-06-25 17:03:50,026 main.py:50] epoch 2097, training loss: 17794.28, average training loss: 18514.13, base loss: 23474.39
[INFO 2017-06-25 17:03:54,303 main.py:50] epoch 2098, training loss: 15522.92, average training loss: 18508.83, base loss: 23467.95
[INFO 2017-06-25 17:03:58,561 main.py:50] epoch 2099, training loss: 18359.19, average training loss: 18505.95, base loss: 23466.71
[INFO 2017-06-25 17:04:02,844 main.py:50] epoch 2100, training loss: 18627.36, average training loss: 18508.05, base loss: 23472.21
[INFO 2017-06-25 17:04:07,139 main.py:50] epoch 2101, training loss: 16741.47, average training loss: 18507.70, base loss: 23472.47
[INFO 2017-06-25 17:04:11,423 main.py:50] epoch 2102, training loss: 19771.72, average training loss: 18507.95, base loss: 23476.48
[INFO 2017-06-25 17:04:15,693 main.py:50] epoch 2103, training loss: 15910.48, average training loss: 18506.20, base loss: 23476.64
[INFO 2017-06-25 17:04:19,996 main.py:50] epoch 2104, training loss: 16308.32, average training loss: 18504.11, base loss: 23471.97
[INFO 2017-06-25 17:04:24,276 main.py:50] epoch 2105, training loss: 25768.69, average training loss: 18515.20, base loss: 23484.92
[INFO 2017-06-25 17:04:28,562 main.py:50] epoch 2106, training loss: 17057.85, average training loss: 18513.75, base loss: 23483.45
[INFO 2017-06-25 17:04:32,850 main.py:50] epoch 2107, training loss: 14897.56, average training loss: 18510.01, base loss: 23479.17
[INFO 2017-06-25 17:04:37,133 main.py:50] epoch 2108, training loss: 20428.46, average training loss: 18508.80, base loss: 23478.31
[INFO 2017-06-25 17:04:41,414 main.py:50] epoch 2109, training loss: 21578.56, average training loss: 18514.74, base loss: 23487.76
[INFO 2017-06-25 17:04:45,680 main.py:50] epoch 2110, training loss: 16743.87, average training loss: 18508.47, base loss: 23479.87
[INFO 2017-06-25 17:04:49,967 main.py:50] epoch 2111, training loss: 16759.28, average training loss: 18507.85, base loss: 23481.58
[INFO 2017-06-25 17:04:54,263 main.py:50] epoch 2112, training loss: 18051.86, average training loss: 18510.95, base loss: 23486.35
[INFO 2017-06-25 17:04:58,554 main.py:50] epoch 2113, training loss: 19764.23, average training loss: 18511.46, base loss: 23484.82
[INFO 2017-06-25 17:05:02,837 main.py:50] epoch 2114, training loss: 14393.00, average training loss: 18510.34, base loss: 23481.98
[INFO 2017-06-25 17:05:07,151 main.py:50] epoch 2115, training loss: 20152.73, average training loss: 18512.77, base loss: 23483.93
[INFO 2017-06-25 17:05:11,399 main.py:50] epoch 2116, training loss: 18818.01, average training loss: 18512.65, base loss: 23484.00
[INFO 2017-06-25 17:05:15,662 main.py:50] epoch 2117, training loss: 17025.74, average training loss: 18511.17, base loss: 23481.62
[INFO 2017-06-25 17:05:19,961 main.py:50] epoch 2118, training loss: 16946.79, average training loss: 18503.02, base loss: 23471.25
[INFO 2017-06-25 17:05:24,235 main.py:50] epoch 2119, training loss: 14231.79, average training loss: 18500.24, base loss: 23467.78
[INFO 2017-06-25 17:05:28,515 main.py:50] epoch 2120, training loss: 16913.58, average training loss: 18500.38, base loss: 23469.83
[INFO 2017-06-25 17:05:32,774 main.py:50] epoch 2121, training loss: 18846.60, average training loss: 18501.13, base loss: 23473.63
[INFO 2017-06-25 17:05:37,065 main.py:50] epoch 2122, training loss: 22387.53, average training loss: 18505.29, base loss: 23480.28
[INFO 2017-06-25 17:05:41,344 main.py:50] epoch 2123, training loss: 19353.25, average training loss: 18505.16, base loss: 23482.66
[INFO 2017-06-25 17:05:45,596 main.py:50] epoch 2124, training loss: 16209.74, average training loss: 18502.58, base loss: 23481.24
[INFO 2017-06-25 17:05:49,877 main.py:50] epoch 2125, training loss: 24016.23, average training loss: 18504.86, base loss: 23486.72
[INFO 2017-06-25 17:05:54,164 main.py:50] epoch 2126, training loss: 13717.10, average training loss: 18495.99, base loss: 23476.96
[INFO 2017-06-25 17:05:58,427 main.py:50] epoch 2127, training loss: 15833.51, average training loss: 18495.61, base loss: 23474.78
[INFO 2017-06-25 17:06:02,725 main.py:50] epoch 2128, training loss: 20761.99, average training loss: 18498.62, base loss: 23479.32
[INFO 2017-06-25 17:06:07,001 main.py:50] epoch 2129, training loss: 19109.43, average training loss: 18499.50, base loss: 23482.32
[INFO 2017-06-25 17:06:11,299 main.py:50] epoch 2130, training loss: 14617.52, average training loss: 18494.24, base loss: 23474.20
[INFO 2017-06-25 17:06:15,572 main.py:50] epoch 2131, training loss: 19617.83, average training loss: 18498.04, base loss: 23483.46
[INFO 2017-06-25 17:06:19,840 main.py:50] epoch 2132, training loss: 15887.58, average training loss: 18491.99, base loss: 23475.53
[INFO 2017-06-25 17:06:24,145 main.py:50] epoch 2133, training loss: 21718.28, average training loss: 18494.13, base loss: 23476.52
[INFO 2017-06-25 17:06:28,414 main.py:50] epoch 2134, training loss: 24397.97, average training loss: 18501.78, base loss: 23485.95
[INFO 2017-06-25 17:06:32,690 main.py:50] epoch 2135, training loss: 22911.40, average training loss: 18504.19, base loss: 23488.58
[INFO 2017-06-25 17:06:36,954 main.py:50] epoch 2136, training loss: 18187.50, average training loss: 18506.10, base loss: 23491.69
[INFO 2017-06-25 17:06:41,222 main.py:50] epoch 2137, training loss: 15018.08, average training loss: 18505.38, base loss: 23489.83
[INFO 2017-06-25 17:06:45,494 main.py:50] epoch 2138, training loss: 20784.97, average training loss: 18510.69, base loss: 23498.77
[INFO 2017-06-25 17:06:49,751 main.py:50] epoch 2139, training loss: 19693.02, average training loss: 18512.23, base loss: 23503.92
[INFO 2017-06-25 17:06:54,021 main.py:50] epoch 2140, training loss: 15451.97, average training loss: 18498.50, base loss: 23488.36
[INFO 2017-06-25 17:06:58,286 main.py:50] epoch 2141, training loss: 16538.15, average training loss: 18497.09, base loss: 23483.40
[INFO 2017-06-25 17:07:02,569 main.py:50] epoch 2142, training loss: 13929.27, average training loss: 18491.43, base loss: 23476.01
[INFO 2017-06-25 17:07:06,844 main.py:50] epoch 2143, training loss: 19265.20, average training loss: 18486.48, base loss: 23471.69
[INFO 2017-06-25 17:07:11,114 main.py:50] epoch 2144, training loss: 18651.40, average training loss: 18487.93, base loss: 23475.17
[INFO 2017-06-25 17:07:15,363 main.py:50] epoch 2145, training loss: 15960.38, average training loss: 18486.61, base loss: 23475.29
[INFO 2017-06-25 17:07:19,646 main.py:50] epoch 2146, training loss: 17052.66, average training loss: 18488.43, base loss: 23477.84
[INFO 2017-06-25 17:07:23,919 main.py:50] epoch 2147, training loss: 16152.20, average training loss: 18487.91, base loss: 23477.35
[INFO 2017-06-25 17:07:28,239 main.py:50] epoch 2148, training loss: 18879.14, average training loss: 18489.92, base loss: 23479.05
[INFO 2017-06-25 17:07:32,518 main.py:50] epoch 2149, training loss: 18830.31, average training loss: 18489.92, base loss: 23480.31
[INFO 2017-06-25 17:07:36,781 main.py:50] epoch 2150, training loss: 15532.39, average training loss: 18489.65, base loss: 23481.64
[INFO 2017-06-25 17:07:41,053 main.py:50] epoch 2151, training loss: 19224.10, average training loss: 18489.59, base loss: 23484.14
[INFO 2017-06-25 17:07:45,337 main.py:50] epoch 2152, training loss: 19145.03, average training loss: 18493.28, base loss: 23487.44
[INFO 2017-06-25 17:07:49,622 main.py:50] epoch 2153, training loss: 17629.08, average training loss: 18495.32, base loss: 23490.94
[INFO 2017-06-25 17:07:53,901 main.py:50] epoch 2154, training loss: 18183.05, average training loss: 18495.12, base loss: 23491.77
[INFO 2017-06-25 17:07:58,180 main.py:50] epoch 2155, training loss: 19698.84, average training loss: 18494.80, base loss: 23493.19
[INFO 2017-06-25 17:08:02,455 main.py:50] epoch 2156, training loss: 20957.71, average training loss: 18499.41, base loss: 23501.32
[INFO 2017-06-25 17:08:06,709 main.py:50] epoch 2157, training loss: 19334.30, average training loss: 18501.24, base loss: 23505.24
[INFO 2017-06-25 17:08:10,992 main.py:50] epoch 2158, training loss: 19909.84, average training loss: 18506.45, base loss: 23512.29
[INFO 2017-06-25 17:08:15,273 main.py:50] epoch 2159, training loss: 18393.70, average training loss: 18505.32, base loss: 23510.18
[INFO 2017-06-25 17:08:19,560 main.py:50] epoch 2160, training loss: 17998.19, average training loss: 18501.73, base loss: 23506.23
[INFO 2017-06-25 17:08:23,859 main.py:50] epoch 2161, training loss: 20750.33, average training loss: 18501.63, base loss: 23508.43
[INFO 2017-06-25 17:08:28,130 main.py:50] epoch 2162, training loss: 20875.68, average training loss: 18504.77, base loss: 23515.09
[INFO 2017-06-25 17:08:32,396 main.py:50] epoch 2163, training loss: 18448.11, average training loss: 18501.85, base loss: 23512.56
[INFO 2017-06-25 17:08:36,665 main.py:50] epoch 2164, training loss: 17573.60, average training loss: 18500.06, base loss: 23510.00
[INFO 2017-06-25 17:08:40,957 main.py:50] epoch 2165, training loss: 19020.42, average training loss: 18503.47, base loss: 23514.84
[INFO 2017-06-25 17:08:45,258 main.py:50] epoch 2166, training loss: 13675.44, average training loss: 18499.03, base loss: 23510.88
[INFO 2017-06-25 17:08:49,535 main.py:50] epoch 2167, training loss: 16608.05, average training loss: 18499.07, base loss: 23510.40
[INFO 2017-06-25 17:08:53,801 main.py:50] epoch 2168, training loss: 17254.14, average training loss: 18497.70, base loss: 23511.02
[INFO 2017-06-25 17:08:58,092 main.py:50] epoch 2169, training loss: 23597.24, average training loss: 18505.46, base loss: 23523.94
[INFO 2017-06-25 17:09:02,391 main.py:50] epoch 2170, training loss: 18231.47, average training loss: 18505.34, base loss: 23522.03
[INFO 2017-06-25 17:09:06,660 main.py:50] epoch 2171, training loss: 16878.00, average training loss: 18504.52, base loss: 23522.41
[INFO 2017-06-25 17:09:10,931 main.py:50] epoch 2172, training loss: 21369.85, average training loss: 18509.25, base loss: 23530.38
[INFO 2017-06-25 17:09:15,261 main.py:50] epoch 2173, training loss: 20298.08, average training loss: 18516.34, base loss: 23539.46
[INFO 2017-06-25 17:09:19,521 main.py:50] epoch 2174, training loss: 16414.79, average training loss: 18509.43, base loss: 23533.37
[INFO 2017-06-25 17:09:23,785 main.py:50] epoch 2175, training loss: 14380.01, average training loss: 18507.66, base loss: 23533.97
[INFO 2017-06-25 17:09:28,056 main.py:50] epoch 2176, training loss: 18309.15, average training loss: 18505.74, base loss: 23534.72
[INFO 2017-06-25 17:09:32,320 main.py:50] epoch 2177, training loss: 15744.35, average training loss: 18504.72, base loss: 23531.73
[INFO 2017-06-25 17:09:36,609 main.py:50] epoch 2178, training loss: 14919.59, average training loss: 18500.57, base loss: 23527.38
[INFO 2017-06-25 17:09:40,866 main.py:50] epoch 2179, training loss: 19562.40, average training loss: 18503.26, base loss: 23529.17
[INFO 2017-06-25 17:09:45,129 main.py:50] epoch 2180, training loss: 16493.17, average training loss: 18503.71, base loss: 23532.24
[INFO 2017-06-25 17:09:49,421 main.py:50] epoch 2181, training loss: 15188.71, average training loss: 18498.57, base loss: 23528.87
[INFO 2017-06-25 17:09:53,685 main.py:50] epoch 2182, training loss: 19834.03, average training loss: 18503.62, base loss: 23538.38
[INFO 2017-06-25 17:09:57,980 main.py:50] epoch 2183, training loss: 20046.95, average training loss: 18503.42, base loss: 23537.71
[INFO 2017-06-25 17:10:02,252 main.py:50] epoch 2184, training loss: 16514.31, average training loss: 18502.92, base loss: 23534.90
[INFO 2017-06-25 17:10:06,547 main.py:50] epoch 2185, training loss: 18011.92, average training loss: 18503.57, base loss: 23536.78
[INFO 2017-06-25 17:10:10,819 main.py:50] epoch 2186, training loss: 21441.81, average training loss: 18506.08, base loss: 23541.23
[INFO 2017-06-25 17:10:15,102 main.py:50] epoch 2187, training loss: 20662.81, average training loss: 18510.08, base loss: 23546.06
[INFO 2017-06-25 17:10:19,354 main.py:50] epoch 2188, training loss: 18979.47, average training loss: 18507.13, base loss: 23543.71
[INFO 2017-06-25 17:10:23,641 main.py:50] epoch 2189, training loss: 15871.34, average training loss: 18508.22, base loss: 23546.50
[INFO 2017-06-25 17:10:27,907 main.py:50] epoch 2190, training loss: 16426.12, average training loss: 18505.07, base loss: 23543.07
[INFO 2017-06-25 17:10:32,193 main.py:50] epoch 2191, training loss: 23830.33, average training loss: 18514.87, base loss: 23555.70
[INFO 2017-06-25 17:10:36,474 main.py:50] epoch 2192, training loss: 16052.29, average training loss: 18510.21, base loss: 23550.31
[INFO 2017-06-25 17:10:40,758 main.py:50] epoch 2193, training loss: 21651.44, average training loss: 18513.46, base loss: 23555.88
[INFO 2017-06-25 17:10:45,011 main.py:50] epoch 2194, training loss: 16182.78, average training loss: 18509.31, base loss: 23553.24
[INFO 2017-06-25 17:10:49,310 main.py:50] epoch 2195, training loss: 18359.29, average training loss: 18510.41, base loss: 23554.95
[INFO 2017-06-25 17:10:53,574 main.py:50] epoch 2196, training loss: 21070.66, average training loss: 18512.50, base loss: 23556.37
[INFO 2017-06-25 17:10:57,847 main.py:50] epoch 2197, training loss: 19356.83, average training loss: 18512.69, base loss: 23558.99
[INFO 2017-06-25 17:11:02,119 main.py:50] epoch 2198, training loss: 18783.92, average training loss: 18514.75, base loss: 23562.21
[INFO 2017-06-25 17:11:06,378 main.py:50] epoch 2199, training loss: 26883.79, average training loss: 18519.50, base loss: 23570.78
[INFO 2017-06-25 17:11:10,640 main.py:50] epoch 2200, training loss: 17255.42, average training loss: 18516.25, base loss: 23569.61
[INFO 2017-06-25 17:11:14,924 main.py:50] epoch 2201, training loss: 20291.57, average training loss: 18518.81, base loss: 23574.71
[INFO 2017-06-25 17:11:19,202 main.py:50] epoch 2202, training loss: 19815.44, average training loss: 18520.76, base loss: 23578.50
[INFO 2017-06-25 17:11:23,477 main.py:50] epoch 2203, training loss: 21090.23, average training loss: 18524.29, base loss: 23581.55
[INFO 2017-06-25 17:11:27,762 main.py:50] epoch 2204, training loss: 22085.72, average training loss: 18521.38, base loss: 23579.64
[INFO 2017-06-25 17:11:32,037 main.py:50] epoch 2205, training loss: 17725.21, average training loss: 18521.46, base loss: 23577.88
[INFO 2017-06-25 17:11:36,297 main.py:50] epoch 2206, training loss: 28875.67, average training loss: 18531.76, base loss: 23591.82
[INFO 2017-06-25 17:11:40,568 main.py:50] epoch 2207, training loss: 15725.09, average training loss: 18528.81, base loss: 23586.76
[INFO 2017-06-25 17:11:44,856 main.py:50] epoch 2208, training loss: 18321.18, average training loss: 18528.35, base loss: 23588.69
[INFO 2017-06-25 17:11:49,144 main.py:50] epoch 2209, training loss: 20761.62, average training loss: 18527.45, base loss: 23587.48
[INFO 2017-06-25 17:11:53,423 main.py:50] epoch 2210, training loss: 16837.39, average training loss: 18517.60, base loss: 23575.52
[INFO 2017-06-25 17:11:57,713 main.py:50] epoch 2211, training loss: 17376.66, average training loss: 18513.44, base loss: 23572.73
[INFO 2017-06-25 17:12:02,024 main.py:50] epoch 2212, training loss: 19592.60, average training loss: 18512.03, base loss: 23570.30
[INFO 2017-06-25 17:12:06,307 main.py:50] epoch 2213, training loss: 18205.56, average training loss: 18515.46, base loss: 23577.37
[INFO 2017-06-25 17:12:10,598 main.py:50] epoch 2214, training loss: 19085.41, average training loss: 18517.07, base loss: 23578.98
[INFO 2017-06-25 17:12:14,902 main.py:50] epoch 2215, training loss: 15380.48, average training loss: 18514.69, base loss: 23576.41
[INFO 2017-06-25 17:12:19,183 main.py:50] epoch 2216, training loss: 19676.27, average training loss: 18516.29, base loss: 23578.52
[INFO 2017-06-25 17:12:23,458 main.py:50] epoch 2217, training loss: 17021.56, average training loss: 18512.67, base loss: 23572.82
[INFO 2017-06-25 17:12:27,742 main.py:50] epoch 2218, training loss: 16563.49, average training loss: 18512.93, base loss: 23574.53
[INFO 2017-06-25 17:12:32,033 main.py:50] epoch 2219, training loss: 18169.57, average training loss: 18513.96, base loss: 23574.40
[INFO 2017-06-25 17:12:36,315 main.py:50] epoch 2220, training loss: 16870.94, average training loss: 18513.53, base loss: 23574.21
[INFO 2017-06-25 17:12:40,602 main.py:50] epoch 2221, training loss: 19853.26, average training loss: 18517.56, base loss: 23579.80
[INFO 2017-06-25 17:12:44,902 main.py:50] epoch 2222, training loss: 21129.63, average training loss: 18523.69, base loss: 23590.02
[INFO 2017-06-25 17:12:49,188 main.py:50] epoch 2223, training loss: 15723.62, average training loss: 18526.12, base loss: 23592.60
[INFO 2017-06-25 17:12:53,478 main.py:50] epoch 2224, training loss: 16748.84, average training loss: 18524.64, base loss: 23592.75
[INFO 2017-06-25 17:12:57,779 main.py:50] epoch 2225, training loss: 14561.25, average training loss: 18520.09, base loss: 23587.15
[INFO 2017-06-25 17:13:02,077 main.py:50] epoch 2226, training loss: 17532.69, average training loss: 18520.85, base loss: 23588.94
[INFO 2017-06-25 17:13:06,378 main.py:50] epoch 2227, training loss: 14522.01, average training loss: 18519.39, base loss: 23588.00
[INFO 2017-06-25 17:13:10,672 main.py:50] epoch 2228, training loss: 17696.53, average training loss: 18520.08, base loss: 23587.93
[INFO 2017-06-25 17:13:14,983 main.py:50] epoch 2229, training loss: 13233.49, average training loss: 18511.45, base loss: 23578.94
[INFO 2017-06-25 17:13:19,297 main.py:50] epoch 2230, training loss: 25079.72, average training loss: 18516.12, base loss: 23585.99
[INFO 2017-06-25 17:13:23,572 main.py:50] epoch 2231, training loss: 18659.70, average training loss: 18516.67, base loss: 23589.12
[INFO 2017-06-25 17:13:27,880 main.py:50] epoch 2232, training loss: 14658.89, average training loss: 18509.38, base loss: 23580.39
[INFO 2017-06-25 17:13:32,185 main.py:50] epoch 2233, training loss: 23057.64, average training loss: 18511.66, base loss: 23584.72
[INFO 2017-06-25 17:13:36,473 main.py:50] epoch 2234, training loss: 15349.96, average training loss: 18509.82, base loss: 23583.26
[INFO 2017-06-25 17:13:40,756 main.py:50] epoch 2235, training loss: 15986.25, average training loss: 18504.12, base loss: 23577.48
[INFO 2017-06-25 17:13:45,048 main.py:50] epoch 2236, training loss: 24487.98, average training loss: 18514.58, base loss: 23593.05
[INFO 2017-06-25 17:13:49,335 main.py:50] epoch 2237, training loss: 16173.71, average training loss: 18516.04, base loss: 23596.26
[INFO 2017-06-25 17:13:53,602 main.py:50] epoch 2238, training loss: 19174.08, average training loss: 18514.60, base loss: 23595.30
[INFO 2017-06-25 17:13:57,912 main.py:50] epoch 2239, training loss: 18822.89, average training loss: 18511.24, base loss: 23594.94
[INFO 2017-06-25 17:14:02,199 main.py:50] epoch 2240, training loss: 13087.44, average training loss: 18504.60, base loss: 23586.07
[INFO 2017-06-25 17:14:06,456 main.py:50] epoch 2241, training loss: 18166.13, average training loss: 18506.79, base loss: 23587.28
[INFO 2017-06-25 17:14:10,765 main.py:50] epoch 2242, training loss: 18780.94, average training loss: 18502.14, base loss: 23580.39
[INFO 2017-06-25 17:14:15,062 main.py:50] epoch 2243, training loss: 17109.20, average training loss: 18500.27, base loss: 23578.93
[INFO 2017-06-25 17:14:19,318 main.py:50] epoch 2244, training loss: 14174.47, average training loss: 18496.69, base loss: 23576.89
[INFO 2017-06-25 17:14:23,614 main.py:50] epoch 2245, training loss: 18076.93, average training loss: 18494.52, base loss: 23576.12
[INFO 2017-06-25 17:14:27,889 main.py:50] epoch 2246, training loss: 20229.62, average training loss: 18497.75, base loss: 23578.54
[INFO 2017-06-25 17:14:32,155 main.py:50] epoch 2247, training loss: 17551.70, average training loss: 18495.59, base loss: 23575.57
[INFO 2017-06-25 17:14:36,426 main.py:50] epoch 2248, training loss: 19350.17, average training loss: 18491.57, base loss: 23572.19
[INFO 2017-06-25 17:14:40,715 main.py:50] epoch 2249, training loss: 22008.49, average training loss: 18492.60, base loss: 23573.96
[INFO 2017-06-25 17:14:45,017 main.py:50] epoch 2250, training loss: 15470.89, average training loss: 18489.80, base loss: 23571.71
[INFO 2017-06-25 17:14:49,307 main.py:50] epoch 2251, training loss: 15232.61, average training loss: 18485.35, base loss: 23566.08
[INFO 2017-06-25 17:14:53,572 main.py:50] epoch 2252, training loss: 20067.51, average training loss: 18487.53, base loss: 23569.70
[INFO 2017-06-25 17:14:57,863 main.py:50] epoch 2253, training loss: 14086.01, average training loss: 18482.49, base loss: 23562.05
[INFO 2017-06-25 17:15:02,148 main.py:50] epoch 2254, training loss: 14788.15, average training loss: 18476.37, base loss: 23554.51
[INFO 2017-06-25 17:15:06,444 main.py:50] epoch 2255, training loss: 22308.55, average training loss: 18478.96, base loss: 23557.99
[INFO 2017-06-25 17:15:10,718 main.py:50] epoch 2256, training loss: 23186.81, average training loss: 18483.19, base loss: 23565.20
[INFO 2017-06-25 17:15:14,993 main.py:50] epoch 2257, training loss: 17590.23, average training loss: 18482.81, base loss: 23567.72
[INFO 2017-06-25 17:15:19,273 main.py:50] epoch 2258, training loss: 20607.12, average training loss: 18484.19, base loss: 23567.83
[INFO 2017-06-25 17:15:23,528 main.py:50] epoch 2259, training loss: 16134.24, average training loss: 18482.71, base loss: 23566.08
[INFO 2017-06-25 17:15:27,800 main.py:50] epoch 2260, training loss: 16528.72, average training loss: 18482.94, base loss: 23569.44
[INFO 2017-06-25 17:15:32,070 main.py:50] epoch 2261, training loss: 14935.03, average training loss: 18479.52, base loss: 23565.76
[INFO 2017-06-25 17:15:36,366 main.py:50] epoch 2262, training loss: 16671.38, average training loss: 18472.76, base loss: 23559.62
[INFO 2017-06-25 17:15:40,641 main.py:50] epoch 2263, training loss: 18966.21, average training loss: 18466.85, base loss: 23555.15
[INFO 2017-06-25 17:15:44,900 main.py:50] epoch 2264, training loss: 19309.98, average training loss: 18462.05, base loss: 23550.55
[INFO 2017-06-25 17:15:49,171 main.py:50] epoch 2265, training loss: 15592.11, average training loss: 18453.03, base loss: 23543.31
[INFO 2017-06-25 17:15:53,452 main.py:50] epoch 2266, training loss: 18763.66, average training loss: 18454.74, base loss: 23548.03
[INFO 2017-06-25 17:15:57,710 main.py:50] epoch 2267, training loss: 19286.08, average training loss: 18452.21, base loss: 23546.35
[INFO 2017-06-25 17:16:01,995 main.py:50] epoch 2268, training loss: 15820.98, average training loss: 18448.72, base loss: 23543.58
[INFO 2017-06-25 17:16:06,278 main.py:50] epoch 2269, training loss: 16685.14, average training loss: 18441.23, base loss: 23536.31
[INFO 2017-06-25 17:16:10,575 main.py:50] epoch 2270, training loss: 15327.30, average training loss: 18436.41, base loss: 23530.61
[INFO 2017-06-25 17:16:14,863 main.py:50] epoch 2271, training loss: 18872.51, average training loss: 18437.09, base loss: 23533.40
[INFO 2017-06-25 17:16:19,125 main.py:50] epoch 2272, training loss: 24690.18, average training loss: 18441.52, base loss: 23540.50
[INFO 2017-06-25 17:16:23,431 main.py:50] epoch 2273, training loss: 14785.34, average training loss: 18434.75, base loss: 23530.44
[INFO 2017-06-25 17:16:27,708 main.py:50] epoch 2274, training loss: 18799.00, average training loss: 18429.36, base loss: 23525.25
[INFO 2017-06-25 17:16:31,987 main.py:50] epoch 2275, training loss: 19575.89, average training loss: 18429.13, base loss: 23525.32
[INFO 2017-06-25 17:16:36,263 main.py:50] epoch 2276, training loss: 16204.83, average training loss: 18428.16, base loss: 23525.75
[INFO 2017-06-25 17:16:40,548 main.py:50] epoch 2277, training loss: 21376.81, average training loss: 18427.23, base loss: 23523.50
[INFO 2017-06-25 17:16:44,828 main.py:50] epoch 2278, training loss: 21004.31, average training loss: 18429.05, base loss: 23532.02
[INFO 2017-06-25 17:16:49,127 main.py:50] epoch 2279, training loss: 21986.63, average training loss: 18429.99, base loss: 23533.21
[INFO 2017-06-25 17:16:53,420 main.py:50] epoch 2280, training loss: 14719.85, average training loss: 18427.27, base loss: 23528.87
[INFO 2017-06-25 17:16:57,704 main.py:50] epoch 2281, training loss: 24404.60, average training loss: 18429.21, base loss: 23533.85
[INFO 2017-06-25 17:17:01,982 main.py:50] epoch 2282, training loss: 21617.54, average training loss: 18431.54, base loss: 23538.64
[INFO 2017-06-25 17:17:06,269 main.py:50] epoch 2283, training loss: 17642.27, average training loss: 18435.33, base loss: 23540.64
[INFO 2017-06-25 17:17:10,554 main.py:50] epoch 2284, training loss: 22315.81, average training loss: 18437.11, base loss: 23545.54
[INFO 2017-06-25 17:17:14,843 main.py:50] epoch 2285, training loss: 21967.43, average training loss: 18437.08, base loss: 23546.56
[INFO 2017-06-25 17:17:19,140 main.py:50] epoch 2286, training loss: 15165.10, average training loss: 18434.69, base loss: 23544.94
[INFO 2017-06-25 17:17:23,432 main.py:50] epoch 2287, training loss: 16530.87, average training loss: 18429.90, base loss: 23538.96
[INFO 2017-06-25 17:17:27,707 main.py:50] epoch 2288, training loss: 21210.77, average training loss: 18431.21, base loss: 23541.80
[INFO 2017-06-25 17:17:31,981 main.py:50] epoch 2289, training loss: 21850.39, average training loss: 18434.68, base loss: 23545.09
[INFO 2017-06-25 17:17:36,256 main.py:50] epoch 2290, training loss: 19045.25, average training loss: 18431.33, base loss: 23543.56
[INFO 2017-06-25 17:17:40,543 main.py:50] epoch 2291, training loss: 20994.59, average training loss: 18429.84, base loss: 23545.38
[INFO 2017-06-25 17:17:44,820 main.py:50] epoch 2292, training loss: 19561.68, average training loss: 18431.13, base loss: 23547.83
[INFO 2017-06-25 17:17:49,109 main.py:50] epoch 2293, training loss: 19065.77, average training loss: 18430.70, base loss: 23548.75
[INFO 2017-06-25 17:17:53,382 main.py:50] epoch 2294, training loss: 15202.87, average training loss: 18427.67, base loss: 23543.23
[INFO 2017-06-25 17:17:57,670 main.py:50] epoch 2295, training loss: 18067.80, average training loss: 18427.11, base loss: 23547.16
[INFO 2017-06-25 17:18:01,954 main.py:50] epoch 2296, training loss: 21174.51, average training loss: 18422.05, base loss: 23543.42
[INFO 2017-06-25 17:18:06,226 main.py:50] epoch 2297, training loss: 18498.64, average training loss: 18423.85, base loss: 23548.63
[INFO 2017-06-25 17:18:10,519 main.py:50] epoch 2298, training loss: 19735.27, average training loss: 18427.06, base loss: 23553.78
[INFO 2017-06-25 17:18:14,812 main.py:50] epoch 2299, training loss: 15279.41, average training loss: 18423.61, base loss: 23550.05
[INFO 2017-06-25 17:18:19,097 main.py:50] epoch 2300, training loss: 17700.45, average training loss: 18425.21, base loss: 23555.24
[INFO 2017-06-25 17:18:23,358 main.py:50] epoch 2301, training loss: 16977.86, average training loss: 18425.03, base loss: 23556.72
[INFO 2017-06-25 17:18:27,651 main.py:50] epoch 2302, training loss: 15370.22, average training loss: 18420.66, base loss: 23551.95
[INFO 2017-06-25 17:18:31,963 main.py:50] epoch 2303, training loss: 18203.00, average training loss: 18423.57, base loss: 23556.28
[INFO 2017-06-25 17:18:36,255 main.py:50] epoch 2304, training loss: 22954.18, average training loss: 18430.23, base loss: 23564.69
[INFO 2017-06-25 17:18:40,552 main.py:50] epoch 2305, training loss: 15853.65, average training loss: 18430.07, base loss: 23564.71
[INFO 2017-06-25 17:18:44,842 main.py:50] epoch 2306, training loss: 16967.36, average training loss: 18428.23, base loss: 23562.08
[INFO 2017-06-25 17:18:49,118 main.py:50] epoch 2307, training loss: 16482.88, average training loss: 18427.65, base loss: 23564.33
[INFO 2017-06-25 17:18:53,411 main.py:50] epoch 2308, training loss: 26456.38, average training loss: 18441.95, base loss: 23582.50
[INFO 2017-06-25 17:18:57,682 main.py:50] epoch 2309, training loss: 16960.88, average training loss: 18440.94, base loss: 23582.82
[INFO 2017-06-25 17:19:01,959 main.py:50] epoch 2310, training loss: 21832.87, average training loss: 18440.83, base loss: 23587.01
[INFO 2017-06-25 17:19:06,217 main.py:50] epoch 2311, training loss: 14612.62, average training loss: 18434.55, base loss: 23579.45
[INFO 2017-06-25 17:19:10,485 main.py:50] epoch 2312, training loss: 15473.12, average training loss: 18429.84, base loss: 23572.52
[INFO 2017-06-25 17:19:14,771 main.py:50] epoch 2313, training loss: 19341.46, average training loss: 18431.25, base loss: 23575.50
[INFO 2017-06-25 17:19:19,062 main.py:50] epoch 2314, training loss: 19799.26, average training loss: 18432.98, base loss: 23576.13
[INFO 2017-06-25 17:19:23,348 main.py:50] epoch 2315, training loss: 18368.53, average training loss: 18432.05, base loss: 23576.45
[INFO 2017-06-25 17:19:27,639 main.py:50] epoch 2316, training loss: 18344.85, average training loss: 18429.35, base loss: 23574.71
[INFO 2017-06-25 17:19:31,929 main.py:50] epoch 2317, training loss: 18087.88, average training loss: 18429.08, base loss: 23576.67
[INFO 2017-06-25 17:19:36,221 main.py:50] epoch 2318, training loss: 18747.49, average training loss: 18430.57, base loss: 23579.86
[INFO 2017-06-25 17:19:40,504 main.py:50] epoch 2319, training loss: 17799.65, average training loss: 18433.19, base loss: 23583.48
[INFO 2017-06-25 17:19:44,815 main.py:50] epoch 2320, training loss: 14359.52, average training loss: 18428.76, base loss: 23578.36
[INFO 2017-06-25 17:19:49,135 main.py:50] epoch 2321, training loss: 18854.06, average training loss: 18429.42, base loss: 23579.35
[INFO 2017-06-25 17:19:53,423 main.py:50] epoch 2322, training loss: 21594.67, average training loss: 18429.97, base loss: 23582.17
[INFO 2017-06-25 17:19:57,711 main.py:50] epoch 2323, training loss: 15804.86, average training loss: 18428.40, base loss: 23578.48
[INFO 2017-06-25 17:20:01,993 main.py:50] epoch 2324, training loss: 23085.46, average training loss: 18433.20, base loss: 23587.77
[INFO 2017-06-25 17:20:06,271 main.py:50] epoch 2325, training loss: 18456.59, average training loss: 18435.11, base loss: 23592.16
[INFO 2017-06-25 17:20:10,551 main.py:50] epoch 2326, training loss: 17305.30, average training loss: 18433.87, base loss: 23590.96
[INFO 2017-06-25 17:20:14,829 main.py:50] epoch 2327, training loss: 16411.93, average training loss: 18429.52, base loss: 23589.24
[INFO 2017-06-25 17:20:19,121 main.py:50] epoch 2328, training loss: 17007.21, average training loss: 18432.50, base loss: 23595.27
[INFO 2017-06-25 17:20:23,401 main.py:50] epoch 2329, training loss: 18192.46, average training loss: 18430.64, base loss: 23592.09
[INFO 2017-06-25 17:20:27,699 main.py:50] epoch 2330, training loss: 13649.73, average training loss: 18424.57, base loss: 23585.47
[INFO 2017-06-25 17:20:31,986 main.py:50] epoch 2331, training loss: 21341.37, average training loss: 18426.04, base loss: 23588.06
[INFO 2017-06-25 17:20:36,313 main.py:50] epoch 2332, training loss: 13468.04, average training loss: 18421.38, base loss: 23584.07
[INFO 2017-06-25 17:20:40,601 main.py:50] epoch 2333, training loss: 14313.98, average training loss: 18419.50, base loss: 23581.20
[INFO 2017-06-25 17:20:44,896 main.py:50] epoch 2334, training loss: 18951.78, average training loss: 18420.91, base loss: 23582.36
[INFO 2017-06-25 17:20:49,168 main.py:50] epoch 2335, training loss: 16409.33, average training loss: 18422.51, base loss: 23583.90
[INFO 2017-06-25 17:20:53,469 main.py:50] epoch 2336, training loss: 15581.60, average training loss: 18420.49, base loss: 23580.54
[INFO 2017-06-25 17:20:57,742 main.py:50] epoch 2337, training loss: 20003.12, average training loss: 18422.68, base loss: 23583.60
[INFO 2017-06-25 17:21:02,058 main.py:50] epoch 2338, training loss: 18208.10, average training loss: 18422.68, base loss: 23584.65
[INFO 2017-06-25 17:21:06,379 main.py:50] epoch 2339, training loss: 25338.21, average training loss: 18429.89, base loss: 23593.80
[INFO 2017-06-25 17:21:10,687 main.py:50] epoch 2340, training loss: 16459.93, average training loss: 18428.69, base loss: 23590.91
[INFO 2017-06-25 17:21:14,977 main.py:50] epoch 2341, training loss: 16918.11, average training loss: 18423.44, base loss: 23585.18
[INFO 2017-06-25 17:21:19,296 main.py:50] epoch 2342, training loss: 20217.62, average training loss: 18427.82, base loss: 23591.69
[INFO 2017-06-25 17:21:23,584 main.py:50] epoch 2343, training loss: 21029.22, average training loss: 18427.78, base loss: 23595.27
[INFO 2017-06-25 17:21:27,862 main.py:50] epoch 2344, training loss: 16749.93, average training loss: 18426.21, base loss: 23594.52
[INFO 2017-06-25 17:21:32,180 main.py:50] epoch 2345, training loss: 19348.47, average training loss: 18425.41, base loss: 23593.47
[INFO 2017-06-25 17:21:36,466 main.py:50] epoch 2346, training loss: 15183.76, average training loss: 18425.22, base loss: 23594.32
[INFO 2017-06-25 17:21:40,754 main.py:50] epoch 2347, training loss: 18277.39, average training loss: 18424.45, base loss: 23594.68
[INFO 2017-06-25 17:21:45,088 main.py:50] epoch 2348, training loss: 16300.56, average training loss: 18423.73, base loss: 23596.18
[INFO 2017-06-25 17:21:49,402 main.py:50] epoch 2349, training loss: 16207.84, average training loss: 18415.78, base loss: 23586.78
[INFO 2017-06-25 17:21:53,686 main.py:50] epoch 2350, training loss: 18095.15, average training loss: 18415.55, base loss: 23587.63
[INFO 2017-06-25 17:21:57,978 main.py:50] epoch 2351, training loss: 16281.30, average training loss: 18413.46, base loss: 23585.59
[INFO 2017-06-25 17:22:02,297 main.py:50] epoch 2352, training loss: 16136.25, average training loss: 18413.78, base loss: 23584.38
[INFO 2017-06-25 17:22:06,577 main.py:50] epoch 2353, training loss: 16063.34, average training loss: 18410.28, base loss: 23580.60
[INFO 2017-06-25 17:22:10,893 main.py:50] epoch 2354, training loss: 14967.45, average training loss: 18405.07, base loss: 23575.04
[INFO 2017-06-25 17:22:15,200 main.py:50] epoch 2355, training loss: 18514.90, average training loss: 18404.16, base loss: 23577.85
[INFO 2017-06-25 17:22:19,480 main.py:50] epoch 2356, training loss: 17527.04, average training loss: 18405.43, base loss: 23580.19
[INFO 2017-06-25 17:22:23,793 main.py:50] epoch 2357, training loss: 21945.33, average training loss: 18408.29, base loss: 23582.55
[INFO 2017-06-25 17:22:28,072 main.py:50] epoch 2358, training loss: 17718.45, average training loss: 18405.71, base loss: 23582.34
[INFO 2017-06-25 17:22:32,390 main.py:50] epoch 2359, training loss: 19126.69, average training loss: 18407.77, base loss: 23585.62
[INFO 2017-06-25 17:22:36,692 main.py:50] epoch 2360, training loss: 20547.60, average training loss: 18408.54, base loss: 23590.83
[INFO 2017-06-25 17:22:40,999 main.py:50] epoch 2361, training loss: 20582.48, average training loss: 18407.90, base loss: 23591.83
[INFO 2017-06-25 17:22:45,319 main.py:50] epoch 2362, training loss: 15209.79, average training loss: 18404.32, base loss: 23587.03
[INFO 2017-06-25 17:22:49,633 main.py:50] epoch 2363, training loss: 16202.42, average training loss: 18395.73, base loss: 23577.96
[INFO 2017-06-25 17:22:53,910 main.py:50] epoch 2364, training loss: 16032.13, average training loss: 18396.75, base loss: 23582.01
[INFO 2017-06-25 17:22:58,216 main.py:50] epoch 2365, training loss: 13956.87, average training loss: 18391.61, base loss: 23576.27
[INFO 2017-06-25 17:23:02,505 main.py:50] epoch 2366, training loss: 20207.63, average training loss: 18394.20, base loss: 23580.28
[INFO 2017-06-25 17:23:06,775 main.py:50] epoch 2367, training loss: 17933.65, average training loss: 18389.40, base loss: 23574.07
[INFO 2017-06-25 17:23:11,083 main.py:50] epoch 2368, training loss: 18851.46, average training loss: 18393.94, base loss: 23581.36
[INFO 2017-06-25 17:23:15,394 main.py:50] epoch 2369, training loss: 18478.14, average training loss: 18394.64, base loss: 23581.41
[INFO 2017-06-25 17:23:19,690 main.py:50] epoch 2370, training loss: 18460.09, average training loss: 18396.03, base loss: 23581.72
[INFO 2017-06-25 17:23:24,009 main.py:50] epoch 2371, training loss: 16532.01, average training loss: 18396.19, base loss: 23582.98
[INFO 2017-06-25 17:23:28,333 main.py:50] epoch 2372, training loss: 17709.81, average training loss: 18399.05, base loss: 23588.09
[INFO 2017-06-25 17:23:32,597 main.py:50] epoch 2373, training loss: 18047.57, average training loss: 18399.14, base loss: 23587.80
[INFO 2017-06-25 17:23:36,876 main.py:50] epoch 2374, training loss: 17936.00, average training loss: 18398.47, base loss: 23586.48
[INFO 2017-06-25 17:23:41,188 main.py:50] epoch 2375, training loss: 16185.93, average training loss: 18398.81, base loss: 23586.95
[INFO 2017-06-25 17:23:45,488 main.py:50] epoch 2376, training loss: 18634.13, average training loss: 18400.81, base loss: 23588.97
[INFO 2017-06-25 17:23:49,788 main.py:50] epoch 2377, training loss: 19992.86, average training loss: 18399.74, base loss: 23585.67
[INFO 2017-06-25 17:23:54,128 main.py:50] epoch 2378, training loss: 19701.45, average training loss: 18399.38, base loss: 23584.71
[INFO 2017-06-25 17:23:58,434 main.py:50] epoch 2379, training loss: 21397.06, average training loss: 18403.00, base loss: 23588.90
[INFO 2017-06-25 17:24:02,714 main.py:50] epoch 2380, training loss: 24120.41, average training loss: 18407.73, base loss: 23593.46
[INFO 2017-06-25 17:24:07,017 main.py:50] epoch 2381, training loss: 19733.22, average training loss: 18410.32, base loss: 23599.82
[INFO 2017-06-25 17:24:11,332 main.py:50] epoch 2382, training loss: 19121.38, average training loss: 18409.41, base loss: 23595.96
[INFO 2017-06-25 17:24:15,616 main.py:50] epoch 2383, training loss: 16235.20, average training loss: 18407.00, base loss: 23593.93
[INFO 2017-06-25 17:24:19,924 main.py:50] epoch 2384, training loss: 17819.27, average training loss: 18405.00, base loss: 23593.45
[INFO 2017-06-25 17:24:24,226 main.py:50] epoch 2385, training loss: 22331.99, average training loss: 18408.53, base loss: 23595.09
[INFO 2017-06-25 17:24:28,541 main.py:50] epoch 2386, training loss: 17520.78, average training loss: 18403.55, base loss: 23590.65
[INFO 2017-06-25 17:24:32,823 main.py:50] epoch 2387, training loss: 17328.31, average training loss: 18404.10, base loss: 23589.39
[INFO 2017-06-25 17:24:37,116 main.py:50] epoch 2388, training loss: 22763.34, average training loss: 18410.00, base loss: 23597.80
[INFO 2017-06-25 17:24:41,407 main.py:50] epoch 2389, training loss: 20668.18, average training loss: 18413.85, base loss: 23604.09
[INFO 2017-06-25 17:24:45,691 main.py:50] epoch 2390, training loss: 20601.15, average training loss: 18418.31, base loss: 23609.92
[INFO 2017-06-25 17:24:49,970 main.py:50] epoch 2391, training loss: 14230.71, average training loss: 18411.80, base loss: 23599.29
[INFO 2017-06-25 17:24:54,277 main.py:50] epoch 2392, training loss: 22682.55, average training loss: 18419.26, base loss: 23607.52
[INFO 2017-06-25 17:24:58,590 main.py:50] epoch 2393, training loss: 14616.94, average training loss: 18413.89, base loss: 23604.69
[INFO 2017-06-25 17:25:02,898 main.py:50] epoch 2394, training loss: 18118.13, average training loss: 18412.43, base loss: 23604.44
[INFO 2017-06-25 17:25:07,215 main.py:50] epoch 2395, training loss: 19354.73, average training loss: 18415.49, base loss: 23611.26
[INFO 2017-06-25 17:25:11,510 main.py:50] epoch 2396, training loss: 19347.02, average training loss: 18413.78, base loss: 23608.62
[INFO 2017-06-25 17:25:15,828 main.py:50] epoch 2397, training loss: 21015.77, average training loss: 18415.34, base loss: 23608.59
[INFO 2017-06-25 17:25:20,133 main.py:50] epoch 2398, training loss: 21779.93, average training loss: 18418.57, base loss: 23613.68
[INFO 2017-06-25 17:25:24,414 main.py:50] epoch 2399, training loss: 16286.12, average training loss: 18419.14, base loss: 23614.38
[INFO 2017-06-25 17:25:28,692 main.py:50] epoch 2400, training loss: 15553.07, average training loss: 18415.64, base loss: 23607.48
[INFO 2017-06-25 17:25:32,972 main.py:50] epoch 2401, training loss: 21504.96, average training loss: 18416.70, base loss: 23609.04
[INFO 2017-06-25 17:25:37,250 main.py:50] epoch 2402, training loss: 20418.32, average training loss: 18418.77, base loss: 23613.01
[INFO 2017-06-25 17:25:41,553 main.py:50] epoch 2403, training loss: 17144.15, average training loss: 18414.80, base loss: 23609.77
[INFO 2017-06-25 17:25:45,834 main.py:50] epoch 2404, training loss: 17847.59, average training loss: 18410.97, base loss: 23605.70
[INFO 2017-06-25 17:25:50,129 main.py:50] epoch 2405, training loss: 20583.03, average training loss: 18410.34, base loss: 23607.78
[INFO 2017-06-25 17:25:54,420 main.py:50] epoch 2406, training loss: 19117.51, average training loss: 18409.79, base loss: 23610.01
[INFO 2017-06-25 17:25:58,726 main.py:50] epoch 2407, training loss: 18223.34, average training loss: 18410.00, base loss: 23610.62
[INFO 2017-06-25 17:26:03,002 main.py:50] epoch 2408, training loss: 17782.45, average training loss: 18404.64, base loss: 23605.45
[INFO 2017-06-25 17:26:07,329 main.py:50] epoch 2409, training loss: 15124.11, average training loss: 18397.46, base loss: 23596.90
[INFO 2017-06-25 17:26:11,665 main.py:50] epoch 2410, training loss: 17021.82, average training loss: 18394.00, base loss: 23591.67
[INFO 2017-06-25 17:26:15,976 main.py:50] epoch 2411, training loss: 18345.88, average training loss: 18396.24, base loss: 23595.15
[INFO 2017-06-25 17:26:20,264 main.py:50] epoch 2412, training loss: 19242.12, average training loss: 18395.15, base loss: 23595.76
[INFO 2017-06-25 17:26:24,566 main.py:50] epoch 2413, training loss: 14699.79, average training loss: 18387.77, base loss: 23586.90
[INFO 2017-06-25 17:26:28,859 main.py:50] epoch 2414, training loss: 19700.27, average training loss: 18385.01, base loss: 23582.47
[INFO 2017-06-25 17:26:33,179 main.py:50] epoch 2415, training loss: 21216.79, average training loss: 18381.84, base loss: 23582.27
[INFO 2017-06-25 17:26:37,474 main.py:50] epoch 2416, training loss: 18445.24, average training loss: 18380.66, base loss: 23579.98
[INFO 2017-06-25 17:26:41,766 main.py:50] epoch 2417, training loss: 18965.70, average training loss: 18378.49, base loss: 23579.10
[INFO 2017-06-25 17:26:46,085 main.py:50] epoch 2418, training loss: 17243.07, average training loss: 18377.27, base loss: 23578.77
[INFO 2017-06-25 17:26:50,382 main.py:50] epoch 2419, training loss: 14483.49, average training loss: 18375.89, base loss: 23576.79
[INFO 2017-06-25 17:26:54,682 main.py:50] epoch 2420, training loss: 13497.12, average training loss: 18367.02, base loss: 23565.37
[INFO 2017-06-25 17:26:58,987 main.py:50] epoch 2421, training loss: 19852.67, average training loss: 18369.37, base loss: 23569.03
[INFO 2017-06-25 17:27:03,279 main.py:50] epoch 2422, training loss: 15996.19, average training loss: 18366.79, base loss: 23564.41
[INFO 2017-06-25 17:27:07,612 main.py:50] epoch 2423, training loss: 17982.75, average training loss: 18365.80, base loss: 23567.18
[INFO 2017-06-25 17:27:11,918 main.py:50] epoch 2424, training loss: 17665.93, average training loss: 18364.96, base loss: 23566.40
[INFO 2017-06-25 17:27:16,215 main.py:50] epoch 2425, training loss: 17093.52, average training loss: 18361.06, base loss: 23563.42
[INFO 2017-06-25 17:27:20,510 main.py:50] epoch 2426, training loss: 17955.54, average training loss: 18360.13, base loss: 23563.02
[INFO 2017-06-25 17:27:24,821 main.py:50] epoch 2427, training loss: 20680.35, average training loss: 18360.34, base loss: 23561.60
[INFO 2017-06-25 17:27:29,128 main.py:50] epoch 2428, training loss: 16936.47, average training loss: 18358.76, base loss: 23559.34
[INFO 2017-06-25 17:27:33,424 main.py:50] epoch 2429, training loss: 19747.29, average training loss: 18360.61, base loss: 23562.03
[INFO 2017-06-25 17:27:37,698 main.py:50] epoch 2430, training loss: 16522.42, average training loss: 18359.79, base loss: 23561.26
[INFO 2017-06-25 17:27:42,011 main.py:50] epoch 2431, training loss: 17558.17, average training loss: 18353.68, base loss: 23551.61
[INFO 2017-06-25 17:27:46,317 main.py:50] epoch 2432, training loss: 19168.79, average training loss: 18351.81, base loss: 23548.63
[INFO 2017-06-25 17:27:50,611 main.py:50] epoch 2433, training loss: 15111.56, average training loss: 18345.67, base loss: 23540.45
[INFO 2017-06-25 17:27:54,907 main.py:50] epoch 2434, training loss: 13082.17, average training loss: 18341.87, base loss: 23536.20
[INFO 2017-06-25 17:27:59,228 main.py:50] epoch 2435, training loss: 15595.94, average training loss: 18338.31, base loss: 23534.47
[INFO 2017-06-25 17:28:03,522 main.py:50] epoch 2436, training loss: 15215.67, average training loss: 18333.30, base loss: 23529.26
[INFO 2017-06-25 17:28:07,839 main.py:50] epoch 2437, training loss: 16203.94, average training loss: 18330.22, base loss: 23526.16
[INFO 2017-06-25 17:28:12,158 main.py:50] epoch 2438, training loss: 21291.80, average training loss: 18334.07, base loss: 23530.82
[INFO 2017-06-25 17:28:16,454 main.py:50] epoch 2439, training loss: 14773.83, average training loss: 18324.02, base loss: 23520.03
[INFO 2017-06-25 17:28:20,775 main.py:50] epoch 2440, training loss: 17063.82, average training loss: 18318.60, base loss: 23515.75
[INFO 2017-06-25 17:28:25,084 main.py:50] epoch 2441, training loss: 18023.54, average training loss: 18318.92, base loss: 23516.79
[INFO 2017-06-25 17:28:29,407 main.py:50] epoch 2442, training loss: 19223.98, average training loss: 18321.21, base loss: 23522.39
[INFO 2017-06-25 17:28:33,733 main.py:50] epoch 2443, training loss: 18260.81, average training loss: 18320.21, base loss: 23520.60
[INFO 2017-06-25 17:28:38,040 main.py:50] epoch 2444, training loss: 20969.99, average training loss: 18324.79, base loss: 23529.00
[INFO 2017-06-25 17:28:42,346 main.py:50] epoch 2445, training loss: 17517.38, average training loss: 18323.98, base loss: 23532.13
[INFO 2017-06-25 17:28:46,631 main.py:50] epoch 2446, training loss: 16544.52, average training loss: 18319.59, base loss: 23527.15
[INFO 2017-06-25 17:28:50,956 main.py:50] epoch 2447, training loss: 19353.85, average training loss: 18324.58, base loss: 23534.53
[INFO 2017-06-25 17:28:55,260 main.py:50] epoch 2448, training loss: 20001.19, average training loss: 18325.49, base loss: 23534.32
[INFO 2017-06-25 17:28:59,552 main.py:50] epoch 2449, training loss: 15314.15, average training loss: 18324.51, base loss: 23533.44
[INFO 2017-06-25 17:29:03,857 main.py:50] epoch 2450, training loss: 18707.30, average training loss: 18322.66, base loss: 23533.42
[INFO 2017-06-25 17:29:08,148 main.py:50] epoch 2451, training loss: 18978.19, average training loss: 18317.99, base loss: 23530.48
[INFO 2017-06-25 17:29:12,464 main.py:50] epoch 2452, training loss: 15740.72, average training loss: 18311.01, base loss: 23523.65
[INFO 2017-06-25 17:29:16,739 main.py:50] epoch 2453, training loss: 16990.57, average training loss: 18310.04, base loss: 23523.50
[INFO 2017-06-25 17:29:21,066 main.py:50] epoch 2454, training loss: 20459.17, average training loss: 18311.37, base loss: 23528.86
[INFO 2017-06-25 17:29:25,375 main.py:50] epoch 2455, training loss: 19319.40, average training loss: 18312.39, base loss: 23531.05
[INFO 2017-06-25 17:29:29,656 main.py:50] epoch 2456, training loss: 19400.31, average training loss: 18315.09, base loss: 23535.74
[INFO 2017-06-25 17:29:33,959 main.py:50] epoch 2457, training loss: 16291.09, average training loss: 18314.35, base loss: 23534.83
[INFO 2017-06-25 17:29:38,268 main.py:50] epoch 2458, training loss: 17629.91, average training loss: 18315.41, base loss: 23536.77
[INFO 2017-06-25 17:29:42,566 main.py:50] epoch 2459, training loss: 19471.76, average training loss: 18318.77, base loss: 23543.76
[INFO 2017-06-25 17:29:46,851 main.py:50] epoch 2460, training loss: 17825.07, average training loss: 18316.11, base loss: 23540.10
[INFO 2017-06-25 17:29:51,159 main.py:50] epoch 2461, training loss: 15998.12, average training loss: 18311.62, base loss: 23537.79
[INFO 2017-06-25 17:29:55,437 main.py:50] epoch 2462, training loss: 15991.86, average training loss: 18306.68, base loss: 23531.90
[INFO 2017-06-25 17:29:59,752 main.py:50] epoch 2463, training loss: 17277.74, average training loss: 18309.77, base loss: 23537.16
[INFO 2017-06-25 17:30:04,022 main.py:50] epoch 2464, training loss: 18903.43, average training loss: 18309.09, base loss: 23537.37
[INFO 2017-06-25 17:30:08,292 main.py:50] epoch 2465, training loss: 16002.47, average training loss: 18303.17, base loss: 23529.04
[INFO 2017-06-25 17:30:12,594 main.py:50] epoch 2466, training loss: 16113.43, average training loss: 18298.73, base loss: 23524.62
[INFO 2017-06-25 17:30:16,888 main.py:50] epoch 2467, training loss: 19274.81, average training loss: 18300.16, base loss: 23525.88
[INFO 2017-06-25 17:30:21,174 main.py:50] epoch 2468, training loss: 20181.26, average training loss: 18301.02, base loss: 23525.41
[INFO 2017-06-25 17:30:25,479 main.py:50] epoch 2469, training loss: 21397.28, average training loss: 18304.95, base loss: 23529.79
[INFO 2017-06-25 17:30:29,789 main.py:50] epoch 2470, training loss: 20497.07, average training loss: 18304.65, base loss: 23527.91
[INFO 2017-06-25 17:30:34,073 main.py:50] epoch 2471, training loss: 21042.54, average training loss: 18305.07, base loss: 23529.40
[INFO 2017-06-25 17:30:38,373 main.py:50] epoch 2472, training loss: 18782.70, average training loss: 18303.99, base loss: 23531.34
[INFO 2017-06-25 17:30:42,668 main.py:50] epoch 2473, training loss: 14726.91, average training loss: 18300.24, base loss: 23526.76
[INFO 2017-06-25 17:30:46,946 main.py:50] epoch 2474, training loss: 22437.37, average training loss: 18303.16, base loss: 23526.77
[INFO 2017-06-25 17:30:51,227 main.py:50] epoch 2475, training loss: 16275.94, average training loss: 18302.16, base loss: 23527.71
[INFO 2017-06-25 17:30:55,509 main.py:50] epoch 2476, training loss: 16000.20, average training loss: 18299.01, base loss: 23524.05
[INFO 2017-06-25 17:30:59,802 main.py:50] epoch 2477, training loss: 16951.10, average training loss: 18296.14, base loss: 23522.74
[INFO 2017-06-25 17:31:04,095 main.py:50] epoch 2478, training loss: 22546.32, average training loss: 18304.03, base loss: 23534.04
[INFO 2017-06-25 17:31:08,400 main.py:50] epoch 2479, training loss: 16668.83, average training loss: 18302.07, base loss: 23533.13
[INFO 2017-06-25 17:31:12,690 main.py:50] epoch 2480, training loss: 22696.59, average training loss: 18308.99, base loss: 23544.19
[INFO 2017-06-25 17:31:16,990 main.py:50] epoch 2481, training loss: 20068.79, average training loss: 18311.65, base loss: 23548.14
[INFO 2017-06-25 17:31:21,284 main.py:50] epoch 2482, training loss: 16452.66, average training loss: 18313.22, base loss: 23549.83
[INFO 2017-06-25 17:31:25,584 main.py:50] epoch 2483, training loss: 21358.48, average training loss: 18317.12, base loss: 23553.60
[INFO 2017-06-25 17:31:29,887 main.py:50] epoch 2484, training loss: 15491.26, average training loss: 18315.80, base loss: 23553.51
[INFO 2017-06-25 17:31:34,164 main.py:50] epoch 2485, training loss: 16535.30, average training loss: 18314.09, base loss: 23548.61
[INFO 2017-06-25 17:31:38,444 main.py:50] epoch 2486, training loss: 17416.93, average training loss: 18306.34, base loss: 23538.50
[INFO 2017-06-25 17:31:42,723 main.py:50] epoch 2487, training loss: 17393.55, average training loss: 18309.76, base loss: 23544.41
[INFO 2017-06-25 17:31:47,033 main.py:50] epoch 2488, training loss: 15383.53, average training loss: 18306.04, base loss: 23540.05
[INFO 2017-06-25 17:31:51,329 main.py:50] epoch 2489, training loss: 18176.35, average training loss: 18306.70, base loss: 23541.59
[INFO 2017-06-25 17:31:55,616 main.py:50] epoch 2490, training loss: 16720.73, average training loss: 18305.48, base loss: 23541.64
[INFO 2017-06-25 17:31:59,910 main.py:50] epoch 2491, training loss: 18458.35, average training loss: 18304.33, base loss: 23541.60
[INFO 2017-06-25 17:32:04,195 main.py:50] epoch 2492, training loss: 23947.04, average training loss: 18309.08, base loss: 23550.28
[INFO 2017-06-25 17:32:08,487 main.py:50] epoch 2493, training loss: 19195.95, average training loss: 18307.02, base loss: 23545.52
[INFO 2017-06-25 17:32:12,781 main.py:50] epoch 2494, training loss: 17193.03, average training loss: 18307.33, base loss: 23545.33
[INFO 2017-06-25 17:32:17,093 main.py:50] epoch 2495, training loss: 20517.38, average training loss: 18309.71, base loss: 23550.60
[INFO 2017-06-25 17:32:21,388 main.py:50] epoch 2496, training loss: 18644.30, average training loss: 18309.86, base loss: 23552.54
[INFO 2017-06-25 17:32:25,657 main.py:50] epoch 2497, training loss: 18834.40, average training loss: 18314.35, base loss: 23559.88
[INFO 2017-06-25 17:32:29,968 main.py:50] epoch 2498, training loss: 17939.05, average training loss: 18316.08, base loss: 23562.29
[INFO 2017-06-25 17:32:34,282 main.py:50] epoch 2499, training loss: 19282.00, average training loss: 18319.17, base loss: 23565.53
[INFO 2017-06-25 17:32:38,549 main.py:50] epoch 2500, training loss: 16197.78, average training loss: 18316.68, base loss: 23562.87
[INFO 2017-06-25 17:32:42,853 main.py:50] epoch 2501, training loss: 14844.90, average training loss: 18317.84, base loss: 23565.69
[INFO 2017-06-25 17:32:47,159 main.py:50] epoch 2502, training loss: 18032.77, average training loss: 18317.15, base loss: 23565.70
[INFO 2017-06-25 17:32:51,430 main.py:50] epoch 2503, training loss: 19760.29, average training loss: 18323.61, base loss: 23575.61
[INFO 2017-06-25 17:32:55,711 main.py:50] epoch 2504, training loss: 15132.40, average training loss: 18311.30, base loss: 23562.41
[INFO 2017-06-25 17:32:59,998 main.py:50] epoch 2505, training loss: 19688.28, average training loss: 18316.62, base loss: 23572.82
[INFO 2017-06-25 17:33:04,311 main.py:50] epoch 2506, training loss: 15013.89, average training loss: 18307.79, base loss: 23559.96
[INFO 2017-06-25 17:33:08,636 main.py:50] epoch 2507, training loss: 23864.05, average training loss: 18309.92, base loss: 23564.28
[INFO 2017-06-25 17:33:12,922 main.py:50] epoch 2508, training loss: 18806.32, average training loss: 18308.15, base loss: 23560.60
[INFO 2017-06-25 17:33:17,191 main.py:50] epoch 2509, training loss: 16920.61, average training loss: 18312.10, base loss: 23565.38
[INFO 2017-06-25 17:33:21,506 main.py:50] epoch 2510, training loss: 23104.32, average training loss: 18317.28, base loss: 23572.22
[INFO 2017-06-25 17:33:25,794 main.py:50] epoch 2511, training loss: 18364.19, average training loss: 18319.76, base loss: 23573.53
[INFO 2017-06-25 17:33:30,082 main.py:50] epoch 2512, training loss: 20060.60, average training loss: 18322.42, base loss: 23577.12
[INFO 2017-06-25 17:33:34,372 main.py:50] epoch 2513, training loss: 14076.77, average training loss: 18320.73, base loss: 23576.32
[INFO 2017-06-25 17:33:38,645 main.py:50] epoch 2514, training loss: 21556.70, average training loss: 18319.05, base loss: 23575.44
[INFO 2017-06-25 17:33:42,943 main.py:50] epoch 2515, training loss: 16932.15, average training loss: 18319.33, base loss: 23575.30
[INFO 2017-06-25 17:33:47,244 main.py:50] epoch 2516, training loss: 18578.60, average training loss: 18318.10, base loss: 23574.35
[INFO 2017-06-25 17:33:51,536 main.py:50] epoch 2517, training loss: 22116.63, average training loss: 18318.82, base loss: 23579.18
[INFO 2017-06-25 17:33:55,833 main.py:50] epoch 2518, training loss: 21387.73, average training loss: 18326.09, base loss: 23590.54
[INFO 2017-06-25 17:34:00,132 main.py:50] epoch 2519, training loss: 18712.43, average training loss: 18324.54, base loss: 23586.55
[INFO 2017-06-25 17:34:04,419 main.py:50] epoch 2520, training loss: 17504.79, average training loss: 18320.37, base loss: 23583.27
[INFO 2017-06-25 17:34:08,715 main.py:50] epoch 2521, training loss: 18668.49, average training loss: 18322.17, base loss: 23584.72
[INFO 2017-06-25 17:34:13,024 main.py:50] epoch 2522, training loss: 12362.58, average training loss: 18317.32, base loss: 23579.82
[INFO 2017-06-25 17:34:17,295 main.py:50] epoch 2523, training loss: 14627.60, average training loss: 18308.82, base loss: 23570.15
[INFO 2017-06-25 17:34:21,594 main.py:50] epoch 2524, training loss: 18517.73, average training loss: 18308.25, base loss: 23569.00
[INFO 2017-06-25 17:34:25,880 main.py:50] epoch 2525, training loss: 19769.26, average training loss: 18308.89, base loss: 23567.93
[INFO 2017-06-25 17:34:30,165 main.py:50] epoch 2526, training loss: 19526.84, average training loss: 18315.47, base loss: 23575.12
[INFO 2017-06-25 17:34:34,450 main.py:50] epoch 2527, training loss: 14376.15, average training loss: 18311.07, base loss: 23566.35
[INFO 2017-06-25 17:34:38,755 main.py:50] epoch 2528, training loss: 19714.21, average training loss: 18311.22, base loss: 23567.24
[INFO 2017-06-25 17:34:43,059 main.py:50] epoch 2529, training loss: 15802.13, average training loss: 18309.96, base loss: 23567.73
[INFO 2017-06-25 17:34:47,365 main.py:50] epoch 2530, training loss: 15065.69, average training loss: 18300.68, base loss: 23557.47
[INFO 2017-06-25 17:34:51,663 main.py:50] epoch 2531, training loss: 21385.22, average training loss: 18306.90, base loss: 23566.66
[INFO 2017-06-25 17:34:55,968 main.py:50] epoch 2532, training loss: 18918.74, average training loss: 18308.20, base loss: 23569.30
[INFO 2017-06-25 17:35:00,255 main.py:50] epoch 2533, training loss: 17106.28, average training loss: 18308.75, base loss: 23568.61
[INFO 2017-06-25 17:35:04,559 main.py:50] epoch 2534, training loss: 18039.61, average training loss: 18309.44, base loss: 23569.81
[INFO 2017-06-25 17:35:08,849 main.py:50] epoch 2535, training loss: 15302.55, average training loss: 18310.52, base loss: 23570.05
[INFO 2017-06-25 17:35:13,151 main.py:50] epoch 2536, training loss: 17239.84, average training loss: 18307.82, base loss: 23566.62
[INFO 2017-06-25 17:35:17,436 main.py:50] epoch 2537, training loss: 13679.66, average training loss: 18305.42, base loss: 23563.38
[INFO 2017-06-25 17:35:21,716 main.py:50] epoch 2538, training loss: 19866.07, average training loss: 18309.61, base loss: 23568.05
[INFO 2017-06-25 17:35:26,012 main.py:50] epoch 2539, training loss: 15984.30, average training loss: 18307.48, base loss: 23564.08
[INFO 2017-06-25 17:35:30,307 main.py:50] epoch 2540, training loss: 17055.87, average training loss: 18305.02, base loss: 23563.26
[INFO 2017-06-25 17:35:34,585 main.py:50] epoch 2541, training loss: 22463.24, average training loss: 18314.41, base loss: 23574.82
[INFO 2017-06-25 17:35:38,875 main.py:50] epoch 2542, training loss: 18105.70, average training loss: 18311.36, base loss: 23571.47
[INFO 2017-06-25 17:35:43,156 main.py:50] epoch 2543, training loss: 16894.94, average training loss: 18309.92, base loss: 23571.60
[INFO 2017-06-25 17:35:47,478 main.py:50] epoch 2544, training loss: 16183.76, average training loss: 18306.74, base loss: 23570.98
[INFO 2017-06-25 17:35:51,742 main.py:50] epoch 2545, training loss: 19121.40, average training loss: 18306.57, base loss: 23572.34
[INFO 2017-06-25 17:35:56,038 main.py:50] epoch 2546, training loss: 20169.15, average training loss: 18307.20, base loss: 23575.23
[INFO 2017-06-25 17:36:00,327 main.py:50] epoch 2547, training loss: 14526.97, average training loss: 18301.47, base loss: 23567.45
[INFO 2017-06-25 17:36:04,603 main.py:50] epoch 2548, training loss: 17763.97, average training loss: 18302.88, base loss: 23569.83
[INFO 2017-06-25 17:36:08,909 main.py:50] epoch 2549, training loss: 18367.30, average training loss: 18297.67, base loss: 23567.37
[INFO 2017-06-25 17:36:13,206 main.py:50] epoch 2550, training loss: 21054.81, average training loss: 18301.40, base loss: 23574.04
[INFO 2017-06-25 17:36:17,505 main.py:50] epoch 2551, training loss: 23768.45, average training loss: 18304.50, base loss: 23574.76
[INFO 2017-06-25 17:36:21,790 main.py:50] epoch 2552, training loss: 17689.42, average training loss: 18304.43, base loss: 23576.03
[INFO 2017-06-25 17:36:26,095 main.py:50] epoch 2553, training loss: 19298.73, average training loss: 18307.12, base loss: 23579.66
[INFO 2017-06-25 17:36:30,388 main.py:50] epoch 2554, training loss: 21263.66, average training loss: 18309.81, base loss: 23581.35
[INFO 2017-06-25 17:36:34,681 main.py:50] epoch 2555, training loss: 21840.46, average training loss: 18315.12, base loss: 23589.03
[INFO 2017-06-25 17:36:38,959 main.py:50] epoch 2556, training loss: 21241.99, average training loss: 18317.94, base loss: 23592.03
[INFO 2017-06-25 17:36:43,225 main.py:50] epoch 2557, training loss: 18934.96, average training loss: 18319.83, base loss: 23594.64
[INFO 2017-06-25 17:36:47,489 main.py:50] epoch 2558, training loss: 18126.06, average training loss: 18320.96, base loss: 23596.16
[INFO 2017-06-25 17:36:51,761 main.py:50] epoch 2559, training loss: 19599.60, average training loss: 18320.78, base loss: 23595.32
[INFO 2017-06-25 17:36:56,055 main.py:50] epoch 2560, training loss: 17937.68, average training loss: 18321.30, base loss: 23596.95
[INFO 2017-06-25 17:37:00,352 main.py:50] epoch 2561, training loss: 16052.87, average training loss: 18314.78, base loss: 23589.39
[INFO 2017-06-25 17:37:04,662 main.py:50] epoch 2562, training loss: 19221.98, average training loss: 18317.85, base loss: 23592.92
[INFO 2017-06-25 17:37:08,947 main.py:50] epoch 2563, training loss: 18164.69, average training loss: 18318.04, base loss: 23593.76
[INFO 2017-06-25 17:37:13,218 main.py:50] epoch 2564, training loss: 14308.60, average training loss: 18317.35, base loss: 23594.64
[INFO 2017-06-25 17:37:17,517 main.py:50] epoch 2565, training loss: 20191.93, average training loss: 18314.89, base loss: 23592.82
[INFO 2017-06-25 17:37:21,772 main.py:50] epoch 2566, training loss: 20495.71, average training loss: 18315.66, base loss: 23595.69
[INFO 2017-06-25 17:37:26,050 main.py:50] epoch 2567, training loss: 21473.13, average training loss: 18318.82, base loss: 23603.84
[INFO 2017-06-25 17:37:30,338 main.py:50] epoch 2568, training loss: 18529.14, average training loss: 18321.46, base loss: 23606.41
[INFO 2017-06-25 17:37:34,619 main.py:50] epoch 2569, training loss: 20604.75, average training loss: 18324.97, base loss: 23611.88
[INFO 2017-06-25 17:37:38,893 main.py:50] epoch 2570, training loss: 19636.47, average training loss: 18328.98, base loss: 23618.03
[INFO 2017-06-25 17:37:43,173 main.py:50] epoch 2571, training loss: 21931.83, average training loss: 18330.56, base loss: 23621.26
[INFO 2017-06-25 17:37:47,457 main.py:50] epoch 2572, training loss: 20840.89, average training loss: 18331.03, base loss: 23623.05
[INFO 2017-06-25 17:37:51,758 main.py:50] epoch 2573, training loss: 18302.81, average training loss: 18333.61, base loss: 23626.56
[INFO 2017-06-25 17:37:56,044 main.py:50] epoch 2574, training loss: 16217.16, average training loss: 18333.08, base loss: 23626.58
[INFO 2017-06-25 17:38:00,313 main.py:50] epoch 2575, training loss: 21502.34, average training loss: 18333.65, base loss: 23630.93
[INFO 2017-06-25 17:38:04,609 main.py:50] epoch 2576, training loss: 20838.28, average training loss: 18335.93, base loss: 23633.43
[INFO 2017-06-25 17:38:08,912 main.py:50] epoch 2577, training loss: 16566.07, average training loss: 18334.41, base loss: 23628.78
[INFO 2017-06-25 17:38:13,199 main.py:50] epoch 2578, training loss: 20790.79, average training loss: 18339.41, base loss: 23633.57
[INFO 2017-06-25 17:38:17,471 main.py:50] epoch 2579, training loss: 20647.65, average training loss: 18339.46, base loss: 23632.13
[INFO 2017-06-25 17:38:21,760 main.py:50] epoch 2580, training loss: 13748.08, average training loss: 18336.83, base loss: 23629.30
[INFO 2017-06-25 17:38:26,073 main.py:50] epoch 2581, training loss: 18326.97, average training loss: 18336.40, base loss: 23629.89
[INFO 2017-06-25 17:38:30,369 main.py:50] epoch 2582, training loss: 19993.48, average training loss: 18337.02, base loss: 23628.87
[INFO 2017-06-25 17:38:34,674 main.py:50] epoch 2583, training loss: 19878.60, average training loss: 18335.63, base loss: 23627.30
[INFO 2017-06-25 17:38:38,947 main.py:50] epoch 2584, training loss: 15164.21, average training loss: 18336.38, base loss: 23629.79
[INFO 2017-06-25 17:38:43,262 main.py:50] epoch 2585, training loss: 19261.35, average training loss: 18340.47, base loss: 23636.94
[INFO 2017-06-25 17:38:47,548 main.py:50] epoch 2586, training loss: 19964.80, average training loss: 18344.84, base loss: 23640.88
[INFO 2017-06-25 17:38:51,840 main.py:50] epoch 2587, training loss: 20788.62, average training loss: 18349.93, base loss: 23647.96
[INFO 2017-06-25 17:38:56,148 main.py:50] epoch 2588, training loss: 21032.60, average training loss: 18354.63, base loss: 23654.32
[INFO 2017-06-25 17:39:00,427 main.py:50] epoch 2589, training loss: 16849.18, average training loss: 18354.10, base loss: 23653.91
[INFO 2017-06-25 17:39:04,735 main.py:50] epoch 2590, training loss: 17431.21, average training loss: 18351.16, base loss: 23646.59
[INFO 2017-06-25 17:39:09,025 main.py:50] epoch 2591, training loss: 18765.23, average training loss: 18354.90, base loss: 23652.38
[INFO 2017-06-25 17:39:13,316 main.py:50] epoch 2592, training loss: 16570.55, average training loss: 18355.86, base loss: 23655.96
[INFO 2017-06-25 17:39:17,617 main.py:50] epoch 2593, training loss: 17743.54, average training loss: 18355.57, base loss: 23657.97
[INFO 2017-06-25 17:39:21,898 main.py:50] epoch 2594, training loss: 24808.39, average training loss: 18362.06, base loss: 23666.56
[INFO 2017-06-25 17:39:26,213 main.py:50] epoch 2595, training loss: 19022.91, average training loss: 18363.72, base loss: 23667.54
[INFO 2017-06-25 17:39:30,531 main.py:50] epoch 2596, training loss: 21547.73, average training loss: 18370.68, base loss: 23676.41
[INFO 2017-06-25 17:39:34,828 main.py:50] epoch 2597, training loss: 20816.92, average training loss: 18372.65, base loss: 23679.33
[INFO 2017-06-25 17:39:39,112 main.py:50] epoch 2598, training loss: 19068.86, average training loss: 18368.87, base loss: 23677.14
[INFO 2017-06-25 17:39:43,408 main.py:50] epoch 2599, training loss: 15673.43, average training loss: 18366.69, base loss: 23674.04
[INFO 2017-06-25 17:39:47,692 main.py:50] epoch 2600, training loss: 14103.77, average training loss: 18362.65, base loss: 23666.37
[INFO 2017-06-25 17:39:51,991 main.py:50] epoch 2601, training loss: 17566.64, average training loss: 18362.20, base loss: 23667.40
[INFO 2017-06-25 17:39:56,258 main.py:50] epoch 2602, training loss: 13583.34, average training loss: 18360.14, base loss: 23664.34
[INFO 2017-06-25 17:40:00,524 main.py:50] epoch 2603, training loss: 18559.18, average training loss: 18359.71, base loss: 23667.04
[INFO 2017-06-25 17:40:04,819 main.py:50] epoch 2604, training loss: 18917.35, average training loss: 18360.73, base loss: 23668.44
[INFO 2017-06-25 17:40:09,089 main.py:50] epoch 2605, training loss: 19188.74, average training loss: 18361.52, base loss: 23671.52
[INFO 2017-06-25 17:40:13,359 main.py:50] epoch 2606, training loss: 18608.97, average training loss: 18362.00, base loss: 23672.31
[INFO 2017-06-25 17:40:17,634 main.py:50] epoch 2607, training loss: 13510.38, average training loss: 18359.53, base loss: 23668.97
[INFO 2017-06-25 17:40:21,909 main.py:50] epoch 2608, training loss: 15711.45, average training loss: 18358.19, base loss: 23667.22
[INFO 2017-06-25 17:40:26,193 main.py:50] epoch 2609, training loss: 17411.97, average training loss: 18353.63, base loss: 23662.75
[INFO 2017-06-25 17:40:30,474 main.py:50] epoch 2610, training loss: 16342.73, average training loss: 18353.21, base loss: 23663.05
[INFO 2017-06-25 17:40:34,749 main.py:50] epoch 2611, training loss: 17390.27, average training loss: 18353.36, base loss: 23660.88
[INFO 2017-06-25 17:40:39,052 main.py:50] epoch 2612, training loss: 15512.62, average training loss: 18353.89, base loss: 23663.67
[INFO 2017-06-25 17:40:43,355 main.py:50] epoch 2613, training loss: 15288.73, average training loss: 18345.52, base loss: 23655.20
[INFO 2017-06-25 17:40:47,667 main.py:50] epoch 2614, training loss: 17890.33, average training loss: 18345.55, base loss: 23656.83
[INFO 2017-06-25 17:40:51,959 main.py:50] epoch 2615, training loss: 22260.10, average training loss: 18351.92, base loss: 23667.09
[INFO 2017-06-25 17:40:56,230 main.py:50] epoch 2616, training loss: 19352.65, average training loss: 18354.31, base loss: 23676.31
[INFO 2017-06-25 17:41:00,523 main.py:50] epoch 2617, training loss: 18662.52, average training loss: 18352.04, base loss: 23674.19
[INFO 2017-06-25 17:41:04,804 main.py:50] epoch 2618, training loss: 18807.39, average training loss: 18345.87, base loss: 23668.62
[INFO 2017-06-25 17:41:09,105 main.py:50] epoch 2619, training loss: 16658.65, average training loss: 18342.20, base loss: 23666.53
[INFO 2017-06-25 17:41:13,450 main.py:50] epoch 2620, training loss: 28004.59, average training loss: 18350.89, base loss: 23675.53
[INFO 2017-06-25 17:41:17,753 main.py:50] epoch 2621, training loss: 17819.35, average training loss: 18348.59, base loss: 23676.09
[INFO 2017-06-25 17:41:22,043 main.py:50] epoch 2622, training loss: 18041.60, average training loss: 18347.75, base loss: 23676.02
[INFO 2017-06-25 17:41:26,353 main.py:50] epoch 2623, training loss: 19474.48, average training loss: 18348.20, base loss: 23674.10
[INFO 2017-06-25 17:41:30,670 main.py:50] epoch 2624, training loss: 19491.15, average training loss: 18343.24, base loss: 23669.50
[INFO 2017-06-25 17:41:34,938 main.py:50] epoch 2625, training loss: 17569.93, average training loss: 18339.65, base loss: 23664.44
[INFO 2017-06-25 17:41:39,240 main.py:50] epoch 2626, training loss: 15787.58, average training loss: 18335.39, base loss: 23659.45
[INFO 2017-06-25 17:41:43,517 main.py:50] epoch 2627, training loss: 19509.64, average training loss: 18335.72, base loss: 23662.58
[INFO 2017-06-25 17:41:47,846 main.py:50] epoch 2628, training loss: 16753.79, average training loss: 18338.34, base loss: 23666.69
[INFO 2017-06-25 17:41:52,173 main.py:50] epoch 2629, training loss: 19813.12, average training loss: 18340.35, base loss: 23670.75
[INFO 2017-06-25 17:41:56,445 main.py:50] epoch 2630, training loss: 17552.63, average training loss: 18336.61, base loss: 23669.00
[INFO 2017-06-25 17:42:00,725 main.py:50] epoch 2631, training loss: 13924.63, average training loss: 18332.28, base loss: 23664.66
[INFO 2017-06-25 17:42:05,057 main.py:50] epoch 2632, training loss: 17775.32, average training loss: 18332.24, base loss: 23666.88
[INFO 2017-06-25 17:42:09,363 main.py:50] epoch 2633, training loss: 22796.86, average training loss: 18333.78, base loss: 23666.50
[INFO 2017-06-25 17:42:13,670 main.py:50] epoch 2634, training loss: 19464.92, average training loss: 18335.80, base loss: 23668.57
[INFO 2017-06-25 17:42:17,960 main.py:50] epoch 2635, training loss: 15921.41, average training loss: 18331.82, base loss: 23667.46
[INFO 2017-06-25 17:42:22,242 main.py:50] epoch 2636, training loss: 14115.32, average training loss: 18327.64, base loss: 23666.09
[INFO 2017-06-25 17:42:26,520 main.py:50] epoch 2637, training loss: 17297.82, average training loss: 18326.66, base loss: 23665.94
[INFO 2017-06-25 17:42:30,795 main.py:50] epoch 2638, training loss: 17009.97, average training loss: 18323.11, base loss: 23663.93
[INFO 2017-06-25 17:42:35,087 main.py:50] epoch 2639, training loss: 19938.53, average training loss: 18324.20, base loss: 23665.68
[INFO 2017-06-25 17:42:39,372 main.py:50] epoch 2640, training loss: 17838.51, average training loss: 18324.54, base loss: 23666.78
[INFO 2017-06-25 17:42:43,689 main.py:50] epoch 2641, training loss: 27551.13, average training loss: 18332.96, base loss: 23679.46
[INFO 2017-06-25 17:42:47,998 main.py:50] epoch 2642, training loss: 19300.74, average training loss: 18333.18, base loss: 23682.58
[INFO 2017-06-25 17:42:52,297 main.py:50] epoch 2643, training loss: 15331.86, average training loss: 18328.97, base loss: 23676.53
[INFO 2017-06-25 17:42:56,570 main.py:50] epoch 2644, training loss: 18703.99, average training loss: 18333.86, base loss: 23682.44
[INFO 2017-06-25 17:43:00,852 main.py:50] epoch 2645, training loss: 13635.11, average training loss: 18326.81, base loss: 23672.72
[INFO 2017-06-25 17:43:05,131 main.py:50] epoch 2646, training loss: 21100.33, average training loss: 18332.76, base loss: 23679.45
[INFO 2017-06-25 17:43:09,395 main.py:50] epoch 2647, training loss: 14269.80, average training loss: 18326.79, base loss: 23673.80
[INFO 2017-06-25 17:43:13,687 main.py:50] epoch 2648, training loss: 19331.00, average training loss: 18328.71, base loss: 23678.69
[INFO 2017-06-25 17:43:17,987 main.py:50] epoch 2649, training loss: 17313.19, average training loss: 18331.96, base loss: 23682.54
[INFO 2017-06-25 17:43:22,269 main.py:50] epoch 2650, training loss: 18170.45, average training loss: 18329.14, base loss: 23680.70
[INFO 2017-06-25 17:43:26,564 main.py:50] epoch 2651, training loss: 17305.34, average training loss: 18323.14, base loss: 23674.55
[INFO 2017-06-25 17:43:30,865 main.py:50] epoch 2652, training loss: 18816.75, average training loss: 18324.87, base loss: 23679.10
[INFO 2017-06-25 17:43:35,165 main.py:50] epoch 2653, training loss: 15139.20, average training loss: 18323.64, base loss: 23679.44
[INFO 2017-06-25 17:43:39,453 main.py:50] epoch 2654, training loss: 17377.59, average training loss: 18324.87, base loss: 23681.05
[INFO 2017-06-25 17:43:43,745 main.py:50] epoch 2655, training loss: 14916.63, average training loss: 18318.61, base loss: 23675.13
[INFO 2017-06-25 17:43:48,017 main.py:50] epoch 2656, training loss: 15225.54, average training loss: 18317.85, base loss: 23675.57
[INFO 2017-06-25 17:43:52,309 main.py:50] epoch 2657, training loss: 18733.40, average training loss: 18317.95, base loss: 23678.13
[INFO 2017-06-25 17:43:56,595 main.py:50] epoch 2658, training loss: 17487.23, average training loss: 18315.85, base loss: 23679.78
[INFO 2017-06-25 17:44:00,858 main.py:50] epoch 2659, training loss: 17454.19, average training loss: 18319.07, base loss: 23684.12
[INFO 2017-06-25 17:44:05,186 main.py:50] epoch 2660, training loss: 17747.72, average training loss: 18320.51, base loss: 23684.97
[INFO 2017-06-25 17:44:09,474 main.py:50] epoch 2661, training loss: 17207.87, average training loss: 18321.68, base loss: 23686.88
[INFO 2017-06-25 17:44:13,811 main.py:50] epoch 2662, training loss: 20448.34, average training loss: 18325.42, base loss: 23692.94
[INFO 2017-06-25 17:44:18,150 main.py:50] epoch 2663, training loss: 15205.54, average training loss: 18322.04, base loss: 23688.85
[INFO 2017-06-25 17:44:22,457 main.py:50] epoch 2664, training loss: 19635.94, average training loss: 18319.31, base loss: 23688.48
[INFO 2017-06-25 17:44:26,738 main.py:50] epoch 2665, training loss: 18807.90, average training loss: 18321.11, base loss: 23689.75
[INFO 2017-06-25 17:44:31,020 main.py:50] epoch 2666, training loss: 20077.29, average training loss: 18321.75, base loss: 23691.51
[INFO 2017-06-25 17:44:35,361 main.py:50] epoch 2667, training loss: 16484.48, average training loss: 18320.37, base loss: 23690.72
[INFO 2017-06-25 17:44:39,647 main.py:50] epoch 2668, training loss: 16400.29, average training loss: 18317.08, base loss: 23687.94
[INFO 2017-06-25 17:44:43,937 main.py:50] epoch 2669, training loss: 23903.27, average training loss: 18319.92, base loss: 23691.90
[INFO 2017-06-25 17:44:48,272 main.py:50] epoch 2670, training loss: 19501.55, average training loss: 18319.56, base loss: 23692.04
[INFO 2017-06-25 17:44:52,575 main.py:50] epoch 2671, training loss: 20793.62, average training loss: 18321.82, base loss: 23693.62
[INFO 2017-06-25 17:44:56,883 main.py:50] epoch 2672, training loss: 19067.94, average training loss: 18320.81, base loss: 23691.15
[INFO 2017-06-25 17:45:01,197 main.py:50] epoch 2673, training loss: 16538.83, average training loss: 18318.97, base loss: 23687.49
[INFO 2017-06-25 17:45:05,509 main.py:50] epoch 2674, training loss: 17967.01, average training loss: 18317.67, base loss: 23685.57
[INFO 2017-06-25 17:45:09,795 main.py:50] epoch 2675, training loss: 15066.61, average training loss: 18310.80, base loss: 23675.83
[INFO 2017-06-25 17:45:14,094 main.py:50] epoch 2676, training loss: 17268.55, average training loss: 18310.57, base loss: 23678.21
[INFO 2017-06-25 17:45:18,376 main.py:50] epoch 2677, training loss: 15224.34, average training loss: 18306.29, base loss: 23671.00
[INFO 2017-06-25 17:45:22,685 main.py:50] epoch 2678, training loss: 16426.99, average training loss: 18303.29, base loss: 23668.32
[INFO 2017-06-25 17:45:26,993 main.py:50] epoch 2679, training loss: 17043.64, average training loss: 18305.34, base loss: 23669.99
[INFO 2017-06-25 17:45:31,301 main.py:50] epoch 2680, training loss: 20464.73, average training loss: 18304.39, base loss: 23667.32
[INFO 2017-06-25 17:45:35,589 main.py:50] epoch 2681, training loss: 21358.51, average training loss: 18307.38, base loss: 23669.23
[INFO 2017-06-25 17:45:39,913 main.py:50] epoch 2682, training loss: 20774.17, average training loss: 18310.27, base loss: 23672.08
[INFO 2017-06-25 17:45:44,203 main.py:50] epoch 2683, training loss: 18035.29, average training loss: 18310.78, base loss: 23671.96
[INFO 2017-06-25 17:45:48,492 main.py:50] epoch 2684, training loss: 15104.98, average training loss: 18308.08, base loss: 23669.33
[INFO 2017-06-25 17:45:52,805 main.py:50] epoch 2685, training loss: 19200.27, average training loss: 18307.68, base loss: 23669.46
[INFO 2017-06-25 17:45:57,113 main.py:50] epoch 2686, training loss: 14305.00, average training loss: 18306.50, base loss: 23669.43
[INFO 2017-06-25 17:46:01,383 main.py:50] epoch 2687, training loss: 23153.61, average training loss: 18313.17, base loss: 23679.66
[INFO 2017-06-25 17:46:05,687 main.py:50] epoch 2688, training loss: 24966.55, average training loss: 18323.19, base loss: 23691.18
[INFO 2017-06-25 17:46:09,973 main.py:50] epoch 2689, training loss: 16606.30, average training loss: 18322.27, base loss: 23692.44
[INFO 2017-06-25 17:46:14,258 main.py:50] epoch 2690, training loss: 17866.60, average training loss: 18321.10, base loss: 23693.26
[INFO 2017-06-25 17:46:18,532 main.py:50] epoch 2691, training loss: 21771.18, average training loss: 18326.73, base loss: 23702.38
[INFO 2017-06-25 17:46:22,819 main.py:50] epoch 2692, training loss: 18432.75, average training loss: 18328.70, base loss: 23707.51
[INFO 2017-06-25 17:46:27,070 main.py:50] epoch 2693, training loss: 19723.66, average training loss: 18329.59, base loss: 23710.41
[INFO 2017-06-25 17:46:31,373 main.py:50] epoch 2694, training loss: 20504.79, average training loss: 18332.15, base loss: 23713.99
[INFO 2017-06-25 17:46:35,678 main.py:50] epoch 2695, training loss: 14653.13, average training loss: 18327.92, base loss: 23707.28
[INFO 2017-06-25 17:46:39,959 main.py:50] epoch 2696, training loss: 17278.68, average training loss: 18329.04, base loss: 23706.90
[INFO 2017-06-25 17:46:44,258 main.py:50] epoch 2697, training loss: 16475.10, average training loss: 18327.87, base loss: 23705.60
[INFO 2017-06-25 17:46:48,523 main.py:50] epoch 2698, training loss: 19398.61, average training loss: 18327.18, base loss: 23706.51
[INFO 2017-06-25 17:46:52,873 main.py:50] epoch 2699, training loss: 22507.67, average training loss: 18331.01, base loss: 23712.94
[INFO 2017-06-25 17:46:57,179 main.py:50] epoch 2700, training loss: 15819.58, average training loss: 18330.05, base loss: 23715.16
[INFO 2017-06-25 17:47:01,466 main.py:50] epoch 2701, training loss: 21630.77, average training loss: 18333.55, base loss: 23719.80
[INFO 2017-06-25 17:47:05,736 main.py:50] epoch 2702, training loss: 16287.50, average training loss: 18334.56, base loss: 23718.31
[INFO 2017-06-25 17:47:10,047 main.py:50] epoch 2703, training loss: 19080.88, average training loss: 18337.74, base loss: 23723.37
[INFO 2017-06-25 17:47:14,322 main.py:50] epoch 2704, training loss: 22584.95, average training loss: 18338.69, base loss: 23727.31
[INFO 2017-06-25 17:47:18,604 main.py:50] epoch 2705, training loss: 18140.86, average training loss: 18338.04, base loss: 23728.95
[INFO 2017-06-25 17:47:22,858 main.py:50] epoch 2706, training loss: 16339.83, average training loss: 18336.00, base loss: 23725.70
[INFO 2017-06-25 17:47:27,141 main.py:50] epoch 2707, training loss: 19226.16, average training loss: 18337.12, base loss: 23730.13
[INFO 2017-06-25 17:47:31,421 main.py:50] epoch 2708, training loss: 17875.77, average training loss: 18338.47, base loss: 23733.86
[INFO 2017-06-25 17:47:35,701 main.py:50] epoch 2709, training loss: 14119.83, average training loss: 18336.69, base loss: 23729.36
[INFO 2017-06-25 17:47:39,981 main.py:50] epoch 2710, training loss: 17965.96, average training loss: 18336.86, base loss: 23729.71
[INFO 2017-06-25 17:47:44,290 main.py:50] epoch 2711, training loss: 19192.40, average training loss: 18340.18, base loss: 23731.57
[INFO 2017-06-25 17:47:48,582 main.py:50] epoch 2712, training loss: 13217.94, average training loss: 18337.36, base loss: 23727.72
[INFO 2017-06-25 17:47:52,885 main.py:50] epoch 2713, training loss: 16945.47, average training loss: 18335.53, base loss: 23724.61
[INFO 2017-06-25 17:47:57,187 main.py:50] epoch 2714, training loss: 18407.09, average training loss: 18338.54, base loss: 23728.37
[INFO 2017-06-25 17:48:01,507 main.py:50] epoch 2715, training loss: 16077.15, average training loss: 18333.47, base loss: 23720.19
[INFO 2017-06-25 17:48:05,817 main.py:50] epoch 2716, training loss: 17719.41, average training loss: 18332.87, base loss: 23718.46
[INFO 2017-06-25 17:48:10,102 main.py:50] epoch 2717, training loss: 15447.58, average training loss: 18326.28, base loss: 23713.75
[INFO 2017-06-25 17:48:14,396 main.py:50] epoch 2718, training loss: 20222.29, average training loss: 18327.48, base loss: 23715.41
[INFO 2017-06-25 17:48:18,705 main.py:50] epoch 2719, training loss: 22330.21, average training loss: 18332.08, base loss: 23716.91
[INFO 2017-06-25 17:48:23,019 main.py:50] epoch 2720, training loss: 16333.95, average training loss: 18326.53, base loss: 23710.58
[INFO 2017-06-25 17:48:27,329 main.py:50] epoch 2721, training loss: 19631.45, average training loss: 18327.60, base loss: 23713.09
[INFO 2017-06-25 17:48:31,629 main.py:50] epoch 2722, training loss: 19747.10, average training loss: 18330.33, base loss: 23715.84
[INFO 2017-06-25 17:48:35,919 main.py:50] epoch 2723, training loss: 16102.85, average training loss: 18328.39, base loss: 23712.53
[INFO 2017-06-25 17:48:40,207 main.py:50] epoch 2724, training loss: 21444.89, average training loss: 18332.58, base loss: 23716.05
[INFO 2017-06-25 17:48:44,485 main.py:50] epoch 2725, training loss: 19653.45, average training loss: 18331.78, base loss: 23717.36
[INFO 2017-06-25 17:48:48,779 main.py:50] epoch 2726, training loss: 19094.26, average training loss: 18333.92, base loss: 23722.39
[INFO 2017-06-25 17:48:53,065 main.py:50] epoch 2727, training loss: 23959.41, average training loss: 18337.39, base loss: 23726.16
[INFO 2017-06-25 17:48:57,374 main.py:50] epoch 2728, training loss: 18060.14, average training loss: 18337.75, base loss: 23727.76
[INFO 2017-06-25 17:49:01,672 main.py:50] epoch 2729, training loss: 18009.74, average training loss: 18341.32, base loss: 23733.92
[INFO 2017-06-25 17:49:05,957 main.py:50] epoch 2730, training loss: 18730.09, average training loss: 18339.26, base loss: 23733.71
[INFO 2017-06-25 17:49:10,252 main.py:50] epoch 2731, training loss: 22523.56, average training loss: 18343.39, base loss: 23739.62
[INFO 2017-06-25 17:49:14,525 main.py:50] epoch 2732, training loss: 21121.41, average training loss: 18342.69, base loss: 23739.12
[INFO 2017-06-25 17:49:18,848 main.py:50] epoch 2733, training loss: 21047.83, average training loss: 18349.57, base loss: 23748.84
[INFO 2017-06-25 17:49:23,150 main.py:50] epoch 2734, training loss: 15736.38, average training loss: 18343.59, base loss: 23740.92
[INFO 2017-06-25 17:49:27,437 main.py:50] epoch 2735, training loss: 16017.32, average training loss: 18336.71, base loss: 23730.54
[INFO 2017-06-25 17:49:31,759 main.py:50] epoch 2736, training loss: 19194.48, average training loss: 18334.06, base loss: 23728.49
[INFO 2017-06-25 17:49:36,048 main.py:50] epoch 2737, training loss: 18708.16, average training loss: 18335.50, base loss: 23730.48
[INFO 2017-06-25 17:49:40,327 main.py:50] epoch 2738, training loss: 18790.30, average training loss: 18333.67, base loss: 23727.66
[INFO 2017-06-25 17:49:44,614 main.py:50] epoch 2739, training loss: 17036.63, average training loss: 18329.30, base loss: 23722.40
[INFO 2017-06-25 17:49:48,904 main.py:50] epoch 2740, training loss: 17248.86, average training loss: 18328.48, base loss: 23721.18
[INFO 2017-06-25 17:49:53,158 main.py:50] epoch 2741, training loss: 20853.55, average training loss: 18326.85, base loss: 23715.64
[INFO 2017-06-25 17:49:57,448 main.py:50] epoch 2742, training loss: 16848.81, average training loss: 18327.21, base loss: 23718.84
[INFO 2017-06-25 17:50:01,747 main.py:50] epoch 2743, training loss: 15415.37, average training loss: 18324.15, base loss: 23713.77
[INFO 2017-06-25 17:50:06,030 main.py:50] epoch 2744, training loss: 14759.71, average training loss: 18320.68, base loss: 23712.42
[INFO 2017-06-25 17:50:10,340 main.py:50] epoch 2745, training loss: 18366.68, average training loss: 18319.13, base loss: 23711.17
[INFO 2017-06-25 17:50:14,630 main.py:50] epoch 2746, training loss: 16641.29, average training loss: 18316.85, base loss: 23706.98
[INFO 2017-06-25 17:50:18,916 main.py:50] epoch 2747, training loss: 18412.61, average training loss: 18317.90, base loss: 23707.86
[INFO 2017-06-25 17:50:23,235 main.py:50] epoch 2748, training loss: 21925.93, average training loss: 18324.78, base loss: 23719.11
[INFO 2017-06-25 17:50:27,539 main.py:50] epoch 2749, training loss: 20501.54, average training loss: 18330.16, base loss: 23725.56
[INFO 2017-06-25 17:50:31,828 main.py:50] epoch 2750, training loss: 23584.83, average training loss: 18338.59, base loss: 23740.34
[INFO 2017-06-25 17:50:36,150 main.py:50] epoch 2751, training loss: 17559.55, average training loss: 18339.02, base loss: 23741.22
[INFO 2017-06-25 17:50:40,439 main.py:50] epoch 2752, training loss: 16084.53, average training loss: 18337.96, base loss: 23741.34
[INFO 2017-06-25 17:50:44,732 main.py:50] epoch 2753, training loss: 15344.28, average training loss: 18338.39, base loss: 23742.88
[INFO 2017-06-25 17:50:49,005 main.py:50] epoch 2754, training loss: 15317.36, average training loss: 18338.87, base loss: 23740.01
[INFO 2017-06-25 17:50:53,312 main.py:50] epoch 2755, training loss: 15574.51, average training loss: 18336.55, base loss: 23735.90
[INFO 2017-06-25 17:50:57,594 main.py:50] epoch 2756, training loss: 19921.52, average training loss: 18337.21, base loss: 23738.25
[INFO 2017-06-25 17:51:01,924 main.py:50] epoch 2757, training loss: 19888.30, average training loss: 18339.65, base loss: 23743.33
[INFO 2017-06-25 17:51:06,242 main.py:50] epoch 2758, training loss: 16409.42, average training loss: 18339.26, base loss: 23741.64
[INFO 2017-06-25 17:51:10,534 main.py:50] epoch 2759, training loss: 15641.91, average training loss: 18334.80, base loss: 23739.19
[INFO 2017-06-25 17:51:14,829 main.py:50] epoch 2760, training loss: 20764.09, average training loss: 18339.30, base loss: 23742.26
[INFO 2017-06-25 17:51:19,137 main.py:50] epoch 2761, training loss: 17079.03, average training loss: 18336.84, base loss: 23741.97
[INFO 2017-06-25 17:51:23,434 main.py:50] epoch 2762, training loss: 16924.25, average training loss: 18333.98, base loss: 23737.45
[INFO 2017-06-25 17:51:27,737 main.py:50] epoch 2763, training loss: 16747.50, average training loss: 18333.29, base loss: 23737.62
[INFO 2017-06-25 17:51:32,039 main.py:50] epoch 2764, training loss: 23352.77, average training loss: 18340.64, base loss: 23743.20
[INFO 2017-06-25 17:51:36,301 main.py:50] epoch 2765, training loss: 16974.31, average training loss: 18338.77, base loss: 23742.08
[INFO 2017-06-25 17:51:40,588 main.py:50] epoch 2766, training loss: 19362.45, average training loss: 18342.47, base loss: 23747.24
[INFO 2017-06-25 17:51:44,893 main.py:50] epoch 2767, training loss: 18683.05, average training loss: 18338.73, base loss: 23744.00
[INFO 2017-06-25 17:51:49,160 main.py:50] epoch 2768, training loss: 19280.71, average training loss: 18342.96, base loss: 23751.01
[INFO 2017-06-25 17:51:53,435 main.py:50] epoch 2769, training loss: 17166.65, average training loss: 18342.78, base loss: 23753.80
[INFO 2017-06-25 17:51:57,728 main.py:50] epoch 2770, training loss: 13690.48, average training loss: 18340.94, base loss: 23753.58
[INFO 2017-06-25 17:52:02,011 main.py:50] epoch 2771, training loss: 14728.07, average training loss: 18328.23, base loss: 23741.45
[INFO 2017-06-25 17:52:06,278 main.py:50] epoch 2772, training loss: 15492.06, average training loss: 18327.90, base loss: 23742.43
[INFO 2017-06-25 17:52:10,563 main.py:50] epoch 2773, training loss: 17036.01, average training loss: 18324.95, base loss: 23739.34
[INFO 2017-06-25 17:52:14,828 main.py:50] epoch 2774, training loss: 15619.46, average training loss: 18323.16, base loss: 23737.30
[INFO 2017-06-25 17:52:19,117 main.py:50] epoch 2775, training loss: 15829.57, average training loss: 18319.16, base loss: 23732.45
[INFO 2017-06-25 17:52:23,384 main.py:50] epoch 2776, training loss: 21681.60, average training loss: 18324.80, base loss: 23739.87
[INFO 2017-06-25 17:52:27,678 main.py:50] epoch 2777, training loss: 18224.05, average training loss: 18329.03, base loss: 23745.73
[INFO 2017-06-25 17:52:31,973 main.py:50] epoch 2778, training loss: 15200.14, average training loss: 18328.73, base loss: 23745.39
[INFO 2017-06-25 17:52:36,265 main.py:50] epoch 2779, training loss: 15652.92, average training loss: 18321.77, base loss: 23738.57
[INFO 2017-06-25 17:52:40,535 main.py:50] epoch 2780, training loss: 18040.04, average training loss: 18322.33, base loss: 23740.08
[INFO 2017-06-25 17:52:44,824 main.py:50] epoch 2781, training loss: 13507.98, average training loss: 18318.34, base loss: 23734.25
[INFO 2017-06-25 17:52:49,139 main.py:50] epoch 2782, training loss: 14268.52, average training loss: 18318.13, base loss: 23735.14
[INFO 2017-06-25 17:52:53,450 main.py:50] epoch 2783, training loss: 18155.75, average training loss: 18317.24, base loss: 23732.99
[INFO 2017-06-25 17:52:57,750 main.py:50] epoch 2784, training loss: 20259.89, average training loss: 18319.90, base loss: 23738.18
[INFO 2017-06-25 17:53:02,026 main.py:50] epoch 2785, training loss: 18927.00, average training loss: 18319.67, base loss: 23736.27
[INFO 2017-06-25 17:53:06,301 main.py:50] epoch 2786, training loss: 18596.24, average training loss: 18318.26, base loss: 23736.37
[INFO 2017-06-25 17:53:10,581 main.py:50] epoch 2787, training loss: 16320.71, average training loss: 18315.05, base loss: 23734.31
[INFO 2017-06-25 17:53:14,876 main.py:50] epoch 2788, training loss: 18122.63, average training loss: 18315.79, base loss: 23734.72
[INFO 2017-06-25 17:53:19,206 main.py:50] epoch 2789, training loss: 16918.71, average training loss: 18313.57, base loss: 23729.92
[INFO 2017-06-25 17:53:23,515 main.py:50] epoch 2790, training loss: 19513.71, average training loss: 18312.20, base loss: 23728.69
[INFO 2017-06-25 17:53:27,794 main.py:50] epoch 2791, training loss: 16922.49, average training loss: 18311.90, base loss: 23728.81
[INFO 2017-06-25 17:53:32,094 main.py:50] epoch 2792, training loss: 19443.97, average training loss: 18312.67, base loss: 23730.66
[INFO 2017-06-25 17:53:36,364 main.py:50] epoch 2793, training loss: 16761.73, average training loss: 18309.85, base loss: 23724.40
[INFO 2017-06-25 17:53:40,651 main.py:50] epoch 2794, training loss: 18610.26, average training loss: 18312.56, base loss: 23727.62
[INFO 2017-06-25 17:53:44,933 main.py:50] epoch 2795, training loss: 19674.29, average training loss: 18316.23, base loss: 23732.00
[INFO 2017-06-25 17:53:49,229 main.py:50] epoch 2796, training loss: 14196.01, average training loss: 18314.40, base loss: 23728.43
[INFO 2017-06-25 17:53:53,532 main.py:50] epoch 2797, training loss: 17392.37, average training loss: 18314.03, base loss: 23729.41
[INFO 2017-06-25 17:53:57,819 main.py:50] epoch 2798, training loss: 16729.26, average training loss: 18312.99, base loss: 23728.40
[INFO 2017-06-25 17:54:02,108 main.py:50] epoch 2799, training loss: 18030.52, average training loss: 18314.62, base loss: 23729.56
[INFO 2017-06-25 17:54:06,406 main.py:50] epoch 2800, training loss: 20827.86, average training loss: 18313.63, base loss: 23729.02
[INFO 2017-06-25 17:54:10,686 main.py:50] epoch 2801, training loss: 17979.55, average training loss: 18313.44, base loss: 23734.06
[INFO 2017-06-25 17:54:14,953 main.py:50] epoch 2802, training loss: 14612.25, average training loss: 18306.76, base loss: 23726.24
[INFO 2017-06-25 17:54:19,216 main.py:50] epoch 2803, training loss: 18006.95, average training loss: 18310.00, base loss: 23725.43
[INFO 2017-06-25 17:54:23,521 main.py:50] epoch 2804, training loss: 13559.29, average training loss: 18303.02, base loss: 23718.98
[INFO 2017-06-25 17:54:27,823 main.py:50] epoch 2805, training loss: 18048.04, average training loss: 18305.45, base loss: 23722.33
[INFO 2017-06-25 17:54:32,123 main.py:50] epoch 2806, training loss: 18704.08, average training loss: 18307.49, base loss: 23725.70
[INFO 2017-06-25 17:54:36,409 main.py:50] epoch 2807, training loss: 15226.38, average training loss: 18305.36, base loss: 23726.26
[INFO 2017-06-25 17:54:40,709 main.py:50] epoch 2808, training loss: 19362.41, average training loss: 18304.26, base loss: 23724.04
[INFO 2017-06-25 17:54:45,006 main.py:50] epoch 2809, training loss: 21863.95, average training loss: 18309.51, base loss: 23730.72
[INFO 2017-06-25 17:54:49,287 main.py:50] epoch 2810, training loss: 18585.06, average training loss: 18307.28, base loss: 23730.52
[INFO 2017-06-25 17:54:53,587 main.py:50] epoch 2811, training loss: 22562.39, average training loss: 18313.43, base loss: 23737.30
[INFO 2017-06-25 17:54:57,840 main.py:50] epoch 2812, training loss: 17833.73, average training loss: 18314.74, base loss: 23737.96
[INFO 2017-06-25 17:55:02,127 main.py:50] epoch 2813, training loss: 16056.86, average training loss: 18310.83, base loss: 23734.86
[INFO 2017-06-25 17:55:06,422 main.py:50] epoch 2814, training loss: 21766.83, average training loss: 18315.50, base loss: 23740.84
[INFO 2017-06-25 17:55:10,729 main.py:50] epoch 2815, training loss: 17714.12, average training loss: 18318.40, base loss: 23744.96
[INFO 2017-06-25 17:55:15,038 main.py:50] epoch 2816, training loss: 16881.76, average training loss: 18313.52, base loss: 23740.84
[INFO 2017-06-25 17:55:19,313 main.py:50] epoch 2817, training loss: 19069.47, average training loss: 18309.30, base loss: 23734.23
[INFO 2017-06-25 17:55:23,640 main.py:50] epoch 2818, training loss: 15031.16, average training loss: 18309.12, base loss: 23735.80
[INFO 2017-06-25 17:55:27,949 main.py:50] epoch 2819, training loss: 14364.08, average training loss: 18306.82, base loss: 23733.41
[INFO 2017-06-25 17:55:32,256 main.py:50] epoch 2820, training loss: 17891.51, average training loss: 18306.64, base loss: 23731.17
[INFO 2017-06-25 17:55:36,544 main.py:50] epoch 2821, training loss: 14443.00, average training loss: 18298.98, base loss: 23721.15
[INFO 2017-06-25 17:55:40,835 main.py:50] epoch 2822, training loss: 19111.02, average training loss: 18300.03, base loss: 23723.62
[INFO 2017-06-25 17:55:45,152 main.py:50] epoch 2823, training loss: 17417.99, average training loss: 18296.60, base loss: 23720.26
[INFO 2017-06-25 17:55:49,474 main.py:50] epoch 2824, training loss: 18109.21, average training loss: 18294.96, base loss: 23717.92
[INFO 2017-06-25 17:55:53,775 main.py:50] epoch 2825, training loss: 18338.89, average training loss: 18295.86, base loss: 23723.41
[INFO 2017-06-25 17:55:58,090 main.py:50] epoch 2826, training loss: 15961.90, average training loss: 18296.11, base loss: 23724.87
[INFO 2017-06-25 17:56:02,378 main.py:50] epoch 2827, training loss: 12079.34, average training loss: 18290.26, base loss: 23712.82
[INFO 2017-06-25 17:56:06,689 main.py:50] epoch 2828, training loss: 19973.03, average training loss: 18288.78, base loss: 23710.35
[INFO 2017-06-25 17:56:10,975 main.py:50] epoch 2829, training loss: 18495.88, average training loss: 18291.76, base loss: 23715.68
[INFO 2017-06-25 17:56:15,292 main.py:50] epoch 2830, training loss: 22804.44, average training loss: 18297.20, base loss: 23723.25
[INFO 2017-06-25 17:56:19,595 main.py:50] epoch 2831, training loss: 15775.00, average training loss: 18296.84, base loss: 23724.27
[INFO 2017-06-25 17:56:23,911 main.py:50] epoch 2832, training loss: 19418.55, average training loss: 18295.86, base loss: 23725.30
[INFO 2017-06-25 17:56:28,214 main.py:50] epoch 2833, training loss: 19306.85, average training loss: 18295.35, base loss: 23719.44
[INFO 2017-06-25 17:56:32,522 main.py:50] epoch 2834, training loss: 20918.74, average training loss: 18297.18, base loss: 23722.34
[INFO 2017-06-25 17:56:36,831 main.py:50] epoch 2835, training loss: 16736.64, average training loss: 18296.93, base loss: 23724.09
[INFO 2017-06-25 17:56:41,139 main.py:50] epoch 2836, training loss: 17935.71, average training loss: 18300.21, base loss: 23728.96
[INFO 2017-06-25 17:56:45,456 main.py:50] epoch 2837, training loss: 15888.11, average training loss: 18296.80, base loss: 23726.07
[INFO 2017-06-25 17:56:49,754 main.py:50] epoch 2838, training loss: 17018.88, average training loss: 18295.15, base loss: 23722.50
[INFO 2017-06-25 17:56:54,066 main.py:50] epoch 2839, training loss: 19245.70, average training loss: 18297.02, base loss: 23728.06
[INFO 2017-06-25 17:56:58,388 main.py:50] epoch 2840, training loss: 20512.30, average training loss: 18300.75, base loss: 23733.21
[INFO 2017-06-25 17:57:02,718 main.py:50] epoch 2841, training loss: 19287.99, average training loss: 18303.35, base loss: 23738.10
[INFO 2017-06-25 17:57:07,039 main.py:50] epoch 2842, training loss: 14263.44, average training loss: 18302.53, base loss: 23734.60
[INFO 2017-06-25 17:57:11,385 main.py:50] epoch 2843, training loss: 13909.18, average training loss: 18297.91, base loss: 23730.24
[INFO 2017-06-25 17:57:15,672 main.py:50] epoch 2844, training loss: 20174.23, average training loss: 18298.98, base loss: 23733.76
[INFO 2017-06-25 17:57:19,975 main.py:50] epoch 2845, training loss: 14861.32, average training loss: 18294.31, base loss: 23729.12
[INFO 2017-06-25 17:57:24,294 main.py:50] epoch 2846, training loss: 18537.18, average training loss: 18294.79, base loss: 23731.27
[INFO 2017-06-25 17:57:28,603 main.py:50] epoch 2847, training loss: 15774.97, average training loss: 18291.57, base loss: 23724.05
[INFO 2017-06-25 17:57:32,927 main.py:50] epoch 2848, training loss: 21131.91, average training loss: 18297.68, base loss: 23732.22
[INFO 2017-06-25 17:57:37,239 main.py:50] epoch 2849, training loss: 15687.15, average training loss: 18296.64, base loss: 23730.86
[INFO 2017-06-25 17:57:41,543 main.py:50] epoch 2850, training loss: 19585.40, average training loss: 18299.29, base loss: 23731.51
[INFO 2017-06-25 17:57:45,827 main.py:50] epoch 2851, training loss: 13696.84, average training loss: 18292.90, base loss: 23720.05
[INFO 2017-06-25 17:57:50,141 main.py:50] epoch 2852, training loss: 15732.05, average training loss: 18287.75, base loss: 23713.25
[INFO 2017-06-25 17:57:54,447 main.py:50] epoch 2853, training loss: 16987.23, average training loss: 18280.95, base loss: 23704.46
[INFO 2017-06-25 17:57:58,733 main.py:50] epoch 2854, training loss: 15180.61, average training loss: 18281.30, base loss: 23708.74
[INFO 2017-06-25 17:58:03,035 main.py:50] epoch 2855, training loss: 15555.59, average training loss: 18277.76, base loss: 23704.99
[INFO 2017-06-25 17:58:07,310 main.py:50] epoch 2856, training loss: 17739.89, average training loss: 18278.55, base loss: 23705.16
[INFO 2017-06-25 17:58:11,617 main.py:50] epoch 2857, training loss: 13105.02, average training loss: 18269.96, base loss: 23694.26
[INFO 2017-06-25 17:58:15,908 main.py:50] epoch 2858, training loss: 15732.02, average training loss: 18268.98, base loss: 23692.77
[INFO 2017-06-25 17:58:20,198 main.py:50] epoch 2859, training loss: 16364.09, average training loss: 18265.33, base loss: 23689.77
[INFO 2017-06-25 17:58:24,488 main.py:50] epoch 2860, training loss: 16056.48, average training loss: 18262.20, base loss: 23686.20
[INFO 2017-06-25 17:58:28,857 main.py:50] epoch 2861, training loss: 19396.24, average training loss: 18263.99, base loss: 23690.14
[INFO 2017-06-25 17:58:33,147 main.py:50] epoch 2862, training loss: 16951.29, average training loss: 18259.05, base loss: 23685.31
[INFO 2017-06-25 17:58:37,443 main.py:50] epoch 2863, training loss: 23539.47, average training loss: 18263.49, base loss: 23691.30
[INFO 2017-06-25 17:58:41,759 main.py:50] epoch 2864, training loss: 22371.53, average training loss: 18270.91, base loss: 23700.37
[INFO 2017-06-25 17:58:46,062 main.py:50] epoch 2865, training loss: 19438.05, average training loss: 18270.81, base loss: 23698.88
[INFO 2017-06-25 17:58:50,369 main.py:50] epoch 2866, training loss: 20517.06, average training loss: 18272.80, base loss: 23701.36
[INFO 2017-06-25 17:58:54,661 main.py:50] epoch 2867, training loss: 18301.04, average training loss: 18276.88, base loss: 23708.07
[INFO 2017-06-25 17:58:58,971 main.py:50] epoch 2868, training loss: 18176.46, average training loss: 18278.34, base loss: 23712.06
[INFO 2017-06-25 17:59:03,275 main.py:50] epoch 2869, training loss: 18041.83, average training loss: 18273.40, base loss: 23706.78
[INFO 2017-06-25 17:59:07,607 main.py:50] epoch 2870, training loss: 19206.25, average training loss: 18273.97, base loss: 23704.08
[INFO 2017-06-25 17:59:11,895 main.py:50] epoch 2871, training loss: 17216.55, average training loss: 18274.38, base loss: 23705.28
[INFO 2017-06-25 17:59:16,201 main.py:50] epoch 2872, training loss: 16267.56, average training loss: 18273.10, base loss: 23705.69
[INFO 2017-06-25 17:59:20,512 main.py:50] epoch 2873, training loss: 13899.66, average training loss: 18270.64, base loss: 23703.72
[INFO 2017-06-25 17:59:24,836 main.py:50] epoch 2874, training loss: 15182.88, average training loss: 18264.25, base loss: 23697.33
[INFO 2017-06-25 17:59:29,138 main.py:50] epoch 2875, training loss: 16109.50, average training loss: 18260.62, base loss: 23692.20
[INFO 2017-06-25 17:59:33,480 main.py:50] epoch 2876, training loss: 15523.40, average training loss: 18257.04, base loss: 23687.86
[INFO 2017-06-25 17:59:37,775 main.py:50] epoch 2877, training loss: 15013.94, average training loss: 18256.12, base loss: 23688.12
[INFO 2017-06-25 17:59:42,088 main.py:50] epoch 2878, training loss: 16200.44, average training loss: 18256.89, base loss: 23688.00
[INFO 2017-06-25 17:59:46,394 main.py:50] epoch 2879, training loss: 18533.29, average training loss: 18255.39, base loss: 23687.29
[INFO 2017-06-25 17:59:50,682 main.py:50] epoch 2880, training loss: 19443.69, average training loss: 18258.51, base loss: 23690.64
[INFO 2017-06-25 17:59:54,981 main.py:50] epoch 2881, training loss: 17049.03, average training loss: 18259.56, base loss: 23692.79
[INFO 2017-06-25 17:59:59,281 main.py:50] epoch 2882, training loss: 16926.93, average training loss: 18260.56, base loss: 23695.98
[INFO 2017-06-25 18:00:03,580 main.py:50] epoch 2883, training loss: 18525.23, average training loss: 18259.91, base loss: 23694.32
[INFO 2017-06-25 18:00:07,868 main.py:50] epoch 2884, training loss: 17254.58, average training loss: 18260.09, base loss: 23695.82
[INFO 2017-06-25 18:00:12,149 main.py:50] epoch 2885, training loss: 14510.79, average training loss: 18258.38, base loss: 23692.54
[INFO 2017-06-25 18:00:16,472 main.py:50] epoch 2886, training loss: 19720.74, average training loss: 18256.18, base loss: 23690.63
[INFO 2017-06-25 18:00:20,776 main.py:50] epoch 2887, training loss: 21945.76, average training loss: 18257.66, base loss: 23692.73
[INFO 2017-06-25 18:00:25,062 main.py:50] epoch 2888, training loss: 15136.40, average training loss: 18255.82, base loss: 23688.72
[INFO 2017-06-25 18:00:29,372 main.py:50] epoch 2889, training loss: 16688.12, average training loss: 18254.10, base loss: 23687.05
[INFO 2017-06-25 18:00:33,679 main.py:50] epoch 2890, training loss: 14062.64, average training loss: 18249.32, base loss: 23681.65
[INFO 2017-06-25 18:00:37,968 main.py:50] epoch 2891, training loss: 17377.55, average training loss: 18252.92, base loss: 23686.62
[INFO 2017-06-25 18:00:42,248 main.py:50] epoch 2892, training loss: 16607.92, average training loss: 18254.16, base loss: 23687.53
[INFO 2017-06-25 18:00:46,529 main.py:50] epoch 2893, training loss: 15519.97, average training loss: 18245.65, base loss: 23679.78
[INFO 2017-06-25 18:00:50,809 main.py:50] epoch 2894, training loss: 16721.00, average training loss: 18245.65, base loss: 23677.28
[INFO 2017-06-25 18:00:55,112 main.py:50] epoch 2895, training loss: 22163.62, average training loss: 18249.44, base loss: 23679.26
[INFO 2017-06-25 18:00:59,422 main.py:50] epoch 2896, training loss: 20516.38, average training loss: 18253.08, base loss: 23684.51
[INFO 2017-06-25 18:01:03,714 main.py:50] epoch 2897, training loss: 16130.77, average training loss: 18252.69, base loss: 23684.06
[INFO 2017-06-25 18:01:08,026 main.py:50] epoch 2898, training loss: 20748.98, average training loss: 18257.12, base loss: 23687.48
[INFO 2017-06-25 18:01:12,365 main.py:50] epoch 2899, training loss: 17137.97, average training loss: 18257.04, base loss: 23687.24
[INFO 2017-06-25 18:01:16,668 main.py:50] epoch 2900, training loss: 15467.73, average training loss: 18255.24, base loss: 23686.20
[INFO 2017-06-25 18:01:20,972 main.py:50] epoch 2901, training loss: 17524.09, average training loss: 18254.15, base loss: 23686.09
[INFO 2017-06-25 18:01:25,307 main.py:50] epoch 2902, training loss: 21090.17, average training loss: 18257.60, base loss: 23689.82
[INFO 2017-06-25 18:01:29,603 main.py:50] epoch 2903, training loss: 16922.08, average training loss: 18255.93, base loss: 23687.87
[INFO 2017-06-25 18:01:33,899 main.py:50] epoch 2904, training loss: 17432.78, average training loss: 18253.62, base loss: 23683.38
[INFO 2017-06-25 18:01:38,212 main.py:50] epoch 2905, training loss: 13789.32, average training loss: 18247.98, base loss: 23676.54
[INFO 2017-06-25 18:01:42,528 main.py:50] epoch 2906, training loss: 19749.39, average training loss: 18250.95, base loss: 23680.29
[INFO 2017-06-25 18:01:46,803 main.py:50] epoch 2907, training loss: 17322.83, average training loss: 18247.44, base loss: 23674.77
[INFO 2017-06-25 18:01:51,152 main.py:50] epoch 2908, training loss: 19405.11, average training loss: 18251.69, base loss: 23678.62
[INFO 2017-06-25 18:01:55,470 main.py:50] epoch 2909, training loss: 17450.20, average training loss: 18247.00, base loss: 23673.60
[INFO 2017-06-25 18:01:59,767 main.py:50] epoch 2910, training loss: 20797.75, average training loss: 18250.15, base loss: 23678.22
[INFO 2017-06-25 18:02:04,047 main.py:50] epoch 2911, training loss: 13421.97, average training loss: 18245.50, base loss: 23674.89
[INFO 2017-06-25 18:02:08,319 main.py:50] epoch 2912, training loss: 18722.55, average training loss: 18246.44, base loss: 23675.02
[INFO 2017-06-25 18:02:12,618 main.py:50] epoch 2913, training loss: 16546.80, average training loss: 18243.18, base loss: 23673.47
[INFO 2017-06-25 18:02:16,905 main.py:50] epoch 2914, training loss: 18197.77, average training loss: 18244.04, base loss: 23674.11
[INFO 2017-06-25 18:02:21,201 main.py:50] epoch 2915, training loss: 18632.99, average training loss: 18248.31, base loss: 23677.30
[INFO 2017-06-25 18:02:25,496 main.py:50] epoch 2916, training loss: 14785.59, average training loss: 18248.94, base loss: 23680.11
[INFO 2017-06-25 18:02:29,804 main.py:50] epoch 2917, training loss: 14466.09, average training loss: 18242.61, base loss: 23671.78
[INFO 2017-06-25 18:02:34,080 main.py:50] epoch 2918, training loss: 13521.78, average training loss: 18238.10, base loss: 23668.09
[INFO 2017-06-25 18:02:38,413 main.py:50] epoch 2919, training loss: 18438.00, average training loss: 18238.75, base loss: 23670.41
[INFO 2017-06-25 18:02:42,691 main.py:50] epoch 2920, training loss: 14952.97, average training loss: 18238.65, base loss: 23670.24
[INFO 2017-06-25 18:02:46,968 main.py:50] epoch 2921, training loss: 19440.10, average training loss: 18241.20, base loss: 23675.79
[INFO 2017-06-25 18:02:51,274 main.py:50] epoch 2922, training loss: 25997.31, average training loss: 18247.60, base loss: 23682.32
[INFO 2017-06-25 18:02:55,555 main.py:50] epoch 2923, training loss: 19573.16, average training loss: 18247.16, base loss: 23682.10
[INFO 2017-06-25 18:02:59,859 main.py:50] epoch 2924, training loss: 16481.31, average training loss: 18241.93, base loss: 23676.10
[INFO 2017-06-25 18:03:04,169 main.py:50] epoch 2925, training loss: 18086.11, average training loss: 18240.67, base loss: 23674.18
[INFO 2017-06-25 18:03:08,465 main.py:50] epoch 2926, training loss: 16983.38, average training loss: 18239.59, base loss: 23672.98
[INFO 2017-06-25 18:03:12,766 main.py:50] epoch 2927, training loss: 19948.00, average training loss: 18241.85, base loss: 23675.43
[INFO 2017-06-25 18:03:17,073 main.py:50] epoch 2928, training loss: 17604.10, average training loss: 18242.82, base loss: 23674.05
[INFO 2017-06-25 18:03:21,352 main.py:50] epoch 2929, training loss: 17903.91, average training loss: 18245.20, base loss: 23676.70
[INFO 2017-06-25 18:03:25,637 main.py:50] epoch 2930, training loss: 22518.89, average training loss: 18252.59, base loss: 23686.58
[INFO 2017-06-25 18:03:29,926 main.py:50] epoch 2931, training loss: 17777.68, average training loss: 18252.11, base loss: 23685.65
[INFO 2017-06-25 18:03:34,240 main.py:50] epoch 2932, training loss: 15157.11, average training loss: 18251.09, base loss: 23683.96
[INFO 2017-06-25 18:03:38,536 main.py:50] epoch 2933, training loss: 14906.83, average training loss: 18246.77, base loss: 23679.70
[INFO 2017-06-25 18:03:42,830 main.py:50] epoch 2934, training loss: 19257.08, average training loss: 18247.41, base loss: 23683.32
[INFO 2017-06-25 18:03:47,104 main.py:50] epoch 2935, training loss: 17372.67, average training loss: 18239.81, base loss: 23674.07
[INFO 2017-06-25 18:03:51,424 main.py:50] epoch 2936, training loss: 15231.82, average training loss: 18238.85, base loss: 23675.03
[INFO 2017-06-25 18:03:55,709 main.py:50] epoch 2937, training loss: 18453.39, average training loss: 18239.96, base loss: 23676.97
[INFO 2017-06-25 18:04:00,008 main.py:50] epoch 2938, training loss: 20758.87, average training loss: 18246.32, base loss: 23685.12
[INFO 2017-06-25 18:04:04,279 main.py:50] epoch 2939, training loss: 15615.00, average training loss: 18243.85, base loss: 23683.47
[INFO 2017-06-25 18:04:08,552 main.py:50] epoch 2940, training loss: 20449.49, average training loss: 18249.78, base loss: 23688.63
[INFO 2017-06-25 18:04:12,841 main.py:50] epoch 2941, training loss: 22527.83, average training loss: 18253.19, base loss: 23692.18
[INFO 2017-06-25 18:04:17,132 main.py:50] epoch 2942, training loss: 18497.49, average training loss: 18257.31, base loss: 23700.12
[INFO 2017-06-25 18:04:21,426 main.py:50] epoch 2943, training loss: 21115.44, average training loss: 18260.33, base loss: 23704.50
[INFO 2017-06-25 18:04:25,709 main.py:50] epoch 2944, training loss: 17468.35, average training loss: 18261.36, base loss: 23706.73
[INFO 2017-06-25 18:04:30,033 main.py:50] epoch 2945, training loss: 17160.51, average training loss: 18254.70, base loss: 23697.59
[INFO 2017-06-25 18:04:34,322 main.py:50] epoch 2946, training loss: 15594.55, average training loss: 18254.21, base loss: 23698.31
[INFO 2017-06-25 18:04:38,625 main.py:50] epoch 2947, training loss: 15898.44, average training loss: 18254.20, base loss: 23698.43
[INFO 2017-06-25 18:04:42,928 main.py:50] epoch 2948, training loss: 17738.69, average training loss: 18251.30, base loss: 23695.76
[INFO 2017-06-25 18:04:47,208 main.py:50] epoch 2949, training loss: 17555.51, average training loss: 18249.36, base loss: 23693.76
[INFO 2017-06-25 18:04:51,478 main.py:50] epoch 2950, training loss: 22389.79, average training loss: 18255.63, base loss: 23701.44
[INFO 2017-06-25 18:04:55,769 main.py:50] epoch 2951, training loss: 20771.75, average training loss: 18252.32, base loss: 23697.28
[INFO 2017-06-25 18:05:00,056 main.py:50] epoch 2952, training loss: 15716.87, average training loss: 18251.42, base loss: 23697.53
[INFO 2017-06-25 18:05:04,341 main.py:50] epoch 2953, training loss: 16867.91, average training loss: 18249.98, base loss: 23694.65
[INFO 2017-06-25 18:05:08,638 main.py:50] epoch 2954, training loss: 17261.28, average training loss: 18248.60, base loss: 23695.28
[INFO 2017-06-25 18:05:12,933 main.py:50] epoch 2955, training loss: 19255.96, average training loss: 18248.91, base loss: 23697.00
[INFO 2017-06-25 18:05:17,234 main.py:50] epoch 2956, training loss: 16683.07, average training loss: 18248.91, base loss: 23696.04
[INFO 2017-06-25 18:05:21,543 main.py:50] epoch 2957, training loss: 14661.52, average training loss: 18245.31, base loss: 23690.72
[INFO 2017-06-25 18:05:25,861 main.py:50] epoch 2958, training loss: 14726.57, average training loss: 18238.63, base loss: 23680.45
[INFO 2017-06-25 18:05:30,176 main.py:50] epoch 2959, training loss: 20154.95, average training loss: 18241.41, base loss: 23684.92
[INFO 2017-06-25 18:05:34,445 main.py:50] epoch 2960, training loss: 15875.65, average training loss: 18238.83, base loss: 23682.04
[INFO 2017-06-25 18:05:38,727 main.py:50] epoch 2961, training loss: 21737.16, average training loss: 18242.47, base loss: 23688.12
[INFO 2017-06-25 18:05:43,030 main.py:50] epoch 2962, training loss: 19902.47, average training loss: 18240.72, base loss: 23688.28
[INFO 2017-06-25 18:05:47,355 main.py:50] epoch 2963, training loss: 20659.59, average training loss: 18244.39, base loss: 23693.63
[INFO 2017-06-25 18:05:51,653 main.py:50] epoch 2964, training loss: 23292.60, average training loss: 18249.54, base loss: 23701.75
[INFO 2017-06-25 18:05:55,959 main.py:50] epoch 2965, training loss: 18619.62, average training loss: 18251.13, base loss: 23702.11
[INFO 2017-06-25 18:06:00,245 main.py:50] epoch 2966, training loss: 15575.06, average training loss: 18249.44, base loss: 23700.66
[INFO 2017-06-25 18:06:04,517 main.py:50] epoch 2967, training loss: 19731.76, average training loss: 18252.87, base loss: 23704.73
[INFO 2017-06-25 18:06:08,807 main.py:50] epoch 2968, training loss: 17379.67, average training loss: 18247.04, base loss: 23700.90
[INFO 2017-06-25 18:06:13,093 main.py:50] epoch 2969, training loss: 17593.21, average training loss: 18246.11, base loss: 23699.16
[INFO 2017-06-25 18:06:17,373 main.py:50] epoch 2970, training loss: 18146.42, average training loss: 18246.74, base loss: 23703.10
[INFO 2017-06-25 18:06:21,678 main.py:50] epoch 2971, training loss: 15625.80, average training loss: 18246.60, base loss: 23703.53
[INFO 2017-06-25 18:06:25,966 main.py:50] epoch 2972, training loss: 15342.16, average training loss: 18245.80, base loss: 23705.87
[INFO 2017-06-25 18:06:30,273 main.py:50] epoch 2973, training loss: 13877.95, average training loss: 18244.88, base loss: 23705.91
[INFO 2017-06-25 18:06:34,572 main.py:50] epoch 2974, training loss: 16656.27, average training loss: 18245.09, base loss: 23706.67
[INFO 2017-06-25 18:06:38,860 main.py:50] epoch 2975, training loss: 18179.03, average training loss: 18240.95, base loss: 23703.05
[INFO 2017-06-25 18:06:43,137 main.py:50] epoch 2976, training loss: 20058.81, average training loss: 18243.91, base loss: 23709.49
[INFO 2017-06-25 18:06:47,432 main.py:50] epoch 2977, training loss: 15994.42, average training loss: 18239.41, base loss: 23706.70
[INFO 2017-06-25 18:06:51,690 main.py:50] epoch 2978, training loss: 17359.87, average training loss: 18238.87, base loss: 23706.42
[INFO 2017-06-25 18:06:55,980 main.py:50] epoch 2979, training loss: 16308.94, average training loss: 18233.66, base loss: 23702.12
[INFO 2017-06-25 18:07:00,284 main.py:50] epoch 2980, training loss: 14219.43, average training loss: 18227.69, base loss: 23693.58
[INFO 2017-06-25 18:07:04,593 main.py:50] epoch 2981, training loss: 15083.44, average training loss: 18222.91, base loss: 23688.00
[INFO 2017-06-25 18:07:08,898 main.py:50] epoch 2982, training loss: 18414.15, average training loss: 18224.72, base loss: 23694.09
[INFO 2017-06-25 18:07:13,200 main.py:50] epoch 2983, training loss: 16967.58, average training loss: 18223.20, base loss: 23693.08
[INFO 2017-06-25 18:07:17,492 main.py:50] epoch 2984, training loss: 19482.75, average training loss: 18221.75, base loss: 23692.16
[INFO 2017-06-25 18:07:21,787 main.py:50] epoch 2985, training loss: 18648.01, average training loss: 18221.44, base loss: 23689.56
[INFO 2017-06-25 18:07:26,084 main.py:50] epoch 2986, training loss: 16449.17, average training loss: 18216.39, base loss: 23682.25
[INFO 2017-06-25 18:07:30,372 main.py:50] epoch 2987, training loss: 15201.81, average training loss: 18215.20, base loss: 23683.28
[INFO 2017-06-25 18:07:34,680 main.py:50] epoch 2988, training loss: 17624.79, average training loss: 18216.38, base loss: 23687.04
[INFO 2017-06-25 18:07:39,011 main.py:50] epoch 2989, training loss: 18418.57, average training loss: 18219.62, base loss: 23692.81
[INFO 2017-06-25 18:07:43,299 main.py:50] epoch 2990, training loss: 13635.54, average training loss: 18218.29, base loss: 23691.14
[INFO 2017-06-25 18:07:47,606 main.py:50] epoch 2991, training loss: 21170.16, average training loss: 18219.45, base loss: 23691.12
[INFO 2017-06-25 18:07:51,892 main.py:50] epoch 2992, training loss: 22058.05, average training loss: 18223.23, base loss: 23695.79
[INFO 2017-06-25 18:07:56,175 main.py:50] epoch 2993, training loss: 14628.75, average training loss: 18218.67, base loss: 23690.31
[INFO 2017-06-25 18:08:00,460 main.py:50] epoch 2994, training loss: 18769.82, average training loss: 18215.38, base loss: 23688.62
[INFO 2017-06-25 18:08:04,725 main.py:50] epoch 2995, training loss: 16448.05, average training loss: 18213.81, base loss: 23687.77
[INFO 2017-06-25 18:08:09,030 main.py:50] epoch 2996, training loss: 24930.35, average training loss: 18220.08, base loss: 23695.93
[INFO 2017-06-25 18:08:13,345 main.py:50] epoch 2997, training loss: 15899.87, average training loss: 18216.54, base loss: 23691.86
[INFO 2017-06-25 18:08:17,658 main.py:50] epoch 2998, training loss: 20284.54, average training loss: 18216.65, base loss: 23692.93
[INFO 2017-06-25 18:08:21,961 main.py:50] epoch 2999, training loss: 16594.53, average training loss: 18212.22, base loss: 23688.33
[INFO 2017-06-25 18:08:21,962 main.py:52] epoch 2999, testing
[INFO 2017-06-25 18:12:01,433 main.py:100] average testing loss: 18221.12
[INFO 2017-06-25 18:12:01,437 main.py:72] model save to ./model/final.pth
[INFO 2017-06-25 18:12:01,443 main.py:76] current best accuracy: 18221.12
[INFO 2017-06-25 18:12:05,750 main.py:50] epoch 3000, training loss: 16756.24, average training loss: 18212.32, base loss: 23688.66
[INFO 2017-06-25 18:12:10,078 main.py:50] epoch 3001, training loss: 16625.12, average training loss: 18212.32, base loss: 23688.86
[INFO 2017-06-25 18:12:14,365 main.py:50] epoch 3002, training loss: 17044.23, average training loss: 18214.33, base loss: 23693.45
[INFO 2017-06-25 18:12:18,666 main.py:50] epoch 3003, training loss: 23212.79, average training loss: 18222.59, base loss: 23704.58
[INFO 2017-06-25 18:12:22,979 main.py:50] epoch 3004, training loss: 20796.06, average training loss: 18223.64, base loss: 23703.21
[INFO 2017-06-25 18:12:27,296 main.py:50] epoch 3005, training loss: 19392.49, average training loss: 18230.15, base loss: 23711.00
[INFO 2017-06-25 18:12:31,595 main.py:50] epoch 3006, training loss: 17832.69, average training loss: 18230.36, base loss: 23706.27
[INFO 2017-06-25 18:12:35,938 main.py:50] epoch 3007, training loss: 19076.74, average training loss: 18226.09, base loss: 23699.20
[INFO 2017-06-25 18:12:40,243 main.py:50] epoch 3008, training loss: 17121.09, average training loss: 18223.96, base loss: 23697.78
[INFO 2017-06-25 18:12:44,537 main.py:50] epoch 3009, training loss: 17426.87, average training loss: 18224.88, base loss: 23700.19
[INFO 2017-06-25 18:12:48,852 main.py:50] epoch 3010, training loss: 18950.55, average training loss: 18227.15, base loss: 23701.67
[INFO 2017-06-25 18:12:53,120 main.py:50] epoch 3011, training loss: 17429.76, average training loss: 18229.45, base loss: 23706.84
[INFO 2017-06-25 18:12:57,399 main.py:50] epoch 3012, training loss: 24458.11, average training loss: 18236.35, base loss: 23715.34
[INFO 2017-06-25 18:13:01,716 main.py:50] epoch 3013, training loss: 16422.02, average training loss: 18235.58, base loss: 23714.58
[INFO 2017-06-25 18:13:06,006 main.py:50] epoch 3014, training loss: 15169.51, average training loss: 18233.47, base loss: 23710.91
[INFO 2017-06-25 18:13:10,302 main.py:50] epoch 3015, training loss: 15738.62, average training loss: 18231.33, base loss: 23706.57
[INFO 2017-06-25 18:13:14,602 main.py:50] epoch 3016, training loss: 18472.62, average training loss: 18229.98, base loss: 23704.73
[INFO 2017-06-25 18:13:18,966 main.py:50] epoch 3017, training loss: 21540.57, average training loss: 18230.95, base loss: 23704.09
[INFO 2017-06-25 18:13:23,289 main.py:50] epoch 3018, training loss: 12979.37, average training loss: 18227.06, base loss: 23699.43
[INFO 2017-06-25 18:13:27,591 main.py:50] epoch 3019, training loss: 16605.10, average training loss: 18226.14, base loss: 23698.50
[INFO 2017-06-25 18:13:31,877 main.py:50] epoch 3020, training loss: 16162.95, average training loss: 18224.13, base loss: 23697.72
[INFO 2017-06-25 18:13:36,180 main.py:50] epoch 3021, training loss: 19766.18, average training loss: 18225.94, base loss: 23699.13
[INFO 2017-06-25 18:13:40,499 main.py:50] epoch 3022, training loss: 21771.86, average training loss: 18231.74, base loss: 23703.28
[INFO 2017-06-25 18:13:44,754 main.py:50] epoch 3023, training loss: 18396.86, average training loss: 18232.44, base loss: 23704.71
[INFO 2017-06-25 18:13:49,041 main.py:50] epoch 3024, training loss: 24717.28, average training loss: 18241.91, base loss: 23718.97
[INFO 2017-06-25 18:13:53,332 main.py:50] epoch 3025, training loss: 17427.22, average training loss: 18241.51, base loss: 23718.81
[INFO 2017-06-25 18:13:57,634 main.py:50] epoch 3026, training loss: 21695.85, average training loss: 18242.40, base loss: 23719.75
[INFO 2017-06-25 18:14:01,931 main.py:50] epoch 3027, training loss: 21606.00, average training loss: 18248.27, base loss: 23726.60
[INFO 2017-06-25 18:14:06,221 main.py:50] epoch 3028, training loss: 18287.55, average training loss: 18248.28, base loss: 23724.62
[INFO 2017-06-25 18:14:10,526 main.py:50] epoch 3029, training loss: 17295.54, average training loss: 18245.07, base loss: 23721.58
[INFO 2017-06-25 18:14:14,878 main.py:50] epoch 3030, training loss: 17712.11, average training loss: 18243.79, base loss: 23720.83
[INFO 2017-06-25 18:14:19,226 main.py:50] epoch 3031, training loss: 16579.20, average training loss: 18238.36, base loss: 23711.64
[INFO 2017-06-25 18:14:23,547 main.py:50] epoch 3032, training loss: 18577.52, average training loss: 18235.95, base loss: 23706.64
[INFO 2017-06-25 18:14:27,905 main.py:50] epoch 3033, training loss: 18018.29, average training loss: 18234.28, base loss: 23705.36
[INFO 2017-06-25 18:14:32,218 main.py:50] epoch 3034, training loss: 15628.31, average training loss: 18230.27, base loss: 23702.73
[INFO 2017-06-25 18:14:36,552 main.py:50] epoch 3035, training loss: 20011.59, average training loss: 18227.42, base loss: 23699.47
[INFO 2017-06-25 18:14:40,888 main.py:50] epoch 3036, training loss: 18882.17, average training loss: 18226.17, base loss: 23699.63
[INFO 2017-06-25 18:14:45,218 main.py:50] epoch 3037, training loss: 16765.68, average training loss: 18224.55, base loss: 23695.09
[INFO 2017-06-25 18:14:49,513 main.py:50] epoch 3038, training loss: 19003.63, average training loss: 18229.80, base loss: 23697.19
[INFO 2017-06-25 18:14:53,808 main.py:50] epoch 3039, training loss: 19212.23, average training loss: 18229.20, base loss: 23697.18
[INFO 2017-06-25 18:14:58,098 main.py:50] epoch 3040, training loss: 18396.43, average training loss: 18228.70, base loss: 23694.39
[INFO 2017-06-25 18:15:02,406 main.py:50] epoch 3041, training loss: 17071.65, average training loss: 18228.98, base loss: 23694.32
[INFO 2017-06-25 18:15:06,698 main.py:50] epoch 3042, training loss: 16956.80, average training loss: 18223.04, base loss: 23688.49
[INFO 2017-06-25 18:15:10,982 main.py:50] epoch 3043, training loss: 15711.58, average training loss: 18217.89, base loss: 23682.84
[INFO 2017-06-25 18:15:15,263 main.py:50] epoch 3044, training loss: 17183.36, average training loss: 18216.94, base loss: 23682.44
[INFO 2017-06-25 18:15:19,565 main.py:50] epoch 3045, training loss: 16851.45, average training loss: 18215.78, base loss: 23682.12
[INFO 2017-06-25 18:15:23,853 main.py:50] epoch 3046, training loss: 18250.54, average training loss: 18214.28, base loss: 23681.70
[INFO 2017-06-25 18:15:28,155 main.py:50] epoch 3047, training loss: 18146.55, average training loss: 18211.80, base loss: 23680.50
[INFO 2017-06-25 18:15:32,454 main.py:50] epoch 3048, training loss: 16554.08, average training loss: 18212.19, base loss: 23681.04
[INFO 2017-06-25 18:15:36,737 main.py:50] epoch 3049, training loss: 22995.39, average training loss: 18218.69, base loss: 23691.32
[INFO 2017-06-25 18:15:41,031 main.py:50] epoch 3050, training loss: 16453.21, average training loss: 18217.11, base loss: 23690.48
[INFO 2017-06-25 18:15:45,345 main.py:50] epoch 3051, training loss: 21149.28, average training loss: 18220.53, base loss: 23694.85
[INFO 2017-06-25 18:15:49,635 main.py:50] epoch 3052, training loss: 16699.60, average training loss: 18215.97, base loss: 23689.22
[INFO 2017-06-25 18:15:53,917 main.py:50] epoch 3053, training loss: 18406.27, average training loss: 18213.66, base loss: 23688.77
[INFO 2017-06-25 18:15:58,200 main.py:50] epoch 3054, training loss: 16392.86, average training loss: 18210.21, base loss: 23684.01
[INFO 2017-06-25 18:16:02,497 main.py:50] epoch 3055, training loss: 18190.77, average training loss: 18210.60, base loss: 23685.62
[INFO 2017-06-25 18:16:06,809 main.py:50] epoch 3056, training loss: 21672.33, average training loss: 18208.63, base loss: 23682.24
[INFO 2017-06-25 18:16:11,118 main.py:50] epoch 3057, training loss: 17868.02, average training loss: 18207.58, base loss: 23679.80
[INFO 2017-06-25 18:16:15,450 main.py:50] epoch 3058, training loss: 16349.40, average training loss: 18209.56, base loss: 23686.63
[INFO 2017-06-25 18:16:19,765 main.py:50] epoch 3059, training loss: 23231.73, average training loss: 18210.56, base loss: 23688.08
[INFO 2017-06-25 18:16:24,042 main.py:50] epoch 3060, training loss: 16686.04, average training loss: 18207.99, base loss: 23686.67
[INFO 2017-06-25 18:16:28,340 main.py:50] epoch 3061, training loss: 18919.39, average training loss: 18207.42, base loss: 23688.15
[INFO 2017-06-25 18:16:32,649 main.py:50] epoch 3062, training loss: 18894.27, average training loss: 18206.22, base loss: 23688.67
[INFO 2017-06-25 18:16:36,925 main.py:50] epoch 3063, training loss: 18174.31, average training loss: 18204.47, base loss: 23687.29
[INFO 2017-06-25 18:16:41,247 main.py:50] epoch 3064, training loss: 18686.85, average training loss: 18204.47, base loss: 23691.45
[INFO 2017-06-25 18:16:45,542 main.py:50] epoch 3065, training loss: 16900.91, average training loss: 18205.08, base loss: 23692.64
[INFO 2017-06-25 18:16:49,868 main.py:50] epoch 3066, training loss: 15818.12, average training loss: 18195.72, base loss: 23682.73
[INFO 2017-06-25 18:16:54,198 main.py:50] epoch 3067, training loss: 17582.93, average training loss: 18196.12, base loss: 23684.39
[INFO 2017-06-25 18:16:58,494 main.py:50] epoch 3068, training loss: 17425.02, average training loss: 18190.63, base loss: 23681.57
[INFO 2017-06-25 18:17:02,803 main.py:50] epoch 3069, training loss: 21509.07, average training loss: 18198.46, base loss: 23693.50
[INFO 2017-06-25 18:17:07,127 main.py:50] epoch 3070, training loss: 14084.08, average training loss: 18195.56, base loss: 23688.70
[INFO 2017-06-25 18:17:11,426 main.py:50] epoch 3071, training loss: 23729.32, average training loss: 18201.07, base loss: 23696.26
[INFO 2017-06-25 18:17:15,731 main.py:50] epoch 3072, training loss: 18654.82, average training loss: 18202.30, base loss: 23698.73
[INFO 2017-06-25 18:17:20,025 main.py:50] epoch 3073, training loss: 17358.35, average training loss: 18200.40, base loss: 23695.94
[INFO 2017-06-25 18:17:24,352 main.py:50] epoch 3074, training loss: 17245.04, average training loss: 18190.49, base loss: 23685.28
[INFO 2017-06-25 18:17:28,663 main.py:50] epoch 3075, training loss: 17991.58, average training loss: 18186.58, base loss: 23682.84
[INFO 2017-06-25 18:17:32,991 main.py:50] epoch 3076, training loss: 18511.48, average training loss: 18186.44, base loss: 23683.91
[INFO 2017-06-25 18:17:37,331 main.py:50] epoch 3077, training loss: 18247.63, average training loss: 18185.98, base loss: 23684.86
[INFO 2017-06-25 18:17:41,613 main.py:50] epoch 3078, training loss: 16084.22, average training loss: 18187.18, base loss: 23684.84
[INFO 2017-06-25 18:17:45,897 main.py:50] epoch 3079, training loss: 16970.74, average training loss: 18184.09, base loss: 23679.92
[INFO 2017-06-25 18:17:50,188 main.py:50] epoch 3080, training loss: 15453.96, average training loss: 18176.56, base loss: 23670.54
[INFO 2017-06-25 18:17:54,489 main.py:50] epoch 3081, training loss: 22207.95, average training loss: 18180.35, base loss: 23677.84
[INFO 2017-06-25 18:17:58,812 main.py:50] epoch 3082, training loss: 20252.62, average training loss: 18179.75, base loss: 23678.08
[INFO 2017-06-25 18:18:03,105 main.py:50] epoch 3083, training loss: 13879.55, average training loss: 18171.29, base loss: 23668.54
[INFO 2017-06-25 18:18:07,418 main.py:50] epoch 3084, training loss: 20807.99, average training loss: 18170.68, base loss: 23669.70
[INFO 2017-06-25 18:18:11,694 main.py:50] epoch 3085, training loss: 17405.41, average training loss: 18168.79, base loss: 23665.42
[INFO 2017-06-25 18:18:16,010 main.py:50] epoch 3086, training loss: 17068.39, average training loss: 18169.82, base loss: 23671.29
[INFO 2017-06-25 18:18:20,317 main.py:50] epoch 3087, training loss: 18722.04, average training loss: 18171.45, base loss: 23671.93
[INFO 2017-06-25 18:18:24,629 main.py:50] epoch 3088, training loss: 16491.21, average training loss: 18171.51, base loss: 23671.54
[INFO 2017-06-25 18:18:28,903 main.py:50] epoch 3089, training loss: 15716.49, average training loss: 18165.19, base loss: 23665.33
[INFO 2017-06-25 18:18:33,219 main.py:50] epoch 3090, training loss: 20362.94, average training loss: 18167.29, base loss: 23669.77
[INFO 2017-06-25 18:18:37,518 main.py:50] epoch 3091, training loss: 17741.79, average training loss: 18165.42, base loss: 23666.76
[INFO 2017-06-25 18:18:41,788 main.py:50] epoch 3092, training loss: 14054.41, average training loss: 18161.32, base loss: 23660.45
[INFO 2017-06-25 18:18:46,096 main.py:50] epoch 3093, training loss: 17550.52, average training loss: 18156.10, base loss: 23656.22
[INFO 2017-06-25 18:18:50,419 main.py:50] epoch 3094, training loss: 19405.70, average training loss: 18158.01, base loss: 23660.22
[INFO 2017-06-25 18:18:54,712 main.py:50] epoch 3095, training loss: 18657.06, average training loss: 18154.18, base loss: 23658.04
[INFO 2017-06-25 18:18:59,042 main.py:50] epoch 3096, training loss: 17780.62, average training loss: 18151.19, base loss: 23652.39
[INFO 2017-06-25 18:19:03,340 main.py:50] epoch 3097, training loss: 14323.21, average training loss: 18147.72, base loss: 23649.40
[INFO 2017-06-25 18:19:07,627 main.py:50] epoch 3098, training loss: 18211.49, average training loss: 18150.41, base loss: 23655.92
[INFO 2017-06-25 18:19:11,938 main.py:50] epoch 3099, training loss: 20433.02, average training loss: 18152.48, base loss: 23657.85
[INFO 2017-06-25 18:19:16,252 main.py:50] epoch 3100, training loss: 19044.70, average training loss: 18152.90, base loss: 23658.14
[INFO 2017-06-25 18:19:20,539 main.py:50] epoch 3101, training loss: 17409.71, average training loss: 18153.57, base loss: 23659.79
[INFO 2017-06-25 18:19:24,850 main.py:50] epoch 3102, training loss: 15834.26, average training loss: 18149.63, base loss: 23651.72
[INFO 2017-06-25 18:19:29,154 main.py:50] epoch 3103, training loss: 16850.00, average training loss: 18150.57, base loss: 23653.77
[INFO 2017-06-25 18:19:33,458 main.py:50] epoch 3104, training loss: 18529.76, average training loss: 18152.79, base loss: 23657.82
[INFO 2017-06-25 18:19:37,733 main.py:50] epoch 3105, training loss: 18521.94, average training loss: 18145.55, base loss: 23650.56
[INFO 2017-06-25 18:19:42,012 main.py:50] epoch 3106, training loss: 20186.99, average training loss: 18148.67, base loss: 23655.08
[INFO 2017-06-25 18:19:46,310 main.py:50] epoch 3107, training loss: 20790.24, average training loss: 18154.57, base loss: 23663.13
[INFO 2017-06-25 18:19:50,590 main.py:50] epoch 3108, training loss: 15161.38, average training loss: 18149.30, base loss: 23654.99
[INFO 2017-06-25 18:19:54,853 main.py:50] epoch 3109, training loss: 18392.43, average training loss: 18146.11, base loss: 23648.58
[INFO 2017-06-25 18:19:59,141 main.py:50] epoch 3110, training loss: 17388.76, average training loss: 18146.76, base loss: 23651.78
[INFO 2017-06-25 18:20:03,440 main.py:50] epoch 3111, training loss: 14893.85, average training loss: 18144.89, base loss: 23649.84
[INFO 2017-06-25 18:20:07,738 main.py:50] epoch 3112, training loss: 19769.40, average training loss: 18146.61, base loss: 23653.38
[INFO 2017-06-25 18:20:12,012 main.py:50] epoch 3113, training loss: 20432.29, average training loss: 18147.28, base loss: 23653.15
[INFO 2017-06-25 18:20:16,320 main.py:50] epoch 3114, training loss: 15028.05, average training loss: 18147.91, base loss: 23656.21
[INFO 2017-06-25 18:20:20,600 main.py:50] epoch 3115, training loss: 13163.04, average training loss: 18140.92, base loss: 23648.25
[INFO 2017-06-25 18:20:24,914 main.py:50] epoch 3116, training loss: 18378.71, average training loss: 18140.49, base loss: 23648.95
[INFO 2017-06-25 18:20:29,207 main.py:50] epoch 3117, training loss: 20326.21, average training loss: 18143.79, base loss: 23653.78
[INFO 2017-06-25 18:20:33,537 main.py:50] epoch 3118, training loss: 17940.83, average training loss: 18144.78, base loss: 23657.62
[INFO 2017-06-25 18:20:37,836 main.py:50] epoch 3119, training loss: 17221.77, average training loss: 18147.77, base loss: 23664.03
[INFO 2017-06-25 18:20:42,123 main.py:50] epoch 3120, training loss: 15301.37, average training loss: 18146.16, base loss: 23664.01
[INFO 2017-06-25 18:20:46,472 main.py:50] epoch 3121, training loss: 13027.86, average training loss: 18140.34, base loss: 23656.44
[INFO 2017-06-25 18:20:50,805 main.py:50] epoch 3122, training loss: 17748.11, average training loss: 18135.70, base loss: 23650.95
[INFO 2017-06-25 18:20:55,138 main.py:50] epoch 3123, training loss: 18969.36, average training loss: 18135.32, base loss: 23649.21
[INFO 2017-06-25 18:20:59,438 main.py:50] epoch 3124, training loss: 15027.95, average training loss: 18134.13, base loss: 23647.73
[INFO 2017-06-25 18:21:03,726 main.py:50] epoch 3125, training loss: 17383.28, average training loss: 18127.50, base loss: 23640.47
[INFO 2017-06-25 18:21:08,041 main.py:50] epoch 3126, training loss: 19982.70, average training loss: 18133.77, base loss: 23650.35
[INFO 2017-06-25 18:21:12,328 main.py:50] epoch 3127, training loss: 19435.77, average training loss: 18137.37, base loss: 23658.88
[INFO 2017-06-25 18:21:16,635 main.py:50] epoch 3128, training loss: 19233.64, average training loss: 18135.84, base loss: 23659.13
[INFO 2017-06-25 18:21:20,943 main.py:50] epoch 3129, training loss: 15678.51, average training loss: 18132.41, base loss: 23652.23
[INFO 2017-06-25 18:21:25,240 main.py:50] epoch 3130, training loss: 17850.61, average training loss: 18135.64, base loss: 23657.91
[INFO 2017-06-25 18:21:29,551 main.py:50] epoch 3131, training loss: 20102.72, average training loss: 18136.13, base loss: 23657.94
[INFO 2017-06-25 18:21:33,865 main.py:50] epoch 3132, training loss: 18462.99, average training loss: 18138.70, base loss: 23661.53
[INFO 2017-06-25 18:21:38,175 main.py:50] epoch 3133, training loss: 18497.01, average training loss: 18135.48, base loss: 23658.66
[INFO 2017-06-25 18:21:42,465 main.py:50] epoch 3134, training loss: 20099.39, average training loss: 18131.18, base loss: 23654.00
[INFO 2017-06-25 18:21:46,771 main.py:50] epoch 3135, training loss: 19157.80, average training loss: 18127.43, base loss: 23649.36
[INFO 2017-06-25 18:21:51,118 main.py:50] epoch 3136, training loss: 22268.00, average training loss: 18131.51, base loss: 23654.62
[INFO 2017-06-25 18:21:55,400 main.py:50] epoch 3137, training loss: 18515.52, average training loss: 18135.01, base loss: 23660.69
[INFO 2017-06-25 18:21:59,702 main.py:50] epoch 3138, training loss: 20932.08, average training loss: 18135.15, base loss: 23658.42
[INFO 2017-06-25 18:22:04,001 main.py:50] epoch 3139, training loss: 16105.71, average training loss: 18131.57, base loss: 23652.33
[INFO 2017-06-25 18:22:08,298 main.py:50] epoch 3140, training loss: 20452.43, average training loss: 18136.57, base loss: 23659.54
[INFO 2017-06-25 18:22:12,636 main.py:50] epoch 3141, training loss: 19664.99, average training loss: 18139.69, base loss: 23664.08
[INFO 2017-06-25 18:22:16,971 main.py:50] epoch 3142, training loss: 18957.03, average training loss: 18144.72, base loss: 23669.74
[INFO 2017-06-25 18:22:21,293 main.py:50] epoch 3143, training loss: 16799.94, average training loss: 18142.26, base loss: 23668.20
[INFO 2017-06-25 18:22:25,562 main.py:50] epoch 3144, training loss: 16890.97, average training loss: 18140.50, base loss: 23666.30
[INFO 2017-06-25 18:22:29,855 main.py:50] epoch 3145, training loss: 16718.77, average training loss: 18141.26, base loss: 23667.17
[INFO 2017-06-25 18:22:34,127 main.py:50] epoch 3146, training loss: 18393.86, average training loss: 18142.60, base loss: 23668.94
[INFO 2017-06-25 18:22:38,439 main.py:50] epoch 3147, training loss: 19619.78, average training loss: 18146.06, base loss: 23673.86
[INFO 2017-06-25 18:22:42,732 main.py:50] epoch 3148, training loss: 16104.01, average training loss: 18143.29, base loss: 23669.03
[INFO 2017-06-25 18:22:47,027 main.py:50] epoch 3149, training loss: 11834.39, average training loss: 18136.29, base loss: 23659.64
[INFO 2017-06-25 18:22:51,333 main.py:50] epoch 3150, training loss: 19692.63, average training loss: 18140.45, base loss: 23666.24
[INFO 2017-06-25 18:22:55,674 main.py:50] epoch 3151, training loss: 15714.32, average training loss: 18136.94, base loss: 23662.00
[INFO 2017-06-25 18:22:59,973 main.py:50] epoch 3152, training loss: 16204.12, average training loss: 18134.00, base loss: 23661.14
[INFO 2017-06-25 18:23:04,308 main.py:50] epoch 3153, training loss: 21279.74, average training loss: 18137.65, base loss: 23666.56
[INFO 2017-06-25 18:23:08,611 main.py:50] epoch 3154, training loss: 16783.49, average training loss: 18136.25, base loss: 23665.16
[INFO 2017-06-25 18:23:12,924 main.py:50] epoch 3155, training loss: 17250.68, average training loss: 18133.81, base loss: 23661.04
[INFO 2017-06-25 18:23:17,252 main.py:50] epoch 3156, training loss: 15741.08, average training loss: 18128.59, base loss: 23654.85
[INFO 2017-06-25 18:23:21,554 main.py:50] epoch 3157, training loss: 21727.57, average training loss: 18130.98, base loss: 23658.12
[INFO 2017-06-25 18:23:25,874 main.py:50] epoch 3158, training loss: 20740.54, average training loss: 18131.81, base loss: 23661.29
[INFO 2017-06-25 18:23:30,196 main.py:50] epoch 3159, training loss: 19516.60, average training loss: 18132.94, base loss: 23662.60
[INFO 2017-06-25 18:23:34,507 main.py:50] epoch 3160, training loss: 22538.15, average training loss: 18137.48, base loss: 23668.55
[INFO 2017-06-25 18:23:38,834 main.py:50] epoch 3161, training loss: 16453.78, average training loss: 18133.18, base loss: 23662.47
[INFO 2017-06-25 18:23:43,119 main.py:50] epoch 3162, training loss: 17838.96, average training loss: 18130.14, base loss: 23658.57
[INFO 2017-06-25 18:23:47,448 main.py:50] epoch 3163, training loss: 16820.17, average training loss: 18128.51, base loss: 23659.95
[INFO 2017-06-25 18:23:51,712 main.py:50] epoch 3164, training loss: 15988.43, average training loss: 18126.93, base loss: 23658.28
[INFO 2017-06-25 18:23:56,006 main.py:50] epoch 3165, training loss: 20235.31, average training loss: 18128.14, base loss: 23659.69
[INFO 2017-06-25 18:24:00,299 main.py:50] epoch 3166, training loss: 18684.15, average training loss: 18133.15, base loss: 23667.74
[INFO 2017-06-25 18:24:04,611 main.py:50] epoch 3167, training loss: 17236.00, average training loss: 18133.78, base loss: 23665.89
[INFO 2017-06-25 18:24:08,947 main.py:50] epoch 3168, training loss: 21450.77, average training loss: 18137.98, base loss: 23670.69
[INFO 2017-06-25 18:24:13,289 main.py:50] epoch 3169, training loss: 16494.30, average training loss: 18130.87, base loss: 23662.84
[INFO 2017-06-25 18:24:17,588 main.py:50] epoch 3170, training loss: 17140.61, average training loss: 18129.78, base loss: 23664.64
[INFO 2017-06-25 18:24:21,914 main.py:50] epoch 3171, training loss: 16679.12, average training loss: 18129.58, base loss: 23663.76
[INFO 2017-06-25 18:24:26,219 main.py:50] epoch 3172, training loss: 18181.43, average training loss: 18126.40, base loss: 23660.51
[INFO 2017-06-25 18:24:30,508 main.py:50] epoch 3173, training loss: 17654.14, average training loss: 18123.75, base loss: 23658.64
[INFO 2017-06-25 18:24:34,812 main.py:50] epoch 3174, training loss: 15241.04, average training loss: 18122.58, base loss: 23657.17
[INFO 2017-06-25 18:24:39,083 main.py:50] epoch 3175, training loss: 14041.84, average training loss: 18122.24, base loss: 23656.31
[INFO 2017-06-25 18:24:43,401 main.py:50] epoch 3176, training loss: 18965.83, average training loss: 18122.90, base loss: 23659.32
[INFO 2017-06-25 18:24:47,692 main.py:50] epoch 3177, training loss: 19826.01, average training loss: 18126.98, base loss: 23666.59
[INFO 2017-06-25 18:24:51,970 main.py:50] epoch 3178, training loss: 19251.87, average training loss: 18131.31, base loss: 23672.92
[INFO 2017-06-25 18:24:56,298 main.py:50] epoch 3179, training loss: 25176.60, average training loss: 18136.93, base loss: 23681.39
[INFO 2017-06-25 18:25:00,602 main.py:50] epoch 3180, training loss: 19159.82, average training loss: 18139.59, base loss: 23685.25
[INFO 2017-06-25 18:25:04,899 main.py:50] epoch 3181, training loss: 21156.29, average training loss: 18145.56, base loss: 23691.51
[INFO 2017-06-25 18:25:09,183 main.py:50] epoch 3182, training loss: 18989.07, average training loss: 18144.71, base loss: 23686.10
[INFO 2017-06-25 18:25:13,471 main.py:50] epoch 3183, training loss: 16556.33, average training loss: 18141.22, base loss: 23683.57
[INFO 2017-06-25 18:25:17,733 main.py:50] epoch 3184, training loss: 20032.40, average training loss: 18144.74, base loss: 23688.63
[INFO 2017-06-25 18:25:22,040 main.py:50] epoch 3185, training loss: 16264.84, average training loss: 18142.99, base loss: 23685.83
[INFO 2017-06-25 18:25:26,336 main.py:50] epoch 3186, training loss: 16693.11, average training loss: 18138.25, base loss: 23677.54
[INFO 2017-06-25 18:25:30,662 main.py:50] epoch 3187, training loss: 13997.06, average training loss: 18131.58, base loss: 23671.01
[INFO 2017-06-25 18:25:34,971 main.py:50] epoch 3188, training loss: 17647.40, average training loss: 18130.25, base loss: 23671.05
[INFO 2017-06-25 18:25:39,294 main.py:50] epoch 3189, training loss: 14103.12, average training loss: 18128.48, base loss: 23667.67
[INFO 2017-06-25 18:25:43,607 main.py:50] epoch 3190, training loss: 17413.00, average training loss: 18129.47, base loss: 23671.52
[INFO 2017-06-25 18:25:47,942 main.py:50] epoch 3191, training loss: 14225.56, average training loss: 18119.86, base loss: 23660.77
[INFO 2017-06-25 18:25:52,271 main.py:50] epoch 3192, training loss: 15484.37, average training loss: 18119.29, base loss: 23657.88
[INFO 2017-06-25 18:25:56,580 main.py:50] epoch 3193, training loss: 16701.85, average training loss: 18114.34, base loss: 23651.14
[INFO 2017-06-25 18:26:00,894 main.py:50] epoch 3194, training loss: 15014.06, average training loss: 18113.18, base loss: 23647.89
[INFO 2017-06-25 18:26:05,213 main.py:50] epoch 3195, training loss: 16107.81, average training loss: 18110.92, base loss: 23647.56
[INFO 2017-06-25 18:26:09,511 main.py:50] epoch 3196, training loss: 15429.98, average training loss: 18105.28, base loss: 23644.08
[INFO 2017-06-25 18:26:13,837 main.py:50] epoch 3197, training loss: 17821.54, average training loss: 18103.75, base loss: 23641.73
[INFO 2017-06-25 18:26:18,124 main.py:50] epoch 3198, training loss: 18834.28, average training loss: 18103.80, base loss: 23638.16
[INFO 2017-06-25 18:26:22,397 main.py:50] epoch 3199, training loss: 17846.65, average training loss: 18094.76, base loss: 23626.08
[INFO 2017-06-25 18:26:26,710 main.py:50] epoch 3200, training loss: 15815.58, average training loss: 18093.32, base loss: 23621.52
[INFO 2017-06-25 18:26:30,975 main.py:50] epoch 3201, training loss: 16668.79, average training loss: 18089.70, base loss: 23613.82
[INFO 2017-06-25 18:26:35,269 main.py:50] epoch 3202, training loss: 14606.78, average training loss: 18084.49, base loss: 23606.40
[INFO 2017-06-25 18:26:39,551 main.py:50] epoch 3203, training loss: 18647.32, average training loss: 18082.05, base loss: 23603.07
[INFO 2017-06-25 18:26:43,812 main.py:50] epoch 3204, training loss: 17030.12, average training loss: 18076.99, base loss: 23597.24
[INFO 2017-06-25 18:26:48,123 main.py:50] epoch 3205, training loss: 14596.45, average training loss: 18073.86, base loss: 23593.99
[INFO 2017-06-25 18:26:52,437 main.py:50] epoch 3206, training loss: 20363.40, average training loss: 18065.35, base loss: 23582.82
[INFO 2017-06-25 18:26:56,728 main.py:50] epoch 3207, training loss: 19370.90, average training loss: 18069.00, base loss: 23588.43
[INFO 2017-06-25 18:27:01,020 main.py:50] epoch 3208, training loss: 19623.27, average training loss: 18070.30, base loss: 23589.01
[INFO 2017-06-25 18:27:05,343 main.py:50] epoch 3209, training loss: 17815.75, average training loss: 18067.35, base loss: 23587.05
[INFO 2017-06-25 18:27:09,645 main.py:50] epoch 3210, training loss: 19055.93, average training loss: 18069.57, base loss: 23591.86
[INFO 2017-06-25 18:27:13,947 main.py:50] epoch 3211, training loss: 22121.98, average training loss: 18074.32, base loss: 23596.58
[INFO 2017-06-25 18:27:18,239 main.py:50] epoch 3212, training loss: 19011.07, average training loss: 18073.74, base loss: 23597.41
[INFO 2017-06-25 18:27:22,526 main.py:50] epoch 3213, training loss: 19669.53, average training loss: 18075.20, base loss: 23600.43
[INFO 2017-06-25 18:27:26,835 main.py:50] epoch 3214, training loss: 16975.53, average training loss: 18073.09, base loss: 23599.51
[INFO 2017-06-25 18:27:31,115 main.py:50] epoch 3215, training loss: 21726.57, average training loss: 18079.44, base loss: 23609.55
[INFO 2017-06-25 18:27:35,436 main.py:50] epoch 3216, training loss: 21210.80, average training loss: 18080.97, base loss: 23611.48
[INFO 2017-06-25 18:27:39,704 main.py:50] epoch 3217, training loss: 18469.26, average training loss: 18082.42, base loss: 23612.58
[INFO 2017-06-25 18:27:43,984 main.py:50] epoch 3218, training loss: 18566.91, average training loss: 18084.42, base loss: 23615.02
[INFO 2017-06-25 18:27:48,271 main.py:50] epoch 3219, training loss: 12990.47, average training loss: 18079.24, base loss: 23610.60
[INFO 2017-06-25 18:27:52,565 main.py:50] epoch 3220, training loss: 19575.30, average training loss: 18081.95, base loss: 23613.95
[INFO 2017-06-25 18:27:56,836 main.py:50] epoch 3221, training loss: 18101.30, average training loss: 18080.19, base loss: 23613.08
[INFO 2017-06-25 18:28:01,147 main.py:50] epoch 3222, training loss: 17406.24, average training loss: 18076.47, base loss: 23606.46
[INFO 2017-06-25 18:28:05,437 main.py:50] epoch 3223, training loss: 19593.19, average training loss: 18080.34, base loss: 23612.92
[INFO 2017-06-25 18:28:09,718 main.py:50] epoch 3224, training loss: 18770.23, average training loss: 18082.36, base loss: 23614.06
[INFO 2017-06-25 18:28:14,001 main.py:50] epoch 3225, training loss: 19644.99, average training loss: 18087.45, base loss: 23619.87
[INFO 2017-06-25 18:28:18,303 main.py:50] epoch 3226, training loss: 20809.80, average training loss: 18090.72, base loss: 23624.66
[INFO 2017-06-25 18:28:22,603 main.py:50] epoch 3227, training loss: 17968.31, average training loss: 18094.17, base loss: 23630.53
[INFO 2017-06-25 18:28:26,893 main.py:50] epoch 3228, training loss: 17026.90, average training loss: 18093.50, base loss: 23630.41
[INFO 2017-06-25 18:28:31,190 main.py:50] epoch 3229, training loss: 17769.42, average training loss: 18098.04, base loss: 23637.68
[INFO 2017-06-25 18:28:35,469 main.py:50] epoch 3230, training loss: 19281.54, average training loss: 18092.24, base loss: 23631.86
[INFO 2017-06-25 18:28:39,764 main.py:50] epoch 3231, training loss: 19096.31, average training loss: 18092.67, base loss: 23632.15
[INFO 2017-06-25 18:28:44,031 main.py:50] epoch 3232, training loss: 21819.66, average training loss: 18099.83, base loss: 23641.58
[INFO 2017-06-25 18:28:48,302 main.py:50] epoch 3233, training loss: 16876.10, average training loss: 18093.65, base loss: 23634.62
[INFO 2017-06-25 18:28:52,611 main.py:50] epoch 3234, training loss: 18601.39, average training loss: 18096.90, base loss: 23639.85
[INFO 2017-06-25 18:28:56,864 main.py:50] epoch 3235, training loss: 24105.06, average training loss: 18105.02, base loss: 23652.07
[INFO 2017-06-25 18:29:01,156 main.py:50] epoch 3236, training loss: 17885.75, average training loss: 18098.42, base loss: 23643.68
[INFO 2017-06-25 18:29:05,441 main.py:50] epoch 3237, training loss: 19825.86, average training loss: 18102.07, base loss: 23646.23
[INFO 2017-06-25 18:29:09,726 main.py:50] epoch 3238, training loss: 15961.93, average training loss: 18098.86, base loss: 23642.64
[INFO 2017-06-25 18:29:14,019 main.py:50] epoch 3239, training loss: 18436.36, average training loss: 18098.47, base loss: 23640.73
[INFO 2017-06-25 18:29:18,332 main.py:50] epoch 3240, training loss: 16622.41, average training loss: 18102.01, base loss: 23647.97
[INFO 2017-06-25 18:29:22,614 main.py:50] epoch 3241, training loss: 25359.88, average training loss: 18109.20, base loss: 23660.44
[INFO 2017-06-25 18:29:26,899 main.py:50] epoch 3242, training loss: 15047.61, average training loss: 18105.47, base loss: 23656.70
[INFO 2017-06-25 18:29:31,190 main.py:50] epoch 3243, training loss: 18067.04, average training loss: 18106.43, base loss: 23657.56
[INFO 2017-06-25 18:29:35,492 main.py:50] epoch 3244, training loss: 18345.52, average training loss: 18110.60, base loss: 23662.02
[INFO 2017-06-25 18:29:39,780 main.py:50] epoch 3245, training loss: 19137.05, average training loss: 18111.66, base loss: 23663.18
[INFO 2017-06-25 18:29:44,078 main.py:50] epoch 3246, training loss: 19731.91, average training loss: 18111.16, base loss: 23666.27
[INFO 2017-06-25 18:29:48,351 main.py:50] epoch 3247, training loss: 14755.42, average training loss: 18108.36, base loss: 23661.77
[INFO 2017-06-25 18:29:52,641 main.py:50] epoch 3248, training loss: 17350.38, average training loss: 18106.37, base loss: 23660.07
[INFO 2017-06-25 18:29:56,930 main.py:50] epoch 3249, training loss: 15008.04, average training loss: 18099.36, base loss: 23650.36
[INFO 2017-06-25 18:30:01,191 main.py:50] epoch 3250, training loss: 19221.12, average training loss: 18103.11, base loss: 23653.52
[INFO 2017-06-25 18:30:05,500 main.py:50] epoch 3251, training loss: 20747.69, average training loss: 18108.63, base loss: 23661.51
[INFO 2017-06-25 18:30:09,784 main.py:50] epoch 3252, training loss: 18102.63, average training loss: 18106.67, base loss: 23657.67
[INFO 2017-06-25 18:30:14,056 main.py:50] epoch 3253, training loss: 16048.25, average training loss: 18108.63, base loss: 23660.94
[INFO 2017-06-25 18:30:18,349 main.py:50] epoch 3254, training loss: 17429.95, average training loss: 18111.27, base loss: 23666.43
[INFO 2017-06-25 18:30:22,641 main.py:50] epoch 3255, training loss: 15166.02, average training loss: 18104.13, base loss: 23657.48
[INFO 2017-06-25 18:30:26,932 main.py:50] epoch 3256, training loss: 20435.28, average training loss: 18101.38, base loss: 23655.12
[INFO 2017-06-25 18:30:31,224 main.py:50] epoch 3257, training loss: 19706.99, average training loss: 18103.49, base loss: 23656.84
[INFO 2017-06-25 18:30:35,511 main.py:50] epoch 3258, training loss: 17966.98, average training loss: 18100.85, base loss: 23656.29
[INFO 2017-06-25 18:30:39,786 main.py:50] epoch 3259, training loss: 17206.45, average training loss: 18101.92, base loss: 23658.44
[INFO 2017-06-25 18:30:44,081 main.py:50] epoch 3260, training loss: 15743.45, average training loss: 18101.14, base loss: 23658.09
[INFO 2017-06-25 18:30:48,369 main.py:50] epoch 3261, training loss: 16854.56, average training loss: 18103.06, base loss: 23661.78
[INFO 2017-06-25 18:30:52,683 main.py:50] epoch 3262, training loss: 16435.64, average training loss: 18102.82, base loss: 23660.27
[INFO 2017-06-25 18:30:56,992 main.py:50] epoch 3263, training loss: 17080.08, average training loss: 18100.94, base loss: 23657.61
[INFO 2017-06-25 18:31:01,303 main.py:50] epoch 3264, training loss: 17645.56, average training loss: 18099.27, base loss: 23653.45
[INFO 2017-06-25 18:31:05,579 main.py:50] epoch 3265, training loss: 23981.49, average training loss: 18107.66, base loss: 23664.70
[INFO 2017-06-25 18:31:09,886 main.py:50] epoch 3266, training loss: 14332.39, average training loss: 18103.23, base loss: 23658.71
[INFO 2017-06-25 18:31:14,145 main.py:50] epoch 3267, training loss: 14908.68, average training loss: 18098.85, base loss: 23654.05
[INFO 2017-06-25 18:31:18,440 main.py:50] epoch 3268, training loss: 16708.00, average training loss: 18099.74, base loss: 23655.94
[INFO 2017-06-25 18:31:22,719 main.py:50] epoch 3269, training loss: 15246.76, average training loss: 18098.30, base loss: 23655.19
[INFO 2017-06-25 18:31:26,996 main.py:50] epoch 3270, training loss: 15190.92, average training loss: 18098.16, base loss: 23655.77
[INFO 2017-06-25 18:31:31,316 main.py:50] epoch 3271, training loss: 17190.47, average training loss: 18096.48, base loss: 23653.59
[INFO 2017-06-25 18:31:35,614 main.py:50] epoch 3272, training loss: 14727.91, average training loss: 18086.52, base loss: 23641.71
[INFO 2017-06-25 18:31:39,909 main.py:50] epoch 3273, training loss: 21082.50, average training loss: 18092.82, base loss: 23653.32
[INFO 2017-06-25 18:31:44,212 main.py:50] epoch 3274, training loss: 18811.06, average training loss: 18092.83, base loss: 23653.60
[INFO 2017-06-25 18:31:48,493 main.py:50] epoch 3275, training loss: 18166.19, average training loss: 18091.42, base loss: 23650.79
[INFO 2017-06-25 18:31:52,783 main.py:50] epoch 3276, training loss: 17640.29, average training loss: 18092.86, base loss: 23651.94
[INFO 2017-06-25 18:31:57,067 main.py:50] epoch 3277, training loss: 14427.03, average training loss: 18085.91, base loss: 23646.61
[INFO 2017-06-25 18:32:01,391 main.py:50] epoch 3278, training loss: 18434.46, average training loss: 18083.34, base loss: 23641.04
[INFO 2017-06-25 18:32:05,650 main.py:50] epoch 3279, training loss: 15942.17, average training loss: 18077.29, base loss: 23632.74
[INFO 2017-06-25 18:32:09,944 main.py:50] epoch 3280, training loss: 15839.90, average training loss: 18078.41, base loss: 23635.42
[INFO 2017-06-25 18:32:14,247 main.py:50] epoch 3281, training loss: 19461.03, average training loss: 18073.47, base loss: 23629.50
[INFO 2017-06-25 18:32:18,534 main.py:50] epoch 3282, training loss: 14638.51, average training loss: 18066.49, base loss: 23621.49
[INFO 2017-06-25 18:32:22,815 main.py:50] epoch 3283, training loss: 18742.20, average training loss: 18067.59, base loss: 23627.95
[INFO 2017-06-25 18:32:27,081 main.py:50] epoch 3284, training loss: 15966.66, average training loss: 18061.24, base loss: 23618.21
[INFO 2017-06-25 18:32:31,386 main.py:50] epoch 3285, training loss: 20688.96, average training loss: 18059.96, base loss: 23616.67
[INFO 2017-06-25 18:32:35,667 main.py:50] epoch 3286, training loss: 18667.59, average training loss: 18063.46, base loss: 23620.52
[INFO 2017-06-25 18:32:39,945 main.py:50] epoch 3287, training loss: 14945.38, average training loss: 18061.88, base loss: 23620.19
[INFO 2017-06-25 18:32:44,226 main.py:50] epoch 3288, training loss: 15527.50, average training loss: 18056.19, base loss: 23613.79
[INFO 2017-06-25 18:32:48,533 main.py:50] epoch 3289, training loss: 19010.83, average training loss: 18053.36, base loss: 23612.08
[INFO 2017-06-25 18:32:52,805 main.py:50] epoch 3290, training loss: 17934.19, average training loss: 18052.24, base loss: 23609.87
[INFO 2017-06-25 18:32:57,102 main.py:50] epoch 3291, training loss: 11320.07, average training loss: 18042.57, base loss: 23596.67
[INFO 2017-06-25 18:33:01,382 main.py:50] epoch 3292, training loss: 17385.94, average training loss: 18040.39, base loss: 23592.09
[INFO 2017-06-25 18:33:05,681 main.py:50] epoch 3293, training loss: 18231.40, average training loss: 18039.56, base loss: 23594.00
[INFO 2017-06-25 18:33:09,982 main.py:50] epoch 3294, training loss: 15450.51, average training loss: 18039.81, base loss: 23598.00
[INFO 2017-06-25 18:33:14,255 main.py:50] epoch 3295, training loss: 20266.00, average training loss: 18042.01, base loss: 23598.59
[INFO 2017-06-25 18:33:18,523 main.py:50] epoch 3296, training loss: 15210.72, average training loss: 18036.04, base loss: 23590.06
[INFO 2017-06-25 18:33:22,831 main.py:50] epoch 3297, training loss: 18037.10, average training loss: 18035.58, base loss: 23588.98
[INFO 2017-06-25 18:33:27,096 main.py:50] epoch 3298, training loss: 15681.99, average training loss: 18031.53, base loss: 23584.63
[INFO 2017-06-25 18:33:31,390 main.py:50] epoch 3299, training loss: 20585.79, average training loss: 18036.83, base loss: 23593.79
[INFO 2017-06-25 18:33:35,702 main.py:50] epoch 3300, training loss: 16877.18, average training loss: 18036.01, base loss: 23588.22
[INFO 2017-06-25 18:33:39,972 main.py:50] epoch 3301, training loss: 18139.76, average training loss: 18037.17, base loss: 23589.51
[INFO 2017-06-25 18:33:44,237 main.py:50] epoch 3302, training loss: 14318.57, average training loss: 18036.12, base loss: 23590.43
[INFO 2017-06-25 18:33:48,511 main.py:50] epoch 3303, training loss: 16154.28, average training loss: 18034.07, base loss: 23587.61
[INFO 2017-06-25 18:33:52,808 main.py:50] epoch 3304, training loss: 17366.16, average training loss: 18028.48, base loss: 23582.51
[INFO 2017-06-25 18:33:57,102 main.py:50] epoch 3305, training loss: 16798.93, average training loss: 18029.43, base loss: 23582.80
[INFO 2017-06-25 18:34:01,370 main.py:50] epoch 3306, training loss: 11608.44, average training loss: 18024.07, base loss: 23577.70
[INFO 2017-06-25 18:34:05,627 main.py:50] epoch 3307, training loss: 19724.80, average training loss: 18027.31, base loss: 23580.57
[INFO 2017-06-25 18:34:09,905 main.py:50] epoch 3308, training loss: 14405.22, average training loss: 18015.26, base loss: 23565.07
[INFO 2017-06-25 18:34:14,182 main.py:50] epoch 3309, training loss: 12546.15, average training loss: 18010.85, base loss: 23559.13
[INFO 2017-06-25 18:34:18,472 main.py:50] epoch 3310, training loss: 18672.82, average training loss: 18007.69, base loss: 23554.17
[INFO 2017-06-25 18:34:22,739 main.py:50] epoch 3311, training loss: 14376.75, average training loss: 18007.45, base loss: 23553.59
[INFO 2017-06-25 18:34:27,039 main.py:50] epoch 3312, training loss: 20266.29, average training loss: 18012.24, base loss: 23562.68
[INFO 2017-06-25 18:34:31,307 main.py:50] epoch 3313, training loss: 17826.06, average training loss: 18010.73, base loss: 23559.47
[INFO 2017-06-25 18:34:35,601 main.py:50] epoch 3314, training loss: 16687.04, average training loss: 18007.62, base loss: 23558.03
[INFO 2017-06-25 18:34:39,884 main.py:50] epoch 3315, training loss: 18259.88, average training loss: 18007.51, base loss: 23559.24
[INFO 2017-06-25 18:34:44,174 main.py:50] epoch 3316, training loss: 13417.42, average training loss: 18002.58, base loss: 23552.46
[INFO 2017-06-25 18:34:48,452 main.py:50] epoch 3317, training loss: 20866.30, average training loss: 18005.36, base loss: 23554.14
[INFO 2017-06-25 18:34:52,737 main.py:50] epoch 3318, training loss: 18633.12, average training loss: 18005.24, base loss: 23552.90
[INFO 2017-06-25 18:34:57,030 main.py:50] epoch 3319, training loss: 22821.02, average training loss: 18010.26, base loss: 23559.02
[INFO 2017-06-25 18:35:01,328 main.py:50] epoch 3320, training loss: 18614.95, average training loss: 18014.52, base loss: 23563.80
[INFO 2017-06-25 18:35:05,641 main.py:50] epoch 3321, training loss: 20657.58, average training loss: 18016.32, base loss: 23566.43
[INFO 2017-06-25 18:35:09,942 main.py:50] epoch 3322, training loss: 22746.63, average training loss: 18017.48, base loss: 23565.87
[INFO 2017-06-25 18:35:14,227 main.py:50] epoch 3323, training loss: 18999.32, average training loss: 18020.67, base loss: 23574.98
[INFO 2017-06-25 18:35:18,515 main.py:50] epoch 3324, training loss: 16875.20, average training loss: 18014.46, base loss: 23566.63
[INFO 2017-06-25 18:35:22,779 main.py:50] epoch 3325, training loss: 17577.39, average training loss: 18013.58, base loss: 23566.30
[INFO 2017-06-25 18:35:27,072 main.py:50] epoch 3326, training loss: 21371.68, average training loss: 18017.65, base loss: 23569.27
[INFO 2017-06-25 18:35:31,353 main.py:50] epoch 3327, training loss: 15645.46, average training loss: 18016.88, base loss: 23567.20
[INFO 2017-06-25 18:35:35,661 main.py:50] epoch 3328, training loss: 19336.36, average training loss: 18019.21, base loss: 23572.81
[INFO 2017-06-25 18:35:39,953 main.py:50] epoch 3329, training loss: 16902.92, average training loss: 18017.92, base loss: 23573.26
[INFO 2017-06-25 18:35:44,220 main.py:50] epoch 3330, training loss: 17742.21, average training loss: 18022.01, base loss: 23577.48
[INFO 2017-06-25 18:35:48,496 main.py:50] epoch 3331, training loss: 16644.41, average training loss: 18017.32, base loss: 23571.57
[INFO 2017-06-25 18:35:52,787 main.py:50] epoch 3332, training loss: 20915.25, average training loss: 18024.76, base loss: 23581.63
[INFO 2017-06-25 18:35:57,086 main.py:50] epoch 3333, training loss: 17078.34, average training loss: 18027.53, base loss: 23587.27
[INFO 2017-06-25 18:36:01,361 main.py:50] epoch 3334, training loss: 16748.70, average training loss: 18025.32, base loss: 23585.09
[INFO 2017-06-25 18:36:05,652 main.py:50] epoch 3335, training loss: 16173.33, average training loss: 18025.09, base loss: 23586.71
[INFO 2017-06-25 18:36:09,946 main.py:50] epoch 3336, training loss: 18716.85, average training loss: 18028.22, base loss: 23591.24
[INFO 2017-06-25 18:36:14,228 main.py:50] epoch 3337, training loss: 15792.95, average training loss: 18024.01, base loss: 23584.17
[INFO 2017-06-25 18:36:18,503 main.py:50] epoch 3338, training loss: 15642.84, average training loss: 18021.45, base loss: 23581.93
[INFO 2017-06-25 18:36:22,791 main.py:50] epoch 3339, training loss: 23026.06, average training loss: 18019.14, base loss: 23580.57
[INFO 2017-06-25 18:36:27,069 main.py:50] epoch 3340, training loss: 24085.50, average training loss: 18026.76, base loss: 23591.83
[INFO 2017-06-25 18:36:31,339 main.py:50] epoch 3341, training loss: 15031.65, average training loss: 18024.88, base loss: 23590.38
[INFO 2017-06-25 18:36:35,608 main.py:50] epoch 3342, training loss: 18562.95, average training loss: 18023.22, base loss: 23588.95
[INFO 2017-06-25 18:36:39,891 main.py:50] epoch 3343, training loss: 18177.92, average training loss: 18020.37, base loss: 23583.37
[INFO 2017-06-25 18:36:44,205 main.py:50] epoch 3344, training loss: 16492.14, average training loss: 18020.11, base loss: 23584.92
[INFO 2017-06-25 18:36:48,499 main.py:50] epoch 3345, training loss: 14695.55, average training loss: 18015.46, base loss: 23583.66
[INFO 2017-06-25 18:36:52,814 main.py:50] epoch 3346, training loss: 16892.35, average training loss: 18017.17, base loss: 23584.25
[INFO 2017-06-25 18:36:57,097 main.py:50] epoch 3347, training loss: 18205.97, average training loss: 18017.10, base loss: 23588.28
[INFO 2017-06-25 18:37:01,374 main.py:50] epoch 3348, training loss: 21042.47, average training loss: 18021.84, base loss: 23589.98
[INFO 2017-06-25 18:37:05,626 main.py:50] epoch 3349, training loss: 16462.77, average training loss: 18022.09, base loss: 23589.45
[INFO 2017-06-25 18:37:09,897 main.py:50] epoch 3350, training loss: 16504.80, average training loss: 18020.50, base loss: 23588.48
[INFO 2017-06-25 18:37:14,157 main.py:50] epoch 3351, training loss: 18809.61, average training loss: 18023.03, base loss: 23593.23
[INFO 2017-06-25 18:37:18,445 main.py:50] epoch 3352, training loss: 15289.55, average training loss: 18022.18, base loss: 23595.95
[INFO 2017-06-25 18:37:22,730 main.py:50] epoch 3353, training loss: 18375.48, average training loss: 18024.50, base loss: 23597.73
[INFO 2017-06-25 18:37:27,015 main.py:50] epoch 3354, training loss: 14640.80, average training loss: 18024.17, base loss: 23598.21
[INFO 2017-06-25 18:37:31,300 main.py:50] epoch 3355, training loss: 16575.54, average training loss: 18022.23, base loss: 23593.34
[INFO 2017-06-25 18:37:35,552 main.py:50] epoch 3356, training loss: 19757.05, average training loss: 18024.46, base loss: 23597.35
[INFO 2017-06-25 18:37:39,817 main.py:50] epoch 3357, training loss: 21864.43, average training loss: 18024.38, base loss: 23599.07
[INFO 2017-06-25 18:37:44,090 main.py:50] epoch 3358, training loss: 17687.17, average training loss: 18024.35, base loss: 23595.90
[INFO 2017-06-25 18:37:48,382 main.py:50] epoch 3359, training loss: 13193.79, average training loss: 18018.41, base loss: 23589.34
[INFO 2017-06-25 18:37:52,659 main.py:50] epoch 3360, training loss: 13891.81, average training loss: 18011.76, base loss: 23578.59
[INFO 2017-06-25 18:37:56,971 main.py:50] epoch 3361, training loss: 20824.54, average training loss: 18012.00, base loss: 23573.24
[INFO 2017-06-25 18:38:01,254 main.py:50] epoch 3362, training loss: 16775.45, average training loss: 18013.57, base loss: 23577.32
[INFO 2017-06-25 18:38:05,532 main.py:50] epoch 3363, training loss: 15338.98, average training loss: 18012.70, base loss: 23575.02
[INFO 2017-06-25 18:38:09,807 main.py:50] epoch 3364, training loss: 17378.05, average training loss: 18014.05, base loss: 23577.61
[INFO 2017-06-25 18:38:14,103 main.py:50] epoch 3365, training loss: 19556.20, average training loss: 18019.65, base loss: 23584.56
[INFO 2017-06-25 18:38:18,358 main.py:50] epoch 3366, training loss: 17981.09, average training loss: 18017.42, base loss: 23579.03
[INFO 2017-06-25 18:38:22,663 main.py:50] epoch 3367, training loss: 19990.71, average training loss: 18019.48, base loss: 23584.65
[INFO 2017-06-25 18:38:26,934 main.py:50] epoch 3368, training loss: 11914.24, average training loss: 18012.54, base loss: 23574.56
[INFO 2017-06-25 18:38:31,223 main.py:50] epoch 3369, training loss: 17758.16, average training loss: 18011.82, base loss: 23577.62
[INFO 2017-06-25 18:38:35,493 main.py:50] epoch 3370, training loss: 18411.81, average training loss: 18011.77, base loss: 23577.03
[INFO 2017-06-25 18:38:39,812 main.py:50] epoch 3371, training loss: 16140.53, average training loss: 18011.38, base loss: 23575.58
[INFO 2017-06-25 18:38:44,114 main.py:50] epoch 3372, training loss: 20478.50, average training loss: 18014.15, base loss: 23576.67
[INFO 2017-06-25 18:38:48,422 main.py:50] epoch 3373, training loss: 18870.01, average training loss: 18014.97, base loss: 23578.18
[INFO 2017-06-25 18:38:52,713 main.py:50] epoch 3374, training loss: 20765.99, average training loss: 18017.80, base loss: 23583.48
[INFO 2017-06-25 18:38:57,005 main.py:50] epoch 3375, training loss: 16712.82, average training loss: 18018.33, base loss: 23584.36
[INFO 2017-06-25 18:39:01,251 main.py:50] epoch 3376, training loss: 15192.55, average training loss: 18014.89, base loss: 23580.97
[INFO 2017-06-25 18:39:05,528 main.py:50] epoch 3377, training loss: 18776.51, average training loss: 18013.67, base loss: 23582.02
[INFO 2017-06-25 18:39:09,797 main.py:50] epoch 3378, training loss: 20280.31, average training loss: 18014.25, base loss: 23584.55
[INFO 2017-06-25 18:39:14,104 main.py:50] epoch 3379, training loss: 17258.07, average training loss: 18010.11, base loss: 23579.93
[INFO 2017-06-25 18:39:18,391 main.py:50] epoch 3380, training loss: 16026.41, average training loss: 18002.02, base loss: 23568.35
[INFO 2017-06-25 18:39:22,672 main.py:50] epoch 3381, training loss: 18371.76, average training loss: 18000.66, base loss: 23566.78
[INFO 2017-06-25 18:39:26,973 main.py:50] epoch 3382, training loss: 16812.39, average training loss: 17998.35, base loss: 23565.19
[INFO 2017-06-25 18:39:31,292 main.py:50] epoch 3383, training loss: 18422.36, average training loss: 18000.53, base loss: 23566.70
[INFO 2017-06-25 18:39:35,591 main.py:50] epoch 3384, training loss: 15371.53, average training loss: 17998.09, base loss: 23564.14
[INFO 2017-06-25 18:39:39,904 main.py:50] epoch 3385, training loss: 19946.24, average training loss: 17995.70, base loss: 23564.36
[INFO 2017-06-25 18:39:44,198 main.py:50] epoch 3386, training loss: 17878.34, average training loss: 17996.06, base loss: 23564.83
[INFO 2017-06-25 18:39:48,483 main.py:50] epoch 3387, training loss: 19427.65, average training loss: 17998.16, base loss: 23571.69
[INFO 2017-06-25 18:39:52,774 main.py:50] epoch 3388, training loss: 18743.44, average training loss: 17994.14, base loss: 23568.65
[INFO 2017-06-25 18:39:57,060 main.py:50] epoch 3389, training loss: 13697.55, average training loss: 17987.17, base loss: 23560.91
[INFO 2017-06-25 18:40:01,373 main.py:50] epoch 3390, training loss: 16137.36, average training loss: 17982.70, base loss: 23556.49
[INFO 2017-06-25 18:40:05,645 main.py:50] epoch 3391, training loss: 20873.82, average training loss: 17989.35, base loss: 23567.17
[INFO 2017-06-25 18:40:09,935 main.py:50] epoch 3392, training loss: 15251.08, average training loss: 17981.92, base loss: 23557.43
[INFO 2017-06-25 18:40:14,225 main.py:50] epoch 3393, training loss: 22067.93, average training loss: 17989.37, base loss: 23563.58
[INFO 2017-06-25 18:40:18,503 main.py:50] epoch 3394, training loss: 20430.51, average training loss: 17991.68, base loss: 23567.96
[INFO 2017-06-25 18:40:22,794 main.py:50] epoch 3395, training loss: 20711.91, average training loss: 17993.04, base loss: 23569.69
[INFO 2017-06-25 18:40:27,098 main.py:50] epoch 3396, training loss: 17965.73, average training loss: 17991.65, base loss: 23569.23
[INFO 2017-06-25 18:40:31,389 main.py:50] epoch 3397, training loss: 16072.16, average training loss: 17986.71, base loss: 23566.71
[INFO 2017-06-25 18:40:35,695 main.py:50] epoch 3398, training loss: 19549.93, average training loss: 17984.48, base loss: 23566.90
[INFO 2017-06-25 18:40:39,993 main.py:50] epoch 3399, training loss: 20258.95, average training loss: 17988.45, base loss: 23573.06
[INFO 2017-06-25 18:40:44,275 main.py:50] epoch 3400, training loss: 18803.52, average training loss: 17991.70, base loss: 23577.35
[INFO 2017-06-25 18:40:48,564 main.py:50] epoch 3401, training loss: 17734.74, average training loss: 17987.93, base loss: 23570.23
[INFO 2017-06-25 18:40:52,842 main.py:50] epoch 3402, training loss: 20568.82, average training loss: 17988.08, base loss: 23570.82
[INFO 2017-06-25 18:40:57,125 main.py:50] epoch 3403, training loss: 14873.05, average training loss: 17985.81, base loss: 23567.73
[INFO 2017-06-25 18:41:01,392 main.py:50] epoch 3404, training loss: 16084.95, average training loss: 17984.05, base loss: 23565.76
[INFO 2017-06-25 18:41:05,667 main.py:50] epoch 3405, training loss: 16573.07, average training loss: 17980.04, base loss: 23558.88
[INFO 2017-06-25 18:41:09,951 main.py:50] epoch 3406, training loss: 17829.20, average training loss: 17978.75, base loss: 23555.69
[INFO 2017-06-25 18:41:14,244 main.py:50] epoch 3407, training loss: 19918.87, average training loss: 17980.45, base loss: 23558.09
[INFO 2017-06-25 18:41:18,524 main.py:50] epoch 3408, training loss: 17580.37, average training loss: 17980.25, base loss: 23559.14
[INFO 2017-06-25 18:41:22,842 main.py:50] epoch 3409, training loss: 20456.85, average training loss: 17985.58, base loss: 23566.42
[INFO 2017-06-25 18:41:27,141 main.py:50] epoch 3410, training loss: 18345.51, average training loss: 17986.90, base loss: 23568.45
[INFO 2017-06-25 18:41:31,440 main.py:50] epoch 3411, training loss: 16829.50, average training loss: 17985.39, base loss: 23567.73
[INFO 2017-06-25 18:41:35,731 main.py:50] epoch 3412, training loss: 19490.46, average training loss: 17985.63, base loss: 23568.65
[INFO 2017-06-25 18:41:40,014 main.py:50] epoch 3413, training loss: 19165.07, average training loss: 17990.10, base loss: 23572.89
[INFO 2017-06-25 18:41:44,305 main.py:50] epoch 3414, training loss: 18281.55, average training loss: 17988.68, base loss: 23573.35
[INFO 2017-06-25 18:41:48,550 main.py:50] epoch 3415, training loss: 14052.22, average training loss: 17981.52, base loss: 23562.04
[INFO 2017-06-25 18:41:52,852 main.py:50] epoch 3416, training loss: 19194.86, average training loss: 17982.27, base loss: 23567.98
[INFO 2017-06-25 18:41:57,140 main.py:50] epoch 3417, training loss: 18183.68, average training loss: 17981.48, base loss: 23566.38
[INFO 2017-06-25 18:42:01,416 main.py:50] epoch 3418, training loss: 18422.57, average training loss: 17982.66, base loss: 23566.42
[INFO 2017-06-25 18:42:05,678 main.py:50] epoch 3419, training loss: 18549.04, average training loss: 17986.73, base loss: 23570.50
[INFO 2017-06-25 18:42:09,971 main.py:50] epoch 3420, training loss: 19101.89, average training loss: 17992.33, base loss: 23579.94
[INFO 2017-06-25 18:42:14,240 main.py:50] epoch 3421, training loss: 16323.38, average training loss: 17988.80, base loss: 23574.57
[INFO 2017-06-25 18:42:18,547 main.py:50] epoch 3422, training loss: 15340.70, average training loss: 17988.15, base loss: 23575.87
[INFO 2017-06-25 18:42:22,816 main.py:50] epoch 3423, training loss: 20479.30, average training loss: 17990.65, base loss: 23577.50
[INFO 2017-06-25 18:42:27,103 main.py:50] epoch 3424, training loss: 21686.90, average training loss: 17994.67, base loss: 23585.10
[INFO 2017-06-25 18:42:31,380 main.py:50] epoch 3425, training loss: 22193.63, average training loss: 17999.77, base loss: 23592.11
[INFO 2017-06-25 18:42:35,648 main.py:50] epoch 3426, training loss: 21823.34, average training loss: 18003.63, base loss: 23595.82
[INFO 2017-06-25 18:42:39,956 main.py:50] epoch 3427, training loss: 15485.86, average training loss: 17998.44, base loss: 23593.36
[INFO 2017-06-25 18:42:44,263 main.py:50] epoch 3428, training loss: 18004.35, average training loss: 17999.51, base loss: 23595.07
[INFO 2017-06-25 18:42:48,540 main.py:50] epoch 3429, training loss: 15232.73, average training loss: 17994.99, base loss: 23589.19
[INFO 2017-06-25 18:42:52,826 main.py:50] epoch 3430, training loss: 17776.26, average training loss: 17996.25, base loss: 23592.48
[INFO 2017-06-25 18:42:57,126 main.py:50] epoch 3431, training loss: 20993.11, average training loss: 17999.68, base loss: 23600.57
[INFO 2017-06-25 18:43:01,427 main.py:50] epoch 3432, training loss: 19858.21, average training loss: 18000.37, base loss: 23603.83
[INFO 2017-06-25 18:43:05,695 main.py:50] epoch 3433, training loss: 17465.92, average training loss: 18002.73, base loss: 23609.04
[INFO 2017-06-25 18:43:09,969 main.py:50] epoch 3434, training loss: 19742.11, average training loss: 18009.39, base loss: 23617.47
[INFO 2017-06-25 18:43:14,277 main.py:50] epoch 3435, training loss: 16907.14, average training loss: 18010.70, base loss: 23618.07
[INFO 2017-06-25 18:43:18,551 main.py:50] epoch 3436, training loss: 17272.99, average training loss: 18012.75, base loss: 23622.18
[INFO 2017-06-25 18:43:22,828 main.py:50] epoch 3437, training loss: 18330.00, average training loss: 18014.88, base loss: 23622.81
[INFO 2017-06-25 18:43:27,132 main.py:50] epoch 3438, training loss: 13426.24, average training loss: 18007.01, base loss: 23611.87
[INFO 2017-06-25 18:43:31,433 main.py:50] epoch 3439, training loss: 14935.37, average training loss: 18007.18, base loss: 23611.91
[INFO 2017-06-25 18:43:35,735 main.py:50] epoch 3440, training loss: 17723.42, average training loss: 18007.84, base loss: 23612.59
[INFO 2017-06-25 18:43:40,063 main.py:50] epoch 3441, training loss: 15756.27, average training loss: 18005.57, base loss: 23609.88
[INFO 2017-06-25 18:43:44,361 main.py:50] epoch 3442, training loss: 14647.04, average training loss: 18000.99, base loss: 23603.24
[INFO 2017-06-25 18:43:48,651 main.py:50] epoch 3443, training loss: 21530.94, average training loss: 18004.26, base loss: 23607.33
[INFO 2017-06-25 18:43:52,943 main.py:50] epoch 3444, training loss: 15922.75, average training loss: 17999.21, base loss: 23600.16
[INFO 2017-06-25 18:43:57,259 main.py:50] epoch 3445, training loss: 17994.32, average training loss: 17999.69, base loss: 23601.23
[INFO 2017-06-25 18:44:01,528 main.py:50] epoch 3446, training loss: 20737.35, average training loss: 18003.88, base loss: 23608.60
[INFO 2017-06-25 18:44:05,786 main.py:50] epoch 3447, training loss: 20042.38, average training loss: 18004.57, base loss: 23609.38
[INFO 2017-06-25 18:44:10,066 main.py:50] epoch 3448, training loss: 10626.63, average training loss: 17995.20, base loss: 23599.30
[INFO 2017-06-25 18:44:14,339 main.py:50] epoch 3449, training loss: 19236.22, average training loss: 17999.12, base loss: 23604.24
[INFO 2017-06-25 18:44:18,622 main.py:50] epoch 3450, training loss: 18508.72, average training loss: 17998.92, base loss: 23602.39
[INFO 2017-06-25 18:44:22,887 main.py:50] epoch 3451, training loss: 19517.74, average training loss: 17999.46, base loss: 23604.12
[INFO 2017-06-25 18:44:27,152 main.py:50] epoch 3452, training loss: 19254.45, average training loss: 18002.98, base loss: 23608.20
[INFO 2017-06-25 18:44:31,433 main.py:50] epoch 3453, training loss: 19991.15, average training loss: 18005.98, base loss: 23610.02
[INFO 2017-06-25 18:44:35,722 main.py:50] epoch 3454, training loss: 19293.83, average training loss: 18004.81, base loss: 23605.82
[INFO 2017-06-25 18:44:40,029 main.py:50] epoch 3455, training loss: 17099.37, average training loss: 18002.59, base loss: 23604.40
[INFO 2017-06-25 18:44:44,318 main.py:50] epoch 3456, training loss: 20119.47, average training loss: 18003.31, base loss: 23603.95
[INFO 2017-06-25 18:44:48,603 main.py:50] epoch 3457, training loss: 18202.51, average training loss: 18005.22, base loss: 23605.31
[INFO 2017-06-25 18:44:52,893 main.py:50] epoch 3458, training loss: 18624.08, average training loss: 18006.22, base loss: 23607.02
[INFO 2017-06-25 18:44:57,186 main.py:50] epoch 3459, training loss: 22955.20, average training loss: 18009.70, base loss: 23611.84
[INFO 2017-06-25 18:45:01,452 main.py:50] epoch 3460, training loss: 17982.70, average training loss: 18009.86, base loss: 23612.75
[INFO 2017-06-25 18:45:05,718 main.py:50] epoch 3461, training loss: 15907.06, average training loss: 18009.77, base loss: 23610.86
[INFO 2017-06-25 18:45:10,028 main.py:50] epoch 3462, training loss: 13505.45, average training loss: 18007.28, base loss: 23609.42
[INFO 2017-06-25 18:45:14,317 main.py:50] epoch 3463, training loss: 20925.38, average training loss: 18010.93, base loss: 23614.84
[INFO 2017-06-25 18:45:18,590 main.py:50] epoch 3464, training loss: 15609.21, average training loss: 18007.63, base loss: 23610.65
[INFO 2017-06-25 18:45:22,880 main.py:50] epoch 3465, training loss: 20890.85, average training loss: 18012.52, base loss: 23617.02
[INFO 2017-06-25 18:45:27,165 main.py:50] epoch 3466, training loss: 24240.60, average training loss: 18020.65, base loss: 23625.60
[INFO 2017-06-25 18:45:31,448 main.py:50] epoch 3467, training loss: 16906.27, average training loss: 18018.28, base loss: 23625.20
[INFO 2017-06-25 18:45:35,732 main.py:50] epoch 3468, training loss: 14548.29, average training loss: 18012.65, base loss: 23621.04
[INFO 2017-06-25 18:45:40,014 main.py:50] epoch 3469, training loss: 13610.43, average training loss: 18004.86, base loss: 23609.87
[INFO 2017-06-25 18:45:44,290 main.py:50] epoch 3470, training loss: 19944.99, average training loss: 18004.31, base loss: 23610.48
[INFO 2017-06-25 18:45:48,606 main.py:50] epoch 3471, training loss: 16504.86, average training loss: 17999.77, base loss: 23604.31
[INFO 2017-06-25 18:45:52,879 main.py:50] epoch 3472, training loss: 18904.71, average training loss: 17999.89, base loss: 23604.58
[INFO 2017-06-25 18:45:57,151 main.py:50] epoch 3473, training loss: 14470.79, average training loss: 17999.64, base loss: 23605.10
[INFO 2017-06-25 18:46:01,452 main.py:50] epoch 3474, training loss: 20200.75, average training loss: 17997.40, base loss: 23604.99
[INFO 2017-06-25 18:46:05,743 main.py:50] epoch 3475, training loss: 20554.37, average training loss: 18001.68, base loss: 23609.32
[INFO 2017-06-25 18:46:10,001 main.py:50] epoch 3476, training loss: 18487.94, average training loss: 18004.16, base loss: 23613.53
[INFO 2017-06-25 18:46:14,280 main.py:50] epoch 3477, training loss: 18868.59, average training loss: 18006.08, base loss: 23614.94
[INFO 2017-06-25 18:46:18,543 main.py:50] epoch 3478, training loss: 15711.56, average training loss: 17999.25, base loss: 23603.86
[INFO 2017-06-25 18:46:22,839 main.py:50] epoch 3479, training loss: 15728.20, average training loss: 17998.31, base loss: 23603.83
[INFO 2017-06-25 18:46:27,149 main.py:50] epoch 3480, training loss: 20188.35, average training loss: 17995.80, base loss: 23598.82
[INFO 2017-06-25 18:46:31,406 main.py:50] epoch 3481, training loss: 15407.65, average training loss: 17991.14, base loss: 23592.46
[INFO 2017-06-25 18:46:35,700 main.py:50] epoch 3482, training loss: 15888.03, average training loss: 17990.57, base loss: 23590.83
[INFO 2017-06-25 18:46:39,993 main.py:50] epoch 3483, training loss: 19261.35, average training loss: 17988.48, base loss: 23590.36
[INFO 2017-06-25 18:46:44,285 main.py:50] epoch 3484, training loss: 12777.97, average training loss: 17985.76, base loss: 23584.02
[INFO 2017-06-25 18:46:48,561 main.py:50] epoch 3485, training loss: 17494.53, average training loss: 17986.72, base loss: 23586.53
[INFO 2017-06-25 18:46:52,870 main.py:50] epoch 3486, training loss: 17062.34, average training loss: 17986.37, base loss: 23586.65
[INFO 2017-06-25 18:46:57,145 main.py:50] epoch 3487, training loss: 18767.91, average training loss: 17987.74, base loss: 23587.05
[INFO 2017-06-25 18:47:01,395 main.py:50] epoch 3488, training loss: 19835.07, average training loss: 17992.19, base loss: 23592.99
[INFO 2017-06-25 18:47:05,692 main.py:50] epoch 3489, training loss: 23324.38, average training loss: 17997.34, base loss: 23601.00
[INFO 2017-06-25 18:47:09,986 main.py:50] epoch 3490, training loss: 14578.42, average training loss: 17995.20, base loss: 23598.91
[INFO 2017-06-25 18:47:14,245 main.py:50] epoch 3491, training loss: 20685.63, average training loss: 17997.43, base loss: 23602.01
[INFO 2017-06-25 18:47:18,526 main.py:50] epoch 3492, training loss: 15493.21, average training loss: 17988.97, base loss: 23589.29
[INFO 2017-06-25 18:47:22,805 main.py:50] epoch 3493, training loss: 18922.30, average training loss: 17988.70, base loss: 23591.57
[INFO 2017-06-25 18:47:27,088 main.py:50] epoch 3494, training loss: 15563.01, average training loss: 17987.07, base loss: 23588.26
[INFO 2017-06-25 18:47:31,368 main.py:50] epoch 3495, training loss: 17381.32, average training loss: 17983.93, base loss: 23583.32
[INFO 2017-06-25 18:47:35,661 main.py:50] epoch 3496, training loss: 14942.47, average training loss: 17980.23, base loss: 23576.19
[INFO 2017-06-25 18:47:39,943 main.py:50] epoch 3497, training loss: 20343.19, average training loss: 17981.74, base loss: 23576.23
[INFO 2017-06-25 18:47:44,224 main.py:50] epoch 3498, training loss: 23322.15, average training loss: 17987.12, base loss: 23585.06
[INFO 2017-06-25 18:47:48,511 main.py:50] epoch 3499, training loss: 18367.00, average training loss: 17986.21, base loss: 23584.87
[INFO 2017-06-25 18:47:52,830 main.py:50] epoch 3500, training loss: 14359.62, average training loss: 17984.37, base loss: 23582.28
[INFO 2017-06-25 18:47:57,115 main.py:50] epoch 3501, training loss: 19714.93, average training loss: 17989.24, base loss: 23587.77
[INFO 2017-06-25 18:48:01,391 main.py:50] epoch 3502, training loss: 19469.14, average training loss: 17990.68, base loss: 23588.96
[INFO 2017-06-25 18:48:05,697 main.py:50] epoch 3503, training loss: 19967.34, average training loss: 17990.88, base loss: 23590.28
[INFO 2017-06-25 18:48:09,991 main.py:50] epoch 3504, training loss: 16611.42, average training loss: 17992.36, base loss: 23591.10
[INFO 2017-06-25 18:48:14,281 main.py:50] epoch 3505, training loss: 14977.99, average training loss: 17987.65, base loss: 23581.79
[INFO 2017-06-25 18:48:18,565 main.py:50] epoch 3506, training loss: 18980.21, average training loss: 17991.62, base loss: 23589.39
[INFO 2017-06-25 18:48:22,865 main.py:50] epoch 3507, training loss: 16858.28, average training loss: 17984.61, base loss: 23580.70
[INFO 2017-06-25 18:48:27,158 main.py:50] epoch 3508, training loss: 16952.41, average training loss: 17982.76, base loss: 23579.78
[INFO 2017-06-25 18:48:31,470 main.py:50] epoch 3509, training loss: 20171.64, average training loss: 17986.01, base loss: 23584.34
[INFO 2017-06-25 18:48:35,751 main.py:50] epoch 3510, training loss: 14279.75, average training loss: 17977.18, base loss: 23571.34
[INFO 2017-06-25 18:48:40,052 main.py:50] epoch 3511, training loss: 20698.46, average training loss: 17979.52, base loss: 23575.42
[INFO 2017-06-25 18:48:44,354 main.py:50] epoch 3512, training loss: 16017.75, average training loss: 17975.48, base loss: 23568.25
[INFO 2017-06-25 18:48:48,621 main.py:50] epoch 3513, training loss: 18371.72, average training loss: 17979.77, base loss: 23575.41
[INFO 2017-06-25 18:48:52,894 main.py:50] epoch 3514, training loss: 14675.50, average training loss: 17972.89, base loss: 23563.11
[INFO 2017-06-25 18:48:57,188 main.py:50] epoch 3515, training loss: 19837.36, average training loss: 17975.80, base loss: 23567.00
[INFO 2017-06-25 18:49:01,458 main.py:50] epoch 3516, training loss: 19621.88, average training loss: 17976.84, base loss: 23567.96
[INFO 2017-06-25 18:49:05,739 main.py:50] epoch 3517, training loss: 18205.25, average training loss: 17972.93, base loss: 23561.12
[INFO 2017-06-25 18:49:10,018 main.py:50] epoch 3518, training loss: 16361.36, average training loss: 17967.90, base loss: 23553.09
[INFO 2017-06-25 18:49:14,318 main.py:50] epoch 3519, training loss: 19552.50, average training loss: 17968.74, base loss: 23555.52
[INFO 2017-06-25 18:49:18,588 main.py:50] epoch 3520, training loss: 18253.50, average training loss: 17969.49, base loss: 23556.53
[INFO 2017-06-25 18:49:22,890 main.py:50] epoch 3521, training loss: 18215.85, average training loss: 17969.04, base loss: 23557.21
[INFO 2017-06-25 18:49:27,137 main.py:50] epoch 3522, training loss: 15779.73, average training loss: 17972.45, base loss: 23560.67
[INFO 2017-06-25 18:49:31,419 main.py:50] epoch 3523, training loss: 17062.98, average training loss: 17974.89, base loss: 23563.04
[INFO 2017-06-25 18:49:35,696 main.py:50] epoch 3524, training loss: 15430.31, average training loss: 17971.80, base loss: 23558.84
[INFO 2017-06-25 18:49:39,972 main.py:50] epoch 3525, training loss: 17748.35, average training loss: 17969.78, base loss: 23558.03
[INFO 2017-06-25 18:49:44,254 main.py:50] epoch 3526, training loss: 24669.12, average training loss: 17974.92, base loss: 23565.44
[INFO 2017-06-25 18:49:48,548 main.py:50] epoch 3527, training loss: 16742.60, average training loss: 17977.29, base loss: 23571.96
[INFO 2017-06-25 18:49:52,842 main.py:50] epoch 3528, training loss: 17808.94, average training loss: 17975.38, base loss: 23569.88
[INFO 2017-06-25 18:49:57,122 main.py:50] epoch 3529, training loss: 14338.37, average training loss: 17973.92, base loss: 23567.51
[INFO 2017-06-25 18:50:01,447 main.py:50] epoch 3530, training loss: 22865.95, average training loss: 17981.72, base loss: 23575.89
[INFO 2017-06-25 18:50:05,706 main.py:50] epoch 3531, training loss: 19433.35, average training loss: 17979.77, base loss: 23571.56
[INFO 2017-06-25 18:50:09,997 main.py:50] epoch 3532, training loss: 19948.15, average training loss: 17980.80, base loss: 23574.35
[INFO 2017-06-25 18:50:14,263 main.py:50] epoch 3533, training loss: 22832.56, average training loss: 17986.52, base loss: 23582.31
[INFO 2017-06-25 18:50:18,540 main.py:50] epoch 3534, training loss: 18652.47, average training loss: 17987.14, base loss: 23583.67
[INFO 2017-06-25 18:50:22,824 main.py:50] epoch 3535, training loss: 16262.63, average training loss: 17988.10, base loss: 23586.42
[INFO 2017-06-25 18:50:27,097 main.py:50] epoch 3536, training loss: 15097.90, average training loss: 17985.96, base loss: 23584.09
[INFO 2017-06-25 18:50:31,376 main.py:50] epoch 3537, training loss: 18783.11, average training loss: 17991.06, base loss: 23591.11
[INFO 2017-06-25 18:50:35,648 main.py:50] epoch 3538, training loss: 18360.03, average training loss: 17989.55, base loss: 23592.00
[INFO 2017-06-25 18:50:39,906 main.py:50] epoch 3539, training loss: 18593.99, average training loss: 17992.16, base loss: 23597.99
[INFO 2017-06-25 18:50:44,183 main.py:50] epoch 3540, training loss: 13075.74, average training loss: 17988.18, base loss: 23591.85
[INFO 2017-06-25 18:50:48,468 main.py:50] epoch 3541, training loss: 15493.70, average training loss: 17981.21, base loss: 23585.01
[INFO 2017-06-25 18:50:52,775 main.py:50] epoch 3542, training loss: 17761.68, average training loss: 17980.87, base loss: 23584.22
[INFO 2017-06-25 18:50:57,043 main.py:50] epoch 3543, training loss: 12867.99, average training loss: 17976.84, base loss: 23575.95
[INFO 2017-06-25 18:51:01,351 main.py:50] epoch 3544, training loss: 15040.50, average training loss: 17975.70, base loss: 23569.83
[INFO 2017-06-25 18:51:05,633 main.py:50] epoch 3545, training loss: 14984.21, average training loss: 17971.56, base loss: 23564.33
[INFO 2017-06-25 18:51:09,927 main.py:50] epoch 3546, training loss: 17328.32, average training loss: 17968.72, base loss: 23558.49
[INFO 2017-06-25 18:51:14,228 main.py:50] epoch 3547, training loss: 16753.97, average training loss: 17970.95, base loss: 23562.71
[INFO 2017-06-25 18:51:18,489 main.py:50] epoch 3548, training loss: 21198.95, average training loss: 17974.38, base loss: 23566.71
[INFO 2017-06-25 18:51:22,761 main.py:50] epoch 3549, training loss: 13776.55, average training loss: 17969.79, base loss: 23558.96
[INFO 2017-06-25 18:51:27,040 main.py:50] epoch 3550, training loss: 15530.44, average training loss: 17964.27, base loss: 23550.76
[INFO 2017-06-25 18:51:31,327 main.py:50] epoch 3551, training loss: 21373.02, average training loss: 17961.87, base loss: 23550.09
[INFO 2017-06-25 18:51:35,608 main.py:50] epoch 3552, training loss: 19791.36, average training loss: 17963.97, base loss: 23552.87
[INFO 2017-06-25 18:51:39,915 main.py:50] epoch 3553, training loss: 17014.24, average training loss: 17961.69, base loss: 23548.90
[INFO 2017-06-25 18:51:44,225 main.py:50] epoch 3554, training loss: 17527.70, average training loss: 17957.95, base loss: 23547.59
[INFO 2017-06-25 18:51:48,519 main.py:50] epoch 3555, training loss: 16625.67, average training loss: 17952.74, base loss: 23540.74
[INFO 2017-06-25 18:51:52,818 main.py:50] epoch 3556, training loss: 18950.94, average training loss: 17950.45, base loss: 23539.81
[INFO 2017-06-25 18:51:57,090 main.py:50] epoch 3557, training loss: 15538.88, average training loss: 17947.05, base loss: 23534.56
[INFO 2017-06-25 18:52:01,362 main.py:50] epoch 3558, training loss: 18932.91, average training loss: 17947.86, base loss: 23535.21
[INFO 2017-06-25 18:52:05,626 main.py:50] epoch 3559, training loss: 16546.18, average training loss: 17944.81, base loss: 23534.16
[INFO 2017-06-25 18:52:09,955 main.py:50] epoch 3560, training loss: 12839.23, average training loss: 17939.71, base loss: 23527.28
[INFO 2017-06-25 18:52:14,241 main.py:50] epoch 3561, training loss: 19701.79, average training loss: 17943.36, base loss: 23530.25
[INFO 2017-06-25 18:52:18,552 main.py:50] epoch 3562, training loss: 20468.17, average training loss: 17944.60, base loss: 23531.44
[INFO 2017-06-25 18:52:22,832 main.py:50] epoch 3563, training loss: 16452.73, average training loss: 17942.89, base loss: 23529.62
[INFO 2017-06-25 18:52:27,095 main.py:50] epoch 3564, training loss: 15499.59, average training loss: 17944.08, base loss: 23530.69
[INFO 2017-06-25 18:52:31,372 main.py:50] epoch 3565, training loss: 17704.65, average training loss: 17941.59, base loss: 23527.82
[INFO 2017-06-25 18:52:35,662 main.py:50] epoch 3566, training loss: 17557.78, average training loss: 17938.66, base loss: 23525.82
[INFO 2017-06-25 18:52:39,948 main.py:50] epoch 3567, training loss: 16253.51, average training loss: 17933.44, base loss: 23517.59
[INFO 2017-06-25 18:52:44,199 main.py:50] epoch 3568, training loss: 21089.62, average training loss: 17936.00, base loss: 23521.71
[INFO 2017-06-25 18:52:48,471 main.py:50] epoch 3569, training loss: 18087.17, average training loss: 17933.48, base loss: 23515.87
[INFO 2017-06-25 18:52:52,742 main.py:50] epoch 3570, training loss: 15709.93, average training loss: 17929.55, base loss: 23509.12
[INFO 2017-06-25 18:52:57,005 main.py:50] epoch 3571, training loss: 16774.98, average training loss: 17924.40, base loss: 23499.83
[INFO 2017-06-25 18:53:01,287 main.py:50] epoch 3572, training loss: 17691.62, average training loss: 17921.25, base loss: 23497.12
[INFO 2017-06-25 18:53:05,565 main.py:50] epoch 3573, training loss: 17626.11, average training loss: 17920.57, base loss: 23498.26
[INFO 2017-06-25 18:53:09,881 main.py:50] epoch 3574, training loss: 18899.40, average training loss: 17923.25, base loss: 23501.84
[INFO 2017-06-25 18:53:14,180 main.py:50] epoch 3575, training loss: 22214.14, average training loss: 17923.96, base loss: 23501.68
[INFO 2017-06-25 18:53:18,477 main.py:50] epoch 3576, training loss: 16548.01, average training loss: 17919.67, base loss: 23498.55
[INFO 2017-06-25 18:53:22,769 main.py:50] epoch 3577, training loss: 16651.55, average training loss: 17919.76, base loss: 23500.68
[INFO 2017-06-25 18:53:27,087 main.py:50] epoch 3578, training loss: 17293.83, average training loss: 17916.26, base loss: 23497.50
[INFO 2017-06-25 18:53:31,371 main.py:50] epoch 3579, training loss: 14922.44, average training loss: 17910.54, base loss: 23492.68
[INFO 2017-06-25 18:53:35,649 main.py:50] epoch 3580, training loss: 20006.24, average training loss: 17916.80, base loss: 23499.82
[INFO 2017-06-25 18:53:39,919 main.py:50] epoch 3581, training loss: 21049.91, average training loss: 17919.52, base loss: 23503.86
[INFO 2017-06-25 18:53:44,172 main.py:50] epoch 3582, training loss: 13570.81, average training loss: 17913.10, base loss: 23497.93
[INFO 2017-06-25 18:53:48,452 main.py:50] epoch 3583, training loss: 21798.33, average training loss: 17915.02, base loss: 23502.07
[INFO 2017-06-25 18:53:52,722 main.py:50] epoch 3584, training loss: 17324.13, average training loss: 17917.17, base loss: 23505.41
[INFO 2017-06-25 18:53:57,026 main.py:50] epoch 3585, training loss: 15283.91, average training loss: 17913.20, base loss: 23500.18
[INFO 2017-06-25 18:54:01,277 main.py:50] epoch 3586, training loss: 15656.34, average training loss: 17908.89, base loss: 23495.81
[INFO 2017-06-25 18:54:05,540 main.py:50] epoch 3587, training loss: 18080.23, average training loss: 17906.18, base loss: 23494.37
[INFO 2017-06-25 18:54:09,839 main.py:50] epoch 3588, training loss: 17102.32, average training loss: 17902.25, base loss: 23489.58
[INFO 2017-06-25 18:54:14,099 main.py:50] epoch 3589, training loss: 16540.81, average training loss: 17901.94, base loss: 23489.88
[INFO 2017-06-25 18:54:18,425 main.py:50] epoch 3590, training loss: 20409.48, average training loss: 17904.92, base loss: 23498.78
[INFO 2017-06-25 18:54:22,695 main.py:50] epoch 3591, training loss: 19267.06, average training loss: 17905.42, base loss: 23496.17
[INFO 2017-06-25 18:54:26,988 main.py:50] epoch 3592, training loss: 16588.59, average training loss: 17905.44, base loss: 23495.55
[INFO 2017-06-25 18:54:31,312 main.py:50] epoch 3593, training loss: 16741.99, average training loss: 17904.44, base loss: 23492.81
[INFO 2017-06-25 18:54:35,603 main.py:50] epoch 3594, training loss: 17391.57, average training loss: 17897.02, base loss: 23483.24
[INFO 2017-06-25 18:54:39,882 main.py:50] epoch 3595, training loss: 14201.75, average training loss: 17892.20, base loss: 23480.19
[INFO 2017-06-25 18:54:44,149 main.py:50] epoch 3596, training loss: 16403.98, average training loss: 17887.06, base loss: 23475.05
[INFO 2017-06-25 18:54:48,440 main.py:50] epoch 3597, training loss: 16107.13, average training loss: 17882.35, base loss: 23471.28
[INFO 2017-06-25 18:54:52,741 main.py:50] epoch 3598, training loss: 15652.71, average training loss: 17878.93, base loss: 23466.15
[INFO 2017-06-25 18:54:57,025 main.py:50] epoch 3599, training loss: 18036.97, average training loss: 17881.29, base loss: 23473.18
[INFO 2017-06-25 18:55:01,320 main.py:50] epoch 3600, training loss: 15370.53, average training loss: 17882.56, base loss: 23477.74
[INFO 2017-06-25 18:55:05,614 main.py:50] epoch 3601, training loss: 17485.64, average training loss: 17882.48, base loss: 23477.68
[INFO 2017-06-25 18:55:09,908 main.py:50] epoch 3602, training loss: 16829.01, average training loss: 17885.73, base loss: 23482.95
[INFO 2017-06-25 18:55:14,199 main.py:50] epoch 3603, training loss: 19587.64, average training loss: 17886.75, base loss: 23485.27
[INFO 2017-06-25 18:55:18,506 main.py:50] epoch 3604, training loss: 15115.56, average training loss: 17882.95, base loss: 23479.39
[INFO 2017-06-25 18:55:22,793 main.py:50] epoch 3605, training loss: 19134.24, average training loss: 17882.90, base loss: 23477.37
[INFO 2017-06-25 18:55:27,096 main.py:50] epoch 3606, training loss: 17901.22, average training loss: 17882.19, base loss: 23474.97
[INFO 2017-06-25 18:55:31,395 main.py:50] epoch 3607, training loss: 15922.33, average training loss: 17884.60, base loss: 23481.25
[INFO 2017-06-25 18:55:35,720 main.py:50] epoch 3608, training loss: 18491.50, average training loss: 17887.38, base loss: 23484.60
[INFO 2017-06-25 18:55:39,980 main.py:50] epoch 3609, training loss: 17062.15, average training loss: 17887.03, base loss: 23484.11
[INFO 2017-06-25 18:55:44,268 main.py:50] epoch 3610, training loss: 14702.78, average training loss: 17885.39, base loss: 23482.51
[INFO 2017-06-25 18:55:48,564 main.py:50] epoch 3611, training loss: 14511.00, average training loss: 17882.51, base loss: 23478.40
[INFO 2017-06-25 18:55:52,856 main.py:50] epoch 3612, training loss: 18263.87, average training loss: 17885.26, base loss: 23481.48
[INFO 2017-06-25 18:55:57,164 main.py:50] epoch 3613, training loss: 21928.35, average training loss: 17891.90, base loss: 23490.34
[INFO 2017-06-25 18:56:01,429 main.py:50] epoch 3614, training loss: 20242.43, average training loss: 17894.26, base loss: 23491.20
[INFO 2017-06-25 18:56:05,722 main.py:50] epoch 3615, training loss: 18241.75, average training loss: 17890.24, base loss: 23486.64
[INFO 2017-06-25 18:56:09,983 main.py:50] epoch 3616, training loss: 16867.43, average training loss: 17887.75, base loss: 23480.05
[INFO 2017-06-25 18:56:14,301 main.py:50] epoch 3617, training loss: 16192.00, average training loss: 17885.28, base loss: 23478.32
[INFO 2017-06-25 18:56:18,569 main.py:50] epoch 3618, training loss: 13413.77, average training loss: 17879.89, base loss: 23469.10
[INFO 2017-06-25 18:56:22,877 main.py:50] epoch 3619, training loss: 17452.94, average training loss: 17880.68, base loss: 23471.31
[INFO 2017-06-25 18:56:27,179 main.py:50] epoch 3620, training loss: 16999.74, average training loss: 17869.68, base loss: 23459.49
[INFO 2017-06-25 18:56:31,472 main.py:50] epoch 3621, training loss: 19493.55, average training loss: 17871.35, base loss: 23457.86
[INFO 2017-06-25 18:56:35,761 main.py:50] epoch 3622, training loss: 20345.37, average training loss: 17873.66, base loss: 23463.90
[INFO 2017-06-25 18:56:40,051 main.py:50] epoch 3623, training loss: 18419.39, average training loss: 17872.60, base loss: 23462.79
[INFO 2017-06-25 18:56:44,356 main.py:50] epoch 3624, training loss: 17921.12, average training loss: 17871.03, base loss: 23459.05
[INFO 2017-06-25 18:56:48,647 main.py:50] epoch 3625, training loss: 15963.23, average training loss: 17869.42, base loss: 23457.74
[INFO 2017-06-25 18:56:52,910 main.py:50] epoch 3626, training loss: 19729.09, average training loss: 17873.37, base loss: 23464.41
[INFO 2017-06-25 18:56:57,196 main.py:50] epoch 3627, training loss: 13341.59, average training loss: 17867.20, base loss: 23457.15
[INFO 2017-06-25 18:57:01,510 main.py:50] epoch 3628, training loss: 18319.04, average training loss: 17868.76, base loss: 23459.28
[INFO 2017-06-25 18:57:05,767 main.py:50] epoch 3629, training loss: 16357.95, average training loss: 17865.31, base loss: 23452.67
[INFO 2017-06-25 18:57:10,069 main.py:50] epoch 3630, training loss: 15400.03, average training loss: 17863.16, base loss: 23447.03
[INFO 2017-06-25 18:57:14,351 main.py:50] epoch 3631, training loss: 18397.88, average training loss: 17867.63, base loss: 23453.43
[INFO 2017-06-25 18:57:18,620 main.py:50] epoch 3632, training loss: 15149.27, average training loss: 17865.00, base loss: 23448.70
[INFO 2017-06-25 18:57:22,902 main.py:50] epoch 3633, training loss: 12476.72, average training loss: 17854.68, base loss: 23438.50
[INFO 2017-06-25 18:57:27,171 main.py:50] epoch 3634, training loss: 24510.54, average training loss: 17859.73, base loss: 23445.97
[INFO 2017-06-25 18:57:31,454 main.py:50] epoch 3635, training loss: 15752.25, average training loss: 17859.56, base loss: 23445.25
[INFO 2017-06-25 18:57:35,738 main.py:50] epoch 3636, training loss: 21352.77, average training loss: 17866.80, base loss: 23453.77
[INFO 2017-06-25 18:57:40,007 main.py:50] epoch 3637, training loss: 19011.54, average training loss: 17868.51, base loss: 23454.45
[INFO 2017-06-25 18:57:44,262 main.py:50] epoch 3638, training loss: 23924.23, average training loss: 17875.42, base loss: 23464.07
[INFO 2017-06-25 18:57:48,567 main.py:50] epoch 3639, training loss: 17128.72, average training loss: 17872.61, base loss: 23460.73
[INFO 2017-06-25 18:57:52,865 main.py:50] epoch 3640, training loss: 18301.62, average training loss: 17873.08, base loss: 23461.47
[INFO 2017-06-25 18:57:57,135 main.py:50] epoch 3641, training loss: 18603.15, average training loss: 17864.13, base loss: 23449.68
[INFO 2017-06-25 18:58:01,415 main.py:50] epoch 3642, training loss: 19382.14, average training loss: 17864.21, base loss: 23450.68
[INFO 2017-06-25 18:58:05,736 main.py:50] epoch 3643, training loss: 19105.17, average training loss: 17867.98, base loss: 23454.24
[INFO 2017-06-25 18:58:10,016 main.py:50] epoch 3644, training loss: 18591.34, average training loss: 17867.87, base loss: 23454.11
[INFO 2017-06-25 18:58:14,293 main.py:50] epoch 3645, training loss: 16137.53, average training loss: 17870.37, base loss: 23456.81
[INFO 2017-06-25 18:58:18,567 main.py:50] epoch 3646, training loss: 16040.67, average training loss: 17865.31, base loss: 23449.22
[INFO 2017-06-25 18:58:22,875 main.py:50] epoch 3647, training loss: 15236.71, average training loss: 17866.28, base loss: 23449.20
[INFO 2017-06-25 18:58:27,180 main.py:50] epoch 3648, training loss: 15293.73, average training loss: 17862.24, base loss: 23445.01
[INFO 2017-06-25 18:58:31,451 main.py:50] epoch 3649, training loss: 20588.83, average training loss: 17865.52, base loss: 23446.27
[INFO 2017-06-25 18:58:35,735 main.py:50] epoch 3650, training loss: 21838.61, average training loss: 17869.19, base loss: 23448.87
[INFO 2017-06-25 18:58:40,012 main.py:50] epoch 3651, training loss: 17489.32, average training loss: 17869.37, base loss: 23447.09
[INFO 2017-06-25 18:58:44,272 main.py:50] epoch 3652, training loss: 17361.04, average training loss: 17867.92, base loss: 23444.51
[INFO 2017-06-25 18:58:48,549 main.py:50] epoch 3653, training loss: 21199.46, average training loss: 17873.98, base loss: 23451.60
[INFO 2017-06-25 18:58:52,817 main.py:50] epoch 3654, training loss: 16632.32, average training loss: 17873.23, base loss: 23452.79
[INFO 2017-06-25 18:58:57,136 main.py:50] epoch 3655, training loss: 17089.39, average training loss: 17875.40, base loss: 23454.56
[INFO 2017-06-25 18:59:01,422 main.py:50] epoch 3656, training loss: 21684.48, average training loss: 17881.86, base loss: 23462.48
[INFO 2017-06-25 18:59:05,712 main.py:50] epoch 3657, training loss: 19973.49, average training loss: 17883.10, base loss: 23461.88
[INFO 2017-06-25 18:59:09,993 main.py:50] epoch 3658, training loss: 14785.40, average training loss: 17880.40, base loss: 23456.84
[INFO 2017-06-25 18:59:14,274 main.py:50] epoch 3659, training loss: 15365.98, average training loss: 17878.31, base loss: 23455.27
[INFO 2017-06-25 18:59:18,555 main.py:50] epoch 3660, training loss: 14995.77, average training loss: 17875.56, base loss: 23452.69
[INFO 2017-06-25 18:59:22,822 main.py:50] epoch 3661, training loss: 27057.56, average training loss: 17885.41, base loss: 23463.53
[INFO 2017-06-25 18:59:27,086 main.py:50] epoch 3662, training loss: 15758.93, average training loss: 17880.72, base loss: 23456.83
[INFO 2017-06-25 18:59:31,359 main.py:50] epoch 3663, training loss: 15463.44, average training loss: 17880.98, base loss: 23456.86
[INFO 2017-06-25 18:59:35,635 main.py:50] epoch 3664, training loss: 17309.21, average training loss: 17878.65, base loss: 23450.89
[INFO 2017-06-25 18:59:39,925 main.py:50] epoch 3665, training loss: 22437.90, average training loss: 17882.28, base loss: 23457.43
[INFO 2017-06-25 18:59:44,219 main.py:50] epoch 3666, training loss: 15978.19, average training loss: 17878.18, base loss: 23453.12
[INFO 2017-06-25 18:59:48,501 main.py:50] epoch 3667, training loss: 18181.87, average training loss: 17879.88, base loss: 23452.73
[INFO 2017-06-25 18:59:52,823 main.py:50] epoch 3668, training loss: 15431.55, average training loss: 17878.91, base loss: 23453.91
[INFO 2017-06-25 18:59:57,088 main.py:50] epoch 3669, training loss: 18251.64, average training loss: 17873.26, base loss: 23448.04
[INFO 2017-06-25 19:00:01,391 main.py:50] epoch 3670, training loss: 20753.56, average training loss: 17874.51, base loss: 23447.06
[INFO 2017-06-25 19:00:05,703 main.py:50] epoch 3671, training loss: 17689.35, average training loss: 17871.41, base loss: 23445.27
[INFO 2017-06-25 19:00:09,997 main.py:50] epoch 3672, training loss: 17531.07, average training loss: 17869.87, base loss: 23443.56
[INFO 2017-06-25 19:00:14,299 main.py:50] epoch 3673, training loss: 18082.21, average training loss: 17871.41, base loss: 23446.80
[INFO 2017-06-25 19:00:18,575 main.py:50] epoch 3674, training loss: 17276.80, average training loss: 17870.72, base loss: 23447.76
[INFO 2017-06-25 19:00:22,858 main.py:50] epoch 3675, training loss: 18832.56, average training loss: 17874.49, base loss: 23451.82
[INFO 2017-06-25 19:00:27,145 main.py:50] epoch 3676, training loss: 18678.76, average training loss: 17875.90, base loss: 23452.77
[INFO 2017-06-25 19:00:31,409 main.py:50] epoch 3677, training loss: 20527.42, average training loss: 17881.20, base loss: 23459.11
[INFO 2017-06-25 19:00:35,686 main.py:50] epoch 3678, training loss: 17958.57, average training loss: 17882.73, base loss: 23460.45
[INFO 2017-06-25 19:00:39,972 main.py:50] epoch 3679, training loss: 20017.25, average training loss: 17885.71, base loss: 23462.54
[INFO 2017-06-25 19:00:44,249 main.py:50] epoch 3680, training loss: 18703.12, average training loss: 17883.95, base loss: 23459.59
[INFO 2017-06-25 19:00:48,516 main.py:50] epoch 3681, training loss: 21923.26, average training loss: 17884.51, base loss: 23462.41
[INFO 2017-06-25 19:00:52,806 main.py:50] epoch 3682, training loss: 21226.27, average training loss: 17884.96, base loss: 23463.24
[INFO 2017-06-25 19:00:57,094 main.py:50] epoch 3683, training loss: 20441.29, average training loss: 17887.37, base loss: 23467.03
[INFO 2017-06-25 19:01:01,382 main.py:50] epoch 3684, training loss: 17748.13, average training loss: 17890.01, base loss: 23470.99
[INFO 2017-06-25 19:01:05,682 main.py:50] epoch 3685, training loss: 17298.03, average training loss: 17888.11, base loss: 23469.95
[INFO 2017-06-25 19:01:09,970 main.py:50] epoch 3686, training loss: 17982.72, average training loss: 17891.79, base loss: 23472.79
[INFO 2017-06-25 19:01:14,284 main.py:50] epoch 3687, training loss: 17068.67, average training loss: 17885.70, base loss: 23465.74
[INFO 2017-06-25 19:01:18,564 main.py:50] epoch 3688, training loss: 16829.03, average training loss: 17877.57, base loss: 23459.93
[INFO 2017-06-25 19:01:22,877 main.py:50] epoch 3689, training loss: 14893.33, average training loss: 17875.85, base loss: 23458.59
[INFO 2017-06-25 19:01:27,164 main.py:50] epoch 3690, training loss: 14144.12, average training loss: 17872.13, base loss: 23452.62
[INFO 2017-06-25 19:01:31,439 main.py:50] epoch 3691, training loss: 19183.49, average training loss: 17869.54, base loss: 23449.97
[INFO 2017-06-25 19:01:35,717 main.py:50] epoch 3692, training loss: 18790.35, average training loss: 17869.90, base loss: 23449.75
[INFO 2017-06-25 19:01:39,978 main.py:50] epoch 3693, training loss: 18478.31, average training loss: 17868.66, base loss: 23446.53
[INFO 2017-06-25 19:01:44,248 main.py:50] epoch 3694, training loss: 18930.27, average training loss: 17867.08, base loss: 23443.20
[INFO 2017-06-25 19:01:48,533 main.py:50] epoch 3695, training loss: 17908.79, average training loss: 17870.34, base loss: 23448.77
[INFO 2017-06-25 19:01:52,847 main.py:50] epoch 3696, training loss: 14699.75, average training loss: 17867.76, base loss: 23447.62
[INFO 2017-06-25 19:01:57,123 main.py:50] epoch 3697, training loss: 17059.58, average training loss: 17868.34, base loss: 23446.42
[INFO 2017-06-25 19:02:01,424 main.py:50] epoch 3698, training loss: 18701.67, average training loss: 17867.64, base loss: 23444.98
[INFO 2017-06-25 19:02:05,696 main.py:50] epoch 3699, training loss: 15125.98, average training loss: 17860.26, base loss: 23434.39
[INFO 2017-06-25 19:02:09,972 main.py:50] epoch 3700, training loss: 15847.29, average training loss: 17860.29, base loss: 23433.80
[INFO 2017-06-25 19:02:14,271 main.py:50] epoch 3701, training loss: 16414.01, average training loss: 17855.07, base loss: 23427.98
[INFO 2017-06-25 19:02:18,548 main.py:50] epoch 3702, training loss: 20885.23, average training loss: 17859.67, base loss: 23437.10
[INFO 2017-06-25 19:02:22,846 main.py:50] epoch 3703, training loss: 21580.03, average training loss: 17862.17, base loss: 23437.59
[INFO 2017-06-25 19:02:27,140 main.py:50] epoch 3704, training loss: 20499.16, average training loss: 17860.09, base loss: 23432.75
[INFO 2017-06-25 19:02:31,419 main.py:50] epoch 3705, training loss: 16236.30, average training loss: 17858.18, base loss: 23430.21
[INFO 2017-06-25 19:02:35,708 main.py:50] epoch 3706, training loss: 17191.50, average training loss: 17859.03, base loss: 23434.16
[INFO 2017-06-25 19:02:40,005 main.py:50] epoch 3707, training loss: 19749.29, average training loss: 17859.56, base loss: 23430.01
[INFO 2017-06-25 19:02:44,284 main.py:50] epoch 3708, training loss: 18523.05, average training loss: 17860.20, base loss: 23428.11
[INFO 2017-06-25 19:02:48,563 main.py:50] epoch 3709, training loss: 21206.21, average training loss: 17867.29, base loss: 23435.70
[INFO 2017-06-25 19:02:52,851 main.py:50] epoch 3710, training loss: 18885.91, average training loss: 17868.21, base loss: 23441.18
[INFO 2017-06-25 19:02:57,122 main.py:50] epoch 3711, training loss: 15382.71, average training loss: 17864.40, base loss: 23436.62
[INFO 2017-06-25 19:03:01,416 main.py:50] epoch 3712, training loss: 16583.45, average training loss: 17867.76, base loss: 23445.21
[INFO 2017-06-25 19:03:05,709 main.py:50] epoch 3713, training loss: 16972.16, average training loss: 17867.79, base loss: 23444.82
[INFO 2017-06-25 19:03:10,004 main.py:50] epoch 3714, training loss: 17382.97, average training loss: 17866.77, base loss: 23445.03
[INFO 2017-06-25 19:03:14,304 main.py:50] epoch 3715, training loss: 17736.16, average training loss: 17868.43, base loss: 23449.42
[INFO 2017-06-25 19:03:18,558 main.py:50] epoch 3716, training loss: 22190.34, average training loss: 17872.90, base loss: 23456.52
[INFO 2017-06-25 19:03:22,842 main.py:50] epoch 3717, training loss: 14642.11, average training loss: 17872.09, base loss: 23454.10
[INFO 2017-06-25 19:03:27,129 main.py:50] epoch 3718, training loss: 15839.56, average training loss: 17867.71, base loss: 23450.70
[INFO 2017-06-25 19:03:31,395 main.py:50] epoch 3719, training loss: 13806.93, average training loss: 17859.19, base loss: 23442.79
[INFO 2017-06-25 19:03:35,712 main.py:50] epoch 3720, training loss: 23192.05, average training loss: 17866.04, base loss: 23449.43
[INFO 2017-06-25 19:03:40,026 main.py:50] epoch 3721, training loss: 19648.64, average training loss: 17866.06, base loss: 23449.13
[INFO 2017-06-25 19:03:44,326 main.py:50] epoch 3722, training loss: 15999.32, average training loss: 17862.31, base loss: 23445.68
[INFO 2017-06-25 19:03:48,619 main.py:50] epoch 3723, training loss: 21356.33, average training loss: 17867.57, base loss: 23454.35
[INFO 2017-06-25 19:03:52,929 main.py:50] epoch 3724, training loss: 19804.75, average training loss: 17865.93, base loss: 23450.75
[INFO 2017-06-25 19:03:57,242 main.py:50] epoch 3725, training loss: 19235.24, average training loss: 17865.51, base loss: 23447.53
[INFO 2017-06-25 19:04:01,535 main.py:50] epoch 3726, training loss: 21810.48, average training loss: 17868.22, base loss: 23450.38
[INFO 2017-06-25 19:04:05,823 main.py:50] epoch 3727, training loss: 17298.19, average training loss: 17861.56, base loss: 23443.44
[INFO 2017-06-25 19:04:10,094 main.py:50] epoch 3728, training loss: 17779.27, average training loss: 17861.28, base loss: 23442.13
[INFO 2017-06-25 19:04:14,412 main.py:50] epoch 3729, training loss: 14149.67, average training loss: 17857.42, base loss: 23435.36
[INFO 2017-06-25 19:04:18,732 main.py:50] epoch 3730, training loss: 17405.26, average training loss: 17856.10, base loss: 23436.86
[INFO 2017-06-25 19:04:23,001 main.py:50] epoch 3731, training loss: 17398.04, average training loss: 17850.97, base loss: 23429.39
[INFO 2017-06-25 19:04:27,300 main.py:50] epoch 3732, training loss: 18303.22, average training loss: 17848.15, base loss: 23426.59
[INFO 2017-06-25 19:04:31,598 main.py:50] epoch 3733, training loss: 18321.68, average training loss: 17845.43, base loss: 23422.51
[INFO 2017-06-25 19:04:35,914 main.py:50] epoch 3734, training loss: 18677.38, average training loss: 17848.37, base loss: 23428.75
[INFO 2017-06-25 19:04:40,190 main.py:50] epoch 3735, training loss: 19988.75, average training loss: 17852.34, base loss: 23438.55
[INFO 2017-06-25 19:04:44,461 main.py:50] epoch 3736, training loss: 19096.05, average training loss: 17852.24, base loss: 23438.58
[INFO 2017-06-25 19:04:48,746 main.py:50] epoch 3737, training loss: 15710.01, average training loss: 17849.24, base loss: 23434.48
[INFO 2017-06-25 19:04:53,044 main.py:50] epoch 3738, training loss: 19299.18, average training loss: 17849.75, base loss: 23439.51
[INFO 2017-06-25 19:04:57,352 main.py:50] epoch 3739, training loss: 18544.73, average training loss: 17851.26, base loss: 23440.80
[INFO 2017-06-25 19:05:01,659 main.py:50] epoch 3740, training loss: 17065.57, average training loss: 17851.08, base loss: 23440.35
[INFO 2017-06-25 19:05:05,923 main.py:50] epoch 3741, training loss: 16718.69, average training loss: 17846.94, base loss: 23438.32
[INFO 2017-06-25 19:05:10,239 main.py:50] epoch 3742, training loss: 15809.30, average training loss: 17845.90, base loss: 23434.42
[INFO 2017-06-25 19:05:14,521 main.py:50] epoch 3743, training loss: 17020.54, average training loss: 17847.51, base loss: 23440.86
[INFO 2017-06-25 19:05:18,847 main.py:50] epoch 3744, training loss: 16547.39, average training loss: 17849.30, base loss: 23443.30
[INFO 2017-06-25 19:05:23,140 main.py:50] epoch 3745, training loss: 15859.83, average training loss: 17846.79, base loss: 23442.00
[INFO 2017-06-25 19:05:27,437 main.py:50] epoch 3746, training loss: 17201.38, average training loss: 17847.35, base loss: 23444.12
[INFO 2017-06-25 19:05:31,731 main.py:50] epoch 3747, training loss: 14931.31, average training loss: 17843.87, base loss: 23442.25
[INFO 2017-06-25 19:05:36,020 main.py:50] epoch 3748, training loss: 16182.79, average training loss: 17838.12, base loss: 23434.53
[INFO 2017-06-25 19:05:40,328 main.py:50] epoch 3749, training loss: 17623.43, average training loss: 17835.25, base loss: 23431.79
[INFO 2017-06-25 19:05:44,592 main.py:50] epoch 3750, training loss: 16549.51, average training loss: 17828.21, base loss: 23421.45
[INFO 2017-06-25 19:05:48,881 main.py:50] epoch 3751, training loss: 21777.83, average training loss: 17832.43, base loss: 23425.17
[INFO 2017-06-25 19:05:53,148 main.py:50] epoch 3752, training loss: 17230.04, average training loss: 17833.58, base loss: 23426.95
[INFO 2017-06-25 19:05:57,429 main.py:50] epoch 3753, training loss: 17083.49, average training loss: 17835.31, base loss: 23428.95
[INFO 2017-06-25 19:06:01,714 main.py:50] epoch 3754, training loss: 16756.79, average training loss: 17836.75, base loss: 23434.44
[INFO 2017-06-25 19:06:05,998 main.py:50] epoch 3755, training loss: 15325.92, average training loss: 17836.51, base loss: 23435.71
[INFO 2017-06-25 19:06:10,311 main.py:50] epoch 3756, training loss: 22176.69, average training loss: 17838.76, base loss: 23439.87
[INFO 2017-06-25 19:06:14,610 main.py:50] epoch 3757, training loss: 18659.47, average training loss: 17837.53, base loss: 23436.29
[INFO 2017-06-25 19:06:18,890 main.py:50] epoch 3758, training loss: 19912.22, average training loss: 17841.03, base loss: 23444.06
[INFO 2017-06-25 19:06:23,157 main.py:50] epoch 3759, training loss: 15325.66, average training loss: 17840.72, base loss: 23441.70
[INFO 2017-06-25 19:06:27,446 main.py:50] epoch 3760, training loss: 20641.62, average training loss: 17840.60, base loss: 23443.86
[INFO 2017-06-25 19:06:31,749 main.py:50] epoch 3761, training loss: 19256.90, average training loss: 17842.77, base loss: 23444.08
[INFO 2017-06-25 19:06:36,033 main.py:50] epoch 3762, training loss: 16102.00, average training loss: 17841.95, base loss: 23445.58
[INFO 2017-06-25 19:06:40,307 main.py:50] epoch 3763, training loss: 17621.11, average training loss: 17842.82, base loss: 23447.38
[INFO 2017-06-25 19:06:44,643 main.py:50] epoch 3764, training loss: 18917.52, average training loss: 17838.39, base loss: 23444.29
[INFO 2017-06-25 19:06:48,966 main.py:50] epoch 3765, training loss: 17513.35, average training loss: 17838.93, base loss: 23446.09
[INFO 2017-06-25 19:06:53,280 main.py:50] epoch 3766, training loss: 15685.21, average training loss: 17835.25, base loss: 23442.25
[INFO 2017-06-25 19:06:57,544 main.py:50] epoch 3767, training loss: 18088.83, average training loss: 17834.66, base loss: 23443.32
[INFO 2017-06-25 19:07:01,850 main.py:50] epoch 3768, training loss: 16751.18, average training loss: 17832.13, base loss: 23438.45
[INFO 2017-06-25 19:07:06,126 main.py:50] epoch 3769, training loss: 18161.42, average training loss: 17833.12, base loss: 23437.90
[INFO 2017-06-25 19:07:10,411 main.py:50] epoch 3770, training loss: 21208.06, average training loss: 17840.64, base loss: 23446.87
[INFO 2017-06-25 19:07:14,713 main.py:50] epoch 3771, training loss: 21928.26, average training loss: 17847.84, base loss: 23455.07
[INFO 2017-06-25 19:07:19,014 main.py:50] epoch 3772, training loss: 18808.66, average training loss: 17851.16, base loss: 23455.89
[INFO 2017-06-25 19:07:23,308 main.py:50] epoch 3773, training loss: 19222.84, average training loss: 17853.34, base loss: 23458.08
[INFO 2017-06-25 19:07:27,587 main.py:50] epoch 3774, training loss: 12820.14, average training loss: 17850.54, base loss: 23453.31
[INFO 2017-06-25 19:07:31,874 main.py:50] epoch 3775, training loss: 17350.00, average training loss: 17852.06, base loss: 23456.79
[INFO 2017-06-25 19:07:36,170 main.py:50] epoch 3776, training loss: 18344.32, average training loss: 17848.73, base loss: 23453.63
[INFO 2017-06-25 19:07:40,471 main.py:50] epoch 3777, training loss: 19762.05, average training loss: 17850.27, base loss: 23456.84
[INFO 2017-06-25 19:07:44,729 main.py:50] epoch 3778, training loss: 17107.46, average training loss: 17852.17, base loss: 23458.24
[INFO 2017-06-25 19:07:48,991 main.py:50] epoch 3779, training loss: 14988.97, average training loss: 17851.51, base loss: 23458.43
[INFO 2017-06-25 19:07:53,278 main.py:50] epoch 3780, training loss: 18801.92, average training loss: 17852.27, base loss: 23458.96
[INFO 2017-06-25 19:07:57,576 main.py:50] epoch 3781, training loss: 21660.55, average training loss: 17860.42, base loss: 23470.81
[INFO 2017-06-25 19:08:01,862 main.py:50] epoch 3782, training loss: 18337.54, average training loss: 17864.49, base loss: 23476.44
[INFO 2017-06-25 19:08:06,130 main.py:50] epoch 3783, training loss: 15796.73, average training loss: 17862.13, base loss: 23476.85
[INFO 2017-06-25 19:08:10,419 main.py:50] epoch 3784, training loss: 16743.16, average training loss: 17858.62, base loss: 23473.45
[INFO 2017-06-25 19:08:14,694 main.py:50] epoch 3785, training loss: 14664.60, average training loss: 17854.35, base loss: 23469.76
[INFO 2017-06-25 19:08:18,972 main.py:50] epoch 3786, training loss: 16622.90, average training loss: 17852.38, base loss: 23468.49
[INFO 2017-06-25 19:08:23,253 main.py:50] epoch 3787, training loss: 17134.35, average training loss: 17853.19, base loss: 23468.44
[INFO 2017-06-25 19:08:27,544 main.py:50] epoch 3788, training loss: 23043.98, average training loss: 17858.12, base loss: 23476.55
[INFO 2017-06-25 19:08:31,819 main.py:50] epoch 3789, training loss: 17315.36, average training loss: 17858.51, base loss: 23480.00
[INFO 2017-06-25 19:08:36,110 main.py:50] epoch 3790, training loss: 20897.85, average training loss: 17859.90, base loss: 23483.15
[INFO 2017-06-25 19:08:40,377 main.py:50] epoch 3791, training loss: 18582.15, average training loss: 17861.56, base loss: 23486.42
[INFO 2017-06-25 19:08:44,651 main.py:50] epoch 3792, training loss: 16109.38, average training loss: 17858.22, base loss: 23480.77
[INFO 2017-06-25 19:08:48,950 main.py:50] epoch 3793, training loss: 17268.65, average training loss: 17858.73, base loss: 23482.72
[INFO 2017-06-25 19:08:53,249 main.py:50] epoch 3794, training loss: 17480.81, average training loss: 17857.60, base loss: 23484.30
[INFO 2017-06-25 19:08:57,513 main.py:50] epoch 3795, training loss: 17859.08, average training loss: 17855.78, base loss: 23481.76
[INFO 2017-06-25 19:09:01,789 main.py:50] epoch 3796, training loss: 19377.62, average training loss: 17860.97, base loss: 23488.01
[INFO 2017-06-25 19:09:06,100 main.py:50] epoch 3797, training loss: 15655.62, average training loss: 17859.23, base loss: 23487.26
[INFO 2017-06-25 19:09:10,371 main.py:50] epoch 3798, training loss: 20916.97, average training loss: 17863.42, base loss: 23495.47
[INFO 2017-06-25 19:09:14,658 main.py:50] epoch 3799, training loss: 16511.67, average training loss: 17861.90, base loss: 23495.98
[INFO 2017-06-25 19:09:18,940 main.py:50] epoch 3800, training loss: 17079.85, average training loss: 17858.15, base loss: 23492.60
[INFO 2017-06-25 19:09:23,249 main.py:50] epoch 3801, training loss: 16485.70, average training loss: 17856.66, base loss: 23489.00
[INFO 2017-06-25 19:09:27,550 main.py:50] epoch 3802, training loss: 15610.73, average training loss: 17857.65, base loss: 23491.20
[INFO 2017-06-25 19:09:31,819 main.py:50] epoch 3803, training loss: 13798.83, average training loss: 17853.45, base loss: 23489.85
[INFO 2017-06-25 19:09:36,111 main.py:50] epoch 3804, training loss: 11506.84, average training loss: 17851.39, base loss: 23486.91
[INFO 2017-06-25 19:09:40,384 main.py:50] epoch 3805, training loss: 15835.16, average training loss: 17849.18, base loss: 23486.49
[INFO 2017-06-25 19:09:44,646 main.py:50] epoch 3806, training loss: 15330.30, average training loss: 17845.81, base loss: 23483.25
[INFO 2017-06-25 19:09:48,907 main.py:50] epoch 3807, training loss: 19217.16, average training loss: 17849.80, base loss: 23485.04
[INFO 2017-06-25 19:09:53,227 main.py:50] epoch 3808, training loss: 16540.65, average training loss: 17846.98, base loss: 23480.87
[INFO 2017-06-25 19:09:57,486 main.py:50] epoch 3809, training loss: 13971.53, average training loss: 17839.08, base loss: 23471.08
[INFO 2017-06-25 19:10:01,780 main.py:50] epoch 3810, training loss: 15429.01, average training loss: 17835.93, base loss: 23466.73
[INFO 2017-06-25 19:10:06,083 main.py:50] epoch 3811, training loss: 22336.77, average training loss: 17835.70, base loss: 23466.71
[INFO 2017-06-25 19:10:10,337 main.py:50] epoch 3812, training loss: 14220.10, average training loss: 17832.09, base loss: 23463.03
[INFO 2017-06-25 19:10:14,609 main.py:50] epoch 3813, training loss: 18024.35, average training loss: 17834.06, base loss: 23464.11
[INFO 2017-06-25 19:10:18,871 main.py:50] epoch 3814, training loss: 14790.25, average training loss: 17827.08, base loss: 23455.87
[INFO 2017-06-25 19:10:23,136 main.py:50] epoch 3815, training loss: 27398.35, average training loss: 17836.76, base loss: 23468.27
[INFO 2017-06-25 19:10:27,410 main.py:50] epoch 3816, training loss: 17027.41, average training loss: 17836.91, base loss: 23469.04
[INFO 2017-06-25 19:10:31,679 main.py:50] epoch 3817, training loss: 17882.74, average training loss: 17835.72, base loss: 23468.35
[INFO 2017-06-25 19:10:35,965 main.py:50] epoch 3818, training loss: 17478.69, average training loss: 17838.17, base loss: 23473.97
[INFO 2017-06-25 19:10:40,234 main.py:50] epoch 3819, training loss: 17466.90, average training loss: 17841.27, base loss: 23479.59
[INFO 2017-06-25 19:10:44,518 main.py:50] epoch 3820, training loss: 17117.77, average training loss: 17840.50, base loss: 23478.42
[INFO 2017-06-25 19:10:48,800 main.py:50] epoch 3821, training loss: 18511.38, average training loss: 17844.57, base loss: 23485.38
[INFO 2017-06-25 19:10:53,098 main.py:50] epoch 3822, training loss: 17842.06, average training loss: 17843.30, base loss: 23483.46
[INFO 2017-06-25 19:10:57,364 main.py:50] epoch 3823, training loss: 16558.58, average training loss: 17842.44, base loss: 23482.80
[INFO 2017-06-25 19:11:01,629 main.py:50] epoch 3824, training loss: 14202.49, average training loss: 17838.53, base loss: 23478.71
[INFO 2017-06-25 19:11:05,911 main.py:50] epoch 3825, training loss: 18954.83, average training loss: 17839.15, base loss: 23479.10
[INFO 2017-06-25 19:11:10,200 main.py:50] epoch 3826, training loss: 20813.75, average training loss: 17844.00, base loss: 23485.81
[INFO 2017-06-25 19:11:14,484 main.py:50] epoch 3827, training loss: 19139.73, average training loss: 17851.06, base loss: 23497.57
[INFO 2017-06-25 19:11:18,753 main.py:50] epoch 3828, training loss: 18374.19, average training loss: 17849.46, base loss: 23495.92
[INFO 2017-06-25 19:11:23,035 main.py:50] epoch 3829, training loss: 17008.12, average training loss: 17847.97, base loss: 23494.08
[INFO 2017-06-25 19:11:27,323 main.py:50] epoch 3830, training loss: 21450.35, average training loss: 17846.62, base loss: 23494.48
[INFO 2017-06-25 19:11:31,621 main.py:50] epoch 3831, training loss: 16270.92, average training loss: 17847.12, base loss: 23494.81
[INFO 2017-06-25 19:11:35,914 main.py:50] epoch 3832, training loss: 18980.87, average training loss: 17846.68, base loss: 23493.49
[INFO 2017-06-25 19:11:40,206 main.py:50] epoch 3833, training loss: 20210.62, average training loss: 17847.58, base loss: 23496.95
[INFO 2017-06-25 19:11:44,492 main.py:50] epoch 3834, training loss: 21899.41, average training loss: 17848.56, base loss: 23495.53
[INFO 2017-06-25 19:11:48,774 main.py:50] epoch 3835, training loss: 20175.77, average training loss: 17852.00, base loss: 23501.19
[INFO 2017-06-25 19:11:53,071 main.py:50] epoch 3836, training loss: 17888.44, average training loss: 17851.95, base loss: 23502.41
[INFO 2017-06-25 19:11:57,383 main.py:50] epoch 3837, training loss: 20715.18, average training loss: 17856.78, base loss: 23507.89
[INFO 2017-06-25 19:12:01,662 main.py:50] epoch 3838, training loss: 13853.76, average training loss: 17853.62, base loss: 23504.70
[INFO 2017-06-25 19:12:05,961 main.py:50] epoch 3839, training loss: 17525.75, average training loss: 17851.90, base loss: 23501.86
[INFO 2017-06-25 19:12:10,223 main.py:50] epoch 3840, training loss: 14954.59, average training loss: 17846.34, base loss: 23495.41
[INFO 2017-06-25 19:12:14,486 main.py:50] epoch 3841, training loss: 13621.66, average training loss: 17840.67, base loss: 23485.78
[INFO 2017-06-25 19:12:18,768 main.py:50] epoch 3842, training loss: 19682.86, average training loss: 17846.09, base loss: 23496.22
[INFO 2017-06-25 19:12:23,038 main.py:50] epoch 3843, training loss: 22983.07, average training loss: 17855.17, base loss: 23507.03
[INFO 2017-06-25 19:12:27,309 main.py:50] epoch 3844, training loss: 16265.18, average training loss: 17851.26, base loss: 23502.59
[INFO 2017-06-25 19:12:31,574 main.py:50] epoch 3845, training loss: 19265.04, average training loss: 17855.66, base loss: 23508.18
[INFO 2017-06-25 19:12:35,854 main.py:50] epoch 3846, training loss: 16450.00, average training loss: 17853.57, base loss: 23504.42
[INFO 2017-06-25 19:12:40,135 main.py:50] epoch 3847, training loss: 21099.92, average training loss: 17858.90, base loss: 23511.02
[INFO 2017-06-25 19:12:44,421 main.py:50] epoch 3848, training loss: 17174.34, average training loss: 17854.94, base loss: 23508.17
[INFO 2017-06-25 19:12:48,708 main.py:50] epoch 3849, training loss: 18546.21, average training loss: 17857.80, base loss: 23512.15
[INFO 2017-06-25 19:12:52,992 main.py:50] epoch 3850, training loss: 18253.77, average training loss: 17856.47, base loss: 23511.65
[INFO 2017-06-25 19:12:57,283 main.py:50] epoch 3851, training loss: 15431.69, average training loss: 17858.20, base loss: 23517.83
[INFO 2017-06-25 19:13:01,557 main.py:50] epoch 3852, training loss: 22120.18, average training loss: 17864.59, base loss: 23526.79
[INFO 2017-06-25 19:13:05,867 main.py:50] epoch 3853, training loss: 17755.32, average training loss: 17865.36, base loss: 23530.44
[INFO 2017-06-25 19:13:10,144 main.py:50] epoch 3854, training loss: 15217.22, average training loss: 17865.40, base loss: 23526.75
[INFO 2017-06-25 19:13:14,417 main.py:50] epoch 3855, training loss: 20213.40, average training loss: 17870.05, base loss: 23534.06
[INFO 2017-06-25 19:13:18,701 main.py:50] epoch 3856, training loss: 21674.39, average training loss: 17873.99, base loss: 23536.85
[INFO 2017-06-25 19:13:22,993 main.py:50] epoch 3857, training loss: 15142.63, average training loss: 17876.03, base loss: 23541.27
[INFO 2017-06-25 19:13:27,307 main.py:50] epoch 3858, training loss: 17086.92, average training loss: 17877.38, base loss: 23542.57
[INFO 2017-06-25 19:13:31,579 main.py:50] epoch 3859, training loss: 15848.11, average training loss: 17876.86, base loss: 23541.59
[INFO 2017-06-25 19:13:35,879 main.py:50] epoch 3860, training loss: 14703.42, average training loss: 17875.51, base loss: 23539.69
[INFO 2017-06-25 19:13:40,158 main.py:50] epoch 3861, training loss: 17040.91, average training loss: 17873.16, base loss: 23534.23
[INFO 2017-06-25 19:13:44,432 main.py:50] epoch 3862, training loss: 16488.34, average training loss: 17872.69, base loss: 23533.75
[INFO 2017-06-25 19:13:48,709 main.py:50] epoch 3863, training loss: 17894.56, average training loss: 17867.05, base loss: 23527.53
[INFO 2017-06-25 19:13:52,984 main.py:50] epoch 3864, training loss: 19064.17, average training loss: 17863.74, base loss: 23525.07
[INFO 2017-06-25 19:13:57,272 main.py:50] epoch 3865, training loss: 13602.75, average training loss: 17857.91, base loss: 23517.67
[INFO 2017-06-25 19:14:01,551 main.py:50] epoch 3866, training loss: 20992.68, average training loss: 17858.38, base loss: 23519.97
[INFO 2017-06-25 19:14:05,846 main.py:50] epoch 3867, training loss: 14748.79, average training loss: 17854.83, base loss: 23518.25
[INFO 2017-06-25 19:14:10,125 main.py:50] epoch 3868, training loss: 15473.14, average training loss: 17852.13, base loss: 23511.19
[INFO 2017-06-25 19:14:14,389 main.py:50] epoch 3869, training loss: 19337.40, average training loss: 17853.42, base loss: 23514.34
[INFO 2017-06-25 19:14:18,662 main.py:50] epoch 3870, training loss: 17756.27, average training loss: 17851.97, base loss: 23514.95
[INFO 2017-06-25 19:14:22,956 main.py:50] epoch 3871, training loss: 18928.47, average training loss: 17853.68, base loss: 23515.16
[INFO 2017-06-25 19:14:27,212 main.py:50] epoch 3872, training loss: 14535.86, average training loss: 17851.95, base loss: 23512.09
[INFO 2017-06-25 19:14:31,520 main.py:50] epoch 3873, training loss: 15973.27, average training loss: 17854.02, base loss: 23513.19
[INFO 2017-06-25 19:14:35,807 main.py:50] epoch 3874, training loss: 19681.84, average training loss: 17858.52, base loss: 23520.34
[INFO 2017-06-25 19:14:40,078 main.py:50] epoch 3875, training loss: 18222.96, average training loss: 17860.64, base loss: 23522.70
[INFO 2017-06-25 19:14:44,359 main.py:50] epoch 3876, training loss: 16136.55, average training loss: 17861.25, base loss: 23523.44
[INFO 2017-06-25 19:14:48,645 main.py:50] epoch 3877, training loss: 20221.08, average training loss: 17866.46, base loss: 23531.47
[INFO 2017-06-25 19:14:52,940 main.py:50] epoch 3878, training loss: 15216.80, average training loss: 17865.47, base loss: 23530.98
[INFO 2017-06-25 19:14:57,243 main.py:50] epoch 3879, training loss: 18263.53, average training loss: 17865.20, base loss: 23529.83
[INFO 2017-06-25 19:15:01,550 main.py:50] epoch 3880, training loss: 18475.45, average training loss: 17864.24, base loss: 23528.93
[INFO 2017-06-25 19:15:05,834 main.py:50] epoch 3881, training loss: 20579.80, average training loss: 17867.77, base loss: 23532.83
[INFO 2017-06-25 19:15:10,120 main.py:50] epoch 3882, training loss: 14119.11, average training loss: 17864.96, base loss: 23526.30
[INFO 2017-06-25 19:15:14,389 main.py:50] epoch 3883, training loss: 19088.97, average training loss: 17865.52, base loss: 23527.65
[INFO 2017-06-25 19:15:18,653 main.py:50] epoch 3884, training loss: 16999.57, average training loss: 17865.27, base loss: 23525.22
[INFO 2017-06-25 19:15:22,916 main.py:50] epoch 3885, training loss: 16117.19, average training loss: 17866.87, base loss: 23529.25
[INFO 2017-06-25 19:15:27,196 main.py:50] epoch 3886, training loss: 19465.39, average training loss: 17866.62, base loss: 23531.45
[INFO 2017-06-25 19:15:31,504 main.py:50] epoch 3887, training loss: 15637.32, average training loss: 17860.31, base loss: 23524.63
[INFO 2017-06-25 19:15:35,761 main.py:50] epoch 3888, training loss: 16977.49, average training loss: 17862.15, base loss: 23529.23
[INFO 2017-06-25 19:15:40,058 main.py:50] epoch 3889, training loss: 16195.07, average training loss: 17861.66, base loss: 23528.45
[INFO 2017-06-25 19:15:44,325 main.py:50] epoch 3890, training loss: 18595.43, average training loss: 17866.19, base loss: 23533.53
[INFO 2017-06-25 19:15:48,561 main.py:50] epoch 3891, training loss: 12527.32, average training loss: 17861.34, base loss: 23527.25
[INFO 2017-06-25 19:15:52,847 main.py:50] epoch 3892, training loss: 15077.72, average training loss: 17859.81, base loss: 23523.27
[INFO 2017-06-25 19:15:57,142 main.py:50] epoch 3893, training loss: 18609.99, average training loss: 17862.90, base loss: 23529.50
[INFO 2017-06-25 19:16:01,415 main.py:50] epoch 3894, training loss: 13159.57, average training loss: 17859.34, base loss: 23526.38
[INFO 2017-06-25 19:16:05,698 main.py:50] epoch 3895, training loss: 13320.65, average training loss: 17850.50, base loss: 23517.24
[INFO 2017-06-25 19:16:09,970 main.py:50] epoch 3896, training loss: 17032.25, average training loss: 17847.01, base loss: 23511.85
[INFO 2017-06-25 19:16:14,268 main.py:50] epoch 3897, training loss: 18667.49, average training loss: 17849.55, base loss: 23518.32
[INFO 2017-06-25 19:16:18,573 main.py:50] epoch 3898, training loss: 12207.74, average training loss: 17841.01, base loss: 23511.18
[INFO 2017-06-25 19:16:22,848 main.py:50] epoch 3899, training loss: 18049.68, average training loss: 17841.92, base loss: 23513.35
[INFO 2017-06-25 19:16:27,132 main.py:50] epoch 3900, training loss: 15384.32, average training loss: 17841.84, base loss: 23513.54
[INFO 2017-06-25 19:16:31,401 main.py:50] epoch 3901, training loss: 19507.09, average training loss: 17843.82, base loss: 23514.60
[INFO 2017-06-25 19:16:35,684 main.py:50] epoch 3902, training loss: 21061.24, average training loss: 17843.79, base loss: 23515.79
[INFO 2017-06-25 19:16:39,977 main.py:50] epoch 3903, training loss: 18161.05, average training loss: 17845.03, base loss: 23517.08
[INFO 2017-06-25 19:16:44,223 main.py:50] epoch 3904, training loss: 17346.32, average training loss: 17844.94, base loss: 23518.90
[INFO 2017-06-25 19:16:48,524 main.py:50] epoch 3905, training loss: 19043.79, average training loss: 17850.20, base loss: 23524.54
[INFO 2017-06-25 19:16:52,823 main.py:50] epoch 3906, training loss: 16613.34, average training loss: 17847.06, base loss: 23522.68
[INFO 2017-06-25 19:16:57,106 main.py:50] epoch 3907, training loss: 17383.81, average training loss: 17847.12, base loss: 23523.46
[INFO 2017-06-25 19:17:01,383 main.py:50] epoch 3908, training loss: 14611.82, average training loss: 17842.33, base loss: 23520.19
[INFO 2017-06-25 19:17:05,665 main.py:50] epoch 3909, training loss: 16719.28, average training loss: 17841.60, base loss: 23518.73
[INFO 2017-06-25 19:17:09,953 main.py:50] epoch 3910, training loss: 19614.33, average training loss: 17840.41, base loss: 23519.88
[INFO 2017-06-25 19:17:14,232 main.py:50] epoch 3911, training loss: 15997.89, average training loss: 17842.99, base loss: 23523.65
[INFO 2017-06-25 19:17:18,502 main.py:50] epoch 3912, training loss: 17486.38, average training loss: 17841.75, base loss: 23524.72
[INFO 2017-06-25 19:17:22,813 main.py:50] epoch 3913, training loss: 19517.71, average training loss: 17844.72, base loss: 23529.35
[INFO 2017-06-25 19:17:27,118 main.py:50] epoch 3914, training loss: 18726.54, average training loss: 17845.25, base loss: 23533.05
[INFO 2017-06-25 19:17:31,401 main.py:50] epoch 3915, training loss: 18077.90, average training loss: 17844.70, base loss: 23531.09
[INFO 2017-06-25 19:17:35,682 main.py:50] epoch 3916, training loss: 17258.69, average training loss: 17847.17, base loss: 23531.00
[INFO 2017-06-25 19:17:39,953 main.py:50] epoch 3917, training loss: 17066.79, average training loss: 17849.77, base loss: 23536.34
[INFO 2017-06-25 19:17:44,251 main.py:50] epoch 3918, training loss: 15773.76, average training loss: 17852.02, base loss: 23539.20
[INFO 2017-06-25 19:17:48,529 main.py:50] epoch 3919, training loss: 16411.01, average training loss: 17850.00, base loss: 23537.17
[INFO 2017-06-25 19:17:52,798 main.py:50] epoch 3920, training loss: 22596.02, average training loss: 17857.64, base loss: 23546.42
[INFO 2017-06-25 19:17:57,097 main.py:50] epoch 3921, training loss: 19257.33, average training loss: 17857.46, base loss: 23543.74
[INFO 2017-06-25 19:18:01,369 main.py:50] epoch 3922, training loss: 14208.83, average training loss: 17845.67, base loss: 23529.90
[INFO 2017-06-25 19:18:05,643 main.py:50] epoch 3923, training loss: 15870.14, average training loss: 17841.97, base loss: 23524.98
[INFO 2017-06-25 19:18:09,957 main.py:50] epoch 3924, training loss: 17877.27, average training loss: 17843.36, base loss: 23528.07
[INFO 2017-06-25 19:18:14,242 main.py:50] epoch 3925, training loss: 21010.66, average training loss: 17846.29, base loss: 23531.92
[INFO 2017-06-25 19:18:18,544 main.py:50] epoch 3926, training loss: 19871.88, average training loss: 17849.18, base loss: 23535.55
[INFO 2017-06-25 19:18:22,860 main.py:50] epoch 3927, training loss: 18182.47, average training loss: 17847.41, base loss: 23534.34
[INFO 2017-06-25 19:18:27,182 main.py:50] epoch 3928, training loss: 15184.24, average training loss: 17844.99, base loss: 23535.37
[INFO 2017-06-25 19:18:31,466 main.py:50] epoch 3929, training loss: 13913.16, average training loss: 17841.00, base loss: 23529.96
[INFO 2017-06-25 19:18:35,721 main.py:50] epoch 3930, training loss: 17380.15, average training loss: 17835.86, base loss: 23523.47
[INFO 2017-06-25 19:18:40,019 main.py:50] epoch 3931, training loss: 19027.19, average training loss: 17837.11, base loss: 23522.96
[INFO 2017-06-25 19:18:44,310 main.py:50] epoch 3932, training loss: 17750.91, average training loss: 17839.70, base loss: 23526.53
[INFO 2017-06-25 19:18:48,589 main.py:50] epoch 3933, training loss: 16085.84, average training loss: 17840.88, base loss: 23526.79
[INFO 2017-06-25 19:18:52,881 main.py:50] epoch 3934, training loss: 18565.74, average training loss: 17840.19, base loss: 23527.06
[INFO 2017-06-25 19:18:57,181 main.py:50] epoch 3935, training loss: 17485.78, average training loss: 17840.30, base loss: 23525.57
[INFO 2017-06-25 19:19:01,470 main.py:50] epoch 3936, training loss: 19874.63, average training loss: 17844.95, base loss: 23530.19
[INFO 2017-06-25 19:19:05,762 main.py:50] epoch 3937, training loss: 16158.31, average training loss: 17842.65, base loss: 23530.24
[INFO 2017-06-25 19:19:10,029 main.py:50] epoch 3938, training loss: 17928.50, average training loss: 17839.82, base loss: 23527.46
[INFO 2017-06-25 19:19:14,301 main.py:50] epoch 3939, training loss: 16955.07, average training loss: 17841.16, base loss: 23529.67
[INFO 2017-06-25 19:19:18,560 main.py:50] epoch 3940, training loss: 14654.59, average training loss: 17835.37, base loss: 23524.31
[INFO 2017-06-25 19:19:22,848 main.py:50] epoch 3941, training loss: 20999.81, average training loss: 17833.84, base loss: 23525.14
[INFO 2017-06-25 19:19:27,146 main.py:50] epoch 3942, training loss: 19682.86, average training loss: 17835.02, base loss: 23523.65
[INFO 2017-06-25 19:19:31,414 main.py:50] epoch 3943, training loss: 12951.25, average training loss: 17826.86, base loss: 23513.02
[INFO 2017-06-25 19:19:35,699 main.py:50] epoch 3944, training loss: 18013.21, average training loss: 17827.40, base loss: 23513.22
[INFO 2017-06-25 19:19:39,993 main.py:50] epoch 3945, training loss: 16157.56, average training loss: 17826.40, base loss: 23511.53
[INFO 2017-06-25 19:19:44,273 main.py:50] epoch 3946, training loss: 16864.41, average training loss: 17827.67, base loss: 23512.91
[INFO 2017-06-25 19:19:48,553 main.py:50] epoch 3947, training loss: 16462.55, average training loss: 17828.24, base loss: 23514.57
[INFO 2017-06-25 19:19:52,860 main.py:50] epoch 3948, training loss: 22779.77, average training loss: 17833.28, base loss: 23522.45
[INFO 2017-06-25 19:19:57,161 main.py:50] epoch 3949, training loss: 15584.18, average training loss: 17831.31, base loss: 23519.69
[INFO 2017-06-25 19:20:01,444 main.py:50] epoch 3950, training loss: 17459.41, average training loss: 17826.38, base loss: 23515.34
[INFO 2017-06-25 19:20:05,722 main.py:50] epoch 3951, training loss: 17888.25, average training loss: 17823.49, base loss: 23512.15
[INFO 2017-06-25 19:20:10,004 main.py:50] epoch 3952, training loss: 16675.42, average training loss: 17824.45, base loss: 23513.80
[INFO 2017-06-25 19:20:14,298 main.py:50] epoch 3953, training loss: 16983.30, average training loss: 17824.57, base loss: 23515.63
[INFO 2017-06-25 19:20:18,591 main.py:50] epoch 3954, training loss: 16665.67, average training loss: 17823.97, base loss: 23513.99
[INFO 2017-06-25 19:20:22,872 main.py:50] epoch 3955, training loss: 17819.13, average training loss: 17822.53, base loss: 23514.63
[INFO 2017-06-25 19:20:27,153 main.py:50] epoch 3956, training loss: 18796.32, average training loss: 17824.65, base loss: 23517.32
[INFO 2017-06-25 19:20:31,439 main.py:50] epoch 3957, training loss: 19102.19, average training loss: 17829.09, base loss: 23526.51
[INFO 2017-06-25 19:20:35,734 main.py:50] epoch 3958, training loss: 15576.60, average training loss: 17829.94, base loss: 23526.71
[INFO 2017-06-25 19:20:40,038 main.py:50] epoch 3959, training loss: 18171.12, average training loss: 17827.95, base loss: 23525.75
[INFO 2017-06-25 19:20:44,328 main.py:50] epoch 3960, training loss: 19189.63, average training loss: 17831.27, base loss: 23529.83
[INFO 2017-06-25 19:20:48,595 main.py:50] epoch 3961, training loss: 22075.53, average training loss: 17831.61, base loss: 23528.98
[INFO 2017-06-25 19:20:52,913 main.py:50] epoch 3962, training loss: 16805.18, average training loss: 17828.51, base loss: 23525.71
[INFO 2017-06-25 19:20:57,206 main.py:50] epoch 3963, training loss: 16881.04, average training loss: 17824.73, base loss: 23519.23
[INFO 2017-06-25 19:21:01,481 main.py:50] epoch 3964, training loss: 18898.87, average training loss: 17820.34, base loss: 23512.14
[INFO 2017-06-25 19:21:05,761 main.py:50] epoch 3965, training loss: 15885.07, average training loss: 17817.60, base loss: 23512.94
[INFO 2017-06-25 19:21:10,032 main.py:50] epoch 3966, training loss: 16637.65, average training loss: 17818.66, base loss: 23514.92
[INFO 2017-06-25 19:21:14,331 main.py:50] epoch 3967, training loss: 20280.00, average training loss: 17819.21, base loss: 23517.29
[INFO 2017-06-25 19:21:18,649 main.py:50] epoch 3968, training loss: 20182.86, average training loss: 17822.02, base loss: 23519.15
[INFO 2017-06-25 19:21:22,960 main.py:50] epoch 3969, training loss: 14462.24, average training loss: 17818.88, base loss: 23514.39
[INFO 2017-06-25 19:21:27,281 main.py:50] epoch 3970, training loss: 17185.04, average training loss: 17817.92, base loss: 23510.51
[INFO 2017-06-25 19:21:31,555 main.py:50] epoch 3971, training loss: 15413.22, average training loss: 17817.71, base loss: 23510.77
[INFO 2017-06-25 19:21:35,872 main.py:50] epoch 3972, training loss: 20416.58, average training loss: 17822.79, base loss: 23517.07
[INFO 2017-06-25 19:21:40,176 main.py:50] epoch 3973, training loss: 14504.97, average training loss: 17823.41, base loss: 23520.72
[INFO 2017-06-25 19:21:44,456 main.py:50] epoch 3974, training loss: 20866.73, average training loss: 17827.62, base loss: 23523.92
[INFO 2017-06-25 19:21:48,757 main.py:50] epoch 3975, training loss: 22595.65, average training loss: 17832.04, base loss: 23529.73
[INFO 2017-06-25 19:21:53,068 main.py:50] epoch 3976, training loss: 18771.47, average training loss: 17830.75, base loss: 23526.27
[INFO 2017-06-25 19:21:57,366 main.py:50] epoch 3977, training loss: 20091.25, average training loss: 17834.85, base loss: 23531.52
[INFO 2017-06-25 19:22:01,668 main.py:50] epoch 3978, training loss: 18297.73, average training loss: 17835.79, base loss: 23532.92
[INFO 2017-06-25 19:22:05,952 main.py:50] epoch 3979, training loss: 18362.51, average training loss: 17837.84, base loss: 23534.46
[INFO 2017-06-25 19:22:10,247 main.py:50] epoch 3980, training loss: 16966.21, average training loss: 17840.59, base loss: 23539.86
[INFO 2017-06-25 19:22:14,537 main.py:50] epoch 3981, training loss: 19754.81, average training loss: 17845.26, base loss: 23544.03
[INFO 2017-06-25 19:22:18,859 main.py:50] epoch 3982, training loss: 16640.56, average training loss: 17843.48, base loss: 23540.33
[INFO 2017-06-25 19:22:23,155 main.py:50] epoch 3983, training loss: 15120.90, average training loss: 17841.64, base loss: 23538.08
[INFO 2017-06-25 19:22:27,442 main.py:50] epoch 3984, training loss: 20512.46, average training loss: 17842.67, base loss: 23541.99
[INFO 2017-06-25 19:22:31,725 main.py:50] epoch 3985, training loss: 16438.88, average training loss: 17840.46, base loss: 23543.06
[INFO 2017-06-25 19:22:35,991 main.py:50] epoch 3986, training loss: 16706.66, average training loss: 17840.72, base loss: 23545.35
[INFO 2017-06-25 19:22:40,293 main.py:50] epoch 3987, training loss: 21492.74, average training loss: 17847.01, base loss: 23553.88
[INFO 2017-06-25 19:22:44,571 main.py:50] epoch 3988, training loss: 20179.84, average training loss: 17849.56, base loss: 23559.12
[INFO 2017-06-25 19:22:48,880 main.py:50] epoch 3989, training loss: 15588.18, average training loss: 17846.73, base loss: 23558.82
[INFO 2017-06-25 19:22:53,178 main.py:50] epoch 3990, training loss: 20028.35, average training loss: 17853.12, base loss: 23566.33
[INFO 2017-06-25 19:22:57,467 main.py:50] epoch 3991, training loss: 17858.42, average training loss: 17849.81, base loss: 23564.64
[INFO 2017-06-25 19:23:01,760 main.py:50] epoch 3992, training loss: 18955.50, average training loss: 17846.71, base loss: 23559.87
[INFO 2017-06-25 19:23:06,056 main.py:50] epoch 3993, training loss: 16755.92, average training loss: 17848.84, base loss: 23564.61
[INFO 2017-06-25 19:23:10,333 main.py:50] epoch 3994, training loss: 23018.96, average training loss: 17853.09, base loss: 23568.35
[INFO 2017-06-25 19:23:14,634 main.py:50] epoch 3995, training loss: 16157.33, average training loss: 17852.80, base loss: 23568.89
[INFO 2017-06-25 19:23:18,953 main.py:50] epoch 3996, training loss: 19278.33, average training loss: 17847.14, base loss: 23565.69
[INFO 2017-06-25 19:23:23,288 main.py:50] epoch 3997, training loss: 16707.05, average training loss: 17847.95, base loss: 23563.34
[INFO 2017-06-25 19:23:27,543 main.py:50] epoch 3998, training loss: 19573.32, average training loss: 17847.24, base loss: 23561.33
[INFO 2017-06-25 19:23:31,835 main.py:50] epoch 3999, training loss: 16138.81, average training loss: 17846.78, base loss: 23559.51
[INFO 2017-06-25 19:23:31,835 main.py:52] epoch 3999, testing
[INFO 2017-06-25 19:27:09,950 main.py:100] average testing loss: 17645.08
[INFO 2017-06-25 19:27:09,953 main.py:72] model save to ./model/final.pth
[INFO 2017-06-25 19:27:09,960 main.py:76] current best accuracy: 17645.08
[INFO 2017-06-25 19:27:14,279 main.py:50] epoch 4000, training loss: 17924.49, average training loss: 17847.95, base loss: 23560.80
[INFO 2017-06-25 19:27:18,562 main.py:50] epoch 4001, training loss: 18966.93, average training loss: 17850.29, base loss: 23563.23
[INFO 2017-06-25 19:27:22,868 main.py:50] epoch 4002, training loss: 17204.92, average training loss: 17850.45, base loss: 23562.14
[INFO 2017-06-25 19:27:27,171 main.py:50] epoch 4003, training loss: 18134.96, average training loss: 17845.38, base loss: 23555.06
[INFO 2017-06-25 19:27:31,469 main.py:50] epoch 4004, training loss: 18444.21, average training loss: 17843.03, base loss: 23552.99
[INFO 2017-06-25 19:27:35,761 main.py:50] epoch 4005, training loss: 17980.34, average training loss: 17841.61, base loss: 23553.37
[INFO 2017-06-25 19:27:40,090 main.py:50] epoch 4006, training loss: 17166.60, average training loss: 17840.95, base loss: 23555.93
[INFO 2017-06-25 19:27:44,435 main.py:50] epoch 4007, training loss: 19003.03, average training loss: 17840.87, base loss: 23557.00
[INFO 2017-06-25 19:27:48,726 main.py:50] epoch 4008, training loss: 21465.97, average training loss: 17845.22, base loss: 23560.44
[INFO 2017-06-25 19:27:53,028 main.py:50] epoch 4009, training loss: 20336.59, average training loss: 17848.13, base loss: 23562.66
[INFO 2017-06-25 19:27:57,292 main.py:50] epoch 4010, training loss: 14769.05, average training loss: 17843.95, base loss: 23558.12
[INFO 2017-06-25 19:28:01,571 main.py:50] epoch 4011, training loss: 16557.11, average training loss: 17843.07, base loss: 23556.18
[INFO 2017-06-25 19:28:05,832 main.py:50] epoch 4012, training loss: 19327.79, average training loss: 17837.94, base loss: 23549.38
[INFO 2017-06-25 19:28:10,100 main.py:50] epoch 4013, training loss: 15606.03, average training loss: 17837.13, base loss: 23547.78
[INFO 2017-06-25 19:28:14,365 main.py:50] epoch 4014, training loss: 19192.70, average training loss: 17841.15, base loss: 23558.31
[INFO 2017-06-25 19:28:18,646 main.py:50] epoch 4015, training loss: 19050.71, average training loss: 17844.46, base loss: 23563.01
[INFO 2017-06-25 19:28:22,948 main.py:50] epoch 4016, training loss: 17621.91, average training loss: 17843.61, base loss: 23561.91
[INFO 2017-06-25 19:28:27,212 main.py:50] epoch 4017, training loss: 19329.05, average training loss: 17841.40, base loss: 23560.44
[INFO 2017-06-25 19:28:31,485 main.py:50] epoch 4018, training loss: 15424.52, average training loss: 17843.85, base loss: 23564.88
[INFO 2017-06-25 19:28:35,787 main.py:50] epoch 4019, training loss: 15609.49, average training loss: 17842.85, base loss: 23563.78
[INFO 2017-06-25 19:28:40,096 main.py:50] epoch 4020, training loss: 18196.18, average training loss: 17844.88, base loss: 23565.84
[INFO 2017-06-25 19:28:44,408 main.py:50] epoch 4021, training loss: 20584.35, average training loss: 17845.70, base loss: 23568.55
[INFO 2017-06-25 19:28:48,714 main.py:50] epoch 4022, training loss: 14598.30, average training loss: 17838.53, base loss: 23562.07
[INFO 2017-06-25 19:28:53,005 main.py:50] epoch 4023, training loss: 16514.71, average training loss: 17836.65, base loss: 23557.07
[INFO 2017-06-25 19:28:57,291 main.py:50] epoch 4024, training loss: 17700.91, average training loss: 17829.63, base loss: 23549.40
[INFO 2017-06-25 19:29:01,566 main.py:50] epoch 4025, training loss: 20965.38, average training loss: 17833.17, base loss: 23556.27
[INFO 2017-06-25 19:29:05,893 main.py:50] epoch 4026, training loss: 17436.04, average training loss: 17828.91, base loss: 23552.99
[INFO 2017-06-25 19:29:10,192 main.py:50] epoch 4027, training loss: 16677.33, average training loss: 17823.98, base loss: 23548.08
[INFO 2017-06-25 19:29:14,508 main.py:50] epoch 4028, training loss: 18326.71, average training loss: 17824.02, base loss: 23554.28
[INFO 2017-06-25 19:29:18,790 main.py:50] epoch 4029, training loss: 14737.83, average training loss: 17821.46, base loss: 23551.66
[INFO 2017-06-25 19:29:23,110 main.py:50] epoch 4030, training loss: 16728.77, average training loss: 17820.48, base loss: 23552.75
[INFO 2017-06-25 19:29:27,412 main.py:50] epoch 4031, training loss: 15752.39, average training loss: 17819.65, base loss: 23554.47
[INFO 2017-06-25 19:29:31,702 main.py:50] epoch 4032, training loss: 21918.12, average training loss: 17822.99, base loss: 23560.56
[INFO 2017-06-25 19:29:35,990 main.py:50] epoch 4033, training loss: 16777.28, average training loss: 17821.75, base loss: 23560.91
[INFO 2017-06-25 19:29:40,314 main.py:50] epoch 4034, training loss: 20901.92, average training loss: 17827.02, base loss: 23570.49
[INFO 2017-06-25 19:29:44,569 main.py:50] epoch 4035, training loss: 17670.67, average training loss: 17824.68, base loss: 23569.08
[INFO 2017-06-25 19:29:48,883 main.py:50] epoch 4036, training loss: 17025.53, average training loss: 17822.83, base loss: 23565.36
[INFO 2017-06-25 19:29:53,164 main.py:50] epoch 4037, training loss: 17825.34, average training loss: 17823.89, base loss: 23567.37
[INFO 2017-06-25 19:29:57,440 main.py:50] epoch 4038, training loss: 17611.65, average training loss: 17822.49, base loss: 23566.71
[INFO 2017-06-25 19:30:01,741 main.py:50] epoch 4039, training loss: 17160.88, average training loss: 17820.44, base loss: 23564.55
[INFO 2017-06-25 19:30:06,029 main.py:50] epoch 4040, training loss: 15305.57, average training loss: 17817.35, base loss: 23562.06
[INFO 2017-06-25 19:30:10,314 main.py:50] epoch 4041, training loss: 13098.72, average training loss: 17813.38, base loss: 23556.00
[INFO 2017-06-25 19:30:14,590 main.py:50] epoch 4042, training loss: 17769.98, average training loss: 17814.19, base loss: 23556.29
[INFO 2017-06-25 19:30:18,879 main.py:50] epoch 4043, training loss: 18398.30, average training loss: 17816.88, base loss: 23559.56
[INFO 2017-06-25 19:30:23,169 main.py:50] epoch 4044, training loss: 15490.33, average training loss: 17815.19, base loss: 23557.31
[INFO 2017-06-25 19:30:27,434 main.py:50] epoch 4045, training loss: 15848.67, average training loss: 17814.18, base loss: 23557.22
[INFO 2017-06-25 19:30:31,698 main.py:50] epoch 4046, training loss: 15249.51, average training loss: 17811.18, base loss: 23554.18
[INFO 2017-06-25 19:30:35,987 main.py:50] epoch 4047, training loss: 18860.66, average training loss: 17811.90, base loss: 23555.79
[INFO 2017-06-25 19:30:40,288 main.py:50] epoch 4048, training loss: 13668.54, average training loss: 17809.01, base loss: 23551.54
[INFO 2017-06-25 19:30:44,574 main.py:50] epoch 4049, training loss: 13490.88, average training loss: 17799.51, base loss: 23536.52
[INFO 2017-06-25 19:30:48,858 main.py:50] epoch 4050, training loss: 15809.09, average training loss: 17798.86, base loss: 23535.94
[INFO 2017-06-25 19:30:53,159 main.py:50] epoch 4051, training loss: 16334.84, average training loss: 17794.05, base loss: 23531.12
[INFO 2017-06-25 19:30:57,445 main.py:50] epoch 4052, training loss: 16841.46, average training loss: 17794.19, base loss: 23532.37
[INFO 2017-06-25 19:31:01,738 main.py:50] epoch 4053, training loss: 13966.85, average training loss: 17789.75, base loss: 23527.03
[INFO 2017-06-25 19:31:06,008 main.py:50] epoch 4054, training loss: 16545.62, average training loss: 17789.90, base loss: 23528.30
[INFO 2017-06-25 19:31:10,298 main.py:50] epoch 4055, training loss: 19403.51, average training loss: 17791.11, base loss: 23530.61
[INFO 2017-06-25 19:31:14,578 main.py:50] epoch 4056, training loss: 17311.03, average training loss: 17786.75, base loss: 23529.20
[INFO 2017-06-25 19:31:18,850 main.py:50] epoch 4057, training loss: 15927.04, average training loss: 17784.81, base loss: 23526.70
[INFO 2017-06-25 19:31:23,111 main.py:50] epoch 4058, training loss: 20142.60, average training loss: 17788.61, base loss: 23528.83
[INFO 2017-06-25 19:31:27,389 main.py:50] epoch 4059, training loss: 17502.42, average training loss: 17782.88, base loss: 23522.00
[INFO 2017-06-25 19:31:31,670 main.py:50] epoch 4060, training loss: 15736.32, average training loss: 17781.93, base loss: 23519.41
[INFO 2017-06-25 19:31:35,950 main.py:50] epoch 4061, training loss: 17375.74, average training loss: 17780.38, base loss: 23517.03
[INFO 2017-06-25 19:31:40,227 main.py:50] epoch 4062, training loss: 20048.51, average training loss: 17781.54, base loss: 23519.04
[INFO 2017-06-25 19:31:44,523 main.py:50] epoch 4063, training loss: 20291.08, average training loss: 17783.65, base loss: 23521.33
[INFO 2017-06-25 19:31:48,815 main.py:50] epoch 4064, training loss: 15580.36, average training loss: 17780.55, base loss: 23516.19
[INFO 2017-06-25 19:31:53,099 main.py:50] epoch 4065, training loss: 15621.54, average training loss: 17779.27, base loss: 23514.29
[INFO 2017-06-25 19:31:57,388 main.py:50] epoch 4066, training loss: 19087.53, average training loss: 17782.54, base loss: 23518.78
[INFO 2017-06-25 19:32:01,647 main.py:50] epoch 4067, training loss: 16767.37, average training loss: 17781.72, base loss: 23517.70
[INFO 2017-06-25 19:32:05,910 main.py:50] epoch 4068, training loss: 17273.05, average training loss: 17781.57, base loss: 23517.73
[INFO 2017-06-25 19:32:10,319 main.py:50] epoch 4069, training loss: 16814.60, average training loss: 17776.88, base loss: 23511.74
[INFO 2017-06-25 19:32:14,591 main.py:50] epoch 4070, training loss: 20109.25, average training loss: 17782.90, base loss: 23521.06
[INFO 2017-06-25 19:32:18,837 main.py:50] epoch 4071, training loss: 14118.17, average training loss: 17773.29, base loss: 23507.95
[INFO 2017-06-25 19:32:23,101 main.py:50] epoch 4072, training loss: 13364.55, average training loss: 17768.00, base loss: 23502.50
[INFO 2017-06-25 19:32:27,367 main.py:50] epoch 4073, training loss: 15859.64, average training loss: 17766.50, base loss: 23501.86
[INFO 2017-06-25 19:32:31,638 main.py:50] epoch 4074, training loss: 18013.57, average training loss: 17767.27, base loss: 23502.26
[INFO 2017-06-25 19:32:35,895 main.py:50] epoch 4075, training loss: 18060.34, average training loss: 17767.34, base loss: 23501.33
[INFO 2017-06-25 19:32:40,162 main.py:50] epoch 4076, training loss: 21984.27, average training loss: 17770.81, base loss: 23503.75
[INFO 2017-06-25 19:32:44,411 main.py:50] epoch 4077, training loss: 14290.05, average training loss: 17766.85, base loss: 23498.86
[INFO 2017-06-25 19:32:48,661 main.py:50] epoch 4078, training loss: 21258.45, average training loss: 17772.03, base loss: 23507.43
[INFO 2017-06-25 19:32:52,954 main.py:50] epoch 4079, training loss: 16191.52, average training loss: 17771.25, base loss: 23508.34
[INFO 2017-06-25 19:32:57,204 main.py:50] epoch 4080, training loss: 18597.60, average training loss: 17774.39, base loss: 23513.20
[INFO 2017-06-25 19:33:01,472 main.py:50] epoch 4081, training loss: 19707.53, average training loss: 17771.89, base loss: 23510.63
[INFO 2017-06-25 19:33:05,751 main.py:50] epoch 4082, training loss: 15822.86, average training loss: 17767.46, base loss: 23505.68
[INFO 2017-06-25 19:33:10,020 main.py:50] epoch 4083, training loss: 20767.85, average training loss: 17774.35, base loss: 23510.91
[INFO 2017-06-25 19:33:14,285 main.py:50] epoch 4084, training loss: 20231.97, average training loss: 17773.77, base loss: 23510.69
[INFO 2017-06-25 19:33:18,596 main.py:50] epoch 4085, training loss: 15227.91, average training loss: 17771.60, base loss: 23511.29
[INFO 2017-06-25 19:33:22,921 main.py:50] epoch 4086, training loss: 16615.03, average training loss: 17771.14, base loss: 23511.47
[INFO 2017-06-25 19:33:27,165 main.py:50] epoch 4087, training loss: 16143.85, average training loss: 17768.56, base loss: 23509.11
[INFO 2017-06-25 19:33:31,424 main.py:50] epoch 4088, training loss: 19524.79, average training loss: 17771.60, base loss: 23512.03
[INFO 2017-06-25 19:33:35,676 main.py:50] epoch 4089, training loss: 14941.80, average training loss: 17770.82, base loss: 23509.37
[INFO 2017-06-25 19:33:39,914 main.py:50] epoch 4090, training loss: 20131.06, average training loss: 17770.59, base loss: 23509.54
[INFO 2017-06-25 19:33:44,166 main.py:50] epoch 4091, training loss: 19037.04, average training loss: 17771.89, base loss: 23512.56
[INFO 2017-06-25 19:33:48,451 main.py:50] epoch 4092, training loss: 21370.13, average training loss: 17779.20, base loss: 23522.88
[INFO 2017-06-25 19:33:52,701 main.py:50] epoch 4093, training loss: 19732.54, average training loss: 17781.38, base loss: 23524.37
[INFO 2017-06-25 19:33:56,957 main.py:50] epoch 4094, training loss: 17133.51, average training loss: 17779.11, base loss: 23521.89
[INFO 2017-06-25 19:34:01,213 main.py:50] epoch 4095, training loss: 16173.63, average training loss: 17776.63, base loss: 23517.65
[INFO 2017-06-25 19:34:05,459 main.py:50] epoch 4096, training loss: 19505.15, average training loss: 17778.35, base loss: 23521.60
[INFO 2017-06-25 19:34:09,740 main.py:50] epoch 4097, training loss: 19177.22, average training loss: 17783.21, base loss: 23525.36
[INFO 2017-06-25 19:34:14,018 main.py:50] epoch 4098, training loss: 18352.12, average training loss: 17783.35, base loss: 23523.90
[INFO 2017-06-25 19:34:18,269 main.py:50] epoch 4099, training loss: 14404.00, average training loss: 17777.32, base loss: 23514.46
[INFO 2017-06-25 19:34:22,619 main.py:50] epoch 4100, training loss: 17178.47, average training loss: 17775.45, base loss: 23508.77
[INFO 2017-06-25 19:34:29,114 main.py:50] epoch 4101, training loss: 16397.64, average training loss: 17774.44, base loss: 23505.36
[INFO 2017-06-25 19:34:36,195 main.py:50] epoch 4102, training loss: 14423.94, average training loss: 17773.03, base loss: 23504.20
[INFO 2017-06-25 19:34:41,328 main.py:50] epoch 4103, training loss: 21464.56, average training loss: 17777.65, base loss: 23509.56
[INFO 2017-06-25 19:34:45,581 main.py:50] epoch 4104, training loss: 18483.97, average training loss: 17777.60, base loss: 23510.15
[INFO 2017-06-25 19:34:49,848 main.py:50] epoch 4105, training loss: 19745.17, average training loss: 17778.82, base loss: 23512.17
[INFO 2017-06-25 19:34:54,114 main.py:50] epoch 4106, training loss: 17432.40, average training loss: 17776.07, base loss: 23509.93
[INFO 2017-06-25 19:34:58,372 main.py:50] epoch 4107, training loss: 14674.79, average training loss: 17769.95, base loss: 23500.13
[INFO 2017-06-25 19:35:02,633 main.py:50] epoch 4108, training loss: 14295.20, average training loss: 17769.09, base loss: 23499.88
[INFO 2017-06-25 19:35:06,903 main.py:50] epoch 4109, training loss: 18722.06, average training loss: 17769.42, base loss: 23502.44
[INFO 2017-06-25 19:35:11,166 main.py:50] epoch 4110, training loss: 14456.52, average training loss: 17766.48, base loss: 23497.16
[INFO 2017-06-25 19:35:15,431 main.py:50] epoch 4111, training loss: 19624.43, average training loss: 17771.21, base loss: 23503.20
[INFO 2017-06-25 19:35:19,668 main.py:50] epoch 4112, training loss: 17385.53, average training loss: 17768.83, base loss: 23500.03
[INFO 2017-06-25 19:35:23,953 main.py:50] epoch 4113, training loss: 18870.48, average training loss: 17767.27, base loss: 23499.64
[INFO 2017-06-25 19:35:28,240 main.py:50] epoch 4114, training loss: 19022.92, average training loss: 17771.26, base loss: 23505.61
[INFO 2017-06-25 19:35:32,538 main.py:50] epoch 4115, training loss: 18888.05, average training loss: 17776.99, base loss: 23513.58
[INFO 2017-06-25 19:35:36,839 main.py:50] epoch 4116, training loss: 15037.50, average training loss: 17773.65, base loss: 23509.31
[INFO 2017-06-25 19:35:41,077 main.py:50] epoch 4117, training loss: 18962.29, average training loss: 17772.28, base loss: 23506.21
[INFO 2017-06-25 19:35:45,377 main.py:50] epoch 4118, training loss: 20016.11, average training loss: 17774.36, base loss: 23507.92
[INFO 2017-06-25 19:35:49,644 main.py:50] epoch 4119, training loss: 20194.55, average training loss: 17777.33, base loss: 23513.38
[INFO 2017-06-25 19:35:53,909 main.py:50] epoch 4120, training loss: 14664.69, average training loss: 17776.69, base loss: 23510.45
[INFO 2017-06-25 19:35:58,165 main.py:50] epoch 4121, training loss: 15660.23, average training loss: 17779.33, base loss: 23514.64
[INFO 2017-06-25 19:36:02,426 main.py:50] epoch 4122, training loss: 15514.93, average training loss: 17777.09, base loss: 23511.60
[INFO 2017-06-25 19:36:06,670 main.py:50] epoch 4123, training loss: 17365.13, average training loss: 17775.49, base loss: 23513.30
[INFO 2017-06-25 19:36:10,946 main.py:50] epoch 4124, training loss: 18290.53, average training loss: 17778.75, base loss: 23518.54
[INFO 2017-06-25 19:36:15,225 main.py:50] epoch 4125, training loss: 16356.08, average training loss: 17777.73, base loss: 23516.83
[INFO 2017-06-25 19:36:19,491 main.py:50] epoch 4126, training loss: 20529.59, average training loss: 17778.27, base loss: 23513.76
[INFO 2017-06-25 19:36:23,748 main.py:50] epoch 4127, training loss: 16394.04, average training loss: 17775.23, base loss: 23509.12
[INFO 2017-06-25 19:36:28,007 main.py:50] epoch 4128, training loss: 18464.14, average training loss: 17774.46, base loss: 23507.87
[INFO 2017-06-25 19:36:32,512 main.py:50] epoch 4129, training loss: 13428.23, average training loss: 17772.21, base loss: 23508.23
[INFO 2017-06-25 19:36:36,786 main.py:50] epoch 4130, training loss: 18757.85, average training loss: 17773.12, base loss: 23508.95
[INFO 2017-06-25 19:36:41,042 main.py:50] epoch 4131, training loss: 17101.71, average training loss: 17770.12, base loss: 23505.89
[INFO 2017-06-25 19:36:45,321 main.py:50] epoch 4132, training loss: 19680.34, average training loss: 17771.33, base loss: 23507.78
[INFO 2017-06-25 19:36:49,603 main.py:50] epoch 4133, training loss: 15787.33, average training loss: 17768.62, base loss: 23504.41
[INFO 2017-06-25 19:36:53,846 main.py:50] epoch 4134, training loss: 16686.57, average training loss: 17765.21, base loss: 23503.60
[INFO 2017-06-25 19:36:58,144 main.py:50] epoch 4135, training loss: 18035.70, average training loss: 17764.09, base loss: 23502.77
[INFO 2017-06-25 19:37:02,702 main.py:50] epoch 4136, training loss: 14460.84, average training loss: 17756.28, base loss: 23494.45
[INFO 2017-06-25 19:37:06,961 main.py:50] epoch 4137, training loss: 16257.61, average training loss: 17754.02, base loss: 23492.39
[INFO 2017-06-25 19:37:11,258 main.py:50] epoch 4138, training loss: 18954.14, average training loss: 17752.05, base loss: 23493.08
[INFO 2017-06-25 19:37:15,542 main.py:50] epoch 4139, training loss: 19192.57, average training loss: 17755.13, base loss: 23498.10
[INFO 2017-06-25 19:37:19,825 main.py:50] epoch 4140, training loss: 18476.80, average training loss: 17753.16, base loss: 23496.63
[INFO 2017-06-25 19:37:24,095 main.py:50] epoch 4141, training loss: 18479.01, average training loss: 17751.97, base loss: 23496.21
[INFO 2017-06-25 19:37:28,510 main.py:50] epoch 4142, training loss: 18447.68, average training loss: 17751.46, base loss: 23496.00
[INFO 2017-06-25 19:37:32,831 main.py:50] epoch 4143, training loss: 19627.01, average training loss: 17754.29, base loss: 23499.49
[INFO 2017-06-25 19:37:37,113 main.py:50] epoch 4144, training loss: 19288.96, average training loss: 17756.69, base loss: 23502.56
[INFO 2017-06-25 19:37:41,375 main.py:50] epoch 4145, training loss: 15677.48, average training loss: 17755.65, base loss: 23499.52
[INFO 2017-06-25 19:37:45,611 main.py:50] epoch 4146, training loss: 21620.38, average training loss: 17758.87, base loss: 23507.19
[INFO 2017-06-25 19:37:49,881 main.py:50] epoch 4147, training loss: 21546.18, average training loss: 17760.80, base loss: 23513.46
[INFO 2017-06-25 19:37:54,145 main.py:50] epoch 4148, training loss: 18538.16, average training loss: 17763.23, base loss: 23517.51
[INFO 2017-06-25 19:37:58,426 main.py:50] epoch 4149, training loss: 18570.05, average training loss: 17769.97, base loss: 23527.50
[INFO 2017-06-25 19:38:02,664 main.py:50] epoch 4150, training loss: 16426.69, average training loss: 17766.70, base loss: 23523.96
[INFO 2017-06-25 19:38:06,950 main.py:50] epoch 4151, training loss: 16573.65, average training loss: 17767.56, base loss: 23523.83
[INFO 2017-06-25 19:38:11,238 main.py:50] epoch 4152, training loss: 16826.08, average training loss: 17768.18, base loss: 23524.33
[INFO 2017-06-25 19:38:15,530 main.py:50] epoch 4153, training loss: 18467.66, average training loss: 17765.37, base loss: 23522.13
[INFO 2017-06-25 19:38:19,788 main.py:50] epoch 4154, training loss: 20230.78, average training loss: 17768.82, base loss: 23527.60
[INFO 2017-06-25 19:38:24,046 main.py:50] epoch 4155, training loss: 15908.73, average training loss: 17767.48, base loss: 23525.51
[INFO 2017-06-25 19:38:28,307 main.py:50] epoch 4156, training loss: 15374.91, average training loss: 17767.11, base loss: 23523.98
[INFO 2017-06-25 19:38:32,586 main.py:50] epoch 4157, training loss: 13451.97, average training loss: 17758.84, base loss: 23513.33
[INFO 2017-06-25 19:38:36,854 main.py:50] epoch 4158, training loss: 16768.60, average training loss: 17754.86, base loss: 23507.53
[INFO 2017-06-25 19:38:41,122 main.py:50] epoch 4159, training loss: 21527.13, average training loss: 17756.87, base loss: 23511.51
[INFO 2017-06-25 19:38:45,390 main.py:50] epoch 4160, training loss: 18390.71, average training loss: 17752.73, base loss: 23506.23
[INFO 2017-06-25 19:38:49,765 main.py:50] epoch 4161, training loss: 20608.44, average training loss: 17756.88, base loss: 23512.12
[INFO 2017-06-25 19:38:54,100 main.py:50] epoch 4162, training loss: 16648.55, average training loss: 17755.69, base loss: 23510.86
[INFO 2017-06-25 19:38:58,353 main.py:50] epoch 4163, training loss: 15046.71, average training loss: 17753.92, base loss: 23504.70
[INFO 2017-06-25 19:39:02,627 main.py:50] epoch 4164, training loss: 13190.79, average training loss: 17751.12, base loss: 23500.99
[INFO 2017-06-25 19:39:06,888 main.py:50] epoch 4165, training loss: 15507.65, average training loss: 17746.39, base loss: 23495.61
[INFO 2017-06-25 19:39:11,132 main.py:50] epoch 4166, training loss: 14851.96, average training loss: 17742.56, base loss: 23490.16
[INFO 2017-06-25 19:39:15,412 main.py:50] epoch 4167, training loss: 18802.80, average training loss: 17744.13, base loss: 23495.07
[INFO 2017-06-25 19:39:19,814 main.py:50] epoch 4168, training loss: 15730.71, average training loss: 17738.41, base loss: 23488.91
[INFO 2017-06-25 19:39:24,138 main.py:50] epoch 4169, training loss: 21129.02, average training loss: 17743.04, base loss: 23493.93
[INFO 2017-06-25 19:39:28,417 main.py:50] epoch 4170, training loss: 17149.96, average training loss: 17743.05, base loss: 23492.64
[INFO 2017-06-25 19:39:32,671 main.py:50] epoch 4171, training loss: 14713.37, average training loss: 17741.09, base loss: 23488.77
[INFO 2017-06-25 19:39:36,967 main.py:50] epoch 4172, training loss: 18229.79, average training loss: 17741.13, base loss: 23488.55
[INFO 2017-06-25 19:39:41,226 main.py:50] epoch 4173, training loss: 17429.95, average training loss: 17740.91, base loss: 23488.58
[INFO 2017-06-25 19:39:45,494 main.py:50] epoch 4174, training loss: 17067.49, average training loss: 17742.74, base loss: 23493.48
[INFO 2017-06-25 19:39:49,759 main.py:50] epoch 4175, training loss: 13725.61, average training loss: 17742.42, base loss: 23491.98
[INFO 2017-06-25 19:39:54,298 main.py:50] epoch 4176, training loss: 17664.11, average training loss: 17741.12, base loss: 23488.19
[INFO 2017-06-25 19:39:58,568 main.py:50] epoch 4177, training loss: 14345.47, average training loss: 17735.64, base loss: 23479.98
[INFO 2017-06-25 19:40:02,814 main.py:50] epoch 4178, training loss: 16417.08, average training loss: 17732.80, base loss: 23476.76
[INFO 2017-06-25 19:40:07,104 main.py:50] epoch 4179, training loss: 18607.58, average training loss: 17726.23, base loss: 23469.13
[INFO 2017-06-25 19:40:11,383 main.py:50] epoch 4180, training loss: 17405.02, average training loss: 17724.48, base loss: 23465.28
[INFO 2017-06-25 19:40:15,839 main.py:50] epoch 4181, training loss: 25764.97, average training loss: 17729.09, base loss: 23468.79
[INFO 2017-06-25 19:40:20,130 main.py:50] epoch 4182, training loss: 17907.26, average training loss: 17728.01, base loss: 23472.81
[INFO 2017-06-25 19:40:24,389 main.py:50] epoch 4183, training loss: 19069.33, average training loss: 17730.52, base loss: 23477.84
[INFO 2017-06-25 19:40:28,659 main.py:50] epoch 4184, training loss: 17666.46, average training loss: 17728.15, base loss: 23476.14
[INFO 2017-06-25 19:40:32,904 main.py:50] epoch 4185, training loss: 22016.17, average training loss: 17733.90, base loss: 23483.18
[INFO 2017-06-25 19:40:37,149 main.py:50] epoch 4186, training loss: 18327.69, average training loss: 17735.54, base loss: 23488.56
[INFO 2017-06-25 19:40:41,398 main.py:50] epoch 4187, training loss: 18554.53, average training loss: 17740.10, base loss: 23492.64
[INFO 2017-06-25 19:40:45,653 main.py:50] epoch 4188, training loss: 17837.44, average training loss: 17740.29, base loss: 23492.20
[INFO 2017-06-25 19:40:49,910 main.py:50] epoch 4189, training loss: 15579.32, average training loss: 17741.76, base loss: 23494.09
[INFO 2017-06-25 19:40:54,176 main.py:50] epoch 4190, training loss: 18478.14, average training loss: 17742.83, base loss: 23493.72
[INFO 2017-06-25 19:40:58,426 main.py:50] epoch 4191, training loss: 14674.70, average training loss: 17743.28, base loss: 23495.08
[INFO 2017-06-25 19:41:02,696 main.py:50] epoch 4192, training loss: 15065.33, average training loss: 17742.86, base loss: 23497.23
[INFO 2017-06-25 19:41:06,960 main.py:50] epoch 4193, training loss: 17407.38, average training loss: 17743.56, base loss: 23498.01
[INFO 2017-06-25 19:41:11,245 main.py:50] epoch 4194, training loss: 16893.73, average training loss: 17745.44, base loss: 23499.41
[INFO 2017-06-25 19:41:15,525 main.py:50] epoch 4195, training loss: 20594.32, average training loss: 17749.93, base loss: 23502.01
[INFO 2017-06-25 19:41:19,798 main.py:50] epoch 4196, training loss: 15427.84, average training loss: 17749.93, base loss: 23502.01
[INFO 2017-06-25 19:41:24,087 main.py:50] epoch 4197, training loss: 19834.74, average training loss: 17751.94, base loss: 23503.76
[INFO 2017-06-25 19:41:28,337 main.py:50] epoch 4198, training loss: 19316.33, average training loss: 17752.42, base loss: 23507.68
[INFO 2017-06-25 19:41:32,606 main.py:50] epoch 4199, training loss: 16137.72, average training loss: 17750.71, base loss: 23505.75
[INFO 2017-06-25 19:41:36,865 main.py:50] epoch 4200, training loss: 19106.59, average training loss: 17754.00, base loss: 23512.61
[INFO 2017-06-25 19:41:41,102 main.py:50] epoch 4201, training loss: 19463.15, average training loss: 17756.80, base loss: 23519.76
[INFO 2017-06-25 19:41:45,360 main.py:50] epoch 4202, training loss: 16606.99, average training loss: 17758.80, base loss: 23522.84
[INFO 2017-06-25 19:41:49,630 main.py:50] epoch 4203, training loss: 16882.30, average training loss: 17757.03, base loss: 23521.56
[INFO 2017-06-25 19:41:54,035 main.py:50] epoch 4204, training loss: 16744.84, average training loss: 17756.75, base loss: 23523.12
[INFO 2017-06-25 19:41:58,271 main.py:50] epoch 4205, training loss: 16573.31, average training loss: 17758.73, base loss: 23530.12
[INFO 2017-06-25 19:42:02,525 main.py:50] epoch 4206, training loss: 16235.54, average training loss: 17754.60, base loss: 23522.54
[INFO 2017-06-25 19:42:06,783 main.py:50] epoch 4207, training loss: 22905.56, average training loss: 17758.13, base loss: 23529.24
[INFO 2017-06-25 19:42:11,071 main.py:50] epoch 4208, training loss: 21114.90, average training loss: 17759.62, base loss: 23531.76
[INFO 2017-06-25 19:42:15,329 main.py:50] epoch 4209, training loss: 21219.27, average training loss: 17763.03, base loss: 23537.70
[INFO 2017-06-25 19:42:19,671 main.py:50] epoch 4210, training loss: 13837.82, average training loss: 17757.81, base loss: 23530.59
[INFO 2017-06-25 19:42:24,026 main.py:50] epoch 4211, training loss: 17255.73, average training loss: 17752.94, base loss: 23524.56
[INFO 2017-06-25 19:42:28,269 main.py:50] epoch 4212, training loss: 13197.42, average training loss: 17747.13, base loss: 23516.57
[INFO 2017-06-25 19:42:32,534 main.py:50] epoch 4213, training loss: 16168.41, average training loss: 17743.63, base loss: 23511.02
[INFO 2017-06-25 19:42:36,787 main.py:50] epoch 4214, training loss: 16329.91, average training loss: 17742.98, base loss: 23509.42
[INFO 2017-06-25 19:42:41,052 main.py:50] epoch 4215, training loss: 18657.84, average training loss: 17739.91, base loss: 23503.35
[INFO 2017-06-25 19:42:45,316 main.py:50] epoch 4216, training loss: 18853.38, average training loss: 17737.56, base loss: 23501.57
[INFO 2017-06-25 19:42:49,572 main.py:50] epoch 4217, training loss: 16415.83, average training loss: 17735.50, base loss: 23501.34
[INFO 2017-06-25 19:42:53,824 main.py:50] epoch 4218, training loss: 17570.60, average training loss: 17734.51, base loss: 23500.12
[INFO 2017-06-25 19:42:58,096 main.py:50] epoch 4219, training loss: 17403.58, average training loss: 17738.92, base loss: 23505.95
[INFO 2017-06-25 19:43:02,372 main.py:50] epoch 4220, training loss: 16645.69, average training loss: 17735.99, base loss: 23500.98
[INFO 2017-06-25 19:43:06,857 main.py:50] epoch 4221, training loss: 15136.26, average training loss: 17733.03, base loss: 23498.27
[INFO 2017-06-25 19:43:11,109 main.py:50] epoch 4222, training loss: 15306.50, average training loss: 17730.93, base loss: 23497.82
[INFO 2017-06-25 19:43:15,381 main.py:50] epoch 4223, training loss: 19327.99, average training loss: 17730.66, base loss: 23496.51
[INFO 2017-06-25 19:43:19,654 main.py:50] epoch 4224, training loss: 18933.06, average training loss: 17730.82, base loss: 23496.67
[INFO 2017-06-25 19:43:23,942 main.py:50] epoch 4225, training loss: 17283.65, average training loss: 17728.46, base loss: 23492.95
[INFO 2017-06-25 19:43:28,233 main.py:50] epoch 4226, training loss: 16717.55, average training loss: 17724.37, base loss: 23489.31
[INFO 2017-06-25 19:43:32,514 main.py:50] epoch 4227, training loss: 21623.01, average training loss: 17728.02, base loss: 23492.05
[INFO 2017-06-25 19:43:36,895 main.py:50] epoch 4228, training loss: 18433.27, average training loss: 17729.43, base loss: 23497.34
[INFO 2017-06-25 19:43:41,228 main.py:50] epoch 4229, training loss: 18104.78, average training loss: 17729.77, base loss: 23499.55
[INFO 2017-06-25 19:43:45,492 main.py:50] epoch 4230, training loss: 16567.16, average training loss: 17727.05, base loss: 23496.58
[INFO 2017-06-25 19:43:49,792 main.py:50] epoch 4231, training loss: 19077.82, average training loss: 17727.03, base loss: 23495.66
[INFO 2017-06-25 19:43:54,069 main.py:50] epoch 4232, training loss: 19292.57, average training loss: 17724.51, base loss: 23494.41
[INFO 2017-06-25 19:43:58,338 main.py:50] epoch 4233, training loss: 18990.69, average training loss: 17726.62, base loss: 23498.63
[INFO 2017-06-25 19:44:02,592 main.py:50] epoch 4234, training loss: 21131.14, average training loss: 17729.15, base loss: 23502.08
[INFO 2017-06-25 19:44:06,876 main.py:50] epoch 4235, training loss: 17173.65, average training loss: 17722.22, base loss: 23493.48
[INFO 2017-06-25 19:44:11,177 main.py:50] epoch 4236, training loss: 19679.58, average training loss: 17724.01, base loss: 23494.04
[INFO 2017-06-25 19:44:15,457 main.py:50] epoch 4237, training loss: 22678.80, average training loss: 17726.87, base loss: 23502.26
[INFO 2017-06-25 19:44:19,742 main.py:50] epoch 4238, training loss: 16539.90, average training loss: 17727.44, base loss: 23501.93
[INFO 2017-06-25 19:44:24,250 main.py:50] epoch 4239, training loss: 18247.25, average training loss: 17727.25, base loss: 23505.31
[INFO 2017-06-25 19:44:28,512 main.py:50] epoch 4240, training loss: 19756.75, average training loss: 17730.39, base loss: 23505.66
[INFO 2017-06-25 19:44:32,991 main.py:50] epoch 4241, training loss: 19004.15, average training loss: 17724.03, base loss: 23496.91
[INFO 2017-06-25 19:44:37,269 main.py:50] epoch 4242, training loss: 15744.44, average training loss: 17724.73, base loss: 23498.30
[INFO 2017-06-25 19:44:41,566 main.py:50] epoch 4243, training loss: 18460.61, average training loss: 17725.12, base loss: 23498.94
[INFO 2017-06-25 19:44:45,814 main.py:50] epoch 4244, training loss: 18205.34, average training loss: 17724.98, base loss: 23499.71
[INFO 2017-06-25 19:44:50,081 main.py:50] epoch 4245, training loss: 17725.67, average training loss: 17723.57, base loss: 23499.91
[INFO 2017-06-25 19:44:54,375 main.py:50] epoch 4246, training loss: 16931.46, average training loss: 17720.77, base loss: 23493.28
[INFO 2017-06-25 19:44:58,606 main.py:50] epoch 4247, training loss: 15248.90, average training loss: 17721.27, base loss: 23497.18
[INFO 2017-06-25 19:45:02,878 main.py:50] epoch 4248, training loss: 14724.39, average training loss: 17718.64, base loss: 23493.50
[INFO 2017-06-25 19:45:07,153 main.py:50] epoch 4249, training loss: 19973.64, average training loss: 17723.60, base loss: 23499.26
[INFO 2017-06-25 19:45:11,438 main.py:50] epoch 4250, training loss: 16454.59, average training loss: 17720.84, base loss: 23498.58
[INFO 2017-06-25 19:45:15,719 main.py:50] epoch 4251, training loss: 22257.12, average training loss: 17722.35, base loss: 23501.15
[INFO 2017-06-25 19:45:19,996 main.py:50] epoch 4252, training loss: 18158.50, average training loss: 17722.40, base loss: 23500.95
[INFO 2017-06-25 19:45:24,294 main.py:50] epoch 4253, training loss: 18467.48, average training loss: 17724.82, base loss: 23503.14
[INFO 2017-06-25 19:45:28,553 main.py:50] epoch 4254, training loss: 21734.06, average training loss: 17729.13, base loss: 23508.99
[INFO 2017-06-25 19:45:32,830 main.py:50] epoch 4255, training loss: 19458.23, average training loss: 17733.42, base loss: 23515.33
[INFO 2017-06-25 19:45:37,107 main.py:50] epoch 4256, training loss: 18830.84, average training loss: 17731.81, base loss: 23511.91
[INFO 2017-06-25 19:45:41,392 main.py:50] epoch 4257, training loss: 16698.61, average training loss: 17728.81, base loss: 23508.67
[INFO 2017-06-25 19:45:45,684 main.py:50] epoch 4258, training loss: 16975.63, average training loss: 17727.82, base loss: 23507.19
[INFO 2017-06-25 19:45:49,948 main.py:50] epoch 4259, training loss: 16021.79, average training loss: 17726.63, base loss: 23506.88
[INFO 2017-06-25 19:45:54,280 main.py:50] epoch 4260, training loss: 18901.03, average training loss: 17729.79, base loss: 23507.34
[INFO 2017-06-25 19:45:58,556 main.py:50] epoch 4261, training loss: 16701.84, average training loss: 17729.64, base loss: 23508.20
[INFO 2017-06-25 19:46:02,803 main.py:50] epoch 4262, training loss: 25714.21, average training loss: 17738.91, base loss: 23519.90
[INFO 2017-06-25 19:46:07,084 main.py:50] epoch 4263, training loss: 17226.53, average training loss: 17739.06, base loss: 23522.36
[INFO 2017-06-25 19:46:11,358 main.py:50] epoch 4264, training loss: 21619.74, average training loss: 17743.03, base loss: 23529.40
[INFO 2017-06-25 19:46:15,657 main.py:50] epoch 4265, training loss: 20313.94, average training loss: 17739.37, base loss: 23524.52
[INFO 2017-06-25 19:46:19,894 main.py:50] epoch 4266, training loss: 21093.17, average training loss: 17746.13, base loss: 23534.34
[INFO 2017-06-25 19:46:24,153 main.py:50] epoch 4267, training loss: 15167.26, average training loss: 17746.39, base loss: 23532.89
[INFO 2017-06-25 19:46:28,414 main.py:50] epoch 4268, training loss: 14748.65, average training loss: 17744.43, base loss: 23528.71
[INFO 2017-06-25 19:46:32,663 main.py:50] epoch 4269, training loss: 16976.75, average training loss: 17746.16, base loss: 23530.20
[INFO 2017-06-25 19:46:36,942 main.py:50] epoch 4270, training loss: 20024.58, average training loss: 17750.99, base loss: 23535.42
[INFO 2017-06-25 19:46:41,193 main.py:50] epoch 4271, training loss: 17488.65, average training loss: 17751.29, base loss: 23536.05
[INFO 2017-06-25 19:46:45,475 main.py:50] epoch 4272, training loss: 16818.76, average training loss: 17753.38, base loss: 23537.67
[INFO 2017-06-25 19:46:49,766 main.py:50] epoch 4273, training loss: 17280.14, average training loss: 17749.58, base loss: 23529.80
[INFO 2017-06-25 19:46:54,024 main.py:50] epoch 4274, training loss: 16864.79, average training loss: 17747.63, base loss: 23526.62
[INFO 2017-06-25 19:46:58,271 main.py:50] epoch 4275, training loss: 15910.77, average training loss: 17745.38, base loss: 23528.44
[INFO 2017-06-25 19:47:02,559 main.py:50] epoch 4276, training loss: 15364.91, average training loss: 17743.10, base loss: 23526.16
[INFO 2017-06-25 19:47:06,870 main.py:50] epoch 4277, training loss: 19515.25, average training loss: 17748.19, base loss: 23534.61
[INFO 2017-06-25 19:47:11,145 main.py:50] epoch 4278, training loss: 15897.49, average training loss: 17745.65, base loss: 23531.41
[INFO 2017-06-25 19:47:15,411 main.py:50] epoch 4279, training loss: 20999.13, average training loss: 17750.71, base loss: 23539.61
[INFO 2017-06-25 19:47:19,695 main.py:50] epoch 4280, training loss: 20811.27, average training loss: 17755.68, base loss: 23545.50
[INFO 2017-06-25 19:47:23,957 main.py:50] epoch 4281, training loss: 15046.58, average training loss: 17751.27, base loss: 23539.17
[INFO 2017-06-25 19:47:28,219 main.py:50] epoch 4282, training loss: 12979.88, average training loss: 17749.61, base loss: 23538.99
[INFO 2017-06-25 19:47:32,479 main.py:50] epoch 4283, training loss: 19316.68, average training loss: 17750.18, base loss: 23540.48
[INFO 2017-06-25 19:47:36,753 main.py:50] epoch 4284, training loss: 15179.86, average training loss: 17749.39, base loss: 23541.89
[INFO 2017-06-25 19:47:41,004 main.py:50] epoch 4285, training loss: 18950.46, average training loss: 17747.66, base loss: 23539.12
[INFO 2017-06-25 19:47:45,266 main.py:50] epoch 4286, training loss: 20222.42, average training loss: 17749.21, base loss: 23540.91
[INFO 2017-06-25 19:47:49,562 main.py:50] epoch 4287, training loss: 19470.64, average training loss: 17753.74, base loss: 23546.14
[INFO 2017-06-25 19:47:53,817 main.py:50] epoch 4288, training loss: 14791.70, average training loss: 17753.00, base loss: 23543.40
[INFO 2017-06-25 19:47:58,083 main.py:50] epoch 4289, training loss: 15391.88, average training loss: 17749.38, base loss: 23538.00
[INFO 2017-06-25 19:48:02,328 main.py:50] epoch 4290, training loss: 17396.67, average training loss: 17748.84, base loss: 23537.73
[INFO 2017-06-25 19:48:06,612 main.py:50] epoch 4291, training loss: 16756.08, average training loss: 17754.28, base loss: 23544.73
[INFO 2017-06-25 19:48:10,902 main.py:50] epoch 4292, training loss: 15597.31, average training loss: 17752.49, base loss: 23545.99
[INFO 2017-06-25 19:48:15,164 main.py:50] epoch 4293, training loss: 19543.32, average training loss: 17753.80, base loss: 23544.45
[INFO 2017-06-25 19:48:19,430 main.py:50] epoch 4294, training loss: 16597.04, average training loss: 17754.95, base loss: 23546.99
[INFO 2017-06-25 19:48:23,691 main.py:50] epoch 4295, training loss: 16413.20, average training loss: 17751.10, base loss: 23542.11
[INFO 2017-06-25 19:48:27,974 main.py:50] epoch 4296, training loss: 15220.51, average training loss: 17751.11, base loss: 23541.80
[INFO 2017-06-25 19:48:32,222 main.py:50] epoch 4297, training loss: 14932.65, average training loss: 17748.00, base loss: 23536.86
[INFO 2017-06-25 19:48:36,507 main.py:50] epoch 4298, training loss: 17334.75, average training loss: 17749.65, base loss: 23539.04
[INFO 2017-06-25 19:48:40,766 main.py:50] epoch 4299, training loss: 20239.07, average training loss: 17749.31, base loss: 23537.84
[INFO 2017-06-25 19:48:45,050 main.py:50] epoch 4300, training loss: 24143.95, average training loss: 17756.57, base loss: 23553.11
[INFO 2017-06-25 19:48:49,339 main.py:50] epoch 4301, training loss: 21091.85, average training loss: 17759.53, base loss: 23557.16
[INFO 2017-06-25 19:48:53,610 main.py:50] epoch 4302, training loss: 16462.47, average training loss: 17761.67, base loss: 23558.45
[INFO 2017-06-25 19:48:57,862 main.py:50] epoch 4303, training loss: 17252.61, average training loss: 17762.77, base loss: 23559.39
[INFO 2017-06-25 19:49:02,138 main.py:50] epoch 4304, training loss: 17381.79, average training loss: 17762.78, base loss: 23557.56
[INFO 2017-06-25 19:49:06,435 main.py:50] epoch 4305, training loss: 16722.86, average training loss: 17762.71, base loss: 23557.15
[INFO 2017-06-25 19:49:10,714 main.py:50] epoch 4306, training loss: 17194.81, average training loss: 17768.30, base loss: 23563.91
[INFO 2017-06-25 19:49:15,002 main.py:50] epoch 4307, training loss: 20991.30, average training loss: 17769.56, base loss: 23565.70
[INFO 2017-06-25 19:49:19,274 main.py:50] epoch 4308, training loss: 16556.50, average training loss: 17771.71, base loss: 23569.60
[INFO 2017-06-25 19:49:23,547 main.py:50] epoch 4309, training loss: 18191.57, average training loss: 17777.36, base loss: 23578.68
[INFO 2017-06-25 19:49:27,817 main.py:50] epoch 4310, training loss: 19642.19, average training loss: 17778.33, base loss: 23578.23
[INFO 2017-06-25 19:49:32,076 main.py:50] epoch 4311, training loss: 14998.79, average training loss: 17778.95, base loss: 23578.56
[INFO 2017-06-25 19:49:36,350 main.py:50] epoch 4312, training loss: 18047.11, average training loss: 17776.73, base loss: 23575.02
[INFO 2017-06-25 19:49:40,625 main.py:50] epoch 4313, training loss: 18312.04, average training loss: 17777.22, base loss: 23576.42
[INFO 2017-06-25 19:49:44,893 main.py:50] epoch 4314, training loss: 17774.47, average training loss: 17778.30, base loss: 23578.57
[INFO 2017-06-25 19:49:49,148 main.py:50] epoch 4315, training loss: 21052.89, average training loss: 17781.10, base loss: 23581.25
[INFO 2017-06-25 19:49:53,439 main.py:50] epoch 4316, training loss: 17571.80, average training loss: 17785.25, base loss: 23588.43
[INFO 2017-06-25 19:49:57,693 main.py:50] epoch 4317, training loss: 14075.82, average training loss: 17778.46, base loss: 23581.76
[INFO 2017-06-25 19:50:01,979 main.py:50] epoch 4318, training loss: 16533.54, average training loss: 17776.36, base loss: 23579.15
[INFO 2017-06-25 19:50:06,256 main.py:50] epoch 4319, training loss: 19435.58, average training loss: 17772.98, base loss: 23577.91
[INFO 2017-06-25 19:50:10,556 main.py:50] epoch 4320, training loss: 17157.94, average training loss: 17771.52, base loss: 23576.77
[INFO 2017-06-25 19:50:15,048 main.py:50] epoch 4321, training loss: 14624.08, average training loss: 17765.49, base loss: 23565.93
[INFO 2017-06-25 19:50:20,238 main.py:50] epoch 4322, training loss: 15203.16, average training loss: 17757.94, base loss: 23559.75
[INFO 2017-06-25 19:50:24,498 main.py:50] epoch 4323, training loss: 25238.50, average training loss: 17764.18, base loss: 23562.77
[INFO 2017-06-25 19:50:28,729 main.py:50] epoch 4324, training loss: 17760.63, average training loss: 17765.07, base loss: 23563.32
[INFO 2017-06-25 19:50:32,987 main.py:50] epoch 4325, training loss: 17626.20, average training loss: 17765.12, base loss: 23560.24
[INFO 2017-06-25 19:50:37,284 main.py:50] epoch 4326, training loss: 15612.04, average training loss: 17759.36, base loss: 23555.83
[INFO 2017-06-25 19:50:41,542 main.py:50] epoch 4327, training loss: 23176.11, average training loss: 17766.89, base loss: 23569.62
[INFO 2017-06-25 19:50:45,810 main.py:50] epoch 4328, training loss: 17476.26, average training loss: 17765.03, base loss: 23568.42
[INFO 2017-06-25 19:50:50,080 main.py:50] epoch 4329, training loss: 19441.25, average training loss: 17767.56, base loss: 23569.62
[INFO 2017-06-25 19:50:54,356 main.py:50] epoch 4330, training loss: 16904.44, average training loss: 17766.73, base loss: 23570.06
[INFO 2017-06-25 19:50:58,639 main.py:50] epoch 4331, training loss: 15801.28, average training loss: 17765.88, base loss: 23569.32
[INFO 2017-06-25 19:51:03,017 main.py:50] epoch 4332, training loss: 22519.66, average training loss: 17767.49, base loss: 23570.48
[INFO 2017-06-25 19:51:07,609 main.py:50] epoch 4333, training loss: 18515.70, average training loss: 17768.93, base loss: 23574.91
[INFO 2017-06-25 19:51:12,239 main.py:50] epoch 4334, training loss: 17430.82, average training loss: 17769.61, base loss: 23575.18
[INFO 2017-06-25 19:51:16,818 main.py:50] epoch 4335, training loss: 17772.24, average training loss: 17771.21, base loss: 23577.24
[INFO 2017-06-25 19:51:21,266 main.py:50] epoch 4336, training loss: 20281.56, average training loss: 17772.77, base loss: 23578.84
[INFO 2017-06-25 19:51:25,537 main.py:50] epoch 4337, training loss: 22492.99, average training loss: 17779.47, base loss: 23589.84
[INFO 2017-06-25 19:51:29,817 main.py:50] epoch 4338, training loss: 18017.09, average training loss: 17781.85, base loss: 23593.04
[INFO 2017-06-25 19:51:34,087 main.py:50] epoch 4339, training loss: 19683.28, average training loss: 17778.50, base loss: 23589.33
[INFO 2017-06-25 19:51:38,351 main.py:50] epoch 4340, training loss: 17974.55, average training loss: 17772.39, base loss: 23577.53
[INFO 2017-06-25 19:51:42,643 main.py:50] epoch 4341, training loss: 17834.95, average training loss: 17775.20, base loss: 23582.56
[INFO 2017-06-25 19:51:47,029 main.py:50] epoch 4342, training loss: 19594.79, average training loss: 17776.23, base loss: 23581.87
[INFO 2017-06-25 19:51:52,802 main.py:50] epoch 4343, training loss: 18771.61, average training loss: 17776.82, base loss: 23586.14
[INFO 2017-06-25 19:51:57,069 main.py:50] epoch 4344, training loss: 20732.87, average training loss: 17781.06, base loss: 23591.10
[INFO 2017-06-25 19:52:01,938 main.py:50] epoch 4345, training loss: 19036.08, average training loss: 17785.40, base loss: 23595.74
[INFO 2017-06-25 19:52:06,502 main.py:50] epoch 4346, training loss: 16947.49, average training loss: 17785.46, base loss: 23598.13
[INFO 2017-06-25 19:52:12,330 main.py:50] epoch 4347, training loss: 17613.43, average training loss: 17784.86, base loss: 23596.31
[INFO 2017-06-25 19:52:16,749 main.py:50] epoch 4348, training loss: 18225.13, average training loss: 17782.05, base loss: 23596.52
[INFO 2017-06-25 19:52:22,510 main.py:50] epoch 4349, training loss: 14848.02, average training loss: 17780.43, base loss: 23594.85
[INFO 2017-06-25 19:52:26,758 main.py:50] epoch 4350, training loss: 15684.88, average training loss: 17779.61, base loss: 23589.95
[INFO 2017-06-25 19:52:31,031 main.py:50] epoch 4351, training loss: 15166.61, average training loss: 17775.97, base loss: 23584.30
[INFO 2017-06-25 19:52:35,304 main.py:50] epoch 4352, training loss: 14510.49, average training loss: 17775.19, base loss: 23583.50
[INFO 2017-06-25 19:52:39,564 main.py:50] epoch 4353, training loss: 15683.02, average training loss: 17772.50, base loss: 23582.21
[INFO 2017-06-25 19:52:43,815 main.py:50] epoch 4354, training loss: 15332.48, average training loss: 17773.19, base loss: 23582.25
[INFO 2017-06-25 19:52:48,103 main.py:50] epoch 4355, training loss: 17367.47, average training loss: 17773.98, base loss: 23584.98
[INFO 2017-06-25 19:52:52,386 main.py:50] epoch 4356, training loss: 14858.48, average training loss: 17769.08, base loss: 23580.81
[INFO 2017-06-25 19:52:56,655 main.py:50] epoch 4357, training loss: 25091.50, average training loss: 17772.31, base loss: 23582.97
[INFO 2017-06-25 19:53:00,927 main.py:50] epoch 4358, training loss: 17160.14, average training loss: 17771.78, base loss: 23586.68
[INFO 2017-06-25 19:53:05,194 main.py:50] epoch 4359, training loss: 19793.65, average training loss: 17778.38, base loss: 23595.42
[INFO 2017-06-25 19:53:09,450 main.py:50] epoch 4360, training loss: 19616.21, average training loss: 17784.11, base loss: 23602.71
[INFO 2017-06-25 19:53:13,732 main.py:50] epoch 4361, training loss: 17687.78, average training loss: 17780.97, base loss: 23602.41
[INFO 2017-06-25 19:53:17,985 main.py:50] epoch 4362, training loss: 20385.10, average training loss: 17784.58, base loss: 23604.70
[INFO 2017-06-25 19:53:22,244 main.py:50] epoch 4363, training loss: 16899.07, average training loss: 17786.14, base loss: 23607.63
[INFO 2017-06-25 19:53:26,480 main.py:50] epoch 4364, training loss: 16599.41, average training loss: 17785.36, base loss: 23608.26
[INFO 2017-06-25 19:53:30,752 main.py:50] epoch 4365, training loss: 22575.19, average training loss: 17788.38, base loss: 23610.78
[INFO 2017-06-25 19:53:35,006 main.py:50] epoch 4366, training loss: 16583.22, average training loss: 17786.98, base loss: 23611.24
[INFO 2017-06-25 19:53:39,263 main.py:50] epoch 4367, training loss: 17801.73, average training loss: 17784.79, base loss: 23607.11
[INFO 2017-06-25 19:53:43,520 main.py:50] epoch 4368, training loss: 19342.39, average training loss: 17792.22, base loss: 23620.94
[INFO 2017-06-25 19:53:47,773 main.py:50] epoch 4369, training loss: 17222.29, average training loss: 17791.69, base loss: 23618.53
[INFO 2017-06-25 19:53:52,047 main.py:50] epoch 4370, training loss: 19927.14, average training loss: 17793.20, base loss: 23619.85
[INFO 2017-06-25 19:53:56,342 main.py:50] epoch 4371, training loss: 15906.07, average training loss: 17792.97, base loss: 23618.39
[INFO 2017-06-25 19:54:00,626 main.py:50] epoch 4372, training loss: 13743.21, average training loss: 17786.23, base loss: 23611.38
[INFO 2017-06-25 19:54:04,883 main.py:50] epoch 4373, training loss: 17069.97, average training loss: 17784.43, base loss: 23609.23
[INFO 2017-06-25 19:54:09,153 main.py:50] epoch 4374, training loss: 19431.40, average training loss: 17783.10, base loss: 23605.30
[INFO 2017-06-25 19:54:13,390 main.py:50] epoch 4375, training loss: 18307.40, average training loss: 17784.69, base loss: 23608.18
[INFO 2017-06-25 19:54:17,663 main.py:50] epoch 4376, training loss: 18438.73, average training loss: 17787.94, base loss: 23612.44
[INFO 2017-06-25 19:54:21,951 main.py:50] epoch 4377, training loss: 20482.65, average training loss: 17789.64, base loss: 23614.04
[INFO 2017-06-25 19:54:26,305 main.py:50] epoch 4378, training loss: 18824.06, average training loss: 17788.19, base loss: 23610.76
[INFO 2017-06-25 19:54:32,070 main.py:50] epoch 4379, training loss: 18557.84, average training loss: 17789.49, base loss: 23611.91
[INFO 2017-06-25 19:54:36,385 main.py:50] epoch 4380, training loss: 15312.52, average training loss: 17788.77, base loss: 23612.73
[INFO 2017-06-25 19:54:40,694 main.py:50] epoch 4381, training loss: 12459.20, average training loss: 17782.86, base loss: 23604.13
[INFO 2017-06-25 19:54:44,953 main.py:50] epoch 4382, training loss: 20714.96, average training loss: 17786.76, base loss: 23609.95
[INFO 2017-06-25 19:54:49,221 main.py:50] epoch 4383, training loss: 20828.56, average training loss: 17789.17, base loss: 23612.97
[INFO 2017-06-25 19:54:53,493 main.py:50] epoch 4384, training loss: 17032.34, average training loss: 17790.83, base loss: 23615.19
[INFO 2017-06-25 19:54:57,743 main.py:50] epoch 4385, training loss: 17592.21, average training loss: 17788.48, base loss: 23610.62
[INFO 2017-06-25 19:55:02,034 main.py:50] epoch 4386, training loss: 19954.57, average training loss: 17790.55, base loss: 23610.95
[INFO 2017-06-25 19:55:06,331 main.py:50] epoch 4387, training loss: 18792.31, average training loss: 17789.92, base loss: 23609.03
[INFO 2017-06-25 19:55:10,622 main.py:50] epoch 4388, training loss: 17077.06, average training loss: 17788.25, base loss: 23605.97
[INFO 2017-06-25 19:55:14,886 main.py:50] epoch 4389, training loss: 17873.52, average training loss: 17792.43, base loss: 23609.38
[INFO 2017-06-25 19:55:19,166 main.py:50] epoch 4390, training loss: 17691.04, average training loss: 17793.98, base loss: 23609.93
[INFO 2017-06-25 19:55:23,440 main.py:50] epoch 4391, training loss: 20158.83, average training loss: 17793.27, base loss: 23609.83
[INFO 2017-06-25 19:55:27,981 main.py:50] epoch 4392, training loss: 14871.84, average training loss: 17792.89, base loss: 23610.66
[INFO 2017-06-25 19:55:32,849 main.py:50] epoch 4393, training loss: 16897.08, average training loss: 17787.72, base loss: 23608.02
[INFO 2017-06-25 19:55:37,114 main.py:50] epoch 4394, training loss: 12222.55, average training loss: 17779.51, base loss: 23596.50
[INFO 2017-06-25 19:55:41,403 main.py:50] epoch 4395, training loss: 16970.77, average training loss: 17775.77, base loss: 23590.18
[INFO 2017-06-25 19:55:45,651 main.py:50] epoch 4396, training loss: 16153.46, average training loss: 17773.95, base loss: 23589.77
[INFO 2017-06-25 19:55:49,932 main.py:50] epoch 4397, training loss: 14704.15, average training loss: 17772.59, base loss: 23588.34
[INFO 2017-06-25 19:55:54,264 main.py:50] epoch 4398, training loss: 17614.01, average training loss: 17770.65, base loss: 23583.35
[INFO 2017-06-25 19:55:58,549 main.py:50] epoch 4399, training loss: 16523.41, average training loss: 17766.91, base loss: 23578.09
[INFO 2017-06-25 19:56:02,824 main.py:50] epoch 4400, training loss: 16038.92, average training loss: 17764.15, base loss: 23575.79
[INFO 2017-06-25 19:56:07,204 main.py:50] epoch 4401, training loss: 20779.37, average training loss: 17767.19, base loss: 23583.62
[INFO 2017-06-25 19:56:11,498 main.py:50] epoch 4402, training loss: 16707.90, average training loss: 17763.33, base loss: 23578.06
[INFO 2017-06-25 19:56:15,777 main.py:50] epoch 4403, training loss: 16354.46, average training loss: 17764.82, base loss: 23581.70
[INFO 2017-06-25 19:56:20,055 main.py:50] epoch 4404, training loss: 15874.99, average training loss: 17764.61, base loss: 23583.08
[INFO 2017-06-25 19:56:24,328 main.py:50] epoch 4405, training loss: 17899.32, average training loss: 17765.93, base loss: 23584.71
[INFO 2017-06-25 19:56:28,585 main.py:50] epoch 4406, training loss: 19570.07, average training loss: 17767.67, base loss: 23589.33
[INFO 2017-06-25 19:56:32,995 main.py:50] epoch 4407, training loss: 16451.23, average training loss: 17764.20, base loss: 23584.89
[INFO 2017-06-25 19:56:38,285 main.py:50] epoch 4408, training loss: 15851.61, average training loss: 17762.48, base loss: 23581.80
[INFO 2017-06-25 19:56:42,540 main.py:50] epoch 4409, training loss: 14742.91, average training loss: 17756.76, base loss: 23576.30
[INFO 2017-06-25 19:56:46,805 main.py:50] epoch 4410, training loss: 20096.45, average training loss: 17758.51, base loss: 23576.96
[INFO 2017-06-25 19:56:51,065 main.py:50] epoch 4411, training loss: 16747.92, average training loss: 17758.43, base loss: 23577.16
[INFO 2017-06-25 19:56:55,314 main.py:50] epoch 4412, training loss: 15263.38, average training loss: 17754.20, base loss: 23569.19
[INFO 2017-06-25 19:56:59,592 main.py:50] epoch 4413, training loss: 20440.44, average training loss: 17755.48, base loss: 23574.02
[INFO 2017-06-25 19:57:03,849 main.py:50] epoch 4414, training loss: 15367.63, average training loss: 17752.57, base loss: 23570.47
[INFO 2017-06-25 19:57:08,122 main.py:50] epoch 4415, training loss: 14004.55, average training loss: 17752.52, base loss: 23571.51
[INFO 2017-06-25 19:57:12,375 main.py:50] epoch 4416, training loss: 20178.83, average training loss: 17753.50, base loss: 23570.23
[INFO 2017-06-25 19:57:16,630 main.py:50] epoch 4417, training loss: 14415.29, average training loss: 17749.73, base loss: 23568.35
[INFO 2017-06-25 19:57:20,906 main.py:50] epoch 4418, training loss: 17077.81, average training loss: 17748.39, base loss: 23567.80
[INFO 2017-06-25 19:57:25,163 main.py:50] epoch 4419, training loss: 12166.27, average training loss: 17742.01, base loss: 23560.17
[INFO 2017-06-25 19:57:29,414 main.py:50] epoch 4420, training loss: 18158.57, average training loss: 17741.06, base loss: 23560.27
[INFO 2017-06-25 19:57:33,681 main.py:50] epoch 4421, training loss: 13374.66, average training loss: 17738.11, base loss: 23557.91
[INFO 2017-06-25 19:57:37,955 main.py:50] epoch 4422, training loss: 17697.52, average training loss: 17740.47, base loss: 23561.14
[INFO 2017-06-25 19:57:42,242 main.py:50] epoch 4423, training loss: 20478.02, average training loss: 17740.47, base loss: 23559.84
[INFO 2017-06-25 19:57:46,509 main.py:50] epoch 4424, training loss: 14100.56, average training loss: 17732.88, base loss: 23549.07
[INFO 2017-06-25 19:57:50,782 main.py:50] epoch 4425, training loss: 17609.16, average training loss: 17728.30, base loss: 23541.11
[INFO 2017-06-25 19:57:55,054 main.py:50] epoch 4426, training loss: 18021.54, average training loss: 17724.50, base loss: 23539.10
[INFO 2017-06-25 19:57:59,391 main.py:50] epoch 4427, training loss: 16520.21, average training loss: 17725.53, base loss: 23541.22
[INFO 2017-06-25 19:58:05,176 main.py:50] epoch 4428, training loss: 19084.01, average training loss: 17726.61, base loss: 23542.67
[INFO 2017-06-25 19:58:09,437 main.py:50] epoch 4429, training loss: 18405.55, average training loss: 17729.78, base loss: 23546.55
[INFO 2017-06-25 19:58:13,703 main.py:50] epoch 4430, training loss: 25614.26, average training loss: 17737.62, base loss: 23556.47
[INFO 2017-06-25 19:58:17,958 main.py:50] epoch 4431, training loss: 17107.17, average training loss: 17733.74, base loss: 23548.91
[INFO 2017-06-25 19:58:22,200 main.py:50] epoch 4432, training loss: 17377.28, average training loss: 17731.26, base loss: 23543.19
[INFO 2017-06-25 19:58:26,465 main.py:50] epoch 4433, training loss: 21410.34, average training loss: 17735.20, base loss: 23544.05
[INFO 2017-06-25 19:58:30,715 main.py:50] epoch 4434, training loss: 20726.00, average training loss: 17736.18, base loss: 23546.51
[INFO 2017-06-25 19:58:34,964 main.py:50] epoch 4435, training loss: 19278.87, average training loss: 17738.56, base loss: 23548.38
[INFO 2017-06-25 19:58:39,219 main.py:50] epoch 4436, training loss: 13306.81, average training loss: 17734.59, base loss: 23542.90
[INFO 2017-06-25 19:58:43,507 main.py:50] epoch 4437, training loss: 17262.88, average training loss: 17733.52, base loss: 23545.06
[INFO 2017-06-25 19:58:47,791 main.py:50] epoch 4438, training loss: 19486.13, average training loss: 17739.58, base loss: 23554.59
[INFO 2017-06-25 19:58:52,070 main.py:50] epoch 4439, training loss: 19669.44, average training loss: 17744.32, base loss: 23559.59
[INFO 2017-06-25 19:58:56,327 main.py:50] epoch 4440, training loss: 14538.37, average training loss: 17741.13, base loss: 23554.92
[INFO 2017-06-25 19:59:00,610 main.py:50] epoch 4441, training loss: 13062.27, average training loss: 17738.44, base loss: 23549.52
[INFO 2017-06-25 19:59:04,891 main.py:50] epoch 4442, training loss: 17813.19, average training loss: 17741.60, base loss: 23556.17
[INFO 2017-06-25 19:59:09,178 main.py:50] epoch 4443, training loss: 25405.22, average training loss: 17745.48, base loss: 23560.78
[INFO 2017-06-25 19:59:13,437 main.py:50] epoch 4444, training loss: 14087.84, average training loss: 17743.64, base loss: 23556.41
[INFO 2017-06-25 19:59:17,705 main.py:50] epoch 4445, training loss: 17579.89, average training loss: 17743.23, base loss: 23553.27
[INFO 2017-06-25 19:59:21,968 main.py:50] epoch 4446, training loss: 16424.89, average training loss: 17738.92, base loss: 23544.70
[INFO 2017-06-25 19:59:26,260 main.py:50] epoch 4447, training loss: 17768.13, average training loss: 17736.64, base loss: 23540.89
[INFO 2017-06-25 19:59:30,517 main.py:50] epoch 4448, training loss: 17796.81, average training loss: 17743.81, base loss: 23549.49
[INFO 2017-06-25 19:59:34,801 main.py:50] epoch 4449, training loss: 17136.38, average training loss: 17741.71, base loss: 23545.44
[INFO 2017-06-25 19:59:39,056 main.py:50] epoch 4450, training loss: 15244.99, average training loss: 17738.45, base loss: 23540.96
[INFO 2017-06-25 19:59:43,425 main.py:50] epoch 4451, training loss: 21314.71, average training loss: 17740.24, base loss: 23541.73
[INFO 2017-06-25 19:59:49,195 main.py:50] epoch 4452, training loss: 20556.41, average training loss: 17741.55, base loss: 23543.20
[INFO 2017-06-25 19:59:53,464 main.py:50] epoch 4453, training loss: 21667.50, average training loss: 17743.22, base loss: 23548.24
[INFO 2017-06-25 19:59:57,740 main.py:50] epoch 4454, training loss: 21835.04, average training loss: 17745.76, base loss: 23553.50
[INFO 2017-06-25 20:00:02,002 main.py:50] epoch 4455, training loss: 19543.87, average training loss: 17748.21, base loss: 23557.35
[INFO 2017-06-25 20:00:06,265 main.py:50] epoch 4456, training loss: 22442.30, average training loss: 17750.53, base loss: 23560.71
[INFO 2017-06-25 20:00:10,545 main.py:50] epoch 4457, training loss: 18925.48, average training loss: 17751.25, base loss: 23564.49
[INFO 2017-06-25 20:00:14,829 main.py:50] epoch 4458, training loss: 21730.75, average training loss: 17754.36, base loss: 23564.72
[INFO 2017-06-25 20:00:19,093 main.py:50] epoch 4459, training loss: 18126.27, average training loss: 17749.53, base loss: 23556.86
[INFO 2017-06-25 20:00:23,364 main.py:50] epoch 4460, training loss: 15541.78, average training loss: 17747.09, base loss: 23555.02
[INFO 2017-06-25 20:00:27,651 main.py:50] epoch 4461, training loss: 15823.41, average training loss: 17747.01, base loss: 23555.64
[INFO 2017-06-25 20:00:31,945 main.py:50] epoch 4462, training loss: 24534.14, average training loss: 17758.04, base loss: 23566.88
[INFO 2017-06-25 20:00:36,211 main.py:50] epoch 4463, training loss: 17442.03, average training loss: 17754.55, base loss: 23561.29
[INFO 2017-06-25 20:00:40,459 main.py:50] epoch 4464, training loss: 19342.75, average training loss: 17758.29, base loss: 23566.69
[INFO 2017-06-25 20:00:44,733 main.py:50] epoch 4465, training loss: 17678.21, average training loss: 17755.07, base loss: 23565.19
[INFO 2017-06-25 20:00:48,992 main.py:50] epoch 4466, training loss: 15204.77, average training loss: 17746.04, base loss: 23557.12
[INFO 2017-06-25 20:00:53,276 main.py:50] epoch 4467, training loss: 17620.86, average training loss: 17746.75, base loss: 23558.96
[INFO 2017-06-25 20:00:57,541 main.py:50] epoch 4468, training loss: 15612.13, average training loss: 17747.82, base loss: 23560.44
[INFO 2017-06-25 20:01:01,808 main.py:50] epoch 4469, training loss: 14775.31, average training loss: 17748.98, base loss: 23564.22
[INFO 2017-06-25 20:01:06,081 main.py:50] epoch 4470, training loss: 17086.25, average training loss: 17746.12, base loss: 23561.19
[INFO 2017-06-25 20:01:10,332 main.py:50] epoch 4471, training loss: 21409.73, average training loss: 17751.03, base loss: 23568.51
[INFO 2017-06-25 20:01:14,589 main.py:50] epoch 4472, training loss: 15948.86, average training loss: 17748.07, base loss: 23565.69
[INFO 2017-06-25 20:01:18,853 main.py:50] epoch 4473, training loss: 16122.16, average training loss: 17749.72, base loss: 23566.99
[INFO 2017-06-25 20:01:23,109 main.py:50] epoch 4474, training loss: 17803.16, average training loss: 17747.33, base loss: 23563.75
[INFO 2017-06-25 20:01:27,379 main.py:50] epoch 4475, training loss: 15629.75, average training loss: 17742.40, base loss: 23558.01
[INFO 2017-06-25 20:01:31,655 main.py:50] epoch 4476, training loss: 16381.56, average training loss: 17740.29, base loss: 23554.56
[INFO 2017-06-25 20:01:35,910 main.py:50] epoch 4477, training loss: 11916.80, average training loss: 17733.34, base loss: 23545.98
[INFO 2017-06-25 20:01:40,353 main.py:50] epoch 4478, training loss: 18128.36, average training loss: 17735.76, base loss: 23547.81
[INFO 2017-06-25 20:01:44,664 main.py:50] epoch 4479, training loss: 19765.72, average training loss: 17739.80, base loss: 23552.92
[INFO 2017-06-25 20:01:48,935 main.py:50] epoch 4480, training loss: 16442.46, average training loss: 17736.05, base loss: 23550.58
[INFO 2017-06-25 20:01:53,194 main.py:50] epoch 4481, training loss: 17484.15, average training loss: 17738.13, base loss: 23554.40
[INFO 2017-06-25 20:01:57,470 main.py:50] epoch 4482, training loss: 15740.95, average training loss: 17737.98, base loss: 23555.69
[INFO 2017-06-25 20:02:01,748 main.py:50] epoch 4483, training loss: 18633.98, average training loss: 17737.35, base loss: 23554.51
[INFO 2017-06-25 20:02:06,008 main.py:50] epoch 4484, training loss: 17785.94, average training loss: 17742.36, base loss: 23564.42
[INFO 2017-06-25 20:02:10,285 main.py:50] epoch 4485, training loss: 20573.58, average training loss: 17745.44, base loss: 23571.19
[INFO 2017-06-25 20:02:14,572 main.py:50] epoch 4486, training loss: 15113.03, average training loss: 17743.49, base loss: 23571.88
[INFO 2017-06-25 20:02:18,825 main.py:50] epoch 4487, training loss: 20039.48, average training loss: 17744.76, base loss: 23577.25
[INFO 2017-06-25 20:02:23,082 main.py:50] epoch 4488, training loss: 18811.83, average training loss: 17743.74, base loss: 23578.53
[INFO 2017-06-25 20:02:27,341 main.py:50] epoch 4489, training loss: 17421.47, average training loss: 17737.84, base loss: 23573.35
[INFO 2017-06-25 20:02:31,614 main.py:50] epoch 4490, training loss: 15396.90, average training loss: 17738.65, base loss: 23572.56
[INFO 2017-06-25 20:02:35,900 main.py:50] epoch 4491, training loss: 16911.69, average training loss: 17734.88, base loss: 23568.17
[INFO 2017-06-25 20:02:40,162 main.py:50] epoch 4492, training loss: 15938.95, average training loss: 17735.33, base loss: 23568.94
[INFO 2017-06-25 20:02:44,467 main.py:50] epoch 4493, training loss: 15717.69, average training loss: 17732.12, base loss: 23564.86
[INFO 2017-06-25 20:02:48,759 main.py:50] epoch 4494, training loss: 16083.95, average training loss: 17732.64, base loss: 23566.05
[INFO 2017-06-25 20:02:53,019 main.py:50] epoch 4495, training loss: 18416.43, average training loss: 17733.68, base loss: 23568.94
[INFO 2017-06-25 20:02:57,306 main.py:50] epoch 4496, training loss: 18851.21, average training loss: 17737.59, base loss: 23577.21
[INFO 2017-06-25 20:03:01,581 main.py:50] epoch 4497, training loss: 18895.33, average training loss: 17736.14, base loss: 23579.09
[INFO 2017-06-25 20:03:05,851 main.py:50] epoch 4498, training loss: 21880.46, average training loss: 17734.70, base loss: 23575.67
[INFO 2017-06-25 20:03:10,124 main.py:50] epoch 4499, training loss: 12836.11, average training loss: 17729.17, base loss: 23567.90
[INFO 2017-06-25 20:03:14,394 main.py:50] epoch 4500, training loss: 15704.72, average training loss: 17730.51, base loss: 23569.58
[INFO 2017-06-25 20:03:18,647 main.py:50] epoch 4501, training loss: 15784.90, average training loss: 17726.58, base loss: 23564.94
[INFO 2017-06-25 20:03:22,921 main.py:50] epoch 4502, training loss: 19495.81, average training loss: 17726.61, base loss: 23568.16
[INFO 2017-06-25 20:03:27,193 main.py:50] epoch 4503, training loss: 16897.85, average training loss: 17723.54, base loss: 23563.92
[INFO 2017-06-25 20:03:31,475 main.py:50] epoch 4504, training loss: 18571.07, average training loss: 17725.50, base loss: 23568.16
[INFO 2017-06-25 20:03:35,725 main.py:50] epoch 4505, training loss: 20396.09, average training loss: 17730.92, base loss: 23575.90
[INFO 2017-06-25 20:03:39,978 main.py:50] epoch 4506, training loss: 23481.08, average training loss: 17735.42, base loss: 23581.97
[INFO 2017-06-25 20:03:44,261 main.py:50] epoch 4507, training loss: 21402.81, average training loss: 17739.96, base loss: 23587.81
[INFO 2017-06-25 20:03:48,526 main.py:50] epoch 4508, training loss: 14424.89, average training loss: 17737.43, base loss: 23587.00
[INFO 2017-06-25 20:03:52,804 main.py:50] epoch 4509, training loss: 18907.47, average training loss: 17736.17, base loss: 23589.60
[INFO 2017-06-25 20:03:57,099 main.py:50] epoch 4510, training loss: 16636.25, average training loss: 17738.53, base loss: 23595.36
[INFO 2017-06-25 20:04:01,366 main.py:50] epoch 4511, training loss: 25860.20, average training loss: 17743.69, base loss: 23600.30
[INFO 2017-06-25 20:04:05,639 main.py:50] epoch 4512, training loss: 16439.10, average training loss: 17744.11, base loss: 23603.66
[INFO 2017-06-25 20:04:09,921 main.py:50] epoch 4513, training loss: 17525.08, average training loss: 17743.26, base loss: 23604.31
[INFO 2017-06-25 20:04:14,214 main.py:50] epoch 4514, training loss: 21618.39, average training loss: 17750.21, base loss: 23615.60
[INFO 2017-06-25 20:04:18,480 main.py:50] epoch 4515, training loss: 18177.90, average training loss: 17748.55, base loss: 23616.21
[INFO 2017-06-25 20:04:22,769 main.py:50] epoch 4516, training loss: 13731.45, average training loss: 17742.66, base loss: 23608.57
[INFO 2017-06-25 20:04:27,048 main.py:50] epoch 4517, training loss: 18054.15, average training loss: 17742.50, base loss: 23610.14
[INFO 2017-06-25 20:04:31,315 main.py:50] epoch 4518, training loss: 19048.55, average training loss: 17745.19, base loss: 23613.52
[INFO 2017-06-25 20:04:35,580 main.py:50] epoch 4519, training loss: 14282.96, average training loss: 17739.92, base loss: 23604.20
[INFO 2017-06-25 20:04:39,856 main.py:50] epoch 4520, training loss: 19693.89, average training loss: 17741.36, base loss: 23605.24
[INFO 2017-06-25 20:04:44,153 main.py:50] epoch 4521, training loss: 13719.04, average training loss: 17736.87, base loss: 23600.81
[INFO 2017-06-25 20:04:48,416 main.py:50] epoch 4522, training loss: 14443.12, average training loss: 17735.53, base loss: 23597.89
[INFO 2017-06-25 20:04:52,693 main.py:50] epoch 4523, training loss: 17539.86, average training loss: 17736.01, base loss: 23599.29
[INFO 2017-06-25 20:04:56,951 main.py:50] epoch 4524, training loss: 20988.68, average training loss: 17741.56, base loss: 23605.89
[INFO 2017-06-25 20:05:01,229 main.py:50] epoch 4525, training loss: 17238.72, average training loss: 17741.06, base loss: 23606.26
[INFO 2017-06-25 20:05:05,511 main.py:50] epoch 4526, training loss: 18607.08, average training loss: 17734.99, base loss: 23600.95
[INFO 2017-06-25 20:05:09,843 main.py:50] epoch 4527, training loss: 17197.08, average training loss: 17735.45, base loss: 23600.93
[INFO 2017-06-25 20:05:14,131 main.py:50] epoch 4528, training loss: 17863.56, average training loss: 17735.50, base loss: 23600.32
[INFO 2017-06-25 20:05:18,406 main.py:50] epoch 4529, training loss: 15518.23, average training loss: 17736.68, base loss: 23599.85
[INFO 2017-06-25 20:05:22,663 main.py:50] epoch 4530, training loss: 16308.58, average training loss: 17730.12, base loss: 23593.00
[INFO 2017-06-25 20:05:26,925 main.py:50] epoch 4531, training loss: 18064.12, average training loss: 17728.76, base loss: 23591.46
[INFO 2017-06-25 20:05:31,234 main.py:50] epoch 4532, training loss: 14881.40, average training loss: 17723.69, base loss: 23582.00
[INFO 2017-06-25 20:05:35,524 main.py:50] epoch 4533, training loss: 16361.57, average training loss: 17717.22, base loss: 23574.78
[INFO 2017-06-25 20:05:39,827 main.py:50] epoch 4534, training loss: 21219.03, average training loss: 17719.78, base loss: 23579.58
[INFO 2017-06-25 20:05:44,120 main.py:50] epoch 4535, training loss: 19432.12, average training loss: 17722.95, base loss: 23583.07
[INFO 2017-06-25 20:05:48,420 main.py:50] epoch 4536, training loss: 19103.07, average training loss: 17726.96, base loss: 23587.67
[INFO 2017-06-25 20:05:52,707 main.py:50] epoch 4537, training loss: 18691.32, average training loss: 17726.87, base loss: 23590.30
[INFO 2017-06-25 20:05:57,005 main.py:50] epoch 4538, training loss: 17716.17, average training loss: 17726.22, base loss: 23587.74
[INFO 2017-06-25 20:06:01,535 main.py:50] epoch 4539, training loss: 14483.97, average training loss: 17722.11, base loss: 23582.06
[INFO 2017-06-25 20:06:05,793 main.py:50] epoch 4540, training loss: 15174.39, average training loss: 17724.21, base loss: 23584.92
[INFO 2017-06-25 20:06:10,074 main.py:50] epoch 4541, training loss: 17035.85, average training loss: 17725.75, base loss: 23587.05
[INFO 2017-06-25 20:06:14,333 main.py:50] epoch 4542, training loss: 19050.29, average training loss: 17727.04, base loss: 23587.83
[INFO 2017-06-25 20:06:18,585 main.py:50] epoch 4543, training loss: 16995.60, average training loss: 17731.17, base loss: 23593.13
[INFO 2017-06-25 20:06:22,859 main.py:50] epoch 4544, training loss: 18703.13, average training loss: 17734.83, base loss: 23601.26
[INFO 2017-06-25 20:06:27,157 main.py:50] epoch 4545, training loss: 17664.27, average training loss: 17737.51, base loss: 23606.47
[INFO 2017-06-25 20:06:31,451 main.py:50] epoch 4546, training loss: 19980.76, average training loss: 17740.17, base loss: 23612.43
[INFO 2017-06-25 20:06:35,704 main.py:50] epoch 4547, training loss: 18062.55, average training loss: 17741.47, base loss: 23613.97
[INFO 2017-06-25 20:06:39,991 main.py:50] epoch 4548, training loss: 14051.21, average training loss: 17734.33, base loss: 23603.79
[INFO 2017-06-25 20:06:44,292 main.py:50] epoch 4549, training loss: 18981.16, average training loss: 17739.53, base loss: 23611.39
[INFO 2017-06-25 20:06:48,542 main.py:50] epoch 4550, training loss: 20481.98, average training loss: 17744.48, base loss: 23617.86
[INFO 2017-06-25 20:06:52,808 main.py:50] epoch 4551, training loss: 19253.52, average training loss: 17742.36, base loss: 23616.15
[INFO 2017-06-25 20:06:57,076 main.py:50] epoch 4552, training loss: 14805.61, average training loss: 17737.38, base loss: 23610.21
[INFO 2017-06-25 20:07:01,339 main.py:50] epoch 4553, training loss: 15232.33, average training loss: 17735.60, base loss: 23610.48
[INFO 2017-06-25 20:07:05,582 main.py:50] epoch 4554, training loss: 18858.50, average training loss: 17736.93, base loss: 23612.14
[INFO 2017-06-25 20:07:09,870 main.py:50] epoch 4555, training loss: 14261.73, average training loss: 17734.56, base loss: 23608.96
[INFO 2017-06-25 20:07:14,132 main.py:50] epoch 4556, training loss: 16971.03, average training loss: 17732.58, base loss: 23607.80
[INFO 2017-06-25 20:07:18,410 main.py:50] epoch 4557, training loss: 13768.62, average training loss: 17730.81, base loss: 23608.26
[INFO 2017-06-25 20:07:22,686 main.py:50] epoch 4558, training loss: 12561.74, average training loss: 17724.44, base loss: 23601.50
[INFO 2017-06-25 20:07:26,958 main.py:50] epoch 4559, training loss: 18116.52, average training loss: 17726.01, base loss: 23603.93
[INFO 2017-06-25 20:07:31,233 main.py:50] epoch 4560, training loss: 23106.34, average training loss: 17736.28, base loss: 23619.24
[INFO 2017-06-25 20:07:35,489 main.py:50] epoch 4561, training loss: 18278.69, average training loss: 17734.86, base loss: 23620.63
[INFO 2017-06-25 20:07:39,750 main.py:50] epoch 4562, training loss: 14140.87, average training loss: 17728.53, base loss: 23613.81
[INFO 2017-06-25 20:07:44,018 main.py:50] epoch 4563, training loss: 16581.11, average training loss: 17728.66, base loss: 23612.13
[INFO 2017-06-25 20:07:48,291 main.py:50] epoch 4564, training loss: 21176.91, average training loss: 17734.33, base loss: 23619.70
[INFO 2017-06-25 20:07:52,577 main.py:50] epoch 4565, training loss: 17703.29, average training loss: 17734.33, base loss: 23617.68
[INFO 2017-06-25 20:07:56,814 main.py:50] epoch 4566, training loss: 22681.05, average training loss: 17739.46, base loss: 23621.85
[INFO 2017-06-25 20:08:01,070 main.py:50] epoch 4567, training loss: 20901.90, average training loss: 17744.10, base loss: 23627.74
[INFO 2017-06-25 20:08:05,333 main.py:50] epoch 4568, training loss: 13998.74, average training loss: 17737.01, base loss: 23617.57
[INFO 2017-06-25 20:08:09,600 main.py:50] epoch 4569, training loss: 19505.75, average training loss: 17738.43, base loss: 23622.63
[INFO 2017-06-25 20:08:13,884 main.py:50] epoch 4570, training loss: 17508.71, average training loss: 17740.23, base loss: 23628.49
[INFO 2017-06-25 20:08:18,148 main.py:50] epoch 4571, training loss: 18662.19, average training loss: 17742.12, base loss: 23633.36
[INFO 2017-06-25 20:08:22,434 main.py:50] epoch 4572, training loss: 15779.52, average training loss: 17740.21, base loss: 23628.77
[INFO 2017-06-25 20:08:26,721 main.py:50] epoch 4573, training loss: 16685.39, average training loss: 17739.26, base loss: 23627.62
[INFO 2017-06-25 20:08:30,986 main.py:50] epoch 4574, training loss: 18615.31, average training loss: 17738.98, base loss: 23629.19
[INFO 2017-06-25 20:08:35,259 main.py:50] epoch 4575, training loss: 14053.47, average training loss: 17730.82, base loss: 23619.65
[INFO 2017-06-25 20:08:39,537 main.py:50] epoch 4576, training loss: 16798.06, average training loss: 17731.07, base loss: 23621.55
[INFO 2017-06-25 20:08:43,820 main.py:50] epoch 4577, training loss: 19384.02, average training loss: 17733.80, base loss: 23623.18
[INFO 2017-06-25 20:08:48,103 main.py:50] epoch 4578, training loss: 22771.87, average training loss: 17739.28, base loss: 23631.62
[INFO 2017-06-25 20:08:52,357 main.py:50] epoch 4579, training loss: 19214.73, average training loss: 17743.57, base loss: 23638.48
[INFO 2017-06-25 20:08:56,645 main.py:50] epoch 4580, training loss: 15763.60, average training loss: 17739.33, base loss: 23635.12
[INFO 2017-06-25 20:09:00,916 main.py:50] epoch 4581, training loss: 21893.67, average training loss: 17740.17, base loss: 23635.90
[INFO 2017-06-25 20:09:05,173 main.py:50] epoch 4582, training loss: 16404.83, average training loss: 17743.01, base loss: 23640.15
[INFO 2017-06-25 20:09:09,433 main.py:50] epoch 4583, training loss: 16909.60, average training loss: 17738.12, base loss: 23632.03
[INFO 2017-06-25 20:09:13,717 main.py:50] epoch 4584, training loss: 20140.64, average training loss: 17740.94, base loss: 23638.03
[INFO 2017-06-25 20:09:17,989 main.py:50] epoch 4585, training loss: 19614.58, average training loss: 17745.27, base loss: 23641.52
[INFO 2017-06-25 20:09:22,229 main.py:50] epoch 4586, training loss: 23034.02, average training loss: 17752.64, base loss: 23650.97
[INFO 2017-06-25 20:09:26,494 main.py:50] epoch 4587, training loss: 19784.77, average training loss: 17754.35, base loss: 23652.11
[INFO 2017-06-25 20:09:30,752 main.py:50] epoch 4588, training loss: 15792.22, average training loss: 17753.04, base loss: 23648.58
[INFO 2017-06-25 20:09:34,995 main.py:50] epoch 4589, training loss: 17485.66, average training loss: 17753.98, base loss: 23649.93
[INFO 2017-06-25 20:09:39,263 main.py:50] epoch 4590, training loss: 19213.82, average training loss: 17752.79, base loss: 23647.65
[INFO 2017-06-25 20:09:43,538 main.py:50] epoch 4591, training loss: 13698.54, average training loss: 17747.22, base loss: 23643.88
[INFO 2017-06-25 20:09:47,786 main.py:50] epoch 4592, training loss: 17970.54, average training loss: 17748.60, base loss: 23645.82
[INFO 2017-06-25 20:09:52,044 main.py:50] epoch 4593, training loss: 16507.16, average training loss: 17748.37, base loss: 23644.12
[INFO 2017-06-25 20:09:56,322 main.py:50] epoch 4594, training loss: 14943.41, average training loss: 17745.92, base loss: 23641.43
[INFO 2017-06-25 20:10:00,621 main.py:50] epoch 4595, training loss: 17890.29, average training loss: 17749.61, base loss: 23644.03
[INFO 2017-06-25 20:10:04,888 main.py:50] epoch 4596, training loss: 24953.29, average training loss: 17758.16, base loss: 23654.70
[INFO 2017-06-25 20:10:09,181 main.py:50] epoch 4597, training loss: 17444.63, average training loss: 17759.49, base loss: 23653.53
[INFO 2017-06-25 20:10:13,463 main.py:50] epoch 4598, training loss: 15378.93, average training loss: 17759.22, base loss: 23652.64
[INFO 2017-06-25 20:10:17,721 main.py:50] epoch 4599, training loss: 14897.32, average training loss: 17756.08, base loss: 23646.12
[INFO 2017-06-25 20:10:21,980 main.py:50] epoch 4600, training loss: 20588.28, average training loss: 17761.30, base loss: 23654.09
[INFO 2017-06-25 20:10:26,253 main.py:50] epoch 4601, training loss: 18940.26, average training loss: 17762.75, base loss: 23655.81
[INFO 2017-06-25 20:10:30,533 main.py:50] epoch 4602, training loss: 17567.35, average training loss: 17763.49, base loss: 23654.81
[INFO 2017-06-25 20:10:34,804 main.py:50] epoch 4603, training loss: 14787.86, average training loss: 17758.69, base loss: 23648.18
[INFO 2017-06-25 20:10:39,072 main.py:50] epoch 4604, training loss: 17992.16, average training loss: 17761.57, base loss: 23650.05
[INFO 2017-06-25 20:10:43,378 main.py:50] epoch 4605, training loss: 15471.67, average training loss: 17757.90, base loss: 23647.35
[INFO 2017-06-25 20:10:47,696 main.py:50] epoch 4606, training loss: 18441.91, average training loss: 17758.45, base loss: 23649.94
[INFO 2017-06-25 20:10:51,992 main.py:50] epoch 4607, training loss: 18442.52, average training loss: 17760.97, base loss: 23651.77
[INFO 2017-06-25 20:10:56,311 main.py:50] epoch 4608, training loss: 15079.08, average training loss: 17757.55, base loss: 23647.84
[INFO 2017-06-25 20:11:00,580 main.py:50] epoch 4609, training loss: 15059.17, average training loss: 17755.55, base loss: 23642.98
[INFO 2017-06-25 20:11:04,823 main.py:50] epoch 4610, training loss: 17502.77, average training loss: 17758.35, base loss: 23647.22
[INFO 2017-06-25 20:11:09,106 main.py:50] epoch 4611, training loss: 22308.46, average training loss: 17766.15, base loss: 23660.63
[INFO 2017-06-25 20:11:13,393 main.py:50] epoch 4612, training loss: 15721.40, average training loss: 17763.61, base loss: 23657.92
[INFO 2017-06-25 20:11:17,666 main.py:50] epoch 4613, training loss: 21445.79, average training loss: 17763.12, base loss: 23657.60
[INFO 2017-06-25 20:11:21,921 main.py:50] epoch 4614, training loss: 14206.94, average training loss: 17757.09, base loss: 23651.10
[INFO 2017-06-25 20:11:26,205 main.py:50] epoch 4615, training loss: 23499.32, average training loss: 17762.34, base loss: 23659.59
[INFO 2017-06-25 20:11:30,465 main.py:50] epoch 4616, training loss: 21195.20, average training loss: 17766.67, base loss: 23665.06
[INFO 2017-06-25 20:11:34,723 main.py:50] epoch 4617, training loss: 19936.51, average training loss: 17770.42, base loss: 23669.30
[INFO 2017-06-25 20:11:39,000 main.py:50] epoch 4618, training loss: 18690.44, average training loss: 17775.69, base loss: 23674.81
[INFO 2017-06-25 20:11:43,299 main.py:50] epoch 4619, training loss: 19901.37, average training loss: 17778.14, base loss: 23674.44
[INFO 2017-06-25 20:11:47,581 main.py:50] epoch 4620, training loss: 16742.17, average training loss: 17777.88, base loss: 23673.43
[INFO 2017-06-25 20:11:51,865 main.py:50] epoch 4621, training loss: 16936.51, average training loss: 17775.33, base loss: 23672.29
[INFO 2017-06-25 20:11:56,144 main.py:50] epoch 4622, training loss: 18799.68, average training loss: 17773.78, base loss: 23669.20
[INFO 2017-06-25 20:12:00,407 main.py:50] epoch 4623, training loss: 18587.57, average training loss: 17773.95, base loss: 23671.73
[INFO 2017-06-25 20:12:04,643 main.py:50] epoch 4624, training loss: 15308.99, average training loss: 17771.34, base loss: 23668.02
[INFO 2017-06-25 20:12:08,918 main.py:50] epoch 4625, training loss: 17342.75, average training loss: 17772.72, base loss: 23669.55
[INFO 2017-06-25 20:12:13,207 main.py:50] epoch 4626, training loss: 15036.26, average training loss: 17768.02, base loss: 23663.57
[INFO 2017-06-25 20:12:17,489 main.py:50] epoch 4627, training loss: 18620.13, average training loss: 17773.30, base loss: 23668.74
[INFO 2017-06-25 20:12:21,757 main.py:50] epoch 4628, training loss: 15249.72, average training loss: 17770.23, base loss: 23666.69
[INFO 2017-06-25 20:12:26,020 main.py:50] epoch 4629, training loss: 17556.53, average training loss: 17771.43, base loss: 23669.66
[INFO 2017-06-25 20:12:30,323 main.py:50] epoch 4630, training loss: 15492.68, average training loss: 17771.53, base loss: 23671.83
[INFO 2017-06-25 20:12:34,566 main.py:50] epoch 4631, training loss: 18104.49, average training loss: 17771.23, base loss: 23671.23
[INFO 2017-06-25 20:12:38,878 main.py:50] epoch 4632, training loss: 16507.19, average training loss: 17772.59, base loss: 23672.57
[INFO 2017-06-25 20:12:43,135 main.py:50] epoch 4633, training loss: 16207.63, average training loss: 17776.32, base loss: 23678.42
[INFO 2017-06-25 20:12:47,407 main.py:50] epoch 4634, training loss: 16712.73, average training loss: 17768.52, base loss: 23667.04
[INFO 2017-06-25 20:12:51,676 main.py:50] epoch 4635, training loss: 16824.69, average training loss: 17769.60, base loss: 23667.56
[INFO 2017-06-25 20:12:55,961 main.py:50] epoch 4636, training loss: 18841.44, average training loss: 17767.08, base loss: 23663.66
[INFO 2017-06-25 20:13:00,250 main.py:50] epoch 4637, training loss: 20726.31, average training loss: 17768.80, base loss: 23664.30
[INFO 2017-06-25 20:13:04,506 main.py:50] epoch 4638, training loss: 17417.65, average training loss: 17762.29, base loss: 23656.79
[INFO 2017-06-25 20:13:08,784 main.py:50] epoch 4639, training loss: 17229.02, average training loss: 17762.39, base loss: 23658.94
[INFO 2017-06-25 20:13:13,055 main.py:50] epoch 4640, training loss: 22042.57, average training loss: 17766.13, base loss: 23660.25
[INFO 2017-06-25 20:13:17,332 main.py:50] epoch 4641, training loss: 14050.34, average training loss: 17761.58, base loss: 23655.31
[INFO 2017-06-25 20:13:21,600 main.py:50] epoch 4642, training loss: 17450.54, average training loss: 17759.65, base loss: 23653.78
[INFO 2017-06-25 20:13:25,884 main.py:50] epoch 4643, training loss: 17028.09, average training loss: 17757.57, base loss: 23652.97
[INFO 2017-06-25 20:13:30,159 main.py:50] epoch 4644, training loss: 16766.41, average training loss: 17755.75, base loss: 23653.06
[INFO 2017-06-25 20:13:34,427 main.py:50] epoch 4645, training loss: 18355.11, average training loss: 17757.96, base loss: 23657.19
[INFO 2017-06-25 20:13:38,684 main.py:50] epoch 4646, training loss: 18581.71, average training loss: 17760.51, base loss: 23660.92
[INFO 2017-06-25 20:13:42,931 main.py:50] epoch 4647, training loss: 20211.26, average training loss: 17765.48, base loss: 23667.71
[INFO 2017-06-25 20:13:47,210 main.py:50] epoch 4648, training loss: 16572.16, average training loss: 17766.76, base loss: 23668.65
[INFO 2017-06-25 20:13:51,492 main.py:50] epoch 4649, training loss: 16461.56, average training loss: 17762.63, base loss: 23666.02
[INFO 2017-06-25 20:13:55,756 main.py:50] epoch 4650, training loss: 14218.46, average training loss: 17755.01, base loss: 23657.75
[INFO 2017-06-25 20:14:00,031 main.py:50] epoch 4651, training loss: 16357.75, average training loss: 17753.88, base loss: 23660.75
[INFO 2017-06-25 20:14:04,324 main.py:50] epoch 4652, training loss: 17369.81, average training loss: 17753.89, base loss: 23661.72
[INFO 2017-06-25 20:14:08,593 main.py:50] epoch 4653, training loss: 18235.73, average training loss: 17750.92, base loss: 23656.33
[INFO 2017-06-25 20:14:12,856 main.py:50] epoch 4654, training loss: 19157.62, average training loss: 17753.45, base loss: 23658.50
[INFO 2017-06-25 20:14:17,164 main.py:50] epoch 4655, training loss: 17487.97, average training loss: 17753.85, base loss: 23661.46
[INFO 2017-06-25 20:14:21,436 main.py:50] epoch 4656, training loss: 16515.47, average training loss: 17748.68, base loss: 23655.32
[INFO 2017-06-25 20:14:25,711 main.py:50] epoch 4657, training loss: 15261.73, average training loss: 17743.97, base loss: 23651.86
[INFO 2017-06-25 20:14:29,981 main.py:50] epoch 4658, training loss: 18397.38, average training loss: 17747.58, base loss: 23657.30
[INFO 2017-06-25 20:14:34,243 main.py:50] epoch 4659, training loss: 20876.69, average training loss: 17753.09, base loss: 23663.32
[INFO 2017-06-25 20:14:38,543 main.py:50] epoch 4660, training loss: 14713.25, average training loss: 17752.81, base loss: 23664.76
[INFO 2017-06-25 20:14:42,820 main.py:50] epoch 4661, training loss: 16157.61, average training loss: 17741.91, base loss: 23654.95
[INFO 2017-06-25 20:14:47,095 main.py:50] epoch 4662, training loss: 15580.11, average training loss: 17741.73, base loss: 23657.27
[INFO 2017-06-25 20:14:51,387 main.py:50] epoch 4663, training loss: 17312.08, average training loss: 17743.58, base loss: 23657.86
[INFO 2017-06-25 20:14:55,654 main.py:50] epoch 4664, training loss: 19145.17, average training loss: 17745.41, base loss: 23661.68
[INFO 2017-06-25 20:14:59,934 main.py:50] epoch 4665, training loss: 18047.68, average training loss: 17741.02, base loss: 23657.00
[INFO 2017-06-25 20:15:04,198 main.py:50] epoch 4666, training loss: 13202.59, average training loss: 17738.25, base loss: 23652.10
[INFO 2017-06-25 20:15:08,484 main.py:50] epoch 4667, training loss: 17385.52, average training loss: 17737.45, base loss: 23652.61
[INFO 2017-06-25 20:15:12,724 main.py:50] epoch 4668, training loss: 19237.55, average training loss: 17741.26, base loss: 23658.99
[INFO 2017-06-25 20:15:16,983 main.py:50] epoch 4669, training loss: 18783.15, average training loss: 17741.79, base loss: 23658.66
[INFO 2017-06-25 20:15:21,281 main.py:50] epoch 4670, training loss: 18970.97, average training loss: 17740.01, base loss: 23658.08
[INFO 2017-06-25 20:15:25,552 main.py:50] epoch 4671, training loss: 14807.56, average training loss: 17737.12, base loss: 23653.68
[INFO 2017-06-25 20:15:29,837 main.py:50] epoch 4672, training loss: 16100.69, average training loss: 17735.69, base loss: 23652.94
[INFO 2017-06-25 20:15:34,135 main.py:50] epoch 4673, training loss: 14661.48, average training loss: 17732.27, base loss: 23648.45
[INFO 2017-06-25 20:15:38,434 main.py:50] epoch 4674, training loss: 22155.64, average training loss: 17737.15, base loss: 23651.89
[INFO 2017-06-25 20:15:42,716 main.py:50] epoch 4675, training loss: 16712.38, average training loss: 17735.03, base loss: 23650.83
[INFO 2017-06-25 20:15:46,993 main.py:50] epoch 4676, training loss: 19317.16, average training loss: 17735.67, base loss: 23652.38
[INFO 2017-06-25 20:15:51,278 main.py:50] epoch 4677, training loss: 15363.23, average training loss: 17730.51, base loss: 23650.39
[INFO 2017-06-25 20:15:55,540 main.py:50] epoch 4678, training loss: 13495.24, average training loss: 17726.04, base loss: 23643.62
[INFO 2017-06-25 20:15:59,820 main.py:50] epoch 4679, training loss: 17614.14, average training loss: 17723.64, base loss: 23642.52
[INFO 2017-06-25 20:16:04,104 main.py:50] epoch 4680, training loss: 19336.07, average training loss: 17724.27, base loss: 23646.22
[INFO 2017-06-25 20:16:08,379 main.py:50] epoch 4681, training loss: 16569.30, average training loss: 17718.92, base loss: 23637.87
[INFO 2017-06-25 20:16:12,639 main.py:50] epoch 4682, training loss: 17383.73, average training loss: 17715.08, base loss: 23632.37
[INFO 2017-06-25 20:16:16,925 main.py:50] epoch 4683, training loss: 16716.90, average training loss: 17711.35, base loss: 23628.90
[INFO 2017-06-25 20:16:21,211 main.py:50] epoch 4684, training loss: 15027.07, average training loss: 17708.63, base loss: 23624.73
[INFO 2017-06-25 20:16:25,476 main.py:50] epoch 4685, training loss: 15385.56, average training loss: 17706.72, base loss: 23624.66
[INFO 2017-06-25 20:16:29,748 main.py:50] epoch 4686, training loss: 16934.89, average training loss: 17705.67, base loss: 23623.50
[INFO 2017-06-25 20:16:34,013 main.py:50] epoch 4687, training loss: 14717.92, average training loss: 17703.32, base loss: 23621.99
[INFO 2017-06-25 20:16:38,297 main.py:50] epoch 4688, training loss: 18516.24, average training loss: 17705.01, base loss: 23621.81
[INFO 2017-06-25 20:16:42,555 main.py:50] epoch 4689, training loss: 17838.54, average training loss: 17707.95, base loss: 23625.56
[INFO 2017-06-25 20:16:46,836 main.py:50] epoch 4690, training loss: 19719.51, average training loss: 17713.53, base loss: 23633.12
[INFO 2017-06-25 20:16:51,116 main.py:50] epoch 4691, training loss: 22365.91, average training loss: 17716.71, base loss: 23639.25
[INFO 2017-06-25 20:16:55,387 main.py:50] epoch 4692, training loss: 15145.16, average training loss: 17713.06, base loss: 23632.61
[INFO 2017-06-25 20:16:59,673 main.py:50] epoch 4693, training loss: 17363.15, average training loss: 17711.95, base loss: 23632.03
[INFO 2017-06-25 20:17:03,934 main.py:50] epoch 4694, training loss: 16070.19, average training loss: 17709.09, base loss: 23630.92
[INFO 2017-06-25 20:17:08,203 main.py:50] epoch 4695, training loss: 18046.33, average training loss: 17709.23, base loss: 23630.54
[INFO 2017-06-25 20:17:12,488 main.py:50] epoch 4696, training loss: 21904.00, average training loss: 17716.43, base loss: 23637.97
[INFO 2017-06-25 20:17:16,763 main.py:50] epoch 4697, training loss: 14557.37, average training loss: 17713.93, base loss: 23637.65
[INFO 2017-06-25 20:17:21,028 main.py:50] epoch 4698, training loss: 18195.40, average training loss: 17713.42, base loss: 23638.05
[INFO 2017-06-25 20:17:25,316 main.py:50] epoch 4699, training loss: 19958.23, average training loss: 17718.26, base loss: 23648.84
[INFO 2017-06-25 20:17:29,620 main.py:50] epoch 4700, training loss: 17078.92, average training loss: 17719.49, base loss: 23650.06
[INFO 2017-06-25 20:17:33,879 main.py:50] epoch 4701, training loss: 15083.19, average training loss: 17718.16, base loss: 23649.46
[INFO 2017-06-25 20:17:38,174 main.py:50] epoch 4702, training loss: 16550.91, average training loss: 17713.82, base loss: 23643.81
[INFO 2017-06-25 20:17:42,434 main.py:50] epoch 4703, training loss: 16110.96, average training loss: 17708.35, base loss: 23640.02
[INFO 2017-06-25 20:17:46,700 main.py:50] epoch 4704, training loss: 18094.97, average training loss: 17705.95, base loss: 23635.17
[INFO 2017-06-25 20:17:50,958 main.py:50] epoch 4705, training loss: 13317.08, average training loss: 17703.03, base loss: 23631.69
[INFO 2017-06-25 20:17:55,220 main.py:50] epoch 4706, training loss: 19563.83, average training loss: 17705.40, base loss: 23632.89
[INFO 2017-06-25 20:17:59,493 main.py:50] epoch 4707, training loss: 15197.62, average training loss: 17700.85, base loss: 23632.04
[INFO 2017-06-25 20:18:03,773 main.py:50] epoch 4708, training loss: 17584.05, average training loss: 17699.91, base loss: 23632.54
[INFO 2017-06-25 20:18:08,047 main.py:50] epoch 4709, training loss: 17948.94, average training loss: 17696.65, base loss: 23630.08
[INFO 2017-06-25 20:18:12,326 main.py:50] epoch 4710, training loss: 15812.30, average training loss: 17693.58, base loss: 23623.26
[INFO 2017-06-25 20:18:16,595 main.py:50] epoch 4711, training loss: 16547.64, average training loss: 17694.75, base loss: 23624.62
[INFO 2017-06-25 20:18:20,863 main.py:50] epoch 4712, training loss: 19715.33, average training loss: 17697.88, base loss: 23628.08
[INFO 2017-06-25 20:18:25,129 main.py:50] epoch 4713, training loss: 16966.88, average training loss: 17697.87, base loss: 23631.29
[INFO 2017-06-25 20:18:29,413 main.py:50] epoch 4714, training loss: 18298.96, average training loss: 17698.79, base loss: 23632.68
[INFO 2017-06-25 20:18:33,685 main.py:50] epoch 4715, training loss: 21465.24, average training loss: 17702.52, base loss: 23638.69
[INFO 2017-06-25 20:18:37,943 main.py:50] epoch 4716, training loss: 17829.99, average training loss: 17698.16, base loss: 23632.57
[INFO 2017-06-25 20:18:42,204 main.py:50] epoch 4717, training loss: 14759.58, average training loss: 17698.27, base loss: 23634.01
[INFO 2017-06-25 20:18:46,486 main.py:50] epoch 4718, training loss: 17637.64, average training loss: 17700.07, base loss: 23635.34
[INFO 2017-06-25 20:18:50,779 main.py:50] epoch 4719, training loss: 20498.56, average training loss: 17706.76, base loss: 23643.62
[INFO 2017-06-25 20:18:55,027 main.py:50] epoch 4720, training loss: 18870.40, average training loss: 17702.44, base loss: 23636.70
[INFO 2017-06-25 20:18:59,294 main.py:50] epoch 4721, training loss: 16413.32, average training loss: 17699.21, base loss: 23630.77
[INFO 2017-06-25 20:19:03,595 main.py:50] epoch 4722, training loss: 20179.27, average training loss: 17703.39, base loss: 23636.15
[INFO 2017-06-25 20:19:07,860 main.py:50] epoch 4723, training loss: 15828.69, average training loss: 17697.86, base loss: 23626.67
[INFO 2017-06-25 20:19:12,129 main.py:50] epoch 4724, training loss: 17797.24, average training loss: 17695.85, base loss: 23625.50
[INFO 2017-06-25 20:19:16,413 main.py:50] epoch 4725, training loss: 21182.50, average training loss: 17697.80, base loss: 23629.76
[INFO 2017-06-25 20:19:20,684 main.py:50] epoch 4726, training loss: 18514.65, average training loss: 17694.50, base loss: 23623.82
[INFO 2017-06-25 20:19:24,948 main.py:50] epoch 4727, training loss: 16708.07, average training loss: 17693.91, base loss: 23624.19
[INFO 2017-06-25 20:19:29,236 main.py:50] epoch 4728, training loss: 19395.18, average training loss: 17695.53, base loss: 23628.16
[INFO 2017-06-25 20:19:33,510 main.py:50] epoch 4729, training loss: 15461.92, average training loss: 17696.84, base loss: 23631.49
[INFO 2017-06-25 20:19:37,755 main.py:50] epoch 4730, training loss: 17352.77, average training loss: 17696.79, base loss: 23627.31
[INFO 2017-06-25 20:19:42,020 main.py:50] epoch 4731, training loss: 21233.33, average training loss: 17700.62, base loss: 23632.94
[INFO 2017-06-25 20:19:46,294 main.py:50] epoch 4732, training loss: 22165.41, average training loss: 17704.49, base loss: 23640.38
[INFO 2017-06-25 20:19:50,557 main.py:50] epoch 4733, training loss: 21190.68, average training loss: 17707.35, base loss: 23641.59
[INFO 2017-06-25 20:19:54,851 main.py:50] epoch 4734, training loss: 13931.47, average training loss: 17702.61, base loss: 23634.31
[INFO 2017-06-25 20:19:59,126 main.py:50] epoch 4735, training loss: 19897.99, average training loss: 17702.52, base loss: 23632.13
[INFO 2017-06-25 20:20:03,355 main.py:50] epoch 4736, training loss: 15481.14, average training loss: 17698.90, base loss: 23626.14
[INFO 2017-06-25 20:20:07,625 main.py:50] epoch 4737, training loss: 24609.60, average training loss: 17707.80, base loss: 23634.65
[INFO 2017-06-25 20:20:11,903 main.py:50] epoch 4738, training loss: 17681.07, average training loss: 17706.18, base loss: 23629.63
[INFO 2017-06-25 20:20:16,186 main.py:50] epoch 4739, training loss: 18792.38, average training loss: 17706.43, base loss: 23631.95
[INFO 2017-06-25 20:20:20,452 main.py:50] epoch 4740, training loss: 21094.72, average training loss: 17710.46, base loss: 23638.14
[INFO 2017-06-25 20:20:24,700 main.py:50] epoch 4741, training loss: 15386.89, average training loss: 17709.13, base loss: 23634.09
[INFO 2017-06-25 20:20:28,961 main.py:50] epoch 4742, training loss: 18673.51, average training loss: 17711.99, base loss: 23638.84
[INFO 2017-06-25 20:20:33,215 main.py:50] epoch 4743, training loss: 16657.48, average training loss: 17711.63, base loss: 23635.66
[INFO 2017-06-25 20:20:37,481 main.py:50] epoch 4744, training loss: 17723.16, average training loss: 17712.81, base loss: 23635.75
[INFO 2017-06-25 20:20:41,759 main.py:50] epoch 4745, training loss: 15871.65, average training loss: 17712.82, base loss: 23634.88
[INFO 2017-06-25 20:20:46,052 main.py:50] epoch 4746, training loss: 13538.71, average training loss: 17709.16, base loss: 23628.99
[INFO 2017-06-25 20:20:50,332 main.py:50] epoch 4747, training loss: 18031.82, average training loss: 17712.26, base loss: 23633.34
[INFO 2017-06-25 20:20:54,600 main.py:50] epoch 4748, training loss: 18584.98, average training loss: 17714.66, base loss: 23635.70
[INFO 2017-06-25 20:20:58,864 main.py:50] epoch 4749, training loss: 17721.25, average training loss: 17714.76, base loss: 23636.92
[INFO 2017-06-25 20:21:03,159 main.py:50] epoch 4750, training loss: 17816.95, average training loss: 17716.02, base loss: 23638.02
[INFO 2017-06-25 20:21:07,408 main.py:50] epoch 4751, training loss: 20007.51, average training loss: 17714.25, base loss: 23636.54
[INFO 2017-06-25 20:21:11,663 main.py:50] epoch 4752, training loss: 18248.82, average training loss: 17715.27, base loss: 23638.02
[INFO 2017-06-25 20:21:15,930 main.py:50] epoch 4753, training loss: 19247.96, average training loss: 17717.44, base loss: 23643.19
[INFO 2017-06-25 20:21:20,198 main.py:50] epoch 4754, training loss: 17092.26, average training loss: 17717.77, base loss: 23644.18
[INFO 2017-06-25 20:21:24,459 main.py:50] epoch 4755, training loss: 16251.96, average training loss: 17718.70, base loss: 23643.91
[INFO 2017-06-25 20:21:28,725 main.py:50] epoch 4756, training loss: 16986.96, average training loss: 17713.51, base loss: 23635.57
[INFO 2017-06-25 20:21:33,003 main.py:50] epoch 4757, training loss: 18144.61, average training loss: 17712.99, base loss: 23635.82
[INFO 2017-06-25 20:21:37,318 main.py:50] epoch 4758, training loss: 17424.09, average training loss: 17710.51, base loss: 23629.36
[INFO 2017-06-25 20:21:41,629 main.py:50] epoch 4759, training loss: 15784.82, average training loss: 17710.96, base loss: 23633.39
[INFO 2017-06-25 20:21:45,885 main.py:50] epoch 4760, training loss: 17509.77, average training loss: 17707.83, base loss: 23628.58
[INFO 2017-06-25 20:21:50,142 main.py:50] epoch 4761, training loss: 16940.62, average training loss: 17705.52, base loss: 23628.26
[INFO 2017-06-25 20:21:54,425 main.py:50] epoch 4762, training loss: 20297.82, average training loss: 17709.71, base loss: 23633.84
[INFO 2017-06-25 20:21:58,683 main.py:50] epoch 4763, training loss: 19628.26, average training loss: 17711.72, base loss: 23637.87
[INFO 2017-06-25 20:22:02,973 main.py:50] epoch 4764, training loss: 17337.27, average training loss: 17710.14, base loss: 23636.34
[INFO 2017-06-25 20:22:07,265 main.py:50] epoch 4765, training loss: 21315.21, average training loss: 17713.94, base loss: 23642.04
[INFO 2017-06-25 20:22:11,527 main.py:50] epoch 4766, training loss: 18368.64, average training loss: 17716.62, base loss: 23643.83
[INFO 2017-06-25 20:22:15,805 main.py:50] epoch 4767, training loss: 18509.30, average training loss: 17717.04, base loss: 23644.36
[INFO 2017-06-25 20:22:20,093 main.py:50] epoch 4768, training loss: 16325.15, average training loss: 17716.62, base loss: 23646.15
[INFO 2017-06-25 20:22:24,365 main.py:50] epoch 4769, training loss: 19369.72, average training loss: 17717.83, base loss: 23647.34
[INFO 2017-06-25 20:22:28,653 main.py:50] epoch 4770, training loss: 17511.54, average training loss: 17714.13, base loss: 23641.54
[INFO 2017-06-25 20:22:32,924 main.py:50] epoch 4771, training loss: 14013.01, average training loss: 17706.22, base loss: 23633.96
[INFO 2017-06-25 20:22:37,215 main.py:50] epoch 4772, training loss: 16556.02, average training loss: 17703.96, base loss: 23633.68
[INFO 2017-06-25 20:22:41,461 main.py:50] epoch 4773, training loss: 22459.52, average training loss: 17707.20, base loss: 23635.90
[INFO 2017-06-25 20:22:45,750 main.py:50] epoch 4774, training loss: 21079.43, average training loss: 17715.46, base loss: 23647.94
[INFO 2017-06-25 20:22:50,054 main.py:50] epoch 4775, training loss: 18501.35, average training loss: 17716.61, base loss: 23647.68
[INFO 2017-06-25 20:22:54,336 main.py:50] epoch 4776, training loss: 16877.77, average training loss: 17715.14, base loss: 23646.27
[INFO 2017-06-25 20:22:58,602 main.py:50] epoch 4777, training loss: 22793.89, average training loss: 17718.18, base loss: 23649.56
[INFO 2017-06-25 20:23:02,885 main.py:50] epoch 4778, training loss: 14751.74, average training loss: 17715.82, base loss: 23647.94
[INFO 2017-06-25 20:23:07,179 main.py:50] epoch 4779, training loss: 21031.95, average training loss: 17721.86, base loss: 23650.62
[INFO 2017-06-25 20:23:11,443 main.py:50] epoch 4780, training loss: 16892.28, average training loss: 17719.95, base loss: 23647.87
[INFO 2017-06-25 20:23:15,726 main.py:50] epoch 4781, training loss: 17442.66, average training loss: 17715.74, base loss: 23644.23
[INFO 2017-06-25 20:23:20,040 main.py:50] epoch 4782, training loss: 19808.59, average training loss: 17717.21, base loss: 23646.14
[INFO 2017-06-25 20:23:24,304 main.py:50] epoch 4783, training loss: 24852.18, average training loss: 17726.26, base loss: 23655.31
[INFO 2017-06-25 20:23:28,577 main.py:50] epoch 4784, training loss: 22102.82, average training loss: 17731.62, base loss: 23660.83
[INFO 2017-06-25 20:23:32,850 main.py:50] epoch 4785, training loss: 17242.50, average training loss: 17734.20, base loss: 23664.91
[INFO 2017-06-25 20:23:37,124 main.py:50] epoch 4786, training loss: 13697.80, average training loss: 17731.27, base loss: 23658.58
[INFO 2017-06-25 20:23:41,391 main.py:50] epoch 4787, training loss: 15066.06, average training loss: 17729.21, base loss: 23655.19
[INFO 2017-06-25 20:23:45,669 main.py:50] epoch 4788, training loss: 19525.08, average training loss: 17725.69, base loss: 23650.91
[INFO 2017-06-25 20:23:49,939 main.py:50] epoch 4789, training loss: 15630.63, average training loss: 17724.00, base loss: 23650.70
[INFO 2017-06-25 20:23:54,252 main.py:50] epoch 4790, training loss: 17836.83, average training loss: 17720.94, base loss: 23646.09
[INFO 2017-06-25 20:23:58,518 main.py:50] epoch 4791, training loss: 18116.08, average training loss: 17720.48, base loss: 23645.46
[INFO 2017-06-25 20:24:02,793 main.py:50] epoch 4792, training loss: 19279.72, average training loss: 17723.65, base loss: 23653.34
[INFO 2017-06-25 20:24:07,053 main.py:50] epoch 4793, training loss: 21848.17, average training loss: 17728.23, base loss: 23656.78
[INFO 2017-06-25 20:24:11,322 main.py:50] epoch 4794, training loss: 12081.10, average training loss: 17722.83, base loss: 23648.28
[INFO 2017-06-25 20:24:15,634 main.py:50] epoch 4795, training loss: 19525.62, average training loss: 17724.49, base loss: 23647.34
[INFO 2017-06-25 20:24:19,924 main.py:50] epoch 4796, training loss: 19808.26, average training loss: 17724.92, base loss: 23648.70
[INFO 2017-06-25 20:24:24,187 main.py:50] epoch 4797, training loss: 17114.32, average training loss: 17726.38, base loss: 23650.27
[INFO 2017-06-25 20:24:28,459 main.py:50] epoch 4798, training loss: 17825.03, average training loss: 17723.29, base loss: 23645.23
[INFO 2017-06-25 20:24:32,732 main.py:50] epoch 4799, training loss: 15628.86, average training loss: 17722.41, base loss: 23642.26
[INFO 2017-06-25 20:24:36,996 main.py:50] epoch 4800, training loss: 18531.60, average training loss: 17723.86, base loss: 23643.95
[INFO 2017-06-25 20:24:41,280 main.py:50] epoch 4801, training loss: 18633.12, average training loss: 17726.01, base loss: 23643.82
[INFO 2017-06-25 20:24:45,540 main.py:50] epoch 4802, training loss: 18443.53, average training loss: 17728.84, base loss: 23649.47
[INFO 2017-06-25 20:24:49,817 main.py:50] epoch 4803, training loss: 17608.96, average training loss: 17732.65, base loss: 23657.95
[INFO 2017-06-25 20:24:54,088 main.py:50] epoch 4804, training loss: 19156.62, average training loss: 17740.30, base loss: 23667.10
[INFO 2017-06-25 20:24:58,348 main.py:50] epoch 4805, training loss: 13651.31, average training loss: 17738.11, base loss: 23662.01
[INFO 2017-06-25 20:25:02,607 main.py:50] epoch 4806, training loss: 16553.73, average training loss: 17739.34, base loss: 23663.86
[INFO 2017-06-25 20:25:06,889 main.py:50] epoch 4807, training loss: 18875.85, average training loss: 17739.00, base loss: 23667.33
[INFO 2017-06-25 20:25:11,187 main.py:50] epoch 4808, training loss: 15992.13, average training loss: 17738.45, base loss: 23668.46
[INFO 2017-06-25 20:25:15,440 main.py:50] epoch 4809, training loss: 17711.53, average training loss: 17742.19, base loss: 23672.83
[INFO 2017-06-25 20:25:19,707 main.py:50] epoch 4810, training loss: 22116.36, average training loss: 17748.88, base loss: 23684.06
[INFO 2017-06-25 20:25:23,985 main.py:50] epoch 4811, training loss: 16955.89, average training loss: 17743.49, base loss: 23676.62
[INFO 2017-06-25 20:25:28,259 main.py:50] epoch 4812, training loss: 21269.24, average training loss: 17750.54, base loss: 23683.13
[INFO 2017-06-25 20:25:32,520 main.py:50] epoch 4813, training loss: 18522.02, average training loss: 17751.04, base loss: 23686.70
[INFO 2017-06-25 20:25:36,793 main.py:50] epoch 4814, training loss: 15386.99, average training loss: 17751.64, base loss: 23688.41
[INFO 2017-06-25 20:25:41,064 main.py:50] epoch 4815, training loss: 19815.35, average training loss: 17744.06, base loss: 23679.66
[INFO 2017-06-25 20:25:45,336 main.py:50] epoch 4816, training loss: 21023.80, average training loss: 17748.05, base loss: 23683.59
[INFO 2017-06-25 20:25:49,599 main.py:50] epoch 4817, training loss: 21351.52, average training loss: 17751.52, base loss: 23686.53
[INFO 2017-06-25 20:25:53,891 main.py:50] epoch 4818, training loss: 18429.86, average training loss: 17752.47, base loss: 23688.75
[INFO 2017-06-25 20:25:58,163 main.py:50] epoch 4819, training loss: 13022.57, average training loss: 17748.03, base loss: 23680.51
[INFO 2017-06-25 20:26:02,444 main.py:50] epoch 4820, training loss: 15226.81, average training loss: 17746.14, base loss: 23681.72
[INFO 2017-06-25 20:26:06,715 main.py:50] epoch 4821, training loss: 14400.95, average training loss: 17742.03, base loss: 23672.67
[INFO 2017-06-25 20:26:10,977 main.py:50] epoch 4822, training loss: 15014.01, average training loss: 17739.20, base loss: 23668.18
[INFO 2017-06-25 20:26:15,235 main.py:50] epoch 4823, training loss: 17218.94, average training loss: 17739.86, base loss: 23667.87
[INFO 2017-06-25 20:26:19,522 main.py:50] epoch 4824, training loss: 14613.78, average training loss: 17740.27, base loss: 23668.29
[INFO 2017-06-25 20:26:23,778 main.py:50] epoch 4825, training loss: 17459.54, average training loss: 17738.77, base loss: 23664.19
[INFO 2017-06-25 20:26:28,048 main.py:50] epoch 4826, training loss: 22325.93, average training loss: 17740.29, base loss: 23667.31
[INFO 2017-06-25 20:26:32,328 main.py:50] epoch 4827, training loss: 20677.42, average training loss: 17741.82, base loss: 23667.79
[INFO 2017-06-25 20:26:36,614 main.py:50] epoch 4828, training loss: 16489.85, average training loss: 17739.94, base loss: 23667.06
[INFO 2017-06-25 20:26:40,895 main.py:50] epoch 4829, training loss: 15740.25, average training loss: 17738.67, base loss: 23664.76
[INFO 2017-06-25 20:26:45,180 main.py:50] epoch 4830, training loss: 16697.14, average training loss: 17733.92, base loss: 23657.92
[INFO 2017-06-25 20:26:49,463 main.py:50] epoch 4831, training loss: 17102.81, average training loss: 17734.75, base loss: 23659.55
[INFO 2017-06-25 20:26:53,718 main.py:50] epoch 4832, training loss: 17214.82, average training loss: 17732.98, base loss: 23657.47
[INFO 2017-06-25 20:26:58,000 main.py:50] epoch 4833, training loss: 16506.67, average training loss: 17729.28, base loss: 23655.63
[INFO 2017-06-25 20:27:02,257 main.py:50] epoch 4834, training loss: 17533.39, average training loss: 17724.91, base loss: 23649.41
[INFO 2017-06-25 20:27:06,530 main.py:50] epoch 4835, training loss: 17045.14, average training loss: 17721.78, base loss: 23643.66
[INFO 2017-06-25 20:27:10,824 main.py:50] epoch 4836, training loss: 18424.50, average training loss: 17722.32, base loss: 23644.53
[INFO 2017-06-25 20:27:15,114 main.py:50] epoch 4837, training loss: 17949.82, average training loss: 17719.55, base loss: 23640.58
[INFO 2017-06-25 20:27:19,385 main.py:50] epoch 4838, training loss: 15791.10, average training loss: 17721.49, base loss: 23641.90
[INFO 2017-06-25 20:27:23,654 main.py:50] epoch 4839, training loss: 16035.49, average training loss: 17720.00, base loss: 23641.70
[INFO 2017-06-25 20:27:27,936 main.py:50] epoch 4840, training loss: 14491.93, average training loss: 17719.54, base loss: 23641.96
[INFO 2017-06-25 20:27:32,199 main.py:50] epoch 4841, training loss: 15465.03, average training loss: 17721.38, base loss: 23645.11
[INFO 2017-06-25 20:27:36,452 main.py:50] epoch 4842, training loss: 18913.90, average training loss: 17720.61, base loss: 23640.98
[INFO 2017-06-25 20:27:40,715 main.py:50] epoch 4843, training loss: 17314.51, average training loss: 17714.94, base loss: 23634.00
[INFO 2017-06-25 20:27:45,020 main.py:50] epoch 4844, training loss: 18528.16, average training loss: 17717.21, base loss: 23636.21
[INFO 2017-06-25 20:27:49,300 main.py:50] epoch 4845, training loss: 20094.34, average training loss: 17718.04, base loss: 23639.02
[INFO 2017-06-25 20:27:53,587 main.py:50] epoch 4846, training loss: 22245.02, average training loss: 17723.83, base loss: 23649.12
[INFO 2017-06-25 20:27:57,849 main.py:50] epoch 4847, training loss: 21300.40, average training loss: 17724.03, base loss: 23651.89
[INFO 2017-06-25 20:28:02,123 main.py:50] epoch 4848, training loss: 13617.78, average training loss: 17720.48, base loss: 23644.90
[INFO 2017-06-25 20:28:06,401 main.py:50] epoch 4849, training loss: 21648.59, average training loss: 17723.58, base loss: 23647.48
[INFO 2017-06-25 20:28:10,676 main.py:50] epoch 4850, training loss: 16507.06, average training loss: 17721.83, base loss: 23645.03
[INFO 2017-06-25 20:28:14,942 main.py:50] epoch 4851, training loss: 18370.87, average training loss: 17724.77, base loss: 23647.38
[INFO 2017-06-25 20:28:19,211 main.py:50] epoch 4852, training loss: 16809.32, average training loss: 17719.46, base loss: 23639.80
[INFO 2017-06-25 20:28:23,471 main.py:50] epoch 4853, training loss: 16370.15, average training loss: 17718.07, base loss: 23635.27
[INFO 2017-06-25 20:28:27,750 main.py:50] epoch 4854, training loss: 18940.81, average training loss: 17721.80, base loss: 23644.60
[INFO 2017-06-25 20:28:32,020 main.py:50] epoch 4855, training loss: 17429.32, average training loss: 17719.01, base loss: 23641.51
[INFO 2017-06-25 20:28:36,294 main.py:50] epoch 4856, training loss: 17128.18, average training loss: 17714.47, base loss: 23637.77
[INFO 2017-06-25 20:28:40,586 main.py:50] epoch 4857, training loss: 19682.53, average training loss: 17719.01, base loss: 23643.57
[INFO 2017-06-25 20:28:44,851 main.py:50] epoch 4858, training loss: 18046.98, average training loss: 17719.97, base loss: 23647.30
[INFO 2017-06-25 20:28:49,121 main.py:50] epoch 4859, training loss: 11850.68, average training loss: 17715.97, base loss: 23640.26
[INFO 2017-06-25 20:28:53,390 main.py:50] epoch 4860, training loss: 18121.22, average training loss: 17719.39, base loss: 23646.53
[INFO 2017-06-25 20:28:57,665 main.py:50] epoch 4861, training loss: 19808.24, average training loss: 17722.16, base loss: 23651.71
[INFO 2017-06-25 20:29:01,936 main.py:50] epoch 4862, training loss: 17624.22, average training loss: 17723.29, base loss: 23653.84
[INFO 2017-06-25 20:29:06,218 main.py:50] epoch 4863, training loss: 20656.03, average training loss: 17726.05, base loss: 23656.85
[INFO 2017-06-25 20:29:10,456 main.py:50] epoch 4864, training loss: 15643.33, average training loss: 17722.63, base loss: 23654.16
[INFO 2017-06-25 20:29:14,751 main.py:50] epoch 4865, training loss: 15065.79, average training loss: 17724.10, base loss: 23657.38
[INFO 2017-06-25 20:29:19,028 main.py:50] epoch 4866, training loss: 18255.87, average training loss: 17721.36, base loss: 23652.82
[INFO 2017-06-25 20:29:23,301 main.py:50] epoch 4867, training loss: 18979.59, average training loss: 17725.59, base loss: 23654.52
[INFO 2017-06-25 20:29:27,606 main.py:50] epoch 4868, training loss: 13823.22, average training loss: 17723.94, base loss: 23653.76
[INFO 2017-06-25 20:29:31,861 main.py:50] epoch 4869, training loss: 22648.38, average training loss: 17727.25, base loss: 23657.80
[INFO 2017-06-25 20:29:36,163 main.py:50] epoch 4870, training loss: 19272.56, average training loss: 17728.77, base loss: 23657.89
[INFO 2017-06-25 20:29:40,441 main.py:50] epoch 4871, training loss: 18171.37, average training loss: 17728.01, base loss: 23659.20
[INFO 2017-06-25 20:29:44,723 main.py:50] epoch 4872, training loss: 17048.23, average training loss: 17730.52, base loss: 23662.64
[INFO 2017-06-25 20:29:49,039 main.py:50] epoch 4873, training loss: 21232.52, average training loss: 17735.78, base loss: 23672.04
[INFO 2017-06-25 20:29:53,346 main.py:50] epoch 4874, training loss: 14304.34, average training loss: 17730.40, base loss: 23663.32
[INFO 2017-06-25 20:29:57,617 main.py:50] epoch 4875, training loss: 17025.72, average training loss: 17729.21, base loss: 23662.03
[INFO 2017-06-25 20:30:01,865 main.py:50] epoch 4876, training loss: 15868.72, average training loss: 17728.94, base loss: 23663.37
[INFO 2017-06-25 20:30:06,142 main.py:50] epoch 4877, training loss: 15930.86, average training loss: 17724.65, base loss: 23657.30
[INFO 2017-06-25 20:30:10,422 main.py:50] epoch 4878, training loss: 19182.24, average training loss: 17728.61, base loss: 23661.03
[INFO 2017-06-25 20:30:14,698 main.py:50] epoch 4879, training loss: 14792.66, average training loss: 17725.14, base loss: 23657.20
[INFO 2017-06-25 20:30:18,991 main.py:50] epoch 4880, training loss: 22886.29, average training loss: 17729.55, base loss: 23662.28
[INFO 2017-06-25 20:30:23,277 main.py:50] epoch 4881, training loss: 18805.04, average training loss: 17727.78, base loss: 23661.07
[INFO 2017-06-25 20:30:27,550 main.py:50] epoch 4882, training loss: 16261.10, average training loss: 17729.92, base loss: 23666.22
[INFO 2017-06-25 20:30:31,822 main.py:50] epoch 4883, training loss: 17343.81, average training loss: 17728.18, base loss: 23664.87
[INFO 2017-06-25 20:30:36,105 main.py:50] epoch 4884, training loss: 15120.63, average training loss: 17726.30, base loss: 23665.73
[INFO 2017-06-25 20:30:40,396 main.py:50] epoch 4885, training loss: 14673.10, average training loss: 17724.85, base loss: 23664.80
[INFO 2017-06-25 20:30:44,694 main.py:50] epoch 4886, training loss: 16865.50, average training loss: 17722.25, base loss: 23660.05
[INFO 2017-06-25 20:30:48,983 main.py:50] epoch 4887, training loss: 19164.37, average training loss: 17725.78, base loss: 23664.62
[INFO 2017-06-25 20:30:53,285 main.py:50] epoch 4888, training loss: 19776.08, average training loss: 17728.58, base loss: 23666.07
[INFO 2017-06-25 20:30:57,556 main.py:50] epoch 4889, training loss: 17052.12, average training loss: 17729.44, base loss: 23669.40
[INFO 2017-06-25 20:31:01,859 main.py:50] epoch 4890, training loss: 16032.94, average training loss: 17726.87, base loss: 23667.94
[INFO 2017-06-25 20:31:06,154 main.py:50] epoch 4891, training loss: 22810.27, average training loss: 17737.16, base loss: 23680.76
[INFO 2017-06-25 20:31:10,433 main.py:50] epoch 4892, training loss: 16744.74, average training loss: 17738.82, base loss: 23685.14
[INFO 2017-06-25 20:31:14,726 main.py:50] epoch 4893, training loss: 16832.54, average training loss: 17737.05, base loss: 23682.45
[INFO 2017-06-25 20:31:19,038 main.py:50] epoch 4894, training loss: 16290.95, average training loss: 17740.18, base loss: 23686.44
[INFO 2017-06-25 20:31:23,306 main.py:50] epoch 4895, training loss: 22364.96, average training loss: 17749.22, base loss: 23701.08
[INFO 2017-06-25 20:31:27,577 main.py:50] epoch 4896, training loss: 13951.19, average training loss: 17746.14, base loss: 23698.01
[INFO 2017-06-25 20:31:31,842 main.py:50] epoch 4897, training loss: 15851.42, average training loss: 17743.32, base loss: 23691.61
[INFO 2017-06-25 20:31:36,107 main.py:50] epoch 4898, training loss: 22287.54, average training loss: 17753.40, base loss: 23705.32
[INFO 2017-06-25 20:31:40,380 main.py:50] epoch 4899, training loss: 16411.15, average training loss: 17751.77, base loss: 23702.04
[INFO 2017-06-25 20:31:44,670 main.py:50] epoch 4900, training loss: 17443.14, average training loss: 17753.82, base loss: 23705.35
[INFO 2017-06-25 20:31:48,950 main.py:50] epoch 4901, training loss: 19357.34, average training loss: 17753.67, base loss: 23707.10
[INFO 2017-06-25 20:31:53,232 main.py:50] epoch 4902, training loss: 22508.52, average training loss: 17755.12, base loss: 23708.96
[INFO 2017-06-25 20:31:57,504 main.py:50] epoch 4903, training loss: 18222.50, average training loss: 17755.18, base loss: 23709.75
[INFO 2017-06-25 20:32:01,787 main.py:50] epoch 4904, training loss: 17881.26, average training loss: 17755.72, base loss: 23708.48
[INFO 2017-06-25 20:32:06,086 main.py:50] epoch 4905, training loss: 15875.50, average training loss: 17752.55, base loss: 23704.84
[INFO 2017-06-25 20:32:10,370 main.py:50] epoch 4906, training loss: 18339.13, average training loss: 17754.28, base loss: 23706.77
[INFO 2017-06-25 20:32:14,608 main.py:50] epoch 4907, training loss: 15551.71, average training loss: 17752.44, base loss: 23706.20
[INFO 2017-06-25 20:32:18,885 main.py:50] epoch 4908, training loss: 14622.68, average training loss: 17752.45, base loss: 23705.12
[INFO 2017-06-25 20:32:23,155 main.py:50] epoch 4909, training loss: 15995.03, average training loss: 17751.73, base loss: 23706.76
[INFO 2017-06-25 20:32:27,420 main.py:50] epoch 4910, training loss: 16882.88, average training loss: 17749.00, base loss: 23701.55
[INFO 2017-06-25 20:32:31,695 main.py:50] epoch 4911, training loss: 20899.63, average training loss: 17753.90, base loss: 23704.99
[INFO 2017-06-25 20:32:35,960 main.py:50] epoch 4912, training loss: 18108.98, average training loss: 17754.52, base loss: 23701.16
[INFO 2017-06-25 20:32:40,246 main.py:50] epoch 4913, training loss: 20001.54, average training loss: 17755.01, base loss: 23701.23
[INFO 2017-06-25 20:32:44,534 main.py:50] epoch 4914, training loss: 15885.34, average training loss: 17752.17, base loss: 23693.41
[INFO 2017-06-25 20:32:48,832 main.py:50] epoch 4915, training loss: 14732.55, average training loss: 17748.82, base loss: 23691.18
[INFO 2017-06-25 20:32:53,135 main.py:50] epoch 4916, training loss: 18980.19, average training loss: 17750.54, base loss: 23694.42
[INFO 2017-06-25 20:32:57,430 main.py:50] epoch 4917, training loss: 14633.22, average training loss: 17748.11, base loss: 23690.58
[INFO 2017-06-25 20:33:01,740 main.py:50] epoch 4918, training loss: 20344.57, average training loss: 17752.68, base loss: 23698.77
[INFO 2017-06-25 20:33:06,032 main.py:50] epoch 4919, training loss: 15522.85, average training loss: 17751.79, base loss: 23698.10
[INFO 2017-06-25 20:33:10,333 main.py:50] epoch 4920, training loss: 20089.24, average training loss: 17749.28, base loss: 23695.06
[INFO 2017-06-25 20:33:14,636 main.py:50] epoch 4921, training loss: 18170.22, average training loss: 17748.20, base loss: 23695.49
[INFO 2017-06-25 20:33:18,921 main.py:50] epoch 4922, training loss: 19497.17, average training loss: 17753.49, base loss: 23703.86
[INFO 2017-06-25 20:33:23,233 main.py:50] epoch 4923, training loss: 15868.88, average training loss: 17753.48, base loss: 23704.45
[INFO 2017-06-25 20:33:27,519 main.py:50] epoch 4924, training loss: 16189.68, average training loss: 17751.80, base loss: 23701.21
[INFO 2017-06-25 20:33:31,817 main.py:50] epoch 4925, training loss: 16050.10, average training loss: 17746.84, base loss: 23695.37
[INFO 2017-06-25 20:33:36,128 main.py:50] epoch 4926, training loss: 17542.56, average training loss: 17744.51, base loss: 23691.14
[INFO 2017-06-25 20:33:40,407 main.py:50] epoch 4927, training loss: 19034.14, average training loss: 17745.36, base loss: 23692.54
[INFO 2017-06-25 20:33:44,696 main.py:50] epoch 4928, training loss: 16668.18, average training loss: 17746.84, base loss: 23693.39
[INFO 2017-06-25 20:33:48,963 main.py:50] epoch 4929, training loss: 14847.83, average training loss: 17747.78, base loss: 23695.98
[INFO 2017-06-25 20:33:53,264 main.py:50] epoch 4930, training loss: 19638.14, average training loss: 17750.03, base loss: 23701.12
[INFO 2017-06-25 20:33:57,544 main.py:50] epoch 4931, training loss: 15225.28, average training loss: 17746.23, base loss: 23698.60
[INFO 2017-06-25 20:34:01,848 main.py:50] epoch 4932, training loss: 19442.52, average training loss: 17747.92, base loss: 23702.37
[INFO 2017-06-25 20:34:06,127 main.py:50] epoch 4933, training loss: 17262.38, average training loss: 17749.10, base loss: 23706.04
[INFO 2017-06-25 20:34:10,447 main.py:50] epoch 4934, training loss: 17192.85, average training loss: 17747.73, base loss: 23704.97
[INFO 2017-06-25 20:34:14,715 main.py:50] epoch 4935, training loss: 19347.44, average training loss: 17749.59, base loss: 23707.34
[INFO 2017-06-25 20:34:19,026 main.py:50] epoch 4936, training loss: 12700.84, average training loss: 17742.42, base loss: 23696.88
[INFO 2017-06-25 20:34:23,340 main.py:50] epoch 4937, training loss: 18246.36, average training loss: 17744.50, base loss: 23696.97
[INFO 2017-06-25 20:34:27,645 main.py:50] epoch 4938, training loss: 14817.53, average training loss: 17741.39, base loss: 23693.38
[INFO 2017-06-25 20:34:31,949 main.py:50] epoch 4939, training loss: 15235.39, average training loss: 17739.67, base loss: 23689.76
[INFO 2017-06-25 20:34:36,256 main.py:50] epoch 4940, training loss: 16128.25, average training loss: 17741.15, base loss: 23693.10
[INFO 2017-06-25 20:34:40,553 main.py:50] epoch 4941, training loss: 16485.53, average training loss: 17736.63, base loss: 23686.95
[INFO 2017-06-25 20:34:44,853 main.py:50] epoch 4942, training loss: 21188.21, average training loss: 17738.14, base loss: 23692.08
[INFO 2017-06-25 20:34:49,153 main.py:50] epoch 4943, training loss: 15582.60, average training loss: 17740.77, base loss: 23695.59
[INFO 2017-06-25 20:34:53,435 main.py:50] epoch 4944, training loss: 21147.29, average training loss: 17743.90, base loss: 23700.95
[INFO 2017-06-25 20:34:57,722 main.py:50] epoch 4945, training loss: 19352.45, average training loss: 17747.10, base loss: 23707.13
[INFO 2017-06-25 20:35:01,998 main.py:50] epoch 4946, training loss: 19950.02, average training loss: 17750.18, base loss: 23711.34
[INFO 2017-06-25 20:35:06,275 main.py:50] epoch 4947, training loss: 14604.42, average training loss: 17748.33, base loss: 23708.51
[INFO 2017-06-25 20:35:10,548 main.py:50] epoch 4948, training loss: 16965.12, average training loss: 17742.51, base loss: 23699.86
[INFO 2017-06-25 20:35:14,833 main.py:50] epoch 4949, training loss: 21460.33, average training loss: 17748.39, base loss: 23708.96
[INFO 2017-06-25 20:35:19,127 main.py:50] epoch 4950, training loss: 19413.14, average training loss: 17750.34, base loss: 23710.67
[INFO 2017-06-25 20:35:23,445 main.py:50] epoch 4951, training loss: 15709.78, average training loss: 17748.16, base loss: 23707.38
[INFO 2017-06-25 20:35:27,706 main.py:50] epoch 4952, training loss: 18505.72, average training loss: 17749.99, base loss: 23710.20
[INFO 2017-06-25 20:35:31,983 main.py:50] epoch 4953, training loss: 18517.74, average training loss: 17751.53, base loss: 23709.79
[INFO 2017-06-25 20:35:36,268 main.py:50] epoch 4954, training loss: 20285.13, average training loss: 17755.15, base loss: 23715.58
[INFO 2017-06-25 20:35:40,561 main.py:50] epoch 4955, training loss: 19459.13, average training loss: 17756.79, base loss: 23715.96
[INFO 2017-06-25 20:35:44,831 main.py:50] epoch 4956, training loss: 20717.43, average training loss: 17758.71, base loss: 23718.00
[INFO 2017-06-25 20:35:49,097 main.py:50] epoch 4957, training loss: 17478.76, average training loss: 17757.08, base loss: 23712.21
[INFO 2017-06-25 20:35:53,377 main.py:50] epoch 4958, training loss: 20774.71, average training loss: 17762.28, base loss: 23720.59
[INFO 2017-06-25 20:35:57,677 main.py:50] epoch 4959, training loss: 17074.57, average training loss: 17761.19, base loss: 23718.18
[INFO 2017-06-25 20:36:01,939 main.py:50] epoch 4960, training loss: 13589.30, average training loss: 17755.59, base loss: 23711.20
[INFO 2017-06-25 20:36:06,247 main.py:50] epoch 4961, training loss: 20621.87, average training loss: 17754.13, base loss: 23709.29
[INFO 2017-06-25 20:36:10,539 main.py:50] epoch 4962, training loss: 17788.68, average training loss: 17755.12, base loss: 23711.86
[INFO 2017-06-25 20:36:14,854 main.py:50] epoch 4963, training loss: 19774.57, average training loss: 17758.01, base loss: 23715.05
[INFO 2017-06-25 20:36:19,160 main.py:50] epoch 4964, training loss: 18945.22, average training loss: 17758.06, base loss: 23714.48
[INFO 2017-06-25 20:36:23,435 main.py:50] epoch 4965, training loss: 17393.78, average training loss: 17759.56, base loss: 23715.36
[INFO 2017-06-25 20:36:27,727 main.py:50] epoch 4966, training loss: 15484.63, average training loss: 17758.41, base loss: 23714.67
[INFO 2017-06-25 20:36:32,032 main.py:50] epoch 4967, training loss: 25681.62, average training loss: 17763.81, base loss: 23720.38
[INFO 2017-06-25 20:36:36,332 main.py:50] epoch 4968, training loss: 19225.87, average training loss: 17762.86, base loss: 23720.52
[INFO 2017-06-25 20:36:40,619 main.py:50] epoch 4969, training loss: 19199.30, average training loss: 17767.59, base loss: 23726.46
[INFO 2017-06-25 20:36:44,890 main.py:50] epoch 4970, training loss: 17246.85, average training loss: 17767.65, base loss: 23727.17
[INFO 2017-06-25 20:36:49,170 main.py:50] epoch 4971, training loss: 16734.76, average training loss: 17768.98, base loss: 23728.35
[INFO 2017-06-25 20:36:53,479 main.py:50] epoch 4972, training loss: 16431.30, average training loss: 17764.99, base loss: 23722.98
[INFO 2017-06-25 20:36:57,752 main.py:50] epoch 4973, training loss: 17933.80, average training loss: 17768.42, base loss: 23724.95
[INFO 2017-06-25 20:37:02,040 main.py:50] epoch 4974, training loss: 14888.67, average training loss: 17762.44, base loss: 23718.72
[INFO 2017-06-25 20:37:06,305 main.py:50] epoch 4975, training loss: 15902.91, average training loss: 17755.75, base loss: 23709.81
[INFO 2017-06-25 20:37:10,575 main.py:50] epoch 4976, training loss: 17535.94, average training loss: 17754.51, base loss: 23710.28
[INFO 2017-06-25 20:37:14,833 main.py:50] epoch 4977, training loss: 13672.89, average training loss: 17748.10, base loss: 23703.13
[INFO 2017-06-25 20:37:19,106 main.py:50] epoch 4978, training loss: 15743.69, average training loss: 17745.54, base loss: 23701.90
[INFO 2017-06-25 20:37:23,373 main.py:50] epoch 4979, training loss: 19313.66, average training loss: 17746.49, base loss: 23702.51
[INFO 2017-06-25 20:37:27,672 main.py:50] epoch 4980, training loss: 17672.23, average training loss: 17747.20, base loss: 23705.23
[INFO 2017-06-25 20:37:31,951 main.py:50] epoch 4981, training loss: 13084.81, average training loss: 17740.53, base loss: 23699.29
[INFO 2017-06-25 20:37:36,226 main.py:50] epoch 4982, training loss: 19337.15, average training loss: 17743.22, base loss: 23703.58
[INFO 2017-06-25 20:37:40,480 main.py:50] epoch 4983, training loss: 17658.35, average training loss: 17745.76, base loss: 23708.60
[INFO 2017-06-25 20:37:44,771 main.py:50] epoch 4984, training loss: 17862.32, average training loss: 17743.11, base loss: 23703.83
[INFO 2017-06-25 20:37:49,066 main.py:50] epoch 4985, training loss: 19757.35, average training loss: 17746.43, base loss: 23706.17
[INFO 2017-06-25 20:37:53,348 main.py:50] epoch 4986, training loss: 19362.37, average training loss: 17749.09, base loss: 23710.76
[INFO 2017-06-25 20:37:57,608 main.py:50] epoch 4987, training loss: 14356.65, average training loss: 17741.95, base loss: 23701.33
[INFO 2017-06-25 20:38:01,905 main.py:50] epoch 4988, training loss: 12829.51, average training loss: 17734.60, base loss: 23690.27
[INFO 2017-06-25 20:38:06,190 main.py:50] epoch 4989, training loss: 15761.26, average training loss: 17734.77, base loss: 23688.48
[INFO 2017-06-25 20:38:10,459 main.py:50] epoch 4990, training loss: 15297.09, average training loss: 17730.04, base loss: 23683.90
[INFO 2017-06-25 20:38:14,750 main.py:50] epoch 4991, training loss: 17175.47, average training loss: 17729.36, base loss: 23682.10
[INFO 2017-06-25 20:38:19,051 main.py:50] epoch 4992, training loss: 17066.31, average training loss: 17727.47, base loss: 23683.09
[INFO 2017-06-25 20:38:23,347 main.py:50] epoch 4993, training loss: 16807.84, average training loss: 17727.52, base loss: 23681.28
[INFO 2017-06-25 20:38:27,632 main.py:50] epoch 4994, training loss: 14686.21, average training loss: 17719.19, base loss: 23670.46
[INFO 2017-06-25 20:38:31,915 main.py:50] epoch 4995, training loss: 21644.47, average training loss: 17724.68, base loss: 23677.06
[INFO 2017-06-25 20:38:36,203 main.py:50] epoch 4996, training loss: 16883.37, average training loss: 17722.28, base loss: 23670.19
[INFO 2017-06-25 20:38:40,474 main.py:50] epoch 4997, training loss: 17644.78, average training loss: 17723.22, base loss: 23672.72
[INFO 2017-06-25 20:38:44,768 main.py:50] epoch 4998, training loss: 22109.30, average training loss: 17725.75, base loss: 23675.60
[INFO 2017-06-25 20:38:49,058 main.py:50] epoch 4999, training loss: 17662.86, average training loss: 17727.28, base loss: 23679.81
[INFO 2017-06-25 20:38:49,058 main.py:52] epoch 4999, testing
[INFO 2017-06-25 20:42:27,286 main.py:100] average testing loss: 17415.84
[INFO 2017-06-25 20:42:27,290 main.py:72] model save to ./model/final.pth
[INFO 2017-06-25 20:42:27,297 main.py:76] current best accuracy: 17415.84
[INFO 2017-06-25 20:42:31,589 main.py:50] epoch 5000, training loss: 16721.11, average training loss: 17726.08, base loss: 23675.78
[INFO 2017-06-25 20:42:35,889 main.py:50] epoch 5001, training loss: 17346.73, average training loss: 17724.46, base loss: 23672.95
[INFO 2017-06-25 20:42:40,172 main.py:50] epoch 5002, training loss: 18958.88, average training loss: 17726.21, base loss: 23676.95
[INFO 2017-06-25 20:42:44,470 main.py:50] epoch 5003, training loss: 21244.14, average training loss: 17729.32, base loss: 23679.98
[INFO 2017-06-25 20:42:48,789 main.py:50] epoch 5004, training loss: 16443.04, average training loss: 17727.32, base loss: 23680.93
[INFO 2017-06-25 20:42:53,123 main.py:50] epoch 5005, training loss: 24466.47, average training loss: 17733.80, base loss: 23686.73
[INFO 2017-06-25 20:42:57,421 main.py:50] epoch 5006, training loss: 17115.82, average training loss: 17733.75, base loss: 23687.93
[INFO 2017-06-25 20:43:01,699 main.py:50] epoch 5007, training loss: 15927.72, average training loss: 17730.68, base loss: 23684.35
[INFO 2017-06-25 20:43:06,010 main.py:50] epoch 5008, training loss: 20271.44, average training loss: 17729.48, base loss: 23679.58
[INFO 2017-06-25 20:43:10,305 main.py:50] epoch 5009, training loss: 15120.38, average training loss: 17724.27, base loss: 23672.59
[INFO 2017-06-25 20:43:14,571 main.py:50] epoch 5010, training loss: 18404.16, average training loss: 17727.90, base loss: 23677.62
[INFO 2017-06-25 20:43:18,854 main.py:50] epoch 5011, training loss: 21135.17, average training loss: 17732.48, base loss: 23683.40
[INFO 2017-06-25 20:43:23,149 main.py:50] epoch 5012, training loss: 21689.94, average training loss: 17734.84, base loss: 23689.46
[INFO 2017-06-25 20:43:27,489 main.py:50] epoch 5013, training loss: 18870.51, average training loss: 17738.11, base loss: 23692.22
[INFO 2017-06-25 20:43:31,794 main.py:50] epoch 5014, training loss: 14684.89, average training loss: 17733.60, base loss: 23682.77
[INFO 2017-06-25 20:43:36,099 main.py:50] epoch 5015, training loss: 14799.44, average training loss: 17729.35, base loss: 23676.30
[INFO 2017-06-25 20:43:40,414 main.py:50] epoch 5016, training loss: 15430.50, average training loss: 17727.16, base loss: 23674.50
[INFO 2017-06-25 20:43:44,714 main.py:50] epoch 5017, training loss: 15560.80, average training loss: 17723.39, base loss: 23670.26
[INFO 2017-06-25 20:43:49,033 main.py:50] epoch 5018, training loss: 18295.44, average training loss: 17726.26, base loss: 23670.92
[INFO 2017-06-25 20:43:53,333 main.py:50] epoch 5019, training loss: 13833.91, average training loss: 17724.48, base loss: 23670.52
[INFO 2017-06-25 20:43:57,641 main.py:50] epoch 5020, training loss: 18911.59, average training loss: 17725.20, base loss: 23670.96
[INFO 2017-06-25 20:44:01,954 main.py:50] epoch 5021, training loss: 16502.60, average training loss: 17721.12, base loss: 23664.53
[INFO 2017-06-25 20:44:06,279 main.py:50] epoch 5022, training loss: 18066.64, average training loss: 17724.58, base loss: 23668.72
[INFO 2017-06-25 20:44:10,576 main.py:50] epoch 5023, training loss: 22963.58, average training loss: 17731.03, base loss: 23676.85
[INFO 2017-06-25 20:44:14,874 main.py:50] epoch 5024, training loss: 15902.51, average training loss: 17729.24, base loss: 23674.36
[INFO 2017-06-25 20:44:19,162 main.py:50] epoch 5025, training loss: 21331.96, average training loss: 17729.60, base loss: 23673.16
[INFO 2017-06-25 20:44:23,477 main.py:50] epoch 5026, training loss: 19675.58, average training loss: 17731.84, base loss: 23673.81
[INFO 2017-06-25 20:44:27,760 main.py:50] epoch 5027, training loss: 15496.41, average training loss: 17730.66, base loss: 23671.87
[INFO 2017-06-25 20:44:32,048 main.py:50] epoch 5028, training loss: 20669.49, average training loss: 17733.00, base loss: 23671.00
[INFO 2017-06-25 20:44:36,388 main.py:50] epoch 5029, training loss: 15735.67, average training loss: 17734.00, base loss: 23671.83
[INFO 2017-06-25 20:44:40,688 main.py:50] epoch 5030, training loss: 18846.33, average training loss: 17736.12, base loss: 23669.87
[INFO 2017-06-25 20:44:45,012 main.py:50] epoch 5031, training loss: 16193.08, average training loss: 17736.56, base loss: 23671.09
[INFO 2017-06-25 20:44:49,320 main.py:50] epoch 5032, training loss: 17296.44, average training loss: 17731.94, base loss: 23665.59
[INFO 2017-06-25 20:44:53,648 main.py:50] epoch 5033, training loss: 17802.88, average training loss: 17732.96, base loss: 23667.07
[INFO 2017-06-25 20:44:57,959 main.py:50] epoch 5034, training loss: 20242.45, average training loss: 17732.30, base loss: 23664.13
[INFO 2017-06-25 20:45:02,249 main.py:50] epoch 5035, training loss: 23235.54, average training loss: 17737.87, base loss: 23670.25
[INFO 2017-06-25 20:45:06,515 main.py:50] epoch 5036, training loss: 16607.53, average training loss: 17737.45, base loss: 23667.89
[INFO 2017-06-25 20:45:10,828 main.py:50] epoch 5037, training loss: 20111.87, average training loss: 17739.74, base loss: 23671.67
[INFO 2017-06-25 20:45:15,134 main.py:50] epoch 5038, training loss: 12792.81, average training loss: 17734.92, base loss: 23666.86
[INFO 2017-06-25 20:45:19,440 main.py:50] epoch 5039, training loss: 17148.88, average training loss: 17734.91, base loss: 23665.14
[INFO 2017-06-25 20:45:23,765 main.py:50] epoch 5040, training loss: 15239.80, average training loss: 17734.84, base loss: 23665.23
[INFO 2017-06-25 20:45:28,070 main.py:50] epoch 5041, training loss: 20391.36, average training loss: 17742.13, base loss: 23677.78
[INFO 2017-06-25 20:45:32,354 main.py:50] epoch 5042, training loss: 16497.71, average training loss: 17740.86, base loss: 23675.93
[INFO 2017-06-25 20:45:36,634 main.py:50] epoch 5043, training loss: 18211.44, average training loss: 17740.67, base loss: 23675.82
[INFO 2017-06-25 20:45:40,939 main.py:50] epoch 5044, training loss: 18326.73, average training loss: 17743.51, base loss: 23676.73
[INFO 2017-06-25 20:45:45,262 main.py:50] epoch 5045, training loss: 17042.81, average training loss: 17744.70, base loss: 23678.47
[INFO 2017-06-25 20:45:49,574 main.py:50] epoch 5046, training loss: 18225.69, average training loss: 17747.68, base loss: 23681.94
[INFO 2017-06-25 20:45:53,862 main.py:50] epoch 5047, training loss: 16050.40, average training loss: 17744.87, base loss: 23679.32
[INFO 2017-06-25 20:45:58,143 main.py:50] epoch 5048, training loss: 17472.02, average training loss: 17748.67, base loss: 23684.97
[INFO 2017-06-25 20:46:02,480 main.py:50] epoch 5049, training loss: 17817.15, average training loss: 17753.00, base loss: 23692.34
[INFO 2017-06-25 20:46:06,761 main.py:50] epoch 5050, training loss: 20469.37, average training loss: 17757.66, base loss: 23694.85
[INFO 2017-06-25 20:46:11,101 main.py:50] epoch 5051, training loss: 16401.44, average training loss: 17757.73, base loss: 23691.02
[INFO 2017-06-25 20:46:15,412 main.py:50] epoch 5052, training loss: 20914.30, average training loss: 17761.80, base loss: 23698.32
[INFO 2017-06-25 20:46:19,719 main.py:50] epoch 5053, training loss: 16537.77, average training loss: 17764.37, base loss: 23701.81
[INFO 2017-06-25 20:46:24,064 main.py:50] epoch 5054, training loss: 15555.98, average training loss: 17763.38, base loss: 23701.08
[INFO 2017-06-25 20:46:28,377 main.py:50] epoch 5055, training loss: 17845.57, average training loss: 17761.82, base loss: 23698.59
[INFO 2017-06-25 20:46:32,697 main.py:50] epoch 5056, training loss: 19112.84, average training loss: 17763.63, base loss: 23697.93
[INFO 2017-06-25 20:46:37,034 main.py:50] epoch 5057, training loss: 14882.65, average training loss: 17762.58, base loss: 23697.92
[INFO 2017-06-25 20:46:41,360 main.py:50] epoch 5058, training loss: 18800.27, average training loss: 17761.24, base loss: 23698.62
[INFO 2017-06-25 20:46:45,647 main.py:50] epoch 5059, training loss: 18890.20, average training loss: 17762.63, base loss: 23705.25
[INFO 2017-06-25 20:46:49,938 main.py:50] epoch 5060, training loss: 13040.19, average training loss: 17759.93, base loss: 23700.52
[INFO 2017-06-25 20:46:54,227 main.py:50] epoch 5061, training loss: 21446.83, average training loss: 17764.00, base loss: 23704.71
[INFO 2017-06-25 20:46:58,528 main.py:50] epoch 5062, training loss: 14492.27, average training loss: 17758.45, base loss: 23698.82
[INFO 2017-06-25 20:47:02,833 main.py:50] epoch 5063, training loss: 16838.31, average training loss: 17754.99, base loss: 23696.67
[INFO 2017-06-25 20:47:07,138 main.py:50] epoch 5064, training loss: 14963.68, average training loss: 17754.38, base loss: 23693.93
[INFO 2017-06-25 20:47:11,440 main.py:50] epoch 5065, training loss: 15504.46, average training loss: 17754.26, base loss: 23694.02
[INFO 2017-06-25 20:47:15,744 main.py:50] epoch 5066, training loss: 17173.60, average training loss: 17752.34, base loss: 23690.98
[INFO 2017-06-25 20:47:20,030 main.py:50] epoch 5067, training loss: 17655.99, average training loss: 17753.23, base loss: 23692.10
[INFO 2017-06-25 20:47:24,330 main.py:50] epoch 5068, training loss: 21817.23, average training loss: 17757.78, base loss: 23699.21
[INFO 2017-06-25 20:47:28,658 main.py:50] epoch 5069, training loss: 16775.65, average training loss: 17757.74, base loss: 23698.63
[INFO 2017-06-25 20:47:32,952 main.py:50] epoch 5070, training loss: 14059.07, average training loss: 17751.69, base loss: 23691.20
[INFO 2017-06-25 20:47:37,277 main.py:50] epoch 5071, training loss: 24143.73, average training loss: 17761.71, base loss: 23705.28
[INFO 2017-06-25 20:47:41,589 main.py:50] epoch 5072, training loss: 13797.00, average training loss: 17762.15, base loss: 23705.69
[INFO 2017-06-25 20:47:45,918 main.py:50] epoch 5073, training loss: 15383.10, average training loss: 17761.67, base loss: 23704.32
[INFO 2017-06-25 20:47:50,245 main.py:50] epoch 5074, training loss: 17293.71, average training loss: 17760.95, base loss: 23705.43
[INFO 2017-06-25 20:47:54,541 main.py:50] epoch 5075, training loss: 16915.95, average training loss: 17759.81, base loss: 23704.64
[INFO 2017-06-25 20:47:58,841 main.py:50] epoch 5076, training loss: 19778.35, average training loss: 17757.60, base loss: 23706.10
[INFO 2017-06-25 20:48:03,145 main.py:50] epoch 5077, training loss: 15947.62, average training loss: 17759.26, base loss: 23705.88
[INFO 2017-06-25 20:48:07,459 main.py:50] epoch 5078, training loss: 15679.85, average training loss: 17753.68, base loss: 23700.75
[INFO 2017-06-25 20:48:11,771 main.py:50] epoch 5079, training loss: 22425.59, average training loss: 17759.91, base loss: 23709.21
[INFO 2017-06-25 20:48:16,097 main.py:50] epoch 5080, training loss: 21998.69, average training loss: 17763.31, base loss: 23715.82
[INFO 2017-06-25 20:48:20,392 main.py:50] epoch 5081, training loss: 14328.18, average training loss: 17757.93, base loss: 23707.48
[INFO 2017-06-25 20:48:24,685 main.py:50] epoch 5082, training loss: 15506.55, average training loss: 17757.62, base loss: 23706.65
[INFO 2017-06-25 20:48:28,974 main.py:50] epoch 5083, training loss: 16225.01, average training loss: 17753.08, base loss: 23705.30
[INFO 2017-06-25 20:48:33,291 main.py:50] epoch 5084, training loss: 16056.33, average training loss: 17748.90, base loss: 23697.01
[INFO 2017-06-25 20:48:37,587 main.py:50] epoch 5085, training loss: 15133.60, average training loss: 17748.81, base loss: 23696.21
[INFO 2017-06-25 20:48:41,866 main.py:50] epoch 5086, training loss: 16847.58, average training loss: 17749.04, base loss: 23695.87
[INFO 2017-06-25 20:48:46,161 main.py:50] epoch 5087, training loss: 16350.04, average training loss: 17749.24, base loss: 23696.47
[INFO 2017-06-25 20:48:50,442 main.py:50] epoch 5088, training loss: 15340.55, average training loss: 17745.06, base loss: 23693.70
[INFO 2017-06-25 20:48:54,721 main.py:50] epoch 5089, training loss: 18708.33, average training loss: 17748.83, base loss: 23700.02
[INFO 2017-06-25 20:48:59,008 main.py:50] epoch 5090, training loss: 19524.81, average training loss: 17748.22, base loss: 23695.90
[INFO 2017-06-25 20:49:03,345 main.py:50] epoch 5091, training loss: 15271.46, average training loss: 17744.45, base loss: 23691.08
[INFO 2017-06-25 20:49:07,662 main.py:50] epoch 5092, training loss: 21758.90, average training loss: 17744.84, base loss: 23691.61
[INFO 2017-06-25 20:49:11,980 main.py:50] epoch 5093, training loss: 14497.43, average training loss: 17739.61, base loss: 23685.05
[INFO 2017-06-25 20:49:16,299 main.py:50] epoch 5094, training loss: 19117.49, average training loss: 17741.59, base loss: 23684.71
[INFO 2017-06-25 20:49:20,613 main.py:50] epoch 5095, training loss: 16183.82, average training loss: 17741.60, base loss: 23684.28
[INFO 2017-06-25 20:49:24,925 main.py:50] epoch 5096, training loss: 20147.05, average training loss: 17742.24, base loss: 23686.62
[INFO 2017-06-25 20:49:29,220 main.py:50] epoch 5097, training loss: 17510.90, average training loss: 17740.58, base loss: 23684.69
[INFO 2017-06-25 20:49:33,514 main.py:50] epoch 5098, training loss: 18510.12, average training loss: 17740.74, base loss: 23687.49
[INFO 2017-06-25 20:49:37,818 main.py:50] epoch 5099, training loss: 20970.77, average training loss: 17747.30, base loss: 23696.97
[INFO 2017-06-25 20:49:42,123 main.py:50] epoch 5100, training loss: 19508.10, average training loss: 17749.63, base loss: 23702.31
[INFO 2017-06-25 20:49:46,442 main.py:50] epoch 5101, training loss: 14812.73, average training loss: 17748.05, base loss: 23703.11
[INFO 2017-06-25 20:49:50,746 main.py:50] epoch 5102, training loss: 18605.20, average training loss: 17752.23, base loss: 23707.96
[INFO 2017-06-25 20:49:55,074 main.py:50] epoch 5103, training loss: 16533.82, average training loss: 17747.30, base loss: 23702.79
[INFO 2017-06-25 20:49:59,385 main.py:50] epoch 5104, training loss: 15745.16, average training loss: 17744.56, base loss: 23699.32
[INFO 2017-06-25 20:50:03,700 main.py:50] epoch 5105, training loss: 20145.13, average training loss: 17744.96, base loss: 23699.95
[INFO 2017-06-25 20:50:08,004 main.py:50] epoch 5106, training loss: 15640.71, average training loss: 17743.17, base loss: 23696.36
[INFO 2017-06-25 20:50:12,308 main.py:50] epoch 5107, training loss: 20170.02, average training loss: 17748.66, base loss: 23708.60
[INFO 2017-06-25 20:50:16,619 main.py:50] epoch 5108, training loss: 16918.88, average training loss: 17751.29, base loss: 23713.94
[INFO 2017-06-25 20:50:20,927 main.py:50] epoch 5109, training loss: 19430.45, average training loss: 17751.99, base loss: 23715.66
[INFO 2017-06-25 20:50:25,227 main.py:50] epoch 5110, training loss: 18562.23, average training loss: 17756.10, base loss: 23723.10
[INFO 2017-06-25 20:50:29,525 main.py:50] epoch 5111, training loss: 17356.24, average training loss: 17753.83, base loss: 23720.51
[INFO 2017-06-25 20:50:33,817 main.py:50] epoch 5112, training loss: 16518.67, average training loss: 17752.97, base loss: 23722.59
[INFO 2017-06-25 20:50:38,117 main.py:50] epoch 5113, training loss: 19424.88, average training loss: 17753.52, base loss: 23724.19
[INFO 2017-06-25 20:50:42,414 main.py:50] epoch 5114, training loss: 17221.95, average training loss: 17751.72, base loss: 23722.17
[INFO 2017-06-25 20:50:46,747 main.py:50] epoch 5115, training loss: 21183.23, average training loss: 17754.01, base loss: 23725.55
[INFO 2017-06-25 20:50:51,065 main.py:50] epoch 5116, training loss: 18408.20, average training loss: 17757.38, base loss: 23728.34
[INFO 2017-06-25 20:50:55,400 main.py:50] epoch 5117, training loss: 18796.66, average training loss: 17757.22, base loss: 23731.39
[INFO 2017-06-25 20:50:59,696 main.py:50] epoch 5118, training loss: 17641.35, average training loss: 17754.84, base loss: 23727.12
[INFO 2017-06-25 20:51:04,005 main.py:50] epoch 5119, training loss: 17592.97, average training loss: 17752.24, base loss: 23722.57
[INFO 2017-06-25 20:51:08,313 main.py:50] epoch 5120, training loss: 15210.59, average training loss: 17752.79, base loss: 23725.60
[INFO 2017-06-25 20:51:12,623 main.py:50] epoch 5121, training loss: 18104.46, average training loss: 17755.23, base loss: 23730.65
[INFO 2017-06-25 20:51:16,940 main.py:50] epoch 5122, training loss: 14345.07, average training loss: 17754.06, base loss: 23730.22
[INFO 2017-06-25 20:51:21,252 main.py:50] epoch 5123, training loss: 14572.00, average training loss: 17751.27, base loss: 23724.10
[INFO 2017-06-25 20:51:25,543 main.py:50] epoch 5124, training loss: 15583.76, average training loss: 17748.56, base loss: 23720.04
[INFO 2017-06-25 20:51:29,861 main.py:50] epoch 5125, training loss: 15347.13, average training loss: 17747.55, base loss: 23717.53
[INFO 2017-06-25 20:51:34,170 main.py:50] epoch 5126, training loss: 14939.58, average training loss: 17741.96, base loss: 23713.54
[INFO 2017-06-25 20:51:38,477 main.py:50] epoch 5127, training loss: 16724.54, average training loss: 17742.29, base loss: 23714.46
[INFO 2017-06-25 20:51:42,760 main.py:50] epoch 5128, training loss: 20919.55, average training loss: 17744.75, base loss: 23716.69
[INFO 2017-06-25 20:51:47,068 main.py:50] epoch 5129, training loss: 15915.00, average training loss: 17747.24, base loss: 23718.49
[INFO 2017-06-25 20:51:51,376 main.py:50] epoch 5130, training loss: 18134.70, average training loss: 17746.61, base loss: 23718.64
[INFO 2017-06-25 20:51:55,672 main.py:50] epoch 5131, training loss: 15873.95, average training loss: 17745.39, base loss: 23719.38
[INFO 2017-06-25 20:51:59,998 main.py:50] epoch 5132, training loss: 16661.16, average training loss: 17742.37, base loss: 23715.63
[INFO 2017-06-25 20:52:04,340 main.py:50] epoch 5133, training loss: 15849.38, average training loss: 17742.43, base loss: 23714.25
[INFO 2017-06-25 20:52:08,636 main.py:50] epoch 5134, training loss: 21533.15, average training loss: 17747.28, base loss: 23719.16
[INFO 2017-06-25 20:52:12,950 main.py:50] epoch 5135, training loss: 16714.71, average training loss: 17745.95, base loss: 23715.01
[INFO 2017-06-25 20:52:17,264 main.py:50] epoch 5136, training loss: 17784.31, average training loss: 17749.28, base loss: 23719.53
[INFO 2017-06-25 20:52:21,578 main.py:50] epoch 5137, training loss: 17584.50, average training loss: 17750.60, base loss: 23721.46
[INFO 2017-06-25 20:52:25,899 main.py:50] epoch 5138, training loss: 16434.01, average training loss: 17748.08, base loss: 23716.79
[INFO 2017-06-25 20:52:30,203 main.py:50] epoch 5139, training loss: 20324.79, average training loss: 17749.22, base loss: 23718.63
[INFO 2017-06-25 20:52:34,528 main.py:50] epoch 5140, training loss: 22425.21, average training loss: 17753.17, base loss: 23721.63
[INFO 2017-06-25 20:52:38,818 main.py:50] epoch 5141, training loss: 16043.67, average training loss: 17750.73, base loss: 23718.73
[INFO 2017-06-25 20:52:43,118 main.py:50] epoch 5142, training loss: 12618.47, average training loss: 17744.90, base loss: 23711.11
[INFO 2017-06-25 20:52:47,432 main.py:50] epoch 5143, training loss: 14161.13, average training loss: 17739.43, base loss: 23704.99
[INFO 2017-06-25 20:52:51,766 main.py:50] epoch 5144, training loss: 15683.57, average training loss: 17735.83, base loss: 23699.68
[INFO 2017-06-25 20:52:56,075 main.py:50] epoch 5145, training loss: 18256.30, average training loss: 17738.41, base loss: 23704.35
[INFO 2017-06-25 20:53:00,361 main.py:50] epoch 5146, training loss: 15268.63, average training loss: 17732.06, base loss: 23695.47
[INFO 2017-06-25 20:53:04,639 main.py:50] epoch 5147, training loss: 16420.01, average training loss: 17726.93, base loss: 23686.91
[INFO 2017-06-25 20:53:08,952 main.py:50] epoch 5148, training loss: 15214.49, average training loss: 17723.61, base loss: 23684.55
[INFO 2017-06-25 20:53:13,247 main.py:50] epoch 5149, training loss: 15523.43, average training loss: 17720.56, base loss: 23680.93
[INFO 2017-06-25 20:53:17,568 main.py:50] epoch 5150, training loss: 15912.98, average training loss: 17720.05, base loss: 23679.19
[INFO 2017-06-25 20:53:21,891 main.py:50] epoch 5151, training loss: 17115.75, average training loss: 17720.59, base loss: 23678.60
[INFO 2017-06-25 20:53:26,194 main.py:50] epoch 5152, training loss: 14706.03, average training loss: 17718.47, base loss: 23676.36
[INFO 2017-06-25 20:53:30,547 main.py:50] epoch 5153, training loss: 20599.74, average training loss: 17720.60, base loss: 23678.34
[INFO 2017-06-25 20:53:34,854 main.py:50] epoch 5154, training loss: 19212.70, average training loss: 17719.58, base loss: 23674.54
[INFO 2017-06-25 20:53:39,169 main.py:50] epoch 5155, training loss: 18531.58, average training loss: 17722.21, base loss: 23675.88
[INFO 2017-06-25 20:53:43,475 main.py:50] epoch 5156, training loss: 16441.94, average training loss: 17723.27, base loss: 23679.65
[INFO 2017-06-25 20:53:47,811 main.py:50] epoch 5157, training loss: 15233.51, average training loss: 17725.05, base loss: 23681.17
[INFO 2017-06-25 20:53:52,149 main.py:50] epoch 5158, training loss: 17527.67, average training loss: 17725.81, base loss: 23680.37
[INFO 2017-06-25 20:53:56,486 main.py:50] epoch 5159, training loss: 15874.55, average training loss: 17720.16, base loss: 23675.37
[INFO 2017-06-25 20:54:00,802 main.py:50] epoch 5160, training loss: 20577.08, average training loss: 17722.35, base loss: 23679.01
[INFO 2017-06-25 20:54:05,117 main.py:50] epoch 5161, training loss: 18083.24, average training loss: 17719.82, base loss: 23675.97
[INFO 2017-06-25 20:54:09,433 main.py:50] epoch 5162, training loss: 14641.77, average training loss: 17717.81, base loss: 23671.17
[INFO 2017-06-25 20:54:13,754 main.py:50] epoch 5163, training loss: 18686.27, average training loss: 17721.45, base loss: 23679.16
[INFO 2017-06-25 20:54:18,062 main.py:50] epoch 5164, training loss: 15858.20, average training loss: 17724.12, base loss: 23683.44
[INFO 2017-06-25 20:54:22,352 main.py:50] epoch 5165, training loss: 20535.28, average training loss: 17729.15, base loss: 23691.93
[INFO 2017-06-25 20:54:26,646 main.py:50] epoch 5166, training loss: 18406.75, average training loss: 17732.70, base loss: 23695.59
[INFO 2017-06-25 20:54:30,975 main.py:50] epoch 5167, training loss: 13683.44, average training loss: 17727.58, base loss: 23689.95
[INFO 2017-06-25 20:54:35,279 main.py:50] epoch 5168, training loss: 20295.05, average training loss: 17732.15, base loss: 23695.85
[INFO 2017-06-25 20:54:39,584 main.py:50] epoch 5169, training loss: 19614.70, average training loss: 17730.63, base loss: 23694.41
[INFO 2017-06-25 20:54:43,879 main.py:50] epoch 5170, training loss: 16091.84, average training loss: 17729.58, base loss: 23697.51
[INFO 2017-06-25 20:54:48,182 main.py:50] epoch 5171, training loss: 18368.13, average training loss: 17733.23, base loss: 23702.49
[INFO 2017-06-25 20:54:52,521 main.py:50] epoch 5172, training loss: 21009.91, average training loss: 17736.01, base loss: 23707.71
[INFO 2017-06-25 20:54:56,821 main.py:50] epoch 5173, training loss: 20580.15, average training loss: 17739.16, base loss: 23711.34
[INFO 2017-06-25 20:55:01,133 main.py:50] epoch 5174, training loss: 21949.52, average training loss: 17744.04, base loss: 23718.96
[INFO 2017-06-25 20:55:05,435 main.py:50] epoch 5175, training loss: 16809.00, average training loss: 17747.13, base loss: 23725.33
[INFO 2017-06-25 20:55:09,760 main.py:50] epoch 5176, training loss: 15576.03, average training loss: 17745.04, base loss: 23721.39
[INFO 2017-06-25 20:55:14,092 main.py:50] epoch 5177, training loss: 15221.88, average training loss: 17745.92, base loss: 23723.91
[INFO 2017-06-25 20:55:18,456 main.py:50] epoch 5178, training loss: 21470.37, average training loss: 17750.97, base loss: 23730.37
[INFO 2017-06-25 20:55:22,776 main.py:50] epoch 5179, training loss: 18342.52, average training loss: 17750.70, base loss: 23733.34
[INFO 2017-06-25 20:55:27,098 main.py:50] epoch 5180, training loss: 20655.83, average training loss: 17753.95, base loss: 23740.22
[INFO 2017-06-25 20:55:31,430 main.py:50] epoch 5181, training loss: 16173.50, average training loss: 17744.36, base loss: 23731.95
[INFO 2017-06-25 20:55:35,736 main.py:50] epoch 5182, training loss: 15143.53, average training loss: 17741.60, base loss: 23726.32
[INFO 2017-06-25 20:55:40,059 main.py:50] epoch 5183, training loss: 15884.56, average training loss: 17738.41, base loss: 23720.80
[INFO 2017-06-25 20:55:44,358 main.py:50] epoch 5184, training loss: 17243.46, average training loss: 17737.99, base loss: 23722.15
[INFO 2017-06-25 20:55:48,756 main.py:50] epoch 5185, training loss: 15990.33, average training loss: 17731.97, base loss: 23715.08
[INFO 2017-06-25 20:55:53,102 main.py:50] epoch 5186, training loss: 14082.00, average training loss: 17727.72, base loss: 23710.08
[INFO 2017-06-25 20:55:57,434 main.py:50] epoch 5187, training loss: 15588.12, average training loss: 17724.75, base loss: 23707.75
[INFO 2017-06-25 20:56:01,732 main.py:50] epoch 5188, training loss: 21184.51, average training loss: 17728.10, base loss: 23712.16
[INFO 2017-06-25 20:56:06,036 main.py:50] epoch 5189, training loss: 20392.41, average training loss: 17732.91, base loss: 23719.56
[INFO 2017-06-25 20:56:10,332 main.py:50] epoch 5190, training loss: 17963.90, average training loss: 17732.40, base loss: 23718.73
[INFO 2017-06-25 20:56:14,640 main.py:50] epoch 5191, training loss: 21339.75, average training loss: 17739.06, base loss: 23726.02
[INFO 2017-06-25 20:56:18,938 main.py:50] epoch 5192, training loss: 18486.20, average training loss: 17742.49, base loss: 23730.33
[INFO 2017-06-25 20:56:23,221 main.py:50] epoch 5193, training loss: 19966.54, average training loss: 17745.04, base loss: 23734.59
[INFO 2017-06-25 20:56:27,510 main.py:50] epoch 5194, training loss: 12445.39, average training loss: 17740.60, base loss: 23730.62
[INFO 2017-06-25 20:56:31,830 main.py:50] epoch 5195, training loss: 19459.29, average training loss: 17739.46, base loss: 23729.83
[INFO 2017-06-25 20:56:36,127 main.py:50] epoch 5196, training loss: 19596.01, average training loss: 17743.63, base loss: 23734.01
[INFO 2017-06-25 20:56:40,438 main.py:50] epoch 5197, training loss: 14098.97, average training loss: 17737.89, base loss: 23727.56
[INFO 2017-06-25 20:56:44,721 main.py:50] epoch 5198, training loss: 18524.54, average training loss: 17737.10, base loss: 23726.59
[INFO 2017-06-25 20:56:49,014 main.py:50] epoch 5199, training loss: 22327.74, average training loss: 17743.29, base loss: 23733.21
[INFO 2017-06-25 20:56:53,352 main.py:50] epoch 5200, training loss: 18571.53, average training loss: 17742.76, base loss: 23733.74
[INFO 2017-06-25 20:56:57,673 main.py:50] epoch 5201, training loss: 17300.33, average training loss: 17740.59, base loss: 23728.84
[INFO 2017-06-25 20:57:01,976 main.py:50] epoch 5202, training loss: 18629.51, average training loss: 17742.62, base loss: 23732.98
[INFO 2017-06-25 20:57:06,283 main.py:50] epoch 5203, training loss: 20813.70, average training loss: 17746.55, base loss: 23738.53
[INFO 2017-06-25 20:57:10,566 main.py:50] epoch 5204, training loss: 14288.15, average training loss: 17744.09, base loss: 23735.65
[INFO 2017-06-25 20:57:14,877 main.py:50] epoch 5205, training loss: 18113.94, average training loss: 17745.63, base loss: 23733.92
