[INFO 2017-06-29 00:00:28,188 main.py:175] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-29 00:00:33,330 main.py:57] epoch 0, training loss: 41037.96, average training loss: 41037.96, base loss: 16504.37
[INFO 2017-06-29 00:00:35,806 main.py:57] epoch 1, training loss: 31411.47, average training loss: 36224.71, base loss: 15599.58
[INFO 2017-06-29 00:00:38,175 main.py:57] epoch 2, training loss: 27788.85, average training loss: 33412.76, base loss: 15525.68
[INFO 2017-06-29 00:00:40,568 main.py:57] epoch 3, training loss: 22490.31, average training loss: 30682.15, base loss: 15376.21
[INFO 2017-06-29 00:00:43,014 main.py:57] epoch 4, training loss: 20478.17, average training loss: 28641.35, base loss: 15712.44
[INFO 2017-06-29 00:00:45,562 main.py:57] epoch 5, training loss: 19756.32, average training loss: 27160.51, base loss: 15743.54
[INFO 2017-06-29 00:00:47,974 main.py:57] epoch 6, training loss: 17481.79, average training loss: 25777.84, base loss: 15720.69
[INFO 2017-06-29 00:00:50,363 main.py:57] epoch 7, training loss: 17364.83, average training loss: 24726.21, base loss: 15965.87
[INFO 2017-06-29 00:00:52,836 main.py:57] epoch 8, training loss: 16452.84, average training loss: 23806.95, base loss: 16061.25
[INFO 2017-06-29 00:00:55,721 main.py:57] epoch 9, training loss: 16373.16, average training loss: 23063.57, base loss: 16206.34
[INFO 2017-06-29 00:00:58,821 main.py:57] epoch 10, training loss: 13539.40, average training loss: 22197.74, base loss: 16144.26
[INFO 2017-06-29 00:01:01,874 main.py:57] epoch 11, training loss: 12694.42, average training loss: 21405.79, base loss: 16017.31
[INFO 2017-06-29 00:01:04,924 main.py:57] epoch 12, training loss: 14544.18, average training loss: 20877.98, base loss: 16126.51
[INFO 2017-06-29 00:01:07,965 main.py:57] epoch 13, training loss: 12450.61, average training loss: 20276.02, base loss: 16078.17
[INFO 2017-06-29 00:01:11,003 main.py:57] epoch 14, training loss: 12692.46, average training loss: 19770.45, base loss: 16065.73
[INFO 2017-06-29 00:01:14,039 main.py:57] epoch 15, training loss: 11333.61, average training loss: 19243.15, base loss: 15979.57
[INFO 2017-06-29 00:01:17,069 main.py:57] epoch 16, training loss: 12170.42, average training loss: 18827.11, base loss: 15971.29
[INFO 2017-06-29 00:01:20,162 main.py:57] epoch 17, training loss: 10704.23, average training loss: 18375.84, base loss: 15872.76
[INFO 2017-06-29 00:01:23,215 main.py:57] epoch 18, training loss: 12291.99, average training loss: 18055.63, base loss: 15902.68
[INFO 2017-06-29 00:01:26,299 main.py:57] epoch 19, training loss: 11801.53, average training loss: 17742.93, base loss: 15902.08
[INFO 2017-06-29 00:01:29,418 main.py:57] epoch 20, training loss: 12021.25, average training loss: 17470.47, base loss: 15905.21
[INFO 2017-06-29 00:01:32,503 main.py:57] epoch 21, training loss: 11493.94, average training loss: 17198.81, base loss: 15886.84
[INFO 2017-06-29 00:01:35,548 main.py:57] epoch 22, training loss: 9991.77, average training loss: 16885.46, base loss: 15803.74
[INFO 2017-06-29 00:01:38,604 main.py:57] epoch 23, training loss: 11595.31, average training loss: 16665.03, base loss: 15808.54
[INFO 2017-06-29 00:01:41,704 main.py:57] epoch 24, training loss: 12731.84, average training loss: 16507.71, base loss: 15843.48
[INFO 2017-06-29 00:01:44,708 main.py:57] epoch 25, training loss: 12947.34, average training loss: 16370.77, base loss: 15877.31
[INFO 2017-06-29 00:01:47,763 main.py:57] epoch 26, training loss: 9316.00, average training loss: 16109.48, base loss: 15768.61
[INFO 2017-06-29 00:01:50,806 main.py:57] epoch 27, training loss: 11984.30, average training loss: 15962.15, base loss: 15782.49
[INFO 2017-06-29 00:01:53,847 main.py:57] epoch 28, training loss: 14152.02, average training loss: 15899.74, base loss: 15887.80
[INFO 2017-06-29 00:01:56,915 main.py:57] epoch 29, training loss: 13522.10, average training loss: 15820.48, base loss: 15961.85
[INFO 2017-06-29 00:01:59,918 main.py:57] epoch 30, training loss: 11963.66, average training loss: 15696.07, base loss: 15973.40
[INFO 2017-06-29 00:02:02,983 main.py:57] epoch 31, training loss: 9804.84, average training loss: 15511.97, base loss: 15896.93
[INFO 2017-06-29 00:02:06,101 main.py:57] epoch 32, training loss: 10557.03, average training loss: 15361.82, base loss: 15859.72
[INFO 2017-06-29 00:02:09,116 main.py:57] epoch 33, training loss: 11869.25, average training loss: 15259.09, base loss: 15877.67
[INFO 2017-06-29 00:02:12,107 main.py:57] epoch 34, training loss: 11609.87, average training loss: 15154.83, base loss: 15885.79
[INFO 2017-06-29 00:02:15,164 main.py:57] epoch 35, training loss: 10219.14, average training loss: 15017.73, base loss: 15835.07
[INFO 2017-06-29 00:02:18,328 main.py:57] epoch 36, training loss: 10309.62, average training loss: 14890.48, base loss: 15788.88
[INFO 2017-06-29 00:02:21,343 main.py:57] epoch 37, training loss: 12882.84, average training loss: 14837.65, base loss: 15842.28
[INFO 2017-06-29 00:02:24,395 main.py:57] epoch 38, training loss: 10365.34, average training loss: 14722.97, base loss: 15809.83
[INFO 2017-06-29 00:02:27,456 main.py:57] epoch 39, training loss: 10056.10, average training loss: 14606.30, base loss: 15765.07
[INFO 2017-06-29 00:02:30,511 main.py:57] epoch 40, training loss: 11753.82, average training loss: 14536.73, base loss: 15762.37
[INFO 2017-06-29 00:02:33,498 main.py:57] epoch 41, training loss: 10737.05, average training loss: 14446.26, base loss: 15746.34
[INFO 2017-06-29 00:02:36,506 main.py:57] epoch 42, training loss: 12062.58, average training loss: 14390.83, base loss: 15773.06
[INFO 2017-06-29 00:02:39,571 main.py:57] epoch 43, training loss: 11565.94, average training loss: 14326.62, base loss: 15778.23
[INFO 2017-06-29 00:02:42,610 main.py:57] epoch 44, training loss: 12961.93, average training loss: 14296.30, base loss: 15820.00
[INFO 2017-06-29 00:02:45,667 main.py:57] epoch 45, training loss: 11316.06, average training loss: 14231.51, base loss: 15820.46
[INFO 2017-06-29 00:02:48,753 main.py:57] epoch 46, training loss: 12141.45, average training loss: 14187.04, base loss: 15846.33
[INFO 2017-06-29 00:02:51,733 main.py:57] epoch 47, training loss: 11530.32, average training loss: 14131.69, base loss: 15853.98
[INFO 2017-06-29 00:02:54,798 main.py:57] epoch 48, training loss: 10454.54, average training loss: 14056.65, base loss: 15824.07
[INFO 2017-06-29 00:02:57,864 main.py:57] epoch 49, training loss: 11062.88, average training loss: 13996.77, base loss: 15822.94
[INFO 2017-06-29 00:03:00,859 main.py:57] epoch 50, training loss: 12039.96, average training loss: 13958.40, base loss: 15835.93
[INFO 2017-06-29 00:03:03,883 main.py:57] epoch 51, training loss: 12486.26, average training loss: 13930.09, base loss: 15853.62
[INFO 2017-06-29 00:03:07,011 main.py:57] epoch 52, training loss: 11583.86, average training loss: 13885.83, base loss: 15848.71
[INFO 2017-06-29 00:03:10,149 main.py:57] epoch 53, training loss: 11450.34, average training loss: 13840.72, base loss: 15851.30
[INFO 2017-06-29 00:03:13,218 main.py:57] epoch 54, training loss: 11230.42, average training loss: 13793.26, base loss: 15848.78
[INFO 2017-06-29 00:03:16,347 main.py:57] epoch 55, training loss: 11795.77, average training loss: 13757.59, base loss: 15858.56
[INFO 2017-06-29 00:03:19,463 main.py:57] epoch 56, training loss: 12494.43, average training loss: 13735.43, base loss: 15880.98
[INFO 2017-06-29 00:03:22,539 main.py:57] epoch 57, training loss: 11712.27, average training loss: 13700.55, base loss: 15887.23
[INFO 2017-06-29 00:03:25,666 main.py:57] epoch 58, training loss: 11249.28, average training loss: 13659.00, base loss: 15886.83
[INFO 2017-06-29 00:03:28,699 main.py:57] epoch 59, training loss: 12746.26, average training loss: 13643.79, base loss: 15925.37
[INFO 2017-06-29 00:03:31,793 main.py:57] epoch 60, training loss: 12266.84, average training loss: 13621.22, base loss: 15942.22
[INFO 2017-06-29 00:03:34,842 main.py:57] epoch 61, training loss: 11723.15, average training loss: 13590.61, base loss: 15946.25
[INFO 2017-06-29 00:03:37,913 main.py:57] epoch 62, training loss: 12873.39, average training loss: 13579.22, base loss: 15977.94
[INFO 2017-06-29 00:03:40,960 main.py:57] epoch 63, training loss: 11666.97, average training loss: 13549.34, base loss: 15996.29
[INFO 2017-06-29 00:03:44,005 main.py:57] epoch 64, training loss: 12204.18, average training loss: 13528.65, base loss: 16014.96
[INFO 2017-06-29 00:03:47,097 main.py:57] epoch 65, training loss: 10139.75, average training loss: 13477.30, base loss: 16010.19
[INFO 2017-06-29 00:03:50,240 main.py:57] epoch 66, training loss: 11029.15, average training loss: 13440.76, base loss: 16004.02
[INFO 2017-06-29 00:03:53,265 main.py:57] epoch 67, training loss: 12204.22, average training loss: 13422.58, base loss: 16024.41
[INFO 2017-06-29 00:03:56,368 main.py:57] epoch 68, training loss: 10135.20, average training loss: 13374.93, base loss: 16009.61
[INFO 2017-06-29 00:03:59,402 main.py:57] epoch 69, training loss: 10154.75, average training loss: 13328.93, base loss: 15986.04
[INFO 2017-06-29 00:04:02,480 main.py:57] epoch 70, training loss: 13173.49, average training loss: 13326.74, base loss: 16035.40
[INFO 2017-06-29 00:04:05,459 main.py:57] epoch 71, training loss: 11890.68, average training loss: 13306.80, base loss: 16037.81
[INFO 2017-06-29 00:04:08,516 main.py:57] epoch 72, training loss: 10510.39, average training loss: 13268.49, base loss: 16026.66
[INFO 2017-06-29 00:04:11,541 main.py:57] epoch 73, training loss: 11191.70, average training loss: 13240.42, base loss: 16030.73
[INFO 2017-06-29 00:04:14,555 main.py:57] epoch 74, training loss: 10336.09, average training loss: 13201.70, base loss: 16019.81
[INFO 2017-06-29 00:04:17,663 main.py:57] epoch 75, training loss: 10035.55, average training loss: 13160.04, base loss: 16010.84
[INFO 2017-06-29 00:04:20,734 main.py:57] epoch 76, training loss: 10569.31, average training loss: 13126.39, base loss: 16003.62
[INFO 2017-06-29 00:04:23,844 main.py:57] epoch 77, training loss: 12171.66, average training loss: 13114.15, base loss: 16024.71
[INFO 2017-06-29 00:04:26,926 main.py:57] epoch 78, training loss: 12421.54, average training loss: 13105.39, base loss: 16050.49
[INFO 2017-06-29 00:04:29,957 main.py:57] epoch 79, training loss: 11918.27, average training loss: 13090.55, base loss: 16065.51
[INFO 2017-06-29 00:04:33,026 main.py:57] epoch 80, training loss: 10423.56, average training loss: 13057.62, base loss: 16055.23
[INFO 2017-06-29 00:04:36,039 main.py:57] epoch 81, training loss: 10325.32, average training loss: 13024.30, base loss: 16047.32
[INFO 2017-06-29 00:04:39,085 main.py:57] epoch 82, training loss: 11299.22, average training loss: 13003.52, base loss: 16053.42
[INFO 2017-06-29 00:04:42,112 main.py:57] epoch 83, training loss: 10392.65, average training loss: 12972.44, base loss: 16050.28
[INFO 2017-06-29 00:04:45,132 main.py:57] epoch 84, training loss: 12263.36, average training loss: 12964.09, base loss: 16070.28
[INFO 2017-06-29 00:04:48,154 main.py:57] epoch 85, training loss: 9712.51, average training loss: 12926.28, base loss: 16060.55
[INFO 2017-06-29 00:04:51,318 main.py:57] epoch 86, training loss: 10741.03, average training loss: 12901.17, base loss: 16061.41
[INFO 2017-06-29 00:04:54,346 main.py:57] epoch 87, training loss: 10060.35, average training loss: 12868.88, base loss: 16048.31
[INFO 2017-06-29 00:04:57,370 main.py:57] epoch 88, training loss: 9458.50, average training loss: 12830.57, base loss: 16030.85
[INFO 2017-06-29 00:05:00,439 main.py:57] epoch 89, training loss: 9833.59, average training loss: 12797.27, base loss: 16016.86
[INFO 2017-06-29 00:05:03,537 main.py:57] epoch 90, training loss: 12441.26, average training loss: 12793.35, base loss: 16039.05
[INFO 2017-06-29 00:05:06,575 main.py:57] epoch 91, training loss: 8627.88, average training loss: 12748.08, base loss: 16006.95
[INFO 2017-06-29 00:05:09,629 main.py:57] epoch 92, training loss: 9914.45, average training loss: 12717.61, base loss: 15992.81
[INFO 2017-06-29 00:05:12,653 main.py:57] epoch 93, training loss: 10457.43, average training loss: 12693.56, base loss: 15996.76
[INFO 2017-06-29 00:05:15,701 main.py:57] epoch 94, training loss: 10771.23, average training loss: 12673.33, base loss: 16000.98
[INFO 2017-06-29 00:05:18,757 main.py:57] epoch 95, training loss: 12004.10, average training loss: 12666.36, base loss: 16018.29
[INFO 2017-06-29 00:05:21,795 main.py:57] epoch 96, training loss: 10762.78, average training loss: 12646.73, base loss: 16019.69
[INFO 2017-06-29 00:05:24,876 main.py:57] epoch 97, training loss: 11250.07, average training loss: 12632.48, base loss: 16031.93
[INFO 2017-06-29 00:05:27,986 main.py:57] epoch 98, training loss: 9637.37, average training loss: 12602.23, base loss: 16021.56
[INFO 2017-06-29 00:05:31,092 main.py:57] epoch 99, training loss: 11902.65, average training loss: 12595.23, base loss: 16048.05
[INFO 2017-06-29 00:05:31,093 main.py:59] epoch 99, testing
[INFO 2017-06-29 00:05:43,994 main.py:104] average testing loss: 11519.73, base loss: 17147.14
[INFO 2017-06-29 00:05:43,994 main.py:105] improve_loss: 5627.41, improve_percent: 0.33
[INFO 2017-06-29 00:05:43,995 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:05:44,031 main.py:71] current best improved percent: 0.33
[INFO 2017-06-29 00:05:47,122 main.py:57] epoch 100, training loss: 11154.68, average training loss: 12580.97, base loss: 16060.45
[INFO 2017-06-29 00:05:50,169 main.py:57] epoch 101, training loss: 9257.03, average training loss: 12548.38, base loss: 16048.75
[INFO 2017-06-29 00:05:53,211 main.py:57] epoch 102, training loss: 10618.61, average training loss: 12529.65, base loss: 16055.19
[INFO 2017-06-29 00:05:56,251 main.py:57] epoch 103, training loss: 11389.96, average training loss: 12518.69, base loss: 16064.65
[INFO 2017-06-29 00:05:59,289 main.py:57] epoch 104, training loss: 10781.40, average training loss: 12502.14, base loss: 16069.71
[INFO 2017-06-29 00:06:02,394 main.py:57] epoch 105, training loss: 9713.92, average training loss: 12475.84, base loss: 16063.28
[INFO 2017-06-29 00:06:05,388 main.py:57] epoch 106, training loss: 10510.09, average training loss: 12457.47, base loss: 16069.75
[INFO 2017-06-29 00:06:08,441 main.py:57] epoch 107, training loss: 11107.15, average training loss: 12444.96, base loss: 16077.52
[INFO 2017-06-29 00:06:11,445 main.py:57] epoch 108, training loss: 10188.04, average training loss: 12424.26, base loss: 16082.19
[INFO 2017-06-29 00:06:14,411 main.py:57] epoch 109, training loss: 9956.16, average training loss: 12401.82, base loss: 16077.13
[INFO 2017-06-29 00:06:17,400 main.py:57] epoch 110, training loss: 9650.52, average training loss: 12377.03, base loss: 16072.33
[INFO 2017-06-29 00:06:20,499 main.py:57] epoch 111, training loss: 11129.65, average training loss: 12365.90, base loss: 16086.77
[INFO 2017-06-29 00:06:23,605 main.py:57] epoch 112, training loss: 10730.76, average training loss: 12351.43, base loss: 16103.09
[INFO 2017-06-29 00:06:26,630 main.py:57] epoch 113, training loss: 9753.38, average training loss: 12328.64, base loss: 16098.21
[INFO 2017-06-29 00:06:29,714 main.py:57] epoch 114, training loss: 10312.14, average training loss: 12311.10, base loss: 16102.16
[INFO 2017-06-29 00:06:32,769 main.py:57] epoch 115, training loss: 9204.18, average training loss: 12284.32, base loss: 16090.71
[INFO 2017-06-29 00:06:35,950 main.py:57] epoch 116, training loss: 10487.96, average training loss: 12268.96, base loss: 16095.37
[INFO 2017-06-29 00:06:39,044 main.py:57] epoch 117, training loss: 9957.51, average training loss: 12249.38, base loss: 16095.13
[INFO 2017-06-29 00:06:42,104 main.py:57] epoch 118, training loss: 10069.57, average training loss: 12231.06, base loss: 16101.88
[INFO 2017-06-29 00:06:45,140 main.py:57] epoch 119, training loss: 10067.55, average training loss: 12213.03, base loss: 16108.40
[INFO 2017-06-29 00:06:48,230 main.py:57] epoch 120, training loss: 9562.10, average training loss: 12191.12, base loss: 16102.54
[INFO 2017-06-29 00:06:51,319 main.py:57] epoch 121, training loss: 9934.20, average training loss: 12172.62, base loss: 16108.74
[INFO 2017-06-29 00:06:54,370 main.py:57] epoch 122, training loss: 8838.08, average training loss: 12145.51, base loss: 16099.15
[INFO 2017-06-29 00:06:57,412 main.py:57] epoch 123, training loss: 9633.03, average training loss: 12125.25, base loss: 16093.89
[INFO 2017-06-29 00:07:00,491 main.py:57] epoch 124, training loss: 9575.53, average training loss: 12104.85, base loss: 16096.48
[INFO 2017-06-29 00:07:03,626 main.py:57] epoch 125, training loss: 8921.67, average training loss: 12079.59, base loss: 16095.65
[INFO 2017-06-29 00:07:06,692 main.py:57] epoch 126, training loss: 9494.33, average training loss: 12059.23, base loss: 16091.82
[INFO 2017-06-29 00:07:09,737 main.py:57] epoch 127, training loss: 8939.65, average training loss: 12034.86, base loss: 16080.47
[INFO 2017-06-29 00:07:12,746 main.py:57] epoch 128, training loss: 8580.90, average training loss: 12008.08, base loss: 16063.79
[INFO 2017-06-29 00:07:15,761 main.py:57] epoch 129, training loss: 9503.80, average training loss: 11988.82, base loss: 16059.58
[INFO 2017-06-29 00:07:18,813 main.py:57] epoch 130, training loss: 10173.96, average training loss: 11974.97, base loss: 16066.36
[INFO 2017-06-29 00:07:21,937 main.py:57] epoch 131, training loss: 10056.07, average training loss: 11960.43, base loss: 16067.10
[INFO 2017-06-29 00:07:24,979 main.py:57] epoch 132, training loss: 10115.30, average training loss: 11946.56, base loss: 16068.56
[INFO 2017-06-29 00:07:28,131 main.py:57] epoch 133, training loss: 9105.11, average training loss: 11925.35, base loss: 16055.46
[INFO 2017-06-29 00:07:31,141 main.py:57] epoch 134, training loss: 10460.73, average training loss: 11914.50, base loss: 16058.55
[INFO 2017-06-29 00:07:34,181 main.py:57] epoch 135, training loss: 8251.64, average training loss: 11887.57, base loss: 16041.80
[INFO 2017-06-29 00:07:37,303 main.py:57] epoch 136, training loss: 11039.91, average training loss: 11881.38, base loss: 16057.83
[INFO 2017-06-29 00:07:40,337 main.py:57] epoch 137, training loss: 9820.84, average training loss: 11866.45, base loss: 16067.23
[INFO 2017-06-29 00:07:43,516 main.py:57] epoch 138, training loss: 8618.17, average training loss: 11843.08, base loss: 16058.59
[INFO 2017-06-29 00:07:46,643 main.py:57] epoch 139, training loss: 9177.09, average training loss: 11824.04, base loss: 16050.56
[INFO 2017-06-29 00:07:49,602 main.py:57] epoch 140, training loss: 9403.34, average training loss: 11806.87, base loss: 16053.42
[INFO 2017-06-29 00:07:52,682 main.py:57] epoch 141, training loss: 8685.80, average training loss: 11784.89, base loss: 16032.50
[INFO 2017-06-29 00:07:55,732 main.py:57] epoch 142, training loss: 8653.16, average training loss: 11762.99, base loss: 16023.41
[INFO 2017-06-29 00:07:58,793 main.py:57] epoch 143, training loss: 9610.76, average training loss: 11748.05, base loss: 16022.76
[INFO 2017-06-29 00:08:01,884 main.py:57] epoch 144, training loss: 9560.75, average training loss: 11732.96, base loss: 16024.29
[INFO 2017-06-29 00:08:04,949 main.py:57] epoch 145, training loss: 9917.29, average training loss: 11720.52, base loss: 16028.33
[INFO 2017-06-29 00:08:08,086 main.py:57] epoch 146, training loss: 10672.67, average training loss: 11713.40, base loss: 16032.84
[INFO 2017-06-29 00:08:11,192 main.py:57] epoch 147, training loss: 10805.05, average training loss: 11707.26, base loss: 16047.53
[INFO 2017-06-29 00:08:14,223 main.py:57] epoch 148, training loss: 9164.04, average training loss: 11690.19, base loss: 16033.71
[INFO 2017-06-29 00:08:17,321 main.py:57] epoch 149, training loss: 9832.88, average training loss: 11677.81, base loss: 16036.00
[INFO 2017-06-29 00:08:20,368 main.py:57] epoch 150, training loss: 8523.70, average training loss: 11656.92, base loss: 16023.14
[INFO 2017-06-29 00:08:23,544 main.py:57] epoch 151, training loss: 10545.99, average training loss: 11649.61, base loss: 16029.87
[INFO 2017-06-29 00:08:26,622 main.py:57] epoch 152, training loss: 9436.58, average training loss: 11635.15, base loss: 16033.78
[INFO 2017-06-29 00:08:29,693 main.py:57] epoch 153, training loss: 9433.28, average training loss: 11620.85, base loss: 16037.30
[INFO 2017-06-29 00:08:32,848 main.py:57] epoch 154, training loss: 9074.24, average training loss: 11604.42, base loss: 16033.08
[INFO 2017-06-29 00:08:35,880 main.py:57] epoch 155, training loss: 10225.94, average training loss: 11595.58, base loss: 16043.35
[INFO 2017-06-29 00:08:39,011 main.py:57] epoch 156, training loss: 9201.38, average training loss: 11580.33, base loss: 16040.15
[INFO 2017-06-29 00:08:42,007 main.py:57] epoch 157, training loss: 11519.16, average training loss: 11579.95, base loss: 16053.25
[INFO 2017-06-29 00:08:45,064 main.py:57] epoch 158, training loss: 10471.12, average training loss: 11572.97, base loss: 16061.07
[INFO 2017-06-29 00:08:48,127 main.py:57] epoch 159, training loss: 8457.14, average training loss: 11553.50, base loss: 16045.96
[INFO 2017-06-29 00:08:51,148 main.py:57] epoch 160, training loss: 10292.35, average training loss: 11545.67, base loss: 16054.05
[INFO 2017-06-29 00:08:54,167 main.py:57] epoch 161, training loss: 9235.82, average training loss: 11531.41, base loss: 16046.14
[INFO 2017-06-29 00:08:57,277 main.py:57] epoch 162, training loss: 9221.16, average training loss: 11517.23, base loss: 16044.10
[INFO 2017-06-29 00:09:00,423 main.py:57] epoch 163, training loss: 8201.56, average training loss: 11497.02, base loss: 16033.85
[INFO 2017-06-29 00:09:03,472 main.py:57] epoch 164, training loss: 9011.87, average training loss: 11481.95, base loss: 16030.69
[INFO 2017-06-29 00:09:06,559 main.py:57] epoch 165, training loss: 10934.88, average training loss: 11478.66, base loss: 16039.42
[INFO 2017-06-29 00:09:09,581 main.py:57] epoch 166, training loss: 8058.72, average training loss: 11458.18, base loss: 16031.62
[INFO 2017-06-29 00:09:12,685 main.py:57] epoch 167, training loss: 9868.65, average training loss: 11448.72, base loss: 16031.85
[INFO 2017-06-29 00:09:15,824 main.py:57] epoch 168, training loss: 9479.95, average training loss: 11437.07, base loss: 16030.28
[INFO 2017-06-29 00:09:18,880 main.py:57] epoch 169, training loss: 9339.91, average training loss: 11424.73, base loss: 16028.02
[INFO 2017-06-29 00:09:21,952 main.py:57] epoch 170, training loss: 8660.22, average training loss: 11408.57, base loss: 16018.42
[INFO 2017-06-29 00:09:25,038 main.py:57] epoch 171, training loss: 9656.12, average training loss: 11398.38, base loss: 16021.84
[INFO 2017-06-29 00:09:28,196 main.py:57] epoch 172, training loss: 8327.59, average training loss: 11380.63, base loss: 16010.23
[INFO 2017-06-29 00:09:31,288 main.py:57] epoch 173, training loss: 10536.74, average training loss: 11375.78, base loss: 16016.09
[INFO 2017-06-29 00:09:34,258 main.py:57] epoch 174, training loss: 8393.28, average training loss: 11358.73, base loss: 16006.19
[INFO 2017-06-29 00:09:37,276 main.py:57] epoch 175, training loss: 9270.08, average training loss: 11346.87, base loss: 16006.09
[INFO 2017-06-29 00:09:40,365 main.py:57] epoch 176, training loss: 10765.87, average training loss: 11343.59, base loss: 16022.40
[INFO 2017-06-29 00:09:43,391 main.py:57] epoch 177, training loss: 8364.09, average training loss: 11326.85, base loss: 16015.14
[INFO 2017-06-29 00:09:46,457 main.py:57] epoch 178, training loss: 9118.35, average training loss: 11314.51, base loss: 16015.43
[INFO 2017-06-29 00:09:49,510 main.py:57] epoch 179, training loss: 10776.77, average training loss: 11311.52, base loss: 16027.63
[INFO 2017-06-29 00:09:52,561 main.py:57] epoch 180, training loss: 9461.93, average training loss: 11301.30, base loss: 16028.75
[INFO 2017-06-29 00:09:55,663 main.py:57] epoch 181, training loss: 9697.85, average training loss: 11292.49, base loss: 16030.58
[INFO 2017-06-29 00:09:58,792 main.py:57] epoch 182, training loss: 9736.71, average training loss: 11283.99, base loss: 16035.78
[INFO 2017-06-29 00:10:01,815 main.py:57] epoch 183, training loss: 10036.77, average training loss: 11277.21, base loss: 16040.49
[INFO 2017-06-29 00:10:04,947 main.py:57] epoch 184, training loss: 10394.80, average training loss: 11272.44, base loss: 16049.05
[INFO 2017-06-29 00:10:08,023 main.py:57] epoch 185, training loss: 11342.63, average training loss: 11272.82, base loss: 16068.43
[INFO 2017-06-29 00:10:11,012 main.py:57] epoch 186, training loss: 9276.51, average training loss: 11262.14, base loss: 16071.51
[INFO 2017-06-29 00:10:14,084 main.py:57] epoch 187, training loss: 8467.52, average training loss: 11247.28, base loss: 16070.33
[INFO 2017-06-29 00:10:17,158 main.py:57] epoch 188, training loss: 10140.00, average training loss: 11241.42, base loss: 16078.56
[INFO 2017-06-29 00:10:20,166 main.py:57] epoch 189, training loss: 9700.01, average training loss: 11233.31, base loss: 16080.50
[INFO 2017-06-29 00:10:23,265 main.py:57] epoch 190, training loss: 9487.44, average training loss: 11224.17, base loss: 16080.32
[INFO 2017-06-29 00:10:26,267 main.py:57] epoch 191, training loss: 8767.59, average training loss: 11211.37, base loss: 16079.95
[INFO 2017-06-29 00:10:29,391 main.py:57] epoch 192, training loss: 9513.88, average training loss: 11202.58, base loss: 16083.13
[INFO 2017-06-29 00:10:32,405 main.py:57] epoch 193, training loss: 9183.79, average training loss: 11192.17, base loss: 16076.07
[INFO 2017-06-29 00:10:35,438 main.py:57] epoch 194, training loss: 8407.95, average training loss: 11177.89, base loss: 16068.49
[INFO 2017-06-29 00:10:38,508 main.py:57] epoch 195, training loss: 9005.50, average training loss: 11166.81, base loss: 16064.72
[INFO 2017-06-29 00:10:41,521 main.py:57] epoch 196, training loss: 9670.50, average training loss: 11159.21, base loss: 16066.79
[INFO 2017-06-29 00:10:44,609 main.py:57] epoch 197, training loss: 8502.62, average training loss: 11145.80, base loss: 16060.45
[INFO 2017-06-29 00:10:47,687 main.py:57] epoch 198, training loss: 10127.10, average training loss: 11140.68, base loss: 16062.10
[INFO 2017-06-29 00:10:50,863 main.py:57] epoch 199, training loss: 8184.47, average training loss: 11125.90, base loss: 16062.81
[INFO 2017-06-29 00:10:50,863 main.py:59] epoch 199, testing
[INFO 2017-06-29 00:11:03,653 main.py:104] average testing loss: 10160.28, base loss: 16873.21
[INFO 2017-06-29 00:11:03,653 main.py:105] improve_loss: 6712.93, improve_percent: 0.40
[INFO 2017-06-29 00:11:03,655 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:11:03,693 main.py:71] current best improved percent: 0.40
[INFO 2017-06-29 00:11:06,745 main.py:57] epoch 200, training loss: 9106.59, average training loss: 11115.85, base loss: 16063.19
[INFO 2017-06-29 00:11:09,873 main.py:57] epoch 201, training loss: 10540.65, average training loss: 11113.00, base loss: 16075.68
[INFO 2017-06-29 00:11:12,951 main.py:57] epoch 202, training loss: 12043.15, average training loss: 11117.58, base loss: 16096.80
[INFO 2017-06-29 00:11:16,091 main.py:57] epoch 203, training loss: 9292.40, average training loss: 11108.64, base loss: 16091.44
[INFO 2017-06-29 00:11:19,164 main.py:57] epoch 204, training loss: 9706.93, average training loss: 11101.80, base loss: 16093.88
[INFO 2017-06-29 00:11:22,339 main.py:57] epoch 205, training loss: 8600.35, average training loss: 11089.66, base loss: 16088.93
[INFO 2017-06-29 00:11:25,347 main.py:57] epoch 206, training loss: 11051.47, average training loss: 11089.47, base loss: 16095.83
[INFO 2017-06-29 00:11:28,421 main.py:57] epoch 207, training loss: 9473.03, average training loss: 11081.70, base loss: 16098.76
[INFO 2017-06-29 00:11:31,486 main.py:57] epoch 208, training loss: 10704.86, average training loss: 11079.90, base loss: 16105.04
[INFO 2017-06-29 00:11:34,548 main.py:57] epoch 209, training loss: 11123.13, average training loss: 11080.10, base loss: 16119.29
[INFO 2017-06-29 00:11:37,636 main.py:57] epoch 210, training loss: 9406.37, average training loss: 11072.17, base loss: 16125.36
[INFO 2017-06-29 00:11:40,765 main.py:57] epoch 211, training loss: 10011.76, average training loss: 11067.17, base loss: 16131.02
[INFO 2017-06-29 00:11:43,840 main.py:57] epoch 212, training loss: 10445.63, average training loss: 11064.25, base loss: 16140.55
[INFO 2017-06-29 00:11:46,932 main.py:57] epoch 213, training loss: 8151.06, average training loss: 11050.64, base loss: 16127.27
[INFO 2017-06-29 00:11:49,971 main.py:57] epoch 214, training loss: 9045.07, average training loss: 11041.31, base loss: 16123.91
[INFO 2017-06-29 00:11:53,019 main.py:57] epoch 215, training loss: 9087.30, average training loss: 11032.26, base loss: 16123.84
[INFO 2017-06-29 00:11:56,029 main.py:57] epoch 216, training loss: 9809.55, average training loss: 11026.63, base loss: 16126.22
[INFO 2017-06-29 00:11:59,155 main.py:57] epoch 217, training loss: 7964.90, average training loss: 11012.58, base loss: 16117.56
[INFO 2017-06-29 00:12:02,300 main.py:57] epoch 218, training loss: 9295.94, average training loss: 11004.75, base loss: 16117.93
[INFO 2017-06-29 00:12:05,488 main.py:57] epoch 219, training loss: 10333.44, average training loss: 11001.70, base loss: 16123.93
[INFO 2017-06-29 00:12:08,504 main.py:57] epoch 220, training loss: 8305.54, average training loss: 10989.50, base loss: 16119.66
[INFO 2017-06-29 00:12:11,774 main.py:57] epoch 221, training loss: 9217.61, average training loss: 10981.51, base loss: 16117.77
[INFO 2017-06-29 00:12:14,857 main.py:57] epoch 222, training loss: 9956.17, average training loss: 10976.92, base loss: 16121.91
[INFO 2017-06-29 00:12:18,005 main.py:57] epoch 223, training loss: 7935.44, average training loss: 10963.34, base loss: 16112.41
[INFO 2017-06-29 00:12:21,096 main.py:57] epoch 224, training loss: 9751.21, average training loss: 10957.95, base loss: 16110.70
[INFO 2017-06-29 00:12:24,082 main.py:57] epoch 225, training loss: 10230.62, average training loss: 10954.73, base loss: 16118.61
[INFO 2017-06-29 00:12:27,132 main.py:57] epoch 226, training loss: 9302.66, average training loss: 10947.45, base loss: 16116.51
[INFO 2017-06-29 00:12:30,198 main.py:57] epoch 227, training loss: 9871.93, average training loss: 10942.74, base loss: 16119.58
[INFO 2017-06-29 00:12:33,277 main.py:57] epoch 228, training loss: 9161.52, average training loss: 10934.96, base loss: 16118.34
[INFO 2017-06-29 00:12:36,354 main.py:57] epoch 229, training loss: 8740.92, average training loss: 10925.42, base loss: 16119.57
[INFO 2017-06-29 00:12:39,452 main.py:57] epoch 230, training loss: 10924.01, average training loss: 10925.41, base loss: 16131.31
[INFO 2017-06-29 00:12:42,491 main.py:57] epoch 231, training loss: 9717.90, average training loss: 10920.21, base loss: 16136.13
[INFO 2017-06-29 00:12:45,587 main.py:57] epoch 232, training loss: 10314.65, average training loss: 10917.61, base loss: 16144.42
[INFO 2017-06-29 00:12:48,770 main.py:57] epoch 233, training loss: 8509.09, average training loss: 10907.32, base loss: 16138.48
[INFO 2017-06-29 00:12:51,892 main.py:57] epoch 234, training loss: 9006.01, average training loss: 10899.23, base loss: 16142.84
[INFO 2017-06-29 00:12:54,991 main.py:57] epoch 235, training loss: 8886.47, average training loss: 10890.70, base loss: 16141.33
[INFO 2017-06-29 00:12:58,022 main.py:57] epoch 236, training loss: 8242.54, average training loss: 10879.52, base loss: 16137.93
[INFO 2017-06-29 00:13:01,161 main.py:57] epoch 237, training loss: 8711.74, average training loss: 10870.42, base loss: 16132.96
[INFO 2017-06-29 00:13:04,249 main.py:57] epoch 238, training loss: 9108.52, average training loss: 10863.04, base loss: 16133.02
[INFO 2017-06-29 00:13:07,365 main.py:57] epoch 239, training loss: 9380.67, average training loss: 10856.87, base loss: 16134.84
[INFO 2017-06-29 00:13:10,550 main.py:57] epoch 240, training loss: 8032.80, average training loss: 10845.15, base loss: 16132.15
[INFO 2017-06-29 00:13:13,762 main.py:57] epoch 241, training loss: 9477.79, average training loss: 10839.50, base loss: 16137.63
[INFO 2017-06-29 00:13:16,872 main.py:57] epoch 242, training loss: 8620.82, average training loss: 10830.37, base loss: 16133.73
[INFO 2017-06-29 00:13:20,035 main.py:57] epoch 243, training loss: 7896.39, average training loss: 10818.34, base loss: 16126.54
[INFO 2017-06-29 00:13:23,149 main.py:57] epoch 244, training loss: 11053.00, average training loss: 10819.30, base loss: 16135.70
[INFO 2017-06-29 00:13:26,371 main.py:57] epoch 245, training loss: 10461.08, average training loss: 10817.85, base loss: 16143.86
[INFO 2017-06-29 00:13:29,373 main.py:57] epoch 246, training loss: 9638.11, average training loss: 10813.07, base loss: 16150.71
[INFO 2017-06-29 00:13:32,496 main.py:57] epoch 247, training loss: 10526.40, average training loss: 10811.91, base loss: 16157.19
[INFO 2017-06-29 00:13:35,561 main.py:57] epoch 248, training loss: 9698.39, average training loss: 10807.44, base loss: 16158.49
[INFO 2017-06-29 00:13:38,624 main.py:57] epoch 249, training loss: 8846.05, average training loss: 10799.60, base loss: 16158.50
[INFO 2017-06-29 00:13:41,755 main.py:57] epoch 250, training loss: 9136.20, average training loss: 10792.97, base loss: 16150.91
[INFO 2017-06-29 00:13:44,795 main.py:57] epoch 251, training loss: 9809.42, average training loss: 10789.07, base loss: 16153.25
[INFO 2017-06-29 00:13:47,925 main.py:57] epoch 252, training loss: 8546.12, average training loss: 10780.20, base loss: 16148.80
[INFO 2017-06-29 00:13:50,988 main.py:57] epoch 253, training loss: 9549.12, average training loss: 10775.35, base loss: 16153.51
[INFO 2017-06-29 00:13:54,115 main.py:57] epoch 254, training loss: 10342.94, average training loss: 10773.66, base loss: 16162.19
[INFO 2017-06-29 00:13:57,248 main.py:57] epoch 255, training loss: 8814.92, average training loss: 10766.01, base loss: 16162.32
[INFO 2017-06-29 00:14:00,338 main.py:57] epoch 256, training loss: 10381.32, average training loss: 10764.51, base loss: 16172.09
[INFO 2017-06-29 00:14:03,417 main.py:57] epoch 257, training loss: 9135.44, average training loss: 10758.20, base loss: 16166.66
[INFO 2017-06-29 00:14:06,459 main.py:57] epoch 258, training loss: 8445.89, average training loss: 10749.27, base loss: 16164.74
[INFO 2017-06-29 00:14:09,662 main.py:57] epoch 259, training loss: 11061.44, average training loss: 10750.47, base loss: 16176.12
[INFO 2017-06-29 00:14:12,888 main.py:57] epoch 260, training loss: 8970.57, average training loss: 10743.65, base loss: 16175.46
[INFO 2017-06-29 00:14:15,898 main.py:57] epoch 261, training loss: 9046.23, average training loss: 10737.17, base loss: 16176.93
[INFO 2017-06-29 00:14:18,980 main.py:57] epoch 262, training loss: 8598.18, average training loss: 10729.04, base loss: 16173.27
[INFO 2017-06-29 00:14:22,079 main.py:57] epoch 263, training loss: 9358.20, average training loss: 10723.84, base loss: 16176.39
[INFO 2017-06-29 00:14:25,172 main.py:57] epoch 264, training loss: 10090.77, average training loss: 10721.46, base loss: 16178.16
[INFO 2017-06-29 00:14:28,263 main.py:57] epoch 265, training loss: 8963.28, average training loss: 10714.85, base loss: 16176.79
[INFO 2017-06-29 00:14:31,354 main.py:57] epoch 266, training loss: 8527.58, average training loss: 10706.65, base loss: 16171.32
[INFO 2017-06-29 00:14:34,346 main.py:57] epoch 267, training loss: 8297.59, average training loss: 10697.66, base loss: 16170.45
[INFO 2017-06-29 00:14:37,440 main.py:57] epoch 268, training loss: 10863.23, average training loss: 10698.28, base loss: 16179.60
[INFO 2017-06-29 00:14:40,486 main.py:57] epoch 269, training loss: 10773.18, average training loss: 10698.56, base loss: 16184.20
[INFO 2017-06-29 00:14:43,567 main.py:57] epoch 270, training loss: 10153.02, average training loss: 10696.54, base loss: 16190.93
[INFO 2017-06-29 00:14:46,584 main.py:57] epoch 271, training loss: 8111.28, average training loss: 10687.04, base loss: 16186.03
[INFO 2017-06-29 00:14:49,659 main.py:57] epoch 272, training loss: 9107.58, average training loss: 10681.25, base loss: 16187.41
[INFO 2017-06-29 00:14:52,769 main.py:57] epoch 273, training loss: 9056.12, average training loss: 10675.32, base loss: 16184.40
[INFO 2017-06-29 00:14:55,775 main.py:57] epoch 274, training loss: 10003.58, average training loss: 10672.88, base loss: 16191.45
[INFO 2017-06-29 00:14:58,912 main.py:57] epoch 275, training loss: 8199.16, average training loss: 10663.92, base loss: 16188.59
[INFO 2017-06-29 00:15:02,019 main.py:57] epoch 276, training loss: 9146.01, average training loss: 10658.44, base loss: 16193.13
[INFO 2017-06-29 00:15:05,047 main.py:57] epoch 277, training loss: 9106.24, average training loss: 10652.85, base loss: 16194.03
[INFO 2017-06-29 00:15:08,188 main.py:57] epoch 278, training loss: 7991.58, average training loss: 10643.32, base loss: 16184.58
[INFO 2017-06-29 00:15:11,211 main.py:57] epoch 279, training loss: 9718.10, average training loss: 10640.01, base loss: 16186.71
[INFO 2017-06-29 00:15:14,289 main.py:57] epoch 280, training loss: 8543.17, average training loss: 10632.55, base loss: 16184.19
[INFO 2017-06-29 00:15:17,311 main.py:57] epoch 281, training loss: 8027.13, average training loss: 10623.31, base loss: 16176.69
[INFO 2017-06-29 00:15:20,352 main.py:57] epoch 282, training loss: 7777.35, average training loss: 10613.25, base loss: 16171.76
[INFO 2017-06-29 00:15:23,483 main.py:57] epoch 283, training loss: 8623.50, average training loss: 10606.25, base loss: 16171.10
[INFO 2017-06-29 00:15:26,588 main.py:57] epoch 284, training loss: 8376.26, average training loss: 10598.42, base loss: 16171.52
[INFO 2017-06-29 00:15:29,651 main.py:57] epoch 285, training loss: 8888.72, average training loss: 10592.45, base loss: 16171.78
[INFO 2017-06-29 00:15:32,861 main.py:57] epoch 286, training loss: 9297.95, average training loss: 10587.93, base loss: 16176.98
[INFO 2017-06-29 00:15:35,930 main.py:57] epoch 287, training loss: 8064.46, average training loss: 10579.17, base loss: 16171.53
[INFO 2017-06-29 00:15:38,983 main.py:57] epoch 288, training loss: 8078.19, average training loss: 10570.52, base loss: 16163.78
[INFO 2017-06-29 00:15:41,983 main.py:57] epoch 289, training loss: 9475.18, average training loss: 10566.74, base loss: 16166.90
[INFO 2017-06-29 00:15:45,035 main.py:57] epoch 290, training loss: 8731.50, average training loss: 10560.44, base loss: 16167.51
[INFO 2017-06-29 00:15:48,104 main.py:57] epoch 291, training loss: 7620.62, average training loss: 10550.37, base loss: 16157.88
[INFO 2017-06-29 00:15:51,134 main.py:57] epoch 292, training loss: 8824.13, average training loss: 10544.48, base loss: 16158.19
[INFO 2017-06-29 00:15:54,203 main.py:57] epoch 293, training loss: 7441.97, average training loss: 10533.92, base loss: 16153.63
[INFO 2017-06-29 00:15:57,229 main.py:57] epoch 294, training loss: 8712.64, average training loss: 10527.75, base loss: 16154.08
[INFO 2017-06-29 00:16:00,217 main.py:57] epoch 295, training loss: 9408.25, average training loss: 10523.97, base loss: 16157.17
[INFO 2017-06-29 00:16:03,354 main.py:57] epoch 296, training loss: 8260.31, average training loss: 10516.35, base loss: 16155.55
[INFO 2017-06-29 00:16:06,460 main.py:57] epoch 297, training loss: 8673.22, average training loss: 10510.16, base loss: 16156.95
[INFO 2017-06-29 00:16:09,584 main.py:57] epoch 298, training loss: 9181.81, average training loss: 10505.72, base loss: 16161.45
[INFO 2017-06-29 00:16:12,690 main.py:57] epoch 299, training loss: 8709.36, average training loss: 10499.73, base loss: 16164.25
[INFO 2017-06-29 00:16:12,691 main.py:59] epoch 299, testing
[INFO 2017-06-29 00:16:25,425 main.py:104] average testing loss: 9139.68, base loss: 16253.96
[INFO 2017-06-29 00:16:25,425 main.py:105] improve_loss: 7114.28, improve_percent: 0.44
[INFO 2017-06-29 00:16:25,427 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:16:25,465 main.py:71] current best improved percent: 0.44
[INFO 2017-06-29 00:16:28,578 main.py:57] epoch 300, training loss: 9518.33, average training loss: 10496.47, base loss: 16166.16
[INFO 2017-06-29 00:16:31,567 main.py:57] epoch 301, training loss: 10311.58, average training loss: 10495.86, base loss: 16175.45
[INFO 2017-06-29 00:16:34,697 main.py:57] epoch 302, training loss: 7883.42, average training loss: 10487.24, base loss: 16167.32
[INFO 2017-06-29 00:16:37,685 main.py:57] epoch 303, training loss: 8534.14, average training loss: 10480.81, base loss: 16165.24
[INFO 2017-06-29 00:16:40,735 main.py:57] epoch 304, training loss: 10280.27, average training loss: 10480.15, base loss: 16171.19
[INFO 2017-06-29 00:16:43,865 main.py:57] epoch 305, training loss: 8705.87, average training loss: 10474.35, base loss: 16172.71
[INFO 2017-06-29 00:16:46,874 main.py:57] epoch 306, training loss: 8780.57, average training loss: 10468.84, base loss: 16173.57
[INFO 2017-06-29 00:16:49,913 main.py:57] epoch 307, training loss: 7480.24, average training loss: 10459.13, base loss: 16168.72
[INFO 2017-06-29 00:16:52,988 main.py:57] epoch 308, training loss: 7739.59, average training loss: 10450.33, base loss: 16162.89
[INFO 2017-06-29 00:16:56,072 main.py:57] epoch 309, training loss: 10161.69, average training loss: 10449.40, base loss: 16166.46
[INFO 2017-06-29 00:16:59,079 main.py:57] epoch 310, training loss: 7614.70, average training loss: 10440.29, base loss: 16159.94
[INFO 2017-06-29 00:17:02,140 main.py:57] epoch 311, training loss: 7965.64, average training loss: 10432.36, base loss: 16153.88
[INFO 2017-06-29 00:17:05,204 main.py:57] epoch 312, training loss: 10920.22, average training loss: 10433.91, base loss: 16164.48
[INFO 2017-06-29 00:17:08,313 main.py:57] epoch 313, training loss: 8196.86, average training loss: 10426.79, base loss: 16161.67
[INFO 2017-06-29 00:17:11,439 main.py:57] epoch 314, training loss: 9081.61, average training loss: 10422.52, base loss: 16161.43
[INFO 2017-06-29 00:17:14,458 main.py:57] epoch 315, training loss: 8869.28, average training loss: 10417.60, base loss: 16163.41
[INFO 2017-06-29 00:17:17,464 main.py:57] epoch 316, training loss: 8266.92, average training loss: 10410.82, base loss: 16159.15
[INFO 2017-06-29 00:17:20,459 main.py:57] epoch 317, training loss: 7745.60, average training loss: 10402.44, base loss: 16154.17
[INFO 2017-06-29 00:17:23,536 main.py:57] epoch 318, training loss: 8116.28, average training loss: 10395.27, base loss: 16151.38
[INFO 2017-06-29 00:17:26,581 main.py:57] epoch 319, training loss: 9188.51, average training loss: 10391.50, base loss: 16152.31
[INFO 2017-06-29 00:17:29,668 main.py:57] epoch 320, training loss: 10351.70, average training loss: 10391.38, base loss: 16157.41
[INFO 2017-06-29 00:17:32,795 main.py:57] epoch 321, training loss: 8447.79, average training loss: 10385.34, base loss: 16154.20
[INFO 2017-06-29 00:17:35,812 main.py:57] epoch 322, training loss: 9271.05, average training loss: 10381.89, base loss: 16156.53
[INFO 2017-06-29 00:17:38,855 main.py:57] epoch 323, training loss: 9255.78, average training loss: 10378.42, base loss: 16159.32
[INFO 2017-06-29 00:17:41,915 main.py:57] epoch 324, training loss: 7630.29, average training loss: 10369.96, base loss: 16155.00
[INFO 2017-06-29 00:17:44,956 main.py:57] epoch 325, training loss: 8339.20, average training loss: 10363.73, base loss: 16152.26
[INFO 2017-06-29 00:17:47,993 main.py:57] epoch 326, training loss: 9137.69, average training loss: 10359.98, base loss: 16156.37
[INFO 2017-06-29 00:17:51,178 main.py:57] epoch 327, training loss: 9147.38, average training loss: 10356.28, base loss: 16156.78
[INFO 2017-06-29 00:17:54,253 main.py:57] epoch 328, training loss: 7857.03, average training loss: 10348.69, base loss: 16153.67
[INFO 2017-06-29 00:17:57,352 main.py:57] epoch 329, training loss: 8438.27, average training loss: 10342.90, base loss: 16153.56
[INFO 2017-06-29 00:18:00,457 main.py:57] epoch 330, training loss: 8795.21, average training loss: 10338.22, base loss: 16156.90
[INFO 2017-06-29 00:18:03,488 main.py:57] epoch 331, training loss: 9108.73, average training loss: 10334.52, base loss: 16157.94
[INFO 2017-06-29 00:18:06,567 main.py:57] epoch 332, training loss: 8640.66, average training loss: 10329.43, base loss: 16156.29
[INFO 2017-06-29 00:18:09,579 main.py:57] epoch 333, training loss: 8644.87, average training loss: 10324.39, base loss: 16154.49
[INFO 2017-06-29 00:18:12,632 main.py:57] epoch 334, training loss: 8315.86, average training loss: 10318.39, base loss: 16148.43
[INFO 2017-06-29 00:18:15,720 main.py:57] epoch 335, training loss: 10379.99, average training loss: 10318.58, base loss: 16155.34
[INFO 2017-06-29 00:18:18,803 main.py:57] epoch 336, training loss: 9887.65, average training loss: 10317.30, base loss: 16163.69
[INFO 2017-06-29 00:18:21,841 main.py:57] epoch 337, training loss: 9147.08, average training loss: 10313.84, base loss: 16165.36
[INFO 2017-06-29 00:18:24,942 main.py:57] epoch 338, training loss: 8988.75, average training loss: 10309.93, base loss: 16166.99
[INFO 2017-06-29 00:18:27,979 main.py:57] epoch 339, training loss: 8622.28, average training loss: 10304.96, base loss: 16166.13
[INFO 2017-06-29 00:18:31,042 main.py:57] epoch 340, training loss: 7466.31, average training loss: 10296.64, base loss: 16162.80
[INFO 2017-06-29 00:18:34,105 main.py:57] epoch 341, training loss: 9624.60, average training loss: 10294.67, base loss: 16165.56
[INFO 2017-06-29 00:18:37,119 main.py:57] epoch 342, training loss: 9125.89, average training loss: 10291.27, base loss: 16168.96
[INFO 2017-06-29 00:18:40,150 main.py:57] epoch 343, training loss: 8134.64, average training loss: 10285.00, base loss: 16163.63
[INFO 2017-06-29 00:18:43,282 main.py:57] epoch 344, training loss: 7184.92, average training loss: 10276.01, base loss: 16157.82
[INFO 2017-06-29 00:18:46,341 main.py:57] epoch 345, training loss: 9765.56, average training loss: 10274.54, base loss: 16162.55
[INFO 2017-06-29 00:18:49,408 main.py:57] epoch 346, training loss: 9314.97, average training loss: 10271.77, base loss: 16167.99
[INFO 2017-06-29 00:18:52,440 main.py:57] epoch 347, training loss: 7971.61, average training loss: 10265.16, base loss: 16166.23
[INFO 2017-06-29 00:18:55,535 main.py:57] epoch 348, training loss: 9598.40, average training loss: 10263.25, base loss: 16167.35
[INFO 2017-06-29 00:18:58,598 main.py:57] epoch 349, training loss: 8582.63, average training loss: 10258.45, base loss: 16167.77
[INFO 2017-06-29 00:19:01,648 main.py:57] epoch 350, training loss: 8747.12, average training loss: 10254.14, base loss: 16168.66
[INFO 2017-06-29 00:19:04,705 main.py:57] epoch 351, training loss: 10589.49, average training loss: 10255.10, base loss: 16177.25
[INFO 2017-06-29 00:19:07,731 main.py:57] epoch 352, training loss: 8764.74, average training loss: 10250.87, base loss: 16175.04
[INFO 2017-06-29 00:19:10,834 main.py:57] epoch 353, training loss: 9290.79, average training loss: 10248.16, base loss: 16177.15
[INFO 2017-06-29 00:19:13,911 main.py:57] epoch 354, training loss: 8285.70, average training loss: 10242.63, base loss: 16174.94
[INFO 2017-06-29 00:19:16,944 main.py:57] epoch 355, training loss: 9070.55, average training loss: 10239.34, base loss: 16179.09
[INFO 2017-06-29 00:19:20,023 main.py:57] epoch 356, training loss: 7983.47, average training loss: 10233.02, base loss: 16176.04
[INFO 2017-06-29 00:19:23,006 main.py:57] epoch 357, training loss: 8762.67, average training loss: 10228.92, base loss: 16175.97
[INFO 2017-06-29 00:19:26,069 main.py:57] epoch 358, training loss: 8262.30, average training loss: 10223.44, base loss: 16172.54
[INFO 2017-06-29 00:19:29,165 main.py:57] epoch 359, training loss: 9912.66, average training loss: 10222.57, base loss: 16176.50
[INFO 2017-06-29 00:19:32,253 main.py:57] epoch 360, training loss: 8151.32, average training loss: 10216.84, base loss: 16174.74
[INFO 2017-06-29 00:19:35,321 main.py:57] epoch 361, training loss: 8190.51, average training loss: 10211.24, base loss: 16174.79
[INFO 2017-06-29 00:19:38,280 main.py:57] epoch 362, training loss: 8075.44, average training loss: 10205.35, base loss: 16172.00
[INFO 2017-06-29 00:19:41,304 main.py:57] epoch 363, training loss: 8426.74, average training loss: 10200.47, base loss: 16170.69
[INFO 2017-06-29 00:19:44,345 main.py:57] epoch 364, training loss: 9113.76, average training loss: 10197.49, base loss: 16169.89
[INFO 2017-06-29 00:19:47,397 main.py:57] epoch 365, training loss: 8098.85, average training loss: 10191.76, base loss: 16165.35
[INFO 2017-06-29 00:19:50,485 main.py:57] epoch 366, training loss: 8026.60, average training loss: 10185.86, base loss: 16159.04
[INFO 2017-06-29 00:19:53,510 main.py:57] epoch 367, training loss: 11196.20, average training loss: 10188.60, base loss: 16169.48
[INFO 2017-06-29 00:19:56,579 main.py:57] epoch 368, training loss: 7899.37, average training loss: 10182.40, base loss: 16167.22
[INFO 2017-06-29 00:19:59,646 main.py:57] epoch 369, training loss: 9737.84, average training loss: 10181.20, base loss: 16171.93
[INFO 2017-06-29 00:20:02,732 main.py:57] epoch 370, training loss: 8777.10, average training loss: 10177.41, base loss: 16171.23
[INFO 2017-06-29 00:20:05,765 main.py:57] epoch 371, training loss: 8950.20, average training loss: 10174.11, base loss: 16175.76
[INFO 2017-06-29 00:20:08,907 main.py:57] epoch 372, training loss: 8027.34, average training loss: 10168.36, base loss: 16170.33
[INFO 2017-06-29 00:20:12,021 main.py:57] epoch 373, training loss: 8346.64, average training loss: 10163.49, base loss: 16171.05
[INFO 2017-06-29 00:20:15,050 main.py:57] epoch 374, training loss: 8144.81, average training loss: 10158.10, base loss: 16167.58
[INFO 2017-06-29 00:20:18,150 main.py:57] epoch 375, training loss: 8334.36, average training loss: 10153.25, base loss: 16164.98
[INFO 2017-06-29 00:20:21,251 main.py:57] epoch 376, training loss: 9894.83, average training loss: 10152.57, base loss: 16169.97
[INFO 2017-06-29 00:20:24,287 main.py:57] epoch 377, training loss: 7787.48, average training loss: 10146.31, base loss: 16170.17
[INFO 2017-06-29 00:20:27,360 main.py:57] epoch 378, training loss: 8237.75, average training loss: 10141.28, base loss: 16169.27
[INFO 2017-06-29 00:20:30,507 main.py:57] epoch 379, training loss: 9817.91, average training loss: 10140.43, base loss: 16170.95
[INFO 2017-06-29 00:20:33,596 main.py:57] epoch 380, training loss: 8721.04, average training loss: 10136.70, base loss: 16173.58
[INFO 2017-06-29 00:20:36,676 main.py:57] epoch 381, training loss: 8376.79, average training loss: 10132.09, base loss: 16173.21
[INFO 2017-06-29 00:20:39,678 main.py:57] epoch 382, training loss: 8607.57, average training loss: 10128.11, base loss: 16175.12
[INFO 2017-06-29 00:20:42,702 main.py:57] epoch 383, training loss: 8047.42, average training loss: 10122.69, base loss: 16175.16
[INFO 2017-06-29 00:20:45,751 main.py:57] epoch 384, training loss: 9319.90, average training loss: 10120.61, base loss: 16177.20
[INFO 2017-06-29 00:20:48,812 main.py:57] epoch 385, training loss: 8213.71, average training loss: 10115.67, base loss: 16172.65
[INFO 2017-06-29 00:20:51,923 main.py:57] epoch 386, training loss: 8784.59, average training loss: 10112.23, base loss: 16175.27
[INFO 2017-06-29 00:20:55,042 main.py:57] epoch 387, training loss: 8813.13, average training loss: 10108.88, base loss: 16177.01
[INFO 2017-06-29 00:20:58,102 main.py:57] epoch 388, training loss: 9350.20, average training loss: 10106.93, base loss: 16180.13
[INFO 2017-06-29 00:21:01,163 main.py:57] epoch 389, training loss: 7792.21, average training loss: 10101.00, base loss: 16174.17
[INFO 2017-06-29 00:21:04,240 main.py:57] epoch 390, training loss: 8787.87, average training loss: 10097.64, base loss: 16177.54
[INFO 2017-06-29 00:21:07,357 main.py:57] epoch 391, training loss: 8855.10, average training loss: 10094.47, base loss: 16178.32
[INFO 2017-06-29 00:21:10,471 main.py:57] epoch 392, training loss: 9256.10, average training loss: 10092.33, base loss: 16179.14
[INFO 2017-06-29 00:21:13,483 main.py:57] epoch 393, training loss: 8764.09, average training loss: 10088.96, base loss: 16180.74
[INFO 2017-06-29 00:21:16,540 main.py:57] epoch 394, training loss: 8099.38, average training loss: 10083.93, base loss: 16179.99
[INFO 2017-06-29 00:21:19,614 main.py:57] epoch 395, training loss: 8347.90, average training loss: 10079.54, base loss: 16182.11
[INFO 2017-06-29 00:21:22,645 main.py:57] epoch 396, training loss: 9749.51, average training loss: 10078.71, base loss: 16184.75
[INFO 2017-06-29 00:21:25,640 main.py:57] epoch 397, training loss: 7640.71, average training loss: 10072.58, base loss: 16181.02
[INFO 2017-06-29 00:21:28,769 main.py:57] epoch 398, training loss: 7268.42, average training loss: 10065.56, base loss: 16176.92
[INFO 2017-06-29 00:21:31,857 main.py:57] epoch 399, training loss: 9230.90, average training loss: 10063.47, base loss: 16177.30
[INFO 2017-06-29 00:21:31,857 main.py:59] epoch 399, testing
[INFO 2017-06-29 00:21:44,556 main.py:104] average testing loss: 9162.11, base loss: 16632.21
[INFO 2017-06-29 00:21:44,556 main.py:105] improve_loss: 7470.10, improve_percent: 0.45
[INFO 2017-06-29 00:21:44,559 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:21:44,608 main.py:71] current best improved percent: 0.45
[INFO 2017-06-29 00:21:47,725 main.py:57] epoch 400, training loss: 8899.56, average training loss: 10060.57, base loss: 16181.70
[INFO 2017-06-29 00:21:50,782 main.py:57] epoch 401, training loss: 9641.66, average training loss: 10059.53, base loss: 16187.23
[INFO 2017-06-29 00:21:53,832 main.py:57] epoch 402, training loss: 8500.92, average training loss: 10055.66, base loss: 16189.41
[INFO 2017-06-29 00:21:56,813 main.py:57] epoch 403, training loss: 7368.43, average training loss: 10049.01, base loss: 16184.39
[INFO 2017-06-29 00:21:59,946 main.py:57] epoch 404, training loss: 9704.41, average training loss: 10048.16, base loss: 16186.28
[INFO 2017-06-29 00:22:03,060 main.py:57] epoch 405, training loss: 9162.35, average training loss: 10045.97, base loss: 16187.78
[INFO 2017-06-29 00:22:06,104 main.py:57] epoch 406, training loss: 8360.77, average training loss: 10041.83, base loss: 16188.23
[INFO 2017-06-29 00:22:09,197 main.py:57] epoch 407, training loss: 9353.19, average training loss: 10040.15, base loss: 16193.43
[INFO 2017-06-29 00:22:12,222 main.py:57] epoch 408, training loss: 9270.64, average training loss: 10038.26, base loss: 16193.89
[INFO 2017-06-29 00:22:15,325 main.py:57] epoch 409, training loss: 8273.43, average training loss: 10033.96, base loss: 16195.22
[INFO 2017-06-29 00:22:18,405 main.py:57] epoch 410, training loss: 9294.97, average training loss: 10032.16, base loss: 16197.80
[INFO 2017-06-29 00:22:21,532 main.py:57] epoch 411, training loss: 8007.38, average training loss: 10027.25, base loss: 16196.81
[INFO 2017-06-29 00:22:24,574 main.py:57] epoch 412, training loss: 8734.36, average training loss: 10024.12, base loss: 16201.26
[INFO 2017-06-29 00:22:27,613 main.py:57] epoch 413, training loss: 8282.29, average training loss: 10019.91, base loss: 16200.59
[INFO 2017-06-29 00:22:30,671 main.py:57] epoch 414, training loss: 8666.68, average training loss: 10016.65, base loss: 16199.04
[INFO 2017-06-29 00:22:33,792 main.py:57] epoch 415, training loss: 7465.15, average training loss: 10010.52, base loss: 16196.35
[INFO 2017-06-29 00:22:36,831 main.py:57] epoch 416, training loss: 7430.80, average training loss: 10004.33, base loss: 16190.33
[INFO 2017-06-29 00:22:39,913 main.py:57] epoch 417, training loss: 8645.56, average training loss: 10001.08, base loss: 16193.03
[INFO 2017-06-29 00:22:42,941 main.py:57] epoch 418, training loss: 10008.93, average training loss: 10001.10, base loss: 16195.58
[INFO 2017-06-29 00:22:45,909 main.py:57] epoch 419, training loss: 8072.28, average training loss: 9996.50, base loss: 16197.08
[INFO 2017-06-29 00:22:48,968 main.py:57] epoch 420, training loss: 9048.80, average training loss: 9994.25, base loss: 16198.63
[INFO 2017-06-29 00:22:51,985 main.py:57] epoch 421, training loss: 8048.71, average training loss: 9989.64, base loss: 16198.31
[INFO 2017-06-29 00:22:55,120 main.py:57] epoch 422, training loss: 7926.24, average training loss: 9984.77, base loss: 16192.22
[INFO 2017-06-29 00:22:58,150 main.py:57] epoch 423, training loss: 8782.64, average training loss: 9981.93, base loss: 16192.19
[INFO 2017-06-29 00:23:01,230 main.py:57] epoch 424, training loss: 7524.98, average training loss: 9976.15, base loss: 16187.03
[INFO 2017-06-29 00:23:04,291 main.py:57] epoch 425, training loss: 9025.75, average training loss: 9973.92, base loss: 16189.28
[INFO 2017-06-29 00:23:07,516 main.py:57] epoch 426, training loss: 7925.21, average training loss: 9969.12, base loss: 16185.32
[INFO 2017-06-29 00:23:10,706 main.py:57] epoch 427, training loss: 9245.73, average training loss: 9967.43, base loss: 16188.06
[INFO 2017-06-29 00:23:13,791 main.py:57] epoch 428, training loss: 8076.60, average training loss: 9963.02, base loss: 16188.01
[INFO 2017-06-29 00:23:16,873 main.py:57] epoch 429, training loss: 8098.56, average training loss: 9958.69, base loss: 16185.22
[INFO 2017-06-29 00:23:19,948 main.py:57] epoch 430, training loss: 8800.84, average training loss: 9956.00, base loss: 16185.98
[INFO 2017-06-29 00:23:23,049 main.py:57] epoch 431, training loss: 8574.98, average training loss: 9952.80, base loss: 16185.80
[INFO 2017-06-29 00:23:26,080 main.py:57] epoch 432, training loss: 7952.92, average training loss: 9948.18, base loss: 16186.15
[INFO 2017-06-29 00:23:29,178 main.py:57] epoch 433, training loss: 7518.68, average training loss: 9942.59, base loss: 16184.61
[INFO 2017-06-29 00:23:32,240 main.py:57] epoch 434, training loss: 9541.36, average training loss: 9941.66, base loss: 16187.44
[INFO 2017-06-29 00:23:35,339 main.py:57] epoch 435, training loss: 8585.68, average training loss: 9938.55, base loss: 16188.35
[INFO 2017-06-29 00:23:38,393 main.py:57] epoch 436, training loss: 7528.71, average training loss: 9933.04, base loss: 16183.97
[INFO 2017-06-29 00:23:41,539 main.py:57] epoch 437, training loss: 9408.03, average training loss: 9931.84, base loss: 16185.15
[INFO 2017-06-29 00:23:44,659 main.py:57] epoch 438, training loss: 8928.60, average training loss: 9929.56, base loss: 16184.89
[INFO 2017-06-29 00:23:47,744 main.py:57] epoch 439, training loss: 8633.80, average training loss: 9926.61, base loss: 16185.69
[INFO 2017-06-29 00:23:50,937 main.py:57] epoch 440, training loss: 10078.78, average training loss: 9926.96, base loss: 16188.47
[INFO 2017-06-29 00:23:53,978 main.py:57] epoch 441, training loss: 8041.48, average training loss: 9922.69, base loss: 16186.73
[INFO 2017-06-29 00:23:57,089 main.py:57] epoch 442, training loss: 9205.05, average training loss: 9921.07, base loss: 16188.40
[INFO 2017-06-29 00:24:00,157 main.py:57] epoch 443, training loss: 7987.27, average training loss: 9916.71, base loss: 16186.91
[INFO 2017-06-29 00:24:03,270 main.py:57] epoch 444, training loss: 8656.25, average training loss: 9913.88, base loss: 16191.27
[INFO 2017-06-29 00:24:06,347 main.py:57] epoch 445, training loss: 8426.86, average training loss: 9910.55, base loss: 16190.58
[INFO 2017-06-29 00:24:09,460 main.py:57] epoch 446, training loss: 8761.59, average training loss: 9907.98, base loss: 16191.20
[INFO 2017-06-29 00:24:12,529 main.py:57] epoch 447, training loss: 7999.18, average training loss: 9903.72, base loss: 16192.40
[INFO 2017-06-29 00:24:15,582 main.py:57] epoch 448, training loss: 8026.87, average training loss: 9899.54, base loss: 16189.65
[INFO 2017-06-29 00:24:18,569 main.py:57] epoch 449, training loss: 8746.87, average training loss: 9896.98, base loss: 16192.33
[INFO 2017-06-29 00:24:21,686 main.py:57] epoch 450, training loss: 8651.18, average training loss: 9894.21, base loss: 16193.12
[INFO 2017-06-29 00:24:24,727 main.py:57] epoch 451, training loss: 8790.40, average training loss: 9891.77, base loss: 16193.82
[INFO 2017-06-29 00:24:27,735 main.py:57] epoch 452, training loss: 7713.02, average training loss: 9886.96, base loss: 16192.06
[INFO 2017-06-29 00:24:30,744 main.py:57] epoch 453, training loss: 9073.13, average training loss: 9885.17, base loss: 16193.03
[INFO 2017-06-29 00:24:33,783 main.py:57] epoch 454, training loss: 9832.16, average training loss: 9885.05, base loss: 16196.62
[INFO 2017-06-29 00:24:36,894 main.py:57] epoch 455, training loss: 9177.93, average training loss: 9883.50, base loss: 16196.68
[INFO 2017-06-29 00:24:39,962 main.py:57] epoch 456, training loss: 7572.25, average training loss: 9878.44, base loss: 16194.77
[INFO 2017-06-29 00:24:42,913 main.py:57] epoch 457, training loss: 6809.25, average training loss: 9871.74, base loss: 16189.63
[INFO 2017-06-29 00:24:46,006 main.py:57] epoch 458, training loss: 8709.15, average training loss: 9869.21, base loss: 16189.38
[INFO 2017-06-29 00:24:49,103 main.py:57] epoch 459, training loss: 8516.71, average training loss: 9866.27, base loss: 16190.63
[INFO 2017-06-29 00:24:52,137 main.py:57] epoch 460, training loss: 7612.41, average training loss: 9861.38, base loss: 16188.43
[INFO 2017-06-29 00:24:55,187 main.py:57] epoch 461, training loss: 8824.40, average training loss: 9859.14, base loss: 16188.36
[INFO 2017-06-29 00:24:58,289 main.py:57] epoch 462, training loss: 8216.73, average training loss: 9855.59, base loss: 16188.20
[INFO 2017-06-29 00:25:01,369 main.py:57] epoch 463, training loss: 8297.58, average training loss: 9852.23, base loss: 16187.36
[INFO 2017-06-29 00:25:04,510 main.py:57] epoch 464, training loss: 7815.15, average training loss: 9847.85, base loss: 16185.39
[INFO 2017-06-29 00:25:07,507 main.py:57] epoch 465, training loss: 9181.14, average training loss: 9846.42, base loss: 16190.82
[INFO 2017-06-29 00:25:10,568 main.py:57] epoch 466, training loss: 7918.10, average training loss: 9842.29, base loss: 16190.55
[INFO 2017-06-29 00:25:13,607 main.py:57] epoch 467, training loss: 7730.85, average training loss: 9837.78, base loss: 16190.63
[INFO 2017-06-29 00:25:16,673 main.py:57] epoch 468, training loss: 8351.44, average training loss: 9834.61, base loss: 16190.80
[INFO 2017-06-29 00:25:19,758 main.py:57] epoch 469, training loss: 7844.64, average training loss: 9830.38, base loss: 16189.55
[INFO 2017-06-29 00:25:22,899 main.py:57] epoch 470, training loss: 10428.51, average training loss: 9831.65, base loss: 16194.69
[INFO 2017-06-29 00:25:26,031 main.py:57] epoch 471, training loss: 9332.21, average training loss: 9830.59, base loss: 16196.24
[INFO 2017-06-29 00:25:29,096 main.py:57] epoch 472, training loss: 8197.76, average training loss: 9827.14, base loss: 16192.55
[INFO 2017-06-29 00:25:32,278 main.py:57] epoch 473, training loss: 8183.70, average training loss: 9823.67, base loss: 16191.18
[INFO 2017-06-29 00:25:35,355 main.py:57] epoch 474, training loss: 8448.59, average training loss: 9820.77, base loss: 16190.52
[INFO 2017-06-29 00:25:38,440 main.py:57] epoch 475, training loss: 8393.39, average training loss: 9817.77, base loss: 16190.99
[INFO 2017-06-29 00:25:41,518 main.py:57] epoch 476, training loss: 7514.68, average training loss: 9812.95, base loss: 16185.40
[INFO 2017-06-29 00:25:44,666 main.py:57] epoch 477, training loss: 9728.20, average training loss: 9812.77, base loss: 16188.44
[INFO 2017-06-29 00:25:47,715 main.py:57] epoch 478, training loss: 8075.35, average training loss: 9809.14, base loss: 16187.22
[INFO 2017-06-29 00:25:50,848 main.py:57] epoch 479, training loss: 8019.66, average training loss: 9805.41, base loss: 16184.86
[INFO 2017-06-29 00:25:53,931 main.py:57] epoch 480, training loss: 7851.50, average training loss: 9801.35, base loss: 16181.49
[INFO 2017-06-29 00:25:57,004 main.py:57] epoch 481, training loss: 9477.96, average training loss: 9800.68, base loss: 16184.31
[INFO 2017-06-29 00:26:00,051 main.py:57] epoch 482, training loss: 8482.35, average training loss: 9797.95, base loss: 16183.17
[INFO 2017-06-29 00:26:03,168 main.py:57] epoch 483, training loss: 8603.73, average training loss: 9795.48, base loss: 16182.94
[INFO 2017-06-29 00:26:06,241 main.py:57] epoch 484, training loss: 7880.79, average training loss: 9791.54, base loss: 16182.01
[INFO 2017-06-29 00:26:09,296 main.py:57] epoch 485, training loss: 8764.88, average training loss: 9789.42, base loss: 16183.17
[INFO 2017-06-29 00:26:12,368 main.py:57] epoch 486, training loss: 8871.45, average training loss: 9787.54, base loss: 16185.67
[INFO 2017-06-29 00:26:15,465 main.py:57] epoch 487, training loss: 8195.93, average training loss: 9784.28, base loss: 16186.46
[INFO 2017-06-29 00:26:18,617 main.py:57] epoch 488, training loss: 8228.88, average training loss: 9781.10, base loss: 16186.48
[INFO 2017-06-29 00:26:21,770 main.py:57] epoch 489, training loss: 9716.81, average training loss: 9780.97, base loss: 16188.04
[INFO 2017-06-29 00:26:24,885 main.py:57] epoch 490, training loss: 8106.61, average training loss: 9777.55, base loss: 16186.80
[INFO 2017-06-29 00:26:27,920 main.py:57] epoch 491, training loss: 8723.73, average training loss: 9775.41, base loss: 16186.95
[INFO 2017-06-29 00:26:31,002 main.py:57] epoch 492, training loss: 8292.34, average training loss: 9772.40, base loss: 16185.11
[INFO 2017-06-29 00:26:34,010 main.py:57] epoch 493, training loss: 7494.79, average training loss: 9767.79, base loss: 16181.64
[INFO 2017-06-29 00:26:37,064 main.py:57] epoch 494, training loss: 9184.64, average training loss: 9766.62, base loss: 16184.18
[INFO 2017-06-29 00:26:40,138 main.py:57] epoch 495, training loss: 8160.50, average training loss: 9763.38, base loss: 16183.98
[INFO 2017-06-29 00:26:43,164 main.py:57] epoch 496, training loss: 10542.42, average training loss: 9764.95, base loss: 16189.56
[INFO 2017-06-29 00:26:46,314 main.py:57] epoch 497, training loss: 8545.97, average training loss: 9762.50, base loss: 16188.59
[INFO 2017-06-29 00:26:49,398 main.py:57] epoch 498, training loss: 9131.96, average training loss: 9761.23, base loss: 16190.70
[INFO 2017-06-29 00:26:52,456 main.py:57] epoch 499, training loss: 8245.84, average training loss: 9758.20, base loss: 16192.06
[INFO 2017-06-29 00:26:52,457 main.py:59] epoch 499, testing
[INFO 2017-06-29 00:27:05,107 main.py:104] average testing loss: 8984.23, base loss: 16748.87
[INFO 2017-06-29 00:27:05,107 main.py:105] improve_loss: 7764.64, improve_percent: 0.46
[INFO 2017-06-29 00:27:05,110 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:27:05,175 main.py:71] current best improved percent: 0.46
[INFO 2017-06-29 00:27:08,281 main.py:57] epoch 500, training loss: 9001.89, average training loss: 9756.69, base loss: 16194.38
[INFO 2017-06-29 00:27:11,307 main.py:57] epoch 501, training loss: 8274.01, average training loss: 9753.74, base loss: 16196.59
[INFO 2017-06-29 00:27:14,476 main.py:57] epoch 502, training loss: 8139.95, average training loss: 9750.53, base loss: 16199.69
[INFO 2017-06-29 00:27:17,569 main.py:57] epoch 503, training loss: 8869.82, average training loss: 9748.78, base loss: 16200.83
[INFO 2017-06-29 00:27:20,707 main.py:57] epoch 504, training loss: 8535.57, average training loss: 9746.38, base loss: 16200.83
[INFO 2017-06-29 00:27:23,726 main.py:57] epoch 505, training loss: 8584.30, average training loss: 9744.09, base loss: 16201.31
[INFO 2017-06-29 00:27:26,774 main.py:57] epoch 506, training loss: 7866.10, average training loss: 9740.38, base loss: 16200.66
[INFO 2017-06-29 00:27:29,822 main.py:57] epoch 507, training loss: 8393.81, average training loss: 9737.73, base loss: 16202.49
[INFO 2017-06-29 00:27:32,937 main.py:57] epoch 508, training loss: 7673.65, average training loss: 9733.68, base loss: 16200.06
[INFO 2017-06-29 00:27:36,045 main.py:57] epoch 509, training loss: 9301.01, average training loss: 9732.83, base loss: 16201.24
[INFO 2017-06-29 00:27:39,110 main.py:57] epoch 510, training loss: 7790.86, average training loss: 9729.03, base loss: 16197.95
[INFO 2017-06-29 00:27:42,118 main.py:57] epoch 511, training loss: 8049.40, average training loss: 9725.75, base loss: 16199.45
[INFO 2017-06-29 00:27:45,200 main.py:57] epoch 512, training loss: 8537.46, average training loss: 9723.43, base loss: 16201.24
[INFO 2017-06-29 00:27:48,358 main.py:57] epoch 513, training loss: 8800.64, average training loss: 9721.63, base loss: 16202.37
[INFO 2017-06-29 00:27:51,417 main.py:57] epoch 514, training loss: 7960.06, average training loss: 9718.21, base loss: 16200.59
[INFO 2017-06-29 00:27:54,495 main.py:57] epoch 515, training loss: 8512.40, average training loss: 9715.88, base loss: 16198.92
[INFO 2017-06-29 00:27:57,558 main.py:57] epoch 516, training loss: 8392.24, average training loss: 9713.32, base loss: 16197.77
[INFO 2017-06-29 00:28:00,737 main.py:57] epoch 517, training loss: 7810.10, average training loss: 9709.64, base loss: 16196.48
[INFO 2017-06-29 00:28:03,756 main.py:57] epoch 518, training loss: 9228.67, average training loss: 9708.72, base loss: 16197.61
[INFO 2017-06-29 00:28:06,910 main.py:57] epoch 519, training loss: 9286.90, average training loss: 9707.90, base loss: 16196.95
[INFO 2017-06-29 00:28:10,054 main.py:57] epoch 520, training loss: 8610.90, average training loss: 9705.80, base loss: 16197.71
[INFO 2017-06-29 00:28:13,091 main.py:57] epoch 521, training loss: 6902.50, average training loss: 9700.43, base loss: 16193.66
[INFO 2017-06-29 00:28:16,165 main.py:57] epoch 522, training loss: 8626.41, average training loss: 9698.38, base loss: 16194.49
[INFO 2017-06-29 00:28:19,206 main.py:57] epoch 523, training loss: 8680.33, average training loss: 9696.43, base loss: 16197.96
[INFO 2017-06-29 00:28:22,331 main.py:57] epoch 524, training loss: 9249.05, average training loss: 9695.58, base loss: 16199.16
[INFO 2017-06-29 00:28:25,495 main.py:57] epoch 525, training loss: 9033.71, average training loss: 9694.32, base loss: 16199.11
[INFO 2017-06-29 00:28:28,580 main.py:57] epoch 526, training loss: 7228.13, average training loss: 9689.64, base loss: 16198.29
[INFO 2017-06-29 00:28:31,726 main.py:57] epoch 527, training loss: 9154.54, average training loss: 9688.63, base loss: 16201.53
[INFO 2017-06-29 00:28:34,783 main.py:57] epoch 528, training loss: 7888.77, average training loss: 9685.23, base loss: 16197.93
[INFO 2017-06-29 00:28:37,836 main.py:57] epoch 529, training loss: 8111.38, average training loss: 9682.26, base loss: 16198.14
[INFO 2017-06-29 00:28:40,946 main.py:57] epoch 530, training loss: 8920.33, average training loss: 9680.82, base loss: 16200.33
[INFO 2017-06-29 00:28:44,075 main.py:57] epoch 531, training loss: 8523.61, average training loss: 9678.65, base loss: 16203.18
[INFO 2017-06-29 00:28:47,220 main.py:57] epoch 532, training loss: 7840.66, average training loss: 9675.20, base loss: 16202.50
[INFO 2017-06-29 00:28:50,349 main.py:57] epoch 533, training loss: 8777.60, average training loss: 9673.52, base loss: 16202.75
[INFO 2017-06-29 00:28:53,444 main.py:57] epoch 534, training loss: 8293.35, average training loss: 9670.94, base loss: 16205.31
[INFO 2017-06-29 00:28:56,462 main.py:57] epoch 535, training loss: 8860.83, average training loss: 9669.43, base loss: 16209.95
[INFO 2017-06-29 00:28:59,517 main.py:57] epoch 536, training loss: 8823.66, average training loss: 9667.85, base loss: 16212.13
[INFO 2017-06-29 00:29:02,550 main.py:57] epoch 537, training loss: 8999.17, average training loss: 9666.61, base loss: 16213.89
[INFO 2017-06-29 00:29:05,606 main.py:57] epoch 538, training loss: 7797.59, average training loss: 9663.14, base loss: 16212.97
[INFO 2017-06-29 00:29:08,649 main.py:57] epoch 539, training loss: 8484.90, average training loss: 9660.96, base loss: 16213.56
[INFO 2017-06-29 00:29:11,726 main.py:57] epoch 540, training loss: 7581.16, average training loss: 9657.11, base loss: 16212.51
[INFO 2017-06-29 00:29:14,770 main.py:57] epoch 541, training loss: 8693.50, average training loss: 9655.34, base loss: 16215.51
[INFO 2017-06-29 00:29:17,844 main.py:57] epoch 542, training loss: 7405.25, average training loss: 9651.19, base loss: 16216.27
[INFO 2017-06-29 00:29:20,928 main.py:57] epoch 543, training loss: 8900.06, average training loss: 9649.81, base loss: 16216.31
[INFO 2017-06-29 00:29:23,949 main.py:57] epoch 544, training loss: 7737.36, average training loss: 9646.30, base loss: 16215.80
[INFO 2017-06-29 00:29:26,985 main.py:57] epoch 545, training loss: 9895.08, average training loss: 9646.76, base loss: 16223.66
[INFO 2017-06-29 00:29:30,044 main.py:57] epoch 546, training loss: 9547.06, average training loss: 9646.58, base loss: 16227.18
[INFO 2017-06-29 00:29:33,156 main.py:57] epoch 547, training loss: 8137.36, average training loss: 9643.82, base loss: 16226.46
[INFO 2017-06-29 00:29:36,280 main.py:57] epoch 548, training loss: 8762.09, average training loss: 9642.22, base loss: 16227.71
[INFO 2017-06-29 00:29:39,376 main.py:57] epoch 549, training loss: 8827.86, average training loss: 9640.74, base loss: 16226.69
[INFO 2017-06-29 00:29:42,416 main.py:57] epoch 550, training loss: 8308.58, average training loss: 9638.32, base loss: 16223.97
[INFO 2017-06-29 00:29:45,531 main.py:57] epoch 551, training loss: 7637.28, average training loss: 9634.69, base loss: 16220.13
[INFO 2017-06-29 00:29:48,570 main.py:57] epoch 552, training loss: 8322.92, average training loss: 9632.32, base loss: 16221.74
[INFO 2017-06-29 00:29:51,674 main.py:57] epoch 553, training loss: 8960.57, average training loss: 9631.11, base loss: 16224.95
[INFO 2017-06-29 00:29:54,713 main.py:57] epoch 554, training loss: 10089.00, average training loss: 9631.93, base loss: 16230.45
[INFO 2017-06-29 00:29:57,806 main.py:57] epoch 555, training loss: 8755.83, average training loss: 9630.36, base loss: 16230.53
[INFO 2017-06-29 00:30:00,874 main.py:57] epoch 556, training loss: 7763.91, average training loss: 9627.01, base loss: 16225.15
[INFO 2017-06-29 00:30:03,900 main.py:57] epoch 557, training loss: 9509.79, average training loss: 9626.80, base loss: 16226.80
[INFO 2017-06-29 00:30:07,008 main.py:57] epoch 558, training loss: 7953.37, average training loss: 9623.80, base loss: 16222.26
[INFO 2017-06-29 00:30:10,092 main.py:57] epoch 559, training loss: 7533.97, average training loss: 9620.07, base loss: 16217.37
[INFO 2017-06-29 00:30:13,172 main.py:57] epoch 560, training loss: 9825.26, average training loss: 9620.44, base loss: 16221.36
[INFO 2017-06-29 00:30:16,261 main.py:57] epoch 561, training loss: 8426.64, average training loss: 9618.31, base loss: 16219.88
[INFO 2017-06-29 00:30:19,343 main.py:57] epoch 562, training loss: 9426.42, average training loss: 9617.97, base loss: 16226.46
[INFO 2017-06-29 00:30:22,301 main.py:57] epoch 563, training loss: 8789.90, average training loss: 9616.50, base loss: 16231.91
[INFO 2017-06-29 00:30:25,412 main.py:57] epoch 564, training loss: 9503.33, average training loss: 9616.30, base loss: 16235.99
[INFO 2017-06-29 00:30:28,504 main.py:57] epoch 565, training loss: 8822.84, average training loss: 9614.90, base loss: 16240.03
[INFO 2017-06-29 00:30:31,619 main.py:57] epoch 566, training loss: 8384.22, average training loss: 9612.73, base loss: 16238.36
[INFO 2017-06-29 00:30:34,643 main.py:57] epoch 567, training loss: 7541.53, average training loss: 9609.08, base loss: 16234.78
[INFO 2017-06-29 00:30:37,778 main.py:57] epoch 568, training loss: 10024.81, average training loss: 9609.82, base loss: 16239.99
[INFO 2017-06-29 00:30:40,838 main.py:57] epoch 569, training loss: 8446.85, average training loss: 9607.78, base loss: 16241.80
[INFO 2017-06-29 00:30:44,023 main.py:57] epoch 570, training loss: 7147.37, average training loss: 9603.47, base loss: 16240.11
[INFO 2017-06-29 00:30:47,090 main.py:57] epoch 571, training loss: 7280.90, average training loss: 9599.41, base loss: 16237.35
[INFO 2017-06-29 00:30:50,268 main.py:57] epoch 572, training loss: 9283.89, average training loss: 9598.86, base loss: 16239.38
[INFO 2017-06-29 00:30:53,359 main.py:57] epoch 573, training loss: 8796.52, average training loss: 9597.46, base loss: 16241.68
[INFO 2017-06-29 00:30:56,449 main.py:57] epoch 574, training loss: 8378.76, average training loss: 9595.34, base loss: 16238.97
[INFO 2017-06-29 00:30:59,516 main.py:57] epoch 575, training loss: 7269.42, average training loss: 9591.30, base loss: 16234.39
[INFO 2017-06-29 00:31:02,589 main.py:57] epoch 576, training loss: 7401.57, average training loss: 9587.50, base loss: 16234.16
[INFO 2017-06-29 00:31:05,779 main.py:57] epoch 577, training loss: 8100.44, average training loss: 9584.93, base loss: 16233.99
[INFO 2017-06-29 00:31:08,923 main.py:57] epoch 578, training loss: 8071.60, average training loss: 9582.32, base loss: 16234.95
[INFO 2017-06-29 00:31:12,072 main.py:57] epoch 579, training loss: 8439.02, average training loss: 9580.35, base loss: 16238.26
[INFO 2017-06-29 00:31:15,265 main.py:57] epoch 580, training loss: 7987.27, average training loss: 9577.61, base loss: 16238.01
[INFO 2017-06-29 00:31:18,418 main.py:57] epoch 581, training loss: 8948.22, average training loss: 9576.52, base loss: 16239.54
[INFO 2017-06-29 00:31:21,507 main.py:57] epoch 582, training loss: 8496.03, average training loss: 9574.67, base loss: 16243.24
[INFO 2017-06-29 00:31:24,528 main.py:57] epoch 583, training loss: 7608.04, average training loss: 9571.30, base loss: 16243.13
[INFO 2017-06-29 00:31:27,621 main.py:57] epoch 584, training loss: 7831.90, average training loss: 9568.33, base loss: 16238.58
[INFO 2017-06-29 00:31:30,748 main.py:57] epoch 585, training loss: 8246.61, average training loss: 9566.07, base loss: 16236.82
[INFO 2017-06-29 00:31:33,847 main.py:57] epoch 586, training loss: 8022.73, average training loss: 9563.44, base loss: 16235.01
[INFO 2017-06-29 00:31:36,932 main.py:57] epoch 587, training loss: 7816.59, average training loss: 9560.47, base loss: 16232.02
[INFO 2017-06-29 00:31:40,004 main.py:57] epoch 588, training loss: 8803.10, average training loss: 9559.19, base loss: 16232.47
[INFO 2017-06-29 00:31:43,022 main.py:57] epoch 589, training loss: 9329.01, average training loss: 9558.80, base loss: 16234.22
[INFO 2017-06-29 00:31:46,066 main.py:57] epoch 590, training loss: 8063.73, average training loss: 9556.27, base loss: 16233.75
[INFO 2017-06-29 00:31:49,209 main.py:57] epoch 591, training loss: 8193.73, average training loss: 9553.97, base loss: 16233.16
[INFO 2017-06-29 00:31:52,287 main.py:57] epoch 592, training loss: 7746.65, average training loss: 9550.92, base loss: 16230.52
[INFO 2017-06-29 00:31:55,347 main.py:57] epoch 593, training loss: 9121.99, average training loss: 9550.20, base loss: 16230.74
[INFO 2017-06-29 00:31:58,431 main.py:57] epoch 594, training loss: 8535.04, average training loss: 9548.49, base loss: 16230.15
[INFO 2017-06-29 00:32:01,546 main.py:57] epoch 595, training loss: 7503.80, average training loss: 9545.06, base loss: 16227.09
[INFO 2017-06-29 00:32:04,618 main.py:57] epoch 596, training loss: 8033.06, average training loss: 9542.53, base loss: 16226.76
[INFO 2017-06-29 00:32:07,696 main.py:57] epoch 597, training loss: 7289.25, average training loss: 9538.76, base loss: 16225.10
[INFO 2017-06-29 00:32:10,814 main.py:57] epoch 598, training loss: 7500.12, average training loss: 9535.36, base loss: 16223.59
[INFO 2017-06-29 00:32:13,843 main.py:57] epoch 599, training loss: 9211.02, average training loss: 9534.82, base loss: 16226.42
[INFO 2017-06-29 00:32:13,843 main.py:59] epoch 599, testing
[INFO 2017-06-29 00:32:26,564 main.py:104] average testing loss: 9175.59, base loss: 16747.99
[INFO 2017-06-29 00:32:26,565 main.py:105] improve_loss: 7572.40, improve_percent: 0.45
[INFO 2017-06-29 00:32:26,566 main.py:71] current best improved percent: 0.46
[INFO 2017-06-29 00:32:29,602 main.py:57] epoch 600, training loss: 8512.24, average training loss: 9533.11, base loss: 16226.73
[INFO 2017-06-29 00:32:32,673 main.py:57] epoch 601, training loss: 8897.46, average training loss: 9532.06, base loss: 16230.42
[INFO 2017-06-29 00:32:35,728 main.py:57] epoch 602, training loss: 8688.72, average training loss: 9530.66, base loss: 16232.28
[INFO 2017-06-29 00:32:38,798 main.py:57] epoch 603, training loss: 7868.64, average training loss: 9527.91, base loss: 16228.13
[INFO 2017-06-29 00:32:41,840 main.py:57] epoch 604, training loss: 9358.15, average training loss: 9527.63, base loss: 16234.84
[INFO 2017-06-29 00:32:44,911 main.py:57] epoch 605, training loss: 8019.37, average training loss: 9525.14, base loss: 16233.31
[INFO 2017-06-29 00:32:48,000 main.py:57] epoch 606, training loss: 8881.62, average training loss: 9524.08, base loss: 16235.05
[INFO 2017-06-29 00:32:51,066 main.py:57] epoch 607, training loss: 8535.31, average training loss: 9522.45, base loss: 16237.15
[INFO 2017-06-29 00:32:54,072 main.py:57] epoch 608, training loss: 8521.72, average training loss: 9520.81, base loss: 16240.54
[INFO 2017-06-29 00:32:57,121 main.py:57] epoch 609, training loss: 8299.81, average training loss: 9518.81, base loss: 16242.19
[INFO 2017-06-29 00:33:00,209 main.py:57] epoch 610, training loss: 8733.13, average training loss: 9517.52, base loss: 16242.32
[INFO 2017-06-29 00:33:03,265 main.py:57] epoch 611, training loss: 7336.42, average training loss: 9513.96, base loss: 16239.30
[INFO 2017-06-29 00:33:06,249 main.py:57] epoch 612, training loss: 8110.17, average training loss: 9511.67, base loss: 16238.43
[INFO 2017-06-29 00:33:09,275 main.py:57] epoch 613, training loss: 7778.11, average training loss: 9508.84, base loss: 16236.21
[INFO 2017-06-29 00:33:12,403 main.py:57] epoch 614, training loss: 8696.57, average training loss: 9507.52, base loss: 16238.82
[INFO 2017-06-29 00:33:15,464 main.py:57] epoch 615, training loss: 7696.00, average training loss: 9504.58, base loss: 16236.24
[INFO 2017-06-29 00:33:18,526 main.py:57] epoch 616, training loss: 8461.60, average training loss: 9502.89, base loss: 16235.07
[INFO 2017-06-29 00:33:21,529 main.py:57] epoch 617, training loss: 9206.00, average training loss: 9502.41, base loss: 16238.89
[INFO 2017-06-29 00:33:24,588 main.py:57] epoch 618, training loss: 8763.25, average training loss: 9501.22, base loss: 16237.84
[INFO 2017-06-29 00:33:27,692 main.py:57] epoch 619, training loss: 8474.16, average training loss: 9499.56, base loss: 16233.46
[INFO 2017-06-29 00:33:30,779 main.py:57] epoch 620, training loss: 8025.89, average training loss: 9497.19, base loss: 16231.91
[INFO 2017-06-29 00:33:33,852 main.py:57] epoch 621, training loss: 7760.54, average training loss: 9494.40, base loss: 16230.26
[INFO 2017-06-29 00:33:36,944 main.py:57] epoch 622, training loss: 7092.76, average training loss: 9490.54, base loss: 16227.84
[INFO 2017-06-29 00:33:40,010 main.py:57] epoch 623, training loss: 8424.02, average training loss: 9488.83, base loss: 16227.90
[INFO 2017-06-29 00:33:43,018 main.py:57] epoch 624, training loss: 8057.77, average training loss: 9486.54, base loss: 16225.55
[INFO 2017-06-29 00:33:46,224 main.py:57] epoch 625, training loss: 9201.34, average training loss: 9486.09, base loss: 16223.53
[INFO 2017-06-29 00:33:49,253 main.py:57] epoch 626, training loss: 7707.95, average training loss: 9483.25, base loss: 16220.05
[INFO 2017-06-29 00:33:52,291 main.py:57] epoch 627, training loss: 8307.14, average training loss: 9481.38, base loss: 16218.43
[INFO 2017-06-29 00:33:55,413 main.py:57] epoch 628, training loss: 7170.12, average training loss: 9477.70, base loss: 16213.88
[INFO 2017-06-29 00:33:58,490 main.py:57] epoch 629, training loss: 9245.79, average training loss: 9477.33, base loss: 16215.07
[INFO 2017-06-29 00:34:01,639 main.py:57] epoch 630, training loss: 8533.97, average training loss: 9475.84, base loss: 16218.23
[INFO 2017-06-29 00:34:04,731 main.py:57] epoch 631, training loss: 8413.26, average training loss: 9474.16, base loss: 16218.44
[INFO 2017-06-29 00:34:07,802 main.py:57] epoch 632, training loss: 7860.77, average training loss: 9471.61, base loss: 16217.69
[INFO 2017-06-29 00:34:10,833 main.py:57] epoch 633, training loss: 7978.07, average training loss: 9469.25, base loss: 16218.44
[INFO 2017-06-29 00:34:14,075 main.py:57] epoch 634, training loss: 7761.26, average training loss: 9466.56, base loss: 16215.59
[INFO 2017-06-29 00:34:17,137 main.py:57] epoch 635, training loss: 9399.86, average training loss: 9466.46, base loss: 16219.43
[INFO 2017-06-29 00:34:20,177 main.py:57] epoch 636, training loss: 7994.86, average training loss: 9464.15, base loss: 16219.85
[INFO 2017-06-29 00:34:23,272 main.py:57] epoch 637, training loss: 8976.97, average training loss: 9463.39, base loss: 16222.56
[INFO 2017-06-29 00:34:26,328 main.py:57] epoch 638, training loss: 7844.59, average training loss: 9460.85, base loss: 16222.07
[INFO 2017-06-29 00:34:29,425 main.py:57] epoch 639, training loss: 8140.12, average training loss: 9458.79, base loss: 16222.31
[INFO 2017-06-29 00:34:32,536 main.py:57] epoch 640, training loss: 9933.01, average training loss: 9459.53, base loss: 16223.91
[INFO 2017-06-29 00:34:35,598 main.py:57] epoch 641, training loss: 8101.49, average training loss: 9457.41, base loss: 16219.31
[INFO 2017-06-29 00:34:38,691 main.py:57] epoch 642, training loss: 9334.07, average training loss: 9457.22, base loss: 16222.40
[INFO 2017-06-29 00:34:41,802 main.py:57] epoch 643, training loss: 8431.04, average training loss: 9455.63, base loss: 16221.98
[INFO 2017-06-29 00:34:44,989 main.py:57] epoch 644, training loss: 7506.27, average training loss: 9452.61, base loss: 16219.66
[INFO 2017-06-29 00:34:48,107 main.py:57] epoch 645, training loss: 7912.95, average training loss: 9450.22, base loss: 16221.26
[INFO 2017-06-29 00:34:51,352 main.py:57] epoch 646, training loss: 7447.14, average training loss: 9447.13, base loss: 16221.30
[INFO 2017-06-29 00:34:54,395 main.py:57] epoch 647, training loss: 8339.89, average training loss: 9445.42, base loss: 16221.52
[INFO 2017-06-29 00:34:57,456 main.py:57] epoch 648, training loss: 8367.76, average training loss: 9443.76, base loss: 16225.02
[INFO 2017-06-29 00:35:00,632 main.py:57] epoch 649, training loss: 7356.09, average training loss: 9440.55, base loss: 16223.09
[INFO 2017-06-29 00:35:03,670 main.py:57] epoch 650, training loss: 8192.94, average training loss: 9438.63, base loss: 16224.22
[INFO 2017-06-29 00:35:06,807 main.py:57] epoch 651, training loss: 9184.94, average training loss: 9438.24, base loss: 16226.09
[INFO 2017-06-29 00:35:09,846 main.py:57] epoch 652, training loss: 7487.06, average training loss: 9435.25, base loss: 16225.47
[INFO 2017-06-29 00:35:12,884 main.py:57] epoch 653, training loss: 7738.06, average training loss: 9432.66, base loss: 16226.01
[INFO 2017-06-29 00:35:15,950 main.py:57] epoch 654, training loss: 8143.24, average training loss: 9430.69, base loss: 16223.01
[INFO 2017-06-29 00:35:19,020 main.py:57] epoch 655, training loss: 8748.88, average training loss: 9429.65, base loss: 16221.69
[INFO 2017-06-29 00:35:22,098 main.py:57] epoch 656, training loss: 8026.67, average training loss: 9427.51, base loss: 16220.77
[INFO 2017-06-29 00:35:25,174 main.py:57] epoch 657, training loss: 8442.68, average training loss: 9426.02, base loss: 16219.34
[INFO 2017-06-29 00:35:28,217 main.py:57] epoch 658, training loss: 8102.23, average training loss: 9424.01, base loss: 16216.35
[INFO 2017-06-29 00:35:31,273 main.py:57] epoch 659, training loss: 8459.01, average training loss: 9422.55, base loss: 16215.19
[INFO 2017-06-29 00:35:34,394 main.py:57] epoch 660, training loss: 8677.42, average training loss: 9421.42, base loss: 16216.81
[INFO 2017-06-29 00:35:37,449 main.py:57] epoch 661, training loss: 7512.03, average training loss: 9418.53, base loss: 16215.14
[INFO 2017-06-29 00:35:40,538 main.py:57] epoch 662, training loss: 7525.84, average training loss: 9415.68, base loss: 16213.29
[INFO 2017-06-29 00:35:43,700 main.py:57] epoch 663, training loss: 8059.64, average training loss: 9413.64, base loss: 16214.58
[INFO 2017-06-29 00:35:46,740 main.py:57] epoch 664, training loss: 8526.98, average training loss: 9412.30, base loss: 16215.89
[INFO 2017-06-29 00:35:49,896 main.py:57] epoch 665, training loss: 9206.54, average training loss: 9411.99, base loss: 16218.23
[INFO 2017-06-29 00:35:53,058 main.py:57] epoch 666, training loss: 8579.07, average training loss: 9410.75, base loss: 16217.07
[INFO 2017-06-29 00:35:56,105 main.py:57] epoch 667, training loss: 7644.57, average training loss: 9408.10, base loss: 16214.57
[INFO 2017-06-29 00:35:59,246 main.py:57] epoch 668, training loss: 8273.37, average training loss: 9406.41, base loss: 16212.00
[INFO 2017-06-29 00:36:02,363 main.py:57] epoch 669, training loss: 8237.40, average training loss: 9404.66, base loss: 16210.39
[INFO 2017-06-29 00:36:05,391 main.py:57] epoch 670, training loss: 8474.77, average training loss: 9403.28, base loss: 16211.50
[INFO 2017-06-29 00:36:08,563 main.py:57] epoch 671, training loss: 7394.64, average training loss: 9400.29, base loss: 16209.92
[INFO 2017-06-29 00:36:11,637 main.py:57] epoch 672, training loss: 8574.70, average training loss: 9399.06, base loss: 16209.41
[INFO 2017-06-29 00:36:14,770 main.py:57] epoch 673, training loss: 9563.93, average training loss: 9399.30, base loss: 16211.46
[INFO 2017-06-29 00:36:17,799 main.py:57] epoch 674, training loss: 7672.73, average training loss: 9396.75, base loss: 16213.16
[INFO 2017-06-29 00:36:20,870 main.py:57] epoch 675, training loss: 8032.65, average training loss: 9394.73, base loss: 16214.74
[INFO 2017-06-29 00:36:24,005 main.py:57] epoch 676, training loss: 8805.45, average training loss: 9393.86, base loss: 16214.59
[INFO 2017-06-29 00:36:27,086 main.py:57] epoch 677, training loss: 8907.65, average training loss: 9393.14, base loss: 16218.96
[INFO 2017-06-29 00:36:30,097 main.py:57] epoch 678, training loss: 7258.83, average training loss: 9390.00, base loss: 16215.45
[INFO 2017-06-29 00:36:33,144 main.py:57] epoch 679, training loss: 7896.93, average training loss: 9387.80, base loss: 16217.40
[INFO 2017-06-29 00:36:36,274 main.py:57] epoch 680, training loss: 8001.18, average training loss: 9385.77, base loss: 16214.79
[INFO 2017-06-29 00:36:39,355 main.py:57] epoch 681, training loss: 7851.33, average training loss: 9383.52, base loss: 16210.38
[INFO 2017-06-29 00:36:42,440 main.py:57] epoch 682, training loss: 9437.99, average training loss: 9383.60, base loss: 16216.10
[INFO 2017-06-29 00:36:45,504 main.py:57] epoch 683, training loss: 9045.15, average training loss: 9383.10, base loss: 16220.74
[INFO 2017-06-29 00:36:48,546 main.py:57] epoch 684, training loss: 7717.43, average training loss: 9380.67, base loss: 16221.53
[INFO 2017-06-29 00:36:51,610 main.py:57] epoch 685, training loss: 9261.96, average training loss: 9380.50, base loss: 16225.51
[INFO 2017-06-29 00:36:54,771 main.py:57] epoch 686, training loss: 6959.04, average training loss: 9376.97, base loss: 16223.18
[INFO 2017-06-29 00:36:57,884 main.py:57] epoch 687, training loss: 8425.14, average training loss: 9375.59, base loss: 16222.09
[INFO 2017-06-29 00:37:00,937 main.py:57] epoch 688, training loss: 6994.47, average training loss: 9372.13, base loss: 16218.19
[INFO 2017-06-29 00:37:03,984 main.py:57] epoch 689, training loss: 10537.12, average training loss: 9373.82, base loss: 16223.51
[INFO 2017-06-29 00:37:07,029 main.py:57] epoch 690, training loss: 9440.17, average training loss: 9373.92, base loss: 16224.79
[INFO 2017-06-29 00:37:10,112 main.py:57] epoch 691, training loss: 8938.08, average training loss: 9373.29, base loss: 16224.54
[INFO 2017-06-29 00:37:13,146 main.py:57] epoch 692, training loss: 7692.21, average training loss: 9370.86, base loss: 16223.19
[INFO 2017-06-29 00:37:16,188 main.py:57] epoch 693, training loss: 7695.46, average training loss: 9368.45, base loss: 16221.32
[INFO 2017-06-29 00:37:19,243 main.py:57] epoch 694, training loss: 8703.29, average training loss: 9367.49, base loss: 16220.69
[INFO 2017-06-29 00:37:22,321 main.py:57] epoch 695, training loss: 8492.41, average training loss: 9366.23, base loss: 16220.46
[INFO 2017-06-29 00:37:25,449 main.py:57] epoch 696, training loss: 7754.01, average training loss: 9363.92, base loss: 16221.65
[INFO 2017-06-29 00:37:28,537 main.py:57] epoch 697, training loss: 6685.25, average training loss: 9360.08, base loss: 16217.45
[INFO 2017-06-29 00:37:31,589 main.py:57] epoch 698, training loss: 9266.32, average training loss: 9359.95, base loss: 16217.95
[INFO 2017-06-29 00:37:34,637 main.py:57] epoch 699, training loss: 6859.70, average training loss: 9356.38, base loss: 16211.63
[INFO 2017-06-29 00:37:34,638 main.py:59] epoch 699, testing
[INFO 2017-06-29 00:37:47,368 main.py:104] average testing loss: 8747.31, base loss: 16192.93
[INFO 2017-06-29 00:37:47,368 main.py:105] improve_loss: 7445.61, improve_percent: 0.46
[INFO 2017-06-29 00:37:47,370 main.py:71] current best improved percent: 0.46
[INFO 2017-06-29 00:37:50,415 main.py:57] epoch 700, training loss: 8091.55, average training loss: 9354.57, base loss: 16209.64
[INFO 2017-06-29 00:37:53,525 main.py:57] epoch 701, training loss: 7901.10, average training loss: 9352.50, base loss: 16208.42
[INFO 2017-06-29 00:37:56,552 main.py:57] epoch 702, training loss: 7794.07, average training loss: 9350.28, base loss: 16208.64
[INFO 2017-06-29 00:37:59,630 main.py:57] epoch 703, training loss: 9204.64, average training loss: 9350.08, base loss: 16211.45
[INFO 2017-06-29 00:38:02,670 main.py:57] epoch 704, training loss: 7606.49, average training loss: 9347.60, base loss: 16208.52
[INFO 2017-06-29 00:38:05,778 main.py:57] epoch 705, training loss: 7611.55, average training loss: 9345.14, base loss: 16203.35
[INFO 2017-06-29 00:38:08,833 main.py:57] epoch 706, training loss: 7786.08, average training loss: 9342.94, base loss: 16200.26
[INFO 2017-06-29 00:38:11,959 main.py:57] epoch 707, training loss: 7713.40, average training loss: 9340.64, base loss: 16198.79
[INFO 2017-06-29 00:38:15,034 main.py:57] epoch 708, training loss: 7792.17, average training loss: 9338.45, base loss: 16197.73
[INFO 2017-06-29 00:38:18,063 main.py:57] epoch 709, training loss: 8226.05, average training loss: 9336.89, base loss: 16197.74
[INFO 2017-06-29 00:38:21,133 main.py:57] epoch 710, training loss: 8426.28, average training loss: 9335.61, base loss: 16200.04
[INFO 2017-06-29 00:38:24,132 main.py:57] epoch 711, training loss: 8503.79, average training loss: 9334.44, base loss: 16201.15
[INFO 2017-06-29 00:38:27,193 main.py:57] epoch 712, training loss: 7479.32, average training loss: 9331.84, base loss: 16200.97
[INFO 2017-06-29 00:38:30,217 main.py:57] epoch 713, training loss: 8084.73, average training loss: 9330.09, base loss: 16199.49
[INFO 2017-06-29 00:38:33,414 main.py:57] epoch 714, training loss: 8729.27, average training loss: 9329.25, base loss: 16201.04
[INFO 2017-06-29 00:38:36,444 main.py:57] epoch 715, training loss: 9316.96, average training loss: 9329.23, base loss: 16204.26
[INFO 2017-06-29 00:38:39,615 main.py:57] epoch 716, training loss: 9173.54, average training loss: 9329.02, base loss: 16206.31
[INFO 2017-06-29 00:38:42,639 main.py:57] epoch 717, training loss: 9299.73, average training loss: 9328.97, base loss: 16211.03
[INFO 2017-06-29 00:38:45,652 main.py:57] epoch 718, training loss: 7818.51, average training loss: 9326.87, base loss: 16208.61
[INFO 2017-06-29 00:38:48,782 main.py:57] epoch 719, training loss: 10472.71, average training loss: 9328.47, base loss: 16215.08
[INFO 2017-06-29 00:38:51,791 main.py:57] epoch 720, training loss: 8298.34, average training loss: 9327.04, base loss: 16216.69
[INFO 2017-06-29 00:38:54,826 main.py:57] epoch 721, training loss: 7493.90, average training loss: 9324.50, base loss: 16216.09
[INFO 2017-06-29 00:38:57,917 main.py:57] epoch 722, training loss: 7536.24, average training loss: 9322.02, base loss: 16212.84
[INFO 2017-06-29 00:39:00,928 main.py:57] epoch 723, training loss: 8185.62, average training loss: 9320.45, base loss: 16211.63
[INFO 2017-06-29 00:39:03,948 main.py:57] epoch 724, training loss: 7249.24, average training loss: 9317.60, base loss: 16209.03
[INFO 2017-06-29 00:39:07,083 main.py:57] epoch 725, training loss: 8152.28, average training loss: 9315.99, base loss: 16207.37
[INFO 2017-06-29 00:39:10,146 main.py:57] epoch 726, training loss: 7164.09, average training loss: 9313.03, base loss: 16204.81
[INFO 2017-06-29 00:39:13,241 main.py:57] epoch 727, training loss: 8233.18, average training loss: 9311.55, base loss: 16204.95
[INFO 2017-06-29 00:39:16,343 main.py:57] epoch 728, training loss: 8506.12, average training loss: 9310.44, base loss: 16206.85
[INFO 2017-06-29 00:39:19,396 main.py:57] epoch 729, training loss: 7795.76, average training loss: 9308.37, base loss: 16205.59
[INFO 2017-06-29 00:39:22,499 main.py:57] epoch 730, training loss: 8093.33, average training loss: 9306.71, base loss: 16204.33
[INFO 2017-06-29 00:39:25,622 main.py:57] epoch 731, training loss: 7261.47, average training loss: 9303.91, base loss: 16202.04
[INFO 2017-06-29 00:39:28,704 main.py:57] epoch 732, training loss: 6698.24, average training loss: 9300.36, base loss: 16197.41
[INFO 2017-06-29 00:39:31,759 main.py:57] epoch 733, training loss: 7637.87, average training loss: 9298.09, base loss: 16195.63
[INFO 2017-06-29 00:39:34,856 main.py:57] epoch 734, training loss: 8023.85, average training loss: 9296.36, base loss: 16194.50
[INFO 2017-06-29 00:39:37,875 main.py:57] epoch 735, training loss: 9083.23, average training loss: 9296.07, base loss: 16196.48
[INFO 2017-06-29 00:39:40,905 main.py:57] epoch 736, training loss: 7917.63, average training loss: 9294.20, base loss: 16195.47
[INFO 2017-06-29 00:39:43,957 main.py:57] epoch 737, training loss: 9248.38, average training loss: 9294.14, base loss: 16199.18
[INFO 2017-06-29 00:39:47,124 main.py:57] epoch 738, training loss: 7820.69, average training loss: 9292.14, base loss: 16198.81
[INFO 2017-06-29 00:39:50,237 main.py:57] epoch 739, training loss: 8028.88, average training loss: 9290.44, base loss: 16198.39
[INFO 2017-06-29 00:39:53,356 main.py:57] epoch 740, training loss: 7755.33, average training loss: 9288.37, base loss: 16196.91
[INFO 2017-06-29 00:39:56,410 main.py:57] epoch 741, training loss: 8170.78, average training loss: 9286.86, base loss: 16196.73
[INFO 2017-06-29 00:39:59,476 main.py:57] epoch 742, training loss: 8855.35, average training loss: 9286.28, base loss: 16196.58
[INFO 2017-06-29 00:40:02,578 main.py:57] epoch 743, training loss: 7552.50, average training loss: 9283.95, base loss: 16194.14
[INFO 2017-06-29 00:40:05,709 main.py:57] epoch 744, training loss: 7446.51, average training loss: 9281.48, base loss: 16192.27
[INFO 2017-06-29 00:40:08,808 main.py:57] epoch 745, training loss: 7457.63, average training loss: 9279.04, base loss: 16191.81
[INFO 2017-06-29 00:40:11,848 main.py:57] epoch 746, training loss: 8825.94, average training loss: 9278.43, base loss: 16194.57
[INFO 2017-06-29 00:40:14,850 main.py:57] epoch 747, training loss: 8696.72, average training loss: 9277.65, base loss: 16197.15
[INFO 2017-06-29 00:40:17,962 main.py:57] epoch 748, training loss: 8970.79, average training loss: 9277.24, base loss: 16199.02
[INFO 2017-06-29 00:40:20,949 main.py:57] epoch 749, training loss: 9368.17, average training loss: 9277.36, base loss: 16201.64
[INFO 2017-06-29 00:40:24,063 main.py:57] epoch 750, training loss: 7443.87, average training loss: 9274.92, base loss: 16199.09
[INFO 2017-06-29 00:40:27,096 main.py:57] epoch 751, training loss: 7863.24, average training loss: 9273.05, base loss: 16198.15
[INFO 2017-06-29 00:40:30,320 main.py:57] epoch 752, training loss: 8427.76, average training loss: 9271.92, base loss: 16199.11
[INFO 2017-06-29 00:40:33,317 main.py:57] epoch 753, training loss: 8629.36, average training loss: 9271.07, base loss: 16199.90
[INFO 2017-06-29 00:40:36,450 main.py:57] epoch 754, training loss: 7764.77, average training loss: 9269.08, base loss: 16199.97
[INFO 2017-06-29 00:40:39,551 main.py:57] epoch 755, training loss: 7892.91, average training loss: 9267.26, base loss: 16200.24
[INFO 2017-06-29 00:40:42,628 main.py:57] epoch 756, training loss: 8373.00, average training loss: 9266.07, base loss: 16202.83
[INFO 2017-06-29 00:40:45,648 main.py:57] epoch 757, training loss: 7754.35, average training loss: 9264.08, base loss: 16202.32
[INFO 2017-06-29 00:40:48,734 main.py:57] epoch 758, training loss: 9156.70, average training loss: 9263.94, base loss: 16203.73
[INFO 2017-06-29 00:40:51,740 main.py:57] epoch 759, training loss: 7649.04, average training loss: 9261.81, base loss: 16204.03
[INFO 2017-06-29 00:40:54,871 main.py:57] epoch 760, training loss: 8604.53, average training loss: 9260.95, base loss: 16205.34
[INFO 2017-06-29 00:40:57,907 main.py:57] epoch 761, training loss: 7083.01, average training loss: 9258.09, base loss: 16201.80
[INFO 2017-06-29 00:41:00,976 main.py:57] epoch 762, training loss: 7565.65, average training loss: 9255.87, base loss: 16201.69
[INFO 2017-06-29 00:41:04,081 main.py:57] epoch 763, training loss: 7647.59, average training loss: 9253.77, base loss: 16199.67
[INFO 2017-06-29 00:41:07,115 main.py:57] epoch 764, training loss: 7662.34, average training loss: 9251.69, base loss: 16199.29
[INFO 2017-06-29 00:41:10,128 main.py:57] epoch 765, training loss: 8591.48, average training loss: 9250.83, base loss: 16199.18
[INFO 2017-06-29 00:41:13,176 main.py:57] epoch 766, training loss: 8929.17, average training loss: 9250.41, base loss: 16200.22
[INFO 2017-06-29 00:41:16,277 main.py:57] epoch 767, training loss: 8076.83, average training loss: 9248.88, base loss: 16200.47
[INFO 2017-06-29 00:41:19,290 main.py:57] epoch 768, training loss: 9897.94, average training loss: 9249.72, base loss: 16202.01
[INFO 2017-06-29 00:41:22,346 main.py:57] epoch 769, training loss: 8619.28, average training loss: 9248.90, base loss: 16202.12
[INFO 2017-06-29 00:41:25,438 main.py:57] epoch 770, training loss: 7977.43, average training loss: 9247.25, base loss: 16203.03
[INFO 2017-06-29 00:41:28,472 main.py:57] epoch 771, training loss: 8956.27, average training loss: 9246.88, base loss: 16207.84
[INFO 2017-06-29 00:41:31,514 main.py:57] epoch 772, training loss: 7957.59, average training loss: 9245.21, base loss: 16206.87
[INFO 2017-06-29 00:41:34,725 main.py:57] epoch 773, training loss: 8282.74, average training loss: 9243.97, base loss: 16204.98
[INFO 2017-06-29 00:41:37,821 main.py:57] epoch 774, training loss: 8510.00, average training loss: 9243.02, base loss: 16204.42
[INFO 2017-06-29 00:41:40,894 main.py:57] epoch 775, training loss: 8205.78, average training loss: 9241.68, base loss: 16203.59
[INFO 2017-06-29 00:41:43,908 main.py:57] epoch 776, training loss: 8367.98, average training loss: 9240.56, base loss: 16205.44
[INFO 2017-06-29 00:41:46,974 main.py:57] epoch 777, training loss: 8662.56, average training loss: 9239.81, base loss: 16207.78
[INFO 2017-06-29 00:41:50,073 main.py:57] epoch 778, training loss: 9710.44, average training loss: 9240.42, base loss: 16211.42
[INFO 2017-06-29 00:41:53,216 main.py:57] epoch 779, training loss: 8209.60, average training loss: 9239.10, base loss: 16213.26
[INFO 2017-06-29 00:41:56,422 main.py:57] epoch 780, training loss: 7739.77, average training loss: 9237.18, base loss: 16212.06
[INFO 2017-06-29 00:41:59,440 main.py:57] epoch 781, training loss: 8087.43, average training loss: 9235.71, base loss: 16211.81
[INFO 2017-06-29 00:42:02,461 main.py:57] epoch 782, training loss: 7776.00, average training loss: 9233.84, base loss: 16212.36
[INFO 2017-06-29 00:42:05,499 main.py:57] epoch 783, training loss: 8400.21, average training loss: 9232.78, base loss: 16213.56
[INFO 2017-06-29 00:42:08,507 main.py:57] epoch 784, training loss: 7340.80, average training loss: 9230.37, base loss: 16213.54
[INFO 2017-06-29 00:42:11,582 main.py:57] epoch 785, training loss: 7804.42, average training loss: 9228.56, base loss: 16213.44
[INFO 2017-06-29 00:42:14,662 main.py:57] epoch 786, training loss: 7449.13, average training loss: 9226.29, base loss: 16212.06
[INFO 2017-06-29 00:42:17,713 main.py:57] epoch 787, training loss: 8119.09, average training loss: 9224.89, base loss: 16213.19
[INFO 2017-06-29 00:42:20,797 main.py:57] epoch 788, training loss: 8031.11, average training loss: 9223.38, base loss: 16210.52
[INFO 2017-06-29 00:42:23,859 main.py:57] epoch 789, training loss: 8787.89, average training loss: 9222.83, base loss: 16213.44
[INFO 2017-06-29 00:42:26,944 main.py:57] epoch 790, training loss: 8099.03, average training loss: 9221.40, base loss: 16213.79
[INFO 2017-06-29 00:42:29,953 main.py:57] epoch 791, training loss: 10059.14, average training loss: 9222.46, base loss: 16216.91
[INFO 2017-06-29 00:42:32,974 main.py:57] epoch 792, training loss: 7871.57, average training loss: 9220.76, base loss: 16214.64
[INFO 2017-06-29 00:42:36,048 main.py:57] epoch 793, training loss: 9354.89, average training loss: 9220.93, base loss: 16214.78
[INFO 2017-06-29 00:42:39,164 main.py:57] epoch 794, training loss: 8861.28, average training loss: 9220.48, base loss: 16217.15
[INFO 2017-06-29 00:42:42,244 main.py:57] epoch 795, training loss: 7344.29, average training loss: 9218.12, base loss: 16211.09
[INFO 2017-06-29 00:42:45,475 main.py:57] epoch 796, training loss: 7305.90, average training loss: 9215.72, base loss: 16210.30
[INFO 2017-06-29 00:42:48,482 main.py:57] epoch 797, training loss: 8576.14, average training loss: 9214.92, base loss: 16212.34
[INFO 2017-06-29 00:42:51,554 main.py:57] epoch 798, training loss: 8837.36, average training loss: 9214.44, base loss: 16213.51
[INFO 2017-06-29 00:42:54,713 main.py:57] epoch 799, training loss: 8042.25, average training loss: 9212.98, base loss: 16212.02
[INFO 2017-06-29 00:42:54,714 main.py:59] epoch 799, testing
[INFO 2017-06-29 00:43:07,600 main.py:104] average testing loss: 8965.55, base loss: 16727.71
[INFO 2017-06-29 00:43:07,600 main.py:105] improve_loss: 7762.16, improve_percent: 0.46
[INFO 2017-06-29 00:43:07,601 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:43:07,641 main.py:71] current best improved percent: 0.46
[INFO 2017-06-29 00:43:10,710 main.py:57] epoch 800, training loss: 7462.99, average training loss: 9210.79, base loss: 16211.28
[INFO 2017-06-29 00:43:13,822 main.py:57] epoch 801, training loss: 8622.03, average training loss: 9210.06, base loss: 16215.98
[INFO 2017-06-29 00:43:16,877 main.py:57] epoch 802, training loss: 8448.67, average training loss: 9209.11, base loss: 16215.66
[INFO 2017-06-29 00:43:19,941 main.py:57] epoch 803, training loss: 8191.06, average training loss: 9207.85, base loss: 16215.52
[INFO 2017-06-29 00:43:23,108 main.py:57] epoch 804, training loss: 8176.79, average training loss: 9206.57, base loss: 16214.14
[INFO 2017-06-29 00:43:26,183 main.py:57] epoch 805, training loss: 8100.84, average training loss: 9205.19, base loss: 16213.10
[INFO 2017-06-29 00:43:29,288 main.py:57] epoch 806, training loss: 9080.45, average training loss: 9205.04, base loss: 16216.64
[INFO 2017-06-29 00:43:32,549 main.py:57] epoch 807, training loss: 8013.70, average training loss: 9203.56, base loss: 16216.89
[INFO 2017-06-29 00:43:35,705 main.py:57] epoch 808, training loss: 7661.94, average training loss: 9201.66, base loss: 16216.33
[INFO 2017-06-29 00:43:38,913 main.py:57] epoch 809, training loss: 9656.97, average training loss: 9202.22, base loss: 16219.75
[INFO 2017-06-29 00:43:41,999 main.py:57] epoch 810, training loss: 7670.49, average training loss: 9200.33, base loss: 16217.16
[INFO 2017-06-29 00:43:45,121 main.py:57] epoch 811, training loss: 9646.85, average training loss: 9200.88, base loss: 16220.24
[INFO 2017-06-29 00:43:48,156 main.py:57] epoch 812, training loss: 7607.65, average training loss: 9198.92, base loss: 16219.51
[INFO 2017-06-29 00:43:51,272 main.py:57] epoch 813, training loss: 8383.54, average training loss: 9197.92, base loss: 16219.56
[INFO 2017-06-29 00:43:54,310 main.py:57] epoch 814, training loss: 8163.39, average training loss: 9196.65, base loss: 16217.48
[INFO 2017-06-29 00:43:57,356 main.py:57] epoch 815, training loss: 7791.67, average training loss: 9194.93, base loss: 16214.93
[INFO 2017-06-29 00:44:00,373 main.py:57] epoch 816, training loss: 8058.60, average training loss: 9193.54, base loss: 16212.97
[INFO 2017-06-29 00:44:03,432 main.py:57] epoch 817, training loss: 10780.78, average training loss: 9195.48, base loss: 16215.92
[INFO 2017-06-29 00:44:06,489 main.py:57] epoch 818, training loss: 6729.36, average training loss: 9192.47, base loss: 16214.76
[INFO 2017-06-29 00:44:09,592 main.py:57] epoch 819, training loss: 8632.69, average training loss: 9191.79, base loss: 16217.24
[INFO 2017-06-29 00:44:12,625 main.py:57] epoch 820, training loss: 8045.42, average training loss: 9190.39, base loss: 16214.85
[INFO 2017-06-29 00:44:15,761 main.py:57] epoch 821, training loss: 8409.04, average training loss: 9189.44, base loss: 16215.61
[INFO 2017-06-29 00:44:18,812 main.py:57] epoch 822, training loss: 6845.78, average training loss: 9186.59, base loss: 16214.03
[INFO 2017-06-29 00:44:21,921 main.py:57] epoch 823, training loss: 8790.27, average training loss: 9186.11, base loss: 16216.65
[INFO 2017-06-29 00:44:24,922 main.py:57] epoch 824, training loss: 7551.02, average training loss: 9184.13, base loss: 16216.07
[INFO 2017-06-29 00:44:28,018 main.py:57] epoch 825, training loss: 8007.45, average training loss: 9182.70, base loss: 16215.34
[INFO 2017-06-29 00:44:31,108 main.py:57] epoch 826, training loss: 7436.61, average training loss: 9180.59, base loss: 16215.37
[INFO 2017-06-29 00:44:34,293 main.py:57] epoch 827, training loss: 7341.49, average training loss: 9178.37, base loss: 16215.90
[INFO 2017-06-29 00:44:37,310 main.py:57] epoch 828, training loss: 7017.16, average training loss: 9175.76, base loss: 16212.82
[INFO 2017-06-29 00:44:40,424 main.py:57] epoch 829, training loss: 8412.60, average training loss: 9174.84, base loss: 16214.12
[INFO 2017-06-29 00:44:43,487 main.py:57] epoch 830, training loss: 7805.42, average training loss: 9173.20, base loss: 16211.11
[INFO 2017-06-29 00:44:46,562 main.py:57] epoch 831, training loss: 8271.25, average training loss: 9172.11, base loss: 16209.27
[INFO 2017-06-29 00:44:49,622 main.py:57] epoch 832, training loss: 7562.77, average training loss: 9170.18, base loss: 16207.90
[INFO 2017-06-29 00:44:52,651 main.py:57] epoch 833, training loss: 9457.03, average training loss: 9170.52, base loss: 16210.71
[INFO 2017-06-29 00:44:55,685 main.py:57] epoch 834, training loss: 8629.44, average training loss: 9169.88, base loss: 16213.23
[INFO 2017-06-29 00:44:58,808 main.py:57] epoch 835, training loss: 7893.68, average training loss: 9168.35, base loss: 16213.02
[INFO 2017-06-29 00:45:01,825 main.py:57] epoch 836, training loss: 7407.87, average training loss: 9166.25, base loss: 16211.34
[INFO 2017-06-29 00:45:04,947 main.py:57] epoch 837, training loss: 8242.92, average training loss: 9165.14, base loss: 16210.92
[INFO 2017-06-29 00:45:08,093 main.py:57] epoch 838, training loss: 6993.99, average training loss: 9162.56, base loss: 16207.70
[INFO 2017-06-29 00:45:11,108 main.py:57] epoch 839, training loss: 7377.48, average training loss: 9160.43, base loss: 16206.55
[INFO 2017-06-29 00:45:14,160 main.py:57] epoch 840, training loss: 8185.22, average training loss: 9159.27, base loss: 16208.96
[INFO 2017-06-29 00:45:17,209 main.py:57] epoch 841, training loss: 8991.17, average training loss: 9159.07, base loss: 16212.90
[INFO 2017-06-29 00:45:20,326 main.py:57] epoch 842, training loss: 7909.56, average training loss: 9157.59, base loss: 16211.50
[INFO 2017-06-29 00:45:23,399 main.py:57] epoch 843, training loss: 6853.38, average training loss: 9154.86, base loss: 16208.50
[INFO 2017-06-29 00:45:26,498 main.py:57] epoch 844, training loss: 9349.51, average training loss: 9155.09, base loss: 16212.73
[INFO 2017-06-29 00:45:29,577 main.py:57] epoch 845, training loss: 6830.64, average training loss: 9152.34, base loss: 16211.93
[INFO 2017-06-29 00:45:32,650 main.py:57] epoch 846, training loss: 10058.58, average training loss: 9153.41, base loss: 16214.94
[INFO 2017-06-29 00:45:35,667 main.py:57] epoch 847, training loss: 7787.83, average training loss: 9151.80, base loss: 16212.75
[INFO 2017-06-29 00:45:38,796 main.py:57] epoch 848, training loss: 7817.42, average training loss: 9150.23, base loss: 16213.29
[INFO 2017-06-29 00:45:41,890 main.py:57] epoch 849, training loss: 7509.63, average training loss: 9148.30, base loss: 16213.50
[INFO 2017-06-29 00:45:44,947 main.py:57] epoch 850, training loss: 9200.29, average training loss: 9148.36, base loss: 16218.11
[INFO 2017-06-29 00:45:47,934 main.py:57] epoch 851, training loss: 9221.25, average training loss: 9148.45, base loss: 16221.31
[INFO 2017-06-29 00:45:50,943 main.py:57] epoch 852, training loss: 8462.62, average training loss: 9147.64, base loss: 16220.59
[INFO 2017-06-29 00:45:53,985 main.py:57] epoch 853, training loss: 7176.74, average training loss: 9145.34, base loss: 16217.86
[INFO 2017-06-29 00:45:57,059 main.py:57] epoch 854, training loss: 7416.97, average training loss: 9143.31, base loss: 16218.03
[INFO 2017-06-29 00:46:00,070 main.py:57] epoch 855, training loss: 8901.72, average training loss: 9143.03, base loss: 16220.38
[INFO 2017-06-29 00:46:03,104 main.py:57] epoch 856, training loss: 7537.37, average training loss: 9141.16, base loss: 16218.35
[INFO 2017-06-29 00:46:06,173 main.py:57] epoch 857, training loss: 8387.38, average training loss: 9140.28, base loss: 16218.26
[INFO 2017-06-29 00:46:09,269 main.py:57] epoch 858, training loss: 8157.10, average training loss: 9139.14, base loss: 16216.81
[INFO 2017-06-29 00:46:12,359 main.py:57] epoch 859, training loss: 8820.65, average training loss: 9138.77, base loss: 16216.48
[INFO 2017-06-29 00:46:15,363 main.py:57] epoch 860, training loss: 7617.54, average training loss: 9137.00, base loss: 16214.43
[INFO 2017-06-29 00:46:18,445 main.py:57] epoch 861, training loss: 8319.47, average training loss: 9136.05, base loss: 16215.31
[INFO 2017-06-29 00:46:21,541 main.py:57] epoch 862, training loss: 8134.33, average training loss: 9134.89, base loss: 16214.71
[INFO 2017-06-29 00:46:24,641 main.py:57] epoch 863, training loss: 7509.87, average training loss: 9133.01, base loss: 16211.63
[INFO 2017-06-29 00:46:27,687 main.py:57] epoch 864, training loss: 7336.37, average training loss: 9130.93, base loss: 16208.32
[INFO 2017-06-29 00:46:30,738 main.py:57] epoch 865, training loss: 9709.57, average training loss: 9131.60, base loss: 16210.96
[INFO 2017-06-29 00:46:33,749 main.py:57] epoch 866, training loss: 7609.77, average training loss: 9129.84, base loss: 16207.98
[INFO 2017-06-29 00:46:36,807 main.py:57] epoch 867, training loss: 7797.25, average training loss: 9128.31, base loss: 16204.92
[INFO 2017-06-29 00:46:39,870 main.py:57] epoch 868, training loss: 8153.14, average training loss: 9127.19, base loss: 16203.39
[INFO 2017-06-29 00:46:43,083 main.py:57] epoch 869, training loss: 8023.70, average training loss: 9125.92, base loss: 16202.00
[INFO 2017-06-29 00:46:46,183 main.py:57] epoch 870, training loss: 7795.95, average training loss: 9124.39, base loss: 16198.18
[INFO 2017-06-29 00:46:49,219 main.py:57] epoch 871, training loss: 7556.60, average training loss: 9122.59, base loss: 16195.32
[INFO 2017-06-29 00:46:52,263 main.py:57] epoch 872, training loss: 7870.18, average training loss: 9121.16, base loss: 16195.62
[INFO 2017-06-29 00:46:55,286 main.py:57] epoch 873, training loss: 8831.45, average training loss: 9120.83, base loss: 16196.91
[INFO 2017-06-29 00:46:58,450 main.py:57] epoch 874, training loss: 8336.48, average training loss: 9119.93, base loss: 16197.35
[INFO 2017-06-29 00:47:01,572 main.py:57] epoch 875, training loss: 8368.46, average training loss: 9119.07, base loss: 16197.98
[INFO 2017-06-29 00:47:04,595 main.py:57] epoch 876, training loss: 7187.89, average training loss: 9116.87, base loss: 16194.02
[INFO 2017-06-29 00:47:07,797 main.py:57] epoch 877, training loss: 8142.90, average training loss: 9115.76, base loss: 16194.51
[INFO 2017-06-29 00:47:10,822 main.py:57] epoch 878, training loss: 6542.50, average training loss: 9112.83, base loss: 16191.72
[INFO 2017-06-29 00:47:13,907 main.py:57] epoch 879, training loss: 7510.91, average training loss: 9111.01, base loss: 16189.90
[INFO 2017-06-29 00:47:16,995 main.py:57] epoch 880, training loss: 9468.90, average training loss: 9111.42, base loss: 16193.15
[INFO 2017-06-29 00:47:20,121 main.py:57] epoch 881, training loss: 8119.52, average training loss: 9110.30, base loss: 16193.03
[INFO 2017-06-29 00:47:23,226 main.py:57] epoch 882, training loss: 9007.79, average training loss: 9110.18, base loss: 16194.28
[INFO 2017-06-29 00:47:26,425 main.py:57] epoch 883, training loss: 7964.31, average training loss: 9108.88, base loss: 16193.30
[INFO 2017-06-29 00:47:29,487 main.py:57] epoch 884, training loss: 7826.68, average training loss: 9107.43, base loss: 16191.90
[INFO 2017-06-29 00:47:32,581 main.py:57] epoch 885, training loss: 8492.30, average training loss: 9106.74, base loss: 16192.86
[INFO 2017-06-29 00:47:35,627 main.py:57] epoch 886, training loss: 7586.76, average training loss: 9105.03, base loss: 16194.01
[INFO 2017-06-29 00:47:38,621 main.py:57] epoch 887, training loss: 8048.42, average training loss: 9103.84, base loss: 16195.60
[INFO 2017-06-29 00:47:41,721 main.py:57] epoch 888, training loss: 9592.71, average training loss: 9104.39, base loss: 16201.63
[INFO 2017-06-29 00:47:44,804 main.py:57] epoch 889, training loss: 8422.58, average training loss: 9103.62, base loss: 16203.37
[INFO 2017-06-29 00:47:47,905 main.py:57] epoch 890, training loss: 7329.37, average training loss: 9101.63, base loss: 16202.09
[INFO 2017-06-29 00:47:50,993 main.py:57] epoch 891, training loss: 8181.48, average training loss: 9100.60, base loss: 16201.73
[INFO 2017-06-29 00:47:54,085 main.py:57] epoch 892, training loss: 7548.40, average training loss: 9098.86, base loss: 16198.11
[INFO 2017-06-29 00:47:57,071 main.py:57] epoch 893, training loss: 7303.30, average training loss: 9096.85, base loss: 16196.11
[INFO 2017-06-29 00:48:00,104 main.py:57] epoch 894, training loss: 8207.29, average training loss: 9095.86, base loss: 16197.18
[INFO 2017-06-29 00:48:03,097 main.py:57] epoch 895, training loss: 8022.16, average training loss: 9094.66, base loss: 16198.04
[INFO 2017-06-29 00:48:06,141 main.py:57] epoch 896, training loss: 7540.79, average training loss: 9092.93, base loss: 16198.22
[INFO 2017-06-29 00:48:09,148 main.py:57] epoch 897, training loss: 7780.12, average training loss: 9091.46, base loss: 16195.84
[INFO 2017-06-29 00:48:12,255 main.py:57] epoch 898, training loss: 7921.06, average training loss: 9090.16, base loss: 16195.00
[INFO 2017-06-29 00:48:15,390 main.py:57] epoch 899, training loss: 8424.19, average training loss: 9089.42, base loss: 16194.47
[INFO 2017-06-29 00:48:15,391 main.py:59] epoch 899, testing
[INFO 2017-06-29 00:48:28,185 main.py:104] average testing loss: 8523.77, base loss: 16194.60
[INFO 2017-06-29 00:48:28,185 main.py:105] improve_loss: 7670.83, improve_percent: 0.47
[INFO 2017-06-29 00:48:28,187 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:48:28,225 main.py:71] current best improved percent: 0.47
[INFO 2017-06-29 00:48:31,322 main.py:57] epoch 900, training loss: 9371.67, average training loss: 9089.74, base loss: 16198.68
[INFO 2017-06-29 00:48:34,358 main.py:57] epoch 901, training loss: 7450.28, average training loss: 9087.92, base loss: 16197.48
[INFO 2017-06-29 00:48:37,444 main.py:57] epoch 902, training loss: 8519.92, average training loss: 9087.29, base loss: 16197.29
[INFO 2017-06-29 00:48:40,515 main.py:57] epoch 903, training loss: 8794.46, average training loss: 9086.97, base loss: 16194.99
[INFO 2017-06-29 00:48:43,599 main.py:57] epoch 904, training loss: 8467.83, average training loss: 9086.28, base loss: 16194.59
[INFO 2017-06-29 00:48:46,805 main.py:57] epoch 905, training loss: 8938.68, average training loss: 9086.12, base loss: 16193.12
[INFO 2017-06-29 00:48:49,850 main.py:57] epoch 906, training loss: 6723.08, average training loss: 9083.51, base loss: 16191.47
[INFO 2017-06-29 00:48:52,922 main.py:57] epoch 907, training loss: 7736.79, average training loss: 9082.03, base loss: 16191.16
[INFO 2017-06-29 00:48:56,054 main.py:57] epoch 908, training loss: 8992.57, average training loss: 9081.93, base loss: 16193.23
[INFO 2017-06-29 00:48:59,137 main.py:57] epoch 909, training loss: 8584.79, average training loss: 9081.39, base loss: 16196.10
[INFO 2017-06-29 00:49:02,159 main.py:57] epoch 910, training loss: 8297.94, average training loss: 9080.53, base loss: 16198.75
[INFO 2017-06-29 00:49:05,314 main.py:57] epoch 911, training loss: 7999.24, average training loss: 9079.34, base loss: 16199.82
[INFO 2017-06-29 00:49:08,349 main.py:57] epoch 912, training loss: 8411.64, average training loss: 9078.61, base loss: 16199.76
[INFO 2017-06-29 00:49:11,486 main.py:57] epoch 913, training loss: 7939.04, average training loss: 9077.36, base loss: 16200.88
[INFO 2017-06-29 00:49:14,618 main.py:57] epoch 914, training loss: 8551.20, average training loss: 9076.79, base loss: 16200.40
[INFO 2017-06-29 00:49:17,704 main.py:57] epoch 915, training loss: 9816.90, average training loss: 9077.59, base loss: 16203.69
[INFO 2017-06-29 00:49:20,813 main.py:57] epoch 916, training loss: 8002.96, average training loss: 9076.42, base loss: 16200.66
[INFO 2017-06-29 00:49:23,977 main.py:57] epoch 917, training loss: 8826.12, average training loss: 9076.15, base loss: 16202.14
[INFO 2017-06-29 00:49:27,075 main.py:57] epoch 918, training loss: 7326.09, average training loss: 9074.25, base loss: 16198.34
[INFO 2017-06-29 00:49:30,136 main.py:57] epoch 919, training loss: 9145.54, average training loss: 9074.32, base loss: 16198.54
[INFO 2017-06-29 00:49:33,141 main.py:57] epoch 920, training loss: 7607.84, average training loss: 9072.73, base loss: 16194.90
[INFO 2017-06-29 00:49:36,185 main.py:57] epoch 921, training loss: 8460.27, average training loss: 9072.07, base loss: 16194.28
[INFO 2017-06-29 00:49:39,275 main.py:57] epoch 922, training loss: 7473.48, average training loss: 9070.33, base loss: 16193.37
[INFO 2017-06-29 00:49:42,378 main.py:57] epoch 923, training loss: 7858.91, average training loss: 9069.02, base loss: 16191.78
[INFO 2017-06-29 00:49:45,413 main.py:57] epoch 924, training loss: 7495.23, average training loss: 9067.32, base loss: 16187.99
[INFO 2017-06-29 00:49:48,477 main.py:57] epoch 925, training loss: 8130.11, average training loss: 9066.31, base loss: 16185.43
[INFO 2017-06-29 00:49:51,504 main.py:57] epoch 926, training loss: 8523.54, average training loss: 9065.72, base loss: 16188.87
[INFO 2017-06-29 00:49:54,556 main.py:57] epoch 927, training loss: 7713.57, average training loss: 9064.27, base loss: 16189.59
[INFO 2017-06-29 00:49:57,586 main.py:57] epoch 928, training loss: 8217.03, average training loss: 9063.36, base loss: 16189.20
[INFO 2017-06-29 00:50:00,697 main.py:57] epoch 929, training loss: 8098.92, average training loss: 9062.32, base loss: 16191.42
[INFO 2017-06-29 00:50:03,971 main.py:57] epoch 930, training loss: 7207.05, average training loss: 9060.33, base loss: 16189.32
[INFO 2017-06-29 00:50:07,031 main.py:57] epoch 931, training loss: 8245.96, average training loss: 9059.45, base loss: 16191.57
[INFO 2017-06-29 00:50:10,097 main.py:57] epoch 932, training loss: 7959.52, average training loss: 9058.27, base loss: 16191.66
[INFO 2017-06-29 00:50:13,198 main.py:57] epoch 933, training loss: 8352.14, average training loss: 9057.52, base loss: 16191.67
[INFO 2017-06-29 00:50:16,279 main.py:57] epoch 934, training loss: 7526.48, average training loss: 9055.88, base loss: 16193.48
[INFO 2017-06-29 00:50:19,403 main.py:57] epoch 935, training loss: 8489.30, average training loss: 9055.27, base loss: 16193.73
[INFO 2017-06-29 00:50:22,442 main.py:57] epoch 936, training loss: 8083.43, average training loss: 9054.24, base loss: 16193.63
[INFO 2017-06-29 00:50:25,496 main.py:57] epoch 937, training loss: 7910.96, average training loss: 9053.02, base loss: 16193.23
[INFO 2017-06-29 00:50:28,570 main.py:57] epoch 938, training loss: 7812.40, average training loss: 9051.70, base loss: 16192.14
[INFO 2017-06-29 00:50:31,727 main.py:57] epoch 939, training loss: 8397.09, average training loss: 9051.00, base loss: 16190.69
[INFO 2017-06-29 00:50:34,762 main.py:57] epoch 940, training loss: 7576.53, average training loss: 9049.43, base loss: 16190.42
[INFO 2017-06-29 00:50:37,840 main.py:57] epoch 941, training loss: 8744.52, average training loss: 9049.11, base loss: 16192.65
[INFO 2017-06-29 00:50:40,930 main.py:57] epoch 942, training loss: 8434.76, average training loss: 9048.46, base loss: 16194.31
[INFO 2017-06-29 00:50:44,027 main.py:57] epoch 943, training loss: 7715.12, average training loss: 9047.05, base loss: 16193.10
[INFO 2017-06-29 00:50:47,077 main.py:57] epoch 944, training loss: 8256.45, average training loss: 9046.21, base loss: 16193.19
[INFO 2017-06-29 00:50:50,146 main.py:57] epoch 945, training loss: 9497.27, average training loss: 9046.69, base loss: 16194.42
[INFO 2017-06-29 00:50:53,185 main.py:57] epoch 946, training loss: 8078.79, average training loss: 9045.66, base loss: 16196.98
[INFO 2017-06-29 00:50:56,321 main.py:57] epoch 947, training loss: 8285.03, average training loss: 9044.86, base loss: 16199.52
[INFO 2017-06-29 00:50:59,362 main.py:57] epoch 948, training loss: 8079.32, average training loss: 9043.84, base loss: 16200.13
[INFO 2017-06-29 00:51:02,383 main.py:57] epoch 949, training loss: 7145.04, average training loss: 9041.85, base loss: 16198.56
[INFO 2017-06-29 00:51:05,450 main.py:57] epoch 950, training loss: 8470.35, average training loss: 9041.24, base loss: 16197.73
[INFO 2017-06-29 00:51:08,505 main.py:57] epoch 951, training loss: 8389.44, average training loss: 9040.56, base loss: 16198.74
[INFO 2017-06-29 00:51:11,544 main.py:57] epoch 952, training loss: 7370.72, average training loss: 9038.81, base loss: 16197.65
[INFO 2017-06-29 00:51:14,744 main.py:57] epoch 953, training loss: 6744.19, average training loss: 9036.40, base loss: 16195.31
[INFO 2017-06-29 00:51:17,794 main.py:57] epoch 954, training loss: 7896.38, average training loss: 9035.21, base loss: 16196.46
[INFO 2017-06-29 00:51:20,831 main.py:57] epoch 955, training loss: 8148.28, average training loss: 9034.28, base loss: 16198.70
[INFO 2017-06-29 00:51:23,840 main.py:57] epoch 956, training loss: 7753.49, average training loss: 9032.94, base loss: 16197.54
[INFO 2017-06-29 00:51:26,887 main.py:57] epoch 957, training loss: 8290.45, average training loss: 9032.17, base loss: 16200.13
[INFO 2017-06-29 00:51:29,962 main.py:57] epoch 958, training loss: 9807.76, average training loss: 9032.98, base loss: 16202.68
[INFO 2017-06-29 00:51:33,056 main.py:57] epoch 959, training loss: 8162.97, average training loss: 9032.07, base loss: 16200.48
[INFO 2017-06-29 00:51:36,140 main.py:57] epoch 960, training loss: 7337.30, average training loss: 9030.31, base loss: 16197.29
[INFO 2017-06-29 00:51:39,183 main.py:57] epoch 961, training loss: 7993.27, average training loss: 9029.23, base loss: 16195.10
[INFO 2017-06-29 00:51:42,212 main.py:57] epoch 962, training loss: 8189.33, average training loss: 9028.36, base loss: 16193.56
[INFO 2017-06-29 00:51:45,250 main.py:57] epoch 963, training loss: 7079.99, average training loss: 9026.34, base loss: 16191.01
[INFO 2017-06-29 00:51:48,306 main.py:57] epoch 964, training loss: 7727.96, average training loss: 9024.99, base loss: 16188.39
[INFO 2017-06-29 00:51:51,412 main.py:57] epoch 965, training loss: 8967.08, average training loss: 9024.93, base loss: 16190.11
[INFO 2017-06-29 00:51:54,501 main.py:57] epoch 966, training loss: 8447.77, average training loss: 9024.33, base loss: 16192.27
[INFO 2017-06-29 00:51:57,573 main.py:57] epoch 967, training loss: 7184.57, average training loss: 9022.43, base loss: 16191.00
[INFO 2017-06-29 00:52:00,631 main.py:57] epoch 968, training loss: 8153.90, average training loss: 9021.54, base loss: 16186.82
[INFO 2017-06-29 00:52:03,754 main.py:57] epoch 969, training loss: 8172.49, average training loss: 9020.66, base loss: 16185.76
[INFO 2017-06-29 00:52:06,796 main.py:57] epoch 970, training loss: 8865.17, average training loss: 9020.50, base loss: 16186.69
[INFO 2017-06-29 00:52:09,864 main.py:57] epoch 971, training loss: 7454.73, average training loss: 9018.89, base loss: 16187.16
[INFO 2017-06-29 00:52:12,952 main.py:57] epoch 972, training loss: 7757.12, average training loss: 9017.59, base loss: 16187.58
[INFO 2017-06-29 00:52:16,104 main.py:57] epoch 973, training loss: 8429.58, average training loss: 9016.99, base loss: 16188.19
[INFO 2017-06-29 00:52:19,240 main.py:57] epoch 974, training loss: 8151.67, average training loss: 9016.10, base loss: 16188.59
[INFO 2017-06-29 00:52:22,264 main.py:57] epoch 975, training loss: 7020.24, average training loss: 9014.06, base loss: 16187.37
[INFO 2017-06-29 00:52:25,283 main.py:57] epoch 976, training loss: 9420.36, average training loss: 9014.47, base loss: 16189.59
[INFO 2017-06-29 00:52:28,323 main.py:57] epoch 977, training loss: 7690.12, average training loss: 9013.12, base loss: 16186.70
[INFO 2017-06-29 00:52:31,407 main.py:57] epoch 978, training loss: 8292.55, average training loss: 9012.38, base loss: 16187.35
[INFO 2017-06-29 00:52:34,500 main.py:57] epoch 979, training loss: 8956.81, average training loss: 9012.33, base loss: 16188.45
[INFO 2017-06-29 00:52:37,495 main.py:57] epoch 980, training loss: 9238.21, average training loss: 9012.56, base loss: 16192.04
[INFO 2017-06-29 00:52:40,555 main.py:57] epoch 981, training loss: 7497.22, average training loss: 9011.01, base loss: 16191.12
[INFO 2017-06-29 00:52:43,561 main.py:57] epoch 982, training loss: 7445.99, average training loss: 9009.42, base loss: 16187.20
[INFO 2017-06-29 00:52:46,620 main.py:57] epoch 983, training loss: 9139.43, average training loss: 9009.55, base loss: 16190.53
[INFO 2017-06-29 00:52:49,731 main.py:57] epoch 984, training loss: 9062.46, average training loss: 9009.61, base loss: 16192.83
[INFO 2017-06-29 00:52:52,846 main.py:57] epoch 985, training loss: 7671.96, average training loss: 9008.25, base loss: 16193.03
[INFO 2017-06-29 00:52:55,987 main.py:57] epoch 986, training loss: 8107.13, average training loss: 9007.34, base loss: 16197.49
[INFO 2017-06-29 00:52:59,135 main.py:57] epoch 987, training loss: 9101.75, average training loss: 9007.43, base loss: 16200.81
[INFO 2017-06-29 00:53:02,255 main.py:57] epoch 988, training loss: 8671.60, average training loss: 9007.09, base loss: 16201.11
[INFO 2017-06-29 00:53:05,294 main.py:57] epoch 989, training loss: 8253.45, average training loss: 9006.33, base loss: 16200.36
[INFO 2017-06-29 00:53:08,396 main.py:57] epoch 990, training loss: 7203.01, average training loss: 9004.51, base loss: 16197.53
[INFO 2017-06-29 00:53:11,438 main.py:57] epoch 991, training loss: 8652.10, average training loss: 9004.16, base loss: 16198.79
[INFO 2017-06-29 00:53:14,444 main.py:57] epoch 992, training loss: 8114.04, average training loss: 9003.26, base loss: 16199.43
[INFO 2017-06-29 00:53:17,551 main.py:57] epoch 993, training loss: 7151.13, average training loss: 9001.40, base loss: 16197.07
[INFO 2017-06-29 00:53:20,599 main.py:57] epoch 994, training loss: 7541.59, average training loss: 8999.93, base loss: 16195.56
[INFO 2017-06-29 00:53:23,693 main.py:57] epoch 995, training loss: 8067.72, average training loss: 8998.99, base loss: 16194.24
[INFO 2017-06-29 00:53:26,679 main.py:57] epoch 996, training loss: 8376.12, average training loss: 8998.37, base loss: 16194.29
[INFO 2017-06-29 00:53:29,713 main.py:57] epoch 997, training loss: 8207.89, average training loss: 8997.58, base loss: 16194.52
[INFO 2017-06-29 00:53:32,808 main.py:57] epoch 998, training loss: 9726.70, average training loss: 8998.31, base loss: 16196.94
[INFO 2017-06-29 00:53:35,942 main.py:57] epoch 999, training loss: 8422.99, average training loss: 8997.73, base loss: 16196.72
[INFO 2017-06-29 00:53:35,943 main.py:59] epoch 999, testing
[INFO 2017-06-29 00:53:48,785 main.py:104] average testing loss: 8682.33, base loss: 16456.18
[INFO 2017-06-29 00:53:48,785 main.py:105] improve_loss: 7773.84, improve_percent: 0.47
[INFO 2017-06-29 00:53:48,786 main.py:71] current best improved percent: 0.47
[INFO 2017-06-29 00:53:51,847 main.py:57] epoch 1000, training loss: 8585.72, average training loss: 8965.28, base loss: 16199.47
[INFO 2017-06-29 00:53:54,855 main.py:57] epoch 1001, training loss: 7618.91, average training loss: 8941.49, base loss: 16200.13
[INFO 2017-06-29 00:53:57,837 main.py:57] epoch 1002, training loss: 8042.50, average training loss: 8921.74, base loss: 16199.60
[INFO 2017-06-29 00:54:00,931 main.py:57] epoch 1003, training loss: 7512.68, average training loss: 8906.76, base loss: 16197.36
[INFO 2017-06-29 00:54:03,964 main.py:57] epoch 1004, training loss: 9209.01, average training loss: 8895.49, base loss: 16200.48
[INFO 2017-06-29 00:54:06,997 main.py:57] epoch 1005, training loss: 7000.71, average training loss: 8882.74, base loss: 16200.25
[INFO 2017-06-29 00:54:10,021 main.py:57] epoch 1006, training loss: 8422.77, average training loss: 8873.68, base loss: 16199.41
[INFO 2017-06-29 00:54:13,053 main.py:57] epoch 1007, training loss: 7718.57, average training loss: 8864.03, base loss: 16198.21
[INFO 2017-06-29 00:54:16,069 main.py:57] epoch 1008, training loss: 7624.31, average training loss: 8855.20, base loss: 16195.60
[INFO 2017-06-29 00:54:19,243 main.py:57] epoch 1009, training loss: 7687.36, average training loss: 8846.52, base loss: 16193.32
[INFO 2017-06-29 00:54:22,361 main.py:57] epoch 1010, training loss: 7092.07, average training loss: 8840.07, base loss: 16190.55
[INFO 2017-06-29 00:54:25,444 main.py:57] epoch 1011, training loss: 8220.11, average training loss: 8835.60, base loss: 16191.71
[INFO 2017-06-29 00:54:28,545 main.py:57] epoch 1012, training loss: 7513.70, average training loss: 8828.57, base loss: 16189.00
[INFO 2017-06-29 00:54:31,632 main.py:57] epoch 1013, training loss: 8585.51, average training loss: 8824.70, base loss: 16188.89
[INFO 2017-06-29 00:54:34,595 main.py:57] epoch 1014, training loss: 9204.73, average training loss: 8821.21, base loss: 16193.22
[INFO 2017-06-29 00:54:37,638 main.py:57] epoch 1015, training loss: 7771.70, average training loss: 8817.65, base loss: 16194.89
[INFO 2017-06-29 00:54:40,741 main.py:57] epoch 1016, training loss: 8707.50, average training loss: 8814.19, base loss: 16194.34
[INFO 2017-06-29 00:54:43,818 main.py:57] epoch 1017, training loss: 9433.27, average training loss: 8812.92, base loss: 16196.37
[INFO 2017-06-29 00:54:46,893 main.py:57] epoch 1018, training loss: 8061.73, average training loss: 8808.69, base loss: 16195.10
[INFO 2017-06-29 00:54:49,914 main.py:57] epoch 1019, training loss: 9318.25, average training loss: 8806.20, base loss: 16194.77
[INFO 2017-06-29 00:54:52,979 main.py:57] epoch 1020, training loss: 7047.46, average training loss: 8801.23, base loss: 16191.63
[INFO 2017-06-29 00:54:56,050 main.py:57] epoch 1021, training loss: 8970.82, average training loss: 8798.71, base loss: 16194.11
[INFO 2017-06-29 00:54:59,062 main.py:57] epoch 1022, training loss: 7624.93, average training loss: 8796.34, base loss: 16193.25
[INFO 2017-06-29 00:55:02,067 main.py:57] epoch 1023, training loss: 8879.17, average training loss: 8793.62, base loss: 16194.59
[INFO 2017-06-29 00:55:05,065 main.py:57] epoch 1024, training loss: 8137.38, average training loss: 8789.03, base loss: 16195.70
[INFO 2017-06-29 00:55:08,102 main.py:57] epoch 1025, training loss: 7254.07, average training loss: 8783.34, base loss: 16194.79
[INFO 2017-06-29 00:55:11,127 main.py:57] epoch 1026, training loss: 8322.04, average training loss: 8782.34, base loss: 16195.30
[INFO 2017-06-29 00:55:14,210 main.py:57] epoch 1027, training loss: 8203.23, average training loss: 8778.56, base loss: 16195.64
[INFO 2017-06-29 00:55:17,224 main.py:57] epoch 1028, training loss: 7939.45, average training loss: 8772.35, base loss: 16193.51
[INFO 2017-06-29 00:55:20,299 main.py:57] epoch 1029, training loss: 7147.13, average training loss: 8765.97, base loss: 16190.23
[INFO 2017-06-29 00:55:23,353 main.py:57] epoch 1030, training loss: 8256.64, average training loss: 8762.27, base loss: 16190.37
[INFO 2017-06-29 00:55:26,443 main.py:57] epoch 1031, training loss: 7172.14, average training loss: 8759.63, base loss: 16187.79
[INFO 2017-06-29 00:55:29,486 main.py:57] epoch 1032, training loss: 8459.33, average training loss: 8757.54, base loss: 16189.04
[INFO 2017-06-29 00:55:32,545 main.py:57] epoch 1033, training loss: 8239.75, average training loss: 8753.91, base loss: 16189.78
[INFO 2017-06-29 00:55:35,663 main.py:57] epoch 1034, training loss: 7757.31, average training loss: 8750.05, base loss: 16190.01
[INFO 2017-06-29 00:55:38,757 main.py:57] epoch 1035, training loss: 7631.58, average training loss: 8747.47, base loss: 16189.26
[INFO 2017-06-29 00:55:41,795 main.py:57] epoch 1036, training loss: 7889.89, average training loss: 8745.05, base loss: 16188.33
[INFO 2017-06-29 00:55:44,827 main.py:57] epoch 1037, training loss: 8853.09, average training loss: 8741.02, base loss: 16189.65
[INFO 2017-06-29 00:55:47,935 main.py:57] epoch 1038, training loss: 8132.86, average training loss: 8738.79, base loss: 16188.65
[INFO 2017-06-29 00:55:51,003 main.py:57] epoch 1039, training loss: 8426.59, average training loss: 8737.16, base loss: 16188.55
[INFO 2017-06-29 00:55:54,016 main.py:57] epoch 1040, training loss: 8198.11, average training loss: 8733.60, base loss: 16186.19
[INFO 2017-06-29 00:55:57,087 main.py:57] epoch 1041, training loss: 6682.29, average training loss: 8729.55, base loss: 16182.92
[INFO 2017-06-29 00:56:00,094 main.py:57] epoch 1042, training loss: 7458.66, average training loss: 8724.94, base loss: 16181.62
[INFO 2017-06-29 00:56:03,181 main.py:57] epoch 1043, training loss: 6986.76, average training loss: 8720.36, base loss: 16179.92
[INFO 2017-06-29 00:56:06,296 main.py:57] epoch 1044, training loss: 8550.78, average training loss: 8715.95, base loss: 16179.81
[INFO 2017-06-29 00:56:09,263 main.py:57] epoch 1045, training loss: 7760.90, average training loss: 8712.40, base loss: 16178.96
[INFO 2017-06-29 00:56:12,328 main.py:57] epoch 1046, training loss: 7942.29, average training loss: 8708.20, base loss: 16179.45
[INFO 2017-06-29 00:56:15,473 main.py:57] epoch 1047, training loss: 7770.04, average training loss: 8704.44, base loss: 16178.38
[INFO 2017-06-29 00:56:18,523 main.py:57] epoch 1048, training loss: 7911.09, average training loss: 8701.89, base loss: 16176.43
[INFO 2017-06-29 00:56:21,555 main.py:57] epoch 1049, training loss: 8954.91, average training loss: 8699.78, base loss: 16177.22
[INFO 2017-06-29 00:56:24,589 main.py:57] epoch 1050, training loss: 8154.95, average training loss: 8695.90, base loss: 16177.49
[INFO 2017-06-29 00:56:27,702 main.py:57] epoch 1051, training loss: 8357.33, average training loss: 8691.77, base loss: 16179.02
[INFO 2017-06-29 00:56:30,723 main.py:57] epoch 1052, training loss: 7788.56, average training loss: 8687.98, base loss: 16176.85
[INFO 2017-06-29 00:56:33,788 main.py:57] epoch 1053, training loss: 7128.25, average training loss: 8683.65, base loss: 16173.20
[INFO 2017-06-29 00:56:36,803 main.py:57] epoch 1054, training loss: 7710.90, average training loss: 8680.13, base loss: 16172.00
[INFO 2017-06-29 00:56:39,788 main.py:57] epoch 1055, training loss: 7468.71, average training loss: 8675.81, base loss: 16171.49
[INFO 2017-06-29 00:56:42,857 main.py:57] epoch 1056, training loss: 8280.11, average training loss: 8671.59, base loss: 16174.76
[INFO 2017-06-29 00:56:45,897 main.py:57] epoch 1057, training loss: 8140.79, average training loss: 8668.02, base loss: 16177.80
[INFO 2017-06-29 00:56:48,957 main.py:57] epoch 1058, training loss: 9549.51, average training loss: 8666.32, base loss: 16181.15
[INFO 2017-06-29 00:56:52,014 main.py:57] epoch 1059, training loss: 8118.92, average training loss: 8661.69, base loss: 16180.66
[INFO 2017-06-29 00:56:55,116 main.py:57] epoch 1060, training loss: 9305.02, average training loss: 8658.73, base loss: 16184.39
[INFO 2017-06-29 00:56:58,345 main.py:57] epoch 1061, training loss: 7650.34, average training loss: 8654.66, base loss: 16184.30
[INFO 2017-06-29 00:57:01,410 main.py:57] epoch 1062, training loss: 8483.77, average training loss: 8650.27, base loss: 16185.08
[INFO 2017-06-29 00:57:04,483 main.py:57] epoch 1063, training loss: 8597.09, average training loss: 8647.20, base loss: 16188.13
[INFO 2017-06-29 00:57:07,568 main.py:57] epoch 1064, training loss: 8150.84, average training loss: 8643.15, base loss: 16187.85
[INFO 2017-06-29 00:57:10,607 main.py:57] epoch 1065, training loss: 8080.04, average training loss: 8641.09, base loss: 16187.96
[INFO 2017-06-29 00:57:13,738 main.py:57] epoch 1066, training loss: 8762.30, average training loss: 8638.82, base loss: 16189.49
[INFO 2017-06-29 00:57:16,786 main.py:57] epoch 1067, training loss: 10720.00, average training loss: 8637.34, base loss: 16194.30
[INFO 2017-06-29 00:57:19,872 main.py:57] epoch 1068, training loss: 8432.24, average training loss: 8635.63, base loss: 16193.99
[INFO 2017-06-29 00:57:22,970 main.py:57] epoch 1069, training loss: 7911.76, average training loss: 8633.39, base loss: 16193.19
[INFO 2017-06-29 00:57:26,023 main.py:57] epoch 1070, training loss: 8063.03, average training loss: 8628.28, base loss: 16193.04
[INFO 2017-06-29 00:57:29,159 main.py:57] epoch 1071, training loss: 7823.32, average training loss: 8624.21, base loss: 16195.23
[INFO 2017-06-29 00:57:32,170 main.py:57] epoch 1072, training loss: 8161.15, average training loss: 8621.86, base loss: 16194.39
[INFO 2017-06-29 00:57:35,304 main.py:57] epoch 1073, training loss: 8804.51, average training loss: 8619.48, base loss: 16194.88
[INFO 2017-06-29 00:57:38,422 main.py:57] epoch 1074, training loss: 8175.41, average training loss: 8617.31, base loss: 16195.93
[INFO 2017-06-29 00:57:41,507 main.py:57] epoch 1075, training loss: 9200.45, average training loss: 8616.48, base loss: 16197.73
[INFO 2017-06-29 00:57:44,616 main.py:57] epoch 1076, training loss: 7735.19, average training loss: 8613.65, base loss: 16198.55
[INFO 2017-06-29 00:57:47,660 main.py:57] epoch 1077, training loss: 7399.07, average training loss: 8608.87, base loss: 16198.81
[INFO 2017-06-29 00:57:50,746 main.py:57] epoch 1078, training loss: 7482.71, average training loss: 8603.93, base loss: 16196.83
[INFO 2017-06-29 00:57:53,775 main.py:57] epoch 1079, training loss: 9980.44, average training loss: 8602.00, base loss: 16199.24
[INFO 2017-06-29 00:57:56,832 main.py:57] epoch 1080, training loss: 9290.77, average training loss: 8600.86, base loss: 16201.17
[INFO 2017-06-29 00:57:59,917 main.py:57] epoch 1081, training loss: 8197.10, average training loss: 8598.74, base loss: 16201.89
[INFO 2017-06-29 00:58:02,920 main.py:57] epoch 1082, training loss: 8344.01, average training loss: 8595.78, base loss: 16206.04
[INFO 2017-06-29 00:58:05,944 main.py:57] epoch 1083, training loss: 8336.45, average training loss: 8593.72, base loss: 16208.67
[INFO 2017-06-29 00:58:09,031 main.py:57] epoch 1084, training loss: 7841.54, average training loss: 8589.30, base loss: 16207.35
[INFO 2017-06-29 00:58:12,037 main.py:57] epoch 1085, training loss: 7479.19, average training loss: 8587.07, base loss: 16206.06
[INFO 2017-06-29 00:58:15,177 main.py:57] epoch 1086, training loss: 7294.08, average training loss: 8583.62, base loss: 16204.43
[INFO 2017-06-29 00:58:18,227 main.py:57] epoch 1087, training loss: 7269.64, average training loss: 8580.83, base loss: 16202.92
[INFO 2017-06-29 00:58:21,301 main.py:57] epoch 1088, training loss: 7069.26, average training loss: 8578.44, base loss: 16200.23
[INFO 2017-06-29 00:58:24,320 main.py:57] epoch 1089, training loss: 8168.25, average training loss: 8576.78, base loss: 16201.07
[INFO 2017-06-29 00:58:27,387 main.py:57] epoch 1090, training loss: 8137.63, average training loss: 8572.47, base loss: 16199.59
[INFO 2017-06-29 00:58:30,495 main.py:57] epoch 1091, training loss: 6892.96, average training loss: 8570.74, base loss: 16196.16
[INFO 2017-06-29 00:58:33,596 main.py:57] epoch 1092, training loss: 8494.65, average training loss: 8569.32, base loss: 16196.58
[INFO 2017-06-29 00:58:36,865 main.py:57] epoch 1093, training loss: 8586.74, average training loss: 8567.45, base loss: 16198.98
[INFO 2017-06-29 00:58:39,996 main.py:57] epoch 1094, training loss: 8995.97, average training loss: 8565.67, base loss: 16200.54
[INFO 2017-06-29 00:58:43,092 main.py:57] epoch 1095, training loss: 7845.27, average training loss: 8561.51, base loss: 16198.41
[INFO 2017-06-29 00:58:46,239 main.py:57] epoch 1096, training loss: 8546.65, average training loss: 8559.30, base loss: 16197.82
[INFO 2017-06-29 00:58:49,278 main.py:57] epoch 1097, training loss: 7951.94, average training loss: 8556.00, base loss: 16197.13
[INFO 2017-06-29 00:58:52,393 main.py:57] epoch 1098, training loss: 7168.82, average training loss: 8553.53, base loss: 16194.93
[INFO 2017-06-29 00:58:55,475 main.py:57] epoch 1099, training loss: 8119.59, average training loss: 8549.75, base loss: 16194.37
[INFO 2017-06-29 00:58:55,476 main.py:59] epoch 1099, testing
[INFO 2017-06-29 00:59:08,311 main.py:104] average testing loss: 8341.71, base loss: 16196.90
[INFO 2017-06-29 00:59:08,311 main.py:105] improve_loss: 7855.19, improve_percent: 0.48
[INFO 2017-06-29 00:59:08,313 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 00:59:08,358 main.py:71] current best improved percent: 0.48
[INFO 2017-06-29 00:59:11,436 main.py:57] epoch 1100, training loss: 7102.53, average training loss: 8545.70, base loss: 16191.73
[INFO 2017-06-29 00:59:14,483 main.py:57] epoch 1101, training loss: 8473.32, average training loss: 8544.91, base loss: 16192.08
[INFO 2017-06-29 00:59:17,598 main.py:57] epoch 1102, training loss: 8333.93, average training loss: 8542.63, base loss: 16189.70
[INFO 2017-06-29 00:59:20,628 main.py:57] epoch 1103, training loss: 7778.06, average training loss: 8539.02, base loss: 16186.91
[INFO 2017-06-29 00:59:23,700 main.py:57] epoch 1104, training loss: 9148.02, average training loss: 8537.38, base loss: 16191.30
[INFO 2017-06-29 00:59:26,863 main.py:57] epoch 1105, training loss: 7672.40, average training loss: 8535.34, base loss: 16192.09
[INFO 2017-06-29 00:59:29,901 main.py:57] epoch 1106, training loss: 8568.97, average training loss: 8533.40, base loss: 16193.68
[INFO 2017-06-29 00:59:33,002 main.py:57] epoch 1107, training loss: 8708.44, average training loss: 8531.00, base loss: 16196.38
[INFO 2017-06-29 00:59:36,148 main.py:57] epoch 1108, training loss: 7431.52, average training loss: 8528.24, base loss: 16192.42
[INFO 2017-06-29 00:59:39,238 main.py:57] epoch 1109, training loss: 8325.59, average training loss: 8526.61, base loss: 16191.97
[INFO 2017-06-29 00:59:42,459 main.py:57] epoch 1110, training loss: 7772.39, average training loss: 8524.74, base loss: 16190.70
[INFO 2017-06-29 00:59:45,537 main.py:57] epoch 1111, training loss: 7769.23, average training loss: 8521.37, base loss: 16190.57
[INFO 2017-06-29 00:59:48,571 main.py:57] epoch 1112, training loss: 7987.70, average training loss: 8518.63, base loss: 16189.70
[INFO 2017-06-29 00:59:51,598 main.py:57] epoch 1113, training loss: 7567.98, average training loss: 8516.45, base loss: 16187.76
[INFO 2017-06-29 00:59:54,615 main.py:57] epoch 1114, training loss: 7275.07, average training loss: 8513.41, base loss: 16185.60
[INFO 2017-06-29 00:59:57,732 main.py:57] epoch 1115, training loss: 7854.75, average training loss: 8512.06, base loss: 16186.56
[INFO 2017-06-29 01:00:00,748 main.py:57] epoch 1116, training loss: 8008.62, average training loss: 8509.58, base loss: 16185.33
[INFO 2017-06-29 01:00:03,768 main.py:57] epoch 1117, training loss: 7314.11, average training loss: 8506.94, base loss: 16185.94
[INFO 2017-06-29 01:00:06,837 main.py:57] epoch 1118, training loss: 8229.15, average training loss: 8505.10, base loss: 16185.41
[INFO 2017-06-29 01:00:09,861 main.py:57] epoch 1119, training loss: 6891.56, average training loss: 8501.92, base loss: 16182.70
[INFO 2017-06-29 01:00:13,039 main.py:57] epoch 1120, training loss: 7846.73, average training loss: 8500.21, base loss: 16182.62
[INFO 2017-06-29 01:00:16,156 main.py:57] epoch 1121, training loss: 7078.44, average training loss: 8497.35, base loss: 16182.70
[INFO 2017-06-29 01:00:19,208 main.py:57] epoch 1122, training loss: 7656.84, average training loss: 8496.17, base loss: 16183.07
[INFO 2017-06-29 01:00:22,299 main.py:57] epoch 1123, training loss: 8422.02, average training loss: 8494.96, base loss: 16184.68
[INFO 2017-06-29 01:00:25,397 main.py:57] epoch 1124, training loss: 7109.25, average training loss: 8492.49, base loss: 16181.30
[INFO 2017-06-29 01:00:28,436 main.py:57] epoch 1125, training loss: 7498.10, average training loss: 8491.07, base loss: 16178.40
[INFO 2017-06-29 01:00:31,535 main.py:57] epoch 1126, training loss: 9078.65, average training loss: 8490.65, base loss: 16180.13
[INFO 2017-06-29 01:00:34,645 main.py:57] epoch 1127, training loss: 8256.16, average training loss: 8489.97, base loss: 16181.17
[INFO 2017-06-29 01:00:37,740 main.py:57] epoch 1128, training loss: 7562.72, average training loss: 8488.95, base loss: 16179.45
[INFO 2017-06-29 01:00:40,869 main.py:57] epoch 1129, training loss: 8051.59, average training loss: 8487.50, base loss: 16180.53
[INFO 2017-06-29 01:00:43,960 main.py:57] epoch 1130, training loss: 8156.30, average training loss: 8485.48, base loss: 16179.17
[INFO 2017-06-29 01:00:47,025 main.py:57] epoch 1131, training loss: 7952.50, average training loss: 8483.38, base loss: 16179.16
[INFO 2017-06-29 01:00:50,137 main.py:57] epoch 1132, training loss: 7401.27, average training loss: 8480.66, base loss: 16179.48
[INFO 2017-06-29 01:00:53,168 main.py:57] epoch 1133, training loss: 8077.48, average training loss: 8479.64, base loss: 16180.26
[INFO 2017-06-29 01:00:56,189 main.py:57] epoch 1134, training loss: 7625.55, average training loss: 8476.80, base loss: 16179.47
[INFO 2017-06-29 01:00:59,237 main.py:57] epoch 1135, training loss: 7148.26, average training loss: 8475.70, base loss: 16178.92
[INFO 2017-06-29 01:01:02,438 main.py:57] epoch 1136, training loss: 8147.98, average training loss: 8472.80, base loss: 16179.81
[INFO 2017-06-29 01:01:05,533 main.py:57] epoch 1137, training loss: 7685.83, average training loss: 8470.67, base loss: 16179.19
[INFO 2017-06-29 01:01:08,624 main.py:57] epoch 1138, training loss: 7171.67, average training loss: 8469.22, base loss: 16177.71
[INFO 2017-06-29 01:01:11,726 main.py:57] epoch 1139, training loss: 7586.26, average training loss: 8467.63, base loss: 16176.28
[INFO 2017-06-29 01:01:14,775 main.py:57] epoch 1140, training loss: 8565.32, average training loss: 8466.79, base loss: 16176.36
[INFO 2017-06-29 01:01:17,913 main.py:57] epoch 1141, training loss: 7840.39, average training loss: 8465.95, base loss: 16173.58
[INFO 2017-06-29 01:01:21,054 main.py:57] epoch 1142, training loss: 7726.82, average training loss: 8465.02, base loss: 16171.77
[INFO 2017-06-29 01:01:24,090 main.py:57] epoch 1143, training loss: 7568.90, average training loss: 8462.98, base loss: 16170.84
[INFO 2017-06-29 01:01:27,055 main.py:57] epoch 1144, training loss: 7286.96, average training loss: 8460.71, base loss: 16168.37
[INFO 2017-06-29 01:01:30,116 main.py:57] epoch 1145, training loss: 8104.00, average training loss: 8458.89, base loss: 16168.54
[INFO 2017-06-29 01:01:33,283 main.py:57] epoch 1146, training loss: 7549.06, average training loss: 8455.77, base loss: 16167.65
[INFO 2017-06-29 01:01:36,388 main.py:57] epoch 1147, training loss: 8111.36, average training loss: 8453.08, base loss: 16167.34
[INFO 2017-06-29 01:01:39,416 main.py:57] epoch 1148, training loss: 9043.20, average training loss: 8452.96, base loss: 16170.86
[INFO 2017-06-29 01:01:42,621 main.py:57] epoch 1149, training loss: 7500.69, average training loss: 8450.62, base loss: 16170.18
[INFO 2017-06-29 01:01:45,665 main.py:57] epoch 1150, training loss: 7315.92, average training loss: 8449.42, base loss: 16169.31
[INFO 2017-06-29 01:01:48,862 main.py:57] epoch 1151, training loss: 7737.40, average training loss: 8446.61, base loss: 16170.46
[INFO 2017-06-29 01:01:52,060 main.py:57] epoch 1152, training loss: 8183.85, average training loss: 8445.35, base loss: 16170.61
[INFO 2017-06-29 01:01:55,195 main.py:57] epoch 1153, training loss: 7563.24, average training loss: 8443.48, base loss: 16168.61
[INFO 2017-06-29 01:01:58,235 main.py:57] epoch 1154, training loss: 7958.16, average training loss: 8442.37, base loss: 16167.13
[INFO 2017-06-29 01:02:01,312 main.py:57] epoch 1155, training loss: 7815.89, average training loss: 8439.96, base loss: 16167.31
[INFO 2017-06-29 01:02:04,404 main.py:57] epoch 1156, training loss: 7932.11, average training loss: 8438.69, base loss: 16168.78
[INFO 2017-06-29 01:02:07,541 main.py:57] epoch 1157, training loss: 7835.86, average training loss: 8435.01, base loss: 16168.50
[INFO 2017-06-29 01:02:10,678 main.py:57] epoch 1158, training loss: 9001.65, average training loss: 8433.54, base loss: 16171.23
[INFO 2017-06-29 01:02:13,735 main.py:57] epoch 1159, training loss: 7957.12, average training loss: 8433.04, base loss: 16171.23
[INFO 2017-06-29 01:02:16,805 main.py:57] epoch 1160, training loss: 7467.64, average training loss: 8430.21, base loss: 16169.40
[INFO 2017-06-29 01:02:19,944 main.py:57] epoch 1161, training loss: 9323.30, average training loss: 8430.30, base loss: 16173.22
[INFO 2017-06-29 01:02:23,007 main.py:57] epoch 1162, training loss: 7397.01, average training loss: 8428.47, base loss: 16171.80
[INFO 2017-06-29 01:02:26,136 main.py:57] epoch 1163, training loss: 8111.14, average training loss: 8428.38, base loss: 16171.82
[INFO 2017-06-29 01:02:29,194 main.py:57] epoch 1164, training loss: 8059.45, average training loss: 8427.43, base loss: 16173.32
[INFO 2017-06-29 01:02:32,282 main.py:57] epoch 1165, training loss: 6864.54, average training loss: 8423.36, base loss: 16170.83
[INFO 2017-06-29 01:02:35,317 main.py:57] epoch 1166, training loss: 8135.45, average training loss: 8423.44, base loss: 16171.92
[INFO 2017-06-29 01:02:38,302 main.py:57] epoch 1167, training loss: 7624.94, average training loss: 8421.19, base loss: 16170.92
[INFO 2017-06-29 01:02:41,393 main.py:57] epoch 1168, training loss: 7410.96, average training loss: 8419.13, base loss: 16170.48
[INFO 2017-06-29 01:02:44,599 main.py:57] epoch 1169, training loss: 7471.66, average training loss: 8417.26, base loss: 16171.27
[INFO 2017-06-29 01:02:47,734 main.py:57] epoch 1170, training loss: 8667.35, average training loss: 8417.26, base loss: 16172.00
[INFO 2017-06-29 01:02:50,856 main.py:57] epoch 1171, training loss: 7704.74, average training loss: 8415.31, base loss: 16171.12
[INFO 2017-06-29 01:02:53,939 main.py:57] epoch 1172, training loss: 7527.58, average training loss: 8414.51, base loss: 16168.70
[INFO 2017-06-29 01:02:57,055 main.py:57] epoch 1173, training loss: 7371.02, average training loss: 8411.35, base loss: 16164.94
[INFO 2017-06-29 01:03:00,166 main.py:57] epoch 1174, training loss: 7071.09, average training loss: 8410.02, base loss: 16160.78
[INFO 2017-06-29 01:03:03,264 main.py:57] epoch 1175, training loss: 8389.31, average training loss: 8409.14, base loss: 16161.57
[INFO 2017-06-29 01:03:06,276 main.py:57] epoch 1176, training loss: 8511.76, average training loss: 8406.89, base loss: 16161.25
[INFO 2017-06-29 01:03:09,371 main.py:57] epoch 1177, training loss: 7476.29, average training loss: 8406.00, base loss: 16159.17
[INFO 2017-06-29 01:03:12,448 main.py:57] epoch 1178, training loss: 9246.34, average training loss: 8406.13, base loss: 16161.84
[INFO 2017-06-29 01:03:15,531 main.py:57] epoch 1179, training loss: 7515.25, average training loss: 8402.87, base loss: 16160.26
[INFO 2017-06-29 01:03:18,558 main.py:57] epoch 1180, training loss: 8205.12, average training loss: 8401.61, base loss: 16160.30
[INFO 2017-06-29 01:03:21,623 main.py:57] epoch 1181, training loss: 8475.62, average training loss: 8400.39, base loss: 16160.85
[INFO 2017-06-29 01:03:24,763 main.py:57] epoch 1182, training loss: 9225.16, average training loss: 8399.88, base loss: 16162.53
[INFO 2017-06-29 01:03:27,854 main.py:57] epoch 1183, training loss: 8412.17, average training loss: 8398.25, base loss: 16164.41
[INFO 2017-06-29 01:03:30,893 main.py:57] epoch 1184, training loss: 7907.63, average training loss: 8395.77, base loss: 16163.93
[INFO 2017-06-29 01:03:33,966 main.py:57] epoch 1185, training loss: 7792.02, average training loss: 8392.22, base loss: 16161.93
[INFO 2017-06-29 01:03:37,023 main.py:57] epoch 1186, training loss: 8319.91, average training loss: 8391.26, base loss: 16163.68
[INFO 2017-06-29 01:03:40,147 main.py:57] epoch 1187, training loss: 7327.85, average training loss: 8390.12, base loss: 16163.20
[INFO 2017-06-29 01:03:43,225 main.py:57] epoch 1188, training loss: 8206.90, average training loss: 8388.19, base loss: 16163.68
[INFO 2017-06-29 01:03:46,246 main.py:57] epoch 1189, training loss: 7419.08, average training loss: 8385.91, base loss: 16163.90
[INFO 2017-06-29 01:03:49,376 main.py:57] epoch 1190, training loss: 8861.59, average training loss: 8385.28, base loss: 16164.05
[INFO 2017-06-29 01:03:52,501 main.py:57] epoch 1191, training loss: 8366.58, average training loss: 8384.88, base loss: 16162.80
[INFO 2017-06-29 01:03:55,593 main.py:57] epoch 1192, training loss: 8511.70, average training loss: 8383.88, base loss: 16161.85
[INFO 2017-06-29 01:03:58,665 main.py:57] epoch 1193, training loss: 8427.60, average training loss: 8383.12, base loss: 16162.05
[INFO 2017-06-29 01:04:01,748 main.py:57] epoch 1194, training loss: 7540.24, average training loss: 8382.25, base loss: 16161.20
[INFO 2017-06-29 01:04:04,844 main.py:57] epoch 1195, training loss: 7919.51, average training loss: 8381.17, base loss: 16160.42
[INFO 2017-06-29 01:04:07,949 main.py:57] epoch 1196, training loss: 7314.11, average training loss: 8378.81, base loss: 16160.42
[INFO 2017-06-29 01:04:10,992 main.py:57] epoch 1197, training loss: 7071.64, average training loss: 8377.38, base loss: 16159.32
[INFO 2017-06-29 01:04:14,141 main.py:57] epoch 1198, training loss: 7350.70, average training loss: 8374.60, base loss: 16156.49
[INFO 2017-06-29 01:04:17,181 main.py:57] epoch 1199, training loss: 7509.33, average training loss: 8373.93, base loss: 16154.47
[INFO 2017-06-29 01:04:17,181 main.py:59] epoch 1199, testing
[INFO 2017-06-29 01:04:29,806 main.py:104] average testing loss: 8896.52, base loss: 17071.24
[INFO 2017-06-29 01:04:29,806 main.py:105] improve_loss: 8174.72, improve_percent: 0.48
[INFO 2017-06-29 01:04:29,807 main.py:71] current best improved percent: 0.48
[INFO 2017-06-29 01:04:32,866 main.py:57] epoch 1200, training loss: 8558.14, average training loss: 8373.38, base loss: 16155.66
[INFO 2017-06-29 01:04:35,923 main.py:57] epoch 1201, training loss: 7876.46, average training loss: 8370.71, base loss: 16155.37
[INFO 2017-06-29 01:04:39,070 main.py:57] epoch 1202, training loss: 7990.67, average training loss: 8366.66, base loss: 16154.68
[INFO 2017-06-29 01:04:42,124 main.py:57] epoch 1203, training loss: 8210.67, average training loss: 8365.58, base loss: 16154.13
[INFO 2017-06-29 01:04:45,180 main.py:57] epoch 1204, training loss: 7565.20, average training loss: 8363.44, base loss: 16152.45
[INFO 2017-06-29 01:04:48,220 main.py:57] epoch 1205, training loss: 7982.40, average training loss: 8362.82, base loss: 16152.40
[INFO 2017-06-29 01:04:51,314 main.py:57] epoch 1206, training loss: 7861.34, average training loss: 8359.63, base loss: 16150.64
[INFO 2017-06-29 01:04:54,363 main.py:57] epoch 1207, training loss: 8527.11, average training loss: 8358.69, base loss: 16152.68
[INFO 2017-06-29 01:04:57,487 main.py:57] epoch 1208, training loss: 9348.79, average training loss: 8357.33, base loss: 16154.81
[INFO 2017-06-29 01:05:00,583 main.py:57] epoch 1209, training loss: 7502.62, average training loss: 8353.71, base loss: 16154.00
[INFO 2017-06-29 01:05:03,675 main.py:57] epoch 1210, training loss: 7946.68, average training loss: 8352.25, base loss: 16153.89
[INFO 2017-06-29 01:05:06,719 main.py:57] epoch 1211, training loss: 7546.62, average training loss: 8349.78, base loss: 16151.07
[INFO 2017-06-29 01:05:09,733 main.py:57] epoch 1212, training loss: 8142.09, average training loss: 8347.48, base loss: 16153.17
[INFO 2017-06-29 01:05:12,868 main.py:57] epoch 1213, training loss: 7504.01, average training loss: 8346.83, base loss: 16152.86
[INFO 2017-06-29 01:05:16,001 main.py:57] epoch 1214, training loss: 7533.05, average training loss: 8345.32, base loss: 16153.19
[INFO 2017-06-29 01:05:19,129 main.py:57] epoch 1215, training loss: 7400.28, average training loss: 8343.63, base loss: 16152.27
[INFO 2017-06-29 01:05:22,266 main.py:57] epoch 1216, training loss: 8297.37, average training loss: 8342.12, base loss: 16155.46
[INFO 2017-06-29 01:05:25,325 main.py:57] epoch 1217, training loss: 6942.64, average training loss: 8341.10, base loss: 16153.29
[INFO 2017-06-29 01:05:28,394 main.py:57] epoch 1218, training loss: 8170.08, average training loss: 8339.97, base loss: 16152.38
[INFO 2017-06-29 01:05:31,492 main.py:57] epoch 1219, training loss: 7912.72, average training loss: 8337.55, base loss: 16151.73
[INFO 2017-06-29 01:05:34,522 main.py:57] epoch 1220, training loss: 7744.09, average training loss: 8336.99, base loss: 16151.26
[INFO 2017-06-29 01:05:37,644 main.py:57] epoch 1221, training loss: 7776.22, average training loss: 8335.55, base loss: 16150.42
[INFO 2017-06-29 01:05:40,755 main.py:57] epoch 1222, training loss: 8744.22, average training loss: 8334.34, base loss: 16151.54
[INFO 2017-06-29 01:05:43,836 main.py:57] epoch 1223, training loss: 8367.87, average training loss: 8334.77, base loss: 16151.63
[INFO 2017-06-29 01:05:46,918 main.py:57] epoch 1224, training loss: 6333.34, average training loss: 8331.35, base loss: 16148.99
[INFO 2017-06-29 01:05:50,022 main.py:57] epoch 1225, training loss: 8417.66, average training loss: 8329.54, base loss: 16150.34
[INFO 2017-06-29 01:05:53,184 main.py:57] epoch 1226, training loss: 8862.94, average training loss: 8329.10, base loss: 16151.59
[INFO 2017-06-29 01:05:56,253 main.py:57] epoch 1227, training loss: 8730.78, average training loss: 8327.96, base loss: 16153.60
[INFO 2017-06-29 01:05:59,304 main.py:57] epoch 1228, training loss: 8187.30, average training loss: 8326.98, base loss: 16152.99
[INFO 2017-06-29 01:06:02,406 main.py:57] epoch 1229, training loss: 7453.48, average training loss: 8325.70, base loss: 16151.76
[INFO 2017-06-29 01:06:05,447 main.py:57] epoch 1230, training loss: 8248.40, average training loss: 8323.02, base loss: 16154.42
[INFO 2017-06-29 01:06:08,520 main.py:57] epoch 1231, training loss: 9061.98, average training loss: 8322.37, base loss: 16156.18
[INFO 2017-06-29 01:06:11,650 main.py:57] epoch 1232, training loss: 7913.32, average training loss: 8319.96, base loss: 16156.09
[INFO 2017-06-29 01:06:14,796 main.py:57] epoch 1233, training loss: 7098.50, average training loss: 8318.55, base loss: 16155.22
[INFO 2017-06-29 01:06:17,840 main.py:57] epoch 1234, training loss: 8106.42, average training loss: 8317.65, base loss: 16153.73
[INFO 2017-06-29 01:06:21,019 main.py:57] epoch 1235, training loss: 8034.78, average training loss: 8316.80, base loss: 16152.82
[INFO 2017-06-29 01:06:24,146 main.py:57] epoch 1236, training loss: 7655.49, average training loss: 8316.22, base loss: 16151.34
[INFO 2017-06-29 01:06:27,312 main.py:57] epoch 1237, training loss: 7299.09, average training loss: 8314.80, base loss: 16150.44
[INFO 2017-06-29 01:06:30,428 main.py:57] epoch 1238, training loss: 8088.44, average training loss: 8313.78, base loss: 16149.42
[INFO 2017-06-29 01:06:33,514 main.py:57] epoch 1239, training loss: 7470.62, average training loss: 8311.87, base loss: 16149.20
[INFO 2017-06-29 01:06:36,567 main.py:57] epoch 1240, training loss: 8118.09, average training loss: 8311.96, base loss: 16149.72
[INFO 2017-06-29 01:06:39,551 main.py:57] epoch 1241, training loss: 8340.55, average training loss: 8310.82, base loss: 16149.57
[INFO 2017-06-29 01:06:42,622 main.py:57] epoch 1242, training loss: 8373.04, average training loss: 8310.57, base loss: 16151.09
[INFO 2017-06-29 01:06:45,710 main.py:57] epoch 1243, training loss: 8267.39, average training loss: 8310.94, base loss: 16150.32
[INFO 2017-06-29 01:06:48,776 main.py:57] epoch 1244, training loss: 8422.31, average training loss: 8308.31, base loss: 16152.91
[INFO 2017-06-29 01:06:51,814 main.py:57] epoch 1245, training loss: 7083.14, average training loss: 8304.94, base loss: 16153.34
[INFO 2017-06-29 01:06:54,876 main.py:57] epoch 1246, training loss: 7485.49, average training loss: 8302.78, base loss: 16151.58
[INFO 2017-06-29 01:06:57,952 main.py:57] epoch 1247, training loss: 7592.92, average training loss: 8299.85, base loss: 16152.06
[INFO 2017-06-29 01:07:01,017 main.py:57] epoch 1248, training loss: 8904.25, average training loss: 8299.06, base loss: 16155.01
[INFO 2017-06-29 01:07:04,059 main.py:57] epoch 1249, training loss: 8361.74, average training loss: 8298.57, base loss: 16155.16
[INFO 2017-06-29 01:07:07,105 main.py:57] epoch 1250, training loss: 8175.41, average training loss: 8297.61, base loss: 16157.27
[INFO 2017-06-29 01:07:10,144 main.py:57] epoch 1251, training loss: 8675.51, average training loss: 8296.48, base loss: 16157.78
[INFO 2017-06-29 01:07:13,271 main.py:57] epoch 1252, training loss: 8023.91, average training loss: 8295.95, base loss: 16158.06
[INFO 2017-06-29 01:07:16,500 main.py:57] epoch 1253, training loss: 7667.82, average training loss: 8294.07, base loss: 16156.13
[INFO 2017-06-29 01:07:19,522 main.py:57] epoch 1254, training loss: 7477.78, average training loss: 8291.21, base loss: 16154.93
[INFO 2017-06-29 01:07:22,614 main.py:57] epoch 1255, training loss: 8111.13, average training loss: 8290.50, base loss: 16156.05
[INFO 2017-06-29 01:07:25,666 main.py:57] epoch 1256, training loss: 7464.42, average training loss: 8287.59, base loss: 16155.53
[INFO 2017-06-29 01:07:28,700 main.py:57] epoch 1257, training loss: 7501.04, average training loss: 8285.95, base loss: 16155.76
[INFO 2017-06-29 01:07:31,835 main.py:57] epoch 1258, training loss: 8989.91, average training loss: 8286.50, base loss: 16156.60
[INFO 2017-06-29 01:07:34,909 main.py:57] epoch 1259, training loss: 7405.67, average training loss: 8282.84, base loss: 16155.62
[INFO 2017-06-29 01:07:38,042 main.py:57] epoch 1260, training loss: 8237.06, average training loss: 8282.11, base loss: 16155.31
[INFO 2017-06-29 01:07:41,102 main.py:57] epoch 1261, training loss: 9055.22, average training loss: 8282.12, base loss: 16157.79
[INFO 2017-06-29 01:07:44,155 main.py:57] epoch 1262, training loss: 7791.21, average training loss: 8281.31, base loss: 16156.71
[INFO 2017-06-29 01:07:47,171 main.py:57] epoch 1263, training loss: 7112.02, average training loss: 8279.06, base loss: 16155.88
[INFO 2017-06-29 01:07:50,207 main.py:57] epoch 1264, training loss: 7987.76, average training loss: 8276.96, base loss: 16156.59
[INFO 2017-06-29 01:07:53,286 main.py:57] epoch 1265, training loss: 7677.00, average training loss: 8275.67, base loss: 16156.47
[INFO 2017-06-29 01:07:56,363 main.py:57] epoch 1266, training loss: 7284.69, average training loss: 8274.43, base loss: 16156.58
[INFO 2017-06-29 01:07:59,431 main.py:57] epoch 1267, training loss: 7118.51, average training loss: 8273.25, base loss: 16153.36
[INFO 2017-06-29 01:08:02,476 main.py:57] epoch 1268, training loss: 7712.78, average training loss: 8270.10, base loss: 16153.15
[INFO 2017-06-29 01:08:05,526 main.py:57] epoch 1269, training loss: 6781.29, average training loss: 8266.11, base loss: 16149.43
[INFO 2017-06-29 01:08:08,572 main.py:57] epoch 1270, training loss: 7350.89, average training loss: 8263.31, base loss: 16148.67
[INFO 2017-06-29 01:08:11,723 main.py:57] epoch 1271, training loss: 7984.71, average training loss: 8263.18, base loss: 16150.49
[INFO 2017-06-29 01:08:14,823 main.py:57] epoch 1272, training loss: 8694.59, average training loss: 8262.77, base loss: 16151.54
[INFO 2017-06-29 01:08:17,921 main.py:57] epoch 1273, training loss: 7818.07, average training loss: 8261.53, base loss: 16150.70
[INFO 2017-06-29 01:08:20,949 main.py:57] epoch 1274, training loss: 7480.56, average training loss: 8259.01, base loss: 16148.71
[INFO 2017-06-29 01:08:24,004 main.py:57] epoch 1275, training loss: 7854.54, average training loss: 8258.66, base loss: 16147.81
[INFO 2017-06-29 01:08:27,102 main.py:57] epoch 1276, training loss: 7726.79, average training loss: 8257.24, base loss: 16146.47
[INFO 2017-06-29 01:08:30,191 main.py:57] epoch 1277, training loss: 8868.48, average training loss: 8257.00, base loss: 16149.38
[INFO 2017-06-29 01:08:33,241 main.py:57] epoch 1278, training loss: 8697.89, average training loss: 8257.71, base loss: 16150.72
[INFO 2017-06-29 01:08:36,350 main.py:57] epoch 1279, training loss: 9466.08, average training loss: 8257.46, base loss: 16154.85
[INFO 2017-06-29 01:08:39,406 main.py:57] epoch 1280, training loss: 7185.92, average training loss: 8256.10, base loss: 16151.87
[INFO 2017-06-29 01:08:42,477 main.py:57] epoch 1281, training loss: 8460.32, average training loss: 8256.54, base loss: 16152.72
[INFO 2017-06-29 01:08:45,515 main.py:57] epoch 1282, training loss: 8524.13, average training loss: 8257.28, base loss: 16151.09
[INFO 2017-06-29 01:08:48,584 main.py:57] epoch 1283, training loss: 7419.16, average training loss: 8256.08, base loss: 16148.87
[INFO 2017-06-29 01:08:51,619 main.py:57] epoch 1284, training loss: 7930.36, average training loss: 8255.63, base loss: 16147.31
[INFO 2017-06-29 01:08:54,690 main.py:57] epoch 1285, training loss: 7959.11, average training loss: 8254.70, base loss: 16148.15
[INFO 2017-06-29 01:08:57,692 main.py:57] epoch 1286, training loss: 8070.48, average training loss: 8253.47, base loss: 16148.26
[INFO 2017-06-29 01:09:00,752 main.py:57] epoch 1287, training loss: 7163.18, average training loss: 8252.57, base loss: 16146.11
[INFO 2017-06-29 01:09:03,821 main.py:57] epoch 1288, training loss: 8044.38, average training loss: 8252.54, base loss: 16147.01
[INFO 2017-06-29 01:09:06,892 main.py:57] epoch 1289, training loss: 7745.56, average training loss: 8250.81, base loss: 16145.70
[INFO 2017-06-29 01:09:09,996 main.py:57] epoch 1290, training loss: 7071.95, average training loss: 8249.15, base loss: 16143.63
[INFO 2017-06-29 01:09:13,090 main.py:57] epoch 1291, training loss: 7757.88, average training loss: 8249.29, base loss: 16142.35
[INFO 2017-06-29 01:09:16,243 main.py:57] epoch 1292, training loss: 7679.42, average training loss: 8248.14, base loss: 16139.58
[INFO 2017-06-29 01:09:19,395 main.py:57] epoch 1293, training loss: 8888.67, average training loss: 8249.59, base loss: 16140.30
[INFO 2017-06-29 01:09:22,421 main.py:57] epoch 1294, training loss: 7525.56, average training loss: 8248.40, base loss: 16140.46
[INFO 2017-06-29 01:09:25,433 main.py:57] epoch 1295, training loss: 7744.13, average training loss: 8246.74, base loss: 16141.03
[INFO 2017-06-29 01:09:28,553 main.py:57] epoch 1296, training loss: 8248.77, average training loss: 8246.73, base loss: 16141.50
[INFO 2017-06-29 01:09:31,613 main.py:57] epoch 1297, training loss: 7237.43, average training loss: 8245.29, base loss: 16141.80
[INFO 2017-06-29 01:09:34,673 main.py:57] epoch 1298, training loss: 7122.20, average training loss: 8243.23, base loss: 16140.42
[INFO 2017-06-29 01:09:37,768 main.py:57] epoch 1299, training loss: 7990.04, average training loss: 8242.51, base loss: 16139.98
[INFO 2017-06-29 01:09:37,768 main.py:59] epoch 1299, testing
[INFO 2017-06-29 01:09:50,422 main.py:104] average testing loss: 8293.83, base loss: 16238.62
[INFO 2017-06-29 01:09:50,422 main.py:105] improve_loss: 7944.79, improve_percent: 0.49
[INFO 2017-06-29 01:09:50,423 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 01:09:50,461 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 01:09:53,539 main.py:57] epoch 1300, training loss: 7713.12, average training loss: 8240.71, base loss: 16138.96
[INFO 2017-06-29 01:09:56,610 main.py:57] epoch 1301, training loss: 6732.52, average training loss: 8237.13, base loss: 16135.95
[INFO 2017-06-29 01:09:59,633 main.py:57] epoch 1302, training loss: 10236.79, average training loss: 8239.48, base loss: 16139.01
[INFO 2017-06-29 01:10:02,653 main.py:57] epoch 1303, training loss: 7629.50, average training loss: 8238.58, base loss: 16138.32
[INFO 2017-06-29 01:10:05,634 main.py:57] epoch 1304, training loss: 7466.67, average training loss: 8235.76, base loss: 16135.19
[INFO 2017-06-29 01:10:08,710 main.py:57] epoch 1305, training loss: 7623.59, average training loss: 8234.68, base loss: 16133.13
[INFO 2017-06-29 01:10:11,780 main.py:57] epoch 1306, training loss: 8110.58, average training loss: 8234.01, base loss: 16136.14
[INFO 2017-06-29 01:10:14,859 main.py:57] epoch 1307, training loss: 9154.20, average training loss: 8235.68, base loss: 16138.86
[INFO 2017-06-29 01:10:17,929 main.py:57] epoch 1308, training loss: 8272.00, average training loss: 8236.22, base loss: 16138.83
[INFO 2017-06-29 01:10:20,966 main.py:57] epoch 1309, training loss: 8597.40, average training loss: 8234.65, base loss: 16140.10
[INFO 2017-06-29 01:10:24,016 main.py:57] epoch 1310, training loss: 9217.63, average training loss: 8236.26, base loss: 16140.96
[INFO 2017-06-29 01:10:27,035 main.py:57] epoch 1311, training loss: 8368.81, average training loss: 8236.66, base loss: 16141.72
[INFO 2017-06-29 01:10:30,083 main.py:57] epoch 1312, training loss: 8353.95, average training loss: 8234.09, base loss: 16142.42
[INFO 2017-06-29 01:10:33,099 main.py:57] epoch 1313, training loss: 7324.32, average training loss: 8233.22, base loss: 16140.85
[INFO 2017-06-29 01:10:36,288 main.py:57] epoch 1314, training loss: 7031.52, average training loss: 8231.17, base loss: 16138.65
[INFO 2017-06-29 01:10:39,296 main.py:57] epoch 1315, training loss: 6926.03, average training loss: 8229.23, base loss: 16135.19
[INFO 2017-06-29 01:10:42,344 main.py:57] epoch 1316, training loss: 8568.10, average training loss: 8229.53, base loss: 16137.22
[INFO 2017-06-29 01:10:45,478 main.py:57] epoch 1317, training loss: 8323.51, average training loss: 8230.11, base loss: 16137.86
[INFO 2017-06-29 01:10:48,521 main.py:57] epoch 1318, training loss: 9067.01, average training loss: 8231.06, base loss: 16139.37
[INFO 2017-06-29 01:10:51,657 main.py:57] epoch 1319, training loss: 7712.85, average training loss: 8229.58, base loss: 16138.91
[INFO 2017-06-29 01:10:54,660 main.py:57] epoch 1320, training loss: 7831.30, average training loss: 8227.06, base loss: 16137.96
[INFO 2017-06-29 01:10:57,694 main.py:57] epoch 1321, training loss: 7031.30, average training loss: 8225.64, base loss: 16134.24
[INFO 2017-06-29 01:11:00,764 main.py:57] epoch 1322, training loss: 7825.40, average training loss: 8224.20, base loss: 16133.54
[INFO 2017-06-29 01:11:03,945 main.py:57] epoch 1323, training loss: 7330.17, average training loss: 8222.27, base loss: 16132.32
[INFO 2017-06-29 01:11:07,012 main.py:57] epoch 1324, training loss: 7337.35, average training loss: 8221.98, base loss: 16130.89
[INFO 2017-06-29 01:11:10,174 main.py:57] epoch 1325, training loss: 7755.57, average training loss: 8221.40, base loss: 16128.48
[INFO 2017-06-29 01:11:13,161 main.py:57] epoch 1326, training loss: 7535.84, average training loss: 8219.79, base loss: 16126.84
[INFO 2017-06-29 01:11:16,258 main.py:57] epoch 1327, training loss: 7793.74, average training loss: 8218.44, base loss: 16126.57
[INFO 2017-06-29 01:11:19,249 main.py:57] epoch 1328, training loss: 7489.84, average training loss: 8218.07, base loss: 16124.48
[INFO 2017-06-29 01:11:22,344 main.py:57] epoch 1329, training loss: 8668.54, average training loss: 8218.30, base loss: 16126.60
[INFO 2017-06-29 01:11:25,353 main.py:57] epoch 1330, training loss: 7819.58, average training loss: 8217.33, base loss: 16125.77
[INFO 2017-06-29 01:11:28,318 main.py:57] epoch 1331, training loss: 7997.38, average training loss: 8216.22, base loss: 16126.28
[INFO 2017-06-29 01:11:31,320 main.py:57] epoch 1332, training loss: 7942.21, average training loss: 8215.52, base loss: 16126.74
[INFO 2017-06-29 01:11:34,349 main.py:57] epoch 1333, training loss: 9253.77, average training loss: 8216.13, base loss: 16131.17
[INFO 2017-06-29 01:11:37,412 main.py:57] epoch 1334, training loss: 7743.11, average training loss: 8215.55, base loss: 16130.02
[INFO 2017-06-29 01:11:40,490 main.py:57] epoch 1335, training loss: 8153.57, average training loss: 8213.33, base loss: 16129.53
[INFO 2017-06-29 01:11:43,637 main.py:57] epoch 1336, training loss: 8304.01, average training loss: 8211.74, base loss: 16129.83
[INFO 2017-06-29 01:11:46,708 main.py:57] epoch 1337, training loss: 7040.10, average training loss: 8209.64, base loss: 16126.53
[INFO 2017-06-29 01:11:49,763 main.py:57] epoch 1338, training loss: 8561.59, average training loss: 8209.21, base loss: 16126.48
[INFO 2017-06-29 01:11:52,803 main.py:57] epoch 1339, training loss: 6730.99, average training loss: 8207.32, base loss: 16123.37
[INFO 2017-06-29 01:11:55,845 main.py:57] epoch 1340, training loss: 9114.93, average training loss: 8208.97, base loss: 16124.09
[INFO 2017-06-29 01:11:58,949 main.py:57] epoch 1341, training loss: 7840.33, average training loss: 8207.18, base loss: 16123.83
[INFO 2017-06-29 01:12:02,070 main.py:57] epoch 1342, training loss: 8398.93, average training loss: 8206.46, base loss: 16124.39
[INFO 2017-06-29 01:12:05,101 main.py:57] epoch 1343, training loss: 8153.99, average training loss: 8206.48, base loss: 16124.97
[INFO 2017-06-29 01:12:08,188 main.py:57] epoch 1344, training loss: 6844.68, average training loss: 8206.14, base loss: 16124.09
[INFO 2017-06-29 01:12:11,308 main.py:57] epoch 1345, training loss: 7360.82, average training loss: 8203.73, base loss: 16123.68
[INFO 2017-06-29 01:12:14,301 main.py:57] epoch 1346, training loss: 8609.55, average training loss: 8203.03, base loss: 16125.52
[INFO 2017-06-29 01:12:17,327 main.py:57] epoch 1347, training loss: 7036.90, average training loss: 8202.09, base loss: 16124.52
[INFO 2017-06-29 01:12:20,394 main.py:57] epoch 1348, training loss: 8001.80, average training loss: 8200.49, base loss: 16126.04
[INFO 2017-06-29 01:12:23,525 main.py:57] epoch 1349, training loss: 7358.80, average training loss: 8199.27, base loss: 16126.71
[INFO 2017-06-29 01:12:26,480 main.py:57] epoch 1350, training loss: 7234.83, average training loss: 8197.76, base loss: 16124.96
[INFO 2017-06-29 01:12:29,557 main.py:57] epoch 1351, training loss: 7416.23, average training loss: 8194.58, base loss: 16122.43
[INFO 2017-06-29 01:12:32,619 main.py:57] epoch 1352, training loss: 8272.17, average training loss: 8194.09, base loss: 16124.46
[INFO 2017-06-29 01:12:35,668 main.py:57] epoch 1353, training loss: 8043.60, average training loss: 8192.84, base loss: 16125.26
[INFO 2017-06-29 01:12:38,767 main.py:57] epoch 1354, training loss: 6550.78, average training loss: 8191.11, base loss: 16121.87
[INFO 2017-06-29 01:12:41,776 main.py:57] epoch 1355, training loss: 7740.02, average training loss: 8189.78, base loss: 16122.33
[INFO 2017-06-29 01:12:44,814 main.py:57] epoch 1356, training loss: 8852.52, average training loss: 8190.65, base loss: 16124.23
[INFO 2017-06-29 01:12:47,874 main.py:57] epoch 1357, training loss: 7517.97, average training loss: 8189.40, base loss: 16123.14
[INFO 2017-06-29 01:12:50,921 main.py:57] epoch 1358, training loss: 7057.31, average training loss: 8188.20, base loss: 16121.90
[INFO 2017-06-29 01:12:54,069 main.py:57] epoch 1359, training loss: 7416.76, average training loss: 8185.70, base loss: 16120.95
[INFO 2017-06-29 01:12:57,105 main.py:57] epoch 1360, training loss: 7768.97, average training loss: 8185.32, base loss: 16119.90
[INFO 2017-06-29 01:13:00,195 main.py:57] epoch 1361, training loss: 7316.90, average training loss: 8184.45, base loss: 16118.94
[INFO 2017-06-29 01:13:03,245 main.py:57] epoch 1362, training loss: 7189.83, average training loss: 8183.56, base loss: 16117.38
[INFO 2017-06-29 01:13:06,276 main.py:57] epoch 1363, training loss: 7589.83, average training loss: 8182.72, base loss: 16116.99
[INFO 2017-06-29 01:13:09,333 main.py:57] epoch 1364, training loss: 7528.03, average training loss: 8181.14, base loss: 16115.27
[INFO 2017-06-29 01:13:12,428 main.py:57] epoch 1365, training loss: 7295.07, average training loss: 8180.33, base loss: 16113.37
[INFO 2017-06-29 01:13:15,469 main.py:57] epoch 1366, training loss: 9406.45, average training loss: 8181.71, base loss: 16114.96
[INFO 2017-06-29 01:13:18,508 main.py:57] epoch 1367, training loss: 8645.60, average training loss: 8179.16, base loss: 16115.53
[INFO 2017-06-29 01:13:21,623 main.py:57] epoch 1368, training loss: 9184.74, average training loss: 8180.45, base loss: 16116.25
[INFO 2017-06-29 01:13:24,643 main.py:57] epoch 1369, training loss: 7722.17, average training loss: 8178.43, base loss: 16116.61
[INFO 2017-06-29 01:13:27,709 main.py:57] epoch 1370, training loss: 7419.55, average training loss: 8177.08, base loss: 16114.14
[INFO 2017-06-29 01:13:30,833 main.py:57] epoch 1371, training loss: 8154.35, average training loss: 8176.28, base loss: 16113.69
[INFO 2017-06-29 01:13:33,861 main.py:57] epoch 1372, training loss: 7959.52, average training loss: 8176.21, base loss: 16115.37
[INFO 2017-06-29 01:13:36,886 main.py:57] epoch 1373, training loss: 8645.35, average training loss: 8176.51, base loss: 16119.25
[INFO 2017-06-29 01:13:39,874 main.py:57] epoch 1374, training loss: 7638.80, average training loss: 8176.01, base loss: 16117.86
[INFO 2017-06-29 01:13:42,983 main.py:57] epoch 1375, training loss: 7302.37, average training loss: 8174.97, base loss: 16116.71
[INFO 2017-06-29 01:13:46,083 main.py:57] epoch 1376, training loss: 7609.93, average training loss: 8172.69, base loss: 16116.78
[INFO 2017-06-29 01:13:49,127 main.py:57] epoch 1377, training loss: 8038.63, average training loss: 8172.94, base loss: 16117.12
[INFO 2017-06-29 01:13:52,254 main.py:57] epoch 1378, training loss: 7480.51, average training loss: 8172.18, base loss: 16116.05
[INFO 2017-06-29 01:13:55,319 main.py:57] epoch 1379, training loss: 8098.53, average training loss: 8170.46, base loss: 16115.63
[INFO 2017-06-29 01:13:58,336 main.py:57] epoch 1380, training loss: 8116.13, average training loss: 8169.86, base loss: 16114.86
[INFO 2017-06-29 01:14:01,326 main.py:57] epoch 1381, training loss: 8449.15, average training loss: 8169.93, base loss: 16114.35
[INFO 2017-06-29 01:14:04,410 main.py:57] epoch 1382, training loss: 8346.73, average training loss: 8169.67, base loss: 16115.33
[INFO 2017-06-29 01:14:07,461 main.py:57] epoch 1383, training loss: 7964.37, average training loss: 8169.59, base loss: 16114.27
[INFO 2017-06-29 01:14:10,546 main.py:57] epoch 1384, training loss: 7335.64, average training loss: 8167.60, base loss: 16113.49
[INFO 2017-06-29 01:14:13,595 main.py:57] epoch 1385, training loss: 7937.87, average training loss: 8167.33, base loss: 16113.17
[INFO 2017-06-29 01:14:16,608 main.py:57] epoch 1386, training loss: 6786.58, average training loss: 8165.33, base loss: 16111.31
[INFO 2017-06-29 01:14:19,624 main.py:57] epoch 1387, training loss: 7133.61, average training loss: 8163.65, base loss: 16110.04
[INFO 2017-06-29 01:14:22,645 main.py:57] epoch 1388, training loss: 7373.28, average training loss: 8161.67, base loss: 16108.44
[INFO 2017-06-29 01:14:25,757 main.py:57] epoch 1389, training loss: 7932.84, average training loss: 8161.81, base loss: 16106.96
[INFO 2017-06-29 01:14:28,778 main.py:57] epoch 1390, training loss: 7360.27, average training loss: 8160.39, base loss: 16105.94
[INFO 2017-06-29 01:14:31,833 main.py:57] epoch 1391, training loss: 7952.80, average training loss: 8159.48, base loss: 16106.43
[INFO 2017-06-29 01:14:34,825 main.py:57] epoch 1392, training loss: 7834.40, average training loss: 8158.06, base loss: 16106.43
[INFO 2017-06-29 01:14:37,929 main.py:57] epoch 1393, training loss: 8657.16, average training loss: 8157.95, base loss: 16106.78
[INFO 2017-06-29 01:14:40,993 main.py:57] epoch 1394, training loss: 7245.89, average training loss: 8157.10, base loss: 16103.28
[INFO 2017-06-29 01:14:44,031 main.py:57] epoch 1395, training loss: 8112.77, average training loss: 8156.87, base loss: 16102.50
[INFO 2017-06-29 01:14:47,082 main.py:57] epoch 1396, training loss: 8788.58, average training loss: 8155.90, base loss: 16104.56
[INFO 2017-06-29 01:14:50,117 main.py:57] epoch 1397, training loss: 7828.03, average training loss: 8156.09, base loss: 16105.26
[INFO 2017-06-29 01:14:53,156 main.py:57] epoch 1398, training loss: 8704.55, average training loss: 8157.53, base loss: 16105.51
[INFO 2017-06-29 01:14:56,220 main.py:57] epoch 1399, training loss: 8295.92, average training loss: 8156.59, base loss: 16104.51
[INFO 2017-06-29 01:14:56,221 main.py:59] epoch 1399, testing
[INFO 2017-06-29 01:15:08,924 main.py:104] average testing loss: 8786.91, base loss: 17135.84
[INFO 2017-06-29 01:15:08,924 main.py:105] improve_loss: 8348.93, improve_percent: 0.49
[INFO 2017-06-29 01:15:08,925 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 01:15:11,979 main.py:57] epoch 1400, training loss: 8359.22, average training loss: 8156.05, base loss: 16105.56
[INFO 2017-06-29 01:15:14,998 main.py:57] epoch 1401, training loss: 7164.04, average training loss: 8153.58, base loss: 16104.27
[INFO 2017-06-29 01:15:18,068 main.py:57] epoch 1402, training loss: 7517.31, average training loss: 8152.59, base loss: 16104.52
[INFO 2017-06-29 01:15:21,162 main.py:57] epoch 1403, training loss: 7431.45, average training loss: 8152.65, base loss: 16103.06
[INFO 2017-06-29 01:15:24,142 main.py:57] epoch 1404, training loss: 7886.73, average training loss: 8150.84, base loss: 16103.01
[INFO 2017-06-29 01:15:27,334 main.py:57] epoch 1405, training loss: 8656.90, average training loss: 8150.33, base loss: 16105.01
[INFO 2017-06-29 01:15:30,401 main.py:57] epoch 1406, training loss: 6825.22, average training loss: 8148.80, base loss: 16101.89
[INFO 2017-06-29 01:15:33,446 main.py:57] epoch 1407, training loss: 8224.28, average training loss: 8147.67, base loss: 16102.21
[INFO 2017-06-29 01:15:36,499 main.py:57] epoch 1408, training loss: 8429.58, average training loss: 8146.83, base loss: 16101.99
[INFO 2017-06-29 01:15:39,519 main.py:57] epoch 1409, training loss: 8105.49, average training loss: 8146.66, base loss: 16101.65
[INFO 2017-06-29 01:15:42,610 main.py:57] epoch 1410, training loss: 6979.03, average training loss: 8144.34, base loss: 16099.63
[INFO 2017-06-29 01:15:45,652 main.py:57] epoch 1411, training loss: 8566.65, average training loss: 8144.90, base loss: 16100.18
[INFO 2017-06-29 01:15:48,683 main.py:57] epoch 1412, training loss: 7177.32, average training loss: 8143.34, base loss: 16098.65
[INFO 2017-06-29 01:15:51,720 main.py:57] epoch 1413, training loss: 8338.36, average training loss: 8143.40, base loss: 16098.56
[INFO 2017-06-29 01:15:54,757 main.py:57] epoch 1414, training loss: 6851.85, average training loss: 8141.59, base loss: 16097.98
[INFO 2017-06-29 01:15:57,792 main.py:57] epoch 1415, training loss: 6768.07, average training loss: 8140.89, base loss: 16097.53
[INFO 2017-06-29 01:16:00,781 main.py:57] epoch 1416, training loss: 7720.41, average training loss: 8141.18, base loss: 16098.69
[INFO 2017-06-29 01:16:03,816 main.py:57] epoch 1417, training loss: 9413.76, average training loss: 8141.95, base loss: 16101.46
[INFO 2017-06-29 01:16:06,814 main.py:57] epoch 1418, training loss: 7038.50, average training loss: 8138.98, base loss: 16099.32
[INFO 2017-06-29 01:16:09,893 main.py:57] epoch 1419, training loss: 7651.07, average training loss: 8138.55, base loss: 16098.54
[INFO 2017-06-29 01:16:12,974 main.py:57] epoch 1420, training loss: 8925.97, average training loss: 8138.43, base loss: 16099.45
[INFO 2017-06-29 01:16:16,065 main.py:57] epoch 1421, training loss: 7551.07, average training loss: 8137.93, base loss: 16099.09
[INFO 2017-06-29 01:16:19,099 main.py:57] epoch 1422, training loss: 7827.46, average training loss: 8137.84, base loss: 16099.02
[INFO 2017-06-29 01:16:22,160 main.py:57] epoch 1423, training loss: 7899.68, average training loss: 8136.95, base loss: 16099.18
[INFO 2017-06-29 01:16:25,192 main.py:57] epoch 1424, training loss: 9219.13, average training loss: 8138.65, base loss: 16102.00
[INFO 2017-06-29 01:16:28,338 main.py:57] epoch 1425, training loss: 8681.38, average training loss: 8138.30, base loss: 16102.91
[INFO 2017-06-29 01:16:31,399 main.py:57] epoch 1426, training loss: 7371.49, average training loss: 8137.75, base loss: 16102.15
[INFO 2017-06-29 01:16:34,402 main.py:57] epoch 1427, training loss: 7884.69, average training loss: 8136.39, base loss: 16101.88
[INFO 2017-06-29 01:16:37,381 main.py:57] epoch 1428, training loss: 7965.01, average training loss: 8136.28, base loss: 16101.10
[INFO 2017-06-29 01:16:40,370 main.py:57] epoch 1429, training loss: 8900.38, average training loss: 8137.08, base loss: 16102.75
[INFO 2017-06-29 01:16:43,409 main.py:57] epoch 1430, training loss: 7612.33, average training loss: 8135.89, base loss: 16101.36
[INFO 2017-06-29 01:16:46,410 main.py:57] epoch 1431, training loss: 9283.76, average training loss: 8136.60, base loss: 16101.82
[INFO 2017-06-29 01:16:49,431 main.py:57] epoch 1432, training loss: 8070.14, average training loss: 8136.72, base loss: 16102.39
[INFO 2017-06-29 01:16:52,506 main.py:57] epoch 1433, training loss: 8068.20, average training loss: 8137.26, base loss: 16101.77
[INFO 2017-06-29 01:16:55,597 main.py:57] epoch 1434, training loss: 10471.26, average training loss: 8138.19, base loss: 16107.36
[INFO 2017-06-29 01:16:58,672 main.py:57] epoch 1435, training loss: 7477.73, average training loss: 8137.09, base loss: 16106.37
[INFO 2017-06-29 01:17:01,723 main.py:57] epoch 1436, training loss: 7738.19, average training loss: 8137.30, base loss: 16107.30
[INFO 2017-06-29 01:17:04,741 main.py:57] epoch 1437, training loss: 7767.17, average training loss: 8135.66, base loss: 16107.45
[INFO 2017-06-29 01:17:07,740 main.py:57] epoch 1438, training loss: 8230.43, average training loss: 8134.96, base loss: 16109.29
[INFO 2017-06-29 01:17:10,807 main.py:57] epoch 1439, training loss: 8079.04, average training loss: 8134.40, base loss: 16110.06
[INFO 2017-06-29 01:17:13,898 main.py:57] epoch 1440, training loss: 7692.16, average training loss: 8132.02, base loss: 16107.86
[INFO 2017-06-29 01:17:17,005 main.py:57] epoch 1441, training loss: 8001.99, average training loss: 8131.98, base loss: 16106.78
[INFO 2017-06-29 01:17:20,068 main.py:57] epoch 1442, training loss: 7565.33, average training loss: 8130.34, base loss: 16106.09
[INFO 2017-06-29 01:17:23,145 main.py:57] epoch 1443, training loss: 8043.31, average training loss: 8130.39, base loss: 16105.42
[INFO 2017-06-29 01:17:26,162 main.py:57] epoch 1444, training loss: 8014.77, average training loss: 8129.75, base loss: 16105.80
[INFO 2017-06-29 01:17:29,293 main.py:57] epoch 1445, training loss: 8409.52, average training loss: 8129.73, base loss: 16106.74
[INFO 2017-06-29 01:17:32,384 main.py:57] epoch 1446, training loss: 7664.85, average training loss: 8128.64, base loss: 16108.88
[INFO 2017-06-29 01:17:35,454 main.py:57] epoch 1447, training loss: 8414.04, average training loss: 8129.05, base loss: 16110.44
[INFO 2017-06-29 01:17:38,580 main.py:57] epoch 1448, training loss: 8168.56, average training loss: 8129.19, base loss: 16111.72
[INFO 2017-06-29 01:17:41,652 main.py:57] epoch 1449, training loss: 9262.29, average training loss: 8129.71, base loss: 16114.88
[INFO 2017-06-29 01:17:44,647 main.py:57] epoch 1450, training loss: 7806.05, average training loss: 8128.86, base loss: 16115.79
[INFO 2017-06-29 01:17:47,678 main.py:57] epoch 1451, training loss: 7645.85, average training loss: 8127.72, base loss: 16115.22
[INFO 2017-06-29 01:17:50,737 main.py:57] epoch 1452, training loss: 7492.19, average training loss: 8127.50, base loss: 16115.99
[INFO 2017-06-29 01:17:53,770 main.py:57] epoch 1453, training loss: 8216.27, average training loss: 8126.64, base loss: 16117.93
[INFO 2017-06-29 01:17:56,785 main.py:57] epoch 1454, training loss: 7434.22, average training loss: 8124.24, base loss: 16115.27
[INFO 2017-06-29 01:17:59,807 main.py:57] epoch 1455, training loss: 6853.20, average training loss: 8121.92, base loss: 16112.89
[INFO 2017-06-29 01:18:02,889 main.py:57] epoch 1456, training loss: 8824.36, average training loss: 8123.17, base loss: 16113.87
[INFO 2017-06-29 01:18:05,977 main.py:57] epoch 1457, training loss: 7813.10, average training loss: 8124.17, base loss: 16113.83
[INFO 2017-06-29 01:18:09,055 main.py:57] epoch 1458, training loss: 8307.18, average training loss: 8123.77, base loss: 16115.66
[INFO 2017-06-29 01:18:12,064 main.py:57] epoch 1459, training loss: 7074.62, average training loss: 8122.33, base loss: 16114.41
[INFO 2017-06-29 01:18:15,124 main.py:57] epoch 1460, training loss: 6630.21, average training loss: 8121.35, base loss: 16112.38
[INFO 2017-06-29 01:18:18,154 main.py:57] epoch 1461, training loss: 8081.32, average training loss: 8120.61, base loss: 16112.94
[INFO 2017-06-29 01:18:21,154 main.py:57] epoch 1462, training loss: 9774.08, average training loss: 8122.16, base loss: 16113.96
[INFO 2017-06-29 01:18:24,221 main.py:57] epoch 1463, training loss: 8562.68, average training loss: 8122.43, base loss: 16115.75
[INFO 2017-06-29 01:18:27,255 main.py:57] epoch 1464, training loss: 8119.47, average training loss: 8122.73, base loss: 16115.82
[INFO 2017-06-29 01:18:30,298 main.py:57] epoch 1465, training loss: 7528.67, average training loss: 8121.08, base loss: 16114.66
[INFO 2017-06-29 01:18:33,356 main.py:57] epoch 1466, training loss: 8401.97, average training loss: 8121.56, base loss: 16116.28
[INFO 2017-06-29 01:18:36,410 main.py:57] epoch 1467, training loss: 7940.63, average training loss: 8121.77, base loss: 16116.70
[INFO 2017-06-29 01:18:39,480 main.py:57] epoch 1468, training loss: 7922.19, average training loss: 8121.34, base loss: 16116.11
[INFO 2017-06-29 01:18:42,469 main.py:57] epoch 1469, training loss: 6687.54, average training loss: 8120.19, base loss: 16113.64
[INFO 2017-06-29 01:18:45,556 main.py:57] epoch 1470, training loss: 8539.16, average training loss: 8118.30, base loss: 16113.38
[INFO 2017-06-29 01:18:48,638 main.py:57] epoch 1471, training loss: 7272.97, average training loss: 8116.24, base loss: 16111.97
[INFO 2017-06-29 01:18:51,674 main.py:57] epoch 1472, training loss: 6567.02, average training loss: 8114.61, base loss: 16109.09
[INFO 2017-06-29 01:18:54,707 main.py:57] epoch 1473, training loss: 7581.48, average training loss: 8114.01, base loss: 16107.91
[INFO 2017-06-29 01:18:57,767 main.py:57] epoch 1474, training loss: 7953.84, average training loss: 8113.51, base loss: 16107.40
[INFO 2017-06-29 01:19:00,845 main.py:57] epoch 1475, training loss: 7977.16, average training loss: 8113.09, base loss: 16107.67
[INFO 2017-06-29 01:19:03,899 main.py:57] epoch 1476, training loss: 7802.38, average training loss: 8113.38, base loss: 16107.32
[INFO 2017-06-29 01:19:06,984 main.py:57] epoch 1477, training loss: 8940.59, average training loss: 8112.59, base loss: 16109.02
[INFO 2017-06-29 01:19:10,013 main.py:57] epoch 1478, training loss: 7335.27, average training loss: 8111.85, base loss: 16107.77
[INFO 2017-06-29 01:19:13,064 main.py:57] epoch 1479, training loss: 7981.16, average training loss: 8111.82, base loss: 16106.84
[INFO 2017-06-29 01:19:16,167 main.py:57] epoch 1480, training loss: 7694.42, average training loss: 8111.66, base loss: 16106.14
[INFO 2017-06-29 01:19:19,228 main.py:57] epoch 1481, training loss: 6841.34, average training loss: 8109.02, base loss: 16103.61
[INFO 2017-06-29 01:19:22,422 main.py:57] epoch 1482, training loss: 7778.27, average training loss: 8108.32, base loss: 16103.51
[INFO 2017-06-29 01:19:25,502 main.py:57] epoch 1483, training loss: 7871.13, average training loss: 8107.59, base loss: 16102.25
[INFO 2017-06-29 01:19:28,534 main.py:57] epoch 1484, training loss: 6863.94, average training loss: 8106.57, base loss: 16100.68
[INFO 2017-06-29 01:19:31,531 main.py:57] epoch 1485, training loss: 8403.74, average training loss: 8106.21, base loss: 16101.51
[INFO 2017-06-29 01:19:34,576 main.py:57] epoch 1486, training loss: 7062.08, average training loss: 8104.40, base loss: 16099.41
[INFO 2017-06-29 01:19:37,589 main.py:57] epoch 1487, training loss: 7571.06, average training loss: 8103.77, base loss: 16099.19
[INFO 2017-06-29 01:19:40,618 main.py:57] epoch 1488, training loss: 7963.07, average training loss: 8103.51, base loss: 16099.38
[INFO 2017-06-29 01:19:43,683 main.py:57] epoch 1489, training loss: 8345.57, average training loss: 8102.14, base loss: 16100.15
[INFO 2017-06-29 01:19:46,708 main.py:57] epoch 1490, training loss: 8075.20, average training loss: 8102.10, base loss: 16100.88
[INFO 2017-06-29 01:19:49,778 main.py:57] epoch 1491, training loss: 7682.17, average training loss: 8101.06, base loss: 16099.80
[INFO 2017-06-29 01:19:52,917 main.py:57] epoch 1492, training loss: 7107.64, average training loss: 8099.88, base loss: 16096.90
[INFO 2017-06-29 01:19:55,939 main.py:57] epoch 1493, training loss: 9640.56, average training loss: 8102.02, base loss: 16097.43
[INFO 2017-06-29 01:19:58,961 main.py:57] epoch 1494, training loss: 7331.59, average training loss: 8100.17, base loss: 16096.87
[INFO 2017-06-29 01:20:02,047 main.py:57] epoch 1495, training loss: 8353.92, average training loss: 8100.36, base loss: 16098.04
[INFO 2017-06-29 01:20:05,096 main.py:57] epoch 1496, training loss: 7584.28, average training loss: 8097.41, base loss: 16096.51
[INFO 2017-06-29 01:20:08,101 main.py:57] epoch 1497, training loss: 8101.08, average training loss: 8096.96, base loss: 16096.00
[INFO 2017-06-29 01:20:11,165 main.py:57] epoch 1498, training loss: 8582.17, average training loss: 8096.41, base loss: 16096.21
[INFO 2017-06-29 01:20:14,206 main.py:57] epoch 1499, training loss: 9346.83, average training loss: 8097.51, base loss: 16098.05
[INFO 2017-06-29 01:20:14,207 main.py:59] epoch 1499, testing
[INFO 2017-06-29 01:20:26,966 main.py:104] average testing loss: 8671.94, base loss: 16860.54
[INFO 2017-06-29 01:20:26,966 main.py:105] improve_loss: 8188.60, improve_percent: 0.49
[INFO 2017-06-29 01:20:26,967 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 01:20:30,036 main.py:57] epoch 1500, training loss: 8466.35, average training loss: 8096.98, base loss: 16098.31
[INFO 2017-06-29 01:20:33,147 main.py:57] epoch 1501, training loss: 7438.08, average training loss: 8096.14, base loss: 16096.56
[INFO 2017-06-29 01:20:36,192 main.py:57] epoch 1502, training loss: 7274.83, average training loss: 8095.28, base loss: 16095.15
[INFO 2017-06-29 01:20:39,195 main.py:57] epoch 1503, training loss: 8478.96, average training loss: 8094.89, base loss: 16095.03
[INFO 2017-06-29 01:20:42,292 main.py:57] epoch 1504, training loss: 8021.62, average training loss: 8094.37, base loss: 16094.37
[INFO 2017-06-29 01:20:45,345 main.py:57] epoch 1505, training loss: 7397.26, average training loss: 8093.18, base loss: 16093.69
[INFO 2017-06-29 01:20:48,366 main.py:57] epoch 1506, training loss: 9189.20, average training loss: 8094.51, base loss: 16095.51
[INFO 2017-06-29 01:20:51,453 main.py:57] epoch 1507, training loss: 8865.49, average training loss: 8094.98, base loss: 16095.45
[INFO 2017-06-29 01:20:54,530 main.py:57] epoch 1508, training loss: 8193.11, average training loss: 8095.50, base loss: 16096.47
[INFO 2017-06-29 01:20:57,552 main.py:57] epoch 1509, training loss: 7365.42, average training loss: 8093.56, base loss: 16096.80
[INFO 2017-06-29 01:21:00,590 main.py:57] epoch 1510, training loss: 8025.77, average training loss: 8093.80, base loss: 16096.48
[INFO 2017-06-29 01:21:03,685 main.py:57] epoch 1511, training loss: 7588.99, average training loss: 8093.34, base loss: 16095.47
[INFO 2017-06-29 01:21:06,764 main.py:57] epoch 1512, training loss: 9280.99, average training loss: 8094.08, base loss: 16098.02
[INFO 2017-06-29 01:21:09,800 main.py:57] epoch 1513, training loss: 7121.67, average training loss: 8092.40, base loss: 16097.62
[INFO 2017-06-29 01:21:12,824 main.py:57] epoch 1514, training loss: 8288.64, average training loss: 8092.73, base loss: 16097.89
[INFO 2017-06-29 01:21:15,888 main.py:57] epoch 1515, training loss: 8216.38, average training loss: 8092.43, base loss: 16098.49
[INFO 2017-06-29 01:21:18,983 main.py:57] epoch 1516, training loss: 8109.62, average training loss: 8092.15, base loss: 16098.31
[INFO 2017-06-29 01:21:22,076 main.py:57] epoch 1517, training loss: 9100.55, average training loss: 8093.44, base loss: 16099.30
[INFO 2017-06-29 01:21:25,183 main.py:57] epoch 1518, training loss: 8024.74, average training loss: 8092.24, base loss: 16098.56
[INFO 2017-06-29 01:21:28,179 main.py:57] epoch 1519, training loss: 8110.57, average training loss: 8091.06, base loss: 16097.93
[INFO 2017-06-29 01:21:31,216 main.py:57] epoch 1520, training loss: 8589.72, average training loss: 8091.04, base loss: 16100.81
[INFO 2017-06-29 01:21:34,220 main.py:57] epoch 1521, training loss: 7436.93, average training loss: 8091.58, base loss: 16101.92
[INFO 2017-06-29 01:21:37,231 main.py:57] epoch 1522, training loss: 7204.50, average training loss: 8090.15, base loss: 16100.03
[INFO 2017-06-29 01:21:40,326 main.py:57] epoch 1523, training loss: 8348.31, average training loss: 8089.82, base loss: 16099.11
[INFO 2017-06-29 01:21:43,383 main.py:57] epoch 1524, training loss: 6883.44, average training loss: 8087.46, base loss: 16095.85
[INFO 2017-06-29 01:21:46,419 main.py:57] epoch 1525, training loss: 7856.39, average training loss: 8086.28, base loss: 16094.90
[INFO 2017-06-29 01:21:49,512 main.py:57] epoch 1526, training loss: 8329.99, average training loss: 8087.38, base loss: 16096.03
[INFO 2017-06-29 01:21:52,589 main.py:57] epoch 1527, training loss: 8239.39, average training loss: 8086.47, base loss: 16096.13
[INFO 2017-06-29 01:21:55,624 main.py:57] epoch 1528, training loss: 7119.78, average training loss: 8085.70, base loss: 16093.55
[INFO 2017-06-29 01:21:58,703 main.py:57] epoch 1529, training loss: 7487.12, average training loss: 8085.07, base loss: 16093.88
[INFO 2017-06-29 01:22:01,894 main.py:57] epoch 1530, training loss: 8712.16, average training loss: 8084.86, base loss: 16095.34
[INFO 2017-06-29 01:22:04,983 main.py:57] epoch 1531, training loss: 7679.82, average training loss: 8084.02, base loss: 16095.06
[INFO 2017-06-29 01:22:07,969 main.py:57] epoch 1532, training loss: 8244.79, average training loss: 8084.42, base loss: 16096.38
[INFO 2017-06-29 01:22:11,050 main.py:57] epoch 1533, training loss: 7736.80, average training loss: 8083.38, base loss: 16095.50
[INFO 2017-06-29 01:22:14,114 main.py:57] epoch 1534, training loss: 7503.81, average training loss: 8082.59, base loss: 16094.66
[INFO 2017-06-29 01:22:17,142 main.py:57] epoch 1535, training loss: 7516.28, average training loss: 8081.25, base loss: 16094.41
[INFO 2017-06-29 01:22:20,117 main.py:57] epoch 1536, training loss: 9030.53, average training loss: 8081.46, base loss: 16096.34
[INFO 2017-06-29 01:22:23,165 main.py:57] epoch 1537, training loss: 8699.53, average training loss: 8081.16, base loss: 16095.82
[INFO 2017-06-29 01:22:26,268 main.py:57] epoch 1538, training loss: 8107.96, average training loss: 8081.47, base loss: 16097.25
[INFO 2017-06-29 01:22:29,421 main.py:57] epoch 1539, training loss: 8651.91, average training loss: 8081.63, base loss: 16096.57
[INFO 2017-06-29 01:22:32,491 main.py:57] epoch 1540, training loss: 7697.58, average training loss: 8081.75, base loss: 16094.78
[INFO 2017-06-29 01:22:35,535 main.py:57] epoch 1541, training loss: 7347.85, average training loss: 8080.40, base loss: 16092.63
[INFO 2017-06-29 01:22:38,592 main.py:57] epoch 1542, training loss: 8592.14, average training loss: 8081.59, base loss: 16095.44
[INFO 2017-06-29 01:22:41,643 main.py:57] epoch 1543, training loss: 7831.43, average training loss: 8080.52, base loss: 16096.26
[INFO 2017-06-29 01:22:44,719 main.py:57] epoch 1544, training loss: 6609.24, average training loss: 8079.40, base loss: 16094.02
[INFO 2017-06-29 01:22:47,815 main.py:57] epoch 1545, training loss: 7066.06, average training loss: 8076.57, base loss: 16092.34
[INFO 2017-06-29 01:22:50,818 main.py:57] epoch 1546, training loss: 7823.96, average training loss: 8074.84, base loss: 16092.45
[INFO 2017-06-29 01:22:53,898 main.py:57] epoch 1547, training loss: 7368.48, average training loss: 8074.07, base loss: 16090.83
[INFO 2017-06-29 01:22:56,956 main.py:57] epoch 1548, training loss: 7025.89, average training loss: 8072.34, base loss: 16088.37
[INFO 2017-06-29 01:22:59,966 main.py:57] epoch 1549, training loss: 8219.24, average training loss: 8071.73, base loss: 16088.53
[INFO 2017-06-29 01:23:03,160 main.py:57] epoch 1550, training loss: 9583.83, average training loss: 8073.00, base loss: 16090.16
[INFO 2017-06-29 01:23:06,201 main.py:57] epoch 1551, training loss: 7773.73, average training loss: 8073.14, base loss: 16089.14
[INFO 2017-06-29 01:23:09,190 main.py:57] epoch 1552, training loss: 7796.08, average training loss: 8072.61, base loss: 16089.47
[INFO 2017-06-29 01:23:12,263 main.py:57] epoch 1553, training loss: 7463.68, average training loss: 8071.12, base loss: 16088.57
[INFO 2017-06-29 01:23:15,324 main.py:57] epoch 1554, training loss: 8058.48, average training loss: 8069.09, base loss: 16089.16
[INFO 2017-06-29 01:23:18,290 main.py:57] epoch 1555, training loss: 7113.51, average training loss: 8067.44, base loss: 16087.52
[INFO 2017-06-29 01:23:21,337 main.py:57] epoch 1556, training loss: 8294.15, average training loss: 8067.97, base loss: 16088.07
[INFO 2017-06-29 01:23:24,381 main.py:57] epoch 1557, training loss: 7939.30, average training loss: 8066.40, base loss: 16088.43
[INFO 2017-06-29 01:23:27,471 main.py:57] epoch 1558, training loss: 8393.04, average training loss: 8066.84, base loss: 16089.44
[INFO 2017-06-29 01:23:30,481 main.py:57] epoch 1559, training loss: 7159.30, average training loss: 8066.47, base loss: 16087.47
[INFO 2017-06-29 01:23:33,470 main.py:57] epoch 1560, training loss: 8987.72, average training loss: 8065.63, base loss: 16089.28
[INFO 2017-06-29 01:23:36,606 main.py:57] epoch 1561, training loss: 7509.50, average training loss: 8064.71, base loss: 16088.61
[INFO 2017-06-29 01:23:39,651 main.py:57] epoch 1562, training loss: 7204.67, average training loss: 8062.49, base loss: 16086.30
[INFO 2017-06-29 01:23:42,788 main.py:57] epoch 1563, training loss: 7087.21, average training loss: 8060.79, base loss: 16083.54
[INFO 2017-06-29 01:23:45,811 main.py:57] epoch 1564, training loss: 7711.74, average training loss: 8059.00, base loss: 16085.57
[INFO 2017-06-29 01:23:48,887 main.py:57] epoch 1565, training loss: 8852.52, average training loss: 8059.03, base loss: 16088.12
[INFO 2017-06-29 01:23:52,042 main.py:57] epoch 1566, training loss: 7771.88, average training loss: 8058.42, base loss: 16087.36
[INFO 2017-06-29 01:23:55,087 main.py:57] epoch 1567, training loss: 7442.09, average training loss: 8058.32, base loss: 16086.25
[INFO 2017-06-29 01:23:58,144 main.py:57] epoch 1568, training loss: 7481.66, average training loss: 8055.77, base loss: 16085.12
[INFO 2017-06-29 01:24:01,166 main.py:57] epoch 1569, training loss: 8002.79, average training loss: 8055.33, base loss: 16085.33
[INFO 2017-06-29 01:24:04,199 main.py:57] epoch 1570, training loss: 8366.41, average training loss: 8056.55, base loss: 16086.83
[INFO 2017-06-29 01:24:07,270 main.py:57] epoch 1571, training loss: 8881.30, average training loss: 8058.15, base loss: 16087.84
[INFO 2017-06-29 01:24:10,312 main.py:57] epoch 1572, training loss: 8285.71, average training loss: 8057.15, base loss: 16088.44
[INFO 2017-06-29 01:24:13,318 main.py:57] epoch 1573, training loss: 8431.86, average training loss: 8056.79, base loss: 16088.45
[INFO 2017-06-29 01:24:16,347 main.py:57] epoch 1574, training loss: 7936.45, average training loss: 8056.34, base loss: 16087.41
[INFO 2017-06-29 01:24:19,378 main.py:57] epoch 1575, training loss: 8037.71, average training loss: 8057.11, base loss: 16086.34
[INFO 2017-06-29 01:24:22,508 main.py:57] epoch 1576, training loss: 7408.11, average training loss: 8057.12, base loss: 16086.38
[INFO 2017-06-29 01:24:25,603 main.py:57] epoch 1577, training loss: 8047.70, average training loss: 8057.07, base loss: 16086.61
[INFO 2017-06-29 01:24:28,704 main.py:57] epoch 1578, training loss: 7665.49, average training loss: 8056.66, base loss: 16085.22
[INFO 2017-06-29 01:24:31,706 main.py:57] epoch 1579, training loss: 7610.88, average training loss: 8055.83, base loss: 16083.94
[INFO 2017-06-29 01:24:34,790 main.py:57] epoch 1580, training loss: 7417.96, average training loss: 8055.26, base loss: 16083.51
[INFO 2017-06-29 01:24:37,825 main.py:57] epoch 1581, training loss: 8329.46, average training loss: 8054.64, base loss: 16084.39
[INFO 2017-06-29 01:24:40,895 main.py:57] epoch 1582, training loss: 6829.60, average training loss: 8052.98, base loss: 16082.26
[INFO 2017-06-29 01:24:43,945 main.py:57] epoch 1583, training loss: 7396.68, average training loss: 8052.77, base loss: 16080.99
[INFO 2017-06-29 01:24:47,040 main.py:57] epoch 1584, training loss: 6717.18, average training loss: 8051.65, base loss: 16079.06
[INFO 2017-06-29 01:24:50,092 main.py:57] epoch 1585, training loss: 7519.63, average training loss: 8050.92, base loss: 16079.62
[INFO 2017-06-29 01:24:53,170 main.py:57] epoch 1586, training loss: 7633.60, average training loss: 8050.53, base loss: 16078.92
[INFO 2017-06-29 01:24:56,252 main.py:57] epoch 1587, training loss: 7143.83, average training loss: 8049.86, base loss: 16077.07
[INFO 2017-06-29 01:24:59,287 main.py:57] epoch 1588, training loss: 7234.39, average training loss: 8048.29, base loss: 16077.07
[INFO 2017-06-29 01:25:02,297 main.py:57] epoch 1589, training loss: 7317.99, average training loss: 8046.28, base loss: 16077.68
[INFO 2017-06-29 01:25:05,361 main.py:57] epoch 1590, training loss: 7263.80, average training loss: 8045.48, base loss: 16075.74
[INFO 2017-06-29 01:25:08,398 main.py:57] epoch 1591, training loss: 8176.70, average training loss: 8045.47, base loss: 16074.73
[INFO 2017-06-29 01:25:11,535 main.py:57] epoch 1592, training loss: 10050.74, average training loss: 8047.77, base loss: 16076.21
[INFO 2017-06-29 01:25:14,510 main.py:57] epoch 1593, training loss: 7459.19, average training loss: 8046.11, base loss: 16074.92
[INFO 2017-06-29 01:25:17,644 main.py:57] epoch 1594, training loss: 7868.77, average training loss: 8045.44, base loss: 16074.89
[INFO 2017-06-29 01:25:20,664 main.py:57] epoch 1595, training loss: 8263.47, average training loss: 8046.20, base loss: 16075.70
[INFO 2017-06-29 01:25:23,694 main.py:57] epoch 1596, training loss: 8012.29, average training loss: 8046.18, base loss: 16075.36
[INFO 2017-06-29 01:25:26,727 main.py:57] epoch 1597, training loss: 7788.42, average training loss: 8046.68, base loss: 16074.67
[INFO 2017-06-29 01:25:29,732 main.py:57] epoch 1598, training loss: 7294.65, average training loss: 8046.47, base loss: 16073.01
[INFO 2017-06-29 01:25:32,803 main.py:57] epoch 1599, training loss: 7672.94, average training loss: 8044.93, base loss: 16073.29
[INFO 2017-06-29 01:25:32,803 main.py:59] epoch 1599, testing
[INFO 2017-06-29 01:25:45,375 main.py:104] average testing loss: 8701.87, base loss: 16786.17
[INFO 2017-06-29 01:25:45,376 main.py:105] improve_loss: 8084.30, improve_percent: 0.48
[INFO 2017-06-29 01:25:45,377 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 01:25:48,479 main.py:57] epoch 1600, training loss: 7294.98, average training loss: 8043.72, base loss: 16073.01
[INFO 2017-06-29 01:25:51,469 main.py:57] epoch 1601, training loss: 7513.45, average training loss: 8042.33, base loss: 16071.86
[INFO 2017-06-29 01:25:54,517 main.py:57] epoch 1602, training loss: 6597.83, average training loss: 8040.24, base loss: 16071.06
[INFO 2017-06-29 01:25:57,605 main.py:57] epoch 1603, training loss: 6847.81, average training loss: 8039.22, base loss: 16070.19
[INFO 2017-06-29 01:26:00,706 main.py:57] epoch 1604, training loss: 8693.61, average training loss: 8038.56, base loss: 16071.25
[INFO 2017-06-29 01:26:03,764 main.py:57] epoch 1605, training loss: 7767.54, average training loss: 8038.31, base loss: 16072.29
[INFO 2017-06-29 01:26:06,823 main.py:57] epoch 1606, training loss: 6542.04, average training loss: 8035.97, base loss: 16069.97
[INFO 2017-06-29 01:26:09,864 main.py:57] epoch 1607, training loss: 9204.65, average training loss: 8036.64, base loss: 16072.34
[INFO 2017-06-29 01:26:12,912 main.py:57] epoch 1608, training loss: 7276.25, average training loss: 8035.39, base loss: 16069.64
[INFO 2017-06-29 01:26:15,985 main.py:57] epoch 1609, training loss: 8713.60, average training loss: 8035.80, base loss: 16068.53
[INFO 2017-06-29 01:26:19,076 main.py:57] epoch 1610, training loss: 7451.20, average training loss: 8034.52, base loss: 16067.89
[INFO 2017-06-29 01:26:22,132 main.py:57] epoch 1611, training loss: 8385.63, average training loss: 8035.57, base loss: 16067.60
[INFO 2017-06-29 01:26:25,238 main.py:57] epoch 1612, training loss: 7905.49, average training loss: 8035.37, base loss: 16067.92
[INFO 2017-06-29 01:26:28,274 main.py:57] epoch 1613, training loss: 7981.31, average training loss: 8035.57, base loss: 16067.26
[INFO 2017-06-29 01:26:31,288 main.py:57] epoch 1614, training loss: 8862.91, average training loss: 8035.74, base loss: 16067.43
[INFO 2017-06-29 01:26:34,377 main.py:57] epoch 1615, training loss: 7005.18, average training loss: 8035.04, base loss: 16064.76
[INFO 2017-06-29 01:26:37,466 main.py:57] epoch 1616, training loss: 7542.50, average training loss: 8034.13, base loss: 16064.95
[INFO 2017-06-29 01:26:40,516 main.py:57] epoch 1617, training loss: 7302.55, average training loss: 8032.22, base loss: 16064.67
[INFO 2017-06-29 01:26:43,609 main.py:57] epoch 1618, training loss: 9357.94, average training loss: 8032.82, base loss: 16068.15
[INFO 2017-06-29 01:26:46,616 main.py:57] epoch 1619, training loss: 7778.68, average training loss: 8032.12, base loss: 16068.94
[INFO 2017-06-29 01:26:49,710 main.py:57] epoch 1620, training loss: 8559.43, average training loss: 8032.65, base loss: 16068.94
[INFO 2017-06-29 01:26:52,725 main.py:57] epoch 1621, training loss: 8563.85, average training loss: 8033.46, base loss: 16069.06
[INFO 2017-06-29 01:26:55,798 main.py:57] epoch 1622, training loss: 8124.05, average training loss: 8034.49, base loss: 16069.79
[INFO 2017-06-29 01:26:58,862 main.py:57] epoch 1623, training loss: 9427.38, average training loss: 8035.49, base loss: 16070.87
[INFO 2017-06-29 01:27:01,885 main.py:57] epoch 1624, training loss: 7442.48, average training loss: 8034.88, base loss: 16068.58
[INFO 2017-06-29 01:27:04,955 main.py:57] epoch 1625, training loss: 7831.15, average training loss: 8033.51, base loss: 16067.38
[INFO 2017-06-29 01:27:08,002 main.py:57] epoch 1626, training loss: 8819.93, average training loss: 8034.62, base loss: 16068.11
[INFO 2017-06-29 01:27:11,018 main.py:57] epoch 1627, training loss: 8401.89, average training loss: 8034.71, base loss: 16069.32
[INFO 2017-06-29 01:27:14,017 main.py:57] epoch 1628, training loss: 7643.65, average training loss: 8035.19, base loss: 16069.12
[INFO 2017-06-29 01:27:17,155 main.py:57] epoch 1629, training loss: 7778.23, average training loss: 8033.72, base loss: 16068.00
[INFO 2017-06-29 01:27:20,235 main.py:57] epoch 1630, training loss: 7202.92, average training loss: 8032.39, base loss: 16065.32
[INFO 2017-06-29 01:27:23,301 main.py:57] epoch 1631, training loss: 7790.74, average training loss: 8031.77, base loss: 16065.36
[INFO 2017-06-29 01:27:26,355 main.py:57] epoch 1632, training loss: 7519.48, average training loss: 8031.43, base loss: 16064.24
[INFO 2017-06-29 01:27:29,415 main.py:57] epoch 1633, training loss: 7724.14, average training loss: 8031.17, base loss: 16064.04
[INFO 2017-06-29 01:27:32,550 main.py:57] epoch 1634, training loss: 8658.20, average training loss: 8032.07, base loss: 16067.05
[INFO 2017-06-29 01:27:35,626 main.py:57] epoch 1635, training loss: 7827.35, average training loss: 8030.50, base loss: 16068.51
[INFO 2017-06-29 01:27:38,732 main.py:57] epoch 1636, training loss: 8510.85, average training loss: 8031.01, base loss: 16070.27
[INFO 2017-06-29 01:27:41,733 main.py:57] epoch 1637, training loss: 8062.28, average training loss: 8030.10, base loss: 16070.61
[INFO 2017-06-29 01:27:44,766 main.py:57] epoch 1638, training loss: 8236.87, average training loss: 8030.49, base loss: 16069.93
[INFO 2017-06-29 01:27:47,809 main.py:57] epoch 1639, training loss: 6876.53, average training loss: 8029.23, base loss: 16067.50
[INFO 2017-06-29 01:27:50,769 main.py:57] epoch 1640, training loss: 7845.62, average training loss: 8027.14, base loss: 16068.50
[INFO 2017-06-29 01:27:53,887 main.py:57] epoch 1641, training loss: 6990.95, average training loss: 8026.03, base loss: 16066.82
[INFO 2017-06-29 01:27:56,973 main.py:57] epoch 1642, training loss: 7687.32, average training loss: 8024.38, base loss: 16067.68
[INFO 2017-06-29 01:28:00,037 main.py:57] epoch 1643, training loss: 8018.65, average training loss: 8023.97, base loss: 16069.03
[INFO 2017-06-29 01:28:03,094 main.py:57] epoch 1644, training loss: 7634.23, average training loss: 8024.10, base loss: 16068.39
[INFO 2017-06-29 01:28:06,154 main.py:57] epoch 1645, training loss: 7397.50, average training loss: 8023.58, base loss: 16067.25
[INFO 2017-06-29 01:28:09,227 main.py:57] epoch 1646, training loss: 8282.89, average training loss: 8024.42, base loss: 16067.24
[INFO 2017-06-29 01:28:12,312 main.py:57] epoch 1647, training loss: 8248.04, average training loss: 8024.33, base loss: 16068.39
[INFO 2017-06-29 01:28:15,356 main.py:57] epoch 1648, training loss: 8289.59, average training loss: 8024.25, base loss: 16069.59
[INFO 2017-06-29 01:28:18,452 main.py:57] epoch 1649, training loss: 8246.97, average training loss: 8025.14, base loss: 16070.28
[INFO 2017-06-29 01:28:21,425 main.py:57] epoch 1650, training loss: 8391.68, average training loss: 8025.34, base loss: 16072.09
[INFO 2017-06-29 01:28:24,512 main.py:57] epoch 1651, training loss: 8281.20, average training loss: 8024.43, base loss: 16073.32
[INFO 2017-06-29 01:28:27,538 main.py:57] epoch 1652, training loss: 8031.63, average training loss: 8024.98, base loss: 16074.06
[INFO 2017-06-29 01:28:30,606 main.py:57] epoch 1653, training loss: 9989.45, average training loss: 8027.23, base loss: 16078.25
[INFO 2017-06-29 01:28:33,647 main.py:57] epoch 1654, training loss: 7133.66, average training loss: 8026.22, base loss: 16076.73
[INFO 2017-06-29 01:28:36,737 main.py:57] epoch 1655, training loss: 8463.08, average training loss: 8025.93, base loss: 16077.56
[INFO 2017-06-29 01:28:39,842 main.py:57] epoch 1656, training loss: 8149.17, average training loss: 8026.06, base loss: 16078.48
[INFO 2017-06-29 01:28:42,933 main.py:57] epoch 1657, training loss: 8908.92, average training loss: 8026.52, base loss: 16080.94
[INFO 2017-06-29 01:28:46,011 main.py:57] epoch 1658, training loss: 8518.12, average training loss: 8026.94, base loss: 16081.90
[INFO 2017-06-29 01:28:49,100 main.py:57] epoch 1659, training loss: 7491.71, average training loss: 8025.97, base loss: 16081.91
[INFO 2017-06-29 01:28:52,163 main.py:57] epoch 1660, training loss: 7243.09, average training loss: 8024.54, base loss: 16081.57
[INFO 2017-06-29 01:28:55,265 main.py:57] epoch 1661, training loss: 7465.03, average training loss: 8024.49, base loss: 16080.81
[INFO 2017-06-29 01:28:58,357 main.py:57] epoch 1662, training loss: 8262.65, average training loss: 8025.23, base loss: 16082.45
[INFO 2017-06-29 01:29:01,495 main.py:57] epoch 1663, training loss: 7101.09, average training loss: 8024.27, base loss: 16081.11
[INFO 2017-06-29 01:29:04,548 main.py:57] epoch 1664, training loss: 7938.14, average training loss: 8023.68, base loss: 16080.24
[INFO 2017-06-29 01:29:07,573 main.py:57] epoch 1665, training loss: 7712.61, average training loss: 8022.18, base loss: 16079.45
[INFO 2017-06-29 01:29:10,568 main.py:57] epoch 1666, training loss: 9155.20, average training loss: 8022.76, base loss: 16080.99
[INFO 2017-06-29 01:29:13,606 main.py:57] epoch 1667, training loss: 8314.69, average training loss: 8023.43, base loss: 16082.20
[INFO 2017-06-29 01:29:16,653 main.py:57] epoch 1668, training loss: 7763.17, average training loss: 8022.92, base loss: 16082.40
[INFO 2017-06-29 01:29:19,695 main.py:57] epoch 1669, training loss: 7848.01, average training loss: 8022.53, base loss: 16083.10
[INFO 2017-06-29 01:29:22,686 main.py:57] epoch 1670, training loss: 7071.98, average training loss: 8021.13, base loss: 16082.37
[INFO 2017-06-29 01:29:25,777 main.py:57] epoch 1671, training loss: 7164.20, average training loss: 8020.90, base loss: 16080.86
[INFO 2017-06-29 01:29:28,843 main.py:57] epoch 1672, training loss: 7762.68, average training loss: 8020.09, base loss: 16080.61
[INFO 2017-06-29 01:29:31,916 main.py:57] epoch 1673, training loss: 7906.46, average training loss: 8018.43, base loss: 16081.78
[INFO 2017-06-29 01:29:35,040 main.py:57] epoch 1674, training loss: 7994.64, average training loss: 8018.75, base loss: 16081.71
[INFO 2017-06-29 01:29:38,076 main.py:57] epoch 1675, training loss: 6846.14, average training loss: 8017.56, base loss: 16081.33
[INFO 2017-06-29 01:29:41,201 main.py:57] epoch 1676, training loss: 7706.19, average training loss: 8016.46, base loss: 16080.27
[INFO 2017-06-29 01:29:44,262 main.py:57] epoch 1677, training loss: 7143.44, average training loss: 8014.70, base loss: 16078.11
[INFO 2017-06-29 01:29:47,295 main.py:57] epoch 1678, training loss: 7663.53, average training loss: 8015.11, base loss: 16078.84
[INFO 2017-06-29 01:29:50,399 main.py:57] epoch 1679, training loss: 7331.47, average training loss: 8014.54, base loss: 16077.91
[INFO 2017-06-29 01:29:53,471 main.py:57] epoch 1680, training loss: 7007.38, average training loss: 8013.55, base loss: 16076.67
[INFO 2017-06-29 01:29:56,563 main.py:57] epoch 1681, training loss: 7547.73, average training loss: 8013.24, base loss: 16076.05
[INFO 2017-06-29 01:29:59,602 main.py:57] epoch 1682, training loss: 6976.87, average training loss: 8010.78, base loss: 16074.13
[INFO 2017-06-29 01:30:02,645 main.py:57] epoch 1683, training loss: 7689.83, average training loss: 8009.43, base loss: 16074.23
[INFO 2017-06-29 01:30:05,720 main.py:57] epoch 1684, training loss: 7960.84, average training loss: 8009.67, base loss: 16074.92
[INFO 2017-06-29 01:30:08,787 main.py:57] epoch 1685, training loss: 9049.90, average training loss: 8009.46, base loss: 16076.71
[INFO 2017-06-29 01:30:11,833 main.py:57] epoch 1686, training loss: 6661.78, average training loss: 8009.16, base loss: 16074.02
[INFO 2017-06-29 01:30:14,836 main.py:57] epoch 1687, training loss: 7894.70, average training loss: 8008.63, base loss: 16073.03
[INFO 2017-06-29 01:30:17,884 main.py:57] epoch 1688, training loss: 7951.50, average training loss: 8009.59, base loss: 16073.34
[INFO 2017-06-29 01:30:20,938 main.py:57] epoch 1689, training loss: 6710.78, average training loss: 8005.76, base loss: 16071.29
[INFO 2017-06-29 01:30:24,002 main.py:57] epoch 1690, training loss: 7794.40, average training loss: 8004.11, base loss: 16071.40
[INFO 2017-06-29 01:30:27,079 main.py:57] epoch 1691, training loss: 7675.78, average training loss: 8002.85, base loss: 16072.68
[INFO 2017-06-29 01:30:30,154 main.py:57] epoch 1692, training loss: 7111.18, average training loss: 8002.27, base loss: 16072.21
[INFO 2017-06-29 01:30:33,243 main.py:57] epoch 1693, training loss: 8335.78, average training loss: 8002.91, base loss: 16073.77
[INFO 2017-06-29 01:30:36,278 main.py:57] epoch 1694, training loss: 8622.51, average training loss: 8002.83, base loss: 16076.27
[INFO 2017-06-29 01:30:39,310 main.py:57] epoch 1695, training loss: 7451.68, average training loss: 8001.79, base loss: 16076.05
[INFO 2017-06-29 01:30:42,346 main.py:57] epoch 1696, training loss: 8350.47, average training loss: 8002.39, base loss: 16077.77
[INFO 2017-06-29 01:30:45,435 main.py:57] epoch 1697, training loss: 7973.90, average training loss: 8003.68, base loss: 16078.36
[INFO 2017-06-29 01:30:48,475 main.py:57] epoch 1698, training loss: 7337.80, average training loss: 8001.75, base loss: 16076.74
[INFO 2017-06-29 01:30:51,509 main.py:57] epoch 1699, training loss: 8371.82, average training loss: 8003.26, base loss: 16076.35
[INFO 2017-06-29 01:30:51,510 main.py:59] epoch 1699, testing
[INFO 2017-06-29 01:31:04,244 main.py:104] average testing loss: 8731.44, base loss: 17201.81
[INFO 2017-06-29 01:31:04,244 main.py:105] improve_loss: 8470.37, improve_percent: 0.49
[INFO 2017-06-29 01:31:04,246 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 01:31:04,284 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 01:31:07,291 main.py:57] epoch 1700, training loss: 7728.32, average training loss: 8002.90, base loss: 16076.61
[INFO 2017-06-29 01:31:10,323 main.py:57] epoch 1701, training loss: 6575.31, average training loss: 8001.57, base loss: 16074.64
[INFO 2017-06-29 01:31:13,409 main.py:57] epoch 1702, training loss: 8410.47, average training loss: 8002.19, base loss: 16074.69
[INFO 2017-06-29 01:31:16,475 main.py:57] epoch 1703, training loss: 7747.38, average training loss: 8000.73, base loss: 16073.16
[INFO 2017-06-29 01:31:19,519 main.py:57] epoch 1704, training loss: 7728.72, average training loss: 8000.85, base loss: 16073.43
[INFO 2017-06-29 01:31:22,607 main.py:57] epoch 1705, training loss: 7305.99, average training loss: 8000.55, base loss: 16071.73
[INFO 2017-06-29 01:31:25,656 main.py:57] epoch 1706, training loss: 8367.50, average training loss: 8001.13, base loss: 16073.52
[INFO 2017-06-29 01:31:28,739 main.py:57] epoch 1707, training loss: 7295.06, average training loss: 8000.71, base loss: 16072.97
[INFO 2017-06-29 01:31:31,781 main.py:57] epoch 1708, training loss: 8178.15, average training loss: 8001.09, base loss: 16070.56
[INFO 2017-06-29 01:31:34,902 main.py:57] epoch 1709, training loss: 8825.33, average training loss: 8001.69, base loss: 16070.91
[INFO 2017-06-29 01:31:37,959 main.py:57] epoch 1710, training loss: 8068.03, average training loss: 8001.34, base loss: 16072.32
[INFO 2017-06-29 01:31:41,005 main.py:57] epoch 1711, training loss: 7439.93, average training loss: 8000.27, base loss: 16071.86
[INFO 2017-06-29 01:31:44,059 main.py:57] epoch 1712, training loss: 8015.37, average training loss: 8000.81, base loss: 16070.48
[INFO 2017-06-29 01:31:47,118 main.py:57] epoch 1713, training loss: 8743.36, average training loss: 8001.47, base loss: 16070.70
[INFO 2017-06-29 01:31:50,211 main.py:57] epoch 1714, training loss: 7983.40, average training loss: 8000.72, base loss: 16070.39
[INFO 2017-06-29 01:31:53,316 main.py:57] epoch 1715, training loss: 7937.20, average training loss: 7999.34, base loss: 16068.88
[INFO 2017-06-29 01:31:56,369 main.py:57] epoch 1716, training loss: 8401.69, average training loss: 7998.57, base loss: 16067.73
[INFO 2017-06-29 01:31:59,455 main.py:57] epoch 1717, training loss: 7214.00, average training loss: 7996.48, base loss: 16065.37
[INFO 2017-06-29 01:32:02,430 main.py:57] epoch 1718, training loss: 7667.72, average training loss: 7996.33, base loss: 16065.33
[INFO 2017-06-29 01:32:05,469 main.py:57] epoch 1719, training loss: 6955.10, average training loss: 7992.81, base loss: 16064.08
[INFO 2017-06-29 01:32:08,607 main.py:57] epoch 1720, training loss: 8581.87, average training loss: 7993.10, base loss: 16066.19
[INFO 2017-06-29 01:32:11,697 main.py:57] epoch 1721, training loss: 7211.86, average training loss: 7992.82, base loss: 16065.46
[INFO 2017-06-29 01:32:14,757 main.py:57] epoch 1722, training loss: 8218.35, average training loss: 7993.50, base loss: 16068.73
[INFO 2017-06-29 01:32:17,816 main.py:57] epoch 1723, training loss: 7935.69, average training loss: 7993.25, base loss: 16070.09
[INFO 2017-06-29 01:32:20,869 main.py:57] epoch 1724, training loss: 7837.42, average training loss: 7993.84, base loss: 16069.26
[INFO 2017-06-29 01:32:23,980 main.py:57] epoch 1725, training loss: 7368.73, average training loss: 7993.05, base loss: 16068.66
[INFO 2017-06-29 01:32:27,067 main.py:57] epoch 1726, training loss: 8162.61, average training loss: 7994.05, base loss: 16069.35
[INFO 2017-06-29 01:32:30,154 main.py:57] epoch 1727, training loss: 9101.62, average training loss: 7994.92, base loss: 16071.53
[INFO 2017-06-29 01:32:33,154 main.py:57] epoch 1728, training loss: 7772.70, average training loss: 7994.19, base loss: 16070.57
[INFO 2017-06-29 01:32:36,207 main.py:57] epoch 1729, training loss: 7543.71, average training loss: 7993.93, base loss: 16069.23
[INFO 2017-06-29 01:32:39,265 main.py:57] epoch 1730, training loss: 7993.01, average training loss: 7993.83, base loss: 16069.24
[INFO 2017-06-29 01:32:42,331 main.py:57] epoch 1731, training loss: 8134.66, average training loss: 7994.71, base loss: 16071.52
[INFO 2017-06-29 01:32:45,370 main.py:57] epoch 1732, training loss: 9647.33, average training loss: 7997.66, base loss: 16075.20
[INFO 2017-06-29 01:32:48,477 main.py:57] epoch 1733, training loss: 8672.35, average training loss: 7998.69, base loss: 16074.22
[INFO 2017-06-29 01:32:51,511 main.py:57] epoch 1734, training loss: 8516.68, average training loss: 7999.18, base loss: 16075.10
[INFO 2017-06-29 01:32:54,555 main.py:57] epoch 1735, training loss: 8804.29, average training loss: 7998.91, base loss: 16075.92
[INFO 2017-06-29 01:32:57,610 main.py:57] epoch 1736, training loss: 7341.16, average training loss: 7998.33, base loss: 16074.29
[INFO 2017-06-29 01:33:00,664 main.py:57] epoch 1737, training loss: 7987.62, average training loss: 7997.07, base loss: 16074.79
[INFO 2017-06-29 01:33:03,694 main.py:57] epoch 1738, training loss: 7912.62, average training loss: 7997.16, base loss: 16075.02
[INFO 2017-06-29 01:33:06,741 main.py:57] epoch 1739, training loss: 7660.09, average training loss: 7996.79, base loss: 16075.44
[INFO 2017-06-29 01:33:09,807 main.py:57] epoch 1740, training loss: 8513.12, average training loss: 7997.55, base loss: 16076.71
[INFO 2017-06-29 01:33:12,838 main.py:57] epoch 1741, training loss: 8555.91, average training loss: 7997.93, base loss: 16077.64
[INFO 2017-06-29 01:33:15,852 main.py:57] epoch 1742, training loss: 9229.72, average training loss: 7998.31, base loss: 16079.95
[INFO 2017-06-29 01:33:18,903 main.py:57] epoch 1743, training loss: 7890.75, average training loss: 7998.65, base loss: 16080.37
[INFO 2017-06-29 01:33:22,020 main.py:57] epoch 1744, training loss: 7577.30, average training loss: 7998.78, base loss: 16079.38
[INFO 2017-06-29 01:33:25,114 main.py:57] epoch 1745, training loss: 8381.61, average training loss: 7999.70, base loss: 16080.79
[INFO 2017-06-29 01:33:28,169 main.py:57] epoch 1746, training loss: 7963.10, average training loss: 7998.84, base loss: 16079.76
[INFO 2017-06-29 01:33:31,194 main.py:57] epoch 1747, training loss: 8533.99, average training loss: 7998.68, base loss: 16079.98
[INFO 2017-06-29 01:33:34,321 main.py:57] epoch 1748, training loss: 7065.90, average training loss: 7996.77, base loss: 16078.82
[INFO 2017-06-29 01:33:37,330 main.py:57] epoch 1749, training loss: 8126.23, average training loss: 7995.53, base loss: 16079.26
[INFO 2017-06-29 01:33:40,325 main.py:57] epoch 1750, training loss: 8335.64, average training loss: 7996.42, base loss: 16080.77
[INFO 2017-06-29 01:33:43,384 main.py:57] epoch 1751, training loss: 7264.90, average training loss: 7995.82, base loss: 16079.41
[INFO 2017-06-29 01:33:46,378 main.py:57] epoch 1752, training loss: 7341.36, average training loss: 7994.74, base loss: 16079.21
[INFO 2017-06-29 01:33:49,381 main.py:57] epoch 1753, training loss: 9202.34, average training loss: 7995.31, base loss: 16080.82
[INFO 2017-06-29 01:33:52,373 main.py:57] epoch 1754, training loss: 8318.87, average training loss: 7995.86, base loss: 16082.10
[INFO 2017-06-29 01:33:55,413 main.py:57] epoch 1755, training loss: 9430.84, average training loss: 7997.40, base loss: 16085.48
[INFO 2017-06-29 01:33:58,462 main.py:57] epoch 1756, training loss: 7754.55, average training loss: 7996.78, base loss: 16085.92
[INFO 2017-06-29 01:34:01,465 main.py:57] epoch 1757, training loss: 8746.16, average training loss: 7997.77, base loss: 16089.10
[INFO 2017-06-29 01:34:04,557 main.py:57] epoch 1758, training loss: 8162.83, average training loss: 7996.78, base loss: 16088.13
[INFO 2017-06-29 01:34:07,650 main.py:57] epoch 1759, training loss: 8096.62, average training loss: 7997.23, base loss: 16087.95
[INFO 2017-06-29 01:34:10,812 main.py:57] epoch 1760, training loss: 7654.26, average training loss: 7996.28, base loss: 16088.27
[INFO 2017-06-29 01:34:13,919 main.py:57] epoch 1761, training loss: 8008.59, average training loss: 7997.20, base loss: 16087.72
[INFO 2017-06-29 01:34:17,008 main.py:57] epoch 1762, training loss: 7695.81, average training loss: 7997.33, base loss: 16086.81
[INFO 2017-06-29 01:34:20,099 main.py:57] epoch 1763, training loss: 8648.77, average training loss: 7998.33, base loss: 16088.77
[INFO 2017-06-29 01:34:23,235 main.py:57] epoch 1764, training loss: 7303.68, average training loss: 7997.98, base loss: 16087.57
[INFO 2017-06-29 01:34:26,289 main.py:57] epoch 1765, training loss: 8733.15, average training loss: 7998.12, base loss: 16090.53
[INFO 2017-06-29 01:34:29,303 main.py:57] epoch 1766, training loss: 7527.91, average training loss: 7996.72, base loss: 16089.65
[INFO 2017-06-29 01:34:32,352 main.py:57] epoch 1767, training loss: 8494.04, average training loss: 7997.13, base loss: 16089.89
[INFO 2017-06-29 01:34:35,481 main.py:57] epoch 1768, training loss: 7571.82, average training loss: 7994.81, base loss: 16088.96
[INFO 2017-06-29 01:34:38,449 main.py:57] epoch 1769, training loss: 8273.18, average training loss: 7994.46, base loss: 16090.03
[INFO 2017-06-29 01:34:41,477 main.py:57] epoch 1770, training loss: 8643.05, average training loss: 7995.13, base loss: 16089.79
[INFO 2017-06-29 01:34:44,534 main.py:57] epoch 1771, training loss: 9168.53, average training loss: 7995.34, base loss: 16091.12
[INFO 2017-06-29 01:34:47,644 main.py:57] epoch 1772, training loss: 6864.37, average training loss: 7994.25, base loss: 16089.31
[INFO 2017-06-29 01:34:50,652 main.py:57] epoch 1773, training loss: 8273.17, average training loss: 7994.24, base loss: 16090.64
[INFO 2017-06-29 01:34:53,666 main.py:57] epoch 1774, training loss: 7701.08, average training loss: 7993.43, base loss: 16089.65
[INFO 2017-06-29 01:34:56,688 main.py:57] epoch 1775, training loss: 7263.14, average training loss: 7992.48, base loss: 16087.82
[INFO 2017-06-29 01:34:59,750 main.py:57] epoch 1776, training loss: 7731.14, average training loss: 7991.85, base loss: 16087.23
[INFO 2017-06-29 01:35:02,806 main.py:57] epoch 1777, training loss: 8143.66, average training loss: 7991.33, base loss: 16087.78
[INFO 2017-06-29 01:35:05,847 main.py:57] epoch 1778, training loss: 7495.60, average training loss: 7989.11, base loss: 16087.67
[INFO 2017-06-29 01:35:08,909 main.py:57] epoch 1779, training loss: 6980.22, average training loss: 7987.88, base loss: 16086.51
[INFO 2017-06-29 01:35:11,900 main.py:57] epoch 1780, training loss: 8447.28, average training loss: 7988.59, base loss: 16086.18
[INFO 2017-06-29 01:35:14,978 main.py:57] epoch 1781, training loss: 9798.29, average training loss: 7990.30, base loss: 16087.27
[INFO 2017-06-29 01:35:18,013 main.py:57] epoch 1782, training loss: 7649.33, average training loss: 7990.18, base loss: 16088.57
[INFO 2017-06-29 01:35:21,061 main.py:57] epoch 1783, training loss: 7096.36, average training loss: 7988.87, base loss: 16087.80
[INFO 2017-06-29 01:35:24,116 main.py:57] epoch 1784, training loss: 8315.85, average training loss: 7989.85, base loss: 16088.73
[INFO 2017-06-29 01:35:27,254 main.py:57] epoch 1785, training loss: 7279.96, average training loss: 7989.32, base loss: 16086.39
[INFO 2017-06-29 01:35:30,387 main.py:57] epoch 1786, training loss: 8865.01, average training loss: 7990.74, base loss: 16088.37
[INFO 2017-06-29 01:35:33,485 main.py:57] epoch 1787, training loss: 7594.82, average training loss: 7990.21, base loss: 16087.78
[INFO 2017-06-29 01:35:36,562 main.py:57] epoch 1788, training loss: 7652.99, average training loss: 7989.84, base loss: 16087.53
[INFO 2017-06-29 01:35:39,678 main.py:57] epoch 1789, training loss: 7701.54, average training loss: 7988.75, base loss: 16089.15
[INFO 2017-06-29 01:35:42,758 main.py:57] epoch 1790, training loss: 7537.56, average training loss: 7988.19, base loss: 16088.03
[INFO 2017-06-29 01:35:45,773 main.py:57] epoch 1791, training loss: 6941.68, average training loss: 7985.07, base loss: 16086.88
[INFO 2017-06-29 01:35:48,882 main.py:57] epoch 1792, training loss: 6904.56, average training loss: 7984.10, base loss: 16084.44
[INFO 2017-06-29 01:35:51,983 main.py:57] epoch 1793, training loss: 7253.43, average training loss: 7982.00, base loss: 16083.45
[INFO 2017-06-29 01:35:55,065 main.py:57] epoch 1794, training loss: 6991.55, average training loss: 7980.13, base loss: 16082.97
[INFO 2017-06-29 01:35:58,159 main.py:57] epoch 1795, training loss: 7549.70, average training loss: 7980.34, base loss: 16082.75
[INFO 2017-06-29 01:36:01,186 main.py:57] epoch 1796, training loss: 9451.47, average training loss: 7982.48, base loss: 16083.68
[INFO 2017-06-29 01:36:04,272 main.py:57] epoch 1797, training loss: 6911.67, average training loss: 7980.82, base loss: 16081.06
[INFO 2017-06-29 01:36:07,260 main.py:57] epoch 1798, training loss: 7174.72, average training loss: 7979.16, base loss: 16081.35
[INFO 2017-06-29 01:36:10,367 main.py:57] epoch 1799, training loss: 7600.23, average training loss: 7978.72, base loss: 16081.91
[INFO 2017-06-29 01:36:10,367 main.py:59] epoch 1799, testing
[INFO 2017-06-29 01:36:22,934 main.py:104] average testing loss: 8419.18, base loss: 16550.31
[INFO 2017-06-29 01:36:22,934 main.py:105] improve_loss: 8131.13, improve_percent: 0.49
[INFO 2017-06-29 01:36:22,936 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 01:36:25,953 main.py:57] epoch 1800, training loss: 7924.49, average training loss: 7979.18, base loss: 16082.50
[INFO 2017-06-29 01:36:28,963 main.py:57] epoch 1801, training loss: 7459.74, average training loss: 7978.01, base loss: 16081.64
[INFO 2017-06-29 01:36:32,078 main.py:57] epoch 1802, training loss: 6969.86, average training loss: 7976.54, base loss: 16080.25
[INFO 2017-06-29 01:36:35,112 main.py:57] epoch 1803, training loss: 7788.82, average training loss: 7976.13, base loss: 16079.69
[INFO 2017-06-29 01:36:38,115 main.py:57] epoch 1804, training loss: 7410.09, average training loss: 7975.37, base loss: 16079.52
[INFO 2017-06-29 01:36:41,216 main.py:57] epoch 1805, training loss: 7098.91, average training loss: 7974.36, base loss: 16080.12
[INFO 2017-06-29 01:36:44,280 main.py:57] epoch 1806, training loss: 7801.27, average training loss: 7973.09, base loss: 16078.93
[INFO 2017-06-29 01:36:47,293 main.py:57] epoch 1807, training loss: 7766.55, average training loss: 7972.84, base loss: 16078.73
[INFO 2017-06-29 01:36:50,385 main.py:57] epoch 1808, training loss: 8998.81, average training loss: 7974.18, base loss: 16080.79
[INFO 2017-06-29 01:36:53,431 main.py:57] epoch 1809, training loss: 7124.65, average training loss: 7971.64, base loss: 16079.60
[INFO 2017-06-29 01:36:56,481 main.py:57] epoch 1810, training loss: 7495.12, average training loss: 7971.47, base loss: 16079.82
[INFO 2017-06-29 01:36:59,565 main.py:57] epoch 1811, training loss: 7939.20, average training loss: 7969.76, base loss: 16080.61
[INFO 2017-06-29 01:37:02,578 main.py:57] epoch 1812, training loss: 7140.14, average training loss: 7969.29, base loss: 16078.91
[INFO 2017-06-29 01:37:05,602 main.py:57] epoch 1813, training loss: 7191.85, average training loss: 7968.10, base loss: 16078.61
[INFO 2017-06-29 01:37:08,633 main.py:57] epoch 1814, training loss: 8685.43, average training loss: 7968.62, base loss: 16079.66
[INFO 2017-06-29 01:37:11,710 main.py:57] epoch 1815, training loss: 8172.32, average training loss: 7969.00, base loss: 16079.13
[INFO 2017-06-29 01:37:14,813 main.py:57] epoch 1816, training loss: 6776.65, average training loss: 7967.72, base loss: 16078.53
[INFO 2017-06-29 01:37:17,827 main.py:57] epoch 1817, training loss: 7092.86, average training loss: 7964.03, base loss: 16078.84
[INFO 2017-06-29 01:37:20,924 main.py:57] epoch 1818, training loss: 7997.28, average training loss: 7965.30, base loss: 16079.31
[INFO 2017-06-29 01:37:23,963 main.py:57] epoch 1819, training loss: 7229.35, average training loss: 7963.90, base loss: 16077.69
[INFO 2017-06-29 01:37:26,978 main.py:57] epoch 1820, training loss: 8994.25, average training loss: 7964.85, base loss: 16079.77
[INFO 2017-06-29 01:37:30,016 main.py:57] epoch 1821, training loss: 8119.30, average training loss: 7964.56, base loss: 16080.25
[INFO 2017-06-29 01:37:33,052 main.py:57] epoch 1822, training loss: 6843.48, average training loss: 7964.55, base loss: 16078.85
[INFO 2017-06-29 01:37:36,112 main.py:57] epoch 1823, training loss: 8208.21, average training loss: 7963.97, base loss: 16079.62
[INFO 2017-06-29 01:37:39,292 main.py:57] epoch 1824, training loss: 7401.24, average training loss: 7963.82, base loss: 16080.02
[INFO 2017-06-29 01:37:42,303 main.py:57] epoch 1825, training loss: 6847.69, average training loss: 7962.66, base loss: 16079.41
[INFO 2017-06-29 01:37:45,293 main.py:57] epoch 1826, training loss: 7307.91, average training loss: 7962.53, base loss: 16078.66
[INFO 2017-06-29 01:37:48,347 main.py:57] epoch 1827, training loss: 7029.98, average training loss: 7962.22, base loss: 16077.61
[INFO 2017-06-29 01:37:51,374 main.py:57] epoch 1828, training loss: 7375.23, average training loss: 7962.58, base loss: 16076.37
[INFO 2017-06-29 01:37:54,383 main.py:57] epoch 1829, training loss: 8172.22, average training loss: 7962.34, base loss: 16077.34
[INFO 2017-06-29 01:37:57,420 main.py:57] epoch 1830, training loss: 7037.22, average training loss: 7961.57, base loss: 16074.36
[INFO 2017-06-29 01:38:00,480 main.py:57] epoch 1831, training loss: 7288.73, average training loss: 7960.59, base loss: 16073.93
[INFO 2017-06-29 01:38:03,514 main.py:57] epoch 1832, training loss: 7758.69, average training loss: 7960.79, base loss: 16073.85
[INFO 2017-06-29 01:38:06,561 main.py:57] epoch 1833, training loss: 8597.72, average training loss: 7959.93, base loss: 16075.48
[INFO 2017-06-29 01:38:09,588 main.py:57] epoch 1834, training loss: 7766.70, average training loss: 7959.06, base loss: 16075.74
[INFO 2017-06-29 01:38:12,688 main.py:57] epoch 1835, training loss: 8060.03, average training loss: 7959.23, base loss: 16077.28
[INFO 2017-06-29 01:38:15,804 main.py:57] epoch 1836, training loss: 8949.46, average training loss: 7960.77, base loss: 16079.26
[INFO 2017-06-29 01:38:18,820 main.py:57] epoch 1837, training loss: 7797.17, average training loss: 7960.33, base loss: 16080.50
[INFO 2017-06-29 01:38:21,858 main.py:57] epoch 1838, training loss: 8190.66, average training loss: 7961.52, base loss: 16080.33
[INFO 2017-06-29 01:38:24,882 main.py:57] epoch 1839, training loss: 6924.09, average training loss: 7961.07, base loss: 16080.12
[INFO 2017-06-29 01:38:27,966 main.py:57] epoch 1840, training loss: 7518.61, average training loss: 7960.40, base loss: 16079.26
[INFO 2017-06-29 01:38:30,943 main.py:57] epoch 1841, training loss: 7208.17, average training loss: 7958.62, base loss: 16077.74
[INFO 2017-06-29 01:38:33,992 main.py:57] epoch 1842, training loss: 7755.48, average training loss: 7958.47, base loss: 16078.29
[INFO 2017-06-29 01:38:37,080 main.py:57] epoch 1843, training loss: 9337.88, average training loss: 7960.95, base loss: 16081.52
[INFO 2017-06-29 01:38:40,170 main.py:57] epoch 1844, training loss: 8866.62, average training loss: 7960.47, base loss: 16083.07
[INFO 2017-06-29 01:38:43,237 main.py:57] epoch 1845, training loss: 7930.37, average training loss: 7961.57, base loss: 16083.60
[INFO 2017-06-29 01:38:46,270 main.py:57] epoch 1846, training loss: 8101.88, average training loss: 7959.61, base loss: 16083.24
[INFO 2017-06-29 01:38:49,292 main.py:57] epoch 1847, training loss: 7647.63, average training loss: 7959.47, base loss: 16081.50
[INFO 2017-06-29 01:38:52,366 main.py:57] epoch 1848, training loss: 7054.43, average training loss: 7958.71, base loss: 16080.41
[INFO 2017-06-29 01:38:55,406 main.py:57] epoch 1849, training loss: 7498.31, average training loss: 7958.70, base loss: 16079.26
[INFO 2017-06-29 01:38:58,471 main.py:57] epoch 1850, training loss: 6961.18, average training loss: 7956.46, base loss: 16078.25
[INFO 2017-06-29 01:39:01,461 main.py:57] epoch 1851, training loss: 6991.01, average training loss: 7954.23, base loss: 16075.87
[INFO 2017-06-29 01:39:04,573 main.py:57] epoch 1852, training loss: 8581.22, average training loss: 7954.35, base loss: 16077.21
[INFO 2017-06-29 01:39:07,647 main.py:57] epoch 1853, training loss: 7314.72, average training loss: 7954.48, base loss: 16076.73
[INFO 2017-06-29 01:39:10,675 main.py:57] epoch 1854, training loss: 9077.95, average training loss: 7956.14, base loss: 16078.33
[INFO 2017-06-29 01:39:13,774 main.py:57] epoch 1855, training loss: 8137.66, average training loss: 7955.38, base loss: 16078.23
[INFO 2017-06-29 01:39:16,894 main.py:57] epoch 1856, training loss: 8465.01, average training loss: 7956.31, base loss: 16079.03
[INFO 2017-06-29 01:39:20,010 main.py:57] epoch 1857, training loss: 8225.36, average training loss: 7956.15, base loss: 16080.23
[INFO 2017-06-29 01:39:23,028 main.py:57] epoch 1858, training loss: 7406.28, average training loss: 7955.39, base loss: 16079.50
[INFO 2017-06-29 01:39:26,130 main.py:57] epoch 1859, training loss: 6934.96, average training loss: 7953.51, base loss: 16078.20
[INFO 2017-06-29 01:39:29,201 main.py:57] epoch 1860, training loss: 7949.83, average training loss: 7953.84, base loss: 16077.90
[INFO 2017-06-29 01:39:32,229 main.py:57] epoch 1861, training loss: 7916.94, average training loss: 7953.44, base loss: 16077.97
[INFO 2017-06-29 01:39:35,311 main.py:57] epoch 1862, training loss: 8578.75, average training loss: 7953.88, base loss: 16078.71
[INFO 2017-06-29 01:39:38,360 main.py:57] epoch 1863, training loss: 7386.51, average training loss: 7953.76, base loss: 16078.87
[INFO 2017-06-29 01:39:41,411 main.py:57] epoch 1864, training loss: 6965.78, average training loss: 7953.39, base loss: 16076.95
[INFO 2017-06-29 01:39:44,433 main.py:57] epoch 1865, training loss: 7568.23, average training loss: 7951.25, base loss: 16075.55
[INFO 2017-06-29 01:39:47,475 main.py:57] epoch 1866, training loss: 6813.51, average training loss: 7950.45, base loss: 16073.10
[INFO 2017-06-29 01:39:50,548 main.py:57] epoch 1867, training loss: 8492.51, average training loss: 7951.15, base loss: 16073.66
[INFO 2017-06-29 01:39:53,627 main.py:57] epoch 1868, training loss: 8470.90, average training loss: 7951.46, base loss: 16074.83
[INFO 2017-06-29 01:39:56,627 main.py:57] epoch 1869, training loss: 7425.64, average training loss: 7950.87, base loss: 16074.04
[INFO 2017-06-29 01:39:59,688 main.py:57] epoch 1870, training loss: 7865.08, average training loss: 7950.94, base loss: 16073.75
[INFO 2017-06-29 01:40:02,738 main.py:57] epoch 1871, training loss: 8019.60, average training loss: 7951.40, base loss: 16073.85
[INFO 2017-06-29 01:40:05,776 main.py:57] epoch 1872, training loss: 6997.97, average training loss: 7950.53, base loss: 16073.32
[INFO 2017-06-29 01:40:08,874 main.py:57] epoch 1873, training loss: 7377.49, average training loss: 7949.07, base loss: 16073.45
[INFO 2017-06-29 01:40:11,929 main.py:57] epoch 1874, training loss: 8595.28, average training loss: 7949.33, base loss: 16075.00
[INFO 2017-06-29 01:40:14,921 main.py:57] epoch 1875, training loss: 7217.25, average training loss: 7948.18, base loss: 16074.07
[INFO 2017-06-29 01:40:17,968 main.py:57] epoch 1876, training loss: 8475.72, average training loss: 7949.47, base loss: 16075.61
[INFO 2017-06-29 01:40:21,067 main.py:57] epoch 1877, training loss: 9004.33, average training loss: 7950.33, base loss: 16078.67
[INFO 2017-06-29 01:40:24,127 main.py:57] epoch 1878, training loss: 7382.04, average training loss: 7951.17, base loss: 16076.07
[INFO 2017-06-29 01:40:27,147 main.py:57] epoch 1879, training loss: 7688.73, average training loss: 7951.35, base loss: 16073.52
[INFO 2017-06-29 01:40:30,296 main.py:57] epoch 1880, training loss: 8498.11, average training loss: 7950.38, base loss: 16073.56
[INFO 2017-06-29 01:40:33,377 main.py:57] epoch 1881, training loss: 7618.59, average training loss: 7949.88, base loss: 16073.09
[INFO 2017-06-29 01:40:36,576 main.py:57] epoch 1882, training loss: 7739.90, average training loss: 7948.61, base loss: 16072.86
[INFO 2017-06-29 01:40:39,548 main.py:57] epoch 1883, training loss: 7184.55, average training loss: 7947.83, base loss: 16071.18
[INFO 2017-06-29 01:40:42,646 main.py:57] epoch 1884, training loss: 7619.51, average training loss: 7947.62, base loss: 16070.86
[INFO 2017-06-29 01:40:45,688 main.py:57] epoch 1885, training loss: 7499.80, average training loss: 7946.63, base loss: 16071.04
[INFO 2017-06-29 01:40:48,751 main.py:57] epoch 1886, training loss: 7994.65, average training loss: 7947.04, base loss: 16072.46
[INFO 2017-06-29 01:40:51,825 main.py:57] epoch 1887, training loss: 7829.94, average training loss: 7946.82, base loss: 16073.41
[INFO 2017-06-29 01:40:54,882 main.py:57] epoch 1888, training loss: 7148.21, average training loss: 7944.37, base loss: 16071.73
[INFO 2017-06-29 01:40:57,952 main.py:57] epoch 1889, training loss: 7845.24, average training loss: 7943.80, base loss: 16071.79
[INFO 2017-06-29 01:41:01,068 main.py:57] epoch 1890, training loss: 8536.55, average training loss: 7945.00, base loss: 16071.38
[INFO 2017-06-29 01:41:04,073 main.py:57] epoch 1891, training loss: 7615.38, average training loss: 7944.44, base loss: 16070.57
[INFO 2017-06-29 01:41:07,188 main.py:57] epoch 1892, training loss: 7178.90, average training loss: 7944.07, base loss: 16071.20
[INFO 2017-06-29 01:41:10,329 main.py:57] epoch 1893, training loss: 7415.74, average training loss: 7944.18, base loss: 16069.79
[INFO 2017-06-29 01:41:13,362 main.py:57] epoch 1894, training loss: 6457.68, average training loss: 7942.43, base loss: 16068.63
[INFO 2017-06-29 01:41:16,427 main.py:57] epoch 1895, training loss: 7934.26, average training loss: 7942.34, base loss: 16068.69
[INFO 2017-06-29 01:41:19,446 main.py:57] epoch 1896, training loss: 8190.93, average training loss: 7942.99, base loss: 16068.04
[INFO 2017-06-29 01:41:22,589 main.py:57] epoch 1897, training loss: 7676.69, average training loss: 7942.89, base loss: 16067.57
[INFO 2017-06-29 01:41:25,683 main.py:57] epoch 1898, training loss: 7892.57, average training loss: 7942.86, base loss: 16068.91
[INFO 2017-06-29 01:41:28,795 main.py:57] epoch 1899, training loss: 8425.90, average training loss: 7942.86, base loss: 16070.86
[INFO 2017-06-29 01:41:28,795 main.py:59] epoch 1899, testing
[INFO 2017-06-29 01:41:41,369 main.py:104] average testing loss: 8112.27, base loss: 16299.17
[INFO 2017-06-29 01:41:41,369 main.py:105] improve_loss: 8186.89, improve_percent: 0.50
[INFO 2017-06-29 01:41:41,370 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 01:41:41,408 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 01:41:44,445 main.py:57] epoch 1900, training loss: 7434.99, average training loss: 7940.93, base loss: 16070.87
[INFO 2017-06-29 01:41:47,509 main.py:57] epoch 1901, training loss: 8869.47, average training loss: 7942.34, base loss: 16073.29
[INFO 2017-06-29 01:41:50,606 main.py:57] epoch 1902, training loss: 8456.68, average training loss: 7942.28, base loss: 16072.99
[INFO 2017-06-29 01:41:53,657 main.py:57] epoch 1903, training loss: 8015.04, average training loss: 7941.50, base loss: 16073.96
[INFO 2017-06-29 01:41:56,667 main.py:57] epoch 1904, training loss: 7480.45, average training loss: 7940.51, base loss: 16073.25
[INFO 2017-06-29 01:41:59,812 main.py:57] epoch 1905, training loss: 7090.64, average training loss: 7938.67, base loss: 16073.03
[INFO 2017-06-29 01:42:02,826 main.py:57] epoch 1906, training loss: 8568.61, average training loss: 7940.51, base loss: 16073.40
[INFO 2017-06-29 01:42:05,855 main.py:57] epoch 1907, training loss: 10084.84, average training loss: 7942.86, base loss: 16074.96
[INFO 2017-06-29 01:42:08,959 main.py:57] epoch 1908, training loss: 7575.77, average training loss: 7941.44, base loss: 16076.09
[INFO 2017-06-29 01:42:11,972 main.py:57] epoch 1909, training loss: 8673.80, average training loss: 7941.53, base loss: 16078.89
[INFO 2017-06-29 01:42:15,062 main.py:57] epoch 1910, training loss: 7050.75, average training loss: 7940.28, base loss: 16078.82
[INFO 2017-06-29 01:42:18,190 main.py:57] epoch 1911, training loss: 8251.30, average training loss: 7940.54, base loss: 16080.02
[INFO 2017-06-29 01:42:21,212 main.py:57] epoch 1912, training loss: 7978.46, average training loss: 7940.10, base loss: 16080.47
[INFO 2017-06-29 01:42:24,227 main.py:57] epoch 1913, training loss: 8251.72, average training loss: 7940.42, base loss: 16081.84
[INFO 2017-06-29 01:42:27,259 main.py:57] epoch 1914, training loss: 6899.68, average training loss: 7938.76, base loss: 16081.09
[INFO 2017-06-29 01:42:30,300 main.py:57] epoch 1915, training loss: 8211.07, average training loss: 7937.16, base loss: 16082.19
[INFO 2017-06-29 01:42:33,346 main.py:57] epoch 1916, training loss: 7719.29, average training loss: 7936.88, base loss: 16081.48
[INFO 2017-06-29 01:42:36,386 main.py:57] epoch 1917, training loss: 7893.69, average training loss: 7935.94, base loss: 16081.01
[INFO 2017-06-29 01:42:39,412 main.py:57] epoch 1918, training loss: 7949.31, average training loss: 7936.57, base loss: 16081.58
[INFO 2017-06-29 01:42:42,413 main.py:57] epoch 1919, training loss: 7804.38, average training loss: 7935.22, base loss: 16081.24
[INFO 2017-06-29 01:42:45,460 main.py:57] epoch 1920, training loss: 8453.14, average training loss: 7936.07, base loss: 16080.68
[INFO 2017-06-29 01:42:48,478 main.py:57] epoch 1921, training loss: 8004.79, average training loss: 7935.61, base loss: 16081.00
[INFO 2017-06-29 01:42:51,485 main.py:57] epoch 1922, training loss: 10533.82, average training loss: 7938.68, base loss: 16083.31
[INFO 2017-06-29 01:42:54,576 main.py:57] epoch 1923, training loss: 7659.67, average training loss: 7938.48, base loss: 16081.78
[INFO 2017-06-29 01:42:57,602 main.py:57] epoch 1924, training loss: 6637.93, average training loss: 7937.62, base loss: 16080.64
[INFO 2017-06-29 01:43:00,696 main.py:57] epoch 1925, training loss: 7483.31, average training loss: 7936.97, base loss: 16081.05
[INFO 2017-06-29 01:43:03,754 main.py:57] epoch 1926, training loss: 8020.04, average training loss: 7936.47, base loss: 16082.42
[INFO 2017-06-29 01:43:06,826 main.py:57] epoch 1927, training loss: 7920.95, average training loss: 7936.68, base loss: 16082.50
[INFO 2017-06-29 01:43:09,916 main.py:57] epoch 1928, training loss: 7556.15, average training loss: 7936.01, base loss: 16081.79
[INFO 2017-06-29 01:43:12,910 main.py:57] epoch 1929, training loss: 8672.58, average training loss: 7936.59, base loss: 16082.90
[INFO 2017-06-29 01:43:15,931 main.py:57] epoch 1930, training loss: 8375.11, average training loss: 7937.76, base loss: 16082.18
[INFO 2017-06-29 01:43:18,922 main.py:57] epoch 1931, training loss: 7441.60, average training loss: 7936.95, base loss: 16081.41
[INFO 2017-06-29 01:43:21,991 main.py:57] epoch 1932, training loss: 7332.06, average training loss: 7936.32, base loss: 16080.32
[INFO 2017-06-29 01:43:25,049 main.py:57] epoch 1933, training loss: 6951.31, average training loss: 7934.92, base loss: 16078.32
[INFO 2017-06-29 01:43:28,114 main.py:57] epoch 1934, training loss: 7616.72, average training loss: 7935.01, base loss: 16076.27
[INFO 2017-06-29 01:43:31,137 main.py:57] epoch 1935, training loss: 8250.13, average training loss: 7934.77, base loss: 16075.70
[INFO 2017-06-29 01:43:34,189 main.py:57] epoch 1936, training loss: 7582.39, average training loss: 7934.27, base loss: 16075.01
[INFO 2017-06-29 01:43:37,233 main.py:57] epoch 1937, training loss: 7590.31, average training loss: 7933.95, base loss: 16076.69
[INFO 2017-06-29 01:43:40,328 main.py:57] epoch 1938, training loss: 8757.24, average training loss: 7934.90, base loss: 16076.45
[INFO 2017-06-29 01:43:43,380 main.py:57] epoch 1939, training loss: 8331.07, average training loss: 7934.83, base loss: 16075.23
[INFO 2017-06-29 01:43:46,407 main.py:57] epoch 1940, training loss: 9624.64, average training loss: 7936.88, base loss: 16076.67
[INFO 2017-06-29 01:43:49,410 main.py:57] epoch 1941, training loss: 7872.50, average training loss: 7936.01, base loss: 16076.38
[INFO 2017-06-29 01:43:52,430 main.py:57] epoch 1942, training loss: 7326.06, average training loss: 7934.90, base loss: 16074.81
[INFO 2017-06-29 01:43:55,472 main.py:57] epoch 1943, training loss: 8413.25, average training loss: 7935.60, base loss: 16075.96
[INFO 2017-06-29 01:43:58,518 main.py:57] epoch 1944, training loss: 8713.67, average training loss: 7936.05, base loss: 16077.46
[INFO 2017-06-29 01:44:01,594 main.py:57] epoch 1945, training loss: 7298.31, average training loss: 7933.86, base loss: 16078.22
[INFO 2017-06-29 01:44:04,605 main.py:57] epoch 1946, training loss: 7497.69, average training loss: 7933.27, base loss: 16078.20
[INFO 2017-06-29 01:44:07,619 main.py:57] epoch 1947, training loss: 8322.58, average training loss: 7933.31, base loss: 16080.18
[INFO 2017-06-29 01:44:10,630 main.py:57] epoch 1948, training loss: 8099.93, average training loss: 7933.33, base loss: 16080.60
[INFO 2017-06-29 01:44:13,689 main.py:57] epoch 1949, training loss: 8043.01, average training loss: 7934.23, base loss: 16082.15
[INFO 2017-06-29 01:44:16,733 main.py:57] epoch 1950, training loss: 8654.50, average training loss: 7934.42, base loss: 16083.39
[INFO 2017-06-29 01:44:19,781 main.py:57] epoch 1951, training loss: 8020.17, average training loss: 7934.05, base loss: 16084.92
[INFO 2017-06-29 01:44:22,790 main.py:57] epoch 1952, training loss: 7773.74, average training loss: 7934.45, base loss: 16086.87
[INFO 2017-06-29 01:44:25,794 main.py:57] epoch 1953, training loss: 7275.53, average training loss: 7934.98, base loss: 16085.95
[INFO 2017-06-29 01:44:28,975 main.py:57] epoch 1954, training loss: 7111.39, average training loss: 7934.20, base loss: 16085.23
[INFO 2017-06-29 01:44:31,975 main.py:57] epoch 1955, training loss: 6941.28, average training loss: 7932.99, base loss: 16083.79
[INFO 2017-06-29 01:44:35,012 main.py:57] epoch 1956, training loss: 10298.01, average training loss: 7935.53, base loss: 16085.69
[INFO 2017-06-29 01:44:38,202 main.py:57] epoch 1957, training loss: 7941.62, average training loss: 7935.18, base loss: 16085.14
[INFO 2017-06-29 01:44:41,204 main.py:57] epoch 1958, training loss: 8717.83, average training loss: 7934.09, base loss: 16085.69
[INFO 2017-06-29 01:44:44,264 main.py:57] epoch 1959, training loss: 7309.41, average training loss: 7933.24, base loss: 16084.72
[INFO 2017-06-29 01:44:47,319 main.py:57] epoch 1960, training loss: 7322.80, average training loss: 7933.23, base loss: 16085.18
[INFO 2017-06-29 01:44:50,336 main.py:57] epoch 1961, training loss: 7721.16, average training loss: 7932.95, base loss: 16086.33
[INFO 2017-06-29 01:44:53,364 main.py:57] epoch 1962, training loss: 8584.99, average training loss: 7933.35, base loss: 16089.07
[INFO 2017-06-29 01:44:56,415 main.py:57] epoch 1963, training loss: 7802.57, average training loss: 7934.07, base loss: 16088.56
[INFO 2017-06-29 01:44:59,472 main.py:57] epoch 1964, training loss: 8278.77, average training loss: 7934.62, base loss: 16090.09
[INFO 2017-06-29 01:45:02,526 main.py:57] epoch 1965, training loss: 8518.97, average training loss: 7934.17, base loss: 16092.31
[INFO 2017-06-29 01:45:05,549 main.py:57] epoch 1966, training loss: 7124.24, average training loss: 7932.85, base loss: 16091.36
[INFO 2017-06-29 01:45:08,545 main.py:57] epoch 1967, training loss: 7956.74, average training loss: 7933.62, base loss: 16092.27
[INFO 2017-06-29 01:45:11,621 main.py:57] epoch 1968, training loss: 7452.00, average training loss: 7932.92, base loss: 16092.58
[INFO 2017-06-29 01:45:14,706 main.py:57] epoch 1969, training loss: 7122.08, average training loss: 7931.87, base loss: 16091.57
[INFO 2017-06-29 01:45:17,782 main.py:57] epoch 1970, training loss: 7845.65, average training loss: 7930.85, base loss: 16091.77
[INFO 2017-06-29 01:45:20,758 main.py:57] epoch 1971, training loss: 7756.89, average training loss: 7931.15, base loss: 16091.38
[INFO 2017-06-29 01:45:23,891 main.py:57] epoch 1972, training loss: 7968.73, average training loss: 7931.37, base loss: 16091.23
[INFO 2017-06-29 01:45:26,955 main.py:57] epoch 1973, training loss: 8061.83, average training loss: 7931.00, base loss: 16090.99
[INFO 2017-06-29 01:45:29,945 main.py:57] epoch 1974, training loss: 8047.24, average training loss: 7930.89, base loss: 16089.18
[INFO 2017-06-29 01:45:33,030 main.py:57] epoch 1975, training loss: 7578.59, average training loss: 7931.45, base loss: 16087.98
[INFO 2017-06-29 01:45:36,089 main.py:57] epoch 1976, training loss: 9176.20, average training loss: 7931.21, base loss: 16091.37
[INFO 2017-06-29 01:45:39,187 main.py:57] epoch 1977, training loss: 8014.53, average training loss: 7931.53, base loss: 16091.29
[INFO 2017-06-29 01:45:42,274 main.py:57] epoch 1978, training loss: 7663.45, average training loss: 7930.90, base loss: 16091.28
[INFO 2017-06-29 01:45:45,324 main.py:57] epoch 1979, training loss: 8516.09, average training loss: 7930.46, base loss: 16091.44
[INFO 2017-06-29 01:45:48,320 main.py:57] epoch 1980, training loss: 7553.30, average training loss: 7928.78, base loss: 16091.25
[INFO 2017-06-29 01:45:51,377 main.py:57] epoch 1981, training loss: 6886.80, average training loss: 7928.17, base loss: 16090.23
[INFO 2017-06-29 01:45:54,496 main.py:57] epoch 1982, training loss: 6887.25, average training loss: 7927.61, base loss: 16088.89
[INFO 2017-06-29 01:45:57,498 main.py:57] epoch 1983, training loss: 8158.23, average training loss: 7926.63, base loss: 16090.04
[INFO 2017-06-29 01:46:00,500 main.py:57] epoch 1984, training loss: 9490.64, average training loss: 7927.05, base loss: 16091.88
[INFO 2017-06-29 01:46:03,547 main.py:57] epoch 1985, training loss: 7252.81, average training loss: 7926.64, base loss: 16090.99
[INFO 2017-06-29 01:46:06,596 main.py:57] epoch 1986, training loss: 6434.58, average training loss: 7924.96, base loss: 16089.22
[INFO 2017-06-29 01:46:09,696 main.py:57] epoch 1987, training loss: 7236.25, average training loss: 7923.10, base loss: 16089.45
[INFO 2017-06-29 01:46:12,823 main.py:57] epoch 1988, training loss: 7871.34, average training loss: 7922.30, base loss: 16089.92
[INFO 2017-06-29 01:46:15,943 main.py:57] epoch 1989, training loss: 7512.82, average training loss: 7921.56, base loss: 16090.60
[INFO 2017-06-29 01:46:18,992 main.py:57] epoch 1990, training loss: 7076.37, average training loss: 7921.43, base loss: 16090.32
[INFO 2017-06-29 01:46:22,060 main.py:57] epoch 1991, training loss: 7971.88, average training loss: 7920.75, base loss: 16091.12
[INFO 2017-06-29 01:46:25,114 main.py:57] epoch 1992, training loss: 8940.33, average training loss: 7921.58, base loss: 16093.08
[INFO 2017-06-29 01:46:28,179 main.py:57] epoch 1993, training loss: 7743.95, average training loss: 7922.17, base loss: 16092.99
[INFO 2017-06-29 01:46:31,206 main.py:57] epoch 1994, training loss: 8729.50, average training loss: 7923.36, base loss: 16092.66
[INFO 2017-06-29 01:46:34,236 main.py:57] epoch 1995, training loss: 8327.59, average training loss: 7923.62, base loss: 16092.72
[INFO 2017-06-29 01:46:37,290 main.py:57] epoch 1996, training loss: 7141.47, average training loss: 7922.38, base loss: 16091.16
[INFO 2017-06-29 01:46:40,339 main.py:57] epoch 1997, training loss: 9131.01, average training loss: 7923.31, base loss: 16092.89
[INFO 2017-06-29 01:46:43,398 main.py:57] epoch 1998, training loss: 7357.39, average training loss: 7920.94, base loss: 16092.61
[INFO 2017-06-29 01:46:46,489 main.py:57] epoch 1999, training loss: 7404.55, average training loss: 7919.92, base loss: 16091.42
[INFO 2017-06-29 01:46:46,489 main.py:59] epoch 1999, testing
[INFO 2017-06-29 01:46:59,109 main.py:104] average testing loss: 8820.17, base loss: 17435.70
[INFO 2017-06-29 01:46:59,109 main.py:105] improve_loss: 8615.53, improve_percent: 0.49
[INFO 2017-06-29 01:46:59,110 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 01:47:02,143 main.py:57] epoch 2000, training loss: 9406.62, average training loss: 7920.74, base loss: 16094.57
[INFO 2017-06-29 01:47:05,258 main.py:57] epoch 2001, training loss: 7438.67, average training loss: 7920.56, base loss: 16094.13
[INFO 2017-06-29 01:47:08,340 main.py:57] epoch 2002, training loss: 8394.87, average training loss: 7920.91, base loss: 16094.83
[INFO 2017-06-29 01:47:11,443 main.py:57] epoch 2003, training loss: 8352.86, average training loss: 7921.75, base loss: 16096.49
[INFO 2017-06-29 01:47:14,588 main.py:57] epoch 2004, training loss: 9102.76, average training loss: 7921.64, base loss: 16097.46
[INFO 2017-06-29 01:47:17,627 main.py:57] epoch 2005, training loss: 7771.14, average training loss: 7922.41, base loss: 16096.55
[INFO 2017-06-29 01:47:20,700 main.py:57] epoch 2006, training loss: 7115.39, average training loss: 7921.11, base loss: 16094.63
[INFO 2017-06-29 01:47:23,742 main.py:57] epoch 2007, training loss: 7461.28, average training loss: 7920.85, base loss: 16093.68
[INFO 2017-06-29 01:47:26,799 main.py:57] epoch 2008, training loss: 7198.30, average training loss: 7920.42, base loss: 16093.68
[INFO 2017-06-29 01:47:29,865 main.py:57] epoch 2009, training loss: 8011.98, average training loss: 7920.75, base loss: 16093.27
[INFO 2017-06-29 01:47:32,918 main.py:57] epoch 2010, training loss: 7985.11, average training loss: 7921.64, base loss: 16093.28
[INFO 2017-06-29 01:47:35,941 main.py:57] epoch 2011, training loss: 7294.82, average training loss: 7920.72, base loss: 16092.57
[INFO 2017-06-29 01:47:38,982 main.py:57] epoch 2012, training loss: 8628.64, average training loss: 7921.83, base loss: 16092.22
[INFO 2017-06-29 01:47:42,014 main.py:57] epoch 2013, training loss: 7303.24, average training loss: 7920.55, base loss: 16090.56
[INFO 2017-06-29 01:47:45,040 main.py:57] epoch 2014, training loss: 7889.54, average training loss: 7919.23, base loss: 16091.21
[INFO 2017-06-29 01:47:48,029 main.py:57] epoch 2015, training loss: 7562.46, average training loss: 7919.02, base loss: 16091.27
[INFO 2017-06-29 01:47:51,033 main.py:57] epoch 2016, training loss: 8247.09, average training loss: 7918.56, base loss: 16091.82
[INFO 2017-06-29 01:47:54,117 main.py:57] epoch 2017, training loss: 7901.42, average training loss: 7917.03, base loss: 16091.67
[INFO 2017-06-29 01:47:57,115 main.py:57] epoch 2018, training loss: 8574.96, average training loss: 7917.55, base loss: 16093.08
[INFO 2017-06-29 01:48:00,113 main.py:57] epoch 2019, training loss: 7413.54, average training loss: 7915.64, base loss: 16092.63
[INFO 2017-06-29 01:48:03,215 main.py:57] epoch 2020, training loss: 8219.34, average training loss: 7916.81, base loss: 16093.04
[INFO 2017-06-29 01:48:06,257 main.py:57] epoch 2021, training loss: 7990.63, average training loss: 7915.83, base loss: 16092.87
[INFO 2017-06-29 01:48:09,316 main.py:57] epoch 2022, training loss: 6950.98, average training loss: 7915.16, base loss: 16093.08
[INFO 2017-06-29 01:48:12,353 main.py:57] epoch 2023, training loss: 7258.91, average training loss: 7913.54, base loss: 16092.51
[INFO 2017-06-29 01:48:15,380 main.py:57] epoch 2024, training loss: 7531.72, average training loss: 7912.93, base loss: 16092.22
[INFO 2017-06-29 01:48:18,446 main.py:57] epoch 2025, training loss: 7789.10, average training loss: 7913.47, base loss: 16092.00
[INFO 2017-06-29 01:48:21,550 main.py:57] epoch 2026, training loss: 7623.97, average training loss: 7912.77, base loss: 16092.69
[INFO 2017-06-29 01:48:24,591 main.py:57] epoch 2027, training loss: 7552.22, average training loss: 7912.12, base loss: 16093.84
[INFO 2017-06-29 01:48:27,736 main.py:57] epoch 2028, training loss: 7306.27, average training loss: 7911.49, base loss: 16093.02
[INFO 2017-06-29 01:48:30,801 main.py:57] epoch 2029, training loss: 7536.54, average training loss: 7911.88, base loss: 16092.73
[INFO 2017-06-29 01:48:33,835 main.py:57] epoch 2030, training loss: 6987.71, average training loss: 7910.61, base loss: 16091.37
[INFO 2017-06-29 01:48:36,911 main.py:57] epoch 2031, training loss: 7448.34, average training loss: 7910.88, base loss: 16090.33
[INFO 2017-06-29 01:48:39,994 main.py:57] epoch 2032, training loss: 8745.07, average training loss: 7911.17, base loss: 16091.69
[INFO 2017-06-29 01:48:42,995 main.py:57] epoch 2033, training loss: 7724.76, average training loss: 7910.65, base loss: 16091.96
[INFO 2017-06-29 01:48:46,027 main.py:57] epoch 2034, training loss: 7631.89, average training loss: 7910.53, base loss: 16092.09
[INFO 2017-06-29 01:48:49,091 main.py:57] epoch 2035, training loss: 8234.77, average training loss: 7911.13, base loss: 16093.86
[INFO 2017-06-29 01:48:52,112 main.py:57] epoch 2036, training loss: 7854.73, average training loss: 7911.10, base loss: 16094.32
[INFO 2017-06-29 01:48:55,118 main.py:57] epoch 2037, training loss: 7496.96, average training loss: 7909.74, base loss: 16092.85
[INFO 2017-06-29 01:48:58,171 main.py:57] epoch 2038, training loss: 9293.54, average training loss: 7910.90, base loss: 16095.24
[INFO 2017-06-29 01:49:01,208 main.py:57] epoch 2039, training loss: 7365.87, average training loss: 7909.84, base loss: 16094.82
[INFO 2017-06-29 01:49:04,301 main.py:57] epoch 2040, training loss: 8371.55, average training loss: 7910.01, base loss: 16096.19
[INFO 2017-06-29 01:49:07,438 main.py:57] epoch 2041, training loss: 8137.65, average training loss: 7911.47, base loss: 16097.00
[INFO 2017-06-29 01:49:10,441 main.py:57] epoch 2042, training loss: 8433.59, average training loss: 7912.44, base loss: 16097.62
[INFO 2017-06-29 01:49:13,535 main.py:57] epoch 2043, training loss: 7667.23, average training loss: 7913.12, base loss: 16096.23
[INFO 2017-06-29 01:49:16,554 main.py:57] epoch 2044, training loss: 7080.52, average training loss: 7911.65, base loss: 16095.21
[INFO 2017-06-29 01:49:19,579 main.py:57] epoch 2045, training loss: 7799.09, average training loss: 7911.69, base loss: 16094.57
[INFO 2017-06-29 01:49:22,595 main.py:57] epoch 2046, training loss: 8353.94, average training loss: 7912.10, base loss: 16095.60
[INFO 2017-06-29 01:49:25,639 main.py:57] epoch 2047, training loss: 9376.22, average training loss: 7913.71, base loss: 16098.20
[INFO 2017-06-29 01:49:28,784 main.py:57] epoch 2048, training loss: 7181.16, average training loss: 7912.98, base loss: 16097.54
[INFO 2017-06-29 01:49:31,867 main.py:57] epoch 2049, training loss: 7489.33, average training loss: 7911.51, base loss: 16097.66
[INFO 2017-06-29 01:49:34,881 main.py:57] epoch 2050, training loss: 7191.50, average training loss: 7910.55, base loss: 16096.54
[INFO 2017-06-29 01:49:37,950 main.py:57] epoch 2051, training loss: 7775.37, average training loss: 7909.97, base loss: 16096.55
[INFO 2017-06-29 01:49:40,992 main.py:57] epoch 2052, training loss: 7719.13, average training loss: 7909.90, base loss: 16095.64
[INFO 2017-06-29 01:49:44,034 main.py:57] epoch 2053, training loss: 7096.41, average training loss: 7909.87, base loss: 16094.72
[INFO 2017-06-29 01:49:47,026 main.py:57] epoch 2054, training loss: 7568.56, average training loss: 7909.73, base loss: 16094.00
[INFO 2017-06-29 01:49:50,080 main.py:57] epoch 2055, training loss: 7037.69, average training loss: 7909.29, base loss: 16091.88
[INFO 2017-06-29 01:49:53,134 main.py:57] epoch 2056, training loss: 8288.69, average training loss: 7909.30, base loss: 16090.75
[INFO 2017-06-29 01:49:56,218 main.py:57] epoch 2057, training loss: 8237.43, average training loss: 7909.40, base loss: 16090.41
[INFO 2017-06-29 01:49:59,231 main.py:57] epoch 2058, training loss: 8493.69, average training loss: 7908.34, base loss: 16092.63
[INFO 2017-06-29 01:50:02,186 main.py:57] epoch 2059, training loss: 8459.40, average training loss: 7908.68, base loss: 16092.92
[INFO 2017-06-29 01:50:05,206 main.py:57] epoch 2060, training loss: 8106.18, average training loss: 7907.49, base loss: 16094.75
[INFO 2017-06-29 01:50:08,246 main.py:57] epoch 2061, training loss: 7852.97, average training loss: 7907.69, base loss: 16096.27
[INFO 2017-06-29 01:50:11,287 main.py:57] epoch 2062, training loss: 6767.43, average training loss: 7905.97, base loss: 16093.86
[INFO 2017-06-29 01:50:14,356 main.py:57] epoch 2063, training loss: 7653.54, average training loss: 7905.03, base loss: 16092.79
[INFO 2017-06-29 01:50:17,428 main.py:57] epoch 2064, training loss: 7120.36, average training loss: 7904.00, base loss: 16091.28
[INFO 2017-06-29 01:50:20,512 main.py:57] epoch 2065, training loss: 8424.70, average training loss: 7904.34, base loss: 16093.15
[INFO 2017-06-29 01:50:23,557 main.py:57] epoch 2066, training loss: 7334.32, average training loss: 7902.91, base loss: 16093.82
[INFO 2017-06-29 01:50:26,579 main.py:57] epoch 2067, training loss: 7017.41, average training loss: 7899.21, base loss: 16092.80
[INFO 2017-06-29 01:50:29,638 main.py:57] epoch 2068, training loss: 7487.32, average training loss: 7898.27, base loss: 16093.10
[INFO 2017-06-29 01:50:32,631 main.py:57] epoch 2069, training loss: 7541.33, average training loss: 7897.90, base loss: 16093.08
[INFO 2017-06-29 01:50:35,680 main.py:57] epoch 2070, training loss: 7158.09, average training loss: 7896.99, base loss: 16092.00
[INFO 2017-06-29 01:50:38,766 main.py:57] epoch 2071, training loss: 7143.34, average training loss: 7896.31, base loss: 16092.76
[INFO 2017-06-29 01:50:41,813 main.py:57] epoch 2072, training loss: 7855.63, average training loss: 7896.01, base loss: 16093.96
[INFO 2017-06-29 01:50:44,836 main.py:57] epoch 2073, training loss: 7445.26, average training loss: 7894.65, base loss: 16094.11
[INFO 2017-06-29 01:50:47,860 main.py:57] epoch 2074, training loss: 8375.96, average training loss: 7894.85, base loss: 16095.06
[INFO 2017-06-29 01:50:50,940 main.py:57] epoch 2075, training loss: 7442.87, average training loss: 7893.09, base loss: 16094.54
[INFO 2017-06-29 01:50:53,949 main.py:57] epoch 2076, training loss: 8169.78, average training loss: 7893.52, base loss: 16096.36
[INFO 2017-06-29 01:50:57,014 main.py:57] epoch 2077, training loss: 7108.44, average training loss: 7893.23, base loss: 16095.40
[INFO 2017-06-29 01:51:00,020 main.py:57] epoch 2078, training loss: 7369.32, average training loss: 7893.12, base loss: 16093.55
[INFO 2017-06-29 01:51:03,042 main.py:57] epoch 2079, training loss: 8782.20, average training loss: 7891.92, base loss: 16094.55
[INFO 2017-06-29 01:51:06,114 main.py:57] epoch 2080, training loss: 8680.66, average training loss: 7891.31, base loss: 16095.28
[INFO 2017-06-29 01:51:09,189 main.py:57] epoch 2081, training loss: 7717.73, average training loss: 7890.83, base loss: 16094.89
[INFO 2017-06-29 01:51:12,266 main.py:57] epoch 2082, training loss: 7145.09, average training loss: 7889.63, base loss: 16093.35
[INFO 2017-06-29 01:51:15,316 main.py:57] epoch 2083, training loss: 7299.27, average training loss: 7888.60, base loss: 16090.94
[INFO 2017-06-29 01:51:18,384 main.py:57] epoch 2084, training loss: 7037.01, average training loss: 7887.79, base loss: 16091.06
[INFO 2017-06-29 01:51:21,453 main.py:57] epoch 2085, training loss: 8655.29, average training loss: 7888.97, base loss: 16093.77
[INFO 2017-06-29 01:51:24,499 main.py:57] epoch 2086, training loss: 8527.56, average training loss: 7890.20, base loss: 16094.44
[INFO 2017-06-29 01:51:27,498 main.py:57] epoch 2087, training loss: 7353.76, average training loss: 7890.29, base loss: 16092.40
[INFO 2017-06-29 01:51:30,585 main.py:57] epoch 2088, training loss: 7993.73, average training loss: 7891.21, base loss: 16090.66
[INFO 2017-06-29 01:51:33,719 main.py:57] epoch 2089, training loss: 6870.64, average training loss: 7889.91, base loss: 16088.33
[INFO 2017-06-29 01:51:36,728 main.py:57] epoch 2090, training loss: 7028.91, average training loss: 7888.80, base loss: 16088.38
[INFO 2017-06-29 01:51:39,786 main.py:57] epoch 2091, training loss: 7375.39, average training loss: 7889.29, base loss: 16090.57
[INFO 2017-06-29 01:51:42,824 main.py:57] epoch 2092, training loss: 8083.59, average training loss: 7888.87, base loss: 16091.52
[INFO 2017-06-29 01:51:45,854 main.py:57] epoch 2093, training loss: 9114.46, average training loss: 7889.40, base loss: 16093.88
[INFO 2017-06-29 01:51:48,890 main.py:57] epoch 2094, training loss: 9900.45, average training loss: 7890.31, base loss: 16097.36
[INFO 2017-06-29 01:51:51,985 main.py:57] epoch 2095, training loss: 7435.51, average training loss: 7889.90, base loss: 16097.58
[INFO 2017-06-29 01:51:55,033 main.py:57] epoch 2096, training loss: 7984.28, average training loss: 7889.33, base loss: 16097.29
[INFO 2017-06-29 01:51:58,154 main.py:57] epoch 2097, training loss: 8816.66, average training loss: 7890.20, base loss: 16099.02
[INFO 2017-06-29 01:52:01,216 main.py:57] epoch 2098, training loss: 7284.19, average training loss: 7890.31, base loss: 16098.78
[INFO 2017-06-29 01:52:04,261 main.py:57] epoch 2099, training loss: 7416.01, average training loss: 7889.61, base loss: 16098.05
[INFO 2017-06-29 01:52:04,262 main.py:59] epoch 2099, testing
[INFO 2017-06-29 01:52:16,803 main.py:104] average testing loss: 8016.38, base loss: 16458.97
[INFO 2017-06-29 01:52:16,803 main.py:105] improve_loss: 8442.60, improve_percent: 0.51
[INFO 2017-06-29 01:52:16,805 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 01:52:16,843 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 01:52:19,929 main.py:57] epoch 2100, training loss: 7746.31, average training loss: 7890.26, base loss: 16098.29
[INFO 2017-06-29 01:52:23,006 main.py:57] epoch 2101, training loss: 7897.87, average training loss: 7889.68, base loss: 16098.33
[INFO 2017-06-29 01:52:26,016 main.py:57] epoch 2102, training loss: 7898.83, average training loss: 7889.24, base loss: 16096.79
[INFO 2017-06-29 01:52:29,003 main.py:57] epoch 2103, training loss: 8242.53, average training loss: 7889.71, base loss: 16097.21
[INFO 2017-06-29 01:52:32,027 main.py:57] epoch 2104, training loss: 7992.04, average training loss: 7888.55, base loss: 16097.18
[INFO 2017-06-29 01:52:35,069 main.py:57] epoch 2105, training loss: 6971.28, average training loss: 7887.85, base loss: 16096.48
[INFO 2017-06-29 01:52:38,165 main.py:57] epoch 2106, training loss: 8462.07, average training loss: 7887.75, base loss: 16096.86
[INFO 2017-06-29 01:52:41,195 main.py:57] epoch 2107, training loss: 7448.64, average training loss: 7886.49, base loss: 16096.54
[INFO 2017-06-29 01:52:44,237 main.py:57] epoch 2108, training loss: 8298.54, average training loss: 7887.35, base loss: 16096.42
[INFO 2017-06-29 01:52:47,278 main.py:57] epoch 2109, training loss: 7693.09, average training loss: 7886.72, base loss: 16096.19
[INFO 2017-06-29 01:52:50,360 main.py:57] epoch 2110, training loss: 7786.01, average training loss: 7886.73, base loss: 16096.58
[INFO 2017-06-29 01:52:53,367 main.py:57] epoch 2111, training loss: 7887.34, average training loss: 7886.85, base loss: 16097.00
[INFO 2017-06-29 01:52:56,409 main.py:57] epoch 2112, training loss: 7969.33, average training loss: 7886.83, base loss: 16096.18
[INFO 2017-06-29 01:52:59,473 main.py:57] epoch 2113, training loss: 6800.61, average training loss: 7886.07, base loss: 16093.77
[INFO 2017-06-29 01:53:02,580 main.py:57] epoch 2114, training loss: 6872.64, average training loss: 7885.66, base loss: 16091.74
[INFO 2017-06-29 01:53:05,607 main.py:57] epoch 2115, training loss: 8910.03, average training loss: 7886.72, base loss: 16092.11
[INFO 2017-06-29 01:53:08,691 main.py:57] epoch 2116, training loss: 7052.13, average training loss: 7885.76, base loss: 16091.47
[INFO 2017-06-29 01:53:11,766 main.py:57] epoch 2117, training loss: 7071.15, average training loss: 7885.52, base loss: 16090.34
[INFO 2017-06-29 01:53:14,806 main.py:57] epoch 2118, training loss: 8929.99, average training loss: 7886.22, base loss: 16091.07
[INFO 2017-06-29 01:53:17,839 main.py:57] epoch 2119, training loss: 7857.82, average training loss: 7887.19, base loss: 16090.09
[INFO 2017-06-29 01:53:20,864 main.py:57] epoch 2120, training loss: 8081.82, average training loss: 7887.42, base loss: 16091.04
[INFO 2017-06-29 01:53:23,949 main.py:57] epoch 2121, training loss: 10302.43, average training loss: 7890.65, base loss: 16094.66
[INFO 2017-06-29 01:53:26,985 main.py:57] epoch 2122, training loss: 7445.40, average training loss: 7890.43, base loss: 16092.95
[INFO 2017-06-29 01:53:30,040 main.py:57] epoch 2123, training loss: 7599.57, average training loss: 7889.61, base loss: 16091.36
[INFO 2017-06-29 01:53:33,068 main.py:57] epoch 2124, training loss: 8109.47, average training loss: 7890.61, base loss: 16092.25
[INFO 2017-06-29 01:53:36,179 main.py:57] epoch 2125, training loss: 7367.95, average training loss: 7890.48, base loss: 16092.53
[INFO 2017-06-29 01:53:39,245 main.py:57] epoch 2126, training loss: 7819.26, average training loss: 7889.22, base loss: 16092.78
[INFO 2017-06-29 01:53:42,322 main.py:57] epoch 2127, training loss: 7709.31, average training loss: 7888.68, base loss: 16091.98
[INFO 2017-06-29 01:53:45,423 main.py:57] epoch 2128, training loss: 7964.12, average training loss: 7889.08, base loss: 16093.23
[INFO 2017-06-29 01:53:48,484 main.py:57] epoch 2129, training loss: 7762.83, average training loss: 7888.79, base loss: 16094.01
[INFO 2017-06-29 01:53:51,540 main.py:57] epoch 2130, training loss: 8683.19, average training loss: 7889.31, base loss: 16094.83
[INFO 2017-06-29 01:53:54,590 main.py:57] epoch 2131, training loss: 8289.94, average training loss: 7889.65, base loss: 16096.53
[INFO 2017-06-29 01:53:57,678 main.py:57] epoch 2132, training loss: 7744.77, average training loss: 7890.00, base loss: 16096.13
[INFO 2017-06-29 01:54:00,775 main.py:57] epoch 2133, training loss: 7908.87, average training loss: 7889.83, base loss: 16096.87
[INFO 2017-06-29 01:54:03,830 main.py:57] epoch 2134, training loss: 8379.71, average training loss: 7890.58, base loss: 16096.78
[INFO 2017-06-29 01:54:06,932 main.py:57] epoch 2135, training loss: 7248.68, average training loss: 7890.68, base loss: 16094.35
[INFO 2017-06-29 01:54:10,001 main.py:57] epoch 2136, training loss: 6929.03, average training loss: 7889.46, base loss: 16093.51
[INFO 2017-06-29 01:54:13,004 main.py:57] epoch 2137, training loss: 8030.24, average training loss: 7889.81, base loss: 16094.23
[INFO 2017-06-29 01:54:16,039 main.py:57] epoch 2138, training loss: 8411.42, average training loss: 7891.05, base loss: 16095.22
[INFO 2017-06-29 01:54:19,080 main.py:57] epoch 2139, training loss: 7661.56, average training loss: 7891.12, base loss: 16095.49
[INFO 2017-06-29 01:54:22,112 main.py:57] epoch 2140, training loss: 7821.77, average training loss: 7890.38, base loss: 16094.92
[INFO 2017-06-29 01:54:25,126 main.py:57] epoch 2141, training loss: 7341.79, average training loss: 7889.88, base loss: 16094.01
[INFO 2017-06-29 01:54:28,104 main.py:57] epoch 2142, training loss: 7478.59, average training loss: 7889.63, base loss: 16093.44
[INFO 2017-06-29 01:54:31,150 main.py:57] epoch 2143, training loss: 8046.58, average training loss: 7890.11, base loss: 16093.61
[INFO 2017-06-29 01:54:34,266 main.py:57] epoch 2144, training loss: 7521.94, average training loss: 7890.34, base loss: 16093.26
[INFO 2017-06-29 01:54:37,342 main.py:57] epoch 2145, training loss: 8240.69, average training loss: 7890.48, base loss: 16093.90
[INFO 2017-06-29 01:54:40,410 main.py:57] epoch 2146, training loss: 7080.80, average training loss: 7890.01, base loss: 16093.11
[INFO 2017-06-29 01:54:43,505 main.py:57] epoch 2147, training loss: 7737.24, average training loss: 7889.64, base loss: 16093.40
[INFO 2017-06-29 01:54:46,624 main.py:57] epoch 2148, training loss: 7238.98, average training loss: 7887.83, base loss: 16092.10
[INFO 2017-06-29 01:54:49,665 main.py:57] epoch 2149, training loss: 7950.53, average training loss: 7888.28, base loss: 16092.99
[INFO 2017-06-29 01:54:52,758 main.py:57] epoch 2150, training loss: 7615.18, average training loss: 7888.58, base loss: 16092.46
[INFO 2017-06-29 01:54:55,864 main.py:57] epoch 2151, training loss: 7435.54, average training loss: 7888.28, base loss: 16091.64
[INFO 2017-06-29 01:54:58,998 main.py:57] epoch 2152, training loss: 7021.65, average training loss: 7887.12, base loss: 16091.19
[INFO 2017-06-29 01:55:02,051 main.py:57] epoch 2153, training loss: 7757.07, average training loss: 7887.31, base loss: 16091.44
[INFO 2017-06-29 01:55:05,113 main.py:57] epoch 2154, training loss: 7496.48, average training loss: 7886.85, base loss: 16091.32
[INFO 2017-06-29 01:55:08,119 main.py:57] epoch 2155, training loss: 7498.72, average training loss: 7886.53, base loss: 16090.95
[INFO 2017-06-29 01:55:11,166 main.py:57] epoch 2156, training loss: 7968.99, average training loss: 7886.57, base loss: 16091.52
[INFO 2017-06-29 01:55:14,224 main.py:57] epoch 2157, training loss: 9147.16, average training loss: 7887.88, base loss: 16093.88
[INFO 2017-06-29 01:55:17,258 main.py:57] epoch 2158, training loss: 8080.91, average training loss: 7886.96, base loss: 16093.02
[INFO 2017-06-29 01:55:20,339 main.py:57] epoch 2159, training loss: 7767.80, average training loss: 7886.77, base loss: 16092.89
[INFO 2017-06-29 01:55:23,396 main.py:57] epoch 2160, training loss: 9240.71, average training loss: 7888.55, base loss: 16094.91
[INFO 2017-06-29 01:55:26,533 main.py:57] epoch 2161, training loss: 8599.27, average training loss: 7887.82, base loss: 16097.34
[INFO 2017-06-29 01:55:29,553 main.py:57] epoch 2162, training loss: 7898.25, average training loss: 7888.32, base loss: 16097.38
[INFO 2017-06-29 01:55:32,613 main.py:57] epoch 2163, training loss: 7512.01, average training loss: 7887.72, base loss: 16098.07
[INFO 2017-06-29 01:55:35,630 main.py:57] epoch 2164, training loss: 7425.21, average training loss: 7887.09, base loss: 16098.48
[INFO 2017-06-29 01:55:38,691 main.py:57] epoch 2165, training loss: 7199.18, average training loss: 7887.42, base loss: 16098.30
[INFO 2017-06-29 01:55:41,709 main.py:57] epoch 2166, training loss: 9313.45, average training loss: 7888.60, base loss: 16098.83
[INFO 2017-06-29 01:55:44,784 main.py:57] epoch 2167, training loss: 8824.28, average training loss: 7889.80, base loss: 16099.81
[INFO 2017-06-29 01:55:47,809 main.py:57] epoch 2168, training loss: 7056.57, average training loss: 7889.45, base loss: 16097.78
[INFO 2017-06-29 01:55:50,852 main.py:57] epoch 2169, training loss: 7911.53, average training loss: 7889.89, base loss: 16096.92
[INFO 2017-06-29 01:55:53,913 main.py:57] epoch 2170, training loss: 8387.83, average training loss: 7889.61, base loss: 16097.83
[INFO 2017-06-29 01:55:56,939 main.py:57] epoch 2171, training loss: 6921.50, average training loss: 7888.82, base loss: 16096.84
[INFO 2017-06-29 01:55:59,983 main.py:57] epoch 2172, training loss: 8706.79, average training loss: 7890.00, base loss: 16097.64
[INFO 2017-06-29 01:56:03,035 main.py:57] epoch 2173, training loss: 7399.72, average training loss: 7890.03, base loss: 16097.67
[INFO 2017-06-29 01:56:06,100 main.py:57] epoch 2174, training loss: 9160.31, average training loss: 7892.12, base loss: 16100.01
[INFO 2017-06-29 01:56:09,118 main.py:57] epoch 2175, training loss: 7369.06, average training loss: 7891.10, base loss: 16100.43
[INFO 2017-06-29 01:56:12,114 main.py:57] epoch 2176, training loss: 7801.58, average training loss: 7890.39, base loss: 16101.06
[INFO 2017-06-29 01:56:15,111 main.py:57] epoch 2177, training loss: 8289.04, average training loss: 7891.20, base loss: 16102.05
[INFO 2017-06-29 01:56:18,155 main.py:57] epoch 2178, training loss: 8124.96, average training loss: 7890.08, base loss: 16103.11
[INFO 2017-06-29 01:56:21,149 main.py:57] epoch 2179, training loss: 8024.90, average training loss: 7890.59, base loss: 16104.39
[INFO 2017-06-29 01:56:24,187 main.py:57] epoch 2180, training loss: 7169.97, average training loss: 7889.56, base loss: 16102.73
[INFO 2017-06-29 01:56:27,284 main.py:57] epoch 2181, training loss: 7928.79, average training loss: 7889.01, base loss: 16103.24
[INFO 2017-06-29 01:56:30,345 main.py:57] epoch 2182, training loss: 7300.91, average training loss: 7887.09, base loss: 16102.60
[INFO 2017-06-29 01:56:33,455 main.py:57] epoch 2183, training loss: 7825.32, average training loss: 7886.50, base loss: 16102.12
[INFO 2017-06-29 01:56:36,506 main.py:57] epoch 2184, training loss: 7430.12, average training loss: 7886.02, base loss: 16101.74
[INFO 2017-06-29 01:56:39,602 main.py:57] epoch 2185, training loss: 7497.60, average training loss: 7885.73, base loss: 16101.72
[INFO 2017-06-29 01:56:42,713 main.py:57] epoch 2186, training loss: 6937.10, average training loss: 7884.34, base loss: 16099.90
[INFO 2017-06-29 01:56:45,756 main.py:57] epoch 2187, training loss: 7686.89, average training loss: 7884.70, base loss: 16101.49
[INFO 2017-06-29 01:56:48,799 main.py:57] epoch 2188, training loss: 7494.84, average training loss: 7883.99, base loss: 16100.66
[INFO 2017-06-29 01:56:51,814 main.py:57] epoch 2189, training loss: 7637.89, average training loss: 7884.21, base loss: 16100.93
[INFO 2017-06-29 01:56:54,911 main.py:57] epoch 2190, training loss: 8053.53, average training loss: 7883.40, base loss: 16102.53
[INFO 2017-06-29 01:56:58,022 main.py:57] epoch 2191, training loss: 7124.83, average training loss: 7882.16, base loss: 16102.84
[INFO 2017-06-29 01:57:01,081 main.py:57] epoch 2192, training loss: 6961.17, average training loss: 7880.61, base loss: 16100.35
[INFO 2017-06-29 01:57:04,104 main.py:57] epoch 2193, training loss: 6831.91, average training loss: 7879.01, base loss: 16098.24
[INFO 2017-06-29 01:57:07,183 main.py:57] epoch 2194, training loss: 7891.07, average training loss: 7879.36, base loss: 16099.32
[INFO 2017-06-29 01:57:10,302 main.py:57] epoch 2195, training loss: 6920.28, average training loss: 7878.37, base loss: 16099.52
[INFO 2017-06-29 01:57:13,317 main.py:57] epoch 2196, training loss: 7469.87, average training loss: 7878.52, base loss: 16099.37
[INFO 2017-06-29 01:57:16,382 main.py:57] epoch 2197, training loss: 7417.64, average training loss: 7878.87, base loss: 16099.59
[INFO 2017-06-29 01:57:19,410 main.py:57] epoch 2198, training loss: 8126.55, average training loss: 7879.64, base loss: 16100.03
[INFO 2017-06-29 01:57:22,485 main.py:57] epoch 2199, training loss: 8733.63, average training loss: 7880.87, base loss: 16101.02
[INFO 2017-06-29 01:57:22,485 main.py:59] epoch 2199, testing
[INFO 2017-06-29 01:57:35,040 main.py:104] average testing loss: 8677.65, base loss: 17371.54
[INFO 2017-06-29 01:57:35,040 main.py:105] improve_loss: 8693.89, improve_percent: 0.50
[INFO 2017-06-29 01:57:35,041 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 01:57:38,040 main.py:57] epoch 2200, training loss: 8973.05, average training loss: 7881.28, base loss: 16101.50
[INFO 2017-06-29 01:57:41,166 main.py:57] epoch 2201, training loss: 8255.14, average training loss: 7881.66, base loss: 16101.61
[INFO 2017-06-29 01:57:44,282 main.py:57] epoch 2202, training loss: 7937.44, average training loss: 7881.61, base loss: 16100.48
[INFO 2017-06-29 01:57:47,399 main.py:57] epoch 2203, training loss: 7054.60, average training loss: 7880.45, base loss: 16098.72
[INFO 2017-06-29 01:57:50,550 main.py:57] epoch 2204, training loss: 7597.84, average training loss: 7880.48, base loss: 16098.24
[INFO 2017-06-29 01:57:53,561 main.py:57] epoch 2205, training loss: 7707.53, average training loss: 7880.21, base loss: 16098.07
[INFO 2017-06-29 01:57:56,678 main.py:57] epoch 2206, training loss: 7129.27, average training loss: 7879.48, base loss: 16097.18
[INFO 2017-06-29 01:57:59,718 main.py:57] epoch 2207, training loss: 8915.75, average training loss: 7879.87, base loss: 16099.10
[INFO 2017-06-29 01:58:02,738 main.py:57] epoch 2208, training loss: 7607.65, average training loss: 7878.13, base loss: 16099.48
[INFO 2017-06-29 01:58:05,758 main.py:57] epoch 2209, training loss: 7429.97, average training loss: 7878.05, base loss: 16100.16
[INFO 2017-06-29 01:58:08,815 main.py:57] epoch 2210, training loss: 7072.09, average training loss: 7877.18, base loss: 16099.25
[INFO 2017-06-29 01:58:11,913 main.py:57] epoch 2211, training loss: 7618.68, average training loss: 7877.25, base loss: 16098.01
[INFO 2017-06-29 01:58:14,998 main.py:57] epoch 2212, training loss: 8176.11, average training loss: 7877.28, base loss: 16099.68
[INFO 2017-06-29 01:58:18,066 main.py:57] epoch 2213, training loss: 7508.68, average training loss: 7877.29, base loss: 16099.42
[INFO 2017-06-29 01:58:21,099 main.py:57] epoch 2214, training loss: 7417.01, average training loss: 7877.17, base loss: 16098.99
[INFO 2017-06-29 01:58:24,186 main.py:57] epoch 2215, training loss: 8471.02, average training loss: 7878.24, base loss: 16099.67
[INFO 2017-06-29 01:58:27,240 main.py:57] epoch 2216, training loss: 8132.48, average training loss: 7878.08, base loss: 16099.91
[INFO 2017-06-29 01:58:30,305 main.py:57] epoch 2217, training loss: 6766.49, average training loss: 7877.90, base loss: 16098.29
[INFO 2017-06-29 01:58:33,354 main.py:57] epoch 2218, training loss: 6566.49, average training loss: 7876.30, base loss: 16096.35
[INFO 2017-06-29 01:58:36,382 main.py:57] epoch 2219, training loss: 7863.52, average training loss: 7876.25, base loss: 16096.74
[INFO 2017-06-29 01:58:39,444 main.py:57] epoch 2220, training loss: 7084.02, average training loss: 7875.59, base loss: 16095.88
[INFO 2017-06-29 01:58:42,470 main.py:57] epoch 2221, training loss: 7858.76, average training loss: 7875.67, base loss: 16095.72
[INFO 2017-06-29 01:58:45,543 main.py:57] epoch 2222, training loss: 7442.44, average training loss: 7874.37, base loss: 16095.39
[INFO 2017-06-29 01:58:48,644 main.py:57] epoch 2223, training loss: 6791.87, average training loss: 7872.79, base loss: 16093.84
[INFO 2017-06-29 01:58:51,698 main.py:57] epoch 2224, training loss: 6594.14, average training loss: 7873.05, base loss: 16092.57
[INFO 2017-06-29 01:58:54,767 main.py:57] epoch 2225, training loss: 8556.10, average training loss: 7873.19, base loss: 16093.85
[INFO 2017-06-29 01:58:57,816 main.py:57] epoch 2226, training loss: 7591.23, average training loss: 7871.92, base loss: 16094.70
[INFO 2017-06-29 01:59:00,925 main.py:57] epoch 2227, training loss: 7870.96, average training loss: 7871.06, base loss: 16095.01
[INFO 2017-06-29 01:59:04,052 main.py:57] epoch 2228, training loss: 7461.18, average training loss: 7870.34, base loss: 16094.96
[INFO 2017-06-29 01:59:07,129 main.py:57] epoch 2229, training loss: 6568.77, average training loss: 7869.45, base loss: 16093.89
[INFO 2017-06-29 01:59:10,160 main.py:57] epoch 2230, training loss: 9014.88, average training loss: 7870.22, base loss: 16095.14
[INFO 2017-06-29 01:59:13,208 main.py:57] epoch 2231, training loss: 7579.68, average training loss: 7868.74, base loss: 16094.61
[INFO 2017-06-29 01:59:16,319 main.py:57] epoch 2232, training loss: 7254.21, average training loss: 7868.08, base loss: 16094.61
[INFO 2017-06-29 01:59:19,398 main.py:57] epoch 2233, training loss: 7219.91, average training loss: 7868.20, base loss: 16095.70
[INFO 2017-06-29 01:59:22,453 main.py:57] epoch 2234, training loss: 8401.25, average training loss: 7868.49, base loss: 16096.95
[INFO 2017-06-29 01:59:25,469 main.py:57] epoch 2235, training loss: 7603.52, average training loss: 7868.06, base loss: 16097.27
[INFO 2017-06-29 01:59:28,521 main.py:57] epoch 2236, training loss: 7607.61, average training loss: 7868.01, base loss: 16097.40
[INFO 2017-06-29 01:59:31,580 main.py:57] epoch 2237, training loss: 7296.55, average training loss: 7868.01, base loss: 16096.97
[INFO 2017-06-29 01:59:34,653 main.py:57] epoch 2238, training loss: 7533.59, average training loss: 7867.46, base loss: 16096.75
[INFO 2017-06-29 01:59:37,702 main.py:57] epoch 2239, training loss: 8181.99, average training loss: 7868.17, base loss: 16098.54
[INFO 2017-06-29 01:59:40,851 main.py:57] epoch 2240, training loss: 7443.77, average training loss: 7867.49, base loss: 16098.87
[INFO 2017-06-29 01:59:43,903 main.py:57] epoch 2241, training loss: 8853.02, average training loss: 7868.01, base loss: 16100.33
[INFO 2017-06-29 01:59:46,957 main.py:57] epoch 2242, training loss: 7448.08, average training loss: 7867.08, base loss: 16100.17
[INFO 2017-06-29 01:59:50,050 main.py:57] epoch 2243, training loss: 8200.06, average training loss: 7867.01, base loss: 16100.90
[INFO 2017-06-29 01:59:53,029 main.py:57] epoch 2244, training loss: 7800.76, average training loss: 7866.39, base loss: 16101.10
[INFO 2017-06-29 01:59:56,080 main.py:57] epoch 2245, training loss: 8314.97, average training loss: 7867.62, base loss: 16102.08
[INFO 2017-06-29 01:59:59,170 main.py:57] epoch 2246, training loss: 7239.39, average training loss: 7867.38, base loss: 16100.58
[INFO 2017-06-29 02:00:02,310 main.py:57] epoch 2247, training loss: 8374.34, average training loss: 7868.16, base loss: 16100.52
[INFO 2017-06-29 02:00:05,389 main.py:57] epoch 2248, training loss: 6858.06, average training loss: 7866.11, base loss: 16099.24
[INFO 2017-06-29 02:00:08,486 main.py:57] epoch 2249, training loss: 7330.53, average training loss: 7865.08, base loss: 16099.26
[INFO 2017-06-29 02:00:11,549 main.py:57] epoch 2250, training loss: 7396.15, average training loss: 7864.30, base loss: 16099.14
[INFO 2017-06-29 02:00:14,555 main.py:57] epoch 2251, training loss: 7482.40, average training loss: 7863.11, base loss: 16098.77
[INFO 2017-06-29 02:00:17,666 main.py:57] epoch 2252, training loss: 8306.57, average training loss: 7863.39, base loss: 16100.26
[INFO 2017-06-29 02:00:20,753 main.py:57] epoch 2253, training loss: 7733.91, average training loss: 7863.46, base loss: 16100.91
[INFO 2017-06-29 02:00:23,851 main.py:57] epoch 2254, training loss: 8143.97, average training loss: 7864.12, base loss: 16100.81
[INFO 2017-06-29 02:00:26,912 main.py:57] epoch 2255, training loss: 6973.31, average training loss: 7862.99, base loss: 16098.78
[INFO 2017-06-29 02:00:30,034 main.py:57] epoch 2256, training loss: 7470.19, average training loss: 7862.99, base loss: 16098.99
[INFO 2017-06-29 02:00:33,130 main.py:57] epoch 2257, training loss: 8155.12, average training loss: 7863.65, base loss: 16100.35
[INFO 2017-06-29 02:00:36,204 main.py:57] epoch 2258, training loss: 7888.15, average training loss: 7862.54, base loss: 16101.22
[INFO 2017-06-29 02:00:39,224 main.py:57] epoch 2259, training loss: 8032.22, average training loss: 7863.17, base loss: 16101.24
[INFO 2017-06-29 02:00:42,234 main.py:57] epoch 2260, training loss: 7031.44, average training loss: 7861.96, base loss: 16098.90
[INFO 2017-06-29 02:00:45,267 main.py:57] epoch 2261, training loss: 7495.28, average training loss: 7860.41, base loss: 16097.88
[INFO 2017-06-29 02:00:48,328 main.py:57] epoch 2262, training loss: 8382.58, average training loss: 7861.00, base loss: 16098.01
[INFO 2017-06-29 02:00:51,412 main.py:57] epoch 2263, training loss: 7918.76, average training loss: 7861.80, base loss: 16097.45
[INFO 2017-06-29 02:00:54,422 main.py:57] epoch 2264, training loss: 8825.38, average training loss: 7862.64, base loss: 16097.84
[INFO 2017-06-29 02:00:57,511 main.py:57] epoch 2265, training loss: 7778.82, average training loss: 7862.74, base loss: 16097.18
[INFO 2017-06-29 02:01:00,667 main.py:57] epoch 2266, training loss: 8287.79, average training loss: 7863.75, base loss: 16098.63
[INFO 2017-06-29 02:01:03,750 main.py:57] epoch 2267, training loss: 10313.45, average training loss: 7866.94, base loss: 16101.39
[INFO 2017-06-29 02:01:06,846 main.py:57] epoch 2268, training loss: 8062.48, average training loss: 7867.29, base loss: 16102.48
[INFO 2017-06-29 02:01:09,888 main.py:57] epoch 2269, training loss: 7429.36, average training loss: 7867.94, base loss: 16101.60
[INFO 2017-06-29 02:01:12,983 main.py:57] epoch 2270, training loss: 7372.27, average training loss: 7867.96, base loss: 16100.67
[INFO 2017-06-29 02:01:15,990 main.py:57] epoch 2271, training loss: 7685.28, average training loss: 7867.66, base loss: 16100.51
[INFO 2017-06-29 02:01:19,025 main.py:57] epoch 2272, training loss: 7675.53, average training loss: 7866.64, base loss: 16099.74
[INFO 2017-06-29 02:01:22,097 main.py:57] epoch 2273, training loss: 8946.73, average training loss: 7867.77, base loss: 16101.29
[INFO 2017-06-29 02:01:25,155 main.py:57] epoch 2274, training loss: 7071.91, average training loss: 7867.36, base loss: 16100.36
[INFO 2017-06-29 02:01:28,191 main.py:57] epoch 2275, training loss: 7342.24, average training loss: 7866.85, base loss: 16100.01
[INFO 2017-06-29 02:01:31,245 main.py:57] epoch 2276, training loss: 7045.05, average training loss: 7866.17, base loss: 16097.65
[INFO 2017-06-29 02:01:34,318 main.py:57] epoch 2277, training loss: 9122.14, average training loss: 7866.42, base loss: 16098.24
[INFO 2017-06-29 02:01:37,413 main.py:57] epoch 2278, training loss: 7874.09, average training loss: 7865.60, base loss: 16097.39
[INFO 2017-06-29 02:01:40,440 main.py:57] epoch 2279, training loss: 8400.03, average training loss: 7864.53, base loss: 16098.79
[INFO 2017-06-29 02:01:43,461 main.py:57] epoch 2280, training loss: 7022.47, average training loss: 7864.37, base loss: 16098.93
[INFO 2017-06-29 02:01:46,549 main.py:57] epoch 2281, training loss: 7602.41, average training loss: 7863.51, base loss: 16099.53
[INFO 2017-06-29 02:01:49,673 main.py:57] epoch 2282, training loss: 6856.91, average training loss: 7861.84, base loss: 16098.25
[INFO 2017-06-29 02:01:52,670 main.py:57] epoch 2283, training loss: 8036.10, average training loss: 7862.46, base loss: 16098.92
[INFO 2017-06-29 02:01:55,727 main.py:57] epoch 2284, training loss: 8208.29, average training loss: 7862.74, base loss: 16100.13
[INFO 2017-06-29 02:01:58,798 main.py:57] epoch 2285, training loss: 8833.46, average training loss: 7863.61, base loss: 16101.53
[INFO 2017-06-29 02:02:01,874 main.py:57] epoch 2286, training loss: 8011.54, average training loss: 7863.55, base loss: 16101.74
[INFO 2017-06-29 02:02:04,910 main.py:57] epoch 2287, training loss: 7405.46, average training loss: 7863.80, base loss: 16101.41
[INFO 2017-06-29 02:02:07,946 main.py:57] epoch 2288, training loss: 7263.61, average training loss: 7863.01, base loss: 16100.66
[INFO 2017-06-29 02:02:11,010 main.py:57] epoch 2289, training loss: 7676.58, average training loss: 7862.95, base loss: 16100.23
[INFO 2017-06-29 02:02:14,055 main.py:57] epoch 2290, training loss: 7321.62, average training loss: 7863.19, base loss: 16100.82
[INFO 2017-06-29 02:02:17,150 main.py:57] epoch 2291, training loss: 7402.32, average training loss: 7862.84, base loss: 16100.33
[INFO 2017-06-29 02:02:20,171 main.py:57] epoch 2292, training loss: 7252.04, average training loss: 7862.41, base loss: 16099.25
[INFO 2017-06-29 02:02:23,249 main.py:57] epoch 2293, training loss: 7829.58, average training loss: 7861.35, base loss: 16099.22
[INFO 2017-06-29 02:02:26,361 main.py:57] epoch 2294, training loss: 7195.26, average training loss: 7861.02, base loss: 16099.57
[INFO 2017-06-29 02:02:29,377 main.py:57] epoch 2295, training loss: 8445.17, average training loss: 7861.72, base loss: 16101.95
[INFO 2017-06-29 02:02:32,452 main.py:57] epoch 2296, training loss: 6409.56, average training loss: 7859.88, base loss: 16100.04
[INFO 2017-06-29 02:02:35,513 main.py:57] epoch 2297, training loss: 7292.80, average training loss: 7859.94, base loss: 16099.84
[INFO 2017-06-29 02:02:38,520 main.py:57] epoch 2298, training loss: 7414.47, average training loss: 7860.23, base loss: 16099.97
[INFO 2017-06-29 02:02:41,501 main.py:57] epoch 2299, training loss: 7001.03, average training loss: 7859.24, base loss: 16099.26
[INFO 2017-06-29 02:02:41,501 main.py:59] epoch 2299, testing
[INFO 2017-06-29 02:02:54,294 main.py:104] average testing loss: 8535.41, base loss: 17249.25
[INFO 2017-06-29 02:02:54,294 main.py:105] improve_loss: 8713.84, improve_percent: 0.51
[INFO 2017-06-29 02:02:54,296 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:02:57,339 main.py:57] epoch 2300, training loss: 7705.62, average training loss: 7859.24, base loss: 16097.38
[INFO 2017-06-29 02:03:00,410 main.py:57] epoch 2301, training loss: 7227.64, average training loss: 7859.73, base loss: 16095.94
[INFO 2017-06-29 02:03:03,477 main.py:57] epoch 2302, training loss: 8682.65, average training loss: 7858.18, base loss: 16097.85
[INFO 2017-06-29 02:03:06,566 main.py:57] epoch 2303, training loss: 7485.56, average training loss: 7858.03, base loss: 16097.42
[INFO 2017-06-29 02:03:09,590 main.py:57] epoch 2304, training loss: 8316.77, average training loss: 7858.88, base loss: 16097.10
[INFO 2017-06-29 02:03:12,650 main.py:57] epoch 2305, training loss: 7383.89, average training loss: 7858.64, base loss: 16095.94
[INFO 2017-06-29 02:03:15,752 main.py:57] epoch 2306, training loss: 6949.36, average training loss: 7857.48, base loss: 16095.82
[INFO 2017-06-29 02:03:18,791 main.py:57] epoch 2307, training loss: 8744.91, average training loss: 7857.07, base loss: 16097.51
[INFO 2017-06-29 02:03:21,865 main.py:57] epoch 2308, training loss: 7141.89, average training loss: 7855.94, base loss: 16096.89
[INFO 2017-06-29 02:03:24,942 main.py:57] epoch 2309, training loss: 8119.81, average training loss: 7855.46, base loss: 16096.91
[INFO 2017-06-29 02:03:28,036 main.py:57] epoch 2310, training loss: 6967.48, average training loss: 7853.21, base loss: 16094.86
[INFO 2017-06-29 02:03:31,145 main.py:57] epoch 2311, training loss: 7285.62, average training loss: 7852.13, base loss: 16094.18
[INFO 2017-06-29 02:03:34,186 main.py:57] epoch 2312, training loss: 6553.31, average training loss: 7850.33, base loss: 16093.83
[INFO 2017-06-29 02:03:37,241 main.py:57] epoch 2313, training loss: 8084.86, average training loss: 7851.09, base loss: 16095.75
[INFO 2017-06-29 02:03:40,320 main.py:57] epoch 2314, training loss: 6713.80, average training loss: 7850.77, base loss: 16094.51
[INFO 2017-06-29 02:03:43,417 main.py:57] epoch 2315, training loss: 6819.31, average training loss: 7850.67, base loss: 16092.97
[INFO 2017-06-29 02:03:46,380 main.py:57] epoch 2316, training loss: 7068.26, average training loss: 7849.17, base loss: 16091.76
[INFO 2017-06-29 02:03:49,436 main.py:57] epoch 2317, training loss: 8435.76, average training loss: 7849.28, base loss: 16091.35
[INFO 2017-06-29 02:03:52,541 main.py:57] epoch 2318, training loss: 8966.53, average training loss: 7849.18, base loss: 16091.45
[INFO 2017-06-29 02:03:55,674 main.py:57] epoch 2319, training loss: 7617.70, average training loss: 7849.08, base loss: 16089.68
[INFO 2017-06-29 02:03:58,674 main.py:57] epoch 2320, training loss: 7928.57, average training loss: 7849.18, base loss: 16089.00
[INFO 2017-06-29 02:04:01,654 main.py:57] epoch 2321, training loss: 7663.41, average training loss: 7849.81, base loss: 16087.55
[INFO 2017-06-29 02:04:04,797 main.py:57] epoch 2322, training loss: 8006.28, average training loss: 7849.99, base loss: 16087.98
[INFO 2017-06-29 02:04:07,855 main.py:57] epoch 2323, training loss: 6430.22, average training loss: 7849.09, base loss: 16085.91
[INFO 2017-06-29 02:04:10,909 main.py:57] epoch 2324, training loss: 8634.66, average training loss: 7850.39, base loss: 16085.81
[INFO 2017-06-29 02:04:13,979 main.py:57] epoch 2325, training loss: 7494.42, average training loss: 7850.13, base loss: 16085.34
[INFO 2017-06-29 02:04:17,088 main.py:57] epoch 2326, training loss: 7529.48, average training loss: 7850.12, base loss: 16084.83
[INFO 2017-06-29 02:04:20,132 main.py:57] epoch 2327, training loss: 7697.62, average training loss: 7850.03, base loss: 16084.48
[INFO 2017-06-29 02:04:23,170 main.py:57] epoch 2328, training loss: 8099.52, average training loss: 7850.64, base loss: 16085.13
[INFO 2017-06-29 02:04:26,203 main.py:57] epoch 2329, training loss: 7728.18, average training loss: 7849.70, base loss: 16084.88
[INFO 2017-06-29 02:04:29,229 main.py:57] epoch 2330, training loss: 7445.08, average training loss: 7849.32, base loss: 16082.66
[INFO 2017-06-29 02:04:32,320 main.py:57] epoch 2331, training loss: 7129.50, average training loss: 7848.45, base loss: 16080.66
[INFO 2017-06-29 02:04:35,345 main.py:57] epoch 2332, training loss: 7094.74, average training loss: 7847.61, base loss: 16081.08
[INFO 2017-06-29 02:04:38,408 main.py:57] epoch 2333, training loss: 7490.98, average training loss: 7845.84, base loss: 16081.35
[INFO 2017-06-29 02:04:41,489 main.py:57] epoch 2334, training loss: 7434.50, average training loss: 7845.54, base loss: 16081.17
[INFO 2017-06-29 02:04:44,530 main.py:57] epoch 2335, training loss: 7874.69, average training loss: 7845.26, base loss: 16081.66
[INFO 2017-06-29 02:04:47,618 main.py:57] epoch 2336, training loss: 8032.85, average training loss: 7844.99, base loss: 16082.23
[INFO 2017-06-29 02:04:50,705 main.py:57] epoch 2337, training loss: 8414.02, average training loss: 7846.36, base loss: 16082.72
[INFO 2017-06-29 02:04:53,759 main.py:57] epoch 2338, training loss: 7837.06, average training loss: 7845.64, base loss: 16082.18
[INFO 2017-06-29 02:04:56,789 main.py:57] epoch 2339, training loss: 8458.12, average training loss: 7847.36, base loss: 16082.01
[INFO 2017-06-29 02:04:59,827 main.py:57] epoch 2340, training loss: 8550.50, average training loss: 7846.80, base loss: 16083.80
[INFO 2017-06-29 02:05:02,828 main.py:57] epoch 2341, training loss: 8616.70, average training loss: 7847.57, base loss: 16085.71
[INFO 2017-06-29 02:05:05,850 main.py:57] epoch 2342, training loss: 7327.85, average training loss: 7846.50, base loss: 16085.26
[INFO 2017-06-29 02:05:08,928 main.py:57] epoch 2343, training loss: 7655.34, average training loss: 7846.00, base loss: 16085.15
[INFO 2017-06-29 02:05:11,944 main.py:57] epoch 2344, training loss: 7809.52, average training loss: 7846.97, base loss: 16085.28
[INFO 2017-06-29 02:05:14,995 main.py:57] epoch 2345, training loss: 8485.25, average training loss: 7848.09, base loss: 16086.07
[INFO 2017-06-29 02:05:18,052 main.py:57] epoch 2346, training loss: 8430.49, average training loss: 7847.91, base loss: 16087.49
[INFO 2017-06-29 02:05:21,173 main.py:57] epoch 2347, training loss: 7730.05, average training loss: 7848.61, base loss: 16087.62
[INFO 2017-06-29 02:05:24,224 main.py:57] epoch 2348, training loss: 7944.65, average training loss: 7848.55, base loss: 16088.14
[INFO 2017-06-29 02:05:27,282 main.py:57] epoch 2349, training loss: 7928.12, average training loss: 7849.12, base loss: 16088.53
[INFO 2017-06-29 02:05:30,366 main.py:57] epoch 2350, training loss: 9526.95, average training loss: 7851.41, base loss: 16090.87
[INFO 2017-06-29 02:05:33,442 main.py:57] epoch 2351, training loss: 7884.84, average training loss: 7851.88, base loss: 16091.69
[INFO 2017-06-29 02:05:36,500 main.py:57] epoch 2352, training loss: 7386.79, average training loss: 7851.00, base loss: 16091.03
[INFO 2017-06-29 02:05:39,552 main.py:57] epoch 2353, training loss: 7847.56, average training loss: 7850.80, base loss: 16091.04
[INFO 2017-06-29 02:05:42,688 main.py:57] epoch 2354, training loss: 7383.32, average training loss: 7851.63, base loss: 16090.31
[INFO 2017-06-29 02:05:45,706 main.py:57] epoch 2355, training loss: 7935.45, average training loss: 7851.83, base loss: 16091.25
[INFO 2017-06-29 02:05:48,718 main.py:57] epoch 2356, training loss: 7281.59, average training loss: 7850.26, base loss: 16090.95
[INFO 2017-06-29 02:05:51,774 main.py:57] epoch 2357, training loss: 8056.13, average training loss: 7850.79, base loss: 16091.80
[INFO 2017-06-29 02:05:54,926 main.py:57] epoch 2358, training loss: 8682.95, average training loss: 7852.42, base loss: 16092.41
[INFO 2017-06-29 02:05:58,024 main.py:57] epoch 2359, training loss: 8785.70, average training loss: 7853.79, base loss: 16093.47
[INFO 2017-06-29 02:06:01,184 main.py:57] epoch 2360, training loss: 7938.88, average training loss: 7853.96, base loss: 16093.79
[INFO 2017-06-29 02:06:04,317 main.py:57] epoch 2361, training loss: 7480.96, average training loss: 7854.12, base loss: 16094.16
[INFO 2017-06-29 02:06:07,421 main.py:57] epoch 2362, training loss: 8100.81, average training loss: 7855.03, base loss: 16094.87
[INFO 2017-06-29 02:06:10,482 main.py:57] epoch 2363, training loss: 7587.03, average training loss: 7855.03, base loss: 16093.17
[INFO 2017-06-29 02:06:13,563 main.py:57] epoch 2364, training loss: 7989.19, average training loss: 7855.49, base loss: 16094.10
[INFO 2017-06-29 02:06:16,625 main.py:57] epoch 2365, training loss: 7528.07, average training loss: 7855.73, base loss: 16093.83
[INFO 2017-06-29 02:06:19,671 main.py:57] epoch 2366, training loss: 7477.38, average training loss: 7853.80, base loss: 16092.80
[INFO 2017-06-29 02:06:22,771 main.py:57] epoch 2367, training loss: 7641.82, average training loss: 7852.79, base loss: 16092.18
[INFO 2017-06-29 02:06:25,808 main.py:57] epoch 2368, training loss: 6830.75, average training loss: 7850.44, base loss: 16091.17
[INFO 2017-06-29 02:06:28,896 main.py:57] epoch 2369, training loss: 7551.78, average training loss: 7850.27, base loss: 16090.36
[INFO 2017-06-29 02:06:31,894 main.py:57] epoch 2370, training loss: 6723.97, average training loss: 7849.57, base loss: 16089.06
[INFO 2017-06-29 02:06:34,963 main.py:57] epoch 2371, training loss: 7365.20, average training loss: 7848.78, base loss: 16088.01
[INFO 2017-06-29 02:06:38,015 main.py:57] epoch 2372, training loss: 8654.33, average training loss: 7849.48, base loss: 16089.65
[INFO 2017-06-29 02:06:41,115 main.py:57] epoch 2373, training loss: 7556.88, average training loss: 7848.39, base loss: 16090.07
[INFO 2017-06-29 02:06:44,234 main.py:57] epoch 2374, training loss: 7941.02, average training loss: 7848.69, base loss: 16090.23
[INFO 2017-06-29 02:06:47,282 main.py:57] epoch 2375, training loss: 7731.50, average training loss: 7849.12, base loss: 16090.62
[INFO 2017-06-29 02:06:50,370 main.py:57] epoch 2376, training loss: 6742.41, average training loss: 7848.25, base loss: 16088.81
[INFO 2017-06-29 02:06:53,393 main.py:57] epoch 2377, training loss: 7428.36, average training loss: 7847.64, base loss: 16088.77
[INFO 2017-06-29 02:06:56,451 main.py:57] epoch 2378, training loss: 8079.35, average training loss: 7848.24, base loss: 16088.39
[INFO 2017-06-29 02:06:59,498 main.py:57] epoch 2379, training loss: 8631.46, average training loss: 7848.78, base loss: 16090.17
[INFO 2017-06-29 02:07:02,567 main.py:57] epoch 2380, training loss: 8360.02, average training loss: 7849.02, base loss: 16091.37
[INFO 2017-06-29 02:07:05,596 main.py:57] epoch 2381, training loss: 7683.25, average training loss: 7848.25, base loss: 16091.24
[INFO 2017-06-29 02:07:08,649 main.py:57] epoch 2382, training loss: 7202.40, average training loss: 7847.11, base loss: 16090.77
[INFO 2017-06-29 02:07:11,689 main.py:57] epoch 2383, training loss: 7864.48, average training loss: 7847.01, base loss: 16090.22
[INFO 2017-06-29 02:07:14,728 main.py:57] epoch 2384, training loss: 8041.04, average training loss: 7847.71, base loss: 16090.41
[INFO 2017-06-29 02:07:17,815 main.py:57] epoch 2385, training loss: 6564.77, average training loss: 7846.34, base loss: 16088.46
[INFO 2017-06-29 02:07:20,852 main.py:57] epoch 2386, training loss: 7558.59, average training loss: 7847.11, base loss: 16088.71
[INFO 2017-06-29 02:07:23,995 main.py:57] epoch 2387, training loss: 6733.86, average training loss: 7846.71, base loss: 16087.71
[INFO 2017-06-29 02:07:27,098 main.py:57] epoch 2388, training loss: 7839.07, average training loss: 7847.18, base loss: 16087.11
[INFO 2017-06-29 02:07:30,145 main.py:57] epoch 2389, training loss: 7712.16, average training loss: 7846.96, base loss: 16086.70
[INFO 2017-06-29 02:07:33,204 main.py:57] epoch 2390, training loss: 6914.34, average training loss: 7846.51, base loss: 16084.58
[INFO 2017-06-29 02:07:36,257 main.py:57] epoch 2391, training loss: 8796.66, average training loss: 7847.36, base loss: 16084.91
[INFO 2017-06-29 02:07:39,280 main.py:57] epoch 2392, training loss: 6971.56, average training loss: 7846.49, base loss: 16083.46
[INFO 2017-06-29 02:07:42,354 main.py:57] epoch 2393, training loss: 7020.25, average training loss: 7844.86, base loss: 16081.60
[INFO 2017-06-29 02:07:45,354 main.py:57] epoch 2394, training loss: 7851.11, average training loss: 7845.46, base loss: 16081.41
[INFO 2017-06-29 02:07:48,384 main.py:57] epoch 2395, training loss: 8026.60, average training loss: 7845.38, base loss: 16081.35
[INFO 2017-06-29 02:07:51,396 main.py:57] epoch 2396, training loss: 7128.54, average training loss: 7843.72, base loss: 16080.72
[INFO 2017-06-29 02:07:54,493 main.py:57] epoch 2397, training loss: 8471.77, average training loss: 7844.36, base loss: 16081.79
[INFO 2017-06-29 02:07:57,473 main.py:57] epoch 2398, training loss: 8551.97, average training loss: 7844.21, base loss: 16084.19
[INFO 2017-06-29 02:08:00,528 main.py:57] epoch 2399, training loss: 8167.92, average training loss: 7844.08, base loss: 16085.45
[INFO 2017-06-29 02:08:00,529 main.py:59] epoch 2399, testing
[INFO 2017-06-29 02:08:13,178 main.py:104] average testing loss: 9071.03, base loss: 17980.31
[INFO 2017-06-29 02:08:13,178 main.py:105] improve_loss: 8909.29, improve_percent: 0.50
[INFO 2017-06-29 02:08:13,180 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:08:16,218 main.py:57] epoch 2400, training loss: 7962.98, average training loss: 7843.68, base loss: 16085.75
[INFO 2017-06-29 02:08:19,360 main.py:57] epoch 2401, training loss: 6640.43, average training loss: 7843.16, base loss: 16084.96
[INFO 2017-06-29 02:08:22,396 main.py:57] epoch 2402, training loss: 7266.62, average training loss: 7842.91, base loss: 16083.58
[INFO 2017-06-29 02:08:25,453 main.py:57] epoch 2403, training loss: 8871.13, average training loss: 7844.35, base loss: 16084.36
[INFO 2017-06-29 02:08:28,494 main.py:57] epoch 2404, training loss: 8173.44, average training loss: 7844.63, base loss: 16085.53
[INFO 2017-06-29 02:08:31,602 main.py:57] epoch 2405, training loss: 7641.94, average training loss: 7843.62, base loss: 16085.52
[INFO 2017-06-29 02:08:34,668 main.py:57] epoch 2406, training loss: 7288.76, average training loss: 7844.08, base loss: 16085.76
[INFO 2017-06-29 02:08:37,755 main.py:57] epoch 2407, training loss: 7658.82, average training loss: 7843.52, base loss: 16086.05
[INFO 2017-06-29 02:08:40,840 main.py:57] epoch 2408, training loss: 7716.16, average training loss: 7842.80, base loss: 16086.05
[INFO 2017-06-29 02:08:43,929 main.py:57] epoch 2409, training loss: 7691.98, average training loss: 7842.39, base loss: 16085.38
[INFO 2017-06-29 02:08:46,929 main.py:57] epoch 2410, training loss: 7629.74, average training loss: 7843.04, base loss: 16084.64
[INFO 2017-06-29 02:08:50,011 main.py:57] epoch 2411, training loss: 6663.13, average training loss: 7841.14, base loss: 16082.70
[INFO 2017-06-29 02:08:53,063 main.py:57] epoch 2412, training loss: 8169.41, average training loss: 7842.13, base loss: 16084.42
[INFO 2017-06-29 02:08:56,082 main.py:57] epoch 2413, training loss: 7322.67, average training loss: 7841.11, base loss: 16084.18
[INFO 2017-06-29 02:08:59,120 main.py:57] epoch 2414, training loss: 7472.10, average training loss: 7841.73, base loss: 16083.20
[INFO 2017-06-29 02:09:02,201 main.py:57] epoch 2415, training loss: 7007.94, average training loss: 7841.97, base loss: 16080.66
[INFO 2017-06-29 02:09:05,245 main.py:57] epoch 2416, training loss: 8253.40, average training loss: 7842.51, base loss: 16080.80
[INFO 2017-06-29 02:09:08,349 main.py:57] epoch 2417, training loss: 6745.63, average training loss: 7839.84, base loss: 16078.79
[INFO 2017-06-29 02:09:11,452 main.py:57] epoch 2418, training loss: 8311.54, average training loss: 7841.11, base loss: 16080.04
[INFO 2017-06-29 02:09:14,494 main.py:57] epoch 2419, training loss: 7658.76, average training loss: 7841.12, base loss: 16080.06
[INFO 2017-06-29 02:09:17,540 main.py:57] epoch 2420, training loss: 7778.71, average training loss: 7839.97, base loss: 16080.00
[INFO 2017-06-29 02:09:20,647 main.py:57] epoch 2421, training loss: 6562.80, average training loss: 7838.98, base loss: 16078.89
[INFO 2017-06-29 02:09:23,667 main.py:57] epoch 2422, training loss: 8633.49, average training loss: 7839.79, base loss: 16080.44
[INFO 2017-06-29 02:09:26,834 main.py:57] epoch 2423, training loss: 7412.55, average training loss: 7839.30, base loss: 16080.37
[INFO 2017-06-29 02:09:29,881 main.py:57] epoch 2424, training loss: 7315.31, average training loss: 7837.40, base loss: 16079.65
[INFO 2017-06-29 02:09:32,935 main.py:57] epoch 2425, training loss: 7211.73, average training loss: 7835.93, base loss: 16080.57
[INFO 2017-06-29 02:09:36,067 main.py:57] epoch 2426, training loss: 7697.06, average training loss: 7836.26, base loss: 16080.73
[INFO 2017-06-29 02:09:39,103 main.py:57] epoch 2427, training loss: 7714.34, average training loss: 7836.09, base loss: 16080.89
[INFO 2017-06-29 02:09:42,149 main.py:57] epoch 2428, training loss: 7655.62, average training loss: 7835.78, base loss: 16081.39
[INFO 2017-06-29 02:09:45,202 main.py:57] epoch 2429, training loss: 8479.21, average training loss: 7835.35, base loss: 16082.94
[INFO 2017-06-29 02:09:48,270 main.py:57] epoch 2430, training loss: 7054.72, average training loss: 7834.80, base loss: 16080.88
[INFO 2017-06-29 02:09:51,304 main.py:57] epoch 2431, training loss: 7607.36, average training loss: 7833.12, base loss: 16080.07
[INFO 2017-06-29 02:09:54,377 main.py:57] epoch 2432, training loss: 7474.73, average training loss: 7832.53, base loss: 16079.22
[INFO 2017-06-29 02:09:57,394 main.py:57] epoch 2433, training loss: 7293.77, average training loss: 7831.75, base loss: 16077.92
[INFO 2017-06-29 02:10:00,391 main.py:57] epoch 2434, training loss: 9082.19, average training loss: 7830.36, base loss: 16078.82
[INFO 2017-06-29 02:10:03,378 main.py:57] epoch 2435, training loss: 7706.94, average training loss: 7830.59, base loss: 16079.99
[INFO 2017-06-29 02:10:06,432 main.py:57] epoch 2436, training loss: 6900.23, average training loss: 7829.75, base loss: 16080.14
[INFO 2017-06-29 02:10:09,501 main.py:57] epoch 2437, training loss: 7267.78, average training loss: 7829.25, base loss: 16080.31
[INFO 2017-06-29 02:10:12,660 main.py:57] epoch 2438, training loss: 7495.17, average training loss: 7828.52, base loss: 16079.78
[INFO 2017-06-29 02:10:15,686 main.py:57] epoch 2439, training loss: 7781.47, average training loss: 7828.22, base loss: 16080.08
[INFO 2017-06-29 02:10:18,765 main.py:57] epoch 2440, training loss: 7395.52, average training loss: 7827.92, base loss: 16079.41
[INFO 2017-06-29 02:10:21,823 main.py:57] epoch 2441, training loss: 8216.79, average training loss: 7828.14, base loss: 16079.68
[INFO 2017-06-29 02:10:24,886 main.py:57] epoch 2442, training loss: 8493.45, average training loss: 7829.07, base loss: 16079.37
[INFO 2017-06-29 02:10:27,973 main.py:57] epoch 2443, training loss: 7630.55, average training loss: 7828.65, base loss: 16079.07
[INFO 2017-06-29 02:10:31,092 main.py:57] epoch 2444, training loss: 8075.62, average training loss: 7828.72, base loss: 16079.57
[INFO 2017-06-29 02:10:34,130 main.py:57] epoch 2445, training loss: 8350.45, average training loss: 7828.66, base loss: 16080.79
[INFO 2017-06-29 02:10:37,150 main.py:57] epoch 2446, training loss: 8120.19, average training loss: 7829.11, base loss: 16080.72
[INFO 2017-06-29 02:10:40,174 main.py:57] epoch 2447, training loss: 7744.03, average training loss: 7828.44, base loss: 16080.05
[INFO 2017-06-29 02:10:43,215 main.py:57] epoch 2448, training loss: 7705.71, average training loss: 7827.98, base loss: 16079.35
[INFO 2017-06-29 02:10:46,288 main.py:57] epoch 2449, training loss: 7864.79, average training loss: 7826.58, base loss: 16079.98
[INFO 2017-06-29 02:10:49,404 main.py:57] epoch 2450, training loss: 6720.80, average training loss: 7825.50, base loss: 16079.11
[INFO 2017-06-29 02:10:52,463 main.py:57] epoch 2451, training loss: 6774.50, average training loss: 7824.62, base loss: 16078.37
[INFO 2017-06-29 02:10:55,515 main.py:57] epoch 2452, training loss: 7316.01, average training loss: 7824.45, base loss: 16078.01
[INFO 2017-06-29 02:10:58,593 main.py:57] epoch 2453, training loss: 7234.04, average training loss: 7823.47, base loss: 16076.96
[INFO 2017-06-29 02:11:01,619 main.py:57] epoch 2454, training loss: 7426.14, average training loss: 7823.46, base loss: 16076.96
[INFO 2017-06-29 02:11:04,679 main.py:57] epoch 2455, training loss: 8266.27, average training loss: 7824.87, base loss: 16078.39
[INFO 2017-06-29 02:11:07,759 main.py:57] epoch 2456, training loss: 8800.24, average training loss: 7824.85, base loss: 16078.49
[INFO 2017-06-29 02:11:10,812 main.py:57] epoch 2457, training loss: 7927.40, average training loss: 7824.96, base loss: 16078.21
[INFO 2017-06-29 02:11:13,849 main.py:57] epoch 2458, training loss: 8026.01, average training loss: 7824.68, base loss: 16078.52
[INFO 2017-06-29 02:11:16,930 main.py:57] epoch 2459, training loss: 8004.52, average training loss: 7825.61, base loss: 16078.53
[INFO 2017-06-29 02:11:19,970 main.py:57] epoch 2460, training loss: 7422.72, average training loss: 7826.40, base loss: 16077.12
[INFO 2017-06-29 02:11:23,117 main.py:57] epoch 2461, training loss: 7132.33, average training loss: 7825.45, base loss: 16075.33
[INFO 2017-06-29 02:11:26,266 main.py:57] epoch 2462, training loss: 7224.79, average training loss: 7822.90, base loss: 16075.26
[INFO 2017-06-29 02:11:29,284 main.py:57] epoch 2463, training loss: 8981.10, average training loss: 7823.32, base loss: 16076.69
[INFO 2017-06-29 02:11:32,386 main.py:57] epoch 2464, training loss: 7356.55, average training loss: 7822.56, base loss: 16077.15
[INFO 2017-06-29 02:11:35,456 main.py:57] epoch 2465, training loss: 6472.99, average training loss: 7821.50, base loss: 16075.97
[INFO 2017-06-29 02:11:38,539 main.py:57] epoch 2466, training loss: 7835.87, average training loss: 7820.94, base loss: 16075.09
[INFO 2017-06-29 02:11:41,567 main.py:57] epoch 2467, training loss: 6776.93, average training loss: 7819.77, base loss: 16072.19
[INFO 2017-06-29 02:11:44,589 main.py:57] epoch 2468, training loss: 8042.19, average training loss: 7819.89, base loss: 16073.10
[INFO 2017-06-29 02:11:47,620 main.py:57] epoch 2469, training loss: 7399.13, average training loss: 7820.61, base loss: 16072.22
[INFO 2017-06-29 02:11:50,641 main.py:57] epoch 2470, training loss: 7354.00, average training loss: 7819.42, base loss: 16071.32
[INFO 2017-06-29 02:11:53,695 main.py:57] epoch 2471, training loss: 6666.54, average training loss: 7818.81, base loss: 16070.98
[INFO 2017-06-29 02:11:56,711 main.py:57] epoch 2472, training loss: 6871.07, average training loss: 7819.12, base loss: 16069.05
[INFO 2017-06-29 02:11:59,752 main.py:57] epoch 2473, training loss: 8163.67, average training loss: 7819.70, base loss: 16069.23
[INFO 2017-06-29 02:12:02,865 main.py:57] epoch 2474, training loss: 7946.54, average training loss: 7819.69, base loss: 16068.35
[INFO 2017-06-29 02:12:05,942 main.py:57] epoch 2475, training loss: 6473.19, average training loss: 7818.19, base loss: 16066.15
[INFO 2017-06-29 02:12:08,946 main.py:57] epoch 2476, training loss: 8086.72, average training loss: 7818.47, base loss: 16065.46
[INFO 2017-06-29 02:12:11,966 main.py:57] epoch 2477, training loss: 8266.51, average training loss: 7817.80, base loss: 16065.51
[INFO 2017-06-29 02:12:15,033 main.py:57] epoch 2478, training loss: 8051.46, average training loss: 7818.52, base loss: 16065.40
[INFO 2017-06-29 02:12:18,123 main.py:57] epoch 2479, training loss: 9329.80, average training loss: 7819.86, base loss: 16066.15
[INFO 2017-06-29 02:12:21,217 main.py:57] epoch 2480, training loss: 7924.75, average training loss: 7820.09, base loss: 16066.17
[INFO 2017-06-29 02:12:24,323 main.py:57] epoch 2481, training loss: 8333.78, average training loss: 7821.59, base loss: 16067.51
[INFO 2017-06-29 02:12:27,384 main.py:57] epoch 2482, training loss: 8603.83, average training loss: 7822.41, base loss: 16068.28
[INFO 2017-06-29 02:12:30,490 main.py:57] epoch 2483, training loss: 7189.94, average training loss: 7821.73, base loss: 16066.74
[INFO 2017-06-29 02:12:33,555 main.py:57] epoch 2484, training loss: 7550.78, average training loss: 7822.42, base loss: 16065.38
[INFO 2017-06-29 02:12:36,659 main.py:57] epoch 2485, training loss: 6847.54, average training loss: 7820.86, base loss: 16064.28
[INFO 2017-06-29 02:12:39,778 main.py:57] epoch 2486, training loss: 6958.30, average training loss: 7820.76, base loss: 16063.32
[INFO 2017-06-29 02:12:42,861 main.py:57] epoch 2487, training loss: 8201.60, average training loss: 7821.39, base loss: 16063.71
[INFO 2017-06-29 02:12:45,927 main.py:57] epoch 2488, training loss: 7784.53, average training loss: 7821.21, base loss: 16062.89
[INFO 2017-06-29 02:12:49,006 main.py:57] epoch 2489, training loss: 6660.15, average training loss: 7819.53, base loss: 16061.38
[INFO 2017-06-29 02:12:52,064 main.py:57] epoch 2490, training loss: 6927.14, average training loss: 7818.38, base loss: 16060.36
[INFO 2017-06-29 02:12:55,130 main.py:57] epoch 2491, training loss: 6864.81, average training loss: 7817.56, base loss: 16059.28
[INFO 2017-06-29 02:12:58,237 main.py:57] epoch 2492, training loss: 7290.09, average training loss: 7817.74, base loss: 16059.26
[INFO 2017-06-29 02:13:01,247 main.py:57] epoch 2493, training loss: 7426.60, average training loss: 7815.53, base loss: 16058.71
[INFO 2017-06-29 02:13:04,284 main.py:57] epoch 2494, training loss: 7403.60, average training loss: 7815.60, base loss: 16059.11
[INFO 2017-06-29 02:13:07,302 main.py:57] epoch 2495, training loss: 7950.40, average training loss: 7815.20, base loss: 16059.09
[INFO 2017-06-29 02:13:10,348 main.py:57] epoch 2496, training loss: 7216.68, average training loss: 7814.83, base loss: 16057.48
[INFO 2017-06-29 02:13:13,343 main.py:57] epoch 2497, training loss: 7276.58, average training loss: 7814.00, base loss: 16055.77
[INFO 2017-06-29 02:13:16,353 main.py:57] epoch 2498, training loss: 7507.74, average training loss: 7812.93, base loss: 16055.58
[INFO 2017-06-29 02:13:19,369 main.py:57] epoch 2499, training loss: 7940.93, average training loss: 7811.52, base loss: 16056.48
[INFO 2017-06-29 02:13:19,370 main.py:59] epoch 2499, testing
[INFO 2017-06-29 02:13:32,053 main.py:104] average testing loss: 8053.34, base loss: 16220.16
[INFO 2017-06-29 02:13:32,053 main.py:105] improve_loss: 8166.82, improve_percent: 0.50
[INFO 2017-06-29 02:13:32,055 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:13:35,078 main.py:57] epoch 2500, training loss: 8032.70, average training loss: 7811.09, base loss: 16056.86
[INFO 2017-06-29 02:13:38,165 main.py:57] epoch 2501, training loss: 8722.56, average training loss: 7812.37, base loss: 16057.25
[INFO 2017-06-29 02:13:41,214 main.py:57] epoch 2502, training loss: 7780.70, average training loss: 7812.88, base loss: 16057.68
[INFO 2017-06-29 02:13:44,247 main.py:57] epoch 2503, training loss: 6960.07, average training loss: 7811.36, base loss: 16056.53
[INFO 2017-06-29 02:13:47,285 main.py:57] epoch 2504, training loss: 8096.96, average training loss: 7811.44, base loss: 16057.53
[INFO 2017-06-29 02:13:50,340 main.py:57] epoch 2505, training loss: 8777.83, average training loss: 7812.82, base loss: 16058.21
[INFO 2017-06-29 02:13:53,371 main.py:57] epoch 2506, training loss: 7316.61, average training loss: 7810.95, base loss: 16057.98
[INFO 2017-06-29 02:13:56,444 main.py:57] epoch 2507, training loss: 7652.61, average training loss: 7809.73, base loss: 16058.82
[INFO 2017-06-29 02:13:59,506 main.py:57] epoch 2508, training loss: 8457.03, average training loss: 7810.00, base loss: 16059.55
[INFO 2017-06-29 02:14:02,565 main.py:57] epoch 2509, training loss: 7760.73, average training loss: 7810.39, base loss: 16058.27
[INFO 2017-06-29 02:14:05,611 main.py:57] epoch 2510, training loss: 7415.56, average training loss: 7809.78, base loss: 16058.17
[INFO 2017-06-29 02:14:08,702 main.py:57] epoch 2511, training loss: 7971.22, average training loss: 7810.16, base loss: 16058.91
[INFO 2017-06-29 02:14:11,752 main.py:57] epoch 2512, training loss: 7754.96, average training loss: 7808.64, base loss: 16059.04
[INFO 2017-06-29 02:14:14,812 main.py:57] epoch 2513, training loss: 7603.31, average training loss: 7809.12, base loss: 16058.84
[INFO 2017-06-29 02:14:17,935 main.py:57] epoch 2514, training loss: 7702.48, average training loss: 7808.53, base loss: 16059.07
[INFO 2017-06-29 02:14:20,992 main.py:57] epoch 2515, training loss: 6767.09, average training loss: 7807.08, base loss: 16058.96
[INFO 2017-06-29 02:14:24,096 main.py:57] epoch 2516, training loss: 7421.46, average training loss: 7806.40, base loss: 16057.96
[INFO 2017-06-29 02:14:27,193 main.py:57] epoch 2517, training loss: 7048.11, average training loss: 7804.34, base loss: 16058.39
[INFO 2017-06-29 02:14:30,236 main.py:57] epoch 2518, training loss: 7715.29, average training loss: 7804.03, base loss: 16059.00
[INFO 2017-06-29 02:14:33,346 main.py:57] epoch 2519, training loss: 8001.81, average training loss: 7803.92, base loss: 16060.21
[INFO 2017-06-29 02:14:36,444 main.py:57] epoch 2520, training loss: 7390.40, average training loss: 7802.73, base loss: 16060.34
[INFO 2017-06-29 02:14:39,555 main.py:57] epoch 2521, training loss: 7654.67, average training loss: 7802.94, base loss: 16061.13
[INFO 2017-06-29 02:14:42,664 main.py:57] epoch 2522, training loss: 8186.13, average training loss: 7803.93, base loss: 16061.62
[INFO 2017-06-29 02:14:45,775 main.py:57] epoch 2523, training loss: 7914.40, average training loss: 7803.49, base loss: 16061.65
[INFO 2017-06-29 02:14:48,810 main.py:57] epoch 2524, training loss: 7156.34, average training loss: 7803.76, base loss: 16060.60
[INFO 2017-06-29 02:14:51,841 main.py:57] epoch 2525, training loss: 7524.48, average training loss: 7803.43, base loss: 16060.09
[INFO 2017-06-29 02:14:54,913 main.py:57] epoch 2526, training loss: 7649.01, average training loss: 7802.75, base loss: 16059.38
[INFO 2017-06-29 02:14:57,967 main.py:57] epoch 2527, training loss: 8256.64, average training loss: 7802.77, base loss: 16059.33
[INFO 2017-06-29 02:15:01,147 main.py:57] epoch 2528, training loss: 8156.08, average training loss: 7803.80, base loss: 16059.51
[INFO 2017-06-29 02:15:04,223 main.py:57] epoch 2529, training loss: 7370.18, average training loss: 7803.69, base loss: 16058.07
[INFO 2017-06-29 02:15:07,278 main.py:57] epoch 2530, training loss: 8102.18, average training loss: 7803.08, base loss: 16058.93
[INFO 2017-06-29 02:15:10,336 main.py:57] epoch 2531, training loss: 7529.24, average training loss: 7802.93, base loss: 16058.24
[INFO 2017-06-29 02:15:13,378 main.py:57] epoch 2532, training loss: 7148.43, average training loss: 7801.83, base loss: 16057.20
[INFO 2017-06-29 02:15:16,422 main.py:57] epoch 2533, training loss: 6985.53, average training loss: 7801.08, base loss: 16057.51
[INFO 2017-06-29 02:15:19,508 main.py:57] epoch 2534, training loss: 8195.86, average training loss: 7801.77, base loss: 16057.72
[INFO 2017-06-29 02:15:22,552 main.py:57] epoch 2535, training loss: 7931.33, average training loss: 7802.19, base loss: 16057.73
[INFO 2017-06-29 02:15:25,617 main.py:57] epoch 2536, training loss: 7797.83, average training loss: 7800.95, base loss: 16056.84
[INFO 2017-06-29 02:15:28,706 main.py:57] epoch 2537, training loss: 8064.11, average training loss: 7800.32, base loss: 16058.34
[INFO 2017-06-29 02:15:31,754 main.py:57] epoch 2538, training loss: 7494.75, average training loss: 7799.71, base loss: 16057.06
[INFO 2017-06-29 02:15:34,893 main.py:57] epoch 2539, training loss: 8917.40, average training loss: 7799.97, base loss: 16058.34
[INFO 2017-06-29 02:15:37,912 main.py:57] epoch 2540, training loss: 7679.46, average training loss: 7799.95, base loss: 16059.01
[INFO 2017-06-29 02:15:40,994 main.py:57] epoch 2541, training loss: 7527.83, average training loss: 7800.13, base loss: 16060.23
[INFO 2017-06-29 02:15:44,040 main.py:57] epoch 2542, training loss: 7911.70, average training loss: 7799.45, base loss: 16060.78
[INFO 2017-06-29 02:15:47,029 main.py:57] epoch 2543, training loss: 7137.62, average training loss: 7798.76, base loss: 16059.08
[INFO 2017-06-29 02:15:50,086 main.py:57] epoch 2544, training loss: 7274.95, average training loss: 7799.42, base loss: 16059.33
[INFO 2017-06-29 02:15:53,165 main.py:57] epoch 2545, training loss: 8104.41, average training loss: 7800.46, base loss: 16058.98
[INFO 2017-06-29 02:15:56,283 main.py:57] epoch 2546, training loss: 7062.26, average training loss: 7799.70, base loss: 16057.38
[INFO 2017-06-29 02:15:59,467 main.py:57] epoch 2547, training loss: 8528.38, average training loss: 7800.86, base loss: 16057.13
[INFO 2017-06-29 02:16:02,558 main.py:57] epoch 2548, training loss: 7052.38, average training loss: 7800.89, base loss: 16056.46
[INFO 2017-06-29 02:16:05,699 main.py:57] epoch 2549, training loss: 9593.98, average training loss: 7802.26, base loss: 16058.11
[INFO 2017-06-29 02:16:08,837 main.py:57] epoch 2550, training loss: 8002.33, average training loss: 7800.68, base loss: 16058.97
[INFO 2017-06-29 02:16:11,943 main.py:57] epoch 2551, training loss: 8019.45, average training loss: 7800.93, base loss: 16060.39
[INFO 2017-06-29 02:16:14,984 main.py:57] epoch 2552, training loss: 7304.23, average training loss: 7800.43, base loss: 16059.32
[INFO 2017-06-29 02:16:18,036 main.py:57] epoch 2553, training loss: 7051.97, average training loss: 7800.02, base loss: 16057.44
[INFO 2017-06-29 02:16:21,065 main.py:57] epoch 2554, training loss: 7880.29, average training loss: 7799.84, base loss: 16057.24
[INFO 2017-06-29 02:16:24,159 main.py:57] epoch 2555, training loss: 6751.87, average training loss: 7799.48, base loss: 16055.99
[INFO 2017-06-29 02:16:27,292 main.py:57] epoch 2556, training loss: 7617.54, average training loss: 7798.81, base loss: 16055.74
[INFO 2017-06-29 02:16:30,465 main.py:57] epoch 2557, training loss: 7390.75, average training loss: 7798.26, base loss: 16055.65
[INFO 2017-06-29 02:16:33,558 main.py:57] epoch 2558, training loss: 6993.83, average training loss: 7796.86, base loss: 16056.07
[INFO 2017-06-29 02:16:36,579 main.py:57] epoch 2559, training loss: 7740.83, average training loss: 7797.44, base loss: 16057.03
[INFO 2017-06-29 02:16:39,578 main.py:57] epoch 2560, training loss: 7683.96, average training loss: 7796.14, base loss: 16056.65
[INFO 2017-06-29 02:16:42,655 main.py:57] epoch 2561, training loss: 8526.70, average training loss: 7797.15, base loss: 16057.23
[INFO 2017-06-29 02:16:45,659 main.py:57] epoch 2562, training loss: 7730.50, average training loss: 7797.68, base loss: 16057.81
[INFO 2017-06-29 02:16:48,729 main.py:57] epoch 2563, training loss: 7831.30, average training loss: 7798.42, base loss: 16059.19
[INFO 2017-06-29 02:16:51,811 main.py:57] epoch 2564, training loss: 7510.97, average training loss: 7798.22, base loss: 16059.41
[INFO 2017-06-29 02:16:54,891 main.py:57] epoch 2565, training loss: 7415.99, average training loss: 7796.79, base loss: 16059.28
[INFO 2017-06-29 02:16:57,889 main.py:57] epoch 2566, training loss: 7923.52, average training loss: 7796.94, base loss: 16059.24
[INFO 2017-06-29 02:17:00,897 main.py:57] epoch 2567, training loss: 7505.98, average training loss: 7797.00, base loss: 16058.62
[INFO 2017-06-29 02:17:03,970 main.py:57] epoch 2568, training loss: 8103.15, average training loss: 7797.62, base loss: 16059.48
[INFO 2017-06-29 02:17:07,010 main.py:57] epoch 2569, training loss: 7233.97, average training loss: 7796.85, base loss: 16058.35
[INFO 2017-06-29 02:17:10,098 main.py:57] epoch 2570, training loss: 7257.82, average training loss: 7795.75, base loss: 16058.05
[INFO 2017-06-29 02:17:13,174 main.py:57] epoch 2571, training loss: 7101.52, average training loss: 7793.97, base loss: 16057.51
[INFO 2017-06-29 02:17:16,214 main.py:57] epoch 2572, training loss: 8176.37, average training loss: 7793.86, base loss: 16057.76
[INFO 2017-06-29 02:17:19,244 main.py:57] epoch 2573, training loss: 7966.36, average training loss: 7793.39, base loss: 16058.08
[INFO 2017-06-29 02:17:22,283 main.py:57] epoch 2574, training loss: 7772.60, average training loss: 7793.23, base loss: 16056.57
[INFO 2017-06-29 02:17:25,326 main.py:57] epoch 2575, training loss: 6517.21, average training loss: 7791.71, base loss: 16053.83
[INFO 2017-06-29 02:17:28,354 main.py:57] epoch 2576, training loss: 7850.03, average training loss: 7792.15, base loss: 16054.61
[INFO 2017-06-29 02:17:31,368 main.py:57] epoch 2577, training loss: 7795.20, average training loss: 7791.90, base loss: 16054.96
[INFO 2017-06-29 02:17:34,382 main.py:57] epoch 2578, training loss: 7899.58, average training loss: 7792.13, base loss: 16054.53
[INFO 2017-06-29 02:17:37,414 main.py:57] epoch 2579, training loss: 7725.53, average training loss: 7792.24, base loss: 16054.17
[INFO 2017-06-29 02:17:40,550 main.py:57] epoch 2580, training loss: 7675.90, average training loss: 7792.50, base loss: 16054.12
[INFO 2017-06-29 02:17:43,626 main.py:57] epoch 2581, training loss: 8538.23, average training loss: 7792.71, base loss: 16054.74
[INFO 2017-06-29 02:17:46,640 main.py:57] epoch 2582, training loss: 9101.17, average training loss: 7794.98, base loss: 16056.65
[INFO 2017-06-29 02:17:49,694 main.py:57] epoch 2583, training loss: 7668.80, average training loss: 7795.26, base loss: 16056.79
[INFO 2017-06-29 02:17:52,829 main.py:57] epoch 2584, training loss: 7860.04, average training loss: 7796.40, base loss: 16057.70
[INFO 2017-06-29 02:17:55,878 main.py:57] epoch 2585, training loss: 7223.30, average training loss: 7796.10, base loss: 16057.04
[INFO 2017-06-29 02:17:58,880 main.py:57] epoch 2586, training loss: 8147.10, average training loss: 7796.62, base loss: 16057.73
[INFO 2017-06-29 02:18:01,998 main.py:57] epoch 2587, training loss: 6915.29, average training loss: 7796.39, base loss: 16057.52
[INFO 2017-06-29 02:18:04,954 main.py:57] epoch 2588, training loss: 7789.07, average training loss: 7796.94, base loss: 16056.31
[INFO 2017-06-29 02:18:08,014 main.py:57] epoch 2589, training loss: 7188.25, average training loss: 7796.81, base loss: 16054.67
[INFO 2017-06-29 02:18:11,067 main.py:57] epoch 2590, training loss: 6518.95, average training loss: 7796.07, base loss: 16053.17
[INFO 2017-06-29 02:18:14,105 main.py:57] epoch 2591, training loss: 7993.53, average training loss: 7795.88, base loss: 16053.36
[INFO 2017-06-29 02:18:17,118 main.py:57] epoch 2592, training loss: 7266.92, average training loss: 7793.10, base loss: 16053.50
[INFO 2017-06-29 02:18:20,162 main.py:57] epoch 2593, training loss: 8149.69, average training loss: 7793.79, base loss: 16054.67
[INFO 2017-06-29 02:18:23,294 main.py:57] epoch 2594, training loss: 8258.87, average training loss: 7794.18, base loss: 16055.89
[INFO 2017-06-29 02:18:26,342 main.py:57] epoch 2595, training loss: 6967.98, average training loss: 7792.88, base loss: 16055.41
[INFO 2017-06-29 02:18:29,446 main.py:57] epoch 2596, training loss: 7572.18, average training loss: 7792.44, base loss: 16056.16
[INFO 2017-06-29 02:18:32,470 main.py:57] epoch 2597, training loss: 8522.77, average training loss: 7793.18, base loss: 16057.00
[INFO 2017-06-29 02:18:35,557 main.py:57] epoch 2598, training loss: 7982.80, average training loss: 7793.87, base loss: 16057.34
[INFO 2017-06-29 02:18:38,598 main.py:57] epoch 2599, training loss: 8570.65, average training loss: 7794.76, base loss: 16057.04
[INFO 2017-06-29 02:18:38,599 main.py:59] epoch 2599, testing
[INFO 2017-06-29 02:18:51,331 main.py:104] average testing loss: 8403.19, base loss: 16590.26
[INFO 2017-06-29 02:18:51,331 main.py:105] improve_loss: 8187.07, improve_percent: 0.49
[INFO 2017-06-29 02:18:51,333 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:18:54,293 main.py:57] epoch 2600, training loss: 7484.77, average training loss: 7794.95, base loss: 16055.88
[INFO 2017-06-29 02:18:57,396 main.py:57] epoch 2601, training loss: 7403.02, average training loss: 7794.84, base loss: 16055.05
[INFO 2017-06-29 02:19:00,400 main.py:57] epoch 2602, training loss: 7080.87, average training loss: 7795.33, base loss: 16053.46
[INFO 2017-06-29 02:19:03,444 main.py:57] epoch 2603, training loss: 6526.02, average training loss: 7795.01, base loss: 16051.72
[INFO 2017-06-29 02:19:06,498 main.py:57] epoch 2604, training loss: 6628.10, average training loss: 7792.94, base loss: 16051.49
[INFO 2017-06-29 02:19:09,478 main.py:57] epoch 2605, training loss: 8324.03, average training loss: 7793.50, base loss: 16051.49
[INFO 2017-06-29 02:19:12,625 main.py:57] epoch 2606, training loss: 8559.11, average training loss: 7795.51, base loss: 16053.02
[INFO 2017-06-29 02:19:15,666 main.py:57] epoch 2607, training loss: 7946.46, average training loss: 7794.26, base loss: 16053.69
[INFO 2017-06-29 02:19:18,720 main.py:57] epoch 2608, training loss: 8825.66, average training loss: 7795.80, base loss: 16054.88
[INFO 2017-06-29 02:19:21,709 main.py:57] epoch 2609, training loss: 8127.77, average training loss: 7795.22, base loss: 16056.44
[INFO 2017-06-29 02:19:24,795 main.py:57] epoch 2610, training loss: 8080.91, average training loss: 7795.85, base loss: 16056.31
[INFO 2017-06-29 02:19:27,908 main.py:57] epoch 2611, training loss: 8303.86, average training loss: 7795.77, base loss: 16056.20
[INFO 2017-06-29 02:19:30,966 main.py:57] epoch 2612, training loss: 9457.43, average training loss: 7797.32, base loss: 16057.38
[INFO 2017-06-29 02:19:34,007 main.py:57] epoch 2613, training loss: 7956.47, average training loss: 7797.29, base loss: 16057.29
[INFO 2017-06-29 02:19:37,072 main.py:57] epoch 2614, training loss: 8484.70, average training loss: 7796.92, base loss: 16056.96
[INFO 2017-06-29 02:19:40,090 main.py:57] epoch 2615, training loss: 7874.10, average training loss: 7797.78, base loss: 16056.72
[INFO 2017-06-29 02:19:43,137 main.py:57] epoch 2616, training loss: 7150.98, average training loss: 7797.39, base loss: 16055.44
[INFO 2017-06-29 02:19:46,158 main.py:57] epoch 2617, training loss: 7028.46, average training loss: 7797.12, base loss: 16053.29
[INFO 2017-06-29 02:19:49,177 main.py:57] epoch 2618, training loss: 7931.00, average training loss: 7795.69, base loss: 16053.96
[INFO 2017-06-29 02:19:52,289 main.py:57] epoch 2619, training loss: 7200.42, average training loss: 7795.11, base loss: 16053.28
[INFO 2017-06-29 02:19:55,351 main.py:57] epoch 2620, training loss: 7209.07, average training loss: 7793.76, base loss: 16051.69
[INFO 2017-06-29 02:19:58,347 main.py:57] epoch 2621, training loss: 7250.58, average training loss: 7792.45, base loss: 16049.46
[INFO 2017-06-29 02:20:01,401 main.py:57] epoch 2622, training loss: 8192.57, average training loss: 7792.52, base loss: 16050.86
[INFO 2017-06-29 02:20:04,469 main.py:57] epoch 2623, training loss: 7875.34, average training loss: 7790.97, base loss: 16050.56
[INFO 2017-06-29 02:20:07,492 main.py:57] epoch 2624, training loss: 6614.88, average training loss: 7790.14, base loss: 16050.17
[INFO 2017-06-29 02:20:10,523 main.py:57] epoch 2625, training loss: 7937.31, average training loss: 7790.25, base loss: 16051.35
[INFO 2017-06-29 02:20:13,549 main.py:57] epoch 2626, training loss: 7844.04, average training loss: 7789.27, base loss: 16051.70
[INFO 2017-06-29 02:20:16,575 main.py:57] epoch 2627, training loss: 6786.61, average training loss: 7787.65, base loss: 16050.60
[INFO 2017-06-29 02:20:19,596 main.py:57] epoch 2628, training loss: 7224.98, average training loss: 7787.24, base loss: 16050.46
[INFO 2017-06-29 02:20:22,625 main.py:57] epoch 2629, training loss: 7497.51, average training loss: 7786.95, base loss: 16050.45
[INFO 2017-06-29 02:20:25,704 main.py:57] epoch 2630, training loss: 7571.87, average training loss: 7787.32, base loss: 16050.24
[INFO 2017-06-29 02:20:28,773 main.py:57] epoch 2631, training loss: 7259.28, average training loss: 7786.79, base loss: 16050.12
[INFO 2017-06-29 02:20:31,825 main.py:57] epoch 2632, training loss: 7709.64, average training loss: 7786.98, base loss: 16049.98
[INFO 2017-06-29 02:20:34,911 main.py:57] epoch 2633, training loss: 7634.38, average training loss: 7786.89, base loss: 16049.08
[INFO 2017-06-29 02:20:37,944 main.py:57] epoch 2634, training loss: 8010.11, average training loss: 7786.24, base loss: 16047.76
[INFO 2017-06-29 02:20:41,010 main.py:57] epoch 2635, training loss: 7747.24, average training loss: 7786.16, base loss: 16046.83
[INFO 2017-06-29 02:20:44,089 main.py:57] epoch 2636, training loss: 9932.83, average training loss: 7787.59, base loss: 16049.93
[INFO 2017-06-29 02:20:47,160 main.py:57] epoch 2637, training loss: 7368.05, average training loss: 7786.89, base loss: 16049.79
[INFO 2017-06-29 02:20:50,199 main.py:57] epoch 2638, training loss: 7016.36, average training loss: 7785.67, base loss: 16049.11
[INFO 2017-06-29 02:20:53,340 main.py:57] epoch 2639, training loss: 7419.79, average training loss: 7786.21, base loss: 16049.32
[INFO 2017-06-29 02:20:56,400 main.py:57] epoch 2640, training loss: 7306.67, average training loss: 7785.68, base loss: 16048.64
[INFO 2017-06-29 02:20:59,464 main.py:57] epoch 2641, training loss: 7403.50, average training loss: 7786.09, base loss: 16049.39
[INFO 2017-06-29 02:21:02,540 main.py:57] epoch 2642, training loss: 7110.80, average training loss: 7785.51, base loss: 16048.31
[INFO 2017-06-29 02:21:05,670 main.py:57] epoch 2643, training loss: 8260.19, average training loss: 7785.75, base loss: 16048.75
[INFO 2017-06-29 02:21:08,745 main.py:57] epoch 2644, training loss: 9283.05, average training loss: 7787.40, base loss: 16050.23
[INFO 2017-06-29 02:21:11,875 main.py:57] epoch 2645, training loss: 8052.21, average training loss: 7788.06, base loss: 16050.23
[INFO 2017-06-29 02:21:14,914 main.py:57] epoch 2646, training loss: 7606.55, average training loss: 7787.38, base loss: 16049.76
[INFO 2017-06-29 02:21:17,963 main.py:57] epoch 2647, training loss: 6785.25, average training loss: 7785.92, base loss: 16047.55
[INFO 2017-06-29 02:21:20,995 main.py:57] epoch 2648, training loss: 7444.33, average training loss: 7785.07, base loss: 16047.29
[INFO 2017-06-29 02:21:24,149 main.py:57] epoch 2649, training loss: 8619.90, average training loss: 7785.45, base loss: 16048.38
[INFO 2017-06-29 02:21:27,321 main.py:57] epoch 2650, training loss: 7469.16, average training loss: 7784.52, base loss: 16048.16
[INFO 2017-06-29 02:21:30,397 main.py:57] epoch 2651, training loss: 7367.19, average training loss: 7783.61, base loss: 16047.17
[INFO 2017-06-29 02:21:33,445 main.py:57] epoch 2652, training loss: 8502.81, average training loss: 7784.08, base loss: 16047.84
[INFO 2017-06-29 02:21:36,546 main.py:57] epoch 2653, training loss: 7615.39, average training loss: 7781.71, base loss: 16047.58
[INFO 2017-06-29 02:21:39,599 main.py:57] epoch 2654, training loss: 7675.20, average training loss: 7782.25, base loss: 16047.54
[INFO 2017-06-29 02:21:42,650 main.py:57] epoch 2655, training loss: 7238.94, average training loss: 7781.02, base loss: 16046.46
[INFO 2017-06-29 02:21:45,747 main.py:57] epoch 2656, training loss: 7883.52, average training loss: 7780.76, base loss: 16046.46
[INFO 2017-06-29 02:21:48,811 main.py:57] epoch 2657, training loss: 7310.72, average training loss: 7779.16, base loss: 16046.02
[INFO 2017-06-29 02:21:51,867 main.py:57] epoch 2658, training loss: 7828.99, average training loss: 7778.47, base loss: 16045.62
[INFO 2017-06-29 02:21:54,869 main.py:57] epoch 2659, training loss: 7487.63, average training loss: 7778.47, base loss: 16044.68
[INFO 2017-06-29 02:21:57,962 main.py:57] epoch 2660, training loss: 6599.92, average training loss: 7777.82, base loss: 16042.78
[INFO 2017-06-29 02:22:00,979 main.py:57] epoch 2661, training loss: 8060.89, average training loss: 7778.42, base loss: 16042.73
[INFO 2017-06-29 02:22:04,085 main.py:57] epoch 2662, training loss: 6931.60, average training loss: 7777.09, base loss: 16041.59
[INFO 2017-06-29 02:22:07,184 main.py:57] epoch 2663, training loss: 7191.82, average training loss: 7777.18, base loss: 16040.73
[INFO 2017-06-29 02:22:10,201 main.py:57] epoch 2664, training loss: 8322.93, average training loss: 7777.56, base loss: 16040.76
[INFO 2017-06-29 02:22:13,176 main.py:57] epoch 2665, training loss: 7076.76, average training loss: 7776.93, base loss: 16040.87
[INFO 2017-06-29 02:22:16,167 main.py:57] epoch 2666, training loss: 8238.68, average training loss: 7776.01, base loss: 16040.92
[INFO 2017-06-29 02:22:19,228 main.py:57] epoch 2667, training loss: 8241.46, average training loss: 7775.94, base loss: 16041.84
[INFO 2017-06-29 02:22:22,284 main.py:57] epoch 2668, training loss: 6736.25, average training loss: 7774.91, base loss: 16040.71
[INFO 2017-06-29 02:22:25,275 main.py:57] epoch 2669, training loss: 7923.68, average training loss: 7774.99, base loss: 16041.33
[INFO 2017-06-29 02:22:28,308 main.py:57] epoch 2670, training loss: 7211.42, average training loss: 7775.13, base loss: 16041.85
[INFO 2017-06-29 02:22:31,352 main.py:57] epoch 2671, training loss: 6770.87, average training loss: 7774.73, base loss: 16040.59
[INFO 2017-06-29 02:22:34,425 main.py:57] epoch 2672, training loss: 7871.75, average training loss: 7774.84, base loss: 16041.15
[INFO 2017-06-29 02:22:37,466 main.py:57] epoch 2673, training loss: 7873.47, average training loss: 7774.81, base loss: 16041.65
[INFO 2017-06-29 02:22:40,561 main.py:57] epoch 2674, training loss: 6249.98, average training loss: 7773.06, base loss: 16040.74
[INFO 2017-06-29 02:22:43,589 main.py:57] epoch 2675, training loss: 7798.84, average training loss: 7774.02, base loss: 16041.09
[INFO 2017-06-29 02:22:46,598 main.py:57] epoch 2676, training loss: 7071.53, average training loss: 7773.38, base loss: 16041.29
[INFO 2017-06-29 02:22:49,668 main.py:57] epoch 2677, training loss: 7826.61, average training loss: 7774.07, base loss: 16041.78
[INFO 2017-06-29 02:22:52,765 main.py:57] epoch 2678, training loss: 7097.20, average training loss: 7773.50, base loss: 16041.59
[INFO 2017-06-29 02:22:55,819 main.py:57] epoch 2679, training loss: 7204.19, average training loss: 7773.37, base loss: 16041.36
[INFO 2017-06-29 02:22:58,933 main.py:57] epoch 2680, training loss: 7632.89, average training loss: 7774.00, base loss: 16041.36
[INFO 2017-06-29 02:23:02,030 main.py:57] epoch 2681, training loss: 7699.51, average training loss: 7774.15, base loss: 16040.79
[INFO 2017-06-29 02:23:05,144 main.py:57] epoch 2682, training loss: 7334.33, average training loss: 7774.51, base loss: 16041.90
[INFO 2017-06-29 02:23:08,248 main.py:57] epoch 2683, training loss: 7941.43, average training loss: 7774.76, base loss: 16042.71
[INFO 2017-06-29 02:23:11,301 main.py:57] epoch 2684, training loss: 7287.97, average training loss: 7774.09, base loss: 16042.62
[INFO 2017-06-29 02:23:14,377 main.py:57] epoch 2685, training loss: 7372.41, average training loss: 7772.41, base loss: 16042.89
[INFO 2017-06-29 02:23:17,454 main.py:57] epoch 2686, training loss: 7112.00, average training loss: 7772.86, base loss: 16041.43
[INFO 2017-06-29 02:23:20,475 main.py:57] epoch 2687, training loss: 7585.91, average training loss: 7772.55, base loss: 16039.71
[INFO 2017-06-29 02:23:23,460 main.py:57] epoch 2688, training loss: 8284.84, average training loss: 7772.88, base loss: 16039.30
[INFO 2017-06-29 02:23:26,548 main.py:57] epoch 2689, training loss: 7223.92, average training loss: 7773.40, base loss: 16037.97
[INFO 2017-06-29 02:23:29,688 main.py:57] epoch 2690, training loss: 8404.44, average training loss: 7774.01, base loss: 16038.59
[INFO 2017-06-29 02:23:32,682 main.py:57] epoch 2691, training loss: 8923.17, average training loss: 7775.25, base loss: 16040.07
[INFO 2017-06-29 02:23:35,782 main.py:57] epoch 2692, training loss: 8272.18, average training loss: 7776.41, base loss: 16040.67
[INFO 2017-06-29 02:23:38,838 main.py:57] epoch 2693, training loss: 8109.84, average training loss: 7776.19, base loss: 16040.36
[INFO 2017-06-29 02:23:41,849 main.py:57] epoch 2694, training loss: 7443.95, average training loss: 7775.01, base loss: 16040.18
[INFO 2017-06-29 02:23:44,877 main.py:57] epoch 2695, training loss: 7312.12, average training loss: 7774.87, base loss: 16040.00
[INFO 2017-06-29 02:23:47,981 main.py:57] epoch 2696, training loss: 7843.03, average training loss: 7774.36, base loss: 16040.13
[INFO 2017-06-29 02:23:51,012 main.py:57] epoch 2697, training loss: 9169.48, average training loss: 7775.56, base loss: 16042.21
[INFO 2017-06-29 02:23:54,060 main.py:57] epoch 2698, training loss: 6768.98, average training loss: 7774.99, base loss: 16040.74
[INFO 2017-06-29 02:23:57,091 main.py:57] epoch 2699, training loss: 7155.37, average training loss: 7773.77, base loss: 16040.26
[INFO 2017-06-29 02:23:57,092 main.py:59] epoch 2699, testing
[INFO 2017-06-29 02:24:09,777 main.py:104] average testing loss: 8873.21, base loss: 17719.79
[INFO 2017-06-29 02:24:09,777 main.py:105] improve_loss: 8846.58, improve_percent: 0.50
[INFO 2017-06-29 02:24:09,778 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:24:12,803 main.py:57] epoch 2700, training loss: 7646.88, average training loss: 7773.69, base loss: 16040.97
[INFO 2017-06-29 02:24:15,924 main.py:57] epoch 2701, training loss: 7331.95, average training loss: 7774.45, base loss: 16040.97
[INFO 2017-06-29 02:24:18,920 main.py:57] epoch 2702, training loss: 8561.44, average training loss: 7774.60, base loss: 16042.33
[INFO 2017-06-29 02:24:21,957 main.py:57] epoch 2703, training loss: 8331.80, average training loss: 7775.18, base loss: 16042.39
[INFO 2017-06-29 02:24:24,954 main.py:57] epoch 2704, training loss: 8230.19, average training loss: 7775.69, base loss: 16043.71
[INFO 2017-06-29 02:24:28,021 main.py:57] epoch 2705, training loss: 7323.49, average training loss: 7775.70, base loss: 16043.13
[INFO 2017-06-29 02:24:31,127 main.py:57] epoch 2706, training loss: 6513.62, average training loss: 7773.85, base loss: 16041.94
[INFO 2017-06-29 02:24:34,148 main.py:57] epoch 2707, training loss: 7799.60, average training loss: 7774.35, base loss: 16042.43
[INFO 2017-06-29 02:24:37,254 main.py:57] epoch 2708, training loss: 7850.48, average training loss: 7774.03, base loss: 16041.89
[INFO 2017-06-29 02:24:40,363 main.py:57] epoch 2709, training loss: 8138.91, average training loss: 7773.34, base loss: 16040.99
[INFO 2017-06-29 02:24:43,427 main.py:57] epoch 2710, training loss: 8106.74, average training loss: 7773.38, base loss: 16042.12
[INFO 2017-06-29 02:24:46,458 main.py:57] epoch 2711, training loss: 8257.74, average training loss: 7774.20, base loss: 16042.72
[INFO 2017-06-29 02:24:49,546 main.py:57] epoch 2712, training loss: 7376.15, average training loss: 7773.56, base loss: 16042.69
[INFO 2017-06-29 02:24:52,604 main.py:57] epoch 2713, training loss: 7721.25, average training loss: 7772.53, base loss: 16042.47
[INFO 2017-06-29 02:24:55,721 main.py:57] epoch 2714, training loss: 6978.19, average training loss: 7771.53, base loss: 16039.93
[INFO 2017-06-29 02:24:58,804 main.py:57] epoch 2715, training loss: 6997.52, average training loss: 7770.59, base loss: 16039.05
[INFO 2017-06-29 02:25:01,864 main.py:57] epoch 2716, training loss: 8180.17, average training loss: 7770.37, base loss: 16039.93
[INFO 2017-06-29 02:25:04,910 main.py:57] epoch 2717, training loss: 7235.38, average training loss: 7770.39, base loss: 16040.15
[INFO 2017-06-29 02:25:07,990 main.py:57] epoch 2718, training loss: 6823.28, average training loss: 7769.55, base loss: 16038.61
[INFO 2017-06-29 02:25:11,064 main.py:57] epoch 2719, training loss: 6769.72, average training loss: 7769.36, base loss: 16037.28
[INFO 2017-06-29 02:25:14,118 main.py:57] epoch 2720, training loss: 8946.11, average training loss: 7769.72, base loss: 16038.14
[INFO 2017-06-29 02:25:17,199 main.py:57] epoch 2721, training loss: 8396.59, average training loss: 7770.91, base loss: 16039.13
[INFO 2017-06-29 02:25:20,197 main.py:57] epoch 2722, training loss: 7580.29, average training loss: 7770.27, base loss: 16040.01
[INFO 2017-06-29 02:25:23,231 main.py:57] epoch 2723, training loss: 6965.09, average training loss: 7769.30, base loss: 16038.65
[INFO 2017-06-29 02:25:26,296 main.py:57] epoch 2724, training loss: 8140.35, average training loss: 7769.60, base loss: 16039.36
[INFO 2017-06-29 02:25:29,319 main.py:57] epoch 2725, training loss: 7417.25, average training loss: 7769.65, base loss: 16039.49
[INFO 2017-06-29 02:25:32,320 main.py:57] epoch 2726, training loss: 7700.87, average training loss: 7769.19, base loss: 16040.17
[INFO 2017-06-29 02:25:35,317 main.py:57] epoch 2727, training loss: 7557.98, average training loss: 7767.65, base loss: 16041.12
[INFO 2017-06-29 02:25:38,346 main.py:57] epoch 2728, training loss: 7115.74, average training loss: 7766.99, base loss: 16040.83
[INFO 2017-06-29 02:25:41,399 main.py:57] epoch 2729, training loss: 7306.83, average training loss: 7766.75, base loss: 16041.59
[INFO 2017-06-29 02:25:44,528 main.py:57] epoch 2730, training loss: 7725.32, average training loss: 7766.48, base loss: 16041.84
[INFO 2017-06-29 02:25:47,693 main.py:57] epoch 2731, training loss: 7394.58, average training loss: 7765.74, base loss: 16041.90
[INFO 2017-06-29 02:25:50,701 main.py:57] epoch 2732, training loss: 7412.65, average training loss: 7763.51, base loss: 16042.54
[INFO 2017-06-29 02:25:53,778 main.py:57] epoch 2733, training loss: 7792.02, average training loss: 7762.63, base loss: 16043.51
[INFO 2017-06-29 02:25:56,835 main.py:57] epoch 2734, training loss: 8025.37, average training loss: 7762.14, base loss: 16044.26
[INFO 2017-06-29 02:25:59,933 main.py:57] epoch 2735, training loss: 9552.26, average training loss: 7762.89, base loss: 16045.36
[INFO 2017-06-29 02:26:02,985 main.py:57] epoch 2736, training loss: 6487.02, average training loss: 7762.03, base loss: 16044.00
[INFO 2017-06-29 02:26:06,005 main.py:57] epoch 2737, training loss: 8240.86, average training loss: 7762.29, base loss: 16043.23
[INFO 2017-06-29 02:26:08,992 main.py:57] epoch 2738, training loss: 6993.31, average training loss: 7761.37, base loss: 16042.40
[INFO 2017-06-29 02:26:12,069 main.py:57] epoch 2739, training loss: 7711.55, average training loss: 7761.42, base loss: 16041.89
[INFO 2017-06-29 02:26:15,158 main.py:57] epoch 2740, training loss: 6764.20, average training loss: 7759.67, base loss: 16041.47
[INFO 2017-06-29 02:26:18,228 main.py:57] epoch 2741, training loss: 8771.31, average training loss: 7759.88, base loss: 16041.77
[INFO 2017-06-29 02:26:21,304 main.py:57] epoch 2742, training loss: 8696.94, average training loss: 7759.35, base loss: 16042.86
[INFO 2017-06-29 02:26:24,360 main.py:57] epoch 2743, training loss: 7516.92, average training loss: 7758.98, base loss: 16043.06
[INFO 2017-06-29 02:26:27,444 main.py:57] epoch 2744, training loss: 7565.20, average training loss: 7758.97, base loss: 16043.47
[INFO 2017-06-29 02:26:30,577 main.py:57] epoch 2745, training loss: 7598.54, average training loss: 7758.18, base loss: 16043.47
[INFO 2017-06-29 02:26:33,671 main.py:57] epoch 2746, training loss: 8073.73, average training loss: 7758.29, base loss: 16043.33
[INFO 2017-06-29 02:26:36,689 main.py:57] epoch 2747, training loss: 7715.57, average training loss: 7757.47, base loss: 16043.58
[INFO 2017-06-29 02:26:39,744 main.py:57] epoch 2748, training loss: 7769.77, average training loss: 7758.18, base loss: 16044.77
[INFO 2017-06-29 02:26:42,828 main.py:57] epoch 2749, training loss: 8296.70, average training loss: 7758.35, base loss: 16045.74
[INFO 2017-06-29 02:26:45,933 main.py:57] epoch 2750, training loss: 7387.52, average training loss: 7757.40, base loss: 16045.27
[INFO 2017-06-29 02:26:49,018 main.py:57] epoch 2751, training loss: 7131.82, average training loss: 7757.27, base loss: 16044.95
[INFO 2017-06-29 02:26:52,079 main.py:57] epoch 2752, training loss: 8252.42, average training loss: 7758.18, base loss: 16045.38
[INFO 2017-06-29 02:26:55,121 main.py:57] epoch 2753, training loss: 6383.20, average training loss: 7755.36, base loss: 16043.32
[INFO 2017-06-29 02:26:58,157 main.py:57] epoch 2754, training loss: 7590.98, average training loss: 7754.63, base loss: 16043.51
[INFO 2017-06-29 02:27:01,145 main.py:57] epoch 2755, training loss: 8346.81, average training loss: 7753.55, base loss: 16043.36
[INFO 2017-06-29 02:27:04,186 main.py:57] epoch 2756, training loss: 7979.27, average training loss: 7753.77, base loss: 16043.34
[INFO 2017-06-29 02:27:07,176 main.py:57] epoch 2757, training loss: 8184.47, average training loss: 7753.21, base loss: 16044.09
[INFO 2017-06-29 02:27:10,245 main.py:57] epoch 2758, training loss: 8101.23, average training loss: 7753.15, base loss: 16044.15
[INFO 2017-06-29 02:27:13,260 main.py:57] epoch 2759, training loss: 8068.18, average training loss: 7753.12, base loss: 16043.97
[INFO 2017-06-29 02:27:16,295 main.py:57] epoch 2760, training loss: 9020.25, average training loss: 7754.49, base loss: 16044.68
[INFO 2017-06-29 02:27:19,359 main.py:57] epoch 2761, training loss: 8097.92, average training loss: 7754.58, base loss: 16045.48
[INFO 2017-06-29 02:27:22,371 main.py:57] epoch 2762, training loss: 7763.40, average training loss: 7754.64, base loss: 16045.66
[INFO 2017-06-29 02:27:25,477 main.py:57] epoch 2763, training loss: 7832.20, average training loss: 7753.83, base loss: 16045.87
[INFO 2017-06-29 02:27:28,497 main.py:57] epoch 2764, training loss: 7437.12, average training loss: 7753.96, base loss: 16046.28
[INFO 2017-06-29 02:27:31,644 main.py:57] epoch 2765, training loss: 8102.58, average training loss: 7753.33, base loss: 16046.65
[INFO 2017-06-29 02:27:34,676 main.py:57] epoch 2766, training loss: 7898.50, average training loss: 7753.70, base loss: 16046.11
[INFO 2017-06-29 02:27:37,649 main.py:57] epoch 2767, training loss: 8422.38, average training loss: 7753.63, base loss: 16047.05
[INFO 2017-06-29 02:27:40,712 main.py:57] epoch 2768, training loss: 7159.95, average training loss: 7753.22, base loss: 16046.96
[INFO 2017-06-29 02:27:43,861 main.py:57] epoch 2769, training loss: 7485.44, average training loss: 7752.43, base loss: 16047.40
[INFO 2017-06-29 02:27:47,025 main.py:57] epoch 2770, training loss: 7461.74, average training loss: 7751.25, base loss: 16047.51
[INFO 2017-06-29 02:27:50,070 main.py:57] epoch 2771, training loss: 7226.87, average training loss: 7749.31, base loss: 16047.96
[INFO 2017-06-29 02:27:53,129 main.py:57] epoch 2772, training loss: 8248.77, average training loss: 7750.69, base loss: 16048.55
[INFO 2017-06-29 02:27:56,145 main.py:57] epoch 2773, training loss: 7595.16, average training loss: 7750.01, base loss: 16049.21
[INFO 2017-06-29 02:27:59,219 main.py:57] epoch 2774, training loss: 7541.00, average training loss: 7749.85, base loss: 16049.04
[INFO 2017-06-29 02:28:02,239 main.py:57] epoch 2775, training loss: 8448.31, average training loss: 7751.04, base loss: 16050.08
[INFO 2017-06-29 02:28:05,294 main.py:57] epoch 2776, training loss: 6905.11, average training loss: 7750.21, base loss: 16049.94
[INFO 2017-06-29 02:28:08,365 main.py:57] epoch 2777, training loss: 9425.13, average training loss: 7751.49, base loss: 16052.14
[INFO 2017-06-29 02:28:11,422 main.py:57] epoch 2778, training loss: 7899.10, average training loss: 7751.90, base loss: 16051.92
[INFO 2017-06-29 02:28:14,524 main.py:57] epoch 2779, training loss: 8332.24, average training loss: 7753.25, base loss: 16052.58
[INFO 2017-06-29 02:28:17,635 main.py:57] epoch 2780, training loss: 7802.71, average training loss: 7752.60, base loss: 16051.61
[INFO 2017-06-29 02:28:20,694 main.py:57] epoch 2781, training loss: 7867.45, average training loss: 7750.67, base loss: 16051.51
[INFO 2017-06-29 02:28:23,757 main.py:57] epoch 2782, training loss: 7888.52, average training loss: 7750.91, base loss: 16052.31
[INFO 2017-06-29 02:28:26,806 main.py:57] epoch 2783, training loss: 7903.30, average training loss: 7751.72, base loss: 16053.29
[INFO 2017-06-29 02:28:29,959 main.py:57] epoch 2784, training loss: 7958.10, average training loss: 7751.36, base loss: 16054.34
[INFO 2017-06-29 02:28:32,999 main.py:57] epoch 2785, training loss: 7666.62, average training loss: 7751.75, base loss: 16055.71
[INFO 2017-06-29 02:28:36,012 main.py:57] epoch 2786, training loss: 6694.07, average training loss: 7749.58, base loss: 16055.17
[INFO 2017-06-29 02:28:39,046 main.py:57] epoch 2787, training loss: 6909.86, average training loss: 7748.89, base loss: 16055.17
[INFO 2017-06-29 02:28:42,037 main.py:57] epoch 2788, training loss: 6808.61, average training loss: 7748.05, base loss: 16054.78
[INFO 2017-06-29 02:28:45,088 main.py:57] epoch 2789, training loss: 7564.81, average training loss: 7747.91, base loss: 16054.36
[INFO 2017-06-29 02:28:48,186 main.py:57] epoch 2790, training loss: 8300.64, average training loss: 7748.67, base loss: 16054.84
[INFO 2017-06-29 02:28:51,241 main.py:57] epoch 2791, training loss: 8266.67, average training loss: 7750.00, base loss: 16054.22
[INFO 2017-06-29 02:28:54,265 main.py:57] epoch 2792, training loss: 9181.54, average training loss: 7752.28, base loss: 16056.04
[INFO 2017-06-29 02:28:57,411 main.py:57] epoch 2793, training loss: 7766.37, average training loss: 7752.79, base loss: 16056.09
[INFO 2017-06-29 02:29:00,404 main.py:57] epoch 2794, training loss: 6402.07, average training loss: 7752.20, base loss: 16054.37
[INFO 2017-06-29 02:29:03,438 main.py:57] epoch 2795, training loss: 8480.74, average training loss: 7753.13, base loss: 16055.37
[INFO 2017-06-29 02:29:06,594 main.py:57] epoch 2796, training loss: 7922.40, average training loss: 7751.60, base loss: 16055.28
[INFO 2017-06-29 02:29:09,651 main.py:57] epoch 2797, training loss: 7274.92, average training loss: 7751.96, base loss: 16054.38
[INFO 2017-06-29 02:29:12,732 main.py:57] epoch 2798, training loss: 7484.44, average training loss: 7752.27, base loss: 16055.06
[INFO 2017-06-29 02:29:15,736 main.py:57] epoch 2799, training loss: 7249.49, average training loss: 7751.92, base loss: 16054.85
[INFO 2017-06-29 02:29:15,736 main.py:59] epoch 2799, testing
[INFO 2017-06-29 02:29:28,571 main.py:104] average testing loss: 8461.67, base loss: 16711.89
[INFO 2017-06-29 02:29:28,571 main.py:105] improve_loss: 8250.22, improve_percent: 0.49
[INFO 2017-06-29 02:29:28,573 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:29:31,640 main.py:57] epoch 2800, training loss: 7228.40, average training loss: 7751.23, base loss: 16054.64
[INFO 2017-06-29 02:29:34,711 main.py:57] epoch 2801, training loss: 7141.79, average training loss: 7750.91, base loss: 16053.45
[INFO 2017-06-29 02:29:37,801 main.py:57] epoch 2802, training loss: 7624.20, average training loss: 7751.56, base loss: 16053.58
[INFO 2017-06-29 02:29:40,840 main.py:57] epoch 2803, training loss: 7278.52, average training loss: 7751.05, base loss: 16052.13
[INFO 2017-06-29 02:29:43,903 main.py:57] epoch 2804, training loss: 7574.97, average training loss: 7751.22, base loss: 16051.84
[INFO 2017-06-29 02:29:46,948 main.py:57] epoch 2805, training loss: 7586.34, average training loss: 7751.71, base loss: 16052.37
[INFO 2017-06-29 02:29:50,039 main.py:57] epoch 2806, training loss: 8574.78, average training loss: 7752.48, base loss: 16053.83
[INFO 2017-06-29 02:29:53,138 main.py:57] epoch 2807, training loss: 8603.59, average training loss: 7753.32, base loss: 16055.52
[INFO 2017-06-29 02:29:56,164 main.py:57] epoch 2808, training loss: 7187.15, average training loss: 7751.50, base loss: 16055.50
[INFO 2017-06-29 02:29:59,226 main.py:57] epoch 2809, training loss: 8010.22, average training loss: 7752.39, base loss: 16055.94
[INFO 2017-06-29 02:30:02,256 main.py:57] epoch 2810, training loss: 8284.49, average training loss: 7753.18, base loss: 16056.47
[INFO 2017-06-29 02:30:05,280 main.py:57] epoch 2811, training loss: 7898.70, average training loss: 7753.14, base loss: 16056.79
[INFO 2017-06-29 02:30:08,329 main.py:57] epoch 2812, training loss: 8294.44, average training loss: 7754.29, base loss: 16057.87
[INFO 2017-06-29 02:30:11,314 main.py:57] epoch 2813, training loss: 7398.13, average training loss: 7754.50, base loss: 16056.64
[INFO 2017-06-29 02:30:14,405 main.py:57] epoch 2814, training loss: 7941.95, average training loss: 7753.76, base loss: 16056.36
[INFO 2017-06-29 02:30:17,457 main.py:57] epoch 2815, training loss: 8835.77, average training loss: 7754.42, base loss: 16058.09
[INFO 2017-06-29 02:30:20,502 main.py:57] epoch 2816, training loss: 7449.72, average training loss: 7755.09, base loss: 16057.51
[INFO 2017-06-29 02:30:23,582 main.py:57] epoch 2817, training loss: 7963.27, average training loss: 7755.96, base loss: 16057.06
[INFO 2017-06-29 02:30:26,576 main.py:57] epoch 2818, training loss: 8284.84, average training loss: 7756.25, base loss: 16057.62
[INFO 2017-06-29 02:30:29,576 main.py:57] epoch 2819, training loss: 7586.47, average training loss: 7756.61, base loss: 16057.51
[INFO 2017-06-29 02:30:32,670 main.py:57] epoch 2820, training loss: 8174.94, average training loss: 7755.79, base loss: 16058.67
[INFO 2017-06-29 02:30:35,759 main.py:57] epoch 2821, training loss: 6859.27, average training loss: 7754.53, base loss: 16058.59
[INFO 2017-06-29 02:30:38,860 main.py:57] epoch 2822, training loss: 8556.15, average training loss: 7756.24, base loss: 16058.92
[INFO 2017-06-29 02:30:41,983 main.py:57] epoch 2823, training loss: 7675.74, average training loss: 7755.71, base loss: 16057.58
[INFO 2017-06-29 02:30:45,035 main.py:57] epoch 2824, training loss: 8924.62, average training loss: 7757.23, base loss: 16059.12
[INFO 2017-06-29 02:30:48,069 main.py:57] epoch 2825, training loss: 6908.27, average training loss: 7757.29, base loss: 16058.91
[INFO 2017-06-29 02:30:51,097 main.py:57] epoch 2826, training loss: 7503.16, average training loss: 7757.49, base loss: 16058.91
[INFO 2017-06-29 02:30:54,143 main.py:57] epoch 2827, training loss: 7608.57, average training loss: 7758.07, base loss: 16058.98
[INFO 2017-06-29 02:30:57,291 main.py:57] epoch 2828, training loss: 7044.79, average training loss: 7757.74, base loss: 16057.67
[INFO 2017-06-29 02:31:00,379 main.py:57] epoch 2829, training loss: 8402.20, average training loss: 7757.97, base loss: 16057.97
[INFO 2017-06-29 02:31:03,465 main.py:57] epoch 2830, training loss: 8483.03, average training loss: 7759.41, base loss: 16059.12
[INFO 2017-06-29 02:31:06,551 main.py:57] epoch 2831, training loss: 6994.54, average training loss: 7759.12, base loss: 16058.39
[INFO 2017-06-29 02:31:09,560 main.py:57] epoch 2832, training loss: 7580.98, average training loss: 7758.94, base loss: 16057.80
[INFO 2017-06-29 02:31:12,611 main.py:57] epoch 2833, training loss: 7529.22, average training loss: 7757.87, base loss: 16056.82
[INFO 2017-06-29 02:31:15,645 main.py:57] epoch 2834, training loss: 7620.52, average training loss: 7757.73, base loss: 16057.12
[INFO 2017-06-29 02:31:18,735 main.py:57] epoch 2835, training loss: 8011.36, average training loss: 7757.68, base loss: 16057.66
[INFO 2017-06-29 02:31:21,871 main.py:57] epoch 2836, training loss: 7099.23, average training loss: 7755.83, base loss: 16056.75
[INFO 2017-06-29 02:31:24,907 main.py:57] epoch 2837, training loss: 7260.50, average training loss: 7755.29, base loss: 16057.12
[INFO 2017-06-29 02:31:28,001 main.py:57] epoch 2838, training loss: 7739.27, average training loss: 7754.84, base loss: 16057.77
[INFO 2017-06-29 02:31:31,077 main.py:57] epoch 2839, training loss: 7159.86, average training loss: 7755.07, base loss: 16056.56
[INFO 2017-06-29 02:31:34,124 main.py:57] epoch 2840, training loss: 8043.00, average training loss: 7755.60, base loss: 16056.86
[INFO 2017-06-29 02:31:37,203 main.py:57] epoch 2841, training loss: 7364.23, average training loss: 7755.75, base loss: 16056.26
[INFO 2017-06-29 02:31:40,224 main.py:57] epoch 2842, training loss: 6676.87, average training loss: 7754.68, base loss: 16055.23
[INFO 2017-06-29 02:31:43,236 main.py:57] epoch 2843, training loss: 7906.70, average training loss: 7753.24, base loss: 16055.40
[INFO 2017-06-29 02:31:46,315 main.py:57] epoch 2844, training loss: 7842.39, average training loss: 7752.22, base loss: 16055.92
[INFO 2017-06-29 02:31:49,419 main.py:57] epoch 2845, training loss: 10435.79, average training loss: 7754.73, base loss: 16058.90
[INFO 2017-06-29 02:31:52,427 main.py:57] epoch 2846, training loss: 7593.57, average training loss: 7754.22, base loss: 16059.32
[INFO 2017-06-29 02:31:55,435 main.py:57] epoch 2847, training loss: 8268.78, average training loss: 7754.84, base loss: 16060.98
[INFO 2017-06-29 02:31:58,407 main.py:57] epoch 2848, training loss: 6832.85, average training loss: 7754.62, base loss: 16060.23
[INFO 2017-06-29 02:32:01,562 main.py:57] epoch 2849, training loss: 7125.51, average training loss: 7754.24, base loss: 16060.11
[INFO 2017-06-29 02:32:04,698 main.py:57] epoch 2850, training loss: 7712.92, average training loss: 7755.00, base loss: 16059.25
[INFO 2017-06-29 02:32:07,693 main.py:57] epoch 2851, training loss: 9281.92, average training loss: 7757.29, base loss: 16060.58
[INFO 2017-06-29 02:32:10,784 main.py:57] epoch 2852, training loss: 7897.26, average training loss: 7756.60, base loss: 16060.90
[INFO 2017-06-29 02:32:13,847 main.py:57] epoch 2853, training loss: 7529.08, average training loss: 7756.82, base loss: 16059.93
[INFO 2017-06-29 02:32:16,814 main.py:57] epoch 2854, training loss: 7784.44, average training loss: 7755.52, base loss: 16060.09
[INFO 2017-06-29 02:32:19,830 main.py:57] epoch 2855, training loss: 7945.74, average training loss: 7755.33, base loss: 16061.53
[INFO 2017-06-29 02:32:22,895 main.py:57] epoch 2856, training loss: 7546.85, average training loss: 7754.41, base loss: 16060.21
[INFO 2017-06-29 02:32:25,945 main.py:57] epoch 2857, training loss: 8558.20, average training loss: 7754.75, base loss: 16060.20
[INFO 2017-06-29 02:32:28,984 main.py:57] epoch 2858, training loss: 8288.33, average training loss: 7755.63, base loss: 16060.85
[INFO 2017-06-29 02:32:31,982 main.py:57] epoch 2859, training loss: 6739.20, average training loss: 7755.43, base loss: 16060.18
[INFO 2017-06-29 02:32:35,074 main.py:57] epoch 2860, training loss: 7244.93, average training loss: 7754.73, base loss: 16060.96
[INFO 2017-06-29 02:32:38,132 main.py:57] epoch 2861, training loss: 7296.74, average training loss: 7754.11, base loss: 16060.77
[INFO 2017-06-29 02:32:41,228 main.py:57] epoch 2862, training loss: 9323.72, average training loss: 7754.85, base loss: 16062.56
[INFO 2017-06-29 02:32:44,256 main.py:57] epoch 2863, training loss: 7769.85, average training loss: 7755.24, base loss: 16062.24
[INFO 2017-06-29 02:32:47,263 main.py:57] epoch 2864, training loss: 7487.36, average training loss: 7755.76, base loss: 16061.54
[INFO 2017-06-29 02:32:50,330 main.py:57] epoch 2865, training loss: 8136.00, average training loss: 7756.33, base loss: 16061.42
[INFO 2017-06-29 02:32:53,420 main.py:57] epoch 2866, training loss: 8148.62, average training loss: 7757.66, base loss: 16062.08
[INFO 2017-06-29 02:32:56,456 main.py:57] epoch 2867, training loss: 6672.52, average training loss: 7755.84, base loss: 16060.72
[INFO 2017-06-29 02:32:59,517 main.py:57] epoch 2868, training loss: 8614.47, average training loss: 7755.98, base loss: 16061.30
[INFO 2017-06-29 02:33:02,594 main.py:57] epoch 2869, training loss: 7813.21, average training loss: 7756.37, base loss: 16061.59
[INFO 2017-06-29 02:33:05,661 main.py:57] epoch 2870, training loss: 7158.50, average training loss: 7755.67, base loss: 16061.48
[INFO 2017-06-29 02:33:08,784 main.py:57] epoch 2871, training loss: 6647.80, average training loss: 7754.29, base loss: 16059.98
[INFO 2017-06-29 02:33:11,845 main.py:57] epoch 2872, training loss: 7556.95, average training loss: 7754.85, base loss: 16060.04
[INFO 2017-06-29 02:33:14,898 main.py:57] epoch 2873, training loss: 6472.14, average training loss: 7753.95, base loss: 16057.94
[INFO 2017-06-29 02:33:17,994 main.py:57] epoch 2874, training loss: 8240.79, average training loss: 7753.59, base loss: 16059.33
[INFO 2017-06-29 02:33:21,027 main.py:57] epoch 2875, training loss: 7456.66, average training loss: 7753.83, base loss: 16059.85
[INFO 2017-06-29 02:33:24,127 main.py:57] epoch 2876, training loss: 7069.33, average training loss: 7752.43, base loss: 16059.29
[INFO 2017-06-29 02:33:27,286 main.py:57] epoch 2877, training loss: 7206.54, average training loss: 7750.63, base loss: 16059.87
[INFO 2017-06-29 02:33:30,272 main.py:57] epoch 2878, training loss: 7748.30, average training loss: 7750.99, base loss: 16060.18
[INFO 2017-06-29 02:33:33,260 main.py:57] epoch 2879, training loss: 7438.78, average training loss: 7750.74, base loss: 16060.92
[INFO 2017-06-29 02:33:36,317 main.py:57] epoch 2880, training loss: 8693.98, average training loss: 7750.94, base loss: 16061.69
[INFO 2017-06-29 02:33:39,389 main.py:57] epoch 2881, training loss: 8391.60, average training loss: 7751.71, base loss: 16061.46
[INFO 2017-06-29 02:33:42,416 main.py:57] epoch 2882, training loss: 7345.16, average training loss: 7751.32, base loss: 16061.65
[INFO 2017-06-29 02:33:45,462 main.py:57] epoch 2883, training loss: 7610.11, average training loss: 7751.74, base loss: 16061.74
[INFO 2017-06-29 02:33:48,489 main.py:57] epoch 2884, training loss: 7505.58, average training loss: 7751.63, base loss: 16060.37
[INFO 2017-06-29 02:33:51,451 main.py:57] epoch 2885, training loss: 7474.95, average training loss: 7751.61, base loss: 16060.06
[INFO 2017-06-29 02:33:54,560 main.py:57] epoch 2886, training loss: 7851.50, average training loss: 7751.46, base loss: 16059.89
[INFO 2017-06-29 02:33:57,630 main.py:57] epoch 2887, training loss: 8787.63, average training loss: 7752.42, base loss: 16060.18
[INFO 2017-06-29 02:34:00,647 main.py:57] epoch 2888, training loss: 7722.84, average training loss: 7752.99, base loss: 16059.87
[INFO 2017-06-29 02:34:03,713 main.py:57] epoch 2889, training loss: 7392.04, average training loss: 7752.54, base loss: 16059.80
[INFO 2017-06-29 02:34:06,761 main.py:57] epoch 2890, training loss: 7774.43, average training loss: 7751.78, base loss: 16060.58
[INFO 2017-06-29 02:34:09,795 main.py:57] epoch 2891, training loss: 7554.03, average training loss: 7751.72, base loss: 16060.85
[INFO 2017-06-29 02:34:12,859 main.py:57] epoch 2892, training loss: 8542.09, average training loss: 7753.08, base loss: 16062.34
[INFO 2017-06-29 02:34:15,936 main.py:57] epoch 2893, training loss: 6771.63, average training loss: 7752.44, base loss: 16060.98
[INFO 2017-06-29 02:34:18,910 main.py:57] epoch 2894, training loss: 7506.96, average training loss: 7753.49, base loss: 16060.64
[INFO 2017-06-29 02:34:21,933 main.py:57] epoch 2895, training loss: 7525.44, average training loss: 7753.08, base loss: 16059.77
[INFO 2017-06-29 02:34:24,969 main.py:57] epoch 2896, training loss: 8172.15, average training loss: 7753.06, base loss: 16060.45
[INFO 2017-06-29 02:34:28,003 main.py:57] epoch 2897, training loss: 7163.21, average training loss: 7752.54, base loss: 16060.31
[INFO 2017-06-29 02:34:31,090 main.py:57] epoch 2898, training loss: 8798.48, average training loss: 7753.45, base loss: 16062.46
[INFO 2017-06-29 02:34:34,186 main.py:57] epoch 2899, training loss: 7575.26, average training loss: 7752.60, base loss: 16063.22
[INFO 2017-06-29 02:34:34,187 main.py:59] epoch 2899, testing
[INFO 2017-06-29 02:34:46,959 main.py:104] average testing loss: 8497.02, base loss: 17128.77
[INFO 2017-06-29 02:34:46,959 main.py:105] improve_loss: 8631.75, improve_percent: 0.50
[INFO 2017-06-29 02:34:46,961 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:34:50,034 main.py:57] epoch 2900, training loss: 6938.30, average training loss: 7752.10, base loss: 16061.64
[INFO 2017-06-29 02:34:53,149 main.py:57] epoch 2901, training loss: 8086.02, average training loss: 7751.32, base loss: 16061.47
[INFO 2017-06-29 02:34:56,174 main.py:57] epoch 2902, training loss: 6315.72, average training loss: 7749.18, base loss: 16059.53
[INFO 2017-06-29 02:34:59,197 main.py:57] epoch 2903, training loss: 7993.90, average training loss: 7749.16, base loss: 16060.03
[INFO 2017-06-29 02:35:02,321 main.py:57] epoch 2904, training loss: 7396.04, average training loss: 7749.07, base loss: 16059.45
[INFO 2017-06-29 02:35:05,381 main.py:57] epoch 2905, training loss: 7344.67, average training loss: 7749.33, base loss: 16058.92
[INFO 2017-06-29 02:35:08,419 main.py:57] epoch 2906, training loss: 8358.38, average training loss: 7749.12, base loss: 16059.02
[INFO 2017-06-29 02:35:11,459 main.py:57] epoch 2907, training loss: 8126.91, average training loss: 7747.16, base loss: 16059.35
[INFO 2017-06-29 02:35:14,596 main.py:57] epoch 2908, training loss: 7967.45, average training loss: 7747.55, base loss: 16059.93
[INFO 2017-06-29 02:35:17,625 main.py:57] epoch 2909, training loss: 7359.77, average training loss: 7746.24, base loss: 16059.82
[INFO 2017-06-29 02:35:20,624 main.py:57] epoch 2910, training loss: 7162.86, average training loss: 7746.35, base loss: 16060.15
[INFO 2017-06-29 02:35:23,739 main.py:57] epoch 2911, training loss: 8402.06, average training loss: 7746.50, base loss: 16061.34
[INFO 2017-06-29 02:35:26,735 main.py:57] epoch 2912, training loss: 7827.69, average training loss: 7746.35, base loss: 16062.04
[INFO 2017-06-29 02:35:29,762 main.py:57] epoch 2913, training loss: 7096.38, average training loss: 7745.19, base loss: 16061.97
[INFO 2017-06-29 02:35:32,778 main.py:57] epoch 2914, training loss: 8974.33, average training loss: 7747.27, base loss: 16062.54
[INFO 2017-06-29 02:35:35,840 main.py:57] epoch 2915, training loss: 7789.20, average training loss: 7746.85, base loss: 16061.80
[INFO 2017-06-29 02:35:38,886 main.py:57] epoch 2916, training loss: 7411.12, average training loss: 7746.54, base loss: 16061.52
[INFO 2017-06-29 02:35:41,968 main.py:57] epoch 2917, training loss: 7702.44, average training loss: 7746.35, base loss: 16060.89
[INFO 2017-06-29 02:35:45,025 main.py:57] epoch 2918, training loss: 7490.96, average training loss: 7745.89, base loss: 16060.86
[INFO 2017-06-29 02:35:48,106 main.py:57] epoch 2919, training loss: 8193.32, average training loss: 7746.28, base loss: 16062.18
[INFO 2017-06-29 02:35:51,179 main.py:57] epoch 2920, training loss: 8156.26, average training loss: 7745.98, base loss: 16062.49
[INFO 2017-06-29 02:35:54,186 main.py:57] epoch 2921, training loss: 7958.77, average training loss: 7745.93, base loss: 16062.69
[INFO 2017-06-29 02:35:57,246 main.py:57] epoch 2922, training loss: 6687.09, average training loss: 7742.09, base loss: 16062.24
[INFO 2017-06-29 02:36:00,345 main.py:57] epoch 2923, training loss: 8601.82, average training loss: 7743.03, base loss: 16063.52
[INFO 2017-06-29 02:36:03,469 main.py:57] epoch 2924, training loss: 6807.47, average training loss: 7743.20, base loss: 16062.48
[INFO 2017-06-29 02:36:06,524 main.py:57] epoch 2925, training loss: 7281.99, average training loss: 7743.00, base loss: 16061.01
[INFO 2017-06-29 02:36:09,593 main.py:57] epoch 2926, training loss: 8969.82, average training loss: 7743.95, base loss: 16062.57
[INFO 2017-06-29 02:36:12,688 main.py:57] epoch 2927, training loss: 7729.83, average training loss: 7743.76, base loss: 16062.05
[INFO 2017-06-29 02:36:15,690 main.py:57] epoch 2928, training loss: 7234.26, average training loss: 7743.44, base loss: 16061.31
[INFO 2017-06-29 02:36:18,731 main.py:57] epoch 2929, training loss: 7980.92, average training loss: 7742.74, base loss: 16062.35
[INFO 2017-06-29 02:36:21,848 main.py:57] epoch 2930, training loss: 7049.76, average training loss: 7741.42, base loss: 16062.37
[INFO 2017-06-29 02:36:24,889 main.py:57] epoch 2931, training loss: 8220.82, average training loss: 7742.20, base loss: 16063.48
[INFO 2017-06-29 02:36:27,966 main.py:57] epoch 2932, training loss: 9104.23, average training loss: 7743.97, base loss: 16064.23
[INFO 2017-06-29 02:36:30,990 main.py:57] epoch 2933, training loss: 7617.86, average training loss: 7744.64, base loss: 16064.02
[INFO 2017-06-29 02:36:34,150 main.py:57] epoch 2934, training loss: 7190.98, average training loss: 7744.21, base loss: 16064.19
[INFO 2017-06-29 02:36:37,167 main.py:57] epoch 2935, training loss: 7331.14, average training loss: 7743.29, base loss: 16064.23
[INFO 2017-06-29 02:36:40,238 main.py:57] epoch 2936, training loss: 8199.43, average training loss: 7743.91, base loss: 16064.76
[INFO 2017-06-29 02:36:43,273 main.py:57] epoch 2937, training loss: 6876.99, average training loss: 7743.20, base loss: 16064.32
[INFO 2017-06-29 02:36:46,218 main.py:57] epoch 2938, training loss: 7926.91, average training loss: 7742.36, base loss: 16063.74
[INFO 2017-06-29 02:36:49,264 main.py:57] epoch 2939, training loss: 7507.96, average training loss: 7741.54, base loss: 16063.74
[INFO 2017-06-29 02:36:52,290 main.py:57] epoch 2940, training loss: 7000.50, average training loss: 7738.92, base loss: 16063.18
[INFO 2017-06-29 02:36:55,332 main.py:57] epoch 2941, training loss: 8267.21, average training loss: 7739.31, base loss: 16064.48
[INFO 2017-06-29 02:36:58,330 main.py:57] epoch 2942, training loss: 7380.92, average training loss: 7739.37, base loss: 16064.31
[INFO 2017-06-29 02:37:01,317 main.py:57] epoch 2943, training loss: 7251.60, average training loss: 7738.21, base loss: 16063.54
[INFO 2017-06-29 02:37:04,391 main.py:57] epoch 2944, training loss: 7243.71, average training loss: 7736.74, base loss: 16062.71
[INFO 2017-06-29 02:37:07,473 main.py:57] epoch 2945, training loss: 7649.39, average training loss: 7737.09, base loss: 16061.58
[INFO 2017-06-29 02:37:10,510 main.py:57] epoch 2946, training loss: 7053.23, average training loss: 7736.64, base loss: 16061.13
[INFO 2017-06-29 02:37:13,550 main.py:57] epoch 2947, training loss: 6503.15, average training loss: 7734.82, base loss: 16060.08
[INFO 2017-06-29 02:37:16,658 main.py:57] epoch 2948, training loss: 8216.06, average training loss: 7734.94, base loss: 16061.28
[INFO 2017-06-29 02:37:19,714 main.py:57] epoch 2949, training loss: 8581.36, average training loss: 7735.48, base loss: 16062.72
[INFO 2017-06-29 02:37:22,788 main.py:57] epoch 2950, training loss: 8349.89, average training loss: 7735.17, base loss: 16062.96
[INFO 2017-06-29 02:37:25,833 main.py:57] epoch 2951, training loss: 7484.38, average training loss: 7734.64, base loss: 16062.42
[INFO 2017-06-29 02:37:28,919 main.py:57] epoch 2952, training loss: 6633.23, average training loss: 7733.50, base loss: 16061.55
[INFO 2017-06-29 02:37:31,973 main.py:57] epoch 2953, training loss: 7611.81, average training loss: 7733.83, base loss: 16061.55
[INFO 2017-06-29 02:37:35,048 main.py:57] epoch 2954, training loss: 7821.73, average training loss: 7734.54, base loss: 16061.89
[INFO 2017-06-29 02:37:38,102 main.py:57] epoch 2955, training loss: 7497.33, average training loss: 7735.10, base loss: 16061.58
[INFO 2017-06-29 02:37:41,118 main.py:57] epoch 2956, training loss: 7820.73, average training loss: 7732.62, base loss: 16061.95
[INFO 2017-06-29 02:37:44,172 main.py:57] epoch 2957, training loss: 7427.91, average training loss: 7732.11, base loss: 16061.30
[INFO 2017-06-29 02:37:47,283 main.py:57] epoch 2958, training loss: 6732.83, average training loss: 7730.12, base loss: 16060.74
[INFO 2017-06-29 02:37:50,351 main.py:57] epoch 2959, training loss: 7103.42, average training loss: 7729.92, base loss: 16060.26
[INFO 2017-06-29 02:37:53,371 main.py:57] epoch 2960, training loss: 7100.38, average training loss: 7729.69, base loss: 16059.45
[INFO 2017-06-29 02:37:56,419 main.py:57] epoch 2961, training loss: 8258.42, average training loss: 7730.23, base loss: 16059.82
[INFO 2017-06-29 02:37:59,525 main.py:57] epoch 2962, training loss: 8261.89, average training loss: 7729.91, base loss: 16059.75
[INFO 2017-06-29 02:38:02,570 main.py:57] epoch 2963, training loss: 7510.57, average training loss: 7729.62, base loss: 16059.16
[INFO 2017-06-29 02:38:05,643 main.py:57] epoch 2964, training loss: 7838.75, average training loss: 7729.18, base loss: 16060.48
[INFO 2017-06-29 02:38:08,676 main.py:57] epoch 2965, training loss: 7618.52, average training loss: 7728.28, base loss: 16061.93
[INFO 2017-06-29 02:38:11,695 main.py:57] epoch 2966, training loss: 8229.54, average training loss: 7729.38, base loss: 16062.96
[INFO 2017-06-29 02:38:14,787 main.py:57] epoch 2967, training loss: 8902.21, average training loss: 7730.33, base loss: 16064.01
[INFO 2017-06-29 02:38:17,810 main.py:57] epoch 2968, training loss: 8613.22, average training loss: 7731.49, base loss: 16064.92
[INFO 2017-06-29 02:38:20,836 main.py:57] epoch 2969, training loss: 8536.74, average training loss: 7732.90, base loss: 16065.17
[INFO 2017-06-29 02:38:23,850 main.py:57] epoch 2970, training loss: 8170.75, average training loss: 7733.23, base loss: 16065.38
[INFO 2017-06-29 02:38:26,847 main.py:57] epoch 2971, training loss: 7949.35, average training loss: 7733.42, base loss: 16066.14
[INFO 2017-06-29 02:38:29,889 main.py:57] epoch 2972, training loss: 7512.98, average training loss: 7732.96, base loss: 16066.97
[INFO 2017-06-29 02:38:32,910 main.py:57] epoch 2973, training loss: 7399.07, average training loss: 7732.30, base loss: 16067.33
[INFO 2017-06-29 02:38:36,022 main.py:57] epoch 2974, training loss: 6428.83, average training loss: 7730.68, base loss: 16066.74
[INFO 2017-06-29 02:38:39,072 main.py:57] epoch 2975, training loss: 7857.92, average training loss: 7730.96, base loss: 16066.67
[INFO 2017-06-29 02:38:42,143 main.py:57] epoch 2976, training loss: 7259.03, average training loss: 7729.05, base loss: 16065.76
[INFO 2017-06-29 02:38:45,127 main.py:57] epoch 2977, training loss: 6935.78, average training loss: 7727.97, base loss: 16064.97
[INFO 2017-06-29 02:38:48,192 main.py:57] epoch 2978, training loss: 7141.04, average training loss: 7727.44, base loss: 16063.93
[INFO 2017-06-29 02:38:51,234 main.py:57] epoch 2979, training loss: 6653.87, average training loss: 7725.58, base loss: 16062.37
[INFO 2017-06-29 02:38:54,287 main.py:57] epoch 2980, training loss: 6956.41, average training loss: 7724.99, base loss: 16061.46
[INFO 2017-06-29 02:38:57,302 main.py:57] epoch 2981, training loss: 8007.63, average training loss: 7726.11, base loss: 16061.87
[INFO 2017-06-29 02:39:00,383 main.py:57] epoch 2982, training loss: 8041.24, average training loss: 7727.26, base loss: 16062.32
[INFO 2017-06-29 02:39:03,466 main.py:57] epoch 2983, training loss: 8038.36, average training loss: 7727.14, base loss: 16062.44
[INFO 2017-06-29 02:39:06,566 main.py:57] epoch 2984, training loss: 8277.67, average training loss: 7725.93, base loss: 16062.95
[INFO 2017-06-29 02:39:09,647 main.py:57] epoch 2985, training loss: 7926.60, average training loss: 7726.60, base loss: 16062.90
[INFO 2017-06-29 02:39:12,857 main.py:57] epoch 2986, training loss: 6863.37, average training loss: 7727.03, base loss: 16060.97
[INFO 2017-06-29 02:39:15,903 main.py:57] epoch 2987, training loss: 7782.41, average training loss: 7727.58, base loss: 16061.31
[INFO 2017-06-29 02:39:18,936 main.py:57] epoch 2988, training loss: 8414.26, average training loss: 7728.12, base loss: 16060.85
[INFO 2017-06-29 02:39:21,933 main.py:57] epoch 2989, training loss: 7221.10, average training loss: 7727.83, base loss: 16060.84
[INFO 2017-06-29 02:39:25,019 main.py:57] epoch 2990, training loss: 7822.27, average training loss: 7728.57, base loss: 16061.01
[INFO 2017-06-29 02:39:28,071 main.py:57] epoch 2991, training loss: 8931.82, average training loss: 7729.53, base loss: 16062.08
[INFO 2017-06-29 02:39:31,070 main.py:57] epoch 2992, training loss: 7133.92, average training loss: 7727.73, base loss: 16061.88
[INFO 2017-06-29 02:39:34,186 main.py:57] epoch 2993, training loss: 7214.72, average training loss: 7727.20, base loss: 16061.75
[INFO 2017-06-29 02:39:37,253 main.py:57] epoch 2994, training loss: 7213.12, average training loss: 7725.68, base loss: 16061.07
[INFO 2017-06-29 02:39:40,358 main.py:57] epoch 2995, training loss: 6839.02, average training loss: 7724.19, base loss: 16060.12
[INFO 2017-06-29 02:39:43,420 main.py:57] epoch 2996, training loss: 7388.52, average training loss: 7724.44, base loss: 16059.26
[INFO 2017-06-29 02:39:46,419 main.py:57] epoch 2997, training loss: 7338.97, average training loss: 7722.65, base loss: 16058.04
[INFO 2017-06-29 02:39:49,477 main.py:57] epoch 2998, training loss: 8582.48, average training loss: 7723.87, base loss: 16059.62
[INFO 2017-06-29 02:39:52,560 main.py:57] epoch 2999, training loss: 6997.11, average training loss: 7723.47, base loss: 16058.71
[INFO 2017-06-29 02:39:52,561 main.py:59] epoch 2999, testing
[INFO 2017-06-29 02:40:05,108 main.py:104] average testing loss: 7908.01, base loss: 15910.02
[INFO 2017-06-29 02:40:05,108 main.py:105] improve_loss: 8002.01, improve_percent: 0.50
[INFO 2017-06-29 02:40:05,109 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:40:08,156 main.py:57] epoch 3000, training loss: 6432.48, average training loss: 7720.49, base loss: 16057.17
[INFO 2017-06-29 02:40:11,149 main.py:57] epoch 3001, training loss: 6822.88, average training loss: 7719.88, base loss: 16055.81
[INFO 2017-06-29 02:40:14,217 main.py:57] epoch 3002, training loss: 8255.75, average training loss: 7719.74, base loss: 16057.25
[INFO 2017-06-29 02:40:17,251 main.py:57] epoch 3003, training loss: 7469.10, average training loss: 7718.85, base loss: 16057.48
[INFO 2017-06-29 02:40:20,387 main.py:57] epoch 3004, training loss: 7705.28, average training loss: 7717.45, base loss: 16057.20
[INFO 2017-06-29 02:40:23,365 main.py:57] epoch 3005, training loss: 7404.77, average training loss: 7717.09, base loss: 16056.76
[INFO 2017-06-29 02:40:26,461 main.py:57] epoch 3006, training loss: 7857.20, average training loss: 7717.83, base loss: 16057.74
[INFO 2017-06-29 02:40:29,502 main.py:57] epoch 3007, training loss: 7416.58, average training loss: 7717.79, base loss: 16057.13
[INFO 2017-06-29 02:40:32,499 main.py:57] epoch 3008, training loss: 6897.11, average training loss: 7717.48, base loss: 16056.40
[INFO 2017-06-29 02:40:35,553 main.py:57] epoch 3009, training loss: 7565.90, average training loss: 7717.04, base loss: 16056.71
[INFO 2017-06-29 02:40:38,648 main.py:57] epoch 3010, training loss: 9003.34, average training loss: 7718.06, base loss: 16056.96
[INFO 2017-06-29 02:40:41,745 main.py:57] epoch 3011, training loss: 7816.59, average training loss: 7718.58, base loss: 16056.37
[INFO 2017-06-29 02:40:44,782 main.py:57] epoch 3012, training loss: 8293.99, average training loss: 7718.24, base loss: 16057.22
[INFO 2017-06-29 02:40:47,805 main.py:57] epoch 3013, training loss: 7582.32, average training loss: 7718.52, base loss: 16057.46
[INFO 2017-06-29 02:40:50,825 main.py:57] epoch 3014, training loss: 7278.16, average training loss: 7717.91, base loss: 16057.02
[INFO 2017-06-29 02:40:53,896 main.py:57] epoch 3015, training loss: 7412.75, average training loss: 7717.76, base loss: 16056.89
[INFO 2017-06-29 02:40:56,949 main.py:57] epoch 3016, training loss: 7070.16, average training loss: 7716.58, base loss: 16055.68
[INFO 2017-06-29 02:41:00,006 main.py:57] epoch 3017, training loss: 7939.29, average training loss: 7716.62, base loss: 16055.47
[INFO 2017-06-29 02:41:03,062 main.py:57] epoch 3018, training loss: 6930.71, average training loss: 7714.98, base loss: 16054.50
[INFO 2017-06-29 02:41:06,110 main.py:57] epoch 3019, training loss: 7223.78, average training loss: 7714.79, base loss: 16054.84
[INFO 2017-06-29 02:41:09,119 main.py:57] epoch 3020, training loss: 7535.38, average training loss: 7714.10, base loss: 16055.33
[INFO 2017-06-29 02:41:12,179 main.py:57] epoch 3021, training loss: 7206.44, average training loss: 7713.32, base loss: 16054.55
[INFO 2017-06-29 02:41:15,240 main.py:57] epoch 3022, training loss: 7548.99, average training loss: 7713.92, base loss: 16053.07
[INFO 2017-06-29 02:41:18,235 main.py:57] epoch 3023, training loss: 7357.23, average training loss: 7714.02, base loss: 16052.41
[INFO 2017-06-29 02:41:21,308 main.py:57] epoch 3024, training loss: 7251.53, average training loss: 7713.74, base loss: 16052.49
[INFO 2017-06-29 02:41:24,336 main.py:57] epoch 3025, training loss: 6641.08, average training loss: 7712.59, base loss: 16051.13
[INFO 2017-06-29 02:41:27,455 main.py:57] epoch 3026, training loss: 6889.61, average training loss: 7711.85, base loss: 16050.26
[INFO 2017-06-29 02:41:30,518 main.py:57] epoch 3027, training loss: 7919.47, average training loss: 7712.22, base loss: 16050.99
[INFO 2017-06-29 02:41:33,616 main.py:57] epoch 3028, training loss: 7102.17, average training loss: 7712.02, base loss: 16049.89
[INFO 2017-06-29 02:41:36,680 main.py:57] epoch 3029, training loss: 7907.43, average training loss: 7712.39, base loss: 16049.39
[INFO 2017-06-29 02:41:39,782 main.py:57] epoch 3030, training loss: 8050.87, average training loss: 7713.45, base loss: 16050.14
[INFO 2017-06-29 02:41:42,853 main.py:57] epoch 3031, training loss: 7497.33, average training loss: 7713.50, base loss: 16049.66
[INFO 2017-06-29 02:41:45,974 main.py:57] epoch 3032, training loss: 8467.98, average training loss: 7713.22, base loss: 16049.81
[INFO 2017-06-29 02:41:49,086 main.py:57] epoch 3033, training loss: 6812.74, average training loss: 7712.31, base loss: 16049.25
[INFO 2017-06-29 02:41:52,104 main.py:57] epoch 3034, training loss: 8191.42, average training loss: 7712.87, base loss: 16050.62
[INFO 2017-06-29 02:41:55,222 main.py:57] epoch 3035, training loss: 7459.87, average training loss: 7712.10, base loss: 16050.33
[INFO 2017-06-29 02:41:58,259 main.py:57] epoch 3036, training loss: 8298.20, average training loss: 7712.54, base loss: 16051.24
[INFO 2017-06-29 02:42:01,311 main.py:57] epoch 3037, training loss: 7062.33, average training loss: 7712.10, base loss: 16050.68
[INFO 2017-06-29 02:42:04,366 main.py:57] epoch 3038, training loss: 7949.70, average training loss: 7710.76, base loss: 16052.65
[INFO 2017-06-29 02:42:07,451 main.py:57] epoch 3039, training loss: 7867.47, average training loss: 7711.26, base loss: 16053.93
[INFO 2017-06-29 02:42:10,573 main.py:57] epoch 3040, training loss: 8510.63, average training loss: 7711.40, base loss: 16055.63
[INFO 2017-06-29 02:42:13,644 main.py:57] epoch 3041, training loss: 8413.57, average training loss: 7711.68, base loss: 16056.83
[INFO 2017-06-29 02:42:16,709 main.py:57] epoch 3042, training loss: 8440.78, average training loss: 7711.68, base loss: 16057.40
[INFO 2017-06-29 02:42:19,705 main.py:57] epoch 3043, training loss: 7760.35, average training loss: 7711.78, base loss: 16056.74
[INFO 2017-06-29 02:42:22,723 main.py:57] epoch 3044, training loss: 7887.29, average training loss: 7712.58, base loss: 16056.04
[INFO 2017-06-29 02:42:25,764 main.py:57] epoch 3045, training loss: 7538.38, average training loss: 7712.32, base loss: 16055.63
[INFO 2017-06-29 02:42:28,800 main.py:57] epoch 3046, training loss: 8192.54, average training loss: 7712.16, base loss: 16055.80
[INFO 2017-06-29 02:42:31,887 main.py:57] epoch 3047, training loss: 7077.47, average training loss: 7709.86, base loss: 16055.88
[INFO 2017-06-29 02:42:34,991 main.py:57] epoch 3048, training loss: 7520.15, average training loss: 7710.20, base loss: 16056.37
[INFO 2017-06-29 02:42:38,094 main.py:57] epoch 3049, training loss: 7652.22, average training loss: 7710.37, base loss: 16057.44
[INFO 2017-06-29 02:42:41,147 main.py:57] epoch 3050, training loss: 7954.45, average training loss: 7711.13, base loss: 16057.95
[INFO 2017-06-29 02:42:44,205 main.py:57] epoch 3051, training loss: 6610.54, average training loss: 7709.96, base loss: 16057.32
[INFO 2017-06-29 02:42:47,226 main.py:57] epoch 3052, training loss: 8036.14, average training loss: 7710.28, base loss: 16057.39
[INFO 2017-06-29 02:42:50,204 main.py:57] epoch 3053, training loss: 7730.19, average training loss: 7710.91, base loss: 16057.89
[INFO 2017-06-29 02:42:53,292 main.py:57] epoch 3054, training loss: 7233.86, average training loss: 7710.58, base loss: 16057.06
[INFO 2017-06-29 02:42:56,347 main.py:57] epoch 3055, training loss: 7039.05, average training loss: 7710.58, base loss: 16055.60
[INFO 2017-06-29 02:42:59,422 main.py:57] epoch 3056, training loss: 6755.81, average training loss: 7709.05, base loss: 16053.85
[INFO 2017-06-29 02:43:02,496 main.py:57] epoch 3057, training loss: 6346.98, average training loss: 7707.16, base loss: 16052.85
[INFO 2017-06-29 02:43:05,481 main.py:57] epoch 3058, training loss: 7343.31, average training loss: 7706.01, base loss: 16052.25
[INFO 2017-06-29 02:43:08,505 main.py:57] epoch 3059, training loss: 9019.53, average training loss: 7706.57, base loss: 16053.51
[INFO 2017-06-29 02:43:11,640 main.py:57] epoch 3060, training loss: 8980.49, average training loss: 7707.44, base loss: 16055.30
[INFO 2017-06-29 02:43:14,708 main.py:57] epoch 3061, training loss: 8004.71, average training loss: 7707.59, base loss: 16056.32
[INFO 2017-06-29 02:43:17,799 main.py:57] epoch 3062, training loss: 7164.03, average training loss: 7707.99, base loss: 16054.97
[INFO 2017-06-29 02:43:20,905 main.py:57] epoch 3063, training loss: 7430.43, average training loss: 7707.77, base loss: 16054.16
[INFO 2017-06-29 02:43:23,908 main.py:57] epoch 3064, training loss: 7386.10, average training loss: 7708.03, base loss: 16054.43
[INFO 2017-06-29 02:43:26,979 main.py:57] epoch 3065, training loss: 8164.06, average training loss: 7707.77, base loss: 16055.13
[INFO 2017-06-29 02:43:30,012 main.py:57] epoch 3066, training loss: 9177.58, average training loss: 7709.62, base loss: 16056.95
[INFO 2017-06-29 02:43:33,031 main.py:57] epoch 3067, training loss: 7561.15, average training loss: 7710.16, base loss: 16057.92
[INFO 2017-06-29 02:43:36,166 main.py:57] epoch 3068, training loss: 8185.11, average training loss: 7710.86, base loss: 16059.12
[INFO 2017-06-29 02:43:39,284 main.py:57] epoch 3069, training loss: 8308.32, average training loss: 7711.62, base loss: 16060.01
[INFO 2017-06-29 02:43:42,394 main.py:57] epoch 3070, training loss: 8495.14, average training loss: 7712.96, base loss: 16060.05
[INFO 2017-06-29 02:43:45,419 main.py:57] epoch 3071, training loss: 7785.54, average training loss: 7713.60, base loss: 16059.95
[INFO 2017-06-29 02:43:48,400 main.py:57] epoch 3072, training loss: 8097.82, average training loss: 7713.85, base loss: 16059.80
[INFO 2017-06-29 02:43:51,391 main.py:57] epoch 3073, training loss: 7901.81, average training loss: 7714.30, base loss: 16059.35
[INFO 2017-06-29 02:43:54,373 main.py:57] epoch 3074, training loss: 7362.49, average training loss: 7713.29, base loss: 16060.06
[INFO 2017-06-29 02:43:57,405 main.py:57] epoch 3075, training loss: 8645.24, average training loss: 7714.49, base loss: 16060.93
[INFO 2017-06-29 02:44:00,459 main.py:57] epoch 3076, training loss: 6530.90, average training loss: 7712.85, base loss: 16060.07
[INFO 2017-06-29 02:44:03,454 main.py:57] epoch 3077, training loss: 7065.20, average training loss: 7712.81, base loss: 16059.71
[INFO 2017-06-29 02:44:06,572 main.py:57] epoch 3078, training loss: 7135.51, average training loss: 7712.58, base loss: 16059.24
[INFO 2017-06-29 02:44:09,607 main.py:57] epoch 3079, training loss: 7184.92, average training loss: 7710.98, base loss: 16059.05
[INFO 2017-06-29 02:44:12,594 main.py:57] epoch 3080, training loss: 7545.89, average training loss: 7709.84, base loss: 16058.84
[INFO 2017-06-29 02:44:15,621 main.py:57] epoch 3081, training loss: 6794.81, average training loss: 7708.92, base loss: 16058.58
[INFO 2017-06-29 02:44:18,726 main.py:57] epoch 3082, training loss: 7179.78, average training loss: 7708.95, base loss: 16058.23
[INFO 2017-06-29 02:44:21,808 main.py:57] epoch 3083, training loss: 7168.10, average training loss: 7708.82, base loss: 16057.65
[INFO 2017-06-29 02:44:24,862 main.py:57] epoch 3084, training loss: 7583.34, average training loss: 7709.37, base loss: 16058.67
[INFO 2017-06-29 02:44:27,887 main.py:57] epoch 3085, training loss: 9124.88, average training loss: 7709.84, base loss: 16060.32
[INFO 2017-06-29 02:44:30,981 main.py:57] epoch 3086, training loss: 8106.99, average training loss: 7709.42, base loss: 16060.73
[INFO 2017-06-29 02:44:34,027 main.py:57] epoch 3087, training loss: 6814.71, average training loss: 7708.88, base loss: 16060.17
[INFO 2017-06-29 02:44:37,067 main.py:57] epoch 3088, training loss: 7763.15, average training loss: 7708.65, base loss: 16059.67
[INFO 2017-06-29 02:44:40,069 main.py:57] epoch 3089, training loss: 7744.10, average training loss: 7709.52, base loss: 16060.15
[INFO 2017-06-29 02:44:43,251 main.py:57] epoch 3090, training loss: 7262.85, average training loss: 7709.76, base loss: 16059.30
[INFO 2017-06-29 02:44:46,279 main.py:57] epoch 3091, training loss: 6697.06, average training loss: 7709.08, base loss: 16058.08
[INFO 2017-06-29 02:44:49,371 main.py:57] epoch 3092, training loss: 8441.66, average training loss: 7709.44, base loss: 16059.59
[INFO 2017-06-29 02:44:52,382 main.py:57] epoch 3093, training loss: 8887.14, average training loss: 7709.21, base loss: 16060.51
[INFO 2017-06-29 02:44:55,410 main.py:57] epoch 3094, training loss: 8175.49, average training loss: 7707.48, base loss: 16061.18
[INFO 2017-06-29 02:44:58,477 main.py:57] epoch 3095, training loss: 6569.63, average training loss: 7706.62, base loss: 16060.27
[INFO 2017-06-29 02:45:01,494 main.py:57] epoch 3096, training loss: 8045.26, average training loss: 7706.68, base loss: 16060.84
[INFO 2017-06-29 02:45:04,586 main.py:57] epoch 3097, training loss: 7033.98, average training loss: 7704.90, base loss: 16060.69
[INFO 2017-06-29 02:45:07,662 main.py:57] epoch 3098, training loss: 6979.70, average training loss: 7704.59, base loss: 16060.48
[INFO 2017-06-29 02:45:10,731 main.py:57] epoch 3099, training loss: 8287.93, average training loss: 7705.46, base loss: 16061.34
[INFO 2017-06-29 02:45:10,732 main.py:59] epoch 3099, testing
[INFO 2017-06-29 02:45:23,344 main.py:104] average testing loss: 7948.84, base loss: 15974.79
[INFO 2017-06-29 02:45:23,344 main.py:105] improve_loss: 8025.95, improve_percent: 0.50
[INFO 2017-06-29 02:45:23,346 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:45:26,468 main.py:57] epoch 3100, training loss: 7574.07, average training loss: 7705.29, base loss: 16061.45
[INFO 2017-06-29 02:45:29,494 main.py:57] epoch 3101, training loss: 7057.82, average training loss: 7704.45, base loss: 16060.29
[INFO 2017-06-29 02:45:32,505 main.py:57] epoch 3102, training loss: 6673.16, average training loss: 7703.23, base loss: 16059.90
[INFO 2017-06-29 02:45:35,592 main.py:57] epoch 3103, training loss: 7609.47, average training loss: 7702.59, base loss: 16060.18
[INFO 2017-06-29 02:45:38,661 main.py:57] epoch 3104, training loss: 7698.26, average training loss: 7702.30, base loss: 16058.74
[INFO 2017-06-29 02:45:41,763 main.py:57] epoch 3105, training loss: 7311.26, average training loss: 7702.64, base loss: 16057.21
[INFO 2017-06-29 02:45:44,788 main.py:57] epoch 3106, training loss: 7160.64, average training loss: 7701.34, base loss: 16056.98
[INFO 2017-06-29 02:45:47,868 main.py:57] epoch 3107, training loss: 7707.58, average training loss: 7701.60, base loss: 16057.23
[INFO 2017-06-29 02:45:50,892 main.py:57] epoch 3108, training loss: 7521.54, average training loss: 7700.82, base loss: 16057.32
[INFO 2017-06-29 02:45:53,998 main.py:57] epoch 3109, training loss: 7989.95, average training loss: 7701.12, base loss: 16058.38
[INFO 2017-06-29 02:45:57,082 main.py:57] epoch 3110, training loss: 7274.36, average training loss: 7700.60, base loss: 16057.99
[INFO 2017-06-29 02:46:00,125 main.py:57] epoch 3111, training loss: 8242.49, average training loss: 7700.96, base loss: 16058.88
[INFO 2017-06-29 02:46:03,178 main.py:57] epoch 3112, training loss: 7114.56, average training loss: 7700.11, base loss: 16057.92
[INFO 2017-06-29 02:46:06,247 main.py:57] epoch 3113, training loss: 8342.71, average training loss: 7701.65, base loss: 16057.72
[INFO 2017-06-29 02:46:09,333 main.py:57] epoch 3114, training loss: 10063.96, average training loss: 7704.84, base loss: 16058.41
[INFO 2017-06-29 02:46:12,358 main.py:57] epoch 3115, training loss: 7402.46, average training loss: 7703.33, base loss: 16057.78
[INFO 2017-06-29 02:46:15,468 main.py:57] epoch 3116, training loss: 7920.27, average training loss: 7704.20, base loss: 16058.19
[INFO 2017-06-29 02:46:18,548 main.py:57] epoch 3117, training loss: 7448.99, average training loss: 7704.58, base loss: 16058.32
[INFO 2017-06-29 02:46:21,604 main.py:57] epoch 3118, training loss: 8207.22, average training loss: 7703.85, base loss: 16058.74
[INFO 2017-06-29 02:46:24,635 main.py:57] epoch 3119, training loss: 7651.58, average training loss: 7703.65, base loss: 16058.51
[INFO 2017-06-29 02:46:27,750 main.py:57] epoch 3120, training loss: 7216.62, average training loss: 7702.78, base loss: 16058.18
[INFO 2017-06-29 02:46:30,841 main.py:57] epoch 3121, training loss: 7540.49, average training loss: 7700.02, base loss: 16057.41
[INFO 2017-06-29 02:46:34,026 main.py:57] epoch 3122, training loss: 7729.36, average training loss: 7700.30, base loss: 16057.37
[INFO 2017-06-29 02:46:37,144 main.py:57] epoch 3123, training loss: 6395.31, average training loss: 7699.10, base loss: 16055.79
[INFO 2017-06-29 02:46:40,216 main.py:57] epoch 3124, training loss: 8118.62, average training loss: 7699.11, base loss: 16056.68
[INFO 2017-06-29 02:46:43,241 main.py:57] epoch 3125, training loss: 7453.67, average training loss: 7699.20, base loss: 16057.26
[INFO 2017-06-29 02:46:46,282 main.py:57] epoch 3126, training loss: 7956.56, average training loss: 7699.33, base loss: 16057.62
[INFO 2017-06-29 02:46:49,337 main.py:57] epoch 3127, training loss: 8249.89, average training loss: 7699.87, base loss: 16058.50
[INFO 2017-06-29 02:46:52,452 main.py:57] epoch 3128, training loss: 8123.35, average training loss: 7700.03, base loss: 16058.88
[INFO 2017-06-29 02:46:55,482 main.py:57] epoch 3129, training loss: 8968.96, average training loss: 7701.24, base loss: 16059.84
[INFO 2017-06-29 02:46:58,563 main.py:57] epoch 3130, training loss: 7423.81, average training loss: 7699.98, base loss: 16060.21
[INFO 2017-06-29 02:47:01,642 main.py:57] epoch 3131, training loss: 8261.06, average training loss: 7699.95, base loss: 16061.05
[INFO 2017-06-29 02:47:04,677 main.py:57] epoch 3132, training loss: 7377.77, average training loss: 7699.58, base loss: 16059.87
[INFO 2017-06-29 02:47:07,728 main.py:57] epoch 3133, training loss: 8106.35, average training loss: 7699.78, base loss: 16059.01
[INFO 2017-06-29 02:47:10,824 main.py:57] epoch 3134, training loss: 8613.23, average training loss: 7700.01, base loss: 16059.46
[INFO 2017-06-29 02:47:13,897 main.py:57] epoch 3135, training loss: 7503.74, average training loss: 7700.27, base loss: 16058.77
[INFO 2017-06-29 02:47:16,915 main.py:57] epoch 3136, training loss: 7866.49, average training loss: 7701.21, base loss: 16058.24
[INFO 2017-06-29 02:47:19,967 main.py:57] epoch 3137, training loss: 6519.67, average training loss: 7699.70, base loss: 16056.60
[INFO 2017-06-29 02:47:22,991 main.py:57] epoch 3138, training loss: 9245.77, average training loss: 7700.53, base loss: 16057.89
[INFO 2017-06-29 02:47:25,998 main.py:57] epoch 3139, training loss: 7059.21, average training loss: 7699.93, base loss: 16057.54
[INFO 2017-06-29 02:47:29,047 main.py:57] epoch 3140, training loss: 7170.76, average training loss: 7699.28, base loss: 16057.71
[INFO 2017-06-29 02:47:32,165 main.py:57] epoch 3141, training loss: 7486.60, average training loss: 7699.42, base loss: 16057.47
[INFO 2017-06-29 02:47:35,187 main.py:57] epoch 3142, training loss: 7866.29, average training loss: 7699.81, base loss: 16058.78
[INFO 2017-06-29 02:47:38,253 main.py:57] epoch 3143, training loss: 7282.18, average training loss: 7699.05, base loss: 16058.65
[INFO 2017-06-29 02:47:41,348 main.py:57] epoch 3144, training loss: 7631.47, average training loss: 7699.16, base loss: 16059.23
[INFO 2017-06-29 02:47:44,395 main.py:57] epoch 3145, training loss: 8363.94, average training loss: 7699.28, base loss: 16059.45
[INFO 2017-06-29 02:47:47,432 main.py:57] epoch 3146, training loss: 7715.58, average training loss: 7699.91, base loss: 16060.04
[INFO 2017-06-29 02:47:50,397 main.py:57] epoch 3147, training loss: 8625.09, average training loss: 7700.80, base loss: 16060.62
[INFO 2017-06-29 02:47:53,455 main.py:57] epoch 3148, training loss: 7716.22, average training loss: 7701.28, base loss: 16060.78
[INFO 2017-06-29 02:47:56,486 main.py:57] epoch 3149, training loss: 7892.91, average training loss: 7701.22, base loss: 16061.79
[INFO 2017-06-29 02:47:59,577 main.py:57] epoch 3150, training loss: 6997.19, average training loss: 7700.60, base loss: 16061.85
[INFO 2017-06-29 02:48:02,630 main.py:57] epoch 3151, training loss: 8986.38, average training loss: 7702.15, base loss: 16062.92
[INFO 2017-06-29 02:48:05,680 main.py:57] epoch 3152, training loss: 8404.44, average training loss: 7703.54, base loss: 16064.63
[INFO 2017-06-29 02:48:08,714 main.py:57] epoch 3153, training loss: 6928.09, average training loss: 7702.71, base loss: 16064.45
[INFO 2017-06-29 02:48:11,761 main.py:57] epoch 3154, training loss: 7614.57, average training loss: 7702.83, base loss: 16064.30
[INFO 2017-06-29 02:48:14,869 main.py:57] epoch 3155, training loss: 7328.57, average training loss: 7702.66, base loss: 16063.70
[INFO 2017-06-29 02:48:17,880 main.py:57] epoch 3156, training loss: 7651.96, average training loss: 7702.34, base loss: 16064.13
[INFO 2017-06-29 02:48:20,972 main.py:57] epoch 3157, training loss: 8026.48, average training loss: 7701.22, base loss: 16065.24
[INFO 2017-06-29 02:48:23,975 main.py:57] epoch 3158, training loss: 7953.69, average training loss: 7701.09, base loss: 16064.22
[INFO 2017-06-29 02:48:27,071 main.py:57] epoch 3159, training loss: 8167.98, average training loss: 7701.49, base loss: 16063.83
[INFO 2017-06-29 02:48:30,117 main.py:57] epoch 3160, training loss: 8032.40, average training loss: 7700.28, base loss: 16064.42
[INFO 2017-06-29 02:48:33,178 main.py:57] epoch 3161, training loss: 7314.09, average training loss: 7699.00, base loss: 16063.86
[INFO 2017-06-29 02:48:36,232 main.py:57] epoch 3162, training loss: 9048.54, average training loss: 7700.15, base loss: 16064.93
[INFO 2017-06-29 02:48:39,255 main.py:57] epoch 3163, training loss: 8305.34, average training loss: 7700.94, base loss: 16064.33
[INFO 2017-06-29 02:48:42,303 main.py:57] epoch 3164, training loss: 8405.66, average training loss: 7701.92, base loss: 16065.65
[INFO 2017-06-29 02:48:45,340 main.py:57] epoch 3165, training loss: 7787.53, average training loss: 7702.51, base loss: 16065.49
[INFO 2017-06-29 02:48:48,399 main.py:57] epoch 3166, training loss: 8389.82, average training loss: 7701.59, base loss: 16065.59
[INFO 2017-06-29 02:48:51,351 main.py:57] epoch 3167, training loss: 7880.88, average training loss: 7700.64, base loss: 16064.20
[INFO 2017-06-29 02:48:54,354 main.py:57] epoch 3168, training loss: 7539.35, average training loss: 7701.13, base loss: 16064.01
[INFO 2017-06-29 02:48:57,401 main.py:57] epoch 3169, training loss: 6619.10, average training loss: 7699.83, base loss: 16062.97
[INFO 2017-06-29 02:49:00,441 main.py:57] epoch 3170, training loss: 7461.20, average training loss: 7698.91, base loss: 16062.35
[INFO 2017-06-29 02:49:03,468 main.py:57] epoch 3171, training loss: 7169.51, average training loss: 7699.15, base loss: 16061.98
[INFO 2017-06-29 02:49:06,557 main.py:57] epoch 3172, training loss: 8273.47, average training loss: 7698.72, base loss: 16062.37
[INFO 2017-06-29 02:49:09,587 main.py:57] epoch 3173, training loss: 7072.32, average training loss: 7698.39, base loss: 16061.37
[INFO 2017-06-29 02:49:12,592 main.py:57] epoch 3174, training loss: 7889.74, average training loss: 7697.12, base loss: 16062.44
[INFO 2017-06-29 02:49:15,557 main.py:57] epoch 3175, training loss: 6991.93, average training loss: 7696.75, base loss: 16062.08
[INFO 2017-06-29 02:49:18,647 main.py:57] epoch 3176, training loss: 7314.45, average training loss: 7696.26, base loss: 16061.65
[INFO 2017-06-29 02:49:21,811 main.py:57] epoch 3177, training loss: 7669.44, average training loss: 7695.64, base loss: 16060.93
[INFO 2017-06-29 02:49:24,877 main.py:57] epoch 3178, training loss: 7211.95, average training loss: 7694.73, base loss: 16061.09
[INFO 2017-06-29 02:49:27,898 main.py:57] epoch 3179, training loss: 8451.92, average training loss: 7695.15, base loss: 16062.90
[INFO 2017-06-29 02:49:30,922 main.py:57] epoch 3180, training loss: 7342.31, average training loss: 7695.33, base loss: 16062.27
[INFO 2017-06-29 02:49:33,973 main.py:57] epoch 3181, training loss: 7121.02, average training loss: 7694.52, base loss: 16061.33
[INFO 2017-06-29 02:49:36,977 main.py:57] epoch 3182, training loss: 7998.84, average training loss: 7695.22, base loss: 16061.99
[INFO 2017-06-29 02:49:40,044 main.py:57] epoch 3183, training loss: 7485.74, average training loss: 7694.88, base loss: 16061.42
[INFO 2017-06-29 02:49:43,084 main.py:57] epoch 3184, training loss: 7768.86, average training loss: 7695.21, base loss: 16062.07
[INFO 2017-06-29 02:49:46,114 main.py:57] epoch 3185, training loss: 8226.76, average training loss: 7695.94, base loss: 16062.95
[INFO 2017-06-29 02:49:49,221 main.py:57] epoch 3186, training loss: 7362.37, average training loss: 7696.37, base loss: 16062.76
[INFO 2017-06-29 02:49:52,310 main.py:57] epoch 3187, training loss: 8167.36, average training loss: 7696.85, base loss: 16062.81
[INFO 2017-06-29 02:49:55,394 main.py:57] epoch 3188, training loss: 7797.06, average training loss: 7697.15, base loss: 16063.49
[INFO 2017-06-29 02:49:58,492 main.py:57] epoch 3189, training loss: 7948.10, average training loss: 7697.46, base loss: 16064.05
[INFO 2017-06-29 02:50:01,576 main.py:57] epoch 3190, training loss: 7346.34, average training loss: 7696.75, base loss: 16064.25
[INFO 2017-06-29 02:50:04,650 main.py:57] epoch 3191, training loss: 8081.05, average training loss: 7697.71, base loss: 16064.84
[INFO 2017-06-29 02:50:07,710 main.py:57] epoch 3192, training loss: 7796.41, average training loss: 7698.55, base loss: 16065.09
[INFO 2017-06-29 02:50:10,825 main.py:57] epoch 3193, training loss: 7811.03, average training loss: 7699.53, base loss: 16065.03
[INFO 2017-06-29 02:50:13,841 main.py:57] epoch 3194, training loss: 8213.86, average training loss: 7699.85, base loss: 16066.32
[INFO 2017-06-29 02:50:16,896 main.py:57] epoch 3195, training loss: 7081.92, average training loss: 7700.01, base loss: 16066.27
[INFO 2017-06-29 02:50:19,920 main.py:57] epoch 3196, training loss: 7330.75, average training loss: 7699.87, base loss: 16065.33
[INFO 2017-06-29 02:50:22,949 main.py:57] epoch 3197, training loss: 8149.12, average training loss: 7700.60, base loss: 16066.19
[INFO 2017-06-29 02:50:26,048 main.py:57] epoch 3198, training loss: 7667.79, average training loss: 7700.14, base loss: 16066.42
[INFO 2017-06-29 02:50:29,073 main.py:57] epoch 3199, training loss: 6720.78, average training loss: 7698.13, base loss: 16066.56
[INFO 2017-06-29 02:50:29,073 main.py:59] epoch 3199, testing
[INFO 2017-06-29 02:50:41,642 main.py:104] average testing loss: 8176.96, base loss: 16498.49
[INFO 2017-06-29 02:50:41,642 main.py:105] improve_loss: 8321.54, improve_percent: 0.50
[INFO 2017-06-29 02:50:41,643 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:50:44,660 main.py:57] epoch 3200, training loss: 8021.47, average training loss: 7697.18, base loss: 16067.24
[INFO 2017-06-29 02:50:47,695 main.py:57] epoch 3201, training loss: 6827.26, average training loss: 7695.75, base loss: 16066.43
[INFO 2017-06-29 02:50:50,797 main.py:57] epoch 3202, training loss: 7633.19, average training loss: 7695.45, base loss: 16067.47
[INFO 2017-06-29 02:50:53,804 main.py:57] epoch 3203, training loss: 9104.93, average training loss: 7697.50, base loss: 16068.73
[INFO 2017-06-29 02:50:56,935 main.py:57] epoch 3204, training loss: 8170.64, average training loss: 7698.07, base loss: 16068.87
[INFO 2017-06-29 02:50:59,984 main.py:57] epoch 3205, training loss: 6974.21, average training loss: 7697.34, base loss: 16067.69
[INFO 2017-06-29 02:51:03,001 main.py:57] epoch 3206, training loss: 7675.43, average training loss: 7697.88, base loss: 16067.52
[INFO 2017-06-29 02:51:06,085 main.py:57] epoch 3207, training loss: 8607.05, average training loss: 7697.57, base loss: 16068.58
[INFO 2017-06-29 02:51:09,140 main.py:57] epoch 3208, training loss: 8253.97, average training loss: 7698.22, base loss: 16069.50
[INFO 2017-06-29 02:51:12,199 main.py:57] epoch 3209, training loss: 7352.20, average training loss: 7698.14, base loss: 16070.95
[INFO 2017-06-29 02:51:15,269 main.py:57] epoch 3210, training loss: 7162.63, average training loss: 7698.23, base loss: 16070.05
[INFO 2017-06-29 02:51:18,341 main.py:57] epoch 3211, training loss: 7395.37, average training loss: 7698.01, base loss: 16069.77
[INFO 2017-06-29 02:51:21,325 main.py:57] epoch 3212, training loss: 7348.15, average training loss: 7697.18, base loss: 16069.04
[INFO 2017-06-29 02:51:24,319 main.py:57] epoch 3213, training loss: 8215.10, average training loss: 7697.89, base loss: 16070.01
[INFO 2017-06-29 02:51:27,407 main.py:57] epoch 3214, training loss: 7597.11, average training loss: 7698.07, base loss: 16069.53
[INFO 2017-06-29 02:51:30,470 main.py:57] epoch 3215, training loss: 8466.02, average training loss: 7698.06, base loss: 16069.92
[INFO 2017-06-29 02:51:33,526 main.py:57] epoch 3216, training loss: 7526.13, average training loss: 7697.46, base loss: 16070.61
[INFO 2017-06-29 02:51:36,577 main.py:57] epoch 3217, training loss: 6750.72, average training loss: 7697.44, base loss: 16070.12
[INFO 2017-06-29 02:51:39,633 main.py:57] epoch 3218, training loss: 8619.61, average training loss: 7699.49, base loss: 16071.59
[INFO 2017-06-29 02:51:42,626 main.py:57] epoch 3219, training loss: 9088.96, average training loss: 7700.72, base loss: 16072.97
[INFO 2017-06-29 02:51:45,732 main.py:57] epoch 3220, training loss: 7365.03, average training loss: 7701.00, base loss: 16072.95
[INFO 2017-06-29 02:51:48,739 main.py:57] epoch 3221, training loss: 7459.81, average training loss: 7700.60, base loss: 16073.24
[INFO 2017-06-29 02:51:51,791 main.py:57] epoch 3222, training loss: 7598.06, average training loss: 7700.76, base loss: 16073.57
[INFO 2017-06-29 02:51:54,786 main.py:57] epoch 3223, training loss: 7358.47, average training loss: 7701.32, base loss: 16073.83
[INFO 2017-06-29 02:51:57,954 main.py:57] epoch 3224, training loss: 7739.10, average training loss: 7702.47, base loss: 16074.40
[INFO 2017-06-29 02:52:01,042 main.py:57] epoch 3225, training loss: 7529.54, average training loss: 7701.44, base loss: 16074.12
[INFO 2017-06-29 02:52:04,193 main.py:57] epoch 3226, training loss: 8054.02, average training loss: 7701.91, base loss: 16074.78
[INFO 2017-06-29 02:52:07,310 main.py:57] epoch 3227, training loss: 8560.94, average training loss: 7702.60, base loss: 16075.39
[INFO 2017-06-29 02:52:10,310 main.py:57] epoch 3228, training loss: 8400.99, average training loss: 7703.54, base loss: 16076.16
[INFO 2017-06-29 02:52:13,292 main.py:57] epoch 3229, training loss: 7540.84, average training loss: 7704.51, base loss: 16076.21
[INFO 2017-06-29 02:52:16,459 main.py:57] epoch 3230, training loss: 7989.27, average training loss: 7703.48, base loss: 16076.57
[INFO 2017-06-29 02:52:19,508 main.py:57] epoch 3231, training loss: 7740.59, average training loss: 7703.64, base loss: 16077.65
[INFO 2017-06-29 02:52:22,578 main.py:57] epoch 3232, training loss: 7690.18, average training loss: 7704.08, base loss: 16078.09
[INFO 2017-06-29 02:52:25,680 main.py:57] epoch 3233, training loss: 7209.47, average training loss: 7704.07, base loss: 16077.61
[INFO 2017-06-29 02:52:28,746 main.py:57] epoch 3234, training loss: 6863.82, average training loss: 7702.53, base loss: 16076.59
[INFO 2017-06-29 02:52:31,731 main.py:57] epoch 3235, training loss: 7414.64, average training loss: 7702.34, base loss: 16077.37
[INFO 2017-06-29 02:52:34,809 main.py:57] epoch 3236, training loss: 7534.60, average training loss: 7702.27, base loss: 16076.27
[INFO 2017-06-29 02:52:37,814 main.py:57] epoch 3237, training loss: 8301.26, average training loss: 7703.27, base loss: 16077.33
[INFO 2017-06-29 02:52:40,861 main.py:57] epoch 3238, training loss: 8195.38, average training loss: 7703.94, base loss: 16077.65
[INFO 2017-06-29 02:52:43,891 main.py:57] epoch 3239, training loss: 8368.53, average training loss: 7704.12, base loss: 16078.39
[INFO 2017-06-29 02:52:46,989 main.py:57] epoch 3240, training loss: 7069.27, average training loss: 7703.75, base loss: 16079.04
[INFO 2017-06-29 02:52:50,041 main.py:57] epoch 3241, training loss: 7378.16, average training loss: 7702.27, base loss: 16079.10
[INFO 2017-06-29 02:52:53,056 main.py:57] epoch 3242, training loss: 8272.61, average training loss: 7703.10, base loss: 16079.67
[INFO 2017-06-29 02:52:56,202 main.py:57] epoch 3243, training loss: 7309.77, average training loss: 7702.21, base loss: 16079.27
[INFO 2017-06-29 02:52:59,259 main.py:57] epoch 3244, training loss: 8328.29, average training loss: 7702.73, base loss: 16080.02
[INFO 2017-06-29 02:53:02,321 main.py:57] epoch 3245, training loss: 7512.48, average training loss: 7701.93, base loss: 16080.50
[INFO 2017-06-29 02:53:05,354 main.py:57] epoch 3246, training loss: 7236.13, average training loss: 7701.93, base loss: 16079.44
[INFO 2017-06-29 02:53:08,347 main.py:57] epoch 3247, training loss: 7082.88, average training loss: 7700.64, base loss: 16078.78
[INFO 2017-06-29 02:53:11,451 main.py:57] epoch 3248, training loss: 8132.45, average training loss: 7701.91, base loss: 16079.44
[INFO 2017-06-29 02:53:14,530 main.py:57] epoch 3249, training loss: 8240.16, average training loss: 7702.82, base loss: 16079.75
[INFO 2017-06-29 02:53:17,536 main.py:57] epoch 3250, training loss: 8079.38, average training loss: 7703.50, base loss: 16080.64
[INFO 2017-06-29 02:53:20,627 main.py:57] epoch 3251, training loss: 7126.34, average training loss: 7703.15, base loss: 16080.54
[INFO 2017-06-29 02:53:23,673 main.py:57] epoch 3252, training loss: 7608.89, average training loss: 7702.45, base loss: 16080.75
[INFO 2017-06-29 02:53:26,764 main.py:57] epoch 3253, training loss: 7506.25, average training loss: 7702.22, base loss: 16080.38
[INFO 2017-06-29 02:53:29,875 main.py:57] epoch 3254, training loss: 7898.94, average training loss: 7701.98, base loss: 16080.36
[INFO 2017-06-29 02:53:32,984 main.py:57] epoch 3255, training loss: 7424.26, average training loss: 7702.43, base loss: 16079.64
[INFO 2017-06-29 02:53:36,041 main.py:57] epoch 3256, training loss: 7373.13, average training loss: 7702.33, base loss: 16080.52
[INFO 2017-06-29 02:53:39,203 main.py:57] epoch 3257, training loss: 7625.89, average training loss: 7701.80, base loss: 16080.77
[INFO 2017-06-29 02:53:42,264 main.py:57] epoch 3258, training loss: 7988.35, average training loss: 7701.90, base loss: 16080.75
[INFO 2017-06-29 02:53:45,293 main.py:57] epoch 3259, training loss: 7588.10, average training loss: 7701.46, base loss: 16080.10
[INFO 2017-06-29 02:53:48,376 main.py:57] epoch 3260, training loss: 7228.20, average training loss: 7701.66, base loss: 16079.29
[INFO 2017-06-29 02:53:51,452 main.py:57] epoch 3261, training loss: 8306.26, average training loss: 7702.47, base loss: 16080.05
[INFO 2017-06-29 02:53:54,484 main.py:57] epoch 3262, training loss: 7032.44, average training loss: 7701.12, base loss: 16079.13
[INFO 2017-06-29 02:53:57,546 main.py:57] epoch 3263, training loss: 6841.84, average training loss: 7700.04, base loss: 16079.02
[INFO 2017-06-29 02:54:00,583 main.py:57] epoch 3264, training loss: 7277.46, average training loss: 7698.49, base loss: 16078.13
[INFO 2017-06-29 02:54:03,628 main.py:57] epoch 3265, training loss: 7361.18, average training loss: 7698.07, base loss: 16077.18
[INFO 2017-06-29 02:54:06,678 main.py:57] epoch 3266, training loss: 7816.22, average training loss: 7697.60, base loss: 16077.10
[INFO 2017-06-29 02:54:09,726 main.py:57] epoch 3267, training loss: 8318.45, average training loss: 7695.61, base loss: 16077.56
[INFO 2017-06-29 02:54:12,750 main.py:57] epoch 3268, training loss: 7394.80, average training loss: 7694.94, base loss: 16077.29
[INFO 2017-06-29 02:54:15,796 main.py:57] epoch 3269, training loss: 8695.76, average training loss: 7696.21, base loss: 16077.84
[INFO 2017-06-29 02:54:18,839 main.py:57] epoch 3270, training loss: 7205.37, average training loss: 7696.04, base loss: 16077.28
[INFO 2017-06-29 02:54:21,905 main.py:57] epoch 3271, training loss: 8258.71, average training loss: 7696.61, base loss: 16078.00
[INFO 2017-06-29 02:54:24,963 main.py:57] epoch 3272, training loss: 7049.30, average training loss: 7695.99, base loss: 16078.44
[INFO 2017-06-29 02:54:27,981 main.py:57] epoch 3273, training loss: 8346.61, average training loss: 7695.39, base loss: 16079.02
[INFO 2017-06-29 02:54:31,056 main.py:57] epoch 3274, training loss: 7287.66, average training loss: 7695.60, base loss: 16079.20
[INFO 2017-06-29 02:54:34,081 main.py:57] epoch 3275, training loss: 8881.01, average training loss: 7697.14, base loss: 16080.60
[INFO 2017-06-29 02:54:37,172 main.py:57] epoch 3276, training loss: 9980.59, average training loss: 7700.08, base loss: 16082.94
[INFO 2017-06-29 02:54:40,225 main.py:57] epoch 3277, training loss: 7606.15, average training loss: 7698.56, base loss: 16083.15
[INFO 2017-06-29 02:54:43,283 main.py:57] epoch 3278, training loss: 8013.59, average training loss: 7698.70, base loss: 16084.26
[INFO 2017-06-29 02:54:46,257 main.py:57] epoch 3279, training loss: 7582.93, average training loss: 7697.88, base loss: 16084.06
[INFO 2017-06-29 02:54:49,374 main.py:57] epoch 3280, training loss: 8022.12, average training loss: 7698.88, base loss: 16084.95
[INFO 2017-06-29 02:54:52,424 main.py:57] epoch 3281, training loss: 6742.82, average training loss: 7698.02, base loss: 16084.71
[INFO 2017-06-29 02:54:55,526 main.py:57] epoch 3282, training loss: 7073.88, average training loss: 7698.24, base loss: 16083.27
[INFO 2017-06-29 02:54:58,576 main.py:57] epoch 3283, training loss: 7091.15, average training loss: 7697.29, base loss: 16082.10
[INFO 2017-06-29 02:55:01,703 main.py:57] epoch 3284, training loss: 8146.05, average training loss: 7697.23, base loss: 16082.19
[INFO 2017-06-29 02:55:04,776 main.py:57] epoch 3285, training loss: 7414.12, average training loss: 7695.81, base loss: 16081.27
[INFO 2017-06-29 02:55:07,820 main.py:57] epoch 3286, training loss: 6947.53, average training loss: 7694.75, base loss: 16079.83
[INFO 2017-06-29 02:55:10,869 main.py:57] epoch 3287, training loss: 8828.88, average training loss: 7696.17, base loss: 16081.11
[INFO 2017-06-29 02:55:13,883 main.py:57] epoch 3288, training loss: 8653.96, average training loss: 7697.56, base loss: 16082.42
[INFO 2017-06-29 02:55:16,905 main.py:57] epoch 3289, training loss: 7271.80, average training loss: 7697.16, base loss: 16083.02
[INFO 2017-06-29 02:55:19,974 main.py:57] epoch 3290, training loss: 7522.29, average training loss: 7697.36, base loss: 16083.22
[INFO 2017-06-29 02:55:23,080 main.py:57] epoch 3291, training loss: 7434.81, average training loss: 7697.39, base loss: 16084.17
[INFO 2017-06-29 02:55:26,136 main.py:57] epoch 3292, training loss: 7553.46, average training loss: 7697.69, base loss: 16083.47
[INFO 2017-06-29 02:55:29,143 main.py:57] epoch 3293, training loss: 7346.92, average training loss: 7697.21, base loss: 16082.84
[INFO 2017-06-29 02:55:32,268 main.py:57] epoch 3294, training loss: 7357.04, average training loss: 7697.37, base loss: 16082.92
[INFO 2017-06-29 02:55:35,306 main.py:57] epoch 3295, training loss: 6990.61, average training loss: 7695.92, base loss: 16082.08
[INFO 2017-06-29 02:55:38,379 main.py:57] epoch 3296, training loss: 7251.77, average training loss: 7696.76, base loss: 16081.45
[INFO 2017-06-29 02:55:41,450 main.py:57] epoch 3297, training loss: 8225.30, average training loss: 7697.69, base loss: 16081.56
[INFO 2017-06-29 02:55:44,533 main.py:57] epoch 3298, training loss: 7074.66, average training loss: 7697.35, base loss: 16081.50
[INFO 2017-06-29 02:55:47,637 main.py:57] epoch 3299, training loss: 7619.14, average training loss: 7697.97, base loss: 16081.66
[INFO 2017-06-29 02:55:47,637 main.py:59] epoch 3299, testing
[INFO 2017-06-29 02:56:00,264 main.py:104] average testing loss: 8219.71, base loss: 16523.52
[INFO 2017-06-29 02:56:00,265 main.py:105] improve_loss: 8303.81, improve_percent: 0.50
[INFO 2017-06-29 02:56:00,266 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 02:56:03,345 main.py:57] epoch 3300, training loss: 7656.33, average training loss: 7697.92, base loss: 16081.01
[INFO 2017-06-29 02:56:06,413 main.py:57] epoch 3301, training loss: 9164.77, average training loss: 7699.86, base loss: 16081.84
[INFO 2017-06-29 02:56:09,424 main.py:57] epoch 3302, training loss: 6930.11, average training loss: 7698.11, base loss: 16081.36
[INFO 2017-06-29 02:56:12,534 main.py:57] epoch 3303, training loss: 7702.54, average training loss: 7698.32, base loss: 16082.35
[INFO 2017-06-29 02:56:15,619 main.py:57] epoch 3304, training loss: 8878.78, average training loss: 7698.88, base loss: 16083.38
[INFO 2017-06-29 02:56:18,719 main.py:57] epoch 3305, training loss: 7870.78, average training loss: 7699.37, base loss: 16083.83
[INFO 2017-06-29 02:56:21,782 main.py:57] epoch 3306, training loss: 8053.28, average training loss: 7700.48, base loss: 16083.79
[INFO 2017-06-29 02:56:24,827 main.py:57] epoch 3307, training loss: 6886.83, average training loss: 7698.62, base loss: 16083.20
[INFO 2017-06-29 02:56:27,932 main.py:57] epoch 3308, training loss: 8124.72, average training loss: 7699.60, base loss: 16083.63
[INFO 2017-06-29 02:56:30,978 main.py:57] epoch 3309, training loss: 6968.52, average training loss: 7698.45, base loss: 16083.00
[INFO 2017-06-29 02:56:34,007 main.py:57] epoch 3310, training loss: 8714.69, average training loss: 7700.20, base loss: 16082.79
[INFO 2017-06-29 02:56:37,066 main.py:57] epoch 3311, training loss: 6782.77, average training loss: 7699.69, base loss: 16081.72
[INFO 2017-06-29 02:56:40,188 main.py:57] epoch 3312, training loss: 7937.18, average training loss: 7701.08, base loss: 16082.66
[INFO 2017-06-29 02:56:43,233 main.py:57] epoch 3313, training loss: 7253.73, average training loss: 7700.25, base loss: 16082.55
[INFO 2017-06-29 02:56:46,282 main.py:57] epoch 3314, training loss: 8433.45, average training loss: 7701.97, base loss: 16083.53
[INFO 2017-06-29 02:56:49,362 main.py:57] epoch 3315, training loss: 7435.10, average training loss: 7702.58, base loss: 16082.73
[INFO 2017-06-29 02:56:52,395 main.py:57] epoch 3316, training loss: 7730.39, average training loss: 7703.24, base loss: 16082.75
[INFO 2017-06-29 02:56:55,422 main.py:57] epoch 3317, training loss: 9106.49, average training loss: 7703.91, base loss: 16083.45
[INFO 2017-06-29 02:56:58,530 main.py:57] epoch 3318, training loss: 7927.15, average training loss: 7702.87, base loss: 16083.68
[INFO 2017-06-29 02:57:01,538 main.py:57] epoch 3319, training loss: 7275.19, average training loss: 7702.53, base loss: 16083.75
[INFO 2017-06-29 02:57:04,600 main.py:57] epoch 3320, training loss: 7621.91, average training loss: 7702.23, base loss: 16084.67
[INFO 2017-06-29 02:57:07,677 main.py:57] epoch 3321, training loss: 6822.79, average training loss: 7701.38, base loss: 16084.12
[INFO 2017-06-29 02:57:10,672 main.py:57] epoch 3322, training loss: 6805.30, average training loss: 7700.18, base loss: 16083.04
[INFO 2017-06-29 02:57:13,724 main.py:57] epoch 3323, training loss: 9198.53, average training loss: 7702.95, base loss: 16083.63
[INFO 2017-06-29 02:57:16,756 main.py:57] epoch 3324, training loss: 7855.65, average training loss: 7702.17, base loss: 16084.68
[INFO 2017-06-29 02:57:19,856 main.py:57] epoch 3325, training loss: 6988.82, average training loss: 7701.67, base loss: 16084.94
[INFO 2017-06-29 02:57:22,896 main.py:57] epoch 3326, training loss: 9522.17, average training loss: 7703.66, base loss: 16086.07
[INFO 2017-06-29 02:57:25,986 main.py:57] epoch 3327, training loss: 8644.38, average training loss: 7704.61, base loss: 16087.20
[INFO 2017-06-29 02:57:29,076 main.py:57] epoch 3328, training loss: 7389.77, average training loss: 7703.90, base loss: 16087.29
[INFO 2017-06-29 02:57:32,171 main.py:57] epoch 3329, training loss: 8041.10, average training loss: 7704.21, base loss: 16088.61
[INFO 2017-06-29 02:57:35,234 main.py:57] epoch 3330, training loss: 9781.76, average training loss: 7706.55, base loss: 16090.28
[INFO 2017-06-29 02:57:38,213 main.py:57] epoch 3331, training loss: 6968.80, average training loss: 7706.39, base loss: 16090.06
[INFO 2017-06-29 02:57:41,259 main.py:57] epoch 3332, training loss: 7861.48, average training loss: 7707.15, base loss: 16090.50
[INFO 2017-06-29 02:57:44,312 main.py:57] epoch 3333, training loss: 6798.74, average training loss: 7706.46, base loss: 16089.43
[INFO 2017-06-29 02:57:47,382 main.py:57] epoch 3334, training loss: 8030.81, average training loss: 7707.06, base loss: 16089.58
[INFO 2017-06-29 02:57:50,470 main.py:57] epoch 3335, training loss: 7135.45, average training loss: 7706.32, base loss: 16088.78
[INFO 2017-06-29 02:57:53,489 main.py:57] epoch 3336, training loss: 8039.64, average training loss: 7706.32, base loss: 16089.32
[INFO 2017-06-29 02:57:56,577 main.py:57] epoch 3337, training loss: 7813.59, average training loss: 7705.72, base loss: 16090.27
[INFO 2017-06-29 02:57:59,625 main.py:57] epoch 3338, training loss: 8187.39, average training loss: 7706.07, base loss: 16090.24
[INFO 2017-06-29 02:58:02,677 main.py:57] epoch 3339, training loss: 7973.43, average training loss: 7705.59, base loss: 16090.88
[INFO 2017-06-29 02:58:05,710 main.py:57] epoch 3340, training loss: 6635.14, average training loss: 7703.67, base loss: 16090.31
[INFO 2017-06-29 02:58:08,839 main.py:57] epoch 3341, training loss: 8903.34, average training loss: 7703.96, base loss: 16091.62
[INFO 2017-06-29 02:58:11,889 main.py:57] epoch 3342, training loss: 8010.51, average training loss: 7704.64, base loss: 16092.23
[INFO 2017-06-29 02:58:15,042 main.py:57] epoch 3343, training loss: 7513.17, average training loss: 7704.50, base loss: 16091.64
[INFO 2017-06-29 02:58:18,106 main.py:57] epoch 3344, training loss: 8215.46, average training loss: 7704.91, base loss: 16091.66
[INFO 2017-06-29 02:58:21,174 main.py:57] epoch 3345, training loss: 8211.69, average training loss: 7704.63, base loss: 16092.55
[INFO 2017-06-29 02:58:24,221 main.py:57] epoch 3346, training loss: 7126.48, average training loss: 7703.33, base loss: 16092.31
[INFO 2017-06-29 02:58:27,282 main.py:57] epoch 3347, training loss: 7988.48, average training loss: 7703.59, base loss: 16092.37
[INFO 2017-06-29 02:58:30,296 main.py:57] epoch 3348, training loss: 6971.40, average training loss: 7702.61, base loss: 16091.43
[INFO 2017-06-29 02:58:33,358 main.py:57] epoch 3349, training loss: 6964.38, average training loss: 7701.65, base loss: 16090.06
[INFO 2017-06-29 02:58:36,429 main.py:57] epoch 3350, training loss: 8054.32, average training loss: 7700.18, base loss: 16090.55
[INFO 2017-06-29 02:58:39,514 main.py:57] epoch 3351, training loss: 8837.93, average training loss: 7701.13, base loss: 16092.12
[INFO 2017-06-29 02:58:42,565 main.py:57] epoch 3352, training loss: 7534.96, average training loss: 7701.28, base loss: 16092.03
[INFO 2017-06-29 02:58:45,631 main.py:57] epoch 3353, training loss: 7596.82, average training loss: 7701.03, base loss: 16091.55
[INFO 2017-06-29 02:58:48,694 main.py:57] epoch 3354, training loss: 8732.96, average training loss: 7702.38, base loss: 16093.10
[INFO 2017-06-29 02:58:51,720 main.py:57] epoch 3355, training loss: 8139.02, average training loss: 7702.58, base loss: 16093.23
[INFO 2017-06-29 02:58:54,806 main.py:57] epoch 3356, training loss: 8446.00, average training loss: 7703.75, base loss: 16093.86
[INFO 2017-06-29 02:58:57,904 main.py:57] epoch 3357, training loss: 6986.17, average training loss: 7702.68, base loss: 16093.96
[INFO 2017-06-29 02:59:01,117 main.py:57] epoch 3358, training loss: 8332.90, average training loss: 7702.33, base loss: 16093.86
[INFO 2017-06-29 02:59:04,141 main.py:57] epoch 3359, training loss: 7291.91, average training loss: 7700.83, base loss: 16093.05
[INFO 2017-06-29 02:59:07,133 main.py:57] epoch 3360, training loss: 6820.84, average training loss: 7699.71, base loss: 16092.53
[INFO 2017-06-29 02:59:10,235 main.py:57] epoch 3361, training loss: 8326.83, average training loss: 7700.56, base loss: 16092.25
[INFO 2017-06-29 02:59:13,235 main.py:57] epoch 3362, training loss: 6613.72, average training loss: 7699.07, base loss: 16090.72
[INFO 2017-06-29 02:59:16,280 main.py:57] epoch 3363, training loss: 6588.61, average training loss: 7698.08, base loss: 16088.94
[INFO 2017-06-29 02:59:19,387 main.py:57] epoch 3364, training loss: 7960.78, average training loss: 7698.05, base loss: 16089.52
[INFO 2017-06-29 02:59:22,385 main.py:57] epoch 3365, training loss: 8331.89, average training loss: 7698.85, base loss: 16090.00
[INFO 2017-06-29 02:59:25,411 main.py:57] epoch 3366, training loss: 7403.87, average training loss: 7698.78, base loss: 16089.79
[INFO 2017-06-29 02:59:28,399 main.py:57] epoch 3367, training loss: 8533.53, average training loss: 7699.67, base loss: 16090.84
[INFO 2017-06-29 02:59:31,482 main.py:57] epoch 3368, training loss: 8194.99, average training loss: 7701.03, base loss: 16089.74
[INFO 2017-06-29 02:59:34,591 main.py:57] epoch 3369, training loss: 7747.40, average training loss: 7701.23, base loss: 16088.15
[INFO 2017-06-29 02:59:37,655 main.py:57] epoch 3370, training loss: 7336.51, average training loss: 7701.84, base loss: 16088.45
[INFO 2017-06-29 02:59:40,676 main.py:57] epoch 3371, training loss: 6742.85, average training loss: 7701.22, base loss: 16087.28
[INFO 2017-06-29 02:59:43,705 main.py:57] epoch 3372, training loss: 6500.06, average training loss: 7699.06, base loss: 16085.90
[INFO 2017-06-29 02:59:46,805 main.py:57] epoch 3373, training loss: 6859.43, average training loss: 7698.37, base loss: 16084.51
[INFO 2017-06-29 02:59:49,895 main.py:57] epoch 3374, training loss: 7637.33, average training loss: 7698.06, base loss: 16084.62
[INFO 2017-06-29 02:59:52,942 main.py:57] epoch 3375, training loss: 7158.26, average training loss: 7697.49, base loss: 16084.63
[INFO 2017-06-29 02:59:55,958 main.py:57] epoch 3376, training loss: 8012.71, average training loss: 7698.76, base loss: 16084.92
[INFO 2017-06-29 02:59:59,029 main.py:57] epoch 3377, training loss: 7222.10, average training loss: 7698.55, base loss: 16084.00
[INFO 2017-06-29 03:00:02,066 main.py:57] epoch 3378, training loss: 7782.19, average training loss: 7698.26, base loss: 16083.74
[INFO 2017-06-29 03:00:05,119 main.py:57] epoch 3379, training loss: 8232.79, average training loss: 7697.86, base loss: 16084.08
[INFO 2017-06-29 03:00:08,160 main.py:57] epoch 3380, training loss: 8180.95, average training loss: 7697.68, base loss: 16085.18
[INFO 2017-06-29 03:00:11,191 main.py:57] epoch 3381, training loss: 7955.87, average training loss: 7697.95, base loss: 16086.13
[INFO 2017-06-29 03:00:14,273 main.py:57] epoch 3382, training loss: 8068.25, average training loss: 7698.82, base loss: 16085.55
[INFO 2017-06-29 03:00:17,327 main.py:57] epoch 3383, training loss: 7635.15, average training loss: 7698.59, base loss: 16086.13
[INFO 2017-06-29 03:00:20,430 main.py:57] epoch 3384, training loss: 8206.91, average training loss: 7698.75, base loss: 16086.97
[INFO 2017-06-29 03:00:23,527 main.py:57] epoch 3385, training loss: 7676.84, average training loss: 7699.87, base loss: 16087.86
[INFO 2017-06-29 03:00:26,588 main.py:57] epoch 3386, training loss: 7556.70, average training loss: 7699.86, base loss: 16087.39
[INFO 2017-06-29 03:00:29,641 main.py:57] epoch 3387, training loss: 8236.85, average training loss: 7701.37, base loss: 16088.23
[INFO 2017-06-29 03:00:32,678 main.py:57] epoch 3388, training loss: 8138.97, average training loss: 7701.67, base loss: 16088.50
[INFO 2017-06-29 03:00:35,742 main.py:57] epoch 3389, training loss: 7209.03, average training loss: 7701.16, base loss: 16087.99
[INFO 2017-06-29 03:00:38,800 main.py:57] epoch 3390, training loss: 8060.34, average training loss: 7702.31, base loss: 16089.26
[INFO 2017-06-29 03:00:41,845 main.py:57] epoch 3391, training loss: 8111.87, average training loss: 7701.63, base loss: 16089.45
[INFO 2017-06-29 03:00:44,845 main.py:57] epoch 3392, training loss: 8581.58, average training loss: 7703.24, base loss: 16089.62
[INFO 2017-06-29 03:00:47,889 main.py:57] epoch 3393, training loss: 6877.68, average training loss: 7703.09, base loss: 16088.54
[INFO 2017-06-29 03:00:50,929 main.py:57] epoch 3394, training loss: 8034.69, average training loss: 7703.28, base loss: 16088.66
[INFO 2017-06-29 03:00:53,965 main.py:57] epoch 3395, training loss: 8199.56, average training loss: 7703.45, base loss: 16089.04
[INFO 2017-06-29 03:00:57,014 main.py:57] epoch 3396, training loss: 7810.63, average training loss: 7704.13, base loss: 16088.68
[INFO 2017-06-29 03:01:00,060 main.py:57] epoch 3397, training loss: 7517.79, average training loss: 7703.18, base loss: 16087.85
[INFO 2017-06-29 03:01:03,111 main.py:57] epoch 3398, training loss: 7240.22, average training loss: 7701.87, base loss: 16087.00
[INFO 2017-06-29 03:01:06,140 main.py:57] epoch 3399, training loss: 7007.72, average training loss: 7700.71, base loss: 16086.39
[INFO 2017-06-29 03:01:06,141 main.py:59] epoch 3399, testing
[INFO 2017-06-29 03:01:18,957 main.py:104] average testing loss: 8588.54, base loss: 17368.04
[INFO 2017-06-29 03:01:18,957 main.py:105] improve_loss: 8779.51, improve_percent: 0.51
[INFO 2017-06-29 03:01:18,958 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 03:01:21,992 main.py:57] epoch 3400, training loss: 6947.51, average training loss: 7699.69, base loss: 16086.56
[INFO 2017-06-29 03:01:25,051 main.py:57] epoch 3401, training loss: 6804.41, average training loss: 7699.85, base loss: 16084.88
[INFO 2017-06-29 03:01:28,041 main.py:57] epoch 3402, training loss: 7092.29, average training loss: 7699.68, base loss: 16084.32
[INFO 2017-06-29 03:01:31,030 main.py:57] epoch 3403, training loss: 7562.17, average training loss: 7698.37, base loss: 16084.42
[INFO 2017-06-29 03:01:34,077 main.py:57] epoch 3404, training loss: 7319.47, average training loss: 7697.52, base loss: 16084.32
[INFO 2017-06-29 03:01:37,128 main.py:57] epoch 3405, training loss: 8520.04, average training loss: 7698.40, base loss: 16085.03
[INFO 2017-06-29 03:01:40,225 main.py:57] epoch 3406, training loss: 7361.48, average training loss: 7698.47, base loss: 16084.19
[INFO 2017-06-29 03:01:43,264 main.py:57] epoch 3407, training loss: 7980.05, average training loss: 7698.79, base loss: 16084.34
[INFO 2017-06-29 03:01:46,331 main.py:57] epoch 3408, training loss: 7730.43, average training loss: 7698.80, base loss: 16084.92
[INFO 2017-06-29 03:01:49,343 main.py:57] epoch 3409, training loss: 10363.48, average training loss: 7701.47, base loss: 16087.65
[INFO 2017-06-29 03:01:52,417 main.py:57] epoch 3410, training loss: 8872.73, average training loss: 7702.72, base loss: 16089.30
[INFO 2017-06-29 03:01:55,485 main.py:57] epoch 3411, training loss: 8359.31, average training loss: 7704.41, base loss: 16090.85
[INFO 2017-06-29 03:01:58,497 main.py:57] epoch 3412, training loss: 7053.44, average training loss: 7703.30, base loss: 16089.91
[INFO 2017-06-29 03:02:01,678 main.py:57] epoch 3413, training loss: 8292.65, average training loss: 7704.27, base loss: 16090.80
[INFO 2017-06-29 03:02:04,724 main.py:57] epoch 3414, training loss: 8534.32, average training loss: 7705.33, base loss: 16091.32
[INFO 2017-06-29 03:02:07,766 main.py:57] epoch 3415, training loss: 7758.91, average training loss: 7706.08, base loss: 16091.26
[INFO 2017-06-29 03:02:10,820 main.py:57] epoch 3416, training loss: 7640.05, average training loss: 7705.47, base loss: 16091.14
[INFO 2017-06-29 03:02:13,894 main.py:57] epoch 3417, training loss: 7009.64, average training loss: 7705.73, base loss: 16090.15
[INFO 2017-06-29 03:02:16,923 main.py:57] epoch 3418, training loss: 7558.67, average training loss: 7704.98, base loss: 16090.35
[INFO 2017-06-29 03:02:19,933 main.py:57] epoch 3419, training loss: 7776.68, average training loss: 7705.10, base loss: 16090.86
[INFO 2017-06-29 03:02:23,099 main.py:57] epoch 3420, training loss: 8419.52, average training loss: 7705.74, base loss: 16092.26
[INFO 2017-06-29 03:02:26,107 main.py:57] epoch 3421, training loss: 6908.00, average training loss: 7706.08, base loss: 16091.74
[INFO 2017-06-29 03:02:29,119 main.py:57] epoch 3422, training loss: 8254.12, average training loss: 7705.70, base loss: 16091.88
[INFO 2017-06-29 03:02:32,105 main.py:57] epoch 3423, training loss: 8091.19, average training loss: 7706.38, base loss: 16091.04
[INFO 2017-06-29 03:02:35,104 main.py:57] epoch 3424, training loss: 6757.57, average training loss: 7705.82, base loss: 16089.58
[INFO 2017-06-29 03:02:38,215 main.py:57] epoch 3425, training loss: 7925.77, average training loss: 7706.54, base loss: 16089.71
[INFO 2017-06-29 03:02:41,295 main.py:57] epoch 3426, training loss: 7112.49, average training loss: 7705.95, base loss: 16088.93
[INFO 2017-06-29 03:02:44,401 main.py:57] epoch 3427, training loss: 7610.23, average training loss: 7705.85, base loss: 16088.46
[INFO 2017-06-29 03:02:47,521 main.py:57] epoch 3428, training loss: 8013.76, average training loss: 7706.21, base loss: 16089.20
[INFO 2017-06-29 03:02:50,565 main.py:57] epoch 3429, training loss: 7793.18, average training loss: 7705.52, base loss: 16089.40
[INFO 2017-06-29 03:02:53,626 main.py:57] epoch 3430, training loss: 8672.05, average training loss: 7707.14, base loss: 16090.95
[INFO 2017-06-29 03:02:56,694 main.py:57] epoch 3431, training loss: 7897.06, average training loss: 7707.43, base loss: 16091.54
[INFO 2017-06-29 03:02:59,748 main.py:57] epoch 3432, training loss: 7743.17, average training loss: 7707.70, base loss: 16090.41
[INFO 2017-06-29 03:03:02,803 main.py:57] epoch 3433, training loss: 7279.54, average training loss: 7707.68, base loss: 16089.02
[INFO 2017-06-29 03:03:05,803 main.py:57] epoch 3434, training loss: 6843.06, average training loss: 7705.44, base loss: 16088.07
[INFO 2017-06-29 03:03:08,915 main.py:57] epoch 3435, training loss: 7094.92, average training loss: 7704.83, base loss: 16087.95
[INFO 2017-06-29 03:03:11,997 main.py:57] epoch 3436, training loss: 7959.46, average training loss: 7705.89, base loss: 16088.32
[INFO 2017-06-29 03:03:15,072 main.py:57] epoch 3437, training loss: 8004.40, average training loss: 7706.63, base loss: 16087.83
[INFO 2017-06-29 03:03:18,161 main.py:57] epoch 3438, training loss: 8867.01, average training loss: 7708.00, base loss: 16089.39
[INFO 2017-06-29 03:03:21,190 main.py:57] epoch 3439, training loss: 7803.34, average training loss: 7708.02, base loss: 16090.52
[INFO 2017-06-29 03:03:24,197 main.py:57] epoch 3440, training loss: 7710.12, average training loss: 7708.34, base loss: 16090.85
[INFO 2017-06-29 03:03:27,206 main.py:57] epoch 3441, training loss: 6351.44, average training loss: 7706.47, base loss: 16089.15
[INFO 2017-06-29 03:03:30,261 main.py:57] epoch 3442, training loss: 6763.50, average training loss: 7704.74, base loss: 16089.33
[INFO 2017-06-29 03:03:33,224 main.py:57] epoch 3443, training loss: 6826.12, average training loss: 7703.94, base loss: 16088.38
[INFO 2017-06-29 03:03:36,300 main.py:57] epoch 3444, training loss: 7175.20, average training loss: 7703.04, base loss: 16088.24
[INFO 2017-06-29 03:03:39,340 main.py:57] epoch 3445, training loss: 8108.75, average training loss: 7702.79, base loss: 16088.82
[INFO 2017-06-29 03:03:42,424 main.py:57] epoch 3446, training loss: 8938.13, average training loss: 7703.61, base loss: 16089.16
[INFO 2017-06-29 03:03:45,465 main.py:57] epoch 3447, training loss: 7938.63, average training loss: 7703.81, base loss: 16089.56
[INFO 2017-06-29 03:03:48,423 main.py:57] epoch 3448, training loss: 7058.92, average training loss: 7703.16, base loss: 16088.72
[INFO 2017-06-29 03:03:51,463 main.py:57] epoch 3449, training loss: 7731.64, average training loss: 7703.03, base loss: 16089.54
[INFO 2017-06-29 03:03:54,519 main.py:57] epoch 3450, training loss: 7494.12, average training loss: 7703.80, base loss: 16089.01
[INFO 2017-06-29 03:03:57,526 main.py:57] epoch 3451, training loss: 7392.22, average training loss: 7704.42, base loss: 16089.79
[INFO 2017-06-29 03:04:00,656 main.py:57] epoch 3452, training loss: 7594.70, average training loss: 7704.70, base loss: 16089.87
[INFO 2017-06-29 03:04:03,685 main.py:57] epoch 3453, training loss: 8397.05, average training loss: 7705.86, base loss: 16090.95
[INFO 2017-06-29 03:04:06,860 main.py:57] epoch 3454, training loss: 8408.51, average training loss: 7706.84, base loss: 16091.10
[INFO 2017-06-29 03:04:09,959 main.py:57] epoch 3455, training loss: 7236.39, average training loss: 7705.81, base loss: 16089.38
[INFO 2017-06-29 03:04:12,980 main.py:57] epoch 3456, training loss: 6892.31, average training loss: 7703.90, base loss: 16088.61
[INFO 2017-06-29 03:04:16,081 main.py:57] epoch 3457, training loss: 8862.39, average training loss: 7704.84, base loss: 16090.20
[INFO 2017-06-29 03:04:19,123 main.py:57] epoch 3458, training loss: 7751.40, average training loss: 7704.56, base loss: 16090.82
[INFO 2017-06-29 03:04:22,238 main.py:57] epoch 3459, training loss: 7355.99, average training loss: 7703.92, base loss: 16090.69
[INFO 2017-06-29 03:04:25,305 main.py:57] epoch 3460, training loss: 7641.02, average training loss: 7704.13, base loss: 16091.17
[INFO 2017-06-29 03:04:28,385 main.py:57] epoch 3461, training loss: 7115.86, average training loss: 7704.12, base loss: 16090.72
[INFO 2017-06-29 03:04:31,411 main.py:57] epoch 3462, training loss: 7766.33, average training loss: 7704.66, base loss: 16091.21
[INFO 2017-06-29 03:04:34,478 main.py:57] epoch 3463, training loss: 6879.82, average training loss: 7702.56, base loss: 16091.12
[INFO 2017-06-29 03:04:37,540 main.py:57] epoch 3464, training loss: 7352.21, average training loss: 7702.55, base loss: 16091.03
[INFO 2017-06-29 03:04:40,641 main.py:57] epoch 3465, training loss: 7811.98, average training loss: 7703.89, base loss: 16091.51
[INFO 2017-06-29 03:04:43,634 main.py:57] epoch 3466, training loss: 7370.15, average training loss: 7703.43, base loss: 16091.90
[INFO 2017-06-29 03:04:46,668 main.py:57] epoch 3467, training loss: 6736.35, average training loss: 7703.39, base loss: 16090.75
[INFO 2017-06-29 03:04:49,759 main.py:57] epoch 3468, training loss: 7815.60, average training loss: 7703.16, base loss: 16091.42
[INFO 2017-06-29 03:04:52,795 main.py:57] epoch 3469, training loss: 6627.41, average training loss: 7702.39, base loss: 16091.54
[INFO 2017-06-29 03:04:55,920 main.py:57] epoch 3470, training loss: 7585.03, average training loss: 7702.62, base loss: 16091.29
[INFO 2017-06-29 03:04:58,936 main.py:57] epoch 3471, training loss: 9642.09, average training loss: 7705.59, base loss: 16093.03
[INFO 2017-06-29 03:05:01,960 main.py:57] epoch 3472, training loss: 8888.43, average training loss: 7707.61, base loss: 16094.31
[INFO 2017-06-29 03:05:05,000 main.py:57] epoch 3473, training loss: 6904.15, average training loss: 7706.35, base loss: 16094.18
[INFO 2017-06-29 03:05:08,050 main.py:57] epoch 3474, training loss: 7930.66, average training loss: 7706.34, base loss: 16095.50
[INFO 2017-06-29 03:05:11,117 main.py:57] epoch 3475, training loss: 7242.25, average training loss: 7707.11, base loss: 16095.59
[INFO 2017-06-29 03:05:14,195 main.py:57] epoch 3476, training loss: 7496.17, average training loss: 7706.52, base loss: 16095.80
[INFO 2017-06-29 03:05:17,233 main.py:57] epoch 3477, training loss: 7854.36, average training loss: 7706.10, base loss: 16095.79
[INFO 2017-06-29 03:05:20,267 main.py:57] epoch 3478, training loss: 8824.58, average training loss: 7706.88, base loss: 16096.65
[INFO 2017-06-29 03:05:23,289 main.py:57] epoch 3479, training loss: 7123.21, average training loss: 7704.67, base loss: 16095.58
[INFO 2017-06-29 03:05:26,316 main.py:57] epoch 3480, training loss: 7907.73, average training loss: 7704.65, base loss: 16096.04
[INFO 2017-06-29 03:05:29,371 main.py:57] epoch 3481, training loss: 8487.37, average training loss: 7704.81, base loss: 16096.10
[INFO 2017-06-29 03:05:32,547 main.py:57] epoch 3482, training loss: 7510.18, average training loss: 7703.71, base loss: 16095.79
[INFO 2017-06-29 03:05:35,595 main.py:57] epoch 3483, training loss: 8343.27, average training loss: 7704.87, base loss: 16096.02
[INFO 2017-06-29 03:05:38,717 main.py:57] epoch 3484, training loss: 8440.03, average training loss: 7705.75, base loss: 16097.32
[INFO 2017-06-29 03:05:41,774 main.py:57] epoch 3485, training loss: 7663.97, average training loss: 7706.57, base loss: 16097.67
[INFO 2017-06-29 03:05:44,796 main.py:57] epoch 3486, training loss: 7203.00, average training loss: 7706.82, base loss: 16096.82
[INFO 2017-06-29 03:05:47,923 main.py:57] epoch 3487, training loss: 7544.31, average training loss: 7706.16, base loss: 16096.44
[INFO 2017-06-29 03:05:51,020 main.py:57] epoch 3488, training loss: 7082.42, average training loss: 7705.46, base loss: 16095.50
[INFO 2017-06-29 03:05:54,137 main.py:57] epoch 3489, training loss: 7394.41, average training loss: 7706.19, base loss: 16095.60
[INFO 2017-06-29 03:05:57,181 main.py:57] epoch 3490, training loss: 7907.08, average training loss: 7707.17, base loss: 16095.72
[INFO 2017-06-29 03:06:00,295 main.py:57] epoch 3491, training loss: 7380.19, average training loss: 7707.69, base loss: 16095.47
[INFO 2017-06-29 03:06:03,344 main.py:57] epoch 3492, training loss: 7019.80, average training loss: 7707.42, base loss: 16095.41
[INFO 2017-06-29 03:06:06,423 main.py:57] epoch 3493, training loss: 7633.92, average training loss: 7707.62, base loss: 16096.00
[INFO 2017-06-29 03:06:09,472 main.py:57] epoch 3494, training loss: 7838.29, average training loss: 7708.06, base loss: 16096.34
[INFO 2017-06-29 03:06:12,545 main.py:57] epoch 3495, training loss: 8118.83, average training loss: 7708.23, base loss: 16097.23
[INFO 2017-06-29 03:06:15,610 main.py:57] epoch 3496, training loss: 7807.12, average training loss: 7708.82, base loss: 16097.05
[INFO 2017-06-29 03:06:18,668 main.py:57] epoch 3497, training loss: 8311.53, average training loss: 7709.85, base loss: 16097.43
[INFO 2017-06-29 03:06:21,734 main.py:57] epoch 3498, training loss: 8246.25, average training loss: 7710.59, base loss: 16097.95
[INFO 2017-06-29 03:06:24,812 main.py:57] epoch 3499, training loss: 7386.57, average training loss: 7710.04, base loss: 16098.00
[INFO 2017-06-29 03:06:24,812 main.py:59] epoch 3499, testing
[INFO 2017-06-29 03:06:37,689 main.py:104] average testing loss: 7758.47, base loss: 15119.17
[INFO 2017-06-29 03:06:37,689 main.py:105] improve_loss: 7360.70, improve_percent: 0.49
[INFO 2017-06-29 03:06:37,690 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 03:06:40,764 main.py:57] epoch 3500, training loss: 8049.87, average training loss: 7710.05, base loss: 16097.05
[INFO 2017-06-29 03:06:43,834 main.py:57] epoch 3501, training loss: 7535.48, average training loss: 7708.87, base loss: 16096.16
[INFO 2017-06-29 03:06:46,916 main.py:57] epoch 3502, training loss: 7309.52, average training loss: 7708.39, base loss: 16096.31
[INFO 2017-06-29 03:06:49,998 main.py:57] epoch 3503, training loss: 7147.30, average training loss: 7708.58, base loss: 16096.32
[INFO 2017-06-29 03:06:53,071 main.py:57] epoch 3504, training loss: 7846.33, average training loss: 7708.33, base loss: 16096.97
[INFO 2017-06-29 03:06:56,178 main.py:57] epoch 3505, training loss: 7386.54, average training loss: 7706.94, base loss: 16097.31
[INFO 2017-06-29 03:06:59,211 main.py:57] epoch 3506, training loss: 7889.49, average training loss: 7707.51, base loss: 16096.66
[INFO 2017-06-29 03:07:02,345 main.py:57] epoch 3507, training loss: 8007.93, average training loss: 7707.87, base loss: 16096.67
[INFO 2017-06-29 03:07:05,363 main.py:57] epoch 3508, training loss: 7839.44, average training loss: 7707.25, base loss: 16095.89
[INFO 2017-06-29 03:07:08,419 main.py:57] epoch 3509, training loss: 8001.90, average training loss: 7707.49, base loss: 16096.18
[INFO 2017-06-29 03:07:11,554 main.py:57] epoch 3510, training loss: 7453.56, average training loss: 7707.53, base loss: 16096.53
[INFO 2017-06-29 03:07:14,597 main.py:57] epoch 3511, training loss: 8130.84, average training loss: 7707.69, base loss: 16097.30
[INFO 2017-06-29 03:07:17,591 main.py:57] epoch 3512, training loss: 7384.96, average training loss: 7707.32, base loss: 16096.02
[INFO 2017-06-29 03:07:20,702 main.py:57] epoch 3513, training loss: 9205.31, average training loss: 7708.92, base loss: 16096.47
[INFO 2017-06-29 03:07:23,797 main.py:57] epoch 3514, training loss: 7467.20, average training loss: 7708.69, base loss: 16095.89
[INFO 2017-06-29 03:07:26,888 main.py:57] epoch 3515, training loss: 7924.60, average training loss: 7709.84, base loss: 16096.42
[INFO 2017-06-29 03:07:30,015 main.py:57] epoch 3516, training loss: 7493.43, average training loss: 7709.92, base loss: 16095.91
[INFO 2017-06-29 03:07:33,088 main.py:57] epoch 3517, training loss: 7317.08, average training loss: 7710.18, base loss: 16095.46
[INFO 2017-06-29 03:07:36,116 main.py:57] epoch 3518, training loss: 6895.72, average training loss: 7709.37, base loss: 16094.26
[INFO 2017-06-29 03:07:39,187 main.py:57] epoch 3519, training loss: 6769.68, average training loss: 7708.13, base loss: 16093.84
[INFO 2017-06-29 03:07:42,248 main.py:57] epoch 3520, training loss: 7044.21, average training loss: 7707.79, base loss: 16093.19
[INFO 2017-06-29 03:07:45,319 main.py:57] epoch 3521, training loss: 8150.87, average training loss: 7708.28, base loss: 16093.26
[INFO 2017-06-29 03:07:48,318 main.py:57] epoch 3522, training loss: 8260.25, average training loss: 7708.36, base loss: 16093.32
[INFO 2017-06-29 03:07:51,386 main.py:57] epoch 3523, training loss: 6917.86, average training loss: 7707.36, base loss: 16093.61
[INFO 2017-06-29 03:07:54,428 main.py:57] epoch 3524, training loss: 7042.43, average training loss: 7707.25, base loss: 16093.06
[INFO 2017-06-29 03:07:57,445 main.py:57] epoch 3525, training loss: 7447.42, average training loss: 7707.17, base loss: 16092.47
[INFO 2017-06-29 03:08:00,457 main.py:57] epoch 3526, training loss: 7184.51, average training loss: 7706.71, base loss: 16092.03
[INFO 2017-06-29 03:08:03,510 main.py:57] epoch 3527, training loss: 7407.25, average training loss: 7705.86, base loss: 16092.44
[INFO 2017-06-29 03:08:06,538 main.py:57] epoch 3528, training loss: 8087.32, average training loss: 7705.79, base loss: 16092.77
[INFO 2017-06-29 03:08:09,534 main.py:57] epoch 3529, training loss: 7828.78, average training loss: 7706.25, base loss: 16093.09
[INFO 2017-06-29 03:08:12,618 main.py:57] epoch 3530, training loss: 7695.60, average training loss: 7705.84, base loss: 16093.29
[INFO 2017-06-29 03:08:15,695 main.py:57] epoch 3531, training loss: 7069.88, average training loss: 7705.38, base loss: 16093.00
[INFO 2017-06-29 03:08:18,752 main.py:57] epoch 3532, training loss: 7699.41, average training loss: 7705.93, base loss: 16093.10
[INFO 2017-06-29 03:08:21,816 main.py:57] epoch 3533, training loss: 8441.05, average training loss: 7707.39, base loss: 16093.95
[INFO 2017-06-29 03:08:24,827 main.py:57] epoch 3534, training loss: 7102.03, average training loss: 7706.29, base loss: 16093.45
[INFO 2017-06-29 03:08:27,870 main.py:57] epoch 3535, training loss: 7024.81, average training loss: 7705.39, base loss: 16092.65
[INFO 2017-06-29 03:08:30,936 main.py:57] epoch 3536, training loss: 6960.45, average training loss: 7704.55, base loss: 16091.89
[INFO 2017-06-29 03:08:34,034 main.py:57] epoch 3537, training loss: 8499.08, average training loss: 7704.98, base loss: 16092.07
[INFO 2017-06-29 03:08:37,093 main.py:57] epoch 3538, training loss: 7816.56, average training loss: 7705.31, base loss: 16091.81
[INFO 2017-06-29 03:08:40,202 main.py:57] epoch 3539, training loss: 6209.96, average training loss: 7702.60, base loss: 16090.54
[INFO 2017-06-29 03:08:43,260 main.py:57] epoch 3540, training loss: 8696.59, average training loss: 7703.61, base loss: 16090.88
[INFO 2017-06-29 03:08:46,338 main.py:57] epoch 3541, training loss: 7384.43, average training loss: 7703.47, base loss: 16090.52
[INFO 2017-06-29 03:08:49,379 main.py:57] epoch 3542, training loss: 10477.39, average training loss: 7706.04, base loss: 16092.25
[INFO 2017-06-29 03:08:52,454 main.py:57] epoch 3543, training loss: 7894.30, average training loss: 7706.79, base loss: 16091.43
[INFO 2017-06-29 03:08:55,468 main.py:57] epoch 3544, training loss: 8467.14, average training loss: 7707.99, base loss: 16092.73
[INFO 2017-06-29 03:08:58,478 main.py:57] epoch 3545, training loss: 7330.59, average training loss: 7707.21, base loss: 16093.68
[INFO 2017-06-29 03:09:01,559 main.py:57] epoch 3546, training loss: 7570.48, average training loss: 7707.72, base loss: 16094.12
[INFO 2017-06-29 03:09:04,625 main.py:57] epoch 3547, training loss: 7493.52, average training loss: 7706.69, base loss: 16094.35
[INFO 2017-06-29 03:09:07,692 main.py:57] epoch 3548, training loss: 7686.65, average training loss: 7707.32, base loss: 16095.23
[INFO 2017-06-29 03:09:10,738 main.py:57] epoch 3549, training loss: 7956.03, average training loss: 7705.68, base loss: 16096.84
[INFO 2017-06-29 03:09:13,894 main.py:57] epoch 3550, training loss: 8317.39, average training loss: 7706.00, base loss: 16097.04
[INFO 2017-06-29 03:09:16,913 main.py:57] epoch 3551, training loss: 7687.80, average training loss: 7705.67, base loss: 16097.35
[INFO 2017-06-29 03:09:19,909 main.py:57] epoch 3552, training loss: 6901.35, average training loss: 7705.26, base loss: 16096.96
[INFO 2017-06-29 03:09:22,992 main.py:57] epoch 3553, training loss: 9299.68, average training loss: 7707.51, base loss: 16098.30
[INFO 2017-06-29 03:09:26,012 main.py:57] epoch 3554, training loss: 8102.31, average training loss: 7707.73, base loss: 16098.37
[INFO 2017-06-29 03:09:29,109 main.py:57] epoch 3555, training loss: 7293.14, average training loss: 7708.27, base loss: 16098.32
[INFO 2017-06-29 03:09:32,183 main.py:57] epoch 3556, training loss: 8126.80, average training loss: 7708.78, base loss: 16099.87
[INFO 2017-06-29 03:09:35,224 main.py:57] epoch 3557, training loss: 7582.51, average training loss: 7708.97, base loss: 16100.17
[INFO 2017-06-29 03:09:38,230 main.py:57] epoch 3558, training loss: 8134.23, average training loss: 7710.11, base loss: 16100.87
[INFO 2017-06-29 03:09:41,238 main.py:57] epoch 3559, training loss: 7498.14, average training loss: 7709.87, base loss: 16101.02
[INFO 2017-06-29 03:09:44,332 main.py:57] epoch 3560, training loss: 6912.23, average training loss: 7709.10, base loss: 16099.59
[INFO 2017-06-29 03:09:47,408 main.py:57] epoch 3561, training loss: 7467.77, average training loss: 7708.04, base loss: 16099.81
[INFO 2017-06-29 03:09:50,413 main.py:57] epoch 3562, training loss: 7698.14, average training loss: 7708.01, base loss: 16100.03
[INFO 2017-06-29 03:09:53,418 main.py:57] epoch 3563, training loss: 7654.88, average training loss: 7707.83, base loss: 16099.62
[INFO 2017-06-29 03:09:56,381 main.py:57] epoch 3564, training loss: 7006.51, average training loss: 7707.33, base loss: 16099.34
[INFO 2017-06-29 03:09:59,408 main.py:57] epoch 3565, training loss: 7533.54, average training loss: 7707.45, base loss: 16099.12
[INFO 2017-06-29 03:10:02,487 main.py:57] epoch 3566, training loss: 8091.19, average training loss: 7707.61, base loss: 16100.04
[INFO 2017-06-29 03:10:05,575 main.py:57] epoch 3567, training loss: 6773.63, average training loss: 7706.88, base loss: 16099.67
[INFO 2017-06-29 03:10:08,606 main.py:57] epoch 3568, training loss: 7597.69, average training loss: 7706.38, base loss: 16099.56
[INFO 2017-06-29 03:10:11,659 main.py:57] epoch 3569, training loss: 7389.33, average training loss: 7706.53, base loss: 16099.10
[INFO 2017-06-29 03:10:14,720 main.py:57] epoch 3570, training loss: 8841.63, average training loss: 7708.11, base loss: 16100.32
[INFO 2017-06-29 03:10:17,711 main.py:57] epoch 3571, training loss: 6792.99, average training loss: 7707.81, base loss: 16100.42
[INFO 2017-06-29 03:10:20,748 main.py:57] epoch 3572, training loss: 7589.55, average training loss: 7707.22, base loss: 16100.10
[INFO 2017-06-29 03:10:23,779 main.py:57] epoch 3573, training loss: 7433.25, average training loss: 7706.69, base loss: 16098.86
[INFO 2017-06-29 03:10:26,875 main.py:57] epoch 3574, training loss: 6517.67, average training loss: 7705.43, base loss: 16097.03
[INFO 2017-06-29 03:10:29,977 main.py:57] epoch 3575, training loss: 8210.50, average training loss: 7707.12, base loss: 16097.93
[INFO 2017-06-29 03:10:33,042 main.py:57] epoch 3576, training loss: 6884.46, average training loss: 7706.16, base loss: 16096.84
[INFO 2017-06-29 03:10:36,118 main.py:57] epoch 3577, training loss: 7991.49, average training loss: 7706.36, base loss: 16096.92
[INFO 2017-06-29 03:10:39,097 main.py:57] epoch 3578, training loss: 8204.85, average training loss: 7706.66, base loss: 16097.75
[INFO 2017-06-29 03:10:42,133 main.py:57] epoch 3579, training loss: 8345.44, average training loss: 7707.28, base loss: 16098.27
[INFO 2017-06-29 03:10:45,202 main.py:57] epoch 3580, training loss: 7721.27, average training loss: 7707.33, base loss: 16098.80
[INFO 2017-06-29 03:10:48,250 main.py:57] epoch 3581, training loss: 7021.57, average training loss: 7705.81, base loss: 16098.72
[INFO 2017-06-29 03:10:51,323 main.py:57] epoch 3582, training loss: 6406.05, average training loss: 7703.11, base loss: 16097.86
[INFO 2017-06-29 03:10:54,421 main.py:57] epoch 3583, training loss: 7841.39, average training loss: 7703.29, base loss: 16098.47
[INFO 2017-06-29 03:10:57,440 main.py:57] epoch 3584, training loss: 7935.60, average training loss: 7703.36, base loss: 16097.67
[INFO 2017-06-29 03:11:00,476 main.py:57] epoch 3585, training loss: 8101.65, average training loss: 7704.24, base loss: 16097.48
[INFO 2017-06-29 03:11:03,566 main.py:57] epoch 3586, training loss: 8225.90, average training loss: 7704.32, base loss: 16097.57
[INFO 2017-06-29 03:11:06,615 main.py:57] epoch 3587, training loss: 8026.42, average training loss: 7705.43, base loss: 16098.16
[INFO 2017-06-29 03:11:09,658 main.py:57] epoch 3588, training loss: 7700.52, average training loss: 7705.34, base loss: 16098.98
[INFO 2017-06-29 03:11:12,708 main.py:57] epoch 3589, training loss: 8125.55, average training loss: 7706.28, base loss: 16100.36
[INFO 2017-06-29 03:11:15,742 main.py:57] epoch 3590, training loss: 6328.99, average training loss: 7706.09, base loss: 16100.02
[INFO 2017-06-29 03:11:18,816 main.py:57] epoch 3591, training loss: 7826.13, average training loss: 7705.92, base loss: 16101.48
[INFO 2017-06-29 03:11:21,911 main.py:57] epoch 3592, training loss: 7915.13, average training loss: 7706.57, base loss: 16101.56
[INFO 2017-06-29 03:11:24,940 main.py:57] epoch 3593, training loss: 7042.78, average training loss: 7705.46, base loss: 16100.80
[INFO 2017-06-29 03:11:27,982 main.py:57] epoch 3594, training loss: 6845.41, average training loss: 7704.05, base loss: 16100.39
[INFO 2017-06-29 03:11:30,978 main.py:57] epoch 3595, training loss: 8263.92, average training loss: 7705.35, base loss: 16101.88
[INFO 2017-06-29 03:11:34,017 main.py:57] epoch 3596, training loss: 7121.12, average training loss: 7704.89, base loss: 16101.25
[INFO 2017-06-29 03:11:37,074 main.py:57] epoch 3597, training loss: 8814.86, average training loss: 7705.19, base loss: 16103.25
[INFO 2017-06-29 03:11:40,139 main.py:57] epoch 3598, training loss: 8295.50, average training loss: 7705.50, base loss: 16103.71
[INFO 2017-06-29 03:11:43,231 main.py:57] epoch 3599, training loss: 7201.41, average training loss: 7704.13, base loss: 16103.02
[INFO 2017-06-29 03:11:43,231 main.py:59] epoch 3599, testing
[INFO 2017-06-29 03:11:55,921 main.py:104] average testing loss: 8289.14, base loss: 17345.67
[INFO 2017-06-29 03:11:55,921 main.py:105] improve_loss: 9056.52, improve_percent: 0.52
[INFO 2017-06-29 03:11:55,922 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 03:11:55,960 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:11:58,987 main.py:57] epoch 3600, training loss: 7557.98, average training loss: 7704.20, base loss: 16103.13
[INFO 2017-06-29 03:12:02,139 main.py:57] epoch 3601, training loss: 7576.83, average training loss: 7704.38, base loss: 16103.09
[INFO 2017-06-29 03:12:05,188 main.py:57] epoch 3602, training loss: 6743.13, average training loss: 7704.04, base loss: 16102.45
[INFO 2017-06-29 03:12:08,224 main.py:57] epoch 3603, training loss: 8413.12, average training loss: 7705.93, base loss: 16103.59
[INFO 2017-06-29 03:12:11,241 main.py:57] epoch 3604, training loss: 7789.13, average training loss: 7707.09, base loss: 16103.44
[INFO 2017-06-29 03:12:14,317 main.py:57] epoch 3605, training loss: 7635.00, average training loss: 7706.40, base loss: 16103.93
[INFO 2017-06-29 03:12:17,414 main.py:57] epoch 3606, training loss: 7569.70, average training loss: 7705.41, base loss: 16104.15
[INFO 2017-06-29 03:12:20,477 main.py:57] epoch 3607, training loss: 7021.96, average training loss: 7704.48, base loss: 16103.55
[INFO 2017-06-29 03:12:23,517 main.py:57] epoch 3608, training loss: 7170.53, average training loss: 7702.83, base loss: 16102.67
[INFO 2017-06-29 03:12:26,561 main.py:57] epoch 3609, training loss: 8946.49, average training loss: 7703.65, base loss: 16103.14
[INFO 2017-06-29 03:12:29,604 main.py:57] epoch 3610, training loss: 6849.45, average training loss: 7702.42, base loss: 16102.48
[INFO 2017-06-29 03:12:32,686 main.py:57] epoch 3611, training loss: 8864.77, average training loss: 7702.98, base loss: 16103.78
[INFO 2017-06-29 03:12:35,710 main.py:57] epoch 3612, training loss: 7749.49, average training loss: 7701.27, base loss: 16103.60
[INFO 2017-06-29 03:12:38,724 main.py:57] epoch 3613, training loss: 8534.26, average training loss: 7701.85, base loss: 16105.21
[INFO 2017-06-29 03:12:41,794 main.py:57] epoch 3614, training loss: 7759.60, average training loss: 7701.12, base loss: 16105.67
[INFO 2017-06-29 03:12:44,848 main.py:57] epoch 3615, training loss: 8965.60, average training loss: 7702.21, base loss: 16106.96
[INFO 2017-06-29 03:12:47,913 main.py:57] epoch 3616, training loss: 7128.40, average training loss: 7702.19, base loss: 16106.06
[INFO 2017-06-29 03:12:50,971 main.py:57] epoch 3617, training loss: 7640.22, average training loss: 7702.80, base loss: 16105.64
[INFO 2017-06-29 03:12:54,037 main.py:57] epoch 3618, training loss: 7021.38, average training loss: 7701.89, base loss: 16105.45
[INFO 2017-06-29 03:12:57,117 main.py:57] epoch 3619, training loss: 7530.57, average training loss: 7702.22, base loss: 16106.01
[INFO 2017-06-29 03:13:00,177 main.py:57] epoch 3620, training loss: 7156.92, average training loss: 7702.17, base loss: 16104.72
[INFO 2017-06-29 03:13:03,234 main.py:57] epoch 3621, training loss: 7344.74, average training loss: 7702.27, base loss: 16103.84
[INFO 2017-06-29 03:13:06,360 main.py:57] epoch 3622, training loss: 7707.93, average training loss: 7701.78, base loss: 16103.88
[INFO 2017-06-29 03:13:09,391 main.py:57] epoch 3623, training loss: 6723.25, average training loss: 7700.63, base loss: 16103.14
[INFO 2017-06-29 03:13:12,390 main.py:57] epoch 3624, training loss: 8673.80, average training loss: 7702.69, base loss: 16104.24
[INFO 2017-06-29 03:13:15,459 main.py:57] epoch 3625, training loss: 7920.71, average training loss: 7702.67, base loss: 16105.73
[INFO 2017-06-29 03:13:18,488 main.py:57] epoch 3626, training loss: 7645.45, average training loss: 7702.47, base loss: 16106.39
[INFO 2017-06-29 03:13:21,578 main.py:57] epoch 3627, training loss: 7717.58, average training loss: 7703.40, base loss: 16106.29
[INFO 2017-06-29 03:13:24,636 main.py:57] epoch 3628, training loss: 8171.79, average training loss: 7704.35, base loss: 16106.14
[INFO 2017-06-29 03:13:27,743 main.py:57] epoch 3629, training loss: 8145.05, average training loss: 7705.00, base loss: 16105.87
[INFO 2017-06-29 03:13:30,935 main.py:57] epoch 3630, training loss: 7731.21, average training loss: 7705.16, base loss: 16105.97
[INFO 2017-06-29 03:13:34,007 main.py:57] epoch 3631, training loss: 6959.59, average training loss: 7704.86, base loss: 16104.85
[INFO 2017-06-29 03:13:37,092 main.py:57] epoch 3632, training loss: 7158.78, average training loss: 7704.31, base loss: 16104.53
[INFO 2017-06-29 03:13:40,208 main.py:57] epoch 3633, training loss: 7678.67, average training loss: 7704.35, base loss: 16104.29
[INFO 2017-06-29 03:13:43,246 main.py:57] epoch 3634, training loss: 7415.91, average training loss: 7703.76, base loss: 16104.32
[INFO 2017-06-29 03:13:46,279 main.py:57] epoch 3635, training loss: 7041.45, average training loss: 7703.05, base loss: 16104.51
[INFO 2017-06-29 03:13:49,322 main.py:57] epoch 3636, training loss: 7709.65, average training loss: 7700.83, base loss: 16104.98
[INFO 2017-06-29 03:13:52,381 main.py:57] epoch 3637, training loss: 7823.92, average training loss: 7701.28, base loss: 16105.52
[INFO 2017-06-29 03:13:55,447 main.py:57] epoch 3638, training loss: 7674.85, average training loss: 7701.94, base loss: 16105.41
[INFO 2017-06-29 03:13:58,509 main.py:57] epoch 3639, training loss: 7634.52, average training loss: 7702.16, base loss: 16105.57
[INFO 2017-06-29 03:14:01,542 main.py:57] epoch 3640, training loss: 7898.66, average training loss: 7702.75, base loss: 16106.58
[INFO 2017-06-29 03:14:04,565 main.py:57] epoch 3641, training loss: 8075.53, average training loss: 7703.42, base loss: 16107.54
[INFO 2017-06-29 03:14:07,676 main.py:57] epoch 3642, training loss: 8316.97, average training loss: 7704.63, base loss: 16108.14
[INFO 2017-06-29 03:14:10,744 main.py:57] epoch 3643, training loss: 7365.98, average training loss: 7703.73, base loss: 16107.70
[INFO 2017-06-29 03:14:13,793 main.py:57] epoch 3644, training loss: 7171.36, average training loss: 7701.62, base loss: 16106.73
[INFO 2017-06-29 03:14:16,957 main.py:57] epoch 3645, training loss: 6869.80, average training loss: 7700.44, base loss: 16105.63
[INFO 2017-06-29 03:14:19,944 main.py:57] epoch 3646, training loss: 7741.72, average training loss: 7700.57, base loss: 16105.21
[INFO 2017-06-29 03:14:22,961 main.py:57] epoch 3647, training loss: 7992.09, average training loss: 7701.78, base loss: 16105.10
[INFO 2017-06-29 03:14:26,034 main.py:57] epoch 3648, training loss: 7884.62, average training loss: 7702.22, base loss: 16105.50
[INFO 2017-06-29 03:14:29,177 main.py:57] epoch 3649, training loss: 7178.99, average training loss: 7700.78, base loss: 16105.25
[INFO 2017-06-29 03:14:32,214 main.py:57] epoch 3650, training loss: 6664.02, average training loss: 7699.98, base loss: 16104.76
[INFO 2017-06-29 03:14:35,314 main.py:57] epoch 3651, training loss: 7420.13, average training loss: 7700.03, base loss: 16104.73
[INFO 2017-06-29 03:14:38,452 main.py:57] epoch 3652, training loss: 7326.31, average training loss: 7698.85, base loss: 16105.09
[INFO 2017-06-29 03:14:41,519 main.py:57] epoch 3653, training loss: 6831.99, average training loss: 7698.07, base loss: 16104.49
[INFO 2017-06-29 03:14:44,583 main.py:57] epoch 3654, training loss: 7592.21, average training loss: 7697.99, base loss: 16103.98
[INFO 2017-06-29 03:14:47,626 main.py:57] epoch 3655, training loss: 7378.67, average training loss: 7698.13, base loss: 16103.21
[INFO 2017-06-29 03:14:50,723 main.py:57] epoch 3656, training loss: 7379.90, average training loss: 7697.62, base loss: 16102.96
[INFO 2017-06-29 03:14:53,796 main.py:57] epoch 3657, training loss: 7045.89, average training loss: 7697.36, base loss: 16102.50
[INFO 2017-06-29 03:14:56,902 main.py:57] epoch 3658, training loss: 6966.04, average training loss: 7696.49, base loss: 16102.02
[INFO 2017-06-29 03:14:59,951 main.py:57] epoch 3659, training loss: 8199.71, average training loss: 7697.21, base loss: 16103.09
[INFO 2017-06-29 03:15:02,997 main.py:57] epoch 3660, training loss: 8006.17, average training loss: 7698.61, base loss: 16103.01
[INFO 2017-06-29 03:15:06,053 main.py:57] epoch 3661, training loss: 8837.15, average training loss: 7699.39, base loss: 16104.08
[INFO 2017-06-29 03:15:09,065 main.py:57] epoch 3662, training loss: 6539.95, average training loss: 7699.00, base loss: 16103.41
[INFO 2017-06-29 03:15:12,082 main.py:57] epoch 3663, training loss: 8013.72, average training loss: 7699.82, base loss: 16103.09
[INFO 2017-06-29 03:15:15,150 main.py:57] epoch 3664, training loss: 7767.67, average training loss: 7699.26, base loss: 16103.64
[INFO 2017-06-29 03:15:18,198 main.py:57] epoch 3665, training loss: 7824.75, average training loss: 7700.01, base loss: 16103.61
[INFO 2017-06-29 03:15:21,230 main.py:57] epoch 3666, training loss: 7153.19, average training loss: 7698.93, base loss: 16104.14
[INFO 2017-06-29 03:15:24,281 main.py:57] epoch 3667, training loss: 6745.03, average training loss: 7697.43, base loss: 16103.27
[INFO 2017-06-29 03:15:27,301 main.py:57] epoch 3668, training loss: 8373.62, average training loss: 7699.07, base loss: 16104.71
[INFO 2017-06-29 03:15:30,306 main.py:57] epoch 3669, training loss: 7843.03, average training loss: 7698.99, base loss: 16105.37
[INFO 2017-06-29 03:15:33,370 main.py:57] epoch 3670, training loss: 7354.42, average training loss: 7699.13, base loss: 16105.31
[INFO 2017-06-29 03:15:36,455 main.py:57] epoch 3671, training loss: 7051.86, average training loss: 7699.41, base loss: 16104.46
[INFO 2017-06-29 03:15:39,517 main.py:57] epoch 3672, training loss: 6912.78, average training loss: 7698.45, base loss: 16103.93
[INFO 2017-06-29 03:15:42,571 main.py:57] epoch 3673, training loss: 7327.52, average training loss: 7697.91, base loss: 16103.71
[INFO 2017-06-29 03:15:45,659 main.py:57] epoch 3674, training loss: 7064.20, average training loss: 7698.72, base loss: 16103.48
[INFO 2017-06-29 03:15:48,685 main.py:57] epoch 3675, training loss: 7538.87, average training loss: 7698.46, base loss: 16103.96
[INFO 2017-06-29 03:15:51,758 main.py:57] epoch 3676, training loss: 7812.30, average training loss: 7699.20, base loss: 16104.37
[INFO 2017-06-29 03:15:54,836 main.py:57] epoch 3677, training loss: 7464.13, average training loss: 7698.84, base loss: 16104.86
[INFO 2017-06-29 03:15:57,894 main.py:57] epoch 3678, training loss: 7129.10, average training loss: 7698.87, base loss: 16104.77
[INFO 2017-06-29 03:16:01,006 main.py:57] epoch 3679, training loss: 7908.07, average training loss: 7699.57, base loss: 16105.39
[INFO 2017-06-29 03:16:04,030 main.py:57] epoch 3680, training loss: 7690.10, average training loss: 7699.63, base loss: 16104.32
[INFO 2017-06-29 03:16:07,015 main.py:57] epoch 3681, training loss: 7417.09, average training loss: 7699.35, base loss: 16104.57
[INFO 2017-06-29 03:16:10,085 main.py:57] epoch 3682, training loss: 7135.15, average training loss: 7699.15, base loss: 16103.80
[INFO 2017-06-29 03:16:13,165 main.py:57] epoch 3683, training loss: 8224.01, average training loss: 7699.43, base loss: 16103.78
[INFO 2017-06-29 03:16:16,219 main.py:57] epoch 3684, training loss: 7623.49, average training loss: 7699.77, base loss: 16103.24
[INFO 2017-06-29 03:16:19,297 main.py:57] epoch 3685, training loss: 7016.95, average training loss: 7699.41, base loss: 16101.89
[INFO 2017-06-29 03:16:22,376 main.py:57] epoch 3686, training loss: 8156.28, average training loss: 7700.46, base loss: 16103.03
[INFO 2017-06-29 03:16:25,432 main.py:57] epoch 3687, training loss: 7105.44, average training loss: 7699.98, base loss: 16102.44
[INFO 2017-06-29 03:16:28,454 main.py:57] epoch 3688, training loss: 7822.07, average training loss: 7699.51, base loss: 16102.18
[INFO 2017-06-29 03:16:31,500 main.py:57] epoch 3689, training loss: 7956.62, average training loss: 7700.25, base loss: 16101.96
[INFO 2017-06-29 03:16:34,559 main.py:57] epoch 3690, training loss: 7029.28, average training loss: 7698.87, base loss: 16100.80
[INFO 2017-06-29 03:16:37,618 main.py:57] epoch 3691, training loss: 7857.90, average training loss: 7697.80, base loss: 16100.75
[INFO 2017-06-29 03:16:40,733 main.py:57] epoch 3692, training loss: 8549.93, average training loss: 7698.08, base loss: 16101.81
[INFO 2017-06-29 03:16:43,787 main.py:57] epoch 3693, training loss: 8228.88, average training loss: 7698.20, base loss: 16102.78
[INFO 2017-06-29 03:16:46,859 main.py:57] epoch 3694, training loss: 7512.89, average training loss: 7698.27, base loss: 16102.29
[INFO 2017-06-29 03:16:49,927 main.py:57] epoch 3695, training loss: 7632.98, average training loss: 7698.59, base loss: 16102.41
[INFO 2017-06-29 03:16:53,027 main.py:57] epoch 3696, training loss: 7037.45, average training loss: 7697.79, base loss: 16102.23
[INFO 2017-06-29 03:16:56,133 main.py:57] epoch 3697, training loss: 7110.48, average training loss: 7695.73, base loss: 16102.34
[INFO 2017-06-29 03:16:59,168 main.py:57] epoch 3698, training loss: 7603.63, average training loss: 7696.56, base loss: 16102.39
[INFO 2017-06-29 03:17:02,247 main.py:57] epoch 3699, training loss: 7072.10, average training loss: 7696.48, base loss: 16102.18
[INFO 2017-06-29 03:17:02,247 main.py:59] epoch 3699, testing
[INFO 2017-06-29 03:17:14,725 main.py:104] average testing loss: 7898.84, base loss: 16057.66
[INFO 2017-06-29 03:17:14,726 main.py:105] improve_loss: 8158.82, improve_percent: 0.51
[INFO 2017-06-29 03:17:14,727 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:17:17,730 main.py:57] epoch 3700, training loss: 7923.47, average training loss: 7696.75, base loss: 16102.32
[INFO 2017-06-29 03:17:20,806 main.py:57] epoch 3701, training loss: 8025.77, average training loss: 7697.45, base loss: 16102.50
[INFO 2017-06-29 03:17:23,831 main.py:57] epoch 3702, training loss: 7938.16, average training loss: 7696.83, base loss: 16103.59
[INFO 2017-06-29 03:17:26,949 main.py:57] epoch 3703, training loss: 7052.55, average training loss: 7695.55, base loss: 16103.86
[INFO 2017-06-29 03:17:29,983 main.py:57] epoch 3704, training loss: 7218.65, average training loss: 7694.53, base loss: 16104.18
[INFO 2017-06-29 03:17:33,176 main.py:57] epoch 3705, training loss: 7535.17, average training loss: 7694.75, base loss: 16104.83
[INFO 2017-06-29 03:17:36,238 main.py:57] epoch 3706, training loss: 7532.09, average training loss: 7695.76, base loss: 16105.42
[INFO 2017-06-29 03:17:39,378 main.py:57] epoch 3707, training loss: 7687.30, average training loss: 7695.65, base loss: 16106.29
[INFO 2017-06-29 03:17:42,397 main.py:57] epoch 3708, training loss: 7209.50, average training loss: 7695.01, base loss: 16106.10
[INFO 2017-06-29 03:17:45,434 main.py:57] epoch 3709, training loss: 7324.05, average training loss: 7694.20, base loss: 16105.23
[INFO 2017-06-29 03:17:48,487 main.py:57] epoch 3710, training loss: 7419.85, average training loss: 7693.51, base loss: 16105.29
[INFO 2017-06-29 03:17:51,577 main.py:57] epoch 3711, training loss: 8660.52, average training loss: 7693.91, base loss: 16106.15
[INFO 2017-06-29 03:17:54,602 main.py:57] epoch 3712, training loss: 8115.38, average training loss: 7694.65, base loss: 16106.87
[INFO 2017-06-29 03:17:57,598 main.py:57] epoch 3713, training loss: 7876.05, average training loss: 7694.81, base loss: 16108.42
[INFO 2017-06-29 03:18:00,708 main.py:57] epoch 3714, training loss: 7567.29, average training loss: 7695.40, base loss: 16107.47
[INFO 2017-06-29 03:18:03,727 main.py:57] epoch 3715, training loss: 7351.22, average training loss: 7695.75, base loss: 16106.89
[INFO 2017-06-29 03:18:06,816 main.py:57] epoch 3716, training loss: 7418.55, average training loss: 7694.99, base loss: 16107.59
[INFO 2017-06-29 03:18:09,813 main.py:57] epoch 3717, training loss: 7579.02, average training loss: 7695.33, base loss: 16108.34
[INFO 2017-06-29 03:18:12,939 main.py:57] epoch 3718, training loss: 7284.98, average training loss: 7695.79, base loss: 16107.63
[INFO 2017-06-29 03:18:15,909 main.py:57] epoch 3719, training loss: 8119.50, average training loss: 7697.14, base loss: 16108.11
[INFO 2017-06-29 03:18:18,923 main.py:57] epoch 3720, training loss: 8198.59, average training loss: 7696.40, base loss: 16108.50
[INFO 2017-06-29 03:18:21,955 main.py:57] epoch 3721, training loss: 8214.22, average training loss: 7696.21, base loss: 16108.50
[INFO 2017-06-29 03:18:24,941 main.py:57] epoch 3722, training loss: 7075.99, average training loss: 7695.71, base loss: 16107.96
[INFO 2017-06-29 03:18:28,034 main.py:57] epoch 3723, training loss: 8261.50, average training loss: 7697.01, base loss: 16108.53
[INFO 2017-06-29 03:18:31,095 main.py:57] epoch 3724, training loss: 7498.28, average training loss: 7696.36, base loss: 16108.93
[INFO 2017-06-29 03:18:34,119 main.py:57] epoch 3725, training loss: 8292.80, average training loss: 7697.24, base loss: 16110.30
[INFO 2017-06-29 03:18:37,146 main.py:57] epoch 3726, training loss: 9844.35, average training loss: 7699.38, base loss: 16112.11
[INFO 2017-06-29 03:18:40,177 main.py:57] epoch 3727, training loss: 7274.85, average training loss: 7699.10, base loss: 16111.91
[INFO 2017-06-29 03:18:43,216 main.py:57] epoch 3728, training loss: 7636.74, average training loss: 7699.62, base loss: 16111.53
[INFO 2017-06-29 03:18:46,274 main.py:57] epoch 3729, training loss: 7828.27, average training loss: 7700.14, base loss: 16111.13
[INFO 2017-06-29 03:18:49,295 main.py:57] epoch 3730, training loss: 7953.62, average training loss: 7700.37, base loss: 16111.92
[INFO 2017-06-29 03:18:52,310 main.py:57] epoch 3731, training loss: 6625.20, average training loss: 7699.60, base loss: 16111.47
[INFO 2017-06-29 03:18:55,395 main.py:57] epoch 3732, training loss: 7780.90, average training loss: 7699.97, base loss: 16111.88
[INFO 2017-06-29 03:18:58,470 main.py:57] epoch 3733, training loss: 8111.31, average training loss: 7700.29, base loss: 16111.91
[INFO 2017-06-29 03:19:01,579 main.py:57] epoch 3734, training loss: 6865.42, average training loss: 7699.13, base loss: 16111.13
[INFO 2017-06-29 03:19:04,656 main.py:57] epoch 3735, training loss: 6948.96, average training loss: 7696.52, base loss: 16110.71
[INFO 2017-06-29 03:19:07,769 main.py:57] epoch 3736, training loss: 8556.43, average training loss: 7698.59, base loss: 16110.55
[INFO 2017-06-29 03:19:10,771 main.py:57] epoch 3737, training loss: 7291.19, average training loss: 7697.64, base loss: 16110.33
[INFO 2017-06-29 03:19:13,784 main.py:57] epoch 3738, training loss: 7896.62, average training loss: 7698.55, base loss: 16110.65
[INFO 2017-06-29 03:19:16,852 main.py:57] epoch 3739, training loss: 7096.88, average training loss: 7697.93, base loss: 16110.39
[INFO 2017-06-29 03:19:19,938 main.py:57] epoch 3740, training loss: 7033.21, average training loss: 7698.20, base loss: 16109.37
[INFO 2017-06-29 03:19:22,942 main.py:57] epoch 3741, training loss: 7927.61, average training loss: 7697.36, base loss: 16109.80
[INFO 2017-06-29 03:19:25,961 main.py:57] epoch 3742, training loss: 7823.61, average training loss: 7696.49, base loss: 16110.52
[INFO 2017-06-29 03:19:29,043 main.py:57] epoch 3743, training loss: 7387.55, average training loss: 7696.36, base loss: 16109.94
[INFO 2017-06-29 03:19:32,166 main.py:57] epoch 3744, training loss: 9524.70, average training loss: 7698.32, base loss: 16111.53
[INFO 2017-06-29 03:19:35,137 main.py:57] epoch 3745, training loss: 7456.74, average training loss: 7698.17, base loss: 16111.93
[INFO 2017-06-29 03:19:38,217 main.py:57] epoch 3746, training loss: 6755.21, average training loss: 7696.85, base loss: 16111.10
[INFO 2017-06-29 03:19:41,278 main.py:57] epoch 3747, training loss: 8327.26, average training loss: 7697.47, base loss: 16111.79
[INFO 2017-06-29 03:19:44,371 main.py:57] epoch 3748, training loss: 7349.44, average training loss: 7697.05, base loss: 16111.84
[INFO 2017-06-29 03:19:47,441 main.py:57] epoch 3749, training loss: 8340.86, average training loss: 7697.09, base loss: 16112.18
[INFO 2017-06-29 03:19:50,517 main.py:57] epoch 3750, training loss: 7591.29, average training loss: 7697.29, base loss: 16111.67
[INFO 2017-06-29 03:19:53,577 main.py:57] epoch 3751, training loss: 7033.15, average training loss: 7697.20, base loss: 16111.26
[INFO 2017-06-29 03:19:56,576 main.py:57] epoch 3752, training loss: 8795.66, average training loss: 7697.74, base loss: 16111.91
[INFO 2017-06-29 03:19:59,682 main.py:57] epoch 3753, training loss: 7205.51, average training loss: 7698.56, base loss: 16111.50
[INFO 2017-06-29 03:20:02,745 main.py:57] epoch 3754, training loss: 8007.72, average training loss: 7698.98, base loss: 16110.96
[INFO 2017-06-29 03:20:05,823 main.py:57] epoch 3755, training loss: 8166.99, average training loss: 7698.80, base loss: 16110.75
[INFO 2017-06-29 03:20:08,838 main.py:57] epoch 3756, training loss: 6831.83, average training loss: 7697.65, base loss: 16109.58
[INFO 2017-06-29 03:20:11,885 main.py:57] epoch 3757, training loss: 8146.87, average training loss: 7697.61, base loss: 16109.32
[INFO 2017-06-29 03:20:14,926 main.py:57] epoch 3758, training loss: 8282.51, average training loss: 7697.79, base loss: 16109.81
[INFO 2017-06-29 03:20:17,982 main.py:57] epoch 3759, training loss: 7479.92, average training loss: 7697.21, base loss: 16109.96
[INFO 2017-06-29 03:20:21,013 main.py:57] epoch 3760, training loss: 8220.93, average training loss: 7696.41, base loss: 16110.79
[INFO 2017-06-29 03:20:24,073 main.py:57] epoch 3761, training loss: 7320.62, average training loss: 7695.63, base loss: 16111.60
[INFO 2017-06-29 03:20:27,218 main.py:57] epoch 3762, training loss: 7187.25, average training loss: 7695.05, base loss: 16110.96
[INFO 2017-06-29 03:20:30,301 main.py:57] epoch 3763, training loss: 7477.31, average training loss: 7694.70, base loss: 16110.84
[INFO 2017-06-29 03:20:33,353 main.py:57] epoch 3764, training loss: 6707.52, average training loss: 7693.97, base loss: 16110.23
[INFO 2017-06-29 03:20:36,352 main.py:57] epoch 3765, training loss: 7842.03, average training loss: 7693.71, base loss: 16110.61
[INFO 2017-06-29 03:20:39,384 main.py:57] epoch 3766, training loss: 6974.38, average training loss: 7692.78, base loss: 16110.07
[INFO 2017-06-29 03:20:42,439 main.py:57] epoch 3767, training loss: 7096.54, average training loss: 7691.46, base loss: 16109.81
[INFO 2017-06-29 03:20:45,458 main.py:57] epoch 3768, training loss: 7337.57, average training loss: 7691.64, base loss: 16108.90
[INFO 2017-06-29 03:20:48,539 main.py:57] epoch 3769, training loss: 8015.22, average training loss: 7692.17, base loss: 16108.34
[INFO 2017-06-29 03:20:51,607 main.py:57] epoch 3770, training loss: 7193.66, average training loss: 7691.90, base loss: 16107.62
[INFO 2017-06-29 03:20:54,666 main.py:57] epoch 3771, training loss: 7824.37, average training loss: 7692.49, base loss: 16107.31
[INFO 2017-06-29 03:20:57,743 main.py:57] epoch 3772, training loss: 6649.14, average training loss: 7690.90, base loss: 16105.92
[INFO 2017-06-29 03:21:00,853 main.py:57] epoch 3773, training loss: 6209.84, average training loss: 7689.51, base loss: 16103.89
[INFO 2017-06-29 03:21:03,903 main.py:57] epoch 3774, training loss: 7174.12, average training loss: 7689.14, base loss: 16103.07
[INFO 2017-06-29 03:21:06,938 main.py:57] epoch 3775, training loss: 7280.49, average training loss: 7687.98, base loss: 16102.24
[INFO 2017-06-29 03:21:09,967 main.py:57] epoch 3776, training loss: 7579.33, average training loss: 7688.65, base loss: 16102.15
[INFO 2017-06-29 03:21:13,029 main.py:57] epoch 3777, training loss: 6964.34, average training loss: 7686.19, base loss: 16101.55
[INFO 2017-06-29 03:21:16,103 main.py:57] epoch 3778, training loss: 8235.74, average training loss: 7686.53, base loss: 16103.27
[INFO 2017-06-29 03:21:19,156 main.py:57] epoch 3779, training loss: 6979.12, average training loss: 7685.17, base loss: 16103.25
[INFO 2017-06-29 03:21:22,210 main.py:57] epoch 3780, training loss: 7794.41, average training loss: 7685.16, base loss: 16103.15
[INFO 2017-06-29 03:21:25,288 main.py:57] epoch 3781, training loss: 7006.83, average training loss: 7684.30, base loss: 16102.39
[INFO 2017-06-29 03:21:28,332 main.py:57] epoch 3782, training loss: 7754.13, average training loss: 7684.17, base loss: 16102.66
[INFO 2017-06-29 03:21:31,419 main.py:57] epoch 3783, training loss: 7560.53, average training loss: 7683.83, base loss: 16103.05
[INFO 2017-06-29 03:21:34,469 main.py:57] epoch 3784, training loss: 8174.02, average training loss: 7684.04, base loss: 16103.37
[INFO 2017-06-29 03:21:37,485 main.py:57] epoch 3785, training loss: 7037.73, average training loss: 7683.41, base loss: 16102.70
[INFO 2017-06-29 03:21:40,534 main.py:57] epoch 3786, training loss: 7853.10, average training loss: 7684.57, base loss: 16102.75
[INFO 2017-06-29 03:21:43,669 main.py:57] epoch 3787, training loss: 8037.22, average training loss: 7685.70, base loss: 16102.61
[INFO 2017-06-29 03:21:46,782 main.py:57] epoch 3788, training loss: 7825.29, average training loss: 7686.72, base loss: 16102.64
[INFO 2017-06-29 03:21:49,810 main.py:57] epoch 3789, training loss: 8663.60, average training loss: 7687.82, base loss: 16104.21
[INFO 2017-06-29 03:21:52,870 main.py:57] epoch 3790, training loss: 6685.89, average training loss: 7686.20, base loss: 16103.58
[INFO 2017-06-29 03:21:55,900 main.py:57] epoch 3791, training loss: 7972.38, average training loss: 7685.91, base loss: 16104.75
[INFO 2017-06-29 03:21:59,041 main.py:57] epoch 3792, training loss: 7715.90, average training loss: 7684.44, base loss: 16104.68
[INFO 2017-06-29 03:22:02,122 main.py:57] epoch 3793, training loss: 6972.74, average training loss: 7683.65, base loss: 16103.72
[INFO 2017-06-29 03:22:05,194 main.py:57] epoch 3794, training loss: 7437.72, average training loss: 7684.68, base loss: 16102.73
[INFO 2017-06-29 03:22:08,142 main.py:57] epoch 3795, training loss: 6702.35, average training loss: 7682.90, base loss: 16101.36
[INFO 2017-06-29 03:22:11,200 main.py:57] epoch 3796, training loss: 7680.76, average training loss: 7682.66, base loss: 16101.03
[INFO 2017-06-29 03:22:14,226 main.py:57] epoch 3797, training loss: 7983.49, average training loss: 7683.37, base loss: 16100.87
[INFO 2017-06-29 03:22:17,336 main.py:57] epoch 3798, training loss: 7367.17, average training loss: 7683.25, base loss: 16100.30
[INFO 2017-06-29 03:22:20,351 main.py:57] epoch 3799, training loss: 7637.13, average training loss: 7683.64, base loss: 16100.31
[INFO 2017-06-29 03:22:20,351 main.py:59] epoch 3799, testing
[INFO 2017-06-29 03:22:32,912 main.py:104] average testing loss: 8082.08, base loss: 16770.30
[INFO 2017-06-29 03:22:32,912 main.py:105] improve_loss: 8688.22, improve_percent: 0.52
[INFO 2017-06-29 03:22:32,913 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:22:35,941 main.py:57] epoch 3800, training loss: 8365.06, average training loss: 7684.78, base loss: 16100.45
[INFO 2017-06-29 03:22:39,056 main.py:57] epoch 3801, training loss: 6579.43, average training loss: 7684.22, base loss: 16099.19
[INFO 2017-06-29 03:22:42,114 main.py:57] epoch 3802, training loss: 8015.16, average training loss: 7684.61, base loss: 16100.19
[INFO 2017-06-29 03:22:45,152 main.py:57] epoch 3803, training loss: 7867.32, average training loss: 7685.20, base loss: 16100.19
[INFO 2017-06-29 03:22:48,221 main.py:57] epoch 3804, training loss: 8149.38, average training loss: 7685.77, base loss: 16100.09
[INFO 2017-06-29 03:22:51,266 main.py:57] epoch 3805, training loss: 8588.74, average training loss: 7686.77, base loss: 16100.93
[INFO 2017-06-29 03:22:54,340 main.py:57] epoch 3806, training loss: 7115.19, average training loss: 7685.31, base loss: 16099.18
[INFO 2017-06-29 03:22:57,340 main.py:57] epoch 3807, training loss: 7344.25, average training loss: 7684.05, base loss: 16098.23
[INFO 2017-06-29 03:23:00,402 main.py:57] epoch 3808, training loss: 7179.90, average training loss: 7684.05, base loss: 16098.45
[INFO 2017-06-29 03:23:03,500 main.py:57] epoch 3809, training loss: 8248.69, average training loss: 7684.28, base loss: 16099.51
[INFO 2017-06-29 03:23:06,586 main.py:57] epoch 3810, training loss: 8411.74, average training loss: 7684.41, base loss: 16100.11
[INFO 2017-06-29 03:23:09,719 main.py:57] epoch 3811, training loss: 7250.87, average training loss: 7683.76, base loss: 16099.96
[INFO 2017-06-29 03:23:12,742 main.py:57] epoch 3812, training loss: 7084.67, average training loss: 7682.55, base loss: 16099.42
[INFO 2017-06-29 03:23:15,841 main.py:57] epoch 3813, training loss: 7350.85, average training loss: 7682.51, base loss: 16098.57
[INFO 2017-06-29 03:23:18,940 main.py:57] epoch 3814, training loss: 6774.44, average training loss: 7681.34, base loss: 16097.35
[INFO 2017-06-29 03:23:22,063 main.py:57] epoch 3815, training loss: 7189.65, average training loss: 7679.69, base loss: 16096.90
[INFO 2017-06-29 03:23:25,148 main.py:57] epoch 3816, training loss: 7476.22, average training loss: 7679.72, base loss: 16096.46
[INFO 2017-06-29 03:23:28,180 main.py:57] epoch 3817, training loss: 6888.63, average training loss: 7678.65, base loss: 16095.60
[INFO 2017-06-29 03:23:31,241 main.py:57] epoch 3818, training loss: 6670.11, average training loss: 7677.03, base loss: 16094.96
[INFO 2017-06-29 03:23:34,364 main.py:57] epoch 3819, training loss: 8085.02, average training loss: 7677.53, base loss: 16094.41
[INFO 2017-06-29 03:23:37,373 main.py:57] epoch 3820, training loss: 7678.59, average training loss: 7677.03, base loss: 16094.41
[INFO 2017-06-29 03:23:40,447 main.py:57] epoch 3821, training loss: 7588.71, average training loss: 7677.76, base loss: 16094.79
[INFO 2017-06-29 03:23:43,473 main.py:57] epoch 3822, training loss: 6906.84, average training loss: 7676.11, base loss: 16094.04
[INFO 2017-06-29 03:23:46,616 main.py:57] epoch 3823, training loss: 8680.60, average training loss: 7677.12, base loss: 16094.68
[INFO 2017-06-29 03:23:49,707 main.py:57] epoch 3824, training loss: 7204.63, average training loss: 7675.40, base loss: 16094.23
[INFO 2017-06-29 03:23:52,758 main.py:57] epoch 3825, training loss: 6715.37, average training loss: 7675.20, base loss: 16092.64
[INFO 2017-06-29 03:23:55,751 main.py:57] epoch 3826, training loss: 7426.92, average training loss: 7675.13, base loss: 16092.89
[INFO 2017-06-29 03:23:58,775 main.py:57] epoch 3827, training loss: 7280.64, average training loss: 7674.80, base loss: 16092.66
[INFO 2017-06-29 03:24:01,865 main.py:57] epoch 3828, training loss: 7963.54, average training loss: 7675.72, base loss: 16093.19
[INFO 2017-06-29 03:24:04,938 main.py:57] epoch 3829, training loss: 7147.96, average training loss: 7674.46, base loss: 16092.81
[INFO 2017-06-29 03:24:08,037 main.py:57] epoch 3830, training loss: 7552.42, average training loss: 7673.53, base loss: 16092.66
[INFO 2017-06-29 03:24:11,144 main.py:57] epoch 3831, training loss: 6850.74, average training loss: 7673.39, base loss: 16091.53
[INFO 2017-06-29 03:24:14,145 main.py:57] epoch 3832, training loss: 7818.12, average training loss: 7673.63, base loss: 16091.85
[INFO 2017-06-29 03:24:17,189 main.py:57] epoch 3833, training loss: 7690.97, average training loss: 7673.79, base loss: 16091.14
[INFO 2017-06-29 03:24:20,206 main.py:57] epoch 3834, training loss: 7230.55, average training loss: 7673.40, base loss: 16090.62
[INFO 2017-06-29 03:24:23,307 main.py:57] epoch 3835, training loss: 7746.56, average training loss: 7673.13, base loss: 16090.97
[INFO 2017-06-29 03:24:26,381 main.py:57] epoch 3836, training loss: 8567.15, average training loss: 7674.60, base loss: 16091.86
[INFO 2017-06-29 03:24:29,456 main.py:57] epoch 3837, training loss: 7594.92, average training loss: 7674.94, base loss: 16092.20
[INFO 2017-06-29 03:24:32,490 main.py:57] epoch 3838, training loss: 7101.62, average training loss: 7674.30, base loss: 16091.30
[INFO 2017-06-29 03:24:35,528 main.py:57] epoch 3839, training loss: 8428.53, average training loss: 7675.57, base loss: 16092.12
[INFO 2017-06-29 03:24:38,574 main.py:57] epoch 3840, training loss: 7092.64, average training loss: 7674.62, base loss: 16092.26
[INFO 2017-06-29 03:24:41,606 main.py:57] epoch 3841, training loss: 6941.38, average training loss: 7674.19, base loss: 16091.41
[INFO 2017-06-29 03:24:44,743 main.py:57] epoch 3842, training loss: 8616.23, average training loss: 7676.13, base loss: 16092.19
[INFO 2017-06-29 03:24:47,811 main.py:57] epoch 3843, training loss: 7534.07, average training loss: 7675.76, base loss: 16092.29
[INFO 2017-06-29 03:24:50,900 main.py:57] epoch 3844, training loss: 8224.35, average training loss: 7676.14, base loss: 16092.62
[INFO 2017-06-29 03:24:53,983 main.py:57] epoch 3845, training loss: 8817.70, average training loss: 7674.53, base loss: 16092.94
[INFO 2017-06-29 03:24:57,046 main.py:57] epoch 3846, training loss: 7704.79, average training loss: 7674.64, base loss: 16092.83
[INFO 2017-06-29 03:25:00,082 main.py:57] epoch 3847, training loss: 7339.40, average training loss: 7673.71, base loss: 16092.55
[INFO 2017-06-29 03:25:03,113 main.py:57] epoch 3848, training loss: 7207.15, average training loss: 7674.08, base loss: 16092.73
[INFO 2017-06-29 03:25:06,160 main.py:57] epoch 3849, training loss: 7186.47, average training loss: 7674.14, base loss: 16092.74
[INFO 2017-06-29 03:25:09,166 main.py:57] epoch 3850, training loss: 7268.29, average training loss: 7673.70, base loss: 16092.63
[INFO 2017-06-29 03:25:12,199 main.py:57] epoch 3851, training loss: 6375.20, average training loss: 7670.79, base loss: 16091.92
[INFO 2017-06-29 03:25:15,233 main.py:57] epoch 3852, training loss: 7187.86, average training loss: 7670.08, base loss: 16091.94
[INFO 2017-06-29 03:25:18,283 main.py:57] epoch 3853, training loss: 8247.16, average training loss: 7670.80, base loss: 16092.65
[INFO 2017-06-29 03:25:21,358 main.py:57] epoch 3854, training loss: 7299.89, average training loss: 7670.32, base loss: 16092.65
[INFO 2017-06-29 03:25:24,347 main.py:57] epoch 3855, training loss: 7365.65, average training loss: 7669.74, base loss: 16092.85
[INFO 2017-06-29 03:25:27,387 main.py:57] epoch 3856, training loss: 8867.08, average training loss: 7671.06, base loss: 16093.94
[INFO 2017-06-29 03:25:30,502 main.py:57] epoch 3857, training loss: 8318.47, average training loss: 7670.82, base loss: 16094.21
[INFO 2017-06-29 03:25:33,575 main.py:57] epoch 3858, training loss: 7213.19, average training loss: 7669.74, base loss: 16094.28
[INFO 2017-06-29 03:25:36,651 main.py:57] epoch 3859, training loss: 7206.15, average training loss: 7670.21, base loss: 16093.80
[INFO 2017-06-29 03:25:39,711 main.py:57] epoch 3860, training loss: 8970.97, average training loss: 7671.93, base loss: 16095.10
[INFO 2017-06-29 03:25:42,827 main.py:57] epoch 3861, training loss: 6623.24, average training loss: 7671.26, base loss: 16094.51
[INFO 2017-06-29 03:25:45,829 main.py:57] epoch 3862, training loss: 6786.47, average training loss: 7668.72, base loss: 16094.32
[INFO 2017-06-29 03:25:48,908 main.py:57] epoch 3863, training loss: 6603.73, average training loss: 7667.56, base loss: 16094.03
[INFO 2017-06-29 03:25:52,068 main.py:57] epoch 3864, training loss: 5990.35, average training loss: 7666.06, base loss: 16093.05
[INFO 2017-06-29 03:25:55,159 main.py:57] epoch 3865, training loss: 8361.74, average training loss: 7666.29, base loss: 16094.31
[INFO 2017-06-29 03:25:58,171 main.py:57] epoch 3866, training loss: 6783.89, average training loss: 7664.92, base loss: 16093.70
[INFO 2017-06-29 03:26:01,244 main.py:57] epoch 3867, training loss: 7972.62, average training loss: 7666.22, base loss: 16093.76
[INFO 2017-06-29 03:26:04,360 main.py:57] epoch 3868, training loss: 6857.38, average training loss: 7664.46, base loss: 16093.18
[INFO 2017-06-29 03:26:07,383 main.py:57] epoch 3869, training loss: 7376.25, average training loss: 7664.03, base loss: 16093.15
[INFO 2017-06-29 03:26:10,435 main.py:57] epoch 3870, training loss: 7186.28, average training loss: 7664.05, base loss: 16092.47
[INFO 2017-06-29 03:26:13,587 main.py:57] epoch 3871, training loss: 6826.30, average training loss: 7664.23, base loss: 16091.86
[INFO 2017-06-29 03:26:16,639 main.py:57] epoch 3872, training loss: 7812.40, average training loss: 7664.49, base loss: 16092.50
[INFO 2017-06-29 03:26:19,650 main.py:57] epoch 3873, training loss: 7525.71, average training loss: 7665.54, base loss: 16093.71
[INFO 2017-06-29 03:26:22,688 main.py:57] epoch 3874, training loss: 7705.44, average training loss: 7665.01, base loss: 16093.68
[INFO 2017-06-29 03:26:25,676 main.py:57] epoch 3875, training loss: 9157.17, average training loss: 7666.71, base loss: 16094.62
[INFO 2017-06-29 03:26:28,785 main.py:57] epoch 3876, training loss: 7969.79, average training loss: 7667.61, base loss: 16094.63
[INFO 2017-06-29 03:26:31,846 main.py:57] epoch 3877, training loss: 8195.40, average training loss: 7668.60, base loss: 16095.04
[INFO 2017-06-29 03:26:34,924 main.py:57] epoch 3878, training loss: 7180.07, average training loss: 7668.03, base loss: 16095.38
[INFO 2017-06-29 03:26:38,057 main.py:57] epoch 3879, training loss: 7461.67, average training loss: 7668.05, base loss: 16095.50
[INFO 2017-06-29 03:26:41,095 main.py:57] epoch 3880, training loss: 7379.76, average training loss: 7666.74, base loss: 16095.57
[INFO 2017-06-29 03:26:44,124 main.py:57] epoch 3881, training loss: 8199.10, average training loss: 7666.54, base loss: 16096.60
[INFO 2017-06-29 03:26:47,321 main.py:57] epoch 3882, training loss: 7974.86, average training loss: 7667.17, base loss: 16096.87
[INFO 2017-06-29 03:26:50,360 main.py:57] epoch 3883, training loss: 7145.06, average training loss: 7666.71, base loss: 16096.58
[INFO 2017-06-29 03:26:53,461 main.py:57] epoch 3884, training loss: 6916.61, average training loss: 7666.12, base loss: 16095.51
[INFO 2017-06-29 03:26:56,497 main.py:57] epoch 3885, training loss: 6616.07, average training loss: 7665.26, base loss: 16095.40
[INFO 2017-06-29 03:26:59,534 main.py:57] epoch 3886, training loss: 8028.49, average training loss: 7665.44, base loss: 16095.74
[INFO 2017-06-29 03:27:02,659 main.py:57] epoch 3887, training loss: 7226.01, average training loss: 7663.88, base loss: 16094.96
[INFO 2017-06-29 03:27:05,760 main.py:57] epoch 3888, training loss: 8783.31, average training loss: 7664.94, base loss: 16095.18
[INFO 2017-06-29 03:27:08,811 main.py:57] epoch 3889, training loss: 7724.19, average training loss: 7665.27, base loss: 16094.96
[INFO 2017-06-29 03:27:11,918 main.py:57] epoch 3890, training loss: 9180.31, average training loss: 7666.68, base loss: 16096.56
[INFO 2017-06-29 03:27:14,961 main.py:57] epoch 3891, training loss: 8207.96, average training loss: 7667.33, base loss: 16097.64
[INFO 2017-06-29 03:27:18,078 main.py:57] epoch 3892, training loss: 6970.30, average training loss: 7665.76, base loss: 16096.35
[INFO 2017-06-29 03:27:21,114 main.py:57] epoch 3893, training loss: 8656.73, average training loss: 7667.64, base loss: 16096.15
[INFO 2017-06-29 03:27:24,178 main.py:57] epoch 3894, training loss: 8675.81, average training loss: 7668.81, base loss: 16096.70
[INFO 2017-06-29 03:27:27,329 main.py:57] epoch 3895, training loss: 7963.06, average training loss: 7669.25, base loss: 16096.53
[INFO 2017-06-29 03:27:30,421 main.py:57] epoch 3896, training loss: 7753.20, average training loss: 7668.83, base loss: 16097.05
[INFO 2017-06-29 03:27:33,426 main.py:57] epoch 3897, training loss: 6882.67, average training loss: 7668.55, base loss: 16096.95
[INFO 2017-06-29 03:27:36,483 main.py:57] epoch 3898, training loss: 7059.79, average training loss: 7666.81, base loss: 16097.01
[INFO 2017-06-29 03:27:39,503 main.py:57] epoch 3899, training loss: 6598.71, average training loss: 7665.83, base loss: 16096.50
[INFO 2017-06-29 03:27:39,504 main.py:59] epoch 3899, testing
[INFO 2017-06-29 03:27:52,157 main.py:104] average testing loss: 7877.49, base loss: 15935.26
[INFO 2017-06-29 03:27:52,157 main.py:105] improve_loss: 8057.77, improve_percent: 0.51
[INFO 2017-06-29 03:27:52,159 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:27:55,233 main.py:57] epoch 3900, training loss: 7399.61, average training loss: 7666.30, base loss: 16096.29
[INFO 2017-06-29 03:27:58,227 main.py:57] epoch 3901, training loss: 7590.34, average training loss: 7665.80, base loss: 16096.15
[INFO 2017-06-29 03:28:01,308 main.py:57] epoch 3902, training loss: 6620.67, average training loss: 7666.10, base loss: 16095.60
[INFO 2017-06-29 03:28:04,351 main.py:57] epoch 3903, training loss: 7171.46, average training loss: 7665.28, base loss: 16095.89
[INFO 2017-06-29 03:28:07,368 main.py:57] epoch 3904, training loss: 7756.55, average training loss: 7665.64, base loss: 16096.08
[INFO 2017-06-29 03:28:10,402 main.py:57] epoch 3905, training loss: 7437.66, average training loss: 7665.74, base loss: 16095.44
[INFO 2017-06-29 03:28:13,423 main.py:57] epoch 3906, training loss: 8552.39, average training loss: 7665.93, base loss: 16096.40
[INFO 2017-06-29 03:28:16,428 main.py:57] epoch 3907, training loss: 7077.96, average training loss: 7664.88, base loss: 16095.92
[INFO 2017-06-29 03:28:19,446 main.py:57] epoch 3908, training loss: 7765.32, average training loss: 7664.68, base loss: 16095.94
[INFO 2017-06-29 03:28:22,488 main.py:57] epoch 3909, training loss: 8161.28, average training loss: 7665.48, base loss: 16096.23
[INFO 2017-06-29 03:28:25,542 main.py:57] epoch 3910, training loss: 7906.29, average training loss: 7666.22, base loss: 16096.65
[INFO 2017-06-29 03:28:28,596 main.py:57] epoch 3911, training loss: 7594.93, average training loss: 7665.42, base loss: 16096.43
[INFO 2017-06-29 03:28:31,668 main.py:57] epoch 3912, training loss: 8048.90, average training loss: 7665.64, base loss: 16097.23
[INFO 2017-06-29 03:28:34,632 main.py:57] epoch 3913, training loss: 7111.60, average training loss: 7665.65, base loss: 16096.11
[INFO 2017-06-29 03:28:37,653 main.py:57] epoch 3914, training loss: 7330.21, average training loss: 7664.01, base loss: 16096.54
[INFO 2017-06-29 03:28:40,741 main.py:57] epoch 3915, training loss: 7223.79, average training loss: 7663.44, base loss: 16096.68
[INFO 2017-06-29 03:28:43,785 main.py:57] epoch 3916, training loss: 6811.94, average training loss: 7662.84, base loss: 16096.36
[INFO 2017-06-29 03:28:46,823 main.py:57] epoch 3917, training loss: 7631.16, average training loss: 7662.77, base loss: 16096.25
[INFO 2017-06-29 03:28:49,948 main.py:57] epoch 3918, training loss: 6390.42, average training loss: 7661.67, base loss: 16094.70
[INFO 2017-06-29 03:28:53,024 main.py:57] epoch 3919, training loss: 7121.64, average training loss: 7660.60, base loss: 16094.02
[INFO 2017-06-29 03:28:56,157 main.py:57] epoch 3920, training loss: 7322.38, average training loss: 7659.77, base loss: 16093.47
[INFO 2017-06-29 03:28:59,201 main.py:57] epoch 3921, training loss: 6853.94, average training loss: 7658.66, base loss: 16092.89
[INFO 2017-06-29 03:29:02,210 main.py:57] epoch 3922, training loss: 7123.44, average training loss: 7659.10, base loss: 16092.60
[INFO 2017-06-29 03:29:05,265 main.py:57] epoch 3923, training loss: 7202.26, average training loss: 7657.70, base loss: 16092.37
[INFO 2017-06-29 03:29:08,306 main.py:57] epoch 3924, training loss: 8357.43, average training loss: 7659.25, base loss: 16092.98
[INFO 2017-06-29 03:29:11,376 main.py:57] epoch 3925, training loss: 8220.80, average training loss: 7660.19, base loss: 16093.03
[INFO 2017-06-29 03:29:14,394 main.py:57] epoch 3926, training loss: 8387.88, average training loss: 7659.61, base loss: 16093.49
[INFO 2017-06-29 03:29:17,450 main.py:57] epoch 3927, training loss: 8196.82, average training loss: 7660.07, base loss: 16093.74
[INFO 2017-06-29 03:29:20,541 main.py:57] epoch 3928, training loss: 6999.56, average training loss: 7659.84, base loss: 16093.01
[INFO 2017-06-29 03:29:23,538 main.py:57] epoch 3929, training loss: 7155.33, average training loss: 7659.01, base loss: 16092.41
[INFO 2017-06-29 03:29:26,660 main.py:57] epoch 3930, training loss: 7733.67, average training loss: 7659.70, base loss: 16092.33
[INFO 2017-06-29 03:29:29,682 main.py:57] epoch 3931, training loss: 8065.23, average training loss: 7659.54, base loss: 16092.80
[INFO 2017-06-29 03:29:32,733 main.py:57] epoch 3932, training loss: 7689.52, average training loss: 7658.13, base loss: 16093.15
[INFO 2017-06-29 03:29:35,815 main.py:57] epoch 3933, training loss: 7941.12, average training loss: 7658.45, base loss: 16093.70
[INFO 2017-06-29 03:29:38,809 main.py:57] epoch 3934, training loss: 7409.75, average training loss: 7658.67, base loss: 16094.28
[INFO 2017-06-29 03:29:41,821 main.py:57] epoch 3935, training loss: 8317.05, average training loss: 7659.65, base loss: 16095.41
[INFO 2017-06-29 03:29:44,964 main.py:57] epoch 3936, training loss: 7059.50, average training loss: 7658.51, base loss: 16094.50
[INFO 2017-06-29 03:29:48,076 main.py:57] epoch 3937, training loss: 6736.50, average training loss: 7658.37, base loss: 16093.96
[INFO 2017-06-29 03:29:51,186 main.py:57] epoch 3938, training loss: 7696.97, average training loss: 7658.14, base loss: 16093.95
[INFO 2017-06-29 03:29:54,246 main.py:57] epoch 3939, training loss: 7791.42, average training loss: 7658.43, base loss: 16093.96
[INFO 2017-06-29 03:29:57,248 main.py:57] epoch 3940, training loss: 8084.15, average training loss: 7659.51, base loss: 16094.13
[INFO 2017-06-29 03:30:00,325 main.py:57] epoch 3941, training loss: 8309.96, average training loss: 7659.55, base loss: 16094.16
[INFO 2017-06-29 03:30:03,337 main.py:57] epoch 3942, training loss: 7697.75, average training loss: 7659.87, base loss: 16093.99
[INFO 2017-06-29 03:30:06,396 main.py:57] epoch 3943, training loss: 7366.60, average training loss: 7659.99, base loss: 16093.54
[INFO 2017-06-29 03:30:09,420 main.py:57] epoch 3944, training loss: 8062.06, average training loss: 7660.80, base loss: 16093.44
[INFO 2017-06-29 03:30:12,477 main.py:57] epoch 3945, training loss: 7880.37, average training loss: 7661.03, base loss: 16093.28
[INFO 2017-06-29 03:30:15,519 main.py:57] epoch 3946, training loss: 6669.16, average training loss: 7660.65, base loss: 16092.43
[INFO 2017-06-29 03:30:18,537 main.py:57] epoch 3947, training loss: 7837.68, average training loss: 7661.99, base loss: 16092.74
[INFO 2017-06-29 03:30:21,617 main.py:57] epoch 3948, training loss: 7886.42, average training loss: 7661.66, base loss: 16093.68
[INFO 2017-06-29 03:30:24,660 main.py:57] epoch 3949, training loss: 7577.76, average training loss: 7660.65, base loss: 16094.29
[INFO 2017-06-29 03:30:27,687 main.py:57] epoch 3950, training loss: 8804.49, average training loss: 7661.11, base loss: 16095.61
[INFO 2017-06-29 03:30:30,746 main.py:57] epoch 3951, training loss: 8201.41, average training loss: 7661.82, base loss: 16095.33
[INFO 2017-06-29 03:30:33,859 main.py:57] epoch 3952, training loss: 8460.77, average training loss: 7663.65, base loss: 16095.78
[INFO 2017-06-29 03:30:36,928 main.py:57] epoch 3953, training loss: 7203.17, average training loss: 7663.24, base loss: 16095.67
[INFO 2017-06-29 03:30:39,995 main.py:57] epoch 3954, training loss: 7408.72, average training loss: 7662.83, base loss: 16094.87
[INFO 2017-06-29 03:30:43,090 main.py:57] epoch 3955, training loss: 7815.07, average training loss: 7663.15, base loss: 16094.71
[INFO 2017-06-29 03:30:46,152 main.py:57] epoch 3956, training loss: 7850.77, average training loss: 7663.18, base loss: 16095.65
[INFO 2017-06-29 03:30:49,215 main.py:57] epoch 3957, training loss: 7393.56, average training loss: 7663.14, base loss: 16096.07
[INFO 2017-06-29 03:30:52,325 main.py:57] epoch 3958, training loss: 7049.17, average training loss: 7663.46, base loss: 16095.95
[INFO 2017-06-29 03:30:55,451 main.py:57] epoch 3959, training loss: 8011.16, average training loss: 7664.37, base loss: 16097.05
[INFO 2017-06-29 03:30:58,520 main.py:57] epoch 3960, training loss: 7675.31, average training loss: 7664.94, base loss: 16097.53
[INFO 2017-06-29 03:31:01,543 main.py:57] epoch 3961, training loss: 6730.55, average training loss: 7663.41, base loss: 16097.07
[INFO 2017-06-29 03:31:04,577 main.py:57] epoch 3962, training loss: 6834.95, average training loss: 7661.99, base loss: 16095.62
[INFO 2017-06-29 03:31:07,646 main.py:57] epoch 3963, training loss: 7565.36, average training loss: 7662.04, base loss: 16095.00
[INFO 2017-06-29 03:31:10,686 main.py:57] epoch 3964, training loss: 6585.96, average training loss: 7660.79, base loss: 16093.44
[INFO 2017-06-29 03:31:13,754 main.py:57] epoch 3965, training loss: 7710.44, average training loss: 7660.88, base loss: 16092.64
[INFO 2017-06-29 03:31:16,816 main.py:57] epoch 3966, training loss: 8829.78, average training loss: 7661.48, base loss: 16093.97
[INFO 2017-06-29 03:31:19,857 main.py:57] epoch 3967, training loss: 7119.46, average training loss: 7659.70, base loss: 16093.60
[INFO 2017-06-29 03:31:22,874 main.py:57] epoch 3968, training loss: 7251.99, average training loss: 7658.34, base loss: 16093.28
[INFO 2017-06-29 03:31:25,893 main.py:57] epoch 3969, training loss: 7265.55, average training loss: 7657.07, base loss: 16092.08
[INFO 2017-06-29 03:31:28,951 main.py:57] epoch 3970, training loss: 7284.04, average training loss: 7656.18, base loss: 16091.37
[INFO 2017-06-29 03:31:32,041 main.py:57] epoch 3971, training loss: 7920.65, average training loss: 7656.15, base loss: 16091.06
[INFO 2017-06-29 03:31:35,150 main.py:57] epoch 3972, training loss: 7581.75, average training loss: 7656.22, base loss: 16091.17
[INFO 2017-06-29 03:31:38,178 main.py:57] epoch 3973, training loss: 8296.64, average training loss: 7657.12, base loss: 16092.09
[INFO 2017-06-29 03:31:41,301 main.py:57] epoch 3974, training loss: 7750.25, average training loss: 7658.44, base loss: 16091.72
[INFO 2017-06-29 03:31:44,314 main.py:57] epoch 3975, training loss: 8436.11, average training loss: 7659.02, base loss: 16092.30
[INFO 2017-06-29 03:31:47,389 main.py:57] epoch 3976, training loss: 8878.52, average training loss: 7660.64, base loss: 16093.62
[INFO 2017-06-29 03:31:50,416 main.py:57] epoch 3977, training loss: 7679.76, average training loss: 7661.38, base loss: 16093.57
[INFO 2017-06-29 03:31:53,458 main.py:57] epoch 3978, training loss: 7470.73, average training loss: 7661.71, base loss: 16093.90
[INFO 2017-06-29 03:31:56,447 main.py:57] epoch 3979, training loss: 7273.42, average training loss: 7662.33, base loss: 16093.67
[INFO 2017-06-29 03:31:59,445 main.py:57] epoch 3980, training loss: 7430.77, average training loss: 7662.80, base loss: 16093.14
[INFO 2017-06-29 03:32:02,497 main.py:57] epoch 3981, training loss: 8001.89, average training loss: 7662.80, base loss: 16093.23
[INFO 2017-06-29 03:32:05,546 main.py:57] epoch 3982, training loss: 8200.16, average training loss: 7662.96, base loss: 16093.11
[INFO 2017-06-29 03:32:08,640 main.py:57] epoch 3983, training loss: 8570.85, average training loss: 7663.49, base loss: 16093.61
[INFO 2017-06-29 03:32:11,693 main.py:57] epoch 3984, training loss: 7496.45, average training loss: 7662.71, base loss: 16092.86
[INFO 2017-06-29 03:32:14,719 main.py:57] epoch 3985, training loss: 7279.14, average training loss: 7662.06, base loss: 16091.87
[INFO 2017-06-29 03:32:17,784 main.py:57] epoch 3986, training loss: 6995.67, average training loss: 7662.19, base loss: 16091.01
[INFO 2017-06-29 03:32:20,816 main.py:57] epoch 3987, training loss: 8075.95, average training loss: 7662.49, base loss: 16091.53
[INFO 2017-06-29 03:32:23,879 main.py:57] epoch 3988, training loss: 6950.21, average training loss: 7661.02, base loss: 16091.32
[INFO 2017-06-29 03:32:26,975 main.py:57] epoch 3989, training loss: 7847.33, average training loss: 7661.65, base loss: 16092.29
[INFO 2017-06-29 03:32:30,027 main.py:57] epoch 3990, training loss: 7647.09, average training loss: 7661.47, base loss: 16092.41
[INFO 2017-06-29 03:32:33,036 main.py:57] epoch 3991, training loss: 8159.20, average training loss: 7660.70, base loss: 16092.88
[INFO 2017-06-29 03:32:36,083 main.py:57] epoch 3992, training loss: 7524.96, average training loss: 7661.09, base loss: 16093.08
[INFO 2017-06-29 03:32:39,183 main.py:57] epoch 3993, training loss: 6831.78, average training loss: 7660.71, base loss: 16092.84
[INFO 2017-06-29 03:32:42,199 main.py:57] epoch 3994, training loss: 6846.34, average training loss: 7660.34, base loss: 16092.38
[INFO 2017-06-29 03:32:45,260 main.py:57] epoch 3995, training loss: 7634.89, average training loss: 7661.14, base loss: 16092.25
[INFO 2017-06-29 03:32:48,305 main.py:57] epoch 3996, training loss: 6397.57, average training loss: 7660.15, base loss: 16091.50
[INFO 2017-06-29 03:32:51,325 main.py:57] epoch 3997, training loss: 7632.58, average training loss: 7660.44, base loss: 16091.54
[INFO 2017-06-29 03:32:54,415 main.py:57] epoch 3998, training loss: 8039.85, average training loss: 7659.90, base loss: 16092.45
[INFO 2017-06-29 03:32:57,440 main.py:57] epoch 3999, training loss: 7580.32, average training loss: 7660.48, base loss: 16092.27
[INFO 2017-06-29 03:32:57,440 main.py:59] epoch 3999, testing
[INFO 2017-06-29 03:33:10,208 main.py:104] average testing loss: 8136.82, base loss: 16590.56
[INFO 2017-06-29 03:33:10,209 main.py:105] improve_loss: 8453.74, improve_percent: 0.51
[INFO 2017-06-29 03:33:10,210 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:33:13,264 main.py:57] epoch 4000, training loss: 7717.53, average training loss: 7661.77, base loss: 16091.45
[INFO 2017-06-29 03:33:16,279 main.py:57] epoch 4001, training loss: 8009.63, average training loss: 7662.95, base loss: 16091.43
[INFO 2017-06-29 03:33:19,319 main.py:57] epoch 4002, training loss: 6938.46, average training loss: 7661.64, base loss: 16091.12
[INFO 2017-06-29 03:33:22,371 main.py:57] epoch 4003, training loss: 9476.58, average training loss: 7663.64, base loss: 16092.23
[INFO 2017-06-29 03:33:25,425 main.py:57] epoch 4004, training loss: 7562.70, average training loss: 7663.50, base loss: 16092.51
[INFO 2017-06-29 03:33:28,462 main.py:57] epoch 4005, training loss: 7681.09, average training loss: 7663.78, base loss: 16092.56
[INFO 2017-06-29 03:33:31,545 main.py:57] epoch 4006, training loss: 8508.65, average training loss: 7664.43, base loss: 16092.82
[INFO 2017-06-29 03:33:34,601 main.py:57] epoch 4007, training loss: 7147.59, average training loss: 7664.16, base loss: 16092.70
[INFO 2017-06-29 03:33:37,630 main.py:57] epoch 4008, training loss: 7612.02, average training loss: 7664.87, base loss: 16091.70
[INFO 2017-06-29 03:33:40,638 main.py:57] epoch 4009, training loss: 8845.86, average training loss: 7666.15, base loss: 16092.37
[INFO 2017-06-29 03:33:43,640 main.py:57] epoch 4010, training loss: 6723.64, average training loss: 7663.87, base loss: 16090.89
[INFO 2017-06-29 03:33:46,739 main.py:57] epoch 4011, training loss: 8233.59, average training loss: 7664.29, base loss: 16090.33
[INFO 2017-06-29 03:33:49,770 main.py:57] epoch 4012, training loss: 9269.03, average training loss: 7665.27, base loss: 16091.36
[INFO 2017-06-29 03:33:52,881 main.py:57] epoch 4013, training loss: 7351.12, average training loss: 7665.04, base loss: 16091.51
[INFO 2017-06-29 03:33:55,951 main.py:57] epoch 4014, training loss: 8599.41, average training loss: 7666.36, base loss: 16091.60
[INFO 2017-06-29 03:33:59,012 main.py:57] epoch 4015, training loss: 8077.85, average training loss: 7667.02, base loss: 16091.04
[INFO 2017-06-29 03:34:02,049 main.py:57] epoch 4016, training loss: 7223.05, average training loss: 7667.17, base loss: 16091.20
[INFO 2017-06-29 03:34:05,105 main.py:57] epoch 4017, training loss: 6954.83, average training loss: 7666.19, base loss: 16091.09
[INFO 2017-06-29 03:34:08,201 main.py:57] epoch 4018, training loss: 8259.42, average training loss: 7667.52, base loss: 16091.27
[INFO 2017-06-29 03:34:11,312 main.py:57] epoch 4019, training loss: 6972.64, average training loss: 7667.27, base loss: 16090.55
[INFO 2017-06-29 03:34:14,386 main.py:57] epoch 4020, training loss: 7484.10, average training loss: 7667.22, base loss: 16089.78
[INFO 2017-06-29 03:34:17,374 main.py:57] epoch 4021, training loss: 8089.61, average training loss: 7668.10, base loss: 16090.11
[INFO 2017-06-29 03:34:20,483 main.py:57] epoch 4022, training loss: 7736.89, average training loss: 7668.29, base loss: 16090.14
[INFO 2017-06-29 03:34:23,479 main.py:57] epoch 4023, training loss: 7570.17, average training loss: 7668.50, base loss: 16090.17
[INFO 2017-06-29 03:34:26,495 main.py:57] epoch 4024, training loss: 7366.48, average training loss: 7668.62, base loss: 16090.54
[INFO 2017-06-29 03:34:29,555 main.py:57] epoch 4025, training loss: 7772.81, average training loss: 7669.75, base loss: 16090.69
[INFO 2017-06-29 03:34:32,627 main.py:57] epoch 4026, training loss: 7040.94, average training loss: 7669.90, base loss: 16089.68
[INFO 2017-06-29 03:34:35,671 main.py:57] epoch 4027, training loss: 8000.85, average training loss: 7669.98, base loss: 16089.42
[INFO 2017-06-29 03:34:38,733 main.py:57] epoch 4028, training loss: 7821.94, average training loss: 7670.70, base loss: 16090.37
[INFO 2017-06-29 03:34:41,789 main.py:57] epoch 4029, training loss: 7029.69, average training loss: 7669.82, base loss: 16090.30
[INFO 2017-06-29 03:34:44,801 main.py:57] epoch 4030, training loss: 8330.28, average training loss: 7670.10, base loss: 16089.71
[INFO 2017-06-29 03:34:47,864 main.py:57] epoch 4031, training loss: 7470.46, average training loss: 7670.07, base loss: 16089.42
[INFO 2017-06-29 03:34:50,999 main.py:57] epoch 4032, training loss: 7907.08, average training loss: 7669.51, base loss: 16089.96
[INFO 2017-06-29 03:34:54,024 main.py:57] epoch 4033, training loss: 6873.53, average training loss: 7669.57, base loss: 16089.91
[INFO 2017-06-29 03:34:57,068 main.py:57] epoch 4034, training loss: 7800.14, average training loss: 7669.18, base loss: 16090.11
[INFO 2017-06-29 03:35:00,165 main.py:57] epoch 4035, training loss: 7501.51, average training loss: 7669.22, base loss: 16090.53
[INFO 2017-06-29 03:35:03,211 main.py:57] epoch 4036, training loss: 8123.69, average training loss: 7669.05, base loss: 16092.11
[INFO 2017-06-29 03:35:06,302 main.py:57] epoch 4037, training loss: 7174.73, average training loss: 7669.16, base loss: 16091.86
[INFO 2017-06-29 03:35:09,310 main.py:57] epoch 4038, training loss: 8474.23, average training loss: 7669.69, base loss: 16091.95
[INFO 2017-06-29 03:35:12,386 main.py:57] epoch 4039, training loss: 7301.87, average training loss: 7669.12, base loss: 16092.35
[INFO 2017-06-29 03:35:15,431 main.py:57] epoch 4040, training loss: 7627.18, average training loss: 7668.24, base loss: 16091.69
[INFO 2017-06-29 03:35:18,490 main.py:57] epoch 4041, training loss: 6959.38, average training loss: 7666.78, base loss: 16090.94
[INFO 2017-06-29 03:35:21,522 main.py:57] epoch 4042, training loss: 6620.61, average training loss: 7664.96, base loss: 16089.59
[INFO 2017-06-29 03:35:24,610 main.py:57] epoch 4043, training loss: 7379.42, average training loss: 7664.58, base loss: 16088.06
[INFO 2017-06-29 03:35:27,644 main.py:57] epoch 4044, training loss: 8358.92, average training loss: 7665.05, base loss: 16088.67
[INFO 2017-06-29 03:35:30,705 main.py:57] epoch 4045, training loss: 8386.07, average training loss: 7665.90, base loss: 16090.53
[INFO 2017-06-29 03:35:33,831 main.py:57] epoch 4046, training loss: 8716.67, average training loss: 7666.43, base loss: 16091.05
[INFO 2017-06-29 03:35:36,932 main.py:57] epoch 4047, training loss: 7001.27, average training loss: 7666.35, base loss: 16090.30
[INFO 2017-06-29 03:35:40,028 main.py:57] epoch 4048, training loss: 8098.62, average training loss: 7666.93, base loss: 16090.33
[INFO 2017-06-29 03:35:43,075 main.py:57] epoch 4049, training loss: 7574.92, average training loss: 7666.85, base loss: 16090.09
[INFO 2017-06-29 03:35:46,163 main.py:57] epoch 4050, training loss: 7517.92, average training loss: 7666.41, base loss: 16090.33
[INFO 2017-06-29 03:35:49,224 main.py:57] epoch 4051, training loss: 6835.45, average training loss: 7666.64, base loss: 16090.20
[INFO 2017-06-29 03:35:52,267 main.py:57] epoch 4052, training loss: 7379.00, average training loss: 7665.98, base loss: 16090.17
[INFO 2017-06-29 03:35:55,380 main.py:57] epoch 4053, training loss: 7618.38, average training loss: 7665.87, base loss: 16091.01
[INFO 2017-06-29 03:35:58,487 main.py:57] epoch 4054, training loss: 8291.74, average training loss: 7666.93, base loss: 16091.23
[INFO 2017-06-29 03:36:01,584 main.py:57] epoch 4055, training loss: 8383.41, average training loss: 7668.27, base loss: 16091.97
[INFO 2017-06-29 03:36:04,685 main.py:57] epoch 4056, training loss: 6872.12, average training loss: 7668.39, base loss: 16091.50
[INFO 2017-06-29 03:36:07,732 main.py:57] epoch 4057, training loss: 6963.20, average training loss: 7669.01, base loss: 16090.76
[INFO 2017-06-29 03:36:10,723 main.py:57] epoch 4058, training loss: 6332.66, average training loss: 7667.99, base loss: 16090.22
[INFO 2017-06-29 03:36:13,714 main.py:57] epoch 4059, training loss: 6936.26, average training loss: 7665.91, base loss: 16090.30
[INFO 2017-06-29 03:36:16,892 main.py:57] epoch 4060, training loss: 7803.13, average training loss: 7664.73, base loss: 16090.33
[INFO 2017-06-29 03:36:19,977 main.py:57] epoch 4061, training loss: 8895.52, average training loss: 7665.62, base loss: 16091.42
[INFO 2017-06-29 03:36:23,015 main.py:57] epoch 4062, training loss: 7718.18, average training loss: 7666.18, base loss: 16090.81
[INFO 2017-06-29 03:36:26,051 main.py:57] epoch 4063, training loss: 7020.23, average training loss: 7665.77, base loss: 16089.77
[INFO 2017-06-29 03:36:29,085 main.py:57] epoch 4064, training loss: 8198.56, average training loss: 7666.58, base loss: 16090.00
[INFO 2017-06-29 03:36:32,228 main.py:57] epoch 4065, training loss: 8078.98, average training loss: 7666.50, base loss: 16090.25
[INFO 2017-06-29 03:36:35,270 main.py:57] epoch 4066, training loss: 7927.67, average training loss: 7665.25, base loss: 16090.78
[INFO 2017-06-29 03:36:38,417 main.py:57] epoch 4067, training loss: 8212.84, average training loss: 7665.90, base loss: 16091.37
[INFO 2017-06-29 03:36:41,475 main.py:57] epoch 4068, training loss: 7882.66, average training loss: 7665.60, base loss: 16091.75
[INFO 2017-06-29 03:36:44,520 main.py:57] epoch 4069, training loss: 7276.38, average training loss: 7664.56, base loss: 16090.97
[INFO 2017-06-29 03:36:47,571 main.py:57] epoch 4070, training loss: 7502.67, average training loss: 7663.57, base loss: 16090.75
[INFO 2017-06-29 03:36:50,548 main.py:57] epoch 4071, training loss: 7530.17, average training loss: 7663.32, base loss: 16091.06
[INFO 2017-06-29 03:36:53,538 main.py:57] epoch 4072, training loss: 6317.09, average training loss: 7661.54, base loss: 16089.07
[INFO 2017-06-29 03:36:56,579 main.py:57] epoch 4073, training loss: 7580.68, average training loss: 7661.21, base loss: 16088.55
[INFO 2017-06-29 03:36:59,624 main.py:57] epoch 4074, training loss: 7380.56, average training loss: 7661.23, base loss: 16088.43
[INFO 2017-06-29 03:37:02,674 main.py:57] epoch 4075, training loss: 7684.80, average training loss: 7660.27, base loss: 16088.58
[INFO 2017-06-29 03:37:05,716 main.py:57] epoch 4076, training loss: 7557.98, average training loss: 7661.30, base loss: 16088.73
[INFO 2017-06-29 03:37:08,705 main.py:57] epoch 4077, training loss: 8112.53, average training loss: 7662.35, base loss: 16089.26
[INFO 2017-06-29 03:37:11,846 main.py:57] epoch 4078, training loss: 7904.65, average training loss: 7663.12, base loss: 16089.76
[INFO 2017-06-29 03:37:14,891 main.py:57] epoch 4079, training loss: 7692.10, average training loss: 7663.62, base loss: 16089.61
[INFO 2017-06-29 03:37:17,870 main.py:57] epoch 4080, training loss: 7254.91, average training loss: 7663.33, base loss: 16089.18
[INFO 2017-06-29 03:37:20,934 main.py:57] epoch 4081, training loss: 6780.12, average training loss: 7663.32, base loss: 16088.55
[INFO 2017-06-29 03:37:24,037 main.py:57] epoch 4082, training loss: 7081.66, average training loss: 7663.22, base loss: 16088.49
[INFO 2017-06-29 03:37:27,156 main.py:57] epoch 4083, training loss: 8235.53, average training loss: 7664.29, base loss: 16088.94
[INFO 2017-06-29 03:37:30,136 main.py:57] epoch 4084, training loss: 6278.94, average training loss: 7662.98, base loss: 16088.34
[INFO 2017-06-29 03:37:33,197 main.py:57] epoch 4085, training loss: 8022.17, average training loss: 7661.88, base loss: 16089.35
[INFO 2017-06-29 03:37:36,267 main.py:57] epoch 4086, training loss: 7119.36, average training loss: 7660.89, base loss: 16089.40
[INFO 2017-06-29 03:37:39,288 main.py:57] epoch 4087, training loss: 7684.12, average training loss: 7661.76, base loss: 16089.64
[INFO 2017-06-29 03:37:42,289 main.py:57] epoch 4088, training loss: 7433.84, average training loss: 7661.43, base loss: 16089.19
[INFO 2017-06-29 03:37:45,371 main.py:57] epoch 4089, training loss: 7012.52, average training loss: 7660.70, base loss: 16088.96
[INFO 2017-06-29 03:37:48,445 main.py:57] epoch 4090, training loss: 6951.88, average training loss: 7660.39, base loss: 16088.44
[INFO 2017-06-29 03:37:51,499 main.py:57] epoch 4091, training loss: 6924.10, average training loss: 7660.62, base loss: 16087.66
[INFO 2017-06-29 03:37:54,551 main.py:57] epoch 4092, training loss: 7643.81, average training loss: 7659.82, base loss: 16087.77
[INFO 2017-06-29 03:37:57,640 main.py:57] epoch 4093, training loss: 8239.00, average training loss: 7659.17, base loss: 16087.32
[INFO 2017-06-29 03:38:00,652 main.py:57] epoch 4094, training loss: 7982.86, average training loss: 7658.98, base loss: 16088.21
[INFO 2017-06-29 03:38:03,691 main.py:57] epoch 4095, training loss: 7009.78, average training loss: 7659.42, base loss: 16087.90
[INFO 2017-06-29 03:38:06,724 main.py:57] epoch 4096, training loss: 7432.46, average training loss: 7658.80, base loss: 16087.52
[INFO 2017-06-29 03:38:09,805 main.py:57] epoch 4097, training loss: 8611.89, average training loss: 7660.38, base loss: 16088.80
[INFO 2017-06-29 03:38:12,850 main.py:57] epoch 4098, training loss: 7645.13, average training loss: 7661.05, base loss: 16088.57
[INFO 2017-06-29 03:38:15,912 main.py:57] epoch 4099, training loss: 8195.31, average training loss: 7660.96, base loss: 16089.26
[INFO 2017-06-29 03:38:15,913 main.py:59] epoch 4099, testing
[INFO 2017-06-29 03:38:28,557 main.py:104] average testing loss: 8108.51, base loss: 16996.96
[INFO 2017-06-29 03:38:28,558 main.py:105] improve_loss: 8888.45, improve_percent: 0.52
[INFO 2017-06-29 03:38:28,559 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 03:38:28,597 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:38:31,635 main.py:57] epoch 4100, training loss: 6982.21, average training loss: 7660.36, base loss: 16088.47
[INFO 2017-06-29 03:38:34,702 main.py:57] epoch 4101, training loss: 7168.02, average training loss: 7660.47, base loss: 16087.79
[INFO 2017-06-29 03:38:37,754 main.py:57] epoch 4102, training loss: 6228.14, average training loss: 7660.03, base loss: 16087.35
[INFO 2017-06-29 03:38:40,721 main.py:57] epoch 4103, training loss: 6401.89, average training loss: 7658.82, base loss: 16086.66
[INFO 2017-06-29 03:38:43,809 main.py:57] epoch 4104, training loss: 9408.83, average training loss: 7660.53, base loss: 16088.03
[INFO 2017-06-29 03:38:46,891 main.py:57] epoch 4105, training loss: 7641.52, average training loss: 7660.86, base loss: 16088.56
[INFO 2017-06-29 03:38:49,902 main.py:57] epoch 4106, training loss: 6751.81, average training loss: 7660.45, base loss: 16087.86
[INFO 2017-06-29 03:38:52,959 main.py:57] epoch 4107, training loss: 8649.50, average training loss: 7661.39, base loss: 16089.51
[INFO 2017-06-29 03:38:56,092 main.py:57] epoch 4108, training loss: 7363.31, average training loss: 7661.24, base loss: 16089.52
[INFO 2017-06-29 03:38:59,118 main.py:57] epoch 4109, training loss: 8406.24, average training loss: 7661.65, base loss: 16090.53
[INFO 2017-06-29 03:39:02,126 main.py:57] epoch 4110, training loss: 7285.22, average training loss: 7661.66, base loss: 16090.91
[INFO 2017-06-29 03:39:05,234 main.py:57] epoch 4111, training loss: 8445.58, average training loss: 7661.87, base loss: 16091.60
[INFO 2017-06-29 03:39:08,287 main.py:57] epoch 4112, training loss: 7490.71, average training loss: 7662.24, base loss: 16091.99
[INFO 2017-06-29 03:39:11,381 main.py:57] epoch 4113, training loss: 8164.23, average training loss: 7662.06, base loss: 16092.58
[INFO 2017-06-29 03:39:14,409 main.py:57] epoch 4114, training loss: 7711.22, average training loss: 7659.71, base loss: 16093.03
[INFO 2017-06-29 03:39:17,389 main.py:57] epoch 4115, training loss: 7071.31, average training loss: 7659.38, base loss: 16092.92
[INFO 2017-06-29 03:39:20,455 main.py:57] epoch 4116, training loss: 7174.97, average training loss: 7658.64, base loss: 16092.69
[INFO 2017-06-29 03:39:23,485 main.py:57] epoch 4117, training loss: 7582.79, average training loss: 7658.77, base loss: 16092.20
[INFO 2017-06-29 03:39:26,564 main.py:57] epoch 4118, training loss: 6773.10, average training loss: 7657.34, base loss: 16091.78
[INFO 2017-06-29 03:39:29,594 main.py:57] epoch 4119, training loss: 7496.51, average training loss: 7657.18, base loss: 16092.17
[INFO 2017-06-29 03:39:32,644 main.py:57] epoch 4120, training loss: 7391.82, average training loss: 7657.36, base loss: 16092.01
[INFO 2017-06-29 03:39:35,624 main.py:57] epoch 4121, training loss: 7075.01, average training loss: 7656.89, base loss: 16091.14
[INFO 2017-06-29 03:39:38,658 main.py:57] epoch 4122, training loss: 7128.23, average training loss: 7656.29, base loss: 16090.43
[INFO 2017-06-29 03:39:41,662 main.py:57] epoch 4123, training loss: 6345.63, average training loss: 7656.24, base loss: 16089.04
[INFO 2017-06-29 03:39:44,714 main.py:57] epoch 4124, training loss: 9050.89, average training loss: 7657.17, base loss: 16090.91
[INFO 2017-06-29 03:39:47,885 main.py:57] epoch 4125, training loss: 7337.34, average training loss: 7657.05, base loss: 16090.62
[INFO 2017-06-29 03:39:50,958 main.py:57] epoch 4126, training loss: 7865.80, average training loss: 7656.96, base loss: 16091.20
[INFO 2017-06-29 03:39:54,086 main.py:57] epoch 4127, training loss: 9099.34, average training loss: 7657.81, base loss: 16091.77
[INFO 2017-06-29 03:39:57,032 main.py:57] epoch 4128, training loss: 8085.50, average training loss: 7657.78, base loss: 16091.81
[INFO 2017-06-29 03:40:00,112 main.py:57] epoch 4129, training loss: 8110.50, average training loss: 7656.92, base loss: 16091.54
[INFO 2017-06-29 03:40:03,176 main.py:57] epoch 4130, training loss: 8540.50, average training loss: 7658.03, base loss: 16092.53
[INFO 2017-06-29 03:40:06,236 main.py:57] epoch 4131, training loss: 7113.72, average training loss: 7656.89, base loss: 16091.58
[INFO 2017-06-29 03:40:09,289 main.py:57] epoch 4132, training loss: 7106.53, average training loss: 7656.62, base loss: 16091.53
[INFO 2017-06-29 03:40:12,382 main.py:57] epoch 4133, training loss: 8297.14, average training loss: 7656.81, base loss: 16092.38
[INFO 2017-06-29 03:40:15,433 main.py:57] epoch 4134, training loss: 7482.03, average training loss: 7655.67, base loss: 16092.26
[INFO 2017-06-29 03:40:18,483 main.py:57] epoch 4135, training loss: 8032.17, average training loss: 7656.20, base loss: 16092.56
[INFO 2017-06-29 03:40:21,520 main.py:57] epoch 4136, training loss: 8053.80, average training loss: 7656.39, base loss: 16093.13
[INFO 2017-06-29 03:40:24,506 main.py:57] epoch 4137, training loss: 7543.47, average training loss: 7657.41, base loss: 16093.32
[INFO 2017-06-29 03:40:27,537 main.py:57] epoch 4138, training loss: 8296.71, average training loss: 7656.47, base loss: 16094.51
[INFO 2017-06-29 03:40:30,541 main.py:57] epoch 4139, training loss: 7965.76, average training loss: 7657.37, base loss: 16094.90
[INFO 2017-06-29 03:40:33,568 main.py:57] epoch 4140, training loss: 7783.41, average training loss: 7657.98, base loss: 16094.29
[INFO 2017-06-29 03:40:36,629 main.py:57] epoch 4141, training loss: 7762.44, average training loss: 7658.26, base loss: 16093.20
[INFO 2017-06-29 03:40:39,724 main.py:57] epoch 4142, training loss: 8111.12, average training loss: 7658.51, base loss: 16094.05
[INFO 2017-06-29 03:40:42,789 main.py:57] epoch 4143, training loss: 7168.40, average training loss: 7658.39, base loss: 16093.86
[INFO 2017-06-29 03:40:45,842 main.py:57] epoch 4144, training loss: 8247.43, average training loss: 7659.01, base loss: 16095.63
[INFO 2017-06-29 03:40:48,911 main.py:57] epoch 4145, training loss: 7545.30, average training loss: 7658.19, base loss: 16095.73
[INFO 2017-06-29 03:40:51,953 main.py:57] epoch 4146, training loss: 7673.80, average training loss: 7658.15, base loss: 16096.19
[INFO 2017-06-29 03:40:55,050 main.py:57] epoch 4147, training loss: 7355.54, average training loss: 7656.88, base loss: 16095.16
[INFO 2017-06-29 03:40:58,123 main.py:57] epoch 4148, training loss: 6960.41, average training loss: 7656.12, base loss: 16095.11
[INFO 2017-06-29 03:41:01,210 main.py:57] epoch 4149, training loss: 8379.00, average training loss: 7656.61, base loss: 16096.46
[INFO 2017-06-29 03:41:04,231 main.py:57] epoch 4150, training loss: 7099.71, average training loss: 7656.71, base loss: 16096.02
[INFO 2017-06-29 03:41:07,278 main.py:57] epoch 4151, training loss: 8176.39, average training loss: 7655.90, base loss: 16096.96
[INFO 2017-06-29 03:41:10,371 main.py:57] epoch 4152, training loss: 7154.59, average training loss: 7654.65, base loss: 16097.44
[INFO 2017-06-29 03:41:13,363 main.py:57] epoch 4153, training loss: 7610.27, average training loss: 7655.33, base loss: 16097.16
[INFO 2017-06-29 03:41:16,392 main.py:57] epoch 4154, training loss: 7869.77, average training loss: 7655.59, base loss: 16097.21
[INFO 2017-06-29 03:41:19,436 main.py:57] epoch 4155, training loss: 7313.93, average training loss: 7655.57, base loss: 16096.59
[INFO 2017-06-29 03:41:22,491 main.py:57] epoch 4156, training loss: 7678.71, average training loss: 7655.60, base loss: 16097.03
[INFO 2017-06-29 03:41:25,527 main.py:57] epoch 4157, training loss: 7874.63, average training loss: 7655.45, base loss: 16097.11
[INFO 2017-06-29 03:41:28,630 main.py:57] epoch 4158, training loss: 7990.61, average training loss: 7655.49, base loss: 16096.85
[INFO 2017-06-29 03:41:31,694 main.py:57] epoch 4159, training loss: 6848.35, average training loss: 7654.17, base loss: 16095.56
[INFO 2017-06-29 03:41:34,745 main.py:57] epoch 4160, training loss: 7258.25, average training loss: 7653.39, base loss: 16096.37
[INFO 2017-06-29 03:41:37,815 main.py:57] epoch 4161, training loss: 7993.15, average training loss: 7654.07, base loss: 16097.08
[INFO 2017-06-29 03:41:40,830 main.py:57] epoch 4162, training loss: 8132.63, average training loss: 7653.15, base loss: 16097.40
[INFO 2017-06-29 03:41:43,868 main.py:57] epoch 4163, training loss: 7333.51, average training loss: 7652.18, base loss: 16097.23
[INFO 2017-06-29 03:41:46,984 main.py:57] epoch 4164, training loss: 7410.44, average training loss: 7651.19, base loss: 16097.50
[INFO 2017-06-29 03:41:50,072 main.py:57] epoch 4165, training loss: 9181.41, average training loss: 7652.58, base loss: 16099.33
[INFO 2017-06-29 03:41:53,101 main.py:57] epoch 4166, training loss: 7781.53, average training loss: 7651.97, base loss: 16099.47
[INFO 2017-06-29 03:41:56,100 main.py:57] epoch 4167, training loss: 7157.67, average training loss: 7651.25, base loss: 16098.66
[INFO 2017-06-29 03:41:59,141 main.py:57] epoch 4168, training loss: 7624.16, average training loss: 7651.33, base loss: 16098.51
[INFO 2017-06-29 03:42:02,217 main.py:57] epoch 4169, training loss: 6664.43, average training loss: 7651.38, base loss: 16097.77
[INFO 2017-06-29 03:42:05,348 main.py:57] epoch 4170, training loss: 8172.85, average training loss: 7652.09, base loss: 16097.46
[INFO 2017-06-29 03:42:08,366 main.py:57] epoch 4171, training loss: 6686.72, average training loss: 7651.61, base loss: 16096.48
[INFO 2017-06-29 03:42:11,417 main.py:57] epoch 4172, training loss: 8593.17, average training loss: 7651.93, base loss: 16097.03
[INFO 2017-06-29 03:42:14,491 main.py:57] epoch 4173, training loss: 7045.14, average training loss: 7651.90, base loss: 16096.42
[INFO 2017-06-29 03:42:17,580 main.py:57] epoch 4174, training loss: 7425.12, average training loss: 7651.44, base loss: 16095.69
[INFO 2017-06-29 03:42:20,632 main.py:57] epoch 4175, training loss: 7348.85, average training loss: 7651.79, base loss: 16094.62
[INFO 2017-06-29 03:42:23,643 main.py:57] epoch 4176, training loss: 8195.47, average training loss: 7652.67, base loss: 16095.36
[INFO 2017-06-29 03:42:26,733 main.py:57] epoch 4177, training loss: 6945.85, average training loss: 7651.95, base loss: 16094.40
[INFO 2017-06-29 03:42:29,797 main.py:57] epoch 4178, training loss: 6751.83, average training loss: 7651.49, base loss: 16093.36
[INFO 2017-06-29 03:42:32,827 main.py:57] epoch 4179, training loss: 7015.32, average training loss: 7650.05, base loss: 16093.23
[INFO 2017-06-29 03:42:35,822 main.py:57] epoch 4180, training loss: 6536.73, average training loss: 7649.25, base loss: 16092.13
[INFO 2017-06-29 03:42:38,936 main.py:57] epoch 4181, training loss: 7035.52, average training loss: 7649.16, base loss: 16091.70
[INFO 2017-06-29 03:42:41,925 main.py:57] epoch 4182, training loss: 10138.31, average training loss: 7651.30, base loss: 16093.94
[INFO 2017-06-29 03:42:44,993 main.py:57] epoch 4183, training loss: 7228.74, average training loss: 7651.05, base loss: 16094.33
[INFO 2017-06-29 03:42:48,004 main.py:57] epoch 4184, training loss: 6812.69, average training loss: 7650.09, base loss: 16093.45
[INFO 2017-06-29 03:42:51,086 main.py:57] epoch 4185, training loss: 6699.92, average training loss: 7648.56, base loss: 16092.50
[INFO 2017-06-29 03:42:54,140 main.py:57] epoch 4186, training loss: 7531.39, average training loss: 7648.73, base loss: 16091.24
[INFO 2017-06-29 03:42:57,174 main.py:57] epoch 4187, training loss: 7460.09, average training loss: 7648.02, base loss: 16090.71
[INFO 2017-06-29 03:43:00,236 main.py:57] epoch 4188, training loss: 8113.52, average training loss: 7648.34, base loss: 16091.29
[INFO 2017-06-29 03:43:03,248 main.py:57] epoch 4189, training loss: 7422.84, average training loss: 7647.82, base loss: 16090.77
[INFO 2017-06-29 03:43:06,252 main.py:57] epoch 4190, training loss: 8369.85, average training loss: 7648.84, base loss: 16091.69
[INFO 2017-06-29 03:43:09,390 main.py:57] epoch 4191, training loss: 7583.58, average training loss: 7648.34, base loss: 16091.25
[INFO 2017-06-29 03:43:12,506 main.py:57] epoch 4192, training loss: 7326.91, average training loss: 7647.87, base loss: 16090.98
[INFO 2017-06-29 03:43:15,565 main.py:57] epoch 4193, training loss: 6400.32, average training loss: 7646.46, base loss: 16089.73
[INFO 2017-06-29 03:43:18,558 main.py:57] epoch 4194, training loss: 8116.71, average training loss: 7646.36, base loss: 16090.15
[INFO 2017-06-29 03:43:21,602 main.py:57] epoch 4195, training loss: 7480.84, average training loss: 7646.76, base loss: 16089.43
[INFO 2017-06-29 03:43:24,693 main.py:57] epoch 4196, training loss: 6980.53, average training loss: 7646.41, base loss: 16088.96
[INFO 2017-06-29 03:43:27,760 main.py:57] epoch 4197, training loss: 8077.01, average training loss: 7646.34, base loss: 16088.92
[INFO 2017-06-29 03:43:30,799 main.py:57] epoch 4198, training loss: 6982.94, average training loss: 7645.66, base loss: 16088.19
[INFO 2017-06-29 03:43:33,814 main.py:57] epoch 4199, training loss: 8021.19, average training loss: 7646.96, base loss: 16088.93
[INFO 2017-06-29 03:43:33,815 main.py:59] epoch 4199, testing
[INFO 2017-06-29 03:43:46,470 main.py:104] average testing loss: 8487.63, base loss: 17319.32
[INFO 2017-06-29 03:43:46,471 main.py:105] improve_loss: 8831.69, improve_percent: 0.51
[INFO 2017-06-29 03:43:46,472 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:43:49,545 main.py:57] epoch 4200, training loss: 7692.50, average training loss: 7646.63, base loss: 16089.44
[INFO 2017-06-29 03:43:52,607 main.py:57] epoch 4201, training loss: 7001.79, average training loss: 7646.80, base loss: 16089.32
[INFO 2017-06-29 03:43:55,615 main.py:57] epoch 4202, training loss: 7884.61, average training loss: 7647.05, base loss: 16090.52
[INFO 2017-06-29 03:43:58,639 main.py:57] epoch 4203, training loss: 7684.44, average training loss: 7645.63, base loss: 16091.62
[INFO 2017-06-29 03:44:01,775 main.py:57] epoch 4204, training loss: 7048.90, average training loss: 7644.51, base loss: 16091.08
[INFO 2017-06-29 03:44:04,826 main.py:57] epoch 4205, training loss: 8710.16, average training loss: 7646.25, base loss: 16091.14
[INFO 2017-06-29 03:44:07,906 main.py:57] epoch 4206, training loss: 8901.33, average training loss: 7647.47, base loss: 16092.91
[INFO 2017-06-29 03:44:10,959 main.py:57] epoch 4207, training loss: 6977.81, average training loss: 7645.84, base loss: 16093.04
[INFO 2017-06-29 03:44:14,073 main.py:57] epoch 4208, training loss: 8015.81, average training loss: 7645.61, base loss: 16093.03
[INFO 2017-06-29 03:44:17,121 main.py:57] epoch 4209, training loss: 8147.81, average training loss: 7646.40, base loss: 16093.06
[INFO 2017-06-29 03:44:20,161 main.py:57] epoch 4210, training loss: 7703.17, average training loss: 7646.94, base loss: 16092.99
[INFO 2017-06-29 03:44:23,225 main.py:57] epoch 4211, training loss: 7647.71, average training loss: 7647.19, base loss: 16092.74
[INFO 2017-06-29 03:44:26,237 main.py:57] epoch 4212, training loss: 6704.79, average training loss: 7646.55, base loss: 16092.21
[INFO 2017-06-29 03:44:29,275 main.py:57] epoch 4213, training loss: 7256.52, average training loss: 7645.59, base loss: 16091.90
[INFO 2017-06-29 03:44:32,331 main.py:57] epoch 4214, training loss: 8771.75, average training loss: 7646.77, base loss: 16092.56
[INFO 2017-06-29 03:44:35,385 main.py:57] epoch 4215, training loss: 8069.23, average training loss: 7646.37, base loss: 16092.92
[INFO 2017-06-29 03:44:38,449 main.py:57] epoch 4216, training loss: 7618.62, average training loss: 7646.46, base loss: 16093.34
[INFO 2017-06-29 03:44:41,523 main.py:57] epoch 4217, training loss: 7589.88, average training loss: 7647.30, base loss: 16093.63
[INFO 2017-06-29 03:44:44,585 main.py:57] epoch 4218, training loss: 7437.23, average training loss: 7646.12, base loss: 16093.49
[INFO 2017-06-29 03:44:47,678 main.py:57] epoch 4219, training loss: 7903.71, average training loss: 7644.93, base loss: 16094.28
[INFO 2017-06-29 03:44:50,728 main.py:57] epoch 4220, training loss: 7239.62, average training loss: 7644.81, base loss: 16094.46
[INFO 2017-06-29 03:44:53,797 main.py:57] epoch 4221, training loss: 8199.30, average training loss: 7645.55, base loss: 16094.89
[INFO 2017-06-29 03:44:56,823 main.py:57] epoch 4222, training loss: 7027.10, average training loss: 7644.98, base loss: 16094.91
[INFO 2017-06-29 03:44:59,879 main.py:57] epoch 4223, training loss: 7434.65, average training loss: 7645.05, base loss: 16094.36
[INFO 2017-06-29 03:45:02,967 main.py:57] epoch 4224, training loss: 7268.31, average training loss: 7644.58, base loss: 16093.82
[INFO 2017-06-29 03:45:06,024 main.py:57] epoch 4225, training loss: 8091.72, average training loss: 7645.14, base loss: 16094.75
[INFO 2017-06-29 03:45:09,051 main.py:57] epoch 4226, training loss: 6761.84, average training loss: 7643.85, base loss: 16094.10
[INFO 2017-06-29 03:45:12,068 main.py:57] epoch 4227, training loss: 7212.81, average training loss: 7642.50, base loss: 16094.00
[INFO 2017-06-29 03:45:15,175 main.py:57] epoch 4228, training loss: 7873.12, average training loss: 7641.98, base loss: 16094.58
[INFO 2017-06-29 03:45:18,232 main.py:57] epoch 4229, training loss: 7160.74, average training loss: 7641.60, base loss: 16094.49
[INFO 2017-06-29 03:45:21,306 main.py:57] epoch 4230, training loss: 7415.97, average training loss: 7641.02, base loss: 16094.08
[INFO 2017-06-29 03:45:24,358 main.py:57] epoch 4231, training loss: 7732.46, average training loss: 7641.02, base loss: 16094.43
[INFO 2017-06-29 03:45:27,409 main.py:57] epoch 4232, training loss: 7304.60, average training loss: 7640.63, base loss: 16094.76
[INFO 2017-06-29 03:45:30,520 main.py:57] epoch 4233, training loss: 7215.59, average training loss: 7640.64, base loss: 16094.68
[INFO 2017-06-29 03:45:33,572 main.py:57] epoch 4234, training loss: 7104.31, average training loss: 7640.88, base loss: 16094.48
[INFO 2017-06-29 03:45:36,594 main.py:57] epoch 4235, training loss: 7104.72, average training loss: 7640.57, base loss: 16093.70
[INFO 2017-06-29 03:45:39,644 main.py:57] epoch 4236, training loss: 7655.60, average training loss: 7640.69, base loss: 16093.37
[INFO 2017-06-29 03:45:42,708 main.py:57] epoch 4237, training loss: 7063.37, average training loss: 7639.45, base loss: 16092.84
[INFO 2017-06-29 03:45:45,781 main.py:57] epoch 4238, training loss: 8615.24, average training loss: 7639.87, base loss: 16093.97
[INFO 2017-06-29 03:45:48,894 main.py:57] epoch 4239, training loss: 6916.59, average training loss: 7638.42, base loss: 16093.52
[INFO 2017-06-29 03:45:52,003 main.py:57] epoch 4240, training loss: 9516.50, average training loss: 7640.86, base loss: 16094.76
[INFO 2017-06-29 03:45:55,019 main.py:57] epoch 4241, training loss: 7088.54, average training loss: 7640.57, base loss: 16094.83
[INFO 2017-06-29 03:45:58,034 main.py:57] epoch 4242, training loss: 6932.76, average training loss: 7639.23, base loss: 16094.44
[INFO 2017-06-29 03:46:01,105 main.py:57] epoch 4243, training loss: 7641.87, average training loss: 7639.57, base loss: 16094.04
[INFO 2017-06-29 03:46:04,101 main.py:57] epoch 4244, training loss: 7378.51, average training loss: 7638.62, base loss: 16093.93
[INFO 2017-06-29 03:46:07,207 main.py:57] epoch 4245, training loss: 7210.80, average training loss: 7638.32, base loss: 16094.11
[INFO 2017-06-29 03:46:10,258 main.py:57] epoch 4246, training loss: 7704.72, average training loss: 7638.78, base loss: 16094.62
[INFO 2017-06-29 03:46:13,343 main.py:57] epoch 4247, training loss: 7017.40, average training loss: 7638.72, base loss: 16093.98
[INFO 2017-06-29 03:46:16,354 main.py:57] epoch 4248, training loss: 6794.77, average training loss: 7637.38, base loss: 16093.34
[INFO 2017-06-29 03:46:19,393 main.py:57] epoch 4249, training loss: 7891.56, average training loss: 7637.03, base loss: 16094.03
[INFO 2017-06-29 03:46:22,438 main.py:57] epoch 4250, training loss: 7523.42, average training loss: 7636.48, base loss: 16095.22
[INFO 2017-06-29 03:46:25,488 main.py:57] epoch 4251, training loss: 7508.69, average training loss: 7636.86, base loss: 16095.70
[INFO 2017-06-29 03:46:28,600 main.py:57] epoch 4252, training loss: 7127.14, average training loss: 7636.38, base loss: 16095.60
[INFO 2017-06-29 03:46:31,681 main.py:57] epoch 4253, training loss: 8834.45, average training loss: 7637.71, base loss: 16096.29
[INFO 2017-06-29 03:46:34,783 main.py:57] epoch 4254, training loss: 6746.08, average training loss: 7636.55, base loss: 16095.34
[INFO 2017-06-29 03:46:37,889 main.py:57] epoch 4255, training loss: 7730.33, average training loss: 7636.86, base loss: 16094.88
[INFO 2017-06-29 03:46:40,966 main.py:57] epoch 4256, training loss: 7431.39, average training loss: 7636.92, base loss: 16095.21
[INFO 2017-06-29 03:46:44,009 main.py:57] epoch 4257, training loss: 7300.33, average training loss: 7636.59, base loss: 16095.59
[INFO 2017-06-29 03:46:47,144 main.py:57] epoch 4258, training loss: 8188.54, average training loss: 7636.79, base loss: 16096.33
[INFO 2017-06-29 03:46:50,236 main.py:57] epoch 4259, training loss: 7546.98, average training loss: 7636.75, base loss: 16096.40
[INFO 2017-06-29 03:46:53,320 main.py:57] epoch 4260, training loss: 7651.80, average training loss: 7637.17, base loss: 16097.11
[INFO 2017-06-29 03:46:56,322 main.py:57] epoch 4261, training loss: 8873.69, average training loss: 7637.74, base loss: 16097.35
[INFO 2017-06-29 03:46:59,369 main.py:57] epoch 4262, training loss: 8414.26, average training loss: 7639.12, base loss: 16097.86
[INFO 2017-06-29 03:47:02,424 main.py:57] epoch 4263, training loss: 7560.48, average training loss: 7639.84, base loss: 16097.90
[INFO 2017-06-29 03:47:05,480 main.py:57] epoch 4264, training loss: 7724.36, average training loss: 7640.29, base loss: 16097.95
[INFO 2017-06-29 03:47:08,557 main.py:57] epoch 4265, training loss: 8126.39, average training loss: 7641.05, base loss: 16097.59
[INFO 2017-06-29 03:47:11,583 main.py:57] epoch 4266, training loss: 8080.88, average training loss: 7641.32, base loss: 16097.98
[INFO 2017-06-29 03:47:14,587 main.py:57] epoch 4267, training loss: 6795.94, average training loss: 7639.80, base loss: 16097.59
[INFO 2017-06-29 03:47:17,679 main.py:57] epoch 4268, training loss: 8265.58, average training loss: 7640.67, base loss: 16098.43
[INFO 2017-06-29 03:47:20,792 main.py:57] epoch 4269, training loss: 7221.48, average training loss: 7639.19, base loss: 16098.33
[INFO 2017-06-29 03:47:23,858 main.py:57] epoch 4270, training loss: 7301.97, average training loss: 7639.29, base loss: 16098.09
[INFO 2017-06-29 03:47:26,911 main.py:57] epoch 4271, training loss: 8149.71, average training loss: 7639.18, base loss: 16097.99
[INFO 2017-06-29 03:47:29,952 main.py:57] epoch 4272, training loss: 7574.81, average training loss: 7639.71, base loss: 16097.98
[INFO 2017-06-29 03:47:33,024 main.py:57] epoch 4273, training loss: 8515.07, average training loss: 7639.87, base loss: 16098.16
[INFO 2017-06-29 03:47:36,045 main.py:57] epoch 4274, training loss: 7992.50, average training loss: 7640.58, base loss: 16098.80
[INFO 2017-06-29 03:47:39,108 main.py:57] epoch 4275, training loss: 7178.66, average training loss: 7638.88, base loss: 16098.29
[INFO 2017-06-29 03:47:42,168 main.py:57] epoch 4276, training loss: 7044.75, average training loss: 7635.94, base loss: 16098.42
[INFO 2017-06-29 03:47:45,282 main.py:57] epoch 4277, training loss: 7283.29, average training loss: 7635.62, base loss: 16098.33
[INFO 2017-06-29 03:47:48,357 main.py:57] epoch 4278, training loss: 6757.09, average training loss: 7634.36, base loss: 16097.41
[INFO 2017-06-29 03:47:51,446 main.py:57] epoch 4279, training loss: 8049.61, average training loss: 7634.83, base loss: 16097.61
[INFO 2017-06-29 03:47:54,523 main.py:57] epoch 4280, training loss: 7050.23, average training loss: 7633.86, base loss: 16097.39
[INFO 2017-06-29 03:47:57,574 main.py:57] epoch 4281, training loss: 7752.68, average training loss: 7634.87, base loss: 16098.20
[INFO 2017-06-29 03:48:00,619 main.py:57] epoch 4282, training loss: 7507.08, average training loss: 7635.30, base loss: 16097.93
[INFO 2017-06-29 03:48:03,611 main.py:57] epoch 4283, training loss: 7622.60, average training loss: 7635.83, base loss: 16097.71
[INFO 2017-06-29 03:48:06,693 main.py:57] epoch 4284, training loss: 7951.60, average training loss: 7635.64, base loss: 16098.00
[INFO 2017-06-29 03:48:09,741 main.py:57] epoch 4285, training loss: 6956.42, average training loss: 7635.18, base loss: 16097.55
[INFO 2017-06-29 03:48:12,731 main.py:57] epoch 4286, training loss: 7722.11, average training loss: 7635.95, base loss: 16097.92
[INFO 2017-06-29 03:48:15,749 main.py:57] epoch 4287, training loss: 7003.37, average training loss: 7634.13, base loss: 16097.69
[INFO 2017-06-29 03:48:18,861 main.py:57] epoch 4288, training loss: 6668.65, average training loss: 7632.14, base loss: 16097.84
[INFO 2017-06-29 03:48:21,998 main.py:57] epoch 4289, training loss: 7490.00, average training loss: 7632.36, base loss: 16098.43
[INFO 2017-06-29 03:48:25,036 main.py:57] epoch 4290, training loss: 8358.09, average training loss: 7633.20, base loss: 16099.80
[INFO 2017-06-29 03:48:28,066 main.py:57] epoch 4291, training loss: 7119.79, average training loss: 7632.88, base loss: 16099.48
[INFO 2017-06-29 03:48:31,115 main.py:57] epoch 4292, training loss: 7574.43, average training loss: 7632.90, base loss: 16100.02
[INFO 2017-06-29 03:48:34,142 main.py:57] epoch 4293, training loss: 8866.79, average training loss: 7634.42, base loss: 16101.23
[INFO 2017-06-29 03:48:37,199 main.py:57] epoch 4294, training loss: 7076.98, average training loss: 7634.14, base loss: 16101.22
[INFO 2017-06-29 03:48:40,241 main.py:57] epoch 4295, training loss: 7206.97, average training loss: 7634.36, base loss: 16100.62
[INFO 2017-06-29 03:48:43,323 main.py:57] epoch 4296, training loss: 8123.31, average training loss: 7635.23, base loss: 16100.82
[INFO 2017-06-29 03:48:46,416 main.py:57] epoch 4297, training loss: 6918.61, average training loss: 7633.92, base loss: 16099.56
[INFO 2017-06-29 03:48:49,468 main.py:57] epoch 4298, training loss: 8059.01, average training loss: 7634.91, base loss: 16099.46
[INFO 2017-06-29 03:48:52,465 main.py:57] epoch 4299, training loss: 6809.79, average training loss: 7634.10, base loss: 16098.62
[INFO 2017-06-29 03:48:52,465 main.py:59] epoch 4299, testing
[INFO 2017-06-29 03:49:05,146 main.py:104] average testing loss: 8179.06, base loss: 16888.69
[INFO 2017-06-29 03:49:05,147 main.py:105] improve_loss: 8709.62, improve_percent: 0.52
[INFO 2017-06-29 03:49:05,148 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:49:08,201 main.py:57] epoch 4300, training loss: 6801.43, average training loss: 7633.24, base loss: 16097.98
[INFO 2017-06-29 03:49:11,347 main.py:57] epoch 4301, training loss: 7919.62, average training loss: 7632.00, base loss: 16097.91
[INFO 2017-06-29 03:49:14,369 main.py:57] epoch 4302, training loss: 7161.39, average training loss: 7632.23, base loss: 16097.53
[INFO 2017-06-29 03:49:17,447 main.py:57] epoch 4303, training loss: 6575.79, average training loss: 7631.10, base loss: 16095.96
[INFO 2017-06-29 03:49:20,504 main.py:57] epoch 4304, training loss: 7622.85, average training loss: 7629.85, base loss: 16095.95
[INFO 2017-06-29 03:49:23,583 main.py:57] epoch 4305, training loss: 8790.48, average training loss: 7630.77, base loss: 16096.72
[INFO 2017-06-29 03:49:26,631 main.py:57] epoch 4306, training loss: 8440.77, average training loss: 7631.15, base loss: 16097.38
[INFO 2017-06-29 03:49:29,687 main.py:57] epoch 4307, training loss: 7367.01, average training loss: 7631.63, base loss: 16097.11
[INFO 2017-06-29 03:49:32,721 main.py:57] epoch 4308, training loss: 6915.97, average training loss: 7630.43, base loss: 16096.48
[INFO 2017-06-29 03:49:35,790 main.py:57] epoch 4309, training loss: 8051.55, average training loss: 7631.51, base loss: 16096.47
[INFO 2017-06-29 03:49:38,809 main.py:57] epoch 4310, training loss: 7320.31, average training loss: 7630.11, base loss: 16096.10
[INFO 2017-06-29 03:49:41,827 main.py:57] epoch 4311, training loss: 8065.16, average training loss: 7631.40, base loss: 16096.43
[INFO 2017-06-29 03:49:44,899 main.py:57] epoch 4312, training loss: 7149.84, average training loss: 7630.61, base loss: 16096.17
[INFO 2017-06-29 03:49:48,106 main.py:57] epoch 4313, training loss: 7609.57, average training loss: 7630.96, base loss: 16095.75
[INFO 2017-06-29 03:49:51,132 main.py:57] epoch 4314, training loss: 7810.02, average training loss: 7630.34, base loss: 16095.10
[INFO 2017-06-29 03:49:54,177 main.py:57] epoch 4315, training loss: 6508.31, average training loss: 7629.41, base loss: 16093.51
[INFO 2017-06-29 03:49:57,229 main.py:57] epoch 4316, training loss: 7249.86, average training loss: 7628.93, base loss: 16092.84
[INFO 2017-06-29 03:50:00,292 main.py:57] epoch 4317, training loss: 7504.11, average training loss: 7627.33, base loss: 16092.73
[INFO 2017-06-29 03:50:03,387 main.py:57] epoch 4318, training loss: 7748.37, average training loss: 7627.15, base loss: 16092.69
[INFO 2017-06-29 03:50:06,400 main.py:57] epoch 4319, training loss: 8194.77, average training loss: 7628.07, base loss: 16092.06
[INFO 2017-06-29 03:50:09,452 main.py:57] epoch 4320, training loss: 8164.00, average training loss: 7628.61, base loss: 16092.81
[INFO 2017-06-29 03:50:12,545 main.py:57] epoch 4321, training loss: 6934.86, average training loss: 7628.73, base loss: 16093.02
[INFO 2017-06-29 03:50:15,567 main.py:57] epoch 4322, training loss: 6928.21, average training loss: 7628.85, base loss: 16092.53
[INFO 2017-06-29 03:50:18,652 main.py:57] epoch 4323, training loss: 7701.46, average training loss: 7627.35, base loss: 16093.21
[INFO 2017-06-29 03:50:21,716 main.py:57] epoch 4324, training loss: 7968.20, average training loss: 7627.47, base loss: 16092.84
[INFO 2017-06-29 03:50:24,831 main.py:57] epoch 4325, training loss: 7415.29, average training loss: 7627.89, base loss: 16092.98
[INFO 2017-06-29 03:50:27,871 main.py:57] epoch 4326, training loss: 7592.81, average training loss: 7625.96, base loss: 16092.81
[INFO 2017-06-29 03:50:31,002 main.py:57] epoch 4327, training loss: 7718.62, average training loss: 7625.04, base loss: 16092.38
[INFO 2017-06-29 03:50:34,055 main.py:57] epoch 4328, training loss: 6672.91, average training loss: 7624.32, base loss: 16092.15
[INFO 2017-06-29 03:50:37,117 main.py:57] epoch 4329, training loss: 8391.69, average training loss: 7624.67, base loss: 16093.03
[INFO 2017-06-29 03:50:40,207 main.py:57] epoch 4330, training loss: 8772.40, average training loss: 7623.66, base loss: 16093.64
[INFO 2017-06-29 03:50:43,327 main.py:57] epoch 4331, training loss: 7580.68, average training loss: 7624.27, base loss: 16093.39
[INFO 2017-06-29 03:50:46,398 main.py:57] epoch 4332, training loss: 7295.79, average training loss: 7623.71, base loss: 16093.44
[INFO 2017-06-29 03:50:49,411 main.py:57] epoch 4333, training loss: 7439.09, average training loss: 7624.35, base loss: 16094.12
[INFO 2017-06-29 03:50:52,492 main.py:57] epoch 4334, training loss: 8491.24, average training loss: 7624.81, base loss: 16094.78
[INFO 2017-06-29 03:50:55,507 main.py:57] epoch 4335, training loss: 6942.12, average training loss: 7624.61, base loss: 16094.67
[INFO 2017-06-29 03:50:58,593 main.py:57] epoch 4336, training loss: 8736.00, average training loss: 7625.31, base loss: 16095.96
[INFO 2017-06-29 03:51:01,727 main.py:57] epoch 4337, training loss: 7678.61, average training loss: 7625.18, base loss: 16096.17
[INFO 2017-06-29 03:51:04,829 main.py:57] epoch 4338, training loss: 7803.11, average training loss: 7624.79, base loss: 16097.23
[INFO 2017-06-29 03:51:07,864 main.py:57] epoch 4339, training loss: 6659.89, average training loss: 7623.48, base loss: 16096.48
[INFO 2017-06-29 03:51:10,892 main.py:57] epoch 4340, training loss: 6607.00, average training loss: 7623.45, base loss: 16095.51
[INFO 2017-06-29 03:51:13,905 main.py:57] epoch 4341, training loss: 7353.08, average training loss: 7621.90, base loss: 16095.60
[INFO 2017-06-29 03:51:16,984 main.py:57] epoch 4342, training loss: 7954.57, average training loss: 7621.84, base loss: 16095.41
[INFO 2017-06-29 03:51:20,050 main.py:57] epoch 4343, training loss: 8190.00, average training loss: 7622.52, base loss: 16095.84
[INFO 2017-06-29 03:51:23,144 main.py:57] epoch 4344, training loss: 8184.59, average training loss: 7622.49, base loss: 16096.06
[INFO 2017-06-29 03:51:26,205 main.py:57] epoch 4345, training loss: 7982.08, average training loss: 7622.26, base loss: 16096.19
[INFO 2017-06-29 03:51:29,284 main.py:57] epoch 4346, training loss: 7065.13, average training loss: 7622.20, base loss: 16095.74
[INFO 2017-06-29 03:51:32,359 main.py:57] epoch 4347, training loss: 7774.91, average training loss: 7621.99, base loss: 16096.59
[INFO 2017-06-29 03:51:35,446 main.py:57] epoch 4348, training loss: 7630.87, average training loss: 7622.64, base loss: 16097.02
[INFO 2017-06-29 03:51:38,545 main.py:57] epoch 4349, training loss: 7089.84, average training loss: 7622.77, base loss: 16096.79
[INFO 2017-06-29 03:51:41,598 main.py:57] epoch 4350, training loss: 7752.42, average training loss: 7622.47, base loss: 16096.71
[INFO 2017-06-29 03:51:44,605 main.py:57] epoch 4351, training loss: 7619.42, average training loss: 7621.25, base loss: 16096.69
[INFO 2017-06-29 03:51:47,682 main.py:57] epoch 4352, training loss: 6910.15, average training loss: 7620.62, base loss: 16095.60
[INFO 2017-06-29 03:51:50,783 main.py:57] epoch 4353, training loss: 8187.00, average training loss: 7621.22, base loss: 16095.47
[INFO 2017-06-29 03:51:53,811 main.py:57] epoch 4354, training loss: 7477.84, average training loss: 7619.96, base loss: 16095.16
[INFO 2017-06-29 03:51:56,839 main.py:57] epoch 4355, training loss: 8790.88, average training loss: 7620.61, base loss: 16096.07
[INFO 2017-06-29 03:51:59,887 main.py:57] epoch 4356, training loss: 7925.84, average training loss: 7620.09, base loss: 16096.78
[INFO 2017-06-29 03:52:02,923 main.py:57] epoch 4357, training loss: 6877.23, average training loss: 7619.98, base loss: 16096.58
[INFO 2017-06-29 03:52:05,996 main.py:57] epoch 4358, training loss: 7771.75, average training loss: 7619.42, base loss: 16096.38
[INFO 2017-06-29 03:52:09,064 main.py:57] epoch 4359, training loss: 7183.88, average training loss: 7619.31, base loss: 16095.07
[INFO 2017-06-29 03:52:12,155 main.py:57] epoch 4360, training loss: 8783.41, average training loss: 7621.28, base loss: 16095.93
[INFO 2017-06-29 03:52:15,156 main.py:57] epoch 4361, training loss: 8164.66, average training loss: 7621.11, base loss: 16096.25
[INFO 2017-06-29 03:52:18,154 main.py:57] epoch 4362, training loss: 8877.22, average training loss: 7623.38, base loss: 16097.33
[INFO 2017-06-29 03:52:21,179 main.py:57] epoch 4363, training loss: 7324.48, average training loss: 7624.11, base loss: 16097.07
[INFO 2017-06-29 03:52:24,299 main.py:57] epoch 4364, training loss: 7952.53, average training loss: 7624.11, base loss: 16097.77
[INFO 2017-06-29 03:52:27,478 main.py:57] epoch 4365, training loss: 6936.78, average training loss: 7622.71, base loss: 16097.54
[INFO 2017-06-29 03:52:30,503 main.py:57] epoch 4366, training loss: 8453.41, average training loss: 7623.76, base loss: 16098.87
[INFO 2017-06-29 03:52:33,502 main.py:57] epoch 4367, training loss: 7479.83, average training loss: 7622.71, base loss: 16099.24
[INFO 2017-06-29 03:52:36,532 main.py:57] epoch 4368, training loss: 6877.59, average training loss: 7621.39, base loss: 16099.24
[INFO 2017-06-29 03:52:39,629 main.py:57] epoch 4369, training loss: 7661.38, average training loss: 7621.30, base loss: 16099.75
[INFO 2017-06-29 03:52:42,696 main.py:57] epoch 4370, training loss: 7674.35, average training loss: 7621.64, base loss: 16100.25
[INFO 2017-06-29 03:52:45,747 main.py:57] epoch 4371, training loss: 7495.88, average training loss: 7622.39, base loss: 16099.93
[INFO 2017-06-29 03:52:48,880 main.py:57] epoch 4372, training loss: 6693.32, average training loss: 7622.59, base loss: 16099.95
[INFO 2017-06-29 03:52:51,961 main.py:57] epoch 4373, training loss: 7412.31, average training loss: 7623.14, base loss: 16100.43
[INFO 2017-06-29 03:52:55,008 main.py:57] epoch 4374, training loss: 8582.90, average training loss: 7624.09, base loss: 16101.45
[INFO 2017-06-29 03:52:58,105 main.py:57] epoch 4375, training loss: 6896.18, average training loss: 7623.82, base loss: 16100.16
[INFO 2017-06-29 03:53:01,162 main.py:57] epoch 4376, training loss: 7859.62, average training loss: 7623.67, base loss: 16100.06
[INFO 2017-06-29 03:53:04,203 main.py:57] epoch 4377, training loss: 7439.20, average training loss: 7623.89, base loss: 16099.54
[INFO 2017-06-29 03:53:07,232 main.py:57] epoch 4378, training loss: 7513.46, average training loss: 7623.62, base loss: 16099.06
[INFO 2017-06-29 03:53:10,290 main.py:57] epoch 4379, training loss: 7306.75, average training loss: 7622.69, base loss: 16098.87
[INFO 2017-06-29 03:53:13,332 main.py:57] epoch 4380, training loss: 8011.15, average training loss: 7622.52, base loss: 16098.76
[INFO 2017-06-29 03:53:16,420 main.py:57] epoch 4381, training loss: 7507.96, average training loss: 7622.07, base loss: 16097.77
[INFO 2017-06-29 03:53:19,449 main.py:57] epoch 4382, training loss: 8052.85, average training loss: 7622.06, base loss: 16098.43
[INFO 2017-06-29 03:53:22,507 main.py:57] epoch 4383, training loss: 6771.09, average training loss: 7621.19, base loss: 16097.94
[INFO 2017-06-29 03:53:25,617 main.py:57] epoch 4384, training loss: 8594.97, average training loss: 7621.58, base loss: 16098.45
[INFO 2017-06-29 03:53:28,697 main.py:57] epoch 4385, training loss: 7277.74, average training loss: 7621.18, base loss: 16098.61
[INFO 2017-06-29 03:53:31,756 main.py:57] epoch 4386, training loss: 8417.83, average training loss: 7622.05, base loss: 16098.70
[INFO 2017-06-29 03:53:34,841 main.py:57] epoch 4387, training loss: 7282.30, average training loss: 7621.09, base loss: 16098.47
[INFO 2017-06-29 03:53:37,932 main.py:57] epoch 4388, training loss: 6886.47, average training loss: 7619.84, base loss: 16097.47
[INFO 2017-06-29 03:53:40,938 main.py:57] epoch 4389, training loss: 7146.66, average training loss: 7619.78, base loss: 16097.64
[INFO 2017-06-29 03:53:43,978 main.py:57] epoch 4390, training loss: 6776.86, average training loss: 7618.49, base loss: 16097.16
[INFO 2017-06-29 03:53:47,041 main.py:57] epoch 4391, training loss: 7237.62, average training loss: 7617.62, base loss: 16096.64
[INFO 2017-06-29 03:53:50,083 main.py:57] epoch 4392, training loss: 6514.65, average training loss: 7615.55, base loss: 16096.38
[INFO 2017-06-29 03:53:53,210 main.py:57] epoch 4393, training loss: 8613.23, average training loss: 7617.29, base loss: 16096.98
[INFO 2017-06-29 03:53:56,253 main.py:57] epoch 4394, training loss: 6427.26, average training loss: 7615.68, base loss: 16095.98
[INFO 2017-06-29 03:53:59,382 main.py:57] epoch 4395, training loss: 5864.87, average training loss: 7613.34, base loss: 16094.43
[INFO 2017-06-29 03:54:02,449 main.py:57] epoch 4396, training loss: 7170.74, average training loss: 7612.70, base loss: 16094.03
[INFO 2017-06-29 03:54:05,503 main.py:57] epoch 4397, training loss: 7762.21, average training loss: 7612.95, base loss: 16094.38
[INFO 2017-06-29 03:54:08,520 main.py:57] epoch 4398, training loss: 6838.68, average training loss: 7612.55, base loss: 16093.72
[INFO 2017-06-29 03:54:11,595 main.py:57] epoch 4399, training loss: 6962.31, average training loss: 7612.50, base loss: 16093.29
[INFO 2017-06-29 03:54:11,595 main.py:59] epoch 4399, testing
[INFO 2017-06-29 03:54:24,158 main.py:104] average testing loss: 8290.42, base loss: 16993.97
[INFO 2017-06-29 03:54:24,158 main.py:105] improve_loss: 8703.56, improve_percent: 0.51
[INFO 2017-06-29 03:54:24,159 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:54:27,330 main.py:57] epoch 4400, training loss: 7151.62, average training loss: 7612.71, base loss: 16092.38
[INFO 2017-06-29 03:54:30,417 main.py:57] epoch 4401, training loss: 7173.96, average training loss: 7613.08, base loss: 16091.99
[INFO 2017-06-29 03:54:33,459 main.py:57] epoch 4402, training loss: 7021.17, average training loss: 7613.00, base loss: 16090.91
[INFO 2017-06-29 03:54:36,531 main.py:57] epoch 4403, training loss: 8664.33, average training loss: 7614.11, base loss: 16091.10
[INFO 2017-06-29 03:54:39,551 main.py:57] epoch 4404, training loss: 8492.36, average training loss: 7615.28, base loss: 16090.91
[INFO 2017-06-29 03:54:42,618 main.py:57] epoch 4405, training loss: 7328.65, average training loss: 7614.09, base loss: 16090.58
[INFO 2017-06-29 03:54:45,634 main.py:57] epoch 4406, training loss: 7900.87, average training loss: 7614.63, base loss: 16091.55
[INFO 2017-06-29 03:54:48,732 main.py:57] epoch 4407, training loss: 8339.21, average training loss: 7614.99, base loss: 16092.86
[INFO 2017-06-29 03:54:51,749 main.py:57] epoch 4408, training loss: 6520.87, average training loss: 7613.78, base loss: 16092.48
[INFO 2017-06-29 03:54:54,842 main.py:57] epoch 4409, training loss: 7393.30, average training loss: 7610.81, base loss: 16092.96
[INFO 2017-06-29 03:54:57,858 main.py:57] epoch 4410, training loss: 6991.22, average training loss: 7608.93, base loss: 16092.76
[INFO 2017-06-29 03:55:00,850 main.py:57] epoch 4411, training loss: 8091.86, average training loss: 7608.66, base loss: 16093.95
[INFO 2017-06-29 03:55:03,821 main.py:57] epoch 4412, training loss: 7807.85, average training loss: 7609.41, base loss: 16093.22
[INFO 2017-06-29 03:55:06,821 main.py:57] epoch 4413, training loss: 6913.05, average training loss: 7608.03, base loss: 16091.94
[INFO 2017-06-29 03:55:09,891 main.py:57] epoch 4414, training loss: 7204.97, average training loss: 7606.70, base loss: 16091.93
[INFO 2017-06-29 03:55:13,014 main.py:57] epoch 4415, training loss: 8011.83, average training loss: 7606.96, base loss: 16091.87
[INFO 2017-06-29 03:55:16,057 main.py:57] epoch 4416, training loss: 8029.25, average training loss: 7607.35, base loss: 16092.08
[INFO 2017-06-29 03:55:19,184 main.py:57] epoch 4417, training loss: 6702.20, average training loss: 7607.04, base loss: 16091.46
[INFO 2017-06-29 03:55:22,196 main.py:57] epoch 4418, training loss: 7940.01, average training loss: 7607.42, base loss: 16092.43
[INFO 2017-06-29 03:55:25,233 main.py:57] epoch 4419, training loss: 7348.97, average training loss: 7606.99, base loss: 16092.34
[INFO 2017-06-29 03:55:28,274 main.py:57] epoch 4420, training loss: 8261.12, average training loss: 7606.83, base loss: 16092.73
[INFO 2017-06-29 03:55:31,320 main.py:57] epoch 4421, training loss: 7839.33, average training loss: 7607.76, base loss: 16092.71
[INFO 2017-06-29 03:55:34,362 main.py:57] epoch 4422, training loss: 7577.59, average training loss: 7607.09, base loss: 16092.28
[INFO 2017-06-29 03:55:37,414 main.py:57] epoch 4423, training loss: 7153.65, average training loss: 7606.15, base loss: 16091.42
[INFO 2017-06-29 03:55:40,546 main.py:57] epoch 4424, training loss: 7152.08, average training loss: 7606.55, base loss: 16091.17
[INFO 2017-06-29 03:55:43,677 main.py:57] epoch 4425, training loss: 7702.09, average training loss: 7606.32, base loss: 16090.35
[INFO 2017-06-29 03:55:46,699 main.py:57] epoch 4426, training loss: 7274.59, average training loss: 7606.48, base loss: 16090.42
[INFO 2017-06-29 03:55:49,748 main.py:57] epoch 4427, training loss: 8076.80, average training loss: 7606.95, base loss: 16090.82
[INFO 2017-06-29 03:55:52,800 main.py:57] epoch 4428, training loss: 7846.00, average training loss: 7606.78, base loss: 16090.63
[INFO 2017-06-29 03:55:55,825 main.py:57] epoch 4429, training loss: 7347.56, average training loss: 7606.34, base loss: 16090.42
[INFO 2017-06-29 03:55:58,927 main.py:57] epoch 4430, training loss: 6646.08, average training loss: 7604.31, base loss: 16089.61
[INFO 2017-06-29 03:56:02,007 main.py:57] epoch 4431, training loss: 7281.88, average training loss: 7603.70, base loss: 16089.14
[INFO 2017-06-29 03:56:05,052 main.py:57] epoch 4432, training loss: 7775.14, average training loss: 7603.73, base loss: 16090.20
[INFO 2017-06-29 03:56:08,074 main.py:57] epoch 4433, training loss: 8903.28, average training loss: 7605.35, base loss: 16090.86
[INFO 2017-06-29 03:56:11,194 main.py:57] epoch 4434, training loss: 7678.28, average training loss: 7606.19, base loss: 16090.94
[INFO 2017-06-29 03:56:14,270 main.py:57] epoch 4435, training loss: 7209.93, average training loss: 7606.30, base loss: 16090.77
[INFO 2017-06-29 03:56:17,298 main.py:57] epoch 4436, training loss: 7972.95, average training loss: 7606.32, base loss: 16091.16
[INFO 2017-06-29 03:56:20,487 main.py:57] epoch 4437, training loss: 7265.90, average training loss: 7605.58, base loss: 16091.05
[INFO 2017-06-29 03:56:23,517 main.py:57] epoch 4438, training loss: 6854.43, average training loss: 7603.56, base loss: 16091.43
[INFO 2017-06-29 03:56:26,710 main.py:57] epoch 4439, training loss: 7467.11, average training loss: 7603.23, base loss: 16091.73
[INFO 2017-06-29 03:56:29,694 main.py:57] epoch 4440, training loss: 8021.00, average training loss: 7603.54, base loss: 16091.86
[INFO 2017-06-29 03:56:32,644 main.py:57] epoch 4441, training loss: 7180.24, average training loss: 7604.37, base loss: 16091.24
[INFO 2017-06-29 03:56:35,639 main.py:57] epoch 4442, training loss: 8665.82, average training loss: 7606.27, base loss: 16091.55
[INFO 2017-06-29 03:56:38,717 main.py:57] epoch 4443, training loss: 6696.75, average training loss: 7606.14, base loss: 16090.89
[INFO 2017-06-29 03:56:41,742 main.py:57] epoch 4444, training loss: 8357.26, average training loss: 7607.32, base loss: 16091.77
[INFO 2017-06-29 03:56:44,830 main.py:57] epoch 4445, training loss: 7540.52, average training loss: 7606.75, base loss: 16092.04
[INFO 2017-06-29 03:56:47,951 main.py:57] epoch 4446, training loss: 7686.27, average training loss: 7605.50, base loss: 16092.15
[INFO 2017-06-29 03:56:51,052 main.py:57] epoch 4447, training loss: 7524.16, average training loss: 7605.09, base loss: 16092.38
[INFO 2017-06-29 03:56:54,074 main.py:57] epoch 4448, training loss: 8236.39, average training loss: 7606.27, base loss: 16093.17
[INFO 2017-06-29 03:56:57,148 main.py:57] epoch 4449, training loss: 8408.34, average training loss: 7606.94, base loss: 16094.28
[INFO 2017-06-29 03:57:00,198 main.py:57] epoch 4450, training loss: 6843.15, average training loss: 7606.29, base loss: 16093.53
[INFO 2017-06-29 03:57:03,266 main.py:57] epoch 4451, training loss: 8085.09, average training loss: 7606.98, base loss: 16094.39
[INFO 2017-06-29 03:57:06,287 main.py:57] epoch 4452, training loss: 6622.55, average training loss: 7606.01, base loss: 16092.73
[INFO 2017-06-29 03:57:09,349 main.py:57] epoch 4453, training loss: 7269.71, average training loss: 7604.88, base loss: 16091.62
[INFO 2017-06-29 03:57:12,340 main.py:57] epoch 4454, training loss: 7129.34, average training loss: 7603.61, base loss: 16090.67
[INFO 2017-06-29 03:57:15,383 main.py:57] epoch 4455, training loss: 7724.99, average training loss: 7604.09, base loss: 16090.75
[INFO 2017-06-29 03:57:18,386 main.py:57] epoch 4456, training loss: 8039.75, average training loss: 7605.24, base loss: 16090.80
[INFO 2017-06-29 03:57:21,430 main.py:57] epoch 4457, training loss: 7470.58, average training loss: 7603.85, base loss: 16091.06
[INFO 2017-06-29 03:57:24,542 main.py:57] epoch 4458, training loss: 6359.22, average training loss: 7602.46, base loss: 16089.97
[INFO 2017-06-29 03:57:27,512 main.py:57] epoch 4459, training loss: 7592.68, average training loss: 7602.69, base loss: 16090.08
[INFO 2017-06-29 03:57:30,536 main.py:57] epoch 4460, training loss: 7471.84, average training loss: 7602.52, base loss: 16089.88
[INFO 2017-06-29 03:57:33,610 main.py:57] epoch 4461, training loss: 7390.35, average training loss: 7602.80, base loss: 16089.97
[INFO 2017-06-29 03:57:36,670 main.py:57] epoch 4462, training loss: 7192.13, average training loss: 7602.23, base loss: 16090.13
[INFO 2017-06-29 03:57:39,624 main.py:57] epoch 4463, training loss: 6952.61, average training loss: 7602.30, base loss: 16090.61
[INFO 2017-06-29 03:57:42,742 main.py:57] epoch 4464, training loss: 8319.25, average training loss: 7603.27, base loss: 16091.57
[INFO 2017-06-29 03:57:45,852 main.py:57] epoch 4465, training loss: 7810.51, average training loss: 7603.26, base loss: 16092.45
[INFO 2017-06-29 03:57:48,945 main.py:57] epoch 4466, training loss: 7620.96, average training loss: 7603.51, base loss: 16092.31
[INFO 2017-06-29 03:57:52,023 main.py:57] epoch 4467, training loss: 8151.89, average training loss: 7604.93, base loss: 16092.02
[INFO 2017-06-29 03:57:55,015 main.py:57] epoch 4468, training loss: 7944.91, average training loss: 7605.06, base loss: 16092.14
[INFO 2017-06-29 03:57:58,083 main.py:57] epoch 4469, training loss: 7868.39, average training loss: 7606.30, base loss: 16092.22
[INFO 2017-06-29 03:58:01,132 main.py:57] epoch 4470, training loss: 7468.13, average training loss: 7606.18, base loss: 16091.80
[INFO 2017-06-29 03:58:04,230 main.py:57] epoch 4471, training loss: 7376.90, average training loss: 7603.92, base loss: 16091.49
[INFO 2017-06-29 03:58:07,277 main.py:57] epoch 4472, training loss: 6332.96, average training loss: 7601.36, base loss: 16091.21
[INFO 2017-06-29 03:58:10,323 main.py:57] epoch 4473, training loss: 8281.86, average training loss: 7602.74, base loss: 16092.64
[INFO 2017-06-29 03:58:13,375 main.py:57] epoch 4474, training loss: 7430.31, average training loss: 7602.24, base loss: 16092.44
[INFO 2017-06-29 03:58:16,436 main.py:57] epoch 4475, training loss: 7239.29, average training loss: 7602.24, base loss: 16091.99
[INFO 2017-06-29 03:58:19,534 main.py:57] epoch 4476, training loss: 6604.65, average training loss: 7601.35, base loss: 16090.92
[INFO 2017-06-29 03:58:22,630 main.py:57] epoch 4477, training loss: 7334.81, average training loss: 7600.83, base loss: 16091.05
[INFO 2017-06-29 03:58:25,638 main.py:57] epoch 4478, training loss: 6946.16, average training loss: 7598.95, base loss: 16090.66
[INFO 2017-06-29 03:58:28,748 main.py:57] epoch 4479, training loss: 7736.56, average training loss: 7599.56, base loss: 16090.75
[INFO 2017-06-29 03:58:31,825 main.py:57] epoch 4480, training loss: 7892.75, average training loss: 7599.55, base loss: 16091.15
[INFO 2017-06-29 03:58:35,015 main.py:57] epoch 4481, training loss: 7208.48, average training loss: 7598.27, base loss: 16091.07
[INFO 2017-06-29 03:58:38,050 main.py:57] epoch 4482, training loss: 7594.57, average training loss: 7598.35, base loss: 16091.54
[INFO 2017-06-29 03:58:41,123 main.py:57] epoch 4483, training loss: 6794.52, average training loss: 7596.80, base loss: 16091.18
[INFO 2017-06-29 03:58:44,111 main.py:57] epoch 4484, training loss: 8207.32, average training loss: 7596.57, base loss: 16090.83
[INFO 2017-06-29 03:58:47,119 main.py:57] epoch 4485, training loss: 6819.04, average training loss: 7595.73, base loss: 16089.78
[INFO 2017-06-29 03:58:50,207 main.py:57] epoch 4486, training loss: 7858.24, average training loss: 7596.38, base loss: 16090.24
[INFO 2017-06-29 03:58:53,245 main.py:57] epoch 4487, training loss: 7052.98, average training loss: 7595.89, base loss: 16090.09
[INFO 2017-06-29 03:58:56,274 main.py:57] epoch 4488, training loss: 6846.17, average training loss: 7595.65, base loss: 16089.65
[INFO 2017-06-29 03:58:59,332 main.py:57] epoch 4489, training loss: 7007.46, average training loss: 7595.27, base loss: 16089.02
[INFO 2017-06-29 03:59:02,382 main.py:57] epoch 4490, training loss: 6853.58, average training loss: 7594.21, base loss: 16088.40
[INFO 2017-06-29 03:59:05,372 main.py:57] epoch 4491, training loss: 6787.16, average training loss: 7593.62, base loss: 16087.81
[INFO 2017-06-29 03:59:08,489 main.py:57] epoch 4492, training loss: 7701.12, average training loss: 7594.30, base loss: 16087.88
[INFO 2017-06-29 03:59:11,530 main.py:57] epoch 4493, training loss: 8090.55, average training loss: 7594.76, base loss: 16088.87
[INFO 2017-06-29 03:59:14,578 main.py:57] epoch 4494, training loss: 7308.71, average training loss: 7594.23, base loss: 16088.22
[INFO 2017-06-29 03:59:17,700 main.py:57] epoch 4495, training loss: 7056.64, average training loss: 7593.17, base loss: 16086.99
[INFO 2017-06-29 03:59:20,764 main.py:57] epoch 4496, training loss: 8941.19, average training loss: 7594.30, base loss: 16087.90
[INFO 2017-06-29 03:59:23,855 main.py:57] epoch 4497, training loss: 7138.90, average training loss: 7593.13, base loss: 16087.26
[INFO 2017-06-29 03:59:26,919 main.py:57] epoch 4498, training loss: 6379.29, average training loss: 7591.26, base loss: 16086.59
[INFO 2017-06-29 03:59:29,984 main.py:57] epoch 4499, training loss: 7664.83, average training loss: 7591.54, base loss: 16086.47
[INFO 2017-06-29 03:59:29,985 main.py:59] epoch 4499, testing
[INFO 2017-06-29 03:59:42,619 main.py:104] average testing loss: 8107.04, base loss: 16506.18
[INFO 2017-06-29 03:59:42,619 main.py:105] improve_loss: 8399.14, improve_percent: 0.51
[INFO 2017-06-29 03:59:42,620 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 03:59:45,708 main.py:57] epoch 4500, training loss: 7621.65, average training loss: 7591.11, base loss: 16085.86
[INFO 2017-06-29 03:59:48,759 main.py:57] epoch 4501, training loss: 7970.50, average training loss: 7591.54, base loss: 16085.32
[INFO 2017-06-29 03:59:51,750 main.py:57] epoch 4502, training loss: 7666.13, average training loss: 7591.90, base loss: 16085.76
[INFO 2017-06-29 03:59:54,896 main.py:57] epoch 4503, training loss: 7073.55, average training loss: 7591.83, base loss: 16085.39
[INFO 2017-06-29 03:59:57,884 main.py:57] epoch 4504, training loss: 7208.84, average training loss: 7591.19, base loss: 16085.01
[INFO 2017-06-29 04:00:00,957 main.py:57] epoch 4505, training loss: 8075.88, average training loss: 7591.88, base loss: 16084.89
[INFO 2017-06-29 04:00:04,019 main.py:57] epoch 4506, training loss: 6735.41, average training loss: 7590.73, base loss: 16083.94
[INFO 2017-06-29 04:00:07,045 main.py:57] epoch 4507, training loss: 7603.64, average training loss: 7590.32, base loss: 16083.81
[INFO 2017-06-29 04:00:10,054 main.py:57] epoch 4508, training loss: 7623.51, average training loss: 7590.11, base loss: 16084.25
[INFO 2017-06-29 04:00:13,101 main.py:57] epoch 4509, training loss: 7324.75, average training loss: 7589.43, base loss: 16083.60
[INFO 2017-06-29 04:00:16,158 main.py:57] epoch 4510, training loss: 7307.43, average training loss: 7589.28, base loss: 16083.80
[INFO 2017-06-29 04:00:19,219 main.py:57] epoch 4511, training loss: 7182.05, average training loss: 7588.33, base loss: 16084.03
[INFO 2017-06-29 04:00:22,244 main.py:57] epoch 4512, training loss: 7300.44, average training loss: 7588.25, base loss: 16084.34
[INFO 2017-06-29 04:00:25,292 main.py:57] epoch 4513, training loss: 6788.45, average training loss: 7585.83, base loss: 16084.70
[INFO 2017-06-29 04:00:28,436 main.py:57] epoch 4514, training loss: 7915.19, average training loss: 7586.28, base loss: 16084.87
[INFO 2017-06-29 04:00:31,509 main.py:57] epoch 4515, training loss: 6633.54, average training loss: 7584.99, base loss: 16084.46
[INFO 2017-06-29 04:00:34,599 main.py:57] epoch 4516, training loss: 7101.65, average training loss: 7584.60, base loss: 16084.27
[INFO 2017-06-29 04:00:37,605 main.py:57] epoch 4517, training loss: 7788.44, average training loss: 7585.07, base loss: 16084.54
[INFO 2017-06-29 04:00:40,618 main.py:57] epoch 4518, training loss: 9499.96, average training loss: 7587.67, base loss: 16085.73
[INFO 2017-06-29 04:00:43,680 main.py:57] epoch 4519, training loss: 7866.63, average training loss: 7588.77, base loss: 16086.50
[INFO 2017-06-29 04:00:46,765 main.py:57] epoch 4520, training loss: 6853.74, average training loss: 7588.58, base loss: 16086.24
[INFO 2017-06-29 04:00:49,787 main.py:57] epoch 4521, training loss: 8082.85, average training loss: 7588.51, base loss: 16086.20
[INFO 2017-06-29 04:00:52,872 main.py:57] epoch 4522, training loss: 7759.92, average training loss: 7588.01, base loss: 16086.64
[INFO 2017-06-29 04:00:55,920 main.py:57] epoch 4523, training loss: 7704.03, average training loss: 7588.80, base loss: 16086.90
[INFO 2017-06-29 04:00:58,971 main.py:57] epoch 4524, training loss: 8613.01, average training loss: 7590.37, base loss: 16087.45
[INFO 2017-06-29 04:01:02,045 main.py:57] epoch 4525, training loss: 7127.66, average training loss: 7590.05, base loss: 16087.53
[INFO 2017-06-29 04:01:05,054 main.py:57] epoch 4526, training loss: 7616.61, average training loss: 7590.48, base loss: 16088.28
[INFO 2017-06-29 04:01:08,078 main.py:57] epoch 4527, training loss: 7058.54, average training loss: 7590.13, base loss: 16088.39
[INFO 2017-06-29 04:01:11,149 main.py:57] epoch 4528, training loss: 7047.71, average training loss: 7589.09, base loss: 16088.37
[INFO 2017-06-29 04:01:14,244 main.py:57] epoch 4529, training loss: 8904.33, average training loss: 7590.17, base loss: 16089.80
[INFO 2017-06-29 04:01:17,331 main.py:57] epoch 4530, training loss: 7792.15, average training loss: 7590.26, base loss: 16090.00
[INFO 2017-06-29 04:01:20,327 main.py:57] epoch 4531, training loss: 8024.50, average training loss: 7591.22, base loss: 16090.48
[INFO 2017-06-29 04:01:23,453 main.py:57] epoch 4532, training loss: 7274.92, average training loss: 7590.79, base loss: 16089.57
[INFO 2017-06-29 04:01:26,493 main.py:57] epoch 4533, training loss: 7034.76, average training loss: 7589.39, base loss: 16089.04
[INFO 2017-06-29 04:01:29,572 main.py:57] epoch 4534, training loss: 7317.20, average training loss: 7589.60, base loss: 16089.15
[INFO 2017-06-29 04:01:32,651 main.py:57] epoch 4535, training loss: 6441.33, average training loss: 7589.02, base loss: 16088.13
[INFO 2017-06-29 04:01:35,689 main.py:57] epoch 4536, training loss: 8081.77, average training loss: 7590.14, base loss: 16087.92
[INFO 2017-06-29 04:01:38,721 main.py:57] epoch 4537, training loss: 7483.71, average training loss: 7589.13, base loss: 16087.98
[INFO 2017-06-29 04:01:41,765 main.py:57] epoch 4538, training loss: 7148.19, average training loss: 7588.46, base loss: 16087.95
[INFO 2017-06-29 04:01:44,803 main.py:57] epoch 4539, training loss: 7405.33, average training loss: 7589.65, base loss: 16087.95
[INFO 2017-06-29 04:01:47,822 main.py:57] epoch 4540, training loss: 6246.88, average training loss: 7587.20, base loss: 16086.67
[INFO 2017-06-29 04:01:50,875 main.py:57] epoch 4541, training loss: 7281.72, average training loss: 7587.10, base loss: 16086.17
[INFO 2017-06-29 04:01:53,925 main.py:57] epoch 4542, training loss: 10047.39, average training loss: 7586.67, base loss: 16087.78
[INFO 2017-06-29 04:01:56,962 main.py:57] epoch 4543, training loss: 8694.53, average training loss: 7587.47, base loss: 16088.54
[INFO 2017-06-29 04:02:00,023 main.py:57] epoch 4544, training loss: 6821.64, average training loss: 7585.82, base loss: 16087.92
[INFO 2017-06-29 04:02:03,042 main.py:57] epoch 4545, training loss: 7134.90, average training loss: 7585.63, base loss: 16087.77
[INFO 2017-06-29 04:02:06,075 main.py:57] epoch 4546, training loss: 7255.22, average training loss: 7585.31, base loss: 16087.87
[INFO 2017-06-29 04:02:09,110 main.py:57] epoch 4547, training loss: 8288.89, average training loss: 7586.11, base loss: 16088.77
[INFO 2017-06-29 04:02:12,226 main.py:57] epoch 4548, training loss: 7644.12, average training loss: 7586.07, base loss: 16088.74
[INFO 2017-06-29 04:02:15,258 main.py:57] epoch 4549, training loss: 7871.48, average training loss: 7585.98, base loss: 16088.84
[INFO 2017-06-29 04:02:18,300 main.py:57] epoch 4550, training loss: 7852.01, average training loss: 7585.52, base loss: 16088.63
[INFO 2017-06-29 04:02:21,356 main.py:57] epoch 4551, training loss: 7915.01, average training loss: 7585.74, base loss: 16089.59
[INFO 2017-06-29 04:02:24,374 main.py:57] epoch 4552, training loss: 8957.21, average training loss: 7587.80, base loss: 16090.80
[INFO 2017-06-29 04:02:27,444 main.py:57] epoch 4553, training loss: 7288.67, average training loss: 7585.79, base loss: 16091.00
[INFO 2017-06-29 04:02:30,479 main.py:57] epoch 4554, training loss: 6775.81, average training loss: 7584.46, base loss: 16090.32
[INFO 2017-06-29 04:02:33,572 main.py:57] epoch 4555, training loss: 7103.58, average training loss: 7584.27, base loss: 16089.92
[INFO 2017-06-29 04:02:36,623 main.py:57] epoch 4556, training loss: 7414.15, average training loss: 7583.56, base loss: 16090.00
[INFO 2017-06-29 04:02:39,677 main.py:57] epoch 4557, training loss: 6422.82, average training loss: 7582.40, base loss: 16089.46
[INFO 2017-06-29 04:02:42,738 main.py:57] epoch 4558, training loss: 6221.11, average training loss: 7580.49, base loss: 16088.89
[INFO 2017-06-29 04:02:45,774 main.py:57] epoch 4559, training loss: 6966.15, average training loss: 7579.95, base loss: 16088.60
[INFO 2017-06-29 04:02:48,864 main.py:57] epoch 4560, training loss: 8427.89, average training loss: 7581.47, base loss: 16089.26
[INFO 2017-06-29 04:02:51,902 main.py:57] epoch 4561, training loss: 8362.99, average training loss: 7582.37, base loss: 16089.73
[INFO 2017-06-29 04:02:54,981 main.py:57] epoch 4562, training loss: 6652.19, average training loss: 7581.32, base loss: 16088.99
[INFO 2017-06-29 04:02:58,067 main.py:57] epoch 4563, training loss: 6543.46, average training loss: 7580.21, base loss: 16087.99
[INFO 2017-06-29 04:03:01,119 main.py:57] epoch 4564, training loss: 7155.23, average training loss: 7580.36, base loss: 16087.88
[INFO 2017-06-29 04:03:04,135 main.py:57] epoch 4565, training loss: 8820.57, average training loss: 7581.64, base loss: 16089.31
[INFO 2017-06-29 04:03:07,179 main.py:57] epoch 4566, training loss: 6870.29, average training loss: 7580.42, base loss: 16088.60
[INFO 2017-06-29 04:03:10,197 main.py:57] epoch 4567, training loss: 7685.95, average training loss: 7581.34, base loss: 16088.56
[INFO 2017-06-29 04:03:13,240 main.py:57] epoch 4568, training loss: 7140.01, average training loss: 7580.88, base loss: 16088.67
[INFO 2017-06-29 04:03:16,250 main.py:57] epoch 4569, training loss: 6980.32, average training loss: 7580.47, base loss: 16088.02
[INFO 2017-06-29 04:03:19,307 main.py:57] epoch 4570, training loss: 7423.15, average training loss: 7579.05, base loss: 16087.50
[INFO 2017-06-29 04:03:22,338 main.py:57] epoch 4571, training loss: 6811.74, average training loss: 7579.07, base loss: 16087.30
[INFO 2017-06-29 04:03:25,364 main.py:57] epoch 4572, training loss: 7707.83, average training loss: 7579.19, base loss: 16086.90
[INFO 2017-06-29 04:03:28,442 main.py:57] epoch 4573, training loss: 7845.39, average training loss: 7579.60, base loss: 16086.71
[INFO 2017-06-29 04:03:31,518 main.py:57] epoch 4574, training loss: 8089.98, average training loss: 7581.17, base loss: 16087.18
[INFO 2017-06-29 04:03:34,544 main.py:57] epoch 4575, training loss: 8284.38, average training loss: 7581.25, base loss: 16087.90
[INFO 2017-06-29 04:03:37,588 main.py:57] epoch 4576, training loss: 6999.25, average training loss: 7581.36, base loss: 16087.58
[INFO 2017-06-29 04:03:40,642 main.py:57] epoch 4577, training loss: 7452.21, average training loss: 7580.82, base loss: 16086.85
[INFO 2017-06-29 04:03:43,663 main.py:57] epoch 4578, training loss: 7219.88, average training loss: 7579.84, base loss: 16086.37
[INFO 2017-06-29 04:03:46,663 main.py:57] epoch 4579, training loss: 6861.78, average training loss: 7578.35, base loss: 16085.95
[INFO 2017-06-29 04:03:49,744 main.py:57] epoch 4580, training loss: 5933.34, average training loss: 7576.56, base loss: 16084.78
[INFO 2017-06-29 04:03:52,836 main.py:57] epoch 4581, training loss: 7741.27, average training loss: 7577.28, base loss: 16084.93
[INFO 2017-06-29 04:03:55,922 main.py:57] epoch 4582, training loss: 8470.27, average training loss: 7579.35, base loss: 16086.37
[INFO 2017-06-29 04:03:58,919 main.py:57] epoch 4583, training loss: 7229.20, average training loss: 7578.74, base loss: 16086.34
[INFO 2017-06-29 04:04:02,007 main.py:57] epoch 4584, training loss: 6336.88, average training loss: 7577.14, base loss: 16085.26
[INFO 2017-06-29 04:04:05,028 main.py:57] epoch 4585, training loss: 7043.41, average training loss: 7576.08, base loss: 16084.59
[INFO 2017-06-29 04:04:08,125 main.py:57] epoch 4586, training loss: 8020.03, average training loss: 7575.87, base loss: 16085.02
[INFO 2017-06-29 04:04:11,176 main.py:57] epoch 4587, training loss: 6890.63, average training loss: 7574.74, base loss: 16084.91
[INFO 2017-06-29 04:04:14,198 main.py:57] epoch 4588, training loss: 7828.22, average training loss: 7574.87, base loss: 16085.23
[INFO 2017-06-29 04:04:17,279 main.py:57] epoch 4589, training loss: 6905.25, average training loss: 7573.65, base loss: 16084.96
[INFO 2017-06-29 04:04:20,318 main.py:57] epoch 4590, training loss: 8405.89, average training loss: 7575.72, base loss: 16085.81
[INFO 2017-06-29 04:04:23,371 main.py:57] epoch 4591, training loss: 7456.91, average training loss: 7575.35, base loss: 16085.39
[INFO 2017-06-29 04:04:26,433 main.py:57] epoch 4592, training loss: 10425.46, average training loss: 7577.86, base loss: 16086.82
[INFO 2017-06-29 04:04:29,469 main.py:57] epoch 4593, training loss: 7166.06, average training loss: 7577.99, base loss: 16086.96
[INFO 2017-06-29 04:04:32,519 main.py:57] epoch 4594, training loss: 6930.70, average training loss: 7578.07, base loss: 16085.86
[INFO 2017-06-29 04:04:35,583 main.py:57] epoch 4595, training loss: 7482.78, average training loss: 7577.29, base loss: 16085.45
[INFO 2017-06-29 04:04:38,681 main.py:57] epoch 4596, training loss: 7742.44, average training loss: 7577.91, base loss: 16086.38
[INFO 2017-06-29 04:04:41,773 main.py:57] epoch 4597, training loss: 7341.93, average training loss: 7576.44, base loss: 16086.17
[INFO 2017-06-29 04:04:44,832 main.py:57] epoch 4598, training loss: 7241.87, average training loss: 7575.39, base loss: 16085.68
[INFO 2017-06-29 04:04:47,890 main.py:57] epoch 4599, training loss: 7518.16, average training loss: 7575.70, base loss: 16085.15
[INFO 2017-06-29 04:04:47,890 main.py:59] epoch 4599, testing
[INFO 2017-06-29 04:05:00,450 main.py:104] average testing loss: 8457.84, base loss: 17095.69
[INFO 2017-06-29 04:05:00,450 main.py:105] improve_loss: 8637.85, improve_percent: 0.51
[INFO 2017-06-29 04:05:00,452 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:05:03,483 main.py:57] epoch 4600, training loss: 9504.40, average training loss: 7577.65, base loss: 16086.90
[INFO 2017-06-29 04:05:06,494 main.py:57] epoch 4601, training loss: 9008.93, average training loss: 7579.08, base loss: 16088.71
[INFO 2017-06-29 04:05:09,509 main.py:57] epoch 4602, training loss: 6755.33, average training loss: 7579.09, base loss: 16087.75
[INFO 2017-06-29 04:05:12,584 main.py:57] epoch 4603, training loss: 7535.32, average training loss: 7578.22, base loss: 16087.76
[INFO 2017-06-29 04:05:15,619 main.py:57] epoch 4604, training loss: 7542.26, average training loss: 7577.97, base loss: 16088.50
[INFO 2017-06-29 04:05:18,728 main.py:57] epoch 4605, training loss: 7795.90, average training loss: 7578.13, base loss: 16089.12
[INFO 2017-06-29 04:05:21,761 main.py:57] epoch 4606, training loss: 8297.74, average training loss: 7578.86, base loss: 16090.01
[INFO 2017-06-29 04:05:24,856 main.py:57] epoch 4607, training loss: 7323.93, average training loss: 7579.16, base loss: 16090.40
[INFO 2017-06-29 04:05:27,856 main.py:57] epoch 4608, training loss: 7690.00, average training loss: 7579.68, base loss: 16090.58
[INFO 2017-06-29 04:05:30,907 main.py:57] epoch 4609, training loss: 8966.64, average training loss: 7579.70, base loss: 16091.63
[INFO 2017-06-29 04:05:33,977 main.py:57] epoch 4610, training loss: 7756.46, average training loss: 7580.61, base loss: 16092.43
[INFO 2017-06-29 04:05:36,951 main.py:57] epoch 4611, training loss: 6929.50, average training loss: 7578.67, base loss: 16092.42
[INFO 2017-06-29 04:05:40,073 main.py:57] epoch 4612, training loss: 7271.42, average training loss: 7578.19, base loss: 16092.32
[INFO 2017-06-29 04:05:43,216 main.py:57] epoch 4613, training loss: 6934.93, average training loss: 7576.59, base loss: 16092.20
[INFO 2017-06-29 04:05:46,251 main.py:57] epoch 4614, training loss: 7439.42, average training loss: 7576.27, base loss: 16091.97
[INFO 2017-06-29 04:05:49,266 main.py:57] epoch 4615, training loss: 8607.47, average training loss: 7575.91, base loss: 16092.61
[INFO 2017-06-29 04:05:52,385 main.py:57] epoch 4616, training loss: 7894.06, average training loss: 7576.68, base loss: 16092.21
[INFO 2017-06-29 04:05:55,501 main.py:57] epoch 4617, training loss: 7538.94, average training loss: 7576.58, base loss: 16091.95
[INFO 2017-06-29 04:05:58,529 main.py:57] epoch 4618, training loss: 7068.78, average training loss: 7576.63, base loss: 16091.98
[INFO 2017-06-29 04:06:01,602 main.py:57] epoch 4619, training loss: 7227.65, average training loss: 7576.32, base loss: 16091.81
[INFO 2017-06-29 04:06:04,635 main.py:57] epoch 4620, training loss: 6702.72, average training loss: 7575.87, base loss: 16090.96
[INFO 2017-06-29 04:06:07,723 main.py:57] epoch 4621, training loss: 8077.37, average training loss: 7576.60, base loss: 16091.54
[INFO 2017-06-29 04:06:10,742 main.py:57] epoch 4622, training loss: 7328.83, average training loss: 7576.22, base loss: 16091.45
[INFO 2017-06-29 04:06:13,919 main.py:57] epoch 4623, training loss: 7726.51, average training loss: 7577.23, base loss: 16091.81
[INFO 2017-06-29 04:06:16,932 main.py:57] epoch 4624, training loss: 7709.83, average training loss: 7576.26, base loss: 16092.78
[INFO 2017-06-29 04:06:19,962 main.py:57] epoch 4625, training loss: 7704.80, average training loss: 7576.05, base loss: 16094.01
[INFO 2017-06-29 04:06:22,998 main.py:57] epoch 4626, training loss: 6733.79, average training loss: 7575.13, base loss: 16093.40
[INFO 2017-06-29 04:06:26,054 main.py:57] epoch 4627, training loss: 6510.55, average training loss: 7573.93, base loss: 16092.74
[INFO 2017-06-29 04:06:29,094 main.py:57] epoch 4628, training loss: 7664.44, average training loss: 7573.42, base loss: 16093.84
[INFO 2017-06-29 04:06:32,169 main.py:57] epoch 4629, training loss: 6055.34, average training loss: 7571.33, base loss: 16093.43
[INFO 2017-06-29 04:06:35,211 main.py:57] epoch 4630, training loss: 7729.34, average training loss: 7571.33, base loss: 16093.24
[INFO 2017-06-29 04:06:38,274 main.py:57] epoch 4631, training loss: 9110.04, average training loss: 7573.48, base loss: 16094.37
[INFO 2017-06-29 04:06:41,370 main.py:57] epoch 4632, training loss: 7389.29, average training loss: 7573.71, base loss: 16093.74
[INFO 2017-06-29 04:06:44,468 main.py:57] epoch 4633, training loss: 7149.55, average training loss: 7573.18, base loss: 16093.17
[INFO 2017-06-29 04:06:47,603 main.py:57] epoch 4634, training loss: 7077.15, average training loss: 7572.84, base loss: 16093.36
[INFO 2017-06-29 04:06:50,596 main.py:57] epoch 4635, training loss: 7087.10, average training loss: 7572.89, base loss: 16093.33
[INFO 2017-06-29 04:06:53,719 main.py:57] epoch 4636, training loss: 8558.98, average training loss: 7573.74, base loss: 16094.02
[INFO 2017-06-29 04:06:56,750 main.py:57] epoch 4637, training loss: 7961.38, average training loss: 7573.87, base loss: 16094.04
[INFO 2017-06-29 04:06:59,835 main.py:57] epoch 4638, training loss: 7581.45, average training loss: 7573.78, base loss: 16093.79
[INFO 2017-06-29 04:07:02,875 main.py:57] epoch 4639, training loss: 7146.03, average training loss: 7573.29, base loss: 16093.68
[INFO 2017-06-29 04:07:05,917 main.py:57] epoch 4640, training loss: 7217.66, average training loss: 7572.61, base loss: 16093.96
[INFO 2017-06-29 04:07:08,900 main.py:57] epoch 4641, training loss: 7351.20, average training loss: 7571.89, base loss: 16094.05
[INFO 2017-06-29 04:07:12,047 main.py:57] epoch 4642, training loss: 7084.37, average training loss: 7570.65, base loss: 16093.50
[INFO 2017-06-29 04:07:15,074 main.py:57] epoch 4643, training loss: 7486.92, average training loss: 7570.78, base loss: 16093.29
[INFO 2017-06-29 04:07:18,127 main.py:57] epoch 4644, training loss: 7550.69, average training loss: 7571.15, base loss: 16093.51
[INFO 2017-06-29 04:07:21,174 main.py:57] epoch 4645, training loss: 8356.99, average training loss: 7572.64, base loss: 16093.70
[INFO 2017-06-29 04:07:24,260 main.py:57] epoch 4646, training loss: 7510.08, average training loss: 7572.41, base loss: 16093.24
[INFO 2017-06-29 04:07:27,332 main.py:57] epoch 4647, training loss: 7037.78, average training loss: 7571.46, base loss: 16093.01
[INFO 2017-06-29 04:07:30,361 main.py:57] epoch 4648, training loss: 7261.93, average training loss: 7570.83, base loss: 16092.40
[INFO 2017-06-29 04:07:33,442 main.py:57] epoch 4649, training loss: 7586.58, average training loss: 7571.24, base loss: 16091.96
[INFO 2017-06-29 04:07:36,444 main.py:57] epoch 4650, training loss: 9220.74, average training loss: 7573.80, base loss: 16092.70
[INFO 2017-06-29 04:07:39,583 main.py:57] epoch 4651, training loss: 6692.72, average training loss: 7573.07, base loss: 16092.53
[INFO 2017-06-29 04:07:42,632 main.py:57] epoch 4652, training loss: 6657.05, average training loss: 7572.40, base loss: 16091.63
[INFO 2017-06-29 04:07:45,740 main.py:57] epoch 4653, training loss: 7580.64, average training loss: 7573.15, base loss: 16091.96
[INFO 2017-06-29 04:07:48,801 main.py:57] epoch 4654, training loss: 6873.28, average training loss: 7572.43, base loss: 16091.67
[INFO 2017-06-29 04:07:51,948 main.py:57] epoch 4655, training loss: 7580.14, average training loss: 7572.63, base loss: 16092.15
[INFO 2017-06-29 04:07:55,014 main.py:57] epoch 4656, training loss: 7072.35, average training loss: 7572.32, base loss: 16092.24
[INFO 2017-06-29 04:07:58,040 main.py:57] epoch 4657, training loss: 7362.21, average training loss: 7572.64, base loss: 16092.05
[INFO 2017-06-29 04:08:01,083 main.py:57] epoch 4658, training loss: 7452.49, average training loss: 7573.13, base loss: 16091.82
[INFO 2017-06-29 04:08:04,140 main.py:57] epoch 4659, training loss: 7953.95, average training loss: 7572.88, base loss: 16091.78
[INFO 2017-06-29 04:08:07,161 main.py:57] epoch 4660, training loss: 6829.81, average training loss: 7571.71, base loss: 16090.86
[INFO 2017-06-29 04:08:10,345 main.py:57] epoch 4661, training loss: 7978.44, average training loss: 7570.85, base loss: 16091.02
[INFO 2017-06-29 04:08:13,421 main.py:57] epoch 4662, training loss: 7424.57, average training loss: 7571.73, base loss: 16090.99
[INFO 2017-06-29 04:08:16,439 main.py:57] epoch 4663, training loss: 7794.69, average training loss: 7571.51, base loss: 16091.38
[INFO 2017-06-29 04:08:19,445 main.py:57] epoch 4664, training loss: 6779.82, average training loss: 7570.52, base loss: 16091.43
[INFO 2017-06-29 04:08:22,507 main.py:57] epoch 4665, training loss: 8094.94, average training loss: 7570.79, base loss: 16092.07
[INFO 2017-06-29 04:08:25,551 main.py:57] epoch 4666, training loss: 7096.46, average training loss: 7570.74, base loss: 16092.23
[INFO 2017-06-29 04:08:28,631 main.py:57] epoch 4667, training loss: 8292.58, average training loss: 7572.29, base loss: 16093.12
[INFO 2017-06-29 04:08:31,716 main.py:57] epoch 4668, training loss: 7262.96, average training loss: 7571.17, base loss: 16092.95
[INFO 2017-06-29 04:08:34,732 main.py:57] epoch 4669, training loss: 7066.39, average training loss: 7570.40, base loss: 16092.39
[INFO 2017-06-29 04:08:37,880 main.py:57] epoch 4670, training loss: 7653.62, average training loss: 7570.70, base loss: 16092.30
[INFO 2017-06-29 04:08:40,916 main.py:57] epoch 4671, training loss: 7318.28, average training loss: 7570.96, base loss: 16091.78
[INFO 2017-06-29 04:08:43,917 main.py:57] epoch 4672, training loss: 9319.73, average training loss: 7573.37, base loss: 16093.27
[INFO 2017-06-29 04:08:47,009 main.py:57] epoch 4673, training loss: 7399.92, average training loss: 7573.44, base loss: 16092.97
[INFO 2017-06-29 04:08:50,088 main.py:57] epoch 4674, training loss: 7789.05, average training loss: 7574.17, base loss: 16093.96
[INFO 2017-06-29 04:08:53,126 main.py:57] epoch 4675, training loss: 8410.93, average training loss: 7575.04, base loss: 16095.08
[INFO 2017-06-29 04:08:56,155 main.py:57] epoch 4676, training loss: 6771.35, average training loss: 7574.00, base loss: 16094.68
[INFO 2017-06-29 04:08:59,231 main.py:57] epoch 4677, training loss: 9482.71, average training loss: 7576.02, base loss: 16095.79
[INFO 2017-06-29 04:09:02,272 main.py:57] epoch 4678, training loss: 9010.62, average training loss: 7577.90, base loss: 16097.33
[INFO 2017-06-29 04:09:05,290 main.py:57] epoch 4679, training loss: 8208.90, average training loss: 7578.20, base loss: 16098.43
[INFO 2017-06-29 04:09:08,303 main.py:57] epoch 4680, training loss: 8171.90, average training loss: 7578.68, base loss: 16099.15
[INFO 2017-06-29 04:09:11,400 main.py:57] epoch 4681, training loss: 6976.04, average training loss: 7578.24, base loss: 16099.28
[INFO 2017-06-29 04:09:14,475 main.py:57] epoch 4682, training loss: 7887.51, average training loss: 7578.99, base loss: 16098.87
[INFO 2017-06-29 04:09:17,570 main.py:57] epoch 4683, training loss: 9366.07, average training loss: 7580.14, base loss: 16099.76
[INFO 2017-06-29 04:09:20,660 main.py:57] epoch 4684, training loss: 6521.52, average training loss: 7579.03, base loss: 16098.71
[INFO 2017-06-29 04:09:23,824 main.py:57] epoch 4685, training loss: 6288.94, average training loss: 7578.31, base loss: 16097.76
[INFO 2017-06-29 04:09:26,839 main.py:57] epoch 4686, training loss: 7864.48, average training loss: 7578.01, base loss: 16097.20
[INFO 2017-06-29 04:09:29,906 main.py:57] epoch 4687, training loss: 8609.38, average training loss: 7579.52, base loss: 16097.65
[INFO 2017-06-29 04:09:32,948 main.py:57] epoch 4688, training loss: 7029.80, average training loss: 7578.72, base loss: 16096.68
[INFO 2017-06-29 04:09:35,970 main.py:57] epoch 4689, training loss: 8488.38, average training loss: 7579.26, base loss: 16096.56
[INFO 2017-06-29 04:09:38,999 main.py:57] epoch 4690, training loss: 6837.44, average training loss: 7579.06, base loss: 16095.50
[INFO 2017-06-29 04:09:42,091 main.py:57] epoch 4691, training loss: 7589.44, average training loss: 7578.80, base loss: 16095.73
[INFO 2017-06-29 04:09:45,164 main.py:57] epoch 4692, training loss: 7278.78, average training loss: 7577.53, base loss: 16095.23
[INFO 2017-06-29 04:09:48,173 main.py:57] epoch 4693, training loss: 8006.46, average training loss: 7577.30, base loss: 16095.35
[INFO 2017-06-29 04:09:51,345 main.py:57] epoch 4694, training loss: 7260.91, average training loss: 7577.05, base loss: 16094.54
[INFO 2017-06-29 04:09:54,394 main.py:57] epoch 4695, training loss: 7862.52, average training loss: 7577.28, base loss: 16094.44
[INFO 2017-06-29 04:09:57,438 main.py:57] epoch 4696, training loss: 8082.63, average training loss: 7578.33, base loss: 16094.67
[INFO 2017-06-29 04:10:00,563 main.py:57] epoch 4697, training loss: 7731.48, average training loss: 7578.95, base loss: 16095.25
[INFO 2017-06-29 04:10:03,647 main.py:57] epoch 4698, training loss: 7386.69, average training loss: 7578.73, base loss: 16095.12
[INFO 2017-06-29 04:10:06,730 main.py:57] epoch 4699, training loss: 7438.15, average training loss: 7579.10, base loss: 16095.66
[INFO 2017-06-29 04:10:06,730 main.py:59] epoch 4699, testing
[INFO 2017-06-29 04:10:19,528 main.py:104] average testing loss: 8249.50, base loss: 17119.50
[INFO 2017-06-29 04:10:19,529 main.py:105] improve_loss: 8870.01, improve_percent: 0.52
[INFO 2017-06-29 04:10:19,530 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:10:22,624 main.py:57] epoch 4700, training loss: 8207.66, average training loss: 7579.38, base loss: 16096.23
[INFO 2017-06-29 04:10:25,643 main.py:57] epoch 4701, training loss: 6491.08, average training loss: 7577.85, base loss: 16095.39
[INFO 2017-06-29 04:10:28,696 main.py:57] epoch 4702, training loss: 8439.46, average training loss: 7578.35, base loss: 16096.29
[INFO 2017-06-29 04:10:31,706 main.py:57] epoch 4703, training loss: 7092.48, average training loss: 7578.39, base loss: 16096.22
[INFO 2017-06-29 04:10:34,758 main.py:57] epoch 4704, training loss: 7291.51, average training loss: 7578.46, base loss: 16096.10
[INFO 2017-06-29 04:10:37,869 main.py:57] epoch 4705, training loss: 8035.85, average training loss: 7578.96, base loss: 16096.59
[INFO 2017-06-29 04:10:40,990 main.py:57] epoch 4706, training loss: 7611.98, average training loss: 7579.04, base loss: 16096.15
[INFO 2017-06-29 04:10:44,108 main.py:57] epoch 4707, training loss: 7274.57, average training loss: 7578.63, base loss: 16095.85
[INFO 2017-06-29 04:10:47,103 main.py:57] epoch 4708, training loss: 6945.71, average training loss: 7578.36, base loss: 16094.93
[INFO 2017-06-29 04:10:50,199 main.py:57] epoch 4709, training loss: 8551.82, average training loss: 7579.59, base loss: 16095.47
[INFO 2017-06-29 04:10:53,240 main.py:57] epoch 4710, training loss: 8566.77, average training loss: 7580.74, base loss: 16096.49
[INFO 2017-06-29 04:10:56,293 main.py:57] epoch 4711, training loss: 8022.42, average training loss: 7580.10, base loss: 16097.03
[INFO 2017-06-29 04:10:59,292 main.py:57] epoch 4712, training loss: 8461.96, average training loss: 7580.45, base loss: 16097.37
[INFO 2017-06-29 04:11:02,376 main.py:57] epoch 4713, training loss: 7909.51, average training loss: 7580.48, base loss: 16097.56
[INFO 2017-06-29 04:11:05,457 main.py:57] epoch 4714, training loss: 6827.58, average training loss: 7579.74, base loss: 16097.09
[INFO 2017-06-29 04:11:08,509 main.py:57] epoch 4715, training loss: 7491.67, average training loss: 7579.88, base loss: 16097.14
[INFO 2017-06-29 04:11:11,626 main.py:57] epoch 4716, training loss: 6627.73, average training loss: 7579.09, base loss: 16096.65
[INFO 2017-06-29 04:11:14,664 main.py:57] epoch 4717, training loss: 7559.23, average training loss: 7579.07, base loss: 16096.63
[INFO 2017-06-29 04:11:17,680 main.py:57] epoch 4718, training loss: 8527.01, average training loss: 7580.31, base loss: 16098.09
[INFO 2017-06-29 04:11:20,676 main.py:57] epoch 4719, training loss: 6798.86, average training loss: 7578.99, base loss: 16097.51
[INFO 2017-06-29 04:11:23,655 main.py:57] epoch 4720, training loss: 8108.94, average training loss: 7578.90, base loss: 16097.62
[INFO 2017-06-29 04:11:26,711 main.py:57] epoch 4721, training loss: 7510.89, average training loss: 7578.20, base loss: 16097.55
[INFO 2017-06-29 04:11:29,751 main.py:57] epoch 4722, training loss: 6706.14, average training loss: 7577.83, base loss: 16096.54
[INFO 2017-06-29 04:11:32,827 main.py:57] epoch 4723, training loss: 8071.82, average training loss: 7577.64, base loss: 16097.36
[INFO 2017-06-29 04:11:36,005 main.py:57] epoch 4724, training loss: 7026.75, average training loss: 7577.17, base loss: 16096.42
[INFO 2017-06-29 04:11:39,084 main.py:57] epoch 4725, training loss: 7742.43, average training loss: 7576.62, base loss: 16096.36
[INFO 2017-06-29 04:11:42,165 main.py:57] epoch 4726, training loss: 7980.24, average training loss: 7574.75, base loss: 16097.22
[INFO 2017-06-29 04:11:45,179 main.py:57] epoch 4727, training loss: 7048.23, average training loss: 7574.53, base loss: 16097.07
[INFO 2017-06-29 04:11:48,242 main.py:57] epoch 4728, training loss: 8442.25, average training loss: 7575.33, base loss: 16098.13
[INFO 2017-06-29 04:11:51,344 main.py:57] epoch 4729, training loss: 5985.14, average training loss: 7573.49, base loss: 16097.26
[INFO 2017-06-29 04:11:54,405 main.py:57] epoch 4730, training loss: 7125.61, average training loss: 7572.66, base loss: 16096.87
[INFO 2017-06-29 04:11:57,438 main.py:57] epoch 4731, training loss: 7580.53, average training loss: 7573.62, base loss: 16096.44
[INFO 2017-06-29 04:12:00,607 main.py:57] epoch 4732, training loss: 7889.34, average training loss: 7573.72, base loss: 16096.97
[INFO 2017-06-29 04:12:03,634 main.py:57] epoch 4733, training loss: 7990.80, average training loss: 7573.60, base loss: 16097.42
[INFO 2017-06-29 04:12:06,704 main.py:57] epoch 4734, training loss: 7772.91, average training loss: 7574.51, base loss: 16098.05
[INFO 2017-06-29 04:12:09,709 main.py:57] epoch 4735, training loss: 7880.00, average training loss: 7575.44, base loss: 16099.49
[INFO 2017-06-29 04:12:12,725 main.py:57] epoch 4736, training loss: 8263.53, average training loss: 7575.15, base loss: 16099.44
[INFO 2017-06-29 04:12:15,764 main.py:57] epoch 4737, training loss: 6824.61, average training loss: 7574.68, base loss: 16098.70
[INFO 2017-06-29 04:12:18,780 main.py:57] epoch 4738, training loss: 7936.94, average training loss: 7574.72, base loss: 16098.81
[INFO 2017-06-29 04:12:21,829 main.py:57] epoch 4739, training loss: 7699.07, average training loss: 7575.33, base loss: 16098.86
[INFO 2017-06-29 04:12:24,959 main.py:57] epoch 4740, training loss: 6174.98, average training loss: 7574.47, base loss: 16098.01
[INFO 2017-06-29 04:12:28,008 main.py:57] epoch 4741, training loss: 8306.32, average training loss: 7574.85, base loss: 16098.37
[INFO 2017-06-29 04:12:31,140 main.py:57] epoch 4742, training loss: 7028.26, average training loss: 7574.05, base loss: 16097.09
[INFO 2017-06-29 04:12:34,206 main.py:57] epoch 4743, training loss: 7468.48, average training loss: 7574.13, base loss: 16096.16
[INFO 2017-06-29 04:12:37,254 main.py:57] epoch 4744, training loss: 9782.07, average training loss: 7574.39, base loss: 16097.03
[INFO 2017-06-29 04:12:40,321 main.py:57] epoch 4745, training loss: 7835.64, average training loss: 7574.77, base loss: 16097.48
[INFO 2017-06-29 04:12:43,399 main.py:57] epoch 4746, training loss: 8247.07, average training loss: 7576.26, base loss: 16097.00
[INFO 2017-06-29 04:12:46,442 main.py:57] epoch 4747, training loss: 7436.15, average training loss: 7575.37, base loss: 16097.21
[INFO 2017-06-29 04:12:49,484 main.py:57] epoch 4748, training loss: 7280.94, average training loss: 7575.30, base loss: 16096.60
[INFO 2017-06-29 04:12:52,515 main.py:57] epoch 4749, training loss: 7560.46, average training loss: 7574.52, base loss: 16096.64
[INFO 2017-06-29 04:12:55,593 main.py:57] epoch 4750, training loss: 7075.73, average training loss: 7574.00, base loss: 16096.26
[INFO 2017-06-29 04:12:58,637 main.py:57] epoch 4751, training loss: 8774.41, average training loss: 7575.75, base loss: 16097.59
[INFO 2017-06-29 04:13:01,695 main.py:57] epoch 4752, training loss: 7363.95, average training loss: 7574.31, base loss: 16097.72
[INFO 2017-06-29 04:13:04,726 main.py:57] epoch 4753, training loss: 7623.09, average training loss: 7574.73, base loss: 16097.54
[INFO 2017-06-29 04:13:07,856 main.py:57] epoch 4754, training loss: 6805.87, average training loss: 7573.53, base loss: 16097.31
[INFO 2017-06-29 04:13:10,984 main.py:57] epoch 4755, training loss: 7035.23, average training loss: 7572.40, base loss: 16097.23
[INFO 2017-06-29 04:13:13,967 main.py:57] epoch 4756, training loss: 7060.33, average training loss: 7572.63, base loss: 16097.05
[INFO 2017-06-29 04:13:16,930 main.py:57] epoch 4757, training loss: 7332.01, average training loss: 7571.81, base loss: 16096.57
[INFO 2017-06-29 04:13:19,945 main.py:57] epoch 4758, training loss: 6453.54, average training loss: 7569.98, base loss: 16095.44
[INFO 2017-06-29 04:13:22,985 main.py:57] epoch 4759, training loss: 6694.17, average training loss: 7569.20, base loss: 16094.69
[INFO 2017-06-29 04:13:26,093 main.py:57] epoch 4760, training loss: 7814.92, average training loss: 7568.79, base loss: 16094.98
[INFO 2017-06-29 04:13:29,111 main.py:57] epoch 4761, training loss: 6818.56, average training loss: 7568.29, base loss: 16094.20
[INFO 2017-06-29 04:13:32,151 main.py:57] epoch 4762, training loss: 7820.86, average training loss: 7568.92, base loss: 16094.28
[INFO 2017-06-29 04:13:35,184 main.py:57] epoch 4763, training loss: 7859.38, average training loss: 7569.30, base loss: 16094.16
[INFO 2017-06-29 04:13:38,246 main.py:57] epoch 4764, training loss: 7713.17, average training loss: 7570.31, base loss: 16094.39
[INFO 2017-06-29 04:13:41,314 main.py:57] epoch 4765, training loss: 7644.85, average training loss: 7570.11, base loss: 16094.69
[INFO 2017-06-29 04:13:44,391 main.py:57] epoch 4766, training loss: 6903.28, average training loss: 7570.04, base loss: 16093.97
[INFO 2017-06-29 04:13:47,527 main.py:57] epoch 4767, training loss: 6663.15, average training loss: 7569.61, base loss: 16093.55
[INFO 2017-06-29 04:13:50,576 main.py:57] epoch 4768, training loss: 7192.27, average training loss: 7569.46, base loss: 16093.03
[INFO 2017-06-29 04:13:53,648 main.py:57] epoch 4769, training loss: 7110.01, average training loss: 7568.56, base loss: 16092.40
[INFO 2017-06-29 04:13:56,730 main.py:57] epoch 4770, training loss: 7050.10, average training loss: 7568.41, base loss: 16092.78
[INFO 2017-06-29 04:13:59,770 main.py:57] epoch 4771, training loss: 7381.71, average training loss: 7567.97, base loss: 16092.91
[INFO 2017-06-29 04:14:02,923 main.py:57] epoch 4772, training loss: 6901.95, average training loss: 7568.22, base loss: 16092.59
[INFO 2017-06-29 04:14:06,027 main.py:57] epoch 4773, training loss: 7785.30, average training loss: 7569.80, base loss: 16093.17
[INFO 2017-06-29 04:14:09,077 main.py:57] epoch 4774, training loss: 9308.78, average training loss: 7571.93, base loss: 16093.60
[INFO 2017-06-29 04:14:12,201 main.py:57] epoch 4775, training loss: 7359.20, average training loss: 7572.01, base loss: 16093.26
[INFO 2017-06-29 04:14:15,191 main.py:57] epoch 4776, training loss: 6999.43, average training loss: 7571.43, base loss: 16093.11
[INFO 2017-06-29 04:14:18,285 main.py:57] epoch 4777, training loss: 7032.14, average training loss: 7571.50, base loss: 16092.56
[INFO 2017-06-29 04:14:21,386 main.py:57] epoch 4778, training loss: 6927.80, average training loss: 7570.19, base loss: 16091.94
[INFO 2017-06-29 04:14:24,407 main.py:57] epoch 4779, training loss: 6828.87, average training loss: 7570.04, base loss: 16091.48
[INFO 2017-06-29 04:14:27,431 main.py:57] epoch 4780, training loss: 6984.34, average training loss: 7569.23, base loss: 16091.63
[INFO 2017-06-29 04:14:30,516 main.py:57] epoch 4781, training loss: 7051.55, average training loss: 7569.28, base loss: 16091.51
[INFO 2017-06-29 04:14:33,599 main.py:57] epoch 4782, training loss: 7700.89, average training loss: 7569.22, base loss: 16092.07
[INFO 2017-06-29 04:14:36,644 main.py:57] epoch 4783, training loss: 6850.63, average training loss: 7568.51, base loss: 16092.59
[INFO 2017-06-29 04:14:39,693 main.py:57] epoch 4784, training loss: 7123.98, average training loss: 7567.46, base loss: 16091.97
[INFO 2017-06-29 04:14:42,729 main.py:57] epoch 4785, training loss: 7284.39, average training loss: 7567.71, base loss: 16091.60
[INFO 2017-06-29 04:14:45,794 main.py:57] epoch 4786, training loss: 7636.39, average training loss: 7567.49, base loss: 16091.81
[INFO 2017-06-29 04:14:48,835 main.py:57] epoch 4787, training loss: 7274.24, average training loss: 7566.73, base loss: 16091.48
[INFO 2017-06-29 04:14:51,896 main.py:57] epoch 4788, training loss: 6725.05, average training loss: 7565.63, base loss: 16090.52
[INFO 2017-06-29 04:14:55,012 main.py:57] epoch 4789, training loss: 8574.10, average training loss: 7565.54, base loss: 16091.70
[INFO 2017-06-29 04:14:58,031 main.py:57] epoch 4790, training loss: 7831.54, average training loss: 7566.69, base loss: 16091.92
[INFO 2017-06-29 04:15:01,076 main.py:57] epoch 4791, training loss: 7207.72, average training loss: 7565.92, base loss: 16091.80
[INFO 2017-06-29 04:15:04,060 main.py:57] epoch 4792, training loss: 7182.60, average training loss: 7565.39, base loss: 16091.80
[INFO 2017-06-29 04:15:07,185 main.py:57] epoch 4793, training loss: 7346.06, average training loss: 7565.76, base loss: 16091.81
[INFO 2017-06-29 04:15:10,216 main.py:57] epoch 4794, training loss: 6778.35, average training loss: 7565.10, base loss: 16090.97
[INFO 2017-06-29 04:15:13,214 main.py:57] epoch 4795, training loss: 7425.45, average training loss: 7565.83, base loss: 16090.47
[INFO 2017-06-29 04:15:16,243 main.py:57] epoch 4796, training loss: 6745.22, average training loss: 7564.89, base loss: 16090.19
[INFO 2017-06-29 04:15:19,292 main.py:57] epoch 4797, training loss: 7409.86, average training loss: 7564.32, base loss: 16090.53
[INFO 2017-06-29 04:15:22,330 main.py:57] epoch 4798, training loss: 7719.38, average training loss: 7564.67, base loss: 16090.26
[INFO 2017-06-29 04:15:25,387 main.py:57] epoch 4799, training loss: 7499.56, average training loss: 7564.53, base loss: 16089.98
[INFO 2017-06-29 04:15:25,387 main.py:59] epoch 4799, testing
[INFO 2017-06-29 04:15:38,013 main.py:104] average testing loss: 7922.19, base loss: 16483.92
[INFO 2017-06-29 04:15:38,013 main.py:105] improve_loss: 8561.73, improve_percent: 0.52
[INFO 2017-06-29 04:15:38,014 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:15:41,029 main.py:57] epoch 4800, training loss: 8207.86, average training loss: 7564.37, base loss: 16090.77
[INFO 2017-06-29 04:15:44,189 main.py:57] epoch 4801, training loss: 7382.36, average training loss: 7565.18, base loss: 16091.33
[INFO 2017-06-29 04:15:47,258 main.py:57] epoch 4802, training loss: 7407.84, average training loss: 7564.57, base loss: 16091.61
[INFO 2017-06-29 04:15:50,385 main.py:57] epoch 4803, training loss: 7942.02, average training loss: 7564.64, base loss: 16091.90
[INFO 2017-06-29 04:15:53,450 main.py:57] epoch 4804, training loss: 6419.10, average training loss: 7562.91, base loss: 16090.99
[INFO 2017-06-29 04:15:56,509 main.py:57] epoch 4805, training loss: 7110.08, average training loss: 7561.44, base loss: 16090.57
[INFO 2017-06-29 04:15:59,589 main.py:57] epoch 4806, training loss: 7553.34, average training loss: 7561.87, base loss: 16090.95
[INFO 2017-06-29 04:16:02,576 main.py:57] epoch 4807, training loss: 7732.62, average training loss: 7562.26, base loss: 16091.27
[INFO 2017-06-29 04:16:05,569 main.py:57] epoch 4808, training loss: 7013.93, average training loss: 7562.10, base loss: 16090.81
[INFO 2017-06-29 04:16:08,602 main.py:57] epoch 4809, training loss: 7022.13, average training loss: 7560.87, base loss: 16090.18
[INFO 2017-06-29 04:16:11,683 main.py:57] epoch 4810, training loss: 7182.97, average training loss: 7559.64, base loss: 16089.80
[INFO 2017-06-29 04:16:14,750 main.py:57] epoch 4811, training loss: 7431.14, average training loss: 7559.82, base loss: 16090.08
[INFO 2017-06-29 04:16:17,773 main.py:57] epoch 4812, training loss: 7960.76, average training loss: 7560.70, base loss: 16091.02
[INFO 2017-06-29 04:16:20,867 main.py:57] epoch 4813, training loss: 7550.48, average training loss: 7560.90, base loss: 16091.03
[INFO 2017-06-29 04:16:23,932 main.py:57] epoch 4814, training loss: 7644.07, average training loss: 7561.77, base loss: 16090.75
[INFO 2017-06-29 04:16:27,004 main.py:57] epoch 4815, training loss: 7528.37, average training loss: 7562.11, base loss: 16090.26
[INFO 2017-06-29 04:16:30,083 main.py:57] epoch 4816, training loss: 7421.62, average training loss: 7562.05, base loss: 16090.29
[INFO 2017-06-29 04:16:33,118 main.py:57] epoch 4817, training loss: 6737.04, average training loss: 7561.90, base loss: 16089.74
[INFO 2017-06-29 04:16:36,136 main.py:57] epoch 4818, training loss: 7220.54, average training loss: 7562.45, base loss: 16089.37
[INFO 2017-06-29 04:16:39,195 main.py:57] epoch 4819, training loss: 9348.76, average training loss: 7563.71, base loss: 16090.83
[INFO 2017-06-29 04:16:42,257 main.py:57] epoch 4820, training loss: 8256.73, average training loss: 7564.29, base loss: 16091.33
[INFO 2017-06-29 04:16:45,284 main.py:57] epoch 4821, training loss: 7987.98, average training loss: 7564.69, base loss: 16091.80
[INFO 2017-06-29 04:16:48,304 main.py:57] epoch 4822, training loss: 8771.84, average training loss: 7566.56, base loss: 16091.60
[INFO 2017-06-29 04:16:51,439 main.py:57] epoch 4823, training loss: 7839.69, average training loss: 7565.71, base loss: 16092.03
[INFO 2017-06-29 04:16:54,551 main.py:57] epoch 4824, training loss: 7477.35, average training loss: 7565.99, base loss: 16092.05
[INFO 2017-06-29 04:16:57,564 main.py:57] epoch 4825, training loss: 7561.66, average training loss: 7566.83, base loss: 16092.03
[INFO 2017-06-29 04:17:00,619 main.py:57] epoch 4826, training loss: 7566.81, average training loss: 7566.97, base loss: 16091.93
[INFO 2017-06-29 04:17:03,666 main.py:57] epoch 4827, training loss: 8098.29, average training loss: 7567.79, base loss: 16091.98
[INFO 2017-06-29 04:17:06,750 main.py:57] epoch 4828, training loss: 7104.03, average training loss: 7566.93, base loss: 16091.90
[INFO 2017-06-29 04:17:09,776 main.py:57] epoch 4829, training loss: 7107.99, average training loss: 7566.89, base loss: 16091.70
[INFO 2017-06-29 04:17:12,919 main.py:57] epoch 4830, training loss: 6929.19, average training loss: 7566.27, base loss: 16091.44
[INFO 2017-06-29 04:17:15,946 main.py:57] epoch 4831, training loss: 6704.42, average training loss: 7566.12, base loss: 16091.30
[INFO 2017-06-29 04:17:19,038 main.py:57] epoch 4832, training loss: 6881.78, average training loss: 7565.19, base loss: 16090.58
[INFO 2017-06-29 04:17:22,042 main.py:57] epoch 4833, training loss: 7576.92, average training loss: 7565.07, base loss: 16089.71
[INFO 2017-06-29 04:17:25,083 main.py:57] epoch 4834, training loss: 7396.87, average training loss: 7565.24, base loss: 16089.61
[INFO 2017-06-29 04:17:28,104 main.py:57] epoch 4835, training loss: 7736.08, average training loss: 7565.23, base loss: 16089.92
[INFO 2017-06-29 04:17:31,129 main.py:57] epoch 4836, training loss: 6489.47, average training loss: 7563.15, base loss: 16089.66
[INFO 2017-06-29 04:17:34,252 main.py:57] epoch 4837, training loss: 7183.32, average training loss: 7562.74, base loss: 16089.40
[INFO 2017-06-29 04:17:37,358 main.py:57] epoch 4838, training loss: 9072.46, average training loss: 7564.71, base loss: 16090.41
[INFO 2017-06-29 04:17:40,386 main.py:57] epoch 4839, training loss: 6690.45, average training loss: 7562.97, base loss: 16090.08
[INFO 2017-06-29 04:17:43,432 main.py:57] epoch 4840, training loss: 7760.54, average training loss: 7563.64, base loss: 16090.10
[INFO 2017-06-29 04:17:46,466 main.py:57] epoch 4841, training loss: 8322.48, average training loss: 7565.02, base loss: 16090.87
[INFO 2017-06-29 04:17:49,505 main.py:57] epoch 4842, training loss: 7425.99, average training loss: 7563.83, base loss: 16091.54
[INFO 2017-06-29 04:17:52,607 main.py:57] epoch 4843, training loss: 6882.28, average training loss: 7563.18, base loss: 16091.12
[INFO 2017-06-29 04:17:55,728 main.py:57] epoch 4844, training loss: 6783.10, average training loss: 7561.74, base loss: 16090.42
[INFO 2017-06-29 04:17:58,773 main.py:57] epoch 4845, training loss: 8053.52, average training loss: 7560.97, base loss: 16090.17
[INFO 2017-06-29 04:18:01,781 main.py:57] epoch 4846, training loss: 9093.94, average training loss: 7562.36, base loss: 16091.60
[INFO 2017-06-29 04:18:04,832 main.py:57] epoch 4847, training loss: 7944.20, average training loss: 7562.97, base loss: 16091.96
[INFO 2017-06-29 04:18:07,880 main.py:57] epoch 4848, training loss: 6826.85, average training loss: 7562.59, base loss: 16091.16
[INFO 2017-06-29 04:18:10,955 main.py:57] epoch 4849, training loss: 7427.74, average training loss: 7562.83, base loss: 16090.39
[INFO 2017-06-29 04:18:14,078 main.py:57] epoch 4850, training loss: 7659.54, average training loss: 7563.22, base loss: 16090.29
[INFO 2017-06-29 04:18:17,114 main.py:57] epoch 4851, training loss: 8470.15, average training loss: 7565.31, base loss: 16090.48
[INFO 2017-06-29 04:18:20,191 main.py:57] epoch 4852, training loss: 8345.99, average training loss: 7566.47, base loss: 16090.96
[INFO 2017-06-29 04:18:23,236 main.py:57] epoch 4853, training loss: 7091.44, average training loss: 7565.32, base loss: 16090.94
[INFO 2017-06-29 04:18:26,286 main.py:57] epoch 4854, training loss: 7211.87, average training loss: 7565.23, base loss: 16090.99
[INFO 2017-06-29 04:18:29,327 main.py:57] epoch 4855, training loss: 7084.42, average training loss: 7564.95, base loss: 16090.62
[INFO 2017-06-29 04:18:32,405 main.py:57] epoch 4856, training loss: 7316.30, average training loss: 7563.40, base loss: 16090.75
[INFO 2017-06-29 04:18:35,470 main.py:57] epoch 4857, training loss: 8221.67, average training loss: 7563.30, base loss: 16091.12
[INFO 2017-06-29 04:18:38,616 main.py:57] epoch 4858, training loss: 7256.10, average training loss: 7563.34, base loss: 16090.35
[INFO 2017-06-29 04:18:41,636 main.py:57] epoch 4859, training loss: 6738.77, average training loss: 7562.88, base loss: 16089.61
[INFO 2017-06-29 04:18:44,680 main.py:57] epoch 4860, training loss: 8034.50, average training loss: 7561.94, base loss: 16090.30
[INFO 2017-06-29 04:18:47,809 main.py:57] epoch 4861, training loss: 6360.73, average training loss: 7561.68, base loss: 16089.88
[INFO 2017-06-29 04:18:50,867 main.py:57] epoch 4862, training loss: 7550.86, average training loss: 7562.44, base loss: 16089.54
[INFO 2017-06-29 04:18:54,043 main.py:57] epoch 4863, training loss: 7555.83, average training loss: 7563.39, base loss: 16089.52
[INFO 2017-06-29 04:18:57,135 main.py:57] epoch 4864, training loss: 8383.07, average training loss: 7565.79, base loss: 16089.93
[INFO 2017-06-29 04:19:00,187 main.py:57] epoch 4865, training loss: 7382.55, average training loss: 7564.81, base loss: 16089.75
[INFO 2017-06-29 04:19:03,290 main.py:57] epoch 4866, training loss: 7360.98, average training loss: 7565.38, base loss: 16089.32
[INFO 2017-06-29 04:19:06,357 main.py:57] epoch 4867, training loss: 8159.31, average training loss: 7565.57, base loss: 16089.72
[INFO 2017-06-29 04:19:09,476 main.py:57] epoch 4868, training loss: 6312.00, average training loss: 7565.02, base loss: 16089.11
[INFO 2017-06-29 04:19:12,527 main.py:57] epoch 4869, training loss: 7944.63, average training loss: 7565.59, base loss: 16089.37
[INFO 2017-06-29 04:19:15,566 main.py:57] epoch 4870, training loss: 8914.85, average training loss: 7567.32, base loss: 16090.31
[INFO 2017-06-29 04:19:18,627 main.py:57] epoch 4871, training loss: 6840.32, average training loss: 7567.34, base loss: 16090.03
[INFO 2017-06-29 04:19:21,695 main.py:57] epoch 4872, training loss: 7627.44, average training loss: 7567.15, base loss: 16090.03
[INFO 2017-06-29 04:19:24,765 main.py:57] epoch 4873, training loss: 8162.21, average training loss: 7567.79, base loss: 16091.57
[INFO 2017-06-29 04:19:27,827 main.py:57] epoch 4874, training loss: 8642.96, average training loss: 7568.72, base loss: 16092.20
[INFO 2017-06-29 04:19:30,869 main.py:57] epoch 4875, training loss: 7456.90, average training loss: 7567.02, base loss: 16091.59
[INFO 2017-06-29 04:19:33,890 main.py:57] epoch 4876, training loss: 7404.39, average training loss: 7566.46, base loss: 16091.51
[INFO 2017-06-29 04:19:37,015 main.py:57] epoch 4877, training loss: 6923.08, average training loss: 7565.19, base loss: 16091.42
[INFO 2017-06-29 04:19:40,061 main.py:57] epoch 4878, training loss: 7571.05, average training loss: 7565.58, base loss: 16091.17
[INFO 2017-06-29 04:19:43,072 main.py:57] epoch 4879, training loss: 7867.12, average training loss: 7565.98, base loss: 16091.08
[INFO 2017-06-29 04:19:46,102 main.py:57] epoch 4880, training loss: 7235.25, average training loss: 7565.84, base loss: 16091.06
[INFO 2017-06-29 04:19:49,186 main.py:57] epoch 4881, training loss: 6756.86, average training loss: 7564.40, base loss: 16090.17
[INFO 2017-06-29 04:19:52,160 main.py:57] epoch 4882, training loss: 7275.84, average training loss: 7563.70, base loss: 16089.69
[INFO 2017-06-29 04:19:55,129 main.py:57] epoch 4883, training loss: 7742.31, average training loss: 7564.29, base loss: 16089.75
[INFO 2017-06-29 04:19:58,162 main.py:57] epoch 4884, training loss: 7854.80, average training loss: 7565.23, base loss: 16090.34
[INFO 2017-06-29 04:20:01,149 main.py:57] epoch 4885, training loss: 7017.98, average training loss: 7565.63, base loss: 16090.46
[INFO 2017-06-29 04:20:04,187 main.py:57] epoch 4886, training loss: 7136.33, average training loss: 7564.74, base loss: 16090.78
[INFO 2017-06-29 04:20:07,181 main.py:57] epoch 4887, training loss: 7517.89, average training loss: 7565.03, base loss: 16091.09
[INFO 2017-06-29 04:20:10,250 main.py:57] epoch 4888, training loss: 7561.31, average training loss: 7563.81, base loss: 16091.34
[INFO 2017-06-29 04:20:13,309 main.py:57] epoch 4889, training loss: 7329.75, average training loss: 7563.42, base loss: 16091.37
[INFO 2017-06-29 04:20:16,406 main.py:57] epoch 4890, training loss: 7947.38, average training loss: 7562.18, base loss: 16091.51
[INFO 2017-06-29 04:20:19,453 main.py:57] epoch 4891, training loss: 7489.99, average training loss: 7561.47, base loss: 16091.54
[INFO 2017-06-29 04:20:22,560 main.py:57] epoch 4892, training loss: 8028.86, average training loss: 7562.53, base loss: 16092.44
[INFO 2017-06-29 04:20:25,608 main.py:57] epoch 4893, training loss: 7268.23, average training loss: 7561.14, base loss: 16092.26
[INFO 2017-06-29 04:20:28,615 main.py:57] epoch 4894, training loss: 6727.36, average training loss: 7559.19, base loss: 16091.98
[INFO 2017-06-29 04:20:31,632 main.py:57] epoch 4895, training loss: 7550.40, average training loss: 7558.78, base loss: 16092.08
[INFO 2017-06-29 04:20:34,741 main.py:57] epoch 4896, training loss: 6651.72, average training loss: 7557.67, base loss: 16092.17
[INFO 2017-06-29 04:20:37,823 main.py:57] epoch 4897, training loss: 5962.15, average training loss: 7556.75, base loss: 16091.22
[INFO 2017-06-29 04:20:40,948 main.py:57] epoch 4898, training loss: 6974.44, average training loss: 7556.67, base loss: 16090.36
[INFO 2017-06-29 04:20:44,003 main.py:57] epoch 4899, training loss: 8025.09, average training loss: 7558.10, base loss: 16090.39
[INFO 2017-06-29 04:20:44,004 main.py:59] epoch 4899, testing
[INFO 2017-06-29 04:20:56,614 main.py:104] average testing loss: 7662.25, base loss: 15926.25
[INFO 2017-06-29 04:20:56,614 main.py:105] improve_loss: 8264.00, improve_percent: 0.52
[INFO 2017-06-29 04:20:56,615 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:20:59,782 main.py:57] epoch 4900, training loss: 8066.80, average training loss: 7558.76, base loss: 16090.73
[INFO 2017-06-29 04:21:02,833 main.py:57] epoch 4901, training loss: 6603.97, average training loss: 7557.78, base loss: 16089.46
[INFO 2017-06-29 04:21:05,844 main.py:57] epoch 4902, training loss: 7958.69, average training loss: 7559.11, base loss: 16089.54
[INFO 2017-06-29 04:21:08,918 main.py:57] epoch 4903, training loss: 8950.41, average training loss: 7560.89, base loss: 16089.43
[INFO 2017-06-29 04:21:11,954 main.py:57] epoch 4904, training loss: 8108.92, average training loss: 7561.25, base loss: 16090.05
[INFO 2017-06-29 04:21:14,977 main.py:57] epoch 4905, training loss: 7146.74, average training loss: 7560.95, base loss: 16089.70
[INFO 2017-06-29 04:21:18,085 main.py:57] epoch 4906, training loss: 7265.37, average training loss: 7559.67, base loss: 16089.78
[INFO 2017-06-29 04:21:21,232 main.py:57] epoch 4907, training loss: 7021.22, average training loss: 7559.61, base loss: 16088.95
[INFO 2017-06-29 04:21:24,293 main.py:57] epoch 4908, training loss: 7121.16, average training loss: 7558.97, base loss: 16089.23
[INFO 2017-06-29 04:21:27,410 main.py:57] epoch 4909, training loss: 7312.58, average training loss: 7558.12, base loss: 16089.33
[INFO 2017-06-29 04:21:30,430 main.py:57] epoch 4910, training loss: 7297.17, average training loss: 7557.51, base loss: 16088.89
[INFO 2017-06-29 04:21:33,523 main.py:57] epoch 4911, training loss: 7640.41, average training loss: 7557.55, base loss: 16089.38
[INFO 2017-06-29 04:21:36,734 main.py:57] epoch 4912, training loss: 7237.37, average training loss: 7556.74, base loss: 16089.62
[INFO 2017-06-29 04:21:39,827 main.py:57] epoch 4913, training loss: 7506.84, average training loss: 7557.14, base loss: 16090.26
[INFO 2017-06-29 04:21:42,987 main.py:57] epoch 4914, training loss: 7063.38, average training loss: 7556.87, base loss: 16090.09
[INFO 2017-06-29 04:21:46,092 main.py:57] epoch 4915, training loss: 8020.52, average training loss: 7557.67, base loss: 16090.13
[INFO 2017-06-29 04:21:49,118 main.py:57] epoch 4916, training loss: 9062.78, average training loss: 7559.92, base loss: 16090.49
[INFO 2017-06-29 04:21:52,139 main.py:57] epoch 4917, training loss: 8270.14, average training loss: 7560.56, base loss: 16090.97
[INFO 2017-06-29 04:21:55,256 main.py:57] epoch 4918, training loss: 7298.33, average training loss: 7561.47, base loss: 16090.71
[INFO 2017-06-29 04:21:58,336 main.py:57] epoch 4919, training loss: 7270.40, average training loss: 7561.61, base loss: 16090.42
[INFO 2017-06-29 04:22:01,455 main.py:57] epoch 4920, training loss: 7158.31, average training loss: 7561.45, base loss: 16089.70
[INFO 2017-06-29 04:22:04,545 main.py:57] epoch 4921, training loss: 7760.12, average training loss: 7562.36, base loss: 16089.75
[INFO 2017-06-29 04:22:07,557 main.py:57] epoch 4922, training loss: 7615.89, average training loss: 7562.85, base loss: 16090.08
[INFO 2017-06-29 04:22:10,617 main.py:57] epoch 4923, training loss: 6540.12, average training loss: 7562.19, base loss: 16089.53
[INFO 2017-06-29 04:22:13,636 main.py:57] epoch 4924, training loss: 6833.56, average training loss: 7560.66, base loss: 16088.83
[INFO 2017-06-29 04:22:16,681 main.py:57] epoch 4925, training loss: 7056.47, average training loss: 7559.50, base loss: 16088.45
[INFO 2017-06-29 04:22:19,677 main.py:57] epoch 4926, training loss: 7284.29, average training loss: 7558.39, base loss: 16088.09
[INFO 2017-06-29 04:22:22,699 main.py:57] epoch 4927, training loss: 8280.91, average training loss: 7558.48, base loss: 16089.41
[INFO 2017-06-29 04:22:25,678 main.py:57] epoch 4928, training loss: 6974.42, average training loss: 7558.45, base loss: 16088.92
[INFO 2017-06-29 04:22:28,783 main.py:57] epoch 4929, training loss: 7197.36, average training loss: 7558.50, base loss: 16088.75
[INFO 2017-06-29 04:22:31,860 main.py:57] epoch 4930, training loss: 7217.81, average training loss: 7557.98, base loss: 16088.41
[INFO 2017-06-29 04:22:34,933 main.py:57] epoch 4931, training loss: 8077.09, average training loss: 7557.99, base loss: 16088.37
[INFO 2017-06-29 04:22:37,951 main.py:57] epoch 4932, training loss: 7072.03, average training loss: 7557.37, base loss: 16087.97
[INFO 2017-06-29 04:22:41,006 main.py:57] epoch 4933, training loss: 8086.22, average training loss: 7557.52, base loss: 16088.24
[INFO 2017-06-29 04:22:44,001 main.py:57] epoch 4934, training loss: 6793.81, average training loss: 7556.90, base loss: 16087.78
[INFO 2017-06-29 04:22:46,991 main.py:57] epoch 4935, training loss: 7590.05, average training loss: 7556.18, base loss: 16088.32
[INFO 2017-06-29 04:22:50,010 main.py:57] epoch 4936, training loss: 7175.82, average training loss: 7556.29, base loss: 16088.35
[INFO 2017-06-29 04:22:53,060 main.py:57] epoch 4937, training loss: 8350.17, average training loss: 7557.91, base loss: 16089.08
[INFO 2017-06-29 04:22:56,075 main.py:57] epoch 4938, training loss: 6374.77, average training loss: 7556.58, base loss: 16088.18
[INFO 2017-06-29 04:22:59,178 main.py:57] epoch 4939, training loss: 7633.88, average training loss: 7556.43, base loss: 16088.21
[INFO 2017-06-29 04:23:02,229 main.py:57] epoch 4940, training loss: 7414.51, average training loss: 7555.76, base loss: 16088.78
[INFO 2017-06-29 04:23:05,226 main.py:57] epoch 4941, training loss: 7679.55, average training loss: 7555.13, base loss: 16089.25
[INFO 2017-06-29 04:23:08,371 main.py:57] epoch 4942, training loss: 6633.45, average training loss: 7554.06, base loss: 16088.99
[INFO 2017-06-29 04:23:11,359 main.py:57] epoch 4943, training loss: 6990.39, average training loss: 7553.69, base loss: 16088.63
[INFO 2017-06-29 04:23:14,481 main.py:57] epoch 4944, training loss: 6608.72, average training loss: 7552.23, base loss: 16088.14
[INFO 2017-06-29 04:23:17,465 main.py:57] epoch 4945, training loss: 7185.46, average training loss: 7551.54, base loss: 16087.54
[INFO 2017-06-29 04:23:20,468 main.py:57] epoch 4946, training loss: 7065.57, average training loss: 7551.93, base loss: 16087.33
[INFO 2017-06-29 04:23:23,555 main.py:57] epoch 4947, training loss: 7442.63, average training loss: 7551.54, base loss: 16087.34
[INFO 2017-06-29 04:23:26,583 main.py:57] epoch 4948, training loss: 8019.23, average training loss: 7551.67, base loss: 16088.19
[INFO 2017-06-29 04:23:29,581 main.py:57] epoch 4949, training loss: 7720.61, average training loss: 7551.81, base loss: 16088.40
[INFO 2017-06-29 04:23:32,614 main.py:57] epoch 4950, training loss: 6397.00, average training loss: 7549.41, base loss: 16087.23
[INFO 2017-06-29 04:23:35,737 main.py:57] epoch 4951, training loss: 8411.04, average training loss: 7549.62, base loss: 16087.39
[INFO 2017-06-29 04:23:38,795 main.py:57] epoch 4952, training loss: 7438.42, average training loss: 7548.59, base loss: 16087.11
[INFO 2017-06-29 04:23:41,897 main.py:57] epoch 4953, training loss: 8132.71, average training loss: 7549.52, base loss: 16087.75
[INFO 2017-06-29 04:23:44,952 main.py:57] epoch 4954, training loss: 7920.96, average training loss: 7550.04, base loss: 16088.32
[INFO 2017-06-29 04:23:48,031 main.py:57] epoch 4955, training loss: 7088.63, average training loss: 7549.31, base loss: 16088.14
[INFO 2017-06-29 04:23:51,048 main.py:57] epoch 4956, training loss: 6491.13, average training loss: 7547.95, base loss: 16087.49
[INFO 2017-06-29 04:23:54,092 main.py:57] epoch 4957, training loss: 7750.14, average training loss: 7548.31, base loss: 16087.92
[INFO 2017-06-29 04:23:57,104 main.py:57] epoch 4958, training loss: 7668.43, average training loss: 7548.93, base loss: 16088.69
[INFO 2017-06-29 04:24:00,246 main.py:57] epoch 4959, training loss: 7188.80, average training loss: 7548.10, base loss: 16089.54
[INFO 2017-06-29 04:24:03,287 main.py:57] epoch 4960, training loss: 6840.44, average training loss: 7547.27, base loss: 16088.81
[INFO 2017-06-29 04:24:06,346 main.py:57] epoch 4961, training loss: 7877.37, average training loss: 7548.42, base loss: 16088.53
[INFO 2017-06-29 04:24:09,464 main.py:57] epoch 4962, training loss: 6866.71, average training loss: 7548.45, base loss: 16087.94
[INFO 2017-06-29 04:24:12,480 main.py:57] epoch 4963, training loss: 7001.40, average training loss: 7547.88, base loss: 16087.61
[INFO 2017-06-29 04:24:15,463 main.py:57] epoch 4964, training loss: 7764.59, average training loss: 7549.06, base loss: 16088.05
[INFO 2017-06-29 04:24:18,531 main.py:57] epoch 4965, training loss: 7332.30, average training loss: 7548.68, base loss: 16087.72
[INFO 2017-06-29 04:24:21,584 main.py:57] epoch 4966, training loss: 7388.99, average training loss: 7547.24, base loss: 16087.89
[INFO 2017-06-29 04:24:24,634 main.py:57] epoch 4967, training loss: 7971.30, average training loss: 7548.10, base loss: 16088.38
[INFO 2017-06-29 04:24:27,669 main.py:57] epoch 4968, training loss: 7384.06, average training loss: 7548.23, base loss: 16088.26
[INFO 2017-06-29 04:24:30,713 main.py:57] epoch 4969, training loss: 7489.23, average training loss: 7548.45, base loss: 16088.30
[INFO 2017-06-29 04:24:33,714 main.py:57] epoch 4970, training loss: 6818.66, average training loss: 7547.99, base loss: 16087.91
[INFO 2017-06-29 04:24:36,729 main.py:57] epoch 4971, training loss: 8275.19, average training loss: 7548.34, base loss: 16088.07
[INFO 2017-06-29 04:24:39,811 main.py:57] epoch 4972, training loss: 6793.67, average training loss: 7547.55, base loss: 16087.35
[INFO 2017-06-29 04:24:42,814 main.py:57] epoch 4973, training loss: 7236.31, average training loss: 7546.49, base loss: 16086.99
[INFO 2017-06-29 04:24:45,877 main.py:57] epoch 4974, training loss: 8543.78, average training loss: 7547.29, base loss: 16087.83
[INFO 2017-06-29 04:24:48,947 main.py:57] epoch 4975, training loss: 7168.23, average training loss: 7546.02, base loss: 16087.79
[INFO 2017-06-29 04:24:51,991 main.py:57] epoch 4976, training loss: 8088.01, average training loss: 7545.23, base loss: 16088.25
[INFO 2017-06-29 04:24:55,013 main.py:57] epoch 4977, training loss: 8704.37, average training loss: 7546.25, base loss: 16089.43
[INFO 2017-06-29 04:24:58,083 main.py:57] epoch 4978, training loss: 8569.42, average training loss: 7547.35, base loss: 16089.85
[INFO 2017-06-29 04:25:01,128 main.py:57] epoch 4979, training loss: 7648.98, average training loss: 7547.73, base loss: 16089.68
[INFO 2017-06-29 04:25:04,215 main.py:57] epoch 4980, training loss: 6658.85, average training loss: 7546.95, base loss: 16089.16
[INFO 2017-06-29 04:25:07,242 main.py:57] epoch 4981, training loss: 8241.10, average training loss: 7547.19, base loss: 16089.52
[INFO 2017-06-29 04:25:10,303 main.py:57] epoch 4982, training loss: 7600.28, average training loss: 7546.59, base loss: 16089.14
[INFO 2017-06-29 04:25:13,428 main.py:57] epoch 4983, training loss: 6984.98, average training loss: 7545.01, base loss: 16088.63
[INFO 2017-06-29 04:25:16,482 main.py:57] epoch 4984, training loss: 10023.75, average training loss: 7547.53, base loss: 16090.39
[INFO 2017-06-29 04:25:19,503 main.py:57] epoch 4985, training loss: 7019.65, average training loss: 7547.27, base loss: 16089.86
[INFO 2017-06-29 04:25:22,538 main.py:57] epoch 4986, training loss: 7565.89, average training loss: 7547.85, base loss: 16089.24
[INFO 2017-06-29 04:25:25,627 main.py:57] epoch 4987, training loss: 8388.80, average training loss: 7548.16, base loss: 16089.97
[INFO 2017-06-29 04:25:28,695 main.py:57] epoch 4988, training loss: 7114.61, average training loss: 7548.32, base loss: 16089.00
[INFO 2017-06-29 04:25:31,786 main.py:57] epoch 4989, training loss: 7455.95, average training loss: 7547.93, base loss: 16089.12
[INFO 2017-06-29 04:25:34,840 main.py:57] epoch 4990, training loss: 6644.49, average training loss: 7546.93, base loss: 16088.44
[INFO 2017-06-29 04:25:37,818 main.py:57] epoch 4991, training loss: 7335.83, average training loss: 7546.11, base loss: 16088.19
[INFO 2017-06-29 04:25:40,893 main.py:57] epoch 4992, training loss: 8173.50, average training loss: 7546.75, base loss: 16088.98
[INFO 2017-06-29 04:25:43,928 main.py:57] epoch 4993, training loss: 8047.43, average training loss: 7547.97, base loss: 16089.25
[INFO 2017-06-29 04:25:46,942 main.py:57] epoch 4994, training loss: 7787.63, average training loss: 7548.91, base loss: 16089.06
[INFO 2017-06-29 04:25:50,016 main.py:57] epoch 4995, training loss: 7476.21, average training loss: 7548.75, base loss: 16088.91
[INFO 2017-06-29 04:25:53,086 main.py:57] epoch 4996, training loss: 7356.21, average training loss: 7549.71, base loss: 16088.75
[INFO 2017-06-29 04:25:56,147 main.py:57] epoch 4997, training loss: 8231.92, average training loss: 7550.31, base loss: 16089.44
[INFO 2017-06-29 04:25:59,221 main.py:57] epoch 4998, training loss: 6650.58, average training loss: 7548.92, base loss: 16088.94
[INFO 2017-06-29 04:26:02,274 main.py:57] epoch 4999, training loss: 7167.27, average training loss: 7548.51, base loss: 16088.45
[INFO 2017-06-29 04:26:02,274 main.py:59] epoch 4999, testing
[INFO 2017-06-29 04:26:14,956 main.py:104] average testing loss: 8077.78, base loss: 16581.09
[INFO 2017-06-29 04:26:14,956 main.py:105] improve_loss: 8503.32, improve_percent: 0.51
[INFO 2017-06-29 04:26:14,958 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:26:17,934 main.py:57] epoch 5000, training loss: 6676.68, average training loss: 7547.47, base loss: 16087.88
[INFO 2017-06-29 04:26:21,026 main.py:57] epoch 5001, training loss: 7615.82, average training loss: 7547.07, base loss: 16087.84
[INFO 2017-06-29 04:26:24,101 main.py:57] epoch 5002, training loss: 9187.27, average training loss: 7549.32, base loss: 16088.99
[INFO 2017-06-29 04:26:27,254 main.py:57] epoch 5003, training loss: 6829.88, average training loss: 7546.67, base loss: 16088.91
[INFO 2017-06-29 04:26:30,299 main.py:57] epoch 5004, training loss: 7479.11, average training loss: 7546.59, base loss: 16088.29
[INFO 2017-06-29 04:26:33,347 main.py:57] epoch 5005, training loss: 6723.46, average training loss: 7545.63, base loss: 16087.44
[INFO 2017-06-29 04:26:36,399 main.py:57] epoch 5006, training loss: 6316.17, average training loss: 7543.44, base loss: 16086.82
[INFO 2017-06-29 04:26:39,504 main.py:57] epoch 5007, training loss: 7925.96, average training loss: 7544.22, base loss: 16086.65
[INFO 2017-06-29 04:26:42,543 main.py:57] epoch 5008, training loss: 8209.90, average training loss: 7544.82, base loss: 16087.47
[INFO 2017-06-29 04:26:45,587 main.py:57] epoch 5009, training loss: 8723.21, average training loss: 7544.69, base loss: 16088.14
[INFO 2017-06-29 04:26:48,650 main.py:57] epoch 5010, training loss: 6458.34, average training loss: 7544.43, base loss: 16087.67
[INFO 2017-06-29 04:26:51,760 main.py:57] epoch 5011, training loss: 7134.76, average training loss: 7543.33, base loss: 16088.12
[INFO 2017-06-29 04:26:54,854 main.py:57] epoch 5012, training loss: 9128.43, average training loss: 7543.19, base loss: 16089.19
[INFO 2017-06-29 04:26:57,871 main.py:57] epoch 5013, training loss: 7724.93, average training loss: 7543.56, base loss: 16089.36
[INFO 2017-06-29 04:27:00,867 main.py:57] epoch 5014, training loss: 7248.00, average training loss: 7542.21, base loss: 16088.71
[INFO 2017-06-29 04:27:03,878 main.py:57] epoch 5015, training loss: 7738.96, average training loss: 7541.87, base loss: 16088.71
[INFO 2017-06-29 04:27:06,929 main.py:57] epoch 5016, training loss: 7991.82, average training loss: 7542.64, base loss: 16088.85
[INFO 2017-06-29 04:27:09,995 main.py:57] epoch 5017, training loss: 7985.43, average training loss: 7543.67, base loss: 16089.28
[INFO 2017-06-29 04:27:13,013 main.py:57] epoch 5018, training loss: 6547.04, average training loss: 7541.96, base loss: 16089.08
[INFO 2017-06-29 04:27:16,068 main.py:57] epoch 5019, training loss: 9256.34, average training loss: 7544.24, base loss: 16089.92
[INFO 2017-06-29 04:27:19,095 main.py:57] epoch 5020, training loss: 7171.52, average training loss: 7543.93, base loss: 16089.84
[INFO 2017-06-29 04:27:22,157 main.py:57] epoch 5021, training loss: 8017.92, average training loss: 7543.86, base loss: 16090.82
[INFO 2017-06-29 04:27:25,140 main.py:57] epoch 5022, training loss: 8890.37, average training loss: 7545.01, base loss: 16091.58
[INFO 2017-06-29 04:27:28,217 main.py:57] epoch 5023, training loss: 7411.32, average training loss: 7544.85, base loss: 16091.94
[INFO 2017-06-29 04:27:31,279 main.py:57] epoch 5024, training loss: 8198.72, average training loss: 7545.69, base loss: 16092.14
[INFO 2017-06-29 04:27:34,291 main.py:57] epoch 5025, training loss: 7563.39, average training loss: 7545.48, base loss: 16092.41
[INFO 2017-06-29 04:27:37,390 main.py:57] epoch 5026, training loss: 7093.47, average training loss: 7545.53, base loss: 16091.79
[INFO 2017-06-29 04:27:40,470 main.py:57] epoch 5027, training loss: 7865.15, average training loss: 7545.39, base loss: 16092.45
[INFO 2017-06-29 04:27:43,533 main.py:57] epoch 5028, training loss: 9466.05, average training loss: 7547.04, base loss: 16093.63
[INFO 2017-06-29 04:27:46,624 main.py:57] epoch 5029, training loss: 6988.45, average training loss: 7547.00, base loss: 16093.04
[INFO 2017-06-29 04:27:49,705 main.py:57] epoch 5030, training loss: 6451.43, average training loss: 7545.12, base loss: 16092.66
[INFO 2017-06-29 04:27:52,747 main.py:57] epoch 5031, training loss: 7542.90, average training loss: 7545.19, base loss: 16093.18
[INFO 2017-06-29 04:27:55,789 main.py:57] epoch 5032, training loss: 7298.50, average training loss: 7544.58, base loss: 16093.04
[INFO 2017-06-29 04:27:58,892 main.py:57] epoch 5033, training loss: 6400.02, average training loss: 7544.11, base loss: 16091.43
[INFO 2017-06-29 04:28:01,967 main.py:57] epoch 5034, training loss: 7831.52, average training loss: 7544.14, base loss: 16091.35
[INFO 2017-06-29 04:28:05,020 main.py:57] epoch 5035, training loss: 8066.44, average training loss: 7544.71, base loss: 16091.29
[INFO 2017-06-29 04:28:08,080 main.py:57] epoch 5036, training loss: 7566.70, average training loss: 7544.15, base loss: 16091.38
[INFO 2017-06-29 04:28:11,145 main.py:57] epoch 5037, training loss: 6901.42, average training loss: 7543.87, base loss: 16090.86
[INFO 2017-06-29 04:28:14,224 main.py:57] epoch 5038, training loss: 7656.97, average training loss: 7543.06, base loss: 16091.52
[INFO 2017-06-29 04:28:17,247 main.py:57] epoch 5039, training loss: 7121.81, average training loss: 7542.88, base loss: 16090.91
[INFO 2017-06-29 04:28:20,293 main.py:57] epoch 5040, training loss: 6982.24, average training loss: 7542.23, base loss: 16090.58
[INFO 2017-06-29 04:28:23,420 main.py:57] epoch 5041, training loss: 7731.14, average training loss: 7543.00, base loss: 16091.06
[INFO 2017-06-29 04:28:26,436 main.py:57] epoch 5042, training loss: 7521.99, average training loss: 7543.91, base loss: 16090.92
[INFO 2017-06-29 04:28:29,470 main.py:57] epoch 5043, training loss: 7964.61, average training loss: 7544.49, base loss: 16090.98
[INFO 2017-06-29 04:28:32,535 main.py:57] epoch 5044, training loss: 6864.62, average training loss: 7543.00, base loss: 16090.66
[INFO 2017-06-29 04:28:35,516 main.py:57] epoch 5045, training loss: 6829.06, average training loss: 7541.44, base loss: 16090.31
[INFO 2017-06-29 04:28:38,619 main.py:57] epoch 5046, training loss: 8900.74, average training loss: 7541.62, base loss: 16090.54
[INFO 2017-06-29 04:28:41,718 main.py:57] epoch 5047, training loss: 6700.14, average training loss: 7541.32, base loss: 16090.17
[INFO 2017-06-29 04:28:44,808 main.py:57] epoch 5048, training loss: 7025.06, average training loss: 7540.25, base loss: 16090.21
[INFO 2017-06-29 04:28:47,864 main.py:57] epoch 5049, training loss: 7186.54, average training loss: 7539.86, base loss: 16090.76
[INFO 2017-06-29 04:28:50,938 main.py:57] epoch 5050, training loss: 7487.94, average training loss: 7539.83, base loss: 16090.90
[INFO 2017-06-29 04:28:53,956 main.py:57] epoch 5051, training loss: 7215.78, average training loss: 7540.21, base loss: 16091.03
[INFO 2017-06-29 04:28:56,960 main.py:57] epoch 5052, training loss: 8699.40, average training loss: 7541.53, base loss: 16092.06
[INFO 2017-06-29 04:29:00,006 main.py:57] epoch 5053, training loss: 7261.07, average training loss: 7541.17, base loss: 16092.24
[INFO 2017-06-29 04:29:03,085 main.py:57] epoch 5054, training loss: 7560.91, average training loss: 7540.44, base loss: 16091.80
[INFO 2017-06-29 04:29:06,156 main.py:57] epoch 5055, training loss: 7216.33, average training loss: 7539.28, base loss: 16091.22
[INFO 2017-06-29 04:29:09,208 main.py:57] epoch 5056, training loss: 7477.63, average training loss: 7539.88, base loss: 16091.34
[INFO 2017-06-29 04:29:12,250 main.py:57] epoch 5057, training loss: 9037.69, average training loss: 7541.96, base loss: 16092.65
[INFO 2017-06-29 04:29:15,284 main.py:57] epoch 5058, training loss: 7403.80, average training loss: 7543.03, base loss: 16091.89
[INFO 2017-06-29 04:29:18,356 main.py:57] epoch 5059, training loss: 7544.76, average training loss: 7543.64, base loss: 16091.72
[INFO 2017-06-29 04:29:21,404 main.py:57] epoch 5060, training loss: 7269.79, average training loss: 7543.10, base loss: 16092.32
[INFO 2017-06-29 04:29:24,475 main.py:57] epoch 5061, training loss: 7005.99, average training loss: 7541.21, base loss: 16091.90
[INFO 2017-06-29 04:29:27,549 main.py:57] epoch 5062, training loss: 7587.13, average training loss: 7541.08, base loss: 16091.67
[INFO 2017-06-29 04:29:30,696 main.py:57] epoch 5063, training loss: 7708.97, average training loss: 7541.77, base loss: 16092.20
[INFO 2017-06-29 04:29:33,705 main.py:57] epoch 5064, training loss: 6904.43, average training loss: 7540.48, base loss: 16092.26
[INFO 2017-06-29 04:29:36,750 main.py:57] epoch 5065, training loss: 9191.67, average training loss: 7541.59, base loss: 16093.56
[INFO 2017-06-29 04:29:39,808 main.py:57] epoch 5066, training loss: 7571.40, average training loss: 7541.23, base loss: 16093.69
[INFO 2017-06-29 04:29:42,829 main.py:57] epoch 5067, training loss: 6477.46, average training loss: 7539.50, base loss: 16093.25
[INFO 2017-06-29 04:29:45,919 main.py:57] epoch 5068, training loss: 7816.35, average training loss: 7539.43, base loss: 16093.02
[INFO 2017-06-29 04:29:48,898 main.py:57] epoch 5069, training loss: 7032.59, average training loss: 7539.19, base loss: 16092.35
[INFO 2017-06-29 04:29:51,948 main.py:57] epoch 5070, training loss: 7037.16, average training loss: 7538.72, base loss: 16091.80
[INFO 2017-06-29 04:29:54,947 main.py:57] epoch 5071, training loss: 8289.34, average training loss: 7539.48, base loss: 16092.99
[INFO 2017-06-29 04:29:57,953 main.py:57] epoch 5072, training loss: 8923.19, average training loss: 7542.09, base loss: 16093.57
[INFO 2017-06-29 04:30:01,006 main.py:57] epoch 5073, training loss: 7469.97, average training loss: 7541.98, base loss: 16094.11
[INFO 2017-06-29 04:30:04,103 main.py:57] epoch 5074, training loss: 7733.92, average training loss: 7542.33, base loss: 16094.38
[INFO 2017-06-29 04:30:07,222 main.py:57] epoch 5075, training loss: 6548.08, average training loss: 7541.19, base loss: 16094.11
[INFO 2017-06-29 04:30:10,281 main.py:57] epoch 5076, training loss: 7547.10, average training loss: 7541.18, base loss: 16094.41
[INFO 2017-06-29 04:30:13,328 main.py:57] epoch 5077, training loss: 7257.84, average training loss: 7540.33, base loss: 16095.03
[INFO 2017-06-29 04:30:16,407 main.py:57] epoch 5078, training loss: 7295.31, average training loss: 7539.72, base loss: 16094.96
[INFO 2017-06-29 04:30:19,466 main.py:57] epoch 5079, training loss: 7424.33, average training loss: 7539.45, base loss: 16094.78
[INFO 2017-06-29 04:30:22,565 main.py:57] epoch 5080, training loss: 7499.36, average training loss: 7539.69, base loss: 16094.73
[INFO 2017-06-29 04:30:25,564 main.py:57] epoch 5081, training loss: 7346.06, average training loss: 7540.26, base loss: 16094.50
[INFO 2017-06-29 04:30:28,650 main.py:57] epoch 5082, training loss: 7628.37, average training loss: 7540.81, base loss: 16095.09
[INFO 2017-06-29 04:30:31,707 main.py:57] epoch 5083, training loss: 7457.90, average training loss: 7540.03, base loss: 16095.30
[INFO 2017-06-29 04:30:34,728 main.py:57] epoch 5084, training loss: 7481.78, average training loss: 7541.23, base loss: 16095.17
[INFO 2017-06-29 04:30:37,911 main.py:57] epoch 5085, training loss: 7857.35, average training loss: 7541.07, base loss: 16095.62
[INFO 2017-06-29 04:30:41,006 main.py:57] epoch 5086, training loss: 7291.88, average training loss: 7541.24, base loss: 16095.16
[INFO 2017-06-29 04:30:44,116 main.py:57] epoch 5087, training loss: 7045.72, average training loss: 7540.60, base loss: 16094.71
[INFO 2017-06-29 04:30:47,166 main.py:57] epoch 5088, training loss: 7118.78, average training loss: 7540.29, base loss: 16094.55
[INFO 2017-06-29 04:30:50,194 main.py:57] epoch 5089, training loss: 8499.80, average training loss: 7541.77, base loss: 16095.35
[INFO 2017-06-29 04:30:53,215 main.py:57] epoch 5090, training loss: 8662.03, average training loss: 7543.48, base loss: 16095.34
[INFO 2017-06-29 04:30:56,286 main.py:57] epoch 5091, training loss: 6954.20, average training loss: 7543.51, base loss: 16094.52
[INFO 2017-06-29 04:30:59,294 main.py:57] epoch 5092, training loss: 7824.19, average training loss: 7543.69, base loss: 16094.98
[INFO 2017-06-29 04:31:02,325 main.py:57] epoch 5093, training loss: 6962.83, average training loss: 7542.42, base loss: 16095.16
[INFO 2017-06-29 04:31:05,449 main.py:57] epoch 5094, training loss: 7141.85, average training loss: 7541.58, base loss: 16095.19
[INFO 2017-06-29 04:31:08,558 main.py:57] epoch 5095, training loss: 7761.01, average training loss: 7542.33, base loss: 16095.34
[INFO 2017-06-29 04:31:11,601 main.py:57] epoch 5096, training loss: 7692.30, average training loss: 7542.59, base loss: 16095.62
[INFO 2017-06-29 04:31:14,636 main.py:57] epoch 5097, training loss: 6901.29, average training loss: 7540.88, base loss: 16095.12
[INFO 2017-06-29 04:31:17,808 main.py:57] epoch 5098, training loss: 7515.72, average training loss: 7540.75, base loss: 16095.46
[INFO 2017-06-29 04:31:20,905 main.py:57] epoch 5099, training loss: 6830.94, average training loss: 7539.38, base loss: 16094.81
[INFO 2017-06-29 04:31:20,906 main.py:59] epoch 5099, testing
[INFO 2017-06-29 04:31:33,525 main.py:104] average testing loss: 8129.97, base loss: 16642.12
[INFO 2017-06-29 04:31:33,525 main.py:105] improve_loss: 8512.15, improve_percent: 0.51
[INFO 2017-06-29 04:31:33,526 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:31:36,525 main.py:57] epoch 5100, training loss: 6915.62, average training loss: 7539.32, base loss: 16094.79
[INFO 2017-06-29 04:31:39,618 main.py:57] epoch 5101, training loss: 6695.42, average training loss: 7538.85, base loss: 16094.50
[INFO 2017-06-29 04:31:42,698 main.py:57] epoch 5102, training loss: 7145.01, average training loss: 7539.76, base loss: 16094.50
[INFO 2017-06-29 04:31:45,757 main.py:57] epoch 5103, training loss: 8428.93, average training loss: 7541.79, base loss: 16095.39
[INFO 2017-06-29 04:31:48,846 main.py:57] epoch 5104, training loss: 7018.22, average training loss: 7539.40, base loss: 16095.03
[INFO 2017-06-29 04:31:51,897 main.py:57] epoch 5105, training loss: 6316.19, average training loss: 7538.07, base loss: 16093.62
[INFO 2017-06-29 04:31:54,993 main.py:57] epoch 5106, training loss: 7362.35, average training loss: 7538.68, base loss: 16093.89
[INFO 2017-06-29 04:31:58,068 main.py:57] epoch 5107, training loss: 8146.31, average training loss: 7538.18, base loss: 16094.42
[INFO 2017-06-29 04:32:01,171 main.py:57] epoch 5108, training loss: 7554.39, average training loss: 7538.37, base loss: 16094.59
[INFO 2017-06-29 04:32:04,222 main.py:57] epoch 5109, training loss: 6898.27, average training loss: 7536.86, base loss: 16094.59
[INFO 2017-06-29 04:32:07,282 main.py:57] epoch 5110, training loss: 7067.53, average training loss: 7536.65, base loss: 16094.35
[INFO 2017-06-29 04:32:10,297 main.py:57] epoch 5111, training loss: 7509.09, average training loss: 7535.71, base loss: 16094.61
[INFO 2017-06-29 04:32:13,372 main.py:57] epoch 5112, training loss: 6799.64, average training loss: 7535.02, base loss: 16094.81
[INFO 2017-06-29 04:32:16,452 main.py:57] epoch 5113, training loss: 7705.50, average training loss: 7534.56, base loss: 16094.51
[INFO 2017-06-29 04:32:19,471 main.py:57] epoch 5114, training loss: 8203.14, average training loss: 7535.05, base loss: 16094.19
[INFO 2017-06-29 04:32:22,527 main.py:57] epoch 5115, training loss: 7678.38, average training loss: 7535.66, base loss: 16093.94
[INFO 2017-06-29 04:32:25,566 main.py:57] epoch 5116, training loss: 7787.18, average training loss: 7536.27, base loss: 16094.34
[INFO 2017-06-29 04:32:28,644 main.py:57] epoch 5117, training loss: 6737.87, average training loss: 7535.43, base loss: 16093.76
[INFO 2017-06-29 04:32:31,680 main.py:57] epoch 5118, training loss: 7337.91, average training loss: 7535.99, base loss: 16093.63
[INFO 2017-06-29 04:32:34,814 main.py:57] epoch 5119, training loss: 7543.74, average training loss: 7536.04, base loss: 16093.90
[INFO 2017-06-29 04:32:37,891 main.py:57] epoch 5120, training loss: 6854.48, average training loss: 7535.50, base loss: 16093.31
[INFO 2017-06-29 04:32:40,912 main.py:57] epoch 5121, training loss: 6975.28, average training loss: 7535.40, base loss: 16092.96
[INFO 2017-06-29 04:32:43,920 main.py:57] epoch 5122, training loss: 9476.44, average training loss: 7537.75, base loss: 16094.62
[INFO 2017-06-29 04:32:46,946 main.py:57] epoch 5123, training loss: 6622.28, average training loss: 7538.03, base loss: 16094.30
[INFO 2017-06-29 04:32:50,051 main.py:57] epoch 5124, training loss: 7415.20, average training loss: 7536.39, base loss: 16094.81
[INFO 2017-06-29 04:32:53,103 main.py:57] epoch 5125, training loss: 7111.44, average training loss: 7536.16, base loss: 16094.75
[INFO 2017-06-29 04:32:56,164 main.py:57] epoch 5126, training loss: 7400.06, average training loss: 7535.70, base loss: 16094.61
[INFO 2017-06-29 04:32:59,224 main.py:57] epoch 5127, training loss: 7642.22, average training loss: 7534.24, base loss: 16094.44
[INFO 2017-06-29 04:33:02,280 main.py:57] epoch 5128, training loss: 8127.95, average training loss: 7534.28, base loss: 16095.21
[INFO 2017-06-29 04:33:05,336 main.py:57] epoch 5129, training loss: 7445.12, average training loss: 7533.62, base loss: 16095.40
[INFO 2017-06-29 04:33:08,390 main.py:57] epoch 5130, training loss: 7138.47, average training loss: 7532.22, base loss: 16095.64
[INFO 2017-06-29 04:33:11,455 main.py:57] epoch 5131, training loss: 6886.97, average training loss: 7531.99, base loss: 16095.83
[INFO 2017-06-29 04:33:14,459 main.py:57] epoch 5132, training loss: 8421.99, average training loss: 7533.30, base loss: 16096.77
[INFO 2017-06-29 04:33:17,516 main.py:57] epoch 5133, training loss: 7488.30, average training loss: 7532.50, base loss: 16096.83
[INFO 2017-06-29 04:33:20,544 main.py:57] epoch 5134, training loss: 6948.80, average training loss: 7531.96, base loss: 16096.40
[INFO 2017-06-29 04:33:23,605 main.py:57] epoch 5135, training loss: 6582.88, average training loss: 7530.51, base loss: 16095.51
[INFO 2017-06-29 04:33:26,712 main.py:57] epoch 5136, training loss: 7659.62, average training loss: 7530.12, base loss: 16095.38
[INFO 2017-06-29 04:33:29,743 main.py:57] epoch 5137, training loss: 8274.40, average training loss: 7530.85, base loss: 16095.90
[INFO 2017-06-29 04:33:32,759 main.py:57] epoch 5138, training loss: 7160.84, average training loss: 7529.71, base loss: 16095.82
[INFO 2017-06-29 04:33:35,810 main.py:57] epoch 5139, training loss: 6899.57, average training loss: 7528.65, base loss: 16096.01
[INFO 2017-06-29 04:33:38,836 main.py:57] epoch 5140, training loss: 7359.97, average training loss: 7528.22, base loss: 16096.04
[INFO 2017-06-29 04:33:41,861 main.py:57] epoch 5141, training loss: 7782.06, average training loss: 7528.24, base loss: 16096.06
[INFO 2017-06-29 04:33:44,939 main.py:57] epoch 5142, training loss: 8553.55, average training loss: 7528.69, base loss: 16097.02
[INFO 2017-06-29 04:33:47,927 main.py:57] epoch 5143, training loss: 7222.46, average training loss: 7528.74, base loss: 16097.01
[INFO 2017-06-29 04:33:50,991 main.py:57] epoch 5144, training loss: 6954.15, average training loss: 7527.45, base loss: 16096.62
[INFO 2017-06-29 04:33:54,043 main.py:57] epoch 5145, training loss: 7556.36, average training loss: 7527.46, base loss: 16096.84
[INFO 2017-06-29 04:33:57,051 main.py:57] epoch 5146, training loss: 7694.25, average training loss: 7527.48, base loss: 16097.71
[INFO 2017-06-29 04:34:00,145 main.py:57] epoch 5147, training loss: 8384.38, average training loss: 7528.51, base loss: 16098.43
[INFO 2017-06-29 04:34:03,200 main.py:57] epoch 5148, training loss: 6549.66, average training loss: 7528.10, base loss: 16097.53
[INFO 2017-06-29 04:34:06,256 main.py:57] epoch 5149, training loss: 7226.27, average training loss: 7526.94, base loss: 16097.21
[INFO 2017-06-29 04:34:09,337 main.py:57] epoch 5150, training loss: 7349.21, average training loss: 7527.19, base loss: 16097.49
[INFO 2017-06-29 04:34:12,381 main.py:57] epoch 5151, training loss: 8210.33, average training loss: 7527.23, base loss: 16098.16
[INFO 2017-06-29 04:34:15,452 main.py:57] epoch 5152, training loss: 7196.77, average training loss: 7527.27, base loss: 16097.77
[INFO 2017-06-29 04:34:18,446 main.py:57] epoch 5153, training loss: 7900.29, average training loss: 7527.56, base loss: 16097.53
[INFO 2017-06-29 04:34:21,552 main.py:57] epoch 5154, training loss: 7019.48, average training loss: 7526.71, base loss: 16097.00
[INFO 2017-06-29 04:34:24,582 main.py:57] epoch 5155, training loss: 7941.89, average training loss: 7527.34, base loss: 16097.32
[INFO 2017-06-29 04:34:27,639 main.py:57] epoch 5156, training loss: 7023.90, average training loss: 7526.68, base loss: 16096.67
[INFO 2017-06-29 04:34:30,711 main.py:57] epoch 5157, training loss: 8085.99, average training loss: 7526.89, base loss: 16096.61
[INFO 2017-06-29 04:34:33,786 main.py:57] epoch 5158, training loss: 6245.54, average training loss: 7525.15, base loss: 16095.68
[INFO 2017-06-29 04:34:36,820 main.py:57] epoch 5159, training loss: 7369.34, average training loss: 7525.67, base loss: 16095.94
[INFO 2017-06-29 04:34:39,888 main.py:57] epoch 5160, training loss: 6861.32, average training loss: 7525.27, base loss: 16095.47
[INFO 2017-06-29 04:34:42,985 main.py:57] epoch 5161, training loss: 7460.15, average training loss: 7524.74, base loss: 16095.35
[INFO 2017-06-29 04:34:45,966 main.py:57] epoch 5162, training loss: 7263.73, average training loss: 7523.87, base loss: 16095.53
[INFO 2017-06-29 04:34:48,984 main.py:57] epoch 5163, training loss: 6296.39, average training loss: 7522.83, base loss: 16094.73
[INFO 2017-06-29 04:34:51,983 main.py:57] epoch 5164, training loss: 7573.17, average training loss: 7523.00, base loss: 16095.00
[INFO 2017-06-29 04:34:55,036 main.py:57] epoch 5165, training loss: 6846.90, average training loss: 7520.66, base loss: 16094.57
[INFO 2017-06-29 04:34:58,185 main.py:57] epoch 5166, training loss: 7750.48, average training loss: 7520.63, base loss: 16095.00
[INFO 2017-06-29 04:35:01,290 main.py:57] epoch 5167, training loss: 6915.73, average training loss: 7520.39, base loss: 16094.79
[INFO 2017-06-29 04:35:04,349 main.py:57] epoch 5168, training loss: 6613.17, average training loss: 7519.38, base loss: 16094.29
[INFO 2017-06-29 04:35:07,462 main.py:57] epoch 5169, training loss: 7978.67, average training loss: 7520.69, base loss: 16094.33
[INFO 2017-06-29 04:35:10,496 main.py:57] epoch 5170, training loss: 6674.09, average training loss: 7519.19, base loss: 16093.26
[INFO 2017-06-29 04:35:13,556 main.py:57] epoch 5171, training loss: 8910.81, average training loss: 7521.42, base loss: 16093.51
[INFO 2017-06-29 04:35:16,594 main.py:57] epoch 5172, training loss: 7882.91, average training loss: 7520.71, base loss: 16093.52
[INFO 2017-06-29 04:35:19,613 main.py:57] epoch 5173, training loss: 7647.63, average training loss: 7521.31, base loss: 16093.39
[INFO 2017-06-29 04:35:22,704 main.py:57] epoch 5174, training loss: 6207.40, average training loss: 7520.09, base loss: 16092.55
[INFO 2017-06-29 04:35:25,730 main.py:57] epoch 5175, training loss: 8481.49, average training loss: 7521.23, base loss: 16093.33
[INFO 2017-06-29 04:35:28,814 main.py:57] epoch 5176, training loss: 7522.85, average training loss: 7520.55, base loss: 16093.85
[INFO 2017-06-29 04:35:31,884 main.py:57] epoch 5177, training loss: 7461.05, average training loss: 7521.07, base loss: 16094.24
[INFO 2017-06-29 04:35:34,949 main.py:57] epoch 5178, training loss: 7638.11, average training loss: 7521.95, base loss: 16094.20
[INFO 2017-06-29 04:35:38,015 main.py:57] epoch 5179, training loss: 6992.10, average training loss: 7521.93, base loss: 16093.68
[INFO 2017-06-29 04:35:41,046 main.py:57] epoch 5180, training loss: 7046.76, average training loss: 7522.44, base loss: 16093.25
[INFO 2017-06-29 04:35:44,163 main.py:57] epoch 5181, training loss: 7047.47, average training loss: 7522.45, base loss: 16092.62
[INFO 2017-06-29 04:35:47,209 main.py:57] epoch 5182, training loss: 7591.82, average training loss: 7519.91, base loss: 16092.53
[INFO 2017-06-29 04:35:50,286 main.py:57] epoch 5183, training loss: 8207.75, average training loss: 7520.89, base loss: 16093.07
[INFO 2017-06-29 04:35:53,295 main.py:57] epoch 5184, training loss: 7547.76, average training loss: 7521.62, base loss: 16092.91
[INFO 2017-06-29 04:35:56,326 main.py:57] epoch 5185, training loss: 6820.29, average training loss: 7521.74, base loss: 16092.17
[INFO 2017-06-29 04:35:59,326 main.py:57] epoch 5186, training loss: 6901.76, average training loss: 7521.11, base loss: 16091.74
[INFO 2017-06-29 04:36:02,359 main.py:57] epoch 5187, training loss: 7018.18, average training loss: 7520.67, base loss: 16091.17
[INFO 2017-06-29 04:36:05,339 main.py:57] epoch 5188, training loss: 6933.13, average training loss: 7519.49, base loss: 16090.88
[INFO 2017-06-29 04:36:08,413 main.py:57] epoch 5189, training loss: 6850.81, average training loss: 7518.92, base loss: 16090.72
[INFO 2017-06-29 04:36:11,492 main.py:57] epoch 5190, training loss: 7171.78, average training loss: 7517.72, base loss: 16090.33
[INFO 2017-06-29 04:36:14,537 main.py:57] epoch 5191, training loss: 7706.12, average training loss: 7517.84, base loss: 16090.09
[INFO 2017-06-29 04:36:17,628 main.py:57] epoch 5192, training loss: 7108.18, average training loss: 7517.62, base loss: 16089.53
[INFO 2017-06-29 04:36:20,756 main.py:57] epoch 5193, training loss: 9132.64, average training loss: 7520.36, base loss: 16090.75
[INFO 2017-06-29 04:36:23,808 main.py:57] epoch 5194, training loss: 7427.88, average training loss: 7519.67, base loss: 16091.00
[INFO 2017-06-29 04:36:26,865 main.py:57] epoch 5195, training loss: 7336.96, average training loss: 7519.52, base loss: 16090.70
[INFO 2017-06-29 04:36:29,865 main.py:57] epoch 5196, training loss: 8326.09, average training loss: 7520.87, base loss: 16090.95
[INFO 2017-06-29 04:36:32,954 main.py:57] epoch 5197, training loss: 6660.36, average training loss: 7519.45, base loss: 16090.57
[INFO 2017-06-29 04:36:36,025 main.py:57] epoch 5198, training loss: 7716.88, average training loss: 7520.19, base loss: 16090.50
[INFO 2017-06-29 04:36:39,121 main.py:57] epoch 5199, training loss: 7799.47, average training loss: 7519.96, base loss: 16090.86
[INFO 2017-06-29 04:36:39,122 main.py:59] epoch 5199, testing
[INFO 2017-06-29 04:36:51,638 main.py:104] average testing loss: 7822.90, base loss: 15827.02
[INFO 2017-06-29 04:36:51,638 main.py:105] improve_loss: 8004.11, improve_percent: 0.51
[INFO 2017-06-29 04:36:51,639 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:36:54,659 main.py:57] epoch 5200, training loss: 6813.28, average training loss: 7519.08, base loss: 16089.71
[INFO 2017-06-29 04:36:57,707 main.py:57] epoch 5201, training loss: 8011.41, average training loss: 7520.09, base loss: 16089.53
[INFO 2017-06-29 04:37:00,767 main.py:57] epoch 5202, training loss: 7713.79, average training loss: 7519.92, base loss: 16089.37
[INFO 2017-06-29 04:37:03,815 main.py:57] epoch 5203, training loss: 7253.30, average training loss: 7519.49, base loss: 16088.95
[INFO 2017-06-29 04:37:06,847 main.py:57] epoch 5204, training loss: 7764.96, average training loss: 7520.21, base loss: 16089.81
[INFO 2017-06-29 04:37:09,858 main.py:57] epoch 5205, training loss: 7525.45, average training loss: 7519.02, base loss: 16089.65
[INFO 2017-06-29 04:37:12,928 main.py:57] epoch 5206, training loss: 7330.12, average training loss: 7517.45, base loss: 16089.36
[INFO 2017-06-29 04:37:15,914 main.py:57] epoch 5207, training loss: 7992.68, average training loss: 7518.47, base loss: 16089.54
[INFO 2017-06-29 04:37:19,006 main.py:57] epoch 5208, training loss: 7617.59, average training loss: 7518.07, base loss: 16090.35
[INFO 2017-06-29 04:37:22,034 main.py:57] epoch 5209, training loss: 6973.34, average training loss: 7516.89, base loss: 16090.50
[INFO 2017-06-29 04:37:25,073 main.py:57] epoch 5210, training loss: 6909.60, average training loss: 7516.10, base loss: 16089.84
[INFO 2017-06-29 04:37:28,119 main.py:57] epoch 5211, training loss: 6750.37, average training loss: 7515.20, base loss: 16089.29
[INFO 2017-06-29 04:37:31,167 main.py:57] epoch 5212, training loss: 7338.41, average training loss: 7515.84, base loss: 16089.77
[INFO 2017-06-29 04:37:34,175 main.py:57] epoch 5213, training loss: 7505.61, average training loss: 7516.09, base loss: 16090.08
[INFO 2017-06-29 04:37:37,233 main.py:57] epoch 5214, training loss: 6622.71, average training loss: 7513.94, base loss: 16089.39
[INFO 2017-06-29 04:37:40,296 main.py:57] epoch 5215, training loss: 8188.82, average training loss: 7514.06, base loss: 16089.48
[INFO 2017-06-29 04:37:43,390 main.py:57] epoch 5216, training loss: 6767.18, average training loss: 7513.21, base loss: 16088.90
[INFO 2017-06-29 04:37:46,467 main.py:57] epoch 5217, training loss: 6484.03, average training loss: 7512.10, base loss: 16087.89
[INFO 2017-06-29 04:37:49,510 main.py:57] epoch 5218, training loss: 8495.63, average training loss: 7513.16, base loss: 16088.01
[INFO 2017-06-29 04:37:52,599 main.py:57] epoch 5219, training loss: 7560.36, average training loss: 7512.81, base loss: 16087.16
[INFO 2017-06-29 04:37:55,648 main.py:57] epoch 5220, training loss: 8173.74, average training loss: 7513.75, base loss: 16087.84
[INFO 2017-06-29 04:37:58,679 main.py:57] epoch 5221, training loss: 8129.97, average training loss: 7513.68, base loss: 16087.94
[INFO 2017-06-29 04:38:01,705 main.py:57] epoch 5222, training loss: 7299.87, average training loss: 7513.95, base loss: 16087.66
[INFO 2017-06-29 04:38:04,689 main.py:57] epoch 5223, training loss: 6444.69, average training loss: 7512.96, base loss: 16086.94
[INFO 2017-06-29 04:38:07,725 main.py:57] epoch 5224, training loss: 7491.29, average training loss: 7513.18, base loss: 16086.95
[INFO 2017-06-29 04:38:10,802 main.py:57] epoch 5225, training loss: 8549.83, average training loss: 7513.64, base loss: 16087.54
[INFO 2017-06-29 04:38:13,820 main.py:57] epoch 5226, training loss: 8154.50, average training loss: 7515.04, base loss: 16088.11
[INFO 2017-06-29 04:38:16,821 main.py:57] epoch 5227, training loss: 7131.96, average training loss: 7514.95, base loss: 16088.22
[INFO 2017-06-29 04:38:19,869 main.py:57] epoch 5228, training loss: 7150.61, average training loss: 7514.23, base loss: 16087.68
[INFO 2017-06-29 04:38:22,982 main.py:57] epoch 5229, training loss: 7852.63, average training loss: 7514.92, base loss: 16087.57
[INFO 2017-06-29 04:38:26,020 main.py:57] epoch 5230, training loss: 8016.29, average training loss: 7515.52, base loss: 16087.35
[INFO 2017-06-29 04:38:29,092 main.py:57] epoch 5231, training loss: 9235.67, average training loss: 7517.03, base loss: 16088.28
[INFO 2017-06-29 04:38:32,089 main.py:57] epoch 5232, training loss: 8018.12, average training loss: 7517.74, base loss: 16088.71
[INFO 2017-06-29 04:38:35,126 main.py:57] epoch 5233, training loss: 7179.40, average training loss: 7517.71, base loss: 16089.01
[INFO 2017-06-29 04:38:38,151 main.py:57] epoch 5234, training loss: 6891.32, average training loss: 7517.49, base loss: 16088.78
[INFO 2017-06-29 04:38:41,198 main.py:57] epoch 5235, training loss: 7468.35, average training loss: 7517.86, base loss: 16088.76
[INFO 2017-06-29 04:38:44,244 main.py:57] epoch 5236, training loss: 6613.34, average training loss: 7516.81, base loss: 16088.15
[INFO 2017-06-29 04:38:47,339 main.py:57] epoch 5237, training loss: 7260.45, average training loss: 7517.01, base loss: 16087.92
[INFO 2017-06-29 04:38:50,450 main.py:57] epoch 5238, training loss: 7106.52, average training loss: 7515.50, base loss: 16087.90
[INFO 2017-06-29 04:38:53,455 main.py:57] epoch 5239, training loss: 8385.67, average training loss: 7516.97, base loss: 16088.88
[INFO 2017-06-29 04:38:56,522 main.py:57] epoch 5240, training loss: 6821.20, average training loss: 7514.28, base loss: 16088.84
[INFO 2017-06-29 04:38:59,582 main.py:57] epoch 5241, training loss: 7102.62, average training loss: 7514.29, base loss: 16088.74
[INFO 2017-06-29 04:39:02,646 main.py:57] epoch 5242, training loss: 7116.32, average training loss: 7514.47, base loss: 16088.67
[INFO 2017-06-29 04:39:05,688 main.py:57] epoch 5243, training loss: 7064.85, average training loss: 7513.90, base loss: 16088.64
[INFO 2017-06-29 04:39:08,776 main.py:57] epoch 5244, training loss: 7002.54, average training loss: 7513.52, base loss: 16087.82
[INFO 2017-06-29 04:39:11,954 main.py:57] epoch 5245, training loss: 7330.84, average training loss: 7513.64, base loss: 16086.99
[INFO 2017-06-29 04:39:15,073 main.py:57] epoch 5246, training loss: 7101.27, average training loss: 7513.04, base loss: 16086.49
[INFO 2017-06-29 04:39:18,076 main.py:57] epoch 5247, training loss: 8037.00, average training loss: 7514.06, base loss: 16086.45
[INFO 2017-06-29 04:39:21,119 main.py:57] epoch 5248, training loss: 9215.20, average training loss: 7516.48, base loss: 16087.18
[INFO 2017-06-29 04:39:24,164 main.py:57] epoch 5249, training loss: 7134.47, average training loss: 7515.72, base loss: 16086.83
[INFO 2017-06-29 04:39:27,141 main.py:57] epoch 5250, training loss: 8621.55, average training loss: 7516.82, base loss: 16087.55
[INFO 2017-06-29 04:39:30,186 main.py:57] epoch 5251, training loss: 7361.63, average training loss: 7516.67, base loss: 16087.00
[INFO 2017-06-29 04:39:33,248 main.py:57] epoch 5252, training loss: 7475.11, average training loss: 7517.02, base loss: 16087.24
[INFO 2017-06-29 04:39:36,272 main.py:57] epoch 5253, training loss: 7756.04, average training loss: 7515.94, base loss: 16088.08
[INFO 2017-06-29 04:39:39,292 main.py:57] epoch 5254, training loss: 7610.54, average training loss: 7516.80, base loss: 16087.90
[INFO 2017-06-29 04:39:42,315 main.py:57] epoch 5255, training loss: 7444.83, average training loss: 7516.52, base loss: 16087.58
[INFO 2017-06-29 04:39:45,360 main.py:57] epoch 5256, training loss: 7828.86, average training loss: 7516.92, base loss: 16088.17
[INFO 2017-06-29 04:39:48,543 main.py:57] epoch 5257, training loss: 7722.40, average training loss: 7517.34, base loss: 16088.17
[INFO 2017-06-29 04:39:51,633 main.py:57] epoch 5258, training loss: 7339.89, average training loss: 7516.49, base loss: 16087.94
[INFO 2017-06-29 04:39:54,759 main.py:57] epoch 5259, training loss: 6885.41, average training loss: 7515.83, base loss: 16087.16
[INFO 2017-06-29 04:39:57,808 main.py:57] epoch 5260, training loss: 7120.90, average training loss: 7515.30, base loss: 16086.52
[INFO 2017-06-29 04:40:00,880 main.py:57] epoch 5261, training loss: 6813.45, average training loss: 7513.24, base loss: 16085.85
[INFO 2017-06-29 04:40:03,919 main.py:57] epoch 5262, training loss: 8287.09, average training loss: 7513.11, base loss: 16086.49
[INFO 2017-06-29 04:40:06,937 main.py:57] epoch 5263, training loss: 6742.81, average training loss: 7512.29, base loss: 16086.04
[INFO 2017-06-29 04:40:09,959 main.py:57] epoch 5264, training loss: 7132.54, average training loss: 7511.70, base loss: 16085.49
[INFO 2017-06-29 04:40:12,996 main.py:57] epoch 5265, training loss: 7186.85, average training loss: 7510.76, base loss: 16085.58
[INFO 2017-06-29 04:40:16,060 main.py:57] epoch 5266, training loss: 8642.78, average training loss: 7511.32, base loss: 16086.34
[INFO 2017-06-29 04:40:19,078 main.py:57] epoch 5267, training loss: 9187.33, average training loss: 7513.71, base loss: 16087.22
[INFO 2017-06-29 04:40:22,153 main.py:57] epoch 5268, training loss: 6572.72, average training loss: 7512.02, base loss: 16086.54
[INFO 2017-06-29 04:40:25,196 main.py:57] epoch 5269, training loss: 6965.53, average training loss: 7511.77, base loss: 16086.02
[INFO 2017-06-29 04:40:28,233 main.py:57] epoch 5270, training loss: 6647.21, average training loss: 7511.11, base loss: 16086.15
[INFO 2017-06-29 04:40:31,288 main.py:57] epoch 5271, training loss: 8110.44, average training loss: 7511.07, base loss: 16086.84
[INFO 2017-06-29 04:40:34,343 main.py:57] epoch 5272, training loss: 7992.51, average training loss: 7511.49, base loss: 16087.02
[INFO 2017-06-29 04:40:37,357 main.py:57] epoch 5273, training loss: 8305.34, average training loss: 7511.28, base loss: 16088.06
[INFO 2017-06-29 04:40:40,426 main.py:57] epoch 5274, training loss: 6506.91, average training loss: 7509.79, base loss: 16087.69
[INFO 2017-06-29 04:40:43,619 main.py:57] epoch 5275, training loss: 7552.51, average training loss: 7510.17, base loss: 16087.88
[INFO 2017-06-29 04:40:46,681 main.py:57] epoch 5276, training loss: 7418.26, average training loss: 7510.54, base loss: 16087.87
[INFO 2017-06-29 04:40:49,731 main.py:57] epoch 5277, training loss: 7949.06, average training loss: 7511.21, base loss: 16087.76
[INFO 2017-06-29 04:40:52,819 main.py:57] epoch 5278, training loss: 7845.79, average training loss: 7512.30, base loss: 16088.26
[INFO 2017-06-29 04:40:55,848 main.py:57] epoch 5279, training loss: 7942.04, average training loss: 7512.19, base loss: 16089.12
[INFO 2017-06-29 04:40:58,860 main.py:57] epoch 5280, training loss: 7397.08, average training loss: 7512.54, base loss: 16089.56
[INFO 2017-06-29 04:41:01,865 main.py:57] epoch 5281, training loss: 7138.10, average training loss: 7511.92, base loss: 16089.09
[INFO 2017-06-29 04:41:04,930 main.py:57] epoch 5282, training loss: 7300.64, average training loss: 7511.71, base loss: 16088.81
[INFO 2017-06-29 04:41:07,915 main.py:57] epoch 5283, training loss: 6759.50, average training loss: 7510.85, base loss: 16088.60
[INFO 2017-06-29 04:41:10,939 main.py:57] epoch 5284, training loss: 7611.76, average training loss: 7510.51, base loss: 16089.52
[INFO 2017-06-29 04:41:13,989 main.py:57] epoch 5285, training loss: 6592.49, average training loss: 7510.15, base loss: 16089.23
[INFO 2017-06-29 04:41:17,007 main.py:57] epoch 5286, training loss: 8149.04, average training loss: 7510.57, base loss: 16090.48
[INFO 2017-06-29 04:41:20,023 main.py:57] epoch 5287, training loss: 9634.94, average training loss: 7513.21, base loss: 16092.43
[INFO 2017-06-29 04:41:23,075 main.py:57] epoch 5288, training loss: 7532.98, average training loss: 7514.07, base loss: 16092.53
[INFO 2017-06-29 04:41:26,057 main.py:57] epoch 5289, training loss: 7233.15, average training loss: 7513.81, base loss: 16092.58
[INFO 2017-06-29 04:41:29,098 main.py:57] epoch 5290, training loss: 7158.86, average training loss: 7512.61, base loss: 16091.93
[INFO 2017-06-29 04:41:32,127 main.py:57] epoch 5291, training loss: 6988.83, average training loss: 7512.48, base loss: 16091.45
[INFO 2017-06-29 04:41:35,131 main.py:57] epoch 5292, training loss: 7911.22, average training loss: 7512.82, base loss: 16092.35
[INFO 2017-06-29 04:41:38,189 main.py:57] epoch 5293, training loss: 7251.37, average training loss: 7511.20, base loss: 16092.34
[INFO 2017-06-29 04:41:41,215 main.py:57] epoch 5294, training loss: 7622.98, average training loss: 7511.75, base loss: 16091.94
[INFO 2017-06-29 04:41:44,294 main.py:57] epoch 5295, training loss: 6987.21, average training loss: 7511.53, base loss: 16091.31
[INFO 2017-06-29 04:41:47,306 main.py:57] epoch 5296, training loss: 6516.95, average training loss: 7509.92, base loss: 16090.16
[INFO 2017-06-29 04:41:50,355 main.py:57] epoch 5297, training loss: 7419.97, average training loss: 7510.43, base loss: 16090.20
[INFO 2017-06-29 04:41:53,428 main.py:57] epoch 5298, training loss: 6960.70, average training loss: 7509.33, base loss: 16089.67
[INFO 2017-06-29 04:41:56,478 main.py:57] epoch 5299, training loss: 6967.67, average training loss: 7509.49, base loss: 16089.29
[INFO 2017-06-29 04:41:56,478 main.py:59] epoch 5299, testing
[INFO 2017-06-29 04:42:09,084 main.py:104] average testing loss: 8022.94, base loss: 16664.97
[INFO 2017-06-29 04:42:09,084 main.py:105] improve_loss: 8642.03, improve_percent: 0.52
[INFO 2017-06-29 04:42:09,085 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:42:12,178 main.py:57] epoch 5300, training loss: 6889.75, average training loss: 7509.57, base loss: 16088.68
[INFO 2017-06-29 04:42:15,176 main.py:57] epoch 5301, training loss: 7895.10, average training loss: 7509.55, base loss: 16088.76
[INFO 2017-06-29 04:42:18,202 main.py:57] epoch 5302, training loss: 8394.00, average training loss: 7510.78, base loss: 16088.67
[INFO 2017-06-29 04:42:21,263 main.py:57] epoch 5303, training loss: 7691.59, average training loss: 7511.90, base loss: 16088.84
[INFO 2017-06-29 04:42:24,351 main.py:57] epoch 5304, training loss: 7800.64, average training loss: 7512.08, base loss: 16088.66
[INFO 2017-06-29 04:42:27,404 main.py:57] epoch 5305, training loss: 6595.13, average training loss: 7509.88, base loss: 16088.37
[INFO 2017-06-29 04:42:30,420 main.py:57] epoch 5306, training loss: 7126.33, average training loss: 7508.57, base loss: 16087.66
[INFO 2017-06-29 04:42:33,423 main.py:57] epoch 5307, training loss: 7485.12, average training loss: 7508.68, base loss: 16087.41
[INFO 2017-06-29 04:42:36,429 main.py:57] epoch 5308, training loss: 7408.84, average training loss: 7509.18, base loss: 16087.64
[INFO 2017-06-29 04:42:39,595 main.py:57] epoch 5309, training loss: 7727.90, average training loss: 7508.85, base loss: 16087.24
[INFO 2017-06-29 04:42:42,610 main.py:57] epoch 5310, training loss: 8086.09, average training loss: 7509.62, base loss: 16087.78
[INFO 2017-06-29 04:42:45,599 main.py:57] epoch 5311, training loss: 6583.31, average training loss: 7508.14, base loss: 16087.55
[INFO 2017-06-29 04:42:48,628 main.py:57] epoch 5312, training loss: 7943.14, average training loss: 7508.93, base loss: 16088.10
[INFO 2017-06-29 04:42:51,756 main.py:57] epoch 5313, training loss: 6794.67, average training loss: 7508.12, base loss: 16087.56
[INFO 2017-06-29 04:42:54,798 main.py:57] epoch 5314, training loss: 7330.14, average training loss: 7507.64, base loss: 16087.45
[INFO 2017-06-29 04:42:57,876 main.py:57] epoch 5315, training loss: 7814.86, average training loss: 7508.94, base loss: 16087.88
[INFO 2017-06-29 04:43:00,872 main.py:57] epoch 5316, training loss: 7678.47, average training loss: 7509.37, base loss: 16087.64
[INFO 2017-06-29 04:43:03,923 main.py:57] epoch 5317, training loss: 7038.64, average training loss: 7508.91, base loss: 16087.29
[INFO 2017-06-29 04:43:06,943 main.py:57] epoch 5318, training loss: 7324.63, average training loss: 7508.48, base loss: 16087.10
[INFO 2017-06-29 04:43:09,993 main.py:57] epoch 5319, training loss: 7383.47, average training loss: 7507.67, base loss: 16087.23
[INFO 2017-06-29 04:43:13,068 main.py:57] epoch 5320, training loss: 7734.64, average training loss: 7507.24, base loss: 16087.75
[INFO 2017-06-29 04:43:16,108 main.py:57] epoch 5321, training loss: 8073.90, average training loss: 7508.38, base loss: 16088.17
[INFO 2017-06-29 04:43:19,165 main.py:57] epoch 5322, training loss: 7345.80, average training loss: 7508.80, base loss: 16088.44
[INFO 2017-06-29 04:43:22,184 main.py:57] epoch 5323, training loss: 7370.18, average training loss: 7508.47, base loss: 16088.38
[INFO 2017-06-29 04:43:25,340 main.py:57] epoch 5324, training loss: 7821.68, average training loss: 7508.32, base loss: 16088.30
[INFO 2017-06-29 04:43:28,360 main.py:57] epoch 5325, training loss: 7942.94, average training loss: 7508.85, base loss: 16088.42
[INFO 2017-06-29 04:43:31,397 main.py:57] epoch 5326, training loss: 7068.11, average training loss: 7508.32, base loss: 16088.23
[INFO 2017-06-29 04:43:34,395 main.py:57] epoch 5327, training loss: 7777.84, average training loss: 7508.38, base loss: 16088.11
[INFO 2017-06-29 04:43:37,458 main.py:57] epoch 5328, training loss: 7841.43, average training loss: 7509.55, base loss: 16088.70
[INFO 2017-06-29 04:43:40,482 main.py:57] epoch 5329, training loss: 6691.36, average training loss: 7507.85, base loss: 16088.12
[INFO 2017-06-29 04:43:43,542 main.py:57] epoch 5330, training loss: 7375.38, average training loss: 7506.45, base loss: 16087.82
[INFO 2017-06-29 04:43:46,600 main.py:57] epoch 5331, training loss: 7118.97, average training loss: 7505.99, base loss: 16088.20
[INFO 2017-06-29 04:43:49,668 main.py:57] epoch 5332, training loss: 7849.70, average training loss: 7506.54, base loss: 16088.22
[INFO 2017-06-29 04:43:52,757 main.py:57] epoch 5333, training loss: 7316.93, average training loss: 7506.42, base loss: 16087.72
[INFO 2017-06-29 04:43:55,863 main.py:57] epoch 5334, training loss: 7652.91, average training loss: 7505.58, base loss: 16088.53
[INFO 2017-06-29 04:43:58,906 main.py:57] epoch 5335, training loss: 7737.19, average training loss: 7506.38, base loss: 16089.15
[INFO 2017-06-29 04:44:01,946 main.py:57] epoch 5336, training loss: 7832.12, average training loss: 7505.48, base loss: 16089.08
[INFO 2017-06-29 04:44:05,017 main.py:57] epoch 5337, training loss: 7096.75, average training loss: 7504.89, base loss: 16089.02
[INFO 2017-06-29 04:44:08,078 main.py:57] epoch 5338, training loss: 8196.72, average training loss: 7505.29, base loss: 16089.31
[INFO 2017-06-29 04:44:11,180 main.py:57] epoch 5339, training loss: 7209.66, average training loss: 7505.84, base loss: 16089.61
[INFO 2017-06-29 04:44:14,220 main.py:57] epoch 5340, training loss: 8415.76, average training loss: 7507.65, base loss: 16090.41
[INFO 2017-06-29 04:44:17,255 main.py:57] epoch 5341, training loss: 7212.54, average training loss: 7507.51, base loss: 16089.67
[INFO 2017-06-29 04:44:20,334 main.py:57] epoch 5342, training loss: 7792.80, average training loss: 7507.34, base loss: 16090.19
[INFO 2017-06-29 04:44:23,395 main.py:57] epoch 5343, training loss: 7755.87, average training loss: 7506.91, base loss: 16090.75
[INFO 2017-06-29 04:44:26,424 main.py:57] epoch 5344, training loss: 7210.98, average training loss: 7505.94, base loss: 16090.01
[INFO 2017-06-29 04:44:29,556 main.py:57] epoch 5345, training loss: 7042.68, average training loss: 7505.00, base loss: 16089.76
[INFO 2017-06-29 04:44:32,597 main.py:57] epoch 5346, training loss: 8427.35, average training loss: 7506.36, base loss: 16090.01
[INFO 2017-06-29 04:44:35,612 main.py:57] epoch 5347, training loss: 7281.50, average training loss: 7505.87, base loss: 16089.60
[INFO 2017-06-29 04:44:38,666 main.py:57] epoch 5348, training loss: 7869.71, average training loss: 7506.10, base loss: 16089.87
[INFO 2017-06-29 04:44:41,763 main.py:57] epoch 5349, training loss: 8603.99, average training loss: 7507.62, base loss: 16090.79
[INFO 2017-06-29 04:44:44,799 main.py:57] epoch 5350, training loss: 7405.20, average training loss: 7507.27, base loss: 16090.50
[INFO 2017-06-29 04:44:47,865 main.py:57] epoch 5351, training loss: 9175.30, average training loss: 7508.83, base loss: 16091.63
[INFO 2017-06-29 04:44:50,928 main.py:57] epoch 5352, training loss: 6782.93, average training loss: 7508.70, base loss: 16091.27
[INFO 2017-06-29 04:44:53,968 main.py:57] epoch 5353, training loss: 7750.51, average training loss: 7508.26, base loss: 16091.55
[INFO 2017-06-29 04:44:56,972 main.py:57] epoch 5354, training loss: 8046.86, average training loss: 7508.83, base loss: 16092.57
[INFO 2017-06-29 04:45:00,008 main.py:57] epoch 5355, training loss: 6993.91, average training loss: 7507.04, base loss: 16092.56
[INFO 2017-06-29 04:45:03,072 main.py:57] epoch 5356, training loss: 7951.61, average training loss: 7507.06, base loss: 16092.84
[INFO 2017-06-29 04:45:06,089 main.py:57] epoch 5357, training loss: 7729.46, average training loss: 7507.91, base loss: 16093.18
[INFO 2017-06-29 04:45:09,126 main.py:57] epoch 5358, training loss: 8015.93, average training loss: 7508.16, base loss: 16092.97
[INFO 2017-06-29 04:45:12,194 main.py:57] epoch 5359, training loss: 7757.97, average training loss: 7508.73, base loss: 16092.72
[INFO 2017-06-29 04:45:15,236 main.py:57] epoch 5360, training loss: 7258.83, average training loss: 7507.21, base loss: 16092.81
[INFO 2017-06-29 04:45:18,286 main.py:57] epoch 5361, training loss: 9414.43, average training loss: 7508.46, base loss: 16094.03
[INFO 2017-06-29 04:45:21,323 main.py:57] epoch 5362, training loss: 6585.46, average training loss: 7506.17, base loss: 16093.46
[INFO 2017-06-29 04:45:24,377 main.py:57] epoch 5363, training loss: 6536.40, average training loss: 7505.38, base loss: 16093.39
[INFO 2017-06-29 04:45:27,406 main.py:57] epoch 5364, training loss: 7608.91, average training loss: 7505.03, base loss: 16093.33
[INFO 2017-06-29 04:45:30,398 main.py:57] epoch 5365, training loss: 6757.66, average training loss: 7504.85, base loss: 16093.27
[INFO 2017-06-29 04:45:33,550 main.py:57] epoch 5366, training loss: 8341.57, average training loss: 7504.74, base loss: 16094.57
[INFO 2017-06-29 04:45:36,643 main.py:57] epoch 5367, training loss: 7537.50, average training loss: 7504.80, base loss: 16094.57
[INFO 2017-06-29 04:45:39,735 main.py:57] epoch 5368, training loss: 8008.20, average training loss: 7505.93, base loss: 16094.85
[INFO 2017-06-29 04:45:42,793 main.py:57] epoch 5369, training loss: 7379.53, average training loss: 7505.65, base loss: 16095.13
[INFO 2017-06-29 04:45:45,948 main.py:57] epoch 5370, training loss: 7992.58, average training loss: 7505.97, base loss: 16095.05
[INFO 2017-06-29 04:45:48,958 main.py:57] epoch 5371, training loss: 7377.01, average training loss: 7505.85, base loss: 16095.01
[INFO 2017-06-29 04:45:51,965 main.py:57] epoch 5372, training loss: 7291.37, average training loss: 7506.45, base loss: 16094.89
[INFO 2017-06-29 04:45:55,044 main.py:57] epoch 5373, training loss: 7080.41, average training loss: 7506.11, base loss: 16094.69
[INFO 2017-06-29 04:45:58,078 main.py:57] epoch 5374, training loss: 6489.29, average training loss: 7504.02, base loss: 16094.02
[INFO 2017-06-29 04:46:01,130 main.py:57] epoch 5375, training loss: 7401.23, average training loss: 7504.53, base loss: 16093.98
[INFO 2017-06-29 04:46:04,189 main.py:57] epoch 5376, training loss: 7435.60, average training loss: 7504.10, base loss: 16094.20
[INFO 2017-06-29 04:46:07,278 main.py:57] epoch 5377, training loss: 8145.53, average training loss: 7504.81, base loss: 16094.76
[INFO 2017-06-29 04:46:10,320 main.py:57] epoch 5378, training loss: 7897.67, average training loss: 7505.19, base loss: 16094.95
[INFO 2017-06-29 04:46:13,476 main.py:57] epoch 5379, training loss: 6827.35, average training loss: 7504.71, base loss: 16094.65
[INFO 2017-06-29 04:46:16,494 main.py:57] epoch 5380, training loss: 8088.26, average training loss: 7504.79, base loss: 16095.27
[INFO 2017-06-29 04:46:19,507 main.py:57] epoch 5381, training loss: 7148.74, average training loss: 7504.43, base loss: 16095.29
[INFO 2017-06-29 04:46:22,536 main.py:57] epoch 5382, training loss: 8162.86, average training loss: 7504.54, base loss: 16095.39
[INFO 2017-06-29 04:46:25,565 main.py:57] epoch 5383, training loss: 8075.58, average training loss: 7505.85, base loss: 16095.51
[INFO 2017-06-29 04:46:28,597 main.py:57] epoch 5384, training loss: 7057.61, average training loss: 7504.31, base loss: 16095.32
[INFO 2017-06-29 04:46:31,604 main.py:57] epoch 5385, training loss: 8091.75, average training loss: 7505.12, base loss: 16095.61
[INFO 2017-06-29 04:46:34,592 main.py:57] epoch 5386, training loss: 7279.83, average training loss: 7503.98, base loss: 16095.48
[INFO 2017-06-29 04:46:37,658 main.py:57] epoch 5387, training loss: 7723.36, average training loss: 7504.42, base loss: 16095.86
[INFO 2017-06-29 04:46:40,688 main.py:57] epoch 5388, training loss: 7023.93, average training loss: 7504.56, base loss: 16095.65
[INFO 2017-06-29 04:46:43,818 main.py:57] epoch 5389, training loss: 7035.87, average training loss: 7504.45, base loss: 16094.92
[INFO 2017-06-29 04:46:46,837 main.py:57] epoch 5390, training loss: 6804.01, average training loss: 7504.48, base loss: 16093.97
[INFO 2017-06-29 04:46:49,840 main.py:57] epoch 5391, training loss: 7701.72, average training loss: 7504.94, base loss: 16093.93
[INFO 2017-06-29 04:46:52,922 main.py:57] epoch 5392, training loss: 7641.27, average training loss: 7506.07, base loss: 16094.21
[INFO 2017-06-29 04:46:55,939 main.py:57] epoch 5393, training loss: 7901.83, average training loss: 7505.36, base loss: 16094.40
[INFO 2017-06-29 04:46:58,974 main.py:57] epoch 5394, training loss: 7450.62, average training loss: 7506.38, base loss: 16094.14
[INFO 2017-06-29 04:47:02,062 main.py:57] epoch 5395, training loss: 9180.26, average training loss: 7509.70, base loss: 16095.47
[INFO 2017-06-29 04:47:05,091 main.py:57] epoch 5396, training loss: 7701.20, average training loss: 7510.23, base loss: 16095.67
[INFO 2017-06-29 04:47:08,162 main.py:57] epoch 5397, training loss: 7360.32, average training loss: 7509.83, base loss: 16095.70
[INFO 2017-06-29 04:47:11,204 main.py:57] epoch 5398, training loss: 6910.03, average training loss: 7509.90, base loss: 16095.40
[INFO 2017-06-29 04:47:14,317 main.py:57] epoch 5399, training loss: 7467.18, average training loss: 7510.40, base loss: 16095.81
[INFO 2017-06-29 04:47:14,318 main.py:59] epoch 5399, testing
[INFO 2017-06-29 04:47:26,946 main.py:104] average testing loss: 8211.35, base loss: 16743.52
[INFO 2017-06-29 04:47:26,946 main.py:105] improve_loss: 8532.17, improve_percent: 0.51
[INFO 2017-06-29 04:47:26,947 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:47:30,019 main.py:57] epoch 5400, training loss: 7973.43, average training loss: 7511.22, base loss: 16096.55
[INFO 2017-06-29 04:47:33,016 main.py:57] epoch 5401, training loss: 6627.29, average training loss: 7510.68, base loss: 16096.03
[INFO 2017-06-29 04:47:36,108 main.py:57] epoch 5402, training loss: 8239.86, average training loss: 7511.90, base loss: 16096.73
[INFO 2017-06-29 04:47:39,207 main.py:57] epoch 5403, training loss: 6707.62, average training loss: 7509.94, base loss: 16096.39
[INFO 2017-06-29 04:47:42,294 main.py:57] epoch 5404, training loss: 6614.19, average training loss: 7508.06, base loss: 16095.54
[INFO 2017-06-29 04:47:45,352 main.py:57] epoch 5405, training loss: 7644.38, average training loss: 7508.38, base loss: 16095.43
[INFO 2017-06-29 04:47:48,450 main.py:57] epoch 5406, training loss: 7696.29, average training loss: 7508.17, base loss: 16095.67
[INFO 2017-06-29 04:47:51,489 main.py:57] epoch 5407, training loss: 6755.91, average training loss: 7506.59, base loss: 16095.31
[INFO 2017-06-29 04:47:54,564 main.py:57] epoch 5408, training loss: 7577.77, average training loss: 7507.65, base loss: 16095.06
[INFO 2017-06-29 04:47:57,612 main.py:57] epoch 5409, training loss: 8012.75, average training loss: 7508.26, base loss: 16095.35
[INFO 2017-06-29 04:48:00,736 main.py:57] epoch 5410, training loss: 7259.22, average training loss: 7508.53, base loss: 16095.13
[INFO 2017-06-29 04:48:03,808 main.py:57] epoch 5411, training loss: 7221.02, average training loss: 7507.66, base loss: 16094.96
[INFO 2017-06-29 04:48:06,888 main.py:57] epoch 5412, training loss: 7623.54, average training loss: 7507.48, base loss: 16095.21
[INFO 2017-06-29 04:48:09,938 main.py:57] epoch 5413, training loss: 7256.41, average training loss: 7507.82, base loss: 16094.74
[INFO 2017-06-29 04:48:13,035 main.py:57] epoch 5414, training loss: 7306.28, average training loss: 7507.92, base loss: 16094.62
[INFO 2017-06-29 04:48:16,063 main.py:57] epoch 5415, training loss: 7497.86, average training loss: 7507.41, base loss: 16095.48
[INFO 2017-06-29 04:48:19,055 main.py:57] epoch 5416, training loss: 7634.97, average training loss: 7507.01, base loss: 16095.52
[INFO 2017-06-29 04:48:22,067 main.py:57] epoch 5417, training loss: 7222.59, average training loss: 7507.53, base loss: 16094.76
[INFO 2017-06-29 04:48:25,120 main.py:57] epoch 5418, training loss: 7301.43, average training loss: 7506.90, base loss: 16094.39
[INFO 2017-06-29 04:48:28,162 main.py:57] epoch 5419, training loss: 7289.75, average training loss: 7506.84, base loss: 16094.27
[INFO 2017-06-29 04:48:31,215 main.py:57] epoch 5420, training loss: 7818.87, average training loss: 7506.39, base loss: 16094.28
[INFO 2017-06-29 04:48:34,282 main.py:57] epoch 5421, training loss: 7693.91, average training loss: 7506.25, base loss: 16094.39
[INFO 2017-06-29 04:48:37,359 main.py:57] epoch 5422, training loss: 8721.22, average training loss: 7507.39, base loss: 16095.12
[INFO 2017-06-29 04:48:40,432 main.py:57] epoch 5423, training loss: 8186.57, average training loss: 7508.43, base loss: 16095.82
[INFO 2017-06-29 04:48:43,513 main.py:57] epoch 5424, training loss: 7332.89, average training loss: 7508.61, base loss: 16096.14
[INFO 2017-06-29 04:48:46,618 main.py:57] epoch 5425, training loss: 7612.40, average training loss: 7508.52, base loss: 16095.96
[INFO 2017-06-29 04:48:49,676 main.py:57] epoch 5426, training loss: 7202.48, average training loss: 7508.44, base loss: 16095.39
[INFO 2017-06-29 04:48:52,698 main.py:57] epoch 5427, training loss: 7884.49, average training loss: 7508.25, base loss: 16095.25
[INFO 2017-06-29 04:48:55,785 main.py:57] epoch 5428, training loss: 6555.69, average training loss: 7506.96, base loss: 16094.75
[INFO 2017-06-29 04:48:58,829 main.py:57] epoch 5429, training loss: 9148.74, average training loss: 7508.76, base loss: 16095.93
[INFO 2017-06-29 04:49:01,921 main.py:57] epoch 5430, training loss: 7452.18, average training loss: 7509.57, base loss: 16097.00
[INFO 2017-06-29 04:49:04,976 main.py:57] epoch 5431, training loss: 6140.52, average training loss: 7508.43, base loss: 16096.35
[INFO 2017-06-29 04:49:07,965 main.py:57] epoch 5432, training loss: 9956.72, average training loss: 7510.61, base loss: 16097.55
[INFO 2017-06-29 04:49:11,004 main.py:57] epoch 5433, training loss: 6697.45, average training loss: 7508.40, base loss: 16097.12
[INFO 2017-06-29 04:49:14,023 main.py:57] epoch 5434, training loss: 7540.67, average training loss: 7508.27, base loss: 16097.70
[INFO 2017-06-29 04:49:17,065 main.py:57] epoch 5435, training loss: 7731.27, average training loss: 7508.79, base loss: 16098.46
[INFO 2017-06-29 04:49:20,132 main.py:57] epoch 5436, training loss: 7576.23, average training loss: 7508.39, base loss: 16097.84
[INFO 2017-06-29 04:49:23,150 main.py:57] epoch 5437, training loss: 8063.02, average training loss: 7509.19, base loss: 16097.50
[INFO 2017-06-29 04:49:26,173 main.py:57] epoch 5438, training loss: 7950.81, average training loss: 7510.28, base loss: 16098.14
[INFO 2017-06-29 04:49:29,201 main.py:57] epoch 5439, training loss: 6916.54, average training loss: 7509.73, base loss: 16097.85
[INFO 2017-06-29 04:49:32,278 main.py:57] epoch 5440, training loss: 7886.33, average training loss: 7509.60, base loss: 16098.49
[INFO 2017-06-29 04:49:35,356 main.py:57] epoch 5441, training loss: 8438.31, average training loss: 7510.86, base loss: 16099.48
[INFO 2017-06-29 04:49:38,403 main.py:57] epoch 5442, training loss: 6944.75, average training loss: 7509.14, base loss: 16099.10
[INFO 2017-06-29 04:49:41,486 main.py:57] epoch 5443, training loss: 7474.51, average training loss: 7509.91, base loss: 16099.39
[INFO 2017-06-29 04:49:44,473 main.py:57] epoch 5444, training loss: 8408.77, average training loss: 7509.97, base loss: 16100.02
[INFO 2017-06-29 04:49:47,499 main.py:57] epoch 5445, training loss: 8152.68, average training loss: 7510.58, base loss: 16100.50
[INFO 2017-06-29 04:49:50,479 main.py:57] epoch 5446, training loss: 6796.23, average training loss: 7509.69, base loss: 16099.87
[INFO 2017-06-29 04:49:53,500 main.py:57] epoch 5447, training loss: 6852.83, average training loss: 7509.02, base loss: 16099.63
[INFO 2017-06-29 04:49:56,509 main.py:57] epoch 5448, training loss: 6590.29, average training loss: 7507.37, base loss: 16099.09
[INFO 2017-06-29 04:49:59,535 main.py:57] epoch 5449, training loss: 8319.74, average training loss: 7507.28, base loss: 16099.77
[INFO 2017-06-29 04:50:02,599 main.py:57] epoch 5450, training loss: 7678.58, average training loss: 7508.12, base loss: 16099.68
[INFO 2017-06-29 04:50:05,704 main.py:57] epoch 5451, training loss: 8096.60, average training loss: 7508.13, base loss: 16100.96
[INFO 2017-06-29 04:50:08,724 main.py:57] epoch 5452, training loss: 8674.43, average training loss: 7510.18, base loss: 16101.21
[INFO 2017-06-29 04:50:11,944 main.py:57] epoch 5453, training loss: 7096.48, average training loss: 7510.01, base loss: 16100.97
[INFO 2017-06-29 04:50:14,970 main.py:57] epoch 5454, training loss: 6625.73, average training loss: 7509.50, base loss: 16100.41
[INFO 2017-06-29 04:50:17,995 main.py:57] epoch 5455, training loss: 6466.25, average training loss: 7508.24, base loss: 16099.97
[INFO 2017-06-29 04:50:21,066 main.py:57] epoch 5456, training loss: 7580.28, average training loss: 7507.79, base loss: 16099.83
[INFO 2017-06-29 04:50:24,071 main.py:57] epoch 5457, training loss: 7931.72, average training loss: 7508.25, base loss: 16100.26
[INFO 2017-06-29 04:50:27,149 main.py:57] epoch 5458, training loss: 8490.16, average training loss: 7510.38, base loss: 16100.33
[INFO 2017-06-29 04:50:30,259 main.py:57] epoch 5459, training loss: 7091.36, average training loss: 7509.88, base loss: 16100.12
[INFO 2017-06-29 04:50:33,331 main.py:57] epoch 5460, training loss: 6523.62, average training loss: 7508.93, base loss: 16099.68
[INFO 2017-06-29 04:50:36,374 main.py:57] epoch 5461, training loss: 6653.28, average training loss: 7508.19, base loss: 16099.33
[INFO 2017-06-29 04:50:39,460 main.py:57] epoch 5462, training loss: 7560.65, average training loss: 7508.56, base loss: 16098.91
[INFO 2017-06-29 04:50:42,563 main.py:57] epoch 5463, training loss: 7297.34, average training loss: 7508.90, base loss: 16098.73
[INFO 2017-06-29 04:50:45,591 main.py:57] epoch 5464, training loss: 7788.20, average training loss: 7508.37, base loss: 16098.32
[INFO 2017-06-29 04:50:48,670 main.py:57] epoch 5465, training loss: 6999.61, average training loss: 7507.56, base loss: 16097.65
[INFO 2017-06-29 04:50:51,684 main.py:57] epoch 5466, training loss: 6394.26, average training loss: 7506.34, base loss: 16097.27
[INFO 2017-06-29 04:50:54,748 main.py:57] epoch 5467, training loss: 7092.92, average training loss: 7505.28, base loss: 16097.44
[INFO 2017-06-29 04:50:57,751 main.py:57] epoch 5468, training loss: 7395.96, average training loss: 7504.73, base loss: 16097.74
[INFO 2017-06-29 04:51:00,744 main.py:57] epoch 5469, training loss: 8647.52, average training loss: 7505.51, base loss: 16098.51
[INFO 2017-06-29 04:51:03,755 main.py:57] epoch 5470, training loss: 6770.85, average training loss: 7504.81, base loss: 16098.55
[INFO 2017-06-29 04:51:06,859 main.py:57] epoch 5471, training loss: 7872.51, average training loss: 7505.30, base loss: 16098.72
[INFO 2017-06-29 04:51:09,845 main.py:57] epoch 5472, training loss: 7375.88, average training loss: 7506.35, base loss: 16099.43
[INFO 2017-06-29 04:51:12,886 main.py:57] epoch 5473, training loss: 7688.02, average training loss: 7505.75, base loss: 16100.25
[INFO 2017-06-29 04:51:15,895 main.py:57] epoch 5474, training loss: 8077.15, average training loss: 7506.40, base loss: 16100.97
[INFO 2017-06-29 04:51:18,944 main.py:57] epoch 5475, training loss: 8237.19, average training loss: 7507.40, base loss: 16101.69
[INFO 2017-06-29 04:51:22,038 main.py:57] epoch 5476, training loss: 6575.04, average training loss: 7507.37, base loss: 16101.04
[INFO 2017-06-29 04:51:25,065 main.py:57] epoch 5477, training loss: 6711.07, average training loss: 7506.75, base loss: 16100.90
[INFO 2017-06-29 04:51:28,134 main.py:57] epoch 5478, training loss: 7762.95, average training loss: 7507.56, base loss: 16101.74
[INFO 2017-06-29 04:51:31,201 main.py:57] epoch 5479, training loss: 8191.41, average training loss: 7508.02, base loss: 16102.35
[INFO 2017-06-29 04:51:34,314 main.py:57] epoch 5480, training loss: 7033.68, average training loss: 7507.16, base loss: 16102.09
[INFO 2017-06-29 04:51:37,338 main.py:57] epoch 5481, training loss: 7167.67, average training loss: 7507.12, base loss: 16102.21
[INFO 2017-06-29 04:51:40,332 main.py:57] epoch 5482, training loss: 8431.69, average training loss: 7507.95, base loss: 16102.73
[INFO 2017-06-29 04:51:43,412 main.py:57] epoch 5483, training loss: 6420.35, average training loss: 7507.58, base loss: 16102.24
[INFO 2017-06-29 04:51:46,402 main.py:57] epoch 5484, training loss: 7191.44, average training loss: 7506.56, base loss: 16102.63
[INFO 2017-06-29 04:51:49,395 main.py:57] epoch 5485, training loss: 7694.21, average training loss: 7507.44, base loss: 16102.49
[INFO 2017-06-29 04:51:52,431 main.py:57] epoch 5486, training loss: 6630.27, average training loss: 7506.21, base loss: 16101.76
[INFO 2017-06-29 04:51:55,494 main.py:57] epoch 5487, training loss: 7326.00, average training loss: 7506.48, base loss: 16101.84
[INFO 2017-06-29 04:51:58,540 main.py:57] epoch 5488, training loss: 7207.38, average training loss: 7506.85, base loss: 16101.85
[INFO 2017-06-29 04:52:01,536 main.py:57] epoch 5489, training loss: 7482.75, average training loss: 7507.32, base loss: 16101.40
[INFO 2017-06-29 04:52:04,597 main.py:57] epoch 5490, training loss: 6683.15, average training loss: 7507.15, base loss: 16100.78
[INFO 2017-06-29 04:52:07,614 main.py:57] epoch 5491, training loss: 7270.13, average training loss: 7507.63, base loss: 16100.62
[INFO 2017-06-29 04:52:10,625 main.py:57] epoch 5492, training loss: 7534.41, average training loss: 7507.47, base loss: 16100.84
[INFO 2017-06-29 04:52:13,699 main.py:57] epoch 5493, training loss: 6961.40, average training loss: 7506.34, base loss: 16100.55
[INFO 2017-06-29 04:52:16,808 main.py:57] epoch 5494, training loss: 8171.74, average training loss: 7507.20, base loss: 16101.49
[INFO 2017-06-29 04:52:19,849 main.py:57] epoch 5495, training loss: 7084.94, average training loss: 7507.23, base loss: 16101.27
[INFO 2017-06-29 04:52:22,900 main.py:57] epoch 5496, training loss: 7794.31, average training loss: 7506.08, base loss: 16101.31
[INFO 2017-06-29 04:52:26,015 main.py:57] epoch 5497, training loss: 7482.87, average training loss: 7506.43, base loss: 16101.02
[INFO 2017-06-29 04:52:29,088 main.py:57] epoch 5498, training loss: 6591.49, average training loss: 7506.64, base loss: 16100.49
[INFO 2017-06-29 04:52:32,084 main.py:57] epoch 5499, training loss: 7417.56, average training loss: 7506.39, base loss: 16100.62
[INFO 2017-06-29 04:52:32,085 main.py:59] epoch 5499, testing
[INFO 2017-06-29 04:52:44,754 main.py:104] average testing loss: 7833.16, base loss: 15957.72
[INFO 2017-06-29 04:52:44,754 main.py:105] improve_loss: 8124.56, improve_percent: 0.51
[INFO 2017-06-29 04:52:44,755 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:52:47,795 main.py:57] epoch 5500, training loss: 7512.39, average training loss: 7506.28, base loss: 16100.43
[INFO 2017-06-29 04:52:50,882 main.py:57] epoch 5501, training loss: 6452.84, average training loss: 7504.76, base loss: 16099.73
[INFO 2017-06-29 04:52:53,977 main.py:57] epoch 5502, training loss: 7687.66, average training loss: 7504.79, base loss: 16100.48
[INFO 2017-06-29 04:52:57,083 main.py:57] epoch 5503, training loss: 7569.98, average training loss: 7505.28, base loss: 16100.79
[INFO 2017-06-29 04:53:00,116 main.py:57] epoch 5504, training loss: 7168.77, average training loss: 7505.24, base loss: 16100.40
[INFO 2017-06-29 04:53:03,137 main.py:57] epoch 5505, training loss: 7721.65, average training loss: 7504.89, base loss: 16100.24
[INFO 2017-06-29 04:53:06,180 main.py:57] epoch 5506, training loss: 7132.74, average training loss: 7505.28, base loss: 16100.32
[INFO 2017-06-29 04:53:09,199 main.py:57] epoch 5507, training loss: 6985.54, average training loss: 7504.67, base loss: 16100.55
[INFO 2017-06-29 04:53:12,231 main.py:57] epoch 5508, training loss: 7944.04, average training loss: 7504.99, base loss: 16101.08
[INFO 2017-06-29 04:53:15,297 main.py:57] epoch 5509, training loss: 6751.36, average training loss: 7504.41, base loss: 16100.54
[INFO 2017-06-29 04:53:18,306 main.py:57] epoch 5510, training loss: 8109.84, average training loss: 7505.22, base loss: 16100.59
[INFO 2017-06-29 04:53:21,387 main.py:57] epoch 5511, training loss: 7720.15, average training loss: 7505.75, base loss: 16100.61
[INFO 2017-06-29 04:53:24,484 main.py:57] epoch 5512, training loss: 8659.66, average training loss: 7507.11, base loss: 16101.67
[INFO 2017-06-29 04:53:27,553 main.py:57] epoch 5513, training loss: 7786.43, average training loss: 7508.11, base loss: 16102.07
[INFO 2017-06-29 04:53:30,536 main.py:57] epoch 5514, training loss: 6813.16, average training loss: 7507.01, base loss: 16101.52
[INFO 2017-06-29 04:53:33,691 main.py:57] epoch 5515, training loss: 7067.71, average training loss: 7507.44, base loss: 16101.38
[INFO 2017-06-29 04:53:36,760 main.py:57] epoch 5516, training loss: 7786.17, average training loss: 7508.13, base loss: 16101.25
[INFO 2017-06-29 04:53:39,779 main.py:57] epoch 5517, training loss: 7328.98, average training loss: 7507.67, base loss: 16100.74
[INFO 2017-06-29 04:53:42,861 main.py:57] epoch 5518, training loss: 7724.06, average training loss: 7505.89, base loss: 16100.96
[INFO 2017-06-29 04:53:45,865 main.py:57] epoch 5519, training loss: 5905.09, average training loss: 7503.93, base loss: 16100.18
[INFO 2017-06-29 04:53:48,951 main.py:57] epoch 5520, training loss: 7350.67, average training loss: 7504.43, base loss: 16100.25
[INFO 2017-06-29 04:53:52,058 main.py:57] epoch 5521, training loss: 6814.41, average training loss: 7503.16, base loss: 16100.06
[INFO 2017-06-29 04:53:55,132 main.py:57] epoch 5522, training loss: 7710.70, average training loss: 7503.11, base loss: 16099.85
[INFO 2017-06-29 04:53:58,130 main.py:57] epoch 5523, training loss: 6797.57, average training loss: 7502.20, base loss: 16099.16
[INFO 2017-06-29 04:54:01,172 main.py:57] epoch 5524, training loss: 6993.87, average training loss: 7500.58, base loss: 16098.57
[INFO 2017-06-29 04:54:04,247 main.py:57] epoch 5525, training loss: 7673.68, average training loss: 7501.13, base loss: 16099.09
[INFO 2017-06-29 04:54:07,297 main.py:57] epoch 5526, training loss: 7879.95, average training loss: 7501.39, base loss: 16099.32
[INFO 2017-06-29 04:54:10,393 main.py:57] epoch 5527, training loss: 8355.84, average training loss: 7502.69, base loss: 16099.85
[INFO 2017-06-29 04:54:13,495 main.py:57] epoch 5528, training loss: 8507.09, average training loss: 7504.15, base loss: 16100.60
[INFO 2017-06-29 04:54:16,473 main.py:57] epoch 5529, training loss: 7097.09, average training loss: 7502.34, base loss: 16099.91
[INFO 2017-06-29 04:54:19,493 main.py:57] epoch 5530, training loss: 7135.47, average training loss: 7501.69, base loss: 16100.21
[INFO 2017-06-29 04:54:22,585 main.py:57] epoch 5531, training loss: 7897.03, average training loss: 7501.56, base loss: 16100.63
[INFO 2017-06-29 04:54:25,602 main.py:57] epoch 5532, training loss: 7398.40, average training loss: 7501.68, base loss: 16101.17
[INFO 2017-06-29 04:54:28,612 main.py:57] epoch 5533, training loss: 6235.27, average training loss: 7500.88, base loss: 16100.85
[INFO 2017-06-29 04:54:31,702 main.py:57] epoch 5534, training loss: 6865.43, average training loss: 7500.43, base loss: 16100.05
[INFO 2017-06-29 04:54:34,755 main.py:57] epoch 5535, training loss: 6410.51, average training loss: 7500.40, base loss: 16099.26
[INFO 2017-06-29 04:54:37,766 main.py:57] epoch 5536, training loss: 7591.62, average training loss: 7499.91, base loss: 16099.68
[INFO 2017-06-29 04:54:40,939 main.py:57] epoch 5537, training loss: 7436.76, average training loss: 7499.86, base loss: 16099.54
[INFO 2017-06-29 04:54:44,020 main.py:57] epoch 5538, training loss: 7943.56, average training loss: 7500.66, base loss: 16099.42
[INFO 2017-06-29 04:54:47,192 main.py:57] epoch 5539, training loss: 7969.56, average training loss: 7501.22, base loss: 16099.11
[INFO 2017-06-29 04:54:50,253 main.py:57] epoch 5540, training loss: 7731.46, average training loss: 7502.71, base loss: 16099.18
[INFO 2017-06-29 04:54:53,251 main.py:57] epoch 5541, training loss: 6604.54, average training loss: 7502.03, base loss: 16098.77
[INFO 2017-06-29 04:54:56,241 main.py:57] epoch 5542, training loss: 7936.63, average training loss: 7499.92, base loss: 16098.96
[INFO 2017-06-29 04:54:59,315 main.py:57] epoch 5543, training loss: 8116.92, average training loss: 7499.34, base loss: 16099.22
[INFO 2017-06-29 04:55:02,333 main.py:57] epoch 5544, training loss: 7102.74, average training loss: 7499.62, base loss: 16098.83
[INFO 2017-06-29 04:55:05,336 main.py:57] epoch 5545, training loss: 7114.42, average training loss: 7499.60, base loss: 16098.13
[INFO 2017-06-29 04:55:08,360 main.py:57] epoch 5546, training loss: 7290.07, average training loss: 7499.64, base loss: 16097.91
[INFO 2017-06-29 04:55:11,403 main.py:57] epoch 5547, training loss: 6531.84, average training loss: 7497.88, base loss: 16097.61
[INFO 2017-06-29 04:55:14,448 main.py:57] epoch 5548, training loss: 6495.52, average training loss: 7496.73, base loss: 16097.30
[INFO 2017-06-29 04:55:17,586 main.py:57] epoch 5549, training loss: 7660.38, average training loss: 7496.52, base loss: 16097.54
[INFO 2017-06-29 04:55:20,625 main.py:57] epoch 5550, training loss: 7460.47, average training loss: 7496.13, base loss: 16097.12
[INFO 2017-06-29 04:55:23,752 main.py:57] epoch 5551, training loss: 8960.97, average training loss: 7497.18, base loss: 16098.10
[INFO 2017-06-29 04:55:26,766 main.py:57] epoch 5552, training loss: 7581.85, average training loss: 7495.80, base loss: 16098.25
[INFO 2017-06-29 04:55:29,800 main.py:57] epoch 5553, training loss: 7376.14, average training loss: 7495.89, base loss: 16098.21
[INFO 2017-06-29 04:55:32,901 main.py:57] epoch 5554, training loss: 7279.54, average training loss: 7496.39, base loss: 16098.21
[INFO 2017-06-29 04:55:35,927 main.py:57] epoch 5555, training loss: 9145.71, average training loss: 7498.43, base loss: 16099.22
[INFO 2017-06-29 04:55:38,996 main.py:57] epoch 5556, training loss: 7109.97, average training loss: 7498.13, base loss: 16098.49
[INFO 2017-06-29 04:55:42,074 main.py:57] epoch 5557, training loss: 8290.21, average training loss: 7500.00, base loss: 16098.93
[INFO 2017-06-29 04:55:45,152 main.py:57] epoch 5558, training loss: 6959.99, average training loss: 7500.74, base loss: 16098.53
[INFO 2017-06-29 04:55:48,205 main.py:57] epoch 5559, training loss: 9416.87, average training loss: 7503.19, base loss: 16099.56
[INFO 2017-06-29 04:55:51,275 main.py:57] epoch 5560, training loss: 7330.80, average training loss: 7502.09, base loss: 16099.59
[INFO 2017-06-29 04:55:54,351 main.py:57] epoch 5561, training loss: 7589.27, average training loss: 7501.32, base loss: 16099.13
[INFO 2017-06-29 04:55:57,415 main.py:57] epoch 5562, training loss: 8149.89, average training loss: 7502.81, base loss: 16099.45
[INFO 2017-06-29 04:56:00,502 main.py:57] epoch 5563, training loss: 7447.71, average training loss: 7503.72, base loss: 16099.95
[INFO 2017-06-29 04:56:03,601 main.py:57] epoch 5564, training loss: 7825.13, average training loss: 7504.39, base loss: 16100.43
[INFO 2017-06-29 04:56:06,701 main.py:57] epoch 5565, training loss: 7306.93, average training loss: 7502.87, base loss: 16101.00
[INFO 2017-06-29 04:56:09,743 main.py:57] epoch 5566, training loss: 6352.64, average training loss: 7502.36, base loss: 16100.30
[INFO 2017-06-29 04:56:12,787 main.py:57] epoch 5567, training loss: 7656.73, average training loss: 7502.33, base loss: 16100.88
[INFO 2017-06-29 04:56:15,891 main.py:57] epoch 5568, training loss: 9313.94, average training loss: 7504.50, base loss: 16102.16
[INFO 2017-06-29 04:56:18,984 main.py:57] epoch 5569, training loss: 7377.76, average training loss: 7504.90, base loss: 16102.13
[INFO 2017-06-29 04:56:21,981 main.py:57] epoch 5570, training loss: 7529.02, average training loss: 7505.00, base loss: 16102.30
[INFO 2017-06-29 04:56:25,024 main.py:57] epoch 5571, training loss: 6720.44, average training loss: 7504.91, base loss: 16101.20
[INFO 2017-06-29 04:56:28,097 main.py:57] epoch 5572, training loss: 7528.21, average training loss: 7504.73, base loss: 16101.13
[INFO 2017-06-29 04:56:31,201 main.py:57] epoch 5573, training loss: 7335.17, average training loss: 7504.22, base loss: 16101.43
[INFO 2017-06-29 04:56:34,293 main.py:57] epoch 5574, training loss: 6635.07, average training loss: 7502.77, base loss: 16100.81
[INFO 2017-06-29 04:56:37,449 main.py:57] epoch 5575, training loss: 7401.56, average training loss: 7501.89, base loss: 16100.36
[INFO 2017-06-29 04:56:40,538 main.py:57] epoch 5576, training loss: 7911.02, average training loss: 7502.80, base loss: 16100.78
[INFO 2017-06-29 04:56:43,612 main.py:57] epoch 5577, training loss: 6702.38, average training loss: 7502.05, base loss: 16100.16
[INFO 2017-06-29 04:56:46,679 main.py:57] epoch 5578, training loss: 8152.19, average training loss: 7502.98, base loss: 16100.35
[INFO 2017-06-29 04:56:49,762 main.py:57] epoch 5579, training loss: 7479.47, average training loss: 7503.60, base loss: 16099.91
[INFO 2017-06-29 04:56:52,852 main.py:57] epoch 5580, training loss: 7287.29, average training loss: 7504.95, base loss: 16100.12
[INFO 2017-06-29 04:56:55,902 main.py:57] epoch 5581, training loss: 6866.37, average training loss: 7504.08, base loss: 16100.19
[INFO 2017-06-29 04:56:58,937 main.py:57] epoch 5582, training loss: 7165.75, average training loss: 7502.77, base loss: 16100.70
[INFO 2017-06-29 04:57:02,075 main.py:57] epoch 5583, training loss: 7505.65, average training loss: 7503.05, base loss: 16100.09
[INFO 2017-06-29 04:57:05,126 main.py:57] epoch 5584, training loss: 7209.14, average training loss: 7503.92, base loss: 16099.66
[INFO 2017-06-29 04:57:08,270 main.py:57] epoch 5585, training loss: 7670.93, average training loss: 7504.55, base loss: 16100.33
[INFO 2017-06-29 04:57:11,370 main.py:57] epoch 5586, training loss: 6751.52, average training loss: 7503.28, base loss: 16100.08
[INFO 2017-06-29 04:57:14,437 main.py:57] epoch 5587, training loss: 7693.19, average training loss: 7504.08, base loss: 16100.33
[INFO 2017-06-29 04:57:17,497 main.py:57] epoch 5588, training loss: 7351.75, average training loss: 7503.61, base loss: 16099.94
[INFO 2017-06-29 04:57:20,551 main.py:57] epoch 5589, training loss: 7763.75, average training loss: 7504.46, base loss: 16100.44
[INFO 2017-06-29 04:57:23,523 main.py:57] epoch 5590, training loss: 7694.07, average training loss: 7503.75, base loss: 16100.94
[INFO 2017-06-29 04:57:26,570 main.py:57] epoch 5591, training loss: 7311.24, average training loss: 7503.61, base loss: 16101.47
[INFO 2017-06-29 04:57:29,589 main.py:57] epoch 5592, training loss: 6929.73, average training loss: 7500.11, base loss: 16101.48
[INFO 2017-06-29 04:57:32,676 main.py:57] epoch 5593, training loss: 8267.51, average training loss: 7501.21, base loss: 16101.91
[INFO 2017-06-29 04:57:35,718 main.py:57] epoch 5594, training loss: 8139.77, average training loss: 7502.42, base loss: 16102.57
[INFO 2017-06-29 04:57:38,782 main.py:57] epoch 5595, training loss: 7357.64, average training loss: 7502.30, base loss: 16102.35
[INFO 2017-06-29 04:57:41,788 main.py:57] epoch 5596, training loss: 6908.28, average training loss: 7501.46, base loss: 16102.04
[INFO 2017-06-29 04:57:44,808 main.py:57] epoch 5597, training loss: 7323.30, average training loss: 7501.44, base loss: 16102.22
[INFO 2017-06-29 04:57:47,895 main.py:57] epoch 5598, training loss: 7427.54, average training loss: 7501.63, base loss: 16101.83
[INFO 2017-06-29 04:57:50,996 main.py:57] epoch 5599, training loss: 9195.02, average training loss: 7503.31, base loss: 16101.96
[INFO 2017-06-29 04:57:50,996 main.py:59] epoch 5599, testing
[INFO 2017-06-29 04:58:03,537 main.py:104] average testing loss: 8233.73, base loss: 17009.39
[INFO 2017-06-29 04:58:03,537 main.py:105] improve_loss: 8775.66, improve_percent: 0.52
[INFO 2017-06-29 04:58:03,539 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 04:58:06,651 main.py:57] epoch 5600, training loss: 6098.89, average training loss: 7499.90, base loss: 16101.01
[INFO 2017-06-29 04:58:09,763 main.py:57] epoch 5601, training loss: 7695.55, average training loss: 7498.59, base loss: 16100.90
[INFO 2017-06-29 04:58:12,773 main.py:57] epoch 5602, training loss: 7231.63, average training loss: 7499.06, base loss: 16100.49
[INFO 2017-06-29 04:58:15,832 main.py:57] epoch 5603, training loss: 7092.70, average training loss: 7498.62, base loss: 16099.77
[INFO 2017-06-29 04:58:18,931 main.py:57] epoch 5604, training loss: 7854.92, average training loss: 7498.93, base loss: 16099.61
[INFO 2017-06-29 04:58:21,961 main.py:57] epoch 5605, training loss: 7250.93, average training loss: 7498.39, base loss: 16099.65
[INFO 2017-06-29 04:58:24,980 main.py:57] epoch 5606, training loss: 7127.02, average training loss: 7497.22, base loss: 16099.64
[INFO 2017-06-29 04:58:28,013 main.py:57] epoch 5607, training loss: 7064.38, average training loss: 7496.96, base loss: 16100.15
[INFO 2017-06-29 04:58:31,068 main.py:57] epoch 5608, training loss: 7828.47, average training loss: 7497.10, base loss: 16100.19
[INFO 2017-06-29 04:58:34,147 main.py:57] epoch 5609, training loss: 9025.39, average training loss: 7497.16, base loss: 16100.67
[INFO 2017-06-29 04:58:37,252 main.py:57] epoch 5610, training loss: 7709.67, average training loss: 7497.11, base loss: 16101.08
[INFO 2017-06-29 04:58:40,316 main.py:57] epoch 5611, training loss: 7449.66, average training loss: 7497.63, base loss: 16101.46
[INFO 2017-06-29 04:58:43,439 main.py:57] epoch 5612, training loss: 7378.73, average training loss: 7497.74, base loss: 16102.11
[INFO 2017-06-29 04:58:46,467 main.py:57] epoch 5613, training loss: 8037.92, average training loss: 7498.84, base loss: 16102.87
[INFO 2017-06-29 04:58:49,469 main.py:57] epoch 5614, training loss: 7233.92, average training loss: 7498.63, base loss: 16102.81
[INFO 2017-06-29 04:58:52,559 main.py:57] epoch 5615, training loss: 6873.89, average training loss: 7496.90, base loss: 16102.64
[INFO 2017-06-29 04:58:55,659 main.py:57] epoch 5616, training loss: 7361.11, average training loss: 7496.37, base loss: 16102.98
[INFO 2017-06-29 04:58:58,719 main.py:57] epoch 5617, training loss: 7058.15, average training loss: 7495.89, base loss: 16102.33
[INFO 2017-06-29 04:59:01,781 main.py:57] epoch 5618, training loss: 7154.01, average training loss: 7495.97, base loss: 16102.13
[INFO 2017-06-29 04:59:04,790 main.py:57] epoch 5619, training loss: 7932.24, average training loss: 7496.68, base loss: 16102.20
[INFO 2017-06-29 04:59:07,885 main.py:57] epoch 5620, training loss: 8747.33, average training loss: 7498.72, base loss: 16103.00
[INFO 2017-06-29 04:59:10,963 main.py:57] epoch 5621, training loss: 7313.11, average training loss: 7497.96, base loss: 16103.05
[INFO 2017-06-29 04:59:14,014 main.py:57] epoch 5622, training loss: 7264.97, average training loss: 7497.89, base loss: 16102.85
[INFO 2017-06-29 04:59:17,091 main.py:57] epoch 5623, training loss: 6740.79, average training loss: 7496.91, base loss: 16101.78
[INFO 2017-06-29 04:59:20,084 main.py:57] epoch 5624, training loss: 7976.19, average training loss: 7497.17, base loss: 16102.28
[INFO 2017-06-29 04:59:23,145 main.py:57] epoch 5625, training loss: 7704.16, average training loss: 7497.17, base loss: 16102.50
[INFO 2017-06-29 04:59:26,276 main.py:57] epoch 5626, training loss: 7223.13, average training loss: 7497.66, base loss: 16102.69
[INFO 2017-06-29 04:59:29,302 main.py:57] epoch 5627, training loss: 7896.02, average training loss: 7499.05, base loss: 16102.80
[INFO 2017-06-29 04:59:32,352 main.py:57] epoch 5628, training loss: 7993.62, average training loss: 7499.38, base loss: 16102.97
[INFO 2017-06-29 04:59:35,394 main.py:57] epoch 5629, training loss: 7198.33, average training loss: 7500.52, base loss: 16102.75
[INFO 2017-06-29 04:59:38,548 main.py:57] epoch 5630, training loss: 7433.84, average training loss: 7500.22, base loss: 16102.50
[INFO 2017-06-29 04:59:41,521 main.py:57] epoch 5631, training loss: 8180.51, average training loss: 7499.29, base loss: 16102.76
[INFO 2017-06-29 04:59:44,600 main.py:57] epoch 5632, training loss: 7670.47, average training loss: 7499.58, base loss: 16103.04
[INFO 2017-06-29 04:59:47,616 main.py:57] epoch 5633, training loss: 8051.64, average training loss: 7500.48, base loss: 16103.28
[INFO 2017-06-29 04:59:50,678 main.py:57] epoch 5634, training loss: 7029.72, average training loss: 7500.43, base loss: 16103.12
[INFO 2017-06-29 04:59:53,755 main.py:57] epoch 5635, training loss: 8464.96, average training loss: 7501.81, base loss: 16104.12
[INFO 2017-06-29 04:59:56,811 main.py:57] epoch 5636, training loss: 7943.50, average training loss: 7501.19, base loss: 16103.83
[INFO 2017-06-29 04:59:59,831 main.py:57] epoch 5637, training loss: 7353.68, average training loss: 7500.59, base loss: 16103.39
[INFO 2017-06-29 05:00:02,891 main.py:57] epoch 5638, training loss: 6955.86, average training loss: 7499.96, base loss: 16103.10
[INFO 2017-06-29 05:00:05,979 main.py:57] epoch 5639, training loss: 7361.99, average training loss: 7500.18, base loss: 16103.33
[INFO 2017-06-29 05:00:09,008 main.py:57] epoch 5640, training loss: 8028.33, average training loss: 7500.99, base loss: 16103.61
[INFO 2017-06-29 05:00:12,067 main.py:57] epoch 5641, training loss: 8100.90, average training loss: 7501.74, base loss: 16103.82
[INFO 2017-06-29 05:00:15,066 main.py:57] epoch 5642, training loss: 7616.04, average training loss: 7502.27, base loss: 16104.08
[INFO 2017-06-29 05:00:18,115 main.py:57] epoch 5643, training loss: 7096.04, average training loss: 7501.88, base loss: 16103.97
[INFO 2017-06-29 05:00:21,211 main.py:57] epoch 5644, training loss: 7842.97, average training loss: 7502.17, base loss: 16104.00
[INFO 2017-06-29 05:00:24,250 main.py:57] epoch 5645, training loss: 7038.74, average training loss: 7500.85, base loss: 16103.93
[INFO 2017-06-29 05:00:27,237 main.py:57] epoch 5646, training loss: 6640.11, average training loss: 7499.98, base loss: 16103.06
[INFO 2017-06-29 05:00:30,297 main.py:57] epoch 5647, training loss: 7665.30, average training loss: 7500.61, base loss: 16103.16
[INFO 2017-06-29 05:00:33,346 main.py:57] epoch 5648, training loss: 7525.83, average training loss: 7500.87, base loss: 16103.76
[INFO 2017-06-29 05:00:36,389 main.py:57] epoch 5649, training loss: 7612.94, average training loss: 7500.90, base loss: 16104.01
[INFO 2017-06-29 05:00:39,435 main.py:57] epoch 5650, training loss: 7866.71, average training loss: 7499.54, base loss: 16104.74
[INFO 2017-06-29 05:00:42,469 main.py:57] epoch 5651, training loss: 6343.40, average training loss: 7499.20, base loss: 16104.24
[INFO 2017-06-29 05:00:45,479 main.py:57] epoch 5652, training loss: 7524.02, average training loss: 7500.06, base loss: 16104.11
[INFO 2017-06-29 05:00:48,617 main.py:57] epoch 5653, training loss: 6575.59, average training loss: 7499.06, base loss: 16103.60
[INFO 2017-06-29 05:00:51,670 main.py:57] epoch 5654, training loss: 8070.38, average training loss: 7500.25, base loss: 16104.20
[INFO 2017-06-29 05:00:54,807 main.py:57] epoch 5655, training loss: 8674.20, average training loss: 7501.35, base loss: 16104.83
[INFO 2017-06-29 05:00:57,888 main.py:57] epoch 5656, training loss: 7150.82, average training loss: 7501.43, base loss: 16104.69
[INFO 2017-06-29 05:01:00,937 main.py:57] epoch 5657, training loss: 7169.02, average training loss: 7501.23, base loss: 16104.84
[INFO 2017-06-29 05:01:04,012 main.py:57] epoch 5658, training loss: 8456.42, average training loss: 7502.24, base loss: 16105.10
[INFO 2017-06-29 05:01:07,086 main.py:57] epoch 5659, training loss: 8227.08, average training loss: 7502.51, base loss: 16105.72
[INFO 2017-06-29 05:01:10,144 main.py:57] epoch 5660, training loss: 7793.97, average training loss: 7503.47, base loss: 16106.13
[INFO 2017-06-29 05:01:13,109 main.py:57] epoch 5661, training loss: 7416.47, average training loss: 7502.91, base loss: 16106.86
[INFO 2017-06-29 05:01:16,106 main.py:57] epoch 5662, training loss: 6641.71, average training loss: 7502.13, base loss: 16106.54
[INFO 2017-06-29 05:01:19,179 main.py:57] epoch 5663, training loss: 8995.99, average training loss: 7503.33, base loss: 16107.34
[INFO 2017-06-29 05:01:22,251 main.py:57] epoch 5664, training loss: 7754.25, average training loss: 7504.31, base loss: 16107.15
[INFO 2017-06-29 05:01:25,349 main.py:57] epoch 5665, training loss: 7589.89, average training loss: 7503.80, base loss: 16106.76
[INFO 2017-06-29 05:01:28,395 main.py:57] epoch 5666, training loss: 7244.23, average training loss: 7503.95, base loss: 16107.41
[INFO 2017-06-29 05:01:31,430 main.py:57] epoch 5667, training loss: 7112.62, average training loss: 7502.77, base loss: 16107.54
[INFO 2017-06-29 05:01:34,450 main.py:57] epoch 5668, training loss: 8064.78, average training loss: 7503.57, base loss: 16107.65
[INFO 2017-06-29 05:01:37,485 main.py:57] epoch 5669, training loss: 7976.35, average training loss: 7504.48, base loss: 16107.45
[INFO 2017-06-29 05:01:40,587 main.py:57] epoch 5670, training loss: 7769.86, average training loss: 7504.60, base loss: 16107.83
[INFO 2017-06-29 05:01:43,670 main.py:57] epoch 5671, training loss: 6950.31, average training loss: 7504.23, base loss: 16107.68
[INFO 2017-06-29 05:01:46,741 main.py:57] epoch 5672, training loss: 8752.38, average training loss: 7503.66, base loss: 16108.42
[INFO 2017-06-29 05:01:49,801 main.py:57] epoch 5673, training loss: 7962.45, average training loss: 7504.22, base loss: 16109.02
[INFO 2017-06-29 05:01:52,934 main.py:57] epoch 5674, training loss: 6175.73, average training loss: 7502.61, base loss: 16108.65
[INFO 2017-06-29 05:01:56,059 main.py:57] epoch 5675, training loss: 7248.61, average training loss: 7501.45, base loss: 16108.89
[INFO 2017-06-29 05:01:59,106 main.py:57] epoch 5676, training loss: 7599.04, average training loss: 7502.28, base loss: 16109.20
[INFO 2017-06-29 05:02:02,107 main.py:57] epoch 5677, training loss: 7032.91, average training loss: 7499.83, base loss: 16108.92
[INFO 2017-06-29 05:02:05,097 main.py:57] epoch 5678, training loss: 7830.35, average training loss: 7498.65, base loss: 16110.14
[INFO 2017-06-29 05:02:08,092 main.py:57] epoch 5679, training loss: 7099.68, average training loss: 7497.54, base loss: 16109.94
[INFO 2017-06-29 05:02:11,180 main.py:57] epoch 5680, training loss: 7236.06, average training loss: 7496.60, base loss: 16110.86
[INFO 2017-06-29 05:02:14,275 main.py:57] epoch 5681, training loss: 6844.79, average training loss: 7496.47, base loss: 16111.44
[INFO 2017-06-29 05:02:17,334 main.py:57] epoch 5682, training loss: 7667.43, average training loss: 7496.25, base loss: 16111.40
[INFO 2017-06-29 05:02:20,405 main.py:57] epoch 5683, training loss: 6797.78, average training loss: 7493.68, base loss: 16110.96
[INFO 2017-06-29 05:02:23,427 main.py:57] epoch 5684, training loss: 6736.58, average training loss: 7493.90, base loss: 16110.79
[INFO 2017-06-29 05:02:26,454 main.py:57] epoch 5685, training loss: 7636.71, average training loss: 7495.24, base loss: 16111.02
[INFO 2017-06-29 05:02:29,503 main.py:57] epoch 5686, training loss: 6494.02, average training loss: 7493.87, base loss: 16109.94
[INFO 2017-06-29 05:02:32,520 main.py:57] epoch 5687, training loss: 7722.47, average training loss: 7492.99, base loss: 16109.75
[INFO 2017-06-29 05:02:35,568 main.py:57] epoch 5688, training loss: 6651.00, average training loss: 7492.61, base loss: 16109.22
[INFO 2017-06-29 05:02:38,638 main.py:57] epoch 5689, training loss: 7234.39, average training loss: 7491.35, base loss: 16108.99
[INFO 2017-06-29 05:02:41,696 main.py:57] epoch 5690, training loss: 8168.81, average training loss: 7492.68, base loss: 16109.85
[INFO 2017-06-29 05:02:44,783 main.py:57] epoch 5691, training loss: 7757.21, average training loss: 7492.85, base loss: 16109.86
[INFO 2017-06-29 05:02:47,805 main.py:57] epoch 5692, training loss: 7314.41, average training loss: 7492.89, base loss: 16109.67
[INFO 2017-06-29 05:02:50,848 main.py:57] epoch 5693, training loss: 7949.99, average training loss: 7492.83, base loss: 16110.03
[INFO 2017-06-29 05:02:53,868 main.py:57] epoch 5694, training loss: 7643.94, average training loss: 7493.21, base loss: 16109.82
[INFO 2017-06-29 05:02:56,929 main.py:57] epoch 5695, training loss: 6781.14, average training loss: 7492.13, base loss: 16109.45
[INFO 2017-06-29 05:03:00,009 main.py:57] epoch 5696, training loss: 8254.52, average training loss: 7492.31, base loss: 16109.61
[INFO 2017-06-29 05:03:03,049 main.py:57] epoch 5697, training loss: 7042.10, average training loss: 7491.62, base loss: 16108.90
[INFO 2017-06-29 05:03:06,115 main.py:57] epoch 5698, training loss: 7720.61, average training loss: 7491.95, base loss: 16109.04
[INFO 2017-06-29 05:03:09,153 main.py:57] epoch 5699, training loss: 6911.20, average training loss: 7491.42, base loss: 16108.55
[INFO 2017-06-29 05:03:09,154 main.py:59] epoch 5699, testing
[INFO 2017-06-29 05:03:21,765 main.py:104] average testing loss: 8285.42, base loss: 17008.96
[INFO 2017-06-29 05:03:21,765 main.py:105] improve_loss: 8723.54, improve_percent: 0.51
[INFO 2017-06-29 05:03:21,767 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:03:24,826 main.py:57] epoch 5700, training loss: 6372.08, average training loss: 7489.59, base loss: 16108.00
[INFO 2017-06-29 05:03:27,944 main.py:57] epoch 5701, training loss: 7265.10, average training loss: 7490.36, base loss: 16108.18
[INFO 2017-06-29 05:03:31,007 main.py:57] epoch 5702, training loss: 7503.08, average training loss: 7489.42, base loss: 16108.16
[INFO 2017-06-29 05:03:34,031 main.py:57] epoch 5703, training loss: 7514.17, average training loss: 7489.85, base loss: 16108.38
[INFO 2017-06-29 05:03:37,106 main.py:57] epoch 5704, training loss: 8100.61, average training loss: 7490.66, base loss: 16108.89
[INFO 2017-06-29 05:03:40,190 main.py:57] epoch 5705, training loss: 7512.41, average training loss: 7490.13, base loss: 16109.58
[INFO 2017-06-29 05:03:43,267 main.py:57] epoch 5706, training loss: 7177.85, average training loss: 7489.70, base loss: 16109.84
[INFO 2017-06-29 05:03:46,317 main.py:57] epoch 5707, training loss: 7299.18, average training loss: 7489.72, base loss: 16110.12
[INFO 2017-06-29 05:03:49,353 main.py:57] epoch 5708, training loss: 7890.44, average training loss: 7490.67, base loss: 16110.34
[INFO 2017-06-29 05:03:52,448 main.py:57] epoch 5709, training loss: 7247.53, average training loss: 7489.36, base loss: 16110.68
[INFO 2017-06-29 05:03:55,447 main.py:57] epoch 5710, training loss: 6742.94, average training loss: 7487.54, base loss: 16110.41
[INFO 2017-06-29 05:03:58,538 main.py:57] epoch 5711, training loss: 6690.18, average training loss: 7486.21, base loss: 16110.27
[INFO 2017-06-29 05:04:01,646 main.py:57] epoch 5712, training loss: 7949.21, average training loss: 7485.69, base loss: 16110.20
[INFO 2017-06-29 05:04:04,761 main.py:57] epoch 5713, training loss: 8195.29, average training loss: 7485.98, base loss: 16110.47
[INFO 2017-06-29 05:04:07,777 main.py:57] epoch 5714, training loss: 7956.36, average training loss: 7487.11, base loss: 16110.99
[INFO 2017-06-29 05:04:10,856 main.py:57] epoch 5715, training loss: 7908.98, average training loss: 7487.53, base loss: 16111.46
[INFO 2017-06-29 05:04:13,971 main.py:57] epoch 5716, training loss: 7215.50, average training loss: 7488.11, base loss: 16111.15
[INFO 2017-06-29 05:04:16,985 main.py:57] epoch 5717, training loss: 7037.39, average training loss: 7487.59, base loss: 16110.57
[INFO 2017-06-29 05:04:19,984 main.py:57] epoch 5718, training loss: 7580.46, average training loss: 7486.65, base loss: 16110.15
[INFO 2017-06-29 05:04:22,983 main.py:57] epoch 5719, training loss: 7058.92, average training loss: 7486.91, base loss: 16109.69
[INFO 2017-06-29 05:04:26,027 main.py:57] epoch 5720, training loss: 7764.46, average training loss: 7486.56, base loss: 16110.27
[INFO 2017-06-29 05:04:29,044 main.py:57] epoch 5721, training loss: 7681.25, average training loss: 7486.73, base loss: 16110.42
[INFO 2017-06-29 05:04:32,151 main.py:57] epoch 5722, training loss: 7483.27, average training loss: 7487.51, base loss: 16110.62
[INFO 2017-06-29 05:04:35,175 main.py:57] epoch 5723, training loss: 7307.38, average training loss: 7486.74, base loss: 16110.43
[INFO 2017-06-29 05:04:38,225 main.py:57] epoch 5724, training loss: 6886.89, average training loss: 7486.60, base loss: 16109.69
[INFO 2017-06-29 05:04:41,280 main.py:57] epoch 5725, training loss: 9357.67, average training loss: 7488.22, base loss: 16110.42
[INFO 2017-06-29 05:04:44,301 main.py:57] epoch 5726, training loss: 7557.57, average training loss: 7487.80, base loss: 16110.68
[INFO 2017-06-29 05:04:47,304 main.py:57] epoch 5727, training loss: 7688.38, average training loss: 7488.44, base loss: 16111.06
[INFO 2017-06-29 05:04:50,395 main.py:57] epoch 5728, training loss: 7676.10, average training loss: 7487.67, base loss: 16110.73
[INFO 2017-06-29 05:04:53,539 main.py:57] epoch 5729, training loss: 7368.42, average training loss: 7489.05, base loss: 16110.51
[INFO 2017-06-29 05:04:56,607 main.py:57] epoch 5730, training loss: 7279.25, average training loss: 7489.21, base loss: 16110.34
[INFO 2017-06-29 05:04:59,695 main.py:57] epoch 5731, training loss: 7460.99, average training loss: 7489.09, base loss: 16109.98
[INFO 2017-06-29 05:05:02,723 main.py:57] epoch 5732, training loss: 7853.62, average training loss: 7489.05, base loss: 16109.99
[INFO 2017-06-29 05:05:05,742 main.py:57] epoch 5733, training loss: 7338.49, average training loss: 7488.40, base loss: 16110.30
[INFO 2017-06-29 05:05:08,742 main.py:57] epoch 5734, training loss: 7947.09, average training loss: 7488.57, base loss: 16110.78
[INFO 2017-06-29 05:05:11,791 main.py:57] epoch 5735, training loss: 7383.01, average training loss: 7488.08, base loss: 16111.32
[INFO 2017-06-29 05:05:14,840 main.py:57] epoch 5736, training loss: 7385.23, average training loss: 7487.20, base loss: 16111.08
[INFO 2017-06-29 05:05:17,898 main.py:57] epoch 5737, training loss: 7143.01, average training loss: 7487.52, base loss: 16110.70
[INFO 2017-06-29 05:05:20,888 main.py:57] epoch 5738, training loss: 7890.91, average training loss: 7487.47, base loss: 16110.56
[INFO 2017-06-29 05:05:23,926 main.py:57] epoch 5739, training loss: 7137.56, average training loss: 7486.91, base loss: 16110.51
[INFO 2017-06-29 05:05:26,974 main.py:57] epoch 5740, training loss: 6810.19, average training loss: 7487.55, base loss: 16110.58
[INFO 2017-06-29 05:05:30,010 main.py:57] epoch 5741, training loss: 7479.43, average training loss: 7486.72, base loss: 16110.77
[INFO 2017-06-29 05:05:33,079 main.py:57] epoch 5742, training loss: 7461.04, average training loss: 7487.15, base loss: 16111.20
[INFO 2017-06-29 05:05:36,109 main.py:57] epoch 5743, training loss: 6641.28, average training loss: 7486.32, base loss: 16110.79
[INFO 2017-06-29 05:05:39,163 main.py:57] epoch 5744, training loss: 6997.67, average training loss: 7483.54, base loss: 16110.47
[INFO 2017-06-29 05:05:42,220 main.py:57] epoch 5745, training loss: 7281.89, average training loss: 7482.99, base loss: 16110.16
[INFO 2017-06-29 05:05:45,249 main.py:57] epoch 5746, training loss: 6892.26, average training loss: 7481.63, base loss: 16109.80
[INFO 2017-06-29 05:05:48,335 main.py:57] epoch 5747, training loss: 7053.79, average training loss: 7481.25, base loss: 16109.65
[INFO 2017-06-29 05:05:51,423 main.py:57] epoch 5748, training loss: 6638.73, average training loss: 7480.61, base loss: 16109.62
[INFO 2017-06-29 05:05:54,479 main.py:57] epoch 5749, training loss: 7530.18, average training loss: 7480.58, base loss: 16109.64
[INFO 2017-06-29 05:05:57,568 main.py:57] epoch 5750, training loss: 8010.57, average training loss: 7481.51, base loss: 16109.61
[INFO 2017-06-29 05:06:00,613 main.py:57] epoch 5751, training loss: 8521.79, average training loss: 7481.26, base loss: 16110.04
[INFO 2017-06-29 05:06:03,644 main.py:57] epoch 5752, training loss: 8037.78, average training loss: 7481.93, base loss: 16109.77
[INFO 2017-06-29 05:06:06,623 main.py:57] epoch 5753, training loss: 6161.06, average training loss: 7480.47, base loss: 16108.90
[INFO 2017-06-29 05:06:09,667 main.py:57] epoch 5754, training loss: 7287.67, average training loss: 7480.95, base loss: 16108.80
[INFO 2017-06-29 05:06:12,798 main.py:57] epoch 5755, training loss: 6695.04, average training loss: 7480.61, base loss: 16108.18
[INFO 2017-06-29 05:06:15,863 main.py:57] epoch 5756, training loss: 7980.99, average training loss: 7481.53, base loss: 16108.39
[INFO 2017-06-29 05:06:18,873 main.py:57] epoch 5757, training loss: 8189.15, average training loss: 7482.39, base loss: 16109.04
[INFO 2017-06-29 05:06:21,921 main.py:57] epoch 5758, training loss: 7214.47, average training loss: 7483.15, base loss: 16109.06
[INFO 2017-06-29 05:06:24,960 main.py:57] epoch 5759, training loss: 7283.17, average training loss: 7483.74, base loss: 16108.51
[INFO 2017-06-29 05:06:28,035 main.py:57] epoch 5760, training loss: 6969.26, average training loss: 7482.89, base loss: 16108.53
[INFO 2017-06-29 05:06:31,094 main.py:57] epoch 5761, training loss: 7807.29, average training loss: 7483.88, base loss: 16109.08
[INFO 2017-06-29 05:06:34,132 main.py:57] epoch 5762, training loss: 6764.15, average training loss: 7482.83, base loss: 16108.92
[INFO 2017-06-29 05:06:37,201 main.py:57] epoch 5763, training loss: 6948.39, average training loss: 7481.91, base loss: 16108.74
[INFO 2017-06-29 05:06:40,278 main.py:57] epoch 5764, training loss: 7816.34, average training loss: 7482.02, base loss: 16108.46
[INFO 2017-06-29 05:06:43,313 main.py:57] epoch 5765, training loss: 7761.04, average training loss: 7482.13, base loss: 16108.53
[INFO 2017-06-29 05:06:46,344 main.py:57] epoch 5766, training loss: 7104.61, average training loss: 7482.34, base loss: 16108.44
[INFO 2017-06-29 05:06:49,397 main.py:57] epoch 5767, training loss: 6965.32, average training loss: 7482.64, base loss: 16108.66
[INFO 2017-06-29 05:06:52,438 main.py:57] epoch 5768, training loss: 8283.99, average training loss: 7483.73, base loss: 16109.42
[INFO 2017-06-29 05:06:55,601 main.py:57] epoch 5769, training loss: 8016.20, average training loss: 7484.64, base loss: 16109.87
[INFO 2017-06-29 05:06:58,657 main.py:57] epoch 5770, training loss: 6964.10, average training loss: 7484.55, base loss: 16109.43
[INFO 2017-06-29 05:07:01,688 main.py:57] epoch 5771, training loss: 6931.98, average training loss: 7484.10, base loss: 16109.27
[INFO 2017-06-29 05:07:04,725 main.py:57] epoch 5772, training loss: 7677.25, average training loss: 7484.88, base loss: 16108.34
[INFO 2017-06-29 05:07:07,851 main.py:57] epoch 5773, training loss: 7829.95, average training loss: 7484.92, base loss: 16107.65
[INFO 2017-06-29 05:07:10,936 main.py:57] epoch 5774, training loss: 6985.47, average training loss: 7482.60, base loss: 16107.79
[INFO 2017-06-29 05:07:13,999 main.py:57] epoch 5775, training loss: 6497.19, average training loss: 7481.73, base loss: 16107.05
[INFO 2017-06-29 05:07:17,082 main.py:57] epoch 5776, training loss: 7024.52, average training loss: 7481.76, base loss: 16106.96
[INFO 2017-06-29 05:07:20,164 main.py:57] epoch 5777, training loss: 6838.28, average training loss: 7481.57, base loss: 16106.93
[INFO 2017-06-29 05:07:23,276 main.py:57] epoch 5778, training loss: 7364.31, average training loss: 7482.00, base loss: 16107.00
[INFO 2017-06-29 05:07:26,335 main.py:57] epoch 5779, training loss: 6556.73, average training loss: 7481.73, base loss: 16106.40
[INFO 2017-06-29 05:07:29,397 main.py:57] epoch 5780, training loss: 7129.41, average training loss: 7481.88, base loss: 16106.48
[INFO 2017-06-29 05:07:32,478 main.py:57] epoch 5781, training loss: 7665.50, average training loss: 7482.49, base loss: 16106.86
[INFO 2017-06-29 05:07:35,570 main.py:57] epoch 5782, training loss: 6462.42, average training loss: 7481.25, base loss: 16106.42
[INFO 2017-06-29 05:07:38,721 main.py:57] epoch 5783, training loss: 7730.99, average training loss: 7482.13, base loss: 16106.27
[INFO 2017-06-29 05:07:41,771 main.py:57] epoch 5784, training loss: 7862.51, average training loss: 7482.87, base loss: 16106.32
[INFO 2017-06-29 05:07:44,834 main.py:57] epoch 5785, training loss: 8285.01, average training loss: 7483.87, base loss: 16107.19
[INFO 2017-06-29 05:07:47,941 main.py:57] epoch 5786, training loss: 7202.55, average training loss: 7483.44, base loss: 16106.54
[INFO 2017-06-29 05:07:51,070 main.py:57] epoch 5787, training loss: 8386.79, average training loss: 7484.55, base loss: 16106.96
[INFO 2017-06-29 05:07:54,098 main.py:57] epoch 5788, training loss: 7435.07, average training loss: 7485.26, base loss: 16106.74
[INFO 2017-06-29 05:07:57,134 main.py:57] epoch 5789, training loss: 7396.45, average training loss: 7484.08, base loss: 16105.82
[INFO 2017-06-29 05:08:00,177 main.py:57] epoch 5790, training loss: 7645.77, average training loss: 7483.90, base loss: 16105.97
[INFO 2017-06-29 05:08:03,230 main.py:57] epoch 5791, training loss: 6827.92, average training loss: 7483.52, base loss: 16105.42
[INFO 2017-06-29 05:08:06,255 main.py:57] epoch 5792, training loss: 8010.23, average training loss: 7484.34, base loss: 16105.69
[INFO 2017-06-29 05:08:09,346 main.py:57] epoch 5793, training loss: 6989.73, average training loss: 7483.99, base loss: 16105.42
[INFO 2017-06-29 05:08:12,440 main.py:57] epoch 5794, training loss: 8128.85, average training loss: 7485.34, base loss: 16106.26
[INFO 2017-06-29 05:08:15,463 main.py:57] epoch 5795, training loss: 6408.73, average training loss: 7484.32, base loss: 16106.53
[INFO 2017-06-29 05:08:18,485 main.py:57] epoch 5796, training loss: 7186.10, average training loss: 7484.76, base loss: 16106.20
[INFO 2017-06-29 05:08:21,509 main.py:57] epoch 5797, training loss: 7560.07, average training loss: 7484.91, base loss: 16106.48
[INFO 2017-06-29 05:08:24,541 main.py:57] epoch 5798, training loss: 6684.59, average training loss: 7483.88, base loss: 16105.87
[INFO 2017-06-29 05:08:27,613 main.py:57] epoch 5799, training loss: 6364.44, average training loss: 7482.74, base loss: 16104.68
[INFO 2017-06-29 05:08:27,613 main.py:59] epoch 5799, testing
[INFO 2017-06-29 05:08:40,183 main.py:104] average testing loss: 8371.49, base loss: 17110.02
[INFO 2017-06-29 05:08:40,183 main.py:105] improve_loss: 8738.53, improve_percent: 0.51
[INFO 2017-06-29 05:08:40,184 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:08:43,262 main.py:57] epoch 5800, training loss: 7619.59, average training loss: 7482.15, base loss: 16104.43
[INFO 2017-06-29 05:08:46,311 main.py:57] epoch 5801, training loss: 6947.94, average training loss: 7481.72, base loss: 16104.14
[INFO 2017-06-29 05:08:49,361 main.py:57] epoch 5802, training loss: 6793.28, average training loss: 7481.10, base loss: 16104.30
[INFO 2017-06-29 05:08:52,392 main.py:57] epoch 5803, training loss: 7042.87, average training loss: 7480.21, base loss: 16104.03
[INFO 2017-06-29 05:08:55,449 main.py:57] epoch 5804, training loss: 9569.89, average training loss: 7483.36, base loss: 16105.00
[INFO 2017-06-29 05:08:58,480 main.py:57] epoch 5805, training loss: 9480.64, average training loss: 7485.73, base loss: 16106.02
[INFO 2017-06-29 05:09:01,511 main.py:57] epoch 5806, training loss: 7195.86, average training loss: 7485.37, base loss: 16105.88
[INFO 2017-06-29 05:09:04,560 main.py:57] epoch 5807, training loss: 7424.83, average training loss: 7485.06, base loss: 16105.33
[INFO 2017-06-29 05:09:07,636 main.py:57] epoch 5808, training loss: 7934.93, average training loss: 7485.98, base loss: 16105.60
[INFO 2017-06-29 05:09:10,646 main.py:57] epoch 5809, training loss: 7925.82, average training loss: 7486.89, base loss: 16105.91
[INFO 2017-06-29 05:09:13,743 main.py:57] epoch 5810, training loss: 7478.29, average training loss: 7487.18, base loss: 16106.14
[INFO 2017-06-29 05:09:16,685 main.py:57] epoch 5811, training loss: 7822.22, average training loss: 7487.57, base loss: 16107.01
[INFO 2017-06-29 05:09:19,785 main.py:57] epoch 5812, training loss: 7132.12, average training loss: 7486.74, base loss: 16106.89
[INFO 2017-06-29 05:09:22,788 main.py:57] epoch 5813, training loss: 6204.75, average training loss: 7485.40, base loss: 16106.46
[INFO 2017-06-29 05:09:25,906 main.py:57] epoch 5814, training loss: 7456.00, average training loss: 7485.21, base loss: 16106.71
[INFO 2017-06-29 05:09:28,969 main.py:57] epoch 5815, training loss: 7356.28, average training loss: 7485.04, base loss: 16106.54
[INFO 2017-06-29 05:09:32,040 main.py:57] epoch 5816, training loss: 7911.90, average training loss: 7485.53, base loss: 16106.51
[INFO 2017-06-29 05:09:35,070 main.py:57] epoch 5817, training loss: 8842.58, average training loss: 7487.63, base loss: 16107.30
[INFO 2017-06-29 05:09:38,151 main.py:57] epoch 5818, training loss: 7645.72, average training loss: 7488.06, base loss: 16107.24
[INFO 2017-06-29 05:09:41,248 main.py:57] epoch 5819, training loss: 6717.12, average training loss: 7485.43, base loss: 16106.82
[INFO 2017-06-29 05:09:44,278 main.py:57] epoch 5820, training loss: 6939.58, average training loss: 7484.11, base loss: 16106.82
[INFO 2017-06-29 05:09:47,291 main.py:57] epoch 5821, training loss: 7673.60, average training loss: 7483.80, base loss: 16107.19
[INFO 2017-06-29 05:09:50,294 main.py:57] epoch 5822, training loss: 7264.82, average training loss: 7482.29, base loss: 16106.60
[INFO 2017-06-29 05:09:53,391 main.py:57] epoch 5823, training loss: 7990.36, average training loss: 7482.44, base loss: 16106.91
[INFO 2017-06-29 05:09:56,432 main.py:57] epoch 5824, training loss: 7090.93, average training loss: 7482.05, base loss: 16106.65
[INFO 2017-06-29 05:09:59,568 main.py:57] epoch 5825, training loss: 7233.50, average training loss: 7481.73, base loss: 16107.08
[INFO 2017-06-29 05:10:02,572 main.py:57] epoch 5826, training loss: 7463.49, average training loss: 7481.62, base loss: 16106.99
[INFO 2017-06-29 05:10:05,782 main.py:57] epoch 5827, training loss: 7028.55, average training loss: 7480.55, base loss: 16106.78
[INFO 2017-06-29 05:10:08,826 main.py:57] epoch 5828, training loss: 7225.80, average training loss: 7480.67, base loss: 16106.68
[INFO 2017-06-29 05:10:11,858 main.py:57] epoch 5829, training loss: 7594.47, average training loss: 7481.16, base loss: 16106.49
[INFO 2017-06-29 05:10:14,990 main.py:57] epoch 5830, training loss: 8674.48, average training loss: 7482.91, base loss: 16107.18
[INFO 2017-06-29 05:10:18,061 main.py:57] epoch 5831, training loss: 6576.77, average training loss: 7482.78, base loss: 16106.37
[INFO 2017-06-29 05:10:21,117 main.py:57] epoch 5832, training loss: 7889.79, average training loss: 7483.79, base loss: 16106.40
[INFO 2017-06-29 05:10:24,217 main.py:57] epoch 5833, training loss: 8252.83, average training loss: 7484.46, base loss: 16106.54
[INFO 2017-06-29 05:10:27,308 main.py:57] epoch 5834, training loss: 7233.90, average training loss: 7484.30, base loss: 16106.25
[INFO 2017-06-29 05:10:30,379 main.py:57] epoch 5835, training loss: 7126.47, average training loss: 7483.69, base loss: 16105.58
[INFO 2017-06-29 05:10:33,393 main.py:57] epoch 5836, training loss: 7500.62, average training loss: 7484.70, base loss: 16106.11
[INFO 2017-06-29 05:10:36,460 main.py:57] epoch 5837, training loss: 8339.30, average training loss: 7485.86, base loss: 16106.83
[INFO 2017-06-29 05:10:39,474 main.py:57] epoch 5838, training loss: 6939.81, average training loss: 7483.72, base loss: 16106.94
[INFO 2017-06-29 05:10:42,492 main.py:57] epoch 5839, training loss: 6999.78, average training loss: 7484.03, base loss: 16106.71
[INFO 2017-06-29 05:10:45,527 main.py:57] epoch 5840, training loss: 8333.03, average training loss: 7484.61, base loss: 16107.57
[INFO 2017-06-29 05:10:48,553 main.py:57] epoch 5841, training loss: 8502.59, average training loss: 7484.79, base loss: 16108.16
[INFO 2017-06-29 05:10:51,631 main.py:57] epoch 5842, training loss: 7858.94, average training loss: 7485.22, base loss: 16109.11
[INFO 2017-06-29 05:10:54,761 main.py:57] epoch 5843, training loss: 7645.70, average training loss: 7485.98, base loss: 16110.01
[INFO 2017-06-29 05:10:57,741 main.py:57] epoch 5844, training loss: 7700.64, average training loss: 7486.90, base loss: 16109.79
[INFO 2017-06-29 05:11:00,736 main.py:57] epoch 5845, training loss: 7360.36, average training loss: 7486.21, base loss: 16109.77
[INFO 2017-06-29 05:11:03,735 main.py:57] epoch 5846, training loss: 7259.27, average training loss: 7484.37, base loss: 16109.65
[INFO 2017-06-29 05:11:06,864 main.py:57] epoch 5847, training loss: 6401.78, average training loss: 7482.83, base loss: 16109.08
[INFO 2017-06-29 05:11:09,846 main.py:57] epoch 5848, training loss: 6928.69, average training loss: 7482.93, base loss: 16108.81
[INFO 2017-06-29 05:11:12,898 main.py:57] epoch 5849, training loss: 7257.22, average training loss: 7482.76, base loss: 16109.07
[INFO 2017-06-29 05:11:15,927 main.py:57] epoch 5850, training loss: 6901.79, average training loss: 7482.00, base loss: 16108.79
[INFO 2017-06-29 05:11:19,065 main.py:57] epoch 5851, training loss: 8383.28, average training loss: 7481.92, base loss: 16109.64
[INFO 2017-06-29 05:11:22,129 main.py:57] epoch 5852, training loss: 7419.14, average training loss: 7480.99, base loss: 16109.92
[INFO 2017-06-29 05:11:25,166 main.py:57] epoch 5853, training loss: 7525.91, average training loss: 7481.42, base loss: 16109.97
[INFO 2017-06-29 05:11:28,252 main.py:57] epoch 5854, training loss: 7473.29, average training loss: 7481.69, base loss: 16109.67
[INFO 2017-06-29 05:11:31,340 main.py:57] epoch 5855, training loss: 7802.31, average training loss: 7482.40, base loss: 16109.99
[INFO 2017-06-29 05:11:34,366 main.py:57] epoch 5856, training loss: 6527.35, average training loss: 7481.61, base loss: 16109.22
[INFO 2017-06-29 05:11:37,398 main.py:57] epoch 5857, training loss: 7415.94, average training loss: 7480.81, base loss: 16108.86
[INFO 2017-06-29 05:11:40,422 main.py:57] epoch 5858, training loss: 7619.13, average training loss: 7481.17, base loss: 16109.80
[INFO 2017-06-29 05:11:43,487 main.py:57] epoch 5859, training loss: 7817.52, average training loss: 7482.25, base loss: 16110.60
[INFO 2017-06-29 05:11:46,576 main.py:57] epoch 5860, training loss: 6812.86, average training loss: 7481.03, base loss: 16110.01
[INFO 2017-06-29 05:11:49,608 main.py:57] epoch 5861, training loss: 7076.34, average training loss: 7481.74, base loss: 16109.74
[INFO 2017-06-29 05:11:52,767 main.py:57] epoch 5862, training loss: 7101.17, average training loss: 7481.29, base loss: 16109.64
[INFO 2017-06-29 05:11:55,834 main.py:57] epoch 5863, training loss: 7536.50, average training loss: 7481.28, base loss: 16110.10
[INFO 2017-06-29 05:11:58,902 main.py:57] epoch 5864, training loss: 8323.43, average training loss: 7481.22, base loss: 16110.69
[INFO 2017-06-29 05:12:02,016 main.py:57] epoch 5865, training loss: 6813.81, average training loss: 7480.65, base loss: 16110.51
[INFO 2017-06-29 05:12:05,037 main.py:57] epoch 5866, training loss: 6671.58, average training loss: 7479.96, base loss: 16110.44
[INFO 2017-06-29 05:12:08,073 main.py:57] epoch 5867, training loss: 7146.10, average training loss: 7478.94, base loss: 16110.49
[INFO 2017-06-29 05:12:11,131 main.py:57] epoch 5868, training loss: 7421.74, average training loss: 7480.05, base loss: 16110.84
[INFO 2017-06-29 05:12:14,162 main.py:57] epoch 5869, training loss: 7580.30, average training loss: 7479.69, base loss: 16110.68
[INFO 2017-06-29 05:12:17,213 main.py:57] epoch 5870, training loss: 6903.95, average training loss: 7477.68, base loss: 16110.19
[INFO 2017-06-29 05:12:20,239 main.py:57] epoch 5871, training loss: 6417.49, average training loss: 7477.26, base loss: 16109.78
[INFO 2017-06-29 05:12:23,273 main.py:57] epoch 5872, training loss: 6659.47, average training loss: 7476.29, base loss: 16109.46
[INFO 2017-06-29 05:12:26,333 main.py:57] epoch 5873, training loss: 6843.76, average training loss: 7474.97, base loss: 16108.91
[INFO 2017-06-29 05:12:29,518 main.py:57] epoch 5874, training loss: 6937.81, average training loss: 7473.26, base loss: 16108.51
[INFO 2017-06-29 05:12:32,621 main.py:57] epoch 5875, training loss: 8420.92, average training loss: 7474.23, base loss: 16109.37
[INFO 2017-06-29 05:12:35,674 main.py:57] epoch 5876, training loss: 8127.36, average training loss: 7474.95, base loss: 16109.72
[INFO 2017-06-29 05:12:38,722 main.py:57] epoch 5877, training loss: 8493.91, average training loss: 7476.52, base loss: 16110.47
[INFO 2017-06-29 05:12:41,735 main.py:57] epoch 5878, training loss: 7266.91, average training loss: 7476.22, base loss: 16110.63
[INFO 2017-06-29 05:12:44,774 main.py:57] epoch 5879, training loss: 7462.36, average training loss: 7475.81, base loss: 16111.00
[INFO 2017-06-29 05:12:47,830 main.py:57] epoch 5880, training loss: 8130.62, average training loss: 7476.71, base loss: 16111.64
[INFO 2017-06-29 05:12:50,807 main.py:57] epoch 5881, training loss: 6335.27, average training loss: 7476.29, base loss: 16110.80
[INFO 2017-06-29 05:12:53,829 main.py:57] epoch 5882, training loss: 7433.24, average training loss: 7476.44, base loss: 16109.89
[INFO 2017-06-29 05:12:56,897 main.py:57] epoch 5883, training loss: 8403.25, average training loss: 7477.11, base loss: 16110.26
[INFO 2017-06-29 05:12:59,948 main.py:57] epoch 5884, training loss: 7910.45, average training loss: 7477.16, base loss: 16110.04
[INFO 2017-06-29 05:13:03,064 main.py:57] epoch 5885, training loss: 7734.88, average training loss: 7477.88, base loss: 16109.78
[INFO 2017-06-29 05:13:06,181 main.py:57] epoch 5886, training loss: 7954.11, average training loss: 7478.70, base loss: 16110.20
[INFO 2017-06-29 05:13:09,244 main.py:57] epoch 5887, training loss: 8813.38, average training loss: 7479.99, base loss: 16110.95
[INFO 2017-06-29 05:13:12,328 main.py:57] epoch 5888, training loss: 7130.37, average training loss: 7479.56, base loss: 16110.61
[INFO 2017-06-29 05:13:15,296 main.py:57] epoch 5889, training loss: 7175.25, average training loss: 7479.41, base loss: 16110.33
[INFO 2017-06-29 05:13:18,346 main.py:57] epoch 5890, training loss: 6735.79, average training loss: 7478.19, base loss: 16110.16
[INFO 2017-06-29 05:13:21,411 main.py:57] epoch 5891, training loss: 6846.54, average training loss: 7477.55, base loss: 16109.69
[INFO 2017-06-29 05:13:24,486 main.py:57] epoch 5892, training loss: 7489.37, average training loss: 7477.01, base loss: 16109.60
[INFO 2017-06-29 05:13:27,483 main.py:57] epoch 5893, training loss: 7930.70, average training loss: 7477.67, base loss: 16110.23
[INFO 2017-06-29 05:13:30,588 main.py:57] epoch 5894, training loss: 7288.32, average training loss: 7478.23, base loss: 16109.84
[INFO 2017-06-29 05:13:33,672 main.py:57] epoch 5895, training loss: 7765.42, average training loss: 7478.45, base loss: 16109.28
[INFO 2017-06-29 05:13:36,769 main.py:57] epoch 5896, training loss: 7354.02, average training loss: 7479.15, base loss: 16109.59
[INFO 2017-06-29 05:13:39,829 main.py:57] epoch 5897, training loss: 7164.48, average training loss: 7480.35, base loss: 16110.17
[INFO 2017-06-29 05:13:42,880 main.py:57] epoch 5898, training loss: 7397.47, average training loss: 7480.78, base loss: 16110.34
[INFO 2017-06-29 05:13:45,880 main.py:57] epoch 5899, training loss: 6635.40, average training loss: 7479.39, base loss: 16109.58
[INFO 2017-06-29 05:13:45,881 main.py:59] epoch 5899, testing
[INFO 2017-06-29 05:13:58,527 main.py:104] average testing loss: 8163.77, base loss: 17017.53
[INFO 2017-06-29 05:13:58,528 main.py:105] improve_loss: 8853.76, improve_percent: 0.52
[INFO 2017-06-29 05:13:58,529 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:14:01,568 main.py:57] epoch 5900, training loss: 7088.31, average training loss: 7478.41, base loss: 16109.18
[INFO 2017-06-29 05:14:04,554 main.py:57] epoch 5901, training loss: 7034.09, average training loss: 7478.84, base loss: 16108.88
[INFO 2017-06-29 05:14:07,623 main.py:57] epoch 5902, training loss: 7400.07, average training loss: 7478.28, base loss: 16108.86
[INFO 2017-06-29 05:14:10,726 main.py:57] epoch 5903, training loss: 7797.16, average training loss: 7477.13, base loss: 16109.43
[INFO 2017-06-29 05:14:13,754 main.py:57] epoch 5904, training loss: 6619.90, average training loss: 7475.64, base loss: 16108.07
[INFO 2017-06-29 05:14:16,739 main.py:57] epoch 5905, training loss: 8315.16, average training loss: 7476.81, base loss: 16108.50
[INFO 2017-06-29 05:14:19,814 main.py:57] epoch 5906, training loss: 7981.21, average training loss: 7477.52, base loss: 16108.78
[INFO 2017-06-29 05:14:22,881 main.py:57] epoch 5907, training loss: 7968.18, average training loss: 7478.47, base loss: 16109.10
[INFO 2017-06-29 05:14:25,958 main.py:57] epoch 5908, training loss: 7470.46, average training loss: 7478.82, base loss: 16109.01
[INFO 2017-06-29 05:14:28,957 main.py:57] epoch 5909, training loss: 7668.38, average training loss: 7479.17, base loss: 16109.03
[INFO 2017-06-29 05:14:31,934 main.py:57] epoch 5910, training loss: 8317.95, average training loss: 7480.20, base loss: 16109.72
[INFO 2017-06-29 05:14:34,972 main.py:57] epoch 5911, training loss: 7352.68, average training loss: 7479.91, base loss: 16109.60
[INFO 2017-06-29 05:14:38,026 main.py:57] epoch 5912, training loss: 7987.42, average training loss: 7480.66, base loss: 16110.15
[INFO 2017-06-29 05:14:41,097 main.py:57] epoch 5913, training loss: 7464.78, average training loss: 7480.62, base loss: 16110.72
[INFO 2017-06-29 05:14:44,103 main.py:57] epoch 5914, training loss: 6738.15, average training loss: 7480.29, base loss: 16109.97
[INFO 2017-06-29 05:14:47,141 main.py:57] epoch 5915, training loss: 7201.30, average training loss: 7479.47, base loss: 16110.14
[INFO 2017-06-29 05:14:50,207 main.py:57] epoch 5916, training loss: 6492.13, average training loss: 7476.90, base loss: 16109.40
[INFO 2017-06-29 05:14:53,151 main.py:57] epoch 5917, training loss: 7228.47, average training loss: 7475.86, base loss: 16109.44
[INFO 2017-06-29 05:14:56,216 main.py:57] epoch 5918, training loss: 7657.26, average training loss: 7476.22, base loss: 16109.91
[INFO 2017-06-29 05:14:59,298 main.py:57] epoch 5919, training loss: 7547.20, average training loss: 7476.49, base loss: 16109.69
[INFO 2017-06-29 05:15:02,322 main.py:57] epoch 5920, training loss: 7906.93, average training loss: 7477.24, base loss: 16109.60
[INFO 2017-06-29 05:15:05,400 main.py:57] epoch 5921, training loss: 6836.73, average training loss: 7476.32, base loss: 16108.96
[INFO 2017-06-29 05:15:08,529 main.py:57] epoch 5922, training loss: 7221.12, average training loss: 7475.93, base loss: 16108.78
[INFO 2017-06-29 05:15:11,562 main.py:57] epoch 5923, training loss: 7315.87, average training loss: 7476.70, base loss: 16108.76
[INFO 2017-06-29 05:15:14,596 main.py:57] epoch 5924, training loss: 7150.02, average training loss: 7477.02, base loss: 16108.26
[INFO 2017-06-29 05:15:17,674 main.py:57] epoch 5925, training loss: 6992.64, average training loss: 7476.95, base loss: 16107.91
[INFO 2017-06-29 05:15:20,658 main.py:57] epoch 5926, training loss: 7329.84, average training loss: 7477.00, base loss: 16108.00
[INFO 2017-06-29 05:15:23,714 main.py:57] epoch 5927, training loss: 7227.98, average training loss: 7475.95, base loss: 16107.75
[INFO 2017-06-29 05:15:26,754 main.py:57] epoch 5928, training loss: 7546.20, average training loss: 7476.52, base loss: 16108.12
[INFO 2017-06-29 05:15:29,786 main.py:57] epoch 5929, training loss: 6828.47, average training loss: 7476.15, base loss: 16107.91
[INFO 2017-06-29 05:15:32,796 main.py:57] epoch 5930, training loss: 6945.19, average training loss: 7475.88, base loss: 16107.61
[INFO 2017-06-29 05:15:35,849 main.py:57] epoch 5931, training loss: 7365.47, average training loss: 7475.16, base loss: 16107.75
[INFO 2017-06-29 05:15:38,887 main.py:57] epoch 5932, training loss: 8186.02, average training loss: 7476.28, base loss: 16107.54
[INFO 2017-06-29 05:15:42,001 main.py:57] epoch 5933, training loss: 6895.29, average training loss: 7475.09, base loss: 16107.07
[INFO 2017-06-29 05:15:45,001 main.py:57] epoch 5934, training loss: 6109.51, average training loss: 7474.40, base loss: 16106.36
[INFO 2017-06-29 05:15:48,014 main.py:57] epoch 5935, training loss: 7923.90, average training loss: 7474.74, base loss: 16106.74
[INFO 2017-06-29 05:15:51,054 main.py:57] epoch 5936, training loss: 7964.94, average training loss: 7475.53, base loss: 16107.26
[INFO 2017-06-29 05:15:54,108 main.py:57] epoch 5937, training loss: 7910.95, average training loss: 7475.09, base loss: 16107.55
[INFO 2017-06-29 05:15:57,223 main.py:57] epoch 5938, training loss: 6641.65, average training loss: 7475.35, base loss: 16107.41
[INFO 2017-06-29 05:16:00,257 main.py:57] epoch 5939, training loss: 7224.46, average training loss: 7474.94, base loss: 16107.10
[INFO 2017-06-29 05:16:03,285 main.py:57] epoch 5940, training loss: 7514.46, average training loss: 7475.04, base loss: 16106.85
[INFO 2017-06-29 05:16:06,324 main.py:57] epoch 5941, training loss: 7696.63, average training loss: 7475.06, base loss: 16106.28
[INFO 2017-06-29 05:16:09,382 main.py:57] epoch 5942, training loss: 8513.35, average training loss: 7476.94, base loss: 16106.38
[INFO 2017-06-29 05:16:12,411 main.py:57] epoch 5943, training loss: 7634.18, average training loss: 7477.59, base loss: 16106.33
[INFO 2017-06-29 05:16:15,455 main.py:57] epoch 5944, training loss: 8135.77, average training loss: 7479.11, base loss: 16106.78
[INFO 2017-06-29 05:16:18,515 main.py:57] epoch 5945, training loss: 7724.62, average training loss: 7479.65, base loss: 16106.52
[INFO 2017-06-29 05:16:21,560 main.py:57] epoch 5946, training loss: 6824.54, average training loss: 7479.41, base loss: 16106.32
[INFO 2017-06-29 05:16:24,646 main.py:57] epoch 5947, training loss: 8051.23, average training loss: 7480.02, base loss: 16106.98
[INFO 2017-06-29 05:16:27,642 main.py:57] epoch 5948, training loss: 7632.90, average training loss: 7479.63, base loss: 16107.11
[INFO 2017-06-29 05:16:30,689 main.py:57] epoch 5949, training loss: 7250.12, average training loss: 7479.16, base loss: 16106.67
[INFO 2017-06-29 05:16:33,707 main.py:57] epoch 5950, training loss: 7408.56, average training loss: 7480.17, base loss: 16107.02
[INFO 2017-06-29 05:16:36,818 main.py:57] epoch 5951, training loss: 7574.64, average training loss: 7479.34, base loss: 16106.73
[INFO 2017-06-29 05:16:39,845 main.py:57] epoch 5952, training loss: 8234.11, average training loss: 7480.13, base loss: 16107.15
[INFO 2017-06-29 05:16:42,866 main.py:57] epoch 5953, training loss: 7674.50, average training loss: 7479.67, base loss: 16107.17
[INFO 2017-06-29 05:16:45,958 main.py:57] epoch 5954, training loss: 7985.61, average training loss: 7479.74, base loss: 16106.70
[INFO 2017-06-29 05:16:49,030 main.py:57] epoch 5955, training loss: 8014.36, average training loss: 7480.67, base loss: 16107.03
[INFO 2017-06-29 05:16:52,028 main.py:57] epoch 5956, training loss: 7867.13, average training loss: 7482.04, base loss: 16107.61
[INFO 2017-06-29 05:16:55,127 main.py:57] epoch 5957, training loss: 7531.62, average training loss: 7481.82, base loss: 16107.59
[INFO 2017-06-29 05:16:58,201 main.py:57] epoch 5958, training loss: 9271.70, average training loss: 7483.43, base loss: 16108.56
[INFO 2017-06-29 05:17:01,316 main.py:57] epoch 5959, training loss: 7310.20, average training loss: 7483.55, base loss: 16108.41
[INFO 2017-06-29 05:17:04,384 main.py:57] epoch 5960, training loss: 6855.65, average training loss: 7483.56, base loss: 16107.99
[INFO 2017-06-29 05:17:07,423 main.py:57] epoch 5961, training loss: 6820.08, average training loss: 7482.51, base loss: 16108.24
[INFO 2017-06-29 05:17:10,497 main.py:57] epoch 5962, training loss: 7744.12, average training loss: 7483.38, base loss: 16107.98
[INFO 2017-06-29 05:17:13,582 main.py:57] epoch 5963, training loss: 6988.43, average training loss: 7483.37, base loss: 16107.77
[INFO 2017-06-29 05:17:16,709 main.py:57] epoch 5964, training loss: 6452.36, average training loss: 7482.06, base loss: 16107.13
[INFO 2017-06-29 05:17:19,705 main.py:57] epoch 5965, training loss: 7297.06, average training loss: 7482.02, base loss: 16107.10
[INFO 2017-06-29 05:17:22,764 main.py:57] epoch 5966, training loss: 7152.55, average training loss: 7481.79, base loss: 16107.06
[INFO 2017-06-29 05:17:25,854 main.py:57] epoch 5967, training loss: 8787.67, average training loss: 7482.60, base loss: 16107.78
[INFO 2017-06-29 05:17:28,909 main.py:57] epoch 5968, training loss: 8031.22, average training loss: 7483.25, base loss: 16107.92
[INFO 2017-06-29 05:17:31,926 main.py:57] epoch 5969, training loss: 7564.49, average training loss: 7483.32, base loss: 16107.52
[INFO 2017-06-29 05:17:35,012 main.py:57] epoch 5970, training loss: 6680.31, average training loss: 7483.19, base loss: 16106.89
[INFO 2017-06-29 05:17:37,994 main.py:57] epoch 5971, training loss: 8430.47, average training loss: 7483.34, base loss: 16107.13
[INFO 2017-06-29 05:17:41,029 main.py:57] epoch 5972, training loss: 8260.82, average training loss: 7484.81, base loss: 16108.31
[INFO 2017-06-29 05:17:44,099 main.py:57] epoch 5973, training loss: 7255.91, average training loss: 7484.83, base loss: 16108.84
[INFO 2017-06-29 05:17:47,169 main.py:57] epoch 5974, training loss: 6976.09, average training loss: 7483.26, base loss: 16109.22
[INFO 2017-06-29 05:17:50,233 main.py:57] epoch 5975, training loss: 7048.74, average training loss: 7483.14, base loss: 16109.00
[INFO 2017-06-29 05:17:53,317 main.py:57] epoch 5976, training loss: 6863.08, average training loss: 7481.92, base loss: 16108.93
[INFO 2017-06-29 05:17:56,326 main.py:57] epoch 5977, training loss: 8246.17, average training loss: 7481.46, base loss: 16109.76
[INFO 2017-06-29 05:17:59,419 main.py:57] epoch 5978, training loss: 7324.77, average training loss: 7480.21, base loss: 16109.87
[INFO 2017-06-29 05:18:02,457 main.py:57] epoch 5979, training loss: 7155.47, average training loss: 7479.72, base loss: 16109.93
[INFO 2017-06-29 05:18:05,532 main.py:57] epoch 5980, training loss: 6832.02, average training loss: 7479.89, base loss: 16109.86
[INFO 2017-06-29 05:18:08,535 main.py:57] epoch 5981, training loss: 6599.31, average training loss: 7478.25, base loss: 16109.08
[INFO 2017-06-29 05:18:11,617 main.py:57] epoch 5982, training loss: 7909.60, average training loss: 7478.56, base loss: 16108.77
[INFO 2017-06-29 05:18:14,634 main.py:57] epoch 5983, training loss: 7120.76, average training loss: 7478.70, base loss: 16108.61
[INFO 2017-06-29 05:18:17,714 main.py:57] epoch 5984, training loss: 7361.60, average training loss: 7476.03, base loss: 16108.37
[INFO 2017-06-29 05:18:20,801 main.py:57] epoch 5985, training loss: 9139.88, average training loss: 7478.15, base loss: 16108.85
[INFO 2017-06-29 05:18:23,847 main.py:57] epoch 5986, training loss: 7283.53, average training loss: 7477.87, base loss: 16108.65
[INFO 2017-06-29 05:18:26,893 main.py:57] epoch 5987, training loss: 7057.50, average training loss: 7476.54, base loss: 16108.42
[INFO 2017-06-29 05:18:29,906 main.py:57] epoch 5988, training loss: 7379.42, average training loss: 7476.81, base loss: 16108.49
[INFO 2017-06-29 05:18:32,930 main.py:57] epoch 5989, training loss: 8566.19, average training loss: 7477.92, base loss: 16109.15
[INFO 2017-06-29 05:18:35,974 main.py:57] epoch 5990, training loss: 6739.02, average training loss: 7478.01, base loss: 16109.27
[INFO 2017-06-29 05:18:39,016 main.py:57] epoch 5991, training loss: 6852.63, average training loss: 7477.53, base loss: 16109.25
[INFO 2017-06-29 05:18:42,083 main.py:57] epoch 5992, training loss: 7047.39, average training loss: 7476.40, base loss: 16109.27
[INFO 2017-06-29 05:18:45,206 main.py:57] epoch 5993, training loss: 8913.05, average training loss: 7477.27, base loss: 16110.21
[INFO 2017-06-29 05:18:48,261 main.py:57] epoch 5994, training loss: 6412.41, average training loss: 7475.89, base loss: 16109.54
[INFO 2017-06-29 05:18:51,299 main.py:57] epoch 5995, training loss: 7705.33, average training loss: 7476.12, base loss: 16109.45
[INFO 2017-06-29 05:18:54,316 main.py:57] epoch 5996, training loss: 7643.36, average training loss: 7476.41, base loss: 16109.86
[INFO 2017-06-29 05:18:57,402 main.py:57] epoch 5997, training loss: 6874.27, average training loss: 7475.05, base loss: 16109.55
[INFO 2017-06-29 05:19:00,481 main.py:57] epoch 5998, training loss: 8579.37, average training loss: 7476.98, base loss: 16109.83
[INFO 2017-06-29 05:19:03,576 main.py:57] epoch 5999, training loss: 6837.15, average training loss: 7476.65, base loss: 16109.39
[INFO 2017-06-29 05:19:03,576 main.py:59] epoch 5999, testing
[INFO 2017-06-29 05:19:16,233 main.py:104] average testing loss: 8178.77, base loss: 16899.66
[INFO 2017-06-29 05:19:16,233 main.py:105] improve_loss: 8720.89, improve_percent: 0.52
[INFO 2017-06-29 05:19:16,234 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:19:19,317 main.py:57] epoch 6000, training loss: 6680.40, average training loss: 7476.65, base loss: 16109.32
[INFO 2017-06-29 05:19:22,291 main.py:57] epoch 6001, training loss: 6488.95, average training loss: 7475.53, base loss: 16109.10
[INFO 2017-06-29 05:19:25,423 main.py:57] epoch 6002, training loss: 7789.31, average training loss: 7474.13, base loss: 16109.71
[INFO 2017-06-29 05:19:28,469 main.py:57] epoch 6003, training loss: 7045.06, average training loss: 7474.34, base loss: 16109.01
[INFO 2017-06-29 05:19:31,514 main.py:57] epoch 6004, training loss: 7531.68, average training loss: 7474.40, base loss: 16108.79
[INFO 2017-06-29 05:19:34,564 main.py:57] epoch 6005, training loss: 7895.59, average training loss: 7475.57, base loss: 16108.60
[INFO 2017-06-29 05:19:37,602 main.py:57] epoch 6006, training loss: 6086.55, average training loss: 7475.34, base loss: 16108.51
[INFO 2017-06-29 05:19:40,671 main.py:57] epoch 6007, training loss: 6882.01, average training loss: 7474.29, base loss: 16108.65
[INFO 2017-06-29 05:19:43,706 main.py:57] epoch 6008, training loss: 6579.00, average training loss: 7472.66, base loss: 16108.08
[INFO 2017-06-29 05:19:46,759 main.py:57] epoch 6009, training loss: 8247.76, average training loss: 7472.19, base loss: 16108.20
[INFO 2017-06-29 05:19:49,806 main.py:57] epoch 6010, training loss: 7011.47, average training loss: 7472.74, base loss: 16107.88
[INFO 2017-06-29 05:19:52,832 main.py:57] epoch 6011, training loss: 6859.25, average training loss: 7472.47, base loss: 16107.58
[INFO 2017-06-29 05:19:55,875 main.py:57] epoch 6012, training loss: 7456.18, average training loss: 7470.79, base loss: 16107.67
[INFO 2017-06-29 05:19:58,892 main.py:57] epoch 6013, training loss: 6986.63, average training loss: 7470.05, base loss: 16108.03
[INFO 2017-06-29 05:20:01,929 main.py:57] epoch 6014, training loss: 8140.29, average training loss: 7470.95, base loss: 16108.12
[INFO 2017-06-29 05:20:04,960 main.py:57] epoch 6015, training loss: 7057.55, average training loss: 7470.27, base loss: 16107.78
[INFO 2017-06-29 05:20:08,118 main.py:57] epoch 6016, training loss: 7628.86, average training loss: 7469.90, base loss: 16108.08
[INFO 2017-06-29 05:20:11,183 main.py:57] epoch 6017, training loss: 8617.16, average training loss: 7470.53, base loss: 16108.71
[INFO 2017-06-29 05:20:14,224 main.py:57] epoch 6018, training loss: 7976.00, average training loss: 7471.96, base loss: 16109.06
[INFO 2017-06-29 05:20:17,318 main.py:57] epoch 6019, training loss: 7841.11, average training loss: 7470.55, base loss: 16109.18
[INFO 2017-06-29 05:20:20,351 main.py:57] epoch 6020, training loss: 7232.81, average training loss: 7470.61, base loss: 16109.10
[INFO 2017-06-29 05:20:23,405 main.py:57] epoch 6021, training loss: 7770.57, average training loss: 7470.36, base loss: 16109.17
[INFO 2017-06-29 05:20:26,409 main.py:57] epoch 6022, training loss: 7202.19, average training loss: 7468.67, base loss: 16108.74
[INFO 2017-06-29 05:20:29,456 main.py:57] epoch 6023, training loss: 7201.48, average training loss: 7468.46, base loss: 16108.73
[INFO 2017-06-29 05:20:32,519 main.py:57] epoch 6024, training loss: 7665.85, average training loss: 7467.93, base loss: 16108.63
[INFO 2017-06-29 05:20:35,622 main.py:57] epoch 6025, training loss: 7347.91, average training loss: 7467.72, base loss: 16108.22
[INFO 2017-06-29 05:20:38,711 main.py:57] epoch 6026, training loss: 7939.91, average training loss: 7468.56, base loss: 16108.35
[INFO 2017-06-29 05:20:41,768 main.py:57] epoch 6027, training loss: 6447.41, average training loss: 7467.14, base loss: 16107.66
[INFO 2017-06-29 05:20:44,895 main.py:57] epoch 6028, training loss: 6813.27, average training loss: 7464.49, base loss: 16107.19
[INFO 2017-06-29 05:20:47,963 main.py:57] epoch 6029, training loss: 6589.23, average training loss: 7464.09, base loss: 16106.38
[INFO 2017-06-29 05:20:50,946 main.py:57] epoch 6030, training loss: 7063.69, average training loss: 7464.70, base loss: 16106.32
[INFO 2017-06-29 05:20:53,947 main.py:57] epoch 6031, training loss: 7813.08, average training loss: 7464.97, base loss: 16106.14
[INFO 2017-06-29 05:20:57,067 main.py:57] epoch 6032, training loss: 7467.36, average training loss: 7465.14, base loss: 16106.63
[INFO 2017-06-29 05:21:00,166 main.py:57] epoch 6033, training loss: 7134.44, average training loss: 7465.88, base loss: 16106.33
[INFO 2017-06-29 05:21:03,235 main.py:57] epoch 6034, training loss: 6811.35, average training loss: 7464.86, base loss: 16105.77
[INFO 2017-06-29 05:21:06,291 main.py:57] epoch 6035, training loss: 8538.44, average training loss: 7465.33, base loss: 16106.36
[INFO 2017-06-29 05:21:09,344 main.py:57] epoch 6036, training loss: 7112.11, average training loss: 7464.88, base loss: 16106.01
[INFO 2017-06-29 05:21:12,452 main.py:57] epoch 6037, training loss: 6062.45, average training loss: 7464.04, base loss: 16105.69
[INFO 2017-06-29 05:21:15,468 main.py:57] epoch 6038, training loss: 8351.52, average training loss: 7464.73, base loss: 16106.11
[INFO 2017-06-29 05:21:18,564 main.py:57] epoch 6039, training loss: 8190.04, average training loss: 7465.80, base loss: 16106.27
[INFO 2017-06-29 05:21:21,580 main.py:57] epoch 6040, training loss: 7621.03, average training loss: 7466.44, base loss: 16106.55
[INFO 2017-06-29 05:21:24,580 main.py:57] epoch 6041, training loss: 8193.13, average training loss: 7466.90, base loss: 16107.25
[INFO 2017-06-29 05:21:27,633 main.py:57] epoch 6042, training loss: 8368.01, average training loss: 7467.75, base loss: 16107.77
[INFO 2017-06-29 05:21:30,700 main.py:57] epoch 6043, training loss: 7248.54, average training loss: 7467.03, base loss: 16107.61
[INFO 2017-06-29 05:21:33,795 main.py:57] epoch 6044, training loss: 7906.83, average training loss: 7468.07, base loss: 16107.74
[INFO 2017-06-29 05:21:36,834 main.py:57] epoch 6045, training loss: 8474.74, average training loss: 7469.72, base loss: 16107.85
[INFO 2017-06-29 05:21:39,863 main.py:57] epoch 6046, training loss: 8026.42, average training loss: 7468.84, base loss: 16107.86
[INFO 2017-06-29 05:21:42,925 main.py:57] epoch 6047, training loss: 7900.87, average training loss: 7470.04, base loss: 16108.34
[INFO 2017-06-29 05:21:46,031 main.py:57] epoch 6048, training loss: 7071.72, average training loss: 7470.09, base loss: 16108.46
[INFO 2017-06-29 05:21:49,074 main.py:57] epoch 6049, training loss: 7303.75, average training loss: 7470.21, base loss: 16108.36
[INFO 2017-06-29 05:21:52,133 main.py:57] epoch 6050, training loss: 7577.60, average training loss: 7470.30, base loss: 16108.29
[INFO 2017-06-29 05:21:55,214 main.py:57] epoch 6051, training loss: 8044.45, average training loss: 7471.13, base loss: 16108.65
[INFO 2017-06-29 05:21:58,240 main.py:57] epoch 6052, training loss: 6814.78, average training loss: 7469.24, base loss: 16108.54
[INFO 2017-06-29 05:22:01,293 main.py:57] epoch 6053, training loss: 7553.78, average training loss: 7469.53, base loss: 16108.88
[INFO 2017-06-29 05:22:04,346 main.py:57] epoch 6054, training loss: 7274.50, average training loss: 7469.25, base loss: 16108.37
[INFO 2017-06-29 05:22:07,392 main.py:57] epoch 6055, training loss: 6913.11, average training loss: 7468.94, base loss: 16107.62
[INFO 2017-06-29 05:22:10,440 main.py:57] epoch 6056, training loss: 8412.18, average training loss: 7469.88, base loss: 16108.12
[INFO 2017-06-29 05:22:13,505 main.py:57] epoch 6057, training loss: 6340.09, average training loss: 7467.18, base loss: 16107.50
[INFO 2017-06-29 05:22:16,604 main.py:57] epoch 6058, training loss: 6583.24, average training loss: 7466.36, base loss: 16107.00
[INFO 2017-06-29 05:22:19,592 main.py:57] epoch 6059, training loss: 8171.64, average training loss: 7466.99, base loss: 16107.36
[INFO 2017-06-29 05:22:22,685 main.py:57] epoch 6060, training loss: 7227.74, average training loss: 7466.95, base loss: 16107.55
[INFO 2017-06-29 05:22:25,789 main.py:57] epoch 6061, training loss: 6863.52, average training loss: 7466.80, base loss: 16107.38
[INFO 2017-06-29 05:22:28,935 main.py:57] epoch 6062, training loss: 6588.88, average training loss: 7465.81, base loss: 16106.61
[INFO 2017-06-29 05:22:32,000 main.py:57] epoch 6063, training loss: 7450.29, average training loss: 7465.55, base loss: 16106.71
[INFO 2017-06-29 05:22:35,011 main.py:57] epoch 6064, training loss: 6692.79, average training loss: 7465.33, base loss: 16106.20
[INFO 2017-06-29 05:22:38,097 main.py:57] epoch 6065, training loss: 7982.56, average training loss: 7464.13, base loss: 16106.52
[INFO 2017-06-29 05:22:41,140 main.py:57] epoch 6066, training loss: 7687.72, average training loss: 7464.24, base loss: 16106.69
[INFO 2017-06-29 05:22:44,155 main.py:57] epoch 6067, training loss: 7176.76, average training loss: 7464.94, base loss: 16106.49
[INFO 2017-06-29 05:22:47,289 main.py:57] epoch 6068, training loss: 7544.84, average training loss: 7464.67, base loss: 16106.46
[INFO 2017-06-29 05:22:50,327 main.py:57] epoch 6069, training loss: 8235.59, average training loss: 7465.87, base loss: 16107.00
[INFO 2017-06-29 05:22:53,395 main.py:57] epoch 6070, training loss: 7942.64, average training loss: 7466.78, base loss: 16106.82
[INFO 2017-06-29 05:22:56,490 main.py:57] epoch 6071, training loss: 7255.20, average training loss: 7465.74, base loss: 16106.34
[INFO 2017-06-29 05:22:59,603 main.py:57] epoch 6072, training loss: 6931.49, average training loss: 7463.75, base loss: 16106.90
[INFO 2017-06-29 05:23:02,659 main.py:57] epoch 6073, training loss: 7152.97, average training loss: 7463.44, base loss: 16106.90
[INFO 2017-06-29 05:23:05,682 main.py:57] epoch 6074, training loss: 7310.96, average training loss: 7463.01, base loss: 16107.19
[INFO 2017-06-29 05:23:08,731 main.py:57] epoch 6075, training loss: 7349.65, average training loss: 7463.81, base loss: 16107.23
[INFO 2017-06-29 05:23:11,839 main.py:57] epoch 6076, training loss: 7871.68, average training loss: 7464.14, base loss: 16107.42
[INFO 2017-06-29 05:23:14,945 main.py:57] epoch 6077, training loss: 7882.55, average training loss: 7464.76, base loss: 16107.51
[INFO 2017-06-29 05:23:17,997 main.py:57] epoch 6078, training loss: 7327.01, average training loss: 7464.80, base loss: 16107.78
[INFO 2017-06-29 05:23:21,027 main.py:57] epoch 6079, training loss: 6653.50, average training loss: 7464.02, base loss: 16107.04
[INFO 2017-06-29 05:23:24,087 main.py:57] epoch 6080, training loss: 6683.15, average training loss: 7463.21, base loss: 16106.41
[INFO 2017-06-29 05:23:27,065 main.py:57] epoch 6081, training loss: 7421.43, average training loss: 7463.28, base loss: 16106.30
[INFO 2017-06-29 05:23:30,092 main.py:57] epoch 6082, training loss: 7938.70, average training loss: 7463.59, base loss: 16106.56
[INFO 2017-06-29 05:23:33,132 main.py:57] epoch 6083, training loss: 8704.75, average training loss: 7464.84, base loss: 16107.23
[INFO 2017-06-29 05:23:36,210 main.py:57] epoch 6084, training loss: 8071.76, average training loss: 7465.43, base loss: 16106.83
[INFO 2017-06-29 05:23:39,283 main.py:57] epoch 6085, training loss: 8443.31, average training loss: 7466.02, base loss: 16107.29
[INFO 2017-06-29 05:23:42,382 main.py:57] epoch 6086, training loss: 7885.53, average training loss: 7466.61, base loss: 16107.65
[INFO 2017-06-29 05:23:45,412 main.py:57] epoch 6087, training loss: 7558.60, average training loss: 7467.12, base loss: 16107.49
[INFO 2017-06-29 05:23:48,421 main.py:57] epoch 6088, training loss: 7987.96, average training loss: 7467.99, base loss: 16107.89
[INFO 2017-06-29 05:23:51,486 main.py:57] epoch 6089, training loss: 8387.14, average training loss: 7467.88, base loss: 16108.64
[INFO 2017-06-29 05:23:54,555 main.py:57] epoch 6090, training loss: 7504.15, average training loss: 7466.72, base loss: 16108.08
[INFO 2017-06-29 05:23:57,631 main.py:57] epoch 6091, training loss: 7086.29, average training loss: 7466.85, base loss: 16107.25
[INFO 2017-06-29 05:24:00,757 main.py:57] epoch 6092, training loss: 8671.70, average training loss: 7467.70, base loss: 16107.84
[INFO 2017-06-29 05:24:03,803 main.py:57] epoch 6093, training loss: 7739.22, average training loss: 7468.48, base loss: 16107.83
[INFO 2017-06-29 05:24:06,851 main.py:57] epoch 6094, training loss: 8214.96, average training loss: 7469.55, base loss: 16108.00
[INFO 2017-06-29 05:24:09,903 main.py:57] epoch 6095, training loss: 7114.81, average training loss: 7468.90, base loss: 16107.83
[INFO 2017-06-29 05:24:13,061 main.py:57] epoch 6096, training loss: 7112.68, average training loss: 7468.33, base loss: 16107.56
[INFO 2017-06-29 05:24:16,071 main.py:57] epoch 6097, training loss: 6533.63, average training loss: 7467.96, base loss: 16107.01
[INFO 2017-06-29 05:24:19,142 main.py:57] epoch 6098, training loss: 8168.91, average training loss: 7468.61, base loss: 16107.40
[INFO 2017-06-29 05:24:22,288 main.py:57] epoch 6099, training loss: 7796.45, average training loss: 7469.58, base loss: 16107.98
[INFO 2017-06-29 05:24:22,288 main.py:59] epoch 6099, testing
[INFO 2017-06-29 05:24:34,907 main.py:104] average testing loss: 8081.16, base loss: 16874.44
[INFO 2017-06-29 05:24:34,907 main.py:105] improve_loss: 8793.28, improve_percent: 0.52
[INFO 2017-06-29 05:24:34,908 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:24:37,996 main.py:57] epoch 6100, training loss: 8030.11, average training loss: 7470.69, base loss: 16108.82
[INFO 2017-06-29 05:24:41,002 main.py:57] epoch 6101, training loss: 7700.80, average training loss: 7471.70, base loss: 16109.09
[INFO 2017-06-29 05:24:44,125 main.py:57] epoch 6102, training loss: 7611.22, average training loss: 7472.16, base loss: 16109.33
[INFO 2017-06-29 05:24:47,174 main.py:57] epoch 6103, training loss: 7067.40, average training loss: 7470.80, base loss: 16109.97
[INFO 2017-06-29 05:24:50,245 main.py:57] epoch 6104, training loss: 8279.25, average training loss: 7472.06, base loss: 16110.44
[INFO 2017-06-29 05:24:53,293 main.py:57] epoch 6105, training loss: 6468.58, average training loss: 7472.21, base loss: 16109.91
[INFO 2017-06-29 05:24:56,316 main.py:57] epoch 6106, training loss: 6997.03, average training loss: 7471.85, base loss: 16109.97
[INFO 2017-06-29 05:24:59,428 main.py:57] epoch 6107, training loss: 6940.22, average training loss: 7470.64, base loss: 16109.89
[INFO 2017-06-29 05:25:02,429 main.py:57] epoch 6108, training loss: 7651.08, average training loss: 7470.74, base loss: 16109.82
[INFO 2017-06-29 05:25:05,444 main.py:57] epoch 6109, training loss: 7323.37, average training loss: 7471.16, base loss: 16109.79
[INFO 2017-06-29 05:25:08,507 main.py:57] epoch 6110, training loss: 6054.72, average training loss: 7470.15, base loss: 16108.95
[INFO 2017-06-29 05:25:11,573 main.py:57] epoch 6111, training loss: 6589.05, average training loss: 7469.23, base loss: 16108.27
[INFO 2017-06-29 05:25:14,618 main.py:57] epoch 6112, training loss: 7433.67, average training loss: 7469.87, base loss: 16107.79
[INFO 2017-06-29 05:25:17,686 main.py:57] epoch 6113, training loss: 7963.91, average training loss: 7470.12, base loss: 16107.63
[INFO 2017-06-29 05:25:20,712 main.py:57] epoch 6114, training loss: 6754.41, average training loss: 7468.68, base loss: 16107.01
[INFO 2017-06-29 05:25:23,798 main.py:57] epoch 6115, training loss: 7400.05, average training loss: 7468.40, base loss: 16106.73
[INFO 2017-06-29 05:25:26,879 main.py:57] epoch 6116, training loss: 6686.35, average training loss: 7467.30, base loss: 16106.43
[INFO 2017-06-29 05:25:29,880 main.py:57] epoch 6117, training loss: 8363.80, average training loss: 7468.92, base loss: 16106.99
[INFO 2017-06-29 05:25:32,867 main.py:57] epoch 6118, training loss: 7040.21, average training loss: 7468.62, base loss: 16106.80
[INFO 2017-06-29 05:25:35,848 main.py:57] epoch 6119, training loss: 7187.44, average training loss: 7468.27, base loss: 16106.55
[INFO 2017-06-29 05:25:38,875 main.py:57] epoch 6120, training loss: 7892.91, average training loss: 7469.31, base loss: 16106.69
[INFO 2017-06-29 05:25:41,887 main.py:57] epoch 6121, training loss: 7671.25, average training loss: 7470.00, base loss: 16106.59
[INFO 2017-06-29 05:25:44,968 main.py:57] epoch 6122, training loss: 7543.38, average training loss: 7468.07, base loss: 16106.77
[INFO 2017-06-29 05:25:48,012 main.py:57] epoch 6123, training loss: 7179.15, average training loss: 7468.63, base loss: 16106.46
[INFO 2017-06-29 05:25:51,084 main.py:57] epoch 6124, training loss: 7125.51, average training loss: 7468.34, base loss: 16106.21
[INFO 2017-06-29 05:25:54,188 main.py:57] epoch 6125, training loss: 7905.15, average training loss: 7469.13, base loss: 16106.21
[INFO 2017-06-29 05:25:57,280 main.py:57] epoch 6126, training loss: 7690.26, average training loss: 7469.42, base loss: 16106.61
[INFO 2017-06-29 05:26:00,374 main.py:57] epoch 6127, training loss: 8233.12, average training loss: 7470.01, base loss: 16107.46
[INFO 2017-06-29 05:26:03,458 main.py:57] epoch 6128, training loss: 6600.89, average training loss: 7468.48, base loss: 16106.93
[INFO 2017-06-29 05:26:06,565 main.py:57] epoch 6129, training loss: 7148.31, average training loss: 7468.19, base loss: 16107.03
[INFO 2017-06-29 05:26:09,582 main.py:57] epoch 6130, training loss: 7365.80, average training loss: 7468.41, base loss: 16106.81
[INFO 2017-06-29 05:26:12,676 main.py:57] epoch 6131, training loss: 7056.94, average training loss: 7468.58, base loss: 16106.54
[INFO 2017-06-29 05:26:15,713 main.py:57] epoch 6132, training loss: 7556.84, average training loss: 7467.72, base loss: 16106.84
[INFO 2017-06-29 05:26:18,798 main.py:57] epoch 6133, training loss: 7401.33, average training loss: 7467.63, base loss: 16106.77
[INFO 2017-06-29 05:26:21,769 main.py:57] epoch 6134, training loss: 7159.54, average training loss: 7467.84, base loss: 16106.95
[INFO 2017-06-29 05:26:24,839 main.py:57] epoch 6135, training loss: 7610.98, average training loss: 7468.87, base loss: 16106.93
[INFO 2017-06-29 05:26:27,861 main.py:57] epoch 6136, training loss: 7064.88, average training loss: 7468.28, base loss: 16106.66
[INFO 2017-06-29 05:26:30,894 main.py:57] epoch 6137, training loss: 7652.30, average training loss: 7467.65, base loss: 16106.85
[INFO 2017-06-29 05:26:33,964 main.py:57] epoch 6138, training loss: 7570.16, average training loss: 7468.06, base loss: 16106.99
[INFO 2017-06-29 05:26:37,027 main.py:57] epoch 6139, training loss: 8858.66, average training loss: 7470.02, base loss: 16107.44
[INFO 2017-06-29 05:26:40,002 main.py:57] epoch 6140, training loss: 7482.98, average training loss: 7470.15, base loss: 16107.46
[INFO 2017-06-29 05:26:43,085 main.py:57] epoch 6141, training loss: 7813.25, average training loss: 7470.18, base loss: 16107.33
[INFO 2017-06-29 05:26:46,212 main.py:57] epoch 6142, training loss: 7912.45, average training loss: 7469.54, base loss: 16107.87
[INFO 2017-06-29 05:26:49,263 main.py:57] epoch 6143, training loss: 6908.68, average training loss: 7469.22, base loss: 16107.70
[INFO 2017-06-29 05:26:52,366 main.py:57] epoch 6144, training loss: 7637.36, average training loss: 7469.91, base loss: 16107.86
[INFO 2017-06-29 05:26:55,417 main.py:57] epoch 6145, training loss: 8822.70, average training loss: 7471.17, base loss: 16108.28
[INFO 2017-06-29 05:26:58,460 main.py:57] epoch 6146, training loss: 6724.28, average training loss: 7470.20, base loss: 16108.20
[INFO 2017-06-29 05:27:01,456 main.py:57] epoch 6147, training loss: 7979.00, average training loss: 7469.80, base loss: 16108.53
[INFO 2017-06-29 05:27:04,511 main.py:57] epoch 6148, training loss: 7372.06, average training loss: 7470.62, base loss: 16108.09
[INFO 2017-06-29 05:27:07,544 main.py:57] epoch 6149, training loss: 8270.85, average training loss: 7471.66, base loss: 16107.98
[INFO 2017-06-29 05:27:10,634 main.py:57] epoch 6150, training loss: 7418.34, average training loss: 7471.73, base loss: 16108.16
[INFO 2017-06-29 05:27:13,687 main.py:57] epoch 6151, training loss: 7926.10, average training loss: 7471.45, base loss: 16108.41
[INFO 2017-06-29 05:27:16,765 main.py:57] epoch 6152, training loss: 7248.44, average training loss: 7471.50, base loss: 16108.38
[INFO 2017-06-29 05:27:19,858 main.py:57] epoch 6153, training loss: 6864.56, average training loss: 7470.46, base loss: 16108.17
[INFO 2017-06-29 05:27:22,862 main.py:57] epoch 6154, training loss: 7372.19, average training loss: 7470.82, base loss: 16108.32
[INFO 2017-06-29 05:27:25,910 main.py:57] epoch 6155, training loss: 7083.33, average training loss: 7469.96, base loss: 16108.14
[INFO 2017-06-29 05:27:28,932 main.py:57] epoch 6156, training loss: 7979.69, average training loss: 7470.91, base loss: 16108.46
[INFO 2017-06-29 05:27:31,998 main.py:57] epoch 6157, training loss: 6800.67, average training loss: 7469.63, base loss: 16108.06
[INFO 2017-06-29 05:27:35,031 main.py:57] epoch 6158, training loss: 7614.22, average training loss: 7471.00, base loss: 16108.14
[INFO 2017-06-29 05:27:38,063 main.py:57] epoch 6159, training loss: 6759.20, average training loss: 7470.39, base loss: 16107.36
[INFO 2017-06-29 05:27:41,126 main.py:57] epoch 6160, training loss: 7516.17, average training loss: 7471.04, base loss: 16106.63
[INFO 2017-06-29 05:27:44,119 main.py:57] epoch 6161, training loss: 8096.07, average training loss: 7471.68, base loss: 16107.08
[INFO 2017-06-29 05:27:47,157 main.py:57] epoch 6162, training loss: 7109.95, average training loss: 7471.52, base loss: 16107.12
[INFO 2017-06-29 05:27:50,210 main.py:57] epoch 6163, training loss: 6870.43, average training loss: 7472.10, base loss: 16107.24
[INFO 2017-06-29 05:27:53,230 main.py:57] epoch 6164, training loss: 7412.27, average training loss: 7471.94, base loss: 16107.11
[INFO 2017-06-29 05:27:56,296 main.py:57] epoch 6165, training loss: 7985.64, average training loss: 7473.08, base loss: 16107.14
[INFO 2017-06-29 05:27:59,355 main.py:57] epoch 6166, training loss: 7357.39, average training loss: 7472.68, base loss: 16107.09
[INFO 2017-06-29 05:28:02,456 main.py:57] epoch 6167, training loss: 6819.20, average training loss: 7472.59, base loss: 16106.70
[INFO 2017-06-29 05:28:05,435 main.py:57] epoch 6168, training loss: 7908.64, average training loss: 7473.88, base loss: 16107.06
[INFO 2017-06-29 05:28:08,509 main.py:57] epoch 6169, training loss: 6476.34, average training loss: 7472.38, base loss: 16106.66
[INFO 2017-06-29 05:28:11,584 main.py:57] epoch 6170, training loss: 7331.12, average training loss: 7473.04, base loss: 16106.84
[INFO 2017-06-29 05:28:14,600 main.py:57] epoch 6171, training loss: 7531.05, average training loss: 7471.66, base loss: 16107.16
[INFO 2017-06-29 05:28:17,663 main.py:57] epoch 6172, training loss: 7651.19, average training loss: 7471.43, base loss: 16107.73
[INFO 2017-06-29 05:28:20,696 main.py:57] epoch 6173, training loss: 8161.97, average training loss: 7471.94, base loss: 16108.41
[INFO 2017-06-29 05:28:23,710 main.py:57] epoch 6174, training loss: 7603.93, average training loss: 7473.34, base loss: 16108.64
[INFO 2017-06-29 05:28:26,720 main.py:57] epoch 6175, training loss: 6864.05, average training loss: 7471.72, base loss: 16108.30
[INFO 2017-06-29 05:28:29,751 main.py:57] epoch 6176, training loss: 7241.69, average training loss: 7471.44, base loss: 16108.54
[INFO 2017-06-29 05:28:32,799 main.py:57] epoch 6177, training loss: 7016.81, average training loss: 7470.99, base loss: 16108.13
[INFO 2017-06-29 05:28:35,808 main.py:57] epoch 6178, training loss: 6540.24, average training loss: 7469.90, base loss: 16108.14
[INFO 2017-06-29 05:28:38,956 main.py:57] epoch 6179, training loss: 7253.89, average training loss: 7470.16, base loss: 16108.26
[INFO 2017-06-29 05:28:41,981 main.py:57] epoch 6180, training loss: 6955.19, average training loss: 7470.07, base loss: 16107.92
[INFO 2017-06-29 05:28:45,037 main.py:57] epoch 6181, training loss: 6545.24, average training loss: 7469.56, base loss: 16107.05
[INFO 2017-06-29 05:28:48,149 main.py:57] epoch 6182, training loss: 8016.21, average training loss: 7469.99, base loss: 16107.77
[INFO 2017-06-29 05:28:51,169 main.py:57] epoch 6183, training loss: 7401.31, average training loss: 7469.18, base loss: 16107.66
[INFO 2017-06-29 05:28:54,153 main.py:57] epoch 6184, training loss: 9256.11, average training loss: 7470.89, base loss: 16108.66
[INFO 2017-06-29 05:28:57,305 main.py:57] epoch 6185, training loss: 6737.25, average training loss: 7470.81, base loss: 16108.46
[INFO 2017-06-29 05:29:00,351 main.py:57] epoch 6186, training loss: 8064.62, average training loss: 7471.97, base loss: 16108.97
[INFO 2017-06-29 05:29:03,451 main.py:57] epoch 6187, training loss: 7408.81, average training loss: 7472.36, base loss: 16108.85
[INFO 2017-06-29 05:29:06,418 main.py:57] epoch 6188, training loss: 6877.06, average training loss: 7472.30, base loss: 16108.32
[INFO 2017-06-29 05:29:09,453 main.py:57] epoch 6189, training loss: 7303.70, average training loss: 7472.76, base loss: 16107.98
[INFO 2017-06-29 05:29:12,526 main.py:57] epoch 6190, training loss: 7502.05, average training loss: 7473.09, base loss: 16107.16
[INFO 2017-06-29 05:29:15,574 main.py:57] epoch 6191, training loss: 7625.60, average training loss: 7473.01, base loss: 16106.55
[INFO 2017-06-29 05:29:18,632 main.py:57] epoch 6192, training loss: 7710.04, average training loss: 7473.61, base loss: 16106.66
[INFO 2017-06-29 05:29:21,755 main.py:57] epoch 6193, training loss: 6991.74, average training loss: 7471.47, base loss: 16106.73
[INFO 2017-06-29 05:29:24,861 main.py:57] epoch 6194, training loss: 7840.52, average training loss: 7471.88, base loss: 16106.77
[INFO 2017-06-29 05:29:27,911 main.py:57] epoch 6195, training loss: 8360.38, average training loss: 7472.90, base loss: 16106.77
[INFO 2017-06-29 05:29:30,942 main.py:57] epoch 6196, training loss: 8436.13, average training loss: 7473.01, base loss: 16107.57
[INFO 2017-06-29 05:29:34,073 main.py:57] epoch 6197, training loss: 7425.99, average training loss: 7473.78, base loss: 16107.51
[INFO 2017-06-29 05:29:37,164 main.py:57] epoch 6198, training loss: 7323.35, average training loss: 7473.39, base loss: 16108.01
[INFO 2017-06-29 05:29:40,240 main.py:57] epoch 6199, training loss: 6853.56, average training loss: 7472.44, base loss: 16108.18
[INFO 2017-06-29 05:29:40,240 main.py:59] epoch 6199, testing
[INFO 2017-06-29 05:29:52,922 main.py:104] average testing loss: 8045.57, base loss: 16876.26
[INFO 2017-06-29 05:29:52,922 main.py:105] improve_loss: 8830.70, improve_percent: 0.52
[INFO 2017-06-29 05:29:52,923 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 05:29:52,962 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:29:55,987 main.py:57] epoch 6200, training loss: 7810.26, average training loss: 7473.44, base loss: 16108.56
[INFO 2017-06-29 05:29:59,132 main.py:57] epoch 6201, training loss: 7011.91, average training loss: 7472.44, base loss: 16108.42
[INFO 2017-06-29 05:30:02,189 main.py:57] epoch 6202, training loss: 7787.72, average training loss: 7472.51, base loss: 16107.84
[INFO 2017-06-29 05:30:05,271 main.py:57] epoch 6203, training loss: 7386.08, average training loss: 7472.64, base loss: 16107.52
[INFO 2017-06-29 05:30:08,298 main.py:57] epoch 6204, training loss: 7254.26, average training loss: 7472.13, base loss: 16106.99
[INFO 2017-06-29 05:30:11,306 main.py:57] epoch 6205, training loss: 7268.57, average training loss: 7471.88, base loss: 16106.79
[INFO 2017-06-29 05:30:14,262 main.py:57] epoch 6206, training loss: 7231.52, average training loss: 7471.78, base loss: 16106.77
[INFO 2017-06-29 05:30:17,356 main.py:57] epoch 6207, training loss: 7175.89, average training loss: 7470.96, base loss: 16106.68
[INFO 2017-06-29 05:30:20,412 main.py:57] epoch 6208, training loss: 7179.41, average training loss: 7470.52, base loss: 16106.53
[INFO 2017-06-29 05:30:23,477 main.py:57] epoch 6209, training loss: 7311.55, average training loss: 7470.86, base loss: 16106.44
[INFO 2017-06-29 05:30:26,550 main.py:57] epoch 6210, training loss: 7108.99, average training loss: 7471.06, base loss: 16106.05
[INFO 2017-06-29 05:30:29,639 main.py:57] epoch 6211, training loss: 7460.84, average training loss: 7471.77, base loss: 16105.98
[INFO 2017-06-29 05:30:32,710 main.py:57] epoch 6212, training loss: 6640.81, average training loss: 7471.07, base loss: 16105.81
[INFO 2017-06-29 05:30:35,724 main.py:57] epoch 6213, training loss: 10092.73, average training loss: 7473.66, base loss: 16106.97
[INFO 2017-06-29 05:30:38,780 main.py:57] epoch 6214, training loss: 7493.89, average training loss: 7474.53, base loss: 16107.00
[INFO 2017-06-29 05:30:41,837 main.py:57] epoch 6215, training loss: 7582.42, average training loss: 7473.93, base loss: 16106.86
[INFO 2017-06-29 05:30:44,887 main.py:57] epoch 6216, training loss: 7741.48, average training loss: 7474.90, base loss: 16106.92
[INFO 2017-06-29 05:30:47,911 main.py:57] epoch 6217, training loss: 7688.13, average training loss: 7476.10, base loss: 16106.60
[INFO 2017-06-29 05:30:51,051 main.py:57] epoch 6218, training loss: 8291.78, average training loss: 7475.90, base loss: 16107.32
[INFO 2017-06-29 05:30:54,150 main.py:57] epoch 6219, training loss: 6650.58, average training loss: 7474.99, base loss: 16106.94
[INFO 2017-06-29 05:30:57,143 main.py:57] epoch 6220, training loss: 7624.18, average training loss: 7474.44, base loss: 16106.91
[INFO 2017-06-29 05:31:00,121 main.py:57] epoch 6221, training loss: 7320.09, average training loss: 7473.63, base loss: 16106.88
[INFO 2017-06-29 05:31:03,168 main.py:57] epoch 6222, training loss: 7971.18, average training loss: 7474.30, base loss: 16106.96
[INFO 2017-06-29 05:31:06,190 main.py:57] epoch 6223, training loss: 7199.20, average training loss: 7475.06, base loss: 16107.26
[INFO 2017-06-29 05:31:09,314 main.py:57] epoch 6224, training loss: 7419.09, average training loss: 7474.98, base loss: 16106.92
[INFO 2017-06-29 05:31:12,366 main.py:57] epoch 6225, training loss: 7846.87, average training loss: 7474.28, base loss: 16106.63
[INFO 2017-06-29 05:31:15,385 main.py:57] epoch 6226, training loss: 7375.43, average training loss: 7473.50, base loss: 16106.95
[INFO 2017-06-29 05:31:18,474 main.py:57] epoch 6227, training loss: 7791.17, average training loss: 7474.16, base loss: 16107.38
[INFO 2017-06-29 05:31:21,586 main.py:57] epoch 6228, training loss: 8505.37, average training loss: 7475.52, base loss: 16108.10
[INFO 2017-06-29 05:31:24,617 main.py:57] epoch 6229, training loss: 7353.95, average training loss: 7475.02, base loss: 16108.16
[INFO 2017-06-29 05:31:27,756 main.py:57] epoch 6230, training loss: 7168.96, average training loss: 7474.17, base loss: 16107.64
[INFO 2017-06-29 05:31:30,862 main.py:57] epoch 6231, training loss: 7143.78, average training loss: 7472.08, base loss: 16107.49
[INFO 2017-06-29 05:31:33,965 main.py:57] epoch 6232, training loss: 6561.75, average training loss: 7470.62, base loss: 16106.69
[INFO 2017-06-29 05:31:37,044 main.py:57] epoch 6233, training loss: 7706.86, average training loss: 7471.15, base loss: 16106.75
[INFO 2017-06-29 05:31:40,040 main.py:57] epoch 6234, training loss: 7093.89, average training loss: 7471.35, base loss: 16106.84
[INFO 2017-06-29 05:31:43,138 main.py:57] epoch 6235, training loss: 7678.04, average training loss: 7471.56, base loss: 16107.38
[INFO 2017-06-29 05:31:46,192 main.py:57] epoch 6236, training loss: 8504.73, average training loss: 7473.45, base loss: 16107.66
[INFO 2017-06-29 05:31:49,319 main.py:57] epoch 6237, training loss: 6840.14, average training loss: 7473.03, base loss: 16107.04
[INFO 2017-06-29 05:31:52,371 main.py:57] epoch 6238, training loss: 7839.53, average training loss: 7473.77, base loss: 16107.23
[INFO 2017-06-29 05:31:55,406 main.py:57] epoch 6239, training loss: 6583.25, average training loss: 7471.96, base loss: 16106.77
[INFO 2017-06-29 05:31:58,416 main.py:57] epoch 6240, training loss: 7951.90, average training loss: 7473.09, base loss: 16107.34
[INFO 2017-06-29 05:32:01,455 main.py:57] epoch 6241, training loss: 7043.18, average training loss: 7473.04, base loss: 16107.34
[INFO 2017-06-29 05:32:04,583 main.py:57] epoch 6242, training loss: 6923.44, average training loss: 7472.84, base loss: 16108.04
[INFO 2017-06-29 05:32:07,612 main.py:57] epoch 6243, training loss: 7603.52, average training loss: 7473.38, base loss: 16108.43
[INFO 2017-06-29 05:32:10,646 main.py:57] epoch 6244, training loss: 7350.21, average training loss: 7473.73, base loss: 16108.16
[INFO 2017-06-29 05:32:13,687 main.py:57] epoch 6245, training loss: 8135.85, average training loss: 7474.53, base loss: 16108.84
[INFO 2017-06-29 05:32:16,784 main.py:57] epoch 6246, training loss: 6910.98, average training loss: 7474.34, base loss: 16109.09
[INFO 2017-06-29 05:32:19,857 main.py:57] epoch 6247, training loss: 6840.79, average training loss: 7473.15, base loss: 16109.25
[INFO 2017-06-29 05:32:22,832 main.py:57] epoch 6248, training loss: 6982.55, average training loss: 7470.91, base loss: 16108.92
[INFO 2017-06-29 05:32:25,934 main.py:57] epoch 6249, training loss: 8539.49, average training loss: 7472.32, base loss: 16109.57
[INFO 2017-06-29 05:32:28,958 main.py:57] epoch 6250, training loss: 6630.03, average training loss: 7470.33, base loss: 16108.97
[INFO 2017-06-29 05:32:32,028 main.py:57] epoch 6251, training loss: 6809.09, average training loss: 7469.78, base loss: 16108.79
[INFO 2017-06-29 05:32:35,032 main.py:57] epoch 6252, training loss: 6169.31, average training loss: 7468.47, base loss: 16108.04
[INFO 2017-06-29 05:32:38,123 main.py:57] epoch 6253, training loss: 7318.56, average training loss: 7468.03, base loss: 16107.80
[INFO 2017-06-29 05:32:41,136 main.py:57] epoch 6254, training loss: 7928.18, average training loss: 7468.35, base loss: 16108.03
[INFO 2017-06-29 05:32:44,193 main.py:57] epoch 6255, training loss: 7026.64, average training loss: 7467.93, base loss: 16107.67
[INFO 2017-06-29 05:32:47,264 main.py:57] epoch 6256, training loss: 7442.42, average training loss: 7467.55, base loss: 16107.58
[INFO 2017-06-29 05:32:50,256 main.py:57] epoch 6257, training loss: 8657.97, average training loss: 7468.48, base loss: 16108.26
[INFO 2017-06-29 05:32:53,323 main.py:57] epoch 6258, training loss: 7689.13, average training loss: 7468.83, base loss: 16108.39
[INFO 2017-06-29 05:32:56,339 main.py:57] epoch 6259, training loss: 8262.39, average training loss: 7470.21, base loss: 16109.02
[INFO 2017-06-29 05:32:59,368 main.py:57] epoch 6260, training loss: 7182.08, average training loss: 7470.27, base loss: 16108.86
[INFO 2017-06-29 05:33:02,381 main.py:57] epoch 6261, training loss: 7013.62, average training loss: 7470.47, base loss: 16108.11
[INFO 2017-06-29 05:33:05,394 main.py:57] epoch 6262, training loss: 6920.45, average training loss: 7469.10, base loss: 16107.70
[INFO 2017-06-29 05:33:08,474 main.py:57] epoch 6263, training loss: 6682.97, average training loss: 7469.04, base loss: 16107.32
[INFO 2017-06-29 05:33:11,522 main.py:57] epoch 6264, training loss: 7430.35, average training loss: 7469.34, base loss: 16107.69
[INFO 2017-06-29 05:33:14,591 main.py:57] epoch 6265, training loss: 7054.31, average training loss: 7469.21, base loss: 16107.14
[INFO 2017-06-29 05:33:17,715 main.py:57] epoch 6266, training loss: 6353.84, average training loss: 7466.92, base loss: 16106.49
[INFO 2017-06-29 05:33:20,775 main.py:57] epoch 6267, training loss: 6782.04, average training loss: 7464.51, base loss: 16106.29
[INFO 2017-06-29 05:33:23,835 main.py:57] epoch 6268, training loss: 6611.74, average training loss: 7464.55, base loss: 16106.37
[INFO 2017-06-29 05:33:26,969 main.py:57] epoch 6269, training loss: 8798.12, average training loss: 7466.38, base loss: 16106.92
[INFO 2017-06-29 05:33:30,091 main.py:57] epoch 6270, training loss: 7693.34, average training loss: 7467.43, base loss: 16107.23
[INFO 2017-06-29 05:33:33,124 main.py:57] epoch 6271, training loss: 7421.53, average training loss: 7466.74, base loss: 16107.25
[INFO 2017-06-29 05:33:36,193 main.py:57] epoch 6272, training loss: 8107.98, average training loss: 7466.86, base loss: 16107.36
[INFO 2017-06-29 05:33:39,312 main.py:57] epoch 6273, training loss: 7042.10, average training loss: 7465.59, base loss: 16107.12
[INFO 2017-06-29 05:33:42,284 main.py:57] epoch 6274, training loss: 7050.99, average training loss: 7466.14, base loss: 16106.88
[INFO 2017-06-29 05:33:45,288 main.py:57] epoch 6275, training loss: 8095.59, average training loss: 7466.68, base loss: 16106.97
[INFO 2017-06-29 05:33:48,412 main.py:57] epoch 6276, training loss: 6986.54, average training loss: 7466.25, base loss: 16106.75
[INFO 2017-06-29 05:33:51,442 main.py:57] epoch 6277, training loss: 7781.39, average training loss: 7466.08, base loss: 16106.64
[INFO 2017-06-29 05:33:54,523 main.py:57] epoch 6278, training loss: 8724.22, average training loss: 7466.96, base loss: 16107.23
[INFO 2017-06-29 05:33:57,567 main.py:57] epoch 6279, training loss: 7211.52, average training loss: 7466.23, base loss: 16107.29
[INFO 2017-06-29 05:34:00,635 main.py:57] epoch 6280, training loss: 7767.14, average training loss: 7466.60, base loss: 16107.46
[INFO 2017-06-29 05:34:03,704 main.py:57] epoch 6281, training loss: 7906.48, average training loss: 7467.37, base loss: 16107.52
[INFO 2017-06-29 05:34:06,739 main.py:57] epoch 6282, training loss: 7950.98, average training loss: 7468.02, base loss: 16108.09
[INFO 2017-06-29 05:34:09,834 main.py:57] epoch 6283, training loss: 6316.77, average training loss: 7467.58, base loss: 16107.15
[INFO 2017-06-29 05:34:12,893 main.py:57] epoch 6284, training loss: 6862.36, average training loss: 7466.83, base loss: 16106.58
[INFO 2017-06-29 05:34:15,967 main.py:57] epoch 6285, training loss: 7960.61, average training loss: 7468.19, base loss: 16106.59
[INFO 2017-06-29 05:34:18,992 main.py:57] epoch 6286, training loss: 7091.83, average training loss: 7467.14, base loss: 16106.87
[INFO 2017-06-29 05:34:22,145 main.py:57] epoch 6287, training loss: 6378.78, average training loss: 7463.88, base loss: 16106.46
[INFO 2017-06-29 05:34:25,193 main.py:57] epoch 6288, training loss: 7590.30, average training loss: 7463.94, base loss: 16107.28
[INFO 2017-06-29 05:34:28,199 main.py:57] epoch 6289, training loss: 8069.84, average training loss: 7464.77, base loss: 16107.20
[INFO 2017-06-29 05:34:31,207 main.py:57] epoch 6290, training loss: 7642.67, average training loss: 7465.26, base loss: 16107.71
[INFO 2017-06-29 05:34:34,270 main.py:57] epoch 6291, training loss: 8021.50, average training loss: 7466.29, base loss: 16108.07
[INFO 2017-06-29 05:34:37,379 main.py:57] epoch 6292, training loss: 7649.04, average training loss: 7466.03, base loss: 16108.15
[INFO 2017-06-29 05:34:40,410 main.py:57] epoch 6293, training loss: 7374.85, average training loss: 7466.15, base loss: 16107.95
[INFO 2017-06-29 05:34:43,410 main.py:57] epoch 6294, training loss: 7008.70, average training loss: 7465.54, base loss: 16107.94
[INFO 2017-06-29 05:34:46,464 main.py:57] epoch 6295, training loss: 8412.28, average training loss: 7466.96, base loss: 16108.85
[INFO 2017-06-29 05:34:49,470 main.py:57] epoch 6296, training loss: 8351.75, average training loss: 7468.80, base loss: 16109.07
[INFO 2017-06-29 05:34:52,576 main.py:57] epoch 6297, training loss: 6903.56, average training loss: 7468.28, base loss: 16108.16
[INFO 2017-06-29 05:34:55,644 main.py:57] epoch 6298, training loss: 7249.54, average training loss: 7468.57, base loss: 16107.82
[INFO 2017-06-29 05:34:58,733 main.py:57] epoch 6299, training loss: 7720.19, average training loss: 7469.32, base loss: 16107.15
[INFO 2017-06-29 05:34:58,734 main.py:59] epoch 6299, testing
[INFO 2017-06-29 05:35:11,322 main.py:104] average testing loss: 7827.06, base loss: 15856.22
[INFO 2017-06-29 05:35:11,322 main.py:105] improve_loss: 8029.16, improve_percent: 0.51
[INFO 2017-06-29 05:35:11,323 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:35:14,370 main.py:57] epoch 6300, training loss: 7185.39, average training loss: 7469.62, base loss: 16107.19
[INFO 2017-06-29 05:35:17,425 main.py:57] epoch 6301, training loss: 7151.70, average training loss: 7468.88, base loss: 16107.43
[INFO 2017-06-29 05:35:20,504 main.py:57] epoch 6302, training loss: 7007.44, average training loss: 7467.49, base loss: 16107.11
[INFO 2017-06-29 05:35:23,649 main.py:57] epoch 6303, training loss: 8461.52, average training loss: 7468.26, base loss: 16107.43
[INFO 2017-06-29 05:35:26,739 main.py:57] epoch 6304, training loss: 6612.73, average training loss: 7467.07, base loss: 16107.63
[INFO 2017-06-29 05:35:29,800 main.py:57] epoch 6305, training loss: 7030.91, average training loss: 7467.51, base loss: 16108.07
[INFO 2017-06-29 05:35:32,795 main.py:57] epoch 6306, training loss: 6834.51, average training loss: 7467.21, base loss: 16107.60
[INFO 2017-06-29 05:35:35,780 main.py:57] epoch 6307, training loss: 6970.08, average training loss: 7466.70, base loss: 16107.40
[INFO 2017-06-29 05:35:38,839 main.py:57] epoch 6308, training loss: 7576.60, average training loss: 7466.87, base loss: 16107.49
[INFO 2017-06-29 05:35:41,984 main.py:57] epoch 6309, training loss: 6417.47, average training loss: 7465.56, base loss: 16106.46
[INFO 2017-06-29 05:35:45,019 main.py:57] epoch 6310, training loss: 7682.33, average training loss: 7465.15, base loss: 16106.89
[INFO 2017-06-29 05:35:48,021 main.py:57] epoch 6311, training loss: 7053.78, average training loss: 7465.62, base loss: 16106.44
[INFO 2017-06-29 05:35:51,110 main.py:57] epoch 6312, training loss: 7081.04, average training loss: 7464.76, base loss: 16106.43
[INFO 2017-06-29 05:35:54,184 main.py:57] epoch 6313, training loss: 7108.57, average training loss: 7465.08, base loss: 16106.23
[INFO 2017-06-29 05:35:57,211 main.py:57] epoch 6314, training loss: 6643.57, average training loss: 7464.39, base loss: 16105.33
[INFO 2017-06-29 05:36:00,272 main.py:57] epoch 6315, training loss: 7109.62, average training loss: 7463.68, base loss: 16104.79
[INFO 2017-06-29 05:36:03,398 main.py:57] epoch 6316, training loss: 7300.60, average training loss: 7463.31, base loss: 16104.49
[INFO 2017-06-29 05:36:06,489 main.py:57] epoch 6317, training loss: 7079.11, average training loss: 7463.35, base loss: 16104.23
[INFO 2017-06-29 05:36:09,488 main.py:57] epoch 6318, training loss: 6978.23, average training loss: 7463.00, base loss: 16104.14
[INFO 2017-06-29 05:36:12,506 main.py:57] epoch 6319, training loss: 7545.85, average training loss: 7463.16, base loss: 16104.16
[INFO 2017-06-29 05:36:15,562 main.py:57] epoch 6320, training loss: 6520.57, average training loss: 7461.95, base loss: 16103.51
[INFO 2017-06-29 05:36:18,609 main.py:57] epoch 6321, training loss: 7973.40, average training loss: 7461.85, base loss: 16103.58
[INFO 2017-06-29 05:36:21,607 main.py:57] epoch 6322, training loss: 6477.15, average training loss: 7460.98, base loss: 16102.88
[INFO 2017-06-29 05:36:24,653 main.py:57] epoch 6323, training loss: 6905.04, average training loss: 7460.51, base loss: 16102.15
[INFO 2017-06-29 05:36:27,736 main.py:57] epoch 6324, training loss: 7397.43, average training loss: 7460.09, base loss: 16102.06
[INFO 2017-06-29 05:36:30,777 main.py:57] epoch 6325, training loss: 7470.31, average training loss: 7459.62, base loss: 16102.06
[INFO 2017-06-29 05:36:33,813 main.py:57] epoch 6326, training loss: 7318.73, average training loss: 7459.87, base loss: 16101.98
[INFO 2017-06-29 05:36:36,877 main.py:57] epoch 6327, training loss: 7359.58, average training loss: 7459.45, base loss: 16101.77
[INFO 2017-06-29 05:36:39,921 main.py:57] epoch 6328, training loss: 7591.19, average training loss: 7459.20, base loss: 16101.68
[INFO 2017-06-29 05:36:43,041 main.py:57] epoch 6329, training loss: 7145.24, average training loss: 7459.65, base loss: 16101.16
[INFO 2017-06-29 05:36:46,080 main.py:57] epoch 6330, training loss: 7041.21, average training loss: 7459.32, base loss: 16100.71
[INFO 2017-06-29 05:36:49,079 main.py:57] epoch 6331, training loss: 8501.23, average training loss: 7460.70, base loss: 16101.30
[INFO 2017-06-29 05:36:52,084 main.py:57] epoch 6332, training loss: 7667.56, average training loss: 7460.52, base loss: 16101.48
[INFO 2017-06-29 05:36:55,142 main.py:57] epoch 6333, training loss: 7426.38, average training loss: 7460.63, base loss: 16101.43
[INFO 2017-06-29 05:36:58,125 main.py:57] epoch 6334, training loss: 6446.97, average training loss: 7459.42, base loss: 16100.94
[INFO 2017-06-29 05:37:01,119 main.py:57] epoch 6335, training loss: 8476.54, average training loss: 7460.16, base loss: 16101.51
[INFO 2017-06-29 05:37:04,311 main.py:57] epoch 6336, training loss: 6322.22, average training loss: 7458.65, base loss: 16100.81
[INFO 2017-06-29 05:37:07,426 main.py:57] epoch 6337, training loss: 7114.52, average training loss: 7458.67, base loss: 16100.63
[INFO 2017-06-29 05:37:10,496 main.py:57] epoch 6338, training loss: 6882.20, average training loss: 7457.36, base loss: 16100.03
[INFO 2017-06-29 05:37:13,574 main.py:57] epoch 6339, training loss: 7282.88, average training loss: 7457.43, base loss: 16099.66
[INFO 2017-06-29 05:37:16,728 main.py:57] epoch 6340, training loss: 6313.23, average training loss: 7455.33, base loss: 16098.93
[INFO 2017-06-29 05:37:19,760 main.py:57] epoch 6341, training loss: 8643.27, average training loss: 7456.76, base loss: 16099.21
[INFO 2017-06-29 05:37:22,810 main.py:57] epoch 6342, training loss: 7250.00, average training loss: 7456.21, base loss: 16098.75
[INFO 2017-06-29 05:37:25,888 main.py:57] epoch 6343, training loss: 7173.63, average training loss: 7455.63, base loss: 16098.54
[INFO 2017-06-29 05:37:28,955 main.py:57] epoch 6344, training loss: 6827.00, average training loss: 7455.25, base loss: 16098.25
[INFO 2017-06-29 05:37:31,970 main.py:57] epoch 6345, training loss: 7702.47, average training loss: 7455.91, base loss: 16098.89
[INFO 2017-06-29 05:37:35,035 main.py:57] epoch 6346, training loss: 7824.03, average training loss: 7455.30, base loss: 16098.97
[INFO 2017-06-29 05:37:38,095 main.py:57] epoch 6347, training loss: 7655.62, average training loss: 7455.68, base loss: 16098.47
[INFO 2017-06-29 05:37:41,123 main.py:57] epoch 6348, training loss: 7571.84, average training loss: 7455.38, base loss: 16098.67
[INFO 2017-06-29 05:37:44,138 main.py:57] epoch 6349, training loss: 6902.77, average training loss: 7453.68, base loss: 16098.37
[INFO 2017-06-29 05:37:47,280 main.py:57] epoch 6350, training loss: 7412.85, average training loss: 7453.69, base loss: 16098.32
[INFO 2017-06-29 05:37:50,329 main.py:57] epoch 6351, training loss: 7123.94, average training loss: 7451.64, base loss: 16098.32
[INFO 2017-06-29 05:37:53,387 main.py:57] epoch 6352, training loss: 7343.83, average training loss: 7452.20, base loss: 16097.67
[INFO 2017-06-29 05:37:56,462 main.py:57] epoch 6353, training loss: 6138.91, average training loss: 7450.58, base loss: 16096.61
[INFO 2017-06-29 05:37:59,510 main.py:57] epoch 6354, training loss: 7396.57, average training loss: 7449.93, base loss: 16096.70
[INFO 2017-06-29 05:38:02,571 main.py:57] epoch 6355, training loss: 8698.05, average training loss: 7451.64, base loss: 16097.41
[INFO 2017-06-29 05:38:05,569 main.py:57] epoch 6356, training loss: 7948.45, average training loss: 7451.64, base loss: 16097.90
[INFO 2017-06-29 05:38:08,627 main.py:57] epoch 6357, training loss: 7851.14, average training loss: 7451.76, base loss: 16098.08
[INFO 2017-06-29 05:38:11,673 main.py:57] epoch 6358, training loss: 7838.53, average training loss: 7451.58, base loss: 16098.01
[INFO 2017-06-29 05:38:14,811 main.py:57] epoch 6359, training loss: 8407.67, average training loss: 7452.23, base loss: 16097.88
[INFO 2017-06-29 05:38:17,865 main.py:57] epoch 6360, training loss: 6898.65, average training loss: 7451.87, base loss: 16097.49
[INFO 2017-06-29 05:38:20,932 main.py:57] epoch 6361, training loss: 8773.42, average training loss: 7451.23, base loss: 16098.20
[INFO 2017-06-29 05:38:23,906 main.py:57] epoch 6362, training loss: 7089.67, average training loss: 7451.73, base loss: 16097.62
[INFO 2017-06-29 05:38:26,935 main.py:57] epoch 6363, training loss: 7686.14, average training loss: 7452.88, base loss: 16097.99
[INFO 2017-06-29 05:38:29,982 main.py:57] epoch 6364, training loss: 7740.83, average training loss: 7453.01, base loss: 16098.02
[INFO 2017-06-29 05:38:33,096 main.py:57] epoch 6365, training loss: 7431.68, average training loss: 7453.69, base loss: 16097.79
[INFO 2017-06-29 05:38:36,148 main.py:57] epoch 6366, training loss: 7135.29, average training loss: 7452.48, base loss: 16097.48
[INFO 2017-06-29 05:38:39,258 main.py:57] epoch 6367, training loss: 7910.79, average training loss: 7452.86, base loss: 16097.52
[INFO 2017-06-29 05:38:42,358 main.py:57] epoch 6368, training loss: 6960.16, average training loss: 7451.81, base loss: 16097.10
[INFO 2017-06-29 05:38:45,398 main.py:57] epoch 6369, training loss: 7876.22, average training loss: 7452.30, base loss: 16097.00
[INFO 2017-06-29 05:38:48,498 main.py:57] epoch 6370, training loss: 7089.96, average training loss: 7451.40, base loss: 16096.91
[INFO 2017-06-29 05:38:51,526 main.py:57] epoch 6371, training loss: 6817.92, average training loss: 7450.84, base loss: 16096.95
[INFO 2017-06-29 05:38:54,538 main.py:57] epoch 6372, training loss: 7623.38, average training loss: 7451.17, base loss: 16096.86
[INFO 2017-06-29 05:38:57,646 main.py:57] epoch 6373, training loss: 7910.80, average training loss: 7452.00, base loss: 16097.41
[INFO 2017-06-29 05:39:00,712 main.py:57] epoch 6374, training loss: 6992.72, average training loss: 7452.51, base loss: 16097.12
[INFO 2017-06-29 05:39:03,745 main.py:57] epoch 6375, training loss: 7890.22, average training loss: 7453.00, base loss: 16097.38
[INFO 2017-06-29 05:39:06,743 main.py:57] epoch 6376, training loss: 7747.47, average training loss: 7453.31, base loss: 16097.92
[INFO 2017-06-29 05:39:09,765 main.py:57] epoch 6377, training loss: 8347.99, average training loss: 7453.51, base loss: 16098.42
[INFO 2017-06-29 05:39:12,824 main.py:57] epoch 6378, training loss: 8006.11, average training loss: 7453.62, base loss: 16098.68
[INFO 2017-06-29 05:39:15,851 main.py:57] epoch 6379, training loss: 7637.06, average training loss: 7454.43, base loss: 16098.67
[INFO 2017-06-29 05:39:18,964 main.py:57] epoch 6380, training loss: 6722.18, average training loss: 7453.06, base loss: 16098.63
[INFO 2017-06-29 05:39:22,033 main.py:57] epoch 6381, training loss: 8154.98, average training loss: 7454.07, base loss: 16099.36
[INFO 2017-06-29 05:39:25,053 main.py:57] epoch 6382, training loss: 6530.88, average training loss: 7452.44, base loss: 16098.58
[INFO 2017-06-29 05:39:28,106 main.py:57] epoch 6383, training loss: 7911.44, average training loss: 7452.27, base loss: 16098.28
[INFO 2017-06-29 05:39:31,174 main.py:57] epoch 6384, training loss: 6784.23, average training loss: 7452.00, base loss: 16097.67
[INFO 2017-06-29 05:39:34,274 main.py:57] epoch 6385, training loss: 7240.80, average training loss: 7451.15, base loss: 16097.59
[INFO 2017-06-29 05:39:37,321 main.py:57] epoch 6386, training loss: 7953.66, average training loss: 7451.82, base loss: 16098.04
[INFO 2017-06-29 05:39:40,435 main.py:57] epoch 6387, training loss: 7456.27, average training loss: 7451.56, base loss: 16098.59
[INFO 2017-06-29 05:39:43,437 main.py:57] epoch 6388, training loss: 7427.77, average training loss: 7451.96, base loss: 16098.48
[INFO 2017-06-29 05:39:46,502 main.py:57] epoch 6389, training loss: 7016.27, average training loss: 7451.94, base loss: 16098.45
[INFO 2017-06-29 05:39:49,501 main.py:57] epoch 6390, training loss: 9390.85, average training loss: 7454.53, base loss: 16099.49
[INFO 2017-06-29 05:39:52,526 main.py:57] epoch 6391, training loss: 6561.23, average training loss: 7453.39, base loss: 16098.97
[INFO 2017-06-29 05:39:55,695 main.py:57] epoch 6392, training loss: 6722.03, average training loss: 7452.47, base loss: 16098.79
[INFO 2017-06-29 05:39:58,827 main.py:57] epoch 6393, training loss: 7521.52, average training loss: 7452.09, base loss: 16099.13
[INFO 2017-06-29 05:40:01,931 main.py:57] epoch 6394, training loss: 6923.08, average training loss: 7451.56, base loss: 16098.94
[INFO 2017-06-29 05:40:05,060 main.py:57] epoch 6395, training loss: 7391.12, average training loss: 7449.77, base loss: 16099.07
[INFO 2017-06-29 05:40:08,073 main.py:57] epoch 6396, training loss: 7160.89, average training loss: 7449.23, base loss: 16099.14
[INFO 2017-06-29 05:40:11,127 main.py:57] epoch 6397, training loss: 7228.30, average training loss: 7449.10, base loss: 16098.76
[INFO 2017-06-29 05:40:14,139 main.py:57] epoch 6398, training loss: 6860.59, average training loss: 7449.05, base loss: 16098.63
[INFO 2017-06-29 05:40:17,262 main.py:57] epoch 6399, training loss: 7749.86, average training loss: 7449.33, base loss: 16098.53
[INFO 2017-06-29 05:40:17,262 main.py:59] epoch 6399, testing
[INFO 2017-06-29 05:40:29,883 main.py:104] average testing loss: 8351.10, base loss: 17102.78
[INFO 2017-06-29 05:40:29,883 main.py:105] improve_loss: 8751.68, improve_percent: 0.51
[INFO 2017-06-29 05:40:29,885 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:40:32,958 main.py:57] epoch 6400, training loss: 8387.78, average training loss: 7449.75, base loss: 16098.70
[INFO 2017-06-29 05:40:35,995 main.py:57] epoch 6401, training loss: 7289.44, average training loss: 7450.41, base loss: 16098.47
[INFO 2017-06-29 05:40:39,023 main.py:57] epoch 6402, training loss: 8302.22, average training loss: 7450.47, base loss: 16098.78
[INFO 2017-06-29 05:40:42,109 main.py:57] epoch 6403, training loss: 7567.99, average training loss: 7451.33, base loss: 16098.82
[INFO 2017-06-29 05:40:45,199 main.py:57] epoch 6404, training loss: 6694.96, average training loss: 7451.41, base loss: 16098.72
[INFO 2017-06-29 05:40:48,248 main.py:57] epoch 6405, training loss: 7653.15, average training loss: 7451.42, base loss: 16099.15
[INFO 2017-06-29 05:40:51,427 main.py:57] epoch 6406, training loss: 6838.69, average training loss: 7450.56, base loss: 16098.48
[INFO 2017-06-29 05:40:54,447 main.py:57] epoch 6407, training loss: 8675.79, average training loss: 7452.48, base loss: 16099.45
[INFO 2017-06-29 05:40:57,526 main.py:57] epoch 6408, training loss: 8113.60, average training loss: 7453.02, base loss: 16099.65
[INFO 2017-06-29 05:41:00,548 main.py:57] epoch 6409, training loss: 7377.80, average training loss: 7452.38, base loss: 16099.48
[INFO 2017-06-29 05:41:03,645 main.py:57] epoch 6410, training loss: 6433.60, average training loss: 7451.56, base loss: 16098.65
[INFO 2017-06-29 05:41:06,802 main.py:57] epoch 6411, training loss: 7463.99, average training loss: 7451.80, base loss: 16098.61
[INFO 2017-06-29 05:41:09,881 main.py:57] epoch 6412, training loss: 6941.99, average training loss: 7451.12, base loss: 16098.36
[INFO 2017-06-29 05:41:12,928 main.py:57] epoch 6413, training loss: 7536.19, average training loss: 7451.40, base loss: 16098.09
[INFO 2017-06-29 05:41:15,973 main.py:57] epoch 6414, training loss: 6786.96, average training loss: 7450.88, base loss: 16098.09
[INFO 2017-06-29 05:41:18,957 main.py:57] epoch 6415, training loss: 6897.97, average training loss: 7450.28, base loss: 16098.16
[INFO 2017-06-29 05:41:22,002 main.py:57] epoch 6416, training loss: 6892.35, average training loss: 7449.54, base loss: 16098.00
[INFO 2017-06-29 05:41:25,072 main.py:57] epoch 6417, training loss: 7507.27, average training loss: 7449.82, base loss: 16098.41
[INFO 2017-06-29 05:41:28,050 main.py:57] epoch 6418, training loss: 6027.11, average training loss: 7448.55, base loss: 16097.71
[INFO 2017-06-29 05:41:31,072 main.py:57] epoch 6419, training loss: 8270.29, average training loss: 7449.53, base loss: 16098.06
[INFO 2017-06-29 05:41:34,180 main.py:57] epoch 6420, training loss: 6619.25, average training loss: 7448.33, base loss: 16097.91
[INFO 2017-06-29 05:41:37,196 main.py:57] epoch 6421, training loss: 8197.35, average training loss: 7448.83, base loss: 16098.29
[INFO 2017-06-29 05:41:40,301 main.py:57] epoch 6422, training loss: 6805.51, average training loss: 7446.92, base loss: 16097.76
[INFO 2017-06-29 05:41:43,283 main.py:57] epoch 6423, training loss: 6607.53, average training loss: 7445.34, base loss: 16097.03
[INFO 2017-06-29 05:41:46,303 main.py:57] epoch 6424, training loss: 8136.39, average training loss: 7446.14, base loss: 16097.09
[INFO 2017-06-29 05:41:49,466 main.py:57] epoch 6425, training loss: 6874.23, average training loss: 7445.40, base loss: 16096.63
[INFO 2017-06-29 05:41:52,490 main.py:57] epoch 6426, training loss: 6598.20, average training loss: 7444.80, base loss: 16096.48
[INFO 2017-06-29 05:41:55,551 main.py:57] epoch 6427, training loss: 8159.98, average training loss: 7445.07, base loss: 16096.85
[INFO 2017-06-29 05:41:58,750 main.py:57] epoch 6428, training loss: 7444.02, average training loss: 7445.96, base loss: 16096.60
[INFO 2017-06-29 05:42:01,833 main.py:57] epoch 6429, training loss: 7380.33, average training loss: 7444.19, base loss: 16096.16
[INFO 2017-06-29 05:42:04,981 main.py:57] epoch 6430, training loss: 7375.09, average training loss: 7444.12, base loss: 16095.95
[INFO 2017-06-29 05:42:07,988 main.py:57] epoch 6431, training loss: 6669.32, average training loss: 7444.64, base loss: 16095.13
[INFO 2017-06-29 05:42:11,025 main.py:57] epoch 6432, training loss: 7396.78, average training loss: 7442.09, base loss: 16094.72
[INFO 2017-06-29 05:42:14,032 main.py:57] epoch 6433, training loss: 7234.41, average training loss: 7442.62, base loss: 16094.98
[INFO 2017-06-29 05:42:17,118 main.py:57] epoch 6434, training loss: 9497.14, average training loss: 7444.58, base loss: 16096.07
[INFO 2017-06-29 05:42:20,174 main.py:57] epoch 6435, training loss: 7371.49, average training loss: 7444.22, base loss: 16095.75
[INFO 2017-06-29 05:42:23,249 main.py:57] epoch 6436, training loss: 6976.36, average training loss: 7443.62, base loss: 16095.10
[INFO 2017-06-29 05:42:26,329 main.py:57] epoch 6437, training loss: 8032.08, average training loss: 7443.59, base loss: 16094.91
[INFO 2017-06-29 05:42:29,389 main.py:57] epoch 6438, training loss: 7913.89, average training loss: 7443.55, base loss: 16094.85
[INFO 2017-06-29 05:42:32,449 main.py:57] epoch 6439, training loss: 9407.26, average training loss: 7446.04, base loss: 16095.72
[INFO 2017-06-29 05:42:35,499 main.py:57] epoch 6440, training loss: 8586.56, average training loss: 7446.74, base loss: 16096.13
[INFO 2017-06-29 05:42:38,499 main.py:57] epoch 6441, training loss: 7144.28, average training loss: 7445.45, base loss: 16095.75
[INFO 2017-06-29 05:42:41,486 main.py:57] epoch 6442, training loss: 7156.81, average training loss: 7445.66, base loss: 16095.10
[INFO 2017-06-29 05:42:44,542 main.py:57] epoch 6443, training loss: 7666.35, average training loss: 7445.85, base loss: 16095.04
[INFO 2017-06-29 05:42:47,652 main.py:57] epoch 6444, training loss: 7080.17, average training loss: 7444.52, base loss: 16094.54
[INFO 2017-06-29 05:42:50,660 main.py:57] epoch 6445, training loss: 7572.36, average training loss: 7443.94, base loss: 16094.58
[INFO 2017-06-29 05:42:53,679 main.py:57] epoch 6446, training loss: 7238.81, average training loss: 7444.39, base loss: 16094.83
[INFO 2017-06-29 05:42:56,807 main.py:57] epoch 6447, training loss: 7700.62, average training loss: 7445.23, base loss: 16095.44
[INFO 2017-06-29 05:42:59,900 main.py:57] epoch 6448, training loss: 7214.05, average training loss: 7445.86, base loss: 16095.87
[INFO 2017-06-29 05:43:02,942 main.py:57] epoch 6449, training loss: 7208.96, average training loss: 7444.75, base loss: 16095.76
[INFO 2017-06-29 05:43:05,962 main.py:57] epoch 6450, training loss: 6744.99, average training loss: 7443.81, base loss: 16095.48
[INFO 2017-06-29 05:43:09,030 main.py:57] epoch 6451, training loss: 7808.58, average training loss: 7443.52, base loss: 16095.89
[INFO 2017-06-29 05:43:12,103 main.py:57] epoch 6452, training loss: 7077.32, average training loss: 7441.93, base loss: 16096.24
[INFO 2017-06-29 05:43:15,121 main.py:57] epoch 6453, training loss: 7396.93, average training loss: 7442.23, base loss: 16096.32
[INFO 2017-06-29 05:43:18,239 main.py:57] epoch 6454, training loss: 7697.48, average training loss: 7443.30, base loss: 16096.27
[INFO 2017-06-29 05:43:21,274 main.py:57] epoch 6455, training loss: 7469.45, average training loss: 7444.30, base loss: 16096.72
[INFO 2017-06-29 05:43:24,326 main.py:57] epoch 6456, training loss: 7839.57, average training loss: 7444.56, base loss: 16097.02
[INFO 2017-06-29 05:43:27,363 main.py:57] epoch 6457, training loss: 7590.94, average training loss: 7444.22, base loss: 16097.55
[INFO 2017-06-29 05:43:30,481 main.py:57] epoch 6458, training loss: 6746.19, average training loss: 7442.48, base loss: 16097.75
[INFO 2017-06-29 05:43:33,519 main.py:57] epoch 6459, training loss: 8003.58, average training loss: 7443.39, base loss: 16098.49
[INFO 2017-06-29 05:43:36,601 main.py:57] epoch 6460, training loss: 8627.70, average training loss: 7445.49, base loss: 16098.90
[INFO 2017-06-29 05:43:39,652 main.py:57] epoch 6461, training loss: 7730.82, average training loss: 7446.57, base loss: 16099.73
[INFO 2017-06-29 05:43:42,689 main.py:57] epoch 6462, training loss: 7479.06, average training loss: 7446.49, base loss: 16099.72
[INFO 2017-06-29 05:43:45,786 main.py:57] epoch 6463, training loss: 7030.38, average training loss: 7446.22, base loss: 16099.81
[INFO 2017-06-29 05:43:48,807 main.py:57] epoch 6464, training loss: 7284.68, average training loss: 7445.72, base loss: 16099.46
[INFO 2017-06-29 05:43:51,889 main.py:57] epoch 6465, training loss: 9111.79, average training loss: 7447.83, base loss: 16100.29
[INFO 2017-06-29 05:43:54,949 main.py:57] epoch 6466, training loss: 7884.52, average training loss: 7449.32, base loss: 16100.55
[INFO 2017-06-29 05:43:58,039 main.py:57] epoch 6467, training loss: 7640.23, average training loss: 7449.87, base loss: 16100.86
[INFO 2017-06-29 05:44:01,033 main.py:57] epoch 6468, training loss: 7312.26, average training loss: 7449.79, base loss: 16101.03
[INFO 2017-06-29 05:44:04,125 main.py:57] epoch 6469, training loss: 7423.38, average training loss: 7448.56, base loss: 16101.16
[INFO 2017-06-29 05:44:07,234 main.py:57] epoch 6470, training loss: 6984.45, average training loss: 7448.77, base loss: 16100.84
[INFO 2017-06-29 05:44:10,341 main.py:57] epoch 6471, training loss: 7675.09, average training loss: 7448.58, base loss: 16100.34
[INFO 2017-06-29 05:44:13,429 main.py:57] epoch 6472, training loss: 6796.71, average training loss: 7448.00, base loss: 16100.81
[INFO 2017-06-29 05:44:16,534 main.py:57] epoch 6473, training loss: 7916.49, average training loss: 7448.23, base loss: 16101.28
[INFO 2017-06-29 05:44:19,626 main.py:57] epoch 6474, training loss: 6736.67, average training loss: 7446.89, base loss: 16100.63
[INFO 2017-06-29 05:44:22,668 main.py:57] epoch 6475, training loss: 8107.57, average training loss: 7446.76, base loss: 16100.78
[INFO 2017-06-29 05:44:25,779 main.py:57] epoch 6476, training loss: 8277.89, average training loss: 7448.46, base loss: 16101.32
[INFO 2017-06-29 05:44:28,841 main.py:57] epoch 6477, training loss: 7027.97, average training loss: 7448.78, base loss: 16101.06
[INFO 2017-06-29 05:44:31,898 main.py:57] epoch 6478, training loss: 7422.39, average training loss: 7448.44, base loss: 16101.12
[INFO 2017-06-29 05:44:34,955 main.py:57] epoch 6479, training loss: 7186.60, average training loss: 7447.43, base loss: 16100.57
[INFO 2017-06-29 05:44:37,964 main.py:57] epoch 6480, training loss: 7850.18, average training loss: 7448.25, base loss: 16100.54
[INFO 2017-06-29 05:44:41,041 main.py:57] epoch 6481, training loss: 7206.03, average training loss: 7448.29, base loss: 16100.44
[INFO 2017-06-29 05:44:44,075 main.py:57] epoch 6482, training loss: 6878.97, average training loss: 7446.73, base loss: 16099.85
[INFO 2017-06-29 05:44:47,129 main.py:57] epoch 6483, training loss: 7293.63, average training loss: 7447.61, base loss: 16099.52
[INFO 2017-06-29 05:44:50,192 main.py:57] epoch 6484, training loss: 6911.98, average training loss: 7447.33, base loss: 16099.22
[INFO 2017-06-29 05:44:53,295 main.py:57] epoch 6485, training loss: 6328.70, average training loss: 7445.96, base loss: 16098.67
[INFO 2017-06-29 05:44:56,304 main.py:57] epoch 6486, training loss: 7175.64, average training loss: 7446.51, base loss: 16098.85
[INFO 2017-06-29 05:44:59,348 main.py:57] epoch 6487, training loss: 7470.02, average training loss: 7446.65, base loss: 16098.84
[INFO 2017-06-29 05:45:02,400 main.py:57] epoch 6488, training loss: 6276.11, average training loss: 7445.72, base loss: 16098.29
[INFO 2017-06-29 05:45:05,516 main.py:57] epoch 6489, training loss: 6634.08, average training loss: 7444.87, base loss: 16098.00
[INFO 2017-06-29 05:45:08,551 main.py:57] epoch 6490, training loss: 7739.70, average training loss: 7445.93, base loss: 16098.42
[INFO 2017-06-29 05:45:11,728 main.py:57] epoch 6491, training loss: 6269.58, average training loss: 7444.93, base loss: 16097.98
[INFO 2017-06-29 05:45:14,790 main.py:57] epoch 6492, training loss: 7997.35, average training loss: 7445.39, base loss: 16098.43
[INFO 2017-06-29 05:45:17,826 main.py:57] epoch 6493, training loss: 6795.86, average training loss: 7445.22, base loss: 16097.61
[INFO 2017-06-29 05:45:20,862 main.py:57] epoch 6494, training loss: 7001.47, average training loss: 7444.05, base loss: 16097.46
[INFO 2017-06-29 05:45:23,894 main.py:57] epoch 6495, training loss: 8024.91, average training loss: 7444.99, base loss: 16097.78
[INFO 2017-06-29 05:45:26,995 main.py:57] epoch 6496, training loss: 7735.90, average training loss: 7444.94, base loss: 16097.77
[INFO 2017-06-29 05:45:30,026 main.py:57] epoch 6497, training loss: 7278.31, average training loss: 7444.73, base loss: 16097.42
[INFO 2017-06-29 05:45:33,160 main.py:57] epoch 6498, training loss: 8041.54, average training loss: 7446.18, base loss: 16098.01
[INFO 2017-06-29 05:45:36,215 main.py:57] epoch 6499, training loss: 7438.86, average training loss: 7446.20, base loss: 16098.38
[INFO 2017-06-29 05:45:36,216 main.py:59] epoch 6499, testing
[INFO 2017-06-29 05:45:48,848 main.py:104] average testing loss: 8394.40, base loss: 17310.61
[INFO 2017-06-29 05:45:48,848 main.py:105] improve_loss: 8916.21, improve_percent: 0.52
[INFO 2017-06-29 05:45:48,849 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:45:51,892 main.py:57] epoch 6500, training loss: 6687.31, average training loss: 7445.38, base loss: 16098.16
[INFO 2017-06-29 05:45:54,994 main.py:57] epoch 6501, training loss: 7719.45, average training loss: 7446.64, base loss: 16098.61
[INFO 2017-06-29 05:45:57,997 main.py:57] epoch 6502, training loss: 7879.27, average training loss: 7446.84, base loss: 16099.18
[INFO 2017-06-29 05:46:01,102 main.py:57] epoch 6503, training loss: 7538.21, average training loss: 7446.80, base loss: 16099.66
[INFO 2017-06-29 05:46:04,148 main.py:57] epoch 6504, training loss: 8563.50, average training loss: 7448.20, base loss: 16099.81
[INFO 2017-06-29 05:46:07,219 main.py:57] epoch 6505, training loss: 7019.09, average training loss: 7447.50, base loss: 16099.28
[INFO 2017-06-29 05:46:10,313 main.py:57] epoch 6506, training loss: 7030.34, average training loss: 7447.39, base loss: 16099.15
[INFO 2017-06-29 05:46:13,346 main.py:57] epoch 6507, training loss: 7219.78, average training loss: 7447.63, base loss: 16099.05
[INFO 2017-06-29 05:46:16,418 main.py:57] epoch 6508, training loss: 7046.02, average training loss: 7446.73, base loss: 16098.88
[INFO 2017-06-29 05:46:19,488 main.py:57] epoch 6509, training loss: 7550.89, average training loss: 7447.53, base loss: 16098.90
[INFO 2017-06-29 05:46:22,547 main.py:57] epoch 6510, training loss: 6749.15, average training loss: 7446.17, base loss: 16098.74
[INFO 2017-06-29 05:46:25,588 main.py:57] epoch 6511, training loss: 7235.29, average training loss: 7445.68, base loss: 16098.77
[INFO 2017-06-29 05:46:28,561 main.py:57] epoch 6512, training loss: 7256.85, average training loss: 7444.28, base loss: 16098.69
[INFO 2017-06-29 05:46:31,593 main.py:57] epoch 6513, training loss: 7397.65, average training loss: 7443.89, base loss: 16098.52
[INFO 2017-06-29 05:46:34,597 main.py:57] epoch 6514, training loss: 7349.19, average training loss: 7444.43, base loss: 16098.37
[INFO 2017-06-29 05:46:37,797 main.py:57] epoch 6515, training loss: 7677.92, average training loss: 7445.04, base loss: 16098.47
[INFO 2017-06-29 05:46:40,872 main.py:57] epoch 6516, training loss: 7311.23, average training loss: 7444.56, base loss: 16098.99
[INFO 2017-06-29 05:46:43,919 main.py:57] epoch 6517, training loss: 7830.55, average training loss: 7445.06, base loss: 16098.95
[INFO 2017-06-29 05:46:46,944 main.py:57] epoch 6518, training loss: 8142.36, average training loss: 7445.48, base loss: 16099.34
[INFO 2017-06-29 05:46:50,022 main.py:57] epoch 6519, training loss: 8576.22, average training loss: 7448.15, base loss: 16099.84
[INFO 2017-06-29 05:46:53,128 main.py:57] epoch 6520, training loss: 7304.86, average training loss: 7448.11, base loss: 16099.81
[INFO 2017-06-29 05:46:56,210 main.py:57] epoch 6521, training loss: 7611.02, average training loss: 7448.91, base loss: 16099.89
[INFO 2017-06-29 05:46:59,293 main.py:57] epoch 6522, training loss: 8215.81, average training loss: 7449.41, base loss: 16100.31
[INFO 2017-06-29 05:47:02,338 main.py:57] epoch 6523, training loss: 6513.66, average training loss: 7449.13, base loss: 16100.01
[INFO 2017-06-29 05:47:05,412 main.py:57] epoch 6524, training loss: 7071.70, average training loss: 7449.20, base loss: 16099.60
[INFO 2017-06-29 05:47:08,423 main.py:57] epoch 6525, training loss: 7424.05, average training loss: 7448.95, base loss: 16099.28
[INFO 2017-06-29 05:47:11,459 main.py:57] epoch 6526, training loss: 7974.84, average training loss: 7449.05, base loss: 16099.33
[INFO 2017-06-29 05:47:14,477 main.py:57] epoch 6527, training loss: 9352.69, average training loss: 7450.05, base loss: 16100.07
[INFO 2017-06-29 05:47:17,507 main.py:57] epoch 6528, training loss: 6685.38, average training loss: 7448.22, base loss: 16099.69
[INFO 2017-06-29 05:47:20,559 main.py:57] epoch 6529, training loss: 7479.35, average training loss: 7448.61, base loss: 16099.94
[INFO 2017-06-29 05:47:23,604 main.py:57] epoch 6530, training loss: 8888.00, average training loss: 7450.36, base loss: 16100.92
[INFO 2017-06-29 05:47:26,621 main.py:57] epoch 6531, training loss: 7984.02, average training loss: 7450.45, base loss: 16100.99
[INFO 2017-06-29 05:47:29,682 main.py:57] epoch 6532, training loss: 6852.05, average training loss: 7449.90, base loss: 16100.88
[INFO 2017-06-29 05:47:32,752 main.py:57] epoch 6533, training loss: 7844.88, average training loss: 7451.51, base loss: 16101.26
[INFO 2017-06-29 05:47:35,851 main.py:57] epoch 6534, training loss: 6916.77, average training loss: 7451.56, base loss: 16101.46
[INFO 2017-06-29 05:47:38,921 main.py:57] epoch 6535, training loss: 7862.37, average training loss: 7453.01, base loss: 16101.58
[INFO 2017-06-29 05:47:41,931 main.py:57] epoch 6536, training loss: 7852.14, average training loss: 7453.27, base loss: 16102.55
[INFO 2017-06-29 05:47:45,001 main.py:57] epoch 6537, training loss: 7952.93, average training loss: 7453.79, base loss: 16103.29
[INFO 2017-06-29 05:47:48,092 main.py:57] epoch 6538, training loss: 7922.93, average training loss: 7453.77, base loss: 16103.35
[INFO 2017-06-29 05:47:51,158 main.py:57] epoch 6539, training loss: 7065.58, average training loss: 7452.86, base loss: 16102.95
[INFO 2017-06-29 05:47:54,285 main.py:57] epoch 6540, training loss: 7611.30, average training loss: 7452.74, base loss: 16102.54
[INFO 2017-06-29 05:47:57,312 main.py:57] epoch 6541, training loss: 7944.99, average training loss: 7454.09, base loss: 16102.48
[INFO 2017-06-29 05:48:00,433 main.py:57] epoch 6542, training loss: 6760.71, average training loss: 7452.91, base loss: 16102.20
[INFO 2017-06-29 05:48:03,504 main.py:57] epoch 6543, training loss: 7137.75, average training loss: 7451.93, base loss: 16102.31
[INFO 2017-06-29 05:48:06,557 main.py:57] epoch 6544, training loss: 7445.42, average training loss: 7452.27, base loss: 16102.43
[INFO 2017-06-29 05:48:09,604 main.py:57] epoch 6545, training loss: 8860.46, average training loss: 7454.02, base loss: 16103.51
[INFO 2017-06-29 05:48:12,629 main.py:57] epoch 6546, training loss: 7915.22, average training loss: 7454.64, base loss: 16104.09
[INFO 2017-06-29 05:48:15,643 main.py:57] epoch 6547, training loss: 6963.81, average training loss: 7455.08, base loss: 16103.89
[INFO 2017-06-29 05:48:18,685 main.py:57] epoch 6548, training loss: 8132.46, average training loss: 7456.71, base loss: 16104.44
[INFO 2017-06-29 05:48:21,731 main.py:57] epoch 6549, training loss: 7378.13, average training loss: 7456.43, base loss: 16104.26
[INFO 2017-06-29 05:48:24,760 main.py:57] epoch 6550, training loss: 7372.03, average training loss: 7456.34, base loss: 16104.84
[INFO 2017-06-29 05:48:27,861 main.py:57] epoch 6551, training loss: 7745.09, average training loss: 7455.13, base loss: 16105.59
[INFO 2017-06-29 05:48:31,001 main.py:57] epoch 6552, training loss: 7555.62, average training loss: 7455.10, base loss: 16106.18
[INFO 2017-06-29 05:48:34,052 main.py:57] epoch 6553, training loss: 7514.78, average training loss: 7455.24, base loss: 16106.16
[INFO 2017-06-29 05:48:37,086 main.py:57] epoch 6554, training loss: 6517.48, average training loss: 7454.48, base loss: 16105.31
[INFO 2017-06-29 05:48:40,112 main.py:57] epoch 6555, training loss: 7395.74, average training loss: 7452.73, base loss: 16105.05
[INFO 2017-06-29 05:48:43,153 main.py:57] epoch 6556, training loss: 8493.91, average training loss: 7454.11, base loss: 16105.79
[INFO 2017-06-29 05:48:46,184 main.py:57] epoch 6557, training loss: 6903.58, average training loss: 7452.72, base loss: 16105.38
[INFO 2017-06-29 05:48:49,264 main.py:57] epoch 6558, training loss: 7300.16, average training loss: 7453.06, base loss: 16105.36
[INFO 2017-06-29 05:48:52,264 main.py:57] epoch 6559, training loss: 7185.60, average training loss: 7450.83, base loss: 16105.03
[INFO 2017-06-29 05:48:55,295 main.py:57] epoch 6560, training loss: 7167.46, average training loss: 7450.67, base loss: 16104.52
[INFO 2017-06-29 05:48:58,361 main.py:57] epoch 6561, training loss: 9081.38, average training loss: 7452.16, base loss: 16105.23
[INFO 2017-06-29 05:49:01,384 main.py:57] epoch 6562, training loss: 7089.02, average training loss: 7451.10, base loss: 16105.15
[INFO 2017-06-29 05:49:04,451 main.py:57] epoch 6563, training loss: 7189.22, average training loss: 7450.84, base loss: 16105.08
[INFO 2017-06-29 05:49:07,483 main.py:57] epoch 6564, training loss: 7032.64, average training loss: 7450.05, base loss: 16104.87
[INFO 2017-06-29 05:49:10,567 main.py:57] epoch 6565, training loss: 7602.70, average training loss: 7450.35, base loss: 16104.87
[INFO 2017-06-29 05:49:13,656 main.py:57] epoch 6566, training loss: 7902.34, average training loss: 7451.90, base loss: 16105.64
[INFO 2017-06-29 05:49:16,676 main.py:57] epoch 6567, training loss: 7519.15, average training loss: 7451.76, base loss: 16105.68
[INFO 2017-06-29 05:49:19,753 main.py:57] epoch 6568, training loss: 8053.89, average training loss: 7450.50, base loss: 16105.92
[INFO 2017-06-29 05:49:22,919 main.py:57] epoch 6569, training loss: 7717.85, average training loss: 7450.84, base loss: 16105.80
[INFO 2017-06-29 05:49:25,959 main.py:57] epoch 6570, training loss: 7703.67, average training loss: 7451.01, base loss: 16106.32
[INFO 2017-06-29 05:49:28,974 main.py:57] epoch 6571, training loss: 8120.29, average training loss: 7452.41, base loss: 16106.83
[INFO 2017-06-29 05:49:32,007 main.py:57] epoch 6572, training loss: 7046.32, average training loss: 7451.93, base loss: 16106.40
[INFO 2017-06-29 05:49:35,176 main.py:57] epoch 6573, training loss: 7386.33, average training loss: 7451.98, base loss: 16106.22
[INFO 2017-06-29 05:49:38,166 main.py:57] epoch 6574, training loss: 7590.89, average training loss: 7452.94, base loss: 16106.52
[INFO 2017-06-29 05:49:41,262 main.py:57] epoch 6575, training loss: 7464.46, average training loss: 7453.00, base loss: 16106.53
[INFO 2017-06-29 05:49:44,319 main.py:57] epoch 6576, training loss: 6981.01, average training loss: 7452.07, base loss: 16105.82
[INFO 2017-06-29 05:49:47,456 main.py:57] epoch 6577, training loss: 7028.97, average training loss: 7452.40, base loss: 16105.13
[INFO 2017-06-29 05:49:50,568 main.py:57] epoch 6578, training loss: 7970.20, average training loss: 7452.21, base loss: 16105.14
[INFO 2017-06-29 05:49:53,600 main.py:57] epoch 6579, training loss: 7915.38, average training loss: 7452.65, base loss: 16105.45
[INFO 2017-06-29 05:49:56,703 main.py:57] epoch 6580, training loss: 6926.18, average training loss: 7452.29, base loss: 16104.89
[INFO 2017-06-29 05:49:59,830 main.py:57] epoch 6581, training loss: 7549.36, average training loss: 7452.97, base loss: 16104.74
[INFO 2017-06-29 05:50:02,918 main.py:57] epoch 6582, training loss: 8833.76, average training loss: 7454.64, base loss: 16105.14
[INFO 2017-06-29 05:50:06,031 main.py:57] epoch 6583, training loss: 7001.12, average training loss: 7454.14, base loss: 16104.86
[INFO 2017-06-29 05:50:09,184 main.py:57] epoch 6584, training loss: 6867.67, average training loss: 7453.79, base loss: 16104.37
[INFO 2017-06-29 05:50:12,235 main.py:57] epoch 6585, training loss: 7548.97, average training loss: 7453.67, base loss: 16104.52
[INFO 2017-06-29 05:50:15,274 main.py:57] epoch 6586, training loss: 7380.35, average training loss: 7454.30, base loss: 16104.83
[INFO 2017-06-29 05:50:18,345 main.py:57] epoch 6587, training loss: 7813.66, average training loss: 7454.42, base loss: 16105.79
[INFO 2017-06-29 05:50:21,406 main.py:57] epoch 6588, training loss: 7690.71, average training loss: 7454.76, base loss: 16106.21
[INFO 2017-06-29 05:50:24,423 main.py:57] epoch 6589, training loss: 6741.15, average training loss: 7453.74, base loss: 16105.66
[INFO 2017-06-29 05:50:27,450 main.py:57] epoch 6590, training loss: 7156.81, average training loss: 7453.20, base loss: 16105.36
[INFO 2017-06-29 05:50:30,520 main.py:57] epoch 6591, training loss: 6390.90, average training loss: 7452.28, base loss: 16104.51
[INFO 2017-06-29 05:50:33,599 main.py:57] epoch 6592, training loss: 7163.38, average training loss: 7452.51, base loss: 16103.94
[INFO 2017-06-29 05:50:36,624 main.py:57] epoch 6593, training loss: 7276.77, average training loss: 7451.52, base loss: 16104.11
[INFO 2017-06-29 05:50:39,712 main.py:57] epoch 6594, training loss: 6431.04, average training loss: 7449.81, base loss: 16103.84
[INFO 2017-06-29 05:50:42,823 main.py:57] epoch 6595, training loss: 6620.32, average training loss: 7449.08, base loss: 16103.22
[INFO 2017-06-29 05:50:45,895 main.py:57] epoch 6596, training loss: 7163.53, average training loss: 7449.33, base loss: 16103.14
[INFO 2017-06-29 05:50:48,998 main.py:57] epoch 6597, training loss: 7585.38, average training loss: 7449.59, base loss: 16103.33
[INFO 2017-06-29 05:50:52,080 main.py:57] epoch 6598, training loss: 7212.15, average training loss: 7449.38, base loss: 16102.94
[INFO 2017-06-29 05:50:55,127 main.py:57] epoch 6599, training loss: 8051.27, average training loss: 7448.24, base loss: 16103.38
[INFO 2017-06-29 05:50:55,127 main.py:59] epoch 6599, testing
[INFO 2017-06-29 05:51:07,790 main.py:104] average testing loss: 8161.88, base loss: 16944.97
[INFO 2017-06-29 05:51:07,790 main.py:105] improve_loss: 8783.09, improve_percent: 0.52
[INFO 2017-06-29 05:51:07,791 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:51:10,831 main.py:57] epoch 6600, training loss: 7691.17, average training loss: 7449.83, base loss: 16103.40
[INFO 2017-06-29 05:51:13,875 main.py:57] epoch 6601, training loss: 6335.21, average training loss: 7448.47, base loss: 16102.88
[INFO 2017-06-29 05:51:16,909 main.py:57] epoch 6602, training loss: 6912.55, average training loss: 7448.15, base loss: 16102.72
[INFO 2017-06-29 05:51:19,977 main.py:57] epoch 6603, training loss: 7443.28, average training loss: 7448.50, base loss: 16102.81
[INFO 2017-06-29 05:51:23,074 main.py:57] epoch 6604, training loss: 7342.62, average training loss: 7447.99, base loss: 16102.88
[INFO 2017-06-29 05:51:26,084 main.py:57] epoch 6605, training loss: 7620.44, average training loss: 7448.36, base loss: 16102.98
[INFO 2017-06-29 05:51:29,177 main.py:57] epoch 6606, training loss: 8141.74, average training loss: 7449.37, base loss: 16103.51
[INFO 2017-06-29 05:51:32,214 main.py:57] epoch 6607, training loss: 6944.12, average training loss: 7449.25, base loss: 16103.10
[INFO 2017-06-29 05:51:35,265 main.py:57] epoch 6608, training loss: 6757.28, average training loss: 7448.18, base loss: 16102.28
[INFO 2017-06-29 05:51:38,346 main.py:57] epoch 6609, training loss: 8369.21, average training loss: 7447.52, base loss: 16102.43
[INFO 2017-06-29 05:51:41,342 main.py:57] epoch 6610, training loss: 7201.58, average training loss: 7447.02, base loss: 16102.27
[INFO 2017-06-29 05:51:44,361 main.py:57] epoch 6611, training loss: 7653.91, average training loss: 7447.22, base loss: 16102.32
[INFO 2017-06-29 05:51:47,457 main.py:57] epoch 6612, training loss: 7034.75, average training loss: 7446.88, base loss: 16102.69
[INFO 2017-06-29 05:51:50,531 main.py:57] epoch 6613, training loss: 7033.07, average training loss: 7445.87, base loss: 16103.19
[INFO 2017-06-29 05:51:53,559 main.py:57] epoch 6614, training loss: 8389.35, average training loss: 7447.03, base loss: 16104.51
[INFO 2017-06-29 05:51:56,577 main.py:57] epoch 6615, training loss: 6781.00, average training loss: 7446.93, base loss: 16104.46
[INFO 2017-06-29 05:51:59,715 main.py:57] epoch 6616, training loss: 6758.94, average training loss: 7446.33, base loss: 16104.04
[INFO 2017-06-29 05:52:02,809 main.py:57] epoch 6617, training loss: 6487.71, average training loss: 7445.76, base loss: 16103.09
[INFO 2017-06-29 05:52:05,911 main.py:57] epoch 6618, training loss: 7703.32, average training loss: 7446.31, base loss: 16102.72
[INFO 2017-06-29 05:52:08,945 main.py:57] epoch 6619, training loss: 8214.17, average training loss: 7446.59, base loss: 16102.99
[INFO 2017-06-29 05:52:12,002 main.py:57] epoch 6620, training loss: 7170.16, average training loss: 7445.01, base loss: 16102.36
[INFO 2017-06-29 05:52:15,034 main.py:57] epoch 6621, training loss: 6645.50, average training loss: 7444.35, base loss: 16101.80
[INFO 2017-06-29 05:52:18,105 main.py:57] epoch 6622, training loss: 6834.86, average training loss: 7443.92, base loss: 16101.37
[INFO 2017-06-29 05:52:21,207 main.py:57] epoch 6623, training loss: 6522.88, average training loss: 7443.70, base loss: 16100.75
[INFO 2017-06-29 05:52:24,203 main.py:57] epoch 6624, training loss: 7243.59, average training loss: 7442.97, base loss: 16100.78
[INFO 2017-06-29 05:52:27,229 main.py:57] epoch 6625, training loss: 7004.40, average training loss: 7442.27, base loss: 16100.67
[INFO 2017-06-29 05:52:30,284 main.py:57] epoch 6626, training loss: 6993.96, average training loss: 7442.04, base loss: 16100.56
[INFO 2017-06-29 05:52:33,333 main.py:57] epoch 6627, training loss: 6638.73, average training loss: 7440.78, base loss: 16100.36
[INFO 2017-06-29 05:52:36,355 main.py:57] epoch 6628, training loss: 6808.70, average training loss: 7439.60, base loss: 16100.10
[INFO 2017-06-29 05:52:39,377 main.py:57] epoch 6629, training loss: 6502.33, average training loss: 7438.90, base loss: 16099.59
[INFO 2017-06-29 05:52:42,389 main.py:57] epoch 6630, training loss: 7673.13, average training loss: 7439.14, base loss: 16099.64
[INFO 2017-06-29 05:52:45,400 main.py:57] epoch 6631, training loss: 7329.07, average training loss: 7438.29, base loss: 16099.70
[INFO 2017-06-29 05:52:48,461 main.py:57] epoch 6632, training loss: 7184.99, average training loss: 7437.80, base loss: 16099.08
[INFO 2017-06-29 05:52:51,528 main.py:57] epoch 6633, training loss: 7114.59, average training loss: 7436.86, base loss: 16098.62
[INFO 2017-06-29 05:52:54,617 main.py:57] epoch 6634, training loss: 6739.22, average training loss: 7436.57, base loss: 16098.26
[INFO 2017-06-29 05:52:57,673 main.py:57] epoch 6635, training loss: 7354.94, average training loss: 7435.46, base loss: 16098.38
[INFO 2017-06-29 05:53:00,765 main.py:57] epoch 6636, training loss: 7831.20, average training loss: 7435.35, base loss: 16099.14
[INFO 2017-06-29 05:53:03,795 main.py:57] epoch 6637, training loss: 8169.08, average training loss: 7436.17, base loss: 16099.21
[INFO 2017-06-29 05:53:06,889 main.py:57] epoch 6638, training loss: 8683.27, average training loss: 7437.89, base loss: 16099.73
[INFO 2017-06-29 05:53:09,961 main.py:57] epoch 6639, training loss: 8006.74, average training loss: 7438.54, base loss: 16099.91
[INFO 2017-06-29 05:53:13,073 main.py:57] epoch 6640, training loss: 7473.26, average training loss: 7437.98, base loss: 16100.01
[INFO 2017-06-29 05:53:16,130 main.py:57] epoch 6641, training loss: 7528.59, average training loss: 7437.41, base loss: 16100.01
[INFO 2017-06-29 05:53:19,160 main.py:57] epoch 6642, training loss: 7003.34, average training loss: 7436.80, base loss: 16099.70
[INFO 2017-06-29 05:53:22,210 main.py:57] epoch 6643, training loss: 7073.79, average training loss: 7436.78, base loss: 16099.60
[INFO 2017-06-29 05:53:25,336 main.py:57] epoch 6644, training loss: 7748.44, average training loss: 7436.68, base loss: 16099.62
[INFO 2017-06-29 05:53:28,329 main.py:57] epoch 6645, training loss: 7551.97, average training loss: 7437.20, base loss: 16099.50
[INFO 2017-06-29 05:53:31,502 main.py:57] epoch 6646, training loss: 7877.18, average training loss: 7438.43, base loss: 16099.75
[INFO 2017-06-29 05:53:34,576 main.py:57] epoch 6647, training loss: 7551.24, average training loss: 7438.32, base loss: 16100.04
[INFO 2017-06-29 05:53:37,620 main.py:57] epoch 6648, training loss: 7077.72, average training loss: 7437.87, base loss: 16100.19
[INFO 2017-06-29 05:53:40,667 main.py:57] epoch 6649, training loss: 7246.72, average training loss: 7437.50, base loss: 16100.12
[INFO 2017-06-29 05:53:43,655 main.py:57] epoch 6650, training loss: 7115.14, average training loss: 7436.75, base loss: 16099.61
[INFO 2017-06-29 05:53:46,720 main.py:57] epoch 6651, training loss: 7578.37, average training loss: 7437.99, base loss: 16099.85
[INFO 2017-06-29 05:53:49,797 main.py:57] epoch 6652, training loss: 7258.36, average training loss: 7437.72, base loss: 16100.36
[INFO 2017-06-29 05:53:52,847 main.py:57] epoch 6653, training loss: 6315.13, average training loss: 7437.46, base loss: 16100.08
[INFO 2017-06-29 05:53:55,828 main.py:57] epoch 6654, training loss: 7933.14, average training loss: 7437.32, base loss: 16100.23
[INFO 2017-06-29 05:53:58,898 main.py:57] epoch 6655, training loss: 7797.99, average training loss: 7436.45, base loss: 16100.32
[INFO 2017-06-29 05:54:01,953 main.py:57] epoch 6656, training loss: 6955.96, average training loss: 7436.25, base loss: 16100.16
[INFO 2017-06-29 05:54:05,039 main.py:57] epoch 6657, training loss: 7497.88, average training loss: 7436.58, base loss: 16100.41
[INFO 2017-06-29 05:54:07,991 main.py:57] epoch 6658, training loss: 8899.65, average training loss: 7437.03, base loss: 16101.10
[INFO 2017-06-29 05:54:11,134 main.py:57] epoch 6659, training loss: 7979.32, average training loss: 7436.78, base loss: 16101.40
[INFO 2017-06-29 05:54:14,155 main.py:57] epoch 6660, training loss: 7411.18, average training loss: 7436.40, base loss: 16101.74
[INFO 2017-06-29 05:54:17,224 main.py:57] epoch 6661, training loss: 7869.50, average training loss: 7436.85, base loss: 16102.19
[INFO 2017-06-29 05:54:20,284 main.py:57] epoch 6662, training loss: 7661.08, average training loss: 7437.87, base loss: 16102.46
[INFO 2017-06-29 05:54:23,321 main.py:57] epoch 6663, training loss: 7888.31, average training loss: 7436.76, base loss: 16103.08
[INFO 2017-06-29 05:54:26,376 main.py:57] epoch 6664, training loss: 7617.35, average training loss: 7436.62, base loss: 16103.33
[INFO 2017-06-29 05:54:29,425 main.py:57] epoch 6665, training loss: 8211.06, average training loss: 7437.24, base loss: 16103.88
[INFO 2017-06-29 05:54:32,449 main.py:57] epoch 6666, training loss: 6328.11, average training loss: 7436.33, base loss: 16103.57
[INFO 2017-06-29 05:54:35,492 main.py:57] epoch 6667, training loss: 7052.44, average training loss: 7436.27, base loss: 16103.52
[INFO 2017-06-29 05:54:38,488 main.py:57] epoch 6668, training loss: 6387.44, average training loss: 7434.59, base loss: 16102.91
[INFO 2017-06-29 05:54:41,566 main.py:57] epoch 6669, training loss: 7266.79, average training loss: 7433.88, base loss: 16102.78
[INFO 2017-06-29 05:54:44,617 main.py:57] epoch 6670, training loss: 6768.82, average training loss: 7432.88, base loss: 16102.55
[INFO 2017-06-29 05:54:47,721 main.py:57] epoch 6671, training loss: 7557.19, average training loss: 7433.49, base loss: 16102.18
[INFO 2017-06-29 05:54:50,762 main.py:57] epoch 6672, training loss: 7123.25, average training loss: 7431.86, base loss: 16101.83
[INFO 2017-06-29 05:54:53,806 main.py:57] epoch 6673, training loss: 8120.53, average training loss: 7432.02, base loss: 16102.06
[INFO 2017-06-29 05:54:56,892 main.py:57] epoch 6674, training loss: 8151.70, average training loss: 7433.99, base loss: 16102.61
[INFO 2017-06-29 05:54:59,944 main.py:57] epoch 6675, training loss: 6264.08, average training loss: 7433.01, base loss: 16102.05
[INFO 2017-06-29 05:55:03,037 main.py:57] epoch 6676, training loss: 6464.86, average training loss: 7431.87, base loss: 16101.68
[INFO 2017-06-29 05:55:06,082 main.py:57] epoch 6677, training loss: 6685.70, average training loss: 7431.53, base loss: 16101.51
[INFO 2017-06-29 05:55:09,099 main.py:57] epoch 6678, training loss: 7791.65, average training loss: 7431.49, base loss: 16101.69
[INFO 2017-06-29 05:55:12,135 main.py:57] epoch 6679, training loss: 8123.44, average training loss: 7432.51, base loss: 16102.07
[INFO 2017-06-29 05:55:15,176 main.py:57] epoch 6680, training loss: 6986.70, average training loss: 7432.26, base loss: 16102.28
[INFO 2017-06-29 05:55:18,226 main.py:57] epoch 6681, training loss: 8493.77, average training loss: 7433.91, base loss: 16102.72
[INFO 2017-06-29 05:55:21,181 main.py:57] epoch 6682, training loss: 7057.48, average training loss: 7433.30, base loss: 16103.29
[INFO 2017-06-29 05:55:24,324 main.py:57] epoch 6683, training loss: 6908.24, average training loss: 7433.41, base loss: 16103.29
[INFO 2017-06-29 05:55:27,351 main.py:57] epoch 6684, training loss: 7075.08, average training loss: 7433.75, base loss: 16102.90
[INFO 2017-06-29 05:55:30,459 main.py:57] epoch 6685, training loss: 7238.72, average training loss: 7433.35, base loss: 16102.47
[INFO 2017-06-29 05:55:33,470 main.py:57] epoch 6686, training loss: 6563.01, average training loss: 7433.42, base loss: 16101.71
[INFO 2017-06-29 05:55:36,540 main.py:57] epoch 6687, training loss: 6311.61, average training loss: 7432.01, base loss: 16100.83
[INFO 2017-06-29 05:55:39,534 main.py:57] epoch 6688, training loss: 7158.42, average training loss: 7432.52, base loss: 16100.41
[INFO 2017-06-29 05:55:42,645 main.py:57] epoch 6689, training loss: 8207.52, average training loss: 7433.49, base loss: 16100.79
[INFO 2017-06-29 05:55:45,667 main.py:57] epoch 6690, training loss: 7423.88, average training loss: 7432.75, base loss: 16101.03
[INFO 2017-06-29 05:55:48,742 main.py:57] epoch 6691, training loss: 7838.35, average training loss: 7432.83, base loss: 16101.52
[INFO 2017-06-29 05:55:51,851 main.py:57] epoch 6692, training loss: 6723.15, average training loss: 7432.24, base loss: 16100.85
[INFO 2017-06-29 05:55:54,876 main.py:57] epoch 6693, training loss: 7364.75, average training loss: 7431.65, base loss: 16100.47
[INFO 2017-06-29 05:55:57,959 main.py:57] epoch 6694, training loss: 8658.01, average training loss: 7432.66, base loss: 16100.92
[INFO 2017-06-29 05:56:01,024 main.py:57] epoch 6695, training loss: 7934.03, average training loss: 7433.82, base loss: 16101.27
[INFO 2017-06-29 05:56:04,041 main.py:57] epoch 6696, training loss: 7296.77, average training loss: 7432.86, base loss: 16101.25
[INFO 2017-06-29 05:56:07,066 main.py:57] epoch 6697, training loss: 7122.87, average training loss: 7432.94, base loss: 16100.95
[INFO 2017-06-29 05:56:10,164 main.py:57] epoch 6698, training loss: 6861.04, average training loss: 7432.08, base loss: 16100.85
[INFO 2017-06-29 05:56:13,227 main.py:57] epoch 6699, training loss: 6703.80, average training loss: 7431.87, base loss: 16099.87
[INFO 2017-06-29 05:56:13,227 main.py:59] epoch 6699, testing
[INFO 2017-06-29 05:56:25,752 main.py:104] average testing loss: 8077.86, base loss: 16714.03
[INFO 2017-06-29 05:56:25,753 main.py:105] improve_loss: 8636.17, improve_percent: 0.52
[INFO 2017-06-29 05:56:25,754 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 05:56:28,882 main.py:57] epoch 6700, training loss: 7743.66, average training loss: 7433.24, base loss: 16099.90
[INFO 2017-06-29 05:56:31,863 main.py:57] epoch 6701, training loss: 8290.92, average training loss: 7434.27, base loss: 16100.13
[INFO 2017-06-29 05:56:34,903 main.py:57] epoch 6702, training loss: 7843.71, average training loss: 7434.61, base loss: 16100.77
[INFO 2017-06-29 05:56:37,869 main.py:57] epoch 6703, training loss: 7954.16, average training loss: 7435.05, base loss: 16101.25
[INFO 2017-06-29 05:56:40,973 main.py:57] epoch 6704, training loss: 7405.34, average training loss: 7434.36, base loss: 16101.26
[INFO 2017-06-29 05:56:43,991 main.py:57] epoch 6705, training loss: 6896.83, average training loss: 7433.74, base loss: 16101.22
[INFO 2017-06-29 05:56:47,011 main.py:57] epoch 6706, training loss: 8253.57, average training loss: 7434.82, base loss: 16101.93
[INFO 2017-06-29 05:56:50,096 main.py:57] epoch 6707, training loss: 7105.07, average training loss: 7434.62, base loss: 16102.21
[INFO 2017-06-29 05:56:53,071 main.py:57] epoch 6708, training loss: 7967.82, average training loss: 7434.70, base loss: 16102.58
[INFO 2017-06-29 05:56:56,101 main.py:57] epoch 6709, training loss: 7290.88, average training loss: 7434.74, base loss: 16102.81
[INFO 2017-06-29 05:56:59,160 main.py:57] epoch 6710, training loss: 7022.51, average training loss: 7435.02, base loss: 16102.80
[INFO 2017-06-29 05:57:02,259 main.py:57] epoch 6711, training loss: 7205.08, average training loss: 7435.54, base loss: 16102.59
[INFO 2017-06-29 05:57:05,316 main.py:57] epoch 6712, training loss: 9121.66, average training loss: 7436.71, base loss: 16103.30
[INFO 2017-06-29 05:57:08,415 main.py:57] epoch 6713, training loss: 6656.44, average training loss: 7435.17, base loss: 16102.80
[INFO 2017-06-29 05:57:11,477 main.py:57] epoch 6714, training loss: 8597.42, average training loss: 7435.81, base loss: 16103.42
[INFO 2017-06-29 05:57:14,521 main.py:57] epoch 6715, training loss: 6951.74, average training loss: 7434.85, base loss: 16103.14
[INFO 2017-06-29 05:57:17,518 main.py:57] epoch 6716, training loss: 7749.28, average training loss: 7435.39, base loss: 16103.53
[INFO 2017-06-29 05:57:20,590 main.py:57] epoch 6717, training loss: 7088.23, average training loss: 7435.44, base loss: 16103.39
[INFO 2017-06-29 05:57:23,687 main.py:57] epoch 6718, training loss: 7981.89, average training loss: 7435.84, base loss: 16103.17
[INFO 2017-06-29 05:57:26,739 main.py:57] epoch 6719, training loss: 6638.73, average training loss: 7435.42, base loss: 16102.54
[INFO 2017-06-29 05:57:29,784 main.py:57] epoch 6720, training loss: 7069.53, average training loss: 7434.73, base loss: 16102.43
[INFO 2017-06-29 05:57:32,840 main.py:57] epoch 6721, training loss: 6727.49, average training loss: 7433.77, base loss: 16102.16
[INFO 2017-06-29 05:57:35,909 main.py:57] epoch 6722, training loss: 6603.60, average training loss: 7432.89, base loss: 16101.63
[INFO 2017-06-29 05:57:38,997 main.py:57] epoch 6723, training loss: 6061.55, average training loss: 7431.65, base loss: 16100.88
[INFO 2017-06-29 05:57:42,037 main.py:57] epoch 6724, training loss: 7558.81, average training loss: 7432.32, base loss: 16101.22
[INFO 2017-06-29 05:57:45,099 main.py:57] epoch 6725, training loss: 7360.31, average training loss: 7430.32, base loss: 16101.53
[INFO 2017-06-29 05:57:48,178 main.py:57] epoch 6726, training loss: 7191.80, average training loss: 7429.95, base loss: 16101.49
[INFO 2017-06-29 05:57:51,242 main.py:57] epoch 6727, training loss: 8043.50, average training loss: 7430.31, base loss: 16101.55
[INFO 2017-06-29 05:57:54,332 main.py:57] epoch 6728, training loss: 6728.23, average training loss: 7429.36, base loss: 16100.98
[INFO 2017-06-29 05:57:57,420 main.py:57] epoch 6729, training loss: 7673.47, average training loss: 7429.67, base loss: 16100.68
[INFO 2017-06-29 05:58:00,504 main.py:57] epoch 6730, training loss: 7000.21, average training loss: 7429.39, base loss: 16100.24
[INFO 2017-06-29 05:58:03,542 main.py:57] epoch 6731, training loss: 7411.00, average training loss: 7429.34, base loss: 16099.92
[INFO 2017-06-29 05:58:06,646 main.py:57] epoch 6732, training loss: 8034.41, average training loss: 7429.52, base loss: 16099.98
[INFO 2017-06-29 05:58:09,740 main.py:57] epoch 6733, training loss: 7475.55, average training loss: 7429.66, base loss: 16099.90
[INFO 2017-06-29 05:58:12,792 main.py:57] epoch 6734, training loss: 6323.29, average training loss: 7428.03, base loss: 16098.94
[INFO 2017-06-29 05:58:15,865 main.py:57] epoch 6735, training loss: 7244.47, average training loss: 7427.89, base loss: 16098.83
[INFO 2017-06-29 05:58:18,937 main.py:57] epoch 6736, training loss: 7038.75, average training loss: 7427.55, base loss: 16098.45
[INFO 2017-06-29 05:58:22,020 main.py:57] epoch 6737, training loss: 6476.92, average training loss: 7426.88, base loss: 16097.55
[INFO 2017-06-29 05:58:25,115 main.py:57] epoch 6738, training loss: 6980.37, average training loss: 7425.97, base loss: 16097.17
[INFO 2017-06-29 05:58:28,228 main.py:57] epoch 6739, training loss: 7844.19, average training loss: 7426.68, base loss: 16097.39
[INFO 2017-06-29 05:58:31,268 main.py:57] epoch 6740, training loss: 7529.63, average training loss: 7427.40, base loss: 16097.39
[INFO 2017-06-29 05:58:34,279 main.py:57] epoch 6741, training loss: 6989.23, average training loss: 7426.91, base loss: 16096.99
[INFO 2017-06-29 05:58:37,324 main.py:57] epoch 6742, training loss: 6761.77, average training loss: 7426.21, base loss: 16096.78
[INFO 2017-06-29 05:58:40,316 main.py:57] epoch 6743, training loss: 6538.56, average training loss: 7426.10, base loss: 16096.10
[INFO 2017-06-29 05:58:43,435 main.py:57] epoch 6744, training loss: 7660.14, average training loss: 7426.77, base loss: 16096.38
[INFO 2017-06-29 05:58:46,482 main.py:57] epoch 6745, training loss: 7170.46, average training loss: 7426.66, base loss: 16096.01
[INFO 2017-06-29 05:58:49,537 main.py:57] epoch 6746, training loss: 7432.54, average training loss: 7427.20, base loss: 16096.68
[INFO 2017-06-29 05:58:52,636 main.py:57] epoch 6747, training loss: 7421.96, average training loss: 7427.56, base loss: 16097.08
[INFO 2017-06-29 05:58:55,636 main.py:57] epoch 6748, training loss: 7806.17, average training loss: 7428.73, base loss: 16097.07
[INFO 2017-06-29 05:58:58,655 main.py:57] epoch 6749, training loss: 7640.77, average training loss: 7428.84, base loss: 16096.83
[INFO 2017-06-29 05:59:01,648 main.py:57] epoch 6750, training loss: 6609.04, average training loss: 7427.44, base loss: 16096.42
[INFO 2017-06-29 05:59:04,696 main.py:57] epoch 6751, training loss: 7370.76, average training loss: 7426.29, base loss: 16096.27
[INFO 2017-06-29 05:59:07,759 main.py:57] epoch 6752, training loss: 6830.36, average training loss: 7425.08, base loss: 16095.92
[INFO 2017-06-29 05:59:10,780 main.py:57] epoch 6753, training loss: 7377.29, average training loss: 7426.30, base loss: 16096.26
[INFO 2017-06-29 05:59:13,843 main.py:57] epoch 6754, training loss: 7280.99, average training loss: 7426.29, base loss: 16096.20
[INFO 2017-06-29 05:59:16,930 main.py:57] epoch 6755, training loss: 7163.63, average training loss: 7426.76, base loss: 16095.73
[INFO 2017-06-29 05:59:19,964 main.py:57] epoch 6756, training loss: 6813.28, average training loss: 7425.59, base loss: 16095.10
[INFO 2017-06-29 05:59:23,085 main.py:57] epoch 6757, training loss: 7340.24, average training loss: 7424.74, base loss: 16095.39
[INFO 2017-06-29 05:59:26,144 main.py:57] epoch 6758, training loss: 8310.83, average training loss: 7425.84, base loss: 16095.41
[INFO 2017-06-29 05:59:29,218 main.py:57] epoch 6759, training loss: 6756.33, average training loss: 7425.31, base loss: 16094.81
[INFO 2017-06-29 05:59:32,244 main.py:57] epoch 6760, training loss: 8868.20, average training loss: 7427.21, base loss: 16095.39
[INFO 2017-06-29 05:59:35,292 main.py:57] epoch 6761, training loss: 7730.38, average training loss: 7427.13, base loss: 16095.91
[INFO 2017-06-29 05:59:38,339 main.py:57] epoch 6762, training loss: 7503.22, average training loss: 7427.87, base loss: 16095.61
[INFO 2017-06-29 05:59:41,421 main.py:57] epoch 6763, training loss: 6592.38, average training loss: 7427.52, base loss: 16094.69
[INFO 2017-06-29 05:59:44,471 main.py:57] epoch 6764, training loss: 8086.72, average training loss: 7427.79, base loss: 16095.08
[INFO 2017-06-29 05:59:47,615 main.py:57] epoch 6765, training loss: 7644.53, average training loss: 7427.67, base loss: 16095.28
[INFO 2017-06-29 05:59:50,612 main.py:57] epoch 6766, training loss: 6974.46, average training loss: 7427.54, base loss: 16095.30
[INFO 2017-06-29 05:59:53,661 main.py:57] epoch 6767, training loss: 6791.45, average training loss: 7427.37, base loss: 16095.40
[INFO 2017-06-29 05:59:56,694 main.py:57] epoch 6768, training loss: 7669.81, average training loss: 7426.75, base loss: 16095.32
[INFO 2017-06-29 05:59:59,742 main.py:57] epoch 6769, training loss: 6704.17, average training loss: 7425.44, base loss: 16094.83
[INFO 2017-06-29 06:00:02,767 main.py:57] epoch 6770, training loss: 6977.79, average training loss: 7425.46, base loss: 16094.47
[INFO 2017-06-29 06:00:05,803 main.py:57] epoch 6771, training loss: 7550.78, average training loss: 7426.07, base loss: 16094.27
[INFO 2017-06-29 06:00:08,903 main.py:57] epoch 6772, training loss: 7579.42, average training loss: 7425.98, base loss: 16094.08
[INFO 2017-06-29 06:00:12,039 main.py:57] epoch 6773, training loss: 6832.26, average training loss: 7424.98, base loss: 16093.59
[INFO 2017-06-29 06:00:15,131 main.py:57] epoch 6774, training loss: 7751.58, average training loss: 7425.74, base loss: 16094.25
[INFO 2017-06-29 06:00:18,149 main.py:57] epoch 6775, training loss: 7156.86, average training loss: 7426.40, base loss: 16094.32
[INFO 2017-06-29 06:00:21,213 main.py:57] epoch 6776, training loss: 7533.47, average training loss: 7426.91, base loss: 16094.71
[INFO 2017-06-29 06:00:24,304 main.py:57] epoch 6777, training loss: 7724.52, average training loss: 7427.80, base loss: 16094.82
[INFO 2017-06-29 06:00:27,323 main.py:57] epoch 6778, training loss: 8577.80, average training loss: 7429.01, base loss: 16095.42
[INFO 2017-06-29 06:00:30,430 main.py:57] epoch 6779, training loss: 7780.33, average training loss: 7430.24, base loss: 16095.70
[INFO 2017-06-29 06:00:33,491 main.py:57] epoch 6780, training loss: 6678.85, average training loss: 7429.79, base loss: 16095.33
[INFO 2017-06-29 06:00:36,514 main.py:57] epoch 6781, training loss: 8494.10, average training loss: 7430.61, base loss: 16095.67
[INFO 2017-06-29 06:00:39,551 main.py:57] epoch 6782, training loss: 7260.52, average training loss: 7431.41, base loss: 16095.54
[INFO 2017-06-29 06:00:42,571 main.py:57] epoch 6783, training loss: 7327.37, average training loss: 7431.01, base loss: 16095.77
[INFO 2017-06-29 06:00:45,656 main.py:57] epoch 6784, training loss: 8526.04, average training loss: 7431.67, base loss: 16095.95
[INFO 2017-06-29 06:00:48,652 main.py:57] epoch 6785, training loss: 8026.20, average training loss: 7431.41, base loss: 16095.79
[INFO 2017-06-29 06:00:51,762 main.py:57] epoch 6786, training loss: 6772.39, average training loss: 7430.98, base loss: 16095.66
[INFO 2017-06-29 06:00:54,793 main.py:57] epoch 6787, training loss: 6297.77, average training loss: 7428.89, base loss: 16095.33
[INFO 2017-06-29 06:00:57,886 main.py:57] epoch 6788, training loss: 7101.05, average training loss: 7428.56, base loss: 16095.31
[INFO 2017-06-29 06:01:00,933 main.py:57] epoch 6789, training loss: 7326.42, average training loss: 7428.49, base loss: 16095.25
[INFO 2017-06-29 06:01:03,978 main.py:57] epoch 6790, training loss: 7358.26, average training loss: 7428.20, base loss: 16094.99
[INFO 2017-06-29 06:01:06,999 main.py:57] epoch 6791, training loss: 8085.83, average training loss: 7429.46, base loss: 16095.36
[INFO 2017-06-29 06:01:10,074 main.py:57] epoch 6792, training loss: 7741.40, average training loss: 7429.19, base loss: 16095.27
[INFO 2017-06-29 06:01:13,124 main.py:57] epoch 6793, training loss: 7687.54, average training loss: 7429.89, base loss: 16095.26
[INFO 2017-06-29 06:01:16,259 main.py:57] epoch 6794, training loss: 7740.35, average training loss: 7429.50, base loss: 16095.22
[INFO 2017-06-29 06:01:19,300 main.py:57] epoch 6795, training loss: 7062.80, average training loss: 7430.16, base loss: 16095.24
[INFO 2017-06-29 06:01:22,321 main.py:57] epoch 6796, training loss: 7174.96, average training loss: 7430.14, base loss: 16095.03
[INFO 2017-06-29 06:01:25,393 main.py:57] epoch 6797, training loss: 8026.57, average training loss: 7430.61, base loss: 16095.59
[INFO 2017-06-29 06:01:28,421 main.py:57] epoch 6798, training loss: 7127.11, average training loss: 7431.05, base loss: 16094.81
[INFO 2017-06-29 06:01:31,456 main.py:57] epoch 6799, training loss: 7516.83, average training loss: 7432.21, base loss: 16094.20
[INFO 2017-06-29 06:01:31,456 main.py:59] epoch 6799, testing
[INFO 2017-06-29 06:01:44,143 main.py:104] average testing loss: 8142.06, base loss: 16886.22
[INFO 2017-06-29 06:01:44,143 main.py:105] improve_loss: 8744.16, improve_percent: 0.52
[INFO 2017-06-29 06:01:44,144 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 06:01:47,190 main.py:57] epoch 6800, training loss: 7309.35, average training loss: 7431.90, base loss: 16094.23
[INFO 2017-06-29 06:01:50,218 main.py:57] epoch 6801, training loss: 7357.73, average training loss: 7432.31, base loss: 16094.58
[INFO 2017-06-29 06:01:53,312 main.py:57] epoch 6802, training loss: 7940.81, average training loss: 7433.45, base loss: 16094.47
[INFO 2017-06-29 06:01:56,390 main.py:57] epoch 6803, training loss: 7361.67, average training loss: 7433.77, base loss: 16093.97
[INFO 2017-06-29 06:01:59,401 main.py:57] epoch 6804, training loss: 7449.71, average training loss: 7431.65, base loss: 16093.90
[INFO 2017-06-29 06:02:02,428 main.py:57] epoch 6805, training loss: 7242.81, average training loss: 7429.41, base loss: 16093.27
[INFO 2017-06-29 06:02:05,490 main.py:57] epoch 6806, training loss: 7457.08, average training loss: 7429.67, base loss: 16093.75
[INFO 2017-06-29 06:02:08,551 main.py:57] epoch 6807, training loss: 6773.86, average training loss: 7429.02, base loss: 16093.64
[INFO 2017-06-29 06:02:11,611 main.py:57] epoch 6808, training loss: 8112.65, average training loss: 7429.20, base loss: 16094.27
[INFO 2017-06-29 06:02:14,623 main.py:57] epoch 6809, training loss: 7113.46, average training loss: 7428.39, base loss: 16094.53
[INFO 2017-06-29 06:02:17,632 main.py:57] epoch 6810, training loss: 7133.63, average training loss: 7428.04, base loss: 16094.53
[INFO 2017-06-29 06:02:20,720 main.py:57] epoch 6811, training loss: 6994.39, average training loss: 7427.22, base loss: 16094.32
[INFO 2017-06-29 06:02:23,802 main.py:57] epoch 6812, training loss: 7399.57, average training loss: 7427.48, base loss: 16094.57
[INFO 2017-06-29 06:02:26,806 main.py:57] epoch 6813, training loss: 7741.51, average training loss: 7429.02, base loss: 16094.64
[INFO 2017-06-29 06:02:29,796 main.py:57] epoch 6814, training loss: 8216.42, average training loss: 7429.78, base loss: 16095.42
[INFO 2017-06-29 06:02:32,857 main.py:57] epoch 6815, training loss: 5810.39, average training loss: 7428.24, base loss: 16094.80
[INFO 2017-06-29 06:02:36,023 main.py:57] epoch 6816, training loss: 7221.12, average training loss: 7427.54, base loss: 16095.15
[INFO 2017-06-29 06:02:39,086 main.py:57] epoch 6817, training loss: 7095.63, average training loss: 7425.80, base loss: 16095.27
[INFO 2017-06-29 06:02:42,159 main.py:57] epoch 6818, training loss: 7443.19, average training loss: 7425.60, base loss: 16095.32
[INFO 2017-06-29 06:02:45,241 main.py:57] epoch 6819, training loss: 7647.54, average training loss: 7426.53, base loss: 16095.13
[INFO 2017-06-29 06:02:48,345 main.py:57] epoch 6820, training loss: 6336.76, average training loss: 7425.92, base loss: 16094.99
[INFO 2017-06-29 06:02:51,405 main.py:57] epoch 6821, training loss: 6910.95, average training loss: 7425.16, base loss: 16094.84
[INFO 2017-06-29 06:02:54,389 main.py:57] epoch 6822, training loss: 6406.81, average training loss: 7424.30, base loss: 16094.42
[INFO 2017-06-29 06:02:57,428 main.py:57] epoch 6823, training loss: 7389.88, average training loss: 7423.70, base loss: 16094.44
[INFO 2017-06-29 06:03:00,488 main.py:57] epoch 6824, training loss: 7370.24, average training loss: 7423.98, base loss: 16094.41
[INFO 2017-06-29 06:03:03,588 main.py:57] epoch 6825, training loss: 6910.61, average training loss: 7423.66, base loss: 16093.71
[INFO 2017-06-29 06:03:06,703 main.py:57] epoch 6826, training loss: 6660.23, average training loss: 7422.85, base loss: 16093.27
[INFO 2017-06-29 06:03:09,782 main.py:57] epoch 6827, training loss: 8030.07, average training loss: 7423.86, base loss: 16093.60
[INFO 2017-06-29 06:03:12,820 main.py:57] epoch 6828, training loss: 6049.58, average training loss: 7422.68, base loss: 16092.62
[INFO 2017-06-29 06:03:15,953 main.py:57] epoch 6829, training loss: 7025.72, average training loss: 7422.11, base loss: 16092.39
[INFO 2017-06-29 06:03:19,050 main.py:57] epoch 6830, training loss: 6829.78, average training loss: 7420.27, base loss: 16092.56
[INFO 2017-06-29 06:03:22,110 main.py:57] epoch 6831, training loss: 6875.70, average training loss: 7420.57, base loss: 16092.33
[INFO 2017-06-29 06:03:25,169 main.py:57] epoch 6832, training loss: 7087.14, average training loss: 7419.76, base loss: 16092.12
[INFO 2017-06-29 06:03:28,207 main.py:57] epoch 6833, training loss: 7125.23, average training loss: 7418.64, base loss: 16091.55
[INFO 2017-06-29 06:03:31,227 main.py:57] epoch 6834, training loss: 6815.62, average training loss: 7418.22, base loss: 16091.33
[INFO 2017-06-29 06:03:34,275 main.py:57] epoch 6835, training loss: 7389.31, average training loss: 7418.48, base loss: 16091.27
[INFO 2017-06-29 06:03:37,341 main.py:57] epoch 6836, training loss: 6949.63, average training loss: 7417.93, base loss: 16090.97
[INFO 2017-06-29 06:03:40,417 main.py:57] epoch 6837, training loss: 7833.74, average training loss: 7417.42, base loss: 16091.12
[INFO 2017-06-29 06:03:43,399 main.py:57] epoch 6838, training loss: 8154.35, average training loss: 7418.64, base loss: 16091.60
[INFO 2017-06-29 06:03:46,416 main.py:57] epoch 6839, training loss: 7552.17, average training loss: 7419.19, base loss: 16092.24
[INFO 2017-06-29 06:03:49,519 main.py:57] epoch 6840, training loss: 6835.55, average training loss: 7417.69, base loss: 16092.14
[INFO 2017-06-29 06:03:52,577 main.py:57] epoch 6841, training loss: 9251.84, average training loss: 7418.44, base loss: 16093.12
[INFO 2017-06-29 06:03:55,578 main.py:57] epoch 6842, training loss: 8138.31, average training loss: 7418.72, base loss: 16093.58
[INFO 2017-06-29 06:03:58,614 main.py:57] epoch 6843, training loss: 8629.22, average training loss: 7419.70, base loss: 16094.28
[INFO 2017-06-29 06:04:01,605 main.py:57] epoch 6844, training loss: 7419.16, average training loss: 7419.42, base loss: 16094.50
[INFO 2017-06-29 06:04:04,624 main.py:57] epoch 6845, training loss: 6893.30, average training loss: 7418.96, base loss: 16094.02
[INFO 2017-06-29 06:04:07,715 main.py:57] epoch 6846, training loss: 7593.31, average training loss: 7419.29, base loss: 16094.17
[INFO 2017-06-29 06:04:10,741 main.py:57] epoch 6847, training loss: 7859.10, average training loss: 7420.75, base loss: 16094.93
[INFO 2017-06-29 06:04:13,819 main.py:57] epoch 6848, training loss: 7269.74, average training loss: 7421.09, base loss: 16095.26
[INFO 2017-06-29 06:04:16,901 main.py:57] epoch 6849, training loss: 6085.91, average training loss: 7419.92, base loss: 16094.39
[INFO 2017-06-29 06:04:19,996 main.py:57] epoch 6850, training loss: 7558.48, average training loss: 7420.57, base loss: 16094.78
[INFO 2017-06-29 06:04:22,992 main.py:57] epoch 6851, training loss: 7994.30, average training loss: 7420.19, base loss: 16095.15
[INFO 2017-06-29 06:04:26,053 main.py:57] epoch 6852, training loss: 7136.41, average training loss: 7419.90, base loss: 16095.28
[INFO 2017-06-29 06:04:29,132 main.py:57] epoch 6853, training loss: 7291.52, average training loss: 7419.67, base loss: 16095.38
[INFO 2017-06-29 06:04:32,216 main.py:57] epoch 6854, training loss: 7254.37, average training loss: 7419.45, base loss: 16095.15
[INFO 2017-06-29 06:04:35,329 main.py:57] epoch 6855, training loss: 6657.26, average training loss: 7418.30, base loss: 16095.06
[INFO 2017-06-29 06:04:38,357 main.py:57] epoch 6856, training loss: 7124.42, average training loss: 7418.90, base loss: 16095.00
[INFO 2017-06-29 06:04:41,406 main.py:57] epoch 6857, training loss: 8197.06, average training loss: 7419.68, base loss: 16095.81
[INFO 2017-06-29 06:04:44,402 main.py:57] epoch 6858, training loss: 7931.49, average training loss: 7419.99, base loss: 16095.63
[INFO 2017-06-29 06:04:47,509 main.py:57] epoch 6859, training loss: 7370.91, average training loss: 7419.55, base loss: 16095.01
[INFO 2017-06-29 06:04:50,597 main.py:57] epoch 6860, training loss: 7762.92, average training loss: 7420.50, base loss: 16095.44
[INFO 2017-06-29 06:04:53,621 main.py:57] epoch 6861, training loss: 6983.07, average training loss: 7420.40, base loss: 16095.24
[INFO 2017-06-29 06:04:56,721 main.py:57] epoch 6862, training loss: 6291.20, average training loss: 7419.59, base loss: 16094.85
[INFO 2017-06-29 06:04:59,713 main.py:57] epoch 6863, training loss: 8187.90, average training loss: 7420.25, base loss: 16095.60
[INFO 2017-06-29 06:05:02,741 main.py:57] epoch 6864, training loss: 6813.15, average training loss: 7418.74, base loss: 16095.35
[INFO 2017-06-29 06:05:05,757 main.py:57] epoch 6865, training loss: 7223.72, average training loss: 7419.15, base loss: 16094.83
[INFO 2017-06-29 06:05:08,773 main.py:57] epoch 6866, training loss: 7645.33, average training loss: 7420.12, base loss: 16095.21
[INFO 2017-06-29 06:05:11,832 main.py:57] epoch 6867, training loss: 7582.65, average training loss: 7420.56, base loss: 16095.75
[INFO 2017-06-29 06:05:14,813 main.py:57] epoch 6868, training loss: 7069.99, average training loss: 7420.20, base loss: 16095.28
[INFO 2017-06-29 06:05:17,814 main.py:57] epoch 6869, training loss: 7173.32, average training loss: 7419.80, base loss: 16095.01
[INFO 2017-06-29 06:05:20,893 main.py:57] epoch 6870, training loss: 8175.50, average training loss: 7421.07, base loss: 16095.04
[INFO 2017-06-29 06:05:23,955 main.py:57] epoch 6871, training loss: 8075.72, average training loss: 7422.73, base loss: 16095.24
[INFO 2017-06-29 06:05:27,035 main.py:57] epoch 6872, training loss: 7126.10, average training loss: 7423.19, base loss: 16094.97
[INFO 2017-06-29 06:05:30,151 main.py:57] epoch 6873, training loss: 6843.64, average training loss: 7423.19, base loss: 16094.99
[INFO 2017-06-29 06:05:33,186 main.py:57] epoch 6874, training loss: 7687.90, average training loss: 7423.94, base loss: 16095.08
[INFO 2017-06-29 06:05:36,192 main.py:57] epoch 6875, training loss: 7840.85, average training loss: 7423.36, base loss: 16094.83
[INFO 2017-06-29 06:05:39,257 main.py:57] epoch 6876, training loss: 7223.02, average training loss: 7422.46, base loss: 16094.45
[INFO 2017-06-29 06:05:42,264 main.py:57] epoch 6877, training loss: 8301.09, average training loss: 7422.27, base loss: 16094.97
[INFO 2017-06-29 06:05:45,250 main.py:57] epoch 6878, training loss: 7458.96, average training loss: 7422.46, base loss: 16095.27
[INFO 2017-06-29 06:05:48,311 main.py:57] epoch 6879, training loss: 7202.16, average training loss: 7422.20, base loss: 16095.09
[INFO 2017-06-29 06:05:51,344 main.py:57] epoch 6880, training loss: 6747.03, average training loss: 7420.81, base loss: 16094.42
[INFO 2017-06-29 06:05:54,495 main.py:57] epoch 6881, training loss: 8314.16, average training loss: 7422.79, base loss: 16094.84
[INFO 2017-06-29 06:05:57,611 main.py:57] epoch 6882, training loss: 7112.72, average training loss: 7422.47, base loss: 16094.55
[INFO 2017-06-29 06:06:00,681 main.py:57] epoch 6883, training loss: 7629.24, average training loss: 7421.70, base loss: 16094.79
[INFO 2017-06-29 06:06:03,736 main.py:57] epoch 6884, training loss: 7129.96, average training loss: 7420.92, base loss: 16094.44
[INFO 2017-06-29 06:06:06,851 main.py:57] epoch 6885, training loss: 8274.33, average training loss: 7421.46, base loss: 16094.33
[INFO 2017-06-29 06:06:09,947 main.py:57] epoch 6886, training loss: 7101.97, average training loss: 7420.61, base loss: 16093.97
[INFO 2017-06-29 06:06:12,975 main.py:57] epoch 6887, training loss: 6991.13, average training loss: 7418.78, base loss: 16093.51
[INFO 2017-06-29 06:06:16,046 main.py:57] epoch 6888, training loss: 7496.20, average training loss: 7419.15, base loss: 16093.47
[INFO 2017-06-29 06:06:19,135 main.py:57] epoch 6889, training loss: 7610.25, average training loss: 7419.58, base loss: 16093.46
[INFO 2017-06-29 06:06:22,162 main.py:57] epoch 6890, training loss: 6656.30, average training loss: 7419.51, base loss: 16092.89
[INFO 2017-06-29 06:06:25,228 main.py:57] epoch 6891, training loss: 6263.40, average training loss: 7418.92, base loss: 16092.19
[INFO 2017-06-29 06:06:28,277 main.py:57] epoch 6892, training loss: 7603.88, average training loss: 7419.04, base loss: 16092.24
[INFO 2017-06-29 06:06:31,305 main.py:57] epoch 6893, training loss: 6891.52, average training loss: 7418.00, base loss: 16092.26
[INFO 2017-06-29 06:06:34,342 main.py:57] epoch 6894, training loss: 7606.36, average training loss: 7418.32, base loss: 16091.99
[INFO 2017-06-29 06:06:37,387 main.py:57] epoch 6895, training loss: 6611.92, average training loss: 7417.16, base loss: 16090.97
[INFO 2017-06-29 06:06:40,365 main.py:57] epoch 6896, training loss: 6616.63, average training loss: 7416.42, base loss: 16090.75
[INFO 2017-06-29 06:06:43,386 main.py:57] epoch 6897, training loss: 7946.11, average training loss: 7417.21, base loss: 16091.14
[INFO 2017-06-29 06:06:46,454 main.py:57] epoch 6898, training loss: 7473.13, average training loss: 7417.28, base loss: 16091.00
[INFO 2017-06-29 06:06:49,551 main.py:57] epoch 6899, training loss: 7031.07, average training loss: 7417.68, base loss: 16090.93
[INFO 2017-06-29 06:06:49,552 main.py:59] epoch 6899, testing
[INFO 2017-06-29 06:07:02,268 main.py:104] average testing loss: 8459.56, base loss: 17489.18
[INFO 2017-06-29 06:07:02,269 main.py:105] improve_loss: 9029.63, improve_percent: 0.52
[INFO 2017-06-29 06:07:02,270 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 06:07:05,414 main.py:57] epoch 6900, training loss: 7027.46, average training loss: 7417.62, base loss: 16091.03
[INFO 2017-06-29 06:07:08,464 main.py:57] epoch 6901, training loss: 7495.07, average training loss: 7418.08, base loss: 16091.58
[INFO 2017-06-29 06:07:11,564 main.py:57] epoch 6902, training loss: 7287.85, average training loss: 7417.97, base loss: 16091.66
[INFO 2017-06-29 06:07:14,746 main.py:57] epoch 6903, training loss: 7583.41, average training loss: 7417.75, base loss: 16091.87
[INFO 2017-06-29 06:07:17,954 main.py:57] epoch 6904, training loss: 8288.89, average training loss: 7419.42, base loss: 16092.14
[INFO 2017-06-29 06:07:21,041 main.py:57] epoch 6905, training loss: 7402.61, average training loss: 7418.51, base loss: 16092.00
[INFO 2017-06-29 06:07:24,080 main.py:57] epoch 6906, training loss: 6911.96, average training loss: 7417.44, base loss: 16091.26
[INFO 2017-06-29 06:07:27,115 main.py:57] epoch 6907, training loss: 8603.06, average training loss: 7418.07, base loss: 16091.60
[INFO 2017-06-29 06:07:30,308 main.py:57] epoch 6908, training loss: 7690.84, average training loss: 7418.29, base loss: 16091.66
[INFO 2017-06-29 06:07:33,375 main.py:57] epoch 6909, training loss: 6826.84, average training loss: 7417.45, base loss: 16091.27
[INFO 2017-06-29 06:07:36,402 main.py:57] epoch 6910, training loss: 6850.00, average training loss: 7415.98, base loss: 16091.17
[INFO 2017-06-29 06:07:39,552 main.py:57] epoch 6911, training loss: 6980.04, average training loss: 7415.61, base loss: 16090.99
[INFO 2017-06-29 06:07:42,636 main.py:57] epoch 6912, training loss: 6809.32, average training loss: 7414.43, base loss: 16090.66
[INFO 2017-06-29 06:07:45,717 main.py:57] epoch 6913, training loss: 7938.17, average training loss: 7414.91, base loss: 16091.31
[INFO 2017-06-29 06:07:48,800 main.py:57] epoch 6914, training loss: 7370.21, average training loss: 7415.54, base loss: 16091.03
[INFO 2017-06-29 06:07:51,852 main.py:57] epoch 6915, training loss: 6976.81, average training loss: 7415.31, base loss: 16090.40
[INFO 2017-06-29 06:07:54,877 main.py:57] epoch 6916, training loss: 7612.22, average training loss: 7416.43, base loss: 16090.39
[INFO 2017-06-29 06:07:57,928 main.py:57] epoch 6917, training loss: 7463.09, average training loss: 7416.67, base loss: 16090.13
[INFO 2017-06-29 06:08:00,964 main.py:57] epoch 6918, training loss: 7708.75, average training loss: 7416.72, base loss: 16090.00
[INFO 2017-06-29 06:08:04,058 main.py:57] epoch 6919, training loss: 7972.44, average training loss: 7417.15, base loss: 16090.33
[INFO 2017-06-29 06:08:07,120 main.py:57] epoch 6920, training loss: 7149.77, average training loss: 7416.39, base loss: 16090.17
[INFO 2017-06-29 06:08:10,225 main.py:57] epoch 6921, training loss: 8285.65, average training loss: 7417.84, base loss: 16090.80
[INFO 2017-06-29 06:08:13,265 main.py:57] epoch 6922, training loss: 6188.25, average training loss: 7416.81, base loss: 16090.35
[INFO 2017-06-29 06:08:16,286 main.py:57] epoch 6923, training loss: 7645.79, average training loss: 7417.14, base loss: 16090.86
[INFO 2017-06-29 06:08:19,280 main.py:57] epoch 6924, training loss: 7055.49, average training loss: 7417.04, base loss: 16090.53
[INFO 2017-06-29 06:08:22,395 main.py:57] epoch 6925, training loss: 7488.97, average training loss: 7417.54, base loss: 16090.50
[INFO 2017-06-29 06:08:25,480 main.py:57] epoch 6926, training loss: 7859.77, average training loss: 7418.07, base loss: 16090.27
[INFO 2017-06-29 06:08:28,560 main.py:57] epoch 6927, training loss: 7818.41, average training loss: 7418.66, base loss: 16090.01
[INFO 2017-06-29 06:08:31,632 main.py:57] epoch 6928, training loss: 8474.61, average training loss: 7419.59, base loss: 16090.89
[INFO 2017-06-29 06:08:34,687 main.py:57] epoch 6929, training loss: 7217.25, average training loss: 7419.97, base loss: 16090.88
[INFO 2017-06-29 06:08:37,758 main.py:57] epoch 6930, training loss: 7791.12, average training loss: 7420.82, base loss: 16090.92
[INFO 2017-06-29 06:08:40,812 main.py:57] epoch 6931, training loss: 7055.42, average training loss: 7420.51, base loss: 16090.87
[INFO 2017-06-29 06:08:43,841 main.py:57] epoch 6932, training loss: 8191.66, average training loss: 7420.52, base loss: 16091.19
[INFO 2017-06-29 06:08:46,841 main.py:57] epoch 6933, training loss: 8565.75, average training loss: 7422.19, base loss: 16091.41
[INFO 2017-06-29 06:08:49,916 main.py:57] epoch 6934, training loss: 7208.67, average training loss: 7423.29, base loss: 16091.37
[INFO 2017-06-29 06:08:52,998 main.py:57] epoch 6935, training loss: 7452.36, average training loss: 7422.81, base loss: 16091.66
[INFO 2017-06-29 06:08:56,096 main.py:57] epoch 6936, training loss: 8669.25, average training loss: 7423.52, base loss: 16092.20
[INFO 2017-06-29 06:08:59,144 main.py:57] epoch 6937, training loss: 8161.92, average training loss: 7423.77, base loss: 16092.15
[INFO 2017-06-29 06:09:02,263 main.py:57] epoch 6938, training loss: 6129.70, average training loss: 7423.26, base loss: 16091.46
[INFO 2017-06-29 06:09:05,305 main.py:57] epoch 6939, training loss: 8091.90, average training loss: 7424.12, base loss: 16091.03
[INFO 2017-06-29 06:09:08,369 main.py:57] epoch 6940, training loss: 7487.56, average training loss: 7424.10, base loss: 16090.99
[INFO 2017-06-29 06:09:11,465 main.py:57] epoch 6941, training loss: 6703.99, average training loss: 7423.11, base loss: 16091.01
[INFO 2017-06-29 06:09:14,573 main.py:57] epoch 6942, training loss: 7372.21, average training loss: 7421.96, base loss: 16091.14
[INFO 2017-06-29 06:09:17,693 main.py:57] epoch 6943, training loss: 7675.93, average training loss: 7422.01, base loss: 16091.45
[INFO 2017-06-29 06:09:20,762 main.py:57] epoch 6944, training loss: 6954.10, average training loss: 7420.82, base loss: 16091.19
[INFO 2017-06-29 06:09:23,831 main.py:57] epoch 6945, training loss: 7575.96, average training loss: 7420.68, base loss: 16091.06
[INFO 2017-06-29 06:09:26,875 main.py:57] epoch 6946, training loss: 7039.26, average training loss: 7420.89, base loss: 16090.51
[INFO 2017-06-29 06:09:30,024 main.py:57] epoch 6947, training loss: 7545.90, average training loss: 7420.38, base loss: 16090.67
[INFO 2017-06-29 06:09:33,044 main.py:57] epoch 6948, training loss: 7545.53, average training loss: 7420.30, base loss: 16090.61
[INFO 2017-06-29 06:09:36,069 main.py:57] epoch 6949, training loss: 7085.12, average training loss: 7420.13, base loss: 16090.47
[INFO 2017-06-29 06:09:39,119 main.py:57] epoch 6950, training loss: 8300.87, average training loss: 7421.02, base loss: 16090.86
[INFO 2017-06-29 06:09:42,121 main.py:57] epoch 6951, training loss: 8359.38, average training loss: 7421.81, base loss: 16091.55
[INFO 2017-06-29 06:09:45,202 main.py:57] epoch 6952, training loss: 7247.91, average training loss: 7420.82, base loss: 16091.20
[INFO 2017-06-29 06:09:48,216 main.py:57] epoch 6953, training loss: 7712.72, average training loss: 7420.86, base loss: 16091.32
[INFO 2017-06-29 06:09:51,269 main.py:57] epoch 6954, training loss: 7651.38, average training loss: 7420.53, base loss: 16091.82
[INFO 2017-06-29 06:09:54,324 main.py:57] epoch 6955, training loss: 7288.13, average training loss: 7419.80, base loss: 16091.49
[INFO 2017-06-29 06:09:57,398 main.py:57] epoch 6956, training loss: 7050.55, average training loss: 7418.98, base loss: 16091.12
[INFO 2017-06-29 06:10:00,534 main.py:57] epoch 6957, training loss: 6417.98, average training loss: 7417.87, base loss: 16090.59
[INFO 2017-06-29 06:10:03,577 main.py:57] epoch 6958, training loss: 6929.83, average training loss: 7415.53, base loss: 16090.38
[INFO 2017-06-29 06:10:06,636 main.py:57] epoch 6959, training loss: 7536.30, average training loss: 7415.76, base loss: 16090.66
[INFO 2017-06-29 06:10:09,665 main.py:57] epoch 6960, training loss: 8153.09, average training loss: 7417.05, base loss: 16091.09
[INFO 2017-06-29 06:10:12,729 main.py:57] epoch 6961, training loss: 6112.46, average training loss: 7416.35, base loss: 16090.31
[INFO 2017-06-29 06:10:15,891 main.py:57] epoch 6962, training loss: 7247.80, average training loss: 7415.85, base loss: 16090.32
[INFO 2017-06-29 06:10:18,936 main.py:57] epoch 6963, training loss: 6109.63, average training loss: 7414.97, base loss: 16089.54
[INFO 2017-06-29 06:10:22,042 main.py:57] epoch 6964, training loss: 6685.66, average training loss: 7415.20, base loss: 16088.84
[INFO 2017-06-29 06:10:25,053 main.py:57] epoch 6965, training loss: 9066.46, average training loss: 7416.97, base loss: 16089.01
[INFO 2017-06-29 06:10:28,083 main.py:57] epoch 6966, training loss: 7542.16, average training loss: 7417.36, base loss: 16089.76
[INFO 2017-06-29 06:10:31,224 main.py:57] epoch 6967, training loss: 8140.14, average training loss: 7416.71, base loss: 16090.00
[INFO 2017-06-29 06:10:34,290 main.py:57] epoch 6968, training loss: 7393.05, average training loss: 7416.08, base loss: 16090.20
[INFO 2017-06-29 06:10:37,317 main.py:57] epoch 6969, training loss: 7109.66, average training loss: 7415.62, base loss: 16090.38
[INFO 2017-06-29 06:10:40,379 main.py:57] epoch 6970, training loss: 7082.30, average training loss: 7416.02, base loss: 16090.05
[INFO 2017-06-29 06:10:43,453 main.py:57] epoch 6971, training loss: 7822.37, average training loss: 7415.42, base loss: 16089.77
[INFO 2017-06-29 06:10:46,457 main.py:57] epoch 6972, training loss: 7105.71, average training loss: 7414.26, base loss: 16090.12
[INFO 2017-06-29 06:10:49,455 main.py:57] epoch 6973, training loss: 6931.30, average training loss: 7413.94, base loss: 16089.96
[INFO 2017-06-29 06:10:52,455 main.py:57] epoch 6974, training loss: 8939.48, average training loss: 7415.90, base loss: 16090.05
[INFO 2017-06-29 06:10:55,539 main.py:57] epoch 6975, training loss: 8588.77, average training loss: 7417.44, base loss: 16090.80
[INFO 2017-06-29 06:10:58,592 main.py:57] epoch 6976, training loss: 7836.73, average training loss: 7418.41, base loss: 16090.78
[INFO 2017-06-29 06:11:01,768 main.py:57] epoch 6977, training loss: 7197.84, average training loss: 7417.36, base loss: 16090.87
[INFO 2017-06-29 06:11:04,941 main.py:57] epoch 6978, training loss: 9988.33, average training loss: 7420.03, base loss: 16091.89
[INFO 2017-06-29 06:11:07,927 main.py:57] epoch 6979, training loss: 7409.91, average training loss: 7420.28, base loss: 16091.73
[INFO 2017-06-29 06:11:11,080 main.py:57] epoch 6980, training loss: 7339.51, average training loss: 7420.79, base loss: 16091.61
[INFO 2017-06-29 06:11:14,129 main.py:57] epoch 6981, training loss: 7773.82, average training loss: 7421.96, base loss: 16091.84
[INFO 2017-06-29 06:11:17,165 main.py:57] epoch 6982, training loss: 6722.80, average training loss: 7420.78, base loss: 16091.58
[INFO 2017-06-29 06:11:20,192 main.py:57] epoch 6983, training loss: 8391.07, average training loss: 7422.05, base loss: 16091.95
[INFO 2017-06-29 06:11:23,263 main.py:57] epoch 6984, training loss: 7109.68, average training loss: 7421.80, base loss: 16091.37
[INFO 2017-06-29 06:11:26,343 main.py:57] epoch 6985, training loss: 7316.22, average training loss: 7419.97, base loss: 16091.23
[INFO 2017-06-29 06:11:29,370 main.py:57] epoch 6986, training loss: 7274.24, average training loss: 7419.96, base loss: 16090.76
[INFO 2017-06-29 06:11:32,474 main.py:57] epoch 6987, training loss: 7274.74, average training loss: 7420.18, base loss: 16090.75
[INFO 2017-06-29 06:11:35,488 main.py:57] epoch 6988, training loss: 7373.36, average training loss: 7420.17, base loss: 16090.45
[INFO 2017-06-29 06:11:38,603 main.py:57] epoch 6989, training loss: 7722.58, average training loss: 7419.33, base loss: 16090.34
[INFO 2017-06-29 06:11:41,620 main.py:57] epoch 6990, training loss: 7250.10, average training loss: 7419.84, base loss: 16089.84
[INFO 2017-06-29 06:11:44,742 main.py:57] epoch 6991, training loss: 8130.49, average training loss: 7421.12, base loss: 16089.87
[INFO 2017-06-29 06:11:47,778 main.py:57] epoch 6992, training loss: 7208.37, average training loss: 7421.28, base loss: 16089.91
[INFO 2017-06-29 06:11:50,906 main.py:57] epoch 6993, training loss: 6800.11, average training loss: 7419.17, base loss: 16089.73
[INFO 2017-06-29 06:11:53,892 main.py:57] epoch 6994, training loss: 8122.97, average training loss: 7420.88, base loss: 16090.68
[INFO 2017-06-29 06:11:56,947 main.py:57] epoch 6995, training loss: 6580.39, average training loss: 7419.75, base loss: 16090.31
[INFO 2017-06-29 06:12:00,062 main.py:57] epoch 6996, training loss: 6116.80, average training loss: 7418.23, base loss: 16089.76
[INFO 2017-06-29 06:12:03,138 main.py:57] epoch 6997, training loss: 7149.93, average training loss: 7418.50, base loss: 16089.43
[INFO 2017-06-29 06:12:06,262 main.py:57] epoch 6998, training loss: 8134.64, average training loss: 7418.06, base loss: 16089.78
[INFO 2017-06-29 06:12:09,280 main.py:57] epoch 6999, training loss: 7112.25, average training loss: 7418.33, base loss: 16089.80
[INFO 2017-06-29 06:12:09,281 main.py:59] epoch 6999, testing
[INFO 2017-06-29 06:12:21,925 main.py:104] average testing loss: 8239.17, base loss: 17196.43
[INFO 2017-06-29 06:12:21,926 main.py:105] improve_loss: 8957.26, improve_percent: 0.52
[INFO 2017-06-29 06:12:21,927 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 06:12:24,969 main.py:57] epoch 7000, training loss: 7368.04, average training loss: 7419.02, base loss: 16089.19
[INFO 2017-06-29 06:12:27,982 main.py:57] epoch 7001, training loss: 8885.12, average training loss: 7421.42, base loss: 16090.00
[INFO 2017-06-29 06:12:31,014 main.py:57] epoch 7002, training loss: 7890.47, average training loss: 7421.52, base loss: 16090.42
[INFO 2017-06-29 06:12:34,066 main.py:57] epoch 7003, training loss: 6926.88, average training loss: 7421.40, base loss: 16090.38
[INFO 2017-06-29 06:12:37,098 main.py:57] epoch 7004, training loss: 7388.53, average training loss: 7421.26, base loss: 16090.46
[INFO 2017-06-29 06:12:40,152 main.py:57] epoch 7005, training loss: 7292.27, average training loss: 7420.65, base loss: 16090.49
[INFO 2017-06-29 06:12:43,185 main.py:57] epoch 7006, training loss: 6968.49, average training loss: 7421.54, base loss: 16090.29
[INFO 2017-06-29 06:12:46,323 main.py:57] epoch 7007, training loss: 7625.57, average training loss: 7422.28, base loss: 16090.42
[INFO 2017-06-29 06:12:49,402 main.py:57] epoch 7008, training loss: 8459.56, average training loss: 7424.16, base loss: 16091.09
[INFO 2017-06-29 06:12:52,518 main.py:57] epoch 7009, training loss: 6758.79, average training loss: 7422.67, base loss: 16090.73
[INFO 2017-06-29 06:12:55,612 main.py:57] epoch 7010, training loss: 7886.36, average training loss: 7423.55, base loss: 16090.99
[INFO 2017-06-29 06:12:58,623 main.py:57] epoch 7011, training loss: 7189.22, average training loss: 7423.88, base loss: 16091.47
[INFO 2017-06-29 06:13:01,766 main.py:57] epoch 7012, training loss: 7545.93, average training loss: 7423.96, base loss: 16091.51
[INFO 2017-06-29 06:13:04,801 main.py:57] epoch 7013, training loss: 6727.80, average training loss: 7423.71, base loss: 16091.03
[INFO 2017-06-29 06:13:07,842 main.py:57] epoch 7014, training loss: 8409.96, average training loss: 7423.98, base loss: 16091.64
[INFO 2017-06-29 06:13:10,827 main.py:57] epoch 7015, training loss: 6574.95, average training loss: 7423.49, base loss: 16090.99
[INFO 2017-06-29 06:13:13,852 main.py:57] epoch 7016, training loss: 7153.53, average training loss: 7423.02, base loss: 16091.07
[INFO 2017-06-29 06:13:16,874 main.py:57] epoch 7017, training loss: 8014.21, average training loss: 7422.41, base loss: 16091.66
[INFO 2017-06-29 06:13:19,916 main.py:57] epoch 7018, training loss: 7045.24, average training loss: 7421.48, base loss: 16091.99
[INFO 2017-06-29 06:13:22,971 main.py:57] epoch 7019, training loss: 6885.41, average training loss: 7420.53, base loss: 16092.08
[INFO 2017-06-29 06:13:26,010 main.py:57] epoch 7020, training loss: 8161.93, average training loss: 7421.46, base loss: 16092.38
[INFO 2017-06-29 06:13:29,087 main.py:57] epoch 7021, training loss: 8054.73, average training loss: 7421.74, base loss: 16092.87
[INFO 2017-06-29 06:13:32,179 main.py:57] epoch 7022, training loss: 7354.62, average training loss: 7421.89, base loss: 16092.61
[INFO 2017-06-29 06:13:35,324 main.py:57] epoch 7023, training loss: 7102.99, average training loss: 7421.80, base loss: 16092.01
[INFO 2017-06-29 06:13:38,346 main.py:57] epoch 7024, training loss: 6996.99, average training loss: 7421.13, base loss: 16092.05
[INFO 2017-06-29 06:13:41,414 main.py:57] epoch 7025, training loss: 8234.75, average training loss: 7422.01, base loss: 16092.53
[INFO 2017-06-29 06:13:44,448 main.py:57] epoch 7026, training loss: 7705.83, average training loss: 7421.78, base loss: 16092.97
[INFO 2017-06-29 06:13:47,447 main.py:57] epoch 7027, training loss: 8737.67, average training loss: 7424.07, base loss: 16093.32
[INFO 2017-06-29 06:13:50,447 main.py:57] epoch 7028, training loss: 6753.14, average training loss: 7424.01, base loss: 16093.15
[INFO 2017-06-29 06:13:53,518 main.py:57] epoch 7029, training loss: 6348.36, average training loss: 7423.77, base loss: 16092.63
[INFO 2017-06-29 06:13:56,571 main.py:57] epoch 7030, training loss: 8256.11, average training loss: 7424.96, base loss: 16093.22
[INFO 2017-06-29 06:13:59,589 main.py:57] epoch 7031, training loss: 8193.94, average training loss: 7425.34, base loss: 16093.67
[INFO 2017-06-29 06:14:02,663 main.py:57] epoch 7032, training loss: 7466.17, average training loss: 7425.34, base loss: 16093.88
[INFO 2017-06-29 06:14:05,750 main.py:57] epoch 7033, training loss: 7525.48, average training loss: 7425.73, base loss: 16094.17
[INFO 2017-06-29 06:14:08,814 main.py:57] epoch 7034, training loss: 7505.29, average training loss: 7426.43, base loss: 16094.49
[INFO 2017-06-29 06:14:11,885 main.py:57] epoch 7035, training loss: 7788.05, average training loss: 7425.68, base loss: 16094.70
[INFO 2017-06-29 06:14:14,915 main.py:57] epoch 7036, training loss: 6806.27, average training loss: 7425.37, base loss: 16094.11
[INFO 2017-06-29 06:14:18,001 main.py:57] epoch 7037, training loss: 6943.49, average training loss: 7426.25, base loss: 16093.75
[INFO 2017-06-29 06:14:21,092 main.py:57] epoch 7038, training loss: 6990.96, average training loss: 7424.89, base loss: 16093.86
[INFO 2017-06-29 06:14:24,109 main.py:57] epoch 7039, training loss: 7434.48, average training loss: 7424.13, base loss: 16093.98
[INFO 2017-06-29 06:14:27,143 main.py:57] epoch 7040, training loss: 7720.24, average training loss: 7424.23, base loss: 16094.41
[INFO 2017-06-29 06:14:30,204 main.py:57] epoch 7041, training loss: 7968.70, average training loss: 7424.01, base loss: 16095.13
[INFO 2017-06-29 06:14:33,226 main.py:57] epoch 7042, training loss: 7531.62, average training loss: 7423.17, base loss: 16095.77
[INFO 2017-06-29 06:14:36,258 main.py:57] epoch 7043, training loss: 6864.00, average training loss: 7422.79, base loss: 16095.45
[INFO 2017-06-29 06:14:39,306 main.py:57] epoch 7044, training loss: 6761.77, average training loss: 7421.64, base loss: 16095.12
[INFO 2017-06-29 06:14:42,348 main.py:57] epoch 7045, training loss: 8442.22, average training loss: 7421.61, base loss: 16095.28
[INFO 2017-06-29 06:14:45,354 main.py:57] epoch 7046, training loss: 7469.36, average training loss: 7421.05, base loss: 16095.12
[INFO 2017-06-29 06:14:48,378 main.py:57] epoch 7047, training loss: 7513.35, average training loss: 7420.67, base loss: 16095.06
[INFO 2017-06-29 06:14:51,426 main.py:57] epoch 7048, training loss: 7788.11, average training loss: 7421.38, base loss: 16095.23
[INFO 2017-06-29 06:14:54,516 main.py:57] epoch 7049, training loss: 7304.09, average training loss: 7421.38, base loss: 16095.14
[INFO 2017-06-29 06:14:57,586 main.py:57] epoch 7050, training loss: 7301.91, average training loss: 7421.11, base loss: 16094.92
[INFO 2017-06-29 06:15:00,619 main.py:57] epoch 7051, training loss: 6727.29, average training loss: 7419.79, base loss: 16094.07
[INFO 2017-06-29 06:15:03,672 main.py:57] epoch 7052, training loss: 8199.32, average training loss: 7421.17, base loss: 16093.99
[INFO 2017-06-29 06:15:06,720 main.py:57] epoch 7053, training loss: 7149.17, average training loss: 7420.77, base loss: 16093.11
[INFO 2017-06-29 06:15:09,751 main.py:57] epoch 7054, training loss: 8247.61, average training loss: 7421.74, base loss: 16093.41
[INFO 2017-06-29 06:15:12,761 main.py:57] epoch 7055, training loss: 7021.64, average training loss: 7421.85, base loss: 16093.15
[INFO 2017-06-29 06:15:15,831 main.py:57] epoch 7056, training loss: 7373.69, average training loss: 7420.81, base loss: 16092.63
[INFO 2017-06-29 06:15:18,945 main.py:57] epoch 7057, training loss: 7571.73, average training loss: 7422.04, base loss: 16092.20
[INFO 2017-06-29 06:15:22,011 main.py:57] epoch 7058, training loss: 7313.51, average training loss: 7422.78, base loss: 16091.94
[INFO 2017-06-29 06:15:25,088 main.py:57] epoch 7059, training loss: 7289.72, average training loss: 7421.89, base loss: 16092.04
[INFO 2017-06-29 06:15:28,137 main.py:57] epoch 7060, training loss: 7772.79, average training loss: 7422.44, base loss: 16092.20
[INFO 2017-06-29 06:15:31,151 main.py:57] epoch 7061, training loss: 7359.72, average training loss: 7422.93, base loss: 16091.80
[INFO 2017-06-29 06:15:34,229 main.py:57] epoch 7062, training loss: 7361.83, average training loss: 7423.71, base loss: 16092.38
[INFO 2017-06-29 06:15:37,266 main.py:57] epoch 7063, training loss: 7505.12, average training loss: 7423.76, base loss: 16092.56
[INFO 2017-06-29 06:15:40,310 main.py:57] epoch 7064, training loss: 6769.69, average training loss: 7423.84, base loss: 16092.48
[INFO 2017-06-29 06:15:43,390 main.py:57] epoch 7065, training loss: 6284.60, average training loss: 7422.14, base loss: 16091.96
[INFO 2017-06-29 06:15:46,536 main.py:57] epoch 7066, training loss: 7857.52, average training loss: 7422.31, base loss: 16092.44
[INFO 2017-06-29 06:15:49,594 main.py:57] epoch 7067, training loss: 8225.96, average training loss: 7423.36, base loss: 16093.16
[INFO 2017-06-29 06:15:52,685 main.py:57] epoch 7068, training loss: 8199.09, average training loss: 7424.01, base loss: 16093.79
[INFO 2017-06-29 06:15:55,791 main.py:57] epoch 7069, training loss: 7573.33, average training loss: 7423.35, base loss: 16093.67
[INFO 2017-06-29 06:15:58,784 main.py:57] epoch 7070, training loss: 7755.27, average training loss: 7423.16, base loss: 16093.91
[INFO 2017-06-29 06:16:01,834 main.py:57] epoch 7071, training loss: 8903.71, average training loss: 7424.81, base loss: 16094.78
[INFO 2017-06-29 06:16:04,864 main.py:57] epoch 7072, training loss: 6894.51, average training loss: 7424.78, base loss: 16095.05
[INFO 2017-06-29 06:16:07,898 main.py:57] epoch 7073, training loss: 8652.87, average training loss: 7426.28, base loss: 16096.06
[INFO 2017-06-29 06:16:10,870 main.py:57] epoch 7074, training loss: 8322.56, average training loss: 7427.29, base loss: 16096.42
[INFO 2017-06-29 06:16:13,892 main.py:57] epoch 7075, training loss: 7793.85, average training loss: 7427.73, base loss: 16096.22
[INFO 2017-06-29 06:16:16,937 main.py:57] epoch 7076, training loss: 7844.00, average training loss: 7427.70, base loss: 16095.88
[INFO 2017-06-29 06:16:19,973 main.py:57] epoch 7077, training loss: 7351.34, average training loss: 7427.17, base loss: 16095.83
[INFO 2017-06-29 06:16:23,059 main.py:57] epoch 7078, training loss: 6897.00, average training loss: 7426.74, base loss: 16095.39
[INFO 2017-06-29 06:16:26,067 main.py:57] epoch 7079, training loss: 6706.51, average training loss: 7426.80, base loss: 16095.36
[INFO 2017-06-29 06:16:29,140 main.py:57] epoch 7080, training loss: 7001.49, average training loss: 7427.11, base loss: 16095.23
[INFO 2017-06-29 06:16:32,211 main.py:57] epoch 7081, training loss: 7409.60, average training loss: 7427.10, base loss: 16094.94
[INFO 2017-06-29 06:16:35,296 main.py:57] epoch 7082, training loss: 7317.98, average training loss: 7426.48, base loss: 16094.67
[INFO 2017-06-29 06:16:38,346 main.py:57] epoch 7083, training loss: 8402.73, average training loss: 7426.18, base loss: 16094.89
[INFO 2017-06-29 06:16:41,375 main.py:57] epoch 7084, training loss: 7041.71, average training loss: 7425.15, base loss: 16094.96
[INFO 2017-06-29 06:16:44,408 main.py:57] epoch 7085, training loss: 7500.98, average training loss: 7424.21, base loss: 16095.24
[INFO 2017-06-29 06:16:47,509 main.py:57] epoch 7086, training loss: 7582.47, average training loss: 7423.90, base loss: 16095.20
[INFO 2017-06-29 06:16:50,626 main.py:57] epoch 7087, training loss: 6949.50, average training loss: 7423.30, base loss: 16094.87
[INFO 2017-06-29 06:16:53,735 main.py:57] epoch 7088, training loss: 7134.89, average training loss: 7422.44, base loss: 16094.34
[INFO 2017-06-29 06:16:56,812 main.py:57] epoch 7089, training loss: 8131.55, average training loss: 7422.19, base loss: 16094.74
[INFO 2017-06-29 06:16:59,865 main.py:57] epoch 7090, training loss: 7918.21, average training loss: 7422.60, base loss: 16095.31
[INFO 2017-06-29 06:17:02,855 main.py:57] epoch 7091, training loss: 7325.37, average training loss: 7422.84, base loss: 16095.58
[INFO 2017-06-29 06:17:05,941 main.py:57] epoch 7092, training loss: 7280.34, average training loss: 7421.45, base loss: 16095.61
[INFO 2017-06-29 06:17:08,956 main.py:57] epoch 7093, training loss: 7497.69, average training loss: 7421.21, base loss: 16096.04
[INFO 2017-06-29 06:17:12,031 main.py:57] epoch 7094, training loss: 7801.21, average training loss: 7420.79, base loss: 16096.39
[INFO 2017-06-29 06:17:15,087 main.py:57] epoch 7095, training loss: 7488.51, average training loss: 7421.17, base loss: 16096.36
[INFO 2017-06-29 06:17:18,146 main.py:57] epoch 7096, training loss: 6567.11, average training loss: 7420.62, base loss: 16095.50
[INFO 2017-06-29 06:17:21,167 main.py:57] epoch 7097, training loss: 7041.48, average training loss: 7421.13, base loss: 16095.83
[INFO 2017-06-29 06:17:24,284 main.py:57] epoch 7098, training loss: 6610.85, average training loss: 7419.57, base loss: 16095.29
[INFO 2017-06-29 06:17:27,361 main.py:57] epoch 7099, training loss: 7446.05, average training loss: 7419.22, base loss: 16095.57
[INFO 2017-06-29 06:17:27,361 main.py:59] epoch 7099, testing
[INFO 2017-06-29 06:17:39,983 main.py:104] average testing loss: 8322.31, base loss: 17233.71
[INFO 2017-06-29 06:17:39,983 main.py:105] improve_loss: 8911.40, improve_percent: 0.52
[INFO 2017-06-29 06:17:39,985 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 06:17:43,112 main.py:57] epoch 7100, training loss: 7727.35, average training loss: 7418.92, base loss: 16095.35
[INFO 2017-06-29 06:17:46,083 main.py:57] epoch 7101, training loss: 6804.64, average training loss: 7418.02, base loss: 16095.11
[INFO 2017-06-29 06:17:49,117 main.py:57] epoch 7102, training loss: 7180.84, average training loss: 7417.59, base loss: 16094.85
[INFO 2017-06-29 06:17:52,178 main.py:57] epoch 7103, training loss: 7079.87, average training loss: 7417.60, base loss: 16094.48
[INFO 2017-06-29 06:17:55,253 main.py:57] epoch 7104, training loss: 8235.14, average training loss: 7417.56, base loss: 16095.24
[INFO 2017-06-29 06:17:58,282 main.py:57] epoch 7105, training loss: 7404.67, average training loss: 7418.50, base loss: 16095.43
[INFO 2017-06-29 06:18:01,365 main.py:57] epoch 7106, training loss: 6198.08, average training loss: 7417.70, base loss: 16094.79
[INFO 2017-06-29 06:18:04,460 main.py:57] epoch 7107, training loss: 7047.09, average training loss: 7417.80, base loss: 16094.45
[INFO 2017-06-29 06:18:07,461 main.py:57] epoch 7108, training loss: 6812.44, average training loss: 7416.97, base loss: 16093.78
[INFO 2017-06-29 06:18:10,557 main.py:57] epoch 7109, training loss: 7445.34, average training loss: 7417.09, base loss: 16093.16
[INFO 2017-06-29 06:18:13,590 main.py:57] epoch 7110, training loss: 8498.79, average training loss: 7419.53, base loss: 16093.66
[INFO 2017-06-29 06:18:16,642 main.py:57] epoch 7111, training loss: 6161.75, average training loss: 7419.10, base loss: 16092.81
[INFO 2017-06-29 06:18:19,679 main.py:57] epoch 7112, training loss: 7341.24, average training loss: 7419.01, base loss: 16092.73
[INFO 2017-06-29 06:18:22,718 main.py:57] epoch 7113, training loss: 6966.32, average training loss: 7418.01, base loss: 16092.73
[INFO 2017-06-29 06:18:25,789 main.py:57] epoch 7114, training loss: 6421.44, average training loss: 7417.68, base loss: 16092.55
[INFO 2017-06-29 06:18:28,884 main.py:57] epoch 7115, training loss: 8860.48, average training loss: 7419.14, base loss: 16093.12
[INFO 2017-06-29 06:18:31,890 main.py:57] epoch 7116, training loss: 7792.05, average training loss: 7420.25, base loss: 16093.17
[INFO 2017-06-29 06:18:34,945 main.py:57] epoch 7117, training loss: 6895.14, average training loss: 7418.78, base loss: 16092.96
[INFO 2017-06-29 06:18:37,989 main.py:57] epoch 7118, training loss: 7424.50, average training loss: 7419.16, base loss: 16093.57
[INFO 2017-06-29 06:18:41,086 main.py:57] epoch 7119, training loss: 8197.51, average training loss: 7420.17, base loss: 16093.95
[INFO 2017-06-29 06:18:44,128 main.py:57] epoch 7120, training loss: 8012.64, average training loss: 7420.29, base loss: 16094.40
[INFO 2017-06-29 06:18:47,159 main.py:57] epoch 7121, training loss: 8030.91, average training loss: 7420.65, base loss: 16094.55
[INFO 2017-06-29 06:18:50,184 main.py:57] epoch 7122, training loss: 8208.84, average training loss: 7421.32, base loss: 16095.15
[INFO 2017-06-29 06:18:53,229 main.py:57] epoch 7123, training loss: 8086.64, average training loss: 7422.22, base loss: 16095.71
[INFO 2017-06-29 06:18:56,322 main.py:57] epoch 7124, training loss: 6601.33, average training loss: 7421.70, base loss: 16095.20
[INFO 2017-06-29 06:18:59,391 main.py:57] epoch 7125, training loss: 7215.09, average training loss: 7421.01, base loss: 16094.88
[INFO 2017-06-29 06:19:02,458 main.py:57] epoch 7126, training loss: 5760.29, average training loss: 7419.08, base loss: 16094.16
[INFO 2017-06-29 06:19:05,594 main.py:57] epoch 7127, training loss: 7096.50, average training loss: 7417.94, base loss: 16094.24
[INFO 2017-06-29 06:19:08,716 main.py:57] epoch 7128, training loss: 7792.71, average training loss: 7419.14, base loss: 16094.26
[INFO 2017-06-29 06:19:11,826 main.py:57] epoch 7129, training loss: 6950.62, average training loss: 7418.94, base loss: 16094.14
[INFO 2017-06-29 06:19:14,864 main.py:57] epoch 7130, training loss: 7506.15, average training loss: 7419.08, base loss: 16094.31
[INFO 2017-06-29 06:19:17,934 main.py:57] epoch 7131, training loss: 8053.71, average training loss: 7420.08, base loss: 16094.79
[INFO 2017-06-29 06:19:20,989 main.py:57] epoch 7132, training loss: 6858.63, average training loss: 7419.38, base loss: 16094.73
[INFO 2017-06-29 06:19:24,067 main.py:57] epoch 7133, training loss: 7307.20, average training loss: 7419.28, base loss: 16094.71
[INFO 2017-06-29 06:19:27,123 main.py:57] epoch 7134, training loss: 6729.58, average training loss: 7418.85, base loss: 16094.47
[INFO 2017-06-29 06:19:30,229 main.py:57] epoch 7135, training loss: 6844.15, average training loss: 7418.09, base loss: 16093.96
[INFO 2017-06-29 06:19:33,330 main.py:57] epoch 7136, training loss: 6930.43, average training loss: 7417.95, base loss: 16093.95
[INFO 2017-06-29 06:19:36,392 main.py:57] epoch 7137, training loss: 7067.90, average training loss: 7417.37, base loss: 16093.93
[INFO 2017-06-29 06:19:39,434 main.py:57] epoch 7138, training loss: 7468.27, average training loss: 7417.27, base loss: 16093.99
[INFO 2017-06-29 06:19:42,483 main.py:57] epoch 7139, training loss: 6913.74, average training loss: 7415.32, base loss: 16093.53
[INFO 2017-06-29 06:19:45,579 main.py:57] epoch 7140, training loss: 7320.41, average training loss: 7415.16, base loss: 16093.48
[INFO 2017-06-29 06:19:48,616 main.py:57] epoch 7141, training loss: 7942.22, average training loss: 7415.29, base loss: 16093.77
[INFO 2017-06-29 06:19:51,630 main.py:57] epoch 7142, training loss: 8327.12, average training loss: 7415.70, base loss: 16094.25
[INFO 2017-06-29 06:19:54,677 main.py:57] epoch 7143, training loss: 7369.58, average training loss: 7416.16, base loss: 16094.51
[INFO 2017-06-29 06:19:57,691 main.py:57] epoch 7144, training loss: 6427.18, average training loss: 7414.95, base loss: 16094.62
[INFO 2017-06-29 06:20:00,733 main.py:57] epoch 7145, training loss: 7950.27, average training loss: 7414.08, base loss: 16095.11
[INFO 2017-06-29 06:20:03,795 main.py:57] epoch 7146, training loss: 9176.53, average training loss: 7416.53, base loss: 16095.77
[INFO 2017-06-29 06:20:06,838 main.py:57] epoch 7147, training loss: 9933.23, average training loss: 7418.49, base loss: 16096.86
[INFO 2017-06-29 06:20:09,894 main.py:57] epoch 7148, training loss: 7905.92, average training loss: 7419.02, base loss: 16097.09
[INFO 2017-06-29 06:20:12,952 main.py:57] epoch 7149, training loss: 7242.53, average training loss: 7417.99, base loss: 16096.59
[INFO 2017-06-29 06:20:16,054 main.py:57] epoch 7150, training loss: 7029.80, average training loss: 7417.60, base loss: 16096.74
[INFO 2017-06-29 06:20:19,131 main.py:57] epoch 7151, training loss: 7491.40, average training loss: 7417.17, base loss: 16097.13
[INFO 2017-06-29 06:20:22,163 main.py:57] epoch 7152, training loss: 7545.98, average training loss: 7417.47, base loss: 16096.88
[INFO 2017-06-29 06:20:25,281 main.py:57] epoch 7153, training loss: 7945.28, average training loss: 7418.55, base loss: 16096.83
[INFO 2017-06-29 06:20:28,325 main.py:57] epoch 7154, training loss: 8132.23, average training loss: 7419.31, base loss: 16097.25
[INFO 2017-06-29 06:20:31,475 main.py:57] epoch 7155, training loss: 7620.80, average training loss: 7419.84, base loss: 16096.86
[INFO 2017-06-29 06:20:34,559 main.py:57] epoch 7156, training loss: 8126.67, average training loss: 7419.99, base loss: 16097.40
[INFO 2017-06-29 06:20:37,608 main.py:57] epoch 7157, training loss: 7200.13, average training loss: 7420.39, base loss: 16097.30
[INFO 2017-06-29 06:20:40,670 main.py:57] epoch 7158, training loss: 7662.15, average training loss: 7420.44, base loss: 16097.37
[INFO 2017-06-29 06:20:43,725 main.py:57] epoch 7159, training loss: 8931.05, average training loss: 7422.61, base loss: 16097.73
[INFO 2017-06-29 06:20:46,763 main.py:57] epoch 7160, training loss: 8375.67, average training loss: 7423.47, base loss: 16097.72
[INFO 2017-06-29 06:20:49,788 main.py:57] epoch 7161, training loss: 7576.57, average training loss: 7422.95, base loss: 16097.54
[INFO 2017-06-29 06:20:52,833 main.py:57] epoch 7162, training loss: 6460.44, average training loss: 7422.30, base loss: 16097.17
[INFO 2017-06-29 06:20:55,864 main.py:57] epoch 7163, training loss: 6687.99, average training loss: 7422.12, base loss: 16096.87
[INFO 2017-06-29 06:20:58,902 main.py:57] epoch 7164, training loss: 7162.82, average training loss: 7421.87, base loss: 16096.30
[INFO 2017-06-29 06:21:01,913 main.py:57] epoch 7165, training loss: 7351.09, average training loss: 7421.23, base loss: 16096.02
[INFO 2017-06-29 06:21:04,893 main.py:57] epoch 7166, training loss: 8364.92, average training loss: 7422.24, base loss: 16096.90
[INFO 2017-06-29 06:21:08,001 main.py:57] epoch 7167, training loss: 7101.07, average training loss: 7422.52, base loss: 16097.00
[INFO 2017-06-29 06:21:11,079 main.py:57] epoch 7168, training loss: 8790.47, average training loss: 7423.41, base loss: 16097.52
[INFO 2017-06-29 06:21:14,128 main.py:57] epoch 7169, training loss: 7379.67, average training loss: 7424.31, base loss: 16097.36
[INFO 2017-06-29 06:21:17,152 main.py:57] epoch 7170, training loss: 6617.50, average training loss: 7423.60, base loss: 16097.10
[INFO 2017-06-29 06:21:20,222 main.py:57] epoch 7171, training loss: 6744.27, average training loss: 7422.81, base loss: 16096.54
[INFO 2017-06-29 06:21:23,344 main.py:57] epoch 7172, training loss: 7388.37, average training loss: 7422.55, base loss: 16096.44
[INFO 2017-06-29 06:21:26,359 main.py:57] epoch 7173, training loss: 6532.39, average training loss: 7420.92, base loss: 16096.09
[INFO 2017-06-29 06:21:29,485 main.py:57] epoch 7174, training loss: 6915.54, average training loss: 7420.23, base loss: 16095.76
[INFO 2017-06-29 06:21:32,527 main.py:57] epoch 7175, training loss: 7135.25, average training loss: 7420.50, base loss: 16095.69
[INFO 2017-06-29 06:21:35,640 main.py:57] epoch 7176, training loss: 7919.81, average training loss: 7421.18, base loss: 16095.79
[INFO 2017-06-29 06:21:38,719 main.py:57] epoch 7177, training loss: 7434.71, average training loss: 7421.60, base loss: 16096.03
[INFO 2017-06-29 06:21:41,781 main.py:57] epoch 7178, training loss: 7956.86, average training loss: 7423.01, base loss: 16095.95
[INFO 2017-06-29 06:21:44,960 main.py:57] epoch 7179, training loss: 7808.20, average training loss: 7423.57, base loss: 16096.30
[INFO 2017-06-29 06:21:47,985 main.py:57] epoch 7180, training loss: 7980.84, average training loss: 7424.59, base loss: 16096.68
[INFO 2017-06-29 06:21:51,091 main.py:57] epoch 7181, training loss: 6888.59, average training loss: 7424.94, base loss: 16096.22
[INFO 2017-06-29 06:21:54,102 main.py:57] epoch 7182, training loss: 6622.36, average training loss: 7423.54, base loss: 16095.55
[INFO 2017-06-29 06:21:57,115 main.py:57] epoch 7183, training loss: 7012.62, average training loss: 7423.15, base loss: 16095.24
[INFO 2017-06-29 06:22:00,227 main.py:57] epoch 7184, training loss: 8227.62, average training loss: 7422.12, base loss: 16095.38
[INFO 2017-06-29 06:22:03,256 main.py:57] epoch 7185, training loss: 7210.43, average training loss: 7422.60, base loss: 16095.31
[INFO 2017-06-29 06:22:06,342 main.py:57] epoch 7186, training loss: 8387.58, average training loss: 7422.92, base loss: 16095.85
[INFO 2017-06-29 06:22:09,338 main.py:57] epoch 7187, training loss: 8889.32, average training loss: 7424.40, base loss: 16096.82
[INFO 2017-06-29 06:22:12,381 main.py:57] epoch 7188, training loss: 6900.92, average training loss: 7424.42, base loss: 16096.92
[INFO 2017-06-29 06:22:15,416 main.py:57] epoch 7189, training loss: 6966.29, average training loss: 7424.09, base loss: 16096.28
[INFO 2017-06-29 06:22:18,426 main.py:57] epoch 7190, training loss: 7882.60, average training loss: 7424.47, base loss: 16096.33
[INFO 2017-06-29 06:22:21,460 main.py:57] epoch 7191, training loss: 6804.06, average training loss: 7423.65, base loss: 16095.65
[INFO 2017-06-29 06:22:24,474 main.py:57] epoch 7192, training loss: 7487.23, average training loss: 7423.42, base loss: 16095.94
[INFO 2017-06-29 06:22:27,540 main.py:57] epoch 7193, training loss: 7911.15, average training loss: 7424.34, base loss: 16096.43
[INFO 2017-06-29 06:22:30,588 main.py:57] epoch 7194, training loss: 7982.23, average training loss: 7424.48, base loss: 16097.01
[INFO 2017-06-29 06:22:33,611 main.py:57] epoch 7195, training loss: 6907.83, average training loss: 7423.03, base loss: 16096.89
[INFO 2017-06-29 06:22:36,746 main.py:57] epoch 7196, training loss: 7965.49, average training loss: 7422.56, base loss: 16097.04
[INFO 2017-06-29 06:22:39,774 main.py:57] epoch 7197, training loss: 7779.13, average training loss: 7422.91, base loss: 16097.23
[INFO 2017-06-29 06:22:42,806 main.py:57] epoch 7198, training loss: 7696.96, average training loss: 7423.29, base loss: 16097.91
[INFO 2017-06-29 06:22:45,900 main.py:57] epoch 7199, training loss: 7287.67, average training loss: 7423.72, base loss: 16097.91
[INFO 2017-06-29 06:22:45,901 main.py:59] epoch 7199, testing
[INFO 2017-06-29 06:22:58,381 main.py:104] average testing loss: 7849.89, base loss: 16614.25
[INFO 2017-06-29 06:22:58,382 main.py:105] improve_loss: 8764.36, improve_percent: 0.53
[INFO 2017-06-29 06:22:58,383 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 06:22:58,421 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:23:01,513 main.py:57] epoch 7200, training loss: 6820.20, average training loss: 7422.73, base loss: 16097.22
[INFO 2017-06-29 06:23:04,607 main.py:57] epoch 7201, training loss: 6517.52, average training loss: 7422.24, base loss: 16096.17
[INFO 2017-06-29 06:23:07,618 main.py:57] epoch 7202, training loss: 10407.17, average training loss: 7424.86, base loss: 16096.96
[INFO 2017-06-29 06:23:10,655 main.py:57] epoch 7203, training loss: 7517.67, average training loss: 7424.99, base loss: 16096.49
[INFO 2017-06-29 06:23:13,699 main.py:57] epoch 7204, training loss: 6874.79, average training loss: 7424.61, base loss: 16096.60
[INFO 2017-06-29 06:23:16,791 main.py:57] epoch 7205, training loss: 7339.56, average training loss: 7424.68, base loss: 16096.71
[INFO 2017-06-29 06:23:19,880 main.py:57] epoch 7206, training loss: 6617.05, average training loss: 7424.07, base loss: 16096.38
[INFO 2017-06-29 06:23:22,859 main.py:57] epoch 7207, training loss: 7434.44, average training loss: 7424.32, base loss: 16096.65
[INFO 2017-06-29 06:23:25,942 main.py:57] epoch 7208, training loss: 7758.73, average training loss: 7424.90, base loss: 16096.32
[INFO 2017-06-29 06:23:28,988 main.py:57] epoch 7209, training loss: 6744.79, average training loss: 7424.34, base loss: 16095.70
[INFO 2017-06-29 06:23:31,958 main.py:57] epoch 7210, training loss: 7090.76, average training loss: 7424.32, base loss: 16095.37
[INFO 2017-06-29 06:23:35,038 main.py:57] epoch 7211, training loss: 7383.40, average training loss: 7424.24, base loss: 16095.34
[INFO 2017-06-29 06:23:38,029 main.py:57] epoch 7212, training loss: 8423.42, average training loss: 7426.02, base loss: 16095.64
[INFO 2017-06-29 06:23:41,098 main.py:57] epoch 7213, training loss: 7359.98, average training loss: 7423.29, base loss: 16095.77
[INFO 2017-06-29 06:23:44,091 main.py:57] epoch 7214, training loss: 6779.76, average training loss: 7422.58, base loss: 16095.58
[INFO 2017-06-29 06:23:47,114 main.py:57] epoch 7215, training loss: 8657.73, average training loss: 7423.65, base loss: 16096.43
[INFO 2017-06-29 06:23:50,144 main.py:57] epoch 7216, training loss: 8023.99, average training loss: 7423.93, base loss: 16096.58
[INFO 2017-06-29 06:23:53,182 main.py:57] epoch 7217, training loss: 7546.33, average training loss: 7423.79, base loss: 16096.48
[INFO 2017-06-29 06:23:56,263 main.py:57] epoch 7218, training loss: 6869.57, average training loss: 7422.37, base loss: 16096.69
[INFO 2017-06-29 06:23:59,299 main.py:57] epoch 7219, training loss: 7831.63, average training loss: 7423.55, base loss: 16096.87
[INFO 2017-06-29 06:24:02,367 main.py:57] epoch 7220, training loss: 7795.29, average training loss: 7423.72, base loss: 16097.48
[INFO 2017-06-29 06:24:05,417 main.py:57] epoch 7221, training loss: 8025.72, average training loss: 7424.43, base loss: 16097.89
[INFO 2017-06-29 06:24:08,481 main.py:57] epoch 7222, training loss: 8542.50, average training loss: 7425.00, base loss: 16098.61
[INFO 2017-06-29 06:24:11,441 main.py:57] epoch 7223, training loss: 6929.58, average training loss: 7424.73, base loss: 16098.65
[INFO 2017-06-29 06:24:14,481 main.py:57] epoch 7224, training loss: 7602.94, average training loss: 7424.91, base loss: 16098.78
[INFO 2017-06-29 06:24:17,474 main.py:57] epoch 7225, training loss: 6667.48, average training loss: 7423.73, base loss: 16098.81
[INFO 2017-06-29 06:24:20,477 main.py:57] epoch 7226, training loss: 6680.88, average training loss: 7423.04, base loss: 16098.78
[INFO 2017-06-29 06:24:23,536 main.py:57] epoch 7227, training loss: 7263.53, average training loss: 7422.51, base loss: 16099.07
[INFO 2017-06-29 06:24:26,615 main.py:57] epoch 7228, training loss: 7421.09, average training loss: 7421.43, base loss: 16099.10
[INFO 2017-06-29 06:24:29,678 main.py:57] epoch 7229, training loss: 7009.84, average training loss: 7421.08, base loss: 16098.63
[INFO 2017-06-29 06:24:32,683 main.py:57] epoch 7230, training loss: 7226.71, average training loss: 7421.14, base loss: 16098.22
[INFO 2017-06-29 06:24:35,782 main.py:57] epoch 7231, training loss: 8369.20, average training loss: 7422.37, base loss: 16098.85
[INFO 2017-06-29 06:24:38,850 main.py:57] epoch 7232, training loss: 6563.33, average training loss: 7422.37, base loss: 16098.65
[INFO 2017-06-29 06:24:41,956 main.py:57] epoch 7233, training loss: 7145.41, average training loss: 7421.81, base loss: 16098.46
[INFO 2017-06-29 06:24:45,023 main.py:57] epoch 7234, training loss: 7226.73, average training loss: 7421.94, base loss: 16098.31
[INFO 2017-06-29 06:24:48,087 main.py:57] epoch 7235, training loss: 7417.32, average training loss: 7421.68, base loss: 16098.07
[INFO 2017-06-29 06:24:51,169 main.py:57] epoch 7236, training loss: 9651.33, average training loss: 7422.83, base loss: 16099.01
[INFO 2017-06-29 06:24:54,273 main.py:57] epoch 7237, training loss: 7350.51, average training loss: 7423.34, base loss: 16098.96
[INFO 2017-06-29 06:24:57,314 main.py:57] epoch 7238, training loss: 7624.46, average training loss: 7423.12, base loss: 16099.23
[INFO 2017-06-29 06:25:00,334 main.py:57] epoch 7239, training loss: 6723.39, average training loss: 7423.26, base loss: 16098.66
[INFO 2017-06-29 06:25:03,342 main.py:57] epoch 7240, training loss: 7546.77, average training loss: 7422.86, base loss: 16098.91
[INFO 2017-06-29 06:25:06,361 main.py:57] epoch 7241, training loss: 7893.70, average training loss: 7423.71, base loss: 16099.18
[INFO 2017-06-29 06:25:09,485 main.py:57] epoch 7242, training loss: 7216.66, average training loss: 7424.00, base loss: 16099.54
[INFO 2017-06-29 06:25:12,544 main.py:57] epoch 7243, training loss: 7531.26, average training loss: 7423.93, base loss: 16100.06
[INFO 2017-06-29 06:25:15,613 main.py:57] epoch 7244, training loss: 6546.16, average training loss: 7423.12, base loss: 16100.10
[INFO 2017-06-29 06:25:18,647 main.py:57] epoch 7245, training loss: 7454.62, average training loss: 7422.44, base loss: 16100.69
[INFO 2017-06-29 06:25:21,676 main.py:57] epoch 7246, training loss: 7523.00, average training loss: 7423.05, base loss: 16100.93
[INFO 2017-06-29 06:25:24,741 main.py:57] epoch 7247, training loss: 8146.07, average training loss: 7424.36, base loss: 16101.36
[INFO 2017-06-29 06:25:27,722 main.py:57] epoch 7248, training loss: 7000.33, average training loss: 7424.38, base loss: 16100.97
[INFO 2017-06-29 06:25:30,745 main.py:57] epoch 7249, training loss: 6832.81, average training loss: 7422.67, base loss: 16100.68
[INFO 2017-06-29 06:25:33,811 main.py:57] epoch 7250, training loss: 7218.89, average training loss: 7423.26, base loss: 16100.52
[INFO 2017-06-29 06:25:36,792 main.py:57] epoch 7251, training loss: 7180.04, average training loss: 7423.63, base loss: 16100.35
[INFO 2017-06-29 06:25:39,896 main.py:57] epoch 7252, training loss: 7516.57, average training loss: 7424.98, base loss: 16100.22
[INFO 2017-06-29 06:25:42,973 main.py:57] epoch 7253, training loss: 6243.69, average training loss: 7423.90, base loss: 16099.34
[INFO 2017-06-29 06:25:46,041 main.py:57] epoch 7254, training loss: 6518.23, average training loss: 7422.49, base loss: 16099.18
[INFO 2017-06-29 06:25:49,075 main.py:57] epoch 7255, training loss: 7165.14, average training loss: 7422.63, base loss: 16099.10
[INFO 2017-06-29 06:25:52,193 main.py:57] epoch 7256, training loss: 8052.67, average training loss: 7423.24, base loss: 16099.88
[INFO 2017-06-29 06:25:55,175 main.py:57] epoch 7257, training loss: 7403.53, average training loss: 7421.99, base loss: 16099.89
[INFO 2017-06-29 06:25:58,208 main.py:57] epoch 7258, training loss: 7520.15, average training loss: 7421.82, base loss: 16100.07
[INFO 2017-06-29 06:26:01,221 main.py:57] epoch 7259, training loss: 7585.10, average training loss: 7421.14, base loss: 16099.96
[INFO 2017-06-29 06:26:04,317 main.py:57] epoch 7260, training loss: 7882.75, average training loss: 7421.84, base loss: 16099.89
[INFO 2017-06-29 06:26:07,368 main.py:57] epoch 7261, training loss: 6793.93, average training loss: 7421.62, base loss: 16099.46
[INFO 2017-06-29 06:26:10,465 main.py:57] epoch 7262, training loss: 8748.96, average training loss: 7423.45, base loss: 16099.76
[INFO 2017-06-29 06:26:13,463 main.py:57] epoch 7263, training loss: 7466.93, average training loss: 7424.24, base loss: 16099.54
[INFO 2017-06-29 06:26:16,555 main.py:57] epoch 7264, training loss: 5877.84, average training loss: 7422.68, base loss: 16098.94
[INFO 2017-06-29 06:26:19,596 main.py:57] epoch 7265, training loss: 8346.75, average training loss: 7423.97, base loss: 16099.45
[INFO 2017-06-29 06:26:22,612 main.py:57] epoch 7266, training loss: 7807.89, average training loss: 7425.43, base loss: 16099.55
[INFO 2017-06-29 06:26:25,603 main.py:57] epoch 7267, training loss: 7093.74, average training loss: 7425.74, base loss: 16099.39
[INFO 2017-06-29 06:26:28,660 main.py:57] epoch 7268, training loss: 7877.92, average training loss: 7427.01, base loss: 16099.42
[INFO 2017-06-29 06:26:31,675 main.py:57] epoch 7269, training loss: 6936.24, average training loss: 7425.15, base loss: 16099.23
[INFO 2017-06-29 06:26:34,714 main.py:57] epoch 7270, training loss: 7875.81, average training loss: 7425.33, base loss: 16098.63
[INFO 2017-06-29 06:26:37,753 main.py:57] epoch 7271, training loss: 8645.00, average training loss: 7426.55, base loss: 16099.09
[INFO 2017-06-29 06:26:40,855 main.py:57] epoch 7272, training loss: 7203.07, average training loss: 7425.65, base loss: 16099.30
[INFO 2017-06-29 06:26:43,908 main.py:57] epoch 7273, training loss: 7574.20, average training loss: 7426.18, base loss: 16099.66
[INFO 2017-06-29 06:26:46,941 main.py:57] epoch 7274, training loss: 6807.47, average training loss: 7425.93, base loss: 16099.24
[INFO 2017-06-29 06:26:49,960 main.py:57] epoch 7275, training loss: 8690.87, average training loss: 7426.53, base loss: 16099.34
[INFO 2017-06-29 06:26:53,014 main.py:57] epoch 7276, training loss: 7872.20, average training loss: 7427.42, base loss: 16099.66
[INFO 2017-06-29 06:26:56,021 main.py:57] epoch 7277, training loss: 8013.79, average training loss: 7427.65, base loss: 16100.05
[INFO 2017-06-29 06:26:59,085 main.py:57] epoch 7278, training loss: 7464.66, average training loss: 7426.39, base loss: 16100.07
[INFO 2017-06-29 06:27:02,188 main.py:57] epoch 7279, training loss: 7991.18, average training loss: 7427.17, base loss: 16100.47
[INFO 2017-06-29 06:27:05,278 main.py:57] epoch 7280, training loss: 7488.37, average training loss: 7426.89, base loss: 16099.81
[INFO 2017-06-29 06:27:08,356 main.py:57] epoch 7281, training loss: 7511.64, average training loss: 7426.49, base loss: 16099.71
[INFO 2017-06-29 06:27:11,427 main.py:57] epoch 7282, training loss: 6903.57, average training loss: 7425.45, base loss: 16099.49
[INFO 2017-06-29 06:27:14,453 main.py:57] epoch 7283, training loss: 7516.72, average training loss: 7426.65, base loss: 16099.26
[INFO 2017-06-29 06:27:17,555 main.py:57] epoch 7284, training loss: 7347.50, average training loss: 7427.13, base loss: 16098.98
[INFO 2017-06-29 06:27:20,619 main.py:57] epoch 7285, training loss: 6761.22, average training loss: 7425.93, base loss: 16098.83
[INFO 2017-06-29 06:27:23,654 main.py:57] epoch 7286, training loss: 7060.09, average training loss: 7425.90, base loss: 16099.01
[INFO 2017-06-29 06:27:26,660 main.py:57] epoch 7287, training loss: 7600.43, average training loss: 7427.12, base loss: 16099.29
[INFO 2017-06-29 06:27:29,718 main.py:57] epoch 7288, training loss: 7526.81, average training loss: 7427.06, base loss: 16099.74
[INFO 2017-06-29 06:27:32,728 main.py:57] epoch 7289, training loss: 8968.06, average training loss: 7427.96, base loss: 16100.70
[INFO 2017-06-29 06:27:35,792 main.py:57] epoch 7290, training loss: 6928.83, average training loss: 7427.24, base loss: 16100.40
[INFO 2017-06-29 06:27:38,794 main.py:57] epoch 7291, training loss: 7357.37, average training loss: 7426.58, base loss: 16100.11
[INFO 2017-06-29 06:27:41,826 main.py:57] epoch 7292, training loss: 7640.22, average training loss: 7426.57, base loss: 16100.58
[INFO 2017-06-29 06:27:44,846 main.py:57] epoch 7293, training loss: 7045.72, average training loss: 7426.24, base loss: 16100.61
[INFO 2017-06-29 06:27:47,876 main.py:57] epoch 7294, training loss: 7515.04, average training loss: 7426.75, base loss: 16100.65
[INFO 2017-06-29 06:27:50,876 main.py:57] epoch 7295, training loss: 7074.91, average training loss: 7425.41, base loss: 16100.76
[INFO 2017-06-29 06:27:53,921 main.py:57] epoch 7296, training loss: 6795.16, average training loss: 7423.85, base loss: 16100.61
[INFO 2017-06-29 06:27:56,959 main.py:57] epoch 7297, training loss: 8267.37, average training loss: 7425.22, base loss: 16100.97
[INFO 2017-06-29 06:27:59,999 main.py:57] epoch 7298, training loss: 7538.18, average training loss: 7425.51, base loss: 16100.80
[INFO 2017-06-29 06:28:03,064 main.py:57] epoch 7299, training loss: 8143.17, average training loss: 7425.93, base loss: 16101.29
[INFO 2017-06-29 06:28:03,065 main.py:59] epoch 7299, testing
[INFO 2017-06-29 06:28:15,687 main.py:104] average testing loss: 7748.74, base loss: 16248.50
[INFO 2017-06-29 06:28:15,687 main.py:105] improve_loss: 8499.76, improve_percent: 0.52
[INFO 2017-06-29 06:28:15,688 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:28:18,782 main.py:57] epoch 7300, training loss: 6496.12, average training loss: 7425.24, base loss: 16100.97
[INFO 2017-06-29 06:28:21,851 main.py:57] epoch 7301, training loss: 6832.65, average training loss: 7424.92, base loss: 16100.50
[INFO 2017-06-29 06:28:24,898 main.py:57] epoch 7302, training loss: 7513.42, average training loss: 7425.43, base loss: 16100.77
[INFO 2017-06-29 06:28:28,013 main.py:57] epoch 7303, training loss: 6729.48, average training loss: 7423.69, base loss: 16100.62
[INFO 2017-06-29 06:28:31,062 main.py:57] epoch 7304, training loss: 8080.57, average training loss: 7425.16, base loss: 16100.93
[INFO 2017-06-29 06:28:34,088 main.py:57] epoch 7305, training loss: 7600.50, average training loss: 7425.73, base loss: 16100.90
[INFO 2017-06-29 06:28:37,128 main.py:57] epoch 7306, training loss: 7536.64, average training loss: 7426.43, base loss: 16100.89
[INFO 2017-06-29 06:28:40,146 main.py:57] epoch 7307, training loss: 7001.23, average training loss: 7426.47, base loss: 16100.51
[INFO 2017-06-29 06:28:43,214 main.py:57] epoch 7308, training loss: 7398.58, average training loss: 7426.29, base loss: 16100.60
[INFO 2017-06-29 06:28:46,363 main.py:57] epoch 7309, training loss: 8764.21, average training loss: 7428.63, base loss: 16101.13
[INFO 2017-06-29 06:28:49,378 main.py:57] epoch 7310, training loss: 7865.78, average training loss: 7428.82, base loss: 16100.90
[INFO 2017-06-29 06:28:52,409 main.py:57] epoch 7311, training loss: 8492.53, average training loss: 7430.26, base loss: 16101.73
[INFO 2017-06-29 06:28:55,502 main.py:57] epoch 7312, training loss: 8097.20, average training loss: 7431.27, base loss: 16102.22
[INFO 2017-06-29 06:28:58,546 main.py:57] epoch 7313, training loss: 7362.91, average training loss: 7431.53, base loss: 16102.25
[INFO 2017-06-29 06:29:01,529 main.py:57] epoch 7314, training loss: 7809.99, average training loss: 7432.69, base loss: 16102.79
[INFO 2017-06-29 06:29:04,559 main.py:57] epoch 7315, training loss: 7585.87, average training loss: 7433.17, base loss: 16103.12
[INFO 2017-06-29 06:29:07,628 main.py:57] epoch 7316, training loss: 6908.25, average training loss: 7432.78, base loss: 16102.75
[INFO 2017-06-29 06:29:10,721 main.py:57] epoch 7317, training loss: 7663.59, average training loss: 7433.36, base loss: 16102.84
[INFO 2017-06-29 06:29:13,739 main.py:57] epoch 7318, training loss: 7219.52, average training loss: 7433.60, base loss: 16102.17
[INFO 2017-06-29 06:29:16,888 main.py:57] epoch 7319, training loss: 7025.29, average training loss: 7433.08, base loss: 16101.31
[INFO 2017-06-29 06:29:19,974 main.py:57] epoch 7320, training loss: 7283.70, average training loss: 7433.85, base loss: 16101.11
[INFO 2017-06-29 06:29:23,052 main.py:57] epoch 7321, training loss: 6867.66, average training loss: 7432.74, base loss: 16100.50
[INFO 2017-06-29 06:29:26,070 main.py:57] epoch 7322, training loss: 6418.66, average training loss: 7432.68, base loss: 16099.66
[INFO 2017-06-29 06:29:29,141 main.py:57] epoch 7323, training loss: 6848.64, average training loss: 7432.63, base loss: 16099.09
[INFO 2017-06-29 06:29:32,177 main.py:57] epoch 7324, training loss: 7142.40, average training loss: 7432.37, base loss: 16099.14
[INFO 2017-06-29 06:29:35,257 main.py:57] epoch 7325, training loss: 6495.80, average training loss: 7431.40, base loss: 16099.02
[INFO 2017-06-29 06:29:38,308 main.py:57] epoch 7326, training loss: 7311.71, average training loss: 7431.39, base loss: 16098.92
[INFO 2017-06-29 06:29:41,353 main.py:57] epoch 7327, training loss: 6793.52, average training loss: 7430.82, base loss: 16098.26
[INFO 2017-06-29 06:29:44,435 main.py:57] epoch 7328, training loss: 6930.62, average training loss: 7430.16, base loss: 16098.03
[INFO 2017-06-29 06:29:47,497 main.py:57] epoch 7329, training loss: 7520.59, average training loss: 7430.54, base loss: 16097.64
[INFO 2017-06-29 06:29:50,530 main.py:57] epoch 7330, training loss: 7094.41, average training loss: 7430.59, base loss: 16097.58
[INFO 2017-06-29 06:29:53,545 main.py:57] epoch 7331, training loss: 6943.62, average training loss: 7429.03, base loss: 16097.38
[INFO 2017-06-29 06:29:56,591 main.py:57] epoch 7332, training loss: 7846.60, average training loss: 7429.21, base loss: 16097.43
[INFO 2017-06-29 06:29:59,741 main.py:57] epoch 7333, training loss: 6425.98, average training loss: 7428.21, base loss: 16096.94
[INFO 2017-06-29 06:30:02,760 main.py:57] epoch 7334, training loss: 7639.99, average training loss: 7429.40, base loss: 16096.95
[INFO 2017-06-29 06:30:05,833 main.py:57] epoch 7335, training loss: 7087.48, average training loss: 7428.02, base loss: 16097.03
[INFO 2017-06-29 06:30:08,812 main.py:57] epoch 7336, training loss: 9062.56, average training loss: 7430.76, base loss: 16098.08
[INFO 2017-06-29 06:30:11,882 main.py:57] epoch 7337, training loss: 7475.73, average training loss: 7431.12, base loss: 16098.15
[INFO 2017-06-29 06:30:14,942 main.py:57] epoch 7338, training loss: 8445.19, average training loss: 7432.68, base loss: 16098.83
[INFO 2017-06-29 06:30:17,955 main.py:57] epoch 7339, training loss: 6820.77, average training loss: 7432.22, base loss: 16098.57
[INFO 2017-06-29 06:30:20,993 main.py:57] epoch 7340, training loss: 7468.36, average training loss: 7433.37, base loss: 16098.70
[INFO 2017-06-29 06:30:24,044 main.py:57] epoch 7341, training loss: 8092.51, average training loss: 7432.82, base loss: 16099.42
[INFO 2017-06-29 06:30:27,056 main.py:57] epoch 7342, training loss: 7311.91, average training loss: 7432.88, base loss: 16099.56
[INFO 2017-06-29 06:30:30,106 main.py:57] epoch 7343, training loss: 7531.07, average training loss: 7433.24, base loss: 16099.90
[INFO 2017-06-29 06:30:33,156 main.py:57] epoch 7344, training loss: 6205.02, average training loss: 7432.62, base loss: 16098.99
[INFO 2017-06-29 06:30:36,237 main.py:57] epoch 7345, training loss: 6703.12, average training loss: 7431.62, base loss: 16098.75
[INFO 2017-06-29 06:30:39,279 main.py:57] epoch 7346, training loss: 6910.67, average training loss: 7430.71, base loss: 16098.88
[INFO 2017-06-29 06:30:42,278 main.py:57] epoch 7347, training loss: 7442.33, average training loss: 7430.49, base loss: 16098.77
[INFO 2017-06-29 06:30:45,419 main.py:57] epoch 7348, training loss: 7243.29, average training loss: 7430.16, base loss: 16098.94
[INFO 2017-06-29 06:30:48,494 main.py:57] epoch 7349, training loss: 6872.79, average training loss: 7430.14, base loss: 16098.79
[INFO 2017-06-29 06:30:51,503 main.py:57] epoch 7350, training loss: 7370.51, average training loss: 7430.09, base loss: 16098.63
[INFO 2017-06-29 06:30:54,554 main.py:57] epoch 7351, training loss: 7314.01, average training loss: 7430.28, base loss: 16098.76
[INFO 2017-06-29 06:30:57,641 main.py:57] epoch 7352, training loss: 7803.11, average training loss: 7430.74, base loss: 16098.97
[INFO 2017-06-29 06:31:00,791 main.py:57] epoch 7353, training loss: 7442.38, average training loss: 7432.05, base loss: 16099.22
[INFO 2017-06-29 06:31:03,912 main.py:57] epoch 7354, training loss: 7933.01, average training loss: 7432.58, base loss: 16099.04
[INFO 2017-06-29 06:31:06,967 main.py:57] epoch 7355, training loss: 8135.01, average training loss: 7432.02, base loss: 16099.21
[INFO 2017-06-29 06:31:10,052 main.py:57] epoch 7356, training loss: 7270.49, average training loss: 7431.34, base loss: 16099.00
[INFO 2017-06-29 06:31:13,140 main.py:57] epoch 7357, training loss: 7707.88, average training loss: 7431.20, base loss: 16099.21
[INFO 2017-06-29 06:31:16,139 main.py:57] epoch 7358, training loss: 7486.71, average training loss: 7430.85, base loss: 16099.09
[INFO 2017-06-29 06:31:19,170 main.py:57] epoch 7359, training loss: 7433.85, average training loss: 7429.87, base loss: 16098.81
[INFO 2017-06-29 06:31:22,265 main.py:57] epoch 7360, training loss: 8386.90, average training loss: 7431.36, base loss: 16098.56
[INFO 2017-06-29 06:31:25,306 main.py:57] epoch 7361, training loss: 6736.52, average training loss: 7429.32, base loss: 16098.39
[INFO 2017-06-29 06:31:28,373 main.py:57] epoch 7362, training loss: 7599.40, average training loss: 7429.83, base loss: 16098.47
[INFO 2017-06-29 06:31:31,460 main.py:57] epoch 7363, training loss: 7482.16, average training loss: 7429.63, base loss: 16098.16
[INFO 2017-06-29 06:31:34,574 main.py:57] epoch 7364, training loss: 6844.09, average training loss: 7428.73, base loss: 16098.09
[INFO 2017-06-29 06:31:37,645 main.py:57] epoch 7365, training loss: 7176.73, average training loss: 7428.48, base loss: 16097.76
[INFO 2017-06-29 06:31:40,726 main.py:57] epoch 7366, training loss: 7861.30, average training loss: 7429.20, base loss: 16098.01
[INFO 2017-06-29 06:31:43,746 main.py:57] epoch 7367, training loss: 7501.37, average training loss: 7428.79, base loss: 16098.24
[INFO 2017-06-29 06:31:46,876 main.py:57] epoch 7368, training loss: 6202.87, average training loss: 7428.04, base loss: 16097.91
[INFO 2017-06-29 06:31:49,908 main.py:57] epoch 7369, training loss: 7235.60, average training loss: 7427.40, base loss: 16097.95
[INFO 2017-06-29 06:31:53,012 main.py:57] epoch 7370, training loss: 7369.67, average training loss: 7427.68, base loss: 16097.96
[INFO 2017-06-29 06:31:56,050 main.py:57] epoch 7371, training loss: 7158.02, average training loss: 7428.02, base loss: 16098.16
[INFO 2017-06-29 06:31:59,041 main.py:57] epoch 7372, training loss: 7106.84, average training loss: 7427.50, base loss: 16098.02
[INFO 2017-06-29 06:32:02,081 main.py:57] epoch 7373, training loss: 7513.55, average training loss: 7427.10, base loss: 16097.74
[INFO 2017-06-29 06:32:05,143 main.py:57] epoch 7374, training loss: 7974.48, average training loss: 7428.08, base loss: 16097.91
[INFO 2017-06-29 06:32:08,254 main.py:57] epoch 7375, training loss: 7321.45, average training loss: 7427.52, base loss: 16098.26
[INFO 2017-06-29 06:32:11,273 main.py:57] epoch 7376, training loss: 6769.73, average training loss: 7426.54, base loss: 16097.83
[INFO 2017-06-29 06:32:14,308 main.py:57] epoch 7377, training loss: 7027.56, average training loss: 7425.22, base loss: 16097.48
[INFO 2017-06-29 06:32:17,359 main.py:57] epoch 7378, training loss: 7610.34, average training loss: 7424.82, base loss: 16097.41
[INFO 2017-06-29 06:32:20,411 main.py:57] epoch 7379, training loss: 6878.76, average training loss: 7424.06, base loss: 16097.22
[INFO 2017-06-29 06:32:23,482 main.py:57] epoch 7380, training loss: 8261.46, average training loss: 7425.60, base loss: 16097.45
[INFO 2017-06-29 06:32:26,470 main.py:57] epoch 7381, training loss: 8328.72, average training loss: 7425.78, base loss: 16098.15
[INFO 2017-06-29 06:32:29,447 main.py:57] epoch 7382, training loss: 7668.51, average training loss: 7426.91, base loss: 16098.51
[INFO 2017-06-29 06:32:32,571 main.py:57] epoch 7383, training loss: 6999.20, average training loss: 7426.00, base loss: 16098.60
[INFO 2017-06-29 06:32:35,630 main.py:57] epoch 7384, training loss: 7567.07, average training loss: 7426.78, base loss: 16098.86
[INFO 2017-06-29 06:32:38,729 main.py:57] epoch 7385, training loss: 8411.10, average training loss: 7427.95, base loss: 16099.44
[INFO 2017-06-29 06:32:41,744 main.py:57] epoch 7386, training loss: 7604.92, average training loss: 7427.61, base loss: 16099.77
[INFO 2017-06-29 06:32:44,832 main.py:57] epoch 7387, training loss: 7085.15, average training loss: 7427.23, base loss: 16099.83
[INFO 2017-06-29 06:32:47,966 main.py:57] epoch 7388, training loss: 6626.27, average training loss: 7426.43, base loss: 16099.83
[INFO 2017-06-29 06:32:50,999 main.py:57] epoch 7389, training loss: 7319.31, average training loss: 7426.74, base loss: 16100.53
[INFO 2017-06-29 06:32:54,032 main.py:57] epoch 7390, training loss: 8109.08, average training loss: 7425.45, base loss: 16100.83
[INFO 2017-06-29 06:32:57,121 main.py:57] epoch 7391, training loss: 6692.76, average training loss: 7425.59, base loss: 16100.49
[INFO 2017-06-29 06:33:00,155 main.py:57] epoch 7392, training loss: 7210.79, average training loss: 7426.07, base loss: 16100.04
[INFO 2017-06-29 06:33:03,208 main.py:57] epoch 7393, training loss: 7164.87, average training loss: 7425.72, base loss: 16099.74
[INFO 2017-06-29 06:33:06,241 main.py:57] epoch 7394, training loss: 7197.28, average training loss: 7425.99, base loss: 16099.71
[INFO 2017-06-29 06:33:09,286 main.py:57] epoch 7395, training loss: 8113.35, average training loss: 7426.71, base loss: 16099.72
[INFO 2017-06-29 06:33:12,410 main.py:57] epoch 7396, training loss: 6855.25, average training loss: 7426.41, base loss: 16099.53
[INFO 2017-06-29 06:33:15,437 main.py:57] epoch 7397, training loss: 7061.29, average training loss: 7426.24, base loss: 16099.73
[INFO 2017-06-29 06:33:18,460 main.py:57] epoch 7398, training loss: 6905.38, average training loss: 7426.29, base loss: 16099.00
[INFO 2017-06-29 06:33:21,538 main.py:57] epoch 7399, training loss: 6785.53, average training loss: 7425.32, base loss: 16098.64
[INFO 2017-06-29 06:33:21,539 main.py:59] epoch 7399, testing
[INFO 2017-06-29 06:33:34,127 main.py:104] average testing loss: 7942.17, base loss: 16835.83
[INFO 2017-06-29 06:33:34,127 main.py:105] improve_loss: 8893.66, improve_percent: 0.53
[INFO 2017-06-29 06:33:34,129 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 06:33:34,166 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:33:37,208 main.py:57] epoch 7400, training loss: 6859.93, average training loss: 7423.79, base loss: 16098.54
[INFO 2017-06-29 06:33:40,192 main.py:57] epoch 7401, training loss: 7249.43, average training loss: 7423.75, base loss: 16098.61
[INFO 2017-06-29 06:33:43,304 main.py:57] epoch 7402, training loss: 6506.65, average training loss: 7421.96, base loss: 16097.71
[INFO 2017-06-29 06:33:46,351 main.py:57] epoch 7403, training loss: 6622.35, average training loss: 7421.01, base loss: 16097.36
[INFO 2017-06-29 06:33:49,367 main.py:57] epoch 7404, training loss: 7606.86, average training loss: 7421.93, base loss: 16097.55
[INFO 2017-06-29 06:33:52,441 main.py:57] epoch 7405, training loss: 7388.97, average training loss: 7421.66, base loss: 16097.63
[INFO 2017-06-29 06:33:55,449 main.py:57] epoch 7406, training loss: 6688.31, average training loss: 7421.51, base loss: 16097.05
[INFO 2017-06-29 06:33:58,470 main.py:57] epoch 7407, training loss: 7233.42, average training loss: 7420.07, base loss: 16096.99
[INFO 2017-06-29 06:34:01,539 main.py:57] epoch 7408, training loss: 6809.80, average training loss: 7418.76, base loss: 16096.60
[INFO 2017-06-29 06:34:04,598 main.py:57] epoch 7409, training loss: 7905.13, average training loss: 7419.29, base loss: 16096.64
[INFO 2017-06-29 06:34:07,650 main.py:57] epoch 7410, training loss: 6728.30, average training loss: 7419.59, base loss: 16096.73
[INFO 2017-06-29 06:34:10,693 main.py:57] epoch 7411, training loss: 6724.24, average training loss: 7418.85, base loss: 16096.43
[INFO 2017-06-29 06:34:13,747 main.py:57] epoch 7412, training loss: 7861.88, average training loss: 7419.77, base loss: 16097.04
[INFO 2017-06-29 06:34:16,811 main.py:57] epoch 7413, training loss: 7228.14, average training loss: 7419.46, base loss: 16096.75
[INFO 2017-06-29 06:34:19,848 main.py:57] epoch 7414, training loss: 7325.72, average training loss: 7420.00, base loss: 16096.52
[INFO 2017-06-29 06:34:22,855 main.py:57] epoch 7415, training loss: 7070.38, average training loss: 7420.17, base loss: 16095.97
[INFO 2017-06-29 06:34:25,870 main.py:57] epoch 7416, training loss: 7734.73, average training loss: 7421.01, base loss: 16096.50
[INFO 2017-06-29 06:34:28,941 main.py:57] epoch 7417, training loss: 6737.71, average training loss: 7420.24, base loss: 16096.02
[INFO 2017-06-29 06:34:32,003 main.py:57] epoch 7418, training loss: 6385.12, average training loss: 7420.60, base loss: 16095.88
[INFO 2017-06-29 06:34:35,135 main.py:57] epoch 7419, training loss: 6571.07, average training loss: 7418.90, base loss: 16095.58
[INFO 2017-06-29 06:34:38,191 main.py:57] epoch 7420, training loss: 6439.75, average training loss: 7418.72, base loss: 16095.07
[INFO 2017-06-29 06:34:41,281 main.py:57] epoch 7421, training loss: 7149.85, average training loss: 7417.67, base loss: 16095.11
[INFO 2017-06-29 06:34:44,339 main.py:57] epoch 7422, training loss: 6675.73, average training loss: 7417.54, base loss: 16094.99
[INFO 2017-06-29 06:34:47,400 main.py:57] epoch 7423, training loss: 7802.98, average training loss: 7418.74, base loss: 16094.98
[INFO 2017-06-29 06:34:50,409 main.py:57] epoch 7424, training loss: 7402.55, average training loss: 7418.01, base loss: 16094.89
[INFO 2017-06-29 06:34:53,422 main.py:57] epoch 7425, training loss: 6856.78, average training loss: 7417.99, base loss: 16094.34
[INFO 2017-06-29 06:34:56,482 main.py:57] epoch 7426, training loss: 7797.24, average training loss: 7419.19, base loss: 16094.87
[INFO 2017-06-29 06:34:59,526 main.py:57] epoch 7427, training loss: 6958.83, average training loss: 7417.99, base loss: 16094.79
[INFO 2017-06-29 06:35:02,644 main.py:57] epoch 7428, training loss: 6882.14, average training loss: 7417.42, base loss: 16094.66
[INFO 2017-06-29 06:35:05,718 main.py:57] epoch 7429, training loss: 7183.25, average training loss: 7417.23, base loss: 16094.59
[INFO 2017-06-29 06:35:08,769 main.py:57] epoch 7430, training loss: 6984.33, average training loss: 7416.84, base loss: 16094.51
[INFO 2017-06-29 06:35:11,868 main.py:57] epoch 7431, training loss: 6808.35, average training loss: 7416.98, base loss: 16093.77
[INFO 2017-06-29 06:35:14,900 main.py:57] epoch 7432, training loss: 7788.42, average training loss: 7417.37, base loss: 16094.19
[INFO 2017-06-29 06:35:17,987 main.py:57] epoch 7433, training loss: 7855.60, average training loss: 7417.99, base loss: 16094.48
[INFO 2017-06-29 06:35:21,069 main.py:57] epoch 7434, training loss: 7575.87, average training loss: 7416.07, base loss: 16095.15
[INFO 2017-06-29 06:35:24,108 main.py:57] epoch 7435, training loss: 6634.08, average training loss: 7415.33, base loss: 16095.00
[INFO 2017-06-29 06:35:27,093 main.py:57] epoch 7436, training loss: 7084.45, average training loss: 7415.44, base loss: 16094.71
[INFO 2017-06-29 06:35:30,181 main.py:57] epoch 7437, training loss: 7067.01, average training loss: 7414.47, base loss: 16094.22
[INFO 2017-06-29 06:35:33,202 main.py:57] epoch 7438, training loss: 7034.49, average training loss: 7413.59, base loss: 16094.05
[INFO 2017-06-29 06:35:36,228 main.py:57] epoch 7439, training loss: 7069.35, average training loss: 7411.26, base loss: 16093.95
[INFO 2017-06-29 06:35:39,214 main.py:57] epoch 7440, training loss: 6762.99, average training loss: 7409.43, base loss: 16093.64
[INFO 2017-06-29 06:35:42,282 main.py:57] epoch 7441, training loss: 8775.90, average training loss: 7411.06, base loss: 16094.68
[INFO 2017-06-29 06:35:45,298 main.py:57] epoch 7442, training loss: 7654.85, average training loss: 7411.56, base loss: 16094.29
[INFO 2017-06-29 06:35:48,327 main.py:57] epoch 7443, training loss: 7713.11, average training loss: 7411.61, base loss: 16094.21
[INFO 2017-06-29 06:35:51,339 main.py:57] epoch 7444, training loss: 8481.23, average training loss: 7413.01, base loss: 16094.65
[INFO 2017-06-29 06:35:54,339 main.py:57] epoch 7445, training loss: 6345.40, average training loss: 7411.78, base loss: 16094.00
[INFO 2017-06-29 06:35:57,440 main.py:57] epoch 7446, training loss: 8069.46, average training loss: 7412.61, base loss: 16094.77
[INFO 2017-06-29 06:36:00,529 main.py:57] epoch 7447, training loss: 7412.76, average training loss: 7412.33, base loss: 16094.84
[INFO 2017-06-29 06:36:03,662 main.py:57] epoch 7448, training loss: 7066.33, average training loss: 7412.18, base loss: 16094.88
[INFO 2017-06-29 06:36:06,687 main.py:57] epoch 7449, training loss: 6140.51, average training loss: 7411.11, base loss: 16093.81
[INFO 2017-06-29 06:36:09,808 main.py:57] epoch 7450, training loss: 7578.31, average training loss: 7411.94, base loss: 16093.87
[INFO 2017-06-29 06:36:12,883 main.py:57] epoch 7451, training loss: 7839.67, average training loss: 7411.97, base loss: 16094.19
[INFO 2017-06-29 06:36:15,956 main.py:57] epoch 7452, training loss: 7134.70, average training loss: 7412.03, base loss: 16094.21
[INFO 2017-06-29 06:36:18,934 main.py:57] epoch 7453, training loss: 7564.96, average training loss: 7412.20, base loss: 16094.32
[INFO 2017-06-29 06:36:22,002 main.py:57] epoch 7454, training loss: 6860.55, average training loss: 7411.36, base loss: 16093.97
[INFO 2017-06-29 06:36:25,074 main.py:57] epoch 7455, training loss: 7840.30, average training loss: 7411.73, base loss: 16094.09
[INFO 2017-06-29 06:36:28,155 main.py:57] epoch 7456, training loss: 7417.54, average training loss: 7411.31, base loss: 16093.53
[INFO 2017-06-29 06:36:31,195 main.py:57] epoch 7457, training loss: 8215.77, average training loss: 7411.94, base loss: 16093.63
[INFO 2017-06-29 06:36:34,207 main.py:57] epoch 7458, training loss: 7820.25, average training loss: 7413.01, base loss: 16093.87
[INFO 2017-06-29 06:36:37,230 main.py:57] epoch 7459, training loss: 7520.72, average training loss: 7412.53, base loss: 16094.28
[INFO 2017-06-29 06:36:40,286 main.py:57] epoch 7460, training loss: 8361.69, average training loss: 7412.26, base loss: 16094.90
[INFO 2017-06-29 06:36:43,344 main.py:57] epoch 7461, training loss: 8071.45, average training loss: 7412.60, base loss: 16095.03
[INFO 2017-06-29 06:36:46,510 main.py:57] epoch 7462, training loss: 8425.14, average training loss: 7413.55, base loss: 16095.26
[INFO 2017-06-29 06:36:49,641 main.py:57] epoch 7463, training loss: 6541.94, average training loss: 7413.06, base loss: 16094.79
[INFO 2017-06-29 06:36:52,659 main.py:57] epoch 7464, training loss: 7655.99, average training loss: 7413.43, base loss: 16095.14
[INFO 2017-06-29 06:36:55,673 main.py:57] epoch 7465, training loss: 7770.73, average training loss: 7412.09, base loss: 16095.22
[INFO 2017-06-29 06:36:58,683 main.py:57] epoch 7466, training loss: 6769.72, average training loss: 7410.97, base loss: 16095.07
[INFO 2017-06-29 06:37:01,699 main.py:57] epoch 7467, training loss: 7571.76, average training loss: 7410.91, base loss: 16095.37
[INFO 2017-06-29 06:37:04,710 main.py:57] epoch 7468, training loss: 6620.23, average training loss: 7410.21, base loss: 16094.74
[INFO 2017-06-29 06:37:07,853 main.py:57] epoch 7469, training loss: 8288.76, average training loss: 7411.08, base loss: 16095.13
[INFO 2017-06-29 06:37:11,018 main.py:57] epoch 7470, training loss: 7205.38, average training loss: 7411.30, base loss: 16095.31
[INFO 2017-06-29 06:37:14,060 main.py:57] epoch 7471, training loss: 7537.01, average training loss: 7411.16, base loss: 16095.54
[INFO 2017-06-29 06:37:17,137 main.py:57] epoch 7472, training loss: 7869.21, average training loss: 7412.23, base loss: 16095.98
[INFO 2017-06-29 06:37:20,238 main.py:57] epoch 7473, training loss: 7974.27, average training loss: 7412.29, base loss: 16096.55
[INFO 2017-06-29 06:37:23,280 main.py:57] epoch 7474, training loss: 7159.32, average training loss: 7412.72, base loss: 16096.18
[INFO 2017-06-29 06:37:26,309 main.py:57] epoch 7475, training loss: 7497.85, average training loss: 7412.11, base loss: 16096.05
[INFO 2017-06-29 06:37:29,345 main.py:57] epoch 7476, training loss: 7088.77, average training loss: 7410.92, base loss: 16096.29
[INFO 2017-06-29 06:37:32,402 main.py:57] epoch 7477, training loss: 8387.25, average training loss: 7412.28, base loss: 16096.73
[INFO 2017-06-29 06:37:35,475 main.py:57] epoch 7478, training loss: 8075.71, average training loss: 7412.93, base loss: 16097.29
[INFO 2017-06-29 06:37:38,535 main.py:57] epoch 7479, training loss: 8732.72, average training loss: 7414.48, base loss: 16097.90
[INFO 2017-06-29 06:37:41,566 main.py:57] epoch 7480, training loss: 7018.14, average training loss: 7413.64, base loss: 16097.60
[INFO 2017-06-29 06:37:44,544 main.py:57] epoch 7481, training loss: 7475.48, average training loss: 7413.91, base loss: 16097.78
[INFO 2017-06-29 06:37:47,589 main.py:57] epoch 7482, training loss: 7469.30, average training loss: 7414.50, base loss: 16097.55
[INFO 2017-06-29 06:37:50,656 main.py:57] epoch 7483, training loss: 7567.48, average training loss: 7414.78, base loss: 16097.60
[INFO 2017-06-29 06:37:53,650 main.py:57] epoch 7484, training loss: 6609.38, average training loss: 7414.47, base loss: 16097.73
[INFO 2017-06-29 06:37:56,746 main.py:57] epoch 7485, training loss: 6865.35, average training loss: 7415.01, base loss: 16097.58
[INFO 2017-06-29 06:37:59,801 main.py:57] epoch 7486, training loss: 7130.37, average training loss: 7414.97, base loss: 16097.40
[INFO 2017-06-29 06:38:02,784 main.py:57] epoch 7487, training loss: 6541.77, average training loss: 7414.04, base loss: 16096.84
[INFO 2017-06-29 06:38:05,812 main.py:57] epoch 7488, training loss: 7462.61, average training loss: 7415.22, base loss: 16097.00
[INFO 2017-06-29 06:38:08,836 main.py:57] epoch 7489, training loss: 7386.84, average training loss: 7415.98, base loss: 16096.90
[INFO 2017-06-29 06:38:11,834 main.py:57] epoch 7490, training loss: 7394.37, average training loss: 7415.63, base loss: 16096.76
[INFO 2017-06-29 06:38:14,863 main.py:57] epoch 7491, training loss: 7369.32, average training loss: 7416.73, base loss: 16096.46
[INFO 2017-06-29 06:38:17,971 main.py:57] epoch 7492, training loss: 7638.53, average training loss: 7416.37, base loss: 16096.46
[INFO 2017-06-29 06:38:21,038 main.py:57] epoch 7493, training loss: 8227.35, average training loss: 7417.80, base loss: 16096.67
[INFO 2017-06-29 06:38:24,075 main.py:57] epoch 7494, training loss: 6949.31, average training loss: 7417.75, base loss: 16096.02
[INFO 2017-06-29 06:38:27,150 main.py:57] epoch 7495, training loss: 8375.23, average training loss: 7418.10, base loss: 16095.84
[INFO 2017-06-29 06:38:30,262 main.py:57] epoch 7496, training loss: 7343.38, average training loss: 7417.71, base loss: 16095.46
[INFO 2017-06-29 06:38:33,346 main.py:57] epoch 7497, training loss: 6979.01, average training loss: 7417.41, base loss: 16094.93
[INFO 2017-06-29 06:38:36,387 main.py:57] epoch 7498, training loss: 8372.47, average training loss: 7417.74, base loss: 16095.06
[INFO 2017-06-29 06:38:39,440 main.py:57] epoch 7499, training loss: 7792.39, average training loss: 7418.09, base loss: 16094.83
[INFO 2017-06-29 06:38:39,441 main.py:59] epoch 7499, testing
[INFO 2017-06-29 06:38:51,947 main.py:104] average testing loss: 7957.93, base loss: 16547.66
[INFO 2017-06-29 06:38:51,947 main.py:105] improve_loss: 8589.73, improve_percent: 0.52
[INFO 2017-06-29 06:38:51,949 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:38:54,954 main.py:57] epoch 7500, training loss: 7642.39, average training loss: 7419.05, base loss: 16095.37
[INFO 2017-06-29 06:38:58,025 main.py:57] epoch 7501, training loss: 7158.09, average training loss: 7418.49, base loss: 16095.06
[INFO 2017-06-29 06:39:01,070 main.py:57] epoch 7502, training loss: 8278.26, average training loss: 7418.89, base loss: 16095.70
[INFO 2017-06-29 06:39:04,128 main.py:57] epoch 7503, training loss: 6777.52, average training loss: 7418.13, base loss: 16095.48
[INFO 2017-06-29 06:39:07,222 main.py:57] epoch 7504, training loss: 7858.96, average training loss: 7417.42, base loss: 16095.70
[INFO 2017-06-29 06:39:10,272 main.py:57] epoch 7505, training loss: 7806.32, average training loss: 7418.21, base loss: 16095.91
[INFO 2017-06-29 06:39:13,308 main.py:57] epoch 7506, training loss: 7358.91, average training loss: 7418.54, base loss: 16096.07
[INFO 2017-06-29 06:39:16,331 main.py:57] epoch 7507, training loss: 7458.43, average training loss: 7418.78, base loss: 16096.00
[INFO 2017-06-29 06:39:19,401 main.py:57] epoch 7508, training loss: 6915.74, average training loss: 7418.65, base loss: 16095.56
[INFO 2017-06-29 06:39:22,482 main.py:57] epoch 7509, training loss: 7505.34, average training loss: 7418.60, base loss: 16095.59
[INFO 2017-06-29 06:39:25,583 main.py:57] epoch 7510, training loss: 7181.92, average training loss: 7419.03, base loss: 16095.51
[INFO 2017-06-29 06:39:28,557 main.py:57] epoch 7511, training loss: 7741.61, average training loss: 7419.54, base loss: 16095.97
[INFO 2017-06-29 06:39:31,640 main.py:57] epoch 7512, training loss: 7334.18, average training loss: 7419.62, base loss: 16095.84
[INFO 2017-06-29 06:39:34,642 main.py:57] epoch 7513, training loss: 7727.57, average training loss: 7419.95, base loss: 16096.22
[INFO 2017-06-29 06:39:37,740 main.py:57] epoch 7514, training loss: 7470.12, average training loss: 7420.07, base loss: 16096.00
[INFO 2017-06-29 06:39:40,816 main.py:57] epoch 7515, training loss: 7483.85, average training loss: 7419.87, base loss: 16096.01
[INFO 2017-06-29 06:39:43,883 main.py:57] epoch 7516, training loss: 6641.61, average training loss: 7419.20, base loss: 16095.84
[INFO 2017-06-29 06:39:46,951 main.py:57] epoch 7517, training loss: 7175.26, average training loss: 7418.55, base loss: 16096.16
[INFO 2017-06-29 06:39:50,090 main.py:57] epoch 7518, training loss: 7046.94, average training loss: 7417.45, base loss: 16095.90
[INFO 2017-06-29 06:39:53,110 main.py:57] epoch 7519, training loss: 8345.40, average training loss: 7417.22, base loss: 16095.84
[INFO 2017-06-29 06:39:56,142 main.py:57] epoch 7520, training loss: 6910.08, average training loss: 7416.83, base loss: 16095.53
[INFO 2017-06-29 06:39:59,153 main.py:57] epoch 7521, training loss: 7391.57, average training loss: 7416.61, base loss: 16095.61
[INFO 2017-06-29 06:40:02,267 main.py:57] epoch 7522, training loss: 7560.33, average training loss: 7415.95, base loss: 16095.63
[INFO 2017-06-29 06:40:05,292 main.py:57] epoch 7523, training loss: 7613.09, average training loss: 7417.05, base loss: 16095.51
[INFO 2017-06-29 06:40:08,264 main.py:57] epoch 7524, training loss: 7589.24, average training loss: 7417.57, base loss: 16095.90
[INFO 2017-06-29 06:40:11,305 main.py:57] epoch 7525, training loss: 7396.13, average training loss: 7417.54, base loss: 16096.15
[INFO 2017-06-29 06:40:14,308 main.py:57] epoch 7526, training loss: 7834.48, average training loss: 7417.40, base loss: 16095.89
[INFO 2017-06-29 06:40:17,379 main.py:57] epoch 7527, training loss: 8022.78, average training loss: 7416.07, base loss: 16096.30
[INFO 2017-06-29 06:40:20,455 main.py:57] epoch 7528, training loss: 6908.58, average training loss: 7416.30, base loss: 16096.43
[INFO 2017-06-29 06:40:23,472 main.py:57] epoch 7529, training loss: 8042.42, average training loss: 7416.86, base loss: 16096.70
[INFO 2017-06-29 06:40:26,581 main.py:57] epoch 7530, training loss: 6733.97, average training loss: 7414.70, base loss: 16096.22
[INFO 2017-06-29 06:40:29,580 main.py:57] epoch 7531, training loss: 6481.71, average training loss: 7413.20, base loss: 16095.79
[INFO 2017-06-29 06:40:32,638 main.py:57] epoch 7532, training loss: 7227.93, average training loss: 7413.58, base loss: 16096.09
[INFO 2017-06-29 06:40:35,677 main.py:57] epoch 7533, training loss: 6828.27, average training loss: 7412.56, base loss: 16095.90
[INFO 2017-06-29 06:40:38,775 main.py:57] epoch 7534, training loss: 6537.69, average training loss: 7412.18, base loss: 16095.91
[INFO 2017-06-29 06:40:41,893 main.py:57] epoch 7535, training loss: 7788.40, average training loss: 7412.11, base loss: 16096.39
[INFO 2017-06-29 06:40:44,924 main.py:57] epoch 7536, training loss: 7303.73, average training loss: 7411.56, base loss: 16096.73
[INFO 2017-06-29 06:40:47,930 main.py:57] epoch 7537, training loss: 7090.44, average training loss: 7410.70, base loss: 16096.67
[INFO 2017-06-29 06:40:50,919 main.py:57] epoch 7538, training loss: 6401.69, average training loss: 7409.18, base loss: 16096.30
[INFO 2017-06-29 06:40:53,990 main.py:57] epoch 7539, training loss: 7888.60, average training loss: 7410.00, base loss: 16096.80
[INFO 2017-06-29 06:40:57,017 main.py:57] epoch 7540, training loss: 6202.45, average training loss: 7408.59, base loss: 16096.12
[INFO 2017-06-29 06:41:00,058 main.py:57] epoch 7541, training loss: 7533.63, average training loss: 7408.18, base loss: 16096.68
[INFO 2017-06-29 06:41:03,144 main.py:57] epoch 7542, training loss: 6138.45, average training loss: 7407.56, base loss: 16096.13
[INFO 2017-06-29 06:41:06,175 main.py:57] epoch 7543, training loss: 7551.90, average training loss: 7407.97, base loss: 16096.55
[INFO 2017-06-29 06:41:09,358 main.py:57] epoch 7544, training loss: 6608.05, average training loss: 7407.13, base loss: 16096.02
[INFO 2017-06-29 06:41:12,426 main.py:57] epoch 7545, training loss: 6854.11, average training loss: 7405.13, base loss: 16095.75
[INFO 2017-06-29 06:41:15,550 main.py:57] epoch 7546, training loss: 7186.03, average training loss: 7404.40, base loss: 16095.87
[INFO 2017-06-29 06:41:18,624 main.py:57] epoch 7547, training loss: 8293.56, average training loss: 7405.73, base loss: 16096.33
[INFO 2017-06-29 06:41:21,756 main.py:57] epoch 7548, training loss: 8432.36, average training loss: 7406.03, base loss: 16097.35
[INFO 2017-06-29 06:41:24,795 main.py:57] epoch 7549, training loss: 6581.60, average training loss: 7405.23, base loss: 16096.72
[INFO 2017-06-29 06:41:27,783 main.py:57] epoch 7550, training loss: 7965.75, average training loss: 7405.82, base loss: 16097.05
[INFO 2017-06-29 06:41:30,850 main.py:57] epoch 7551, training loss: 7178.38, average training loss: 7405.26, base loss: 16096.92
[INFO 2017-06-29 06:41:33,888 main.py:57] epoch 7552, training loss: 6781.50, average training loss: 7404.48, base loss: 16096.53
[INFO 2017-06-29 06:41:37,029 main.py:57] epoch 7553, training loss: 7046.04, average training loss: 7404.01, base loss: 16096.88
[INFO 2017-06-29 06:41:40,043 main.py:57] epoch 7554, training loss: 8247.10, average training loss: 7405.74, base loss: 16097.10
[INFO 2017-06-29 06:41:43,081 main.py:57] epoch 7555, training loss: 6853.83, average training loss: 7405.20, base loss: 16096.72
[INFO 2017-06-29 06:41:46,161 main.py:57] epoch 7556, training loss: 9081.13, average training loss: 7405.79, base loss: 16097.32
[INFO 2017-06-29 06:41:49,237 main.py:57] epoch 7557, training loss: 7227.91, average training loss: 7406.11, base loss: 16097.22
[INFO 2017-06-29 06:41:52,277 main.py:57] epoch 7558, training loss: 7684.43, average training loss: 7406.50, base loss: 16097.32
[INFO 2017-06-29 06:41:55,401 main.py:57] epoch 7559, training loss: 7067.31, average training loss: 7406.38, base loss: 16097.03
[INFO 2017-06-29 06:41:58,501 main.py:57] epoch 7560, training loss: 6750.13, average training loss: 7405.96, base loss: 16096.72
[INFO 2017-06-29 06:42:01,516 main.py:57] epoch 7561, training loss: 6877.76, average training loss: 7403.76, base loss: 16096.57
[INFO 2017-06-29 06:42:04,570 main.py:57] epoch 7562, training loss: 7452.71, average training loss: 7404.12, base loss: 16096.76
[INFO 2017-06-29 06:42:07,647 main.py:57] epoch 7563, training loss: 8075.71, average training loss: 7405.01, base loss: 16096.67
[INFO 2017-06-29 06:42:10,729 main.py:57] epoch 7564, training loss: 6938.98, average training loss: 7404.92, base loss: 16096.55
[INFO 2017-06-29 06:42:13,744 main.py:57] epoch 7565, training loss: 8140.56, average training loss: 7405.45, base loss: 16096.91
[INFO 2017-06-29 06:42:16,812 main.py:57] epoch 7566, training loss: 7309.26, average training loss: 7404.86, base loss: 16096.59
[INFO 2017-06-29 06:42:19,847 main.py:57] epoch 7567, training loss: 6596.83, average training loss: 7403.94, base loss: 16095.94
[INFO 2017-06-29 06:42:22,870 main.py:57] epoch 7568, training loss: 7514.83, average training loss: 7403.40, base loss: 16096.23
[INFO 2017-06-29 06:42:25,865 main.py:57] epoch 7569, training loss: 6612.67, average training loss: 7402.29, base loss: 16095.82
[INFO 2017-06-29 06:42:28,847 main.py:57] epoch 7570, training loss: 7502.04, average training loss: 7402.09, base loss: 16096.08
[INFO 2017-06-29 06:42:31,943 main.py:57] epoch 7571, training loss: 7473.40, average training loss: 7401.45, base loss: 16096.17
[INFO 2017-06-29 06:42:35,027 main.py:57] epoch 7572, training loss: 7758.72, average training loss: 7402.16, base loss: 16096.49
[INFO 2017-06-29 06:42:38,078 main.py:57] epoch 7573, training loss: 7367.07, average training loss: 7402.14, base loss: 16096.40
[INFO 2017-06-29 06:42:41,204 main.py:57] epoch 7574, training loss: 7905.46, average training loss: 7402.45, base loss: 16096.44
[INFO 2017-06-29 06:42:44,300 main.py:57] epoch 7575, training loss: 7211.52, average training loss: 7402.20, base loss: 16096.40
[INFO 2017-06-29 06:42:47,434 main.py:57] epoch 7576, training loss: 8701.14, average training loss: 7403.92, base loss: 16096.58
[INFO 2017-06-29 06:42:50,465 main.py:57] epoch 7577, training loss: 7193.31, average training loss: 7404.08, base loss: 16096.63
[INFO 2017-06-29 06:42:53,467 main.py:57] epoch 7578, training loss: 6975.59, average training loss: 7403.09, base loss: 16096.96
[INFO 2017-06-29 06:42:56,501 main.py:57] epoch 7579, training loss: 7143.34, average training loss: 7402.32, base loss: 16097.22
[INFO 2017-06-29 06:42:59,630 main.py:57] epoch 7580, training loss: 6999.02, average training loss: 7402.39, base loss: 16097.01
[INFO 2017-06-29 06:43:02,696 main.py:57] epoch 7581, training loss: 7456.85, average training loss: 7402.30, base loss: 16097.06
[INFO 2017-06-29 06:43:05,755 main.py:57] epoch 7582, training loss: 7728.55, average training loss: 7401.19, base loss: 16097.03
[INFO 2017-06-29 06:43:08,855 main.py:57] epoch 7583, training loss: 7002.90, average training loss: 7401.19, base loss: 16096.96
[INFO 2017-06-29 06:43:11,915 main.py:57] epoch 7584, training loss: 7315.87, average training loss: 7401.64, base loss: 16096.88
[INFO 2017-06-29 06:43:14,989 main.py:57] epoch 7585, training loss: 7782.76, average training loss: 7401.88, base loss: 16096.58
[INFO 2017-06-29 06:43:18,098 main.py:57] epoch 7586, training loss: 7763.75, average training loss: 7402.26, base loss: 16096.62
[INFO 2017-06-29 06:43:21,265 main.py:57] epoch 7587, training loss: 6227.36, average training loss: 7400.67, base loss: 16096.30
[INFO 2017-06-29 06:43:24,378 main.py:57] epoch 7588, training loss: 7364.58, average training loss: 7400.35, base loss: 16096.41
[INFO 2017-06-29 06:43:27,451 main.py:57] epoch 7589, training loss: 6673.94, average training loss: 7400.28, base loss: 16095.76
[INFO 2017-06-29 06:43:30,488 main.py:57] epoch 7590, training loss: 7477.20, average training loss: 7400.60, base loss: 16095.47
[INFO 2017-06-29 06:43:33,514 main.py:57] epoch 7591, training loss: 7035.32, average training loss: 7401.25, base loss: 16095.36
[INFO 2017-06-29 06:43:36,627 main.py:57] epoch 7592, training loss: 8298.47, average training loss: 7402.38, base loss: 16095.75
[INFO 2017-06-29 06:43:39,668 main.py:57] epoch 7593, training loss: 7475.84, average training loss: 7402.58, base loss: 16095.50
[INFO 2017-06-29 06:43:42,742 main.py:57] epoch 7594, training loss: 7532.69, average training loss: 7403.68, base loss: 16095.84
[INFO 2017-06-29 06:43:45,799 main.py:57] epoch 7595, training loss: 7326.11, average training loss: 7404.39, base loss: 16096.21
[INFO 2017-06-29 06:43:48,829 main.py:57] epoch 7596, training loss: 6784.36, average training loss: 7404.01, base loss: 16095.41
[INFO 2017-06-29 06:43:51,839 main.py:57] epoch 7597, training loss: 7291.94, average training loss: 7403.71, base loss: 16095.10
[INFO 2017-06-29 06:43:54,865 main.py:57] epoch 7598, training loss: 7839.46, average training loss: 7404.34, base loss: 16095.15
[INFO 2017-06-29 06:43:57,925 main.py:57] epoch 7599, training loss: 7340.10, average training loss: 7403.63, base loss: 16095.16
[INFO 2017-06-29 06:43:57,925 main.py:59] epoch 7599, testing
[INFO 2017-06-29 06:44:10,653 main.py:104] average testing loss: 7944.98, base loss: 16642.21
[INFO 2017-06-29 06:44:10,653 main.py:105] improve_loss: 8697.23, improve_percent: 0.52
[INFO 2017-06-29 06:44:10,654 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:44:13,622 main.py:57] epoch 7600, training loss: 8105.48, average training loss: 7404.04, base loss: 16095.81
[INFO 2017-06-29 06:44:16,654 main.py:57] epoch 7601, training loss: 7044.85, average training loss: 7404.75, base loss: 16095.53
[INFO 2017-06-29 06:44:19,712 main.py:57] epoch 7602, training loss: 7339.49, average training loss: 7405.18, base loss: 16095.37
[INFO 2017-06-29 06:44:22,865 main.py:57] epoch 7603, training loss: 6822.75, average training loss: 7404.56, base loss: 16095.32
[INFO 2017-06-29 06:44:25,940 main.py:57] epoch 7604, training loss: 7703.01, average training loss: 7404.92, base loss: 16095.68
[INFO 2017-06-29 06:44:28,954 main.py:57] epoch 7605, training loss: 6837.77, average training loss: 7404.14, base loss: 16095.41
[INFO 2017-06-29 06:44:32,049 main.py:57] epoch 7606, training loss: 6941.55, average training loss: 7402.94, base loss: 16094.89
[INFO 2017-06-29 06:44:35,098 main.py:57] epoch 7607, training loss: 6641.17, average training loss: 7402.64, base loss: 16094.13
[INFO 2017-06-29 06:44:38,169 main.py:57] epoch 7608, training loss: 7227.37, average training loss: 7403.11, base loss: 16094.31
[INFO 2017-06-29 06:44:41,244 main.py:57] epoch 7609, training loss: 6497.64, average training loss: 7401.23, base loss: 16093.85
[INFO 2017-06-29 06:44:44,292 main.py:57] epoch 7610, training loss: 7145.02, average training loss: 7401.18, base loss: 16093.60
[INFO 2017-06-29 06:44:47,325 main.py:57] epoch 7611, training loss: 7037.76, average training loss: 7400.56, base loss: 16093.32
[INFO 2017-06-29 06:44:50,354 main.py:57] epoch 7612, training loss: 7530.25, average training loss: 7401.06, base loss: 16093.17
[INFO 2017-06-29 06:44:53,430 main.py:57] epoch 7613, training loss: 7246.79, average training loss: 7401.27, base loss: 16093.10
[INFO 2017-06-29 06:44:56,492 main.py:57] epoch 7614, training loss: 7782.34, average training loss: 7400.66, base loss: 16093.36
[INFO 2017-06-29 06:44:59,546 main.py:57] epoch 7615, training loss: 6327.55, average training loss: 7400.21, base loss: 16092.88
[INFO 2017-06-29 06:45:02,628 main.py:57] epoch 7616, training loss: 7402.07, average training loss: 7400.85, base loss: 16092.50
[INFO 2017-06-29 06:45:05,625 main.py:57] epoch 7617, training loss: 7257.83, average training loss: 7401.62, base loss: 16092.34
[INFO 2017-06-29 06:45:08,695 main.py:57] epoch 7618, training loss: 7077.33, average training loss: 7401.00, base loss: 16091.91
[INFO 2017-06-29 06:45:11,764 main.py:57] epoch 7619, training loss: 7534.37, average training loss: 7400.32, base loss: 16092.06
[INFO 2017-06-29 06:45:14,791 main.py:57] epoch 7620, training loss: 6678.42, average training loss: 7399.83, base loss: 16091.68
[INFO 2017-06-29 06:45:17,905 main.py:57] epoch 7621, training loss: 6475.50, average training loss: 7399.66, base loss: 16091.53
[INFO 2017-06-29 06:45:20,941 main.py:57] epoch 7622, training loss: 7249.25, average training loss: 7400.07, base loss: 16091.69
[INFO 2017-06-29 06:45:24,019 main.py:57] epoch 7623, training loss: 8394.95, average training loss: 7401.94, base loss: 16091.40
[INFO 2017-06-29 06:45:27,064 main.py:57] epoch 7624, training loss: 7189.50, average training loss: 7401.89, base loss: 16091.32
[INFO 2017-06-29 06:45:30,083 main.py:57] epoch 7625, training loss: 7574.44, average training loss: 7402.46, base loss: 16091.33
[INFO 2017-06-29 06:45:33,158 main.py:57] epoch 7626, training loss: 6882.60, average training loss: 7402.35, base loss: 16091.40
[INFO 2017-06-29 06:45:36,288 main.py:57] epoch 7627, training loss: 6164.58, average training loss: 7401.87, base loss: 16091.06
[INFO 2017-06-29 06:45:39,321 main.py:57] epoch 7628, training loss: 8361.31, average training loss: 7403.43, base loss: 16091.87
[INFO 2017-06-29 06:45:42,301 main.py:57] epoch 7629, training loss: 7431.95, average training loss: 7404.35, base loss: 16091.91
[INFO 2017-06-29 06:45:45,310 main.py:57] epoch 7630, training loss: 7889.65, average training loss: 7404.57, base loss: 16091.74
[INFO 2017-06-29 06:45:48,365 main.py:57] epoch 7631, training loss: 7206.99, average training loss: 7404.45, base loss: 16091.70
[INFO 2017-06-29 06:45:51,345 main.py:57] epoch 7632, training loss: 7132.74, average training loss: 7404.40, base loss: 16091.34
[INFO 2017-06-29 06:45:54,331 main.py:57] epoch 7633, training loss: 6679.81, average training loss: 7403.96, base loss: 16090.89
[INFO 2017-06-29 06:45:57,444 main.py:57] epoch 7634, training loss: 7540.13, average training loss: 7404.76, base loss: 16091.56
[INFO 2017-06-29 06:46:00,510 main.py:57] epoch 7635, training loss: 6547.92, average training loss: 7403.96, base loss: 16091.22
[INFO 2017-06-29 06:46:03,582 main.py:57] epoch 7636, training loss: 6535.22, average training loss: 7402.66, base loss: 16090.86
[INFO 2017-06-29 06:46:06,681 main.py:57] epoch 7637, training loss: 8427.74, average training loss: 7402.92, base loss: 16091.30
[INFO 2017-06-29 06:46:09,755 main.py:57] epoch 7638, training loss: 6699.31, average training loss: 7400.93, base loss: 16091.13
[INFO 2017-06-29 06:46:12,816 main.py:57] epoch 7639, training loss: 7784.02, average training loss: 7400.71, base loss: 16091.30
[INFO 2017-06-29 06:46:15,819 main.py:57] epoch 7640, training loss: 7045.73, average training loss: 7400.28, base loss: 16090.94
[INFO 2017-06-29 06:46:18,845 main.py:57] epoch 7641, training loss: 6912.57, average training loss: 7399.67, base loss: 16090.62
[INFO 2017-06-29 06:46:21,874 main.py:57] epoch 7642, training loss: 7283.65, average training loss: 7399.95, base loss: 16090.71
[INFO 2017-06-29 06:46:24,945 main.py:57] epoch 7643, training loss: 6756.79, average training loss: 7399.63, base loss: 16090.59
[INFO 2017-06-29 06:46:27,961 main.py:57] epoch 7644, training loss: 8466.12, average training loss: 7400.35, base loss: 16090.88
[INFO 2017-06-29 06:46:30,995 main.py:57] epoch 7645, training loss: 7549.84, average training loss: 7400.35, base loss: 16090.88
[INFO 2017-06-29 06:46:34,079 main.py:57] epoch 7646, training loss: 7616.10, average training loss: 7400.09, base loss: 16091.00
[INFO 2017-06-29 06:46:37,139 main.py:57] epoch 7647, training loss: 7199.75, average training loss: 7399.73, base loss: 16090.95
[INFO 2017-06-29 06:46:40,169 main.py:57] epoch 7648, training loss: 7922.82, average training loss: 7400.58, base loss: 16091.68
[INFO 2017-06-29 06:46:43,217 main.py:57] epoch 7649, training loss: 8313.71, average training loss: 7401.65, base loss: 16092.34
[INFO 2017-06-29 06:46:46,290 main.py:57] epoch 7650, training loss: 6512.24, average training loss: 7401.04, base loss: 16091.64
[INFO 2017-06-29 06:46:49,405 main.py:57] epoch 7651, training loss: 6411.48, average training loss: 7399.88, base loss: 16090.93
[INFO 2017-06-29 06:46:52,413 main.py:57] epoch 7652, training loss: 7107.35, average training loss: 7399.73, base loss: 16091.10
[INFO 2017-06-29 06:46:55,479 main.py:57] epoch 7653, training loss: 7807.53, average training loss: 7401.22, base loss: 16091.43
[INFO 2017-06-29 06:46:58,607 main.py:57] epoch 7654, training loss: 7067.66, average training loss: 7400.35, base loss: 16091.15
[INFO 2017-06-29 06:47:01,705 main.py:57] epoch 7655, training loss: 7059.58, average training loss: 7399.61, base loss: 16091.07
[INFO 2017-06-29 06:47:04,767 main.py:57] epoch 7656, training loss: 7401.92, average training loss: 7400.06, base loss: 16091.07
[INFO 2017-06-29 06:47:07,870 main.py:57] epoch 7657, training loss: 6667.35, average training loss: 7399.23, base loss: 16090.77
[INFO 2017-06-29 06:47:10,950 main.py:57] epoch 7658, training loss: 7078.00, average training loss: 7397.41, base loss: 16090.53
[INFO 2017-06-29 06:47:14,010 main.py:57] epoch 7659, training loss: 6891.54, average training loss: 7396.32, base loss: 16090.42
[INFO 2017-06-29 06:47:17,066 main.py:57] epoch 7660, training loss: 6764.33, average training loss: 7395.67, base loss: 16090.81
[INFO 2017-06-29 06:47:20,103 main.py:57] epoch 7661, training loss: 7965.06, average training loss: 7395.77, base loss: 16091.00
[INFO 2017-06-29 06:47:23,190 main.py:57] epoch 7662, training loss: 6191.48, average training loss: 7394.30, base loss: 16090.57
[INFO 2017-06-29 06:47:26,271 main.py:57] epoch 7663, training loss: 6539.14, average training loss: 7392.95, base loss: 16090.11
[INFO 2017-06-29 06:47:29,323 main.py:57] epoch 7664, training loss: 6996.73, average training loss: 7392.33, base loss: 16090.40
[INFO 2017-06-29 06:47:32,316 main.py:57] epoch 7665, training loss: 7082.04, average training loss: 7391.20, base loss: 16090.57
[INFO 2017-06-29 06:47:35,451 main.py:57] epoch 7666, training loss: 7382.16, average training loss: 7392.26, base loss: 16090.85
[INFO 2017-06-29 06:47:38,536 main.py:57] epoch 7667, training loss: 8481.91, average training loss: 7393.68, base loss: 16091.62
[INFO 2017-06-29 06:47:41,630 main.py:57] epoch 7668, training loss: 9115.00, average training loss: 7396.41, base loss: 16092.16
[INFO 2017-06-29 06:47:44,670 main.py:57] epoch 7669, training loss: 7146.14, average training loss: 7396.29, base loss: 16091.97
[INFO 2017-06-29 06:47:47,760 main.py:57] epoch 7670, training loss: 6852.27, average training loss: 7396.37, base loss: 16091.68
[INFO 2017-06-29 06:47:50,802 main.py:57] epoch 7671, training loss: 8199.88, average training loss: 7397.02, base loss: 16091.92
[INFO 2017-06-29 06:47:53,802 main.py:57] epoch 7672, training loss: 6649.93, average training loss: 7396.54, base loss: 16092.01
[INFO 2017-06-29 06:47:56,804 main.py:57] epoch 7673, training loss: 7313.65, average training loss: 7395.74, base loss: 16092.00
[INFO 2017-06-29 06:47:59,867 main.py:57] epoch 7674, training loss: 8423.26, average training loss: 7396.01, base loss: 16092.44
[INFO 2017-06-29 06:48:02,849 main.py:57] epoch 7675, training loss: 6591.99, average training loss: 7396.34, base loss: 16092.01
[INFO 2017-06-29 06:48:05,938 main.py:57] epoch 7676, training loss: 7072.48, average training loss: 7396.94, base loss: 16091.63
[INFO 2017-06-29 06:48:09,020 main.py:57] epoch 7677, training loss: 7816.55, average training loss: 7398.08, base loss: 16091.82
[INFO 2017-06-29 06:48:11,989 main.py:57] epoch 7678, training loss: 7457.68, average training loss: 7397.74, base loss: 16091.32
[INFO 2017-06-29 06:48:15,041 main.py:57] epoch 7679, training loss: 7902.05, average training loss: 7397.52, base loss: 16091.13
[INFO 2017-06-29 06:48:18,186 main.py:57] epoch 7680, training loss: 6852.66, average training loss: 7397.39, base loss: 16090.98
[INFO 2017-06-29 06:48:21,220 main.py:57] epoch 7681, training loss: 8136.03, average training loss: 7397.03, base loss: 16091.51
[INFO 2017-06-29 06:48:24,280 main.py:57] epoch 7682, training loss: 6829.98, average training loss: 7396.80, base loss: 16091.15
[INFO 2017-06-29 06:48:27,364 main.py:57] epoch 7683, training loss: 7081.66, average training loss: 7396.97, base loss: 16090.70
[INFO 2017-06-29 06:48:30,466 main.py:57] epoch 7684, training loss: 7021.12, average training loss: 7396.92, base loss: 16090.36
[INFO 2017-06-29 06:48:33,527 main.py:57] epoch 7685, training loss: 6305.46, average training loss: 7395.99, base loss: 16089.69
[INFO 2017-06-29 06:48:36,521 main.py:57] epoch 7686, training loss: 7057.75, average training loss: 7396.48, base loss: 16089.42
[INFO 2017-06-29 06:48:39,559 main.py:57] epoch 7687, training loss: 7529.56, average training loss: 7397.70, base loss: 16089.76
[INFO 2017-06-29 06:48:42,605 main.py:57] epoch 7688, training loss: 7702.32, average training loss: 7398.24, base loss: 16090.11
[INFO 2017-06-29 06:48:45,697 main.py:57] epoch 7689, training loss: 8560.55, average training loss: 7398.60, base loss: 16090.85
[INFO 2017-06-29 06:48:48,741 main.py:57] epoch 7690, training loss: 8012.27, average training loss: 7399.18, base loss: 16090.86
[INFO 2017-06-29 06:48:51,859 main.py:57] epoch 7691, training loss: 7038.29, average training loss: 7398.38, base loss: 16090.81
[INFO 2017-06-29 06:48:54,880 main.py:57] epoch 7692, training loss: 7752.29, average training loss: 7399.41, base loss: 16091.12
[INFO 2017-06-29 06:48:57,870 main.py:57] epoch 7693, training loss: 8113.45, average training loss: 7400.16, base loss: 16091.34
[INFO 2017-06-29 06:49:00,932 main.py:57] epoch 7694, training loss: 6928.52, average training loss: 7398.43, base loss: 16090.34
[INFO 2017-06-29 06:49:03,984 main.py:57] epoch 7695, training loss: 8337.18, average training loss: 7398.84, base loss: 16090.35
[INFO 2017-06-29 06:49:07,089 main.py:57] epoch 7696, training loss: 6738.16, average training loss: 7398.28, base loss: 16089.98
[INFO 2017-06-29 06:49:10,138 main.py:57] epoch 7697, training loss: 8255.93, average training loss: 7399.41, base loss: 16090.22
[INFO 2017-06-29 06:49:13,206 main.py:57] epoch 7698, training loss: 7584.67, average training loss: 7400.13, base loss: 16090.45
[INFO 2017-06-29 06:49:16,238 main.py:57] epoch 7699, training loss: 6937.67, average training loss: 7400.37, base loss: 16090.32
[INFO 2017-06-29 06:49:16,238 main.py:59] epoch 7699, testing
[INFO 2017-06-29 06:49:28,883 main.py:104] average testing loss: 8101.25, base loss: 16956.99
[INFO 2017-06-29 06:49:28,884 main.py:105] improve_loss: 8855.73, improve_percent: 0.52
[INFO 2017-06-29 06:49:28,885 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:49:31,961 main.py:57] epoch 7700, training loss: 6968.56, average training loss: 7399.59, base loss: 16090.12
[INFO 2017-06-29 06:49:35,034 main.py:57] epoch 7701, training loss: 6260.85, average training loss: 7397.56, base loss: 16089.53
[INFO 2017-06-29 06:49:38,147 main.py:57] epoch 7702, training loss: 7355.67, average training loss: 7397.08, base loss: 16089.32
[INFO 2017-06-29 06:49:41,184 main.py:57] epoch 7703, training loss: 6734.85, average training loss: 7395.86, base loss: 16088.89
[INFO 2017-06-29 06:49:44,270 main.py:57] epoch 7704, training loss: 7001.21, average training loss: 7395.45, base loss: 16088.97
[INFO 2017-06-29 06:49:47,261 main.py:57] epoch 7705, training loss: 8567.24, average training loss: 7397.12, base loss: 16089.08
[INFO 2017-06-29 06:49:50,288 main.py:57] epoch 7706, training loss: 8407.78, average training loss: 7397.28, base loss: 16089.17
[INFO 2017-06-29 06:49:53,332 main.py:57] epoch 7707, training loss: 8552.23, average training loss: 7398.72, base loss: 16089.64
[INFO 2017-06-29 06:49:56,365 main.py:57] epoch 7708, training loss: 7007.13, average training loss: 7397.76, base loss: 16089.59
[INFO 2017-06-29 06:49:59,431 main.py:57] epoch 7709, training loss: 7334.72, average training loss: 7397.81, base loss: 16089.48
[INFO 2017-06-29 06:50:02,535 main.py:57] epoch 7710, training loss: 7333.42, average training loss: 7398.12, base loss: 16089.36
[INFO 2017-06-29 06:50:05,635 main.py:57] epoch 7711, training loss: 7392.68, average training loss: 7398.31, base loss: 16089.21
[INFO 2017-06-29 06:50:08,705 main.py:57] epoch 7712, training loss: 7081.29, average training loss: 7396.26, base loss: 16089.20
[INFO 2017-06-29 06:50:11,701 main.py:57] epoch 7713, training loss: 7108.89, average training loss: 7396.72, base loss: 16089.13
[INFO 2017-06-29 06:50:14,719 main.py:57] epoch 7714, training loss: 7827.41, average training loss: 7395.95, base loss: 16089.78
[INFO 2017-06-29 06:50:17,838 main.py:57] epoch 7715, training loss: 7682.60, average training loss: 7396.68, base loss: 16090.61
[INFO 2017-06-29 06:50:20,865 main.py:57] epoch 7716, training loss: 7473.15, average training loss: 7396.40, base loss: 16090.59
[INFO 2017-06-29 06:50:23,926 main.py:57] epoch 7717, training loss: 6298.61, average training loss: 7395.61, base loss: 16090.11
[INFO 2017-06-29 06:50:27,017 main.py:57] epoch 7718, training loss: 8582.80, average training loss: 7396.21, base loss: 16090.48
[INFO 2017-06-29 06:50:30,173 main.py:57] epoch 7719, training loss: 7480.11, average training loss: 7397.05, base loss: 16090.46
[INFO 2017-06-29 06:50:33,277 main.py:57] epoch 7720, training loss: 7754.82, average training loss: 7397.74, base loss: 16090.97
[INFO 2017-06-29 06:50:36,310 main.py:57] epoch 7721, training loss: 7669.72, average training loss: 7398.68, base loss: 16091.77
[INFO 2017-06-29 06:50:39,450 main.py:57] epoch 7722, training loss: 7145.86, average training loss: 7399.22, base loss: 16091.32
[INFO 2017-06-29 06:50:42,493 main.py:57] epoch 7723, training loss: 8742.27, average training loss: 7401.90, base loss: 16091.35
[INFO 2017-06-29 06:50:45,520 main.py:57] epoch 7724, training loss: 8519.76, average training loss: 7402.87, base loss: 16091.60
[INFO 2017-06-29 06:50:48,574 main.py:57] epoch 7725, training loss: 7276.16, average training loss: 7402.78, base loss: 16091.30
[INFO 2017-06-29 06:50:51,602 main.py:57] epoch 7726, training loss: 7323.82, average training loss: 7402.91, base loss: 16091.05
[INFO 2017-06-29 06:50:54,630 main.py:57] epoch 7727, training loss: 7280.51, average training loss: 7402.15, base loss: 16090.42
[INFO 2017-06-29 06:50:57,584 main.py:57] epoch 7728, training loss: 7317.41, average training loss: 7402.74, base loss: 16090.45
[INFO 2017-06-29 06:51:00,579 main.py:57] epoch 7729, training loss: 7522.68, average training loss: 7402.59, base loss: 16090.33
[INFO 2017-06-29 06:51:03,704 main.py:57] epoch 7730, training loss: 7188.83, average training loss: 7402.78, base loss: 16089.83
[INFO 2017-06-29 06:51:06,704 main.py:57] epoch 7731, training loss: 8823.00, average training loss: 7404.19, base loss: 16089.98
[INFO 2017-06-29 06:51:09,710 main.py:57] epoch 7732, training loss: 8249.84, average training loss: 7404.41, base loss: 16090.50
[INFO 2017-06-29 06:51:12,742 main.py:57] epoch 7733, training loss: 7166.28, average training loss: 7404.10, base loss: 16090.46
[INFO 2017-06-29 06:51:15,805 main.py:57] epoch 7734, training loss: 6992.05, average training loss: 7404.76, base loss: 16090.24
[INFO 2017-06-29 06:51:18,879 main.py:57] epoch 7735, training loss: 7114.29, average training loss: 7404.63, base loss: 16090.19
[INFO 2017-06-29 06:51:21,879 main.py:57] epoch 7736, training loss: 8498.69, average training loss: 7406.09, base loss: 16091.06
[INFO 2017-06-29 06:51:24,935 main.py:57] epoch 7737, training loss: 6912.36, average training loss: 7406.53, base loss: 16090.98
[INFO 2017-06-29 06:51:28,042 main.py:57] epoch 7738, training loss: 7830.13, average training loss: 7407.38, base loss: 16091.22
[INFO 2017-06-29 06:51:31,095 main.py:57] epoch 7739, training loss: 7637.89, average training loss: 7407.17, base loss: 16091.56
[INFO 2017-06-29 06:51:34,182 main.py:57] epoch 7740, training loss: 6930.01, average training loss: 7406.57, base loss: 16091.29
[INFO 2017-06-29 06:51:37,248 main.py:57] epoch 7741, training loss: 7690.60, average training loss: 7407.28, base loss: 16091.53
[INFO 2017-06-29 06:51:40,278 main.py:57] epoch 7742, training loss: 7166.77, average training loss: 7407.68, base loss: 16091.74
[INFO 2017-06-29 06:51:43,312 main.py:57] epoch 7743, training loss: 7561.30, average training loss: 7408.70, base loss: 16091.98
[INFO 2017-06-29 06:51:46,346 main.py:57] epoch 7744, training loss: 7372.00, average training loss: 7408.41, base loss: 16091.94
[INFO 2017-06-29 06:51:49,416 main.py:57] epoch 7745, training loss: 7122.36, average training loss: 7408.37, base loss: 16091.79
[INFO 2017-06-29 06:51:52,488 main.py:57] epoch 7746, training loss: 7392.85, average training loss: 7408.33, base loss: 16091.87
[INFO 2017-06-29 06:51:55,587 main.py:57] epoch 7747, training loss: 7002.21, average training loss: 7407.91, base loss: 16091.80
[INFO 2017-06-29 06:51:58,677 main.py:57] epoch 7748, training loss: 8584.44, average training loss: 7408.69, base loss: 16091.94
[INFO 2017-06-29 06:52:01,666 main.py:57] epoch 7749, training loss: 7531.91, average training loss: 7408.58, base loss: 16091.67
[INFO 2017-06-29 06:52:04,780 main.py:57] epoch 7750, training loss: 6991.60, average training loss: 7408.96, base loss: 16091.01
[INFO 2017-06-29 06:52:07,863 main.py:57] epoch 7751, training loss: 8080.78, average training loss: 7409.67, base loss: 16090.88
[INFO 2017-06-29 06:52:10,921 main.py:57] epoch 7752, training loss: 7304.67, average training loss: 7410.14, base loss: 16090.98
[INFO 2017-06-29 06:52:13,980 main.py:57] epoch 7753, training loss: 6745.08, average training loss: 7409.51, base loss: 16090.67
[INFO 2017-06-29 06:52:16,985 main.py:57] epoch 7754, training loss: 7713.16, average training loss: 7409.94, base loss: 16091.28
[INFO 2017-06-29 06:52:19,998 main.py:57] epoch 7755, training loss: 7214.94, average training loss: 7409.99, base loss: 16091.31
[INFO 2017-06-29 06:52:23,043 main.py:57] epoch 7756, training loss: 7399.64, average training loss: 7410.58, base loss: 16091.10
[INFO 2017-06-29 06:52:26,043 main.py:57] epoch 7757, training loss: 7723.54, average training loss: 7410.96, base loss: 16091.46
[INFO 2017-06-29 06:52:29,060 main.py:57] epoch 7758, training loss: 8378.96, average training loss: 7411.03, base loss: 16092.24
[INFO 2017-06-29 06:52:32,139 main.py:57] epoch 7759, training loss: 8361.57, average training loss: 7412.64, base loss: 16092.87
[INFO 2017-06-29 06:52:35,267 main.py:57] epoch 7760, training loss: 8446.25, average training loss: 7412.22, base loss: 16093.22
[INFO 2017-06-29 06:52:38,313 main.py:57] epoch 7761, training loss: 7620.05, average training loss: 7412.11, base loss: 16092.55
[INFO 2017-06-29 06:52:41,407 main.py:57] epoch 7762, training loss: 9132.67, average training loss: 7413.74, base loss: 16093.17
[INFO 2017-06-29 06:52:44,476 main.py:57] epoch 7763, training loss: 6805.11, average training loss: 7413.95, base loss: 16092.70
[INFO 2017-06-29 06:52:47,508 main.py:57] epoch 7764, training loss: 7150.64, average training loss: 7413.01, base loss: 16092.54
[INFO 2017-06-29 06:52:50,572 main.py:57] epoch 7765, training loss: 6471.54, average training loss: 7411.84, base loss: 16092.38
[INFO 2017-06-29 06:52:53,602 main.py:57] epoch 7766, training loss: 7782.32, average training loss: 7412.65, base loss: 16092.58
[INFO 2017-06-29 06:52:56,637 main.py:57] epoch 7767, training loss: 6576.03, average training loss: 7412.43, base loss: 16092.49
[INFO 2017-06-29 06:52:59,708 main.py:57] epoch 7768, training loss: 6816.52, average training loss: 7411.58, base loss: 16091.93
[INFO 2017-06-29 06:53:02,723 main.py:57] epoch 7769, training loss: 7552.20, average training loss: 7412.43, base loss: 16092.31
[INFO 2017-06-29 06:53:05,742 main.py:57] epoch 7770, training loss: 7110.93, average training loss: 7412.56, base loss: 16092.23
[INFO 2017-06-29 06:53:08,785 main.py:57] epoch 7771, training loss: 7561.49, average training loss: 7412.57, base loss: 16092.50
[INFO 2017-06-29 06:53:11,830 main.py:57] epoch 7772, training loss: 7021.61, average training loss: 7412.01, base loss: 16092.35
[INFO 2017-06-29 06:53:14,878 main.py:57] epoch 7773, training loss: 7573.83, average training loss: 7412.75, base loss: 16092.23
[INFO 2017-06-29 06:53:17,988 main.py:57] epoch 7774, training loss: 7667.61, average training loss: 7412.67, base loss: 16092.31
[INFO 2017-06-29 06:53:21,044 main.py:57] epoch 7775, training loss: 6960.25, average training loss: 7412.47, base loss: 16092.26
[INFO 2017-06-29 06:53:24,032 main.py:57] epoch 7776, training loss: 6998.40, average training loss: 7411.94, base loss: 16092.01
[INFO 2017-06-29 06:53:27,046 main.py:57] epoch 7777, training loss: 8262.88, average training loss: 7412.48, base loss: 16092.57
[INFO 2017-06-29 06:53:30,056 main.py:57] epoch 7778, training loss: 7506.89, average training loss: 7411.41, base loss: 16092.44
[INFO 2017-06-29 06:53:33,149 main.py:57] epoch 7779, training loss: 6824.73, average training loss: 7410.45, base loss: 16092.11
[INFO 2017-06-29 06:53:36,184 main.py:57] epoch 7780, training loss: 6983.77, average training loss: 7410.75, base loss: 16091.93
[INFO 2017-06-29 06:53:39,261 main.py:57] epoch 7781, training loss: 6795.55, average training loss: 7409.06, base loss: 16091.13
[INFO 2017-06-29 06:53:42,316 main.py:57] epoch 7782, training loss: 7104.19, average training loss: 7408.90, base loss: 16090.57
[INFO 2017-06-29 06:53:45,406 main.py:57] epoch 7783, training loss: 7474.04, average training loss: 7409.05, base loss: 16090.33
[INFO 2017-06-29 06:53:48,493 main.py:57] epoch 7784, training loss: 7269.05, average training loss: 7407.79, base loss: 16090.24
[INFO 2017-06-29 06:53:51,606 main.py:57] epoch 7785, training loss: 6643.07, average training loss: 7406.41, base loss: 16089.83
[INFO 2017-06-29 06:53:54,686 main.py:57] epoch 7786, training loss: 6754.26, average training loss: 7406.39, base loss: 16089.50
[INFO 2017-06-29 06:53:57,751 main.py:57] epoch 7787, training loss: 6711.15, average training loss: 7406.80, base loss: 16088.94
[INFO 2017-06-29 06:54:00,773 main.py:57] epoch 7788, training loss: 7836.19, average training loss: 7407.54, base loss: 16089.07
[INFO 2017-06-29 06:54:03,889 main.py:57] epoch 7789, training loss: 6709.50, average training loss: 7406.92, base loss: 16088.85
[INFO 2017-06-29 06:54:06,915 main.py:57] epoch 7790, training loss: 6502.58, average training loss: 7406.06, base loss: 16088.74
[INFO 2017-06-29 06:54:10,016 main.py:57] epoch 7791, training loss: 6391.40, average training loss: 7404.37, base loss: 16088.69
[INFO 2017-06-29 06:54:13,076 main.py:57] epoch 7792, training loss: 8796.87, average training loss: 7405.43, base loss: 16089.40
[INFO 2017-06-29 06:54:16,098 main.py:57] epoch 7793, training loss: 7666.47, average training loss: 7405.40, base loss: 16089.74
[INFO 2017-06-29 06:54:19,150 main.py:57] epoch 7794, training loss: 7242.79, average training loss: 7404.91, base loss: 16089.63
[INFO 2017-06-29 06:54:22,211 main.py:57] epoch 7795, training loss: 6984.29, average training loss: 7404.83, base loss: 16089.40
[INFO 2017-06-29 06:54:25,308 main.py:57] epoch 7796, training loss: 6902.69, average training loss: 7404.56, base loss: 16089.44
[INFO 2017-06-29 06:54:28,389 main.py:57] epoch 7797, training loss: 8281.21, average training loss: 7404.81, base loss: 16089.78
[INFO 2017-06-29 06:54:31,450 main.py:57] epoch 7798, training loss: 7272.50, average training loss: 7404.96, base loss: 16089.82
[INFO 2017-06-29 06:54:34,554 main.py:57] epoch 7799, training loss: 5861.93, average training loss: 7403.30, base loss: 16089.15
[INFO 2017-06-29 06:54:34,555 main.py:59] epoch 7799, testing
[INFO 2017-06-29 06:54:47,268 main.py:104] average testing loss: 8246.22, base loss: 17150.23
[INFO 2017-06-29 06:54:47,268 main.py:105] improve_loss: 8904.01, improve_percent: 0.52
[INFO 2017-06-29 06:54:47,269 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 06:54:50,394 main.py:57] epoch 7800, training loss: 7204.86, average training loss: 7403.20, base loss: 16089.17
[INFO 2017-06-29 06:54:53,396 main.py:57] epoch 7801, training loss: 7636.77, average training loss: 7403.48, base loss: 16089.66
[INFO 2017-06-29 06:54:56,463 main.py:57] epoch 7802, training loss: 6628.06, average training loss: 7402.16, base loss: 16089.10
[INFO 2017-06-29 06:54:59,502 main.py:57] epoch 7803, training loss: 6513.67, average training loss: 7401.31, base loss: 16088.65
[INFO 2017-06-29 06:55:02,521 main.py:57] epoch 7804, training loss: 6637.54, average training loss: 7400.50, base loss: 16088.20
[INFO 2017-06-29 06:55:05,567 main.py:57] epoch 7805, training loss: 6866.12, average training loss: 7400.13, base loss: 16088.08
[INFO 2017-06-29 06:55:08,733 main.py:57] epoch 7806, training loss: 8315.98, average training loss: 7400.98, base loss: 16088.76
[INFO 2017-06-29 06:55:11,784 main.py:57] epoch 7807, training loss: 8955.62, average training loss: 7403.17, base loss: 16089.43
[INFO 2017-06-29 06:55:14,884 main.py:57] epoch 7808, training loss: 6889.57, average training loss: 7401.94, base loss: 16089.36
[INFO 2017-06-29 06:55:18,005 main.py:57] epoch 7809, training loss: 8200.75, average training loss: 7403.03, base loss: 16089.96
[INFO 2017-06-29 06:55:21,060 main.py:57] epoch 7810, training loss: 7371.82, average training loss: 7403.27, base loss: 16090.20
[INFO 2017-06-29 06:55:24,214 main.py:57] epoch 7811, training loss: 7922.99, average training loss: 7404.20, base loss: 16090.51
[INFO 2017-06-29 06:55:27,289 main.py:57] epoch 7812, training loss: 7513.58, average training loss: 7404.31, base loss: 16090.70
[INFO 2017-06-29 06:55:30,357 main.py:57] epoch 7813, training loss: 6821.58, average training loss: 7403.39, base loss: 16090.59
[INFO 2017-06-29 06:55:33,365 main.py:57] epoch 7814, training loss: 7568.31, average training loss: 7402.74, base loss: 16090.95
[INFO 2017-06-29 06:55:36,434 main.py:57] epoch 7815, training loss: 7608.93, average training loss: 7404.54, base loss: 16091.37
[INFO 2017-06-29 06:55:39,505 main.py:57] epoch 7816, training loss: 7078.99, average training loss: 7404.40, base loss: 16091.81
[INFO 2017-06-29 06:55:42,526 main.py:57] epoch 7817, training loss: 7652.82, average training loss: 7404.96, base loss: 16092.16
[INFO 2017-06-29 06:55:45,560 main.py:57] epoch 7818, training loss: 6892.16, average training loss: 7404.41, base loss: 16091.50
[INFO 2017-06-29 06:55:48,619 main.py:57] epoch 7819, training loss: 8108.29, average training loss: 7404.87, base loss: 16091.64
[INFO 2017-06-29 06:55:51,747 main.py:57] epoch 7820, training loss: 7684.44, average training loss: 7406.21, base loss: 16091.55
[INFO 2017-06-29 06:55:54,776 main.py:57] epoch 7821, training loss: 6559.58, average training loss: 7405.86, base loss: 16091.09
[INFO 2017-06-29 06:55:57,845 main.py:57] epoch 7822, training loss: 7547.19, average training loss: 7407.00, base loss: 16091.10
[INFO 2017-06-29 06:56:00,866 main.py:57] epoch 7823, training loss: 6831.46, average training loss: 7406.45, base loss: 16090.78
[INFO 2017-06-29 06:56:03,890 main.py:57] epoch 7824, training loss: 7440.01, average training loss: 7406.51, base loss: 16090.93
[INFO 2017-06-29 06:56:06,969 main.py:57] epoch 7825, training loss: 6926.22, average training loss: 7406.53, base loss: 16091.00
[INFO 2017-06-29 06:56:10,021 main.py:57] epoch 7826, training loss: 7591.86, average training loss: 7407.46, base loss: 16091.06
[INFO 2017-06-29 06:56:13,098 main.py:57] epoch 7827, training loss: 8260.54, average training loss: 7407.69, base loss: 16091.32
[INFO 2017-06-29 06:56:16,173 main.py:57] epoch 7828, training loss: 7777.87, average training loss: 7409.42, base loss: 16091.37
[INFO 2017-06-29 06:56:19,255 main.py:57] epoch 7829, training loss: 6967.58, average training loss: 7409.36, base loss: 16091.02
[INFO 2017-06-29 06:56:22,348 main.py:57] epoch 7830, training loss: 7063.53, average training loss: 7409.60, base loss: 16091.10
[INFO 2017-06-29 06:56:25,438 main.py:57] epoch 7831, training loss: 7113.59, average training loss: 7409.83, base loss: 16090.98
[INFO 2017-06-29 06:56:28,527 main.py:57] epoch 7832, training loss: 7289.87, average training loss: 7410.04, base loss: 16090.77
[INFO 2017-06-29 06:56:31,584 main.py:57] epoch 7833, training loss: 8513.42, average training loss: 7411.43, base loss: 16091.48
[INFO 2017-06-29 06:56:34,610 main.py:57] epoch 7834, training loss: 7520.91, average training loss: 7412.13, base loss: 16091.74
[INFO 2017-06-29 06:56:37,670 main.py:57] epoch 7835, training loss: 7306.67, average training loss: 7412.05, base loss: 16091.92
[INFO 2017-06-29 06:56:40,758 main.py:57] epoch 7836, training loss: 7593.02, average training loss: 7412.69, base loss: 16091.83
[INFO 2017-06-29 06:56:43,833 main.py:57] epoch 7837, training loss: 8677.94, average training loss: 7413.54, base loss: 16092.12
[INFO 2017-06-29 06:56:46,811 main.py:57] epoch 7838, training loss: 6986.79, average training loss: 7412.37, base loss: 16091.63
[INFO 2017-06-29 06:56:49,821 main.py:57] epoch 7839, training loss: 6735.87, average training loss: 7411.55, base loss: 16091.33
[INFO 2017-06-29 06:56:52,974 main.py:57] epoch 7840, training loss: 6816.41, average training loss: 7411.53, base loss: 16091.37
[INFO 2017-06-29 06:56:56,013 main.py:57] epoch 7841, training loss: 7918.07, average training loss: 7410.20, base loss: 16091.60
[INFO 2017-06-29 06:56:59,017 main.py:57] epoch 7842, training loss: 7667.75, average training loss: 7409.73, base loss: 16091.98
[INFO 2017-06-29 06:57:02,132 main.py:57] epoch 7843, training loss: 7570.88, average training loss: 7408.67, base loss: 16092.22
[INFO 2017-06-29 06:57:05,218 main.py:57] epoch 7844, training loss: 7290.81, average training loss: 7408.54, base loss: 16092.52
[INFO 2017-06-29 06:57:08,253 main.py:57] epoch 7845, training loss: 6807.13, average training loss: 7408.46, base loss: 16092.26
[INFO 2017-06-29 06:57:11,275 main.py:57] epoch 7846, training loss: 6855.59, average training loss: 7407.72, base loss: 16091.91
[INFO 2017-06-29 06:57:14,309 main.py:57] epoch 7847, training loss: 7313.09, average training loss: 7407.17, base loss: 16092.02
[INFO 2017-06-29 06:57:17,399 main.py:57] epoch 7848, training loss: 7343.06, average training loss: 7407.24, base loss: 16091.96
[INFO 2017-06-29 06:57:20,489 main.py:57] epoch 7849, training loss: 7130.18, average training loss: 7408.29, base loss: 16092.12
[INFO 2017-06-29 06:57:23,586 main.py:57] epoch 7850, training loss: 8154.10, average training loss: 7408.88, base loss: 16092.49
[INFO 2017-06-29 06:57:26,639 main.py:57] epoch 7851, training loss: 6891.35, average training loss: 7407.78, base loss: 16092.13
[INFO 2017-06-29 06:57:29,711 main.py:57] epoch 7852, training loss: 6974.26, average training loss: 7407.62, base loss: 16091.85
[INFO 2017-06-29 06:57:32,794 main.py:57] epoch 7853, training loss: 7961.43, average training loss: 7408.29, base loss: 16091.83
[INFO 2017-06-29 06:57:35,783 main.py:57] epoch 7854, training loss: 7468.15, average training loss: 7408.50, base loss: 16091.81
[INFO 2017-06-29 06:57:38,876 main.py:57] epoch 7855, training loss: 7339.15, average training loss: 7409.19, base loss: 16091.86
[INFO 2017-06-29 06:57:41,943 main.py:57] epoch 7856, training loss: 7027.41, average training loss: 7409.09, base loss: 16091.64
[INFO 2017-06-29 06:57:45,091 main.py:57] epoch 7857, training loss: 7915.04, average training loss: 7408.81, base loss: 16091.93
[INFO 2017-06-29 06:57:48,224 main.py:57] epoch 7858, training loss: 6824.94, average training loss: 7407.70, base loss: 16091.83
[INFO 2017-06-29 06:57:51,293 main.py:57] epoch 7859, training loss: 8067.54, average training loss: 7408.40, base loss: 16091.82
[INFO 2017-06-29 06:57:54,434 main.py:57] epoch 7860, training loss: 7887.61, average training loss: 7408.52, base loss: 16092.12
[INFO 2017-06-29 06:57:57,462 main.py:57] epoch 7861, training loss: 6761.70, average training loss: 7408.30, base loss: 16092.20
[INFO 2017-06-29 06:58:00,518 main.py:57] epoch 7862, training loss: 7645.47, average training loss: 7409.65, base loss: 16092.22
[INFO 2017-06-29 06:58:03,518 main.py:57] epoch 7863, training loss: 7471.76, average training loss: 7408.94, base loss: 16092.10
[INFO 2017-06-29 06:58:06,564 main.py:57] epoch 7864, training loss: 7234.42, average training loss: 7409.36, base loss: 16092.20
[INFO 2017-06-29 06:58:09,616 main.py:57] epoch 7865, training loss: 8161.48, average training loss: 7410.30, base loss: 16092.71
[INFO 2017-06-29 06:58:12,621 main.py:57] epoch 7866, training loss: 6658.01, average training loss: 7409.31, base loss: 16091.91
[INFO 2017-06-29 06:58:15,673 main.py:57] epoch 7867, training loss: 7350.82, average training loss: 7409.08, base loss: 16092.08
[INFO 2017-06-29 06:58:18,754 main.py:57] epoch 7868, training loss: 7367.64, average training loss: 7409.38, base loss: 16092.07
[INFO 2017-06-29 06:58:21,786 main.py:57] epoch 7869, training loss: 7991.13, average training loss: 7410.19, base loss: 16092.43
[INFO 2017-06-29 06:58:24,841 main.py:57] epoch 7870, training loss: 7877.37, average training loss: 7409.89, base loss: 16092.73
[INFO 2017-06-29 06:58:27,903 main.py:57] epoch 7871, training loss: 6845.65, average training loss: 7408.66, base loss: 16092.78
[INFO 2017-06-29 06:58:30,954 main.py:57] epoch 7872, training loss: 7814.51, average training loss: 7409.35, base loss: 16093.09
[INFO 2017-06-29 06:58:34,047 main.py:57] epoch 7873, training loss: 6667.76, average training loss: 7409.18, base loss: 16092.50
[INFO 2017-06-29 06:58:37,102 main.py:57] epoch 7874, training loss: 6951.43, average training loss: 7408.44, base loss: 16092.69
[INFO 2017-06-29 06:58:40,163 main.py:57] epoch 7875, training loss: 7401.20, average training loss: 7408.00, base loss: 16092.70
[INFO 2017-06-29 06:58:43,302 main.py:57] epoch 7876, training loss: 7153.19, average training loss: 7407.93, base loss: 16093.00
[INFO 2017-06-29 06:58:46,359 main.py:57] epoch 7877, training loss: 8229.89, average training loss: 7407.86, base loss: 16093.74
[INFO 2017-06-29 06:58:49,547 main.py:57] epoch 7878, training loss: 7462.11, average training loss: 7407.86, base loss: 16093.88
[INFO 2017-06-29 06:58:52,585 main.py:57] epoch 7879, training loss: 6084.82, average training loss: 7406.75, base loss: 16093.32
[INFO 2017-06-29 06:58:55,606 main.py:57] epoch 7880, training loss: 7397.77, average training loss: 7407.40, base loss: 16093.38
[INFO 2017-06-29 06:58:58,689 main.py:57] epoch 7881, training loss: 6209.98, average training loss: 7405.29, base loss: 16093.11
[INFO 2017-06-29 06:59:01,723 main.py:57] epoch 7882, training loss: 6773.34, average training loss: 7404.95, base loss: 16092.99
[INFO 2017-06-29 06:59:04,750 main.py:57] epoch 7883, training loss: 8122.38, average training loss: 7405.45, base loss: 16093.76
[INFO 2017-06-29 06:59:07,827 main.py:57] epoch 7884, training loss: 8252.22, average training loss: 7406.57, base loss: 16093.58
[INFO 2017-06-29 06:59:10,920 main.py:57] epoch 7885, training loss: 7526.46, average training loss: 7405.82, base loss: 16093.88
[INFO 2017-06-29 06:59:14,005 main.py:57] epoch 7886, training loss: 6954.67, average training loss: 7405.67, base loss: 16093.27
[INFO 2017-06-29 06:59:17,106 main.py:57] epoch 7887, training loss: 7226.60, average training loss: 7405.91, base loss: 16092.72
[INFO 2017-06-29 06:59:20,170 main.py:57] epoch 7888, training loss: 7540.63, average training loss: 7405.95, base loss: 16093.07
[INFO 2017-06-29 06:59:23,228 main.py:57] epoch 7889, training loss: 6982.59, average training loss: 7405.33, base loss: 16093.08
[INFO 2017-06-29 06:59:26,345 main.py:57] epoch 7890, training loss: 8183.67, average training loss: 7406.85, base loss: 16093.58
[INFO 2017-06-29 06:59:29,410 main.py:57] epoch 7891, training loss: 7041.48, average training loss: 7407.63, base loss: 16093.36
[INFO 2017-06-29 06:59:32,478 main.py:57] epoch 7892, training loss: 7483.07, average training loss: 7407.51, base loss: 16093.60
[INFO 2017-06-29 06:59:35,556 main.py:57] epoch 7893, training loss: 7191.83, average training loss: 7407.81, base loss: 16093.67
[INFO 2017-06-29 06:59:38,660 main.py:57] epoch 7894, training loss: 6975.18, average training loss: 7407.18, base loss: 16093.35
[INFO 2017-06-29 06:59:41,760 main.py:57] epoch 7895, training loss: 7405.06, average training loss: 7407.97, base loss: 16093.35
[INFO 2017-06-29 06:59:44,832 main.py:57] epoch 7896, training loss: 6987.27, average training loss: 7408.34, base loss: 16093.43
[INFO 2017-06-29 06:59:47,905 main.py:57] epoch 7897, training loss: 7236.22, average training loss: 7407.63, base loss: 16093.10
[INFO 2017-06-29 06:59:50,896 main.py:57] epoch 7898, training loss: 7540.45, average training loss: 7407.70, base loss: 16093.37
[INFO 2017-06-29 06:59:53,974 main.py:57] epoch 7899, training loss: 6240.70, average training loss: 7406.91, base loss: 16092.79
[INFO 2017-06-29 06:59:53,974 main.py:59] epoch 7899, testing
[INFO 2017-06-29 07:00:06,541 main.py:104] average testing loss: 7667.73, base loss: 16319.14
[INFO 2017-06-29 07:00:06,541 main.py:105] improve_loss: 8651.41, improve_percent: 0.53
[INFO 2017-06-29 07:00:06,543 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 07:00:06,581 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:00:09,592 main.py:57] epoch 7900, training loss: 8247.35, average training loss: 7408.13, base loss: 16093.56
[INFO 2017-06-29 07:00:12,614 main.py:57] epoch 7901, training loss: 7701.70, average training loss: 7408.34, base loss: 16093.93
[INFO 2017-06-29 07:00:15,650 main.py:57] epoch 7902, training loss: 7896.40, average training loss: 7408.95, base loss: 16094.39
[INFO 2017-06-29 07:00:18,698 main.py:57] epoch 7903, training loss: 7408.75, average training loss: 7408.77, base loss: 16094.26
[INFO 2017-06-29 07:00:21,771 main.py:57] epoch 7904, training loss: 6697.55, average training loss: 7407.18, base loss: 16093.75
[INFO 2017-06-29 07:00:24,867 main.py:57] epoch 7905, training loss: 7496.63, average training loss: 7407.27, base loss: 16093.90
[INFO 2017-06-29 07:00:27,960 main.py:57] epoch 7906, training loss: 7586.39, average training loss: 7407.95, base loss: 16093.90
[INFO 2017-06-29 07:00:31,023 main.py:57] epoch 7907, training loss: 8036.98, average training loss: 7407.38, base loss: 16094.41
[INFO 2017-06-29 07:00:34,067 main.py:57] epoch 7908, training loss: 7637.42, average training loss: 7407.33, base loss: 16094.60
[INFO 2017-06-29 07:00:37,147 main.py:57] epoch 7909, training loss: 7692.73, average training loss: 7408.19, base loss: 16094.95
[INFO 2017-06-29 07:00:40,206 main.py:57] epoch 7910, training loss: 6754.93, average training loss: 7408.10, base loss: 16094.64
[INFO 2017-06-29 07:00:43,252 main.py:57] epoch 7911, training loss: 6824.99, average training loss: 7407.94, base loss: 16094.32
[INFO 2017-06-29 07:00:46,367 main.py:57] epoch 7912, training loss: 7442.17, average training loss: 7408.58, base loss: 16094.13
[INFO 2017-06-29 07:00:49,489 main.py:57] epoch 7913, training loss: 7457.45, average training loss: 7408.10, base loss: 16094.13
[INFO 2017-06-29 07:00:52,491 main.py:57] epoch 7914, training loss: 6660.34, average training loss: 7407.39, base loss: 16093.83
[INFO 2017-06-29 07:00:55,587 main.py:57] epoch 7915, training loss: 6651.09, average training loss: 7407.06, base loss: 16093.68
[INFO 2017-06-29 07:00:58,588 main.py:57] epoch 7916, training loss: 7164.06, average training loss: 7406.61, base loss: 16093.21
[INFO 2017-06-29 07:01:01,546 main.py:57] epoch 7917, training loss: 8682.31, average training loss: 7407.83, base loss: 16094.20
[INFO 2017-06-29 07:01:04,586 main.py:57] epoch 7918, training loss: 7598.29, average training loss: 7407.72, base loss: 16094.27
[INFO 2017-06-29 07:01:07,609 main.py:57] epoch 7919, training loss: 7399.86, average training loss: 7407.15, base loss: 16094.22
[INFO 2017-06-29 07:01:10,678 main.py:57] epoch 7920, training loss: 7100.01, average training loss: 7407.10, base loss: 16093.99
[INFO 2017-06-29 07:01:13,796 main.py:57] epoch 7921, training loss: 7267.81, average training loss: 7406.08, base loss: 16093.94
[INFO 2017-06-29 07:01:16,779 main.py:57] epoch 7922, training loss: 7013.09, average training loss: 7406.91, base loss: 16093.36
[INFO 2017-06-29 07:01:19,810 main.py:57] epoch 7923, training loss: 8103.56, average training loss: 7407.36, base loss: 16093.29
[INFO 2017-06-29 07:01:22,800 main.py:57] epoch 7924, training loss: 7161.79, average training loss: 7407.47, base loss: 16092.93
[INFO 2017-06-29 07:01:25,871 main.py:57] epoch 7925, training loss: 7281.58, average training loss: 7407.26, base loss: 16092.72
[INFO 2017-06-29 07:01:28,857 main.py:57] epoch 7926, training loss: 7201.77, average training loss: 7406.60, base loss: 16092.44
[INFO 2017-06-29 07:01:31,860 main.py:57] epoch 7927, training loss: 7159.48, average training loss: 7405.95, base loss: 16092.14
[INFO 2017-06-29 07:01:34,887 main.py:57] epoch 7928, training loss: 6900.06, average training loss: 7404.37, base loss: 16092.03
[INFO 2017-06-29 07:01:37,975 main.py:57] epoch 7929, training loss: 7980.91, average training loss: 7405.13, base loss: 16092.37
[INFO 2017-06-29 07:01:41,039 main.py:57] epoch 7930, training loss: 6422.69, average training loss: 7403.77, base loss: 16091.66
[INFO 2017-06-29 07:01:44,072 main.py:57] epoch 7931, training loss: 7332.51, average training loss: 7404.04, base loss: 16091.62
[INFO 2017-06-29 07:01:47,077 main.py:57] epoch 7932, training loss: 6786.91, average training loss: 7402.64, base loss: 16090.87
[INFO 2017-06-29 07:01:50,117 main.py:57] epoch 7933, training loss: 7270.86, average training loss: 7401.34, base loss: 16090.69
[INFO 2017-06-29 07:01:53,144 main.py:57] epoch 7934, training loss: 7017.02, average training loss: 7401.15, base loss: 16090.52
[INFO 2017-06-29 07:01:56,173 main.py:57] epoch 7935, training loss: 8542.48, average training loss: 7402.24, base loss: 16090.90
[INFO 2017-06-29 07:01:59,298 main.py:57] epoch 7936, training loss: 7204.73, average training loss: 7400.78, base loss: 16091.18
[INFO 2017-06-29 07:02:02,385 main.py:57] epoch 7937, training loss: 7174.32, average training loss: 7399.79, base loss: 16091.15
[INFO 2017-06-29 07:02:05,402 main.py:57] epoch 7938, training loss: 7727.94, average training loss: 7401.39, base loss: 16090.99
[INFO 2017-06-29 07:02:08,475 main.py:57] epoch 7939, training loss: 7015.83, average training loss: 7400.31, base loss: 16090.83
[INFO 2017-06-29 07:02:11,492 main.py:57] epoch 7940, training loss: 7698.64, average training loss: 7400.52, base loss: 16091.20
[INFO 2017-06-29 07:02:14,493 main.py:57] epoch 7941, training loss: 7239.82, average training loss: 7401.06, base loss: 16090.84
[INFO 2017-06-29 07:02:17,592 main.py:57] epoch 7942, training loss: 7716.06, average training loss: 7401.40, base loss: 16090.66
[INFO 2017-06-29 07:02:20,680 main.py:57] epoch 7943, training loss: 8160.38, average training loss: 7401.89, base loss: 16091.23
[INFO 2017-06-29 07:02:23,783 main.py:57] epoch 7944, training loss: 7419.78, average training loss: 7402.35, base loss: 16091.38
[INFO 2017-06-29 07:02:26,791 main.py:57] epoch 7945, training loss: 7217.78, average training loss: 7402.00, base loss: 16091.01
[INFO 2017-06-29 07:02:29,907 main.py:57] epoch 7946, training loss: 7064.23, average training loss: 7402.02, base loss: 16090.85
[INFO 2017-06-29 07:02:32,993 main.py:57] epoch 7947, training loss: 7279.51, average training loss: 7401.75, base loss: 16091.01
[INFO 2017-06-29 07:02:36,009 main.py:57] epoch 7948, training loss: 7324.25, average training loss: 7401.53, base loss: 16090.76
[INFO 2017-06-29 07:02:39,054 main.py:57] epoch 7949, training loss: 8170.12, average training loss: 7402.62, base loss: 16091.03
[INFO 2017-06-29 07:02:42,115 main.py:57] epoch 7950, training loss: 6565.07, average training loss: 7400.88, base loss: 16090.31
[INFO 2017-06-29 07:02:45,238 main.py:57] epoch 7951, training loss: 7378.03, average training loss: 7399.90, base loss: 16089.89
[INFO 2017-06-29 07:02:48,297 main.py:57] epoch 7952, training loss: 8307.43, average training loss: 7400.96, base loss: 16089.86
[INFO 2017-06-29 07:02:51,312 main.py:57] epoch 7953, training loss: 6903.99, average training loss: 7400.15, base loss: 16089.49
[INFO 2017-06-29 07:02:54,252 main.py:57] epoch 7954, training loss: 7734.64, average training loss: 7400.23, base loss: 16089.67
[INFO 2017-06-29 07:02:57,252 main.py:57] epoch 7955, training loss: 7395.38, average training loss: 7400.34, base loss: 16089.90
[INFO 2017-06-29 07:03:00,356 main.py:57] epoch 7956, training loss: 7574.93, average training loss: 7400.87, base loss: 16090.51
[INFO 2017-06-29 07:03:03,444 main.py:57] epoch 7957, training loss: 7265.78, average training loss: 7401.71, base loss: 16090.37
[INFO 2017-06-29 07:03:06,479 main.py:57] epoch 7958, training loss: 8101.96, average training loss: 7402.89, base loss: 16090.48
[INFO 2017-06-29 07:03:09,599 main.py:57] epoch 7959, training loss: 6702.14, average training loss: 7402.05, base loss: 16090.12
[INFO 2017-06-29 07:03:12,651 main.py:57] epoch 7960, training loss: 6786.97, average training loss: 7400.69, base loss: 16089.87
[INFO 2017-06-29 07:03:15,718 main.py:57] epoch 7961, training loss: 7741.37, average training loss: 7402.31, base loss: 16089.86
[INFO 2017-06-29 07:03:18,808 main.py:57] epoch 7962, training loss: 7765.02, average training loss: 7402.83, base loss: 16090.52
[INFO 2017-06-29 07:03:21,797 main.py:57] epoch 7963, training loss: 7625.83, average training loss: 7404.35, base loss: 16090.99
[INFO 2017-06-29 07:03:24,831 main.py:57] epoch 7964, training loss: 7767.46, average training loss: 7405.43, base loss: 16090.90
[INFO 2017-06-29 07:03:27,893 main.py:57] epoch 7965, training loss: 7447.28, average training loss: 7403.81, base loss: 16091.07
[INFO 2017-06-29 07:03:31,005 main.py:57] epoch 7966, training loss: 7043.11, average training loss: 7403.31, base loss: 16091.19
[INFO 2017-06-29 07:03:34,065 main.py:57] epoch 7967, training loss: 6925.77, average training loss: 7402.10, base loss: 16091.13
[INFO 2017-06-29 07:03:37,174 main.py:57] epoch 7968, training loss: 6601.61, average training loss: 7401.31, base loss: 16090.96
[INFO 2017-06-29 07:03:40,251 main.py:57] epoch 7969, training loss: 7958.48, average training loss: 7402.15, base loss: 16091.47
[INFO 2017-06-29 07:03:43,305 main.py:57] epoch 7970, training loss: 6902.33, average training loss: 7401.97, base loss: 16091.63
[INFO 2017-06-29 07:03:46,420 main.py:57] epoch 7971, training loss: 8433.25, average training loss: 7402.59, base loss: 16092.19
[INFO 2017-06-29 07:03:49,470 main.py:57] epoch 7972, training loss: 6711.22, average training loss: 7402.19, base loss: 16091.79
[INFO 2017-06-29 07:03:52,543 main.py:57] epoch 7973, training loss: 8140.09, average training loss: 7403.40, base loss: 16091.92
[INFO 2017-06-29 07:03:55,591 main.py:57] epoch 7974, training loss: 6462.77, average training loss: 7400.92, base loss: 16091.35
[INFO 2017-06-29 07:03:58,636 main.py:57] epoch 7975, training loss: 8068.66, average training loss: 7400.40, base loss: 16091.48
[INFO 2017-06-29 07:04:01,750 main.py:57] epoch 7976, training loss: 8771.02, average training loss: 7401.34, base loss: 16091.87
[INFO 2017-06-29 07:04:04,784 main.py:57] epoch 7977, training loss: 7668.05, average training loss: 7401.81, base loss: 16091.73
[INFO 2017-06-29 07:04:07,796 main.py:57] epoch 7978, training loss: 6915.60, average training loss: 7398.73, base loss: 16091.87
[INFO 2017-06-29 07:04:10,855 main.py:57] epoch 7979, training loss: 7922.66, average training loss: 7399.25, base loss: 16092.06
[INFO 2017-06-29 07:04:13,833 main.py:57] epoch 7980, training loss: 7240.98, average training loss: 7399.15, base loss: 16092.06
[INFO 2017-06-29 07:04:16,866 main.py:57] epoch 7981, training loss: 7595.26, average training loss: 7398.97, base loss: 16092.73
[INFO 2017-06-29 07:04:19,838 main.py:57] epoch 7982, training loss: 6767.22, average training loss: 7399.01, base loss: 16092.23
[INFO 2017-06-29 07:04:22,979 main.py:57] epoch 7983, training loss: 7730.82, average training loss: 7398.35, base loss: 16092.39
[INFO 2017-06-29 07:04:26,041 main.py:57] epoch 7984, training loss: 8054.00, average training loss: 7399.30, base loss: 16092.68
[INFO 2017-06-29 07:04:29,107 main.py:57] epoch 7985, training loss: 7156.63, average training loss: 7399.14, base loss: 16092.24
[INFO 2017-06-29 07:04:32,236 main.py:57] epoch 7986, training loss: 7737.71, average training loss: 7399.60, base loss: 16092.64
[INFO 2017-06-29 07:04:35,345 main.py:57] epoch 7987, training loss: 6432.47, average training loss: 7398.76, base loss: 16092.23
[INFO 2017-06-29 07:04:38,366 main.py:57] epoch 7988, training loss: 7537.53, average training loss: 7398.92, base loss: 16091.57
[INFO 2017-06-29 07:04:41,495 main.py:57] epoch 7989, training loss: 7214.84, average training loss: 7398.42, base loss: 16090.72
[INFO 2017-06-29 07:04:44,506 main.py:57] epoch 7990, training loss: 7596.08, average training loss: 7398.76, base loss: 16090.68
[INFO 2017-06-29 07:04:47,504 main.py:57] epoch 7991, training loss: 8128.81, average training loss: 7398.76, base loss: 16090.87
[INFO 2017-06-29 07:04:50,519 main.py:57] epoch 7992, training loss: 7450.54, average training loss: 7399.00, base loss: 16090.94
[INFO 2017-06-29 07:04:53,624 main.py:57] epoch 7993, training loss: 7483.21, average training loss: 7399.69, base loss: 16091.19
[INFO 2017-06-29 07:04:56,656 main.py:57] epoch 7994, training loss: 6604.84, average training loss: 7398.17, base loss: 16090.91
[INFO 2017-06-29 07:04:59,632 main.py:57] epoch 7995, training loss: 7056.99, average training loss: 7398.64, base loss: 16090.58
[INFO 2017-06-29 07:05:02,636 main.py:57] epoch 7996, training loss: 7476.93, average training loss: 7400.00, base loss: 16090.65
[INFO 2017-06-29 07:05:05,759 main.py:57] epoch 7997, training loss: 7353.53, average training loss: 7400.21, base loss: 16090.68
[INFO 2017-06-29 07:05:08,819 main.py:57] epoch 7998, training loss: 6571.79, average training loss: 7398.65, base loss: 16090.21
[INFO 2017-06-29 07:05:11,906 main.py:57] epoch 7999, training loss: 6969.53, average training loss: 7398.50, base loss: 16090.06
[INFO 2017-06-29 07:05:11,907 main.py:59] epoch 7999, testing
[INFO 2017-06-29 07:05:24,641 main.py:104] average testing loss: 8063.22, base loss: 17031.24
[INFO 2017-06-29 07:05:24,641 main.py:105] improve_loss: 8968.02, improve_percent: 0.53
[INFO 2017-06-29 07:05:24,642 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:05:27,639 main.py:57] epoch 8000, training loss: 8661.86, average training loss: 7399.80, base loss: 16090.48
[INFO 2017-06-29 07:05:30,702 main.py:57] epoch 8001, training loss: 7010.58, average training loss: 7397.92, base loss: 16090.38
[INFO 2017-06-29 07:05:33,733 main.py:57] epoch 8002, training loss: 7349.32, average training loss: 7397.38, base loss: 16090.41
[INFO 2017-06-29 07:05:36,758 main.py:57] epoch 8003, training loss: 6587.97, average training loss: 7397.04, base loss: 16090.14
[INFO 2017-06-29 07:05:39,815 main.py:57] epoch 8004, training loss: 7409.35, average training loss: 7397.06, base loss: 16090.03
[INFO 2017-06-29 07:05:42,942 main.py:57] epoch 8005, training loss: 7101.40, average training loss: 7396.87, base loss: 16089.65
[INFO 2017-06-29 07:05:45,960 main.py:57] epoch 8006, training loss: 6541.75, average training loss: 7396.45, base loss: 16089.12
[INFO 2017-06-29 07:05:48,963 main.py:57] epoch 8007, training loss: 7429.86, average training loss: 7396.25, base loss: 16089.15
[INFO 2017-06-29 07:05:51,973 main.py:57] epoch 8008, training loss: 7450.49, average training loss: 7395.24, base loss: 16089.09
[INFO 2017-06-29 07:05:55,028 main.py:57] epoch 8009, training loss: 7922.98, average training loss: 7396.40, base loss: 16089.21
[INFO 2017-06-29 07:05:58,025 main.py:57] epoch 8010, training loss: 6314.65, average training loss: 7394.83, base loss: 16088.64
[INFO 2017-06-29 07:06:01,108 main.py:57] epoch 8011, training loss: 6794.72, average training loss: 7394.44, base loss: 16088.30
[INFO 2017-06-29 07:06:04,148 main.py:57] epoch 8012, training loss: 6952.27, average training loss: 7393.84, base loss: 16088.63
[INFO 2017-06-29 07:06:07,172 main.py:57] epoch 8013, training loss: 7176.39, average training loss: 7394.29, base loss: 16088.72
[INFO 2017-06-29 07:06:10,212 main.py:57] epoch 8014, training loss: 7395.66, average training loss: 7393.28, base loss: 16088.61
[INFO 2017-06-29 07:06:13,265 main.py:57] epoch 8015, training loss: 7507.11, average training loss: 7394.21, base loss: 16088.94
[INFO 2017-06-29 07:06:16,331 main.py:57] epoch 8016, training loss: 7497.19, average training loss: 7394.56, base loss: 16089.24
[INFO 2017-06-29 07:06:19,429 main.py:57] epoch 8017, training loss: 8285.08, average training loss: 7394.83, base loss: 16089.33
[INFO 2017-06-29 07:06:22,502 main.py:57] epoch 8018, training loss: 7288.09, average training loss: 7395.07, base loss: 16089.48
[INFO 2017-06-29 07:06:25,546 main.py:57] epoch 8019, training loss: 8166.18, average training loss: 7396.35, base loss: 16089.74
[INFO 2017-06-29 07:06:28,610 main.py:57] epoch 8020, training loss: 7979.39, average training loss: 7396.17, base loss: 16089.92
[INFO 2017-06-29 07:06:31,634 main.py:57] epoch 8021, training loss: 8963.06, average training loss: 7397.08, base loss: 16090.45
[INFO 2017-06-29 07:06:34,701 main.py:57] epoch 8022, training loss: 6680.03, average training loss: 7396.40, base loss: 16090.16
[INFO 2017-06-29 07:06:37,764 main.py:57] epoch 8023, training loss: 7592.49, average training loss: 7396.89, base loss: 16090.23
[INFO 2017-06-29 07:06:40,786 main.py:57] epoch 8024, training loss: 7054.45, average training loss: 7396.95, base loss: 16090.22
[INFO 2017-06-29 07:06:43,768 main.py:57] epoch 8025, training loss: 6800.86, average training loss: 7395.51, base loss: 16090.43
[INFO 2017-06-29 07:06:46,812 main.py:57] epoch 8026, training loss: 7278.33, average training loss: 7395.09, base loss: 16090.67
[INFO 2017-06-29 07:06:49,851 main.py:57] epoch 8027, training loss: 7530.64, average training loss: 7393.88, base loss: 16090.87
[INFO 2017-06-29 07:06:52,884 main.py:57] epoch 8028, training loss: 6628.71, average training loss: 7393.75, base loss: 16090.68
[INFO 2017-06-29 07:06:55,900 main.py:57] epoch 8029, training loss: 7141.60, average training loss: 7394.55, base loss: 16090.92
[INFO 2017-06-29 07:06:59,000 main.py:57] epoch 8030, training loss: 6798.17, average training loss: 7393.09, base loss: 16090.73
[INFO 2017-06-29 07:07:02,032 main.py:57] epoch 8031, training loss: 7499.50, average training loss: 7392.40, base loss: 16091.00
[INFO 2017-06-29 07:07:05,183 main.py:57] epoch 8032, training loss: 7164.67, average training loss: 7392.09, base loss: 16090.92
[INFO 2017-06-29 07:07:08,230 main.py:57] epoch 8033, training loss: 7216.20, average training loss: 7391.78, base loss: 16091.06
[INFO 2017-06-29 07:07:11,341 main.py:57] epoch 8034, training loss: 7748.00, average training loss: 7392.03, base loss: 16091.38
[INFO 2017-06-29 07:07:14,384 main.py:57] epoch 8035, training loss: 7581.36, average training loss: 7391.82, base loss: 16091.49
[INFO 2017-06-29 07:07:17,456 main.py:57] epoch 8036, training loss: 7809.10, average training loss: 7392.82, base loss: 16091.78
[INFO 2017-06-29 07:07:20,558 main.py:57] epoch 8037, training loss: 7085.52, average training loss: 7392.97, base loss: 16091.51
[INFO 2017-06-29 07:07:23,579 main.py:57] epoch 8038, training loss: 7135.52, average training loss: 7393.11, base loss: 16091.52
[INFO 2017-06-29 07:07:26,604 main.py:57] epoch 8039, training loss: 7676.31, average training loss: 7393.35, base loss: 16091.77
[INFO 2017-06-29 07:07:29,641 main.py:57] epoch 8040, training loss: 6848.38, average training loss: 7392.48, base loss: 16091.70
[INFO 2017-06-29 07:07:32,654 main.py:57] epoch 8041, training loss: 7785.26, average training loss: 7392.30, base loss: 16092.23
[INFO 2017-06-29 07:07:35,697 main.py:57] epoch 8042, training loss: 7039.55, average training loss: 7391.80, base loss: 16092.16
[INFO 2017-06-29 07:07:38,716 main.py:57] epoch 8043, training loss: 6800.83, average training loss: 7391.74, base loss: 16091.97
[INFO 2017-06-29 07:07:41,725 main.py:57] epoch 8044, training loss: 7482.47, average training loss: 7392.46, base loss: 16092.00
[INFO 2017-06-29 07:07:44,842 main.py:57] epoch 8045, training loss: 7486.65, average training loss: 7391.51, base loss: 16092.35
[INFO 2017-06-29 07:07:47,881 main.py:57] epoch 8046, training loss: 7490.36, average training loss: 7391.53, base loss: 16092.56
[INFO 2017-06-29 07:07:50,902 main.py:57] epoch 8047, training loss: 7119.46, average training loss: 7391.13, base loss: 16092.95
[INFO 2017-06-29 07:07:53,930 main.py:57] epoch 8048, training loss: 7869.15, average training loss: 7391.21, base loss: 16093.24
[INFO 2017-06-29 07:07:57,029 main.py:57] epoch 8049, training loss: 7917.38, average training loss: 7391.83, base loss: 16093.61
[INFO 2017-06-29 07:08:00,128 main.py:57] epoch 8050, training loss: 7411.96, average training loss: 7391.94, base loss: 16094.01
[INFO 2017-06-29 07:08:03,176 main.py:57] epoch 8051, training loss: 6891.44, average training loss: 7392.10, base loss: 16093.81
[INFO 2017-06-29 07:08:06,220 main.py:57] epoch 8052, training loss: 7473.49, average training loss: 7391.38, base loss: 16093.77
[INFO 2017-06-29 07:08:09,280 main.py:57] epoch 8053, training loss: 7220.39, average training loss: 7391.45, base loss: 16093.75
[INFO 2017-06-29 07:08:12,356 main.py:57] epoch 8054, training loss: 8181.01, average training loss: 7391.38, base loss: 16094.08
[INFO 2017-06-29 07:08:15,445 main.py:57] epoch 8055, training loss: 6764.29, average training loss: 7391.12, base loss: 16094.03
[INFO 2017-06-29 07:08:18,504 main.py:57] epoch 8056, training loss: 6682.00, average training loss: 7390.43, base loss: 16093.58
[INFO 2017-06-29 07:08:21,642 main.py:57] epoch 8057, training loss: 7489.44, average training loss: 7390.35, base loss: 16093.15
[INFO 2017-06-29 07:08:24,694 main.py:57] epoch 8058, training loss: 5959.83, average training loss: 7389.00, base loss: 16092.41
[INFO 2017-06-29 07:08:27,767 main.py:57] epoch 8059, training loss: 7598.03, average training loss: 7389.30, base loss: 16092.53
[INFO 2017-06-29 07:08:30,786 main.py:57] epoch 8060, training loss: 7130.35, average training loss: 7388.66, base loss: 16092.20
[INFO 2017-06-29 07:08:33,854 main.py:57] epoch 8061, training loss: 7070.41, average training loss: 7388.37, base loss: 16091.89
[INFO 2017-06-29 07:08:37,013 main.py:57] epoch 8062, training loss: 7372.13, average training loss: 7388.38, base loss: 16092.29
[INFO 2017-06-29 07:08:40,090 main.py:57] epoch 8063, training loss: 7302.49, average training loss: 7388.18, base loss: 16092.51
[INFO 2017-06-29 07:08:43,120 main.py:57] epoch 8064, training loss: 7542.66, average training loss: 7388.95, base loss: 16092.26
[INFO 2017-06-29 07:08:46,124 main.py:57] epoch 8065, training loss: 8058.63, average training loss: 7390.73, base loss: 16092.80
[INFO 2017-06-29 07:08:49,139 main.py:57] epoch 8066, training loss: 8113.50, average training loss: 7390.98, base loss: 16093.31
[INFO 2017-06-29 07:08:52,148 main.py:57] epoch 8067, training loss: 6267.14, average training loss: 7389.02, base loss: 16092.95
[INFO 2017-06-29 07:08:55,217 main.py:57] epoch 8068, training loss: 7378.19, average training loss: 7388.20, base loss: 16092.89
[INFO 2017-06-29 07:08:58,252 main.py:57] epoch 8069, training loss: 8043.36, average training loss: 7388.67, base loss: 16092.87
[INFO 2017-06-29 07:09:01,402 main.py:57] epoch 8070, training loss: 7088.94, average training loss: 7388.01, base loss: 16092.89
[INFO 2017-06-29 07:09:04,426 main.py:57] epoch 8071, training loss: 6877.99, average training loss: 7385.98, base loss: 16093.01
[INFO 2017-06-29 07:09:07,567 main.py:57] epoch 8072, training loss: 6454.35, average training loss: 7385.54, base loss: 16092.79
[INFO 2017-06-29 07:09:10,662 main.py:57] epoch 8073, training loss: 7533.28, average training loss: 7384.42, base loss: 16092.80
[INFO 2017-06-29 07:09:13,679 main.py:57] epoch 8074, training loss: 6905.15, average training loss: 7383.00, base loss: 16092.79
[INFO 2017-06-29 07:09:16,780 main.py:57] epoch 8075, training loss: 7756.77, average training loss: 7382.97, base loss: 16092.85
[INFO 2017-06-29 07:09:19,886 main.py:57] epoch 8076, training loss: 7983.98, average training loss: 7383.11, base loss: 16093.14
[INFO 2017-06-29 07:09:22,937 main.py:57] epoch 8077, training loss: 7298.58, average training loss: 7383.05, base loss: 16093.34
[INFO 2017-06-29 07:09:25,968 main.py:57] epoch 8078, training loss: 7113.68, average training loss: 7383.27, base loss: 16092.65
[INFO 2017-06-29 07:09:29,032 main.py:57] epoch 8079, training loss: 7618.65, average training loss: 7384.18, base loss: 16092.73
[INFO 2017-06-29 07:09:32,080 main.py:57] epoch 8080, training loss: 6740.69, average training loss: 7383.92, base loss: 16092.68
[INFO 2017-06-29 07:09:35,168 main.py:57] epoch 8081, training loss: 7181.83, average training loss: 7383.69, base loss: 16092.94
[INFO 2017-06-29 07:09:38,253 main.py:57] epoch 8082, training loss: 6600.73, average training loss: 7382.98, base loss: 16092.46
[INFO 2017-06-29 07:09:41,393 main.py:57] epoch 8083, training loss: 6809.95, average training loss: 7381.38, base loss: 16092.01
[INFO 2017-06-29 07:09:44,458 main.py:57] epoch 8084, training loss: 6872.86, average training loss: 7381.22, base loss: 16092.09
[INFO 2017-06-29 07:09:47,596 main.py:57] epoch 8085, training loss: 6874.10, average training loss: 7380.59, base loss: 16092.03
[INFO 2017-06-29 07:09:50,650 main.py:57] epoch 8086, training loss: 6897.11, average training loss: 7379.90, base loss: 16092.11
[INFO 2017-06-29 07:09:53,646 main.py:57] epoch 8087, training loss: 8428.84, average training loss: 7381.38, base loss: 16092.74
[INFO 2017-06-29 07:09:56,756 main.py:57] epoch 8088, training loss: 6865.62, average training loss: 7381.11, base loss: 16092.77
[INFO 2017-06-29 07:09:59,807 main.py:57] epoch 8089, training loss: 6964.42, average training loss: 7379.95, base loss: 16092.64
[INFO 2017-06-29 07:10:02,860 main.py:57] epoch 8090, training loss: 6351.55, average training loss: 7378.38, base loss: 16092.45
[INFO 2017-06-29 07:10:05,905 main.py:57] epoch 8091, training loss: 7343.48, average training loss: 7378.40, base loss: 16093.03
[INFO 2017-06-29 07:10:09,011 main.py:57] epoch 8092, training loss: 6952.01, average training loss: 7378.07, base loss: 16093.06
[INFO 2017-06-29 07:10:12,079 main.py:57] epoch 8093, training loss: 6841.82, average training loss: 7377.41, base loss: 16092.70
[INFO 2017-06-29 07:10:15,097 main.py:57] epoch 8094, training loss: 7041.20, average training loss: 7376.65, base loss: 16092.46
[INFO 2017-06-29 07:10:18,150 main.py:57] epoch 8095, training loss: 7675.20, average training loss: 7376.84, base loss: 16092.63
[INFO 2017-06-29 07:10:21,163 main.py:57] epoch 8096, training loss: 7023.91, average training loss: 7377.30, base loss: 16092.66
[INFO 2017-06-29 07:10:24,251 main.py:57] epoch 8097, training loss: 6806.29, average training loss: 7377.06, base loss: 16092.27
[INFO 2017-06-29 07:10:27,264 main.py:57] epoch 8098, training loss: 6784.25, average training loss: 7377.24, base loss: 16091.94
[INFO 2017-06-29 07:10:30,358 main.py:57] epoch 8099, training loss: 7952.10, average training loss: 7377.74, base loss: 16092.30
[INFO 2017-06-29 07:10:30,359 main.py:59] epoch 8099, testing
[INFO 2017-06-29 07:10:43,078 main.py:104] average testing loss: 8112.56, base loss: 16705.37
[INFO 2017-06-29 07:10:43,078 main.py:105] improve_loss: 8592.81, improve_percent: 0.51
[INFO 2017-06-29 07:10:43,079 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:10:46,152 main.py:57] epoch 8100, training loss: 7291.94, average training loss: 7377.31, base loss: 16092.44
[INFO 2017-06-29 07:10:49,182 main.py:57] epoch 8101, training loss: 6898.45, average training loss: 7377.40, base loss: 16092.32
[INFO 2017-06-29 07:10:52,302 main.py:57] epoch 8102, training loss: 7121.82, average training loss: 7377.34, base loss: 16092.80
[INFO 2017-06-29 07:10:55,312 main.py:57] epoch 8103, training loss: 7024.02, average training loss: 7377.29, base loss: 16092.78
[INFO 2017-06-29 07:10:58,374 main.py:57] epoch 8104, training loss: 7315.97, average training loss: 7376.37, base loss: 16092.71
[INFO 2017-06-29 07:11:01,378 main.py:57] epoch 8105, training loss: 6546.80, average training loss: 7375.51, base loss: 16092.07
[INFO 2017-06-29 07:11:04,476 main.py:57] epoch 8106, training loss: 8386.68, average training loss: 7377.70, base loss: 16092.70
[INFO 2017-06-29 07:11:07,469 main.py:57] epoch 8107, training loss: 6335.70, average training loss: 7376.99, base loss: 16092.24
[INFO 2017-06-29 07:11:10,556 main.py:57] epoch 8108, training loss: 7582.44, average training loss: 7377.76, base loss: 16092.39
[INFO 2017-06-29 07:11:13,650 main.py:57] epoch 8109, training loss: 7978.99, average training loss: 7378.29, base loss: 16092.76
[INFO 2017-06-29 07:11:16,759 main.py:57] epoch 8110, training loss: 7121.58, average training loss: 7376.91, base loss: 16092.57
[INFO 2017-06-29 07:11:19,788 main.py:57] epoch 8111, training loss: 7288.83, average training loss: 7378.04, base loss: 16092.46
[INFO 2017-06-29 07:11:22,769 main.py:57] epoch 8112, training loss: 7611.29, average training loss: 7378.31, base loss: 16092.81
[INFO 2017-06-29 07:11:25,930 main.py:57] epoch 8113, training loss: 7073.43, average training loss: 7378.42, base loss: 16092.77
[INFO 2017-06-29 07:11:29,011 main.py:57] epoch 8114, training loss: 8352.76, average training loss: 7380.35, base loss: 16093.08
[INFO 2017-06-29 07:11:32,120 main.py:57] epoch 8115, training loss: 7858.73, average training loss: 7379.35, base loss: 16093.37
[INFO 2017-06-29 07:11:35,134 main.py:57] epoch 8116, training loss: 7368.40, average training loss: 7378.92, base loss: 16093.50
[INFO 2017-06-29 07:11:38,186 main.py:57] epoch 8117, training loss: 6535.59, average training loss: 7378.56, base loss: 16093.24
[INFO 2017-06-29 07:11:41,217 main.py:57] epoch 8118, training loss: 7626.85, average training loss: 7378.76, base loss: 16093.20
[INFO 2017-06-29 07:11:44,267 main.py:57] epoch 8119, training loss: 6704.13, average training loss: 7377.27, base loss: 16092.73
[INFO 2017-06-29 07:11:47,298 main.py:57] epoch 8120, training loss: 6556.65, average training loss: 7375.82, base loss: 16092.03
[INFO 2017-06-29 07:11:50,419 main.py:57] epoch 8121, training loss: 7044.91, average training loss: 7374.83, base loss: 16091.92
[INFO 2017-06-29 07:11:53,492 main.py:57] epoch 8122, training loss: 6523.78, average training loss: 7373.14, base loss: 16091.70
[INFO 2017-06-29 07:11:56,530 main.py:57] epoch 8123, training loss: 7607.56, average training loss: 7372.67, base loss: 16091.59
[INFO 2017-06-29 07:11:59,498 main.py:57] epoch 8124, training loss: 7798.83, average training loss: 7373.86, base loss: 16092.01
[INFO 2017-06-29 07:12:02,556 main.py:57] epoch 8125, training loss: 8119.71, average training loss: 7374.77, base loss: 16092.36
[INFO 2017-06-29 07:12:05,611 main.py:57] epoch 8126, training loss: 7247.91, average training loss: 7376.26, base loss: 16092.34
[INFO 2017-06-29 07:12:08,656 main.py:57] epoch 8127, training loss: 7901.95, average training loss: 7377.06, base loss: 16092.52
[INFO 2017-06-29 07:12:11,713 main.py:57] epoch 8128, training loss: 6772.14, average training loss: 7376.04, base loss: 16092.31
[INFO 2017-06-29 07:12:14,821 main.py:57] epoch 8129, training loss: 7345.20, average training loss: 7376.43, base loss: 16092.59
[INFO 2017-06-29 07:12:17,874 main.py:57] epoch 8130, training loss: 7044.53, average training loss: 7375.97, base loss: 16092.81
[INFO 2017-06-29 07:12:20,961 main.py:57] epoch 8131, training loss: 8681.20, average training loss: 7376.60, base loss: 16093.14
[INFO 2017-06-29 07:12:24,040 main.py:57] epoch 8132, training loss: 7715.74, average training loss: 7377.46, base loss: 16093.37
[INFO 2017-06-29 07:12:27,136 main.py:57] epoch 8133, training loss: 6922.29, average training loss: 7377.07, base loss: 16093.37
[INFO 2017-06-29 07:12:30,169 main.py:57] epoch 8134, training loss: 7371.36, average training loss: 7377.71, base loss: 16093.35
[INFO 2017-06-29 07:12:33,308 main.py:57] epoch 8135, training loss: 6916.58, average training loss: 7377.79, base loss: 16093.21
[INFO 2017-06-29 07:12:36,394 main.py:57] epoch 8136, training loss: 9293.22, average training loss: 7380.15, base loss: 16093.87
[INFO 2017-06-29 07:12:39,432 main.py:57] epoch 8137, training loss: 6495.91, average training loss: 7379.58, base loss: 16093.42
[INFO 2017-06-29 07:12:42,519 main.py:57] epoch 8138, training loss: 7602.84, average training loss: 7379.71, base loss: 16093.87
[INFO 2017-06-29 07:12:45,549 main.py:57] epoch 8139, training loss: 7697.45, average training loss: 7380.50, base loss: 16094.14
[INFO 2017-06-29 07:12:48,662 main.py:57] epoch 8140, training loss: 6751.50, average training loss: 7379.93, base loss: 16093.83
[INFO 2017-06-29 07:12:51,701 main.py:57] epoch 8141, training loss: 8066.95, average training loss: 7380.05, base loss: 16094.23
[INFO 2017-06-29 07:12:54,749 main.py:57] epoch 8142, training loss: 7629.22, average training loss: 7379.35, base loss: 16094.35
[INFO 2017-06-29 07:12:57,854 main.py:57] epoch 8143, training loss: 7357.06, average training loss: 7379.34, base loss: 16094.14
[INFO 2017-06-29 07:13:00,864 main.py:57] epoch 8144, training loss: 7808.58, average training loss: 7380.72, base loss: 16094.43
[INFO 2017-06-29 07:13:03,924 main.py:57] epoch 8145, training loss: 7232.77, average training loss: 7380.01, base loss: 16094.49
[INFO 2017-06-29 07:13:07,110 main.py:57] epoch 8146, training loss: 7734.80, average training loss: 7378.56, base loss: 16094.82
[INFO 2017-06-29 07:13:10,179 main.py:57] epoch 8147, training loss: 7822.95, average training loss: 7376.45, base loss: 16094.95
[INFO 2017-06-29 07:13:13,299 main.py:57] epoch 8148, training loss: 6842.77, average training loss: 7375.39, base loss: 16094.79
[INFO 2017-06-29 07:13:16,369 main.py:57] epoch 8149, training loss: 7431.29, average training loss: 7375.58, base loss: 16094.99
[INFO 2017-06-29 07:13:19,385 main.py:57] epoch 8150, training loss: 6895.89, average training loss: 7375.44, base loss: 16095.23
[INFO 2017-06-29 07:13:22,483 main.py:57] epoch 8151, training loss: 7999.53, average training loss: 7375.95, base loss: 16095.60
[INFO 2017-06-29 07:13:25,543 main.py:57] epoch 8152, training loss: 6942.08, average training loss: 7375.35, base loss: 16095.51
[INFO 2017-06-29 07:13:28,675 main.py:57] epoch 8153, training loss: 7240.41, average training loss: 7374.64, base loss: 16095.68
[INFO 2017-06-29 07:13:31,691 main.py:57] epoch 8154, training loss: 6626.90, average training loss: 7373.14, base loss: 16095.70
[INFO 2017-06-29 07:13:34,779 main.py:57] epoch 8155, training loss: 7500.78, average training loss: 7373.02, base loss: 16095.88
[INFO 2017-06-29 07:13:37,785 main.py:57] epoch 8156, training loss: 6740.89, average training loss: 7371.63, base loss: 16095.49
[INFO 2017-06-29 07:13:40,812 main.py:57] epoch 8157, training loss: 7684.57, average training loss: 7372.12, base loss: 16095.81
[INFO 2017-06-29 07:13:43,881 main.py:57] epoch 8158, training loss: 6776.20, average training loss: 7371.23, base loss: 16095.80
[INFO 2017-06-29 07:13:46,895 main.py:57] epoch 8159, training loss: 6741.79, average training loss: 7369.04, base loss: 16095.59
[INFO 2017-06-29 07:13:49,930 main.py:57] epoch 8160, training loss: 7317.38, average training loss: 7367.98, base loss: 16095.75
[INFO 2017-06-29 07:13:53,016 main.py:57] epoch 8161, training loss: 7097.70, average training loss: 7367.51, base loss: 16095.39
[INFO 2017-06-29 07:13:56,111 main.py:57] epoch 8162, training loss: 6775.99, average training loss: 7367.82, base loss: 16095.46
[INFO 2017-06-29 07:13:59,201 main.py:57] epoch 8163, training loss: 6509.91, average training loss: 7367.64, base loss: 16095.28
[INFO 2017-06-29 07:14:02,230 main.py:57] epoch 8164, training loss: 8003.39, average training loss: 7368.48, base loss: 16095.51
[INFO 2017-06-29 07:14:05,319 main.py:57] epoch 8165, training loss: 7097.69, average training loss: 7368.23, base loss: 16095.10
[INFO 2017-06-29 07:14:08,355 main.py:57] epoch 8166, training loss: 6987.50, average training loss: 7366.85, base loss: 16094.84
[INFO 2017-06-29 07:14:11,387 main.py:57] epoch 8167, training loss: 8177.93, average training loss: 7367.93, base loss: 16095.07
[INFO 2017-06-29 07:14:14,400 main.py:57] epoch 8168, training loss: 7372.28, average training loss: 7366.51, base loss: 16095.47
[INFO 2017-06-29 07:14:17,403 main.py:57] epoch 8169, training loss: 6802.07, average training loss: 7365.93, base loss: 16095.21
[INFO 2017-06-29 07:14:20,409 main.py:57] epoch 8170, training loss: 6805.20, average training loss: 7366.12, base loss: 16094.64
[INFO 2017-06-29 07:14:23,571 main.py:57] epoch 8171, training loss: 8215.06, average training loss: 7367.59, base loss: 16094.97
[INFO 2017-06-29 07:14:26,619 main.py:57] epoch 8172, training loss: 6807.89, average training loss: 7367.01, base loss: 16094.60
[INFO 2017-06-29 07:14:29,704 main.py:57] epoch 8173, training loss: 6466.79, average training loss: 7366.95, base loss: 16094.18
[INFO 2017-06-29 07:14:32,809 main.py:57] epoch 8174, training loss: 6782.28, average training loss: 7366.81, base loss: 16093.92
[INFO 2017-06-29 07:14:35,817 main.py:57] epoch 8175, training loss: 6831.45, average training loss: 7366.51, base loss: 16093.87
[INFO 2017-06-29 07:14:38,837 main.py:57] epoch 8176, training loss: 7089.42, average training loss: 7365.68, base loss: 16093.98
[INFO 2017-06-29 07:14:41,853 main.py:57] epoch 8177, training loss: 6605.34, average training loss: 7364.85, base loss: 16093.96
[INFO 2017-06-29 07:14:44,950 main.py:57] epoch 8178, training loss: 8717.23, average training loss: 7365.61, base loss: 16094.27
[INFO 2017-06-29 07:14:48,002 main.py:57] epoch 8179, training loss: 7646.45, average training loss: 7365.45, base loss: 16094.00
[INFO 2017-06-29 07:14:51,076 main.py:57] epoch 8180, training loss: 7229.31, average training loss: 7364.70, base loss: 16093.93
[INFO 2017-06-29 07:14:54,175 main.py:57] epoch 8181, training loss: 8121.54, average training loss: 7365.93, base loss: 16094.39
[INFO 2017-06-29 07:14:57,289 main.py:57] epoch 8182, training loss: 7556.00, average training loss: 7366.86, base loss: 16095.01
[INFO 2017-06-29 07:15:00,379 main.py:57] epoch 8183, training loss: 7005.16, average training loss: 7366.86, base loss: 16095.14
[INFO 2017-06-29 07:15:03,549 main.py:57] epoch 8184, training loss: 8038.00, average training loss: 7366.67, base loss: 16095.30
[INFO 2017-06-29 07:15:06,599 main.py:57] epoch 8185, training loss: 6574.20, average training loss: 7366.03, base loss: 16094.98
[INFO 2017-06-29 07:15:09,576 main.py:57] epoch 8186, training loss: 6596.25, average training loss: 7364.24, base loss: 16094.61
[INFO 2017-06-29 07:15:12,693 main.py:57] epoch 8187, training loss: 7520.61, average training loss: 7362.87, base loss: 16094.78
[INFO 2017-06-29 07:15:15,706 main.py:57] epoch 8188, training loss: 7359.52, average training loss: 7363.33, base loss: 16094.59
[INFO 2017-06-29 07:15:18,733 main.py:57] epoch 8189, training loss: 7168.34, average training loss: 7363.53, base loss: 16094.61
[INFO 2017-06-29 07:15:21,790 main.py:57] epoch 8190, training loss: 7518.65, average training loss: 7363.17, base loss: 16094.52
[INFO 2017-06-29 07:15:24,842 main.py:57] epoch 8191, training loss: 6421.22, average training loss: 7362.78, base loss: 16094.22
[INFO 2017-06-29 07:15:27,844 main.py:57] epoch 8192, training loss: 7002.72, average training loss: 7362.30, base loss: 16094.05
[INFO 2017-06-29 07:15:31,017 main.py:57] epoch 8193, training loss: 7649.64, average training loss: 7362.04, base loss: 16094.10
[INFO 2017-06-29 07:15:34,062 main.py:57] epoch 8194, training loss: 8132.47, average training loss: 7362.19, base loss: 16094.32
[INFO 2017-06-29 07:15:37,188 main.py:57] epoch 8195, training loss: 6954.71, average training loss: 7362.23, base loss: 16094.03
[INFO 2017-06-29 07:15:40,190 main.py:57] epoch 8196, training loss: 6824.66, average training loss: 7361.09, base loss: 16093.85
[INFO 2017-06-29 07:15:43,314 main.py:57] epoch 8197, training loss: 7335.70, average training loss: 7360.65, base loss: 16093.82
[INFO 2017-06-29 07:15:46,330 main.py:57] epoch 8198, training loss: 6565.80, average training loss: 7359.52, base loss: 16093.64
[INFO 2017-06-29 07:15:49,396 main.py:57] epoch 8199, training loss: 7663.32, average training loss: 7359.89, base loss: 16093.69
[INFO 2017-06-29 07:15:49,397 main.py:59] epoch 8199, testing
[INFO 2017-06-29 07:16:01,820 main.py:104] average testing loss: 8019.73, base loss: 16362.64
[INFO 2017-06-29 07:16:01,820 main.py:105] improve_loss: 8342.91, improve_percent: 0.51
[INFO 2017-06-29 07:16:01,821 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:16:04,923 main.py:57] epoch 8200, training loss: 7591.85, average training loss: 7360.67, base loss: 16094.19
[INFO 2017-06-29 07:16:07,984 main.py:57] epoch 8201, training loss: 7618.88, average training loss: 7361.77, base loss: 16094.29
[INFO 2017-06-29 07:16:11,049 main.py:57] epoch 8202, training loss: 6835.25, average training loss: 7358.20, base loss: 16094.30
[INFO 2017-06-29 07:16:14,101 main.py:57] epoch 8203, training loss: 7692.77, average training loss: 7358.37, base loss: 16094.79
[INFO 2017-06-29 07:16:17,130 main.py:57] epoch 8204, training loss: 7316.76, average training loss: 7358.81, base loss: 16094.71
[INFO 2017-06-29 07:16:20,177 main.py:57] epoch 8205, training loss: 6839.58, average training loss: 7358.31, base loss: 16094.47
[INFO 2017-06-29 07:16:23,168 main.py:57] epoch 8206, training loss: 7346.34, average training loss: 7359.04, base loss: 16094.13
[INFO 2017-06-29 07:16:26,227 main.py:57] epoch 8207, training loss: 6925.57, average training loss: 7358.53, base loss: 16093.72
[INFO 2017-06-29 07:16:29,279 main.py:57] epoch 8208, training loss: 7442.92, average training loss: 7358.22, base loss: 16093.90
[INFO 2017-06-29 07:16:32,277 main.py:57] epoch 8209, training loss: 7109.02, average training loss: 7358.58, base loss: 16093.87
[INFO 2017-06-29 07:16:35,348 main.py:57] epoch 8210, training loss: 7901.32, average training loss: 7359.39, base loss: 16094.20
[INFO 2017-06-29 07:16:38,381 main.py:57] epoch 8211, training loss: 7345.09, average training loss: 7359.35, base loss: 16094.48
[INFO 2017-06-29 07:16:41,429 main.py:57] epoch 8212, training loss: 7260.24, average training loss: 7358.19, base loss: 16093.99
[INFO 2017-06-29 07:16:44,442 main.py:57] epoch 8213, training loss: 8192.52, average training loss: 7359.02, base loss: 16093.89
[INFO 2017-06-29 07:16:47,555 main.py:57] epoch 8214, training loss: 6902.46, average training loss: 7359.15, base loss: 16093.59
[INFO 2017-06-29 07:16:50,588 main.py:57] epoch 8215, training loss: 7197.60, average training loss: 7357.69, base loss: 16093.17
[INFO 2017-06-29 07:16:53,654 main.py:57] epoch 8216, training loss: 8099.66, average training loss: 7357.76, base loss: 16093.21
[INFO 2017-06-29 07:16:56,792 main.py:57] epoch 8217, training loss: 7031.03, average training loss: 7357.25, base loss: 16093.15
[INFO 2017-06-29 07:16:59,869 main.py:57] epoch 8218, training loss: 7239.42, average training loss: 7357.62, base loss: 16093.20
[INFO 2017-06-29 07:17:02,914 main.py:57] epoch 8219, training loss: 7037.44, average training loss: 7356.82, base loss: 16093.15
[INFO 2017-06-29 07:17:05,949 main.py:57] epoch 8220, training loss: 8030.83, average training loss: 7357.06, base loss: 16093.18
[INFO 2017-06-29 07:17:09,098 main.py:57] epoch 8221, training loss: 8335.27, average training loss: 7357.37, base loss: 16093.81
[INFO 2017-06-29 07:17:12,165 main.py:57] epoch 8222, training loss: 7241.02, average training loss: 7356.07, base loss: 16094.21
[INFO 2017-06-29 07:17:15,214 main.py:57] epoch 8223, training loss: 7704.07, average training loss: 7356.84, base loss: 16094.55
[INFO 2017-06-29 07:17:18,412 main.py:57] epoch 8224, training loss: 7650.44, average training loss: 7356.89, base loss: 16095.15
[INFO 2017-06-29 07:17:21,454 main.py:57] epoch 8225, training loss: 8110.35, average training loss: 7358.33, base loss: 16095.86
[INFO 2017-06-29 07:17:24,537 main.py:57] epoch 8226, training loss: 7771.53, average training loss: 7359.42, base loss: 16096.12
[INFO 2017-06-29 07:17:27,578 main.py:57] epoch 8227, training loss: 7031.48, average training loss: 7359.19, base loss: 16096.01
[INFO 2017-06-29 07:17:30,612 main.py:57] epoch 8228, training loss: 7685.40, average training loss: 7359.45, base loss: 16096.80
[INFO 2017-06-29 07:17:33,762 main.py:57] epoch 8229, training loss: 6898.68, average training loss: 7359.34, base loss: 16096.93
[INFO 2017-06-29 07:17:36,819 main.py:57] epoch 8230, training loss: 7527.94, average training loss: 7359.64, base loss: 16096.90
[INFO 2017-06-29 07:17:39,943 main.py:57] epoch 8231, training loss: 7071.18, average training loss: 7358.35, base loss: 16096.47
[INFO 2017-06-29 07:17:42,969 main.py:57] epoch 8232, training loss: 7650.89, average training loss: 7359.43, base loss: 16096.74
[INFO 2017-06-29 07:17:46,096 main.py:57] epoch 8233, training loss: 6924.95, average training loss: 7359.21, base loss: 16096.82
[INFO 2017-06-29 07:17:49,148 main.py:57] epoch 8234, training loss: 7364.99, average training loss: 7359.35, base loss: 16096.90
[INFO 2017-06-29 07:17:52,262 main.py:57] epoch 8235, training loss: 8177.09, average training loss: 7360.11, base loss: 16097.39
[INFO 2017-06-29 07:17:55,340 main.py:57] epoch 8236, training loss: 7401.63, average training loss: 7357.86, base loss: 16097.37
[INFO 2017-06-29 07:17:58,424 main.py:57] epoch 8237, training loss: 7401.27, average training loss: 7357.91, base loss: 16097.11
[INFO 2017-06-29 07:18:01,510 main.py:57] epoch 8238, training loss: 7191.13, average training loss: 7357.48, base loss: 16096.90
[INFO 2017-06-29 07:18:04,560 main.py:57] epoch 8239, training loss: 6942.52, average training loss: 7357.70, base loss: 16096.57
[INFO 2017-06-29 07:18:07,624 main.py:57] epoch 8240, training loss: 7373.96, average training loss: 7357.52, base loss: 16096.72
[INFO 2017-06-29 07:18:10,731 main.py:57] epoch 8241, training loss: 6761.47, average training loss: 7356.39, base loss: 16096.67
[INFO 2017-06-29 07:18:13,786 main.py:57] epoch 8242, training loss: 7227.16, average training loss: 7356.40, base loss: 16096.60
[INFO 2017-06-29 07:18:16,784 main.py:57] epoch 8243, training loss: 7747.61, average training loss: 7356.62, base loss: 16096.72
[INFO 2017-06-29 07:18:19,874 main.py:57] epoch 8244, training loss: 7362.43, average training loss: 7357.44, base loss: 16096.97
[INFO 2017-06-29 07:18:22,937 main.py:57] epoch 8245, training loss: 6855.72, average training loss: 7356.84, base loss: 16096.43
[INFO 2017-06-29 07:18:25,975 main.py:57] epoch 8246, training loss: 7042.14, average training loss: 7356.36, base loss: 16096.20
[INFO 2017-06-29 07:18:29,029 main.py:57] epoch 8247, training loss: 7209.16, average training loss: 7355.42, base loss: 16095.72
[INFO 2017-06-29 07:18:32,074 main.py:57] epoch 8248, training loss: 6538.70, average training loss: 7354.96, base loss: 16095.38
[INFO 2017-06-29 07:18:35,106 main.py:57] epoch 8249, training loss: 6653.63, average training loss: 7354.78, base loss: 16095.33
[INFO 2017-06-29 07:18:38,149 main.py:57] epoch 8250, training loss: 7143.44, average training loss: 7354.70, base loss: 16095.16
[INFO 2017-06-29 07:18:41,202 main.py:57] epoch 8251, training loss: 8756.04, average training loss: 7356.28, base loss: 16095.75
[INFO 2017-06-29 07:18:44,262 main.py:57] epoch 8252, training loss: 6509.26, average training loss: 7355.27, base loss: 16095.44
[INFO 2017-06-29 07:18:47,358 main.py:57] epoch 8253, training loss: 7272.50, average training loss: 7356.30, base loss: 16095.32
[INFO 2017-06-29 07:18:50,415 main.py:57] epoch 8254, training loss: 8257.09, average training loss: 7358.04, base loss: 16095.61
[INFO 2017-06-29 07:18:53,468 main.py:57] epoch 8255, training loss: 6790.26, average training loss: 7357.66, base loss: 16095.60
[INFO 2017-06-29 07:18:56,599 main.py:57] epoch 8256, training loss: 6850.80, average training loss: 7356.46, base loss: 16095.39
[INFO 2017-06-29 07:18:59,637 main.py:57] epoch 8257, training loss: 7681.94, average training loss: 7356.74, base loss: 16096.11
[INFO 2017-06-29 07:19:02,666 main.py:57] epoch 8258, training loss: 7332.17, average training loss: 7356.55, base loss: 16096.42
[INFO 2017-06-29 07:19:05,734 main.py:57] epoch 8259, training loss: 6801.73, average training loss: 7355.77, base loss: 16096.50
[INFO 2017-06-29 07:19:08,774 main.py:57] epoch 8260, training loss: 7175.45, average training loss: 7355.06, base loss: 16096.29
[INFO 2017-06-29 07:19:11,797 main.py:57] epoch 8261, training loss: 6954.11, average training loss: 7355.22, base loss: 16095.96
[INFO 2017-06-29 07:19:14,827 main.py:57] epoch 8262, training loss: 8810.84, average training loss: 7355.28, base loss: 16096.27
[INFO 2017-06-29 07:19:17,907 main.py:57] epoch 8263, training loss: 6800.56, average training loss: 7354.62, base loss: 16095.89
[INFO 2017-06-29 07:19:20,925 main.py:57] epoch 8264, training loss: 6583.06, average training loss: 7355.32, base loss: 16095.50
[INFO 2017-06-29 07:19:23,918 main.py:57] epoch 8265, training loss: 7590.57, average training loss: 7354.57, base loss: 16095.43
[INFO 2017-06-29 07:19:26,939 main.py:57] epoch 8266, training loss: 6784.40, average training loss: 7353.54, base loss: 16095.50
[INFO 2017-06-29 07:19:30,059 main.py:57] epoch 8267, training loss: 7177.66, average training loss: 7353.63, base loss: 16095.50
[INFO 2017-06-29 07:19:33,167 main.py:57] epoch 8268, training loss: 6777.48, average training loss: 7352.53, base loss: 16095.62
[INFO 2017-06-29 07:19:36,226 main.py:57] epoch 8269, training loss: 7479.88, average training loss: 7353.07, base loss: 16095.83
[INFO 2017-06-29 07:19:39,343 main.py:57] epoch 8270, training loss: 8295.08, average training loss: 7353.49, base loss: 16096.46
[INFO 2017-06-29 07:19:42,355 main.py:57] epoch 8271, training loss: 8323.56, average training loss: 7353.17, base loss: 16096.98
[INFO 2017-06-29 07:19:45,395 main.py:57] epoch 8272, training loss: 7149.07, average training loss: 7353.11, base loss: 16097.11
[INFO 2017-06-29 07:19:48,366 main.py:57] epoch 8273, training loss: 6397.10, average training loss: 7351.94, base loss: 16097.11
[INFO 2017-06-29 07:19:51,401 main.py:57] epoch 8274, training loss: 7539.76, average training loss: 7352.67, base loss: 16097.26
[INFO 2017-06-29 07:19:54,461 main.py:57] epoch 8275, training loss: 7566.13, average training loss: 7351.54, base loss: 16097.65
[INFO 2017-06-29 07:19:57,478 main.py:57] epoch 8276, training loss: 7500.78, average training loss: 7351.17, base loss: 16097.70
[INFO 2017-06-29 07:20:00,523 main.py:57] epoch 8277, training loss: 8506.22, average training loss: 7351.67, base loss: 16097.64
[INFO 2017-06-29 07:20:03,530 main.py:57] epoch 8278, training loss: 6162.36, average training loss: 7350.36, base loss: 16097.15
[INFO 2017-06-29 07:20:06,639 main.py:57] epoch 8279, training loss: 7746.61, average training loss: 7350.12, base loss: 16097.35
[INFO 2017-06-29 07:20:09,679 main.py:57] epoch 8280, training loss: 7056.44, average training loss: 7349.69, base loss: 16096.71
[INFO 2017-06-29 07:20:12,728 main.py:57] epoch 8281, training loss: 7087.32, average training loss: 7349.26, base loss: 16096.13
[INFO 2017-06-29 07:20:15,781 main.py:57] epoch 8282, training loss: 7189.32, average training loss: 7349.55, base loss: 16095.67
[INFO 2017-06-29 07:20:18,824 main.py:57] epoch 8283, training loss: 7103.42, average training loss: 7349.13, base loss: 16095.40
[INFO 2017-06-29 07:20:21,805 main.py:57] epoch 8284, training loss: 6484.32, average training loss: 7348.27, base loss: 16095.43
[INFO 2017-06-29 07:20:24,848 main.py:57] epoch 8285, training loss: 6678.31, average training loss: 7348.19, base loss: 16095.46
[INFO 2017-06-29 07:20:27,914 main.py:57] epoch 8286, training loss: 6166.87, average training loss: 7347.30, base loss: 16095.11
[INFO 2017-06-29 07:20:31,007 main.py:57] epoch 8287, training loss: 7356.48, average training loss: 7347.05, base loss: 16095.38
[INFO 2017-06-29 07:20:34,017 main.py:57] epoch 8288, training loss: 7000.28, average training loss: 7346.52, base loss: 16095.20
[INFO 2017-06-29 07:20:37,056 main.py:57] epoch 8289, training loss: 6486.70, average training loss: 7344.04, base loss: 16094.60
[INFO 2017-06-29 07:20:40,077 main.py:57] epoch 8290, training loss: 8106.40, average training loss: 7345.22, base loss: 16095.23
[INFO 2017-06-29 07:20:43,182 main.py:57] epoch 8291, training loss: 7666.64, average training loss: 7345.53, base loss: 16095.35
[INFO 2017-06-29 07:20:46,206 main.py:57] epoch 8292, training loss: 6695.13, average training loss: 7344.59, base loss: 16094.81
[INFO 2017-06-29 07:20:49,247 main.py:57] epoch 8293, training loss: 7260.56, average training loss: 7344.80, base loss: 16094.33
[INFO 2017-06-29 07:20:52,320 main.py:57] epoch 8294, training loss: 8418.77, average training loss: 7345.70, base loss: 16094.84
[INFO 2017-06-29 07:20:55,444 main.py:57] epoch 8295, training loss: 7404.65, average training loss: 7346.03, base loss: 16095.11
[INFO 2017-06-29 07:20:58,447 main.py:57] epoch 8296, training loss: 7138.90, average training loss: 7346.38, base loss: 16094.83
[INFO 2017-06-29 07:21:01,508 main.py:57] epoch 8297, training loss: 6950.07, average training loss: 7345.06, base loss: 16094.46
[INFO 2017-06-29 07:21:04,568 main.py:57] epoch 8298, training loss: 7162.43, average training loss: 7344.68, base loss: 16094.59
[INFO 2017-06-29 07:21:07,647 main.py:57] epoch 8299, training loss: 7430.57, average training loss: 7343.97, base loss: 16094.65
[INFO 2017-06-29 07:21:07,647 main.py:59] epoch 8299, testing
[INFO 2017-06-29 07:21:20,322 main.py:104] average testing loss: 8370.76, base loss: 16810.31
[INFO 2017-06-29 07:21:20,322 main.py:105] improve_loss: 8439.55, improve_percent: 0.50
[INFO 2017-06-29 07:21:20,323 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:21:23,387 main.py:57] epoch 8300, training loss: 7276.05, average training loss: 7344.75, base loss: 16094.42
[INFO 2017-06-29 07:21:26,494 main.py:57] epoch 8301, training loss: 7613.42, average training loss: 7345.53, base loss: 16094.74
[INFO 2017-06-29 07:21:29,495 main.py:57] epoch 8302, training loss: 7512.30, average training loss: 7345.53, base loss: 16094.67
[INFO 2017-06-29 07:21:32,597 main.py:57] epoch 8303, training loss: 7594.38, average training loss: 7346.40, base loss: 16094.81
[INFO 2017-06-29 07:21:35,619 main.py:57] epoch 8304, training loss: 6811.99, average training loss: 7345.13, base loss: 16094.93
[INFO 2017-06-29 07:21:38,654 main.py:57] epoch 8305, training loss: 7681.92, average training loss: 7345.21, base loss: 16095.31
[INFO 2017-06-29 07:21:41,772 main.py:57] epoch 8306, training loss: 6954.54, average training loss: 7344.63, base loss: 16095.38
[INFO 2017-06-29 07:21:44,858 main.py:57] epoch 8307, training loss: 6689.56, average training loss: 7344.32, base loss: 16095.32
[INFO 2017-06-29 07:21:47,876 main.py:57] epoch 8308, training loss: 6706.98, average training loss: 7343.62, base loss: 16094.80
[INFO 2017-06-29 07:21:50,897 main.py:57] epoch 8309, training loss: 7334.65, average training loss: 7342.19, base loss: 16094.71
[INFO 2017-06-29 07:21:53,996 main.py:57] epoch 8310, training loss: 7213.67, average training loss: 7341.54, base loss: 16095.28
[INFO 2017-06-29 07:21:57,043 main.py:57] epoch 8311, training loss: 6891.35, average training loss: 7339.94, base loss: 16095.27
[INFO 2017-06-29 07:22:00,094 main.py:57] epoch 8312, training loss: 8047.63, average training loss: 7339.89, base loss: 16095.29
[INFO 2017-06-29 07:22:03,178 main.py:57] epoch 8313, training loss: 7564.69, average training loss: 7340.09, base loss: 16095.49
[INFO 2017-06-29 07:22:06,203 main.py:57] epoch 8314, training loss: 7228.27, average training loss: 7339.51, base loss: 16095.60
[INFO 2017-06-29 07:22:09,222 main.py:57] epoch 8315, training loss: 6434.60, average training loss: 7338.36, base loss: 16095.26
[INFO 2017-06-29 07:22:12,306 main.py:57] epoch 8316, training loss: 7721.79, average training loss: 7339.17, base loss: 16095.27
[INFO 2017-06-29 07:22:15,392 main.py:57] epoch 8317, training loss: 6272.78, average training loss: 7337.78, base loss: 16094.60
[INFO 2017-06-29 07:22:18,384 main.py:57] epoch 8318, training loss: 6900.67, average training loss: 7337.46, base loss: 16094.21
[INFO 2017-06-29 07:22:21,408 main.py:57] epoch 8319, training loss: 6898.87, average training loss: 7337.34, base loss: 16093.92
[INFO 2017-06-29 07:22:24,500 main.py:57] epoch 8320, training loss: 7888.80, average training loss: 7337.94, base loss: 16093.92
[INFO 2017-06-29 07:22:27,486 main.py:57] epoch 8321, training loss: 7799.37, average training loss: 7338.87, base loss: 16094.08
[INFO 2017-06-29 07:22:30,471 main.py:57] epoch 8322, training loss: 6447.53, average training loss: 7338.90, base loss: 16093.66
[INFO 2017-06-29 07:22:33,619 main.py:57] epoch 8323, training loss: 7108.50, average training loss: 7339.16, base loss: 16093.74
[INFO 2017-06-29 07:22:36,689 main.py:57] epoch 8324, training loss: 8601.28, average training loss: 7340.62, base loss: 16093.47
[INFO 2017-06-29 07:22:39,771 main.py:57] epoch 8325, training loss: 8197.03, average training loss: 7342.32, base loss: 16093.56
[INFO 2017-06-29 07:22:42,857 main.py:57] epoch 8326, training loss: 6886.70, average training loss: 7341.90, base loss: 16093.47
[INFO 2017-06-29 07:22:45,864 main.py:57] epoch 8327, training loss: 7503.66, average training loss: 7342.61, base loss: 16093.83
[INFO 2017-06-29 07:22:48,895 main.py:57] epoch 8328, training loss: 7686.14, average training loss: 7343.36, base loss: 16093.69
[INFO 2017-06-29 07:22:51,916 main.py:57] epoch 8329, training loss: 7190.75, average training loss: 7343.03, base loss: 16093.63
[INFO 2017-06-29 07:22:54,983 main.py:57] epoch 8330, training loss: 7917.17, average training loss: 7343.86, base loss: 16093.55
[INFO 2017-06-29 07:22:57,983 main.py:57] epoch 8331, training loss: 6495.82, average training loss: 7343.41, base loss: 16093.51
[INFO 2017-06-29 07:23:01,049 main.py:57] epoch 8332, training loss: 7428.17, average training loss: 7342.99, base loss: 16093.76
[INFO 2017-06-29 07:23:04,066 main.py:57] epoch 8333, training loss: 6557.18, average training loss: 7343.12, base loss: 16093.85
[INFO 2017-06-29 07:23:07,147 main.py:57] epoch 8334, training loss: 7758.82, average training loss: 7343.24, base loss: 16094.25
[INFO 2017-06-29 07:23:10,242 main.py:57] epoch 8335, training loss: 7956.76, average training loss: 7344.11, base loss: 16094.80
[INFO 2017-06-29 07:23:13,389 main.py:57] epoch 8336, training loss: 7337.47, average training loss: 7342.38, base loss: 16094.82
[INFO 2017-06-29 07:23:16,457 main.py:57] epoch 8337, training loss: 6973.60, average training loss: 7341.88, base loss: 16094.63
[INFO 2017-06-29 07:23:19,453 main.py:57] epoch 8338, training loss: 6931.00, average training loss: 7340.37, base loss: 16094.54
[INFO 2017-06-29 07:23:22,463 main.py:57] epoch 8339, training loss: 7063.65, average training loss: 7340.61, base loss: 16094.72
[INFO 2017-06-29 07:23:25,475 main.py:57] epoch 8340, training loss: 8504.67, average training loss: 7341.65, base loss: 16094.87
[INFO 2017-06-29 07:23:28,539 main.py:57] epoch 8341, training loss: 8096.48, average training loss: 7341.65, base loss: 16095.19
[INFO 2017-06-29 07:23:31,583 main.py:57] epoch 8342, training loss: 7214.51, average training loss: 7341.55, base loss: 16095.33
[INFO 2017-06-29 07:23:34,622 main.py:57] epoch 8343, training loss: 6706.63, average training loss: 7340.73, base loss: 16095.08
[INFO 2017-06-29 07:23:37,714 main.py:57] epoch 8344, training loss: 8345.64, average training loss: 7342.87, base loss: 16095.19
[INFO 2017-06-29 07:23:40,753 main.py:57] epoch 8345, training loss: 8121.91, average training loss: 7344.29, base loss: 16095.99
[INFO 2017-06-29 07:23:43,784 main.py:57] epoch 8346, training loss: 7135.31, average training loss: 7344.51, base loss: 16095.79
[INFO 2017-06-29 07:23:46,890 main.py:57] epoch 8347, training loss: 7002.24, average training loss: 7344.07, base loss: 16095.59
[INFO 2017-06-29 07:23:50,000 main.py:57] epoch 8348, training loss: 7000.67, average training loss: 7343.83, base loss: 16095.68
[INFO 2017-06-29 07:23:53,104 main.py:57] epoch 8349, training loss: 6936.14, average training loss: 7343.89, base loss: 16095.55
[INFO 2017-06-29 07:23:56,121 main.py:57] epoch 8350, training loss: 6898.88, average training loss: 7343.42, base loss: 16095.11
[INFO 2017-06-29 07:23:59,116 main.py:57] epoch 8351, training loss: 8320.93, average training loss: 7344.43, base loss: 16095.21
[INFO 2017-06-29 07:24:02,137 main.py:57] epoch 8352, training loss: 8529.04, average training loss: 7345.16, base loss: 16095.62
[INFO 2017-06-29 07:24:05,182 main.py:57] epoch 8353, training loss: 7506.34, average training loss: 7345.22, base loss: 16095.83
[INFO 2017-06-29 07:24:08,287 main.py:57] epoch 8354, training loss: 8018.82, average training loss: 7345.31, base loss: 16096.00
[INFO 2017-06-29 07:24:11,340 main.py:57] epoch 8355, training loss: 6708.77, average training loss: 7343.88, base loss: 16095.69
[INFO 2017-06-29 07:24:14,410 main.py:57] epoch 8356, training loss: 8022.95, average training loss: 7344.63, base loss: 16096.08
[INFO 2017-06-29 07:24:17,510 main.py:57] epoch 8357, training loss: 6812.92, average training loss: 7343.74, base loss: 16095.89
[INFO 2017-06-29 07:24:20,585 main.py:57] epoch 8358, training loss: 7227.57, average training loss: 7343.48, base loss: 16096.06
[INFO 2017-06-29 07:24:23,615 main.py:57] epoch 8359, training loss: 6816.50, average training loss: 7342.86, base loss: 16095.97
[INFO 2017-06-29 07:24:26,687 main.py:57] epoch 8360, training loss: 7373.49, average training loss: 7341.85, base loss: 16096.12
[INFO 2017-06-29 07:24:29,770 main.py:57] epoch 8361, training loss: 8595.58, average training loss: 7343.71, base loss: 16096.41
[INFO 2017-06-29 07:24:32,818 main.py:57] epoch 8362, training loss: 6982.26, average training loss: 7343.09, base loss: 16095.89
[INFO 2017-06-29 07:24:35,879 main.py:57] epoch 8363, training loss: 7887.72, average training loss: 7343.49, base loss: 16096.30
[INFO 2017-06-29 07:24:38,963 main.py:57] epoch 8364, training loss: 7544.74, average training loss: 7344.19, base loss: 16096.65
[INFO 2017-06-29 07:24:42,078 main.py:57] epoch 8365, training loss: 8061.73, average training loss: 7345.08, base loss: 16097.10
[INFO 2017-06-29 07:24:45,131 main.py:57] epoch 8366, training loss: 6951.48, average training loss: 7344.17, base loss: 16096.71
[INFO 2017-06-29 07:24:48,153 main.py:57] epoch 8367, training loss: 7222.34, average training loss: 7343.89, base loss: 16096.47
[INFO 2017-06-29 07:24:51,206 main.py:57] epoch 8368, training loss: 6824.88, average training loss: 7344.51, base loss: 16096.03
[INFO 2017-06-29 07:24:54,251 main.py:57] epoch 8369, training loss: 7433.34, average training loss: 7344.71, base loss: 16096.07
[INFO 2017-06-29 07:24:57,274 main.py:57] epoch 8370, training loss: 7105.73, average training loss: 7344.45, base loss: 16095.67
[INFO 2017-06-29 07:25:00,306 main.py:57] epoch 8371, training loss: 7720.00, average training loss: 7345.01, base loss: 16095.86
[INFO 2017-06-29 07:25:03,388 main.py:57] epoch 8372, training loss: 7070.25, average training loss: 7344.97, base loss: 16095.80
[INFO 2017-06-29 07:25:06,419 main.py:57] epoch 8373, training loss: 6728.28, average training loss: 7344.19, base loss: 16095.66
[INFO 2017-06-29 07:25:09,492 main.py:57] epoch 8374, training loss: 8419.71, average training loss: 7344.63, base loss: 16096.40
[INFO 2017-06-29 07:25:12,527 main.py:57] epoch 8375, training loss: 7525.82, average training loss: 7344.84, base loss: 16096.39
[INFO 2017-06-29 07:25:15,687 main.py:57] epoch 8376, training loss: 7565.48, average training loss: 7345.63, base loss: 16096.59
[INFO 2017-06-29 07:25:18,745 main.py:57] epoch 8377, training loss: 6806.52, average training loss: 7345.41, base loss: 16096.26
[INFO 2017-06-29 07:25:21,779 main.py:57] epoch 8378, training loss: 6963.52, average training loss: 7344.76, base loss: 16096.32
[INFO 2017-06-29 07:25:24,779 main.py:57] epoch 8379, training loss: 6743.05, average training loss: 7344.63, base loss: 16096.01
[INFO 2017-06-29 07:25:27,878 main.py:57] epoch 8380, training loss: 7319.31, average training loss: 7343.69, base loss: 16096.15
[INFO 2017-06-29 07:25:30,902 main.py:57] epoch 8381, training loss: 6126.33, average training loss: 7341.48, base loss: 16095.42
[INFO 2017-06-29 07:25:33,905 main.py:57] epoch 8382, training loss: 7612.59, average training loss: 7341.43, base loss: 16095.45
[INFO 2017-06-29 07:25:36,929 main.py:57] epoch 8383, training loss: 7414.90, average training loss: 7341.84, base loss: 16095.34
[INFO 2017-06-29 07:25:40,036 main.py:57] epoch 8384, training loss: 6760.34, average training loss: 7341.04, base loss: 16095.22
[INFO 2017-06-29 07:25:43,113 main.py:57] epoch 8385, training loss: 7186.55, average training loss: 7339.81, base loss: 16095.08
[INFO 2017-06-29 07:25:46,215 main.py:57] epoch 8386, training loss: 7880.65, average training loss: 7340.09, base loss: 16095.58
[INFO 2017-06-29 07:25:49,274 main.py:57] epoch 8387, training loss: 7216.47, average training loss: 7340.22, base loss: 16095.61
[INFO 2017-06-29 07:25:52,291 main.py:57] epoch 8388, training loss: 7321.36, average training loss: 7340.91, base loss: 16095.54
[INFO 2017-06-29 07:25:55,425 main.py:57] epoch 8389, training loss: 6939.38, average training loss: 7340.53, base loss: 16095.45
[INFO 2017-06-29 07:25:58,501 main.py:57] epoch 8390, training loss: 6712.58, average training loss: 7339.14, base loss: 16095.04
[INFO 2017-06-29 07:26:01,648 main.py:57] epoch 8391, training loss: 8103.47, average training loss: 7340.55, base loss: 16095.03
[INFO 2017-06-29 07:26:04,756 main.py:57] epoch 8392, training loss: 7573.34, average training loss: 7340.91, base loss: 16095.36
[INFO 2017-06-29 07:26:07,827 main.py:57] epoch 8393, training loss: 7319.25, average training loss: 7341.07, base loss: 16096.04
[INFO 2017-06-29 07:26:10,826 main.py:57] epoch 8394, training loss: 7547.15, average training loss: 7341.42, base loss: 16095.82
[INFO 2017-06-29 07:26:13,897 main.py:57] epoch 8395, training loss: 8604.90, average training loss: 7341.91, base loss: 16096.32
[INFO 2017-06-29 07:26:16,925 main.py:57] epoch 8396, training loss: 7558.44, average training loss: 7342.61, base loss: 16096.40
[INFO 2017-06-29 07:26:19,944 main.py:57] epoch 8397, training loss: 6916.88, average training loss: 7342.47, base loss: 16095.95
[INFO 2017-06-29 07:26:23,003 main.py:57] epoch 8398, training loss: 6653.56, average training loss: 7342.21, base loss: 16095.43
[INFO 2017-06-29 07:26:26,116 main.py:57] epoch 8399, training loss: 6500.34, average training loss: 7341.93, base loss: 16095.04
[INFO 2017-06-29 07:26:26,116 main.py:59] epoch 8399, testing
[INFO 2017-06-29 07:26:38,633 main.py:104] average testing loss: 8190.83, base loss: 16796.40
[INFO 2017-06-29 07:26:38,633 main.py:105] improve_loss: 8605.57, improve_percent: 0.51
[INFO 2017-06-29 07:26:38,634 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:26:41,714 main.py:57] epoch 8400, training loss: 6974.19, average training loss: 7342.04, base loss: 16094.33
[INFO 2017-06-29 07:26:44,844 main.py:57] epoch 8401, training loss: 7741.33, average training loss: 7342.54, base loss: 16094.40
[INFO 2017-06-29 07:26:47,920 main.py:57] epoch 8402, training loss: 7487.11, average training loss: 7343.52, base loss: 16094.79
[INFO 2017-06-29 07:26:50,973 main.py:57] epoch 8403, training loss: 7247.46, average training loss: 7344.14, base loss: 16094.71
[INFO 2017-06-29 07:26:54,033 main.py:57] epoch 8404, training loss: 6855.32, average training loss: 7343.39, base loss: 16094.38
[INFO 2017-06-29 07:26:57,165 main.py:57] epoch 8405, training loss: 7315.90, average training loss: 7343.32, base loss: 16094.53
[INFO 2017-06-29 07:27:00,220 main.py:57] epoch 8406, training loss: 8204.06, average training loss: 7344.83, base loss: 16094.83
[INFO 2017-06-29 07:27:03,276 main.py:57] epoch 8407, training loss: 8483.54, average training loss: 7346.08, base loss: 16095.43
[INFO 2017-06-29 07:27:06,379 main.py:57] epoch 8408, training loss: 6749.89, average training loss: 7346.02, base loss: 16095.38
[INFO 2017-06-29 07:27:09,430 main.py:57] epoch 8409, training loss: 8783.65, average training loss: 7346.90, base loss: 16095.86
[INFO 2017-06-29 07:27:12,503 main.py:57] epoch 8410, training loss: 7249.49, average training loss: 7347.42, base loss: 16095.81
[INFO 2017-06-29 07:27:15,571 main.py:57] epoch 8411, training loss: 7064.77, average training loss: 7347.76, base loss: 16095.73
[INFO 2017-06-29 07:27:18,603 main.py:57] epoch 8412, training loss: 7370.18, average training loss: 7347.27, base loss: 16095.41
[INFO 2017-06-29 07:27:21,686 main.py:57] epoch 8413, training loss: 8055.49, average training loss: 7348.10, base loss: 16095.52
[INFO 2017-06-29 07:27:24,744 main.py:57] epoch 8414, training loss: 6401.36, average training loss: 7347.17, base loss: 16094.89
[INFO 2017-06-29 07:27:27,822 main.py:57] epoch 8415, training loss: 7742.00, average training loss: 7347.85, base loss: 16094.74
[INFO 2017-06-29 07:27:30,849 main.py:57] epoch 8416, training loss: 6969.44, average training loss: 7347.08, base loss: 16094.48
[INFO 2017-06-29 07:27:33,903 main.py:57] epoch 8417, training loss: 8829.47, average training loss: 7349.17, base loss: 16095.27
[INFO 2017-06-29 07:27:36,913 main.py:57] epoch 8418, training loss: 7858.11, average training loss: 7350.64, base loss: 16095.34
[INFO 2017-06-29 07:27:39,928 main.py:57] epoch 8419, training loss: 7804.81, average training loss: 7351.88, base loss: 16095.24
[INFO 2017-06-29 07:27:43,040 main.py:57] epoch 8420, training loss: 7277.25, average training loss: 7352.72, base loss: 16095.05
[INFO 2017-06-29 07:27:46,108 main.py:57] epoch 8421, training loss: 7892.62, average training loss: 7353.46, base loss: 16095.02
[INFO 2017-06-29 07:27:49,190 main.py:57] epoch 8422, training loss: 7150.09, average training loss: 7353.93, base loss: 16095.01
[INFO 2017-06-29 07:27:52,184 main.py:57] epoch 8423, training loss: 7645.72, average training loss: 7353.78, base loss: 16095.28
[INFO 2017-06-29 07:27:55,288 main.py:57] epoch 8424, training loss: 7857.43, average training loss: 7354.23, base loss: 16095.58
[INFO 2017-06-29 07:27:58,445 main.py:57] epoch 8425, training loss: 7587.67, average training loss: 7354.96, base loss: 16095.80
[INFO 2017-06-29 07:28:01,591 main.py:57] epoch 8426, training loss: 7810.03, average training loss: 7354.97, base loss: 16095.99
[INFO 2017-06-29 07:28:04,664 main.py:57] epoch 8427, training loss: 7355.03, average training loss: 7355.37, base loss: 16095.97
[INFO 2017-06-29 07:28:07,702 main.py:57] epoch 8428, training loss: 7352.20, average training loss: 7355.84, base loss: 16096.29
[INFO 2017-06-29 07:28:10,760 main.py:57] epoch 8429, training loss: 6852.58, average training loss: 7355.51, base loss: 16096.10
[INFO 2017-06-29 07:28:13,810 main.py:57] epoch 8430, training loss: 6762.50, average training loss: 7355.29, base loss: 16095.67
[INFO 2017-06-29 07:28:16,829 main.py:57] epoch 8431, training loss: 8344.16, average training loss: 7356.82, base loss: 16095.86
[INFO 2017-06-29 07:28:19,835 main.py:57] epoch 8432, training loss: 6865.57, average training loss: 7355.90, base loss: 16095.42
[INFO 2017-06-29 07:28:22,875 main.py:57] epoch 8433, training loss: 6747.99, average training loss: 7354.79, base loss: 16094.86
[INFO 2017-06-29 07:28:25,998 main.py:57] epoch 8434, training loss: 7005.88, average training loss: 7354.22, base loss: 16094.77
[INFO 2017-06-29 07:28:29,045 main.py:57] epoch 8435, training loss: 8130.47, average training loss: 7355.72, base loss: 16095.30
[INFO 2017-06-29 07:28:32,145 main.py:57] epoch 8436, training loss: 6659.23, average training loss: 7355.29, base loss: 16094.91
[INFO 2017-06-29 07:28:35,171 main.py:57] epoch 8437, training loss: 6952.16, average training loss: 7355.18, base loss: 16094.70
[INFO 2017-06-29 07:28:38,190 main.py:57] epoch 8438, training loss: 7378.44, average training loss: 7355.52, base loss: 16094.80
[INFO 2017-06-29 07:28:41,206 main.py:57] epoch 8439, training loss: 7794.87, average training loss: 7356.25, base loss: 16095.26
[INFO 2017-06-29 07:28:44,276 main.py:57] epoch 8440, training loss: 7392.69, average training loss: 7356.88, base loss: 16095.35
[INFO 2017-06-29 07:28:47,306 main.py:57] epoch 8441, training loss: 6505.30, average training loss: 7354.61, base loss: 16095.15
[INFO 2017-06-29 07:28:50,396 main.py:57] epoch 8442, training loss: 7226.45, average training loss: 7354.18, base loss: 16095.19
[INFO 2017-06-29 07:28:53,376 main.py:57] epoch 8443, training loss: 7007.42, average training loss: 7353.47, base loss: 16095.58
[INFO 2017-06-29 07:28:56,407 main.py:57] epoch 8444, training loss: 6931.07, average training loss: 7351.92, base loss: 16095.35
[INFO 2017-06-29 07:28:59,403 main.py:57] epoch 8445, training loss: 8389.06, average training loss: 7353.97, base loss: 16095.96
[INFO 2017-06-29 07:29:02,418 main.py:57] epoch 8446, training loss: 7625.72, average training loss: 7353.52, base loss: 16095.92
[INFO 2017-06-29 07:29:05,486 main.py:57] epoch 8447, training loss: 7248.51, average training loss: 7353.36, base loss: 16095.41
[INFO 2017-06-29 07:29:08,484 main.py:57] epoch 8448, training loss: 7587.44, average training loss: 7353.88, base loss: 16095.48
[INFO 2017-06-29 07:29:11,458 main.py:57] epoch 8449, training loss: 7028.74, average training loss: 7354.77, base loss: 16095.54
[INFO 2017-06-29 07:29:14,476 main.py:57] epoch 8450, training loss: 7365.88, average training loss: 7354.56, base loss: 16095.36
[INFO 2017-06-29 07:29:17,505 main.py:57] epoch 8451, training loss: 7384.17, average training loss: 7354.10, base loss: 16095.19
[INFO 2017-06-29 07:29:20,571 main.py:57] epoch 8452, training loss: 6819.82, average training loss: 7353.79, base loss: 16094.69
[INFO 2017-06-29 07:29:23,651 main.py:57] epoch 8453, training loss: 6366.43, average training loss: 7352.59, base loss: 16094.04
[INFO 2017-06-29 07:29:26,702 main.py:57] epoch 8454, training loss: 6927.46, average training loss: 7352.65, base loss: 16093.77
[INFO 2017-06-29 07:29:29,820 main.py:57] epoch 8455, training loss: 7307.34, average training loss: 7352.12, base loss: 16093.86
[INFO 2017-06-29 07:29:32,927 main.py:57] epoch 8456, training loss: 6693.20, average training loss: 7351.40, base loss: 16093.26
[INFO 2017-06-29 07:29:35,988 main.py:57] epoch 8457, training loss: 7698.60, average training loss: 7350.88, base loss: 16093.35
[INFO 2017-06-29 07:29:39,010 main.py:57] epoch 8458, training loss: 7673.70, average training loss: 7350.73, base loss: 16093.78
[INFO 2017-06-29 07:29:42,053 main.py:57] epoch 8459, training loss: 8450.85, average training loss: 7351.66, base loss: 16094.11
[INFO 2017-06-29 07:29:45,154 main.py:57] epoch 8460, training loss: 6620.10, average training loss: 7349.92, base loss: 16093.73
[INFO 2017-06-29 07:29:48,252 main.py:57] epoch 8461, training loss: 7841.62, average training loss: 7349.69, base loss: 16093.88
[INFO 2017-06-29 07:29:51,347 main.py:57] epoch 8462, training loss: 6754.99, average training loss: 7348.02, base loss: 16093.14
[INFO 2017-06-29 07:29:54,335 main.py:57] epoch 8463, training loss: 6934.64, average training loss: 7348.42, base loss: 16092.49
[INFO 2017-06-29 07:29:57,414 main.py:57] epoch 8464, training loss: 8964.44, average training loss: 7349.72, base loss: 16093.20
[INFO 2017-06-29 07:30:00,546 main.py:57] epoch 8465, training loss: 7586.74, average training loss: 7349.54, base loss: 16093.26
[INFO 2017-06-29 07:30:03,551 main.py:57] epoch 8466, training loss: 7164.53, average training loss: 7349.93, base loss: 16092.91
[INFO 2017-06-29 07:30:06,637 main.py:57] epoch 8467, training loss: 6330.37, average training loss: 7348.69, base loss: 16092.09
[INFO 2017-06-29 07:30:09,706 main.py:57] epoch 8468, training loss: 7979.55, average training loss: 7350.05, base loss: 16092.52
[INFO 2017-06-29 07:30:12,735 main.py:57] epoch 8469, training loss: 6786.33, average training loss: 7348.55, base loss: 16092.13
[INFO 2017-06-29 07:30:15,861 main.py:57] epoch 8470, training loss: 6778.84, average training loss: 7348.12, base loss: 16092.14
[INFO 2017-06-29 07:30:18,937 main.py:57] epoch 8471, training loss: 7885.12, average training loss: 7348.47, base loss: 16092.26
[INFO 2017-06-29 07:30:21,946 main.py:57] epoch 8472, training loss: 7729.37, average training loss: 7348.33, base loss: 16092.87
[INFO 2017-06-29 07:30:24,928 main.py:57] epoch 8473, training loss: 7743.30, average training loss: 7348.10, base loss: 16093.15
[INFO 2017-06-29 07:30:27,946 main.py:57] epoch 8474, training loss: 7017.73, average training loss: 7347.96, base loss: 16093.06
[INFO 2017-06-29 07:30:30,955 main.py:57] epoch 8475, training loss: 7404.21, average training loss: 7347.87, base loss: 16092.85
[INFO 2017-06-29 07:30:34,064 main.py:57] epoch 8476, training loss: 7555.00, average training loss: 7348.33, base loss: 16092.98
[INFO 2017-06-29 07:30:37,140 main.py:57] epoch 8477, training loss: 7839.23, average training loss: 7347.78, base loss: 16093.13
[INFO 2017-06-29 07:30:40,144 main.py:57] epoch 8478, training loss: 6906.68, average training loss: 7346.61, base loss: 16092.72
[INFO 2017-06-29 07:30:43,145 main.py:57] epoch 8479, training loss: 6791.23, average training loss: 7344.67, base loss: 16092.41
[INFO 2017-06-29 07:30:46,236 main.py:57] epoch 8480, training loss: 7998.94, average training loss: 7345.65, base loss: 16092.54
[INFO 2017-06-29 07:30:49,290 main.py:57] epoch 8481, training loss: 7122.03, average training loss: 7345.30, base loss: 16092.45
[INFO 2017-06-29 07:30:52,355 main.py:57] epoch 8482, training loss: 6804.49, average training loss: 7344.64, base loss: 16091.96
[INFO 2017-06-29 07:30:55,427 main.py:57] epoch 8483, training loss: 8224.05, average training loss: 7345.29, base loss: 16092.19
[INFO 2017-06-29 07:30:58,494 main.py:57] epoch 8484, training loss: 6887.64, average training loss: 7345.57, base loss: 16092.00
[INFO 2017-06-29 07:31:01,589 main.py:57] epoch 8485, training loss: 7089.77, average training loss: 7345.79, base loss: 16091.60
[INFO 2017-06-29 07:31:04,618 main.py:57] epoch 8486, training loss: 7100.69, average training loss: 7345.77, base loss: 16091.79
[INFO 2017-06-29 07:31:07,752 main.py:57] epoch 8487, training loss: 7675.08, average training loss: 7346.90, base loss: 16092.08
[INFO 2017-06-29 07:31:10,737 main.py:57] epoch 8488, training loss: 6970.76, average training loss: 7346.41, base loss: 16092.27
[INFO 2017-06-29 07:31:13,827 main.py:57] epoch 8489, training loss: 7807.04, average training loss: 7346.83, base loss: 16092.87
[INFO 2017-06-29 07:31:16,872 main.py:57] epoch 8490, training loss: 7887.68, average training loss: 7347.32, base loss: 16093.25
[INFO 2017-06-29 07:31:19,908 main.py:57] epoch 8491, training loss: 7333.50, average training loss: 7347.28, base loss: 16093.35
[INFO 2017-06-29 07:31:22,963 main.py:57] epoch 8492, training loss: 9158.19, average training loss: 7348.80, base loss: 16094.74
[INFO 2017-06-29 07:31:26,022 main.py:57] epoch 8493, training loss: 6301.69, average training loss: 7346.88, base loss: 16094.87
[INFO 2017-06-29 07:31:29,057 main.py:57] epoch 8494, training loss: 7389.99, average training loss: 7347.32, base loss: 16094.89
[INFO 2017-06-29 07:31:32,188 main.py:57] epoch 8495, training loss: 8033.27, average training loss: 7346.98, base loss: 16095.25
[INFO 2017-06-29 07:31:35,277 main.py:57] epoch 8496, training loss: 8809.82, average training loss: 7348.44, base loss: 16095.15
[INFO 2017-06-29 07:31:38,410 main.py:57] epoch 8497, training loss: 7328.82, average training loss: 7348.79, base loss: 16094.67
[INFO 2017-06-29 07:31:41,420 main.py:57] epoch 8498, training loss: 7247.77, average training loss: 7347.67, base loss: 16094.84
[INFO 2017-06-29 07:31:44,448 main.py:57] epoch 8499, training loss: 8432.43, average training loss: 7348.31, base loss: 16095.09
[INFO 2017-06-29 07:31:44,449 main.py:59] epoch 8499, testing
[INFO 2017-06-29 07:31:56,977 main.py:104] average testing loss: 7861.01, base loss: 16366.92
[INFO 2017-06-29 07:31:56,977 main.py:105] improve_loss: 8505.91, improve_percent: 0.52
[INFO 2017-06-29 07:31:56,978 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:32:00,004 main.py:57] epoch 8500, training loss: 7324.86, average training loss: 7347.99, base loss: 16095.66
[INFO 2017-06-29 07:32:03,048 main.py:57] epoch 8501, training loss: 7089.54, average training loss: 7347.92, base loss: 16095.60
[INFO 2017-06-29 07:32:06,098 main.py:57] epoch 8502, training loss: 8574.42, average training loss: 7348.22, base loss: 16095.80
[INFO 2017-06-29 07:32:09,122 main.py:57] epoch 8503, training loss: 7342.79, average training loss: 7348.78, base loss: 16095.79
[INFO 2017-06-29 07:32:12,236 main.py:57] epoch 8504, training loss: 7800.09, average training loss: 7348.73, base loss: 16096.01
[INFO 2017-06-29 07:32:15,311 main.py:57] epoch 8505, training loss: 7712.54, average training loss: 7348.63, base loss: 16095.99
[INFO 2017-06-29 07:32:18,401 main.py:57] epoch 8506, training loss: 7772.93, average training loss: 7349.05, base loss: 16096.15
[INFO 2017-06-29 07:32:21,479 main.py:57] epoch 8507, training loss: 7123.00, average training loss: 7348.71, base loss: 16096.29
[INFO 2017-06-29 07:32:24,547 main.py:57] epoch 8508, training loss: 6464.34, average training loss: 7348.26, base loss: 16095.83
[INFO 2017-06-29 07:32:27,555 main.py:57] epoch 8509, training loss: 8910.10, average training loss: 7349.66, base loss: 16096.48
[INFO 2017-06-29 07:32:30,608 main.py:57] epoch 8510, training loss: 7041.37, average training loss: 7349.52, base loss: 16096.51
[INFO 2017-06-29 07:32:33,723 main.py:57] epoch 8511, training loss: 6926.17, average training loss: 7348.71, base loss: 16096.44
[INFO 2017-06-29 07:32:36,788 main.py:57] epoch 8512, training loss: 7370.40, average training loss: 7348.74, base loss: 16096.19
[INFO 2017-06-29 07:32:39,841 main.py:57] epoch 8513, training loss: 6803.93, average training loss: 7347.82, base loss: 16095.80
[INFO 2017-06-29 07:32:42,890 main.py:57] epoch 8514, training loss: 6816.29, average training loss: 7347.17, base loss: 16095.46
[INFO 2017-06-29 07:32:45,964 main.py:57] epoch 8515, training loss: 9025.36, average training loss: 7348.71, base loss: 16096.20
[INFO 2017-06-29 07:32:48,987 main.py:57] epoch 8516, training loss: 7650.28, average training loss: 7349.72, base loss: 16096.58
[INFO 2017-06-29 07:32:51,990 main.py:57] epoch 8517, training loss: 7213.51, average training loss: 7349.75, base loss: 16096.48
[INFO 2017-06-29 07:32:55,037 main.py:57] epoch 8518, training loss: 7615.69, average training loss: 7350.32, base loss: 16096.47
[INFO 2017-06-29 07:32:58,116 main.py:57] epoch 8519, training loss: 7057.78, average training loss: 7349.04, base loss: 16096.09
[INFO 2017-06-29 07:33:01,247 main.py:57] epoch 8520, training loss: 7557.41, average training loss: 7349.68, base loss: 16096.37
[INFO 2017-06-29 07:33:04,305 main.py:57] epoch 8521, training loss: 8530.92, average training loss: 7350.82, base loss: 16096.96
[INFO 2017-06-29 07:33:07,295 main.py:57] epoch 8522, training loss: 6934.98, average training loss: 7350.20, base loss: 16096.67
[INFO 2017-06-29 07:33:10,366 main.py:57] epoch 8523, training loss: 7705.94, average training loss: 7350.29, base loss: 16096.72
[INFO 2017-06-29 07:33:13,459 main.py:57] epoch 8524, training loss: 8634.91, average training loss: 7351.34, base loss: 16097.00
[INFO 2017-06-29 07:33:16,496 main.py:57] epoch 8525, training loss: 8517.76, average training loss: 7352.46, base loss: 16097.03
[INFO 2017-06-29 07:33:19,626 main.py:57] epoch 8526, training loss: 7992.32, average training loss: 7352.61, base loss: 16097.31
[INFO 2017-06-29 07:33:22,649 main.py:57] epoch 8527, training loss: 6669.75, average training loss: 7351.26, base loss: 16096.96
[INFO 2017-06-29 07:33:25,719 main.py:57] epoch 8528, training loss: 7392.54, average training loss: 7351.75, base loss: 16096.60
[INFO 2017-06-29 07:33:28,772 main.py:57] epoch 8529, training loss: 6462.26, average training loss: 7350.17, base loss: 16095.74
[INFO 2017-06-29 07:33:31,831 main.py:57] epoch 8530, training loss: 8254.95, average training loss: 7351.69, base loss: 16095.77
[INFO 2017-06-29 07:33:34,802 main.py:57] epoch 8531, training loss: 7778.66, average training loss: 7352.98, base loss: 16095.91
[INFO 2017-06-29 07:33:37,910 main.py:57] epoch 8532, training loss: 7212.11, average training loss: 7352.97, base loss: 16095.97
[INFO 2017-06-29 07:33:40,967 main.py:57] epoch 8533, training loss: 7329.96, average training loss: 7353.47, base loss: 16095.68
[INFO 2017-06-29 07:33:43,968 main.py:57] epoch 8534, training loss: 7508.39, average training loss: 7354.44, base loss: 16095.92
[INFO 2017-06-29 07:33:47,006 main.py:57] epoch 8535, training loss: 6123.96, average training loss: 7352.78, base loss: 16095.45
[INFO 2017-06-29 07:33:50,038 main.py:57] epoch 8536, training loss: 8635.94, average training loss: 7354.11, base loss: 16096.18
[INFO 2017-06-29 07:33:53,146 main.py:57] epoch 8537, training loss: 7140.82, average training loss: 7354.16, base loss: 16096.13
[INFO 2017-06-29 07:33:56,137 main.py:57] epoch 8538, training loss: 7024.09, average training loss: 7354.78, base loss: 16096.30
[INFO 2017-06-29 07:33:59,230 main.py:57] epoch 8539, training loss: 7570.83, average training loss: 7354.46, base loss: 16096.72
[INFO 2017-06-29 07:34:02,281 main.py:57] epoch 8540, training loss: 8039.75, average training loss: 7356.30, base loss: 16097.05
[INFO 2017-06-29 07:34:05,314 main.py:57] epoch 8541, training loss: 8078.73, average training loss: 7356.85, base loss: 16097.53
[INFO 2017-06-29 07:34:08,331 main.py:57] epoch 8542, training loss: 6238.17, average training loss: 7356.95, base loss: 16096.44
[INFO 2017-06-29 07:34:11,371 main.py:57] epoch 8543, training loss: 6793.40, average training loss: 7356.19, base loss: 16096.15
[INFO 2017-06-29 07:34:14,450 main.py:57] epoch 8544, training loss: 7702.35, average training loss: 7357.28, base loss: 16096.34
[INFO 2017-06-29 07:34:17,446 main.py:57] epoch 8545, training loss: 7641.94, average training loss: 7358.07, base loss: 16096.37
[INFO 2017-06-29 07:34:20,555 main.py:57] epoch 8546, training loss: 7053.13, average training loss: 7357.94, base loss: 16096.18
[INFO 2017-06-29 07:34:23,581 main.py:57] epoch 8547, training loss: 7378.10, average training loss: 7357.02, base loss: 16096.29
[INFO 2017-06-29 07:34:26,695 main.py:57] epoch 8548, training loss: 7612.49, average training loss: 7356.20, base loss: 16096.34
[INFO 2017-06-29 07:34:29,713 main.py:57] epoch 8549, training loss: 8217.15, average training loss: 7357.84, base loss: 16096.64
[INFO 2017-06-29 07:34:32,733 main.py:57] epoch 8550, training loss: 7769.59, average training loss: 7357.64, base loss: 16096.63
[INFO 2017-06-29 07:34:35,775 main.py:57] epoch 8551, training loss: 7973.59, average training loss: 7358.44, base loss: 16096.78
[INFO 2017-06-29 07:34:38,879 main.py:57] epoch 8552, training loss: 7023.39, average training loss: 7358.68, base loss: 16096.88
[INFO 2017-06-29 07:34:41,897 main.py:57] epoch 8553, training loss: 7639.93, average training loss: 7359.27, base loss: 16096.86
[INFO 2017-06-29 07:34:44,910 main.py:57] epoch 8554, training loss: 7037.17, average training loss: 7358.06, base loss: 16096.43
[INFO 2017-06-29 07:34:47,982 main.py:57] epoch 8555, training loss: 8749.13, average training loss: 7359.96, base loss: 16097.29
[INFO 2017-06-29 07:34:51,008 main.py:57] epoch 8556, training loss: 7797.20, average training loss: 7358.67, base loss: 16097.61
[INFO 2017-06-29 07:34:54,000 main.py:57] epoch 8557, training loss: 7788.88, average training loss: 7359.23, base loss: 16097.79
[INFO 2017-06-29 07:34:57,029 main.py:57] epoch 8558, training loss: 8393.81, average training loss: 7359.94, base loss: 16098.33
[INFO 2017-06-29 07:35:00,091 main.py:57] epoch 8559, training loss: 7602.06, average training loss: 7360.48, base loss: 16098.44
[INFO 2017-06-29 07:35:03,118 main.py:57] epoch 8560, training loss: 6553.21, average training loss: 7360.28, base loss: 16098.25
[INFO 2017-06-29 07:35:06,191 main.py:57] epoch 8561, training loss: 6237.00, average training loss: 7359.64, base loss: 16097.96
[INFO 2017-06-29 07:35:09,273 main.py:57] epoch 8562, training loss: 7632.40, average training loss: 7359.82, base loss: 16097.89
[INFO 2017-06-29 07:35:12,259 main.py:57] epoch 8563, training loss: 7532.47, average training loss: 7359.28, base loss: 16098.21
[INFO 2017-06-29 07:35:15,323 main.py:57] epoch 8564, training loss: 8387.13, average training loss: 7360.72, base loss: 16098.87
[INFO 2017-06-29 07:35:18,415 main.py:57] epoch 8565, training loss: 6933.53, average training loss: 7359.52, base loss: 16098.30
[INFO 2017-06-29 07:35:21,449 main.py:57] epoch 8566, training loss: 6674.88, average training loss: 7358.88, base loss: 16098.31
[INFO 2017-06-29 07:35:24,465 main.py:57] epoch 8567, training loss: 7482.06, average training loss: 7359.77, base loss: 16098.39
[INFO 2017-06-29 07:35:27,559 main.py:57] epoch 8568, training loss: 7531.67, average training loss: 7359.79, base loss: 16098.16
[INFO 2017-06-29 07:35:30,674 main.py:57] epoch 8569, training loss: 8031.94, average training loss: 7361.20, base loss: 16098.16
[INFO 2017-06-29 07:35:33,743 main.py:57] epoch 8570, training loss: 7237.80, average training loss: 7360.94, base loss: 16097.84
[INFO 2017-06-29 07:35:36,798 main.py:57] epoch 8571, training loss: 6942.47, average training loss: 7360.41, base loss: 16097.23
[INFO 2017-06-29 07:35:39,850 main.py:57] epoch 8572, training loss: 6529.20, average training loss: 7359.18, base loss: 16096.80
[INFO 2017-06-29 07:35:42,996 main.py:57] epoch 8573, training loss: 7249.53, average training loss: 7359.06, base loss: 16096.29
[INFO 2017-06-29 07:35:46,001 main.py:57] epoch 8574, training loss: 7135.25, average training loss: 7358.29, base loss: 16096.32
[INFO 2017-06-29 07:35:49,032 main.py:57] epoch 8575, training loss: 7170.78, average training loss: 7358.25, base loss: 16096.52
[INFO 2017-06-29 07:35:52,004 main.py:57] epoch 8576, training loss: 6525.31, average training loss: 7356.08, base loss: 16096.24
[INFO 2017-06-29 07:35:55,075 main.py:57] epoch 8577, training loss: 7687.71, average training loss: 7356.57, base loss: 16096.59
[INFO 2017-06-29 07:35:58,170 main.py:57] epoch 8578, training loss: 6850.62, average training loss: 7356.44, base loss: 16096.35
[INFO 2017-06-29 07:36:01,206 main.py:57] epoch 8579, training loss: 7159.45, average training loss: 7356.46, base loss: 16096.35
[INFO 2017-06-29 07:36:04,246 main.py:57] epoch 8580, training loss: 6685.57, average training loss: 7356.15, base loss: 16096.04
[INFO 2017-06-29 07:36:07,328 main.py:57] epoch 8581, training loss: 7461.21, average training loss: 7356.15, base loss: 16096.33
[INFO 2017-06-29 07:36:10,406 main.py:57] epoch 8582, training loss: 7427.76, average training loss: 7355.85, base loss: 16096.50
[INFO 2017-06-29 07:36:13,469 main.py:57] epoch 8583, training loss: 6688.70, average training loss: 7355.54, base loss: 16096.56
[INFO 2017-06-29 07:36:16,487 main.py:57] epoch 8584, training loss: 7706.31, average training loss: 7355.93, base loss: 16097.01
[INFO 2017-06-29 07:36:19,599 main.py:57] epoch 8585, training loss: 6988.05, average training loss: 7355.13, base loss: 16096.75
[INFO 2017-06-29 07:36:22,697 main.py:57] epoch 8586, training loss: 8176.88, average training loss: 7355.55, base loss: 16096.93
[INFO 2017-06-29 07:36:25,651 main.py:57] epoch 8587, training loss: 7009.53, average training loss: 7356.33, base loss: 16097.17
[INFO 2017-06-29 07:36:28,738 main.py:57] epoch 8588, training loss: 7318.08, average training loss: 7356.28, base loss: 16097.40
[INFO 2017-06-29 07:36:31,776 main.py:57] epoch 8589, training loss: 7382.41, average training loss: 7356.99, base loss: 16097.65
[INFO 2017-06-29 07:36:34,909 main.py:57] epoch 8590, training loss: 7726.89, average training loss: 7357.24, base loss: 16097.09
[INFO 2017-06-29 07:36:37,997 main.py:57] epoch 8591, training loss: 7052.61, average training loss: 7357.26, base loss: 16096.60
[INFO 2017-06-29 07:36:41,026 main.py:57] epoch 8592, training loss: 7246.33, average training loss: 7356.20, base loss: 16096.44
[INFO 2017-06-29 07:36:44,241 main.py:57] epoch 8593, training loss: 7048.35, average training loss: 7355.78, base loss: 16095.85
[INFO 2017-06-29 07:36:47,312 main.py:57] epoch 8594, training loss: 8259.34, average training loss: 7356.50, base loss: 16096.28
[INFO 2017-06-29 07:36:50,329 main.py:57] epoch 8595, training loss: 7761.49, average training loss: 7356.94, base loss: 16096.46
[INFO 2017-06-29 07:36:53,324 main.py:57] epoch 8596, training loss: 7327.44, average training loss: 7357.48, base loss: 16096.47
[INFO 2017-06-29 07:36:56,437 main.py:57] epoch 8597, training loss: 8190.35, average training loss: 7358.38, base loss: 16096.82
[INFO 2017-06-29 07:36:59,502 main.py:57] epoch 8598, training loss: 6998.97, average training loss: 7357.54, base loss: 16096.62
[INFO 2017-06-29 07:37:02,539 main.py:57] epoch 8599, training loss: 7905.14, average training loss: 7358.11, base loss: 16097.15
[INFO 2017-06-29 07:37:02,539 main.py:59] epoch 8599, testing
[INFO 2017-06-29 07:37:15,159 main.py:104] average testing loss: 7832.84, base loss: 16178.16
[INFO 2017-06-29 07:37:15,159 main.py:105] improve_loss: 8345.32, improve_percent: 0.52
[INFO 2017-06-29 07:37:15,160 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:37:18,201 main.py:57] epoch 8600, training loss: 7580.37, average training loss: 7357.58, base loss: 16096.96
[INFO 2017-06-29 07:37:21,245 main.py:57] epoch 8601, training loss: 6959.17, average training loss: 7357.49, base loss: 16096.58
[INFO 2017-06-29 07:37:24,273 main.py:57] epoch 8602, training loss: 8263.30, average training loss: 7358.42, base loss: 16096.85
[INFO 2017-06-29 07:37:27,328 main.py:57] epoch 8603, training loss: 6645.28, average training loss: 7358.24, base loss: 16096.94
[INFO 2017-06-29 07:37:30,409 main.py:57] epoch 8604, training loss: 6682.63, average training loss: 7357.22, base loss: 16096.72
[INFO 2017-06-29 07:37:33,489 main.py:57] epoch 8605, training loss: 6886.80, average training loss: 7357.27, base loss: 16096.75
[INFO 2017-06-29 07:37:36,555 main.py:57] epoch 8606, training loss: 6816.84, average training loss: 7357.14, base loss: 16096.46
[INFO 2017-06-29 07:37:39,591 main.py:57] epoch 8607, training loss: 6982.36, average training loss: 7357.49, base loss: 16096.34
[INFO 2017-06-29 07:37:42,671 main.py:57] epoch 8608, training loss: 7824.37, average training loss: 7358.08, base loss: 16096.38
[INFO 2017-06-29 07:37:45,682 main.py:57] epoch 8609, training loss: 6869.96, average training loss: 7358.46, base loss: 16095.73
[INFO 2017-06-29 07:37:48,738 main.py:57] epoch 8610, training loss: 7406.36, average training loss: 7358.72, base loss: 16095.44
[INFO 2017-06-29 07:37:51,779 main.py:57] epoch 8611, training loss: 7802.13, average training loss: 7359.48, base loss: 16095.35
[INFO 2017-06-29 07:37:54,813 main.py:57] epoch 8612, training loss: 6532.20, average training loss: 7358.48, base loss: 16094.92
[INFO 2017-06-29 07:37:57,859 main.py:57] epoch 8613, training loss: 8195.94, average training loss: 7359.43, base loss: 16095.53
[INFO 2017-06-29 07:38:00,906 main.py:57] epoch 8614, training loss: 7178.82, average training loss: 7358.83, base loss: 16095.97
[INFO 2017-06-29 07:38:03,924 main.py:57] epoch 8615, training loss: 7488.52, average training loss: 7359.99, base loss: 16096.30
[INFO 2017-06-29 07:38:07,034 main.py:57] epoch 8616, training loss: 7208.20, average training loss: 7359.80, base loss: 16096.60
[INFO 2017-06-29 07:38:10,141 main.py:57] epoch 8617, training loss: 7340.03, average training loss: 7359.88, base loss: 16096.71
[INFO 2017-06-29 07:38:13,242 main.py:57] epoch 8618, training loss: 7080.08, average training loss: 7359.88, base loss: 16096.78
[INFO 2017-06-29 07:38:16,280 main.py:57] epoch 8619, training loss: 7227.74, average training loss: 7359.57, base loss: 16096.84
[INFO 2017-06-29 07:38:19,409 main.py:57] epoch 8620, training loss: 6868.46, average training loss: 7359.76, base loss: 16096.49
[INFO 2017-06-29 07:38:22,436 main.py:57] epoch 8621, training loss: 6943.63, average training loss: 7360.23, base loss: 16096.30
[INFO 2017-06-29 07:38:25,515 main.py:57] epoch 8622, training loss: 6565.63, average training loss: 7359.55, base loss: 16095.41
[INFO 2017-06-29 07:38:28,561 main.py:57] epoch 8623, training loss: 7115.34, average training loss: 7358.27, base loss: 16094.92
[INFO 2017-06-29 07:38:31,636 main.py:57] epoch 8624, training loss: 7061.82, average training loss: 7358.14, base loss: 16094.87
[INFO 2017-06-29 07:38:34,721 main.py:57] epoch 8625, training loss: 6968.54, average training loss: 7357.54, base loss: 16094.87
[INFO 2017-06-29 07:38:37,804 main.py:57] epoch 8626, training loss: 9493.02, average training loss: 7360.15, base loss: 16095.84
[INFO 2017-06-29 07:38:40,867 main.py:57] epoch 8627, training loss: 7520.27, average training loss: 7361.50, base loss: 16096.22
[INFO 2017-06-29 07:38:43,937 main.py:57] epoch 8628, training loss: 7550.52, average training loss: 7360.69, base loss: 16096.08
[INFO 2017-06-29 07:38:46,993 main.py:57] epoch 8629, training loss: 6827.28, average training loss: 7360.09, base loss: 16095.70
[INFO 2017-06-29 07:38:50,058 main.py:57] epoch 8630, training loss: 6839.89, average training loss: 7359.04, base loss: 16095.09
[INFO 2017-06-29 07:38:53,122 main.py:57] epoch 8631, training loss: 6504.59, average training loss: 7358.33, base loss: 16094.60
[INFO 2017-06-29 07:38:56,212 main.py:57] epoch 8632, training loss: 6646.94, average training loss: 7357.85, base loss: 16094.23
[INFO 2017-06-29 07:38:59,224 main.py:57] epoch 8633, training loss: 6716.34, average training loss: 7357.88, base loss: 16093.90
[INFO 2017-06-29 07:39:02,293 main.py:57] epoch 8634, training loss: 6970.08, average training loss: 7357.31, base loss: 16093.61
[INFO 2017-06-29 07:39:05,424 main.py:57] epoch 8635, training loss: 7903.47, average training loss: 7358.67, base loss: 16094.13
[INFO 2017-06-29 07:39:08,491 main.py:57] epoch 8636, training loss: 8795.26, average training loss: 7360.93, base loss: 16094.55
[INFO 2017-06-29 07:39:11,598 main.py:57] epoch 8637, training loss: 7008.85, average training loss: 7359.51, base loss: 16094.52
[INFO 2017-06-29 07:39:14,669 main.py:57] epoch 8638, training loss: 7028.51, average training loss: 7359.84, base loss: 16094.26
[INFO 2017-06-29 07:39:17,727 main.py:57] epoch 8639, training loss: 6605.86, average training loss: 7358.66, base loss: 16094.27
[INFO 2017-06-29 07:39:20,715 main.py:57] epoch 8640, training loss: 7199.09, average training loss: 7358.82, base loss: 16094.32
[INFO 2017-06-29 07:39:23,781 main.py:57] epoch 8641, training loss: 7529.90, average training loss: 7359.43, base loss: 16094.21
[INFO 2017-06-29 07:39:26,799 main.py:57] epoch 8642, training loss: 7625.16, average training loss: 7359.77, base loss: 16094.25
[INFO 2017-06-29 07:39:29,855 main.py:57] epoch 8643, training loss: 6075.37, average training loss: 7359.09, base loss: 16093.73
[INFO 2017-06-29 07:39:32,997 main.py:57] epoch 8644, training loss: 7592.73, average training loss: 7358.22, base loss: 16094.34
[INFO 2017-06-29 07:39:36,001 main.py:57] epoch 8645, training loss: 7439.08, average training loss: 7358.11, base loss: 16094.99
[INFO 2017-06-29 07:39:38,996 main.py:57] epoch 8646, training loss: 8159.21, average training loss: 7358.65, base loss: 16095.47
[INFO 2017-06-29 07:39:42,121 main.py:57] epoch 8647, training loss: 7572.47, average training loss: 7359.02, base loss: 16095.71
[INFO 2017-06-29 07:39:45,184 main.py:57] epoch 8648, training loss: 7242.42, average training loss: 7358.34, base loss: 16095.74
[INFO 2017-06-29 07:39:48,193 main.py:57] epoch 8649, training loss: 8335.06, average training loss: 7358.37, base loss: 16096.45
[INFO 2017-06-29 07:39:51,231 main.py:57] epoch 8650, training loss: 7549.29, average training loss: 7359.40, base loss: 16096.29
[INFO 2017-06-29 07:39:54,338 main.py:57] epoch 8651, training loss: 8194.19, average training loss: 7361.19, base loss: 16096.43
[INFO 2017-06-29 07:39:57,450 main.py:57] epoch 8652, training loss: 6062.86, average training loss: 7360.14, base loss: 16095.84
[INFO 2017-06-29 07:40:00,515 main.py:57] epoch 8653, training loss: 6537.67, average training loss: 7358.87, base loss: 16095.46
[INFO 2017-06-29 07:40:03,530 main.py:57] epoch 8654, training loss: 8209.61, average training loss: 7360.01, base loss: 16095.68
[INFO 2017-06-29 07:40:06,600 main.py:57] epoch 8655, training loss: 7298.55, average training loss: 7360.25, base loss: 16095.84
[INFO 2017-06-29 07:40:09,764 main.py:57] epoch 8656, training loss: 6401.12, average training loss: 7359.25, base loss: 16095.19
[INFO 2017-06-29 07:40:12,798 main.py:57] epoch 8657, training loss: 6461.20, average training loss: 7359.05, base loss: 16094.77
[INFO 2017-06-29 07:40:15,811 main.py:57] epoch 8658, training loss: 7816.62, average training loss: 7359.78, base loss: 16095.28
[INFO 2017-06-29 07:40:18,968 main.py:57] epoch 8659, training loss: 8397.30, average training loss: 7361.29, base loss: 16095.85
[INFO 2017-06-29 07:40:22,090 main.py:57] epoch 8660, training loss: 7342.87, average training loss: 7361.87, base loss: 16095.82
[INFO 2017-06-29 07:40:25,264 main.py:57] epoch 8661, training loss: 7234.00, average training loss: 7361.14, base loss: 16095.73
[INFO 2017-06-29 07:40:28,393 main.py:57] epoch 8662, training loss: 7063.95, average training loss: 7362.01, base loss: 16095.70
[INFO 2017-06-29 07:40:31,670 main.py:57] epoch 8663, training loss: 7628.68, average training loss: 7363.10, base loss: 16096.03
[INFO 2017-06-29 07:40:34,696 main.py:57] epoch 8664, training loss: 6998.77, average training loss: 7363.10, base loss: 16095.98
[INFO 2017-06-29 07:40:37,814 main.py:57] epoch 8665, training loss: 6641.37, average training loss: 7362.66, base loss: 16095.73
[INFO 2017-06-29 07:40:40,954 main.py:57] epoch 8666, training loss: 7743.22, average training loss: 7363.02, base loss: 16095.69
[INFO 2017-06-29 07:40:44,157 main.py:57] epoch 8667, training loss: 8291.97, average training loss: 7362.83, base loss: 16096.31
[INFO 2017-06-29 07:40:47,278 main.py:57] epoch 8668, training loss: 7729.69, average training loss: 7361.45, base loss: 16096.82
[INFO 2017-06-29 07:40:50,401 main.py:57] epoch 8669, training loss: 6426.15, average training loss: 7360.73, base loss: 16097.02
[INFO 2017-06-29 07:40:53,440 main.py:57] epoch 8670, training loss: 7439.64, average training loss: 7361.31, base loss: 16097.14
[INFO 2017-06-29 07:40:56,435 main.py:57] epoch 8671, training loss: 7341.78, average training loss: 7360.46, base loss: 16096.80
[INFO 2017-06-29 07:40:59,501 main.py:57] epoch 8672, training loss: 7589.08, average training loss: 7361.39, base loss: 16096.64
[INFO 2017-06-29 07:41:02,622 main.py:57] epoch 8673, training loss: 7086.45, average training loss: 7361.17, base loss: 16096.75
[INFO 2017-06-29 07:41:05,690 main.py:57] epoch 8674, training loss: 8078.82, average training loss: 7360.82, base loss: 16097.00
[INFO 2017-06-29 07:41:08,689 main.py:57] epoch 8675, training loss: 6602.84, average training loss: 7360.83, base loss: 16096.77
[INFO 2017-06-29 07:41:11,786 main.py:57] epoch 8676, training loss: 6818.88, average training loss: 7360.58, base loss: 16096.77
[INFO 2017-06-29 07:41:14,829 main.py:57] epoch 8677, training loss: 6926.99, average training loss: 7359.69, base loss: 16096.85
[INFO 2017-06-29 07:41:17,831 main.py:57] epoch 8678, training loss: 5989.77, average training loss: 7358.22, base loss: 16096.40
[INFO 2017-06-29 07:41:20,855 main.py:57] epoch 8679, training loss: 6872.44, average training loss: 7357.19, base loss: 16096.22
[INFO 2017-06-29 07:41:23,931 main.py:57] epoch 8680, training loss: 7270.40, average training loss: 7357.61, base loss: 16096.21
[INFO 2017-06-29 07:41:27,021 main.py:57] epoch 8681, training loss: 6768.17, average training loss: 7356.24, base loss: 16096.00
[INFO 2017-06-29 07:41:30,090 main.py:57] epoch 8682, training loss: 7378.44, average training loss: 7356.79, base loss: 16096.14
[INFO 2017-06-29 07:41:33,186 main.py:57] epoch 8683, training loss: 7212.38, average training loss: 7356.92, base loss: 16096.20
[INFO 2017-06-29 07:41:36,316 main.py:57] epoch 8684, training loss: 6901.60, average training loss: 7356.80, base loss: 16095.70
[INFO 2017-06-29 07:41:39,415 main.py:57] epoch 8685, training loss: 7014.13, average training loss: 7357.51, base loss: 16095.62
[INFO 2017-06-29 07:41:42,413 main.py:57] epoch 8686, training loss: 7041.21, average training loss: 7357.49, base loss: 16095.20
[INFO 2017-06-29 07:41:45,408 main.py:57] epoch 8687, training loss: 7388.25, average training loss: 7357.35, base loss: 16095.12
[INFO 2017-06-29 07:41:48,450 main.py:57] epoch 8688, training loss: 7057.66, average training loss: 7356.71, base loss: 16094.79
[INFO 2017-06-29 07:41:51,503 main.py:57] epoch 8689, training loss: 7643.29, average training loss: 7355.79, base loss: 16094.94
[INFO 2017-06-29 07:41:54,616 main.py:57] epoch 8690, training loss: 6671.12, average training loss: 7354.45, base loss: 16094.84
[INFO 2017-06-29 07:41:57,670 main.py:57] epoch 8691, training loss: 7351.52, average training loss: 7354.76, base loss: 16095.17
[INFO 2017-06-29 07:42:00,720 main.py:57] epoch 8692, training loss: 7027.39, average training loss: 7354.04, base loss: 16095.16
[INFO 2017-06-29 07:42:03,793 main.py:57] epoch 8693, training loss: 7056.66, average training loss: 7352.98, base loss: 16095.27
[INFO 2017-06-29 07:42:06,801 main.py:57] epoch 8694, training loss: 6917.44, average training loss: 7352.97, base loss: 16094.85
[INFO 2017-06-29 07:42:09,806 main.py:57] epoch 8695, training loss: 7885.57, average training loss: 7352.52, base loss: 16095.03
[INFO 2017-06-29 07:42:12,950 main.py:57] epoch 8696, training loss: 7584.88, average training loss: 7353.37, base loss: 16095.09
[INFO 2017-06-29 07:42:16,012 main.py:57] epoch 8697, training loss: 6713.47, average training loss: 7351.82, base loss: 16095.20
[INFO 2017-06-29 07:42:19,073 main.py:57] epoch 8698, training loss: 7473.76, average training loss: 7351.71, base loss: 16094.78
[INFO 2017-06-29 07:42:22,144 main.py:57] epoch 8699, training loss: 7197.31, average training loss: 7351.97, base loss: 16094.56
[INFO 2017-06-29 07:42:22,145 main.py:59] epoch 8699, testing
[INFO 2017-06-29 07:42:34,696 main.py:104] average testing loss: 7552.43, base loss: 15640.26
[INFO 2017-06-29 07:42:34,696 main.py:105] improve_loss: 8087.83, improve_percent: 0.52
[INFO 2017-06-29 07:42:34,697 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:42:37,797 main.py:57] epoch 8700, training loss: 6624.67, average training loss: 7351.63, base loss: 16094.35
[INFO 2017-06-29 07:42:40,894 main.py:57] epoch 8701, training loss: 7190.66, average training loss: 7352.56, base loss: 16094.71
[INFO 2017-06-29 07:42:43,971 main.py:57] epoch 8702, training loss: 7971.84, average training loss: 7353.17, base loss: 16095.47
[INFO 2017-06-29 07:42:47,038 main.py:57] epoch 8703, training loss: 8975.98, average training loss: 7355.42, base loss: 16095.96
[INFO 2017-06-29 07:42:50,090 main.py:57] epoch 8704, training loss: 8188.30, average training loss: 7356.60, base loss: 16096.31
[INFO 2017-06-29 07:42:53,056 main.py:57] epoch 8705, training loss: 7559.20, average training loss: 7355.59, base loss: 16096.63
[INFO 2017-06-29 07:42:56,076 main.py:57] epoch 8706, training loss: 7815.95, average training loss: 7355.00, base loss: 16097.16
[INFO 2017-06-29 07:42:59,202 main.py:57] epoch 8707, training loss: 7333.80, average training loss: 7353.78, base loss: 16097.51
[INFO 2017-06-29 07:43:02,294 main.py:57] epoch 8708, training loss: 7424.07, average training loss: 7354.20, base loss: 16097.15
[INFO 2017-06-29 07:43:05,304 main.py:57] epoch 8709, training loss: 7365.03, average training loss: 7354.23, base loss: 16096.67
[INFO 2017-06-29 07:43:08,362 main.py:57] epoch 8710, training loss: 7004.90, average training loss: 7353.90, base loss: 16096.39
[INFO 2017-06-29 07:43:11,455 main.py:57] epoch 8711, training loss: 8263.48, average training loss: 7354.77, base loss: 16096.55
[INFO 2017-06-29 07:43:14,489 main.py:57] epoch 8712, training loss: 7026.24, average training loss: 7354.72, base loss: 16096.60
[INFO 2017-06-29 07:43:17,535 main.py:57] epoch 8713, training loss: 6600.49, average training loss: 7354.21, base loss: 16096.37
[INFO 2017-06-29 07:43:20,635 main.py:57] epoch 8714, training loss: 6889.98, average training loss: 7353.27, base loss: 16095.78
[INFO 2017-06-29 07:43:23,656 main.py:57] epoch 8715, training loss: 7419.24, average training loss: 7353.01, base loss: 16095.48
[INFO 2017-06-29 07:43:26,726 main.py:57] epoch 8716, training loss: 7119.09, average training loss: 7352.66, base loss: 16095.54
[INFO 2017-06-29 07:43:29,762 main.py:57] epoch 8717, training loss: 6726.91, average training loss: 7353.08, base loss: 16095.05
[INFO 2017-06-29 07:43:32,832 main.py:57] epoch 8718, training loss: 6402.47, average training loss: 7350.90, base loss: 16094.52
[INFO 2017-06-29 07:43:35,875 main.py:57] epoch 8719, training loss: 6725.41, average training loss: 7350.15, base loss: 16094.20
[INFO 2017-06-29 07:43:38,945 main.py:57] epoch 8720, training loss: 7325.04, average training loss: 7349.72, base loss: 16093.77
[INFO 2017-06-29 07:43:42,123 main.py:57] epoch 8721, training loss: 8230.35, average training loss: 7350.28, base loss: 16094.25
[INFO 2017-06-29 07:43:45,149 main.py:57] epoch 8722, training loss: 8395.87, average training loss: 7351.53, base loss: 16094.43
[INFO 2017-06-29 07:43:48,219 main.py:57] epoch 8723, training loss: 6953.63, average training loss: 7349.74, base loss: 16094.22
[INFO 2017-06-29 07:43:51,345 main.py:57] epoch 8724, training loss: 7347.28, average training loss: 7348.57, base loss: 16094.41
[INFO 2017-06-29 07:43:54,430 main.py:57] epoch 8725, training loss: 7134.16, average training loss: 7348.43, base loss: 16094.24
[INFO 2017-06-29 07:43:57,495 main.py:57] epoch 8726, training loss: 7013.62, average training loss: 7348.12, base loss: 16094.07
[INFO 2017-06-29 07:44:00,518 main.py:57] epoch 8727, training loss: 7152.37, average training loss: 7347.99, base loss: 16094.09
[INFO 2017-06-29 07:44:03,534 main.py:57] epoch 8728, training loss: 7419.51, average training loss: 7348.09, base loss: 16093.90
[INFO 2017-06-29 07:44:06,609 main.py:57] epoch 8729, training loss: 7087.38, average training loss: 7347.65, base loss: 16093.67
[INFO 2017-06-29 07:44:09,711 main.py:57] epoch 8730, training loss: 6891.27, average training loss: 7347.36, base loss: 16093.50
[INFO 2017-06-29 07:44:12,789 main.py:57] epoch 8731, training loss: 7448.16, average training loss: 7345.98, base loss: 16093.75
[INFO 2017-06-29 07:44:15,825 main.py:57] epoch 8732, training loss: 7445.47, average training loss: 7345.18, base loss: 16093.66
[INFO 2017-06-29 07:44:18,917 main.py:57] epoch 8733, training loss: 7221.17, average training loss: 7345.23, base loss: 16093.99
[INFO 2017-06-29 07:44:22,024 main.py:57] epoch 8734, training loss: 6803.73, average training loss: 7345.04, base loss: 16093.93
[INFO 2017-06-29 07:44:25,144 main.py:57] epoch 8735, training loss: 7118.18, average training loss: 7345.05, base loss: 16093.83
[INFO 2017-06-29 07:44:28,225 main.py:57] epoch 8736, training loss: 6020.88, average training loss: 7342.57, base loss: 16093.46
[INFO 2017-06-29 07:44:31,251 main.py:57] epoch 8737, training loss: 7026.63, average training loss: 7342.69, base loss: 16093.52
[INFO 2017-06-29 07:44:34,302 main.py:57] epoch 8738, training loss: 8028.40, average training loss: 7342.88, base loss: 16093.96
[INFO 2017-06-29 07:44:37,337 main.py:57] epoch 8739, training loss: 8189.42, average training loss: 7343.43, base loss: 16094.55
[INFO 2017-06-29 07:44:40,421 main.py:57] epoch 8740, training loss: 7179.42, average training loss: 7343.68, base loss: 16094.31
[INFO 2017-06-29 07:44:43,463 main.py:57] epoch 8741, training loss: 6410.04, average training loss: 7342.40, base loss: 16094.00
[INFO 2017-06-29 07:44:46,522 main.py:57] epoch 8742, training loss: 7223.64, average training loss: 7342.46, base loss: 16094.36
[INFO 2017-06-29 07:44:49,546 main.py:57] epoch 8743, training loss: 7470.07, average training loss: 7342.37, base loss: 16094.64
[INFO 2017-06-29 07:44:52,655 main.py:57] epoch 8744, training loss: 7909.34, average training loss: 7342.91, base loss: 16095.31
[INFO 2017-06-29 07:44:55,733 main.py:57] epoch 8745, training loss: 7264.80, average training loss: 7343.05, base loss: 16095.26
[INFO 2017-06-29 07:44:58,771 main.py:57] epoch 8746, training loss: 7213.21, average training loss: 7342.87, base loss: 16095.66
[INFO 2017-06-29 07:45:01,839 main.py:57] epoch 8747, training loss: 9025.83, average training loss: 7344.89, base loss: 16096.54
[INFO 2017-06-29 07:45:04,935 main.py:57] epoch 8748, training loss: 6933.22, average training loss: 7343.24, base loss: 16096.10
[INFO 2017-06-29 07:45:07,965 main.py:57] epoch 8749, training loss: 7391.84, average training loss: 7343.10, base loss: 16095.85
[INFO 2017-06-29 07:45:11,056 main.py:57] epoch 8750, training loss: 7428.01, average training loss: 7343.54, base loss: 16095.99
[INFO 2017-06-29 07:45:14,094 main.py:57] epoch 8751, training loss: 7197.37, average training loss: 7342.65, base loss: 16096.17
[INFO 2017-06-29 07:45:17,119 main.py:57] epoch 8752, training loss: 7746.31, average training loss: 7343.10, base loss: 16095.80
[INFO 2017-06-29 07:45:20,147 main.py:57] epoch 8753, training loss: 7010.36, average training loss: 7343.36, base loss: 16095.58
[INFO 2017-06-29 07:45:23,222 main.py:57] epoch 8754, training loss: 6954.62, average training loss: 7342.60, base loss: 16095.40
[INFO 2017-06-29 07:45:26,351 main.py:57] epoch 8755, training loss: 7307.88, average training loss: 7342.70, base loss: 16095.18
[INFO 2017-06-29 07:45:29,376 main.py:57] epoch 8756, training loss: 6840.91, average training loss: 7342.14, base loss: 16094.94
[INFO 2017-06-29 07:45:32,537 main.py:57] epoch 8757, training loss: 7400.36, average training loss: 7341.81, base loss: 16094.81
[INFO 2017-06-29 07:45:35,612 main.py:57] epoch 8758, training loss: 8664.59, average training loss: 7342.10, base loss: 16095.39
[INFO 2017-06-29 07:45:38,601 main.py:57] epoch 8759, training loss: 7569.43, average training loss: 7341.31, base loss: 16095.29
[INFO 2017-06-29 07:45:41,657 main.py:57] epoch 8760, training loss: 7409.84, average training loss: 7340.27, base loss: 16094.97
[INFO 2017-06-29 07:45:44,757 main.py:57] epoch 8761, training loss: 7408.29, average training loss: 7340.06, base loss: 16095.07
[INFO 2017-06-29 07:45:47,792 main.py:57] epoch 8762, training loss: 6698.71, average training loss: 7337.63, base loss: 16095.02
[INFO 2017-06-29 07:45:50,872 main.py:57] epoch 8763, training loss: 7716.90, average training loss: 7338.54, base loss: 16095.61
[INFO 2017-06-29 07:45:53,957 main.py:57] epoch 8764, training loss: 7254.12, average training loss: 7338.64, base loss: 16095.40
[INFO 2017-06-29 07:45:56,975 main.py:57] epoch 8765, training loss: 6655.21, average training loss: 7338.82, base loss: 16095.38
[INFO 2017-06-29 07:46:00,069 main.py:57] epoch 8766, training loss: 7606.72, average training loss: 7338.65, base loss: 16095.43
[INFO 2017-06-29 07:46:03,147 main.py:57] epoch 8767, training loss: 6877.47, average training loss: 7338.95, base loss: 16095.30
[INFO 2017-06-29 07:46:06,139 main.py:57] epoch 8768, training loss: 7845.58, average training loss: 7339.98, base loss: 16095.47
[INFO 2017-06-29 07:46:09,257 main.py:57] epoch 8769, training loss: 6289.66, average training loss: 7338.72, base loss: 16095.07
[INFO 2017-06-29 07:46:12,277 main.py:57] epoch 8770, training loss: 7040.52, average training loss: 7338.65, base loss: 16094.89
[INFO 2017-06-29 07:46:15,380 main.py:57] epoch 8771, training loss: 7105.67, average training loss: 7338.19, base loss: 16094.73
[INFO 2017-06-29 07:46:18,465 main.py:57] epoch 8772, training loss: 6948.10, average training loss: 7338.12, base loss: 16094.33
[INFO 2017-06-29 07:46:21,463 main.py:57] epoch 8773, training loss: 6310.63, average training loss: 7336.85, base loss: 16093.58
[INFO 2017-06-29 07:46:24,510 main.py:57] epoch 8774, training loss: 7431.31, average training loss: 7336.62, base loss: 16093.75
[INFO 2017-06-29 07:46:27,614 main.py:57] epoch 8775, training loss: 6374.22, average training loss: 7336.03, base loss: 16093.25
[INFO 2017-06-29 07:46:30,679 main.py:57] epoch 8776, training loss: 6867.10, average training loss: 7335.90, base loss: 16092.66
[INFO 2017-06-29 07:46:33,737 main.py:57] epoch 8777, training loss: 7019.99, average training loss: 7334.66, base loss: 16092.37
[INFO 2017-06-29 07:46:36,837 main.py:57] epoch 8778, training loss: 7436.65, average training loss: 7334.59, base loss: 16092.31
[INFO 2017-06-29 07:46:39,912 main.py:57] epoch 8779, training loss: 6758.58, average training loss: 7334.52, base loss: 16091.95
[INFO 2017-06-29 07:46:42,978 main.py:57] epoch 8780, training loss: 8557.05, average training loss: 7336.09, base loss: 16092.35
[INFO 2017-06-29 07:46:46,060 main.py:57] epoch 8781, training loss: 7296.43, average training loss: 7336.60, base loss: 16092.44
[INFO 2017-06-29 07:46:49,071 main.py:57] epoch 8782, training loss: 7796.46, average training loss: 7337.29, base loss: 16092.64
[INFO 2017-06-29 07:46:52,136 main.py:57] epoch 8783, training loss: 7123.02, average training loss: 7336.94, base loss: 16092.66
[INFO 2017-06-29 07:46:55,207 main.py:57] epoch 8784, training loss: 6965.13, average training loss: 7336.63, base loss: 16092.34
[INFO 2017-06-29 07:46:58,210 main.py:57] epoch 8785, training loss: 7242.42, average training loss: 7337.23, base loss: 16092.39
[INFO 2017-06-29 07:47:01,210 main.py:57] epoch 8786, training loss: 6621.72, average training loss: 7337.10, base loss: 16091.88
[INFO 2017-06-29 07:47:04,256 main.py:57] epoch 8787, training loss: 7881.60, average training loss: 7338.27, base loss: 16092.30
[INFO 2017-06-29 07:47:07,360 main.py:57] epoch 8788, training loss: 6920.38, average training loss: 7337.35, base loss: 16091.91
[INFO 2017-06-29 07:47:10,398 main.py:57] epoch 8789, training loss: 7516.83, average training loss: 7338.16, base loss: 16091.74
[INFO 2017-06-29 07:47:13,355 main.py:57] epoch 8790, training loss: 6907.67, average training loss: 7338.57, base loss: 16091.81
[INFO 2017-06-29 07:47:16,406 main.py:57] epoch 8791, training loss: 7852.22, average training loss: 7340.03, base loss: 16092.14
[INFO 2017-06-29 07:47:19,397 main.py:57] epoch 8792, training loss: 7594.28, average training loss: 7338.82, base loss: 16092.03
[INFO 2017-06-29 07:47:22,506 main.py:57] epoch 8793, training loss: 7205.15, average training loss: 7338.36, base loss: 16092.11
[INFO 2017-06-29 07:47:25,563 main.py:57] epoch 8794, training loss: 7487.58, average training loss: 7338.61, base loss: 16092.48
[INFO 2017-06-29 07:47:28,596 main.py:57] epoch 8795, training loss: 7201.83, average training loss: 7338.83, base loss: 16092.95
[INFO 2017-06-29 07:47:31,751 main.py:57] epoch 8796, training loss: 7116.24, average training loss: 7339.04, base loss: 16092.77
[INFO 2017-06-29 07:47:34,789 main.py:57] epoch 8797, training loss: 7663.82, average training loss: 7338.42, base loss: 16093.03
[INFO 2017-06-29 07:47:37,863 main.py:57] epoch 8798, training loss: 7457.17, average training loss: 7338.61, base loss: 16092.72
[INFO 2017-06-29 07:47:40,969 main.py:57] epoch 8799, training loss: 7787.36, average training loss: 7340.53, base loss: 16092.72
[INFO 2017-06-29 07:47:40,970 main.py:59] epoch 8799, testing
[INFO 2017-06-29 07:47:53,644 main.py:104] average testing loss: 8352.81, base loss: 17253.22
[INFO 2017-06-29 07:47:53,644 main.py:105] improve_loss: 8900.41, improve_percent: 0.52
[INFO 2017-06-29 07:47:53,645 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:47:56,743 main.py:57] epoch 8800, training loss: 6116.93, average training loss: 7339.44, base loss: 16092.47
[INFO 2017-06-29 07:47:59,834 main.py:57] epoch 8801, training loss: 8070.27, average training loss: 7339.88, base loss: 16092.73
[INFO 2017-06-29 07:48:02,885 main.py:57] epoch 8802, training loss: 6832.84, average training loss: 7340.08, base loss: 16092.32
[INFO 2017-06-29 07:48:05,901 main.py:57] epoch 8803, training loss: 6495.72, average training loss: 7340.06, base loss: 16091.66
[INFO 2017-06-29 07:48:08,951 main.py:57] epoch 8804, training loss: 7369.57, average training loss: 7340.80, base loss: 16091.59
[INFO 2017-06-29 07:48:12,001 main.py:57] epoch 8805, training loss: 7084.91, average training loss: 7341.02, base loss: 16091.66
[INFO 2017-06-29 07:48:15,088 main.py:57] epoch 8806, training loss: 8444.08, average training loss: 7341.14, base loss: 16091.91
[INFO 2017-06-29 07:48:18,147 main.py:57] epoch 8807, training loss: 6441.33, average training loss: 7338.63, base loss: 16091.75
[INFO 2017-06-29 07:48:21,223 main.py:57] epoch 8808, training loss: 8780.16, average training loss: 7340.52, base loss: 16092.69
[INFO 2017-06-29 07:48:24,355 main.py:57] epoch 8809, training loss: 7735.20, average training loss: 7340.05, base loss: 16092.83
[INFO 2017-06-29 07:48:27,420 main.py:57] epoch 8810, training loss: 7500.16, average training loss: 7340.18, base loss: 16092.94
[INFO 2017-06-29 07:48:30,459 main.py:57] epoch 8811, training loss: 9327.20, average training loss: 7341.59, base loss: 16093.73
[INFO 2017-06-29 07:48:33,478 main.py:57] epoch 8812, training loss: 7543.85, average training loss: 7341.62, base loss: 16093.51
[INFO 2017-06-29 07:48:36,546 main.py:57] epoch 8813, training loss: 6549.09, average training loss: 7341.34, base loss: 16093.30
[INFO 2017-06-29 07:48:39,648 main.py:57] epoch 8814, training loss: 7699.15, average training loss: 7341.48, base loss: 16093.43
[INFO 2017-06-29 07:48:42,676 main.py:57] epoch 8815, training loss: 7517.97, average training loss: 7341.38, base loss: 16093.66
[INFO 2017-06-29 07:48:45,727 main.py:57] epoch 8816, training loss: 7253.83, average training loss: 7341.56, base loss: 16093.79
[INFO 2017-06-29 07:48:48,752 main.py:57] epoch 8817, training loss: 7275.98, average training loss: 7341.18, base loss: 16094.00
[INFO 2017-06-29 07:48:51,796 main.py:57] epoch 8818, training loss: 8564.71, average training loss: 7342.85, base loss: 16094.79
[INFO 2017-06-29 07:48:54,791 main.py:57] epoch 8819, training loss: 8276.09, average training loss: 7343.02, base loss: 16095.64
[INFO 2017-06-29 07:48:57,818 main.py:57] epoch 8820, training loss: 7761.42, average training loss: 7343.10, base loss: 16096.09
[INFO 2017-06-29 07:49:00,969 main.py:57] epoch 8821, training loss: 6626.11, average training loss: 7343.17, base loss: 16095.29
[INFO 2017-06-29 07:49:04,023 main.py:57] epoch 8822, training loss: 7288.55, average training loss: 7342.91, base loss: 16095.22
[INFO 2017-06-29 07:49:07,073 main.py:57] epoch 8823, training loss: 7073.02, average training loss: 7343.15, base loss: 16095.28
[INFO 2017-06-29 07:49:10,158 main.py:57] epoch 8824, training loss: 7535.73, average training loss: 7343.24, base loss: 16095.12
[INFO 2017-06-29 07:49:13,208 main.py:57] epoch 8825, training loss: 6868.82, average training loss: 7343.19, base loss: 16094.73
[INFO 2017-06-29 07:49:16,209 main.py:57] epoch 8826, training loss: 7911.72, average training loss: 7343.51, base loss: 16095.00
[INFO 2017-06-29 07:49:19,226 main.py:57] epoch 8827, training loss: 7682.02, average training loss: 7342.93, base loss: 16094.99
[INFO 2017-06-29 07:49:22,314 main.py:57] epoch 8828, training loss: 6770.91, average training loss: 7341.92, base loss: 16094.66
[INFO 2017-06-29 07:49:25,339 main.py:57] epoch 8829, training loss: 8315.99, average training loss: 7343.27, base loss: 16095.13
[INFO 2017-06-29 07:49:28,399 main.py:57] epoch 8830, training loss: 8805.19, average training loss: 7345.01, base loss: 16095.27
[INFO 2017-06-29 07:49:31,518 main.py:57] epoch 8831, training loss: 7705.18, average training loss: 7345.60, base loss: 16095.06
[INFO 2017-06-29 07:49:34,568 main.py:57] epoch 8832, training loss: 7722.74, average training loss: 7346.04, base loss: 16095.55
[INFO 2017-06-29 07:49:37,599 main.py:57] epoch 8833, training loss: 7853.87, average training loss: 7345.38, base loss: 16095.36
[INFO 2017-06-29 07:49:40,700 main.py:57] epoch 8834, training loss: 7473.53, average training loss: 7345.33, base loss: 16095.41
[INFO 2017-06-29 07:49:43,737 main.py:57] epoch 8835, training loss: 7486.37, average training loss: 7345.51, base loss: 16095.54
[INFO 2017-06-29 07:49:46,751 main.py:57] epoch 8836, training loss: 7765.48, average training loss: 7345.68, base loss: 16095.56
[INFO 2017-06-29 07:49:49,821 main.py:57] epoch 8837, training loss: 9059.85, average training loss: 7346.06, base loss: 16096.28
[INFO 2017-06-29 07:49:52,870 main.py:57] epoch 8838, training loss: 7490.82, average training loss: 7346.57, base loss: 16096.32
[INFO 2017-06-29 07:49:55,979 main.py:57] epoch 8839, training loss: 7229.98, average training loss: 7347.06, base loss: 16096.68
[INFO 2017-06-29 07:49:59,056 main.py:57] epoch 8840, training loss: 7701.34, average training loss: 7347.95, base loss: 16096.65
[INFO 2017-06-29 07:50:02,069 main.py:57] epoch 8841, training loss: 7397.47, average training loss: 7347.43, base loss: 16096.31
[INFO 2017-06-29 07:50:05,077 main.py:57] epoch 8842, training loss: 7835.78, average training loss: 7347.59, base loss: 16096.68
[INFO 2017-06-29 07:50:08,159 main.py:57] epoch 8843, training loss: 7587.34, average training loss: 7347.61, base loss: 16096.99
[INFO 2017-06-29 07:50:11,213 main.py:57] epoch 8844, training loss: 7697.10, average training loss: 7348.02, base loss: 16097.48
[INFO 2017-06-29 07:50:14,244 main.py:57] epoch 8845, training loss: 6365.83, average training loss: 7347.58, base loss: 16097.01
[INFO 2017-06-29 07:50:17,269 main.py:57] epoch 8846, training loss: 7221.08, average training loss: 7347.94, base loss: 16097.26
[INFO 2017-06-29 07:50:20,355 main.py:57] epoch 8847, training loss: 7505.68, average training loss: 7348.13, base loss: 16097.47
[INFO 2017-06-29 07:50:23,344 main.py:57] epoch 8848, training loss: 7484.31, average training loss: 7348.27, base loss: 16097.82
[INFO 2017-06-29 07:50:26,405 main.py:57] epoch 8849, training loss: 7735.98, average training loss: 7348.88, base loss: 16098.00
[INFO 2017-06-29 07:50:29,402 main.py:57] epoch 8850, training loss: 7260.03, average training loss: 7347.99, base loss: 16098.38
[INFO 2017-06-29 07:50:32,516 main.py:57] epoch 8851, training loss: 7173.26, average training loss: 7348.27, base loss: 16098.46
[INFO 2017-06-29 07:50:35,529 main.py:57] epoch 8852, training loss: 7017.75, average training loss: 7348.31, base loss: 16098.36
[INFO 2017-06-29 07:50:38,585 main.py:57] epoch 8853, training loss: 8015.78, average training loss: 7348.37, base loss: 16098.35
[INFO 2017-06-29 07:50:41,632 main.py:57] epoch 8854, training loss: 7375.22, average training loss: 7348.27, base loss: 16098.57
[INFO 2017-06-29 07:50:44,688 main.py:57] epoch 8855, training loss: 8570.10, average training loss: 7349.50, base loss: 16099.36
[INFO 2017-06-29 07:50:47,763 main.py:57] epoch 8856, training loss: 8545.48, average training loss: 7351.02, base loss: 16100.32
[INFO 2017-06-29 07:50:50,802 main.py:57] epoch 8857, training loss: 7714.18, average training loss: 7350.82, base loss: 16100.44
[INFO 2017-06-29 07:50:53,831 main.py:57] epoch 8858, training loss: 7071.42, average training loss: 7351.07, base loss: 16100.21
[INFO 2017-06-29 07:50:56,907 main.py:57] epoch 8859, training loss: 7918.54, average training loss: 7350.92, base loss: 16100.31
[INFO 2017-06-29 07:51:00,034 main.py:57] epoch 8860, training loss: 7106.51, average training loss: 7350.14, base loss: 16100.63
[INFO 2017-06-29 07:51:03,102 main.py:57] epoch 8861, training loss: 7943.42, average training loss: 7351.32, base loss: 16101.17
[INFO 2017-06-29 07:51:06,162 main.py:57] epoch 8862, training loss: 7339.19, average training loss: 7351.01, base loss: 16101.54
[INFO 2017-06-29 07:51:09,161 main.py:57] epoch 8863, training loss: 8754.43, average training loss: 7352.30, base loss: 16102.17
[INFO 2017-06-29 07:51:12,243 main.py:57] epoch 8864, training loss: 7146.82, average training loss: 7352.21, base loss: 16101.85
[INFO 2017-06-29 07:51:15,275 main.py:57] epoch 8865, training loss: 7705.69, average training loss: 7351.75, base loss: 16101.58
[INFO 2017-06-29 07:51:18,343 main.py:57] epoch 8866, training loss: 7997.87, average training loss: 7353.09, base loss: 16101.87
[INFO 2017-06-29 07:51:21,369 main.py:57] epoch 8867, training loss: 7905.59, average training loss: 7353.65, base loss: 16101.86
[INFO 2017-06-29 07:51:24,522 main.py:57] epoch 8868, training loss: 7748.31, average training loss: 7354.03, base loss: 16102.09
[INFO 2017-06-29 07:51:27,573 main.py:57] epoch 8869, training loss: 6995.58, average training loss: 7353.03, base loss: 16102.46
[INFO 2017-06-29 07:51:30,679 main.py:57] epoch 8870, training loss: 7029.59, average training loss: 7352.18, base loss: 16102.44
[INFO 2017-06-29 07:51:33,780 main.py:57] epoch 8871, training loss: 6581.88, average training loss: 7351.92, base loss: 16102.48
[INFO 2017-06-29 07:51:36,853 main.py:57] epoch 8872, training loss: 7758.48, average training loss: 7351.86, base loss: 16102.95
[INFO 2017-06-29 07:51:39,909 main.py:57] epoch 8873, training loss: 7469.41, average training loss: 7352.67, base loss: 16103.18
[INFO 2017-06-29 07:51:42,942 main.py:57] epoch 8874, training loss: 7462.82, average training loss: 7353.18, base loss: 16103.15
[INFO 2017-06-29 07:51:46,017 main.py:57] epoch 8875, training loss: 7252.91, average training loss: 7353.03, base loss: 16103.25
[INFO 2017-06-29 07:51:49,040 main.py:57] epoch 8876, training loss: 7349.21, average training loss: 7353.23, base loss: 16103.03
[INFO 2017-06-29 07:51:52,113 main.py:57] epoch 8877, training loss: 6643.86, average training loss: 7351.64, base loss: 16102.34
[INFO 2017-06-29 07:51:55,168 main.py:57] epoch 8878, training loss: 7634.55, average training loss: 7351.81, base loss: 16102.91
[INFO 2017-06-29 07:51:58,253 main.py:57] epoch 8879, training loss: 6949.20, average training loss: 7352.68, base loss: 16102.80
[INFO 2017-06-29 07:52:01,327 main.py:57] epoch 8880, training loss: 6985.94, average training loss: 7352.26, base loss: 16102.55
[INFO 2017-06-29 07:52:04,370 main.py:57] epoch 8881, training loss: 7611.94, average training loss: 7353.67, base loss: 16102.43
[INFO 2017-06-29 07:52:07,436 main.py:57] epoch 8882, training loss: 7413.88, average training loss: 7354.31, base loss: 16102.61
[INFO 2017-06-29 07:52:10,494 main.py:57] epoch 8883, training loss: 7468.98, average training loss: 7353.65, base loss: 16102.57
[INFO 2017-06-29 07:52:13,581 main.py:57] epoch 8884, training loss: 6005.91, average training loss: 7351.41, base loss: 16101.98
[INFO 2017-06-29 07:52:16,692 main.py:57] epoch 8885, training loss: 6836.41, average training loss: 7350.72, base loss: 16101.87
[INFO 2017-06-29 07:52:19,785 main.py:57] epoch 8886, training loss: 7052.70, average training loss: 7350.82, base loss: 16101.85
[INFO 2017-06-29 07:52:22,835 main.py:57] epoch 8887, training loss: 6759.76, average training loss: 7350.35, base loss: 16101.78
[INFO 2017-06-29 07:52:25,898 main.py:57] epoch 8888, training loss: 6781.11, average training loss: 7349.59, base loss: 16101.60
[INFO 2017-06-29 07:52:28,988 main.py:57] epoch 8889, training loss: 8308.92, average training loss: 7350.92, base loss: 16101.86
[INFO 2017-06-29 07:52:31,973 main.py:57] epoch 8890, training loss: 7558.16, average training loss: 7350.29, base loss: 16102.07
[INFO 2017-06-29 07:52:35,042 main.py:57] epoch 8891, training loss: 7553.55, average training loss: 7350.80, base loss: 16102.38
[INFO 2017-06-29 07:52:38,097 main.py:57] epoch 8892, training loss: 7350.06, average training loss: 7350.67, base loss: 16102.62
[INFO 2017-06-29 07:52:41,098 main.py:57] epoch 8893, training loss: 7060.19, average training loss: 7350.54, base loss: 16102.77
[INFO 2017-06-29 07:52:44,140 main.py:57] epoch 8894, training loss: 7335.59, average training loss: 7350.90, base loss: 16102.62
[INFO 2017-06-29 07:52:47,201 main.py:57] epoch 8895, training loss: 8500.10, average training loss: 7351.99, base loss: 16103.08
[INFO 2017-06-29 07:52:50,230 main.py:57] epoch 8896, training loss: 7022.16, average training loss: 7352.03, base loss: 16102.95
[INFO 2017-06-29 07:52:53,259 main.py:57] epoch 8897, training loss: 7087.27, average training loss: 7351.88, base loss: 16103.23
[INFO 2017-06-29 07:52:56,352 main.py:57] epoch 8898, training loss: 9315.19, average training loss: 7353.65, base loss: 16103.97
[INFO 2017-06-29 07:52:59,403 main.py:57] epoch 8899, training loss: 7018.77, average training loss: 7354.43, base loss: 16103.52
[INFO 2017-06-29 07:52:59,404 main.py:59] epoch 8899, testing
[INFO 2017-06-29 07:53:12,061 main.py:104] average testing loss: 8020.01, base loss: 16694.54
[INFO 2017-06-29 07:53:12,061 main.py:105] improve_loss: 8674.53, improve_percent: 0.52
[INFO 2017-06-29 07:53:12,062 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:53:15,042 main.py:57] epoch 8900, training loss: 6867.74, average training loss: 7353.05, base loss: 16103.31
[INFO 2017-06-29 07:53:18,091 main.py:57] epoch 8901, training loss: 7476.14, average training loss: 7352.83, base loss: 16103.40
[INFO 2017-06-29 07:53:21,191 main.py:57] epoch 8902, training loss: 7262.67, average training loss: 7352.19, base loss: 16103.66
[INFO 2017-06-29 07:53:24,264 main.py:57] epoch 8903, training loss: 7856.87, average training loss: 7352.64, base loss: 16103.80
[INFO 2017-06-29 07:53:27,345 main.py:57] epoch 8904, training loss: 7654.37, average training loss: 7353.60, base loss: 16103.63
[INFO 2017-06-29 07:53:30,400 main.py:57] epoch 8905, training loss: 8241.12, average training loss: 7354.34, base loss: 16104.09
[INFO 2017-06-29 07:53:33,476 main.py:57] epoch 8906, training loss: 7165.05, average training loss: 7353.92, base loss: 16103.93
[INFO 2017-06-29 07:53:36,631 main.py:57] epoch 8907, training loss: 6583.56, average training loss: 7352.47, base loss: 16103.88
[INFO 2017-06-29 07:53:39,747 main.py:57] epoch 8908, training loss: 6993.15, average training loss: 7351.82, base loss: 16103.97
[INFO 2017-06-29 07:53:42,766 main.py:57] epoch 8909, training loss: 7506.60, average training loss: 7351.64, base loss: 16104.02
[INFO 2017-06-29 07:53:45,827 main.py:57] epoch 8910, training loss: 8167.49, average training loss: 7353.05, base loss: 16104.48
[INFO 2017-06-29 07:53:48,867 main.py:57] epoch 8911, training loss: 7669.79, average training loss: 7353.89, base loss: 16104.81
[INFO 2017-06-29 07:53:51,936 main.py:57] epoch 8912, training loss: 6486.79, average training loss: 7352.94, base loss: 16104.28
[INFO 2017-06-29 07:53:54,959 main.py:57] epoch 8913, training loss: 7731.08, average training loss: 7353.21, base loss: 16104.19
[INFO 2017-06-29 07:53:57,935 main.py:57] epoch 8914, training loss: 8815.42, average training loss: 7355.37, base loss: 16104.57
[INFO 2017-06-29 07:54:00,932 main.py:57] epoch 8915, training loss: 7882.76, average training loss: 7356.60, base loss: 16104.64
[INFO 2017-06-29 07:54:04,039 main.py:57] epoch 8916, training loss: 6459.64, average training loss: 7355.89, base loss: 16104.23
[INFO 2017-06-29 07:54:07,124 main.py:57] epoch 8917, training loss: 6992.34, average training loss: 7354.20, base loss: 16103.79
[INFO 2017-06-29 07:54:10,229 main.py:57] epoch 8918, training loss: 6857.81, average training loss: 7353.46, base loss: 16103.53
[INFO 2017-06-29 07:54:13,256 main.py:57] epoch 8919, training loss: 7309.28, average training loss: 7353.37, base loss: 16103.35
[INFO 2017-06-29 07:54:16,295 main.py:57] epoch 8920, training loss: 6948.12, average training loss: 7353.22, base loss: 16103.16
[INFO 2017-06-29 07:54:19,338 main.py:57] epoch 8921, training loss: 7200.55, average training loss: 7353.15, base loss: 16103.12
[INFO 2017-06-29 07:54:22,378 main.py:57] epoch 8922, training loss: 6749.07, average training loss: 7352.89, base loss: 16102.93
[INFO 2017-06-29 07:54:25,421 main.py:57] epoch 8923, training loss: 7503.04, average training loss: 7352.29, base loss: 16103.05
[INFO 2017-06-29 07:54:28,478 main.py:57] epoch 8924, training loss: 6463.53, average training loss: 7351.59, base loss: 16102.96
[INFO 2017-06-29 07:54:31,464 main.py:57] epoch 8925, training loss: 8403.51, average training loss: 7352.71, base loss: 16103.22
[INFO 2017-06-29 07:54:34,528 main.py:57] epoch 8926, training loss: 7268.44, average training loss: 7352.78, base loss: 16103.29
[INFO 2017-06-29 07:54:37,663 main.py:57] epoch 8927, training loss: 7934.38, average training loss: 7353.55, base loss: 16103.65
[INFO 2017-06-29 07:54:40,726 main.py:57] epoch 8928, training loss: 6332.24, average training loss: 7352.99, base loss: 16102.82
[INFO 2017-06-29 07:54:43,744 main.py:57] epoch 8929, training loss: 7320.51, average training loss: 7352.33, base loss: 16103.15
[INFO 2017-06-29 07:54:46,735 main.py:57] epoch 8930, training loss: 6394.08, average training loss: 7352.30, base loss: 16102.81
[INFO 2017-06-29 07:54:49,783 main.py:57] epoch 8931, training loss: 7263.81, average training loss: 7352.23, base loss: 16103.32
[INFO 2017-06-29 07:54:52,865 main.py:57] epoch 8932, training loss: 7721.73, average training loss: 7353.16, base loss: 16103.47
[INFO 2017-06-29 07:54:55,925 main.py:57] epoch 8933, training loss: 6411.37, average training loss: 7352.30, base loss: 16103.20
[INFO 2017-06-29 07:54:58,929 main.py:57] epoch 8934, training loss: 6792.02, average training loss: 7352.08, base loss: 16102.80
[INFO 2017-06-29 07:55:01,908 main.py:57] epoch 8935, training loss: 6655.65, average training loss: 7350.19, base loss: 16102.44
[INFO 2017-06-29 07:55:04,931 main.py:57] epoch 8936, training loss: 7305.25, average training loss: 7350.29, base loss: 16102.58
[INFO 2017-06-29 07:55:08,018 main.py:57] epoch 8937, training loss: 6812.70, average training loss: 7349.93, base loss: 16102.25
[INFO 2017-06-29 07:55:11,024 main.py:57] epoch 8938, training loss: 8121.92, average training loss: 7350.33, base loss: 16102.27
[INFO 2017-06-29 07:55:14,096 main.py:57] epoch 8939, training loss: 6876.62, average training loss: 7350.19, base loss: 16102.00
[INFO 2017-06-29 07:55:17,082 main.py:57] epoch 8940, training loss: 7218.77, average training loss: 7349.71, base loss: 16101.72
[INFO 2017-06-29 07:55:20,111 main.py:57] epoch 8941, training loss: 7097.77, average training loss: 7349.56, base loss: 16101.75
[INFO 2017-06-29 07:55:23,137 main.py:57] epoch 8942, training loss: 7237.09, average training loss: 7349.09, base loss: 16101.41
[INFO 2017-06-29 07:55:26,201 main.py:57] epoch 8943, training loss: 6827.24, average training loss: 7347.75, base loss: 16101.23
[INFO 2017-06-29 07:55:29,364 main.py:57] epoch 8944, training loss: 8208.17, average training loss: 7348.54, base loss: 16101.59
[INFO 2017-06-29 07:55:32,480 main.py:57] epoch 8945, training loss: 7263.96, average training loss: 7348.59, base loss: 16101.81
[INFO 2017-06-29 07:55:35,512 main.py:57] epoch 8946, training loss: 6444.27, average training loss: 7347.97, base loss: 16101.42
[INFO 2017-06-29 07:55:38,524 main.py:57] epoch 8947, training loss: 7519.93, average training loss: 7348.21, base loss: 16101.57
[INFO 2017-06-29 07:55:41,591 main.py:57] epoch 8948, training loss: 7726.53, average training loss: 7348.61, base loss: 16101.75
[INFO 2017-06-29 07:55:44,669 main.py:57] epoch 8949, training loss: 7705.59, average training loss: 7348.15, base loss: 16101.94
[INFO 2017-06-29 07:55:47,717 main.py:57] epoch 8950, training loss: 7301.53, average training loss: 7348.88, base loss: 16101.61
[INFO 2017-06-29 07:55:50,755 main.py:57] epoch 8951, training loss: 7820.18, average training loss: 7349.32, base loss: 16101.53
[INFO 2017-06-29 07:55:53,836 main.py:57] epoch 8952, training loss: 6222.19, average training loss: 7347.24, base loss: 16101.23
[INFO 2017-06-29 07:55:56,864 main.py:57] epoch 8953, training loss: 7279.08, average training loss: 7347.61, base loss: 16101.52
[INFO 2017-06-29 07:55:59,951 main.py:57] epoch 8954, training loss: 8485.40, average training loss: 7348.36, base loss: 16101.94
[INFO 2017-06-29 07:56:03,018 main.py:57] epoch 8955, training loss: 7753.70, average training loss: 7348.72, base loss: 16102.14
[INFO 2017-06-29 07:56:06,048 main.py:57] epoch 8956, training loss: 6636.48, average training loss: 7347.78, base loss: 16102.03
[INFO 2017-06-29 07:56:09,130 main.py:57] epoch 8957, training loss: 7476.39, average training loss: 7348.00, base loss: 16102.20
[INFO 2017-06-29 07:56:12,204 main.py:57] epoch 8958, training loss: 7004.31, average training loss: 7346.90, base loss: 16102.15
[INFO 2017-06-29 07:56:15,275 main.py:57] epoch 8959, training loss: 6972.15, average training loss: 7347.17, base loss: 16102.26
[INFO 2017-06-29 07:56:18,352 main.py:57] epoch 8960, training loss: 7101.45, average training loss: 7347.48, base loss: 16102.57
[INFO 2017-06-29 07:56:21,411 main.py:57] epoch 8961, training loss: 7674.38, average training loss: 7347.41, base loss: 16102.76
[INFO 2017-06-29 07:56:24,488 main.py:57] epoch 8962, training loss: 7154.80, average training loss: 7346.80, base loss: 16102.57
[INFO 2017-06-29 07:56:27,566 main.py:57] epoch 8963, training loss: 8053.92, average training loss: 7347.23, base loss: 16102.68
[INFO 2017-06-29 07:56:30,610 main.py:57] epoch 8964, training loss: 7123.36, average training loss: 7346.59, base loss: 16102.82
[INFO 2017-06-29 07:56:33,653 main.py:57] epoch 8965, training loss: 7213.02, average training loss: 7346.35, base loss: 16102.94
[INFO 2017-06-29 07:56:36,698 main.py:57] epoch 8966, training loss: 7661.09, average training loss: 7346.97, base loss: 16103.04
[INFO 2017-06-29 07:56:39,762 main.py:57] epoch 8967, training loss: 7977.39, average training loss: 7348.02, base loss: 16103.38
[INFO 2017-06-29 07:56:42,888 main.py:57] epoch 8968, training loss: 7818.67, average training loss: 7349.24, base loss: 16103.08
[INFO 2017-06-29 07:56:46,041 main.py:57] epoch 8969, training loss: 7199.96, average training loss: 7348.48, base loss: 16103.03
[INFO 2017-06-29 07:56:49,122 main.py:57] epoch 8970, training loss: 6352.68, average training loss: 7347.93, base loss: 16102.81
[INFO 2017-06-29 07:56:52,123 main.py:57] epoch 8971, training loss: 6730.74, average training loss: 7346.23, base loss: 16102.88
[INFO 2017-06-29 07:56:55,215 main.py:57] epoch 8972, training loss: 7103.84, average training loss: 7346.62, base loss: 16102.82
[INFO 2017-06-29 07:56:58,364 main.py:57] epoch 8973, training loss: 8234.50, average training loss: 7346.72, base loss: 16103.32
[INFO 2017-06-29 07:57:01,394 main.py:57] epoch 8974, training loss: 7660.77, average training loss: 7347.92, base loss: 16103.48
[INFO 2017-06-29 07:57:04,434 main.py:57] epoch 8975, training loss: 7475.80, average training loss: 7347.32, base loss: 16103.66
[INFO 2017-06-29 07:57:07,504 main.py:57] epoch 8976, training loss: 7225.58, average training loss: 7345.78, base loss: 16102.93
[INFO 2017-06-29 07:57:10,550 main.py:57] epoch 8977, training loss: 7194.14, average training loss: 7345.30, base loss: 16102.61
[INFO 2017-06-29 07:57:13,585 main.py:57] epoch 8978, training loss: 6551.26, average training loss: 7344.94, base loss: 16102.44
[INFO 2017-06-29 07:57:16,727 main.py:57] epoch 8979, training loss: 7591.35, average training loss: 7344.61, base loss: 16102.66
[INFO 2017-06-29 07:57:19,803 main.py:57] epoch 8980, training loss: 8326.15, average training loss: 7345.69, base loss: 16102.60
[INFO 2017-06-29 07:57:22,869 main.py:57] epoch 8981, training loss: 7135.72, average training loss: 7345.23, base loss: 16102.30
[INFO 2017-06-29 07:57:25,876 main.py:57] epoch 8982, training loss: 6403.77, average training loss: 7344.87, base loss: 16102.01
[INFO 2017-06-29 07:57:28,903 main.py:57] epoch 8983, training loss: 7370.58, average training loss: 7344.51, base loss: 16102.21
[INFO 2017-06-29 07:57:31,942 main.py:57] epoch 8984, training loss: 6975.41, average training loss: 7343.43, base loss: 16102.17
[INFO 2017-06-29 07:57:34,969 main.py:57] epoch 8985, training loss: 7020.33, average training loss: 7343.29, base loss: 16102.18
[INFO 2017-06-29 07:57:37,969 main.py:57] epoch 8986, training loss: 6634.89, average training loss: 7342.19, base loss: 16101.86
[INFO 2017-06-29 07:57:40,986 main.py:57] epoch 8987, training loss: 8151.39, average training loss: 7343.91, base loss: 16102.40
[INFO 2017-06-29 07:57:44,080 main.py:57] epoch 8988, training loss: 7673.40, average training loss: 7344.05, base loss: 16102.53
[INFO 2017-06-29 07:57:47,239 main.py:57] epoch 8989, training loss: 6784.83, average training loss: 7343.62, base loss: 16102.39
[INFO 2017-06-29 07:57:50,286 main.py:57] epoch 8990, training loss: 6923.80, average training loss: 7342.94, base loss: 16102.20
[INFO 2017-06-29 07:57:53,314 main.py:57] epoch 8991, training loss: 7108.56, average training loss: 7341.92, base loss: 16101.95
[INFO 2017-06-29 07:57:56,426 main.py:57] epoch 8992, training loss: 8420.60, average training loss: 7342.89, base loss: 16102.39
[INFO 2017-06-29 07:57:59,459 main.py:57] epoch 8993, training loss: 7311.36, average training loss: 7342.72, base loss: 16102.52
[INFO 2017-06-29 07:58:02,479 main.py:57] epoch 8994, training loss: 6358.47, average training loss: 7342.48, base loss: 16102.54
[INFO 2017-06-29 07:58:05,446 main.py:57] epoch 8995, training loss: 6889.56, average training loss: 7342.31, base loss: 16102.54
[INFO 2017-06-29 07:58:08,479 main.py:57] epoch 8996, training loss: 7274.32, average training loss: 7342.11, base loss: 16103.08
[INFO 2017-06-29 07:58:11,523 main.py:57] epoch 8997, training loss: 8364.30, average training loss: 7343.12, base loss: 16103.61
[INFO 2017-06-29 07:58:14,649 main.py:57] epoch 8998, training loss: 7950.90, average training loss: 7344.50, base loss: 16103.53
[INFO 2017-06-29 07:58:17,716 main.py:57] epoch 8999, training loss: 6831.21, average training loss: 7344.36, base loss: 16102.96
[INFO 2017-06-29 07:58:17,717 main.py:59] epoch 8999, testing
[INFO 2017-06-29 07:58:30,377 main.py:104] average testing loss: 8302.80, base loss: 17209.07
[INFO 2017-06-29 07:58:30,377 main.py:105] improve_loss: 8906.27, improve_percent: 0.52
[INFO 2017-06-29 07:58:30,378 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 07:58:33,386 main.py:57] epoch 9000, training loss: 6711.70, average training loss: 7342.41, base loss: 16102.70
[INFO 2017-06-29 07:58:36,522 main.py:57] epoch 9001, training loss: 7842.57, average training loss: 7343.24, base loss: 16102.67
[INFO 2017-06-29 07:58:39,525 main.py:57] epoch 9002, training loss: 6975.29, average training loss: 7342.87, base loss: 16102.98
[INFO 2017-06-29 07:58:42,521 main.py:57] epoch 9003, training loss: 7246.98, average training loss: 7343.52, base loss: 16103.18
[INFO 2017-06-29 07:58:45,522 main.py:57] epoch 9004, training loss: 6418.30, average training loss: 7342.53, base loss: 16102.97
[INFO 2017-06-29 07:58:48,537 main.py:57] epoch 9005, training loss: 6820.42, average training loss: 7342.25, base loss: 16102.68
[INFO 2017-06-29 07:58:51,518 main.py:57] epoch 9006, training loss: 7569.40, average training loss: 7343.28, base loss: 16102.43
[INFO 2017-06-29 07:58:54,626 main.py:57] epoch 9007, training loss: 8213.70, average training loss: 7344.06, base loss: 16102.46
[INFO 2017-06-29 07:58:57,693 main.py:57] epoch 9008, training loss: 8127.65, average training loss: 7344.74, base loss: 16102.43
[INFO 2017-06-29 07:59:00,748 main.py:57] epoch 9009, training loss: 6845.96, average training loss: 7343.66, base loss: 16102.16
[INFO 2017-06-29 07:59:03,793 main.py:57] epoch 9010, training loss: 6992.23, average training loss: 7344.34, base loss: 16102.13
[INFO 2017-06-29 07:59:06,814 main.py:57] epoch 9011, training loss: 7193.34, average training loss: 7344.74, base loss: 16101.93
[INFO 2017-06-29 07:59:09,848 main.py:57] epoch 9012, training loss: 7593.73, average training loss: 7345.38, base loss: 16101.91
[INFO 2017-06-29 07:59:12,908 main.py:57] epoch 9013, training loss: 7266.94, average training loss: 7345.47, base loss: 16101.90
[INFO 2017-06-29 07:59:16,016 main.py:57] epoch 9014, training loss: 7254.02, average training loss: 7345.33, base loss: 16102.06
[INFO 2017-06-29 07:59:19,076 main.py:57] epoch 9015, training loss: 7514.71, average training loss: 7345.34, base loss: 16102.18
[INFO 2017-06-29 07:59:22,194 main.py:57] epoch 9016, training loss: 8096.71, average training loss: 7345.94, base loss: 16102.48
[INFO 2017-06-29 07:59:25,246 main.py:57] epoch 9017, training loss: 7139.69, average training loss: 7344.79, base loss: 16102.08
[INFO 2017-06-29 07:59:28,272 main.py:57] epoch 9018, training loss: 6819.67, average training loss: 7344.32, base loss: 16101.86
[INFO 2017-06-29 07:59:31,299 main.py:57] epoch 9019, training loss: 6491.53, average training loss: 7342.65, base loss: 16101.68
[INFO 2017-06-29 07:59:34,392 main.py:57] epoch 9020, training loss: 7513.09, average training loss: 7342.18, base loss: 16101.46
[INFO 2017-06-29 07:59:37,421 main.py:57] epoch 9021, training loss: 6917.85, average training loss: 7340.14, base loss: 16101.20
[INFO 2017-06-29 07:59:40,428 main.py:57] epoch 9022, training loss: 6890.62, average training loss: 7340.35, base loss: 16101.10
[INFO 2017-06-29 07:59:43,512 main.py:57] epoch 9023, training loss: 6996.91, average training loss: 7339.75, base loss: 16100.60
[INFO 2017-06-29 07:59:46,607 main.py:57] epoch 9024, training loss: 7612.72, average training loss: 7340.31, base loss: 16100.78
[INFO 2017-06-29 07:59:49,667 main.py:57] epoch 9025, training loss: 6293.46, average training loss: 7339.80, base loss: 16100.18
[INFO 2017-06-29 07:59:52,729 main.py:57] epoch 9026, training loss: 7604.54, average training loss: 7340.13, base loss: 16100.42
[INFO 2017-06-29 07:59:55,747 main.py:57] epoch 9027, training loss: 8501.90, average training loss: 7341.10, base loss: 16100.90
[INFO 2017-06-29 07:59:58,778 main.py:57] epoch 9028, training loss: 6577.39, average training loss: 7341.05, base loss: 16100.58
[INFO 2017-06-29 08:00:01,798 main.py:57] epoch 9029, training loss: 7260.16, average training loss: 7341.17, base loss: 16100.53
[INFO 2017-06-29 08:00:04,818 main.py:57] epoch 9030, training loss: 8580.60, average training loss: 7342.95, base loss: 16101.03
[INFO 2017-06-29 08:00:07,889 main.py:57] epoch 9031, training loss: 7710.83, average training loss: 7343.16, base loss: 16101.01
[INFO 2017-06-29 08:00:10,881 main.py:57] epoch 9032, training loss: 6905.51, average training loss: 7342.90, base loss: 16100.68
[INFO 2017-06-29 08:00:13,902 main.py:57] epoch 9033, training loss: 7036.76, average training loss: 7342.72, base loss: 16100.51
[INFO 2017-06-29 08:00:16,900 main.py:57] epoch 9034, training loss: 8813.04, average training loss: 7343.79, base loss: 16101.15
[INFO 2017-06-29 08:00:19,937 main.py:57] epoch 9035, training loss: 7453.18, average training loss: 7343.66, base loss: 16101.38
[INFO 2017-06-29 08:00:22,926 main.py:57] epoch 9036, training loss: 7728.44, average training loss: 7343.58, base loss: 16101.50
[INFO 2017-06-29 08:00:25,962 main.py:57] epoch 9037, training loss: 7291.36, average training loss: 7343.79, base loss: 16101.25
[INFO 2017-06-29 08:00:29,022 main.py:57] epoch 9038, training loss: 6819.10, average training loss: 7343.47, base loss: 16101.11
[INFO 2017-06-29 08:00:32,020 main.py:57] epoch 9039, training loss: 6571.01, average training loss: 7342.36, base loss: 16100.91
[INFO 2017-06-29 08:00:35,064 main.py:57] epoch 9040, training loss: 7387.68, average training loss: 7342.90, base loss: 16101.04
[INFO 2017-06-29 08:00:38,184 main.py:57] epoch 9041, training loss: 7352.14, average training loss: 7342.47, base loss: 16101.09
[INFO 2017-06-29 08:00:41,239 main.py:57] epoch 9042, training loss: 6834.57, average training loss: 7342.26, base loss: 16100.90
[INFO 2017-06-29 08:00:44,268 main.py:57] epoch 9043, training loss: 6879.68, average training loss: 7342.34, base loss: 16100.61
[INFO 2017-06-29 08:00:47,290 main.py:57] epoch 9044, training loss: 7897.27, average training loss: 7342.76, base loss: 16100.98
[INFO 2017-06-29 08:00:50,286 main.py:57] epoch 9045, training loss: 7537.27, average training loss: 7342.81, base loss: 16101.32
[INFO 2017-06-29 08:00:53,286 main.py:57] epoch 9046, training loss: 7842.00, average training loss: 7343.16, base loss: 16101.27
[INFO 2017-06-29 08:00:56,320 main.py:57] epoch 9047, training loss: 6621.52, average training loss: 7342.66, base loss: 16101.03
[INFO 2017-06-29 08:00:59,338 main.py:57] epoch 9048, training loss: 7768.37, average training loss: 7342.56, base loss: 16100.99
[INFO 2017-06-29 08:01:02,356 main.py:57] epoch 9049, training loss: 7001.55, average training loss: 7341.65, base loss: 16101.02
[INFO 2017-06-29 08:01:05,379 main.py:57] epoch 9050, training loss: 6702.41, average training loss: 7340.94, base loss: 16101.07
[INFO 2017-06-29 08:01:08,392 main.py:57] epoch 9051, training loss: 7120.55, average training loss: 7341.17, base loss: 16100.94
[INFO 2017-06-29 08:01:11,436 main.py:57] epoch 9052, training loss: 6330.55, average training loss: 7340.02, base loss: 16100.48
[INFO 2017-06-29 08:01:14,486 main.py:57] epoch 9053, training loss: 7366.42, average training loss: 7340.17, base loss: 16100.69
[INFO 2017-06-29 08:01:17,518 main.py:57] epoch 9054, training loss: 7288.84, average training loss: 7339.28, base loss: 16100.98
[INFO 2017-06-29 08:01:20,567 main.py:57] epoch 9055, training loss: 6846.03, average training loss: 7339.36, base loss: 16100.67
[INFO 2017-06-29 08:01:23,571 main.py:57] epoch 9056, training loss: 7914.29, average training loss: 7340.59, base loss: 16100.76
[INFO 2017-06-29 08:01:26,663 main.py:57] epoch 9057, training loss: 7113.01, average training loss: 7340.21, base loss: 16100.65
[INFO 2017-06-29 08:01:29,697 main.py:57] epoch 9058, training loss: 9357.01, average training loss: 7343.61, base loss: 16101.43
[INFO 2017-06-29 08:01:32,734 main.py:57] epoch 9059, training loss: 7221.03, average training loss: 7343.23, base loss: 16101.61
[INFO 2017-06-29 08:01:35,794 main.py:57] epoch 9060, training loss: 7283.67, average training loss: 7343.39, base loss: 16101.37
[INFO 2017-06-29 08:01:38,846 main.py:57] epoch 9061, training loss: 6476.52, average training loss: 7342.79, base loss: 16101.36
[INFO 2017-06-29 08:01:41,902 main.py:57] epoch 9062, training loss: 6141.49, average training loss: 7341.56, base loss: 16100.71
[INFO 2017-06-29 08:01:44,985 main.py:57] epoch 9063, training loss: 7277.93, average training loss: 7341.54, base loss: 16100.50
[INFO 2017-06-29 08:01:47,995 main.py:57] epoch 9064, training loss: 7744.76, average training loss: 7341.74, base loss: 16100.59
[INFO 2017-06-29 08:01:51,047 main.py:57] epoch 9065, training loss: 6047.71, average training loss: 7339.73, base loss: 16100.22
[INFO 2017-06-29 08:01:54,110 main.py:57] epoch 9066, training loss: 6188.59, average training loss: 7337.80, base loss: 16099.69
[INFO 2017-06-29 08:01:57,133 main.py:57] epoch 9067, training loss: 7520.55, average training loss: 7339.06, base loss: 16099.77
[INFO 2017-06-29 08:02:00,217 main.py:57] epoch 9068, training loss: 9338.41, average training loss: 7341.02, base loss: 16100.46
[INFO 2017-06-29 08:02:03,261 main.py:57] epoch 9069, training loss: 6974.07, average training loss: 7339.95, base loss: 16100.15
[INFO 2017-06-29 08:02:06,387 main.py:57] epoch 9070, training loss: 7648.60, average training loss: 7340.51, base loss: 16100.08
[INFO 2017-06-29 08:02:09,412 main.py:57] epoch 9071, training loss: 7804.79, average training loss: 7341.44, base loss: 16100.37
[INFO 2017-06-29 08:02:12,468 main.py:57] epoch 9072, training loss: 7421.81, average training loss: 7342.40, base loss: 16100.25
[INFO 2017-06-29 08:02:15,534 main.py:57] epoch 9073, training loss: 7086.52, average training loss: 7341.96, base loss: 16100.16
[INFO 2017-06-29 08:02:18,613 main.py:57] epoch 9074, training loss: 7686.27, average training loss: 7342.74, base loss: 16100.51
[INFO 2017-06-29 08:02:21,597 main.py:57] epoch 9075, training loss: 7553.21, average training loss: 7342.53, base loss: 16100.62
[INFO 2017-06-29 08:02:24,649 main.py:57] epoch 9076, training loss: 6846.61, average training loss: 7341.40, base loss: 16100.83
[INFO 2017-06-29 08:02:27,755 main.py:57] epoch 9077, training loss: 8660.99, average training loss: 7342.76, base loss: 16101.35
[INFO 2017-06-29 08:02:30,870 main.py:57] epoch 9078, training loss: 6997.73, average training loss: 7342.64, base loss: 16101.24
[INFO 2017-06-29 08:02:33,942 main.py:57] epoch 9079, training loss: 6884.89, average training loss: 7341.91, base loss: 16101.05
[INFO 2017-06-29 08:02:37,021 main.py:57] epoch 9080, training loss: 7748.87, average training loss: 7342.92, base loss: 16100.58
[INFO 2017-06-29 08:02:40,072 main.py:57] epoch 9081, training loss: 7598.38, average training loss: 7343.33, base loss: 16100.29
[INFO 2017-06-29 08:02:43,107 main.py:57] epoch 9082, training loss: 7307.27, average training loss: 7344.04, base loss: 16100.31
[INFO 2017-06-29 08:02:46,193 main.py:57] epoch 9083, training loss: 7551.48, average training loss: 7344.78, base loss: 16100.36
[INFO 2017-06-29 08:02:49,201 main.py:57] epoch 9084, training loss: 7221.19, average training loss: 7345.13, base loss: 16100.28
[INFO 2017-06-29 08:02:52,209 main.py:57] epoch 9085, training loss: 6341.37, average training loss: 7344.60, base loss: 16100.03
[INFO 2017-06-29 08:02:55,279 main.py:57] epoch 9086, training loss: 6967.67, average training loss: 7344.67, base loss: 16099.47
[INFO 2017-06-29 08:02:58,335 main.py:57] epoch 9087, training loss: 6555.57, average training loss: 7342.80, base loss: 16098.90
[INFO 2017-06-29 08:03:01,391 main.py:57] epoch 9088, training loss: 7505.02, average training loss: 7343.43, base loss: 16099.02
[INFO 2017-06-29 08:03:04,371 main.py:57] epoch 9089, training loss: 8626.50, average training loss: 7345.10, base loss: 16099.60
[INFO 2017-06-29 08:03:07,421 main.py:57] epoch 9090, training loss: 6542.54, average training loss: 7345.29, base loss: 16099.49
[INFO 2017-06-29 08:03:10,409 main.py:57] epoch 9091, training loss: 7668.26, average training loss: 7345.61, base loss: 16100.05
[INFO 2017-06-29 08:03:13,502 main.py:57] epoch 9092, training loss: 7370.32, average training loss: 7346.03, base loss: 16100.22
[INFO 2017-06-29 08:03:16,556 main.py:57] epoch 9093, training loss: 6940.90, average training loss: 7346.13, base loss: 16100.03
[INFO 2017-06-29 08:03:19,701 main.py:57] epoch 9094, training loss: 7211.12, average training loss: 7346.30, base loss: 16100.36
[INFO 2017-06-29 08:03:22,728 main.py:57] epoch 9095, training loss: 7430.46, average training loss: 7346.05, base loss: 16100.73
[INFO 2017-06-29 08:03:25,832 main.py:57] epoch 9096, training loss: 6542.08, average training loss: 7345.57, base loss: 16100.35
[INFO 2017-06-29 08:03:28,901 main.py:57] epoch 9097, training loss: 7515.23, average training loss: 7346.28, base loss: 16100.21
[INFO 2017-06-29 08:03:31,900 main.py:57] epoch 9098, training loss: 8175.11, average training loss: 7347.67, base loss: 16100.55
[INFO 2017-06-29 08:03:34,909 main.py:57] epoch 9099, training loss: 8529.80, average training loss: 7348.25, base loss: 16101.30
[INFO 2017-06-29 08:03:34,910 main.py:59] epoch 9099, testing
[INFO 2017-06-29 08:03:47,571 main.py:104] average testing loss: 7764.03, base loss: 16103.94
[INFO 2017-06-29 08:03:47,571 main.py:105] improve_loss: 8339.90, improve_percent: 0.52
[INFO 2017-06-29 08:03:47,572 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:03:50,590 main.py:57] epoch 9100, training loss: 8426.81, average training loss: 7349.39, base loss: 16102.02
[INFO 2017-06-29 08:03:53,585 main.py:57] epoch 9101, training loss: 7056.50, average training loss: 7349.54, base loss: 16101.95
[INFO 2017-06-29 08:03:56,638 main.py:57] epoch 9102, training loss: 7205.39, average training loss: 7349.63, base loss: 16101.92
[INFO 2017-06-29 08:03:59,678 main.py:57] epoch 9103, training loss: 7811.41, average training loss: 7350.41, base loss: 16102.17
[INFO 2017-06-29 08:04:02,689 main.py:57] epoch 9104, training loss: 7056.41, average training loss: 7350.15, base loss: 16102.16
[INFO 2017-06-29 08:04:05,755 main.py:57] epoch 9105, training loss: 7029.31, average training loss: 7350.64, base loss: 16102.02
[INFO 2017-06-29 08:04:08,849 main.py:57] epoch 9106, training loss: 7081.57, average training loss: 7349.33, base loss: 16102.20
[INFO 2017-06-29 08:04:11,923 main.py:57] epoch 9107, training loss: 5978.44, average training loss: 7348.97, base loss: 16102.02
[INFO 2017-06-29 08:04:15,018 main.py:57] epoch 9108, training loss: 7699.83, average training loss: 7349.09, base loss: 16102.07
[INFO 2017-06-29 08:04:18,031 main.py:57] epoch 9109, training loss: 7296.71, average training loss: 7348.41, base loss: 16102.26
[INFO 2017-06-29 08:04:21,111 main.py:57] epoch 9110, training loss: 7836.24, average training loss: 7349.12, base loss: 16102.58
[INFO 2017-06-29 08:04:24,229 main.py:57] epoch 9111, training loss: 7599.34, average training loss: 7349.44, base loss: 16102.72
[INFO 2017-06-29 08:04:27,246 main.py:57] epoch 9112, training loss: 6482.71, average training loss: 7348.31, base loss: 16102.01
[INFO 2017-06-29 08:04:30,374 main.py:57] epoch 9113, training loss: 8759.44, average training loss: 7349.99, base loss: 16102.42
[INFO 2017-06-29 08:04:33,361 main.py:57] epoch 9114, training loss: 6188.94, average training loss: 7347.83, base loss: 16101.72
[INFO 2017-06-29 08:04:36,461 main.py:57] epoch 9115, training loss: 7404.63, average training loss: 7347.37, base loss: 16101.56
[INFO 2017-06-29 08:04:39,468 main.py:57] epoch 9116, training loss: 7100.54, average training loss: 7347.11, base loss: 16101.81
[INFO 2017-06-29 08:04:42,506 main.py:57] epoch 9117, training loss: 7400.65, average training loss: 7347.97, base loss: 16101.99
[INFO 2017-06-29 08:04:45,573 main.py:57] epoch 9118, training loss: 7143.15, average training loss: 7347.49, base loss: 16102.24
[INFO 2017-06-29 08:04:48,679 main.py:57] epoch 9119, training loss: 6752.83, average training loss: 7347.54, base loss: 16102.13
[INFO 2017-06-29 08:04:51,703 main.py:57] epoch 9120, training loss: 7477.81, average training loss: 7348.46, base loss: 16102.62
[INFO 2017-06-29 08:04:54,798 main.py:57] epoch 9121, training loss: 6982.40, average training loss: 7348.40, base loss: 16103.08
[INFO 2017-06-29 08:04:57,867 main.py:57] epoch 9122, training loss: 7519.79, average training loss: 7349.39, base loss: 16102.87
[INFO 2017-06-29 08:05:00,889 main.py:57] epoch 9123, training loss: 7187.08, average training loss: 7348.97, base loss: 16102.66
[INFO 2017-06-29 08:05:03,931 main.py:57] epoch 9124, training loss: 6929.63, average training loss: 7348.10, base loss: 16102.36
[INFO 2017-06-29 08:05:07,050 main.py:57] epoch 9125, training loss: 6815.72, average training loss: 7346.80, base loss: 16102.33
[INFO 2017-06-29 08:05:10,075 main.py:57] epoch 9126, training loss: 7827.95, average training loss: 7347.38, base loss: 16102.63
[INFO 2017-06-29 08:05:13,088 main.py:57] epoch 9127, training loss: 7339.97, average training loss: 7346.82, base loss: 16102.60
[INFO 2017-06-29 08:05:16,169 main.py:57] epoch 9128, training loss: 6637.83, average training loss: 7346.68, base loss: 16102.54
[INFO 2017-06-29 08:05:19,204 main.py:57] epoch 9129, training loss: 6403.16, average training loss: 7345.74, base loss: 16102.53
[INFO 2017-06-29 08:05:22,259 main.py:57] epoch 9130, training loss: 7134.98, average training loss: 7345.83, base loss: 16102.75
[INFO 2017-06-29 08:05:25,303 main.py:57] epoch 9131, training loss: 7389.97, average training loss: 7344.54, base loss: 16102.89
[INFO 2017-06-29 08:05:28,352 main.py:57] epoch 9132, training loss: 6786.42, average training loss: 7343.61, base loss: 16102.90
[INFO 2017-06-29 08:05:31,401 main.py:57] epoch 9133, training loss: 7998.75, average training loss: 7344.69, base loss: 16103.36
[INFO 2017-06-29 08:05:34,414 main.py:57] epoch 9134, training loss: 8317.45, average training loss: 7345.63, base loss: 16103.88
[INFO 2017-06-29 08:05:37,438 main.py:57] epoch 9135, training loss: 7260.36, average training loss: 7345.98, base loss: 16103.79
[INFO 2017-06-29 08:05:40,483 main.py:57] epoch 9136, training loss: 6949.22, average training loss: 7343.63, base loss: 16103.51
[INFO 2017-06-29 08:05:43,514 main.py:57] epoch 9137, training loss: 6961.69, average training loss: 7344.10, base loss: 16103.29
[INFO 2017-06-29 08:05:46,580 main.py:57] epoch 9138, training loss: 7071.74, average training loss: 7343.57, base loss: 16103.23
[INFO 2017-06-29 08:05:49,615 main.py:57] epoch 9139, training loss: 7694.70, average training loss: 7343.56, base loss: 16102.99
[INFO 2017-06-29 08:05:52,650 main.py:57] epoch 9140, training loss: 6955.74, average training loss: 7343.77, base loss: 16102.74
[INFO 2017-06-29 08:05:55,739 main.py:57] epoch 9141, training loss: 7651.13, average training loss: 7343.35, base loss: 16102.95
[INFO 2017-06-29 08:05:58,808 main.py:57] epoch 9142, training loss: 7021.42, average training loss: 7342.74, base loss: 16103.27
[INFO 2017-06-29 08:06:01,836 main.py:57] epoch 9143, training loss: 8408.06, average training loss: 7343.80, base loss: 16103.76
[INFO 2017-06-29 08:06:04,915 main.py:57] epoch 9144, training loss: 7919.04, average training loss: 7343.91, base loss: 16104.32
[INFO 2017-06-29 08:06:08,023 main.py:57] epoch 9145, training loss: 7371.99, average training loss: 7344.05, base loss: 16104.46
[INFO 2017-06-29 08:06:11,087 main.py:57] epoch 9146, training loss: 6937.61, average training loss: 7343.25, base loss: 16104.22
[INFO 2017-06-29 08:06:14,111 main.py:57] epoch 9147, training loss: 7222.15, average training loss: 7342.65, base loss: 16104.19
[INFO 2017-06-29 08:06:17,127 main.py:57] epoch 9148, training loss: 8016.79, average training loss: 7343.82, base loss: 16104.65
[INFO 2017-06-29 08:06:20,121 main.py:57] epoch 9149, training loss: 7093.98, average training loss: 7343.48, base loss: 16104.85
[INFO 2017-06-29 08:06:23,259 main.py:57] epoch 9150, training loss: 7084.82, average training loss: 7343.67, base loss: 16104.64
[INFO 2017-06-29 08:06:26,347 main.py:57] epoch 9151, training loss: 6876.02, average training loss: 7342.55, base loss: 16104.36
[INFO 2017-06-29 08:06:29,431 main.py:57] epoch 9152, training loss: 6940.36, average training loss: 7342.55, base loss: 16103.91
[INFO 2017-06-29 08:06:32,429 main.py:57] epoch 9153, training loss: 7588.76, average training loss: 7342.90, base loss: 16103.60
[INFO 2017-06-29 08:06:35,547 main.py:57] epoch 9154, training loss: 6635.79, average training loss: 7342.90, base loss: 16103.33
[INFO 2017-06-29 08:06:38,590 main.py:57] epoch 9155, training loss: 6373.12, average training loss: 7341.78, base loss: 16103.05
[INFO 2017-06-29 08:06:41,618 main.py:57] epoch 9156, training loss: 7537.26, average training loss: 7342.57, base loss: 16103.47
[INFO 2017-06-29 08:06:44,676 main.py:57] epoch 9157, training loss: 6915.16, average training loss: 7341.80, base loss: 16103.57
[INFO 2017-06-29 08:06:47,803 main.py:57] epoch 9158, training loss: 7379.54, average training loss: 7342.41, base loss: 16103.22
[INFO 2017-06-29 08:06:50,882 main.py:57] epoch 9159, training loss: 7220.07, average training loss: 7342.89, base loss: 16102.95
[INFO 2017-06-29 08:06:53,952 main.py:57] epoch 9160, training loss: 6756.00, average training loss: 7342.32, base loss: 16102.46
[INFO 2017-06-29 08:06:56,964 main.py:57] epoch 9161, training loss: 7953.21, average training loss: 7343.18, base loss: 16102.57
[INFO 2017-06-29 08:07:00,022 main.py:57] epoch 9162, training loss: 7542.21, average training loss: 7343.95, base loss: 16102.56
[INFO 2017-06-29 08:07:03,159 main.py:57] epoch 9163, training loss: 7263.85, average training loss: 7344.70, base loss: 16102.49
[INFO 2017-06-29 08:07:06,221 main.py:57] epoch 9164, training loss: 7014.42, average training loss: 7343.71, base loss: 16102.39
[INFO 2017-06-29 08:07:09,252 main.py:57] epoch 9165, training loss: 7254.87, average training loss: 7343.87, base loss: 16102.44
[INFO 2017-06-29 08:07:12,294 main.py:57] epoch 9166, training loss: 6909.46, average training loss: 7343.79, base loss: 16102.11
[INFO 2017-06-29 08:07:15,344 main.py:57] epoch 9167, training loss: 7853.13, average training loss: 7343.47, base loss: 16102.62
[INFO 2017-06-29 08:07:18,368 main.py:57] epoch 9168, training loss: 6461.68, average training loss: 7342.55, base loss: 16101.90
[INFO 2017-06-29 08:07:21,394 main.py:57] epoch 9169, training loss: 7433.21, average training loss: 7343.19, base loss: 16101.58
[INFO 2017-06-29 08:07:24,469 main.py:57] epoch 9170, training loss: 6488.10, average training loss: 7342.87, base loss: 16101.46
[INFO 2017-06-29 08:07:27,502 main.py:57] epoch 9171, training loss: 6551.02, average training loss: 7341.20, base loss: 16101.22
[INFO 2017-06-29 08:07:30,494 main.py:57] epoch 9172, training loss: 7801.62, average training loss: 7342.20, base loss: 16100.99
[INFO 2017-06-29 08:07:33,614 main.py:57] epoch 9173, training loss: 7853.38, average training loss: 7343.59, base loss: 16100.79
[INFO 2017-06-29 08:07:36,754 main.py:57] epoch 9174, training loss: 6925.00, average training loss: 7343.73, base loss: 16100.65
[INFO 2017-06-29 08:07:39,797 main.py:57] epoch 9175, training loss: 7606.97, average training loss: 7344.50, base loss: 16100.36
[INFO 2017-06-29 08:07:42,813 main.py:57] epoch 9176, training loss: 7530.83, average training loss: 7344.94, base loss: 16099.86
[INFO 2017-06-29 08:07:45,902 main.py:57] epoch 9177, training loss: 7475.52, average training loss: 7345.81, base loss: 16099.50
[INFO 2017-06-29 08:07:48,976 main.py:57] epoch 9178, training loss: 7464.33, average training loss: 7344.56, base loss: 16099.79
[INFO 2017-06-29 08:07:52,035 main.py:57] epoch 9179, training loss: 7882.12, average training loss: 7344.80, base loss: 16100.42
[INFO 2017-06-29 08:07:55,195 main.py:57] epoch 9180, training loss: 7191.70, average training loss: 7344.76, base loss: 16100.52
[INFO 2017-06-29 08:07:58,191 main.py:57] epoch 9181, training loss: 7010.54, average training loss: 7343.65, base loss: 16101.00
[INFO 2017-06-29 08:08:01,337 main.py:57] epoch 9182, training loss: 6756.57, average training loss: 7342.85, base loss: 16100.88
[INFO 2017-06-29 08:08:04,322 main.py:57] epoch 9183, training loss: 7244.13, average training loss: 7343.09, base loss: 16100.51
[INFO 2017-06-29 08:08:07,370 main.py:57] epoch 9184, training loss: 6397.15, average training loss: 7341.45, base loss: 16100.03
[INFO 2017-06-29 08:08:10,382 main.py:57] epoch 9185, training loss: 8032.76, average training loss: 7342.91, base loss: 16100.36
[INFO 2017-06-29 08:08:13,443 main.py:57] epoch 9186, training loss: 6972.24, average training loss: 7343.28, base loss: 16099.76
[INFO 2017-06-29 08:08:16,503 main.py:57] epoch 9187, training loss: 7996.83, average training loss: 7343.76, base loss: 16100.40
[INFO 2017-06-29 08:08:19,550 main.py:57] epoch 9188, training loss: 7824.24, average training loss: 7344.22, base loss: 16100.64
[INFO 2017-06-29 08:08:22,648 main.py:57] epoch 9189, training loss: 7517.53, average training loss: 7344.57, base loss: 16100.44
[INFO 2017-06-29 08:08:25,717 main.py:57] epoch 9190, training loss: 7419.33, average training loss: 7344.47, base loss: 16100.40
[INFO 2017-06-29 08:08:28,729 main.py:57] epoch 9191, training loss: 6830.31, average training loss: 7344.88, base loss: 16100.07
[INFO 2017-06-29 08:08:31,821 main.py:57] epoch 9192, training loss: 7285.74, average training loss: 7345.17, base loss: 16100.25
[INFO 2017-06-29 08:08:34,854 main.py:57] epoch 9193, training loss: 7581.72, average training loss: 7345.10, base loss: 16100.60
[INFO 2017-06-29 08:08:37,908 main.py:57] epoch 9194, training loss: 7730.51, average training loss: 7344.70, base loss: 16100.81
[INFO 2017-06-29 08:08:40,884 main.py:57] epoch 9195, training loss: 8254.78, average training loss: 7346.00, base loss: 16100.97
[INFO 2017-06-29 08:08:43,934 main.py:57] epoch 9196, training loss: 7567.28, average training loss: 7346.74, base loss: 16101.23
[INFO 2017-06-29 08:08:46,957 main.py:57] epoch 9197, training loss: 7816.06, average training loss: 7347.22, base loss: 16101.58
[INFO 2017-06-29 08:08:50,035 main.py:57] epoch 9198, training loss: 6762.53, average training loss: 7347.42, base loss: 16101.43
[INFO 2017-06-29 08:08:53,102 main.py:57] epoch 9199, training loss: 8800.85, average training loss: 7348.55, base loss: 16102.07
[INFO 2017-06-29 08:08:53,103 main.py:59] epoch 9199, testing
[INFO 2017-06-29 08:09:05,744 main.py:104] average testing loss: 8051.27, base loss: 16679.36
[INFO 2017-06-29 08:09:05,744 main.py:105] improve_loss: 8628.08, improve_percent: 0.52
[INFO 2017-06-29 08:09:05,746 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:09:08,809 main.py:57] epoch 9200, training loss: 8199.27, average training loss: 7349.16, base loss: 16102.75
[INFO 2017-06-29 08:09:11,798 main.py:57] epoch 9201, training loss: 7288.02, average training loss: 7348.83, base loss: 16102.84
[INFO 2017-06-29 08:09:14,815 main.py:57] epoch 9202, training loss: 7044.67, average training loss: 7349.04, base loss: 16102.68
[INFO 2017-06-29 08:09:17,879 main.py:57] epoch 9203, training loss: 7510.17, average training loss: 7348.86, base loss: 16102.56
[INFO 2017-06-29 08:09:20,935 main.py:57] epoch 9204, training loss: 6461.84, average training loss: 7348.00, base loss: 16102.51
[INFO 2017-06-29 08:09:24,024 main.py:57] epoch 9205, training loss: 6770.83, average training loss: 7347.93, base loss: 16102.59
[INFO 2017-06-29 08:09:27,104 main.py:57] epoch 9206, training loss: 7994.85, average training loss: 7348.58, base loss: 16102.70
[INFO 2017-06-29 08:09:30,198 main.py:57] epoch 9207, training loss: 7409.17, average training loss: 7349.06, base loss: 16102.88
[INFO 2017-06-29 08:09:33,242 main.py:57] epoch 9208, training loss: 7584.32, average training loss: 7349.21, base loss: 16103.11
[INFO 2017-06-29 08:09:36,264 main.py:57] epoch 9209, training loss: 7439.69, average training loss: 7349.54, base loss: 16103.20
[INFO 2017-06-29 08:09:39,346 main.py:57] epoch 9210, training loss: 7122.39, average training loss: 7348.76, base loss: 16103.09
[INFO 2017-06-29 08:09:42,419 main.py:57] epoch 9211, training loss: 6921.81, average training loss: 7348.33, base loss: 16103.08
[INFO 2017-06-29 08:09:45,515 main.py:57] epoch 9212, training loss: 8310.84, average training loss: 7349.39, base loss: 16103.34
[INFO 2017-06-29 08:09:48,580 main.py:57] epoch 9213, training loss: 6744.67, average training loss: 7347.94, base loss: 16102.91
[INFO 2017-06-29 08:09:51,590 main.py:57] epoch 9214, training loss: 6911.69, average training loss: 7347.95, base loss: 16103.05
[INFO 2017-06-29 08:09:54,635 main.py:57] epoch 9215, training loss: 7421.89, average training loss: 7348.17, base loss: 16103.16
[INFO 2017-06-29 08:09:57,723 main.py:57] epoch 9216, training loss: 6422.69, average training loss: 7346.49, base loss: 16102.36
[INFO 2017-06-29 08:10:00,775 main.py:57] epoch 9217, training loss: 7493.16, average training loss: 7346.96, base loss: 16102.42
[INFO 2017-06-29 08:10:03,792 main.py:57] epoch 9218, training loss: 7698.77, average training loss: 7347.42, base loss: 16102.64
[INFO 2017-06-29 08:10:06,796 main.py:57] epoch 9219, training loss: 6939.83, average training loss: 7347.32, base loss: 16102.67
[INFO 2017-06-29 08:10:09,833 main.py:57] epoch 9220, training loss: 7645.63, average training loss: 7346.93, base loss: 16102.62
[INFO 2017-06-29 08:10:12,896 main.py:57] epoch 9221, training loss: 7966.25, average training loss: 7346.56, base loss: 16102.51
[INFO 2017-06-29 08:10:15,971 main.py:57] epoch 9222, training loss: 6522.92, average training loss: 7345.85, base loss: 16102.21
[INFO 2017-06-29 08:10:18,961 main.py:57] epoch 9223, training loss: 7659.95, average training loss: 7345.80, base loss: 16102.42
[INFO 2017-06-29 08:10:21,986 main.py:57] epoch 9224, training loss: 7822.26, average training loss: 7345.97, base loss: 16102.29
[INFO 2017-06-29 08:10:25,045 main.py:57] epoch 9225, training loss: 7499.31, average training loss: 7345.36, base loss: 16102.27
[INFO 2017-06-29 08:10:28,164 main.py:57] epoch 9226, training loss: 8381.93, average training loss: 7345.97, base loss: 16102.81
[INFO 2017-06-29 08:10:31,279 main.py:57] epoch 9227, training loss: 8378.80, average training loss: 7347.32, base loss: 16102.90
[INFO 2017-06-29 08:10:34,342 main.py:57] epoch 9228, training loss: 7756.01, average training loss: 7347.39, base loss: 16102.87
[INFO 2017-06-29 08:10:37,356 main.py:57] epoch 9229, training loss: 7228.24, average training loss: 7347.72, base loss: 16102.73
[INFO 2017-06-29 08:10:40,369 main.py:57] epoch 9230, training loss: 7647.28, average training loss: 7347.84, base loss: 16103.15
[INFO 2017-06-29 08:10:43,450 main.py:57] epoch 9231, training loss: 7369.04, average training loss: 7348.14, base loss: 16103.20
[INFO 2017-06-29 08:10:46,498 main.py:57] epoch 9232, training loss: 7276.26, average training loss: 7347.76, base loss: 16102.84
[INFO 2017-06-29 08:10:49,560 main.py:57] epoch 9233, training loss: 6383.47, average training loss: 7347.22, base loss: 16102.39
[INFO 2017-06-29 08:10:52,631 main.py:57] epoch 9234, training loss: 7051.02, average training loss: 7346.91, base loss: 16102.53
[INFO 2017-06-29 08:10:55,699 main.py:57] epoch 9235, training loss: 7546.83, average training loss: 7346.28, base loss: 16102.70
[INFO 2017-06-29 08:10:58,764 main.py:57] epoch 9236, training loss: 7157.18, average training loss: 7346.03, base loss: 16102.51
[INFO 2017-06-29 08:11:01,894 main.py:57] epoch 9237, training loss: 6508.03, average training loss: 7345.14, base loss: 16102.39
[INFO 2017-06-29 08:11:04,982 main.py:57] epoch 9238, training loss: 7166.48, average training loss: 7345.11, base loss: 16102.31
[INFO 2017-06-29 08:11:08,034 main.py:57] epoch 9239, training loss: 6974.46, average training loss: 7345.15, base loss: 16102.11
[INFO 2017-06-29 08:11:11,094 main.py:57] epoch 9240, training loss: 8674.96, average training loss: 7346.45, base loss: 16102.31
[INFO 2017-06-29 08:11:14,220 main.py:57] epoch 9241, training loss: 7931.46, average training loss: 7347.62, base loss: 16102.40
[INFO 2017-06-29 08:11:17,336 main.py:57] epoch 9242, training loss: 6987.15, average training loss: 7347.38, base loss: 16102.23
[INFO 2017-06-29 08:11:20,431 main.py:57] epoch 9243, training loss: 8186.48, average training loss: 7347.82, base loss: 16102.85
[INFO 2017-06-29 08:11:23,419 main.py:57] epoch 9244, training loss: 7969.14, average training loss: 7348.42, base loss: 16103.65
[INFO 2017-06-29 08:11:26,417 main.py:57] epoch 9245, training loss: 7696.25, average training loss: 7349.26, base loss: 16104.10
[INFO 2017-06-29 08:11:29,413 main.py:57] epoch 9246, training loss: 6666.70, average training loss: 7348.89, base loss: 16103.81
[INFO 2017-06-29 08:11:32,465 main.py:57] epoch 9247, training loss: 6894.79, average training loss: 7348.57, base loss: 16103.38
[INFO 2017-06-29 08:11:35,471 main.py:57] epoch 9248, training loss: 8676.94, average training loss: 7350.71, base loss: 16103.28
[INFO 2017-06-29 08:11:38,468 main.py:57] epoch 9249, training loss: 7510.05, average training loss: 7351.57, base loss: 16103.17
[INFO 2017-06-29 08:11:41,528 main.py:57] epoch 9250, training loss: 7531.42, average training loss: 7351.96, base loss: 16103.69
[INFO 2017-06-29 08:11:44,542 main.py:57] epoch 9251, training loss: 7447.36, average training loss: 7350.65, base loss: 16103.99
[INFO 2017-06-29 08:11:47,612 main.py:57] epoch 9252, training loss: 6931.79, average training loss: 7351.07, base loss: 16103.62
[INFO 2017-06-29 08:11:50,649 main.py:57] epoch 9253, training loss: 8539.28, average training loss: 7352.34, base loss: 16104.20
[INFO 2017-06-29 08:11:53,705 main.py:57] epoch 9254, training loss: 7680.95, average training loss: 7351.76, base loss: 16104.33
[INFO 2017-06-29 08:11:56,774 main.py:57] epoch 9255, training loss: 7553.48, average training loss: 7352.52, base loss: 16104.10
[INFO 2017-06-29 08:11:59,853 main.py:57] epoch 9256, training loss: 6778.58, average training loss: 7352.45, base loss: 16103.83
[INFO 2017-06-29 08:12:02,924 main.py:57] epoch 9257, training loss: 7107.65, average training loss: 7351.88, base loss: 16103.84
[INFO 2017-06-29 08:12:05,967 main.py:57] epoch 9258, training loss: 7142.66, average training loss: 7351.69, base loss: 16103.88
[INFO 2017-06-29 08:12:08,992 main.py:57] epoch 9259, training loss: 7158.27, average training loss: 7352.04, base loss: 16103.82
[INFO 2017-06-29 08:12:12,011 main.py:57] epoch 9260, training loss: 7559.05, average training loss: 7352.43, base loss: 16103.40
[INFO 2017-06-29 08:12:15,109 main.py:57] epoch 9261, training loss: 6249.41, average training loss: 7351.72, base loss: 16102.68
[INFO 2017-06-29 08:12:18,192 main.py:57] epoch 9262, training loss: 7683.83, average training loss: 7350.60, base loss: 16102.82
[INFO 2017-06-29 08:12:21,210 main.py:57] epoch 9263, training loss: 7262.85, average training loss: 7351.06, base loss: 16103.04
[INFO 2017-06-29 08:12:24,235 main.py:57] epoch 9264, training loss: 7800.76, average training loss: 7352.28, base loss: 16102.98
[INFO 2017-06-29 08:12:27,272 main.py:57] epoch 9265, training loss: 8020.73, average training loss: 7352.71, base loss: 16103.04
[INFO 2017-06-29 08:12:30,302 main.py:57] epoch 9266, training loss: 7219.82, average training loss: 7353.14, base loss: 16103.16
[INFO 2017-06-29 08:12:33,340 main.py:57] epoch 9267, training loss: 6780.37, average training loss: 7352.74, base loss: 16103.10
[INFO 2017-06-29 08:12:36,461 main.py:57] epoch 9268, training loss: 8044.26, average training loss: 7354.01, base loss: 16103.79
[INFO 2017-06-29 08:12:39,422 main.py:57] epoch 9269, training loss: 6646.45, average training loss: 7353.18, base loss: 16103.66
[INFO 2017-06-29 08:12:42,429 main.py:57] epoch 9270, training loss: 7546.20, average training loss: 7352.43, base loss: 16103.92
[INFO 2017-06-29 08:12:45,498 main.py:57] epoch 9271, training loss: 5928.28, average training loss: 7350.03, base loss: 16103.44
[INFO 2017-06-29 08:12:48,539 main.py:57] epoch 9272, training loss: 7237.93, average training loss: 7350.12, base loss: 16103.41
[INFO 2017-06-29 08:12:51,690 main.py:57] epoch 9273, training loss: 7137.59, average training loss: 7350.86, base loss: 16103.10
[INFO 2017-06-29 08:12:54,696 main.py:57] epoch 9274, training loss: 7199.87, average training loss: 7350.52, base loss: 16103.13
[INFO 2017-06-29 08:12:57,759 main.py:57] epoch 9275, training loss: 7420.56, average training loss: 7350.38, base loss: 16103.40
[INFO 2017-06-29 08:13:00,890 main.py:57] epoch 9276, training loss: 7021.49, average training loss: 7349.90, base loss: 16103.66
[INFO 2017-06-29 08:13:03,890 main.py:57] epoch 9277, training loss: 7288.62, average training loss: 7348.68, base loss: 16103.87
[INFO 2017-06-29 08:13:06,946 main.py:57] epoch 9278, training loss: 7111.02, average training loss: 7349.63, base loss: 16103.54
[INFO 2017-06-29 08:13:09,956 main.py:57] epoch 9279, training loss: 7349.03, average training loss: 7349.23, base loss: 16103.62
[INFO 2017-06-29 08:13:12,968 main.py:57] epoch 9280, training loss: 7241.21, average training loss: 7349.42, base loss: 16103.72
[INFO 2017-06-29 08:13:16,025 main.py:57] epoch 9281, training loss: 6307.72, average training loss: 7348.64, base loss: 16103.37
[INFO 2017-06-29 08:13:19,049 main.py:57] epoch 9282, training loss: 6554.06, average training loss: 7348.00, base loss: 16103.22
[INFO 2017-06-29 08:13:22,134 main.py:57] epoch 9283, training loss: 7407.69, average training loss: 7348.31, base loss: 16103.44
[INFO 2017-06-29 08:13:25,217 main.py:57] epoch 9284, training loss: 7950.93, average training loss: 7349.77, base loss: 16103.53
[INFO 2017-06-29 08:13:28,200 main.py:57] epoch 9285, training loss: 7838.70, average training loss: 7350.93, base loss: 16103.64
[INFO 2017-06-29 08:13:31,246 main.py:57] epoch 9286, training loss: 7522.57, average training loss: 7352.29, base loss: 16103.93
[INFO 2017-06-29 08:13:34,274 main.py:57] epoch 9287, training loss: 6761.50, average training loss: 7351.69, base loss: 16103.64
[INFO 2017-06-29 08:13:37,360 main.py:57] epoch 9288, training loss: 8082.90, average training loss: 7352.78, base loss: 16104.34
[INFO 2017-06-29 08:13:40,361 main.py:57] epoch 9289, training loss: 7946.66, average training loss: 7354.24, base loss: 16104.76
[INFO 2017-06-29 08:13:43,382 main.py:57] epoch 9290, training loss: 7143.56, average training loss: 7353.27, base loss: 16105.24
[INFO 2017-06-29 08:13:46,330 main.py:57] epoch 9291, training loss: 6600.60, average training loss: 7352.21, base loss: 16105.30
[INFO 2017-06-29 08:13:49,370 main.py:57] epoch 9292, training loss: 7415.92, average training loss: 7352.93, base loss: 16105.52
[INFO 2017-06-29 08:13:52,476 main.py:57] epoch 9293, training loss: 7962.54, average training loss: 7353.63, base loss: 16106.09
[INFO 2017-06-29 08:13:55,517 main.py:57] epoch 9294, training loss: 8153.61, average training loss: 7353.37, base loss: 16106.45
[INFO 2017-06-29 08:13:58,557 main.py:57] epoch 9295, training loss: 6839.15, average training loss: 7352.80, base loss: 16106.34
[INFO 2017-06-29 08:14:01,589 main.py:57] epoch 9296, training loss: 6580.24, average training loss: 7352.24, base loss: 16106.21
[INFO 2017-06-29 08:14:04,761 main.py:57] epoch 9297, training loss: 6849.54, average training loss: 7352.14, base loss: 16106.23
[INFO 2017-06-29 08:14:07,766 main.py:57] epoch 9298, training loss: 8458.33, average training loss: 7353.44, base loss: 16106.85
[INFO 2017-06-29 08:14:10,815 main.py:57] epoch 9299, training loss: 7860.97, average training loss: 7353.87, base loss: 16107.16
[INFO 2017-06-29 08:14:10,816 main.py:59] epoch 9299, testing
[INFO 2017-06-29 08:14:23,403 main.py:104] average testing loss: 8317.75, base loss: 17564.26
[INFO 2017-06-29 08:14:23,403 main.py:105] improve_loss: 9246.50, improve_percent: 0.53
[INFO 2017-06-29 08:14:23,404 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:14:26,471 main.py:57] epoch 9300, training loss: 7007.64, average training loss: 7353.60, base loss: 16107.20
[INFO 2017-06-29 08:14:29,456 main.py:57] epoch 9301, training loss: 7878.06, average training loss: 7353.86, base loss: 16107.78
[INFO 2017-06-29 08:14:32,518 main.py:57] epoch 9302, training loss: 7020.41, average training loss: 7353.37, base loss: 16107.78
[INFO 2017-06-29 08:14:35,614 main.py:57] epoch 9303, training loss: 7198.59, average training loss: 7352.98, base loss: 16107.77
[INFO 2017-06-29 08:14:38,603 main.py:57] epoch 9304, training loss: 7028.27, average training loss: 7353.19, base loss: 16107.95
[INFO 2017-06-29 08:14:41,648 main.py:57] epoch 9305, training loss: 8256.05, average training loss: 7353.77, base loss: 16107.89
[INFO 2017-06-29 08:14:44,649 main.py:57] epoch 9306, training loss: 6638.13, average training loss: 7353.45, base loss: 16107.45
[INFO 2017-06-29 08:14:47,775 main.py:57] epoch 9307, training loss: 7155.20, average training loss: 7353.91, base loss: 16107.20
[INFO 2017-06-29 08:14:50,771 main.py:57] epoch 9308, training loss: 7419.77, average training loss: 7354.63, base loss: 16107.25
[INFO 2017-06-29 08:14:53,813 main.py:57] epoch 9309, training loss: 7062.44, average training loss: 7354.36, base loss: 16107.00
[INFO 2017-06-29 08:14:56,798 main.py:57] epoch 9310, training loss: 6308.40, average training loss: 7353.45, base loss: 16106.44
[INFO 2017-06-29 08:14:59,827 main.py:57] epoch 9311, training loss: 7280.36, average training loss: 7353.84, base loss: 16106.50
[INFO 2017-06-29 08:15:02,802 main.py:57] epoch 9312, training loss: 8355.95, average training loss: 7354.15, base loss: 16106.77
[INFO 2017-06-29 08:15:05,835 main.py:57] epoch 9313, training loss: 7408.85, average training loss: 7353.99, base loss: 16106.47
[INFO 2017-06-29 08:15:08,945 main.py:57] epoch 9314, training loss: 8412.43, average training loss: 7355.18, base loss: 16106.46
[INFO 2017-06-29 08:15:11,946 main.py:57] epoch 9315, training loss: 5777.66, average training loss: 7354.52, base loss: 16105.91
[INFO 2017-06-29 08:15:14,977 main.py:57] epoch 9316, training loss: 8006.38, average training loss: 7354.80, base loss: 16106.17
[INFO 2017-06-29 08:15:18,136 main.py:57] epoch 9317, training loss: 7577.45, average training loss: 7356.11, base loss: 16106.37
[INFO 2017-06-29 08:15:21,188 main.py:57] epoch 9318, training loss: 7272.03, average training loss: 7356.48, base loss: 16106.70
[INFO 2017-06-29 08:15:24,270 main.py:57] epoch 9319, training loss: 7701.81, average training loss: 7357.28, base loss: 16106.79
[INFO 2017-06-29 08:15:27,375 main.py:57] epoch 9320, training loss: 7202.94, average training loss: 7356.60, base loss: 16106.81
[INFO 2017-06-29 08:15:30,417 main.py:57] epoch 9321, training loss: 7045.42, average training loss: 7355.84, base loss: 16106.93
[INFO 2017-06-29 08:15:33,421 main.py:57] epoch 9322, training loss: 6772.21, average training loss: 7356.17, base loss: 16106.52
[INFO 2017-06-29 08:15:36,505 main.py:57] epoch 9323, training loss: 8026.26, average training loss: 7357.09, base loss: 16106.63
[INFO 2017-06-29 08:15:39,490 main.py:57] epoch 9324, training loss: 7006.74, average training loss: 7355.49, base loss: 16106.39
[INFO 2017-06-29 08:15:42,531 main.py:57] epoch 9325, training loss: 6930.02, average training loss: 7354.22, base loss: 16106.06
[INFO 2017-06-29 08:15:45,552 main.py:57] epoch 9326, training loss: 6937.70, average training loss: 7354.27, base loss: 16105.78
[INFO 2017-06-29 08:15:48,623 main.py:57] epoch 9327, training loss: 7508.84, average training loss: 7354.28, base loss: 16106.26
[INFO 2017-06-29 08:15:51,710 main.py:57] epoch 9328, training loss: 7744.79, average training loss: 7354.34, base loss: 16106.60
[INFO 2017-06-29 08:15:54,830 main.py:57] epoch 9329, training loss: 7489.78, average training loss: 7354.64, base loss: 16106.81
[INFO 2017-06-29 08:15:57,900 main.py:57] epoch 9330, training loss: 6004.27, average training loss: 7352.72, base loss: 16106.17
[INFO 2017-06-29 08:16:00,957 main.py:57] epoch 9331, training loss: 7502.56, average training loss: 7353.73, base loss: 16106.17
[INFO 2017-06-29 08:16:04,007 main.py:57] epoch 9332, training loss: 6721.97, average training loss: 7353.02, base loss: 16105.76
[INFO 2017-06-29 08:16:07,094 main.py:57] epoch 9333, training loss: 7158.48, average training loss: 7353.63, base loss: 16105.27
[INFO 2017-06-29 08:16:10,171 main.py:57] epoch 9334, training loss: 6545.88, average training loss: 7352.41, base loss: 16105.00
[INFO 2017-06-29 08:16:13,200 main.py:57] epoch 9335, training loss: 6965.82, average training loss: 7351.42, base loss: 16105.30
[INFO 2017-06-29 08:16:16,252 main.py:57] epoch 9336, training loss: 8028.19, average training loss: 7352.11, base loss: 16105.33
[INFO 2017-06-29 08:16:19,331 main.py:57] epoch 9337, training loss: 7987.01, average training loss: 7353.13, base loss: 16105.51
[INFO 2017-06-29 08:16:22,390 main.py:57] epoch 9338, training loss: 6690.58, average training loss: 7352.89, base loss: 16105.42
[INFO 2017-06-29 08:16:25,427 main.py:57] epoch 9339, training loss: 6924.09, average training loss: 7352.75, base loss: 16105.37
[INFO 2017-06-29 08:16:28,512 main.py:57] epoch 9340, training loss: 7247.18, average training loss: 7351.49, base loss: 16105.34
[INFO 2017-06-29 08:16:31,576 main.py:57] epoch 9341, training loss: 7382.56, average training loss: 7350.78, base loss: 16105.14
[INFO 2017-06-29 08:16:34,602 main.py:57] epoch 9342, training loss: 7208.12, average training loss: 7350.77, base loss: 16105.05
[INFO 2017-06-29 08:16:37,684 main.py:57] epoch 9343, training loss: 6949.31, average training loss: 7351.01, base loss: 16104.88
[INFO 2017-06-29 08:16:40,735 main.py:57] epoch 9344, training loss: 6387.16, average training loss: 7349.05, base loss: 16104.62
[INFO 2017-06-29 08:16:43,804 main.py:57] epoch 9345, training loss: 8258.09, average training loss: 7349.19, base loss: 16105.16
[INFO 2017-06-29 08:16:46,944 main.py:57] epoch 9346, training loss: 7399.04, average training loss: 7349.45, base loss: 16105.19
[INFO 2017-06-29 08:16:49,995 main.py:57] epoch 9347, training loss: 7695.40, average training loss: 7350.15, base loss: 16105.37
[INFO 2017-06-29 08:16:53,033 main.py:57] epoch 9348, training loss: 7134.73, average training loss: 7350.28, base loss: 16105.44
[INFO 2017-06-29 08:16:56,070 main.py:57] epoch 9349, training loss: 7116.40, average training loss: 7350.46, base loss: 16105.09
[INFO 2017-06-29 08:16:59,098 main.py:57] epoch 9350, training loss: 6729.41, average training loss: 7350.29, base loss: 16104.78
[INFO 2017-06-29 08:17:02,256 main.py:57] epoch 9351, training loss: 6530.90, average training loss: 7348.50, base loss: 16104.56
[INFO 2017-06-29 08:17:05,369 main.py:57] epoch 9352, training loss: 7066.38, average training loss: 7347.04, base loss: 16104.70
[INFO 2017-06-29 08:17:08,401 main.py:57] epoch 9353, training loss: 6296.18, average training loss: 7345.83, base loss: 16104.74
[INFO 2017-06-29 08:17:11,513 main.py:57] epoch 9354, training loss: 6860.60, average training loss: 7344.67, base loss: 16104.43
[INFO 2017-06-29 08:17:14,599 main.py:57] epoch 9355, training loss: 7725.64, average training loss: 7345.69, base loss: 16104.50
[INFO 2017-06-29 08:17:17,685 main.py:57] epoch 9356, training loss: 7329.27, average training loss: 7344.99, base loss: 16104.63
[INFO 2017-06-29 08:17:20,836 main.py:57] epoch 9357, training loss: 7805.33, average training loss: 7345.99, base loss: 16104.97
[INFO 2017-06-29 08:17:23,837 main.py:57] epoch 9358, training loss: 7411.70, average training loss: 7346.17, base loss: 16105.21
[INFO 2017-06-29 08:17:26,916 main.py:57] epoch 9359, training loss: 6915.74, average training loss: 7346.27, base loss: 16105.07
[INFO 2017-06-29 08:17:29,987 main.py:57] epoch 9360, training loss: 6514.81, average training loss: 7345.41, base loss: 16104.83
[INFO 2017-06-29 08:17:33,021 main.py:57] epoch 9361, training loss: 7459.29, average training loss: 7344.27, base loss: 16105.02
[INFO 2017-06-29 08:17:36,047 main.py:57] epoch 9362, training loss: 6598.54, average training loss: 7343.89, base loss: 16104.64
[INFO 2017-06-29 08:17:39,073 main.py:57] epoch 9363, training loss: 7375.15, average training loss: 7343.38, base loss: 16104.88
[INFO 2017-06-29 08:17:42,079 main.py:57] epoch 9364, training loss: 7661.75, average training loss: 7343.49, base loss: 16105.21
[INFO 2017-06-29 08:17:45,085 main.py:57] epoch 9365, training loss: 8094.29, average training loss: 7343.53, base loss: 16105.66
[INFO 2017-06-29 08:17:48,127 main.py:57] epoch 9366, training loss: 7159.31, average training loss: 7343.73, base loss: 16105.73
[INFO 2017-06-29 08:17:51,185 main.py:57] epoch 9367, training loss: 7318.63, average training loss: 7343.83, base loss: 16105.64
[INFO 2017-06-29 08:17:54,251 main.py:57] epoch 9368, training loss: 7466.00, average training loss: 7344.47, base loss: 16105.33
[INFO 2017-06-29 08:17:57,278 main.py:57] epoch 9369, training loss: 7426.09, average training loss: 7344.47, base loss: 16105.34
[INFO 2017-06-29 08:18:00,328 main.py:57] epoch 9370, training loss: 7154.27, average training loss: 7344.51, base loss: 16105.09
[INFO 2017-06-29 08:18:03,387 main.py:57] epoch 9371, training loss: 6428.41, average training loss: 7343.22, base loss: 16104.73
[INFO 2017-06-29 08:18:06,393 main.py:57] epoch 9372, training loss: 7297.62, average training loss: 7343.45, base loss: 16104.59
[INFO 2017-06-29 08:18:09,445 main.py:57] epoch 9373, training loss: 7260.65, average training loss: 7343.98, base loss: 16104.34
[INFO 2017-06-29 08:18:12,508 main.py:57] epoch 9374, training loss: 7868.40, average training loss: 7343.43, base loss: 16104.76
[INFO 2017-06-29 08:18:15,606 main.py:57] epoch 9375, training loss: 6336.04, average training loss: 7342.24, base loss: 16104.22
[INFO 2017-06-29 08:18:18,698 main.py:57] epoch 9376, training loss: 6672.53, average training loss: 7341.35, base loss: 16103.81
[INFO 2017-06-29 08:18:21,751 main.py:57] epoch 9377, training loss: 7357.41, average training loss: 7341.90, base loss: 16103.81
[INFO 2017-06-29 08:18:24,803 main.py:57] epoch 9378, training loss: 6817.81, average training loss: 7341.75, base loss: 16103.68
[INFO 2017-06-29 08:18:27,828 main.py:57] epoch 9379, training loss: 8679.55, average training loss: 7343.69, base loss: 16104.26
[INFO 2017-06-29 08:18:30,834 main.py:57] epoch 9380, training loss: 7336.55, average training loss: 7343.71, base loss: 16103.90
[INFO 2017-06-29 08:18:33,835 main.py:57] epoch 9381, training loss: 8276.41, average training loss: 7345.86, base loss: 16104.03
[INFO 2017-06-29 08:18:36,859 main.py:57] epoch 9382, training loss: 7978.02, average training loss: 7346.22, base loss: 16104.46
[INFO 2017-06-29 08:18:39,964 main.py:57] epoch 9383, training loss: 6894.15, average training loss: 7345.70, base loss: 16104.39
[INFO 2017-06-29 08:18:43,045 main.py:57] epoch 9384, training loss: 8213.16, average training loss: 7347.15, base loss: 16104.68
[INFO 2017-06-29 08:18:46,128 main.py:57] epoch 9385, training loss: 6830.95, average training loss: 7346.80, base loss: 16104.73
[INFO 2017-06-29 08:18:49,188 main.py:57] epoch 9386, training loss: 7091.26, average training loss: 7346.01, base loss: 16104.62
[INFO 2017-06-29 08:18:52,181 main.py:57] epoch 9387, training loss: 7389.74, average training loss: 7346.18, base loss: 16105.01
[INFO 2017-06-29 08:18:55,198 main.py:57] epoch 9388, training loss: 6675.12, average training loss: 7345.54, base loss: 16104.71
[INFO 2017-06-29 08:18:58,257 main.py:57] epoch 9389, training loss: 6607.35, average training loss: 7345.20, base loss: 16104.04
[INFO 2017-06-29 08:19:01,325 main.py:57] epoch 9390, training loss: 7446.75, average training loss: 7345.94, base loss: 16104.09
[INFO 2017-06-29 08:19:04,375 main.py:57] epoch 9391, training loss: 7958.29, average training loss: 7345.79, base loss: 16104.49
[INFO 2017-06-29 08:19:07,376 main.py:57] epoch 9392, training loss: 7773.74, average training loss: 7345.99, base loss: 16104.76
[INFO 2017-06-29 08:19:10,445 main.py:57] epoch 9393, training loss: 7448.84, average training loss: 7346.12, base loss: 16104.84
[INFO 2017-06-29 08:19:13,430 main.py:57] epoch 9394, training loss: 7211.05, average training loss: 7345.79, base loss: 16104.65
[INFO 2017-06-29 08:19:16,442 main.py:57] epoch 9395, training loss: 7238.55, average training loss: 7344.42, base loss: 16104.70
[INFO 2017-06-29 08:19:19,460 main.py:57] epoch 9396, training loss: 7012.70, average training loss: 7343.88, base loss: 16104.55
[INFO 2017-06-29 08:19:22,522 main.py:57] epoch 9397, training loss: 8334.20, average training loss: 7345.29, base loss: 16104.82
[INFO 2017-06-29 08:19:25,557 main.py:57] epoch 9398, training loss: 6917.24, average training loss: 7345.56, base loss: 16104.70
[INFO 2017-06-29 08:19:28,567 main.py:57] epoch 9399, training loss: 6342.41, average training loss: 7345.40, base loss: 16104.09
[INFO 2017-06-29 08:19:28,567 main.py:59] epoch 9399, testing
[INFO 2017-06-29 08:19:41,162 main.py:104] average testing loss: 7982.49, base loss: 16854.15
[INFO 2017-06-29 08:19:41,162 main.py:105] improve_loss: 8871.65, improve_percent: 0.53
[INFO 2017-06-29 08:19:41,164 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:19:44,167 main.py:57] epoch 9400, training loss: 7766.15, average training loss: 7346.19, base loss: 16104.70
[INFO 2017-06-29 08:19:47,197 main.py:57] epoch 9401, training loss: 7361.20, average training loss: 7345.81, base loss: 16104.97
[INFO 2017-06-29 08:19:50,267 main.py:57] epoch 9402, training loss: 5993.82, average training loss: 7344.32, base loss: 16104.43
[INFO 2017-06-29 08:19:53,399 main.py:57] epoch 9403, training loss: 6676.60, average training loss: 7343.75, base loss: 16103.68
[INFO 2017-06-29 08:19:56,480 main.py:57] epoch 9404, training loss: 6540.07, average training loss: 7343.43, base loss: 16103.59
[INFO 2017-06-29 08:19:59,482 main.py:57] epoch 9405, training loss: 6648.79, average training loss: 7342.76, base loss: 16103.63
[INFO 2017-06-29 08:20:02,580 main.py:57] epoch 9406, training loss: 7645.72, average training loss: 7342.21, base loss: 16104.13
[INFO 2017-06-29 08:20:05,674 main.py:57] epoch 9407, training loss: 6529.43, average training loss: 7340.25, base loss: 16103.97
[INFO 2017-06-29 08:20:08,664 main.py:57] epoch 9408, training loss: 7561.87, average training loss: 7341.06, base loss: 16104.48
[INFO 2017-06-29 08:20:11,776 main.py:57] epoch 9409, training loss: 7411.23, average training loss: 7339.69, base loss: 16105.06
[INFO 2017-06-29 08:20:14,783 main.py:57] epoch 9410, training loss: 7701.54, average training loss: 7340.14, base loss: 16105.29
[INFO 2017-06-29 08:20:17,838 main.py:57] epoch 9411, training loss: 7043.24, average training loss: 7340.12, base loss: 16104.92
[INFO 2017-06-29 08:20:20,845 main.py:57] epoch 9412, training loss: 6970.23, average training loss: 7339.72, base loss: 16104.57
[INFO 2017-06-29 08:20:23,915 main.py:57] epoch 9413, training loss: 7939.75, average training loss: 7339.61, base loss: 16105.00
[INFO 2017-06-29 08:20:26,940 main.py:57] epoch 9414, training loss: 6320.81, average training loss: 7339.52, base loss: 16104.71
[INFO 2017-06-29 08:20:29,971 main.py:57] epoch 9415, training loss: 6955.13, average training loss: 7338.74, base loss: 16104.49
[INFO 2017-06-29 08:20:32,997 main.py:57] epoch 9416, training loss: 7470.95, average training loss: 7339.24, base loss: 16104.81
[INFO 2017-06-29 08:20:36,101 main.py:57] epoch 9417, training loss: 7784.24, average training loss: 7338.19, base loss: 16105.14
[INFO 2017-06-29 08:20:39,195 main.py:57] epoch 9418, training loss: 7194.89, average training loss: 7337.53, base loss: 16104.82
[INFO 2017-06-29 08:20:42,230 main.py:57] epoch 9419, training loss: 8178.92, average training loss: 7337.91, base loss: 16105.22
[INFO 2017-06-29 08:20:45,282 main.py:57] epoch 9420, training loss: 7400.14, average training loss: 7338.03, base loss: 16105.75
[INFO 2017-06-29 08:20:48,299 main.py:57] epoch 9421, training loss: 6827.31, average training loss: 7336.96, base loss: 16105.83
[INFO 2017-06-29 08:20:51,366 main.py:57] epoch 9422, training loss: 6987.96, average training loss: 7336.80, base loss: 16105.80
[INFO 2017-06-29 08:20:54,391 main.py:57] epoch 9423, training loss: 6754.26, average training loss: 7335.91, base loss: 16105.41
[INFO 2017-06-29 08:20:57,374 main.py:57] epoch 9424, training loss: 7749.51, average training loss: 7335.80, base loss: 16105.77
[INFO 2017-06-29 08:21:00,421 main.py:57] epoch 9425, training loss: 7719.60, average training loss: 7335.93, base loss: 16106.19
[INFO 2017-06-29 08:21:03,463 main.py:57] epoch 9426, training loss: 7379.39, average training loss: 7335.50, base loss: 16106.32
[INFO 2017-06-29 08:21:06,532 main.py:57] epoch 9427, training loss: 6466.16, average training loss: 7334.61, base loss: 16105.89
[INFO 2017-06-29 08:21:09,605 main.py:57] epoch 9428, training loss: 7816.80, average training loss: 7335.08, base loss: 16105.93
[INFO 2017-06-29 08:21:12,668 main.py:57] epoch 9429, training loss: 6473.27, average training loss: 7334.70, base loss: 16105.47
[INFO 2017-06-29 08:21:15,737 main.py:57] epoch 9430, training loss: 6893.20, average training loss: 7334.83, base loss: 16105.15
[INFO 2017-06-29 08:21:18,725 main.py:57] epoch 9431, training loss: 7948.24, average training loss: 7334.43, base loss: 16105.39
[INFO 2017-06-29 08:21:21,780 main.py:57] epoch 9432, training loss: 6929.63, average training loss: 7334.50, base loss: 16105.18
[INFO 2017-06-29 08:21:24,835 main.py:57] epoch 9433, training loss: 7997.51, average training loss: 7335.75, base loss: 16105.22
[INFO 2017-06-29 08:21:27,913 main.py:57] epoch 9434, training loss: 7104.19, average training loss: 7335.85, base loss: 16105.33
[INFO 2017-06-29 08:21:30,974 main.py:57] epoch 9435, training loss: 7557.51, average training loss: 7335.27, base loss: 16105.45
[INFO 2017-06-29 08:21:33,962 main.py:57] epoch 9436, training loss: 7388.54, average training loss: 7336.00, base loss: 16105.02
[INFO 2017-06-29 08:21:37,073 main.py:57] epoch 9437, training loss: 7757.46, average training loss: 7336.81, base loss: 16104.98
[INFO 2017-06-29 08:21:40,137 main.py:57] epoch 9438, training loss: 7133.28, average training loss: 7336.56, base loss: 16104.92
[INFO 2017-06-29 08:21:43,168 main.py:57] epoch 9439, training loss: 7809.85, average training loss: 7336.58, base loss: 16105.11
[INFO 2017-06-29 08:21:46,215 main.py:57] epoch 9440, training loss: 7354.25, average training loss: 7336.54, base loss: 16105.12
[INFO 2017-06-29 08:21:49,200 main.py:57] epoch 9441, training loss: 7114.79, average training loss: 7337.15, base loss: 16105.51
[INFO 2017-06-29 08:21:52,308 main.py:57] epoch 9442, training loss: 6868.13, average training loss: 7336.79, base loss: 16105.67
[INFO 2017-06-29 08:21:55,428 main.py:57] epoch 9443, training loss: 7582.88, average training loss: 7337.37, base loss: 16106.11
[INFO 2017-06-29 08:21:58,459 main.py:57] epoch 9444, training loss: 8147.99, average training loss: 7338.58, base loss: 16106.66
[INFO 2017-06-29 08:22:01,515 main.py:57] epoch 9445, training loss: 7365.32, average training loss: 7337.56, base loss: 16106.72
[INFO 2017-06-29 08:22:04,620 main.py:57] epoch 9446, training loss: 7513.65, average training loss: 7337.45, base loss: 16106.86
[INFO 2017-06-29 08:22:07,607 main.py:57] epoch 9447, training loss: 6476.06, average training loss: 7336.67, base loss: 16105.98
[INFO 2017-06-29 08:22:10,652 main.py:57] epoch 9448, training loss: 6899.81, average training loss: 7335.99, base loss: 16105.84
[INFO 2017-06-29 08:22:13,747 main.py:57] epoch 9449, training loss: 6556.77, average training loss: 7335.51, base loss: 16105.51
[INFO 2017-06-29 08:22:16,778 main.py:57] epoch 9450, training loss: 8195.54, average training loss: 7336.34, base loss: 16105.73
[INFO 2017-06-29 08:22:19,795 main.py:57] epoch 9451, training loss: 7527.77, average training loss: 7336.49, base loss: 16105.86
[INFO 2017-06-29 08:22:22,823 main.py:57] epoch 9452, training loss: 6496.47, average training loss: 7336.16, base loss: 16105.92
[INFO 2017-06-29 08:22:25,843 main.py:57] epoch 9453, training loss: 6336.02, average training loss: 7336.13, base loss: 16105.75
[INFO 2017-06-29 08:22:28,912 main.py:57] epoch 9454, training loss: 7588.97, average training loss: 7336.80, base loss: 16105.80
[INFO 2017-06-29 08:22:31,964 main.py:57] epoch 9455, training loss: 6906.44, average training loss: 7336.39, base loss: 16105.43
[INFO 2017-06-29 08:22:35,016 main.py:57] epoch 9456, training loss: 7946.84, average training loss: 7337.65, base loss: 16105.81
[INFO 2017-06-29 08:22:38,031 main.py:57] epoch 9457, training loss: 8068.57, average training loss: 7338.02, base loss: 16106.20
[INFO 2017-06-29 08:22:41,130 main.py:57] epoch 9458, training loss: 7275.51, average training loss: 7337.62, base loss: 16106.49
[INFO 2017-06-29 08:22:44,161 main.py:57] epoch 9459, training loss: 8551.79, average training loss: 7337.72, base loss: 16106.84
[INFO 2017-06-29 08:22:47,239 main.py:57] epoch 9460, training loss: 7331.79, average training loss: 7338.43, base loss: 16106.64
[INFO 2017-06-29 08:22:50,267 main.py:57] epoch 9461, training loss: 6986.90, average training loss: 7337.58, base loss: 16106.46
[INFO 2017-06-29 08:22:53,384 main.py:57] epoch 9462, training loss: 7771.01, average training loss: 7338.59, base loss: 16106.49
[INFO 2017-06-29 08:22:56,392 main.py:57] epoch 9463, training loss: 7437.90, average training loss: 7339.10, base loss: 16106.98
[INFO 2017-06-29 08:22:59,427 main.py:57] epoch 9464, training loss: 7516.35, average training loss: 7337.65, base loss: 16106.97
[INFO 2017-06-29 08:23:02,497 main.py:57] epoch 9465, training loss: 6687.79, average training loss: 7336.75, base loss: 16106.71
[INFO 2017-06-29 08:23:05,611 main.py:57] epoch 9466, training loss: 7316.71, average training loss: 7336.90, base loss: 16106.56
[INFO 2017-06-29 08:23:08,658 main.py:57] epoch 9467, training loss: 7570.93, average training loss: 7338.14, base loss: 16107.00
[INFO 2017-06-29 08:23:11,736 main.py:57] epoch 9468, training loss: 7278.00, average training loss: 7337.44, base loss: 16106.87
[INFO 2017-06-29 08:23:14,757 main.py:57] epoch 9469, training loss: 7332.32, average training loss: 7337.99, base loss: 16106.69
[INFO 2017-06-29 08:23:17,817 main.py:57] epoch 9470, training loss: 9421.07, average training loss: 7340.63, base loss: 16107.54
[INFO 2017-06-29 08:23:20,892 main.py:57] epoch 9471, training loss: 6781.03, average training loss: 7339.53, base loss: 16107.41
[INFO 2017-06-29 08:23:23,955 main.py:57] epoch 9472, training loss: 7320.94, average training loss: 7339.12, base loss: 16107.45
[INFO 2017-06-29 08:23:27,027 main.py:57] epoch 9473, training loss: 7287.29, average training loss: 7338.66, base loss: 16107.54
[INFO 2017-06-29 08:23:30,178 main.py:57] epoch 9474, training loss: 7429.23, average training loss: 7339.07, base loss: 16107.59
[INFO 2017-06-29 08:23:33,252 main.py:57] epoch 9475, training loss: 7156.61, average training loss: 7338.82, base loss: 16107.68
[INFO 2017-06-29 08:23:36,242 main.py:57] epoch 9476, training loss: 7651.42, average training loss: 7338.92, base loss: 16107.32
[INFO 2017-06-29 08:23:39,282 main.py:57] epoch 9477, training loss: 7281.11, average training loss: 7338.36, base loss: 16107.45
[INFO 2017-06-29 08:23:42,314 main.py:57] epoch 9478, training loss: 7099.43, average training loss: 7338.56, base loss: 16107.54
[INFO 2017-06-29 08:23:45,365 main.py:57] epoch 9479, training loss: 6512.69, average training loss: 7338.28, base loss: 16107.38
[INFO 2017-06-29 08:23:48,404 main.py:57] epoch 9480, training loss: 7656.01, average training loss: 7337.93, base loss: 16107.98
[INFO 2017-06-29 08:23:51,422 main.py:57] epoch 9481, training loss: 8062.47, average training loss: 7338.87, base loss: 16108.12
[INFO 2017-06-29 08:23:54,518 main.py:57] epoch 9482, training loss: 6889.00, average training loss: 7338.96, base loss: 16108.01
[INFO 2017-06-29 08:23:57,510 main.py:57] epoch 9483, training loss: 7643.71, average training loss: 7338.38, base loss: 16108.39
[INFO 2017-06-29 08:24:00,667 main.py:57] epoch 9484, training loss: 7048.65, average training loss: 7338.54, base loss: 16108.42
[INFO 2017-06-29 08:24:03,692 main.py:57] epoch 9485, training loss: 6955.16, average training loss: 7338.41, base loss: 16108.55
[INFO 2017-06-29 08:24:06,723 main.py:57] epoch 9486, training loss: 6941.26, average training loss: 7338.25, base loss: 16108.18
[INFO 2017-06-29 08:24:09,747 main.py:57] epoch 9487, training loss: 7042.12, average training loss: 7337.61, base loss: 16107.74
[INFO 2017-06-29 08:24:12,828 main.py:57] epoch 9488, training loss: 6992.89, average training loss: 7337.64, base loss: 16107.81
[INFO 2017-06-29 08:24:15,819 main.py:57] epoch 9489, training loss: 7760.27, average training loss: 7337.59, base loss: 16107.85
[INFO 2017-06-29 08:24:18,887 main.py:57] epoch 9490, training loss: 7542.55, average training loss: 7337.24, base loss: 16107.96
[INFO 2017-06-29 08:24:22,049 main.py:57] epoch 9491, training loss: 6273.00, average training loss: 7336.18, base loss: 16107.90
[INFO 2017-06-29 08:24:25,099 main.py:57] epoch 9492, training loss: 7708.77, average training loss: 7334.73, base loss: 16107.76
[INFO 2017-06-29 08:24:28,133 main.py:57] epoch 9493, training loss: 7273.07, average training loss: 7335.70, base loss: 16107.45
[INFO 2017-06-29 08:24:31,174 main.py:57] epoch 9494, training loss: 7268.79, average training loss: 7335.58, base loss: 16107.72
[INFO 2017-06-29 08:24:34,283 main.py:57] epoch 9495, training loss: 6411.61, average training loss: 7333.96, base loss: 16107.38
[INFO 2017-06-29 08:24:37,261 main.py:57] epoch 9496, training loss: 7534.06, average training loss: 7332.69, base loss: 16107.57
[INFO 2017-06-29 08:24:40,316 main.py:57] epoch 9497, training loss: 7558.50, average training loss: 7332.92, base loss: 16108.02
[INFO 2017-06-29 08:24:43,356 main.py:57] epoch 9498, training loss: 6686.52, average training loss: 7332.35, base loss: 16108.00
[INFO 2017-06-29 08:24:46,381 main.py:57] epoch 9499, training loss: 7458.60, average training loss: 7331.38, base loss: 16107.82
[INFO 2017-06-29 08:24:46,382 main.py:59] epoch 9499, testing
[INFO 2017-06-29 08:24:58,954 main.py:104] average testing loss: 8230.23, base loss: 17058.37
[INFO 2017-06-29 08:24:58,954 main.py:105] improve_loss: 8828.14, improve_percent: 0.52
[INFO 2017-06-29 08:24:58,955 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:25:02,008 main.py:57] epoch 9500, training loss: 6865.81, average training loss: 7330.92, base loss: 16107.63
[INFO 2017-06-29 08:25:05,046 main.py:57] epoch 9501, training loss: 7259.12, average training loss: 7331.09, base loss: 16107.62
[INFO 2017-06-29 08:25:08,082 main.py:57] epoch 9502, training loss: 7105.18, average training loss: 7329.62, base loss: 16107.71
[INFO 2017-06-29 08:25:11,164 main.py:57] epoch 9503, training loss: 7254.69, average training loss: 7329.53, base loss: 16107.73
[INFO 2017-06-29 08:25:14,204 main.py:57] epoch 9504, training loss: 7840.05, average training loss: 7329.57, base loss: 16108.00
[INFO 2017-06-29 08:25:17,231 main.py:57] epoch 9505, training loss: 8153.81, average training loss: 7330.02, base loss: 16108.33
[INFO 2017-06-29 08:25:20,343 main.py:57] epoch 9506, training loss: 6654.72, average training loss: 7328.90, base loss: 16108.18
[INFO 2017-06-29 08:25:23,354 main.py:57] epoch 9507, training loss: 7807.26, average training loss: 7329.58, base loss: 16108.36
[INFO 2017-06-29 08:25:26,337 main.py:57] epoch 9508, training loss: 7642.84, average training loss: 7330.76, base loss: 16108.70
[INFO 2017-06-29 08:25:29,435 main.py:57] epoch 9509, training loss: 7908.68, average training loss: 7329.76, base loss: 16108.92
[INFO 2017-06-29 08:25:32,534 main.py:57] epoch 9510, training loss: 7756.97, average training loss: 7330.47, base loss: 16109.03
[INFO 2017-06-29 08:25:35,598 main.py:57] epoch 9511, training loss: 7472.77, average training loss: 7331.02, base loss: 16109.10
[INFO 2017-06-29 08:25:38,614 main.py:57] epoch 9512, training loss: 6790.51, average training loss: 7330.44, base loss: 16108.89
[INFO 2017-06-29 08:25:41,695 main.py:57] epoch 9513, training loss: 7930.84, average training loss: 7331.57, base loss: 16109.36
[INFO 2017-06-29 08:25:44,781 main.py:57] epoch 9514, training loss: 6721.87, average training loss: 7331.47, base loss: 16109.05
[INFO 2017-06-29 08:25:47,832 main.py:57] epoch 9515, training loss: 6873.61, average training loss: 7329.32, base loss: 16108.74
[INFO 2017-06-29 08:25:50,885 main.py:57] epoch 9516, training loss: 7057.39, average training loss: 7328.73, base loss: 16108.41
[INFO 2017-06-29 08:25:54,019 main.py:57] epoch 9517, training loss: 6651.35, average training loss: 7328.17, base loss: 16107.95
[INFO 2017-06-29 08:25:57,081 main.py:57] epoch 9518, training loss: 7596.25, average training loss: 7328.15, base loss: 16108.35
[INFO 2017-06-29 08:26:00,182 main.py:57] epoch 9519, training loss: 8167.23, average training loss: 7329.26, base loss: 16108.48
[INFO 2017-06-29 08:26:03,233 main.py:57] epoch 9520, training loss: 6992.99, average training loss: 7328.69, base loss: 16108.25
[INFO 2017-06-29 08:26:06,315 main.py:57] epoch 9521, training loss: 7469.66, average training loss: 7327.63, base loss: 16108.18
[INFO 2017-06-29 08:26:09,368 main.py:57] epoch 9522, training loss: 6763.52, average training loss: 7327.46, base loss: 16108.28
[INFO 2017-06-29 08:26:12,422 main.py:57] epoch 9523, training loss: 6945.08, average training loss: 7326.70, base loss: 16108.03
[INFO 2017-06-29 08:26:15,548 main.py:57] epoch 9524, training loss: 7138.06, average training loss: 7325.20, base loss: 16107.75
[INFO 2017-06-29 08:26:18,587 main.py:57] epoch 9525, training loss: 7279.70, average training loss: 7323.96, base loss: 16107.96
[INFO 2017-06-29 08:26:21,631 main.py:57] epoch 9526, training loss: 7036.99, average training loss: 7323.01, base loss: 16108.03
[INFO 2017-06-29 08:26:24,687 main.py:57] epoch 9527, training loss: 6715.48, average training loss: 7323.05, base loss: 16107.83
[INFO 2017-06-29 08:26:27,725 main.py:57] epoch 9528, training loss: 6849.06, average training loss: 7322.51, base loss: 16107.49
[INFO 2017-06-29 08:26:30,800 main.py:57] epoch 9529, training loss: 7984.73, average training loss: 7324.03, base loss: 16107.97
[INFO 2017-06-29 08:26:33,818 main.py:57] epoch 9530, training loss: 7887.98, average training loss: 7323.67, base loss: 16108.47
[INFO 2017-06-29 08:26:36,837 main.py:57] epoch 9531, training loss: 7052.81, average training loss: 7322.94, base loss: 16108.63
[INFO 2017-06-29 08:26:39,831 main.py:57] epoch 9532, training loss: 7699.55, average training loss: 7323.43, base loss: 16108.85
[INFO 2017-06-29 08:26:42,894 main.py:57] epoch 9533, training loss: 7052.42, average training loss: 7323.15, base loss: 16108.61
[INFO 2017-06-29 08:26:46,042 main.py:57] epoch 9534, training loss: 7661.33, average training loss: 7323.30, base loss: 16108.61
[INFO 2017-06-29 08:26:49,110 main.py:57] epoch 9535, training loss: 6672.84, average training loss: 7323.85, base loss: 16108.42
[INFO 2017-06-29 08:26:52,091 main.py:57] epoch 9536, training loss: 6718.59, average training loss: 7321.93, base loss: 16108.03
[INFO 2017-06-29 08:26:55,118 main.py:57] epoch 9537, training loss: 8766.09, average training loss: 7323.56, base loss: 16108.23
[INFO 2017-06-29 08:26:58,240 main.py:57] epoch 9538, training loss: 6654.99, average training loss: 7323.19, base loss: 16107.65
[INFO 2017-06-29 08:27:01,310 main.py:57] epoch 9539, training loss: 7276.06, average training loss: 7322.90, base loss: 16107.24
[INFO 2017-06-29 08:27:04,339 main.py:57] epoch 9540, training loss: 6577.69, average training loss: 7321.43, base loss: 16107.03
[INFO 2017-06-29 08:27:07,383 main.py:57] epoch 9541, training loss: 6588.59, average training loss: 7319.94, base loss: 16106.66
[INFO 2017-06-29 08:27:10,455 main.py:57] epoch 9542, training loss: 7033.31, average training loss: 7320.74, base loss: 16106.54
[INFO 2017-06-29 08:27:13,533 main.py:57] epoch 9543, training loss: 8683.34, average training loss: 7322.63, base loss: 16106.88
[INFO 2017-06-29 08:27:16,548 main.py:57] epoch 9544, training loss: 8169.23, average training loss: 7323.10, base loss: 16107.13
[INFO 2017-06-29 08:27:19,547 main.py:57] epoch 9545, training loss: 7861.21, average training loss: 7323.31, base loss: 16107.21
[INFO 2017-06-29 08:27:22,576 main.py:57] epoch 9546, training loss: 6886.99, average training loss: 7323.15, base loss: 16107.31
[INFO 2017-06-29 08:27:25,622 main.py:57] epoch 9547, training loss: 7191.67, average training loss: 7322.96, base loss: 16107.40
[INFO 2017-06-29 08:27:28,631 main.py:57] epoch 9548, training loss: 7899.50, average training loss: 7323.25, base loss: 16107.17
[INFO 2017-06-29 08:27:31,686 main.py:57] epoch 9549, training loss: 6490.94, average training loss: 7321.52, base loss: 16106.63
[INFO 2017-06-29 08:27:34,703 main.py:57] epoch 9550, training loss: 7919.31, average training loss: 7321.67, base loss: 16106.93
[INFO 2017-06-29 08:27:37,792 main.py:57] epoch 9551, training loss: 6743.80, average training loss: 7320.44, base loss: 16106.56
[INFO 2017-06-29 08:27:40,857 main.py:57] epoch 9552, training loss: 7381.06, average training loss: 7320.80, base loss: 16106.57
[INFO 2017-06-29 08:27:43,956 main.py:57] epoch 9553, training loss: 7990.39, average training loss: 7321.15, base loss: 16106.70
[INFO 2017-06-29 08:27:47,003 main.py:57] epoch 9554, training loss: 8959.06, average training loss: 7323.07, base loss: 16107.31
[INFO 2017-06-29 08:27:50,049 main.py:57] epoch 9555, training loss: 7723.99, average training loss: 7322.05, base loss: 16107.45
[INFO 2017-06-29 08:27:53,159 main.py:57] epoch 9556, training loss: 7399.29, average training loss: 7321.65, base loss: 16107.82
[INFO 2017-06-29 08:27:56,300 main.py:57] epoch 9557, training loss: 7451.32, average training loss: 7321.31, base loss: 16107.84
[INFO 2017-06-29 08:27:59,325 main.py:57] epoch 9558, training loss: 6785.90, average training loss: 7319.70, base loss: 16107.39
[INFO 2017-06-29 08:28:02,437 main.py:57] epoch 9559, training loss: 7778.69, average training loss: 7319.88, base loss: 16107.39
[INFO 2017-06-29 08:28:05,663 main.py:57] epoch 9560, training loss: 7681.41, average training loss: 7321.01, base loss: 16107.32
[INFO 2017-06-29 08:28:08,706 main.py:57] epoch 9561, training loss: 8497.11, average training loss: 7323.27, base loss: 16107.69
[INFO 2017-06-29 08:28:11,727 main.py:57] epoch 9562, training loss: 7949.85, average training loss: 7323.59, base loss: 16107.66
[INFO 2017-06-29 08:28:14,784 main.py:57] epoch 9563, training loss: 8150.98, average training loss: 7324.21, base loss: 16107.93
[INFO 2017-06-29 08:28:17,876 main.py:57] epoch 9564, training loss: 6390.13, average training loss: 7322.21, base loss: 16107.79
[INFO 2017-06-29 08:28:20,950 main.py:57] epoch 9565, training loss: 8316.13, average training loss: 7323.59, base loss: 16108.39
[INFO 2017-06-29 08:28:23,942 main.py:57] epoch 9566, training loss: 8052.30, average training loss: 7324.97, base loss: 16108.67
[INFO 2017-06-29 08:28:26,998 main.py:57] epoch 9567, training loss: 7035.12, average training loss: 7324.52, base loss: 16108.44
[INFO 2017-06-29 08:28:30,022 main.py:57] epoch 9568, training loss: 7762.80, average training loss: 7324.75, base loss: 16108.58
[INFO 2017-06-29 08:28:33,051 main.py:57] epoch 9569, training loss: 7582.51, average training loss: 7324.30, base loss: 16108.61
[INFO 2017-06-29 08:28:36,056 main.py:57] epoch 9570, training loss: 7215.40, average training loss: 7324.28, base loss: 16108.60
[INFO 2017-06-29 08:28:39,117 main.py:57] epoch 9571, training loss: 8332.01, average training loss: 7325.67, base loss: 16108.95
[INFO 2017-06-29 08:28:42,183 main.py:57] epoch 9572, training loss: 7099.53, average training loss: 7326.24, base loss: 16108.71
[INFO 2017-06-29 08:28:45,235 main.py:57] epoch 9573, training loss: 7141.36, average training loss: 7326.13, base loss: 16108.51
[INFO 2017-06-29 08:28:48,224 main.py:57] epoch 9574, training loss: 7229.65, average training loss: 7326.23, base loss: 16108.35
[INFO 2017-06-29 08:28:51,310 main.py:57] epoch 9575, training loss: 7972.05, average training loss: 7327.03, base loss: 16108.57
[INFO 2017-06-29 08:28:54,431 main.py:57] epoch 9576, training loss: 6758.65, average training loss: 7327.26, base loss: 16108.42
[INFO 2017-06-29 08:28:57,459 main.py:57] epoch 9577, training loss: 7396.89, average training loss: 7326.97, base loss: 16108.59
[INFO 2017-06-29 08:29:00,516 main.py:57] epoch 9578, training loss: 8986.11, average training loss: 7329.11, base loss: 16109.33
[INFO 2017-06-29 08:29:03,540 main.py:57] epoch 9579, training loss: 7065.97, average training loss: 7329.01, base loss: 16109.62
[INFO 2017-06-29 08:29:06,638 main.py:57] epoch 9580, training loss: 8168.12, average training loss: 7330.50, base loss: 16110.28
[INFO 2017-06-29 08:29:09,709 main.py:57] epoch 9581, training loss: 7737.79, average training loss: 7330.77, base loss: 16110.71
[INFO 2017-06-29 08:29:12,763 main.py:57] epoch 9582, training loss: 7417.50, average training loss: 7330.76, base loss: 16110.62
[INFO 2017-06-29 08:29:15,733 main.py:57] epoch 9583, training loss: 6927.70, average training loss: 7331.00, base loss: 16110.45
[INFO 2017-06-29 08:29:18,722 main.py:57] epoch 9584, training loss: 9088.56, average training loss: 7332.38, base loss: 16110.69
[INFO 2017-06-29 08:29:21,746 main.py:57] epoch 9585, training loss: 7709.62, average training loss: 7333.10, base loss: 16110.48
[INFO 2017-06-29 08:29:24,794 main.py:57] epoch 9586, training loss: 7751.97, average training loss: 7332.68, base loss: 16110.34
[INFO 2017-06-29 08:29:27,972 main.py:57] epoch 9587, training loss: 8582.83, average training loss: 7334.25, base loss: 16110.37
[INFO 2017-06-29 08:29:31,078 main.py:57] epoch 9588, training loss: 6586.96, average training loss: 7333.52, base loss: 16110.21
[INFO 2017-06-29 08:29:34,189 main.py:57] epoch 9589, training loss: 7345.54, average training loss: 7333.48, base loss: 16110.37
[INFO 2017-06-29 08:29:37,247 main.py:57] epoch 9590, training loss: 7311.03, average training loss: 7333.07, base loss: 16110.51
[INFO 2017-06-29 08:29:40,368 main.py:57] epoch 9591, training loss: 6901.95, average training loss: 7332.92, base loss: 16110.32
[INFO 2017-06-29 08:29:43,456 main.py:57] epoch 9592, training loss: 7975.36, average training loss: 7333.65, base loss: 16110.81
[INFO 2017-06-29 08:29:46,444 main.py:57] epoch 9593, training loss: 7449.39, average training loss: 7334.05, base loss: 16110.76
[INFO 2017-06-29 08:29:49,476 main.py:57] epoch 9594, training loss: 7188.30, average training loss: 7332.98, base loss: 16110.50
[INFO 2017-06-29 08:29:52,479 main.py:57] epoch 9595, training loss: 8249.94, average training loss: 7333.47, base loss: 16110.96
[INFO 2017-06-29 08:29:55,519 main.py:57] epoch 9596, training loss: 6268.83, average training loss: 7332.41, base loss: 16110.53
[INFO 2017-06-29 08:29:58,526 main.py:57] epoch 9597, training loss: 6117.02, average training loss: 7330.33, base loss: 16109.84
[INFO 2017-06-29 08:30:01,541 main.py:57] epoch 9598, training loss: 7991.15, average training loss: 7331.33, base loss: 16110.24
[INFO 2017-06-29 08:30:04,597 main.py:57] epoch 9599, training loss: 6103.37, average training loss: 7329.52, base loss: 16109.72
[INFO 2017-06-29 08:30:04,597 main.py:59] epoch 9599, testing
[INFO 2017-06-29 08:30:17,363 main.py:104] average testing loss: 7886.47, base loss: 16595.12
[INFO 2017-06-29 08:30:17,363 main.py:105] improve_loss: 8708.65, improve_percent: 0.52
[INFO 2017-06-29 08:30:17,365 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:30:20,442 main.py:57] epoch 9600, training loss: 7160.67, average training loss: 7329.10, base loss: 16109.67
[INFO 2017-06-29 08:30:23,461 main.py:57] epoch 9601, training loss: 6861.88, average training loss: 7329.01, base loss: 16109.28
[INFO 2017-06-29 08:30:26,545 main.py:57] epoch 9602, training loss: 7058.02, average training loss: 7327.80, base loss: 16109.09
[INFO 2017-06-29 08:30:29,597 main.py:57] epoch 9603, training loss: 7217.30, average training loss: 7328.37, base loss: 16108.73
[INFO 2017-06-29 08:30:32,716 main.py:57] epoch 9604, training loss: 6728.47, average training loss: 7328.42, base loss: 16108.16
[INFO 2017-06-29 08:30:35,785 main.py:57] epoch 9605, training loss: 7002.66, average training loss: 7328.54, base loss: 16107.64
[INFO 2017-06-29 08:30:38,791 main.py:57] epoch 9606, training loss: 6904.11, average training loss: 7328.62, base loss: 16107.29
[INFO 2017-06-29 08:30:41,822 main.py:57] epoch 9607, training loss: 8412.84, average training loss: 7330.05, base loss: 16107.68
[INFO 2017-06-29 08:30:44,854 main.py:57] epoch 9608, training loss: 6368.74, average training loss: 7328.60, base loss: 16107.43
[INFO 2017-06-29 08:30:47,882 main.py:57] epoch 9609, training loss: 6779.63, average training loss: 7328.51, base loss: 16107.20
[INFO 2017-06-29 08:30:50,894 main.py:57] epoch 9610, training loss: 7352.62, average training loss: 7328.45, base loss: 16107.24
[INFO 2017-06-29 08:30:53,893 main.py:57] epoch 9611, training loss: 6742.97, average training loss: 7327.39, base loss: 16107.12
[INFO 2017-06-29 08:30:56,987 main.py:57] epoch 9612, training loss: 6491.76, average training loss: 7327.35, base loss: 16106.93
[INFO 2017-06-29 08:31:00,020 main.py:57] epoch 9613, training loss: 8471.21, average training loss: 7327.63, base loss: 16107.60
[INFO 2017-06-29 08:31:03,173 main.py:57] epoch 9614, training loss: 8131.73, average training loss: 7328.58, base loss: 16108.18
[INFO 2017-06-29 08:31:06,235 main.py:57] epoch 9615, training loss: 7454.81, average training loss: 7328.55, base loss: 16108.26
[INFO 2017-06-29 08:31:09,285 main.py:57] epoch 9616, training loss: 8274.13, average training loss: 7329.61, base loss: 16108.70
[INFO 2017-06-29 08:31:12,338 main.py:57] epoch 9617, training loss: 6660.53, average training loss: 7328.93, base loss: 16108.72
[INFO 2017-06-29 08:31:15,360 main.py:57] epoch 9618, training loss: 8567.01, average training loss: 7330.42, base loss: 16109.24
[INFO 2017-06-29 08:31:18,406 main.py:57] epoch 9619, training loss: 7067.75, average training loss: 7330.26, base loss: 16109.26
[INFO 2017-06-29 08:31:21,413 main.py:57] epoch 9620, training loss: 6665.68, average training loss: 7330.06, base loss: 16108.78
[INFO 2017-06-29 08:31:24,458 main.py:57] epoch 9621, training loss: 6745.10, average training loss: 7329.86, base loss: 16108.42
[INFO 2017-06-29 08:31:27,535 main.py:57] epoch 9622, training loss: 7303.20, average training loss: 7330.60, base loss: 16108.22
[INFO 2017-06-29 08:31:30,565 main.py:57] epoch 9623, training loss: 7463.34, average training loss: 7330.95, base loss: 16108.10
[INFO 2017-06-29 08:31:33,607 main.py:57] epoch 9624, training loss: 6922.82, average training loss: 7330.81, base loss: 16108.01
[INFO 2017-06-29 08:31:36,699 main.py:57] epoch 9625, training loss: 7203.03, average training loss: 7331.04, base loss: 16108.20
[INFO 2017-06-29 08:31:39,759 main.py:57] epoch 9626, training loss: 7557.02, average training loss: 7329.11, base loss: 16108.74
[INFO 2017-06-29 08:31:42,766 main.py:57] epoch 9627, training loss: 6999.77, average training loss: 7328.58, base loss: 16108.66
[INFO 2017-06-29 08:31:45,799 main.py:57] epoch 9628, training loss: 7546.38, average training loss: 7328.58, base loss: 16108.58
[INFO 2017-06-29 08:31:48,799 main.py:57] epoch 9629, training loss: 7943.08, average training loss: 7329.70, base loss: 16108.66
[INFO 2017-06-29 08:31:51,813 main.py:57] epoch 9630, training loss: 6966.49, average training loss: 7329.82, base loss: 16108.20
[INFO 2017-06-29 08:31:54,905 main.py:57] epoch 9631, training loss: 7710.76, average training loss: 7331.03, base loss: 16108.00
[INFO 2017-06-29 08:31:57,952 main.py:57] epoch 9632, training loss: 7438.78, average training loss: 7331.82, base loss: 16108.15
[INFO 2017-06-29 08:32:01,027 main.py:57] epoch 9633, training loss: 6522.70, average training loss: 7331.63, base loss: 16108.02
[INFO 2017-06-29 08:32:04,142 main.py:57] epoch 9634, training loss: 7405.35, average training loss: 7332.06, base loss: 16108.13
[INFO 2017-06-29 08:32:07,249 main.py:57] epoch 9635, training loss: 7822.48, average training loss: 7331.98, base loss: 16108.22
[INFO 2017-06-29 08:32:10,250 main.py:57] epoch 9636, training loss: 6379.89, average training loss: 7329.57, base loss: 16107.82
[INFO 2017-06-29 08:32:13,237 main.py:57] epoch 9637, training loss: 7733.17, average training loss: 7330.29, base loss: 16107.81
[INFO 2017-06-29 08:32:16,310 main.py:57] epoch 9638, training loss: 7562.18, average training loss: 7330.82, base loss: 16107.98
[INFO 2017-06-29 08:32:19,397 main.py:57] epoch 9639, training loss: 6692.35, average training loss: 7330.91, base loss: 16107.82
[INFO 2017-06-29 08:32:22,433 main.py:57] epoch 9640, training loss: 7853.17, average training loss: 7331.57, base loss: 16108.13
[INFO 2017-06-29 08:32:25,511 main.py:57] epoch 9641, training loss: 7382.50, average training loss: 7331.42, base loss: 16108.05
[INFO 2017-06-29 08:32:28,594 main.py:57] epoch 9642, training loss: 7274.21, average training loss: 7331.07, base loss: 16108.22
[INFO 2017-06-29 08:32:31,607 main.py:57] epoch 9643, training loss: 6930.46, average training loss: 7331.92, base loss: 16108.08
[INFO 2017-06-29 08:32:34,683 main.py:57] epoch 9644, training loss: 6657.18, average training loss: 7330.99, base loss: 16107.90
[INFO 2017-06-29 08:32:37,730 main.py:57] epoch 9645, training loss: 6587.26, average training loss: 7330.13, base loss: 16107.35
[INFO 2017-06-29 08:32:40,776 main.py:57] epoch 9646, training loss: 7963.94, average training loss: 7329.94, base loss: 16107.51
[INFO 2017-06-29 08:32:43,868 main.py:57] epoch 9647, training loss: 6764.91, average training loss: 7329.13, base loss: 16107.24
[INFO 2017-06-29 08:32:46,891 main.py:57] epoch 9648, training loss: 6903.52, average training loss: 7328.79, base loss: 16107.11
[INFO 2017-06-29 08:32:50,009 main.py:57] epoch 9649, training loss: 7090.03, average training loss: 7327.55, base loss: 16107.00
[INFO 2017-06-29 08:32:53,121 main.py:57] epoch 9650, training loss: 8181.13, average training loss: 7328.18, base loss: 16107.21
[INFO 2017-06-29 08:32:56,236 main.py:57] epoch 9651, training loss: 6904.08, average training loss: 7326.89, base loss: 16106.89
[INFO 2017-06-29 08:32:59,355 main.py:57] epoch 9652, training loss: 6468.20, average training loss: 7327.29, base loss: 16106.72
[INFO 2017-06-29 08:33:02,432 main.py:57] epoch 9653, training loss: 6882.84, average training loss: 7327.64, base loss: 16106.73
[INFO 2017-06-29 08:33:05,507 main.py:57] epoch 9654, training loss: 7393.31, average training loss: 7326.82, base loss: 16106.77
[INFO 2017-06-29 08:33:08,569 main.py:57] epoch 9655, training loss: 7386.31, average training loss: 7326.91, base loss: 16106.82
[INFO 2017-06-29 08:33:11,665 main.py:57] epoch 9656, training loss: 6865.17, average training loss: 7327.38, base loss: 16106.81
[INFO 2017-06-29 08:33:14,753 main.py:57] epoch 9657, training loss: 7087.53, average training loss: 7328.00, base loss: 16106.80
[INFO 2017-06-29 08:33:17,748 main.py:57] epoch 9658, training loss: 8145.42, average training loss: 7328.33, base loss: 16107.65
[INFO 2017-06-29 08:33:20,810 main.py:57] epoch 9659, training loss: 6942.21, average training loss: 7326.88, base loss: 16107.54
[INFO 2017-06-29 08:33:23,894 main.py:57] epoch 9660, training loss: 6642.03, average training loss: 7326.17, base loss: 16107.75
[INFO 2017-06-29 08:33:26,935 main.py:57] epoch 9661, training loss: 7118.09, average training loss: 7326.06, base loss: 16108.03
[INFO 2017-06-29 08:33:29,991 main.py:57] epoch 9662, training loss: 7051.49, average training loss: 7326.05, base loss: 16108.04
[INFO 2017-06-29 08:33:33,093 main.py:57] epoch 9663, training loss: 6392.28, average training loss: 7324.81, base loss: 16107.64
[INFO 2017-06-29 08:33:36,108 main.py:57] epoch 9664, training loss: 7367.97, average training loss: 7325.18, base loss: 16107.17
[INFO 2017-06-29 08:33:39,153 main.py:57] epoch 9665, training loss: 7241.53, average training loss: 7325.78, base loss: 16106.58
[INFO 2017-06-29 08:33:42,191 main.py:57] epoch 9666, training loss: 7079.52, average training loss: 7325.12, base loss: 16106.51
[INFO 2017-06-29 08:33:45,254 main.py:57] epoch 9667, training loss: 8162.61, average training loss: 7324.99, base loss: 16106.68
[INFO 2017-06-29 08:33:48,331 main.py:57] epoch 9668, training loss: 8264.96, average training loss: 7325.52, base loss: 16107.26
[INFO 2017-06-29 08:33:51,389 main.py:57] epoch 9669, training loss: 7273.78, average training loss: 7326.37, base loss: 16107.47
[INFO 2017-06-29 08:33:54,442 main.py:57] epoch 9670, training loss: 6514.16, average training loss: 7325.44, base loss: 16107.05
[INFO 2017-06-29 08:33:57,511 main.py:57] epoch 9671, training loss: 7810.27, average training loss: 7325.91, base loss: 16107.71
[INFO 2017-06-29 08:34:00,616 main.py:57] epoch 9672, training loss: 7948.93, average training loss: 7326.27, base loss: 16107.85
[INFO 2017-06-29 08:34:03,653 main.py:57] epoch 9673, training loss: 6749.17, average training loss: 7325.93, base loss: 16107.41
[INFO 2017-06-29 08:34:06,712 main.py:57] epoch 9674, training loss: 7767.87, average training loss: 7325.62, base loss: 16107.67
[INFO 2017-06-29 08:34:09,788 main.py:57] epoch 9675, training loss: 6955.48, average training loss: 7325.98, base loss: 16107.71
[INFO 2017-06-29 08:34:12,781 main.py:57] epoch 9676, training loss: 7095.47, average training loss: 7326.25, base loss: 16107.82
[INFO 2017-06-29 08:34:15,877 main.py:57] epoch 9677, training loss: 6411.02, average training loss: 7325.74, base loss: 16107.49
[INFO 2017-06-29 08:34:18,896 main.py:57] epoch 9678, training loss: 6935.79, average training loss: 7326.68, base loss: 16107.37
[INFO 2017-06-29 08:34:21,923 main.py:57] epoch 9679, training loss: 7310.80, average training loss: 7327.12, base loss: 16107.36
[INFO 2017-06-29 08:34:25,045 main.py:57] epoch 9680, training loss: 7019.58, average training loss: 7326.87, base loss: 16107.10
[INFO 2017-06-29 08:34:28,055 main.py:57] epoch 9681, training loss: 6874.94, average training loss: 7326.98, base loss: 16106.98
[INFO 2017-06-29 08:34:31,125 main.py:57] epoch 9682, training loss: 6567.18, average training loss: 7326.17, base loss: 16107.13
[INFO 2017-06-29 08:34:34,155 main.py:57] epoch 9683, training loss: 7993.79, average training loss: 7326.95, base loss: 16107.46
[INFO 2017-06-29 08:34:37,239 main.py:57] epoch 9684, training loss: 6855.06, average training loss: 7326.90, base loss: 16106.96
[INFO 2017-06-29 08:34:40,237 main.py:57] epoch 9685, training loss: 7334.82, average training loss: 7327.22, base loss: 16106.45
[INFO 2017-06-29 08:34:43,306 main.py:57] epoch 9686, training loss: 7295.23, average training loss: 7327.48, base loss: 16106.51
[INFO 2017-06-29 08:34:46,476 main.py:57] epoch 9687, training loss: 7094.80, average training loss: 7327.18, base loss: 16106.17
[INFO 2017-06-29 08:34:49,492 main.py:57] epoch 9688, training loss: 5932.82, average training loss: 7326.06, base loss: 16105.76
[INFO 2017-06-29 08:34:52,576 main.py:57] epoch 9689, training loss: 7094.13, average training loss: 7325.51, base loss: 16105.97
[INFO 2017-06-29 08:34:55,611 main.py:57] epoch 9690, training loss: 7206.43, average training loss: 7326.04, base loss: 16105.97
[INFO 2017-06-29 08:34:58,619 main.py:57] epoch 9691, training loss: 6895.69, average training loss: 7325.59, base loss: 16105.60
[INFO 2017-06-29 08:35:01,615 main.py:57] epoch 9692, training loss: 6472.01, average training loss: 7325.03, base loss: 16105.23
[INFO 2017-06-29 08:35:04,727 main.py:57] epoch 9693, training loss: 7426.94, average training loss: 7325.40, base loss: 16105.02
[INFO 2017-06-29 08:35:07,697 main.py:57] epoch 9694, training loss: 7010.09, average training loss: 7325.50, base loss: 16105.27
[INFO 2017-06-29 08:35:10,739 main.py:57] epoch 9695, training loss: 6689.48, average training loss: 7324.30, base loss: 16105.19
[INFO 2017-06-29 08:35:13,705 main.py:57] epoch 9696, training loss: 7707.07, average training loss: 7324.42, base loss: 16105.54
[INFO 2017-06-29 08:35:16,725 main.py:57] epoch 9697, training loss: 7475.71, average training loss: 7325.18, base loss: 16105.66
[INFO 2017-06-29 08:35:19,761 main.py:57] epoch 9698, training loss: 7902.90, average training loss: 7325.61, base loss: 16105.76
[INFO 2017-06-29 08:35:22,786 main.py:57] epoch 9699, training loss: 7045.18, average training loss: 7325.46, base loss: 16105.82
[INFO 2017-06-29 08:35:22,786 main.py:59] epoch 9699, testing
[INFO 2017-06-29 08:35:35,440 main.py:104] average testing loss: 8115.38, base loss: 17185.20
[INFO 2017-06-29 08:35:35,440 main.py:105] improve_loss: 9069.82, improve_percent: 0.53
[INFO 2017-06-29 08:35:35,442 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:35:38,431 main.py:57] epoch 9700, training loss: 6721.27, average training loss: 7325.56, base loss: 16105.31
[INFO 2017-06-29 08:35:41,479 main.py:57] epoch 9701, training loss: 6744.47, average training loss: 7325.11, base loss: 16104.84
[INFO 2017-06-29 08:35:44,452 main.py:57] epoch 9702, training loss: 6380.43, average training loss: 7323.52, base loss: 16104.27
[INFO 2017-06-29 08:35:47,499 main.py:57] epoch 9703, training loss: 7439.44, average training loss: 7321.98, base loss: 16103.72
[INFO 2017-06-29 08:35:50,516 main.py:57] epoch 9704, training loss: 8279.96, average training loss: 7322.07, base loss: 16104.75
[INFO 2017-06-29 08:35:53,514 main.py:57] epoch 9705, training loss: 7644.10, average training loss: 7322.16, base loss: 16105.25
[INFO 2017-06-29 08:35:56,562 main.py:57] epoch 9706, training loss: 7821.98, average training loss: 7322.17, base loss: 16105.28
[INFO 2017-06-29 08:35:59,607 main.py:57] epoch 9707, training loss: 6458.78, average training loss: 7321.29, base loss: 16104.96
[INFO 2017-06-29 08:36:02,579 main.py:57] epoch 9708, training loss: 6713.93, average training loss: 7320.58, base loss: 16104.67
[INFO 2017-06-29 08:36:05,704 main.py:57] epoch 9709, training loss: 6222.34, average training loss: 7319.44, base loss: 16104.15
[INFO 2017-06-29 08:36:08,717 main.py:57] epoch 9710, training loss: 7327.04, average training loss: 7319.76, base loss: 16104.17
[INFO 2017-06-29 08:36:11,743 main.py:57] epoch 9711, training loss: 7198.10, average training loss: 7318.69, base loss: 16103.93
[INFO 2017-06-29 08:36:14,860 main.py:57] epoch 9712, training loss: 6678.54, average training loss: 7318.35, base loss: 16103.53
[INFO 2017-06-29 08:36:17,930 main.py:57] epoch 9713, training loss: 6630.15, average training loss: 7318.38, base loss: 16103.23
[INFO 2017-06-29 08:36:20,923 main.py:57] epoch 9714, training loss: 6730.41, average training loss: 7318.22, base loss: 16103.19
[INFO 2017-06-29 08:36:23,986 main.py:57] epoch 9715, training loss: 8077.69, average training loss: 7318.88, base loss: 16103.33
[INFO 2017-06-29 08:36:27,032 main.py:57] epoch 9716, training loss: 6477.07, average training loss: 7318.23, base loss: 16103.09
[INFO 2017-06-29 08:36:30,088 main.py:57] epoch 9717, training loss: 7014.45, average training loss: 7318.52, base loss: 16103.08
[INFO 2017-06-29 08:36:33,117 main.py:57] epoch 9718, training loss: 7121.39, average training loss: 7319.24, base loss: 16102.84
[INFO 2017-06-29 08:36:36,171 main.py:57] epoch 9719, training loss: 6914.50, average training loss: 7319.43, base loss: 16102.38
[INFO 2017-06-29 08:36:39,156 main.py:57] epoch 9720, training loss: 6849.23, average training loss: 7318.95, base loss: 16101.87
[INFO 2017-06-29 08:36:42,193 main.py:57] epoch 9721, training loss: 7282.54, average training loss: 7318.01, base loss: 16101.83
[INFO 2017-06-29 08:36:45,235 main.py:57] epoch 9722, training loss: 7471.64, average training loss: 7317.08, base loss: 16102.00
[INFO 2017-06-29 08:36:48,319 main.py:57] epoch 9723, training loss: 8288.59, average training loss: 7318.42, base loss: 16102.59
[INFO 2017-06-29 08:36:51,334 main.py:57] epoch 9724, training loss: 8020.35, average training loss: 7319.09, base loss: 16102.88
[INFO 2017-06-29 08:36:54,399 main.py:57] epoch 9725, training loss: 8288.61, average training loss: 7320.24, base loss: 16103.11
[INFO 2017-06-29 08:36:57,430 main.py:57] epoch 9726, training loss: 7041.64, average training loss: 7320.27, base loss: 16102.97
[INFO 2017-06-29 08:37:00,483 main.py:57] epoch 9727, training loss: 7363.28, average training loss: 7320.48, base loss: 16102.99
[INFO 2017-06-29 08:37:03,495 main.py:57] epoch 9728, training loss: 7715.33, average training loss: 7320.78, base loss: 16103.36
[INFO 2017-06-29 08:37:06,577 main.py:57] epoch 9729, training loss: 6502.21, average training loss: 7320.19, base loss: 16103.09
[INFO 2017-06-29 08:37:09,720 main.py:57] epoch 9730, training loss: 7738.03, average training loss: 7321.04, base loss: 16102.93
[INFO 2017-06-29 08:37:12,809 main.py:57] epoch 9731, training loss: 8276.23, average training loss: 7321.87, base loss: 16103.03
[INFO 2017-06-29 08:37:15,874 main.py:57] epoch 9732, training loss: 6665.79, average training loss: 7321.09, base loss: 16102.74
[INFO 2017-06-29 08:37:18,916 main.py:57] epoch 9733, training loss: 7719.65, average training loss: 7321.59, base loss: 16103.03
[INFO 2017-06-29 08:37:21,924 main.py:57] epoch 9734, training loss: 8251.29, average training loss: 7323.03, base loss: 16103.48
[INFO 2017-06-29 08:37:24,990 main.py:57] epoch 9735, training loss: 7060.10, average training loss: 7322.98, base loss: 16103.15
[INFO 2017-06-29 08:37:28,040 main.py:57] epoch 9736, training loss: 7169.52, average training loss: 7324.12, base loss: 16103.00
[INFO 2017-06-29 08:37:31,056 main.py:57] epoch 9737, training loss: 7491.77, average training loss: 7324.59, base loss: 16103.05
[INFO 2017-06-29 08:37:34,109 main.py:57] epoch 9738, training loss: 6743.27, average training loss: 7323.30, base loss: 16102.79
[INFO 2017-06-29 08:37:37,138 main.py:57] epoch 9739, training loss: 7578.08, average training loss: 7322.69, base loss: 16102.97
[INFO 2017-06-29 08:37:40,157 main.py:57] epoch 9740, training loss: 6906.01, average training loss: 7322.42, base loss: 16102.92
[INFO 2017-06-29 08:37:43,161 main.py:57] epoch 9741, training loss: 6934.56, average training loss: 7322.94, base loss: 16103.01
[INFO 2017-06-29 08:37:46,225 main.py:57] epoch 9742, training loss: 6891.27, average training loss: 7322.61, base loss: 16102.83
[INFO 2017-06-29 08:37:49,308 main.py:57] epoch 9743, training loss: 8083.98, average training loss: 7323.23, base loss: 16102.98
[INFO 2017-06-29 08:37:52,380 main.py:57] epoch 9744, training loss: 6901.25, average training loss: 7322.22, base loss: 16102.65
[INFO 2017-06-29 08:37:55,450 main.py:57] epoch 9745, training loss: 8409.10, average training loss: 7323.36, base loss: 16102.99
[INFO 2017-06-29 08:37:58,496 main.py:57] epoch 9746, training loss: 8270.78, average training loss: 7324.42, base loss: 16103.35
[INFO 2017-06-29 08:38:01,540 main.py:57] epoch 9747, training loss: 6682.66, average training loss: 7322.08, base loss: 16103.11
[INFO 2017-06-29 08:38:04,564 main.py:57] epoch 9748, training loss: 7949.96, average training loss: 7323.09, base loss: 16102.97
[INFO 2017-06-29 08:38:07,646 main.py:57] epoch 9749, training loss: 6834.88, average training loss: 7322.54, base loss: 16102.79
[INFO 2017-06-29 08:38:10,703 main.py:57] epoch 9750, training loss: 6807.17, average training loss: 7321.92, base loss: 16102.50
[INFO 2017-06-29 08:38:13,704 main.py:57] epoch 9751, training loss: 7496.81, average training loss: 7322.22, base loss: 16102.40
[INFO 2017-06-29 08:38:16,725 main.py:57] epoch 9752, training loss: 8020.26, average training loss: 7322.49, base loss: 16102.70
[INFO 2017-06-29 08:38:19,761 main.py:57] epoch 9753, training loss: 7996.64, average training loss: 7323.48, base loss: 16103.15
[INFO 2017-06-29 08:38:22,771 main.py:57] epoch 9754, training loss: 5980.75, average training loss: 7322.50, base loss: 16102.78
[INFO 2017-06-29 08:38:25,884 main.py:57] epoch 9755, training loss: 6934.75, average training loss: 7322.13, base loss: 16103.05
[INFO 2017-06-29 08:38:28,995 main.py:57] epoch 9756, training loss: 6967.99, average training loss: 7322.26, base loss: 16102.86
[INFO 2017-06-29 08:38:32,059 main.py:57] epoch 9757, training loss: 6926.35, average training loss: 7321.78, base loss: 16102.78
[INFO 2017-06-29 08:38:35,125 main.py:57] epoch 9758, training loss: 7537.46, average training loss: 7320.65, base loss: 16103.25
[INFO 2017-06-29 08:38:38,128 main.py:57] epoch 9759, training loss: 7867.07, average training loss: 7320.95, base loss: 16103.61
[INFO 2017-06-29 08:38:41,195 main.py:57] epoch 9760, training loss: 6190.64, average training loss: 7319.73, base loss: 16103.10
[INFO 2017-06-29 08:38:44,241 main.py:57] epoch 9761, training loss: 6646.11, average training loss: 7318.97, base loss: 16102.58
[INFO 2017-06-29 08:38:47,283 main.py:57] epoch 9762, training loss: 7258.25, average training loss: 7319.53, base loss: 16102.33
[INFO 2017-06-29 08:38:50,295 main.py:57] epoch 9763, training loss: 8085.53, average training loss: 7319.90, base loss: 16102.31
[INFO 2017-06-29 08:38:53,379 main.py:57] epoch 9764, training loss: 7782.75, average training loss: 7320.43, base loss: 16102.81
[INFO 2017-06-29 08:38:56,371 main.py:57] epoch 9765, training loss: 8043.79, average training loss: 7321.82, base loss: 16103.47
[INFO 2017-06-29 08:38:59,408 main.py:57] epoch 9766, training loss: 6896.57, average training loss: 7321.11, base loss: 16103.37
[INFO 2017-06-29 08:39:02,489 main.py:57] epoch 9767, training loss: 6023.89, average training loss: 7320.25, base loss: 16103.02
[INFO 2017-06-29 08:39:05,603 main.py:57] epoch 9768, training loss: 6755.15, average training loss: 7319.16, base loss: 16102.71
[INFO 2017-06-29 08:39:08,638 main.py:57] epoch 9769, training loss: 7395.68, average training loss: 7320.27, base loss: 16102.52
[INFO 2017-06-29 08:39:11,698 main.py:57] epoch 9770, training loss: 6493.32, average training loss: 7319.72, base loss: 16102.16
[INFO 2017-06-29 08:39:14,739 main.py:57] epoch 9771, training loss: 6900.04, average training loss: 7319.51, base loss: 16102.35
[INFO 2017-06-29 08:39:17,819 main.py:57] epoch 9772, training loss: 7942.64, average training loss: 7320.51, base loss: 16102.33
[INFO 2017-06-29 08:39:20,811 main.py:57] epoch 9773, training loss: 6446.86, average training loss: 7320.65, base loss: 16101.67
[INFO 2017-06-29 08:39:23,912 main.py:57] epoch 9774, training loss: 7835.37, average training loss: 7321.05, base loss: 16101.82
[INFO 2017-06-29 08:39:26,915 main.py:57] epoch 9775, training loss: 6374.74, average training loss: 7321.05, base loss: 16101.43
[INFO 2017-06-29 08:39:29,915 main.py:57] epoch 9776, training loss: 7563.49, average training loss: 7321.75, base loss: 16101.36
[INFO 2017-06-29 08:39:32,944 main.py:57] epoch 9777, training loss: 6748.72, average training loss: 7321.48, base loss: 16101.36
[INFO 2017-06-29 08:39:36,027 main.py:57] epoch 9778, training loss: 6697.98, average training loss: 7320.74, base loss: 16101.01
[INFO 2017-06-29 08:39:39,049 main.py:57] epoch 9779, training loss: 7586.41, average training loss: 7321.56, base loss: 16101.11
[INFO 2017-06-29 08:39:42,075 main.py:57] epoch 9780, training loss: 7140.55, average training loss: 7320.15, base loss: 16100.83
[INFO 2017-06-29 08:39:45,162 main.py:57] epoch 9781, training loss: 7294.87, average training loss: 7320.15, base loss: 16100.73
[INFO 2017-06-29 08:39:48,309 main.py:57] epoch 9782, training loss: 7826.80, average training loss: 7320.18, base loss: 16100.95
[INFO 2017-06-29 08:39:51,375 main.py:57] epoch 9783, training loss: 7452.58, average training loss: 7320.51, base loss: 16100.97
[INFO 2017-06-29 08:39:54,423 main.py:57] epoch 9784, training loss: 6936.59, average training loss: 7320.48, base loss: 16100.96
[INFO 2017-06-29 08:39:57,495 main.py:57] epoch 9785, training loss: 6851.59, average training loss: 7320.09, base loss: 16100.68
[INFO 2017-06-29 08:40:00,548 main.py:57] epoch 9786, training loss: 7410.36, average training loss: 7320.88, base loss: 16100.78
[INFO 2017-06-29 08:40:03,565 main.py:57] epoch 9787, training loss: 7767.64, average training loss: 7320.76, base loss: 16101.14
[INFO 2017-06-29 08:40:06,677 main.py:57] epoch 9788, training loss: 7412.53, average training loss: 7321.25, base loss: 16101.30
[INFO 2017-06-29 08:40:09,676 main.py:57] epoch 9789, training loss: 7411.65, average training loss: 7321.15, base loss: 16101.28
[INFO 2017-06-29 08:40:12,680 main.py:57] epoch 9790, training loss: 6382.07, average training loss: 7320.62, base loss: 16101.00
[INFO 2017-06-29 08:40:15,687 main.py:57] epoch 9791, training loss: 6759.47, average training loss: 7319.53, base loss: 16100.71
[INFO 2017-06-29 08:40:18,730 main.py:57] epoch 9792, training loss: 6809.35, average training loss: 7318.75, base loss: 16100.49
[INFO 2017-06-29 08:40:21,773 main.py:57] epoch 9793, training loss: 6752.54, average training loss: 7318.29, base loss: 16100.65
[INFO 2017-06-29 08:40:24,820 main.py:57] epoch 9794, training loss: 7841.36, average training loss: 7318.65, base loss: 16101.14
[INFO 2017-06-29 08:40:27,856 main.py:57] epoch 9795, training loss: 7585.44, average training loss: 7319.03, base loss: 16101.37
[INFO 2017-06-29 08:40:31,008 main.py:57] epoch 9796, training loss: 7395.82, average training loss: 7319.31, base loss: 16101.66
[INFO 2017-06-29 08:40:34,054 main.py:57] epoch 9797, training loss: 6976.05, average training loss: 7318.62, base loss: 16101.88
[INFO 2017-06-29 08:40:37,086 main.py:57] epoch 9798, training loss: 7750.16, average training loss: 7318.91, base loss: 16102.34
[INFO 2017-06-29 08:40:40,135 main.py:57] epoch 9799, training loss: 6739.84, average training loss: 7317.87, base loss: 16102.30
[INFO 2017-06-29 08:40:40,135 main.py:59] epoch 9799, testing
[INFO 2017-06-29 08:40:52,826 main.py:104] average testing loss: 7926.01, base loss: 16629.10
[INFO 2017-06-29 08:40:52,827 main.py:105] improve_loss: 8703.09, improve_percent: 0.52
[INFO 2017-06-29 08:40:52,828 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:40:55,898 main.py:57] epoch 9800, training loss: 7903.00, average training loss: 7319.65, base loss: 16102.48
[INFO 2017-06-29 08:40:58,964 main.py:57] epoch 9801, training loss: 8018.15, average training loss: 7319.60, base loss: 16102.44
[INFO 2017-06-29 08:41:02,013 main.py:57] epoch 9802, training loss: 6674.06, average training loss: 7319.44, base loss: 16102.27
[INFO 2017-06-29 08:41:05,057 main.py:57] epoch 9803, training loss: 7506.65, average training loss: 7320.45, base loss: 16102.50
[INFO 2017-06-29 08:41:08,121 main.py:57] epoch 9804, training loss: 7876.65, average training loss: 7320.96, base loss: 16103.07
[INFO 2017-06-29 08:41:11,182 main.py:57] epoch 9805, training loss: 7616.66, average training loss: 7321.49, base loss: 16103.46
[INFO 2017-06-29 08:41:14,243 main.py:57] epoch 9806, training loss: 6932.02, average training loss: 7319.98, base loss: 16103.03
[INFO 2017-06-29 08:41:17,295 main.py:57] epoch 9807, training loss: 9023.87, average training loss: 7322.56, base loss: 16103.59
[INFO 2017-06-29 08:41:20,312 main.py:57] epoch 9808, training loss: 7904.10, average training loss: 7321.69, base loss: 16104.10
[INFO 2017-06-29 08:41:23,333 main.py:57] epoch 9809, training loss: 6765.03, average training loss: 7320.72, base loss: 16104.07
[INFO 2017-06-29 08:41:26,420 main.py:57] epoch 9810, training loss: 7970.29, average training loss: 7321.19, base loss: 16104.39
[INFO 2017-06-29 08:41:29,469 main.py:57] epoch 9811, training loss: 6361.40, average training loss: 7318.22, base loss: 16103.98
[INFO 2017-06-29 08:41:32,440 main.py:57] epoch 9812, training loss: 7036.59, average training loss: 7317.71, base loss: 16103.95
[INFO 2017-06-29 08:41:35,488 main.py:57] epoch 9813, training loss: 7237.88, average training loss: 7318.40, base loss: 16104.13
[INFO 2017-06-29 08:41:38,558 main.py:57] epoch 9814, training loss: 6123.44, average training loss: 7316.83, base loss: 16103.74
[INFO 2017-06-29 08:41:41,576 main.py:57] epoch 9815, training loss: 6856.04, average training loss: 7316.16, base loss: 16103.60
[INFO 2017-06-29 08:41:44,600 main.py:57] epoch 9816, training loss: 6871.54, average training loss: 7315.78, base loss: 16103.15
[INFO 2017-06-29 08:41:47,631 main.py:57] epoch 9817, training loss: 7810.58, average training loss: 7316.32, base loss: 16103.15
[INFO 2017-06-29 08:41:50,723 main.py:57] epoch 9818, training loss: 8100.49, average training loss: 7315.85, base loss: 16103.52
[INFO 2017-06-29 08:41:53,720 main.py:57] epoch 9819, training loss: 6618.53, average training loss: 7314.20, base loss: 16103.10
[INFO 2017-06-29 08:41:56,796 main.py:57] epoch 9820, training loss: 6784.12, average training loss: 7313.22, base loss: 16103.19
[INFO 2017-06-29 08:41:59,886 main.py:57] epoch 9821, training loss: 7148.84, average training loss: 7313.74, base loss: 16103.23
[INFO 2017-06-29 08:42:02,979 main.py:57] epoch 9822, training loss: 7980.68, average training loss: 7314.43, base loss: 16103.39
[INFO 2017-06-29 08:42:06,025 main.py:57] epoch 9823, training loss: 7384.11, average training loss: 7314.74, base loss: 16103.29
[INFO 2017-06-29 08:42:09,046 main.py:57] epoch 9824, training loss: 6443.94, average training loss: 7313.65, base loss: 16103.23
[INFO 2017-06-29 08:42:12,120 main.py:57] epoch 9825, training loss: 7150.28, average training loss: 7313.93, base loss: 16103.30
[INFO 2017-06-29 08:42:15,144 main.py:57] epoch 9826, training loss: 7573.58, average training loss: 7313.60, base loss: 16103.35
[INFO 2017-06-29 08:42:18,205 main.py:57] epoch 9827, training loss: 8072.83, average training loss: 7313.99, base loss: 16103.63
[INFO 2017-06-29 08:42:21,209 main.py:57] epoch 9828, training loss: 6100.17, average training loss: 7313.32, base loss: 16103.28
[INFO 2017-06-29 08:42:24,248 main.py:57] epoch 9829, training loss: 6825.63, average training loss: 7311.83, base loss: 16103.07
[INFO 2017-06-29 08:42:27,275 main.py:57] epoch 9830, training loss: 7285.24, average training loss: 7310.31, base loss: 16103.23
[INFO 2017-06-29 08:42:30,353 main.py:57] epoch 9831, training loss: 7415.31, average training loss: 7310.02, base loss: 16103.08
[INFO 2017-06-29 08:42:33,374 main.py:57] epoch 9832, training loss: 6871.45, average training loss: 7309.16, base loss: 16102.99
[INFO 2017-06-29 08:42:36,409 main.py:57] epoch 9833, training loss: 7751.47, average training loss: 7309.06, base loss: 16103.12
[INFO 2017-06-29 08:42:39,456 main.py:57] epoch 9834, training loss: 8251.21, average training loss: 7309.84, base loss: 16103.06
[INFO 2017-06-29 08:42:42,504 main.py:57] epoch 9835, training loss: 7050.64, average training loss: 7309.40, base loss: 16102.78
[INFO 2017-06-29 08:42:45,541 main.py:57] epoch 9836, training loss: 6930.56, average training loss: 7308.57, base loss: 16102.75
[INFO 2017-06-29 08:42:48,581 main.py:57] epoch 9837, training loss: 7306.54, average training loss: 7306.82, base loss: 16103.04
[INFO 2017-06-29 08:42:51,634 main.py:57] epoch 9838, training loss: 8545.26, average training loss: 7307.87, base loss: 16103.66
[INFO 2017-06-29 08:42:54,665 main.py:57] epoch 9839, training loss: 8126.93, average training loss: 7308.77, base loss: 16104.31
[INFO 2017-06-29 08:42:57,714 main.py:57] epoch 9840, training loss: 6873.41, average training loss: 7307.94, base loss: 16104.24
[INFO 2017-06-29 08:43:00,735 main.py:57] epoch 9841, training loss: 7255.52, average training loss: 7307.80, base loss: 16104.21
[INFO 2017-06-29 08:43:03,819 main.py:57] epoch 9842, training loss: 6453.89, average training loss: 7306.42, base loss: 16103.78
[INFO 2017-06-29 08:43:06,869 main.py:57] epoch 9843, training loss: 6390.45, average training loss: 7305.22, base loss: 16103.53
[INFO 2017-06-29 08:43:09,954 main.py:57] epoch 9844, training loss: 6852.87, average training loss: 7304.37, base loss: 16102.85
[INFO 2017-06-29 08:43:13,048 main.py:57] epoch 9845, training loss: 6701.68, average training loss: 7304.71, base loss: 16102.18
[INFO 2017-06-29 08:43:16,096 main.py:57] epoch 9846, training loss: 7315.00, average training loss: 7304.80, base loss: 16102.20
[INFO 2017-06-29 08:43:19,155 main.py:57] epoch 9847, training loss: 8675.28, average training loss: 7305.97, base loss: 16102.77
[INFO 2017-06-29 08:43:22,222 main.py:57] epoch 9848, training loss: 7721.68, average training loss: 7306.21, base loss: 16103.00
[INFO 2017-06-29 08:43:25,248 main.py:57] epoch 9849, training loss: 6941.27, average training loss: 7305.42, base loss: 16102.75
[INFO 2017-06-29 08:43:28,289 main.py:57] epoch 9850, training loss: 7368.34, average training loss: 7305.52, base loss: 16102.57
[INFO 2017-06-29 08:43:31,440 main.py:57] epoch 9851, training loss: 6311.42, average training loss: 7304.66, base loss: 16101.64
[INFO 2017-06-29 08:43:34,489 main.py:57] epoch 9852, training loss: 7027.48, average training loss: 7304.67, base loss: 16101.59
[INFO 2017-06-29 08:43:37,546 main.py:57] epoch 9853, training loss: 7864.44, average training loss: 7304.52, base loss: 16101.83
[INFO 2017-06-29 08:43:40,623 main.py:57] epoch 9854, training loss: 8413.09, average training loss: 7305.56, base loss: 16102.10
[INFO 2017-06-29 08:43:43,660 main.py:57] epoch 9855, training loss: 6522.54, average training loss: 7303.51, base loss: 16101.64
[INFO 2017-06-29 08:43:46,733 main.py:57] epoch 9856, training loss: 7919.19, average training loss: 7302.88, base loss: 16101.50
[INFO 2017-06-29 08:43:49,814 main.py:57] epoch 9857, training loss: 7646.94, average training loss: 7302.82, base loss: 16101.54
[INFO 2017-06-29 08:43:52,860 main.py:57] epoch 9858, training loss: 6900.40, average training loss: 7302.65, base loss: 16101.23
[INFO 2017-06-29 08:43:55,888 main.py:57] epoch 9859, training loss: 7341.20, average training loss: 7302.07, base loss: 16101.14
[INFO 2017-06-29 08:43:58,963 main.py:57] epoch 9860, training loss: 6931.24, average training loss: 7301.89, base loss: 16100.57
[INFO 2017-06-29 08:44:01,948 main.py:57] epoch 9861, training loss: 8888.12, average training loss: 7302.84, base loss: 16101.36
[INFO 2017-06-29 08:44:04,980 main.py:57] epoch 9862, training loss: 7023.43, average training loss: 7302.52, base loss: 16101.15
[INFO 2017-06-29 08:44:07,989 main.py:57] epoch 9863, training loss: 8667.21, average training loss: 7302.44, base loss: 16101.51
[INFO 2017-06-29 08:44:11,035 main.py:57] epoch 9864, training loss: 7073.77, average training loss: 7302.36, base loss: 16101.22
[INFO 2017-06-29 08:44:14,074 main.py:57] epoch 9865, training loss: 7258.40, average training loss: 7301.92, base loss: 16101.21
[INFO 2017-06-29 08:44:17,136 main.py:57] epoch 9866, training loss: 7414.89, average training loss: 7301.33, base loss: 16101.16
[INFO 2017-06-29 08:44:20,233 main.py:57] epoch 9867, training loss: 8167.54, average training loss: 7301.59, base loss: 16101.33
[INFO 2017-06-29 08:44:23,431 main.py:57] epoch 9868, training loss: 8272.23, average training loss: 7302.12, base loss: 16101.89
[INFO 2017-06-29 08:44:26,434 main.py:57] epoch 9869, training loss: 7460.43, average training loss: 7302.58, base loss: 16102.20
[INFO 2017-06-29 08:44:29,498 main.py:57] epoch 9870, training loss: 8873.66, average training loss: 7304.43, base loss: 16102.99
[INFO 2017-06-29 08:44:32,577 main.py:57] epoch 9871, training loss: 6833.01, average training loss: 7304.68, base loss: 16102.97
[INFO 2017-06-29 08:44:35,590 main.py:57] epoch 9872, training loss: 6311.70, average training loss: 7303.23, base loss: 16102.60
[INFO 2017-06-29 08:44:38,612 main.py:57] epoch 9873, training loss: 7070.87, average training loss: 7302.83, base loss: 16102.88
[INFO 2017-06-29 08:44:41,666 main.py:57] epoch 9874, training loss: 6958.01, average training loss: 7302.33, base loss: 16102.40
[INFO 2017-06-29 08:44:44,690 main.py:57] epoch 9875, training loss: 6280.85, average training loss: 7301.36, base loss: 16101.65
[INFO 2017-06-29 08:44:47,800 main.py:57] epoch 9876, training loss: 7192.72, average training loss: 7301.20, base loss: 16101.73
[INFO 2017-06-29 08:44:50,772 main.py:57] epoch 9877, training loss: 7736.02, average training loss: 7302.29, base loss: 16101.99
[INFO 2017-06-29 08:44:53,875 main.py:57] epoch 9878, training loss: 8698.99, average training loss: 7303.36, base loss: 16102.20
[INFO 2017-06-29 08:44:56,895 main.py:57] epoch 9879, training loss: 6898.97, average training loss: 7303.31, base loss: 16101.81
[INFO 2017-06-29 08:44:59,991 main.py:57] epoch 9880, training loss: 7510.96, average training loss: 7303.83, base loss: 16102.23
[INFO 2017-06-29 08:45:03,082 main.py:57] epoch 9881, training loss: 7808.17, average training loss: 7304.03, base loss: 16102.73
[INFO 2017-06-29 08:45:06,155 main.py:57] epoch 9882, training loss: 7653.58, average training loss: 7304.27, base loss: 16102.81
[INFO 2017-06-29 08:45:09,208 main.py:57] epoch 9883, training loss: 7161.20, average training loss: 7303.96, base loss: 16102.77
[INFO 2017-06-29 08:45:12,251 main.py:57] epoch 9884, training loss: 6904.59, average training loss: 7304.86, base loss: 16102.70
[INFO 2017-06-29 08:45:15,294 main.py:57] epoch 9885, training loss: 7189.56, average training loss: 7305.21, base loss: 16102.70
[INFO 2017-06-29 08:45:18,337 main.py:57] epoch 9886, training loss: 6798.82, average training loss: 7304.96, base loss: 16102.46
[INFO 2017-06-29 08:45:21,475 main.py:57] epoch 9887, training loss: 7168.77, average training loss: 7305.37, base loss: 16102.24
[INFO 2017-06-29 08:45:24,559 main.py:57] epoch 9888, training loss: 8301.48, average training loss: 7306.89, base loss: 16102.63
[INFO 2017-06-29 08:45:27,695 main.py:57] epoch 9889, training loss: 7376.66, average training loss: 7305.95, base loss: 16102.76
[INFO 2017-06-29 08:45:30,779 main.py:57] epoch 9890, training loss: 8124.27, average training loss: 7306.52, base loss: 16102.95
[INFO 2017-06-29 08:45:33,817 main.py:57] epoch 9891, training loss: 6743.88, average training loss: 7305.71, base loss: 16102.73
[INFO 2017-06-29 08:45:36,907 main.py:57] epoch 9892, training loss: 6286.96, average training loss: 7304.65, base loss: 16102.39
[INFO 2017-06-29 08:45:39,902 main.py:57] epoch 9893, training loss: 6705.51, average training loss: 7304.29, base loss: 16101.98
[INFO 2017-06-29 08:45:42,941 main.py:57] epoch 9894, training loss: 7233.11, average training loss: 7304.19, base loss: 16101.97
[INFO 2017-06-29 08:45:46,018 main.py:57] epoch 9895, training loss: 7506.14, average training loss: 7303.20, base loss: 16101.84
[INFO 2017-06-29 08:45:49,113 main.py:57] epoch 9896, training loss: 7167.71, average training loss: 7303.34, base loss: 16101.98
[INFO 2017-06-29 08:45:52,101 main.py:57] epoch 9897, training loss: 8850.09, average training loss: 7305.10, base loss: 16102.47
[INFO 2017-06-29 08:45:55,173 main.py:57] epoch 9898, training loss: 7145.07, average training loss: 7302.93, base loss: 16102.28
[INFO 2017-06-29 08:45:58,192 main.py:57] epoch 9899, training loss: 7778.43, average training loss: 7303.69, base loss: 16102.57
[INFO 2017-06-29 08:45:58,193 main.py:59] epoch 9899, testing
[INFO 2017-06-29 08:46:10,749 main.py:104] average testing loss: 7731.00, base loss: 15970.38
[INFO 2017-06-29 08:46:10,749 main.py:105] improve_loss: 8239.37, improve_percent: 0.52
[INFO 2017-06-29 08:46:10,750 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:46:13,838 main.py:57] epoch 9900, training loss: 8048.31, average training loss: 7304.87, base loss: 16102.84
[INFO 2017-06-29 08:46:16,879 main.py:57] epoch 9901, training loss: 6941.60, average training loss: 7304.34, base loss: 16102.75
[INFO 2017-06-29 08:46:20,019 main.py:57] epoch 9902, training loss: 7362.70, average training loss: 7304.44, base loss: 16103.22
[INFO 2017-06-29 08:46:23,051 main.py:57] epoch 9903, training loss: 7450.18, average training loss: 7304.03, base loss: 16103.25
[INFO 2017-06-29 08:46:26,063 main.py:57] epoch 9904, training loss: 7073.98, average training loss: 7303.45, base loss: 16102.97
[INFO 2017-06-29 08:46:29,099 main.py:57] epoch 9905, training loss: 6594.86, average training loss: 7301.81, base loss: 16102.69
[INFO 2017-06-29 08:46:32,165 main.py:57] epoch 9906, training loss: 8274.77, average training loss: 7302.92, base loss: 16103.42
[INFO 2017-06-29 08:46:35,271 main.py:57] epoch 9907, training loss: 6655.60, average training loss: 7302.99, base loss: 16103.22
[INFO 2017-06-29 08:46:38,377 main.py:57] epoch 9908, training loss: 6745.75, average training loss: 7302.74, base loss: 16103.32
[INFO 2017-06-29 08:46:41,486 main.py:57] epoch 9909, training loss: 7304.80, average training loss: 7302.54, base loss: 16103.25
[INFO 2017-06-29 08:46:44,526 main.py:57] epoch 9910, training loss: 6816.19, average training loss: 7301.19, base loss: 16102.94
[INFO 2017-06-29 08:46:47,501 main.py:57] epoch 9911, training loss: 7097.11, average training loss: 7300.62, base loss: 16102.51
[INFO 2017-06-29 08:46:50,570 main.py:57] epoch 9912, training loss: 9187.52, average training loss: 7303.32, base loss: 16103.39
[INFO 2017-06-29 08:46:53,621 main.py:57] epoch 9913, training loss: 7035.00, average training loss: 7302.62, base loss: 16103.55
[INFO 2017-06-29 08:46:56,708 main.py:57] epoch 9914, training loss: 7185.09, average training loss: 7300.99, base loss: 16103.60
[INFO 2017-06-29 08:46:59,791 main.py:57] epoch 9915, training loss: 7462.72, average training loss: 7300.57, base loss: 16103.52
[INFO 2017-06-29 08:47:02,926 main.py:57] epoch 9916, training loss: 7339.38, average training loss: 7301.45, base loss: 16103.66
[INFO 2017-06-29 08:47:05,951 main.py:57] epoch 9917, training loss: 6678.05, average training loss: 7301.14, base loss: 16103.66
[INFO 2017-06-29 08:47:08,923 main.py:57] epoch 9918, training loss: 7277.37, average training loss: 7301.55, base loss: 16103.56
[INFO 2017-06-29 08:47:11,961 main.py:57] epoch 9919, training loss: 8100.19, average training loss: 7302.35, base loss: 16103.79
[INFO 2017-06-29 08:47:15,031 main.py:57] epoch 9920, training loss: 7561.75, average training loss: 7302.96, base loss: 16103.67
[INFO 2017-06-29 08:47:18,123 main.py:57] epoch 9921, training loss: 7789.65, average training loss: 7303.55, base loss: 16103.45
[INFO 2017-06-29 08:47:21,183 main.py:57] epoch 9922, training loss: 6667.20, average training loss: 7303.47, base loss: 16103.04
[INFO 2017-06-29 08:47:24,226 main.py:57] epoch 9923, training loss: 7255.33, average training loss: 7303.22, base loss: 16103.11
[INFO 2017-06-29 08:47:27,371 main.py:57] epoch 9924, training loss: 7230.00, average training loss: 7303.99, base loss: 16102.72
[INFO 2017-06-29 08:47:30,417 main.py:57] epoch 9925, training loss: 7337.18, average training loss: 7302.92, base loss: 16102.57
[INFO 2017-06-29 08:47:33,522 main.py:57] epoch 9926, training loss: 7146.00, average training loss: 7302.80, base loss: 16102.36
[INFO 2017-06-29 08:47:36,662 main.py:57] epoch 9927, training loss: 7185.27, average training loss: 7302.05, base loss: 16102.35
[INFO 2017-06-29 08:47:39,729 main.py:57] epoch 9928, training loss: 7202.49, average training loss: 7302.92, base loss: 16102.60
[INFO 2017-06-29 08:47:42,781 main.py:57] epoch 9929, training loss: 7049.60, average training loss: 7302.65, base loss: 16102.51
[INFO 2017-06-29 08:47:45,776 main.py:57] epoch 9930, training loss: 6939.15, average training loss: 7303.19, base loss: 16102.07
[INFO 2017-06-29 08:47:48,805 main.py:57] epoch 9931, training loss: 6305.73, average training loss: 7302.23, base loss: 16101.50
[INFO 2017-06-29 08:47:51,923 main.py:57] epoch 9932, training loss: 7185.72, average training loss: 7301.70, base loss: 16101.56
[INFO 2017-06-29 08:47:54,995 main.py:57] epoch 9933, training loss: 7122.32, average training loss: 7302.41, base loss: 16101.63
[INFO 2017-06-29 08:47:57,969 main.py:57] epoch 9934, training loss: 6965.93, average training loss: 7302.58, base loss: 16101.47
[INFO 2017-06-29 08:48:01,002 main.py:57] epoch 9935, training loss: 7172.65, average training loss: 7303.10, base loss: 16101.66
[INFO 2017-06-29 08:48:04,052 main.py:57] epoch 9936, training loss: 6807.28, average training loss: 7302.60, base loss: 16101.11
[INFO 2017-06-29 08:48:07,057 main.py:57] epoch 9937, training loss: 7289.33, average training loss: 7303.08, base loss: 16100.63
[INFO 2017-06-29 08:48:10,098 main.py:57] epoch 9938, training loss: 8243.29, average training loss: 7303.20, base loss: 16101.08
[INFO 2017-06-29 08:48:13,193 main.py:57] epoch 9939, training loss: 8642.74, average training loss: 7304.97, base loss: 16101.88
[INFO 2017-06-29 08:48:16,171 main.py:57] epoch 9940, training loss: 8092.91, average training loss: 7305.84, base loss: 16102.32
[INFO 2017-06-29 08:48:19,171 main.py:57] epoch 9941, training loss: 7295.89, average training loss: 7306.04, base loss: 16102.52
[INFO 2017-06-29 08:48:22,236 main.py:57] epoch 9942, training loss: 8007.36, average training loss: 7306.81, base loss: 16102.82
[INFO 2017-06-29 08:48:25,264 main.py:57] epoch 9943, training loss: 8680.22, average training loss: 7308.66, base loss: 16103.22
[INFO 2017-06-29 08:48:28,292 main.py:57] epoch 9944, training loss: 7222.86, average training loss: 7307.68, base loss: 16103.26
[INFO 2017-06-29 08:48:31,325 main.py:57] epoch 9945, training loss: 7256.29, average training loss: 7307.67, base loss: 16103.52
[INFO 2017-06-29 08:48:34,343 main.py:57] epoch 9946, training loss: 7703.82, average training loss: 7308.93, base loss: 16103.27
[INFO 2017-06-29 08:48:37,411 main.py:57] epoch 9947, training loss: 7695.41, average training loss: 7309.10, base loss: 16103.23
[INFO 2017-06-29 08:48:40,444 main.py:57] epoch 9948, training loss: 7472.23, average training loss: 7308.85, base loss: 16103.73
[INFO 2017-06-29 08:48:43,536 main.py:57] epoch 9949, training loss: 6740.92, average training loss: 7307.88, base loss: 16103.76
[INFO 2017-06-29 08:48:46,555 main.py:57] epoch 9950, training loss: 7591.80, average training loss: 7308.17, base loss: 16103.95
[INFO 2017-06-29 08:48:49,593 main.py:57] epoch 9951, training loss: 6915.64, average training loss: 7307.27, base loss: 16103.75
[INFO 2017-06-29 08:48:52,697 main.py:57] epoch 9952, training loss: 8640.19, average training loss: 7309.69, base loss: 16104.05
[INFO 2017-06-29 08:48:55,742 main.py:57] epoch 9953, training loss: 6625.29, average training loss: 7309.03, base loss: 16103.66
[INFO 2017-06-29 08:48:58,756 main.py:57] epoch 9954, training loss: 8287.67, average training loss: 7308.84, base loss: 16104.16
[INFO 2017-06-29 08:49:01,803 main.py:57] epoch 9955, training loss: 7194.52, average training loss: 7308.28, base loss: 16104.34
[INFO 2017-06-29 08:49:04,841 main.py:57] epoch 9956, training loss: 7077.06, average training loss: 7308.72, base loss: 16103.80
[INFO 2017-06-29 08:49:07,935 main.py:57] epoch 9957, training loss: 6955.71, average training loss: 7308.20, base loss: 16103.46
[INFO 2017-06-29 08:49:10,963 main.py:57] epoch 9958, training loss: 7938.50, average training loss: 7309.13, base loss: 16104.05
[INFO 2017-06-29 08:49:13,979 main.py:57] epoch 9959, training loss: 7326.97, average training loss: 7309.49, base loss: 16104.31
[INFO 2017-06-29 08:49:17,037 main.py:57] epoch 9960, training loss: 9030.49, average training loss: 7311.42, base loss: 16105.09
[INFO 2017-06-29 08:49:20,049 main.py:57] epoch 9961, training loss: 8063.33, average training loss: 7311.80, base loss: 16105.23
[INFO 2017-06-29 08:49:23,088 main.py:57] epoch 9962, training loss: 7437.98, average training loss: 7312.09, base loss: 16105.50
[INFO 2017-06-29 08:49:26,119 main.py:57] epoch 9963, training loss: 7863.94, average training loss: 7311.90, base loss: 16105.75
[INFO 2017-06-29 08:49:29,184 main.py:57] epoch 9964, training loss: 7312.08, average training loss: 7312.09, base loss: 16105.71
[INFO 2017-06-29 08:49:32,279 main.py:57] epoch 9965, training loss: 6951.98, average training loss: 7311.83, base loss: 16105.35
[INFO 2017-06-29 08:49:35,327 main.py:57] epoch 9966, training loss: 8008.51, average training loss: 7312.17, base loss: 16105.81
[INFO 2017-06-29 08:49:38,334 main.py:57] epoch 9967, training loss: 7139.52, average training loss: 7311.33, base loss: 16105.46
[INFO 2017-06-29 08:49:41,396 main.py:57] epoch 9968, training loss: 7999.01, average training loss: 7311.52, base loss: 16105.77
[INFO 2017-06-29 08:49:44,418 main.py:57] epoch 9969, training loss: 6275.73, average training loss: 7310.59, base loss: 16105.35
[INFO 2017-06-29 08:49:47,519 main.py:57] epoch 9970, training loss: 7714.35, average training loss: 7311.95, base loss: 16105.19
[INFO 2017-06-29 08:49:50,685 main.py:57] epoch 9971, training loss: 6980.64, average training loss: 7312.20, base loss: 16105.00
[INFO 2017-06-29 08:49:53,800 main.py:57] epoch 9972, training loss: 7185.98, average training loss: 7312.28, base loss: 16104.84
[INFO 2017-06-29 08:49:56,825 main.py:57] epoch 9973, training loss: 7388.27, average training loss: 7311.44, base loss: 16104.90
[INFO 2017-06-29 08:49:59,918 main.py:57] epoch 9974, training loss: 7454.40, average training loss: 7311.23, base loss: 16105.11
[INFO 2017-06-29 08:50:03,006 main.py:57] epoch 9975, training loss: 7659.38, average training loss: 7311.42, base loss: 16105.28
[INFO 2017-06-29 08:50:06,092 main.py:57] epoch 9976, training loss: 7887.65, average training loss: 7312.08, base loss: 16105.47
[INFO 2017-06-29 08:50:09,109 main.py:57] epoch 9977, training loss: 7484.31, average training loss: 7312.37, base loss: 16105.54
[INFO 2017-06-29 08:50:12,081 main.py:57] epoch 9978, training loss: 7148.29, average training loss: 7312.96, base loss: 16105.42
[INFO 2017-06-29 08:50:15,170 main.py:57] epoch 9979, training loss: 7270.59, average training loss: 7312.64, base loss: 16105.40
[INFO 2017-06-29 08:50:18,200 main.py:57] epoch 9980, training loss: 7180.62, average training loss: 7311.50, base loss: 16105.78
[INFO 2017-06-29 08:50:21,291 main.py:57] epoch 9981, training loss: 7406.14, average training loss: 7311.77, base loss: 16106.55
[INFO 2017-06-29 08:50:24,398 main.py:57] epoch 9982, training loss: 6371.42, average training loss: 7311.74, base loss: 16105.97
[INFO 2017-06-29 08:50:27,455 main.py:57] epoch 9983, training loss: 7082.37, average training loss: 7311.45, base loss: 16105.92
[INFO 2017-06-29 08:50:30,441 main.py:57] epoch 9984, training loss: 7273.12, average training loss: 7311.75, base loss: 16106.06
[INFO 2017-06-29 08:50:33,566 main.py:57] epoch 9985, training loss: 6486.69, average training loss: 7311.21, base loss: 16105.63
[INFO 2017-06-29 08:50:36,674 main.py:57] epoch 9986, training loss: 6802.79, average training loss: 7311.38, base loss: 16105.58
[INFO 2017-06-29 08:50:39,800 main.py:57] epoch 9987, training loss: 6441.05, average training loss: 7309.67, base loss: 16105.35
[INFO 2017-06-29 08:50:42,888 main.py:57] epoch 9988, training loss: 7011.05, average training loss: 7309.01, base loss: 16105.07
[INFO 2017-06-29 08:50:45,913 main.py:57] epoch 9989, training loss: 6665.36, average training loss: 7308.89, base loss: 16104.65
[INFO 2017-06-29 08:50:49,065 main.py:57] epoch 9990, training loss: 8272.99, average training loss: 7310.24, base loss: 16104.75
[INFO 2017-06-29 08:50:52,082 main.py:57] epoch 9991, training loss: 8163.70, average training loss: 7311.29, base loss: 16104.95
[INFO 2017-06-29 08:50:55,189 main.py:57] epoch 9992, training loss: 6737.22, average training loss: 7309.61, base loss: 16104.76
[INFO 2017-06-29 08:50:58,264 main.py:57] epoch 9993, training loss: 6820.08, average training loss: 7309.12, base loss: 16104.69
[INFO 2017-06-29 08:51:01,281 main.py:57] epoch 9994, training loss: 7764.35, average training loss: 7310.52, base loss: 16104.77
[INFO 2017-06-29 08:51:04,330 main.py:57] epoch 9995, training loss: 6657.24, average training loss: 7310.29, base loss: 16104.43
[INFO 2017-06-29 08:51:07,359 main.py:57] epoch 9996, training loss: 6984.55, average training loss: 7310.00, base loss: 16104.32
[INFO 2017-06-29 08:51:10,528 main.py:57] epoch 9997, training loss: 7021.74, average training loss: 7308.66, base loss: 16104.20
[INFO 2017-06-29 08:51:13,563 main.py:57] epoch 9998, training loss: 6945.52, average training loss: 7307.65, base loss: 16104.12
[INFO 2017-06-29 08:51:16,678 main.py:57] epoch 9999, training loss: 6541.75, average training loss: 7307.36, base loss: 16103.61
[INFO 2017-06-29 08:51:16,678 main.py:59] epoch 9999, testing
[INFO 2017-06-29 08:51:29,242 main.py:104] average testing loss: 7743.70, base loss: 16413.09
[INFO 2017-06-29 08:51:29,243 main.py:105] improve_loss: 8669.39, improve_percent: 0.53
[INFO 2017-06-29 08:51:29,244 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:51:32,350 main.py:57] epoch 10000, training loss: 6946.34, average training loss: 7307.60, base loss: 16103.53
[INFO 2017-06-29 08:51:35,379 main.py:57] epoch 10001, training loss: 7077.39, average training loss: 7306.83, base loss: 16103.46
[INFO 2017-06-29 08:51:38,485 main.py:57] epoch 10002, training loss: 7533.87, average training loss: 7307.39, base loss: 16103.31
[INFO 2017-06-29 08:51:41,520 main.py:57] epoch 10003, training loss: 6300.82, average training loss: 7306.45, base loss: 16102.62
[INFO 2017-06-29 08:51:44,549 main.py:57] epoch 10004, training loss: 7024.50, average training loss: 7307.05, base loss: 16102.62
[INFO 2017-06-29 08:51:47,590 main.py:57] epoch 10005, training loss: 6676.79, average training loss: 7306.91, base loss: 16102.33
[INFO 2017-06-29 08:51:50,700 main.py:57] epoch 10006, training loss: 8395.93, average training loss: 7307.74, base loss: 16102.89
[INFO 2017-06-29 08:51:53,730 main.py:57] epoch 10007, training loss: 7250.23, average training loss: 7306.77, base loss: 16103.00
[INFO 2017-06-29 08:51:56,844 main.py:57] epoch 10008, training loss: 6829.27, average training loss: 7305.47, base loss: 16102.78
[INFO 2017-06-29 08:51:59,948 main.py:57] epoch 10009, training loss: 8224.51, average training loss: 7306.85, base loss: 16103.43
[INFO 2017-06-29 08:52:02,990 main.py:57] epoch 10010, training loss: 6672.77, average training loss: 7306.53, base loss: 16103.26
[INFO 2017-06-29 08:52:06,024 main.py:57] epoch 10011, training loss: 6640.05, average training loss: 7305.98, base loss: 16103.27
[INFO 2017-06-29 08:52:09,057 main.py:57] epoch 10012, training loss: 6406.70, average training loss: 7304.79, base loss: 16102.99
[INFO 2017-06-29 08:52:12,131 main.py:57] epoch 10013, training loss: 7136.88, average training loss: 7304.66, base loss: 16102.92
[INFO 2017-06-29 08:52:15,118 main.py:57] epoch 10014, training loss: 6748.82, average training loss: 7304.16, base loss: 16102.86
[INFO 2017-06-29 08:52:18,284 main.py:57] epoch 10015, training loss: 7323.27, average training loss: 7303.97, base loss: 16102.82
[INFO 2017-06-29 08:52:21,311 main.py:57] epoch 10016, training loss: 7392.99, average training loss: 7303.26, base loss: 16103.17
[INFO 2017-06-29 08:52:24,364 main.py:57] epoch 10017, training loss: 6789.66, average training loss: 7302.91, base loss: 16103.15
[INFO 2017-06-29 08:52:27,386 main.py:57] epoch 10018, training loss: 7678.65, average training loss: 7303.77, base loss: 16103.56
[INFO 2017-06-29 08:52:30,483 main.py:57] epoch 10019, training loss: 6937.27, average training loss: 7304.22, base loss: 16103.36
[INFO 2017-06-29 08:52:33,559 main.py:57] epoch 10020, training loss: 6976.80, average training loss: 7303.68, base loss: 16103.16
[INFO 2017-06-29 08:52:36,681 main.py:57] epoch 10021, training loss: 6772.75, average training loss: 7303.54, base loss: 16102.90
[INFO 2017-06-29 08:52:39,772 main.py:57] epoch 10022, training loss: 7365.06, average training loss: 7304.01, base loss: 16103.20
[INFO 2017-06-29 08:52:42,830 main.py:57] epoch 10023, training loss: 7425.36, average training loss: 7304.44, base loss: 16103.38
[INFO 2017-06-29 08:52:45,814 main.py:57] epoch 10024, training loss: 7152.10, average training loss: 7303.98, base loss: 16103.25
[INFO 2017-06-29 08:52:48,916 main.py:57] epoch 10025, training loss: 6755.16, average training loss: 7304.44, base loss: 16102.97
[INFO 2017-06-29 08:52:51,934 main.py:57] epoch 10026, training loss: 6063.21, average training loss: 7302.90, base loss: 16102.35
[INFO 2017-06-29 08:52:54,998 main.py:57] epoch 10027, training loss: 7772.56, average training loss: 7302.17, base loss: 16102.32
[INFO 2017-06-29 08:52:58,044 main.py:57] epoch 10028, training loss: 7433.41, average training loss: 7303.02, base loss: 16102.60
[INFO 2017-06-29 08:53:01,168 main.py:57] epoch 10029, training loss: 7076.12, average training loss: 7302.84, base loss: 16102.57
[INFO 2017-06-29 08:53:04,141 main.py:57] epoch 10030, training loss: 6735.61, average training loss: 7301.00, base loss: 16102.36
[INFO 2017-06-29 08:53:07,270 main.py:57] epoch 10031, training loss: 7868.97, average training loss: 7301.15, base loss: 16102.38
[INFO 2017-06-29 08:53:10,369 main.py:57] epoch 10032, training loss: 7600.80, average training loss: 7301.85, base loss: 16102.30
[INFO 2017-06-29 08:53:13,385 main.py:57] epoch 10033, training loss: 6963.45, average training loss: 7301.78, base loss: 16102.13
[INFO 2017-06-29 08:53:16,470 main.py:57] epoch 10034, training loss: 7400.10, average training loss: 7300.36, base loss: 16102.47
[INFO 2017-06-29 08:53:19,534 main.py:57] epoch 10035, training loss: 7753.22, average training loss: 7300.66, base loss: 16102.77
[INFO 2017-06-29 08:53:22,597 main.py:57] epoch 10036, training loss: 7211.63, average training loss: 7300.15, base loss: 16102.79
[INFO 2017-06-29 08:53:25,602 main.py:57] epoch 10037, training loss: 7113.03, average training loss: 7299.97, base loss: 16102.76
[INFO 2017-06-29 08:53:28,591 main.py:57] epoch 10038, training loss: 7393.49, average training loss: 7300.54, base loss: 16102.95
[INFO 2017-06-29 08:53:31,690 main.py:57] epoch 10039, training loss: 6091.32, average training loss: 7300.06, base loss: 16102.82
[INFO 2017-06-29 08:53:34,699 main.py:57] epoch 10040, training loss: 6706.41, average training loss: 7299.38, base loss: 16102.80
[INFO 2017-06-29 08:53:37,762 main.py:57] epoch 10041, training loss: 7862.54, average training loss: 7299.89, base loss: 16102.94
[INFO 2017-06-29 08:53:40,776 main.py:57] epoch 10042, training loss: 7447.28, average training loss: 7300.50, base loss: 16103.16
[INFO 2017-06-29 08:53:43,820 main.py:57] epoch 10043, training loss: 8556.70, average training loss: 7302.18, base loss: 16103.78
[INFO 2017-06-29 08:53:46,845 main.py:57] epoch 10044, training loss: 6888.13, average training loss: 7301.17, base loss: 16103.82
[INFO 2017-06-29 08:53:49,864 main.py:57] epoch 10045, training loss: 7827.55, average training loss: 7301.46, base loss: 16104.24
[INFO 2017-06-29 08:53:52,982 main.py:57] epoch 10046, training loss: 7298.01, average training loss: 7300.92, base loss: 16104.06
[INFO 2017-06-29 08:53:56,030 main.py:57] epoch 10047, training loss: 7287.13, average training loss: 7301.58, base loss: 16104.16
[INFO 2017-06-29 08:53:59,127 main.py:57] epoch 10048, training loss: 7241.85, average training loss: 7301.06, base loss: 16104.15
[INFO 2017-06-29 08:54:02,148 main.py:57] epoch 10049, training loss: 6716.18, average training loss: 7300.77, base loss: 16103.65
[INFO 2017-06-29 08:54:05,177 main.py:57] epoch 10050, training loss: 6849.41, average training loss: 7300.92, base loss: 16103.56
[INFO 2017-06-29 08:54:08,197 main.py:57] epoch 10051, training loss: 6350.13, average training loss: 7300.15, base loss: 16103.24
[INFO 2017-06-29 08:54:11,264 main.py:57] epoch 10052, training loss: 7654.67, average training loss: 7301.47, base loss: 16103.43
[INFO 2017-06-29 08:54:14,299 main.py:57] epoch 10053, training loss: 7309.77, average training loss: 7301.42, base loss: 16103.54
[INFO 2017-06-29 08:54:17,338 main.py:57] epoch 10054, training loss: 7017.49, average training loss: 7301.14, base loss: 16103.38
[INFO 2017-06-29 08:54:20,423 main.py:57] epoch 10055, training loss: 8244.99, average training loss: 7302.54, base loss: 16103.78
[INFO 2017-06-29 08:54:23,471 main.py:57] epoch 10056, training loss: 7126.92, average training loss: 7301.76, base loss: 16103.79
[INFO 2017-06-29 08:54:26,527 main.py:57] epoch 10057, training loss: 7789.70, average training loss: 7302.43, base loss: 16104.01
[INFO 2017-06-29 08:54:29,575 main.py:57] epoch 10058, training loss: 7023.36, average training loss: 7300.10, base loss: 16103.85
[INFO 2017-06-29 08:54:32,605 main.py:57] epoch 10059, training loss: 7210.62, average training loss: 7300.09, base loss: 16103.97
[INFO 2017-06-29 08:54:35,623 main.py:57] epoch 10060, training loss: 7678.87, average training loss: 7300.48, base loss: 16104.01
[INFO 2017-06-29 08:54:38,644 main.py:57] epoch 10061, training loss: 7725.36, average training loss: 7301.73, base loss: 16104.19
[INFO 2017-06-29 08:54:41,781 main.py:57] epoch 10062, training loss: 8439.90, average training loss: 7304.03, base loss: 16104.56
[INFO 2017-06-29 08:54:44,838 main.py:57] epoch 10063, training loss: 8325.26, average training loss: 7305.08, base loss: 16104.82
[INFO 2017-06-29 08:54:47,892 main.py:57] epoch 10064, training loss: 7730.81, average training loss: 7305.06, base loss: 16105.00
[INFO 2017-06-29 08:54:50,989 main.py:57] epoch 10065, training loss: 8324.81, average training loss: 7307.34, base loss: 16105.22
[INFO 2017-06-29 08:54:54,043 main.py:57] epoch 10066, training loss: 6991.60, average training loss: 7308.14, base loss: 16104.94
[INFO 2017-06-29 08:54:57,096 main.py:57] epoch 10067, training loss: 6818.31, average training loss: 7307.44, base loss: 16104.52
[INFO 2017-06-29 08:55:00,127 main.py:57] epoch 10068, training loss: 7965.28, average training loss: 7306.07, base loss: 16104.86
[INFO 2017-06-29 08:55:03,172 main.py:57] epoch 10069, training loss: 6398.14, average training loss: 7305.49, base loss: 16104.66
[INFO 2017-06-29 08:55:06,288 main.py:57] epoch 10070, training loss: 7133.31, average training loss: 7304.98, base loss: 16104.47
[INFO 2017-06-29 08:55:09,339 main.py:57] epoch 10071, training loss: 8623.54, average training loss: 7305.80, base loss: 16104.88
[INFO 2017-06-29 08:55:12,381 main.py:57] epoch 10072, training loss: 6478.98, average training loss: 7304.85, base loss: 16104.56
[INFO 2017-06-29 08:55:15,441 main.py:57] epoch 10073, training loss: 7459.81, average training loss: 7305.23, base loss: 16104.57
[INFO 2017-06-29 08:55:18,518 main.py:57] epoch 10074, training loss: 6533.32, average training loss: 7304.07, base loss: 16104.37
[INFO 2017-06-29 08:55:21,602 main.py:57] epoch 10075, training loss: 7542.14, average training loss: 7304.06, base loss: 16104.68
[INFO 2017-06-29 08:55:24,637 main.py:57] epoch 10076, training loss: 6700.05, average training loss: 7303.92, base loss: 16104.73
[INFO 2017-06-29 08:55:27,621 main.py:57] epoch 10077, training loss: 7246.59, average training loss: 7302.50, base loss: 16105.02
[INFO 2017-06-29 08:55:30,625 main.py:57] epoch 10078, training loss: 7442.66, average training loss: 7302.95, base loss: 16104.73
[INFO 2017-06-29 08:55:33,588 main.py:57] epoch 10079, training loss: 7760.44, average training loss: 7303.82, base loss: 16104.84
[INFO 2017-06-29 08:55:36,585 main.py:57] epoch 10080, training loss: 7260.61, average training loss: 7303.33, base loss: 16104.81
[INFO 2017-06-29 08:55:39,662 main.py:57] epoch 10081, training loss: 8986.62, average training loss: 7304.72, base loss: 16105.22
[INFO 2017-06-29 08:55:42,757 main.py:57] epoch 10082, training loss: 6250.08, average training loss: 7303.67, base loss: 16104.62
[INFO 2017-06-29 08:55:45,802 main.py:57] epoch 10083, training loss: 7758.92, average training loss: 7303.87, base loss: 16104.82
[INFO 2017-06-29 08:55:48,837 main.py:57] epoch 10084, training loss: 7071.40, average training loss: 7303.72, base loss: 16104.76
[INFO 2017-06-29 08:55:51,838 main.py:57] epoch 10085, training loss: 6186.57, average training loss: 7303.57, base loss: 16104.48
[INFO 2017-06-29 08:55:54,902 main.py:57] epoch 10086, training loss: 7162.21, average training loss: 7303.76, base loss: 16104.56
[INFO 2017-06-29 08:55:57,918 main.py:57] epoch 10087, training loss: 7756.82, average training loss: 7304.96, base loss: 16104.53
[INFO 2017-06-29 08:56:01,045 main.py:57] epoch 10088, training loss: 7817.71, average training loss: 7305.28, base loss: 16104.92
[INFO 2017-06-29 08:56:03,989 main.py:57] epoch 10089, training loss: 7408.42, average training loss: 7304.06, base loss: 16105.03
[INFO 2017-06-29 08:56:07,069 main.py:57] epoch 10090, training loss: 7184.25, average training loss: 7304.70, base loss: 16105.23
[INFO 2017-06-29 08:56:10,150 main.py:57] epoch 10091, training loss: 7418.66, average training loss: 7304.45, base loss: 16105.52
[INFO 2017-06-29 08:56:13,219 main.py:57] epoch 10092, training loss: 6788.60, average training loss: 7303.87, base loss: 16105.39
[INFO 2017-06-29 08:56:16,293 main.py:57] epoch 10093, training loss: 7646.74, average training loss: 7304.58, base loss: 16105.66
[INFO 2017-06-29 08:56:19,271 main.py:57] epoch 10094, training loss: 6614.87, average training loss: 7303.98, base loss: 16105.48
[INFO 2017-06-29 08:56:22,360 main.py:57] epoch 10095, training loss: 7576.92, average training loss: 7304.13, base loss: 16105.57
[INFO 2017-06-29 08:56:25,367 main.py:57] epoch 10096, training loss: 6632.08, average training loss: 7304.22, base loss: 16105.33
[INFO 2017-06-29 08:56:28,457 main.py:57] epoch 10097, training loss: 6932.61, average training loss: 7303.63, base loss: 16105.32
[INFO 2017-06-29 08:56:31,523 main.py:57] epoch 10098, training loss: 7967.16, average training loss: 7303.42, base loss: 16105.77
[INFO 2017-06-29 08:56:34,602 main.py:57] epoch 10099, training loss: 7389.18, average training loss: 7302.28, base loss: 16105.83
[INFO 2017-06-29 08:56:34,603 main.py:59] epoch 10099, testing
[INFO 2017-06-29 08:56:47,156 main.py:104] average testing loss: 7730.56, base loss: 16031.43
[INFO 2017-06-29 08:56:47,156 main.py:105] improve_loss: 8300.87, improve_percent: 0.52
[INFO 2017-06-29 08:56:47,158 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 08:56:50,201 main.py:57] epoch 10100, training loss: 6641.71, average training loss: 7300.50, base loss: 16105.28
[INFO 2017-06-29 08:56:53,271 main.py:57] epoch 10101, training loss: 7355.07, average training loss: 7300.80, base loss: 16104.92
[INFO 2017-06-29 08:56:56,342 main.py:57] epoch 10102, training loss: 6963.26, average training loss: 7300.56, base loss: 16104.73
[INFO 2017-06-29 08:56:59,442 main.py:57] epoch 10103, training loss: 7412.36, average training loss: 7300.16, base loss: 16104.74
[INFO 2017-06-29 08:57:02,458 main.py:57] epoch 10104, training loss: 7297.14, average training loss: 7300.40, base loss: 16104.85
[INFO 2017-06-29 08:57:05,486 main.py:57] epoch 10105, training loss: 7273.70, average training loss: 7300.64, base loss: 16105.17
[INFO 2017-06-29 08:57:08,491 main.py:57] epoch 10106, training loss: 7794.37, average training loss: 7301.35, base loss: 16105.46
[INFO 2017-06-29 08:57:11,558 main.py:57] epoch 10107, training loss: 6988.43, average training loss: 7302.36, base loss: 16105.46
[INFO 2017-06-29 08:57:14,658 main.py:57] epoch 10108, training loss: 7104.17, average training loss: 7301.77, base loss: 16105.53
[INFO 2017-06-29 08:57:17,697 main.py:57] epoch 10109, training loss: 6878.81, average training loss: 7301.35, base loss: 16105.71
[INFO 2017-06-29 08:57:20,739 main.py:57] epoch 10110, training loss: 7353.86, average training loss: 7300.87, base loss: 16105.90
[INFO 2017-06-29 08:57:23,752 main.py:57] epoch 10111, training loss: 6853.66, average training loss: 7300.12, base loss: 16105.93
[INFO 2017-06-29 08:57:26,841 main.py:57] epoch 10112, training loss: 7329.50, average training loss: 7300.97, base loss: 16105.84
[INFO 2017-06-29 08:57:29,983 main.py:57] epoch 10113, training loss: 7007.14, average training loss: 7299.22, base loss: 16105.83
[INFO 2017-06-29 08:57:33,063 main.py:57] epoch 10114, training loss: 7412.71, average training loss: 7300.44, base loss: 16105.84
[INFO 2017-06-29 08:57:36,100 main.py:57] epoch 10115, training loss: 6294.22, average training loss: 7299.33, base loss: 16105.47
[INFO 2017-06-29 08:57:39,138 main.py:57] epoch 10116, training loss: 7638.95, average training loss: 7299.87, base loss: 16105.77
[INFO 2017-06-29 08:57:42,194 main.py:57] epoch 10117, training loss: 8075.40, average training loss: 7300.54, base loss: 16106.03
[INFO 2017-06-29 08:57:45,213 main.py:57] epoch 10118, training loss: 6631.93, average training loss: 7300.03, base loss: 16105.93
[INFO 2017-06-29 08:57:48,207 main.py:57] epoch 10119, training loss: 7763.68, average training loss: 7301.04, base loss: 16106.32
[INFO 2017-06-29 08:57:51,284 main.py:57] epoch 10120, training loss: 7125.16, average training loss: 7300.69, base loss: 16106.41
[INFO 2017-06-29 08:57:54,337 main.py:57] epoch 10121, training loss: 7559.58, average training loss: 7301.27, base loss: 16106.81
[INFO 2017-06-29 08:57:57,386 main.py:57] epoch 10122, training loss: 6423.11, average training loss: 7300.17, base loss: 16106.49
[INFO 2017-06-29 08:58:00,516 main.py:57] epoch 10123, training loss: 7509.43, average training loss: 7300.49, base loss: 16106.47
[INFO 2017-06-29 08:58:03,606 main.py:57] epoch 10124, training loss: 7852.51, average training loss: 7301.42, base loss: 16106.63
[INFO 2017-06-29 08:58:06,730 main.py:57] epoch 10125, training loss: 6362.24, average training loss: 7300.96, base loss: 16106.40
[INFO 2017-06-29 08:58:09,737 main.py:57] epoch 10126, training loss: 6754.23, average training loss: 7299.89, base loss: 16106.17
[INFO 2017-06-29 08:58:12,749 main.py:57] epoch 10127, training loss: 6714.71, average training loss: 7299.26, base loss: 16106.45
[INFO 2017-06-29 08:58:15,779 main.py:57] epoch 10128, training loss: 6816.96, average training loss: 7299.44, base loss: 16106.28
[INFO 2017-06-29 08:58:18,883 main.py:57] epoch 10129, training loss: 8508.54, average training loss: 7301.55, base loss: 16106.96
[INFO 2017-06-29 08:58:21,885 main.py:57] epoch 10130, training loss: 7343.27, average training loss: 7301.76, base loss: 16106.96
[INFO 2017-06-29 08:58:24,973 main.py:57] epoch 10131, training loss: 7088.54, average training loss: 7301.46, base loss: 16106.85
[INFO 2017-06-29 08:58:28,111 main.py:57] epoch 10132, training loss: 7862.57, average training loss: 7302.53, base loss: 16107.21
[INFO 2017-06-29 08:58:31,147 main.py:57] epoch 10133, training loss: 6582.42, average training loss: 7301.12, base loss: 16106.84
[INFO 2017-06-29 08:58:34,275 main.py:57] epoch 10134, training loss: 8020.75, average training loss: 7300.82, base loss: 16107.23
[INFO 2017-06-29 08:58:37,296 main.py:57] epoch 10135, training loss: 8661.72, average training loss: 7302.22, base loss: 16107.69
[INFO 2017-06-29 08:58:40,303 main.py:57] epoch 10136, training loss: 7299.41, average training loss: 7302.57, base loss: 16107.88
[INFO 2017-06-29 08:58:43,383 main.py:57] epoch 10137, training loss: 6742.56, average training loss: 7302.35, base loss: 16107.84
[INFO 2017-06-29 08:58:46,426 main.py:57] epoch 10138, training loss: 7375.44, average training loss: 7302.65, base loss: 16107.53
[INFO 2017-06-29 08:58:49,546 main.py:57] epoch 10139, training loss: 7380.38, average training loss: 7302.34, base loss: 16107.34
[INFO 2017-06-29 08:58:52,642 main.py:57] epoch 10140, training loss: 7138.28, average training loss: 7302.52, base loss: 16107.31
[INFO 2017-06-29 08:58:55,655 main.py:57] epoch 10141, training loss: 7032.59, average training loss: 7301.90, base loss: 16107.33
[INFO 2017-06-29 08:58:58,734 main.py:57] epoch 10142, training loss: 7009.50, average training loss: 7301.89, base loss: 16107.36
[INFO 2017-06-29 08:59:01,756 main.py:57] epoch 10143, training loss: 7037.77, average training loss: 7300.52, base loss: 16106.90
[INFO 2017-06-29 08:59:04,836 main.py:57] epoch 10144, training loss: 6523.85, average training loss: 7299.13, base loss: 16106.44
[INFO 2017-06-29 08:59:07,852 main.py:57] epoch 10145, training loss: 7452.23, average training loss: 7299.21, base loss: 16106.78
[INFO 2017-06-29 08:59:10,884 main.py:57] epoch 10146, training loss: 6916.84, average training loss: 7299.19, base loss: 16106.42
[INFO 2017-06-29 08:59:13,943 main.py:57] epoch 10147, training loss: 8519.62, average training loss: 7300.48, base loss: 16106.27
[INFO 2017-06-29 08:59:17,005 main.py:57] epoch 10148, training loss: 6868.56, average training loss: 7299.34, base loss: 16106.40
[INFO 2017-06-29 08:59:20,096 main.py:57] epoch 10149, training loss: 7616.99, average training loss: 7299.86, base loss: 16106.88
[INFO 2017-06-29 08:59:23,157 main.py:57] epoch 10150, training loss: 7475.18, average training loss: 7300.25, base loss: 16106.92
[INFO 2017-06-29 08:59:26,178 main.py:57] epoch 10151, training loss: 6746.01, average training loss: 7300.12, base loss: 16107.10
[INFO 2017-06-29 08:59:29,160 main.py:57] epoch 10152, training loss: 6750.42, average training loss: 7299.93, base loss: 16106.84
[INFO 2017-06-29 08:59:32,206 main.py:57] epoch 10153, training loss: 7556.13, average training loss: 7299.90, base loss: 16107.12
[INFO 2017-06-29 08:59:35,252 main.py:57] epoch 10154, training loss: 8198.62, average training loss: 7301.46, base loss: 16107.44
[INFO 2017-06-29 08:59:38,343 main.py:57] epoch 10155, training loss: 7397.16, average training loss: 7302.48, base loss: 16107.48
[INFO 2017-06-29 08:59:41,520 main.py:57] epoch 10156, training loss: 7153.26, average training loss: 7302.10, base loss: 16107.62
[INFO 2017-06-29 08:59:44,563 main.py:57] epoch 10157, training loss: 7358.76, average training loss: 7302.54, base loss: 16107.65
[INFO 2017-06-29 08:59:47,652 main.py:57] epoch 10158, training loss: 6598.83, average training loss: 7301.76, base loss: 16107.46
[INFO 2017-06-29 08:59:50,727 main.py:57] epoch 10159, training loss: 7046.45, average training loss: 7301.59, base loss: 16107.49
[INFO 2017-06-29 08:59:53,732 main.py:57] epoch 10160, training loss: 6606.63, average training loss: 7301.44, base loss: 16107.59
[INFO 2017-06-29 08:59:56,843 main.py:57] epoch 10161, training loss: 7203.56, average training loss: 7300.69, base loss: 16107.62
[INFO 2017-06-29 08:59:59,853 main.py:57] epoch 10162, training loss: 6433.19, average training loss: 7299.58, base loss: 16107.43
[INFO 2017-06-29 09:00:02,889 main.py:57] epoch 10163, training loss: 8169.99, average training loss: 7300.49, base loss: 16107.60
[INFO 2017-06-29 09:00:05,909 main.py:57] epoch 10164, training loss: 6919.23, average training loss: 7300.39, base loss: 16107.59
[INFO 2017-06-29 09:00:08,906 main.py:57] epoch 10165, training loss: 6203.21, average training loss: 7299.34, base loss: 16107.27
[INFO 2017-06-29 09:00:11,985 main.py:57] epoch 10166, training loss: 8127.22, average training loss: 7300.56, base loss: 16108.04
[INFO 2017-06-29 09:00:15,019 main.py:57] epoch 10167, training loss: 7467.58, average training loss: 7300.17, base loss: 16108.01
[INFO 2017-06-29 09:00:18,028 main.py:57] epoch 10168, training loss: 7662.66, average training loss: 7301.37, base loss: 16108.06
[INFO 2017-06-29 09:00:21,030 main.py:57] epoch 10169, training loss: 7574.03, average training loss: 7301.51, base loss: 16108.29
[INFO 2017-06-29 09:00:24,070 main.py:57] epoch 10170, training loss: 7632.92, average training loss: 7302.66, base loss: 16108.26
[INFO 2017-06-29 09:00:27,112 main.py:57] epoch 10171, training loss: 7220.54, average training loss: 7303.33, base loss: 16107.97
[INFO 2017-06-29 09:00:30,159 main.py:57] epoch 10172, training loss: 7598.86, average training loss: 7303.13, base loss: 16108.59
[INFO 2017-06-29 09:00:33,222 main.py:57] epoch 10173, training loss: 7355.70, average training loss: 7302.63, base loss: 16108.52
[INFO 2017-06-29 09:00:36,321 main.py:57] epoch 10174, training loss: 7967.28, average training loss: 7303.67, base loss: 16109.32
[INFO 2017-06-29 09:00:39,391 main.py:57] epoch 10175, training loss: 7247.74, average training loss: 7303.31, base loss: 16109.34
[INFO 2017-06-29 09:00:42,367 main.py:57] epoch 10176, training loss: 8218.58, average training loss: 7304.00, base loss: 16109.41
[INFO 2017-06-29 09:00:45,386 main.py:57] epoch 10177, training loss: 6916.16, average training loss: 7303.44, base loss: 16109.33
[INFO 2017-06-29 09:00:48,487 main.py:57] epoch 10178, training loss: 6739.05, average training loss: 7302.71, base loss: 16109.28
[INFO 2017-06-29 09:00:51,542 main.py:57] epoch 10179, training loss: 6918.15, average training loss: 7301.75, base loss: 16109.18
[INFO 2017-06-29 09:00:54,559 main.py:57] epoch 10180, training loss: 7073.02, average training loss: 7301.63, base loss: 16108.87
[INFO 2017-06-29 09:00:57,625 main.py:57] epoch 10181, training loss: 7446.56, average training loss: 7302.07, base loss: 16108.90
[INFO 2017-06-29 09:01:00,671 main.py:57] epoch 10182, training loss: 8671.90, average training loss: 7303.98, base loss: 16109.36
[INFO 2017-06-29 09:01:03,715 main.py:57] epoch 10183, training loss: 7692.82, average training loss: 7304.43, base loss: 16109.53
[INFO 2017-06-29 09:01:06,753 main.py:57] epoch 10184, training loss: 7146.89, average training loss: 7305.18, base loss: 16109.51
[INFO 2017-06-29 09:01:09,766 main.py:57] epoch 10185, training loss: 7075.64, average training loss: 7304.22, base loss: 16109.35
[INFO 2017-06-29 09:01:12,822 main.py:57] epoch 10186, training loss: 7032.55, average training loss: 7304.28, base loss: 16109.39
[INFO 2017-06-29 09:01:15,881 main.py:57] epoch 10187, training loss: 6881.75, average training loss: 7303.17, base loss: 16109.28
[INFO 2017-06-29 09:01:18,998 main.py:57] epoch 10188, training loss: 7666.58, average training loss: 7303.01, base loss: 16109.75
[INFO 2017-06-29 09:01:22,023 main.py:57] epoch 10189, training loss: 7325.15, average training loss: 7302.82, base loss: 16109.76
[INFO 2017-06-29 09:01:25,100 main.py:57] epoch 10190, training loss: 8005.96, average training loss: 7303.41, base loss: 16110.23
[INFO 2017-06-29 09:01:28,112 main.py:57] epoch 10191, training loss: 7553.20, average training loss: 7304.13, base loss: 16110.32
[INFO 2017-06-29 09:01:31,129 main.py:57] epoch 10192, training loss: 7525.35, average training loss: 7304.37, base loss: 16109.90
[INFO 2017-06-29 09:01:34,252 main.py:57] epoch 10193, training loss: 8549.49, average training loss: 7305.34, base loss: 16110.20
[INFO 2017-06-29 09:01:37,250 main.py:57] epoch 10194, training loss: 8477.77, average training loss: 7306.08, base loss: 16110.56
[INFO 2017-06-29 09:01:40,344 main.py:57] epoch 10195, training loss: 7534.35, average training loss: 7305.36, base loss: 16110.73
[INFO 2017-06-29 09:01:43,380 main.py:57] epoch 10196, training loss: 7002.56, average training loss: 7304.80, base loss: 16110.93
[INFO 2017-06-29 09:01:46,457 main.py:57] epoch 10197, training loss: 6639.40, average training loss: 7303.62, base loss: 16110.54
[INFO 2017-06-29 09:01:49,463 main.py:57] epoch 10198, training loss: 6808.30, average training loss: 7303.67, base loss: 16110.35
[INFO 2017-06-29 09:01:52,529 main.py:57] epoch 10199, training loss: 8740.20, average training loss: 7303.61, base loss: 16110.95
[INFO 2017-06-29 09:01:52,530 main.py:59] epoch 10199, testing
[INFO 2017-06-29 09:02:05,181 main.py:104] average testing loss: 8484.88, base loss: 17356.05
[INFO 2017-06-29 09:02:05,181 main.py:105] improve_loss: 8871.17, improve_percent: 0.51
[INFO 2017-06-29 09:02:05,183 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:02:08,226 main.py:57] epoch 10200, training loss: 8658.89, average training loss: 7304.07, base loss: 16111.72
[INFO 2017-06-29 09:02:11,257 main.py:57] epoch 10201, training loss: 7380.43, average training loss: 7304.16, base loss: 16111.75
[INFO 2017-06-29 09:02:14,287 main.py:57] epoch 10202, training loss: 6722.70, average training loss: 7303.84, base loss: 16111.47
[INFO 2017-06-29 09:02:17,351 main.py:57] epoch 10203, training loss: 7073.97, average training loss: 7303.40, base loss: 16111.44
[INFO 2017-06-29 09:02:20,440 main.py:57] epoch 10204, training loss: 7523.91, average training loss: 7304.46, base loss: 16111.40
[INFO 2017-06-29 09:02:23,473 main.py:57] epoch 10205, training loss: 7288.63, average training loss: 7304.98, base loss: 16111.40
[INFO 2017-06-29 09:02:26,492 main.py:57] epoch 10206, training loss: 7259.03, average training loss: 7304.24, base loss: 16111.40
[INFO 2017-06-29 09:02:29,612 main.py:57] epoch 10207, training loss: 6673.34, average training loss: 7303.51, base loss: 16111.19
[INFO 2017-06-29 09:02:32,698 main.py:57] epoch 10208, training loss: 6341.25, average training loss: 7302.27, base loss: 16110.99
[INFO 2017-06-29 09:02:35,668 main.py:57] epoch 10209, training loss: 6228.45, average training loss: 7301.05, base loss: 16110.69
[INFO 2017-06-29 09:02:38,751 main.py:57] epoch 10210, training loss: 8308.53, average training loss: 7302.24, base loss: 16111.04
[INFO 2017-06-29 09:02:41,788 main.py:57] epoch 10211, training loss: 7790.32, average training loss: 7303.11, base loss: 16111.30
[INFO 2017-06-29 09:02:44,834 main.py:57] epoch 10212, training loss: 7884.75, average training loss: 7302.68, base loss: 16111.40
[INFO 2017-06-29 09:02:47,891 main.py:57] epoch 10213, training loss: 8554.47, average training loss: 7304.49, base loss: 16111.81
[INFO 2017-06-29 09:02:50,970 main.py:57] epoch 10214, training loss: 7938.46, average training loss: 7305.52, base loss: 16111.65
[INFO 2017-06-29 09:02:53,984 main.py:57] epoch 10215, training loss: 6315.82, average training loss: 7304.41, base loss: 16111.22
[INFO 2017-06-29 09:02:57,152 main.py:57] epoch 10216, training loss: 6762.60, average training loss: 7304.75, base loss: 16111.08
[INFO 2017-06-29 09:03:00,237 main.py:57] epoch 10217, training loss: 7997.22, average training loss: 7305.26, base loss: 16111.59
[INFO 2017-06-29 09:03:03,330 main.py:57] epoch 10218, training loss: 6628.63, average training loss: 7304.19, base loss: 16111.59
[INFO 2017-06-29 09:03:06,409 main.py:57] epoch 10219, training loss: 7788.78, average training loss: 7305.04, base loss: 16111.87
[INFO 2017-06-29 09:03:09,472 main.py:57] epoch 10220, training loss: 6382.36, average training loss: 7303.77, base loss: 16111.57
[INFO 2017-06-29 09:03:12,513 main.py:57] epoch 10221, training loss: 6733.87, average training loss: 7302.54, base loss: 16111.09
[INFO 2017-06-29 09:03:15,587 main.py:57] epoch 10222, training loss: 7140.28, average training loss: 7303.16, base loss: 16111.10
[INFO 2017-06-29 09:03:18,688 main.py:57] epoch 10223, training loss: 7278.76, average training loss: 7302.78, base loss: 16110.92
[INFO 2017-06-29 09:03:21,845 main.py:57] epoch 10224, training loss: 8240.39, average training loss: 7303.19, base loss: 16111.53
[INFO 2017-06-29 09:03:24,850 main.py:57] epoch 10225, training loss: 7718.20, average training loss: 7303.41, base loss: 16111.79
[INFO 2017-06-29 09:03:27,859 main.py:57] epoch 10226, training loss: 7195.51, average training loss: 7302.23, base loss: 16111.84
[INFO 2017-06-29 09:03:31,000 main.py:57] epoch 10227, training loss: 7317.80, average training loss: 7301.17, base loss: 16111.92
[INFO 2017-06-29 09:03:34,122 main.py:57] epoch 10228, training loss: 6660.93, average training loss: 7300.07, base loss: 16111.67
[INFO 2017-06-29 09:03:37,179 main.py:57] epoch 10229, training loss: 7578.17, average training loss: 7300.42, base loss: 16111.54
[INFO 2017-06-29 09:03:40,200 main.py:57] epoch 10230, training loss: 7648.93, average training loss: 7300.42, base loss: 16111.73
[INFO 2017-06-29 09:03:43,396 main.py:57] epoch 10231, training loss: 6323.51, average training loss: 7299.38, base loss: 16111.34
[INFO 2017-06-29 09:03:46,426 main.py:57] epoch 10232, training loss: 6455.10, average training loss: 7298.56, base loss: 16111.20
[INFO 2017-06-29 09:03:49,531 main.py:57] epoch 10233, training loss: 7751.04, average training loss: 7299.92, base loss: 16111.64
[INFO 2017-06-29 09:03:52,607 main.py:57] epoch 10234, training loss: 6722.79, average training loss: 7299.60, base loss: 16111.50
[INFO 2017-06-29 09:03:55,653 main.py:57] epoch 10235, training loss: 6776.91, average training loss: 7298.83, base loss: 16111.18
[INFO 2017-06-29 09:03:58,688 main.py:57] epoch 10236, training loss: 7778.87, average training loss: 7299.45, base loss: 16111.49
[INFO 2017-06-29 09:04:01,724 main.py:57] epoch 10237, training loss: 6692.46, average training loss: 7299.63, base loss: 16111.62
[INFO 2017-06-29 09:04:04,811 main.py:57] epoch 10238, training loss: 6704.58, average training loss: 7299.17, base loss: 16111.37
[INFO 2017-06-29 09:04:07,843 main.py:57] epoch 10239, training loss: 6974.27, average training loss: 7299.17, base loss: 16111.24
[INFO 2017-06-29 09:04:10,855 main.py:57] epoch 10240, training loss: 7016.27, average training loss: 7297.51, base loss: 16110.61
[INFO 2017-06-29 09:04:13,879 main.py:57] epoch 10241, training loss: 7535.43, average training loss: 7297.11, base loss: 16110.70
[INFO 2017-06-29 09:04:16,963 main.py:57] epoch 10242, training loss: 7171.85, average training loss: 7297.30, base loss: 16110.96
[INFO 2017-06-29 09:04:20,163 main.py:57] epoch 10243, training loss: 7792.90, average training loss: 7296.91, base loss: 16111.42
[INFO 2017-06-29 09:04:23,171 main.py:57] epoch 10244, training loss: 7535.61, average training loss: 7296.47, base loss: 16111.77
[INFO 2017-06-29 09:04:26,170 main.py:57] epoch 10245, training loss: 6204.49, average training loss: 7294.98, base loss: 16111.37
[INFO 2017-06-29 09:04:29,195 main.py:57] epoch 10246, training loss: 8069.49, average training loss: 7296.38, base loss: 16111.62
[INFO 2017-06-29 09:04:32,262 main.py:57] epoch 10247, training loss: 7203.43, average training loss: 7296.69, base loss: 16111.58
[INFO 2017-06-29 09:04:35,353 main.py:57] epoch 10248, training loss: 7515.39, average training loss: 7295.53, base loss: 16111.25
[INFO 2017-06-29 09:04:38,401 main.py:57] epoch 10249, training loss: 6809.83, average training loss: 7294.83, base loss: 16110.94
[INFO 2017-06-29 09:04:41,419 main.py:57] epoch 10250, training loss: 7412.06, average training loss: 7294.71, base loss: 16111.09
[INFO 2017-06-29 09:04:44,555 main.py:57] epoch 10251, training loss: 7288.71, average training loss: 7294.55, base loss: 16111.30
[INFO 2017-06-29 09:04:47,564 main.py:57] epoch 10252, training loss: 7308.39, average training loss: 7294.93, base loss: 16111.30
[INFO 2017-06-29 09:04:50,698 main.py:57] epoch 10253, training loss: 6878.89, average training loss: 7293.27, base loss: 16111.28
[INFO 2017-06-29 09:04:53,717 main.py:57] epoch 10254, training loss: 6957.28, average training loss: 7292.54, base loss: 16111.26
[INFO 2017-06-29 09:04:56,773 main.py:57] epoch 10255, training loss: 7195.87, average training loss: 7292.19, base loss: 16111.33
[INFO 2017-06-29 09:04:59,759 main.py:57] epoch 10256, training loss: 8055.57, average training loss: 7293.46, base loss: 16111.52
[INFO 2017-06-29 09:05:02,754 main.py:57] epoch 10257, training loss: 7569.21, average training loss: 7293.93, base loss: 16111.42
[INFO 2017-06-29 09:05:05,786 main.py:57] epoch 10258, training loss: 6822.17, average training loss: 7293.61, base loss: 16111.32
[INFO 2017-06-29 09:05:08,872 main.py:57] epoch 10259, training loss: 8471.39, average training loss: 7294.92, base loss: 16111.60
[INFO 2017-06-29 09:05:11,961 main.py:57] epoch 10260, training loss: 7215.67, average training loss: 7294.58, base loss: 16111.68
[INFO 2017-06-29 09:05:15,064 main.py:57] epoch 10261, training loss: 7898.09, average training loss: 7296.22, base loss: 16111.93
[INFO 2017-06-29 09:05:18,156 main.py:57] epoch 10262, training loss: 8529.36, average training loss: 7297.07, base loss: 16112.71
[INFO 2017-06-29 09:05:21,274 main.py:57] epoch 10263, training loss: 8035.23, average training loss: 7297.84, base loss: 16113.17
[INFO 2017-06-29 09:05:24,384 main.py:57] epoch 10264, training loss: 5932.49, average training loss: 7295.97, base loss: 16112.71
[INFO 2017-06-29 09:05:27,463 main.py:57] epoch 10265, training loss: 8239.62, average training loss: 7296.19, base loss: 16113.24
[INFO 2017-06-29 09:05:30,478 main.py:57] epoch 10266, training loss: 8266.31, average training loss: 7297.24, base loss: 16113.43
[INFO 2017-06-29 09:05:33,576 main.py:57] epoch 10267, training loss: 7200.39, average training loss: 7297.66, base loss: 16113.49
[INFO 2017-06-29 09:05:36,699 main.py:57] epoch 10268, training loss: 7178.13, average training loss: 7296.79, base loss: 16113.21
[INFO 2017-06-29 09:05:39,751 main.py:57] epoch 10269, training loss: 7276.15, average training loss: 7297.42, base loss: 16113.28
[INFO 2017-06-29 09:05:42,806 main.py:57] epoch 10270, training loss: 6638.96, average training loss: 7296.52, base loss: 16113.03
[INFO 2017-06-29 09:05:45,861 main.py:57] epoch 10271, training loss: 7378.56, average training loss: 7297.97, base loss: 16113.08
[INFO 2017-06-29 09:05:48,969 main.py:57] epoch 10272, training loss: 7393.41, average training loss: 7298.12, base loss: 16113.05
[INFO 2017-06-29 09:05:51,995 main.py:57] epoch 10273, training loss: 7183.18, average training loss: 7298.17, base loss: 16113.03
[INFO 2017-06-29 09:05:55,046 main.py:57] epoch 10274, training loss: 7360.31, average training loss: 7298.33, base loss: 16113.15
[INFO 2017-06-29 09:05:58,057 main.py:57] epoch 10275, training loss: 7440.29, average training loss: 7298.35, base loss: 16113.08
[INFO 2017-06-29 09:06:01,107 main.py:57] epoch 10276, training loss: 8464.73, average training loss: 7299.79, base loss: 16113.35
[INFO 2017-06-29 09:06:04,219 main.py:57] epoch 10277, training loss: 7036.85, average training loss: 7299.54, base loss: 16113.10
[INFO 2017-06-29 09:06:07,286 main.py:57] epoch 10278, training loss: 7746.95, average training loss: 7300.17, base loss: 16113.15
[INFO 2017-06-29 09:06:10,246 main.py:57] epoch 10279, training loss: 7484.52, average training loss: 7300.31, base loss: 16113.10
[INFO 2017-06-29 09:06:13,356 main.py:57] epoch 10280, training loss: 8197.28, average training loss: 7301.27, base loss: 16113.53
[INFO 2017-06-29 09:06:16,415 main.py:57] epoch 10281, training loss: 7795.02, average training loss: 7302.75, base loss: 16113.81
[INFO 2017-06-29 09:06:19,461 main.py:57] epoch 10282, training loss: 7129.98, average training loss: 7303.33, base loss: 16113.40
[INFO 2017-06-29 09:06:22,514 main.py:57] epoch 10283, training loss: 8443.93, average training loss: 7304.36, base loss: 16113.96
[INFO 2017-06-29 09:06:25,568 main.py:57] epoch 10284, training loss: 7977.87, average training loss: 7304.39, base loss: 16114.11
[INFO 2017-06-29 09:06:28,640 main.py:57] epoch 10285, training loss: 8434.04, average training loss: 7304.99, base loss: 16114.42
[INFO 2017-06-29 09:06:31,677 main.py:57] epoch 10286, training loss: 7744.77, average training loss: 7305.21, base loss: 16114.42
[INFO 2017-06-29 09:06:34,756 main.py:57] epoch 10287, training loss: 6920.91, average training loss: 7305.37, base loss: 16114.50
[INFO 2017-06-29 09:06:37,835 main.py:57] epoch 10288, training loss: 7038.14, average training loss: 7304.32, base loss: 16114.34
[INFO 2017-06-29 09:06:40,875 main.py:57] epoch 10289, training loss: 7093.97, average training loss: 7303.47, base loss: 16114.49
[INFO 2017-06-29 09:06:43,939 main.py:57] epoch 10290, training loss: 7335.76, average training loss: 7303.66, base loss: 16114.34
[INFO 2017-06-29 09:06:46,956 main.py:57] epoch 10291, training loss: 7597.21, average training loss: 7304.66, base loss: 16114.29
[INFO 2017-06-29 09:06:50,073 main.py:57] epoch 10292, training loss: 6985.04, average training loss: 7304.23, base loss: 16114.42
[INFO 2017-06-29 09:06:53,163 main.py:57] epoch 10293, training loss: 7516.01, average training loss: 7303.78, base loss: 16114.26
[INFO 2017-06-29 09:06:56,280 main.py:57] epoch 10294, training loss: 7430.69, average training loss: 7303.06, base loss: 16114.45
[INFO 2017-06-29 09:06:59,301 main.py:57] epoch 10295, training loss: 8436.10, average training loss: 7304.66, base loss: 16114.86
[INFO 2017-06-29 09:07:02,299 main.py:57] epoch 10296, training loss: 7198.19, average training loss: 7305.27, base loss: 16115.17
[INFO 2017-06-29 09:07:05,363 main.py:57] epoch 10297, training loss: 6197.94, average training loss: 7304.62, base loss: 16114.87
[INFO 2017-06-29 09:07:08,411 main.py:57] epoch 10298, training loss: 7172.96, average training loss: 7303.34, base loss: 16114.84
[INFO 2017-06-29 09:07:11,474 main.py:57] epoch 10299, training loss: 6549.89, average training loss: 7302.03, base loss: 16114.81
[INFO 2017-06-29 09:07:11,475 main.py:59] epoch 10299, testing
[INFO 2017-06-29 09:07:24,065 main.py:104] average testing loss: 7992.26, base loss: 16827.51
[INFO 2017-06-29 09:07:24,065 main.py:105] improve_loss: 8835.24, improve_percent: 0.53
[INFO 2017-06-29 09:07:24,066 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:07:27,109 main.py:57] epoch 10300, training loss: 6878.12, average training loss: 7301.90, base loss: 16114.38
[INFO 2017-06-29 09:07:30,107 main.py:57] epoch 10301, training loss: 7511.81, average training loss: 7301.53, base loss: 16114.28
[INFO 2017-06-29 09:07:33,187 main.py:57] epoch 10302, training loss: 6243.19, average training loss: 7300.75, base loss: 16114.14
[INFO 2017-06-29 09:07:36,229 main.py:57] epoch 10303, training loss: 7823.49, average training loss: 7301.38, base loss: 16114.59
[INFO 2017-06-29 09:07:39,303 main.py:57] epoch 10304, training loss: 8203.18, average training loss: 7302.55, base loss: 16114.84
[INFO 2017-06-29 09:07:42,305 main.py:57] epoch 10305, training loss: 7642.08, average training loss: 7301.94, base loss: 16115.03
[INFO 2017-06-29 09:07:45,341 main.py:57] epoch 10306, training loss: 7208.54, average training loss: 7302.51, base loss: 16115.20
[INFO 2017-06-29 09:07:48,433 main.py:57] epoch 10307, training loss: 6780.22, average training loss: 7302.14, base loss: 16114.83
[INFO 2017-06-29 09:07:51,472 main.py:57] epoch 10308, training loss: 7760.51, average training loss: 7302.48, base loss: 16114.66
[INFO 2017-06-29 09:07:54,533 main.py:57] epoch 10309, training loss: 7455.16, average training loss: 7302.87, base loss: 16114.91
[INFO 2017-06-29 09:07:57,644 main.py:57] epoch 10310, training loss: 7229.07, average training loss: 7303.79, base loss: 16115.11
[INFO 2017-06-29 09:08:00,647 main.py:57] epoch 10311, training loss: 6108.39, average training loss: 7302.62, base loss: 16114.79
[INFO 2017-06-29 09:08:03,734 main.py:57] epoch 10312, training loss: 6761.73, average training loss: 7301.02, base loss: 16114.54
[INFO 2017-06-29 09:08:06,784 main.py:57] epoch 10313, training loss: 8095.57, average training loss: 7301.71, base loss: 16115.08
[INFO 2017-06-29 09:08:09,819 main.py:57] epoch 10314, training loss: 7133.75, average training loss: 7300.43, base loss: 16115.39
[INFO 2017-06-29 09:08:12,937 main.py:57] epoch 10315, training loss: 6826.16, average training loss: 7301.48, base loss: 16115.41
[INFO 2017-06-29 09:08:16,059 main.py:57] epoch 10316, training loss: 7603.61, average training loss: 7301.08, base loss: 16115.49
[INFO 2017-06-29 09:08:19,160 main.py:57] epoch 10317, training loss: 6309.18, average training loss: 7299.81, base loss: 16115.19
[INFO 2017-06-29 09:08:22,240 main.py:57] epoch 10318, training loss: 7410.66, average training loss: 7299.95, base loss: 16115.28
[INFO 2017-06-29 09:08:25,240 main.py:57] epoch 10319, training loss: 6968.23, average training loss: 7299.21, base loss: 16115.00
[INFO 2017-06-29 09:08:28,228 main.py:57] epoch 10320, training loss: 7415.92, average training loss: 7299.43, base loss: 16114.96
[INFO 2017-06-29 09:08:31,266 main.py:57] epoch 10321, training loss: 6477.59, average training loss: 7298.86, base loss: 16114.79
[INFO 2017-06-29 09:08:34,273 main.py:57] epoch 10322, training loss: 8514.85, average training loss: 7300.60, base loss: 16115.18
[INFO 2017-06-29 09:08:37,340 main.py:57] epoch 10323, training loss: 6946.43, average training loss: 7299.52, base loss: 16114.83
[INFO 2017-06-29 09:08:40,400 main.py:57] epoch 10324, training loss: 6499.96, average training loss: 7299.01, base loss: 16114.63
[INFO 2017-06-29 09:08:43,455 main.py:57] epoch 10325, training loss: 6204.19, average training loss: 7298.29, base loss: 16114.11
[INFO 2017-06-29 09:08:46,519 main.py:57] epoch 10326, training loss: 6058.02, average training loss: 7297.41, base loss: 16113.40
[INFO 2017-06-29 09:08:49,599 main.py:57] epoch 10327, training loss: 7781.86, average training loss: 7297.68, base loss: 16113.44
[INFO 2017-06-29 09:08:52,663 main.py:57] epoch 10328, training loss: 7640.25, average training loss: 7297.58, base loss: 16113.31
[INFO 2017-06-29 09:08:55,727 main.py:57] epoch 10329, training loss: 6943.90, average training loss: 7297.03, base loss: 16113.31
[INFO 2017-06-29 09:08:58,798 main.py:57] epoch 10330, training loss: 7137.57, average training loss: 7298.17, base loss: 16113.21
[INFO 2017-06-29 09:09:01,870 main.py:57] epoch 10331, training loss: 6920.10, average training loss: 7297.58, base loss: 16113.12
[INFO 2017-06-29 09:09:04,918 main.py:57] epoch 10332, training loss: 6966.11, average training loss: 7297.83, base loss: 16112.86
[INFO 2017-06-29 09:09:07,974 main.py:57] epoch 10333, training loss: 8504.37, average training loss: 7299.17, base loss: 16113.44
[INFO 2017-06-29 09:09:10,996 main.py:57] epoch 10334, training loss: 7647.08, average training loss: 7300.27, base loss: 16113.53
[INFO 2017-06-29 09:09:14,040 main.py:57] epoch 10335, training loss: 8125.63, average training loss: 7301.43, base loss: 16113.88
[INFO 2017-06-29 09:09:17,125 main.py:57] epoch 10336, training loss: 6706.99, average training loss: 7300.11, base loss: 16113.66
[INFO 2017-06-29 09:09:20,115 main.py:57] epoch 10337, training loss: 6847.73, average training loss: 7298.97, base loss: 16113.56
[INFO 2017-06-29 09:09:23,145 main.py:57] epoch 10338, training loss: 7036.46, average training loss: 7299.32, base loss: 16113.57
[INFO 2017-06-29 09:09:26,195 main.py:57] epoch 10339, training loss: 6346.76, average training loss: 7298.74, base loss: 16112.98
[INFO 2017-06-29 09:09:29,245 main.py:57] epoch 10340, training loss: 7529.95, average training loss: 7299.02, base loss: 16113.21
[INFO 2017-06-29 09:09:32,325 main.py:57] epoch 10341, training loss: 7454.38, average training loss: 7299.10, base loss: 16113.54
[INFO 2017-06-29 09:09:35,400 main.py:57] epoch 10342, training loss: 6645.23, average training loss: 7298.53, base loss: 16113.31
[INFO 2017-06-29 09:09:38,443 main.py:57] epoch 10343, training loss: 6729.81, average training loss: 7298.31, base loss: 16113.13
[INFO 2017-06-29 09:09:41,547 main.py:57] epoch 10344, training loss: 6777.00, average training loss: 7298.70, base loss: 16113.08
[INFO 2017-06-29 09:09:44,575 main.py:57] epoch 10345, training loss: 8670.99, average training loss: 7299.12, base loss: 16113.28
[INFO 2017-06-29 09:09:47,671 main.py:57] epoch 10346, training loss: 6938.95, average training loss: 7298.66, base loss: 16113.40
[INFO 2017-06-29 09:09:50,678 main.py:57] epoch 10347, training loss: 6540.98, average training loss: 7297.50, base loss: 16113.39
[INFO 2017-06-29 09:09:53,703 main.py:57] epoch 10348, training loss: 8441.33, average training loss: 7298.81, base loss: 16114.21
[INFO 2017-06-29 09:09:56,754 main.py:57] epoch 10349, training loss: 6889.04, average training loss: 7298.58, base loss: 16114.34
[INFO 2017-06-29 09:09:59,788 main.py:57] epoch 10350, training loss: 7360.09, average training loss: 7299.21, base loss: 16114.67
[INFO 2017-06-29 09:10:02,894 main.py:57] epoch 10351, training loss: 6573.87, average training loss: 7299.26, base loss: 16114.42
[INFO 2017-06-29 09:10:05,913 main.py:57] epoch 10352, training loss: 8048.34, average training loss: 7300.24, base loss: 16114.58
[INFO 2017-06-29 09:10:09,096 main.py:57] epoch 10353, training loss: 7552.19, average training loss: 7301.49, base loss: 16114.81
[INFO 2017-06-29 09:10:12,169 main.py:57] epoch 10354, training loss: 7702.70, average training loss: 7302.34, base loss: 16114.72
[INFO 2017-06-29 09:10:15,176 main.py:57] epoch 10355, training loss: 8084.68, average training loss: 7302.69, base loss: 16114.39
[INFO 2017-06-29 09:10:18,225 main.py:57] epoch 10356, training loss: 7083.78, average training loss: 7302.45, base loss: 16114.37
[INFO 2017-06-29 09:10:21,278 main.py:57] epoch 10357, training loss: 7347.05, average training loss: 7301.99, base loss: 16114.56
[INFO 2017-06-29 09:10:24,377 main.py:57] epoch 10358, training loss: 7062.97, average training loss: 7301.64, base loss: 16114.52
[INFO 2017-06-29 09:10:27,413 main.py:57] epoch 10359, training loss: 8467.15, average training loss: 7303.19, base loss: 16115.04
[INFO 2017-06-29 09:10:30,453 main.py:57] epoch 10360, training loss: 6940.75, average training loss: 7303.62, base loss: 16114.96
[INFO 2017-06-29 09:10:33,468 main.py:57] epoch 10361, training loss: 6774.55, average training loss: 7302.93, base loss: 16114.84
[INFO 2017-06-29 09:10:36,573 main.py:57] epoch 10362, training loss: 7042.09, average training loss: 7303.38, base loss: 16114.66
[INFO 2017-06-29 09:10:39,610 main.py:57] epoch 10363, training loss: 7335.60, average training loss: 7303.34, base loss: 16114.73
[INFO 2017-06-29 09:10:42,707 main.py:57] epoch 10364, training loss: 9122.18, average training loss: 7304.80, base loss: 16115.38
[INFO 2017-06-29 09:10:45,767 main.py:57] epoch 10365, training loss: 6557.19, average training loss: 7303.26, base loss: 16115.05
[INFO 2017-06-29 09:10:48,739 main.py:57] epoch 10366, training loss: 6640.69, average training loss: 7302.74, base loss: 16114.63
[INFO 2017-06-29 09:10:51,750 main.py:57] epoch 10367, training loss: 6793.94, average training loss: 7302.22, base loss: 16114.47
[INFO 2017-06-29 09:10:54,769 main.py:57] epoch 10368, training loss: 7684.89, average training loss: 7302.44, base loss: 16114.96
[INFO 2017-06-29 09:10:57,798 main.py:57] epoch 10369, training loss: 7197.18, average training loss: 7302.21, base loss: 16115.17
[INFO 2017-06-29 09:11:00,843 main.py:57] epoch 10370, training loss: 7623.99, average training loss: 7302.68, base loss: 16115.13
[INFO 2017-06-29 09:11:03,898 main.py:57] epoch 10371, training loss: 7019.12, average training loss: 7303.27, base loss: 16115.21
[INFO 2017-06-29 09:11:06,917 main.py:57] epoch 10372, training loss: 6927.56, average training loss: 7302.90, base loss: 16115.00
[INFO 2017-06-29 09:11:09,947 main.py:57] epoch 10373, training loss: 7081.51, average training loss: 7302.72, base loss: 16115.01
[INFO 2017-06-29 09:11:12,987 main.py:57] epoch 10374, training loss: 6553.59, average training loss: 7301.40, base loss: 16114.61
[INFO 2017-06-29 09:11:16,061 main.py:57] epoch 10375, training loss: 7485.34, average training loss: 7302.55, base loss: 16114.60
[INFO 2017-06-29 09:11:19,126 main.py:57] epoch 10376, training loss: 6925.77, average training loss: 7302.81, base loss: 16114.70
[INFO 2017-06-29 09:11:22,148 main.py:57] epoch 10377, training loss: 8014.92, average training loss: 7303.46, base loss: 16115.17
[INFO 2017-06-29 09:11:25,297 main.py:57] epoch 10378, training loss: 8136.17, average training loss: 7304.78, base loss: 16115.48
[INFO 2017-06-29 09:11:28,324 main.py:57] epoch 10379, training loss: 6814.99, average training loss: 7302.92, base loss: 16115.32
[INFO 2017-06-29 09:11:31,331 main.py:57] epoch 10380, training loss: 8086.85, average training loss: 7303.67, base loss: 16115.76
[INFO 2017-06-29 09:11:34,358 main.py:57] epoch 10381, training loss: 7463.84, average training loss: 7302.86, base loss: 16115.64
[INFO 2017-06-29 09:11:37,400 main.py:57] epoch 10382, training loss: 6964.02, average training loss: 7301.84, base loss: 16115.61
[INFO 2017-06-29 09:11:40,424 main.py:57] epoch 10383, training loss: 6998.83, average training loss: 7301.95, base loss: 16115.68
[INFO 2017-06-29 09:11:43,434 main.py:57] epoch 10384, training loss: 7221.81, average training loss: 7300.96, base loss: 16115.61
[INFO 2017-06-29 09:11:46,449 main.py:57] epoch 10385, training loss: 7226.38, average training loss: 7301.35, base loss: 16115.80
[INFO 2017-06-29 09:11:49,524 main.py:57] epoch 10386, training loss: 7185.11, average training loss: 7301.44, base loss: 16115.80
[INFO 2017-06-29 09:11:52,577 main.py:57] epoch 10387, training loss: 6604.43, average training loss: 7300.66, base loss: 16115.68
[INFO 2017-06-29 09:11:55,640 main.py:57] epoch 10388, training loss: 7590.21, average training loss: 7301.57, base loss: 16115.95
[INFO 2017-06-29 09:11:58,702 main.py:57] epoch 10389, training loss: 7425.70, average training loss: 7302.39, base loss: 16116.19
[INFO 2017-06-29 09:12:01,737 main.py:57] epoch 10390, training loss: 6882.12, average training loss: 7301.83, base loss: 16116.32
[INFO 2017-06-29 09:12:04,820 main.py:57] epoch 10391, training loss: 6833.92, average training loss: 7300.70, base loss: 16116.39
[INFO 2017-06-29 09:12:07,814 main.py:57] epoch 10392, training loss: 6775.48, average training loss: 7299.71, base loss: 16116.38
[INFO 2017-06-29 09:12:10,903 main.py:57] epoch 10393, training loss: 8141.04, average training loss: 7300.40, base loss: 16116.99
[INFO 2017-06-29 09:12:13,932 main.py:57] epoch 10394, training loss: 6755.39, average training loss: 7299.94, base loss: 16117.04
[INFO 2017-06-29 09:12:17,129 main.py:57] epoch 10395, training loss: 7832.76, average training loss: 7300.54, base loss: 16117.36
[INFO 2017-06-29 09:12:20,200 main.py:57] epoch 10396, training loss: 6076.21, average training loss: 7299.60, base loss: 16117.00
[INFO 2017-06-29 09:12:23,221 main.py:57] epoch 10397, training loss: 8108.15, average training loss: 7299.37, base loss: 16117.00
[INFO 2017-06-29 09:12:26,298 main.py:57] epoch 10398, training loss: 7378.56, average training loss: 7299.84, base loss: 16116.95
[INFO 2017-06-29 09:12:29,408 main.py:57] epoch 10399, training loss: 6487.48, average training loss: 7299.98, base loss: 16116.54
[INFO 2017-06-29 09:12:29,409 main.py:59] epoch 10399, testing
[INFO 2017-06-29 09:12:41,975 main.py:104] average testing loss: 7586.53, base loss: 15827.37
[INFO 2017-06-29 09:12:41,975 main.py:105] improve_loss: 8240.84, improve_percent: 0.52
[INFO 2017-06-29 09:12:41,976 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:12:45,034 main.py:57] epoch 10400, training loss: 7008.66, average training loss: 7299.22, base loss: 16116.40
[INFO 2017-06-29 09:12:48,133 main.py:57] epoch 10401, training loss: 6777.65, average training loss: 7298.64, base loss: 16116.31
[INFO 2017-06-29 09:12:51,163 main.py:57] epoch 10402, training loss: 6373.38, average training loss: 7299.02, base loss: 16115.84
[INFO 2017-06-29 09:12:54,170 main.py:57] epoch 10403, training loss: 6491.35, average training loss: 7298.83, base loss: 16115.58
[INFO 2017-06-29 09:12:57,225 main.py:57] epoch 10404, training loss: 6671.37, average training loss: 7298.96, base loss: 16115.56
[INFO 2017-06-29 09:13:00,388 main.py:57] epoch 10405, training loss: 8054.98, average training loss: 7300.37, base loss: 16115.64
[INFO 2017-06-29 09:13:03,371 main.py:57] epoch 10406, training loss: 6943.26, average training loss: 7299.67, base loss: 16115.19
[INFO 2017-06-29 09:13:06,410 main.py:57] epoch 10407, training loss: 6202.72, average training loss: 7299.34, base loss: 16114.63
[INFO 2017-06-29 09:13:09,417 main.py:57] epoch 10408, training loss: 7316.78, average training loss: 7299.10, base loss: 16114.62
[INFO 2017-06-29 09:13:12,467 main.py:57] epoch 10409, training loss: 7698.42, average training loss: 7299.38, base loss: 16114.61
[INFO 2017-06-29 09:13:15,536 main.py:57] epoch 10410, training loss: 7067.23, average training loss: 7298.75, base loss: 16114.61
[INFO 2017-06-29 09:13:18,623 main.py:57] epoch 10411, training loss: 7846.48, average training loss: 7299.55, base loss: 16114.86
[INFO 2017-06-29 09:13:21,624 main.py:57] epoch 10412, training loss: 8078.10, average training loss: 7300.66, base loss: 16115.88
[INFO 2017-06-29 09:13:24,721 main.py:57] epoch 10413, training loss: 7298.08, average training loss: 7300.02, base loss: 16116.09
[INFO 2017-06-29 09:13:27,787 main.py:57] epoch 10414, training loss: 7005.07, average training loss: 7300.70, base loss: 16116.06
[INFO 2017-06-29 09:13:30,802 main.py:57] epoch 10415, training loss: 6499.56, average training loss: 7300.25, base loss: 16115.51
[INFO 2017-06-29 09:13:34,053 main.py:57] epoch 10416, training loss: 7225.94, average training loss: 7300.00, base loss: 16115.37
[INFO 2017-06-29 09:13:37,061 main.py:57] epoch 10417, training loss: 6016.57, average training loss: 7298.24, base loss: 16115.01
[INFO 2017-06-29 09:13:40,165 main.py:57] epoch 10418, training loss: 7968.84, average training loss: 7299.01, base loss: 16115.08
[INFO 2017-06-29 09:13:43,294 main.py:57] epoch 10419, training loss: 6537.16, average training loss: 7297.37, base loss: 16114.66
[INFO 2017-06-29 09:13:46,327 main.py:57] epoch 10420, training loss: 7049.57, average training loss: 7297.02, base loss: 16114.79
[INFO 2017-06-29 09:13:49,356 main.py:57] epoch 10421, training loss: 6747.59, average training loss: 7296.94, base loss: 16114.47
[INFO 2017-06-29 09:13:52,415 main.py:57] epoch 10422, training loss: 7428.90, average training loss: 7297.38, base loss: 16114.05
[INFO 2017-06-29 09:13:55,505 main.py:57] epoch 10423, training loss: 7896.77, average training loss: 7298.52, base loss: 16114.16
[INFO 2017-06-29 09:13:58,568 main.py:57] epoch 10424, training loss: 6797.37, average training loss: 7297.57, base loss: 16114.12
[INFO 2017-06-29 09:14:01,680 main.py:57] epoch 10425, training loss: 7540.64, average training loss: 7297.39, base loss: 16114.21
[INFO 2017-06-29 09:14:04,740 main.py:57] epoch 10426, training loss: 6755.73, average training loss: 7296.77, base loss: 16114.13
[INFO 2017-06-29 09:14:07,852 main.py:57] epoch 10427, training loss: 7504.72, average training loss: 7297.80, base loss: 16114.11
[INFO 2017-06-29 09:14:10,905 main.py:57] epoch 10428, training loss: 7947.32, average training loss: 7297.93, base loss: 16114.25
[INFO 2017-06-29 09:14:13,881 main.py:57] epoch 10429, training loss: 7422.91, average training loss: 7298.88, base loss: 16114.26
[INFO 2017-06-29 09:14:16,906 main.py:57] epoch 10430, training loss: 6903.09, average training loss: 7298.89, base loss: 16114.44
[INFO 2017-06-29 09:14:19,949 main.py:57] epoch 10431, training loss: 7147.11, average training loss: 7298.09, base loss: 16114.34
[INFO 2017-06-29 09:14:22,962 main.py:57] epoch 10432, training loss: 7795.17, average training loss: 7298.96, base loss: 16114.47
[INFO 2017-06-29 09:14:26,046 main.py:57] epoch 10433, training loss: 7113.73, average training loss: 7298.07, base loss: 16114.23
[INFO 2017-06-29 09:14:29,098 main.py:57] epoch 10434, training loss: 8057.66, average training loss: 7299.03, base loss: 16114.28
[INFO 2017-06-29 09:14:32,196 main.py:57] epoch 10435, training loss: 7473.11, average training loss: 7298.94, base loss: 16114.45
[INFO 2017-06-29 09:14:35,391 main.py:57] epoch 10436, training loss: 8067.56, average training loss: 7299.62, base loss: 16114.79
[INFO 2017-06-29 09:14:38,490 main.py:57] epoch 10437, training loss: 7314.04, average training loss: 7299.18, base loss: 16114.75
[INFO 2017-06-29 09:14:41,552 main.py:57] epoch 10438, training loss: 8019.68, average training loss: 7300.07, base loss: 16114.99
[INFO 2017-06-29 09:14:44,577 main.py:57] epoch 10439, training loss: 8217.02, average training loss: 7300.47, base loss: 16115.51
[INFO 2017-06-29 09:14:47,648 main.py:57] epoch 10440, training loss: 6798.52, average training loss: 7299.92, base loss: 16115.46
[INFO 2017-06-29 09:14:50,730 main.py:57] epoch 10441, training loss: 6586.65, average training loss: 7299.39, base loss: 16115.30
[INFO 2017-06-29 09:14:53,764 main.py:57] epoch 10442, training loss: 6482.87, average training loss: 7299.00, base loss: 16114.76
[INFO 2017-06-29 09:14:56,830 main.py:57] epoch 10443, training loss: 7805.86, average training loss: 7299.23, base loss: 16114.78
[INFO 2017-06-29 09:14:59,868 main.py:57] epoch 10444, training loss: 7854.97, average training loss: 7298.93, base loss: 16115.02
[INFO 2017-06-29 09:15:02,947 main.py:57] epoch 10445, training loss: 7193.67, average training loss: 7298.76, base loss: 16115.11
[INFO 2017-06-29 09:15:06,012 main.py:57] epoch 10446, training loss: 7627.49, average training loss: 7298.88, base loss: 16115.52
[INFO 2017-06-29 09:15:09,027 main.py:57] epoch 10447, training loss: 7121.05, average training loss: 7299.52, base loss: 16115.68
[INFO 2017-06-29 09:15:12,104 main.py:57] epoch 10448, training loss: 6437.88, average training loss: 7299.06, base loss: 16115.32
[INFO 2017-06-29 09:15:15,136 main.py:57] epoch 10449, training loss: 7616.60, average training loss: 7300.12, base loss: 16115.08
[INFO 2017-06-29 09:15:18,246 main.py:57] epoch 10450, training loss: 6708.17, average training loss: 7298.63, base loss: 16114.81
[INFO 2017-06-29 09:15:21,225 main.py:57] epoch 10451, training loss: 7700.48, average training loss: 7298.80, base loss: 16115.10
[INFO 2017-06-29 09:15:24,278 main.py:57] epoch 10452, training loss: 6692.90, average training loss: 7299.00, base loss: 16114.91
[INFO 2017-06-29 09:15:27,303 main.py:57] epoch 10453, training loss: 7102.82, average training loss: 7299.77, base loss: 16114.81
[INFO 2017-06-29 09:15:30,388 main.py:57] epoch 10454, training loss: 6751.80, average training loss: 7298.93, base loss: 16114.39
[INFO 2017-06-29 09:15:33,440 main.py:57] epoch 10455, training loss: 8326.55, average training loss: 7300.35, base loss: 16114.47
[INFO 2017-06-29 09:15:36,496 main.py:57] epoch 10456, training loss: 7571.19, average training loss: 7299.97, base loss: 16114.23
[INFO 2017-06-29 09:15:39,557 main.py:57] epoch 10457, training loss: 6795.56, average training loss: 7298.70, base loss: 16113.94
[INFO 2017-06-29 09:15:42,634 main.py:57] epoch 10458, training loss: 8036.60, average training loss: 7299.46, base loss: 16114.22
[INFO 2017-06-29 09:15:45,658 main.py:57] epoch 10459, training loss: 7680.31, average training loss: 7298.59, base loss: 16114.29
[INFO 2017-06-29 09:15:48,837 main.py:57] epoch 10460, training loss: 7243.33, average training loss: 7298.50, base loss: 16114.44
[INFO 2017-06-29 09:15:51,876 main.py:57] epoch 10461, training loss: 6700.21, average training loss: 7298.22, base loss: 16114.37
[INFO 2017-06-29 09:15:54,992 main.py:57] epoch 10462, training loss: 6154.63, average training loss: 7296.60, base loss: 16113.79
[INFO 2017-06-29 09:15:58,032 main.py:57] epoch 10463, training loss: 8212.71, average training loss: 7297.37, base loss: 16113.95
[INFO 2017-06-29 09:16:01,054 main.py:57] epoch 10464, training loss: 6992.96, average training loss: 7296.85, base loss: 16113.91
[INFO 2017-06-29 09:16:04,129 main.py:57] epoch 10465, training loss: 6668.42, average training loss: 7296.83, base loss: 16113.68
[INFO 2017-06-29 09:16:07,208 main.py:57] epoch 10466, training loss: 7428.77, average training loss: 7296.94, base loss: 16114.14
[INFO 2017-06-29 09:16:10,269 main.py:57] epoch 10467, training loss: 7910.81, average training loss: 7297.28, base loss: 16114.17
[INFO 2017-06-29 09:16:13,281 main.py:57] epoch 10468, training loss: 7168.15, average training loss: 7297.17, base loss: 16113.76
[INFO 2017-06-29 09:16:16,345 main.py:57] epoch 10469, training loss: 9466.47, average training loss: 7299.31, base loss: 16114.09
[INFO 2017-06-29 09:16:19,403 main.py:57] epoch 10470, training loss: 7458.60, average training loss: 7297.35, base loss: 16114.37
[INFO 2017-06-29 09:16:22,435 main.py:57] epoch 10471, training loss: 7703.56, average training loss: 7298.27, base loss: 16114.75
[INFO 2017-06-29 09:16:25,458 main.py:57] epoch 10472, training loss: 6877.73, average training loss: 7297.83, base loss: 16114.88
[INFO 2017-06-29 09:16:28,516 main.py:57] epoch 10473, training loss: 6314.76, average training loss: 7296.85, base loss: 16114.92
[INFO 2017-06-29 09:16:31,550 main.py:57] epoch 10474, training loss: 8148.80, average training loss: 7297.57, base loss: 16115.09
[INFO 2017-06-29 09:16:34,581 main.py:57] epoch 10475, training loss: 6954.07, average training loss: 7297.37, base loss: 16114.86
[INFO 2017-06-29 09:16:37,653 main.py:57] epoch 10476, training loss: 6710.48, average training loss: 7296.43, base loss: 16114.98
[INFO 2017-06-29 09:16:40,779 main.py:57] epoch 10477, training loss: 7952.51, average training loss: 7297.10, base loss: 16115.31
[INFO 2017-06-29 09:16:43,825 main.py:57] epoch 10478, training loss: 7898.65, average training loss: 7297.90, base loss: 16115.54
[INFO 2017-06-29 09:16:46,839 main.py:57] epoch 10479, training loss: 8442.25, average training loss: 7299.83, base loss: 16116.06
[INFO 2017-06-29 09:16:49,904 main.py:57] epoch 10480, training loss: 7046.22, average training loss: 7299.22, base loss: 16115.95
[INFO 2017-06-29 09:16:52,991 main.py:57] epoch 10481, training loss: 8000.42, average training loss: 7299.16, base loss: 16116.62
[INFO 2017-06-29 09:16:56,071 main.py:57] epoch 10482, training loss: 9156.78, average training loss: 7301.42, base loss: 16117.19
[INFO 2017-06-29 09:16:59,129 main.py:57] epoch 10483, training loss: 7789.91, average training loss: 7301.57, base loss: 16117.54
[INFO 2017-06-29 09:17:02,212 main.py:57] epoch 10484, training loss: 7760.62, average training loss: 7302.28, base loss: 16117.61
[INFO 2017-06-29 09:17:05,347 main.py:57] epoch 10485, training loss: 8076.10, average training loss: 7303.40, base loss: 16118.02
[INFO 2017-06-29 09:17:08,356 main.py:57] epoch 10486, training loss: 7317.12, average training loss: 7303.78, base loss: 16117.80
[INFO 2017-06-29 09:17:11,423 main.py:57] epoch 10487, training loss: 6420.06, average training loss: 7303.16, base loss: 16117.44
[INFO 2017-06-29 09:17:14,545 main.py:57] epoch 10488, training loss: 6939.71, average training loss: 7303.10, base loss: 16117.19
[INFO 2017-06-29 09:17:17,615 main.py:57] epoch 10489, training loss: 7705.44, average training loss: 7303.05, base loss: 16117.30
[INFO 2017-06-29 09:17:20,730 main.py:57] epoch 10490, training loss: 7239.93, average training loss: 7302.75, base loss: 16117.30
[INFO 2017-06-29 09:17:23,764 main.py:57] epoch 10491, training loss: 8123.77, average training loss: 7304.60, base loss: 16117.58
[INFO 2017-06-29 09:17:26,830 main.py:57] epoch 10492, training loss: 8651.15, average training loss: 7305.54, base loss: 16118.17
[INFO 2017-06-29 09:17:29,929 main.py:57] epoch 10493, training loss: 7644.74, average training loss: 7305.91, base loss: 16118.32
[INFO 2017-06-29 09:17:33,144 main.py:57] epoch 10494, training loss: 6447.73, average training loss: 7305.09, base loss: 16117.90
[INFO 2017-06-29 09:17:36,236 main.py:57] epoch 10495, training loss: 6940.10, average training loss: 7305.62, base loss: 16117.81
[INFO 2017-06-29 09:17:39,314 main.py:57] epoch 10496, training loss: 6441.65, average training loss: 7304.53, base loss: 16117.46
[INFO 2017-06-29 09:17:42,357 main.py:57] epoch 10497, training loss: 6676.36, average training loss: 7303.64, base loss: 16117.25
[INFO 2017-06-29 09:17:45,449 main.py:57] epoch 10498, training loss: 7281.37, average training loss: 7304.24, base loss: 16117.49
[INFO 2017-06-29 09:17:48,499 main.py:57] epoch 10499, training loss: 6128.53, average training loss: 7302.91, base loss: 16117.14
[INFO 2017-06-29 09:17:48,500 main.py:59] epoch 10499, testing
[INFO 2017-06-29 09:18:01,334 main.py:104] average testing loss: 8195.10, base loss: 16861.52
[INFO 2017-06-29 09:18:01,334 main.py:105] improve_loss: 8666.42, improve_percent: 0.51
[INFO 2017-06-29 09:18:01,336 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:18:04,386 main.py:57] epoch 10500, training loss: 6782.78, average training loss: 7302.83, base loss: 16117.27
[INFO 2017-06-29 09:18:07,455 main.py:57] epoch 10501, training loss: 6350.17, average training loss: 7301.92, base loss: 16117.13
[INFO 2017-06-29 09:18:10,512 main.py:57] epoch 10502, training loss: 7153.53, average training loss: 7301.97, base loss: 16117.12
[INFO 2017-06-29 09:18:13,602 main.py:57] epoch 10503, training loss: 8045.70, average training loss: 7302.76, base loss: 16117.27
[INFO 2017-06-29 09:18:16,709 main.py:57] epoch 10504, training loss: 6716.85, average training loss: 7301.63, base loss: 16116.72
[INFO 2017-06-29 09:18:19,840 main.py:57] epoch 10505, training loss: 8018.47, average training loss: 7301.50, base loss: 16116.61
[INFO 2017-06-29 09:18:22,852 main.py:57] epoch 10506, training loss: 7072.15, average training loss: 7301.92, base loss: 16116.65
[INFO 2017-06-29 09:18:25,918 main.py:57] epoch 10507, training loss: 7776.82, average training loss: 7301.89, base loss: 16116.89
[INFO 2017-06-29 09:18:28,961 main.py:57] epoch 10508, training loss: 7480.95, average training loss: 7301.72, base loss: 16117.29
[INFO 2017-06-29 09:18:32,033 main.py:57] epoch 10509, training loss: 7669.62, average training loss: 7301.48, base loss: 16117.67
[INFO 2017-06-29 09:18:35,112 main.py:57] epoch 10510, training loss: 7272.66, average training loss: 7301.00, base loss: 16117.90
[INFO 2017-06-29 09:18:38,191 main.py:57] epoch 10511, training loss: 7613.36, average training loss: 7301.14, base loss: 16118.17
[INFO 2017-06-29 09:18:41,219 main.py:57] epoch 10512, training loss: 6162.12, average training loss: 7300.51, base loss: 16118.02
[INFO 2017-06-29 09:18:44,293 main.py:57] epoch 10513, training loss: 7250.24, average training loss: 7299.83, base loss: 16118.15
[INFO 2017-06-29 09:18:47,367 main.py:57] epoch 10514, training loss: 6864.75, average training loss: 7299.97, base loss: 16118.23
[INFO 2017-06-29 09:18:50,367 main.py:57] epoch 10515, training loss: 7653.80, average training loss: 7300.75, base loss: 16118.47
[INFO 2017-06-29 09:18:53,396 main.py:57] epoch 10516, training loss: 7331.68, average training loss: 7301.03, base loss: 16118.26
[INFO 2017-06-29 09:18:56,449 main.py:57] epoch 10517, training loss: 6742.02, average training loss: 7301.12, base loss: 16117.77
[INFO 2017-06-29 09:18:59,476 main.py:57] epoch 10518, training loss: 7726.86, average training loss: 7301.25, base loss: 16117.93
[INFO 2017-06-29 09:19:02,479 main.py:57] epoch 10519, training loss: 5747.07, average training loss: 7298.83, base loss: 16117.10
[INFO 2017-06-29 09:19:05,497 main.py:57] epoch 10520, training loss: 7013.09, average training loss: 7298.85, base loss: 16117.22
[INFO 2017-06-29 09:19:08,541 main.py:57] epoch 10521, training loss: 7463.49, average training loss: 7298.84, base loss: 16117.47
[INFO 2017-06-29 09:19:11,634 main.py:57] epoch 10522, training loss: 6655.53, average training loss: 7298.74, base loss: 16117.22
[INFO 2017-06-29 09:19:14,650 main.py:57] epoch 10523, training loss: 6031.59, average training loss: 7297.82, base loss: 16116.80
[INFO 2017-06-29 09:19:17,740 main.py:57] epoch 10524, training loss: 7544.01, average training loss: 7298.23, base loss: 16116.44
[INFO 2017-06-29 09:19:20,782 main.py:57] epoch 10525, training loss: 7910.16, average training loss: 7298.86, base loss: 16116.31
[INFO 2017-06-29 09:19:23,765 main.py:57] epoch 10526, training loss: 8151.93, average training loss: 7299.97, base loss: 16116.43
[INFO 2017-06-29 09:19:26,864 main.py:57] epoch 10527, training loss: 7725.89, average training loss: 7300.98, base loss: 16116.42
[INFO 2017-06-29 09:19:29,988 main.py:57] epoch 10528, training loss: 7290.23, average training loss: 7301.43, base loss: 16116.67
[INFO 2017-06-29 09:19:33,000 main.py:57] epoch 10529, training loss: 7620.08, average training loss: 7301.06, base loss: 16117.20
[INFO 2017-06-29 09:19:36,078 main.py:57] epoch 10530, training loss: 7354.00, average training loss: 7300.53, base loss: 16117.34
[INFO 2017-06-29 09:19:39,108 main.py:57] epoch 10531, training loss: 7329.41, average training loss: 7300.80, base loss: 16117.45
[INFO 2017-06-29 09:19:42,171 main.py:57] epoch 10532, training loss: 6761.72, average training loss: 7299.87, base loss: 16117.18
[INFO 2017-06-29 09:19:45,213 main.py:57] epoch 10533, training loss: 6774.59, average training loss: 7299.59, base loss: 16116.94
[INFO 2017-06-29 09:19:48,199 main.py:57] epoch 10534, training loss: 7228.90, average training loss: 7299.16, base loss: 16116.76
[INFO 2017-06-29 09:19:51,198 main.py:57] epoch 10535, training loss: 6464.91, average training loss: 7298.95, base loss: 16116.38
[INFO 2017-06-29 09:19:54,237 main.py:57] epoch 10536, training loss: 6938.69, average training loss: 7299.17, base loss: 16116.30
[INFO 2017-06-29 09:19:57,265 main.py:57] epoch 10537, training loss: 7401.48, average training loss: 7297.80, base loss: 16116.73
[INFO 2017-06-29 09:20:00,276 main.py:57] epoch 10538, training loss: 7257.05, average training loss: 7298.41, base loss: 16116.77
[INFO 2017-06-29 09:20:03,340 main.py:57] epoch 10539, training loss: 6883.32, average training loss: 7298.01, base loss: 16116.55
[INFO 2017-06-29 09:20:06,461 main.py:57] epoch 10540, training loss: 6897.74, average training loss: 7298.33, base loss: 16116.50
[INFO 2017-06-29 09:20:09,538 main.py:57] epoch 10541, training loss: 7441.97, average training loss: 7299.19, base loss: 16116.56
[INFO 2017-06-29 09:20:12,622 main.py:57] epoch 10542, training loss: 7081.68, average training loss: 7299.23, base loss: 16116.42
[INFO 2017-06-29 09:20:15,705 main.py:57] epoch 10543, training loss: 6728.75, average training loss: 7297.28, base loss: 16116.11
[INFO 2017-06-29 09:20:18,703 main.py:57] epoch 10544, training loss: 6885.62, average training loss: 7296.00, base loss: 16115.72
[INFO 2017-06-29 09:20:21,731 main.py:57] epoch 10545, training loss: 7199.91, average training loss: 7295.33, base loss: 16115.61
[INFO 2017-06-29 09:20:24,757 main.py:57] epoch 10546, training loss: 6619.10, average training loss: 7295.07, base loss: 16115.38
[INFO 2017-06-29 09:20:27,820 main.py:57] epoch 10547, training loss: 7239.44, average training loss: 7295.11, base loss: 16115.41
[INFO 2017-06-29 09:20:30,849 main.py:57] epoch 10548, training loss: 6804.85, average training loss: 7294.02, base loss: 16115.04
[INFO 2017-06-29 09:20:33,902 main.py:57] epoch 10549, training loss: 7167.13, average training loss: 7294.70, base loss: 16114.87
[INFO 2017-06-29 09:20:36,923 main.py:57] epoch 10550, training loss: 7260.25, average training loss: 7294.04, base loss: 16114.94
[INFO 2017-06-29 09:20:40,007 main.py:57] epoch 10551, training loss: 7703.09, average training loss: 7295.00, base loss: 16114.89
[INFO 2017-06-29 09:20:43,016 main.py:57] epoch 10552, training loss: 7607.71, average training loss: 7295.22, base loss: 16115.40
[INFO 2017-06-29 09:20:46,125 main.py:57] epoch 10553, training loss: 6760.52, average training loss: 7293.99, base loss: 16115.31
[INFO 2017-06-29 09:20:49,182 main.py:57] epoch 10554, training loss: 7806.42, average training loss: 7292.84, base loss: 16115.50
[INFO 2017-06-29 09:20:52,305 main.py:57] epoch 10555, training loss: 6475.45, average training loss: 7291.59, base loss: 16115.26
[INFO 2017-06-29 09:20:55,342 main.py:57] epoch 10556, training loss: 7105.78, average training loss: 7291.30, base loss: 16114.84
[INFO 2017-06-29 09:20:58,476 main.py:57] epoch 10557, training loss: 7012.92, average training loss: 7290.86, base loss: 16114.37
[INFO 2017-06-29 09:21:01,517 main.py:57] epoch 10558, training loss: 7075.86, average training loss: 7291.15, base loss: 16114.46
[INFO 2017-06-29 09:21:04,585 main.py:57] epoch 10559, training loss: 7920.17, average training loss: 7291.29, base loss: 16114.80
[INFO 2017-06-29 09:21:07,658 main.py:57] epoch 10560, training loss: 7917.81, average training loss: 7291.53, base loss: 16115.18
[INFO 2017-06-29 09:21:10,739 main.py:57] epoch 10561, training loss: 6615.30, average training loss: 7289.65, base loss: 16115.19
[INFO 2017-06-29 09:21:13,779 main.py:57] epoch 10562, training loss: 6843.82, average training loss: 7288.54, base loss: 16114.73
[INFO 2017-06-29 09:21:16,808 main.py:57] epoch 10563, training loss: 7153.79, average training loss: 7287.54, base loss: 16114.54
[INFO 2017-06-29 09:21:19,884 main.py:57] epoch 10564, training loss: 6795.14, average training loss: 7287.95, base loss: 16114.45
[INFO 2017-06-29 09:21:22,961 main.py:57] epoch 10565, training loss: 7536.34, average training loss: 7287.17, base loss: 16114.60
[INFO 2017-06-29 09:21:25,959 main.py:57] epoch 10566, training loss: 7503.27, average training loss: 7286.62, base loss: 16114.89
[INFO 2017-06-29 09:21:28,958 main.py:57] epoch 10567, training loss: 6767.88, average training loss: 7286.35, base loss: 16114.77
[INFO 2017-06-29 09:21:32,037 main.py:57] epoch 10568, training loss: 7175.93, average training loss: 7285.76, base loss: 16114.51
[INFO 2017-06-29 09:21:35,098 main.py:57] epoch 10569, training loss: 8095.84, average training loss: 7286.28, base loss: 16114.94
[INFO 2017-06-29 09:21:38,142 main.py:57] epoch 10570, training loss: 6806.81, average training loss: 7285.87, base loss: 16114.44
[INFO 2017-06-29 09:21:41,204 main.py:57] epoch 10571, training loss: 6898.68, average training loss: 7284.44, base loss: 16114.12
[INFO 2017-06-29 09:21:44,170 main.py:57] epoch 10572, training loss: 6855.83, average training loss: 7284.19, base loss: 16113.87
[INFO 2017-06-29 09:21:47,267 main.py:57] epoch 10573, training loss: 6823.46, average training loss: 7283.87, base loss: 16113.63
[INFO 2017-06-29 09:21:50,269 main.py:57] epoch 10574, training loss: 7169.60, average training loss: 7283.81, base loss: 16113.51
[INFO 2017-06-29 09:21:53,354 main.py:57] epoch 10575, training loss: 8060.06, average training loss: 7283.90, base loss: 16113.74
[INFO 2017-06-29 09:21:56,410 main.py:57] epoch 10576, training loss: 7065.83, average training loss: 7284.21, base loss: 16113.55
[INFO 2017-06-29 09:21:59,405 main.py:57] epoch 10577, training loss: 7046.61, average training loss: 7283.86, base loss: 16113.38
[INFO 2017-06-29 09:22:02,535 main.py:57] epoch 10578, training loss: 7111.35, average training loss: 7281.98, base loss: 16113.71
[INFO 2017-06-29 09:22:05,557 main.py:57] epoch 10579, training loss: 6096.06, average training loss: 7281.01, base loss: 16113.34
[INFO 2017-06-29 09:22:08,670 main.py:57] epoch 10580, training loss: 7806.75, average training loss: 7280.65, base loss: 16113.65
[INFO 2017-06-29 09:22:11,702 main.py:57] epoch 10581, training loss: 6359.10, average training loss: 7279.27, base loss: 16113.49
[INFO 2017-06-29 09:22:14,777 main.py:57] epoch 10582, training loss: 8169.30, average training loss: 7280.03, base loss: 16113.55
[INFO 2017-06-29 09:22:17,865 main.py:57] epoch 10583, training loss: 6399.34, average training loss: 7279.50, base loss: 16112.75
[INFO 2017-06-29 09:22:20,856 main.py:57] epoch 10584, training loss: 7755.71, average training loss: 7278.17, base loss: 16113.00
[INFO 2017-06-29 09:22:23,868 main.py:57] epoch 10585, training loss: 7626.69, average training loss: 7278.08, base loss: 16112.96
[INFO 2017-06-29 09:22:26,950 main.py:57] epoch 10586, training loss: 7496.11, average training loss: 7277.83, base loss: 16113.09
[INFO 2017-06-29 09:22:30,016 main.py:57] epoch 10587, training loss: 7268.82, average training loss: 7276.51, base loss: 16113.47
[INFO 2017-06-29 09:22:33,102 main.py:57] epoch 10588, training loss: 7531.08, average training loss: 7277.46, base loss: 16113.87
[INFO 2017-06-29 09:22:36,216 main.py:57] epoch 10589, training loss: 7177.57, average training loss: 7277.29, base loss: 16114.07
[INFO 2017-06-29 09:22:39,345 main.py:57] epoch 10590, training loss: 5864.32, average training loss: 7275.84, base loss: 16113.68
[INFO 2017-06-29 09:22:42,333 main.py:57] epoch 10591, training loss: 6430.51, average training loss: 7275.37, base loss: 16113.45
[INFO 2017-06-29 09:22:45,425 main.py:57] epoch 10592, training loss: 7879.96, average training loss: 7275.28, base loss: 16113.47
[INFO 2017-06-29 09:22:48,497 main.py:57] epoch 10593, training loss: 7929.41, average training loss: 7275.76, base loss: 16113.62
[INFO 2017-06-29 09:22:51,513 main.py:57] epoch 10594, training loss: 6980.93, average training loss: 7275.55, base loss: 16113.62
[INFO 2017-06-29 09:22:54,562 main.py:57] epoch 10595, training loss: 6727.59, average training loss: 7274.03, base loss: 16113.44
[INFO 2017-06-29 09:22:57,572 main.py:57] epoch 10596, training loss: 6514.30, average training loss: 7274.27, base loss: 16113.04
[INFO 2017-06-29 09:23:00,660 main.py:57] epoch 10597, training loss: 7689.45, average training loss: 7275.84, base loss: 16113.33
[INFO 2017-06-29 09:23:03,707 main.py:57] epoch 10598, training loss: 7741.96, average training loss: 7275.59, base loss: 16113.48
[INFO 2017-06-29 09:23:06,761 main.py:57] epoch 10599, training loss: 8044.97, average training loss: 7277.54, base loss: 16113.75
[INFO 2017-06-29 09:23:06,762 main.py:59] epoch 10599, testing
[INFO 2017-06-29 09:23:19,438 main.py:104] average testing loss: 8641.34, base loss: 17627.45
[INFO 2017-06-29 09:23:19,439 main.py:105] improve_loss: 8986.11, improve_percent: 0.51
[INFO 2017-06-29 09:23:19,440 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:23:22,488 main.py:57] epoch 10600, training loss: 7086.79, average training loss: 7277.46, base loss: 16113.60
[INFO 2017-06-29 09:23:25,532 main.py:57] epoch 10601, training loss: 7684.68, average training loss: 7278.28, base loss: 16113.76
[INFO 2017-06-29 09:23:28,595 main.py:57] epoch 10602, training loss: 7587.22, average training loss: 7278.81, base loss: 16113.78
[INFO 2017-06-29 09:23:31,786 main.py:57] epoch 10603, training loss: 7290.41, average training loss: 7278.89, base loss: 16113.93
[INFO 2017-06-29 09:23:34,846 main.py:57] epoch 10604, training loss: 6930.00, average training loss: 7279.09, base loss: 16113.98
[INFO 2017-06-29 09:23:37,862 main.py:57] epoch 10605, training loss: 7876.81, average training loss: 7279.96, base loss: 16114.19
[INFO 2017-06-29 09:23:40,922 main.py:57] epoch 10606, training loss: 6674.28, average training loss: 7279.73, base loss: 16113.94
[INFO 2017-06-29 09:23:43,984 main.py:57] epoch 10607, training loss: 9564.12, average training loss: 7280.88, base loss: 16115.03
[INFO 2017-06-29 09:23:47,013 main.py:57] epoch 10608, training loss: 7324.41, average training loss: 7281.84, base loss: 16115.11
[INFO 2017-06-29 09:23:50,129 main.py:57] epoch 10609, training loss: 7563.22, average training loss: 7282.62, base loss: 16115.13
[INFO 2017-06-29 09:23:53,166 main.py:57] epoch 10610, training loss: 7820.47, average training loss: 7283.09, base loss: 16115.38
[INFO 2017-06-29 09:23:56,271 main.py:57] epoch 10611, training loss: 7334.00, average training loss: 7283.68, base loss: 16115.50
[INFO 2017-06-29 09:23:59,348 main.py:57] epoch 10612, training loss: 7416.98, average training loss: 7284.61, base loss: 16115.45
[INFO 2017-06-29 09:24:02,335 main.py:57] epoch 10613, training loss: 7076.72, average training loss: 7283.21, base loss: 16115.41
[INFO 2017-06-29 09:24:05,429 main.py:57] epoch 10614, training loss: 7112.66, average training loss: 7282.19, base loss: 16114.88
[INFO 2017-06-29 09:24:08,503 main.py:57] epoch 10615, training loss: 6726.08, average training loss: 7281.47, base loss: 16114.28
[INFO 2017-06-29 09:24:11,510 main.py:57] epoch 10616, training loss: 7200.46, average training loss: 7280.39, base loss: 16114.56
[INFO 2017-06-29 09:24:14,588 main.py:57] epoch 10617, training loss: 6202.08, average training loss: 7279.93, base loss: 16114.06
[INFO 2017-06-29 09:24:17,661 main.py:57] epoch 10618, training loss: 7122.58, average training loss: 7278.49, base loss: 16113.79
[INFO 2017-06-29 09:24:20,743 main.py:57] epoch 10619, training loss: 6928.68, average training loss: 7278.35, base loss: 16113.68
[INFO 2017-06-29 09:24:23,838 main.py:57] epoch 10620, training loss: 8491.16, average training loss: 7280.18, base loss: 16113.92
[INFO 2017-06-29 09:24:26,886 main.py:57] epoch 10621, training loss: 7316.76, average training loss: 7280.75, base loss: 16113.69
[INFO 2017-06-29 09:24:29,928 main.py:57] epoch 10622, training loss: 7724.10, average training loss: 7281.17, base loss: 16113.98
[INFO 2017-06-29 09:24:32,974 main.py:57] epoch 10623, training loss: 6845.85, average training loss: 7280.55, base loss: 16113.82
[INFO 2017-06-29 09:24:35,969 main.py:57] epoch 10624, training loss: 6457.33, average training loss: 7280.08, base loss: 16113.79
[INFO 2017-06-29 09:24:39,035 main.py:57] epoch 10625, training loss: 6007.88, average training loss: 7278.89, base loss: 16113.44
[INFO 2017-06-29 09:24:42,096 main.py:57] epoch 10626, training loss: 6832.64, average training loss: 7278.17, base loss: 16112.87
[INFO 2017-06-29 09:24:45,121 main.py:57] epoch 10627, training loss: 6864.56, average training loss: 7278.03, base loss: 16112.62
[INFO 2017-06-29 09:24:48,158 main.py:57] epoch 10628, training loss: 7346.30, average training loss: 7277.83, base loss: 16112.59
[INFO 2017-06-29 09:24:51,317 main.py:57] epoch 10629, training loss: 7799.71, average training loss: 7277.69, base loss: 16113.16
[INFO 2017-06-29 09:24:54,333 main.py:57] epoch 10630, training loss: 8018.96, average training loss: 7278.74, base loss: 16113.36
[INFO 2017-06-29 09:24:57,416 main.py:57] epoch 10631, training loss: 7640.99, average training loss: 7278.67, base loss: 16113.23
[INFO 2017-06-29 09:25:00,431 main.py:57] epoch 10632, training loss: 7240.99, average training loss: 7278.47, base loss: 16113.24
[INFO 2017-06-29 09:25:03,444 main.py:57] epoch 10633, training loss: 7434.50, average training loss: 7279.38, base loss: 16113.16
[INFO 2017-06-29 09:25:06,525 main.py:57] epoch 10634, training loss: 6204.29, average training loss: 7278.18, base loss: 16112.91
[INFO 2017-06-29 09:25:09,597 main.py:57] epoch 10635, training loss: 7112.75, average training loss: 7277.47, base loss: 16112.87
[INFO 2017-06-29 09:25:12,666 main.py:57] epoch 10636, training loss: 7241.03, average training loss: 7278.33, base loss: 16112.94
[INFO 2017-06-29 09:25:15,714 main.py:57] epoch 10637, training loss: 7095.90, average training loss: 7277.70, base loss: 16112.89
[INFO 2017-06-29 09:25:18,791 main.py:57] epoch 10638, training loss: 7478.55, average training loss: 7277.61, base loss: 16113.20
[INFO 2017-06-29 09:25:21,904 main.py:57] epoch 10639, training loss: 7019.07, average training loss: 7277.94, base loss: 16113.02
[INFO 2017-06-29 09:25:24,968 main.py:57] epoch 10640, training loss: 7235.62, average training loss: 7277.32, base loss: 16113.11
[INFO 2017-06-29 09:25:28,005 main.py:57] epoch 10641, training loss: 7491.78, average training loss: 7277.43, base loss: 16113.16
[INFO 2017-06-29 09:25:31,043 main.py:57] epoch 10642, training loss: 6357.35, average training loss: 7276.51, base loss: 16112.42
[INFO 2017-06-29 09:25:34,098 main.py:57] epoch 10643, training loss: 7319.10, average training loss: 7276.90, base loss: 16112.21
[INFO 2017-06-29 09:25:37,188 main.py:57] epoch 10644, training loss: 6921.48, average training loss: 7277.17, base loss: 16111.99
[INFO 2017-06-29 09:25:40,210 main.py:57] epoch 10645, training loss: 9076.95, average training loss: 7279.66, base loss: 16112.46
[INFO 2017-06-29 09:25:43,268 main.py:57] epoch 10646, training loss: 7711.67, average training loss: 7279.40, base loss: 16112.43
[INFO 2017-06-29 09:25:46,304 main.py:57] epoch 10647, training loss: 8185.59, average training loss: 7280.83, base loss: 16112.41
[INFO 2017-06-29 09:25:49,392 main.py:57] epoch 10648, training loss: 6766.86, average training loss: 7280.69, base loss: 16112.12
[INFO 2017-06-29 09:25:52,422 main.py:57] epoch 10649, training loss: 7327.24, average training loss: 7280.93, base loss: 16112.08
[INFO 2017-06-29 09:25:55,507 main.py:57] epoch 10650, training loss: 8343.92, average training loss: 7281.09, base loss: 16112.41
[INFO 2017-06-29 09:25:58,534 main.py:57] epoch 10651, training loss: 7370.97, average training loss: 7281.56, base loss: 16112.85
[INFO 2017-06-29 09:26:01,598 main.py:57] epoch 10652, training loss: 7114.89, average training loss: 7282.20, base loss: 16112.84
[INFO 2017-06-29 09:26:04,627 main.py:57] epoch 10653, training loss: 7969.24, average training loss: 7283.29, base loss: 16112.92
[INFO 2017-06-29 09:26:07,687 main.py:57] epoch 10654, training loss: 7768.39, average training loss: 7283.66, base loss: 16113.04
[INFO 2017-06-29 09:26:10,724 main.py:57] epoch 10655, training loss: 6642.38, average training loss: 7282.92, base loss: 16112.85
[INFO 2017-06-29 09:26:13,752 main.py:57] epoch 10656, training loss: 7370.41, average training loss: 7283.43, base loss: 16113.10
[INFO 2017-06-29 09:26:16,796 main.py:57] epoch 10657, training loss: 8118.42, average training loss: 7284.46, base loss: 16113.43
[INFO 2017-06-29 09:26:19,821 main.py:57] epoch 10658, training loss: 7469.81, average training loss: 7283.78, base loss: 16113.37
[INFO 2017-06-29 09:26:22,886 main.py:57] epoch 10659, training loss: 7785.41, average training loss: 7284.62, base loss: 16113.52
[INFO 2017-06-29 09:26:25,898 main.py:57] epoch 10660, training loss: 6870.36, average training loss: 7284.85, base loss: 16113.33
[INFO 2017-06-29 09:26:28,919 main.py:57] epoch 10661, training loss: 6581.51, average training loss: 7284.32, base loss: 16113.22
[INFO 2017-06-29 09:26:32,012 main.py:57] epoch 10662, training loss: 7254.81, average training loss: 7284.52, base loss: 16113.18
[INFO 2017-06-29 09:26:34,978 main.py:57] epoch 10663, training loss: 8848.36, average training loss: 7286.97, base loss: 16113.70
[INFO 2017-06-29 09:26:38,066 main.py:57] epoch 10664, training loss: 7250.55, average training loss: 7286.86, base loss: 16113.72
[INFO 2017-06-29 09:26:41,099 main.py:57] epoch 10665, training loss: 7712.64, average training loss: 7287.33, base loss: 16113.86
[INFO 2017-06-29 09:26:44,211 main.py:57] epoch 10666, training loss: 7235.99, average training loss: 7287.48, base loss: 16113.78
[INFO 2017-06-29 09:26:47,232 main.py:57] epoch 10667, training loss: 6502.19, average training loss: 7285.82, base loss: 16113.33
[INFO 2017-06-29 09:26:50,300 main.py:57] epoch 10668, training loss: 7339.11, average training loss: 7284.90, base loss: 16113.27
[INFO 2017-06-29 09:26:53,359 main.py:57] epoch 10669, training loss: 6992.92, average training loss: 7284.62, base loss: 16113.09
[INFO 2017-06-29 09:26:56,347 main.py:57] epoch 10670, training loss: 7176.89, average training loss: 7285.28, base loss: 16113.09
[INFO 2017-06-29 09:26:59,386 main.py:57] epoch 10671, training loss: 7238.84, average training loss: 7284.71, base loss: 16113.25
[INFO 2017-06-29 09:27:02,432 main.py:57] epoch 10672, training loss: 6674.08, average training loss: 7283.43, base loss: 16113.12
[INFO 2017-06-29 09:27:05,461 main.py:57] epoch 10673, training loss: 6954.87, average training loss: 7283.64, base loss: 16113.26
[INFO 2017-06-29 09:27:08,562 main.py:57] epoch 10674, training loss: 7237.75, average training loss: 7283.11, base loss: 16113.33
[INFO 2017-06-29 09:27:11,636 main.py:57] epoch 10675, training loss: 6334.41, average training loss: 7282.49, base loss: 16112.94
[INFO 2017-06-29 09:27:14,819 main.py:57] epoch 10676, training loss: 7281.51, average training loss: 7282.67, base loss: 16113.27
[INFO 2017-06-29 09:27:17,928 main.py:57] epoch 10677, training loss: 7166.68, average training loss: 7283.43, base loss: 16113.31
[INFO 2017-06-29 09:27:20,987 main.py:57] epoch 10678, training loss: 6833.67, average training loss: 7283.33, base loss: 16113.00
[INFO 2017-06-29 09:27:23,998 main.py:57] epoch 10679, training loss: 7076.91, average training loss: 7283.09, base loss: 16112.79
[INFO 2017-06-29 09:27:27,046 main.py:57] epoch 10680, training loss: 7293.68, average training loss: 7283.37, base loss: 16112.77
[INFO 2017-06-29 09:27:30,091 main.py:57] epoch 10681, training loss: 7936.62, average training loss: 7284.43, base loss: 16112.97
[INFO 2017-06-29 09:27:33,075 main.py:57] epoch 10682, training loss: 7383.71, average training loss: 7285.25, base loss: 16113.18
[INFO 2017-06-29 09:27:36,182 main.py:57] epoch 10683, training loss: 6893.28, average training loss: 7284.15, base loss: 16113.21
[INFO 2017-06-29 09:27:39,303 main.py:57] epoch 10684, training loss: 7364.25, average training loss: 7284.66, base loss: 16113.13
[INFO 2017-06-29 09:27:42,323 main.py:57] epoch 10685, training loss: 6910.50, average training loss: 7284.23, base loss: 16112.77
[INFO 2017-06-29 09:27:45,368 main.py:57] epoch 10686, training loss: 6563.29, average training loss: 7283.50, base loss: 16112.59
[INFO 2017-06-29 09:27:48,478 main.py:57] epoch 10687, training loss: 7099.09, average training loss: 7283.50, base loss: 16112.47
[INFO 2017-06-29 09:27:51,490 main.py:57] epoch 10688, training loss: 6852.85, average training loss: 7284.42, base loss: 16112.36
[INFO 2017-06-29 09:27:54,538 main.py:57] epoch 10689, training loss: 6998.63, average training loss: 7284.33, base loss: 16112.19
[INFO 2017-06-29 09:27:57,642 main.py:57] epoch 10690, training loss: 6321.51, average training loss: 7283.44, base loss: 16111.99
[INFO 2017-06-29 09:28:00,680 main.py:57] epoch 10691, training loss: 6824.52, average training loss: 7283.37, base loss: 16111.84
[INFO 2017-06-29 09:28:03,726 main.py:57] epoch 10692, training loss: 7636.80, average training loss: 7284.54, base loss: 16111.91
[INFO 2017-06-29 09:28:06,731 main.py:57] epoch 10693, training loss: 6680.25, average training loss: 7283.79, base loss: 16111.46
[INFO 2017-06-29 09:28:09,807 main.py:57] epoch 10694, training loss: 6823.53, average training loss: 7283.60, base loss: 16111.44
[INFO 2017-06-29 09:28:12,931 main.py:57] epoch 10695, training loss: 7565.97, average training loss: 7284.48, base loss: 16111.65
[INFO 2017-06-29 09:28:16,052 main.py:57] epoch 10696, training loss: 7342.30, average training loss: 7284.12, base loss: 16111.87
[INFO 2017-06-29 09:28:19,103 main.py:57] epoch 10697, training loss: 7884.48, average training loss: 7284.52, base loss: 16112.34
[INFO 2017-06-29 09:28:22,150 main.py:57] epoch 10698, training loss: 8018.94, average training loss: 7284.64, base loss: 16112.57
[INFO 2017-06-29 09:28:25,197 main.py:57] epoch 10699, training loss: 6201.59, average training loss: 7283.80, base loss: 16112.67
[INFO 2017-06-29 09:28:25,197 main.py:59] epoch 10699, testing
[INFO 2017-06-29 09:28:37,870 main.py:104] average testing loss: 7773.50, base loss: 16412.73
[INFO 2017-06-29 09:28:37,870 main.py:105] improve_loss: 8639.22, improve_percent: 0.53
[INFO 2017-06-29 09:28:37,871 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:28:41,005 main.py:57] epoch 10700, training loss: 8240.90, average training loss: 7285.32, base loss: 16113.05
[INFO 2017-06-29 09:28:44,050 main.py:57] epoch 10701, training loss: 7604.92, average training loss: 7286.18, base loss: 16113.29
[INFO 2017-06-29 09:28:47,059 main.py:57] epoch 10702, training loss: 6847.06, average training loss: 7286.64, base loss: 16113.42
[INFO 2017-06-29 09:28:50,143 main.py:57] epoch 10703, training loss: 7065.60, average training loss: 7286.27, base loss: 16113.45
[INFO 2017-06-29 09:28:53,265 main.py:57] epoch 10704, training loss: 5869.43, average training loss: 7283.86, base loss: 16113.04
[INFO 2017-06-29 09:28:56,337 main.py:57] epoch 10705, training loss: 6532.71, average training loss: 7282.75, base loss: 16112.66
[INFO 2017-06-29 09:28:59,366 main.py:57] epoch 10706, training loss: 7183.91, average training loss: 7282.11, base loss: 16112.31
[INFO 2017-06-29 09:29:02,441 main.py:57] epoch 10707, training loss: 8277.68, average training loss: 7283.93, base loss: 16112.60
[INFO 2017-06-29 09:29:05,566 main.py:57] epoch 10708, training loss: 7112.23, average training loss: 7284.33, base loss: 16112.61
[INFO 2017-06-29 09:29:08,580 main.py:57] epoch 10709, training loss: 7460.61, average training loss: 7285.56, base loss: 16112.61
[INFO 2017-06-29 09:29:11,576 main.py:57] epoch 10710, training loss: 6864.29, average training loss: 7285.10, base loss: 16112.63
[INFO 2017-06-29 09:29:14,626 main.py:57] epoch 10711, training loss: 7425.73, average training loss: 7285.33, base loss: 16112.90
[INFO 2017-06-29 09:29:17,770 main.py:57] epoch 10712, training loss: 6748.88, average training loss: 7285.40, base loss: 16112.77
[INFO 2017-06-29 09:29:20,766 main.py:57] epoch 10713, training loss: 6722.46, average training loss: 7285.49, base loss: 16112.85
[INFO 2017-06-29 09:29:23,925 main.py:57] epoch 10714, training loss: 7058.45, average training loss: 7285.82, base loss: 16112.72
[INFO 2017-06-29 09:29:27,032 main.py:57] epoch 10715, training loss: 6659.03, average training loss: 7284.40, base loss: 16112.63
[INFO 2017-06-29 09:29:30,086 main.py:57] epoch 10716, training loss: 7271.16, average training loss: 7285.20, base loss: 16112.65
[INFO 2017-06-29 09:29:33,183 main.py:57] epoch 10717, training loss: 7340.01, average training loss: 7285.52, base loss: 16112.51
[INFO 2017-06-29 09:29:36,252 main.py:57] epoch 10718, training loss: 8314.22, average training loss: 7286.71, base loss: 16112.95
[INFO 2017-06-29 09:29:39,361 main.py:57] epoch 10719, training loss: 6203.62, average training loss: 7286.00, base loss: 16112.45
[INFO 2017-06-29 09:29:42,388 main.py:57] epoch 10720, training loss: 6633.33, average training loss: 7285.79, base loss: 16112.46
[INFO 2017-06-29 09:29:45,437 main.py:57] epoch 10721, training loss: 7459.40, average training loss: 7285.96, base loss: 16112.74
[INFO 2017-06-29 09:29:48,515 main.py:57] epoch 10722, training loss: 7070.18, average training loss: 7285.56, base loss: 16112.85
[INFO 2017-06-29 09:29:51,637 main.py:57] epoch 10723, training loss: 6706.03, average training loss: 7283.98, base loss: 16112.55
[INFO 2017-06-29 09:29:54,711 main.py:57] epoch 10724, training loss: 7197.14, average training loss: 7283.16, base loss: 16112.56
[INFO 2017-06-29 09:29:57,832 main.py:57] epoch 10725, training loss: 7787.59, average training loss: 7282.66, base loss: 16112.58
[INFO 2017-06-29 09:30:00,828 main.py:57] epoch 10726, training loss: 7163.61, average training loss: 7282.78, base loss: 16112.94
[INFO 2017-06-29 09:30:03,931 main.py:57] epoch 10727, training loss: 7358.51, average training loss: 7282.77, base loss: 16113.17
[INFO 2017-06-29 09:30:06,953 main.py:57] epoch 10728, training loss: 7309.68, average training loss: 7282.37, base loss: 16113.30
[INFO 2017-06-29 09:30:09,957 main.py:57] epoch 10729, training loss: 7062.05, average training loss: 7282.93, base loss: 16113.19
[INFO 2017-06-29 09:30:12,989 main.py:57] epoch 10730, training loss: 6472.44, average training loss: 7281.66, base loss: 16112.98
[INFO 2017-06-29 09:30:15,993 main.py:57] epoch 10731, training loss: 6388.20, average training loss: 7279.77, base loss: 16112.75
[INFO 2017-06-29 09:30:19,035 main.py:57] epoch 10732, training loss: 7137.79, average training loss: 7280.25, base loss: 16112.67
[INFO 2017-06-29 09:30:22,041 main.py:57] epoch 10733, training loss: 7625.14, average training loss: 7280.15, base loss: 16112.74
[INFO 2017-06-29 09:30:25,107 main.py:57] epoch 10734, training loss: 7123.41, average training loss: 7279.02, base loss: 16112.78
[INFO 2017-06-29 09:30:28,146 main.py:57] epoch 10735, training loss: 7349.43, average training loss: 7279.31, base loss: 16112.97
[INFO 2017-06-29 09:30:31,191 main.py:57] epoch 10736, training loss: 7358.70, average training loss: 7279.50, base loss: 16113.01
[INFO 2017-06-29 09:30:34,256 main.py:57] epoch 10737, training loss: 6909.61, average training loss: 7278.92, base loss: 16113.12
[INFO 2017-06-29 09:30:37,345 main.py:57] epoch 10738, training loss: 6652.13, average training loss: 7278.83, base loss: 16112.87
[INFO 2017-06-29 09:30:40,454 main.py:57] epoch 10739, training loss: 7621.93, average training loss: 7278.87, base loss: 16112.92
[INFO 2017-06-29 09:30:43,485 main.py:57] epoch 10740, training loss: 7422.28, average training loss: 7279.39, base loss: 16113.10
[INFO 2017-06-29 09:30:46,554 main.py:57] epoch 10741, training loss: 6477.22, average training loss: 7278.93, base loss: 16113.22
[INFO 2017-06-29 09:30:49,603 main.py:57] epoch 10742, training loss: 7614.26, average training loss: 7279.65, base loss: 16113.75
[INFO 2017-06-29 09:30:52,686 main.py:57] epoch 10743, training loss: 7457.53, average training loss: 7279.03, base loss: 16113.95
[INFO 2017-06-29 09:30:55,757 main.py:57] epoch 10744, training loss: 6951.75, average training loss: 7279.08, base loss: 16113.29
[INFO 2017-06-29 09:30:58,781 main.py:57] epoch 10745, training loss: 7177.46, average training loss: 7277.85, base loss: 16112.91
[INFO 2017-06-29 09:31:01,821 main.py:57] epoch 10746, training loss: 7465.11, average training loss: 7277.04, base loss: 16112.97
[INFO 2017-06-29 09:31:04,868 main.py:57] epoch 10747, training loss: 7232.19, average training loss: 7277.59, base loss: 16113.19
[INFO 2017-06-29 09:31:07,881 main.py:57] epoch 10748, training loss: 7042.90, average training loss: 7276.68, base loss: 16112.95
[INFO 2017-06-29 09:31:10,919 main.py:57] epoch 10749, training loss: 7859.47, average training loss: 7277.71, base loss: 16112.97
[INFO 2017-06-29 09:31:13,981 main.py:57] epoch 10750, training loss: 7353.37, average training loss: 7278.25, base loss: 16113.04
[INFO 2017-06-29 09:31:17,043 main.py:57] epoch 10751, training loss: 6595.65, average training loss: 7277.35, base loss: 16112.74
[INFO 2017-06-29 09:31:20,155 main.py:57] epoch 10752, training loss: 7682.67, average training loss: 7277.02, base loss: 16112.88
[INFO 2017-06-29 09:31:23,209 main.py:57] epoch 10753, training loss: 6625.80, average training loss: 7275.64, base loss: 16112.64
[INFO 2017-06-29 09:31:26,229 main.py:57] epoch 10754, training loss: 7310.26, average training loss: 7276.97, base loss: 16112.68
[INFO 2017-06-29 09:31:29,325 main.py:57] epoch 10755, training loss: 6838.44, average training loss: 7276.88, base loss: 16112.53
[INFO 2017-06-29 09:31:32,394 main.py:57] epoch 10756, training loss: 6081.63, average training loss: 7275.99, base loss: 16111.72
[INFO 2017-06-29 09:31:35,443 main.py:57] epoch 10757, training loss: 7243.88, average training loss: 7276.31, base loss: 16111.86
[INFO 2017-06-29 09:31:38,454 main.py:57] epoch 10758, training loss: 6934.50, average training loss: 7275.71, base loss: 16111.67
[INFO 2017-06-29 09:31:41,555 main.py:57] epoch 10759, training loss: 6962.27, average training loss: 7274.80, base loss: 16111.57
[INFO 2017-06-29 09:31:44,564 main.py:57] epoch 10760, training loss: 6570.29, average training loss: 7275.18, base loss: 16111.37
[INFO 2017-06-29 09:31:47,622 main.py:57] epoch 10761, training loss: 6960.94, average training loss: 7275.50, base loss: 16111.29
[INFO 2017-06-29 09:31:50,668 main.py:57] epoch 10762, training loss: 6979.68, average training loss: 7275.22, base loss: 16111.20
[INFO 2017-06-29 09:31:53,798 main.py:57] epoch 10763, training loss: 7593.07, average training loss: 7274.72, base loss: 16111.19
[INFO 2017-06-29 09:31:56,897 main.py:57] epoch 10764, training loss: 7889.96, average training loss: 7274.83, base loss: 16111.07
[INFO 2017-06-29 09:31:59,923 main.py:57] epoch 10765, training loss: 7118.71, average training loss: 7273.91, base loss: 16110.98
[INFO 2017-06-29 09:32:02,988 main.py:57] epoch 10766, training loss: 6542.18, average training loss: 7273.55, base loss: 16110.73
[INFO 2017-06-29 09:32:06,091 main.py:57] epoch 10767, training loss: 7162.42, average training loss: 7274.69, base loss: 16110.64
[INFO 2017-06-29 09:32:09,074 main.py:57] epoch 10768, training loss: 7490.87, average training loss: 7275.43, base loss: 16110.91
[INFO 2017-06-29 09:32:12,145 main.py:57] epoch 10769, training loss: 7293.12, average training loss: 7275.32, base loss: 16111.51
[INFO 2017-06-29 09:32:15,173 main.py:57] epoch 10770, training loss: 6977.73, average training loss: 7275.81, base loss: 16111.68
[INFO 2017-06-29 09:32:18,237 main.py:57] epoch 10771, training loss: 7117.30, average training loss: 7276.03, base loss: 16111.76
[INFO 2017-06-29 09:32:21,328 main.py:57] epoch 10772, training loss: 7086.80, average training loss: 7275.17, base loss: 16111.77
[INFO 2017-06-29 09:32:24,368 main.py:57] epoch 10773, training loss: 6854.97, average training loss: 7275.58, base loss: 16111.56
[INFO 2017-06-29 09:32:27,421 main.py:57] epoch 10774, training loss: 6505.90, average training loss: 7274.25, base loss: 16111.10
[INFO 2017-06-29 09:32:30,454 main.py:57] epoch 10775, training loss: 6993.04, average training loss: 7274.87, base loss: 16110.85
[INFO 2017-06-29 09:32:33,536 main.py:57] epoch 10776, training loss: 7357.23, average training loss: 7274.66, base loss: 16110.89
[INFO 2017-06-29 09:32:36,652 main.py:57] epoch 10777, training loss: 6734.43, average training loss: 7274.65, base loss: 16110.79
[INFO 2017-06-29 09:32:39,742 main.py:57] epoch 10778, training loss: 6692.14, average training loss: 7274.64, base loss: 16110.54
[INFO 2017-06-29 09:32:42,796 main.py:57] epoch 10779, training loss: 7598.37, average training loss: 7274.65, base loss: 16110.62
[INFO 2017-06-29 09:32:45,875 main.py:57] epoch 10780, training loss: 7695.95, average training loss: 7275.21, base loss: 16110.71
[INFO 2017-06-29 09:32:48,979 main.py:57] epoch 10781, training loss: 8358.48, average training loss: 7276.27, base loss: 16110.95
[INFO 2017-06-29 09:32:51,997 main.py:57] epoch 10782, training loss: 7057.95, average training loss: 7275.50, base loss: 16111.10
[INFO 2017-06-29 09:32:55,038 main.py:57] epoch 10783, training loss: 7603.93, average training loss: 7275.65, base loss: 16110.90
[INFO 2017-06-29 09:32:58,057 main.py:57] epoch 10784, training loss: 7554.63, average training loss: 7276.27, base loss: 16110.67
[INFO 2017-06-29 09:33:01,133 main.py:57] epoch 10785, training loss: 7900.54, average training loss: 7277.32, base loss: 16110.57
[INFO 2017-06-29 09:33:04,218 main.py:57] epoch 10786, training loss: 7170.72, average training loss: 7277.08, base loss: 16110.64
[INFO 2017-06-29 09:33:07,267 main.py:57] epoch 10787, training loss: 7507.04, average training loss: 7276.82, base loss: 16110.77
[INFO 2017-06-29 09:33:10,357 main.py:57] epoch 10788, training loss: 6442.76, average training loss: 7275.85, base loss: 16110.81
[INFO 2017-06-29 09:33:13,388 main.py:57] epoch 10789, training loss: 7052.20, average training loss: 7275.49, base loss: 16111.06
[INFO 2017-06-29 09:33:16,386 main.py:57] epoch 10790, training loss: 7470.49, average training loss: 7276.58, base loss: 16110.91
[INFO 2017-06-29 09:33:19,443 main.py:57] epoch 10791, training loss: 6462.77, average training loss: 7276.28, base loss: 16110.41
[INFO 2017-06-29 09:33:22,489 main.py:57] epoch 10792, training loss: 7145.27, average training loss: 7276.62, base loss: 16110.39
[INFO 2017-06-29 09:33:25,597 main.py:57] epoch 10793, training loss: 6932.12, average training loss: 7276.80, base loss: 16110.12
[INFO 2017-06-29 09:33:28,642 main.py:57] epoch 10794, training loss: 7096.08, average training loss: 7276.05, base loss: 16109.89
[INFO 2017-06-29 09:33:31,653 main.py:57] epoch 10795, training loss: 7534.43, average training loss: 7276.00, base loss: 16110.01
[INFO 2017-06-29 09:33:34,720 main.py:57] epoch 10796, training loss: 7982.45, average training loss: 7276.59, base loss: 16110.51
[INFO 2017-06-29 09:33:37,714 main.py:57] epoch 10797, training loss: 6651.73, average training loss: 7276.26, base loss: 16110.58
[INFO 2017-06-29 09:33:40,784 main.py:57] epoch 10798, training loss: 6456.48, average training loss: 7274.97, base loss: 16110.18
[INFO 2017-06-29 09:33:43,851 main.py:57] epoch 10799, training loss: 6840.45, average training loss: 7275.07, base loss: 16110.14
[INFO 2017-06-29 09:33:43,852 main.py:59] epoch 10799, testing
[INFO 2017-06-29 09:33:56,419 main.py:104] average testing loss: 8259.61, base loss: 17083.60
[INFO 2017-06-29 09:33:56,419 main.py:105] improve_loss: 8823.98, improve_percent: 0.52
[INFO 2017-06-29 09:33:56,420 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:33:59,456 main.py:57] epoch 10800, training loss: 7490.16, average training loss: 7274.66, base loss: 16109.84
[INFO 2017-06-29 09:34:02,488 main.py:57] epoch 10801, training loss: 6709.44, average training loss: 7273.35, base loss: 16109.52
[INFO 2017-06-29 09:34:05,622 main.py:57] epoch 10802, training loss: 7924.22, average training loss: 7274.60, base loss: 16109.30
[INFO 2017-06-29 09:34:08,612 main.py:57] epoch 10803, training loss: 6532.07, average training loss: 7273.63, base loss: 16108.89
[INFO 2017-06-29 09:34:11,685 main.py:57] epoch 10804, training loss: 6455.48, average training loss: 7272.20, base loss: 16108.78
[INFO 2017-06-29 09:34:14,735 main.py:57] epoch 10805, training loss: 6734.41, average training loss: 7271.32, base loss: 16108.58
[INFO 2017-06-29 09:34:17,814 main.py:57] epoch 10806, training loss: 6601.82, average training loss: 7270.99, base loss: 16108.57
[INFO 2017-06-29 09:34:20,933 main.py:57] epoch 10807, training loss: 6953.97, average training loss: 7268.92, base loss: 16108.56
[INFO 2017-06-29 09:34:23,963 main.py:57] epoch 10808, training loss: 6561.95, average training loss: 7267.58, base loss: 16108.18
[INFO 2017-06-29 09:34:26,983 main.py:57] epoch 10809, training loss: 7019.71, average training loss: 7267.83, base loss: 16108.39
[INFO 2017-06-29 09:34:30,079 main.py:57] epoch 10810, training loss: 7075.70, average training loss: 7266.94, base loss: 16108.22
[INFO 2017-06-29 09:34:33,102 main.py:57] epoch 10811, training loss: 6833.05, average training loss: 7267.41, base loss: 16108.20
[INFO 2017-06-29 09:34:36,200 main.py:57] epoch 10812, training loss: 6585.53, average training loss: 7266.96, base loss: 16108.03
[INFO 2017-06-29 09:34:39,206 main.py:57] epoch 10813, training loss: 6315.94, average training loss: 7266.04, base loss: 16107.84
[INFO 2017-06-29 09:34:42,250 main.py:57] epoch 10814, training loss: 7085.87, average training loss: 7267.00, base loss: 16107.84
[INFO 2017-06-29 09:34:45,310 main.py:57] epoch 10815, training loss: 8421.53, average training loss: 7268.57, base loss: 16108.02
[INFO 2017-06-29 09:34:48,294 main.py:57] epoch 10816, training loss: 7696.47, average training loss: 7269.39, base loss: 16108.76
[INFO 2017-06-29 09:34:51,345 main.py:57] epoch 10817, training loss: 6741.30, average training loss: 7268.32, base loss: 16108.72
[INFO 2017-06-29 09:34:54,401 main.py:57] epoch 10818, training loss: 7775.16, average training loss: 7268.00, base loss: 16108.56
[INFO 2017-06-29 09:34:57,415 main.py:57] epoch 10819, training loss: 7503.58, average training loss: 7268.88, base loss: 16108.49
[INFO 2017-06-29 09:35:00,484 main.py:57] epoch 10820, training loss: 7577.43, average training loss: 7269.68, base loss: 16109.05
[INFO 2017-06-29 09:35:03,603 main.py:57] epoch 10821, training loss: 7011.33, average training loss: 7269.54, base loss: 16109.16
[INFO 2017-06-29 09:35:06,712 main.py:57] epoch 10822, training loss: 7221.05, average training loss: 7268.78, base loss: 16109.30
[INFO 2017-06-29 09:35:09,820 main.py:57] epoch 10823, training loss: 7594.86, average training loss: 7268.99, base loss: 16109.50
[INFO 2017-06-29 09:35:12,840 main.py:57] epoch 10824, training loss: 6668.63, average training loss: 7269.21, base loss: 16109.19
[INFO 2017-06-29 09:35:15,975 main.py:57] epoch 10825, training loss: 6928.35, average training loss: 7268.99, base loss: 16109.21
[INFO 2017-06-29 09:35:18,978 main.py:57] epoch 10826, training loss: 6372.14, average training loss: 7267.79, base loss: 16109.30
[INFO 2017-06-29 09:35:21,994 main.py:57] epoch 10827, training loss: 6392.34, average training loss: 7266.11, base loss: 16109.32
[INFO 2017-06-29 09:35:25,020 main.py:57] epoch 10828, training loss: 7690.15, average training loss: 7267.70, base loss: 16109.37
[INFO 2017-06-29 09:35:28,145 main.py:57] epoch 10829, training loss: 7240.46, average training loss: 7268.11, base loss: 16109.21
[INFO 2017-06-29 09:35:31,282 main.py:57] epoch 10830, training loss: 7022.37, average training loss: 7267.85, base loss: 16108.98
[INFO 2017-06-29 09:35:34,407 main.py:57] epoch 10831, training loss: 7953.92, average training loss: 7268.39, base loss: 16109.16
[INFO 2017-06-29 09:35:37,559 main.py:57] epoch 10832, training loss: 8380.02, average training loss: 7269.90, base loss: 16109.47
[INFO 2017-06-29 09:35:40,636 main.py:57] epoch 10833, training loss: 6732.24, average training loss: 7268.88, base loss: 16109.27
[INFO 2017-06-29 09:35:43,699 main.py:57] epoch 10834, training loss: 8386.43, average training loss: 7269.01, base loss: 16109.93
[INFO 2017-06-29 09:35:46,788 main.py:57] epoch 10835, training loss: 6618.46, average training loss: 7268.58, base loss: 16109.84
[INFO 2017-06-29 09:35:49,808 main.py:57] epoch 10836, training loss: 6661.04, average training loss: 7268.31, base loss: 16109.72
[INFO 2017-06-29 09:35:52,927 main.py:57] epoch 10837, training loss: 7256.03, average training loss: 7268.26, base loss: 16109.84
[INFO 2017-06-29 09:35:55,985 main.py:57] epoch 10838, training loss: 8330.09, average training loss: 7268.05, base loss: 16110.12
[INFO 2017-06-29 09:35:58,965 main.py:57] epoch 10839, training loss: 8372.61, average training loss: 7268.29, base loss: 16110.46
[INFO 2017-06-29 09:36:02,055 main.py:57] epoch 10840, training loss: 6742.12, average training loss: 7268.16, base loss: 16110.59
[INFO 2017-06-29 09:36:05,106 main.py:57] epoch 10841, training loss: 7662.00, average training loss: 7268.57, base loss: 16110.81
[INFO 2017-06-29 09:36:08,237 main.py:57] epoch 10842, training loss: 6067.02, average training loss: 7268.18, base loss: 16110.37
[INFO 2017-06-29 09:36:11,350 main.py:57] epoch 10843, training loss: 7986.23, average training loss: 7269.78, base loss: 16110.58
[INFO 2017-06-29 09:36:14,406 main.py:57] epoch 10844, training loss: 7282.79, average training loss: 7270.21, base loss: 16110.61
[INFO 2017-06-29 09:36:17,407 main.py:57] epoch 10845, training loss: 6385.94, average training loss: 7269.89, base loss: 16110.42
[INFO 2017-06-29 09:36:20,469 main.py:57] epoch 10846, training loss: 7386.18, average training loss: 7269.96, base loss: 16110.48
[INFO 2017-06-29 09:36:23,480 main.py:57] epoch 10847, training loss: 7725.72, average training loss: 7269.01, base loss: 16110.46
[INFO 2017-06-29 09:36:26,563 main.py:57] epoch 10848, training loss: 6352.06, average training loss: 7267.64, base loss: 16110.55
[INFO 2017-06-29 09:36:29,597 main.py:57] epoch 10849, training loss: 6731.56, average training loss: 7267.43, base loss: 16110.55
[INFO 2017-06-29 09:36:32,657 main.py:57] epoch 10850, training loss: 7205.42, average training loss: 7267.27, base loss: 16110.30
[INFO 2017-06-29 09:36:35,646 main.py:57] epoch 10851, training loss: 6745.83, average training loss: 7267.71, base loss: 16109.82
[INFO 2017-06-29 09:36:38,753 main.py:57] epoch 10852, training loss: 7316.11, average training loss: 7267.99, base loss: 16109.88
[INFO 2017-06-29 09:36:41,903 main.py:57] epoch 10853, training loss: 8040.72, average training loss: 7268.17, base loss: 16110.24
[INFO 2017-06-29 09:36:44,945 main.py:57] epoch 10854, training loss: 6876.60, average training loss: 7266.63, base loss: 16109.93
[INFO 2017-06-29 09:36:47,938 main.py:57] epoch 10855, training loss: 7239.77, average training loss: 7267.35, base loss: 16109.55
[INFO 2017-06-29 09:36:51,045 main.py:57] epoch 10856, training loss: 8064.15, average training loss: 7267.50, base loss: 16109.64
[INFO 2017-06-29 09:36:54,078 main.py:57] epoch 10857, training loss: 7597.53, average training loss: 7267.45, base loss: 16109.57
[INFO 2017-06-29 09:36:57,131 main.py:57] epoch 10858, training loss: 6568.94, average training loss: 7267.12, base loss: 16109.28
[INFO 2017-06-29 09:37:00,202 main.py:57] epoch 10859, training loss: 6832.93, average training loss: 7266.61, base loss: 16109.20
[INFO 2017-06-29 09:37:03,239 main.py:57] epoch 10860, training loss: 7415.20, average training loss: 7267.09, base loss: 16109.31
[INFO 2017-06-29 09:37:06,250 main.py:57] epoch 10861, training loss: 7212.07, average training loss: 7265.41, base loss: 16109.28
[INFO 2017-06-29 09:37:09,269 main.py:57] epoch 10862, training loss: 9382.64, average training loss: 7267.77, base loss: 16109.90
[INFO 2017-06-29 09:37:12,243 main.py:57] epoch 10863, training loss: 7296.73, average training loss: 7266.40, base loss: 16109.88
[INFO 2017-06-29 09:37:15,317 main.py:57] epoch 10864, training loss: 7714.39, average training loss: 7267.04, base loss: 16109.93
[INFO 2017-06-29 09:37:18,355 main.py:57] epoch 10865, training loss: 7886.44, average training loss: 7267.67, base loss: 16109.93
[INFO 2017-06-29 09:37:21,419 main.py:57] epoch 10866, training loss: 6784.55, average training loss: 7267.04, base loss: 16109.75
[INFO 2017-06-29 09:37:24,387 main.py:57] epoch 10867, training loss: 6501.23, average training loss: 7265.38, base loss: 16109.34
[INFO 2017-06-29 09:37:27,404 main.py:57] epoch 10868, training loss: 7007.26, average training loss: 7264.11, base loss: 16109.26
[INFO 2017-06-29 09:37:30,408 main.py:57] epoch 10869, training loss: 7923.56, average training loss: 7264.57, base loss: 16109.50
[INFO 2017-06-29 09:37:33,453 main.py:57] epoch 10870, training loss: 7442.72, average training loss: 7263.14, base loss: 16109.74
[INFO 2017-06-29 09:37:36,519 main.py:57] epoch 10871, training loss: 7229.98, average training loss: 7263.54, base loss: 16110.07
[INFO 2017-06-29 09:37:39,522 main.py:57] epoch 10872, training loss: 6464.96, average training loss: 7263.69, base loss: 16109.91
[INFO 2017-06-29 09:37:42,555 main.py:57] epoch 10873, training loss: 7349.55, average training loss: 7263.97, base loss: 16110.01
[INFO 2017-06-29 09:37:45,595 main.py:57] epoch 10874, training loss: 7908.10, average training loss: 7264.92, base loss: 16110.33
[INFO 2017-06-29 09:37:48,632 main.py:57] epoch 10875, training loss: 8080.41, average training loss: 7266.72, base loss: 16110.60
[INFO 2017-06-29 09:37:51,673 main.py:57] epoch 10876, training loss: 7122.52, average training loss: 7266.65, base loss: 16110.80
[INFO 2017-06-29 09:37:54,774 main.py:57] epoch 10877, training loss: 6683.72, average training loss: 7265.60, base loss: 16110.59
[INFO 2017-06-29 09:37:57,885 main.py:57] epoch 10878, training loss: 7697.14, average training loss: 7264.60, base loss: 16110.73
[INFO 2017-06-29 09:38:00,894 main.py:57] epoch 10879, training loss: 7487.99, average training loss: 7265.19, base loss: 16110.70
[INFO 2017-06-29 09:38:03,966 main.py:57] epoch 10880, training loss: 6428.86, average training loss: 7264.10, base loss: 16110.61
[INFO 2017-06-29 09:38:07,033 main.py:57] epoch 10881, training loss: 7732.85, average training loss: 7264.03, base loss: 16111.18
[INFO 2017-06-29 09:38:10,088 main.py:57] epoch 10882, training loss: 6780.01, average training loss: 7263.15, base loss: 16110.77
[INFO 2017-06-29 09:38:13,095 main.py:57] epoch 10883, training loss: 7619.06, average training loss: 7263.61, base loss: 16111.12
[INFO 2017-06-29 09:38:16,145 main.py:57] epoch 10884, training loss: 7207.39, average training loss: 7263.92, base loss: 16110.87
[INFO 2017-06-29 09:38:19,130 main.py:57] epoch 10885, training loss: 7626.30, average training loss: 7264.35, base loss: 16111.28
[INFO 2017-06-29 09:38:22,165 main.py:57] epoch 10886, training loss: 6931.60, average training loss: 7264.49, base loss: 16110.71
[INFO 2017-06-29 09:38:25,177 main.py:57] epoch 10887, training loss: 7705.85, average training loss: 7265.02, base loss: 16110.68
[INFO 2017-06-29 09:38:28,218 main.py:57] epoch 10888, training loss: 7498.70, average training loss: 7264.22, base loss: 16110.67
[INFO 2017-06-29 09:38:31,328 main.py:57] epoch 10889, training loss: 7481.67, average training loss: 7264.32, base loss: 16110.49
[INFO 2017-06-29 09:38:34,339 main.py:57] epoch 10890, training loss: 6930.76, average training loss: 7263.13, base loss: 16110.54
[INFO 2017-06-29 09:38:37,351 main.py:57] epoch 10891, training loss: 6821.92, average training loss: 7263.21, base loss: 16110.36
[INFO 2017-06-29 09:38:40,495 main.py:57] epoch 10892, training loss: 7539.80, average training loss: 7264.46, base loss: 16110.35
[INFO 2017-06-29 09:38:43,507 main.py:57] epoch 10893, training loss: 7569.90, average training loss: 7265.33, base loss: 16110.20
[INFO 2017-06-29 09:38:46,538 main.py:57] epoch 10894, training loss: 6894.08, average training loss: 7264.99, base loss: 16110.40
[INFO 2017-06-29 09:38:49,590 main.py:57] epoch 10895, training loss: 6245.86, average training loss: 7263.73, base loss: 16109.99
[INFO 2017-06-29 09:38:52,631 main.py:57] epoch 10896, training loss: 7198.93, average training loss: 7263.76, base loss: 16110.11
[INFO 2017-06-29 09:38:55,675 main.py:57] epoch 10897, training loss: 7580.81, average training loss: 7262.49, base loss: 16110.36
[INFO 2017-06-29 09:38:58,700 main.py:57] epoch 10898, training loss: 7986.11, average training loss: 7263.33, base loss: 16110.72
[INFO 2017-06-29 09:39:01,759 main.py:57] epoch 10899, training loss: 7217.97, average training loss: 7262.77, base loss: 16110.71
[INFO 2017-06-29 09:39:01,760 main.py:59] epoch 10899, testing
[INFO 2017-06-29 09:39:14,442 main.py:104] average testing loss: 7852.08, base loss: 16566.75
[INFO 2017-06-29 09:39:14,443 main.py:105] improve_loss: 8714.67, improve_percent: 0.53
[INFO 2017-06-29 09:39:14,444 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:39:17,543 main.py:57] epoch 10900, training loss: 7992.92, average training loss: 7262.71, base loss: 16111.08
[INFO 2017-06-29 09:39:20,620 main.py:57] epoch 10901, training loss: 7613.24, average training loss: 7263.39, base loss: 16111.28
[INFO 2017-06-29 09:39:23,656 main.py:57] epoch 10902, training loss: 8009.80, average training loss: 7264.03, base loss: 16111.68
[INFO 2017-06-29 09:39:26,675 main.py:57] epoch 10903, training loss: 7111.29, average training loss: 7263.69, base loss: 16111.51
[INFO 2017-06-29 09:39:29,732 main.py:57] epoch 10904, training loss: 6805.03, average training loss: 7263.42, base loss: 16111.32
[INFO 2017-06-29 09:39:32,752 main.py:57] epoch 10905, training loss: 7429.76, average training loss: 7264.26, base loss: 16111.25
[INFO 2017-06-29 09:39:35,809 main.py:57] epoch 10906, training loss: 7483.58, average training loss: 7263.47, base loss: 16111.31
[INFO 2017-06-29 09:39:38,874 main.py:57] epoch 10907, training loss: 7137.22, average training loss: 7263.95, base loss: 16111.20
[INFO 2017-06-29 09:39:41,885 main.py:57] epoch 10908, training loss: 6937.84, average training loss: 7264.14, base loss: 16110.91
[INFO 2017-06-29 09:39:44,896 main.py:57] epoch 10909, training loss: 6597.06, average training loss: 7263.43, base loss: 16110.45
[INFO 2017-06-29 09:39:47,880 main.py:57] epoch 10910, training loss: 7526.35, average training loss: 7264.14, base loss: 16110.45
[INFO 2017-06-29 09:39:50,872 main.py:57] epoch 10911, training loss: 7751.48, average training loss: 7264.80, base loss: 16110.80
[INFO 2017-06-29 09:39:53,920 main.py:57] epoch 10912, training loss: 7092.10, average training loss: 7262.70, base loss: 16111.11
[INFO 2017-06-29 09:39:56,932 main.py:57] epoch 10913, training loss: 7530.54, average training loss: 7263.20, base loss: 16111.15
[INFO 2017-06-29 09:39:59,994 main.py:57] epoch 10914, training loss: 7665.40, average training loss: 7263.68, base loss: 16111.35
[INFO 2017-06-29 09:40:03,140 main.py:57] epoch 10915, training loss: 6820.58, average training loss: 7263.04, base loss: 16111.02
[INFO 2017-06-29 09:40:06,193 main.py:57] epoch 10916, training loss: 7265.91, average training loss: 7262.96, base loss: 16111.04
[INFO 2017-06-29 09:40:09,240 main.py:57] epoch 10917, training loss: 7346.08, average training loss: 7263.63, base loss: 16110.82
[INFO 2017-06-29 09:40:12,399 main.py:57] epoch 10918, training loss: 7300.13, average training loss: 7263.65, base loss: 16110.87
[INFO 2017-06-29 09:40:15,468 main.py:57] epoch 10919, training loss: 7181.75, average training loss: 7262.74, base loss: 16111.10
[INFO 2017-06-29 09:40:18,563 main.py:57] epoch 10920, training loss: 6853.42, average training loss: 7262.03, base loss: 16111.02
[INFO 2017-06-29 09:40:21,616 main.py:57] epoch 10921, training loss: 7647.69, average training loss: 7261.89, base loss: 16111.37
[INFO 2017-06-29 09:40:24,762 main.py:57] epoch 10922, training loss: 7028.98, average training loss: 7262.25, base loss: 16111.49
[INFO 2017-06-29 09:40:27,798 main.py:57] epoch 10923, training loss: 7230.91, average training loss: 7262.22, base loss: 16111.42
[INFO 2017-06-29 09:40:30,802 main.py:57] epoch 10924, training loss: 7649.70, average training loss: 7262.64, base loss: 16111.48
[INFO 2017-06-29 09:40:33,826 main.py:57] epoch 10925, training loss: 7978.07, average training loss: 7263.28, base loss: 16111.64
[INFO 2017-06-29 09:40:36,901 main.py:57] epoch 10926, training loss: 7914.55, average training loss: 7264.05, base loss: 16111.60
[INFO 2017-06-29 09:40:39,888 main.py:57] epoch 10927, training loss: 6705.31, average training loss: 7263.57, base loss: 16111.27
[INFO 2017-06-29 09:40:43,023 main.py:57] epoch 10928, training loss: 7222.55, average training loss: 7263.59, base loss: 16111.57
[INFO 2017-06-29 09:40:46,072 main.py:57] epoch 10929, training loss: 7006.76, average training loss: 7263.55, base loss: 16111.99
[INFO 2017-06-29 09:40:49,071 main.py:57] epoch 10930, training loss: 6572.66, average training loss: 7263.18, base loss: 16111.39
[INFO 2017-06-29 09:40:52,050 main.py:57] epoch 10931, training loss: 6881.38, average training loss: 7263.76, base loss: 16111.36
[INFO 2017-06-29 09:40:55,107 main.py:57] epoch 10932, training loss: 6947.37, average training loss: 7263.52, base loss: 16111.44
[INFO 2017-06-29 09:40:58,152 main.py:57] epoch 10933, training loss: 6641.06, average training loss: 7263.04, base loss: 16111.37
[INFO 2017-06-29 09:41:01,237 main.py:57] epoch 10934, training loss: 7836.34, average training loss: 7263.91, base loss: 16111.76
[INFO 2017-06-29 09:41:04,245 main.py:57] epoch 10935, training loss: 7052.74, average training loss: 7263.79, base loss: 16111.88
[INFO 2017-06-29 09:41:07,276 main.py:57] epoch 10936, training loss: 7069.49, average training loss: 7264.05, base loss: 16111.45
[INFO 2017-06-29 09:41:10,405 main.py:57] epoch 10937, training loss: 8226.89, average training loss: 7264.99, base loss: 16111.65
[INFO 2017-06-29 09:41:13,446 main.py:57] epoch 10938, training loss: 7906.24, average training loss: 7264.65, base loss: 16112.08
[INFO 2017-06-29 09:41:16,473 main.py:57] epoch 10939, training loss: 7109.36, average training loss: 7263.12, base loss: 16112.32
[INFO 2017-06-29 09:41:19,555 main.py:57] epoch 10940, training loss: 7132.03, average training loss: 7262.16, base loss: 16112.22
[INFO 2017-06-29 09:41:22,610 main.py:57] epoch 10941, training loss: 6918.52, average training loss: 7261.78, base loss: 16111.93
[INFO 2017-06-29 09:41:25,779 main.py:57] epoch 10942, training loss: 8013.24, average training loss: 7261.79, base loss: 16112.25
[INFO 2017-06-29 09:41:28,765 main.py:57] epoch 10943, training loss: 6892.54, average training loss: 7260.00, base loss: 16112.23
[INFO 2017-06-29 09:41:31,788 main.py:57] epoch 10944, training loss: 6924.82, average training loss: 7259.70, base loss: 16112.25
[INFO 2017-06-29 09:41:34,901 main.py:57] epoch 10945, training loss: 8065.73, average training loss: 7260.51, base loss: 16112.52
[INFO 2017-06-29 09:41:37,942 main.py:57] epoch 10946, training loss: 6689.42, average training loss: 7259.50, base loss: 16112.50
[INFO 2017-06-29 09:41:41,002 main.py:57] epoch 10947, training loss: 7456.79, average training loss: 7259.26, base loss: 16112.54
[INFO 2017-06-29 09:41:44,025 main.py:57] epoch 10948, training loss: 8464.44, average training loss: 7260.25, base loss: 16112.83
[INFO 2017-06-29 09:41:47,107 main.py:57] epoch 10949, training loss: 7636.23, average training loss: 7261.15, base loss: 16112.63
[INFO 2017-06-29 09:41:50,188 main.py:57] epoch 10950, training loss: 7286.19, average training loss: 7260.84, base loss: 16112.38
[INFO 2017-06-29 09:41:53,218 main.py:57] epoch 10951, training loss: 6617.25, average training loss: 7260.54, base loss: 16112.03
[INFO 2017-06-29 09:41:56,248 main.py:57] epoch 10952, training loss: 7691.80, average training loss: 7259.59, base loss: 16112.07
[INFO 2017-06-29 09:41:59,301 main.py:57] epoch 10953, training loss: 7983.71, average training loss: 7260.95, base loss: 16112.32
[INFO 2017-06-29 09:42:02,380 main.py:57] epoch 10954, training loss: 7385.46, average training loss: 7260.05, base loss: 16112.60
[INFO 2017-06-29 09:42:05,391 main.py:57] epoch 10955, training loss: 6931.64, average training loss: 7259.79, base loss: 16112.60
[INFO 2017-06-29 09:42:08,417 main.py:57] epoch 10956, training loss: 7455.35, average training loss: 7260.16, base loss: 16112.70
[INFO 2017-06-29 09:42:11,452 main.py:57] epoch 10957, training loss: 7019.03, average training loss: 7260.23, base loss: 16112.64
[INFO 2017-06-29 09:42:14,486 main.py:57] epoch 10958, training loss: 7412.73, average training loss: 7259.70, base loss: 16112.67
[INFO 2017-06-29 09:42:17,518 main.py:57] epoch 10959, training loss: 8201.59, average training loss: 7260.58, base loss: 16112.83
[INFO 2017-06-29 09:42:20,496 main.py:57] epoch 10960, training loss: 7821.76, average training loss: 7259.37, base loss: 16113.42
[INFO 2017-06-29 09:42:23,603 main.py:57] epoch 10961, training loss: 6328.28, average training loss: 7257.63, base loss: 16113.04
[INFO 2017-06-29 09:42:26,599 main.py:57] epoch 10962, training loss: 7141.75, average training loss: 7257.34, base loss: 16112.94
[INFO 2017-06-29 09:42:29,572 main.py:57] epoch 10963, training loss: 8530.43, average training loss: 7258.00, base loss: 16112.88
[INFO 2017-06-29 09:42:32,667 main.py:57] epoch 10964, training loss: 6981.35, average training loss: 7257.67, base loss: 16112.81
[INFO 2017-06-29 09:42:35,774 main.py:57] epoch 10965, training loss: 7434.03, average training loss: 7258.15, base loss: 16112.65
[INFO 2017-06-29 09:42:38,901 main.py:57] epoch 10966, training loss: 8070.52, average training loss: 7258.22, base loss: 16112.70
[INFO 2017-06-29 09:42:42,035 main.py:57] epoch 10967, training loss: 7895.78, average training loss: 7258.97, base loss: 16112.65
[INFO 2017-06-29 09:42:45,103 main.py:57] epoch 10968, training loss: 7580.86, average training loss: 7258.55, base loss: 16112.94
[INFO 2017-06-29 09:42:48,176 main.py:57] epoch 10969, training loss: 6904.44, average training loss: 7259.18, base loss: 16112.70
[INFO 2017-06-29 09:42:51,172 main.py:57] epoch 10970, training loss: 7364.80, average training loss: 7258.83, base loss: 16112.67
[INFO 2017-06-29 09:42:54,217 main.py:57] epoch 10971, training loss: 7731.17, average training loss: 7259.58, base loss: 16112.58
[INFO 2017-06-29 09:42:57,245 main.py:57] epoch 10972, training loss: 7300.43, average training loss: 7259.70, base loss: 16113.01
[INFO 2017-06-29 09:43:00,344 main.py:57] epoch 10973, training loss: 7797.13, average training loss: 7260.11, base loss: 16113.44
[INFO 2017-06-29 09:43:03,357 main.py:57] epoch 10974, training loss: 7596.20, average training loss: 7260.25, base loss: 16113.90
[INFO 2017-06-29 09:43:06,390 main.py:57] epoch 10975, training loss: 7711.70, average training loss: 7260.30, base loss: 16114.12
[INFO 2017-06-29 09:43:09,449 main.py:57] epoch 10976, training loss: 7223.51, average training loss: 7259.64, base loss: 16114.44
[INFO 2017-06-29 09:43:12,478 main.py:57] epoch 10977, training loss: 6729.67, average training loss: 7258.88, base loss: 16114.45
[INFO 2017-06-29 09:43:15,489 main.py:57] epoch 10978, training loss: 8043.59, average training loss: 7259.78, base loss: 16114.51
[INFO 2017-06-29 09:43:18,514 main.py:57] epoch 10979, training loss: 6585.90, average training loss: 7259.09, base loss: 16114.12
[INFO 2017-06-29 09:43:21,536 main.py:57] epoch 10980, training loss: 7138.78, average training loss: 7259.05, base loss: 16114.17
[INFO 2017-06-29 09:43:24,634 main.py:57] epoch 10981, training loss: 6832.27, average training loss: 7258.48, base loss: 16113.94
[INFO 2017-06-29 09:43:27,625 main.py:57] epoch 10982, training loss: 6436.27, average training loss: 7258.54, base loss: 16113.82
[INFO 2017-06-29 09:43:30,702 main.py:57] epoch 10983, training loss: 7370.58, average training loss: 7258.83, base loss: 16113.85
[INFO 2017-06-29 09:43:33,802 main.py:57] epoch 10984, training loss: 6240.91, average training loss: 7257.80, base loss: 16113.69
[INFO 2017-06-29 09:43:36,858 main.py:57] epoch 10985, training loss: 7329.07, average training loss: 7258.64, base loss: 16113.85
[INFO 2017-06-29 09:43:39,913 main.py:57] epoch 10986, training loss: 7589.00, average training loss: 7259.43, base loss: 16113.98
[INFO 2017-06-29 09:43:43,002 main.py:57] epoch 10987, training loss: 7347.87, average training loss: 7260.33, base loss: 16114.02
[INFO 2017-06-29 09:43:46,143 main.py:57] epoch 10988, training loss: 6780.67, average training loss: 7260.10, base loss: 16113.88
[INFO 2017-06-29 09:43:49,185 main.py:57] epoch 10989, training loss: 6082.11, average training loss: 7259.52, base loss: 16113.44
[INFO 2017-06-29 09:43:52,267 main.py:57] epoch 10990, training loss: 6763.70, average training loss: 7258.01, base loss: 16113.31
[INFO 2017-06-29 09:43:55,286 main.py:57] epoch 10991, training loss: 6456.82, average training loss: 7256.30, base loss: 16113.14
[INFO 2017-06-29 09:43:58,343 main.py:57] epoch 10992, training loss: 6800.34, average training loss: 7256.37, base loss: 16113.26
[INFO 2017-06-29 09:44:01,404 main.py:57] epoch 10993, training loss: 7936.49, average training loss: 7257.48, base loss: 16113.61
[INFO 2017-06-29 09:44:04,456 main.py:57] epoch 10994, training loss: 6914.87, average training loss: 7256.63, base loss: 16113.35
[INFO 2017-06-29 09:44:07,548 main.py:57] epoch 10995, training loss: 6529.73, average training loss: 7256.51, base loss: 16112.94
[INFO 2017-06-29 09:44:10,554 main.py:57] epoch 10996, training loss: 7698.34, average training loss: 7257.22, base loss: 16112.67
[INFO 2017-06-29 09:44:13,648 main.py:57] epoch 10997, training loss: 7035.08, average training loss: 7257.23, base loss: 16112.29
[INFO 2017-06-29 09:44:16,699 main.py:57] epoch 10998, training loss: 7263.90, average training loss: 7257.55, base loss: 16112.51
[INFO 2017-06-29 09:44:19,837 main.py:57] epoch 10999, training loss: 7362.33, average training loss: 7258.37, base loss: 16112.55
[INFO 2017-06-29 09:44:19,837 main.py:59] epoch 10999, testing
[INFO 2017-06-29 09:44:32,234 main.py:104] average testing loss: 7893.26, base loss: 16368.39
[INFO 2017-06-29 09:44:32,234 main.py:105] improve_loss: 8475.12, improve_percent: 0.52
[INFO 2017-06-29 09:44:32,235 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:44:35,346 main.py:57] epoch 11000, training loss: 7340.21, average training loss: 7258.77, base loss: 16112.04
[INFO 2017-06-29 09:44:38,409 main.py:57] epoch 11001, training loss: 7442.19, average training loss: 7259.13, base loss: 16111.91
[INFO 2017-06-29 09:44:41,448 main.py:57] epoch 11002, training loss: 7932.09, average training loss: 7259.53, base loss: 16112.18
[INFO 2017-06-29 09:44:44,503 main.py:57] epoch 11003, training loss: 6934.24, average training loss: 7260.16, base loss: 16112.09
[INFO 2017-06-29 09:44:47,511 main.py:57] epoch 11004, training loss: 6516.02, average training loss: 7259.65, base loss: 16112.00
[INFO 2017-06-29 09:44:50,616 main.py:57] epoch 11005, training loss: 7541.36, average training loss: 7260.52, base loss: 16112.18
[INFO 2017-06-29 09:44:53,596 main.py:57] epoch 11006, training loss: 7608.02, average training loss: 7259.73, base loss: 16111.87
[INFO 2017-06-29 09:44:56,690 main.py:57] epoch 11007, training loss: 6624.64, average training loss: 7259.11, base loss: 16111.37
[INFO 2017-06-29 09:44:59,764 main.py:57] epoch 11008, training loss: 6618.66, average training loss: 7258.89, base loss: 16111.36
[INFO 2017-06-29 09:45:02,780 main.py:57] epoch 11009, training loss: 6711.43, average training loss: 7257.38, base loss: 16111.16
[INFO 2017-06-29 09:45:05,868 main.py:57] epoch 11010, training loss: 6975.96, average training loss: 7257.69, base loss: 16111.05
[INFO 2017-06-29 09:45:08,990 main.py:57] epoch 11011, training loss: 7431.46, average training loss: 7258.48, base loss: 16111.23
[INFO 2017-06-29 09:45:11,982 main.py:57] epoch 11012, training loss: 7284.60, average training loss: 7259.35, base loss: 16111.02
[INFO 2017-06-29 09:45:15,109 main.py:57] epoch 11013, training loss: 7381.82, average training loss: 7259.60, base loss: 16110.96
[INFO 2017-06-29 09:45:18,136 main.py:57] epoch 11014, training loss: 7583.54, average training loss: 7260.43, base loss: 16111.25
[INFO 2017-06-29 09:45:21,158 main.py:57] epoch 11015, training loss: 7066.32, average training loss: 7260.18, base loss: 16111.16
[INFO 2017-06-29 09:45:24,205 main.py:57] epoch 11016, training loss: 7526.46, average training loss: 7260.31, base loss: 16111.42
[INFO 2017-06-29 09:45:27,363 main.py:57] epoch 11017, training loss: 6854.42, average training loss: 7260.38, base loss: 16111.25
[INFO 2017-06-29 09:45:30,361 main.py:57] epoch 11018, training loss: 6468.47, average training loss: 7259.17, base loss: 16111.18
[INFO 2017-06-29 09:45:33,395 main.py:57] epoch 11019, training loss: 7200.00, average training loss: 7259.43, base loss: 16111.31
[INFO 2017-06-29 09:45:36,524 main.py:57] epoch 11020, training loss: 6644.20, average training loss: 7259.10, base loss: 16111.22
[INFO 2017-06-29 09:45:39,564 main.py:57] epoch 11021, training loss: 7358.92, average training loss: 7259.68, base loss: 16111.47
[INFO 2017-06-29 09:45:42,566 main.py:57] epoch 11022, training loss: 6643.14, average training loss: 7258.96, base loss: 16111.42
[INFO 2017-06-29 09:45:45,570 main.py:57] epoch 11023, training loss: 6706.94, average training loss: 7258.24, base loss: 16111.51
[INFO 2017-06-29 09:45:48,610 main.py:57] epoch 11024, training loss: 6522.17, average training loss: 7257.61, base loss: 16111.14
[INFO 2017-06-29 09:45:51,649 main.py:57] epoch 11025, training loss: 6455.59, average training loss: 7257.31, base loss: 16110.66
[INFO 2017-06-29 09:45:54,679 main.py:57] epoch 11026, training loss: 7294.82, average training loss: 7258.54, base loss: 16110.59
[INFO 2017-06-29 09:45:57,776 main.py:57] epoch 11027, training loss: 6675.15, average training loss: 7257.45, base loss: 16110.80
[INFO 2017-06-29 09:46:00,928 main.py:57] epoch 11028, training loss: 7545.61, average training loss: 7257.56, base loss: 16111.18
[INFO 2017-06-29 09:46:03,964 main.py:57] epoch 11029, training loss: 8048.18, average training loss: 7258.53, base loss: 16111.47
[INFO 2017-06-29 09:46:07,031 main.py:57] epoch 11030, training loss: 7457.00, average training loss: 7259.25, base loss: 16111.67
[INFO 2017-06-29 09:46:10,073 main.py:57] epoch 11031, training loss: 7255.15, average training loss: 7258.64, base loss: 16111.56
[INFO 2017-06-29 09:46:13,090 main.py:57] epoch 11032, training loss: 6862.52, average training loss: 7257.90, base loss: 16111.34
[INFO 2017-06-29 09:46:16,134 main.py:57] epoch 11033, training loss: 6730.65, average training loss: 7257.67, base loss: 16110.80
[INFO 2017-06-29 09:46:19,225 main.py:57] epoch 11034, training loss: 7539.04, average training loss: 7257.81, base loss: 16110.65
[INFO 2017-06-29 09:46:22,325 main.py:57] epoch 11035, training loss: 7329.50, average training loss: 7257.38, base loss: 16110.60
[INFO 2017-06-29 09:46:25,370 main.py:57] epoch 11036, training loss: 8400.15, average training loss: 7258.57, base loss: 16110.98
[INFO 2017-06-29 09:46:28,437 main.py:57] epoch 11037, training loss: 6813.25, average training loss: 7258.27, base loss: 16111.21
[INFO 2017-06-29 09:46:31,521 main.py:57] epoch 11038, training loss: 7011.02, average training loss: 7257.89, base loss: 16111.39
[INFO 2017-06-29 09:46:34,487 main.py:57] epoch 11039, training loss: 6504.98, average training loss: 7258.30, base loss: 16111.25
[INFO 2017-06-29 09:46:37,521 main.py:57] epoch 11040, training loss: 6616.13, average training loss: 7258.21, base loss: 16111.24
[INFO 2017-06-29 09:46:40,654 main.py:57] epoch 11041, training loss: 7153.81, average training loss: 7257.50, base loss: 16111.36
[INFO 2017-06-29 09:46:43,675 main.py:57] epoch 11042, training loss: 7121.69, average training loss: 7257.18, base loss: 16111.02
[INFO 2017-06-29 09:46:46,703 main.py:57] epoch 11043, training loss: 7074.41, average training loss: 7255.69, base loss: 16110.88
[INFO 2017-06-29 09:46:49,748 main.py:57] epoch 11044, training loss: 6471.45, average training loss: 7255.28, base loss: 16110.70
[INFO 2017-06-29 09:46:52,798 main.py:57] epoch 11045, training loss: 6860.71, average training loss: 7254.31, base loss: 16110.60
[INFO 2017-06-29 09:46:55,832 main.py:57] epoch 11046, training loss: 6681.94, average training loss: 7253.70, base loss: 16110.01
[INFO 2017-06-29 09:46:58,892 main.py:57] epoch 11047, training loss: 8150.07, average training loss: 7254.56, base loss: 16110.13
[INFO 2017-06-29 09:47:01,971 main.py:57] epoch 11048, training loss: 7351.27, average training loss: 7254.67, base loss: 16110.22
[INFO 2017-06-29 09:47:05,029 main.py:57] epoch 11049, training loss: 7306.43, average training loss: 7255.26, base loss: 16110.46
[INFO 2017-06-29 09:47:08,122 main.py:57] epoch 11050, training loss: 6177.87, average training loss: 7254.59, base loss: 16110.13
[INFO 2017-06-29 09:47:11,180 main.py:57] epoch 11051, training loss: 7619.98, average training loss: 7255.86, base loss: 16110.20
[INFO 2017-06-29 09:47:14,225 main.py:57] epoch 11052, training loss: 7181.56, average training loss: 7255.38, base loss: 16110.12
[INFO 2017-06-29 09:47:17,240 main.py:57] epoch 11053, training loss: 7696.68, average training loss: 7255.77, base loss: 16110.31
[INFO 2017-06-29 09:47:20,278 main.py:57] epoch 11054, training loss: 6724.04, average training loss: 7255.48, base loss: 16110.32
[INFO 2017-06-29 09:47:23,304 main.py:57] epoch 11055, training loss: 6520.38, average training loss: 7253.75, base loss: 16110.20
[INFO 2017-06-29 09:47:26,342 main.py:57] epoch 11056, training loss: 6554.61, average training loss: 7253.18, base loss: 16110.02
[INFO 2017-06-29 09:47:29,378 main.py:57] epoch 11057, training loss: 8147.63, average training loss: 7253.54, base loss: 16110.00
[INFO 2017-06-29 09:47:32,553 main.py:57] epoch 11058, training loss: 6199.52, average training loss: 7252.71, base loss: 16109.33
[INFO 2017-06-29 09:47:35,593 main.py:57] epoch 11059, training loss: 8370.06, average training loss: 7253.87, base loss: 16109.39
[INFO 2017-06-29 09:47:38,575 main.py:57] epoch 11060, training loss: 6669.61, average training loss: 7252.86, base loss: 16109.26
[INFO 2017-06-29 09:47:41,637 main.py:57] epoch 11061, training loss: 6734.04, average training loss: 7251.87, base loss: 16109.06
[INFO 2017-06-29 09:47:44,681 main.py:57] epoch 11062, training loss: 7374.64, average training loss: 7250.81, base loss: 16109.57
[INFO 2017-06-29 09:47:47,805 main.py:57] epoch 11063, training loss: 7211.38, average training loss: 7249.69, base loss: 16109.90
[INFO 2017-06-29 09:47:50,873 main.py:57] epoch 11064, training loss: 7658.99, average training loss: 7249.62, base loss: 16110.14
[INFO 2017-06-29 09:47:53,906 main.py:57] epoch 11065, training loss: 6585.06, average training loss: 7247.88, base loss: 16110.08
[INFO 2017-06-29 09:47:56,957 main.py:57] epoch 11066, training loss: 7060.53, average training loss: 7247.95, base loss: 16109.97
[INFO 2017-06-29 09:47:59,975 main.py:57] epoch 11067, training loss: 7320.89, average training loss: 7248.45, base loss: 16110.00
[INFO 2017-06-29 09:48:03,042 main.py:57] epoch 11068, training loss: 7846.69, average training loss: 7248.33, base loss: 16110.24
[INFO 2017-06-29 09:48:06,139 main.py:57] epoch 11069, training loss: 7443.00, average training loss: 7249.38, base loss: 16110.04
[INFO 2017-06-29 09:48:09,181 main.py:57] epoch 11070, training loss: 6465.26, average training loss: 7248.71, base loss: 16109.76
[INFO 2017-06-29 09:48:12,214 main.py:57] epoch 11071, training loss: 6554.91, average training loss: 7246.64, base loss: 16109.57
[INFO 2017-06-29 09:48:15,259 main.py:57] epoch 11072, training loss: 7317.40, average training loss: 7247.48, base loss: 16109.54
[INFO 2017-06-29 09:48:18,309 main.py:57] epoch 11073, training loss: 6674.49, average training loss: 7246.70, base loss: 16109.13
[INFO 2017-06-29 09:48:21,471 main.py:57] epoch 11074, training loss: 7985.14, average training loss: 7248.15, base loss: 16109.00
[INFO 2017-06-29 09:48:24,502 main.py:57] epoch 11075, training loss: 7188.24, average training loss: 7247.79, base loss: 16108.67
[INFO 2017-06-29 09:48:27,546 main.py:57] epoch 11076, training loss: 6691.10, average training loss: 7247.79, base loss: 16108.63
[INFO 2017-06-29 09:48:30,595 main.py:57] epoch 11077, training loss: 7910.79, average training loss: 7248.45, base loss: 16108.65
[INFO 2017-06-29 09:48:33,613 main.py:57] epoch 11078, training loss: 6905.59, average training loss: 7247.91, base loss: 16108.36
[INFO 2017-06-29 09:48:36,647 main.py:57] epoch 11079, training loss: 6471.86, average training loss: 7246.62, base loss: 16107.93
[INFO 2017-06-29 09:48:39,709 main.py:57] epoch 11080, training loss: 7264.42, average training loss: 7246.63, base loss: 16108.35
[INFO 2017-06-29 09:48:42,751 main.py:57] epoch 11081, training loss: 6402.01, average training loss: 7244.04, base loss: 16108.31
[INFO 2017-06-29 09:48:45,788 main.py:57] epoch 11082, training loss: 7314.69, average training loss: 7245.11, base loss: 16108.55
[INFO 2017-06-29 09:48:48,885 main.py:57] epoch 11083, training loss: 7783.98, average training loss: 7245.13, base loss: 16108.74
[INFO 2017-06-29 09:48:51,940 main.py:57] epoch 11084, training loss: 7287.03, average training loss: 7245.35, base loss: 16108.54
[INFO 2017-06-29 09:48:54,994 main.py:57] epoch 11085, training loss: 7213.68, average training loss: 7246.38, base loss: 16108.41
[INFO 2017-06-29 09:48:58,022 main.py:57] epoch 11086, training loss: 6804.70, average training loss: 7246.02, base loss: 16108.31
[INFO 2017-06-29 09:49:01,088 main.py:57] epoch 11087, training loss: 6328.84, average training loss: 7244.59, base loss: 16107.99
[INFO 2017-06-29 09:49:04,116 main.py:57] epoch 11088, training loss: 6878.72, average training loss: 7243.65, base loss: 16107.88
[INFO 2017-06-29 09:49:07,181 main.py:57] epoch 11089, training loss: 6862.03, average training loss: 7243.10, base loss: 16107.97
[INFO 2017-06-29 09:49:10,271 main.py:57] epoch 11090, training loss: 8370.44, average training loss: 7244.29, base loss: 16108.48
[INFO 2017-06-29 09:49:13,232 main.py:57] epoch 11091, training loss: 7567.23, average training loss: 7244.44, base loss: 16108.61
[INFO 2017-06-29 09:49:16,288 main.py:57] epoch 11092, training loss: 8129.04, average training loss: 7245.78, base loss: 16108.87
[INFO 2017-06-29 09:49:19,316 main.py:57] epoch 11093, training loss: 6909.02, average training loss: 7245.04, base loss: 16108.89
[INFO 2017-06-29 09:49:22,372 main.py:57] epoch 11094, training loss: 7511.70, average training loss: 7245.94, base loss: 16109.15
[INFO 2017-06-29 09:49:25,451 main.py:57] epoch 11095, training loss: 7437.27, average training loss: 7245.80, base loss: 16109.16
[INFO 2017-06-29 09:49:28,530 main.py:57] epoch 11096, training loss: 6854.29, average training loss: 7246.02, base loss: 16109.01
[INFO 2017-06-29 09:49:31,613 main.py:57] epoch 11097, training loss: 7587.99, average training loss: 7246.68, base loss: 16109.00
[INFO 2017-06-29 09:49:34,697 main.py:57] epoch 11098, training loss: 6617.34, average training loss: 7245.33, base loss: 16108.71
[INFO 2017-06-29 09:49:37,739 main.py:57] epoch 11099, training loss: 7637.69, average training loss: 7245.58, base loss: 16108.95
[INFO 2017-06-29 09:49:37,740 main.py:59] epoch 11099, testing
[INFO 2017-06-29 09:49:50,495 main.py:104] average testing loss: 8487.19, base loss: 17566.57
[INFO 2017-06-29 09:49:50,495 main.py:105] improve_loss: 9079.38, improve_percent: 0.52
[INFO 2017-06-29 09:49:50,496 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:49:53,628 main.py:57] epoch 11100, training loss: 7802.21, average training loss: 7246.74, base loss: 16109.13
[INFO 2017-06-29 09:49:56,702 main.py:57] epoch 11101, training loss: 7571.85, average training loss: 7246.95, base loss: 16109.37
[INFO 2017-06-29 09:49:59,765 main.py:57] epoch 11102, training loss: 6772.43, average training loss: 7246.76, base loss: 16109.17
[INFO 2017-06-29 09:50:02,754 main.py:57] epoch 11103, training loss: 6332.69, average training loss: 7245.68, base loss: 16108.82
[INFO 2017-06-29 09:50:05,835 main.py:57] epoch 11104, training loss: 6886.76, average training loss: 7245.27, base loss: 16108.67
[INFO 2017-06-29 09:50:08,921 main.py:57] epoch 11105, training loss: 7253.07, average training loss: 7245.25, base loss: 16108.70
[INFO 2017-06-29 09:50:11,973 main.py:57] epoch 11106, training loss: 6857.30, average training loss: 7244.31, base loss: 16108.41
[INFO 2017-06-29 09:50:15,109 main.py:57] epoch 11107, training loss: 7758.13, average training loss: 7245.08, base loss: 16108.61
[INFO 2017-06-29 09:50:18,206 main.py:57] epoch 11108, training loss: 7609.73, average training loss: 7245.59, base loss: 16108.90
[INFO 2017-06-29 09:50:21,202 main.py:57] epoch 11109, training loss: 8379.03, average training loss: 7247.09, base loss: 16109.17
[INFO 2017-06-29 09:50:24,265 main.py:57] epoch 11110, training loss: 6382.25, average training loss: 7246.12, base loss: 16108.90
[INFO 2017-06-29 09:50:27,399 main.py:57] epoch 11111, training loss: 7669.13, average training loss: 7246.93, base loss: 16109.09
[INFO 2017-06-29 09:50:30,424 main.py:57] epoch 11112, training loss: 6632.88, average training loss: 7246.24, base loss: 16109.05
[INFO 2017-06-29 09:50:33,509 main.py:57] epoch 11113, training loss: 6872.26, average training loss: 7246.10, base loss: 16109.08
[INFO 2017-06-29 09:50:36,544 main.py:57] epoch 11114, training loss: 7731.45, average training loss: 7246.42, base loss: 16109.51
[INFO 2017-06-29 09:50:39,609 main.py:57] epoch 11115, training loss: 6826.83, average training loss: 7246.95, base loss: 16109.44
[INFO 2017-06-29 09:50:42,717 main.py:57] epoch 11116, training loss: 8082.82, average training loss: 7247.40, base loss: 16109.56
[INFO 2017-06-29 09:50:45,822 main.py:57] epoch 11117, training loss: 7543.27, average training loss: 7246.86, base loss: 16109.27
[INFO 2017-06-29 09:50:48,853 main.py:57] epoch 11118, training loss: 9591.45, average training loss: 7249.82, base loss: 16109.97
[INFO 2017-06-29 09:50:51,880 main.py:57] epoch 11119, training loss: 8448.63, average training loss: 7250.51, base loss: 16110.79
[INFO 2017-06-29 09:50:54,915 main.py:57] epoch 11120, training loss: 7531.08, average training loss: 7250.92, base loss: 16111.08
[INFO 2017-06-29 09:50:58,005 main.py:57] epoch 11121, training loss: 7142.15, average training loss: 7250.50, base loss: 16110.81
[INFO 2017-06-29 09:51:00,994 main.py:57] epoch 11122, training loss: 8615.17, average training loss: 7252.69, base loss: 16111.10
[INFO 2017-06-29 09:51:04,039 main.py:57] epoch 11123, training loss: 7024.17, average training loss: 7252.20, base loss: 16111.24
[INFO 2017-06-29 09:51:07,147 main.py:57] epoch 11124, training loss: 9183.27, average training loss: 7253.54, base loss: 16111.52
[INFO 2017-06-29 09:51:10,289 main.py:57] epoch 11125, training loss: 6622.82, average training loss: 7253.80, base loss: 16111.07
[INFO 2017-06-29 09:51:13,389 main.py:57] epoch 11126, training loss: 7393.59, average training loss: 7254.44, base loss: 16111.19
[INFO 2017-06-29 09:51:16,497 main.py:57] epoch 11127, training loss: 6940.86, average training loss: 7254.66, base loss: 16111.10
[INFO 2017-06-29 09:51:19,520 main.py:57] epoch 11128, training loss: 6636.94, average training loss: 7254.48, base loss: 16110.98
[INFO 2017-06-29 09:51:22,572 main.py:57] epoch 11129, training loss: 7168.46, average training loss: 7253.14, base loss: 16111.03
[INFO 2017-06-29 09:51:25,610 main.py:57] epoch 11130, training loss: 8589.19, average training loss: 7254.39, base loss: 16111.20
[INFO 2017-06-29 09:51:28,635 main.py:57] epoch 11131, training loss: 8887.41, average training loss: 7256.19, base loss: 16111.60
[INFO 2017-06-29 09:51:31,654 main.py:57] epoch 11132, training loss: 7901.47, average training loss: 7256.23, base loss: 16111.92
[INFO 2017-06-29 09:51:34,724 main.py:57] epoch 11133, training loss: 7632.97, average training loss: 7257.28, base loss: 16112.16
[INFO 2017-06-29 09:51:37,747 main.py:57] epoch 11134, training loss: 6731.71, average training loss: 7255.99, base loss: 16111.95
[INFO 2017-06-29 09:51:40,799 main.py:57] epoch 11135, training loss: 7275.97, average training loss: 7254.60, base loss: 16111.87
[INFO 2017-06-29 09:51:43,885 main.py:57] epoch 11136, training loss: 6269.32, average training loss: 7253.57, base loss: 16111.43
[INFO 2017-06-29 09:51:46,973 main.py:57] epoch 11137, training loss: 8393.11, average training loss: 7255.22, base loss: 16111.96
[INFO 2017-06-29 09:51:49,971 main.py:57] epoch 11138, training loss: 7117.60, average training loss: 7254.96, base loss: 16111.81
[INFO 2017-06-29 09:51:52,992 main.py:57] epoch 11139, training loss: 7451.38, average training loss: 7255.03, base loss: 16111.77
[INFO 2017-06-29 09:51:56,064 main.py:57] epoch 11140, training loss: 6937.56, average training loss: 7254.83, base loss: 16111.43
[INFO 2017-06-29 09:51:59,199 main.py:57] epoch 11141, training loss: 6931.07, average training loss: 7254.73, base loss: 16110.99
[INFO 2017-06-29 09:52:02,261 main.py:57] epoch 11142, training loss: 7105.38, average training loss: 7254.83, base loss: 16110.83
[INFO 2017-06-29 09:52:05,331 main.py:57] epoch 11143, training loss: 7772.64, average training loss: 7255.56, base loss: 16111.04
[INFO 2017-06-29 09:52:08,402 main.py:57] epoch 11144, training loss: 7595.62, average training loss: 7256.63, base loss: 16111.03
[INFO 2017-06-29 09:52:11,463 main.py:57] epoch 11145, training loss: 7009.21, average training loss: 7256.19, base loss: 16111.04
[INFO 2017-06-29 09:52:14,509 main.py:57] epoch 11146, training loss: 6042.47, average training loss: 7255.32, base loss: 16110.91
[INFO 2017-06-29 09:52:17,582 main.py:57] epoch 11147, training loss: 7003.67, average training loss: 7253.80, base loss: 16110.80
[INFO 2017-06-29 09:52:20,704 main.py:57] epoch 11148, training loss: 7094.09, average training loss: 7254.03, base loss: 16110.65
[INFO 2017-06-29 09:52:23,738 main.py:57] epoch 11149, training loss: 6664.15, average training loss: 7253.07, base loss: 16110.59
[INFO 2017-06-29 09:52:26,788 main.py:57] epoch 11150, training loss: 7282.49, average training loss: 7252.88, base loss: 16110.57
[INFO 2017-06-29 09:52:29,819 main.py:57] epoch 11151, training loss: 6595.52, average training loss: 7252.73, base loss: 16110.11
[INFO 2017-06-29 09:52:32,850 main.py:57] epoch 11152, training loss: 8631.75, average training loss: 7254.61, base loss: 16110.32
[INFO 2017-06-29 09:52:35,950 main.py:57] epoch 11153, training loss: 7199.23, average training loss: 7254.26, base loss: 16110.52
[INFO 2017-06-29 09:52:39,064 main.py:57] epoch 11154, training loss: 7081.75, average training loss: 7253.14, base loss: 16110.47
[INFO 2017-06-29 09:52:42,195 main.py:57] epoch 11155, training loss: 7405.58, average training loss: 7253.15, base loss: 16110.63
[INFO 2017-06-29 09:52:45,248 main.py:57] epoch 11156, training loss: 6920.45, average training loss: 7252.91, base loss: 16110.28
[INFO 2017-06-29 09:52:48,300 main.py:57] epoch 11157, training loss: 6980.91, average training loss: 7252.54, base loss: 16109.99
[INFO 2017-06-29 09:52:51,370 main.py:57] epoch 11158, training loss: 6942.55, average training loss: 7252.88, base loss: 16109.86
[INFO 2017-06-29 09:52:54,476 main.py:57] epoch 11159, training loss: 7540.86, average training loss: 7253.37, base loss: 16110.07
[INFO 2017-06-29 09:52:57,504 main.py:57] epoch 11160, training loss: 7863.66, average training loss: 7254.63, base loss: 16110.58
[INFO 2017-06-29 09:53:00,536 main.py:57] epoch 11161, training loss: 7438.36, average training loss: 7254.87, base loss: 16110.69
[INFO 2017-06-29 09:53:03,607 main.py:57] epoch 11162, training loss: 8012.71, average training loss: 7256.45, base loss: 16111.09
[INFO 2017-06-29 09:53:06,663 main.py:57] epoch 11163, training loss: 7375.63, average training loss: 7255.65, base loss: 16111.02
[INFO 2017-06-29 09:53:09,722 main.py:57] epoch 11164, training loss: 7586.49, average training loss: 7256.32, base loss: 16111.20
[INFO 2017-06-29 09:53:12,777 main.py:57] epoch 11165, training loss: 6454.04, average training loss: 7256.57, base loss: 16111.00
[INFO 2017-06-29 09:53:15,845 main.py:57] epoch 11166, training loss: 7906.40, average training loss: 7256.35, base loss: 16111.20
[INFO 2017-06-29 09:53:18,878 main.py:57] epoch 11167, training loss: 7455.71, average training loss: 7256.34, base loss: 16111.19
[INFO 2017-06-29 09:53:21,931 main.py:57] epoch 11168, training loss: 6558.51, average training loss: 7255.23, base loss: 16111.10
[INFO 2017-06-29 09:53:25,018 main.py:57] epoch 11169, training loss: 6558.61, average training loss: 7254.22, base loss: 16110.78
[INFO 2017-06-29 09:53:28,067 main.py:57] epoch 11170, training loss: 6858.25, average training loss: 7253.44, base loss: 16110.59
[INFO 2017-06-29 09:53:31,149 main.py:57] epoch 11171, training loss: 6885.04, average training loss: 7253.11, base loss: 16110.36
[INFO 2017-06-29 09:53:34,245 main.py:57] epoch 11172, training loss: 7892.38, average training loss: 7253.40, base loss: 16110.79
[INFO 2017-06-29 09:53:37,297 main.py:57] epoch 11173, training loss: 7280.83, average training loss: 7253.33, base loss: 16110.73
[INFO 2017-06-29 09:53:40,396 main.py:57] epoch 11174, training loss: 7354.31, average training loss: 7252.71, base loss: 16110.72
[INFO 2017-06-29 09:53:43,403 main.py:57] epoch 11175, training loss: 7009.31, average training loss: 7252.47, base loss: 16110.59
[INFO 2017-06-29 09:53:46,468 main.py:57] epoch 11176, training loss: 8363.28, average training loss: 7252.62, base loss: 16110.95
[INFO 2017-06-29 09:53:49,526 main.py:57] epoch 11177, training loss: 7056.84, average training loss: 7252.76, base loss: 16110.65
[INFO 2017-06-29 09:53:52,575 main.py:57] epoch 11178, training loss: 7840.08, average training loss: 7253.86, base loss: 16110.96
[INFO 2017-06-29 09:53:55,599 main.py:57] epoch 11179, training loss: 6622.38, average training loss: 7253.56, base loss: 16110.71
[INFO 2017-06-29 09:53:58,666 main.py:57] epoch 11180, training loss: 6458.94, average training loss: 7252.95, base loss: 16110.19
[INFO 2017-06-29 09:54:01,772 main.py:57] epoch 11181, training loss: 8289.51, average training loss: 7253.79, base loss: 16110.49
[INFO 2017-06-29 09:54:04,761 main.py:57] epoch 11182, training loss: 6896.20, average training loss: 7252.02, base loss: 16110.55
[INFO 2017-06-29 09:54:07,806 main.py:57] epoch 11183, training loss: 6112.88, average training loss: 7250.44, base loss: 16110.58
[INFO 2017-06-29 09:54:10,816 main.py:57] epoch 11184, training loss: 7935.75, average training loss: 7251.23, base loss: 16110.72
[INFO 2017-06-29 09:54:13,842 main.py:57] epoch 11185, training loss: 7255.86, average training loss: 7251.41, base loss: 16110.85
[INFO 2017-06-29 09:54:16,903 main.py:57] epoch 11186, training loss: 7821.34, average training loss: 7252.20, base loss: 16111.24
[INFO 2017-06-29 09:54:19,920 main.py:57] epoch 11187, training loss: 7431.90, average training loss: 7252.75, base loss: 16111.38
[INFO 2017-06-29 09:54:22,909 main.py:57] epoch 11188, training loss: 6511.61, average training loss: 7251.59, base loss: 16111.28
[INFO 2017-06-29 09:54:25,981 main.py:57] epoch 11189, training loss: 7466.95, average training loss: 7251.73, base loss: 16111.50
[INFO 2017-06-29 09:54:29,003 main.py:57] epoch 11190, training loss: 7149.49, average training loss: 7250.88, base loss: 16111.62
[INFO 2017-06-29 09:54:32,100 main.py:57] epoch 11191, training loss: 7392.00, average training loss: 7250.72, base loss: 16111.42
[INFO 2017-06-29 09:54:35,118 main.py:57] epoch 11192, training loss: 8556.80, average training loss: 7251.75, base loss: 16111.54
[INFO 2017-06-29 09:54:38,172 main.py:57] epoch 11193, training loss: 7741.76, average training loss: 7250.94, base loss: 16111.45
[INFO 2017-06-29 09:54:41,235 main.py:57] epoch 11194, training loss: 6807.58, average training loss: 7249.27, base loss: 16111.22
[INFO 2017-06-29 09:54:44,271 main.py:57] epoch 11195, training loss: 6909.63, average training loss: 7248.64, base loss: 16111.07
[INFO 2017-06-29 09:54:47,344 main.py:57] epoch 11196, training loss: 8104.53, average training loss: 7249.75, base loss: 16111.25
[INFO 2017-06-29 09:54:50,420 main.py:57] epoch 11197, training loss: 8446.60, average training loss: 7251.55, base loss: 16111.42
[INFO 2017-06-29 09:54:53,499 main.py:57] epoch 11198, training loss: 6985.10, average training loss: 7251.73, base loss: 16111.11
[INFO 2017-06-29 09:54:56,573 main.py:57] epoch 11199, training loss: 7015.70, average training loss: 7250.01, base loss: 16111.00
[INFO 2017-06-29 09:54:56,573 main.py:59] epoch 11199, testing
[INFO 2017-06-29 09:55:09,319 main.py:104] average testing loss: 8491.09, base loss: 17634.70
[INFO 2017-06-29 09:55:09,320 main.py:105] improve_loss: 9143.61, improve_percent: 0.52
[INFO 2017-06-29 09:55:09,321 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 09:55:12,422 main.py:57] epoch 11200, training loss: 7406.92, average training loss: 7248.75, base loss: 16110.99
[INFO 2017-06-29 09:55:15,460 main.py:57] epoch 11201, training loss: 7018.49, average training loss: 7248.39, base loss: 16110.64
[INFO 2017-06-29 09:55:18,536 main.py:57] epoch 11202, training loss: 8127.89, average training loss: 7249.80, base loss: 16110.75
[INFO 2017-06-29 09:55:21,612 main.py:57] epoch 11203, training loss: 6702.28, average training loss: 7249.43, base loss: 16110.30
[INFO 2017-06-29 09:55:24,676 main.py:57] epoch 11204, training loss: 7256.66, average training loss: 7249.16, base loss: 16110.36
[INFO 2017-06-29 09:55:27,796 main.py:57] epoch 11205, training loss: 6850.80, average training loss: 7248.72, base loss: 16110.19
[INFO 2017-06-29 09:55:30,815 main.py:57] epoch 11206, training loss: 6763.66, average training loss: 7248.22, base loss: 16110.07
[INFO 2017-06-29 09:55:33,899 main.py:57] epoch 11207, training loss: 8458.21, average training loss: 7250.01, base loss: 16110.41
[INFO 2017-06-29 09:55:36,946 main.py:57] epoch 11208, training loss: 7920.84, average training loss: 7251.59, base loss: 16110.32
[INFO 2017-06-29 09:55:40,018 main.py:57] epoch 11209, training loss: 6956.47, average training loss: 7252.32, base loss: 16110.16
[INFO 2017-06-29 09:55:43,023 main.py:57] epoch 11210, training loss: 6733.34, average training loss: 7250.74, base loss: 16109.83
[INFO 2017-06-29 09:55:46,064 main.py:57] epoch 11211, training loss: 7727.85, average training loss: 7250.68, base loss: 16110.05
[INFO 2017-06-29 09:55:49,097 main.py:57] epoch 11212, training loss: 6759.69, average training loss: 7249.55, base loss: 16109.80
[INFO 2017-06-29 09:55:52,131 main.py:57] epoch 11213, training loss: 7948.61, average training loss: 7248.95, base loss: 16109.72
[INFO 2017-06-29 09:55:55,204 main.py:57] epoch 11214, training loss: 6905.33, average training loss: 7247.92, base loss: 16109.59
[INFO 2017-06-29 09:55:58,232 main.py:57] epoch 11215, training loss: 7795.84, average training loss: 7249.40, base loss: 16109.55
[INFO 2017-06-29 09:56:01,267 main.py:57] epoch 11216, training loss: 8970.07, average training loss: 7251.60, base loss: 16110.01
[INFO 2017-06-29 09:56:04,277 main.py:57] epoch 11217, training loss: 6834.16, average training loss: 7250.44, base loss: 16109.90
[INFO 2017-06-29 09:56:07,308 main.py:57] epoch 11218, training loss: 7139.19, average training loss: 7250.95, base loss: 16109.80
[INFO 2017-06-29 09:56:10,352 main.py:57] epoch 11219, training loss: 8854.59, average training loss: 7252.02, base loss: 16110.12
[INFO 2017-06-29 09:56:13,444 main.py:57] epoch 11220, training loss: 6753.50, average training loss: 7252.39, base loss: 16109.91
[INFO 2017-06-29 09:56:16,438 main.py:57] epoch 11221, training loss: 7200.22, average training loss: 7252.85, base loss: 16109.90
[INFO 2017-06-29 09:56:19,547 main.py:57] epoch 11222, training loss: 7292.83, average training loss: 7253.01, base loss: 16109.62
[INFO 2017-06-29 09:56:22,676 main.py:57] epoch 11223, training loss: 6898.78, average training loss: 7252.63, base loss: 16109.32
[INFO 2017-06-29 09:56:25,732 main.py:57] epoch 11224, training loss: 7219.72, average training loss: 7251.61, base loss: 16109.29
[INFO 2017-06-29 09:56:28,743 main.py:57] epoch 11225, training loss: 6433.96, average training loss: 7250.32, base loss: 16109.21
[INFO 2017-06-29 09:56:31,774 main.py:57] epoch 11226, training loss: 6833.50, average training loss: 7249.96, base loss: 16108.96
[INFO 2017-06-29 09:56:34,860 main.py:57] epoch 11227, training loss: 7059.31, average training loss: 7249.70, base loss: 16108.98
[INFO 2017-06-29 09:56:37,914 main.py:57] epoch 11228, training loss: 7701.85, average training loss: 7250.74, base loss: 16109.21
[INFO 2017-06-29 09:56:40,963 main.py:57] epoch 11229, training loss: 8552.31, average training loss: 7251.72, base loss: 16109.88
[INFO 2017-06-29 09:56:43,998 main.py:57] epoch 11230, training loss: 6981.57, average training loss: 7251.05, base loss: 16109.93
[INFO 2017-06-29 09:56:47,055 main.py:57] epoch 11231, training loss: 6672.78, average training loss: 7251.40, base loss: 16109.62
[INFO 2017-06-29 09:56:50,134 main.py:57] epoch 11232, training loss: 8023.34, average training loss: 7252.97, base loss: 16109.69
[INFO 2017-06-29 09:56:53,225 main.py:57] epoch 11233, training loss: 7401.78, average training loss: 7252.62, base loss: 16109.80
[INFO 2017-06-29 09:56:56,251 main.py:57] epoch 11234, training loss: 7988.13, average training loss: 7253.88, base loss: 16109.93
[INFO 2017-06-29 09:56:59,458 main.py:57] epoch 11235, training loss: 7267.16, average training loss: 7254.37, base loss: 16109.99
[INFO 2017-06-29 09:57:02,533 main.py:57] epoch 11236, training loss: 7238.57, average training loss: 7253.83, base loss: 16109.80
[INFO 2017-06-29 09:57:05,627 main.py:57] epoch 11237, training loss: 7418.93, average training loss: 7254.56, base loss: 16109.92
[INFO 2017-06-29 09:57:08,652 main.py:57] epoch 11238, training loss: 6718.54, average training loss: 7254.57, base loss: 16109.58
[INFO 2017-06-29 09:57:11,742 main.py:57] epoch 11239, training loss: 7605.49, average training loss: 7255.20, base loss: 16109.87
[INFO 2017-06-29 09:57:14,816 main.py:57] epoch 11240, training loss: 7737.84, average training loss: 7255.93, base loss: 16109.97
[INFO 2017-06-29 09:57:17,991 main.py:57] epoch 11241, training loss: 6745.39, average training loss: 7255.14, base loss: 16110.02
[INFO 2017-06-29 09:57:21,062 main.py:57] epoch 11242, training loss: 7079.83, average training loss: 7255.04, base loss: 16109.97
[INFO 2017-06-29 09:57:24,130 main.py:57] epoch 11243, training loss: 6916.75, average training loss: 7254.17, base loss: 16109.97
[INFO 2017-06-29 09:57:27,135 main.py:57] epoch 11244, training loss: 6877.46, average training loss: 7253.51, base loss: 16109.93
[INFO 2017-06-29 09:57:30,154 main.py:57] epoch 11245, training loss: 7371.94, average training loss: 7254.68, base loss: 16110.05
[INFO 2017-06-29 09:57:33,243 main.py:57] epoch 11246, training loss: 6151.83, average training loss: 7252.76, base loss: 16109.83
[INFO 2017-06-29 09:57:36,293 main.py:57] epoch 11247, training loss: 6856.40, average training loss: 7252.41, base loss: 16109.85
[INFO 2017-06-29 09:57:39,305 main.py:57] epoch 11248, training loss: 7041.39, average training loss: 7251.94, base loss: 16109.95
[INFO 2017-06-29 09:57:42,266 main.py:57] epoch 11249, training loss: 7713.90, average training loss: 7252.84, base loss: 16110.38
[INFO 2017-06-29 09:57:45,292 main.py:57] epoch 11250, training loss: 6977.75, average training loss: 7252.41, base loss: 16110.65
[INFO 2017-06-29 09:57:48,361 main.py:57] epoch 11251, training loss: 6257.02, average training loss: 7251.38, base loss: 16110.53
[INFO 2017-06-29 09:57:51,350 main.py:57] epoch 11252, training loss: 7255.52, average training loss: 7251.32, base loss: 16110.44
[INFO 2017-06-29 09:57:54,422 main.py:57] epoch 11253, training loss: 7222.24, average training loss: 7251.67, base loss: 16110.45
[INFO 2017-06-29 09:57:57,546 main.py:57] epoch 11254, training loss: 7571.79, average training loss: 7252.28, base loss: 16110.40
[INFO 2017-06-29 09:58:00,612 main.py:57] epoch 11255, training loss: 7775.21, average training loss: 7252.86, base loss: 16110.09
[INFO 2017-06-29 09:58:03,652 main.py:57] epoch 11256, training loss: 6746.77, average training loss: 7251.55, base loss: 16109.87
[INFO 2017-06-29 09:58:06,651 main.py:57] epoch 11257, training loss: 7146.58, average training loss: 7251.13, base loss: 16109.75
[INFO 2017-06-29 09:58:09,667 main.py:57] epoch 11258, training loss: 7411.33, average training loss: 7251.72, base loss: 16109.95
[INFO 2017-06-29 09:58:12,729 main.py:57] epoch 11259, training loss: 7525.94, average training loss: 7250.77, base loss: 16110.12
[INFO 2017-06-29 09:58:15,774 main.py:57] epoch 11260, training loss: 6452.43, average training loss: 7250.01, base loss: 16109.85
[INFO 2017-06-29 09:58:18,841 main.py:57] epoch 11261, training loss: 6684.48, average training loss: 7248.80, base loss: 16109.50
[INFO 2017-06-29 09:58:21,871 main.py:57] epoch 11262, training loss: 7355.09, average training loss: 7247.62, base loss: 16109.63
[INFO 2017-06-29 09:58:24,941 main.py:57] epoch 11263, training loss: 7074.14, average training loss: 7246.66, base loss: 16109.72
[INFO 2017-06-29 09:58:28,046 main.py:57] epoch 11264, training loss: 7421.24, average training loss: 7248.15, base loss: 16109.96
[INFO 2017-06-29 09:58:31,099 main.py:57] epoch 11265, training loss: 7551.79, average training loss: 7247.46, base loss: 16110.10
[INFO 2017-06-29 09:58:34,106 main.py:57] epoch 11266, training loss: 6792.71, average training loss: 7245.99, base loss: 16110.05
[INFO 2017-06-29 09:58:37,178 main.py:57] epoch 11267, training loss: 8664.36, average training loss: 7247.45, base loss: 16110.73
[INFO 2017-06-29 09:58:40,288 main.py:57] epoch 11268, training loss: 7573.12, average training loss: 7247.85, base loss: 16110.74
[INFO 2017-06-29 09:58:43,337 main.py:57] epoch 11269, training loss: 6612.08, average training loss: 7247.18, base loss: 16110.45
[INFO 2017-06-29 09:58:46,471 main.py:57] epoch 11270, training loss: 7638.02, average training loss: 7248.18, base loss: 16110.40
[INFO 2017-06-29 09:58:49,524 main.py:57] epoch 11271, training loss: 6763.66, average training loss: 7247.57, base loss: 16110.17
[INFO 2017-06-29 09:58:52,629 main.py:57] epoch 11272, training loss: 8521.16, average training loss: 7248.69, base loss: 16110.59
[INFO 2017-06-29 09:58:55,648 main.py:57] epoch 11273, training loss: 6337.97, average training loss: 7247.85, base loss: 16110.19
[INFO 2017-06-29 09:58:58,708 main.py:57] epoch 11274, training loss: 7023.77, average training loss: 7247.51, base loss: 16110.27
[INFO 2017-06-29 09:59:01,731 main.py:57] epoch 11275, training loss: 7012.31, average training loss: 7247.08, base loss: 16110.18
[INFO 2017-06-29 09:59:04,785 main.py:57] epoch 11276, training loss: 6518.33, average training loss: 7245.14, base loss: 16109.96
[INFO 2017-06-29 09:59:07,808 main.py:57] epoch 11277, training loss: 7255.87, average training loss: 7245.36, base loss: 16110.08
[INFO 2017-06-29 09:59:10,849 main.py:57] epoch 11278, training loss: 6850.45, average training loss: 7244.46, base loss: 16109.97
[INFO 2017-06-29 09:59:13,958 main.py:57] epoch 11279, training loss: 7037.22, average training loss: 7244.01, base loss: 16109.98
[INFO 2017-06-29 09:59:17,056 main.py:57] epoch 11280, training loss: 7882.99, average training loss: 7243.70, base loss: 16110.27
[INFO 2017-06-29 09:59:20,089 main.py:57] epoch 11281, training loss: 7943.99, average training loss: 7243.85, base loss: 16110.68
[INFO 2017-06-29 09:59:23,178 main.py:57] epoch 11282, training loss: 6426.01, average training loss: 7243.14, base loss: 16110.16
[INFO 2017-06-29 09:59:26,178 main.py:57] epoch 11283, training loss: 7580.02, average training loss: 7242.28, base loss: 16110.13
[INFO 2017-06-29 09:59:29,341 main.py:57] epoch 11284, training loss: 7012.65, average training loss: 7241.32, base loss: 16110.04
[INFO 2017-06-29 09:59:32,393 main.py:57] epoch 11285, training loss: 6716.13, average training loss: 7239.60, base loss: 16109.70
[INFO 2017-06-29 09:59:35,428 main.py:57] epoch 11286, training loss: 8485.88, average training loss: 7240.34, base loss: 16109.90
[INFO 2017-06-29 09:59:38,400 main.py:57] epoch 11287, training loss: 7996.98, average training loss: 7241.41, base loss: 16110.34
[INFO 2017-06-29 09:59:41,389 main.py:57] epoch 11288, training loss: 6370.91, average training loss: 7240.75, base loss: 16110.10
[INFO 2017-06-29 09:59:44,495 main.py:57] epoch 11289, training loss: 6883.87, average training loss: 7240.54, base loss: 16110.12
[INFO 2017-06-29 09:59:47,570 main.py:57] epoch 11290, training loss: 7120.11, average training loss: 7240.32, base loss: 16109.60
[INFO 2017-06-29 09:59:50,584 main.py:57] epoch 11291, training loss: 7339.35, average training loss: 7240.06, base loss: 16109.29
[INFO 2017-06-29 09:59:53,670 main.py:57] epoch 11292, training loss: 7609.35, average training loss: 7240.69, base loss: 16109.48
[INFO 2017-06-29 09:59:56,757 main.py:57] epoch 11293, training loss: 6620.62, average training loss: 7239.79, base loss: 16109.20
[INFO 2017-06-29 09:59:59,802 main.py:57] epoch 11294, training loss: 6526.60, average training loss: 7238.89, base loss: 16109.07
[INFO 2017-06-29 10:00:02,901 main.py:57] epoch 11295, training loss: 6948.88, average training loss: 7237.40, base loss: 16108.91
[INFO 2017-06-29 10:00:05,962 main.py:57] epoch 11296, training loss: 6780.38, average training loss: 7236.98, base loss: 16108.98
[INFO 2017-06-29 10:00:09,018 main.py:57] epoch 11297, training loss: 6805.27, average training loss: 7237.59, base loss: 16108.83
[INFO 2017-06-29 10:00:12,124 main.py:57] epoch 11298, training loss: 6421.67, average training loss: 7236.84, base loss: 16108.54
[INFO 2017-06-29 10:00:15,173 main.py:57] epoch 11299, training loss: 6868.18, average training loss: 7237.16, base loss: 16108.31
[INFO 2017-06-29 10:00:15,173 main.py:59] epoch 11299, testing
[INFO 2017-06-29 10:00:27,741 main.py:104] average testing loss: 7794.27, base loss: 16362.30
[INFO 2017-06-29 10:00:27,741 main.py:105] improve_loss: 8568.04, improve_percent: 0.52
[INFO 2017-06-29 10:00:27,742 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:00:30,852 main.py:57] epoch 11300, training loss: 6977.52, average training loss: 7237.26, base loss: 16107.82
[INFO 2017-06-29 10:00:33,873 main.py:57] epoch 11301, training loss: 7135.62, average training loss: 7236.88, base loss: 16107.71
[INFO 2017-06-29 10:00:36,996 main.py:57] epoch 11302, training loss: 7370.71, average training loss: 7238.01, base loss: 16108.09
[INFO 2017-06-29 10:00:40,045 main.py:57] epoch 11303, training loss: 6721.31, average training loss: 7236.91, base loss: 16108.18
[INFO 2017-06-29 10:00:43,130 main.py:57] epoch 11304, training loss: 6527.28, average training loss: 7235.23, base loss: 16107.73
[INFO 2017-06-29 10:00:46,168 main.py:57] epoch 11305, training loss: 7267.68, average training loss: 7234.86, base loss: 16107.95
[INFO 2017-06-29 10:00:49,195 main.py:57] epoch 11306, training loss: 7507.93, average training loss: 7235.16, base loss: 16108.22
[INFO 2017-06-29 10:00:52,243 main.py:57] epoch 11307, training loss: 6677.64, average training loss: 7235.05, base loss: 16108.17
[INFO 2017-06-29 10:00:55,350 main.py:57] epoch 11308, training loss: 7786.57, average training loss: 7235.08, base loss: 16108.61
[INFO 2017-06-29 10:00:58,447 main.py:57] epoch 11309, training loss: 7985.37, average training loss: 7235.61, base loss: 16109.02
[INFO 2017-06-29 10:01:01,459 main.py:57] epoch 11310, training loss: 7597.68, average training loss: 7235.98, base loss: 16109.13
[INFO 2017-06-29 10:01:04,601 main.py:57] epoch 11311, training loss: 6667.86, average training loss: 7236.54, base loss: 16108.82
[INFO 2017-06-29 10:01:07,696 main.py:57] epoch 11312, training loss: 6951.93, average training loss: 7236.73, base loss: 16108.59
[INFO 2017-06-29 10:01:10,837 main.py:57] epoch 11313, training loss: 7722.04, average training loss: 7236.35, base loss: 16108.77
[INFO 2017-06-29 10:01:13,873 main.py:57] epoch 11314, training loss: 7536.79, average training loss: 7236.76, base loss: 16108.79
[INFO 2017-06-29 10:01:16,912 main.py:57] epoch 11315, training loss: 7538.74, average training loss: 7237.47, base loss: 16109.04
[INFO 2017-06-29 10:01:19,990 main.py:57] epoch 11316, training loss: 7330.97, average training loss: 7237.20, base loss: 16109.31
[INFO 2017-06-29 10:01:23,070 main.py:57] epoch 11317, training loss: 8600.04, average training loss: 7239.49, base loss: 16109.85
[INFO 2017-06-29 10:01:26,158 main.py:57] epoch 11318, training loss: 8148.63, average training loss: 7240.23, base loss: 16110.19
[INFO 2017-06-29 10:01:29,272 main.py:57] epoch 11319, training loss: 7457.50, average training loss: 7240.71, base loss: 16110.37
[INFO 2017-06-29 10:01:32,303 main.py:57] epoch 11320, training loss: 6980.82, average training loss: 7240.28, base loss: 16110.36
[INFO 2017-06-29 10:01:35,358 main.py:57] epoch 11321, training loss: 7884.30, average training loss: 7241.69, base loss: 16110.82
[INFO 2017-06-29 10:01:38,437 main.py:57] epoch 11322, training loss: 6973.45, average training loss: 7240.15, base loss: 16110.89
[INFO 2017-06-29 10:01:41,513 main.py:57] epoch 11323, training loss: 7360.41, average training loss: 7240.56, base loss: 16111.33
[INFO 2017-06-29 10:01:44,531 main.py:57] epoch 11324, training loss: 7744.90, average training loss: 7241.80, base loss: 16111.28
[INFO 2017-06-29 10:01:47,587 main.py:57] epoch 11325, training loss: 7345.68, average training loss: 7242.95, base loss: 16111.41
[INFO 2017-06-29 10:01:50,658 main.py:57] epoch 11326, training loss: 6919.67, average training loss: 7243.81, base loss: 16111.37
[INFO 2017-06-29 10:01:53,735 main.py:57] epoch 11327, training loss: 7330.39, average training loss: 7243.36, base loss: 16111.37
[INFO 2017-06-29 10:01:56,809 main.py:57] epoch 11328, training loss: 6811.81, average training loss: 7242.53, base loss: 16111.19
[INFO 2017-06-29 10:01:59,834 main.py:57] epoch 11329, training loss: 6605.66, average training loss: 7242.19, base loss: 16111.06
[INFO 2017-06-29 10:02:02,885 main.py:57] epoch 11330, training loss: 8137.90, average training loss: 7243.19, base loss: 16111.13
[INFO 2017-06-29 10:02:05,973 main.py:57] epoch 11331, training loss: 7118.16, average training loss: 7243.39, base loss: 16111.24
[INFO 2017-06-29 10:02:09,113 main.py:57] epoch 11332, training loss: 8086.08, average training loss: 7244.51, base loss: 16111.35
[INFO 2017-06-29 10:02:12,135 main.py:57] epoch 11333, training loss: 7387.40, average training loss: 7243.39, base loss: 16111.19
[INFO 2017-06-29 10:02:15,194 main.py:57] epoch 11334, training loss: 6851.96, average training loss: 7242.60, base loss: 16111.25
[INFO 2017-06-29 10:02:18,282 main.py:57] epoch 11335, training loss: 7100.69, average training loss: 7241.57, base loss: 16111.43
[INFO 2017-06-29 10:02:21,335 main.py:57] epoch 11336, training loss: 8065.54, average training loss: 7242.93, base loss: 16112.05
[INFO 2017-06-29 10:02:24,349 main.py:57] epoch 11337, training loss: 6586.78, average training loss: 7242.67, base loss: 16111.90
[INFO 2017-06-29 10:02:27,356 main.py:57] epoch 11338, training loss: 7654.60, average training loss: 7243.29, base loss: 16112.03
[INFO 2017-06-29 10:02:30,516 main.py:57] epoch 11339, training loss: 8295.34, average training loss: 7245.23, base loss: 16112.10
[INFO 2017-06-29 10:02:33,587 main.py:57] epoch 11340, training loss: 6954.36, average training loss: 7244.66, base loss: 16112.15
[INFO 2017-06-29 10:02:36,584 main.py:57] epoch 11341, training loss: 6949.84, average training loss: 7244.15, base loss: 16112.14
[INFO 2017-06-29 10:02:39,622 main.py:57] epoch 11342, training loss: 7247.27, average training loss: 7244.76, base loss: 16112.11
[INFO 2017-06-29 10:02:42,705 main.py:57] epoch 11343, training loss: 7168.31, average training loss: 7245.19, base loss: 16112.11
[INFO 2017-06-29 10:02:45,780 main.py:57] epoch 11344, training loss: 7510.09, average training loss: 7245.93, base loss: 16111.99
[INFO 2017-06-29 10:02:48,798 main.py:57] epoch 11345, training loss: 7183.96, average training loss: 7244.44, base loss: 16111.87
[INFO 2017-06-29 10:02:51,807 main.py:57] epoch 11346, training loss: 6595.88, average training loss: 7244.10, base loss: 16111.61
[INFO 2017-06-29 10:02:54,875 main.py:57] epoch 11347, training loss: 6757.35, average training loss: 7244.31, base loss: 16111.43
[INFO 2017-06-29 10:02:57,932 main.py:57] epoch 11348, training loss: 8368.03, average training loss: 7244.24, base loss: 16111.60
[INFO 2017-06-29 10:03:01,022 main.py:57] epoch 11349, training loss: 6949.76, average training loss: 7244.30, base loss: 16111.32
[INFO 2017-06-29 10:03:04,015 main.py:57] epoch 11350, training loss: 7773.70, average training loss: 7244.72, base loss: 16111.66
[INFO 2017-06-29 10:03:07,056 main.py:57] epoch 11351, training loss: 7782.75, average training loss: 7245.92, base loss: 16111.89
[INFO 2017-06-29 10:03:10,090 main.py:57] epoch 11352, training loss: 7659.11, average training loss: 7245.54, base loss: 16112.18
[INFO 2017-06-29 10:03:13,116 main.py:57] epoch 11353, training loss: 7010.56, average training loss: 7244.99, base loss: 16112.34
[INFO 2017-06-29 10:03:16,212 main.py:57] epoch 11354, training loss: 7778.58, average training loss: 7245.07, base loss: 16112.36
[INFO 2017-06-29 10:03:19,302 main.py:57] epoch 11355, training loss: 7740.48, average training loss: 7244.73, base loss: 16112.43
[INFO 2017-06-29 10:03:22,358 main.py:57] epoch 11356, training loss: 6475.57, average training loss: 7244.12, base loss: 16112.30
[INFO 2017-06-29 10:03:25,411 main.py:57] epoch 11357, training loss: 7151.83, average training loss: 7243.92, base loss: 16112.64
[INFO 2017-06-29 10:03:28,444 main.py:57] epoch 11358, training loss: 7322.40, average training loss: 7244.18, base loss: 16112.28
[INFO 2017-06-29 10:03:31,534 main.py:57] epoch 11359, training loss: 6694.95, average training loss: 7242.41, base loss: 16111.93
[INFO 2017-06-29 10:03:34,565 main.py:57] epoch 11360, training loss: 6660.44, average training loss: 7242.13, base loss: 16111.73
[INFO 2017-06-29 10:03:37,678 main.py:57] epoch 11361, training loss: 6552.91, average training loss: 7241.91, base loss: 16111.31
[INFO 2017-06-29 10:03:40,822 main.py:57] epoch 11362, training loss: 6788.14, average training loss: 7241.65, base loss: 16111.29
[INFO 2017-06-29 10:03:43,851 main.py:57] epoch 11363, training loss: 8471.90, average training loss: 7242.79, base loss: 16112.18
[INFO 2017-06-29 10:03:46,958 main.py:57] epoch 11364, training loss: 7562.03, average training loss: 7241.23, base loss: 16112.04
[INFO 2017-06-29 10:03:49,976 main.py:57] epoch 11365, training loss: 6463.55, average training loss: 7241.14, base loss: 16111.36
[INFO 2017-06-29 10:03:52,976 main.py:57] epoch 11366, training loss: 7450.34, average training loss: 7241.95, base loss: 16111.56
[INFO 2017-06-29 10:03:55,998 main.py:57] epoch 11367, training loss: 6676.59, average training loss: 7241.83, base loss: 16111.81
[INFO 2017-06-29 10:03:59,060 main.py:57] epoch 11368, training loss: 7029.85, average training loss: 7241.17, base loss: 16111.80
[INFO 2017-06-29 10:04:02,175 main.py:57] epoch 11369, training loss: 6789.55, average training loss: 7240.77, base loss: 16111.57
[INFO 2017-06-29 10:04:05,267 main.py:57] epoch 11370, training loss: 6920.84, average training loss: 7240.06, base loss: 16111.46
[INFO 2017-06-29 10:04:08,335 main.py:57] epoch 11371, training loss: 7688.74, average training loss: 7240.73, base loss: 16111.58
[INFO 2017-06-29 10:04:11,383 main.py:57] epoch 11372, training loss: 7671.13, average training loss: 7241.48, base loss: 16111.65
[INFO 2017-06-29 10:04:14,488 main.py:57] epoch 11373, training loss: 7197.33, average training loss: 7241.59, base loss: 16111.78
[INFO 2017-06-29 10:04:17,559 main.py:57] epoch 11374, training loss: 8554.17, average training loss: 7243.59, base loss: 16112.19
[INFO 2017-06-29 10:04:20,691 main.py:57] epoch 11375, training loss: 7319.16, average training loss: 7243.43, base loss: 16112.16
[INFO 2017-06-29 10:04:23,722 main.py:57] epoch 11376, training loss: 6997.68, average training loss: 7243.50, base loss: 16111.66
[INFO 2017-06-29 10:04:26,740 main.py:57] epoch 11377, training loss: 6930.55, average training loss: 7242.41, base loss: 16111.19
[INFO 2017-06-29 10:04:29,811 main.py:57] epoch 11378, training loss: 6654.17, average training loss: 7240.93, base loss: 16111.20
[INFO 2017-06-29 10:04:32,812 main.py:57] epoch 11379, training loss: 6777.22, average training loss: 7240.89, base loss: 16111.23
[INFO 2017-06-29 10:04:35,786 main.py:57] epoch 11380, training loss: 6571.46, average training loss: 7239.38, base loss: 16110.77
[INFO 2017-06-29 10:04:38,934 main.py:57] epoch 11381, training loss: 8187.14, average training loss: 7240.10, base loss: 16110.74
[INFO 2017-06-29 10:04:41,994 main.py:57] epoch 11382, training loss: 7916.30, average training loss: 7241.05, base loss: 16110.74
[INFO 2017-06-29 10:04:45,081 main.py:57] epoch 11383, training loss: 6806.36, average training loss: 7240.86, base loss: 16110.40
[INFO 2017-06-29 10:04:48,080 main.py:57] epoch 11384, training loss: 6826.07, average training loss: 7240.47, base loss: 16110.31
[INFO 2017-06-29 10:04:51,107 main.py:57] epoch 11385, training loss: 7182.76, average training loss: 7240.42, base loss: 16110.42
[INFO 2017-06-29 10:04:54,140 main.py:57] epoch 11386, training loss: 6178.16, average training loss: 7239.41, base loss: 16109.96
[INFO 2017-06-29 10:04:57,241 main.py:57] epoch 11387, training loss: 7285.59, average training loss: 7240.10, base loss: 16110.07
[INFO 2017-06-29 10:05:00,341 main.py:57] epoch 11388, training loss: 8779.54, average training loss: 7241.29, base loss: 16110.44
[INFO 2017-06-29 10:05:03,364 main.py:57] epoch 11389, training loss: 7343.95, average training loss: 7241.20, base loss: 16110.40
[INFO 2017-06-29 10:05:06,457 main.py:57] epoch 11390, training loss: 7425.19, average training loss: 7241.75, base loss: 16110.40
[INFO 2017-06-29 10:05:09,526 main.py:57] epoch 11391, training loss: 6967.65, average training loss: 7241.88, base loss: 16110.18
[INFO 2017-06-29 10:05:12,650 main.py:57] epoch 11392, training loss: 6572.39, average training loss: 7241.68, base loss: 16110.12
[INFO 2017-06-29 10:05:15,785 main.py:57] epoch 11393, training loss: 6523.91, average training loss: 7240.06, base loss: 16109.96
[INFO 2017-06-29 10:05:18,790 main.py:57] epoch 11394, training loss: 7918.65, average training loss: 7241.22, base loss: 16109.92
[INFO 2017-06-29 10:05:21,916 main.py:57] epoch 11395, training loss: 7439.70, average training loss: 7240.83, base loss: 16109.96
[INFO 2017-06-29 10:05:25,009 main.py:57] epoch 11396, training loss: 6829.88, average training loss: 7241.58, base loss: 16110.03
[INFO 2017-06-29 10:05:28,041 main.py:57] epoch 11397, training loss: 7198.58, average training loss: 7240.67, base loss: 16110.21
[INFO 2017-06-29 10:05:31,061 main.py:57] epoch 11398, training loss: 7825.48, average training loss: 7241.12, base loss: 16110.27
[INFO 2017-06-29 10:05:34,132 main.py:57] epoch 11399, training loss: 6577.62, average training loss: 7241.21, base loss: 16109.86
[INFO 2017-06-29 10:05:34,132 main.py:59] epoch 11399, testing
[INFO 2017-06-29 10:05:46,655 main.py:104] average testing loss: 8448.73, base loss: 17268.10
[INFO 2017-06-29 10:05:46,655 main.py:105] improve_loss: 8819.37, improve_percent: 0.51
[INFO 2017-06-29 10:05:46,657 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:05:49,691 main.py:57] epoch 11400, training loss: 7568.32, average training loss: 7241.77, base loss: 16109.98
[INFO 2017-06-29 10:05:52,742 main.py:57] epoch 11401, training loss: 8012.66, average training loss: 7243.01, base loss: 16110.29
[INFO 2017-06-29 10:05:55,776 main.py:57] epoch 11402, training loss: 7599.92, average training loss: 7244.23, base loss: 16110.11
[INFO 2017-06-29 10:05:58,832 main.py:57] epoch 11403, training loss: 8497.68, average training loss: 7246.24, base loss: 16110.39
[INFO 2017-06-29 10:06:01,843 main.py:57] epoch 11404, training loss: 7740.34, average training loss: 7247.31, base loss: 16110.34
[INFO 2017-06-29 10:06:04,875 main.py:57] epoch 11405, training loss: 7240.61, average training loss: 7246.49, base loss: 16110.36
[INFO 2017-06-29 10:06:07,868 main.py:57] epoch 11406, training loss: 7844.43, average training loss: 7247.39, base loss: 16110.55
[INFO 2017-06-29 10:06:10,951 main.py:57] epoch 11407, training loss: 6888.54, average training loss: 7248.08, base loss: 16110.42
[INFO 2017-06-29 10:06:13,980 main.py:57] epoch 11408, training loss: 7665.23, average training loss: 7248.43, base loss: 16110.47
[INFO 2017-06-29 10:06:17,057 main.py:57] epoch 11409, training loss: 6497.34, average training loss: 7247.23, base loss: 16110.22
[INFO 2017-06-29 10:06:20,117 main.py:57] epoch 11410, training loss: 7051.77, average training loss: 7247.21, base loss: 16110.28
[INFO 2017-06-29 10:06:23,159 main.py:57] epoch 11411, training loss: 7702.04, average training loss: 7247.07, base loss: 16110.80
[INFO 2017-06-29 10:06:26,299 main.py:57] epoch 11412, training loss: 7549.76, average training loss: 7246.54, base loss: 16110.89
[INFO 2017-06-29 10:06:29,365 main.py:57] epoch 11413, training loss: 6500.00, average training loss: 7245.74, base loss: 16110.49
[INFO 2017-06-29 10:06:32,502 main.py:57] epoch 11414, training loss: 8380.74, average training loss: 7247.12, base loss: 16110.81
[INFO 2017-06-29 10:06:35,584 main.py:57] epoch 11415, training loss: 7048.72, average training loss: 7247.67, base loss: 16110.63
[INFO 2017-06-29 10:06:38,604 main.py:57] epoch 11416, training loss: 7235.51, average training loss: 7247.68, base loss: 16110.75
[INFO 2017-06-29 10:06:41,671 main.py:57] epoch 11417, training loss: 7434.85, average training loss: 7249.09, base loss: 16110.81
[INFO 2017-06-29 10:06:44,818 main.py:57] epoch 11418, training loss: 6389.04, average training loss: 7247.51, base loss: 16110.46
[INFO 2017-06-29 10:06:47,915 main.py:57] epoch 11419, training loss: 6603.78, average training loss: 7247.58, base loss: 16110.20
[INFO 2017-06-29 10:06:50,922 main.py:57] epoch 11420, training loss: 7450.15, average training loss: 7247.98, base loss: 16110.62
[INFO 2017-06-29 10:06:53,953 main.py:57] epoch 11421, training loss: 6331.02, average training loss: 7247.56, base loss: 16110.53
[INFO 2017-06-29 10:06:57,002 main.py:57] epoch 11422, training loss: 7111.25, average training loss: 7247.25, base loss: 16110.70
[INFO 2017-06-29 10:07:00,050 main.py:57] epoch 11423, training loss: 7914.42, average training loss: 7247.26, base loss: 16111.07
[INFO 2017-06-29 10:07:03,052 main.py:57] epoch 11424, training loss: 7243.95, average training loss: 7247.71, base loss: 16110.89
[INFO 2017-06-29 10:07:06,121 main.py:57] epoch 11425, training loss: 7873.71, average training loss: 7248.04, base loss: 16111.01
[INFO 2017-06-29 10:07:09,205 main.py:57] epoch 11426, training loss: 7468.65, average training loss: 7248.76, base loss: 16111.27
[INFO 2017-06-29 10:07:12,228 main.py:57] epoch 11427, training loss: 6951.40, average training loss: 7248.20, base loss: 16111.16
[INFO 2017-06-29 10:07:15,384 main.py:57] epoch 11428, training loss: 7302.84, average training loss: 7247.56, base loss: 16111.31
[INFO 2017-06-29 10:07:18,451 main.py:57] epoch 11429, training loss: 6786.39, average training loss: 7246.92, base loss: 16111.11
[INFO 2017-06-29 10:07:21,541 main.py:57] epoch 11430, training loss: 6958.06, average training loss: 7246.98, base loss: 16111.28
[INFO 2017-06-29 10:07:24,651 main.py:57] epoch 11431, training loss: 7062.04, average training loss: 7246.89, base loss: 16111.38
[INFO 2017-06-29 10:07:27,777 main.py:57] epoch 11432, training loss: 7741.65, average training loss: 7246.84, base loss: 16111.60
[INFO 2017-06-29 10:07:30,756 main.py:57] epoch 11433, training loss: 6819.79, average training loss: 7246.55, base loss: 16111.73
[INFO 2017-06-29 10:07:33,847 main.py:57] epoch 11434, training loss: 7209.40, average training loss: 7245.70, base loss: 16111.73
[INFO 2017-06-29 10:07:36,832 main.py:57] epoch 11435, training loss: 7164.74, average training loss: 7245.39, base loss: 16111.73
[INFO 2017-06-29 10:07:39,940 main.py:57] epoch 11436, training loss: 7424.05, average training loss: 7244.75, base loss: 16112.00
[INFO 2017-06-29 10:07:42,976 main.py:57] epoch 11437, training loss: 7028.79, average training loss: 7244.46, base loss: 16111.93
[INFO 2017-06-29 10:07:46,142 main.py:57] epoch 11438, training loss: 7447.03, average training loss: 7243.89, base loss: 16111.79
[INFO 2017-06-29 10:07:49,156 main.py:57] epoch 11439, training loss: 7119.13, average training loss: 7242.79, base loss: 16111.84
[INFO 2017-06-29 10:07:52,212 main.py:57] epoch 11440, training loss: 6983.14, average training loss: 7242.97, base loss: 16111.74
[INFO 2017-06-29 10:07:55,254 main.py:57] epoch 11441, training loss: 6659.92, average training loss: 7243.05, base loss: 16111.74
[INFO 2017-06-29 10:07:58,256 main.py:57] epoch 11442, training loss: 8826.01, average training loss: 7245.39, base loss: 16112.24
[INFO 2017-06-29 10:08:01,262 main.py:57] epoch 11443, training loss: 7499.67, average training loss: 7245.08, base loss: 16112.40
[INFO 2017-06-29 10:08:04,338 main.py:57] epoch 11444, training loss: 5895.27, average training loss: 7243.12, base loss: 16111.84
[INFO 2017-06-29 10:08:07,347 main.py:57] epoch 11445, training loss: 6290.87, average training loss: 7242.22, base loss: 16111.33
[INFO 2017-06-29 10:08:10,390 main.py:57] epoch 11446, training loss: 6821.84, average training loss: 7241.42, base loss: 16111.00
[INFO 2017-06-29 10:08:13,495 main.py:57] epoch 11447, training loss: 6562.06, average training loss: 7240.86, base loss: 16110.65
[INFO 2017-06-29 10:08:16,607 main.py:57] epoch 11448, training loss: 8504.33, average training loss: 7242.92, base loss: 16110.87
[INFO 2017-06-29 10:08:19,592 main.py:57] epoch 11449, training loss: 8012.26, average training loss: 7243.32, base loss: 16110.94
[INFO 2017-06-29 10:08:22,656 main.py:57] epoch 11450, training loss: 7302.77, average training loss: 7243.91, base loss: 16110.99
[INFO 2017-06-29 10:08:25,683 main.py:57] epoch 11451, training loss: 8644.71, average training loss: 7244.86, base loss: 16111.10
[INFO 2017-06-29 10:08:28,738 main.py:57] epoch 11452, training loss: 7376.44, average training loss: 7245.54, base loss: 16111.41
[INFO 2017-06-29 10:08:31,843 main.py:57] epoch 11453, training loss: 7114.78, average training loss: 7245.55, base loss: 16111.59
[INFO 2017-06-29 10:08:34,937 main.py:57] epoch 11454, training loss: 7031.83, average training loss: 7245.83, base loss: 16111.47
[INFO 2017-06-29 10:08:38,022 main.py:57] epoch 11455, training loss: 7175.46, average training loss: 7244.68, base loss: 16111.63
[INFO 2017-06-29 10:08:41,054 main.py:57] epoch 11456, training loss: 6567.01, average training loss: 7243.68, base loss: 16111.37
[INFO 2017-06-29 10:08:44,133 main.py:57] epoch 11457, training loss: 6745.11, average training loss: 7243.63, base loss: 16111.03
[INFO 2017-06-29 10:08:47,232 main.py:57] epoch 11458, training loss: 7864.75, average training loss: 7243.46, base loss: 16111.09
[INFO 2017-06-29 10:08:50,261 main.py:57] epoch 11459, training loss: 6678.24, average training loss: 7242.45, base loss: 16110.83
[INFO 2017-06-29 10:08:53,272 main.py:57] epoch 11460, training loss: 7500.96, average training loss: 7242.71, base loss: 16110.95
[INFO 2017-06-29 10:08:56,330 main.py:57] epoch 11461, training loss: 8524.46, average training loss: 7244.54, base loss: 16111.46
[INFO 2017-06-29 10:08:59,376 main.py:57] epoch 11462, training loss: 7701.97, average training loss: 7246.08, base loss: 16111.88
[INFO 2017-06-29 10:09:02,444 main.py:57] epoch 11463, training loss: 6610.87, average training loss: 7244.48, base loss: 16111.58
[INFO 2017-06-29 10:09:05,502 main.py:57] epoch 11464, training loss: 7328.77, average training loss: 7244.82, base loss: 16111.83
[INFO 2017-06-29 10:09:08,477 main.py:57] epoch 11465, training loss: 6783.47, average training loss: 7244.93, base loss: 16111.55
[INFO 2017-06-29 10:09:11,606 main.py:57] epoch 11466, training loss: 8051.93, average training loss: 7245.56, base loss: 16112.07
[INFO 2017-06-29 10:09:14,772 main.py:57] epoch 11467, training loss: 7159.81, average training loss: 7244.80, base loss: 16112.47
[INFO 2017-06-29 10:09:17,815 main.py:57] epoch 11468, training loss: 8119.75, average training loss: 7245.76, base loss: 16112.62
[INFO 2017-06-29 10:09:20,865 main.py:57] epoch 11469, training loss: 6848.95, average training loss: 7243.14, base loss: 16112.30
[INFO 2017-06-29 10:09:23,941 main.py:57] epoch 11470, training loss: 7258.19, average training loss: 7242.94, base loss: 16112.60
[INFO 2017-06-29 10:09:27,043 main.py:57] epoch 11471, training loss: 6719.90, average training loss: 7241.95, base loss: 16112.19
[INFO 2017-06-29 10:09:30,022 main.py:57] epoch 11472, training loss: 7128.50, average training loss: 7242.21, base loss: 16112.20
[INFO 2017-06-29 10:09:33,209 main.py:57] epoch 11473, training loss: 7672.93, average training loss: 7243.56, base loss: 16112.42
[INFO 2017-06-29 10:09:36,251 main.py:57] epoch 11474, training loss: 5953.41, average training loss: 7241.37, base loss: 16111.94
[INFO 2017-06-29 10:09:39,334 main.py:57] epoch 11475, training loss: 7119.69, average training loss: 7241.53, base loss: 16111.79
[INFO 2017-06-29 10:09:42,399 main.py:57] epoch 11476, training loss: 7273.56, average training loss: 7242.10, base loss: 16111.75
[INFO 2017-06-29 10:09:45,439 main.py:57] epoch 11477, training loss: 7147.90, average training loss: 7241.29, base loss: 16111.69
[INFO 2017-06-29 10:09:48,483 main.py:57] epoch 11478, training loss: 7011.41, average training loss: 7240.41, base loss: 16111.81
[INFO 2017-06-29 10:09:51,530 main.py:57] epoch 11479, training loss: 8283.91, average training loss: 7240.25, base loss: 16111.95
[INFO 2017-06-29 10:09:54,588 main.py:57] epoch 11480, training loss: 8176.12, average training loss: 7241.38, base loss: 16112.12
[INFO 2017-06-29 10:09:57,615 main.py:57] epoch 11481, training loss: 7253.46, average training loss: 7240.63, base loss: 16112.08
[INFO 2017-06-29 10:10:00,655 main.py:57] epoch 11482, training loss: 7599.32, average training loss: 7239.07, base loss: 16112.41
[INFO 2017-06-29 10:10:03,734 main.py:57] epoch 11483, training loss: 6961.71, average training loss: 7238.24, base loss: 16112.32
[INFO 2017-06-29 10:10:06,816 main.py:57] epoch 11484, training loss: 8339.03, average training loss: 7238.82, base loss: 16112.61
[INFO 2017-06-29 10:10:09,864 main.py:57] epoch 11485, training loss: 7762.56, average training loss: 7238.51, base loss: 16112.63
[INFO 2017-06-29 10:10:12,895 main.py:57] epoch 11486, training loss: 6720.04, average training loss: 7237.91, base loss: 16112.60
[INFO 2017-06-29 10:10:15,874 main.py:57] epoch 11487, training loss: 7760.85, average training loss: 7239.25, base loss: 16112.79
[INFO 2017-06-29 10:10:18,909 main.py:57] epoch 11488, training loss: 6904.64, average training loss: 7239.22, base loss: 16112.49
[INFO 2017-06-29 10:10:21,954 main.py:57] epoch 11489, training loss: 7593.25, average training loss: 7239.11, base loss: 16112.55
[INFO 2017-06-29 10:10:25,041 main.py:57] epoch 11490, training loss: 7595.61, average training loss: 7239.46, base loss: 16113.03
[INFO 2017-06-29 10:10:28,075 main.py:57] epoch 11491, training loss: 6592.42, average training loss: 7237.93, base loss: 16113.00
[INFO 2017-06-29 10:10:31,119 main.py:57] epoch 11492, training loss: 6836.99, average training loss: 7236.12, base loss: 16113.07
[INFO 2017-06-29 10:10:34,157 main.py:57] epoch 11493, training loss: 7204.43, average training loss: 7235.68, base loss: 16113.25
[INFO 2017-06-29 10:10:37,282 main.py:57] epoch 11494, training loss: 6831.77, average training loss: 7236.06, base loss: 16113.23
[INFO 2017-06-29 10:10:40,340 main.py:57] epoch 11495, training loss: 7899.58, average training loss: 7237.02, base loss: 16113.52
[INFO 2017-06-29 10:10:43,398 main.py:57] epoch 11496, training loss: 6940.00, average training loss: 7237.52, base loss: 16113.87
[INFO 2017-06-29 10:10:46,398 main.py:57] epoch 11497, training loss: 7605.71, average training loss: 7238.45, base loss: 16114.04
[INFO 2017-06-29 10:10:49,435 main.py:57] epoch 11498, training loss: 6659.55, average training loss: 7237.82, base loss: 16113.80
[INFO 2017-06-29 10:10:52,473 main.py:57] epoch 11499, training loss: 7198.56, average training loss: 7238.89, base loss: 16113.69
[INFO 2017-06-29 10:10:52,473 main.py:59] epoch 11499, testing
[INFO 2017-06-29 10:11:05,065 main.py:104] average testing loss: 7890.67, base loss: 16537.69
[INFO 2017-06-29 10:11:05,065 main.py:105] improve_loss: 8647.02, improve_percent: 0.52
[INFO 2017-06-29 10:11:05,067 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:11:08,118 main.py:57] epoch 11500, training loss: 7072.87, average training loss: 7239.18, base loss: 16113.68
[INFO 2017-06-29 10:11:11,189 main.py:57] epoch 11501, training loss: 6967.97, average training loss: 7239.80, base loss: 16113.53
[INFO 2017-06-29 10:11:14,223 main.py:57] epoch 11502, training loss: 7525.79, average training loss: 7240.17, base loss: 16113.89
[INFO 2017-06-29 10:11:17,324 main.py:57] epoch 11503, training loss: 7670.72, average training loss: 7239.80, base loss: 16114.08
[INFO 2017-06-29 10:11:20,439 main.py:57] epoch 11504, training loss: 7285.94, average training loss: 7240.37, base loss: 16113.82
[INFO 2017-06-29 10:11:23,593 main.py:57] epoch 11505, training loss: 7015.13, average training loss: 7239.37, base loss: 16113.48
[INFO 2017-06-29 10:11:26,686 main.py:57] epoch 11506, training loss: 8203.24, average training loss: 7240.50, base loss: 16113.98
[INFO 2017-06-29 10:11:29,705 main.py:57] epoch 11507, training loss: 7115.52, average training loss: 7239.84, base loss: 16113.67
[INFO 2017-06-29 10:11:32,742 main.py:57] epoch 11508, training loss: 7069.09, average training loss: 7239.42, base loss: 16113.72
[INFO 2017-06-29 10:11:35,780 main.py:57] epoch 11509, training loss: 6688.07, average training loss: 7238.44, base loss: 16113.50
[INFO 2017-06-29 10:11:38,867 main.py:57] epoch 11510, training loss: 6558.39, average training loss: 7237.73, base loss: 16113.41
[INFO 2017-06-29 10:11:41,962 main.py:57] epoch 11511, training loss: 6937.15, average training loss: 7237.05, base loss: 16113.37
[INFO 2017-06-29 10:11:45,055 main.py:57] epoch 11512, training loss: 7070.08, average training loss: 7237.96, base loss: 16113.35
[INFO 2017-06-29 10:11:48,142 main.py:57] epoch 11513, training loss: 6545.75, average training loss: 7237.25, base loss: 16113.24
[INFO 2017-06-29 10:11:51,208 main.py:57] epoch 11514, training loss: 7131.95, average training loss: 7237.52, base loss: 16113.21
[INFO 2017-06-29 10:11:54,312 main.py:57] epoch 11515, training loss: 7876.92, average training loss: 7237.75, base loss: 16113.30
[INFO 2017-06-29 10:11:57,327 main.py:57] epoch 11516, training loss: 7113.28, average training loss: 7237.53, base loss: 16113.08
[INFO 2017-06-29 10:12:00,357 main.py:57] epoch 11517, training loss: 7165.65, average training loss: 7237.95, base loss: 16113.05
[INFO 2017-06-29 10:12:03,415 main.py:57] epoch 11518, training loss: 7808.97, average training loss: 7238.03, base loss: 16113.06
[INFO 2017-06-29 10:12:06,532 main.py:57] epoch 11519, training loss: 7948.33, average training loss: 7240.23, base loss: 16113.36
[INFO 2017-06-29 10:12:09,553 main.py:57] epoch 11520, training loss: 6512.36, average training loss: 7239.73, base loss: 16113.03
[INFO 2017-06-29 10:12:12,581 main.py:57] epoch 11521, training loss: 7916.10, average training loss: 7240.19, base loss: 16113.18
[INFO 2017-06-29 10:12:15,640 main.py:57] epoch 11522, training loss: 6738.40, average training loss: 7240.27, base loss: 16112.99
[INFO 2017-06-29 10:12:18,735 main.py:57] epoch 11523, training loss: 6703.40, average training loss: 7240.94, base loss: 16112.71
[INFO 2017-06-29 10:12:21,811 main.py:57] epoch 11524, training loss: 7340.36, average training loss: 7240.74, base loss: 16112.37
[INFO 2017-06-29 10:12:24,889 main.py:57] epoch 11525, training loss: 7459.87, average training loss: 7240.29, base loss: 16112.20
[INFO 2017-06-29 10:12:28,055 main.py:57] epoch 11526, training loss: 6875.35, average training loss: 7239.01, base loss: 16112.15
[INFO 2017-06-29 10:12:31,085 main.py:57] epoch 11527, training loss: 7164.70, average training loss: 7238.45, base loss: 16111.96
[INFO 2017-06-29 10:12:34,151 main.py:57] epoch 11528, training loss: 6773.60, average training loss: 7237.93, base loss: 16111.83
[INFO 2017-06-29 10:12:37,326 main.py:57] epoch 11529, training loss: 7405.14, average training loss: 7237.72, base loss: 16111.88
[INFO 2017-06-29 10:12:40,450 main.py:57] epoch 11530, training loss: 7136.52, average training loss: 7237.50, base loss: 16111.96
[INFO 2017-06-29 10:12:43,522 main.py:57] epoch 11531, training loss: 7226.88, average training loss: 7237.40, base loss: 16112.04
[INFO 2017-06-29 10:12:46,536 main.py:57] epoch 11532, training loss: 7173.00, average training loss: 7237.81, base loss: 16112.06
[INFO 2017-06-29 10:12:49,642 main.py:57] epoch 11533, training loss: 6366.62, average training loss: 7237.40, base loss: 16111.53
[INFO 2017-06-29 10:12:52,734 main.py:57] epoch 11534, training loss: 7004.68, average training loss: 7237.18, base loss: 16111.58
[INFO 2017-06-29 10:12:55,812 main.py:57] epoch 11535, training loss: 7672.48, average training loss: 7238.38, base loss: 16111.74
[INFO 2017-06-29 10:12:58,826 main.py:57] epoch 11536, training loss: 8888.82, average training loss: 7240.33, base loss: 16112.19
[INFO 2017-06-29 10:13:01,908 main.py:57] epoch 11537, training loss: 6133.06, average training loss: 7239.07, base loss: 16111.55
[INFO 2017-06-29 10:13:04,962 main.py:57] epoch 11538, training loss: 8013.26, average training loss: 7239.82, base loss: 16112.16
[INFO 2017-06-29 10:13:08,046 main.py:57] epoch 11539, training loss: 6199.00, average training loss: 7239.14, base loss: 16112.17
[INFO 2017-06-29 10:13:11,021 main.py:57] epoch 11540, training loss: 8050.42, average training loss: 7240.29, base loss: 16112.65
[INFO 2017-06-29 10:13:14,171 main.py:57] epoch 11541, training loss: 6419.23, average training loss: 7239.27, base loss: 16112.29
[INFO 2017-06-29 10:13:17,274 main.py:57] epoch 11542, training loss: 7286.00, average training loss: 7239.47, base loss: 16112.32
[INFO 2017-06-29 10:13:20,328 main.py:57] epoch 11543, training loss: 6848.76, average training loss: 7239.59, base loss: 16112.41
[INFO 2017-06-29 10:13:23,408 main.py:57] epoch 11544, training loss: 6732.76, average training loss: 7239.44, base loss: 16112.09
[INFO 2017-06-29 10:13:26,505 main.py:57] epoch 11545, training loss: 6764.17, average training loss: 7239.00, base loss: 16111.93
[INFO 2017-06-29 10:13:29,564 main.py:57] epoch 11546, training loss: 6584.81, average training loss: 7238.97, base loss: 16111.80
[INFO 2017-06-29 10:13:32,631 main.py:57] epoch 11547, training loss: 6924.25, average training loss: 7238.65, base loss: 16111.77
[INFO 2017-06-29 10:13:35,647 main.py:57] epoch 11548, training loss: 6757.93, average training loss: 7238.61, base loss: 16111.86
[INFO 2017-06-29 10:13:38,728 main.py:57] epoch 11549, training loss: 6898.13, average training loss: 7238.34, base loss: 16112.09
[INFO 2017-06-29 10:13:41,750 main.py:57] epoch 11550, training loss: 8313.86, average training loss: 7239.39, base loss: 16112.15
[INFO 2017-06-29 10:13:44,781 main.py:57] epoch 11551, training loss: 7389.50, average training loss: 7239.08, base loss: 16112.12
[INFO 2017-06-29 10:13:47,823 main.py:57] epoch 11552, training loss: 8071.05, average training loss: 7239.54, base loss: 16112.48
[INFO 2017-06-29 10:13:50,825 main.py:57] epoch 11553, training loss: 6815.90, average training loss: 7239.60, base loss: 16112.24
[INFO 2017-06-29 10:13:53,902 main.py:57] epoch 11554, training loss: 7186.06, average training loss: 7238.98, base loss: 16112.52
[INFO 2017-06-29 10:13:56,936 main.py:57] epoch 11555, training loss: 6883.92, average training loss: 7239.38, base loss: 16112.10
[INFO 2017-06-29 10:13:59,981 main.py:57] epoch 11556, training loss: 7688.74, average training loss: 7239.97, base loss: 16112.22
[INFO 2017-06-29 10:14:03,091 main.py:57] epoch 11557, training loss: 6591.63, average training loss: 7239.55, base loss: 16111.76
[INFO 2017-06-29 10:14:06,110 main.py:57] epoch 11558, training loss: 7570.33, average training loss: 7240.04, base loss: 16112.09
[INFO 2017-06-29 10:14:09,142 main.py:57] epoch 11559, training loss: 7482.87, average training loss: 7239.60, base loss: 16112.36
[INFO 2017-06-29 10:14:12,227 main.py:57] epoch 11560, training loss: 8037.07, average training loss: 7239.72, base loss: 16112.55
[INFO 2017-06-29 10:14:15,297 main.py:57] epoch 11561, training loss: 8331.99, average training loss: 7241.44, base loss: 16112.74
[INFO 2017-06-29 10:14:18,340 main.py:57] epoch 11562, training loss: 7590.71, average training loss: 7242.19, base loss: 16112.78
[INFO 2017-06-29 10:14:21,398 main.py:57] epoch 11563, training loss: 7325.94, average training loss: 7242.36, base loss: 16112.80
[INFO 2017-06-29 10:14:24,447 main.py:57] epoch 11564, training loss: 7483.39, average training loss: 7243.05, base loss: 16112.73
[INFO 2017-06-29 10:14:27,483 main.py:57] epoch 11565, training loss: 7398.27, average training loss: 7242.91, base loss: 16112.54
[INFO 2017-06-29 10:14:30,546 main.py:57] epoch 11566, training loss: 6840.70, average training loss: 7242.25, base loss: 16112.42
[INFO 2017-06-29 10:14:33,566 main.py:57] epoch 11567, training loss: 7544.48, average training loss: 7243.02, base loss: 16112.54
[INFO 2017-06-29 10:14:36,626 main.py:57] epoch 11568, training loss: 9248.86, average training loss: 7245.10, base loss: 16113.30
[INFO 2017-06-29 10:14:39,660 main.py:57] epoch 11569, training loss: 7475.04, average training loss: 7244.47, base loss: 16113.57
[INFO 2017-06-29 10:14:42,687 main.py:57] epoch 11570, training loss: 6849.22, average training loss: 7244.52, base loss: 16113.63
[INFO 2017-06-29 10:14:45,740 main.py:57] epoch 11571, training loss: 8463.64, average training loss: 7246.08, base loss: 16114.11
[INFO 2017-06-29 10:14:48,796 main.py:57] epoch 11572, training loss: 7218.47, average training loss: 7246.44, base loss: 16114.13
[INFO 2017-06-29 10:14:51,811 main.py:57] epoch 11573, training loss: 7881.96, average training loss: 7247.50, base loss: 16114.09
[INFO 2017-06-29 10:14:54,957 main.py:57] epoch 11574, training loss: 8153.51, average training loss: 7248.49, base loss: 16114.28
[INFO 2017-06-29 10:14:58,015 main.py:57] epoch 11575, training loss: 7551.35, average training loss: 7247.98, base loss: 16114.14
[INFO 2017-06-29 10:15:01,055 main.py:57] epoch 11576, training loss: 7737.71, average training loss: 7248.65, base loss: 16114.12
[INFO 2017-06-29 10:15:04,073 main.py:57] epoch 11577, training loss: 7581.18, average training loss: 7249.18, base loss: 16114.53
[INFO 2017-06-29 10:15:07,088 main.py:57] epoch 11578, training loss: 7161.36, average training loss: 7249.23, base loss: 16114.51
[INFO 2017-06-29 10:15:10,194 main.py:57] epoch 11579, training loss: 7233.66, average training loss: 7250.37, base loss: 16114.48
[INFO 2017-06-29 10:15:13,206 main.py:57] epoch 11580, training loss: 7706.37, average training loss: 7250.27, base loss: 16114.62
[INFO 2017-06-29 10:15:16,237 main.py:57] epoch 11581, training loss: 7956.66, average training loss: 7251.87, base loss: 16114.93
[INFO 2017-06-29 10:15:19,286 main.py:57] epoch 11582, training loss: 7957.48, average training loss: 7251.66, base loss: 16115.57
[INFO 2017-06-29 10:15:22,377 main.py:57] epoch 11583, training loss: 7217.59, average training loss: 7252.48, base loss: 16115.43
[INFO 2017-06-29 10:15:25,440 main.py:57] epoch 11584, training loss: 7078.70, average training loss: 7251.80, base loss: 16115.27
[INFO 2017-06-29 10:15:28,527 main.py:57] epoch 11585, training loss: 7268.72, average training loss: 7251.44, base loss: 16115.25
[INFO 2017-06-29 10:15:31,580 main.py:57] epoch 11586, training loss: 7634.12, average training loss: 7251.58, base loss: 16115.10
[INFO 2017-06-29 10:15:34,611 main.py:57] epoch 11587, training loss: 7626.79, average training loss: 7251.94, base loss: 16115.19
[INFO 2017-06-29 10:15:37,723 main.py:57] epoch 11588, training loss: 7101.84, average training loss: 7251.51, base loss: 16115.20
[INFO 2017-06-29 10:15:40,882 main.py:57] epoch 11589, training loss: 7153.00, average training loss: 7251.48, base loss: 16115.11
[INFO 2017-06-29 10:15:43,883 main.py:57] epoch 11590, training loss: 6689.01, average training loss: 7252.31, base loss: 16115.13
[INFO 2017-06-29 10:15:47,004 main.py:57] epoch 11591, training loss: 6685.17, average training loss: 7252.56, base loss: 16115.05
[INFO 2017-06-29 10:15:50,120 main.py:57] epoch 11592, training loss: 8336.53, average training loss: 7253.02, base loss: 16115.44
[INFO 2017-06-29 10:15:53,180 main.py:57] epoch 11593, training loss: 7448.80, average training loss: 7252.54, base loss: 16115.47
[INFO 2017-06-29 10:15:56,187 main.py:57] epoch 11594, training loss: 7236.68, average training loss: 7252.79, base loss: 16115.43
[INFO 2017-06-29 10:15:59,243 main.py:57] epoch 11595, training loss: 6986.98, average training loss: 7253.05, base loss: 16115.21
[INFO 2017-06-29 10:16:02,349 main.py:57] epoch 11596, training loss: 6603.71, average training loss: 7253.14, base loss: 16115.30
[INFO 2017-06-29 10:16:05,413 main.py:57] epoch 11597, training loss: 6334.48, average training loss: 7251.79, base loss: 16115.41
[INFO 2017-06-29 10:16:08,433 main.py:57] epoch 11598, training loss: 7555.36, average training loss: 7251.60, base loss: 16115.40
[INFO 2017-06-29 10:16:11,499 main.py:57] epoch 11599, training loss: 7162.85, average training loss: 7250.72, base loss: 16115.23
[INFO 2017-06-29 10:16:11,499 main.py:59] epoch 11599, testing
[INFO 2017-06-29 10:16:24,179 main.py:104] average testing loss: 7555.51, base loss: 16171.01
[INFO 2017-06-29 10:16:24,179 main.py:105] improve_loss: 8615.50, improve_percent: 0.53
[INFO 2017-06-29 10:16:24,181 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 10:16:24,219 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:16:27,219 main.py:57] epoch 11600, training loss: 7653.40, average training loss: 7251.29, base loss: 16115.34
[INFO 2017-06-29 10:16:30,313 main.py:57] epoch 11601, training loss: 8262.99, average training loss: 7251.86, base loss: 16116.02
[INFO 2017-06-29 10:16:33,348 main.py:57] epoch 11602, training loss: 7899.17, average training loss: 7252.18, base loss: 16115.91
[INFO 2017-06-29 10:16:36,306 main.py:57] epoch 11603, training loss: 6518.71, average training loss: 7251.40, base loss: 16115.37
[INFO 2017-06-29 10:16:39,380 main.py:57] epoch 11604, training loss: 6690.96, average training loss: 7251.17, base loss: 16115.44
[INFO 2017-06-29 10:16:42,367 main.py:57] epoch 11605, training loss: 6783.76, average training loss: 7250.07, base loss: 16115.15
[INFO 2017-06-29 10:16:45,428 main.py:57] epoch 11606, training loss: 7638.06, average training loss: 7251.04, base loss: 16115.44
[INFO 2017-06-29 10:16:48,514 main.py:57] epoch 11607, training loss: 6740.29, average training loss: 7248.21, base loss: 16115.22
[INFO 2017-06-29 10:16:51,484 main.py:57] epoch 11608, training loss: 7726.05, average training loss: 7248.61, base loss: 16115.44
[INFO 2017-06-29 10:16:54,516 main.py:57] epoch 11609, training loss: 6786.57, average training loss: 7247.84, base loss: 16115.47
[INFO 2017-06-29 10:16:57,528 main.py:57] epoch 11610, training loss: 7238.63, average training loss: 7247.26, base loss: 16115.26
[INFO 2017-06-29 10:17:00,631 main.py:57] epoch 11611, training loss: 6804.22, average training loss: 7246.73, base loss: 16114.81
[INFO 2017-06-29 10:17:03,705 main.py:57] epoch 11612, training loss: 8734.31, average training loss: 7248.04, base loss: 16115.38
[INFO 2017-06-29 10:17:06,727 main.py:57] epoch 11613, training loss: 6698.52, average training loss: 7247.66, base loss: 16115.24
[INFO 2017-06-29 10:17:09,782 main.py:57] epoch 11614, training loss: 7709.86, average training loss: 7248.26, base loss: 16115.22
[INFO 2017-06-29 10:17:12,851 main.py:57] epoch 11615, training loss: 7520.68, average training loss: 7249.06, base loss: 16115.15
[INFO 2017-06-29 10:17:15,856 main.py:57] epoch 11616, training loss: 7187.13, average training loss: 7249.04, base loss: 16115.33
[INFO 2017-06-29 10:17:18,900 main.py:57] epoch 11617, training loss: 6709.66, average training loss: 7249.55, base loss: 16115.01
[INFO 2017-06-29 10:17:21,938 main.py:57] epoch 11618, training loss: 7401.29, average training loss: 7249.83, base loss: 16114.75
[INFO 2017-06-29 10:17:24,983 main.py:57] epoch 11619, training loss: 6747.54, average training loss: 7249.65, base loss: 16114.38
[INFO 2017-06-29 10:17:27,997 main.py:57] epoch 11620, training loss: 6907.38, average training loss: 7248.06, base loss: 16114.17
[INFO 2017-06-29 10:17:31,042 main.py:57] epoch 11621, training loss: 8149.66, average training loss: 7248.90, base loss: 16114.51
[INFO 2017-06-29 10:17:34,086 main.py:57] epoch 11622, training loss: 7670.37, average training loss: 7248.84, base loss: 16114.56
[INFO 2017-06-29 10:17:37,125 main.py:57] epoch 11623, training loss: 7130.20, average training loss: 7249.13, base loss: 16114.66
[INFO 2017-06-29 10:17:40,144 main.py:57] epoch 11624, training loss: 6239.04, average training loss: 7248.91, base loss: 16114.13
[INFO 2017-06-29 10:17:43,185 main.py:57] epoch 11625, training loss: 7522.55, average training loss: 7250.42, base loss: 16114.38
[INFO 2017-06-29 10:17:46,246 main.py:57] epoch 11626, training loss: 6859.21, average training loss: 7250.45, base loss: 16114.32
[INFO 2017-06-29 10:17:49,319 main.py:57] epoch 11627, training loss: 7347.51, average training loss: 7250.93, base loss: 16114.52
[INFO 2017-06-29 10:17:52,359 main.py:57] epoch 11628, training loss: 7235.24, average training loss: 7250.82, base loss: 16114.51
[INFO 2017-06-29 10:17:55,458 main.py:57] epoch 11629, training loss: 7603.89, average training loss: 7250.63, base loss: 16114.66
[INFO 2017-06-29 10:17:58,464 main.py:57] epoch 11630, training loss: 7088.45, average training loss: 7249.70, base loss: 16114.68
[INFO 2017-06-29 10:18:01,490 main.py:57] epoch 11631, training loss: 6921.02, average training loss: 7248.98, base loss: 16114.34
[INFO 2017-06-29 10:18:04,527 main.py:57] epoch 11632, training loss: 6682.55, average training loss: 7248.42, base loss: 16114.30
[INFO 2017-06-29 10:18:07,589 main.py:57] epoch 11633, training loss: 7325.25, average training loss: 7248.31, base loss: 16114.39
[INFO 2017-06-29 10:18:10,671 main.py:57] epoch 11634, training loss: 7156.42, average training loss: 7249.26, base loss: 16114.25
[INFO 2017-06-29 10:18:13,787 main.py:57] epoch 11635, training loss: 7953.24, average training loss: 7250.10, base loss: 16114.64
[INFO 2017-06-29 10:18:16,832 main.py:57] epoch 11636, training loss: 7226.76, average training loss: 7250.09, base loss: 16114.24
[INFO 2017-06-29 10:18:19,845 main.py:57] epoch 11637, training loss: 7433.69, average training loss: 7250.43, base loss: 16113.98
[INFO 2017-06-29 10:18:22,905 main.py:57] epoch 11638, training loss: 6934.33, average training loss: 7249.88, base loss: 16113.79
[INFO 2017-06-29 10:18:25,918 main.py:57] epoch 11639, training loss: 6719.83, average training loss: 7249.58, base loss: 16113.46
[INFO 2017-06-29 10:18:28,940 main.py:57] epoch 11640, training loss: 8153.84, average training loss: 7250.50, base loss: 16113.57
[INFO 2017-06-29 10:18:31,972 main.py:57] epoch 11641, training loss: 7337.43, average training loss: 7250.35, base loss: 16113.45
[INFO 2017-06-29 10:18:35,056 main.py:57] epoch 11642, training loss: 6275.64, average training loss: 7250.26, base loss: 16113.40
[INFO 2017-06-29 10:18:38,133 main.py:57] epoch 11643, training loss: 8278.23, average training loss: 7251.22, base loss: 16113.95
[INFO 2017-06-29 10:18:41,186 main.py:57] epoch 11644, training loss: 7901.86, average training loss: 7252.20, base loss: 16114.12
[INFO 2017-06-29 10:18:44,231 main.py:57] epoch 11645, training loss: 7692.86, average training loss: 7250.82, base loss: 16114.48
[INFO 2017-06-29 10:18:47,274 main.py:57] epoch 11646, training loss: 7118.92, average training loss: 7250.23, base loss: 16114.42
[INFO 2017-06-29 10:18:50,263 main.py:57] epoch 11647, training loss: 7138.47, average training loss: 7249.18, base loss: 16114.31
[INFO 2017-06-29 10:18:53,304 main.py:57] epoch 11648, training loss: 7238.74, average training loss: 7249.65, base loss: 16114.56
[INFO 2017-06-29 10:18:56,366 main.py:57] epoch 11649, training loss: 7280.22, average training loss: 7249.60, base loss: 16114.59
[INFO 2017-06-29 10:18:59,448 main.py:57] epoch 11650, training loss: 6450.39, average training loss: 7247.71, base loss: 16114.41
[INFO 2017-06-29 10:19:02,504 main.py:57] epoch 11651, training loss: 7930.08, average training loss: 7248.27, base loss: 16114.79
[INFO 2017-06-29 10:19:05,539 main.py:57] epoch 11652, training loss: 6703.71, average training loss: 7247.86, base loss: 16114.54
[INFO 2017-06-29 10:19:08,575 main.py:57] epoch 11653, training loss: 6308.73, average training loss: 7246.20, base loss: 16114.29
[INFO 2017-06-29 10:19:11,719 main.py:57] epoch 11654, training loss: 7553.33, average training loss: 7245.98, base loss: 16114.66
[INFO 2017-06-29 10:19:14,854 main.py:57] epoch 11655, training loss: 7467.55, average training loss: 7246.81, base loss: 16114.72
[INFO 2017-06-29 10:19:17,861 main.py:57] epoch 11656, training loss: 6955.39, average training loss: 7246.39, base loss: 16114.49
[INFO 2017-06-29 10:19:20,974 main.py:57] epoch 11657, training loss: 7485.48, average training loss: 7245.76, base loss: 16114.47
[INFO 2017-06-29 10:19:24,043 main.py:57] epoch 11658, training loss: 6750.98, average training loss: 7245.04, base loss: 16114.46
[INFO 2017-06-29 10:19:27,113 main.py:57] epoch 11659, training loss: 6930.41, average training loss: 7244.19, base loss: 16114.46
[INFO 2017-06-29 10:19:30,165 main.py:57] epoch 11660, training loss: 7007.76, average training loss: 7244.32, base loss: 16114.39
[INFO 2017-06-29 10:19:33,168 main.py:57] epoch 11661, training loss: 7063.78, average training loss: 7244.81, base loss: 16114.42
[INFO 2017-06-29 10:19:36,223 main.py:57] epoch 11662, training loss: 7435.54, average training loss: 7244.99, base loss: 16114.37
[INFO 2017-06-29 10:19:39,308 main.py:57] epoch 11663, training loss: 6910.17, average training loss: 7243.05, base loss: 16114.21
[INFO 2017-06-29 10:19:42,388 main.py:57] epoch 11664, training loss: 7157.89, average training loss: 7242.96, base loss: 16114.21
[INFO 2017-06-29 10:19:45,448 main.py:57] epoch 11665, training loss: 6784.09, average training loss: 7242.03, base loss: 16113.90
[INFO 2017-06-29 10:19:48,511 main.py:57] epoch 11666, training loss: 6815.36, average training loss: 7241.61, base loss: 16113.91
[INFO 2017-06-29 10:19:51,534 main.py:57] epoch 11667, training loss: 6582.14, average training loss: 7241.69, base loss: 16113.71
[INFO 2017-06-29 10:19:54,547 main.py:57] epoch 11668, training loss: 7198.94, average training loss: 7241.55, base loss: 16113.83
[INFO 2017-06-29 10:19:57,615 main.py:57] epoch 11669, training loss: 6754.76, average training loss: 7241.31, base loss: 16113.85
[INFO 2017-06-29 10:20:00,678 main.py:57] epoch 11670, training loss: 6543.26, average training loss: 7240.67, base loss: 16113.43
[INFO 2017-06-29 10:20:03,727 main.py:57] epoch 11671, training loss: 7139.17, average training loss: 7240.58, base loss: 16113.07
[INFO 2017-06-29 10:20:06,775 main.py:57] epoch 11672, training loss: 7510.04, average training loss: 7241.41, base loss: 16113.35
[INFO 2017-06-29 10:20:09,852 main.py:57] epoch 11673, training loss: 6511.54, average training loss: 7240.97, base loss: 16113.31
[INFO 2017-06-29 10:20:12,937 main.py:57] epoch 11674, training loss: 6870.65, average training loss: 7240.60, base loss: 16113.27
[INFO 2017-06-29 10:20:15,981 main.py:57] epoch 11675, training loss: 7507.22, average training loss: 7241.77, base loss: 16113.43
[INFO 2017-06-29 10:20:19,065 main.py:57] epoch 11676, training loss: 8808.12, average training loss: 7243.30, base loss: 16114.04
[INFO 2017-06-29 10:20:22,189 main.py:57] epoch 11677, training loss: 6869.33, average training loss: 7243.00, base loss: 16113.97
[INFO 2017-06-29 10:20:25,263 main.py:57] epoch 11678, training loss: 6766.93, average training loss: 7242.94, base loss: 16113.74
[INFO 2017-06-29 10:20:28,421 main.py:57] epoch 11679, training loss: 7945.64, average training loss: 7243.80, base loss: 16113.85
[INFO 2017-06-29 10:20:31,436 main.py:57] epoch 11680, training loss: 7517.90, average training loss: 7244.03, base loss: 16113.92
[INFO 2017-06-29 10:20:34,495 main.py:57] epoch 11681, training loss: 7028.09, average training loss: 7243.12, base loss: 16113.64
[INFO 2017-06-29 10:20:37,529 main.py:57] epoch 11682, training loss: 7159.49, average training loss: 7242.90, base loss: 16113.94
[INFO 2017-06-29 10:20:40,599 main.py:57] epoch 11683, training loss: 8130.98, average training loss: 7244.13, base loss: 16114.04
[INFO 2017-06-29 10:20:43,654 main.py:57] epoch 11684, training loss: 6311.15, average training loss: 7243.08, base loss: 16113.93
[INFO 2017-06-29 10:20:46,693 main.py:57] epoch 11685, training loss: 6130.46, average training loss: 7242.30, base loss: 16113.58
[INFO 2017-06-29 10:20:49,745 main.py:57] epoch 11686, training loss: 7545.13, average training loss: 7243.28, base loss: 16113.56
[INFO 2017-06-29 10:20:52,817 main.py:57] epoch 11687, training loss: 8138.62, average training loss: 7244.32, base loss: 16113.74
[INFO 2017-06-29 10:20:55,857 main.py:57] epoch 11688, training loss: 7147.60, average training loss: 7244.62, base loss: 16113.42
[INFO 2017-06-29 10:20:58,872 main.py:57] epoch 11689, training loss: 7264.23, average training loss: 7244.88, base loss: 16113.10
[INFO 2017-06-29 10:21:01,937 main.py:57] epoch 11690, training loss: 6998.82, average training loss: 7245.56, base loss: 16113.09
[INFO 2017-06-29 10:21:04,943 main.py:57] epoch 11691, training loss: 6905.62, average training loss: 7245.64, base loss: 16113.09
[INFO 2017-06-29 10:21:08,030 main.py:57] epoch 11692, training loss: 6762.16, average training loss: 7244.77, base loss: 16113.12
[INFO 2017-06-29 10:21:11,223 main.py:57] epoch 11693, training loss: 5950.24, average training loss: 7244.04, base loss: 16112.90
[INFO 2017-06-29 10:21:14,288 main.py:57] epoch 11694, training loss: 8506.58, average training loss: 7245.72, base loss: 16113.17
[INFO 2017-06-29 10:21:17,326 main.py:57] epoch 11695, training loss: 6717.52, average training loss: 7244.87, base loss: 16112.97
[INFO 2017-06-29 10:21:20,375 main.py:57] epoch 11696, training loss: 7562.37, average training loss: 7245.09, base loss: 16113.24
[INFO 2017-06-29 10:21:23,434 main.py:57] epoch 11697, training loss: 7580.31, average training loss: 7244.79, base loss: 16113.58
[INFO 2017-06-29 10:21:26,600 main.py:57] epoch 11698, training loss: 7184.16, average training loss: 7243.95, base loss: 16113.43
[INFO 2017-06-29 10:21:29,654 main.py:57] epoch 11699, training loss: 7504.30, average training loss: 7245.25, base loss: 16113.33
[INFO 2017-06-29 10:21:29,655 main.py:59] epoch 11699, testing
[INFO 2017-06-29 10:21:42,325 main.py:104] average testing loss: 7916.92, base loss: 16370.26
[INFO 2017-06-29 10:21:42,325 main.py:105] improve_loss: 8453.34, improve_percent: 0.52
[INFO 2017-06-29 10:21:42,327 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:21:45,456 main.py:57] epoch 11700, training loss: 7809.35, average training loss: 7244.82, base loss: 16113.46
[INFO 2017-06-29 10:21:48,466 main.py:57] epoch 11701, training loss: 7177.93, average training loss: 7244.40, base loss: 16113.48
[INFO 2017-06-29 10:21:51,505 main.py:57] epoch 11702, training loss: 8363.26, average training loss: 7245.91, base loss: 16113.60
[INFO 2017-06-29 10:21:54,559 main.py:57] epoch 11703, training loss: 6650.99, average training loss: 7245.50, base loss: 16113.30
[INFO 2017-06-29 10:21:57,575 main.py:57] epoch 11704, training loss: 6612.02, average training loss: 7246.24, base loss: 16112.98
[INFO 2017-06-29 10:22:00,684 main.py:57] epoch 11705, training loss: 7988.79, average training loss: 7247.70, base loss: 16113.25
[INFO 2017-06-29 10:22:03,760 main.py:57] epoch 11706, training loss: 8868.15, average training loss: 7249.38, base loss: 16113.78
[INFO 2017-06-29 10:22:06,826 main.py:57] epoch 11707, training loss: 6993.37, average training loss: 7248.10, base loss: 16113.78
[INFO 2017-06-29 10:22:09,923 main.py:57] epoch 11708, training loss: 6613.52, average training loss: 7247.60, base loss: 16113.74
[INFO 2017-06-29 10:22:13,002 main.py:57] epoch 11709, training loss: 7289.09, average training loss: 7247.43, base loss: 16113.96
[INFO 2017-06-29 10:22:16,007 main.py:57] epoch 11710, training loss: 6517.21, average training loss: 7247.08, base loss: 16113.79
[INFO 2017-06-29 10:22:19,031 main.py:57] epoch 11711, training loss: 7640.38, average training loss: 7247.29, base loss: 16113.83
[INFO 2017-06-29 10:22:22,091 main.py:57] epoch 11712, training loss: 6678.20, average training loss: 7247.22, base loss: 16113.78
[INFO 2017-06-29 10:22:25,160 main.py:57] epoch 11713, training loss: 6921.88, average training loss: 7247.42, base loss: 16113.92
[INFO 2017-06-29 10:22:28,213 main.py:57] epoch 11714, training loss: 7875.42, average training loss: 7248.24, base loss: 16114.06
[INFO 2017-06-29 10:22:31,368 main.py:57] epoch 11715, training loss: 7082.51, average training loss: 7248.66, base loss: 16114.19
[INFO 2017-06-29 10:22:34,369 main.py:57] epoch 11716, training loss: 8079.68, average training loss: 7249.47, base loss: 16114.83
[INFO 2017-06-29 10:22:37,406 main.py:57] epoch 11717, training loss: 7325.19, average training loss: 7249.46, base loss: 16115.08
[INFO 2017-06-29 10:22:40,510 main.py:57] epoch 11718, training loss: 7505.21, average training loss: 7248.65, base loss: 16115.12
[INFO 2017-06-29 10:22:43,545 main.py:57] epoch 11719, training loss: 6822.94, average training loss: 7249.27, base loss: 16114.96
[INFO 2017-06-29 10:22:46,635 main.py:57] epoch 11720, training loss: 6973.83, average training loss: 7249.61, base loss: 16114.95
[INFO 2017-06-29 10:22:49,710 main.py:57] epoch 11721, training loss: 6800.11, average training loss: 7248.95, base loss: 16114.74
[INFO 2017-06-29 10:22:52,792 main.py:57] epoch 11722, training loss: 6769.72, average training loss: 7248.65, base loss: 16114.55
[INFO 2017-06-29 10:22:55,854 main.py:57] epoch 11723, training loss: 6312.01, average training loss: 7248.25, base loss: 16114.11
[INFO 2017-06-29 10:22:58,897 main.py:57] epoch 11724, training loss: 7759.20, average training loss: 7248.82, base loss: 16114.19
[INFO 2017-06-29 10:23:02,003 main.py:57] epoch 11725, training loss: 6121.56, average training loss: 7247.15, base loss: 16113.59
[INFO 2017-06-29 10:23:05,110 main.py:57] epoch 11726, training loss: 7902.74, average training loss: 7247.89, base loss: 16113.64
[INFO 2017-06-29 10:23:08,120 main.py:57] epoch 11727, training loss: 6839.15, average training loss: 7247.37, base loss: 16113.38
[INFO 2017-06-29 10:23:11,138 main.py:57] epoch 11728, training loss: 6920.63, average training loss: 7246.98, base loss: 16113.31
[INFO 2017-06-29 10:23:14,146 main.py:57] epoch 11729, training loss: 7674.11, average training loss: 7247.59, base loss: 16113.47
[INFO 2017-06-29 10:23:17,218 main.py:57] epoch 11730, training loss: 6790.16, average training loss: 7247.91, base loss: 16113.36
[INFO 2017-06-29 10:23:20,288 main.py:57] epoch 11731, training loss: 7354.48, average training loss: 7248.88, base loss: 16113.65
[INFO 2017-06-29 10:23:23,262 main.py:57] epoch 11732, training loss: 7049.16, average training loss: 7248.79, base loss: 16113.56
[INFO 2017-06-29 10:23:26,311 main.py:57] epoch 11733, training loss: 7180.84, average training loss: 7248.34, base loss: 16113.37
[INFO 2017-06-29 10:23:29,376 main.py:57] epoch 11734, training loss: 7132.73, average training loss: 7248.35, base loss: 16113.53
[INFO 2017-06-29 10:23:32,397 main.py:57] epoch 11735, training loss: 7004.21, average training loss: 7248.01, base loss: 16113.35
[INFO 2017-06-29 10:23:35,518 main.py:57] epoch 11736, training loss: 7154.76, average training loss: 7247.80, base loss: 16113.45
[INFO 2017-06-29 10:23:38,584 main.py:57] epoch 11737, training loss: 8311.05, average training loss: 7249.21, base loss: 16113.58
[INFO 2017-06-29 10:23:41,654 main.py:57] epoch 11738, training loss: 7658.87, average training loss: 7250.21, base loss: 16113.69
[INFO 2017-06-29 10:23:44,714 main.py:57] epoch 11739, training loss: 7204.31, average training loss: 7249.79, base loss: 16113.83
[INFO 2017-06-29 10:23:47,729 main.py:57] epoch 11740, training loss: 6285.05, average training loss: 7248.66, base loss: 16113.38
[INFO 2017-06-29 10:23:50,814 main.py:57] epoch 11741, training loss: 7097.98, average training loss: 7249.28, base loss: 16113.13
[INFO 2017-06-29 10:23:53,871 main.py:57] epoch 11742, training loss: 7633.22, average training loss: 7249.30, base loss: 16113.46
[INFO 2017-06-29 10:23:57,019 main.py:57] epoch 11743, training loss: 7403.24, average training loss: 7249.24, base loss: 16113.51
[INFO 2017-06-29 10:24:00,102 main.py:57] epoch 11744, training loss: 7140.29, average training loss: 7249.43, base loss: 16113.39
[INFO 2017-06-29 10:24:03,209 main.py:57] epoch 11745, training loss: 6408.13, average training loss: 7248.66, base loss: 16113.18
[INFO 2017-06-29 10:24:06,274 main.py:57] epoch 11746, training loss: 7726.24, average training loss: 7248.92, base loss: 16113.89
[INFO 2017-06-29 10:24:09,335 main.py:57] epoch 11747, training loss: 7260.92, average training loss: 7248.95, base loss: 16114.07
[INFO 2017-06-29 10:24:12,353 main.py:57] epoch 11748, training loss: 7197.93, average training loss: 7249.11, base loss: 16113.93
[INFO 2017-06-29 10:24:15,386 main.py:57] epoch 11749, training loss: 7153.75, average training loss: 7248.40, base loss: 16113.76
[INFO 2017-06-29 10:24:18,465 main.py:57] epoch 11750, training loss: 7165.82, average training loss: 7248.21, base loss: 16113.77
[INFO 2017-06-29 10:24:21,537 main.py:57] epoch 11751, training loss: 6628.66, average training loss: 7248.25, base loss: 16113.34
[INFO 2017-06-29 10:24:24,525 main.py:57] epoch 11752, training loss: 6549.27, average training loss: 7247.11, base loss: 16112.91
[INFO 2017-06-29 10:24:27,602 main.py:57] epoch 11753, training loss: 6515.66, average training loss: 7247.00, base loss: 16112.61
[INFO 2017-06-29 10:24:30,643 main.py:57] epoch 11754, training loss: 7981.12, average training loss: 7247.67, base loss: 16112.92
[INFO 2017-06-29 10:24:33,750 main.py:57] epoch 11755, training loss: 7200.21, average training loss: 7248.04, base loss: 16112.85
[INFO 2017-06-29 10:24:36,857 main.py:57] epoch 11756, training loss: 7457.60, average training loss: 7249.41, base loss: 16112.70
[INFO 2017-06-29 10:24:39,890 main.py:57] epoch 11757, training loss: 7396.50, average training loss: 7249.56, base loss: 16112.75
[INFO 2017-06-29 10:24:42,939 main.py:57] epoch 11758, training loss: 7588.06, average training loss: 7250.22, base loss: 16112.94
[INFO 2017-06-29 10:24:46,080 main.py:57] epoch 11759, training loss: 6615.96, average training loss: 7249.87, base loss: 16112.92
[INFO 2017-06-29 10:24:49,109 main.py:57] epoch 11760, training loss: 6445.95, average training loss: 7249.75, base loss: 16112.63
[INFO 2017-06-29 10:24:52,189 main.py:57] epoch 11761, training loss: 7755.38, average training loss: 7250.54, base loss: 16112.99
[INFO 2017-06-29 10:24:55,196 main.py:57] epoch 11762, training loss: 6245.15, average training loss: 7249.81, base loss: 16112.77
[INFO 2017-06-29 10:24:58,232 main.py:57] epoch 11763, training loss: 7046.45, average training loss: 7249.26, base loss: 16112.73
[INFO 2017-06-29 10:25:01,290 main.py:57] epoch 11764, training loss: 7533.64, average training loss: 7248.90, base loss: 16112.75
[INFO 2017-06-29 10:25:04,322 main.py:57] epoch 11765, training loss: 6301.39, average training loss: 7248.09, base loss: 16112.38
[INFO 2017-06-29 10:25:07,350 main.py:57] epoch 11766, training loss: 6663.27, average training loss: 7248.21, base loss: 16112.22
[INFO 2017-06-29 10:25:10,358 main.py:57] epoch 11767, training loss: 7372.88, average training loss: 7248.42, base loss: 16112.27
[INFO 2017-06-29 10:25:13,483 main.py:57] epoch 11768, training loss: 6859.47, average training loss: 7247.79, base loss: 16112.31
[INFO 2017-06-29 10:25:16,498 main.py:57] epoch 11769, training loss: 8358.61, average training loss: 7248.85, base loss: 16112.60
[INFO 2017-06-29 10:25:19,649 main.py:57] epoch 11770, training loss: 6958.30, average training loss: 7248.83, base loss: 16112.14
[INFO 2017-06-29 10:25:22,712 main.py:57] epoch 11771, training loss: 8311.58, average training loss: 7250.03, base loss: 16112.03
[INFO 2017-06-29 10:25:25,742 main.py:57] epoch 11772, training loss: 7402.60, average training loss: 7250.34, base loss: 16112.24
[INFO 2017-06-29 10:25:28,817 main.py:57] epoch 11773, training loss: 7129.85, average training loss: 7250.62, base loss: 16112.24
[INFO 2017-06-29 10:25:31,837 main.py:57] epoch 11774, training loss: 6879.83, average training loss: 7250.99, base loss: 16112.11
[INFO 2017-06-29 10:25:34,903 main.py:57] epoch 11775, training loss: 6431.26, average training loss: 7250.43, base loss: 16111.81
[INFO 2017-06-29 10:25:37,936 main.py:57] epoch 11776, training loss: 6543.67, average training loss: 7249.62, base loss: 16111.68
[INFO 2017-06-29 10:25:40,959 main.py:57] epoch 11777, training loss: 6787.81, average training loss: 7249.67, base loss: 16111.72
[INFO 2017-06-29 10:25:43,977 main.py:57] epoch 11778, training loss: 6425.23, average training loss: 7249.40, base loss: 16111.61
[INFO 2017-06-29 10:25:46,967 main.py:57] epoch 11779, training loss: 7525.00, average training loss: 7249.33, base loss: 16111.85
[INFO 2017-06-29 10:25:50,029 main.py:57] epoch 11780, training loss: 6606.90, average training loss: 7248.24, base loss: 16111.67
[INFO 2017-06-29 10:25:53,114 main.py:57] epoch 11781, training loss: 6145.79, average training loss: 7246.03, base loss: 16111.36
[INFO 2017-06-29 10:25:56,123 main.py:57] epoch 11782, training loss: 7193.16, average training loss: 7246.16, base loss: 16111.32
[INFO 2017-06-29 10:25:59,187 main.py:57] epoch 11783, training loss: 8238.72, average training loss: 7246.80, base loss: 16111.77
[INFO 2017-06-29 10:26:02,220 main.py:57] epoch 11784, training loss: 7417.11, average training loss: 7246.66, base loss: 16111.93
[INFO 2017-06-29 10:26:05,227 main.py:57] epoch 11785, training loss: 6448.33, average training loss: 7245.21, base loss: 16111.69
[INFO 2017-06-29 10:26:08,289 main.py:57] epoch 11786, training loss: 7415.70, average training loss: 7245.45, base loss: 16111.74
[INFO 2017-06-29 10:26:11,326 main.py:57] epoch 11787, training loss: 7298.38, average training loss: 7245.24, base loss: 16112.06
[INFO 2017-06-29 10:26:14,330 main.py:57] epoch 11788, training loss: 8734.35, average training loss: 7247.54, base loss: 16112.21
[INFO 2017-06-29 10:26:17,399 main.py:57] epoch 11789, training loss: 6705.30, average training loss: 7247.19, base loss: 16112.05
[INFO 2017-06-29 10:26:20,401 main.py:57] epoch 11790, training loss: 7343.71, average training loss: 7247.06, base loss: 16112.26
[INFO 2017-06-29 10:26:23,473 main.py:57] epoch 11791, training loss: 6504.88, average training loss: 7247.10, base loss: 16112.05
[INFO 2017-06-29 10:26:26,542 main.py:57] epoch 11792, training loss: 7484.10, average training loss: 7247.44, base loss: 16112.19
[INFO 2017-06-29 10:26:29,601 main.py:57] epoch 11793, training loss: 7107.33, average training loss: 7247.62, base loss: 16112.39
[INFO 2017-06-29 10:26:32,673 main.py:57] epoch 11794, training loss: 6800.80, average training loss: 7247.32, base loss: 16112.17
[INFO 2017-06-29 10:26:35,674 main.py:57] epoch 11795, training loss: 7062.94, average training loss: 7246.85, base loss: 16112.19
[INFO 2017-06-29 10:26:38,767 main.py:57] epoch 11796, training loss: 6977.00, average training loss: 7245.85, base loss: 16112.08
[INFO 2017-06-29 10:26:41,809 main.py:57] epoch 11797, training loss: 6212.98, average training loss: 7245.41, base loss: 16111.59
[INFO 2017-06-29 10:26:44,821 main.py:57] epoch 11798, training loss: 6842.20, average training loss: 7245.79, base loss: 16111.63
[INFO 2017-06-29 10:26:47,991 main.py:57] epoch 11799, training loss: 7366.50, average training loss: 7246.32, base loss: 16111.70
[INFO 2017-06-29 10:26:47,991 main.py:59] epoch 11799, testing
[INFO 2017-06-29 10:27:00,708 main.py:104] average testing loss: 8349.79, base loss: 17426.42
[INFO 2017-06-29 10:27:00,708 main.py:105] improve_loss: 9076.63, improve_percent: 0.52
[INFO 2017-06-29 10:27:00,709 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:27:03,745 main.py:57] epoch 11800, training loss: 6795.71, average training loss: 7245.62, base loss: 16111.68
[INFO 2017-06-29 10:27:06,755 main.py:57] epoch 11801, training loss: 8241.85, average training loss: 7247.16, base loss: 16111.90
[INFO 2017-06-29 10:27:09,808 main.py:57] epoch 11802, training loss: 7022.61, average training loss: 7246.26, base loss: 16112.13
[INFO 2017-06-29 10:27:12,882 main.py:57] epoch 11803, training loss: 6270.74, average training loss: 7245.99, base loss: 16112.02
[INFO 2017-06-29 10:27:15,900 main.py:57] epoch 11804, training loss: 6303.17, average training loss: 7245.84, base loss: 16111.87
[INFO 2017-06-29 10:27:18,940 main.py:57] epoch 11805, training loss: 6723.17, average training loss: 7245.83, base loss: 16111.78
[INFO 2017-06-29 10:27:21,965 main.py:57] epoch 11806, training loss: 6466.53, average training loss: 7245.70, base loss: 16111.70
[INFO 2017-06-29 10:27:25,106 main.py:57] epoch 11807, training loss: 8099.88, average training loss: 7246.84, base loss: 16112.22
[INFO 2017-06-29 10:27:28,161 main.py:57] epoch 11808, training loss: 7672.57, average training loss: 7247.95, base loss: 16112.17
[INFO 2017-06-29 10:27:31,194 main.py:57] epoch 11809, training loss: 7298.83, average training loss: 7248.23, base loss: 16112.06
[INFO 2017-06-29 10:27:34,287 main.py:57] epoch 11810, training loss: 7291.57, average training loss: 7248.45, base loss: 16111.85
[INFO 2017-06-29 10:27:37,460 main.py:57] epoch 11811, training loss: 7619.83, average training loss: 7249.23, base loss: 16111.82
[INFO 2017-06-29 10:27:40,535 main.py:57] epoch 11812, training loss: 7173.42, average training loss: 7249.82, base loss: 16111.72
[INFO 2017-06-29 10:27:43,588 main.py:57] epoch 11813, training loss: 7209.22, average training loss: 7250.71, base loss: 16111.93
[INFO 2017-06-29 10:27:46,655 main.py:57] epoch 11814, training loss: 7297.91, average training loss: 7250.93, base loss: 16111.87
[INFO 2017-06-29 10:27:49,678 main.py:57] epoch 11815, training loss: 7274.44, average training loss: 7249.78, base loss: 16111.85
[INFO 2017-06-29 10:27:52,699 main.py:57] epoch 11816, training loss: 6872.91, average training loss: 7248.96, base loss: 16111.59
[INFO 2017-06-29 10:27:55,695 main.py:57] epoch 11817, training loss: 7555.49, average training loss: 7249.77, base loss: 16111.76
[INFO 2017-06-29 10:27:58,678 main.py:57] epoch 11818, training loss: 7396.15, average training loss: 7249.39, base loss: 16111.65
[INFO 2017-06-29 10:28:01,697 main.py:57] epoch 11819, training loss: 8551.55, average training loss: 7250.44, base loss: 16111.60
[INFO 2017-06-29 10:28:04,719 main.py:57] epoch 11820, training loss: 7393.36, average training loss: 7250.26, base loss: 16111.48
[INFO 2017-06-29 10:28:07,792 main.py:57] epoch 11821, training loss: 6294.01, average training loss: 7249.54, base loss: 16111.14
[INFO 2017-06-29 10:28:10,882 main.py:57] epoch 11822, training loss: 7668.90, average training loss: 7249.99, base loss: 16111.28
[INFO 2017-06-29 10:28:13,940 main.py:57] epoch 11823, training loss: 6929.80, average training loss: 7249.32, base loss: 16110.95
[INFO 2017-06-29 10:28:17,008 main.py:57] epoch 11824, training loss: 7037.96, average training loss: 7249.69, base loss: 16111.03
[INFO 2017-06-29 10:28:20,021 main.py:57] epoch 11825, training loss: 7230.41, average training loss: 7249.99, base loss: 16110.85
[INFO 2017-06-29 10:28:23,040 main.py:57] epoch 11826, training loss: 8280.85, average training loss: 7251.90, base loss: 16110.82
[INFO 2017-06-29 10:28:26,065 main.py:57] epoch 11827, training loss: 8108.35, average training loss: 7253.62, base loss: 16110.83
[INFO 2017-06-29 10:28:29,197 main.py:57] epoch 11828, training loss: 7503.19, average training loss: 7253.43, base loss: 16110.73
[INFO 2017-06-29 10:28:32,179 main.py:57] epoch 11829, training loss: 6599.37, average training loss: 7252.79, base loss: 16110.50
[INFO 2017-06-29 10:28:35,296 main.py:57] epoch 11830, training loss: 6676.92, average training loss: 7252.44, base loss: 16110.45
[INFO 2017-06-29 10:28:38,429 main.py:57] epoch 11831, training loss: 7413.15, average training loss: 7251.90, base loss: 16110.22
[INFO 2017-06-29 10:28:41,483 main.py:57] epoch 11832, training loss: 6714.19, average training loss: 7250.24, base loss: 16109.93
[INFO 2017-06-29 10:28:44,601 main.py:57] epoch 11833, training loss: 7211.62, average training loss: 7250.72, base loss: 16109.70
[INFO 2017-06-29 10:28:47,581 main.py:57] epoch 11834, training loss: 7119.97, average training loss: 7249.45, base loss: 16109.64
[INFO 2017-06-29 10:28:50,649 main.py:57] epoch 11835, training loss: 7363.64, average training loss: 7250.19, base loss: 16109.43
[INFO 2017-06-29 10:28:53,673 main.py:57] epoch 11836, training loss: 6696.79, average training loss: 7250.23, base loss: 16109.35
[INFO 2017-06-29 10:28:56,743 main.py:57] epoch 11837, training loss: 6585.95, average training loss: 7249.56, base loss: 16109.33
[INFO 2017-06-29 10:28:59,806 main.py:57] epoch 11838, training loss: 6848.04, average training loss: 7248.08, base loss: 16109.19
[INFO 2017-06-29 10:29:02,874 main.py:57] epoch 11839, training loss: 6960.37, average training loss: 7246.67, base loss: 16109.35
[INFO 2017-06-29 10:29:05,946 main.py:57] epoch 11840, training loss: 7792.89, average training loss: 7247.72, base loss: 16109.34
[INFO 2017-06-29 10:29:08,987 main.py:57] epoch 11841, training loss: 7664.25, average training loss: 7247.72, base loss: 16109.51
[INFO 2017-06-29 10:29:11,987 main.py:57] epoch 11842, training loss: 6294.35, average training loss: 7247.95, base loss: 16109.16
[INFO 2017-06-29 10:29:14,998 main.py:57] epoch 11843, training loss: 6566.28, average training loss: 7246.53, base loss: 16109.06
[INFO 2017-06-29 10:29:18,102 main.py:57] epoch 11844, training loss: 7863.73, average training loss: 7247.11, base loss: 16108.98
[INFO 2017-06-29 10:29:21,173 main.py:57] epoch 11845, training loss: 6918.76, average training loss: 7247.64, base loss: 16108.52
[INFO 2017-06-29 10:29:24,252 main.py:57] epoch 11846, training loss: 6991.64, average training loss: 7247.25, base loss: 16108.47
[INFO 2017-06-29 10:29:27,235 main.py:57] epoch 11847, training loss: 8405.47, average training loss: 7247.93, base loss: 16108.60
[INFO 2017-06-29 10:29:30,268 main.py:57] epoch 11848, training loss: 7095.47, average training loss: 7248.67, base loss: 16108.76
[INFO 2017-06-29 10:29:33,395 main.py:57] epoch 11849, training loss: 6580.79, average training loss: 7248.52, base loss: 16108.59
[INFO 2017-06-29 10:29:36,407 main.py:57] epoch 11850, training loss: 7434.03, average training loss: 7248.75, base loss: 16108.47
[INFO 2017-06-29 10:29:39,452 main.py:57] epoch 11851, training loss: 6692.11, average training loss: 7248.69, base loss: 16108.37
[INFO 2017-06-29 10:29:42,441 main.py:57] epoch 11852, training loss: 6893.16, average training loss: 7248.27, base loss: 16108.18
[INFO 2017-06-29 10:29:45,465 main.py:57] epoch 11853, training loss: 7688.07, average training loss: 7247.92, base loss: 16108.21
[INFO 2017-06-29 10:29:48,457 main.py:57] epoch 11854, training loss: 6622.22, average training loss: 7247.66, base loss: 16108.00
[INFO 2017-06-29 10:29:51,530 main.py:57] epoch 11855, training loss: 7054.55, average training loss: 7247.48, base loss: 16108.09
[INFO 2017-06-29 10:29:54,570 main.py:57] epoch 11856, training loss: 7802.74, average training loss: 7247.22, base loss: 16108.17
[INFO 2017-06-29 10:29:57,604 main.py:57] epoch 11857, training loss: 6693.91, average training loss: 7246.31, base loss: 16107.82
[INFO 2017-06-29 10:30:00,604 main.py:57] epoch 11858, training loss: 6316.06, average training loss: 7246.06, base loss: 16107.62
[INFO 2017-06-29 10:30:03,676 main.py:57] epoch 11859, training loss: 6359.27, average training loss: 7245.59, base loss: 16107.40
[INFO 2017-06-29 10:30:06,776 main.py:57] epoch 11860, training loss: 7507.68, average training loss: 7245.68, base loss: 16107.31
[INFO 2017-06-29 10:30:09,803 main.py:57] epoch 11861, training loss: 7432.59, average training loss: 7245.90, base loss: 16107.27
[INFO 2017-06-29 10:30:12,851 main.py:57] epoch 11862, training loss: 7203.34, average training loss: 7243.72, base loss: 16107.24
[INFO 2017-06-29 10:30:15,891 main.py:57] epoch 11863, training loss: 7153.77, average training loss: 7243.58, base loss: 16107.15
[INFO 2017-06-29 10:30:18,920 main.py:57] epoch 11864, training loss: 7375.26, average training loss: 7243.24, base loss: 16107.10
[INFO 2017-06-29 10:30:21,979 main.py:57] epoch 11865, training loss: 6850.84, average training loss: 7242.20, base loss: 16107.14
[INFO 2017-06-29 10:30:24,991 main.py:57] epoch 11866, training loss: 7727.84, average training loss: 7243.15, base loss: 16107.57
[INFO 2017-06-29 10:30:28,034 main.py:57] epoch 11867, training loss: 6257.67, average training loss: 7242.90, base loss: 16107.64
[INFO 2017-06-29 10:30:31,024 main.py:57] epoch 11868, training loss: 6950.26, average training loss: 7242.85, base loss: 16107.82
[INFO 2017-06-29 10:30:34,093 main.py:57] epoch 11869, training loss: 6941.65, average training loss: 7241.86, base loss: 16107.79
[INFO 2017-06-29 10:30:37,074 main.py:57] epoch 11870, training loss: 7055.00, average training loss: 7241.48, base loss: 16107.65
[INFO 2017-06-29 10:30:40,097 main.py:57] epoch 11871, training loss: 6375.79, average training loss: 7240.62, base loss: 16107.44
[INFO 2017-06-29 10:30:43,191 main.py:57] epoch 11872, training loss: 6437.51, average training loss: 7240.59, base loss: 16107.68
[INFO 2017-06-29 10:30:46,209 main.py:57] epoch 11873, training loss: 7917.73, average training loss: 7241.16, base loss: 16108.02
[INFO 2017-06-29 10:30:49,306 main.py:57] epoch 11874, training loss: 6861.18, average training loss: 7240.12, base loss: 16108.06
[INFO 2017-06-29 10:30:52,313 main.py:57] epoch 11875, training loss: 7202.75, average training loss: 7239.24, base loss: 16108.29
[INFO 2017-06-29 10:30:55,350 main.py:57] epoch 11876, training loss: 7478.56, average training loss: 7239.59, base loss: 16108.60
[INFO 2017-06-29 10:30:58,437 main.py:57] epoch 11877, training loss: 7192.30, average training loss: 7240.10, base loss: 16108.82
[INFO 2017-06-29 10:31:01,489 main.py:57] epoch 11878, training loss: 6106.64, average training loss: 7238.51, base loss: 16108.42
[INFO 2017-06-29 10:31:04,622 main.py:57] epoch 11879, training loss: 7070.43, average training loss: 7238.09, base loss: 16108.34
[INFO 2017-06-29 10:31:07,655 main.py:57] epoch 11880, training loss: 7339.77, average training loss: 7239.00, base loss: 16108.22
[INFO 2017-06-29 10:31:10,660 main.py:57] epoch 11881, training loss: 8341.71, average training loss: 7239.61, base loss: 16108.14
[INFO 2017-06-29 10:31:13,784 main.py:57] epoch 11882, training loss: 7091.29, average training loss: 7239.93, base loss: 16108.02
[INFO 2017-06-29 10:31:16,918 main.py:57] epoch 11883, training loss: 7076.85, average training loss: 7239.38, base loss: 16108.00
[INFO 2017-06-29 10:31:19,974 main.py:57] epoch 11884, training loss: 6759.18, average training loss: 7238.93, base loss: 16107.66
[INFO 2017-06-29 10:31:23,130 main.py:57] epoch 11885, training loss: 7918.78, average training loss: 7239.23, base loss: 16107.98
[INFO 2017-06-29 10:31:26,190 main.py:57] epoch 11886, training loss: 7251.69, average training loss: 7239.55, base loss: 16107.82
[INFO 2017-06-29 10:31:29,232 main.py:57] epoch 11887, training loss: 6329.62, average training loss: 7238.17, base loss: 16107.36
[INFO 2017-06-29 10:31:32,323 main.py:57] epoch 11888, training loss: 7088.19, average training loss: 7237.76, base loss: 16107.26
[INFO 2017-06-29 10:31:35,400 main.py:57] epoch 11889, training loss: 8204.62, average training loss: 7238.48, base loss: 16107.55
[INFO 2017-06-29 10:31:38,496 main.py:57] epoch 11890, training loss: 6806.73, average training loss: 7238.36, base loss: 16107.51
[INFO 2017-06-29 10:31:41,558 main.py:57] epoch 11891, training loss: 6916.67, average training loss: 7238.45, base loss: 16107.50
[INFO 2017-06-29 10:31:44,633 main.py:57] epoch 11892, training loss: 6541.82, average training loss: 7237.46, base loss: 16107.22
[INFO 2017-06-29 10:31:47,617 main.py:57] epoch 11893, training loss: 7327.27, average training loss: 7237.21, base loss: 16106.94
[INFO 2017-06-29 10:31:50,723 main.py:57] epoch 11894, training loss: 6684.98, average training loss: 7237.00, base loss: 16106.94
[INFO 2017-06-29 10:31:53,777 main.py:57] epoch 11895, training loss: 7366.67, average training loss: 7238.13, base loss: 16107.04
[INFO 2017-06-29 10:31:56,870 main.py:57] epoch 11896, training loss: 8195.23, average training loss: 7239.12, base loss: 16107.17
[INFO 2017-06-29 10:31:59,914 main.py:57] epoch 11897, training loss: 7404.87, average training loss: 7238.95, base loss: 16107.11
[INFO 2017-06-29 10:32:02,993 main.py:57] epoch 11898, training loss: 7267.64, average training loss: 7238.23, base loss: 16107.16
[INFO 2017-06-29 10:32:06,028 main.py:57] epoch 11899, training loss: 6795.85, average training loss: 7237.81, base loss: 16107.10
[INFO 2017-06-29 10:32:06,028 main.py:59] epoch 11899, testing
[INFO 2017-06-29 10:32:18,665 main.py:104] average testing loss: 7967.45, base loss: 16655.86
[INFO 2017-06-29 10:32:18,665 main.py:105] improve_loss: 8688.41, improve_percent: 0.52
[INFO 2017-06-29 10:32:18,667 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:32:21,713 main.py:57] epoch 11900, training loss: 6651.41, average training loss: 7236.46, base loss: 16107.24
[INFO 2017-06-29 10:32:24,824 main.py:57] epoch 11901, training loss: 7842.09, average training loss: 7236.69, base loss: 16107.60
[INFO 2017-06-29 10:32:27,827 main.py:57] epoch 11902, training loss: 7069.15, average training loss: 7235.75, base loss: 16107.46
[INFO 2017-06-29 10:32:30,823 main.py:57] epoch 11903, training loss: 6595.91, average training loss: 7235.24, base loss: 16107.17
[INFO 2017-06-29 10:32:33,887 main.py:57] epoch 11904, training loss: 6411.36, average training loss: 7234.84, base loss: 16107.07
[INFO 2017-06-29 10:32:36,916 main.py:57] epoch 11905, training loss: 6590.69, average training loss: 7234.00, base loss: 16107.15
[INFO 2017-06-29 10:32:39,918 main.py:57] epoch 11906, training loss: 7027.66, average training loss: 7233.55, base loss: 16106.79
[INFO 2017-06-29 10:32:42,997 main.py:57] epoch 11907, training loss: 6375.59, average training loss: 7232.79, base loss: 16106.52
[INFO 2017-06-29 10:32:46,063 main.py:57] epoch 11908, training loss: 6943.34, average training loss: 7232.79, base loss: 16106.37
[INFO 2017-06-29 10:32:49,124 main.py:57] epoch 11909, training loss: 7068.13, average training loss: 7233.26, base loss: 16106.28
[INFO 2017-06-29 10:32:52,192 main.py:57] epoch 11910, training loss: 7009.23, average training loss: 7232.75, base loss: 16106.15
[INFO 2017-06-29 10:32:55,242 main.py:57] epoch 11911, training loss: 7824.80, average training loss: 7232.82, base loss: 16106.28
[INFO 2017-06-29 10:32:58,233 main.py:57] epoch 11912, training loss: 7705.61, average training loss: 7233.43, base loss: 16106.30
[INFO 2017-06-29 10:33:01,288 main.py:57] epoch 11913, training loss: 6873.31, average training loss: 7232.78, base loss: 16106.36
[INFO 2017-06-29 10:33:04,286 main.py:57] epoch 11914, training loss: 7521.41, average training loss: 7232.63, base loss: 16106.36
[INFO 2017-06-29 10:33:07,357 main.py:57] epoch 11915, training loss: 7420.67, average training loss: 7233.23, base loss: 16106.63
[INFO 2017-06-29 10:33:10,433 main.py:57] epoch 11916, training loss: 7611.10, average training loss: 7233.58, base loss: 16106.59
[INFO 2017-06-29 10:33:13,464 main.py:57] epoch 11917, training loss: 8230.74, average training loss: 7234.46, base loss: 16106.86
[INFO 2017-06-29 10:33:16,484 main.py:57] epoch 11918, training loss: 7391.33, average training loss: 7234.55, base loss: 16106.75
[INFO 2017-06-29 10:33:19,511 main.py:57] epoch 11919, training loss: 6675.00, average training loss: 7234.05, base loss: 16106.49
[INFO 2017-06-29 10:33:22,593 main.py:57] epoch 11920, training loss: 7304.66, average training loss: 7234.50, base loss: 16106.62
[INFO 2017-06-29 10:33:25,655 main.py:57] epoch 11921, training loss: 7070.83, average training loss: 7233.92, base loss: 16106.75
[INFO 2017-06-29 10:33:28,800 main.py:57] epoch 11922, training loss: 8117.24, average training loss: 7235.01, base loss: 16107.12
[INFO 2017-06-29 10:33:31,849 main.py:57] epoch 11923, training loss: 6428.82, average training loss: 7234.21, base loss: 16106.91
[INFO 2017-06-29 10:33:34,928 main.py:57] epoch 11924, training loss: 7065.18, average training loss: 7233.62, base loss: 16106.78
[INFO 2017-06-29 10:33:37,969 main.py:57] epoch 11925, training loss: 8387.45, average training loss: 7234.03, base loss: 16107.07
[INFO 2017-06-29 10:33:41,055 main.py:57] epoch 11926, training loss: 7219.95, average training loss: 7233.34, base loss: 16107.01
[INFO 2017-06-29 10:33:44,142 main.py:57] epoch 11927, training loss: 6754.00, average training loss: 7233.39, base loss: 16106.83
[INFO 2017-06-29 10:33:47,138 main.py:57] epoch 11928, training loss: 7513.92, average training loss: 7233.68, base loss: 16106.77
[INFO 2017-06-29 10:33:50,184 main.py:57] epoch 11929, training loss: 6580.59, average training loss: 7233.25, base loss: 16106.52
[INFO 2017-06-29 10:33:53,231 main.py:57] epoch 11930, training loss: 7400.25, average training loss: 7234.08, base loss: 16106.31
[INFO 2017-06-29 10:33:56,224 main.py:57] epoch 11931, training loss: 8564.48, average training loss: 7235.76, base loss: 16106.43
[INFO 2017-06-29 10:33:59,241 main.py:57] epoch 11932, training loss: 7718.99, average training loss: 7236.53, base loss: 16106.44
[INFO 2017-06-29 10:34:02,303 main.py:57] epoch 11933, training loss: 6578.28, average training loss: 7236.47, base loss: 16106.20
[INFO 2017-06-29 10:34:05,416 main.py:57] epoch 11934, training loss: 7127.24, average training loss: 7235.76, base loss: 16106.30
[INFO 2017-06-29 10:34:08,514 main.py:57] epoch 11935, training loss: 7326.62, average training loss: 7236.03, base loss: 16106.60
[INFO 2017-06-29 10:34:11,560 main.py:57] epoch 11936, training loss: 7007.82, average training loss: 7235.97, base loss: 16106.49
[INFO 2017-06-29 10:34:14,712 main.py:57] epoch 11937, training loss: 7404.61, average training loss: 7235.15, base loss: 16106.59
[INFO 2017-06-29 10:34:17,695 main.py:57] epoch 11938, training loss: 9267.54, average training loss: 7236.51, base loss: 16107.03
[INFO 2017-06-29 10:34:20,776 main.py:57] epoch 11939, training loss: 8715.69, average training loss: 7238.12, base loss: 16107.45
[INFO 2017-06-29 10:34:23,858 main.py:57] epoch 11940, training loss: 7274.65, average training loss: 7238.26, base loss: 16107.17
[INFO 2017-06-29 10:34:26,988 main.py:57] epoch 11941, training loss: 7126.07, average training loss: 7238.47, base loss: 16107.07
[INFO 2017-06-29 10:34:30,066 main.py:57] epoch 11942, training loss: 7399.98, average training loss: 7237.86, base loss: 16107.11
[INFO 2017-06-29 10:34:33,107 main.py:57] epoch 11943, training loss: 7210.54, average training loss: 7238.17, base loss: 16107.13
[INFO 2017-06-29 10:34:36,194 main.py:57] epoch 11944, training loss: 8504.37, average training loss: 7239.75, base loss: 16107.43
[INFO 2017-06-29 10:34:39,285 main.py:57] epoch 11945, training loss: 6581.72, average training loss: 7238.27, base loss: 16107.21
[INFO 2017-06-29 10:34:42,370 main.py:57] epoch 11946, training loss: 7742.62, average training loss: 7239.32, base loss: 16107.45
[INFO 2017-06-29 10:34:45,492 main.py:57] epoch 11947, training loss: 6210.36, average training loss: 7238.08, base loss: 16107.10
[INFO 2017-06-29 10:34:48,565 main.py:57] epoch 11948, training loss: 7360.91, average training loss: 7236.97, base loss: 16107.06
[INFO 2017-06-29 10:34:51,616 main.py:57] epoch 11949, training loss: 8326.01, average training loss: 7237.66, base loss: 16107.55
[INFO 2017-06-29 10:34:54,688 main.py:57] epoch 11950, training loss: 6614.34, average training loss: 7236.99, base loss: 16107.10
[INFO 2017-06-29 10:34:57,698 main.py:57] epoch 11951, training loss: 6623.64, average training loss: 7237.00, base loss: 16106.45
[INFO 2017-06-29 10:35:00,743 main.py:57] epoch 11952, training loss: 7258.96, average training loss: 7236.56, base loss: 16106.46
[INFO 2017-06-29 10:35:03,830 main.py:57] epoch 11953, training loss: 7102.96, average training loss: 7235.68, base loss: 16106.38
[INFO 2017-06-29 10:35:06,810 main.py:57] epoch 11954, training loss: 6501.28, average training loss: 7234.80, base loss: 16106.08
[INFO 2017-06-29 10:35:09,889 main.py:57] epoch 11955, training loss: 8893.41, average training loss: 7236.76, base loss: 16106.50
[INFO 2017-06-29 10:35:12,890 main.py:57] epoch 11956, training loss: 6463.73, average training loss: 7235.77, base loss: 16106.35
[INFO 2017-06-29 10:35:15,934 main.py:57] epoch 11957, training loss: 7273.52, average training loss: 7236.02, base loss: 16106.22
[INFO 2017-06-29 10:35:18,948 main.py:57] epoch 11958, training loss: 6815.05, average training loss: 7235.43, base loss: 16106.02
[INFO 2017-06-29 10:35:21,995 main.py:57] epoch 11959, training loss: 7128.11, average training loss: 7234.35, base loss: 16105.98
[INFO 2017-06-29 10:35:25,030 main.py:57] epoch 11960, training loss: 8855.99, average training loss: 7235.39, base loss: 16106.68
[INFO 2017-06-29 10:35:28,072 main.py:57] epoch 11961, training loss: 7818.37, average training loss: 7236.88, base loss: 16106.77
[INFO 2017-06-29 10:35:31,175 main.py:57] epoch 11962, training loss: 6745.85, average training loss: 7236.48, base loss: 16106.78
[INFO 2017-06-29 10:35:34,175 main.py:57] epoch 11963, training loss: 8070.84, average training loss: 7236.02, base loss: 16106.86
[INFO 2017-06-29 10:35:37,212 main.py:57] epoch 11964, training loss: 6894.02, average training loss: 7235.93, base loss: 16106.66
[INFO 2017-06-29 10:35:40,372 main.py:57] epoch 11965, training loss: 6582.66, average training loss: 7235.08, base loss: 16106.50
[INFO 2017-06-29 10:35:43,354 main.py:57] epoch 11966, training loss: 7015.16, average training loss: 7234.03, base loss: 16106.37
[INFO 2017-06-29 10:35:46,430 main.py:57] epoch 11967, training loss: 7973.93, average training loss: 7234.10, base loss: 16106.75
[INFO 2017-06-29 10:35:49,449 main.py:57] epoch 11968, training loss: 7758.75, average training loss: 7234.28, base loss: 16106.87
[INFO 2017-06-29 10:35:52,528 main.py:57] epoch 11969, training loss: 6936.99, average training loss: 7234.32, base loss: 16106.56
[INFO 2017-06-29 10:35:55,545 main.py:57] epoch 11970, training loss: 6799.37, average training loss: 7233.75, base loss: 16106.40
[INFO 2017-06-29 10:35:58,536 main.py:57] epoch 11971, training loss: 6538.84, average training loss: 7232.56, base loss: 16106.21
[INFO 2017-06-29 10:36:01,582 main.py:57] epoch 11972, training loss: 7553.89, average training loss: 7232.81, base loss: 16106.42
[INFO 2017-06-29 10:36:04,629 main.py:57] epoch 11973, training loss: 6707.79, average training loss: 7231.72, base loss: 16106.31
[INFO 2017-06-29 10:36:07,661 main.py:57] epoch 11974, training loss: 7292.55, average training loss: 7231.42, base loss: 16106.06
[INFO 2017-06-29 10:36:10,710 main.py:57] epoch 11975, training loss: 7561.13, average training loss: 7231.27, base loss: 16106.13
[INFO 2017-06-29 10:36:13,795 main.py:57] epoch 11976, training loss: 7682.04, average training loss: 7231.73, base loss: 16106.24
[INFO 2017-06-29 10:36:16,862 main.py:57] epoch 11977, training loss: 7676.15, average training loss: 7232.67, base loss: 16106.55
[INFO 2017-06-29 10:36:19,977 main.py:57] epoch 11978, training loss: 6846.49, average training loss: 7231.48, base loss: 16106.62
[INFO 2017-06-29 10:36:22,981 main.py:57] epoch 11979, training loss: 6022.98, average training loss: 7230.91, base loss: 16106.30
[INFO 2017-06-29 10:36:26,089 main.py:57] epoch 11980, training loss: 6335.05, average training loss: 7230.11, base loss: 16106.04
[INFO 2017-06-29 10:36:29,159 main.py:57] epoch 11981, training loss: 8027.31, average training loss: 7231.30, base loss: 16106.22
[INFO 2017-06-29 10:36:32,171 main.py:57] epoch 11982, training loss: 6807.53, average training loss: 7231.68, base loss: 16106.09
[INFO 2017-06-29 10:36:35,226 main.py:57] epoch 11983, training loss: 7627.30, average training loss: 7231.93, base loss: 16106.31
[INFO 2017-06-29 10:36:38,240 main.py:57] epoch 11984, training loss: 8168.48, average training loss: 7233.86, base loss: 16106.42
[INFO 2017-06-29 10:36:41,297 main.py:57] epoch 11985, training loss: 7612.58, average training loss: 7234.14, base loss: 16106.56
[INFO 2017-06-29 10:36:44,337 main.py:57] epoch 11986, training loss: 6640.46, average training loss: 7233.19, base loss: 16106.50
[INFO 2017-06-29 10:36:47,383 main.py:57] epoch 11987, training loss: 7112.20, average training loss: 7232.96, base loss: 16106.82
[INFO 2017-06-29 10:36:50,468 main.py:57] epoch 11988, training loss: 7014.21, average training loss: 7233.19, base loss: 16106.77
[INFO 2017-06-29 10:36:53,465 main.py:57] epoch 11989, training loss: 7492.02, average training loss: 7234.60, base loss: 16107.03
[INFO 2017-06-29 10:36:56,502 main.py:57] epoch 11990, training loss: 6241.61, average training loss: 7234.08, base loss: 16106.87
[INFO 2017-06-29 10:36:59,620 main.py:57] epoch 11991, training loss: 6744.01, average training loss: 7234.37, base loss: 16106.51
[INFO 2017-06-29 10:37:02,642 main.py:57] epoch 11992, training loss: 7237.09, average training loss: 7234.80, base loss: 16106.94
[INFO 2017-06-29 10:37:05,748 main.py:57] epoch 11993, training loss: 8498.27, average training loss: 7235.37, base loss: 16107.27
[INFO 2017-06-29 10:37:08,826 main.py:57] epoch 11994, training loss: 7391.25, average training loss: 7235.84, base loss: 16107.15
[INFO 2017-06-29 10:37:11,853 main.py:57] epoch 11995, training loss: 7481.81, average training loss: 7236.79, base loss: 16107.38
[INFO 2017-06-29 10:37:14,896 main.py:57] epoch 11996, training loss: 6980.16, average training loss: 7236.08, base loss: 16107.51
[INFO 2017-06-29 10:37:17,992 main.py:57] epoch 11997, training loss: 8009.10, average training loss: 7237.05, base loss: 16107.65
[INFO 2017-06-29 10:37:20,986 main.py:57] epoch 11998, training loss: 6556.51, average training loss: 7236.34, base loss: 16107.51
[INFO 2017-06-29 10:37:24,072 main.py:57] epoch 11999, training loss: 7062.72, average training loss: 7236.04, base loss: 16107.56
[INFO 2017-06-29 10:37:24,072 main.py:59] epoch 11999, testing
[INFO 2017-06-29 10:37:36,680 main.py:104] average testing loss: 8316.20, base loss: 17263.69
[INFO 2017-06-29 10:37:36,681 main.py:105] improve_loss: 8947.49, improve_percent: 0.52
[INFO 2017-06-29 10:37:36,682 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:37:39,739 main.py:57] epoch 12000, training loss: 7431.22, average training loss: 7236.13, base loss: 16107.82
[INFO 2017-06-29 10:37:42,830 main.py:57] epoch 12001, training loss: 7853.67, average training loss: 7236.55, base loss: 16107.80
[INFO 2017-06-29 10:37:45,838 main.py:57] epoch 12002, training loss: 8326.30, average training loss: 7236.94, base loss: 16107.75
[INFO 2017-06-29 10:37:48,913 main.py:57] epoch 12003, training loss: 7730.35, average training loss: 7237.74, base loss: 16107.75
[INFO 2017-06-29 10:37:51,962 main.py:57] epoch 12004, training loss: 7397.12, average training loss: 7238.62, base loss: 16107.76
[INFO 2017-06-29 10:37:54,990 main.py:57] epoch 12005, training loss: 6714.47, average training loss: 7237.79, base loss: 16107.46
[INFO 2017-06-29 10:37:58,087 main.py:57] epoch 12006, training loss: 6875.28, average training loss: 7237.06, base loss: 16107.45
[INFO 2017-06-29 10:38:01,137 main.py:57] epoch 12007, training loss: 7250.71, average training loss: 7237.68, base loss: 16107.80
[INFO 2017-06-29 10:38:04,151 main.py:57] epoch 12008, training loss: 7729.34, average training loss: 7238.79, base loss: 16107.92
[INFO 2017-06-29 10:38:07,200 main.py:57] epoch 12009, training loss: 7326.56, average training loss: 7239.41, base loss: 16108.00
[INFO 2017-06-29 10:38:10,341 main.py:57] epoch 12010, training loss: 6683.44, average training loss: 7239.12, base loss: 16107.91
[INFO 2017-06-29 10:38:13,395 main.py:57] epoch 12011, training loss: 6401.49, average training loss: 7238.09, base loss: 16107.78
[INFO 2017-06-29 10:38:16,406 main.py:57] epoch 12012, training loss: 6437.23, average training loss: 7237.24, base loss: 16107.79
[INFO 2017-06-29 10:38:19,384 main.py:57] epoch 12013, training loss: 6935.67, average training loss: 7236.79, base loss: 16107.47
[INFO 2017-06-29 10:38:22,376 main.py:57] epoch 12014, training loss: 7874.26, average training loss: 7237.08, base loss: 16107.43
[INFO 2017-06-29 10:38:25,416 main.py:57] epoch 12015, training loss: 6240.55, average training loss: 7236.26, base loss: 16107.07
[INFO 2017-06-29 10:38:28,443 main.py:57] epoch 12016, training loss: 7279.39, average training loss: 7236.01, base loss: 16106.83
[INFO 2017-06-29 10:38:31,515 main.py:57] epoch 12017, training loss: 6814.56, average training loss: 7235.97, base loss: 16106.79
[INFO 2017-06-29 10:38:34,629 main.py:57] epoch 12018, training loss: 7007.42, average training loss: 7236.51, base loss: 16106.85
[INFO 2017-06-29 10:38:37,685 main.py:57] epoch 12019, training loss: 8589.32, average training loss: 7237.90, base loss: 16107.18
[INFO 2017-06-29 10:38:40,736 main.py:57] epoch 12020, training loss: 7425.35, average training loss: 7238.68, base loss: 16107.55
[INFO 2017-06-29 10:38:43,732 main.py:57] epoch 12021, training loss: 7166.28, average training loss: 7238.49, base loss: 16107.63
[INFO 2017-06-29 10:38:46,771 main.py:57] epoch 12022, training loss: 7241.33, average training loss: 7239.09, base loss: 16107.38
[INFO 2017-06-29 10:38:49,861 main.py:57] epoch 12023, training loss: 8021.88, average training loss: 7240.40, base loss: 16107.40
[INFO 2017-06-29 10:38:52,908 main.py:57] epoch 12024, training loss: 7360.90, average training loss: 7241.24, base loss: 16107.55
[INFO 2017-06-29 10:38:55,885 main.py:57] epoch 12025, training loss: 7485.79, average training loss: 7242.27, base loss: 16107.52
[INFO 2017-06-29 10:38:58,911 main.py:57] epoch 12026, training loss: 6797.05, average training loss: 7241.77, base loss: 16107.63
[INFO 2017-06-29 10:39:02,003 main.py:57] epoch 12027, training loss: 6842.89, average training loss: 7241.94, base loss: 16107.36
[INFO 2017-06-29 10:39:05,026 main.py:57] epoch 12028, training loss: 7522.37, average training loss: 7241.92, base loss: 16107.52
[INFO 2017-06-29 10:39:08,045 main.py:57] epoch 12029, training loss: 7082.47, average training loss: 7240.95, base loss: 16107.38
[INFO 2017-06-29 10:39:11,059 main.py:57] epoch 12030, training loss: 6566.81, average training loss: 7240.06, base loss: 16107.08
[INFO 2017-06-29 10:39:14,115 main.py:57] epoch 12031, training loss: 7709.25, average training loss: 7240.51, base loss: 16107.17
[INFO 2017-06-29 10:39:17,206 main.py:57] epoch 12032, training loss: 7233.14, average training loss: 7240.89, base loss: 16107.33
[INFO 2017-06-29 10:39:20,300 main.py:57] epoch 12033, training loss: 6571.44, average training loss: 7240.73, base loss: 16107.25
[INFO 2017-06-29 10:39:23,327 main.py:57] epoch 12034, training loss: 6420.68, average training loss: 7239.61, base loss: 16106.94
[INFO 2017-06-29 10:39:26,320 main.py:57] epoch 12035, training loss: 7203.14, average training loss: 7239.48, base loss: 16106.90
[INFO 2017-06-29 10:39:29,389 main.py:57] epoch 12036, training loss: 7439.77, average training loss: 7238.52, base loss: 16106.78
[INFO 2017-06-29 10:39:32,447 main.py:57] epoch 12037, training loss: 7864.82, average training loss: 7239.57, base loss: 16107.08
[INFO 2017-06-29 10:39:35,477 main.py:57] epoch 12038, training loss: 7589.88, average training loss: 7240.15, base loss: 16107.03
[INFO 2017-06-29 10:39:38,648 main.py:57] epoch 12039, training loss: 6532.93, average training loss: 7240.18, base loss: 16106.63
[INFO 2017-06-29 10:39:41,758 main.py:57] epoch 12040, training loss: 7227.09, average training loss: 7240.79, base loss: 16106.74
[INFO 2017-06-29 10:39:44,819 main.py:57] epoch 12041, training loss: 6520.72, average training loss: 7240.16, base loss: 16106.56
[INFO 2017-06-29 10:39:47,864 main.py:57] epoch 12042, training loss: 6765.96, average training loss: 7239.80, base loss: 16106.43
[INFO 2017-06-29 10:39:50,895 main.py:57] epoch 12043, training loss: 7040.50, average training loss: 7239.77, base loss: 16106.43
[INFO 2017-06-29 10:39:53,927 main.py:57] epoch 12044, training loss: 6690.94, average training loss: 7239.99, base loss: 16106.06
[INFO 2017-06-29 10:39:56,997 main.py:57] epoch 12045, training loss: 7392.48, average training loss: 7240.52, base loss: 16106.22
[INFO 2017-06-29 10:40:00,067 main.py:57] epoch 12046, training loss: 7724.10, average training loss: 7241.56, base loss: 16106.23
[INFO 2017-06-29 10:40:03,197 main.py:57] epoch 12047, training loss: 7635.22, average training loss: 7241.05, base loss: 16106.44
[INFO 2017-06-29 10:40:06,282 main.py:57] epoch 12048, training loss: 7005.02, average training loss: 7240.70, base loss: 16106.51
[INFO 2017-06-29 10:40:09,348 main.py:57] epoch 12049, training loss: 6334.29, average training loss: 7239.73, base loss: 16106.12
[INFO 2017-06-29 10:40:12,446 main.py:57] epoch 12050, training loss: 6967.96, average training loss: 7240.52, base loss: 16106.07
[INFO 2017-06-29 10:40:15,656 main.py:57] epoch 12051, training loss: 6314.88, average training loss: 7239.21, base loss: 16105.89
[INFO 2017-06-29 10:40:18,767 main.py:57] epoch 12052, training loss: 7497.07, average training loss: 7239.53, base loss: 16106.03
[INFO 2017-06-29 10:40:21,875 main.py:57] epoch 12053, training loss: 7454.38, average training loss: 7239.29, base loss: 16106.09
[INFO 2017-06-29 10:40:24,926 main.py:57] epoch 12054, training loss: 7380.20, average training loss: 7239.94, base loss: 16106.05
[INFO 2017-06-29 10:40:28,070 main.py:57] epoch 12055, training loss: 7550.10, average training loss: 7240.97, base loss: 16106.09
[INFO 2017-06-29 10:40:31,101 main.py:57] epoch 12056, training loss: 7721.87, average training loss: 7242.14, base loss: 16106.10
[INFO 2017-06-29 10:40:34,239 main.py:57] epoch 12057, training loss: 5845.29, average training loss: 7239.84, base loss: 16105.61
[INFO 2017-06-29 10:40:37,415 main.py:57] epoch 12058, training loss: 6879.10, average training loss: 7240.52, base loss: 16105.66
[INFO 2017-06-29 10:40:40,461 main.py:57] epoch 12059, training loss: 7237.78, average training loss: 7239.38, base loss: 16105.76
[INFO 2017-06-29 10:40:43,510 main.py:57] epoch 12060, training loss: 7158.01, average training loss: 7239.87, base loss: 16105.66
[INFO 2017-06-29 10:40:46,564 main.py:57] epoch 12061, training loss: 6565.47, average training loss: 7239.70, base loss: 16105.53
[INFO 2017-06-29 10:40:49,617 main.py:57] epoch 12062, training loss: 7510.47, average training loss: 7239.84, base loss: 16105.48
[INFO 2017-06-29 10:40:52,699 main.py:57] epoch 12063, training loss: 6224.91, average training loss: 7238.85, base loss: 16104.91
[INFO 2017-06-29 10:40:55,742 main.py:57] epoch 12064, training loss: 6332.77, average training loss: 7237.53, base loss: 16104.79
[INFO 2017-06-29 10:40:58,769 main.py:57] epoch 12065, training loss: 7133.17, average training loss: 7238.08, base loss: 16104.73
[INFO 2017-06-29 10:41:01,883 main.py:57] epoch 12066, training loss: 7285.71, average training loss: 7238.30, base loss: 16104.59
[INFO 2017-06-29 10:41:05,022 main.py:57] epoch 12067, training loss: 7448.39, average training loss: 7238.43, base loss: 16104.68
[INFO 2017-06-29 10:41:08,131 main.py:57] epoch 12068, training loss: 7356.98, average training loss: 7237.94, base loss: 16104.63
[INFO 2017-06-29 10:41:11,203 main.py:57] epoch 12069, training loss: 6632.32, average training loss: 7237.13, base loss: 16104.48
[INFO 2017-06-29 10:41:14,276 main.py:57] epoch 12070, training loss: 7556.41, average training loss: 7238.22, base loss: 16104.40
[INFO 2017-06-29 10:41:17,361 main.py:57] epoch 12071, training loss: 7306.64, average training loss: 7238.97, base loss: 16104.27
[INFO 2017-06-29 10:41:20,490 main.py:57] epoch 12072, training loss: 6938.42, average training loss: 7238.59, base loss: 16104.15
[INFO 2017-06-29 10:41:23,454 main.py:57] epoch 12073, training loss: 7351.60, average training loss: 7239.27, base loss: 16104.32
[INFO 2017-06-29 10:41:26,502 main.py:57] epoch 12074, training loss: 7294.13, average training loss: 7238.58, base loss: 16104.18
[INFO 2017-06-29 10:41:29,611 main.py:57] epoch 12075, training loss: 7783.92, average training loss: 7239.17, base loss: 16104.18
[INFO 2017-06-29 10:41:32,704 main.py:57] epoch 12076, training loss: 6916.32, average training loss: 7239.40, base loss: 16104.04
[INFO 2017-06-29 10:41:35,800 main.py:57] epoch 12077, training loss: 7782.40, average training loss: 7239.27, base loss: 16104.02
[INFO 2017-06-29 10:41:38,867 main.py:57] epoch 12078, training loss: 8740.88, average training loss: 7241.11, base loss: 16104.83
[INFO 2017-06-29 10:41:41,974 main.py:57] epoch 12079, training loss: 7821.66, average training loss: 7242.46, base loss: 16105.09
[INFO 2017-06-29 10:41:45,056 main.py:57] epoch 12080, training loss: 6721.47, average training loss: 7241.91, base loss: 16104.84
[INFO 2017-06-29 10:41:48,124 main.py:57] epoch 12081, training loss: 7416.23, average training loss: 7242.93, base loss: 16104.97
[INFO 2017-06-29 10:41:51,293 main.py:57] epoch 12082, training loss: 7521.27, average training loss: 7243.13, base loss: 16105.11
[INFO 2017-06-29 10:41:54,400 main.py:57] epoch 12083, training loss: 7435.50, average training loss: 7242.78, base loss: 16105.10
[INFO 2017-06-29 10:41:57,475 main.py:57] epoch 12084, training loss: 7597.06, average training loss: 7243.09, base loss: 16105.09
[INFO 2017-06-29 10:42:00,574 main.py:57] epoch 12085, training loss: 6757.06, average training loss: 7242.64, base loss: 16104.86
[INFO 2017-06-29 10:42:03,668 main.py:57] epoch 12086, training loss: 7783.09, average training loss: 7243.62, base loss: 16105.13
[INFO 2017-06-29 10:42:06,660 main.py:57] epoch 12087, training loss: 6403.13, average training loss: 7243.69, base loss: 16105.02
[INFO 2017-06-29 10:42:09,761 main.py:57] epoch 12088, training loss: 8028.74, average training loss: 7244.84, base loss: 16105.00
[INFO 2017-06-29 10:42:12,783 main.py:57] epoch 12089, training loss: 7492.67, average training loss: 7245.47, base loss: 16104.84
[INFO 2017-06-29 10:42:15,843 main.py:57] epoch 12090, training loss: 7940.40, average training loss: 7245.04, base loss: 16105.17
[INFO 2017-06-29 10:42:18,934 main.py:57] epoch 12091, training loss: 7140.01, average training loss: 7244.61, base loss: 16105.51
[INFO 2017-06-29 10:42:21,972 main.py:57] epoch 12092, training loss: 7252.35, average training loss: 7243.74, base loss: 16105.61
[INFO 2017-06-29 10:42:25,041 main.py:57] epoch 12093, training loss: 7328.00, average training loss: 7244.16, base loss: 16105.61
[INFO 2017-06-29 10:42:28,142 main.py:57] epoch 12094, training loss: 6750.04, average training loss: 7243.39, base loss: 16105.53
[INFO 2017-06-29 10:42:31,253 main.py:57] epoch 12095, training loss: 7736.90, average training loss: 7243.69, base loss: 16105.83
[INFO 2017-06-29 10:42:34,238 main.py:57] epoch 12096, training loss: 7136.47, average training loss: 7243.98, base loss: 16105.93
[INFO 2017-06-29 10:42:37,285 main.py:57] epoch 12097, training loss: 7338.58, average training loss: 7243.73, base loss: 16106.03
[INFO 2017-06-29 10:42:40,340 main.py:57] epoch 12098, training loss: 8051.37, average training loss: 7245.16, base loss: 16106.54
[INFO 2017-06-29 10:42:43,423 main.py:57] epoch 12099, training loss: 6466.72, average training loss: 7243.99, base loss: 16106.60
[INFO 2017-06-29 10:42:43,423 main.py:59] epoch 12099, testing
[INFO 2017-06-29 10:42:56,083 main.py:104] average testing loss: 7851.01, base loss: 16092.70
[INFO 2017-06-29 10:42:56,083 main.py:105] improve_loss: 8241.69, improve_percent: 0.51
[INFO 2017-06-29 10:42:56,084 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:42:59,140 main.py:57] epoch 12100, training loss: 7179.95, average training loss: 7243.37, base loss: 16106.56
[INFO 2017-06-29 10:43:02,180 main.py:57] epoch 12101, training loss: 7630.92, average training loss: 7243.43, base loss: 16106.80
[INFO 2017-06-29 10:43:05,232 main.py:57] epoch 12102, training loss: 6731.90, average training loss: 7243.39, base loss: 16106.98
[INFO 2017-06-29 10:43:08,296 main.py:57] epoch 12103, training loss: 7457.27, average training loss: 7244.51, base loss: 16107.22
[INFO 2017-06-29 10:43:11,308 main.py:57] epoch 12104, training loss: 6619.63, average training loss: 7244.24, base loss: 16107.32
[INFO 2017-06-29 10:43:14,446 main.py:57] epoch 12105, training loss: 7413.70, average training loss: 7244.40, base loss: 16107.58
[INFO 2017-06-29 10:43:17,610 main.py:57] epoch 12106, training loss: 7967.68, average training loss: 7245.51, base loss: 16107.92
[INFO 2017-06-29 10:43:20,637 main.py:57] epoch 12107, training loss: 6732.22, average training loss: 7244.49, base loss: 16107.81
[INFO 2017-06-29 10:43:23,658 main.py:57] epoch 12108, training loss: 7129.16, average training loss: 7244.01, base loss: 16107.41
[INFO 2017-06-29 10:43:26,644 main.py:57] epoch 12109, training loss: 7828.36, average training loss: 7243.46, base loss: 16107.40
[INFO 2017-06-29 10:43:29,674 main.py:57] epoch 12110, training loss: 6836.00, average training loss: 7243.91, base loss: 16107.00
[INFO 2017-06-29 10:43:32,727 main.py:57] epoch 12111, training loss: 7068.56, average training loss: 7243.31, base loss: 16106.70
[INFO 2017-06-29 10:43:35,752 main.py:57] epoch 12112, training loss: 6897.57, average training loss: 7243.58, base loss: 16106.50
[INFO 2017-06-29 10:43:38,775 main.py:57] epoch 12113, training loss: 7574.50, average training loss: 7244.28, base loss: 16106.70
[INFO 2017-06-29 10:43:41,903 main.py:57] epoch 12114, training loss: 6056.02, average training loss: 7242.60, base loss: 16106.81
[INFO 2017-06-29 10:43:44,992 main.py:57] epoch 12115, training loss: 6966.45, average training loss: 7242.74, base loss: 16106.96
[INFO 2017-06-29 10:43:48,133 main.py:57] epoch 12116, training loss: 7729.96, average training loss: 7242.39, base loss: 16107.30
[INFO 2017-06-29 10:43:51,237 main.py:57] epoch 12117, training loss: 6905.88, average training loss: 7241.75, base loss: 16107.24
[INFO 2017-06-29 10:43:54,274 main.py:57] epoch 12118, training loss: 7093.26, average training loss: 7239.25, base loss: 16107.10
[INFO 2017-06-29 10:43:57,362 main.py:57] epoch 12119, training loss: 7376.66, average training loss: 7238.18, base loss: 16107.33
[INFO 2017-06-29 10:44:00,476 main.py:57] epoch 12120, training loss: 7594.51, average training loss: 7238.25, base loss: 16107.44
[INFO 2017-06-29 10:44:03,571 main.py:57] epoch 12121, training loss: 7733.43, average training loss: 7238.84, base loss: 16107.74
[INFO 2017-06-29 10:44:06,616 main.py:57] epoch 12122, training loss: 6994.79, average training loss: 7237.22, base loss: 16107.65
[INFO 2017-06-29 10:44:09,628 main.py:57] epoch 12123, training loss: 7301.92, average training loss: 7237.49, base loss: 16107.71
[INFO 2017-06-29 10:44:12,692 main.py:57] epoch 12124, training loss: 6841.10, average training loss: 7235.15, base loss: 16107.58
[INFO 2017-06-29 10:44:15,722 main.py:57] epoch 12125, training loss: 7320.19, average training loss: 7235.85, base loss: 16107.43
[INFO 2017-06-29 10:44:18,720 main.py:57] epoch 12126, training loss: 8049.65, average training loss: 7236.51, base loss: 16107.78
[INFO 2017-06-29 10:44:21,852 main.py:57] epoch 12127, training loss: 6850.91, average training loss: 7236.42, base loss: 16107.94
[INFO 2017-06-29 10:44:24,895 main.py:57] epoch 12128, training loss: 6995.75, average training loss: 7236.77, base loss: 16107.81
[INFO 2017-06-29 10:44:27,962 main.py:57] epoch 12129, training loss: 7800.76, average training loss: 7237.41, base loss: 16108.05
[INFO 2017-06-29 10:44:30,972 main.py:57] epoch 12130, training loss: 6687.81, average training loss: 7235.50, base loss: 16108.05
[INFO 2017-06-29 10:44:34,006 main.py:57] epoch 12131, training loss: 6377.35, average training loss: 7232.99, base loss: 16107.84
[INFO 2017-06-29 10:44:37,045 main.py:57] epoch 12132, training loss: 6682.80, average training loss: 7231.78, base loss: 16107.80
[INFO 2017-06-29 10:44:40,084 main.py:57] epoch 12133, training loss: 6491.48, average training loss: 7230.63, base loss: 16107.50
[INFO 2017-06-29 10:44:43,158 main.py:57] epoch 12134, training loss: 7984.78, average training loss: 7231.89, base loss: 16107.40
[INFO 2017-06-29 10:44:46,206 main.py:57] epoch 12135, training loss: 7572.70, average training loss: 7232.18, base loss: 16107.23
[INFO 2017-06-29 10:44:49,266 main.py:57] epoch 12136, training loss: 6821.70, average training loss: 7232.74, base loss: 16107.12
[INFO 2017-06-29 10:44:52,310 main.py:57] epoch 12137, training loss: 6765.54, average training loss: 7231.11, base loss: 16106.94
[INFO 2017-06-29 10:44:55,321 main.py:57] epoch 12138, training loss: 7161.72, average training loss: 7231.15, base loss: 16107.03
[INFO 2017-06-29 10:44:58,339 main.py:57] epoch 12139, training loss: 6835.69, average training loss: 7230.54, base loss: 16107.19
[INFO 2017-06-29 10:45:01,340 main.py:57] epoch 12140, training loss: 7121.84, average training loss: 7230.72, base loss: 16107.26
[INFO 2017-06-29 10:45:04,403 main.py:57] epoch 12141, training loss: 6798.49, average training loss: 7230.59, base loss: 16107.36
[INFO 2017-06-29 10:45:07,468 main.py:57] epoch 12142, training loss: 7030.50, average training loss: 7230.51, base loss: 16107.24
[INFO 2017-06-29 10:45:10,588 main.py:57] epoch 12143, training loss: 7766.74, average training loss: 7230.51, base loss: 16107.41
[INFO 2017-06-29 10:45:13,634 main.py:57] epoch 12144, training loss: 7143.46, average training loss: 7230.06, base loss: 16107.24
[INFO 2017-06-29 10:45:16,628 main.py:57] epoch 12145, training loss: 6706.43, average training loss: 7229.75, base loss: 16107.06
[INFO 2017-06-29 10:45:19,744 main.py:57] epoch 12146, training loss: 8075.26, average training loss: 7231.79, base loss: 16107.12
[INFO 2017-06-29 10:45:22,784 main.py:57] epoch 12147, training loss: 6807.94, average training loss: 7231.59, base loss: 16106.79
[INFO 2017-06-29 10:45:25,766 main.py:57] epoch 12148, training loss: 7637.25, average training loss: 7232.13, base loss: 16106.96
[INFO 2017-06-29 10:45:28,819 main.py:57] epoch 12149, training loss: 7246.58, average training loss: 7232.72, base loss: 16107.22
[INFO 2017-06-29 10:45:31,845 main.py:57] epoch 12150, training loss: 9417.72, average training loss: 7234.85, base loss: 16107.94
[INFO 2017-06-29 10:45:34,938 main.py:57] epoch 12151, training loss: 7753.97, average training loss: 7236.01, base loss: 16108.07
[INFO 2017-06-29 10:45:37,992 main.py:57] epoch 12152, training loss: 7153.41, average training loss: 7234.53, base loss: 16108.35
[INFO 2017-06-29 10:45:41,080 main.py:57] epoch 12153, training loss: 8741.85, average training loss: 7236.07, base loss: 16108.67
[INFO 2017-06-29 10:45:44,086 main.py:57] epoch 12154, training loss: 7364.89, average training loss: 7236.36, base loss: 16108.98
[INFO 2017-06-29 10:45:47,165 main.py:57] epoch 12155, training loss: 7201.74, average training loss: 7236.15, base loss: 16108.98
[INFO 2017-06-29 10:45:50,167 main.py:57] epoch 12156, training loss: 7830.60, average training loss: 7237.06, base loss: 16109.17
[INFO 2017-06-29 10:45:53,276 main.py:57] epoch 12157, training loss: 7224.44, average training loss: 7237.31, base loss: 16109.15
[INFO 2017-06-29 10:45:56,399 main.py:57] epoch 12158, training loss: 7499.45, average training loss: 7237.86, base loss: 16109.15
[INFO 2017-06-29 10:45:59,482 main.py:57] epoch 12159, training loss: 7315.95, average training loss: 7237.64, base loss: 16109.37
[INFO 2017-06-29 10:46:02,510 main.py:57] epoch 12160, training loss: 6732.86, average training loss: 7236.51, base loss: 16109.12
[INFO 2017-06-29 10:46:05,552 main.py:57] epoch 12161, training loss: 7860.15, average training loss: 7236.93, base loss: 16109.11
[INFO 2017-06-29 10:46:08,636 main.py:57] epoch 12162, training loss: 8352.07, average training loss: 7237.27, base loss: 16109.68
[INFO 2017-06-29 10:46:11,693 main.py:57] epoch 12163, training loss: 7665.58, average training loss: 7237.56, base loss: 16110.17
[INFO 2017-06-29 10:46:14,735 main.py:57] epoch 12164, training loss: 6331.88, average training loss: 7236.30, base loss: 16109.94
[INFO 2017-06-29 10:46:17,835 main.py:57] epoch 12165, training loss: 6263.14, average training loss: 7236.11, base loss: 16109.52
[INFO 2017-06-29 10:46:20,982 main.py:57] epoch 12166, training loss: 8680.75, average training loss: 7236.89, base loss: 16109.77
[INFO 2017-06-29 10:46:24,048 main.py:57] epoch 12167, training loss: 7689.11, average training loss: 7237.12, base loss: 16109.78
[INFO 2017-06-29 10:46:27,096 main.py:57] epoch 12168, training loss: 6551.70, average training loss: 7237.11, base loss: 16109.64
[INFO 2017-06-29 10:46:30,329 main.py:57] epoch 12169, training loss: 6759.05, average training loss: 7237.32, base loss: 16109.35
[INFO 2017-06-29 10:46:33,469 main.py:57] epoch 12170, training loss: 7386.91, average training loss: 7237.84, base loss: 16109.25
[INFO 2017-06-29 10:46:36,543 main.py:57] epoch 12171, training loss: 7652.00, average training loss: 7238.61, base loss: 16109.64
[INFO 2017-06-29 10:46:39,586 main.py:57] epoch 12172, training loss: 6595.76, average training loss: 7237.31, base loss: 16109.27
[INFO 2017-06-29 10:46:42,591 main.py:57] epoch 12173, training loss: 7126.81, average training loss: 7237.16, base loss: 16109.13
[INFO 2017-06-29 10:46:45,705 main.py:57] epoch 12174, training loss: 6815.19, average training loss: 7236.62, base loss: 16108.85
[INFO 2017-06-29 10:46:48,691 main.py:57] epoch 12175, training loss: 6518.04, average training loss: 7236.13, base loss: 16108.64
[INFO 2017-06-29 10:46:51,726 main.py:57] epoch 12176, training loss: 7033.13, average training loss: 7234.80, base loss: 16108.63
[INFO 2017-06-29 10:46:54,712 main.py:57] epoch 12177, training loss: 7594.62, average training loss: 7235.34, base loss: 16108.79
[INFO 2017-06-29 10:46:57,728 main.py:57] epoch 12178, training loss: 8081.33, average training loss: 7235.58, base loss: 16108.91
[INFO 2017-06-29 10:47:00,873 main.py:57] epoch 12179, training loss: 6387.30, average training loss: 7235.34, base loss: 16108.76
[INFO 2017-06-29 10:47:03,910 main.py:57] epoch 12180, training loss: 8156.04, average training loss: 7237.04, base loss: 16108.67
[INFO 2017-06-29 10:47:06,897 main.py:57] epoch 12181, training loss: 7100.73, average training loss: 7235.85, base loss: 16108.27
[INFO 2017-06-29 10:47:09,916 main.py:57] epoch 12182, training loss: 7397.12, average training loss: 7236.35, base loss: 16108.09
[INFO 2017-06-29 10:47:12,968 main.py:57] epoch 12183, training loss: 6804.33, average training loss: 7237.04, base loss: 16107.84
[INFO 2017-06-29 10:47:16,088 main.py:57] epoch 12184, training loss: 7214.95, average training loss: 7236.32, base loss: 16107.48
[INFO 2017-06-29 10:47:19,168 main.py:57] epoch 12185, training loss: 7937.18, average training loss: 7237.00, base loss: 16107.45
[INFO 2017-06-29 10:47:22,217 main.py:57] epoch 12186, training loss: 8230.79, average training loss: 7237.41, base loss: 16107.71
[INFO 2017-06-29 10:47:25,205 main.py:57] epoch 12187, training loss: 7252.46, average training loss: 7237.24, base loss: 16107.67
[INFO 2017-06-29 10:47:28,207 main.py:57] epoch 12188, training loss: 7733.16, average training loss: 7238.46, base loss: 16107.71
[INFO 2017-06-29 10:47:31,280 main.py:57] epoch 12189, training loss: 6727.68, average training loss: 7237.72, base loss: 16107.48
[INFO 2017-06-29 10:47:34,308 main.py:57] epoch 12190, training loss: 7787.14, average training loss: 7238.35, base loss: 16107.78
[INFO 2017-06-29 10:47:37,355 main.py:57] epoch 12191, training loss: 6652.87, average training loss: 7237.62, base loss: 16107.60
[INFO 2017-06-29 10:47:40,460 main.py:57] epoch 12192, training loss: 7093.94, average training loss: 7236.15, base loss: 16107.65
[INFO 2017-06-29 10:47:43,504 main.py:57] epoch 12193, training loss: 7734.91, average training loss: 7236.15, base loss: 16107.90
[INFO 2017-06-29 10:47:46,554 main.py:57] epoch 12194, training loss: 6628.76, average training loss: 7235.97, base loss: 16107.63
[INFO 2017-06-29 10:47:49,644 main.py:57] epoch 12195, training loss: 7930.96, average training loss: 7236.99, base loss: 16107.96
[INFO 2017-06-29 10:47:52,717 main.py:57] epoch 12196, training loss: 8036.73, average training loss: 7236.92, base loss: 16108.42
[INFO 2017-06-29 10:47:55,802 main.py:57] epoch 12197, training loss: 7237.67, average training loss: 7235.71, base loss: 16108.54
[INFO 2017-06-29 10:47:58,849 main.py:57] epoch 12198, training loss: 6618.53, average training loss: 7235.35, base loss: 16108.37
[INFO 2017-06-29 10:48:01,933 main.py:57] epoch 12199, training loss: 6993.02, average training loss: 7235.32, base loss: 16108.30
[INFO 2017-06-29 10:48:01,933 main.py:59] epoch 12199, testing
[INFO 2017-06-29 10:48:14,633 main.py:104] average testing loss: 8303.32, base loss: 17836.81
[INFO 2017-06-29 10:48:14,633 main.py:105] improve_loss: 9533.49, improve_percent: 0.53
[INFO 2017-06-29 10:48:14,635 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 10:48:14,673 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:48:17,699 main.py:57] epoch 12200, training loss: 6769.80, average training loss: 7234.69, base loss: 16108.15
[INFO 2017-06-29 10:48:20,737 main.py:57] epoch 12201, training loss: 7401.31, average training loss: 7235.07, base loss: 16108.40
[INFO 2017-06-29 10:48:23,752 main.py:57] epoch 12202, training loss: 8098.22, average training loss: 7235.04, base loss: 16108.29
[INFO 2017-06-29 10:48:26,822 main.py:57] epoch 12203, training loss: 7387.03, average training loss: 7235.72, base loss: 16108.20
[INFO 2017-06-29 10:48:29,866 main.py:57] epoch 12204, training loss: 7484.27, average training loss: 7235.95, base loss: 16108.23
[INFO 2017-06-29 10:48:32,878 main.py:57] epoch 12205, training loss: 7025.28, average training loss: 7236.13, base loss: 16108.14
[INFO 2017-06-29 10:48:35,908 main.py:57] epoch 12206, training loss: 6937.78, average training loss: 7236.30, base loss: 16108.08
[INFO 2017-06-29 10:48:38,919 main.py:57] epoch 12207, training loss: 7526.72, average training loss: 7235.37, base loss: 16108.18
[INFO 2017-06-29 10:48:41,990 main.py:57] epoch 12208, training loss: 8284.08, average training loss: 7235.73, base loss: 16108.63
[INFO 2017-06-29 10:48:45,095 main.py:57] epoch 12209, training loss: 7280.10, average training loss: 7236.05, base loss: 16108.78
[INFO 2017-06-29 10:48:48,199 main.py:57] epoch 12210, training loss: 7419.13, average training loss: 7236.74, base loss: 16108.72
[INFO 2017-06-29 10:48:51,225 main.py:57] epoch 12211, training loss: 7932.58, average training loss: 7236.95, base loss: 16108.72
[INFO 2017-06-29 10:48:54,230 main.py:57] epoch 12212, training loss: 6651.12, average training loss: 7236.84, base loss: 16108.76
[INFO 2017-06-29 10:48:57,300 main.py:57] epoch 12213, training loss: 6591.41, average training loss: 7235.48, base loss: 16108.97
[INFO 2017-06-29 10:49:00,355 main.py:57] epoch 12214, training loss: 6966.40, average training loss: 7235.54, base loss: 16109.03
[INFO 2017-06-29 10:49:03,452 main.py:57] epoch 12215, training loss: 7754.75, average training loss: 7235.50, base loss: 16109.28
[INFO 2017-06-29 10:49:06,509 main.py:57] epoch 12216, training loss: 6811.05, average training loss: 7233.34, base loss: 16109.21
[INFO 2017-06-29 10:49:09,683 main.py:57] epoch 12217, training loss: 7014.03, average training loss: 7233.52, base loss: 16108.74
[INFO 2017-06-29 10:49:12,702 main.py:57] epoch 12218, training loss: 7391.47, average training loss: 7233.77, base loss: 16108.60
[INFO 2017-06-29 10:49:15,739 main.py:57] epoch 12219, training loss: 7063.42, average training loss: 7231.98, base loss: 16108.24
[INFO 2017-06-29 10:49:18,767 main.py:57] epoch 12220, training loss: 7541.53, average training loss: 7232.77, base loss: 16108.42
[INFO 2017-06-29 10:49:21,811 main.py:57] epoch 12221, training loss: 6474.64, average training loss: 7232.04, base loss: 16108.44
[INFO 2017-06-29 10:49:24,923 main.py:57] epoch 12222, training loss: 7023.69, average training loss: 7231.77, base loss: 16108.21
[INFO 2017-06-29 10:49:28,040 main.py:57] epoch 12223, training loss: 7580.12, average training loss: 7232.46, base loss: 16108.24
[INFO 2017-06-29 10:49:31,126 main.py:57] epoch 12224, training loss: 7295.52, average training loss: 7232.53, base loss: 16108.36
[INFO 2017-06-29 10:49:34,149 main.py:57] epoch 12225, training loss: 7066.46, average training loss: 7233.16, base loss: 16108.57
[INFO 2017-06-29 10:49:37,194 main.py:57] epoch 12226, training loss: 6864.78, average training loss: 7233.20, base loss: 16108.14
[INFO 2017-06-29 10:49:40,236 main.py:57] epoch 12227, training loss: 6665.80, average training loss: 7232.80, base loss: 16107.91
[INFO 2017-06-29 10:49:43,251 main.py:57] epoch 12228, training loss: 8256.18, average training loss: 7233.36, base loss: 16108.16
[INFO 2017-06-29 10:49:46,240 main.py:57] epoch 12229, training loss: 7272.74, average training loss: 7232.08, base loss: 16108.17
[INFO 2017-06-29 10:49:49,306 main.py:57] epoch 12230, training loss: 8166.69, average training loss: 7233.26, base loss: 16108.57
[INFO 2017-06-29 10:49:52,388 main.py:57] epoch 12231, training loss: 7318.49, average training loss: 7233.91, base loss: 16108.54
[INFO 2017-06-29 10:49:55,408 main.py:57] epoch 12232, training loss: 8055.16, average training loss: 7233.94, base loss: 16108.93
[INFO 2017-06-29 10:49:58,462 main.py:57] epoch 12233, training loss: 6876.58, average training loss: 7233.41, base loss: 16108.85
[INFO 2017-06-29 10:50:01,531 main.py:57] epoch 12234, training loss: 7126.03, average training loss: 7232.55, base loss: 16108.88
[INFO 2017-06-29 10:50:04,606 main.py:57] epoch 12235, training loss: 7307.55, average training loss: 7232.59, base loss: 16108.87
[INFO 2017-06-29 10:50:07,642 main.py:57] epoch 12236, training loss: 7337.63, average training loss: 7232.69, base loss: 16108.93
[INFO 2017-06-29 10:50:10,812 main.py:57] epoch 12237, training loss: 6857.91, average training loss: 7232.13, base loss: 16108.81
[INFO 2017-06-29 10:50:13,905 main.py:57] epoch 12238, training loss: 7007.59, average training loss: 7232.42, base loss: 16108.76
[INFO 2017-06-29 10:50:17,014 main.py:57] epoch 12239, training loss: 8212.89, average training loss: 7233.03, base loss: 16109.34
[INFO 2017-06-29 10:50:20,081 main.py:57] epoch 12240, training loss: 7509.43, average training loss: 7232.80, base loss: 16109.78
[INFO 2017-06-29 10:50:23,116 main.py:57] epoch 12241, training loss: 9042.08, average training loss: 7235.10, base loss: 16110.48
[INFO 2017-06-29 10:50:26,123 main.py:57] epoch 12242, training loss: 7166.08, average training loss: 7235.18, base loss: 16110.64
[INFO 2017-06-29 10:50:29,163 main.py:57] epoch 12243, training loss: 6460.49, average training loss: 7234.73, base loss: 16110.46
[INFO 2017-06-29 10:50:32,240 main.py:57] epoch 12244, training loss: 7105.35, average training loss: 7234.95, base loss: 16110.34
[INFO 2017-06-29 10:50:35,317 main.py:57] epoch 12245, training loss: 7565.00, average training loss: 7235.15, base loss: 16110.82
[INFO 2017-06-29 10:50:38,337 main.py:57] epoch 12246, training loss: 6670.60, average training loss: 7235.67, base loss: 16110.73
[INFO 2017-06-29 10:50:41,353 main.py:57] epoch 12247, training loss: 6870.28, average training loss: 7235.68, base loss: 16110.84
[INFO 2017-06-29 10:50:44,403 main.py:57] epoch 12248, training loss: 7622.07, average training loss: 7236.26, base loss: 16111.22
[INFO 2017-06-29 10:50:47,462 main.py:57] epoch 12249, training loss: 7414.65, average training loss: 7235.96, base loss: 16111.46
[INFO 2017-06-29 10:50:50,429 main.py:57] epoch 12250, training loss: 7032.04, average training loss: 7236.01, base loss: 16111.32
[INFO 2017-06-29 10:50:53,472 main.py:57] epoch 12251, training loss: 7718.68, average training loss: 7237.48, base loss: 16111.24
[INFO 2017-06-29 10:50:56,470 main.py:57] epoch 12252, training loss: 7172.83, average training loss: 7237.39, base loss: 16111.13
[INFO 2017-06-29 10:50:59,525 main.py:57] epoch 12253, training loss: 6949.75, average training loss: 7237.12, base loss: 16110.99
[INFO 2017-06-29 10:51:02,557 main.py:57] epoch 12254, training loss: 7074.60, average training loss: 7236.62, base loss: 16110.83
[INFO 2017-06-29 10:51:05,606 main.py:57] epoch 12255, training loss: 7954.12, average training loss: 7236.80, base loss: 16110.66
[INFO 2017-06-29 10:51:08,596 main.py:57] epoch 12256, training loss: 6731.22, average training loss: 7236.79, base loss: 16110.54
[INFO 2017-06-29 10:51:11,609 main.py:57] epoch 12257, training loss: 7837.29, average training loss: 7237.48, base loss: 16110.62
[INFO 2017-06-29 10:51:14,681 main.py:57] epoch 12258, training loss: 7071.37, average training loss: 7237.14, base loss: 16110.44
[INFO 2017-06-29 10:51:17,804 main.py:57] epoch 12259, training loss: 6967.77, average training loss: 7236.58, base loss: 16110.22
[INFO 2017-06-29 10:51:20,902 main.py:57] epoch 12260, training loss: 7635.70, average training loss: 7237.76, base loss: 16110.15
[INFO 2017-06-29 10:51:23,989 main.py:57] epoch 12261, training loss: 7560.62, average training loss: 7238.64, base loss: 16110.02
[INFO 2017-06-29 10:51:27,076 main.py:57] epoch 12262, training loss: 7099.47, average training loss: 7238.38, base loss: 16110.15
[INFO 2017-06-29 10:51:30,101 main.py:57] epoch 12263, training loss: 6193.33, average training loss: 7237.50, base loss: 16110.02
[INFO 2017-06-29 10:51:33,111 main.py:57] epoch 12264, training loss: 6132.12, average training loss: 7236.21, base loss: 16109.87
[INFO 2017-06-29 10:51:36,146 main.py:57] epoch 12265, training loss: 7871.43, average training loss: 7236.53, base loss: 16110.22
[INFO 2017-06-29 10:51:39,166 main.py:57] epoch 12266, training loss: 7023.80, average training loss: 7236.76, base loss: 16109.98
[INFO 2017-06-29 10:51:42,187 main.py:57] epoch 12267, training loss: 7343.78, average training loss: 7235.44, base loss: 16110.06
[INFO 2017-06-29 10:51:45,171 main.py:57] epoch 12268, training loss: 7313.31, average training loss: 7235.18, base loss: 16109.99
[INFO 2017-06-29 10:51:48,247 main.py:57] epoch 12269, training loss: 6809.29, average training loss: 7235.38, base loss: 16110.03
[INFO 2017-06-29 10:51:51,291 main.py:57] epoch 12270, training loss: 7004.50, average training loss: 7234.75, base loss: 16110.11
[INFO 2017-06-29 10:51:54,309 main.py:57] epoch 12271, training loss: 7152.35, average training loss: 7235.14, base loss: 16109.92
[INFO 2017-06-29 10:51:57,412 main.py:57] epoch 12272, training loss: 6920.46, average training loss: 7233.54, base loss: 16109.84
[INFO 2017-06-29 10:52:00,456 main.py:57] epoch 12273, training loss: 7810.52, average training loss: 7235.01, base loss: 16109.76
[INFO 2017-06-29 10:52:03,508 main.py:57] epoch 12274, training loss: 7553.22, average training loss: 7235.54, base loss: 16109.75
[INFO 2017-06-29 10:52:06,609 main.py:57] epoch 12275, training loss: 6569.03, average training loss: 7235.09, base loss: 16109.37
[INFO 2017-06-29 10:52:09,624 main.py:57] epoch 12276, training loss: 7261.04, average training loss: 7235.84, base loss: 16109.38
[INFO 2017-06-29 10:52:12,712 main.py:57] epoch 12277, training loss: 7806.30, average training loss: 7236.39, base loss: 16109.83
[INFO 2017-06-29 10:52:15,903 main.py:57] epoch 12278, training loss: 6193.43, average training loss: 7235.73, base loss: 16109.73
[INFO 2017-06-29 10:52:19,022 main.py:57] epoch 12279, training loss: 7606.30, average training loss: 7236.30, base loss: 16109.97
[INFO 2017-06-29 10:52:22,119 main.py:57] epoch 12280, training loss: 7241.76, average training loss: 7235.66, base loss: 16109.87
[INFO 2017-06-29 10:52:25,155 main.py:57] epoch 12281, training loss: 6621.77, average training loss: 7234.34, base loss: 16109.52
[INFO 2017-06-29 10:52:28,207 main.py:57] epoch 12282, training loss: 6835.30, average training loss: 7234.75, base loss: 16109.75
[INFO 2017-06-29 10:52:31,280 main.py:57] epoch 12283, training loss: 7926.49, average training loss: 7235.09, base loss: 16110.28
[INFO 2017-06-29 10:52:34,286 main.py:57] epoch 12284, training loss: 6943.94, average training loss: 7235.02, base loss: 16110.23
[INFO 2017-06-29 10:52:37,444 main.py:57] epoch 12285, training loss: 6792.22, average training loss: 7235.10, base loss: 16110.44
[INFO 2017-06-29 10:52:40,466 main.py:57] epoch 12286, training loss: 6046.33, average training loss: 7232.66, base loss: 16109.96
[INFO 2017-06-29 10:52:43,512 main.py:57] epoch 12287, training loss: 6813.91, average training loss: 7231.48, base loss: 16109.62
[INFO 2017-06-29 10:52:46,532 main.py:57] epoch 12288, training loss: 7690.38, average training loss: 7232.80, base loss: 16109.66
[INFO 2017-06-29 10:52:49,636 main.py:57] epoch 12289, training loss: 7559.15, average training loss: 7233.47, base loss: 16109.92
[INFO 2017-06-29 10:52:52,664 main.py:57] epoch 12290, training loss: 7045.26, average training loss: 7233.40, base loss: 16109.77
[INFO 2017-06-29 10:52:55,715 main.py:57] epoch 12291, training loss: 6825.51, average training loss: 7232.88, base loss: 16109.91
[INFO 2017-06-29 10:52:58,715 main.py:57] epoch 12292, training loss: 6770.11, average training loss: 7232.04, base loss: 16109.70
[INFO 2017-06-29 10:53:01,792 main.py:57] epoch 12293, training loss: 7522.25, average training loss: 7232.95, base loss: 16109.67
[INFO 2017-06-29 10:53:04,813 main.py:57] epoch 12294, training loss: 6995.05, average training loss: 7233.41, base loss: 16109.29
[INFO 2017-06-29 10:53:07,921 main.py:57] epoch 12295, training loss: 7958.45, average training loss: 7234.42, base loss: 16109.48
[INFO 2017-06-29 10:53:10,993 main.py:57] epoch 12296, training loss: 6905.71, average training loss: 7234.55, base loss: 16109.31
[INFO 2017-06-29 10:53:14,164 main.py:57] epoch 12297, training loss: 6641.24, average training loss: 7234.38, base loss: 16108.88
[INFO 2017-06-29 10:53:17,186 main.py:57] epoch 12298, training loss: 7076.10, average training loss: 7235.04, base loss: 16109.15
[INFO 2017-06-29 10:53:20,304 main.py:57] epoch 12299, training loss: 7100.91, average training loss: 7235.27, base loss: 16109.34
[INFO 2017-06-29 10:53:20,305 main.py:59] epoch 12299, testing
[INFO 2017-06-29 10:53:32,985 main.py:104] average testing loss: 7885.43, base loss: 16310.07
[INFO 2017-06-29 10:53:32,985 main.py:105] improve_loss: 8424.64, improve_percent: 0.52
[INFO 2017-06-29 10:53:32,986 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:53:36,091 main.py:57] epoch 12300, training loss: 7174.79, average training loss: 7235.47, base loss: 16109.38
[INFO 2017-06-29 10:53:39,137 main.py:57] epoch 12301, training loss: 8136.80, average training loss: 7236.47, base loss: 16109.80
[INFO 2017-06-29 10:53:42,169 main.py:57] epoch 12302, training loss: 6591.70, average training loss: 7235.69, base loss: 16109.58
[INFO 2017-06-29 10:53:45,252 main.py:57] epoch 12303, training loss: 8318.96, average training loss: 7237.29, base loss: 16110.10
[INFO 2017-06-29 10:53:48,333 main.py:57] epoch 12304, training loss: 7693.29, average training loss: 7238.46, base loss: 16110.20
[INFO 2017-06-29 10:53:51,369 main.py:57] epoch 12305, training loss: 6918.59, average training loss: 7238.11, base loss: 16110.15
[INFO 2017-06-29 10:53:54,454 main.py:57] epoch 12306, training loss: 7721.31, average training loss: 7238.32, base loss: 16109.86
[INFO 2017-06-29 10:53:57,500 main.py:57] epoch 12307, training loss: 7243.27, average training loss: 7238.88, base loss: 16109.57
[INFO 2017-06-29 10:54:00,633 main.py:57] epoch 12308, training loss: 7214.83, average training loss: 7238.31, base loss: 16109.65
[INFO 2017-06-29 10:54:03,718 main.py:57] epoch 12309, training loss: 6696.41, average training loss: 7237.02, base loss: 16109.54
[INFO 2017-06-29 10:54:06,750 main.py:57] epoch 12310, training loss: 6815.84, average training loss: 7236.24, base loss: 16109.12
[INFO 2017-06-29 10:54:09,792 main.py:57] epoch 12311, training loss: 7263.08, average training loss: 7236.84, base loss: 16108.80
[INFO 2017-06-29 10:54:12,870 main.py:57] epoch 12312, training loss: 6402.74, average training loss: 7236.29, base loss: 16108.41
[INFO 2017-06-29 10:54:15,941 main.py:57] epoch 12313, training loss: 6239.00, average training loss: 7234.81, base loss: 16108.06
[INFO 2017-06-29 10:54:18,961 main.py:57] epoch 12314, training loss: 6891.89, average training loss: 7234.16, base loss: 16107.65
[INFO 2017-06-29 10:54:22,016 main.py:57] epoch 12315, training loss: 7811.92, average training loss: 7234.43, base loss: 16107.60
[INFO 2017-06-29 10:54:25,075 main.py:57] epoch 12316, training loss: 7144.30, average training loss: 7234.25, base loss: 16107.53
[INFO 2017-06-29 10:54:28,082 main.py:57] epoch 12317, training loss: 7156.79, average training loss: 7232.80, base loss: 16107.49
[INFO 2017-06-29 10:54:31,178 main.py:57] epoch 12318, training loss: 7329.27, average training loss: 7231.98, base loss: 16107.46
[INFO 2017-06-29 10:54:34,241 main.py:57] epoch 12319, training loss: 6877.34, average training loss: 7231.40, base loss: 16107.40
[INFO 2017-06-29 10:54:37,262 main.py:57] epoch 12320, training loss: 6707.43, average training loss: 7231.13, base loss: 16107.08
[INFO 2017-06-29 10:54:40,356 main.py:57] epoch 12321, training loss: 6483.27, average training loss: 7229.73, base loss: 16106.71
[INFO 2017-06-29 10:54:43,447 main.py:57] epoch 12322, training loss: 8623.04, average training loss: 7231.38, base loss: 16107.20
[INFO 2017-06-29 10:54:46,477 main.py:57] epoch 12323, training loss: 7240.35, average training loss: 7231.26, base loss: 16107.23
[INFO 2017-06-29 10:54:49,515 main.py:57] epoch 12324, training loss: 6620.36, average training loss: 7230.13, base loss: 16107.04
[INFO 2017-06-29 10:54:52,563 main.py:57] epoch 12325, training loss: 7139.33, average training loss: 7229.93, base loss: 16106.88
[INFO 2017-06-29 10:54:55,650 main.py:57] epoch 12326, training loss: 6534.63, average training loss: 7229.54, base loss: 16106.51
[INFO 2017-06-29 10:54:58,683 main.py:57] epoch 12327, training loss: 6232.82, average training loss: 7228.45, base loss: 16106.33
[INFO 2017-06-29 10:55:01,736 main.py:57] epoch 12328, training loss: 7524.72, average training loss: 7229.16, base loss: 16106.62
[INFO 2017-06-29 10:55:04,798 main.py:57] epoch 12329, training loss: 7118.62, average training loss: 7229.67, base loss: 16106.84
[INFO 2017-06-29 10:55:07,899 main.py:57] epoch 12330, training loss: 6908.18, average training loss: 7228.44, base loss: 16106.69
[INFO 2017-06-29 10:55:10,886 main.py:57] epoch 12331, training loss: 6833.41, average training loss: 7228.16, base loss: 16106.53
[INFO 2017-06-29 10:55:13,931 main.py:57] epoch 12332, training loss: 7010.38, average training loss: 7227.08, base loss: 16106.76
[INFO 2017-06-29 10:55:17,125 main.py:57] epoch 12333, training loss: 7776.74, average training loss: 7227.47, base loss: 16107.21
[INFO 2017-06-29 10:55:20,187 main.py:57] epoch 12334, training loss: 7161.05, average training loss: 7227.78, base loss: 16107.22
[INFO 2017-06-29 10:55:23,269 main.py:57] epoch 12335, training loss: 7221.65, average training loss: 7227.90, base loss: 16107.25
[INFO 2017-06-29 10:55:26,319 main.py:57] epoch 12336, training loss: 7552.70, average training loss: 7227.39, base loss: 16107.44
[INFO 2017-06-29 10:55:29,430 main.py:57] epoch 12337, training loss: 6852.98, average training loss: 7227.65, base loss: 16107.37
[INFO 2017-06-29 10:55:32,474 main.py:57] epoch 12338, training loss: 6478.91, average training loss: 7226.48, base loss: 16107.15
[INFO 2017-06-29 10:55:35,534 main.py:57] epoch 12339, training loss: 6461.69, average training loss: 7224.64, base loss: 16107.10
[INFO 2017-06-29 10:55:38,557 main.py:57] epoch 12340, training loss: 7317.69, average training loss: 7225.01, base loss: 16107.19
[INFO 2017-06-29 10:55:41,555 main.py:57] epoch 12341, training loss: 7155.60, average training loss: 7225.21, base loss: 16107.23
[INFO 2017-06-29 10:55:44,620 main.py:57] epoch 12342, training loss: 6472.66, average training loss: 7224.44, base loss: 16107.20
[INFO 2017-06-29 10:55:47,649 main.py:57] epoch 12343, training loss: 7551.86, average training loss: 7224.82, base loss: 16107.34
[INFO 2017-06-29 10:55:50,712 main.py:57] epoch 12344, training loss: 6912.06, average training loss: 7224.22, base loss: 16107.28
[INFO 2017-06-29 10:55:53,807 main.py:57] epoch 12345, training loss: 6841.97, average training loss: 7223.88, base loss: 16107.28
[INFO 2017-06-29 10:55:56,933 main.py:57] epoch 12346, training loss: 6597.35, average training loss: 7223.88, base loss: 16107.27
[INFO 2017-06-29 10:56:00,021 main.py:57] epoch 12347, training loss: 6800.52, average training loss: 7223.93, base loss: 16107.23
[INFO 2017-06-29 10:56:03,077 main.py:57] epoch 12348, training loss: 7358.22, average training loss: 7222.92, base loss: 16107.32
[INFO 2017-06-29 10:56:06,255 main.py:57] epoch 12349, training loss: 7240.31, average training loss: 7223.21, base loss: 16107.47
[INFO 2017-06-29 10:56:09,265 main.py:57] epoch 12350, training loss: 6954.74, average training loss: 7222.39, base loss: 16107.35
[INFO 2017-06-29 10:56:12,295 main.py:57] epoch 12351, training loss: 7449.31, average training loss: 7222.06, base loss: 16107.61
[INFO 2017-06-29 10:56:15,430 main.py:57] epoch 12352, training loss: 7039.08, average training loss: 7221.44, base loss: 16107.55
[INFO 2017-06-29 10:56:18,526 main.py:57] epoch 12353, training loss: 7393.06, average training loss: 7221.82, base loss: 16107.53
[INFO 2017-06-29 10:56:21,556 main.py:57] epoch 12354, training loss: 6924.88, average training loss: 7220.96, base loss: 16107.64
[INFO 2017-06-29 10:56:24,607 main.py:57] epoch 12355, training loss: 6858.32, average training loss: 7220.08, base loss: 16107.52
[INFO 2017-06-29 10:56:27,676 main.py:57] epoch 12356, training loss: 6828.52, average training loss: 7220.44, base loss: 16107.57
[INFO 2017-06-29 10:56:30,745 main.py:57] epoch 12357, training loss: 6574.84, average training loss: 7219.86, base loss: 16107.45
[INFO 2017-06-29 10:56:33,779 main.py:57] epoch 12358, training loss: 7510.58, average training loss: 7220.05, base loss: 16107.69
[INFO 2017-06-29 10:56:36,818 main.py:57] epoch 12359, training loss: 6650.01, average training loss: 7220.00, base loss: 16107.58
[INFO 2017-06-29 10:56:39,862 main.py:57] epoch 12360, training loss: 7203.88, average training loss: 7220.55, base loss: 16107.63
[INFO 2017-06-29 10:56:42,915 main.py:57] epoch 12361, training loss: 6346.21, average training loss: 7220.34, base loss: 16107.36
[INFO 2017-06-29 10:56:45,966 main.py:57] epoch 12362, training loss: 7494.09, average training loss: 7221.04, base loss: 16107.41
[INFO 2017-06-29 10:56:49,052 main.py:57] epoch 12363, training loss: 7007.35, average training loss: 7219.58, base loss: 16107.51
[INFO 2017-06-29 10:56:52,095 main.py:57] epoch 12364, training loss: 6853.22, average training loss: 7218.87, base loss: 16107.39
[INFO 2017-06-29 10:56:55,180 main.py:57] epoch 12365, training loss: 6467.41, average training loss: 7218.87, base loss: 16107.26
[INFO 2017-06-29 10:56:58,242 main.py:57] epoch 12366, training loss: 7210.39, average training loss: 7218.64, base loss: 16107.73
[INFO 2017-06-29 10:57:01,243 main.py:57] epoch 12367, training loss: 6400.59, average training loss: 7218.36, base loss: 16107.66
[INFO 2017-06-29 10:57:04,293 main.py:57] epoch 12368, training loss: 6803.26, average training loss: 7218.13, base loss: 16107.68
[INFO 2017-06-29 10:57:07,321 main.py:57] epoch 12369, training loss: 7343.23, average training loss: 7218.69, base loss: 16107.90
[INFO 2017-06-29 10:57:10,382 main.py:57] epoch 12370, training loss: 7185.37, average training loss: 7218.95, base loss: 16107.92
[INFO 2017-06-29 10:57:13,455 main.py:57] epoch 12371, training loss: 6841.65, average training loss: 7218.10, base loss: 16107.84
[INFO 2017-06-29 10:57:16,493 main.py:57] epoch 12372, training loss: 7085.17, average training loss: 7217.52, base loss: 16108.03
[INFO 2017-06-29 10:57:19,571 main.py:57] epoch 12373, training loss: 6969.53, average training loss: 7217.29, base loss: 16107.74
[INFO 2017-06-29 10:57:22,618 main.py:57] epoch 12374, training loss: 6904.08, average training loss: 7215.64, base loss: 16107.72
[INFO 2017-06-29 10:57:25,703 main.py:57] epoch 12375, training loss: 7116.15, average training loss: 7215.44, base loss: 16107.77
[INFO 2017-06-29 10:57:28,702 main.py:57] epoch 12376, training loss: 6185.77, average training loss: 7214.62, base loss: 16107.58
[INFO 2017-06-29 10:57:31,927 main.py:57] epoch 12377, training loss: 6480.44, average training loss: 7214.17, base loss: 16107.49
[INFO 2017-06-29 10:57:34,972 main.py:57] epoch 12378, training loss: 7530.79, average training loss: 7215.05, base loss: 16107.65
[INFO 2017-06-29 10:57:38,006 main.py:57] epoch 12379, training loss: 6808.24, average training loss: 7215.08, base loss: 16107.61
[INFO 2017-06-29 10:57:41,055 main.py:57] epoch 12380, training loss: 6525.86, average training loss: 7215.04, base loss: 16107.51
[INFO 2017-06-29 10:57:44,109 main.py:57] epoch 12381, training loss: 6730.17, average training loss: 7213.58, base loss: 16107.32
[INFO 2017-06-29 10:57:47,187 main.py:57] epoch 12382, training loss: 6272.84, average training loss: 7211.94, base loss: 16107.16
[INFO 2017-06-29 10:57:50,203 main.py:57] epoch 12383, training loss: 6836.22, average training loss: 7211.97, base loss: 16107.24
[INFO 2017-06-29 10:57:53,285 main.py:57] epoch 12384, training loss: 8076.22, average training loss: 7213.22, base loss: 16107.38
[INFO 2017-06-29 10:57:56,335 main.py:57] epoch 12385, training loss: 8486.71, average training loss: 7214.52, base loss: 16107.58
[INFO 2017-06-29 10:57:59,409 main.py:57] epoch 12386, training loss: 7099.23, average training loss: 7215.44, base loss: 16107.52
[INFO 2017-06-29 10:58:02,446 main.py:57] epoch 12387, training loss: 7711.56, average training loss: 7215.87, base loss: 16107.73
[INFO 2017-06-29 10:58:05,543 main.py:57] epoch 12388, training loss: 6764.48, average training loss: 7213.85, base loss: 16107.29
[INFO 2017-06-29 10:58:08,553 main.py:57] epoch 12389, training loss: 6648.32, average training loss: 7213.16, base loss: 16106.89
[INFO 2017-06-29 10:58:11,555 main.py:57] epoch 12390, training loss: 6962.58, average training loss: 7212.69, base loss: 16107.04
[INFO 2017-06-29 10:58:14,622 main.py:57] epoch 12391, training loss: 6832.30, average training loss: 7212.56, base loss: 16106.98
[INFO 2017-06-29 10:58:17,682 main.py:57] epoch 12392, training loss: 6732.76, average training loss: 7212.72, base loss: 16107.03
[INFO 2017-06-29 10:58:20,694 main.py:57] epoch 12393, training loss: 8080.68, average training loss: 7214.28, base loss: 16107.41
[INFO 2017-06-29 10:58:23,779 main.py:57] epoch 12394, training loss: 7074.86, average training loss: 7213.43, base loss: 16107.44
[INFO 2017-06-29 10:58:26,748 main.py:57] epoch 12395, training loss: 6314.12, average training loss: 7212.31, base loss: 16106.96
[INFO 2017-06-29 10:58:29,795 main.py:57] epoch 12396, training loss: 7551.18, average training loss: 7213.03, base loss: 16107.08
[INFO 2017-06-29 10:58:32,823 main.py:57] epoch 12397, training loss: 6478.78, average training loss: 7212.31, base loss: 16106.77
[INFO 2017-06-29 10:58:35,872 main.py:57] epoch 12398, training loss: 8115.35, average training loss: 7212.60, base loss: 16106.92
[INFO 2017-06-29 10:58:38,916 main.py:57] epoch 12399, training loss: 7073.48, average training loss: 7213.09, base loss: 16106.81
[INFO 2017-06-29 10:58:38,916 main.py:59] epoch 12399, testing
[INFO 2017-06-29 10:58:51,475 main.py:104] average testing loss: 8039.75, base loss: 16659.63
[INFO 2017-06-29 10:58:51,476 main.py:105] improve_loss: 8619.88, improve_percent: 0.52
[INFO 2017-06-29 10:58:51,477 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 10:58:54,501 main.py:57] epoch 12400, training loss: 7036.25, average training loss: 7212.56, base loss: 16106.60
[INFO 2017-06-29 10:58:57,543 main.py:57] epoch 12401, training loss: 7366.42, average training loss: 7211.92, base loss: 16106.42
[INFO 2017-06-29 10:59:00,556 main.py:57] epoch 12402, training loss: 7857.04, average training loss: 7212.17, base loss: 16106.84
[INFO 2017-06-29 10:59:03,525 main.py:57] epoch 12403, training loss: 7630.86, average training loss: 7211.31, base loss: 16106.99
[INFO 2017-06-29 10:59:06,530 main.py:57] epoch 12404, training loss: 6655.32, average training loss: 7210.22, base loss: 16106.69
[INFO 2017-06-29 10:59:09,633 main.py:57] epoch 12405, training loss: 6668.63, average training loss: 7209.65, base loss: 16106.24
[INFO 2017-06-29 10:59:12,691 main.py:57] epoch 12406, training loss: 7074.07, average training loss: 7208.88, base loss: 16106.02
[INFO 2017-06-29 10:59:15,758 main.py:57] epoch 12407, training loss: 6902.55, average training loss: 7208.89, base loss: 16105.87
[INFO 2017-06-29 10:59:18,812 main.py:57] epoch 12408, training loss: 6862.65, average training loss: 7208.09, base loss: 16106.07
[INFO 2017-06-29 10:59:21,883 main.py:57] epoch 12409, training loss: 7432.56, average training loss: 7209.02, base loss: 16106.04
[INFO 2017-06-29 10:59:24,875 main.py:57] epoch 12410, training loss: 7364.42, average training loss: 7209.34, base loss: 16106.03
[INFO 2017-06-29 10:59:27,963 main.py:57] epoch 12411, training loss: 6587.75, average training loss: 7208.22, base loss: 16105.71
[INFO 2017-06-29 10:59:31,024 main.py:57] epoch 12412, training loss: 7392.44, average training loss: 7208.07, base loss: 16105.71
[INFO 2017-06-29 10:59:34,074 main.py:57] epoch 12413, training loss: 7044.82, average training loss: 7208.61, base loss: 16105.62
[INFO 2017-06-29 10:59:37,129 main.py:57] epoch 12414, training loss: 7130.09, average training loss: 7207.36, base loss: 16105.84
[INFO 2017-06-29 10:59:40,153 main.py:57] epoch 12415, training loss: 6915.39, average training loss: 7207.23, base loss: 16105.89
[INFO 2017-06-29 10:59:43,126 main.py:57] epoch 12416, training loss: 7244.76, average training loss: 7207.24, base loss: 16105.75
[INFO 2017-06-29 10:59:46,245 main.py:57] epoch 12417, training loss: 7559.88, average training loss: 7207.36, base loss: 16105.50
[INFO 2017-06-29 10:59:49,300 main.py:57] epoch 12418, training loss: 6765.92, average training loss: 7207.74, base loss: 16105.58
[INFO 2017-06-29 10:59:52,292 main.py:57] epoch 12419, training loss: 6510.04, average training loss: 7207.64, base loss: 16105.69
[INFO 2017-06-29 10:59:55,379 main.py:57] epoch 12420, training loss: 6859.74, average training loss: 7207.05, base loss: 16105.56
[INFO 2017-06-29 10:59:58,420 main.py:57] epoch 12421, training loss: 6782.57, average training loss: 7207.51, base loss: 16105.40
[INFO 2017-06-29 11:00:01,472 main.py:57] epoch 12422, training loss: 6653.72, average training loss: 7207.05, base loss: 16105.16
[INFO 2017-06-29 11:00:04,575 main.py:57] epoch 12423, training loss: 7661.96, average training loss: 7206.80, base loss: 16105.28
[INFO 2017-06-29 11:00:07,605 main.py:57] epoch 12424, training loss: 7540.81, average training loss: 7207.09, base loss: 16105.44
[INFO 2017-06-29 11:00:10,669 main.py:57] epoch 12425, training loss: 7562.85, average training loss: 7206.78, base loss: 16105.49
[INFO 2017-06-29 11:00:13,736 main.py:57] epoch 12426, training loss: 7438.53, average training loss: 7206.75, base loss: 16105.82
[INFO 2017-06-29 11:00:16,818 main.py:57] epoch 12427, training loss: 7589.06, average training loss: 7207.39, base loss: 16106.08
[INFO 2017-06-29 11:00:19,876 main.py:57] epoch 12428, training loss: 6806.96, average training loss: 7206.89, base loss: 16106.02
[INFO 2017-06-29 11:00:22,918 main.py:57] epoch 12429, training loss: 7127.38, average training loss: 7207.23, base loss: 16106.19
[INFO 2017-06-29 11:00:25,970 main.py:57] epoch 12430, training loss: 6949.23, average training loss: 7207.23, base loss: 16106.25
[INFO 2017-06-29 11:00:29,020 main.py:57] epoch 12431, training loss: 6196.86, average training loss: 7206.36, base loss: 16105.90
[INFO 2017-06-29 11:00:32,031 main.py:57] epoch 12432, training loss: 6948.43, average training loss: 7205.57, base loss: 16105.92
[INFO 2017-06-29 11:00:35,062 main.py:57] epoch 12433, training loss: 6236.95, average training loss: 7204.98, base loss: 16105.68
[INFO 2017-06-29 11:00:38,076 main.py:57] epoch 12434, training loss: 8072.67, average training loss: 7205.85, base loss: 16105.57
[INFO 2017-06-29 11:00:41,192 main.py:57] epoch 12435, training loss: 8176.89, average training loss: 7206.86, base loss: 16105.57
[INFO 2017-06-29 11:00:44,213 main.py:57] epoch 12436, training loss: 7031.15, average training loss: 7206.47, base loss: 16105.54
[INFO 2017-06-29 11:00:47,325 main.py:57] epoch 12437, training loss: 7048.49, average training loss: 7206.49, base loss: 16105.49
[INFO 2017-06-29 11:00:50,317 main.py:57] epoch 12438, training loss: 7429.73, average training loss: 7206.47, base loss: 16105.62
[INFO 2017-06-29 11:00:53,366 main.py:57] epoch 12439, training loss: 6813.55, average training loss: 7206.16, base loss: 16105.44
[INFO 2017-06-29 11:00:56,409 main.py:57] epoch 12440, training loss: 7336.07, average training loss: 7206.52, base loss: 16105.65
[INFO 2017-06-29 11:00:59,429 main.py:57] epoch 12441, training loss: 6844.98, average training loss: 7206.70, base loss: 16105.46
[INFO 2017-06-29 11:01:02,453 main.py:57] epoch 12442, training loss: 8338.93, average training loss: 7206.21, base loss: 16105.67
[INFO 2017-06-29 11:01:05,556 main.py:57] epoch 12443, training loss: 6548.78, average training loss: 7205.26, base loss: 16105.17
[INFO 2017-06-29 11:01:08,567 main.py:57] epoch 12444, training loss: 7222.56, average training loss: 7206.59, base loss: 16104.94
[INFO 2017-06-29 11:01:11,556 main.py:57] epoch 12445, training loss: 6740.04, average training loss: 7207.04, base loss: 16104.51
[INFO 2017-06-29 11:01:14,637 main.py:57] epoch 12446, training loss: 6983.92, average training loss: 7207.20, base loss: 16104.12
[INFO 2017-06-29 11:01:17,669 main.py:57] epoch 12447, training loss: 7740.94, average training loss: 7208.38, base loss: 16104.12
[INFO 2017-06-29 11:01:20,793 main.py:57] epoch 12448, training loss: 6550.33, average training loss: 7206.43, base loss: 16103.86
[INFO 2017-06-29 11:01:23,860 main.py:57] epoch 12449, training loss: 6857.09, average training loss: 7205.27, base loss: 16103.66
[INFO 2017-06-29 11:01:26,883 main.py:57] epoch 12450, training loss: 7694.79, average training loss: 7205.66, base loss: 16103.82
[INFO 2017-06-29 11:01:29,908 main.py:57] epoch 12451, training loss: 6857.20, average training loss: 7203.88, base loss: 16103.96
[INFO 2017-06-29 11:01:33,038 main.py:57] epoch 12452, training loss: 7387.41, average training loss: 7203.89, base loss: 16104.18
[INFO 2017-06-29 11:01:36,049 main.py:57] epoch 12453, training loss: 7131.86, average training loss: 7203.90, base loss: 16104.07
[INFO 2017-06-29 11:01:39,105 main.py:57] epoch 12454, training loss: 6688.82, average training loss: 7203.56, base loss: 16103.85
[INFO 2017-06-29 11:01:42,143 main.py:57] epoch 12455, training loss: 8138.87, average training loss: 7204.52, base loss: 16104.08
[INFO 2017-06-29 11:01:45,153 main.py:57] epoch 12456, training loss: 7101.00, average training loss: 7205.06, base loss: 16104.25
[INFO 2017-06-29 11:01:48,224 main.py:57] epoch 12457, training loss: 6797.01, average training loss: 7205.11, base loss: 16104.13
[INFO 2017-06-29 11:01:51,304 main.py:57] epoch 12458, training loss: 6961.27, average training loss: 7204.21, base loss: 16104.09
[INFO 2017-06-29 11:01:54,426 main.py:57] epoch 12459, training loss: 7408.83, average training loss: 7204.94, base loss: 16104.13
[INFO 2017-06-29 11:01:57,484 main.py:57] epoch 12460, training loss: 7477.91, average training loss: 7204.91, base loss: 16104.14
[INFO 2017-06-29 11:02:00,599 main.py:57] epoch 12461, training loss: 7069.44, average training loss: 7203.46, base loss: 16104.11
[INFO 2017-06-29 11:02:03,620 main.py:57] epoch 12462, training loss: 7446.44, average training loss: 7203.20, base loss: 16104.41
[INFO 2017-06-29 11:02:06,686 main.py:57] epoch 12463, training loss: 6669.48, average training loss: 7203.26, base loss: 16104.52
[INFO 2017-06-29 11:02:09,734 main.py:57] epoch 12464, training loss: 6606.25, average training loss: 7202.54, base loss: 16104.65
[INFO 2017-06-29 11:02:12,766 main.py:57] epoch 12465, training loss: 7764.07, average training loss: 7203.52, base loss: 16104.93
[INFO 2017-06-29 11:02:15,823 main.py:57] epoch 12466, training loss: 6031.37, average training loss: 7201.50, base loss: 16104.65
[INFO 2017-06-29 11:02:18,893 main.py:57] epoch 12467, training loss: 6553.66, average training loss: 7200.89, base loss: 16104.63
[INFO 2017-06-29 11:02:21,953 main.py:57] epoch 12468, training loss: 8147.33, average training loss: 7200.92, base loss: 16105.05
[INFO 2017-06-29 11:02:25,014 main.py:57] epoch 12469, training loss: 6748.60, average training loss: 7200.82, base loss: 16104.90
[INFO 2017-06-29 11:02:28,039 main.py:57] epoch 12470, training loss: 8113.29, average training loss: 7201.68, base loss: 16105.13
[INFO 2017-06-29 11:02:31,058 main.py:57] epoch 12471, training loss: 7772.61, average training loss: 7202.73, base loss: 16105.03
[INFO 2017-06-29 11:02:34,063 main.py:57] epoch 12472, training loss: 7069.66, average training loss: 7202.67, base loss: 16105.18
[INFO 2017-06-29 11:02:37,172 main.py:57] epoch 12473, training loss: 7154.59, average training loss: 7202.15, base loss: 16105.39
[INFO 2017-06-29 11:02:40,217 main.py:57] epoch 12474, training loss: 9389.64, average training loss: 7205.59, base loss: 16106.18
[INFO 2017-06-29 11:02:43,236 main.py:57] epoch 12475, training loss: 8108.65, average training loss: 7206.58, base loss: 16106.37
[INFO 2017-06-29 11:02:46,251 main.py:57] epoch 12476, training loss: 7171.73, average training loss: 7206.48, base loss: 16106.24
[INFO 2017-06-29 11:02:49,282 main.py:57] epoch 12477, training loss: 7237.13, average training loss: 7206.56, base loss: 16106.38
[INFO 2017-06-29 11:02:52,316 main.py:57] epoch 12478, training loss: 6970.12, average training loss: 7206.52, base loss: 16106.17
[INFO 2017-06-29 11:02:55,433 main.py:57] epoch 12479, training loss: 7070.16, average training loss: 7205.31, base loss: 16105.98
[INFO 2017-06-29 11:02:58,488 main.py:57] epoch 12480, training loss: 7635.88, average training loss: 7204.77, base loss: 16106.08
[INFO 2017-06-29 11:03:01,473 main.py:57] epoch 12481, training loss: 7324.24, average training loss: 7204.84, base loss: 16106.12
[INFO 2017-06-29 11:03:04,449 main.py:57] epoch 12482, training loss: 6809.41, average training loss: 7204.05, base loss: 16106.00
[INFO 2017-06-29 11:03:07,486 main.py:57] epoch 12483, training loss: 7105.08, average training loss: 7204.19, base loss: 16106.11
[INFO 2017-06-29 11:03:10,613 main.py:57] epoch 12484, training loss: 6570.62, average training loss: 7202.42, base loss: 16106.12
[INFO 2017-06-29 11:03:13,648 main.py:57] epoch 12485, training loss: 7980.08, average training loss: 7202.64, base loss: 16106.27
[INFO 2017-06-29 11:03:16,672 main.py:57] epoch 12486, training loss: 7125.89, average training loss: 7203.05, base loss: 16106.05
[INFO 2017-06-29 11:03:19,633 main.py:57] epoch 12487, training loss: 6681.09, average training loss: 7201.97, base loss: 16105.49
[INFO 2017-06-29 11:03:22,609 main.py:57] epoch 12488, training loss: 7387.25, average training loss: 7202.45, base loss: 16105.73
[INFO 2017-06-29 11:03:25,670 main.py:57] epoch 12489, training loss: 6647.44, average training loss: 7201.51, base loss: 16105.61
[INFO 2017-06-29 11:03:28,692 main.py:57] epoch 12490, training loss: 7383.83, average training loss: 7201.29, base loss: 16105.57
[INFO 2017-06-29 11:03:31,800 main.py:57] epoch 12491, training loss: 7059.15, average training loss: 7201.76, base loss: 16105.70
[INFO 2017-06-29 11:03:34,867 main.py:57] epoch 12492, training loss: 7994.92, average training loss: 7202.92, base loss: 16105.89
[INFO 2017-06-29 11:03:37,947 main.py:57] epoch 12493, training loss: 7689.20, average training loss: 7203.40, base loss: 16105.97
[INFO 2017-06-29 11:03:40,995 main.py:57] epoch 12494, training loss: 7483.21, average training loss: 7204.05, base loss: 16106.06
[INFO 2017-06-29 11:03:44,086 main.py:57] epoch 12495, training loss: 7065.87, average training loss: 7203.22, base loss: 16105.95
[INFO 2017-06-29 11:03:47,104 main.py:57] epoch 12496, training loss: 6374.48, average training loss: 7202.66, base loss: 16105.37
[INFO 2017-06-29 11:03:50,205 main.py:57] epoch 12497, training loss: 7825.89, average training loss: 7202.88, base loss: 16105.42
[INFO 2017-06-29 11:03:53,215 main.py:57] epoch 12498, training loss: 8012.27, average training loss: 7204.23, base loss: 16105.69
[INFO 2017-06-29 11:03:56,297 main.py:57] epoch 12499, training loss: 6980.73, average training loss: 7204.01, base loss: 16105.74
[INFO 2017-06-29 11:03:56,297 main.py:59] epoch 12499, testing
[INFO 2017-06-29 11:04:08,934 main.py:104] average testing loss: 8338.01, base loss: 17527.28
[INFO 2017-06-29 11:04:08,934 main.py:105] improve_loss: 9189.27, improve_percent: 0.52
[INFO 2017-06-29 11:04:08,935 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 11:04:11,970 main.py:57] epoch 12500, training loss: 7808.73, average training loss: 7204.75, base loss: 16105.95
[INFO 2017-06-29 11:04:15,068 main.py:57] epoch 12501, training loss: 6192.01, average training loss: 7203.97, base loss: 16105.69
[INFO 2017-06-29 11:04:18,120 main.py:57] epoch 12502, training loss: 7384.90, average training loss: 7203.83, base loss: 16105.77
[INFO 2017-06-29 11:04:21,189 main.py:57] epoch 12503, training loss: 8155.35, average training loss: 7204.31, base loss: 16106.04
[INFO 2017-06-29 11:04:24,269 main.py:57] epoch 12504, training loss: 8710.46, average training loss: 7205.74, base loss: 16106.55
[INFO 2017-06-29 11:04:27,341 main.py:57] epoch 12505, training loss: 6596.19, average training loss: 7205.32, base loss: 16106.37
[INFO 2017-06-29 11:04:30,427 main.py:57] epoch 12506, training loss: 6858.41, average training loss: 7203.97, base loss: 16106.58
[INFO 2017-06-29 11:04:33,529 main.py:57] epoch 12507, training loss: 6586.96, average training loss: 7203.45, base loss: 16106.64
[INFO 2017-06-29 11:04:36,601 main.py:57] epoch 12508, training loss: 7787.98, average training loss: 7204.16, base loss: 16106.89
[INFO 2017-06-29 11:04:39,688 main.py:57] epoch 12509, training loss: 7007.83, average training loss: 7204.48, base loss: 16106.90
[INFO 2017-06-29 11:04:42,715 main.py:57] epoch 12510, training loss: 6380.42, average training loss: 7204.31, base loss: 16106.61
[INFO 2017-06-29 11:04:45,771 main.py:57] epoch 12511, training loss: 7179.22, average training loss: 7204.55, base loss: 16106.75
[INFO 2017-06-29 11:04:48,834 main.py:57] epoch 12512, training loss: 8939.42, average training loss: 7206.42, base loss: 16107.49
[INFO 2017-06-29 11:04:51,897 main.py:57] epoch 12513, training loss: 6476.83, average training loss: 7206.35, base loss: 16107.51
[INFO 2017-06-29 11:04:54,920 main.py:57] epoch 12514, training loss: 6054.36, average training loss: 7205.27, base loss: 16107.24
[INFO 2017-06-29 11:04:57,988 main.py:57] epoch 12515, training loss: 7493.56, average training loss: 7204.89, base loss: 16107.78
[INFO 2017-06-29 11:05:01,040 main.py:57] epoch 12516, training loss: 8720.50, average training loss: 7206.50, base loss: 16108.31
[INFO 2017-06-29 11:05:04,075 main.py:57] epoch 12517, training loss: 7072.83, average training loss: 7206.40, base loss: 16108.23
[INFO 2017-06-29 11:05:07,253 main.py:57] epoch 12518, training loss: 7115.97, average training loss: 7205.71, base loss: 16107.91
[INFO 2017-06-29 11:05:10,354 main.py:57] epoch 12519, training loss: 7361.58, average training loss: 7205.12, base loss: 16107.70
[INFO 2017-06-29 11:05:13,410 main.py:57] epoch 12520, training loss: 7119.28, average training loss: 7205.73, base loss: 16107.91
[INFO 2017-06-29 11:05:16,497 main.py:57] epoch 12521, training loss: 6709.60, average training loss: 7204.52, base loss: 16107.71
[INFO 2017-06-29 11:05:19,544 main.py:57] epoch 12522, training loss: 6959.94, average training loss: 7204.74, base loss: 16107.81
[INFO 2017-06-29 11:05:22,529 main.py:57] epoch 12523, training loss: 7648.81, average training loss: 7205.69, base loss: 16107.82
[INFO 2017-06-29 11:05:25,618 main.py:57] epoch 12524, training loss: 6844.10, average training loss: 7205.19, base loss: 16107.82
[INFO 2017-06-29 11:05:28,703 main.py:57] epoch 12525, training loss: 8106.02, average training loss: 7205.84, base loss: 16108.05
[INFO 2017-06-29 11:05:31,792 main.py:57] epoch 12526, training loss: 5767.00, average training loss: 7204.73, base loss: 16107.53
[INFO 2017-06-29 11:05:34,862 main.py:57] epoch 12527, training loss: 6526.08, average training loss: 7204.09, base loss: 16107.33
[INFO 2017-06-29 11:05:37,952 main.py:57] epoch 12528, training loss: 6840.67, average training loss: 7204.16, base loss: 16107.24
[INFO 2017-06-29 11:05:41,014 main.py:57] epoch 12529, training loss: 7505.61, average training loss: 7204.26, base loss: 16107.23
[INFO 2017-06-29 11:05:44,107 main.py:57] epoch 12530, training loss: 7496.60, average training loss: 7204.62, base loss: 16107.79
[INFO 2017-06-29 11:05:47,218 main.py:57] epoch 12531, training loss: 7543.81, average training loss: 7204.94, base loss: 16108.07
[INFO 2017-06-29 11:05:50,243 main.py:57] epoch 12532, training loss: 7595.53, average training loss: 7205.36, base loss: 16108.31
[INFO 2017-06-29 11:05:53,267 main.py:57] epoch 12533, training loss: 6764.16, average training loss: 7205.76, base loss: 16108.07
[INFO 2017-06-29 11:05:56,295 main.py:57] epoch 12534, training loss: 8401.44, average training loss: 7207.15, base loss: 16108.13
[INFO 2017-06-29 11:05:59,406 main.py:57] epoch 12535, training loss: 7713.95, average training loss: 7207.20, base loss: 16108.14
[INFO 2017-06-29 11:06:02,502 main.py:57] epoch 12536, training loss: 8547.00, average training loss: 7206.85, base loss: 16108.52
[INFO 2017-06-29 11:06:05,536 main.py:57] epoch 12537, training loss: 7194.90, average training loss: 7207.92, base loss: 16108.45
[INFO 2017-06-29 11:06:08,572 main.py:57] epoch 12538, training loss: 7017.94, average training loss: 7206.92, base loss: 16108.18
[INFO 2017-06-29 11:06:11,641 main.py:57] epoch 12539, training loss: 6653.23, average training loss: 7207.37, base loss: 16107.96
[INFO 2017-06-29 11:06:14,719 main.py:57] epoch 12540, training loss: 7344.45, average training loss: 7206.67, base loss: 16107.89
[INFO 2017-06-29 11:06:17,808 main.py:57] epoch 12541, training loss: 8561.15, average training loss: 7208.81, base loss: 16108.43
[INFO 2017-06-29 11:06:20,875 main.py:57] epoch 12542, training loss: 7006.74, average training loss: 7208.53, base loss: 16108.19
[INFO 2017-06-29 11:06:23,937 main.py:57] epoch 12543, training loss: 6962.96, average training loss: 7208.65, base loss: 16107.97
[INFO 2017-06-29 11:06:27,000 main.py:57] epoch 12544, training loss: 7893.51, average training loss: 7209.81, base loss: 16108.38
[INFO 2017-06-29 11:06:29,989 main.py:57] epoch 12545, training loss: 7759.50, average training loss: 7210.80, base loss: 16108.60
[INFO 2017-06-29 11:06:32,990 main.py:57] epoch 12546, training loss: 7874.31, average training loss: 7212.09, base loss: 16108.77
[INFO 2017-06-29 11:06:36,090 main.py:57] epoch 12547, training loss: 6432.30, average training loss: 7211.60, base loss: 16108.47
[INFO 2017-06-29 11:06:39,103 main.py:57] epoch 12548, training loss: 6710.67, average training loss: 7211.55, base loss: 16108.33
[INFO 2017-06-29 11:06:42,227 main.py:57] epoch 12549, training loss: 7906.64, average training loss: 7212.56, base loss: 16108.65
[INFO 2017-06-29 11:06:45,322 main.py:57] epoch 12550, training loss: 7044.78, average training loss: 7211.29, base loss: 16108.78
[INFO 2017-06-29 11:06:48,343 main.py:57] epoch 12551, training loss: 6940.74, average training loss: 7210.84, base loss: 16108.84
[INFO 2017-06-29 11:06:51,401 main.py:57] epoch 12552, training loss: 8007.58, average training loss: 7210.78, base loss: 16108.92
[INFO 2017-06-29 11:06:54,562 main.py:57] epoch 12553, training loss: 7145.63, average training loss: 7211.11, base loss: 16108.77
[INFO 2017-06-29 11:06:57,571 main.py:57] epoch 12554, training loss: 6711.15, average training loss: 7210.63, base loss: 16108.69
[INFO 2017-06-29 11:07:00,665 main.py:57] epoch 12555, training loss: 6975.43, average training loss: 7210.73, base loss: 16108.62
[INFO 2017-06-29 11:07:03,716 main.py:57] epoch 12556, training loss: 7486.87, average training loss: 7210.52, base loss: 16108.79
[INFO 2017-06-29 11:07:06,758 main.py:57] epoch 12557, training loss: 6634.75, average training loss: 7210.57, base loss: 16108.73
[INFO 2017-06-29 11:07:09,827 main.py:57] epoch 12558, training loss: 6903.74, average training loss: 7209.90, base loss: 16108.74
[INFO 2017-06-29 11:07:12,842 main.py:57] epoch 12559, training loss: 7255.97, average training loss: 7209.67, base loss: 16109.00
[INFO 2017-06-29 11:07:15,953 main.py:57] epoch 12560, training loss: 7866.12, average training loss: 7209.50, base loss: 16109.30
[INFO 2017-06-29 11:07:18,951 main.py:57] epoch 12561, training loss: 6637.30, average training loss: 7207.81, base loss: 16109.11
[INFO 2017-06-29 11:07:21,990 main.py:57] epoch 12562, training loss: 6276.07, average training loss: 7206.49, base loss: 16108.86
[INFO 2017-06-29 11:07:25,026 main.py:57] epoch 12563, training loss: 6789.61, average training loss: 7205.96, base loss: 16108.94
[INFO 2017-06-29 11:07:28,067 main.py:57] epoch 12564, training loss: 8253.59, average training loss: 7206.73, base loss: 16109.30
[INFO 2017-06-29 11:07:31,106 main.py:57] epoch 12565, training loss: 7097.09, average training loss: 7206.43, base loss: 16108.98
[INFO 2017-06-29 11:07:34,192 main.py:57] epoch 12566, training loss: 6615.44, average training loss: 7206.20, base loss: 16108.99
[INFO 2017-06-29 11:07:37,254 main.py:57] epoch 12567, training loss: 7329.73, average training loss: 7205.99, base loss: 16109.26
[INFO 2017-06-29 11:07:40,244 main.py:57] epoch 12568, training loss: 6789.44, average training loss: 7203.53, base loss: 16109.43
[INFO 2017-06-29 11:07:43,267 main.py:57] epoch 12569, training loss: 7277.03, average training loss: 7203.33, base loss: 16109.52
[INFO 2017-06-29 11:07:46,285 main.py:57] epoch 12570, training loss: 7327.93, average training loss: 7203.81, base loss: 16109.31
[INFO 2017-06-29 11:07:49,317 main.py:57] epoch 12571, training loss: 6789.88, average training loss: 7202.13, base loss: 16108.97
[INFO 2017-06-29 11:07:52,375 main.py:57] epoch 12572, training loss: 6093.77, average training loss: 7201.01, base loss: 16108.77
[INFO 2017-06-29 11:07:55,439 main.py:57] epoch 12573, training loss: 7456.28, average training loss: 7200.58, base loss: 16108.97
[INFO 2017-06-29 11:07:58,535 main.py:57] epoch 12574, training loss: 7094.24, average training loss: 7199.52, base loss: 16109.02
[INFO 2017-06-29 11:08:01,595 main.py:57] epoch 12575, training loss: 7226.66, average training loss: 7199.20, base loss: 16109.14
[INFO 2017-06-29 11:08:04,654 main.py:57] epoch 12576, training loss: 6338.48, average training loss: 7197.80, base loss: 16108.84
[INFO 2017-06-29 11:08:07,716 main.py:57] epoch 12577, training loss: 7076.63, average training loss: 7197.30, base loss: 16108.86
[INFO 2017-06-29 11:08:10,766 main.py:57] epoch 12578, training loss: 6353.26, average training loss: 7196.49, base loss: 16108.59
[INFO 2017-06-29 11:08:13,890 main.py:57] epoch 12579, training loss: 6605.98, average training loss: 7195.86, base loss: 16108.33
[INFO 2017-06-29 11:08:16,965 main.py:57] epoch 12580, training loss: 6525.09, average training loss: 7194.68, base loss: 16108.19
[INFO 2017-06-29 11:08:20,014 main.py:57] epoch 12581, training loss: 6959.86, average training loss: 7193.68, base loss: 16107.92
[INFO 2017-06-29 11:08:23,049 main.py:57] epoch 12582, training loss: 7224.21, average training loss: 7192.95, base loss: 16107.83
[INFO 2017-06-29 11:08:26,167 main.py:57] epoch 12583, training loss: 7045.30, average training loss: 7192.78, base loss: 16107.57
[INFO 2017-06-29 11:08:29,172 main.py:57] epoch 12584, training loss: 7538.52, average training loss: 7193.24, base loss: 16107.35
[INFO 2017-06-29 11:08:32,258 main.py:57] epoch 12585, training loss: 7115.65, average training loss: 7193.08, base loss: 16107.02
[INFO 2017-06-29 11:08:35,308 main.py:57] epoch 12586, training loss: 8064.22, average training loss: 7193.51, base loss: 16107.30
[INFO 2017-06-29 11:08:38,397 main.py:57] epoch 12587, training loss: 6720.77, average training loss: 7192.61, base loss: 16107.08
[INFO 2017-06-29 11:08:41,504 main.py:57] epoch 12588, training loss: 6553.30, average training loss: 7192.06, base loss: 16106.80
[INFO 2017-06-29 11:08:44,586 main.py:57] epoch 12589, training loss: 7461.67, average training loss: 7192.37, base loss: 16106.83
[INFO 2017-06-29 11:08:47,705 main.py:57] epoch 12590, training loss: 8157.82, average training loss: 7193.84, base loss: 16107.03
[INFO 2017-06-29 11:08:50,691 main.py:57] epoch 12591, training loss: 6904.85, average training loss: 7194.06, base loss: 16106.87
[INFO 2017-06-29 11:08:53,748 main.py:57] epoch 12592, training loss: 6922.59, average training loss: 7192.64, base loss: 16106.72
[INFO 2017-06-29 11:08:56,861 main.py:57] epoch 12593, training loss: 7806.29, average training loss: 7193.00, base loss: 16106.83
[INFO 2017-06-29 11:08:59,889 main.py:57] epoch 12594, training loss: 7158.75, average training loss: 7192.92, base loss: 16106.85
[INFO 2017-06-29 11:09:02,959 main.py:57] epoch 12595, training loss: 7027.30, average training loss: 7192.96, base loss: 16106.57
[INFO 2017-06-29 11:09:06,020 main.py:57] epoch 12596, training loss: 7521.32, average training loss: 7193.88, base loss: 16106.59
[INFO 2017-06-29 11:09:09,068 main.py:57] epoch 12597, training loss: 7026.69, average training loss: 7194.57, base loss: 16106.47
[INFO 2017-06-29 11:09:12,151 main.py:57] epoch 12598, training loss: 7966.94, average training loss: 7194.98, base loss: 16106.63
[INFO 2017-06-29 11:09:15,235 main.py:57] epoch 12599, training loss: 8168.55, average training loss: 7195.99, base loss: 16106.86
[INFO 2017-06-29 11:09:15,235 main.py:59] epoch 12599, testing
[INFO 2017-06-29 11:09:27,981 main.py:104] average testing loss: 7836.45, base loss: 16528.17
[INFO 2017-06-29 11:09:27,981 main.py:105] improve_loss: 8691.72, improve_percent: 0.53
[INFO 2017-06-29 11:09:27,982 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 11:09:31,097 main.py:57] epoch 12600, training loss: 7604.88, average training loss: 7195.94, base loss: 16106.93
[INFO 2017-06-29 11:09:34,170 main.py:57] epoch 12601, training loss: 6974.45, average training loss: 7194.65, base loss: 16106.93
[INFO 2017-06-29 11:09:37,265 main.py:57] epoch 12602, training loss: 6337.90, average training loss: 7193.09, base loss: 16106.58
[INFO 2017-06-29 11:09:40,300 main.py:57] epoch 12603, training loss: 7023.34, average training loss: 7193.59, base loss: 16106.37
[INFO 2017-06-29 11:09:43,415 main.py:57] epoch 12604, training loss: 6286.13, average training loss: 7193.19, base loss: 16106.19
[INFO 2017-06-29 11:09:46,460 main.py:57] epoch 12605, training loss: 8124.41, average training loss: 7194.53, base loss: 16106.44
[INFO 2017-06-29 11:09:49,485 main.py:57] epoch 12606, training loss: 6947.36, average training loss: 7193.84, base loss: 16106.57
[INFO 2017-06-29 11:09:52,533 main.py:57] epoch 12607, training loss: 7089.01, average training loss: 7194.19, base loss: 16106.63
[INFO 2017-06-29 11:09:55,676 main.py:57] epoch 12608, training loss: 7037.21, average training loss: 7193.50, base loss: 16106.65
[INFO 2017-06-29 11:09:58,663 main.py:57] epoch 12609, training loss: 6675.09, average training loss: 7193.39, base loss: 16106.49
[INFO 2017-06-29 11:10:01,700 main.py:57] epoch 12610, training loss: 7025.78, average training loss: 7193.18, base loss: 16106.42
[INFO 2017-06-29 11:10:04,781 main.py:57] epoch 12611, training loss: 5957.31, average training loss: 7192.33, base loss: 16105.92
[INFO 2017-06-29 11:10:07,900 main.py:57] epoch 12612, training loss: 7311.25, average training loss: 7190.91, base loss: 16106.02
[INFO 2017-06-29 11:10:10,965 main.py:57] epoch 12613, training loss: 8322.84, average training loss: 7192.53, base loss: 16106.26
[INFO 2017-06-29 11:10:14,049 main.py:57] epoch 12614, training loss: 7544.46, average training loss: 7192.36, base loss: 16106.74
[INFO 2017-06-29 11:10:17,127 main.py:57] epoch 12615, training loss: 7446.26, average training loss: 7192.29, base loss: 16106.88
[INFO 2017-06-29 11:10:20,199 main.py:57] epoch 12616, training loss: 7103.29, average training loss: 7192.21, base loss: 16106.94
[INFO 2017-06-29 11:10:23,312 main.py:57] epoch 12617, training loss: 7286.37, average training loss: 7192.78, base loss: 16107.27
[INFO 2017-06-29 11:10:26,401 main.py:57] epoch 12618, training loss: 7362.84, average training loss: 7192.74, base loss: 16107.31
[INFO 2017-06-29 11:10:29,420 main.py:57] epoch 12619, training loss: 7336.99, average training loss: 7193.33, base loss: 16107.47
[INFO 2017-06-29 11:10:32,518 main.py:57] epoch 12620, training loss: 6737.31, average training loss: 7193.16, base loss: 16107.27
[INFO 2017-06-29 11:10:35,583 main.py:57] epoch 12621, training loss: 7768.44, average training loss: 7192.78, base loss: 16107.32
[INFO 2017-06-29 11:10:38,664 main.py:57] epoch 12622, training loss: 7851.97, average training loss: 7192.96, base loss: 16107.86
[INFO 2017-06-29 11:10:41,700 main.py:57] epoch 12623, training loss: 6386.27, average training loss: 7192.22, base loss: 16107.74
[INFO 2017-06-29 11:10:44,704 main.py:57] epoch 12624, training loss: 6419.01, average training loss: 7192.40, base loss: 16107.64
[INFO 2017-06-29 11:10:47,763 main.py:57] epoch 12625, training loss: 6855.33, average training loss: 7191.73, base loss: 16107.61
[INFO 2017-06-29 11:10:50,818 main.py:57] epoch 12626, training loss: 6941.93, average training loss: 7191.82, base loss: 16107.38
[INFO 2017-06-29 11:10:53,850 main.py:57] epoch 12627, training loss: 7076.56, average training loss: 7191.54, base loss: 16107.22
[INFO 2017-06-29 11:10:56,879 main.py:57] epoch 12628, training loss: 7213.09, average training loss: 7191.52, base loss: 16107.53
[INFO 2017-06-29 11:10:59,880 main.py:57] epoch 12629, training loss: 7730.09, average training loss: 7191.65, base loss: 16107.71
[INFO 2017-06-29 11:11:02,949 main.py:57] epoch 12630, training loss: 6535.30, average training loss: 7191.10, base loss: 16107.40
[INFO 2017-06-29 11:11:05,980 main.py:57] epoch 12631, training loss: 7039.99, average training loss: 7191.21, base loss: 16107.27
[INFO 2017-06-29 11:11:09,014 main.py:57] epoch 12632, training loss: 7000.91, average training loss: 7191.53, base loss: 16107.04
[INFO 2017-06-29 11:11:12,026 main.py:57] epoch 12633, training loss: 6949.88, average training loss: 7191.16, base loss: 16106.73
[INFO 2017-06-29 11:11:15,094 main.py:57] epoch 12634, training loss: 6721.88, average training loss: 7190.72, base loss: 16106.73
[INFO 2017-06-29 11:11:18,116 main.py:57] epoch 12635, training loss: 6640.30, average training loss: 7189.41, base loss: 16106.49
[INFO 2017-06-29 11:11:21,154 main.py:57] epoch 12636, training loss: 6993.85, average training loss: 7189.18, base loss: 16106.43
[INFO 2017-06-29 11:11:24,277 main.py:57] epoch 12637, training loss: 6431.35, average training loss: 7188.17, base loss: 16106.30
[INFO 2017-06-29 11:11:27,357 main.py:57] epoch 12638, training loss: 6607.58, average training loss: 7187.85, base loss: 16105.93
[INFO 2017-06-29 11:11:30,472 main.py:57] epoch 12639, training loss: 7757.83, average training loss: 7188.89, base loss: 16106.02
[INFO 2017-06-29 11:11:33,542 main.py:57] epoch 12640, training loss: 7881.95, average training loss: 7188.61, base loss: 16106.03
[INFO 2017-06-29 11:11:36,680 main.py:57] epoch 12641, training loss: 6587.71, average training loss: 7187.86, base loss: 16105.85
[INFO 2017-06-29 11:11:39,786 main.py:57] epoch 12642, training loss: 7228.54, average training loss: 7188.82, base loss: 16105.70
[INFO 2017-06-29 11:11:42,826 main.py:57] epoch 12643, training loss: 6748.14, average training loss: 7187.29, base loss: 16105.82
[INFO 2017-06-29 11:11:45,900 main.py:57] epoch 12644, training loss: 8097.72, average training loss: 7187.48, base loss: 16105.89
[INFO 2017-06-29 11:11:48,982 main.py:57] epoch 12645, training loss: 6223.51, average training loss: 7186.01, base loss: 16105.41
[INFO 2017-06-29 11:11:52,077 main.py:57] epoch 12646, training loss: 6542.55, average training loss: 7185.44, base loss: 16105.23
[INFO 2017-06-29 11:11:55,073 main.py:57] epoch 12647, training loss: 7645.63, average training loss: 7185.94, base loss: 16105.57
[INFO 2017-06-29 11:11:58,167 main.py:57] epoch 12648, training loss: 7410.64, average training loss: 7186.12, base loss: 16105.73
[INFO 2017-06-29 11:12:01,162 main.py:57] epoch 12649, training loss: 7956.34, average training loss: 7186.79, base loss: 16105.93
[INFO 2017-06-29 11:12:04,217 main.py:57] epoch 12650, training loss: 7453.40, average training loss: 7187.80, base loss: 16106.26
[INFO 2017-06-29 11:12:07,312 main.py:57] epoch 12651, training loss: 7546.90, average training loss: 7187.41, base loss: 16106.45
[INFO 2017-06-29 11:12:10,406 main.py:57] epoch 12652, training loss: 7010.86, average training loss: 7187.72, base loss: 16106.29
[INFO 2017-06-29 11:12:13,403 main.py:57] epoch 12653, training loss: 6884.00, average training loss: 7188.29, base loss: 16106.07
[INFO 2017-06-29 11:12:16,464 main.py:57] epoch 12654, training loss: 6558.68, average training loss: 7187.30, base loss: 16106.02
[INFO 2017-06-29 11:12:19,470 main.py:57] epoch 12655, training loss: 7199.06, average training loss: 7187.03, base loss: 16106.16
[INFO 2017-06-29 11:12:22,444 main.py:57] epoch 12656, training loss: 6879.05, average training loss: 7186.96, base loss: 16106.02
[INFO 2017-06-29 11:12:25,585 main.py:57] epoch 12657, training loss: 7598.21, average training loss: 7187.07, base loss: 16106.31
[INFO 2017-06-29 11:12:28,565 main.py:57] epoch 12658, training loss: 6256.17, average training loss: 7186.57, base loss: 16106.09
[INFO 2017-06-29 11:12:31,649 main.py:57] epoch 12659, training loss: 7782.02, average training loss: 7187.42, base loss: 16106.32
[INFO 2017-06-29 11:12:34,643 main.py:57] epoch 12660, training loss: 6304.87, average training loss: 7186.72, base loss: 16106.05
[INFO 2017-06-29 11:12:37,723 main.py:57] epoch 12661, training loss: 7743.67, average training loss: 7187.40, base loss: 16106.31
[INFO 2017-06-29 11:12:40,790 main.py:57] epoch 12662, training loss: 8033.10, average training loss: 7188.00, base loss: 16106.69
[INFO 2017-06-29 11:12:43,791 main.py:57] epoch 12663, training loss: 7796.50, average training loss: 7188.89, base loss: 16106.80
[INFO 2017-06-29 11:12:46,877 main.py:57] epoch 12664, training loss: 7434.56, average training loss: 7189.16, base loss: 16106.83
[INFO 2017-06-29 11:12:49,974 main.py:57] epoch 12665, training loss: 7394.13, average training loss: 7189.77, base loss: 16106.82
[INFO 2017-06-29 11:12:52,991 main.py:57] epoch 12666, training loss: 7157.92, average training loss: 7190.12, base loss: 16106.70
[INFO 2017-06-29 11:12:56,074 main.py:57] epoch 12667, training loss: 6919.24, average training loss: 7190.45, base loss: 16106.42
[INFO 2017-06-29 11:12:59,117 main.py:57] epoch 12668, training loss: 7098.20, average training loss: 7190.35, base loss: 16106.26
[INFO 2017-06-29 11:13:02,202 main.py:57] epoch 12669, training loss: 7694.30, average training loss: 7191.29, base loss: 16106.45
[INFO 2017-06-29 11:13:05,273 main.py:57] epoch 12670, training loss: 8753.06, average training loss: 7193.50, base loss: 16106.73
[INFO 2017-06-29 11:13:08,337 main.py:57] epoch 12671, training loss: 7654.83, average training loss: 7194.02, base loss: 16106.73
[INFO 2017-06-29 11:13:11,395 main.py:57] epoch 12672, training loss: 6758.86, average training loss: 7193.27, base loss: 16106.49
[INFO 2017-06-29 11:13:14,416 main.py:57] epoch 12673, training loss: 6830.76, average training loss: 7193.58, base loss: 16106.31
[INFO 2017-06-29 11:13:17,435 main.py:57] epoch 12674, training loss: 7826.67, average training loss: 7194.54, base loss: 16106.38
[INFO 2017-06-29 11:13:20,435 main.py:57] epoch 12675, training loss: 6550.82, average training loss: 7193.58, base loss: 16106.22
[INFO 2017-06-29 11:13:23,543 main.py:57] epoch 12676, training loss: 8819.13, average training loss: 7193.60, base loss: 16106.67
[INFO 2017-06-29 11:13:26,535 main.py:57] epoch 12677, training loss: 6886.93, average training loss: 7193.61, base loss: 16106.61
[INFO 2017-06-29 11:13:29,653 main.py:57] epoch 12678, training loss: 6422.38, average training loss: 7193.27, base loss: 16106.24
[INFO 2017-06-29 11:13:32,658 main.py:57] epoch 12679, training loss: 6578.85, average training loss: 7191.90, base loss: 16106.09
[INFO 2017-06-29 11:13:35,679 main.py:57] epoch 12680, training loss: 7423.16, average training loss: 7191.81, base loss: 16105.98
[INFO 2017-06-29 11:13:38,828 main.py:57] epoch 12681, training loss: 7239.37, average training loss: 7192.02, base loss: 16106.30
[INFO 2017-06-29 11:13:41,941 main.py:57] epoch 12682, training loss: 8967.23, average training loss: 7193.83, base loss: 16107.10
[INFO 2017-06-29 11:13:44,981 main.py:57] epoch 12683, training loss: 7797.52, average training loss: 7193.49, base loss: 16107.47
[INFO 2017-06-29 11:13:48,057 main.py:57] epoch 12684, training loss: 6822.31, average training loss: 7194.00, base loss: 16107.41
[INFO 2017-06-29 11:13:51,092 main.py:57] epoch 12685, training loss: 7022.58, average training loss: 7194.90, base loss: 16107.25
[INFO 2017-06-29 11:13:54,107 main.py:57] epoch 12686, training loss: 8246.21, average training loss: 7195.60, base loss: 16107.75
[INFO 2017-06-29 11:13:57,133 main.py:57] epoch 12687, training loss: 8049.63, average training loss: 7195.51, base loss: 16107.92
[INFO 2017-06-29 11:14:00,156 main.py:57] epoch 12688, training loss: 7921.37, average training loss: 7196.28, base loss: 16108.21
[INFO 2017-06-29 11:14:03,229 main.py:57] epoch 12689, training loss: 7925.07, average training loss: 7196.94, base loss: 16108.49
[INFO 2017-06-29 11:14:06,247 main.py:57] epoch 12690, training loss: 8007.31, average training loss: 7197.95, base loss: 16108.83
[INFO 2017-06-29 11:14:09,269 main.py:57] epoch 12691, training loss: 6800.34, average training loss: 7197.85, base loss: 16108.84
[INFO 2017-06-29 11:14:12,361 main.py:57] epoch 12692, training loss: 6844.62, average training loss: 7197.93, base loss: 16108.81
[INFO 2017-06-29 11:14:15,484 main.py:57] epoch 12693, training loss: 7358.79, average training loss: 7199.34, base loss: 16108.81
[INFO 2017-06-29 11:14:18,538 main.py:57] epoch 12694, training loss: 7517.02, average training loss: 7198.35, base loss: 16109.17
[INFO 2017-06-29 11:14:21,559 main.py:57] epoch 12695, training loss: 6832.80, average training loss: 7198.46, base loss: 16108.92
[INFO 2017-06-29 11:14:24,722 main.py:57] epoch 12696, training loss: 7290.83, average training loss: 7198.19, base loss: 16108.98
[INFO 2017-06-29 11:14:27,792 main.py:57] epoch 12697, training loss: 8166.77, average training loss: 7198.78, base loss: 16109.17
[INFO 2017-06-29 11:14:30,845 main.py:57] epoch 12698, training loss: 7538.61, average training loss: 7199.13, base loss: 16109.31
[INFO 2017-06-29 11:14:33,916 main.py:57] epoch 12699, training loss: 6601.33, average training loss: 7198.23, base loss: 16108.88
[INFO 2017-06-29 11:14:33,916 main.py:59] epoch 12699, testing
[INFO 2017-06-29 11:14:46,728 main.py:104] average testing loss: 8002.07, base loss: 16668.72
[INFO 2017-06-29 11:14:46,728 main.py:105] improve_loss: 8666.65, improve_percent: 0.52
[INFO 2017-06-29 11:14:46,729 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 11:14:49,719 main.py:57] epoch 12700, training loss: 8007.04, average training loss: 7198.43, base loss: 16109.04
[INFO 2017-06-29 11:14:52,720 main.py:57] epoch 12701, training loss: 7201.93, average training loss: 7198.45, base loss: 16108.84
[INFO 2017-06-29 11:14:55,705 main.py:57] epoch 12702, training loss: 6690.39, average training loss: 7196.78, base loss: 16108.48
[INFO 2017-06-29 11:14:58,695 main.py:57] epoch 12703, training loss: 7665.83, average training loss: 7197.79, base loss: 16108.81
[INFO 2017-06-29 11:15:01,733 main.py:57] epoch 12704, training loss: 7411.38, average training loss: 7198.59, base loss: 16108.91
[INFO 2017-06-29 11:15:04,881 main.py:57] epoch 12705, training loss: 7561.13, average training loss: 7198.16, base loss: 16109.06
[INFO 2017-06-29 11:15:07,955 main.py:57] epoch 12706, training loss: 7936.65, average training loss: 7197.23, base loss: 16109.25
[INFO 2017-06-29 11:15:10,992 main.py:57] epoch 12707, training loss: 7041.98, average training loss: 7197.28, base loss: 16109.31
[INFO 2017-06-29 11:15:14,037 main.py:57] epoch 12708, training loss: 7188.74, average training loss: 7197.86, base loss: 16109.22
[INFO 2017-06-29 11:15:17,106 main.py:57] epoch 12709, training loss: 7692.01, average training loss: 7198.26, base loss: 16109.24
[INFO 2017-06-29 11:15:20,120 main.py:57] epoch 12710, training loss: 7866.98, average training loss: 7199.61, base loss: 16109.63
[INFO 2017-06-29 11:15:23,102 main.py:57] epoch 12711, training loss: 6750.73, average training loss: 7198.72, base loss: 16109.57
[INFO 2017-06-29 11:15:26,144 main.py:57] epoch 12712, training loss: 7329.49, average training loss: 7199.37, base loss: 16109.73
[INFO 2017-06-29 11:15:29,234 main.py:57] epoch 12713, training loss: 7727.02, average training loss: 7200.18, base loss: 16109.98
[INFO 2017-06-29 11:15:32,313 main.py:57] epoch 12714, training loss: 6962.26, average training loss: 7199.26, base loss: 16110.07
[INFO 2017-06-29 11:15:35,394 main.py:57] epoch 12715, training loss: 6912.35, average training loss: 7199.09, base loss: 16110.00
[INFO 2017-06-29 11:15:38,459 main.py:57] epoch 12716, training loss: 6897.98, average training loss: 7197.91, base loss: 16109.75
[INFO 2017-06-29 11:15:41,606 main.py:57] epoch 12717, training loss: 6262.89, average training loss: 7196.85, base loss: 16109.54
[INFO 2017-06-29 11:15:44,689 main.py:57] epoch 12718, training loss: 6843.01, average training loss: 7196.19, base loss: 16109.45
[INFO 2017-06-29 11:15:47,751 main.py:57] epoch 12719, training loss: 7375.09, average training loss: 7196.74, base loss: 16109.62
[INFO 2017-06-29 11:15:50,799 main.py:57] epoch 12720, training loss: 6638.73, average training loss: 7196.40, base loss: 16109.48
[INFO 2017-06-29 11:15:53,877 main.py:57] epoch 12721, training loss: 6896.53, average training loss: 7196.50, base loss: 16109.53
[INFO 2017-06-29 11:15:56,920 main.py:57] epoch 12722, training loss: 7194.69, average training loss: 7196.92, base loss: 16109.56
[INFO 2017-06-29 11:16:00,037 main.py:57] epoch 12723, training loss: 7431.61, average training loss: 7198.04, base loss: 16109.64
[INFO 2017-06-29 11:16:03,134 main.py:57] epoch 12724, training loss: 7109.80, average training loss: 7197.39, base loss: 16109.52
[INFO 2017-06-29 11:16:06,268 main.py:57] epoch 12725, training loss: 6913.98, average training loss: 7198.19, base loss: 16109.65
[INFO 2017-06-29 11:16:09,248 main.py:57] epoch 12726, training loss: 7650.11, average training loss: 7197.93, base loss: 16110.05
[INFO 2017-06-29 11:16:12,348 main.py:57] epoch 12727, training loss: 7349.91, average training loss: 7198.45, base loss: 16110.08
[INFO 2017-06-29 11:16:15,327 main.py:57] epoch 12728, training loss: 7592.61, average training loss: 7199.12, base loss: 16110.05
[INFO 2017-06-29 11:16:18,374 main.py:57] epoch 12729, training loss: 6776.14, average training loss: 7198.22, base loss: 16109.87
[INFO 2017-06-29 11:16:21,509 main.py:57] epoch 12730, training loss: 6534.08, average training loss: 7197.96, base loss: 16109.59
[INFO 2017-06-29 11:16:24,568 main.py:57] epoch 12731, training loss: 6920.16, average training loss: 7197.53, base loss: 16109.69
[INFO 2017-06-29 11:16:27,589 main.py:57] epoch 12732, training loss: 7227.32, average training loss: 7197.71, base loss: 16109.65
[INFO 2017-06-29 11:16:30,658 main.py:57] epoch 12733, training loss: 7254.50, average training loss: 7197.78, base loss: 16109.80
[INFO 2017-06-29 11:16:33,662 main.py:57] epoch 12734, training loss: 6725.17, average training loss: 7197.37, base loss: 16109.52
[INFO 2017-06-29 11:16:36,657 main.py:57] epoch 12735, training loss: 7510.78, average training loss: 7197.88, base loss: 16109.48
[INFO 2017-06-29 11:16:39,808 main.py:57] epoch 12736, training loss: 7511.94, average training loss: 7198.24, base loss: 16109.59
[INFO 2017-06-29 11:16:42,834 main.py:57] epoch 12737, training loss: 6384.43, average training loss: 7196.31, base loss: 16109.36
[INFO 2017-06-29 11:16:45,847 main.py:57] epoch 12738, training loss: 6466.55, average training loss: 7195.12, base loss: 16109.26
[INFO 2017-06-29 11:16:48,898 main.py:57] epoch 12739, training loss: 6409.91, average training loss: 7194.32, base loss: 16109.02
[INFO 2017-06-29 11:16:51,993 main.py:57] epoch 12740, training loss: 7452.51, average training loss: 7195.49, base loss: 16109.14
[INFO 2017-06-29 11:16:54,982 main.py:57] epoch 12741, training loss: 6234.10, average training loss: 7194.63, base loss: 16108.79
[INFO 2017-06-29 11:16:58,069 main.py:57] epoch 12742, training loss: 6818.07, average training loss: 7193.81, base loss: 16108.62
[INFO 2017-06-29 11:17:01,150 main.py:57] epoch 12743, training loss: 7721.32, average training loss: 7194.13, base loss: 16108.85
[INFO 2017-06-29 11:17:04,163 main.py:57] epoch 12744, training loss: 7062.44, average training loss: 7194.05, base loss: 16108.81
[INFO 2017-06-29 11:17:07,170 main.py:57] epoch 12745, training loss: 7667.91, average training loss: 7195.31, base loss: 16109.05
[INFO 2017-06-29 11:17:10,217 main.py:57] epoch 12746, training loss: 6941.91, average training loss: 7194.53, base loss: 16108.90
[INFO 2017-06-29 11:17:13,312 main.py:57] epoch 12747, training loss: 7242.13, average training loss: 7194.51, base loss: 16109.02
[INFO 2017-06-29 11:17:16,381 main.py:57] epoch 12748, training loss: 7938.47, average training loss: 7195.25, base loss: 16109.09
[INFO 2017-06-29 11:17:19,391 main.py:57] epoch 12749, training loss: 7144.45, average training loss: 7195.24, base loss: 16109.17
[INFO 2017-06-29 11:17:22,355 main.py:57] epoch 12750, training loss: 6606.72, average training loss: 7194.68, base loss: 16108.79
[INFO 2017-06-29 11:17:25,347 main.py:57] epoch 12751, training loss: 6913.37, average training loss: 7194.97, base loss: 16108.86
[INFO 2017-06-29 11:17:28,328 main.py:57] epoch 12752, training loss: 7318.29, average training loss: 7195.73, base loss: 16108.55
[INFO 2017-06-29 11:17:31,377 main.py:57] epoch 12753, training loss: 7901.78, average training loss: 7197.12, base loss: 16108.77
[INFO 2017-06-29 11:17:34,407 main.py:57] epoch 12754, training loss: 6689.53, average training loss: 7195.83, base loss: 16108.48
[INFO 2017-06-29 11:17:37,457 main.py:57] epoch 12755, training loss: 7608.49, average training loss: 7196.24, base loss: 16108.36
[INFO 2017-06-29 11:17:40,518 main.py:57] epoch 12756, training loss: 6929.19, average training loss: 7195.71, base loss: 16108.16
[INFO 2017-06-29 11:17:43,622 main.py:57] epoch 12757, training loss: 6688.44, average training loss: 7195.00, base loss: 16108.01
[INFO 2017-06-29 11:17:46,656 main.py:57] epoch 12758, training loss: 6821.35, average training loss: 7194.23, base loss: 16107.94
[INFO 2017-06-29 11:17:49,779 main.py:57] epoch 12759, training loss: 7482.64, average training loss: 7195.10, base loss: 16108.41
[INFO 2017-06-29 11:17:52,768 main.py:57] epoch 12760, training loss: 6218.45, average training loss: 7194.87, base loss: 16107.97
[INFO 2017-06-29 11:17:55,869 main.py:57] epoch 12761, training loss: 7419.37, average training loss: 7194.54, base loss: 16108.08
[INFO 2017-06-29 11:17:58,879 main.py:57] epoch 12762, training loss: 7234.49, average training loss: 7195.53, base loss: 16108.06
[INFO 2017-06-29 11:18:01,916 main.py:57] epoch 12763, training loss: 7453.74, average training loss: 7195.93, base loss: 16108.46
[INFO 2017-06-29 11:18:04,938 main.py:57] epoch 12764, training loss: 7325.57, average training loss: 7195.73, base loss: 16108.61
[INFO 2017-06-29 11:18:07,918 main.py:57] epoch 12765, training loss: 6554.43, average training loss: 7195.98, base loss: 16108.31
[INFO 2017-06-29 11:18:10,953 main.py:57] epoch 12766, training loss: 7218.86, average training loss: 7196.53, base loss: 16108.16
[INFO 2017-06-29 11:18:14,013 main.py:57] epoch 12767, training loss: 7608.53, average training loss: 7196.77, base loss: 16108.28
[INFO 2017-06-29 11:18:17,063 main.py:57] epoch 12768, training loss: 6784.26, average training loss: 7196.70, base loss: 16108.15
[INFO 2017-06-29 11:18:20,132 main.py:57] epoch 12769, training loss: 7199.09, average training loss: 7195.54, base loss: 16107.92
[INFO 2017-06-29 11:18:23,191 main.py:57] epoch 12770, training loss: 6450.29, average training loss: 7195.03, base loss: 16107.67
[INFO 2017-06-29 11:18:26,260 main.py:57] epoch 12771, training loss: 6664.68, average training loss: 7193.38, base loss: 16107.63
[INFO 2017-06-29 11:18:29,306 main.py:57] epoch 12772, training loss: 7071.62, average training loss: 7193.05, base loss: 16107.49
[INFO 2017-06-29 11:18:32,301 main.py:57] epoch 12773, training loss: 6978.78, average training loss: 7192.90, base loss: 16107.45
[INFO 2017-06-29 11:18:35,372 main.py:57] epoch 12774, training loss: 7068.01, average training loss: 7193.09, base loss: 16107.61
[INFO 2017-06-29 11:18:38,419 main.py:57] epoch 12775, training loss: 7884.65, average training loss: 7194.54, base loss: 16107.81
[INFO 2017-06-29 11:18:41,559 main.py:57] epoch 12776, training loss: 7400.72, average training loss: 7195.40, base loss: 16107.72
[INFO 2017-06-29 11:18:44,653 main.py:57] epoch 12777, training loss: 7309.14, average training loss: 7195.92, base loss: 16107.98
[INFO 2017-06-29 11:18:47,667 main.py:57] epoch 12778, training loss: 8115.15, average training loss: 7197.61, base loss: 16108.18
[INFO 2017-06-29 11:18:50,726 main.py:57] epoch 12779, training loss: 8076.51, average training loss: 7198.16, base loss: 16108.42
[INFO 2017-06-29 11:18:53,833 main.py:57] epoch 12780, training loss: 6992.38, average training loss: 7198.55, base loss: 16108.41
[INFO 2017-06-29 11:18:56,869 main.py:57] epoch 12781, training loss: 7404.69, average training loss: 7199.80, base loss: 16108.39
[INFO 2017-06-29 11:18:59,921 main.py:57] epoch 12782, training loss: 8993.06, average training loss: 7201.60, base loss: 16108.91
[INFO 2017-06-29 11:19:02,966 main.py:57] epoch 12783, training loss: 6279.88, average training loss: 7199.65, base loss: 16108.68
[INFO 2017-06-29 11:19:06,057 main.py:57] epoch 12784, training loss: 7265.05, average training loss: 7199.49, base loss: 16108.40
[INFO 2017-06-29 11:19:09,115 main.py:57] epoch 12785, training loss: 6754.64, average training loss: 7199.80, base loss: 16108.10
[INFO 2017-06-29 11:19:12,158 main.py:57] epoch 12786, training loss: 6717.40, average training loss: 7199.10, base loss: 16108.17
[INFO 2017-06-29 11:19:15,257 main.py:57] epoch 12787, training loss: 7499.38, average training loss: 7199.30, base loss: 16108.51
[INFO 2017-06-29 11:19:18,258 main.py:57] epoch 12788, training loss: 7230.94, average training loss: 7197.80, base loss: 16108.19
[INFO 2017-06-29 11:19:21,354 main.py:57] epoch 12789, training loss: 7304.52, average training loss: 7198.40, base loss: 16108.13
[INFO 2017-06-29 11:19:24,375 main.py:57] epoch 12790, training loss: 6876.53, average training loss: 7197.93, base loss: 16107.92
[INFO 2017-06-29 11:19:27,480 main.py:57] epoch 12791, training loss: 7224.65, average training loss: 7198.65, base loss: 16107.72
[INFO 2017-06-29 11:19:30,553 main.py:57] epoch 12792, training loss: 6203.98, average training loss: 7197.37, base loss: 16107.39
[INFO 2017-06-29 11:19:33,621 main.py:57] epoch 12793, training loss: 6793.32, average training loss: 7197.06, base loss: 16107.42
[INFO 2017-06-29 11:19:36,652 main.py:57] epoch 12794, training loss: 7608.70, average training loss: 7197.86, base loss: 16107.40
[INFO 2017-06-29 11:19:39,803 main.py:57] epoch 12795, training loss: 6317.71, average training loss: 7197.12, base loss: 16106.95
[INFO 2017-06-29 11:19:42,827 main.py:57] epoch 12796, training loss: 7352.89, average training loss: 7197.50, base loss: 16107.36
[INFO 2017-06-29 11:19:45,918 main.py:57] epoch 12797, training loss: 7178.97, average training loss: 7198.46, base loss: 16107.44
[INFO 2017-06-29 11:19:48,976 main.py:57] epoch 12798, training loss: 7368.15, average training loss: 7198.99, base loss: 16107.53
[INFO 2017-06-29 11:19:52,113 main.py:57] epoch 12799, training loss: 6776.48, average training loss: 7198.40, base loss: 16107.34
[INFO 2017-06-29 11:19:52,114 main.py:59] epoch 12799, testing
[INFO 2017-06-29 11:20:05,007 main.py:104] average testing loss: 7570.28, base loss: 15955.46
[INFO 2017-06-29 11:20:05,007 main.py:105] improve_loss: 8385.18, improve_percent: 0.53
[INFO 2017-06-29 11:20:05,009 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 11:20:08,143 main.py:57] epoch 12800, training loss: 7087.72, average training loss: 7198.69, base loss: 16107.38
[INFO 2017-06-29 11:20:11,169 main.py:57] epoch 12801, training loss: 6755.42, average training loss: 7197.20, base loss: 16107.26
[INFO 2017-06-29 11:20:14,242 main.py:57] epoch 12802, training loss: 7269.25, average training loss: 7197.45, base loss: 16107.10
[INFO 2017-06-29 11:20:17,248 main.py:57] epoch 12803, training loss: 7551.39, average training loss: 7198.73, base loss: 16107.35
[INFO 2017-06-29 11:20:20,354 main.py:57] epoch 12804, training loss: 6906.36, average training loss: 7199.33, base loss: 16107.11
[INFO 2017-06-29 11:20:23,438 main.py:57] epoch 12805, training loss: 6419.73, average training loss: 7199.03, base loss: 16106.98
[INFO 2017-06-29 11:20:26,500 main.py:57] epoch 12806, training loss: 7575.69, average training loss: 7200.14, base loss: 16107.00
[INFO 2017-06-29 11:20:29,602 main.py:57] epoch 12807, training loss: 6154.30, average training loss: 7198.19, base loss: 16106.72
[INFO 2017-06-29 11:20:32,700 main.py:57] epoch 12808, training loss: 7162.98, average training loss: 7197.68, base loss: 16106.61
[INFO 2017-06-29 11:20:35,720 main.py:57] epoch 12809, training loss: 6961.04, average training loss: 7197.35, base loss: 16106.70
[INFO 2017-06-29 11:20:38,858 main.py:57] epoch 12810, training loss: 7370.29, average training loss: 7197.42, base loss: 16106.87
[INFO 2017-06-29 11:20:41,954 main.py:57] epoch 12811, training loss: 7648.49, average training loss: 7197.45, base loss: 16106.99
[INFO 2017-06-29 11:20:45,123 main.py:57] epoch 12812, training loss: 8147.69, average training loss: 7198.43, base loss: 16107.35
[INFO 2017-06-29 11:20:48,145 main.py:57] epoch 12813, training loss: 7911.24, average training loss: 7199.13, base loss: 16107.55
[INFO 2017-06-29 11:20:51,235 main.py:57] epoch 12814, training loss: 6341.55, average training loss: 7198.17, base loss: 16107.81
[INFO 2017-06-29 11:20:54,232 main.py:57] epoch 12815, training loss: 7629.97, average training loss: 7198.53, base loss: 16107.96
[INFO 2017-06-29 11:20:57,328 main.py:57] epoch 12816, training loss: 6406.10, average training loss: 7198.06, base loss: 16107.42
[INFO 2017-06-29 11:21:00,357 main.py:57] epoch 12817, training loss: 7272.97, average training loss: 7197.78, base loss: 16107.26
[INFO 2017-06-29 11:21:03,483 main.py:57] epoch 12818, training loss: 6736.92, average training loss: 7197.12, base loss: 16107.06
[INFO 2017-06-29 11:21:06,591 main.py:57] epoch 12819, training loss: 8027.05, average training loss: 7196.60, base loss: 16107.20
[INFO 2017-06-29 11:21:09,823 main.py:57] epoch 12820, training loss: 6475.74, average training loss: 7195.68, base loss: 16107.11
