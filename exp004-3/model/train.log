[INFO 2017-06-29 17:07:30,051 main.py:175] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-29 17:07:35,183 main.py:57] epoch 0, training loss: 41729.07, average training loss: 41729.07, base loss: 15516.35
[INFO 2017-06-29 17:07:37,642 main.py:57] epoch 1, training loss: 35006.80, average training loss: 38367.94, base loss: 15811.44
[INFO 2017-06-29 17:07:40,047 main.py:57] epoch 2, training loss: 33395.05, average training loss: 36710.31, base loss: 15619.50
[INFO 2017-06-29 17:07:42,729 main.py:57] epoch 3, training loss: 28038.84, average training loss: 34542.44, base loss: 15849.76
[INFO 2017-06-29 17:07:45,066 main.py:57] epoch 4, training loss: 26905.17, average training loss: 33014.99, base loss: 16419.72
[INFO 2017-06-29 17:07:47,504 main.py:57] epoch 5, training loss: 23473.37, average training loss: 31424.72, base loss: 15951.12
[INFO 2017-06-29 17:07:49,861 main.py:57] epoch 6, training loss: 22225.87, average training loss: 30110.60, base loss: 15708.53
[INFO 2017-06-29 17:07:52,459 main.py:57] epoch 7, training loss: 21245.24, average training loss: 29002.43, base loss: 15610.51
[INFO 2017-06-29 17:07:54,899 main.py:57] epoch 8, training loss: 20892.88, average training loss: 28101.37, base loss: 15736.45
[INFO 2017-06-29 17:07:57,332 main.py:57] epoch 9, training loss: 18152.20, average training loss: 27106.45, base loss: 15616.49
[INFO 2017-06-29 17:07:59,801 main.py:57] epoch 10, training loss: 17145.18, average training loss: 26200.88, base loss: 15524.88
[INFO 2017-06-29 17:08:02,158 main.py:57] epoch 11, training loss: 16150.39, average training loss: 25363.34, base loss: 15550.13
[INFO 2017-06-29 17:08:04,723 main.py:57] epoch 12, training loss: 14358.63, average training loss: 24516.82, base loss: 15389.55
[INFO 2017-06-29 17:08:07,224 main.py:57] epoch 13, training loss: 14391.41, average training loss: 23793.58, base loss: 15330.40
[INFO 2017-06-29 17:08:09,656 main.py:57] epoch 14, training loss: 13710.03, average training loss: 23121.34, base loss: 15262.78
[INFO 2017-06-29 17:08:12,217 main.py:57] epoch 15, training loss: 13244.91, average training loss: 22504.07, base loss: 15219.00
[INFO 2017-06-29 17:08:15,051 main.py:57] epoch 16, training loss: 12903.02, average training loss: 21939.30, base loss: 15187.11
[INFO 2017-06-29 17:08:18,097 main.py:57] epoch 17, training loss: 13327.38, average training loss: 21460.86, base loss: 15206.29
[INFO 2017-06-29 17:08:21,215 main.py:57] epoch 18, training loss: 12939.19, average training loss: 21012.35, base loss: 15255.67
[INFO 2017-06-29 17:08:24,264 main.py:57] epoch 19, training loss: 11648.21, average training loss: 20544.14, base loss: 15209.69
[INFO 2017-06-29 17:08:27,349 main.py:57] epoch 20, training loss: 11855.74, average training loss: 20130.41, base loss: 15176.79
[INFO 2017-06-29 17:08:30,472 main.py:57] epoch 21, training loss: 12211.29, average training loss: 19770.45, base loss: 15215.81
[INFO 2017-06-29 17:08:33,588 main.py:57] epoch 22, training loss: 12894.58, average training loss: 19471.50, base loss: 15285.89
[INFO 2017-06-29 17:08:36,625 main.py:57] epoch 23, training loss: 11024.89, average training loss: 19119.56, base loss: 15276.19
[INFO 2017-06-29 17:08:39,665 main.py:57] epoch 24, training loss: 10529.04, average training loss: 18775.93, base loss: 15247.84
[INFO 2017-06-29 17:08:42,780 main.py:57] epoch 25, training loss: 12012.78, average training loss: 18515.81, base loss: 15305.21
[INFO 2017-06-29 17:08:45,830 main.py:57] epoch 26, training loss: 10764.65, average training loss: 18228.73, base loss: 15318.68
[INFO 2017-06-29 17:08:48,917 main.py:57] epoch 27, training loss: 11467.79, average training loss: 17987.27, base loss: 15377.78
[INFO 2017-06-29 17:08:52,095 main.py:57] epoch 28, training loss: 11730.41, average training loss: 17771.52, base loss: 15413.95
[INFO 2017-06-29 17:08:55,209 main.py:57] epoch 29, training loss: 10213.89, average training loss: 17519.60, base loss: 15384.14
[INFO 2017-06-29 17:08:58,309 main.py:57] epoch 30, training loss: 10172.44, average training loss: 17282.59, base loss: 15367.31
[INFO 2017-06-29 17:09:01,341 main.py:57] epoch 31, training loss: 10166.28, average training loss: 17060.21, base loss: 15361.28
[INFO 2017-06-29 17:09:04,384 main.py:57] epoch 32, training loss: 10379.28, average training loss: 16857.75, base loss: 15371.24
[INFO 2017-06-29 17:09:07,435 main.py:57] epoch 33, training loss: 10552.35, average training loss: 16672.30, base loss: 15387.21
[INFO 2017-06-29 17:09:10,385 main.py:57] epoch 34, training loss: 10501.19, average training loss: 16495.98, base loss: 15379.65
[INFO 2017-06-29 17:09:13,480 main.py:57] epoch 35, training loss: 10452.42, average training loss: 16328.11, base loss: 15391.46
[INFO 2017-06-29 17:09:16,529 main.py:57] epoch 36, training loss: 9770.39, average training loss: 16150.87, base loss: 15377.66
[INFO 2017-06-29 17:09:19,670 main.py:57] epoch 37, training loss: 10761.73, average training loss: 16009.05, base loss: 15388.08
[INFO 2017-06-29 17:09:22,716 main.py:57] epoch 38, training loss: 9683.47, average training loss: 15846.86, base loss: 15383.85
[INFO 2017-06-29 17:09:25,692 main.py:57] epoch 39, training loss: 10780.52, average training loss: 15720.20, base loss: 15393.31
[INFO 2017-06-29 17:09:28,881 main.py:57] epoch 40, training loss: 11248.06, average training loss: 15611.12, base loss: 15422.26
[INFO 2017-06-29 17:09:31,887 main.py:57] epoch 41, training loss: 10864.21, average training loss: 15498.10, base loss: 15446.55
[INFO 2017-06-29 17:09:34,937 main.py:57] epoch 42, training loss: 9668.21, average training loss: 15362.52, base loss: 15422.66
[INFO 2017-06-29 17:09:37,999 main.py:57] epoch 43, training loss: 9377.84, average training loss: 15226.51, base loss: 15386.76
[INFO 2017-06-29 17:09:40,994 main.py:57] epoch 44, training loss: 10227.76, average training loss: 15115.42, base loss: 15393.27
[INFO 2017-06-29 17:09:44,010 main.py:57] epoch 45, training loss: 10879.44, average training loss: 15023.34, base loss: 15427.23
[INFO 2017-06-29 17:09:47,050 main.py:57] epoch 46, training loss: 10079.23, average training loss: 14918.14, base loss: 15422.67
[INFO 2017-06-29 17:09:50,124 main.py:57] epoch 47, training loss: 10750.32, average training loss: 14831.31, base loss: 15450.93
[INFO 2017-06-29 17:09:53,146 main.py:57] epoch 48, training loss: 8883.15, average training loss: 14709.92, base loss: 15412.20
[INFO 2017-06-29 17:09:56,150 main.py:57] epoch 49, training loss: 9543.95, average training loss: 14606.60, base loss: 15417.56
[INFO 2017-06-29 17:09:59,172 main.py:57] epoch 50, training loss: 9755.94, average training loss: 14511.49, base loss: 15405.70
[INFO 2017-06-29 17:10:02,206 main.py:57] epoch 51, training loss: 9630.92, average training loss: 14417.63, base loss: 15394.43
[INFO 2017-06-29 17:10:05,259 main.py:57] epoch 52, training loss: 10366.94, average training loss: 14341.21, base loss: 15408.08
[INFO 2017-06-29 17:10:08,385 main.py:57] epoch 53, training loss: 8995.34, average training loss: 14242.21, base loss: 15392.79
[INFO 2017-06-29 17:10:11,393 main.py:57] epoch 54, training loss: 11443.92, average training loss: 14191.33, base loss: 15431.18
[INFO 2017-06-29 17:10:14,397 main.py:57] epoch 55, training loss: 9248.90, average training loss: 14103.07, base loss: 15425.28
[INFO 2017-06-29 17:10:17,427 main.py:57] epoch 56, training loss: 10421.39, average training loss: 14038.48, base loss: 15448.75
[INFO 2017-06-29 17:10:20,420 main.py:57] epoch 57, training loss: 10349.26, average training loss: 13974.87, base loss: 15448.59
[INFO 2017-06-29 17:10:23,446 main.py:57] epoch 58, training loss: 9681.91, average training loss: 13902.11, base loss: 15451.52
[INFO 2017-06-29 17:10:26,530 main.py:57] epoch 59, training loss: 9251.52, average training loss: 13824.60, base loss: 15434.65
[INFO 2017-06-29 17:10:29,549 main.py:57] epoch 60, training loss: 10175.74, average training loss: 13764.78, base loss: 15452.88
[INFO 2017-06-29 17:10:32,625 main.py:57] epoch 61, training loss: 9519.83, average training loss: 13696.32, base loss: 15443.83
[INFO 2017-06-29 17:10:35,624 main.py:57] epoch 62, training loss: 10302.10, average training loss: 13642.44, base loss: 15455.50
[INFO 2017-06-29 17:10:38,719 main.py:57] epoch 63, training loss: 11063.56, average training loss: 13602.15, base loss: 15479.00
[INFO 2017-06-29 17:10:41,739 main.py:57] epoch 64, training loss: 9683.70, average training loss: 13541.86, base loss: 15476.53
[INFO 2017-06-29 17:10:44,868 main.py:57] epoch 65, training loss: 9489.74, average training loss: 13480.47, base loss: 15462.35
[INFO 2017-06-29 17:10:47,843 main.py:57] epoch 66, training loss: 10060.65, average training loss: 13429.42, base loss: 15456.13
[INFO 2017-06-29 17:10:50,893 main.py:57] epoch 67, training loss: 8657.36, average training loss: 13359.25, base loss: 15432.78
[INFO 2017-06-29 17:10:53,865 main.py:57] epoch 68, training loss: 9772.43, average training loss: 13307.26, base loss: 15428.68
[INFO 2017-06-29 17:10:56,968 main.py:57] epoch 69, training loss: 9417.30, average training loss: 13251.69, base loss: 15416.13
[INFO 2017-06-29 17:10:59,977 main.py:57] epoch 70, training loss: 9036.41, average training loss: 13192.32, base loss: 15404.02
[INFO 2017-06-29 17:11:03,049 main.py:57] epoch 71, training loss: 10581.52, average training loss: 13156.06, base loss: 15408.05
[INFO 2017-06-29 17:11:06,105 main.py:57] epoch 72, training loss: 9148.37, average training loss: 13101.16, base loss: 15391.89
[INFO 2017-06-29 17:11:09,048 main.py:57] epoch 73, training loss: 9455.08, average training loss: 13051.89, base loss: 15383.04
[INFO 2017-06-29 17:11:12,027 main.py:57] epoch 74, training loss: 9581.52, average training loss: 13005.62, base loss: 15379.60
[INFO 2017-06-29 17:11:15,038 main.py:57] epoch 75, training loss: 8897.45, average training loss: 12951.56, base loss: 15362.37
[INFO 2017-06-29 17:11:18,105 main.py:57] epoch 76, training loss: 9398.55, average training loss: 12905.42, base loss: 15360.02
[INFO 2017-06-29 17:11:21,038 main.py:57] epoch 77, training loss: 10855.70, average training loss: 12879.14, base loss: 15383.86
[INFO 2017-06-29 17:11:24,088 main.py:57] epoch 78, training loss: 8031.75, average training loss: 12817.78, base loss: 15359.18
[INFO 2017-06-29 17:11:27,243 main.py:57] epoch 79, training loss: 10114.50, average training loss: 12783.99, base loss: 15355.06
[INFO 2017-06-29 17:11:30,211 main.py:57] epoch 80, training loss: 10222.80, average training loss: 12752.37, base loss: 15375.65
[INFO 2017-06-29 17:11:33,248 main.py:57] epoch 81, training loss: 9224.29, average training loss: 12709.35, base loss: 15359.84
[INFO 2017-06-29 17:11:36,279 main.py:57] epoch 82, training loss: 9321.97, average training loss: 12668.54, base loss: 15359.10
[INFO 2017-06-29 17:11:39,188 main.py:57] epoch 83, training loss: 12036.53, average training loss: 12661.01, base loss: 15397.03
[INFO 2017-06-29 17:11:42,311 main.py:57] epoch 84, training loss: 10790.01, average training loss: 12639.00, base loss: 15409.68
[INFO 2017-06-29 17:11:45,377 main.py:57] epoch 85, training loss: 10112.02, average training loss: 12609.62, base loss: 15421.45
[INFO 2017-06-29 17:11:48,470 main.py:57] epoch 86, training loss: 11317.76, average training loss: 12594.77, base loss: 15447.23
[INFO 2017-06-29 17:11:51,480 main.py:57] epoch 87, training loss: 9563.01, average training loss: 12560.32, base loss: 15445.34
[INFO 2017-06-29 17:11:54,559 main.py:57] epoch 88, training loss: 9174.98, average training loss: 12522.28, base loss: 15440.14
[INFO 2017-06-29 17:11:57,586 main.py:57] epoch 89, training loss: 9153.49, average training loss: 12484.85, base loss: 15428.49
[INFO 2017-06-29 17:12:00,598 main.py:57] epoch 90, training loss: 8269.27, average training loss: 12438.52, base loss: 15409.73
[INFO 2017-06-29 17:12:03,606 main.py:57] epoch 91, training loss: 9712.61, average training loss: 12408.89, base loss: 15419.67
[INFO 2017-06-29 17:12:06,579 main.py:57] epoch 92, training loss: 11487.68, average training loss: 12398.99, base loss: 15451.54
[INFO 2017-06-29 17:12:09,675 main.py:57] epoch 93, training loss: 10615.36, average training loss: 12380.01, base loss: 15474.55
[INFO 2017-06-29 17:12:12,783 main.py:57] epoch 94, training loss: 11046.24, average training loss: 12365.97, base loss: 15494.20
[INFO 2017-06-29 17:12:15,778 main.py:57] epoch 95, training loss: 9524.68, average training loss: 12336.38, base loss: 15493.37
[INFO 2017-06-29 17:12:18,855 main.py:57] epoch 96, training loss: 9880.81, average training loss: 12311.06, base loss: 15509.91
[INFO 2017-06-29 17:12:21,861 main.py:57] epoch 97, training loss: 9112.97, average training loss: 12278.43, base loss: 15512.17
[INFO 2017-06-29 17:12:24,902 main.py:57] epoch 98, training loss: 9418.99, average training loss: 12249.54, base loss: 15522.40
[INFO 2017-06-29 17:12:27,879 main.py:57] epoch 99, training loss: 9361.52, average training loss: 12220.66, base loss: 15524.89
[INFO 2017-06-29 17:12:27,880 main.py:59] epoch 99, testing
[INFO 2017-06-29 17:12:40,708 main.py:104] average testing loss: 10052.19, base loss: 16241.69
[INFO 2017-06-29 17:12:40,709 main.py:105] improve_loss: 6189.50, improve_percent: 0.38
[INFO 2017-06-29 17:12:40,710 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:12:40,735 main.py:71] current best improved percent: 0.38
[INFO 2017-06-29 17:12:43,726 main.py:57] epoch 100, training loss: 9076.14, average training loss: 12189.53, base loss: 15519.65
[INFO 2017-06-29 17:12:46,756 main.py:57] epoch 101, training loss: 8263.54, average training loss: 12151.04, base loss: 15508.78
[INFO 2017-06-29 17:12:49,830 main.py:57] epoch 102, training loss: 7478.03, average training loss: 12105.67, base loss: 15489.55
[INFO 2017-06-29 17:12:52,836 main.py:57] epoch 103, training loss: 9027.80, average training loss: 12076.08, base loss: 15491.06
[INFO 2017-06-29 17:12:55,817 main.py:57] epoch 104, training loss: 9032.00, average training loss: 12047.08, base loss: 15481.94
[INFO 2017-06-29 17:12:58,807 main.py:57] epoch 105, training loss: 9196.59, average training loss: 12020.19, base loss: 15479.45
[INFO 2017-06-29 17:13:01,832 main.py:57] epoch 106, training loss: 8709.55, average training loss: 11989.25, base loss: 15472.21
[INFO 2017-06-29 17:13:04,836 main.py:57] epoch 107, training loss: 12298.16, average training loss: 11992.11, base loss: 15508.02
[INFO 2017-06-29 17:13:07,958 main.py:57] epoch 108, training loss: 9563.45, average training loss: 11969.83, base loss: 15514.41
[INFO 2017-06-29 17:13:11,103 main.py:57] epoch 109, training loss: 9473.45, average training loss: 11947.14, base loss: 15521.25
[INFO 2017-06-29 17:13:14,178 main.py:57] epoch 110, training loss: 9177.41, average training loss: 11922.18, base loss: 15513.14
[INFO 2017-06-29 17:13:17,315 main.py:57] epoch 111, training loss: 8586.31, average training loss: 11892.40, base loss: 15506.74
[INFO 2017-06-29 17:13:20,349 main.py:57] epoch 112, training loss: 8825.50, average training loss: 11865.26, base loss: 15508.14
[INFO 2017-06-29 17:13:23,370 main.py:57] epoch 113, training loss: 8952.16, average training loss: 11839.71, base loss: 15504.85
[INFO 2017-06-29 17:13:26,437 main.py:57] epoch 114, training loss: 9501.13, average training loss: 11819.37, base loss: 15504.19
[INFO 2017-06-29 17:13:29,448 main.py:57] epoch 115, training loss: 9429.59, average training loss: 11798.77, base loss: 15505.51
[INFO 2017-06-29 17:13:32,660 main.py:57] epoch 116, training loss: 8324.69, average training loss: 11769.08, base loss: 15490.26
[INFO 2017-06-29 17:13:35,727 main.py:57] epoch 117, training loss: 8987.32, average training loss: 11745.50, base loss: 15486.43
[INFO 2017-06-29 17:13:38,687 main.py:57] epoch 118, training loss: 9008.52, average training loss: 11722.50, base loss: 15478.90
[INFO 2017-06-29 17:13:41,717 main.py:57] epoch 119, training loss: 8669.98, average training loss: 11697.06, base loss: 15469.18
[INFO 2017-06-29 17:13:44,853 main.py:57] epoch 120, training loss: 9276.36, average training loss: 11677.06, base loss: 15472.44
[INFO 2017-06-29 17:13:47,824 main.py:57] epoch 121, training loss: 7952.10, average training loss: 11646.53, base loss: 15460.47
[INFO 2017-06-29 17:13:50,913 main.py:57] epoch 122, training loss: 8439.65, average training loss: 11620.45, base loss: 15452.26
[INFO 2017-06-29 17:13:53,919 main.py:57] epoch 123, training loss: 9464.03, average training loss: 11603.06, base loss: 15461.89
[INFO 2017-06-29 17:13:56,907 main.py:57] epoch 124, training loss: 8309.83, average training loss: 11576.72, base loss: 15452.53
[INFO 2017-06-29 17:13:59,992 main.py:57] epoch 125, training loss: 8207.10, average training loss: 11549.97, base loss: 15435.01
[INFO 2017-06-29 17:14:03,105 main.py:57] epoch 126, training loss: 10430.15, average training loss: 11541.16, base loss: 15455.12
[INFO 2017-06-29 17:14:06,144 main.py:57] epoch 127, training loss: 8182.70, average training loss: 11514.92, base loss: 15442.66
[INFO 2017-06-29 17:14:09,245 main.py:57] epoch 128, training loss: 7759.60, average training loss: 11485.81, base loss: 15427.39
[INFO 2017-06-29 17:14:12,463 main.py:57] epoch 129, training loss: 8911.23, average training loss: 11466.00, base loss: 15421.22
[INFO 2017-06-29 17:14:15,556 main.py:57] epoch 130, training loss: 9081.48, average training loss: 11447.80, base loss: 15429.60
[INFO 2017-06-29 17:14:18,484 main.py:57] epoch 131, training loss: 9521.51, average training loss: 11433.21, base loss: 15433.39
[INFO 2017-06-29 17:14:21,458 main.py:57] epoch 132, training loss: 8991.27, average training loss: 11414.85, base loss: 15429.15
[INFO 2017-06-29 17:14:24,703 main.py:57] epoch 133, training loss: 8323.46, average training loss: 11391.78, base loss: 15415.25
[INFO 2017-06-29 17:14:27,825 main.py:57] epoch 134, training loss: 9365.47, average training loss: 11376.77, base loss: 15415.13
[INFO 2017-06-29 17:14:30,851 main.py:57] epoch 135, training loss: 8294.54, average training loss: 11354.10, base loss: 15408.89
[INFO 2017-06-29 17:14:33,842 main.py:57] epoch 136, training loss: 9150.96, average training loss: 11338.02, base loss: 15413.59
[INFO 2017-06-29 17:14:36,846 main.py:57] epoch 137, training loss: 8902.95, average training loss: 11320.38, base loss: 15410.02
[INFO 2017-06-29 17:14:39,833 main.py:57] epoch 138, training loss: 8992.77, average training loss: 11303.63, base loss: 15412.67
[INFO 2017-06-29 17:14:42,882 main.py:57] epoch 139, training loss: 7695.32, average training loss: 11277.86, base loss: 15395.80
[INFO 2017-06-29 17:14:46,115 main.py:57] epoch 140, training loss: 7565.04, average training loss: 11251.53, base loss: 15383.13
[INFO 2017-06-29 17:14:49,239 main.py:57] epoch 141, training loss: 9292.18, average training loss: 11237.73, base loss: 15384.36
[INFO 2017-06-29 17:14:52,275 main.py:57] epoch 142, training loss: 8951.77, average training loss: 11221.74, base loss: 15388.81
[INFO 2017-06-29 17:14:55,289 main.py:57] epoch 143, training loss: 8711.97, average training loss: 11204.31, base loss: 15388.15
[INFO 2017-06-29 17:14:58,298 main.py:57] epoch 144, training loss: 8926.28, average training loss: 11188.60, base loss: 15383.29
[INFO 2017-06-29 17:15:01,295 main.py:57] epoch 145, training loss: 8552.81, average training loss: 11170.55, base loss: 15384.37
[INFO 2017-06-29 17:15:04,292 main.py:57] epoch 146, training loss: 9396.70, average training loss: 11158.48, base loss: 15387.18
[INFO 2017-06-29 17:15:07,292 main.py:57] epoch 147, training loss: 9069.25, average training loss: 11144.37, base loss: 15392.37
[INFO 2017-06-29 17:15:10,265 main.py:57] epoch 148, training loss: 9519.34, average training loss: 11133.46, base loss: 15397.14
[INFO 2017-06-29 17:15:13,274 main.py:57] epoch 149, training loss: 7956.99, average training loss: 11112.28, base loss: 15389.63
[INFO 2017-06-29 17:15:16,276 main.py:57] epoch 150, training loss: 10446.19, average training loss: 11107.87, base loss: 15398.25
[INFO 2017-06-29 17:15:19,219 main.py:57] epoch 151, training loss: 10024.89, average training loss: 11100.75, base loss: 15404.78
[INFO 2017-06-29 17:15:22,286 main.py:57] epoch 152, training loss: 8934.16, average training loss: 11086.59, base loss: 15405.39
[INFO 2017-06-29 17:15:25,235 main.py:57] epoch 153, training loss: 8407.74, average training loss: 11069.19, base loss: 15407.45
[INFO 2017-06-29 17:15:28,270 main.py:57] epoch 154, training loss: 8152.86, average training loss: 11050.38, base loss: 15400.69
[INFO 2017-06-29 17:15:31,264 main.py:57] epoch 155, training loss: 9463.61, average training loss: 11040.20, base loss: 15408.62
[INFO 2017-06-29 17:15:34,269 main.py:57] epoch 156, training loss: 10329.85, average training loss: 11035.68, base loss: 15429.59
[INFO 2017-06-29 17:15:37,231 main.py:57] epoch 157, training loss: 9217.19, average training loss: 11024.17, base loss: 15430.76
[INFO 2017-06-29 17:15:40,305 main.py:57] epoch 158, training loss: 9459.47, average training loss: 11014.33, base loss: 15438.45
[INFO 2017-06-29 17:15:43,337 main.py:57] epoch 159, training loss: 9052.15, average training loss: 11002.07, base loss: 15445.72
[INFO 2017-06-29 17:15:46,313 main.py:57] epoch 160, training loss: 9156.72, average training loss: 10990.60, base loss: 15445.75
[INFO 2017-06-29 17:15:49,331 main.py:57] epoch 161, training loss: 8560.14, average training loss: 10975.60, base loss: 15445.58
[INFO 2017-06-29 17:15:52,444 main.py:57] epoch 162, training loss: 9416.90, average training loss: 10966.04, base loss: 15451.56
[INFO 2017-06-29 17:15:55,566 main.py:57] epoch 163, training loss: 9181.58, average training loss: 10955.16, base loss: 15447.81
[INFO 2017-06-29 17:15:58,582 main.py:57] epoch 164, training loss: 8177.13, average training loss: 10938.32, base loss: 15437.05
[INFO 2017-06-29 17:16:01,685 main.py:57] epoch 165, training loss: 7980.00, average training loss: 10920.50, base loss: 15426.76
[INFO 2017-06-29 17:16:04,660 main.py:57] epoch 166, training loss: 8776.01, average training loss: 10907.66, base loss: 15421.85
[INFO 2017-06-29 17:16:07,701 main.py:57] epoch 167, training loss: 7926.17, average training loss: 10889.91, base loss: 15413.42
[INFO 2017-06-29 17:16:10,650 main.py:57] epoch 168, training loss: 8595.53, average training loss: 10876.34, base loss: 15408.08
[INFO 2017-06-29 17:16:13,747 main.py:57] epoch 169, training loss: 8422.91, average training loss: 10861.90, base loss: 15407.51
[INFO 2017-06-29 17:16:16,804 main.py:57] epoch 170, training loss: 8661.13, average training loss: 10849.03, base loss: 15402.25
[INFO 2017-06-29 17:16:19,767 main.py:57] epoch 171, training loss: 9121.93, average training loss: 10838.99, base loss: 15401.91
[INFO 2017-06-29 17:16:22,819 main.py:57] epoch 172, training loss: 8677.75, average training loss: 10826.50, base loss: 15398.42
[INFO 2017-06-29 17:16:25,877 main.py:57] epoch 173, training loss: 7291.63, average training loss: 10806.18, base loss: 15385.47
[INFO 2017-06-29 17:16:28,875 main.py:57] epoch 174, training loss: 8278.01, average training loss: 10791.74, base loss: 15377.98
[INFO 2017-06-29 17:16:31,904 main.py:57] epoch 175, training loss: 8625.39, average training loss: 10779.43, base loss: 15382.46
[INFO 2017-06-29 17:16:34,915 main.py:57] epoch 176, training loss: 9770.21, average training loss: 10773.73, base loss: 15390.27
[INFO 2017-06-29 17:16:37,942 main.py:57] epoch 177, training loss: 8041.60, average training loss: 10758.38, base loss: 15385.49
[INFO 2017-06-29 17:16:40,975 main.py:57] epoch 178, training loss: 8031.01, average training loss: 10743.14, base loss: 15381.72
[INFO 2017-06-29 17:16:44,004 main.py:57] epoch 179, training loss: 8373.94, average training loss: 10729.98, base loss: 15379.36
[INFO 2017-06-29 17:16:47,025 main.py:57] epoch 180, training loss: 8985.78, average training loss: 10720.34, base loss: 15376.81
[INFO 2017-06-29 17:16:50,010 main.py:57] epoch 181, training loss: 9313.97, average training loss: 10712.62, base loss: 15384.77
[INFO 2017-06-29 17:16:53,108 main.py:57] epoch 182, training loss: 8207.27, average training loss: 10698.93, base loss: 15374.94
[INFO 2017-06-29 17:16:56,093 main.py:57] epoch 183, training loss: 8587.65, average training loss: 10687.45, base loss: 15375.43
[INFO 2017-06-29 17:16:59,055 main.py:57] epoch 184, training loss: 8889.27, average training loss: 10677.73, base loss: 15376.39
[INFO 2017-06-29 17:17:02,011 main.py:57] epoch 185, training loss: 8463.77, average training loss: 10665.83, base loss: 15375.52
[INFO 2017-06-29 17:17:04,979 main.py:57] epoch 186, training loss: 8049.51, average training loss: 10651.84, base loss: 15367.87
[INFO 2017-06-29 17:17:08,016 main.py:57] epoch 187, training loss: 10022.04, average training loss: 10648.49, base loss: 15380.09
[INFO 2017-06-29 17:17:11,016 main.py:57] epoch 188, training loss: 8340.90, average training loss: 10636.28, base loss: 15380.93
[INFO 2017-06-29 17:17:14,019 main.py:57] epoch 189, training loss: 9252.97, average training loss: 10629.00, base loss: 15384.17
[INFO 2017-06-29 17:17:17,006 main.py:57] epoch 190, training loss: 8882.09, average training loss: 10619.85, base loss: 15386.36
[INFO 2017-06-29 17:17:20,034 main.py:57] epoch 191, training loss: 8361.87, average training loss: 10608.09, base loss: 15381.41
[INFO 2017-06-29 17:17:23,050 main.py:57] epoch 192, training loss: 8304.86, average training loss: 10596.16, base loss: 15384.85
[INFO 2017-06-29 17:17:26,055 main.py:57] epoch 193, training loss: 8054.71, average training loss: 10583.06, base loss: 15382.60
[INFO 2017-06-29 17:17:29,026 main.py:57] epoch 194, training loss: 8700.48, average training loss: 10573.40, base loss: 15389.76
[INFO 2017-06-29 17:17:32,049 main.py:57] epoch 195, training loss: 9090.09, average training loss: 10565.83, base loss: 15392.29
[INFO 2017-06-29 17:17:35,145 main.py:57] epoch 196, training loss: 9576.43, average training loss: 10560.81, base loss: 15395.07
[INFO 2017-06-29 17:17:38,097 main.py:57] epoch 197, training loss: 8070.98, average training loss: 10548.24, base loss: 15388.78
[INFO 2017-06-29 17:17:41,068 main.py:57] epoch 198, training loss: 8916.02, average training loss: 10540.04, base loss: 15390.50
[INFO 2017-06-29 17:17:44,111 main.py:57] epoch 199, training loss: 8768.72, average training loss: 10531.18, base loss: 15390.08
[INFO 2017-06-29 17:17:44,111 main.py:59] epoch 199, testing
[INFO 2017-06-29 17:17:56,548 main.py:104] average testing loss: 9425.02, base loss: 16577.11
[INFO 2017-06-29 17:17:56,548 main.py:105] improve_loss: 7152.09, improve_percent: 0.43
[INFO 2017-06-29 17:17:56,550 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:17:56,576 main.py:71] current best improved percent: 0.43
[INFO 2017-06-29 17:17:59,649 main.py:57] epoch 200, training loss: 8793.78, average training loss: 10522.53, base loss: 15389.97
[INFO 2017-06-29 17:18:02,605 main.py:57] epoch 201, training loss: 8027.15, average training loss: 10510.18, base loss: 15389.60
[INFO 2017-06-29 17:18:05,549 main.py:57] epoch 202, training loss: 8673.68, average training loss: 10501.13, base loss: 15383.72
[INFO 2017-06-29 17:18:08,692 main.py:57] epoch 203, training loss: 8385.32, average training loss: 10490.76, base loss: 15384.03
[INFO 2017-06-29 17:18:11,869 main.py:57] epoch 204, training loss: 9580.04, average training loss: 10486.32, base loss: 15389.90
[INFO 2017-06-29 17:18:14,798 main.py:57] epoch 205, training loss: 8232.99, average training loss: 10475.38, base loss: 15387.51
[INFO 2017-06-29 17:18:17,822 main.py:57] epoch 206, training loss: 8778.29, average training loss: 10467.18, base loss: 15393.75
[INFO 2017-06-29 17:18:20,772 main.py:57] epoch 207, training loss: 9001.89, average training loss: 10460.14, base loss: 15395.14
[INFO 2017-06-29 17:18:23,833 main.py:57] epoch 208, training loss: 8293.08, average training loss: 10449.77, base loss: 15386.32
[INFO 2017-06-29 17:18:26,824 main.py:57] epoch 209, training loss: 9141.95, average training loss: 10443.54, base loss: 15388.26
[INFO 2017-06-29 17:18:29,794 main.py:57] epoch 210, training loss: 8245.81, average training loss: 10433.13, base loss: 15386.52
[INFO 2017-06-29 17:18:32,789 main.py:57] epoch 211, training loss: 7842.89, average training loss: 10420.91, base loss: 15376.70
[INFO 2017-06-29 17:18:35,740 main.py:57] epoch 212, training loss: 7385.36, average training loss: 10406.66, base loss: 15366.98
[INFO 2017-06-29 17:18:38,668 main.py:57] epoch 213, training loss: 9145.10, average training loss: 10400.76, base loss: 15372.31
[INFO 2017-06-29 17:18:41,580 main.py:57] epoch 214, training loss: 8160.35, average training loss: 10390.34, base loss: 15371.47
[INFO 2017-06-29 17:18:44,581 main.py:57] epoch 215, training loss: 8760.01, average training loss: 10382.79, base loss: 15368.95
[INFO 2017-06-29 17:18:47,583 main.py:57] epoch 216, training loss: 8953.09, average training loss: 10376.20, base loss: 15370.85
[INFO 2017-06-29 17:18:50,604 main.py:57] epoch 217, training loss: 9231.08, average training loss: 10370.95, base loss: 15374.22
[INFO 2017-06-29 17:18:53,683 main.py:57] epoch 218, training loss: 9463.47, average training loss: 10366.81, base loss: 15380.10
[INFO 2017-06-29 17:18:56,675 main.py:57] epoch 219, training loss: 7571.92, average training loss: 10354.10, base loss: 15370.06
[INFO 2017-06-29 17:18:59,620 main.py:57] epoch 220, training loss: 8264.62, average training loss: 10344.65, base loss: 15369.87
[INFO 2017-06-29 17:19:02,622 main.py:57] epoch 221, training loss: 8472.60, average training loss: 10336.22, base loss: 15365.02
[INFO 2017-06-29 17:19:05,663 main.py:57] epoch 222, training loss: 8610.86, average training loss: 10328.48, base loss: 15367.63
[INFO 2017-06-29 17:19:08,739 main.py:57] epoch 223, training loss: 8225.83, average training loss: 10319.09, base loss: 15362.69
[INFO 2017-06-29 17:19:11,679 main.py:57] epoch 224, training loss: 9581.61, average training loss: 10315.82, base loss: 15367.49
[INFO 2017-06-29 17:19:14,665 main.py:57] epoch 225, training loss: 8839.73, average training loss: 10309.28, base loss: 15370.16
[INFO 2017-06-29 17:19:17,608 main.py:57] epoch 226, training loss: 8449.55, average training loss: 10301.09, base loss: 15369.70
[INFO 2017-06-29 17:19:20,711 main.py:57] epoch 227, training loss: 8306.21, average training loss: 10292.34, base loss: 15366.58
[INFO 2017-06-29 17:19:23,745 main.py:57] epoch 228, training loss: 8815.03, average training loss: 10285.89, base loss: 15368.59
[INFO 2017-06-29 17:19:26,783 main.py:57] epoch 229, training loss: 8963.85, average training loss: 10280.14, base loss: 15371.25
[INFO 2017-06-29 17:19:29,808 main.py:57] epoch 230, training loss: 9123.54, average training loss: 10275.14, base loss: 15373.63
[INFO 2017-06-29 17:19:32,797 main.py:57] epoch 231, training loss: 8070.61, average training loss: 10265.63, base loss: 15369.54
[INFO 2017-06-29 17:19:35,753 main.py:57] epoch 232, training loss: 9631.64, average training loss: 10262.91, base loss: 15374.83
[INFO 2017-06-29 17:19:38,796 main.py:57] epoch 233, training loss: 8697.80, average training loss: 10256.22, base loss: 15372.34
[INFO 2017-06-29 17:19:41,757 main.py:57] epoch 234, training loss: 7738.64, average training loss: 10245.51, base loss: 15366.56
[INFO 2017-06-29 17:19:44,811 main.py:57] epoch 235, training loss: 9202.05, average training loss: 10241.09, base loss: 15370.83
[INFO 2017-06-29 17:19:47,757 main.py:57] epoch 236, training loss: 8504.28, average training loss: 10233.76, base loss: 15368.93
[INFO 2017-06-29 17:19:50,697 main.py:57] epoch 237, training loss: 9165.27, average training loss: 10229.27, base loss: 15373.83
[INFO 2017-06-29 17:19:53,898 main.py:57] epoch 238, training loss: 8007.10, average training loss: 10219.97, base loss: 15368.48
[INFO 2017-06-29 17:19:56,933 main.py:57] epoch 239, training loss: 9692.16, average training loss: 10217.77, base loss: 15372.34
[INFO 2017-06-29 17:19:59,963 main.py:57] epoch 240, training loss: 9376.84, average training loss: 10214.29, base loss: 15376.06
[INFO 2017-06-29 17:20:02,983 main.py:57] epoch 241, training loss: 8204.47, average training loss: 10205.98, base loss: 15376.51
[INFO 2017-06-29 17:20:06,032 main.py:57] epoch 242, training loss: 8576.08, average training loss: 10199.27, base loss: 15375.27
[INFO 2017-06-29 17:20:09,004 main.py:57] epoch 243, training loss: 8205.45, average training loss: 10191.10, base loss: 15367.78
[INFO 2017-06-29 17:20:11,988 main.py:57] epoch 244, training loss: 8223.28, average training loss: 10183.07, base loss: 15367.64
[INFO 2017-06-29 17:20:15,126 main.py:57] epoch 245, training loss: 8070.20, average training loss: 10174.48, base loss: 15365.79
[INFO 2017-06-29 17:20:18,102 main.py:57] epoch 246, training loss: 8821.42, average training loss: 10169.00, base loss: 15365.89
[INFO 2017-06-29 17:20:21,183 main.py:57] epoch 247, training loss: 9239.08, average training loss: 10165.25, base loss: 15369.15
[INFO 2017-06-29 17:20:24,211 main.py:57] epoch 248, training loss: 7669.75, average training loss: 10155.23, base loss: 15361.70
[INFO 2017-06-29 17:20:27,253 main.py:57] epoch 249, training loss: 8514.88, average training loss: 10148.67, base loss: 15363.15
[INFO 2017-06-29 17:20:30,440 main.py:57] epoch 250, training loss: 10051.21, average training loss: 10148.28, base loss: 15373.64
[INFO 2017-06-29 17:20:33,509 main.py:57] epoch 251, training loss: 8043.90, average training loss: 10139.93, base loss: 15367.46
[INFO 2017-06-29 17:20:36,663 main.py:57] epoch 252, training loss: 8704.66, average training loss: 10134.26, base loss: 15367.14
[INFO 2017-06-29 17:20:39,680 main.py:57] epoch 253, training loss: 8167.15, average training loss: 10126.51, base loss: 15365.46
[INFO 2017-06-29 17:20:42,744 main.py:57] epoch 254, training loss: 9190.81, average training loss: 10122.84, base loss: 15373.59
[INFO 2017-06-29 17:20:45,796 main.py:57] epoch 255, training loss: 7914.77, average training loss: 10114.22, base loss: 15371.24
[INFO 2017-06-29 17:20:48,803 main.py:57] epoch 256, training loss: 9095.15, average training loss: 10110.25, base loss: 15375.30
[INFO 2017-06-29 17:20:51,804 main.py:57] epoch 257, training loss: 8238.59, average training loss: 10103.00, base loss: 15372.00
[INFO 2017-06-29 17:20:54,840 main.py:57] epoch 258, training loss: 8102.57, average training loss: 10095.27, base loss: 15366.63
[INFO 2017-06-29 17:20:57,814 main.py:57] epoch 259, training loss: 8700.33, average training loss: 10089.91, base loss: 15367.73
[INFO 2017-06-29 17:21:00,886 main.py:57] epoch 260, training loss: 8719.66, average training loss: 10084.66, base loss: 15365.67
[INFO 2017-06-29 17:21:03,942 main.py:57] epoch 261, training loss: 8455.77, average training loss: 10078.44, base loss: 15363.93
[INFO 2017-06-29 17:21:06,916 main.py:57] epoch 262, training loss: 9269.24, average training loss: 10075.37, base loss: 15363.28
[INFO 2017-06-29 17:21:09,875 main.py:57] epoch 263, training loss: 9554.57, average training loss: 10073.39, base loss: 15371.87
[INFO 2017-06-29 17:21:12,886 main.py:57] epoch 264, training loss: 8395.11, average training loss: 10067.06, base loss: 15369.84
[INFO 2017-06-29 17:21:16,025 main.py:57] epoch 265, training loss: 7495.90, average training loss: 10057.39, base loss: 15364.03
[INFO 2017-06-29 17:21:19,029 main.py:57] epoch 266, training loss: 8827.89, average training loss: 10052.79, base loss: 15364.72
[INFO 2017-06-29 17:21:21,997 main.py:57] epoch 267, training loss: 8805.19, average training loss: 10048.13, base loss: 15365.16
[INFO 2017-06-29 17:21:25,031 main.py:57] epoch 268, training loss: 7879.83, average training loss: 10040.07, base loss: 15358.72
[INFO 2017-06-29 17:21:28,070 main.py:57] epoch 269, training loss: 8983.71, average training loss: 10036.16, base loss: 15361.02
[INFO 2017-06-29 17:21:31,098 main.py:57] epoch 270, training loss: 8328.29, average training loss: 10029.86, base loss: 15360.90
[INFO 2017-06-29 17:21:34,133 main.py:57] epoch 271, training loss: 8197.06, average training loss: 10023.12, base loss: 15361.43
[INFO 2017-06-29 17:21:37,106 main.py:57] epoch 272, training loss: 10472.90, average training loss: 10024.77, base loss: 15372.44
[INFO 2017-06-29 17:21:40,248 main.py:57] epoch 273, training loss: 9368.19, average training loss: 10022.37, base loss: 15379.96
[INFO 2017-06-29 17:21:43,254 main.py:57] epoch 274, training loss: 8502.73, average training loss: 10016.85, base loss: 15380.44
[INFO 2017-06-29 17:21:46,282 main.py:57] epoch 275, training loss: 8807.60, average training loss: 10012.46, base loss: 15386.14
[INFO 2017-06-29 17:21:49,283 main.py:57] epoch 276, training loss: 8050.90, average training loss: 10005.38, base loss: 15385.17
[INFO 2017-06-29 17:21:52,383 main.py:57] epoch 277, training loss: 9017.70, average training loss: 10001.83, base loss: 15388.81
[INFO 2017-06-29 17:21:55,315 main.py:57] epoch 278, training loss: 7963.05, average training loss: 9994.52, base loss: 15383.52
[INFO 2017-06-29 17:21:58,311 main.py:57] epoch 279, training loss: 8470.73, average training loss: 9989.08, base loss: 15385.03
[INFO 2017-06-29 17:22:01,295 main.py:57] epoch 280, training loss: 7814.25, average training loss: 9981.34, base loss: 15380.73
[INFO 2017-06-29 17:22:04,266 main.py:57] epoch 281, training loss: 9336.97, average training loss: 9979.06, base loss: 15385.17
[INFO 2017-06-29 17:22:07,282 main.py:57] epoch 282, training loss: 8818.42, average training loss: 9974.95, base loss: 15386.14
[INFO 2017-06-29 17:22:10,260 main.py:57] epoch 283, training loss: 8848.51, average training loss: 9970.99, base loss: 15389.49
[INFO 2017-06-29 17:22:13,245 main.py:57] epoch 284, training loss: 7600.54, average training loss: 9962.67, base loss: 15387.36
[INFO 2017-06-29 17:22:16,177 main.py:57] epoch 285, training loss: 7249.96, average training loss: 9953.19, base loss: 15380.70
[INFO 2017-06-29 17:22:19,131 main.py:57] epoch 286, training loss: 9044.60, average training loss: 9950.02, base loss: 15387.02
[INFO 2017-06-29 17:22:22,081 main.py:57] epoch 287, training loss: 8249.52, average training loss: 9944.12, base loss: 15385.28
[INFO 2017-06-29 17:22:25,090 main.py:57] epoch 288, training loss: 8509.49, average training loss: 9939.15, base loss: 15387.48
[INFO 2017-06-29 17:22:28,184 main.py:57] epoch 289, training loss: 8425.97, average training loss: 9933.93, base loss: 15386.74
[INFO 2017-06-29 17:22:31,162 main.py:57] epoch 290, training loss: 8411.98, average training loss: 9928.70, base loss: 15389.20
[INFO 2017-06-29 17:22:34,115 main.py:57] epoch 291, training loss: 7842.30, average training loss: 9921.56, base loss: 15385.38
[INFO 2017-06-29 17:22:37,166 main.py:57] epoch 292, training loss: 7766.97, average training loss: 9914.20, base loss: 15378.26
[INFO 2017-06-29 17:22:40,180 main.py:57] epoch 293, training loss: 8340.84, average training loss: 9908.85, base loss: 15375.97
[INFO 2017-06-29 17:22:43,106 main.py:57] epoch 294, training loss: 8676.86, average training loss: 9904.68, base loss: 15378.55
[INFO 2017-06-29 17:22:46,042 main.py:57] epoch 295, training loss: 7598.43, average training loss: 9896.89, base loss: 15376.08
[INFO 2017-06-29 17:22:48,979 main.py:57] epoch 296, training loss: 7930.80, average training loss: 9890.27, base loss: 15374.09
[INFO 2017-06-29 17:22:51,989 main.py:57] epoch 297, training loss: 7570.75, average training loss: 9882.48, base loss: 15370.31
[INFO 2017-06-29 17:22:54,994 main.py:57] epoch 298, training loss: 7767.57, average training loss: 9875.41, base loss: 15368.40
[INFO 2017-06-29 17:22:58,007 main.py:57] epoch 299, training loss: 8154.77, average training loss: 9869.67, base loss: 15367.02
[INFO 2017-06-29 17:22:58,007 main.py:59] epoch 299, testing
[INFO 2017-06-29 17:23:10,169 main.py:104] average testing loss: 9083.42, base loss: 16142.44
[INFO 2017-06-29 17:23:10,169 main.py:105] improve_loss: 7059.02, improve_percent: 0.44
[INFO 2017-06-29 17:23:10,171 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:23:10,198 main.py:71] current best improved percent: 0.44
[INFO 2017-06-29 17:23:13,188 main.py:57] epoch 300, training loss: 8722.09, average training loss: 9865.86, base loss: 15369.35
[INFO 2017-06-29 17:23:16,144 main.py:57] epoch 301, training loss: 8392.82, average training loss: 9860.98, base loss: 15369.58
[INFO 2017-06-29 17:23:19,149 main.py:57] epoch 302, training loss: 7759.94, average training loss: 9854.05, base loss: 15369.56
[INFO 2017-06-29 17:23:22,212 main.py:57] epoch 303, training loss: 7631.80, average training loss: 9846.74, base loss: 15364.54
[INFO 2017-06-29 17:23:25,244 main.py:57] epoch 304, training loss: 10088.33, average training loss: 9847.53, base loss: 15373.02
[INFO 2017-06-29 17:23:28,272 main.py:57] epoch 305, training loss: 7758.22, average training loss: 9840.70, base loss: 15371.60
[INFO 2017-06-29 17:23:31,355 main.py:57] epoch 306, training loss: 8345.95, average training loss: 9835.83, base loss: 15375.52
[INFO 2017-06-29 17:23:34,368 main.py:57] epoch 307, training loss: 7286.97, average training loss: 9827.56, base loss: 15370.49
[INFO 2017-06-29 17:23:37,303 main.py:57] epoch 308, training loss: 8048.93, average training loss: 9821.80, base loss: 15369.54
[INFO 2017-06-29 17:23:40,304 main.py:57] epoch 309, training loss: 7219.77, average training loss: 9813.41, base loss: 15361.48
[INFO 2017-06-29 17:23:43,296 main.py:57] epoch 310, training loss: 7812.88, average training loss: 9806.98, base loss: 15358.16
[INFO 2017-06-29 17:23:46,249 main.py:57] epoch 311, training loss: 7658.29, average training loss: 9800.09, base loss: 15354.69
[INFO 2017-06-29 17:23:49,172 main.py:57] epoch 312, training loss: 7992.50, average training loss: 9794.31, base loss: 15354.14
[INFO 2017-06-29 17:23:52,212 main.py:57] epoch 313, training loss: 9056.31, average training loss: 9791.96, base loss: 15358.66
[INFO 2017-06-29 17:23:55,183 main.py:57] epoch 314, training loss: 8556.63, average training loss: 9788.04, base loss: 15359.57
[INFO 2017-06-29 17:23:58,099 main.py:57] epoch 315, training loss: 7770.83, average training loss: 9781.66, base loss: 15356.83
[INFO 2017-06-29 17:24:01,138 main.py:57] epoch 316, training loss: 7067.48, average training loss: 9773.10, base loss: 15350.99
[INFO 2017-06-29 17:24:04,047 main.py:57] epoch 317, training loss: 8978.52, average training loss: 9770.60, base loss: 15356.13
[INFO 2017-06-29 17:24:07,036 main.py:57] epoch 318, training loss: 8178.89, average training loss: 9765.61, base loss: 15353.43
[INFO 2017-06-29 17:24:10,046 main.py:57] epoch 319, training loss: 7194.74, average training loss: 9757.57, base loss: 15349.11
[INFO 2017-06-29 17:24:13,053 main.py:57] epoch 320, training loss: 7596.13, average training loss: 9750.84, base loss: 15344.91
[INFO 2017-06-29 17:24:16,086 main.py:57] epoch 321, training loss: 7760.34, average training loss: 9744.66, base loss: 15343.38
[INFO 2017-06-29 17:24:19,016 main.py:57] epoch 322, training loss: 8877.96, average training loss: 9741.98, base loss: 15345.54
[INFO 2017-06-29 17:24:21,923 main.py:57] epoch 323, training loss: 8792.77, average training loss: 9739.05, base loss: 15350.41
[INFO 2017-06-29 17:24:24,937 main.py:57] epoch 324, training loss: 8888.18, average training loss: 9736.43, base loss: 15354.41
[INFO 2017-06-29 17:24:27,907 main.py:57] epoch 325, training loss: 7775.41, average training loss: 9730.41, base loss: 15352.95
[INFO 2017-06-29 17:24:30,993 main.py:57] epoch 326, training loss: 9122.60, average training loss: 9728.55, base loss: 15359.79
[INFO 2017-06-29 17:24:33,964 main.py:57] epoch 327, training loss: 7936.45, average training loss: 9723.09, base loss: 15362.05
[INFO 2017-06-29 17:24:36,955 main.py:57] epoch 328, training loss: 8191.66, average training loss: 9718.44, base loss: 15361.54
[INFO 2017-06-29 17:24:39,960 main.py:57] epoch 329, training loss: 8102.83, average training loss: 9713.54, base loss: 15360.29
[INFO 2017-06-29 17:24:42,911 main.py:57] epoch 330, training loss: 7199.94, average training loss: 9705.95, base loss: 15353.37
[INFO 2017-06-29 17:24:45,947 main.py:57] epoch 331, training loss: 7645.99, average training loss: 9699.74, base loss: 15349.24
[INFO 2017-06-29 17:24:48,891 main.py:57] epoch 332, training loss: 9262.97, average training loss: 9698.43, base loss: 15352.40
[INFO 2017-06-29 17:24:51,854 main.py:57] epoch 333, training loss: 7243.99, average training loss: 9691.08, base loss: 15348.87
[INFO 2017-06-29 17:24:54,869 main.py:57] epoch 334, training loss: 9245.41, average training loss: 9689.75, base loss: 15353.64
[INFO 2017-06-29 17:24:57,810 main.py:57] epoch 335, training loss: 8443.88, average training loss: 9686.04, base loss: 15356.13
[INFO 2017-06-29 17:25:00,838 main.py:57] epoch 336, training loss: 7829.58, average training loss: 9680.53, base loss: 15350.34
[INFO 2017-06-29 17:25:03,830 main.py:57] epoch 337, training loss: 7961.01, average training loss: 9675.45, base loss: 15348.85
[INFO 2017-06-29 17:25:06,857 main.py:57] epoch 338, training loss: 8609.50, average training loss: 9672.30, base loss: 15351.19
[INFO 2017-06-29 17:25:09,873 main.py:57] epoch 339, training loss: 9443.02, average training loss: 9671.63, base loss: 15353.32
[INFO 2017-06-29 17:25:12,872 main.py:57] epoch 340, training loss: 7514.80, average training loss: 9665.30, base loss: 15348.81
[INFO 2017-06-29 17:25:15,841 main.py:57] epoch 341, training loss: 8232.58, average training loss: 9661.11, base loss: 15348.93
[INFO 2017-06-29 17:25:18,801 main.py:57] epoch 342, training loss: 7245.34, average training loss: 9654.07, base loss: 15343.78
[INFO 2017-06-29 17:25:21,855 main.py:57] epoch 343, training loss: 8020.57, average training loss: 9649.32, base loss: 15341.91
[INFO 2017-06-29 17:25:24,796 main.py:57] epoch 344, training loss: 8013.40, average training loss: 9644.58, base loss: 15343.94
[INFO 2017-06-29 17:25:27,826 main.py:57] epoch 345, training loss: 7876.27, average training loss: 9639.47, base loss: 15343.08
[INFO 2017-06-29 17:25:30,923 main.py:57] epoch 346, training loss: 9435.42, average training loss: 9638.88, base loss: 15347.24
[INFO 2017-06-29 17:25:33,947 main.py:57] epoch 347, training loss: 8212.94, average training loss: 9634.78, base loss: 15345.52
[INFO 2017-06-29 17:25:36,907 main.py:57] epoch 348, training loss: 8195.57, average training loss: 9630.66, base loss: 15345.38
[INFO 2017-06-29 17:25:39,904 main.py:57] epoch 349, training loss: 7753.40, average training loss: 9625.30, base loss: 15346.03
[INFO 2017-06-29 17:25:42,917 main.py:57] epoch 350, training loss: 8729.38, average training loss: 9622.74, base loss: 15348.40
[INFO 2017-06-29 17:25:45,921 main.py:57] epoch 351, training loss: 9354.41, average training loss: 9621.98, base loss: 15355.14
[INFO 2017-06-29 17:25:48,864 main.py:57] epoch 352, training loss: 8168.84, average training loss: 9617.87, base loss: 15355.63
[INFO 2017-06-29 17:25:51,839 main.py:57] epoch 353, training loss: 8338.15, average training loss: 9614.25, base loss: 15357.09
[INFO 2017-06-29 17:25:54,831 main.py:57] epoch 354, training loss: 8363.83, average training loss: 9610.73, base loss: 15357.98
[INFO 2017-06-29 17:25:57,760 main.py:57] epoch 355, training loss: 8704.96, average training loss: 9608.18, base loss: 15360.64
[INFO 2017-06-29 17:26:00,724 main.py:57] epoch 356, training loss: 8123.06, average training loss: 9604.02, base loss: 15358.12
[INFO 2017-06-29 17:26:03,700 main.py:57] epoch 357, training loss: 8630.68, average training loss: 9601.30, base loss: 15358.95
[INFO 2017-06-29 17:26:06,770 main.py:57] epoch 358, training loss: 8782.76, average training loss: 9599.02, base loss: 15363.54
[INFO 2017-06-29 17:26:09,694 main.py:57] epoch 359, training loss: 7689.05, average training loss: 9593.72, base loss: 15359.97
[INFO 2017-06-29 17:26:12,666 main.py:57] epoch 360, training loss: 7246.79, average training loss: 9587.22, base loss: 15355.90
[INFO 2017-06-29 17:26:15,687 main.py:57] epoch 361, training loss: 7865.80, average training loss: 9582.46, base loss: 15354.08
[INFO 2017-06-29 17:26:18,688 main.py:57] epoch 362, training loss: 8251.46, average training loss: 9578.80, base loss: 15356.13
[INFO 2017-06-29 17:26:21,615 main.py:57] epoch 363, training loss: 7453.61, average training loss: 9572.96, base loss: 15352.39
[INFO 2017-06-29 17:26:24,600 main.py:57] epoch 364, training loss: 7576.71, average training loss: 9567.49, base loss: 15348.49
[INFO 2017-06-29 17:26:27,524 main.py:57] epoch 365, training loss: 8400.81, average training loss: 9564.30, base loss: 15352.04
[INFO 2017-06-29 17:26:30,511 main.py:57] epoch 366, training loss: 8571.59, average training loss: 9561.60, base loss: 15352.54
[INFO 2017-06-29 17:26:33,418 main.py:57] epoch 367, training loss: 8556.05, average training loss: 9558.86, base loss: 15355.58
[INFO 2017-06-29 17:26:36,383 main.py:57] epoch 368, training loss: 8945.71, average training loss: 9557.20, base loss: 15358.08
[INFO 2017-06-29 17:26:39,304 main.py:57] epoch 369, training loss: 8409.91, average training loss: 9554.10, base loss: 15361.00
[INFO 2017-06-29 17:26:42,320 main.py:57] epoch 370, training loss: 8263.01, average training loss: 9550.62, base loss: 15360.18
[INFO 2017-06-29 17:26:45,282 main.py:57] epoch 371, training loss: 9434.71, average training loss: 9550.31, base loss: 15364.51
[INFO 2017-06-29 17:26:48,277 main.py:57] epoch 372, training loss: 7480.65, average training loss: 9544.76, base loss: 15360.16
[INFO 2017-06-29 17:26:51,206 main.py:57] epoch 373, training loss: 7966.63, average training loss: 9540.54, base loss: 15361.06
[INFO 2017-06-29 17:26:54,131 main.py:57] epoch 374, training loss: 7443.11, average training loss: 9534.95, base loss: 15355.62
[INFO 2017-06-29 17:26:57,139 main.py:57] epoch 375, training loss: 7814.32, average training loss: 9530.37, base loss: 15354.85
[INFO 2017-06-29 17:27:00,212 main.py:57] epoch 376, training loss: 7683.30, average training loss: 9525.47, base loss: 15353.51
[INFO 2017-06-29 17:27:03,203 main.py:57] epoch 377, training loss: 7889.95, average training loss: 9521.15, base loss: 15350.58
[INFO 2017-06-29 17:27:06,162 main.py:57] epoch 378, training loss: 8685.56, average training loss: 9518.94, base loss: 15351.24
[INFO 2017-06-29 17:27:09,238 main.py:57] epoch 379, training loss: 8814.76, average training loss: 9517.09, base loss: 15356.30
[INFO 2017-06-29 17:27:12,208 main.py:57] epoch 380, training loss: 7496.43, average training loss: 9511.78, base loss: 15355.88
[INFO 2017-06-29 17:27:15,276 main.py:57] epoch 381, training loss: 7088.81, average training loss: 9505.44, base loss: 15352.35
[INFO 2017-06-29 17:27:18,224 main.py:57] epoch 382, training loss: 8280.30, average training loss: 9502.24, base loss: 15353.17
[INFO 2017-06-29 17:27:21,249 main.py:57] epoch 383, training loss: 7080.76, average training loss: 9495.94, base loss: 15350.51
[INFO 2017-06-29 17:27:24,266 main.py:57] epoch 384, training loss: 9016.99, average training loss: 9494.69, base loss: 15352.44
[INFO 2017-06-29 17:27:27,198 main.py:57] epoch 385, training loss: 8832.67, average training loss: 9492.98, base loss: 15354.78
[INFO 2017-06-29 17:27:30,194 main.py:57] epoch 386, training loss: 7779.11, average training loss: 9488.55, base loss: 15352.28
[INFO 2017-06-29 17:27:33,180 main.py:57] epoch 387, training loss: 8306.11, average training loss: 9485.50, base loss: 15353.71
[INFO 2017-06-29 17:27:36,146 main.py:57] epoch 388, training loss: 7613.18, average training loss: 9480.69, base loss: 15352.31
[INFO 2017-06-29 17:27:39,200 main.py:57] epoch 389, training loss: 8843.70, average training loss: 9479.06, base loss: 15357.26
[INFO 2017-06-29 17:27:42,223 main.py:57] epoch 390, training loss: 7441.73, average training loss: 9473.84, base loss: 15354.60
[INFO 2017-06-29 17:27:45,178 main.py:57] epoch 391, training loss: 7717.71, average training loss: 9469.36, base loss: 15353.83
[INFO 2017-06-29 17:27:48,116 main.py:57] epoch 392, training loss: 7858.35, average training loss: 9465.27, base loss: 15351.88
[INFO 2017-06-29 17:27:51,073 main.py:57] epoch 393, training loss: 7839.81, average training loss: 9461.14, base loss: 15351.92
[INFO 2017-06-29 17:27:54,008 main.py:57] epoch 394, training loss: 7900.19, average training loss: 9457.19, base loss: 15349.78
[INFO 2017-06-29 17:27:56,991 main.py:57] epoch 395, training loss: 8057.76, average training loss: 9453.65, base loss: 15352.27
[INFO 2017-06-29 17:27:59,963 main.py:57] epoch 396, training loss: 7953.42, average training loss: 9449.88, base loss: 15351.78
[INFO 2017-06-29 17:28:02,946 main.py:57] epoch 397, training loss: 9016.54, average training loss: 9448.79, base loss: 15355.12
[INFO 2017-06-29 17:28:05,947 main.py:57] epoch 398, training loss: 9005.24, average training loss: 9447.67, base loss: 15357.54
[INFO 2017-06-29 17:28:09,004 main.py:57] epoch 399, training loss: 8336.00, average training loss: 9444.90, base loss: 15357.93
[INFO 2017-06-29 17:28:09,005 main.py:59] epoch 399, testing
[INFO 2017-06-29 17:28:21,476 main.py:104] average testing loss: 8133.92, base loss: 15400.82
[INFO 2017-06-29 17:28:21,476 main.py:105] improve_loss: 7266.90, improve_percent: 0.47
[INFO 2017-06-29 17:28:21,479 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:28:21,505 main.py:71] current best improved percent: 0.47
[INFO 2017-06-29 17:28:24,450 main.py:57] epoch 400, training loss: 9799.19, average training loss: 9445.78, base loss: 15362.07
[INFO 2017-06-29 17:28:27,482 main.py:57] epoch 401, training loss: 9314.33, average training loss: 9445.45, base loss: 15362.99
[INFO 2017-06-29 17:28:30,472 main.py:57] epoch 402, training loss: 9185.12, average training loss: 9444.81, base loss: 15364.89
[INFO 2017-06-29 17:28:33,542 main.py:57] epoch 403, training loss: 8684.68, average training loss: 9442.92, base loss: 15365.66
[INFO 2017-06-29 17:28:36,537 main.py:57] epoch 404, training loss: 7507.90, average training loss: 9438.15, base loss: 15362.07
[INFO 2017-06-29 17:28:39,590 main.py:57] epoch 405, training loss: 8872.17, average training loss: 9436.75, base loss: 15364.58
[INFO 2017-06-29 17:28:42,706 main.py:57] epoch 406, training loss: 7574.86, average training loss: 9432.18, base loss: 15364.11
[INFO 2017-06-29 17:28:45,686 main.py:57] epoch 407, training loss: 8075.09, average training loss: 9428.85, base loss: 15364.55
[INFO 2017-06-29 17:28:48,733 main.py:57] epoch 408, training loss: 8158.87, average training loss: 9425.75, base loss: 15367.08
[INFO 2017-06-29 17:28:51,775 main.py:57] epoch 409, training loss: 8042.90, average training loss: 9422.37, base loss: 15365.60
[INFO 2017-06-29 17:28:54,783 main.py:57] epoch 410, training loss: 9608.62, average training loss: 9422.83, base loss: 15368.38
[INFO 2017-06-29 17:28:57,746 main.py:57] epoch 411, training loss: 7163.14, average training loss: 9417.34, base loss: 15363.23
[INFO 2017-06-29 17:29:00,692 main.py:57] epoch 412, training loss: 8123.36, average training loss: 9414.21, base loss: 15362.52
[INFO 2017-06-29 17:29:03,630 main.py:57] epoch 413, training loss: 8436.85, average training loss: 9411.85, base loss: 15365.24
[INFO 2017-06-29 17:29:06,600 main.py:57] epoch 414, training loss: 7931.67, average training loss: 9408.28, base loss: 15363.83
[INFO 2017-06-29 17:29:09,586 main.py:57] epoch 415, training loss: 7656.70, average training loss: 9404.07, base loss: 15360.66
[INFO 2017-06-29 17:29:12,555 main.py:57] epoch 416, training loss: 7247.40, average training loss: 9398.90, base loss: 15358.16
[INFO 2017-06-29 17:29:15,507 main.py:57] epoch 417, training loss: 7539.55, average training loss: 9394.45, base loss: 15354.88
[INFO 2017-06-29 17:29:18,452 main.py:57] epoch 418, training loss: 7317.28, average training loss: 9389.49, base loss: 15353.87
[INFO 2017-06-29 17:29:21,449 main.py:57] epoch 419, training loss: 7251.64, average training loss: 9384.40, base loss: 15350.68
[INFO 2017-06-29 17:29:24,417 main.py:57] epoch 420, training loss: 8990.08, average training loss: 9383.47, base loss: 15355.32
[INFO 2017-06-29 17:29:27,426 main.py:57] epoch 421, training loss: 7356.85, average training loss: 9378.66, base loss: 15352.65
[INFO 2017-06-29 17:29:30,399 main.py:57] epoch 422, training loss: 8132.27, average training loss: 9375.72, base loss: 15352.14
[INFO 2017-06-29 17:29:33,349 main.py:57] epoch 423, training loss: 7879.68, average training loss: 9372.19, base loss: 15353.13
[INFO 2017-06-29 17:29:36,330 main.py:57] epoch 424, training loss: 8419.74, average training loss: 9369.95, base loss: 15353.10
[INFO 2017-06-29 17:29:39,403 main.py:57] epoch 425, training loss: 7027.39, average training loss: 9364.45, base loss: 15351.88
[INFO 2017-06-29 17:29:42,377 main.py:57] epoch 426, training loss: 7534.48, average training loss: 9360.16, base loss: 15349.30
[INFO 2017-06-29 17:29:45,352 main.py:57] epoch 427, training loss: 8104.29, average training loss: 9357.23, base loss: 15346.68
[INFO 2017-06-29 17:29:48,370 main.py:57] epoch 428, training loss: 7949.69, average training loss: 9353.95, base loss: 15346.67
[INFO 2017-06-29 17:29:51,298 main.py:57] epoch 429, training loss: 8052.05, average training loss: 9350.92, base loss: 15346.74
[INFO 2017-06-29 17:29:54,380 main.py:57] epoch 430, training loss: 8994.09, average training loss: 9350.09, base loss: 15347.43
[INFO 2017-06-29 17:29:57,419 main.py:57] epoch 431, training loss: 7554.64, average training loss: 9345.94, base loss: 15345.92
[INFO 2017-06-29 17:30:00,347 main.py:57] epoch 432, training loss: 8559.14, average training loss: 9344.12, base loss: 15350.43
[INFO 2017-06-29 17:30:03,261 main.py:57] epoch 433, training loss: 8334.21, average training loss: 9341.79, base loss: 15350.72
[INFO 2017-06-29 17:30:06,339 main.py:57] epoch 434, training loss: 7700.59, average training loss: 9338.02, base loss: 15348.01
[INFO 2017-06-29 17:30:09,282 main.py:57] epoch 435, training loss: 7770.06, average training loss: 9334.42, base loss: 15344.20
[INFO 2017-06-29 17:30:12,270 main.py:57] epoch 436, training loss: 6920.98, average training loss: 9328.90, base loss: 15338.85
[INFO 2017-06-29 17:30:15,211 main.py:57] epoch 437, training loss: 8672.68, average training loss: 9327.40, base loss: 15338.32
[INFO 2017-06-29 17:30:18,282 main.py:57] epoch 438, training loss: 9075.28, average training loss: 9326.83, base loss: 15340.95
[INFO 2017-06-29 17:30:21,494 main.py:57] epoch 439, training loss: 7535.87, average training loss: 9322.76, base loss: 15338.53
[INFO 2017-06-29 17:30:24,575 main.py:57] epoch 440, training loss: 9774.17, average training loss: 9323.78, base loss: 15343.28
[INFO 2017-06-29 17:30:27,563 main.py:57] epoch 441, training loss: 8617.96, average training loss: 9322.18, base loss: 15345.13
[INFO 2017-06-29 17:30:30,521 main.py:57] epoch 442, training loss: 7771.99, average training loss: 9318.69, base loss: 15340.45
[INFO 2017-06-29 17:30:33,586 main.py:57] epoch 443, training loss: 8988.97, average training loss: 9317.94, base loss: 15342.35
[INFO 2017-06-29 17:30:36,575 main.py:57] epoch 444, training loss: 8419.04, average training loss: 9315.92, base loss: 15345.70
[INFO 2017-06-29 17:30:39,569 main.py:57] epoch 445, training loss: 8432.29, average training loss: 9313.94, base loss: 15345.27
[INFO 2017-06-29 17:30:42,477 main.py:57] epoch 446, training loss: 8322.74, average training loss: 9311.72, base loss: 15347.56
[INFO 2017-06-29 17:30:45,480 main.py:57] epoch 447, training loss: 8656.82, average training loss: 9310.26, base loss: 15350.97
[INFO 2017-06-29 17:30:48,537 main.py:57] epoch 448, training loss: 7787.34, average training loss: 9306.87, base loss: 15348.44
[INFO 2017-06-29 17:30:51,538 main.py:57] epoch 449, training loss: 7937.22, average training loss: 9303.83, base loss: 15345.65
[INFO 2017-06-29 17:30:54,570 main.py:57] epoch 450, training loss: 7327.99, average training loss: 9299.45, base loss: 15344.44
[INFO 2017-06-29 17:30:57,519 main.py:57] epoch 451, training loss: 8651.26, average training loss: 9298.01, base loss: 15345.07
[INFO 2017-06-29 17:31:00,641 main.py:57] epoch 452, training loss: 8220.43, average training loss: 9295.63, base loss: 15344.59
[INFO 2017-06-29 17:31:03,673 main.py:57] epoch 453, training loss: 8306.45, average training loss: 9293.45, base loss: 15346.36
[INFO 2017-06-29 17:31:06,706 main.py:57] epoch 454, training loss: 7701.37, average training loss: 9289.96, base loss: 15345.04
[INFO 2017-06-29 17:31:09,640 main.py:57] epoch 455, training loss: 7819.32, average training loss: 9286.73, base loss: 15345.93
[INFO 2017-06-29 17:31:12,736 main.py:57] epoch 456, training loss: 7647.50, average training loss: 9283.14, base loss: 15343.39
[INFO 2017-06-29 17:31:15,687 main.py:57] epoch 457, training loss: 7891.25, average training loss: 9280.10, base loss: 15341.00
[INFO 2017-06-29 17:31:18,735 main.py:57] epoch 458, training loss: 9119.68, average training loss: 9279.75, base loss: 15344.80
[INFO 2017-06-29 17:31:21,729 main.py:57] epoch 459, training loss: 7904.38, average training loss: 9276.76, base loss: 15345.96
[INFO 2017-06-29 17:31:24,703 main.py:57] epoch 460, training loss: 7652.92, average training loss: 9273.24, base loss: 15343.85
[INFO 2017-06-29 17:31:27,712 main.py:57] epoch 461, training loss: 7814.06, average training loss: 9270.08, base loss: 15343.78
[INFO 2017-06-29 17:31:30,737 main.py:57] epoch 462, training loss: 8525.58, average training loss: 9268.48, base loss: 15345.94
[INFO 2017-06-29 17:31:33,781 main.py:57] epoch 463, training loss: 8726.32, average training loss: 9267.31, base loss: 15348.38
[INFO 2017-06-29 17:31:36,825 main.py:57] epoch 464, training loss: 7792.30, average training loss: 9264.14, base loss: 15346.63
[INFO 2017-06-29 17:31:39,869 main.py:57] epoch 465, training loss: 7795.78, average training loss: 9260.98, base loss: 15345.29
[INFO 2017-06-29 17:31:42,879 main.py:57] epoch 466, training loss: 7788.90, average training loss: 9257.83, base loss: 15345.72
[INFO 2017-06-29 17:31:45,851 main.py:57] epoch 467, training loss: 7519.74, average training loss: 9254.12, base loss: 15344.77
[INFO 2017-06-29 17:31:48,872 main.py:57] epoch 468, training loss: 8265.47, average training loss: 9252.01, base loss: 15345.44
[INFO 2017-06-29 17:31:51,844 main.py:57] epoch 469, training loss: 7588.09, average training loss: 9248.47, base loss: 15344.99
[INFO 2017-06-29 17:31:54,862 main.py:57] epoch 470, training loss: 8357.86, average training loss: 9246.58, base loss: 15346.29
[INFO 2017-06-29 17:31:57,834 main.py:57] epoch 471, training loss: 7715.59, average training loss: 9243.34, base loss: 15345.13
[INFO 2017-06-29 17:32:00,827 main.py:57] epoch 472, training loss: 8660.06, average training loss: 9242.10, base loss: 15346.35
[INFO 2017-06-29 17:32:03,872 main.py:57] epoch 473, training loss: 7533.46, average training loss: 9238.50, base loss: 15342.22
[INFO 2017-06-29 17:32:06,824 main.py:57] epoch 474, training loss: 9969.25, average training loss: 9240.04, base loss: 15349.46
[INFO 2017-06-29 17:32:09,851 main.py:57] epoch 475, training loss: 7999.69, average training loss: 9237.43, base loss: 15350.01
[INFO 2017-06-29 17:32:12,753 main.py:57] epoch 476, training loss: 7697.32, average training loss: 9234.20, base loss: 15350.29
[INFO 2017-06-29 17:32:15,719 main.py:57] epoch 477, training loss: 8203.73, average training loss: 9232.05, base loss: 15352.49
[INFO 2017-06-29 17:32:18,633 main.py:57] epoch 478, training loss: 7487.29, average training loss: 9228.40, base loss: 15351.74
[INFO 2017-06-29 17:32:21,594 main.py:57] epoch 479, training loss: 8434.26, average training loss: 9226.75, base loss: 15356.34
[INFO 2017-06-29 17:32:24,609 main.py:57] epoch 480, training loss: 8688.50, average training loss: 9225.63, base loss: 15360.01
[INFO 2017-06-29 17:32:27,689 main.py:57] epoch 481, training loss: 6979.98, average training loss: 9220.97, base loss: 15356.73
[INFO 2017-06-29 17:32:30,606 main.py:57] epoch 482, training loss: 7947.02, average training loss: 9218.33, base loss: 15357.28
[INFO 2017-06-29 17:32:33,630 main.py:57] epoch 483, training loss: 7634.33, average training loss: 9215.06, base loss: 15355.63
[INFO 2017-06-29 17:32:36,646 main.py:57] epoch 484, training loss: 7754.32, average training loss: 9212.05, base loss: 15355.25
[INFO 2017-06-29 17:32:39,722 main.py:57] epoch 485, training loss: 9198.23, average training loss: 9212.02, base loss: 15356.77
[INFO 2017-06-29 17:32:42,738 main.py:57] epoch 486, training loss: 8211.00, average training loss: 9209.96, base loss: 15357.52
[INFO 2017-06-29 17:32:45,731 main.py:57] epoch 487, training loss: 8808.71, average training loss: 9209.14, base loss: 15360.03
[INFO 2017-06-29 17:32:48,705 main.py:57] epoch 488, training loss: 8222.62, average training loss: 9207.12, base loss: 15359.80
[INFO 2017-06-29 17:32:51,744 main.py:57] epoch 489, training loss: 8150.66, average training loss: 9204.97, base loss: 15359.61
[INFO 2017-06-29 17:32:54,735 main.py:57] epoch 490, training loss: 7290.91, average training loss: 9201.07, base loss: 15357.27
[INFO 2017-06-29 17:32:57,721 main.py:57] epoch 491, training loss: 9120.16, average training loss: 9200.91, base loss: 15361.26
[INFO 2017-06-29 17:33:00,818 main.py:57] epoch 492, training loss: 7544.29, average training loss: 9197.55, base loss: 15358.90
[INFO 2017-06-29 17:33:03,905 main.py:57] epoch 493, training loss: 8196.60, average training loss: 9195.52, base loss: 15362.25
[INFO 2017-06-29 17:33:06,818 main.py:57] epoch 494, training loss: 9051.15, average training loss: 9195.23, base loss: 15366.93
[INFO 2017-06-29 17:33:09,780 main.py:57] epoch 495, training loss: 8078.62, average training loss: 9192.98, base loss: 15367.02
[INFO 2017-06-29 17:33:12,755 main.py:57] epoch 496, training loss: 9753.43, average training loss: 9194.10, base loss: 15373.78
[INFO 2017-06-29 17:33:15,793 main.py:57] epoch 497, training loss: 7985.74, average training loss: 9191.68, base loss: 15372.91
[INFO 2017-06-29 17:33:18,823 main.py:57] epoch 498, training loss: 9091.10, average training loss: 9191.48, base loss: 15374.16
[INFO 2017-06-29 17:33:21,770 main.py:57] epoch 499, training loss: 8149.49, average training loss: 9189.39, base loss: 15376.75
[INFO 2017-06-29 17:33:21,771 main.py:59] epoch 499, testing
[INFO 2017-06-29 17:33:34,261 main.py:104] average testing loss: 8295.90, base loss: 15591.30
[INFO 2017-06-29 17:33:34,262 main.py:105] improve_loss: 7295.40, improve_percent: 0.47
[INFO 2017-06-29 17:33:34,263 main.py:71] current best improved percent: 0.47
[INFO 2017-06-29 17:33:37,242 main.py:57] epoch 500, training loss: 8599.67, average training loss: 9188.22, base loss: 15376.73
[INFO 2017-06-29 17:33:40,262 main.py:57] epoch 501, training loss: 7779.34, average training loss: 9185.41, base loss: 15376.94
[INFO 2017-06-29 17:33:43,276 main.py:57] epoch 502, training loss: 7784.00, average training loss: 9182.62, base loss: 15376.67
[INFO 2017-06-29 17:33:46,322 main.py:57] epoch 503, training loss: 8143.55, average training loss: 9180.56, base loss: 15378.03
[INFO 2017-06-29 17:33:49,295 main.py:57] epoch 504, training loss: 8360.94, average training loss: 9178.94, base loss: 15378.24
[INFO 2017-06-29 17:33:52,316 main.py:57] epoch 505, training loss: 9055.81, average training loss: 9178.69, base loss: 15380.98
[INFO 2017-06-29 17:33:55,314 main.py:57] epoch 506, training loss: 7893.64, average training loss: 9176.16, base loss: 15380.55
[INFO 2017-06-29 17:33:58,296 main.py:57] epoch 507, training loss: 9722.45, average training loss: 9177.24, base loss: 15385.98
[INFO 2017-06-29 17:34:01,306 main.py:57] epoch 508, training loss: 8487.52, average training loss: 9175.88, base loss: 15384.53
[INFO 2017-06-29 17:34:04,276 main.py:57] epoch 509, training loss: 7859.11, average training loss: 9173.30, base loss: 15378.35
[INFO 2017-06-29 17:34:07,281 main.py:57] epoch 510, training loss: 8143.81, average training loss: 9171.28, base loss: 15380.76
[INFO 2017-06-29 17:34:10,289 main.py:57] epoch 511, training loss: 7404.70, average training loss: 9167.83, base loss: 15380.24
[INFO 2017-06-29 17:34:13,369 main.py:57] epoch 512, training loss: 7690.02, average training loss: 9164.95, base loss: 15378.96
[INFO 2017-06-29 17:34:16,265 main.py:57] epoch 513, training loss: 7640.10, average training loss: 9161.99, base loss: 15379.13
[INFO 2017-06-29 17:34:19,235 main.py:57] epoch 514, training loss: 8367.86, average training loss: 9160.44, base loss: 15381.09
[INFO 2017-06-29 17:34:22,202 main.py:57] epoch 515, training loss: 8814.10, average training loss: 9159.77, base loss: 15383.48
[INFO 2017-06-29 17:34:25,140 main.py:57] epoch 516, training loss: 8530.00, average training loss: 9158.55, base loss: 15382.78
[INFO 2017-06-29 17:34:28,103 main.py:57] epoch 517, training loss: 7798.83, average training loss: 9155.93, base loss: 15380.36
[INFO 2017-06-29 17:34:31,030 main.py:57] epoch 518, training loss: 8188.45, average training loss: 9154.07, base loss: 15382.71
[INFO 2017-06-29 17:34:34,034 main.py:57] epoch 519, training loss: 7387.51, average training loss: 9150.67, base loss: 15380.11
[INFO 2017-06-29 17:34:37,002 main.py:57] epoch 520, training loss: 7247.82, average training loss: 9147.02, base loss: 15379.30
[INFO 2017-06-29 17:34:39,950 main.py:57] epoch 521, training loss: 8788.52, average training loss: 9146.33, base loss: 15383.58
[INFO 2017-06-29 17:34:42,919 main.py:57] epoch 522, training loss: 7569.82, average training loss: 9143.32, base loss: 15382.09
[INFO 2017-06-29 17:34:45,876 main.py:57] epoch 523, training loss: 7505.58, average training loss: 9140.19, base loss: 15383.60
[INFO 2017-06-29 17:34:48,883 main.py:57] epoch 524, training loss: 8960.56, average training loss: 9139.85, base loss: 15389.21
[INFO 2017-06-29 17:34:51,897 main.py:57] epoch 525, training loss: 7100.96, average training loss: 9135.97, base loss: 15388.04
[INFO 2017-06-29 17:34:54,908 main.py:57] epoch 526, training loss: 7904.99, average training loss: 9133.64, base loss: 15390.16
[INFO 2017-06-29 17:34:57,877 main.py:57] epoch 527, training loss: 8499.81, average training loss: 9132.44, base loss: 15392.11
[INFO 2017-06-29 17:35:00,895 main.py:57] epoch 528, training loss: 7948.16, average training loss: 9130.20, base loss: 15394.74
[INFO 2017-06-29 17:35:03,860 main.py:57] epoch 529, training loss: 7500.00, average training loss: 9127.12, base loss: 15394.42
[INFO 2017-06-29 17:35:06,791 main.py:57] epoch 530, training loss: 7090.18, average training loss: 9123.28, base loss: 15393.48
[INFO 2017-06-29 17:35:09,717 main.py:57] epoch 531, training loss: 7319.78, average training loss: 9119.89, base loss: 15394.56
[INFO 2017-06-29 17:35:12,713 main.py:57] epoch 532, training loss: 8177.88, average training loss: 9118.13, base loss: 15397.45
[INFO 2017-06-29 17:35:15,726 main.py:57] epoch 533, training loss: 8210.88, average training loss: 9116.43, base loss: 15397.06
[INFO 2017-06-29 17:35:18,766 main.py:57] epoch 534, training loss: 7598.04, average training loss: 9113.59, base loss: 15395.67
[INFO 2017-06-29 17:35:21,697 main.py:57] epoch 535, training loss: 7472.22, average training loss: 9110.53, base loss: 15393.61
[INFO 2017-06-29 17:35:24,627 main.py:57] epoch 536, training loss: 7570.09, average training loss: 9107.66, base loss: 15391.99
[INFO 2017-06-29 17:35:27,602 main.py:57] epoch 537, training loss: 8806.74, average training loss: 9107.10, base loss: 15395.42
[INFO 2017-06-29 17:35:30,632 main.py:57] epoch 538, training loss: 8641.75, average training loss: 9106.24, base loss: 15398.02
[INFO 2017-06-29 17:35:33,638 main.py:57] epoch 539, training loss: 8505.64, average training loss: 9105.12, base loss: 15400.62
[INFO 2017-06-29 17:35:36,717 main.py:57] epoch 540, training loss: 8589.53, average training loss: 9104.17, base loss: 15403.65
[INFO 2017-06-29 17:35:39,712 main.py:57] epoch 541, training loss: 9265.01, average training loss: 9104.47, base loss: 15410.35
[INFO 2017-06-29 17:35:42,678 main.py:57] epoch 542, training loss: 7571.17, average training loss: 9101.64, base loss: 15407.45
[INFO 2017-06-29 17:35:45,587 main.py:57] epoch 543, training loss: 8072.80, average training loss: 9099.75, base loss: 15408.51
[INFO 2017-06-29 17:35:48,592 main.py:57] epoch 544, training loss: 7872.81, average training loss: 9097.50, base loss: 15408.89
[INFO 2017-06-29 17:35:51,522 main.py:57] epoch 545, training loss: 8278.08, average training loss: 9096.00, base loss: 15408.55
[INFO 2017-06-29 17:35:54,479 main.py:57] epoch 546, training loss: 7934.17, average training loss: 9093.88, base loss: 15408.58
[INFO 2017-06-29 17:35:57,562 main.py:57] epoch 547, training loss: 8403.51, average training loss: 9092.62, base loss: 15410.80
[INFO 2017-06-29 17:36:00,753 main.py:57] epoch 548, training loss: 7403.07, average training loss: 9089.54, base loss: 15410.66
[INFO 2017-06-29 17:36:03,752 main.py:57] epoch 549, training loss: 8606.39, average training loss: 9088.66, base loss: 15413.00
[INFO 2017-06-29 17:36:06,749 main.py:57] epoch 550, training loss: 8243.26, average training loss: 9087.13, base loss: 15414.54
[INFO 2017-06-29 17:36:09,904 main.py:57] epoch 551, training loss: 8807.35, average training loss: 9086.62, base loss: 15417.06
[INFO 2017-06-29 17:36:12,868 main.py:57] epoch 552, training loss: 8226.40, average training loss: 9085.06, base loss: 15418.13
[INFO 2017-06-29 17:36:15,888 main.py:57] epoch 553, training loss: 8850.74, average training loss: 9084.64, base loss: 15417.61
[INFO 2017-06-29 17:36:18,907 main.py:57] epoch 554, training loss: 8387.48, average training loss: 9083.39, base loss: 15416.60
[INFO 2017-06-29 17:36:21,877 main.py:57] epoch 555, training loss: 9246.99, average training loss: 9083.68, base loss: 15419.04
[INFO 2017-06-29 17:36:24,924 main.py:57] epoch 556, training loss: 8884.06, average training loss: 9083.32, base loss: 15420.41
[INFO 2017-06-29 17:36:27,852 main.py:57] epoch 557, training loss: 7967.82, average training loss: 9081.32, base loss: 15419.68
[INFO 2017-06-29 17:36:30,821 main.py:57] epoch 558, training loss: 7488.29, average training loss: 9078.47, base loss: 15418.77
[INFO 2017-06-29 17:36:33,801 main.py:57] epoch 559, training loss: 9233.21, average training loss: 9078.75, base loss: 15422.87
[INFO 2017-06-29 17:36:36,779 main.py:57] epoch 560, training loss: 8398.12, average training loss: 9077.54, base loss: 15426.03
[INFO 2017-06-29 17:36:39,743 main.py:57] epoch 561, training loss: 8280.59, average training loss: 9076.12, base loss: 15425.80
[INFO 2017-06-29 17:36:42,728 main.py:57] epoch 562, training loss: 8330.74, average training loss: 9074.79, base loss: 15427.72
[INFO 2017-06-29 17:36:45,706 main.py:57] epoch 563, training loss: 7339.06, average training loss: 9071.72, base loss: 15425.55
[INFO 2017-06-29 17:36:48,681 main.py:57] epoch 564, training loss: 8366.10, average training loss: 9070.47, base loss: 15425.36
[INFO 2017-06-29 17:36:51,660 main.py:57] epoch 565, training loss: 8327.88, average training loss: 9069.15, base loss: 15426.84
[INFO 2017-06-29 17:36:54,587 main.py:57] epoch 566, training loss: 8496.31, average training loss: 9068.14, base loss: 15427.22
[INFO 2017-06-29 17:36:57,523 main.py:57] epoch 567, training loss: 8206.74, average training loss: 9066.63, base loss: 15427.56
[INFO 2017-06-29 17:37:00,490 main.py:57] epoch 568, training loss: 8664.46, average training loss: 9065.92, base loss: 15428.71
[INFO 2017-06-29 17:37:03,485 main.py:57] epoch 569, training loss: 8069.53, average training loss: 9064.17, base loss: 15428.85
[INFO 2017-06-29 17:37:06,430 main.py:57] epoch 570, training loss: 7953.94, average training loss: 9062.23, base loss: 15427.98
[INFO 2017-06-29 17:37:09,408 main.py:57] epoch 571, training loss: 7877.24, average training loss: 9060.16, base loss: 15425.66
[INFO 2017-06-29 17:37:12,420 main.py:57] epoch 572, training loss: 8147.61, average training loss: 9058.56, base loss: 15425.66
[INFO 2017-06-29 17:37:15,462 main.py:57] epoch 573, training loss: 8108.90, average training loss: 9056.91, base loss: 15424.64
[INFO 2017-06-29 17:37:18,409 main.py:57] epoch 574, training loss: 7726.30, average training loss: 9054.60, base loss: 15423.78
[INFO 2017-06-29 17:37:21,349 main.py:57] epoch 575, training loss: 7172.45, average training loss: 9051.33, base loss: 15420.46
[INFO 2017-06-29 17:37:24,302 main.py:57] epoch 576, training loss: 7993.53, average training loss: 9049.50, base loss: 15419.62
[INFO 2017-06-29 17:37:27,272 main.py:57] epoch 577, training loss: 7005.42, average training loss: 9045.96, base loss: 15416.34
[INFO 2017-06-29 17:37:30,334 main.py:57] epoch 578, training loss: 7003.78, average training loss: 9042.43, base loss: 15412.26
[INFO 2017-06-29 17:37:33,414 main.py:57] epoch 579, training loss: 9042.90, average training loss: 9042.43, base loss: 15415.00
[INFO 2017-06-29 17:37:36,359 main.py:57] epoch 580, training loss: 9333.63, average training loss: 9042.93, base loss: 15418.39
[INFO 2017-06-29 17:37:39,334 main.py:57] epoch 581, training loss: 8695.14, average training loss: 9042.34, base loss: 15418.08
[INFO 2017-06-29 17:37:42,258 main.py:57] epoch 582, training loss: 7564.50, average training loss: 9039.80, base loss: 15415.93
[INFO 2017-06-29 17:37:45,203 main.py:57] epoch 583, training loss: 8561.51, average training loss: 9038.98, base loss: 15416.99
[INFO 2017-06-29 17:37:48,290 main.py:57] epoch 584, training loss: 8446.07, average training loss: 9037.97, base loss: 15419.64
[INFO 2017-06-29 17:37:51,296 main.py:57] epoch 585, training loss: 7558.15, average training loss: 9035.44, base loss: 15417.16
[INFO 2017-06-29 17:37:54,232 main.py:57] epoch 586, training loss: 8045.19, average training loss: 9033.76, base loss: 15420.57
[INFO 2017-06-29 17:37:57,179 main.py:57] epoch 587, training loss: 8672.23, average training loss: 9033.14, base loss: 15425.63
[INFO 2017-06-29 17:38:00,216 main.py:57] epoch 588, training loss: 8289.36, average training loss: 9031.88, base loss: 15426.17
[INFO 2017-06-29 17:38:03,188 main.py:57] epoch 589, training loss: 8089.25, average training loss: 9030.28, base loss: 15427.16
[INFO 2017-06-29 17:38:06,179 main.py:57] epoch 590, training loss: 8268.71, average training loss: 9028.99, base loss: 15426.87
[INFO 2017-06-29 17:38:09,194 main.py:57] epoch 591, training loss: 7416.77, average training loss: 9026.27, base loss: 15424.47
[INFO 2017-06-29 17:38:12,106 main.py:57] epoch 592, training loss: 8118.96, average training loss: 9024.74, base loss: 15424.66
[INFO 2017-06-29 17:38:15,081 main.py:57] epoch 593, training loss: 7973.89, average training loss: 9022.97, base loss: 15424.83
[INFO 2017-06-29 17:38:18,045 main.py:57] epoch 594, training loss: 9120.50, average training loss: 9023.13, base loss: 15427.13
[INFO 2017-06-29 17:38:21,074 main.py:57] epoch 595, training loss: 8642.69, average training loss: 9022.50, base loss: 15427.47
[INFO 2017-06-29 17:38:23,988 main.py:57] epoch 596, training loss: 7594.81, average training loss: 9020.10, base loss: 15427.73
[INFO 2017-06-29 17:38:27,008 main.py:57] epoch 597, training loss: 7676.24, average training loss: 9017.86, base loss: 15427.97
[INFO 2017-06-29 17:38:29,930 main.py:57] epoch 598, training loss: 8244.88, average training loss: 9016.57, base loss: 15427.71
[INFO 2017-06-29 17:38:32,919 main.py:57] epoch 599, training loss: 8636.54, average training loss: 9015.93, base loss: 15431.26
[INFO 2017-06-29 17:38:32,919 main.py:59] epoch 599, testing
[INFO 2017-06-29 17:38:45,296 main.py:104] average testing loss: 8282.62, base loss: 15790.82
[INFO 2017-06-29 17:38:45,296 main.py:105] improve_loss: 7508.20, improve_percent: 0.48
[INFO 2017-06-29 17:38:45,298 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:38:45,325 main.py:71] current best improved percent: 0.48
[INFO 2017-06-29 17:38:48,356 main.py:57] epoch 600, training loss: 8454.39, average training loss: 9015.00, base loss: 15433.06
[INFO 2017-06-29 17:38:51,254 main.py:57] epoch 601, training loss: 7335.86, average training loss: 9012.21, base loss: 15429.02
[INFO 2017-06-29 17:38:54,266 main.py:57] epoch 602, training loss: 7711.53, average training loss: 9010.05, base loss: 15430.06
[INFO 2017-06-29 17:38:57,414 main.py:57] epoch 603, training loss: 7875.27, average training loss: 9008.17, base loss: 15429.92
[INFO 2017-06-29 17:39:00,359 main.py:57] epoch 604, training loss: 8166.07, average training loss: 9006.78, base loss: 15428.58
[INFO 2017-06-29 17:39:03,341 main.py:57] epoch 605, training loss: 7608.17, average training loss: 9004.47, base loss: 15427.71
[INFO 2017-06-29 17:39:06,344 main.py:57] epoch 606, training loss: 7535.44, average training loss: 9002.05, base loss: 15427.96
[INFO 2017-06-29 17:39:09,273 main.py:57] epoch 607, training loss: 7476.75, average training loss: 8999.54, base loss: 15427.76
[INFO 2017-06-29 17:39:12,338 main.py:57] epoch 608, training loss: 7852.31, average training loss: 8997.66, base loss: 15422.24
[INFO 2017-06-29 17:39:15,332 main.py:57] epoch 609, training loss: 7361.50, average training loss: 8994.98, base loss: 15417.63
[INFO 2017-06-29 17:39:18,325 main.py:57] epoch 610, training loss: 7642.04, average training loss: 8992.76, base loss: 15417.69
[INFO 2017-06-29 17:39:21,296 main.py:57] epoch 611, training loss: 9387.72, average training loss: 8993.41, base loss: 15422.38
[INFO 2017-06-29 17:39:24,273 main.py:57] epoch 612, training loss: 7286.12, average training loss: 8990.62, base loss: 15420.82
[INFO 2017-06-29 17:39:27,204 main.py:57] epoch 613, training loss: 8694.22, average training loss: 8990.14, base loss: 15421.38
[INFO 2017-06-29 17:39:30,183 main.py:57] epoch 614, training loss: 8717.31, average training loss: 8989.70, base loss: 15421.03
[INFO 2017-06-29 17:39:33,172 main.py:57] epoch 615, training loss: 7376.67, average training loss: 8987.08, base loss: 15419.99
[INFO 2017-06-29 17:39:36,091 main.py:57] epoch 616, training loss: 7994.59, average training loss: 8985.47, base loss: 15419.78
[INFO 2017-06-29 17:39:39,231 main.py:57] epoch 617, training loss: 7807.86, average training loss: 8983.57, base loss: 15417.43
[INFO 2017-06-29 17:39:42,117 main.py:57] epoch 618, training loss: 6859.80, average training loss: 8980.13, base loss: 15415.23
[INFO 2017-06-29 17:39:45,063 main.py:57] epoch 619, training loss: 8147.15, average training loss: 8978.79, base loss: 15415.42
[INFO 2017-06-29 17:39:48,062 main.py:57] epoch 620, training loss: 7362.21, average training loss: 8976.19, base loss: 15413.88
[INFO 2017-06-29 17:39:50,922 main.py:57] epoch 621, training loss: 7922.39, average training loss: 8974.49, base loss: 15412.35
[INFO 2017-06-29 17:39:53,890 main.py:57] epoch 622, training loss: 8433.58, average training loss: 8973.63, base loss: 15414.00
[INFO 2017-06-29 17:39:56,903 main.py:57] epoch 623, training loss: 7801.83, average training loss: 8971.75, base loss: 15415.25
[INFO 2017-06-29 17:39:59,858 main.py:57] epoch 624, training loss: 7874.95, average training loss: 8969.99, base loss: 15415.28
[INFO 2017-06-29 17:40:02,825 main.py:57] epoch 625, training loss: 7795.79, average training loss: 8968.12, base loss: 15415.36
[INFO 2017-06-29 17:40:05,806 main.py:57] epoch 626, training loss: 7148.28, average training loss: 8965.21, base loss: 15410.90
[INFO 2017-06-29 17:40:08,687 main.py:57] epoch 627, training loss: 10080.99, average training loss: 8966.99, base loss: 15411.71
[INFO 2017-06-29 17:40:11,623 main.py:57] epoch 628, training loss: 7491.28, average training loss: 8964.65, base loss: 15410.32
[INFO 2017-06-29 17:40:14,610 main.py:57] epoch 629, training loss: 7537.39, average training loss: 8962.38, base loss: 15409.64
[INFO 2017-06-29 17:40:17,580 main.py:57] epoch 630, training loss: 7934.95, average training loss: 8960.75, base loss: 15407.97
[INFO 2017-06-29 17:40:20,627 main.py:57] epoch 631, training loss: 7389.51, average training loss: 8958.27, base loss: 15405.77
[INFO 2017-06-29 17:40:23,722 main.py:57] epoch 632, training loss: 6906.65, average training loss: 8955.02, base loss: 15403.74
[INFO 2017-06-29 17:40:26,698 main.py:57] epoch 633, training loss: 7331.25, average training loss: 8952.46, base loss: 15402.23
[INFO 2017-06-29 17:40:29,755 main.py:57] epoch 634, training loss: 7659.27, average training loss: 8950.43, base loss: 15401.92
[INFO 2017-06-29 17:40:32,696 main.py:57] epoch 635, training loss: 7671.43, average training loss: 8948.42, base loss: 15403.16
[INFO 2017-06-29 17:40:35,697 main.py:57] epoch 636, training loss: 6882.33, average training loss: 8945.17, base loss: 15400.17
[INFO 2017-06-29 17:40:38,661 main.py:57] epoch 637, training loss: 7281.21, average training loss: 8942.56, base loss: 15399.01
[INFO 2017-06-29 17:40:41,669 main.py:57] epoch 638, training loss: 9248.90, average training loss: 8943.04, base loss: 15402.58
[INFO 2017-06-29 17:40:44,622 main.py:57] epoch 639, training loss: 7829.54, average training loss: 8941.30, base loss: 15402.42
[INFO 2017-06-29 17:40:47,651 main.py:57] epoch 640, training loss: 8420.47, average training loss: 8940.49, base loss: 15403.62
[INFO 2017-06-29 17:40:50,544 main.py:57] epoch 641, training loss: 7377.56, average training loss: 8938.06, base loss: 15401.49
[INFO 2017-06-29 17:40:53,521 main.py:57] epoch 642, training loss: 7711.47, average training loss: 8936.15, base loss: 15399.45
[INFO 2017-06-29 17:40:56,490 main.py:57] epoch 643, training loss: 7773.53, average training loss: 8934.34, base loss: 15398.64
[INFO 2017-06-29 17:40:59,459 main.py:57] epoch 644, training loss: 8010.18, average training loss: 8932.91, base loss: 15399.88
[INFO 2017-06-29 17:41:02,433 main.py:57] epoch 645, training loss: 9582.89, average training loss: 8933.92, base loss: 15404.77
[INFO 2017-06-29 17:41:05,316 main.py:57] epoch 646, training loss: 7342.86, average training loss: 8931.46, base loss: 15404.23
[INFO 2017-06-29 17:41:08,317 main.py:57] epoch 647, training loss: 8288.68, average training loss: 8930.47, base loss: 15403.41
[INFO 2017-06-29 17:41:11,264 main.py:57] epoch 648, training loss: 7510.44, average training loss: 8928.28, base loss: 15402.43
[INFO 2017-06-29 17:41:14,210 main.py:57] epoch 649, training loss: 8954.40, average training loss: 8928.32, base loss: 15407.78
[INFO 2017-06-29 17:41:17,200 main.py:57] epoch 650, training loss: 7331.69, average training loss: 8925.87, base loss: 15407.21
[INFO 2017-06-29 17:41:20,154 main.py:57] epoch 651, training loss: 8925.91, average training loss: 8925.87, base loss: 15413.17
[INFO 2017-06-29 17:41:23,142 main.py:57] epoch 652, training loss: 7365.09, average training loss: 8923.48, base loss: 15411.44
[INFO 2017-06-29 17:41:26,119 main.py:57] epoch 653, training loss: 7541.47, average training loss: 8921.36, base loss: 15411.14
[INFO 2017-06-29 17:41:29,115 main.py:57] epoch 654, training loss: 8028.17, average training loss: 8920.00, base loss: 15407.67
[INFO 2017-06-29 17:41:32,082 main.py:57] epoch 655, training loss: 7871.07, average training loss: 8918.40, base loss: 15404.16
[INFO 2017-06-29 17:41:35,023 main.py:57] epoch 656, training loss: 7503.95, average training loss: 8916.25, base loss: 15406.90
[INFO 2017-06-29 17:41:38,027 main.py:57] epoch 657, training loss: 8115.41, average training loss: 8915.03, base loss: 15407.98
[INFO 2017-06-29 17:41:41,068 main.py:57] epoch 658, training loss: 7649.35, average training loss: 8913.11, base loss: 15408.70
[INFO 2017-06-29 17:41:43,937 main.py:57] epoch 659, training loss: 7894.64, average training loss: 8911.57, base loss: 15408.80
[INFO 2017-06-29 17:41:46,923 main.py:57] epoch 660, training loss: 7129.72, average training loss: 8908.87, base loss: 15403.53
[INFO 2017-06-29 17:41:49,951 main.py:57] epoch 661, training loss: 8168.80, average training loss: 8907.75, base loss: 15401.53
[INFO 2017-06-29 17:41:52,899 main.py:57] epoch 662, training loss: 7104.51, average training loss: 8905.03, base loss: 15400.49
[INFO 2017-06-29 17:41:55,830 main.py:57] epoch 663, training loss: 7560.70, average training loss: 8903.01, base loss: 15402.13
[INFO 2017-06-29 17:41:58,798 main.py:57] epoch 664, training loss: 7773.84, average training loss: 8901.31, base loss: 15402.73
[INFO 2017-06-29 17:42:01,706 main.py:57] epoch 665, training loss: 8173.55, average training loss: 8900.22, base loss: 15404.17
[INFO 2017-06-29 17:42:04,749 main.py:57] epoch 666, training loss: 7603.02, average training loss: 8898.27, base loss: 15401.38
[INFO 2017-06-29 17:42:07,841 main.py:57] epoch 667, training loss: 7414.57, average training loss: 8896.05, base loss: 15399.36
[INFO 2017-06-29 17:42:10,835 main.py:57] epoch 668, training loss: 8137.78, average training loss: 8894.92, base loss: 15400.12
[INFO 2017-06-29 17:42:13,817 main.py:57] epoch 669, training loss: 7371.00, average training loss: 8892.64, base loss: 15396.69
[INFO 2017-06-29 17:42:16,817 main.py:57] epoch 670, training loss: 7821.10, average training loss: 8891.05, base loss: 15394.62
[INFO 2017-06-29 17:42:19,776 main.py:57] epoch 671, training loss: 7298.48, average training loss: 8888.68, base loss: 15393.36
[INFO 2017-06-29 17:42:22,786 main.py:57] epoch 672, training loss: 7359.20, average training loss: 8886.40, base loss: 15388.70
[INFO 2017-06-29 17:42:25,740 main.py:57] epoch 673, training loss: 7282.51, average training loss: 8884.02, base loss: 15385.32
[INFO 2017-06-29 17:42:28,662 main.py:57] epoch 674, training loss: 8009.92, average training loss: 8882.73, base loss: 15385.17
[INFO 2017-06-29 17:42:31,661 main.py:57] epoch 675, training loss: 7662.88, average training loss: 8880.92, base loss: 15384.34
[INFO 2017-06-29 17:42:34,621 main.py:57] epoch 676, training loss: 7281.79, average training loss: 8878.56, base loss: 15383.58
[INFO 2017-06-29 17:42:37,592 main.py:57] epoch 677, training loss: 7172.81, average training loss: 8876.05, base loss: 15381.61
[INFO 2017-06-29 17:42:40,661 main.py:57] epoch 678, training loss: 7906.05, average training loss: 8874.62, base loss: 15382.30
[INFO 2017-06-29 17:42:43,618 main.py:57] epoch 679, training loss: 7166.91, average training loss: 8872.11, base loss: 15381.43
[INFO 2017-06-29 17:42:46,573 main.py:57] epoch 680, training loss: 8592.70, average training loss: 8871.70, base loss: 15383.61
[INFO 2017-06-29 17:42:49,520 main.py:57] epoch 681, training loss: 7348.10, average training loss: 8869.46, base loss: 15382.90
[INFO 2017-06-29 17:42:52,465 main.py:57] epoch 682, training loss: 7664.75, average training loss: 8867.70, base loss: 15382.79
[INFO 2017-06-29 17:42:55,426 main.py:57] epoch 683, training loss: 7362.76, average training loss: 8865.50, base loss: 15379.52
[INFO 2017-06-29 17:42:58,383 main.py:57] epoch 684, training loss: 7021.33, average training loss: 8862.81, base loss: 15373.50
[INFO 2017-06-29 17:43:01,336 main.py:57] epoch 685, training loss: 8660.35, average training loss: 8862.51, base loss: 15374.68
[INFO 2017-06-29 17:43:04,314 main.py:57] epoch 686, training loss: 8695.94, average training loss: 8862.27, base loss: 15374.68
[INFO 2017-06-29 17:43:07,269 main.py:57] epoch 687, training loss: 7045.72, average training loss: 8859.63, base loss: 15370.29
[INFO 2017-06-29 17:43:10,231 main.py:57] epoch 688, training loss: 7370.38, average training loss: 8857.47, base loss: 15366.98
[INFO 2017-06-29 17:43:13,162 main.py:57] epoch 689, training loss: 8317.37, average training loss: 8856.68, base loss: 15368.38
[INFO 2017-06-29 17:43:16,177 main.py:57] epoch 690, training loss: 7493.43, average training loss: 8854.71, base loss: 15367.37
[INFO 2017-06-29 17:43:19,163 main.py:57] epoch 691, training loss: 7674.90, average training loss: 8853.01, base loss: 15364.46
[INFO 2017-06-29 17:43:22,129 main.py:57] epoch 692, training loss: 7938.48, average training loss: 8851.69, base loss: 15363.58
[INFO 2017-06-29 17:43:25,089 main.py:57] epoch 693, training loss: 7282.34, average training loss: 8849.43, base loss: 15358.98
[INFO 2017-06-29 17:43:28,116 main.py:57] epoch 694, training loss: 8145.93, average training loss: 8848.41, base loss: 15359.53
[INFO 2017-06-29 17:43:31,019 main.py:57] epoch 695, training loss: 7947.73, average training loss: 8847.12, base loss: 15358.22
[INFO 2017-06-29 17:43:34,113 main.py:57] epoch 696, training loss: 8112.35, average training loss: 8846.07, base loss: 15358.18
[INFO 2017-06-29 17:43:37,057 main.py:57] epoch 697, training loss: 7620.19, average training loss: 8844.31, base loss: 15355.63
[INFO 2017-06-29 17:43:40,054 main.py:57] epoch 698, training loss: 6779.54, average training loss: 8841.35, base loss: 15352.53
[INFO 2017-06-29 17:43:42,991 main.py:57] epoch 699, training loss: 7416.06, average training loss: 8839.32, base loss: 15350.42
[INFO 2017-06-29 17:43:42,992 main.py:59] epoch 699, testing
[INFO 2017-06-29 17:43:55,308 main.py:104] average testing loss: 8143.79, base loss: 15574.19
[INFO 2017-06-29 17:43:55,308 main.py:105] improve_loss: 7430.41, improve_percent: 0.48
[INFO 2017-06-29 17:43:55,311 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:43:55,337 main.py:71] current best improved percent: 0.48
[INFO 2017-06-29 17:43:58,285 main.py:57] epoch 700, training loss: 7303.38, average training loss: 8837.13, base loss: 15349.28
[INFO 2017-06-29 17:44:01,238 main.py:57] epoch 701, training loss: 8536.42, average training loss: 8836.70, base loss: 15353.64
[INFO 2017-06-29 17:44:04,186 main.py:57] epoch 702, training loss: 8296.92, average training loss: 8835.93, base loss: 15355.40
[INFO 2017-06-29 17:44:07,129 main.py:57] epoch 703, training loss: 8093.99, average training loss: 8834.88, base loss: 15357.72
[INFO 2017-06-29 17:44:10,097 main.py:57] epoch 704, training loss: 7746.62, average training loss: 8833.33, base loss: 15359.35
[INFO 2017-06-29 17:44:13,079 main.py:57] epoch 705, training loss: 8522.09, average training loss: 8832.89, base loss: 15363.99
[INFO 2017-06-29 17:44:16,122 main.py:57] epoch 706, training loss: 7852.87, average training loss: 8831.51, base loss: 15363.81
[INFO 2017-06-29 17:44:19,064 main.py:57] epoch 707, training loss: 7269.94, average training loss: 8829.30, base loss: 15362.92
[INFO 2017-06-29 17:44:22,066 main.py:57] epoch 708, training loss: 8097.90, average training loss: 8828.27, base loss: 15363.38
[INFO 2017-06-29 17:44:25,006 main.py:57] epoch 709, training loss: 7078.92, average training loss: 8825.81, base loss: 15361.23
[INFO 2017-06-29 17:44:28,022 main.py:57] epoch 710, training loss: 8038.15, average training loss: 8824.70, base loss: 15361.62
[INFO 2017-06-29 17:44:31,098 main.py:57] epoch 711, training loss: 8819.47, average training loss: 8824.69, base loss: 15362.64
[INFO 2017-06-29 17:44:34,092 main.py:57] epoch 712, training loss: 7270.25, average training loss: 8822.51, base loss: 15362.01
[INFO 2017-06-29 17:44:36,965 main.py:57] epoch 713, training loss: 8250.05, average training loss: 8821.71, base loss: 15362.35
[INFO 2017-06-29 17:44:39,943 main.py:57] epoch 714, training loss: 8900.76, average training loss: 8821.82, base loss: 15361.26
[INFO 2017-06-29 17:44:42,971 main.py:57] epoch 715, training loss: 8915.75, average training loss: 8821.95, base loss: 15359.68
[INFO 2017-06-29 17:44:45,973 main.py:57] epoch 716, training loss: 7789.24, average training loss: 8820.51, base loss: 15357.73
[INFO 2017-06-29 17:44:48,963 main.py:57] epoch 717, training loss: 7854.46, average training loss: 8819.16, base loss: 15356.77
[INFO 2017-06-29 17:44:51,858 main.py:57] epoch 718, training loss: 8271.68, average training loss: 8818.40, base loss: 15357.86
[INFO 2017-06-29 17:44:54,879 main.py:57] epoch 719, training loss: 7392.37, average training loss: 8816.42, base loss: 15356.11
[INFO 2017-06-29 17:44:57,812 main.py:57] epoch 720, training loss: 8355.95, average training loss: 8815.78, base loss: 15356.88
[INFO 2017-06-29 17:45:00,850 main.py:57] epoch 721, training loss: 8630.03, average training loss: 8815.53, base loss: 15357.32
[INFO 2017-06-29 17:45:03,883 main.py:57] epoch 722, training loss: 8123.76, average training loss: 8814.57, base loss: 15358.42
[INFO 2017-06-29 17:45:06,936 main.py:57] epoch 723, training loss: 7470.87, average training loss: 8812.71, base loss: 15356.57
[INFO 2017-06-29 17:45:09,998 main.py:57] epoch 724, training loss: 9175.25, average training loss: 8813.21, base loss: 15358.70
[INFO 2017-06-29 17:45:12,942 main.py:57] epoch 725, training loss: 8856.88, average training loss: 8813.27, base loss: 15359.91
[INFO 2017-06-29 17:45:15,892 main.py:57] epoch 726, training loss: 7090.54, average training loss: 8810.90, base loss: 15357.01
[INFO 2017-06-29 17:45:18,830 main.py:57] epoch 727, training loss: 8802.72, average training loss: 8810.89, base loss: 15359.23
[INFO 2017-06-29 17:45:21,810 main.py:57] epoch 728, training loss: 7069.04, average training loss: 8808.50, base loss: 15356.80
[INFO 2017-06-29 17:45:24,718 main.py:57] epoch 729, training loss: 8550.66, average training loss: 8808.15, base loss: 15356.98
[INFO 2017-06-29 17:45:27,714 main.py:57] epoch 730, training loss: 7329.44, average training loss: 8806.13, base loss: 15355.04
[INFO 2017-06-29 17:45:30,642 main.py:57] epoch 731, training loss: 8218.95, average training loss: 8805.33, base loss: 15356.87
[INFO 2017-06-29 17:45:33,592 main.py:57] epoch 732, training loss: 7823.39, average training loss: 8803.99, base loss: 15358.91
[INFO 2017-06-29 17:45:36,569 main.py:57] epoch 733, training loss: 8827.44, average training loss: 8804.02, base loss: 15364.39
[INFO 2017-06-29 17:45:39,676 main.py:57] epoch 734, training loss: 7438.94, average training loss: 8802.16, base loss: 15362.56
[INFO 2017-06-29 17:45:42,714 main.py:57] epoch 735, training loss: 8915.19, average training loss: 8802.31, base loss: 15367.29
[INFO 2017-06-29 17:45:45,635 main.py:57] epoch 736, training loss: 7108.86, average training loss: 8800.02, base loss: 15364.68
[INFO 2017-06-29 17:45:48,611 main.py:57] epoch 737, training loss: 8698.67, average training loss: 8799.88, base loss: 15366.97
[INFO 2017-06-29 17:45:51,580 main.py:57] epoch 738, training loss: 7918.49, average training loss: 8798.69, base loss: 15368.01
[INFO 2017-06-29 17:45:54,602 main.py:57] epoch 739, training loss: 8414.18, average training loss: 8798.17, base loss: 15368.73
[INFO 2017-06-29 17:45:57,533 main.py:57] epoch 740, training loss: 7474.20, average training loss: 8796.38, base loss: 15366.69
[INFO 2017-06-29 17:46:00,511 main.py:57] epoch 741, training loss: 7860.84, average training loss: 8795.12, base loss: 15365.21
[INFO 2017-06-29 17:46:03,460 main.py:57] epoch 742, training loss: 8426.87, average training loss: 8794.62, base loss: 15367.09
[INFO 2017-06-29 17:46:06,548 main.py:57] epoch 743, training loss: 7956.30, average training loss: 8793.50, base loss: 15369.00
[INFO 2017-06-29 17:46:09,586 main.py:57] epoch 744, training loss: 7350.22, average training loss: 8791.56, base loss: 15366.77
[INFO 2017-06-29 17:46:12,567 main.py:57] epoch 745, training loss: 8111.06, average training loss: 8790.65, base loss: 15368.42
[INFO 2017-06-29 17:46:15,525 main.py:57] epoch 746, training loss: 7713.29, average training loss: 8789.21, base loss: 15367.68
[INFO 2017-06-29 17:46:18,564 main.py:57] epoch 747, training loss: 8694.54, average training loss: 8789.08, base loss: 15368.56
[INFO 2017-06-29 17:46:21,572 main.py:57] epoch 748, training loss: 8518.39, average training loss: 8788.72, base loss: 15369.72
[INFO 2017-06-29 17:46:24,478 main.py:57] epoch 749, training loss: 7917.72, average training loss: 8787.56, base loss: 15371.28
[INFO 2017-06-29 17:46:27,447 main.py:57] epoch 750, training loss: 7607.45, average training loss: 8785.98, base loss: 15372.22
[INFO 2017-06-29 17:46:30,439 main.py:57] epoch 751, training loss: 7444.76, average training loss: 8784.20, base loss: 15371.85
[INFO 2017-06-29 17:46:33,397 main.py:57] epoch 752, training loss: 8237.53, average training loss: 8783.48, base loss: 15370.31
[INFO 2017-06-29 17:46:36,279 main.py:57] epoch 753, training loss: 7641.09, average training loss: 8781.96, base loss: 15368.37
[INFO 2017-06-29 17:46:39,252 main.py:57] epoch 754, training loss: 8981.51, average training loss: 8782.22, base loss: 15374.73
[INFO 2017-06-29 17:46:42,223 main.py:57] epoch 755, training loss: 8146.55, average training loss: 8781.38, base loss: 15376.38
[INFO 2017-06-29 17:46:45,159 main.py:57] epoch 756, training loss: 8463.03, average training loss: 8780.96, base loss: 15380.46
[INFO 2017-06-29 17:46:48,079 main.py:57] epoch 757, training loss: 8222.20, average training loss: 8780.23, base loss: 15384.80
[INFO 2017-06-29 17:46:51,076 main.py:57] epoch 758, training loss: 7280.45, average training loss: 8778.25, base loss: 15382.34
[INFO 2017-06-29 17:46:54,084 main.py:57] epoch 759, training loss: 7463.34, average training loss: 8776.52, base loss: 15381.66
[INFO 2017-06-29 17:46:57,095 main.py:57] epoch 760, training loss: 7401.83, average training loss: 8774.71, base loss: 15378.25
[INFO 2017-06-29 17:47:00,054 main.py:57] epoch 761, training loss: 7802.41, average training loss: 8773.44, base loss: 15377.09
[INFO 2017-06-29 17:47:03,072 main.py:57] epoch 762, training loss: 7253.12, average training loss: 8771.44, base loss: 15374.71
[INFO 2017-06-29 17:47:06,006 main.py:57] epoch 763, training loss: 8513.52, average training loss: 8771.11, base loss: 15376.90
[INFO 2017-06-29 17:47:09,001 main.py:57] epoch 764, training loss: 7201.03, average training loss: 8769.05, base loss: 15375.88
[INFO 2017-06-29 17:47:12,009 main.py:57] epoch 765, training loss: 8389.42, average training loss: 8768.56, base loss: 15377.94
[INFO 2017-06-29 17:47:14,991 main.py:57] epoch 766, training loss: 8669.77, average training loss: 8768.43, base loss: 15380.68
[INFO 2017-06-29 17:47:18,004 main.py:57] epoch 767, training loss: 8236.39, average training loss: 8767.74, base loss: 15382.18
[INFO 2017-06-29 17:47:20,995 main.py:57] epoch 768, training loss: 6695.93, average training loss: 8765.04, base loss: 15378.12
[INFO 2017-06-29 17:47:23,909 main.py:57] epoch 769, training loss: 7856.57, average training loss: 8763.86, base loss: 15378.85
[INFO 2017-06-29 17:47:26,870 main.py:57] epoch 770, training loss: 7017.06, average training loss: 8761.60, base loss: 15378.47
[INFO 2017-06-29 17:47:29,885 main.py:57] epoch 771, training loss: 7464.84, average training loss: 8759.92, base loss: 15376.77
[INFO 2017-06-29 17:47:32,869 main.py:57] epoch 772, training loss: 8166.52, average training loss: 8759.15, base loss: 15376.39
[INFO 2017-06-29 17:47:35,830 main.py:57] epoch 773, training loss: 7869.61, average training loss: 8758.00, base loss: 15375.33
[INFO 2017-06-29 17:47:38,774 main.py:57] epoch 774, training loss: 8590.92, average training loss: 8757.79, base loss: 15377.65
[INFO 2017-06-29 17:47:41,697 main.py:57] epoch 775, training loss: 8000.16, average training loss: 8756.81, base loss: 15377.56
[INFO 2017-06-29 17:47:44,675 main.py:57] epoch 776, training loss: 8491.57, average training loss: 8756.47, base loss: 15377.96
[INFO 2017-06-29 17:47:47,670 main.py:57] epoch 777, training loss: 6747.47, average training loss: 8753.89, base loss: 15374.95
[INFO 2017-06-29 17:47:50,618 main.py:57] epoch 778, training loss: 8147.23, average training loss: 8753.11, base loss: 15378.36
[INFO 2017-06-29 17:47:53,710 main.py:57] epoch 779, training loss: 7530.57, average training loss: 8751.54, base loss: 15381.33
[INFO 2017-06-29 17:47:56,692 main.py:57] epoch 780, training loss: 7583.89, average training loss: 8750.04, base loss: 15381.06
[INFO 2017-06-29 17:47:59,629 main.py:57] epoch 781, training loss: 8554.45, average training loss: 8749.79, base loss: 15384.59
[INFO 2017-06-29 17:48:02,564 main.py:57] epoch 782, training loss: 7712.93, average training loss: 8748.47, base loss: 15385.07
[INFO 2017-06-29 17:48:05,510 main.py:57] epoch 783, training loss: 8988.09, average training loss: 8748.78, base loss: 15388.13
[INFO 2017-06-29 17:48:08,459 main.py:57] epoch 784, training loss: 8083.37, average training loss: 8747.93, base loss: 15386.62
[INFO 2017-06-29 17:48:11,403 main.py:57] epoch 785, training loss: 7190.43, average training loss: 8745.95, base loss: 15384.83
[INFO 2017-06-29 17:48:14,440 main.py:57] epoch 786, training loss: 7895.70, average training loss: 8744.87, base loss: 15386.84
[INFO 2017-06-29 17:48:17,417 main.py:57] epoch 787, training loss: 8447.04, average training loss: 8744.49, base loss: 15388.27
[INFO 2017-06-29 17:48:20,447 main.py:57] epoch 788, training loss: 7984.41, average training loss: 8743.53, base loss: 15388.20
[INFO 2017-06-29 17:48:23,435 main.py:57] epoch 789, training loss: 8106.23, average training loss: 8742.72, base loss: 15387.84
[INFO 2017-06-29 17:48:26,428 main.py:57] epoch 790, training loss: 7234.14, average training loss: 8740.81, base loss: 15388.18
[INFO 2017-06-29 17:48:29,458 main.py:57] epoch 791, training loss: 7716.77, average training loss: 8739.52, base loss: 15387.93
[INFO 2017-06-29 17:48:32,439 main.py:57] epoch 792, training loss: 7543.97, average training loss: 8738.01, base loss: 15385.85
[INFO 2017-06-29 17:48:35,473 main.py:57] epoch 793, training loss: 7154.54, average training loss: 8736.02, base loss: 15384.60
[INFO 2017-06-29 17:48:38,398 main.py:57] epoch 794, training loss: 9146.21, average training loss: 8736.53, base loss: 15385.97
[INFO 2017-06-29 17:48:41,382 main.py:57] epoch 795, training loss: 8210.21, average training loss: 8735.87, base loss: 15387.12
[INFO 2017-06-29 17:48:44,414 main.py:57] epoch 796, training loss: 7574.96, average training loss: 8734.41, base loss: 15384.67
[INFO 2017-06-29 17:48:47,429 main.py:57] epoch 797, training loss: 7994.67, average training loss: 8733.49, base loss: 15383.75
[INFO 2017-06-29 17:48:50,334 main.py:57] epoch 798, training loss: 7978.24, average training loss: 8732.54, base loss: 15385.37
[INFO 2017-06-29 17:48:53,382 main.py:57] epoch 799, training loss: 7809.62, average training loss: 8731.39, base loss: 15386.27
[INFO 2017-06-29 17:48:53,382 main.py:59] epoch 799, testing
[INFO 2017-06-29 17:49:05,841 main.py:104] average testing loss: 8469.32, base loss: 16464.69
[INFO 2017-06-29 17:49:05,841 main.py:105] improve_loss: 7995.37, improve_percent: 0.49
[INFO 2017-06-29 17:49:05,843 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:49:05,869 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 17:49:08,813 main.py:57] epoch 800, training loss: 7899.80, average training loss: 8730.35, base loss: 15386.51
[INFO 2017-06-29 17:49:11,807 main.py:57] epoch 801, training loss: 9506.27, average training loss: 8731.32, base loss: 15389.57
[INFO 2017-06-29 17:49:14,799 main.py:57] epoch 802, training loss: 7626.79, average training loss: 8729.94, base loss: 15387.61
[INFO 2017-06-29 17:49:17,785 main.py:57] epoch 803, training loss: 8260.84, average training loss: 8729.36, base loss: 15387.88
[INFO 2017-06-29 17:49:20,736 main.py:57] epoch 804, training loss: 7550.38, average training loss: 8727.89, base loss: 15386.93
[INFO 2017-06-29 17:49:23,701 main.py:57] epoch 805, training loss: 7460.54, average training loss: 8726.32, base loss: 15385.40
[INFO 2017-06-29 17:49:26,647 main.py:57] epoch 806, training loss: 9789.58, average training loss: 8727.64, base loss: 15389.40
[INFO 2017-06-29 17:49:29,658 main.py:57] epoch 807, training loss: 8315.99, average training loss: 8727.13, base loss: 15389.51
[INFO 2017-06-29 17:49:32,638 main.py:57] epoch 808, training loss: 8196.27, average training loss: 8726.47, base loss: 15391.39
[INFO 2017-06-29 17:49:35,544 main.py:57] epoch 809, training loss: 7313.61, average training loss: 8724.73, base loss: 15392.49
[INFO 2017-06-29 17:49:38,520 main.py:57] epoch 810, training loss: 7405.00, average training loss: 8723.10, base loss: 15391.74
[INFO 2017-06-29 17:49:41,520 main.py:57] epoch 811, training loss: 7621.16, average training loss: 8721.75, base loss: 15389.03
[INFO 2017-06-29 17:49:44,525 main.py:57] epoch 812, training loss: 7300.65, average training loss: 8720.00, base loss: 15385.95
[INFO 2017-06-29 17:49:47,609 main.py:57] epoch 813, training loss: 7661.87, average training loss: 8718.70, base loss: 15382.38
[INFO 2017-06-29 17:49:50,569 main.py:57] epoch 814, training loss: 8370.96, average training loss: 8718.27, base loss: 15384.23
[INFO 2017-06-29 17:49:53,597 main.py:57] epoch 815, training loss: 8429.80, average training loss: 8717.92, base loss: 15384.25
[INFO 2017-06-29 17:49:56,597 main.py:57] epoch 816, training loss: 7506.55, average training loss: 8716.43, base loss: 15382.46
[INFO 2017-06-29 17:49:59,543 main.py:57] epoch 817, training loss: 8479.32, average training loss: 8716.14, base loss: 15383.42
[INFO 2017-06-29 17:50:02,565 main.py:57] epoch 818, training loss: 7281.95, average training loss: 8714.39, base loss: 15381.61
[INFO 2017-06-29 17:50:05,507 main.py:57] epoch 819, training loss: 8836.87, average training loss: 8714.54, base loss: 15384.46
[INFO 2017-06-29 17:50:08,462 main.py:57] epoch 820, training loss: 6824.76, average training loss: 8712.24, base loss: 15380.84
[INFO 2017-06-29 17:50:11,350 main.py:57] epoch 821, training loss: 7828.21, average training loss: 8711.17, base loss: 15378.11
[INFO 2017-06-29 17:50:14,328 main.py:57] epoch 822, training loss: 7840.60, average training loss: 8710.11, base loss: 15377.61
[INFO 2017-06-29 17:50:17,315 main.py:57] epoch 823, training loss: 8170.11, average training loss: 8709.45, base loss: 15378.49
[INFO 2017-06-29 17:50:20,274 main.py:57] epoch 824, training loss: 7478.10, average training loss: 8707.96, base loss: 15375.22
[INFO 2017-06-29 17:50:23,213 main.py:57] epoch 825, training loss: 7392.31, average training loss: 8706.37, base loss: 15374.31
[INFO 2017-06-29 17:50:26,159 main.py:57] epoch 826, training loss: 7751.64, average training loss: 8705.21, base loss: 15375.53
[INFO 2017-06-29 17:50:29,113 main.py:57] epoch 827, training loss: 8462.23, average training loss: 8704.92, base loss: 15378.68
[INFO 2017-06-29 17:50:32,018 main.py:57] epoch 828, training loss: 8728.10, average training loss: 8704.95, base loss: 15381.21
[INFO 2017-06-29 17:50:34,973 main.py:57] epoch 829, training loss: 7492.96, average training loss: 8703.49, base loss: 15381.46
[INFO 2017-06-29 17:50:38,010 main.py:57] epoch 830, training loss: 8051.21, average training loss: 8702.70, base loss: 15382.79
[INFO 2017-06-29 17:50:40,967 main.py:57] epoch 831, training loss: 6617.71, average training loss: 8700.20, base loss: 15381.47
[INFO 2017-06-29 17:50:43,984 main.py:57] epoch 832, training loss: 7298.06, average training loss: 8698.51, base loss: 15379.85
[INFO 2017-06-29 17:50:46,995 main.py:57] epoch 833, training loss: 7159.50, average training loss: 8696.67, base loss: 15378.94
[INFO 2017-06-29 17:50:49,937 main.py:57] epoch 834, training loss: 6550.93, average training loss: 8694.10, base loss: 15375.43
[INFO 2017-06-29 17:50:53,008 main.py:57] epoch 835, training loss: 7896.39, average training loss: 8693.14, base loss: 15376.18
[INFO 2017-06-29 17:50:55,954 main.py:57] epoch 836, training loss: 7541.09, average training loss: 8691.77, base loss: 15377.02
[INFO 2017-06-29 17:50:58,934 main.py:57] epoch 837, training loss: 7596.30, average training loss: 8690.46, base loss: 15377.91
[INFO 2017-06-29 17:51:01,859 main.py:57] epoch 838, training loss: 7138.58, average training loss: 8688.61, base loss: 15376.75
[INFO 2017-06-29 17:51:04,772 main.py:57] epoch 839, training loss: 7639.83, average training loss: 8687.36, base loss: 15377.25
[INFO 2017-06-29 17:51:07,783 main.py:57] epoch 840, training loss: 7305.53, average training loss: 8685.72, base loss: 15376.83
[INFO 2017-06-29 17:51:10,722 main.py:57] epoch 841, training loss: 8989.59, average training loss: 8686.08, base loss: 15379.93
[INFO 2017-06-29 17:51:13,744 main.py:57] epoch 842, training loss: 8307.01, average training loss: 8685.63, base loss: 15382.14
[INFO 2017-06-29 17:51:16,652 main.py:57] epoch 843, training loss: 9137.84, average training loss: 8686.17, base loss: 15383.56
[INFO 2017-06-29 17:51:19,591 main.py:57] epoch 844, training loss: 7847.76, average training loss: 8685.17, base loss: 15384.75
[INFO 2017-06-29 17:51:22,621 main.py:57] epoch 845, training loss: 8067.42, average training loss: 8684.44, base loss: 15384.24
[INFO 2017-06-29 17:51:25,590 main.py:57] epoch 846, training loss: 7876.65, average training loss: 8683.49, base loss: 15385.50
[INFO 2017-06-29 17:51:28,542 main.py:57] epoch 847, training loss: 6994.00, average training loss: 8681.50, base loss: 15384.53
[INFO 2017-06-29 17:51:31,506 main.py:57] epoch 848, training loss: 7546.16, average training loss: 8680.16, base loss: 15384.02
[INFO 2017-06-29 17:51:34,469 main.py:57] epoch 849, training loss: 7357.57, average training loss: 8678.60, base loss: 15384.34
[INFO 2017-06-29 17:51:37,482 main.py:57] epoch 850, training loss: 7365.40, average training loss: 8677.06, base loss: 15383.67
[INFO 2017-06-29 17:51:40,461 main.py:57] epoch 851, training loss: 7667.49, average training loss: 8675.88, base loss: 15380.21
[INFO 2017-06-29 17:51:43,412 main.py:57] epoch 852, training loss: 6949.27, average training loss: 8673.85, base loss: 15377.80
[INFO 2017-06-29 17:51:46,356 main.py:57] epoch 853, training loss: 7607.98, average training loss: 8672.60, base loss: 15374.90
[INFO 2017-06-29 17:51:49,361 main.py:57] epoch 854, training loss: 8909.00, average training loss: 8672.88, base loss: 15377.66
[INFO 2017-06-29 17:51:52,387 main.py:57] epoch 855, training loss: 7417.05, average training loss: 8671.41, base loss: 15378.52
[INFO 2017-06-29 17:51:55,399 main.py:57] epoch 856, training loss: 8803.28, average training loss: 8671.57, base loss: 15381.32
[INFO 2017-06-29 17:51:58,401 main.py:57] epoch 857, training loss: 7482.67, average training loss: 8670.18, base loss: 15380.97
[INFO 2017-06-29 17:52:01,487 main.py:57] epoch 858, training loss: 7563.84, average training loss: 8668.89, base loss: 15379.94
[INFO 2017-06-29 17:52:04,587 main.py:57] epoch 859, training loss: 7852.11, average training loss: 8667.94, base loss: 15380.50
[INFO 2017-06-29 17:52:07,583 main.py:57] epoch 860, training loss: 8050.58, average training loss: 8667.23, base loss: 15382.94
[INFO 2017-06-29 17:52:10,543 main.py:57] epoch 861, training loss: 8319.55, average training loss: 8666.82, base loss: 15385.26
[INFO 2017-06-29 17:52:13,516 main.py:57] epoch 862, training loss: 6326.47, average training loss: 8664.11, base loss: 15382.68
[INFO 2017-06-29 17:52:16,507 main.py:57] epoch 863, training loss: 7818.93, average training loss: 8663.13, base loss: 15381.14
[INFO 2017-06-29 17:52:19,481 main.py:57] epoch 864, training loss: 6529.61, average training loss: 8660.67, base loss: 15378.15
[INFO 2017-06-29 17:52:22,437 main.py:57] epoch 865, training loss: 8010.97, average training loss: 8659.92, base loss: 15381.52
[INFO 2017-06-29 17:52:25,445 main.py:57] epoch 866, training loss: 7785.14, average training loss: 8658.91, base loss: 15382.26
[INFO 2017-06-29 17:52:28,438 main.py:57] epoch 867, training loss: 6903.45, average training loss: 8656.88, base loss: 15380.74
[INFO 2017-06-29 17:52:31,451 main.py:57] epoch 868, training loss: 7315.18, average training loss: 8655.34, base loss: 15379.27
[INFO 2017-06-29 17:52:34,467 main.py:57] epoch 869, training loss: 7283.48, average training loss: 8653.76, base loss: 15376.86
[INFO 2017-06-29 17:52:37,470 main.py:57] epoch 870, training loss: 7639.62, average training loss: 8652.60, base loss: 15377.38
[INFO 2017-06-29 17:52:40,463 main.py:57] epoch 871, training loss: 7135.15, average training loss: 8650.86, base loss: 15377.95
[INFO 2017-06-29 17:52:43,477 main.py:57] epoch 872, training loss: 8145.26, average training loss: 8650.28, base loss: 15381.23
[INFO 2017-06-29 17:52:46,425 main.py:57] epoch 873, training loss: 8686.23, average training loss: 8650.32, base loss: 15384.09
[INFO 2017-06-29 17:52:49,382 main.py:57] epoch 874, training loss: 7597.34, average training loss: 8649.12, base loss: 15380.91
[INFO 2017-06-29 17:52:52,328 main.py:57] epoch 875, training loss: 7008.95, average training loss: 8647.25, base loss: 15378.14
[INFO 2017-06-29 17:52:55,310 main.py:57] epoch 876, training loss: 7830.22, average training loss: 8646.31, base loss: 15376.63
[INFO 2017-06-29 17:52:58,286 main.py:57] epoch 877, training loss: 7962.17, average training loss: 8645.53, base loss: 15375.33
[INFO 2017-06-29 17:53:01,172 main.py:57] epoch 878, training loss: 7662.40, average training loss: 8644.42, base loss: 15374.21
[INFO 2017-06-29 17:53:04,171 main.py:57] epoch 879, training loss: 7714.81, average training loss: 8643.36, base loss: 15373.03
[INFO 2017-06-29 17:53:07,224 main.py:57] epoch 880, training loss: 7932.63, average training loss: 8642.55, base loss: 15372.38
[INFO 2017-06-29 17:53:10,161 main.py:57] epoch 881, training loss: 7512.57, average training loss: 8641.27, base loss: 15371.94
[INFO 2017-06-29 17:53:13,183 main.py:57] epoch 882, training loss: 8379.80, average training loss: 8640.98, base loss: 15374.35
[INFO 2017-06-29 17:53:16,162 main.py:57] epoch 883, training loss: 8203.84, average training loss: 8640.48, base loss: 15377.96
[INFO 2017-06-29 17:53:19,073 main.py:57] epoch 884, training loss: 7030.86, average training loss: 8638.66, base loss: 15374.13
[INFO 2017-06-29 17:53:22,120 main.py:57] epoch 885, training loss: 8011.68, average training loss: 8637.96, base loss: 15375.20
[INFO 2017-06-29 17:53:25,112 main.py:57] epoch 886, training loss: 7218.89, average training loss: 8636.36, base loss: 15374.63
[INFO 2017-06-29 17:53:28,147 main.py:57] epoch 887, training loss: 6999.06, average training loss: 8634.51, base loss: 15374.42
[INFO 2017-06-29 17:53:31,105 main.py:57] epoch 888, training loss: 7330.47, average training loss: 8633.04, base loss: 15371.92
[INFO 2017-06-29 17:53:34,054 main.py:57] epoch 889, training loss: 7974.18, average training loss: 8632.30, base loss: 15370.20
[INFO 2017-06-29 17:53:37,027 main.py:57] epoch 890, training loss: 7224.29, average training loss: 8630.72, base loss: 15368.46
[INFO 2017-06-29 17:53:39,966 main.py:57] epoch 891, training loss: 7210.06, average training loss: 8629.13, base loss: 15367.44
[INFO 2017-06-29 17:53:42,939 main.py:57] epoch 892, training loss: 7919.92, average training loss: 8628.34, base loss: 15367.44
[INFO 2017-06-29 17:53:46,028 main.py:57] epoch 893, training loss: 7456.35, average training loss: 8627.03, base loss: 15367.05
[INFO 2017-06-29 17:53:49,005 main.py:57] epoch 894, training loss: 7513.41, average training loss: 8625.78, base loss: 15367.54
[INFO 2017-06-29 17:53:51,885 main.py:57] epoch 895, training loss: 8136.21, average training loss: 8625.24, base loss: 15368.82
[INFO 2017-06-29 17:53:54,881 main.py:57] epoch 896, training loss: 8259.50, average training loss: 8624.83, base loss: 15369.78
[INFO 2017-06-29 17:53:57,799 main.py:57] epoch 897, training loss: 7742.17, average training loss: 8623.84, base loss: 15368.95
[INFO 2017-06-29 17:54:00,757 main.py:57] epoch 898, training loss: 7650.02, average training loss: 8622.76, base loss: 15367.21
[INFO 2017-06-29 17:54:03,739 main.py:57] epoch 899, training loss: 6879.45, average training loss: 8620.82, base loss: 15365.37
[INFO 2017-06-29 17:54:03,740 main.py:59] epoch 899, testing
[INFO 2017-06-29 17:54:16,007 main.py:104] average testing loss: 8344.53, base loss: 16498.43
[INFO 2017-06-29 17:54:16,007 main.py:105] improve_loss: 8153.89, improve_percent: 0.49
[INFO 2017-06-29 17:54:16,010 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 17:54:16,036 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 17:54:18,939 main.py:57] epoch 900, training loss: 7454.10, average training loss: 8619.53, base loss: 15366.02
[INFO 2017-06-29 17:54:21,967 main.py:57] epoch 901, training loss: 7168.40, average training loss: 8617.92, base loss: 15364.59
[INFO 2017-06-29 17:54:24,942 main.py:57] epoch 902, training loss: 7805.90, average training loss: 8617.02, base loss: 15365.41
[INFO 2017-06-29 17:54:27,937 main.py:57] epoch 903, training loss: 7820.65, average training loss: 8616.14, base loss: 15367.80
[INFO 2017-06-29 17:54:30,833 main.py:57] epoch 904, training loss: 9173.18, average training loss: 8616.76, base loss: 15370.52
[INFO 2017-06-29 17:54:33,882 main.py:57] epoch 905, training loss: 8680.19, average training loss: 8616.83, base loss: 15371.95
[INFO 2017-06-29 17:54:36,900 main.py:57] epoch 906, training loss: 7705.64, average training loss: 8615.82, base loss: 15371.70
[INFO 2017-06-29 17:54:39,891 main.py:57] epoch 907, training loss: 7486.10, average training loss: 8614.58, base loss: 15369.75
[INFO 2017-06-29 17:54:42,858 main.py:57] epoch 908, training loss: 7563.88, average training loss: 8613.42, base loss: 15367.83
[INFO 2017-06-29 17:54:45,867 main.py:57] epoch 909, training loss: 7130.89, average training loss: 8611.79, base loss: 15366.56
[INFO 2017-06-29 17:54:49,025 main.py:57] epoch 910, training loss: 7918.19, average training loss: 8611.03, base loss: 15367.31
[INFO 2017-06-29 17:54:52,052 main.py:57] epoch 911, training loss: 7352.76, average training loss: 8609.65, base loss: 15366.39
[INFO 2017-06-29 17:54:55,017 main.py:57] epoch 912, training loss: 9508.73, average training loss: 8610.64, base loss: 15368.50
[INFO 2017-06-29 17:54:58,023 main.py:57] epoch 913, training loss: 8162.06, average training loss: 8610.15, base loss: 15368.02
[INFO 2017-06-29 17:55:00,989 main.py:57] epoch 914, training loss: 7255.92, average training loss: 8608.67, base loss: 15366.62
[INFO 2017-06-29 17:55:03,970 main.py:57] epoch 915, training loss: 7443.22, average training loss: 8607.39, base loss: 15363.95
[INFO 2017-06-29 17:55:06,956 main.py:57] epoch 916, training loss: 7384.55, average training loss: 8606.06, base loss: 15364.03
[INFO 2017-06-29 17:55:09,938 main.py:57] epoch 917, training loss: 7436.29, average training loss: 8604.79, base loss: 15364.15
[INFO 2017-06-29 17:55:12,918 main.py:57] epoch 918, training loss: 6965.48, average training loss: 8603.00, base loss: 15360.82
[INFO 2017-06-29 17:55:15,841 main.py:57] epoch 919, training loss: 7498.71, average training loss: 8601.80, base loss: 15360.91
[INFO 2017-06-29 17:55:18,957 main.py:57] epoch 920, training loss: 7787.10, average training loss: 8600.92, base loss: 15360.77
[INFO 2017-06-29 17:55:21,883 main.py:57] epoch 921, training loss: 7902.02, average training loss: 8600.16, base loss: 15361.46
[INFO 2017-06-29 17:55:24,836 main.py:57] epoch 922, training loss: 8343.78, average training loss: 8599.88, base loss: 15364.00
[INFO 2017-06-29 17:55:27,853 main.py:57] epoch 923, training loss: 8137.71, average training loss: 8599.38, base loss: 15368.81
[INFO 2017-06-29 17:55:30,873 main.py:57] epoch 924, training loss: 7947.66, average training loss: 8598.68, base loss: 15371.18
[INFO 2017-06-29 17:55:33,847 main.py:57] epoch 925, training loss: 8526.11, average training loss: 8598.60, base loss: 15374.85
[INFO 2017-06-29 17:55:36,930 main.py:57] epoch 926, training loss: 7865.35, average training loss: 8597.81, base loss: 15375.51
[INFO 2017-06-29 17:55:39,956 main.py:57] epoch 927, training loss: 8857.78, average training loss: 8598.09, base loss: 15376.38
[INFO 2017-06-29 17:55:42,875 main.py:57] epoch 928, training loss: 8293.52, average training loss: 8597.76, base loss: 15377.84
[INFO 2017-06-29 17:55:45,856 main.py:57] epoch 929, training loss: 6889.07, average training loss: 8595.92, base loss: 15376.90
[INFO 2017-06-29 17:55:48,830 main.py:57] epoch 930, training loss: 7259.92, average training loss: 8594.49, base loss: 15375.49
[INFO 2017-06-29 17:55:51,845 main.py:57] epoch 931, training loss: 7468.57, average training loss: 8593.28, base loss: 15375.58
[INFO 2017-06-29 17:55:54,784 main.py:57] epoch 932, training loss: 7436.57, average training loss: 8592.04, base loss: 15374.41
[INFO 2017-06-29 17:55:57,679 main.py:57] epoch 933, training loss: 8296.00, average training loss: 8591.72, base loss: 15375.33
[INFO 2017-06-29 17:56:00,717 main.py:57] epoch 934, training loss: 7638.90, average training loss: 8590.70, base loss: 15374.89
[INFO 2017-06-29 17:56:03,672 main.py:57] epoch 935, training loss: 8375.64, average training loss: 8590.47, base loss: 15374.73
[INFO 2017-06-29 17:56:06,559 main.py:57] epoch 936, training loss: 8428.44, average training loss: 8590.30, base loss: 15375.10
[INFO 2017-06-29 17:56:09,481 main.py:57] epoch 937, training loss: 8036.85, average training loss: 8589.71, base loss: 15374.98
[INFO 2017-06-29 17:56:12,429 main.py:57] epoch 938, training loss: 7719.09, average training loss: 8588.78, base loss: 15376.66
[INFO 2017-06-29 17:56:15,420 main.py:57] epoch 939, training loss: 7249.35, average training loss: 8587.36, base loss: 15376.42
[INFO 2017-06-29 17:56:18,419 main.py:57] epoch 940, training loss: 7095.49, average training loss: 8585.77, base loss: 15373.85
[INFO 2017-06-29 17:56:21,421 main.py:57] epoch 941, training loss: 8393.91, average training loss: 8585.57, base loss: 15375.36
[INFO 2017-06-29 17:56:24,393 main.py:57] epoch 942, training loss: 7799.16, average training loss: 8584.73, base loss: 15376.11
[INFO 2017-06-29 17:56:27,358 main.py:57] epoch 943, training loss: 7809.25, average training loss: 8583.91, base loss: 15377.39
[INFO 2017-06-29 17:56:30,450 main.py:57] epoch 944, training loss: 8670.66, average training loss: 8584.01, base loss: 15379.29
[INFO 2017-06-29 17:56:33,418 main.py:57] epoch 945, training loss: 8055.91, average training loss: 8583.45, base loss: 15380.17
[INFO 2017-06-29 17:56:36,493 main.py:57] epoch 946, training loss: 7381.19, average training loss: 8582.18, base loss: 15379.15
[INFO 2017-06-29 17:56:39,464 main.py:57] epoch 947, training loss: 7131.25, average training loss: 8580.65, base loss: 15378.24
[INFO 2017-06-29 17:56:42,468 main.py:57] epoch 948, training loss: 7278.19, average training loss: 8579.27, base loss: 15375.75
[INFO 2017-06-29 17:56:45,479 main.py:57] epoch 949, training loss: 8033.85, average training loss: 8578.70, base loss: 15375.73
[INFO 2017-06-29 17:56:48,447 main.py:57] epoch 950, training loss: 7286.86, average training loss: 8577.34, base loss: 15374.40
[INFO 2017-06-29 17:56:51,415 main.py:57] epoch 951, training loss: 6889.53, average training loss: 8575.57, base loss: 15370.74
[INFO 2017-06-29 17:56:54,414 main.py:57] epoch 952, training loss: 7178.99, average training loss: 8574.10, base loss: 15369.43
[INFO 2017-06-29 17:56:57,473 main.py:57] epoch 953, training loss: 7965.64, average training loss: 8573.47, base loss: 15370.06
[INFO 2017-06-29 17:57:00,523 main.py:57] epoch 954, training loss: 8418.88, average training loss: 8573.30, base loss: 15372.46
[INFO 2017-06-29 17:57:03,541 main.py:57] epoch 955, training loss: 8195.57, average training loss: 8572.91, base loss: 15374.60
[INFO 2017-06-29 17:57:06,536 main.py:57] epoch 956, training loss: 7396.98, average training loss: 8571.68, base loss: 15373.25
[INFO 2017-06-29 17:57:09,512 main.py:57] epoch 957, training loss: 7481.00, average training loss: 8570.54, base loss: 15373.99
[INFO 2017-06-29 17:57:12,482 main.py:57] epoch 958, training loss: 6791.38, average training loss: 8568.69, base loss: 15370.47
[INFO 2017-06-29 17:57:15,448 main.py:57] epoch 959, training loss: 8469.40, average training loss: 8568.58, base loss: 15370.58
[INFO 2017-06-29 17:57:18,557 main.py:57] epoch 960, training loss: 8275.63, average training loss: 8568.28, base loss: 15370.06
[INFO 2017-06-29 17:57:21,489 main.py:57] epoch 961, training loss: 7793.04, average training loss: 8567.47, base loss: 15370.35
[INFO 2017-06-29 17:57:24,506 main.py:57] epoch 962, training loss: 8289.79, average training loss: 8567.18, base loss: 15372.02
[INFO 2017-06-29 17:57:27,536 main.py:57] epoch 963, training loss: 7109.09, average training loss: 8565.67, base loss: 15369.98
[INFO 2017-06-29 17:57:30,550 main.py:57] epoch 964, training loss: 7247.48, average training loss: 8564.31, base loss: 15368.90
[INFO 2017-06-29 17:57:33,541 main.py:57] epoch 965, training loss: 7642.31, average training loss: 8563.35, base loss: 15367.26
[INFO 2017-06-29 17:57:36,492 main.py:57] epoch 966, training loss: 7475.85, average training loss: 8562.23, base loss: 15367.27
[INFO 2017-06-29 17:57:39,492 main.py:57] epoch 967, training loss: 8439.41, average training loss: 8562.10, base loss: 15370.54
[INFO 2017-06-29 17:57:42,483 main.py:57] epoch 968, training loss: 7488.52, average training loss: 8560.99, base loss: 15371.04
[INFO 2017-06-29 17:57:45,491 main.py:57] epoch 969, training loss: 8251.84, average training loss: 8560.67, base loss: 15371.31
[INFO 2017-06-29 17:57:48,547 main.py:57] epoch 970, training loss: 8230.80, average training loss: 8560.33, base loss: 15369.52
[INFO 2017-06-29 17:57:51,508 main.py:57] epoch 971, training loss: 7865.55, average training loss: 8559.62, base loss: 15368.10
[INFO 2017-06-29 17:57:54,512 main.py:57] epoch 972, training loss: 7244.79, average training loss: 8558.27, base loss: 15365.63
[INFO 2017-06-29 17:57:57,520 main.py:57] epoch 973, training loss: 8916.53, average training loss: 8558.63, base loss: 15367.64
[INFO 2017-06-29 17:58:00,476 main.py:57] epoch 974, training loss: 8712.93, average training loss: 8558.79, base loss: 15368.56
[INFO 2017-06-29 17:58:03,473 main.py:57] epoch 975, training loss: 7877.37, average training loss: 8558.09, base loss: 15367.99
[INFO 2017-06-29 17:58:06,494 main.py:57] epoch 976, training loss: 7514.41, average training loss: 8557.03, base loss: 15368.50
[INFO 2017-06-29 17:58:09,530 main.py:57] epoch 977, training loss: 8074.88, average training loss: 8556.53, base loss: 15369.87
[INFO 2017-06-29 17:58:12,616 main.py:57] epoch 978, training loss: 7202.64, average training loss: 8555.15, base loss: 15367.80
[INFO 2017-06-29 17:58:15,669 main.py:57] epoch 979, training loss: 8182.69, average training loss: 8554.77, base loss: 15366.25
[INFO 2017-06-29 17:58:18,585 main.py:57] epoch 980, training loss: 7429.24, average training loss: 8553.62, base loss: 15366.21
[INFO 2017-06-29 17:58:21,544 main.py:57] epoch 981, training loss: 7357.79, average training loss: 8552.41, base loss: 15365.09
[INFO 2017-06-29 17:58:24,503 main.py:57] epoch 982, training loss: 7583.02, average training loss: 8551.42, base loss: 15364.40
[INFO 2017-06-29 17:58:27,501 main.py:57] epoch 983, training loss: 8022.50, average training loss: 8550.88, base loss: 15365.03
[INFO 2017-06-29 17:58:30,452 main.py:57] epoch 984, training loss: 8781.99, average training loss: 8551.12, base loss: 15367.24
[INFO 2017-06-29 17:58:33,427 main.py:57] epoch 985, training loss: 8837.52, average training loss: 8551.41, base loss: 15370.33
[INFO 2017-06-29 17:58:36,393 main.py:57] epoch 986, training loss: 8523.45, average training loss: 8551.38, base loss: 15371.89
[INFO 2017-06-29 17:58:39,414 main.py:57] epoch 987, training loss: 7783.32, average training loss: 8550.60, base loss: 15373.39
[INFO 2017-06-29 17:58:42,414 main.py:57] epoch 988, training loss: 8151.38, average training loss: 8550.20, base loss: 15374.44
[INFO 2017-06-29 17:58:45,354 main.py:57] epoch 989, training loss: 8132.85, average training loss: 8549.78, base loss: 15373.63
[INFO 2017-06-29 17:58:48,333 main.py:57] epoch 990, training loss: 8784.62, average training loss: 8550.01, base loss: 15376.74
[INFO 2017-06-29 17:58:51,273 main.py:57] epoch 991, training loss: 7542.19, average training loss: 8549.00, base loss: 15374.82
[INFO 2017-06-29 17:58:54,252 main.py:57] epoch 992, training loss: 8575.66, average training loss: 8549.02, base loss: 15376.24
[INFO 2017-06-29 17:58:57,220 main.py:57] epoch 993, training loss: 8113.74, average training loss: 8548.59, base loss: 15376.20
[INFO 2017-06-29 17:59:00,195 main.py:57] epoch 994, training loss: 8042.08, average training loss: 8548.08, base loss: 15377.21
[INFO 2017-06-29 17:59:03,118 main.py:57] epoch 995, training loss: 7978.78, average training loss: 8547.51, base loss: 15375.62
[INFO 2017-06-29 17:59:06,099 main.py:57] epoch 996, training loss: 7847.91, average training loss: 8546.80, base loss: 15376.39
[INFO 2017-06-29 17:59:09,086 main.py:57] epoch 997, training loss: 8260.11, average training loss: 8546.52, base loss: 15378.33
[INFO 2017-06-29 17:59:12,035 main.py:57] epoch 998, training loss: 8361.14, average training loss: 8546.33, base loss: 15378.94
[INFO 2017-06-29 17:59:15,042 main.py:57] epoch 999, training loss: 7410.06, average training loss: 8545.19, base loss: 15377.10
[INFO 2017-06-29 17:59:15,043 main.py:59] epoch 999, testing
[INFO 2017-06-29 17:59:27,486 main.py:104] average testing loss: 8019.37, base loss: 15461.14
[INFO 2017-06-29 17:59:27,486 main.py:105] improve_loss: 7441.77, improve_percent: 0.48
[INFO 2017-06-29 17:59:27,489 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 17:59:30,438 main.py:57] epoch 1000, training loss: 8054.25, average training loss: 8511.52, base loss: 15375.68
[INFO 2017-06-29 17:59:33,451 main.py:57] epoch 1001, training loss: 7072.06, average training loss: 8483.58, base loss: 15371.56
[INFO 2017-06-29 17:59:36,414 main.py:57] epoch 1002, training loss: 7354.09, average training loss: 8457.54, base loss: 15371.20
[INFO 2017-06-29 17:59:39,452 main.py:57] epoch 1003, training loss: 7692.90, average training loss: 8437.20, base loss: 15371.57
[INFO 2017-06-29 17:59:42,379 main.py:57] epoch 1004, training loss: 8017.20, average training loss: 8418.31, base loss: 15371.51
[INFO 2017-06-29 17:59:45,319 main.py:57] epoch 1005, training loss: 7420.20, average training loss: 8402.26, base loss: 15370.59
[INFO 2017-06-29 17:59:48,312 main.py:57] epoch 1006, training loss: 8778.41, average training loss: 8388.81, base loss: 15373.02
[INFO 2017-06-29 17:59:51,254 main.py:57] epoch 1007, training loss: 7946.00, average training loss: 8375.51, base loss: 15374.08
[INFO 2017-06-29 17:59:54,192 main.py:57] epoch 1008, training loss: 6840.68, average training loss: 8361.46, base loss: 15370.17
[INFO 2017-06-29 17:59:57,287 main.py:57] epoch 1009, training loss: 7758.87, average training loss: 8351.06, base loss: 15369.10
[INFO 2017-06-29 18:00:00,251 main.py:57] epoch 1010, training loss: 7338.41, average training loss: 8341.26, base loss: 15369.11
[INFO 2017-06-29 18:00:03,165 main.py:57] epoch 1011, training loss: 8820.14, average training loss: 8333.93, base loss: 15371.68
[INFO 2017-06-29 18:00:06,145 main.py:57] epoch 1012, training loss: 7601.99, average training loss: 8327.17, base loss: 15371.93
[INFO 2017-06-29 18:00:09,125 main.py:57] epoch 1013, training loss: 7478.58, average training loss: 8320.26, base loss: 15372.02
[INFO 2017-06-29 18:00:12,110 main.py:57] epoch 1014, training loss: 7880.89, average training loss: 8314.43, base loss: 15374.45
[INFO 2017-06-29 18:00:15,088 main.py:57] epoch 1015, training loss: 8325.54, average training loss: 8309.51, base loss: 15376.01
[INFO 2017-06-29 18:00:18,111 main.py:57] epoch 1016, training loss: 7369.72, average training loss: 8303.98, base loss: 15376.54
[INFO 2017-06-29 18:00:21,061 main.py:57] epoch 1017, training loss: 7825.15, average training loss: 8298.47, base loss: 15377.33
[INFO 2017-06-29 18:00:24,032 main.py:57] epoch 1018, training loss: 7653.10, average training loss: 8293.19, base loss: 15376.35
[INFO 2017-06-29 18:00:27,082 main.py:57] epoch 1019, training loss: 7101.18, average training loss: 8288.64, base loss: 15375.75
[INFO 2017-06-29 18:00:30,059 main.py:57] epoch 1020, training loss: 7589.54, average training loss: 8284.37, base loss: 15376.18
[INFO 2017-06-29 18:00:33,072 main.py:57] epoch 1021, training loss: 7372.42, average training loss: 8279.54, base loss: 15376.10
[INFO 2017-06-29 18:00:36,061 main.py:57] epoch 1022, training loss: 7005.88, average training loss: 8273.65, base loss: 15373.41
[INFO 2017-06-29 18:00:39,036 main.py:57] epoch 1023, training loss: 8471.98, average training loss: 8271.09, base loss: 15375.83
[INFO 2017-06-29 18:00:42,006 main.py:57] epoch 1024, training loss: 8674.99, average training loss: 8269.24, base loss: 15376.64
[INFO 2017-06-29 18:00:44,994 main.py:57] epoch 1025, training loss: 7648.52, average training loss: 8264.88, base loss: 15377.59
[INFO 2017-06-29 18:00:47,985 main.py:57] epoch 1026, training loss: 7382.09, average training loss: 8261.49, base loss: 15374.67
[INFO 2017-06-29 18:00:50,944 main.py:57] epoch 1027, training loss: 7203.44, average training loss: 8257.23, base loss: 15371.16
[INFO 2017-06-29 18:00:53,932 main.py:57] epoch 1028, training loss: 7502.00, average training loss: 8253.00, base loss: 15370.44
[INFO 2017-06-29 18:00:56,964 main.py:57] epoch 1029, training loss: 8834.90, average training loss: 8251.62, base loss: 15373.25
[INFO 2017-06-29 18:00:59,953 main.py:57] epoch 1030, training loss: 6653.12, average training loss: 8248.10, base loss: 15370.38
[INFO 2017-06-29 18:01:02,906 main.py:57] epoch 1031, training loss: 7200.93, average training loss: 8245.14, base loss: 15370.03
[INFO 2017-06-29 18:01:05,977 main.py:57] epoch 1032, training loss: 7381.50, average training loss: 8242.14, base loss: 15369.57
[INFO 2017-06-29 18:01:08,974 main.py:57] epoch 1033, training loss: 7404.37, average training loss: 8238.99, base loss: 15369.27
[INFO 2017-06-29 18:01:11,991 main.py:57] epoch 1034, training loss: 7535.93, average training loss: 8236.03, base loss: 15368.55
[INFO 2017-06-29 18:01:14,957 main.py:57] epoch 1035, training loss: 8315.72, average training loss: 8233.89, base loss: 15370.05
[INFO 2017-06-29 18:01:17,918 main.py:57] epoch 1036, training loss: 6944.84, average training loss: 8231.06, base loss: 15370.56
[INFO 2017-06-29 18:01:20,871 main.py:57] epoch 1037, training loss: 7838.01, average training loss: 8228.14, base loss: 15370.64
[INFO 2017-06-29 18:01:23,922 main.py:57] epoch 1038, training loss: 9085.42, average training loss: 8227.54, base loss: 15374.10
[INFO 2017-06-29 18:01:26,875 main.py:57] epoch 1039, training loss: 8774.09, average training loss: 8225.54, base loss: 15376.68
[INFO 2017-06-29 18:01:29,810 main.py:57] epoch 1040, training loss: 8212.83, average training loss: 8222.50, base loss: 15378.30
[INFO 2017-06-29 18:01:32,775 main.py:57] epoch 1041, training loss: 8942.19, average training loss: 8220.58, base loss: 15378.91
[INFO 2017-06-29 18:01:35,779 main.py:57] epoch 1042, training loss: 7717.08, average training loss: 8218.63, base loss: 15378.15
[INFO 2017-06-29 18:01:38,746 main.py:57] epoch 1043, training loss: 7928.66, average training loss: 8217.18, base loss: 15379.66
[INFO 2017-06-29 18:01:41,730 main.py:57] epoch 1044, training loss: 8111.05, average training loss: 8215.06, base loss: 15378.63
[INFO 2017-06-29 18:01:44,686 main.py:57] epoch 1045, training loss: 6966.69, average training loss: 8211.15, base loss: 15375.34
[INFO 2017-06-29 18:01:47,649 main.py:57] epoch 1046, training loss: 9453.71, average training loss: 8210.52, base loss: 15376.78
[INFO 2017-06-29 18:01:50,592 main.py:57] epoch 1047, training loss: 8287.83, average training loss: 8208.06, base loss: 15376.32
[INFO 2017-06-29 18:01:53,820 main.py:57] epoch 1048, training loss: 6857.21, average training loss: 8206.03, base loss: 15374.73
[INFO 2017-06-29 18:01:56,760 main.py:57] epoch 1049, training loss: 8276.33, average training loss: 8204.77, base loss: 15375.83
[INFO 2017-06-29 18:01:59,759 main.py:57] epoch 1050, training loss: 7714.11, average training loss: 8202.73, base loss: 15374.76
[INFO 2017-06-29 18:02:02,774 main.py:57] epoch 1051, training loss: 6828.07, average training loss: 8199.92, base loss: 15373.22
[INFO 2017-06-29 18:02:05,745 main.py:57] epoch 1052, training loss: 7849.78, average training loss: 8197.41, base loss: 15374.97
[INFO 2017-06-29 18:02:08,724 main.py:57] epoch 1053, training loss: 8107.78, average training loss: 8196.52, base loss: 15376.02
[INFO 2017-06-29 18:02:11,681 main.py:57] epoch 1054, training loss: 7647.70, average training loss: 8192.72, base loss: 15374.88
[INFO 2017-06-29 18:02:14,651 main.py:57] epoch 1055, training loss: 7578.99, average training loss: 8191.05, base loss: 15373.11
[INFO 2017-06-29 18:02:17,622 main.py:57] epoch 1056, training loss: 8779.67, average training loss: 8189.41, base loss: 15375.70
[INFO 2017-06-29 18:02:20,646 main.py:57] epoch 1057, training loss: 7929.87, average training loss: 8186.99, base loss: 15377.36
[INFO 2017-06-29 18:02:23,624 main.py:57] epoch 1058, training loss: 8434.70, average training loss: 8185.74, base loss: 15377.06
[INFO 2017-06-29 18:02:26,598 main.py:57] epoch 1059, training loss: 8132.00, average training loss: 8184.62, base loss: 15377.24
[INFO 2017-06-29 18:02:29,561 main.py:57] epoch 1060, training loss: 7597.66, average training loss: 8182.05, base loss: 15376.95
[INFO 2017-06-29 18:02:32,561 main.py:57] epoch 1061, training loss: 7256.85, average training loss: 8179.78, base loss: 15375.79
[INFO 2017-06-29 18:02:35,545 main.py:57] epoch 1062, training loss: 7564.18, average training loss: 8177.04, base loss: 15373.91
[INFO 2017-06-29 18:02:38,487 main.py:57] epoch 1063, training loss: 7765.57, average training loss: 8173.75, base loss: 15372.45
[INFO 2017-06-29 18:02:41,476 main.py:57] epoch 1064, training loss: 7122.50, average training loss: 8171.19, base loss: 15370.97
[INFO 2017-06-29 18:02:44,518 main.py:57] epoch 1065, training loss: 7559.39, average training loss: 8169.25, base loss: 15371.51
[INFO 2017-06-29 18:02:47,458 main.py:57] epoch 1066, training loss: 7900.99, average training loss: 8167.10, base loss: 15370.83
[INFO 2017-06-29 18:02:50,475 main.py:57] epoch 1067, training loss: 7624.96, average training loss: 8166.06, base loss: 15371.48
[INFO 2017-06-29 18:02:53,360 main.py:57] epoch 1068, training loss: 7212.02, average training loss: 8163.50, base loss: 15371.30
[INFO 2017-06-29 18:02:56,456 main.py:57] epoch 1069, training loss: 8565.69, average training loss: 8162.65, base loss: 15372.78
[INFO 2017-06-29 18:02:59,471 main.py:57] epoch 1070, training loss: 7868.45, average training loss: 8161.48, base loss: 15374.68
[INFO 2017-06-29 18:03:02,497 main.py:57] epoch 1071, training loss: 7262.13, average training loss: 8158.16, base loss: 15374.14
[INFO 2017-06-29 18:03:05,429 main.py:57] epoch 1072, training loss: 8199.87, average training loss: 8157.22, base loss: 15373.89
[INFO 2017-06-29 18:03:08,394 main.py:57] epoch 1073, training loss: 7960.85, average training loss: 8155.72, base loss: 15372.80
[INFO 2017-06-29 18:03:11,353 main.py:57] epoch 1074, training loss: 7785.41, average training loss: 8153.92, base loss: 15372.54
[INFO 2017-06-29 18:03:14,313 main.py:57] epoch 1075, training loss: 7601.16, average training loss: 8152.63, base loss: 15373.47
[INFO 2017-06-29 18:03:17,288 main.py:57] epoch 1076, training loss: 6611.91, average training loss: 8149.84, base loss: 15369.85
[INFO 2017-06-29 18:03:20,338 main.py:57] epoch 1077, training loss: 8651.92, average training loss: 8147.64, base loss: 15370.70
[INFO 2017-06-29 18:03:23,248 main.py:57] epoch 1078, training loss: 7913.95, average training loss: 8147.52, base loss: 15372.26
[INFO 2017-06-29 18:03:26,215 main.py:57] epoch 1079, training loss: 6349.90, average training loss: 8143.76, base loss: 15367.79
[INFO 2017-06-29 18:03:29,184 main.py:57] epoch 1080, training loss: 7592.21, average training loss: 8141.13, base loss: 15368.32
[INFO 2017-06-29 18:03:32,115 main.py:57] epoch 1081, training loss: 7541.49, average training loss: 8139.44, base loss: 15367.89
[INFO 2017-06-29 18:03:35,095 main.py:57] epoch 1082, training loss: 8753.41, average training loss: 8138.87, base loss: 15370.36
[INFO 2017-06-29 18:03:38,030 main.py:57] epoch 1083, training loss: 7754.52, average training loss: 8134.59, base loss: 15369.60
[INFO 2017-06-29 18:03:40,988 main.py:57] epoch 1084, training loss: 6847.67, average training loss: 8130.65, base loss: 15369.44
[INFO 2017-06-29 18:03:44,007 main.py:57] epoch 1085, training loss: 8559.55, average training loss: 8129.10, base loss: 15371.46
[INFO 2017-06-29 18:03:47,026 main.py:57] epoch 1086, training loss: 7214.03, average training loss: 8124.99, base loss: 15370.87
[INFO 2017-06-29 18:03:50,025 main.py:57] epoch 1087, training loss: 7634.95, average training loss: 8123.07, base loss: 15369.34
[INFO 2017-06-29 18:03:53,081 main.py:57] epoch 1088, training loss: 8286.37, average training loss: 8122.18, base loss: 15368.88
[INFO 2017-06-29 18:03:56,015 main.py:57] epoch 1089, training loss: 7726.54, average training loss: 8120.75, base loss: 15366.63
[INFO 2017-06-29 18:03:58,979 main.py:57] epoch 1090, training loss: 8304.10, average training loss: 8120.78, base loss: 15368.29
[INFO 2017-06-29 18:04:01,902 main.py:57] epoch 1091, training loss: 7960.74, average training loss: 8119.03, base loss: 15369.20
[INFO 2017-06-29 18:04:04,912 main.py:57] epoch 1092, training loss: 8404.58, average training loss: 8115.95, base loss: 15371.01
[INFO 2017-06-29 18:04:07,861 main.py:57] epoch 1093, training loss: 8887.41, average training loss: 8114.22, base loss: 15374.15
[INFO 2017-06-29 18:04:10,906 main.py:57] epoch 1094, training loss: 7677.92, average training loss: 8110.85, base loss: 15375.98
[INFO 2017-06-29 18:04:13,863 main.py:57] epoch 1095, training loss: 8422.44, average training loss: 8109.75, base loss: 15377.20
[INFO 2017-06-29 18:04:16,874 main.py:57] epoch 1096, training loss: 7828.17, average training loss: 8107.70, base loss: 15378.28
[INFO 2017-06-29 18:04:19,860 main.py:57] epoch 1097, training loss: 7224.70, average training loss: 8105.81, base loss: 15376.85
[INFO 2017-06-29 18:04:22,837 main.py:57] epoch 1098, training loss: 7880.54, average training loss: 8104.27, base loss: 15377.81
[INFO 2017-06-29 18:04:25,939 main.py:57] epoch 1099, training loss: 7426.82, average training loss: 8102.34, base loss: 15377.98
[INFO 2017-06-29 18:04:25,940 main.py:59] epoch 1099, testing
[INFO 2017-06-29 18:04:38,337 main.py:104] average testing loss: 7950.20, base loss: 15562.79
[INFO 2017-06-29 18:04:38,337 main.py:105] improve_loss: 7612.59, improve_percent: 0.49
[INFO 2017-06-29 18:04:38,338 main.py:71] current best improved percent: 0.49
[INFO 2017-06-29 18:04:41,291 main.py:57] epoch 1100, training loss: 8110.78, average training loss: 8101.37, base loss: 15379.99
[INFO 2017-06-29 18:04:44,303 main.py:57] epoch 1101, training loss: 7734.65, average training loss: 8100.84, base loss: 15379.44
[INFO 2017-06-29 18:04:47,354 main.py:57] epoch 1102, training loss: 7794.30, average training loss: 8101.16, base loss: 15378.96
[INFO 2017-06-29 18:04:50,352 main.py:57] epoch 1103, training loss: 6746.18, average training loss: 8098.88, base loss: 15376.28
[INFO 2017-06-29 18:04:53,301 main.py:57] epoch 1104, training loss: 7360.73, average training loss: 8097.21, base loss: 15375.88
[INFO 2017-06-29 18:04:56,350 main.py:57] epoch 1105, training loss: 7523.45, average training loss: 8095.53, base loss: 15377.30
[INFO 2017-06-29 18:04:59,459 main.py:57] epoch 1106, training loss: 6611.40, average training loss: 8093.43, base loss: 15376.50
[INFO 2017-06-29 18:05:02,395 main.py:57] epoch 1107, training loss: 7843.56, average training loss: 8088.98, base loss: 15377.13
[INFO 2017-06-29 18:05:05,394 main.py:57] epoch 1108, training loss: 8611.01, average training loss: 8088.03, base loss: 15376.24
[INFO 2017-06-29 18:05:08,299 main.py:57] epoch 1109, training loss: 6420.23, average training loss: 8084.97, base loss: 15371.55
[INFO 2017-06-29 18:05:11,279 main.py:57] epoch 1110, training loss: 7590.03, average training loss: 8083.39, base loss: 15370.31
[INFO 2017-06-29 18:05:14,222 main.py:57] epoch 1111, training loss: 7510.42, average training loss: 8082.31, base loss: 15368.41
[INFO 2017-06-29 18:05:17,178 main.py:57] epoch 1112, training loss: 7885.23, average training loss: 8081.37, base loss: 15368.04
[INFO 2017-06-29 18:05:20,142 main.py:57] epoch 1113, training loss: 8214.61, average training loss: 8080.63, base loss: 15368.60
[INFO 2017-06-29 18:05:23,139 main.py:57] epoch 1114, training loss: 7123.80, average training loss: 8078.26, base loss: 15368.60
[INFO 2017-06-29 18:05:26,106 main.py:57] epoch 1115, training loss: 7588.26, average training loss: 8076.41, base loss: 15368.48
[INFO 2017-06-29 18:05:29,045 main.py:57] epoch 1116, training loss: 7945.90, average training loss: 8076.04, base loss: 15368.78
[INFO 2017-06-29 18:05:31,969 main.py:57] epoch 1117, training loss: 8133.76, average training loss: 8075.18, base loss: 15371.45
[INFO 2017-06-29 18:05:34,957 main.py:57] epoch 1118, training loss: 6895.83, average training loss: 8073.07, base loss: 15371.51
[INFO 2017-06-29 18:05:37,890 main.py:57] epoch 1119, training loss: 7261.97, average training loss: 8071.66, base loss: 15371.17
[INFO 2017-06-29 18:05:40,836 main.py:57] epoch 1120, training loss: 9047.45, average training loss: 8071.43, base loss: 15374.23
[INFO 2017-06-29 18:05:43,764 main.py:57] epoch 1121, training loss: 7110.14, average training loss: 8070.59, base loss: 15374.40
[INFO 2017-06-29 18:05:46,633 main.py:57] epoch 1122, training loss: 7575.89, average training loss: 8069.73, base loss: 15373.63
[INFO 2017-06-29 18:05:49,675 main.py:57] epoch 1123, training loss: 7295.80, average training loss: 8067.56, base loss: 15373.76
[INFO 2017-06-29 18:05:52,638 main.py:57] epoch 1124, training loss: 7809.57, average training loss: 8067.06, base loss: 15373.90
[INFO 2017-06-29 18:05:55,666 main.py:57] epoch 1125, training loss: 9007.45, average training loss: 8067.86, base loss: 15377.24
[INFO 2017-06-29 18:05:58,735 main.py:57] epoch 1126, training loss: 6884.74, average training loss: 8064.31, base loss: 15374.75
[INFO 2017-06-29 18:06:01,652 main.py:57] epoch 1127, training loss: 6898.38, average training loss: 8063.03, base loss: 15372.56
[INFO 2017-06-29 18:06:04,659 main.py:57] epoch 1128, training loss: 7199.94, average training loss: 8062.47, base loss: 15371.43
[INFO 2017-06-29 18:06:07,691 main.py:57] epoch 1129, training loss: 8094.30, average training loss: 8061.65, base loss: 15372.54
[INFO 2017-06-29 18:06:10,585 main.py:57] epoch 1130, training loss: 8012.17, average training loss: 8060.58, base loss: 15372.97
[INFO 2017-06-29 18:06:13,560 main.py:57] epoch 1131, training loss: 7928.17, average training loss: 8058.99, base loss: 15375.37
[INFO 2017-06-29 18:06:16,529 main.py:57] epoch 1132, training loss: 6894.82, average training loss: 8056.89, base loss: 15374.67
[INFO 2017-06-29 18:06:19,521 main.py:57] epoch 1133, training loss: 7559.09, average training loss: 8056.13, base loss: 15375.45
[INFO 2017-06-29 18:06:22,498 main.py:57] epoch 1134, training loss: 7979.84, average training loss: 8054.74, base loss: 15375.51
[INFO 2017-06-29 18:06:25,529 main.py:57] epoch 1135, training loss: 8192.63, average training loss: 8054.64, base loss: 15374.70
[INFO 2017-06-29 18:06:28,526 main.py:57] epoch 1136, training loss: 7833.76, average training loss: 8053.32, base loss: 15374.70
[INFO 2017-06-29 18:06:31,435 main.py:57] epoch 1137, training loss: 7328.41, average training loss: 8051.75, base loss: 15375.24
[INFO 2017-06-29 18:06:34,415 main.py:57] epoch 1138, training loss: 8585.56, average training loss: 8051.34, base loss: 15376.93
[INFO 2017-06-29 18:06:37,367 main.py:57] epoch 1139, training loss: 8562.22, average training loss: 8052.21, base loss: 15377.43
[INFO 2017-06-29 18:06:40,415 main.py:57] epoch 1140, training loss: 8853.30, average training loss: 8053.50, base loss: 15379.84
[INFO 2017-06-29 18:06:43,373 main.py:57] epoch 1141, training loss: 7975.90, average training loss: 8052.18, base loss: 15379.78
[INFO 2017-06-29 18:06:46,305 main.py:57] epoch 1142, training loss: 7976.24, average training loss: 8051.21, base loss: 15379.49
[INFO 2017-06-29 18:06:49,324 main.py:57] epoch 1143, training loss: 7361.41, average training loss: 8049.86, base loss: 15378.81
[INFO 2017-06-29 18:06:52,238 main.py:57] epoch 1144, training loss: 8395.46, average training loss: 8049.32, base loss: 15379.51
[INFO 2017-06-29 18:06:55,192 main.py:57] epoch 1145, training loss: 8599.61, average training loss: 8049.37, base loss: 15380.44
[INFO 2017-06-29 18:06:58,235 main.py:57] epoch 1146, training loss: 7504.93, average training loss: 8047.48, base loss: 15378.43
[INFO 2017-06-29 18:07:01,300 main.py:57] epoch 1147, training loss: 8240.49, average training loss: 8046.65, base loss: 15379.21
[INFO 2017-06-29 18:07:04,252 main.py:57] epoch 1148, training loss: 8090.99, average training loss: 8045.22, base loss: 15379.96
[INFO 2017-06-29 18:07:07,249 main.py:57] epoch 1149, training loss: 6687.18, average training loss: 8043.95, base loss: 15376.99
[INFO 2017-06-29 18:07:10,153 main.py:57] epoch 1150, training loss: 7447.83, average training loss: 8040.95, base loss: 15377.18
[INFO 2017-06-29 18:07:13,053 main.py:57] epoch 1151, training loss: 7581.42, average training loss: 8038.51, base loss: 15376.38
[INFO 2017-06-29 18:07:16,005 main.py:57] epoch 1152, training loss: 7172.60, average training loss: 8036.75, base loss: 15374.64
[INFO 2017-06-29 18:07:18,923 main.py:57] epoch 1153, training loss: 7526.34, average training loss: 8035.87, base loss: 15373.85
[INFO 2017-06-29 18:07:21,956 main.py:57] epoch 1154, training loss: 7021.50, average training loss: 8034.74, base loss: 15371.33
[INFO 2017-06-29 18:07:24,952 main.py:57] epoch 1155, training loss: 8794.39, average training loss: 8034.07, base loss: 15374.77
[INFO 2017-06-29 18:07:27,915 main.py:57] epoch 1156, training loss: 6732.39, average training loss: 8030.47, base loss: 15373.31
[INFO 2017-06-29 18:07:30,860 main.py:57] epoch 1157, training loss: 7419.93, average training loss: 8028.67, base loss: 15372.21
[INFO 2017-06-29 18:07:33,815 main.py:57] epoch 1158, training loss: 6867.09, average training loss: 8026.08, base loss: 15371.92
[INFO 2017-06-29 18:07:36,773 main.py:57] epoch 1159, training loss: 8094.14, average training loss: 8025.12, base loss: 15371.49
[INFO 2017-06-29 18:07:39,716 main.py:57] epoch 1160, training loss: 7179.57, average training loss: 8023.14, base loss: 15370.44
[INFO 2017-06-29 18:07:42,684 main.py:57] epoch 1161, training loss: 7671.37, average training loss: 8022.26, base loss: 15372.16
[INFO 2017-06-29 18:07:45,691 main.py:57] epoch 1162, training loss: 7949.72, average training loss: 8020.79, base loss: 15373.01
[INFO 2017-06-29 18:07:48,735 main.py:57] epoch 1163, training loss: 8108.24, average training loss: 8019.72, base loss: 15375.01
[INFO 2017-06-29 18:07:51,647 main.py:57] epoch 1164, training loss: 7397.41, average training loss: 8018.94, base loss: 15373.70
[INFO 2017-06-29 18:07:54,630 main.py:57] epoch 1165, training loss: 8284.83, average training loss: 8019.24, base loss: 15372.78
[INFO 2017-06-29 18:07:57,583 main.py:57] epoch 1166, training loss: 7625.08, average training loss: 8018.09, base loss: 15373.63
[INFO 2017-06-29 18:08:00,528 main.py:57] epoch 1167, training loss: 7662.45, average training loss: 8017.83, base loss: 15374.61
[INFO 2017-06-29 18:08:03,520 main.py:57] epoch 1168, training loss: 7828.20, average training loss: 8017.06, base loss: 15374.16
[INFO 2017-06-29 18:08:06,503 main.py:57] epoch 1169, training loss: 7065.82, average training loss: 8015.70, base loss: 15371.50
[INFO 2017-06-29 18:08:09,586 main.py:57] epoch 1170, training loss: 7297.63, average training loss: 8014.34, base loss: 15370.65
[INFO 2017-06-29 18:08:12,558 main.py:57] epoch 1171, training loss: 7967.00, average training loss: 8013.18, base loss: 15370.54
[INFO 2017-06-29 18:08:15,563 main.py:57] epoch 1172, training loss: 8009.50, average training loss: 8012.52, base loss: 15371.74
[INFO 2017-06-29 18:08:18,555 main.py:57] epoch 1173, training loss: 7621.51, average training loss: 8012.84, base loss: 15371.68
[INFO 2017-06-29 18:08:21,537 main.py:57] epoch 1174, training loss: 7401.06, average training loss: 8011.97, base loss: 15372.45
[INFO 2017-06-29 18:08:24,478 main.py:57] epoch 1175, training loss: 7041.24, average training loss: 8010.38, base loss: 15372.71
[INFO 2017-06-29 18:08:27,422 main.py:57] epoch 1176, training loss: 7727.11, average training loss: 8008.34, base loss: 15371.44
[INFO 2017-06-29 18:08:30,369 main.py:57] epoch 1177, training loss: 7106.59, average training loss: 8007.41, base loss: 15370.44
[INFO 2017-06-29 18:08:33,391 main.py:57] epoch 1178, training loss: 8982.30, average training loss: 8008.36, base loss: 15371.49
[INFO 2017-06-29 18:08:36,348 main.py:57] epoch 1179, training loss: 7966.15, average training loss: 8007.95, base loss: 15371.63
[INFO 2017-06-29 18:08:39,392 main.py:57] epoch 1180, training loss: 7683.42, average training loss: 8006.65, base loss: 15372.73
[INFO 2017-06-29 18:08:42,334 main.py:57] epoch 1181, training loss: 6334.83, average training loss: 8003.67, base loss: 15370.98
[INFO 2017-06-29 18:08:45,312 main.py:57] epoch 1182, training loss: 7418.03, average training loss: 8002.88, base loss: 15369.12
[INFO 2017-06-29 18:08:48,313 main.py:57] epoch 1183, training loss: 7779.93, average training loss: 8002.07, base loss: 15369.25
[INFO 2017-06-29 18:08:51,302 main.py:57] epoch 1184, training loss: 7707.01, average training loss: 8000.89, base loss: 15370.11
[INFO 2017-06-29 18:08:54,313 main.py:57] epoch 1185, training loss: 7471.55, average training loss: 7999.90, base loss: 15370.58
[INFO 2017-06-29 18:08:57,275 main.py:57] epoch 1186, training loss: 8477.92, average training loss: 8000.32, base loss: 15373.58
[INFO 2017-06-29 18:09:00,209 main.py:57] epoch 1187, training loss: 8365.83, average training loss: 7998.67, base loss: 15374.10
[INFO 2017-06-29 18:09:03,257 main.py:57] epoch 1188, training loss: 7457.07, average training loss: 7997.78, base loss: 15374.32
[INFO 2017-06-29 18:09:06,155 main.py:57] epoch 1189, training loss: 7328.78, average training loss: 7995.86, base loss: 15373.09
[INFO 2017-06-29 18:09:09,155 main.py:57] epoch 1190, training loss: 7591.13, average training loss: 7994.57, base loss: 15372.44
[INFO 2017-06-29 18:09:12,053 main.py:57] epoch 1191, training loss: 7835.98, average training loss: 7994.04, base loss: 15371.47
[INFO 2017-06-29 18:09:14,972 main.py:57] epoch 1192, training loss: 7728.52, average training loss: 7993.47, base loss: 15370.10
[INFO 2017-06-29 18:09:17,897 main.py:57] epoch 1193, training loss: 7119.47, average training loss: 7992.53, base loss: 15368.91
[INFO 2017-06-29 18:09:20,858 main.py:57] epoch 1194, training loss: 7207.19, average training loss: 7991.04, base loss: 15367.99
[INFO 2017-06-29 18:09:23,883 main.py:57] epoch 1195, training loss: 7114.78, average training loss: 7989.06, base loss: 15367.19
[INFO 2017-06-29 18:09:26,850 main.py:57] epoch 1196, training loss: 7520.46, average training loss: 7987.01, base loss: 15366.89
[INFO 2017-06-29 18:09:29,761 main.py:57] epoch 1197, training loss: 7744.00, average training loss: 7986.68, base loss: 15366.03
[INFO 2017-06-29 18:09:32,718 main.py:57] epoch 1198, training loss: 8134.15, average training loss: 7985.90, base loss: 15367.29
[INFO 2017-06-29 18:09:35,625 main.py:57] epoch 1199, training loss: 7920.67, average training loss: 7985.05, base loss: 15368.62
[INFO 2017-06-29 18:09:35,626 main.py:59] epoch 1199, testing
[INFO 2017-06-29 18:09:47,950 main.py:104] average testing loss: 8074.80, base loss: 16253.98
[INFO 2017-06-29 18:09:47,950 main.py:105] improve_loss: 8179.17, improve_percent: 0.50
[INFO 2017-06-29 18:09:47,952 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 18:09:47,978 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 18:09:50,911 main.py:57] epoch 1200, training loss: 8505.79, average training loss: 7984.76, base loss: 15370.56
[INFO 2017-06-29 18:09:53,877 main.py:57] epoch 1201, training loss: 9147.89, average training loss: 7985.88, base loss: 15372.93
[INFO 2017-06-29 18:09:56,802 main.py:57] epoch 1202, training loss: 8106.90, average training loss: 7985.32, base loss: 15371.29
[INFO 2017-06-29 18:09:59,887 main.py:57] epoch 1203, training loss: 8411.11, average training loss: 7985.34, base loss: 15371.34
[INFO 2017-06-29 18:10:02,819 main.py:57] epoch 1204, training loss: 8385.56, average training loss: 7984.15, base loss: 15373.87
[INFO 2017-06-29 18:10:05,877 main.py:57] epoch 1205, training loss: 7704.00, average training loss: 7983.62, base loss: 15373.15
[INFO 2017-06-29 18:10:08,897 main.py:57] epoch 1206, training loss: 8569.69, average training loss: 7983.41, base loss: 15375.27
[INFO 2017-06-29 18:10:11,840 main.py:57] epoch 1207, training loss: 7969.33, average training loss: 7982.38, base loss: 15373.79
[INFO 2017-06-29 18:10:14,923 main.py:57] epoch 1208, training loss: 8938.68, average training loss: 7983.02, base loss: 15375.57
[INFO 2017-06-29 18:10:17,878 main.py:57] epoch 1209, training loss: 7597.87, average training loss: 7981.48, base loss: 15374.39
[INFO 2017-06-29 18:10:20,881 main.py:57] epoch 1210, training loss: 7477.78, average training loss: 7980.71, base loss: 15373.91
[INFO 2017-06-29 18:10:23,797 main.py:57] epoch 1211, training loss: 8807.69, average training loss: 7981.68, base loss: 15373.88
[INFO 2017-06-29 18:10:26,726 main.py:57] epoch 1212, training loss: 7723.30, average training loss: 7982.01, base loss: 15373.13
[INFO 2017-06-29 18:10:29,700 main.py:57] epoch 1213, training loss: 7690.53, average training loss: 7980.56, base loss: 15372.63
[INFO 2017-06-29 18:10:32,715 main.py:57] epoch 1214, training loss: 7327.38, average training loss: 7979.73, base loss: 15371.57
[INFO 2017-06-29 18:10:35,651 main.py:57] epoch 1215, training loss: 7076.66, average training loss: 7978.04, base loss: 15369.70
[INFO 2017-06-29 18:10:38,579 main.py:57] epoch 1216, training loss: 7353.62, average training loss: 7976.44, base loss: 15368.84
[INFO 2017-06-29 18:10:41,543 main.py:57] epoch 1217, training loss: 7046.55, average training loss: 7974.26, base loss: 15367.13
[INFO 2017-06-29 18:10:44,580 main.py:57] epoch 1218, training loss: 9121.29, average training loss: 7973.92, base loss: 15371.44
[INFO 2017-06-29 18:10:47,592 main.py:57] epoch 1219, training loss: 8055.38, average training loss: 7974.40, base loss: 15373.47
[INFO 2017-06-29 18:10:50,578 main.py:57] epoch 1220, training loss: 8201.80, average training loss: 7974.34, base loss: 15375.73
[INFO 2017-06-29 18:10:53,607 main.py:57] epoch 1221, training loss: 7915.05, average training loss: 7973.78, base loss: 15377.84
[INFO 2017-06-29 18:10:56,606 main.py:57] epoch 1222, training loss: 8620.83, average training loss: 7973.79, base loss: 15379.49
[INFO 2017-06-29 18:10:59,580 main.py:57] epoch 1223, training loss: 7576.76, average training loss: 7973.14, base loss: 15378.16
[INFO 2017-06-29 18:11:02,533 main.py:57] epoch 1224, training loss: 7656.90, average training loss: 7971.22, base loss: 15377.67
[INFO 2017-06-29 18:11:05,557 main.py:57] epoch 1225, training loss: 7330.48, average training loss: 7969.71, base loss: 15375.89
[INFO 2017-06-29 18:11:08,575 main.py:57] epoch 1226, training loss: 8015.66, average training loss: 7969.27, base loss: 15377.66
[INFO 2017-06-29 18:11:11,549 main.py:57] epoch 1227, training loss: 8283.40, average training loss: 7969.25, base loss: 15380.25
[INFO 2017-06-29 18:11:14,587 main.py:57] epoch 1228, training loss: 8035.86, average training loss: 7968.47, base loss: 15381.42
[INFO 2017-06-29 18:11:17,510 main.py:57] epoch 1229, training loss: 7699.76, average training loss: 7967.21, base loss: 15382.73
[INFO 2017-06-29 18:11:20,456 main.py:57] epoch 1230, training loss: 8428.54, average training loss: 7966.51, base loss: 15383.98
[INFO 2017-06-29 18:11:23,429 main.py:57] epoch 1231, training loss: 7749.38, average training loss: 7966.19, base loss: 15384.81
[INFO 2017-06-29 18:11:26,407 main.py:57] epoch 1232, training loss: 7424.60, average training loss: 7963.98, base loss: 15383.78
[INFO 2017-06-29 18:11:29,427 main.py:57] epoch 1233, training loss: 7447.27, average training loss: 7962.73, base loss: 15382.26
[INFO 2017-06-29 18:11:32,404 main.py:57] epoch 1234, training loss: 6716.76, average training loss: 7961.71, base loss: 15380.41
[INFO 2017-06-29 18:11:35,412 main.py:57] epoch 1235, training loss: 8551.18, average training loss: 7961.06, base loss: 15383.33
[INFO 2017-06-29 18:11:38,352 main.py:57] epoch 1236, training loss: 6923.67, average training loss: 7959.48, base loss: 15382.00
[INFO 2017-06-29 18:11:41,319 main.py:57] epoch 1237, training loss: 7937.51, average training loss: 7958.25, base loss: 15382.06
[INFO 2017-06-29 18:11:44,247 main.py:57] epoch 1238, training loss: 7563.41, average training loss: 7957.81, base loss: 15381.06
[INFO 2017-06-29 18:11:47,223 main.py:57] epoch 1239, training loss: 7055.06, average training loss: 7955.17, base loss: 15379.44
[INFO 2017-06-29 18:11:50,192 main.py:57] epoch 1240, training loss: 8003.50, average training loss: 7953.80, base loss: 15380.35
[INFO 2017-06-29 18:11:53,124 main.py:57] epoch 1241, training loss: 7382.49, average training loss: 7952.98, base loss: 15381.20
[INFO 2017-06-29 18:11:56,210 main.py:57] epoch 1242, training loss: 7961.55, average training loss: 7952.36, base loss: 15381.52
[INFO 2017-06-29 18:11:59,135 main.py:57] epoch 1243, training loss: 7999.42, average training loss: 7952.16, base loss: 15380.16
[INFO 2017-06-29 18:12:02,122 main.py:57] epoch 1244, training loss: 7480.19, average training loss: 7951.41, base loss: 15379.98
[INFO 2017-06-29 18:12:05,219 main.py:57] epoch 1245, training loss: 7182.13, average training loss: 7950.52, base loss: 15379.72
[INFO 2017-06-29 18:12:08,176 main.py:57] epoch 1246, training loss: 7709.39, average training loss: 7949.41, base loss: 15379.92
[INFO 2017-06-29 18:12:11,137 main.py:57] epoch 1247, training loss: 7710.59, average training loss: 7947.88, base loss: 15378.82
[INFO 2017-06-29 18:12:14,104 main.py:57] epoch 1248, training loss: 7889.60, average training loss: 7948.10, base loss: 15381.03
[INFO 2017-06-29 18:12:17,030 main.py:57] epoch 1249, training loss: 8590.95, average training loss: 7948.18, base loss: 15383.84
[INFO 2017-06-29 18:12:20,032 main.py:57] epoch 1250, training loss: 7620.62, average training loss: 7945.75, base loss: 15383.03
[INFO 2017-06-29 18:12:22,971 main.py:57] epoch 1251, training loss: 7305.04, average training loss: 7945.01, base loss: 15382.21
[INFO 2017-06-29 18:12:25,974 main.py:57] epoch 1252, training loss: 9580.88, average training loss: 7945.89, base loss: 15387.13
[INFO 2017-06-29 18:12:28,948 main.py:57] epoch 1253, training loss: 7573.34, average training loss: 7945.29, base loss: 15387.24
[INFO 2017-06-29 18:12:31,879 main.py:57] epoch 1254, training loss: 7299.85, average training loss: 7943.40, base loss: 15387.40
[INFO 2017-06-29 18:12:34,863 main.py:57] epoch 1255, training loss: 7769.52, average training loss: 7943.26, base loss: 15388.88
[INFO 2017-06-29 18:12:37,863 main.py:57] epoch 1256, training loss: 8055.21, average training loss: 7942.22, base loss: 15389.10
[INFO 2017-06-29 18:12:40,767 main.py:57] epoch 1257, training loss: 7104.35, average training loss: 7941.08, base loss: 15388.95
[INFO 2017-06-29 18:12:43,830 main.py:57] epoch 1258, training loss: 8161.48, average training loss: 7941.14, base loss: 15388.44
[INFO 2017-06-29 18:12:46,835 main.py:57] epoch 1259, training loss: 7047.49, average training loss: 7939.49, base loss: 15386.04
[INFO 2017-06-29 18:12:49,779 main.py:57] epoch 1260, training loss: 7645.10, average training loss: 7938.41, base loss: 15386.79
[INFO 2017-06-29 18:12:52,687 main.py:57] epoch 1261, training loss: 7045.56, average training loss: 7937.00, base loss: 15387.20
[INFO 2017-06-29 18:12:55,623 main.py:57] epoch 1262, training loss: 7805.42, average training loss: 7935.54, base loss: 15389.28
[INFO 2017-06-29 18:12:58,544 main.py:57] epoch 1263, training loss: 7498.78, average training loss: 7933.48, base loss: 15389.50
[INFO 2017-06-29 18:13:01,487 main.py:57] epoch 1264, training loss: 7940.86, average training loss: 7933.03, base loss: 15390.09
[INFO 2017-06-29 18:13:04,499 main.py:57] epoch 1265, training loss: 8410.78, average training loss: 7933.94, base loss: 15392.20
[INFO 2017-06-29 18:13:07,480 main.py:57] epoch 1266, training loss: 7526.98, average training loss: 7932.64, base loss: 15392.25
[INFO 2017-06-29 18:13:10,456 main.py:57] epoch 1267, training loss: 8205.02, average training loss: 7932.04, base loss: 15394.02
[INFO 2017-06-29 18:13:13,319 main.py:57] epoch 1268, training loss: 7999.72, average training loss: 7932.16, base loss: 15393.32
[INFO 2017-06-29 18:13:16,317 main.py:57] epoch 1269, training loss: 7820.29, average training loss: 7931.00, base loss: 15393.06
[INFO 2017-06-29 18:13:19,239 main.py:57] epoch 1270, training loss: 8419.95, average training loss: 7931.09, base loss: 15392.83
[INFO 2017-06-29 18:13:22,226 main.py:57] epoch 1271, training loss: 7932.12, average training loss: 7930.83, base loss: 15392.71
[INFO 2017-06-29 18:13:25,201 main.py:57] epoch 1272, training loss: 7593.11, average training loss: 7927.95, base loss: 15393.59
[INFO 2017-06-29 18:13:28,124 main.py:57] epoch 1273, training loss: 8372.49, average training loss: 7926.95, base loss: 15395.99
[INFO 2017-06-29 18:13:31,049 main.py:57] epoch 1274, training loss: 7688.87, average training loss: 7926.14, base loss: 15394.91
[INFO 2017-06-29 18:13:34,005 main.py:57] epoch 1275, training loss: 7611.05, average training loss: 7924.94, base loss: 15395.01
[INFO 2017-06-29 18:13:36,943 main.py:57] epoch 1276, training loss: 8333.40, average training loss: 7925.22, base loss: 15396.33
[INFO 2017-06-29 18:13:39,896 main.py:57] epoch 1277, training loss: 8018.18, average training loss: 7924.22, base loss: 15397.41
[INFO 2017-06-29 18:13:42,886 main.py:57] epoch 1278, training loss: 7277.79, average training loss: 7923.54, base loss: 15395.06
[INFO 2017-06-29 18:13:45,863 main.py:57] epoch 1279, training loss: 7879.73, average training loss: 7922.95, base loss: 15394.41
[INFO 2017-06-29 18:13:48,849 main.py:57] epoch 1280, training loss: 7595.58, average training loss: 7922.73, base loss: 15395.51
[INFO 2017-06-29 18:13:51,813 main.py:57] epoch 1281, training loss: 7249.88, average training loss: 7920.64, base loss: 15395.28
[INFO 2017-06-29 18:13:54,766 main.py:57] epoch 1282, training loss: 8780.75, average training loss: 7920.60, base loss: 15398.80
[INFO 2017-06-29 18:13:57,745 main.py:57] epoch 1283, training loss: 7792.80, average training loss: 7919.55, base loss: 15400.22
[INFO 2017-06-29 18:14:00,786 main.py:57] epoch 1284, training loss: 7032.30, average training loss: 7918.98, base loss: 15398.40
[INFO 2017-06-29 18:14:03,710 main.py:57] epoch 1285, training loss: 8407.40, average training loss: 7920.14, base loss: 15399.83
[INFO 2017-06-29 18:14:06,655 main.py:57] epoch 1286, training loss: 8029.44, average training loss: 7919.12, base loss: 15400.13
[INFO 2017-06-29 18:14:09,525 main.py:57] epoch 1287, training loss: 8452.51, average training loss: 7919.33, base loss: 15402.58
[INFO 2017-06-29 18:14:12,583 main.py:57] epoch 1288, training loss: 8111.87, average training loss: 7918.93, base loss: 15403.97
[INFO 2017-06-29 18:14:15,612 main.py:57] epoch 1289, training loss: 7015.65, average training loss: 7917.52, base loss: 15404.21
[INFO 2017-06-29 18:14:18,629 main.py:57] epoch 1290, training loss: 7143.59, average training loss: 7916.25, base loss: 15402.94
[INFO 2017-06-29 18:14:21,557 main.py:57] epoch 1291, training loss: 7206.94, average training loss: 7915.61, base loss: 15402.10
[INFO 2017-06-29 18:14:24,612 main.py:57] epoch 1292, training loss: 7425.40, average training loss: 7915.27, base loss: 15400.85
[INFO 2017-06-29 18:14:27,662 main.py:57] epoch 1293, training loss: 8192.22, average training loss: 7915.12, base loss: 15401.89
[INFO 2017-06-29 18:14:30,608 main.py:57] epoch 1294, training loss: 8807.83, average training loss: 7915.25, base loss: 15403.84
[INFO 2017-06-29 18:14:33,562 main.py:57] epoch 1295, training loss: 7654.87, average training loss: 7915.31, base loss: 15402.55
[INFO 2017-06-29 18:14:36,518 main.py:57] epoch 1296, training loss: 7806.88, average training loss: 7915.19, base loss: 15401.72
[INFO 2017-06-29 18:14:39,546 main.py:57] epoch 1297, training loss: 8130.07, average training loss: 7915.75, base loss: 15401.05
[INFO 2017-06-29 18:14:42,505 main.py:57] epoch 1298, training loss: 7454.97, average training loss: 7915.43, base loss: 15401.36
[INFO 2017-06-29 18:14:45,513 main.py:57] epoch 1299, training loss: 7969.16, average training loss: 7915.25, base loss: 15401.07
[INFO 2017-06-29 18:14:45,514 main.py:59] epoch 1299, testing
[INFO 2017-06-29 18:14:57,954 main.py:104] average testing loss: 8346.09, base loss: 16369.49
[INFO 2017-06-29 18:14:57,954 main.py:105] improve_loss: 8023.40, improve_percent: 0.49
[INFO 2017-06-29 18:14:57,956 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 18:15:00,836 main.py:57] epoch 1300, training loss: 8404.50, average training loss: 7914.93, base loss: 15402.23
[INFO 2017-06-29 18:15:03,810 main.py:57] epoch 1301, training loss: 6899.90, average training loss: 7913.44, base loss: 15399.18
[INFO 2017-06-29 18:15:06,883 main.py:57] epoch 1302, training loss: 7701.00, average training loss: 7913.38, base loss: 15399.85
[INFO 2017-06-29 18:15:10,053 main.py:57] epoch 1303, training loss: 7048.92, average training loss: 7912.80, base loss: 15399.43
[INFO 2017-06-29 18:15:13,000 main.py:57] epoch 1304, training loss: 7204.38, average training loss: 7909.91, base loss: 15397.96
[INFO 2017-06-29 18:15:15,982 main.py:57] epoch 1305, training loss: 8215.38, average training loss: 7910.37, base loss: 15398.41
[INFO 2017-06-29 18:15:18,962 main.py:57] epoch 1306, training loss: 7449.07, average training loss: 7909.47, base loss: 15398.29
[INFO 2017-06-29 18:15:21,890 main.py:57] epoch 1307, training loss: 7670.83, average training loss: 7909.86, base loss: 15397.25
[INFO 2017-06-29 18:15:24,856 main.py:57] epoch 1308, training loss: 7838.68, average training loss: 7909.65, base loss: 15397.98
[INFO 2017-06-29 18:15:27,876 main.py:57] epoch 1309, training loss: 7947.23, average training loss: 7910.37, base loss: 15399.65
[INFO 2017-06-29 18:15:30,789 main.py:57] epoch 1310, training loss: 7904.01, average training loss: 7910.46, base loss: 15399.09
[INFO 2017-06-29 18:15:33,916 main.py:57] epoch 1311, training loss: 7674.00, average training loss: 7910.48, base loss: 15399.37
[INFO 2017-06-29 18:15:36,857 main.py:57] epoch 1312, training loss: 7705.46, average training loss: 7910.19, base loss: 15399.90
[INFO 2017-06-29 18:15:39,902 main.py:57] epoch 1313, training loss: 7150.19, average training loss: 7908.29, base loss: 15398.12
[INFO 2017-06-29 18:15:42,797 main.py:57] epoch 1314, training loss: 7299.69, average training loss: 7907.03, base loss: 15397.45
[INFO 2017-06-29 18:15:45,776 main.py:57] epoch 1315, training loss: 7731.12, average training loss: 7906.99, base loss: 15398.17
[INFO 2017-06-29 18:15:48,728 main.py:57] epoch 1316, training loss: 7224.45, average training loss: 7907.15, base loss: 15396.36
[INFO 2017-06-29 18:15:51,711 main.py:57] epoch 1317, training loss: 7203.14, average training loss: 7905.37, base loss: 15393.64
[INFO 2017-06-29 18:15:54,679 main.py:57] epoch 1318, training loss: 7470.50, average training loss: 7904.66, base loss: 15393.76
[INFO 2017-06-29 18:15:57,811 main.py:57] epoch 1319, training loss: 7679.14, average training loss: 7905.15, base loss: 15393.15
[INFO 2017-06-29 18:16:00,832 main.py:57] epoch 1320, training loss: 7135.60, average training loss: 7904.69, base loss: 15390.53
[INFO 2017-06-29 18:16:03,850 main.py:57] epoch 1321, training loss: 6759.87, average training loss: 7903.69, base loss: 15387.60
[INFO 2017-06-29 18:16:06,777 main.py:57] epoch 1322, training loss: 7553.18, average training loss: 7902.36, base loss: 15387.75
[INFO 2017-06-29 18:16:09,786 main.py:57] epoch 1323, training loss: 6961.18, average training loss: 7900.53, base loss: 15387.46
[INFO 2017-06-29 18:16:12,781 main.py:57] epoch 1324, training loss: 7576.87, average training loss: 7899.22, base loss: 15386.12
[INFO 2017-06-29 18:16:15,758 main.py:57] epoch 1325, training loss: 8089.56, average training loss: 7899.53, base loss: 15386.81
[INFO 2017-06-29 18:16:18,801 main.py:57] epoch 1326, training loss: 8341.12, average training loss: 7898.75, base loss: 15388.76
[INFO 2017-06-29 18:16:21,835 main.py:57] epoch 1327, training loss: 8336.22, average training loss: 7899.15, base loss: 15389.90
[INFO 2017-06-29 18:16:24,786 main.py:57] epoch 1328, training loss: 7199.38, average training loss: 7898.16, base loss: 15388.97
[INFO 2017-06-29 18:16:27,760 main.py:57] epoch 1329, training loss: 7773.82, average training loss: 7897.83, base loss: 15387.99
[INFO 2017-06-29 18:16:30,725 main.py:57] epoch 1330, training loss: 8312.22, average training loss: 7898.94, base loss: 15389.27
[INFO 2017-06-29 18:16:33,743 main.py:57] epoch 1331, training loss: 7857.40, average training loss: 7899.15, base loss: 15389.48
[INFO 2017-06-29 18:16:36,723 main.py:57] epoch 1332, training loss: 8449.36, average training loss: 7898.34, base loss: 15392.43
[INFO 2017-06-29 18:16:39,837 main.py:57] epoch 1333, training loss: 6697.61, average training loss: 7897.79, base loss: 15390.59
[INFO 2017-06-29 18:16:42,872 main.py:57] epoch 1334, training loss: 7901.56, average training loss: 7896.45, base loss: 15391.12
[INFO 2017-06-29 18:16:45,834 main.py:57] epoch 1335, training loss: 8098.51, average training loss: 7896.10, base loss: 15391.58
[INFO 2017-06-29 18:16:48,800 main.py:57] epoch 1336, training loss: 7630.98, average training loss: 7895.91, base loss: 15389.88
[INFO 2017-06-29 18:16:51,888 main.py:57] epoch 1337, training loss: 7612.50, average training loss: 7895.56, base loss: 15388.90
[INFO 2017-06-29 18:16:54,989 main.py:57] epoch 1338, training loss: 7652.92, average training loss: 7894.60, base loss: 15390.48
[INFO 2017-06-29 18:16:57,978 main.py:57] epoch 1339, training loss: 7703.65, average training loss: 7892.86, base loss: 15391.81
[INFO 2017-06-29 18:17:00,991 main.py:57] epoch 1340, training loss: 8362.28, average training loss: 7893.71, base loss: 15391.66
[INFO 2017-06-29 18:17:03,960 main.py:57] epoch 1341, training loss: 8030.00, average training loss: 7893.51, base loss: 15390.44
[INFO 2017-06-29 18:17:07,083 main.py:57] epoch 1342, training loss: 7212.32, average training loss: 7893.47, base loss: 15388.87
[INFO 2017-06-29 18:17:10,000 main.py:57] epoch 1343, training loss: 7523.81, average training loss: 7892.98, base loss: 15388.95
[INFO 2017-06-29 18:17:13,045 main.py:57] epoch 1344, training loss: 7708.76, average training loss: 7892.67, base loss: 15388.89
[INFO 2017-06-29 18:17:16,024 main.py:57] epoch 1345, training loss: 7003.33, average training loss: 7891.80, base loss: 15388.90
[INFO 2017-06-29 18:17:19,084 main.py:57] epoch 1346, training loss: 8074.65, average training loss: 7890.44, base loss: 15389.94
[INFO 2017-06-29 18:17:22,057 main.py:57] epoch 1347, training loss: 8137.75, average training loss: 7890.36, base loss: 15390.01
[INFO 2017-06-29 18:17:25,071 main.py:57] epoch 1348, training loss: 7783.75, average training loss: 7889.95, base loss: 15390.90
[INFO 2017-06-29 18:17:28,047 main.py:57] epoch 1349, training loss: 7235.81, average training loss: 7889.43, base loss: 15389.41
[INFO 2017-06-29 18:17:31,072 main.py:57] epoch 1350, training loss: 7645.62, average training loss: 7888.35, base loss: 15389.99
[INFO 2017-06-29 18:17:34,115 main.py:57] epoch 1351, training loss: 7480.58, average training loss: 7886.48, base loss: 15390.79
[INFO 2017-06-29 18:17:37,065 main.py:57] epoch 1352, training loss: 7954.03, average training loss: 7886.26, base loss: 15392.53
[INFO 2017-06-29 18:17:40,087 main.py:57] epoch 1353, training loss: 9232.42, average training loss: 7887.16, base loss: 15395.77
[INFO 2017-06-29 18:17:43,093 main.py:57] epoch 1354, training loss: 7586.80, average training loss: 7886.38, base loss: 15395.82
[INFO 2017-06-29 18:17:46,166 main.py:57] epoch 1355, training loss: 8360.03, average training loss: 7886.03, base loss: 15397.42
[INFO 2017-06-29 18:17:49,255 main.py:57] epoch 1356, training loss: 7237.28, average training loss: 7885.15, base loss: 15395.69
[INFO 2017-06-29 18:17:52,256 main.py:57] epoch 1357, training loss: 7605.94, average training loss: 7884.12, base loss: 15396.42
[INFO 2017-06-29 18:17:55,237 main.py:57] epoch 1358, training loss: 8013.58, average training loss: 7883.35, base loss: 15396.36
[INFO 2017-06-29 18:17:58,275 main.py:57] epoch 1359, training loss: 8525.50, average training loss: 7884.19, base loss: 15397.77
[INFO 2017-06-29 18:18:01,221 main.py:57] epoch 1360, training loss: 7542.44, average training loss: 7884.49, base loss: 15397.17
[INFO 2017-06-29 18:18:04,210 main.py:57] epoch 1361, training loss: 7784.90, average training loss: 7884.41, base loss: 15396.89
[INFO 2017-06-29 18:18:07,203 main.py:57] epoch 1362, training loss: 7956.26, average training loss: 7884.11, base loss: 15398.23
[INFO 2017-06-29 18:18:10,114 main.py:57] epoch 1363, training loss: 7323.47, average training loss: 7883.98, base loss: 15397.58
[INFO 2017-06-29 18:18:13,094 main.py:57] epoch 1364, training loss: 8878.77, average training loss: 7885.28, base loss: 15399.41
[INFO 2017-06-29 18:18:16,088 main.py:57] epoch 1365, training loss: 7266.42, average training loss: 7884.15, base loss: 15397.15
[INFO 2017-06-29 18:18:19,018 main.py:57] epoch 1366, training loss: 7690.67, average training loss: 7883.27, base loss: 15396.34
[INFO 2017-06-29 18:18:22,012 main.py:57] epoch 1367, training loss: 7218.12, average training loss: 7881.93, base loss: 15395.74
[INFO 2017-06-29 18:18:25,003 main.py:57] epoch 1368, training loss: 7702.00, average training loss: 7880.68, base loss: 15396.83
[INFO 2017-06-29 18:18:28,016 main.py:57] epoch 1369, training loss: 7954.22, average training loss: 7880.23, base loss: 15396.63
[INFO 2017-06-29 18:18:31,006 main.py:57] epoch 1370, training loss: 6805.20, average training loss: 7878.77, base loss: 15395.80
[INFO 2017-06-29 18:18:33,931 main.py:57] epoch 1371, training loss: 8506.11, average training loss: 7877.84, base loss: 15398.10
[INFO 2017-06-29 18:18:36,923 main.py:57] epoch 1372, training loss: 8263.23, average training loss: 7878.63, base loss: 15400.51
[INFO 2017-06-29 18:18:39,872 main.py:57] epoch 1373, training loss: 6695.67, average training loss: 7877.35, base loss: 15399.70
[INFO 2017-06-29 18:18:42,828 main.py:57] epoch 1374, training loss: 6769.83, average training loss: 7876.68, base loss: 15398.72
[INFO 2017-06-29 18:18:45,796 main.py:57] epoch 1375, training loss: 8437.79, average training loss: 7877.30, base loss: 15400.60
[INFO 2017-06-29 18:18:48,748 main.py:57] epoch 1376, training loss: 8008.29, average training loss: 7877.63, base loss: 15399.88
[INFO 2017-06-29 18:18:51,762 main.py:57] epoch 1377, training loss: 7434.18, average training loss: 7877.17, base loss: 15398.12
[INFO 2017-06-29 18:18:54,750 main.py:57] epoch 1378, training loss: 7970.70, average training loss: 7876.46, base loss: 15400.13
[INFO 2017-06-29 18:18:57,800 main.py:57] epoch 1379, training loss: 7859.12, average training loss: 7875.50, base loss: 15400.80
[INFO 2017-06-29 18:19:00,799 main.py:57] epoch 1380, training loss: 8239.53, average training loss: 7876.25, base loss: 15400.85
[INFO 2017-06-29 18:19:03,796 main.py:57] epoch 1381, training loss: 7353.81, average training loss: 7876.51, base loss: 15399.34
[INFO 2017-06-29 18:19:06,754 main.py:57] epoch 1382, training loss: 8473.77, average training loss: 7876.70, base loss: 15401.53
[INFO 2017-06-29 18:19:09,882 main.py:57] epoch 1383, training loss: 8179.21, average training loss: 7877.80, base loss: 15402.24
[INFO 2017-06-29 18:19:12,888 main.py:57] epoch 1384, training loss: 7748.63, average training loss: 7876.54, base loss: 15403.60
[INFO 2017-06-29 18:19:15,873 main.py:57] epoch 1385, training loss: 8418.81, average training loss: 7876.12, base loss: 15404.99
[INFO 2017-06-29 18:19:18,872 main.py:57] epoch 1386, training loss: 7236.78, average training loss: 7875.58, base loss: 15403.57
[INFO 2017-06-29 18:19:21,972 main.py:57] epoch 1387, training loss: 7934.44, average training loss: 7875.21, base loss: 15404.40
[INFO 2017-06-29 18:19:24,973 main.py:57] epoch 1388, training loss: 7126.98, average training loss: 7874.72, base loss: 15403.46
[INFO 2017-06-29 18:19:28,070 main.py:57] epoch 1389, training loss: 8685.76, average training loss: 7874.56, base loss: 15404.65
[INFO 2017-06-29 18:19:31,068 main.py:57] epoch 1390, training loss: 7845.46, average training loss: 7874.97, base loss: 15405.32
[INFO 2017-06-29 18:19:34,056 main.py:57] epoch 1391, training loss: 7515.28, average training loss: 7874.76, base loss: 15405.43
[INFO 2017-06-29 18:19:37,160 main.py:57] epoch 1392, training loss: 7217.14, average training loss: 7874.12, base loss: 15403.91
[INFO 2017-06-29 18:19:40,152 main.py:57] epoch 1393, training loss: 8048.93, average training loss: 7874.33, base loss: 15404.39
[INFO 2017-06-29 18:19:43,226 main.py:57] epoch 1394, training loss: 8398.53, average training loss: 7874.83, base loss: 15404.52
[INFO 2017-06-29 18:19:46,261 main.py:57] epoch 1395, training loss: 8580.46, average training loss: 7875.35, base loss: 15405.15
[INFO 2017-06-29 18:19:49,140 main.py:57] epoch 1396, training loss: 7286.42, average training loss: 7874.69, base loss: 15405.39
[INFO 2017-06-29 18:19:52,083 main.py:57] epoch 1397, training loss: 8011.83, average training loss: 7873.68, base loss: 15406.53
[INFO 2017-06-29 18:19:54,991 main.py:57] epoch 1398, training loss: 7956.71, average training loss: 7872.63, base loss: 15406.00
[INFO 2017-06-29 18:19:57,954 main.py:57] epoch 1399, training loss: 7719.04, average training loss: 7872.02, base loss: 15406.22
[INFO 2017-06-29 18:19:57,955 main.py:59] epoch 1399, testing
[INFO 2017-06-29 18:20:10,471 main.py:104] average testing loss: 7931.24, base loss: 15813.95
[INFO 2017-06-29 18:20:10,472 main.py:105] improve_loss: 7882.71, improve_percent: 0.50
[INFO 2017-06-29 18:20:10,473 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 18:20:13,490 main.py:57] epoch 1400, training loss: 8026.54, average training loss: 7870.24, base loss: 15406.06
[INFO 2017-06-29 18:20:16,516 main.py:57] epoch 1401, training loss: 8006.19, average training loss: 7868.94, base loss: 15406.78
[INFO 2017-06-29 18:20:19,433 main.py:57] epoch 1402, training loss: 8189.48, average training loss: 7867.94, base loss: 15408.83
[INFO 2017-06-29 18:20:22,359 main.py:57] epoch 1403, training loss: 8492.85, average training loss: 7867.75, base loss: 15409.26
[INFO 2017-06-29 18:20:25,313 main.py:57] epoch 1404, training loss: 7897.90, average training loss: 7868.14, base loss: 15409.04
[INFO 2017-06-29 18:20:28,261 main.py:57] epoch 1405, training loss: 7082.52, average training loss: 7866.35, base loss: 15408.54
[INFO 2017-06-29 18:20:31,186 main.py:57] epoch 1406, training loss: 7783.49, average training loss: 7866.56, base loss: 15408.72
[INFO 2017-06-29 18:20:34,119 main.py:57] epoch 1407, training loss: 7732.97, average training loss: 7866.21, base loss: 15408.39
[INFO 2017-06-29 18:20:37,079 main.py:57] epoch 1408, training loss: 8043.73, average training loss: 7866.10, base loss: 15410.36
[INFO 2017-06-29 18:20:40,009 main.py:57] epoch 1409, training loss: 6836.89, average training loss: 7864.89, base loss: 15409.13
[INFO 2017-06-29 18:20:43,022 main.py:57] epoch 1410, training loss: 7154.25, average training loss: 7862.44, base loss: 15408.84
[INFO 2017-06-29 18:20:45,960 main.py:57] epoch 1411, training loss: 7726.04, average training loss: 7863.00, base loss: 15408.33
[INFO 2017-06-29 18:20:48,945 main.py:57] epoch 1412, training loss: 8785.01, average training loss: 7863.66, base loss: 15410.24
[INFO 2017-06-29 18:20:51,971 main.py:57] epoch 1413, training loss: 7887.79, average training loss: 7863.11, base loss: 15411.67
[INFO 2017-06-29 18:20:54,977 main.py:57] epoch 1414, training loss: 7690.33, average training loss: 7862.87, base loss: 15412.50
[INFO 2017-06-29 18:20:57,949 main.py:57] epoch 1415, training loss: 8083.07, average training loss: 7863.30, base loss: 15413.84
[INFO 2017-06-29 18:21:00,901 main.py:57] epoch 1416, training loss: 7108.91, average training loss: 7863.16, base loss: 15412.70
[INFO 2017-06-29 18:21:03,892 main.py:57] epoch 1417, training loss: 6615.81, average training loss: 7862.24, base loss: 15409.16
[INFO 2017-06-29 18:21:06,871 main.py:57] epoch 1418, training loss: 8573.30, average training loss: 7863.49, base loss: 15411.12
[INFO 2017-06-29 18:21:09,825 main.py:57] epoch 1419, training loss: 7819.37, average training loss: 7864.06, base loss: 15411.53
[INFO 2017-06-29 18:21:12,740 main.py:57] epoch 1420, training loss: 6912.48, average training loss: 7861.98, base loss: 15411.01
[INFO 2017-06-29 18:21:15,741 main.py:57] epoch 1421, training loss: 7102.89, average training loss: 7861.73, base loss: 15410.85
[INFO 2017-06-29 18:21:18,753 main.py:57] epoch 1422, training loss: 8746.66, average training loss: 7862.34, base loss: 15412.02
[INFO 2017-06-29 18:21:21,703 main.py:57] epoch 1423, training loss: 7663.71, average training loss: 7862.13, base loss: 15412.93
[INFO 2017-06-29 18:21:24,728 main.py:57] epoch 1424, training loss: 7521.47, average training loss: 7861.23, base loss: 15411.01
[INFO 2017-06-29 18:21:27,707 main.py:57] epoch 1425, training loss: 7988.82, average training loss: 7862.19, base loss: 15410.93
[INFO 2017-06-29 18:21:30,674 main.py:57] epoch 1426, training loss: 6943.37, average training loss: 7861.60, base loss: 15408.65
[INFO 2017-06-29 18:21:33,798 main.py:57] epoch 1427, training loss: 7507.04, average training loss: 7861.00, base loss: 15408.72
[INFO 2017-06-29 18:21:36,803 main.py:57] epoch 1428, training loss: 8542.33, average training loss: 7861.60, base loss: 15409.85
[INFO 2017-06-29 18:21:39,749 main.py:57] epoch 1429, training loss: 8213.79, average training loss: 7861.76, base loss: 15412.11
[INFO 2017-06-29 18:21:42,735 main.py:57] epoch 1430, training loss: 7618.95, average training loss: 7860.38, base loss: 15412.22
[INFO 2017-06-29 18:21:45,670 main.py:57] epoch 1431, training loss: 7489.50, average training loss: 7860.32, base loss: 15411.41
[INFO 2017-06-29 18:21:48,776 main.py:57] epoch 1432, training loss: 7468.71, average training loss: 7859.23, base loss: 15410.82
[INFO 2017-06-29 18:21:51,797 main.py:57] epoch 1433, training loss: 7122.56, average training loss: 7858.01, base loss: 15410.89
[INFO 2017-06-29 18:21:54,806 main.py:57] epoch 1434, training loss: 7607.31, average training loss: 7857.92, base loss: 15409.34
[INFO 2017-06-29 18:21:57,859 main.py:57] epoch 1435, training loss: 8528.62, average training loss: 7858.68, base loss: 15410.65
[INFO 2017-06-29 18:22:01,007 main.py:57] epoch 1436, training loss: 7853.23, average training loss: 7859.61, base loss: 15411.51
[INFO 2017-06-29 18:22:03,955 main.py:57] epoch 1437, training loss: 8095.58, average training loss: 7859.04, base loss: 15413.05
[INFO 2017-06-29 18:22:06,985 main.py:57] epoch 1438, training loss: 8018.55, average training loss: 7857.98, base loss: 15413.94
[INFO 2017-06-29 18:22:09,957 main.py:57] epoch 1439, training loss: 7236.70, average training loss: 7857.68, base loss: 15412.83
[INFO 2017-06-29 18:22:12,901 main.py:57] epoch 1440, training loss: 7523.62, average training loss: 7855.43, base loss: 15410.91
[INFO 2017-06-29 18:22:15,892 main.py:57] epoch 1441, training loss: 8061.41, average training loss: 7854.87, base loss: 15410.78
[INFO 2017-06-29 18:22:18,865 main.py:57] epoch 1442, training loss: 7872.86, average training loss: 7854.97, base loss: 15410.23
[INFO 2017-06-29 18:22:21,950 main.py:57] epoch 1443, training loss: 7819.55, average training loss: 7853.80, base loss: 15409.65
[INFO 2017-06-29 18:22:24,948 main.py:57] epoch 1444, training loss: 7982.42, average training loss: 7853.37, base loss: 15410.14
[INFO 2017-06-29 18:22:27,850 main.py:57] epoch 1445, training loss: 7914.22, average training loss: 7852.85, base loss: 15410.28
[INFO 2017-06-29 18:22:30,828 main.py:57] epoch 1446, training loss: 6836.60, average training loss: 7851.36, base loss: 15409.18
[INFO 2017-06-29 18:22:33,818 main.py:57] epoch 1447, training loss: 8243.92, average training loss: 7850.95, base loss: 15411.55
[INFO 2017-06-29 18:22:36,817 main.py:57] epoch 1448, training loss: 7338.84, average training loss: 7850.50, base loss: 15410.15
[INFO 2017-06-29 18:22:39,816 main.py:57] epoch 1449, training loss: 7041.10, average training loss: 7849.61, base loss: 15406.05
[INFO 2017-06-29 18:22:42,799 main.py:57] epoch 1450, training loss: 7559.28, average training loss: 7849.84, base loss: 15407.36
[INFO 2017-06-29 18:22:45,803 main.py:57] epoch 1451, training loss: 7946.87, average training loss: 7849.13, base loss: 15406.59
[INFO 2017-06-29 18:22:48,909 main.py:57] epoch 1452, training loss: 7458.21, average training loss: 7848.37, base loss: 15406.84
[INFO 2017-06-29 18:22:51,905 main.py:57] epoch 1453, training loss: 7802.38, average training loss: 7847.87, base loss: 15407.43
[INFO 2017-06-29 18:22:55,004 main.py:57] epoch 1454, training loss: 7807.92, average training loss: 7847.97, base loss: 15407.98
[INFO 2017-06-29 18:22:57,934 main.py:57] epoch 1455, training loss: 7219.60, average training loss: 7847.37, base loss: 15405.56
[INFO 2017-06-29 18:23:00,857 main.py:57] epoch 1456, training loss: 7107.52, average training loss: 7846.83, base loss: 15405.54
[INFO 2017-06-29 18:23:03,799 main.py:57] epoch 1457, training loss: 7865.37, average training loss: 7846.81, base loss: 15405.53
[INFO 2017-06-29 18:23:06,775 main.py:57] epoch 1458, training loss: 8535.42, average training loss: 7846.22, base loss: 15405.62
[INFO 2017-06-29 18:23:09,832 main.py:57] epoch 1459, training loss: 7104.30, average training loss: 7845.42, base loss: 15403.46
[INFO 2017-06-29 18:23:12,771 main.py:57] epoch 1460, training loss: 7268.18, average training loss: 7845.04, base loss: 15401.81
[INFO 2017-06-29 18:23:15,774 main.py:57] epoch 1461, training loss: 7435.65, average training loss: 7844.66, base loss: 15399.51
[INFO 2017-06-29 18:23:18,749 main.py:57] epoch 1462, training loss: 7249.15, average training loss: 7843.38, base loss: 15399.67
[INFO 2017-06-29 18:23:21,789 main.py:57] epoch 1463, training loss: 8937.68, average training loss: 7843.59, base loss: 15404.19
[INFO 2017-06-29 18:23:24,820 main.py:57] epoch 1464, training loss: 7204.93, average training loss: 7843.01, base loss: 15402.95
[INFO 2017-06-29 18:23:27,762 main.py:57] epoch 1465, training loss: 7050.08, average training loss: 7842.26, base loss: 15401.59
[INFO 2017-06-29 18:23:30,808 main.py:57] epoch 1466, training loss: 8278.08, average training loss: 7842.75, base loss: 15404.59
[INFO 2017-06-29 18:23:33,905 main.py:57] epoch 1467, training loss: 6716.14, average training loss: 7841.95, base loss: 15403.08
[INFO 2017-06-29 18:23:36,913 main.py:57] epoch 1468, training loss: 8731.03, average training loss: 7842.41, base loss: 15404.22
[INFO 2017-06-29 18:23:39,992 main.py:57] epoch 1469, training loss: 6725.31, average training loss: 7841.55, base loss: 15402.51
[INFO 2017-06-29 18:23:42,964 main.py:57] epoch 1470, training loss: 7215.37, average training loss: 7840.41, base loss: 15402.10
[INFO 2017-06-29 18:23:45,964 main.py:57] epoch 1471, training loss: 7755.83, average training loss: 7840.45, base loss: 15402.00
[INFO 2017-06-29 18:23:48,955 main.py:57] epoch 1472, training loss: 7521.49, average training loss: 7839.31, base loss: 15401.66
[INFO 2017-06-29 18:23:52,091 main.py:57] epoch 1473, training loss: 7446.42, average training loss: 7839.22, base loss: 15401.19
[INFO 2017-06-29 18:23:55,147 main.py:57] epoch 1474, training loss: 8459.69, average training loss: 7837.71, base loss: 15402.44
[INFO 2017-06-29 18:23:58,125 main.py:57] epoch 1475, training loss: 7970.29, average training loss: 7837.68, base loss: 15401.74
[INFO 2017-06-29 18:24:01,102 main.py:57] epoch 1476, training loss: 7269.90, average training loss: 7837.26, base loss: 15400.70
[INFO 2017-06-29 18:24:04,029 main.py:57] epoch 1477, training loss: 7192.44, average training loss: 7836.24, base loss: 15399.65
[INFO 2017-06-29 18:24:07,012 main.py:57] epoch 1478, training loss: 7335.35, average training loss: 7836.09, base loss: 15400.22
[INFO 2017-06-29 18:24:10,019 main.py:57] epoch 1479, training loss: 7779.82, average training loss: 7835.44, base loss: 15400.86
[INFO 2017-06-29 18:24:13,012 main.py:57] epoch 1480, training loss: 7523.74, average training loss: 7834.27, base loss: 15399.99
[INFO 2017-06-29 18:24:16,043 main.py:57] epoch 1481, training loss: 6506.24, average training loss: 7833.80, base loss: 15398.47
[INFO 2017-06-29 18:24:19,010 main.py:57] epoch 1482, training loss: 7859.66, average training loss: 7833.71, base loss: 15398.18
[INFO 2017-06-29 18:24:22,106 main.py:57] epoch 1483, training loss: 8303.84, average training loss: 7834.38, base loss: 15398.68
[INFO 2017-06-29 18:24:25,153 main.py:57] epoch 1484, training loss: 8307.53, average training loss: 7834.93, base loss: 15401.09
[INFO 2017-06-29 18:24:28,233 main.py:57] epoch 1485, training loss: 8005.43, average training loss: 7833.74, base loss: 15402.45
[INFO 2017-06-29 18:24:31,198 main.py:57] epoch 1486, training loss: 7223.29, average training loss: 7832.75, base loss: 15401.35
[INFO 2017-06-29 18:24:34,134 main.py:57] epoch 1487, training loss: 7639.32, average training loss: 7831.58, base loss: 15400.23
[INFO 2017-06-29 18:24:37,213 main.py:57] epoch 1488, training loss: 8357.12, average training loss: 7831.72, base loss: 15400.72
[INFO 2017-06-29 18:24:40,198 main.py:57] epoch 1489, training loss: 7829.05, average training loss: 7831.40, base loss: 15400.00
[INFO 2017-06-29 18:24:43,154 main.py:57] epoch 1490, training loss: 7323.83, average training loss: 7831.43, base loss: 15400.14
[INFO 2017-06-29 18:24:46,143 main.py:57] epoch 1491, training loss: 8070.85, average training loss: 7830.38, base loss: 15400.97
[INFO 2017-06-29 18:24:49,108 main.py:57] epoch 1492, training loss: 7457.74, average training loss: 7830.29, base loss: 15399.18
[INFO 2017-06-29 18:24:52,093 main.py:57] epoch 1493, training loss: 6504.01, average training loss: 7828.60, base loss: 15395.14
[INFO 2017-06-29 18:24:55,044 main.py:57] epoch 1494, training loss: 7510.49, average training loss: 7827.06, base loss: 15394.99
[INFO 2017-06-29 18:24:58,117 main.py:57] epoch 1495, training loss: 6754.02, average training loss: 7825.74, base loss: 15394.10
[INFO 2017-06-29 18:25:01,157 main.py:57] epoch 1496, training loss: 7843.12, average training loss: 7823.83, base loss: 15394.59
[INFO 2017-06-29 18:25:04,115 main.py:57] epoch 1497, training loss: 7330.45, average training loss: 7823.17, base loss: 15393.87
[INFO 2017-06-29 18:25:07,132 main.py:57] epoch 1498, training loss: 7823.63, average training loss: 7821.90, base loss: 15393.34
[INFO 2017-06-29 18:25:10,093 main.py:57] epoch 1499, training loss: 8046.33, average training loss: 7821.80, base loss: 15391.90
[INFO 2017-06-29 18:25:10,094 main.py:59] epoch 1499, testing
[INFO 2017-06-29 18:25:22,561 main.py:104] average testing loss: 7915.37, base loss: 16019.51
[INFO 2017-06-29 18:25:22,561 main.py:105] improve_loss: 8104.15, improve_percent: 0.51
[INFO 2017-06-29 18:25:22,563 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 18:25:22,590 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:25:25,664 main.py:57] epoch 1500, training loss: 8290.83, average training loss: 7821.49, base loss: 15392.92
[INFO 2017-06-29 18:25:28,611 main.py:57] epoch 1501, training loss: 7806.44, average training loss: 7821.52, base loss: 15392.83
[INFO 2017-06-29 18:25:31,618 main.py:57] epoch 1502, training loss: 7285.48, average training loss: 7821.02, base loss: 15391.76
[INFO 2017-06-29 18:25:34,583 main.py:57] epoch 1503, training loss: 7975.68, average training loss: 7820.85, base loss: 15392.22
[INFO 2017-06-29 18:25:37,572 main.py:57] epoch 1504, training loss: 7209.62, average training loss: 7819.70, base loss: 15392.69
[INFO 2017-06-29 18:25:40,619 main.py:57] epoch 1505, training loss: 7644.28, average training loss: 7818.29, base loss: 15393.36
[INFO 2017-06-29 18:25:43,584 main.py:57] epoch 1506, training loss: 6730.57, average training loss: 7817.13, base loss: 15391.83
[INFO 2017-06-29 18:25:46,584 main.py:57] epoch 1507, training loss: 7446.51, average training loss: 7814.85, base loss: 15390.91
[INFO 2017-06-29 18:25:49,531 main.py:57] epoch 1508, training loss: 7529.75, average training loss: 7813.89, base loss: 15390.40
[INFO 2017-06-29 18:25:52,589 main.py:57] epoch 1509, training loss: 8667.51, average training loss: 7814.70, base loss: 15390.93
[INFO 2017-06-29 18:25:55,542 main.py:57] epoch 1510, training loss: 7370.77, average training loss: 7813.93, base loss: 15389.86
[INFO 2017-06-29 18:25:58,494 main.py:57] epoch 1511, training loss: 7274.07, average training loss: 7813.80, base loss: 15388.49
[INFO 2017-06-29 18:26:01,550 main.py:57] epoch 1512, training loss: 7674.67, average training loss: 7813.78, base loss: 15388.61
[INFO 2017-06-29 18:26:04,508 main.py:57] epoch 1513, training loss: 7702.67, average training loss: 7813.84, base loss: 15389.39
[INFO 2017-06-29 18:26:07,463 main.py:57] epoch 1514, training loss: 6627.87, average training loss: 7812.10, base loss: 15386.31
[INFO 2017-06-29 18:26:10,508 main.py:57] epoch 1515, training loss: 7464.35, average training loss: 7810.76, base loss: 15385.51
[INFO 2017-06-29 18:26:13,586 main.py:57] epoch 1516, training loss: 6913.38, average training loss: 7809.14, base loss: 15383.91
[INFO 2017-06-29 18:26:16,675 main.py:57] epoch 1517, training loss: 8415.13, average training loss: 7809.75, base loss: 15385.10
[INFO 2017-06-29 18:26:19,658 main.py:57] epoch 1518, training loss: 7176.72, average training loss: 7808.74, base loss: 15384.90
[INFO 2017-06-29 18:26:22,724 main.py:57] epoch 1519, training loss: 7069.07, average training loss: 7808.42, base loss: 15385.31
[INFO 2017-06-29 18:26:25,858 main.py:57] epoch 1520, training loss: 7633.19, average training loss: 7808.81, base loss: 15384.50
[INFO 2017-06-29 18:26:28,956 main.py:57] epoch 1521, training loss: 7026.96, average training loss: 7807.05, base loss: 15383.21
[INFO 2017-06-29 18:26:31,914 main.py:57] epoch 1522, training loss: 6830.02, average training loss: 7806.31, base loss: 15382.48
[INFO 2017-06-29 18:26:34,908 main.py:57] epoch 1523, training loss: 7155.54, average training loss: 7805.96, base loss: 15382.82
[INFO 2017-06-29 18:26:37,850 main.py:57] epoch 1524, training loss: 7180.25, average training loss: 7804.18, base loss: 15382.24
[INFO 2017-06-29 18:26:40,875 main.py:57] epoch 1525, training loss: 7315.86, average training loss: 7804.39, base loss: 15381.94
[INFO 2017-06-29 18:26:43,868 main.py:57] epoch 1526, training loss: 7433.92, average training loss: 7803.92, base loss: 15380.36
[INFO 2017-06-29 18:26:46,830 main.py:57] epoch 1527, training loss: 7689.40, average training loss: 7803.11, base loss: 15380.64
[INFO 2017-06-29 18:26:49,863 main.py:57] epoch 1528, training loss: 7410.55, average training loss: 7802.57, base loss: 15380.62
[INFO 2017-06-29 18:26:52,873 main.py:57] epoch 1529, training loss: 7285.36, average training loss: 7802.36, base loss: 15379.69
[INFO 2017-06-29 18:26:55,916 main.py:57] epoch 1530, training loss: 6583.20, average training loss: 7801.85, base loss: 15376.52
[INFO 2017-06-29 18:26:58,964 main.py:57] epoch 1531, training loss: 6949.63, average training loss: 7801.48, base loss: 15374.98
[INFO 2017-06-29 18:27:01,963 main.py:57] epoch 1532, training loss: 7096.40, average training loss: 7800.40, base loss: 15374.75
[INFO 2017-06-29 18:27:04,914 main.py:57] epoch 1533, training loss: 7743.53, average training loss: 7799.93, base loss: 15374.48
[INFO 2017-06-29 18:27:07,911 main.py:57] epoch 1534, training loss: 7976.03, average training loss: 7800.31, base loss: 15375.69
[INFO 2017-06-29 18:27:10,940 main.py:57] epoch 1535, training loss: 8045.24, average training loss: 7800.88, base loss: 15376.54
[INFO 2017-06-29 18:27:14,034 main.py:57] epoch 1536, training loss: 6819.77, average training loss: 7800.13, base loss: 15373.79
[INFO 2017-06-29 18:27:17,147 main.py:57] epoch 1537, training loss: 8180.33, average training loss: 7799.51, base loss: 15374.11
[INFO 2017-06-29 18:27:20,094 main.py:57] epoch 1538, training loss: 7240.24, average training loss: 7798.11, base loss: 15375.14
[INFO 2017-06-29 18:27:23,035 main.py:57] epoch 1539, training loss: 8300.75, average training loss: 7797.90, base loss: 15377.20
[INFO 2017-06-29 18:27:26,025 main.py:57] epoch 1540, training loss: 8387.50, average training loss: 7797.70, base loss: 15378.58
[INFO 2017-06-29 18:27:29,062 main.py:57] epoch 1541, training loss: 7411.57, average training loss: 7795.85, base loss: 15379.00
[INFO 2017-06-29 18:27:32,002 main.py:57] epoch 1542, training loss: 8963.90, average training loss: 7797.24, base loss: 15380.07
[INFO 2017-06-29 18:27:34,981 main.py:57] epoch 1543, training loss: 8056.44, average training loss: 7797.22, base loss: 15379.20
[INFO 2017-06-29 18:27:37,948 main.py:57] epoch 1544, training loss: 6283.55, average training loss: 7795.63, base loss: 15376.84
[INFO 2017-06-29 18:27:40,933 main.py:57] epoch 1545, training loss: 9079.94, average training loss: 7796.43, base loss: 15379.69
[INFO 2017-06-29 18:27:43,910 main.py:57] epoch 1546, training loss: 8444.80, average training loss: 7796.95, base loss: 15380.86
[INFO 2017-06-29 18:27:46,832 main.py:57] epoch 1547, training loss: 8425.64, average training loss: 7796.97, base loss: 15382.74
[INFO 2017-06-29 18:27:49,785 main.py:57] epoch 1548, training loss: 7196.53, average training loss: 7796.76, base loss: 15381.33
[INFO 2017-06-29 18:27:52,799 main.py:57] epoch 1549, training loss: 7573.57, average training loss: 7795.73, base loss: 15379.54
[INFO 2017-06-29 18:27:55,720 main.py:57] epoch 1550, training loss: 7696.13, average training loss: 7795.18, base loss: 15379.72
[INFO 2017-06-29 18:27:58,693 main.py:57] epoch 1551, training loss: 7906.82, average training loss: 7794.28, base loss: 15379.92
[INFO 2017-06-29 18:28:01,684 main.py:57] epoch 1552, training loss: 7748.14, average training loss: 7793.80, base loss: 15379.62
[INFO 2017-06-29 18:28:04,664 main.py:57] epoch 1553, training loss: 7671.45, average training loss: 7792.62, base loss: 15378.00
[INFO 2017-06-29 18:28:07,635 main.py:57] epoch 1554, training loss: 7670.14, average training loss: 7791.91, base loss: 15377.68
[INFO 2017-06-29 18:28:10,643 main.py:57] epoch 1555, training loss: 7746.42, average training loss: 7790.41, base loss: 15377.95
[INFO 2017-06-29 18:28:13,632 main.py:57] epoch 1556, training loss: 7751.55, average training loss: 7789.27, base loss: 15376.95
[INFO 2017-06-29 18:28:16,752 main.py:57] epoch 1557, training loss: 7677.19, average training loss: 7788.98, base loss: 15375.65
[INFO 2017-06-29 18:28:19,688 main.py:57] epoch 1558, training loss: 6450.23, average training loss: 7787.94, base loss: 15373.64
[INFO 2017-06-29 18:28:22,618 main.py:57] epoch 1559, training loss: 6708.57, average training loss: 7785.42, base loss: 15373.77
[INFO 2017-06-29 18:28:25,579 main.py:57] epoch 1560, training loss: 7850.88, average training loss: 7784.87, base loss: 15374.53
[INFO 2017-06-29 18:28:28,455 main.py:57] epoch 1561, training loss: 8239.02, average training loss: 7784.83, base loss: 15375.75
[INFO 2017-06-29 18:28:31,494 main.py:57] epoch 1562, training loss: 8034.43, average training loss: 7784.53, base loss: 15375.82
[INFO 2017-06-29 18:28:34,457 main.py:57] epoch 1563, training loss: 6971.28, average training loss: 7784.17, base loss: 15374.37
[INFO 2017-06-29 18:28:37,419 main.py:57] epoch 1564, training loss: 7280.83, average training loss: 7783.08, base loss: 15373.25
[INFO 2017-06-29 18:28:40,423 main.py:57] epoch 1565, training loss: 7634.65, average training loss: 7782.39, base loss: 15372.30
[INFO 2017-06-29 18:28:43,360 main.py:57] epoch 1566, training loss: 7343.46, average training loss: 7781.24, base loss: 15372.27
[INFO 2017-06-29 18:28:46,411 main.py:57] epoch 1567, training loss: 7685.08, average training loss: 7780.71, base loss: 15371.51
[INFO 2017-06-29 18:28:49,376 main.py:57] epoch 1568, training loss: 7807.29, average training loss: 7779.86, base loss: 15372.83
[INFO 2017-06-29 18:28:52,362 main.py:57] epoch 1569, training loss: 7465.09, average training loss: 7779.25, base loss: 15372.48
[INFO 2017-06-29 18:28:55,307 main.py:57] epoch 1570, training loss: 6496.86, average training loss: 7777.79, base loss: 15370.71
[INFO 2017-06-29 18:28:58,237 main.py:57] epoch 1571, training loss: 7136.41, average training loss: 7777.05, base loss: 15370.32
[INFO 2017-06-29 18:29:01,273 main.py:57] epoch 1572, training loss: 8222.93, average training loss: 7777.13, base loss: 15372.56
[INFO 2017-06-29 18:29:04,268 main.py:57] epoch 1573, training loss: 7791.52, average training loss: 7776.81, base loss: 15373.16
[INFO 2017-06-29 18:29:07,223 main.py:57] epoch 1574, training loss: 7742.12, average training loss: 7776.83, base loss: 15372.80
[INFO 2017-06-29 18:29:10,222 main.py:57] epoch 1575, training loss: 7584.23, average training loss: 7777.24, base loss: 15371.25
[INFO 2017-06-29 18:29:13,183 main.py:57] epoch 1576, training loss: 7497.37, average training loss: 7776.74, base loss: 15370.27
[INFO 2017-06-29 18:29:16,236 main.py:57] epoch 1577, training loss: 7417.97, average training loss: 7777.16, base loss: 15369.03
[INFO 2017-06-29 18:29:19,172 main.py:57] epoch 1578, training loss: 7285.18, average training loss: 7777.44, base loss: 15368.08
[INFO 2017-06-29 18:29:22,141 main.py:57] epoch 1579, training loss: 7982.44, average training loss: 7776.38, base loss: 15368.44
[INFO 2017-06-29 18:29:25,118 main.py:57] epoch 1580, training loss: 7576.76, average training loss: 7774.62, base loss: 15367.31
[INFO 2017-06-29 18:29:28,085 main.py:57] epoch 1581, training loss: 7906.12, average training loss: 7773.83, base loss: 15367.97
[INFO 2017-06-29 18:29:31,035 main.py:57] epoch 1582, training loss: 7481.70, average training loss: 7773.75, base loss: 15364.77
[INFO 2017-06-29 18:29:34,014 main.py:57] epoch 1583, training loss: 8000.53, average training loss: 7773.19, base loss: 15362.95
[INFO 2017-06-29 18:29:36,979 main.py:57] epoch 1584, training loss: 6943.05, average training loss: 7771.68, base loss: 15362.13
[INFO 2017-06-29 18:29:39,988 main.py:57] epoch 1585, training loss: 8328.15, average training loss: 7772.45, base loss: 15363.17
[INFO 2017-06-29 18:29:42,950 main.py:57] epoch 1586, training loss: 7202.38, average training loss: 7771.61, base loss: 15362.72
[INFO 2017-06-29 18:29:46,011 main.py:57] epoch 1587, training loss: 7488.97, average training loss: 7770.43, base loss: 15362.37
[INFO 2017-06-29 18:29:48,920 main.py:57] epoch 1588, training loss: 7645.29, average training loss: 7769.78, base loss: 15361.39
[INFO 2017-06-29 18:29:52,014 main.py:57] epoch 1589, training loss: 8544.24, average training loss: 7770.24, base loss: 15361.41
[INFO 2017-06-29 18:29:55,031 main.py:57] epoch 1590, training loss: 7040.55, average training loss: 7769.01, base loss: 15359.69
[INFO 2017-06-29 18:29:58,076 main.py:57] epoch 1591, training loss: 8392.55, average training loss: 7769.99, base loss: 15361.08
[INFO 2017-06-29 18:30:01,116 main.py:57] epoch 1592, training loss: 7555.63, average training loss: 7769.42, base loss: 15361.17
[INFO 2017-06-29 18:30:04,096 main.py:57] epoch 1593, training loss: 7423.51, average training loss: 7768.87, base loss: 15361.28
[INFO 2017-06-29 18:30:07,048 main.py:57] epoch 1594, training loss: 8438.05, average training loss: 7768.19, base loss: 15363.17
[INFO 2017-06-29 18:30:09,971 main.py:57] epoch 1595, training loss: 8298.33, average training loss: 7767.85, base loss: 15362.77
[INFO 2017-06-29 18:30:12,993 main.py:57] epoch 1596, training loss: 7469.18, average training loss: 7767.72, base loss: 15363.62
[INFO 2017-06-29 18:30:16,016 main.py:57] epoch 1597, training loss: 8174.04, average training loss: 7768.22, base loss: 15365.26
[INFO 2017-06-29 18:30:18,939 main.py:57] epoch 1598, training loss: 7199.71, average training loss: 7767.17, base loss: 15364.21
[INFO 2017-06-29 18:30:21,859 main.py:57] epoch 1599, training loss: 7285.77, average training loss: 7765.82, base loss: 15362.33
[INFO 2017-06-29 18:30:21,860 main.py:59] epoch 1599, testing
[INFO 2017-06-29 18:30:34,179 main.py:104] average testing loss: 7944.85, base loss: 15426.96
[INFO 2017-06-29 18:30:34,180 main.py:105] improve_loss: 7482.11, improve_percent: 0.49
[INFO 2017-06-29 18:30:34,181 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:30:37,232 main.py:57] epoch 1600, training loss: 8769.11, average training loss: 7766.14, base loss: 15362.96
[INFO 2017-06-29 18:30:40,227 main.py:57] epoch 1601, training loss: 8113.90, average training loss: 7766.91, base loss: 15362.29
[INFO 2017-06-29 18:30:43,179 main.py:57] epoch 1602, training loss: 7707.74, average training loss: 7766.91, base loss: 15361.62
[INFO 2017-06-29 18:30:46,138 main.py:57] epoch 1603, training loss: 7371.61, average training loss: 7766.41, base loss: 15361.08
[INFO 2017-06-29 18:30:49,150 main.py:57] epoch 1604, training loss: 7847.82, average training loss: 7766.09, base loss: 15359.96
[INFO 2017-06-29 18:30:52,080 main.py:57] epoch 1605, training loss: 8321.78, average training loss: 7766.80, base loss: 15359.85
[INFO 2017-06-29 18:30:55,073 main.py:57] epoch 1606, training loss: 7421.65, average training loss: 7766.69, base loss: 15359.95
[INFO 2017-06-29 18:30:58,130 main.py:57] epoch 1607, training loss: 7303.98, average training loss: 7766.52, base loss: 15359.91
[INFO 2017-06-29 18:31:01,149 main.py:57] epoch 1608, training loss: 8283.87, average training loss: 7766.95, base loss: 15360.98
[INFO 2017-06-29 18:31:04,208 main.py:57] epoch 1609, training loss: 7110.83, average training loss: 7766.70, base loss: 15361.11
[INFO 2017-06-29 18:31:07,170 main.py:57] epoch 1610, training loss: 7486.45, average training loss: 7766.54, base loss: 15359.63
[INFO 2017-06-29 18:31:10,166 main.py:57] epoch 1611, training loss: 7245.51, average training loss: 7764.40, base loss: 15356.92
[INFO 2017-06-29 18:31:13,191 main.py:57] epoch 1612, training loss: 7584.24, average training loss: 7764.70, base loss: 15356.15
[INFO 2017-06-29 18:31:16,119 main.py:57] epoch 1613, training loss: 7963.78, average training loss: 7763.97, base loss: 15356.78
[INFO 2017-06-29 18:31:19,151 main.py:57] epoch 1614, training loss: 7593.90, average training loss: 7762.84, base loss: 15356.70
[INFO 2017-06-29 18:31:22,065 main.py:57] epoch 1615, training loss: 7103.85, average training loss: 7762.57, base loss: 15355.68
[INFO 2017-06-29 18:31:25,022 main.py:57] epoch 1616, training loss: 7383.83, average training loss: 7761.96, base loss: 15354.84
[INFO 2017-06-29 18:31:27,962 main.py:57] epoch 1617, training loss: 7735.31, average training loss: 7761.89, base loss: 15355.40
[INFO 2017-06-29 18:31:30,915 main.py:57] epoch 1618, training loss: 6833.14, average training loss: 7761.86, base loss: 15354.14
[INFO 2017-06-29 18:31:33,874 main.py:57] epoch 1619, training loss: 7591.44, average training loss: 7761.31, base loss: 15352.89
[INFO 2017-06-29 18:31:36,854 main.py:57] epoch 1620, training loss: 7246.33, average training loss: 7761.19, base loss: 15351.01
[INFO 2017-06-29 18:31:39,763 main.py:57] epoch 1621, training loss: 7214.72, average training loss: 7760.48, base loss: 15348.31
[INFO 2017-06-29 18:31:42,720 main.py:57] epoch 1622, training loss: 7013.97, average training loss: 7759.06, base loss: 15347.54
[INFO 2017-06-29 18:31:45,767 main.py:57] epoch 1623, training loss: 7669.78, average training loss: 7758.93, base loss: 15348.30
[INFO 2017-06-29 18:31:48,764 main.py:57] epoch 1624, training loss: 7364.48, average training loss: 7758.42, base loss: 15347.69
[INFO 2017-06-29 18:31:51,699 main.py:57] epoch 1625, training loss: 8660.38, average training loss: 7759.28, base loss: 15349.17
[INFO 2017-06-29 18:31:54,684 main.py:57] epoch 1626, training loss: 7766.92, average training loss: 7759.90, base loss: 15349.89
[INFO 2017-06-29 18:31:57,605 main.py:57] epoch 1627, training loss: 7418.16, average training loss: 7757.24, base loss: 15351.37
[INFO 2017-06-29 18:32:00,577 main.py:57] epoch 1628, training loss: 6933.37, average training loss: 7756.68, base loss: 15350.65
[INFO 2017-06-29 18:32:03,522 main.py:57] epoch 1629, training loss: 7459.82, average training loss: 7756.60, base loss: 15351.09
[INFO 2017-06-29 18:32:06,627 main.py:57] epoch 1630, training loss: 7179.95, average training loss: 7755.85, base loss: 15348.72
[INFO 2017-06-29 18:32:09,630 main.py:57] epoch 1631, training loss: 8304.48, average training loss: 7756.76, base loss: 15349.53
[INFO 2017-06-29 18:32:12,570 main.py:57] epoch 1632, training loss: 7167.45, average training loss: 7757.03, base loss: 15348.57
[INFO 2017-06-29 18:32:15,581 main.py:57] epoch 1633, training loss: 7079.54, average training loss: 7756.77, base loss: 15346.96
[INFO 2017-06-29 18:32:18,632 main.py:57] epoch 1634, training loss: 7253.50, average training loss: 7756.37, base loss: 15346.04
[INFO 2017-06-29 18:32:21,777 main.py:57] epoch 1635, training loss: 6843.22, average training loss: 7755.54, base loss: 15344.20
[INFO 2017-06-29 18:32:24,784 main.py:57] epoch 1636, training loss: 8247.97, average training loss: 7756.91, base loss: 15344.24
[INFO 2017-06-29 18:32:27,900 main.py:57] epoch 1637, training loss: 7077.06, average training loss: 7756.70, base loss: 15342.99
[INFO 2017-06-29 18:32:31,056 main.py:57] epoch 1638, training loss: 7705.92, average training loss: 7755.16, base loss: 15341.80
[INFO 2017-06-29 18:32:34,166 main.py:57] epoch 1639, training loss: 7583.61, average training loss: 7754.91, base loss: 15341.24
[INFO 2017-06-29 18:32:37,206 main.py:57] epoch 1640, training loss: 8410.76, average training loss: 7754.90, base loss: 15342.35
[INFO 2017-06-29 18:32:40,301 main.py:57] epoch 1641, training loss: 8623.86, average training loss: 7756.15, base loss: 15343.09
[INFO 2017-06-29 18:32:43,295 main.py:57] epoch 1642, training loss: 6989.05, average training loss: 7755.43, base loss: 15343.16
[INFO 2017-06-29 18:32:46,319 main.py:57] epoch 1643, training loss: 8132.79, average training loss: 7755.79, base loss: 15345.38
[INFO 2017-06-29 18:32:49,330 main.py:57] epoch 1644, training loss: 7712.77, average training loss: 7755.49, base loss: 15344.64
[INFO 2017-06-29 18:32:52,286 main.py:57] epoch 1645, training loss: 7553.94, average training loss: 7753.46, base loss: 15344.69
[INFO 2017-06-29 18:32:55,253 main.py:57] epoch 1646, training loss: 7986.38, average training loss: 7754.10, base loss: 15343.14
[INFO 2017-06-29 18:32:58,288 main.py:57] epoch 1647, training loss: 8353.13, average training loss: 7754.17, base loss: 15342.85
[INFO 2017-06-29 18:33:01,278 main.py:57] epoch 1648, training loss: 7241.67, average training loss: 7753.90, base loss: 15342.66
[INFO 2017-06-29 18:33:04,238 main.py:57] epoch 1649, training loss: 7654.34, average training loss: 7752.60, base loss: 15343.03
[INFO 2017-06-29 18:33:07,242 main.py:57] epoch 1650, training loss: 7772.75, average training loss: 7753.04, base loss: 15343.76
[INFO 2017-06-29 18:33:10,183 main.py:57] epoch 1651, training loss: 7796.62, average training loss: 7751.91, base loss: 15344.43
[INFO 2017-06-29 18:33:13,257 main.py:57] epoch 1652, training loss: 7394.79, average training loss: 7751.94, base loss: 15344.32
[INFO 2017-06-29 18:33:16,198 main.py:57] epoch 1653, training loss: 7816.10, average training loss: 7752.21, base loss: 15346.58
[INFO 2017-06-29 18:33:19,289 main.py:57] epoch 1654, training loss: 6805.84, average training loss: 7750.99, base loss: 15345.44
[INFO 2017-06-29 18:33:22,343 main.py:57] epoch 1655, training loss: 7686.58, average training loss: 7750.81, base loss: 15346.13
[INFO 2017-06-29 18:33:25,373 main.py:57] epoch 1656, training loss: 8227.13, average training loss: 7751.53, base loss: 15348.12
[INFO 2017-06-29 18:33:28,343 main.py:57] epoch 1657, training loss: 6945.24, average training loss: 7750.36, base loss: 15346.87
[INFO 2017-06-29 18:33:31,304 main.py:57] epoch 1658, training loss: 6670.06, average training loss: 7749.38, base loss: 15344.79
[INFO 2017-06-29 18:33:34,284 main.py:57] epoch 1659, training loss: 7704.38, average training loss: 7749.19, base loss: 15344.74
[INFO 2017-06-29 18:33:37,242 main.py:57] epoch 1660, training loss: 6921.18, average training loss: 7748.98, base loss: 15343.64
[INFO 2017-06-29 18:33:40,257 main.py:57] epoch 1661, training loss: 7875.26, average training loss: 7748.69, base loss: 15343.94
[INFO 2017-06-29 18:33:43,201 main.py:57] epoch 1662, training loss: 7077.62, average training loss: 7748.66, base loss: 15344.64
[INFO 2017-06-29 18:33:46,178 main.py:57] epoch 1663, training loss: 7070.81, average training loss: 7748.17, base loss: 15343.62
[INFO 2017-06-29 18:33:49,184 main.py:57] epoch 1664, training loss: 7489.50, average training loss: 7747.89, base loss: 15344.59
[INFO 2017-06-29 18:33:52,146 main.py:57] epoch 1665, training loss: 8326.81, average training loss: 7748.04, base loss: 15345.65
[INFO 2017-06-29 18:33:55,047 main.py:57] epoch 1666, training loss: 8179.71, average training loss: 7748.62, base loss: 15346.24
[INFO 2017-06-29 18:33:58,026 main.py:57] epoch 1667, training loss: 8364.57, average training loss: 7749.57, base loss: 15345.99
[INFO 2017-06-29 18:34:01,021 main.py:57] epoch 1668, training loss: 7004.58, average training loss: 7748.43, base loss: 15344.13
[INFO 2017-06-29 18:34:04,112 main.py:57] epoch 1669, training loss: 8098.24, average training loss: 7749.16, base loss: 15343.99
[INFO 2017-06-29 18:34:07,070 main.py:57] epoch 1670, training loss: 7623.79, average training loss: 7748.96, base loss: 15344.23
[INFO 2017-06-29 18:34:10,075 main.py:57] epoch 1671, training loss: 7833.26, average training loss: 7749.50, base loss: 15345.17
[INFO 2017-06-29 18:34:13,084 main.py:57] epoch 1672, training loss: 7154.89, average training loss: 7749.29, base loss: 15343.86
[INFO 2017-06-29 18:34:16,032 main.py:57] epoch 1673, training loss: 7296.77, average training loss: 7749.31, base loss: 15343.23
[INFO 2017-06-29 18:34:18,980 main.py:57] epoch 1674, training loss: 7263.13, average training loss: 7748.56, base loss: 15341.90
[INFO 2017-06-29 18:34:21,927 main.py:57] epoch 1675, training loss: 8289.58, average training loss: 7749.19, base loss: 15342.03
[INFO 2017-06-29 18:34:24,840 main.py:57] epoch 1676, training loss: 8031.24, average training loss: 7749.94, base loss: 15342.02
[INFO 2017-06-29 18:34:27,832 main.py:57] epoch 1677, training loss: 6716.01, average training loss: 7749.48, base loss: 15341.40
[INFO 2017-06-29 18:34:30,802 main.py:57] epoch 1678, training loss: 8222.94, average training loss: 7749.80, base loss: 15341.70
[INFO 2017-06-29 18:34:33,731 main.py:57] epoch 1679, training loss: 8091.96, average training loss: 7750.72, base loss: 15342.24
[INFO 2017-06-29 18:34:36,748 main.py:57] epoch 1680, training loss: 8110.04, average training loss: 7750.24, base loss: 15342.97
[INFO 2017-06-29 18:34:39,724 main.py:57] epoch 1681, training loss: 8001.43, average training loss: 7750.89, base loss: 15343.80
[INFO 2017-06-29 18:34:42,641 main.py:57] epoch 1682, training loss: 9366.69, average training loss: 7752.60, base loss: 15346.12
[INFO 2017-06-29 18:34:45,578 main.py:57] epoch 1683, training loss: 7274.98, average training loss: 7752.51, base loss: 15346.44
[INFO 2017-06-29 18:34:48,602 main.py:57] epoch 1684, training loss: 7007.95, average training loss: 7752.50, base loss: 15346.49
[INFO 2017-06-29 18:34:51,511 main.py:57] epoch 1685, training loss: 7908.21, average training loss: 7751.74, base loss: 15346.36
[INFO 2017-06-29 18:34:54,444 main.py:57] epoch 1686, training loss: 7695.15, average training loss: 7750.74, base loss: 15346.65
[INFO 2017-06-29 18:34:57,468 main.py:57] epoch 1687, training loss: 7269.10, average training loss: 7750.97, base loss: 15346.48
[INFO 2017-06-29 18:35:00,361 main.py:57] epoch 1688, training loss: 7714.07, average training loss: 7751.31, base loss: 15344.67
[INFO 2017-06-29 18:35:03,310 main.py:57] epoch 1689, training loss: 6635.30, average training loss: 7749.63, base loss: 15342.39
[INFO 2017-06-29 18:35:06,328 main.py:57] epoch 1690, training loss: 8076.55, average training loss: 7750.21, base loss: 15342.39
[INFO 2017-06-29 18:35:09,282 main.py:57] epoch 1691, training loss: 7616.26, average training loss: 7750.15, base loss: 15340.95
[INFO 2017-06-29 18:35:12,265 main.py:57] epoch 1692, training loss: 7221.25, average training loss: 7749.43, base loss: 15340.83
[INFO 2017-06-29 18:35:15,295 main.py:57] epoch 1693, training loss: 7543.75, average training loss: 7749.70, base loss: 15341.06
[INFO 2017-06-29 18:35:18,245 main.py:57] epoch 1694, training loss: 7762.30, average training loss: 7749.31, base loss: 15341.40
[INFO 2017-06-29 18:35:21,349 main.py:57] epoch 1695, training loss: 7298.33, average training loss: 7748.66, base loss: 15340.36
[INFO 2017-06-29 18:35:24,283 main.py:57] epoch 1696, training loss: 7327.02, average training loss: 7747.88, base loss: 15340.68
[INFO 2017-06-29 18:35:27,300 main.py:57] epoch 1697, training loss: 7576.08, average training loss: 7747.83, base loss: 15340.83
[INFO 2017-06-29 18:35:30,319 main.py:57] epoch 1698, training loss: 6955.03, average training loss: 7748.01, base loss: 15338.07
[INFO 2017-06-29 18:35:33,337 main.py:57] epoch 1699, training loss: 7997.75, average training loss: 7748.59, base loss: 15338.35
[INFO 2017-06-29 18:35:33,338 main.py:59] epoch 1699, testing
[INFO 2017-06-29 18:35:45,732 main.py:104] average testing loss: 8467.67, base loss: 17048.30
[INFO 2017-06-29 18:35:45,732 main.py:105] improve_loss: 8580.63, improve_percent: 0.50
[INFO 2017-06-29 18:35:45,735 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:35:48,704 main.py:57] epoch 1700, training loss: 9036.37, average training loss: 7750.32, base loss: 15339.38
[INFO 2017-06-29 18:35:51,734 main.py:57] epoch 1701, training loss: 6844.76, average training loss: 7748.63, base loss: 15337.43
[INFO 2017-06-29 18:35:54,712 main.py:57] epoch 1702, training loss: 7854.92, average training loss: 7748.19, base loss: 15338.82
[INFO 2017-06-29 18:35:57,648 main.py:57] epoch 1703, training loss: 7309.48, average training loss: 7747.41, base loss: 15339.56
[INFO 2017-06-29 18:36:00,712 main.py:57] epoch 1704, training loss: 8249.76, average training loss: 7747.91, base loss: 15340.67
[INFO 2017-06-29 18:36:03,670 main.py:57] epoch 1705, training loss: 7521.98, average training loss: 7746.91, base loss: 15339.83
[INFO 2017-06-29 18:36:06,572 main.py:57] epoch 1706, training loss: 7805.97, average training loss: 7746.86, base loss: 15340.54
[INFO 2017-06-29 18:36:09,498 main.py:57] epoch 1707, training loss: 7628.94, average training loss: 7747.22, base loss: 15340.85
[INFO 2017-06-29 18:36:12,452 main.py:57] epoch 1708, training loss: 7119.52, average training loss: 7746.24, base loss: 15340.48
[INFO 2017-06-29 18:36:15,486 main.py:57] epoch 1709, training loss: 8405.70, average training loss: 7747.57, base loss: 15341.76
[INFO 2017-06-29 18:36:18,404 main.py:57] epoch 1710, training loss: 7601.69, average training loss: 7747.13, base loss: 15341.83
[INFO 2017-06-29 18:36:21,319 main.py:57] epoch 1711, training loss: 8254.11, average training loss: 7746.57, base loss: 15343.79
[INFO 2017-06-29 18:36:24,345 main.py:57] epoch 1712, training loss: 7235.32, average training loss: 7746.53, base loss: 15344.47
[INFO 2017-06-29 18:36:27,342 main.py:57] epoch 1713, training loss: 8179.04, average training loss: 7746.46, base loss: 15346.11
[INFO 2017-06-29 18:36:30,355 main.py:57] epoch 1714, training loss: 7793.25, average training loss: 7745.35, base loss: 15346.63
[INFO 2017-06-29 18:36:33,347 main.py:57] epoch 1715, training loss: 7733.78, average training loss: 7744.17, base loss: 15347.67
[INFO 2017-06-29 18:36:36,299 main.py:57] epoch 1716, training loss: 8307.68, average training loss: 7744.69, base loss: 15348.88
[INFO 2017-06-29 18:36:39,263 main.py:57] epoch 1717, training loss: 7769.52, average training loss: 7744.61, base loss: 15350.35
[INFO 2017-06-29 18:36:42,253 main.py:57] epoch 1718, training loss: 6973.49, average training loss: 7743.31, base loss: 15349.44
[INFO 2017-06-29 18:36:45,208 main.py:57] epoch 1719, training loss: 7501.17, average training loss: 7743.42, base loss: 15349.61
[INFO 2017-06-29 18:36:48,085 main.py:57] epoch 1720, training loss: 6640.29, average training loss: 7741.70, base loss: 15348.46
[INFO 2017-06-29 18:36:51,164 main.py:57] epoch 1721, training loss: 6973.88, average training loss: 7740.04, base loss: 15346.73
[INFO 2017-06-29 18:36:54,108 main.py:57] epoch 1722, training loss: 6814.95, average training loss: 7738.74, base loss: 15344.13
[INFO 2017-06-29 18:36:57,099 main.py:57] epoch 1723, training loss: 7645.25, average training loss: 7738.91, base loss: 15342.60
[INFO 2017-06-29 18:37:00,050 main.py:57] epoch 1724, training loss: 8868.51, average training loss: 7738.60, base loss: 15343.87
[INFO 2017-06-29 18:37:03,048 main.py:57] epoch 1725, training loss: 8745.55, average training loss: 7738.49, base loss: 15344.89
[INFO 2017-06-29 18:37:05,956 main.py:57] epoch 1726, training loss: 7689.20, average training loss: 7739.09, base loss: 15343.69
[INFO 2017-06-29 18:37:08,939 main.py:57] epoch 1727, training loss: 7027.41, average training loss: 7737.31, base loss: 15343.09
[INFO 2017-06-29 18:37:11,915 main.py:57] epoch 1728, training loss: 8382.76, average training loss: 7738.63, base loss: 15343.38
[INFO 2017-06-29 18:37:14,859 main.py:57] epoch 1729, training loss: 7795.87, average training loss: 7737.87, base loss: 15345.13
[INFO 2017-06-29 18:37:17,910 main.py:57] epoch 1730, training loss: 7915.43, average training loss: 7738.46, base loss: 15346.38
[INFO 2017-06-29 18:37:20,904 main.py:57] epoch 1731, training loss: 7864.09, average training loss: 7738.10, base loss: 15346.09
[INFO 2017-06-29 18:37:23,824 main.py:57] epoch 1732, training loss: 8070.84, average training loss: 7738.35, base loss: 15345.93
[INFO 2017-06-29 18:37:26,838 main.py:57] epoch 1733, training loss: 7637.96, average training loss: 7737.16, base loss: 15345.46
[INFO 2017-06-29 18:37:29,865 main.py:57] epoch 1734, training loss: 8704.73, average training loss: 7738.43, base loss: 15348.12
[INFO 2017-06-29 18:37:32,819 main.py:57] epoch 1735, training loss: 7096.45, average training loss: 7736.61, base loss: 15348.41
[INFO 2017-06-29 18:37:35,835 main.py:57] epoch 1736, training loss: 7357.66, average training loss: 7736.86, base loss: 15348.78
[INFO 2017-06-29 18:37:38,830 main.py:57] epoch 1737, training loss: 7211.57, average training loss: 7735.37, base loss: 15349.36
[INFO 2017-06-29 18:37:41,803 main.py:57] epoch 1738, training loss: 7206.12, average training loss: 7734.66, base loss: 15349.11
[INFO 2017-06-29 18:37:44,710 main.py:57] epoch 1739, training loss: 7265.69, average training loss: 7733.51, base loss: 15349.23
[INFO 2017-06-29 18:37:47,682 main.py:57] epoch 1740, training loss: 7465.31, average training loss: 7733.50, base loss: 15348.38
[INFO 2017-06-29 18:37:50,679 main.py:57] epoch 1741, training loss: 7409.03, average training loss: 7733.05, base loss: 15348.19
[INFO 2017-06-29 18:37:53,657 main.py:57] epoch 1742, training loss: 6653.45, average training loss: 7731.28, base loss: 15346.89
[INFO 2017-06-29 18:37:56,686 main.py:57] epoch 1743, training loss: 6412.01, average training loss: 7729.73, base loss: 15345.09
[INFO 2017-06-29 18:37:59,743 main.py:57] epoch 1744, training loss: 7430.27, average training loss: 7729.81, base loss: 15344.34
[INFO 2017-06-29 18:38:02,721 main.py:57] epoch 1745, training loss: 7710.82, average training loss: 7729.41, base loss: 15343.65
[INFO 2017-06-29 18:38:05,728 main.py:57] epoch 1746, training loss: 8071.22, average training loss: 7729.77, base loss: 15344.80
[INFO 2017-06-29 18:38:08,678 main.py:57] epoch 1747, training loss: 7816.77, average training loss: 7728.89, base loss: 15345.54
[INFO 2017-06-29 18:38:11,692 main.py:57] epoch 1748, training loss: 7464.14, average training loss: 7727.84, base loss: 15345.90
[INFO 2017-06-29 18:38:14,639 main.py:57] epoch 1749, training loss: 6722.08, average training loss: 7726.64, base loss: 15344.89
[INFO 2017-06-29 18:38:17,601 main.py:57] epoch 1750, training loss: 8544.67, average training loss: 7727.58, base loss: 15345.19
[INFO 2017-06-29 18:38:20,599 main.py:57] epoch 1751, training loss: 8762.00, average training loss: 7728.90, base loss: 15345.56
[INFO 2017-06-29 18:38:23,669 main.py:57] epoch 1752, training loss: 7667.88, average training loss: 7728.33, base loss: 15345.41
[INFO 2017-06-29 18:38:26,631 main.py:57] epoch 1753, training loss: 7359.00, average training loss: 7728.05, base loss: 15346.91
[INFO 2017-06-29 18:38:29,607 main.py:57] epoch 1754, training loss: 7711.16, average training loss: 7726.77, base loss: 15345.95
[INFO 2017-06-29 18:38:32,577 main.py:57] epoch 1755, training loss: 7810.34, average training loss: 7726.44, base loss: 15344.70
[INFO 2017-06-29 18:38:35,582 main.py:57] epoch 1756, training loss: 7695.11, average training loss: 7725.67, base loss: 15345.49
[INFO 2017-06-29 18:38:38,610 main.py:57] epoch 1757, training loss: 8279.94, average training loss: 7725.73, base loss: 15346.38
[INFO 2017-06-29 18:38:41,555 main.py:57] epoch 1758, training loss: 6587.70, average training loss: 7725.04, base loss: 15344.90
[INFO 2017-06-29 18:38:44,569 main.py:57] epoch 1759, training loss: 7414.23, average training loss: 7724.99, base loss: 15345.49
[INFO 2017-06-29 18:38:47,566 main.py:57] epoch 1760, training loss: 6776.70, average training loss: 7724.36, base loss: 15345.34
[INFO 2017-06-29 18:38:50,509 main.py:57] epoch 1761, training loss: 6899.63, average training loss: 7723.46, base loss: 15344.93
[INFO 2017-06-29 18:38:53,487 main.py:57] epoch 1762, training loss: 6955.15, average training loss: 7723.16, base loss: 15345.07
[INFO 2017-06-29 18:38:56,416 main.py:57] epoch 1763, training loss: 7296.91, average training loss: 7721.94, base loss: 15345.60
[INFO 2017-06-29 18:38:59,382 main.py:57] epoch 1764, training loss: 8578.22, average training loss: 7723.32, base loss: 15347.07
[INFO 2017-06-29 18:39:02,297 main.py:57] epoch 1765, training loss: 7078.88, average training loss: 7722.01, base loss: 15346.12
[INFO 2017-06-29 18:39:05,314 main.py:57] epoch 1766, training loss: 7994.36, average training loss: 7721.34, base loss: 15346.85
[INFO 2017-06-29 18:39:08,271 main.py:57] epoch 1767, training loss: 6869.08, average training loss: 7719.97, base loss: 15346.84
[INFO 2017-06-29 18:39:11,254 main.py:57] epoch 1768, training loss: 7375.68, average training loss: 7720.65, base loss: 15347.48
[INFO 2017-06-29 18:39:14,287 main.py:57] epoch 1769, training loss: 7900.88, average training loss: 7720.69, base loss: 15347.84
[INFO 2017-06-29 18:39:17,341 main.py:57] epoch 1770, training loss: 7585.05, average training loss: 7721.26, base loss: 15348.71
[INFO 2017-06-29 18:39:20,382 main.py:57] epoch 1771, training loss: 7685.18, average training loss: 7721.48, base loss: 15348.25
[INFO 2017-06-29 18:39:23,434 main.py:57] epoch 1772, training loss: 8655.22, average training loss: 7721.97, base loss: 15350.38
[INFO 2017-06-29 18:39:26,388 main.py:57] epoch 1773, training loss: 6928.22, average training loss: 7721.03, base loss: 15350.18
[INFO 2017-06-29 18:39:29,373 main.py:57] epoch 1774, training loss: 7564.89, average training loss: 7720.00, base loss: 15349.86
[INFO 2017-06-29 18:39:32,403 main.py:57] epoch 1775, training loss: 7024.86, average training loss: 7719.03, base loss: 15349.75
[INFO 2017-06-29 18:39:35,463 main.py:57] epoch 1776, training loss: 7647.07, average training loss: 7718.18, base loss: 15348.47
[INFO 2017-06-29 18:39:38,441 main.py:57] epoch 1777, training loss: 8093.79, average training loss: 7719.53, base loss: 15347.32
[INFO 2017-06-29 18:39:41,387 main.py:57] epoch 1778, training loss: 7459.96, average training loss: 7718.84, base loss: 15346.70
[INFO 2017-06-29 18:39:44,289 main.py:57] epoch 1779, training loss: 7319.87, average training loss: 7718.63, base loss: 15346.60
[INFO 2017-06-29 18:39:47,388 main.py:57] epoch 1780, training loss: 7369.41, average training loss: 7718.42, base loss: 15345.26
[INFO 2017-06-29 18:39:50,365 main.py:57] epoch 1781, training loss: 8337.60, average training loss: 7718.20, base loss: 15346.09
[INFO 2017-06-29 18:39:53,379 main.py:57] epoch 1782, training loss: 7458.99, average training loss: 7717.94, base loss: 15346.56
[INFO 2017-06-29 18:39:56,296 main.py:57] epoch 1783, training loss: 6915.97, average training loss: 7715.87, base loss: 15346.46
[INFO 2017-06-29 18:39:59,294 main.py:57] epoch 1784, training loss: 7217.76, average training loss: 7715.01, base loss: 15346.32
[INFO 2017-06-29 18:40:02,247 main.py:57] epoch 1785, training loss: 8472.02, average training loss: 7716.29, base loss: 15347.69
[INFO 2017-06-29 18:40:05,174 main.py:57] epoch 1786, training loss: 7462.84, average training loss: 7715.86, base loss: 15348.09
[INFO 2017-06-29 18:40:08,107 main.py:57] epoch 1787, training loss: 7029.33, average training loss: 7714.44, base loss: 15347.28
[INFO 2017-06-29 18:40:11,052 main.py:57] epoch 1788, training loss: 7858.82, average training loss: 7714.31, base loss: 15347.11
[INFO 2017-06-29 18:40:14,036 main.py:57] epoch 1789, training loss: 7006.59, average training loss: 7713.21, base loss: 15346.42
[INFO 2017-06-29 18:40:17,080 main.py:57] epoch 1790, training loss: 8113.59, average training loss: 7714.09, base loss: 15345.36
[INFO 2017-06-29 18:40:20,008 main.py:57] epoch 1791, training loss: 7248.97, average training loss: 7713.62, base loss: 15342.81
[INFO 2017-06-29 18:40:22,991 main.py:57] epoch 1792, training loss: 6843.05, average training loss: 7712.92, base loss: 15342.52
[INFO 2017-06-29 18:40:25,961 main.py:57] epoch 1793, training loss: 7977.51, average training loss: 7713.75, base loss: 15342.39
[INFO 2017-06-29 18:40:28,970 main.py:57] epoch 1794, training loss: 6630.86, average training loss: 7711.23, base loss: 15342.10
[INFO 2017-06-29 18:40:31,946 main.py:57] epoch 1795, training loss: 7140.63, average training loss: 7710.16, base loss: 15341.23
[INFO 2017-06-29 18:40:34,913 main.py:57] epoch 1796, training loss: 7975.83, average training loss: 7710.56, base loss: 15342.28
[INFO 2017-06-29 18:40:37,788 main.py:57] epoch 1797, training loss: 8167.24, average training loss: 7710.74, base loss: 15343.92
[INFO 2017-06-29 18:40:40,750 main.py:57] epoch 1798, training loss: 8284.11, average training loss: 7711.04, base loss: 15343.59
[INFO 2017-06-29 18:40:43,733 main.py:57] epoch 1799, training loss: 7313.49, average training loss: 7710.54, base loss: 15341.97
[INFO 2017-06-29 18:40:43,734 main.py:59] epoch 1799, testing
[INFO 2017-06-29 18:40:55,946 main.py:104] average testing loss: 7857.70, base loss: 15850.33
[INFO 2017-06-29 18:40:55,946 main.py:105] improve_loss: 7992.64, improve_percent: 0.50
[INFO 2017-06-29 18:40:55,948 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:40:58,897 main.py:57] epoch 1800, training loss: 7375.72, average training loss: 7710.02, base loss: 15341.52
[INFO 2017-06-29 18:41:01,809 main.py:57] epoch 1801, training loss: 7757.20, average training loss: 7708.27, base loss: 15341.88
[INFO 2017-06-29 18:41:04,815 main.py:57] epoch 1802, training loss: 7173.83, average training loss: 7707.82, base loss: 15339.73
[INFO 2017-06-29 18:41:07,774 main.py:57] epoch 1803, training loss: 7386.66, average training loss: 7706.94, base loss: 15338.88
[INFO 2017-06-29 18:41:10,661 main.py:57] epoch 1804, training loss: 7640.72, average training loss: 7707.03, base loss: 15339.71
[INFO 2017-06-29 18:41:13,662 main.py:57] epoch 1805, training loss: 7304.08, average training loss: 7706.88, base loss: 15340.41
[INFO 2017-06-29 18:41:16,686 main.py:57] epoch 1806, training loss: 6743.93, average training loss: 7703.83, base loss: 15338.07
[INFO 2017-06-29 18:41:19,607 main.py:57] epoch 1807, training loss: 6978.50, average training loss: 7702.50, base loss: 15337.00
[INFO 2017-06-29 18:41:22,566 main.py:57] epoch 1808, training loss: 7897.69, average training loss: 7702.20, base loss: 15338.75
[INFO 2017-06-29 18:41:25,479 main.py:57] epoch 1809, training loss: 6769.50, average training loss: 7701.65, base loss: 15338.88
[INFO 2017-06-29 18:41:28,507 main.py:57] epoch 1810, training loss: 6782.18, average training loss: 7701.03, base loss: 15337.40
[INFO 2017-06-29 18:41:31,464 main.py:57] epoch 1811, training loss: 7473.78, average training loss: 7700.88, base loss: 15337.98
[INFO 2017-06-29 18:41:34,478 main.py:57] epoch 1812, training loss: 7604.43, average training loss: 7701.19, base loss: 15337.46
[INFO 2017-06-29 18:41:37,485 main.py:57] epoch 1813, training loss: 8094.87, average training loss: 7701.62, base loss: 15337.43
[INFO 2017-06-29 18:41:40,471 main.py:57] epoch 1814, training loss: 7437.75, average training loss: 7700.69, base loss: 15337.07
[INFO 2017-06-29 18:41:43,504 main.py:57] epoch 1815, training loss: 7652.26, average training loss: 7699.91, base loss: 15337.33
[INFO 2017-06-29 18:41:46,491 main.py:57] epoch 1816, training loss: 6303.11, average training loss: 7698.70, base loss: 15334.83
[INFO 2017-06-29 18:41:49,476 main.py:57] epoch 1817, training loss: 8073.30, average training loss: 7698.30, base loss: 15335.23
[INFO 2017-06-29 18:41:52,437 main.py:57] epoch 1818, training loss: 8104.73, average training loss: 7699.12, base loss: 15336.53
[INFO 2017-06-29 18:41:55,385 main.py:57] epoch 1819, training loss: 8510.76, average training loss: 7698.80, base loss: 15337.59
[INFO 2017-06-29 18:41:58,307 main.py:57] epoch 1820, training loss: 7735.27, average training loss: 7699.71, base loss: 15338.79
[INFO 2017-06-29 18:42:01,291 main.py:57] epoch 1821, training loss: 8001.87, average training loss: 7699.88, base loss: 15340.35
[INFO 2017-06-29 18:42:04,286 main.py:57] epoch 1822, training loss: 7802.32, average training loss: 7699.84, base loss: 15340.21
[INFO 2017-06-29 18:42:07,251 main.py:57] epoch 1823, training loss: 7992.60, average training loss: 7699.66, base loss: 15340.59
[INFO 2017-06-29 18:42:10,180 main.py:57] epoch 1824, training loss: 7607.48, average training loss: 7699.79, base loss: 15340.43
[INFO 2017-06-29 18:42:13,165 main.py:57] epoch 1825, training loss: 6499.30, average training loss: 7698.90, base loss: 15339.35
[INFO 2017-06-29 18:42:16,151 main.py:57] epoch 1826, training loss: 7626.59, average training loss: 7698.78, base loss: 15339.26
[INFO 2017-06-29 18:42:19,123 main.py:57] epoch 1827, training loss: 8049.23, average training loss: 7698.36, base loss: 15339.71
[INFO 2017-06-29 18:42:22,061 main.py:57] epoch 1828, training loss: 7618.54, average training loss: 7697.25, base loss: 15339.99
[INFO 2017-06-29 18:42:25,007 main.py:57] epoch 1829, training loss: 7730.59, average training loss: 7697.49, base loss: 15340.53
[INFO 2017-06-29 18:42:27,974 main.py:57] epoch 1830, training loss: 8239.64, average training loss: 7697.68, base loss: 15340.71
[INFO 2017-06-29 18:42:30,907 main.py:57] epoch 1831, training loss: 7457.43, average training loss: 7698.52, base loss: 15340.72
[INFO 2017-06-29 18:42:33,850 main.py:57] epoch 1832, training loss: 7411.45, average training loss: 7698.63, base loss: 15339.87
[INFO 2017-06-29 18:42:36,811 main.py:57] epoch 1833, training loss: 7148.36, average training loss: 7698.62, base loss: 15339.87
[INFO 2017-06-29 18:42:39,748 main.py:57] epoch 1834, training loss: 8314.05, average training loss: 7700.38, base loss: 15341.24
[INFO 2017-06-29 18:42:42,660 main.py:57] epoch 1835, training loss: 6816.18, average training loss: 7699.30, base loss: 15339.30
[INFO 2017-06-29 18:42:45,583 main.py:57] epoch 1836, training loss: 6811.92, average training loss: 7698.57, base loss: 15338.52
[INFO 2017-06-29 18:42:48,633 main.py:57] epoch 1837, training loss: 8204.20, average training loss: 7699.18, base loss: 15339.51
[INFO 2017-06-29 18:42:51,682 main.py:57] epoch 1838, training loss: 8085.38, average training loss: 7700.13, base loss: 15340.47
[INFO 2017-06-29 18:42:54,710 main.py:57] epoch 1839, training loss: 8328.22, average training loss: 7700.82, base loss: 15341.64
[INFO 2017-06-29 18:42:57,713 main.py:57] epoch 1840, training loss: 7717.04, average training loss: 7701.23, base loss: 15341.87
[INFO 2017-06-29 18:43:00,706 main.py:57] epoch 1841, training loss: 7269.33, average training loss: 7699.51, base loss: 15341.11
[INFO 2017-06-29 18:43:03,708 main.py:57] epoch 1842, training loss: 6467.13, average training loss: 7697.67, base loss: 15339.75
[INFO 2017-06-29 18:43:06,676 main.py:57] epoch 1843, training loss: 8514.51, average training loss: 7697.05, base loss: 15340.13
[INFO 2017-06-29 18:43:09,649 main.py:57] epoch 1844, training loss: 7347.25, average training loss: 7696.55, base loss: 15339.65
[INFO 2017-06-29 18:43:12,555 main.py:57] epoch 1845, training loss: 9482.55, average training loss: 7697.96, base loss: 15342.62
[INFO 2017-06-29 18:43:15,510 main.py:57] epoch 1846, training loss: 7683.35, average training loss: 7697.77, base loss: 15343.27
[INFO 2017-06-29 18:43:18,413 main.py:57] epoch 1847, training loss: 7792.58, average training loss: 7698.57, base loss: 15344.48
[INFO 2017-06-29 18:43:21,393 main.py:57] epoch 1848, training loss: 7483.24, average training loss: 7698.50, base loss: 15344.80
[INFO 2017-06-29 18:43:24,411 main.py:57] epoch 1849, training loss: 7016.30, average training loss: 7698.16, base loss: 15343.86
[INFO 2017-06-29 18:43:27,386 main.py:57] epoch 1850, training loss: 7996.56, average training loss: 7698.79, base loss: 15344.53
[INFO 2017-06-29 18:43:30,270 main.py:57] epoch 1851, training loss: 7763.82, average training loss: 7698.89, base loss: 15345.09
[INFO 2017-06-29 18:43:33,236 main.py:57] epoch 1852, training loss: 7719.00, average training loss: 7699.66, base loss: 15347.08
[INFO 2017-06-29 18:43:36,244 main.py:57] epoch 1853, training loss: 8038.02, average training loss: 7700.09, base loss: 15347.51
[INFO 2017-06-29 18:43:39,212 main.py:57] epoch 1854, training loss: 6953.35, average training loss: 7698.13, base loss: 15347.47
[INFO 2017-06-29 18:43:42,135 main.py:57] epoch 1855, training loss: 7598.10, average training loss: 7698.31, base loss: 15348.06
[INFO 2017-06-29 18:43:45,068 main.py:57] epoch 1856, training loss: 8017.72, average training loss: 7697.53, base loss: 15348.50
[INFO 2017-06-29 18:43:48,074 main.py:57] epoch 1857, training loss: 9007.36, average training loss: 7699.05, base loss: 15350.74
[INFO 2017-06-29 18:43:51,108 main.py:57] epoch 1858, training loss: 7599.64, average training loss: 7699.09, base loss: 15351.62
[INFO 2017-06-29 18:43:54,077 main.py:57] epoch 1859, training loss: 7123.54, average training loss: 7698.36, base loss: 15351.61
[INFO 2017-06-29 18:43:57,045 main.py:57] epoch 1860, training loss: 7687.30, average training loss: 7698.00, base loss: 15352.26
[INFO 2017-06-29 18:44:00,019 main.py:57] epoch 1861, training loss: 7425.78, average training loss: 7697.10, base loss: 15351.34
[INFO 2017-06-29 18:44:03,013 main.py:57] epoch 1862, training loss: 7312.60, average training loss: 7698.09, base loss: 15350.87
[INFO 2017-06-29 18:44:05,990 main.py:57] epoch 1863, training loss: 7347.77, average training loss: 7697.62, base loss: 15351.25
[INFO 2017-06-29 18:44:08,923 main.py:57] epoch 1864, training loss: 7276.27, average training loss: 7698.36, base loss: 15350.42
[INFO 2017-06-29 18:44:11,824 main.py:57] epoch 1865, training loss: 7569.44, average training loss: 7697.92, base loss: 15351.49
[INFO 2017-06-29 18:44:14,803 main.py:57] epoch 1866, training loss: 7350.65, average training loss: 7697.49, base loss: 15351.13
[INFO 2017-06-29 18:44:17,749 main.py:57] epoch 1867, training loss: 8270.30, average training loss: 7698.86, base loss: 15353.11
[INFO 2017-06-29 18:44:20,681 main.py:57] epoch 1868, training loss: 7374.34, average training loss: 7698.91, base loss: 15353.67
[INFO 2017-06-29 18:44:23,629 main.py:57] epoch 1869, training loss: 7291.48, average training loss: 7698.92, base loss: 15354.19
[INFO 2017-06-29 18:44:26,653 main.py:57] epoch 1870, training loss: 8912.52, average training loss: 7700.20, base loss: 15355.15
[INFO 2017-06-29 18:44:29,646 main.py:57] epoch 1871, training loss: 7198.17, average training loss: 7700.26, base loss: 15353.97
[INFO 2017-06-29 18:44:32,644 main.py:57] epoch 1872, training loss: 7708.76, average training loss: 7699.82, base loss: 15352.72
[INFO 2017-06-29 18:44:35,689 main.py:57] epoch 1873, training loss: 7503.90, average training loss: 7698.64, base loss: 15352.03
[INFO 2017-06-29 18:44:38,630 main.py:57] epoch 1874, training loss: 7313.29, average training loss: 7698.36, base loss: 15351.79
[INFO 2017-06-29 18:44:41,591 main.py:57] epoch 1875, training loss: 8229.11, average training loss: 7699.58, base loss: 15353.43
[INFO 2017-06-29 18:44:44,549 main.py:57] epoch 1876, training loss: 6942.92, average training loss: 7698.69, base loss: 15352.75
[INFO 2017-06-29 18:44:47,590 main.py:57] epoch 1877, training loss: 8484.97, average training loss: 7699.21, base loss: 15354.04
[INFO 2017-06-29 18:44:50,584 main.py:57] epoch 1878, training loss: 7665.56, average training loss: 7699.21, base loss: 15353.64
[INFO 2017-06-29 18:44:53,549 main.py:57] epoch 1879, training loss: 7921.72, average training loss: 7699.42, base loss: 15352.91
[INFO 2017-06-29 18:44:56,519 main.py:57] epoch 1880, training loss: 7807.01, average training loss: 7699.30, base loss: 15353.81
[INFO 2017-06-29 18:44:59,570 main.py:57] epoch 1881, training loss: 7080.75, average training loss: 7698.86, base loss: 15353.17
[INFO 2017-06-29 18:45:02,549 main.py:57] epoch 1882, training loss: 7017.49, average training loss: 7697.50, base loss: 15353.00
[INFO 2017-06-29 18:45:05,419 main.py:57] epoch 1883, training loss: 8707.84, average training loss: 7698.01, base loss: 15356.33
[INFO 2017-06-29 18:45:08,357 main.py:57] epoch 1884, training loss: 7004.58, average training loss: 7697.98, base loss: 15355.27
[INFO 2017-06-29 18:45:11,373 main.py:57] epoch 1885, training loss: 8032.20, average training loss: 7698.00, base loss: 15355.44
[INFO 2017-06-29 18:45:14,368 main.py:57] epoch 1886, training loss: 7200.49, average training loss: 7697.98, base loss: 15355.01
[INFO 2017-06-29 18:45:17,362 main.py:57] epoch 1887, training loss: 7507.83, average training loss: 7698.49, base loss: 15355.88
[INFO 2017-06-29 18:45:20,383 main.py:57] epoch 1888, training loss: 7549.96, average training loss: 7698.71, base loss: 15356.19
[INFO 2017-06-29 18:45:23,300 main.py:57] epoch 1889, training loss: 6860.48, average training loss: 7697.60, base loss: 15355.30
[INFO 2017-06-29 18:45:26,314 main.py:57] epoch 1890, training loss: 6499.01, average training loss: 7696.87, base loss: 15354.12
[INFO 2017-06-29 18:45:29,240 main.py:57] epoch 1891, training loss: 6696.65, average training loss: 7696.36, base loss: 15353.15
[INFO 2017-06-29 18:45:32,233 main.py:57] epoch 1892, training loss: 7421.27, average training loss: 7695.86, base loss: 15353.41
[INFO 2017-06-29 18:45:35,232 main.py:57] epoch 1893, training loss: 7195.85, average training loss: 7695.60, base loss: 15351.66
[INFO 2017-06-29 18:45:38,178 main.py:57] epoch 1894, training loss: 7710.23, average training loss: 7695.80, base loss: 15352.11
[INFO 2017-06-29 18:45:41,170 main.py:57] epoch 1895, training loss: 7623.54, average training loss: 7695.28, base loss: 15351.94
[INFO 2017-06-29 18:45:44,161 main.py:57] epoch 1896, training loss: 7840.23, average training loss: 7694.86, base loss: 15353.94
[INFO 2017-06-29 18:45:47,145 main.py:57] epoch 1897, training loss: 7957.79, average training loss: 7695.08, base loss: 15354.35
[INFO 2017-06-29 18:45:50,125 main.py:57] epoch 1898, training loss: 7287.36, average training loss: 7694.72, base loss: 15355.31
[INFO 2017-06-29 18:45:53,131 main.py:57] epoch 1899, training loss: 8040.49, average training loss: 7695.88, base loss: 15356.68
[INFO 2017-06-29 18:45:53,132 main.py:59] epoch 1899, testing
[INFO 2017-06-29 18:46:05,527 main.py:104] average testing loss: 8016.05, base loss: 16289.97
[INFO 2017-06-29 18:46:05,527 main.py:105] improve_loss: 8273.92, improve_percent: 0.51
[INFO 2017-06-29 18:46:05,529 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 18:46:05,555 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:46:08,517 main.py:57] epoch 1900, training loss: 6974.81, average training loss: 7695.40, base loss: 15355.67
[INFO 2017-06-29 18:46:11,504 main.py:57] epoch 1901, training loss: 7344.43, average training loss: 7695.57, base loss: 15355.63
[INFO 2017-06-29 18:46:14,446 main.py:57] epoch 1902, training loss: 7234.75, average training loss: 7695.00, base loss: 15353.39
[INFO 2017-06-29 18:46:17,459 main.py:57] epoch 1903, training loss: 8151.69, average training loss: 7695.33, base loss: 15352.68
[INFO 2017-06-29 18:46:20,441 main.py:57] epoch 1904, training loss: 6766.83, average training loss: 7692.93, base loss: 15351.29
[INFO 2017-06-29 18:46:23,386 main.py:57] epoch 1905, training loss: 7396.51, average training loss: 7691.64, base loss: 15350.89
[INFO 2017-06-29 18:46:26,312 main.py:57] epoch 1906, training loss: 7584.58, average training loss: 7691.52, base loss: 15349.69
[INFO 2017-06-29 18:46:29,279 main.py:57] epoch 1907, training loss: 7098.54, average training loss: 7691.14, base loss: 15349.10
[INFO 2017-06-29 18:46:32,241 main.py:57] epoch 1908, training loss: 8328.56, average training loss: 7691.90, base loss: 15350.33
[INFO 2017-06-29 18:46:35,245 main.py:57] epoch 1909, training loss: 8423.83, average training loss: 7693.19, base loss: 15350.48
[INFO 2017-06-29 18:46:38,284 main.py:57] epoch 1910, training loss: 7803.03, average training loss: 7693.08, base loss: 15349.88
[INFO 2017-06-29 18:46:41,188 main.py:57] epoch 1911, training loss: 7508.66, average training loss: 7693.23, base loss: 15349.45
[INFO 2017-06-29 18:46:44,172 main.py:57] epoch 1912, training loss: 6355.05, average training loss: 7690.08, base loss: 15347.81
[INFO 2017-06-29 18:46:47,125 main.py:57] epoch 1913, training loss: 7515.64, average training loss: 7689.43, base loss: 15348.67
[INFO 2017-06-29 18:46:50,112 main.py:57] epoch 1914, training loss: 7591.11, average training loss: 7689.77, base loss: 15349.32
[INFO 2017-06-29 18:46:53,063 main.py:57] epoch 1915, training loss: 7877.88, average training loss: 7690.20, base loss: 15349.63
[INFO 2017-06-29 18:46:56,062 main.py:57] epoch 1916, training loss: 7677.30, average training loss: 7690.50, base loss: 15349.37
[INFO 2017-06-29 18:46:59,022 main.py:57] epoch 1917, training loss: 6900.59, average training loss: 7689.96, base loss: 15348.18
[INFO 2017-06-29 18:47:02,039 main.py:57] epoch 1918, training loss: 7886.89, average training loss: 7690.88, base loss: 15347.98
[INFO 2017-06-29 18:47:04,982 main.py:57] epoch 1919, training loss: 7718.35, average training loss: 7691.10, base loss: 15349.53
[INFO 2017-06-29 18:47:07,969 main.py:57] epoch 1920, training loss: 8095.60, average training loss: 7691.41, base loss: 15350.04
[INFO 2017-06-29 18:47:10,832 main.py:57] epoch 1921, training loss: 6950.69, average training loss: 7690.46, base loss: 15348.72
[INFO 2017-06-29 18:47:13,717 main.py:57] epoch 1922, training loss: 7036.48, average training loss: 7689.15, base loss: 15347.64
[INFO 2017-06-29 18:47:16,687 main.py:57] epoch 1923, training loss: 7831.96, average training loss: 7688.85, base loss: 15348.51
[INFO 2017-06-29 18:47:19,574 main.py:57] epoch 1924, training loss: 7227.72, average training loss: 7688.13, base loss: 15348.68
[INFO 2017-06-29 18:47:22,551 main.py:57] epoch 1925, training loss: 7073.74, average training loss: 7686.67, base loss: 15347.76
[INFO 2017-06-29 18:47:25,572 main.py:57] epoch 1926, training loss: 7711.65, average training loss: 7686.52, base loss: 15348.92
[INFO 2017-06-29 18:47:28,509 main.py:57] epoch 1927, training loss: 8130.10, average training loss: 7685.79, base loss: 15350.53
[INFO 2017-06-29 18:47:31,472 main.py:57] epoch 1928, training loss: 7537.12, average training loss: 7685.04, base loss: 15350.12
[INFO 2017-06-29 18:47:34,462 main.py:57] epoch 1929, training loss: 7740.71, average training loss: 7685.89, base loss: 15349.99
[INFO 2017-06-29 18:47:37,372 main.py:57] epoch 1930, training loss: 8326.17, average training loss: 7686.95, base loss: 15350.86
[INFO 2017-06-29 18:47:40,309 main.py:57] epoch 1931, training loss: 6812.93, average training loss: 7686.30, base loss: 15348.54
[INFO 2017-06-29 18:47:43,350 main.py:57] epoch 1932, training loss: 8052.10, average training loss: 7686.91, base loss: 15349.73
[INFO 2017-06-29 18:47:46,255 main.py:57] epoch 1933, training loss: 6286.72, average training loss: 7684.90, base loss: 15348.15
[INFO 2017-06-29 18:47:49,251 main.py:57] epoch 1934, training loss: 7186.23, average training loss: 7684.45, base loss: 15347.62
[INFO 2017-06-29 18:47:52,214 main.py:57] epoch 1935, training loss: 7308.57, average training loss: 7683.38, base loss: 15347.84
[INFO 2017-06-29 18:47:55,247 main.py:57] epoch 1936, training loss: 7012.49, average training loss: 7681.97, base loss: 15346.62
[INFO 2017-06-29 18:47:58,210 main.py:57] epoch 1937, training loss: 7907.48, average training loss: 7681.84, base loss: 15345.93
[INFO 2017-06-29 18:48:01,167 main.py:57] epoch 1938, training loss: 7957.98, average training loss: 7682.08, base loss: 15346.62
[INFO 2017-06-29 18:48:04,219 main.py:57] epoch 1939, training loss: 8314.50, average training loss: 7683.14, base loss: 15348.44
[INFO 2017-06-29 18:48:07,185 main.py:57] epoch 1940, training loss: 7992.75, average training loss: 7684.04, base loss: 15348.46
[INFO 2017-06-29 18:48:10,192 main.py:57] epoch 1941, training loss: 8662.89, average training loss: 7684.31, base loss: 15350.00
[INFO 2017-06-29 18:48:13,155 main.py:57] epoch 1942, training loss: 6932.15, average training loss: 7683.44, base loss: 15349.74
[INFO 2017-06-29 18:48:16,144 main.py:57] epoch 1943, training loss: 7510.59, average training loss: 7683.14, base loss: 15348.51
[INFO 2017-06-29 18:48:19,067 main.py:57] epoch 1944, training loss: 7475.64, average training loss: 7681.95, base loss: 15348.82
[INFO 2017-06-29 18:48:22,067 main.py:57] epoch 1945, training loss: 7515.47, average training loss: 7681.41, base loss: 15348.07
[INFO 2017-06-29 18:48:25,010 main.py:57] epoch 1946, training loss: 7832.83, average training loss: 7681.86, base loss: 15348.49
[INFO 2017-06-29 18:48:27,944 main.py:57] epoch 1947, training loss: 6943.06, average training loss: 7681.67, base loss: 15347.69
[INFO 2017-06-29 18:48:30,884 main.py:57] epoch 1948, training loss: 7738.27, average training loss: 7682.13, base loss: 15347.60
[INFO 2017-06-29 18:48:33,883 main.py:57] epoch 1949, training loss: 7151.25, average training loss: 7681.25, base loss: 15345.76
[INFO 2017-06-29 18:48:36,842 main.py:57] epoch 1950, training loss: 7119.53, average training loss: 7681.08, base loss: 15345.12
[INFO 2017-06-29 18:48:39,753 main.py:57] epoch 1951, training loss: 6648.93, average training loss: 7680.84, base loss: 15343.30
[INFO 2017-06-29 18:48:42,703 main.py:57] epoch 1952, training loss: 7567.95, average training loss: 7681.23, base loss: 15343.97
[INFO 2017-06-29 18:48:45,653 main.py:57] epoch 1953, training loss: 7813.29, average training loss: 7681.08, base loss: 15346.50
[INFO 2017-06-29 18:48:48,537 main.py:57] epoch 1954, training loss: 6587.04, average training loss: 7679.25, base loss: 15344.94
[INFO 2017-06-29 18:48:51,536 main.py:57] epoch 1955, training loss: 7732.22, average training loss: 7678.78, base loss: 15344.27
[INFO 2017-06-29 18:48:54,461 main.py:57] epoch 1956, training loss: 7073.73, average training loss: 7678.46, base loss: 15344.42
[INFO 2017-06-29 18:48:57,455 main.py:57] epoch 1957, training loss: 7353.08, average training loss: 7678.33, base loss: 15344.50
[INFO 2017-06-29 18:49:00,405 main.py:57] epoch 1958, training loss: 7317.49, average training loss: 7678.86, base loss: 15345.02
[INFO 2017-06-29 18:49:03,350 main.py:57] epoch 1959, training loss: 7397.75, average training loss: 7677.79, base loss: 15344.11
[INFO 2017-06-29 18:49:06,385 main.py:57] epoch 1960, training loss: 7361.85, average training loss: 7676.87, base loss: 15343.93
[INFO 2017-06-29 18:49:09,380 main.py:57] epoch 1961, training loss: 7826.07, average training loss: 7676.91, base loss: 15344.69
[INFO 2017-06-29 18:49:12,356 main.py:57] epoch 1962, training loss: 8124.93, average training loss: 7676.74, base loss: 15344.61
[INFO 2017-06-29 18:49:15,256 main.py:57] epoch 1963, training loss: 7647.79, average training loss: 7677.28, base loss: 15344.55
[INFO 2017-06-29 18:49:18,195 main.py:57] epoch 1964, training loss: 6888.64, average training loss: 7676.92, base loss: 15343.94
[INFO 2017-06-29 18:49:21,175 main.py:57] epoch 1965, training loss: 7611.55, average training loss: 7676.89, base loss: 15343.21
[INFO 2017-06-29 18:49:24,097 main.py:57] epoch 1966, training loss: 7895.71, average training loss: 7677.31, base loss: 15342.66
[INFO 2017-06-29 18:49:27,042 main.py:57] epoch 1967, training loss: 8158.17, average training loss: 7677.03, base loss: 15343.25
[INFO 2017-06-29 18:49:30,113 main.py:57] epoch 1968, training loss: 7981.57, average training loss: 7677.52, base loss: 15344.69
[INFO 2017-06-29 18:49:33,023 main.py:57] epoch 1969, training loss: 7479.38, average training loss: 7676.75, base loss: 15344.85
[INFO 2017-06-29 18:49:36,015 main.py:57] epoch 1970, training loss: 6770.31, average training loss: 7675.29, base loss: 15342.44
[INFO 2017-06-29 18:49:39,079 main.py:57] epoch 1971, training loss: 7399.57, average training loss: 7674.82, base loss: 15341.37
[INFO 2017-06-29 18:49:42,053 main.py:57] epoch 1972, training loss: 8329.12, average training loss: 7675.91, base loss: 15342.93
[INFO 2017-06-29 18:49:45,084 main.py:57] epoch 1973, training loss: 7366.75, average training loss: 7674.36, base loss: 15343.50
[INFO 2017-06-29 18:49:48,053 main.py:57] epoch 1974, training loss: 7065.32, average training loss: 7672.71, base loss: 15343.07
[INFO 2017-06-29 18:49:50,964 main.py:57] epoch 1975, training loss: 7645.93, average training loss: 7672.48, base loss: 15343.10
[INFO 2017-06-29 18:49:53,910 main.py:57] epoch 1976, training loss: 6334.75, average training loss: 7671.30, base loss: 15340.62
[INFO 2017-06-29 18:49:56,927 main.py:57] epoch 1977, training loss: 7865.35, average training loss: 7671.09, base loss: 15341.68
[INFO 2017-06-29 18:49:59,931 main.py:57] epoch 1978, training loss: 7818.38, average training loss: 7671.70, base loss: 15341.80
[INFO 2017-06-29 18:50:02,880 main.py:57] epoch 1979, training loss: 7506.33, average training loss: 7671.03, base loss: 15341.96
[INFO 2017-06-29 18:50:05,904 main.py:57] epoch 1980, training loss: 7736.75, average training loss: 7671.34, base loss: 15343.17
[INFO 2017-06-29 18:50:08,779 main.py:57] epoch 1981, training loss: 7289.59, average training loss: 7671.27, base loss: 15343.04
[INFO 2017-06-29 18:50:11,772 main.py:57] epoch 1982, training loss: 7295.79, average training loss: 7670.98, base loss: 15343.55
[INFO 2017-06-29 18:50:14,738 main.py:57] epoch 1983, training loss: 7763.86, average training loss: 7670.72, base loss: 15344.41
[INFO 2017-06-29 18:50:17,663 main.py:57] epoch 1984, training loss: 6525.80, average training loss: 7668.47, base loss: 15342.10
[INFO 2017-06-29 18:50:20,634 main.py:57] epoch 1985, training loss: 7130.41, average training loss: 7666.76, base loss: 15342.13
[INFO 2017-06-29 18:50:23,539 main.py:57] epoch 1986, training loss: 7090.18, average training loss: 7665.32, base loss: 15341.03
[INFO 2017-06-29 18:50:26,531 main.py:57] epoch 1987, training loss: 7426.77, average training loss: 7664.97, base loss: 15340.22
[INFO 2017-06-29 18:50:29,494 main.py:57] epoch 1988, training loss: 7348.75, average training loss: 7664.17, base loss: 15340.82
[INFO 2017-06-29 18:50:32,431 main.py:57] epoch 1989, training loss: 8446.79, average training loss: 7664.48, base loss: 15342.93
[INFO 2017-06-29 18:50:35,354 main.py:57] epoch 1990, training loss: 7714.95, average training loss: 7663.41, base loss: 15342.69
[INFO 2017-06-29 18:50:38,279 main.py:57] epoch 1991, training loss: 8898.99, average training loss: 7664.77, base loss: 15344.65
[INFO 2017-06-29 18:50:41,279 main.py:57] epoch 1992, training loss: 8440.86, average training loss: 7664.63, base loss: 15346.01
[INFO 2017-06-29 18:50:44,263 main.py:57] epoch 1993, training loss: 7938.06, average training loss: 7664.46, base loss: 15347.34
[INFO 2017-06-29 18:50:47,277 main.py:57] epoch 1994, training loss: 7339.91, average training loss: 7663.75, base loss: 15346.19
[INFO 2017-06-29 18:50:50,278 main.py:57] epoch 1995, training loss: 7207.76, average training loss: 7662.98, base loss: 15343.96
[INFO 2017-06-29 18:50:53,232 main.py:57] epoch 1996, training loss: 7607.44, average training loss: 7662.74, base loss: 15344.62
[INFO 2017-06-29 18:50:56,199 main.py:57] epoch 1997, training loss: 8431.68, average training loss: 7662.91, base loss: 15345.74
[INFO 2017-06-29 18:50:59,236 main.py:57] epoch 1998, training loss: 7430.87, average training loss: 7661.98, base loss: 15345.53
[INFO 2017-06-29 18:51:02,238 main.py:57] epoch 1999, training loss: 8032.15, average training loss: 7662.61, base loss: 15346.18
[INFO 2017-06-29 18:51:02,239 main.py:59] epoch 1999, testing
[INFO 2017-06-29 18:51:14,572 main.py:104] average testing loss: 7908.15, base loss: 16080.51
[INFO 2017-06-29 18:51:14,572 main.py:105] improve_loss: 8172.36, improve_percent: 0.51
[INFO 2017-06-29 18:51:14,575 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 18:51:14,601 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:51:17,530 main.py:57] epoch 2000, training loss: 8386.38, average training loss: 7662.94, base loss: 15347.21
[INFO 2017-06-29 18:51:20,485 main.py:57] epoch 2001, training loss: 7475.25, average training loss: 7663.34, base loss: 15346.56
[INFO 2017-06-29 18:51:23,429 main.py:57] epoch 2002, training loss: 7312.98, average training loss: 7663.30, base loss: 15346.52
[INFO 2017-06-29 18:51:26,407 main.py:57] epoch 2003, training loss: 6895.04, average training loss: 7662.50, base loss: 15345.88
[INFO 2017-06-29 18:51:29,369 main.py:57] epoch 2004, training loss: 7593.55, average training loss: 7662.08, base loss: 15346.06
[INFO 2017-06-29 18:51:32,316 main.py:57] epoch 2005, training loss: 8300.01, average training loss: 7662.96, base loss: 15346.00
[INFO 2017-06-29 18:51:35,243 main.py:57] epoch 2006, training loss: 6951.15, average training loss: 7661.13, base loss: 15345.95
[INFO 2017-06-29 18:51:38,268 main.py:57] epoch 2007, training loss: 6769.96, average training loss: 7659.96, base loss: 15344.27
[INFO 2017-06-29 18:51:41,195 main.py:57] epoch 2008, training loss: 7878.51, average training loss: 7660.99, base loss: 15342.96
[INFO 2017-06-29 18:51:44,220 main.py:57] epoch 2009, training loss: 7646.05, average training loss: 7660.88, base loss: 15341.77
[INFO 2017-06-29 18:51:47,211 main.py:57] epoch 2010, training loss: 6683.59, average training loss: 7660.23, base loss: 15341.13
[INFO 2017-06-29 18:51:50,214 main.py:57] epoch 2011, training loss: 6528.08, average training loss: 7657.93, base loss: 15339.65
[INFO 2017-06-29 18:51:53,139 main.py:57] epoch 2012, training loss: 6892.21, average training loss: 7657.22, base loss: 15337.34
[INFO 2017-06-29 18:51:56,148 main.py:57] epoch 2013, training loss: 7111.57, average training loss: 7656.86, base loss: 15336.61
[INFO 2017-06-29 18:51:59,169 main.py:57] epoch 2014, training loss: 7182.23, average training loss: 7656.16, base loss: 15335.09
[INFO 2017-06-29 18:52:02,142 main.py:57] epoch 2015, training loss: 7374.68, average training loss: 7655.21, base loss: 15333.83
[INFO 2017-06-29 18:52:05,046 main.py:57] epoch 2016, training loss: 6545.70, average training loss: 7654.38, base loss: 15332.86
[INFO 2017-06-29 18:52:08,009 main.py:57] epoch 2017, training loss: 8005.64, average training loss: 7654.56, base loss: 15334.59
[INFO 2017-06-29 18:52:10,968 main.py:57] epoch 2018, training loss: 6839.72, average training loss: 7653.75, base loss: 15333.33
[INFO 2017-06-29 18:52:13,940 main.py:57] epoch 2019, training loss: 7740.28, average training loss: 7654.39, base loss: 15333.82
[INFO 2017-06-29 18:52:16,888 main.py:57] epoch 2020, training loss: 7422.41, average training loss: 7654.22, base loss: 15334.82
[INFO 2017-06-29 18:52:19,888 main.py:57] epoch 2021, training loss: 6803.99, average training loss: 7653.65, base loss: 15334.65
[INFO 2017-06-29 18:52:22,909 main.py:57] epoch 2022, training loss: 6997.87, average training loss: 7653.65, base loss: 15333.12
[INFO 2017-06-29 18:52:25,925 main.py:57] epoch 2023, training loss: 8061.70, average training loss: 7653.24, base loss: 15334.09
[INFO 2017-06-29 18:52:28,956 main.py:57] epoch 2024, training loss: 7195.09, average training loss: 7651.76, base loss: 15334.06
[INFO 2017-06-29 18:52:31,916 main.py:57] epoch 2025, training loss: 7304.84, average training loss: 7651.41, base loss: 15333.62
[INFO 2017-06-29 18:52:34,874 main.py:57] epoch 2026, training loss: 7266.93, average training loss: 7651.30, base loss: 15332.68
[INFO 2017-06-29 18:52:37,809 main.py:57] epoch 2027, training loss: 8016.96, average training loss: 7652.11, base loss: 15333.74
[INFO 2017-06-29 18:52:40,837 main.py:57] epoch 2028, training loss: 7342.04, average training loss: 7651.95, base loss: 15334.95
[INFO 2017-06-29 18:52:43,761 main.py:57] epoch 2029, training loss: 7171.01, average training loss: 7650.29, base loss: 15334.44
[INFO 2017-06-29 18:52:46,734 main.py:57] epoch 2030, training loss: 7066.35, average training loss: 7650.70, base loss: 15333.60
[INFO 2017-06-29 18:52:49,702 main.py:57] epoch 2031, training loss: 7825.83, average training loss: 7651.32, base loss: 15332.54
[INFO 2017-06-29 18:52:52,641 main.py:57] epoch 2032, training loss: 7253.69, average training loss: 7651.20, base loss: 15331.48
[INFO 2017-06-29 18:52:55,590 main.py:57] epoch 2033, training loss: 7538.94, average training loss: 7651.33, base loss: 15330.77
[INFO 2017-06-29 18:52:58,537 main.py:57] epoch 2034, training loss: 8416.16, average training loss: 7652.21, base loss: 15331.34
[INFO 2017-06-29 18:53:01,455 main.py:57] epoch 2035, training loss: 7480.19, average training loss: 7651.38, base loss: 15330.37
[INFO 2017-06-29 18:53:04,449 main.py:57] epoch 2036, training loss: 8677.53, average training loss: 7653.11, base loss: 15332.71
[INFO 2017-06-29 18:53:07,425 main.py:57] epoch 2037, training loss: 7049.49, average training loss: 7652.32, base loss: 15333.02
[INFO 2017-06-29 18:53:10,365 main.py:57] epoch 2038, training loss: 7699.39, average training loss: 7650.93, base loss: 15332.29
[INFO 2017-06-29 18:53:13,323 main.py:57] epoch 2039, training loss: 7946.33, average training loss: 7650.11, base loss: 15332.29
[INFO 2017-06-29 18:53:16,247 main.py:57] epoch 2040, training loss: 8097.07, average training loss: 7649.99, base loss: 15332.14
[INFO 2017-06-29 18:53:19,192 main.py:57] epoch 2041, training loss: 7589.26, average training loss: 7648.64, base loss: 15333.27
[INFO 2017-06-29 18:53:22,058 main.py:57] epoch 2042, training loss: 8126.12, average training loss: 7649.05, base loss: 15333.93
[INFO 2017-06-29 18:53:25,061 main.py:57] epoch 2043, training loss: 7815.63, average training loss: 7648.93, base loss: 15335.11
[INFO 2017-06-29 18:53:28,010 main.py:57] epoch 2044, training loss: 7259.66, average training loss: 7648.08, base loss: 15335.12
[INFO 2017-06-29 18:53:30,970 main.py:57] epoch 2045, training loss: 6966.30, average training loss: 7648.08, base loss: 15333.84
[INFO 2017-06-29 18:53:33,978 main.py:57] epoch 2046, training loss: 7438.74, average training loss: 7646.07, base loss: 15334.32
[INFO 2017-06-29 18:53:36,982 main.py:57] epoch 2047, training loss: 7063.24, average training loss: 7644.84, base loss: 15333.32
[INFO 2017-06-29 18:53:39,958 main.py:57] epoch 2048, training loss: 8665.19, average training loss: 7646.65, base loss: 15333.93
[INFO 2017-06-29 18:53:42,965 main.py:57] epoch 2049, training loss: 7001.39, average training loss: 7645.38, base loss: 15332.60
[INFO 2017-06-29 18:53:45,819 main.py:57] epoch 2050, training loss: 7367.16, average training loss: 7645.03, base loss: 15332.13
[INFO 2017-06-29 18:53:48,740 main.py:57] epoch 2051, training loss: 6802.13, average training loss: 7645.00, base loss: 15331.16
[INFO 2017-06-29 18:53:51,679 main.py:57] epoch 2052, training loss: 7626.81, average training loss: 7644.78, base loss: 15332.04
[INFO 2017-06-29 18:53:54,616 main.py:57] epoch 2053, training loss: 7653.44, average training loss: 7644.32, base loss: 15332.82
[INFO 2017-06-29 18:53:57,569 main.py:57] epoch 2054, training loss: 7897.11, average training loss: 7644.57, base loss: 15333.92
[INFO 2017-06-29 18:54:00,538 main.py:57] epoch 2055, training loss: 7403.52, average training loss: 7644.40, base loss: 15335.07
[INFO 2017-06-29 18:54:03,668 main.py:57] epoch 2056, training loss: 6887.42, average training loss: 7642.51, base loss: 15333.70
[INFO 2017-06-29 18:54:06,603 main.py:57] epoch 2057, training loss: 6885.90, average training loss: 7641.46, base loss: 15332.12
[INFO 2017-06-29 18:54:09,520 main.py:57] epoch 2058, training loss: 7916.89, average training loss: 7640.94, base loss: 15332.43
[INFO 2017-06-29 18:54:12,472 main.py:57] epoch 2059, training loss: 8185.53, average training loss: 7641.00, base loss: 15333.11
[INFO 2017-06-29 18:54:15,409 main.py:57] epoch 2060, training loss: 6937.77, average training loss: 7640.34, base loss: 15333.17
[INFO 2017-06-29 18:54:18,418 main.py:57] epoch 2061, training loss: 7545.07, average training loss: 7640.63, base loss: 15333.56
[INFO 2017-06-29 18:54:21,485 main.py:57] epoch 2062, training loss: 7898.09, average training loss: 7640.96, base loss: 15333.31
[INFO 2017-06-29 18:54:24,517 main.py:57] epoch 2063, training loss: 7768.39, average training loss: 7640.96, base loss: 15333.79
[INFO 2017-06-29 18:54:27,478 main.py:57] epoch 2064, training loss: 7004.77, average training loss: 7640.85, base loss: 15333.50
[INFO 2017-06-29 18:54:30,390 main.py:57] epoch 2065, training loss: 8546.36, average training loss: 7641.83, base loss: 15334.52
[INFO 2017-06-29 18:54:33,303 main.py:57] epoch 2066, training loss: 7557.73, average training loss: 7641.49, base loss: 15332.91
[INFO 2017-06-29 18:54:36,258 main.py:57] epoch 2067, training loss: 6959.34, average training loss: 7640.82, base loss: 15331.97
[INFO 2017-06-29 18:54:39,223 main.py:57] epoch 2068, training loss: 7775.26, average training loss: 7641.39, base loss: 15332.59
[INFO 2017-06-29 18:54:42,111 main.py:57] epoch 2069, training loss: 7635.37, average training loss: 7640.46, base loss: 15332.77
[INFO 2017-06-29 18:54:45,042 main.py:57] epoch 2070, training loss: 7677.67, average training loss: 7640.27, base loss: 15332.60
[INFO 2017-06-29 18:54:48,083 main.py:57] epoch 2071, training loss: 7535.04, average training loss: 7640.54, base loss: 15333.10
[INFO 2017-06-29 18:54:51,075 main.py:57] epoch 2072, training loss: 7350.90, average training loss: 7639.69, base loss: 15332.31
[INFO 2017-06-29 18:54:54,007 main.py:57] epoch 2073, training loss: 7037.88, average training loss: 7638.77, base loss: 15331.07
[INFO 2017-06-29 18:54:57,117 main.py:57] epoch 2074, training loss: 7801.23, average training loss: 7638.78, base loss: 15331.73
[INFO 2017-06-29 18:55:00,178 main.py:57] epoch 2075, training loss: 7165.86, average training loss: 7638.35, base loss: 15329.85
[INFO 2017-06-29 18:55:03,125 main.py:57] epoch 2076, training loss: 9122.17, average training loss: 7640.86, base loss: 15332.39
[INFO 2017-06-29 18:55:06,047 main.py:57] epoch 2077, training loss: 7420.96, average training loss: 7639.63, base loss: 15332.46
[INFO 2017-06-29 18:55:09,055 main.py:57] epoch 2078, training loss: 7472.03, average training loss: 7639.18, base loss: 15332.59
[INFO 2017-06-29 18:55:12,026 main.py:57] epoch 2079, training loss: 7621.04, average training loss: 7640.46, base loss: 15331.18
[INFO 2017-06-29 18:55:14,977 main.py:57] epoch 2080, training loss: 7000.05, average training loss: 7639.86, base loss: 15330.61
[INFO 2017-06-29 18:55:17,914 main.py:57] epoch 2081, training loss: 7081.56, average training loss: 7639.40, base loss: 15330.34
[INFO 2017-06-29 18:55:20,883 main.py:57] epoch 2082, training loss: 7797.48, average training loss: 7638.45, base loss: 15330.20
[INFO 2017-06-29 18:55:23,867 main.py:57] epoch 2083, training loss: 7343.47, average training loss: 7638.04, base loss: 15330.07
[INFO 2017-06-29 18:55:26,781 main.py:57] epoch 2084, training loss: 8071.42, average training loss: 7639.26, base loss: 15331.54
[INFO 2017-06-29 18:55:29,745 main.py:57] epoch 2085, training loss: 6650.34, average training loss: 7637.35, base loss: 15330.57
[INFO 2017-06-29 18:55:32,781 main.py:57] epoch 2086, training loss: 7249.24, average training loss: 7637.39, base loss: 15330.15
[INFO 2017-06-29 18:55:35,824 main.py:57] epoch 2087, training loss: 6781.19, average training loss: 7636.53, base loss: 15328.78
[INFO 2017-06-29 18:55:38,858 main.py:57] epoch 2088, training loss: 8134.93, average training loss: 7636.38, base loss: 15328.23
[INFO 2017-06-29 18:55:41,838 main.py:57] epoch 2089, training loss: 8902.49, average training loss: 7637.56, base loss: 15330.24
[INFO 2017-06-29 18:55:44,810 main.py:57] epoch 2090, training loss: 7757.14, average training loss: 7637.01, base loss: 15330.37
[INFO 2017-06-29 18:55:47,820 main.py:57] epoch 2091, training loss: 8738.45, average training loss: 7637.79, base loss: 15332.04
[INFO 2017-06-29 18:55:50,851 main.py:57] epoch 2092, training loss: 7315.55, average training loss: 7636.70, base loss: 15331.49
[INFO 2017-06-29 18:55:53,841 main.py:57] epoch 2093, training loss: 7146.92, average training loss: 7634.96, base loss: 15330.72
[INFO 2017-06-29 18:55:56,777 main.py:57] epoch 2094, training loss: 7231.97, average training loss: 7634.51, base loss: 15330.82
[INFO 2017-06-29 18:55:59,770 main.py:57] epoch 2095, training loss: 7062.75, average training loss: 7633.15, base loss: 15330.83
[INFO 2017-06-29 18:56:02,742 main.py:57] epoch 2096, training loss: 7003.25, average training loss: 7632.33, base loss: 15329.06
[INFO 2017-06-29 18:56:05,709 main.py:57] epoch 2097, training loss: 7505.45, average training loss: 7632.61, base loss: 15329.58
[INFO 2017-06-29 18:56:08,649 main.py:57] epoch 2098, training loss: 7514.02, average training loss: 7632.24, base loss: 15330.16
[INFO 2017-06-29 18:56:11,579 main.py:57] epoch 2099, training loss: 7921.48, average training loss: 7632.74, base loss: 15331.44
[INFO 2017-06-29 18:56:11,579 main.py:59] epoch 2099, testing
[INFO 2017-06-29 18:56:23,822 main.py:104] average testing loss: 8158.04, base loss: 16510.42
[INFO 2017-06-29 18:56:23,822 main.py:105] improve_loss: 8352.38, improve_percent: 0.51
[INFO 2017-06-29 18:56:23,824 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 18:56:26,754 main.py:57] epoch 2100, training loss: 7777.45, average training loss: 7632.40, base loss: 15331.69
[INFO 2017-06-29 18:56:29,751 main.py:57] epoch 2101, training loss: 7634.75, average training loss: 7632.30, base loss: 15331.32
[INFO 2017-06-29 18:56:32,683 main.py:57] epoch 2102, training loss: 7610.52, average training loss: 7632.12, base loss: 15329.94
[INFO 2017-06-29 18:56:35,606 main.py:57] epoch 2103, training loss: 6738.46, average training loss: 7632.11, base loss: 15327.49
[INFO 2017-06-29 18:56:38,565 main.py:57] epoch 2104, training loss: 6948.03, average training loss: 7631.70, base loss: 15327.28
[INFO 2017-06-29 18:56:41,443 main.py:57] epoch 2105, training loss: 8098.68, average training loss: 7632.27, base loss: 15327.06
[INFO 2017-06-29 18:56:44,458 main.py:57] epoch 2106, training loss: 8283.64, average training loss: 7633.95, base loss: 15326.50
[INFO 2017-06-29 18:56:47,422 main.py:57] epoch 2107, training loss: 7119.04, average training loss: 7633.22, base loss: 15325.91
[INFO 2017-06-29 18:56:50,485 main.py:57] epoch 2108, training loss: 7385.52, average training loss: 7632.00, base loss: 15326.71
[INFO 2017-06-29 18:56:53,432 main.py:57] epoch 2109, training loss: 7838.34, average training loss: 7633.42, base loss: 15326.93
[INFO 2017-06-29 18:56:56,351 main.py:57] epoch 2110, training loss: 7193.14, average training loss: 7633.02, base loss: 15326.57
[INFO 2017-06-29 18:56:59,346 main.py:57] epoch 2111, training loss: 7323.28, average training loss: 7632.83, base loss: 15326.29
[INFO 2017-06-29 18:57:02,320 main.py:57] epoch 2112, training loss: 7495.42, average training loss: 7632.44, base loss: 15324.00
[INFO 2017-06-29 18:57:05,300 main.py:57] epoch 2113, training loss: 7434.14, average training loss: 7631.66, base loss: 15322.84
[INFO 2017-06-29 18:57:08,286 main.py:57] epoch 2114, training loss: 7368.85, average training loss: 7631.91, base loss: 15322.79
[INFO 2017-06-29 18:57:11,245 main.py:57] epoch 2115, training loss: 7641.79, average training loss: 7631.96, base loss: 15322.26
[INFO 2017-06-29 18:57:14,133 main.py:57] epoch 2116, training loss: 7059.71, average training loss: 7631.07, base loss: 15321.04
[INFO 2017-06-29 18:57:17,090 main.py:57] epoch 2117, training loss: 6986.32, average training loss: 7629.93, base loss: 15319.67
[INFO 2017-06-29 18:57:20,056 main.py:57] epoch 2118, training loss: 7224.95, average training loss: 7630.25, base loss: 15320.39
[INFO 2017-06-29 18:57:22,994 main.py:57] epoch 2119, training loss: 7242.64, average training loss: 7630.24, base loss: 15320.07
[INFO 2017-06-29 18:57:26,005 main.py:57] epoch 2120, training loss: 7737.69, average training loss: 7628.93, base loss: 15320.33
[INFO 2017-06-29 18:57:28,962 main.py:57] epoch 2121, training loss: 7373.46, average training loss: 7629.19, base loss: 15319.95
[INFO 2017-06-29 18:57:31,907 main.py:57] epoch 2122, training loss: 8417.22, average training loss: 7630.03, base loss: 15320.59
[INFO 2017-06-29 18:57:34,830 main.py:57] epoch 2123, training loss: 8125.77, average training loss: 7630.86, base loss: 15320.40
[INFO 2017-06-29 18:57:37,721 main.py:57] epoch 2124, training loss: 8118.29, average training loss: 7631.17, base loss: 15321.14
[INFO 2017-06-29 18:57:40,749 main.py:57] epoch 2125, training loss: 7638.77, average training loss: 7629.80, base loss: 15320.37
[INFO 2017-06-29 18:57:43,721 main.py:57] epoch 2126, training loss: 7684.65, average training loss: 7630.60, base loss: 15321.43
[INFO 2017-06-29 18:57:46,668 main.py:57] epoch 2127, training loss: 7295.87, average training loss: 7631.00, base loss: 15321.05
[INFO 2017-06-29 18:57:49,649 main.py:57] epoch 2128, training loss: 8238.97, average training loss: 7632.04, base loss: 15321.16
[INFO 2017-06-29 18:57:52,597 main.py:57] epoch 2129, training loss: 7777.62, average training loss: 7631.72, base loss: 15320.60
[INFO 2017-06-29 18:57:55,529 main.py:57] epoch 2130, training loss: 7508.55, average training loss: 7631.22, base loss: 15321.08
[INFO 2017-06-29 18:57:58,532 main.py:57] epoch 2131, training loss: 7571.91, average training loss: 7630.86, base loss: 15321.11
[INFO 2017-06-29 18:58:01,444 main.py:57] epoch 2132, training loss: 7111.70, average training loss: 7631.08, base loss: 15321.41
[INFO 2017-06-29 18:58:04,398 main.py:57] epoch 2133, training loss: 8070.31, average training loss: 7631.59, base loss: 15322.33
[INFO 2017-06-29 18:58:07,388 main.py:57] epoch 2134, training loss: 8219.47, average training loss: 7631.83, base loss: 15322.43
[INFO 2017-06-29 18:58:10,352 main.py:57] epoch 2135, training loss: 7537.92, average training loss: 7631.17, base loss: 15321.78
[INFO 2017-06-29 18:58:13,297 main.py:57] epoch 2136, training loss: 7462.69, average training loss: 7630.80, base loss: 15322.11
[INFO 2017-06-29 18:58:16,288 main.py:57] epoch 2137, training loss: 7456.27, average training loss: 7630.93, base loss: 15322.62
[INFO 2017-06-29 18:58:19,401 main.py:57] epoch 2138, training loss: 7994.93, average training loss: 7630.34, base loss: 15321.92
[INFO 2017-06-29 18:58:22,330 main.py:57] epoch 2139, training loss: 7152.43, average training loss: 7628.93, base loss: 15320.06
[INFO 2017-06-29 18:58:25,335 main.py:57] epoch 2140, training loss: 8240.70, average training loss: 7628.32, base loss: 15321.90
[INFO 2017-06-29 18:58:28,313 main.py:57] epoch 2141, training loss: 7057.94, average training loss: 7627.40, base loss: 15322.33
[INFO 2017-06-29 18:58:31,208 main.py:57] epoch 2142, training loss: 8620.61, average training loss: 7628.04, base loss: 15324.59
[INFO 2017-06-29 18:58:34,215 main.py:57] epoch 2143, training loss: 7985.16, average training loss: 7628.67, base loss: 15325.62
[INFO 2017-06-29 18:58:37,157 main.py:57] epoch 2144, training loss: 8036.73, average training loss: 7628.31, base loss: 15325.36
[INFO 2017-06-29 18:58:40,176 main.py:57] epoch 2145, training loss: 7822.57, average training loss: 7627.53, base loss: 15326.90
[INFO 2017-06-29 18:58:43,107 main.py:57] epoch 2146, training loss: 7637.30, average training loss: 7627.66, base loss: 15326.73
[INFO 2017-06-29 18:58:46,035 main.py:57] epoch 2147, training loss: 6722.77, average training loss: 7626.15, base loss: 15325.43
[INFO 2017-06-29 18:58:48,988 main.py:57] epoch 2148, training loss: 7665.70, average training loss: 7625.72, base loss: 15325.87
[INFO 2017-06-29 18:58:51,918 main.py:57] epoch 2149, training loss: 8339.01, average training loss: 7627.37, base loss: 15327.57
[INFO 2017-06-29 18:58:54,919 main.py:57] epoch 2150, training loss: 7545.62, average training loss: 7627.47, base loss: 15327.42
[INFO 2017-06-29 18:58:57,866 main.py:57] epoch 2151, training loss: 7485.07, average training loss: 7627.37, base loss: 15328.13
[INFO 2017-06-29 18:59:00,778 main.py:57] epoch 2152, training loss: 7562.24, average training loss: 7627.76, base loss: 15327.01
[INFO 2017-06-29 18:59:03,755 main.py:57] epoch 2153, training loss: 8499.60, average training loss: 7628.74, base loss: 15327.56
[INFO 2017-06-29 18:59:06,744 main.py:57] epoch 2154, training loss: 7204.97, average training loss: 7628.92, base loss: 15325.91
[INFO 2017-06-29 18:59:09,670 main.py:57] epoch 2155, training loss: 7186.70, average training loss: 7627.31, base loss: 15325.61
[INFO 2017-06-29 18:59:12,674 main.py:57] epoch 2156, training loss: 7330.51, average training loss: 7627.91, base loss: 15324.90
[INFO 2017-06-29 18:59:15,689 main.py:57] epoch 2157, training loss: 7113.12, average training loss: 7627.60, base loss: 15324.29
[INFO 2017-06-29 18:59:18,678 main.py:57] epoch 2158, training loss: 8514.27, average training loss: 7629.25, base loss: 15325.86
[INFO 2017-06-29 18:59:21,666 main.py:57] epoch 2159, training loss: 8186.17, average training loss: 7629.34, base loss: 15326.71
[INFO 2017-06-29 18:59:24,664 main.py:57] epoch 2160, training loss: 7609.34, average training loss: 7629.77, base loss: 15327.04
[INFO 2017-06-29 18:59:27,610 main.py:57] epoch 2161, training loss: 7459.22, average training loss: 7629.56, base loss: 15327.28
[INFO 2017-06-29 18:59:30,636 main.py:57] epoch 2162, training loss: 7501.45, average training loss: 7629.11, base loss: 15328.10
[INFO 2017-06-29 18:59:33,663 main.py:57] epoch 2163, training loss: 7412.35, average training loss: 7628.42, base loss: 15328.50
[INFO 2017-06-29 18:59:36,568 main.py:57] epoch 2164, training loss: 6720.33, average training loss: 7627.74, base loss: 15327.61
[INFO 2017-06-29 18:59:39,583 main.py:57] epoch 2165, training loss: 7999.76, average training loss: 7627.45, base loss: 15330.05
[INFO 2017-06-29 18:59:42,590 main.py:57] epoch 2166, training loss: 6951.33, average training loss: 7626.78, base loss: 15329.68
[INFO 2017-06-29 18:59:45,618 main.py:57] epoch 2167, training loss: 7148.10, average training loss: 7626.27, base loss: 15329.23
[INFO 2017-06-29 18:59:48,606 main.py:57] epoch 2168, training loss: 8514.97, average training loss: 7626.95, base loss: 15331.13
[INFO 2017-06-29 18:59:51,548 main.py:57] epoch 2169, training loss: 8339.66, average training loss: 7628.23, base loss: 15332.27
[INFO 2017-06-29 18:59:54,514 main.py:57] epoch 2170, training loss: 7784.21, average training loss: 7628.71, base loss: 15332.32
[INFO 2017-06-29 18:59:57,483 main.py:57] epoch 2171, training loss: 7665.05, average training loss: 7628.41, base loss: 15332.85
[INFO 2017-06-29 19:00:00,535 main.py:57] epoch 2172, training loss: 7187.07, average training loss: 7627.59, base loss: 15331.85
[INFO 2017-06-29 19:00:03,443 main.py:57] epoch 2173, training loss: 6910.91, average training loss: 7626.88, base loss: 15331.78
[INFO 2017-06-29 19:00:06,319 main.py:57] epoch 2174, training loss: 7048.24, average training loss: 7626.53, base loss: 15330.82
[INFO 2017-06-29 19:00:09,217 main.py:57] epoch 2175, training loss: 8479.79, average training loss: 7627.96, base loss: 15331.64
[INFO 2017-06-29 19:00:12,321 main.py:57] epoch 2176, training loss: 6950.13, average training loss: 7627.19, base loss: 15331.49
[INFO 2017-06-29 19:00:15,361 main.py:57] epoch 2177, training loss: 7149.87, average training loss: 7627.23, base loss: 15331.16
[INFO 2017-06-29 19:00:18,293 main.py:57] epoch 2178, training loss: 7817.41, average training loss: 7626.07, base loss: 15332.06
[INFO 2017-06-29 19:00:21,315 main.py:57] epoch 2179, training loss: 8067.18, average training loss: 7626.17, base loss: 15332.12
[INFO 2017-06-29 19:00:24,327 main.py:57] epoch 2180, training loss: 7431.20, average training loss: 7625.91, base loss: 15332.32
[INFO 2017-06-29 19:00:27,289 main.py:57] epoch 2181, training loss: 7091.79, average training loss: 7626.67, base loss: 15330.98
[INFO 2017-06-29 19:00:30,285 main.py:57] epoch 2182, training loss: 7817.01, average training loss: 7627.07, base loss: 15331.39
[INFO 2017-06-29 19:00:33,243 main.py:57] epoch 2183, training loss: 7678.28, average training loss: 7626.97, base loss: 15331.27
[INFO 2017-06-29 19:00:36,149 main.py:57] epoch 2184, training loss: 6973.88, average training loss: 7626.24, base loss: 15330.39
[INFO 2017-06-29 19:00:39,132 main.py:57] epoch 2185, training loss: 6894.59, average training loss: 7625.66, base loss: 15329.79
[INFO 2017-06-29 19:00:42,051 main.py:57] epoch 2186, training loss: 8138.80, average training loss: 7625.32, base loss: 15330.16
[INFO 2017-06-29 19:00:44,967 main.py:57] epoch 2187, training loss: 6903.49, average training loss: 7623.86, base loss: 15329.04
[INFO 2017-06-29 19:00:47,949 main.py:57] epoch 2188, training loss: 7195.78, average training loss: 7623.60, base loss: 15327.94
[INFO 2017-06-29 19:00:51,031 main.py:57] epoch 2189, training loss: 6982.22, average training loss: 7623.25, base loss: 15326.56
[INFO 2017-06-29 19:00:54,093 main.py:57] epoch 2190, training loss: 6399.28, average training loss: 7622.06, base loss: 15325.23
[INFO 2017-06-29 19:00:57,051 main.py:57] epoch 2191, training loss: 8415.44, average training loss: 7622.64, base loss: 15326.36
[INFO 2017-06-29 19:01:00,008 main.py:57] epoch 2192, training loss: 7939.06, average training loss: 7622.85, base loss: 15327.02
[INFO 2017-06-29 19:01:03,033 main.py:57] epoch 2193, training loss: 7732.97, average training loss: 7623.46, base loss: 15327.48
[INFO 2017-06-29 19:01:05,957 main.py:57] epoch 2194, training loss: 8294.70, average training loss: 7624.55, base loss: 15329.89
[INFO 2017-06-29 19:01:08,917 main.py:57] epoch 2195, training loss: 7946.09, average training loss: 7625.38, base loss: 15330.96
[INFO 2017-06-29 19:01:11,814 main.py:57] epoch 2196, training loss: 7454.49, average training loss: 7625.31, base loss: 15330.57
[INFO 2017-06-29 19:01:14,738 main.py:57] epoch 2197, training loss: 8475.38, average training loss: 7626.05, base loss: 15331.97
[INFO 2017-06-29 19:01:17,731 main.py:57] epoch 2198, training loss: 6386.81, average training loss: 7624.30, base loss: 15330.06
[INFO 2017-06-29 19:01:20,687 main.py:57] epoch 2199, training loss: 8100.97, average training loss: 7624.48, base loss: 15330.60
[INFO 2017-06-29 19:01:20,688 main.py:59] epoch 2199, testing
[INFO 2017-06-29 19:01:32,963 main.py:104] average testing loss: 7990.17, base loss: 16431.88
[INFO 2017-06-29 19:01:32,963 main.py:105] improve_loss: 8441.71, improve_percent: 0.51
[INFO 2017-06-29 19:01:32,965 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 19:01:32,991 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 19:01:36,013 main.py:57] epoch 2200, training loss: 8412.55, average training loss: 7624.39, base loss: 15331.64
[INFO 2017-06-29 19:01:38,974 main.py:57] epoch 2201, training loss: 7510.63, average training loss: 7622.75, base loss: 15331.12
[INFO 2017-06-29 19:01:41,894 main.py:57] epoch 2202, training loss: 7232.74, average training loss: 7621.87, base loss: 15332.11
[INFO 2017-06-29 19:01:44,881 main.py:57] epoch 2203, training loss: 7272.53, average training loss: 7620.74, base loss: 15332.66
[INFO 2017-06-29 19:01:47,920 main.py:57] epoch 2204, training loss: 7687.59, average training loss: 7620.04, base loss: 15332.93
[INFO 2017-06-29 19:01:50,909 main.py:57] epoch 2205, training loss: 7880.23, average training loss: 7620.21, base loss: 15333.35
[INFO 2017-06-29 19:01:53,862 main.py:57] epoch 2206, training loss: 6874.82, average training loss: 7618.52, base loss: 15333.08
[INFO 2017-06-29 19:01:56,893 main.py:57] epoch 2207, training loss: 7001.09, average training loss: 7617.55, base loss: 15332.78
[INFO 2017-06-29 19:01:59,819 main.py:57] epoch 2208, training loss: 7999.30, average training loss: 7616.61, base loss: 15333.17
[INFO 2017-06-29 19:02:02,743 main.py:57] epoch 2209, training loss: 7930.90, average training loss: 7616.94, base loss: 15333.51
[INFO 2017-06-29 19:02:05,790 main.py:57] epoch 2210, training loss: 6801.37, average training loss: 7616.27, base loss: 15332.94
[INFO 2017-06-29 19:02:08,781 main.py:57] epoch 2211, training loss: 6397.85, average training loss: 7613.86, base loss: 15330.96
[INFO 2017-06-29 19:02:11,787 main.py:57] epoch 2212, training loss: 8270.21, average training loss: 7614.40, base loss: 15330.92
[INFO 2017-06-29 19:02:14,685 main.py:57] epoch 2213, training loss: 7243.53, average training loss: 7613.96, base loss: 15329.19
[INFO 2017-06-29 19:02:17,689 main.py:57] epoch 2214, training loss: 6941.99, average training loss: 7613.57, base loss: 15328.66
[INFO 2017-06-29 19:02:20,682 main.py:57] epoch 2215, training loss: 6893.92, average training loss: 7613.39, base loss: 15327.87
[INFO 2017-06-29 19:02:23,691 main.py:57] epoch 2216, training loss: 6243.37, average training loss: 7612.28, base loss: 15326.39
[INFO 2017-06-29 19:02:26,685 main.py:57] epoch 2217, training loss: 8322.33, average training loss: 7613.56, base loss: 15327.61
[INFO 2017-06-29 19:02:29,619 main.py:57] epoch 2218, training loss: 8111.78, average training loss: 7612.55, base loss: 15329.82
[INFO 2017-06-29 19:02:32,512 main.py:57] epoch 2219, training loss: 6054.90, average training loss: 7610.55, base loss: 15327.93
[INFO 2017-06-29 19:02:35,445 main.py:57] epoch 2220, training loss: 7462.46, average training loss: 7609.81, base loss: 15327.02
[INFO 2017-06-29 19:02:38,377 main.py:57] epoch 2221, training loss: 7488.01, average training loss: 7609.38, base loss: 15327.21
[INFO 2017-06-29 19:02:41,314 main.py:57] epoch 2222, training loss: 8058.56, average training loss: 7608.82, base loss: 15327.96
[INFO 2017-06-29 19:02:44,266 main.py:57] epoch 2223, training loss: 8448.71, average training loss: 7609.69, base loss: 15327.66
[INFO 2017-06-29 19:02:47,232 main.py:57] epoch 2224, training loss: 7191.29, average training loss: 7609.22, base loss: 15328.09
[INFO 2017-06-29 19:02:50,226 main.py:57] epoch 2225, training loss: 7639.55, average training loss: 7609.53, base loss: 15328.45
[INFO 2017-06-29 19:02:53,130 main.py:57] epoch 2226, training loss: 7636.40, average training loss: 7609.15, base loss: 15328.03
[INFO 2017-06-29 19:02:56,031 main.py:57] epoch 2227, training loss: 8339.65, average training loss: 7609.21, base loss: 15329.36
[INFO 2017-06-29 19:02:58,937 main.py:57] epoch 2228, training loss: 7408.09, average training loss: 7608.58, base loss: 15330.04
[INFO 2017-06-29 19:03:01,958 main.py:57] epoch 2229, training loss: 7816.42, average training loss: 7608.70, base loss: 15330.84
[INFO 2017-06-29 19:03:04,982 main.py:57] epoch 2230, training loss: 7758.88, average training loss: 7608.03, base loss: 15330.67
[INFO 2017-06-29 19:03:07,872 main.py:57] epoch 2231, training loss: 7097.58, average training loss: 7607.38, base loss: 15329.46
[INFO 2017-06-29 19:03:10,745 main.py:57] epoch 2232, training loss: 7599.07, average training loss: 7607.55, base loss: 15329.15
[INFO 2017-06-29 19:03:13,693 main.py:57] epoch 2233, training loss: 7698.97, average training loss: 7607.80, base loss: 15329.50
[INFO 2017-06-29 19:03:16,640 main.py:57] epoch 2234, training loss: 6803.49, average training loss: 7607.89, base loss: 15328.82
[INFO 2017-06-29 19:03:19,599 main.py:57] epoch 2235, training loss: 7286.19, average training loss: 7606.62, base loss: 15329.20
[INFO 2017-06-29 19:03:22,639 main.py:57] epoch 2236, training loss: 7931.79, average training loss: 7607.63, base loss: 15330.00
[INFO 2017-06-29 19:03:25,578 main.py:57] epoch 2237, training loss: 7651.40, average training loss: 7607.35, base loss: 15331.07
[INFO 2017-06-29 19:03:28,563 main.py:57] epoch 2238, training loss: 6785.97, average training loss: 7606.57, base loss: 15330.19
[INFO 2017-06-29 19:03:31,491 main.py:57] epoch 2239, training loss: 8293.25, average training loss: 7607.81, base loss: 15331.39
[INFO 2017-06-29 19:03:34,467 main.py:57] epoch 2240, training loss: 6772.99, average training loss: 7606.58, base loss: 15329.69
[INFO 2017-06-29 19:03:37,508 main.py:57] epoch 2241, training loss: 7745.72, average training loss: 7606.94, base loss: 15330.05
[INFO 2017-06-29 19:03:40,501 main.py:57] epoch 2242, training loss: 7126.30, average training loss: 7606.10, base loss: 15329.36
[INFO 2017-06-29 19:03:43,540 main.py:57] epoch 2243, training loss: 7984.41, average training loss: 7606.09, base loss: 15328.78
[INFO 2017-06-29 19:03:46,514 main.py:57] epoch 2244, training loss: 7298.52, average training loss: 7605.91, base loss: 15327.48
[INFO 2017-06-29 19:03:49,496 main.py:57] epoch 2245, training loss: 8354.16, average training loss: 7607.08, base loss: 15328.47
[INFO 2017-06-29 19:03:52,466 main.py:57] epoch 2246, training loss: 7831.47, average training loss: 7607.20, base loss: 15328.32
[INFO 2017-06-29 19:03:55,407 main.py:57] epoch 2247, training loss: 7287.64, average training loss: 7606.78, base loss: 15327.81
[INFO 2017-06-29 19:03:58,333 main.py:57] epoch 2248, training loss: 7839.46, average training loss: 7606.73, base loss: 15328.51
[INFO 2017-06-29 19:04:01,254 main.py:57] epoch 2249, training loss: 8592.96, average training loss: 7606.73, base loss: 15329.55
[INFO 2017-06-29 19:04:04,306 main.py:57] epoch 2250, training loss: 7555.52, average training loss: 7606.67, base loss: 15329.98
[INFO 2017-06-29 19:04:07,306 main.py:57] epoch 2251, training loss: 7358.55, average training loss: 7606.72, base loss: 15330.33
[INFO 2017-06-29 19:04:10,280 main.py:57] epoch 2252, training loss: 7716.60, average training loss: 7604.85, base loss: 15331.10
[INFO 2017-06-29 19:04:13,188 main.py:57] epoch 2253, training loss: 7514.69, average training loss: 7604.80, base loss: 15331.00
[INFO 2017-06-29 19:04:16,181 main.py:57] epoch 2254, training loss: 7057.38, average training loss: 7604.55, base loss: 15330.93
[INFO 2017-06-29 19:04:19,120 main.py:57] epoch 2255, training loss: 6778.42, average training loss: 7603.56, base loss: 15330.49
[INFO 2017-06-29 19:04:22,163 main.py:57] epoch 2256, training loss: 7021.12, average training loss: 7602.53, base loss: 15329.43
[INFO 2017-06-29 19:04:25,078 main.py:57] epoch 2257, training loss: 7953.17, average training loss: 7603.38, base loss: 15330.26
[INFO 2017-06-29 19:04:28,072 main.py:57] epoch 2258, training loss: 7497.11, average training loss: 7602.71, base loss: 15329.29
[INFO 2017-06-29 19:04:31,060 main.py:57] epoch 2259, training loss: 7037.62, average training loss: 7602.70, base loss: 15328.42
[INFO 2017-06-29 19:04:34,042 main.py:57] epoch 2260, training loss: 7486.07, average training loss: 7602.54, base loss: 15328.46
[INFO 2017-06-29 19:04:36,944 main.py:57] epoch 2261, training loss: 6988.14, average training loss: 7602.49, base loss: 15327.92
[INFO 2017-06-29 19:04:39,918 main.py:57] epoch 2262, training loss: 7248.03, average training loss: 7601.93, base loss: 15327.60
[INFO 2017-06-29 19:04:42,902 main.py:57] epoch 2263, training loss: 7608.26, average training loss: 7602.04, base loss: 15328.15
[INFO 2017-06-29 19:04:45,846 main.py:57] epoch 2264, training loss: 8166.76, average training loss: 7602.26, base loss: 15328.10
[INFO 2017-06-29 19:04:48,799 main.py:57] epoch 2265, training loss: 7767.43, average training loss: 7601.62, base loss: 15328.28
[INFO 2017-06-29 19:04:51,839 main.py:57] epoch 2266, training loss: 6743.63, average training loss: 7600.84, base loss: 15326.64
[INFO 2017-06-29 19:04:54,697 main.py:57] epoch 2267, training loss: 6769.58, average training loss: 7599.40, base loss: 15325.13
[INFO 2017-06-29 19:04:57,676 main.py:57] epoch 2268, training loss: 6960.50, average training loss: 7598.36, base loss: 15325.85
[INFO 2017-06-29 19:05:00,580 main.py:57] epoch 2269, training loss: 7651.36, average training loss: 7598.19, base loss: 15327.23
[INFO 2017-06-29 19:05:03,452 main.py:57] epoch 2270, training loss: 7015.97, average training loss: 7596.79, base loss: 15326.06
[INFO 2017-06-29 19:05:06,360 main.py:57] epoch 2271, training loss: 6704.58, average training loss: 7595.56, base loss: 15324.14
[INFO 2017-06-29 19:05:09,331 main.py:57] epoch 2272, training loss: 7196.21, average training loss: 7595.17, base loss: 15324.10
[INFO 2017-06-29 19:05:12,278 main.py:57] epoch 2273, training loss: 6976.61, average training loss: 7593.77, base loss: 15322.83
[INFO 2017-06-29 19:05:15,218 main.py:57] epoch 2274, training loss: 7934.63, average training loss: 7594.02, base loss: 15323.51
[INFO 2017-06-29 19:05:18,299 main.py:57] epoch 2275, training loss: 6553.93, average training loss: 7592.96, base loss: 15322.15
[INFO 2017-06-29 19:05:21,223 main.py:57] epoch 2276, training loss: 7760.36, average training loss: 7592.39, base loss: 15323.21
[INFO 2017-06-29 19:05:24,222 main.py:57] epoch 2277, training loss: 7757.46, average training loss: 7592.12, base loss: 15324.41
[INFO 2017-06-29 19:05:27,191 main.py:57] epoch 2278, training loss: 8269.59, average training loss: 7593.12, base loss: 15325.53
[INFO 2017-06-29 19:05:30,118 main.py:57] epoch 2279, training loss: 7528.78, average training loss: 7592.77, base loss: 15325.44
[INFO 2017-06-29 19:05:33,139 main.py:57] epoch 2280, training loss: 8192.85, average training loss: 7593.36, base loss: 15326.63
[INFO 2017-06-29 19:05:36,118 main.py:57] epoch 2281, training loss: 7739.60, average training loss: 7593.85, base loss: 15326.92
[INFO 2017-06-29 19:05:39,155 main.py:57] epoch 2282, training loss: 7493.89, average training loss: 7592.57, base loss: 15327.53
[INFO 2017-06-29 19:05:42,154 main.py:57] epoch 2283, training loss: 7606.25, average training loss: 7592.38, base loss: 15328.19
[INFO 2017-06-29 19:05:45,136 main.py:57] epoch 2284, training loss: 7618.06, average training loss: 7592.97, base loss: 15327.64
[INFO 2017-06-29 19:05:48,044 main.py:57] epoch 2285, training loss: 7843.27, average training loss: 7592.40, base loss: 15328.04
[INFO 2017-06-29 19:05:51,018 main.py:57] epoch 2286, training loss: 7423.08, average training loss: 7591.79, base loss: 15328.43
[INFO 2017-06-29 19:05:54,015 main.py:57] epoch 2287, training loss: 6838.99, average training loss: 7590.18, base loss: 15327.11
[INFO 2017-06-29 19:05:57,016 main.py:57] epoch 2288, training loss: 8913.19, average training loss: 7590.98, base loss: 15329.38
[INFO 2017-06-29 19:05:59,992 main.py:57] epoch 2289, training loss: 6967.46, average training loss: 7590.93, base loss: 15328.21
[INFO 2017-06-29 19:06:02,991 main.py:57] epoch 2290, training loss: 7656.48, average training loss: 7591.45, base loss: 15328.16
[INFO 2017-06-29 19:06:05,970 main.py:57] epoch 2291, training loss: 7506.40, average training loss: 7591.75, base loss: 15327.95
[INFO 2017-06-29 19:06:08,934 main.py:57] epoch 2292, training loss: 6852.10, average training loss: 7591.17, base loss: 15326.81
[INFO 2017-06-29 19:06:11,887 main.py:57] epoch 2293, training loss: 8187.50, average training loss: 7591.17, base loss: 15327.28
[INFO 2017-06-29 19:06:14,801 main.py:57] epoch 2294, training loss: 7386.47, average training loss: 7589.75, base loss: 15327.48
[INFO 2017-06-29 19:06:17,816 main.py:57] epoch 2295, training loss: 6036.92, average training loss: 7588.13, base loss: 15325.73
[INFO 2017-06-29 19:06:20,787 main.py:57] epoch 2296, training loss: 7435.78, average training loss: 7587.76, base loss: 15325.74
[INFO 2017-06-29 19:06:23,746 main.py:57] epoch 2297, training loss: 7191.20, average training loss: 7586.82, base loss: 15324.91
[INFO 2017-06-29 19:06:26,664 main.py:57] epoch 2298, training loss: 7368.52, average training loss: 7586.73, base loss: 15324.64
[INFO 2017-06-29 19:06:29,703 main.py:57] epoch 2299, training loss: 7210.65, average training loss: 7585.97, base loss: 15325.18
[INFO 2017-06-29 19:06:29,703 main.py:59] epoch 2299, testing
[INFO 2017-06-29 19:06:42,169 main.py:104] average testing loss: 7939.79, base loss: 16305.24
[INFO 2017-06-29 19:06:42,169 main.py:105] improve_loss: 8365.45, improve_percent: 0.51
[INFO 2017-06-29 19:06:42,172 main.py:71] current best improved percent: 0.51
[INFO 2017-06-29 19:06:45,255 main.py:57] epoch 2300, training loss: 6433.11, average training loss: 7584.00, base loss: 15323.07
[INFO 2017-06-29 19:06:48,238 main.py:57] epoch 2301, training loss: 6883.84, average training loss: 7583.99, base loss: 15322.34
[INFO 2017-06-29 19:06:51,260 main.py:57] epoch 2302, training loss: 7791.94, average training loss: 7584.08, base loss: 15323.26
[INFO 2017-06-29 19:06:54,279 main.py:57] epoch 2303, training loss: 7742.60, average training loss: 7584.77, base loss: 15323.28
[INFO 2017-06-29 19:06:57,210 main.py:57] epoch 2304, training loss: 7642.84, average training loss: 7585.21, base loss: 15323.16
[INFO 2017-06-29 19:07:00,187 main.py:57] epoch 2305, training loss: 7757.95, average training loss: 7584.75, base loss: 15322.99
[INFO 2017-06-29 19:07:03,080 main.py:57] epoch 2306, training loss: 8132.33, average training loss: 7585.44, base loss: 15323.05
[INFO 2017-06-29 19:07:06,071 main.py:57] epoch 2307, training loss: 7801.61, average training loss: 7585.57, base loss: 15323.10
[INFO 2017-06-29 19:07:09,053 main.py:57] epoch 2308, training loss: 7964.04, average training loss: 7585.69, base loss: 15322.19
[INFO 2017-06-29 19:07:12,011 main.py:57] epoch 2309, training loss: 7419.47, average training loss: 7585.16, base loss: 15320.56
[INFO 2017-06-29 19:07:15,129 main.py:57] epoch 2310, training loss: 7502.74, average training loss: 7584.76, base loss: 15320.35
[INFO 2017-06-29 19:07:18,125 main.py:57] epoch 2311, training loss: 8179.53, average training loss: 7585.27, base loss: 15320.87
[INFO 2017-06-29 19:07:21,085 main.py:57] epoch 2312, training loss: 7897.54, average training loss: 7585.46, base loss: 15320.94
[INFO 2017-06-29 19:07:24,116 main.py:57] epoch 2313, training loss: 7490.39, average training loss: 7585.80, base loss: 15321.29
[INFO 2017-06-29 19:07:27,097 main.py:57] epoch 2314, training loss: 7501.50, average training loss: 7586.00, base loss: 15321.19
[INFO 2017-06-29 19:07:30,139 main.py:57] epoch 2315, training loss: 7736.78, average training loss: 7586.01, base loss: 15323.08
[INFO 2017-06-29 19:07:33,115 main.py:57] epoch 2316, training loss: 7640.32, average training loss: 7586.42, base loss: 15323.63
[INFO 2017-06-29 19:07:36,073 main.py:57] epoch 2317, training loss: 7007.22, average training loss: 7586.23, base loss: 15323.40
[INFO 2017-06-29 19:07:39,084 main.py:57] epoch 2318, training loss: 7437.47, average training loss: 7586.20, base loss: 15322.99
[INFO 2017-06-29 19:07:42,142 main.py:57] epoch 2319, training loss: 7381.15, average training loss: 7585.90, base loss: 15322.76
[INFO 2017-06-29 19:07:45,148 main.py:57] epoch 2320, training loss: 7291.77, average training loss: 7586.05, base loss: 15322.59
[INFO 2017-06-29 19:07:48,169 main.py:57] epoch 2321, training loss: 7436.27, average training loss: 7586.73, base loss: 15323.22
[INFO 2017-06-29 19:07:51,166 main.py:57] epoch 2322, training loss: 6787.09, average training loss: 7585.96, base loss: 15323.26
[INFO 2017-06-29 19:07:54,100 main.py:57] epoch 2323, training loss: 7356.00, average training loss: 7586.36, base loss: 15322.25
[INFO 2017-06-29 19:07:57,007 main.py:57] epoch 2324, training loss: 7827.47, average training loss: 7586.61, base loss: 15322.32
[INFO 2017-06-29 19:07:59,985 main.py:57] epoch 2325, training loss: 7966.78, average training loss: 7586.49, base loss: 15321.98
[INFO 2017-06-29 19:08:02,947 main.py:57] epoch 2326, training loss: 7198.13, average training loss: 7585.34, base loss: 15322.19
[INFO 2017-06-29 19:08:05,948 main.py:57] epoch 2327, training loss: 7707.08, average training loss: 7584.71, base loss: 15322.14
[INFO 2017-06-29 19:08:08,982 main.py:57] epoch 2328, training loss: 6302.95, average training loss: 7583.82, base loss: 15320.96
[INFO 2017-06-29 19:08:11,905 main.py:57] epoch 2329, training loss: 6498.40, average training loss: 7582.54, base loss: 15320.23
[INFO 2017-06-29 19:08:14,868 main.py:57] epoch 2330, training loss: 7497.77, average training loss: 7581.73, base loss: 15321.63
[INFO 2017-06-29 19:08:17,919 main.py:57] epoch 2331, training loss: 6342.86, average training loss: 7580.21, base loss: 15320.56
[INFO 2017-06-29 19:08:20,835 main.py:57] epoch 2332, training loss: 6765.76, average training loss: 7578.53, base loss: 15319.84
[INFO 2017-06-29 19:08:23,753 main.py:57] epoch 2333, training loss: 7374.05, average training loss: 7579.21, base loss: 15319.39
[INFO 2017-06-29 19:08:26,770 main.py:57] epoch 2334, training loss: 8436.16, average training loss: 7579.74, base loss: 15319.90
[INFO 2017-06-29 19:08:29,716 main.py:57] epoch 2335, training loss: 8333.46, average training loss: 7579.98, base loss: 15320.83
[INFO 2017-06-29 19:08:32,670 main.py:57] epoch 2336, training loss: 8498.23, average training loss: 7580.84, base loss: 15322.88
[INFO 2017-06-29 19:08:35,677 main.py:57] epoch 2337, training loss: 6586.71, average training loss: 7579.82, base loss: 15322.65
[INFO 2017-06-29 19:08:38,659 main.py:57] epoch 2338, training loss: 7758.69, average training loss: 7579.92, base loss: 15323.85
[INFO 2017-06-29 19:08:41,598 main.py:57] epoch 2339, training loss: 6888.58, average training loss: 7579.11, base loss: 15323.10
[INFO 2017-06-29 19:08:44,561 main.py:57] epoch 2340, training loss: 7277.30, average training loss: 7578.02, base loss: 15323.16
[INFO 2017-06-29 19:08:47,456 main.py:57] epoch 2341, training loss: 7046.45, average training loss: 7577.04, base loss: 15322.63
[INFO 2017-06-29 19:08:50,446 main.py:57] epoch 2342, training loss: 7724.20, average training loss: 7577.55, base loss: 15322.18
[INFO 2017-06-29 19:08:53,528 main.py:57] epoch 2343, training loss: 6803.96, average training loss: 7576.83, base loss: 15320.90
[INFO 2017-06-29 19:08:56,498 main.py:57] epoch 2344, training loss: 8595.57, average training loss: 7577.72, base loss: 15320.84
[INFO 2017-06-29 19:08:59,477 main.py:57] epoch 2345, training loss: 8527.83, average training loss: 7579.24, base loss: 15321.64
[INFO 2017-06-29 19:09:02,407 main.py:57] epoch 2346, training loss: 8176.34, average training loss: 7579.34, base loss: 15321.33
[INFO 2017-06-29 19:09:05,505 main.py:57] epoch 2347, training loss: 8255.75, average training loss: 7579.46, base loss: 15322.59
[INFO 2017-06-29 19:09:08,471 main.py:57] epoch 2348, training loss: 7473.49, average training loss: 7579.15, base loss: 15321.97
[INFO 2017-06-29 19:09:11,443 main.py:57] epoch 2349, training loss: 8556.31, average training loss: 7580.47, base loss: 15322.91
[INFO 2017-06-29 19:09:14,388 main.py:57] epoch 2350, training loss: 7935.03, average training loss: 7580.76, base loss: 15322.64
[INFO 2017-06-29 19:09:17,388 main.py:57] epoch 2351, training loss: 7515.60, average training loss: 7580.80, base loss: 15322.10
[INFO 2017-06-29 19:09:20,481 main.py:57] epoch 2352, training loss: 7790.36, average training loss: 7580.63, base loss: 15322.47
[INFO 2017-06-29 19:09:23,665 main.py:57] epoch 2353, training loss: 7749.64, average training loss: 7579.15, base loss: 15322.08
[INFO 2017-06-29 19:09:26,659 main.py:57] epoch 2354, training loss: 7642.96, average training loss: 7579.21, base loss: 15320.88
[INFO 2017-06-29 19:09:29,675 main.py:57] epoch 2355, training loss: 7288.49, average training loss: 7578.14, base loss: 15320.74
[INFO 2017-06-29 19:09:32,653 main.py:57] epoch 2356, training loss: 6748.77, average training loss: 7577.65, base loss: 15319.26
[INFO 2017-06-29 19:09:35,710 main.py:57] epoch 2357, training loss: 7756.69, average training loss: 7577.80, base loss: 15318.81
[INFO 2017-06-29 19:09:38,741 main.py:57] epoch 2358, training loss: 7695.70, average training loss: 7577.48, base loss: 15319.25
[INFO 2017-06-29 19:09:41,749 main.py:57] epoch 2359, training loss: 7713.34, average training loss: 7576.67, base loss: 15319.44
[INFO 2017-06-29 19:09:44,635 main.py:57] epoch 2360, training loss: 7869.48, average training loss: 7576.99, base loss: 15320.15
[INFO 2017-06-29 19:09:47,625 main.py:57] epoch 2361, training loss: 7528.48, average training loss: 7576.74, base loss: 15321.34
[INFO 2017-06-29 19:09:50,576 main.py:57] epoch 2362, training loss: 7516.66, average training loss: 7576.30, base loss: 15320.83
[INFO 2017-06-29 19:09:53,508 main.py:57] epoch 2363, training loss: 7989.39, average training loss: 7576.96, base loss: 15321.34
[INFO 2017-06-29 19:09:56,498 main.py:57] epoch 2364, training loss: 8158.51, average training loss: 7576.24, base loss: 15322.58
[INFO 2017-06-29 19:09:59,518 main.py:57] epoch 2365, training loss: 7210.37, average training loss: 7576.19, base loss: 15322.44
[INFO 2017-06-29 19:10:02,482 main.py:57] epoch 2366, training loss: 7385.64, average training loss: 7575.88, base loss: 15322.32
[INFO 2017-06-29 19:10:05,491 main.py:57] epoch 2367, training loss: 7167.25, average training loss: 7575.83, base loss: 15321.15
[INFO 2017-06-29 19:10:08,429 main.py:57] epoch 2368, training loss: 7295.02, average training loss: 7575.43, base loss: 15321.40
[INFO 2017-06-29 19:10:11,362 main.py:57] epoch 2369, training loss: 7352.05, average training loss: 7574.82, base loss: 15321.35
[INFO 2017-06-29 19:10:14,555 main.py:57] epoch 2370, training loss: 7091.51, average training loss: 7575.11, base loss: 15320.30
[INFO 2017-06-29 19:10:17,606 main.py:57] epoch 2371, training loss: 7015.98, average training loss: 7573.62, base loss: 15319.79
[INFO 2017-06-29 19:10:20,610 main.py:57] epoch 2372, training loss: 8200.81, average training loss: 7573.56, base loss: 15320.58
[INFO 2017-06-29 19:10:23,573 main.py:57] epoch 2373, training loss: 7875.30, average training loss: 7574.74, base loss: 15320.94
[INFO 2017-06-29 19:10:26,563 main.py:57] epoch 2374, training loss: 7796.56, average training loss: 7575.76, base loss: 15321.25
[INFO 2017-06-29 19:10:29,532 main.py:57] epoch 2375, training loss: 6710.47, average training loss: 7574.04, base loss: 15320.01
[INFO 2017-06-29 19:10:32,540 main.py:57] epoch 2376, training loss: 7716.52, average training loss: 7573.74, base loss: 15320.53
[INFO 2017-06-29 19:10:35,508 main.py:57] epoch 2377, training loss: 7457.01, average training loss: 7573.77, base loss: 15320.47
[INFO 2017-06-29 19:10:38,508 main.py:57] epoch 2378, training loss: 6461.05, average training loss: 7572.26, base loss: 15319.48
[INFO 2017-06-29 19:10:41,523 main.py:57] epoch 2379, training loss: 6525.52, average training loss: 7570.92, base loss: 15318.22
[INFO 2017-06-29 19:10:44,434 main.py:57] epoch 2380, training loss: 7672.35, average training loss: 7570.36, base loss: 15318.82
[INFO 2017-06-29 19:10:47,389 main.py:57] epoch 2381, training loss: 7716.13, average training loss: 7570.72, base loss: 15319.89
[INFO 2017-06-29 19:10:50,400 main.py:57] epoch 2382, training loss: 7064.61, average training loss: 7569.31, base loss: 15318.70
[INFO 2017-06-29 19:10:53,385 main.py:57] epoch 2383, training loss: 7252.84, average training loss: 7568.38, base loss: 15318.07
[INFO 2017-06-29 19:10:56,323 main.py:57] epoch 2384, training loss: 7650.87, average training loss: 7568.29, base loss: 15318.14
[INFO 2017-06-29 19:10:59,308 main.py:57] epoch 2385, training loss: 7005.12, average training loss: 7566.87, base loss: 15317.59
[INFO 2017-06-29 19:11:02,272 main.py:57] epoch 2386, training loss: 7210.41, average training loss: 7566.85, base loss: 15317.30
[INFO 2017-06-29 19:11:05,243 main.py:57] epoch 2387, training loss: 6776.16, average training loss: 7565.69, base loss: 15316.02
[INFO 2017-06-29 19:11:08,220 main.py:57] epoch 2388, training loss: 8025.61, average training loss: 7566.59, base loss: 15316.16
[INFO 2017-06-29 19:11:11,163 main.py:57] epoch 2389, training loss: 7242.23, average training loss: 7565.14, base loss: 15315.53
[INFO 2017-06-29 19:11:14,218 main.py:57] epoch 2390, training loss: 7398.88, average training loss: 7564.70, base loss: 15314.45
[INFO 2017-06-29 19:11:17,184 main.py:57] epoch 2391, training loss: 8012.54, average training loss: 7565.19, base loss: 15314.86
[INFO 2017-06-29 19:11:20,171 main.py:57] epoch 2392, training loss: 6696.45, average training loss: 7564.67, base loss: 15314.02
[INFO 2017-06-29 19:11:23,161 main.py:57] epoch 2393, training loss: 7295.80, average training loss: 7563.92, base loss: 15314.73
[INFO 2017-06-29 19:11:26,099 main.py:57] epoch 2394, training loss: 7695.85, average training loss: 7563.22, base loss: 15315.26
[INFO 2017-06-29 19:11:29,053 main.py:57] epoch 2395, training loss: 7696.63, average training loss: 7562.33, base loss: 15316.63
[INFO 2017-06-29 19:11:31,989 main.py:57] epoch 2396, training loss: 7414.57, average training loss: 7562.46, base loss: 15316.80
[INFO 2017-06-29 19:11:34,934 main.py:57] epoch 2397, training loss: 7044.17, average training loss: 7561.49, base loss: 15316.52
[INFO 2017-06-29 19:11:37,920 main.py:57] epoch 2398, training loss: 7018.07, average training loss: 7560.55, base loss: 15316.01
[INFO 2017-06-29 19:11:40,901 main.py:57] epoch 2399, training loss: 6976.10, average training loss: 7559.81, base loss: 15314.71
[INFO 2017-06-29 19:11:40,902 main.py:59] epoch 2399, testing
[INFO 2017-06-29 19:11:53,344 main.py:104] average testing loss: 7900.25, base loss: 16339.77
[INFO 2017-06-29 19:11:53,344 main.py:105] improve_loss: 8439.53, improve_percent: 0.52
[INFO 2017-06-29 19:11:53,346 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 19:11:53,385 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 19:11:56,430 main.py:57] epoch 2400, training loss: 8245.23, average training loss: 7560.03, base loss: 15315.58
[INFO 2017-06-29 19:11:59,405 main.py:57] epoch 2401, training loss: 6817.67, average training loss: 7558.84, base loss: 15315.20
[INFO 2017-06-29 19:12:02,431 main.py:57] epoch 2402, training loss: 7298.83, average training loss: 7557.95, base loss: 15316.26
[INFO 2017-06-29 19:12:05,508 main.py:57] epoch 2403, training loss: 8116.01, average training loss: 7557.57, base loss: 15317.84
[INFO 2017-06-29 19:12:08,461 main.py:57] epoch 2404, training loss: 7468.55, average training loss: 7557.14, base loss: 15318.02
[INFO 2017-06-29 19:12:11,387 main.py:57] epoch 2405, training loss: 7114.83, average training loss: 7557.18, base loss: 15317.42
[INFO 2017-06-29 19:12:14,369 main.py:57] epoch 2406, training loss: 6931.11, average training loss: 7556.32, base loss: 15316.43
[INFO 2017-06-29 19:12:17,387 main.py:57] epoch 2407, training loss: 8588.88, average training loss: 7557.18, base loss: 15316.85
[INFO 2017-06-29 19:12:20,367 main.py:57] epoch 2408, training loss: 6741.57, average training loss: 7555.88, base loss: 15315.78
[INFO 2017-06-29 19:12:23,301 main.py:57] epoch 2409, training loss: 6683.44, average training loss: 7555.73, base loss: 15314.74
[INFO 2017-06-29 19:12:26,347 main.py:57] epoch 2410, training loss: 8874.00, average training loss: 7557.44, base loss: 15315.54
[INFO 2017-06-29 19:12:29,328 main.py:57] epoch 2411, training loss: 7772.94, average training loss: 7557.49, base loss: 15314.81
[INFO 2017-06-29 19:12:32,292 main.py:57] epoch 2412, training loss: 7515.89, average training loss: 7556.22, base loss: 15315.34
[INFO 2017-06-29 19:12:35,331 main.py:57] epoch 2413, training loss: 8142.71, average training loss: 7556.48, base loss: 15315.92
[INFO 2017-06-29 19:12:38,381 main.py:57] epoch 2414, training loss: 7193.36, average training loss: 7555.98, base loss: 15314.84
[INFO 2017-06-29 19:12:41,306 main.py:57] epoch 2415, training loss: 8934.97, average training loss: 7556.83, base loss: 15316.78
[INFO 2017-06-29 19:12:44,392 main.py:57] epoch 2416, training loss: 7821.39, average training loss: 7557.54, base loss: 15318.20
[INFO 2017-06-29 19:12:47,366 main.py:57] epoch 2417, training loss: 6511.92, average training loss: 7557.44, base loss: 15317.77
[INFO 2017-06-29 19:12:50,317 main.py:57] epoch 2418, training loss: 7375.44, average training loss: 7556.24, base loss: 15318.59
[INFO 2017-06-29 19:12:53,321 main.py:57] epoch 2419, training loss: 7373.60, average training loss: 7555.80, base loss: 15317.69
[INFO 2017-06-29 19:12:56,273 main.py:57] epoch 2420, training loss: 7247.25, average training loss: 7556.13, base loss: 15317.80
[INFO 2017-06-29 19:12:59,307 main.py:57] epoch 2421, training loss: 7382.55, average training loss: 7556.41, base loss: 15317.50
[INFO 2017-06-29 19:13:02,262 main.py:57] epoch 2422, training loss: 7056.42, average training loss: 7554.72, base loss: 15316.99
[INFO 2017-06-29 19:13:05,356 main.py:57] epoch 2423, training loss: 7377.95, average training loss: 7554.44, base loss: 15316.70
[INFO 2017-06-29 19:13:08,267 main.py:57] epoch 2424, training loss: 7159.54, average training loss: 7554.07, base loss: 15316.37
[INFO 2017-06-29 19:13:11,195 main.py:57] epoch 2425, training loss: 6562.30, average training loss: 7552.65, base loss: 15315.28
[INFO 2017-06-29 19:13:14,173 main.py:57] epoch 2426, training loss: 7227.97, average training loss: 7552.93, base loss: 15315.13
[INFO 2017-06-29 19:13:17,092 main.py:57] epoch 2427, training loss: 7295.61, average training loss: 7552.72, base loss: 15316.33
[INFO 2017-06-29 19:13:20,047 main.py:57] epoch 2428, training loss: 8108.17, average training loss: 7552.29, base loss: 15317.60
[INFO 2017-06-29 19:13:22,950 main.py:57] epoch 2429, training loss: 7356.92, average training loss: 7551.43, base loss: 15317.60
[INFO 2017-06-29 19:13:25,854 main.py:57] epoch 2430, training loss: 7694.04, average training loss: 7551.50, base loss: 15318.59
[INFO 2017-06-29 19:13:28,892 main.py:57] epoch 2431, training loss: 7655.94, average training loss: 7551.67, base loss: 15318.71
[INFO 2017-06-29 19:13:31,858 main.py:57] epoch 2432, training loss: 7258.96, average training loss: 7551.46, base loss: 15317.90
[INFO 2017-06-29 19:13:34,955 main.py:57] epoch 2433, training loss: 7041.27, average training loss: 7551.38, base loss: 15318.28
[INFO 2017-06-29 19:13:37,929 main.py:57] epoch 2434, training loss: 8171.56, average training loss: 7551.94, base loss: 15317.86
[INFO 2017-06-29 19:13:40,984 main.py:57] epoch 2435, training loss: 8064.92, average training loss: 7551.48, base loss: 15317.56
[INFO 2017-06-29 19:13:43,924 main.py:57] epoch 2436, training loss: 8545.14, average training loss: 7552.17, base loss: 15319.18
[INFO 2017-06-29 19:13:46,858 main.py:57] epoch 2437, training loss: 8548.75, average training loss: 7552.63, base loss: 15319.94
[INFO 2017-06-29 19:13:49,844 main.py:57] epoch 2438, training loss: 7421.90, average training loss: 7552.03, base loss: 15319.72
[INFO 2017-06-29 19:13:52,779 main.py:57] epoch 2439, training loss: 7398.08, average training loss: 7552.19, base loss: 15319.70
[INFO 2017-06-29 19:13:55,778 main.py:57] epoch 2440, training loss: 6838.69, average training loss: 7551.51, base loss: 15319.24
[INFO 2017-06-29 19:13:58,752 main.py:57] epoch 2441, training loss: 7779.29, average training loss: 7551.22, base loss: 15320.54
[INFO 2017-06-29 19:14:01,827 main.py:57] epoch 2442, training loss: 8208.34, average training loss: 7551.56, base loss: 15320.56
[INFO 2017-06-29 19:14:04,738 main.py:57] epoch 2443, training loss: 7704.09, average training loss: 7551.44, base loss: 15321.20
[INFO 2017-06-29 19:14:07,711 main.py:57] epoch 2444, training loss: 7789.81, average training loss: 7551.25, base loss: 15322.08
[INFO 2017-06-29 19:14:10,714 main.py:57] epoch 2445, training loss: 7362.89, average training loss: 7550.70, base loss: 15323.53
[INFO 2017-06-29 19:14:13,717 main.py:57] epoch 2446, training loss: 6894.04, average training loss: 7550.76, base loss: 15322.73
[INFO 2017-06-29 19:14:16,698 main.py:57] epoch 2447, training loss: 7712.76, average training loss: 7550.23, base loss: 15323.25
[INFO 2017-06-29 19:14:19,745 main.py:57] epoch 2448, training loss: 8528.00, average training loss: 7551.41, base loss: 15323.40
[INFO 2017-06-29 19:14:22,718 main.py:57] epoch 2449, training loss: 7906.67, average training loss: 7552.28, base loss: 15323.43
[INFO 2017-06-29 19:14:25,772 main.py:57] epoch 2450, training loss: 7128.32, average training loss: 7551.85, base loss: 15323.42
[INFO 2017-06-29 19:14:28,741 main.py:57] epoch 2451, training loss: 7693.53, average training loss: 7551.60, base loss: 15323.39
[INFO 2017-06-29 19:14:31,842 main.py:57] epoch 2452, training loss: 8058.24, average training loss: 7552.20, base loss: 15323.09
[INFO 2017-06-29 19:14:34,838 main.py:57] epoch 2453, training loss: 6818.18, average training loss: 7551.21, base loss: 15322.53
[INFO 2017-06-29 19:14:37,876 main.py:57] epoch 2454, training loss: 6773.99, average training loss: 7550.18, base loss: 15322.32
[INFO 2017-06-29 19:14:40,937 main.py:57] epoch 2455, training loss: 7750.52, average training loss: 7550.71, base loss: 15322.09
[INFO 2017-06-29 19:14:43,923 main.py:57] epoch 2456, training loss: 6683.31, average training loss: 7550.28, base loss: 15321.46
[INFO 2017-06-29 19:14:46,913 main.py:57] epoch 2457, training loss: 7822.37, average training loss: 7550.24, base loss: 15322.40
[INFO 2017-06-29 19:14:49,893 main.py:57] epoch 2458, training loss: 7785.52, average training loss: 7549.49, base loss: 15322.71
[INFO 2017-06-29 19:14:52,824 main.py:57] epoch 2459, training loss: 6723.19, average training loss: 7549.11, base loss: 15322.32
[INFO 2017-06-29 19:14:55,810 main.py:57] epoch 2460, training loss: 8180.67, average training loss: 7550.02, base loss: 15322.85
[INFO 2017-06-29 19:14:58,803 main.py:57] epoch 2461, training loss: 7170.93, average training loss: 7549.76, base loss: 15322.67
[INFO 2017-06-29 19:15:01,755 main.py:57] epoch 2462, training loss: 7451.82, average training loss: 7549.96, base loss: 15323.88
[INFO 2017-06-29 19:15:04,719 main.py:57] epoch 2463, training loss: 7400.31, average training loss: 7548.42, base loss: 15323.69
[INFO 2017-06-29 19:15:07,724 main.py:57] epoch 2464, training loss: 7485.35, average training loss: 7548.70, base loss: 15323.65
[INFO 2017-06-29 19:15:10,740 main.py:57] epoch 2465, training loss: 7797.27, average training loss: 7549.45, base loss: 15323.95
[INFO 2017-06-29 19:15:13,712 main.py:57] epoch 2466, training loss: 7900.28, average training loss: 7549.07, base loss: 15324.80
[INFO 2017-06-29 19:15:16,692 main.py:57] epoch 2467, training loss: 7501.44, average training loss: 7549.86, base loss: 15324.31
[INFO 2017-06-29 19:15:19,635 main.py:57] epoch 2468, training loss: 7511.13, average training loss: 7548.64, base loss: 15323.33
[INFO 2017-06-29 19:15:22,588 main.py:57] epoch 2469, training loss: 7661.77, average training loss: 7549.58, base loss: 15323.23
[INFO 2017-06-29 19:15:25,585 main.py:57] epoch 2470, training loss: 7899.14, average training loss: 7550.26, base loss: 15322.30
[INFO 2017-06-29 19:15:28,558 main.py:57] epoch 2471, training loss: 7536.64, average training loss: 7550.04, base loss: 15322.48
[INFO 2017-06-29 19:15:31,627 main.py:57] epoch 2472, training loss: 8067.42, average training loss: 7550.59, base loss: 15323.37
[INFO 2017-06-29 19:15:34,609 main.py:57] epoch 2473, training loss: 7215.37, average training loss: 7550.35, base loss: 15323.75
[INFO 2017-06-29 19:15:37,631 main.py:57] epoch 2474, training loss: 6865.16, average training loss: 7548.76, base loss: 15323.81
[INFO 2017-06-29 19:15:40,572 main.py:57] epoch 2475, training loss: 7960.20, average training loss: 7548.75, base loss: 15323.64
[INFO 2017-06-29 19:15:43,500 main.py:57] epoch 2476, training loss: 7945.87, average training loss: 7549.43, base loss: 15323.65
[INFO 2017-06-29 19:15:46,471 main.py:57] epoch 2477, training loss: 8239.93, average training loss: 7550.47, base loss: 15323.87
[INFO 2017-06-29 19:15:49,432 main.py:57] epoch 2478, training loss: 7186.87, average training loss: 7550.33, base loss: 15323.68
[INFO 2017-06-29 19:15:52,468 main.py:57] epoch 2479, training loss: 6861.87, average training loss: 7549.41, base loss: 15322.72
[INFO 2017-06-29 19:15:55,466 main.py:57] epoch 2480, training loss: 6990.39, average training loss: 7548.87, base loss: 15321.47
[INFO 2017-06-29 19:15:58,473 main.py:57] epoch 2481, training loss: 6682.11, average training loss: 7549.05, base loss: 15320.01
[INFO 2017-06-29 19:16:01,438 main.py:57] epoch 2482, training loss: 7434.90, average training loss: 7548.63, base loss: 15321.12
[INFO 2017-06-29 19:16:04,341 main.py:57] epoch 2483, training loss: 8846.32, average training loss: 7549.17, base loss: 15323.07
[INFO 2017-06-29 19:16:07,335 main.py:57] epoch 2484, training loss: 7801.18, average training loss: 7548.66, base loss: 15322.67
[INFO 2017-06-29 19:16:10,312 main.py:57] epoch 2485, training loss: 8025.28, average training loss: 7548.68, base loss: 15323.41
[INFO 2017-06-29 19:16:13,287 main.py:57] epoch 2486, training loss: 7438.40, average training loss: 7548.90, base loss: 15323.52
[INFO 2017-06-29 19:16:16,259 main.py:57] epoch 2487, training loss: 7013.15, average training loss: 7548.27, base loss: 15322.98
[INFO 2017-06-29 19:16:19,221 main.py:57] epoch 2488, training loss: 7129.47, average training loss: 7547.04, base loss: 15321.26
[INFO 2017-06-29 19:16:22,213 main.py:57] epoch 2489, training loss: 7341.93, average training loss: 7546.56, base loss: 15321.01
[INFO 2017-06-29 19:16:25,330 main.py:57] epoch 2490, training loss: 7181.27, average training loss: 7546.41, base loss: 15321.50
[INFO 2017-06-29 19:16:28,347 main.py:57] epoch 2491, training loss: 7826.31, average training loss: 7546.17, base loss: 15321.19
[INFO 2017-06-29 19:16:31,315 main.py:57] epoch 2492, training loss: 8375.54, average training loss: 7547.09, base loss: 15322.02
[INFO 2017-06-29 19:16:34,281 main.py:57] epoch 2493, training loss: 6387.45, average training loss: 7546.97, base loss: 15320.59
[INFO 2017-06-29 19:16:37,208 main.py:57] epoch 2494, training loss: 8384.46, average training loss: 7547.84, base loss: 15322.20
[INFO 2017-06-29 19:16:40,196 main.py:57] epoch 2495, training loss: 7484.90, average training loss: 7548.57, base loss: 15322.10
[INFO 2017-06-29 19:16:43,115 main.py:57] epoch 2496, training loss: 7640.15, average training loss: 7548.37, base loss: 15322.07
[INFO 2017-06-29 19:16:46,132 main.py:57] epoch 2497, training loss: 7978.01, average training loss: 7549.02, base loss: 15321.79
[INFO 2017-06-29 19:16:49,123 main.py:57] epoch 2498, training loss: 8267.17, average training loss: 7549.46, base loss: 15323.18
[INFO 2017-06-29 19:16:52,074 main.py:57] epoch 2499, training loss: 6581.49, average training loss: 7548.00, base loss: 15323.16
[INFO 2017-06-29 19:16:52,074 main.py:59] epoch 2499, testing
[INFO 2017-06-29 19:17:04,559 main.py:104] average testing loss: 7770.73, base loss: 15993.29
[INFO 2017-06-29 19:17:04,559 main.py:105] improve_loss: 8222.56, improve_percent: 0.51
[INFO 2017-06-29 19:17:04,560 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 19:17:07,512 main.py:57] epoch 2500, training loss: 7298.95, average training loss: 7547.01, base loss: 15323.44
[INFO 2017-06-29 19:17:10,585 main.py:57] epoch 2501, training loss: 7377.35, average training loss: 7546.58, base loss: 15322.61
[INFO 2017-06-29 19:17:13,616 main.py:57] epoch 2502, training loss: 7216.70, average training loss: 7546.51, base loss: 15322.47
[INFO 2017-06-29 19:17:16,511 main.py:57] epoch 2503, training loss: 8242.64, average training loss: 7546.77, base loss: 15323.96
[INFO 2017-06-29 19:17:19,424 main.py:57] epoch 2504, training loss: 8437.22, average training loss: 7548.00, base loss: 15325.12
[INFO 2017-06-29 19:17:22,385 main.py:57] epoch 2505, training loss: 7340.72, average training loss: 7547.70, base loss: 15325.29
[INFO 2017-06-29 19:17:25,347 main.py:57] epoch 2506, training loss: 7898.93, average training loss: 7548.87, base loss: 15325.36
[INFO 2017-06-29 19:17:28,242 main.py:57] epoch 2507, training loss: 7587.59, average training loss: 7549.01, base loss: 15325.80
[INFO 2017-06-29 19:17:31,192 main.py:57] epoch 2508, training loss: 7683.08, average training loss: 7549.16, base loss: 15325.90
[INFO 2017-06-29 19:17:34,152 main.py:57] epoch 2509, training loss: 7224.43, average training loss: 7547.72, base loss: 15325.66
[INFO 2017-06-29 19:17:37,089 main.py:57] epoch 2510, training loss: 8317.70, average training loss: 7548.67, base loss: 15326.79
[INFO 2017-06-29 19:17:40,096 main.py:57] epoch 2511, training loss: 6842.49, average training loss: 7548.23, base loss: 15327.15
[INFO 2017-06-29 19:17:43,154 main.py:57] epoch 2512, training loss: 6512.69, average training loss: 7547.07, base loss: 15325.75
[INFO 2017-06-29 19:17:46,068 main.py:57] epoch 2513, training loss: 6859.39, average training loss: 7546.23, base loss: 15324.13
[INFO 2017-06-29 19:17:49,039 main.py:57] epoch 2514, training loss: 8178.58, average training loss: 7547.78, base loss: 15324.26
[INFO 2017-06-29 19:17:52,059 main.py:57] epoch 2515, training loss: 6980.72, average training loss: 7547.30, base loss: 15323.47
[INFO 2017-06-29 19:17:55,003 main.py:57] epoch 2516, training loss: 8091.71, average training loss: 7548.47, base loss: 15325.06
[INFO 2017-06-29 19:17:58,000 main.py:57] epoch 2517, training loss: 6567.37, average training loss: 7546.63, base loss: 15325.33
[INFO 2017-06-29 19:18:00,973 main.py:57] epoch 2518, training loss: 7531.74, average training loss: 7546.98, base loss: 15325.19
[INFO 2017-06-29 19:18:03,955 main.py:57] epoch 2519, training loss: 6528.73, average training loss: 7546.44, base loss: 15324.21
[INFO 2017-06-29 19:18:06,973 main.py:57] epoch 2520, training loss: 7209.18, average training loss: 7546.02, base loss: 15324.53
[INFO 2017-06-29 19:18:09,995 main.py:57] epoch 2521, training loss: 8108.29, average training loss: 7547.10, base loss: 15325.80
[INFO 2017-06-29 19:18:13,015 main.py:57] epoch 2522, training loss: 9254.93, average training loss: 7549.52, base loss: 15326.64
[INFO 2017-06-29 19:18:16,021 main.py:57] epoch 2523, training loss: 6851.12, average training loss: 7549.22, base loss: 15325.06
[INFO 2017-06-29 19:18:18,896 main.py:57] epoch 2524, training loss: 8158.68, average training loss: 7550.20, base loss: 15325.30
[INFO 2017-06-29 19:18:21,823 main.py:57] epoch 2525, training loss: 8440.48, average training loss: 7551.32, base loss: 15325.97
[INFO 2017-06-29 19:18:24,752 main.py:57] epoch 2526, training loss: 7427.85, average training loss: 7551.32, base loss: 15325.17
[INFO 2017-06-29 19:18:27,697 main.py:57] epoch 2527, training loss: 7848.95, average training loss: 7551.48, base loss: 15325.15
[INFO 2017-06-29 19:18:30,661 main.py:57] epoch 2528, training loss: 7515.00, average training loss: 7551.58, base loss: 15324.05
[INFO 2017-06-29 19:18:33,589 main.py:57] epoch 2529, training loss: 7749.57, average training loss: 7552.04, base loss: 15324.38
[INFO 2017-06-29 19:18:36,584 main.py:57] epoch 2530, training loss: 6786.93, average training loss: 7552.25, base loss: 15323.17
[INFO 2017-06-29 19:18:39,475 main.py:57] epoch 2531, training loss: 6754.07, average training loss: 7552.05, base loss: 15322.34
[INFO 2017-06-29 19:18:42,450 main.py:57] epoch 2532, training loss: 6937.65, average training loss: 7551.89, base loss: 15321.55
[INFO 2017-06-29 19:18:45,358 main.py:57] epoch 2533, training loss: 8480.29, average training loss: 7552.63, base loss: 15322.31
[INFO 2017-06-29 19:18:48,309 main.py:57] epoch 2534, training loss: 7856.95, average training loss: 7552.51, base loss: 15323.05
[INFO 2017-06-29 19:18:51,301 main.py:57] epoch 2535, training loss: 6201.64, average training loss: 7550.67, base loss: 15321.73
[INFO 2017-06-29 19:18:54,278 main.py:57] epoch 2536, training loss: 7110.08, average training loss: 7550.96, base loss: 15321.60
[INFO 2017-06-29 19:18:57,324 main.py:57] epoch 2537, training loss: 7495.76, average training loss: 7550.27, base loss: 15320.79
[INFO 2017-06-29 19:19:00,272 main.py:57] epoch 2538, training loss: 6699.63, average training loss: 7549.73, base loss: 15320.18
[INFO 2017-06-29 19:19:03,245 main.py:57] epoch 2539, training loss: 7551.57, average training loss: 7548.98, base loss: 15319.75
[INFO 2017-06-29 19:19:06,153 main.py:57] epoch 2540, training loss: 8206.74, average training loss: 7548.80, base loss: 15320.66
[INFO 2017-06-29 19:19:09,168 main.py:57] epoch 2541, training loss: 7096.13, average training loss: 7548.49, base loss: 15320.65
[INFO 2017-06-29 19:19:12,156 main.py:57] epoch 2542, training loss: 7711.03, average training loss: 7547.23, base loss: 15320.52
[INFO 2017-06-29 19:19:15,144 main.py:57] epoch 2543, training loss: 7407.76, average training loss: 7546.59, base loss: 15320.97
[INFO 2017-06-29 19:19:18,139 main.py:57] epoch 2544, training loss: 7353.33, average training loss: 7547.66, base loss: 15321.60
[INFO 2017-06-29 19:19:21,042 main.py:57] epoch 2545, training loss: 7008.43, average training loss: 7545.58, base loss: 15322.23
[INFO 2017-06-29 19:19:24,027 main.py:57] epoch 2546, training loss: 7820.66, average training loss: 7544.96, base loss: 15321.94
[INFO 2017-06-29 19:19:26,989 main.py:57] epoch 2547, training loss: 8236.96, average training loss: 7544.77, base loss: 15322.28
[INFO 2017-06-29 19:19:29,921 main.py:57] epoch 2548, training loss: 8450.74, average training loss: 7546.03, base loss: 15322.61
[INFO 2017-06-29 19:19:32,910 main.py:57] epoch 2549, training loss: 8112.42, average training loss: 7546.56, base loss: 15322.85
[INFO 2017-06-29 19:19:35,917 main.py:57] epoch 2550, training loss: 8029.64, average training loss: 7546.90, base loss: 15323.90
[INFO 2017-06-29 19:19:38,928 main.py:57] epoch 2551, training loss: 8066.50, average training loss: 7547.06, base loss: 15324.44
[INFO 2017-06-29 19:19:41,859 main.py:57] epoch 2552, training loss: 6434.47, average training loss: 7545.74, base loss: 15324.02
[INFO 2017-06-29 19:19:44,813 main.py:57] epoch 2553, training loss: 7415.81, average training loss: 7545.49, base loss: 15324.53
[INFO 2017-06-29 19:19:47,870 main.py:57] epoch 2554, training loss: 7135.59, average training loss: 7544.95, base loss: 15325.00
[INFO 2017-06-29 19:19:50,873 main.py:57] epoch 2555, training loss: 7504.09, average training loss: 7544.71, base loss: 15325.63
[INFO 2017-06-29 19:19:53,844 main.py:57] epoch 2556, training loss: 6147.12, average training loss: 7543.11, base loss: 15325.21
[INFO 2017-06-29 19:19:56,847 main.py:57] epoch 2557, training loss: 7592.45, average training loss: 7543.02, base loss: 15326.19
[INFO 2017-06-29 19:19:59,920 main.py:57] epoch 2558, training loss: 6705.45, average training loss: 7543.28, base loss: 15324.10
[INFO 2017-06-29 19:20:02,937 main.py:57] epoch 2559, training loss: 7723.94, average training loss: 7544.29, base loss: 15324.12
[INFO 2017-06-29 19:20:05,903 main.py:57] epoch 2560, training loss: 7487.50, average training loss: 7543.93, base loss: 15325.02
[INFO 2017-06-29 19:20:08,816 main.py:57] epoch 2561, training loss: 7523.00, average training loss: 7543.21, base loss: 15325.65
[INFO 2017-06-29 19:20:11,808 main.py:57] epoch 2562, training loss: 7818.33, average training loss: 7543.00, base loss: 15326.67
[INFO 2017-06-29 19:20:14,784 main.py:57] epoch 2563, training loss: 7230.26, average training loss: 7543.26, base loss: 15327.02
[INFO 2017-06-29 19:20:17,709 main.py:57] epoch 2564, training loss: 7807.53, average training loss: 7543.78, base loss: 15327.49
[INFO 2017-06-29 19:20:20,673 main.py:57] epoch 2565, training loss: 7059.93, average training loss: 7543.21, base loss: 15326.90
[INFO 2017-06-29 19:20:23,586 main.py:57] epoch 2566, training loss: 7150.69, average training loss: 7543.02, base loss: 15326.36
[INFO 2017-06-29 19:20:26,530 main.py:57] epoch 2567, training loss: 6616.53, average training loss: 7541.95, base loss: 15325.90
[INFO 2017-06-29 19:20:29,659 main.py:57] epoch 2568, training loss: 7480.62, average training loss: 7541.62, base loss: 15326.35
[INFO 2017-06-29 19:20:32,604 main.py:57] epoch 2569, training loss: 7634.00, average training loss: 7541.79, base loss: 15326.95
[INFO 2017-06-29 19:20:35,561 main.py:57] epoch 2570, training loss: 7666.12, average training loss: 7542.96, base loss: 15326.56
[INFO 2017-06-29 19:20:38,459 main.py:57] epoch 2571, training loss: 7119.98, average training loss: 7542.94, base loss: 15326.02
[INFO 2017-06-29 19:20:41,433 main.py:57] epoch 2572, training loss: 6767.78, average training loss: 7541.49, base loss: 15324.87
[INFO 2017-06-29 19:20:44,438 main.py:57] epoch 2573, training loss: 7946.67, average training loss: 7541.64, base loss: 15325.61
[INFO 2017-06-29 19:20:47,362 main.py:57] epoch 2574, training loss: 7306.89, average training loss: 7541.21, base loss: 15325.40
[INFO 2017-06-29 19:20:50,302 main.py:57] epoch 2575, training loss: 7866.60, average training loss: 7541.49, base loss: 15326.10
[INFO 2017-06-29 19:20:53,200 main.py:57] epoch 2576, training loss: 6991.15, average training loss: 7540.98, base loss: 15324.28
[INFO 2017-06-29 19:20:56,113 main.py:57] epoch 2577, training loss: 8020.10, average training loss: 7541.58, base loss: 15324.01
[INFO 2017-06-29 19:20:59,006 main.py:57] epoch 2578, training loss: 8124.59, average training loss: 7542.42, base loss: 15323.91
[INFO 2017-06-29 19:21:01,992 main.py:57] epoch 2579, training loss: 6889.75, average training loss: 7541.33, base loss: 15322.46
[INFO 2017-06-29 19:21:04,895 main.py:57] epoch 2580, training loss: 8642.01, average training loss: 7542.40, base loss: 15323.87
[INFO 2017-06-29 19:21:07,926 main.py:57] epoch 2581, training loss: 6907.88, average training loss: 7541.40, base loss: 15322.07
[INFO 2017-06-29 19:21:10,912 main.py:57] epoch 2582, training loss: 6856.00, average training loss: 7540.77, base loss: 15321.43
[INFO 2017-06-29 19:21:13,916 main.py:57] epoch 2583, training loss: 8204.90, average training loss: 7540.98, base loss: 15321.35
[INFO 2017-06-29 19:21:16,883 main.py:57] epoch 2584, training loss: 7201.17, average training loss: 7541.24, base loss: 15321.31
[INFO 2017-06-29 19:21:19,831 main.py:57] epoch 2585, training loss: 7101.66, average training loss: 7540.01, base loss: 15320.66
[INFO 2017-06-29 19:21:22,866 main.py:57] epoch 2586, training loss: 7150.43, average training loss: 7539.96, base loss: 15320.63
[INFO 2017-06-29 19:21:25,808 main.py:57] epoch 2587, training loss: 7200.35, average training loss: 7539.67, base loss: 15319.96
[INFO 2017-06-29 19:21:28,758 main.py:57] epoch 2588, training loss: 6449.43, average training loss: 7538.47, base loss: 15319.15
[INFO 2017-06-29 19:21:31,730 main.py:57] epoch 2589, training loss: 7661.66, average training loss: 7537.59, base loss: 15319.02
[INFO 2017-06-29 19:21:34,684 main.py:57] epoch 2590, training loss: 7049.14, average training loss: 7537.60, base loss: 15319.32
[INFO 2017-06-29 19:21:37,646 main.py:57] epoch 2591, training loss: 7978.79, average training loss: 7537.18, base loss: 15319.69
[INFO 2017-06-29 19:21:40,594 main.py:57] epoch 2592, training loss: 7325.59, average training loss: 7536.95, base loss: 15319.64
[INFO 2017-06-29 19:21:43,572 main.py:57] epoch 2593, training loss: 7755.45, average training loss: 7537.29, base loss: 15319.75
[INFO 2017-06-29 19:21:46,546 main.py:57] epoch 2594, training loss: 7617.84, average training loss: 7536.47, base loss: 15319.90
[INFO 2017-06-29 19:21:49,437 main.py:57] epoch 2595, training loss: 7793.34, average training loss: 7535.96, base loss: 15321.12
[INFO 2017-06-29 19:21:52,449 main.py:57] epoch 2596, training loss: 7780.90, average training loss: 7536.27, base loss: 15321.41
[INFO 2017-06-29 19:21:55,361 main.py:57] epoch 2597, training loss: 7047.58, average training loss: 7535.15, base loss: 15321.06
[INFO 2017-06-29 19:21:58,311 main.py:57] epoch 2598, training loss: 7599.22, average training loss: 7535.55, base loss: 15321.06
[INFO 2017-06-29 19:22:01,258 main.py:57] epoch 2599, training loss: 7828.65, average training loss: 7536.09, base loss: 15320.21
[INFO 2017-06-29 19:22:01,258 main.py:59] epoch 2599, testing
[INFO 2017-06-29 19:22:13,754 main.py:104] average testing loss: 7830.48, base loss: 16228.49
[INFO 2017-06-29 19:22:13,754 main.py:105] improve_loss: 8398.01, improve_percent: 0.52
[INFO 2017-06-29 19:22:13,756 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 19:22:13,782 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 19:22:16,798 main.py:57] epoch 2600, training loss: 7623.65, average training loss: 7534.94, base loss: 15321.36
[INFO 2017-06-29 19:22:19,778 main.py:57] epoch 2601, training loss: 7436.06, average training loss: 7534.27, base loss: 15321.26
[INFO 2017-06-29 19:22:22,743 main.py:57] epoch 2602, training loss: 6503.49, average training loss: 7533.06, base loss: 15320.09
[INFO 2017-06-29 19:22:25,693 main.py:57] epoch 2603, training loss: 7614.06, average training loss: 7533.30, base loss: 15320.92
[INFO 2017-06-29 19:22:28,728 main.py:57] epoch 2604, training loss: 7679.14, average training loss: 7533.14, base loss: 15321.30
[INFO 2017-06-29 19:22:31,653 main.py:57] epoch 2605, training loss: 7700.82, average training loss: 7532.51, base loss: 15321.09
[INFO 2017-06-29 19:22:34,679 main.py:57] epoch 2606, training loss: 7512.36, average training loss: 7532.61, base loss: 15319.87
[INFO 2017-06-29 19:22:37,638 main.py:57] epoch 2607, training loss: 6682.80, average training loss: 7531.98, base loss: 15318.39
[INFO 2017-06-29 19:22:40,561 main.py:57] epoch 2608, training loss: 7127.67, average training loss: 7530.83, base loss: 15318.34
[INFO 2017-06-29 19:22:43,458 main.py:57] epoch 2609, training loss: 7709.76, average training loss: 7531.43, base loss: 15318.40
[INFO 2017-06-29 19:22:46,422 main.py:57] epoch 2610, training loss: 6951.49, average training loss: 7530.89, base loss: 15318.90
[INFO 2017-06-29 19:22:49,468 main.py:57] epoch 2611, training loss: 7393.00, average training loss: 7531.04, base loss: 15319.05
[INFO 2017-06-29 19:22:52,406 main.py:57] epoch 2612, training loss: 7147.79, average training loss: 7530.60, base loss: 15318.83
[INFO 2017-06-29 19:22:55,343 main.py:57] epoch 2613, training loss: 7270.59, average training loss: 7529.91, base loss: 15318.41
[INFO 2017-06-29 19:22:58,379 main.py:57] epoch 2614, training loss: 6837.16, average training loss: 7529.15, base loss: 15316.95
[INFO 2017-06-29 19:23:01,328 main.py:57] epoch 2615, training loss: 6889.44, average training loss: 7528.94, base loss: 15316.09
[INFO 2017-06-29 19:23:04,253 main.py:57] epoch 2616, training loss: 7484.90, average training loss: 7529.04, base loss: 15315.58
[INFO 2017-06-29 19:23:07,235 main.py:57] epoch 2617, training loss: 7703.63, average training loss: 7529.01, base loss: 15315.78
[INFO 2017-06-29 19:23:10,161 main.py:57] epoch 2618, training loss: 7920.83, average training loss: 7530.10, base loss: 15316.71
[INFO 2017-06-29 19:23:13,040 main.py:57] epoch 2619, training loss: 6914.07, average training loss: 7529.42, base loss: 15316.24
[INFO 2017-06-29 19:23:16,014 main.py:57] epoch 2620, training loss: 6745.94, average training loss: 7528.92, base loss: 15315.85
[INFO 2017-06-29 19:23:19,008 main.py:57] epoch 2621, training loss: 7187.73, average training loss: 7528.89, base loss: 15315.83
[INFO 2017-06-29 19:23:21,913 main.py:57] epoch 2622, training loss: 7432.33, average training loss: 7529.31, base loss: 15315.76
[INFO 2017-06-29 19:23:24,791 main.py:57] epoch 2623, training loss: 7606.38, average training loss: 7529.25, base loss: 15316.91
[INFO 2017-06-29 19:23:27,743 main.py:57] epoch 2624, training loss: 7565.72, average training loss: 7529.45, base loss: 15318.03
[INFO 2017-06-29 19:23:30,774 main.py:57] epoch 2625, training loss: 7967.25, average training loss: 7528.75, base loss: 15317.98
[INFO 2017-06-29 19:23:33,697 main.py:57] epoch 2626, training loss: 8212.91, average training loss: 7529.20, base loss: 15318.65
[INFO 2017-06-29 19:23:36,700 main.py:57] epoch 2627, training loss: 8197.51, average training loss: 7529.98, base loss: 15319.34
[INFO 2017-06-29 19:23:39,709 main.py:57] epoch 2628, training loss: 7952.94, average training loss: 7531.00, base loss: 15319.79
[INFO 2017-06-29 19:23:42,682 main.py:57] epoch 2629, training loss: 8323.91, average training loss: 7531.86, base loss: 15320.90
[INFO 2017-06-29 19:23:45,681 main.py:57] epoch 2630, training loss: 7367.04, average training loss: 7532.05, base loss: 15320.95
[INFO 2017-06-29 19:23:48,661 main.py:57] epoch 2631, training loss: 7599.56, average training loss: 7531.34, base loss: 15320.64
[INFO 2017-06-29 19:23:51,606 main.py:57] epoch 2632, training loss: 7432.93, average training loss: 7531.61, base loss: 15319.94
[INFO 2017-06-29 19:23:54,588 main.py:57] epoch 2633, training loss: 7262.79, average training loss: 7531.79, base loss: 15319.70
[INFO 2017-06-29 19:23:57,572 main.py:57] epoch 2634, training loss: 6935.60, average training loss: 7531.48, base loss: 15318.17
[INFO 2017-06-29 19:24:00,548 main.py:57] epoch 2635, training loss: 6798.51, average training loss: 7531.43, base loss: 15316.60
[INFO 2017-06-29 19:24:03,562 main.py:57] epoch 2636, training loss: 7265.79, average training loss: 7530.45, base loss: 15317.05
[INFO 2017-06-29 19:24:06,520 main.py:57] epoch 2637, training loss: 7567.72, average training loss: 7530.94, base loss: 15317.40
[INFO 2017-06-29 19:24:09,544 main.py:57] epoch 2638, training loss: 7357.99, average training loss: 7530.59, base loss: 15317.45
[INFO 2017-06-29 19:24:12,543 main.py:57] epoch 2639, training loss: 7708.55, average training loss: 7530.72, base loss: 15317.88
[INFO 2017-06-29 19:24:15,506 main.py:57] epoch 2640, training loss: 8357.25, average training loss: 7530.66, base loss: 15318.69
[INFO 2017-06-29 19:24:18,412 main.py:57] epoch 2641, training loss: 7072.49, average training loss: 7529.11, base loss: 15317.74
[INFO 2017-06-29 19:24:21,337 main.py:57] epoch 2642, training loss: 7216.61, average training loss: 7529.34, base loss: 15317.41
[INFO 2017-06-29 19:24:24,298 main.py:57] epoch 2643, training loss: 7423.97, average training loss: 7528.63, base loss: 15317.70
[INFO 2017-06-29 19:24:27,279 main.py:57] epoch 2644, training loss: 7076.65, average training loss: 7527.99, base loss: 15316.42
[INFO 2017-06-29 19:24:30,239 main.py:57] epoch 2645, training loss: 6504.99, average training loss: 7526.95, base loss: 15315.27
[INFO 2017-06-29 19:24:33,236 main.py:57] epoch 2646, training loss: 7593.22, average training loss: 7526.55, base loss: 15316.08
[INFO 2017-06-29 19:24:36,231 main.py:57] epoch 2647, training loss: 7870.44, average training loss: 7526.07, base loss: 15315.73
[INFO 2017-06-29 19:24:39,181 main.py:57] epoch 2648, training loss: 8157.26, average training loss: 7526.99, base loss: 15316.53
[INFO 2017-06-29 19:24:42,127 main.py:57] epoch 2649, training loss: 7096.59, average training loss: 7526.43, base loss: 15316.60
[INFO 2017-06-29 19:24:45,058 main.py:57] epoch 2650, training loss: 7175.43, average training loss: 7525.83, base loss: 15316.11
[INFO 2017-06-29 19:24:48,084 main.py:57] epoch 2651, training loss: 7391.16, average training loss: 7525.42, base loss: 15315.70
[INFO 2017-06-29 19:24:51,052 main.py:57] epoch 2652, training loss: 7039.71, average training loss: 7525.07, base loss: 15315.84
[INFO 2017-06-29 19:24:54,024 main.py:57] epoch 2653, training loss: 7802.42, average training loss: 7525.06, base loss: 15317.49
[INFO 2017-06-29 19:24:57,005 main.py:57] epoch 2654, training loss: 7727.13, average training loss: 7525.98, base loss: 15317.44
[INFO 2017-06-29 19:24:59,943 main.py:57] epoch 2655, training loss: 6296.59, average training loss: 7524.59, base loss: 15315.53
[INFO 2017-06-29 19:25:02,913 main.py:57] epoch 2656, training loss: 7379.09, average training loss: 7523.74, base loss: 15314.61
[INFO 2017-06-29 19:25:05,783 main.py:57] epoch 2657, training loss: 6576.23, average training loss: 7523.37, base loss: 15312.93
[INFO 2017-06-29 19:25:08,724 main.py:57] epoch 2658, training loss: 7496.40, average training loss: 7524.20, base loss: 15314.59
[INFO 2017-06-29 19:25:11,617 main.py:57] epoch 2659, training loss: 8149.42, average training loss: 7524.64, base loss: 15316.48
[INFO 2017-06-29 19:25:14,517 main.py:57] epoch 2660, training loss: 6858.99, average training loss: 7524.58, base loss: 15315.77
[INFO 2017-06-29 19:25:17,470 main.py:57] epoch 2661, training loss: 6953.27, average training loss: 7523.66, base loss: 15315.27
[INFO 2017-06-29 19:25:20,412 main.py:57] epoch 2662, training loss: 7041.97, average training loss: 7523.62, base loss: 15315.39
[INFO 2017-06-29 19:25:23,382 main.py:57] epoch 2663, training loss: 6558.94, average training loss: 7523.11, base loss: 15313.65
[INFO 2017-06-29 19:25:26,443 main.py:57] epoch 2664, training loss: 7522.08, average training loss: 7523.14, base loss: 15313.60
[INFO 2017-06-29 19:25:29,434 main.py:57] epoch 2665, training loss: 7691.61, average training loss: 7522.51, base loss: 15314.44
[INFO 2017-06-29 19:25:32,404 main.py:57] epoch 2666, training loss: 7975.87, average training loss: 7522.30, base loss: 15315.37
[INFO 2017-06-29 19:25:35,410 main.py:57] epoch 2667, training loss: 7513.89, average training loss: 7521.45, base loss: 15315.51
[INFO 2017-06-29 19:25:38,398 main.py:57] epoch 2668, training loss: 8026.95, average training loss: 7522.47, base loss: 15316.68
[INFO 2017-06-29 19:25:41,291 main.py:57] epoch 2669, training loss: 7497.33, average training loss: 7521.87, base loss: 15317.59
[INFO 2017-06-29 19:25:44,256 main.py:57] epoch 2670, training loss: 8016.58, average training loss: 7522.27, base loss: 15317.46
[INFO 2017-06-29 19:25:47,168 main.py:57] epoch 2671, training loss: 7066.75, average training loss: 7521.50, base loss: 15317.24
[INFO 2017-06-29 19:25:50,241 main.py:57] epoch 2672, training loss: 9326.19, average training loss: 7523.67, base loss: 15319.79
[INFO 2017-06-29 19:25:53,265 main.py:57] epoch 2673, training loss: 6294.58, average training loss: 7522.67, base loss: 15318.75
[INFO 2017-06-29 19:25:56,266 main.py:57] epoch 2674, training loss: 7721.21, average training loss: 7523.13, base loss: 15320.50
[INFO 2017-06-29 19:25:59,248 main.py:57] epoch 2675, training loss: 6374.04, average training loss: 7521.21, base loss: 15319.86
[INFO 2017-06-29 19:26:02,232 main.py:57] epoch 2676, training loss: 7723.56, average training loss: 7520.90, base loss: 15319.62
[INFO 2017-06-29 19:26:05,272 main.py:57] epoch 2677, training loss: 6411.10, average training loss: 7520.60, base loss: 15318.10
[INFO 2017-06-29 19:26:08,207 main.py:57] epoch 2678, training loss: 7215.23, average training loss: 7519.59, base loss: 15317.89
[INFO 2017-06-29 19:26:11,153 main.py:57] epoch 2679, training loss: 6880.79, average training loss: 7518.38, base loss: 15317.72
[INFO 2017-06-29 19:26:14,062 main.py:57] epoch 2680, training loss: 7128.33, average training loss: 7517.40, base loss: 15317.07
[INFO 2017-06-29 19:26:17,023 main.py:57] epoch 2681, training loss: 6587.06, average training loss: 7515.98, base loss: 15316.52
[INFO 2017-06-29 19:26:19,965 main.py:57] epoch 2682, training loss: 7237.32, average training loss: 7513.86, base loss: 15316.08
[INFO 2017-06-29 19:26:22,914 main.py:57] epoch 2683, training loss: 7701.74, average training loss: 7514.28, base loss: 15317.27
[INFO 2017-06-29 19:26:25,888 main.py:57] epoch 2684, training loss: 7957.25, average training loss: 7515.23, base loss: 15317.37
[INFO 2017-06-29 19:26:28,903 main.py:57] epoch 2685, training loss: 7836.54, average training loss: 7515.16, base loss: 15318.15
[INFO 2017-06-29 19:26:31,890 main.py:57] epoch 2686, training loss: 7685.80, average training loss: 7515.15, base loss: 15317.86
[INFO 2017-06-29 19:26:34,811 main.py:57] epoch 2687, training loss: 7597.38, average training loss: 7515.48, base loss: 15317.39
[INFO 2017-06-29 19:26:37,767 main.py:57] epoch 2688, training loss: 7789.59, average training loss: 7515.55, base loss: 15317.08
[INFO 2017-06-29 19:26:40,693 main.py:57] epoch 2689, training loss: 6738.65, average training loss: 7515.66, base loss: 15315.63
[INFO 2017-06-29 19:26:43,593 main.py:57] epoch 2690, training loss: 7318.04, average training loss: 7514.90, base loss: 15315.57
[INFO 2017-06-29 19:26:46,588 main.py:57] epoch 2691, training loss: 7483.00, average training loss: 7514.77, base loss: 15315.30
[INFO 2017-06-29 19:26:49,555 main.py:57] epoch 2692, training loss: 7822.59, average training loss: 7515.37, base loss: 15315.00
[INFO 2017-06-29 19:26:52,523 main.py:57] epoch 2693, training loss: 7409.95, average training loss: 7515.23, base loss: 15314.94
[INFO 2017-06-29 19:26:55,562 main.py:57] epoch 2694, training loss: 6813.58, average training loss: 7514.28, base loss: 15314.20
[INFO 2017-06-29 19:26:58,488 main.py:57] epoch 2695, training loss: 6871.80, average training loss: 7513.86, base loss: 15314.11
[INFO 2017-06-29 19:27:01,566 main.py:57] epoch 2696, training loss: 6590.93, average training loss: 7513.12, base loss: 15312.84
[INFO 2017-06-29 19:27:04,552 main.py:57] epoch 2697, training loss: 7599.77, average training loss: 7513.15, base loss: 15311.85
[INFO 2017-06-29 19:27:07,524 main.py:57] epoch 2698, training loss: 7897.92, average training loss: 7514.09, base loss: 15312.50
[INFO 2017-06-29 19:27:10,434 main.py:57] epoch 2699, training loss: 6932.74, average training loss: 7513.02, base loss: 15312.37
[INFO 2017-06-29 19:27:10,435 main.py:59] epoch 2699, testing
[INFO 2017-06-29 19:27:22,834 main.py:104] average testing loss: 7727.00, base loss: 16007.24
[INFO 2017-06-29 19:27:22,834 main.py:105] improve_loss: 8280.23, improve_percent: 0.52
[INFO 2017-06-29 19:27:22,835 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 19:27:25,755 main.py:57] epoch 2700, training loss: 7492.83, average training loss: 7511.48, base loss: 15311.82
[INFO 2017-06-29 19:27:28,669 main.py:57] epoch 2701, training loss: 7776.54, average training loss: 7512.41, base loss: 15311.53
[INFO 2017-06-29 19:27:31,612 main.py:57] epoch 2702, training loss: 6294.57, average training loss: 7510.85, base loss: 15310.54
[INFO 2017-06-29 19:27:34,745 main.py:57] epoch 2703, training loss: 7123.30, average training loss: 7510.66, base loss: 15310.63
[INFO 2017-06-29 19:27:37,682 main.py:57] epoch 2704, training loss: 7751.80, average training loss: 7510.17, base loss: 15311.56
[INFO 2017-06-29 19:27:40,594 main.py:57] epoch 2705, training loss: 8327.17, average training loss: 7510.97, base loss: 15312.68
[INFO 2017-06-29 19:27:43,609 main.py:57] epoch 2706, training loss: 7108.39, average training loss: 7510.27, base loss: 15311.85
[INFO 2017-06-29 19:27:46,555 main.py:57] epoch 2707, training loss: 6834.06, average training loss: 7509.48, base loss: 15311.54
[INFO 2017-06-29 19:27:49,535 main.py:57] epoch 2708, training loss: 8455.89, average training loss: 7510.82, base loss: 15312.87
[INFO 2017-06-29 19:27:52,541 main.py:57] epoch 2709, training loss: 7202.65, average training loss: 7509.61, base loss: 15312.87
[INFO 2017-06-29 19:27:55,532 main.py:57] epoch 2710, training loss: 7646.07, average training loss: 7509.66, base loss: 15312.15
[INFO 2017-06-29 19:27:58,490 main.py:57] epoch 2711, training loss: 7840.57, average training loss: 7509.24, base loss: 15311.98
[INFO 2017-06-29 19:28:01,486 main.py:57] epoch 2712, training loss: 7826.18, average training loss: 7509.83, base loss: 15311.95
[INFO 2017-06-29 19:28:04,454 main.py:57] epoch 2713, training loss: 7985.61, average training loss: 7509.64, base loss: 15313.39
[INFO 2017-06-29 19:28:07,342 main.py:57] epoch 2714, training loss: 7500.04, average training loss: 7509.35, base loss: 15313.33
[INFO 2017-06-29 19:28:10,333 main.py:57] epoch 2715, training loss: 6783.63, average training loss: 7508.40, base loss: 15312.78
[INFO 2017-06-29 19:28:13,278 main.py:57] epoch 2716, training loss: 6935.76, average training loss: 7507.03, base loss: 15312.13
[INFO 2017-06-29 19:28:16,197 main.py:57] epoch 2717, training loss: 7118.41, average training loss: 7506.37, base loss: 15311.82
[INFO 2017-06-29 19:28:19,211 main.py:57] epoch 2718, training loss: 7439.44, average training loss: 7506.84, base loss: 15312.22
[INFO 2017-06-29 19:28:22,173 main.py:57] epoch 2719, training loss: 7726.11, average training loss: 7507.07, base loss: 15313.50
[INFO 2017-06-29 19:28:25,149 main.py:57] epoch 2720, training loss: 6999.90, average training loss: 7507.43, base loss: 15314.85
[INFO 2017-06-29 19:28:28,274 main.py:57] epoch 2721, training loss: 6740.37, average training loss: 7507.19, base loss: 15314.34
[INFO 2017-06-29 19:28:31,217 main.py:57] epoch 2722, training loss: 8104.75, average training loss: 7508.48, base loss: 15315.19
[INFO 2017-06-29 19:28:34,196 main.py:57] epoch 2723, training loss: 8032.21, average training loss: 7508.87, base loss: 15316.57
[INFO 2017-06-29 19:28:37,179 main.py:57] epoch 2724, training loss: 7635.46, average training loss: 7507.64, base loss: 15316.67
[INFO 2017-06-29 19:28:40,270 main.py:57] epoch 2725, training loss: 8200.06, average training loss: 7507.09, base loss: 15316.79
[INFO 2017-06-29 19:28:43,270 main.py:57] epoch 2726, training loss: 7790.11, average training loss: 7507.19, base loss: 15316.62
[INFO 2017-06-29 19:28:46,308 main.py:57] epoch 2727, training loss: 7468.82, average training loss: 7507.63, base loss: 15316.52
[INFO 2017-06-29 19:28:49,303 main.py:57] epoch 2728, training loss: 8476.52, average training loss: 7507.73, base loss: 15318.09
[INFO 2017-06-29 19:28:52,256 main.py:57] epoch 2729, training loss: 7646.52, average training loss: 7507.58, base loss: 15318.11
[INFO 2017-06-29 19:28:55,307 main.py:57] epoch 2730, training loss: 7519.49, average training loss: 7507.18, base loss: 15318.65
[INFO 2017-06-29 19:28:58,221 main.py:57] epoch 2731, training loss: 8077.53, average training loss: 7507.39, base loss: 15319.47
[INFO 2017-06-29 19:29:01,258 main.py:57] epoch 2732, training loss: 8124.33, average training loss: 7507.45, base loss: 15319.28
[INFO 2017-06-29 19:29:04,258 main.py:57] epoch 2733, training loss: 7115.56, average training loss: 7506.93, base loss: 15318.53
[INFO 2017-06-29 19:29:07,144 main.py:57] epoch 2734, training loss: 6577.78, average training loss: 7504.80, base loss: 15316.93
[INFO 2017-06-29 19:29:10,046 main.py:57] epoch 2735, training loss: 8193.08, average training loss: 7505.90, base loss: 15317.21
[INFO 2017-06-29 19:29:13,037 main.py:57] epoch 2736, training loss: 7890.13, average training loss: 7506.43, base loss: 15317.37
[INFO 2017-06-29 19:29:15,998 main.py:57] epoch 2737, training loss: 6991.38, average training loss: 7506.21, base loss: 15317.72
[INFO 2017-06-29 19:29:18,976 main.py:57] epoch 2738, training loss: 8004.45, average training loss: 7507.01, base loss: 15318.18
[INFO 2017-06-29 19:29:21,909 main.py:57] epoch 2739, training loss: 6656.79, average training loss: 7506.40, base loss: 15318.40
[INFO 2017-06-29 19:29:24,901 main.py:57] epoch 2740, training loss: 7438.56, average training loss: 7506.37, base loss: 15317.89
[INFO 2017-06-29 19:29:27,809 main.py:57] epoch 2741, training loss: 7098.84, average training loss: 7506.06, base loss: 15317.11
[INFO 2017-06-29 19:29:30,858 main.py:57] epoch 2742, training loss: 7952.80, average training loss: 7507.36, base loss: 15317.86
[INFO 2017-06-29 19:29:33,872 main.py:57] epoch 2743, training loss: 7803.34, average training loss: 7508.75, base loss: 15318.20
[INFO 2017-06-29 19:29:36,936 main.py:57] epoch 2744, training loss: 7340.61, average training loss: 7508.66, base loss: 15318.97
[INFO 2017-06-29 19:29:39,990 main.py:57] epoch 2745, training loss: 7192.52, average training loss: 7508.14, base loss: 15319.06
[INFO 2017-06-29 19:29:42,972 main.py:57] epoch 2746, training loss: 6581.31, average training loss: 7506.65, base loss: 15317.82
[INFO 2017-06-29 19:29:45,870 main.py:57] epoch 2747, training loss: 7719.71, average training loss: 7506.56, base loss: 15318.14
[INFO 2017-06-29 19:29:48,958 main.py:57] epoch 2748, training loss: 7927.46, average training loss: 7507.02, base loss: 15318.49
[INFO 2017-06-29 19:29:51,947 main.py:57] epoch 2749, training loss: 6605.95, average training loss: 7506.90, base loss: 15317.62
[INFO 2017-06-29 19:29:54,874 main.py:57] epoch 2750, training loss: 8059.55, average training loss: 7506.42, base loss: 15316.56
[INFO 2017-06-29 19:29:57,882 main.py:57] epoch 2751, training loss: 6489.43, average training loss: 7504.15, base loss: 15314.37
[INFO 2017-06-29 19:30:00,920 main.py:57] epoch 2752, training loss: 6757.15, average training loss: 7503.23, base loss: 15313.58
[INFO 2017-06-29 19:30:03,857 main.py:57] epoch 2753, training loss: 6889.48, average training loss: 7502.76, base loss: 15313.26
[INFO 2017-06-29 19:30:06,869 main.py:57] epoch 2754, training loss: 7701.44, average training loss: 7502.76, base loss: 15313.23
[INFO 2017-06-29 19:30:09,962 main.py:57] epoch 2755, training loss: 6873.73, average training loss: 7501.82, base loss: 15313.02
[INFO 2017-06-29 19:30:12,952 main.py:57] epoch 2756, training loss: 7819.57, average training loss: 7501.94, base loss: 15313.43
[INFO 2017-06-29 19:30:15,926 main.py:57] epoch 2757, training loss: 6895.05, average training loss: 7500.56, base loss: 15313.33
[INFO 2017-06-29 19:30:18,872 main.py:57] epoch 2758, training loss: 6205.32, average training loss: 7500.18, base loss: 15312.15
[INFO 2017-06-29 19:30:21,850 main.py:57] epoch 2759, training loss: 7562.37, average training loss: 7500.32, base loss: 15311.77
[INFO 2017-06-29 19:30:24,819 main.py:57] epoch 2760, training loss: 7216.52, average training loss: 7500.76, base loss: 15311.73
[INFO 2017-06-29 19:30:27,806 main.py:57] epoch 2761, training loss: 7260.80, average training loss: 7501.12, base loss: 15311.13
[INFO 2017-06-29 19:30:30,703 main.py:57] epoch 2762, training loss: 7352.87, average training loss: 7501.52, base loss: 15310.84
[INFO 2017-06-29 19:30:33,615 main.py:57] epoch 2763, training loss: 7178.53, average training loss: 7501.40, base loss: 15310.00
[INFO 2017-06-29 19:30:36,537 main.py:57] epoch 2764, training loss: 8035.92, average training loss: 7500.86, base loss: 15311.80
[INFO 2017-06-29 19:30:39,460 main.py:57] epoch 2765, training loss: 7465.81, average training loss: 7501.25, base loss: 15312.02
[INFO 2017-06-29 19:30:42,479 main.py:57] epoch 2766, training loss: 8169.51, average training loss: 7501.42, base loss: 15312.64
[INFO 2017-06-29 19:30:45,476 main.py:57] epoch 2767, training loss: 8335.80, average training loss: 7502.89, base loss: 15313.72
[INFO 2017-06-29 19:30:48,477 main.py:57] epoch 2768, training loss: 7366.81, average training loss: 7502.88, base loss: 15313.74
[INFO 2017-06-29 19:30:51,408 main.py:57] epoch 2769, training loss: 7759.04, average training loss: 7502.74, base loss: 15313.48
[INFO 2017-06-29 19:30:54,338 main.py:57] epoch 2770, training loss: 7645.17, average training loss: 7502.80, base loss: 15313.85
[INFO 2017-06-29 19:30:57,345 main.py:57] epoch 2771, training loss: 7471.31, average training loss: 7502.59, base loss: 15313.04
[INFO 2017-06-29 19:31:00,266 main.py:57] epoch 2772, training loss: 7730.81, average training loss: 7501.66, base loss: 15314.22
[INFO 2017-06-29 19:31:03,232 main.py:57] epoch 2773, training loss: 7238.51, average training loss: 7501.97, base loss: 15314.23
[INFO 2017-06-29 19:31:06,227 main.py:57] epoch 2774, training loss: 6713.85, average training loss: 7501.12, base loss: 15313.69
[INFO 2017-06-29 19:31:09,218 main.py:57] epoch 2775, training loss: 7961.51, average training loss: 7502.06, base loss: 15314.74
[INFO 2017-06-29 19:31:12,182 main.py:57] epoch 2776, training loss: 8399.71, average training loss: 7502.81, base loss: 15315.77
[INFO 2017-06-29 19:31:15,118 main.py:57] epoch 2777, training loss: 7877.32, average training loss: 7502.59, base loss: 15316.40
[INFO 2017-06-29 19:31:18,078 main.py:57] epoch 2778, training loss: 7688.14, average training loss: 7502.82, base loss: 15317.33
[INFO 2017-06-29 19:31:21,021 main.py:57] epoch 2779, training loss: 7618.74, average training loss: 7503.12, base loss: 15316.86
[INFO 2017-06-29 19:31:24,025 main.py:57] epoch 2780, training loss: 6943.24, average training loss: 7502.69, base loss: 15316.11
[INFO 2017-06-29 19:31:27,053 main.py:57] epoch 2781, training loss: 7554.18, average training loss: 7501.91, base loss: 15315.66
[INFO 2017-06-29 19:31:30,047 main.py:57] epoch 2782, training loss: 7952.05, average training loss: 7502.40, base loss: 15315.71
[INFO 2017-06-29 19:31:33,008 main.py:57] epoch 2783, training loss: 9173.26, average training loss: 7504.66, base loss: 15317.26
[INFO 2017-06-29 19:31:35,959 main.py:57] epoch 2784, training loss: 7876.47, average training loss: 7505.32, base loss: 15318.54
[INFO 2017-06-29 19:31:38,912 main.py:57] epoch 2785, training loss: 6485.32, average training loss: 7503.33, base loss: 15318.32
[INFO 2017-06-29 19:31:41,940 main.py:57] epoch 2786, training loss: 6643.90, average training loss: 7502.51, base loss: 15317.65
[INFO 2017-06-29 19:31:44,994 main.py:57] epoch 2787, training loss: 6898.12, average training loss: 7502.38, base loss: 15317.42
[INFO 2017-06-29 19:31:47,935 main.py:57] epoch 2788, training loss: 7344.01, average training loss: 7501.87, base loss: 15317.33
[INFO 2017-06-29 19:31:50,887 main.py:57] epoch 2789, training loss: 7017.01, average training loss: 7501.88, base loss: 15317.43
[INFO 2017-06-29 19:31:53,886 main.py:57] epoch 2790, training loss: 7249.27, average training loss: 7501.01, base loss: 15317.10
[INFO 2017-06-29 19:31:56,885 main.py:57] epoch 2791, training loss: 8056.66, average training loss: 7501.82, base loss: 15317.36
[INFO 2017-06-29 19:31:59,863 main.py:57] epoch 2792, training loss: 7498.35, average training loss: 7502.48, base loss: 15317.77
[INFO 2017-06-29 19:32:02,779 main.py:57] epoch 2793, training loss: 7606.61, average training loss: 7502.11, base loss: 15317.78
[INFO 2017-06-29 19:32:05,726 main.py:57] epoch 2794, training loss: 6770.07, average training loss: 7502.25, base loss: 15317.54
[INFO 2017-06-29 19:32:08,748 main.py:57] epoch 2795, training loss: 7162.05, average training loss: 7502.27, base loss: 15317.47
[INFO 2017-06-29 19:32:11,720 main.py:57] epoch 2796, training loss: 8143.45, average training loss: 7502.44, base loss: 15317.36
[INFO 2017-06-29 19:32:14,654 main.py:57] epoch 2797, training loss: 7128.91, average training loss: 7501.40, base loss: 15316.53
[INFO 2017-06-29 19:32:17,547 main.py:57] epoch 2798, training loss: 6984.22, average training loss: 7500.10, base loss: 15316.27
[INFO 2017-06-29 19:32:20,489 main.py:57] epoch 2799, training loss: 6956.27, average training loss: 7499.74, base loss: 15316.82
[INFO 2017-06-29 19:32:20,489 main.py:59] epoch 2799, testing
[INFO 2017-06-29 19:32:32,897 main.py:104] average testing loss: 7331.79, base loss: 15382.80
[INFO 2017-06-29 19:32:32,897 main.py:105] improve_loss: 8051.02, improve_percent: 0.52
[INFO 2017-06-29 19:32:32,899 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 19:32:32,925 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 19:32:35,899 main.py:57] epoch 2800, training loss: 6827.65, average training loss: 7499.19, base loss: 15316.45
[INFO 2017-06-29 19:32:38,843 main.py:57] epoch 2801, training loss: 7375.47, average training loss: 7498.81, base loss: 15316.82
[INFO 2017-06-29 19:32:41,831 main.py:57] epoch 2802, training loss: 7162.15, average training loss: 7498.80, base loss: 15317.79
[INFO 2017-06-29 19:32:44,808 main.py:57] epoch 2803, training loss: 6962.07, average training loss: 7498.37, base loss: 15317.62
[INFO 2017-06-29 19:32:47,811 main.py:57] epoch 2804, training loss: 7199.20, average training loss: 7497.93, base loss: 15317.54
[INFO 2017-06-29 19:32:50,787 main.py:57] epoch 2805, training loss: 6994.62, average training loss: 7497.62, base loss: 15317.05
[INFO 2017-06-29 19:32:53,806 main.py:57] epoch 2806, training loss: 7973.36, average training loss: 7498.85, base loss: 15317.45
[INFO 2017-06-29 19:32:56,756 main.py:57] epoch 2807, training loss: 7694.98, average training loss: 7499.57, base loss: 15317.47
[INFO 2017-06-29 19:32:59,746 main.py:57] epoch 2808, training loss: 7955.50, average training loss: 7499.63, base loss: 15317.81
[INFO 2017-06-29 19:33:02,674 main.py:57] epoch 2809, training loss: 7137.06, average training loss: 7499.99, base loss: 15317.28
[INFO 2017-06-29 19:33:05,668 main.py:57] epoch 2810, training loss: 6648.77, average training loss: 7499.86, base loss: 15315.36
[INFO 2017-06-29 19:33:08,638 main.py:57] epoch 2811, training loss: 8530.25, average training loss: 7500.92, base loss: 15315.13
[INFO 2017-06-29 19:33:11,612 main.py:57] epoch 2812, training loss: 7551.16, average training loss: 7500.86, base loss: 15315.67
[INFO 2017-06-29 19:33:14,564 main.py:57] epoch 2813, training loss: 7502.26, average training loss: 7500.27, base loss: 15316.14
[INFO 2017-06-29 19:33:17,514 main.py:57] epoch 2814, training loss: 6783.43, average training loss: 7499.62, base loss: 15315.28
[INFO 2017-06-29 19:33:20,450 main.py:57] epoch 2815, training loss: 6720.12, average training loss: 7498.68, base loss: 15314.70
[INFO 2017-06-29 19:33:23,430 main.py:57] epoch 2816, training loss: 8265.63, average training loss: 7500.65, base loss: 15315.13
[INFO 2017-06-29 19:33:26,481 main.py:57] epoch 2817, training loss: 6879.58, average training loss: 7499.45, base loss: 15314.51
[INFO 2017-06-29 19:33:29,486 main.py:57] epoch 2818, training loss: 8242.91, average training loss: 7499.59, base loss: 15315.47
[INFO 2017-06-29 19:33:32,373 main.py:57] epoch 2819, training loss: 7684.51, average training loss: 7498.77, base loss: 15315.51
[INFO 2017-06-29 19:33:35,344 main.py:57] epoch 2820, training loss: 6819.70, average training loss: 7497.85, base loss: 15315.17
[INFO 2017-06-29 19:33:38,385 main.py:57] epoch 2821, training loss: 8430.64, average training loss: 7498.28, base loss: 15316.57
[INFO 2017-06-29 19:33:41,314 main.py:57] epoch 2822, training loss: 7111.80, average training loss: 7497.59, base loss: 15316.71
[INFO 2017-06-29 19:33:44,218 main.py:57] epoch 2823, training loss: 6704.75, average training loss: 7496.30, base loss: 15315.49
[INFO 2017-06-29 19:33:47,177 main.py:57] epoch 2824, training loss: 6673.51, average training loss: 7495.37, base loss: 15314.85
[INFO 2017-06-29 19:33:50,154 main.py:57] epoch 2825, training loss: 6769.34, average training loss: 7495.64, base loss: 15314.74
[INFO 2017-06-29 19:33:53,111 main.py:57] epoch 2826, training loss: 6543.22, average training loss: 7494.55, base loss: 15313.28
[INFO 2017-06-29 19:33:56,130 main.py:57] epoch 2827, training loss: 8125.47, average training loss: 7494.63, base loss: 15313.34
[INFO 2017-06-29 19:33:59,286 main.py:57] epoch 2828, training loss: 7463.80, average training loss: 7494.47, base loss: 15313.52
[INFO 2017-06-29 19:34:02,325 main.py:57] epoch 2829, training loss: 7203.88, average training loss: 7493.95, base loss: 15313.14
[INFO 2017-06-29 19:34:05,345 main.py:57] epoch 2830, training loss: 7288.60, average training loss: 7493.00, base loss: 15312.67
[INFO 2017-06-29 19:34:08,249 main.py:57] epoch 2831, training loss: 7408.01, average training loss: 7492.95, base loss: 15312.73
[INFO 2017-06-29 19:34:11,168 main.py:57] epoch 2832, training loss: 7633.27, average training loss: 7493.17, base loss: 15312.91
[INFO 2017-06-29 19:34:14,145 main.py:57] epoch 2833, training loss: 7995.52, average training loss: 7494.02, base loss: 15313.82
[INFO 2017-06-29 19:34:17,193 main.py:57] epoch 2834, training loss: 7700.38, average training loss: 7493.40, base loss: 15313.64
[INFO 2017-06-29 19:34:20,163 main.py:57] epoch 2835, training loss: 7116.17, average training loss: 7493.70, base loss: 15313.29
[INFO 2017-06-29 19:34:23,155 main.py:57] epoch 2836, training loss: 7690.32, average training loss: 7494.58, base loss: 15313.23
[INFO 2017-06-29 19:34:26,171 main.py:57] epoch 2837, training loss: 7548.96, average training loss: 7493.93, base loss: 15312.89
[INFO 2017-06-29 19:34:29,155 main.py:57] epoch 2838, training loss: 7246.39, average training loss: 7493.09, base loss: 15313.09
[INFO 2017-06-29 19:34:32,142 main.py:57] epoch 2839, training loss: 6405.71, average training loss: 7491.16, base loss: 15311.71
[INFO 2017-06-29 19:34:35,111 main.py:57] epoch 2840, training loss: 6529.86, average training loss: 7489.98, base loss: 15311.49
[INFO 2017-06-29 19:34:38,036 main.py:57] epoch 2841, training loss: 7336.37, average training loss: 7490.04, base loss: 15310.92
[INFO 2017-06-29 19:34:40,964 main.py:57] epoch 2842, training loss: 7470.66, average training loss: 7491.05, base loss: 15310.55
[INFO 2017-06-29 19:34:43,942 main.py:57] epoch 2843, training loss: 7100.91, average training loss: 7489.63, base loss: 15310.41
[INFO 2017-06-29 19:34:46,877 main.py:57] epoch 2844, training loss: 7565.94, average training loss: 7489.85, base loss: 15310.82
[INFO 2017-06-29 19:34:49,864 main.py:57] epoch 2845, training loss: 7703.26, average training loss: 7488.07, base loss: 15311.11
[INFO 2017-06-29 19:34:52,866 main.py:57] epoch 2846, training loss: 7012.31, average training loss: 7487.40, base loss: 15311.59
