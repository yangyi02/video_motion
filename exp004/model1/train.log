[INFO 2017-06-27 20:23:55,671 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=25, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-64-few', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64-few', train_epoch=10000)
[INFO 2017-06-27 20:23:58,061 main.py:51] epoch 0, training loss: 30750.37, average training loss: 30750.37, base loss: 22498.29
[INFO 2017-06-27 20:23:58,374 main.py:51] epoch 1, training loss: 29877.82, average training loss: 30314.09, base loss: 22326.35
[INFO 2017-06-27 20:23:58,678 main.py:51] epoch 2, training loss: 24543.36, average training loss: 28390.52, base loss: 20643.22
[INFO 2017-06-27 20:23:58,987 main.py:51] epoch 3, training loss: 24283.55, average training loss: 27363.77, base loss: 20173.35
[INFO 2017-06-27 20:23:59,300 main.py:51] epoch 4, training loss: 23963.39, average training loss: 26683.70, base loss: 20242.13
[INFO 2017-06-27 20:23:59,614 main.py:51] epoch 5, training loss: 21454.62, average training loss: 25812.18, base loss: 20022.95
[INFO 2017-06-27 20:23:59,927 main.py:51] epoch 6, training loss: 21720.84, average training loss: 25227.71, base loss: 19875.94
[INFO 2017-06-27 20:24:00,238 main.py:51] epoch 7, training loss: 20715.56, average training loss: 24663.69, base loss: 19790.41
[INFO 2017-06-27 20:24:00,552 main.py:51] epoch 8, training loss: 23232.04, average training loss: 24504.62, base loss: 20058.69
[INFO 2017-06-27 20:24:00,865 main.py:51] epoch 9, training loss: 20828.03, average training loss: 24136.96, base loss: 20046.06
[INFO 2017-06-27 20:24:01,179 main.py:51] epoch 10, training loss: 22110.64, average training loss: 23952.75, base loss: 20157.29
[INFO 2017-06-27 20:24:01,483 main.py:51] epoch 11, training loss: 19220.58, average training loss: 23558.40, base loss: 20019.06
[INFO 2017-06-27 20:24:01,799 main.py:51] epoch 12, training loss: 20065.68, average training loss: 23289.73, base loss: 19949.51
[INFO 2017-06-27 20:24:02,108 main.py:51] epoch 13, training loss: 22261.23, average training loss: 23216.26, base loss: 20121.15
[INFO 2017-06-27 20:24:02,414 main.py:51] epoch 14, training loss: 20001.24, average training loss: 23001.93, base loss: 20116.84
[INFO 2017-06-27 20:24:02,718 main.py:51] epoch 15, training loss: 17824.13, average training loss: 22678.32, base loss: 19965.10
[INFO 2017-06-27 20:24:03,031 main.py:51] epoch 16, training loss: 22178.82, average training loss: 22648.94, base loss: 20126.45
[INFO 2017-06-27 20:24:03,342 main.py:51] epoch 17, training loss: 17727.39, average training loss: 22375.52, base loss: 19987.16
[INFO 2017-06-27 20:24:03,650 main.py:51] epoch 18, training loss: 19032.97, average training loss: 22199.59, base loss: 19942.48
[INFO 2017-06-27 20:24:03,966 main.py:51] epoch 19, training loss: 21767.68, average training loss: 22178.00, base loss: 20072.97
[INFO 2017-06-27 20:24:04,285 main.py:51] epoch 20, training loss: 20400.39, average training loss: 22093.35, base loss: 20125.28
[INFO 2017-06-27 20:24:04,597 main.py:51] epoch 21, training loss: 17350.95, average training loss: 21877.79, base loss: 20017.96
[INFO 2017-06-27 20:24:04,911 main.py:51] epoch 22, training loss: 20803.76, average training loss: 21831.09, base loss: 20088.85
[INFO 2017-06-27 20:24:05,226 main.py:51] epoch 23, training loss: 17164.66, average training loss: 21636.65, base loss: 19985.71
[INFO 2017-06-27 20:24:05,539 main.py:51] epoch 24, training loss: 18717.40, average training loss: 21519.88, base loss: 19956.68
[INFO 2017-06-27 20:24:05,851 main.py:51] epoch 25, training loss: 20981.73, average training loss: 21499.19, base loss: 20035.01
[INFO 2017-06-27 20:24:06,166 main.py:51] epoch 26, training loss: 17042.37, average training loss: 21334.12, base loss: 19951.75
[INFO 2017-06-27 20:24:06,469 main.py:51] epoch 27, training loss: 16389.48, average training loss: 21157.52, base loss: 19833.30
[INFO 2017-06-27 20:24:06,775 main.py:51] epoch 28, training loss: 16878.56, average training loss: 21009.97, base loss: 19759.30
[INFO 2017-06-27 20:24:07,090 main.py:51] epoch 29, training loss: 21741.77, average training loss: 21034.37, base loss: 19859.48
[INFO 2017-06-27 20:24:07,410 main.py:51] epoch 30, training loss: 17425.86, average training loss: 20917.96, base loss: 19804.10
[INFO 2017-06-27 20:24:07,722 main.py:51] epoch 31, training loss: 21088.65, average training loss: 20923.30, base loss: 19889.47
[INFO 2017-06-27 20:24:08,035 main.py:51] epoch 32, training loss: 19428.77, average training loss: 20878.01, base loss: 19908.65
[INFO 2017-06-27 20:24:08,341 main.py:51] epoch 33, training loss: 19566.10, average training loss: 20839.42, base loss: 19920.01
[INFO 2017-06-27 20:24:08,663 main.py:51] epoch 34, training loss: 19036.06, average training loss: 20787.90, base loss: 19933.46
[INFO 2017-06-27 20:24:08,979 main.py:51] epoch 35, training loss: 19018.34, average training loss: 20738.74, base loss: 19940.40
[INFO 2017-06-27 20:24:09,291 main.py:51] epoch 36, training loss: 20007.46, average training loss: 20718.98, base loss: 19975.27
[INFO 2017-06-27 20:24:09,598 main.py:51] epoch 37, training loss: 19352.42, average training loss: 20683.02, base loss: 19994.26
[INFO 2017-06-27 20:24:09,914 main.py:51] epoch 38, training loss: 19868.63, average training loss: 20662.14, base loss: 20020.15
[INFO 2017-06-27 20:24:10,230 main.py:51] epoch 39, training loss: 18449.48, average training loss: 20606.82, base loss: 20025.10
[INFO 2017-06-27 20:24:10,547 main.py:51] epoch 40, training loss: 15212.76, average training loss: 20475.26, base loss: 19929.70
[INFO 2017-06-27 20:24:10,860 main.py:51] epoch 41, training loss: 19805.37, average training loss: 20459.31, base loss: 19955.20
[INFO 2017-06-27 20:24:11,174 main.py:51] epoch 42, training loss: 19037.57, average training loss: 20426.24, base loss: 19971.11
[INFO 2017-06-27 20:24:11,494 main.py:51] epoch 43, training loss: 19790.38, average training loss: 20411.79, base loss: 20003.21
[INFO 2017-06-27 20:24:11,809 main.py:51] epoch 44, training loss: 17341.22, average training loss: 20343.56, base loss: 19965.81
[INFO 2017-06-27 20:24:12,116 main.py:51] epoch 45, training loss: 16268.51, average training loss: 20254.97, base loss: 19907.80
[INFO 2017-06-27 20:24:12,433 main.py:51] epoch 46, training loss: 18904.29, average training loss: 20226.23, base loss: 19917.42
[INFO 2017-06-27 20:24:12,743 main.py:51] epoch 47, training loss: 16491.34, average training loss: 20148.42, base loss: 19873.44
[INFO 2017-06-27 20:24:13,056 main.py:51] epoch 48, training loss: 19423.91, average training loss: 20133.64, base loss: 19905.75
[INFO 2017-06-27 20:24:13,366 main.py:51] epoch 49, training loss: 19525.15, average training loss: 20121.47, base loss: 19925.89
[INFO 2017-06-27 20:24:13,678 main.py:51] epoch 50, training loss: 18455.98, average training loss: 20088.81, base loss: 19931.46
[INFO 2017-06-27 20:24:13,984 main.py:51] epoch 51, training loss: 17757.51, average training loss: 20043.98, base loss: 19912.46
[INFO 2017-06-27 20:24:14,300 main.py:51] epoch 52, training loss: 16371.63, average training loss: 19974.69, base loss: 19866.82
[INFO 2017-06-27 20:24:14,614 main.py:51] epoch 53, training loss: 21790.42, average training loss: 20008.31, base loss: 19930.84
[INFO 2017-06-27 20:24:14,919 main.py:51] epoch 54, training loss: 16804.38, average training loss: 19950.06, base loss: 19903.84
[INFO 2017-06-27 20:24:15,226 main.py:51] epoch 55, training loss: 16363.92, average training loss: 19886.02, base loss: 19870.65
[INFO 2017-06-27 20:24:15,546 main.py:51] epoch 56, training loss: 17552.56, average training loss: 19845.08, base loss: 19868.63
[INFO 2017-06-27 20:24:15,861 main.py:51] epoch 57, training loss: 18262.20, average training loss: 19817.79, base loss: 19871.62
[INFO 2017-06-27 20:24:16,174 main.py:51] epoch 58, training loss: 15229.72, average training loss: 19740.03, base loss: 19812.91
[INFO 2017-06-27 20:24:16,489 main.py:51] epoch 59, training loss: 17594.57, average training loss: 19704.27, base loss: 19798.31
[INFO 2017-06-27 20:24:16,803 main.py:51] epoch 60, training loss: 17587.15, average training loss: 19669.56, base loss: 19792.41
[INFO 2017-06-27 20:24:17,117 main.py:51] epoch 61, training loss: 20971.20, average training loss: 19690.56, base loss: 19849.32
[INFO 2017-06-27 20:24:17,432 main.py:51] epoch 62, training loss: 19040.79, average training loss: 19680.24, base loss: 19864.76
[INFO 2017-06-27 20:24:17,752 main.py:51] epoch 63, training loss: 19320.87, average training loss: 19674.63, base loss: 19879.68
[INFO 2017-06-27 20:24:18,070 main.py:51] epoch 64, training loss: 18364.65, average training loss: 19654.47, base loss: 19893.77
[INFO 2017-06-27 20:24:18,392 main.py:51] epoch 65, training loss: 19646.08, average training loss: 19654.35, base loss: 19923.86
[INFO 2017-06-27 20:24:18,716 main.py:51] epoch 66, training loss: 18338.52, average training loss: 19634.71, base loss: 19924.42
[INFO 2017-06-27 20:24:19,038 main.py:51] epoch 67, training loss: 17959.95, average training loss: 19610.08, base loss: 19935.99
[INFO 2017-06-27 20:24:19,365 main.py:51] epoch 68, training loss: 16996.93, average training loss: 19572.21, base loss: 19920.75
[INFO 2017-06-27 20:24:19,698 main.py:51] epoch 69, training loss: 19663.53, average training loss: 19573.51, base loss: 19952.49
[INFO 2017-06-27 20:24:20,026 main.py:51] epoch 70, training loss: 17099.05, average training loss: 19538.66, base loss: 19939.35
[INFO 2017-06-27 20:24:20,365 main.py:51] epoch 71, training loss: 20098.75, average training loss: 19546.44, base loss: 19973.66
[INFO 2017-06-27 20:24:20,706 main.py:51] epoch 72, training loss: 16210.10, average training loss: 19500.74, base loss: 19940.61
[INFO 2017-06-27 20:24:21,050 main.py:51] epoch 73, training loss: 17645.81, average training loss: 19475.67, base loss: 19933.09
[INFO 2017-06-27 20:24:21,388 main.py:51] epoch 74, training loss: 18372.90, average training loss: 19460.97, base loss: 19949.57
[INFO 2017-06-27 20:24:21,738 main.py:51] epoch 75, training loss: 20257.47, average training loss: 19471.45, base loss: 19978.80
[INFO 2017-06-27 20:24:22,080 main.py:51] epoch 76, training loss: 18739.84, average training loss: 19461.95, base loss: 19986.43
[INFO 2017-06-27 20:24:22,426 main.py:51] epoch 77, training loss: 18073.00, average training loss: 19444.14, base loss: 19985.68
[INFO 2017-06-27 20:24:22,764 main.py:51] epoch 78, training loss: 18083.60, average training loss: 19426.92, base loss: 19988.40
[INFO 2017-06-27 20:24:23,105 main.py:51] epoch 79, training loss: 18735.95, average training loss: 19418.28, base loss: 20006.39
[INFO 2017-06-27 20:24:23,447 main.py:51] epoch 80, training loss: 18312.33, average training loss: 19404.63, base loss: 20013.31
[INFO 2017-06-27 20:24:23,790 main.py:51] epoch 81, training loss: 15658.58, average training loss: 19358.94, base loss: 19979.22
[INFO 2017-06-27 20:24:24,140 main.py:51] epoch 82, training loss: 18182.36, average training loss: 19344.77, base loss: 19989.64
[INFO 2017-06-27 20:24:24,494 main.py:51] epoch 83, training loss: 17462.27, average training loss: 19322.36, base loss: 19984.12
[INFO 2017-06-27 20:24:24,859 main.py:51] epoch 84, training loss: 19050.76, average training loss: 19319.16, base loss: 20000.47
[INFO 2017-06-27 20:24:25,227 main.py:51] epoch 85, training loss: 17767.04, average training loss: 19301.11, base loss: 19996.09
[INFO 2017-06-27 20:24:25,602 main.py:51] epoch 86, training loss: 17737.26, average training loss: 19283.14, base loss: 20007.24
[INFO 2017-06-27 20:24:25,971 main.py:51] epoch 87, training loss: 22047.01, average training loss: 19314.54, base loss: 20064.54
[INFO 2017-06-27 20:24:26,343 main.py:51] epoch 88, training loss: 17195.72, average training loss: 19290.74, base loss: 20052.07
[INFO 2017-06-27 20:24:26,718 main.py:51] epoch 89, training loss: 15404.25, average training loss: 19247.55, base loss: 20027.85
[INFO 2017-06-27 20:24:27,098 main.py:51] epoch 90, training loss: 18398.26, average training loss: 19238.22, base loss: 20029.82
[INFO 2017-06-27 20:24:27,471 main.py:51] epoch 91, training loss: 18689.25, average training loss: 19232.25, base loss: 20046.60
[INFO 2017-06-27 20:24:27,850 main.py:51] epoch 92, training loss: 18101.86, average training loss: 19220.10, base loss: 20058.41
[INFO 2017-06-27 20:24:28,223 main.py:51] epoch 93, training loss: 18895.42, average training loss: 19216.65, base loss: 20086.96
[INFO 2017-06-27 20:24:28,605 main.py:51] epoch 94, training loss: 17003.86, average training loss: 19193.35, base loss: 20084.71
[INFO 2017-06-27 20:24:28,977 main.py:51] epoch 95, training loss: 18332.95, average training loss: 19184.39, base loss: 20099.16
[INFO 2017-06-27 20:24:29,355 main.py:51] epoch 96, training loss: 18872.20, average training loss: 19181.17, base loss: 20112.20
[INFO 2017-06-27 20:24:29,732 main.py:51] epoch 97, training loss: 17760.21, average training loss: 19166.67, base loss: 20106.08
[INFO 2017-06-27 20:24:30,110 main.py:51] epoch 98, training loss: 18490.33, average training loss: 19159.84, base loss: 20114.49
[INFO 2017-06-27 20:24:30,492 main.py:51] epoch 99, training loss: 17836.98, average training loss: 19146.61, base loss: 20118.31
[INFO 2017-06-27 20:24:30,492 main.py:53] epoch 99, testing
[INFO 2017-06-27 20:24:32,103 main.py:105] average testing loss: 17263.60, base loss: 19673.68
[INFO 2017-06-27 20:24:32,103 main.py:106] improve_loss: 2410.08, improve_percent: 0.12
[INFO 2017-06-27 20:24:32,104 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:24:32,115 main.py:76] current best improved percent: 0.12
[INFO 2017-06-27 20:24:32,491 main.py:51] epoch 100, training loss: 16771.36, average training loss: 19123.09, base loss: 20105.49
[INFO 2017-06-27 20:24:32,875 main.py:51] epoch 101, training loss: 17708.61, average training loss: 19109.23, base loss: 20093.58
[INFO 2017-06-27 20:24:33,257 main.py:51] epoch 102, training loss: 18003.86, average training loss: 19098.50, base loss: 20101.26
[INFO 2017-06-27 20:24:33,639 main.py:51] epoch 103, training loss: 15160.18, average training loss: 19060.63, base loss: 20072.99
[INFO 2017-06-27 20:24:34,018 main.py:51] epoch 104, training loss: 18069.46, average training loss: 19051.19, base loss: 20076.49
[INFO 2017-06-27 20:24:34,402 main.py:51] epoch 105, training loss: 21366.84, average training loss: 19073.03, base loss: 20116.20
[INFO 2017-06-27 20:24:34,783 main.py:51] epoch 106, training loss: 19206.54, average training loss: 19074.28, base loss: 20135.83
[INFO 2017-06-27 20:24:35,155 main.py:51] epoch 107, training loss: 17200.04, average training loss: 19056.93, base loss: 20138.70
[INFO 2017-06-27 20:24:35,531 main.py:51] epoch 108, training loss: 19977.68, average training loss: 19065.37, base loss: 20164.52
[INFO 2017-06-27 20:24:35,907 main.py:51] epoch 109, training loss: 21712.45, average training loss: 19089.44, base loss: 20214.02
[INFO 2017-06-27 20:24:36,283 main.py:51] epoch 110, training loss: 15882.32, average training loss: 19060.55, base loss: 20196.45
[INFO 2017-06-27 20:24:36,655 main.py:51] epoch 111, training loss: 16741.77, average training loss: 19039.84, base loss: 20185.20
[INFO 2017-06-27 20:24:37,033 main.py:51] epoch 112, training loss: 15301.91, average training loss: 19006.76, base loss: 20158.43
[INFO 2017-06-27 20:24:37,408 main.py:51] epoch 113, training loss: 17983.99, average training loss: 18997.79, base loss: 20161.15
[INFO 2017-06-27 20:24:37,786 main.py:51] epoch 114, training loss: 16900.02, average training loss: 18979.55, base loss: 20157.40
[INFO 2017-06-27 20:24:38,161 main.py:51] epoch 115, training loss: 17583.31, average training loss: 18967.51, base loss: 20158.97
[INFO 2017-06-27 20:24:38,536 main.py:51] epoch 116, training loss: 17722.48, average training loss: 18956.87, base loss: 20158.59
[INFO 2017-06-27 20:24:38,910 main.py:51] epoch 117, training loss: 17022.95, average training loss: 18940.48, base loss: 20160.41
[INFO 2017-06-27 20:24:39,280 main.py:51] epoch 118, training loss: 14707.64, average training loss: 18904.91, base loss: 20132.99
[INFO 2017-06-27 20:24:39,656 main.py:51] epoch 119, training loss: 17675.09, average training loss: 18894.66, base loss: 20138.21
[INFO 2017-06-27 20:24:40,030 main.py:51] epoch 120, training loss: 14450.20, average training loss: 18857.93, base loss: 20107.58
[INFO 2017-06-27 20:24:40,407 main.py:51] epoch 121, training loss: 15284.91, average training loss: 18828.65, base loss: 20086.68
[INFO 2017-06-27 20:24:40,784 main.py:51] epoch 122, training loss: 16725.20, average training loss: 18811.54, base loss: 20079.34
[INFO 2017-06-27 20:24:41,157 main.py:51] epoch 123, training loss: 16728.36, average training loss: 18794.75, base loss: 20074.69
[INFO 2017-06-27 20:24:41,535 main.py:51] epoch 124, training loss: 15877.99, average training loss: 18771.41, base loss: 20060.29
[INFO 2017-06-27 20:24:41,923 main.py:51] epoch 125, training loss: 17137.06, average training loss: 18758.44, base loss: 20060.12
[INFO 2017-06-27 20:24:42,304 main.py:51] epoch 126, training loss: 16735.48, average training loss: 18742.51, base loss: 20051.85
[INFO 2017-06-27 20:24:42,681 main.py:51] epoch 127, training loss: 14698.67, average training loss: 18710.92, base loss: 20025.26
[INFO 2017-06-27 20:24:43,060 main.py:51] epoch 128, training loss: 18769.71, average training loss: 18711.37, base loss: 20039.09
[INFO 2017-06-27 20:24:43,431 main.py:51] epoch 129, training loss: 15443.59, average training loss: 18686.24, base loss: 20020.37
[INFO 2017-06-27 20:24:43,822 main.py:51] epoch 130, training loss: 15040.93, average training loss: 18658.41, base loss: 20005.70
[INFO 2017-06-27 20:24:44,196 main.py:51] epoch 131, training loss: 16188.19, average training loss: 18639.70, base loss: 19995.45
[INFO 2017-06-27 20:24:44,569 main.py:51] epoch 132, training loss: 17199.50, average training loss: 18628.87, base loss: 19997.43
[INFO 2017-06-27 20:24:44,944 main.py:51] epoch 133, training loss: 17772.85, average training loss: 18622.48, base loss: 19998.14
[INFO 2017-06-27 20:24:45,314 main.py:51] epoch 134, training loss: 16036.91, average training loss: 18603.33, base loss: 19990.05
[INFO 2017-06-27 20:24:45,692 main.py:51] epoch 135, training loss: 16331.30, average training loss: 18586.62, base loss: 19974.86
[INFO 2017-06-27 20:24:46,066 main.py:51] epoch 136, training loss: 16939.44, average training loss: 18574.60, base loss: 19978.08
[INFO 2017-06-27 20:24:46,442 main.py:51] epoch 137, training loss: 15039.66, average training loss: 18548.98, base loss: 19961.34
[INFO 2017-06-27 20:24:46,820 main.py:51] epoch 138, training loss: 15767.86, average training loss: 18528.98, base loss: 19960.74
[INFO 2017-06-27 20:24:47,195 main.py:51] epoch 139, training loss: 17076.52, average training loss: 18518.60, base loss: 19965.72
[INFO 2017-06-27 20:24:47,569 main.py:51] epoch 140, training loss: 15978.82, average training loss: 18500.59, base loss: 19953.39
[INFO 2017-06-27 20:24:47,939 main.py:51] epoch 141, training loss: 13421.67, average training loss: 18464.82, base loss: 19924.57
[INFO 2017-06-27 20:24:48,316 main.py:51] epoch 142, training loss: 17558.88, average training loss: 18458.49, base loss: 19930.12
[INFO 2017-06-27 20:24:48,693 main.py:51] epoch 143, training loss: 16442.40, average training loss: 18444.48, base loss: 19926.17
[INFO 2017-06-27 20:24:49,070 main.py:51] epoch 144, training loss: 18630.04, average training loss: 18445.76, base loss: 19941.90
[INFO 2017-06-27 20:24:49,446 main.py:51] epoch 145, training loss: 15683.66, average training loss: 18426.85, base loss: 19940.04
[INFO 2017-06-27 20:24:49,831 main.py:51] epoch 146, training loss: 16692.64, average training loss: 18415.05, base loss: 19935.20
[INFO 2017-06-27 20:24:50,207 main.py:51] epoch 147, training loss: 18854.97, average training loss: 18418.02, base loss: 19953.91
[INFO 2017-06-27 20:24:50,587 main.py:51] epoch 148, training loss: 17635.99, average training loss: 18412.77, base loss: 19965.55
[INFO 2017-06-27 20:24:50,965 main.py:51] epoch 149, training loss: 16140.24, average training loss: 18397.62, base loss: 19959.69
[INFO 2017-06-27 20:24:51,343 main.py:51] epoch 150, training loss: 16475.22, average training loss: 18384.89, base loss: 19959.96
[INFO 2017-06-27 20:24:51,718 main.py:51] epoch 151, training loss: 16692.88, average training loss: 18373.76, base loss: 19970.20
[INFO 2017-06-27 20:24:52,103 main.py:51] epoch 152, training loss: 17080.44, average training loss: 18365.31, base loss: 19972.69
[INFO 2017-06-27 20:24:52,482 main.py:51] epoch 153, training loss: 14811.00, average training loss: 18342.23, base loss: 19949.81
[INFO 2017-06-27 20:24:52,858 main.py:51] epoch 154, training loss: 18493.88, average training loss: 18343.21, base loss: 19964.43
[INFO 2017-06-27 20:24:53,233 main.py:51] epoch 155, training loss: 21051.11, average training loss: 18360.56, base loss: 19997.07
[INFO 2017-06-27 20:24:53,615 main.py:51] epoch 156, training loss: 17844.39, average training loss: 18357.28, base loss: 20002.29
[INFO 2017-06-27 20:24:53,988 main.py:51] epoch 157, training loss: 16730.71, average training loss: 18346.98, base loss: 20002.89
[INFO 2017-06-27 20:24:54,370 main.py:51] epoch 158, training loss: 18860.62, average training loss: 18350.21, base loss: 20022.82
[INFO 2017-06-27 20:24:54,747 main.py:51] epoch 159, training loss: 16570.24, average training loss: 18339.09, base loss: 20016.16
[INFO 2017-06-27 20:24:55,154 main.py:51] epoch 160, training loss: 16439.83, average training loss: 18327.29, base loss: 20011.16
[INFO 2017-06-27 20:24:55,542 main.py:51] epoch 161, training loss: 14938.80, average training loss: 18306.37, base loss: 20001.42
[INFO 2017-06-27 20:24:55,926 main.py:51] epoch 162, training loss: 16235.46, average training loss: 18293.67, base loss: 20001.82
[INFO 2017-06-27 20:24:56,313 main.py:51] epoch 163, training loss: 17629.87, average training loss: 18289.62, base loss: 20009.06
[INFO 2017-06-27 20:24:56,716 main.py:51] epoch 164, training loss: 17352.22, average training loss: 18283.94, base loss: 20015.94
[INFO 2017-06-27 20:24:57,180 main.py:51] epoch 165, training loss: 15467.25, average training loss: 18266.97, base loss: 20007.04
[INFO 2017-06-27 20:24:57,633 main.py:51] epoch 166, training loss: 13999.16, average training loss: 18241.42, base loss: 19983.39
[INFO 2017-06-27 20:24:58,091 main.py:51] epoch 167, training loss: 17510.85, average training loss: 18237.07, base loss: 19992.36
[INFO 2017-06-27 20:24:58,528 main.py:51] epoch 168, training loss: 15952.37, average training loss: 18223.55, base loss: 19993.92
[INFO 2017-06-27 20:24:58,966 main.py:51] epoch 169, training loss: 17620.00, average training loss: 18220.00, base loss: 20004.84
[INFO 2017-06-27 20:24:59,359 main.py:51] epoch 170, training loss: 17028.03, average training loss: 18213.03, base loss: 20014.60
[INFO 2017-06-27 20:24:59,740 main.py:51] epoch 171, training loss: 16659.39, average training loss: 18203.99, base loss: 20014.41
[INFO 2017-06-27 20:25:00,125 main.py:51] epoch 172, training loss: 16322.28, average training loss: 18193.12, base loss: 20016.82
[INFO 2017-06-27 20:25:00,504 main.py:51] epoch 173, training loss: 17561.95, average training loss: 18189.49, base loss: 20026.81
[INFO 2017-06-27 20:25:00,881 main.py:51] epoch 174, training loss: 18379.40, average training loss: 18190.58, base loss: 20036.51
[INFO 2017-06-27 20:25:01,258 main.py:51] epoch 175, training loss: 19969.05, average training loss: 18200.68, base loss: 20059.86
[INFO 2017-06-27 20:25:01,639 main.py:51] epoch 176, training loss: 16756.05, average training loss: 18192.52, base loss: 20064.52
[INFO 2017-06-27 20:25:02,025 main.py:51] epoch 177, training loss: 18269.65, average training loss: 18192.95, base loss: 20073.11
[INFO 2017-06-27 20:25:02,404 main.py:51] epoch 178, training loss: 15181.29, average training loss: 18176.13, base loss: 20063.37
[INFO 2017-06-27 20:25:02,783 main.py:51] epoch 179, training loss: 14578.54, average training loss: 18156.14, base loss: 20048.97
[INFO 2017-06-27 20:25:03,156 main.py:51] epoch 180, training loss: 14466.45, average training loss: 18135.76, base loss: 20032.95
[INFO 2017-06-27 20:25:03,534 main.py:51] epoch 181, training loss: 18741.56, average training loss: 18139.08, base loss: 20049.84
[INFO 2017-06-27 20:25:03,905 main.py:51] epoch 182, training loss: 16464.16, average training loss: 18129.93, base loss: 20049.94
[INFO 2017-06-27 20:25:04,289 main.py:51] epoch 183, training loss: 15060.53, average training loss: 18113.25, base loss: 20037.35
[INFO 2017-06-27 20:25:04,660 main.py:51] epoch 184, training loss: 15739.66, average training loss: 18100.42, base loss: 20029.95
[INFO 2017-06-27 20:25:05,033 main.py:51] epoch 185, training loss: 17122.03, average training loss: 18095.16, base loss: 20038.72
[INFO 2017-06-27 20:25:05,406 main.py:51] epoch 186, training loss: 14350.83, average training loss: 18075.14, base loss: 20023.05
[INFO 2017-06-27 20:25:05,781 main.py:51] epoch 187, training loss: 16490.27, average training loss: 18066.71, base loss: 20023.71
[INFO 2017-06-27 20:25:06,156 main.py:51] epoch 188, training loss: 18079.79, average training loss: 18066.78, base loss: 20033.28
[INFO 2017-06-27 20:25:06,532 main.py:51] epoch 189, training loss: 16282.01, average training loss: 18057.38, base loss: 20039.12
[INFO 2017-06-27 20:25:06,909 main.py:51] epoch 190, training loss: 16915.38, average training loss: 18051.40, base loss: 20044.29
[INFO 2017-06-27 20:25:07,284 main.py:51] epoch 191, training loss: 16526.56, average training loss: 18043.46, base loss: 20044.07
[INFO 2017-06-27 20:25:07,661 main.py:51] epoch 192, training loss: 16651.55, average training loss: 18036.25, base loss: 20041.49
[INFO 2017-06-27 20:25:08,037 main.py:51] epoch 193, training loss: 15469.92, average training loss: 18023.02, base loss: 20030.86
[INFO 2017-06-27 20:25:08,415 main.py:51] epoch 194, training loss: 16625.02, average training loss: 18015.85, base loss: 20034.05
[INFO 2017-06-27 20:25:08,791 main.py:51] epoch 195, training loss: 16348.49, average training loss: 18007.34, base loss: 20032.34
[INFO 2017-06-27 20:25:09,166 main.py:51] epoch 196, training loss: 16461.38, average training loss: 17999.50, base loss: 20033.42
[INFO 2017-06-27 20:25:09,541 main.py:51] epoch 197, training loss: 17170.66, average training loss: 17995.31, base loss: 20043.38
[INFO 2017-06-27 20:25:09,920 main.py:51] epoch 198, training loss: 16819.40, average training loss: 17989.40, base loss: 20045.24
[INFO 2017-06-27 20:25:10,297 main.py:51] epoch 199, training loss: 14800.14, average training loss: 17973.46, base loss: 20032.46
[INFO 2017-06-27 20:25:10,297 main.py:53] epoch 199, testing
[INFO 2017-06-27 20:25:11,915 main.py:105] average testing loss: 16141.02, base loss: 19676.22
[INFO 2017-06-27 20:25:11,915 main.py:106] improve_loss: 3535.20, improve_percent: 0.18
[INFO 2017-06-27 20:25:11,916 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:25:11,928 main.py:76] current best improved percent: 0.18
[INFO 2017-06-27 20:25:12,301 main.py:51] epoch 200, training loss: 16397.89, average training loss: 17965.62, base loss: 20034.07
[INFO 2017-06-27 20:25:12,686 main.py:51] epoch 201, training loss: 17317.88, average training loss: 17962.41, base loss: 20040.71
[INFO 2017-06-27 20:25:13,069 main.py:51] epoch 202, training loss: 14784.30, average training loss: 17946.75, base loss: 20030.77
[INFO 2017-06-27 20:25:13,452 main.py:51] epoch 203, training loss: 15476.14, average training loss: 17934.64, base loss: 20018.87
[INFO 2017-06-27 20:25:13,858 main.py:51] epoch 204, training loss: 16450.72, average training loss: 17927.40, base loss: 20017.19
[INFO 2017-06-27 20:25:14,244 main.py:51] epoch 205, training loss: 14548.31, average training loss: 17911.00, base loss: 20010.01
[INFO 2017-06-27 20:25:14,634 main.py:51] epoch 206, training loss: 15725.13, average training loss: 17900.44, base loss: 20002.64
[INFO 2017-06-27 20:25:15,027 main.py:51] epoch 207, training loss: 14363.81, average training loss: 17883.44, base loss: 19989.08
[INFO 2017-06-27 20:25:15,426 main.py:51] epoch 208, training loss: 17077.98, average training loss: 17879.58, base loss: 19997.36
[INFO 2017-06-27 20:25:15,872 main.py:51] epoch 209, training loss: 14825.02, average training loss: 17865.04, base loss: 19991.01
[INFO 2017-06-27 20:25:16,319 main.py:51] epoch 210, training loss: 14600.77, average training loss: 17849.57, base loss: 19985.34
[INFO 2017-06-27 20:25:16,766 main.py:51] epoch 211, training loss: 15146.18, average training loss: 17836.82, base loss: 19981.01
[INFO 2017-06-27 20:25:17,209 main.py:51] epoch 212, training loss: 16741.85, average training loss: 17831.68, base loss: 19986.34
[INFO 2017-06-27 20:25:17,642 main.py:51] epoch 213, training loss: 14709.30, average training loss: 17817.09, base loss: 19978.56
[INFO 2017-06-27 20:25:18,040 main.py:51] epoch 214, training loss: 14539.25, average training loss: 17801.84, base loss: 19969.60
[INFO 2017-06-27 20:25:18,513 main.py:51] epoch 215, training loss: 17759.10, average training loss: 17801.64, base loss: 19984.86
[INFO 2017-06-27 20:25:18,987 main.py:51] epoch 216, training loss: 14064.71, average training loss: 17784.42, base loss: 19968.04
[INFO 2017-06-27 20:25:19,394 main.py:51] epoch 217, training loss: 16868.32, average training loss: 17780.22, base loss: 19974.56
[INFO 2017-06-27 20:25:19,771 main.py:51] epoch 218, training loss: 15993.24, average training loss: 17772.06, base loss: 19968.26
[INFO 2017-06-27 20:25:20,149 main.py:51] epoch 219, training loss: 17578.39, average training loss: 17771.18, base loss: 19977.61
[INFO 2017-06-27 20:25:20,533 main.py:51] epoch 220, training loss: 14278.28, average training loss: 17755.37, base loss: 19963.56
[INFO 2017-06-27 20:25:20,914 main.py:51] epoch 221, training loss: 15608.17, average training loss: 17745.70, base loss: 19962.11
[INFO 2017-06-27 20:25:21,294 main.py:51] epoch 222, training loss: 15008.60, average training loss: 17733.43, base loss: 19961.18
[INFO 2017-06-27 20:25:21,672 main.py:51] epoch 223, training loss: 17066.27, average training loss: 17730.45, base loss: 19963.84
[INFO 2017-06-27 20:25:22,049 main.py:51] epoch 224, training loss: 13441.96, average training loss: 17711.39, base loss: 19945.91
[INFO 2017-06-27 20:25:22,447 main.py:51] epoch 225, training loss: 18327.79, average training loss: 17714.12, base loss: 19956.34
[INFO 2017-06-27 20:25:22,845 main.py:51] epoch 226, training loss: 14635.48, average training loss: 17700.55, base loss: 19942.58
[INFO 2017-06-27 20:25:23,229 main.py:51] epoch 227, training loss: 14023.76, average training loss: 17684.43, base loss: 19928.14
[INFO 2017-06-27 20:25:23,611 main.py:51] epoch 228, training loss: 17207.52, average training loss: 17682.35, base loss: 19933.59
[INFO 2017-06-27 20:25:23,991 main.py:51] epoch 229, training loss: 14757.55, average training loss: 17669.63, base loss: 19928.59
[INFO 2017-06-27 20:25:24,368 main.py:51] epoch 230, training loss: 14347.01, average training loss: 17655.25, base loss: 19920.04
[INFO 2017-06-27 20:25:24,750 main.py:51] epoch 231, training loss: 14392.68, average training loss: 17641.18, base loss: 19910.89
[INFO 2017-06-27 20:25:25,156 main.py:51] epoch 232, training loss: 15972.92, average training loss: 17634.02, base loss: 19913.44
[INFO 2017-06-27 20:25:25,571 main.py:51] epoch 233, training loss: 17855.46, average training loss: 17634.97, base loss: 19923.80
[INFO 2017-06-27 20:25:25,964 main.py:51] epoch 234, training loss: 17394.21, average training loss: 17633.94, base loss: 19935.03
[INFO 2017-06-27 20:25:26,340 main.py:51] epoch 235, training loss: 14272.76, average training loss: 17619.70, base loss: 19926.60
[INFO 2017-06-27 20:25:26,758 main.py:51] epoch 236, training loss: 14694.31, average training loss: 17607.36, base loss: 19917.06
[INFO 2017-06-27 20:25:27,175 main.py:51] epoch 237, training loss: 17542.29, average training loss: 17607.09, base loss: 19921.69
[INFO 2017-06-27 20:25:27,558 main.py:51] epoch 238, training loss: 18757.06, average training loss: 17611.90, base loss: 19937.91
[INFO 2017-06-27 20:25:27,941 main.py:51] epoch 239, training loss: 16145.35, average training loss: 17605.79, base loss: 19941.11
[INFO 2017-06-27 20:25:28,324 main.py:51] epoch 240, training loss: 15948.91, average training loss: 17598.91, base loss: 19939.11
[INFO 2017-06-27 20:25:28,708 main.py:51] epoch 241, training loss: 13566.85, average training loss: 17582.25, base loss: 19929.81
[INFO 2017-06-27 20:25:29,092 main.py:51] epoch 242, training loss: 15436.71, average training loss: 17573.42, base loss: 19928.52
[INFO 2017-06-27 20:25:29,470 main.py:51] epoch 243, training loss: 13901.97, average training loss: 17558.37, base loss: 19912.59
[INFO 2017-06-27 20:25:29,855 main.py:51] epoch 244, training loss: 15758.64, average training loss: 17551.03, base loss: 19913.45
[INFO 2017-06-27 20:25:30,233 main.py:51] epoch 245, training loss: 17174.70, average training loss: 17549.50, base loss: 19917.76
[INFO 2017-06-27 20:25:30,663 main.py:51] epoch 246, training loss: 15114.88, average training loss: 17539.64, base loss: 19915.50
[INFO 2017-06-27 20:25:31,069 main.py:51] epoch 247, training loss: 15249.56, average training loss: 17530.41, base loss: 19912.21
[INFO 2017-06-27 20:25:31,446 main.py:51] epoch 248, training loss: 15067.01, average training loss: 17520.51, base loss: 19909.89
[INFO 2017-06-27 20:25:31,832 main.py:51] epoch 249, training loss: 18952.94, average training loss: 17526.24, base loss: 19924.88
[INFO 2017-06-27 20:25:32,216 main.py:51] epoch 250, training loss: 17748.38, average training loss: 17527.13, base loss: 19935.43
[INFO 2017-06-27 20:25:32,592 main.py:51] epoch 251, training loss: 16518.30, average training loss: 17523.13, base loss: 19937.85
[INFO 2017-06-27 20:25:32,969 main.py:51] epoch 252, training loss: 17726.96, average training loss: 17523.93, base loss: 19947.21
[INFO 2017-06-27 20:25:33,349 main.py:51] epoch 253, training loss: 15903.61, average training loss: 17517.55, base loss: 19945.56
[INFO 2017-06-27 20:25:33,827 main.py:51] epoch 254, training loss: 15002.26, average training loss: 17507.69, base loss: 19938.85
[INFO 2017-06-27 20:25:34,209 main.py:51] epoch 255, training loss: 16470.40, average training loss: 17503.64, base loss: 19945.85
[INFO 2017-06-27 20:25:34,586 main.py:51] epoch 256, training loss: 16276.45, average training loss: 17498.86, base loss: 19947.93
[INFO 2017-06-27 20:25:35,038 main.py:51] epoch 257, training loss: 14663.17, average training loss: 17487.87, base loss: 19940.47
[INFO 2017-06-27 20:25:35,450 main.py:51] epoch 258, training loss: 18010.18, average training loss: 17489.89, base loss: 19948.65
[INFO 2017-06-27 20:25:35,828 main.py:51] epoch 259, training loss: 17365.04, average training loss: 17489.41, base loss: 19955.59
[INFO 2017-06-27 20:25:36,206 main.py:51] epoch 260, training loss: 18060.87, average training loss: 17491.60, base loss: 19963.25
[INFO 2017-06-27 20:25:36,585 main.py:51] epoch 261, training loss: 16805.82, average training loss: 17488.98, base loss: 19972.86
[INFO 2017-06-27 20:25:36,961 main.py:51] epoch 262, training loss: 14413.48, average training loss: 17477.28, base loss: 19965.82
[INFO 2017-06-27 20:25:37,340 main.py:51] epoch 263, training loss: 16721.24, average training loss: 17474.42, base loss: 19965.03
[INFO 2017-06-27 20:25:37,717 main.py:51] epoch 264, training loss: 13992.70, average training loss: 17461.28, base loss: 19955.78
[INFO 2017-06-27 20:25:38,089 main.py:51] epoch 265, training loss: 14460.72, average training loss: 17450.00, base loss: 19947.64
[INFO 2017-06-27 20:25:38,464 main.py:51] epoch 266, training loss: 15407.00, average training loss: 17442.35, base loss: 19945.09
[INFO 2017-06-27 20:25:38,836 main.py:51] epoch 267, training loss: 18980.11, average training loss: 17448.09, base loss: 19962.96
[INFO 2017-06-27 20:25:39,212 main.py:51] epoch 268, training loss: 14765.24, average training loss: 17438.11, base loss: 19960.30
[INFO 2017-06-27 20:25:39,588 main.py:51] epoch 269, training loss: 15193.35, average training loss: 17429.80, base loss: 19952.48
[INFO 2017-06-27 20:25:39,979 main.py:51] epoch 270, training loss: 14202.54, average training loss: 17417.89, base loss: 19942.59
[INFO 2017-06-27 20:25:40,355 main.py:51] epoch 271, training loss: 15336.29, average training loss: 17410.24, base loss: 19942.74
[INFO 2017-06-27 20:25:40,726 main.py:51] epoch 272, training loss: 16304.36, average training loss: 17406.19, base loss: 19946.49
[INFO 2017-06-27 20:25:41,099 main.py:51] epoch 273, training loss: 14961.71, average training loss: 17397.27, base loss: 19938.80
[INFO 2017-06-27 20:25:41,475 main.py:51] epoch 274, training loss: 14957.17, average training loss: 17388.39, base loss: 19929.13
[INFO 2017-06-27 20:25:41,893 main.py:51] epoch 275, training loss: 14895.03, average training loss: 17379.36, base loss: 19926.59
[INFO 2017-06-27 20:25:42,310 main.py:51] epoch 276, training loss: 15348.76, average training loss: 17372.03, base loss: 19923.50
[INFO 2017-06-27 20:25:42,688 main.py:51] epoch 277, training loss: 14192.69, average training loss: 17360.59, base loss: 19913.12
[INFO 2017-06-27 20:25:43,153 main.py:51] epoch 278, training loss: 16725.68, average training loss: 17358.32, base loss: 19917.33
[INFO 2017-06-27 20:25:43,561 main.py:51] epoch 279, training loss: 16442.57, average training loss: 17355.05, base loss: 19921.29
[INFO 2017-06-27 20:25:43,960 main.py:51] epoch 280, training loss: 16312.96, average training loss: 17351.34, base loss: 19921.02
[INFO 2017-06-27 20:25:44,365 main.py:51] epoch 281, training loss: 19190.32, average training loss: 17357.86, base loss: 19937.60
[INFO 2017-06-27 20:25:44,741 main.py:51] epoch 282, training loss: 15257.33, average training loss: 17350.44, base loss: 19934.99
[INFO 2017-06-27 20:25:45,118 main.py:51] epoch 283, training loss: 17126.70, average training loss: 17349.65, base loss: 19938.67
[INFO 2017-06-27 20:25:45,509 main.py:51] epoch 284, training loss: 14044.60, average training loss: 17338.05, base loss: 19928.86
[INFO 2017-06-27 20:25:45,894 main.py:51] epoch 285, training loss: 15489.98, average training loss: 17331.59, base loss: 19923.08
[INFO 2017-06-27 20:25:46,270 main.py:51] epoch 286, training loss: 15549.96, average training loss: 17325.38, base loss: 19923.23
[INFO 2017-06-27 20:25:46,747 main.py:51] epoch 287, training loss: 14804.14, average training loss: 17316.63, base loss: 19919.60
[INFO 2017-06-27 20:25:47,136 main.py:51] epoch 288, training loss: 16453.12, average training loss: 17313.64, base loss: 19923.23
[INFO 2017-06-27 20:25:47,512 main.py:51] epoch 289, training loss: 16761.71, average training loss: 17311.74, base loss: 19924.82
[INFO 2017-06-27 20:25:47,896 main.py:51] epoch 290, training loss: 15053.58, average training loss: 17303.98, base loss: 19922.20
[INFO 2017-06-27 20:25:48,272 main.py:51] epoch 291, training loss: 14553.78, average training loss: 17294.56, base loss: 19917.53
[INFO 2017-06-27 20:25:48,647 main.py:51] epoch 292, training loss: 15965.82, average training loss: 17290.02, base loss: 19915.01
[INFO 2017-06-27 20:25:49,015 main.py:51] epoch 293, training loss: 17068.88, average training loss: 17289.27, base loss: 19921.37
[INFO 2017-06-27 20:25:49,388 main.py:51] epoch 294, training loss: 14127.21, average training loss: 17278.55, base loss: 19916.42
[INFO 2017-06-27 20:25:49,766 main.py:51] epoch 295, training loss: 14120.04, average training loss: 17267.88, base loss: 19908.85
[INFO 2017-06-27 20:25:50,139 main.py:51] epoch 296, training loss: 15876.70, average training loss: 17263.20, base loss: 19910.50
[INFO 2017-06-27 20:25:50,513 main.py:51] epoch 297, training loss: 16171.94, average training loss: 17259.54, base loss: 19912.48
[INFO 2017-06-27 20:25:50,978 main.py:51] epoch 298, training loss: 15694.71, average training loss: 17254.30, base loss: 19911.96
[INFO 2017-06-27 20:25:51,374 main.py:51] epoch 299, training loss: 15399.45, average training loss: 17248.12, base loss: 19908.49
[INFO 2017-06-27 20:25:51,374 main.py:53] epoch 299, testing
[INFO 2017-06-27 20:25:53,000 main.py:105] average testing loss: 15904.36, base loss: 19894.28
[INFO 2017-06-27 20:25:53,000 main.py:106] improve_loss: 3989.92, improve_percent: 0.20
[INFO 2017-06-27 20:25:53,001 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:25:53,013 main.py:76] current best improved percent: 0.20
[INFO 2017-06-27 20:25:53,444 main.py:51] epoch 300, training loss: 15084.72, average training loss: 17240.93, base loss: 19908.79
[INFO 2017-06-27 20:25:53,847 main.py:51] epoch 301, training loss: 15341.56, average training loss: 17234.64, base loss: 19908.04
[INFO 2017-06-27 20:25:54,225 main.py:51] epoch 302, training loss: 16432.10, average training loss: 17231.99, base loss: 19909.60
[INFO 2017-06-27 20:25:54,603 main.py:51] epoch 303, training loss: 15879.64, average training loss: 17227.55, base loss: 19907.28
[INFO 2017-06-27 20:25:54,981 main.py:51] epoch 304, training loss: 16146.21, average training loss: 17224.00, base loss: 19908.57
[INFO 2017-06-27 20:25:55,357 main.py:51] epoch 305, training loss: 16170.13, average training loss: 17220.56, base loss: 19908.87
[INFO 2017-06-27 20:25:55,734 main.py:51] epoch 306, training loss: 15739.15, average training loss: 17215.73, base loss: 19909.90
[INFO 2017-06-27 20:25:56,127 main.py:51] epoch 307, training loss: 15840.72, average training loss: 17211.27, base loss: 19910.81
[INFO 2017-06-27 20:25:56,576 main.py:51] epoch 308, training loss: 16109.99, average training loss: 17207.70, base loss: 19911.51
[INFO 2017-06-27 20:25:56,954 main.py:51] epoch 309, training loss: 14820.44, average training loss: 17200.00, base loss: 19908.04
[INFO 2017-06-27 20:25:57,344 main.py:51] epoch 310, training loss: 17577.06, average training loss: 17201.21, base loss: 19919.37
[INFO 2017-06-27 20:25:57,736 main.py:51] epoch 311, training loss: 15647.56, average training loss: 17196.23, base loss: 19920.04
[INFO 2017-06-27 20:25:58,123 main.py:51] epoch 312, training loss: 14531.90, average training loss: 17187.72, base loss: 19911.96
[INFO 2017-06-27 20:25:58,499 main.py:51] epoch 313, training loss: 14417.14, average training loss: 17178.90, base loss: 19904.99
[INFO 2017-06-27 20:25:58,883 main.py:51] epoch 314, training loss: 13295.47, average training loss: 17166.57, base loss: 19897.89
[INFO 2017-06-27 20:25:59,295 main.py:51] epoch 315, training loss: 15078.23, average training loss: 17159.96, base loss: 19894.59
[INFO 2017-06-27 20:25:59,672 main.py:51] epoch 316, training loss: 14967.82, average training loss: 17153.05, base loss: 19892.46
[INFO 2017-06-27 20:26:00,055 main.py:51] epoch 317, training loss: 15309.01, average training loss: 17147.25, base loss: 19889.23
[INFO 2017-06-27 20:26:00,441 main.py:51] epoch 318, training loss: 16318.96, average training loss: 17144.65, base loss: 19891.18
[INFO 2017-06-27 20:26:00,819 main.py:51] epoch 319, training loss: 13490.69, average training loss: 17133.23, base loss: 19882.35
[INFO 2017-06-27 20:26:01,198 main.py:51] epoch 320, training loss: 14035.44, average training loss: 17123.58, base loss: 19877.96
[INFO 2017-06-27 20:26:01,575 main.py:51] epoch 321, training loss: 13727.79, average training loss: 17113.04, base loss: 19872.16
[INFO 2017-06-27 20:26:01,987 main.py:51] epoch 322, training loss: 17479.94, average training loss: 17114.17, base loss: 19881.28
[INFO 2017-06-27 20:26:02,377 main.py:51] epoch 323, training loss: 14733.80, average training loss: 17106.83, base loss: 19879.44
[INFO 2017-06-27 20:26:02,764 main.py:51] epoch 324, training loss: 15405.51, average training loss: 17101.59, base loss: 19877.10
[INFO 2017-06-27 20:26:03,149 main.py:51] epoch 325, training loss: 15721.40, average training loss: 17097.36, base loss: 19881.33
[INFO 2017-06-27 20:26:03,553 main.py:51] epoch 326, training loss: 15530.58, average training loss: 17092.57, base loss: 19882.32
[INFO 2017-06-27 20:26:04,031 main.py:51] epoch 327, training loss: 14360.05, average training loss: 17084.23, base loss: 19878.48
[INFO 2017-06-27 20:26:04,489 main.py:51] epoch 328, training loss: 12298.38, average training loss: 17069.69, base loss: 19861.90
[INFO 2017-06-27 20:26:04,933 main.py:51] epoch 329, training loss: 15653.84, average training loss: 17065.40, base loss: 19863.05
[INFO 2017-06-27 20:26:05,375 main.py:51] epoch 330, training loss: 14768.89, average training loss: 17058.46, base loss: 19858.88
[INFO 2017-06-27 20:26:05,810 main.py:51] epoch 331, training loss: 15371.94, average training loss: 17053.38, base loss: 19856.12
[INFO 2017-06-27 20:26:06,221 main.py:51] epoch 332, training loss: 14836.24, average training loss: 17046.72, base loss: 19854.52
[INFO 2017-06-27 20:26:06,687 main.py:51] epoch 333, training loss: 18148.81, average training loss: 17050.02, base loss: 19863.02
[INFO 2017-06-27 20:26:07,151 main.py:51] epoch 334, training loss: 16321.41, average training loss: 17047.85, base loss: 19861.63
[INFO 2017-06-27 20:26:07,552 main.py:51] epoch 335, training loss: 15041.04, average training loss: 17041.87, base loss: 19858.98
[INFO 2017-06-27 20:26:07,934 main.py:51] epoch 336, training loss: 15727.47, average training loss: 17037.97, base loss: 19859.06
[INFO 2017-06-27 20:26:08,306 main.py:51] epoch 337, training loss: 16194.86, average training loss: 17035.48, base loss: 19861.92
[INFO 2017-06-27 20:26:08,679 main.py:51] epoch 338, training loss: 16318.44, average training loss: 17033.36, base loss: 19865.01
[INFO 2017-06-27 20:26:09,055 main.py:51] epoch 339, training loss: 15188.90, average training loss: 17027.94, base loss: 19860.54
[INFO 2017-06-27 20:26:09,529 main.py:51] epoch 340, training loss: 14698.92, average training loss: 17021.11, base loss: 19856.97
[INFO 2017-06-27 20:26:09,931 main.py:51] epoch 341, training loss: 16835.98, average training loss: 17020.57, base loss: 19860.00
[INFO 2017-06-27 20:26:10,313 main.py:51] epoch 342, training loss: 14034.67, average training loss: 17011.86, base loss: 19851.54
[INFO 2017-06-27 20:26:10,698 main.py:51] epoch 343, training loss: 15814.68, average training loss: 17008.38, base loss: 19850.61
[INFO 2017-06-27 20:26:11,076 main.py:51] epoch 344, training loss: 17550.91, average training loss: 17009.95, base loss: 19858.37
[INFO 2017-06-27 20:26:11,452 main.py:51] epoch 345, training loss: 15626.17, average training loss: 17005.96, base loss: 19856.96
[INFO 2017-06-27 20:26:11,828 main.py:51] epoch 346, training loss: 16059.77, average training loss: 17003.23, base loss: 19858.78
[INFO 2017-06-27 20:26:12,206 main.py:51] epoch 347, training loss: 20747.04, average training loss: 17013.99, base loss: 19877.30
[INFO 2017-06-27 20:26:12,586 main.py:51] epoch 348, training loss: 14442.61, average training loss: 17006.62, base loss: 19875.14
[INFO 2017-06-27 20:26:12,960 main.py:51] epoch 349, training loss: 15313.03, average training loss: 17001.78, base loss: 19875.35
[INFO 2017-06-27 20:26:13,332 main.py:51] epoch 350, training loss: 14567.79, average training loss: 16994.85, base loss: 19870.00
[INFO 2017-06-27 20:26:13,703 main.py:51] epoch 351, training loss: 13610.78, average training loss: 16985.23, base loss: 19861.55
[INFO 2017-06-27 20:26:14,080 main.py:51] epoch 352, training loss: 17280.03, average training loss: 16986.07, base loss: 19867.24
[INFO 2017-06-27 20:26:14,454 main.py:51] epoch 353, training loss: 12753.91, average training loss: 16974.11, base loss: 19856.18
[INFO 2017-06-27 20:26:14,827 main.py:51] epoch 354, training loss: 15111.62, average training loss: 16968.87, base loss: 19857.93
[INFO 2017-06-27 20:26:15,198 main.py:51] epoch 355, training loss: 15562.52, average training loss: 16964.91, base loss: 19857.92
[INFO 2017-06-27 20:26:15,572 main.py:51] epoch 356, training loss: 13708.18, average training loss: 16955.79, base loss: 19850.64
[INFO 2017-06-27 20:26:15,945 main.py:51] epoch 357, training loss: 15116.50, average training loss: 16950.65, base loss: 19848.22
[INFO 2017-06-27 20:26:16,321 main.py:51] epoch 358, training loss: 16872.90, average training loss: 16950.44, base loss: 19853.83
[INFO 2017-06-27 20:26:16,696 main.py:51] epoch 359, training loss: 17277.93, average training loss: 16951.35, base loss: 19862.41
[INFO 2017-06-27 20:26:17,071 main.py:51] epoch 360, training loss: 14212.70, average training loss: 16943.76, base loss: 19857.41
[INFO 2017-06-27 20:26:17,448 main.py:51] epoch 361, training loss: 16464.18, average training loss: 16942.44, base loss: 19864.92
[INFO 2017-06-27 20:26:17,823 main.py:51] epoch 362, training loss: 14787.61, average training loss: 16936.50, base loss: 19863.69
[INFO 2017-06-27 20:26:18,237 main.py:51] epoch 363, training loss: 14483.26, average training loss: 16929.76, base loss: 19862.37
[INFO 2017-06-27 20:26:18,658 main.py:51] epoch 364, training loss: 15373.47, average training loss: 16925.50, base loss: 19861.89
[INFO 2017-06-27 20:26:19,040 main.py:51] epoch 365, training loss: 17308.05, average training loss: 16926.54, base loss: 19868.63
[INFO 2017-06-27 20:26:19,418 main.py:51] epoch 366, training loss: 15273.50, average training loss: 16922.04, base loss: 19869.17
[INFO 2017-06-27 20:26:19,804 main.py:51] epoch 367, training loss: 16494.58, average training loss: 16920.88, base loss: 19874.22
[INFO 2017-06-27 20:26:20,182 main.py:51] epoch 368, training loss: 16534.83, average training loss: 16919.83, base loss: 19878.76
[INFO 2017-06-27 20:26:20,565 main.py:51] epoch 369, training loss: 15340.11, average training loss: 16915.56, base loss: 19879.66
[INFO 2017-06-27 20:26:20,948 main.py:51] epoch 370, training loss: 16862.96, average training loss: 16915.42, base loss: 19886.89
[INFO 2017-06-27 20:26:21,324 main.py:51] epoch 371, training loss: 15598.89, average training loss: 16911.88, base loss: 19889.37
[INFO 2017-06-27 20:26:21,699 main.py:51] epoch 372, training loss: 14206.51, average training loss: 16904.63, base loss: 19886.43
[INFO 2017-06-27 20:26:22,184 main.py:51] epoch 373, training loss: 14557.87, average training loss: 16898.35, base loss: 19882.03
[INFO 2017-06-27 20:26:22,565 main.py:51] epoch 374, training loss: 15468.42, average training loss: 16894.54, base loss: 19883.21
[INFO 2017-06-27 20:26:22,941 main.py:51] epoch 375, training loss: 14660.34, average training loss: 16888.60, base loss: 19879.30
[INFO 2017-06-27 20:26:23,333 main.py:51] epoch 376, training loss: 15466.00, average training loss: 16884.82, base loss: 19880.49
[INFO 2017-06-27 20:26:23,715 main.py:51] epoch 377, training loss: 16515.79, average training loss: 16883.85, base loss: 19884.98
[INFO 2017-06-27 20:26:24,092 main.py:51] epoch 378, training loss: 16347.08, average training loss: 16882.43, base loss: 19888.73
[INFO 2017-06-27 20:26:24,466 main.py:51] epoch 379, training loss: 15817.23, average training loss: 16879.63, base loss: 19891.03
[INFO 2017-06-27 20:26:24,839 main.py:51] epoch 380, training loss: 15455.86, average training loss: 16875.89, base loss: 19891.06
[INFO 2017-06-27 20:26:25,215 main.py:51] epoch 381, training loss: 13625.13, average training loss: 16867.38, base loss: 19885.76
[INFO 2017-06-27 20:26:25,591 main.py:51] epoch 382, training loss: 16002.04, average training loss: 16865.12, base loss: 19884.85
[INFO 2017-06-27 20:26:25,968 main.py:51] epoch 383, training loss: 15015.35, average training loss: 16860.30, base loss: 19886.82
[INFO 2017-06-27 20:26:26,343 main.py:51] epoch 384, training loss: 13333.42, average training loss: 16851.14, base loss: 19881.08
[INFO 2017-06-27 20:26:26,719 main.py:51] epoch 385, training loss: 15722.33, average training loss: 16848.22, base loss: 19880.81
[INFO 2017-06-27 20:26:27,092 main.py:51] epoch 386, training loss: 15838.98, average training loss: 16845.61, base loss: 19882.57
[INFO 2017-06-27 20:26:27,465 main.py:51] epoch 387, training loss: 16449.72, average training loss: 16844.59, base loss: 19885.95
[INFO 2017-06-27 20:26:27,849 main.py:51] epoch 388, training loss: 13093.82, average training loss: 16834.95, base loss: 19879.39
[INFO 2017-06-27 20:26:28,262 main.py:51] epoch 389, training loss: 16189.77, average training loss: 16833.29, base loss: 19883.01
[INFO 2017-06-27 20:26:28,681 main.py:51] epoch 390, training loss: 14686.43, average training loss: 16827.80, base loss: 19881.03
[INFO 2017-06-27 20:26:29,058 main.py:51] epoch 391, training loss: 16614.27, average training loss: 16827.26, base loss: 19886.73
[INFO 2017-06-27 20:26:29,433 main.py:51] epoch 392, training loss: 13115.02, average training loss: 16817.81, base loss: 19879.15
[INFO 2017-06-27 20:26:29,811 main.py:51] epoch 393, training loss: 13951.59, average training loss: 16810.54, base loss: 19871.22
[INFO 2017-06-27 20:26:30,267 main.py:51] epoch 394, training loss: 14044.12, average training loss: 16803.54, base loss: 19868.30
[INFO 2017-06-27 20:26:30,677 main.py:51] epoch 395, training loss: 16471.86, average training loss: 16802.70, base loss: 19874.58
[INFO 2017-06-27 20:26:31,064 main.py:51] epoch 396, training loss: 13780.66, average training loss: 16795.09, base loss: 19870.89
[INFO 2017-06-27 20:26:31,519 main.py:51] epoch 397, training loss: 15510.17, average training loss: 16791.86, base loss: 19869.29
[INFO 2017-06-27 20:26:31,928 main.py:51] epoch 398, training loss: 17006.37, average training loss: 16792.39, base loss: 19875.97
[INFO 2017-06-27 20:26:32,313 main.py:51] epoch 399, training loss: 15766.01, average training loss: 16789.83, base loss: 19880.52
[INFO 2017-06-27 20:26:32,313 main.py:53] epoch 399, testing
[INFO 2017-06-27 20:26:34,047 main.py:105] average testing loss: 14820.49, base loss: 19370.01
[INFO 2017-06-27 20:26:34,047 main.py:106] improve_loss: 4549.52, improve_percent: 0.23
[INFO 2017-06-27 20:26:34,047 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:26:34,060 main.py:76] current best improved percent: 0.23
[INFO 2017-06-27 20:26:34,437 main.py:51] epoch 400, training loss: 16155.91, average training loss: 16788.25, base loss: 19887.28
[INFO 2017-06-27 20:26:34,814 main.py:51] epoch 401, training loss: 13496.87, average training loss: 16780.06, base loss: 19877.48
[INFO 2017-06-27 20:26:35,188 main.py:51] epoch 402, training loss: 16986.45, average training loss: 16780.57, base loss: 19882.32
[INFO 2017-06-27 20:26:35,591 main.py:51] epoch 403, training loss: 13430.53, average training loss: 16772.28, base loss: 19876.37
[INFO 2017-06-27 20:26:35,990 main.py:51] epoch 404, training loss: 14132.10, average training loss: 16765.76, base loss: 19872.55
[INFO 2017-06-27 20:26:36,386 main.py:51] epoch 405, training loss: 17109.56, average training loss: 16766.61, base loss: 19877.39
[INFO 2017-06-27 20:26:36,763 main.py:51] epoch 406, training loss: 16320.01, average training loss: 16765.51, base loss: 19879.96
[INFO 2017-06-27 20:26:37,147 main.py:51] epoch 407, training loss: 15329.31, average training loss: 16761.99, base loss: 19880.44
[INFO 2017-06-27 20:26:37,523 main.py:51] epoch 408, training loss: 15048.47, average training loss: 16757.80, base loss: 19880.47
[INFO 2017-06-27 20:26:37,903 main.py:51] epoch 409, training loss: 15084.83, average training loss: 16753.72, base loss: 19880.48
[INFO 2017-06-27 20:26:38,280 main.py:51] epoch 410, training loss: 15329.72, average training loss: 16750.26, base loss: 19882.55
[INFO 2017-06-27 20:26:38,656 main.py:51] epoch 411, training loss: 14168.75, average training loss: 16743.99, base loss: 19877.70
[INFO 2017-06-27 20:26:39,034 main.py:51] epoch 412, training loss: 13398.66, average training loss: 16735.89, base loss: 19872.23
[INFO 2017-06-27 20:26:39,416 main.py:51] epoch 413, training loss: 13923.21, average training loss: 16729.10, base loss: 19867.66
[INFO 2017-06-27 20:26:39,796 main.py:51] epoch 414, training loss: 14562.41, average training loss: 16723.88, base loss: 19867.60
[INFO 2017-06-27 20:26:40,183 main.py:51] epoch 415, training loss: 13573.12, average training loss: 16716.30, base loss: 19862.28
[INFO 2017-06-27 20:26:40,561 main.py:51] epoch 416, training loss: 15246.71, average training loss: 16712.78, base loss: 19862.62
[INFO 2017-06-27 20:26:40,959 main.py:51] epoch 417, training loss: 14289.41, average training loss: 16706.98, base loss: 19860.68
[INFO 2017-06-27 20:26:41,348 main.py:51] epoch 418, training loss: 15646.36, average training loss: 16704.45, base loss: 19864.58
[INFO 2017-06-27 20:26:41,730 main.py:51] epoch 419, training loss: 15113.07, average training loss: 16700.66, base loss: 19865.34
[INFO 2017-06-27 20:26:42,119 main.py:51] epoch 420, training loss: 15005.32, average training loss: 16696.63, base loss: 19862.61
[INFO 2017-06-27 20:26:42,505 main.py:51] epoch 421, training loss: 14752.57, average training loss: 16692.03, base loss: 19861.78
[INFO 2017-06-27 20:26:42,937 main.py:51] epoch 422, training loss: 17739.44, average training loss: 16694.50, base loss: 19872.65
[INFO 2017-06-27 20:26:43,369 main.py:51] epoch 423, training loss: 16326.72, average training loss: 16693.63, base loss: 19876.05
[INFO 2017-06-27 20:26:43,799 main.py:51] epoch 424, training loss: 15420.04, average training loss: 16690.64, base loss: 19877.98
[INFO 2017-06-27 20:26:44,261 main.py:51] epoch 425, training loss: 18991.74, average training loss: 16696.04, base loss: 19889.99
[INFO 2017-06-27 20:26:44,703 main.py:51] epoch 426, training loss: 15321.12, average training loss: 16692.82, base loss: 19888.85
[INFO 2017-06-27 20:26:45,094 main.py:51] epoch 427, training loss: 18016.08, average training loss: 16695.91, base loss: 19895.12
[INFO 2017-06-27 20:26:45,558 main.py:51] epoch 428, training loss: 15561.23, average training loss: 16693.27, base loss: 19897.67
[INFO 2017-06-27 20:26:46,021 main.py:51] epoch 429, training loss: 14984.12, average training loss: 16689.29, base loss: 19895.13
[INFO 2017-06-27 20:26:46,418 main.py:51] epoch 430, training loss: 13854.51, average training loss: 16682.71, base loss: 19894.08
[INFO 2017-06-27 20:26:46,804 main.py:51] epoch 431, training loss: 13784.83, average training loss: 16676.01, base loss: 19892.25
[INFO 2017-06-27 20:26:47,181 main.py:51] epoch 432, training loss: 14580.50, average training loss: 16671.17, base loss: 19893.02
[INFO 2017-06-27 20:26:47,558 main.py:51] epoch 433, training loss: 15027.15, average training loss: 16667.38, base loss: 19892.03
[INFO 2017-06-27 20:26:47,979 main.py:51] epoch 434, training loss: 17040.64, average training loss: 16668.24, base loss: 19896.62
[INFO 2017-06-27 20:26:48,395 main.py:51] epoch 435, training loss: 17682.41, average training loss: 16670.56, base loss: 19905.83
[INFO 2017-06-27 20:26:48,781 main.py:51] epoch 436, training loss: 14479.20, average training loss: 16665.55, base loss: 19903.32
[INFO 2017-06-27 20:26:49,183 main.py:51] epoch 437, training loss: 13566.82, average training loss: 16658.47, base loss: 19897.88
[INFO 2017-06-27 20:26:49,561 main.py:51] epoch 438, training loss: 15677.85, average training loss: 16656.24, base loss: 19897.04
[INFO 2017-06-27 20:26:49,935 main.py:51] epoch 439, training loss: 16240.51, average training loss: 16655.29, base loss: 19900.19
[INFO 2017-06-27 20:26:50,395 main.py:51] epoch 440, training loss: 14444.49, average training loss: 16650.28, base loss: 19897.55
[INFO 2017-06-27 20:26:50,806 main.py:51] epoch 441, training loss: 14781.38, average training loss: 16646.05, base loss: 19894.33
[INFO 2017-06-27 20:26:51,196 main.py:51] epoch 442, training loss: 13787.03, average training loss: 16639.60, base loss: 19891.23
[INFO 2017-06-27 20:26:51,574 main.py:51] epoch 443, training loss: 14998.09, average training loss: 16635.90, base loss: 19890.53
[INFO 2017-06-27 20:26:52,071 main.py:51] epoch 444, training loss: 14235.81, average training loss: 16630.51, base loss: 19889.89
[INFO 2017-06-27 20:26:52,460 main.py:51] epoch 445, training loss: 15403.88, average training loss: 16627.76, base loss: 19889.99
[INFO 2017-06-27 20:26:52,842 main.py:51] epoch 446, training loss: 13078.66, average training loss: 16619.82, base loss: 19884.90
[INFO 2017-06-27 20:26:53,307 main.py:51] epoch 447, training loss: 15740.75, average training loss: 16617.86, base loss: 19885.31
[INFO 2017-06-27 20:26:53,687 main.py:51] epoch 448, training loss: 15357.58, average training loss: 16615.05, base loss: 19886.62
[INFO 2017-06-27 20:26:54,116 main.py:51] epoch 449, training loss: 18065.52, average training loss: 16618.27, base loss: 19898.44
[INFO 2017-06-27 20:26:54,538 main.py:51] epoch 450, training loss: 16166.23, average training loss: 16617.27, base loss: 19901.06
[INFO 2017-06-27 20:26:54,922 main.py:51] epoch 451, training loss: 14001.36, average training loss: 16611.48, base loss: 19897.27
[INFO 2017-06-27 20:26:55,300 main.py:51] epoch 452, training loss: 16103.97, average training loss: 16610.36, base loss: 19902.01
[INFO 2017-06-27 20:26:55,685 main.py:51] epoch 453, training loss: 14734.14, average training loss: 16606.23, base loss: 19898.81
[INFO 2017-06-27 20:26:56,063 main.py:51] epoch 454, training loss: 15920.77, average training loss: 16604.72, base loss: 19902.08
[INFO 2017-06-27 20:26:56,436 main.py:51] epoch 455, training loss: 14009.70, average training loss: 16599.03, base loss: 19898.90
[INFO 2017-06-27 20:26:56,809 main.py:51] epoch 456, training loss: 15138.77, average training loss: 16595.84, base loss: 19899.23
[INFO 2017-06-27 20:26:57,208 main.py:51] epoch 457, training loss: 14097.02, average training loss: 16590.38, base loss: 19898.50
[INFO 2017-06-27 20:26:57,587 main.py:51] epoch 458, training loss: 13566.66, average training loss: 16583.79, base loss: 19892.93
[INFO 2017-06-27 20:26:57,962 main.py:51] epoch 459, training loss: 13572.25, average training loss: 16577.25, base loss: 19886.96
[INFO 2017-06-27 20:26:58,339 main.py:51] epoch 460, training loss: 14385.13, average training loss: 16572.49, base loss: 19883.55
[INFO 2017-06-27 20:26:58,721 main.py:51] epoch 461, training loss: 16125.76, average training loss: 16571.53, base loss: 19888.49
[INFO 2017-06-27 20:26:59,099 main.py:51] epoch 462, training loss: 14294.58, average training loss: 16566.61, base loss: 19887.02
[INFO 2017-06-27 20:26:59,475 main.py:51] epoch 463, training loss: 14239.33, average training loss: 16561.59, base loss: 19885.77
[INFO 2017-06-27 20:26:59,853 main.py:51] epoch 464, training loss: 14309.03, average training loss: 16556.75, base loss: 19881.37
[INFO 2017-06-27 20:27:00,233 main.py:51] epoch 465, training loss: 15655.97, average training loss: 16554.81, base loss: 19884.55
[INFO 2017-06-27 20:27:00,608 main.py:51] epoch 466, training loss: 15310.26, average training loss: 16552.15, base loss: 19886.12
[INFO 2017-06-27 20:27:00,982 main.py:51] epoch 467, training loss: 13529.96, average training loss: 16545.69, base loss: 19880.83
[INFO 2017-06-27 20:27:01,354 main.py:51] epoch 468, training loss: 13487.53, average training loss: 16539.17, base loss: 19874.79
[INFO 2017-06-27 20:27:01,732 main.py:51] epoch 469, training loss: 16288.99, average training loss: 16538.64, base loss: 19879.29
[INFO 2017-06-27 20:27:02,109 main.py:51] epoch 470, training loss: 16824.42, average training loss: 16539.25, base loss: 19885.85
[INFO 2017-06-27 20:27:02,482 main.py:51] epoch 471, training loss: 16439.05, average training loss: 16539.03, base loss: 19890.59
[INFO 2017-06-27 20:27:02,857 main.py:51] epoch 472, training loss: 11937.88, average training loss: 16529.31, base loss: 19883.80
[INFO 2017-06-27 20:27:03,231 main.py:51] epoch 473, training loss: 15196.64, average training loss: 16526.49, base loss: 19883.90
[INFO 2017-06-27 20:27:03,607 main.py:51] epoch 474, training loss: 15306.26, average training loss: 16523.93, base loss: 19885.27
[INFO 2017-06-27 20:27:03,981 main.py:51] epoch 475, training loss: 13813.69, average training loss: 16518.23, base loss: 19881.60
[INFO 2017-06-27 20:27:04,359 main.py:51] epoch 476, training loss: 15203.62, average training loss: 16515.48, base loss: 19880.94
[INFO 2017-06-27 20:27:04,735 main.py:51] epoch 477, training loss: 15112.08, average training loss: 16512.54, base loss: 19879.98
[INFO 2017-06-27 20:27:05,110 main.py:51] epoch 478, training loss: 14958.82, average training loss: 16509.30, base loss: 19880.30
[INFO 2017-06-27 20:27:05,482 main.py:51] epoch 479, training loss: 17004.51, average training loss: 16510.33, base loss: 19887.65
[INFO 2017-06-27 20:27:05,858 main.py:51] epoch 480, training loss: 16035.63, average training loss: 16509.34, base loss: 19891.10
[INFO 2017-06-27 20:27:06,229 main.py:51] epoch 481, training loss: 14363.24, average training loss: 16504.89, base loss: 19890.58
[INFO 2017-06-27 20:27:06,600 main.py:51] epoch 482, training loss: 14387.59, average training loss: 16500.50, base loss: 19889.94
[INFO 2017-06-27 20:27:06,980 main.py:51] epoch 483, training loss: 13574.49, average training loss: 16494.46, base loss: 19885.97
[INFO 2017-06-27 20:27:07,359 main.py:51] epoch 484, training loss: 15305.79, average training loss: 16492.01, base loss: 19887.58
[INFO 2017-06-27 20:27:07,737 main.py:51] epoch 485, training loss: 14158.03, average training loss: 16487.21, base loss: 19887.12
[INFO 2017-06-27 20:27:08,110 main.py:51] epoch 486, training loss: 17433.08, average training loss: 16489.15, base loss: 19890.83
[INFO 2017-06-27 20:27:08,480 main.py:51] epoch 487, training loss: 14508.06, average training loss: 16485.09, base loss: 19890.42
[INFO 2017-06-27 20:27:08,851 main.py:51] epoch 488, training loss: 14111.76, average training loss: 16480.24, base loss: 19889.58
[INFO 2017-06-27 20:27:09,222 main.py:51] epoch 489, training loss: 13265.63, average training loss: 16473.67, base loss: 19885.41
[INFO 2017-06-27 20:27:09,594 main.py:51] epoch 490, training loss: 15834.19, average training loss: 16472.37, base loss: 19890.04
[INFO 2017-06-27 20:27:09,971 main.py:51] epoch 491, training loss: 12357.73, average training loss: 16464.01, base loss: 19881.44
[INFO 2017-06-27 20:27:10,346 main.py:51] epoch 492, training loss: 14148.64, average training loss: 16459.31, base loss: 19880.36
[INFO 2017-06-27 20:27:10,717 main.py:51] epoch 493, training loss: 15970.51, average training loss: 16458.32, base loss: 19885.69
[INFO 2017-06-27 20:27:11,094 main.py:51] epoch 494, training loss: 14104.76, average training loss: 16453.57, base loss: 19883.22
[INFO 2017-06-27 20:27:11,473 main.py:51] epoch 495, training loss: 15023.95, average training loss: 16450.69, base loss: 19884.09
[INFO 2017-06-27 20:27:11,853 main.py:51] epoch 496, training loss: 14056.75, average training loss: 16445.87, base loss: 19879.04
[INFO 2017-06-27 20:27:12,226 main.py:51] epoch 497, training loss: 14757.45, average training loss: 16442.48, base loss: 19880.29
[INFO 2017-06-27 20:27:12,601 main.py:51] epoch 498, training loss: 16854.01, average training loss: 16443.30, base loss: 19887.80
[INFO 2017-06-27 20:27:12,973 main.py:51] epoch 499, training loss: 15003.71, average training loss: 16440.42, base loss: 19888.99
[INFO 2017-06-27 20:27:12,973 main.py:53] epoch 499, testing
[INFO 2017-06-27 20:27:14,595 main.py:105] average testing loss: 14825.71, base loss: 19698.84
[INFO 2017-06-27 20:27:14,595 main.py:106] improve_loss: 4873.13, improve_percent: 0.25
[INFO 2017-06-27 20:27:14,595 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:27:14,608 main.py:76] current best improved percent: 0.25
[INFO 2017-06-27 20:27:14,980 main.py:51] epoch 500, training loss: 14904.80, average training loss: 16437.36, base loss: 19887.31
[INFO 2017-06-27 20:27:15,355 main.py:51] epoch 501, training loss: 15684.62, average training loss: 16435.86, base loss: 19890.23
[INFO 2017-06-27 20:27:15,729 main.py:51] epoch 502, training loss: 15157.06, average training loss: 16433.32, base loss: 19892.61
[INFO 2017-06-27 20:27:16,105 main.py:51] epoch 503, training loss: 13521.18, average training loss: 16427.54, base loss: 19886.94
[INFO 2017-06-27 20:27:16,476 main.py:51] epoch 504, training loss: 14749.14, average training loss: 16424.22, base loss: 19885.06
[INFO 2017-06-27 20:27:16,847 main.py:51] epoch 505, training loss: 14978.88, average training loss: 16421.36, base loss: 19882.94
[INFO 2017-06-27 20:27:17,226 main.py:51] epoch 506, training loss: 16110.81, average training loss: 16420.75, base loss: 19886.79
[INFO 2017-06-27 20:27:17,599 main.py:51] epoch 507, training loss: 15360.91, average training loss: 16418.66, base loss: 19889.13
[INFO 2017-06-27 20:27:17,976 main.py:51] epoch 508, training loss: 15930.45, average training loss: 16417.70, base loss: 19892.04
[INFO 2017-06-27 20:27:18,357 main.py:51] epoch 509, training loss: 14442.10, average training loss: 16413.83, base loss: 19891.09
[INFO 2017-06-27 20:27:18,746 main.py:51] epoch 510, training loss: 15205.36, average training loss: 16411.46, base loss: 19893.24
[INFO 2017-06-27 20:27:19,140 main.py:51] epoch 511, training loss: 14485.27, average training loss: 16407.70, base loss: 19892.61
[INFO 2017-06-27 20:27:19,518 main.py:51] epoch 512, training loss: 13955.20, average training loss: 16402.92, base loss: 19889.34
[INFO 2017-06-27 20:27:19,898 main.py:51] epoch 513, training loss: 14414.45, average training loss: 16399.05, base loss: 19887.32
[INFO 2017-06-27 20:27:20,274 main.py:51] epoch 514, training loss: 13245.64, average training loss: 16392.93, base loss: 19880.25
[INFO 2017-06-27 20:27:20,662 main.py:51] epoch 515, training loss: 13977.03, average training loss: 16388.25, base loss: 19876.15
[INFO 2017-06-27 20:27:21,044 main.py:51] epoch 516, training loss: 12745.83, average training loss: 16381.20, base loss: 19867.90
[INFO 2017-06-27 20:27:21,425 main.py:51] epoch 517, training loss: 13907.92, average training loss: 16376.43, base loss: 19866.46
[INFO 2017-06-27 20:27:21,810 main.py:51] epoch 518, training loss: 15757.73, average training loss: 16375.23, base loss: 19867.26
[INFO 2017-06-27 20:27:22,197 main.py:51] epoch 519, training loss: 15947.04, average training loss: 16374.41, base loss: 19869.34
[INFO 2017-06-27 20:27:22,579 main.py:51] epoch 520, training loss: 13741.64, average training loss: 16369.36, base loss: 19865.92
[INFO 2017-06-27 20:27:22,960 main.py:51] epoch 521, training loss: 14162.78, average training loss: 16365.13, base loss: 19862.99
[INFO 2017-06-27 20:27:23,340 main.py:51] epoch 522, training loss: 17611.79, average training loss: 16367.51, base loss: 19871.57
[INFO 2017-06-27 20:27:23,729 main.py:51] epoch 523, training loss: 15974.02, average training loss: 16366.76, base loss: 19873.33
[INFO 2017-06-27 20:27:24,121 main.py:51] epoch 524, training loss: 16328.57, average training loss: 16366.69, base loss: 19874.58
[INFO 2017-06-27 20:27:24,505 main.py:51] epoch 525, training loss: 14878.46, average training loss: 16363.86, base loss: 19875.14
[INFO 2017-06-27 20:27:24,892 main.py:51] epoch 526, training loss: 13531.63, average training loss: 16358.49, base loss: 19871.71
[INFO 2017-06-27 20:27:25,306 main.py:51] epoch 527, training loss: 14772.22, average training loss: 16355.48, base loss: 19872.48
[INFO 2017-06-27 20:27:25,767 main.py:51] epoch 528, training loss: 15620.18, average training loss: 16354.09, base loss: 19877.01
[INFO 2017-06-27 20:27:26,202 main.py:51] epoch 529, training loss: 14307.79, average training loss: 16350.23, base loss: 19876.34
[INFO 2017-06-27 20:27:26,625 main.py:51] epoch 530, training loss: 13927.60, average training loss: 16345.67, base loss: 19871.99
[INFO 2017-06-27 20:27:27,067 main.py:51] epoch 531, training loss: 14094.00, average training loss: 16341.44, base loss: 19868.08
[INFO 2017-06-27 20:27:27,477 main.py:51] epoch 532, training loss: 16631.83, average training loss: 16341.98, base loss: 19874.55
[INFO 2017-06-27 20:27:27,890 main.py:51] epoch 533, training loss: 15664.45, average training loss: 16340.71, base loss: 19876.03
[INFO 2017-06-27 20:27:28,344 main.py:51] epoch 534, training loss: 15348.32, average training loss: 16338.86, base loss: 19878.33
[INFO 2017-06-27 20:27:28,795 main.py:51] epoch 535, training loss: 15650.79, average training loss: 16337.57, base loss: 19879.75
[INFO 2017-06-27 20:27:29,178 main.py:51] epoch 536, training loss: 15271.34, average training loss: 16335.59, base loss: 19881.14
[INFO 2017-06-27 20:27:29,560 main.py:51] epoch 537, training loss: 13295.76, average training loss: 16329.94, base loss: 19876.04
[INFO 2017-06-27 20:27:29,939 main.py:51] epoch 538, training loss: 14132.87, average training loss: 16325.86, base loss: 19873.04
[INFO 2017-06-27 20:27:30,315 main.py:51] epoch 539, training loss: 15224.25, average training loss: 16323.82, base loss: 19873.43
[INFO 2017-06-27 20:27:30,692 main.py:51] epoch 540, training loss: 14440.48, average training loss: 16320.34, base loss: 19872.18
[INFO 2017-06-27 20:27:31,070 main.py:51] epoch 541, training loss: 13722.82, average training loss: 16315.55, base loss: 19869.61
[INFO 2017-06-27 20:27:31,446 main.py:51] epoch 542, training loss: 14663.46, average training loss: 16312.51, base loss: 19869.09
[INFO 2017-06-27 20:27:31,915 main.py:51] epoch 543, training loss: 18347.40, average training loss: 16316.25, base loss: 19878.65
[INFO 2017-06-27 20:27:32,308 main.py:51] epoch 544, training loss: 14307.88, average training loss: 16312.56, base loss: 19878.56
[INFO 2017-06-27 20:27:32,687 main.py:51] epoch 545, training loss: 13588.05, average training loss: 16307.57, base loss: 19877.03
[INFO 2017-06-27 20:27:33,064 main.py:51] epoch 546, training loss: 13689.05, average training loss: 16302.78, base loss: 19874.38
[INFO 2017-06-27 20:27:33,463 main.py:51] epoch 547, training loss: 14580.78, average training loss: 16299.64, base loss: 19873.36
[INFO 2017-06-27 20:27:33,905 main.py:51] epoch 548, training loss: 12807.89, average training loss: 16293.28, base loss: 19869.04
[INFO 2017-06-27 20:27:34,305 main.py:51] epoch 549, training loss: 13835.61, average training loss: 16288.81, base loss: 19864.86
[INFO 2017-06-27 20:27:34,691 main.py:51] epoch 550, training loss: 14266.88, average training loss: 16285.14, base loss: 19864.17
[INFO 2017-06-27 20:27:35,193 main.py:51] epoch 551, training loss: 13968.41, average training loss: 16280.95, base loss: 19863.28
[INFO 2017-06-27 20:27:35,637 main.py:51] epoch 552, training loss: 16295.27, average training loss: 16280.97, base loss: 19867.45
[INFO 2017-06-27 20:27:36,013 main.py:51] epoch 553, training loss: 14678.29, average training loss: 16278.08, base loss: 19866.79
[INFO 2017-06-27 20:27:36,453 main.py:51] epoch 554, training loss: 14321.41, average training loss: 16274.55, base loss: 19863.93
[INFO 2017-06-27 20:27:36,853 main.py:51] epoch 555, training loss: 14445.04, average training loss: 16271.26, base loss: 19862.78
[INFO 2017-06-27 20:27:37,231 main.py:51] epoch 556, training loss: 14620.59, average training loss: 16268.30, base loss: 19862.33
[INFO 2017-06-27 20:27:37,607 main.py:51] epoch 557, training loss: 13904.32, average training loss: 16264.06, base loss: 19858.67
[INFO 2017-06-27 20:27:37,984 main.py:51] epoch 558, training loss: 15407.36, average training loss: 16262.53, base loss: 19859.48
[INFO 2017-06-27 20:27:38,359 main.py:51] epoch 559, training loss: 17212.61, average training loss: 16264.23, base loss: 19866.39
[INFO 2017-06-27 20:27:38,740 main.py:51] epoch 560, training loss: 12848.19, average training loss: 16258.14, base loss: 19861.39
[INFO 2017-06-27 20:27:39,131 main.py:51] epoch 561, training loss: 15344.23, average training loss: 16256.51, base loss: 19863.37
[INFO 2017-06-27 20:27:39,507 main.py:51] epoch 562, training loss: 12997.51, average training loss: 16250.72, base loss: 19859.11
[INFO 2017-06-27 20:27:39,883 main.py:51] epoch 563, training loss: 14979.93, average training loss: 16248.47, base loss: 19860.83
[INFO 2017-06-27 20:27:40,262 main.py:51] epoch 564, training loss: 14013.38, average training loss: 16244.51, base loss: 19859.32
[INFO 2017-06-27 20:27:40,638 main.py:51] epoch 565, training loss: 14075.46, average training loss: 16240.68, base loss: 19858.79
[INFO 2017-06-27 20:27:41,013 main.py:51] epoch 566, training loss: 15095.01, average training loss: 16238.66, base loss: 19861.18
[INFO 2017-06-27 20:27:41,385 main.py:51] epoch 567, training loss: 13407.65, average training loss: 16233.68, base loss: 19857.54
[INFO 2017-06-27 20:27:41,757 main.py:51] epoch 568, training loss: 15085.32, average training loss: 16231.66, base loss: 19858.40
[INFO 2017-06-27 20:27:42,134 main.py:51] epoch 569, training loss: 12951.21, average training loss: 16225.90, base loss: 19855.04
[INFO 2017-06-27 20:27:42,514 main.py:51] epoch 570, training loss: 15751.97, average training loss: 16225.07, base loss: 19860.23
[INFO 2017-06-27 20:27:42,887 main.py:51] epoch 571, training loss: 12280.14, average training loss: 16218.18, base loss: 19855.23
[INFO 2017-06-27 20:27:43,264 main.py:51] epoch 572, training loss: 15815.55, average training loss: 16217.47, base loss: 19856.71
[INFO 2017-06-27 20:27:43,637 main.py:51] epoch 573, training loss: 15841.92, average training loss: 16216.82, base loss: 19858.58
[INFO 2017-06-27 20:27:44,010 main.py:51] epoch 574, training loss: 13711.01, average training loss: 16212.46, base loss: 19854.61
[INFO 2017-06-27 20:27:44,386 main.py:51] epoch 575, training loss: 15946.23, average training loss: 16212.00, base loss: 19856.32
[INFO 2017-06-27 20:27:44,755 main.py:51] epoch 576, training loss: 16254.19, average training loss: 16212.07, base loss: 19860.35
[INFO 2017-06-27 20:27:45,130 main.py:51] epoch 577, training loss: 14374.84, average training loss: 16208.89, base loss: 19861.43
[INFO 2017-06-27 20:27:45,506 main.py:51] epoch 578, training loss: 14574.00, average training loss: 16206.07, base loss: 19861.84
[INFO 2017-06-27 20:27:45,878 main.py:51] epoch 579, training loss: 12549.28, average training loss: 16199.77, base loss: 19853.77
[INFO 2017-06-27 20:27:46,256 main.py:51] epoch 580, training loss: 15835.67, average training loss: 16199.14, base loss: 19856.63
[INFO 2017-06-27 20:27:46,632 main.py:51] epoch 581, training loss: 16976.06, average training loss: 16200.47, base loss: 19861.73
[INFO 2017-06-27 20:27:47,007 main.py:51] epoch 582, training loss: 12777.11, average training loss: 16194.60, base loss: 19857.38
[INFO 2017-06-27 20:27:47,384 main.py:51] epoch 583, training loss: 14209.39, average training loss: 16191.20, base loss: 19855.66
[INFO 2017-06-27 20:27:47,757 main.py:51] epoch 584, training loss: 15989.10, average training loss: 16190.86, base loss: 19859.37
[INFO 2017-06-27 20:27:48,128 main.py:51] epoch 585, training loss: 14113.64, average training loss: 16187.31, base loss: 19858.89
[INFO 2017-06-27 20:27:48,503 main.py:51] epoch 586, training loss: 12696.93, average training loss: 16181.37, base loss: 19854.24
[INFO 2017-06-27 20:27:48,880 main.py:51] epoch 587, training loss: 13828.86, average training loss: 16177.37, base loss: 19851.21
[INFO 2017-06-27 20:27:49,258 main.py:51] epoch 588, training loss: 15495.87, average training loss: 16176.21, base loss: 19854.20
[INFO 2017-06-27 20:27:49,631 main.py:51] epoch 589, training loss: 15894.69, average training loss: 16175.73, base loss: 19857.33
[INFO 2017-06-27 20:27:50,000 main.py:51] epoch 590, training loss: 15666.23, average training loss: 16174.87, base loss: 19858.93
[INFO 2017-06-27 20:27:50,378 main.py:51] epoch 591, training loss: 15842.80, average training loss: 16174.31, base loss: 19863.21
[INFO 2017-06-27 20:27:50,753 main.py:51] epoch 592, training loss: 17144.80, average training loss: 16175.95, base loss: 19870.22
[INFO 2017-06-27 20:27:51,124 main.py:51] epoch 593, training loss: 15406.23, average training loss: 16174.65, base loss: 19871.10
[INFO 2017-06-27 20:27:51,500 main.py:51] epoch 594, training loss: 13429.36, average training loss: 16170.04, base loss: 19868.55
[INFO 2017-06-27 20:27:51,871 main.py:51] epoch 595, training loss: 11677.74, average training loss: 16162.50, base loss: 19859.29
[INFO 2017-06-27 20:27:52,245 main.py:51] epoch 596, training loss: 13306.34, average training loss: 16157.71, base loss: 19857.03
[INFO 2017-06-27 20:27:52,625 main.py:51] epoch 597, training loss: 13290.54, average training loss: 16152.92, base loss: 19855.53
[INFO 2017-06-27 20:27:52,996 main.py:51] epoch 598, training loss: 14767.91, average training loss: 16150.61, base loss: 19856.39
[INFO 2017-06-27 20:27:53,366 main.py:51] epoch 599, training loss: 15353.14, average training loss: 16149.28, base loss: 19856.59
[INFO 2017-06-27 20:27:53,366 main.py:53] epoch 599, testing
[INFO 2017-06-27 20:27:54,979 main.py:105] average testing loss: 14752.49, base loss: 19927.70
[INFO 2017-06-27 20:27:54,979 main.py:106] improve_loss: 5175.22, improve_percent: 0.26
[INFO 2017-06-27 20:27:54,979 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:27:54,992 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 20:27:55,369 main.py:51] epoch 600, training loss: 13906.48, average training loss: 16145.55, base loss: 19854.49
[INFO 2017-06-27 20:27:55,741 main.py:51] epoch 601, training loss: 15070.50, average training loss: 16143.76, base loss: 19853.64
[INFO 2017-06-27 20:27:56,115 main.py:51] epoch 602, training loss: 15618.42, average training loss: 16142.89, base loss: 19855.59
[INFO 2017-06-27 20:27:56,495 main.py:51] epoch 603, training loss: 14411.82, average training loss: 16140.02, base loss: 19853.03
[INFO 2017-06-27 20:27:56,877 main.py:51] epoch 604, training loss: 15995.85, average training loss: 16139.79, base loss: 19855.86
[INFO 2017-06-27 20:27:57,252 main.py:51] epoch 605, training loss: 14502.99, average training loss: 16137.08, base loss: 19854.57
[INFO 2017-06-27 20:27:57,627 main.py:51] epoch 606, training loss: 13813.85, average training loss: 16133.26, base loss: 19854.12
[INFO 2017-06-27 20:27:58,057 main.py:51] epoch 607, training loss: 13548.79, average training loss: 16129.01, base loss: 19850.83
[INFO 2017-06-27 20:27:58,473 main.py:51] epoch 608, training loss: 17518.76, average training loss: 16131.29, base loss: 19857.08
[INFO 2017-06-27 20:27:58,854 main.py:51] epoch 609, training loss: 13503.94, average training loss: 16126.98, base loss: 19854.57
[INFO 2017-06-27 20:27:59,247 main.py:51] epoch 610, training loss: 15600.88, average training loss: 16126.12, base loss: 19856.69
[INFO 2017-06-27 20:27:59,627 main.py:51] epoch 611, training loss: 12767.29, average training loss: 16120.63, base loss: 19852.34
[INFO 2017-06-27 20:28:00,003 main.py:51] epoch 612, training loss: 14305.31, average training loss: 16117.67, base loss: 19852.32
[INFO 2017-06-27 20:28:00,400 main.py:51] epoch 613, training loss: 15171.73, average training loss: 16116.13, base loss: 19853.47
[INFO 2017-06-27 20:28:00,776 main.py:51] epoch 614, training loss: 12277.59, average training loss: 16109.89, base loss: 19848.43
[INFO 2017-06-27 20:28:01,150 main.py:51] epoch 615, training loss: 14345.26, average training loss: 16107.02, base loss: 19848.31
[INFO 2017-06-27 20:28:01,528 main.py:51] epoch 616, training loss: 16502.32, average training loss: 16107.66, base loss: 19853.21
[INFO 2017-06-27 20:28:01,904 main.py:51] epoch 617, training loss: 13870.41, average training loss: 16104.04, base loss: 19850.32
[INFO 2017-06-27 20:28:02,282 main.py:51] epoch 618, training loss: 13975.62, average training loss: 16100.61, base loss: 19848.46
[INFO 2017-06-27 20:28:02,655 main.py:51] epoch 619, training loss: 15624.51, average training loss: 16099.84, base loss: 19852.82
[INFO 2017-06-27 20:28:03,033 main.py:51] epoch 620, training loss: 16316.73, average training loss: 16100.19, base loss: 19857.83
[INFO 2017-06-27 20:28:03,416 main.py:51] epoch 621, training loss: 16180.18, average training loss: 16100.32, base loss: 19862.84
[INFO 2017-06-27 20:28:03,789 main.py:51] epoch 622, training loss: 15492.41, average training loss: 16099.34, base loss: 19863.01
[INFO 2017-06-27 20:28:04,167 main.py:51] epoch 623, training loss: 12968.44, average training loss: 16094.32, base loss: 19857.51
[INFO 2017-06-27 20:28:04,539 main.py:51] epoch 624, training loss: 13919.54, average training loss: 16090.84, base loss: 19857.26
[INFO 2017-06-27 20:28:04,915 main.py:51] epoch 625, training loss: 14889.21, average training loss: 16088.92, base loss: 19858.42
[INFO 2017-06-27 20:28:05,293 main.py:51] epoch 626, training loss: 16181.92, average training loss: 16089.07, base loss: 19861.78
[INFO 2017-06-27 20:28:05,669 main.py:51] epoch 627, training loss: 14562.00, average training loss: 16086.64, base loss: 19861.82
[INFO 2017-06-27 20:28:06,050 main.py:51] epoch 628, training loss: 13493.31, average training loss: 16082.52, base loss: 19856.75
[INFO 2017-06-27 20:28:06,435 main.py:51] epoch 629, training loss: 15389.26, average training loss: 16081.42, base loss: 19857.93
[INFO 2017-06-27 20:28:06,808 main.py:51] epoch 630, training loss: 14923.03, average training loss: 16079.58, base loss: 19859.18
[INFO 2017-06-27 20:28:07,225 main.py:51] epoch 631, training loss: 14668.20, average training loss: 16077.35, base loss: 19861.62
[INFO 2017-06-27 20:28:07,639 main.py:51] epoch 632, training loss: 13575.00, average training loss: 16073.39, base loss: 19860.36
[INFO 2017-06-27 20:28:08,026 main.py:51] epoch 633, training loss: 13728.36, average training loss: 16069.70, base loss: 19859.42
[INFO 2017-06-27 20:28:08,415 main.py:51] epoch 634, training loss: 14979.46, average training loss: 16067.98, base loss: 19859.89
[INFO 2017-06-27 20:28:08,797 main.py:51] epoch 635, training loss: 15558.24, average training loss: 16067.18, base loss: 19862.75
[INFO 2017-06-27 20:28:09,264 main.py:51] epoch 636, training loss: 13938.04, average training loss: 16063.83, base loss: 19861.24
[INFO 2017-06-27 20:28:09,664 main.py:51] epoch 637, training loss: 14992.32, average training loss: 16062.16, base loss: 19865.41
[INFO 2017-06-27 20:28:10,047 main.py:51] epoch 638, training loss: 14233.93, average training loss: 16059.29, base loss: 19864.02
[INFO 2017-06-27 20:28:10,424 main.py:51] epoch 639, training loss: 13933.32, average training loss: 16055.97, base loss: 19862.44
[INFO 2017-06-27 20:28:10,802 main.py:51] epoch 640, training loss: 13066.77, average training loss: 16051.31, base loss: 19860.33
[INFO 2017-06-27 20:28:11,180 main.py:51] epoch 641, training loss: 14707.43, average training loss: 16049.22, base loss: 19861.27
[INFO 2017-06-27 20:28:11,562 main.py:51] epoch 642, training loss: 16226.04, average training loss: 16049.49, base loss: 19864.66
[INFO 2017-06-27 20:28:11,938 main.py:51] epoch 643, training loss: 14191.53, average training loss: 16046.61, base loss: 19864.10
[INFO 2017-06-27 20:28:12,425 main.py:51] epoch 644, training loss: 13321.62, average training loss: 16042.38, base loss: 19862.78
[INFO 2017-06-27 20:28:12,803 main.py:51] epoch 645, training loss: 14390.72, average training loss: 16039.82, base loss: 19862.99
[INFO 2017-06-27 20:28:13,281 main.py:51] epoch 646, training loss: 12357.49, average training loss: 16034.13, base loss: 19857.64
[INFO 2017-06-27 20:28:13,666 main.py:51] epoch 647, training loss: 15293.26, average training loss: 16032.99, base loss: 19861.06
[INFO 2017-06-27 20:28:14,061 main.py:51] epoch 648, training loss: 15809.33, average training loss: 16032.64, base loss: 19862.48
[INFO 2017-06-27 20:28:14,461 main.py:51] epoch 649, training loss: 13930.34, average training loss: 16029.41, base loss: 19861.71
[INFO 2017-06-27 20:28:14,843 main.py:51] epoch 650, training loss: 15621.29, average training loss: 16028.78, base loss: 19863.44
[INFO 2017-06-27 20:28:15,219 main.py:51] epoch 651, training loss: 14410.61, average training loss: 16026.30, base loss: 19863.32
[INFO 2017-06-27 20:28:15,687 main.py:51] epoch 652, training loss: 13906.51, average training loss: 16023.06, base loss: 19860.53
[INFO 2017-06-27 20:28:16,088 main.py:51] epoch 653, training loss: 12686.10, average training loss: 16017.95, base loss: 19855.65
[INFO 2017-06-27 20:28:16,472 main.py:51] epoch 654, training loss: 14707.81, average training loss: 16015.95, base loss: 19855.29
[INFO 2017-06-27 20:28:16,923 main.py:51] epoch 655, training loss: 14011.27, average training loss: 16012.90, base loss: 19853.65
[INFO 2017-06-27 20:28:17,357 main.py:51] epoch 656, training loss: 15297.34, average training loss: 16011.81, base loss: 19855.46
[INFO 2017-06-27 20:28:17,735 main.py:51] epoch 657, training loss: 17784.19, average training loss: 16014.50, base loss: 19862.33
[INFO 2017-06-27 20:28:18,125 main.py:51] epoch 658, training loss: 13345.76, average training loss: 16010.45, base loss: 19859.06
[INFO 2017-06-27 20:28:18,524 main.py:51] epoch 659, training loss: 15842.73, average training loss: 16010.20, base loss: 19862.06
[INFO 2017-06-27 20:28:18,903 main.py:51] epoch 660, training loss: 14064.02, average training loss: 16007.25, base loss: 19861.24
[INFO 2017-06-27 20:28:19,372 main.py:51] epoch 661, training loss: 13530.99, average training loss: 16003.51, base loss: 19858.94
[INFO 2017-06-27 20:28:19,759 main.py:51] epoch 662, training loss: 17086.06, average training loss: 16005.15, base loss: 19864.31
[INFO 2017-06-27 20:28:20,170 main.py:51] epoch 663, training loss: 14813.02, average training loss: 16003.35, base loss: 19865.62
[INFO 2017-06-27 20:28:20,579 main.py:51] epoch 664, training loss: 12664.54, average training loss: 15998.33, base loss: 19859.42
[INFO 2017-06-27 20:28:20,957 main.py:51] epoch 665, training loss: 14317.66, average training loss: 15995.81, base loss: 19857.21
[INFO 2017-06-27 20:28:21,337 main.py:51] epoch 666, training loss: 14314.47, average training loss: 15993.28, base loss: 19857.41
[INFO 2017-06-27 20:28:21,808 main.py:51] epoch 667, training loss: 14332.74, average training loss: 15990.80, base loss: 19856.70
[INFO 2017-06-27 20:28:22,199 main.py:51] epoch 668, training loss: 15000.31, average training loss: 15989.32, base loss: 19858.90
[INFO 2017-06-27 20:28:22,624 main.py:51] epoch 669, training loss: 15888.92, average training loss: 15989.17, base loss: 19862.25
[INFO 2017-06-27 20:28:23,045 main.py:51] epoch 670, training loss: 16625.88, average training loss: 15990.12, base loss: 19865.79
[INFO 2017-06-27 20:28:23,436 main.py:51] epoch 671, training loss: 13436.41, average training loss: 15986.32, base loss: 19866.02
[INFO 2017-06-27 20:28:23,825 main.py:51] epoch 672, training loss: 12562.55, average training loss: 15981.23, base loss: 19862.34
[INFO 2017-06-27 20:28:24,235 main.py:51] epoch 673, training loss: 14740.99, average training loss: 15979.39, base loss: 19862.32
[INFO 2017-06-27 20:28:24,620 main.py:51] epoch 674, training loss: 13342.88, average training loss: 15975.48, base loss: 19859.53
[INFO 2017-06-27 20:28:25,013 main.py:51] epoch 675, training loss: 13506.78, average training loss: 15971.83, base loss: 19857.96
[INFO 2017-06-27 20:28:25,458 main.py:51] epoch 676, training loss: 13227.73, average training loss: 15967.78, base loss: 19854.25
[INFO 2017-06-27 20:28:25,843 main.py:51] epoch 677, training loss: 14786.24, average training loss: 15966.04, base loss: 19855.15
[INFO 2017-06-27 20:28:26,242 main.py:51] epoch 678, training loss: 13625.30, average training loss: 15962.59, base loss: 19854.05
[INFO 2017-06-27 20:28:26,727 main.py:51] epoch 679, training loss: 16182.03, average training loss: 15962.91, base loss: 19857.81
[INFO 2017-06-27 20:28:27,128 main.py:51] epoch 680, training loss: 15594.26, average training loss: 15962.37, base loss: 19861.40
[INFO 2017-06-27 20:28:27,600 main.py:51] epoch 681, training loss: 13797.28, average training loss: 15959.20, base loss: 19860.07
[INFO 2017-06-27 20:28:28,012 main.py:51] epoch 682, training loss: 14762.81, average training loss: 15957.44, base loss: 19860.32
[INFO 2017-06-27 20:28:28,391 main.py:51] epoch 683, training loss: 14421.51, average training loss: 15955.20, base loss: 19861.67
[INFO 2017-06-27 20:28:28,775 main.py:51] epoch 684, training loss: 11194.31, average training loss: 15948.25, base loss: 19853.98
[INFO 2017-06-27 20:28:29,162 main.py:51] epoch 685, training loss: 13744.61, average training loss: 15945.04, base loss: 19852.29
[INFO 2017-06-27 20:28:29,545 main.py:51] epoch 686, training loss: 14282.33, average training loss: 15942.62, base loss: 19852.02
[INFO 2017-06-27 20:28:29,922 main.py:51] epoch 687, training loss: 14249.83, average training loss: 15940.16, base loss: 19852.12
[INFO 2017-06-27 20:28:30,298 main.py:51] epoch 688, training loss: 13396.34, average training loss: 15936.46, base loss: 19849.46
[INFO 2017-06-27 20:28:30,673 main.py:51] epoch 689, training loss: 16369.62, average training loss: 15937.09, base loss: 19853.68
[INFO 2017-06-27 20:28:31,050 main.py:51] epoch 690, training loss: 15314.74, average training loss: 15936.19, base loss: 19854.57
[INFO 2017-06-27 20:28:31,426 main.py:51] epoch 691, training loss: 16463.77, average training loss: 15936.95, base loss: 19859.47
[INFO 2017-06-27 20:28:31,809 main.py:51] epoch 692, training loss: 14887.41, average training loss: 15935.44, base loss: 19860.91
[INFO 2017-06-27 20:28:32,185 main.py:51] epoch 693, training loss: 13605.94, average training loss: 15932.08, base loss: 19860.16
[INFO 2017-06-27 20:28:32,561 main.py:51] epoch 694, training loss: 14417.16, average training loss: 15929.90, base loss: 19859.63
[INFO 2017-06-27 20:28:32,937 main.py:51] epoch 695, training loss: 14562.56, average training loss: 15927.94, base loss: 19860.90
[INFO 2017-06-27 20:28:33,316 main.py:51] epoch 696, training loss: 14949.11, average training loss: 15926.53, base loss: 19863.26
[INFO 2017-06-27 20:28:33,693 main.py:51] epoch 697, training loss: 12486.80, average training loss: 15921.60, base loss: 19859.26
[INFO 2017-06-27 20:28:34,070 main.py:51] epoch 698, training loss: 13227.31, average training loss: 15917.75, base loss: 19855.87
[INFO 2017-06-27 20:28:34,447 main.py:51] epoch 699, training loss: 15185.19, average training loss: 15916.70, base loss: 19859.10
[INFO 2017-06-27 20:28:34,447 main.py:53] epoch 699, testing
[INFO 2017-06-27 20:28:36,045 main.py:105] average testing loss: 14660.48, base loss: 20459.69
[INFO 2017-06-27 20:28:36,045 main.py:106] improve_loss: 5799.21, improve_percent: 0.28
[INFO 2017-06-27 20:28:36,046 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:28:36,058 main.py:76] current best improved percent: 0.28
[INFO 2017-06-27 20:28:36,435 main.py:51] epoch 700, training loss: 12944.35, average training loss: 15912.46, base loss: 19856.17
[INFO 2017-06-27 20:28:36,819 main.py:51] epoch 701, training loss: 12441.84, average training loss: 15907.52, base loss: 19850.76
[INFO 2017-06-27 20:28:37,195 main.py:51] epoch 702, training loss: 14142.14, average training loss: 15905.01, base loss: 19849.25
[INFO 2017-06-27 20:28:37,572 main.py:51] epoch 703, training loss: 14598.55, average training loss: 15903.15, base loss: 19850.68
[INFO 2017-06-27 20:28:37,948 main.py:51] epoch 704, training loss: 13595.60, average training loss: 15899.88, base loss: 19848.01
[INFO 2017-06-27 20:28:38,423 main.py:51] epoch 705, training loss: 13472.03, average training loss: 15896.44, base loss: 19846.12
[INFO 2017-06-27 20:28:38,806 main.py:51] epoch 706, training loss: 15718.46, average training loss: 15896.19, base loss: 19848.98
[INFO 2017-06-27 20:28:39,192 main.py:51] epoch 707, training loss: 13407.99, average training loss: 15892.67, base loss: 19846.84
[INFO 2017-06-27 20:28:39,573 main.py:51] epoch 708, training loss: 15064.97, average training loss: 15891.51, base loss: 19847.89
[INFO 2017-06-27 20:28:39,953 main.py:51] epoch 709, training loss: 14620.11, average training loss: 15889.72, base loss: 19848.32
[INFO 2017-06-27 20:28:40,333 main.py:51] epoch 710, training loss: 16899.65, average training loss: 15891.14, base loss: 19855.32
[INFO 2017-06-27 20:28:40,722 main.py:51] epoch 711, training loss: 14152.05, average training loss: 15888.69, base loss: 19854.39
[INFO 2017-06-27 20:28:41,105 main.py:51] epoch 712, training loss: 14645.17, average training loss: 15886.95, base loss: 19854.83
[INFO 2017-06-27 20:28:41,495 main.py:51] epoch 713, training loss: 15110.04, average training loss: 15885.86, base loss: 19856.43
[INFO 2017-06-27 20:28:41,875 main.py:51] epoch 714, training loss: 12385.10, average training loss: 15880.97, base loss: 19852.25
[INFO 2017-06-27 20:28:42,274 main.py:51] epoch 715, training loss: 13227.94, average training loss: 15877.26, base loss: 19849.86
[INFO 2017-06-27 20:28:42,657 main.py:51] epoch 716, training loss: 15976.85, average training loss: 15877.40, base loss: 19852.80
[INFO 2017-06-27 20:28:43,056 main.py:51] epoch 717, training loss: 12824.96, average training loss: 15873.15, base loss: 19850.43
[INFO 2017-06-27 20:28:43,441 main.py:51] epoch 718, training loss: 14759.72, average training loss: 15871.60, base loss: 19851.10
[INFO 2017-06-27 20:28:43,833 main.py:51] epoch 719, training loss: 14590.55, average training loss: 15869.82, base loss: 19851.93
[INFO 2017-06-27 20:28:44,317 main.py:51] epoch 720, training loss: 14667.34, average training loss: 15868.15, base loss: 19852.89
[INFO 2017-06-27 20:28:44,760 main.py:51] epoch 721, training loss: 14588.42, average training loss: 15866.38, base loss: 19852.08
[INFO 2017-06-27 20:28:45,217 main.py:51] epoch 722, training loss: 13523.87, average training loss: 15863.14, base loss: 19851.61
[INFO 2017-06-27 20:28:45,657 main.py:51] epoch 723, training loss: 12998.46, average training loss: 15859.18, base loss: 19849.20
[INFO 2017-06-27 20:28:46,071 main.py:51] epoch 724, training loss: 11967.63, average training loss: 15853.82, base loss: 19843.56
[INFO 2017-06-27 20:28:46,480 main.py:51] epoch 725, training loss: 14095.43, average training loss: 15851.39, base loss: 19843.92
[INFO 2017-06-27 20:28:46,944 main.py:51] epoch 726, training loss: 14751.88, average training loss: 15849.88, base loss: 19846.50
[INFO 2017-06-27 20:28:47,409 main.py:51] epoch 727, training loss: 14466.89, average training loss: 15847.98, base loss: 19845.79
[INFO 2017-06-27 20:28:47,814 main.py:51] epoch 728, training loss: 17647.69, average training loss: 15850.45, base loss: 19852.38
[INFO 2017-06-27 20:28:48,195 main.py:51] epoch 729, training loss: 13905.76, average training loss: 15847.79, base loss: 19851.88
[INFO 2017-06-27 20:28:48,573 main.py:51] epoch 730, training loss: 15324.82, average training loss: 15847.07, base loss: 19853.05
[INFO 2017-06-27 20:28:48,949 main.py:51] epoch 731, training loss: 16598.85, average training loss: 15848.10, base loss: 19857.76
[INFO 2017-06-27 20:28:49,326 main.py:51] epoch 732, training loss: 12300.97, average training loss: 15843.26, base loss: 19852.18
[INFO 2017-06-27 20:28:49,710 main.py:51] epoch 733, training loss: 13905.90, average training loss: 15840.62, base loss: 19852.26
[INFO 2017-06-27 20:28:50,086 main.py:51] epoch 734, training loss: 16675.60, average training loss: 15841.76, base loss: 19857.89
[INFO 2017-06-27 20:28:50,461 main.py:51] epoch 735, training loss: 12512.04, average training loss: 15837.23, base loss: 19854.80
[INFO 2017-06-27 20:28:50,836 main.py:51] epoch 736, training loss: 14467.25, average training loss: 15835.37, base loss: 19855.35
[INFO 2017-06-27 20:28:51,212 main.py:51] epoch 737, training loss: 13932.83, average training loss: 15832.79, base loss: 19854.09
[INFO 2017-06-27 20:28:51,593 main.py:51] epoch 738, training loss: 14154.73, average training loss: 15830.52, base loss: 19854.07
[INFO 2017-06-27 20:28:51,969 main.py:51] epoch 739, training loss: 15022.13, average training loss: 15829.43, base loss: 19854.85
[INFO 2017-06-27 20:28:52,345 main.py:51] epoch 740, training loss: 13620.22, average training loss: 15826.45, base loss: 19854.02
[INFO 2017-06-27 20:28:52,722 main.py:51] epoch 741, training loss: 13142.86, average training loss: 15822.83, base loss: 19853.94
[INFO 2017-06-27 20:28:53,098 main.py:51] epoch 742, training loss: 14371.79, average training loss: 15820.88, base loss: 19855.77
[INFO 2017-06-27 20:28:53,474 main.py:51] epoch 743, training loss: 13502.87, average training loss: 15817.76, base loss: 19855.07
[INFO 2017-06-27 20:28:53,849 main.py:51] epoch 744, training loss: 14054.51, average training loss: 15815.40, base loss: 19855.01
[INFO 2017-06-27 20:28:54,226 main.py:51] epoch 745, training loss: 11913.37, average training loss: 15810.17, base loss: 19850.94
[INFO 2017-06-27 20:28:54,603 main.py:51] epoch 746, training loss: 17153.30, average training loss: 15811.97, base loss: 19856.70
[INFO 2017-06-27 20:28:55,058 main.py:51] epoch 747, training loss: 13334.49, average training loss: 15808.65, base loss: 19855.59
[INFO 2017-06-27 20:28:55,456 main.py:51] epoch 748, training loss: 15668.75, average training loss: 15808.47, base loss: 19858.44
[INFO 2017-06-27 20:28:55,835 main.py:51] epoch 749, training loss: 13571.39, average training loss: 15805.48, base loss: 19855.06
[INFO 2017-06-27 20:28:56,216 main.py:51] epoch 750, training loss: 14236.78, average training loss: 15803.40, base loss: 19855.06
[INFO 2017-06-27 20:28:56,594 main.py:51] epoch 751, training loss: 15178.66, average training loss: 15802.56, base loss: 19855.74
[INFO 2017-06-27 20:28:56,978 main.py:51] epoch 752, training loss: 13694.82, average training loss: 15799.77, base loss: 19853.23
[INFO 2017-06-27 20:28:57,361 main.py:51] epoch 753, training loss: 13605.71, average training loss: 15796.86, base loss: 19851.85
[INFO 2017-06-27 20:28:57,767 main.py:51] epoch 754, training loss: 16539.03, average training loss: 15797.84, base loss: 19856.34
[INFO 2017-06-27 20:28:58,144 main.py:51] epoch 755, training loss: 13852.12, average training loss: 15795.26, base loss: 19855.83
[INFO 2017-06-27 20:28:58,522 main.py:51] epoch 756, training loss: 14875.64, average training loss: 15794.05, base loss: 19856.59
[INFO 2017-06-27 20:28:58,905 main.py:51] epoch 757, training loss: 16201.89, average training loss: 15794.59, base loss: 19860.14
[INFO 2017-06-27 20:28:59,283 main.py:51] epoch 758, training loss: 14307.67, average training loss: 15792.63, base loss: 19859.93
[INFO 2017-06-27 20:28:59,661 main.py:51] epoch 759, training loss: 15950.91, average training loss: 15792.84, base loss: 19863.38
[INFO 2017-06-27 20:29:00,038 main.py:51] epoch 760, training loss: 12511.35, average training loss: 15788.52, base loss: 19860.01
[INFO 2017-06-27 20:29:00,415 main.py:51] epoch 761, training loss: 15420.17, average training loss: 15788.04, base loss: 19861.60
[INFO 2017-06-27 20:29:00,793 main.py:51] epoch 762, training loss: 13754.11, average training loss: 15785.38, base loss: 19860.72
[INFO 2017-06-27 20:29:01,171 main.py:51] epoch 763, training loss: 15593.27, average training loss: 15785.12, base loss: 19864.04
[INFO 2017-06-27 20:29:01,553 main.py:51] epoch 764, training loss: 15418.82, average training loss: 15784.65, base loss: 19865.81
[INFO 2017-06-27 20:29:01,930 main.py:51] epoch 765, training loss: 15588.10, average training loss: 15784.39, base loss: 19870.01
[INFO 2017-06-27 20:29:02,307 main.py:51] epoch 766, training loss: 13788.96, average training loss: 15781.79, base loss: 19870.20
[INFO 2017-06-27 20:29:02,685 main.py:51] epoch 767, training loss: 14758.47, average training loss: 15780.45, base loss: 19872.38
[INFO 2017-06-27 20:29:03,061 main.py:51] epoch 768, training loss: 14658.55, average training loss: 15779.00, base loss: 19872.57
[INFO 2017-06-27 20:29:03,445 main.py:51] epoch 769, training loss: 14269.74, average training loss: 15777.04, base loss: 19872.45
[INFO 2017-06-27 20:29:03,830 main.py:51] epoch 770, training loss: 14319.47, average training loss: 15775.15, base loss: 19872.09
[INFO 2017-06-27 20:29:04,212 main.py:51] epoch 771, training loss: 12355.78, average training loss: 15770.72, base loss: 19868.47
[INFO 2017-06-27 20:29:04,589 main.py:51] epoch 772, training loss: 12930.87, average training loss: 15767.04, base loss: 19865.06
[INFO 2017-06-27 20:29:04,966 main.py:51] epoch 773, training loss: 11767.26, average training loss: 15761.87, base loss: 19860.43
[INFO 2017-06-27 20:29:05,349 main.py:51] epoch 774, training loss: 12017.42, average training loss: 15757.04, base loss: 19857.11
[INFO 2017-06-27 20:29:05,725 main.py:51] epoch 775, training loss: 15209.47, average training loss: 15756.34, base loss: 19858.57
[INFO 2017-06-27 20:29:06,103 main.py:51] epoch 776, training loss: 15517.30, average training loss: 15756.03, base loss: 19860.65
[INFO 2017-06-27 20:29:06,487 main.py:51] epoch 777, training loss: 13751.86, average training loss: 15753.45, base loss: 19859.87
[INFO 2017-06-27 20:29:06,864 main.py:51] epoch 778, training loss: 13189.02, average training loss: 15750.16, base loss: 19858.65
[INFO 2017-06-27 20:29:07,242 main.py:51] epoch 779, training loss: 14120.42, average training loss: 15748.07, base loss: 19858.66
[INFO 2017-06-27 20:29:07,621 main.py:51] epoch 780, training loss: 15210.35, average training loss: 15747.38, base loss: 19859.87
[INFO 2017-06-27 20:29:08,002 main.py:51] epoch 781, training loss: 13419.54, average training loss: 15744.41, base loss: 19858.73
[INFO 2017-06-27 20:29:08,380 main.py:51] epoch 782, training loss: 15152.43, average training loss: 15743.65, base loss: 19860.41
[INFO 2017-06-27 20:29:08,765 main.py:51] epoch 783, training loss: 15798.08, average training loss: 15743.72, base loss: 19863.76
[INFO 2017-06-27 20:29:09,150 main.py:51] epoch 784, training loss: 14953.50, average training loss: 15742.71, base loss: 19866.20
[INFO 2017-06-27 20:29:09,526 main.py:51] epoch 785, training loss: 16959.28, average training loss: 15744.26, base loss: 19873.60
[INFO 2017-06-27 20:29:09,904 main.py:51] epoch 786, training loss: 14670.89, average training loss: 15742.90, base loss: 19873.00
[INFO 2017-06-27 20:29:10,281 main.py:51] epoch 787, training loss: 13063.75, average training loss: 15739.50, base loss: 19869.64
[INFO 2017-06-27 20:29:10,667 main.py:51] epoch 788, training loss: 13874.31, average training loss: 15737.13, base loss: 19870.06
[INFO 2017-06-27 20:29:11,044 main.py:51] epoch 789, training loss: 14689.09, average training loss: 15735.81, base loss: 19872.67
[INFO 2017-06-27 20:29:11,420 main.py:51] epoch 790, training loss: 14022.53, average training loss: 15733.64, base loss: 19872.41
[INFO 2017-06-27 20:29:11,805 main.py:51] epoch 791, training loss: 13835.08, average training loss: 15731.24, base loss: 19872.07
[INFO 2017-06-27 20:29:12,188 main.py:51] epoch 792, training loss: 12098.99, average training loss: 15726.66, base loss: 19867.17
[INFO 2017-06-27 20:29:12,565 main.py:51] epoch 793, training loss: 15364.92, average training loss: 15726.21, base loss: 19868.98
[INFO 2017-06-27 20:29:12,950 main.py:51] epoch 794, training loss: 15195.84, average training loss: 15725.54, base loss: 19870.45
[INFO 2017-06-27 20:29:13,335 main.py:51] epoch 795, training loss: 13960.84, average training loss: 15723.32, base loss: 19869.76
[INFO 2017-06-27 20:29:13,719 main.py:51] epoch 796, training loss: 12998.60, average training loss: 15719.91, base loss: 19867.20
[INFO 2017-06-27 20:29:14,096 main.py:51] epoch 797, training loss: 14580.80, average training loss: 15718.48, base loss: 19868.33
[INFO 2017-06-27 20:29:14,476 main.py:51] epoch 798, training loss: 13630.45, average training loss: 15715.86, base loss: 19867.60
[INFO 2017-06-27 20:29:14,854 main.py:51] epoch 799, training loss: 13926.49, average training loss: 15713.63, base loss: 19866.16
[INFO 2017-06-27 20:29:14,854 main.py:53] epoch 799, testing
[INFO 2017-06-27 20:29:16,452 main.py:105] average testing loss: 13813.39, base loss: 19573.13
[INFO 2017-06-27 20:29:16,452 main.py:106] improve_loss: 5759.75, improve_percent: 0.29
[INFO 2017-06-27 20:29:16,453 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:29:16,475 main.py:76] current best improved percent: 0.29
[INFO 2017-06-27 20:29:16,882 main.py:51] epoch 800, training loss: 15063.05, average training loss: 15712.82, base loss: 19869.00
[INFO 2017-06-27 20:29:17,253 main.py:51] epoch 801, training loss: 13174.09, average training loss: 15709.65, base loss: 19867.41
[INFO 2017-06-27 20:29:17,629 main.py:51] epoch 802, training loss: 13102.81, average training loss: 15706.40, base loss: 19866.40
[INFO 2017-06-27 20:29:18,007 main.py:51] epoch 803, training loss: 13781.07, average training loss: 15704.01, base loss: 19866.33
[INFO 2017-06-27 20:29:18,386 main.py:51] epoch 804, training loss: 13183.16, average training loss: 15700.88, base loss: 19863.82
[INFO 2017-06-27 20:29:18,760 main.py:51] epoch 805, training loss: 13354.20, average training loss: 15697.97, base loss: 19862.16
[INFO 2017-06-27 20:29:19,138 main.py:51] epoch 806, training loss: 14361.24, average training loss: 15696.31, base loss: 19863.15
[INFO 2017-06-27 20:29:19,514 main.py:51] epoch 807, training loss: 14533.19, average training loss: 15694.87, base loss: 19864.97
[INFO 2017-06-27 20:29:19,884 main.py:51] epoch 808, training loss: 15417.81, average training loss: 15694.53, base loss: 19868.01
[INFO 2017-06-27 20:29:20,262 main.py:51] epoch 809, training loss: 13243.10, average training loss: 15691.50, base loss: 19866.19
[INFO 2017-06-27 20:29:20,646 main.py:51] epoch 810, training loss: 13169.39, average training loss: 15688.39, base loss: 19863.91
[INFO 2017-06-27 20:29:21,023 main.py:51] epoch 811, training loss: 14329.34, average training loss: 15686.72, base loss: 19863.96
[INFO 2017-06-27 20:29:21,402 main.py:51] epoch 812, training loss: 13480.68, average training loss: 15684.00, base loss: 19864.32
[INFO 2017-06-27 20:29:21,788 main.py:51] epoch 813, training loss: 15245.92, average training loss: 15683.47, base loss: 19867.67
[INFO 2017-06-27 20:29:22,185 main.py:51] epoch 814, training loss: 14847.04, average training loss: 15682.44, base loss: 19869.63
[INFO 2017-06-27 20:29:22,583 main.py:51] epoch 815, training loss: 16226.08, average training loss: 15683.11, base loss: 19874.20
[INFO 2017-06-27 20:29:22,972 main.py:51] epoch 816, training loss: 14318.91, average training loss: 15681.44, base loss: 19874.85
[INFO 2017-06-27 20:29:23,365 main.py:51] epoch 817, training loss: 13836.66, average training loss: 15679.18, base loss: 19874.23
[INFO 2017-06-27 20:29:23,755 main.py:51] epoch 818, training loss: 13480.15, average training loss: 15676.50, base loss: 19873.34
[INFO 2017-06-27 20:29:24,140 main.py:51] epoch 819, training loss: 11420.46, average training loss: 15671.31, base loss: 19867.52
[INFO 2017-06-27 20:29:24,524 main.py:51] epoch 820, training loss: 14976.92, average training loss: 15670.46, base loss: 19870.25
[INFO 2017-06-27 20:29:24,945 main.py:51] epoch 821, training loss: 14182.04, average training loss: 15668.65, base loss: 19871.31
[INFO 2017-06-27 20:29:25,402 main.py:51] epoch 822, training loss: 12984.76, average training loss: 15665.39, base loss: 19869.54
[INFO 2017-06-27 20:29:25,850 main.py:51] epoch 823, training loss: 14148.54, average training loss: 15663.55, base loss: 19870.39
[INFO 2017-06-27 20:29:26,302 main.py:51] epoch 824, training loss: 13303.46, average training loss: 15660.69, base loss: 19869.25
[INFO 2017-06-27 20:29:26,751 main.py:51] epoch 825, training loss: 13871.58, average training loss: 15658.52, base loss: 19868.87
[INFO 2017-06-27 20:29:27,153 main.py:51] epoch 826, training loss: 16233.47, average training loss: 15659.22, base loss: 19872.86
[INFO 2017-06-27 20:29:27,582 main.py:51] epoch 827, training loss: 15556.17, average training loss: 15659.09, base loss: 19876.72
[INFO 2017-06-27 20:29:28,052 main.py:51] epoch 828, training loss: 12072.09, average training loss: 15654.76, base loss: 19872.02
[INFO 2017-06-27 20:29:28,458 main.py:51] epoch 829, training loss: 15442.74, average training loss: 15654.51, base loss: 19875.22
[INFO 2017-06-27 20:29:28,847 main.py:51] epoch 830, training loss: 14117.82, average training loss: 15652.66, base loss: 19875.76
[INFO 2017-06-27 20:29:29,233 main.py:51] epoch 831, training loss: 14812.38, average training loss: 15651.65, base loss: 19875.74
[INFO 2017-06-27 20:29:29,612 main.py:51] epoch 832, training loss: 15165.04, average training loss: 15651.07, base loss: 19878.15
[INFO 2017-06-27 20:29:29,990 main.py:51] epoch 833, training loss: 13119.89, average training loss: 15648.03, base loss: 19876.83
[INFO 2017-06-27 20:29:30,370 main.py:51] epoch 834, training loss: 14689.82, average training loss: 15646.88, base loss: 19879.38
[INFO 2017-06-27 20:29:30,755 main.py:51] epoch 835, training loss: 13474.13, average training loss: 15644.28, base loss: 19879.22
[INFO 2017-06-27 20:29:31,132 main.py:51] epoch 836, training loss: 14199.95, average training loss: 15642.56, base loss: 19878.93
[INFO 2017-06-27 20:29:31,510 main.py:51] epoch 837, training loss: 13539.74, average training loss: 15640.05, base loss: 19878.69
[INFO 2017-06-27 20:29:31,887 main.py:51] epoch 838, training loss: 13121.99, average training loss: 15637.05, base loss: 19877.14
[INFO 2017-06-27 20:29:32,263 main.py:51] epoch 839, training loss: 15042.62, average training loss: 15636.34, base loss: 19879.02
[INFO 2017-06-27 20:29:32,635 main.py:51] epoch 840, training loss: 13193.74, average training loss: 15633.44, base loss: 19877.41
[INFO 2017-06-27 20:29:33,007 main.py:51] epoch 841, training loss: 13684.76, average training loss: 15631.12, base loss: 19876.09
[INFO 2017-06-27 20:29:33,380 main.py:51] epoch 842, training loss: 15103.20, average training loss: 15630.50, base loss: 19878.38
[INFO 2017-06-27 20:29:33,757 main.py:51] epoch 843, training loss: 15205.37, average training loss: 15629.99, base loss: 19880.34
[INFO 2017-06-27 20:29:34,150 main.py:51] epoch 844, training loss: 15327.91, average training loss: 15629.63, base loss: 19882.94
[INFO 2017-06-27 20:29:34,527 main.py:51] epoch 845, training loss: 13265.13, average training loss: 15626.84, base loss: 19881.53
[INFO 2017-06-27 20:29:34,914 main.py:51] epoch 846, training loss: 13705.51, average training loss: 15624.57, base loss: 19880.80
[INFO 2017-06-27 20:29:35,292 main.py:51] epoch 847, training loss: 13085.39, average training loss: 15621.58, base loss: 19877.45
[INFO 2017-06-27 20:29:35,665 main.py:51] epoch 848, training loss: 12278.95, average training loss: 15617.64, base loss: 19872.45
[INFO 2017-06-27 20:29:36,041 main.py:51] epoch 849, training loss: 13848.46, average training loss: 15615.56, base loss: 19872.39
[INFO 2017-06-27 20:29:36,418 main.py:51] epoch 850, training loss: 12564.56, average training loss: 15611.97, base loss: 19870.25
[INFO 2017-06-27 20:29:36,793 main.py:51] epoch 851, training loss: 13971.60, average training loss: 15610.05, base loss: 19870.34
[INFO 2017-06-27 20:29:37,169 main.py:51] epoch 852, training loss: 13204.07, average training loss: 15607.23, base loss: 19869.22
[INFO 2017-06-27 20:29:37,547 main.py:51] epoch 853, training loss: 14112.72, average training loss: 15605.48, base loss: 19868.60
[INFO 2017-06-27 20:29:37,924 main.py:51] epoch 854, training loss: 14195.46, average training loss: 15603.83, base loss: 19869.45
[INFO 2017-06-27 20:29:38,300 main.py:51] epoch 855, training loss: 14903.22, average training loss: 15603.01, base loss: 19870.33
[INFO 2017-06-27 20:29:38,676 main.py:51] epoch 856, training loss: 14004.84, average training loss: 15601.14, base loss: 19869.35
[INFO 2017-06-27 20:29:39,126 main.py:51] epoch 857, training loss: 15305.34, average training loss: 15600.80, base loss: 19870.30
[INFO 2017-06-27 20:29:39,527 main.py:51] epoch 858, training loss: 15866.30, average training loss: 15601.11, base loss: 19874.04
[INFO 2017-06-27 20:29:39,906 main.py:51] epoch 859, training loss: 13876.77, average training loss: 15599.10, base loss: 19873.24
[INFO 2017-06-27 20:29:40,309 main.py:51] epoch 860, training loss: 11541.42, average training loss: 15594.39, base loss: 19867.26
[INFO 2017-06-27 20:29:40,681 main.py:51] epoch 861, training loss: 14867.45, average training loss: 15593.55, base loss: 19868.60
[INFO 2017-06-27 20:29:41,058 main.py:51] epoch 862, training loss: 13170.30, average training loss: 15590.74, base loss: 19865.94
[INFO 2017-06-27 20:29:41,428 main.py:51] epoch 863, training loss: 14551.59, average training loss: 15589.54, base loss: 19866.14
[INFO 2017-06-27 20:29:41,803 main.py:51] epoch 864, training loss: 13652.83, average training loss: 15587.30, base loss: 19864.88
[INFO 2017-06-27 20:29:42,178 main.py:51] epoch 865, training loss: 14328.10, average training loss: 15585.84, base loss: 19864.54
[INFO 2017-06-27 20:29:42,552 main.py:51] epoch 866, training loss: 12338.22, average training loss: 15582.10, base loss: 19861.05
[INFO 2017-06-27 20:29:42,923 main.py:51] epoch 867, training loss: 14983.22, average training loss: 15581.41, base loss: 19864.02
[INFO 2017-06-27 20:29:43,294 main.py:51] epoch 868, training loss: 12766.75, average training loss: 15578.17, base loss: 19861.86
[INFO 2017-06-27 20:29:43,664 main.py:51] epoch 869, training loss: 14773.22, average training loss: 15577.24, base loss: 19863.95
[INFO 2017-06-27 20:29:44,041 main.py:51] epoch 870, training loss: 14945.16, average training loss: 15576.52, base loss: 19866.08
[INFO 2017-06-27 20:29:44,416 main.py:51] epoch 871, training loss: 14503.51, average training loss: 15575.29, base loss: 19867.96
[INFO 2017-06-27 20:29:44,787 main.py:51] epoch 872, training loss: 14395.58, average training loss: 15573.94, base loss: 19869.72
[INFO 2017-06-27 20:29:45,162 main.py:51] epoch 873, training loss: 14177.82, average training loss: 15572.34, base loss: 19870.89
[INFO 2017-06-27 20:29:45,538 main.py:51] epoch 874, training loss: 15033.12, average training loss: 15571.72, base loss: 19872.98
[INFO 2017-06-27 20:29:45,912 main.py:51] epoch 875, training loss: 15364.55, average training loss: 15571.49, base loss: 19874.73
[INFO 2017-06-27 20:29:46,287 main.py:51] epoch 876, training loss: 14861.02, average training loss: 15570.68, base loss: 19876.06
[INFO 2017-06-27 20:29:46,749 main.py:51] epoch 877, training loss: 13484.51, average training loss: 15568.30, base loss: 19875.95
[INFO 2017-06-27 20:29:47,146 main.py:51] epoch 878, training loss: 13860.91, average training loss: 15566.36, base loss: 19876.34
[INFO 2017-06-27 20:29:47,523 main.py:51] epoch 879, training loss: 15735.14, average training loss: 15566.55, base loss: 19879.80
[INFO 2017-06-27 20:29:47,901 main.py:51] epoch 880, training loss: 11788.75, average training loss: 15562.26, base loss: 19875.70
[INFO 2017-06-27 20:29:48,361 main.py:51] epoch 881, training loss: 11814.37, average training loss: 15558.01, base loss: 19871.98
[INFO 2017-06-27 20:29:48,759 main.py:51] epoch 882, training loss: 14937.09, average training loss: 15557.31, base loss: 19873.99
[INFO 2017-06-27 20:29:49,138 main.py:51] epoch 883, training loss: 14215.07, average training loss: 15555.79, base loss: 19874.30
[INFO 2017-06-27 20:29:49,524 main.py:51] epoch 884, training loss: 14444.64, average training loss: 15554.53, base loss: 19874.85
[INFO 2017-06-27 20:29:49,901 main.py:51] epoch 885, training loss: 15038.05, average training loss: 15553.95, base loss: 19876.17
[INFO 2017-06-27 20:29:50,277 main.py:51] epoch 886, training loss: 15302.83, average training loss: 15553.67, base loss: 19878.91
[INFO 2017-06-27 20:29:50,651 main.py:51] epoch 887, training loss: 13100.52, average training loss: 15550.91, base loss: 19877.45
[INFO 2017-06-27 20:29:51,025 main.py:51] epoch 888, training loss: 13794.57, average training loss: 15548.93, base loss: 19877.25
[INFO 2017-06-27 20:29:51,401 main.py:51] epoch 889, training loss: 13515.84, average training loss: 15546.65, base loss: 19876.61
[INFO 2017-06-27 20:29:51,787 main.py:51] epoch 890, training loss: 13672.96, average training loss: 15544.54, base loss: 19875.94
[INFO 2017-06-27 20:29:52,164 main.py:51] epoch 891, training loss: 14713.64, average training loss: 15543.61, base loss: 19877.47
[INFO 2017-06-27 20:29:52,539 main.py:51] epoch 892, training loss: 14168.90, average training loss: 15542.07, base loss: 19877.01
[INFO 2017-06-27 20:29:52,919 main.py:51] epoch 893, training loss: 13751.00, average training loss: 15540.07, base loss: 19876.51
[INFO 2017-06-27 20:29:53,300 main.py:51] epoch 894, training loss: 12685.22, average training loss: 15536.88, base loss: 19873.69
[INFO 2017-06-27 20:29:53,676 main.py:51] epoch 895, training loss: 14492.71, average training loss: 15535.71, base loss: 19875.46
[INFO 2017-06-27 20:29:54,052 main.py:51] epoch 896, training loss: 13978.35, average training loss: 15533.98, base loss: 19876.13
[INFO 2017-06-27 20:29:54,429 main.py:51] epoch 897, training loss: 15387.90, average training loss: 15533.82, base loss: 19879.57
[INFO 2017-06-27 20:29:54,808 main.py:51] epoch 898, training loss: 14011.98, average training loss: 15532.12, base loss: 19878.84
[INFO 2017-06-27 20:29:55,183 main.py:51] epoch 899, training loss: 12942.03, average training loss: 15529.24, base loss: 19876.16
[INFO 2017-06-27 20:29:55,184 main.py:53] epoch 899, testing
[INFO 2017-06-27 20:29:56,781 main.py:105] average testing loss: 13988.10, base loss: 19716.69
[INFO 2017-06-27 20:29:56,781 main.py:106] improve_loss: 5728.59, improve_percent: 0.29
[INFO 2017-06-27 20:29:56,781 main.py:76] current best improved percent: 0.29
[INFO 2017-06-27 20:29:57,161 main.py:51] epoch 900, training loss: 13448.81, average training loss: 15526.94, base loss: 19875.13
[INFO 2017-06-27 20:29:57,534 main.py:51] epoch 901, training loss: 15337.40, average training loss: 15526.73, base loss: 19878.56
[INFO 2017-06-27 20:29:57,905 main.py:51] epoch 902, training loss: 12691.18, average training loss: 15523.59, base loss: 19874.84
[INFO 2017-06-27 20:29:58,276 main.py:51] epoch 903, training loss: 13942.24, average training loss: 15521.84, base loss: 19875.00
[INFO 2017-06-27 20:29:58,658 main.py:51] epoch 904, training loss: 15003.73, average training loss: 15521.26, base loss: 19877.80
[INFO 2017-06-27 20:29:59,035 main.py:51] epoch 905, training loss: 15645.92, average training loss: 15521.40, base loss: 19881.12
[INFO 2017-06-27 20:29:59,413 main.py:51] epoch 906, training loss: 13056.23, average training loss: 15518.68, base loss: 19878.69
[INFO 2017-06-27 20:29:59,876 main.py:51] epoch 907, training loss: 14204.59, average training loss: 15517.24, base loss: 19879.20
[INFO 2017-06-27 20:30:00,276 main.py:51] epoch 908, training loss: 12335.01, average training loss: 15513.73, base loss: 19876.59
[INFO 2017-06-27 20:30:00,658 main.py:51] epoch 909, training loss: 15629.75, average training loss: 15513.86, base loss: 19880.25
[INFO 2017-06-27 20:30:01,037 main.py:51] epoch 910, training loss: 15447.42, average training loss: 15513.79, base loss: 19883.18
[INFO 2017-06-27 20:30:01,418 main.py:51] epoch 911, training loss: 12822.25, average training loss: 15510.84, base loss: 19880.94
[INFO 2017-06-27 20:30:01,802 main.py:51] epoch 912, training loss: 15649.66, average training loss: 15510.99, base loss: 19883.03
[INFO 2017-06-27 20:30:02,177 main.py:51] epoch 913, training loss: 12550.86, average training loss: 15507.75, base loss: 19880.30
[INFO 2017-06-27 20:30:02,637 main.py:51] epoch 914, training loss: 14329.43, average training loss: 15506.46, base loss: 19880.30
[INFO 2017-06-27 20:30:03,074 main.py:51] epoch 915, training loss: 12709.33, average training loss: 15503.41, base loss: 19877.38
[INFO 2017-06-27 20:30:03,456 main.py:51] epoch 916, training loss: 15572.20, average training loss: 15503.49, base loss: 19879.25
[INFO 2017-06-27 20:30:03,836 main.py:51] epoch 917, training loss: 13145.58, average training loss: 15500.92, base loss: 19876.79
[INFO 2017-06-27 20:30:04,234 main.py:51] epoch 918, training loss: 12910.22, average training loss: 15498.10, base loss: 19874.92
[INFO 2017-06-27 20:30:04,614 main.py:51] epoch 919, training loss: 15398.51, average training loss: 15497.99, base loss: 19877.31
[INFO 2017-06-27 20:30:04,988 main.py:51] epoch 920, training loss: 14921.27, average training loss: 15497.36, base loss: 19878.48
[INFO 2017-06-27 20:30:05,366 main.py:51] epoch 921, training loss: 11259.48, average training loss: 15492.77, base loss: 19873.68
[INFO 2017-06-27 20:30:05,756 main.py:51] epoch 922, training loss: 13295.67, average training loss: 15490.39, base loss: 19873.69
[INFO 2017-06-27 20:30:06,140 main.py:51] epoch 923, training loss: 13481.58, average training loss: 15488.21, base loss: 19873.72
[INFO 2017-06-27 20:30:06,521 main.py:51] epoch 924, training loss: 12291.28, average training loss: 15484.76, base loss: 19871.83
[INFO 2017-06-27 20:30:06,899 main.py:51] epoch 925, training loss: 13396.87, average training loss: 15482.50, base loss: 19871.69
[INFO 2017-06-27 20:30:07,278 main.py:51] epoch 926, training loss: 13247.05, average training loss: 15480.09, base loss: 19871.35
[INFO 2017-06-27 20:30:07,655 main.py:51] epoch 927, training loss: 16238.21, average training loss: 15480.91, base loss: 19875.17
[INFO 2017-06-27 20:30:08,031 main.py:51] epoch 928, training loss: 14252.91, average training loss: 15479.59, base loss: 19876.55
[INFO 2017-06-27 20:30:08,409 main.py:51] epoch 929, training loss: 13928.92, average training loss: 15477.92, base loss: 19877.05
[INFO 2017-06-27 20:30:08,789 main.py:51] epoch 930, training loss: 13250.80, average training loss: 15475.53, base loss: 19875.89
[INFO 2017-06-27 20:30:09,168 main.py:51] epoch 931, training loss: 14508.54, average training loss: 15474.49, base loss: 19875.69
[INFO 2017-06-27 20:30:09,551 main.py:51] epoch 932, training loss: 13386.62, average training loss: 15472.25, base loss: 19873.16
[INFO 2017-06-27 20:30:09,933 main.py:51] epoch 933, training loss: 11814.11, average training loss: 15468.33, base loss: 19867.63
[INFO 2017-06-27 20:30:10,309 main.py:51] epoch 934, training loss: 12241.46, average training loss: 15464.88, base loss: 19865.13
[INFO 2017-06-27 20:30:10,684 main.py:51] epoch 935, training loss: 12474.72, average training loss: 15461.69, base loss: 19861.48
[INFO 2017-06-27 20:30:11,063 main.py:51] epoch 936, training loss: 13926.06, average training loss: 15460.05, base loss: 19862.11
[INFO 2017-06-27 20:30:11,444 main.py:51] epoch 937, training loss: 13545.64, average training loss: 15458.01, base loss: 19860.53
[INFO 2017-06-27 20:30:11,822 main.py:51] epoch 938, training loss: 14902.58, average training loss: 15457.42, base loss: 19862.14
[INFO 2017-06-27 20:30:12,202 main.py:51] epoch 939, training loss: 14133.96, average training loss: 15456.01, base loss: 19862.70
[INFO 2017-06-27 20:30:12,580 main.py:51] epoch 940, training loss: 13385.32, average training loss: 15453.81, base loss: 19862.51
[INFO 2017-06-27 20:30:12,990 main.py:51] epoch 941, training loss: 15703.23, average training loss: 15454.07, base loss: 19866.76
[INFO 2017-06-27 20:30:13,367 main.py:51] epoch 942, training loss: 13134.64, average training loss: 15451.61, base loss: 19866.34
[INFO 2017-06-27 20:30:13,741 main.py:51] epoch 943, training loss: 13255.31, average training loss: 15449.29, base loss: 19864.36
[INFO 2017-06-27 20:30:14,113 main.py:51] epoch 944, training loss: 13125.87, average training loss: 15446.83, base loss: 19863.36
[INFO 2017-06-27 20:30:14,493 main.py:51] epoch 945, training loss: 14604.88, average training loss: 15445.94, base loss: 19864.72
[INFO 2017-06-27 20:30:14,870 main.py:51] epoch 946, training loss: 13094.54, average training loss: 15443.45, base loss: 19863.82
[INFO 2017-06-27 20:30:15,251 main.py:51] epoch 947, training loss: 15933.88, average training loss: 15443.97, base loss: 19867.59
[INFO 2017-06-27 20:30:15,627 main.py:51] epoch 948, training loss: 14032.95, average training loss: 15442.49, base loss: 19868.87
[INFO 2017-06-27 20:30:16,005 main.py:51] epoch 949, training loss: 14754.53, average training loss: 15441.76, base loss: 19870.63
[INFO 2017-06-27 20:30:16,384 main.py:51] epoch 950, training loss: 12855.29, average training loss: 15439.04, base loss: 19869.26
[INFO 2017-06-27 20:30:16,759 main.py:51] epoch 951, training loss: 13829.67, average training loss: 15437.35, base loss: 19869.12
[INFO 2017-06-27 20:30:17,135 main.py:51] epoch 952, training loss: 13030.30, average training loss: 15434.83, base loss: 19867.99
[INFO 2017-06-27 20:30:17,508 main.py:51] epoch 953, training loss: 12755.11, average training loss: 15432.02, base loss: 19866.85
[INFO 2017-06-27 20:30:17,892 main.py:51] epoch 954, training loss: 12472.09, average training loss: 15428.92, base loss: 19863.76
[INFO 2017-06-27 20:30:18,265 main.py:51] epoch 955, training loss: 13289.73, average training loss: 15426.68, base loss: 19862.32
[INFO 2017-06-27 20:30:18,642 main.py:51] epoch 956, training loss: 13005.72, average training loss: 15424.15, base loss: 19861.87
[INFO 2017-06-27 20:30:19,019 main.py:51] epoch 957, training loss: 14684.51, average training loss: 15423.38, base loss: 19863.83
[INFO 2017-06-27 20:30:19,399 main.py:51] epoch 958, training loss: 12562.10, average training loss: 15420.39, base loss: 19861.28
[INFO 2017-06-27 20:30:19,778 main.py:51] epoch 959, training loss: 13283.44, average training loss: 15418.17, base loss: 19860.01
[INFO 2017-06-27 20:30:20,155 main.py:51] epoch 960, training loss: 15247.32, average training loss: 15417.99, base loss: 19862.11
[INFO 2017-06-27 20:30:20,532 main.py:51] epoch 961, training loss: 13362.44, average training loss: 15415.85, base loss: 19861.66
[INFO 2017-06-27 20:30:20,909 main.py:51] epoch 962, training loss: 12215.51, average training loss: 15412.53, base loss: 19858.66
[INFO 2017-06-27 20:30:21,286 main.py:51] epoch 963, training loss: 14252.21, average training loss: 15411.33, base loss: 19859.43
[INFO 2017-06-27 20:30:21,662 main.py:51] epoch 964, training loss: 13133.66, average training loss: 15408.97, base loss: 19859.41
[INFO 2017-06-27 20:30:22,049 main.py:51] epoch 965, training loss: 13811.91, average training loss: 15407.31, base loss: 19860.34
[INFO 2017-06-27 20:30:22,425 main.py:51] epoch 966, training loss: 13427.85, average training loss: 15405.27, base loss: 19858.94
[INFO 2017-06-27 20:30:22,805 main.py:51] epoch 967, training loss: 14277.47, average training loss: 15404.10, base loss: 19860.38
[INFO 2017-06-27 20:30:23,182 main.py:51] epoch 968, training loss: 14151.14, average training loss: 15402.81, base loss: 19861.36
[INFO 2017-06-27 20:30:23,561 main.py:51] epoch 969, training loss: 13530.82, average training loss: 15400.88, base loss: 19860.60
[INFO 2017-06-27 20:30:23,941 main.py:51] epoch 970, training loss: 13201.99, average training loss: 15398.61, base loss: 19859.35
[INFO 2017-06-27 20:30:24,316 main.py:51] epoch 971, training loss: 13269.08, average training loss: 15396.42, base loss: 19859.10
[INFO 2017-06-27 20:30:24,688 main.py:51] epoch 972, training loss: 11664.42, average training loss: 15392.59, base loss: 19854.41
[INFO 2017-06-27 20:30:25,065 main.py:51] epoch 973, training loss: 13254.74, average training loss: 15390.39, base loss: 19852.96
[INFO 2017-06-27 20:30:25,438 main.py:51] epoch 974, training loss: 12674.20, average training loss: 15387.61, base loss: 19851.20
[INFO 2017-06-27 20:30:25,814 main.py:51] epoch 975, training loss: 13766.14, average training loss: 15385.94, base loss: 19850.60
[INFO 2017-06-27 20:30:26,192 main.py:51] epoch 976, training loss: 12910.30, average training loss: 15383.41, base loss: 19850.43
[INFO 2017-06-27 20:30:26,569 main.py:51] epoch 977, training loss: 11424.85, average training loss: 15379.36, base loss: 19846.72
[INFO 2017-06-27 20:30:26,944 main.py:51] epoch 978, training loss: 15610.62, average training loss: 15379.60, base loss: 19849.37
[INFO 2017-06-27 20:30:27,317 main.py:51] epoch 979, training loss: 12089.30, average training loss: 15376.24, base loss: 19846.21
[INFO 2017-06-27 20:30:27,691 main.py:51] epoch 980, training loss: 14316.04, average training loss: 15375.16, base loss: 19847.44
[INFO 2017-06-27 20:30:28,063 main.py:51] epoch 981, training loss: 13405.37, average training loss: 15373.16, base loss: 19847.70
[INFO 2017-06-27 20:30:28,440 main.py:51] epoch 982, training loss: 13525.72, average training loss: 15371.28, base loss: 19848.06
[INFO 2017-06-27 20:30:28,820 main.py:51] epoch 983, training loss: 13767.43, average training loss: 15369.65, base loss: 19847.15
[INFO 2017-06-27 20:30:29,195 main.py:51] epoch 984, training loss: 12550.29, average training loss: 15366.78, base loss: 19844.67
[INFO 2017-06-27 20:30:29,572 main.py:51] epoch 985, training loss: 13957.81, average training loss: 15365.35, base loss: 19844.83
[INFO 2017-06-27 20:30:29,949 main.py:51] epoch 986, training loss: 12662.10, average training loss: 15362.62, base loss: 19843.85
[INFO 2017-06-27 20:30:30,326 main.py:51] epoch 987, training loss: 11741.28, average training loss: 15358.95, base loss: 19840.75
[INFO 2017-06-27 20:30:30,702 main.py:51] epoch 988, training loss: 14935.44, average training loss: 15358.52, base loss: 19843.00
[INFO 2017-06-27 20:30:31,077 main.py:51] epoch 989, training loss: 12313.55, average training loss: 15355.45, base loss: 19840.62
[INFO 2017-06-27 20:30:31,452 main.py:51] epoch 990, training loss: 12640.14, average training loss: 15352.71, base loss: 19838.80
[INFO 2017-06-27 20:30:31,822 main.py:51] epoch 991, training loss: 12813.73, average training loss: 15350.15, base loss: 19837.99
[INFO 2017-06-27 20:30:32,198 main.py:51] epoch 992, training loss: 11904.41, average training loss: 15346.68, base loss: 19835.16
[INFO 2017-06-27 20:30:32,583 main.py:51] epoch 993, training loss: 14499.88, average training loss: 15345.83, base loss: 19836.43
[INFO 2017-06-27 20:30:32,962 main.py:51] epoch 994, training loss: 12173.11, average training loss: 15342.64, base loss: 19834.70
[INFO 2017-06-27 20:30:33,339 main.py:51] epoch 995, training loss: 14229.87, average training loss: 15341.52, base loss: 19835.39
[INFO 2017-06-27 20:30:33,798 main.py:51] epoch 996, training loss: 14989.82, average training loss: 15341.17, base loss: 19837.16
[INFO 2017-06-27 20:30:34,197 main.py:51] epoch 997, training loss: 15656.01, average training loss: 15341.48, base loss: 19840.30
[INFO 2017-06-27 20:30:34,576 main.py:51] epoch 998, training loss: 13581.18, average training loss: 15339.72, base loss: 19841.16
[INFO 2017-06-27 20:30:34,961 main.py:51] epoch 999, training loss: 12794.28, average training loss: 15337.17, base loss: 19838.97
[INFO 2017-06-27 20:30:34,961 main.py:53] epoch 999, testing
[INFO 2017-06-27 20:30:36,573 main.py:105] average testing loss: 13647.08, base loss: 19895.23
[INFO 2017-06-27 20:30:36,574 main.py:106] improve_loss: 6248.14, improve_percent: 0.31
[INFO 2017-06-27 20:30:36,574 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:30:36,586 main.py:76] current best improved percent: 0.31
[INFO 2017-06-27 20:30:36,963 main.py:51] epoch 1000, training loss: 13784.97, average training loss: 15320.21, base loss: 19836.69
[INFO 2017-06-27 20:30:37,341 main.py:51] epoch 1001, training loss: 13566.61, average training loss: 15303.90, base loss: 19834.21
[INFO 2017-06-27 20:30:37,721 main.py:51] epoch 1002, training loss: 13246.67, average training loss: 15292.60, base loss: 19835.77
[INFO 2017-06-27 20:30:38,101 main.py:51] epoch 1003, training loss: 14714.91, average training loss: 15283.03, base loss: 19837.20
[INFO 2017-06-27 20:30:38,483 main.py:51] epoch 1004, training loss: 12820.83, average training loss: 15271.89, base loss: 19835.25
[INFO 2017-06-27 20:30:38,860 main.py:51] epoch 1005, training loss: 14724.96, average training loss: 15265.16, base loss: 19838.37
[INFO 2017-06-27 20:30:39,253 main.py:51] epoch 1006, training loss: 12921.36, average training loss: 15256.36, base loss: 19837.85
[INFO 2017-06-27 20:30:39,629 main.py:51] epoch 1007, training loss: 14589.94, average training loss: 15250.24, base loss: 19840.35
[INFO 2017-06-27 20:30:40,009 main.py:51] epoch 1008, training loss: 14064.07, average training loss: 15241.07, base loss: 19839.25
[INFO 2017-06-27 20:30:40,386 main.py:51] epoch 1009, training loss: 16296.81, average training loss: 15236.54, base loss: 19844.49
[INFO 2017-06-27 20:30:40,762 main.py:51] epoch 1010, training loss: 12795.02, average training loss: 15227.22, base loss: 19840.91
[INFO 2017-06-27 20:30:41,138 main.py:51] epoch 1011, training loss: 12327.17, average training loss: 15220.33, base loss: 19839.04
[INFO 2017-06-27 20:30:41,515 main.py:51] epoch 1012, training loss: 14960.41, average training loss: 15215.22, base loss: 19840.90
[INFO 2017-06-27 20:30:41,892 main.py:51] epoch 1013, training loss: 12795.99, average training loss: 15205.76, base loss: 19837.37
[INFO 2017-06-27 20:30:42,267 main.py:51] epoch 1014, training loss: 12313.87, average training loss: 15198.07, base loss: 19832.89
[INFO 2017-06-27 20:30:42,643 main.py:51] epoch 1015, training loss: 14305.26, average training loss: 15194.55, base loss: 19835.23
[INFO 2017-06-27 20:30:43,020 main.py:51] epoch 1016, training loss: 14341.45, average training loss: 15186.71, base loss: 19833.29
[INFO 2017-06-27 20:30:43,398 main.py:51] epoch 1017, training loss: 14707.62, average training loss: 15183.69, base loss: 19837.57
[INFO 2017-06-27 20:30:43,776 main.py:51] epoch 1018, training loss: 13150.54, average training loss: 15177.81, base loss: 19837.09
[INFO 2017-06-27 20:30:44,153 main.py:51] epoch 1019, training loss: 12849.74, average training loss: 15168.89, base loss: 19832.97
[INFO 2017-06-27 20:30:44,530 main.py:51] epoch 1020, training loss: 13461.70, average training loss: 15161.95, base loss: 19831.10
[INFO 2017-06-27 20:30:44,906 main.py:51] epoch 1021, training loss: 14029.56, average training loss: 15158.63, base loss: 19833.98
[INFO 2017-06-27 20:30:45,282 main.py:51] epoch 1022, training loss: 12172.67, average training loss: 15150.00, base loss: 19829.88
[INFO 2017-06-27 20:30:45,659 main.py:51] epoch 1023, training loss: 13056.41, average training loss: 15145.89, base loss: 19832.17
[INFO 2017-06-27 20:30:46,042 main.py:51] epoch 1024, training loss: 13473.50, average training loss: 15140.65, base loss: 19831.42
[INFO 2017-06-27 20:30:46,419 main.py:51] epoch 1025, training loss: 15068.12, average training loss: 15134.74, base loss: 19831.99
[INFO 2017-06-27 20:30:46,795 main.py:51] epoch 1026, training loss: 14480.44, average training loss: 15132.17, base loss: 19836.12
[INFO 2017-06-27 20:30:47,173 main.py:51] epoch 1027, training loss: 13271.42, average training loss: 15129.06, base loss: 19839.58
[INFO 2017-06-27 20:30:47,552 main.py:51] epoch 1028, training loss: 13102.51, average training loss: 15125.28, base loss: 19840.39
[INFO 2017-06-27 20:30:47,928 main.py:51] epoch 1029, training loss: 13852.56, average training loss: 15117.39, base loss: 19837.98
[INFO 2017-06-27 20:30:48,304 main.py:51] epoch 1030, training loss: 13205.95, average training loss: 15113.17, base loss: 19839.34
[INFO 2017-06-27 20:30:48,680 main.py:51] epoch 1031, training loss: 11826.88, average training loss: 15103.91, base loss: 19833.96
[INFO 2017-06-27 20:30:49,056 main.py:51] epoch 1032, training loss: 12487.15, average training loss: 15096.97, base loss: 19832.22
[INFO 2017-06-27 20:30:49,439 main.py:51] epoch 1033, training loss: 13923.93, average training loss: 15091.33, base loss: 19831.81
[INFO 2017-06-27 20:30:49,821 main.py:51] epoch 1034, training loss: 10457.54, average training loss: 15082.75, base loss: 19826.00
[INFO 2017-06-27 20:30:50,198 main.py:51] epoch 1035, training loss: 13002.77, average training loss: 15076.73, base loss: 19825.84
[INFO 2017-06-27 20:30:50,574 main.py:51] epoch 1036, training loss: 14029.59, average training loss: 15070.75, base loss: 19825.69
[INFO 2017-06-27 20:30:50,952 main.py:51] epoch 1037, training loss: 13959.76, average training loss: 15065.36, base loss: 19825.54
[INFO 2017-06-27 20:30:51,329 main.py:51] epoch 1038, training loss: 12005.23, average training loss: 15057.50, base loss: 19821.13
[INFO 2017-06-27 20:30:51,709 main.py:51] epoch 1039, training loss: 16097.26, average training loss: 15055.14, base loss: 19825.60
[INFO 2017-06-27 20:30:52,093 main.py:51] epoch 1040, training loss: 13879.89, average training loss: 15053.81, base loss: 19830.76
[INFO 2017-06-27 20:30:52,470 main.py:51] epoch 1041, training loss: 13343.22, average training loss: 15047.35, base loss: 19827.88
[INFO 2017-06-27 20:30:52,848 main.py:51] epoch 1042, training loss: 12814.74, average training loss: 15041.13, base loss: 19826.12
[INFO 2017-06-27 20:30:53,231 main.py:51] epoch 1043, training loss: 13834.00, average training loss: 15035.17, base loss: 19825.60
[INFO 2017-06-27 20:30:53,607 main.py:51] epoch 1044, training loss: 12967.39, average training loss: 15030.80, base loss: 19825.94
[INFO 2017-06-27 20:30:53,997 main.py:51] epoch 1045, training loss: 12583.87, average training loss: 15027.11, base loss: 19825.40
[INFO 2017-06-27 20:30:54,376 main.py:51] epoch 1046, training loss: 14016.66, average training loss: 15022.22, base loss: 19826.57
[INFO 2017-06-27 20:30:54,754 main.py:51] epoch 1047, training loss: 14440.95, average training loss: 15020.17, base loss: 19829.83
[INFO 2017-06-27 20:30:55,147 main.py:51] epoch 1048, training loss: 13323.72, average training loss: 15014.07, base loss: 19828.27
[INFO 2017-06-27 20:30:55,528 main.py:51] epoch 1049, training loss: 11589.57, average training loss: 15006.14, base loss: 19823.27
[INFO 2017-06-27 20:30:55,914 main.py:51] epoch 1050, training loss: 17071.18, average training loss: 15004.75, base loss: 19827.64
[INFO 2017-06-27 20:30:56,296 main.py:51] epoch 1051, training loss: 11914.00, average training loss: 14998.91, base loss: 19825.80
[INFO 2017-06-27 20:30:56,716 main.py:51] epoch 1052, training loss: 12444.14, average training loss: 14994.98, base loss: 19826.61
[INFO 2017-06-27 20:30:57,127 main.py:51] epoch 1053, training loss: 12872.20, average training loss: 14986.06, base loss: 19821.33
[INFO 2017-06-27 20:30:57,536 main.py:51] epoch 1054, training loss: 12563.22, average training loss: 14981.82, base loss: 19820.17
[INFO 2017-06-27 20:30:57,945 main.py:51] epoch 1055, training loss: 14991.75, average training loss: 14980.45, base loss: 19823.23
[INFO 2017-06-27 20:30:58,356 main.py:51] epoch 1056, training loss: 13123.50, average training loss: 14976.02, base loss: 19822.66
[INFO 2017-06-27 20:30:58,770 main.py:51] epoch 1057, training loss: 13922.84, average training loss: 14971.68, base loss: 19823.72
[INFO 2017-06-27 20:30:59,213 main.py:51] epoch 1058, training loss: 14159.62, average training loss: 14970.61, base loss: 19827.57
[INFO 2017-06-27 20:30:59,667 main.py:51] epoch 1059, training loss: 11717.21, average training loss: 14964.74, base loss: 19825.28
[INFO 2017-06-27 20:31:00,111 main.py:51] epoch 1060, training loss: 13425.49, average training loss: 14960.57, base loss: 19825.68
[INFO 2017-06-27 20:31:00,572 main.py:51] epoch 1061, training loss: 13079.30, average training loss: 14952.68, base loss: 19821.64
[INFO 2017-06-27 20:31:00,983 main.py:51] epoch 1062, training loss: 12961.06, average training loss: 14946.60, base loss: 19819.65
[INFO 2017-06-27 20:31:01,422 main.py:51] epoch 1063, training loss: 13289.69, average training loss: 14940.57, base loss: 19817.65
[INFO 2017-06-27 20:31:01,886 main.py:51] epoch 1064, training loss: 12582.89, average training loss: 14934.79, base loss: 19814.24
[INFO 2017-06-27 20:31:02,290 main.py:51] epoch 1065, training loss: 12027.00, average training loss: 14927.17, base loss: 19810.32
[INFO 2017-06-27 20:31:02,674 main.py:51] epoch 1066, training loss: 12859.33, average training loss: 14921.69, base loss: 19809.68
[INFO 2017-06-27 20:31:03,051 main.py:51] epoch 1067, training loss: 11679.71, average training loss: 14915.41, base loss: 19805.70
[INFO 2017-06-27 20:31:03,427 main.py:51] epoch 1068, training loss: 14322.34, average training loss: 14912.74, base loss: 19809.06
[INFO 2017-06-27 20:31:03,804 main.py:51] epoch 1069, training loss: 14361.71, average training loss: 14907.43, base loss: 19807.66
[INFO 2017-06-27 20:31:04,179 main.py:51] epoch 1070, training loss: 16479.34, average training loss: 14906.81, base loss: 19813.11
[INFO 2017-06-27 20:31:04,554 main.py:51] epoch 1071, training loss: 14897.75, average training loss: 14901.61, base loss: 19812.99
[INFO 2017-06-27 20:31:04,944 main.py:51] epoch 1072, training loss: 11471.39, average training loss: 14896.87, base loss: 19810.06
[INFO 2017-06-27 20:31:05,324 main.py:51] epoch 1073, training loss: 14574.68, average training loss: 14893.80, base loss: 19812.34
[INFO 2017-06-27 20:31:05,701 main.py:51] epoch 1074, training loss: 14613.48, average training loss: 14890.04, base loss: 19811.32
[INFO 2017-06-27 20:31:06,073 main.py:51] epoch 1075, training loss: 13665.04, average training loss: 14883.45, base loss: 19809.51
[INFO 2017-06-27 20:31:06,446 main.py:51] epoch 1076, training loss: 14326.09, average training loss: 14879.04, base loss: 19810.08
[INFO 2017-06-27 20:31:06,816 main.py:51] epoch 1077, training loss: 13119.31, average training loss: 14874.08, base loss: 19809.49
[INFO 2017-06-27 20:31:07,190 main.py:51] epoch 1078, training loss: 13665.20, average training loss: 14869.67, base loss: 19809.77
[INFO 2017-06-27 20:31:07,564 main.py:51] epoch 1079, training loss: 13042.99, average training loss: 14863.97, base loss: 19807.23
[INFO 2017-06-27 20:31:07,939 main.py:51] epoch 1080, training loss: 17004.05, average training loss: 14862.66, base loss: 19812.17
[INFO 2017-06-27 20:31:08,315 main.py:51] epoch 1081, training loss: 11607.38, average training loss: 14858.61, base loss: 19810.71
[INFO 2017-06-27 20:31:08,685 main.py:51] epoch 1082, training loss: 13828.09, average training loss: 14854.26, base loss: 19810.44
[INFO 2017-06-27 20:31:09,078 main.py:51] epoch 1083, training loss: 14108.26, average training loss: 14850.90, base loss: 19811.91
[INFO 2017-06-27 20:31:09,453 main.py:51] epoch 1084, training loss: 14260.73, average training loss: 14846.11, base loss: 19811.66
[INFO 2017-06-27 20:31:09,825 main.py:51] epoch 1085, training loss: 13413.81, average training loss: 14841.76, base loss: 19811.36
[INFO 2017-06-27 20:31:10,201 main.py:51] epoch 1086, training loss: 13138.18, average training loss: 14837.16, base loss: 19809.83
[INFO 2017-06-27 20:31:10,578 main.py:51] epoch 1087, training loss: 13525.96, average training loss: 14828.64, base loss: 19804.03
[INFO 2017-06-27 20:31:10,953 main.py:51] epoch 1088, training loss: 11688.12, average training loss: 14823.13, base loss: 19800.83
[INFO 2017-06-27 20:31:11,326 main.py:51] epoch 1089, training loss: 13031.86, average training loss: 14820.76, base loss: 19801.24
[INFO 2017-06-27 20:31:11,698 main.py:51] epoch 1090, training loss: 12127.51, average training loss: 14814.49, base loss: 19798.10
[INFO 2017-06-27 20:31:12,070 main.py:51] epoch 1091, training loss: 12869.61, average training loss: 14808.67, base loss: 19794.79
[INFO 2017-06-27 20:31:12,442 main.py:51] epoch 1092, training loss: 14296.99, average training loss: 14804.87, base loss: 19795.34
[INFO 2017-06-27 20:31:12,823 main.py:51] epoch 1093, training loss: 14392.82, average training loss: 14800.36, base loss: 19794.47
[INFO 2017-06-27 20:31:13,197 main.py:51] epoch 1094, training loss: 13598.37, average training loss: 14796.96, base loss: 19795.37
[INFO 2017-06-27 20:31:13,573 main.py:51] epoch 1095, training loss: 12823.74, average training loss: 14791.45, base loss: 19791.08
[INFO 2017-06-27 20:31:13,949 main.py:51] epoch 1096, training loss: 14471.26, average training loss: 14787.05, base loss: 19790.43
[INFO 2017-06-27 20:31:14,324 main.py:51] epoch 1097, training loss: 11691.96, average training loss: 14780.98, base loss: 19787.13
[INFO 2017-06-27 20:31:14,693 main.py:51] epoch 1098, training loss: 17001.98, average training loss: 14779.49, base loss: 19792.25
[INFO 2017-06-27 20:31:15,066 main.py:51] epoch 1099, training loss: 13425.37, average training loss: 14775.08, base loss: 19791.69
[INFO 2017-06-27 20:31:15,066 main.py:53] epoch 1099, testing
[INFO 2017-06-27 20:31:16,668 main.py:105] average testing loss: 13473.72, base loss: 19943.51
[INFO 2017-06-27 20:31:16,668 main.py:106] improve_loss: 6469.79, improve_percent: 0.32
[INFO 2017-06-27 20:31:16,669 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:31:16,681 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 20:31:17,057 main.py:51] epoch 1100, training loss: 12002.82, average training loss: 14770.31, base loss: 19789.75
[INFO 2017-06-27 20:31:17,434 main.py:51] epoch 1101, training loss: 14036.50, average training loss: 14766.64, base loss: 19791.83
[INFO 2017-06-27 20:31:17,810 main.py:51] epoch 1102, training loss: 13788.69, average training loss: 14762.42, base loss: 19792.07
[INFO 2017-06-27 20:31:18,186 main.py:51] epoch 1103, training loss: 11569.88, average training loss: 14758.83, base loss: 19791.69
[INFO 2017-06-27 20:31:18,557 main.py:51] epoch 1104, training loss: 13369.80, average training loss: 14754.13, base loss: 19790.82
[INFO 2017-06-27 20:31:18,934 main.py:51] epoch 1105, training loss: 12329.82, average training loss: 14745.10, base loss: 19783.73
[INFO 2017-06-27 20:31:19,315 main.py:51] epoch 1106, training loss: 12680.71, average training loss: 14738.57, base loss: 19779.24
[INFO 2017-06-27 20:31:19,689 main.py:51] epoch 1107, training loss: 13672.30, average training loss: 14735.04, base loss: 19780.00
[INFO 2017-06-27 20:31:20,062 main.py:51] epoch 1108, training loss: 12480.99, average training loss: 14727.55, base loss: 19776.17
[INFO 2017-06-27 20:31:20,437 main.py:51] epoch 1109, training loss: 13454.55, average training loss: 14719.29, base loss: 19771.38
[INFO 2017-06-27 20:31:20,810 main.py:51] epoch 1110, training loss: 12564.66, average training loss: 14715.97, base loss: 19770.23
[INFO 2017-06-27 20:31:21,190 main.py:51] epoch 1111, training loss: 13785.57, average training loss: 14713.02, base loss: 19771.64
[INFO 2017-06-27 20:31:21,565 main.py:51] epoch 1112, training loss: 15394.29, average training loss: 14713.11, base loss: 19777.36
[INFO 2017-06-27 20:31:21,940 main.py:51] epoch 1113, training loss: 16045.42, average training loss: 14711.17, base loss: 19779.58
[INFO 2017-06-27 20:31:22,316 main.py:51] epoch 1114, training loss: 11253.68, average training loss: 14705.52, base loss: 19775.92
[INFO 2017-06-27 20:31:22,702 main.py:51] epoch 1115, training loss: 15312.45, average training loss: 14703.25, base loss: 19778.58
[INFO 2017-06-27 20:31:23,082 main.py:51] epoch 1116, training loss: 13075.99, average training loss: 14698.61, base loss: 19777.27
[INFO 2017-06-27 20:31:23,456 main.py:51] epoch 1117, training loss: 12601.31, average training loss: 14694.18, base loss: 19776.36
[INFO 2017-06-27 20:31:23,829 main.py:51] epoch 1118, training loss: 13129.23, average training loss: 14692.61, base loss: 19778.72
[INFO 2017-06-27 20:31:24,203 main.py:51] epoch 1119, training loss: 12977.22, average training loss: 14687.91, base loss: 19777.33
[INFO 2017-06-27 20:31:24,576 main.py:51] epoch 1120, training loss: 12153.66, average training loss: 14685.61, base loss: 19778.11
[INFO 2017-06-27 20:31:24,952 main.py:51] epoch 1121, training loss: 14589.79, average training loss: 14684.92, base loss: 19783.04
[INFO 2017-06-27 20:31:25,328 main.py:51] epoch 1122, training loss: 11642.82, average training loss: 14679.83, base loss: 19780.74
[INFO 2017-06-27 20:31:25,704 main.py:51] epoch 1123, training loss: 13209.59, average training loss: 14676.31, base loss: 19780.20
[INFO 2017-06-27 20:31:26,079 main.py:51] epoch 1124, training loss: 13386.68, average training loss: 14673.82, base loss: 19781.43
[INFO 2017-06-27 20:31:26,457 main.py:51] epoch 1125, training loss: 13812.92, average training loss: 14670.50, base loss: 19782.32
[INFO 2017-06-27 20:31:26,835 main.py:51] epoch 1126, training loss: 13374.83, average training loss: 14667.14, base loss: 19783.38
[INFO 2017-06-27 20:31:27,211 main.py:51] epoch 1127, training loss: 15319.49, average training loss: 14667.76, base loss: 19789.60
[INFO 2017-06-27 20:31:27,585 main.py:51] epoch 1128, training loss: 13086.53, average training loss: 14662.08, base loss: 19786.89
[INFO 2017-06-27 20:31:27,962 main.py:51] epoch 1129, training loss: 13148.81, average training loss: 14659.78, base loss: 19788.77
[INFO 2017-06-27 20:31:28,339 main.py:51] epoch 1130, training loss: 12213.12, average training loss: 14656.95, base loss: 19788.78
[INFO 2017-06-27 20:31:28,710 main.py:51] epoch 1131, training loss: 14866.66, average training loss: 14655.63, base loss: 19791.40
[INFO 2017-06-27 20:31:29,084 main.py:51] epoch 1132, training loss: 12608.93, average training loss: 14651.04, base loss: 19788.98
[INFO 2017-06-27 20:31:29,462 main.py:51] epoch 1133, training loss: 12452.13, average training loss: 14645.72, base loss: 19787.02
[INFO 2017-06-27 20:31:29,836 main.py:51] epoch 1134, training loss: 15806.54, average training loss: 14645.49, base loss: 19791.49
[INFO 2017-06-27 20:31:30,212 main.py:51] epoch 1135, training loss: 13908.70, average training loss: 14643.07, base loss: 19794.01
[INFO 2017-06-27 20:31:30,591 main.py:51] epoch 1136, training loss: 12289.32, average training loss: 14638.42, base loss: 19791.52
[INFO 2017-06-27 20:31:30,961 main.py:51] epoch 1137, training loss: 14293.43, average training loss: 14637.67, base loss: 19795.72
[INFO 2017-06-27 20:31:31,335 main.py:51] epoch 1138, training loss: 11933.55, average training loss: 14633.84, base loss: 19793.86
[INFO 2017-06-27 20:31:31,705 main.py:51] epoch 1139, training loss: 13887.11, average training loss: 14630.65, base loss: 19793.65
[INFO 2017-06-27 20:31:32,078 main.py:51] epoch 1140, training loss: 13199.85, average training loss: 14627.87, base loss: 19793.83
[INFO 2017-06-27 20:31:32,456 main.py:51] epoch 1141, training loss: 14554.61, average training loss: 14629.00, base loss: 19800.13
[INFO 2017-06-27 20:31:32,831 main.py:51] epoch 1142, training loss: 15406.43, average training loss: 14626.85, base loss: 19801.83
[INFO 2017-06-27 20:31:33,207 main.py:51] epoch 1143, training loss: 13077.19, average training loss: 14623.48, base loss: 19800.93
[INFO 2017-06-27 20:31:33,581 main.py:51] epoch 1144, training loss: 11963.59, average training loss: 14616.82, base loss: 19795.26
[INFO 2017-06-27 20:31:33,962 main.py:51] epoch 1145, training loss: 12695.06, average training loss: 14613.83, base loss: 19794.80
[INFO 2017-06-27 20:31:34,344 main.py:51] epoch 1146, training loss: 13626.33, average training loss: 14610.76, base loss: 19795.99
[INFO 2017-06-27 20:31:34,721 main.py:51] epoch 1147, training loss: 13500.81, average training loss: 14605.41, base loss: 19792.37
[INFO 2017-06-27 20:31:35,096 main.py:51] epoch 1148, training loss: 17064.73, average training loss: 14604.84, base loss: 19795.10
[INFO 2017-06-27 20:31:35,472 main.py:51] epoch 1149, training loss: 13798.32, average training loss: 14602.50, base loss: 19797.10
[INFO 2017-06-27 20:31:35,847 main.py:51] epoch 1150, training loss: 15442.29, average training loss: 14601.46, base loss: 19800.54
[INFO 2017-06-27 20:31:36,219 main.py:51] epoch 1151, training loss: 12419.98, average training loss: 14597.19, base loss: 19797.50
[INFO 2017-06-27 20:31:36,596 main.py:51] epoch 1152, training loss: 13385.49, average training loss: 14593.49, base loss: 19796.74
[INFO 2017-06-27 20:31:36,973 main.py:51] epoch 1153, training loss: 13351.89, average training loss: 14592.04, base loss: 19799.84
[INFO 2017-06-27 20:31:37,353 main.py:51] epoch 1154, training loss: 12282.00, average training loss: 14585.82, base loss: 19796.07
[INFO 2017-06-27 20:31:37,728 main.py:51] epoch 1155, training loss: 15365.85, average training loss: 14580.14, base loss: 19794.31
[INFO 2017-06-27 20:31:38,105 main.py:51] epoch 1156, training loss: 13894.25, average training loss: 14576.19, base loss: 19794.36
[INFO 2017-06-27 20:31:38,480 main.py:51] epoch 1157, training loss: 13245.18, average training loss: 14572.70, base loss: 19793.59
[INFO 2017-06-27 20:31:38,947 main.py:51] epoch 1158, training loss: 14756.85, average training loss: 14568.60, base loss: 19792.41
[INFO 2017-06-27 20:31:39,331 main.py:51] epoch 1159, training loss: 13264.16, average training loss: 14565.29, base loss: 19793.50
[INFO 2017-06-27 20:31:39,711 main.py:51] epoch 1160, training loss: 13025.17, average training loss: 14561.88, base loss: 19793.54
[INFO 2017-06-27 20:31:40,089 main.py:51] epoch 1161, training loss: 15048.02, average training loss: 14561.99, base loss: 19798.05
[INFO 2017-06-27 20:31:40,478 main.py:51] epoch 1162, training loss: 12063.92, average training loss: 14557.82, base loss: 19794.63
[INFO 2017-06-27 20:31:40,855 main.py:51] epoch 1163, training loss: 13588.62, average training loss: 14553.77, base loss: 19794.43
[INFO 2017-06-27 20:31:41,237 main.py:51] epoch 1164, training loss: 12998.85, average training loss: 14549.42, base loss: 19792.83
[INFO 2017-06-27 20:31:41,613 main.py:51] epoch 1165, training loss: 13324.10, average training loss: 14547.28, base loss: 19794.17
[INFO 2017-06-27 20:31:41,989 main.py:51] epoch 1166, training loss: 12726.08, average training loss: 14546.00, base loss: 19796.63
[INFO 2017-06-27 20:31:42,365 main.py:51] epoch 1167, training loss: 13413.86, average training loss: 14541.91, base loss: 19796.00
[INFO 2017-06-27 20:31:42,745 main.py:51] epoch 1168, training loss: 14622.04, average training loss: 14540.58, base loss: 19797.69
[INFO 2017-06-27 20:31:43,117 main.py:51] epoch 1169, training loss: 11883.30, average training loss: 14534.84, base loss: 19793.59
[INFO 2017-06-27 20:31:43,490 main.py:51] epoch 1170, training loss: 13821.94, average training loss: 14531.63, base loss: 19791.15
[INFO 2017-06-27 20:31:43,867 main.py:51] epoch 1171, training loss: 12682.07, average training loss: 14527.66, base loss: 19789.62
[INFO 2017-06-27 20:31:44,243 main.py:51] epoch 1172, training loss: 10816.38, average training loss: 14522.15, base loss: 19784.40
[INFO 2017-06-27 20:31:44,615 main.py:51] epoch 1173, training loss: 13557.86, average training loss: 14518.15, base loss: 19781.53
[INFO 2017-06-27 20:31:44,999 main.py:51] epoch 1174, training loss: 14272.96, average training loss: 14514.04, base loss: 19781.43
[INFO 2017-06-27 20:31:45,384 main.py:51] epoch 1175, training loss: 13214.23, average training loss: 14507.29, base loss: 19776.14
[INFO 2017-06-27 20:31:45,768 main.py:51] epoch 1176, training loss: 14173.11, average training loss: 14504.70, base loss: 19776.87
[INFO 2017-06-27 20:31:46,145 main.py:51] epoch 1177, training loss: 14026.01, average training loss: 14500.46, base loss: 19776.13
[INFO 2017-06-27 20:31:46,520 main.py:51] epoch 1178, training loss: 12828.30, average training loss: 14498.11, base loss: 19776.83
[INFO 2017-06-27 20:31:46,899 main.py:51] epoch 1179, training loss: 11953.59, average training loss: 14495.48, base loss: 19777.06
[INFO 2017-06-27 20:31:47,279 main.py:51] epoch 1180, training loss: 12072.68, average training loss: 14493.09, base loss: 19777.83
[INFO 2017-06-27 20:31:47,654 main.py:51] epoch 1181, training loss: 13921.12, average training loss: 14488.27, base loss: 19776.25
[INFO 2017-06-27 20:31:48,028 main.py:51] epoch 1182, training loss: 13584.93, average training loss: 14485.39, base loss: 19776.33
[INFO 2017-06-27 20:31:48,405 main.py:51] epoch 1183, training loss: 16145.44, average training loss: 14486.47, base loss: 19782.55
[INFO 2017-06-27 20:31:48,783 main.py:51] epoch 1184, training loss: 13975.74, average training loss: 14484.71, base loss: 19785.29
[INFO 2017-06-27 20:31:49,159 main.py:51] epoch 1185, training loss: 12540.44, average training loss: 14480.13, base loss: 19781.42
[INFO 2017-06-27 20:31:49,539 main.py:51] epoch 1186, training loss: 11391.56, average training loss: 14477.17, base loss: 19779.25
[INFO 2017-06-27 20:31:49,917 main.py:51] epoch 1187, training loss: 13120.87, average training loss: 14473.80, base loss: 19777.38
[INFO 2017-06-27 20:31:50,292 main.py:51] epoch 1188, training loss: 13480.88, average training loss: 14469.20, base loss: 19774.91
[INFO 2017-06-27 20:31:50,668 main.py:51] epoch 1189, training loss: 14541.22, average training loss: 14467.46, base loss: 19775.03
[INFO 2017-06-27 20:31:51,047 main.py:51] epoch 1190, training loss: 13253.61, average training loss: 14463.80, base loss: 19773.34
[INFO 2017-06-27 20:31:51,417 main.py:51] epoch 1191, training loss: 12739.97, average training loss: 14460.01, base loss: 19771.79
[INFO 2017-06-27 20:31:51,790 main.py:51] epoch 1192, training loss: 13664.89, average training loss: 14457.02, base loss: 19771.90
[INFO 2017-06-27 20:31:52,166 main.py:51] epoch 1193, training loss: 13069.31, average training loss: 14454.62, base loss: 19773.89
[INFO 2017-06-27 20:31:52,538 main.py:51] epoch 1194, training loss: 12327.27, average training loss: 14450.33, base loss: 19770.40
[INFO 2017-06-27 20:31:52,937 main.py:51] epoch 1195, training loss: 14354.39, average training loss: 14448.33, base loss: 19771.93
[INFO 2017-06-27 20:31:53,312 main.py:51] epoch 1196, training loss: 13849.18, average training loss: 14445.72, base loss: 19771.72
[INFO 2017-06-27 20:31:53,687 main.py:51] epoch 1197, training loss: 12415.88, average training loss: 14440.96, base loss: 19767.01
[INFO 2017-06-27 20:31:54,065 main.py:51] epoch 1198, training loss: 14486.84, average training loss: 14438.63, base loss: 19767.78
[INFO 2017-06-27 20:31:54,442 main.py:51] epoch 1199, training loss: 11498.38, average training loss: 14435.33, base loss: 19767.28
[INFO 2017-06-27 20:31:54,442 main.py:53] epoch 1199, testing
[INFO 2017-06-27 20:31:56,056 main.py:105] average testing loss: 13092.17, base loss: 19856.43
[INFO 2017-06-27 20:31:56,056 main.py:106] improve_loss: 6764.26, improve_percent: 0.34
[INFO 2017-06-27 20:31:56,056 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:31:56,069 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 20:31:56,443 main.py:51] epoch 1200, training loss: 12673.27, average training loss: 14431.61, base loss: 19766.51
[INFO 2017-06-27 20:31:56,815 main.py:51] epoch 1201, training loss: 11536.62, average training loss: 14425.82, base loss: 19760.83
[INFO 2017-06-27 20:31:57,186 main.py:51] epoch 1202, training loss: 11462.48, average training loss: 14422.50, base loss: 19759.77
[INFO 2017-06-27 20:31:57,576 main.py:51] epoch 1203, training loss: 13789.32, average training loss: 14420.82, base loss: 19762.72
[INFO 2017-06-27 20:31:57,955 main.py:51] epoch 1204, training loss: 15492.03, average training loss: 14419.86, base loss: 19766.82
[INFO 2017-06-27 20:31:58,332 main.py:51] epoch 1205, training loss: 12946.07, average training loss: 14418.25, base loss: 19767.27
[INFO 2017-06-27 20:31:58,712 main.py:51] epoch 1206, training loss: 14566.27, average training loss: 14417.10, base loss: 19770.55
[INFO 2017-06-27 20:31:59,092 main.py:51] epoch 1207, training loss: 11800.22, average training loss: 14414.53, base loss: 19770.70
[INFO 2017-06-27 20:31:59,468 main.py:51] epoch 1208, training loss: 13590.79, average training loss: 14411.05, base loss: 19768.95
[INFO 2017-06-27 20:31:59,857 main.py:51] epoch 1209, training loss: 14204.28, average training loss: 14410.42, base loss: 19771.76
[INFO 2017-06-27 20:32:00,239 main.py:51] epoch 1210, training loss: 12173.11, average training loss: 14408.00, base loss: 19770.73
[INFO 2017-06-27 20:32:00,619 main.py:51] epoch 1211, training loss: 11925.28, average training loss: 14404.78, base loss: 19769.68
[INFO 2017-06-27 20:32:01,003 main.py:51] epoch 1212, training loss: 13934.70, average training loss: 14401.97, base loss: 19768.91
[INFO 2017-06-27 20:32:01,388 main.py:51] epoch 1213, training loss: 12976.06, average training loss: 14400.24, base loss: 19769.84
[INFO 2017-06-27 20:32:01,775 main.py:51] epoch 1214, training loss: 14828.65, average training loss: 14400.53, base loss: 19773.85
[INFO 2017-06-27 20:32:02,164 main.py:51] epoch 1215, training loss: 13220.21, average training loss: 14395.99, base loss: 19768.96
[INFO 2017-06-27 20:32:02,597 main.py:51] epoch 1216, training loss: 12607.21, average training loss: 14394.53, base loss: 19770.57
[INFO 2017-06-27 20:32:02,982 main.py:51] epoch 1217, training loss: 14893.68, average training loss: 14392.55, base loss: 19772.11
[INFO 2017-06-27 20:32:03,370 main.py:51] epoch 1218, training loss: 14997.82, average training loss: 14391.56, base loss: 19774.57
[INFO 2017-06-27 20:32:03,754 main.py:51] epoch 1219, training loss: 13969.60, average training loss: 14387.95, base loss: 19773.96
[INFO 2017-06-27 20:32:04,142 main.py:51] epoch 1220, training loss: 11999.45, average training loss: 14385.67, base loss: 19774.66
[INFO 2017-06-27 20:32:04,591 main.py:51] epoch 1221, training loss: 13356.35, average training loss: 14383.42, base loss: 19774.44
[INFO 2017-06-27 20:32:05,046 main.py:51] epoch 1222, training loss: 15742.70, average training loss: 14384.15, base loss: 19778.74
[INFO 2017-06-27 20:32:05,487 main.py:51] epoch 1223, training loss: 11577.54, average training loss: 14378.66, base loss: 19775.07
[INFO 2017-06-27 20:32:05,931 main.py:51] epoch 1224, training loss: 14054.96, average training loss: 14379.28, base loss: 19779.62
[INFO 2017-06-27 20:32:06,350 main.py:51] epoch 1225, training loss: 14290.09, average training loss: 14375.24, base loss: 19777.86
[INFO 2017-06-27 20:32:06,741 main.py:51] epoch 1226, training loss: 13445.80, average training loss: 14374.05, base loss: 19781.14
[INFO 2017-06-27 20:32:07,184 main.py:51] epoch 1227, training loss: 12444.63, average training loss: 14372.47, base loss: 19782.96
[INFO 2017-06-27 20:32:07,667 main.py:51] epoch 1228, training loss: 14336.78, average training loss: 14369.60, base loss: 19782.57
[INFO 2017-06-27 20:32:08,059 main.py:51] epoch 1229, training loss: 12653.90, average training loss: 14367.50, base loss: 19781.54
[INFO 2017-06-27 20:32:08,439 main.py:51] epoch 1230, training loss: 12833.52, average training loss: 14365.98, base loss: 19783.43
[INFO 2017-06-27 20:32:08,816 main.py:51] epoch 1231, training loss: 14410.62, average training loss: 14366.00, base loss: 19787.19
[INFO 2017-06-27 20:32:09,193 main.py:51] epoch 1232, training loss: 13432.43, average training loss: 14363.46, base loss: 19786.25
[INFO 2017-06-27 20:32:09,570 main.py:51] epoch 1233, training loss: 12746.11, average training loss: 14358.35, base loss: 19783.30
[INFO 2017-06-27 20:32:09,952 main.py:51] epoch 1234, training loss: 14023.46, average training loss: 14354.98, base loss: 19781.99
[INFO 2017-06-27 20:32:10,332 main.py:51] epoch 1235, training loss: 15449.67, average training loss: 14356.16, base loss: 19787.71
[INFO 2017-06-27 20:32:10,736 main.py:51] epoch 1236, training loss: 11832.68, average training loss: 14353.30, base loss: 19787.16
[INFO 2017-06-27 20:32:11,119 main.py:51] epoch 1237, training loss: 11682.01, average training loss: 14347.44, base loss: 19782.18
[INFO 2017-06-27 20:32:11,501 main.py:51] epoch 1238, training loss: 14194.82, average training loss: 14342.87, base loss: 19780.63
[INFO 2017-06-27 20:32:11,877 main.py:51] epoch 1239, training loss: 12766.22, average training loss: 14339.49, base loss: 19779.34
[INFO 2017-06-27 20:32:12,253 main.py:51] epoch 1240, training loss: 13257.88, average training loss: 14336.80, base loss: 19779.95
[INFO 2017-06-27 20:32:12,630 main.py:51] epoch 1241, training loss: 13985.13, average training loss: 14337.22, base loss: 19783.36
[INFO 2017-06-27 20:32:13,005 main.py:51] epoch 1242, training loss: 11523.07, average training loss: 14333.31, base loss: 19780.37
[INFO 2017-06-27 20:32:13,380 main.py:51] epoch 1243, training loss: 12838.10, average training loss: 14332.24, base loss: 19784.25
[INFO 2017-06-27 20:32:13,756 main.py:51] epoch 1244, training loss: 14090.56, average training loss: 14330.58, base loss: 19786.02
[INFO 2017-06-27 20:32:14,133 main.py:51] epoch 1245, training loss: 12196.15, average training loss: 14325.60, base loss: 19783.01
[INFO 2017-06-27 20:32:14,508 main.py:51] epoch 1246, training loss: 11477.77, average training loss: 14321.96, base loss: 19779.79
[INFO 2017-06-27 20:32:14,892 main.py:51] epoch 1247, training loss: 11747.57, average training loss: 14318.46, base loss: 19776.84
[INFO 2017-06-27 20:32:15,269 main.py:51] epoch 1248, training loss: 15430.96, average training loss: 14318.82, base loss: 19781.01
[INFO 2017-06-27 20:32:15,646 main.py:51] epoch 1249, training loss: 11543.14, average training loss: 14311.41, base loss: 19773.67
[INFO 2017-06-27 20:32:16,027 main.py:51] epoch 1250, training loss: 13201.73, average training loss: 14306.87, base loss: 19771.23
[INFO 2017-06-27 20:32:16,403 main.py:51] epoch 1251, training loss: 13696.43, average training loss: 14304.04, base loss: 19770.40
[INFO 2017-06-27 20:32:16,783 main.py:51] epoch 1252, training loss: 14193.88, average training loss: 14300.51, base loss: 19768.81
[INFO 2017-06-27 20:32:17,160 main.py:51] epoch 1253, training loss: 13591.11, average training loss: 14298.20, base loss: 19769.82
[INFO 2017-06-27 20:32:17,545 main.py:51] epoch 1254, training loss: 12439.35, average training loss: 14295.64, base loss: 19769.83
[INFO 2017-06-27 20:32:17,929 main.py:51] epoch 1255, training loss: 12952.60, average training loss: 14292.12, base loss: 19767.28
[INFO 2017-06-27 20:32:18,309 main.py:51] epoch 1256, training loss: 13329.28, average training loss: 14289.17, base loss: 19766.26
[INFO 2017-06-27 20:32:18,722 main.py:51] epoch 1257, training loss: 14015.63, average training loss: 14288.52, base loss: 19768.79
[INFO 2017-06-27 20:32:19,125 main.py:51] epoch 1258, training loss: 12778.01, average training loss: 14283.29, base loss: 19766.20
[INFO 2017-06-27 20:32:19,504 main.py:51] epoch 1259, training loss: 13249.85, average training loss: 14279.18, base loss: 19763.77
[INFO 2017-06-27 20:32:19,881 main.py:51] epoch 1260, training loss: 13827.57, average training loss: 14274.94, base loss: 19763.10
[INFO 2017-06-27 20:32:20,273 main.py:51] epoch 1261, training loss: 12186.85, average training loss: 14270.32, base loss: 19759.31
[INFO 2017-06-27 20:32:20,655 main.py:51] epoch 1262, training loss: 13486.59, average training loss: 14269.40, base loss: 19762.27
[INFO 2017-06-27 20:32:21,032 main.py:51] epoch 1263, training loss: 12426.03, average training loss: 14265.10, base loss: 19760.76
[INFO 2017-06-27 20:32:21,415 main.py:51] epoch 1264, training loss: 13118.49, average training loss: 14264.23, base loss: 19762.39
[INFO 2017-06-27 20:32:21,795 main.py:51] epoch 1265, training loss: 13264.52, average training loss: 14263.03, base loss: 19764.25
[INFO 2017-06-27 20:32:22,183 main.py:51] epoch 1266, training loss: 13431.89, average training loss: 14261.06, base loss: 19765.46
[INFO 2017-06-27 20:32:22,561 main.py:51] epoch 1267, training loss: 11742.50, average training loss: 14253.82, base loss: 19757.89
[INFO 2017-06-27 20:32:22,959 main.py:51] epoch 1268, training loss: 14817.89, average training loss: 14253.87, base loss: 19761.65
[INFO 2017-06-27 20:32:23,337 main.py:51] epoch 1269, training loss: 11663.72, average training loss: 14250.34, base loss: 19760.11
[INFO 2017-06-27 20:32:23,715 main.py:51] epoch 1270, training loss: 15687.59, average training loss: 14251.83, base loss: 19766.66
[INFO 2017-06-27 20:32:24,096 main.py:51] epoch 1271, training loss: 13382.70, average training loss: 14249.87, base loss: 19767.63
[INFO 2017-06-27 20:32:24,472 main.py:51] epoch 1272, training loss: 12770.38, average training loss: 14246.34, base loss: 19765.88
[INFO 2017-06-27 20:32:24,850 main.py:51] epoch 1273, training loss: 16611.86, average training loss: 14247.99, base loss: 19772.95
[INFO 2017-06-27 20:32:25,229 main.py:51] epoch 1274, training loss: 11412.15, average training loss: 14244.44, base loss: 19772.24
[INFO 2017-06-27 20:32:25,605 main.py:51] epoch 1275, training loss: 11978.05, average training loss: 14241.53, base loss: 19771.14
[INFO 2017-06-27 20:32:25,980 main.py:51] epoch 1276, training loss: 11648.54, average training loss: 14237.83, base loss: 19769.53
[INFO 2017-06-27 20:32:26,356 main.py:51] epoch 1277, training loss: 14907.73, average training loss: 14238.54, base loss: 19774.13
[INFO 2017-06-27 20:32:26,730 main.py:51] epoch 1278, training loss: 13506.71, average training loss: 14235.32, base loss: 19772.83
[INFO 2017-06-27 20:32:27,104 main.py:51] epoch 1279, training loss: 14079.61, average training loss: 14232.96, base loss: 19773.05
[INFO 2017-06-27 20:32:27,481 main.py:51] epoch 1280, training loss: 13389.00, average training loss: 14230.04, base loss: 19773.76
[INFO 2017-06-27 20:32:27,858 main.py:51] epoch 1281, training loss: 11482.68, average training loss: 14222.33, base loss: 19766.13
[INFO 2017-06-27 20:32:28,234 main.py:51] epoch 1282, training loss: 15266.14, average training loss: 14222.34, base loss: 19770.08
[INFO 2017-06-27 20:32:28,609 main.py:51] epoch 1283, training loss: 12337.18, average training loss: 14217.55, base loss: 19767.37
[INFO 2017-06-27 20:32:28,985 main.py:51] epoch 1284, training loss: 15567.20, average training loss: 14219.07, base loss: 19774.28
[INFO 2017-06-27 20:32:29,368 main.py:51] epoch 1285, training loss: 14191.29, average training loss: 14217.77, base loss: 19778.08
[INFO 2017-06-27 20:32:29,751 main.py:51] epoch 1286, training loss: 12658.14, average training loss: 14214.88, base loss: 19776.27
[INFO 2017-06-27 20:32:30,129 main.py:51] epoch 1287, training loss: 11509.71, average training loss: 14211.58, base loss: 19773.51
[INFO 2017-06-27 20:32:30,594 main.py:51] epoch 1288, training loss: 14869.36, average training loss: 14210.00, base loss: 19774.75
[INFO 2017-06-27 20:32:30,990 main.py:51] epoch 1289, training loss: 10881.08, average training loss: 14204.12, base loss: 19770.85
[INFO 2017-06-27 20:32:31,373 main.py:51] epoch 1290, training loss: 13414.83, average training loss: 14202.48, base loss: 19771.80
[INFO 2017-06-27 20:32:31,760 main.py:51] epoch 1291, training loss: 13200.82, average training loss: 14201.13, base loss: 19773.89
[INFO 2017-06-27 20:32:32,159 main.py:51] epoch 1292, training loss: 14491.73, average training loss: 14199.65, base loss: 19776.48
[INFO 2017-06-27 20:32:32,535 main.py:51] epoch 1293, training loss: 11571.37, average training loss: 14194.16, base loss: 19770.33
[INFO 2017-06-27 20:32:32,914 main.py:51] epoch 1294, training loss: 12242.08, average training loss: 14192.27, base loss: 19770.01
[INFO 2017-06-27 20:32:33,291 main.py:51] epoch 1295, training loss: 13573.94, average training loss: 14191.73, base loss: 19772.53
[INFO 2017-06-27 20:32:33,666 main.py:51] epoch 1296, training loss: 12211.14, average training loss: 14188.06, base loss: 19770.64
[INFO 2017-06-27 20:32:34,041 main.py:51] epoch 1297, training loss: 11094.68, average training loss: 14182.98, base loss: 19766.35
[INFO 2017-06-27 20:32:34,419 main.py:51] epoch 1298, training loss: 13048.35, average training loss: 14180.34, base loss: 19765.14
[INFO 2017-06-27 20:32:34,801 main.py:51] epoch 1299, training loss: 12275.92, average training loss: 14177.21, base loss: 19765.31
[INFO 2017-06-27 20:32:34,801 main.py:53] epoch 1299, testing
[INFO 2017-06-27 20:32:36,419 main.py:105] average testing loss: 12719.19, base loss: 19140.13
[INFO 2017-06-27 20:32:36,419 main.py:106] improve_loss: 6420.94, improve_percent: 0.34
[INFO 2017-06-27 20:32:36,420 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 20:32:36,819 main.py:51] epoch 1300, training loss: 13887.16, average training loss: 14176.02, base loss: 19766.60
[INFO 2017-06-27 20:32:37,195 main.py:51] epoch 1301, training loss: 12592.73, average training loss: 14173.27, base loss: 19766.26
[INFO 2017-06-27 20:32:37,570 main.py:51] epoch 1302, training loss: 12867.40, average training loss: 14169.70, base loss: 19764.51
[INFO 2017-06-27 20:32:37,942 main.py:51] epoch 1303, training loss: 13924.01, average training loss: 14167.75, base loss: 19767.30
[INFO 2017-06-27 20:32:38,319 main.py:51] epoch 1304, training loss: 13744.50, average training loss: 14165.34, base loss: 19767.39
[INFO 2017-06-27 20:32:38,699 main.py:51] epoch 1305, training loss: 14544.48, average training loss: 14163.72, base loss: 19768.98
[INFO 2017-06-27 20:32:39,087 main.py:51] epoch 1306, training loss: 13597.03, average training loss: 14161.58, base loss: 19769.57
[INFO 2017-06-27 20:32:39,539 main.py:51] epoch 1307, training loss: 12040.60, average training loss: 14157.78, base loss: 19766.95
[INFO 2017-06-27 20:32:39,962 main.py:51] epoch 1308, training loss: 11848.20, average training loss: 14153.52, base loss: 19764.31
[INFO 2017-06-27 20:32:40,345 main.py:51] epoch 1309, training loss: 13411.75, average training loss: 14152.11, base loss: 19765.56
[INFO 2017-06-27 20:32:40,724 main.py:51] epoch 1310, training loss: 13090.80, average training loss: 14147.62, base loss: 19761.35
[INFO 2017-06-27 20:32:41,185 main.py:51] epoch 1311, training loss: 12653.79, average training loss: 14144.63, base loss: 19760.27
[INFO 2017-06-27 20:32:41,598 main.py:51] epoch 1312, training loss: 13326.94, average training loss: 14143.42, base loss: 19763.23
[INFO 2017-06-27 20:32:41,979 main.py:51] epoch 1313, training loss: 14537.30, average training loss: 14143.54, base loss: 19767.90
[INFO 2017-06-27 20:32:42,369 main.py:51] epoch 1314, training loss: 12790.24, average training loss: 14143.04, base loss: 19769.61
[INFO 2017-06-27 20:32:42,768 main.py:51] epoch 1315, training loss: 14152.70, average training loss: 14142.11, base loss: 19772.08
[INFO 2017-06-27 20:32:43,151 main.py:51] epoch 1316, training loss: 14197.23, average training loss: 14141.34, base loss: 19774.33
[INFO 2017-06-27 20:32:43,529 main.py:51] epoch 1317, training loss: 13038.19, average training loss: 14139.07, base loss: 19775.87
[INFO 2017-06-27 20:32:43,913 main.py:51] epoch 1318, training loss: 12586.52, average training loss: 14135.34, base loss: 19772.79
[INFO 2017-06-27 20:32:44,290 main.py:51] epoch 1319, training loss: 14497.22, average training loss: 14136.34, base loss: 19777.17
[INFO 2017-06-27 20:32:44,676 main.py:51] epoch 1320, training loss: 12592.77, average training loss: 14134.90, base loss: 19776.60
[INFO 2017-06-27 20:32:45,054 main.py:51] epoch 1321, training loss: 13588.09, average training loss: 14134.76, base loss: 19780.47
[INFO 2017-06-27 20:32:45,431 main.py:51] epoch 1322, training loss: 12383.25, average training loss: 14129.66, base loss: 19775.57
[INFO 2017-06-27 20:32:45,809 main.py:51] epoch 1323, training loss: 13692.33, average training loss: 14128.62, base loss: 19777.01
[INFO 2017-06-27 20:32:46,186 main.py:51] epoch 1324, training loss: 15193.97, average training loss: 14128.41, base loss: 19781.34
[INFO 2017-06-27 20:32:46,570 main.py:51] epoch 1325, training loss: 12696.22, average training loss: 14125.39, base loss: 19778.01
[INFO 2017-06-27 20:32:46,949 main.py:51] epoch 1326, training loss: 13944.62, average training loss: 14123.80, base loss: 19779.26
[INFO 2017-06-27 20:32:47,326 main.py:51] epoch 1327, training loss: 12755.64, average training loss: 14122.20, base loss: 19779.56
[INFO 2017-06-27 20:32:47,709 main.py:51] epoch 1328, training loss: 14386.25, average training loss: 14124.28, base loss: 19787.03
[INFO 2017-06-27 20:32:48,088 main.py:51] epoch 1329, training loss: 13117.12, average training loss: 14121.75, base loss: 19787.28
[INFO 2017-06-27 20:32:48,465 main.py:51] epoch 1330, training loss: 13767.37, average training loss: 14120.75, base loss: 19789.76
[INFO 2017-06-27 20:32:48,848 main.py:51] epoch 1331, training loss: 12890.38, average training loss: 14118.26, base loss: 19789.60
[INFO 2017-06-27 20:32:49,226 main.py:51] epoch 1332, training loss: 14195.78, average training loss: 14117.62, base loss: 19790.77
[INFO 2017-06-27 20:32:49,605 main.py:51] epoch 1333, training loss: 13736.33, average training loss: 14113.21, base loss: 19789.84
[INFO 2017-06-27 20:32:49,989 main.py:51] epoch 1334, training loss: 15442.81, average training loss: 14112.33, base loss: 19793.62
[INFO 2017-06-27 20:32:50,365 main.py:51] epoch 1335, training loss: 11391.25, average training loss: 14108.68, base loss: 19791.80
[INFO 2017-06-27 20:32:50,743 main.py:51] epoch 1336, training loss: 11135.15, average training loss: 14104.09, base loss: 19787.82
[INFO 2017-06-27 20:32:51,121 main.py:51] epoch 1337, training loss: 11954.45, average training loss: 14099.85, base loss: 19785.65
[INFO 2017-06-27 20:32:51,498 main.py:51] epoch 1338, training loss: 12385.36, average training loss: 14095.92, base loss: 19783.41
[INFO 2017-06-27 20:32:51,915 main.py:51] epoch 1339, training loss: 13973.46, average training loss: 14094.70, base loss: 19786.83
[INFO 2017-06-27 20:32:52,321 main.py:51] epoch 1340, training loss: 12978.56, average training loss: 14092.98, base loss: 19787.82
[INFO 2017-06-27 20:32:52,699 main.py:51] epoch 1341, training loss: 13436.13, average training loss: 14089.58, base loss: 19787.12
[INFO 2017-06-27 20:32:53,082 main.py:51] epoch 1342, training loss: 15745.38, average training loss: 14091.29, base loss: 19793.97
[INFO 2017-06-27 20:32:53,460 main.py:51] epoch 1343, training loss: 12752.89, average training loss: 14088.23, base loss: 19794.22
[INFO 2017-06-27 20:32:53,837 main.py:51] epoch 1344, training loss: 11458.96, average training loss: 14082.14, base loss: 19787.70
[INFO 2017-06-27 20:32:54,242 main.py:51] epoch 1345, training loss: 13575.35, average training loss: 14080.09, base loss: 19789.38
[INFO 2017-06-27 20:32:54,617 main.py:51] epoch 1346, training loss: 12834.74, average training loss: 14076.86, base loss: 19787.50
[INFO 2017-06-27 20:32:54,996 main.py:51] epoch 1347, training loss: 12974.00, average training loss: 14069.09, base loss: 19780.79
[INFO 2017-06-27 20:32:55,378 main.py:51] epoch 1348, training loss: 14135.16, average training loss: 14068.78, base loss: 19783.89
[INFO 2017-06-27 20:32:55,753 main.py:51] epoch 1349, training loss: 12380.59, average training loss: 14065.85, base loss: 19783.13
[INFO 2017-06-27 20:32:56,131 main.py:51] epoch 1350, training loss: 12778.91, average training loss: 14064.06, base loss: 19784.66
[INFO 2017-06-27 20:32:56,510 main.py:51] epoch 1351, training loss: 12274.02, average training loss: 14062.72, base loss: 19786.01
[INFO 2017-06-27 20:32:56,887 main.py:51] epoch 1352, training loss: 12774.23, average training loss: 14058.22, base loss: 19783.78
[INFO 2017-06-27 20:32:57,265 main.py:51] epoch 1353, training loss: 12278.69, average training loss: 14057.74, base loss: 19786.11
[INFO 2017-06-27 20:32:57,639 main.py:51] epoch 1354, training loss: 12418.34, average training loss: 14055.05, base loss: 19784.62
[INFO 2017-06-27 20:32:58,017 main.py:51] epoch 1355, training loss: 13818.34, average training loss: 14053.31, base loss: 19785.79
[INFO 2017-06-27 20:32:58,393 main.py:51] epoch 1356, training loss: 11886.63, average training loss: 14051.48, base loss: 19786.44
[INFO 2017-06-27 20:32:58,769 main.py:51] epoch 1357, training loss: 12781.34, average training loss: 14049.15, base loss: 19787.69
[INFO 2017-06-27 20:32:59,144 main.py:51] epoch 1358, training loss: 13414.87, average training loss: 14045.69, base loss: 19787.03
[INFO 2017-06-27 20:32:59,521 main.py:51] epoch 1359, training loss: 12848.60, average training loss: 14041.26, base loss: 19782.94
[INFO 2017-06-27 20:32:59,895 main.py:51] epoch 1360, training loss: 13767.96, average training loss: 14040.82, base loss: 19786.36
[INFO 2017-06-27 20:33:00,272 main.py:51] epoch 1361, training loss: 12584.38, average training loss: 14036.94, base loss: 19782.89
[INFO 2017-06-27 20:33:00,650 main.py:51] epoch 1362, training loss: 12611.89, average training loss: 14034.76, base loss: 19783.44
[INFO 2017-06-27 20:33:01,037 main.py:51] epoch 1363, training loss: 16014.25, average training loss: 14036.29, base loss: 19790.35
[INFO 2017-06-27 20:33:01,414 main.py:51] epoch 1364, training loss: 13675.99, average training loss: 14034.59, base loss: 19791.44
[INFO 2017-06-27 20:33:01,785 main.py:51] epoch 1365, training loss: 12936.31, average training loss: 14030.22, base loss: 19789.50
[INFO 2017-06-27 20:33:02,258 main.py:51] epoch 1366, training loss: 13245.04, average training loss: 14028.19, base loss: 19789.67
[INFO 2017-06-27 20:33:02,643 main.py:51] epoch 1367, training loss: 13387.89, average training loss: 14025.09, base loss: 19789.04
[INFO 2017-06-27 20:33:03,020 main.py:51] epoch 1368, training loss: 12183.29, average training loss: 14020.74, base loss: 19786.39
[INFO 2017-06-27 20:33:03,411 main.py:51] epoch 1369, training loss: 14022.04, average training loss: 14019.42, base loss: 19788.33
[INFO 2017-06-27 20:33:03,804 main.py:51] epoch 1370, training loss: 12566.64, average training loss: 14015.12, base loss: 19785.71
[INFO 2017-06-27 20:33:04,182 main.py:51] epoch 1371, training loss: 12980.89, average training loss: 14012.50, base loss: 19784.33
[INFO 2017-06-27 20:33:04,561 main.py:51] epoch 1372, training loss: 14281.55, average training loss: 14012.58, base loss: 19787.75
[INFO 2017-06-27 20:33:04,941 main.py:51] epoch 1373, training loss: 13315.87, average training loss: 14011.34, base loss: 19789.64
[INFO 2017-06-27 20:33:05,312 main.py:51] epoch 1374, training loss: 12887.04, average training loss: 14008.76, base loss: 19789.50
[INFO 2017-06-27 20:33:05,686 main.py:51] epoch 1375, training loss: 10977.55, average training loss: 14005.07, base loss: 19788.19
[INFO 2017-06-27 20:33:06,058 main.py:51] epoch 1376, training loss: 12273.07, average training loss: 14001.88, base loss: 19786.40
[INFO 2017-06-27 20:33:06,436 main.py:51] epoch 1377, training loss: 11679.38, average training loss: 13997.04, base loss: 19782.37
[INFO 2017-06-27 20:33:06,811 main.py:51] epoch 1378, training loss: 13376.68, average training loss: 13994.07, base loss: 19781.70
[INFO 2017-06-27 20:33:07,190 main.py:51] epoch 1379, training loss: 12854.44, average training loss: 13991.11, base loss: 19780.96
[INFO 2017-06-27 20:33:07,561 main.py:51] epoch 1380, training loss: 11224.49, average training loss: 13986.88, base loss: 19776.97
[INFO 2017-06-27 20:33:07,937 main.py:51] epoch 1381, training loss: 14345.50, average training loss: 13987.60, base loss: 19781.77
[INFO 2017-06-27 20:33:08,315 main.py:51] epoch 1382, training loss: 13125.41, average training loss: 13984.72, base loss: 19781.65
[INFO 2017-06-27 20:33:08,690 main.py:51] epoch 1383, training loss: 10051.50, average training loss: 13979.76, base loss: 19775.45
[INFO 2017-06-27 20:33:09,063 main.py:51] epoch 1384, training loss: 10996.55, average training loss: 13977.42, base loss: 19773.64
[INFO 2017-06-27 20:33:09,441 main.py:51] epoch 1385, training loss: 13010.70, average training loss: 13974.71, base loss: 19773.77
[INFO 2017-06-27 20:33:09,817 main.py:51] epoch 1386, training loss: 13580.67, average training loss: 13972.45, base loss: 19773.44
[INFO 2017-06-27 20:33:10,194 main.py:51] epoch 1387, training loss: 12916.82, average training loss: 13968.92, base loss: 19770.75
[INFO 2017-06-27 20:33:10,575 main.py:51] epoch 1388, training loss: 11956.19, average training loss: 13967.78, base loss: 19771.68
[INFO 2017-06-27 20:33:10,951 main.py:51] epoch 1389, training loss: 11642.07, average training loss: 13963.23, base loss: 19767.42
[INFO 2017-06-27 20:33:11,332 main.py:51] epoch 1390, training loss: 13072.32, average training loss: 13961.62, base loss: 19768.99
[INFO 2017-06-27 20:33:11,705 main.py:51] epoch 1391, training loss: 11992.59, average training loss: 13957.00, base loss: 19765.56
[INFO 2017-06-27 20:33:12,081 main.py:51] epoch 1392, training loss: 12726.53, average training loss: 13956.61, base loss: 19767.91
[INFO 2017-06-27 20:33:12,457 main.py:51] epoch 1393, training loss: 13226.83, average training loss: 13955.88, base loss: 19770.98
[INFO 2017-06-27 20:33:12,835 main.py:51] epoch 1394, training loss: 13024.30, average training loss: 13954.86, base loss: 19771.32
[INFO 2017-06-27 20:33:13,212 main.py:51] epoch 1395, training loss: 11719.10, average training loss: 13950.11, base loss: 19765.65
[INFO 2017-06-27 20:33:13,589 main.py:51] epoch 1396, training loss: 13434.23, average training loss: 13949.77, base loss: 19768.54
[INFO 2017-06-27 20:33:13,973 main.py:51] epoch 1397, training loss: 11876.43, average training loss: 13946.13, base loss: 19766.73
[INFO 2017-06-27 20:33:14,347 main.py:51] epoch 1398, training loss: 13019.14, average training loss: 13942.14, base loss: 19763.38
[INFO 2017-06-27 20:33:14,723 main.py:51] epoch 1399, training loss: 13136.02, average training loss: 13939.51, base loss: 19761.81
[INFO 2017-06-27 20:33:14,723 main.py:53] epoch 1399, testing
[INFO 2017-06-27 20:33:16,332 main.py:105] average testing loss: 12437.56, base loss: 18975.82
[INFO 2017-06-27 20:33:16,332 main.py:106] improve_loss: 6538.26, improve_percent: 0.34
[INFO 2017-06-27 20:33:16,332 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:33:16,347 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 20:33:16,724 main.py:51] epoch 1400, training loss: 13063.52, average training loss: 13936.42, base loss: 19759.32
[INFO 2017-06-27 20:33:17,100 main.py:51] epoch 1401, training loss: 11458.70, average training loss: 13934.38, base loss: 19760.47
[INFO 2017-06-27 20:33:17,480 main.py:51] epoch 1402, training loss: 14726.27, average training loss: 13932.12, base loss: 19761.03
[INFO 2017-06-27 20:33:17,854 main.py:51] epoch 1403, training loss: 12234.83, average training loss: 13930.93, base loss: 19760.64
[INFO 2017-06-27 20:33:18,232 main.py:51] epoch 1404, training loss: 13606.76, average training loss: 13930.40, base loss: 19763.64
[INFO 2017-06-27 20:33:18,607 main.py:51] epoch 1405, training loss: 13856.55, average training loss: 13927.15, base loss: 19764.01
[INFO 2017-06-27 20:33:18,986 main.py:51] epoch 1406, training loss: 13330.73, average training loss: 13924.16, base loss: 19762.81
[INFO 2017-06-27 20:33:19,364 main.py:51] epoch 1407, training loss: 12126.70, average training loss: 13920.96, base loss: 19761.35
[INFO 2017-06-27 20:33:19,737 main.py:51] epoch 1408, training loss: 12861.84, average training loss: 13918.77, base loss: 19762.02
[INFO 2017-06-27 20:33:20,115 main.py:51] epoch 1409, training loss: 11840.56, average training loss: 13915.53, base loss: 19759.59
[INFO 2017-06-27 20:33:20,496 main.py:51] epoch 1410, training loss: 13200.64, average training loss: 13913.40, base loss: 19759.16
[INFO 2017-06-27 20:33:20,873 main.py:51] epoch 1411, training loss: 14203.44, average training loss: 13913.43, base loss: 19763.13
[INFO 2017-06-27 20:33:21,246 main.py:51] epoch 1412, training loss: 13772.22, average training loss: 13913.81, base loss: 19767.28
[INFO 2017-06-27 20:33:21,622 main.py:51] epoch 1413, training loss: 14222.54, average training loss: 13914.11, base loss: 19771.37
[INFO 2017-06-27 20:33:22,002 main.py:51] epoch 1414, training loss: 10766.93, average training loss: 13910.31, base loss: 19766.31
[INFO 2017-06-27 20:33:22,381 main.py:51] epoch 1415, training loss: 11238.38, average training loss: 13907.97, base loss: 19766.03
[INFO 2017-06-27 20:33:22,757 main.py:51] epoch 1416, training loss: 15223.33, average training loss: 13907.95, base loss: 19769.31
[INFO 2017-06-27 20:33:23,135 main.py:51] epoch 1417, training loss: 12738.36, average training loss: 13906.40, base loss: 19768.81
[INFO 2017-06-27 20:33:23,512 main.py:51] epoch 1418, training loss: 12227.44, average training loss: 13902.98, base loss: 19764.11
[INFO 2017-06-27 20:33:23,891 main.py:51] epoch 1419, training loss: 13041.28, average training loss: 13900.91, base loss: 19763.33
[INFO 2017-06-27 20:33:24,268 main.py:51] epoch 1420, training loss: 10321.98, average training loss: 13896.23, base loss: 19759.64
[INFO 2017-06-27 20:33:24,642 main.py:51] epoch 1421, training loss: 13445.44, average training loss: 13894.92, base loss: 19760.92
[INFO 2017-06-27 20:33:25,035 main.py:51] epoch 1422, training loss: 14082.12, average training loss: 13891.26, base loss: 19758.60
[INFO 2017-06-27 20:33:25,413 main.py:51] epoch 1423, training loss: 13351.63, average training loss: 13888.29, base loss: 19757.45
[INFO 2017-06-27 20:33:25,801 main.py:51] epoch 1424, training loss: 12288.71, average training loss: 13885.16, base loss: 19755.25
[INFO 2017-06-27 20:33:26,179 main.py:51] epoch 1425, training loss: 11851.40, average training loss: 13878.02, base loss: 19747.27
[INFO 2017-06-27 20:33:26,557 main.py:51] epoch 1426, training loss: 12786.91, average training loss: 13875.48, base loss: 19746.72
[INFO 2017-06-27 20:33:26,933 main.py:51] epoch 1427, training loss: 13885.60, average training loss: 13871.35, base loss: 19746.00
[INFO 2017-06-27 20:33:27,310 main.py:51] epoch 1428, training loss: 14500.45, average training loss: 13870.29, base loss: 19747.28
[INFO 2017-06-27 20:33:27,684 main.py:51] epoch 1429, training loss: 14666.31, average training loss: 13869.97, base loss: 19751.45
[INFO 2017-06-27 20:33:28,062 main.py:51] epoch 1430, training loss: 13475.64, average training loss: 13869.59, base loss: 19752.97
[INFO 2017-06-27 20:33:28,436 main.py:51] epoch 1431, training loss: 12548.48, average training loss: 13868.36, base loss: 19753.37
[INFO 2017-06-27 20:33:28,815 main.py:51] epoch 1432, training loss: 11722.19, average training loss: 13865.50, base loss: 19749.82
[INFO 2017-06-27 20:33:29,198 main.py:51] epoch 1433, training loss: 11846.07, average training loss: 13862.32, base loss: 19748.16
[INFO 2017-06-27 20:33:29,579 main.py:51] epoch 1434, training loss: 12204.36, average training loss: 13857.48, base loss: 19743.66
[INFO 2017-06-27 20:33:30,060 main.py:51] epoch 1435, training loss: 12534.66, average training loss: 13852.33, base loss: 19738.74
[INFO 2017-06-27 20:33:30,451 main.py:51] epoch 1436, training loss: 13015.73, average training loss: 13850.87, base loss: 19740.01
[INFO 2017-06-27 20:33:30,848 main.py:51] epoch 1437, training loss: 13289.68, average training loss: 13850.59, base loss: 19743.07
[INFO 2017-06-27 20:33:31,245 main.py:51] epoch 1438, training loss: 12372.02, average training loss: 13847.29, base loss: 19742.05
[INFO 2017-06-27 20:33:31,629 main.py:51] epoch 1439, training loss: 12242.17, average training loss: 13843.29, base loss: 19738.45
[INFO 2017-06-27 20:33:32,008 main.py:51] epoch 1440, training loss: 14622.19, average training loss: 13843.47, base loss: 19741.73
[INFO 2017-06-27 20:33:32,387 main.py:51] epoch 1441, training loss: 13555.86, average training loss: 13842.24, base loss: 19743.76
[INFO 2017-06-27 20:33:32,760 main.py:51] epoch 1442, training loss: 16085.05, average training loss: 13844.54, base loss: 19751.26
[INFO 2017-06-27 20:33:33,138 main.py:51] epoch 1443, training loss: 12730.78, average training loss: 13842.27, base loss: 19751.11
[INFO 2017-06-27 20:33:33,516 main.py:51] epoch 1444, training loss: 13000.87, average training loss: 13841.04, base loss: 19751.59
[INFO 2017-06-27 20:33:33,891 main.py:51] epoch 1445, training loss: 12215.61, average training loss: 13837.85, base loss: 19748.95
[INFO 2017-06-27 20:33:34,273 main.py:51] epoch 1446, training loss: 12056.12, average training loss: 13836.83, base loss: 19749.25
[INFO 2017-06-27 20:33:34,647 main.py:51] epoch 1447, training loss: 12935.34, average training loss: 13834.02, base loss: 19749.26
[INFO 2017-06-27 20:33:35,018 main.py:51] epoch 1448, training loss: 13518.00, average training loss: 13832.18, base loss: 19749.02
[INFO 2017-06-27 20:33:35,394 main.py:51] epoch 1449, training loss: 14450.26, average training loss: 13828.57, base loss: 19745.06
[INFO 2017-06-27 20:33:35,772 main.py:51] epoch 1450, training loss: 12907.54, average training loss: 13825.31, base loss: 19743.63
[INFO 2017-06-27 20:33:36,148 main.py:51] epoch 1451, training loss: 12559.14, average training loss: 13823.86, base loss: 19744.64
[INFO 2017-06-27 20:33:36,526 main.py:51] epoch 1452, training loss: 12344.39, average training loss: 13820.11, base loss: 19741.40
[INFO 2017-06-27 20:33:36,907 main.py:51] epoch 1453, training loss: 13029.04, average training loss: 13818.40, base loss: 19741.80
[INFO 2017-06-27 20:33:37,283 main.py:51] epoch 1454, training loss: 12243.63, average training loss: 13814.72, base loss: 19738.31
[INFO 2017-06-27 20:33:37,654 main.py:51] epoch 1455, training loss: 14349.56, average training loss: 13815.06, base loss: 19742.09
[INFO 2017-06-27 20:33:38,033 main.py:51] epoch 1456, training loss: 12499.43, average training loss: 13812.42, base loss: 19740.69
[INFO 2017-06-27 20:33:38,406 main.py:51] epoch 1457, training loss: 13239.14, average training loss: 13811.57, base loss: 19742.03
[INFO 2017-06-27 20:33:38,784 main.py:51] epoch 1458, training loss: 10851.59, average training loss: 13808.85, base loss: 19740.74
[INFO 2017-06-27 20:33:39,165 main.py:51] epoch 1459, training loss: 11128.69, average training loss: 13806.41, base loss: 19740.63
[INFO 2017-06-27 20:33:39,551 main.py:51] epoch 1460, training loss: 12214.75, average training loss: 13804.24, base loss: 19740.27
[INFO 2017-06-27 20:33:39,930 main.py:51] epoch 1461, training loss: 12983.38, average training loss: 13801.09, base loss: 19737.68
[INFO 2017-06-27 20:33:40,307 main.py:51] epoch 1462, training loss: 11637.30, average training loss: 13798.44, base loss: 19736.86
[INFO 2017-06-27 20:33:40,686 main.py:51] epoch 1463, training loss: 14355.24, average training loss: 13798.55, base loss: 19738.95
[INFO 2017-06-27 20:33:41,064 main.py:51] epoch 1464, training loss: 16258.78, average training loss: 13800.50, base loss: 19746.57
[INFO 2017-06-27 20:33:41,441 main.py:51] epoch 1465, training loss: 12615.58, average training loss: 13797.46, base loss: 19745.08
[INFO 2017-06-27 20:33:41,815 main.py:51] epoch 1466, training loss: 12303.37, average training loss: 13794.46, base loss: 19743.33
[INFO 2017-06-27 20:33:42,194 main.py:51] epoch 1467, training loss: 11721.15, average training loss: 13792.65, base loss: 19743.38
[INFO 2017-06-27 20:33:42,573 main.py:51] epoch 1468, training loss: 12681.31, average training loss: 13791.84, base loss: 19745.16
[INFO 2017-06-27 20:33:42,952 main.py:51] epoch 1469, training loss: 13701.84, average training loss: 13789.25, base loss: 19745.03
[INFO 2017-06-27 20:33:43,329 main.py:51] epoch 1470, training loss: 14416.58, average training loss: 13786.85, base loss: 19744.97
[INFO 2017-06-27 20:33:43,701 main.py:51] epoch 1471, training loss: 12313.09, average training loss: 13782.72, base loss: 19741.42
[INFO 2017-06-27 20:33:44,084 main.py:51] epoch 1472, training loss: 11799.08, average training loss: 13782.58, base loss: 19741.81
[INFO 2017-06-27 20:33:44,457 main.py:51] epoch 1473, training loss: 13493.96, average training loss: 13780.88, base loss: 19742.23
[INFO 2017-06-27 20:33:44,833 main.py:51] epoch 1474, training loss: 12997.82, average training loss: 13778.57, base loss: 19741.60
[INFO 2017-06-27 20:33:45,209 main.py:51] epoch 1475, training loss: 12223.33, average training loss: 13776.98, base loss: 19742.32
[INFO 2017-06-27 20:33:45,590 main.py:51] epoch 1476, training loss: 11931.48, average training loss: 13773.71, base loss: 19741.77
[INFO 2017-06-27 20:33:45,966 main.py:51] epoch 1477, training loss: 11546.32, average training loss: 13770.14, base loss: 19740.50
[INFO 2017-06-27 20:33:46,341 main.py:51] epoch 1478, training loss: 13807.07, average training loss: 13768.99, base loss: 19741.86
[INFO 2017-06-27 20:33:46,717 main.py:51] epoch 1479, training loss: 14021.16, average training loss: 13766.01, base loss: 19740.55
[INFO 2017-06-27 20:33:47,090 main.py:51] epoch 1480, training loss: 13369.14, average training loss: 13763.34, base loss: 19739.68
[INFO 2017-06-27 20:33:47,469 main.py:51] epoch 1481, training loss: 12537.41, average training loss: 13761.51, base loss: 19738.48
[INFO 2017-06-27 20:33:47,845 main.py:51] epoch 1482, training loss: 11338.75, average training loss: 13758.46, base loss: 19736.85
[INFO 2017-06-27 20:33:48,224 main.py:51] epoch 1483, training loss: 13036.58, average training loss: 13757.93, base loss: 19739.88
[INFO 2017-06-27 20:33:48,600 main.py:51] epoch 1484, training loss: 12088.44, average training loss: 13754.71, base loss: 19738.58
[INFO 2017-06-27 20:33:48,977 main.py:51] epoch 1485, training loss: 14265.51, average training loss: 13754.82, base loss: 19740.73
[INFO 2017-06-27 20:33:49,350 main.py:51] epoch 1486, training loss: 11736.62, average training loss: 13749.12, base loss: 19736.85
[INFO 2017-06-27 20:33:49,729 main.py:51] epoch 1487, training loss: 13821.65, average training loss: 13748.43, base loss: 19739.12
[INFO 2017-06-27 20:33:50,105 main.py:51] epoch 1488, training loss: 15009.53, average training loss: 13749.33, base loss: 19743.77
[INFO 2017-06-27 20:33:50,481 main.py:51] epoch 1489, training loss: 11747.21, average training loss: 13747.81, base loss: 19743.47
[INFO 2017-06-27 20:33:50,858 main.py:51] epoch 1490, training loss: 11756.91, average training loss: 13743.74, base loss: 19739.55
[INFO 2017-06-27 20:33:51,235 main.py:51] epoch 1491, training loss: 12278.62, average training loss: 13743.66, base loss: 19744.37
[INFO 2017-06-27 20:33:51,611 main.py:51] epoch 1492, training loss: 12403.47, average training loss: 13741.91, base loss: 19743.34
[INFO 2017-06-27 20:33:51,989 main.py:51] epoch 1493, training loss: 12384.03, average training loss: 13738.33, base loss: 19740.68
[INFO 2017-06-27 20:33:52,366 main.py:51] epoch 1494, training loss: 13649.79, average training loss: 13737.87, base loss: 19742.70
[INFO 2017-06-27 20:33:52,743 main.py:51] epoch 1495, training loss: 14474.91, average training loss: 13737.32, base loss: 19744.89
[INFO 2017-06-27 20:33:53,116 main.py:51] epoch 1496, training loss: 13849.57, average training loss: 13737.11, base loss: 19749.06
[INFO 2017-06-27 20:33:53,493 main.py:51] epoch 1497, training loss: 13623.89, average training loss: 13735.98, base loss: 19749.14
[INFO 2017-06-27 20:33:53,869 main.py:51] epoch 1498, training loss: 13579.96, average training loss: 13732.71, base loss: 19746.38
[INFO 2017-06-27 20:33:54,250 main.py:51] epoch 1499, training loss: 14036.18, average training loss: 13731.74, base loss: 19747.57
[INFO 2017-06-27 20:33:54,251 main.py:53] epoch 1499, testing
[INFO 2017-06-27 20:33:55,858 main.py:105] average testing loss: 13032.72, base loss: 20267.08
[INFO 2017-06-27 20:33:55,858 main.py:106] improve_loss: 7234.36, improve_percent: 0.36
[INFO 2017-06-27 20:33:55,858 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:33:55,871 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 20:33:56,246 main.py:51] epoch 1500, training loss: 12200.73, average training loss: 13729.03, base loss: 19747.37
[INFO 2017-06-27 20:33:56,621 main.py:51] epoch 1501, training loss: 12357.51, average training loss: 13725.71, base loss: 19743.74
[INFO 2017-06-27 20:33:56,997 main.py:51] epoch 1502, training loss: 12620.55, average training loss: 13723.17, base loss: 19741.13
[INFO 2017-06-27 20:33:57,375 main.py:51] epoch 1503, training loss: 13558.45, average training loss: 13723.21, base loss: 19745.82
[INFO 2017-06-27 20:33:57,752 main.py:51] epoch 1504, training loss: 12627.77, average training loss: 13721.09, base loss: 19746.09
[INFO 2017-06-27 20:33:58,130 main.py:51] epoch 1505, training loss: 13418.74, average training loss: 13719.53, base loss: 19748.36
[INFO 2017-06-27 20:33:58,506 main.py:51] epoch 1506, training loss: 11071.68, average training loss: 13714.49, base loss: 19742.69
[INFO 2017-06-27 20:33:58,885 main.py:51] epoch 1507, training loss: 11570.50, average training loss: 13710.70, base loss: 19739.26
[INFO 2017-06-27 20:33:59,261 main.py:51] epoch 1508, training loss: 11323.95, average training loss: 13706.09, base loss: 19734.87
[INFO 2017-06-27 20:33:59,636 main.py:51] epoch 1509, training loss: 12288.39, average training loss: 13703.94, base loss: 19734.18
[INFO 2017-06-27 20:34:00,014 main.py:51] epoch 1510, training loss: 12142.33, average training loss: 13700.87, base loss: 19730.89
[INFO 2017-06-27 20:34:00,389 main.py:51] epoch 1511, training loss: 13597.13, average training loss: 13699.99, base loss: 19732.43
[INFO 2017-06-27 20:34:00,763 main.py:51] epoch 1512, training loss: 13451.65, average training loss: 13699.48, base loss: 19735.46
[INFO 2017-06-27 20:34:01,134 main.py:51] epoch 1513, training loss: 9848.17, average training loss: 13694.92, base loss: 19729.93
[INFO 2017-06-27 20:34:01,511 main.py:51] epoch 1514, training loss: 13598.90, average training loss: 13695.27, base loss: 19734.63
[INFO 2017-06-27 20:34:01,887 main.py:51] epoch 1515, training loss: 12956.79, average training loss: 13694.25, base loss: 19736.64
[INFO 2017-06-27 20:34:02,258 main.py:51] epoch 1516, training loss: 12983.67, average training loss: 13694.49, base loss: 19740.76
[INFO 2017-06-27 20:34:02,636 main.py:51] epoch 1517, training loss: 12078.99, average training loss: 13692.66, base loss: 19740.17
[INFO 2017-06-27 20:34:03,008 main.py:51] epoch 1518, training loss: 14396.11, average training loss: 13691.30, base loss: 19742.57
[INFO 2017-06-27 20:34:03,391 main.py:51] epoch 1519, training loss: 10940.77, average training loss: 13686.29, base loss: 19737.39
[INFO 2017-06-27 20:34:03,764 main.py:51] epoch 1520, training loss: 13383.53, average training loss: 13685.93, base loss: 19740.77
[INFO 2017-06-27 20:34:04,139 main.py:51] epoch 1521, training loss: 12342.38, average training loss: 13684.11, base loss: 19741.11
[INFO 2017-06-27 20:34:04,516 main.py:51] epoch 1522, training loss: 12298.80, average training loss: 13678.80, base loss: 19735.57
[INFO 2017-06-27 20:34:04,892 main.py:51] epoch 1523, training loss: 12291.71, average training loss: 13675.12, base loss: 19733.72
[INFO 2017-06-27 20:34:05,268 main.py:51] epoch 1524, training loss: 12284.96, average training loss: 13671.07, base loss: 19732.86
[INFO 2017-06-27 20:34:05,643 main.py:51] epoch 1525, training loss: 14966.58, average training loss: 13671.16, base loss: 19736.17
[INFO 2017-06-27 20:34:06,020 main.py:51] epoch 1526, training loss: 13189.76, average training loss: 13670.82, base loss: 19738.38
[INFO 2017-06-27 20:34:06,397 main.py:51] epoch 1527, training loss: 12164.12, average training loss: 13668.21, base loss: 19736.92
[INFO 2017-06-27 20:34:06,773 main.py:51] epoch 1528, training loss: 13141.25, average training loss: 13665.73, base loss: 19734.39
[INFO 2017-06-27 20:34:07,153 main.py:51] epoch 1529, training loss: 13668.10, average training loss: 13665.09, base loss: 19736.77
[INFO 2017-06-27 20:34:07,533 main.py:51] epoch 1530, training loss: 12622.67, average training loss: 13663.79, base loss: 19738.35
[INFO 2017-06-27 20:34:07,910 main.py:51] epoch 1531, training loss: 11104.58, average training loss: 13660.80, base loss: 19737.39
[INFO 2017-06-27 20:34:08,286 main.py:51] epoch 1532, training loss: 12042.72, average training loss: 13656.21, base loss: 19733.23
[INFO 2017-06-27 20:34:08,657 main.py:51] epoch 1533, training loss: 10781.68, average training loss: 13651.33, base loss: 19729.04
[INFO 2017-06-27 20:34:09,045 main.py:51] epoch 1534, training loss: 14053.74, average training loss: 13650.03, base loss: 19729.29
[INFO 2017-06-27 20:34:09,420 main.py:51] epoch 1535, training loss: 12668.18, average training loss: 13647.05, base loss: 19727.86
[INFO 2017-06-27 20:34:09,795 main.py:51] epoch 1536, training loss: 11902.76, average training loss: 13643.68, base loss: 19725.54
[INFO 2017-06-27 20:34:10,175 main.py:51] epoch 1537, training loss: 13111.56, average training loss: 13643.50, base loss: 19727.81
[INFO 2017-06-27 20:34:10,552 main.py:51] epoch 1538, training loss: 12673.37, average training loss: 13642.04, base loss: 19728.58
[INFO 2017-06-27 20:34:10,926 main.py:51] epoch 1539, training loss: 11394.65, average training loss: 13638.21, base loss: 19726.43
[INFO 2017-06-27 20:34:11,305 main.py:51] epoch 1540, training loss: 13401.50, average training loss: 13637.17, base loss: 19728.15
[INFO 2017-06-27 20:34:11,684 main.py:51] epoch 1541, training loss: 12324.97, average training loss: 13635.77, base loss: 19727.56
[INFO 2017-06-27 20:34:12,096 main.py:51] epoch 1542, training loss: 12804.88, average training loss: 13633.91, base loss: 19728.43
[INFO 2017-06-27 20:34:12,474 main.py:51] epoch 1543, training loss: 13923.31, average training loss: 13629.49, base loss: 19724.82
[INFO 2017-06-27 20:34:12,852 main.py:51] epoch 1544, training loss: 11457.60, average training loss: 13626.64, base loss: 19723.65
[INFO 2017-06-27 20:34:13,236 main.py:51] epoch 1545, training loss: 11435.03, average training loss: 13624.48, base loss: 19721.90
[INFO 2017-06-27 20:34:13,643 main.py:51] epoch 1546, training loss: 11829.81, average training loss: 13622.63, base loss: 19722.23
[INFO 2017-06-27 20:34:14,045 main.py:51] epoch 1547, training loss: 13155.73, average training loss: 13621.20, base loss: 19722.61
[INFO 2017-06-27 20:34:14,449 main.py:51] epoch 1548, training loss: 14328.50, average training loss: 13622.72, base loss: 19727.86
[INFO 2017-06-27 20:34:14,831 main.py:51] epoch 1549, training loss: 11835.71, average training loss: 13620.72, base loss: 19728.07
[INFO 2017-06-27 20:34:15,213 main.py:51] epoch 1550, training loss: 11516.72, average training loss: 13617.97, base loss: 19725.84
[INFO 2017-06-27 20:34:15,597 main.py:51] epoch 1551, training loss: 12636.65, average training loss: 13616.64, base loss: 19725.46
[INFO 2017-06-27 20:34:16,005 main.py:51] epoch 1552, training loss: 11049.23, average training loss: 13611.39, base loss: 19721.25
[INFO 2017-06-27 20:34:16,512 main.py:51] epoch 1553, training loss: 11260.29, average training loss: 13607.97, base loss: 19720.01
[INFO 2017-06-27 20:34:16,953 main.py:51] epoch 1554, training loss: 11554.38, average training loss: 13605.21, base loss: 19718.49
[INFO 2017-06-27 20:34:17,397 main.py:51] epoch 1555, training loss: 11584.74, average training loss: 13602.35, base loss: 19716.89
[INFO 2017-06-27 20:34:17,839 main.py:51] epoch 1556, training loss: 11251.80, average training loss: 13598.98, base loss: 19714.97
[INFO 2017-06-27 20:34:18,264 main.py:51] epoch 1557, training loss: 12256.30, average training loss: 13597.33, base loss: 19715.16
[INFO 2017-06-27 20:34:18,661 main.py:51] epoch 1558, training loss: 12861.27, average training loss: 13594.78, base loss: 19714.63
[INFO 2017-06-27 20:34:19,127 main.py:51] epoch 1559, training loss: 11770.59, average training loss: 13589.34, base loss: 19708.85
[INFO 2017-06-27 20:34:19,590 main.py:51] epoch 1560, training loss: 13983.98, average training loss: 13590.48, base loss: 19713.23
[INFO 2017-06-27 20:34:19,979 main.py:51] epoch 1561, training loss: 12904.71, average training loss: 13588.04, base loss: 19712.02
[INFO 2017-06-27 20:34:20,359 main.py:51] epoch 1562, training loss: 12889.01, average training loss: 13587.93, base loss: 19714.11
[INFO 2017-06-27 20:34:20,739 main.py:51] epoch 1563, training loss: 11456.80, average training loss: 13584.41, base loss: 19711.22
[INFO 2017-06-27 20:34:21,115 main.py:51] epoch 1564, training loss: 12152.82, average training loss: 13582.55, base loss: 19710.38
[INFO 2017-06-27 20:34:21,509 main.py:51] epoch 1565, training loss: 12342.72, average training loss: 13580.81, base loss: 19708.72
[INFO 2017-06-27 20:34:21,890 main.py:51] epoch 1566, training loss: 12127.33, average training loss: 13577.85, base loss: 19706.46
[INFO 2017-06-27 20:34:22,267 main.py:51] epoch 1567, training loss: 11568.47, average training loss: 13576.01, base loss: 19706.19
[INFO 2017-06-27 20:34:22,650 main.py:51] epoch 1568, training loss: 12920.98, average training loss: 13573.84, base loss: 19705.64
[INFO 2017-06-27 20:34:23,025 main.py:51] epoch 1569, training loss: 11510.51, average training loss: 13572.40, base loss: 19705.73
[INFO 2017-06-27 20:34:23,406 main.py:51] epoch 1570, training loss: 12207.41, average training loss: 13568.86, base loss: 19701.17
[INFO 2017-06-27 20:34:23,789 main.py:51] epoch 1571, training loss: 14608.69, average training loss: 13571.19, base loss: 19705.85
[INFO 2017-06-27 20:34:24,173 main.py:51] epoch 1572, training loss: 12035.00, average training loss: 13567.41, base loss: 19704.49
[INFO 2017-06-27 20:34:24,556 main.py:51] epoch 1573, training loss: 10840.63, average training loss: 13562.40, base loss: 19700.11
[INFO 2017-06-27 20:34:24,937 main.py:51] epoch 1574, training loss: 14580.47, average training loss: 13563.27, base loss: 19705.84
[INFO 2017-06-27 20:34:25,312 main.py:51] epoch 1575, training loss: 12883.13, average training loss: 13560.21, base loss: 19704.73
[INFO 2017-06-27 20:34:25,688 main.py:51] epoch 1576, training loss: 12804.50, average training loss: 13556.76, base loss: 19701.78
[INFO 2017-06-27 20:34:26,065 main.py:51] epoch 1577, training loss: 13986.04, average training loss: 13556.37, base loss: 19703.08
[INFO 2017-06-27 20:34:26,450 main.py:51] epoch 1578, training loss: 13645.73, average training loss: 13555.44, base loss: 19704.04
[INFO 2017-06-27 20:34:26,826 main.py:51] epoch 1579, training loss: 12046.77, average training loss: 13554.94, base loss: 19706.77
[INFO 2017-06-27 20:34:27,209 main.py:51] epoch 1580, training loss: 12932.28, average training loss: 13552.04, base loss: 19705.08
[INFO 2017-06-27 20:34:27,589 main.py:51] epoch 1581, training loss: 13562.32, average training loss: 13548.62, base loss: 19703.05
[INFO 2017-06-27 20:34:27,965 main.py:51] epoch 1582, training loss: 11927.30, average training loss: 13547.77, base loss: 19703.19
[INFO 2017-06-27 20:34:28,342 main.py:51] epoch 1583, training loss: 11475.52, average training loss: 13545.04, base loss: 19701.51
[INFO 2017-06-27 20:34:28,721 main.py:51] epoch 1584, training loss: 13068.41, average training loss: 13542.12, base loss: 19700.54
[INFO 2017-06-27 20:34:29,097 main.py:51] epoch 1585, training loss: 13576.51, average training loss: 13541.58, base loss: 19701.98
[INFO 2017-06-27 20:34:29,473 main.py:51] epoch 1586, training loss: 12183.72, average training loss: 13541.07, base loss: 19703.68
[INFO 2017-06-27 20:34:29,849 main.py:51] epoch 1587, training loss: 13009.73, average training loss: 13540.25, base loss: 19705.73
[INFO 2017-06-27 20:34:30,225 main.py:51] epoch 1588, training loss: 13244.36, average training loss: 13538.00, base loss: 19704.86
[INFO 2017-06-27 20:34:30,605 main.py:51] epoch 1589, training loss: 12406.21, average training loss: 13534.51, base loss: 19702.10
[INFO 2017-06-27 20:34:30,989 main.py:51] epoch 1590, training loss: 12140.59, average training loss: 13530.98, base loss: 19698.09
[INFO 2017-06-27 20:34:31,373 main.py:51] epoch 1591, training loss: 12385.07, average training loss: 13527.53, base loss: 19695.12
[INFO 2017-06-27 20:34:31,756 main.py:51] epoch 1592, training loss: 13754.70, average training loss: 13524.14, base loss: 19692.56
[INFO 2017-06-27 20:34:32,133 main.py:51] epoch 1593, training loss: 12922.75, average training loss: 13521.65, base loss: 19691.84
[INFO 2017-06-27 20:34:32,508 main.py:51] epoch 1594, training loss: 13789.68, average training loss: 13522.01, base loss: 19694.99
[INFO 2017-06-27 20:34:32,891 main.py:51] epoch 1595, training loss: 13253.71, average training loss: 13523.59, base loss: 19701.62
[INFO 2017-06-27 20:34:33,267 main.py:51] epoch 1596, training loss: 11006.45, average training loss: 13521.29, base loss: 19699.18
[INFO 2017-06-27 20:34:33,643 main.py:51] epoch 1597, training loss: 12038.19, average training loss: 13520.04, base loss: 19698.29
[INFO 2017-06-27 20:34:34,020 main.py:51] epoch 1598, training loss: 12568.67, average training loss: 13517.84, base loss: 19697.14
[INFO 2017-06-27 20:34:34,402 main.py:51] epoch 1599, training loss: 12693.29, average training loss: 13515.18, base loss: 19696.18
[INFO 2017-06-27 20:34:34,402 main.py:53] epoch 1599, testing
[INFO 2017-06-27 20:34:36,000 main.py:105] average testing loss: 12813.42, base loss: 19443.65
[INFO 2017-06-27 20:34:36,000 main.py:106] improve_loss: 6630.23, improve_percent: 0.34
[INFO 2017-06-27 20:34:36,000 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 20:34:36,376 main.py:51] epoch 1600, training loss: 10973.12, average training loss: 13512.24, base loss: 19692.60
[INFO 2017-06-27 20:34:36,752 main.py:51] epoch 1601, training loss: 11689.68, average training loss: 13508.86, base loss: 19690.45
[INFO 2017-06-27 20:34:37,135 main.py:51] epoch 1602, training loss: 12922.23, average training loss: 13506.17, base loss: 19689.33
[INFO 2017-06-27 20:34:37,512 main.py:51] epoch 1603, training loss: 14306.46, average training loss: 13506.06, base loss: 19693.05
[INFO 2017-06-27 20:34:37,888 main.py:51] epoch 1604, training loss: 13615.60, average training loss: 13503.68, base loss: 19692.29
[INFO 2017-06-27 20:34:38,278 main.py:51] epoch 1605, training loss: 13666.09, average training loss: 13502.85, base loss: 19693.48
[INFO 2017-06-27 20:34:38,654 main.py:51] epoch 1606, training loss: 11818.74, average training loss: 13500.85, base loss: 19691.97
[INFO 2017-06-27 20:34:39,030 main.py:51] epoch 1607, training loss: 14335.86, average training loss: 13501.64, base loss: 19696.77
[INFO 2017-06-27 20:34:39,407 main.py:51] epoch 1608, training loss: 13331.28, average training loss: 13497.45, base loss: 19694.91
[INFO 2017-06-27 20:34:39,794 main.py:51] epoch 1609, training loss: 14676.17, average training loss: 13498.62, base loss: 19699.77
[INFO 2017-06-27 20:34:40,172 main.py:51] epoch 1610, training loss: 11839.88, average training loss: 13494.86, base loss: 19696.12
[INFO 2017-06-27 20:34:40,550 main.py:51] epoch 1611, training loss: 11708.49, average training loss: 13493.80, base loss: 19696.86
[INFO 2017-06-27 20:34:40,936 main.py:51] epoch 1612, training loss: 13252.85, average training loss: 13492.75, base loss: 19697.25
[INFO 2017-06-27 20:34:41,313 main.py:51] epoch 1613, training loss: 13570.78, average training loss: 13491.15, base loss: 19696.59
[INFO 2017-06-27 20:34:41,692 main.py:51] epoch 1614, training loss: 14916.32, average training loss: 13493.79, base loss: 19702.32
[INFO 2017-06-27 20:34:42,071 main.py:51] epoch 1615, training loss: 13924.98, average training loss: 13493.37, base loss: 19703.97
[INFO 2017-06-27 20:34:42,453 main.py:51] epoch 1616, training loss: 12726.70, average training loss: 13489.59, base loss: 19699.77
[INFO 2017-06-27 20:34:42,837 main.py:51] epoch 1617, training loss: 13963.77, average training loss: 13489.69, base loss: 19704.81
[INFO 2017-06-27 20:34:43,214 main.py:51] epoch 1618, training loss: 12236.28, average training loss: 13487.95, base loss: 19704.61
[INFO 2017-06-27 20:34:43,591 main.py:51] epoch 1619, training loss: 15186.21, average training loss: 13487.51, base loss: 19705.06
[INFO 2017-06-27 20:34:43,967 main.py:51] epoch 1620, training loss: 13071.96, average training loss: 13484.26, base loss: 19702.32
[INFO 2017-06-27 20:34:44,344 main.py:51] epoch 1621, training loss: 13398.39, average training loss: 13481.48, base loss: 19699.92
[INFO 2017-06-27 20:34:44,789 main.py:51] epoch 1622, training loss: 13421.62, average training loss: 13479.41, base loss: 19700.30
[INFO 2017-06-27 20:34:45,185 main.py:51] epoch 1623, training loss: 13034.11, average training loss: 13479.48, base loss: 19704.69
[INFO 2017-06-27 20:34:45,571 main.py:51] epoch 1624, training loss: 13639.67, average training loss: 13479.20, base loss: 19706.14
[INFO 2017-06-27 20:34:45,957 main.py:51] epoch 1625, training loss: 12822.66, average training loss: 13477.13, base loss: 19705.89
[INFO 2017-06-27 20:34:46,335 main.py:51] epoch 1626, training loss: 11939.08, average training loss: 13472.89, base loss: 19701.53
[INFO 2017-06-27 20:34:46,712 main.py:51] epoch 1627, training loss: 13002.20, average training loss: 13471.33, base loss: 19701.74
[INFO 2017-06-27 20:34:47,089 main.py:51] epoch 1628, training loss: 12738.58, average training loss: 13470.57, base loss: 19704.39
[INFO 2017-06-27 20:34:47,459 main.py:51] epoch 1629, training loss: 13741.45, average training loss: 13468.92, base loss: 19705.37
[INFO 2017-06-27 20:34:47,836 main.py:51] epoch 1630, training loss: 11169.97, average training loss: 13465.17, base loss: 19702.16
[INFO 2017-06-27 20:34:48,214 main.py:51] epoch 1631, training loss: 13545.97, average training loss: 13464.05, base loss: 19702.63
[INFO 2017-06-27 20:34:48,614 main.py:51] epoch 1632, training loss: 12399.48, average training loss: 13462.87, base loss: 19703.91
[INFO 2017-06-27 20:34:48,990 main.py:51] epoch 1633, training loss: 12952.55, average training loss: 13462.10, base loss: 19704.22
[INFO 2017-06-27 20:34:49,367 main.py:51] epoch 1634, training loss: 13581.60, average training loss: 13460.70, base loss: 19705.72
[INFO 2017-06-27 20:34:49,750 main.py:51] epoch 1635, training loss: 13481.41, average training loss: 13458.62, base loss: 19704.99
[INFO 2017-06-27 20:34:50,129 main.py:51] epoch 1636, training loss: 13827.08, average training loss: 13458.51, base loss: 19707.87
[INFO 2017-06-27 20:34:50,502 main.py:51] epoch 1637, training loss: 12828.19, average training loss: 13456.35, base loss: 19704.63
[INFO 2017-06-27 20:34:50,883 main.py:51] epoch 1638, training loss: 13247.75, average training loss: 13455.36, base loss: 19706.84
[INFO 2017-06-27 20:34:51,263 main.py:51] epoch 1639, training loss: 14309.00, average training loss: 13455.74, base loss: 19711.02
[INFO 2017-06-27 20:34:51,639 main.py:51] epoch 1640, training loss: 12955.57, average training loss: 13455.63, base loss: 19713.11
[INFO 2017-06-27 20:34:52,016 main.py:51] epoch 1641, training loss: 14129.54, average training loss: 13455.05, base loss: 19713.91
[INFO 2017-06-27 20:34:52,393 main.py:51] epoch 1642, training loss: 14624.27, average training loss: 13453.45, base loss: 19715.59
[INFO 2017-06-27 20:34:52,769 main.py:51] epoch 1643, training loss: 10610.38, average training loss: 13449.87, base loss: 19711.87
[INFO 2017-06-27 20:34:53,145 main.py:51] epoch 1644, training loss: 12427.46, average training loss: 13448.97, base loss: 19711.16
[INFO 2017-06-27 20:34:53,520 main.py:51] epoch 1645, training loss: 10711.71, average training loss: 13445.29, base loss: 19707.92
[INFO 2017-06-27 20:34:53,892 main.py:51] epoch 1646, training loss: 12919.17, average training loss: 13445.85, base loss: 19712.64
[INFO 2017-06-27 20:34:54,271 main.py:51] epoch 1647, training loss: 11456.75, average training loss: 13442.02, base loss: 19707.47
[INFO 2017-06-27 20:34:54,647 main.py:51] epoch 1648, training loss: 11459.72, average training loss: 13437.67, base loss: 19704.11
[INFO 2017-06-27 20:34:55,025 main.py:51] epoch 1649, training loss: 13482.42, average training loss: 13437.22, base loss: 19706.56
[INFO 2017-06-27 20:34:55,402 main.py:51] epoch 1650, training loss: 13697.34, average training loss: 13435.30, base loss: 19706.81
[INFO 2017-06-27 20:34:55,780 main.py:51] epoch 1651, training loss: 12215.60, average training loss: 13433.10, base loss: 19705.42
[INFO 2017-06-27 20:34:56,157 main.py:51] epoch 1652, training loss: 12875.27, average training loss: 13432.07, base loss: 19707.60
[INFO 2017-06-27 20:34:56,636 main.py:51] epoch 1653, training loss: 12315.28, average training loss: 13431.70, base loss: 19709.50
[INFO 2017-06-27 20:34:57,037 main.py:51] epoch 1654, training loss: 13402.50, average training loss: 13430.39, base loss: 19710.78
[INFO 2017-06-27 20:34:57,419 main.py:51] epoch 1655, training loss: 11533.40, average training loss: 13427.92, base loss: 19710.45
[INFO 2017-06-27 20:34:57,796 main.py:51] epoch 1656, training loss: 12065.91, average training loss: 13424.68, base loss: 19708.70
[INFO 2017-06-27 20:34:58,173 main.py:51] epoch 1657, training loss: 13827.09, average training loss: 13420.73, base loss: 19705.97
[INFO 2017-06-27 20:34:58,551 main.py:51] epoch 1658, training loss: 12721.46, average training loss: 13420.10, base loss: 19708.17
[INFO 2017-06-27 20:34:58,923 main.py:51] epoch 1659, training loss: 11906.79, average training loss: 13416.17, base loss: 19705.54
[INFO 2017-06-27 20:34:59,298 main.py:51] epoch 1660, training loss: 11030.17, average training loss: 13413.13, base loss: 19703.29
[INFO 2017-06-27 20:34:59,678 main.py:51] epoch 1661, training loss: 12634.44, average training loss: 13412.24, base loss: 19704.45
[INFO 2017-06-27 20:35:00,055 main.py:51] epoch 1662, training loss: 12184.44, average training loss: 13407.33, base loss: 19700.63
[INFO 2017-06-27 20:35:00,432 main.py:51] epoch 1663, training loss: 11632.47, average training loss: 13404.15, base loss: 19696.78
[INFO 2017-06-27 20:35:00,812 main.py:51] epoch 1664, training loss: 11024.46, average training loss: 13402.51, base loss: 19697.67
[INFO 2017-06-27 20:35:01,189 main.py:51] epoch 1665, training loss: 13717.73, average training loss: 13401.91, base loss: 19700.62
[INFO 2017-06-27 20:35:01,562 main.py:51] epoch 1666, training loss: 12787.68, average training loss: 13400.39, base loss: 19700.35
[INFO 2017-06-27 20:35:01,939 main.py:51] epoch 1667, training loss: 13296.74, average training loss: 13399.35, base loss: 19702.50
[INFO 2017-06-27 20:35:02,322 main.py:51] epoch 1668, training loss: 12542.46, average training loss: 13396.89, base loss: 19701.57
[INFO 2017-06-27 20:35:02,698 main.py:51] epoch 1669, training loss: 13500.71, average training loss: 13394.51, base loss: 19701.27
[INFO 2017-06-27 20:35:03,081 main.py:51] epoch 1670, training loss: 11270.47, average training loss: 13389.15, base loss: 19697.61
[INFO 2017-06-27 20:35:03,466 main.py:51] epoch 1671, training loss: 13017.42, average training loss: 13388.73, base loss: 19698.04
[INFO 2017-06-27 20:35:03,845 main.py:51] epoch 1672, training loss: 14642.12, average training loss: 13390.81, base loss: 19702.97
[INFO 2017-06-27 20:35:04,236 main.py:51] epoch 1673, training loss: 11791.67, average training loss: 13387.86, base loss: 19701.45
[INFO 2017-06-27 20:35:04,618 main.py:51] epoch 1674, training loss: 11552.01, average training loss: 13386.07, base loss: 19700.59
[INFO 2017-06-27 20:35:04,996 main.py:51] epoch 1675, training loss: 12317.80, average training loss: 13384.88, base loss: 19699.97
[INFO 2017-06-27 20:35:05,372 main.py:51] epoch 1676, training loss: 14869.08, average training loss: 13386.52, base loss: 19705.15
[INFO 2017-06-27 20:35:05,746 main.py:51] epoch 1677, training loss: 12662.29, average training loss: 13384.40, base loss: 19704.25
[INFO 2017-06-27 20:35:06,123 main.py:51] epoch 1678, training loss: 11574.79, average training loss: 13382.35, base loss: 19703.39
[INFO 2017-06-27 20:35:06,506 main.py:51] epoch 1679, training loss: 13050.95, average training loss: 13379.22, base loss: 19700.45
[INFO 2017-06-27 20:35:06,884 main.py:51] epoch 1680, training loss: 12885.67, average training loss: 13376.51, base loss: 19698.13
[INFO 2017-06-27 20:35:07,263 main.py:51] epoch 1681, training loss: 12951.56, average training loss: 13375.66, base loss: 19698.65
[INFO 2017-06-27 20:35:07,640 main.py:51] epoch 1682, training loss: 11715.31, average training loss: 13372.62, base loss: 19696.20
[INFO 2017-06-27 20:35:08,017 main.py:51] epoch 1683, training loss: 13398.18, average training loss: 13371.59, base loss: 19696.45
[INFO 2017-06-27 20:35:08,392 main.py:51] epoch 1684, training loss: 12394.48, average training loss: 13372.79, base loss: 19701.46
[INFO 2017-06-27 20:35:08,768 main.py:51] epoch 1685, training loss: 11483.51, average training loss: 13370.53, base loss: 19700.41
[INFO 2017-06-27 20:35:09,141 main.py:51] epoch 1686, training loss: 12815.08, average training loss: 13369.06, base loss: 19700.91
[INFO 2017-06-27 20:35:09,518 main.py:51] epoch 1687, training loss: 11899.09, average training loss: 13366.71, base loss: 19699.49
[INFO 2017-06-27 20:35:09,896 main.py:51] epoch 1688, training loss: 14845.60, average training loss: 13368.16, base loss: 19706.36
[INFO 2017-06-27 20:35:10,272 main.py:51] epoch 1689, training loss: 11126.32, average training loss: 13362.92, base loss: 19700.27
[INFO 2017-06-27 20:35:10,652 main.py:51] epoch 1690, training loss: 12891.27, average training loss: 13360.50, base loss: 19699.70
[INFO 2017-06-27 20:35:11,029 main.py:51] epoch 1691, training loss: 13459.84, average training loss: 13357.49, base loss: 19696.61
[INFO 2017-06-27 20:35:11,402 main.py:51] epoch 1692, training loss: 13714.51, average training loss: 13356.32, base loss: 19696.57
[INFO 2017-06-27 20:35:11,776 main.py:51] epoch 1693, training loss: 11437.50, average training loss: 13354.15, base loss: 19695.04
[INFO 2017-06-27 20:35:12,149 main.py:51] epoch 1694, training loss: 12482.72, average training loss: 13352.22, base loss: 19694.98
[INFO 2017-06-27 20:35:12,526 main.py:51] epoch 1695, training loss: 11558.91, average training loss: 13349.21, base loss: 19692.13
[INFO 2017-06-27 20:35:12,905 main.py:51] epoch 1696, training loss: 12463.06, average training loss: 13346.73, base loss: 19691.10
[INFO 2017-06-27 20:35:13,283 main.py:51] epoch 1697, training loss: 12518.73, average training loss: 13346.76, base loss: 19693.27
[INFO 2017-06-27 20:35:13,662 main.py:51] epoch 1698, training loss: 14350.82, average training loss: 13347.88, base loss: 19698.53
[INFO 2017-06-27 20:35:14,038 main.py:51] epoch 1699, training loss: 11901.21, average training loss: 13344.60, base loss: 19694.74
[INFO 2017-06-27 20:35:14,038 main.py:53] epoch 1699, testing
[INFO 2017-06-27 20:35:15,649 main.py:105] average testing loss: 12273.26, base loss: 19337.15
[INFO 2017-06-27 20:35:15,649 main.py:106] improve_loss: 7063.89, improve_percent: 0.37
[INFO 2017-06-27 20:35:15,649 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:35:15,662 main.py:76] current best improved percent: 0.37
[INFO 2017-06-27 20:35:16,040 main.py:51] epoch 1700, training loss: 11573.39, average training loss: 13343.23, base loss: 19693.67
[INFO 2017-06-27 20:35:16,418 main.py:51] epoch 1701, training loss: 12029.66, average training loss: 13342.81, base loss: 19696.29
[INFO 2017-06-27 20:35:16,795 main.py:51] epoch 1702, training loss: 12970.79, average training loss: 13341.64, base loss: 19698.17
[INFO 2017-06-27 20:35:17,173 main.py:51] epoch 1703, training loss: 13045.43, average training loss: 13340.09, base loss: 19697.27
[INFO 2017-06-27 20:35:17,553 main.py:51] epoch 1704, training loss: 14064.86, average training loss: 13340.56, base loss: 19700.95
[INFO 2017-06-27 20:35:17,931 main.py:51] epoch 1705, training loss: 11004.73, average training loss: 13338.09, base loss: 19697.73
[INFO 2017-06-27 20:35:18,309 main.py:51] epoch 1706, training loss: 13661.83, average training loss: 13336.04, base loss: 19698.03
[INFO 2017-06-27 20:35:18,688 main.py:51] epoch 1707, training loss: 13455.57, average training loss: 13336.08, base loss: 19702.09
[INFO 2017-06-27 20:35:19,065 main.py:51] epoch 1708, training loss: 14233.26, average training loss: 13335.25, base loss: 19704.08
[INFO 2017-06-27 20:35:19,451 main.py:51] epoch 1709, training loss: 12141.27, average training loss: 13332.77, base loss: 19702.60
[INFO 2017-06-27 20:35:19,829 main.py:51] epoch 1710, training loss: 12904.55, average training loss: 13328.78, base loss: 19698.02
[INFO 2017-06-27 20:35:20,207 main.py:51] epoch 1711, training loss: 15019.59, average training loss: 13329.64, base loss: 19702.00
[INFO 2017-06-27 20:35:20,585 main.py:51] epoch 1712, training loss: 11944.27, average training loss: 13326.94, base loss: 19699.90
[INFO 2017-06-27 20:35:20,966 main.py:51] epoch 1713, training loss: 10616.90, average training loss: 13322.45, base loss: 19695.25
[INFO 2017-06-27 20:35:21,344 main.py:51] epoch 1714, training loss: 11963.47, average training loss: 13322.03, base loss: 19698.40
[INFO 2017-06-27 20:35:21,727 main.py:51] epoch 1715, training loss: 11695.34, average training loss: 13320.50, base loss: 19698.82
[INFO 2017-06-27 20:35:22,110 main.py:51] epoch 1716, training loss: 12988.32, average training loss: 13317.51, base loss: 19698.39
[INFO 2017-06-27 20:35:22,494 main.py:51] epoch 1717, training loss: 11434.64, average training loss: 13316.12, base loss: 19696.95
[INFO 2017-06-27 20:35:22,875 main.py:51] epoch 1718, training loss: 11638.76, average training loss: 13313.00, base loss: 19693.74
[INFO 2017-06-27 20:35:23,263 main.py:51] epoch 1719, training loss: 12595.55, average training loss: 13311.00, base loss: 19692.99
[INFO 2017-06-27 20:35:23,643 main.py:51] epoch 1720, training loss: 12611.89, average training loss: 13308.95, base loss: 19692.89
[INFO 2017-06-27 20:35:24,043 main.py:51] epoch 1721, training loss: 11396.34, average training loss: 13305.75, base loss: 19691.30
[INFO 2017-06-27 20:35:24,505 main.py:51] epoch 1722, training loss: 14342.79, average training loss: 13306.57, base loss: 19693.78
[INFO 2017-06-27 20:35:24,936 main.py:51] epoch 1723, training loss: 11990.16, average training loss: 13305.56, base loss: 19694.10
[INFO 2017-06-27 20:35:25,399 main.py:51] epoch 1724, training loss: 11883.52, average training loss: 13305.48, base loss: 19696.06
[INFO 2017-06-27 20:35:25,834 main.py:51] epoch 1725, training loss: 12070.64, average training loss: 13303.46, base loss: 19695.75
[INFO 2017-06-27 20:35:26,259 main.py:51] epoch 1726, training loss: 12412.17, average training loss: 13301.12, base loss: 19693.02
[INFO 2017-06-27 20:35:26,660 main.py:51] epoch 1727, training loss: 12712.28, average training loss: 13299.36, base loss: 19694.20
[INFO 2017-06-27 20:35:27,116 main.py:51] epoch 1728, training loss: 12528.49, average training loss: 13294.24, base loss: 19688.44
[INFO 2017-06-27 20:35:27,579 main.py:51] epoch 1729, training loss: 11212.21, average training loss: 13291.55, base loss: 19685.52
[INFO 2017-06-27 20:35:27,970 main.py:51] epoch 1730, training loss: 14552.03, average training loss: 13290.78, base loss: 19688.46
[INFO 2017-06-27 20:35:28,349 main.py:51] epoch 1731, training loss: 12532.43, average training loss: 13286.71, base loss: 19685.72
[INFO 2017-06-27 20:35:28,728 main.py:51] epoch 1732, training loss: 12580.84, average training loss: 13286.99, base loss: 19690.06
[INFO 2017-06-27 20:35:29,104 main.py:51] epoch 1733, training loss: 13239.23, average training loss: 13286.32, base loss: 19690.45
[INFO 2017-06-27 20:35:29,481 main.py:51] epoch 1734, training loss: 12605.74, average training loss: 13282.25, base loss: 19686.69
[INFO 2017-06-27 20:35:29,857 main.py:51] epoch 1735, training loss: 11584.43, average training loss: 13281.33, base loss: 19686.54
[INFO 2017-06-27 20:35:30,234 main.py:51] epoch 1736, training loss: 12388.84, average training loss: 13279.25, base loss: 19686.34
[INFO 2017-06-27 20:35:30,641 main.py:51] epoch 1737, training loss: 12102.97, average training loss: 13277.42, base loss: 19685.95
[INFO 2017-06-27 20:35:31,026 main.py:51] epoch 1738, training loss: 12951.90, average training loss: 13276.21, base loss: 19687.12
[INFO 2017-06-27 20:35:31,402 main.py:51] epoch 1739, training loss: 11438.03, average training loss: 13272.63, base loss: 19684.38
[INFO 2017-06-27 20:35:31,786 main.py:51] epoch 1740, training loss: 10344.30, average training loss: 13269.35, base loss: 19681.55
[INFO 2017-06-27 20:35:32,163 main.py:51] epoch 1741, training loss: 12233.99, average training loss: 13268.45, base loss: 19680.97
[INFO 2017-06-27 20:35:32,541 main.py:51] epoch 1742, training loss: 11788.92, average training loss: 13265.86, base loss: 19678.10
[INFO 2017-06-27 20:35:32,919 main.py:51] epoch 1743, training loss: 12026.97, average training loss: 13264.39, base loss: 19678.07
[INFO 2017-06-27 20:35:33,295 main.py:51] epoch 1744, training loss: 11590.03, average training loss: 13261.92, base loss: 19675.35
[INFO 2017-06-27 20:35:33,677 main.py:51] epoch 1745, training loss: 13485.78, average training loss: 13263.49, base loss: 19679.89
[INFO 2017-06-27 20:35:34,053 main.py:51] epoch 1746, training loss: 12837.82, average training loss: 13259.18, base loss: 19677.02
[INFO 2017-06-27 20:35:34,430 main.py:51] epoch 1747, training loss: 11674.15, average training loss: 13257.52, base loss: 19676.21
[INFO 2017-06-27 20:35:34,806 main.py:51] epoch 1748, training loss: 13710.43, average training loss: 13255.56, base loss: 19676.35
[INFO 2017-06-27 20:35:35,183 main.py:51] epoch 1749, training loss: 12230.23, average training loss: 13254.22, base loss: 19678.83
[INFO 2017-06-27 20:35:35,559 main.py:51] epoch 1750, training loss: 13708.01, average training loss: 13253.69, base loss: 19680.39
[INFO 2017-06-27 20:35:35,936 main.py:51] epoch 1751, training loss: 12052.82, average training loss: 13250.56, base loss: 19678.85
[INFO 2017-06-27 20:35:36,311 main.py:51] epoch 1752, training loss: 13467.03, average training loss: 13250.34, base loss: 19683.10
[INFO 2017-06-27 20:35:36,688 main.py:51] epoch 1753, training loss: 12985.96, average training loss: 13249.72, base loss: 19684.55
[INFO 2017-06-27 20:35:37,064 main.py:51] epoch 1754, training loss: 12565.59, average training loss: 13245.74, base loss: 19681.09
[INFO 2017-06-27 20:35:37,440 main.py:51] epoch 1755, training loss: 12659.01, average training loss: 13244.55, base loss: 19681.78
[INFO 2017-06-27 20:35:37,816 main.py:51] epoch 1756, training loss: 12427.44, average training loss: 13242.10, base loss: 19681.54
[INFO 2017-06-27 20:35:38,193 main.py:51] epoch 1757, training loss: 10376.99, average training loss: 13236.28, base loss: 19675.40
[INFO 2017-06-27 20:35:38,571 main.py:51] epoch 1758, training loss: 13359.48, average training loss: 13235.33, base loss: 19676.20
[INFO 2017-06-27 20:35:38,947 main.py:51] epoch 1759, training loss: 13637.74, average training loss: 13233.02, base loss: 19674.61
[INFO 2017-06-27 20:35:39,323 main.py:51] epoch 1760, training loss: 13885.18, average training loss: 13234.39, base loss: 19679.06
[INFO 2017-06-27 20:35:39,699 main.py:51] epoch 1761, training loss: 11991.31, average training loss: 13230.96, base loss: 19675.36
[INFO 2017-06-27 20:35:40,077 main.py:51] epoch 1762, training loss: 11757.22, average training loss: 13228.96, base loss: 19674.63
[INFO 2017-06-27 20:35:40,459 main.py:51] epoch 1763, training loss: 12994.14, average training loss: 13226.37, base loss: 19672.98
[INFO 2017-06-27 20:35:40,842 main.py:51] epoch 1764, training loss: 12868.26, average training loss: 13223.81, base loss: 19672.49
[INFO 2017-06-27 20:35:41,221 main.py:51] epoch 1765, training loss: 10890.38, average training loss: 13219.12, base loss: 19665.68
[INFO 2017-06-27 20:35:41,668 main.py:51] epoch 1766, training loss: 12397.02, average training loss: 13217.72, base loss: 19664.72
[INFO 2017-06-27 20:35:42,064 main.py:51] epoch 1767, training loss: 13060.18, average training loss: 13216.03, base loss: 19663.71
[INFO 2017-06-27 20:35:42,443 main.py:51] epoch 1768, training loss: 11843.12, average training loss: 13213.21, base loss: 19661.28
[INFO 2017-06-27 20:35:42,821 main.py:51] epoch 1769, training loss: 12162.23, average training loss: 13211.10, base loss: 19660.72
[INFO 2017-06-27 20:35:43,227 main.py:51] epoch 1770, training loss: 11097.97, average training loss: 13207.88, base loss: 19658.52
[INFO 2017-06-27 20:35:43,616 main.py:51] epoch 1771, training loss: 12788.56, average training loss: 13208.31, base loss: 19661.90
[INFO 2017-06-27 20:35:43,992 main.py:51] epoch 1772, training loss: 11589.73, average training loss: 13206.97, base loss: 19663.71
[INFO 2017-06-27 20:35:44,367 main.py:51] epoch 1773, training loss: 12351.64, average training loss: 13207.56, base loss: 19667.12
[INFO 2017-06-27 20:35:44,740 main.py:51] epoch 1774, training loss: 14589.27, average training loss: 13210.13, base loss: 19673.29
[INFO 2017-06-27 20:35:45,111 main.py:51] epoch 1775, training loss: 12541.85, average training loss: 13207.46, base loss: 19670.45
[INFO 2017-06-27 20:35:45,487 main.py:51] epoch 1776, training loss: 13005.11, average training loss: 13204.95, base loss: 19669.20
[INFO 2017-06-27 20:35:45,858 main.py:51] epoch 1777, training loss: 11231.26, average training loss: 13202.43, base loss: 19667.35
[INFO 2017-06-27 20:35:46,235 main.py:51] epoch 1778, training loss: 11491.26, average training loss: 13200.73, base loss: 19666.20
[INFO 2017-06-27 20:35:46,619 main.py:51] epoch 1779, training loss: 11284.16, average training loss: 13197.90, base loss: 19663.07
[INFO 2017-06-27 20:35:46,989 main.py:51] epoch 1780, training loss: 13489.17, average training loss: 13196.17, base loss: 19662.92
[INFO 2017-06-27 20:35:47,360 main.py:51] epoch 1781, training loss: 11898.73, average training loss: 13194.65, base loss: 19662.88
[INFO 2017-06-27 20:35:47,732 main.py:51] epoch 1782, training loss: 10637.95, average training loss: 13190.14, base loss: 19658.21
[INFO 2017-06-27 20:35:48,111 main.py:51] epoch 1783, training loss: 12191.43, average training loss: 13186.53, base loss: 19653.82
[INFO 2017-06-27 20:35:48,490 main.py:51] epoch 1784, training loss: 13893.15, average training loss: 13185.47, base loss: 19654.41
[INFO 2017-06-27 20:35:48,864 main.py:51] epoch 1785, training loss: 12003.34, average training loss: 13180.52, base loss: 19647.62
[INFO 2017-06-27 20:35:49,243 main.py:51] epoch 1786, training loss: 12686.22, average training loss: 13178.53, base loss: 19647.98
[INFO 2017-06-27 20:35:49,620 main.py:51] epoch 1787, training loss: 14519.44, average training loss: 13179.99, base loss: 19653.07
[INFO 2017-06-27 20:35:49,992 main.py:51] epoch 1788, training loss: 12230.65, average training loss: 13178.34, base loss: 19651.35
[INFO 2017-06-27 20:35:50,372 main.py:51] epoch 1789, training loss: 11357.32, average training loss: 13175.01, base loss: 19646.02
[INFO 2017-06-27 20:35:50,744 main.py:51] epoch 1790, training loss: 14694.60, average training loss: 13175.68, base loss: 19649.62
[INFO 2017-06-27 20:35:51,117 main.py:51] epoch 1791, training loss: 11755.28, average training loss: 13173.60, base loss: 19647.63
[INFO 2017-06-27 20:35:51,494 main.py:51] epoch 1792, training loss: 13553.70, average training loss: 13175.06, base loss: 19653.31
[INFO 2017-06-27 20:35:51,873 main.py:51] epoch 1793, training loss: 12785.28, average training loss: 13172.48, base loss: 19652.21
[INFO 2017-06-27 20:35:52,246 main.py:51] epoch 1794, training loss: 11069.54, average training loss: 13168.35, base loss: 19648.45
[INFO 2017-06-27 20:35:52,620 main.py:51] epoch 1795, training loss: 10195.24, average training loss: 13164.59, base loss: 19644.09
[INFO 2017-06-27 20:35:52,997 main.py:51] epoch 1796, training loss: 13310.74, average training loss: 13164.90, base loss: 19647.00
[INFO 2017-06-27 20:35:53,374 main.py:51] epoch 1797, training loss: 13789.50, average training loss: 13164.11, base loss: 19648.37
[INFO 2017-06-27 20:35:53,747 main.py:51] epoch 1798, training loss: 11987.09, average training loss: 13162.46, base loss: 19646.98
[INFO 2017-06-27 20:35:54,124 main.py:51] epoch 1799, training loss: 13395.70, average training loss: 13161.93, base loss: 19650.48
[INFO 2017-06-27 20:35:54,125 main.py:53] epoch 1799, testing
[INFO 2017-06-27 20:35:55,733 main.py:105] average testing loss: 12704.01, base loss: 19940.35
[INFO 2017-06-27 20:35:55,733 main.py:106] improve_loss: 7236.34, improve_percent: 0.36
[INFO 2017-06-27 20:35:55,734 main.py:76] current best improved percent: 0.37
[INFO 2017-06-27 20:35:56,108 main.py:51] epoch 1800, training loss: 11739.53, average training loss: 13158.61, base loss: 19645.96
[INFO 2017-06-27 20:35:56,477 main.py:51] epoch 1801, training loss: 12118.41, average training loss: 13157.55, base loss: 19647.35
[INFO 2017-06-27 20:35:56,855 main.py:51] epoch 1802, training loss: 14208.86, average training loss: 13158.66, base loss: 19651.21
[INFO 2017-06-27 20:35:57,232 main.py:51] epoch 1803, training loss: 12891.58, average training loss: 13157.77, base loss: 19651.71
[INFO 2017-06-27 20:35:57,606 main.py:51] epoch 1804, training loss: 13498.65, average training loss: 13158.09, base loss: 19655.88
[INFO 2017-06-27 20:35:57,985 main.py:51] epoch 1805, training loss: 12790.85, average training loss: 13157.52, base loss: 19658.41
[INFO 2017-06-27 20:35:58,361 main.py:51] epoch 1806, training loss: 12719.03, average training loss: 13155.88, base loss: 19656.73
[INFO 2017-06-27 20:35:58,830 main.py:51] epoch 1807, training loss: 10999.17, average training loss: 13152.35, base loss: 19651.19
[INFO 2017-06-27 20:35:59,221 main.py:51] epoch 1808, training loss: 11466.16, average training loss: 13148.40, base loss: 19646.79
[INFO 2017-06-27 20:35:59,602 main.py:51] epoch 1809, training loss: 13062.49, average training loss: 13148.21, base loss: 19648.47
[INFO 2017-06-27 20:35:59,984 main.py:51] epoch 1810, training loss: 13096.81, average training loss: 13148.14, base loss: 19650.63
[INFO 2017-06-27 20:36:00,381 main.py:51] epoch 1811, training loss: 12814.72, average training loss: 13146.63, base loss: 19651.37
[INFO 2017-06-27 20:36:00,761 main.py:51] epoch 1812, training loss: 15362.29, average training loss: 13148.51, base loss: 19656.62
[INFO 2017-06-27 20:36:01,136 main.py:51] epoch 1813, training loss: 13764.93, average training loss: 13147.03, base loss: 19656.40
[INFO 2017-06-27 20:36:01,508 main.py:51] epoch 1814, training loss: 13527.13, average training loss: 13145.71, base loss: 19655.75
[INFO 2017-06-27 20:36:01,879 main.py:51] epoch 1815, training loss: 11129.85, average training loss: 13140.61, base loss: 19648.71
[INFO 2017-06-27 20:36:02,255 main.py:51] epoch 1816, training loss: 11513.86, average training loss: 13137.81, base loss: 19646.18
[INFO 2017-06-27 20:36:02,629 main.py:51] epoch 1817, training loss: 12053.87, average training loss: 13136.02, base loss: 19645.11
[INFO 2017-06-27 20:36:03,009 main.py:51] epoch 1818, training loss: 12051.91, average training loss: 13134.60, base loss: 19644.66
[INFO 2017-06-27 20:36:03,388 main.py:51] epoch 1819, training loss: 11854.11, average training loss: 13135.03, base loss: 19648.09
[INFO 2017-06-27 20:36:03,763 main.py:51] epoch 1820, training loss: 11672.14, average training loss: 13131.72, base loss: 19642.85
[INFO 2017-06-27 20:36:04,214 main.py:51] epoch 1821, training loss: 12370.83, average training loss: 13129.91, base loss: 19641.62
[INFO 2017-06-27 20:36:04,631 main.py:51] epoch 1822, training loss: 13517.60, average training loss: 13130.45, base loss: 19643.59
[INFO 2017-06-27 20:36:05,008 main.py:51] epoch 1823, training loss: 12192.75, average training loss: 13128.49, base loss: 19642.22
[INFO 2017-06-27 20:36:05,388 main.py:51] epoch 1824, training loss: 11562.22, average training loss: 13126.75, base loss: 19641.12
[INFO 2017-06-27 20:36:05,777 main.py:51] epoch 1825, training loss: 12298.81, average training loss: 13125.18, base loss: 19641.21
[INFO 2017-06-27 20:36:06,156 main.py:51] epoch 1826, training loss: 10249.66, average training loss: 13119.19, base loss: 19633.76
[INFO 2017-06-27 20:36:06,529 main.py:51] epoch 1827, training loss: 13245.61, average training loss: 13116.88, base loss: 19630.99
[INFO 2017-06-27 20:36:06,907 main.py:51] epoch 1828, training loss: 12785.87, average training loss: 13117.60, base loss: 19635.92
[INFO 2017-06-27 20:36:07,277 main.py:51] epoch 1829, training loss: 11697.62, average training loss: 13113.85, base loss: 19630.62
[INFO 2017-06-27 20:36:07,659 main.py:51] epoch 1830, training loss: 12500.83, average training loss: 13112.23, base loss: 19630.11
[INFO 2017-06-27 20:36:08,037 main.py:51] epoch 1831, training loss: 13639.34, average training loss: 13111.06, base loss: 19633.25
[INFO 2017-06-27 20:36:08,492 main.py:51] epoch 1832, training loss: 12908.88, average training loss: 13108.80, base loss: 19632.49
[INFO 2017-06-27 20:36:08,955 main.py:51] epoch 1833, training loss: 11108.89, average training loss: 13106.79, base loss: 19631.46
[INFO 2017-06-27 20:36:09,344 main.py:51] epoch 1834, training loss: 11835.86, average training loss: 13103.94, base loss: 19627.32
[INFO 2017-06-27 20:36:09,741 main.py:51] epoch 1835, training loss: 14986.16, average training loss: 13105.45, base loss: 19631.54
[INFO 2017-06-27 20:36:10,122 main.py:51] epoch 1836, training loss: 11418.16, average training loss: 13102.67, base loss: 19629.37
[INFO 2017-06-27 20:36:10,498 main.py:51] epoch 1837, training loss: 10475.90, average training loss: 13099.61, base loss: 19626.49
[INFO 2017-06-27 20:36:10,872 main.py:51] epoch 1838, training loss: 13798.53, average training loss: 13100.28, base loss: 19630.48
[INFO 2017-06-27 20:36:11,247 main.py:51] epoch 1839, training loss: 11944.20, average training loss: 13097.18, base loss: 19626.84
[INFO 2017-06-27 20:36:11,620 main.py:51] epoch 1840, training loss: 13392.98, average training loss: 13097.38, base loss: 19628.88
[INFO 2017-06-27 20:36:11,996 main.py:51] epoch 1841, training loss: 13723.66, average training loss: 13097.42, base loss: 19631.84
[INFO 2017-06-27 20:36:12,376 main.py:51] epoch 1842, training loss: 13201.54, average training loss: 13095.52, base loss: 19631.87
[INFO 2017-06-27 20:36:12,755 main.py:51] epoch 1843, training loss: 13023.51, average training loss: 13093.34, base loss: 19631.11
[INFO 2017-06-27 20:36:13,173 main.py:51] epoch 1844, training loss: 11873.48, average training loss: 13089.88, base loss: 19626.97
[INFO 2017-06-27 20:36:13,586 main.py:51] epoch 1845, training loss: 13757.63, average training loss: 13090.38, base loss: 19629.60
[INFO 2017-06-27 20:36:13,982 main.py:51] epoch 1846, training loss: 12538.10, average training loss: 13089.21, base loss: 19629.35
[INFO 2017-06-27 20:36:14,380 main.py:51] epoch 1847, training loss: 12029.76, average training loss: 13088.15, base loss: 19631.87
[INFO 2017-06-27 20:36:14,791 main.py:51] epoch 1848, training loss: 12032.60, average training loss: 13087.91, base loss: 19635.63
[INFO 2017-06-27 20:36:15,170 main.py:51] epoch 1849, training loss: 10845.70, average training loss: 13084.90, base loss: 19632.74
[INFO 2017-06-27 20:36:15,543 main.py:51] epoch 1850, training loss: 11800.62, average training loss: 13084.14, base loss: 19633.63
[INFO 2017-06-27 20:36:15,926 main.py:51] epoch 1851, training loss: 11683.03, average training loss: 13081.85, base loss: 19631.39
[INFO 2017-06-27 20:36:16,300 main.py:51] epoch 1852, training loss: 10992.50, average training loss: 13079.64, base loss: 19629.13
[INFO 2017-06-27 20:36:16,683 main.py:51] epoch 1853, training loss: 13742.02, average training loss: 13079.27, base loss: 19631.33
[INFO 2017-06-27 20:36:17,059 main.py:51] epoch 1854, training loss: 11559.73, average training loss: 13076.63, base loss: 19628.82
[INFO 2017-06-27 20:36:17,439 main.py:51] epoch 1855, training loss: 12546.31, average training loss: 13074.28, base loss: 19627.48
[INFO 2017-06-27 20:36:17,821 main.py:51] epoch 1856, training loss: 11287.79, average training loss: 13071.56, base loss: 19626.14
[INFO 2017-06-27 20:36:18,197 main.py:51] epoch 1857, training loss: 13948.84, average training loss: 13070.20, base loss: 19627.68
[INFO 2017-06-27 20:36:18,577 main.py:51] epoch 1858, training loss: 14077.73, average training loss: 13068.42, base loss: 19627.23
[INFO 2017-06-27 20:36:18,961 main.py:51] epoch 1859, training loss: 11952.29, average training loss: 13066.49, base loss: 19625.55
[INFO 2017-06-27 20:36:19,338 main.py:51] epoch 1860, training loss: 12233.74, average training loss: 13067.18, base loss: 19630.77
[INFO 2017-06-27 20:36:19,713 main.py:51] epoch 1861, training loss: 12715.42, average training loss: 13065.03, base loss: 19630.18
[INFO 2017-06-27 20:36:20,090 main.py:51] epoch 1862, training loss: 12298.21, average training loss: 13064.16, base loss: 19631.45
[INFO 2017-06-27 20:36:20,469 main.py:51] epoch 1863, training loss: 11467.97, average training loss: 13061.08, base loss: 19629.34
[INFO 2017-06-27 20:36:20,845 main.py:51] epoch 1864, training loss: 13152.72, average training loss: 13060.58, base loss: 19631.67
[INFO 2017-06-27 20:36:21,221 main.py:51] epoch 1865, training loss: 13979.79, average training loss: 13060.23, base loss: 19635.36
[INFO 2017-06-27 20:36:21,605 main.py:51] epoch 1866, training loss: 13010.11, average training loss: 13060.90, base loss: 19640.32
[INFO 2017-06-27 20:36:21,982 main.py:51] epoch 1867, training loss: 13859.80, average training loss: 13059.78, base loss: 19640.59
[INFO 2017-06-27 20:36:22,366 main.py:51] epoch 1868, training loss: 14221.01, average training loss: 13061.23, base loss: 19644.57
[INFO 2017-06-27 20:36:22,747 main.py:51] epoch 1869, training loss: 12515.74, average training loss: 13058.97, base loss: 19642.71
[INFO 2017-06-27 20:36:23,122 main.py:51] epoch 1870, training loss: 12615.93, average training loss: 13056.64, base loss: 19641.17
[INFO 2017-06-27 20:36:23,498 main.py:51] epoch 1871, training loss: 12211.86, average training loss: 13054.35, base loss: 19639.05
[INFO 2017-06-27 20:36:23,879 main.py:51] epoch 1872, training loss: 12288.68, average training loss: 13052.24, base loss: 19636.26
[INFO 2017-06-27 20:36:24,258 main.py:51] epoch 1873, training loss: 12365.08, average training loss: 13050.43, base loss: 19634.42
[INFO 2017-06-27 20:36:24,640 main.py:51] epoch 1874, training loss: 12815.68, average training loss: 13048.21, base loss: 19633.24
[INFO 2017-06-27 20:36:25,022 main.py:51] epoch 1875, training loss: 13907.12, average training loss: 13046.76, base loss: 19633.89
[INFO 2017-06-27 20:36:25,399 main.py:51] epoch 1876, training loss: 13085.69, average training loss: 13044.98, base loss: 19634.38
[INFO 2017-06-27 20:36:25,802 main.py:51] epoch 1877, training loss: 14408.98, average training loss: 13045.91, base loss: 19637.39
[INFO 2017-06-27 20:36:26,198 main.py:51] epoch 1878, training loss: 13237.38, average training loss: 13045.28, base loss: 19638.85
[INFO 2017-06-27 20:36:26,574 main.py:51] epoch 1879, training loss: 13403.67, average training loss: 13042.95, base loss: 19637.49
[INFO 2017-06-27 20:36:26,958 main.py:51] epoch 1880, training loss: 12310.71, average training loss: 13043.47, base loss: 19640.25
[INFO 2017-06-27 20:36:27,359 main.py:51] epoch 1881, training loss: 11639.57, average training loss: 13043.30, base loss: 19641.55
[INFO 2017-06-27 20:36:27,743 main.py:51] epoch 1882, training loss: 13041.38, average training loss: 13041.40, base loss: 19641.01
[INFO 2017-06-27 20:36:28,123 main.py:51] epoch 1883, training loss: 11829.32, average training loss: 13039.02, base loss: 19639.84
[INFO 2017-06-27 20:36:28,497 main.py:51] epoch 1884, training loss: 13714.17, average training loss: 13038.29, base loss: 19641.14
[INFO 2017-06-27 20:36:28,881 main.py:51] epoch 1885, training loss: 11698.11, average training loss: 13034.95, base loss: 19638.13
[INFO 2017-06-27 20:36:29,258 main.py:51] epoch 1886, training loss: 12903.01, average training loss: 13032.55, base loss: 19636.64
[INFO 2017-06-27 20:36:29,637 main.py:51] epoch 1887, training loss: 12027.75, average training loss: 13031.47, base loss: 19638.09
[INFO 2017-06-27 20:36:30,019 main.py:51] epoch 1888, training loss: 10610.80, average training loss: 13028.29, base loss: 19633.57
[INFO 2017-06-27 20:36:30,395 main.py:51] epoch 1889, training loss: 12612.22, average training loss: 13027.39, base loss: 19634.84
[INFO 2017-06-27 20:36:30,776 main.py:51] epoch 1890, training loss: 14864.02, average training loss: 13028.58, base loss: 19639.75
[INFO 2017-06-27 20:36:31,158 main.py:51] epoch 1891, training loss: 11876.80, average training loss: 13025.74, base loss: 19637.59
[INFO 2017-06-27 20:36:31,534 main.py:51] epoch 1892, training loss: 13340.70, average training loss: 13024.91, base loss: 19639.46
[INFO 2017-06-27 20:36:31,917 main.py:51] epoch 1893, training loss: 14454.31, average training loss: 13025.62, base loss: 19644.11
[INFO 2017-06-27 20:36:32,294 main.py:51] epoch 1894, training loss: 12653.49, average training loss: 13025.58, base loss: 19647.47
[INFO 2017-06-27 20:36:32,669 main.py:51] epoch 1895, training loss: 12512.33, average training loss: 13023.60, base loss: 19645.98
[INFO 2017-06-27 20:36:33,052 main.py:51] epoch 1896, training loss: 12085.67, average training loss: 13021.71, base loss: 19644.84
[INFO 2017-06-27 20:36:33,428 main.py:51] epoch 1897, training loss: 11939.66, average training loss: 13018.26, base loss: 19640.67
[INFO 2017-06-27 20:36:33,813 main.py:51] epoch 1898, training loss: 11726.38, average training loss: 13015.98, base loss: 19640.10
[INFO 2017-06-27 20:36:34,197 main.py:51] epoch 1899, training loss: 12085.94, average training loss: 13015.12, base loss: 19640.82
[INFO 2017-06-27 20:36:34,197 main.py:53] epoch 1899, testing
[INFO 2017-06-27 20:36:35,792 main.py:105] average testing loss: 12619.65, base loss: 19802.30
[INFO 2017-06-27 20:36:35,792 main.py:106] improve_loss: 7182.65, improve_percent: 0.36
[INFO 2017-06-27 20:36:35,793 main.py:76] current best improved percent: 0.37
[INFO 2017-06-27 20:36:36,168 main.py:51] epoch 1900, training loss: 13171.16, average training loss: 13014.84, base loss: 19642.61
[INFO 2017-06-27 20:36:36,543 main.py:51] epoch 1901, training loss: 11711.76, average training loss: 13011.22, base loss: 19638.05
[INFO 2017-06-27 20:36:36,922 main.py:51] epoch 1902, training loss: 12385.10, average training loss: 13010.91, base loss: 19641.91
[INFO 2017-06-27 20:36:37,307 main.py:51] epoch 1903, training loss: 12039.58, average training loss: 13009.01, base loss: 19640.75
[INFO 2017-06-27 20:36:37,684 main.py:51] epoch 1904, training loss: 12427.73, average training loss: 13006.43, base loss: 19637.80
[INFO 2017-06-27 20:36:38,060 main.py:51] epoch 1905, training loss: 12100.22, average training loss: 13002.89, base loss: 19635.32
[INFO 2017-06-27 20:36:38,525 main.py:51] epoch 1906, training loss: 12818.75, average training loss: 13002.65, base loss: 19638.64
[INFO 2017-06-27 20:36:38,922 main.py:51] epoch 1907, training loss: 12424.71, average training loss: 13000.87, base loss: 19637.06
[INFO 2017-06-27 20:36:39,299 main.py:51] epoch 1908, training loss: 11208.69, average training loss: 12999.74, base loss: 19637.23
[INFO 2017-06-27 20:36:39,735 main.py:51] epoch 1909, training loss: 12365.16, average training loss: 12996.48, base loss: 19633.16
[INFO 2017-06-27 20:36:40,149 main.py:51] epoch 1910, training loss: 11705.89, average training loss: 12992.74, base loss: 19630.05
[INFO 2017-06-27 20:36:40,532 main.py:51] epoch 1911, training loss: 11812.33, average training loss: 12991.73, base loss: 19630.55
[INFO 2017-06-27 20:36:40,922 main.py:51] epoch 1912, training loss: 12240.17, average training loss: 12988.32, base loss: 19627.63
[INFO 2017-06-27 20:36:41,329 main.py:51] epoch 1913, training loss: 11959.58, average training loss: 12987.73, base loss: 19629.39
[INFO 2017-06-27 20:36:41,705 main.py:51] epoch 1914, training loss: 11379.83, average training loss: 12984.78, base loss: 19626.54
[INFO 2017-06-27 20:36:42,085 main.py:51] epoch 1915, training loss: 12937.06, average training loss: 12985.00, base loss: 19629.28
[INFO 2017-06-27 20:36:42,465 main.py:51] epoch 1916, training loss: 15202.22, average training loss: 12984.64, base loss: 19632.03
[INFO 2017-06-27 20:36:42,844 main.py:51] epoch 1917, training loss: 11508.69, average training loss: 12983.00, base loss: 19633.22
[INFO 2017-06-27 20:36:43,221 main.py:51] epoch 1918, training loss: 12288.17, average training loss: 12982.38, base loss: 19634.51
[INFO 2017-06-27 20:36:43,596 main.py:51] epoch 1919, training loss: 13107.43, average training loss: 12980.08, base loss: 19632.80
[INFO 2017-06-27 20:36:43,975 main.py:51] epoch 1920, training loss: 12388.88, average training loss: 12977.55, base loss: 19631.18
[INFO 2017-06-27 20:36:44,360 main.py:51] epoch 1921, training loss: 10226.86, average training loss: 12976.52, base loss: 19630.94
[INFO 2017-06-27 20:36:44,745 main.py:51] epoch 1922, training loss: 12124.32, average training loss: 12975.35, base loss: 19629.83
[INFO 2017-06-27 20:36:45,129 main.py:51] epoch 1923, training loss: 11363.57, average training loss: 12973.23, base loss: 19627.87
[INFO 2017-06-27 20:36:45,508 main.py:51] epoch 1924, training loss: 12652.79, average training loss: 12973.59, base loss: 19630.46
[INFO 2017-06-27 20:36:45,896 main.py:51] epoch 1925, training loss: 11970.55, average training loss: 12972.17, base loss: 19630.05
[INFO 2017-06-27 20:36:46,271 main.py:51] epoch 1926, training loss: 13223.10, average training loss: 12972.14, base loss: 19631.87
[INFO 2017-06-27 20:36:46,645 main.py:51] epoch 1927, training loss: 10965.50, average training loss: 12966.87, base loss: 19625.45
[INFO 2017-06-27 20:36:47,023 main.py:51] epoch 1928, training loss: 12429.75, average training loss: 12965.05, base loss: 19624.51
[INFO 2017-06-27 20:36:47,403 main.py:51] epoch 1929, training loss: 13274.94, average training loss: 12964.39, base loss: 19625.79
[INFO 2017-06-27 20:36:47,791 main.py:51] epoch 1930, training loss: 14033.87, average training loss: 12965.18, base loss: 19629.79
[INFO 2017-06-27 20:36:48,169 main.py:51] epoch 1931, training loss: 12478.84, average training loss: 12963.15, base loss: 19628.83
[INFO 2017-06-27 20:36:48,551 main.py:51] epoch 1932, training loss: 11385.97, average training loss: 12961.14, base loss: 19629.72
[INFO 2017-06-27 20:36:48,928 main.py:51] epoch 1933, training loss: 12495.37, average training loss: 12961.83, base loss: 19634.65
[INFO 2017-06-27 20:36:49,305 main.py:51] epoch 1934, training loss: 12354.46, average training loss: 12961.94, base loss: 19635.61
[INFO 2017-06-27 20:36:49,680 main.py:51] epoch 1935, training loss: 13278.20, average training loss: 12962.74, base loss: 19640.78
[INFO 2017-06-27 20:36:50,057 main.py:51] epoch 1936, training loss: 13429.34, average training loss: 12962.25, base loss: 19641.26
[INFO 2017-06-27 20:36:50,443 main.py:51] epoch 1937, training loss: 11927.92, average training loss: 12960.63, base loss: 19641.91
[INFO 2017-06-27 20:36:50,819 main.py:51] epoch 1938, training loss: 12315.70, average training loss: 12958.04, base loss: 19640.01
[INFO 2017-06-27 20:36:51,297 main.py:51] epoch 1939, training loss: 12208.21, average training loss: 12956.12, base loss: 19638.95
[INFO 2017-06-27 20:36:51,674 main.py:51] epoch 1940, training loss: 11745.01, average training loss: 12954.48, base loss: 19637.79
[INFO 2017-06-27 20:36:52,057 main.py:51] epoch 1941, training loss: 12413.79, average training loss: 12951.19, base loss: 19633.14
[INFO 2017-06-27 20:36:52,451 main.py:51] epoch 1942, training loss: 14995.26, average training loss: 12953.05, base loss: 19638.45
[INFO 2017-06-27 20:36:52,836 main.py:51] epoch 1943, training loss: 11400.91, average training loss: 12951.19, base loss: 19637.64
[INFO 2017-06-27 20:36:53,214 main.py:51] epoch 1944, training loss: 13780.16, average training loss: 12951.85, base loss: 19641.16
[INFO 2017-06-27 20:36:53,592 main.py:51] epoch 1945, training loss: 11425.04, average training loss: 12948.67, base loss: 19637.95
[INFO 2017-06-27 20:36:53,978 main.py:51] epoch 1946, training loss: 11621.04, average training loss: 12947.19, base loss: 19637.27
[INFO 2017-06-27 20:36:54,356 main.py:51] epoch 1947, training loss: 12414.11, average training loss: 12943.67, base loss: 19634.45
[INFO 2017-06-27 20:36:54,733 main.py:51] epoch 1948, training loss: 12555.46, average training loss: 12942.20, base loss: 19633.43
[INFO 2017-06-27 20:36:55,116 main.py:51] epoch 1949, training loss: 12200.50, average training loss: 12939.64, base loss: 19632.19
[INFO 2017-06-27 20:36:55,492 main.py:51] epoch 1950, training loss: 12163.91, average training loss: 12938.95, base loss: 19632.76
[INFO 2017-06-27 20:36:55,875 main.py:51] epoch 1951, training loss: 11337.14, average training loss: 12936.46, base loss: 19631.06
[INFO 2017-06-27 20:36:56,252 main.py:51] epoch 1952, training loss: 12305.87, average training loss: 12935.73, base loss: 19632.05
[INFO 2017-06-27 20:36:56,630 main.py:51] epoch 1953, training loss: 15027.77, average training loss: 12938.01, base loss: 19636.89
[INFO 2017-06-27 20:36:57,007 main.py:51] epoch 1954, training loss: 12571.24, average training loss: 12938.11, base loss: 19639.29
[INFO 2017-06-27 20:36:57,383 main.py:51] epoch 1955, training loss: 12047.74, average training loss: 12936.86, base loss: 19639.58
[INFO 2017-06-27 20:36:57,760 main.py:51] epoch 1956, training loss: 13832.52, average training loss: 12937.69, base loss: 19642.68
[INFO 2017-06-27 20:36:58,137 main.py:51] epoch 1957, training loss: 11516.48, average training loss: 12934.52, base loss: 19638.06
[INFO 2017-06-27 20:36:58,514 main.py:51] epoch 1958, training loss: 13263.69, average training loss: 12935.22, base loss: 19641.27
[INFO 2017-06-27 20:36:58,894 main.py:51] epoch 1959, training loss: 11712.74, average training loss: 12933.65, base loss: 19641.64
[INFO 2017-06-27 20:36:59,270 main.py:51] epoch 1960, training loss: 11415.92, average training loss: 12929.82, base loss: 19638.15
[INFO 2017-06-27 20:36:59,647 main.py:51] epoch 1961, training loss: 12811.72, average training loss: 12929.27, base loss: 19638.74
[INFO 2017-06-27 20:37:00,022 main.py:51] epoch 1962, training loss: 12986.03, average training loss: 12930.04, base loss: 19642.01
[INFO 2017-06-27 20:37:00,399 main.py:51] epoch 1963, training loss: 13414.14, average training loss: 12929.20, base loss: 19642.71
[INFO 2017-06-27 20:37:00,779 main.py:51] epoch 1964, training loss: 11135.33, average training loss: 12927.20, base loss: 19640.71
[INFO 2017-06-27 20:37:01,156 main.py:51] epoch 1965, training loss: 12205.81, average training loss: 12925.60, base loss: 19638.36
[INFO 2017-06-27 20:37:01,533 main.py:51] epoch 1966, training loss: 13058.91, average training loss: 12925.23, base loss: 19640.14
[INFO 2017-06-27 20:37:01,909 main.py:51] epoch 1967, training loss: 14141.37, average training loss: 12925.09, base loss: 19642.15
[INFO 2017-06-27 20:37:02,289 main.py:51] epoch 1968, training loss: 15515.77, average training loss: 12926.46, base loss: 19645.93
[INFO 2017-06-27 20:37:02,666 main.py:51] epoch 1969, training loss: 11968.58, average training loss: 12924.90, base loss: 19645.92
[INFO 2017-06-27 20:37:03,049 main.py:51] epoch 1970, training loss: 11686.43, average training loss: 12923.38, base loss: 19645.84
[INFO 2017-06-27 20:37:03,426 main.py:51] epoch 1971, training loss: 12081.67, average training loss: 12922.19, base loss: 19646.80
[INFO 2017-06-27 20:37:03,802 main.py:51] epoch 1972, training loss: 11910.87, average training loss: 12922.44, base loss: 19650.30
[INFO 2017-06-27 20:37:04,184 main.py:51] epoch 1973, training loss: 11187.86, average training loss: 12920.37, base loss: 19648.30
[INFO 2017-06-27 20:37:04,561 main.py:51] epoch 1974, training loss: 13981.58, average training loss: 12921.68, base loss: 19652.33
[INFO 2017-06-27 20:37:04,937 main.py:51] epoch 1975, training loss: 13470.12, average training loss: 12921.38, base loss: 19654.18
[INFO 2017-06-27 20:37:05,313 main.py:51] epoch 1976, training loss: 13009.65, average training loss: 12921.48, base loss: 19655.33
[INFO 2017-06-27 20:37:05,690 main.py:51] epoch 1977, training loss: 11713.24, average training loss: 12921.77, base loss: 19656.85
[INFO 2017-06-27 20:37:06,067 main.py:51] epoch 1978, training loss: 13487.68, average training loss: 12919.65, base loss: 19655.52
[INFO 2017-06-27 20:37:06,443 main.py:51] epoch 1979, training loss: 11679.25, average training loss: 12919.24, base loss: 19656.35
[INFO 2017-06-27 20:37:06,817 main.py:51] epoch 1980, training loss: 11765.31, average training loss: 12916.69, base loss: 19653.22
[INFO 2017-06-27 20:37:07,200 main.py:51] epoch 1981, training loss: 11779.39, average training loss: 12915.06, base loss: 19652.71
[INFO 2017-06-27 20:37:07,577 main.py:51] epoch 1982, training loss: 11786.51, average training loss: 12913.32, base loss: 19650.24
[INFO 2017-06-27 20:37:07,955 main.py:51] epoch 1983, training loss: 12718.82, average training loss: 12912.27, base loss: 19652.20
[INFO 2017-06-27 20:37:08,331 main.py:51] epoch 1984, training loss: 11289.70, average training loss: 12911.01, base loss: 19652.84
[INFO 2017-06-27 20:37:08,708 main.py:51] epoch 1985, training loss: 11711.16, average training loss: 12908.77, base loss: 19651.67
[INFO 2017-06-27 20:37:09,089 main.py:51] epoch 1986, training loss: 13194.05, average training loss: 12909.30, base loss: 19652.66
[INFO 2017-06-27 20:37:09,466 main.py:51] epoch 1987, training loss: 11694.04, average training loss: 12909.25, base loss: 19654.09
[INFO 2017-06-27 20:37:09,843 main.py:51] epoch 1988, training loss: 12203.20, average training loss: 12906.52, base loss: 19650.92
[INFO 2017-06-27 20:37:10,227 main.py:51] epoch 1989, training loss: 11581.50, average training loss: 12905.79, base loss: 19651.21
[INFO 2017-06-27 20:37:10,603 main.py:51] epoch 1990, training loss: 13121.81, average training loss: 12906.27, base loss: 19654.25
[INFO 2017-06-27 20:37:10,980 main.py:51] epoch 1991, training loss: 11473.36, average training loss: 12904.93, base loss: 19652.92
[INFO 2017-06-27 20:37:11,357 main.py:51] epoch 1992, training loss: 12709.89, average training loss: 12905.73, base loss: 19655.62
[INFO 2017-06-27 20:37:11,732 main.py:51] epoch 1993, training loss: 10807.07, average training loss: 12902.04, base loss: 19650.33
[INFO 2017-06-27 20:37:12,109 main.py:51] epoch 1994, training loss: 13319.19, average training loss: 12903.19, base loss: 19654.39
[INFO 2017-06-27 20:37:12,491 main.py:51] epoch 1995, training loss: 12412.13, average training loss: 12901.37, base loss: 19653.01
[INFO 2017-06-27 20:37:12,867 main.py:51] epoch 1996, training loss: 12166.48, average training loss: 12898.55, base loss: 19649.18
[INFO 2017-06-27 20:37:13,243 main.py:51] epoch 1997, training loss: 12888.31, average training loss: 12895.78, base loss: 19646.21
[INFO 2017-06-27 20:37:13,626 main.py:51] epoch 1998, training loss: 15387.22, average training loss: 12897.58, base loss: 19650.83
[INFO 2017-06-27 20:37:14,002 main.py:51] epoch 1999, training loss: 12678.68, average training loss: 12897.47, base loss: 19653.92
[INFO 2017-06-27 20:37:14,002 main.py:53] epoch 1999, testing
[INFO 2017-06-27 20:37:15,602 main.py:105] average testing loss: 12897.26, base loss: 20401.57
[INFO 2017-06-27 20:37:15,602 main.py:106] improve_loss: 7504.31, improve_percent: 0.37
[INFO 2017-06-27 20:37:15,603 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:37:15,615 main.py:76] current best improved percent: 0.37
[INFO 2017-06-27 20:37:15,991 main.py:51] epoch 2000, training loss: 12307.34, average training loss: 12895.99, base loss: 19652.61
[INFO 2017-06-27 20:37:16,367 main.py:51] epoch 2001, training loss: 13079.12, average training loss: 12895.50, base loss: 19655.48
[INFO 2017-06-27 20:37:16,743 main.py:51] epoch 2002, training loss: 11130.18, average training loss: 12893.39, base loss: 19654.02
[INFO 2017-06-27 20:37:17,132 main.py:51] epoch 2003, training loss: 12689.72, average training loss: 12891.36, base loss: 19654.40
[INFO 2017-06-27 20:37:17,509 main.py:51] epoch 2004, training loss: 12032.86, average training loss: 12890.57, base loss: 19654.84
[INFO 2017-06-27 20:37:17,886 main.py:51] epoch 2005, training loss: 13886.96, average training loss: 12889.74, base loss: 19655.16
[INFO 2017-06-27 20:37:18,269 main.py:51] epoch 2006, training loss: 12964.19, average training loss: 12889.78, base loss: 19657.01
[INFO 2017-06-27 20:37:18,648 main.py:51] epoch 2007, training loss: 12119.31, average training loss: 12887.31, base loss: 19655.14
[INFO 2017-06-27 20:37:19,030 main.py:51] epoch 2008, training loss: 13111.62, average training loss: 12886.36, base loss: 19655.29
[INFO 2017-06-27 20:37:19,408 main.py:51] epoch 2009, training loss: 12699.28, average training loss: 12882.76, base loss: 19650.62
[INFO 2017-06-27 20:37:19,786 main.py:51] epoch 2010, training loss: 12321.04, average training loss: 12882.28, base loss: 19652.09
[INFO 2017-06-27 20:37:20,162 main.py:51] epoch 2011, training loss: 13685.60, average training loss: 12883.64, base loss: 19658.26
[INFO 2017-06-27 20:37:20,542 main.py:51] epoch 2012, training loss: 13671.94, average training loss: 12882.35, base loss: 19659.69
[INFO 2017-06-27 20:37:20,926 main.py:51] epoch 2013, training loss: 11925.59, average training loss: 12881.48, base loss: 19659.00
[INFO 2017-06-27 20:37:21,305 main.py:51] epoch 2014, training loss: 12473.20, average training loss: 12881.64, base loss: 19662.67
[INFO 2017-06-27 20:37:21,683 main.py:51] epoch 2015, training loss: 12620.24, average training loss: 12879.96, base loss: 19662.64
[INFO 2017-06-27 20:37:22,060 main.py:51] epoch 2016, training loss: 13189.51, average training loss: 12878.81, base loss: 19664.22
[INFO 2017-06-27 20:37:22,437 main.py:51] epoch 2017, training loss: 11918.97, average training loss: 12876.02, base loss: 19661.43
[INFO 2017-06-27 20:37:22,822 main.py:51] epoch 2018, training loss: 11591.50, average training loss: 12874.46, base loss: 19660.95
[INFO 2017-06-27 20:37:23,199 main.py:51] epoch 2019, training loss: 11721.33, average training loss: 12873.33, base loss: 19660.48
[INFO 2017-06-27 20:37:23,575 main.py:51] epoch 2020, training loss: 12721.01, average training loss: 12872.59, base loss: 19661.46
[INFO 2017-06-27 20:37:23,959 main.py:51] epoch 2021, training loss: 11874.12, average training loss: 12870.43, base loss: 19659.62
[INFO 2017-06-27 20:37:24,337 main.py:51] epoch 2022, training loss: 12281.25, average training loss: 12870.54, base loss: 19660.87
[INFO 2017-06-27 20:37:24,715 main.py:51] epoch 2023, training loss: 12411.67, average training loss: 12869.90, base loss: 19660.65
[INFO 2017-06-27 20:37:25,092 main.py:51] epoch 2024, training loss: 12367.68, average training loss: 12868.79, base loss: 19663.12
[INFO 2017-06-27 20:37:25,469 main.py:51] epoch 2025, training loss: 11933.82, average training loss: 12865.66, base loss: 19659.87
[INFO 2017-06-27 20:37:25,854 main.py:51] epoch 2026, training loss: 12354.12, average training loss: 12863.53, base loss: 19657.52
[INFO 2017-06-27 20:37:26,232 main.py:51] epoch 2027, training loss: 13841.75, average training loss: 12864.10, base loss: 19660.90
[INFO 2017-06-27 20:37:26,610 main.py:51] epoch 2028, training loss: 12543.66, average training loss: 12863.54, base loss: 19662.65
[INFO 2017-06-27 20:37:26,986 main.py:51] epoch 2029, training loss: 11452.29, average training loss: 12861.14, base loss: 19659.82
[INFO 2017-06-27 20:37:27,364 main.py:51] epoch 2030, training loss: 11328.61, average training loss: 12859.27, base loss: 19658.02
[INFO 2017-06-27 20:37:27,742 main.py:51] epoch 2031, training loss: 10471.23, average training loss: 12857.91, base loss: 19656.85
[INFO 2017-06-27 20:37:28,119 main.py:51] epoch 2032, training loss: 11809.21, average training loss: 12857.23, base loss: 19657.00
[INFO 2017-06-27 20:37:28,497 main.py:51] epoch 2033, training loss: 13157.17, average training loss: 12856.47, base loss: 19657.61
[INFO 2017-06-27 20:37:28,874 main.py:51] epoch 2034, training loss: 12526.38, average training loss: 12858.53, base loss: 19663.99
[INFO 2017-06-27 20:37:29,258 main.py:51] epoch 2035, training loss: 12416.45, average training loss: 12857.95, base loss: 19664.18
[INFO 2017-06-27 20:37:29,635 main.py:51] epoch 2036, training loss: 11782.47, average training loss: 12855.70, base loss: 19661.11
[INFO 2017-06-27 20:37:30,018 main.py:51] epoch 2037, training loss: 11486.39, average training loss: 12853.23, base loss: 19659.08
[INFO 2017-06-27 20:37:30,395 main.py:51] epoch 2038, training loss: 11332.37, average training loss: 12852.55, base loss: 19659.94
[INFO 2017-06-27 20:37:30,773 main.py:51] epoch 2039, training loss: 11259.51, average training loss: 12847.72, base loss: 19652.29
[INFO 2017-06-27 20:37:31,150 main.py:51] epoch 2040, training loss: 11551.22, average training loss: 12845.39, base loss: 19649.53
[INFO 2017-06-27 20:37:31,531 main.py:51] epoch 2041, training loss: 14518.15, average training loss: 12846.56, base loss: 19655.01
[INFO 2017-06-27 20:37:31,910 main.py:51] epoch 2042, training loss: 10477.35, average training loss: 12844.23, base loss: 19652.70
[INFO 2017-06-27 20:37:32,287 main.py:51] epoch 2043, training loss: 12956.95, average training loss: 12843.35, base loss: 19652.55
[INFO 2017-06-27 20:37:32,672 main.py:51] epoch 2044, training loss: 12619.68, average training loss: 12843.00, base loss: 19653.46
[INFO 2017-06-27 20:37:33,054 main.py:51] epoch 2045, training loss: 11985.53, average training loss: 12842.40, base loss: 19655.78
[INFO 2017-06-27 20:37:33,431 main.py:51] epoch 2046, training loss: 10107.25, average training loss: 12838.49, base loss: 19649.54
[INFO 2017-06-27 20:37:33,808 main.py:51] epoch 2047, training loss: 12351.61, average training loss: 12836.40, base loss: 19647.96
[INFO 2017-06-27 20:37:34,185 main.py:51] epoch 2048, training loss: 12752.78, average training loss: 12835.83, base loss: 19647.95
[INFO 2017-06-27 20:37:34,645 main.py:51] epoch 2049, training loss: 14797.29, average training loss: 12839.04, base loss: 19655.70
[INFO 2017-06-27 20:37:35,052 main.py:51] epoch 2050, training loss: 12162.57, average training loss: 12834.13, base loss: 19651.08
[INFO 2017-06-27 20:37:35,440 main.py:51] epoch 2051, training loss: 13276.51, average training loss: 12835.49, base loss: 19656.06
[INFO 2017-06-27 20:37:35,843 main.py:51] epoch 2052, training loss: 13106.08, average training loss: 12836.16, base loss: 19658.72
[INFO 2017-06-27 20:37:36,223 main.py:51] epoch 2053, training loss: 10651.74, average training loss: 12833.94, base loss: 19656.73
[INFO 2017-06-27 20:37:36,620 main.py:51] epoch 2054, training loss: 13335.26, average training loss: 12834.71, base loss: 19662.61
[INFO 2017-06-27 20:37:37,004 main.py:51] epoch 2055, training loss: 12308.27, average training loss: 12832.02, base loss: 19660.26
[INFO 2017-06-27 20:37:37,381 main.py:51] epoch 2056, training loss: 13015.28, average training loss: 12831.92, base loss: 19662.28
[INFO 2017-06-27 20:37:37,759 main.py:51] epoch 2057, training loss: 11328.97, average training loss: 12829.32, base loss: 19658.99
[INFO 2017-06-27 20:37:38,137 main.py:51] epoch 2058, training loss: 13421.35, average training loss: 12828.58, base loss: 19660.12
[INFO 2017-06-27 20:37:38,522 main.py:51] epoch 2059, training loss: 10730.97, average training loss: 12827.60, base loss: 19659.50
[INFO 2017-06-27 20:37:38,898 main.py:51] epoch 2060, training loss: 12424.55, average training loss: 12826.60, base loss: 19659.84
[INFO 2017-06-27 20:37:39,282 main.py:51] epoch 2061, training loss: 13080.37, average training loss: 12826.60, base loss: 19661.80
[INFO 2017-06-27 20:37:39,659 main.py:51] epoch 2062, training loss: 10495.41, average training loss: 12824.13, base loss: 19659.16
[INFO 2017-06-27 20:37:40,039 main.py:51] epoch 2063, training loss: 11939.09, average training loss: 12822.78, base loss: 19658.88
[INFO 2017-06-27 20:37:40,414 main.py:51] epoch 2064, training loss: 11454.34, average training loss: 12821.65, base loss: 19659.27
[INFO 2017-06-27 20:37:40,797 main.py:51] epoch 2065, training loss: 13043.68, average training loss: 12822.67, base loss: 19662.69
[INFO 2017-06-27 20:37:41,182 main.py:51] epoch 2066, training loss: 12910.92, average training loss: 12822.72, base loss: 19663.42
[INFO 2017-06-27 20:37:41,565 main.py:51] epoch 2067, training loss: 13149.64, average training loss: 12824.19, base loss: 19668.34
[INFO 2017-06-27 20:37:41,947 main.py:51] epoch 2068, training loss: 12114.13, average training loss: 12821.98, base loss: 19665.34
[INFO 2017-06-27 20:37:42,323 main.py:51] epoch 2069, training loss: 11356.35, average training loss: 12818.98, base loss: 19662.89
[INFO 2017-06-27 20:37:42,706 main.py:51] epoch 2070, training loss: 12384.03, average training loss: 12814.88, base loss: 19658.12
[INFO 2017-06-27 20:37:43,087 main.py:51] epoch 2071, training loss: 10308.95, average training loss: 12810.29, base loss: 19651.10
[INFO 2017-06-27 20:37:43,469 main.py:51] epoch 2072, training loss: 12188.04, average training loss: 12811.01, base loss: 19655.12
[INFO 2017-06-27 20:37:43,849 main.py:51] epoch 2073, training loss: 12174.92, average training loss: 12808.61, base loss: 19652.91
[INFO 2017-06-27 20:37:44,231 main.py:51] epoch 2074, training loss: 11889.58, average training loss: 12805.89, base loss: 19651.45
[INFO 2017-06-27 20:37:44,609 main.py:51] epoch 2075, training loss: 12118.58, average training loss: 12804.34, base loss: 19650.61
[INFO 2017-06-27 20:37:44,987 main.py:51] epoch 2076, training loss: 12900.50, average training loss: 12802.91, base loss: 19650.13
[INFO 2017-06-27 20:37:45,370 main.py:51] epoch 2077, training loss: 12439.45, average training loss: 12802.23, base loss: 19650.29
[INFO 2017-06-27 20:37:45,751 main.py:51] epoch 2078, training loss: 12678.45, average training loss: 12801.25, base loss: 19651.15
[INFO 2017-06-27 20:37:46,138 main.py:51] epoch 2079, training loss: 11206.98, average training loss: 12799.41, base loss: 19649.94
[INFO 2017-06-27 20:37:46,518 main.py:51] epoch 2080, training loss: 12148.35, average training loss: 12794.56, base loss: 19643.41
[INFO 2017-06-27 20:37:46,896 main.py:51] epoch 2081, training loss: 12377.71, average training loss: 12795.33, base loss: 19647.32
[INFO 2017-06-27 20:37:47,276 main.py:51] epoch 2082, training loss: 12580.82, average training loss: 12794.08, base loss: 19645.05
[INFO 2017-06-27 20:37:47,661 main.py:51] epoch 2083, training loss: 12541.00, average training loss: 12792.51, base loss: 19643.37
[INFO 2017-06-27 20:37:48,042 main.py:51] epoch 2084, training loss: 11687.77, average training loss: 12789.94, base loss: 19640.47
[INFO 2017-06-27 20:37:48,423 main.py:51] epoch 2085, training loss: 12158.07, average training loss: 12788.68, base loss: 19640.00
[INFO 2017-06-27 20:37:48,805 main.py:51] epoch 2086, training loss: 12715.17, average training loss: 12788.26, base loss: 19640.29
[INFO 2017-06-27 20:37:49,189 main.py:51] epoch 2087, training loss: 13128.34, average training loss: 12787.86, base loss: 19642.36
[INFO 2017-06-27 20:37:49,566 main.py:51] epoch 2088, training loss: 11440.34, average training loss: 12787.61, base loss: 19644.25
[INFO 2017-06-27 20:37:49,946 main.py:51] epoch 2089, training loss: 11718.83, average training loss: 12786.30, base loss: 19644.27
[INFO 2017-06-27 20:37:50,328 main.py:51] epoch 2090, training loss: 13120.52, average training loss: 12787.29, base loss: 19648.73
[INFO 2017-06-27 20:37:50,714 main.py:51] epoch 2091, training loss: 13108.41, average training loss: 12787.53, base loss: 19651.30
[INFO 2017-06-27 20:37:51,094 main.py:51] epoch 2092, training loss: 11736.70, average training loss: 12784.97, base loss: 19648.49
[INFO 2017-06-27 20:37:51,478 main.py:51] epoch 2093, training loss: 11794.98, average training loss: 12782.38, base loss: 19645.44
[INFO 2017-06-27 20:37:51,864 main.py:51] epoch 2094, training loss: 13590.88, average training loss: 12782.37, base loss: 19646.75
[INFO 2017-06-27 20:37:52,244 main.py:51] epoch 2095, training loss: 14167.49, average training loss: 12783.71, base loss: 19653.70
[INFO 2017-06-27 20:37:52,621 main.py:51] epoch 2096, training loss: 12429.75, average training loss: 12781.67, base loss: 19653.15
[INFO 2017-06-27 20:37:52,998 main.py:51] epoch 2097, training loss: 13173.56, average training loss: 12783.15, base loss: 19658.17
[INFO 2017-06-27 20:37:53,377 main.py:51] epoch 2098, training loss: 11365.28, average training loss: 12777.52, base loss: 19650.19
[INFO 2017-06-27 20:37:53,756 main.py:51] epoch 2099, training loss: 10369.11, average training loss: 12774.46, base loss: 19646.82
[INFO 2017-06-27 20:37:53,756 main.py:53] epoch 2099, testing
[INFO 2017-06-27 20:37:55,376 main.py:105] average testing loss: 12265.56, base loss: 19685.16
[INFO 2017-06-27 20:37:55,376 main.py:106] improve_loss: 7419.60, improve_percent: 0.38
[INFO 2017-06-27 20:37:55,377 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:37:55,389 main.py:76] current best improved percent: 0.38
[INFO 2017-06-27 20:37:55,767 main.py:51] epoch 2100, training loss: 13186.10, average training loss: 12775.64, base loss: 19650.91
[INFO 2017-06-27 20:37:56,147 main.py:51] epoch 2101, training loss: 11576.93, average training loss: 12773.18, base loss: 19648.87
[INFO 2017-06-27 20:37:56,525 main.py:51] epoch 2102, training loss: 13527.12, average training loss: 12772.92, base loss: 19650.19
[INFO 2017-06-27 20:37:56,899 main.py:51] epoch 2103, training loss: 12633.68, average training loss: 12773.98, base loss: 19654.32
[INFO 2017-06-27 20:37:57,274 main.py:51] epoch 2104, training loss: 13089.24, average training loss: 12773.70, base loss: 19655.34
[INFO 2017-06-27 20:37:57,699 main.py:51] epoch 2105, training loss: 11245.82, average training loss: 12772.62, base loss: 19657.33
[INFO 2017-06-27 20:37:58,084 main.py:51] epoch 2106, training loss: 13229.00, average training loss: 12773.17, base loss: 19660.96
[INFO 2017-06-27 20:37:58,465 main.py:51] epoch 2107, training loss: 11412.87, average training loss: 12770.91, base loss: 19656.63
[INFO 2017-06-27 20:37:58,844 main.py:51] epoch 2108, training loss: 10976.76, average training loss: 12769.40, base loss: 19654.73
[INFO 2017-06-27 20:37:59,240 main.py:51] epoch 2109, training loss: 11302.25, average training loss: 12767.25, base loss: 19652.47
[INFO 2017-06-27 20:37:59,618 main.py:51] epoch 2110, training loss: 11310.23, average training loss: 12766.00, base loss: 19653.65
[INFO 2017-06-27 20:37:59,997 main.py:51] epoch 2111, training loss: 10996.16, average training loss: 12763.21, base loss: 19650.63
[INFO 2017-06-27 20:38:00,375 main.py:51] epoch 2112, training loss: 11235.77, average training loss: 12759.05, base loss: 19645.77
[INFO 2017-06-27 20:38:00,751 main.py:51] epoch 2113, training loss: 10025.80, average training loss: 12753.03, base loss: 19638.55
[INFO 2017-06-27 20:38:01,126 main.py:51] epoch 2114, training loss: 11826.78, average training loss: 12753.60, base loss: 19642.16
[INFO 2017-06-27 20:38:01,510 main.py:51] epoch 2115, training loss: 11548.11, average training loss: 12749.84, base loss: 19636.70
[INFO 2017-06-27 20:38:01,889 main.py:51] epoch 2116, training loss: 13446.52, average training loss: 12750.21, base loss: 19639.41
[INFO 2017-06-27 20:38:02,267 main.py:51] epoch 2117, training loss: 13603.37, average training loss: 12751.21, base loss: 19642.99
[INFO 2017-06-27 20:38:02,653 main.py:51] epoch 2118, training loss: 13190.38, average training loss: 12751.27, base loss: 19645.44
[INFO 2017-06-27 20:38:03,046 main.py:51] epoch 2119, training loss: 12379.40, average training loss: 12750.68, base loss: 19646.01
[INFO 2017-06-27 20:38:03,434 main.py:51] epoch 2120, training loss: 13403.50, average training loss: 12751.93, base loss: 19650.50
[INFO 2017-06-27 20:38:03,822 main.py:51] epoch 2121, training loss: 13386.81, average training loss: 12750.72, base loss: 19649.86
[INFO 2017-06-27 20:38:04,200 main.py:51] epoch 2122, training loss: 12026.56, average training loss: 12751.11, base loss: 19652.19
[INFO 2017-06-27 20:38:04,586 main.py:51] epoch 2123, training loss: 14127.32, average training loss: 12752.02, base loss: 19655.99
[INFO 2017-06-27 20:38:04,969 main.py:51] epoch 2124, training loss: 12250.53, average training loss: 12750.89, base loss: 19656.28
[INFO 2017-06-27 20:38:05,349 main.py:51] epoch 2125, training loss: 11678.69, average training loss: 12748.75, base loss: 19653.20
[INFO 2017-06-27 20:38:05,727 main.py:51] epoch 2126, training loss: 10743.96, average training loss: 12746.12, base loss: 19650.13
[INFO 2017-06-27 20:38:06,106 main.py:51] epoch 2127, training loss: 9855.88, average training loss: 12740.66, base loss: 19642.47
[INFO 2017-06-27 20:38:06,494 main.py:51] epoch 2128, training loss: 12226.93, average training loss: 12739.80, base loss: 19643.02
[INFO 2017-06-27 20:38:06,877 main.py:51] epoch 2129, training loss: 13543.25, average training loss: 12740.19, base loss: 19645.84
[INFO 2017-06-27 20:38:07,268 main.py:51] epoch 2130, training loss: 11776.01, average training loss: 12739.76, base loss: 19646.70
[INFO 2017-06-27 20:38:07,653 main.py:51] epoch 2131, training loss: 11735.78, average training loss: 12736.63, base loss: 19643.66
[INFO 2017-06-27 20:38:08,040 main.py:51] epoch 2132, training loss: 13794.74, average training loss: 12737.81, base loss: 19649.63
[INFO 2017-06-27 20:38:08,420 main.py:51] epoch 2133, training loss: 11663.62, average training loss: 12737.02, base loss: 19649.94
[INFO 2017-06-27 20:38:08,803 main.py:51] epoch 2134, training loss: 11591.15, average training loss: 12732.81, base loss: 19644.34
[INFO 2017-06-27 20:38:09,186 main.py:51] epoch 2135, training loss: 11685.06, average training loss: 12730.58, base loss: 19642.93
[INFO 2017-06-27 20:38:09,576 main.py:51] epoch 2136, training loss: 12298.00, average training loss: 12730.59, base loss: 19644.39
[INFO 2017-06-27 20:38:09,971 main.py:51] epoch 2137, training loss: 11415.17, average training loss: 12727.71, base loss: 19640.18
[INFO 2017-06-27 20:38:10,352 main.py:51] epoch 2138, training loss: 9952.60, average training loss: 12725.73, base loss: 19637.01
[INFO 2017-06-27 20:38:10,732 main.py:51] epoch 2139, training loss: 12784.12, average training loss: 12724.63, base loss: 19638.17
[INFO 2017-06-27 20:38:11,108 main.py:51] epoch 2140, training loss: 11012.73, average training loss: 12722.44, base loss: 19637.00
[INFO 2017-06-27 20:38:11,481 main.py:51] epoch 2141, training loss: 12274.93, average training loss: 12720.16, base loss: 19635.05
[INFO 2017-06-27 20:38:11,857 main.py:51] epoch 2142, training loss: 12385.51, average training loss: 12717.14, base loss: 19631.90
[INFO 2017-06-27 20:38:12,231 main.py:51] epoch 2143, training loss: 12529.21, average training loss: 12716.59, base loss: 19634.30
[INFO 2017-06-27 20:38:12,613 main.py:51] epoch 2144, training loss: 10793.59, average training loss: 12715.42, base loss: 19634.06
[INFO 2017-06-27 20:38:12,993 main.py:51] epoch 2145, training loss: 11959.73, average training loss: 12714.69, base loss: 19633.73
[INFO 2017-06-27 20:38:13,367 main.py:51] epoch 2146, training loss: 10510.88, average training loss: 12711.57, base loss: 19629.64
[INFO 2017-06-27 20:38:13,743 main.py:51] epoch 2147, training loss: 12226.75, average training loss: 12710.30, base loss: 19629.90
[INFO 2017-06-27 20:38:14,119 main.py:51] epoch 2148, training loss: 12244.86, average training loss: 12705.48, base loss: 19624.64
[INFO 2017-06-27 20:38:14,495 main.py:51] epoch 2149, training loss: 12572.27, average training loss: 12704.25, base loss: 19623.61
[INFO 2017-06-27 20:38:14,871 main.py:51] epoch 2150, training loss: 12124.14, average training loss: 12700.94, base loss: 19620.93
[INFO 2017-06-27 20:38:15,253 main.py:51] epoch 2151, training loss: 12249.43, average training loss: 12700.77, base loss: 19621.38
[INFO 2017-06-27 20:38:15,632 main.py:51] epoch 2152, training loss: 11111.31, average training loss: 12698.49, base loss: 19618.92
[INFO 2017-06-27 20:38:16,008 main.py:51] epoch 2153, training loss: 12820.22, average training loss: 12697.96, base loss: 19619.81
[INFO 2017-06-27 20:38:16,385 main.py:51] epoch 2154, training loss: 11829.58, average training loss: 12697.51, base loss: 19619.91
[INFO 2017-06-27 20:38:16,759 main.py:51] epoch 2155, training loss: 12611.99, average training loss: 12694.75, base loss: 19616.46
[INFO 2017-06-27 20:38:17,133 main.py:51] epoch 2156, training loss: 12723.57, average training loss: 12693.58, base loss: 19615.81
[INFO 2017-06-27 20:38:17,592 main.py:51] epoch 2157, training loss: 10531.08, average training loss: 12690.87, base loss: 19612.98
[INFO 2017-06-27 20:38:17,993 main.py:51] epoch 2158, training loss: 12270.13, average training loss: 12688.38, base loss: 19610.78
[INFO 2017-06-27 20:38:18,377 main.py:51] epoch 2159, training loss: 12716.24, average training loss: 12687.83, base loss: 19612.17
[INFO 2017-06-27 20:38:18,758 main.py:51] epoch 2160, training loss: 12509.79, average training loss: 12687.32, base loss: 19613.45
[INFO 2017-06-27 20:38:19,136 main.py:51] epoch 2161, training loss: 12799.08, average training loss: 12685.07, base loss: 19611.08
[INFO 2017-06-27 20:38:19,549 main.py:51] epoch 2162, training loss: 11445.52, average training loss: 12684.45, base loss: 19611.86
[INFO 2017-06-27 20:38:19,923 main.py:51] epoch 2163, training loss: 11269.90, average training loss: 12682.13, base loss: 19608.54
[INFO 2017-06-27 20:38:20,305 main.py:51] epoch 2164, training loss: 10536.77, average training loss: 12679.67, base loss: 19605.53
[INFO 2017-06-27 20:38:20,683 main.py:51] epoch 2165, training loss: 12194.92, average training loss: 12678.54, base loss: 19605.59
[INFO 2017-06-27 20:38:21,062 main.py:51] epoch 2166, training loss: 11878.58, average training loss: 12677.69, base loss: 19605.99
[INFO 2017-06-27 20:38:21,441 main.py:51] epoch 2167, training loss: 11980.86, average training loss: 12676.26, base loss: 19604.54
[INFO 2017-06-27 20:38:21,819 main.py:51] epoch 2168, training loss: 12604.85, average training loss: 12674.24, base loss: 19602.27
[INFO 2017-06-27 20:38:22,199 main.py:51] epoch 2169, training loss: 12092.14, average training loss: 12674.45, base loss: 19603.65
[INFO 2017-06-27 20:38:22,579 main.py:51] epoch 2170, training loss: 12526.42, average training loss: 12673.16, base loss: 19604.99
[INFO 2017-06-27 20:38:22,957 main.py:51] epoch 2171, training loss: 11989.63, average training loss: 12672.46, base loss: 19605.83
[INFO 2017-06-27 20:38:23,333 main.py:51] epoch 2172, training loss: 11378.81, average training loss: 12673.03, base loss: 19608.73
[INFO 2017-06-27 20:38:23,715 main.py:51] epoch 2173, training loss: 11676.68, average training loss: 12671.15, base loss: 19608.32
[INFO 2017-06-27 20:38:24,098 main.py:51] epoch 2174, training loss: 11651.80, average training loss: 12668.52, base loss: 19605.67
[INFO 2017-06-27 20:38:24,478 main.py:51] epoch 2175, training loss: 12978.30, average training loss: 12668.29, base loss: 19606.84
[INFO 2017-06-27 20:38:24,871 main.py:51] epoch 2176, training loss: 13111.57, average training loss: 12667.23, base loss: 19607.12
[INFO 2017-06-27 20:38:25,253 main.py:51] epoch 2177, training loss: 11234.78, average training loss: 12664.44, base loss: 19603.90
[INFO 2017-06-27 20:38:25,637 main.py:51] epoch 2178, training loss: 12330.44, average training loss: 12663.94, base loss: 19606.35
[INFO 2017-06-27 20:38:26,019 main.py:51] epoch 2179, training loss: 13136.23, average training loss: 12665.12, base loss: 19609.76
[INFO 2017-06-27 20:38:26,401 main.py:51] epoch 2180, training loss: 11617.33, average training loss: 12664.66, base loss: 19610.66
[INFO 2017-06-27 20:38:26,779 main.py:51] epoch 2181, training loss: 13482.02, average training loss: 12664.23, base loss: 19611.03
[INFO 2017-06-27 20:38:27,156 main.py:51] epoch 2182, training loss: 10936.44, average training loss: 12661.58, base loss: 19608.16
[INFO 2017-06-27 20:38:27,534 main.py:51] epoch 2183, training loss: 9861.27, average training loss: 12655.29, base loss: 19598.58
[INFO 2017-06-27 20:38:27,912 main.py:51] epoch 2184, training loss: 11881.02, average training loss: 12653.20, base loss: 19597.10
[INFO 2017-06-27 20:38:28,293 main.py:51] epoch 2185, training loss: 10551.77, average training loss: 12651.21, base loss: 19596.12
[INFO 2017-06-27 20:38:28,669 main.py:51] epoch 2186, training loss: 14872.60, average training loss: 12654.69, base loss: 19604.43
[INFO 2017-06-27 20:38:29,044 main.py:51] epoch 2187, training loss: 12897.89, average training loss: 12654.47, base loss: 19606.50
[INFO 2017-06-27 20:38:29,424 main.py:51] epoch 2188, training loss: 13102.81, average training loss: 12654.09, base loss: 19608.27
[INFO 2017-06-27 20:38:29,797 main.py:51] epoch 2189, training loss: 10566.62, average training loss: 12650.12, base loss: 19602.84
[INFO 2017-06-27 20:38:30,177 main.py:51] epoch 2190, training loss: 12036.31, average training loss: 12648.90, base loss: 19602.93
[INFO 2017-06-27 20:38:30,561 main.py:51] epoch 2191, training loss: 13512.96, average training loss: 12649.67, base loss: 19607.32
[INFO 2017-06-27 20:38:30,943 main.py:51] epoch 2192, training loss: 13400.66, average training loss: 12649.41, base loss: 19610.04
[INFO 2017-06-27 20:38:31,321 main.py:51] epoch 2193, training loss: 12669.78, average training loss: 12649.01, base loss: 19609.97
[INFO 2017-06-27 20:38:31,699 main.py:51] epoch 2194, training loss: 11321.96, average training loss: 12648.00, base loss: 19609.86
[INFO 2017-06-27 20:38:32,084 main.py:51] epoch 2195, training loss: 11838.66, average training loss: 12645.49, base loss: 19608.04
[INFO 2017-06-27 20:38:32,460 main.py:51] epoch 2196, training loss: 12061.22, average training loss: 12643.70, base loss: 19608.91
[INFO 2017-06-27 20:38:32,841 main.py:51] epoch 2197, training loss: 11025.74, average training loss: 12642.31, base loss: 19609.37
[INFO 2017-06-27 20:38:33,223 main.py:51] epoch 2198, training loss: 13396.91, average training loss: 12641.22, base loss: 19608.30
[INFO 2017-06-27 20:38:33,604 main.py:51] epoch 2199, training loss: 12318.03, average training loss: 12642.04, base loss: 19610.59
[INFO 2017-06-27 20:38:33,604 main.py:53] epoch 2199, testing
[INFO 2017-06-27 20:38:35,216 main.py:105] average testing loss: 12222.37, base loss: 19548.27
[INFO 2017-06-27 20:38:35,216 main.py:106] improve_loss: 7325.89, improve_percent: 0.37
[INFO 2017-06-27 20:38:35,216 main.py:76] current best improved percent: 0.38
[INFO 2017-06-27 20:38:35,592 main.py:51] epoch 2200, training loss: 12177.14, average training loss: 12641.54, base loss: 19609.66
[INFO 2017-06-27 20:38:35,968 main.py:51] epoch 2201, training loss: 11774.06, average training loss: 12641.78, base loss: 19613.04
[INFO 2017-06-27 20:38:36,350 main.py:51] epoch 2202, training loss: 13627.72, average training loss: 12643.94, base loss: 19618.18
[INFO 2017-06-27 20:38:36,726 main.py:51] epoch 2203, training loss: 12645.31, average training loss: 12642.80, base loss: 19618.19
[INFO 2017-06-27 20:38:37,101 main.py:51] epoch 2204, training loss: 11883.98, average training loss: 12639.19, base loss: 19613.74
[INFO 2017-06-27 20:38:37,477 main.py:51] epoch 2205, training loss: 10753.78, average training loss: 12637.00, base loss: 19610.51
[INFO 2017-06-27 20:38:37,854 main.py:51] epoch 2206, training loss: 11657.52, average training loss: 12634.09, base loss: 19607.45
[INFO 2017-06-27 20:38:38,227 main.py:51] epoch 2207, training loss: 11751.38, average training loss: 12634.04, base loss: 19608.57
[INFO 2017-06-27 20:38:38,601 main.py:51] epoch 2208, training loss: 12709.31, average training loss: 12633.16, base loss: 19608.93
[INFO 2017-06-27 20:38:38,981 main.py:51] epoch 2209, training loss: 14565.26, average training loss: 12633.52, base loss: 19610.77
[INFO 2017-06-27 20:38:39,358 main.py:51] epoch 2210, training loss: 11345.33, average training loss: 12632.69, base loss: 19610.75
[INFO 2017-06-27 20:38:39,735 main.py:51] epoch 2211, training loss: 13899.45, average training loss: 12634.67, base loss: 19615.47
[INFO 2017-06-27 20:38:40,111 main.py:51] epoch 2212, training loss: 11596.66, average training loss: 12632.33, base loss: 19612.95
[INFO 2017-06-27 20:38:40,483 main.py:51] epoch 2213, training loss: 14358.64, average training loss: 12633.71, base loss: 19617.33
[INFO 2017-06-27 20:38:40,867 main.py:51] epoch 2214, training loss: 11867.42, average training loss: 12630.75, base loss: 19614.30
[INFO 2017-06-27 20:38:41,243 main.py:51] epoch 2215, training loss: 12000.18, average training loss: 12629.53, base loss: 19613.85
[INFO 2017-06-27 20:38:41,615 main.py:51] epoch 2216, training loss: 12229.32, average training loss: 12629.15, base loss: 19615.15
[INFO 2017-06-27 20:38:41,987 main.py:51] epoch 2217, training loss: 12572.35, average training loss: 12626.83, base loss: 19611.49
[INFO 2017-06-27 20:38:42,361 main.py:51] epoch 2218, training loss: 12836.27, average training loss: 12624.67, base loss: 19611.33
[INFO 2017-06-27 20:38:42,737 main.py:51] epoch 2219, training loss: 11633.60, average training loss: 12622.33, base loss: 19607.81
[INFO 2017-06-27 20:38:43,109 main.py:51] epoch 2220, training loss: 11909.87, average training loss: 12622.25, base loss: 19609.37
[INFO 2017-06-27 20:38:43,480 main.py:51] epoch 2221, training loss: 10663.44, average training loss: 12619.55, base loss: 19606.91
[INFO 2017-06-27 20:38:43,858 main.py:51] epoch 2222, training loss: 11486.20, average training loss: 12615.30, base loss: 19601.23
[INFO 2017-06-27 20:38:44,242 main.py:51] epoch 2223, training loss: 13080.38, average training loss: 12616.80, base loss: 19605.37
[INFO 2017-06-27 20:38:44,617 main.py:51] epoch 2224, training loss: 12375.75, average training loss: 12615.12, base loss: 19605.13
[INFO 2017-06-27 20:38:44,991 main.py:51] epoch 2225, training loss: 12393.83, average training loss: 12613.22, base loss: 19604.34
[INFO 2017-06-27 20:38:45,368 main.py:51] epoch 2226, training loss: 11148.41, average training loss: 12610.93, base loss: 19601.81
[INFO 2017-06-27 20:38:45,744 main.py:51] epoch 2227, training loss: 12690.92, average training loss: 12611.17, base loss: 19604.32
[INFO 2017-06-27 20:38:46,118 main.py:51] epoch 2228, training loss: 12948.93, average training loss: 12609.78, base loss: 19604.58
[INFO 2017-06-27 20:38:46,491 main.py:51] epoch 2229, training loss: 11892.20, average training loss: 12609.02, base loss: 19605.44
[INFO 2017-06-27 20:38:46,874 main.py:51] epoch 2230, training loss: 13591.63, average training loss: 12609.78, base loss: 19608.29
[INFO 2017-06-27 20:38:47,257 main.py:51] epoch 2231, training loss: 11630.62, average training loss: 12607.00, base loss: 19605.63
[INFO 2017-06-27 20:38:47,630 main.py:51] epoch 2232, training loss: 12236.83, average training loss: 12605.80, base loss: 19605.24
[INFO 2017-06-27 20:38:48,009 main.py:51] epoch 2233, training loss: 11198.16, average training loss: 12604.26, base loss: 19604.01
[INFO 2017-06-27 20:38:48,385 main.py:51] epoch 2234, training loss: 12324.11, average training loss: 12602.56, base loss: 19601.87
[INFO 2017-06-27 20:38:48,762 main.py:51] epoch 2235, training loss: 11912.38, average training loss: 12599.02, base loss: 19597.27
[INFO 2017-06-27 20:38:49,141 main.py:51] epoch 2236, training loss: 12092.60, average training loss: 12599.28, base loss: 19599.62
[INFO 2017-06-27 20:38:49,513 main.py:51] epoch 2237, training loss: 11957.13, average training loss: 12599.56, base loss: 19603.07
[INFO 2017-06-27 20:38:49,889 main.py:51] epoch 2238, training loss: 14665.73, average training loss: 12600.03, base loss: 19604.71
[INFO 2017-06-27 20:38:50,264 main.py:51] epoch 2239, training loss: 12099.16, average training loss: 12599.36, base loss: 19605.46
[INFO 2017-06-27 20:38:50,649 main.py:51] epoch 2240, training loss: 11636.58, average training loss: 12597.74, base loss: 19603.90
[INFO 2017-06-27 20:38:51,025 main.py:51] epoch 2241, training loss: 12689.19, average training loss: 12596.44, base loss: 19603.35
[INFO 2017-06-27 20:38:51,404 main.py:51] epoch 2242, training loss: 10979.59, average training loss: 12595.90, base loss: 19604.01
[INFO 2017-06-27 20:38:51,781 main.py:51] epoch 2243, training loss: 10446.06, average training loss: 12593.51, base loss: 19600.51
[INFO 2017-06-27 20:38:52,157 main.py:51] epoch 2244, training loss: 10210.04, average training loss: 12589.63, base loss: 19594.42
[INFO 2017-06-27 20:38:52,534 main.py:51] epoch 2245, training loss: 12845.56, average training loss: 12590.28, base loss: 19597.09
[INFO 2017-06-27 20:38:52,910 main.py:51] epoch 2246, training loss: 12111.68, average training loss: 12590.91, base loss: 19600.66
[INFO 2017-06-27 20:38:53,281 main.py:51] epoch 2247, training loss: 13094.58, average training loss: 12592.26, base loss: 19606.00
[INFO 2017-06-27 20:38:53,659 main.py:51] epoch 2248, training loss: 11064.49, average training loss: 12587.89, base loss: 19599.63
[INFO 2017-06-27 20:38:54,035 main.py:51] epoch 2249, training loss: 11605.49, average training loss: 12587.95, base loss: 19601.26
[INFO 2017-06-27 20:38:54,412 main.py:51] epoch 2250, training loss: 11016.44, average training loss: 12585.77, base loss: 19598.09
[INFO 2017-06-27 20:38:54,789 main.py:51] epoch 2251, training loss: 11544.67, average training loss: 12583.62, base loss: 19596.68
[INFO 2017-06-27 20:38:55,172 main.py:51] epoch 2252, training loss: 11747.95, average training loss: 12581.17, base loss: 19593.96
[INFO 2017-06-27 20:38:55,554 main.py:51] epoch 2253, training loss: 13074.21, average training loss: 12580.65, base loss: 19594.97
[INFO 2017-06-27 20:38:55,929 main.py:51] epoch 2254, training loss: 12036.46, average training loss: 12580.25, base loss: 19596.21
[INFO 2017-06-27 20:38:56,302 main.py:51] epoch 2255, training loss: 13704.20, average training loss: 12581.00, base loss: 19599.81
[INFO 2017-06-27 20:38:56,680 main.py:51] epoch 2256, training loss: 12422.28, average training loss: 12580.09, base loss: 19599.74
[INFO 2017-06-27 20:38:57,056 main.py:51] epoch 2257, training loss: 12769.16, average training loss: 12578.85, base loss: 19600.18
[INFO 2017-06-27 20:38:57,436 main.py:51] epoch 2258, training loss: 10869.74, average training loss: 12576.94, base loss: 19597.97
[INFO 2017-06-27 20:38:57,813 main.py:51] epoch 2259, training loss: 13967.99, average training loss: 12577.66, base loss: 19601.66
[INFO 2017-06-27 20:38:58,190 main.py:51] epoch 2260, training loss: 13372.84, average training loss: 12577.20, base loss: 19601.67
[INFO 2017-06-27 20:38:58,570 main.py:51] epoch 2261, training loss: 12307.24, average training loss: 12577.32, base loss: 19602.18
[INFO 2017-06-27 20:38:58,947 main.py:51] epoch 2262, training loss: 12006.88, average training loss: 12575.84, base loss: 19601.18
[INFO 2017-06-27 20:38:59,322 main.py:51] epoch 2263, training loss: 12538.10, average training loss: 12575.96, base loss: 19603.44
[INFO 2017-06-27 20:38:59,691 main.py:51] epoch 2264, training loss: 11733.66, average training loss: 12574.57, base loss: 19603.54
[INFO 2017-06-27 20:39:00,066 main.py:51] epoch 2265, training loss: 11483.11, average training loss: 12572.79, base loss: 19601.98
[INFO 2017-06-27 20:39:00,444 main.py:51] epoch 2266, training loss: 13263.34, average training loss: 12572.62, base loss: 19602.86
[INFO 2017-06-27 20:39:00,819 main.py:51] epoch 2267, training loss: 11675.12, average training loss: 12572.55, base loss: 19604.62
[INFO 2017-06-27 20:39:01,195 main.py:51] epoch 2268, training loss: 11352.87, average training loss: 12569.09, base loss: 19599.12
[INFO 2017-06-27 20:39:01,570 main.py:51] epoch 2269, training loss: 13110.79, average training loss: 12570.54, base loss: 19604.52
[INFO 2017-06-27 20:39:01,950 main.py:51] epoch 2270, training loss: 11645.90, average training loss: 12566.49, base loss: 19600.03
[INFO 2017-06-27 20:39:02,326 main.py:51] epoch 2271, training loss: 11471.73, average training loss: 12564.58, base loss: 19597.23
[INFO 2017-06-27 20:39:02,702 main.py:51] epoch 2272, training loss: 14367.67, average training loss: 12566.18, base loss: 19601.18
[INFO 2017-06-27 20:39:03,073 main.py:51] epoch 2273, training loss: 12616.32, average training loss: 12562.18, base loss: 19597.15
[INFO 2017-06-27 20:39:03,450 main.py:51] epoch 2274, training loss: 12353.14, average training loss: 12563.13, base loss: 19600.65
[INFO 2017-06-27 20:39:03,821 main.py:51] epoch 2275, training loss: 10951.98, average training loss: 12562.10, base loss: 19599.89
[INFO 2017-06-27 20:39:04,200 main.py:51] epoch 2276, training loss: 11070.29, average training loss: 12561.52, base loss: 19599.84
[INFO 2017-06-27 20:39:04,578 main.py:51] epoch 2277, training loss: 12288.76, average training loss: 12558.90, base loss: 19598.97
[INFO 2017-06-27 20:39:04,955 main.py:51] epoch 2278, training loss: 11116.14, average training loss: 12556.51, base loss: 19597.38
[INFO 2017-06-27 20:39:05,335 main.py:51] epoch 2279, training loss: 13921.11, average training loss: 12556.35, base loss: 19598.46
[INFO 2017-06-27 20:39:05,708 main.py:51] epoch 2280, training loss: 11101.77, average training loss: 12554.07, base loss: 19594.74
[INFO 2017-06-27 20:39:06,082 main.py:51] epoch 2281, training loss: 11543.61, average training loss: 12554.13, base loss: 19597.10
[INFO 2017-06-27 20:39:06,458 main.py:51] epoch 2282, training loss: 11608.09, average training loss: 12550.47, base loss: 19591.53
[INFO 2017-06-27 20:39:06,833 main.py:51] epoch 2283, training loss: 12340.94, average training loss: 12550.47, base loss: 19593.07
[INFO 2017-06-27 20:39:07,211 main.py:51] epoch 2284, training loss: 11972.85, average training loss: 12546.88, base loss: 19587.86
[INFO 2017-06-27 20:39:07,582 main.py:51] epoch 2285, training loss: 12140.12, average training loss: 12544.83, base loss: 19586.01
[INFO 2017-06-27 20:39:07,957 main.py:51] epoch 2286, training loss: 10686.75, average training loss: 12542.86, base loss: 19584.49
[INFO 2017-06-27 20:39:08,333 main.py:51] epoch 2287, training loss: 13421.11, average training loss: 12544.77, base loss: 19590.66
[INFO 2017-06-27 20:39:08,704 main.py:51] epoch 2288, training loss: 13550.53, average training loss: 12543.45, base loss: 19590.79
[INFO 2017-06-27 20:39:09,075 main.py:51] epoch 2289, training loss: 11495.85, average training loss: 12544.06, base loss: 19592.57
[INFO 2017-06-27 20:39:09,448 main.py:51] epoch 2290, training loss: 10469.23, average training loss: 12541.12, base loss: 19588.68
[INFO 2017-06-27 20:39:09,827 main.py:51] epoch 2291, training loss: 13035.06, average training loss: 12540.95, base loss: 19590.30
[INFO 2017-06-27 20:39:10,206 main.py:51] epoch 2292, training loss: 10641.99, average training loss: 12537.10, base loss: 19586.75
[INFO 2017-06-27 20:39:10,578 main.py:51] epoch 2293, training loss: 10491.98, average training loss: 12536.02, base loss: 19586.75
[INFO 2017-06-27 20:39:10,953 main.py:51] epoch 2294, training loss: 11765.98, average training loss: 12535.55, base loss: 19587.35
[INFO 2017-06-27 20:39:11,328 main.py:51] epoch 2295, training loss: 12627.40, average training loss: 12534.60, base loss: 19587.39
[INFO 2017-06-27 20:39:11,699 main.py:51] epoch 2296, training loss: 11893.41, average training loss: 12534.28, base loss: 19587.40
[INFO 2017-06-27 20:39:12,082 main.py:51] epoch 2297, training loss: 12232.51, average training loss: 12535.42, base loss: 19591.08
[INFO 2017-06-27 20:39:12,454 main.py:51] epoch 2298, training loss: 11250.78, average training loss: 12533.62, base loss: 19590.83
[INFO 2017-06-27 20:39:12,829 main.py:51] epoch 2299, training loss: 10663.87, average training loss: 12532.01, base loss: 19587.66
[INFO 2017-06-27 20:39:12,829 main.py:53] epoch 2299, testing
[INFO 2017-06-27 20:39:14,442 main.py:105] average testing loss: 12105.85, base loss: 19766.80
[INFO 2017-06-27 20:39:14,442 main.py:106] improve_loss: 7660.96, improve_percent: 0.39
[INFO 2017-06-27 20:39:14,443 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:39:14,455 main.py:76] current best improved percent: 0.39
[INFO 2017-06-27 20:39:14,834 main.py:51] epoch 2300, training loss: 13099.42, average training loss: 12531.22, base loss: 19587.81
[INFO 2017-06-27 20:39:15,209 main.py:51] epoch 2301, training loss: 10475.10, average training loss: 12529.10, base loss: 19585.30
[INFO 2017-06-27 20:39:15,590 main.py:51] epoch 2302, training loss: 12673.83, average training loss: 12528.91, base loss: 19588.20
[INFO 2017-06-27 20:39:15,967 main.py:51] epoch 2303, training loss: 11686.23, average training loss: 12526.67, base loss: 19584.52
[INFO 2017-06-27 20:39:16,343 main.py:51] epoch 2304, training loss: 14322.73, average training loss: 12527.25, base loss: 19587.94
[INFO 2017-06-27 20:39:16,720 main.py:51] epoch 2305, training loss: 11800.80, average training loss: 12524.51, base loss: 19584.80
[INFO 2017-06-27 20:39:17,100 main.py:51] epoch 2306, training loss: 13069.43, average training loss: 12523.98, base loss: 19584.20
[INFO 2017-06-27 20:39:17,470 main.py:51] epoch 2307, training loss: 12968.30, average training loss: 12524.91, base loss: 19587.97
[INFO 2017-06-27 20:39:17,855 main.py:51] epoch 2308, training loss: 10974.07, average training loss: 12524.03, base loss: 19586.91
[INFO 2017-06-27 20:39:18,232 main.py:51] epoch 2309, training loss: 12873.65, average training loss: 12523.50, base loss: 19589.24
[INFO 2017-06-27 20:39:18,608 main.py:51] epoch 2310, training loss: 12461.94, average training loss: 12522.87, base loss: 19590.48
[INFO 2017-06-27 20:39:18,995 main.py:51] epoch 2311, training loss: 10611.73, average training loss: 12520.82, base loss: 19587.94
[INFO 2017-06-27 20:39:19,373 main.py:51] epoch 2312, training loss: 13427.24, average training loss: 12520.93, base loss: 19589.16
[INFO 2017-06-27 20:39:19,748 main.py:51] epoch 2313, training loss: 11258.79, average training loss: 12517.65, base loss: 19583.38
[INFO 2017-06-27 20:39:20,135 main.py:51] epoch 2314, training loss: 11877.32, average training loss: 12516.73, base loss: 19583.24
[INFO 2017-06-27 20:39:20,513 main.py:51] epoch 2315, training loss: 12498.87, average training loss: 12515.08, base loss: 19582.37
[INFO 2017-06-27 20:39:20,890 main.py:51] epoch 2316, training loss: 11638.82, average training loss: 12512.52, base loss: 19580.13
[INFO 2017-06-27 20:39:21,269 main.py:51] epoch 2317, training loss: 12119.78, average training loss: 12511.60, base loss: 19579.88
[INFO 2017-06-27 20:39:21,652 main.py:51] epoch 2318, training loss: 13994.06, average training loss: 12513.01, base loss: 19585.82
[INFO 2017-06-27 20:39:22,030 main.py:51] epoch 2319, training loss: 12070.52, average training loss: 12510.58, base loss: 19584.39
[INFO 2017-06-27 20:39:22,411 main.py:51] epoch 2320, training loss: 11552.84, average training loss: 12509.54, base loss: 19585.16
[INFO 2017-06-27 20:39:22,796 main.py:51] epoch 2321, training loss: 10094.37, average training loss: 12506.05, base loss: 19578.71
[INFO 2017-06-27 20:39:23,179 main.py:51] epoch 2322, training loss: 13490.36, average training loss: 12507.16, base loss: 19583.36
[INFO 2017-06-27 20:39:23,562 main.py:51] epoch 2323, training loss: 12385.21, average training loss: 12505.85, base loss: 19583.75
[INFO 2017-06-27 20:39:23,945 main.py:51] epoch 2324, training loss: 12464.74, average training loss: 12503.12, base loss: 19580.83
[INFO 2017-06-27 20:39:24,345 main.py:51] epoch 2325, training loss: 12046.30, average training loss: 12502.47, base loss: 19582.37
[INFO 2017-06-27 20:39:24,728 main.py:51] epoch 2326, training loss: 10335.95, average training loss: 12498.86, base loss: 19577.99
[INFO 2017-06-27 20:39:25,143 main.py:51] epoch 2327, training loss: 12409.16, average training loss: 12498.52, base loss: 19580.00
[INFO 2017-06-27 20:39:25,589 main.py:51] epoch 2328, training loss: 12989.10, average training loss: 12497.12, base loss: 19578.90
[INFO 2017-06-27 20:39:26,039 main.py:51] epoch 2329, training loss: 13492.84, average training loss: 12497.49, base loss: 19581.61
[INFO 2017-06-27 20:39:26,503 main.py:51] epoch 2330, training loss: 11447.47, average training loss: 12495.17, base loss: 19580.03
[INFO 2017-06-27 20:39:26,932 main.py:51] epoch 2331, training loss: 12758.12, average training loss: 12495.04, base loss: 19583.12
[INFO 2017-06-27 20:39:27,343 main.py:51] epoch 2332, training loss: 12051.92, average training loss: 12492.90, base loss: 19581.36
[INFO 2017-06-27 20:39:27,778 main.py:51] epoch 2333, training loss: 10881.19, average training loss: 12490.04, base loss: 19577.38
[INFO 2017-06-27 20:39:28,253 main.py:51] epoch 2334, training loss: 12055.65, average training loss: 12486.66, base loss: 19573.07
[INFO 2017-06-27 20:39:28,652 main.py:51] epoch 2335, training loss: 10966.05, average training loss: 12486.23, base loss: 19572.64
[INFO 2017-06-27 20:39:29,035 main.py:51] epoch 2336, training loss: 13045.91, average training loss: 12488.14, base loss: 19577.98
[INFO 2017-06-27 20:39:29,415 main.py:51] epoch 2337, training loss: 12324.89, average training loss: 12488.51, base loss: 19579.39
[INFO 2017-06-27 20:39:29,793 main.py:51] epoch 2338, training loss: 11470.87, average training loss: 12487.60, base loss: 19579.32
[INFO 2017-06-27 20:39:30,169 main.py:51] epoch 2339, training loss: 11690.90, average training loss: 12485.32, base loss: 19576.34
[INFO 2017-06-27 20:39:30,576 main.py:51] epoch 2340, training loss: 10370.05, average training loss: 12482.71, base loss: 19573.39
[INFO 2017-06-27 20:39:30,954 main.py:51] epoch 2341, training loss: 11947.23, average training loss: 12481.22, base loss: 19571.65
[INFO 2017-06-27 20:39:31,332 main.py:51] epoch 2342, training loss: 11440.10, average training loss: 12476.91, base loss: 19565.78
[INFO 2017-06-27 20:39:31,703 main.py:51] epoch 2343, training loss: 11170.18, average training loss: 12475.33, base loss: 19563.55
[INFO 2017-06-27 20:39:32,078 main.py:51] epoch 2344, training loss: 12484.02, average training loss: 12476.35, base loss: 19568.26
[INFO 2017-06-27 20:39:32,451 main.py:51] epoch 2345, training loss: 12496.39, average training loss: 12475.28, base loss: 19567.06
[INFO 2017-06-27 20:39:32,823 main.py:51] epoch 2346, training loss: 11866.71, average training loss: 12474.31, base loss: 19567.90
[INFO 2017-06-27 20:39:33,194 main.py:51] epoch 2347, training loss: 12347.95, average training loss: 12473.68, base loss: 19569.17
[INFO 2017-06-27 20:39:33,565 main.py:51] epoch 2348, training loss: 13160.78, average training loss: 12472.71, base loss: 19568.71
[INFO 2017-06-27 20:39:33,937 main.py:51] epoch 2349, training loss: 13793.74, average training loss: 12474.12, base loss: 19572.32
[INFO 2017-06-27 20:39:34,315 main.py:51] epoch 2350, training loss: 12745.46, average training loss: 12474.09, base loss: 19573.05
[INFO 2017-06-27 20:39:34,685 main.py:51] epoch 2351, training loss: 11525.95, average training loss: 12473.34, base loss: 19572.40
[INFO 2017-06-27 20:39:35,059 main.py:51] epoch 2352, training loss: 11912.63, average training loss: 12472.48, base loss: 19572.37
[INFO 2017-06-27 20:39:35,441 main.py:51] epoch 2353, training loss: 11658.40, average training loss: 12471.86, base loss: 19573.70
[INFO 2017-06-27 20:39:35,816 main.py:51] epoch 2354, training loss: 13481.00, average training loss: 12472.92, base loss: 19577.79
[INFO 2017-06-27 20:39:36,219 main.py:51] epoch 2355, training loss: 11858.86, average training loss: 12470.96, base loss: 19576.24
[INFO 2017-06-27 20:39:36,649 main.py:51] epoch 2356, training loss: 12647.95, average training loss: 12471.72, base loss: 19579.34
[INFO 2017-06-27 20:39:37,026 main.py:51] epoch 2357, training loss: 12873.31, average training loss: 12471.81, base loss: 19578.69
[INFO 2017-06-27 20:39:37,420 main.py:51] epoch 2358, training loss: 12647.86, average training loss: 12471.05, base loss: 19578.94
[INFO 2017-06-27 20:39:37,801 main.py:51] epoch 2359, training loss: 12417.03, average training loss: 12470.62, base loss: 19579.16
[INFO 2017-06-27 20:39:38,196 main.py:51] epoch 2360, training loss: 13148.54, average training loss: 12470.00, base loss: 19579.82
[INFO 2017-06-27 20:39:38,571 main.py:51] epoch 2361, training loss: 11430.32, average training loss: 12468.84, base loss: 19579.48
[INFO 2017-06-27 20:39:38,947 main.py:51] epoch 2362, training loss: 12334.34, average training loss: 12468.56, base loss: 19580.70
[INFO 2017-06-27 20:39:39,326 main.py:51] epoch 2363, training loss: 14602.94, average training loss: 12467.15, base loss: 19578.89
[INFO 2017-06-27 20:39:39,702 main.py:51] epoch 2364, training loss: 11844.24, average training loss: 12465.32, base loss: 19577.72
[INFO 2017-06-27 20:39:40,078 main.py:51] epoch 2365, training loss: 12334.28, average training loss: 12464.72, base loss: 19577.22
[INFO 2017-06-27 20:39:40,453 main.py:51] epoch 2366, training loss: 13251.11, average training loss: 12464.73, base loss: 19579.60
[INFO 2017-06-27 20:39:40,824 main.py:51] epoch 2367, training loss: 13012.96, average training loss: 12464.35, base loss: 19579.11
[INFO 2017-06-27 20:39:41,201 main.py:51] epoch 2368, training loss: 13076.75, average training loss: 12465.24, base loss: 19581.30
[INFO 2017-06-27 20:39:41,586 main.py:51] epoch 2369, training loss: 12895.96, average training loss: 12464.12, base loss: 19580.97
[INFO 2017-06-27 20:39:41,958 main.py:51] epoch 2370, training loss: 12349.91, average training loss: 12463.90, base loss: 19580.41
[INFO 2017-06-27 20:39:42,335 main.py:51] epoch 2371, training loss: 11648.68, average training loss: 12462.57, base loss: 19579.31
[INFO 2017-06-27 20:39:42,711 main.py:51] epoch 2372, training loss: 12724.62, average training loss: 12461.01, base loss: 19578.30
[INFO 2017-06-27 20:39:43,085 main.py:51] epoch 2373, training loss: 11306.92, average training loss: 12459.00, base loss: 19576.86
[INFO 2017-06-27 20:39:43,462 main.py:51] epoch 2374, training loss: 12302.75, average training loss: 12458.42, base loss: 19576.84
[INFO 2017-06-27 20:39:43,840 main.py:51] epoch 2375, training loss: 12883.01, average training loss: 12460.32, base loss: 19581.15
[INFO 2017-06-27 20:39:44,211 main.py:51] epoch 2376, training loss: 12058.95, average training loss: 12460.11, base loss: 19581.79
[INFO 2017-06-27 20:39:44,589 main.py:51] epoch 2377, training loss: 11551.99, average training loss: 12459.98, base loss: 19583.31
[INFO 2017-06-27 20:39:44,965 main.py:51] epoch 2378, training loss: 11258.00, average training loss: 12457.86, base loss: 19581.10
[INFO 2017-06-27 20:39:45,336 main.py:51] epoch 2379, training loss: 14010.91, average training loss: 12459.02, base loss: 19584.98
[INFO 2017-06-27 20:39:45,705 main.py:51] epoch 2380, training loss: 11605.31, average training loss: 12459.40, base loss: 19587.87
[INFO 2017-06-27 20:39:46,079 main.py:51] epoch 2381, training loss: 12979.94, average training loss: 12458.04, base loss: 19586.44
[INFO 2017-06-27 20:39:46,457 main.py:51] epoch 2382, training loss: 12712.98, average training loss: 12457.62, base loss: 19588.49
[INFO 2017-06-27 20:39:46,837 main.py:51] epoch 2383, training loss: 13713.81, average training loss: 12461.29, base loss: 19594.70
[INFO 2017-06-27 20:39:47,216 main.py:51] epoch 2384, training loss: 13118.64, average training loss: 12463.41, base loss: 19601.10
[INFO 2017-06-27 20:39:47,587 main.py:51] epoch 2385, training loss: 12150.30, average training loss: 12462.55, base loss: 19600.94
[INFO 2017-06-27 20:39:47,965 main.py:51] epoch 2386, training loss: 11132.30, average training loss: 12460.10, base loss: 19597.22
[INFO 2017-06-27 20:39:48,342 main.py:51] epoch 2387, training loss: 12158.67, average training loss: 12459.34, base loss: 19598.13
[INFO 2017-06-27 20:39:48,720 main.py:51] epoch 2388, training loss: 13093.83, average training loss: 12460.48, base loss: 19601.30
[INFO 2017-06-27 20:39:49,096 main.py:51] epoch 2389, training loss: 12193.44, average training loss: 12461.03, base loss: 19603.67
[INFO 2017-06-27 20:39:49,471 main.py:51] epoch 2390, training loss: 13582.92, average training loss: 12461.54, base loss: 19604.04
[INFO 2017-06-27 20:39:49,845 main.py:51] epoch 2391, training loss: 10673.70, average training loss: 12460.22, base loss: 19602.56
[INFO 2017-06-27 20:39:50,224 main.py:51] epoch 2392, training loss: 12949.89, average training loss: 12460.44, base loss: 19604.22
[INFO 2017-06-27 20:39:50,599 main.py:51] epoch 2393, training loss: 13559.35, average training loss: 12460.78, base loss: 19606.12
[INFO 2017-06-27 20:39:50,968 main.py:51] epoch 2394, training loss: 12974.44, average training loss: 12460.73, base loss: 19607.82
[INFO 2017-06-27 20:39:51,344 main.py:51] epoch 2395, training loss: 12106.81, average training loss: 12461.12, base loss: 19609.14
[INFO 2017-06-27 20:39:51,718 main.py:51] epoch 2396, training loss: 11130.28, average training loss: 12458.81, base loss: 19605.25
[INFO 2017-06-27 20:39:52,090 main.py:51] epoch 2397, training loss: 11405.31, average training loss: 12458.34, base loss: 19606.77
[INFO 2017-06-27 20:39:52,465 main.py:51] epoch 2398, training loss: 10272.54, average training loss: 12455.59, base loss: 19603.40
[INFO 2017-06-27 20:39:52,841 main.py:51] epoch 2399, training loss: 13386.90, average training loss: 12455.84, base loss: 19605.89
[INFO 2017-06-27 20:39:52,841 main.py:53] epoch 2399, testing
[INFO 2017-06-27 20:39:54,439 main.py:105] average testing loss: 12216.79, base loss: 20140.08
[INFO 2017-06-27 20:39:54,439 main.py:106] improve_loss: 7923.29, improve_percent: 0.39
[INFO 2017-06-27 20:39:54,440 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:39:54,452 main.py:76] current best improved percent: 0.39
[INFO 2017-06-27 20:39:54,829 main.py:51] epoch 2400, training loss: 13333.84, average training loss: 12456.11, base loss: 19608.21
[INFO 2017-06-27 20:39:55,264 main.py:51] epoch 2401, training loss: 12202.74, average training loss: 12456.86, base loss: 19610.28
[INFO 2017-06-27 20:39:55,665 main.py:51] epoch 2402, training loss: 12153.28, average training loss: 12454.29, base loss: 19607.89
[INFO 2017-06-27 20:39:56,055 main.py:51] epoch 2403, training loss: 13308.57, average training loss: 12455.36, base loss: 19612.61
[INFO 2017-06-27 20:39:56,450 main.py:51] epoch 2404, training loss: 13246.12, average training loss: 12455.00, base loss: 19613.00
[INFO 2017-06-27 20:39:56,845 main.py:51] epoch 2405, training loss: 10750.80, average training loss: 12451.89, base loss: 19608.15
[INFO 2017-06-27 20:39:57,221 main.py:51] epoch 2406, training loss: 12078.77, average training loss: 12450.64, base loss: 19607.77
[INFO 2017-06-27 20:39:57,595 main.py:51] epoch 2407, training loss: 12988.38, average training loss: 12451.50, base loss: 19609.63
[INFO 2017-06-27 20:39:58,007 main.py:51] epoch 2408, training loss: 12546.75, average training loss: 12451.19, base loss: 19609.81
[INFO 2017-06-27 20:39:58,384 main.py:51] epoch 2409, training loss: 11941.78, average training loss: 12451.29, base loss: 19612.04
[INFO 2017-06-27 20:39:58,765 main.py:51] epoch 2410, training loss: 12444.27, average training loss: 12450.53, base loss: 19612.18
[INFO 2017-06-27 20:39:59,143 main.py:51] epoch 2411, training loss: 12185.12, average training loss: 12448.51, base loss: 19610.79
[INFO 2017-06-27 20:39:59,520 main.py:51] epoch 2412, training loss: 12827.24, average training loss: 12447.57, base loss: 19610.40
[INFO 2017-06-27 20:39:59,901 main.py:51] epoch 2413, training loss: 13121.03, average training loss: 12446.47, base loss: 19610.37
[INFO 2017-06-27 20:40:00,278 main.py:51] epoch 2414, training loss: 11790.97, average training loss: 12447.49, base loss: 19613.78
[INFO 2017-06-27 20:40:00,657 main.py:51] epoch 2415, training loss: 11985.22, average training loss: 12448.24, base loss: 19615.71
[INFO 2017-06-27 20:40:01,030 main.py:51] epoch 2416, training loss: 12960.77, average training loss: 12445.98, base loss: 19613.48
[INFO 2017-06-27 20:40:01,405 main.py:51] epoch 2417, training loss: 12143.53, average training loss: 12445.38, base loss: 19614.45
[INFO 2017-06-27 20:40:01,777 main.py:51] epoch 2418, training loss: 10759.99, average training loss: 12443.91, base loss: 19615.01
[INFO 2017-06-27 20:40:02,153 main.py:51] epoch 2419, training loss: 11283.61, average training loss: 12442.16, base loss: 19613.81
[INFO 2017-06-27 20:40:02,527 main.py:51] epoch 2420, training loss: 11524.85, average training loss: 12443.36, base loss: 19617.46
[INFO 2017-06-27 20:40:02,903 main.py:51] epoch 2421, training loss: 12376.09, average training loss: 12442.29, base loss: 19617.88
[INFO 2017-06-27 20:40:03,277 main.py:51] epoch 2422, training loss: 10284.51, average training loss: 12438.49, base loss: 19612.41
[INFO 2017-06-27 20:40:03,648 main.py:51] epoch 2423, training loss: 12096.32, average training loss: 12437.24, base loss: 19612.02
[INFO 2017-06-27 20:40:04,021 main.py:51] epoch 2424, training loss: 11700.15, average training loss: 12436.65, base loss: 19612.92
[INFO 2017-06-27 20:40:04,396 main.py:51] epoch 2425, training loss: 13354.85, average training loss: 12438.15, base loss: 19618.92
[INFO 2017-06-27 20:40:04,773 main.py:51] epoch 2426, training loss: 11362.24, average training loss: 12436.73, base loss: 19618.94
[INFO 2017-06-27 20:40:05,147 main.py:51] epoch 2427, training loss: 13361.07, average training loss: 12436.20, base loss: 19619.65
[INFO 2017-06-27 20:40:05,524 main.py:51] epoch 2428, training loss: 11823.01, average training loss: 12433.52, base loss: 19616.48
[INFO 2017-06-27 20:40:05,900 main.py:51] epoch 2429, training loss: 12247.98, average training loss: 12431.11, base loss: 19613.66
[INFO 2017-06-27 20:40:06,275 main.py:51] epoch 2430, training loss: 12769.27, average training loss: 12430.40, base loss: 19614.13
[INFO 2017-06-27 20:40:06,648 main.py:51] epoch 2431, training loss: 11529.08, average training loss: 12429.38, base loss: 19612.36
[INFO 2017-06-27 20:40:07,022 main.py:51] epoch 2432, training loss: 12249.65, average training loss: 12429.91, base loss: 19616.72
[INFO 2017-06-27 20:40:07,398 main.py:51] epoch 2433, training loss: 12126.67, average training loss: 12430.19, base loss: 19619.17
[INFO 2017-06-27 20:40:07,773 main.py:51] epoch 2434, training loss: 11069.94, average training loss: 12429.05, base loss: 19619.57
[INFO 2017-06-27 20:40:08,149 main.py:51] epoch 2435, training loss: 11926.59, average training loss: 12428.45, base loss: 19620.01
[INFO 2017-06-27 20:40:08,530 main.py:51] epoch 2436, training loss: 12413.59, average training loss: 12427.84, base loss: 19620.77
[INFO 2017-06-27 20:40:08,903 main.py:51] epoch 2437, training loss: 12165.18, average training loss: 12426.72, base loss: 19620.22
[INFO 2017-06-27 20:40:09,282 main.py:51] epoch 2438, training loss: 11635.60, average training loss: 12425.98, base loss: 19620.45
[INFO 2017-06-27 20:40:09,761 main.py:51] epoch 2439, training loss: 11505.91, average training loss: 12425.25, base loss: 19622.24
[INFO 2017-06-27 20:40:10,154 main.py:51] epoch 2440, training loss: 10960.82, average training loss: 12421.59, base loss: 19618.65
[INFO 2017-06-27 20:40:10,534 main.py:51] epoch 2441, training loss: 11068.74, average training loss: 12419.10, base loss: 19616.12
[INFO 2017-06-27 20:40:10,918 main.py:51] epoch 2442, training loss: 11888.92, average training loss: 12414.90, base loss: 19609.73
[INFO 2017-06-27 20:40:11,309 main.py:51] epoch 2443, training loss: 12747.62, average training loss: 12414.92, base loss: 19611.67
[INFO 2017-06-27 20:40:11,693 main.py:51] epoch 2444, training loss: 12466.36, average training loss: 12414.38, base loss: 19611.84
[INFO 2017-06-27 20:40:12,067 main.py:51] epoch 2445, training loss: 10902.96, average training loss: 12413.07, base loss: 19612.34
[INFO 2017-06-27 20:40:12,444 main.py:51] epoch 2446, training loss: 12434.49, average training loss: 12413.45, base loss: 19615.45
[INFO 2017-06-27 20:40:12,822 main.py:51] epoch 2447, training loss: 12080.48, average training loss: 12412.60, base loss: 19615.72
[INFO 2017-06-27 20:40:13,196 main.py:51] epoch 2448, training loss: 10193.56, average training loss: 12409.27, base loss: 19611.09
[INFO 2017-06-27 20:40:13,570 main.py:51] epoch 2449, training loss: 11891.97, average training loss: 12406.71, base loss: 19609.10
[INFO 2017-06-27 20:40:13,943 main.py:51] epoch 2450, training loss: 12267.17, average training loss: 12406.07, base loss: 19609.60
[INFO 2017-06-27 20:40:14,318 main.py:51] epoch 2451, training loss: 13162.51, average training loss: 12406.68, base loss: 19612.36
[INFO 2017-06-27 20:40:14,687 main.py:51] epoch 2452, training loss: 13131.06, average training loss: 12407.46, base loss: 19615.44
[INFO 2017-06-27 20:40:15,066 main.py:51] epoch 2453, training loss: 12988.46, average training loss: 12407.42, base loss: 19617.52
[INFO 2017-06-27 20:40:15,437 main.py:51] epoch 2454, training loss: 11973.60, average training loss: 12407.15, base loss: 19618.62
[INFO 2017-06-27 20:40:15,809 main.py:51] epoch 2455, training loss: 12499.35, average training loss: 12405.30, base loss: 19617.76
[INFO 2017-06-27 20:40:16,183 main.py:51] epoch 2456, training loss: 12605.89, average training loss: 12405.41, base loss: 19620.22
[INFO 2017-06-27 20:40:16,555 main.py:51] epoch 2457, training loss: 11679.51, average training loss: 12403.85, base loss: 19618.33
[INFO 2017-06-27 20:40:16,939 main.py:51] epoch 2458, training loss: 11669.97, average training loss: 12404.67, base loss: 19619.59
[INFO 2017-06-27 20:40:17,314 main.py:51] epoch 2459, training loss: 13270.99, average training loss: 12406.81, base loss: 19623.83
[INFO 2017-06-27 20:40:17,691 main.py:51] epoch 2460, training loss: 12557.95, average training loss: 12407.15, base loss: 19626.02
[INFO 2017-06-27 20:40:18,062 main.py:51] epoch 2461, training loss: 11567.80, average training loss: 12405.74, base loss: 19624.67
[INFO 2017-06-27 20:40:18,440 main.py:51] epoch 2462, training loss: 13298.81, average training loss: 12407.40, base loss: 19626.65
[INFO 2017-06-27 20:40:18,821 main.py:51] epoch 2463, training loss: 11153.14, average training loss: 12404.20, base loss: 19622.67
[INFO 2017-06-27 20:40:19,194 main.py:51] epoch 2464, training loss: 14255.85, average training loss: 12402.19, base loss: 19621.99
[INFO 2017-06-27 20:40:19,574 main.py:51] epoch 2465, training loss: 13501.83, average training loss: 12403.08, base loss: 19624.64
[INFO 2017-06-27 20:40:19,982 main.py:51] epoch 2466, training loss: 13222.17, average training loss: 12404.00, base loss: 19626.74
[INFO 2017-06-27 20:40:20,384 main.py:51] epoch 2467, training loss: 11694.79, average training loss: 12403.97, base loss: 19628.71
[INFO 2017-06-27 20:40:20,761 main.py:51] epoch 2468, training loss: 11649.39, average training loss: 12402.94, base loss: 19628.46
[INFO 2017-06-27 20:40:21,148 main.py:51] epoch 2469, training loss: 10622.07, average training loss: 12399.86, base loss: 19624.41
[INFO 2017-06-27 20:40:21,529 main.py:51] epoch 2470, training loss: 12535.99, average training loss: 12397.98, base loss: 19621.65
[INFO 2017-06-27 20:40:21,928 main.py:51] epoch 2471, training loss: 14918.56, average training loss: 12400.59, base loss: 19628.15
[INFO 2017-06-27 20:40:22,302 main.py:51] epoch 2472, training loss: 13325.33, average training loss: 12402.11, base loss: 19633.84
[INFO 2017-06-27 20:40:22,672 main.py:51] epoch 2473, training loss: 12567.47, average training loss: 12401.18, base loss: 19633.40
[INFO 2017-06-27 20:40:23,043 main.py:51] epoch 2474, training loss: 12425.58, average training loss: 12400.61, base loss: 19634.30
[INFO 2017-06-27 20:40:23,420 main.py:51] epoch 2475, training loss: 12161.41, average training loss: 12400.55, base loss: 19634.05
[INFO 2017-06-27 20:40:23,807 main.py:51] epoch 2476, training loss: 14224.90, average training loss: 12402.84, base loss: 19639.23
[INFO 2017-06-27 20:40:24,177 main.py:51] epoch 2477, training loss: 11864.94, average training loss: 12403.16, base loss: 19640.43
[INFO 2017-06-27 20:40:24,547 main.py:51] epoch 2478, training loss: 12470.53, average training loss: 12401.83, base loss: 19638.48
[INFO 2017-06-27 20:40:24,924 main.py:51] epoch 2479, training loss: 11678.10, average training loss: 12399.48, base loss: 19635.36
[INFO 2017-06-27 20:40:25,295 main.py:51] epoch 2480, training loss: 11311.24, average training loss: 12397.43, base loss: 19633.69
[INFO 2017-06-27 20:40:25,670 main.py:51] epoch 2481, training loss: 11476.37, average training loss: 12396.36, base loss: 19633.14
[INFO 2017-06-27 20:40:26,048 main.py:51] epoch 2482, training loss: 12205.63, average training loss: 12397.23, base loss: 19635.09
[INFO 2017-06-27 20:40:26,421 main.py:51] epoch 2483, training loss: 12261.15, average training loss: 12396.46, base loss: 19635.61
[INFO 2017-06-27 20:40:26,791 main.py:51] epoch 2484, training loss: 11524.85, average training loss: 12395.89, base loss: 19635.24
[INFO 2017-06-27 20:40:27,165 main.py:51] epoch 2485, training loss: 11313.60, average training loss: 12392.94, base loss: 19631.51
[INFO 2017-06-27 20:40:27,534 main.py:51] epoch 2486, training loss: 13201.10, average training loss: 12394.40, base loss: 19635.98
[INFO 2017-06-27 20:40:27,910 main.py:51] epoch 2487, training loss: 12293.64, average training loss: 12392.88, base loss: 19634.24
[INFO 2017-06-27 20:40:28,279 main.py:51] epoch 2488, training loss: 9008.75, average training loss: 12386.88, base loss: 19622.33
[INFO 2017-06-27 20:40:28,651 main.py:51] epoch 2489, training loss: 11244.76, average training loss: 12386.37, base loss: 19622.35
[INFO 2017-06-27 20:40:29,024 main.py:51] epoch 2490, training loss: 11587.37, average training loss: 12386.20, base loss: 19622.88
[INFO 2017-06-27 20:40:29,401 main.py:51] epoch 2491, training loss: 11492.79, average training loss: 12385.42, base loss: 19619.95
[INFO 2017-06-27 20:40:29,777 main.py:51] epoch 2492, training loss: 13606.06, average training loss: 12386.62, base loss: 19623.56
[INFO 2017-06-27 20:40:30,156 main.py:51] epoch 2493, training loss: 11161.44, average training loss: 12385.40, base loss: 19622.13
[INFO 2017-06-27 20:40:30,528 main.py:51] epoch 2494, training loss: 11729.32, average training loss: 12383.48, base loss: 19620.06
[INFO 2017-06-27 20:40:30,903 main.py:51] epoch 2495, training loss: 13575.48, average training loss: 12382.58, base loss: 19619.42
[INFO 2017-06-27 20:40:31,278 main.py:51] epoch 2496, training loss: 11016.09, average training loss: 12379.74, base loss: 19614.24
[INFO 2017-06-27 20:40:31,649 main.py:51] epoch 2497, training loss: 11561.52, average training loss: 12377.68, base loss: 19612.06
[INFO 2017-06-27 20:40:32,025 main.py:51] epoch 2498, training loss: 12400.23, average training loss: 12376.50, base loss: 19611.42
[INFO 2017-06-27 20:40:32,398 main.py:51] epoch 2499, training loss: 11864.72, average training loss: 12374.33, base loss: 19609.33
[INFO 2017-06-27 20:40:32,398 main.py:53] epoch 2499, testing
[INFO 2017-06-27 20:40:33,995 main.py:105] average testing loss: 12352.14, base loss: 20272.82
[INFO 2017-06-27 20:40:33,995 main.py:106] improve_loss: 7920.68, improve_percent: 0.39
[INFO 2017-06-27 20:40:33,996 main.py:76] current best improved percent: 0.39
[INFO 2017-06-27 20:40:34,371 main.py:51] epoch 2500, training loss: 12470.31, average training loss: 12374.60, base loss: 19611.04
[INFO 2017-06-27 20:40:34,748 main.py:51] epoch 2501, training loss: 11685.31, average training loss: 12373.93, base loss: 19612.15
[INFO 2017-06-27 20:40:35,121 main.py:51] epoch 2502, training loss: 13600.64, average training loss: 12374.91, base loss: 19614.98
[INFO 2017-06-27 20:40:35,492 main.py:51] epoch 2503, training loss: 13086.47, average training loss: 12374.44, base loss: 19615.84
[INFO 2017-06-27 20:40:35,872 main.py:51] epoch 2504, training loss: 10944.79, average training loss: 12372.75, base loss: 19614.54
[INFO 2017-06-27 20:40:36,246 main.py:51] epoch 2505, training loss: 10381.32, average training loss: 12369.72, base loss: 19610.21
[INFO 2017-06-27 20:40:36,617 main.py:51] epoch 2506, training loss: 10725.57, average training loss: 12369.37, base loss: 19611.32
[INFO 2017-06-27 20:40:36,993 main.py:51] epoch 2507, training loss: 11920.83, average training loss: 12369.72, base loss: 19613.43
[INFO 2017-06-27 20:40:37,369 main.py:51] epoch 2508, training loss: 10768.39, average training loss: 12369.16, base loss: 19613.47
[INFO 2017-06-27 20:40:37,746 main.py:51] epoch 2509, training loss: 12998.34, average training loss: 12369.87, base loss: 19616.16
[INFO 2017-06-27 20:40:38,118 main.py:51] epoch 2510, training loss: 12464.61, average training loss: 12370.20, base loss: 19619.70
[INFO 2017-06-27 20:40:38,498 main.py:51] epoch 2511, training loss: 12713.29, average training loss: 12369.31, base loss: 19619.23
[INFO 2017-06-27 20:40:38,874 main.py:51] epoch 2512, training loss: 11802.67, average training loss: 12367.66, base loss: 19616.96
[INFO 2017-06-27 20:40:39,250 main.py:51] epoch 2513, training loss: 11371.45, average training loss: 12369.19, base loss: 19620.98
[INFO 2017-06-27 20:40:39,627 main.py:51] epoch 2514, training loss: 12488.88, average training loss: 12368.08, base loss: 19620.51
[INFO 2017-06-27 20:40:40,004 main.py:51] epoch 2515, training loss: 12749.04, average training loss: 12367.87, base loss: 19620.38
[INFO 2017-06-27 20:40:40,381 main.py:51] epoch 2516, training loss: 12221.65, average training loss: 12367.11, base loss: 19620.80
[INFO 2017-06-27 20:40:40,754 main.py:51] epoch 2517, training loss: 11424.53, average training loss: 12366.45, base loss: 19620.35
[INFO 2017-06-27 20:40:41,143 main.py:51] epoch 2518, training loss: 10820.74, average training loss: 12362.88, base loss: 19615.28
[INFO 2017-06-27 20:40:41,517 main.py:51] epoch 2519, training loss: 12008.70, average training loss: 12363.95, base loss: 19619.19
[INFO 2017-06-27 20:40:41,897 main.py:51] epoch 2520, training loss: 11809.59, average training loss: 12362.37, base loss: 19617.45
[INFO 2017-06-27 20:40:42,276 main.py:51] epoch 2521, training loss: 13477.26, average training loss: 12363.51, base loss: 19622.62
[INFO 2017-06-27 20:40:42,652 main.py:51] epoch 2522, training loss: 12120.96, average training loss: 12363.33, base loss: 19624.59
[INFO 2017-06-27 20:40:43,025 main.py:51] epoch 2523, training loss: 11150.26, average training loss: 12362.19, base loss: 19623.22
[INFO 2017-06-27 20:40:43,401 main.py:51] epoch 2524, training loss: 11345.54, average training loss: 12361.25, base loss: 19621.16
[INFO 2017-06-27 20:40:43,774 main.py:51] epoch 2525, training loss: 12152.78, average training loss: 12358.43, base loss: 19616.82
[INFO 2017-06-27 20:40:44,157 main.py:51] epoch 2526, training loss: 13502.64, average training loss: 12358.75, base loss: 19619.71
[INFO 2017-06-27 20:40:44,532 main.py:51] epoch 2527, training loss: 12002.59, average training loss: 12358.59, base loss: 19620.19
[INFO 2017-06-27 20:40:44,905 main.py:51] epoch 2528, training loss: 12442.82, average training loss: 12357.89, base loss: 19621.58
[INFO 2017-06-27 20:40:45,278 main.py:51] epoch 2529, training loss: 12055.82, average training loss: 12356.27, base loss: 19619.46
[INFO 2017-06-27 20:40:45,649 main.py:51] epoch 2530, training loss: 12132.38, average training loss: 12355.78, base loss: 19619.56
[INFO 2017-06-27 20:40:46,027 main.py:51] epoch 2531, training loss: 12886.05, average training loss: 12357.57, base loss: 19623.98
[INFO 2017-06-27 20:40:46,403 main.py:51] epoch 2532, training loss: 10803.01, average training loss: 12356.33, base loss: 19623.13
[INFO 2017-06-27 20:40:46,778 main.py:51] epoch 2533, training loss: 11716.65, average training loss: 12357.26, base loss: 19624.25
[INFO 2017-06-27 20:40:47,159 main.py:51] epoch 2534, training loss: 11928.52, average training loss: 12355.14, base loss: 19622.94
[INFO 2017-06-27 20:40:47,538 main.py:51] epoch 2535, training loss: 12655.76, average training loss: 12355.12, base loss: 19624.69
[INFO 2017-06-27 20:40:47,914 main.py:51] epoch 2536, training loss: 12449.54, average training loss: 12355.67, base loss: 19626.68
[INFO 2017-06-27 20:40:48,395 main.py:51] epoch 2537, training loss: 12052.66, average training loss: 12354.61, base loss: 19628.67
[INFO 2017-06-27 20:40:48,778 main.py:51] epoch 2538, training loss: 12963.99, average training loss: 12354.90, base loss: 19631.42
[INFO 2017-06-27 20:40:49,159 main.py:51] epoch 2539, training loss: 11682.97, average training loss: 12355.19, base loss: 19633.38
[INFO 2017-06-27 20:40:49,544 main.py:51] epoch 2540, training loss: 12161.07, average training loss: 12353.95, base loss: 19632.80
[INFO 2017-06-27 20:40:49,935 main.py:51] epoch 2541, training loss: 11592.57, average training loss: 12353.22, base loss: 19633.44
[INFO 2017-06-27 20:40:50,319 main.py:51] epoch 2542, training loss: 12348.03, average training loss: 12352.76, base loss: 19632.77
[INFO 2017-06-27 20:40:50,696 main.py:51] epoch 2543, training loss: 11132.14, average training loss: 12349.97, base loss: 19628.85
[INFO 2017-06-27 20:40:51,073 main.py:51] epoch 2544, training loss: 10018.16, average training loss: 12348.53, base loss: 19626.49
[INFO 2017-06-27 20:40:51,449 main.py:51] epoch 2545, training loss: 12205.48, average training loss: 12349.30, base loss: 19628.12
[INFO 2017-06-27 20:40:51,825 main.py:51] epoch 2546, training loss: 12595.85, average training loss: 12350.07, base loss: 19630.44
[INFO 2017-06-27 20:40:52,202 main.py:51] epoch 2547, training loss: 12296.15, average training loss: 12349.21, base loss: 19630.27
[INFO 2017-06-27 20:40:52,587 main.py:51] epoch 2548, training loss: 13747.15, average training loss: 12348.63, base loss: 19629.95
[INFO 2017-06-27 20:40:52,964 main.py:51] epoch 2549, training loss: 13228.62, average training loss: 12350.02, base loss: 19634.21
[INFO 2017-06-27 20:40:53,342 main.py:51] epoch 2550, training loss: 11289.68, average training loss: 12349.79, base loss: 19635.29
[INFO 2017-06-27 20:40:53,717 main.py:51] epoch 2551, training loss: 10226.54, average training loss: 12347.38, base loss: 19632.78
[INFO 2017-06-27 20:40:54,094 main.py:51] epoch 2552, training loss: 10524.86, average training loss: 12346.86, base loss: 19631.24
[INFO 2017-06-27 20:40:54,471 main.py:51] epoch 2553, training loss: 12940.53, average training loss: 12348.54, base loss: 19634.50
[INFO 2017-06-27 20:40:54,847 main.py:51] epoch 2554, training loss: 12443.79, average training loss: 12349.43, base loss: 19638.64
[INFO 2017-06-27 20:40:55,228 main.py:51] epoch 2555, training loss: 12011.66, average training loss: 12349.85, base loss: 19639.82
[INFO 2017-06-27 20:40:55,612 main.py:51] epoch 2556, training loss: 11683.13, average training loss: 12350.29, base loss: 19642.04
[INFO 2017-06-27 20:40:55,988 main.py:51] epoch 2557, training loss: 12360.83, average training loss: 12350.39, base loss: 19643.84
[INFO 2017-06-27 20:40:56,363 main.py:51] epoch 2558, training loss: 11766.65, average training loss: 12349.30, base loss: 19642.26
[INFO 2017-06-27 20:40:56,741 main.py:51] epoch 2559, training loss: 11120.25, average training loss: 12348.64, base loss: 19643.53
[INFO 2017-06-27 20:40:57,122 main.py:51] epoch 2560, training loss: 11513.42, average training loss: 12346.17, base loss: 19639.89
[INFO 2017-06-27 20:40:57,499 main.py:51] epoch 2561, training loss: 11668.23, average training loss: 12344.94, base loss: 19638.73
[INFO 2017-06-27 20:40:57,880 main.py:51] epoch 2562, training loss: 12742.32, average training loss: 12344.79, base loss: 19640.01
[INFO 2017-06-27 20:40:58,262 main.py:51] epoch 2563, training loss: 11183.93, average training loss: 12344.52, base loss: 19639.36
[INFO 2017-06-27 20:40:58,631 main.py:51] epoch 2564, training loss: 10609.51, average training loss: 12342.97, base loss: 19638.24
[INFO 2017-06-27 20:40:59,005 main.py:51] epoch 2565, training loss: 12483.74, average training loss: 12343.12, base loss: 19640.90
[INFO 2017-06-27 20:40:59,382 main.py:51] epoch 2566, training loss: 9923.93, average training loss: 12340.91, base loss: 19637.98
[INFO 2017-06-27 20:40:59,757 main.py:51] epoch 2567, training loss: 12309.64, average training loss: 12341.65, base loss: 19640.71
[INFO 2017-06-27 20:41:00,134 main.py:51] epoch 2568, training loss: 11615.26, average training loss: 12340.35, base loss: 19639.79
[INFO 2017-06-27 20:41:00,509 main.py:51] epoch 2569, training loss: 11752.09, average training loss: 12340.59, base loss: 19640.72
[INFO 2017-06-27 20:41:00,886 main.py:51] epoch 2570, training loss: 11127.68, average training loss: 12339.51, base loss: 19639.94
[INFO 2017-06-27 20:41:01,262 main.py:51] epoch 2571, training loss: 11997.14, average training loss: 12336.90, base loss: 19638.25
[INFO 2017-06-27 20:41:01,639 main.py:51] epoch 2572, training loss: 13833.47, average training loss: 12338.70, base loss: 19641.62
[INFO 2017-06-27 20:41:02,013 main.py:51] epoch 2573, training loss: 11970.46, average training loss: 12339.83, base loss: 19644.95
[INFO 2017-06-27 20:41:02,386 main.py:51] epoch 2574, training loss: 11501.74, average training loss: 12336.75, base loss: 19639.76
[INFO 2017-06-27 20:41:02,763 main.py:51] epoch 2575, training loss: 12976.92, average training loss: 12336.84, base loss: 19641.00
[INFO 2017-06-27 20:41:03,141 main.py:51] epoch 2576, training loss: 14609.40, average training loss: 12338.65, base loss: 19645.71
[INFO 2017-06-27 20:41:03,518 main.py:51] epoch 2577, training loss: 12547.94, average training loss: 12337.21, base loss: 19644.23
[INFO 2017-06-27 20:41:03,892 main.py:51] epoch 2578, training loss: 12580.57, average training loss: 12336.14, base loss: 19643.84
[INFO 2017-06-27 20:41:04,268 main.py:51] epoch 2579, training loss: 11547.46, average training loss: 12335.64, base loss: 19644.09
[INFO 2017-06-27 20:41:04,648 main.py:51] epoch 2580, training loss: 13790.25, average training loss: 12336.50, base loss: 19647.39
[INFO 2017-06-27 20:41:05,024 main.py:51] epoch 2581, training loss: 11074.43, average training loss: 12334.01, base loss: 19643.73
[INFO 2017-06-27 20:41:05,404 main.py:51] epoch 2582, training loss: 11072.62, average training loss: 12333.16, base loss: 19644.26
[INFO 2017-06-27 20:41:05,781 main.py:51] epoch 2583, training loss: 11634.91, average training loss: 12333.32, base loss: 19646.41
[INFO 2017-06-27 20:41:06,157 main.py:51] epoch 2584, training loss: 11459.76, average training loss: 12331.71, base loss: 19644.54
[INFO 2017-06-27 20:41:06,532 main.py:51] epoch 2585, training loss: 12539.98, average training loss: 12330.67, base loss: 19644.48
[INFO 2017-06-27 20:41:06,910 main.py:51] epoch 2586, training loss: 11248.52, average training loss: 12329.74, base loss: 19643.83
[INFO 2017-06-27 20:41:07,288 main.py:51] epoch 2587, training loss: 12672.77, average training loss: 12329.40, base loss: 19643.70
[INFO 2017-06-27 20:41:07,669 main.py:51] epoch 2588, training loss: 11889.86, average training loss: 12328.05, base loss: 19642.42
[INFO 2017-06-27 20:41:08,123 main.py:51] epoch 2589, training loss: 13879.14, average training loss: 12329.52, base loss: 19646.13
[INFO 2017-06-27 20:41:08,521 main.py:51] epoch 2590, training loss: 11611.11, average training loss: 12328.99, base loss: 19648.45
[INFO 2017-06-27 20:41:08,903 main.py:51] epoch 2591, training loss: 12614.20, average training loss: 12329.22, base loss: 19650.25
[INFO 2017-06-27 20:41:09,286 main.py:51] epoch 2592, training loss: 11564.39, average training loss: 12327.03, base loss: 19647.20
[INFO 2017-06-27 20:41:09,668 main.py:51] epoch 2593, training loss: 12337.46, average training loss: 12326.44, base loss: 19647.94
[INFO 2017-06-27 20:41:10,148 main.py:51] epoch 2594, training loss: 10591.72, average training loss: 12323.25, base loss: 19643.84
[INFO 2017-06-27 20:41:10,553 main.py:51] epoch 2595, training loss: 12925.15, average training loss: 12322.92, base loss: 19644.98
[INFO 2017-06-27 20:41:10,945 main.py:51] epoch 2596, training loss: 11164.57, average training loss: 12323.08, base loss: 19647.66
[INFO 2017-06-27 20:41:11,349 main.py:51] epoch 2597, training loss: 12488.22, average training loss: 12323.53, base loss: 19650.19
[INFO 2017-06-27 20:41:11,725 main.py:51] epoch 2598, training loss: 12344.03, average training loss: 12323.30, base loss: 19651.29
[INFO 2017-06-27 20:41:12,102 main.py:51] epoch 2599, training loss: 12301.12, average training loss: 12322.91, base loss: 19652.60
[INFO 2017-06-27 20:41:12,102 main.py:53] epoch 2599, testing
[INFO 2017-06-27 20:41:13,703 main.py:105] average testing loss: 11867.16, base loss: 19697.20
[INFO 2017-06-27 20:41:13,703 main.py:106] improve_loss: 7830.04, improve_percent: 0.40
[INFO 2017-06-27 20:41:13,703 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:41:13,716 main.py:76] current best improved percent: 0.40
[INFO 2017-06-27 20:41:14,098 main.py:51] epoch 2600, training loss: 11899.80, average training loss: 12323.84, base loss: 19657.53
[INFO 2017-06-27 20:41:14,474 main.py:51] epoch 2601, training loss: 10352.86, average training loss: 12322.50, base loss: 19658.20
[INFO 2017-06-27 20:41:14,858 main.py:51] epoch 2602, training loss: 11019.92, average training loss: 12320.60, base loss: 19655.60
[INFO 2017-06-27 20:41:15,233 main.py:51] epoch 2603, training loss: 12143.82, average training loss: 12318.43, base loss: 19654.58
[INFO 2017-06-27 20:41:15,610 main.py:51] epoch 2604, training loss: 15702.29, average training loss: 12320.52, base loss: 19659.18
[INFO 2017-06-27 20:41:15,992 main.py:51] epoch 2605, training loss: 11276.89, average training loss: 12318.13, base loss: 19657.65
[INFO 2017-06-27 20:41:16,369 main.py:51] epoch 2606, training loss: 13943.01, average training loss: 12320.26, base loss: 19663.25
[INFO 2017-06-27 20:41:16,753 main.py:51] epoch 2607, training loss: 13117.01, average training loss: 12319.04, base loss: 19662.94
[INFO 2017-06-27 20:41:17,217 main.py:51] epoch 2608, training loss: 11588.28, average training loss: 12317.29, base loss: 19660.16
[INFO 2017-06-27 20:41:17,612 main.py:51] epoch 2609, training loss: 11454.20, average training loss: 12314.07, base loss: 19656.06
[INFO 2017-06-27 20:41:17,995 main.py:51] epoch 2610, training loss: 13189.32, average training loss: 12315.42, base loss: 19660.21
[INFO 2017-06-27 20:41:18,401 main.py:51] epoch 2611, training loss: 11967.98, average training loss: 12315.68, base loss: 19661.37
[INFO 2017-06-27 20:41:18,775 main.py:51] epoch 2612, training loss: 10750.20, average training loss: 12313.18, base loss: 19659.50
[INFO 2017-06-27 20:41:19,160 main.py:51] epoch 2613, training loss: 11784.46, average training loss: 12311.39, base loss: 19658.82
[INFO 2017-06-27 20:41:19,537 main.py:51] epoch 2614, training loss: 12810.01, average training loss: 12309.29, base loss: 19658.85
[INFO 2017-06-27 20:41:19,910 main.py:51] epoch 2615, training loss: 12654.46, average training loss: 12308.01, base loss: 19658.44
[INFO 2017-06-27 20:41:20,287 main.py:51] epoch 2616, training loss: 12163.75, average training loss: 12307.45, base loss: 19659.77
[INFO 2017-06-27 20:41:20,662 main.py:51] epoch 2617, training loss: 11448.15, average training loss: 12304.94, base loss: 19655.02
[INFO 2017-06-27 20:41:21,037 main.py:51] epoch 2618, training loss: 11124.86, average training loss: 12303.82, base loss: 19653.69
[INFO 2017-06-27 20:41:21,412 main.py:51] epoch 2619, training loss: 12321.27, average training loss: 12300.96, base loss: 19651.39
[INFO 2017-06-27 20:41:21,788 main.py:51] epoch 2620, training loss: 12416.02, average training loss: 12300.30, base loss: 19652.84
[INFO 2017-06-27 20:41:22,168 main.py:51] epoch 2621, training loss: 10893.64, average training loss: 12297.80, base loss: 19650.16
[INFO 2017-06-27 20:41:22,543 main.py:51] epoch 2622, training loss: 13182.43, average training loss: 12297.56, base loss: 19651.50
[INFO 2017-06-27 20:41:22,919 main.py:51] epoch 2623, training loss: 13532.66, average training loss: 12298.06, base loss: 19653.22
[INFO 2017-06-27 20:41:23,298 main.py:51] epoch 2624, training loss: 11860.88, average training loss: 12296.28, base loss: 19651.83
[INFO 2017-06-27 20:41:23,703 main.py:51] epoch 2625, training loss: 13202.02, average training loss: 12296.66, base loss: 19652.90
[INFO 2017-06-27 20:41:24,109 main.py:51] epoch 2626, training loss: 12609.11, average training loss: 12297.33, base loss: 19655.08
[INFO 2017-06-27 20:41:24,489 main.py:51] epoch 2627, training loss: 15129.84, average training loss: 12299.46, base loss: 19659.99
[INFO 2017-06-27 20:41:24,869 main.py:51] epoch 2628, training loss: 12630.66, average training loss: 12299.35, base loss: 19661.91
[INFO 2017-06-27 20:41:25,258 main.py:51] epoch 2629, training loss: 10681.57, average training loss: 12296.29, base loss: 19657.38
[INFO 2017-06-27 20:41:25,634 main.py:51] epoch 2630, training loss: 11966.37, average training loss: 12297.09, base loss: 19659.10
[INFO 2017-06-27 20:41:26,011 main.py:51] epoch 2631, training loss: 11745.64, average training loss: 12295.28, base loss: 19656.75
[INFO 2017-06-27 20:41:26,388 main.py:51] epoch 2632, training loss: 12796.70, average training loss: 12295.68, base loss: 19657.90
[INFO 2017-06-27 20:41:26,766 main.py:51] epoch 2633, training loss: 13297.19, average training loss: 12296.03, base loss: 19661.06
[INFO 2017-06-27 20:41:27,138 main.py:51] epoch 2634, training loss: 11792.92, average training loss: 12294.24, base loss: 19658.63
[INFO 2017-06-27 20:41:27,515 main.py:51] epoch 2635, training loss: 12002.38, average training loss: 12292.76, base loss: 19658.69
[INFO 2017-06-27 20:41:27,891 main.py:51] epoch 2636, training loss: 10155.00, average training loss: 12289.09, base loss: 19652.85
[INFO 2017-06-27 20:41:28,274 main.py:51] epoch 2637, training loss: 11990.35, average training loss: 12288.25, base loss: 19652.98
[INFO 2017-06-27 20:41:28,650 main.py:51] epoch 2638, training loss: 12996.00, average training loss: 12288.00, base loss: 19652.46
[INFO 2017-06-27 20:41:29,029 main.py:51] epoch 2639, training loss: 12800.12, average training loss: 12286.49, base loss: 19650.25
[INFO 2017-06-27 20:41:29,405 main.py:51] epoch 2640, training loss: 12599.71, average training loss: 12286.13, base loss: 19650.26
[INFO 2017-06-27 20:41:29,786 main.py:51] epoch 2641, training loss: 13692.17, average training loss: 12285.70, base loss: 19651.72
[INFO 2017-06-27 20:41:30,251 main.py:51] epoch 2642, training loss: 13200.77, average training loss: 12284.27, base loss: 19648.98
[INFO 2017-06-27 20:41:30,661 main.py:51] epoch 2643, training loss: 11035.49, average training loss: 12284.70, base loss: 19650.91
[INFO 2017-06-27 20:41:31,039 main.py:51] epoch 2644, training loss: 11817.93, average training loss: 12284.09, base loss: 19651.92
[INFO 2017-06-27 20:41:31,419 main.py:51] epoch 2645, training loss: 14456.67, average training loss: 12287.83, base loss: 19659.50
[INFO 2017-06-27 20:41:31,811 main.py:51] epoch 2646, training loss: 10841.78, average training loss: 12285.75, base loss: 19655.35
[INFO 2017-06-27 20:41:32,198 main.py:51] epoch 2647, training loss: 12258.57, average training loss: 12286.56, base loss: 19659.61
[INFO 2017-06-27 20:41:32,580 main.py:51] epoch 2648, training loss: 12356.76, average training loss: 12287.45, base loss: 19662.34
[INFO 2017-06-27 20:41:32,952 main.py:51] epoch 2649, training loss: 12673.63, average training loss: 12286.64, base loss: 19662.33
[INFO 2017-06-27 20:41:33,327 main.py:51] epoch 2650, training loss: 12336.28, average training loss: 12285.28, base loss: 19660.80
[INFO 2017-06-27 20:41:33,698 main.py:51] epoch 2651, training loss: 12792.94, average training loss: 12285.86, base loss: 19663.55
[INFO 2017-06-27 20:41:34,093 main.py:51] epoch 2652, training loss: 11647.77, average training loss: 12284.63, base loss: 19662.62
[INFO 2017-06-27 20:41:34,469 main.py:51] epoch 2653, training loss: 12525.15, average training loss: 12284.84, base loss: 19664.42
[INFO 2017-06-27 20:41:34,933 main.py:51] epoch 2654, training loss: 13216.00, average training loss: 12284.66, base loss: 19665.92
[INFO 2017-06-27 20:41:35,329 main.py:51] epoch 2655, training loss: 13701.97, average training loss: 12286.83, base loss: 19670.22
[INFO 2017-06-27 20:41:35,708 main.py:51] epoch 2656, training loss: 11327.02, average training loss: 12286.09, base loss: 19670.07
[INFO 2017-06-27 20:41:36,176 main.py:51] epoch 2657, training loss: 11070.12, average training loss: 12283.33, base loss: 19666.41
[INFO 2017-06-27 20:41:36,575 main.py:51] epoch 2658, training loss: 12819.68, average training loss: 12283.43, base loss: 19666.72
[INFO 2017-06-27 20:41:36,960 main.py:51] epoch 2659, training loss: 12347.28, average training loss: 12283.87, base loss: 19668.12
[INFO 2017-06-27 20:41:37,338 main.py:51] epoch 2660, training loss: 11651.81, average training loss: 12284.49, base loss: 19670.54
[INFO 2017-06-27 20:41:37,717 main.py:51] epoch 2661, training loss: 10767.00, average training loss: 12282.62, base loss: 19668.67
[INFO 2017-06-27 20:41:38,117 main.py:51] epoch 2662, training loss: 12099.86, average training loss: 12282.54, base loss: 19669.37
[INFO 2017-06-27 20:41:38,496 main.py:51] epoch 2663, training loss: 12747.58, average training loss: 12283.65, base loss: 19674.40
[INFO 2017-06-27 20:41:38,873 main.py:51] epoch 2664, training loss: 12211.70, average training loss: 12284.84, base loss: 19676.41
[INFO 2017-06-27 20:41:39,249 main.py:51] epoch 2665, training loss: 10871.50, average training loss: 12281.99, base loss: 19673.90
[INFO 2017-06-27 20:41:39,622 main.py:51] epoch 2666, training loss: 11629.19, average training loss: 12280.84, base loss: 19672.16
[INFO 2017-06-27 20:41:39,999 main.py:51] epoch 2667, training loss: 12059.84, average training loss: 12279.60, base loss: 19669.24
[INFO 2017-06-27 20:41:40,372 main.py:51] epoch 2668, training loss: 10779.95, average training loss: 12277.84, base loss: 19665.24
[INFO 2017-06-27 20:41:40,755 main.py:51] epoch 2669, training loss: 12116.05, average training loss: 12276.45, base loss: 19663.70
[INFO 2017-06-27 20:41:41,131 main.py:51] epoch 2670, training loss: 11417.61, average training loss: 12276.60, base loss: 19664.48
[INFO 2017-06-27 20:41:41,502 main.py:51] epoch 2671, training loss: 11393.64, average training loss: 12274.97, base loss: 19663.53
[INFO 2017-06-27 20:41:41,877 main.py:51] epoch 2672, training loss: 10443.75, average training loss: 12270.78, base loss: 19658.00
[INFO 2017-06-27 20:41:42,258 main.py:51] epoch 2673, training loss: 12685.65, average training loss: 12271.67, base loss: 19661.23
[INFO 2017-06-27 20:41:42,630 main.py:51] epoch 2674, training loss: 10997.79, average training loss: 12271.12, base loss: 19661.98
[INFO 2017-06-27 20:41:43,024 main.py:51] epoch 2675, training loss: 14865.21, average training loss: 12273.66, base loss: 19669.95
[INFO 2017-06-27 20:41:43,424 main.py:51] epoch 2676, training loss: 11599.22, average training loss: 12270.39, base loss: 19666.33
[INFO 2017-06-27 20:41:43,803 main.py:51] epoch 2677, training loss: 13233.18, average training loss: 12270.96, base loss: 19668.40
[INFO 2017-06-27 20:41:44,193 main.py:51] epoch 2678, training loss: 12050.84, average training loss: 12271.44, base loss: 19670.27
[INFO 2017-06-27 20:41:44,600 main.py:51] epoch 2679, training loss: 10552.16, average training loss: 12268.94, base loss: 19666.92
[INFO 2017-06-27 20:41:44,971 main.py:51] epoch 2680, training loss: 12002.56, average training loss: 12268.06, base loss: 19666.92
[INFO 2017-06-27 20:41:45,344 main.py:51] epoch 2681, training loss: 10468.59, average training loss: 12265.58, base loss: 19663.76
[INFO 2017-06-27 20:41:45,722 main.py:51] epoch 2682, training loss: 11952.51, average training loss: 12265.81, base loss: 19665.55
[INFO 2017-06-27 20:41:46,095 main.py:51] epoch 2683, training loss: 11891.17, average training loss: 12264.31, base loss: 19664.16
[INFO 2017-06-27 20:41:46,466 main.py:51] epoch 2684, training loss: 12826.27, average training loss: 12264.74, base loss: 19665.21
[INFO 2017-06-27 20:41:46,850 main.py:51] epoch 2685, training loss: 12619.32, average training loss: 12265.87, base loss: 19669.20
[INFO 2017-06-27 20:41:47,231 main.py:51] epoch 2686, training loss: 14364.02, average training loss: 12267.42, base loss: 19674.71
[INFO 2017-06-27 20:41:47,612 main.py:51] epoch 2687, training loss: 11760.27, average training loss: 12267.28, base loss: 19675.84
[INFO 2017-06-27 20:41:47,988 main.py:51] epoch 2688, training loss: 12072.12, average training loss: 12264.51, base loss: 19669.81
[INFO 2017-06-27 20:41:48,369 main.py:51] epoch 2689, training loss: 11808.76, average training loss: 12265.19, base loss: 19671.77
[INFO 2017-06-27 20:41:48,746 main.py:51] epoch 2690, training loss: 11793.44, average training loss: 12264.10, base loss: 19671.18
[INFO 2017-06-27 20:41:49,125 main.py:51] epoch 2691, training loss: 11210.98, average training loss: 12261.85, base loss: 19668.39
[INFO 2017-06-27 20:41:49,501 main.py:51] epoch 2692, training loss: 12308.50, average training loss: 12260.44, base loss: 19668.47
[INFO 2017-06-27 20:41:49,877 main.py:51] epoch 2693, training loss: 12366.16, average training loss: 12261.37, base loss: 19671.03
[INFO 2017-06-27 20:41:50,255 main.py:51] epoch 2694, training loss: 10681.46, average training loss: 12259.57, base loss: 19669.12
[INFO 2017-06-27 20:41:50,639 main.py:51] epoch 2695, training loss: 12814.39, average training loss: 12260.82, base loss: 19672.24
[INFO 2017-06-27 20:41:51,014 main.py:51] epoch 2696, training loss: 12650.95, average training loss: 12261.01, base loss: 19672.43
[INFO 2017-06-27 20:41:51,392 main.py:51] epoch 2697, training loss: 12105.68, average training loss: 12260.60, base loss: 19672.45
[INFO 2017-06-27 20:41:51,772 main.py:51] epoch 2698, training loss: 12984.31, average training loss: 12259.23, base loss: 19671.28
[INFO 2017-06-27 20:41:52,150 main.py:51] epoch 2699, training loss: 11380.88, average training loss: 12258.71, base loss: 19671.98
[INFO 2017-06-27 20:41:52,150 main.py:53] epoch 2699, testing
[INFO 2017-06-27 20:41:53,857 main.py:105] average testing loss: 12332.82, base loss: 20258.75
[INFO 2017-06-27 20:41:53,857 main.py:106] improve_loss: 7925.93, improve_percent: 0.39
[INFO 2017-06-27 20:41:53,858 main.py:76] current best improved percent: 0.40
[INFO 2017-06-27 20:41:54,247 main.py:51] epoch 2700, training loss: 11292.67, average training loss: 12258.43, base loss: 19673.29
[INFO 2017-06-27 20:41:54,625 main.py:51] epoch 2701, training loss: 13453.95, average training loss: 12259.85, base loss: 19678.02
[INFO 2017-06-27 20:41:55,001 main.py:51] epoch 2702, training loss: 12818.83, average training loss: 12259.70, base loss: 19678.17
[INFO 2017-06-27 20:41:55,377 main.py:51] epoch 2703, training loss: 12869.71, average training loss: 12259.53, base loss: 19680.29
[INFO 2017-06-27 20:41:55,755 main.py:51] epoch 2704, training loss: 12262.49, average training loss: 12257.72, base loss: 19679.64
[INFO 2017-06-27 20:41:56,141 main.py:51] epoch 2705, training loss: 12535.27, average training loss: 12259.26, base loss: 19684.74
[INFO 2017-06-27 20:41:56,522 main.py:51] epoch 2706, training loss: 13444.05, average training loss: 12259.04, base loss: 19684.78
[INFO 2017-06-27 20:41:56,906 main.py:51] epoch 2707, training loss: 12353.52, average training loss: 12257.94, base loss: 19683.01
[INFO 2017-06-27 20:41:57,311 main.py:51] epoch 2708, training loss: 11927.16, average training loss: 12255.63, base loss: 19680.80
[INFO 2017-06-27 20:41:57,694 main.py:51] epoch 2709, training loss: 11847.08, average training loss: 12255.33, base loss: 19681.35
[INFO 2017-06-27 20:41:58,072 main.py:51] epoch 2710, training loss: 13159.04, average training loss: 12255.59, base loss: 19682.38
[INFO 2017-06-27 20:41:58,461 main.py:51] epoch 2711, training loss: 11572.12, average training loss: 12252.14, base loss: 19678.12
[INFO 2017-06-27 20:41:58,830 main.py:51] epoch 2712, training loss: 13023.84, average training loss: 12253.22, base loss: 19682.95
[INFO 2017-06-27 20:41:59,201 main.py:51] epoch 2713, training loss: 13411.86, average training loss: 12256.02, base loss: 19690.03
[INFO 2017-06-27 20:41:59,574 main.py:51] epoch 2714, training loss: 11890.89, average training loss: 12255.94, base loss: 19689.49
[INFO 2017-06-27 20:41:59,951 main.py:51] epoch 2715, training loss: 11454.57, average training loss: 12255.70, base loss: 19689.71
[INFO 2017-06-27 20:42:00,333 main.py:51] epoch 2716, training loss: 12127.23, average training loss: 12254.84, base loss: 19687.89
[INFO 2017-06-27 20:42:00,706 main.py:51] epoch 2717, training loss: 10516.15, average training loss: 12253.92, base loss: 19686.94
[INFO 2017-06-27 20:42:01,082 main.py:51] epoch 2718, training loss: 11734.91, average training loss: 12254.02, base loss: 19688.98
[INFO 2017-06-27 20:42:01,456 main.py:51] epoch 2719, training loss: 10588.82, average training loss: 12252.01, base loss: 19685.97
[INFO 2017-06-27 20:42:01,830 main.py:51] epoch 2720, training loss: 11656.71, average training loss: 12251.06, base loss: 19685.36
[INFO 2017-06-27 20:42:02,206 main.py:51] epoch 2721, training loss: 11945.51, average training loss: 12251.61, base loss: 19687.13
[INFO 2017-06-27 20:42:02,584 main.py:51] epoch 2722, training loss: 11418.52, average training loss: 12248.68, base loss: 19683.53
[INFO 2017-06-27 20:42:02,957 main.py:51] epoch 2723, training loss: 13328.62, average training loss: 12250.02, base loss: 19687.71
[INFO 2017-06-27 20:42:03,333 main.py:51] epoch 2724, training loss: 11863.47, average training loss: 12250.00, base loss: 19689.11
[INFO 2017-06-27 20:42:03,710 main.py:51] epoch 2725, training loss: 11941.05, average training loss: 12249.87, base loss: 19688.56
[INFO 2017-06-27 20:42:04,081 main.py:51] epoch 2726, training loss: 12034.46, average training loss: 12249.49, base loss: 19689.50
[INFO 2017-06-27 20:42:04,455 main.py:51] epoch 2727, training loss: 12546.00, average training loss: 12249.33, base loss: 19689.10
[INFO 2017-06-27 20:42:04,830 main.py:51] epoch 2728, training loss: 11328.62, average training loss: 12248.13, base loss: 19688.66
[INFO 2017-06-27 20:42:05,202 main.py:51] epoch 2729, training loss: 12171.99, average training loss: 12249.09, base loss: 19692.90
[INFO 2017-06-27 20:42:05,573 main.py:51] epoch 2730, training loss: 11686.04, average training loss: 12246.22, base loss: 19688.74
[INFO 2017-06-27 20:42:05,948 main.py:51] epoch 2731, training loss: 13234.77, average training loss: 12246.92, base loss: 19690.59
[INFO 2017-06-27 20:42:06,332 main.py:51] epoch 2732, training loss: 13742.52, average training loss: 12248.09, base loss: 19693.93
[INFO 2017-06-27 20:42:06,709 main.py:51] epoch 2733, training loss: 10919.15, average training loss: 12245.77, base loss: 19690.70
[INFO 2017-06-27 20:42:07,084 main.py:51] epoch 2734, training loss: 10916.48, average training loss: 12244.08, base loss: 19689.19
[INFO 2017-06-27 20:42:07,544 main.py:51] epoch 2735, training loss: 11276.45, average training loss: 12243.77, base loss: 19689.56
[INFO 2017-06-27 20:42:07,951 main.py:51] epoch 2736, training loss: 11563.64, average training loss: 12242.94, base loss: 19689.52
[INFO 2017-06-27 20:42:08,333 main.py:51] epoch 2737, training loss: 13252.41, average training loss: 12244.09, base loss: 19694.30
[INFO 2017-06-27 20:42:08,722 main.py:51] epoch 2738, training loss: 11088.62, average training loss: 12242.23, base loss: 19691.52
[INFO 2017-06-27 20:42:09,107 main.py:51] epoch 2739, training loss: 11434.31, average training loss: 12242.23, base loss: 19693.54
[INFO 2017-06-27 20:42:09,489 main.py:51] epoch 2740, training loss: 14464.33, average training loss: 12246.35, base loss: 19701.71
[INFO 2017-06-27 20:42:09,877 main.py:51] epoch 2741, training loss: 11933.54, average training loss: 12246.05, base loss: 19701.63
[INFO 2017-06-27 20:42:10,256 main.py:51] epoch 2742, training loss: 12663.94, average training loss: 12246.92, base loss: 19704.01
[INFO 2017-06-27 20:42:10,633 main.py:51] epoch 2743, training loss: 11219.79, average training loss: 12246.11, base loss: 19703.11
[INFO 2017-06-27 20:42:11,012 main.py:51] epoch 2744, training loss: 11496.48, average training loss: 12246.02, base loss: 19705.61
[INFO 2017-06-27 20:42:11,391 main.py:51] epoch 2745, training loss: 13961.87, average training loss: 12246.50, base loss: 19708.24
[INFO 2017-06-27 20:42:11,773 main.py:51] epoch 2746, training loss: 10542.25, average training loss: 12244.20, base loss: 19703.99
[INFO 2017-06-27 20:42:12,173 main.py:51] epoch 2747, training loss: 12197.96, average training loss: 12244.72, base loss: 19706.02
[INFO 2017-06-27 20:42:12,576 main.py:51] epoch 2748, training loss: 11802.31, average training loss: 12242.82, base loss: 19703.77
[INFO 2017-06-27 20:42:12,969 main.py:51] epoch 2749, training loss: 12731.82, average training loss: 12243.32, base loss: 19705.18
[INFO 2017-06-27 20:42:13,350 main.py:51] epoch 2750, training loss: 11787.07, average training loss: 12241.40, base loss: 19702.71
[INFO 2017-06-27 20:42:13,740 main.py:51] epoch 2751, training loss: 9748.57, average training loss: 12239.09, base loss: 19699.53
[INFO 2017-06-27 20:42:14,124 main.py:51] epoch 2752, training loss: 12525.98, average training loss: 12238.15, base loss: 19697.93
[INFO 2017-06-27 20:42:14,539 main.py:51] epoch 2753, training loss: 11129.45, average training loss: 12236.29, base loss: 19695.15
[INFO 2017-06-27 20:42:14,967 main.py:51] epoch 2754, training loss: 10874.44, average training loss: 12234.60, base loss: 19693.12
[INFO 2017-06-27 20:42:15,392 main.py:51] epoch 2755, training loss: 12844.39, average training loss: 12234.79, base loss: 19694.92
[INFO 2017-06-27 20:42:15,852 main.py:51] epoch 2756, training loss: 12352.16, average training loss: 12234.71, base loss: 19695.22
[INFO 2017-06-27 20:42:16,300 main.py:51] epoch 2757, training loss: 12230.67, average training loss: 12236.57, base loss: 19699.60
[INFO 2017-06-27 20:42:16,714 main.py:51] epoch 2758, training loss: 11918.96, average training loss: 12235.13, base loss: 19698.07
[INFO 2017-06-27 20:42:17,134 main.py:51] epoch 2759, training loss: 11422.71, average training loss: 12232.91, base loss: 19695.49
[INFO 2017-06-27 20:42:17,594 main.py:51] epoch 2760, training loss: 11743.98, average training loss: 12230.77, base loss: 19694.14
[INFO 2017-06-27 20:42:18,032 main.py:51] epoch 2761, training loss: 11105.60, average training loss: 12229.88, base loss: 19695.73
[INFO 2017-06-27 20:42:18,412 main.py:51] epoch 2762, training loss: 11830.00, average training loss: 12229.96, base loss: 19697.37
[INFO 2017-06-27 20:42:18,789 main.py:51] epoch 2763, training loss: 11155.91, average training loss: 12228.12, base loss: 19693.68
[INFO 2017-06-27 20:42:19,162 main.py:51] epoch 2764, training loss: 12882.57, average training loss: 12228.13, base loss: 19694.45
[INFO 2017-06-27 20:42:19,552 main.py:51] epoch 2765, training loss: 11469.38, average training loss: 12228.71, base loss: 19696.88
[INFO 2017-06-27 20:42:19,928 main.py:51] epoch 2766, training loss: 11313.91, average training loss: 12227.63, base loss: 19697.28
[INFO 2017-06-27 20:42:20,300 main.py:51] epoch 2767, training loss: 11256.99, average training loss: 12225.83, base loss: 19695.48
[INFO 2017-06-27 20:42:20,682 main.py:51] epoch 2768, training loss: 12215.12, average training loss: 12226.20, base loss: 19698.71
[INFO 2017-06-27 20:42:21,053 main.py:51] epoch 2769, training loss: 12597.87, average training loss: 12226.63, base loss: 19699.40
[INFO 2017-06-27 20:42:21,432 main.py:51] epoch 2770, training loss: 11691.15, average training loss: 12227.23, base loss: 19700.80
[INFO 2017-06-27 20:42:21,804 main.py:51] epoch 2771, training loss: 12079.21, average training loss: 12226.52, base loss: 19701.29
[INFO 2017-06-27 20:42:22,177 main.py:51] epoch 2772, training loss: 12233.33, average training loss: 12227.16, base loss: 19702.53
[INFO 2017-06-27 20:42:22,550 main.py:51] epoch 2773, training loss: 11316.88, average training loss: 12226.13, base loss: 19700.73
[INFO 2017-06-27 20:42:22,921 main.py:51] epoch 2774, training loss: 10922.42, average training loss: 12222.46, base loss: 19695.08
[INFO 2017-06-27 20:42:23,294 main.py:51] epoch 2775, training loss: 11515.67, average training loss: 12221.43, base loss: 19695.40
[INFO 2017-06-27 20:42:23,665 main.py:51] epoch 2776, training loss: 12090.61, average training loss: 12220.52, base loss: 19694.71
[INFO 2017-06-27 20:42:24,040 main.py:51] epoch 2777, training loss: 10721.06, average training loss: 12220.01, base loss: 19694.36
[INFO 2017-06-27 20:42:24,441 main.py:51] epoch 2778, training loss: 12472.01, average training loss: 12220.99, base loss: 19697.00
[INFO 2017-06-27 20:42:24,834 main.py:51] epoch 2779, training loss: 11809.92, average training loss: 12221.52, base loss: 19699.44
[INFO 2017-06-27 20:42:25,213 main.py:51] epoch 2780, training loss: 10401.20, average training loss: 12218.43, base loss: 19695.56
[INFO 2017-06-27 20:42:25,609 main.py:51] epoch 2781, training loss: 10787.94, average training loss: 12217.32, base loss: 19693.75
[INFO 2017-06-27 20:42:25,992 main.py:51] epoch 2782, training loss: 12893.29, average training loss: 12219.57, base loss: 19698.94
[INFO 2017-06-27 20:42:26,392 main.py:51] epoch 2783, training loss: 11165.04, average training loss: 12218.55, base loss: 19699.36
[INFO 2017-06-27 20:42:26,774 main.py:51] epoch 2784, training loss: 12140.14, average training loss: 12216.79, base loss: 19697.06
[INFO 2017-06-27 20:42:27,153 main.py:51] epoch 2785, training loss: 11385.42, average training loss: 12216.17, base loss: 19696.48
[INFO 2017-06-27 20:42:27,528 main.py:51] epoch 2786, training loss: 10558.39, average training loss: 12214.05, base loss: 19693.49
[INFO 2017-06-27 20:42:27,906 main.py:51] epoch 2787, training loss: 11377.61, average training loss: 12210.90, base loss: 19689.54
[INFO 2017-06-27 20:42:28,282 main.py:51] epoch 2788, training loss: 12438.29, average training loss: 12211.11, base loss: 19691.26
[INFO 2017-06-27 20:42:28,659 main.py:51] epoch 2789, training loss: 12486.71, average training loss: 12212.24, base loss: 19695.35
[INFO 2017-06-27 20:42:29,037 main.py:51] epoch 2790, training loss: 12011.23, average training loss: 12209.56, base loss: 19691.99
[INFO 2017-06-27 20:42:29,413 main.py:51] epoch 2791, training loss: 12435.18, average training loss: 12210.24, base loss: 19695.26
[INFO 2017-06-27 20:42:29,792 main.py:51] epoch 2792, training loss: 13003.55, average training loss: 12209.69, base loss: 19696.47
[INFO 2017-06-27 20:42:30,167 main.py:51] epoch 2793, training loss: 10797.41, average training loss: 12207.70, base loss: 19694.07
[INFO 2017-06-27 20:42:30,542 main.py:51] epoch 2794, training loss: 11039.40, average training loss: 12207.67, base loss: 19694.64
[INFO 2017-06-27 20:42:30,918 main.py:51] epoch 2795, training loss: 10987.40, average training loss: 12208.46, base loss: 19697.17
[INFO 2017-06-27 20:42:31,294 main.py:51] epoch 2796, training loss: 12881.84, average training loss: 12208.03, base loss: 19697.93
[INFO 2017-06-27 20:42:31,672 main.py:51] epoch 2797, training loss: 12180.81, average training loss: 12206.42, base loss: 19696.42
[INFO 2017-06-27 20:42:32,049 main.py:51] epoch 2798, training loss: 12089.51, average training loss: 12206.53, base loss: 19698.30
[INFO 2017-06-27 20:42:32,429 main.py:51] epoch 2799, training loss: 12136.62, average training loss: 12205.27, base loss: 19695.34
[INFO 2017-06-27 20:42:32,429 main.py:53] epoch 2799, testing
[INFO 2017-06-27 20:42:34,030 main.py:105] average testing loss: 11985.91, base loss: 19929.79
[INFO 2017-06-27 20:42:34,031 main.py:106] improve_loss: 7943.88, improve_percent: 0.40
[INFO 2017-06-27 20:42:34,031 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:42:34,043 main.py:76] current best improved percent: 0.40
[INFO 2017-06-27 20:42:34,419 main.py:51] epoch 2800, training loss: 12280.63, average training loss: 12205.81, base loss: 19697.96
[INFO 2017-06-27 20:42:34,795 main.py:51] epoch 2801, training loss: 10718.44, average training loss: 12204.41, base loss: 19695.70
[INFO 2017-06-27 20:42:35,171 main.py:51] epoch 2802, training loss: 10511.49, average training loss: 12200.71, base loss: 19689.07
[INFO 2017-06-27 20:42:35,554 main.py:51] epoch 2803, training loss: 12997.84, average training loss: 12200.82, base loss: 19690.56
[INFO 2017-06-27 20:42:35,931 main.py:51] epoch 2804, training loss: 11170.50, average training loss: 12198.49, base loss: 19686.70
[INFO 2017-06-27 20:42:36,306 main.py:51] epoch 2805, training loss: 10970.36, average training loss: 12196.67, base loss: 19683.84
[INFO 2017-06-27 20:42:36,682 main.py:51] epoch 2806, training loss: 12249.97, average training loss: 12196.20, base loss: 19685.55
[INFO 2017-06-27 20:42:37,056 main.py:51] epoch 2807, training loss: 11045.75, average training loss: 12196.25, base loss: 19687.70
[INFO 2017-06-27 20:42:37,426 main.py:51] epoch 2808, training loss: 12483.93, average training loss: 12197.26, base loss: 19690.53
[INFO 2017-06-27 20:42:37,892 main.py:51] epoch 2809, training loss: 11451.59, average training loss: 12195.65, base loss: 19689.72
[INFO 2017-06-27 20:42:38,299 main.py:51] epoch 2810, training loss: 12193.30, average training loss: 12194.75, base loss: 19690.68
[INFO 2017-06-27 20:42:38,698 main.py:51] epoch 2811, training loss: 13002.03, average training loss: 12194.94, base loss: 19693.07
[INFO 2017-06-27 20:42:39,078 main.py:51] epoch 2812, training loss: 12483.63, average training loss: 12192.06, base loss: 19689.87
[INFO 2017-06-27 20:42:39,460 main.py:51] epoch 2813, training loss: 11869.08, average training loss: 12190.16, base loss: 19688.20
[INFO 2017-06-27 20:42:39,862 main.py:51] epoch 2814, training loss: 13005.09, average training loss: 12189.64, base loss: 19689.81
[INFO 2017-06-27 20:42:40,238 main.py:51] epoch 2815, training loss: 10615.29, average training loss: 12189.13, base loss: 19690.12
[INFO 2017-06-27 20:42:40,613 main.py:51] epoch 2816, training loss: 10711.08, average training loss: 12188.32, base loss: 19689.19
[INFO 2017-06-27 20:42:40,990 main.py:51] epoch 2817, training loss: 11600.06, average training loss: 12187.87, base loss: 19689.29
[INFO 2017-06-27 20:42:41,374 main.py:51] epoch 2818, training loss: 12232.82, average training loss: 12188.05, base loss: 19690.25
[INFO 2017-06-27 20:42:41,751 main.py:51] epoch 2819, training loss: 13315.14, average training loss: 12189.51, base loss: 19693.84
[INFO 2017-06-27 20:42:42,125 main.py:51] epoch 2820, training loss: 11724.76, average training loss: 12189.56, base loss: 19697.19
[INFO 2017-06-27 20:42:42,507 main.py:51] epoch 2821, training loss: 11111.39, average training loss: 12188.31, base loss: 19695.22
[INFO 2017-06-27 20:42:42,879 main.py:51] epoch 2822, training loss: 11825.76, average training loss: 12186.61, base loss: 19692.83
[INFO 2017-06-27 20:42:43,253 main.py:51] epoch 2823, training loss: 11353.00, average training loss: 12185.77, base loss: 19692.92
[INFO 2017-06-27 20:42:43,632 main.py:51] epoch 2824, training loss: 11725.03, average training loss: 12185.94, base loss: 19695.21
[INFO 2017-06-27 20:42:44,007 main.py:51] epoch 2825, training loss: 11630.48, average training loss: 12185.27, base loss: 19694.54
[INFO 2017-06-27 20:42:44,387 main.py:51] epoch 2826, training loss: 11480.31, average training loss: 12186.50, base loss: 19698.51
[INFO 2017-06-27 20:42:44,762 main.py:51] epoch 2827, training loss: 12222.76, average training loss: 12185.48, base loss: 19697.81
[INFO 2017-06-27 20:42:45,137 main.py:51] epoch 2828, training loss: 12056.00, average training loss: 12184.75, base loss: 19696.71
[INFO 2017-06-27 20:42:45,515 main.py:51] epoch 2829, training loss: 11922.19, average training loss: 12184.97, base loss: 19698.83
[INFO 2017-06-27 20:42:45,891 main.py:51] epoch 2830, training loss: 13019.88, average training loss: 12185.49, base loss: 19700.93
[INFO 2017-06-27 20:42:46,266 main.py:51] epoch 2831, training loss: 12607.68, average training loss: 12184.46, base loss: 19698.57
[INFO 2017-06-27 20:42:46,642 main.py:51] epoch 2832, training loss: 11263.10, average training loss: 12182.81, base loss: 19695.89
[INFO 2017-06-27 20:42:47,018 main.py:51] epoch 2833, training loss: 12220.86, average training loss: 12183.92, base loss: 19696.43
[INFO 2017-06-27 20:42:47,392 main.py:51] epoch 2834, training loss: 10736.65, average training loss: 12182.82, base loss: 19696.09
[INFO 2017-06-27 20:42:47,761 main.py:51] epoch 2835, training loss: 12411.91, average training loss: 12180.25, base loss: 19693.07
[INFO 2017-06-27 20:42:48,134 main.py:51] epoch 2836, training loss: 13074.31, average training loss: 12181.91, base loss: 19698.32
[INFO 2017-06-27 20:42:48,508 main.py:51] epoch 2837, training loss: 12197.80, average training loss: 12183.63, base loss: 19701.27
[INFO 2017-06-27 20:42:48,878 main.py:51] epoch 2838, training loss: 10457.05, average training loss: 12180.29, base loss: 19694.26
[INFO 2017-06-27 20:42:49,258 main.py:51] epoch 2839, training loss: 11056.87, average training loss: 12179.40, base loss: 19694.79
[INFO 2017-06-27 20:42:49,632 main.py:51] epoch 2840, training loss: 12225.83, average training loss: 12178.23, base loss: 19695.08
[INFO 2017-06-27 20:42:50,007 main.py:51] epoch 2841, training loss: 10904.99, average training loss: 12175.41, base loss: 19690.66
[INFO 2017-06-27 20:42:50,477 main.py:51] epoch 2842, training loss: 11792.27, average training loss: 12174.00, base loss: 19688.27
[INFO 2017-06-27 20:42:50,860 main.py:51] epoch 2843, training loss: 12931.12, average training loss: 12173.91, base loss: 19688.62
[INFO 2017-06-27 20:42:51,246 main.py:51] epoch 2844, training loss: 11903.23, average training loss: 12173.94, base loss: 19689.98
[INFO 2017-06-27 20:42:51,639 main.py:51] epoch 2845, training loss: 12343.84, average training loss: 12172.53, base loss: 19689.30
[INFO 2017-06-27 20:42:52,041 main.py:51] epoch 2846, training loss: 10624.18, average training loss: 12170.61, base loss: 19686.24
[INFO 2017-06-27 20:42:52,417 main.py:51] epoch 2847, training loss: 10908.64, average training loss: 12169.49, base loss: 19684.24
[INFO 2017-06-27 20:42:52,793 main.py:51] epoch 2848, training loss: 13237.53, average training loss: 12170.70, base loss: 19687.55
[INFO 2017-06-27 20:42:53,170 main.py:51] epoch 2849, training loss: 12913.94, average training loss: 12172.77, base loss: 19692.74
[INFO 2017-06-27 20:42:53,548 main.py:51] epoch 2850, training loss: 12965.40, average training loss: 12173.93, base loss: 19696.36
[INFO 2017-06-27 20:42:53,924 main.py:51] epoch 2851, training loss: 12599.07, average training loss: 12174.85, base loss: 19698.66
[INFO 2017-06-27 20:42:54,301 main.py:51] epoch 2852, training loss: 11311.75, average training loss: 12175.17, base loss: 19702.15
[INFO 2017-06-27 20:42:54,748 main.py:51] epoch 2853, training loss: 11860.35, average training loss: 12173.28, base loss: 19701.04
[INFO 2017-06-27 20:42:55,150 main.py:51] epoch 2854, training loss: 14158.53, average training loss: 12175.88, base loss: 19707.34
[INFO 2017-06-27 20:42:55,527 main.py:51] epoch 2855, training loss: 13821.38, average training loss: 12177.16, base loss: 19710.88
[INFO 2017-06-27 20:42:55,913 main.py:51] epoch 2856, training loss: 10873.31, average training loss: 12176.74, base loss: 19710.55
[INFO 2017-06-27 20:42:56,291 main.py:51] epoch 2857, training loss: 14343.76, average training loss: 12177.14, base loss: 19712.96
[INFO 2017-06-27 20:42:56,667 main.py:51] epoch 2858, training loss: 10999.70, average training loss: 12174.06, base loss: 19707.38
[INFO 2017-06-27 20:42:57,129 main.py:51] epoch 2859, training loss: 12103.66, average training loss: 12174.21, base loss: 19710.15
[INFO 2017-06-27 20:42:57,520 main.py:51] epoch 2860, training loss: 14365.70, average training loss: 12176.34, base loss: 19713.93
[INFO 2017-06-27 20:42:57,900 main.py:51] epoch 2861, training loss: 10753.49, average training loss: 12174.38, base loss: 19709.81
[INFO 2017-06-27 20:42:58,277 main.py:51] epoch 2862, training loss: 12887.49, average training loss: 12174.97, base loss: 19711.61
[INFO 2017-06-27 20:42:58,655 main.py:51] epoch 2863, training loss: 10919.10, average training loss: 12174.42, base loss: 19711.00
[INFO 2017-06-27 20:42:59,046 main.py:51] epoch 2864, training loss: 12785.48, average training loss: 12174.06, base loss: 19712.85
[INFO 2017-06-27 20:42:59,424 main.py:51] epoch 2865, training loss: 11024.54, average training loss: 12171.10, base loss: 19706.71
[INFO 2017-06-27 20:42:59,796 main.py:51] epoch 2866, training loss: 12282.84, average training loss: 12170.37, base loss: 19705.78
[INFO 2017-06-27 20:43:00,172 main.py:51] epoch 2867, training loss: 12927.80, average training loss: 12169.44, base loss: 19704.42
[INFO 2017-06-27 20:43:00,543 main.py:51] epoch 2868, training loss: 12829.55, average training loss: 12168.05, base loss: 19704.45
[INFO 2017-06-27 20:43:00,919 main.py:51] epoch 2869, training loss: 13698.53, average training loss: 12169.23, base loss: 19707.56
[INFO 2017-06-27 20:43:01,298 main.py:51] epoch 2870, training loss: 11882.16, average training loss: 12168.50, base loss: 19707.81
[INFO 2017-06-27 20:43:01,775 main.py:51] epoch 2871, training loss: 11842.14, average training loss: 12168.13, base loss: 19707.83
[INFO 2017-06-27 20:43:02,159 main.py:51] epoch 2872, training loss: 11369.88, average training loss: 12167.21, base loss: 19706.57
[INFO 2017-06-27 20:43:02,538 main.py:51] epoch 2873, training loss: 12330.63, average training loss: 12167.18, base loss: 19707.89
[INFO 2017-06-27 20:43:02,917 main.py:51] epoch 2874, training loss: 11349.10, average training loss: 12165.71, base loss: 19706.48
[INFO 2017-06-27 20:43:03,387 main.py:51] epoch 2875, training loss: 12003.11, average training loss: 12163.81, base loss: 19703.22
[INFO 2017-06-27 20:43:03,778 main.py:51] epoch 2876, training loss: 11596.23, average training loss: 12162.32, base loss: 19701.46
[INFO 2017-06-27 20:43:04,155 main.py:51] epoch 2877, training loss: 10112.25, average training loss: 12158.02, base loss: 19694.19
[INFO 2017-06-27 20:43:04,613 main.py:51] epoch 2878, training loss: 11708.24, average training loss: 12156.49, base loss: 19692.06
[INFO 2017-06-27 20:43:05,068 main.py:51] epoch 2879, training loss: 10644.40, average training loss: 12153.73, base loss: 19688.56
[INFO 2017-06-27 20:43:05,480 main.py:51] epoch 2880, training loss: 12966.89, average training loss: 12154.39, base loss: 19690.98
[INFO 2017-06-27 20:43:05,932 main.py:51] epoch 2881, training loss: 11940.06, average training loss: 12154.69, base loss: 19693.79
[INFO 2017-06-27 20:43:06,336 main.py:51] epoch 2882, training loss: 12233.93, average training loss: 12153.88, base loss: 19692.82
[INFO 2017-06-27 20:43:06,719 main.py:51] epoch 2883, training loss: 12485.62, average training loss: 12154.54, base loss: 19695.07
[INFO 2017-06-27 20:43:07,106 main.py:51] epoch 2884, training loss: 12256.73, average training loss: 12153.08, base loss: 19693.90
[INFO 2017-06-27 20:43:07,482 main.py:51] epoch 2885, training loss: 11815.99, average training loss: 12153.20, base loss: 19696.10
[INFO 2017-06-27 20:43:07,865 main.py:51] epoch 2886, training loss: 10851.87, average training loss: 12151.15, base loss: 19692.34
[INFO 2017-06-27 20:43:08,240 main.py:51] epoch 2887, training loss: 14495.03, average training loss: 12153.61, base loss: 19696.75
[INFO 2017-06-27 20:43:08,611 main.py:51] epoch 2888, training loss: 12549.04, average training loss: 12155.55, base loss: 19702.54
[INFO 2017-06-27 20:43:08,986 main.py:51] epoch 2889, training loss: 11695.92, average training loss: 12154.63, base loss: 19701.33
[INFO 2017-06-27 20:43:09,363 main.py:51] epoch 2890, training loss: 10945.74, average training loss: 12150.72, base loss: 19694.79
[INFO 2017-06-27 20:43:09,739 main.py:51] epoch 2891, training loss: 9994.23, average training loss: 12148.83, base loss: 19690.54
[INFO 2017-06-27 20:43:10,114 main.py:51] epoch 2892, training loss: 12257.31, average training loss: 12147.75, base loss: 19690.04
[INFO 2017-06-27 20:43:10,485 main.py:51] epoch 2893, training loss: 11871.99, average training loss: 12145.17, base loss: 19686.46
[INFO 2017-06-27 20:43:10,860 main.py:51] epoch 2894, training loss: 11939.83, average training loss: 12144.45, base loss: 19684.98
[INFO 2017-06-27 20:43:11,235 main.py:51] epoch 2895, training loss: 9886.29, average training loss: 12141.83, base loss: 19681.02
[INFO 2017-06-27 20:43:11,613 main.py:51] epoch 2896, training loss: 12949.53, average training loss: 12142.69, base loss: 19682.96
[INFO 2017-06-27 20:43:11,995 main.py:51] epoch 2897, training loss: 11265.10, average training loss: 12142.02, base loss: 19683.21
[INFO 2017-06-27 20:43:12,369 main.py:51] epoch 2898, training loss: 10907.58, average training loss: 12141.20, base loss: 19682.31
[INFO 2017-06-27 20:43:12,745 main.py:51] epoch 2899, training loss: 13553.30, average training loss: 12142.67, base loss: 19686.16
[INFO 2017-06-27 20:43:12,745 main.py:53] epoch 2899, testing
[INFO 2017-06-27 20:43:14,443 main.py:105] average testing loss: 12423.03, base loss: 20443.88
[INFO 2017-06-27 20:43:14,443 main.py:106] improve_loss: 8020.84, improve_percent: 0.39
[INFO 2017-06-27 20:43:14,444 main.py:76] current best improved percent: 0.40
[INFO 2017-06-27 20:43:14,819 main.py:51] epoch 2900, training loss: 11293.88, average training loss: 12140.79, base loss: 19684.06
[INFO 2017-06-27 20:43:15,195 main.py:51] epoch 2901, training loss: 12308.10, average training loss: 12141.39, base loss: 19687.34
[INFO 2017-06-27 20:43:15,571 main.py:51] epoch 2902, training loss: 11144.72, average training loss: 12140.14, base loss: 19685.05
[INFO 2017-06-27 20:43:15,947 main.py:51] epoch 2903, training loss: 10340.89, average training loss: 12138.45, base loss: 19682.85
[INFO 2017-06-27 20:43:16,325 main.py:51] epoch 2904, training loss: 11779.11, average training loss: 12137.80, base loss: 19683.41
[INFO 2017-06-27 20:43:16,703 main.py:51] epoch 2905, training loss: 14174.93, average training loss: 12139.87, base loss: 19688.17
[INFO 2017-06-27 20:43:17,074 main.py:51] epoch 2906, training loss: 12322.73, average training loss: 12139.38, base loss: 19688.72
[INFO 2017-06-27 20:43:17,449 main.py:51] epoch 2907, training loss: 11967.74, average training loss: 12138.92, base loss: 19689.70
[INFO 2017-06-27 20:43:17,823 main.py:51] epoch 2908, training loss: 12417.50, average training loss: 12140.13, base loss: 19692.91
[INFO 2017-06-27 20:43:18,218 main.py:51] epoch 2909, training loss: 12777.70, average training loss: 12140.54, base loss: 19694.59
[INFO 2017-06-27 20:43:18,594 main.py:51] epoch 2910, training loss: 12000.05, average training loss: 12140.83, base loss: 19695.69
[INFO 2017-06-27 20:43:18,969 main.py:51] epoch 2911, training loss: 11195.49, average training loss: 12140.22, base loss: 19694.39
[INFO 2017-06-27 20:43:19,345 main.py:51] epoch 2912, training loss: 11435.87, average training loss: 12139.41, base loss: 19694.46
[INFO 2017-06-27 20:43:19,724 main.py:51] epoch 2913, training loss: 10457.59, average training loss: 12137.91, base loss: 19692.55
[INFO 2017-06-27 20:43:20,102 main.py:51] epoch 2914, training loss: 11203.45, average training loss: 12137.74, base loss: 19693.60
[INFO 2017-06-27 20:43:20,477 main.py:51] epoch 2915, training loss: 11220.39, average training loss: 12136.02, base loss: 19693.02
[INFO 2017-06-27 20:43:20,852 main.py:51] epoch 2916, training loss: 12072.14, average training loss: 12132.89, base loss: 19689.57
[INFO 2017-06-27 20:43:21,228 main.py:51] epoch 2917, training loss: 10283.92, average training loss: 12131.66, base loss: 19687.55
[INFO 2017-06-27 20:43:21,605 main.py:51] epoch 2918, training loss: 11034.01, average training loss: 12130.41, base loss: 19686.90
[INFO 2017-06-27 20:43:21,981 main.py:51] epoch 2919, training loss: 10961.89, average training loss: 12128.26, base loss: 19683.80
[INFO 2017-06-27 20:43:22,356 main.py:51] epoch 2920, training loss: 12412.63, average training loss: 12128.29, base loss: 19686.11
[INFO 2017-06-27 20:43:22,731 main.py:51] epoch 2921, training loss: 13644.88, average training loss: 12131.71, base loss: 19694.57
[INFO 2017-06-27 20:43:23,108 main.py:51] epoch 2922, training loss: 11971.67, average training loss: 12131.55, base loss: 19696.04
[INFO 2017-06-27 20:43:23,489 main.py:51] epoch 2923, training loss: 11455.45, average training loss: 12131.65, base loss: 19696.95
[INFO 2017-06-27 20:43:23,870 main.py:51] epoch 2924, training loss: 12178.93, average training loss: 12131.17, base loss: 19697.04
[INFO 2017-06-27 20:43:24,248 main.py:51] epoch 2925, training loss: 12387.94, average training loss: 12131.59, base loss: 19698.25
[INFO 2017-06-27 20:43:24,620 main.py:51] epoch 2926, training loss: 10930.73, average training loss: 12129.30, base loss: 19694.90
[INFO 2017-06-27 20:43:24,994 main.py:51] epoch 2927, training loss: 9873.47, average training loss: 12128.20, base loss: 19693.38
[INFO 2017-06-27 20:43:25,430 main.py:51] epoch 2928, training loss: 11368.78, average training loss: 12127.14, base loss: 19691.87
[INFO 2017-06-27 20:43:25,814 main.py:51] epoch 2929, training loss: 11398.21, average training loss: 12125.27, base loss: 19688.64
[INFO 2017-06-27 20:43:26,198 main.py:51] epoch 2930, training loss: 14303.74, average training loss: 12125.54, base loss: 19691.05
[INFO 2017-06-27 20:43:26,580 main.py:51] epoch 2931, training loss: 11683.88, average training loss: 12124.74, base loss: 19692.34
[INFO 2017-06-27 20:43:26,964 main.py:51] epoch 2932, training loss: 11131.25, average training loss: 12124.49, base loss: 19693.17
[INFO 2017-06-27 20:43:27,341 main.py:51] epoch 2933, training loss: 13448.82, average training loss: 12125.44, base loss: 19695.15
[INFO 2017-06-27 20:43:27,723 main.py:51] epoch 2934, training loss: 11677.13, average training loss: 12124.76, base loss: 19695.81
[INFO 2017-06-27 20:43:28,101 main.py:51] epoch 2935, training loss: 13403.13, average training loss: 12124.89, base loss: 19696.73
[INFO 2017-06-27 20:43:28,477 main.py:51] epoch 2936, training loss: 11253.80, average training loss: 12122.71, base loss: 19693.81
[INFO 2017-06-27 20:43:28,870 main.py:51] epoch 2937, training loss: 10750.34, average training loss: 12121.53, base loss: 19692.41
[INFO 2017-06-27 20:43:29,271 main.py:51] epoch 2938, training loss: 11734.83, average training loss: 12120.95, base loss: 19692.48
[INFO 2017-06-27 20:43:29,649 main.py:51] epoch 2939, training loss: 12888.11, average training loss: 12121.63, base loss: 19694.28
[INFO 2017-06-27 20:43:30,029 main.py:51] epoch 2940, training loss: 12336.05, average training loss: 12122.22, base loss: 19696.84
[INFO 2017-06-27 20:43:30,410 main.py:51] epoch 2941, training loss: 10757.42, average training loss: 12120.57, base loss: 19694.72
[INFO 2017-06-27 20:43:30,786 main.py:51] epoch 2942, training loss: 10183.72, average training loss: 12115.76, base loss: 19685.78
[INFO 2017-06-27 20:43:31,163 main.py:51] epoch 2943, training loss: 12697.58, average training loss: 12117.05, base loss: 19690.04
[INFO 2017-06-27 20:43:31,545 main.py:51] epoch 2944, training loss: 13253.38, average training loss: 12116.53, base loss: 19690.45
[INFO 2017-06-27 20:43:31,923 main.py:51] epoch 2945, training loss: 11309.53, average training loss: 12116.41, base loss: 19690.35
[INFO 2017-06-27 20:43:32,300 main.py:51] epoch 2946, training loss: 13735.33, average training loss: 12118.53, base loss: 19696.32
[INFO 2017-06-27 20:43:32,677 main.py:51] epoch 2947, training loss: 13382.17, average training loss: 12119.49, base loss: 19698.40
[INFO 2017-06-27 20:43:33,054 main.py:51] epoch 2948, training loss: 12242.88, average training loss: 12119.18, base loss: 19699.05
[INFO 2017-06-27 20:43:33,430 main.py:51] epoch 2949, training loss: 10863.61, average training loss: 12117.84, base loss: 19696.28
[INFO 2017-06-27 20:43:33,807 main.py:51] epoch 2950, training loss: 10735.03, average training loss: 12116.42, base loss: 19694.10
[INFO 2017-06-27 20:43:34,186 main.py:51] epoch 2951, training loss: 11058.37, average training loss: 12116.14, base loss: 19694.52
[INFO 2017-06-27 20:43:34,562 main.py:51] epoch 2952, training loss: 13066.78, average training loss: 12116.90, base loss: 19696.47
[INFO 2017-06-27 20:43:35,014 main.py:51] epoch 2953, training loss: 10499.63, average training loss: 12112.37, base loss: 19689.18
[INFO 2017-06-27 20:43:35,420 main.py:51] epoch 2954, training loss: 11652.46, average training loss: 12111.45, base loss: 19688.98
[INFO 2017-06-27 20:43:35,797 main.py:51] epoch 2955, training loss: 11368.48, average training loss: 12110.77, base loss: 19689.51
[INFO 2017-06-27 20:43:36,177 main.py:51] epoch 2956, training loss: 11162.24, average training loss: 12108.10, base loss: 19685.58
[INFO 2017-06-27 20:43:36,560 main.py:51] epoch 2957, training loss: 13078.69, average training loss: 12109.66, base loss: 19690.91
[INFO 2017-06-27 20:43:36,946 main.py:51] epoch 2958, training loss: 11629.44, average training loss: 12108.03, base loss: 19689.63
[INFO 2017-06-27 20:43:37,323 main.py:51] epoch 2959, training loss: 12142.11, average training loss: 12108.46, base loss: 19691.05
[INFO 2017-06-27 20:43:37,696 main.py:51] epoch 2960, training loss: 11390.88, average training loss: 12108.43, base loss: 19691.09
[INFO 2017-06-27 20:43:38,069 main.py:51] epoch 2961, training loss: 12160.04, average training loss: 12107.78, base loss: 19692.01
[INFO 2017-06-27 20:43:38,444 main.py:51] epoch 2962, training loss: 9837.15, average training loss: 12104.63, base loss: 19687.99
[INFO 2017-06-27 20:43:38,815 main.py:51] epoch 2963, training loss: 11580.62, average training loss: 12102.80, base loss: 19685.83
[INFO 2017-06-27 20:43:39,195 main.py:51] epoch 2964, training loss: 11682.23, average training loss: 12103.35, base loss: 19687.52
[INFO 2017-06-27 20:43:39,572 main.py:51] epoch 2965, training loss: 12225.01, average training loss: 12103.37, base loss: 19689.90
[INFO 2017-06-27 20:43:39,949 main.py:51] epoch 2966, training loss: 12426.08, average training loss: 12102.73, base loss: 19690.35
[INFO 2017-06-27 20:43:40,325 main.py:51] epoch 2967, training loss: 10129.72, average training loss: 12098.72, base loss: 19683.69
[INFO 2017-06-27 20:43:40,697 main.py:51] epoch 2968, training loss: 10818.24, average training loss: 12094.02, base loss: 19677.06
[INFO 2017-06-27 20:43:41,108 main.py:51] epoch 2969, training loss: 11102.12, average training loss: 12093.16, base loss: 19676.04
[INFO 2017-06-27 20:43:41,524 main.py:51] epoch 2970, training loss: 10080.78, average training loss: 12091.55, base loss: 19672.80
[INFO 2017-06-27 20:43:41,905 main.py:51] epoch 2971, training loss: 11010.52, average training loss: 12090.48, base loss: 19670.56
[INFO 2017-06-27 20:43:42,282 main.py:51] epoch 2972, training loss: 11694.51, average training loss: 12090.26, base loss: 19671.30
[INFO 2017-06-27 20:43:42,674 main.py:51] epoch 2973, training loss: 11662.19, average training loss: 12090.74, base loss: 19673.99
[INFO 2017-06-27 20:43:43,060 main.py:51] epoch 2974, training loss: 9401.77, average training loss: 12086.16, base loss: 19667.34
[INFO 2017-06-27 20:43:43,460 main.py:51] epoch 2975, training loss: 11153.31, average training loss: 12083.84, base loss: 19664.39
[INFO 2017-06-27 20:43:43,841 main.py:51] epoch 2976, training loss: 12585.65, average training loss: 12083.42, base loss: 19664.76
[INFO 2017-06-27 20:43:44,217 main.py:51] epoch 2977, training loss: 12175.72, average training loss: 12083.88, base loss: 19667.52
[INFO 2017-06-27 20:43:44,591 main.py:51] epoch 2978, training loss: 13076.94, average training loss: 12083.47, base loss: 19669.35
[INFO 2017-06-27 20:43:44,969 main.py:51] epoch 2979, training loss: 11276.69, average training loss: 12083.07, base loss: 19670.86
[INFO 2017-06-27 20:43:45,346 main.py:51] epoch 2980, training loss: 13038.20, average training loss: 12084.34, base loss: 19675.33
[INFO 2017-06-27 20:43:45,724 main.py:51] epoch 2981, training loss: 11326.01, average training loss: 12083.89, base loss: 19674.30
[INFO 2017-06-27 20:43:46,099 main.py:51] epoch 2982, training loss: 10622.32, average training loss: 12082.72, base loss: 19674.32
[INFO 2017-06-27 20:43:46,484 main.py:51] epoch 2983, training loss: 11927.63, average training loss: 12081.93, base loss: 19674.01
[INFO 2017-06-27 20:43:46,861 main.py:51] epoch 2984, training loss: 11192.94, average training loss: 12081.83, base loss: 19675.43
[INFO 2017-06-27 20:43:47,239 main.py:51] epoch 2985, training loss: 11111.67, average training loss: 12081.23, base loss: 19675.17
[INFO 2017-06-27 20:43:47,620 main.py:51] epoch 2986, training loss: 11870.42, average training loss: 12079.91, base loss: 19675.35
[INFO 2017-06-27 20:43:47,997 main.py:51] epoch 2987, training loss: 12407.99, average training loss: 12080.62, base loss: 19678.59
[INFO 2017-06-27 20:43:48,374 main.py:51] epoch 2988, training loss: 10637.31, average training loss: 12079.06, base loss: 19678.27
[INFO 2017-06-27 20:43:48,750 main.py:51] epoch 2989, training loss: 11850.11, average training loss: 12079.33, base loss: 19679.73
[INFO 2017-06-27 20:43:49,127 main.py:51] epoch 2990, training loss: 11384.58, average training loss: 12077.59, base loss: 19677.28
[INFO 2017-06-27 20:43:49,500 main.py:51] epoch 2991, training loss: 12736.10, average training loss: 12078.85, base loss: 19681.52
[INFO 2017-06-27 20:43:49,877 main.py:51] epoch 2992, training loss: 11541.93, average training loss: 12077.68, base loss: 19680.65
[INFO 2017-06-27 20:43:50,255 main.py:51] epoch 2993, training loss: 11622.18, average training loss: 12078.50, base loss: 19683.73
[INFO 2017-06-27 20:43:50,633 main.py:51] epoch 2994, training loss: 11851.10, average training loss: 12077.03, base loss: 19681.28
[INFO 2017-06-27 20:43:51,101 main.py:51] epoch 2995, training loss: 11223.67, average training loss: 12075.84, base loss: 19681.76
[INFO 2017-06-27 20:43:51,490 main.py:51] epoch 2996, training loss: 10264.08, average training loss: 12073.94, base loss: 19679.97
[INFO 2017-06-27 20:43:51,876 main.py:51] epoch 2997, training loss: 11353.29, average training loss: 12072.41, base loss: 19678.48
[INFO 2017-06-27 20:43:52,254 main.py:51] epoch 2998, training loss: 14412.18, average training loss: 12071.43, base loss: 19677.27
[INFO 2017-06-27 20:43:52,637 main.py:51] epoch 2999, training loss: 11661.97, average training loss: 12070.41, base loss: 19676.80
[INFO 2017-06-27 20:43:52,637 main.py:53] epoch 2999, testing
[INFO 2017-06-27 20:43:54,254 main.py:105] average testing loss: 11558.35, base loss: 19136.07
[INFO 2017-06-27 20:43:54,254 main.py:106] improve_loss: 7577.72, improve_percent: 0.40
[INFO 2017-06-27 20:43:54,254 main.py:76] current best improved percent: 0.40
[INFO 2017-06-27 20:43:54,631 main.py:51] epoch 3000, training loss: 11811.50, average training loss: 12069.92, base loss: 19678.26
[INFO 2017-06-27 20:43:55,004 main.py:51] epoch 3001, training loss: 11204.79, average training loss: 12068.04, base loss: 19674.42
[INFO 2017-06-27 20:43:55,386 main.py:51] epoch 3002, training loss: 12024.26, average training loss: 12068.94, base loss: 19678.26
[INFO 2017-06-27 20:43:55,764 main.py:51] epoch 3003, training loss: 12990.24, average training loss: 12069.24, base loss: 19680.32
[INFO 2017-06-27 20:43:56,149 main.py:51] epoch 3004, training loss: 11574.52, average training loss: 12068.78, base loss: 19679.77
[INFO 2017-06-27 20:43:56,526 main.py:51] epoch 3005, training loss: 10504.90, average training loss: 12065.40, base loss: 19674.71
[INFO 2017-06-27 20:43:56,903 main.py:51] epoch 3006, training loss: 12169.03, average training loss: 12064.60, base loss: 19675.10
[INFO 2017-06-27 20:43:57,284 main.py:51] epoch 3007, training loss: 10983.92, average training loss: 12063.47, base loss: 19671.93
[INFO 2017-06-27 20:43:57,662 main.py:51] epoch 3008, training loss: 11782.63, average training loss: 12062.14, base loss: 19670.60
[INFO 2017-06-27 20:43:58,040 main.py:51] epoch 3009, training loss: 11943.70, average training loss: 12061.38, base loss: 19671.28
[INFO 2017-06-27 20:43:58,462 main.py:51] epoch 3010, training loss: 10721.39, average training loss: 12059.78, base loss: 19669.34
[INFO 2017-06-27 20:43:58,884 main.py:51] epoch 3011, training loss: 14218.44, average training loss: 12060.32, base loss: 19672.40
[INFO 2017-06-27 20:43:59,268 main.py:51] epoch 3012, training loss: 13676.74, average training loss: 12060.32, base loss: 19673.36
[INFO 2017-06-27 20:43:59,655 main.py:51] epoch 3013, training loss: 11996.94, average training loss: 12060.39, base loss: 19674.58
[INFO 2017-06-27 20:44:00,122 main.py:51] epoch 3014, training loss: 10619.78, average training loss: 12058.54, base loss: 19671.96
[INFO 2017-06-27 20:44:00,523 main.py:51] epoch 3015, training loss: 12749.32, average training loss: 12058.67, base loss: 19673.92
[INFO 2017-06-27 20:44:00,908 main.py:51] epoch 3016, training loss: 12392.24, average training loss: 12057.87, base loss: 19671.64
[INFO 2017-06-27 20:44:01,292 main.py:51] epoch 3017, training loss: 11125.35, average training loss: 12057.08, base loss: 19670.92
[INFO 2017-06-27 20:44:01,694 main.py:51] epoch 3018, training loss: 11276.38, average training loss: 12056.76, base loss: 19671.32
[INFO 2017-06-27 20:44:02,071 main.py:51] epoch 3019, training loss: 12449.83, average training loss: 12057.49, base loss: 19673.88
[INFO 2017-06-27 20:44:02,445 main.py:51] epoch 3020, training loss: 10115.09, average training loss: 12054.88, base loss: 19669.01
[INFO 2017-06-27 20:44:02,818 main.py:51] epoch 3021, training loss: 13082.08, average training loss: 12056.09, base loss: 19672.02
[INFO 2017-06-27 20:44:03,195 main.py:51] epoch 3022, training loss: 10717.82, average training loss: 12054.53, base loss: 19670.58
[INFO 2017-06-27 20:44:03,569 main.py:51] epoch 3023, training loss: 11499.85, average training loss: 12053.62, base loss: 19670.18
[INFO 2017-06-27 20:44:03,943 main.py:51] epoch 3024, training loss: 14210.24, average training loss: 12055.46, base loss: 19675.07
[INFO 2017-06-27 20:44:04,321 main.py:51] epoch 3025, training loss: 11085.96, average training loss: 12054.61, base loss: 19674.15
[INFO 2017-06-27 20:44:04,700 main.py:51] epoch 3026, training loss: 10096.66, average training loss: 12052.35, base loss: 19670.22
[INFO 2017-06-27 20:44:05,150 main.py:51] epoch 3027, training loss: 10870.83, average training loss: 12049.38, base loss: 19665.14
[INFO 2017-06-27 20:44:05,566 main.py:51] epoch 3028, training loss: 12565.20, average training loss: 12049.41, base loss: 19666.26
[INFO 2017-06-27 20:44:05,948 main.py:51] epoch 3029, training loss: 12824.62, average training loss: 12050.78, base loss: 19670.12
[INFO 2017-06-27 20:44:06,349 main.py:51] epoch 3030, training loss: 12045.58, average training loss: 12051.49, base loss: 19672.22
[INFO 2017-06-27 20:44:06,731 main.py:51] epoch 3031, training loss: 12151.51, average training loss: 12053.17, base loss: 19676.49
[INFO 2017-06-27 20:44:07,111 main.py:51] epoch 3032, training loss: 10584.29, average training loss: 12051.95, base loss: 19675.23
[INFO 2017-06-27 20:44:07,501 main.py:51] epoch 3033, training loss: 12006.36, average training loss: 12050.80, base loss: 19674.95
[INFO 2017-06-27 20:44:07,879 main.py:51] epoch 3034, training loss: 12463.34, average training loss: 12050.74, base loss: 19675.52
[INFO 2017-06-27 20:44:08,255 main.py:51] epoch 3035, training loss: 9156.09, average training loss: 12047.48, base loss: 19669.91
[INFO 2017-06-27 20:44:08,629 main.py:51] epoch 3036, training loss: 11206.01, average training loss: 12046.90, base loss: 19670.22
[INFO 2017-06-27 20:44:09,002 main.py:51] epoch 3037, training loss: 12245.09, average training loss: 12047.66, base loss: 19672.60
[INFO 2017-06-27 20:44:09,383 main.py:51] epoch 3038, training loss: 12351.68, average training loss: 12048.68, base loss: 19674.86
[INFO 2017-06-27 20:44:09,754 main.py:51] epoch 3039, training loss: 12466.71, average training loss: 12049.88, base loss: 19677.69
[INFO 2017-06-27 20:44:10,123 main.py:51] epoch 3040, training loss: 11782.34, average training loss: 12050.12, base loss: 19679.28
[INFO 2017-06-27 20:44:10,495 main.py:51] epoch 3041, training loss: 13353.64, average training loss: 12048.95, base loss: 19678.77
[INFO 2017-06-27 20:44:10,871 main.py:51] epoch 3042, training loss: 12253.35, average training loss: 12050.73, base loss: 19682.72
[INFO 2017-06-27 20:44:11,247 main.py:51] epoch 3043, training loss: 10876.64, average training loss: 12048.65, base loss: 19680.84
[INFO 2017-06-27 20:44:11,622 main.py:51] epoch 3044, training loss: 12917.38, average training loss: 12048.94, base loss: 19684.66
[INFO 2017-06-27 20:44:12,001 main.py:51] epoch 3045, training loss: 11126.40, average training loss: 12048.09, base loss: 19684.02
[INFO 2017-06-27 20:44:12,448 main.py:51] epoch 3046, training loss: 10678.01, average training loss: 12048.66, base loss: 19686.15
[INFO 2017-06-27 20:44:12,852 main.py:51] epoch 3047, training loss: 10785.29, average training loss: 12047.09, base loss: 19684.99
[INFO 2017-06-27 20:44:13,230 main.py:51] epoch 3048, training loss: 12059.24, average training loss: 12046.40, base loss: 19686.49
[INFO 2017-06-27 20:44:13,636 main.py:51] epoch 3049, training loss: 10539.70, average training loss: 12042.14, base loss: 19679.74
[INFO 2017-06-27 20:44:14,023 main.py:51] epoch 3050, training loss: 12238.51, average training loss: 12042.21, base loss: 19680.73
[INFO 2017-06-27 20:44:14,400 main.py:51] epoch 3051, training loss: 13715.46, average training loss: 12042.65, base loss: 19682.11
[INFO 2017-06-27 20:44:14,774 main.py:51] epoch 3052, training loss: 10776.31, average training loss: 12040.32, base loss: 19678.23
[INFO 2017-06-27 20:44:15,153 main.py:51] epoch 3053, training loss: 11189.89, average training loss: 12040.86, base loss: 19680.36
[INFO 2017-06-27 20:44:15,529 main.py:51] epoch 3054, training loss: 11987.91, average training loss: 12039.51, base loss: 19677.29
[INFO 2017-06-27 20:44:15,918 main.py:51] epoch 3055, training loss: 11633.30, average training loss: 12038.84, base loss: 19678.17
[INFO 2017-06-27 20:44:16,304 main.py:51] epoch 3056, training loss: 13706.02, average training loss: 12039.53, base loss: 19678.53
[INFO 2017-06-27 20:44:16,682 main.py:51] epoch 3057, training loss: 12637.42, average training loss: 12040.84, base loss: 19682.49
[INFO 2017-06-27 20:44:17,125 main.py:51] epoch 3058, training loss: 11540.50, average training loss: 12038.96, base loss: 19680.22
[INFO 2017-06-27 20:44:17,504 main.py:51] epoch 3059, training loss: 10758.24, average training loss: 12038.99, base loss: 19681.75
[INFO 2017-06-27 20:44:17,894 main.py:51] epoch 3060, training loss: 10909.81, average training loss: 12037.47, base loss: 19679.68
[INFO 2017-06-27 20:44:18,284 main.py:51] epoch 3061, training loss: 11827.22, average training loss: 12036.22, base loss: 19677.69
[INFO 2017-06-27 20:44:18,662 main.py:51] epoch 3062, training loss: 12810.50, average training loss: 12038.53, base loss: 19683.76
[INFO 2017-06-27 20:44:19,039 main.py:51] epoch 3063, training loss: 13056.63, average training loss: 12039.65, base loss: 19687.99
[INFO 2017-06-27 20:44:19,414 main.py:51] epoch 3064, training loss: 10765.64, average training loss: 12038.96, base loss: 19688.14
[INFO 2017-06-27 20:44:19,790 main.py:51] epoch 3065, training loss: 12113.56, average training loss: 12038.03, base loss: 19686.84
[INFO 2017-06-27 20:44:20,170 main.py:51] epoch 3066, training loss: 12352.66, average training loss: 12037.47, base loss: 19687.44
[INFO 2017-06-27 20:44:20,545 main.py:51] epoch 3067, training loss: 12290.31, average training loss: 12036.61, base loss: 19686.24
[INFO 2017-06-27 20:44:20,917 main.py:51] epoch 3068, training loss: 10818.75, average training loss: 12035.32, base loss: 19685.50
[INFO 2017-06-27 20:44:21,290 main.py:51] epoch 3069, training loss: 13185.37, average training loss: 12037.15, base loss: 19690.07
[INFO 2017-06-27 20:44:21,682 main.py:51] epoch 3070, training loss: 11537.67, average training loss: 12036.30, base loss: 19690.30
[INFO 2017-06-27 20:44:22,065 main.py:51] epoch 3071, training loss: 11106.98, average training loss: 12037.10, base loss: 19693.01
[INFO 2017-06-27 20:44:22,441 main.py:51] epoch 3072, training loss: 11758.85, average training loss: 12036.67, base loss: 19694.93
[INFO 2017-06-27 20:44:22,817 main.py:51] epoch 3073, training loss: 10944.24, average training loss: 12035.44, base loss: 19693.11
[INFO 2017-06-27 20:44:23,189 main.py:51] epoch 3074, training loss: 10950.38, average training loss: 12034.50, base loss: 19692.41
[INFO 2017-06-27 20:44:23,560 main.py:51] epoch 3075, training loss: 10671.16, average training loss: 12033.05, base loss: 19690.01
[INFO 2017-06-27 20:44:23,931 main.py:51] epoch 3076, training loss: 13874.67, average training loss: 12034.03, base loss: 19693.79
[INFO 2017-06-27 20:44:24,307 main.py:51] epoch 3077, training loss: 13304.10, average training loss: 12034.89, base loss: 19696.91
[INFO 2017-06-27 20:44:24,680 main.py:51] epoch 3078, training loss: 12469.82, average training loss: 12034.68, base loss: 19696.47
[INFO 2017-06-27 20:44:25,055 main.py:51] epoch 3079, training loss: 11031.73, average training loss: 12034.51, base loss: 19698.30
[INFO 2017-06-27 20:44:25,428 main.py:51] epoch 3080, training loss: 10810.35, average training loss: 12033.17, base loss: 19697.55
[INFO 2017-06-27 20:44:25,800 main.py:51] epoch 3081, training loss: 12156.43, average training loss: 12032.95, base loss: 19698.17
[INFO 2017-06-27 20:44:26,172 main.py:51] epoch 3082, training loss: 11143.02, average training loss: 12031.51, base loss: 19698.64
[INFO 2017-06-27 20:44:26,544 main.py:51] epoch 3083, training loss: 11501.34, average training loss: 12030.47, base loss: 19699.11
[INFO 2017-06-27 20:44:26,920 main.py:51] epoch 3084, training loss: 11146.79, average training loss: 12029.93, base loss: 19700.93
[INFO 2017-06-27 20:44:27,293 main.py:51] epoch 3085, training loss: 14519.47, average training loss: 12032.29, base loss: 19704.41
[INFO 2017-06-27 20:44:27,669 main.py:51] epoch 3086, training loss: 12701.61, average training loss: 12032.28, base loss: 19705.67
[INFO 2017-06-27 20:44:28,044 main.py:51] epoch 3087, training loss: 10520.29, average training loss: 12029.67, base loss: 19701.78
[INFO 2017-06-27 20:44:28,417 main.py:51] epoch 3088, training loss: 11962.66, average training loss: 12030.19, base loss: 19704.69
[INFO 2017-06-27 20:44:28,793 main.py:51] epoch 3089, training loss: 11445.21, average training loss: 12029.92, base loss: 19705.55
[INFO 2017-06-27 20:44:29,172 main.py:51] epoch 3090, training loss: 12433.37, average training loss: 12029.23, base loss: 19705.44
[INFO 2017-06-27 20:44:29,547 main.py:51] epoch 3091, training loss: 13689.36, average training loss: 12029.81, base loss: 19708.36
[INFO 2017-06-27 20:44:29,920 main.py:51] epoch 3092, training loss: 12460.33, average training loss: 12030.54, base loss: 19711.31
[INFO 2017-06-27 20:44:30,298 main.py:51] epoch 3093, training loss: 9880.35, average training loss: 12028.62, base loss: 19708.35
[INFO 2017-06-27 20:44:30,675 main.py:51] epoch 3094, training loss: 11363.29, average training loss: 12026.39, base loss: 19704.22
[INFO 2017-06-27 20:44:31,048 main.py:51] epoch 3095, training loss: 12100.36, average training loss: 12024.33, base loss: 19700.42
[INFO 2017-06-27 20:44:31,420 main.py:51] epoch 3096, training loss: 12688.03, average training loss: 12024.58, base loss: 19701.78
[INFO 2017-06-27 20:44:31,796 main.py:51] epoch 3097, training loss: 11275.96, average training loss: 12022.69, base loss: 19699.97
[INFO 2017-06-27 20:44:32,173 main.py:51] epoch 3098, training loss: 12020.73, average training loss: 12023.34, base loss: 19703.35
[INFO 2017-06-27 20:44:32,545 main.py:51] epoch 3099, training loss: 12719.83, average training loss: 12025.69, base loss: 19707.93
[INFO 2017-06-27 20:44:32,545 main.py:53] epoch 3099, testing
[INFO 2017-06-27 20:44:34,156 main.py:105] average testing loss: 11443.79, base loss: 18917.93
[INFO 2017-06-27 20:44:34,156 main.py:106] improve_loss: 7474.15, improve_percent: 0.40
[INFO 2017-06-27 20:44:34,157 main.py:76] current best improved percent: 0.40
[INFO 2017-06-27 20:44:34,530 main.py:51] epoch 3100, training loss: 11714.54, average training loss: 12024.22, base loss: 19706.77
[INFO 2017-06-27 20:44:34,907 main.py:51] epoch 3101, training loss: 10498.58, average training loss: 12023.14, base loss: 19705.80
[INFO 2017-06-27 20:44:35,283 main.py:51] epoch 3102, training loss: 10264.87, average training loss: 12019.88, base loss: 19699.70
[INFO 2017-06-27 20:44:35,655 main.py:51] epoch 3103, training loss: 11777.41, average training loss: 12019.02, base loss: 19698.89
[INFO 2017-06-27 20:44:36,031 main.py:51] epoch 3104, training loss: 11263.38, average training loss: 12017.20, base loss: 19696.97
[INFO 2017-06-27 20:44:36,403 main.py:51] epoch 3105, training loss: 12042.18, average training loss: 12018.00, base loss: 19698.61
[INFO 2017-06-27 20:44:36,776 main.py:51] epoch 3106, training loss: 10732.12, average training loss: 12015.50, base loss: 19693.70
[INFO 2017-06-27 20:44:37,188 main.py:51] epoch 3107, training loss: 10936.91, average training loss: 12015.02, base loss: 19695.38
[INFO 2017-06-27 20:44:37,594 main.py:51] epoch 3108, training loss: 11854.33, average training loss: 12015.90, base loss: 19697.88
[INFO 2017-06-27 20:44:37,977 main.py:51] epoch 3109, training loss: 12255.70, average training loss: 12016.85, base loss: 19700.10
[INFO 2017-06-27 20:44:38,361 main.py:51] epoch 3110, training loss: 11972.60, average training loss: 12017.52, base loss: 19700.81
[INFO 2017-06-27 20:44:38,737 main.py:51] epoch 3111, training loss: 11021.02, average training loss: 12017.54, base loss: 19702.66
[INFO 2017-06-27 20:44:39,111 main.py:51] epoch 3112, training loss: 10603.12, average training loss: 12016.91, base loss: 19701.66
[INFO 2017-06-27 20:44:39,533 main.py:51] epoch 3113, training loss: 12718.60, average training loss: 12019.60, base loss: 19707.46
[INFO 2017-06-27 20:44:39,954 main.py:51] epoch 3114, training loss: 9865.59, average training loss: 12017.64, base loss: 19703.49
[INFO 2017-06-27 20:44:40,346 main.py:51] epoch 3115, training loss: 11380.62, average training loss: 12017.47, base loss: 19704.75
[INFO 2017-06-27 20:44:40,728 main.py:51] epoch 3116, training loss: 10370.42, average training loss: 12014.40, base loss: 19699.42
[INFO 2017-06-27 20:44:41,112 main.py:51] epoch 3117, training loss: 11055.76, average training loss: 12011.85, base loss: 19695.09
[INFO 2017-06-27 20:44:41,522 main.py:51] epoch 3118, training loss: 12506.37, average training loss: 12011.16, base loss: 19694.31
[INFO 2017-06-27 20:44:41,898 main.py:51] epoch 3119, training loss: 11396.07, average training loss: 12010.18, base loss: 19692.29
[INFO 2017-06-27 20:44:42,281 main.py:51] epoch 3120, training loss: 13514.48, average training loss: 12010.29, base loss: 19693.39
[INFO 2017-06-27 20:44:42,656 main.py:51] epoch 3121, training loss: 13558.96, average training loss: 12010.46, base loss: 19694.20
[INFO 2017-06-27 20:44:43,029 main.py:51] epoch 3122, training loss: 10901.22, average training loss: 12009.34, base loss: 19692.54
[INFO 2017-06-27 20:44:43,404 main.py:51] epoch 3123, training loss: 10761.64, average training loss: 12005.97, base loss: 19686.94
[INFO 2017-06-27 20:44:43,804 main.py:51] epoch 3124, training loss: 12828.74, average training loss: 12006.55, base loss: 19688.83
[INFO 2017-06-27 20:44:44,181 main.py:51] epoch 3125, training loss: 12825.97, average training loss: 12007.70, base loss: 19693.89
[INFO 2017-06-27 20:44:44,551 main.py:51] epoch 3126, training loss: 12706.06, average training loss: 12009.66, base loss: 19699.46
[INFO 2017-06-27 20:44:44,929 main.py:51] epoch 3127, training loss: 10476.99, average training loss: 12010.28, base loss: 19700.50
[INFO 2017-06-27 20:44:45,306 main.py:51] epoch 3128, training loss: 12411.65, average training loss: 12010.47, base loss: 19701.63
[INFO 2017-06-27 20:44:45,678 main.py:51] epoch 3129, training loss: 9890.91, average training loss: 12006.81, base loss: 19695.20
[INFO 2017-06-27 20:44:46,056 main.py:51] epoch 3130, training loss: 11345.79, average training loss: 12006.38, base loss: 19694.63
[INFO 2017-06-27 20:44:46,433 main.py:51] epoch 3131, training loss: 11819.35, average training loss: 12006.47, base loss: 19695.83
[INFO 2017-06-27 20:44:46,812 main.py:51] epoch 3132, training loss: 11698.40, average training loss: 12004.37, base loss: 19691.89
[INFO 2017-06-27 20:44:47,188 main.py:51] epoch 3133, training loss: 12591.53, average training loss: 12005.30, base loss: 19694.78
[INFO 2017-06-27 20:44:47,563 main.py:51] epoch 3134, training loss: 11003.17, average training loss: 12004.71, base loss: 19694.89
[INFO 2017-06-27 20:44:47,941 main.py:51] epoch 3135, training loss: 10794.05, average training loss: 12003.82, base loss: 19693.84
[INFO 2017-06-27 20:44:48,317 main.py:51] epoch 3136, training loss: 11596.89, average training loss: 12003.12, base loss: 19692.58
[INFO 2017-06-27 20:44:48,688 main.py:51] epoch 3137, training loss: 12875.31, average training loss: 12004.58, base loss: 19697.25
[INFO 2017-06-27 20:44:49,065 main.py:51] epoch 3138, training loss: 11045.80, average training loss: 12005.67, base loss: 19701.93
[INFO 2017-06-27 20:44:49,442 main.py:51] epoch 3139, training loss: 11945.07, average training loss: 12004.83, base loss: 19701.04
[INFO 2017-06-27 20:44:49,816 main.py:51] epoch 3140, training loss: 13582.05, average training loss: 12007.40, base loss: 19706.74
[INFO 2017-06-27 20:44:50,193 main.py:51] epoch 3141, training loss: 10433.70, average training loss: 12005.56, base loss: 19703.76
[INFO 2017-06-27 20:44:50,570 main.py:51] epoch 3142, training loss: 11521.98, average training loss: 12004.70, base loss: 19703.66
[INFO 2017-06-27 20:44:50,946 main.py:51] epoch 3143, training loss: 12863.42, average training loss: 12005.03, base loss: 19704.67
[INFO 2017-06-27 20:44:51,321 main.py:51] epoch 3144, training loss: 10635.31, average training loss: 12004.87, base loss: 19706.03
[INFO 2017-06-27 20:44:51,698 main.py:51] epoch 3145, training loss: 10983.68, average training loss: 12003.90, base loss: 19703.98
[INFO 2017-06-27 20:44:52,074 main.py:51] epoch 3146, training loss: 12376.30, average training loss: 12005.76, base loss: 19708.57
[INFO 2017-06-27 20:44:52,450 main.py:51] epoch 3147, training loss: 10440.57, average training loss: 12003.98, base loss: 19705.80
[INFO 2017-06-27 20:44:52,826 main.py:51] epoch 3148, training loss: 11914.18, average training loss: 12003.65, base loss: 19705.97
[INFO 2017-06-27 20:44:53,294 main.py:51] epoch 3149, training loss: 11615.24, average training loss: 12002.69, base loss: 19705.17
[INFO 2017-06-27 20:44:53,683 main.py:51] epoch 3150, training loss: 12618.88, average training loss: 12003.18, base loss: 19706.52
[INFO 2017-06-27 20:44:54,067 main.py:51] epoch 3151, training loss: 12158.96, average training loss: 12003.09, base loss: 19708.14
[INFO 2017-06-27 20:44:54,446 main.py:51] epoch 3152, training loss: 11649.66, average training loss: 12003.63, base loss: 19711.27
[INFO 2017-06-27 20:44:54,824 main.py:51] epoch 3153, training loss: 11172.73, average training loss: 12001.98, base loss: 19709.51
[INFO 2017-06-27 20:44:55,204 main.py:51] epoch 3154, training loss: 11125.68, average training loss: 12001.28, base loss: 19709.57
[INFO 2017-06-27 20:44:55,583 main.py:51] epoch 3155, training loss: 11837.18, average training loss: 12000.51, base loss: 19709.24
[INFO 2017-06-27 20:44:55,959 main.py:51] epoch 3156, training loss: 11920.75, average training loss: 11999.70, base loss: 19709.22
[INFO 2017-06-27 20:44:56,374 main.py:51] epoch 3157, training loss: 12446.09, average training loss: 12001.62, base loss: 19714.15
[INFO 2017-06-27 20:44:56,791 main.py:51] epoch 3158, training loss: 10613.55, average training loss: 11999.96, base loss: 19712.38
[INFO 2017-06-27 20:44:57,194 main.py:51] epoch 3159, training loss: 13455.47, average training loss: 12000.70, base loss: 19713.48
[INFO 2017-06-27 20:44:57,582 main.py:51] epoch 3160, training loss: 12551.19, average training loss: 12000.74, base loss: 19714.37
[INFO 2017-06-27 20:44:57,963 main.py:51] epoch 3161, training loss: 12091.63, average training loss: 12000.03, base loss: 19713.83
[INFO 2017-06-27 20:44:58,337 main.py:51] epoch 3162, training loss: 12026.91, average training loss: 12000.62, base loss: 19715.66
[INFO 2017-06-27 20:44:58,714 main.py:51] epoch 3163, training loss: 11340.04, average training loss: 12000.69, base loss: 19718.55
[INFO 2017-06-27 20:44:59,092 main.py:51] epoch 3164, training loss: 10198.36, average training loss: 12000.35, base loss: 19718.14
[INFO 2017-06-27 20:44:59,485 main.py:51] epoch 3165, training loss: 12251.46, average training loss: 12000.40, base loss: 19719.64
[INFO 2017-06-27 20:44:59,861 main.py:51] epoch 3166, training loss: 12792.01, average training loss: 12001.32, base loss: 19722.35
[INFO 2017-06-27 20:45:00,241 main.py:51] epoch 3167, training loss: 12242.78, average training loss: 12001.58, base loss: 19723.85
[INFO 2017-06-27 20:45:00,616 main.py:51] epoch 3168, training loss: 12158.24, average training loss: 12001.13, base loss: 19723.61
[INFO 2017-06-27 20:45:00,992 main.py:51] epoch 3169, training loss: 12883.33, average training loss: 12001.92, base loss: 19726.83
[INFO 2017-06-27 20:45:01,368 main.py:51] epoch 3170, training loss: 10918.36, average training loss: 12000.32, base loss: 19724.21
[INFO 2017-06-27 20:45:01,741 main.py:51] epoch 3171, training loss: 11594.84, average training loss: 11999.92, base loss: 19724.06
[INFO 2017-06-27 20:45:02,120 main.py:51] epoch 3172, training loss: 10797.29, average training loss: 11999.34, base loss: 19724.21
[INFO 2017-06-27 20:45:02,496 main.py:51] epoch 3173, training loss: 10588.54, average training loss: 11998.25, base loss: 19723.45
[INFO 2017-06-27 20:45:02,870 main.py:51] epoch 3174, training loss: 11537.76, average training loss: 11998.14, base loss: 19723.38
[INFO 2017-06-27 20:45:03,261 main.py:51] epoch 3175, training loss: 11653.23, average training loss: 11996.81, base loss: 19723.25
[INFO 2017-06-27 20:45:03,634 main.py:51] epoch 3176, training loss: 11100.20, average training loss: 11994.80, base loss: 19719.35
[INFO 2017-06-27 20:45:04,005 main.py:51] epoch 3177, training loss: 13582.38, average training loss: 11997.15, base loss: 19725.08
[INFO 2017-06-27 20:45:04,381 main.py:51] epoch 3178, training loss: 11412.23, average training loss: 11996.23, base loss: 19723.48
[INFO 2017-06-27 20:45:04,750 main.py:51] epoch 3179, training loss: 11200.35, average training loss: 11994.29, base loss: 19720.92
[INFO 2017-06-27 20:45:05,126 main.py:51] epoch 3180, training loss: 12317.97, average training loss: 11995.00, base loss: 19722.43
[INFO 2017-06-27 20:45:05,501 main.py:51] epoch 3181, training loss: 10918.45, average training loss: 11992.43, base loss: 19719.33
[INFO 2017-06-27 20:45:05,872 main.py:51] epoch 3182, training loss: 12483.38, average training loss: 11993.98, base loss: 19722.81
[INFO 2017-06-27 20:45:06,248 main.py:51] epoch 3183, training loss: 12055.27, average training loss: 11996.17, base loss: 19728.67
[INFO 2017-06-27 20:45:06,624 main.py:51] epoch 3184, training loss: 10160.00, average training loss: 11994.45, base loss: 19724.91
[INFO 2017-06-27 20:45:07,002 main.py:51] epoch 3185, training loss: 10182.81, average training loss: 11994.08, base loss: 19725.65
[INFO 2017-06-27 20:45:07,382 main.py:51] epoch 3186, training loss: 12358.72, average training loss: 11991.57, base loss: 19723.15
[INFO 2017-06-27 20:45:07,759 main.py:51] epoch 3187, training loss: 12707.12, average training loss: 11991.38, base loss: 19724.02
[INFO 2017-06-27 20:45:08,133 main.py:51] epoch 3188, training loss: 13132.50, average training loss: 11991.41, base loss: 19725.94
[INFO 2017-06-27 20:45:08,513 main.py:51] epoch 3189, training loss: 10898.76, average training loss: 11991.74, base loss: 19728.49
[INFO 2017-06-27 20:45:08,891 main.py:51] epoch 3190, training loss: 9881.83, average training loss: 11989.59, base loss: 19725.23
[INFO 2017-06-27 20:45:09,267 main.py:51] epoch 3191, training loss: 11608.48, average training loss: 11987.68, base loss: 19721.52
[INFO 2017-06-27 20:45:09,638 main.py:51] epoch 3192, training loss: 11669.19, average training loss: 11985.95, base loss: 19717.62
[INFO 2017-06-27 20:45:10,010 main.py:51] epoch 3193, training loss: 13214.69, average training loss: 11986.49, base loss: 19720.82
[INFO 2017-06-27 20:45:10,385 main.py:51] epoch 3194, training loss: 11012.85, average training loss: 11986.19, base loss: 19723.00
[INFO 2017-06-27 20:45:10,756 main.py:51] epoch 3195, training loss: 12500.98, average training loss: 11986.85, base loss: 19724.73
[INFO 2017-06-27 20:45:11,131 main.py:51] epoch 3196, training loss: 10408.32, average training loss: 11985.19, base loss: 19721.06
[INFO 2017-06-27 20:45:11,511 main.py:51] epoch 3197, training loss: 10604.26, average training loss: 11984.77, base loss: 19720.52
[INFO 2017-06-27 20:45:11,903 main.py:51] epoch 3198, training loss: 11820.47, average training loss: 11983.20, base loss: 19720.82
[INFO 2017-06-27 20:45:12,276 main.py:51] epoch 3199, training loss: 11555.79, average training loss: 11982.43, base loss: 19720.85
[INFO 2017-06-27 20:45:12,276 main.py:53] epoch 3199, testing
[INFO 2017-06-27 20:45:13,872 main.py:105] average testing loss: 11777.12, base loss: 19803.77
[INFO 2017-06-27 20:45:13,872 main.py:106] improve_loss: 8026.65, improve_percent: 0.41
[INFO 2017-06-27 20:45:13,873 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:45:13,885 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 20:45:14,261 main.py:51] epoch 3200, training loss: 12311.47, average training loss: 11982.57, base loss: 19723.14
[INFO 2017-06-27 20:45:14,639 main.py:51] epoch 3201, training loss: 11880.49, average training loss: 11982.68, base loss: 19723.70
[INFO 2017-06-27 20:45:15,017 main.py:51] epoch 3202, training loss: 10988.43, average training loss: 11980.04, base loss: 19719.86
[INFO 2017-06-27 20:45:15,395 main.py:51] epoch 3203, training loss: 12248.42, average training loss: 11979.64, base loss: 19720.42
[INFO 2017-06-27 20:45:15,770 main.py:51] epoch 3204, training loss: 11209.22, average training loss: 11978.96, base loss: 19719.74
[INFO 2017-06-27 20:45:16,146 main.py:51] epoch 3205, training loss: 12211.77, average training loss: 11980.42, base loss: 19724.58
[INFO 2017-06-27 20:45:16,518 main.py:51] epoch 3206, training loss: 10867.92, average training loss: 11979.63, base loss: 19723.01
[INFO 2017-06-27 20:45:16,892 main.py:51] epoch 3207, training loss: 11729.99, average training loss: 11979.61, base loss: 19724.47
[INFO 2017-06-27 20:45:17,268 main.py:51] epoch 3208, training loss: 10623.69, average training loss: 11977.53, base loss: 19721.67
[INFO 2017-06-27 20:45:17,640 main.py:51] epoch 3209, training loss: 12754.57, average training loss: 11975.72, base loss: 19719.17
[INFO 2017-06-27 20:45:18,018 main.py:51] epoch 3210, training loss: 10633.08, average training loss: 11975.00, base loss: 19718.06
[INFO 2017-06-27 20:45:18,401 main.py:51] epoch 3211, training loss: 10765.60, average training loss: 11971.87, base loss: 19712.75
[INFO 2017-06-27 20:45:18,784 main.py:51] epoch 3212, training loss: 10921.01, average training loss: 11971.19, base loss: 19713.06
[INFO 2017-06-27 20:45:19,162 main.py:51] epoch 3213, training loss: 12789.97, average training loss: 11969.62, base loss: 19710.36
[INFO 2017-06-27 20:45:19,536 main.py:51] epoch 3214, training loss: 11392.05, average training loss: 11969.15, base loss: 19709.10
[INFO 2017-06-27 20:45:19,921 main.py:51] epoch 3215, training loss: 12891.99, average training loss: 11970.04, base loss: 19712.49
[INFO 2017-06-27 20:45:20,309 main.py:51] epoch 3216, training loss: 11055.12, average training loss: 11968.87, base loss: 19711.09
[INFO 2017-06-27 20:45:20,707 main.py:51] epoch 3217, training loss: 12375.30, average training loss: 11968.67, base loss: 19713.23
[INFO 2017-06-27 20:45:21,087 main.py:51] epoch 3218, training loss: 12237.28, average training loss: 11968.07, base loss: 19712.06
[INFO 2017-06-27 20:45:21,477 main.py:51] epoch 3219, training loss: 11633.99, average training loss: 11968.07, base loss: 19713.50
[INFO 2017-06-27 20:45:21,859 main.py:51] epoch 3220, training loss: 12549.84, average training loss: 11968.71, base loss: 19715.56
[INFO 2017-06-27 20:45:22,246 main.py:51] epoch 3221, training loss: 10835.77, average training loss: 11968.88, base loss: 19716.64
[INFO 2017-06-27 20:45:22,634 main.py:51] epoch 3222, training loss: 10068.00, average training loss: 11967.47, base loss: 19713.01
[INFO 2017-06-27 20:45:23,036 main.py:51] epoch 3223, training loss: 11159.72, average training loss: 11965.54, base loss: 19709.63
[INFO 2017-06-27 20:45:23,521 main.py:51] epoch 3224, training loss: 11171.60, average training loss: 11964.34, base loss: 19708.12
[INFO 2017-06-27 20:45:23,955 main.py:51] epoch 3225, training loss: 10972.87, average training loss: 11962.92, base loss: 19706.89
[INFO 2017-06-27 20:45:24,413 main.py:51] epoch 3226, training loss: 12090.29, average training loss: 11963.86, base loss: 19708.28
[INFO 2017-06-27 20:45:24,861 main.py:51] epoch 3227, training loss: 13722.32, average training loss: 11964.89, base loss: 19710.47
[INFO 2017-06-27 20:45:25,270 main.py:51] epoch 3228, training loss: 9733.12, average training loss: 11961.68, base loss: 19705.26
[INFO 2017-06-27 20:45:25,680 main.py:51] epoch 3229, training loss: 12789.13, average training loss: 11962.57, base loss: 19707.82
[INFO 2017-06-27 20:45:26,149 main.py:51] epoch 3230, training loss: 10807.50, average training loss: 11959.79, base loss: 19702.36
[INFO 2017-06-27 20:45:26,569 main.py:51] epoch 3231, training loss: 10888.22, average training loss: 11959.05, base loss: 19701.53
[INFO 2017-06-27 20:45:26,948 main.py:51] epoch 3232, training loss: 12561.74, average training loss: 11959.37, base loss: 19703.30
[INFO 2017-06-27 20:45:27,325 main.py:51] epoch 3233, training loss: 12019.05, average training loss: 11960.19, base loss: 19704.60
[INFO 2017-06-27 20:45:27,702 main.py:51] epoch 3234, training loss: 10377.76, average training loss: 11958.25, base loss: 19703.27
[INFO 2017-06-27 20:45:28,078 main.py:51] epoch 3235, training loss: 11400.70, average training loss: 11957.74, base loss: 19703.56
[INFO 2017-06-27 20:45:28,486 main.py:51] epoch 3236, training loss: 11424.10, average training loss: 11957.07, base loss: 19703.42
[INFO 2017-06-27 20:45:28,866 main.py:51] epoch 3237, training loss: 11100.11, average training loss: 11956.21, base loss: 19701.16
[INFO 2017-06-27 20:45:29,245 main.py:51] epoch 3238, training loss: 12734.03, average training loss: 11954.28, base loss: 19699.25
[INFO 2017-06-27 20:45:29,618 main.py:51] epoch 3239, training loss: 12498.36, average training loss: 11954.68, base loss: 19701.89
[INFO 2017-06-27 20:45:29,989 main.py:51] epoch 3240, training loss: 12087.34, average training loss: 11955.13, base loss: 19704.29
[INFO 2017-06-27 20:45:30,367 main.py:51] epoch 3241, training loss: 11234.98, average training loss: 11953.67, base loss: 19702.47
[INFO 2017-06-27 20:45:30,742 main.py:51] epoch 3242, training loss: 9903.26, average training loss: 11952.60, base loss: 19701.08
[INFO 2017-06-27 20:45:31,119 main.py:51] epoch 3243, training loss: 10758.88, average training loss: 11952.91, base loss: 19702.37
[INFO 2017-06-27 20:45:31,492 main.py:51] epoch 3244, training loss: 11285.84, average training loss: 11953.99, base loss: 19704.96
[INFO 2017-06-27 20:45:31,868 main.py:51] epoch 3245, training loss: 11916.82, average training loss: 11953.06, base loss: 19705.27
[INFO 2017-06-27 20:45:32,242 main.py:51] epoch 3246, training loss: 12439.92, average training loss: 11953.39, base loss: 19705.53
[INFO 2017-06-27 20:45:32,617 main.py:51] epoch 3247, training loss: 11565.01, average training loss: 11951.86, base loss: 19704.79
[INFO 2017-06-27 20:45:32,990 main.py:51] epoch 3248, training loss: 12114.48, average training loss: 11952.91, base loss: 19707.98
[INFO 2017-06-27 20:45:33,365 main.py:51] epoch 3249, training loss: 12346.69, average training loss: 11953.65, base loss: 19711.11
[INFO 2017-06-27 20:45:33,736 main.py:51] epoch 3250, training loss: 10718.70, average training loss: 11953.35, base loss: 19712.41
[INFO 2017-06-27 20:45:34,109 main.py:51] epoch 3251, training loss: 12004.66, average training loss: 11953.81, base loss: 19714.74
[INFO 2017-06-27 20:45:34,489 main.py:51] epoch 3252, training loss: 11183.12, average training loss: 11953.24, base loss: 19715.19
[INFO 2017-06-27 20:45:34,869 main.py:51] epoch 3253, training loss: 11553.05, average training loss: 11951.72, base loss: 19713.35
[INFO 2017-06-27 20:45:35,245 main.py:51] epoch 3254, training loss: 11026.85, average training loss: 11950.71, base loss: 19711.69
[INFO 2017-06-27 20:45:35,622 main.py:51] epoch 3255, training loss: 11150.79, average training loss: 11948.16, base loss: 19707.23
[INFO 2017-06-27 20:45:36,003 main.py:51] epoch 3256, training loss: 12018.92, average training loss: 11947.76, base loss: 19707.95
[INFO 2017-06-27 20:45:36,380 main.py:51] epoch 3257, training loss: 11999.77, average training loss: 11946.99, base loss: 19707.49
[INFO 2017-06-27 20:45:36,757 main.py:51] epoch 3258, training loss: 11523.86, average training loss: 11947.64, base loss: 19709.72
[INFO 2017-06-27 20:45:37,130 main.py:51] epoch 3259, training loss: 11277.08, average training loss: 11944.95, base loss: 19704.72
[INFO 2017-06-27 20:45:37,508 main.py:51] epoch 3260, training loss: 12000.85, average training loss: 11943.58, base loss: 19703.25
[INFO 2017-06-27 20:45:37,880 main.py:51] epoch 3261, training loss: 11058.74, average training loss: 11942.33, base loss: 19702.90
[INFO 2017-06-27 20:45:38,254 main.py:51] epoch 3262, training loss: 11673.47, average training loss: 11942.00, base loss: 19702.61
[INFO 2017-06-27 20:45:38,627 main.py:51] epoch 3263, training loss: 13489.72, average training loss: 11942.95, base loss: 19705.85
[INFO 2017-06-27 20:45:39,003 main.py:51] epoch 3264, training loss: 11820.33, average training loss: 11943.04, base loss: 19706.22
[INFO 2017-06-27 20:45:39,379 main.py:51] epoch 3265, training loss: 11190.31, average training loss: 11942.74, base loss: 19707.01
[INFO 2017-06-27 20:45:39,755 main.py:51] epoch 3266, training loss: 11330.24, average training loss: 11940.81, base loss: 19703.99
[INFO 2017-06-27 20:45:40,133 main.py:51] epoch 3267, training loss: 11363.65, average training loss: 11940.50, base loss: 19703.90
[INFO 2017-06-27 20:45:40,517 main.py:51] epoch 3268, training loss: 11807.23, average training loss: 11940.95, base loss: 19706.37
[INFO 2017-06-27 20:45:40,895 main.py:51] epoch 3269, training loss: 11211.42, average training loss: 11939.05, base loss: 19703.74
[INFO 2017-06-27 20:45:41,272 main.py:51] epoch 3270, training loss: 12944.34, average training loss: 11940.35, base loss: 19707.38
[INFO 2017-06-27 20:45:41,706 main.py:51] epoch 3271, training loss: 11746.96, average training loss: 11940.63, base loss: 19709.07
[INFO 2017-06-27 20:45:42,085 main.py:51] epoch 3272, training loss: 11573.45, average training loss: 11937.83, base loss: 19704.47
[INFO 2017-06-27 20:45:42,473 main.py:51] epoch 3273, training loss: 12180.17, average training loss: 11937.40, base loss: 19705.71
[INFO 2017-06-27 20:45:42,925 main.py:51] epoch 3274, training loss: 11011.74, average training loss: 11936.05, base loss: 19703.76
[INFO 2017-06-27 20:45:43,363 main.py:51] epoch 3275, training loss: 13018.47, average training loss: 11938.12, base loss: 19709.69
[INFO 2017-06-27 20:45:43,741 main.py:51] epoch 3276, training loss: 11999.67, average training loss: 11939.05, base loss: 19713.03
[INFO 2017-06-27 20:45:44,144 main.py:51] epoch 3277, training loss: 11692.62, average training loss: 11938.45, base loss: 19712.38
[INFO 2017-06-27 20:45:44,523 main.py:51] epoch 3278, training loss: 12057.40, average training loss: 11939.40, base loss: 19715.10
[INFO 2017-06-27 20:45:44,901 main.py:51] epoch 3279, training loss: 11569.35, average training loss: 11937.04, base loss: 19712.81
[INFO 2017-06-27 20:45:45,292 main.py:51] epoch 3280, training loss: 10744.83, average training loss: 11936.69, base loss: 19713.41
[INFO 2017-06-27 20:45:45,667 main.py:51] epoch 3281, training loss: 11219.44, average training loss: 11936.36, base loss: 19712.07
[INFO 2017-06-27 20:45:46,043 main.py:51] epoch 3282, training loss: 12046.65, average training loss: 11936.80, base loss: 19714.82
[INFO 2017-06-27 20:45:46,422 main.py:51] epoch 3283, training loss: 10473.13, average training loss: 11934.93, base loss: 19711.56
[INFO 2017-06-27 20:45:46,798 main.py:51] epoch 3284, training loss: 12290.09, average training loss: 11935.25, base loss: 19713.06
[INFO 2017-06-27 20:45:47,175 main.py:51] epoch 3285, training loss: 10281.51, average training loss: 11933.39, base loss: 19709.64
[INFO 2017-06-27 20:45:47,551 main.py:51] epoch 3286, training loss: 12822.65, average training loss: 11935.53, base loss: 19714.91
[INFO 2017-06-27 20:45:47,936 main.py:51] epoch 3287, training loss: 11597.63, average training loss: 11933.70, base loss: 19713.20
[INFO 2017-06-27 20:45:48,313 main.py:51] epoch 3288, training loss: 11655.13, average training loss: 11931.81, base loss: 19710.48
[INFO 2017-06-27 20:45:48,712 main.py:51] epoch 3289, training loss: 10354.68, average training loss: 11930.67, base loss: 19709.52
[INFO 2017-06-27 20:45:49,109 main.py:51] epoch 3290, training loss: 11538.72, average training loss: 11931.74, base loss: 19711.92
[INFO 2017-06-27 20:45:49,493 main.py:51] epoch 3291, training loss: 12477.43, average training loss: 11931.18, base loss: 19710.07
[INFO 2017-06-27 20:45:49,878 main.py:51] epoch 3292, training loss: 12793.40, average training loss: 11933.33, base loss: 19713.99
[INFO 2017-06-27 20:45:50,290 main.py:51] epoch 3293, training loss: 10977.04, average training loss: 11933.82, base loss: 19717.45
[INFO 2017-06-27 20:45:50,670 main.py:51] epoch 3294, training loss: 11905.15, average training loss: 11933.96, base loss: 19719.02
[INFO 2017-06-27 20:45:51,050 main.py:51] epoch 3295, training loss: 12333.27, average training loss: 11933.66, base loss: 19719.42
[INFO 2017-06-27 20:45:51,432 main.py:51] epoch 3296, training loss: 11513.48, average training loss: 11933.28, base loss: 19719.90
[INFO 2017-06-27 20:45:51,805 main.py:51] epoch 3297, training loss: 10600.04, average training loss: 11931.65, base loss: 19717.77
[INFO 2017-06-27 20:45:52,182 main.py:51] epoch 3298, training loss: 12242.84, average training loss: 11932.64, base loss: 19719.18
[INFO 2017-06-27 20:45:52,560 main.py:51] epoch 3299, training loss: 11656.19, average training loss: 11933.63, base loss: 19723.22
[INFO 2017-06-27 20:45:52,561 main.py:53] epoch 3299, testing
[INFO 2017-06-27 20:45:54,184 main.py:105] average testing loss: 11778.20, base loss: 19475.56
[INFO 2017-06-27 20:45:54,184 main.py:106] improve_loss: 7697.36, improve_percent: 0.40
[INFO 2017-06-27 20:45:54,185 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 20:45:54,567 main.py:51] epoch 3300, training loss: 12377.04, average training loss: 11932.91, base loss: 19722.46
[INFO 2017-06-27 20:45:54,943 main.py:51] epoch 3301, training loss: 10191.40, average training loss: 11932.63, base loss: 19722.27
[INFO 2017-06-27 20:45:55,317 main.py:51] epoch 3302, training loss: 11645.74, average training loss: 11931.60, base loss: 19721.24
[INFO 2017-06-27 20:45:55,696 main.py:51] epoch 3303, training loss: 12203.35, average training loss: 11932.12, base loss: 19724.15
[INFO 2017-06-27 20:45:56,080 main.py:51] epoch 3304, training loss: 11629.05, average training loss: 11929.42, base loss: 19718.67
[INFO 2017-06-27 20:45:56,456 main.py:51] epoch 3305, training loss: 11801.69, average training loss: 11929.42, base loss: 19720.47
[INFO 2017-06-27 20:45:56,911 main.py:51] epoch 3306, training loss: 10367.12, average training loss: 11926.72, base loss: 19717.31
[INFO 2017-06-27 20:45:57,315 main.py:51] epoch 3307, training loss: 12196.73, average training loss: 11925.95, base loss: 19717.73
[INFO 2017-06-27 20:45:57,702 main.py:51] epoch 3308, training loss: 11811.50, average training loss: 11926.79, base loss: 19721.69
[INFO 2017-06-27 20:45:58,126 main.py:51] epoch 3309, training loss: 11123.70, average training loss: 11925.04, base loss: 19717.69
[INFO 2017-06-27 20:45:58,601 main.py:51] epoch 3310, training loss: 12897.24, average training loss: 11925.47, base loss: 19719.55
[INFO 2017-06-27 20:45:58,982 main.py:51] epoch 3311, training loss: 11829.28, average training loss: 11926.69, base loss: 19723.13
[INFO 2017-06-27 20:45:59,360 main.py:51] epoch 3312, training loss: 11448.50, average training loss: 11924.71, base loss: 19721.70
[INFO 2017-06-27 20:45:59,753 main.py:51] epoch 3313, training loss: 11154.90, average training loss: 11924.61, base loss: 19723.32
[INFO 2017-06-27 20:46:00,129 main.py:51] epoch 3314, training loss: 13021.94, average training loss: 11925.75, base loss: 19726.45
[INFO 2017-06-27 20:46:00,505 main.py:51] epoch 3315, training loss: 11508.77, average training loss: 11924.76, base loss: 19726.45
[INFO 2017-06-27 20:46:00,881 main.py:51] epoch 3316, training loss: 10776.23, average training loss: 11923.90, base loss: 19725.87
[INFO 2017-06-27 20:46:01,256 main.py:51] epoch 3317, training loss: 11771.58, average training loss: 11923.55, base loss: 19725.91
[INFO 2017-06-27 20:46:01,627 main.py:51] epoch 3318, training loss: 11077.17, average training loss: 11920.63, base loss: 19720.79
[INFO 2017-06-27 20:46:02,001 main.py:51] epoch 3319, training loss: 12366.17, average training loss: 11920.93, base loss: 19721.57
[INFO 2017-06-27 20:46:02,376 main.py:51] epoch 3320, training loss: 12154.78, average training loss: 11921.53, base loss: 19724.49
[INFO 2017-06-27 20:46:02,750 main.py:51] epoch 3321, training loss: 12370.16, average training loss: 11923.81, base loss: 19730.85
[INFO 2017-06-27 20:46:03,126 main.py:51] epoch 3322, training loss: 12120.37, average training loss: 11922.44, base loss: 19729.92
[INFO 2017-06-27 20:46:03,502 main.py:51] epoch 3323, training loss: 11841.29, average training loss: 11921.89, base loss: 19729.12
[INFO 2017-06-27 20:46:03,878 main.py:51] epoch 3324, training loss: 12192.78, average training loss: 11921.62, base loss: 19727.82
[INFO 2017-06-27 20:46:04,339 main.py:51] epoch 3325, training loss: 11284.07, average training loss: 11920.86, base loss: 19728.08
[INFO 2017-06-27 20:46:04,760 main.py:51] epoch 3326, training loss: 13105.47, average training loss: 11923.63, base loss: 19733.76
[INFO 2017-06-27 20:46:05,145 main.py:51] epoch 3327, training loss: 12417.86, average training loss: 11923.64, base loss: 19733.27
[INFO 2017-06-27 20:46:05,525 main.py:51] epoch 3328, training loss: 13149.58, average training loss: 11923.80, base loss: 19735.01
[INFO 2017-06-27 20:46:05,998 main.py:51] epoch 3329, training loss: 12364.62, average training loss: 11922.67, base loss: 19733.03
[INFO 2017-06-27 20:46:06,402 main.py:51] epoch 3330, training loss: 11968.49, average training loss: 11923.19, base loss: 19734.39
[INFO 2017-06-27 20:46:06,852 main.py:51] epoch 3331, training loss: 12931.67, average training loss: 11923.36, base loss: 19735.40
[INFO 2017-06-27 20:46:07,257 main.py:51] epoch 3332, training loss: 11428.50, average training loss: 11922.74, base loss: 19735.09
[INFO 2017-06-27 20:46:07,726 main.py:51] epoch 3333, training loss: 11446.91, average training loss: 11923.31, base loss: 19736.24
[INFO 2017-06-27 20:46:08,178 main.py:51] epoch 3334, training loss: 12561.77, average training loss: 11923.81, base loss: 19737.85
[INFO 2017-06-27 20:46:08,564 main.py:51] epoch 3335, training loss: 12265.84, average training loss: 11925.11, base loss: 19741.70
[INFO 2017-06-27 20:46:08,944 main.py:51] epoch 3336, training loss: 10700.33, average training loss: 11922.77, base loss: 19737.29
[INFO 2017-06-27 20:46:09,321 main.py:51] epoch 3337, training loss: 11500.46, average training loss: 11921.94, base loss: 19736.73
[INFO 2017-06-27 20:46:09,793 main.py:51] epoch 3338, training loss: 12724.12, average training loss: 11923.20, base loss: 19739.63
[INFO 2017-06-27 20:46:10,179 main.py:51] epoch 3339, training loss: 11185.40, average training loss: 11922.69, base loss: 19739.59
[INFO 2017-06-27 20:46:10,628 main.py:51] epoch 3340, training loss: 11927.68, average training loss: 11924.25, base loss: 19743.33
[INFO 2017-06-27 20:46:11,045 main.py:51] epoch 3341, training loss: 11375.82, average training loss: 11923.68, base loss: 19743.13
[INFO 2017-06-27 20:46:11,425 main.py:51] epoch 3342, training loss: 12158.66, average training loss: 11924.40, base loss: 19745.56
[INFO 2017-06-27 20:46:11,833 main.py:51] epoch 3343, training loss: 12450.40, average training loss: 11925.68, base loss: 19748.83
[INFO 2017-06-27 20:46:12,262 main.py:51] epoch 3344, training loss: 11614.98, average training loss: 11924.81, base loss: 19746.99
[INFO 2017-06-27 20:46:12,644 main.py:51] epoch 3345, training loss: 11301.19, average training loss: 11923.61, base loss: 19744.66
[INFO 2017-06-27 20:46:13,015 main.py:51] epoch 3346, training loss: 12774.12, average training loss: 11924.52, base loss: 19745.77
[INFO 2017-06-27 20:46:13,398 main.py:51] epoch 3347, training loss: 12380.47, average training loss: 11924.55, base loss: 19746.48
[INFO 2017-06-27 20:46:13,770 main.py:51] epoch 3348, training loss: 11091.74, average training loss: 11922.48, base loss: 19743.26
[INFO 2017-06-27 20:46:14,143 main.py:51] epoch 3349, training loss: 13026.25, average training loss: 11921.71, base loss: 19742.52
[INFO 2017-06-27 20:46:14,517 main.py:51] epoch 3350, training loss: 11544.68, average training loss: 11920.51, base loss: 19742.45
[INFO 2017-06-27 20:46:14,892 main.py:51] epoch 3351, training loss: 12285.61, average training loss: 11921.27, base loss: 19744.96
[INFO 2017-06-27 20:46:15,264 main.py:51] epoch 3352, training loss: 11139.56, average training loss: 11920.50, base loss: 19744.47
[INFO 2017-06-27 20:46:15,641 main.py:51] epoch 3353, training loss: 11545.56, average training loss: 11920.39, base loss: 19744.00
[INFO 2017-06-27 20:46:16,015 main.py:51] epoch 3354, training loss: 11393.17, average training loss: 11918.30, base loss: 19741.30
[INFO 2017-06-27 20:46:16,391 main.py:51] epoch 3355, training loss: 11255.65, average training loss: 11917.70, base loss: 19740.24
[INFO 2017-06-27 20:46:16,766 main.py:51] epoch 3356, training loss: 9946.15, average training loss: 11914.99, base loss: 19735.41
[INFO 2017-06-27 20:46:17,147 main.py:51] epoch 3357, training loss: 12792.40, average training loss: 11914.91, base loss: 19737.45
[INFO 2017-06-27 20:46:17,524 main.py:51] epoch 3358, training loss: 11118.56, average training loss: 11913.38, base loss: 19734.59
[INFO 2017-06-27 20:46:17,905 main.py:51] epoch 3359, training loss: 11778.26, average training loss: 11912.75, base loss: 19736.01
[INFO 2017-06-27 20:46:18,282 main.py:51] epoch 3360, training loss: 12414.81, average training loss: 11912.01, base loss: 19734.38
[INFO 2017-06-27 20:46:18,673 main.py:51] epoch 3361, training loss: 15201.82, average training loss: 11915.78, base loss: 19741.71
[INFO 2017-06-27 20:46:19,074 main.py:51] epoch 3362, training loss: 12491.96, average training loss: 11915.94, base loss: 19740.41
[INFO 2017-06-27 20:46:19,457 main.py:51] epoch 3363, training loss: 12899.27, average training loss: 11914.24, base loss: 19737.70
[INFO 2017-06-27 20:46:19,837 main.py:51] epoch 3364, training loss: 11673.51, average training loss: 11914.07, base loss: 19737.44
[INFO 2017-06-27 20:46:20,224 main.py:51] epoch 3365, training loss: 12354.00, average training loss: 11914.09, base loss: 19738.26
[INFO 2017-06-27 20:46:20,598 main.py:51] epoch 3366, training loss: 12345.10, average training loss: 11913.18, base loss: 19737.08
[INFO 2017-06-27 20:46:20,975 main.py:51] epoch 3367, training loss: 11019.18, average training loss: 11911.19, base loss: 19735.58
[INFO 2017-06-27 20:46:21,445 main.py:51] epoch 3368, training loss: 12199.42, average training loss: 11910.31, base loss: 19735.42
[INFO 2017-06-27 20:46:21,839 main.py:51] epoch 3369, training loss: 11577.34, average training loss: 11908.99, base loss: 19733.59
[INFO 2017-06-27 20:46:22,218 main.py:51] epoch 3370, training loss: 11390.94, average training loss: 11908.03, base loss: 19732.75
[INFO 2017-06-27 20:46:22,597 main.py:51] epoch 3371, training loss: 11658.11, average training loss: 11908.04, base loss: 19733.54
[INFO 2017-06-27 20:46:22,976 main.py:51] epoch 3372, training loss: 11172.37, average training loss: 11906.49, base loss: 19730.28
[INFO 2017-06-27 20:46:23,357 main.py:51] epoch 3373, training loss: 12085.18, average training loss: 11907.27, base loss: 19732.52
[INFO 2017-06-27 20:46:23,727 main.py:51] epoch 3374, training loss: 12046.57, average training loss: 11907.01, base loss: 19732.97
[INFO 2017-06-27 20:46:24,206 main.py:51] epoch 3375, training loss: 12051.67, average training loss: 11906.18, base loss: 19731.67
[INFO 2017-06-27 20:46:24,589 main.py:51] epoch 3376, training loss: 13450.58, average training loss: 11907.57, base loss: 19735.18
[INFO 2017-06-27 20:46:24,971 main.py:51] epoch 3377, training loss: 11524.99, average training loss: 11907.54, base loss: 19734.94
[INFO 2017-06-27 20:46:25,356 main.py:51] epoch 3378, training loss: 11534.03, average training loss: 11907.82, base loss: 19734.83
[INFO 2017-06-27 20:46:25,748 main.py:51] epoch 3379, training loss: 10881.75, average training loss: 11904.69, base loss: 19728.46
[INFO 2017-06-27 20:46:26,125 main.py:51] epoch 3380, training loss: 12764.58, average training loss: 11905.85, base loss: 19730.47
[INFO 2017-06-27 20:46:26,507 main.py:51] epoch 3381, training loss: 14046.27, average training loss: 11906.92, base loss: 19733.20
[INFO 2017-06-27 20:46:26,882 main.py:51] epoch 3382, training loss: 13014.09, average training loss: 11907.22, base loss: 19734.87
[INFO 2017-06-27 20:46:27,259 main.py:51] epoch 3383, training loss: 11569.02, average training loss: 11905.07, base loss: 19734.54
[INFO 2017-06-27 20:46:27,636 main.py:51] epoch 3384, training loss: 12641.09, average training loss: 11904.60, base loss: 19733.74
[INFO 2017-06-27 20:46:28,016 main.py:51] epoch 3385, training loss: 12305.92, average training loss: 11904.75, base loss: 19735.40
[INFO 2017-06-27 20:46:28,393 main.py:51] epoch 3386, training loss: 10624.03, average training loss: 11904.24, base loss: 19735.67
[INFO 2017-06-27 20:46:28,775 main.py:51] epoch 3387, training loss: 12043.88, average training loss: 11904.13, base loss: 19737.81
[INFO 2017-06-27 20:46:29,155 main.py:51] epoch 3388, training loss: 11021.92, average training loss: 11902.06, base loss: 19734.83
[INFO 2017-06-27 20:46:29,532 main.py:51] epoch 3389, training loss: 11643.52, average training loss: 11901.51, base loss: 19735.39
[INFO 2017-06-27 20:46:29,909 main.py:51] epoch 3390, training loss: 10501.33, average training loss: 11898.42, base loss: 19731.08
[INFO 2017-06-27 20:46:30,288 main.py:51] epoch 3391, training loss: 12646.82, average training loss: 11900.40, base loss: 19735.04
[INFO 2017-06-27 20:46:30,665 main.py:51] epoch 3392, training loss: 11624.23, average training loss: 11899.07, base loss: 19732.67
[INFO 2017-06-27 20:46:31,046 main.py:51] epoch 3393, training loss: 11327.08, average training loss: 11896.84, base loss: 19730.55
[INFO 2017-06-27 20:46:31,442 main.py:51] epoch 3394, training loss: 13359.76, average training loss: 11897.23, base loss: 19732.04
[INFO 2017-06-27 20:46:31,826 main.py:51] epoch 3395, training loss: 11493.85, average training loss: 11896.61, base loss: 19733.48
[INFO 2017-06-27 20:46:32,203 main.py:51] epoch 3396, training loss: 11416.66, average training loss: 11896.90, base loss: 19735.93
[INFO 2017-06-27 20:46:32,586 main.py:51] epoch 3397, training loss: 11851.19, average training loss: 11897.34, base loss: 19736.69
[INFO 2017-06-27 20:46:32,964 main.py:51] epoch 3398, training loss: 12737.44, average training loss: 11899.81, base loss: 19742.80
[INFO 2017-06-27 20:46:33,340 main.py:51] epoch 3399, training loss: 10152.93, average training loss: 11896.58, base loss: 19736.99
[INFO 2017-06-27 20:46:33,340 main.py:53] epoch 3399, testing
[INFO 2017-06-27 20:46:34,951 main.py:105] average testing loss: 11335.51, base loss: 18661.77
[INFO 2017-06-27 20:46:34,951 main.py:106] improve_loss: 7326.26, improve_percent: 0.39
[INFO 2017-06-27 20:46:34,952 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 20:46:35,328 main.py:51] epoch 3400, training loss: 11564.54, average training loss: 11894.81, base loss: 19734.69
[INFO 2017-06-27 20:46:35,704 main.py:51] epoch 3401, training loss: 11988.73, average training loss: 11894.59, base loss: 19735.44
[INFO 2017-06-27 20:46:36,081 main.py:51] epoch 3402, training loss: 10285.29, average training loss: 11892.72, base loss: 19732.87
[INFO 2017-06-27 20:46:36,460 main.py:51] epoch 3403, training loss: 12367.74, average training loss: 11891.78, base loss: 19732.28
[INFO 2017-06-27 20:46:36,841 main.py:51] epoch 3404, training loss: 11063.36, average training loss: 11889.60, base loss: 19728.17
[INFO 2017-06-27 20:46:37,224 main.py:51] epoch 3405, training loss: 9590.69, average training loss: 11888.44, base loss: 19725.62
[INFO 2017-06-27 20:46:37,599 main.py:51] epoch 3406, training loss: 11911.00, average training loss: 11888.27, base loss: 19726.79
[INFO 2017-06-27 20:46:37,976 main.py:51] epoch 3407, training loss: 11188.67, average training loss: 11886.47, base loss: 19724.81
[INFO 2017-06-27 20:46:38,359 main.py:51] epoch 3408, training loss: 12224.45, average training loss: 11886.15, base loss: 19724.62
[INFO 2017-06-27 20:46:38,736 main.py:51] epoch 3409, training loss: 9990.75, average training loss: 11884.20, base loss: 19721.88
[INFO 2017-06-27 20:46:39,112 main.py:51] epoch 3410, training loss: 11206.57, average training loss: 11882.96, base loss: 19720.23
[INFO 2017-06-27 20:46:39,494 main.py:51] epoch 3411, training loss: 11137.26, average training loss: 11881.91, base loss: 19717.33
[INFO 2017-06-27 20:46:39,871 main.py:51] epoch 3412, training loss: 13190.18, average training loss: 11882.28, base loss: 19718.49
[INFO 2017-06-27 20:46:40,246 main.py:51] epoch 3413, training loss: 11563.54, average training loss: 11880.72, base loss: 19716.04
[INFO 2017-06-27 20:46:40,621 main.py:51] epoch 3414, training loss: 11979.69, average training loss: 11880.91, base loss: 19718.07
[INFO 2017-06-27 20:46:40,995 main.py:51] epoch 3415, training loss: 10904.32, average training loss: 11879.83, base loss: 19717.67
[INFO 2017-06-27 20:46:41,376 main.py:51] epoch 3416, training loss: 11196.95, average training loss: 11878.06, base loss: 19715.20
[INFO 2017-06-27 20:46:41,752 main.py:51] epoch 3417, training loss: 10646.14, average training loss: 11876.57, base loss: 19712.61
[INFO 2017-06-27 20:46:42,133 main.py:51] epoch 3418, training loss: 10336.63, average training loss: 11876.14, base loss: 19712.22
[INFO 2017-06-27 20:46:42,521 main.py:51] epoch 3419, training loss: 11941.67, average training loss: 11876.80, base loss: 19714.89
[INFO 2017-06-27 20:46:42,897 main.py:51] epoch 3420, training loss: 11185.61, average training loss: 11876.46, base loss: 19715.44
[INFO 2017-06-27 20:46:43,273 main.py:51] epoch 3421, training loss: 11832.29, average training loss: 11875.92, base loss: 19713.92
[INFO 2017-06-27 20:46:43,748 main.py:51] epoch 3422, training loss: 11761.91, average training loss: 11877.40, base loss: 19718.33
[INFO 2017-06-27 20:46:44,146 main.py:51] epoch 3423, training loss: 11729.64, average training loss: 11877.03, base loss: 19718.44
[INFO 2017-06-27 20:46:44,530 main.py:51] epoch 3424, training loss: 11678.78, average training loss: 11877.01, base loss: 19717.67
[INFO 2017-06-27 20:46:44,910 main.py:51] epoch 3425, training loss: 11549.01, average training loss: 11875.20, base loss: 19713.47
[INFO 2017-06-27 20:46:45,298 main.py:51] epoch 3426, training loss: 10981.77, average training loss: 11874.82, base loss: 19711.69
[INFO 2017-06-27 20:46:45,678 main.py:51] epoch 3427, training loss: 11935.73, average training loss: 11873.40, base loss: 19709.60
[INFO 2017-06-27 20:46:46,054 main.py:51] epoch 3428, training loss: 10749.24, average training loss: 11872.32, base loss: 19707.68
[INFO 2017-06-27 20:46:46,439 main.py:51] epoch 3429, training loss: 10594.32, average training loss: 11870.67, base loss: 19704.99
[INFO 2017-06-27 20:46:46,816 main.py:51] epoch 3430, training loss: 11728.74, average training loss: 11869.63, base loss: 19704.22
[INFO 2017-06-27 20:46:47,218 main.py:51] epoch 3431, training loss: 10719.61, average training loss: 11868.82, base loss: 19704.34
[INFO 2017-06-27 20:46:47,621 main.py:51] epoch 3432, training loss: 11834.71, average training loss: 11868.40, base loss: 19703.81
[INFO 2017-06-27 20:46:48,003 main.py:51] epoch 3433, training loss: 11816.06, average training loss: 11868.09, base loss: 19703.09
[INFO 2017-06-27 20:46:48,383 main.py:51] epoch 3434, training loss: 12192.08, average training loss: 11869.21, base loss: 19706.38
[INFO 2017-06-27 20:46:48,763 main.py:51] epoch 3435, training loss: 10243.45, average training loss: 11867.53, base loss: 19702.88
[INFO 2017-06-27 20:46:49,231 main.py:51] epoch 3436, training loss: 12339.97, average training loss: 11867.46, base loss: 19703.28
[INFO 2017-06-27 20:46:49,633 main.py:51] epoch 3437, training loss: 11827.21, average training loss: 11867.12, base loss: 19703.83
[INFO 2017-06-27 20:46:50,033 main.py:51] epoch 3438, training loss: 12363.63, average training loss: 11867.85, base loss: 19706.53
[INFO 2017-06-27 20:46:50,414 main.py:51] epoch 3439, training loss: 10234.32, average training loss: 11866.58, base loss: 19703.38
[INFO 2017-06-27 20:46:50,792 main.py:51] epoch 3440, training loss: 13494.02, average training loss: 11869.11, base loss: 19707.49
[INFO 2017-06-27 20:46:51,170 main.py:51] epoch 3441, training loss: 11388.12, average training loss: 11869.43, base loss: 19708.64
[INFO 2017-06-27 20:46:51,557 main.py:51] epoch 3442, training loss: 11018.15, average training loss: 11868.56, base loss: 19707.36
[INFO 2017-06-27 20:46:51,934 main.py:51] epoch 3443, training loss: 13808.80, average training loss: 11869.62, base loss: 19707.86
[INFO 2017-06-27 20:46:52,310 main.py:51] epoch 3444, training loss: 11684.76, average training loss: 11868.84, base loss: 19706.87
[INFO 2017-06-27 20:46:52,688 main.py:51] epoch 3445, training loss: 12142.67, average training loss: 11870.08, base loss: 19710.95
[INFO 2017-06-27 20:46:53,062 main.py:51] epoch 3446, training loss: 11413.30, average training loss: 11869.06, base loss: 19709.17
[INFO 2017-06-27 20:46:53,437 main.py:51] epoch 3447, training loss: 13011.10, average training loss: 11869.99, base loss: 19710.77
[INFO 2017-06-27 20:46:53,821 main.py:51] epoch 3448, training loss: 11905.47, average training loss: 11871.70, base loss: 19715.39
[INFO 2017-06-27 20:46:54,272 main.py:51] epoch 3449, training loss: 12863.38, average training loss: 11872.67, base loss: 19716.93
[INFO 2017-06-27 20:46:54,672 main.py:51] epoch 3450, training loss: 11186.45, average training loss: 11871.59, base loss: 19715.77
[INFO 2017-06-27 20:46:55,051 main.py:51] epoch 3451, training loss: 11510.99, average training loss: 11869.94, base loss: 19713.34
[INFO 2017-06-27 20:46:55,448 main.py:51] epoch 3452, training loss: 12449.15, average training loss: 11869.26, base loss: 19711.75
[INFO 2017-06-27 20:46:55,838 main.py:51] epoch 3453, training loss: 10812.39, average training loss: 11867.08, base loss: 19708.99
[INFO 2017-06-27 20:46:56,220 main.py:51] epoch 3454, training loss: 11384.71, average training loss: 11866.49, base loss: 19708.66
[INFO 2017-06-27 20:46:56,598 main.py:51] epoch 3455, training loss: 12828.02, average training loss: 11866.82, base loss: 19709.28
[INFO 2017-06-27 20:46:56,973 main.py:51] epoch 3456, training loss: 11018.50, average training loss: 11865.23, base loss: 19706.71
[INFO 2017-06-27 20:46:57,348 main.py:51] epoch 3457, training loss: 12034.95, average training loss: 11865.59, base loss: 19707.26
[INFO 2017-06-27 20:46:57,719 main.py:51] epoch 3458, training loss: 11956.65, average training loss: 11865.87, base loss: 19710.76
[INFO 2017-06-27 20:46:58,098 main.py:51] epoch 3459, training loss: 11724.82, average training loss: 11864.33, base loss: 19709.36
[INFO 2017-06-27 20:46:58,490 main.py:51] epoch 3460, training loss: 14266.20, average training loss: 11866.04, base loss: 19714.22
[INFO 2017-06-27 20:46:58,867 main.py:51] epoch 3461, training loss: 12309.72, average training loss: 11866.78, base loss: 19717.31
[INFO 2017-06-27 20:46:59,263 main.py:51] epoch 3462, training loss: 11327.26, average training loss: 11864.81, base loss: 19715.46
[INFO 2017-06-27 20:46:59,656 main.py:51] epoch 3463, training loss: 11103.58, average training loss: 11864.76, base loss: 19716.84
[INFO 2017-06-27 20:47:00,040 main.py:51] epoch 3464, training loss: 11911.52, average training loss: 11862.41, base loss: 19711.67
[INFO 2017-06-27 20:47:00,495 main.py:51] epoch 3465, training loss: 11674.50, average training loss: 11860.59, base loss: 19708.33
[INFO 2017-06-27 20:47:00,893 main.py:51] epoch 3466, training loss: 12275.94, average training loss: 11859.64, base loss: 19709.10
[INFO 2017-06-27 20:47:01,275 main.py:51] epoch 3467, training loss: 11744.43, average training loss: 11859.69, base loss: 19709.39
[INFO 2017-06-27 20:47:01,654 main.py:51] epoch 3468, training loss: 10248.66, average training loss: 11858.29, base loss: 19707.53
[INFO 2017-06-27 20:47:02,060 main.py:51] epoch 3469, training loss: 12625.04, average training loss: 11860.29, base loss: 19711.83
[INFO 2017-06-27 20:47:02,435 main.py:51] epoch 3470, training loss: 12377.42, average training loss: 11860.13, base loss: 19712.37
[INFO 2017-06-27 20:47:02,811 main.py:51] epoch 3471, training loss: 13443.48, average training loss: 11858.66, base loss: 19710.46
[INFO 2017-06-27 20:47:03,187 main.py:51] epoch 3472, training loss: 12109.16, average training loss: 11857.44, base loss: 19708.46
[INFO 2017-06-27 20:47:03,558 main.py:51] epoch 3473, training loss: 11783.89, average training loss: 11856.66, base loss: 19709.65
[INFO 2017-06-27 20:47:04,007 main.py:51] epoch 3474, training loss: 12051.41, average training loss: 11856.28, base loss: 19708.68
[INFO 2017-06-27 20:47:04,406 main.py:51] epoch 3475, training loss: 11060.69, average training loss: 11855.18, base loss: 19708.91
[INFO 2017-06-27 20:47:04,786 main.py:51] epoch 3476, training loss: 12926.62, average training loss: 11853.88, base loss: 19707.76
[INFO 2017-06-27 20:47:05,165 main.py:51] epoch 3477, training loss: 11556.49, average training loss: 11853.58, base loss: 19708.17
[INFO 2017-06-27 20:47:05,556 main.py:51] epoch 3478, training loss: 11554.82, average training loss: 11852.66, base loss: 19708.93
[INFO 2017-06-27 20:47:05,934 main.py:51] epoch 3479, training loss: 12006.02, average training loss: 11852.99, base loss: 19710.70
[INFO 2017-06-27 20:47:06,310 main.py:51] epoch 3480, training loss: 11976.41, average training loss: 11853.65, base loss: 19712.39
[INFO 2017-06-27 20:47:06,687 main.py:51] epoch 3481, training loss: 12069.58, average training loss: 11854.25, base loss: 19714.79
[INFO 2017-06-27 20:47:07,169 main.py:51] epoch 3482, training loss: 12118.33, average training loss: 11854.16, base loss: 19716.02
[INFO 2017-06-27 20:47:07,548 main.py:51] epoch 3483, training loss: 11872.35, average training loss: 11853.77, base loss: 19715.23
[INFO 2017-06-27 20:47:07,922 main.py:51] epoch 3484, training loss: 11933.39, average training loss: 11854.18, base loss: 19716.32
[INFO 2017-06-27 20:47:08,302 main.py:51] epoch 3485, training loss: 11832.56, average training loss: 11854.70, base loss: 19718.34
[INFO 2017-06-27 20:47:08,682 main.py:51] epoch 3486, training loss: 11303.09, average training loss: 11852.80, base loss: 19714.41
[INFO 2017-06-27 20:47:09,064 main.py:51] epoch 3487, training loss: 10869.48, average training loss: 11851.38, base loss: 19713.41
[INFO 2017-06-27 20:47:09,443 main.py:51] epoch 3488, training loss: 10128.09, average training loss: 11852.50, base loss: 19717.14
[INFO 2017-06-27 20:47:09,818 main.py:51] epoch 3489, training loss: 10943.80, average training loss: 11852.19, base loss: 19717.71
[INFO 2017-06-27 20:47:10,206 main.py:51] epoch 3490, training loss: 11720.21, average training loss: 11852.33, base loss: 19718.51
[INFO 2017-06-27 20:47:10,581 main.py:51] epoch 3491, training loss: 12227.37, average training loss: 11853.06, base loss: 19722.18
[INFO 2017-06-27 20:47:10,955 main.py:51] epoch 3492, training loss: 13181.20, average training loss: 11852.64, base loss: 19722.33
[INFO 2017-06-27 20:47:11,327 main.py:51] epoch 3493, training loss: 12107.75, average training loss: 11853.58, base loss: 19724.53
[INFO 2017-06-27 20:47:11,702 main.py:51] epoch 3494, training loss: 11502.04, average training loss: 11853.36, base loss: 19726.23
[INFO 2017-06-27 20:47:12,072 main.py:51] epoch 3495, training loss: 10956.08, average training loss: 11850.74, base loss: 19722.40
[INFO 2017-06-27 20:47:12,442 main.py:51] epoch 3496, training loss: 13650.21, average training loss: 11853.37, base loss: 19730.38
[INFO 2017-06-27 20:47:12,820 main.py:51] epoch 3497, training loss: 10938.06, average training loss: 11852.75, base loss: 19729.93
[INFO 2017-06-27 20:47:13,203 main.py:51] epoch 3498, training loss: 11013.12, average training loss: 11851.36, base loss: 19728.08
[INFO 2017-06-27 20:47:13,583 main.py:51] epoch 3499, training loss: 12417.83, average training loss: 11851.91, base loss: 19730.02
[INFO 2017-06-27 20:47:13,583 main.py:53] epoch 3499, testing
[INFO 2017-06-27 20:47:15,189 main.py:105] average testing loss: 11805.53, base loss: 19976.74
[INFO 2017-06-27 20:47:15,190 main.py:106] improve_loss: 8171.20, improve_percent: 0.41
[INFO 2017-06-27 20:47:15,190 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:47:15,203 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 20:47:15,580 main.py:51] epoch 3500, training loss: 10456.22, average training loss: 11849.90, base loss: 19726.56
[INFO 2017-06-27 20:47:16,050 main.py:51] epoch 3501, training loss: 10382.02, average training loss: 11848.60, base loss: 19723.91
[INFO 2017-06-27 20:47:16,455 main.py:51] epoch 3502, training loss: 11663.16, average training loss: 11846.66, base loss: 19722.99
[INFO 2017-06-27 20:47:16,838 main.py:51] epoch 3503, training loss: 10875.08, average training loss: 11844.45, base loss: 19718.55
[INFO 2017-06-27 20:47:17,217 main.py:51] epoch 3504, training loss: 11331.69, average training loss: 11844.83, base loss: 19719.91
[INFO 2017-06-27 20:47:17,595 main.py:51] epoch 3505, training loss: 12164.01, average training loss: 11846.62, base loss: 19723.33
[INFO 2017-06-27 20:47:17,971 main.py:51] epoch 3506, training loss: 13662.43, average training loss: 11849.55, base loss: 19729.51
[INFO 2017-06-27 20:47:18,349 main.py:51] epoch 3507, training loss: 11869.57, average training loss: 11849.50, base loss: 19729.23
[INFO 2017-06-27 20:47:18,724 main.py:51] epoch 3508, training loss: 11325.51, average training loss: 11850.06, base loss: 19730.79
[INFO 2017-06-27 20:47:19,098 main.py:51] epoch 3509, training loss: 11126.25, average training loss: 11848.19, base loss: 19727.92
[INFO 2017-06-27 20:47:19,470 main.py:51] epoch 3510, training loss: 11869.31, average training loss: 11847.59, base loss: 19727.17
[INFO 2017-06-27 20:47:19,850 main.py:51] epoch 3511, training loss: 11186.03, average training loss: 11846.06, base loss: 19726.23
[INFO 2017-06-27 20:47:20,228 main.py:51] epoch 3512, training loss: 11394.67, average training loss: 11845.66, base loss: 19727.79
[INFO 2017-06-27 20:47:20,607 main.py:51] epoch 3513, training loss: 11278.08, average training loss: 11845.56, base loss: 19729.82
[INFO 2017-06-27 20:47:20,987 main.py:51] epoch 3514, training loss: 11314.21, average training loss: 11844.39, base loss: 19728.55
[INFO 2017-06-27 20:47:21,366 main.py:51] epoch 3515, training loss: 11056.41, average training loss: 11842.70, base loss: 19727.58
[INFO 2017-06-27 20:47:21,812 main.py:51] epoch 3516, training loss: 13531.67, average training loss: 11844.01, base loss: 19730.08
[INFO 2017-06-27 20:47:22,236 main.py:51] epoch 3517, training loss: 12400.29, average training loss: 11844.98, base loss: 19732.68
[INFO 2017-06-27 20:47:22,621 main.py:51] epoch 3518, training loss: 11615.65, average training loss: 11845.78, base loss: 19736.05
[INFO 2017-06-27 20:47:23,008 main.py:51] epoch 3519, training loss: 11500.74, average training loss: 11845.27, base loss: 19735.37
[INFO 2017-06-27 20:47:23,401 main.py:51] epoch 3520, training loss: 10462.39, average training loss: 11843.92, base loss: 19732.74
[INFO 2017-06-27 20:47:23,848 main.py:51] epoch 3521, training loss: 11606.15, average training loss: 11842.05, base loss: 19728.20
[INFO 2017-06-27 20:47:24,283 main.py:51] epoch 3522, training loss: 9885.45, average training loss: 11839.82, base loss: 19723.25
[INFO 2017-06-27 20:47:24,728 main.py:51] epoch 3523, training loss: 11188.26, average training loss: 11839.85, base loss: 19724.88
[INFO 2017-06-27 20:47:25,190 main.py:51] epoch 3524, training loss: 11999.10, average training loss: 11840.51, base loss: 19727.97
[INFO 2017-06-27 20:47:25,623 main.py:51] epoch 3525, training loss: 10306.58, average training loss: 11838.66, base loss: 19726.02
[INFO 2017-06-27 20:47:26,022 main.py:51] epoch 3526, training loss: 13273.90, average training loss: 11838.43, base loss: 19725.47
[INFO 2017-06-27 20:47:26,474 main.py:51] epoch 3527, training loss: 11713.73, average training loss: 11838.14, base loss: 19727.14
[INFO 2017-06-27 20:47:26,958 main.py:51] epoch 3528, training loss: 11627.56, average training loss: 11837.33, base loss: 19726.21
[INFO 2017-06-27 20:47:27,357 main.py:51] epoch 3529, training loss: 10056.23, average training loss: 11835.33, base loss: 19723.50
[INFO 2017-06-27 20:47:27,743 main.py:51] epoch 3530, training loss: 11164.76, average training loss: 11834.36, base loss: 19723.39
[INFO 2017-06-27 20:47:28,119 main.py:51] epoch 3531, training loss: 12148.26, average training loss: 11833.62, base loss: 19723.05
[INFO 2017-06-27 20:47:28,532 main.py:51] epoch 3532, training loss: 11299.41, average training loss: 11834.12, base loss: 19724.48
[INFO 2017-06-27 20:47:28,912 main.py:51] epoch 3533, training loss: 13366.40, average training loss: 11835.77, base loss: 19729.81
[INFO 2017-06-27 20:47:29,286 main.py:51] epoch 3534, training loss: 10943.99, average training loss: 11834.78, base loss: 19728.28
[INFO 2017-06-27 20:47:29,661 main.py:51] epoch 3535, training loss: 9957.87, average training loss: 11832.09, base loss: 19724.12
[INFO 2017-06-27 20:47:30,036 main.py:51] epoch 3536, training loss: 12422.73, average training loss: 11832.06, base loss: 19724.95
[INFO 2017-06-27 20:47:30,410 main.py:51] epoch 3537, training loss: 10897.91, average training loss: 11830.90, base loss: 19721.83
[INFO 2017-06-27 20:47:30,786 main.py:51] epoch 3538, training loss: 11586.28, average training loss: 11829.53, base loss: 19718.95
[INFO 2017-06-27 20:47:31,161 main.py:51] epoch 3539, training loss: 12069.03, average training loss: 11829.91, base loss: 19719.88
[INFO 2017-06-27 20:47:31,532 main.py:51] epoch 3540, training loss: 10848.49, average training loss: 11828.60, base loss: 19718.33
[INFO 2017-06-27 20:47:31,905 main.py:51] epoch 3541, training loss: 12619.94, average training loss: 11829.63, base loss: 19721.64
[INFO 2017-06-27 20:47:32,281 main.py:51] epoch 3542, training loss: 12742.86, average training loss: 11830.02, base loss: 19724.37
[INFO 2017-06-27 20:47:32,657 main.py:51] epoch 3543, training loss: 12023.63, average training loss: 11830.91, base loss: 19727.26
[INFO 2017-06-27 20:47:33,033 main.py:51] epoch 3544, training loss: 12552.47, average training loss: 11833.45, base loss: 19732.98
[INFO 2017-06-27 20:47:33,411 main.py:51] epoch 3545, training loss: 11419.12, average training loss: 11832.66, base loss: 19732.94
[INFO 2017-06-27 20:47:33,787 main.py:51] epoch 3546, training loss: 10675.42, average training loss: 11830.74, base loss: 19729.91
[INFO 2017-06-27 20:47:34,169 main.py:51] epoch 3547, training loss: 11737.64, average training loss: 11830.18, base loss: 19730.70
[INFO 2017-06-27 20:47:34,548 main.py:51] epoch 3548, training loss: 10309.07, average training loss: 11826.75, base loss: 19725.34
[INFO 2017-06-27 20:47:34,944 main.py:51] epoch 3549, training loss: 12113.51, average training loss: 11825.63, base loss: 19724.15
[INFO 2017-06-27 20:47:35,350 main.py:51] epoch 3550, training loss: 10877.19, average training loss: 11825.22, base loss: 19723.41
[INFO 2017-06-27 20:47:35,728 main.py:51] epoch 3551, training loss: 11253.88, average training loss: 11826.24, base loss: 19726.10
[INFO 2017-06-27 20:47:36,170 main.py:51] epoch 3552, training loss: 11905.04, average training loss: 11827.63, base loss: 19729.68
[INFO 2017-06-27 20:47:36,613 main.py:51] epoch 3553, training loss: 12305.96, average training loss: 11826.99, base loss: 19729.43
[INFO 2017-06-27 20:47:36,992 main.py:51] epoch 3554, training loss: 10680.74, average training loss: 11825.23, base loss: 19725.44
[INFO 2017-06-27 20:47:37,370 main.py:51] epoch 3555, training loss: 13391.69, average training loss: 11826.61, base loss: 19729.39
[INFO 2017-06-27 20:47:37,763 main.py:51] epoch 3556, training loss: 10529.89, average training loss: 11825.45, base loss: 19726.19
[INFO 2017-06-27 20:47:38,141 main.py:51] epoch 3557, training loss: 12160.98, average training loss: 11825.25, base loss: 19726.67
[INFO 2017-06-27 20:47:38,518 main.py:51] epoch 3558, training loss: 12065.02, average training loss: 11825.55, base loss: 19728.72
[INFO 2017-06-27 20:47:38,891 main.py:51] epoch 3559, training loss: 12191.07, average training loss: 11826.62, base loss: 19730.87
[INFO 2017-06-27 20:47:39,268 main.py:51] epoch 3560, training loss: 10559.06, average training loss: 11825.67, base loss: 19729.87
[INFO 2017-06-27 20:47:39,643 main.py:51] epoch 3561, training loss: 10990.05, average training loss: 11824.99, base loss: 19728.38
[INFO 2017-06-27 20:47:40,013 main.py:51] epoch 3562, training loss: 12335.98, average training loss: 11824.58, base loss: 19729.19
[INFO 2017-06-27 20:47:40,387 main.py:51] epoch 3563, training loss: 10284.69, average training loss: 11823.69, base loss: 19728.48
[INFO 2017-06-27 20:47:40,763 main.py:51] epoch 3564, training loss: 12339.39, average training loss: 11825.42, base loss: 19732.66
[INFO 2017-06-27 20:47:41,138 main.py:51] epoch 3565, training loss: 11966.32, average training loss: 11824.90, base loss: 19732.17
[INFO 2017-06-27 20:47:41,519 main.py:51] epoch 3566, training loss: 10907.86, average training loss: 11825.88, base loss: 19734.01
[INFO 2017-06-27 20:47:41,907 main.py:51] epoch 3567, training loss: 11076.34, average training loss: 11824.65, base loss: 19732.72
[INFO 2017-06-27 20:47:42,284 main.py:51] epoch 3568, training loss: 11699.75, average training loss: 11824.73, base loss: 19734.07
[INFO 2017-06-27 20:47:42,662 main.py:51] epoch 3569, training loss: 12048.36, average training loss: 11825.03, base loss: 19735.03
[INFO 2017-06-27 20:47:43,040 main.py:51] epoch 3570, training loss: 11901.62, average training loss: 11825.80, base loss: 19738.52
[INFO 2017-06-27 20:47:43,412 main.py:51] epoch 3571, training loss: 11718.63, average training loss: 11825.52, base loss: 19737.59
[INFO 2017-06-27 20:47:43,791 main.py:51] epoch 3572, training loss: 9877.08, average training loss: 11821.57, base loss: 19730.92
[INFO 2017-06-27 20:47:44,167 main.py:51] epoch 3573, training loss: 12196.43, average training loss: 11821.79, base loss: 19732.23
[INFO 2017-06-27 20:47:44,542 main.py:51] epoch 3574, training loss: 10596.29, average training loss: 11820.89, base loss: 19732.73
[INFO 2017-06-27 20:47:44,914 main.py:51] epoch 3575, training loss: 12074.36, average training loss: 11819.99, base loss: 19733.05
[INFO 2017-06-27 20:47:45,285 main.py:51] epoch 3576, training loss: 10813.08, average training loss: 11816.19, base loss: 19726.82
[INFO 2017-06-27 20:47:45,661 main.py:51] epoch 3577, training loss: 12093.88, average training loss: 11815.74, base loss: 19728.72
[INFO 2017-06-27 20:47:46,037 main.py:51] epoch 3578, training loss: 11700.80, average training loss: 11814.86, base loss: 19727.85
[INFO 2017-06-27 20:47:46,407 main.py:51] epoch 3579, training loss: 11378.95, average training loss: 11814.69, base loss: 19727.38
[INFO 2017-06-27 20:47:46,798 main.py:51] epoch 3580, training loss: 12292.22, average training loss: 11813.19, base loss: 19724.66
[INFO 2017-06-27 20:47:47,177 main.py:51] epoch 3581, training loss: 11890.53, average training loss: 11814.01, base loss: 19728.18
[INFO 2017-06-27 20:47:47,554 main.py:51] epoch 3582, training loss: 11938.01, average training loss: 11814.87, base loss: 19730.93
[INFO 2017-06-27 20:47:47,930 main.py:51] epoch 3583, training loss: 10477.32, average training loss: 11813.71, base loss: 19729.23
[INFO 2017-06-27 20:47:48,341 main.py:51] epoch 3584, training loss: 11603.92, average training loss: 11813.86, base loss: 19729.54
[INFO 2017-06-27 20:47:48,757 main.py:51] epoch 3585, training loss: 11113.25, average training loss: 11812.43, base loss: 19726.62
[INFO 2017-06-27 20:47:49,139 main.py:51] epoch 3586, training loss: 10658.76, average training loss: 11811.84, base loss: 19724.16
[INFO 2017-06-27 20:47:49,516 main.py:51] epoch 3587, training loss: 9745.90, average training loss: 11808.91, base loss: 19719.63
[INFO 2017-06-27 20:47:49,913 main.py:51] epoch 3588, training loss: 11053.53, average training loss: 11808.08, base loss: 19718.71
[INFO 2017-06-27 20:47:50,317 main.py:51] epoch 3589, training loss: 11681.77, average training loss: 11805.88, base loss: 19715.52
[INFO 2017-06-27 20:47:50,697 main.py:51] epoch 3590, training loss: 12577.77, average training loss: 11806.85, base loss: 19717.85
[INFO 2017-06-27 20:47:51,075 main.py:51] epoch 3591, training loss: 13053.63, average training loss: 11807.29, base loss: 19719.82
[INFO 2017-06-27 20:47:51,453 main.py:51] epoch 3592, training loss: 11583.89, average training loss: 11807.31, base loss: 19720.76
[INFO 2017-06-27 20:47:51,830 main.py:51] epoch 3593, training loss: 10823.22, average training loss: 11805.79, base loss: 19718.74
[INFO 2017-06-27 20:47:52,206 main.py:51] epoch 3594, training loss: 12327.20, average training loss: 11807.53, base loss: 19723.19
[INFO 2017-06-27 20:47:52,583 main.py:51] epoch 3595, training loss: 10137.51, average training loss: 11804.74, base loss: 19718.81
[INFO 2017-06-27 20:47:52,960 main.py:51] epoch 3596, training loss: 10535.39, average training loss: 11804.11, base loss: 19716.92
[INFO 2017-06-27 20:47:53,335 main.py:51] epoch 3597, training loss: 11658.82, average training loss: 11803.28, base loss: 19716.27
[INFO 2017-06-27 20:47:53,711 main.py:51] epoch 3598, training loss: 13172.80, average training loss: 11804.11, base loss: 19718.92
[INFO 2017-06-27 20:47:54,082 main.py:51] epoch 3599, training loss: 10267.01, average training loss: 11802.08, base loss: 19715.71
[INFO 2017-06-27 20:47:54,082 main.py:53] epoch 3599, testing
[INFO 2017-06-27 20:47:55,695 main.py:105] average testing loss: 11491.20, base loss: 19387.37
[INFO 2017-06-27 20:47:55,695 main.py:106] improve_loss: 7896.18, improve_percent: 0.41
[INFO 2017-06-27 20:47:55,695 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 20:47:56,067 main.py:51] epoch 3600, training loss: 12568.64, average training loss: 11802.74, base loss: 19715.91
[INFO 2017-06-27 20:47:56,442 main.py:51] epoch 3601, training loss: 11382.71, average training loss: 11803.77, base loss: 19716.59
[INFO 2017-06-27 20:47:56,822 main.py:51] epoch 3602, training loss: 11943.29, average training loss: 11804.70, base loss: 19721.10
[INFO 2017-06-27 20:47:57,202 main.py:51] epoch 3603, training loss: 11784.09, average training loss: 11804.34, base loss: 19720.77
[INFO 2017-06-27 20:47:57,580 main.py:51] epoch 3604, training loss: 12631.87, average training loss: 11801.27, base loss: 19718.10
[INFO 2017-06-27 20:47:57,955 main.py:51] epoch 3605, training loss: 10602.92, average training loss: 11800.59, base loss: 19717.21
[INFO 2017-06-27 20:47:58,328 main.py:51] epoch 3606, training loss: 11084.32, average training loss: 11797.74, base loss: 19713.24
[INFO 2017-06-27 20:47:58,701 main.py:51] epoch 3607, training loss: 11517.07, average training loss: 11796.14, base loss: 19710.15
[INFO 2017-06-27 20:47:59,081 main.py:51] epoch 3608, training loss: 11600.99, average training loss: 11796.15, base loss: 19710.19
[INFO 2017-06-27 20:47:59,457 main.py:51] epoch 3609, training loss: 12595.55, average training loss: 11797.29, base loss: 19713.10
[INFO 2017-06-27 20:47:59,829 main.py:51] epoch 3610, training loss: 10975.19, average training loss: 11795.08, base loss: 19708.68
[INFO 2017-06-27 20:48:00,212 main.py:51] epoch 3611, training loss: 12219.74, average training loss: 11795.33, base loss: 19710.96
[INFO 2017-06-27 20:48:00,585 main.py:51] epoch 3612, training loss: 11570.60, average training loss: 11796.15, base loss: 19712.02
[INFO 2017-06-27 20:48:00,957 main.py:51] epoch 3613, training loss: 12679.47, average training loss: 11797.04, base loss: 19714.48
[INFO 2017-06-27 20:48:01,332 main.py:51] epoch 3614, training loss: 11618.75, average training loss: 11795.85, base loss: 19711.14
[INFO 2017-06-27 20:48:01,705 main.py:51] epoch 3615, training loss: 11604.59, average training loss: 11794.80, base loss: 19708.92
[INFO 2017-06-27 20:48:02,081 main.py:51] epoch 3616, training loss: 13244.70, average training loss: 11795.88, base loss: 19712.60
[INFO 2017-06-27 20:48:02,456 main.py:51] epoch 3617, training loss: 13141.08, average training loss: 11797.58, base loss: 19718.34
[INFO 2017-06-27 20:48:02,830 main.py:51] epoch 3618, training loss: 11388.85, average training loss: 11797.84, base loss: 19720.00
[INFO 2017-06-27 20:48:03,203 main.py:51] epoch 3619, training loss: 11214.67, average training loss: 11796.73, base loss: 19719.15
[INFO 2017-06-27 20:48:03,581 main.py:51] epoch 3620, training loss: 10668.68, average training loss: 11794.99, base loss: 19715.25
[INFO 2017-06-27 20:48:03,962 main.py:51] epoch 3621, training loss: 12227.72, average training loss: 11796.32, base loss: 19719.35
[INFO 2017-06-27 20:48:04,337 main.py:51] epoch 3622, training loss: 9706.01, average training loss: 11792.84, base loss: 19712.88
[INFO 2017-06-27 20:48:04,715 main.py:51] epoch 3623, training loss: 11327.90, average training loss: 11790.64, base loss: 19709.80
[INFO 2017-06-27 20:48:05,096 main.py:51] epoch 3624, training loss: 12360.10, average training loss: 11791.14, base loss: 19711.03
[INFO 2017-06-27 20:48:05,473 main.py:51] epoch 3625, training loss: 10577.64, average training loss: 11788.51, base loss: 19708.28
[INFO 2017-06-27 20:48:05,850 main.py:51] epoch 3626, training loss: 12252.55, average training loss: 11788.16, base loss: 19709.91
[INFO 2017-06-27 20:48:06,286 main.py:51] epoch 3627, training loss: 13207.89, average training loss: 11786.23, base loss: 19707.59
[INFO 2017-06-27 20:48:06,670 main.py:51] epoch 3628, training loss: 13184.08, average training loss: 11786.79, base loss: 19709.18
[INFO 2017-06-27 20:48:07,049 main.py:51] epoch 3629, training loss: 10707.38, average training loss: 11786.81, base loss: 19711.18
[INFO 2017-06-27 20:48:07,446 main.py:51] epoch 3630, training loss: 11321.20, average training loss: 11786.17, base loss: 19711.90
[INFO 2017-06-27 20:48:07,829 main.py:51] epoch 3631, training loss: 12037.86, average training loss: 11786.46, base loss: 19713.67
[INFO 2017-06-27 20:48:08,210 main.py:51] epoch 3632, training loss: 11059.13, average training loss: 11784.72, base loss: 19710.95
[INFO 2017-06-27 20:48:08,587 main.py:51] epoch 3633, training loss: 11231.50, average training loss: 11782.66, base loss: 19707.43
[INFO 2017-06-27 20:48:08,963 main.py:51] epoch 3634, training loss: 13680.50, average training loss: 11784.55, base loss: 19711.30
[INFO 2017-06-27 20:48:09,344 main.py:51] epoch 3635, training loss: 12361.53, average training loss: 11784.90, base loss: 19711.18
[INFO 2017-06-27 20:48:09,731 main.py:51] epoch 3636, training loss: 10402.76, average training loss: 11785.15, base loss: 19712.30
[INFO 2017-06-27 20:48:10,115 main.py:51] epoch 3637, training loss: 12013.31, average training loss: 11785.17, base loss: 19712.72
[INFO 2017-06-27 20:48:10,498 main.py:51] epoch 3638, training loss: 12043.83, average training loss: 11784.22, base loss: 19712.27
[INFO 2017-06-27 20:48:10,875 main.py:51] epoch 3639, training loss: 12060.84, average training loss: 11783.48, base loss: 19713.05
[INFO 2017-06-27 20:48:11,253 main.py:51] epoch 3640, training loss: 11726.47, average training loss: 11782.61, base loss: 19713.26
[INFO 2017-06-27 20:48:11,630 main.py:51] epoch 3641, training loss: 11320.81, average training loss: 11780.24, base loss: 19709.21
[INFO 2017-06-27 20:48:12,006 main.py:51] epoch 3642, training loss: 12855.13, average training loss: 11779.89, base loss: 19710.45
[INFO 2017-06-27 20:48:12,385 main.py:51] epoch 3643, training loss: 11943.57, average training loss: 11780.80, base loss: 19712.92
[INFO 2017-06-27 20:48:12,760 main.py:51] epoch 3644, training loss: 12961.46, average training loss: 11781.94, base loss: 19715.46
[INFO 2017-06-27 20:48:13,137 main.py:51] epoch 3645, training loss: 10848.77, average training loss: 11778.34, base loss: 19710.07
[INFO 2017-06-27 20:48:13,519 main.py:51] epoch 3646, training loss: 13018.13, average training loss: 11780.51, base loss: 19716.33
[INFO 2017-06-27 20:48:13,896 main.py:51] epoch 3647, training loss: 12223.89, average training loss: 11780.48, base loss: 19715.16
[INFO 2017-06-27 20:48:14,275 main.py:51] epoch 3648, training loss: 11162.75, average training loss: 11779.28, base loss: 19713.87
[INFO 2017-06-27 20:48:14,652 main.py:51] epoch 3649, training loss: 10896.97, average training loss: 11777.51, base loss: 19710.57
[INFO 2017-06-27 20:48:15,078 main.py:51] epoch 3650, training loss: 11195.39, average training loss: 11776.37, base loss: 19709.97
[INFO 2017-06-27 20:48:15,491 main.py:51] epoch 3651, training loss: 10821.06, average training loss: 11774.40, base loss: 19706.89
[INFO 2017-06-27 20:48:15,879 main.py:51] epoch 3652, training loss: 12907.75, average training loss: 11775.66, base loss: 19708.73
[INFO 2017-06-27 20:48:16,259 main.py:51] epoch 3653, training loss: 12295.78, average training loss: 11775.43, base loss: 19709.57
[INFO 2017-06-27 20:48:16,711 main.py:51] epoch 3654, training loss: 12979.08, average training loss: 11775.19, base loss: 19709.96
[INFO 2017-06-27 20:48:17,124 main.py:51] epoch 3655, training loss: 10035.52, average training loss: 11771.52, base loss: 19703.78
[INFO 2017-06-27 20:48:17,505 main.py:51] epoch 3656, training loss: 11423.68, average training loss: 11771.62, base loss: 19703.75
[INFO 2017-06-27 20:48:17,981 main.py:51] epoch 3657, training loss: 10499.74, average training loss: 11771.05, base loss: 19702.61
[INFO 2017-06-27 20:48:18,360 main.py:51] epoch 3658, training loss: 11923.56, average training loss: 11770.15, base loss: 19703.42
[INFO 2017-06-27 20:48:18,739 main.py:51] epoch 3659, training loss: 14054.85, average training loss: 11771.86, base loss: 19707.02
[INFO 2017-06-27 20:48:19,119 main.py:51] epoch 3660, training loss: 10891.21, average training loss: 11771.10, base loss: 19706.22
[INFO 2017-06-27 20:48:19,568 main.py:51] epoch 3661, training loss: 11037.01, average training loss: 11771.37, base loss: 19707.09
[INFO 2017-06-27 20:48:19,962 main.py:51] epoch 3662, training loss: 11016.47, average training loss: 11770.29, base loss: 19705.62
[INFO 2017-06-27 20:48:20,338 main.py:51] epoch 3663, training loss: 12832.40, average training loss: 11770.37, base loss: 19706.36
[INFO 2017-06-27 20:48:20,723 main.py:51] epoch 3664, training loss: 12475.42, average training loss: 11770.63, base loss: 19709.59
[INFO 2017-06-27 20:48:21,191 main.py:51] epoch 3665, training loss: 11924.97, average training loss: 11771.69, base loss: 19710.93
[INFO 2017-06-27 20:48:21,591 main.py:51] epoch 3666, training loss: 10961.72, average training loss: 11771.02, base loss: 19710.43
[INFO 2017-06-27 20:48:21,972 main.py:51] epoch 3667, training loss: 10008.88, average training loss: 11768.97, base loss: 19708.76
[INFO 2017-06-27 20:48:22,354 main.py:51] epoch 3668, training loss: 10820.60, average training loss: 11769.01, base loss: 19711.04
[INFO 2017-06-27 20:48:22,748 main.py:51] epoch 3669, training loss: 11541.03, average training loss: 11768.44, base loss: 19711.00
[INFO 2017-06-27 20:48:23,129 main.py:51] epoch 3670, training loss: 10615.74, average training loss: 11767.63, base loss: 19709.41
[INFO 2017-06-27 20:48:23,507 main.py:51] epoch 3671, training loss: 12017.55, average training loss: 11768.26, base loss: 19711.22
[INFO 2017-06-27 20:48:23,887 main.py:51] epoch 3672, training loss: 12276.66, average training loss: 11770.09, base loss: 19715.01
[INFO 2017-06-27 20:48:24,269 main.py:51] epoch 3673, training loss: 12010.05, average training loss: 11769.41, base loss: 19713.13
[INFO 2017-06-27 20:48:24,650 main.py:51] epoch 3674, training loss: 10546.20, average training loss: 11768.96, base loss: 19712.54
[INFO 2017-06-27 20:48:25,032 main.py:51] epoch 3675, training loss: 10185.61, average training loss: 11764.28, base loss: 19703.03
[INFO 2017-06-27 20:48:25,408 main.py:51] epoch 3676, training loss: 10709.02, average training loss: 11763.39, base loss: 19702.40
[INFO 2017-06-27 20:48:25,827 main.py:51] epoch 3677, training loss: 10140.11, average training loss: 11760.30, base loss: 19698.11
[INFO 2017-06-27 20:48:26,259 main.py:51] epoch 3678, training loss: 11479.70, average training loss: 11759.73, base loss: 19696.50
[INFO 2017-06-27 20:48:26,638 main.py:51] epoch 3679, training loss: 12053.85, average training loss: 11761.23, base loss: 19701.11
[INFO 2017-06-27 20:48:27,019 main.py:51] epoch 3680, training loss: 11807.12, average training loss: 11761.04, base loss: 19701.88
[INFO 2017-06-27 20:48:27,404 main.py:51] epoch 3681, training loss: 12068.63, average training loss: 11762.64, base loss: 19706.86
[INFO 2017-06-27 20:48:27,791 main.py:51] epoch 3682, training loss: 10396.26, average training loss: 11761.08, base loss: 19705.27
[INFO 2017-06-27 20:48:28,172 main.py:51] epoch 3683, training loss: 12908.78, average training loss: 11762.10, base loss: 19708.34
[INFO 2017-06-27 20:48:28,553 main.py:51] epoch 3684, training loss: 12337.71, average training loss: 11761.61, base loss: 19708.22
[INFO 2017-06-27 20:48:28,932 main.py:51] epoch 3685, training loss: 12234.42, average training loss: 11761.22, base loss: 19707.33
[INFO 2017-06-27 20:48:29,314 main.py:51] epoch 3686, training loss: 10164.17, average training loss: 11757.02, base loss: 19698.75
[INFO 2017-06-27 20:48:29,689 main.py:51] epoch 3687, training loss: 9939.41, average training loss: 11755.20, base loss: 19695.10
[INFO 2017-06-27 20:48:30,067 main.py:51] epoch 3688, training loss: 12894.44, average training loss: 11756.02, base loss: 19697.62
[INFO 2017-06-27 20:48:30,439 main.py:51] epoch 3689, training loss: 11443.22, average training loss: 11755.66, base loss: 19698.70
[INFO 2017-06-27 20:48:30,815 main.py:51] epoch 3690, training loss: 10378.04, average training loss: 11754.24, base loss: 19697.11
[INFO 2017-06-27 20:48:31,203 main.py:51] epoch 3691, training loss: 11118.23, average training loss: 11754.15, base loss: 19698.68
[INFO 2017-06-27 20:48:31,580 main.py:51] epoch 3692, training loss: 12009.72, average training loss: 11753.85, base loss: 19697.07
[INFO 2017-06-27 20:48:31,956 main.py:51] epoch 3693, training loss: 10757.52, average training loss: 11752.24, base loss: 19695.18
[INFO 2017-06-27 20:48:32,416 main.py:51] epoch 3694, training loss: 10993.49, average training loss: 11752.56, base loss: 19695.23
[INFO 2017-06-27 20:48:32,813 main.py:51] epoch 3695, training loss: 11316.59, average training loss: 11751.06, base loss: 19694.30
[INFO 2017-06-27 20:48:33,195 main.py:51] epoch 3696, training loss: 10520.48, average training loss: 11748.93, base loss: 19690.02
[INFO 2017-06-27 20:48:33,588 main.py:51] epoch 3697, training loss: 13237.29, average training loss: 11750.06, base loss: 19693.99
[INFO 2017-06-27 20:48:33,979 main.py:51] epoch 3698, training loss: 12491.65, average training loss: 11749.57, base loss: 19693.85
[INFO 2017-06-27 20:48:34,358 main.py:51] epoch 3699, training loss: 11950.33, average training loss: 11750.14, base loss: 19695.51
[INFO 2017-06-27 20:48:34,358 main.py:53] epoch 3699, testing
[INFO 2017-06-27 20:48:35,952 main.py:105] average testing loss: 11034.95, base loss: 19003.15
[INFO 2017-06-27 20:48:35,952 main.py:106] improve_loss: 7968.21, improve_percent: 0.42
[INFO 2017-06-27 20:48:35,952 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:48:35,965 main.py:76] current best improved percent: 0.42
[INFO 2017-06-27 20:48:36,344 main.py:51] epoch 3700, training loss: 10914.24, average training loss: 11749.76, base loss: 19696.03
[INFO 2017-06-27 20:48:36,715 main.py:51] epoch 3701, training loss: 11511.24, average training loss: 11747.81, base loss: 19692.38
[INFO 2017-06-27 20:48:37,091 main.py:51] epoch 3702, training loss: 11480.27, average training loss: 11746.48, base loss: 19691.41
[INFO 2017-06-27 20:48:37,468 main.py:51] epoch 3703, training loss: 14482.53, average training loss: 11748.09, base loss: 19694.43
[INFO 2017-06-27 20:48:37,839 main.py:51] epoch 3704, training loss: 11357.05, average training loss: 11747.18, base loss: 19692.51
[INFO 2017-06-27 20:48:38,212 main.py:51] epoch 3705, training loss: 12922.28, average training loss: 11747.57, base loss: 19695.19
[INFO 2017-06-27 20:48:38,586 main.py:51] epoch 3706, training loss: 10934.55, average training loss: 11745.06, base loss: 19691.75
[INFO 2017-06-27 20:48:38,959 main.py:51] epoch 3707, training loss: 12724.60, average training loss: 11745.43, base loss: 19693.33
[INFO 2017-06-27 20:48:39,336 main.py:51] epoch 3708, training loss: 12697.04, average training loss: 11746.20, base loss: 19693.97
[INFO 2017-06-27 20:48:39,714 main.py:51] epoch 3709, training loss: 10887.88, average training loss: 11745.24, base loss: 19692.95
[INFO 2017-06-27 20:48:40,091 main.py:51] epoch 3710, training loss: 13227.90, average training loss: 11745.31, base loss: 19695.46
[INFO 2017-06-27 20:48:40,480 main.py:51] epoch 3711, training loss: 11462.50, average training loss: 11745.20, base loss: 19695.89
[INFO 2017-06-27 20:48:40,859 main.py:51] epoch 3712, training loss: 10453.56, average training loss: 11742.63, base loss: 19690.14
[INFO 2017-06-27 20:48:41,237 main.py:51] epoch 3713, training loss: 11212.51, average training loss: 11740.43, base loss: 19686.35
[INFO 2017-06-27 20:48:41,614 main.py:51] epoch 3714, training loss: 10753.70, average training loss: 11739.30, base loss: 19684.83
[INFO 2017-06-27 20:48:41,990 main.py:51] epoch 3715, training loss: 12327.56, average training loss: 11740.17, base loss: 19686.09
[INFO 2017-06-27 20:48:42,365 main.py:51] epoch 3716, training loss: 11776.76, average training loss: 11739.82, base loss: 19687.32
[INFO 2017-06-27 20:48:42,742 main.py:51] epoch 3717, training loss: 9833.34, average training loss: 11739.13, base loss: 19687.87
[INFO 2017-06-27 20:48:43,111 main.py:51] epoch 3718, training loss: 10832.88, average training loss: 11738.23, base loss: 19686.59
[INFO 2017-06-27 20:48:43,501 main.py:51] epoch 3719, training loss: 11323.74, average training loss: 11738.97, base loss: 19689.00
[INFO 2017-06-27 20:48:43,881 main.py:51] epoch 3720, training loss: 11109.17, average training loss: 11738.42, base loss: 19687.84
[INFO 2017-06-27 20:48:44,260 main.py:51] epoch 3721, training loss: 12538.56, average training loss: 11739.01, base loss: 19688.76
[INFO 2017-06-27 20:48:44,640 main.py:51] epoch 3722, training loss: 10791.26, average training loss: 11738.39, base loss: 19688.12
[INFO 2017-06-27 20:48:45,036 main.py:51] epoch 3723, training loss: 11780.51, average training loss: 11736.84, base loss: 19685.66
[INFO 2017-06-27 20:48:45,427 main.py:51] epoch 3724, training loss: 12114.63, average training loss: 11737.09, base loss: 19686.67
[INFO 2017-06-27 20:48:45,810 main.py:51] epoch 3725, training loss: 12626.96, average training loss: 11737.78, base loss: 19689.36
[INFO 2017-06-27 20:48:46,194 main.py:51] epoch 3726, training loss: 11960.41, average training loss: 11737.70, base loss: 19690.80
[INFO 2017-06-27 20:48:46,587 main.py:51] epoch 3727, training loss: 12183.61, average training loss: 11737.34, base loss: 19691.50
[INFO 2017-06-27 20:48:46,979 main.py:51] epoch 3728, training loss: 11418.59, average training loss: 11737.43, base loss: 19693.31
[INFO 2017-06-27 20:48:47,406 main.py:51] epoch 3729, training loss: 12040.70, average training loss: 11737.30, base loss: 19694.28
[INFO 2017-06-27 20:48:47,845 main.py:51] epoch 3730, training loss: 10277.56, average training loss: 11735.89, base loss: 19691.86
[INFO 2017-06-27 20:48:48,293 main.py:51] epoch 3731, training loss: 10231.75, average training loss: 11732.89, base loss: 19686.22
[INFO 2017-06-27 20:48:48,742 main.py:51] epoch 3732, training loss: 11341.17, average training loss: 11730.48, base loss: 19681.62
[INFO 2017-06-27 20:48:49,199 main.py:51] epoch 3733, training loss: 11532.80, average training loss: 11731.10, base loss: 19684.14
[INFO 2017-06-27 20:48:49,609 main.py:51] epoch 3734, training loss: 11576.75, average training loss: 11731.76, base loss: 19685.50
[INFO 2017-06-27 20:48:50,048 main.py:51] epoch 3735, training loss: 10885.14, average training loss: 11731.37, base loss: 19685.33
[INFO 2017-06-27 20:48:50,523 main.py:51] epoch 3736, training loss: 11580.69, average training loss: 11731.38, base loss: 19685.93
[INFO 2017-06-27 20:48:50,934 main.py:51] epoch 3737, training loss: 11837.52, average training loss: 11729.97, base loss: 19684.42
[INFO 2017-06-27 20:48:51,313 main.py:51] epoch 3738, training loss: 12977.67, average training loss: 11731.86, base loss: 19689.40
[INFO 2017-06-27 20:48:51,688 main.py:51] epoch 3739, training loss: 10846.35, average training loss: 11731.27, base loss: 19689.04
[INFO 2017-06-27 20:48:52,064 main.py:51] epoch 3740, training loss: 10830.27, average training loss: 11727.64, base loss: 19682.92
[INFO 2017-06-27 20:48:52,456 main.py:51] epoch 3741, training loss: 11768.38, average training loss: 11727.47, base loss: 19683.69
[INFO 2017-06-27 20:48:52,835 main.py:51] epoch 3742, training loss: 11424.88, average training loss: 11726.23, base loss: 19683.19
[INFO 2017-06-27 20:48:53,211 main.py:51] epoch 3743, training loss: 10243.96, average training loss: 11725.26, base loss: 19681.94
[INFO 2017-06-27 20:48:53,589 main.py:51] epoch 3744, training loss: 12283.06, average training loss: 11726.04, base loss: 19683.70
[INFO 2017-06-27 20:48:53,962 main.py:51] epoch 3745, training loss: 13165.32, average training loss: 11725.25, base loss: 19682.10
[INFO 2017-06-27 20:48:54,334 main.py:51] epoch 3746, training loss: 11245.63, average training loss: 11725.95, base loss: 19684.06
[INFO 2017-06-27 20:48:54,717 main.py:51] epoch 3747, training loss: 11400.03, average training loss: 11725.15, base loss: 19683.23
[INFO 2017-06-27 20:48:55,089 main.py:51] epoch 3748, training loss: 13270.19, average training loss: 11726.62, base loss: 19686.27
[INFO 2017-06-27 20:48:55,467 main.py:51] epoch 3749, training loss: 11654.17, average training loss: 11725.54, base loss: 19684.82
[INFO 2017-06-27 20:48:55,842 main.py:51] epoch 3750, training loss: 11792.40, average training loss: 11725.55, base loss: 19685.87
[INFO 2017-06-27 20:48:56,216 main.py:51] epoch 3751, training loss: 10383.02, average training loss: 11726.18, base loss: 19687.07
[INFO 2017-06-27 20:48:56,596 main.py:51] epoch 3752, training loss: 9897.96, average training loss: 11723.55, base loss: 19681.74
[INFO 2017-06-27 20:48:56,971 main.py:51] epoch 3753, training loss: 11045.64, average training loss: 11723.47, base loss: 19682.40
[INFO 2017-06-27 20:48:57,358 main.py:51] epoch 3754, training loss: 12217.08, average training loss: 11724.81, base loss: 19686.14
[INFO 2017-06-27 20:48:57,733 main.py:51] epoch 3755, training loss: 11905.89, average training loss: 11723.87, base loss: 19683.56
[INFO 2017-06-27 20:48:58,110 main.py:51] epoch 3756, training loss: 9777.88, average training loss: 11721.30, base loss: 19678.52
[INFO 2017-06-27 20:48:58,492 main.py:51] epoch 3757, training loss: 10382.47, average training loss: 11719.45, base loss: 19675.22
[INFO 2017-06-27 20:48:58,864 main.py:51] epoch 3758, training loss: 10478.59, average training loss: 11718.01, base loss: 19674.37
[INFO 2017-06-27 20:48:59,342 main.py:51] epoch 3759, training loss: 13784.66, average training loss: 11720.37, base loss: 19680.33
[INFO 2017-06-27 20:48:59,731 main.py:51] epoch 3760, training loss: 9028.41, average training loss: 11717.66, base loss: 19674.92
[INFO 2017-06-27 20:49:00,111 main.py:51] epoch 3761, training loss: 11880.96, average training loss: 11718.43, base loss: 19675.93
[INFO 2017-06-27 20:49:00,490 main.py:51] epoch 3762, training loss: 10985.06, average training loss: 11717.59, base loss: 19674.94
[INFO 2017-06-27 20:49:00,888 main.py:51] epoch 3763, training loss: 10527.37, average training loss: 11716.96, base loss: 19675.34
[INFO 2017-06-27 20:49:01,275 main.py:51] epoch 3764, training loss: 12027.73, average training loss: 11716.10, base loss: 19675.18
[INFO 2017-06-27 20:49:01,653 main.py:51] epoch 3765, training loss: 12615.09, average training loss: 11717.25, base loss: 19678.55
[INFO 2017-06-27 20:49:02,030 main.py:51] epoch 3766, training loss: 12225.58, average training loss: 11718.16, base loss: 19679.80
[INFO 2017-06-27 20:49:02,406 main.py:51] epoch 3767, training loss: 12965.38, average training loss: 11719.87, base loss: 19683.72
[INFO 2017-06-27 20:49:02,785 main.py:51] epoch 3768, training loss: 11383.44, average training loss: 11719.04, base loss: 19681.96
[INFO 2017-06-27 20:49:03,162 main.py:51] epoch 3769, training loss: 10520.51, average training loss: 11716.96, base loss: 19679.79
[INFO 2017-06-27 20:49:03,540 main.py:51] epoch 3770, training loss: 11135.98, average training loss: 11716.41, base loss: 19679.51
[INFO 2017-06-27 20:49:03,917 main.py:51] epoch 3771, training loss: 11545.04, average training loss: 11715.87, base loss: 19678.07
[INFO 2017-06-27 20:49:04,297 main.py:51] epoch 3772, training loss: 11434.93, average training loss: 11715.07, base loss: 19677.96
[INFO 2017-06-27 20:49:04,674 main.py:51] epoch 3773, training loss: 12088.73, average training loss: 11715.85, base loss: 19681.25
[INFO 2017-06-27 20:49:05,054 main.py:51] epoch 3774, training loss: 11404.28, average training loss: 11716.33, base loss: 19683.04
[INFO 2017-06-27 20:49:05,432 main.py:51] epoch 3775, training loss: 11276.41, average training loss: 11716.09, base loss: 19683.43
[INFO 2017-06-27 20:49:05,811 main.py:51] epoch 3776, training loss: 11903.40, average training loss: 11715.90, base loss: 19684.61
[INFO 2017-06-27 20:49:06,263 main.py:51] epoch 3777, training loss: 10231.23, average training loss: 11715.41, base loss: 19683.97
[INFO 2017-06-27 20:49:06,640 main.py:51] epoch 3778, training loss: 11614.62, average training loss: 11714.55, base loss: 19683.18
[INFO 2017-06-27 20:49:07,038 main.py:51] epoch 3779, training loss: 13175.76, average training loss: 11715.92, base loss: 19687.38
[INFO 2017-06-27 20:49:07,418 main.py:51] epoch 3780, training loss: 11865.97, average training loss: 11717.38, base loss: 19691.08
[INFO 2017-06-27 20:49:07,795 main.py:51] epoch 3781, training loss: 10277.03, average training loss: 11716.87, base loss: 19691.89
[INFO 2017-06-27 20:49:08,169 main.py:51] epoch 3782, training loss: 11913.72, average training loss: 11715.89, base loss: 19689.99
[INFO 2017-06-27 20:49:08,569 main.py:51] epoch 3783, training loss: 12594.61, average training loss: 11717.32, base loss: 19694.65
[INFO 2017-06-27 20:49:08,945 main.py:51] epoch 3784, training loss: 12246.27, average training loss: 11717.43, base loss: 19694.44
[INFO 2017-06-27 20:49:09,322 main.py:51] epoch 3785, training loss: 12578.36, average training loss: 11718.62, base loss: 19698.27
[INFO 2017-06-27 20:49:09,699 main.py:51] epoch 3786, training loss: 11755.93, average training loss: 11719.82, base loss: 19702.17
[INFO 2017-06-27 20:49:10,075 main.py:51] epoch 3787, training loss: 10024.84, average training loss: 11718.47, base loss: 19700.51
[INFO 2017-06-27 20:49:10,454 main.py:51] epoch 3788, training loss: 14481.59, average training loss: 11720.51, base loss: 19705.80
[INFO 2017-06-27 20:49:10,829 main.py:51] epoch 3789, training loss: 11564.37, average training loss: 11719.59, base loss: 19705.09
[INFO 2017-06-27 20:49:11,202 main.py:51] epoch 3790, training loss: 10749.98, average training loss: 11718.33, base loss: 19703.61
[INFO 2017-06-27 20:49:11,576 main.py:51] epoch 3791, training loss: 10227.55, average training loss: 11716.12, base loss: 19699.41
[INFO 2017-06-27 20:49:11,952 main.py:51] epoch 3792, training loss: 12165.42, average training loss: 11715.28, base loss: 19697.56
[INFO 2017-06-27 20:49:12,327 main.py:51] epoch 3793, training loss: 12519.92, average training loss: 11717.00, base loss: 19701.22
[INFO 2017-06-27 20:49:12,698 main.py:51] epoch 3794, training loss: 12054.76, average training loss: 11718.02, base loss: 19704.61
[INFO 2017-06-27 20:49:13,074 main.py:51] epoch 3795, training loss: 11241.50, average training loss: 11718.27, base loss: 19705.71
[INFO 2017-06-27 20:49:13,453 main.py:51] epoch 3796, training loss: 10602.21, average training loss: 11715.99, base loss: 19703.08
[INFO 2017-06-27 20:49:13,829 main.py:51] epoch 3797, training loss: 11807.62, average training loss: 11715.62, base loss: 19702.13
[INFO 2017-06-27 20:49:14,206 main.py:51] epoch 3798, training loss: 12631.81, average training loss: 11716.16, base loss: 19704.27
[INFO 2017-06-27 20:49:14,591 main.py:51] epoch 3799, training loss: 10119.25, average training loss: 11714.15, base loss: 19701.82
[INFO 2017-06-27 20:49:14,591 main.py:53] epoch 3799, testing
[INFO 2017-06-27 20:49:16,312 main.py:105] average testing loss: 11669.14, base loss: 19997.22
[INFO 2017-06-27 20:49:16,312 main.py:106] improve_loss: 8328.08, improve_percent: 0.42
[INFO 2017-06-27 20:49:16,313 main.py:76] current best improved percent: 0.42
[INFO 2017-06-27 20:49:16,717 main.py:51] epoch 3800, training loss: 10232.31, average training loss: 11712.10, base loss: 19699.28
[INFO 2017-06-27 20:49:17,100 main.py:51] epoch 3801, training loss: 11520.52, average training loss: 11712.90, base loss: 19700.25
[INFO 2017-06-27 20:49:17,496 main.py:51] epoch 3802, training loss: 11834.16, average training loss: 11714.22, base loss: 19703.60
[INFO 2017-06-27 20:49:17,880 main.py:51] epoch 3803, training loss: 13722.03, average training loss: 11714.95, base loss: 19706.10
[INFO 2017-06-27 20:49:18,256 main.py:51] epoch 3804, training loss: 11754.71, average training loss: 11715.53, base loss: 19708.24
[INFO 2017-06-27 20:49:18,646 main.py:51] epoch 3805, training loss: 11197.52, average training loss: 11715.76, base loss: 19709.72
[INFO 2017-06-27 20:49:19,021 main.py:51] epoch 3806, training loss: 13626.56, average training loss: 11717.13, base loss: 19712.06
[INFO 2017-06-27 20:49:19,397 main.py:51] epoch 3807, training loss: 12342.60, average training loss: 11718.43, base loss: 19715.11
[INFO 2017-06-27 20:49:19,773 main.py:51] epoch 3808, training loss: 10738.11, average training loss: 11716.68, base loss: 19713.57
[INFO 2017-06-27 20:49:20,148 main.py:51] epoch 3809, training loss: 11347.29, average training loss: 11716.58, base loss: 19714.70
[INFO 2017-06-27 20:49:20,523 main.py:51] epoch 3810, training loss: 12043.80, average training loss: 11716.43, base loss: 19714.44
[INFO 2017-06-27 20:49:20,899 main.py:51] epoch 3811, training loss: 10588.93, average training loss: 11714.02, base loss: 19708.85
[INFO 2017-06-27 20:49:21,274 main.py:51] epoch 3812, training loss: 12605.27, average training loss: 11714.14, base loss: 19706.37
[INFO 2017-06-27 20:49:21,647 main.py:51] epoch 3813, training loss: 10802.95, average training loss: 11713.07, base loss: 19704.19
[INFO 2017-06-27 20:49:22,027 main.py:51] epoch 3814, training loss: 11283.64, average training loss: 11711.35, base loss: 19700.02
[INFO 2017-06-27 20:49:22,413 main.py:51] epoch 3815, training loss: 14068.71, average training loss: 11714.81, base loss: 19707.13
[INFO 2017-06-27 20:49:22,826 main.py:51] epoch 3816, training loss: 12260.93, average training loss: 11716.36, base loss: 19711.59
[INFO 2017-06-27 20:49:23,202 main.py:51] epoch 3817, training loss: 10723.25, average training loss: 11715.48, base loss: 19710.85
[INFO 2017-06-27 20:49:23,574 main.py:51] epoch 3818, training loss: 10420.71, average training loss: 11713.67, base loss: 19708.33
[INFO 2017-06-27 20:49:23,947 main.py:51] epoch 3819, training loss: 12236.73, average training loss: 11712.59, base loss: 19707.44
[INFO 2017-06-27 20:49:24,326 main.py:51] epoch 3820, training loss: 11664.49, average training loss: 11712.53, base loss: 19707.16
[INFO 2017-06-27 20:49:24,704 main.py:51] epoch 3821, training loss: 11795.28, average training loss: 11713.21, base loss: 19710.25
[INFO 2017-06-27 20:49:25,084 main.py:51] epoch 3822, training loss: 11070.72, average training loss: 11712.46, base loss: 19712.24
[INFO 2017-06-27 20:49:25,513 main.py:51] epoch 3823, training loss: 9727.20, average training loss: 11710.83, base loss: 19709.10
[INFO 2017-06-27 20:49:25,926 main.py:51] epoch 3824, training loss: 10991.13, average training loss: 11710.10, base loss: 19707.50
[INFO 2017-06-27 20:49:26,309 main.py:51] epoch 3825, training loss: 11812.55, average training loss: 11710.28, base loss: 19707.60
[INFO 2017-06-27 20:49:26,685 main.py:51] epoch 3826, training loss: 10829.75, average training loss: 11709.63, base loss: 19706.46
[INFO 2017-06-27 20:49:27,100 main.py:51] epoch 3827, training loss: 12419.08, average training loss: 11709.82, base loss: 19708.75
[INFO 2017-06-27 20:49:27,529 main.py:51] epoch 3828, training loss: 11215.69, average training loss: 11708.98, base loss: 19707.15
[INFO 2017-06-27 20:49:27,907 main.py:51] epoch 3829, training loss: 10019.20, average training loss: 11707.08, base loss: 19704.02
[INFO 2017-06-27 20:49:28,348 main.py:51] epoch 3830, training loss: 11333.36, average training loss: 11705.39, base loss: 19702.12
[INFO 2017-06-27 20:49:28,764 main.py:51] epoch 3831, training loss: 12147.94, average training loss: 11704.93, base loss: 19703.32
[INFO 2017-06-27 20:49:29,146 main.py:51] epoch 3832, training loss: 10731.66, average training loss: 11704.40, base loss: 19702.24
[INFO 2017-06-27 20:49:29,637 main.py:51] epoch 3833, training loss: 12659.29, average training loss: 11704.84, base loss: 19706.50
[INFO 2017-06-27 20:49:30,026 main.py:51] epoch 3834, training loss: 11152.80, average training loss: 11705.26, base loss: 19707.14
[INFO 2017-06-27 20:49:30,403 main.py:51] epoch 3835, training loss: 12318.21, average training loss: 11705.16, base loss: 19707.42
[INFO 2017-06-27 20:49:30,873 main.py:51] epoch 3836, training loss: 11255.90, average training loss: 11703.35, base loss: 19704.58
[INFO 2017-06-27 20:49:31,270 main.py:51] epoch 3837, training loss: 10267.34, average training loss: 11701.42, base loss: 19701.96
[INFO 2017-06-27 20:49:31,649 main.py:51] epoch 3838, training loss: 11745.36, average training loss: 11702.70, base loss: 19705.58
[INFO 2017-06-27 20:49:32,028 main.py:51] epoch 3839, training loss: 12565.84, average training loss: 11704.21, base loss: 19709.02
[INFO 2017-06-27 20:49:32,412 main.py:51] epoch 3840, training loss: 12373.26, average training loss: 11704.36, base loss: 19708.68
[INFO 2017-06-27 20:49:32,786 main.py:51] epoch 3841, training loss: 12038.99, average training loss: 11705.49, base loss: 19713.42
[INFO 2017-06-27 20:49:33,160 main.py:51] epoch 3842, training loss: 9686.80, average training loss: 11703.39, base loss: 19710.06
[INFO 2017-06-27 20:49:33,537 main.py:51] epoch 3843, training loss: 10607.82, average training loss: 11701.07, base loss: 19706.49
[INFO 2017-06-27 20:49:33,911 main.py:51] epoch 3844, training loss: 10864.75, average training loss: 11700.03, base loss: 19705.81
[INFO 2017-06-27 20:49:34,288 main.py:51] epoch 3845, training loss: 12204.20, average training loss: 11699.89, base loss: 19706.84
[INFO 2017-06-27 20:49:34,661 main.py:51] epoch 3846, training loss: 12503.91, average training loss: 11701.77, base loss: 19712.66
[INFO 2017-06-27 20:49:35,073 main.py:51] epoch 3847, training loss: 11864.54, average training loss: 11702.72, base loss: 19714.57
[INFO 2017-06-27 20:49:35,490 main.py:51] epoch 3848, training loss: 12295.73, average training loss: 11701.78, base loss: 19713.74
[INFO 2017-06-27 20:49:35,875 main.py:51] epoch 3849, training loss: 11114.09, average training loss: 11699.98, base loss: 19710.18
[INFO 2017-06-27 20:49:36,326 main.py:51] epoch 3850, training loss: 12486.31, average training loss: 11699.50, base loss: 19709.38
[INFO 2017-06-27 20:49:36,728 main.py:51] epoch 3851, training loss: 12308.15, average training loss: 11699.21, base loss: 19710.85
[INFO 2017-06-27 20:49:37,118 main.py:51] epoch 3852, training loss: 12099.24, average training loss: 11700.00, base loss: 19711.75
[INFO 2017-06-27 20:49:37,501 main.py:51] epoch 3853, training loss: 12058.31, average training loss: 11700.20, base loss: 19712.29
[INFO 2017-06-27 20:49:37,911 main.py:51] epoch 3854, training loss: 10913.31, average training loss: 11696.95, base loss: 19706.17
[INFO 2017-06-27 20:49:38,288 main.py:51] epoch 3855, training loss: 10992.38, average training loss: 11694.12, base loss: 19702.80
[INFO 2017-06-27 20:49:38,669 main.py:51] epoch 3856, training loss: 12306.88, average training loss: 11695.56, base loss: 19707.53
[INFO 2017-06-27 20:49:39,046 main.py:51] epoch 3857, training loss: 12020.83, average training loss: 11693.23, base loss: 19703.67
[INFO 2017-06-27 20:49:39,420 main.py:51] epoch 3858, training loss: 11462.25, average training loss: 11693.70, base loss: 19705.40
[INFO 2017-06-27 20:49:39,798 main.py:51] epoch 3859, training loss: 12805.71, average training loss: 11694.40, base loss: 19707.42
[INFO 2017-06-27 20:49:40,176 main.py:51] epoch 3860, training loss: 11804.93, average training loss: 11691.84, base loss: 19703.83
[INFO 2017-06-27 20:49:40,547 main.py:51] epoch 3861, training loss: 12210.96, average training loss: 11693.29, base loss: 19709.13
[INFO 2017-06-27 20:49:40,920 main.py:51] epoch 3862, training loss: 11654.52, average training loss: 11692.06, base loss: 19709.08
[INFO 2017-06-27 20:49:41,291 main.py:51] epoch 3863, training loss: 12631.80, average training loss: 11693.77, base loss: 19714.37
[INFO 2017-06-27 20:49:41,667 main.py:51] epoch 3864, training loss: 10793.68, average training loss: 11691.78, base loss: 19709.31
[INFO 2017-06-27 20:49:42,045 main.py:51] epoch 3865, training loss: 14422.30, average training loss: 11695.18, base loss: 19717.99
[INFO 2017-06-27 20:49:42,420 main.py:51] epoch 3866, training loss: 11999.85, average training loss: 11694.90, base loss: 19717.82
[INFO 2017-06-27 20:49:42,796 main.py:51] epoch 3867, training loss: 11771.55, average training loss: 11693.74, base loss: 19716.44
[INFO 2017-06-27 20:49:43,178 main.py:51] epoch 3868, training loss: 10375.71, average training loss: 11691.29, base loss: 19711.37
[INFO 2017-06-27 20:49:43,555 main.py:51] epoch 3869, training loss: 11827.07, average training loss: 11689.42, base loss: 19708.97
[INFO 2017-06-27 20:49:43,983 main.py:51] epoch 3870, training loss: 10903.30, average training loss: 11688.44, base loss: 19707.56
[INFO 2017-06-27 20:49:44,393 main.py:51] epoch 3871, training loss: 12690.01, average training loss: 11689.28, base loss: 19709.58
[INFO 2017-06-27 20:49:44,776 main.py:51] epoch 3872, training loss: 10795.35, average training loss: 11688.71, base loss: 19711.05
[INFO 2017-06-27 20:49:45,164 main.py:51] epoch 3873, training loss: 11867.46, average training loss: 11688.25, base loss: 19711.34
[INFO 2017-06-27 20:49:45,543 main.py:51] epoch 3874, training loss: 11336.17, average training loss: 11688.23, base loss: 19713.11
[INFO 2017-06-27 20:49:46,002 main.py:51] epoch 3875, training loss: 12446.97, average training loss: 11688.68, base loss: 19715.95
[INFO 2017-06-27 20:49:46,406 main.py:51] epoch 3876, training loss: 11693.31, average training loss: 11688.77, base loss: 19715.63
[INFO 2017-06-27 20:49:46,787 main.py:51] epoch 3877, training loss: 11734.97, average training loss: 11690.40, base loss: 19720.84
[INFO 2017-06-27 20:49:47,170 main.py:51] epoch 3878, training loss: 11025.40, average training loss: 11689.71, base loss: 19720.19
[INFO 2017-06-27 20:49:47,550 main.py:51] epoch 3879, training loss: 12294.54, average training loss: 11691.36, base loss: 19723.89
[INFO 2017-06-27 20:49:47,924 main.py:51] epoch 3880, training loss: 11204.26, average training loss: 11689.60, base loss: 19721.27
[INFO 2017-06-27 20:49:48,300 main.py:51] epoch 3881, training loss: 12911.96, average training loss: 11690.57, base loss: 19723.77
[INFO 2017-06-27 20:49:48,676 main.py:51] epoch 3882, training loss: 11124.77, average training loss: 11689.47, base loss: 19721.87
[INFO 2017-06-27 20:49:49,128 main.py:51] epoch 3883, training loss: 12278.72, average training loss: 11689.26, base loss: 19722.68
[INFO 2017-06-27 20:49:49,528 main.py:51] epoch 3884, training loss: 11326.90, average training loss: 11688.33, base loss: 19720.77
[INFO 2017-06-27 20:49:49,914 main.py:51] epoch 3885, training loss: 12240.20, average training loss: 11688.75, base loss: 19721.31
[INFO 2017-06-27 20:49:50,299 main.py:51] epoch 3886, training loss: 12458.02, average training loss: 11690.36, base loss: 19726.07
[INFO 2017-06-27 20:49:50,683 main.py:51] epoch 3887, training loss: 9036.06, average training loss: 11684.90, base loss: 19716.20
[INFO 2017-06-27 20:49:51,064 main.py:51] epoch 3888, training loss: 12555.59, average training loss: 11684.91, base loss: 19716.16
[INFO 2017-06-27 20:49:51,446 main.py:51] epoch 3889, training loss: 11513.16, average training loss: 11684.72, base loss: 19716.64
[INFO 2017-06-27 20:49:51,826 main.py:51] epoch 3890, training loss: 12718.30, average training loss: 11686.50, base loss: 19721.50
[INFO 2017-06-27 20:49:52,201 main.py:51] epoch 3891, training loss: 11551.58, average training loss: 11688.05, base loss: 19726.50
[INFO 2017-06-27 20:49:52,574 main.py:51] epoch 3892, training loss: 11126.66, average training loss: 11686.92, base loss: 19724.49
[INFO 2017-06-27 20:49:52,949 main.py:51] epoch 3893, training loss: 10919.68, average training loss: 11685.97, base loss: 19721.50
[INFO 2017-06-27 20:49:53,328 main.py:51] epoch 3894, training loss: 13138.20, average training loss: 11687.17, base loss: 19725.17
[INFO 2017-06-27 20:49:53,703 main.py:51] epoch 3895, training loss: 11658.44, average training loss: 11688.94, base loss: 19729.61
[INFO 2017-06-27 20:49:54,078 main.py:51] epoch 3896, training loss: 11551.25, average training loss: 11687.54, base loss: 19729.01
[INFO 2017-06-27 20:49:54,456 main.py:51] epoch 3897, training loss: 10958.74, average training loss: 11687.24, base loss: 19728.64
[INFO 2017-06-27 20:49:54,836 main.py:51] epoch 3898, training loss: 11219.65, average training loss: 11687.55, base loss: 19729.03
[INFO 2017-06-27 20:49:55,211 main.py:51] epoch 3899, training loss: 10568.38, average training loss: 11684.56, base loss: 19724.70
[INFO 2017-06-27 20:49:55,212 main.py:53] epoch 3899, testing
[INFO 2017-06-27 20:49:56,845 main.py:105] average testing loss: 11666.42, base loss: 19996.83
[INFO 2017-06-27 20:49:56,846 main.py:106] improve_loss: 8330.41, improve_percent: 0.42
[INFO 2017-06-27 20:49:56,846 main.py:76] current best improved percent: 0.42
[INFO 2017-06-27 20:49:57,251 main.py:51] epoch 3900, training loss: 9721.72, average training loss: 11682.99, base loss: 19722.29
[INFO 2017-06-27 20:49:57,634 main.py:51] epoch 3901, training loss: 11038.52, average training loss: 11681.72, base loss: 19718.17
[INFO 2017-06-27 20:49:58,114 main.py:51] epoch 3902, training loss: 12343.18, average training loss: 11682.92, base loss: 19721.29
[INFO 2017-06-27 20:49:58,533 main.py:51] epoch 3903, training loss: 10866.90, average training loss: 11683.45, base loss: 19722.29
[INFO 2017-06-27 20:49:58,917 main.py:51] epoch 3904, training loss: 11295.11, average training loss: 11682.96, base loss: 19721.72
[INFO 2017-06-27 20:49:59,302 main.py:51] epoch 3905, training loss: 12052.84, average training loss: 11680.84, base loss: 19717.59
[INFO 2017-06-27 20:49:59,787 main.py:51] epoch 3906, training loss: 10012.37, average training loss: 11678.53, base loss: 19712.86
[INFO 2017-06-27 20:50:00,181 main.py:51] epoch 3907, training loss: 13173.55, average training loss: 11679.74, base loss: 19716.26
[INFO 2017-06-27 20:50:00,563 main.py:51] epoch 3908, training loss: 12132.44, average training loss: 11679.45, base loss: 19717.94
[INFO 2017-06-27 20:50:00,947 main.py:51] epoch 3909, training loss: 10011.27, average training loss: 11676.68, base loss: 19714.89
[INFO 2017-06-27 20:50:01,350 main.py:51] epoch 3910, training loss: 11851.88, average training loss: 11676.54, base loss: 19714.49
[INFO 2017-06-27 20:50:01,726 main.py:51] epoch 3911, training loss: 10174.38, average training loss: 11675.51, base loss: 19714.06
[INFO 2017-06-27 20:50:02,104 main.py:51] epoch 3912, training loss: 10881.05, average training loss: 11674.96, base loss: 19712.45
[INFO 2017-06-27 20:50:02,482 main.py:51] epoch 3913, training loss: 12560.60, average training loss: 11677.06, base loss: 19716.28
[INFO 2017-06-27 20:50:02,854 main.py:51] epoch 3914, training loss: 10466.53, average training loss: 11676.33, base loss: 19714.72
[INFO 2017-06-27 20:50:03,230 main.py:51] epoch 3915, training loss: 13081.03, average training loss: 11678.19, base loss: 19718.13
[INFO 2017-06-27 20:50:03,610 main.py:51] epoch 3916, training loss: 12028.27, average training loss: 11678.14, base loss: 19718.54
[INFO 2017-06-27 20:50:03,985 main.py:51] epoch 3917, training loss: 12334.37, average training loss: 11680.19, base loss: 19722.26
[INFO 2017-06-27 20:50:04,384 main.py:51] epoch 3918, training loss: 11988.64, average training loss: 11681.15, base loss: 19724.02
[INFO 2017-06-27 20:50:04,761 main.py:51] epoch 3919, training loss: 12732.77, average training loss: 11682.92, base loss: 19728.60
[INFO 2017-06-27 20:50:05,137 main.py:51] epoch 3920, training loss: 12897.33, average training loss: 11683.40, base loss: 19729.62
[INFO 2017-06-27 20:50:05,508 main.py:51] epoch 3921, training loss: 11119.90, average training loss: 11680.88, base loss: 19724.67
[INFO 2017-06-27 20:50:05,881 main.py:51] epoch 3922, training loss: 11541.65, average training loss: 11680.45, base loss: 19725.46
[INFO 2017-06-27 20:50:06,254 main.py:51] epoch 3923, training loss: 10403.62, average training loss: 11679.40, base loss: 19723.98
[INFO 2017-06-27 20:50:06,627 main.py:51] epoch 3924, training loss: 11793.24, average training loss: 11679.01, base loss: 19724.54
[INFO 2017-06-27 20:50:07,004 main.py:51] epoch 3925, training loss: 11126.59, average training loss: 11677.75, base loss: 19722.82
[INFO 2017-06-27 20:50:07,383 main.py:51] epoch 3926, training loss: 11401.19, average training loss: 11678.22, base loss: 19724.24
[INFO 2017-06-27 20:50:07,759 main.py:51] epoch 3927, training loss: 10702.23, average training loss: 11679.05, base loss: 19726.78
[INFO 2017-06-27 20:50:08,139 main.py:51] epoch 3928, training loss: 11453.48, average training loss: 11679.13, base loss: 19728.23
[INFO 2017-06-27 20:50:08,512 main.py:51] epoch 3929, training loss: 10598.60, average training loss: 11678.33, base loss: 19727.43
[INFO 2017-06-27 20:50:08,888 main.py:51] epoch 3930, training loss: 10762.33, average training loss: 11674.79, base loss: 19720.80
[INFO 2017-06-27 20:50:09,258 main.py:51] epoch 3931, training loss: 10127.82, average training loss: 11673.24, base loss: 19717.40
[INFO 2017-06-27 20:50:09,635 main.py:51] epoch 3932, training loss: 11134.10, average training loss: 11673.24, base loss: 19717.34
[INFO 2017-06-27 20:50:10,017 main.py:51] epoch 3933, training loss: 12134.07, average training loss: 11671.92, base loss: 19717.04
[INFO 2017-06-27 20:50:10,393 main.py:51] epoch 3934, training loss: 12322.14, average training loss: 11672.57, base loss: 19718.24
[INFO 2017-06-27 20:50:10,765 main.py:51] epoch 3935, training loss: 10890.95, average training loss: 11670.06, base loss: 19713.11
[INFO 2017-06-27 20:50:11,239 main.py:51] epoch 3936, training loss: 11028.22, average training loss: 11669.83, base loss: 19713.91
[INFO 2017-06-27 20:50:11,639 main.py:51] epoch 3937, training loss: 9425.31, average training loss: 11668.51, base loss: 19710.93
[INFO 2017-06-27 20:50:12,018 main.py:51] epoch 3938, training loss: 10915.21, average training loss: 11667.69, base loss: 19709.56
[INFO 2017-06-27 20:50:12,466 main.py:51] epoch 3939, training loss: 10762.92, average training loss: 11665.56, base loss: 19707.01
[INFO 2017-06-27 20:50:12,920 main.py:51] epoch 3940, training loss: 12046.51, average training loss: 11665.27, base loss: 19706.85
[INFO 2017-06-27 20:50:13,301 main.py:51] epoch 3941, training loss: 11448.03, average training loss: 11665.96, base loss: 19709.35
[INFO 2017-06-27 20:50:13,732 main.py:51] epoch 3942, training loss: 11828.35, average training loss: 11667.61, base loss: 19713.88
[INFO 2017-06-27 20:50:14,146 main.py:51] epoch 3943, training loss: 10938.19, average training loss: 11665.85, base loss: 19709.40
[INFO 2017-06-27 20:50:14,527 main.py:51] epoch 3944, training loss: 11405.89, average training loss: 11664.00, base loss: 19706.16
[INFO 2017-06-27 20:50:14,924 main.py:51] epoch 3945, training loss: 12224.81, average training loss: 11664.92, base loss: 19709.58
[INFO 2017-06-27 20:50:15,307 main.py:51] epoch 3946, training loss: 11912.45, average training loss: 11663.09, base loss: 19705.68
[INFO 2017-06-27 20:50:15,684 main.py:51] epoch 3947, training loss: 11860.29, average training loss: 11661.57, base loss: 19703.14
[INFO 2017-06-27 20:50:16,060 main.py:51] epoch 3948, training loss: 10844.64, average training loss: 11660.17, base loss: 19701.23
[INFO 2017-06-27 20:50:16,456 main.py:51] epoch 3949, training loss: 11635.61, average training loss: 11660.95, base loss: 19704.69
[INFO 2017-06-27 20:50:16,831 main.py:51] epoch 3950, training loss: 11096.70, average training loss: 11661.31, base loss: 19706.95
[INFO 2017-06-27 20:50:17,214 main.py:51] epoch 3951, training loss: 14404.00, average training loss: 11664.65, base loss: 19714.52
[INFO 2017-06-27 20:50:17,590 main.py:51] epoch 3952, training loss: 12190.09, average training loss: 11663.78, base loss: 19713.79
[INFO 2017-06-27 20:50:17,965 main.py:51] epoch 3953, training loss: 12210.43, average training loss: 11665.49, base loss: 19717.60
[INFO 2017-06-27 20:50:18,338 main.py:51] epoch 3954, training loss: 11525.05, average training loss: 11665.36, base loss: 19718.93
[INFO 2017-06-27 20:50:18,713 main.py:51] epoch 3955, training loss: 11101.89, average training loss: 11665.09, base loss: 19718.38
[INFO 2017-06-27 20:50:19,087 main.py:51] epoch 3956, training loss: 11587.62, average training loss: 11665.52, base loss: 19719.75
[INFO 2017-06-27 20:50:19,463 main.py:51] epoch 3957, training loss: 11900.36, average training loss: 11664.34, base loss: 19716.83
[INFO 2017-06-27 20:50:19,835 main.py:51] epoch 3958, training loss: 10945.09, average training loss: 11663.66, base loss: 19716.49
[INFO 2017-06-27 20:50:20,206 main.py:51] epoch 3959, training loss: 11945.88, average training loss: 11663.46, base loss: 19717.01
[INFO 2017-06-27 20:50:20,580 main.py:51] epoch 3960, training loss: 12650.56, average training loss: 11664.72, base loss: 19720.58
[INFO 2017-06-27 20:50:20,953 main.py:51] epoch 3961, training loss: 10842.34, average training loss: 11663.40, base loss: 19717.81
[INFO 2017-06-27 20:50:21,326 main.py:51] epoch 3962, training loss: 13839.50, average training loss: 11667.40, base loss: 19726.04
[INFO 2017-06-27 20:50:21,698 main.py:51] epoch 3963, training loss: 10940.08, average training loss: 11666.76, base loss: 19725.37
[INFO 2017-06-27 20:50:22,074 main.py:51] epoch 3964, training loss: 11604.16, average training loss: 11666.68, base loss: 19725.67
[INFO 2017-06-27 20:50:22,452 main.py:51] epoch 3965, training loss: 12432.39, average training loss: 11666.89, base loss: 19726.68
[INFO 2017-06-27 20:50:22,827 main.py:51] epoch 3966, training loss: 10416.45, average training loss: 11664.88, base loss: 19724.09
[INFO 2017-06-27 20:50:23,299 main.py:51] epoch 3967, training loss: 13112.47, average training loss: 11667.87, base loss: 19731.03
[INFO 2017-06-27 20:50:23,686 main.py:51] epoch 3968, training loss: 9572.38, average training loss: 11666.62, base loss: 19729.46
[INFO 2017-06-27 20:50:24,064 main.py:51] epoch 3969, training loss: 11336.35, average training loss: 11666.85, base loss: 19729.28
[INFO 2017-06-27 20:50:24,528 main.py:51] epoch 3970, training loss: 10886.20, average training loss: 11667.66, base loss: 19731.85
[INFO 2017-06-27 20:50:24,930 main.py:51] epoch 3971, training loss: 12136.73, average training loss: 11668.79, base loss: 19735.58
[INFO 2017-06-27 20:50:25,317 main.py:51] epoch 3972, training loss: 12072.85, average training loss: 11669.16, base loss: 19737.39
[INFO 2017-06-27 20:50:25,700 main.py:51] epoch 3973, training loss: 11183.53, average training loss: 11668.68, base loss: 19737.96
[INFO 2017-06-27 20:50:26,081 main.py:51] epoch 3974, training loss: 12613.37, average training loss: 11671.90, base loss: 19745.01
[INFO 2017-06-27 20:50:26,450 main.py:51] epoch 3975, training loss: 11619.60, average training loss: 11672.36, base loss: 19747.64
[INFO 2017-06-27 20:50:26,827 main.py:51] epoch 3976, training loss: 11451.24, average training loss: 11671.23, base loss: 19745.83
[INFO 2017-06-27 20:50:27,202 main.py:51] epoch 3977, training loss: 10450.19, average training loss: 11669.50, base loss: 19743.41
[INFO 2017-06-27 20:50:27,604 main.py:51] epoch 3978, training loss: 11354.65, average training loss: 11667.78, base loss: 19739.63
[INFO 2017-06-27 20:50:27,982 main.py:51] epoch 3979, training loss: 11001.21, average training loss: 11667.51, base loss: 19739.54
[INFO 2017-06-27 20:50:28,361 main.py:51] epoch 3980, training loss: 12139.14, average training loss: 11666.61, base loss: 19738.00
[INFO 2017-06-27 20:50:28,737 main.py:51] epoch 3981, training loss: 10842.27, average training loss: 11666.12, base loss: 19737.84
[INFO 2017-06-27 20:50:29,112 main.py:51] epoch 3982, training loss: 10870.06, average training loss: 11666.37, base loss: 19737.69
[INFO 2017-06-27 20:50:29,487 main.py:51] epoch 3983, training loss: 11416.27, average training loss: 11665.86, base loss: 19737.17
[INFO 2017-06-27 20:50:29,863 main.py:51] epoch 3984, training loss: 12010.44, average training loss: 11666.68, base loss: 19737.92
[INFO 2017-06-27 20:50:30,241 main.py:51] epoch 3985, training loss: 10972.37, average training loss: 11666.54, base loss: 19738.64
[INFO 2017-06-27 20:50:30,620 main.py:51] epoch 3986, training loss: 11008.16, average training loss: 11665.67, base loss: 19737.05
[INFO 2017-06-27 20:50:30,998 main.py:51] epoch 3987, training loss: 11092.81, average training loss: 11664.36, base loss: 19735.84
[INFO 2017-06-27 20:50:31,376 main.py:51] epoch 3988, training loss: 11826.56, average training loss: 11665.55, base loss: 19737.43
[INFO 2017-06-27 20:50:31,819 main.py:51] epoch 3989, training loss: 11305.91, average training loss: 11665.00, base loss: 19737.15
[INFO 2017-06-27 20:50:32,208 main.py:51] epoch 3990, training loss: 11343.74, average training loss: 11664.96, base loss: 19738.43
[INFO 2017-06-27 20:50:32,588 main.py:51] epoch 3991, training loss: 12450.94, average training loss: 11664.68, base loss: 19738.82
[INFO 2017-06-27 20:50:32,986 main.py:51] epoch 3992, training loss: 11974.50, average training loss: 11665.11, base loss: 19741.86
[INFO 2017-06-27 20:50:33,390 main.py:51] epoch 3993, training loss: 11388.58, average training loss: 11664.88, base loss: 19742.41
[INFO 2017-06-27 20:50:33,766 main.py:51] epoch 3994, training loss: 10339.49, average training loss: 11663.37, base loss: 19739.02
[INFO 2017-06-27 20:50:34,141 main.py:51] epoch 3995, training loss: 10859.91, average training loss: 11663.00, base loss: 19737.93
[INFO 2017-06-27 20:50:34,522 main.py:51] epoch 3996, training loss: 12726.60, average training loss: 11665.46, base loss: 19745.07
[INFO 2017-06-27 20:50:34,902 main.py:51] epoch 3997, training loss: 12776.33, average training loss: 11666.89, base loss: 19748.21
[INFO 2017-06-27 20:50:35,278 main.py:51] epoch 3998, training loss: 11353.72, average training loss: 11663.83, base loss: 19743.33
[INFO 2017-06-27 20:50:35,655 main.py:51] epoch 3999, training loss: 11041.49, average training loss: 11663.21, base loss: 19742.66
[INFO 2017-06-27 20:50:35,655 main.py:53] epoch 3999, testing
[INFO 2017-06-27 20:50:37,492 main.py:105] average testing loss: 11820.47, base loss: 20706.76
[INFO 2017-06-27 20:50:37,492 main.py:106] improve_loss: 8886.29, improve_percent: 0.43
[INFO 2017-06-27 20:50:37,492 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:50:37,506 main.py:76] current best improved percent: 0.43
[INFO 2017-06-27 20:50:37,886 main.py:51] epoch 4000, training loss: 11878.30, average training loss: 11663.28, base loss: 19742.68
[INFO 2017-06-27 20:50:38,261 main.py:51] epoch 4001, training loss: 10777.58, average training loss: 11662.85, base loss: 19742.66
[INFO 2017-06-27 20:50:38,631 main.py:51] epoch 4002, training loss: 10768.12, average training loss: 11661.59, base loss: 19738.71
[INFO 2017-06-27 20:50:39,074 main.py:51] epoch 4003, training loss: 11171.60, average training loss: 11659.77, base loss: 19735.67
[INFO 2017-06-27 20:50:39,472 main.py:51] epoch 4004, training loss: 11545.79, average training loss: 11659.74, base loss: 19737.07
[INFO 2017-06-27 20:50:39,855 main.py:51] epoch 4005, training loss: 11335.33, average training loss: 11660.58, base loss: 19739.20
[INFO 2017-06-27 20:50:40,244 main.py:51] epoch 4006, training loss: 12127.71, average training loss: 11660.53, base loss: 19739.65
[INFO 2017-06-27 20:50:40,628 main.py:51] epoch 4007, training loss: 11902.50, average training loss: 11661.45, base loss: 19743.74
[INFO 2017-06-27 20:50:41,013 main.py:51] epoch 4008, training loss: 11388.56, average training loss: 11661.06, base loss: 19743.72
[INFO 2017-06-27 20:50:41,391 main.py:51] epoch 4009, training loss: 11465.66, average training loss: 11660.58, base loss: 19742.06
[INFO 2017-06-27 20:50:41,767 main.py:51] epoch 4010, training loss: 11562.71, average training loss: 11661.42, base loss: 19744.52
[INFO 2017-06-27 20:50:42,155 main.py:51] epoch 4011, training loss: 11178.79, average training loss: 11658.38, base loss: 19737.27
[INFO 2017-06-27 20:50:42,538 main.py:51] epoch 4012, training loss: 12365.54, average training loss: 11657.07, base loss: 19735.68
[INFO 2017-06-27 20:50:42,917 main.py:51] epoch 4013, training loss: 11518.14, average training loss: 11656.59, base loss: 19736.36
[INFO 2017-06-27 20:50:43,298 main.py:51] epoch 4014, training loss: 10537.06, average training loss: 11656.51, base loss: 19737.52
[INFO 2017-06-27 20:50:43,695 main.py:51] epoch 4015, training loss: 11196.64, average training loss: 11654.96, base loss: 19735.24
[INFO 2017-06-27 20:50:44,090 main.py:51] epoch 4016, training loss: 11490.83, average training loss: 11654.06, base loss: 19734.71
[INFO 2017-06-27 20:50:44,475 main.py:51] epoch 4017, training loss: 10554.30, average training loss: 11653.48, base loss: 19734.70
[INFO 2017-06-27 20:50:44,862 main.py:51] epoch 4018, training loss: 11331.74, average training loss: 11653.54, base loss: 19735.82
[INFO 2017-06-27 20:50:45,251 main.py:51] epoch 4019, training loss: 11292.92, average training loss: 11652.38, base loss: 19734.94
[INFO 2017-06-27 20:50:45,636 main.py:51] epoch 4020, training loss: 11826.29, average training loss: 11654.09, base loss: 19739.24
[INFO 2017-06-27 20:50:46,023 main.py:51] epoch 4021, training loss: 11359.55, average training loss: 11652.37, base loss: 19738.16
[INFO 2017-06-27 20:50:46,445 main.py:51] epoch 4022, training loss: 10969.25, average training loss: 11652.62, base loss: 19739.78
[INFO 2017-06-27 20:50:46,898 main.py:51] epoch 4023, training loss: 11908.07, average training loss: 11653.03, base loss: 19741.45
[INFO 2017-06-27 20:50:47,339 main.py:51] epoch 4024, training loss: 10451.33, average training loss: 11649.27, base loss: 19733.01
[INFO 2017-06-27 20:50:47,798 main.py:51] epoch 4025, training loss: 10385.31, average training loss: 11648.57, base loss: 19731.81
[INFO 2017-06-27 20:50:48,245 main.py:51] epoch 4026, training loss: 11897.52, average training loss: 11650.37, base loss: 19736.29
[INFO 2017-06-27 20:50:48,649 main.py:51] epoch 4027, training loss: 11140.75, average training loss: 11650.64, base loss: 19736.38
[INFO 2017-06-27 20:50:49,066 main.py:51] epoch 4028, training loss: 12578.32, average training loss: 11650.66, base loss: 19736.97
[INFO 2017-06-27 20:50:49,517 main.py:51] epoch 4029, training loss: 13121.35, average training loss: 11650.95, base loss: 19738.50
[INFO 2017-06-27 20:50:49,985 main.py:51] epoch 4030, training loss: 11577.65, average training loss: 11650.48, base loss: 19738.02
[INFO 2017-06-27 20:50:50,373 main.py:51] epoch 4031, training loss: 12945.94, average training loss: 11651.28, base loss: 19739.59
[INFO 2017-06-27 20:50:50,754 main.py:51] epoch 4032, training loss: 11799.11, average training loss: 11652.49, base loss: 19743.46
[INFO 2017-06-27 20:50:51,133 main.py:51] epoch 4033, training loss: 12621.45, average training loss: 11653.11, base loss: 19745.19
[INFO 2017-06-27 20:50:51,512 main.py:51] epoch 4034, training loss: 12147.81, average training loss: 11652.79, base loss: 19745.74
[INFO 2017-06-27 20:50:51,921 main.py:51] epoch 4035, training loss: 12245.64, average training loss: 11655.88, base loss: 19752.79
[INFO 2017-06-27 20:50:52,296 main.py:51] epoch 4036, training loss: 12228.13, average training loss: 11656.90, base loss: 19755.26
[INFO 2017-06-27 20:50:52,674 main.py:51] epoch 4037, training loss: 11965.07, average training loss: 11656.62, base loss: 19755.00
[INFO 2017-06-27 20:50:53,052 main.py:51] epoch 4038, training loss: 10330.97, average training loss: 11654.60, base loss: 19752.45
[INFO 2017-06-27 20:50:53,429 main.py:51] epoch 4039, training loss: 12362.20, average training loss: 11654.50, base loss: 19754.04
[INFO 2017-06-27 20:50:53,807 main.py:51] epoch 4040, training loss: 13725.93, average training loss: 11656.44, base loss: 19756.68
[INFO 2017-06-27 20:50:54,184 main.py:51] epoch 4041, training loss: 10812.90, average training loss: 11653.90, base loss: 19750.06
[INFO 2017-06-27 20:50:54,562 main.py:51] epoch 4042, training loss: 11403.02, average training loss: 11653.05, base loss: 19749.52
[INFO 2017-06-27 20:50:54,941 main.py:51] epoch 4043, training loss: 11742.71, average training loss: 11653.92, base loss: 19750.07
[INFO 2017-06-27 20:50:55,314 main.py:51] epoch 4044, training loss: 10786.90, average training loss: 11651.79, base loss: 19745.74
[INFO 2017-06-27 20:50:55,691 main.py:51] epoch 4045, training loss: 11373.60, average training loss: 11652.03, base loss: 19746.46
[INFO 2017-06-27 20:50:56,066 main.py:51] epoch 4046, training loss: 10968.01, average training loss: 11652.32, base loss: 19747.90
[INFO 2017-06-27 20:50:56,442 main.py:51] epoch 4047, training loss: 12290.33, average training loss: 11653.83, base loss: 19751.04
[INFO 2017-06-27 20:50:56,818 main.py:51] epoch 4048, training loss: 10566.65, average training loss: 11652.34, base loss: 19747.06
[INFO 2017-06-27 20:50:57,193 main.py:51] epoch 4049, training loss: 11127.24, average training loss: 11652.92, base loss: 19748.97
[INFO 2017-06-27 20:50:57,567 main.py:51] epoch 4050, training loss: 12619.76, average training loss: 11653.31, base loss: 19749.88
[INFO 2017-06-27 20:50:57,945 main.py:51] epoch 4051, training loss: 11200.69, average training loss: 11650.79, base loss: 19745.17
[INFO 2017-06-27 20:50:58,322 main.py:51] epoch 4052, training loss: 11836.41, average training loss: 11651.85, base loss: 19749.14
[INFO 2017-06-27 20:50:58,697 main.py:51] epoch 4053, training loss: 11573.59, average training loss: 11652.23, base loss: 19750.35
[INFO 2017-06-27 20:50:59,073 main.py:51] epoch 4054, training loss: 11666.51, average training loss: 11651.91, base loss: 19750.35
[INFO 2017-06-27 20:50:59,450 main.py:51] epoch 4055, training loss: 11365.00, average training loss: 11651.65, base loss: 19749.46
[INFO 2017-06-27 20:50:59,828 main.py:51] epoch 4056, training loss: 11683.43, average training loss: 11649.62, base loss: 19747.30
[INFO 2017-06-27 20:51:00,211 main.py:51] epoch 4057, training loss: 11168.84, average training loss: 11648.15, base loss: 19745.54
[INFO 2017-06-27 20:51:00,587 main.py:51] epoch 4058, training loss: 11420.20, average training loss: 11648.03, base loss: 19747.31
[INFO 2017-06-27 20:51:00,958 main.py:51] epoch 4059, training loss: 11678.81, average training loss: 11648.95, base loss: 19750.66
[INFO 2017-06-27 20:51:01,330 main.py:51] epoch 4060, training loss: 11364.93, average training loss: 11649.41, base loss: 19752.29
[INFO 2017-06-27 20:51:01,708 main.py:51] epoch 4061, training loss: 12014.93, average training loss: 11649.60, base loss: 19753.46
[INFO 2017-06-27 20:51:02,079 main.py:51] epoch 4062, training loss: 12342.39, average training loss: 11649.13, base loss: 19751.60
[INFO 2017-06-27 20:51:02,456 main.py:51] epoch 4063, training loss: 11433.23, average training loss: 11647.51, base loss: 19748.86
[INFO 2017-06-27 20:51:02,832 main.py:51] epoch 4064, training loss: 10912.88, average training loss: 11647.65, base loss: 19751.12
[INFO 2017-06-27 20:51:03,204 main.py:51] epoch 4065, training loss: 9890.66, average training loss: 11645.43, base loss: 19747.49
[INFO 2017-06-27 20:51:03,577 main.py:51] epoch 4066, training loss: 12387.48, average training loss: 11645.46, base loss: 19749.28
[INFO 2017-06-27 20:51:03,950 main.py:51] epoch 4067, training loss: 11453.38, average training loss: 11644.63, base loss: 19748.06
[INFO 2017-06-27 20:51:04,326 main.py:51] epoch 4068, training loss: 11457.40, average training loss: 11645.27, base loss: 19749.12
[INFO 2017-06-27 20:51:04,701 main.py:51] epoch 4069, training loss: 11822.32, average training loss: 11643.90, base loss: 19746.59
[INFO 2017-06-27 20:51:05,077 main.py:51] epoch 4070, training loss: 11059.62, average training loss: 11643.43, base loss: 19745.12
[INFO 2017-06-27 20:51:05,448 main.py:51] epoch 4071, training loss: 11418.52, average training loss: 11643.74, base loss: 19747.09
[INFO 2017-06-27 20:51:05,824 main.py:51] epoch 4072, training loss: 11682.81, average training loss: 11643.66, base loss: 19746.53
[INFO 2017-06-27 20:51:06,194 main.py:51] epoch 4073, training loss: 11329.24, average training loss: 11644.05, base loss: 19749.39
[INFO 2017-06-27 20:51:06,566 main.py:51] epoch 4074, training loss: 11466.15, average training loss: 11644.56, base loss: 19751.45
[INFO 2017-06-27 20:51:06,945 main.py:51] epoch 4075, training loss: 9959.61, average training loss: 11643.85, base loss: 19750.93
[INFO 2017-06-27 20:51:07,318 main.py:51] epoch 4076, training loss: 11142.35, average training loss: 11641.12, base loss: 19745.00
[INFO 2017-06-27 20:51:07,689 main.py:51] epoch 4077, training loss: 10152.45, average training loss: 11637.97, base loss: 19739.21
[INFO 2017-06-27 20:51:08,060 main.py:51] epoch 4078, training loss: 13177.19, average training loss: 11638.67, base loss: 19741.72
[INFO 2017-06-27 20:51:08,435 main.py:51] epoch 4079, training loss: 10607.83, average training loss: 11638.25, base loss: 19740.87
[INFO 2017-06-27 20:51:08,809 main.py:51] epoch 4080, training loss: 12231.95, average training loss: 11639.67, base loss: 19743.18
[INFO 2017-06-27 20:51:09,183 main.py:51] epoch 4081, training loss: 11232.82, average training loss: 11638.75, base loss: 19743.10
[INFO 2017-06-27 20:51:09,556 main.py:51] epoch 4082, training loss: 13406.53, average training loss: 11641.01, base loss: 19748.12
[INFO 2017-06-27 20:51:09,928 main.py:51] epoch 4083, training loss: 11288.49, average training loss: 11640.80, base loss: 19746.92
[INFO 2017-06-27 20:51:10,308 main.py:51] epoch 4084, training loss: 11042.46, average training loss: 11640.69, base loss: 19744.45
[INFO 2017-06-27 20:51:10,686 main.py:51] epoch 4085, training loss: 11985.83, average training loss: 11638.16, base loss: 19742.13
[INFO 2017-06-27 20:51:11,057 main.py:51] epoch 4086, training loss: 11299.49, average training loss: 11636.76, base loss: 19740.22
[INFO 2017-06-27 20:51:11,431 main.py:51] epoch 4087, training loss: 11894.66, average training loss: 11638.13, base loss: 19744.55
[INFO 2017-06-27 20:51:11,804 main.py:51] epoch 4088, training loss: 11115.14, average training loss: 11637.28, base loss: 19743.23
[INFO 2017-06-27 20:51:12,179 main.py:51] epoch 4089, training loss: 13198.09, average training loss: 11639.04, base loss: 19747.48
[INFO 2017-06-27 20:51:12,555 main.py:51] epoch 4090, training loss: 12214.10, average training loss: 11638.82, base loss: 19746.22
[INFO 2017-06-27 20:51:12,930 main.py:51] epoch 4091, training loss: 9924.13, average training loss: 11635.05, base loss: 19739.05
[INFO 2017-06-27 20:51:13,306 main.py:51] epoch 4092, training loss: 11692.15, average training loss: 11634.29, base loss: 19738.48
[INFO 2017-06-27 20:51:13,678 main.py:51] epoch 4093, training loss: 10074.33, average training loss: 11634.48, base loss: 19739.38
[INFO 2017-06-27 20:51:14,054 main.py:51] epoch 4094, training loss: 11299.05, average training loss: 11634.41, base loss: 19741.26
[INFO 2017-06-27 20:51:14,425 main.py:51] epoch 4095, training loss: 11123.68, average training loss: 11633.44, base loss: 19739.74
[INFO 2017-06-27 20:51:14,797 main.py:51] epoch 4096, training loss: 10994.61, average training loss: 11631.74, base loss: 19736.24
[INFO 2017-06-27 20:51:15,174 main.py:51] epoch 4097, training loss: 10740.00, average training loss: 11631.21, base loss: 19733.72
[INFO 2017-06-27 20:51:15,549 main.py:51] epoch 4098, training loss: 12102.76, average training loss: 11631.29, base loss: 19733.44
[INFO 2017-06-27 20:51:15,927 main.py:51] epoch 4099, training loss: 11671.90, average training loss: 11630.24, base loss: 19732.50
[INFO 2017-06-27 20:51:15,928 main.py:53] epoch 4099, testing
[INFO 2017-06-27 20:51:17,635 main.py:105] average testing loss: 11431.53, base loss: 19560.44
[INFO 2017-06-27 20:51:17,636 main.py:106] improve_loss: 8128.90, improve_percent: 0.42
[INFO 2017-06-27 20:51:17,636 main.py:76] current best improved percent: 0.43
[INFO 2017-06-27 20:51:18,015 main.py:51] epoch 4100, training loss: 10201.07, average training loss: 11628.73, base loss: 19729.69
[INFO 2017-06-27 20:51:18,451 main.py:51] epoch 4101, training loss: 12828.90, average training loss: 11631.06, base loss: 19733.85
[INFO 2017-06-27 20:51:18,878 main.py:51] epoch 4102, training loss: 12655.82, average training loss: 11633.45, base loss: 19739.89
[INFO 2017-06-27 20:51:19,266 main.py:51] epoch 4103, training loss: 12745.18, average training loss: 11634.42, base loss: 19742.41
[INFO 2017-06-27 20:51:19,648 main.py:51] epoch 4104, training loss: 10524.36, average training loss: 11633.68, base loss: 19741.65
[INFO 2017-06-27 20:51:20,026 main.py:51] epoch 4105, training loss: 11580.09, average training loss: 11633.22, base loss: 19740.06
[INFO 2017-06-27 20:51:20,405 main.py:51] epoch 4106, training loss: 11513.27, average training loss: 11634.00, base loss: 19744.63
[INFO 2017-06-27 20:51:20,837 main.py:51] epoch 4107, training loss: 11283.76, average training loss: 11634.35, base loss: 19746.50
[INFO 2017-06-27 20:51:21,235 main.py:51] epoch 4108, training loss: 10981.83, average training loss: 11633.47, base loss: 19745.99
[INFO 2017-06-27 20:51:21,619 main.py:51] epoch 4109, training loss: 9642.83, average training loss: 11630.86, base loss: 19740.45
[INFO 2017-06-27 20:51:22,000 main.py:51] epoch 4110, training loss: 10840.62, average training loss: 11629.73, base loss: 19740.94
[INFO 2017-06-27 20:51:22,377 main.py:51] epoch 4111, training loss: 10837.23, average training loss: 11629.54, base loss: 19740.04
[INFO 2017-06-27 20:51:22,831 main.py:51] epoch 4112, training loss: 14698.93, average training loss: 11633.64, base loss: 19749.49
[INFO 2017-06-27 20:51:23,228 main.py:51] epoch 4113, training loss: 10657.24, average training loss: 11631.58, base loss: 19746.31
[INFO 2017-06-27 20:51:23,609 main.py:51] epoch 4114, training loss: 12284.48, average training loss: 11634.00, base loss: 19752.82
[INFO 2017-06-27 20:51:23,985 main.py:51] epoch 4115, training loss: 10446.67, average training loss: 11633.06, base loss: 19751.71
[INFO 2017-06-27 20:51:24,419 main.py:51] epoch 4116, training loss: 11192.70, average training loss: 11633.89, base loss: 19753.00
[INFO 2017-06-27 20:51:24,835 main.py:51] epoch 4117, training loss: 10077.83, average training loss: 11632.91, base loss: 19750.97
[INFO 2017-06-27 20:51:25,217 main.py:51] epoch 4118, training loss: 13434.75, average training loss: 11633.84, base loss: 19753.18
[INFO 2017-06-27 20:51:25,605 main.py:51] epoch 4119, training loss: 11699.78, average training loss: 11634.14, base loss: 19755.63
[INFO 2017-06-27 20:51:26,015 main.py:51] epoch 4120, training loss: 10218.35, average training loss: 11630.84, base loss: 19748.89
[INFO 2017-06-27 20:51:26,401 main.py:51] epoch 4121, training loss: 11756.29, average training loss: 11629.04, base loss: 19746.47
[INFO 2017-06-27 20:51:26,779 main.py:51] epoch 4122, training loss: 11878.76, average training loss: 11630.02, base loss: 19748.96
[INFO 2017-06-27 20:51:27,159 main.py:51] epoch 4123, training loss: 10212.20, average training loss: 11629.47, base loss: 19749.27
[INFO 2017-06-27 20:51:27,535 main.py:51] epoch 4124, training loss: 11455.48, average training loss: 11628.10, base loss: 19747.80
[INFO 2017-06-27 20:51:27,909 main.py:51] epoch 4125, training loss: 10783.93, average training loss: 11626.05, base loss: 19743.27
[INFO 2017-06-27 20:51:28,379 main.py:51] epoch 4126, training loss: 11233.98, average training loss: 11624.58, base loss: 19740.65
[INFO 2017-06-27 20:51:28,772 main.py:51] epoch 4127, training loss: 11137.78, average training loss: 11625.24, base loss: 19742.26
[INFO 2017-06-27 20:51:29,149 main.py:51] epoch 4128, training loss: 10823.57, average training loss: 11623.65, base loss: 19739.58
[INFO 2017-06-27 20:51:29,636 main.py:51] epoch 4129, training loss: 12742.13, average training loss: 11626.51, base loss: 19745.60
[INFO 2017-06-27 20:51:30,030 main.py:51] epoch 4130, training loss: 11483.93, average training loss: 11626.64, base loss: 19747.29
[INFO 2017-06-27 20:51:30,499 main.py:51] epoch 4131, training loss: 12490.20, average training loss: 11627.31, base loss: 19749.87
[INFO 2017-06-27 20:51:30,943 main.py:51] epoch 4132, training loss: 11849.96, average training loss: 11627.47, base loss: 19749.75
[INFO 2017-06-27 20:51:31,409 main.py:51] epoch 4133, training loss: 12147.77, average training loss: 11627.02, base loss: 19749.01
[INFO 2017-06-27 20:51:31,811 main.py:51] epoch 4134, training loss: 11866.19, average training loss: 11627.89, base loss: 19752.79
[INFO 2017-06-27 20:51:32,218 main.py:51] epoch 4135, training loss: 11056.49, average training loss: 11628.15, base loss: 19752.48
[INFO 2017-06-27 20:51:32,622 main.py:51] epoch 4136, training loss: 12110.43, average training loss: 11628.66, base loss: 19755.37
[INFO 2017-06-27 20:51:33,007 main.py:51] epoch 4137, training loss: 12001.50, average training loss: 11627.79, base loss: 19754.22
[INFO 2017-06-27 20:51:33,392 main.py:51] epoch 4138, training loss: 12893.52, average training loss: 11629.64, base loss: 19757.08
[INFO 2017-06-27 20:51:33,877 main.py:51] epoch 4139, training loss: 10001.83, average training loss: 11627.69, base loss: 19752.80
[INFO 2017-06-27 20:51:34,265 main.py:51] epoch 4140, training loss: 12820.09, average training loss: 11626.93, base loss: 19753.36
[INFO 2017-06-27 20:51:34,705 main.py:51] epoch 4141, training loss: 11653.15, average training loss: 11628.15, base loss: 19756.27
[INFO 2017-06-27 20:51:35,167 main.py:51] epoch 4142, training loss: 10609.11, average training loss: 11627.24, base loss: 19754.38
[INFO 2017-06-27 20:51:35,548 main.py:51] epoch 4143, training loss: 11508.58, average training loss: 11625.88, base loss: 19752.61
[INFO 2017-06-27 20:51:35,927 main.py:51] epoch 4144, training loss: 12695.50, average training loss: 11627.94, base loss: 19757.30
[INFO 2017-06-27 20:51:36,305 main.py:51] epoch 4145, training loss: 11829.83, average training loss: 11628.79, base loss: 19761.34
[INFO 2017-06-27 20:51:36,759 main.py:51] epoch 4146, training loss: 13317.10, average training loss: 11629.73, base loss: 19764.28
[INFO 2017-06-27 20:51:37,160 main.py:51] epoch 4147, training loss: 10886.85, average training loss: 11630.18, base loss: 19767.34
[INFO 2017-06-27 20:51:37,539 main.py:51] epoch 4148, training loss: 13876.39, average training loss: 11632.14, base loss: 19771.59
[INFO 2017-06-27 20:51:37,917 main.py:51] epoch 4149, training loss: 11525.74, average training loss: 11632.05, base loss: 19771.46
[INFO 2017-06-27 20:51:38,299 main.py:51] epoch 4150, training loss: 12337.12, average training loss: 11631.77, base loss: 19770.94
[INFO 2017-06-27 20:51:38,699 main.py:51] epoch 4151, training loss: 11776.35, average training loss: 11631.38, base loss: 19770.59
[INFO 2017-06-27 20:51:39,103 main.py:51] epoch 4152, training loss: 12654.61, average training loss: 11632.39, base loss: 19771.95
[INFO 2017-06-27 20:51:39,489 main.py:51] epoch 4153, training loss: 9986.66, average training loss: 11631.20, base loss: 19769.03
[INFO 2017-06-27 20:51:39,884 main.py:51] epoch 4154, training loss: 12292.54, average training loss: 11632.37, base loss: 19771.98
[INFO 2017-06-27 20:51:40,288 main.py:51] epoch 4155, training loss: 11340.32, average training loss: 11631.87, base loss: 19772.45
[INFO 2017-06-27 20:51:40,670 main.py:51] epoch 4156, training loss: 11607.54, average training loss: 11631.56, base loss: 19771.44
[INFO 2017-06-27 20:51:41,054 main.py:51] epoch 4157, training loss: 11128.02, average training loss: 11630.24, base loss: 19768.13
[INFO 2017-06-27 20:51:41,520 main.py:51] epoch 4158, training loss: 12501.18, average training loss: 11632.13, base loss: 19770.76
[INFO 2017-06-27 20:51:41,920 main.py:51] epoch 4159, training loss: 11755.57, average training loss: 11630.43, base loss: 19768.22
[INFO 2017-06-27 20:51:42,298 main.py:51] epoch 4160, training loss: 11300.68, average training loss: 11629.18, base loss: 19765.78
[INFO 2017-06-27 20:51:42,678 main.py:51] epoch 4161, training loss: 12277.31, average training loss: 11629.36, base loss: 19767.24
[INFO 2017-06-27 20:51:43,152 main.py:51] epoch 4162, training loss: 11475.81, average training loss: 11628.81, base loss: 19767.37
[INFO 2017-06-27 20:51:43,530 main.py:51] epoch 4163, training loss: 12683.04, average training loss: 11630.16, base loss: 19770.02
[INFO 2017-06-27 20:51:43,914 main.py:51] epoch 4164, training loss: 11007.12, average training loss: 11630.97, base loss: 19772.40
[INFO 2017-06-27 20:51:44,299 main.py:51] epoch 4165, training loss: 11513.68, average training loss: 11630.23, base loss: 19771.19
[INFO 2017-06-27 20:51:44,674 main.py:51] epoch 4166, training loss: 12060.78, average training loss: 11629.50, base loss: 19770.07
[INFO 2017-06-27 20:51:45,053 main.py:51] epoch 4167, training loss: 10624.61, average training loss: 11627.88, base loss: 19768.28
[INFO 2017-06-27 20:51:45,515 main.py:51] epoch 4168, training loss: 9371.20, average training loss: 11625.09, base loss: 19764.58
[INFO 2017-06-27 20:51:45,922 main.py:51] epoch 4169, training loss: 12306.17, average training loss: 11624.51, base loss: 19763.13
[INFO 2017-06-27 20:51:46,304 main.py:51] epoch 4170, training loss: 12441.85, average training loss: 11626.04, base loss: 19767.79
[INFO 2017-06-27 20:51:46,683 main.py:51] epoch 4171, training loss: 11104.56, average training loss: 11625.55, base loss: 19767.06
[INFO 2017-06-27 20:51:47,168 main.py:51] epoch 4172, training loss: 10820.81, average training loss: 11625.57, base loss: 19766.67
[INFO 2017-06-27 20:51:47,549 main.py:51] epoch 4173, training loss: 11598.86, average training loss: 11626.58, base loss: 19768.57
[INFO 2017-06-27 20:51:47,928 main.py:51] epoch 4174, training loss: 10895.15, average training loss: 11625.94, base loss: 19768.59
[INFO 2017-06-27 20:51:48,308 main.py:51] epoch 4175, training loss: 12892.77, average training loss: 11627.18, base loss: 19771.01
[INFO 2017-06-27 20:51:48,722 main.py:51] epoch 4176, training loss: 13572.80, average training loss: 11629.65, base loss: 19776.69
[INFO 2017-06-27 20:51:49,142 main.py:51] epoch 4177, training loss: 12651.23, average training loss: 11628.72, base loss: 19775.10
[INFO 2017-06-27 20:51:49,521 main.py:51] epoch 4178, training loss: 10764.17, average training loss: 11628.07, base loss: 19773.28
[INFO 2017-06-27 20:51:49,906 main.py:51] epoch 4179, training loss: 10955.72, average training loss: 11627.83, base loss: 19774.48
[INFO 2017-06-27 20:51:50,321 main.py:51] epoch 4180, training loss: 12862.37, average training loss: 11628.37, base loss: 19776.05
[INFO 2017-06-27 20:51:50,753 main.py:51] epoch 4181, training loss: 12977.15, average training loss: 11630.43, base loss: 19779.73
[INFO 2017-06-27 20:51:51,133 main.py:51] epoch 4182, training loss: 9758.16, average training loss: 11627.70, base loss: 19775.21
[INFO 2017-06-27 20:51:51,523 main.py:51] epoch 4183, training loss: 12146.70, average training loss: 11627.80, base loss: 19776.46
[INFO 2017-06-27 20:51:52,007 main.py:51] epoch 4184, training loss: 10424.31, average training loss: 11628.06, base loss: 19777.93
[INFO 2017-06-27 20:51:52,393 main.py:51] epoch 4185, training loss: 12457.26, average training loss: 11630.33, base loss: 19780.88
[INFO 2017-06-27 20:51:52,781 main.py:51] epoch 4186, training loss: 9347.42, average training loss: 11627.32, base loss: 19775.83
[INFO 2017-06-27 20:51:53,192 main.py:51] epoch 4187, training loss: 10937.74, average training loss: 11625.55, base loss: 19772.78
[INFO 2017-06-27 20:51:53,571 main.py:51] epoch 4188, training loss: 11249.25, average training loss: 11623.67, base loss: 19769.57
[INFO 2017-06-27 20:51:53,952 main.py:51] epoch 4189, training loss: 11115.72, average training loss: 11623.89, base loss: 19769.83
[INFO 2017-06-27 20:51:54,435 main.py:51] epoch 4190, training loss: 11628.17, average training loss: 11625.63, base loss: 19773.65
[INFO 2017-06-27 20:51:54,815 main.py:51] epoch 4191, training loss: 11651.23, average training loss: 11625.68, base loss: 19775.80
[INFO 2017-06-27 20:51:55,290 main.py:51] epoch 4192, training loss: 11171.21, average training loss: 11625.18, base loss: 19777.24
[INFO 2017-06-27 20:51:55,704 main.py:51] epoch 4193, training loss: 9881.35, average training loss: 11621.85, base loss: 19769.92
[INFO 2017-06-27 20:51:56,090 main.py:51] epoch 4194, training loss: 10515.37, average training loss: 11621.35, base loss: 19768.84
[INFO 2017-06-27 20:51:56,501 main.py:51] epoch 4195, training loss: 11060.79, average training loss: 11619.91, base loss: 19767.06
[INFO 2017-06-27 20:51:56,911 main.py:51] epoch 4196, training loss: 11897.05, average training loss: 11621.40, base loss: 19770.38
[INFO 2017-06-27 20:51:57,289 main.py:51] epoch 4197, training loss: 12614.81, average training loss: 11623.41, base loss: 19775.34
[INFO 2017-06-27 20:51:57,669 main.py:51] epoch 4198, training loss: 11038.66, average training loss: 11622.63, base loss: 19772.83
[INFO 2017-06-27 20:51:58,053 main.py:51] epoch 4199, training loss: 11766.22, average training loss: 11622.84, base loss: 19773.97
[INFO 2017-06-27 20:51:58,053 main.py:53] epoch 4199, testing
[INFO 2017-06-27 20:51:59,732 main.py:105] average testing loss: 11718.87, base loss: 20790.50
[INFO 2017-06-27 20:51:59,733 main.py:106] improve_loss: 9071.63, improve_percent: 0.44
[INFO 2017-06-27 20:51:59,733 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:51:59,746 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:52:00,127 main.py:51] epoch 4200, training loss: 11368.22, average training loss: 11621.89, base loss: 19772.76
[INFO 2017-06-27 20:52:00,511 main.py:51] epoch 4201, training loss: 11149.79, average training loss: 11621.16, base loss: 19771.93
[INFO 2017-06-27 20:52:00,885 main.py:51] epoch 4202, training loss: 12657.74, average training loss: 11622.83, base loss: 19775.81
[INFO 2017-06-27 20:52:01,261 main.py:51] epoch 4203, training loss: 11576.81, average training loss: 11622.16, base loss: 19774.60
[INFO 2017-06-27 20:52:01,639 main.py:51] epoch 4204, training loss: 9763.22, average training loss: 11620.71, base loss: 19772.69
[INFO 2017-06-27 20:52:02,015 main.py:51] epoch 4205, training loss: 11119.31, average training loss: 11619.62, base loss: 19772.01
[INFO 2017-06-27 20:52:02,395 main.py:51] epoch 4206, training loss: 11250.84, average training loss: 11620.00, base loss: 19773.82
[INFO 2017-06-27 20:52:02,773 main.py:51] epoch 4207, training loss: 11437.21, average training loss: 11619.71, base loss: 19773.94
[INFO 2017-06-27 20:52:03,185 main.py:51] epoch 4208, training loss: 11803.64, average training loss: 11620.89, base loss: 19777.13
[INFO 2017-06-27 20:52:03,565 main.py:51] epoch 4209, training loss: 11183.23, average training loss: 11619.32, base loss: 19774.52
[INFO 2017-06-27 20:52:03,942 main.py:51] epoch 4210, training loss: 10225.28, average training loss: 11618.91, base loss: 19775.20
[INFO 2017-06-27 20:52:04,325 main.py:51] epoch 4211, training loss: 11049.47, average training loss: 11619.20, base loss: 19776.36
[INFO 2017-06-27 20:52:04,702 main.py:51] epoch 4212, training loss: 10172.17, average training loss: 11618.45, base loss: 19775.39
[INFO 2017-06-27 20:52:05,076 main.py:51] epoch 4213, training loss: 11586.53, average training loss: 11617.24, base loss: 19775.25
[INFO 2017-06-27 20:52:05,452 main.py:51] epoch 4214, training loss: 11477.40, average training loss: 11617.33, base loss: 19778.30
[INFO 2017-06-27 20:52:05,837 main.py:51] epoch 4215, training loss: 10011.96, average training loss: 11614.45, base loss: 19773.32
[INFO 2017-06-27 20:52:06,215 main.py:51] epoch 4216, training loss: 12330.60, average training loss: 11615.72, base loss: 19777.06
[INFO 2017-06-27 20:52:06,600 main.py:51] epoch 4217, training loss: 11039.35, average training loss: 11614.39, base loss: 19774.64
[INFO 2017-06-27 20:52:06,979 main.py:51] epoch 4218, training loss: 10874.13, average training loss: 11613.03, base loss: 19773.14
[INFO 2017-06-27 20:52:07,362 main.py:51] epoch 4219, training loss: 11930.68, average training loss: 11613.32, base loss: 19774.07
[INFO 2017-06-27 20:52:07,740 main.py:51] epoch 4220, training loss: 10404.90, average training loss: 11611.18, base loss: 19769.81
[INFO 2017-06-27 20:52:08,122 main.py:51] epoch 4221, training loss: 11693.48, average training loss: 11612.03, base loss: 19771.20
[INFO 2017-06-27 20:52:08,500 main.py:51] epoch 4222, training loss: 11482.95, average training loss: 11613.45, base loss: 19776.06
[INFO 2017-06-27 20:52:08,877 main.py:51] epoch 4223, training loss: 13034.34, average training loss: 11615.32, base loss: 19780.25
[INFO 2017-06-27 20:52:09,262 main.py:51] epoch 4224, training loss: 10681.80, average training loss: 11614.83, base loss: 19779.56
[INFO 2017-06-27 20:52:09,647 main.py:51] epoch 4225, training loss: 11142.62, average training loss: 11615.00, base loss: 19780.64
[INFO 2017-06-27 20:52:10,026 main.py:51] epoch 4226, training loss: 12031.23, average training loss: 11614.94, base loss: 19782.14
[INFO 2017-06-27 20:52:10,408 main.py:51] epoch 4227, training loss: 14832.06, average training loss: 11616.05, base loss: 19784.57
[INFO 2017-06-27 20:52:10,786 main.py:51] epoch 4228, training loss: 12344.40, average training loss: 11618.67, base loss: 19790.68
[INFO 2017-06-27 20:52:11,164 main.py:51] epoch 4229, training loss: 9856.65, average training loss: 11615.73, base loss: 19785.84
[INFO 2017-06-27 20:52:11,543 main.py:51] epoch 4230, training loss: 12089.40, average training loss: 11617.02, base loss: 19790.12
[INFO 2017-06-27 20:52:11,917 main.py:51] epoch 4231, training loss: 10669.38, average training loss: 11616.80, base loss: 19789.37
[INFO 2017-06-27 20:52:12,294 main.py:51] epoch 4232, training loss: 9483.88, average training loss: 11613.72, base loss: 19783.72
[INFO 2017-06-27 20:52:12,676 main.py:51] epoch 4233, training loss: 11865.38, average training loss: 11613.56, base loss: 19784.66
[INFO 2017-06-27 20:52:13,051 main.py:51] epoch 4234, training loss: 10406.16, average training loss: 11613.59, base loss: 19783.27
[INFO 2017-06-27 20:52:13,427 main.py:51] epoch 4235, training loss: 13026.15, average training loss: 11615.22, base loss: 19786.75
[INFO 2017-06-27 20:52:13,803 main.py:51] epoch 4236, training loss: 11749.07, average training loss: 11615.54, base loss: 19787.49
[INFO 2017-06-27 20:52:14,180 main.py:51] epoch 4237, training loss: 10685.61, average training loss: 11615.13, base loss: 19786.80
[INFO 2017-06-27 20:52:14,559 main.py:51] epoch 4238, training loss: 11852.92, average training loss: 11614.25, base loss: 19784.99
[INFO 2017-06-27 20:52:14,934 main.py:51] epoch 4239, training loss: 11246.68, average training loss: 11613.00, base loss: 19781.58
[INFO 2017-06-27 20:52:15,309 main.py:51] epoch 4240, training loss: 11924.71, average training loss: 11612.83, base loss: 19780.70
[INFO 2017-06-27 20:52:15,684 main.py:51] epoch 4241, training loss: 10031.06, average training loss: 11611.63, base loss: 19777.58
[INFO 2017-06-27 20:52:16,059 main.py:51] epoch 4242, training loss: 11620.68, average training loss: 11613.35, base loss: 19781.31
[INFO 2017-06-27 20:52:16,432 main.py:51] epoch 4243, training loss: 11878.11, average training loss: 11614.47, base loss: 19783.53
[INFO 2017-06-27 20:52:16,811 main.py:51] epoch 4244, training loss: 13659.54, average training loss: 11616.84, base loss: 19788.56
[INFO 2017-06-27 20:52:17,188 main.py:51] epoch 4245, training loss: 12607.31, average training loss: 11617.53, base loss: 19789.29
[INFO 2017-06-27 20:52:17,572 main.py:51] epoch 4246, training loss: 11437.16, average training loss: 11616.53, base loss: 19789.17
[INFO 2017-06-27 20:52:17,949 main.py:51] epoch 4247, training loss: 11273.13, average training loss: 11616.24, base loss: 19787.80
[INFO 2017-06-27 20:52:18,327 main.py:51] epoch 4248, training loss: 10117.21, average training loss: 11614.24, base loss: 19784.84
[INFO 2017-06-27 20:52:18,702 main.py:51] epoch 4249, training loss: 11203.21, average training loss: 11613.10, base loss: 19783.26
[INFO 2017-06-27 20:52:19,080 main.py:51] epoch 4250, training loss: 11431.11, average training loss: 11613.81, base loss: 19785.73
[INFO 2017-06-27 20:52:19,458 main.py:51] epoch 4251, training loss: 9936.41, average training loss: 11611.74, base loss: 19781.03
[INFO 2017-06-27 20:52:19,830 main.py:51] epoch 4252, training loss: 10585.54, average training loss: 11611.14, base loss: 19781.23
[INFO 2017-06-27 20:52:20,210 main.py:51] epoch 4253, training loss: 10947.49, average training loss: 11610.54, base loss: 19780.36
[INFO 2017-06-27 20:52:20,582 main.py:51] epoch 4254, training loss: 12051.70, average training loss: 11611.56, base loss: 19784.19
[INFO 2017-06-27 20:52:20,964 main.py:51] epoch 4255, training loss: 11602.17, average training loss: 11612.01, base loss: 19785.80
[INFO 2017-06-27 20:52:21,340 main.py:51] epoch 4256, training loss: 11908.69, average training loss: 11611.90, base loss: 19786.20
[INFO 2017-06-27 20:52:21,723 main.py:51] epoch 4257, training loss: 11235.94, average training loss: 11611.14, base loss: 19784.93
[INFO 2017-06-27 20:52:22,099 main.py:51] epoch 4258, training loss: 12264.26, average training loss: 11611.88, base loss: 19787.22
[INFO 2017-06-27 20:52:22,484 main.py:51] epoch 4259, training loss: 12853.92, average training loss: 11613.46, base loss: 19791.18
[INFO 2017-06-27 20:52:22,860 main.py:51] epoch 4260, training loss: 10821.00, average training loss: 11612.28, base loss: 19789.88
[INFO 2017-06-27 20:52:23,236 main.py:51] epoch 4261, training loss: 10927.61, average training loss: 11612.14, base loss: 19789.62
[INFO 2017-06-27 20:52:23,616 main.py:51] epoch 4262, training loss: 13254.23, average training loss: 11613.73, base loss: 19792.48
[INFO 2017-06-27 20:52:23,989 main.py:51] epoch 4263, training loss: 11888.00, average training loss: 11612.12, base loss: 19790.06
[INFO 2017-06-27 20:52:24,365 main.py:51] epoch 4264, training loss: 10441.30, average training loss: 11610.74, base loss: 19787.86
[INFO 2017-06-27 20:52:24,744 main.py:51] epoch 4265, training loss: 9752.48, average training loss: 11609.31, base loss: 19785.00
[INFO 2017-06-27 20:52:25,119 main.py:51] epoch 4266, training loss: 10642.52, average training loss: 11608.62, base loss: 19784.10
[INFO 2017-06-27 20:52:25,494 main.py:51] epoch 4267, training loss: 12806.50, average training loss: 11610.06, base loss: 19788.15
[INFO 2017-06-27 20:52:25,871 main.py:51] epoch 4268, training loss: 12256.98, average training loss: 11610.51, base loss: 19789.21
[INFO 2017-06-27 20:52:26,246 main.py:51] epoch 4269, training loss: 12268.53, average training loss: 11611.57, base loss: 19791.93
[INFO 2017-06-27 20:52:26,621 main.py:51] epoch 4270, training loss: 11876.49, average training loss: 11610.50, base loss: 19788.98
[INFO 2017-06-27 20:52:27,005 main.py:51] epoch 4271, training loss: 12072.69, average training loss: 11610.83, base loss: 19789.16
[INFO 2017-06-27 20:52:27,385 main.py:51] epoch 4272, training loss: 11595.71, average training loss: 11610.85, base loss: 19790.43
[INFO 2017-06-27 20:52:27,760 main.py:51] epoch 4273, training loss: 11366.38, average training loss: 11610.04, base loss: 19788.23
[INFO 2017-06-27 20:52:28,141 main.py:51] epoch 4274, training loss: 12208.89, average training loss: 11611.23, base loss: 19791.14
[INFO 2017-06-27 20:52:28,524 main.py:51] epoch 4275, training loss: 11330.65, average training loss: 11609.54, base loss: 19787.44
[INFO 2017-06-27 20:52:28,908 main.py:51] epoch 4276, training loss: 11640.73, average training loss: 11609.19, base loss: 19786.66
[INFO 2017-06-27 20:52:29,286 main.py:51] epoch 4277, training loss: 11007.37, average training loss: 11608.50, base loss: 19785.53
[INFO 2017-06-27 20:52:29,667 main.py:51] epoch 4278, training loss: 11466.46, average training loss: 11607.91, base loss: 19783.65
[INFO 2017-06-27 20:52:30,044 main.py:51] epoch 4279, training loss: 13062.04, average training loss: 11609.40, base loss: 19784.81
[INFO 2017-06-27 20:52:30,425 main.py:51] epoch 4280, training loss: 9902.28, average training loss: 11608.56, base loss: 19784.25
[INFO 2017-06-27 20:52:30,806 main.py:51] epoch 4281, training loss: 12012.01, average training loss: 11609.35, base loss: 19788.90
[INFO 2017-06-27 20:52:31,189 main.py:51] epoch 4282, training loss: 11440.17, average training loss: 11608.75, base loss: 19787.91
[INFO 2017-06-27 20:52:31,566 main.py:51] epoch 4283, training loss: 11456.54, average training loss: 11609.73, base loss: 19790.98
[INFO 2017-06-27 20:52:31,948 main.py:51] epoch 4284, training loss: 11543.76, average training loss: 11608.98, base loss: 19789.71
[INFO 2017-06-27 20:52:32,327 main.py:51] epoch 4285, training loss: 11304.02, average training loss: 11610.01, base loss: 19792.75
[INFO 2017-06-27 20:52:32,700 main.py:51] epoch 4286, training loss: 10447.14, average training loss: 11607.63, base loss: 19788.49
[INFO 2017-06-27 20:52:33,071 main.py:51] epoch 4287, training loss: 11229.21, average training loss: 11607.26, base loss: 19787.60
[INFO 2017-06-27 20:52:33,446 main.py:51] epoch 4288, training loss: 11868.41, average training loss: 11607.47, base loss: 19788.51
[INFO 2017-06-27 20:52:33,822 main.py:51] epoch 4289, training loss: 12750.30, average training loss: 11609.87, base loss: 19792.78
[INFO 2017-06-27 20:52:34,219 main.py:51] epoch 4290, training loss: 11611.60, average training loss: 11609.94, base loss: 19793.45
[INFO 2017-06-27 20:52:34,597 main.py:51] epoch 4291, training loss: 11071.34, average training loss: 11608.54, base loss: 19791.19
[INFO 2017-06-27 20:52:34,973 main.py:51] epoch 4292, training loss: 13207.51, average training loss: 11608.95, base loss: 19791.37
[INFO 2017-06-27 20:52:35,344 main.py:51] epoch 4293, training loss: 12310.86, average training loss: 11610.29, base loss: 19794.03
[INFO 2017-06-27 20:52:35,721 main.py:51] epoch 4294, training loss: 11699.19, average training loss: 11610.08, base loss: 19793.99
[INFO 2017-06-27 20:52:36,101 main.py:51] epoch 4295, training loss: 10014.56, average training loss: 11607.76, base loss: 19790.08
[INFO 2017-06-27 20:52:36,486 main.py:51] epoch 4296, training loss: 10825.25, average training loss: 11607.07, base loss: 19789.31
[INFO 2017-06-27 20:52:36,870 main.py:51] epoch 4297, training loss: 12844.47, average training loss: 11609.32, base loss: 19795.50
[INFO 2017-06-27 20:52:37,246 main.py:51] epoch 4298, training loss: 10245.89, average training loss: 11607.32, base loss: 19792.75
[INFO 2017-06-27 20:52:37,621 main.py:51] epoch 4299, training loss: 11984.68, average training loss: 11607.65, base loss: 19793.26
[INFO 2017-06-27 20:52:37,621 main.py:53] epoch 4299, testing
[INFO 2017-06-27 20:52:39,232 main.py:105] average testing loss: 11347.62, base loss: 19377.94
[INFO 2017-06-27 20:52:39,233 main.py:106] improve_loss: 8030.32, improve_percent: 0.41
[INFO 2017-06-27 20:52:39,233 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:52:39,605 main.py:51] epoch 4300, training loss: 9613.29, average training loss: 11604.88, base loss: 19788.30
[INFO 2017-06-27 20:52:39,978 main.py:51] epoch 4301, training loss: 12324.76, average training loss: 11607.02, base loss: 19792.81
[INFO 2017-06-27 20:52:40,351 main.py:51] epoch 4302, training loss: 11695.49, average training loss: 11607.07, base loss: 19793.13
[INFO 2017-06-27 20:52:40,726 main.py:51] epoch 4303, training loss: 11850.70, average training loss: 11606.71, base loss: 19792.00
[INFO 2017-06-27 20:52:41,102 main.py:51] epoch 4304, training loss: 11313.73, average training loss: 11606.40, base loss: 19792.96
[INFO 2017-06-27 20:52:41,483 main.py:51] epoch 4305, training loss: 12492.51, average training loss: 11607.09, base loss: 19795.09
[INFO 2017-06-27 20:52:41,857 main.py:51] epoch 4306, training loss: 11261.28, average training loss: 11607.98, base loss: 19798.41
[INFO 2017-06-27 20:52:42,236 main.py:51] epoch 4307, training loss: 10927.17, average training loss: 11606.71, base loss: 19796.03
[INFO 2017-06-27 20:52:42,611 main.py:51] epoch 4308, training loss: 11916.07, average training loss: 11606.82, base loss: 19796.16
[INFO 2017-06-27 20:52:42,982 main.py:51] epoch 4309, training loss: 11239.55, average training loss: 11606.94, base loss: 19797.78
[INFO 2017-06-27 20:52:43,360 main.py:51] epoch 4310, training loss: 10794.38, average training loss: 11604.83, base loss: 19794.17
[INFO 2017-06-27 20:52:43,741 main.py:51] epoch 4311, training loss: 11884.35, average training loss: 11604.89, base loss: 19794.26
[INFO 2017-06-27 20:52:44,125 main.py:51] epoch 4312, training loss: 11093.96, average training loss: 11604.53, base loss: 19793.36
[INFO 2017-06-27 20:52:44,504 main.py:51] epoch 4313, training loss: 10216.70, average training loss: 11603.59, base loss: 19793.05
[INFO 2017-06-27 20:52:44,879 main.py:51] epoch 4314, training loss: 9672.91, average training loss: 11600.25, base loss: 19786.16
[INFO 2017-06-27 20:52:45,354 main.py:51] epoch 4315, training loss: 11813.54, average training loss: 11600.55, base loss: 19786.21
[INFO 2017-06-27 20:52:45,747 main.py:51] epoch 4316, training loss: 10468.66, average training loss: 11600.24, base loss: 19785.48
[INFO 2017-06-27 20:52:46,124 main.py:51] epoch 4317, training loss: 11276.06, average training loss: 11599.75, base loss: 19783.74
[INFO 2017-06-27 20:52:46,543 main.py:51] epoch 4318, training loss: 11826.99, average training loss: 11600.50, base loss: 19785.83
[INFO 2017-06-27 20:52:46,962 main.py:51] epoch 4319, training loss: 11582.24, average training loss: 11599.71, base loss: 19784.26
[INFO 2017-06-27 20:52:47,347 main.py:51] epoch 4320, training loss: 10931.80, average training loss: 11598.49, base loss: 19782.49
[INFO 2017-06-27 20:52:47,742 main.py:51] epoch 4321, training loss: 11677.40, average training loss: 11597.80, base loss: 19780.74
[INFO 2017-06-27 20:52:48,126 main.py:51] epoch 4322, training loss: 12991.03, average training loss: 11598.67, base loss: 19782.46
[INFO 2017-06-27 20:52:48,561 main.py:51] epoch 4323, training loss: 11407.08, average training loss: 11598.23, base loss: 19780.41
[INFO 2017-06-27 20:52:48,959 main.py:51] epoch 4324, training loss: 11090.53, average training loss: 11597.13, base loss: 19780.86
[INFO 2017-06-27 20:52:49,344 main.py:51] epoch 4325, training loss: 12500.54, average training loss: 11598.35, base loss: 19782.29
[INFO 2017-06-27 20:52:49,725 main.py:51] epoch 4326, training loss: 11852.43, average training loss: 11597.10, base loss: 19780.08
[INFO 2017-06-27 20:52:50,151 main.py:51] epoch 4327, training loss: 11179.70, average training loss: 11595.86, base loss: 19779.12
[INFO 2017-06-27 20:52:50,562 main.py:51] epoch 4328, training loss: 10750.39, average training loss: 11593.46, base loss: 19774.49
[INFO 2017-06-27 20:52:50,948 main.py:51] epoch 4329, training loss: 12451.44, average training loss: 11593.54, base loss: 19775.46
[INFO 2017-06-27 20:52:51,445 main.py:51] epoch 4330, training loss: 13621.02, average training loss: 11595.20, base loss: 19778.70
[INFO 2017-06-27 20:52:51,831 main.py:51] epoch 4331, training loss: 11258.77, average training loss: 11593.52, base loss: 19775.47
[INFO 2017-06-27 20:52:52,215 main.py:51] epoch 4332, training loss: 10917.22, average training loss: 11593.01, base loss: 19775.22
[INFO 2017-06-27 20:52:52,594 main.py:51] epoch 4333, training loss: 11192.20, average training loss: 11592.76, base loss: 19775.13
[INFO 2017-06-27 20:52:52,982 main.py:51] epoch 4334, training loss: 10376.88, average training loss: 11590.57, base loss: 19772.72
[INFO 2017-06-27 20:52:53,359 main.py:51] epoch 4335, training loss: 12140.16, average training loss: 11590.45, base loss: 19774.68
[INFO 2017-06-27 20:52:53,824 main.py:51] epoch 4336, training loss: 11994.59, average training loss: 11591.74, base loss: 19777.69
[INFO 2017-06-27 20:52:54,244 main.py:51] epoch 4337, training loss: 10957.07, average training loss: 11591.20, base loss: 19776.97
[INFO 2017-06-27 20:52:54,621 main.py:51] epoch 4338, training loss: 11274.25, average training loss: 11589.75, base loss: 19775.01
[INFO 2017-06-27 20:52:55,002 main.py:51] epoch 4339, training loss: 11452.33, average training loss: 11590.02, base loss: 19777.06
[INFO 2017-06-27 20:52:55,385 main.py:51] epoch 4340, training loss: 11342.07, average training loss: 11589.43, base loss: 19776.57
[INFO 2017-06-27 20:52:55,859 main.py:51] epoch 4341, training loss: 12137.08, average training loss: 11590.19, base loss: 19779.91
[INFO 2017-06-27 20:52:56,254 main.py:51] epoch 4342, training loss: 12549.82, average training loss: 11590.58, base loss: 19781.79
[INFO 2017-06-27 20:52:56,641 main.py:51] epoch 4343, training loss: 11444.93, average training loss: 11589.58, base loss: 19779.97
[INFO 2017-06-27 20:52:57,026 main.py:51] epoch 4344, training loss: 11278.96, average training loss: 11589.24, base loss: 19780.95
[INFO 2017-06-27 20:52:57,505 main.py:51] epoch 4345, training loss: 11847.28, average training loss: 11589.79, base loss: 19784.49
[INFO 2017-06-27 20:52:57,905 main.py:51] epoch 4346, training loss: 11538.56, average training loss: 11588.55, base loss: 19783.39
[INFO 2017-06-27 20:52:58,288 main.py:51] epoch 4347, training loss: 11042.74, average training loss: 11587.21, base loss: 19781.33
[INFO 2017-06-27 20:52:58,764 main.py:51] epoch 4348, training loss: 11265.21, average training loss: 11587.39, base loss: 19780.71
[INFO 2017-06-27 20:52:59,171 main.py:51] epoch 4349, training loss: 12259.09, average training loss: 11586.62, base loss: 19779.61
[INFO 2017-06-27 20:52:59,550 main.py:51] epoch 4350, training loss: 11631.42, average training loss: 11586.71, base loss: 19780.07
[INFO 2017-06-27 20:52:59,926 main.py:51] epoch 4351, training loss: 10919.88, average training loss: 11585.34, base loss: 19778.84
[INFO 2017-06-27 20:53:00,309 main.py:51] epoch 4352, training loss: 11043.08, average training loss: 11585.24, base loss: 19778.25
[INFO 2017-06-27 20:53:00,687 main.py:51] epoch 4353, training loss: 10849.49, average training loss: 11584.55, base loss: 19777.17
[INFO 2017-06-27 20:53:01,062 main.py:51] epoch 4354, training loss: 11588.77, average training loss: 11584.74, base loss: 19776.06
[INFO 2017-06-27 20:53:01,435 main.py:51] epoch 4355, training loss: 12321.62, average training loss: 11585.81, base loss: 19779.23
[INFO 2017-06-27 20:53:01,858 main.py:51] epoch 4356, training loss: 10875.79, average training loss: 11586.74, base loss: 19780.81
[INFO 2017-06-27 20:53:02,288 main.py:51] epoch 4357, training loss: 10978.41, average training loss: 11584.93, base loss: 19778.35
[INFO 2017-06-27 20:53:02,675 main.py:51] epoch 4358, training loss: 12877.34, average training loss: 11586.68, base loss: 19782.03
[INFO 2017-06-27 20:53:03,051 main.py:51] epoch 4359, training loss: 11772.51, average training loss: 11586.68, base loss: 19782.81
[INFO 2017-06-27 20:53:03,508 main.py:51] epoch 4360, training loss: 10832.42, average training loss: 11585.10, base loss: 19781.83
[INFO 2017-06-27 20:53:03,909 main.py:51] epoch 4361, training loss: 11654.59, average training loss: 11581.55, base loss: 19775.36
[INFO 2017-06-27 20:53:04,313 main.py:51] epoch 4362, training loss: 10943.79, average training loss: 11580.00, base loss: 19774.20
[INFO 2017-06-27 20:53:04,783 main.py:51] epoch 4363, training loss: 10812.89, average training loss: 11577.91, base loss: 19771.18
[INFO 2017-06-27 20:53:05,166 main.py:51] epoch 4364, training loss: 10310.70, average training loss: 11576.55, base loss: 19768.48
[INFO 2017-06-27 20:53:05,569 main.py:51] epoch 4365, training loss: 13053.00, average training loss: 11577.25, base loss: 19770.80
[INFO 2017-06-27 20:53:05,948 main.py:51] epoch 4366, training loss: 10165.57, average training loss: 11575.07, base loss: 19766.25
[INFO 2017-06-27 20:53:06,323 main.py:51] epoch 4367, training loss: 11241.46, average training loss: 11575.29, base loss: 19766.05
[INFO 2017-06-27 20:53:06,705 main.py:51] epoch 4368, training loss: 11400.74, average training loss: 11574.49, base loss: 19765.66
[INFO 2017-06-27 20:53:07,082 main.py:51] epoch 4369, training loss: 10719.88, average training loss: 11573.64, base loss: 19764.63
[INFO 2017-06-27 20:53:07,457 main.py:51] epoch 4370, training loss: 11452.63, average training loss: 11573.70, base loss: 19764.97
[INFO 2017-06-27 20:53:07,834 main.py:51] epoch 4371, training loss: 11842.58, average training loss: 11573.88, base loss: 19765.41
[INFO 2017-06-27 20:53:08,210 main.py:51] epoch 4372, training loss: 10639.67, average training loss: 11573.35, base loss: 19765.04
[INFO 2017-06-27 20:53:08,590 main.py:51] epoch 4373, training loss: 13459.31, average training loss: 11574.73, base loss: 19767.09
[INFO 2017-06-27 20:53:08,966 main.py:51] epoch 4374, training loss: 12085.88, average training loss: 11574.76, base loss: 19767.95
[INFO 2017-06-27 20:53:09,343 main.py:51] epoch 4375, training loss: 11344.49, average training loss: 11574.06, base loss: 19767.48
[INFO 2017-06-27 20:53:09,842 main.py:51] epoch 4376, training loss: 11157.03, average training loss: 11571.76, base loss: 19764.25
[INFO 2017-06-27 20:53:10,219 main.py:51] epoch 4377, training loss: 11529.21, average training loss: 11571.77, base loss: 19765.78
[INFO 2017-06-27 20:53:10,598 main.py:51] epoch 4378, training loss: 12585.47, average training loss: 11572.82, base loss: 19770.51
[INFO 2017-06-27 20:53:10,977 main.py:51] epoch 4379, training loss: 13348.78, average training loss: 11575.29, base loss: 19777.81
[INFO 2017-06-27 20:53:11,462 main.py:51] epoch 4380, training loss: 11457.76, average training loss: 11573.98, base loss: 19775.75
[INFO 2017-06-27 20:53:11,842 main.py:51] epoch 4381, training loss: 11689.41, average training loss: 11571.62, base loss: 19771.86
[INFO 2017-06-27 20:53:12,221 main.py:51] epoch 4382, training loss: 10795.37, average training loss: 11569.40, base loss: 19767.11
[INFO 2017-06-27 20:53:12,608 main.py:51] epoch 4383, training loss: 11186.10, average training loss: 11569.02, base loss: 19765.85
[INFO 2017-06-27 20:53:12,985 main.py:51] epoch 4384, training loss: 10194.43, average training loss: 11566.57, base loss: 19761.55
[INFO 2017-06-27 20:53:13,403 main.py:51] epoch 4385, training loss: 12129.71, average training loss: 11566.40, base loss: 19760.67
[INFO 2017-06-27 20:53:13,816 main.py:51] epoch 4386, training loss: 10835.73, average training loss: 11566.61, base loss: 19762.42
[INFO 2017-06-27 20:53:14,198 main.py:51] epoch 4387, training loss: 11630.60, average training loss: 11566.20, base loss: 19761.06
[INFO 2017-06-27 20:53:14,622 main.py:51] epoch 4388, training loss: 11957.64, average training loss: 11567.13, base loss: 19764.41
[INFO 2017-06-27 20:53:15,086 main.py:51] epoch 4389, training loss: 11681.55, average training loss: 11567.17, base loss: 19765.78
[INFO 2017-06-27 20:53:15,491 main.py:51] epoch 4390, training loss: 12235.30, average training loss: 11568.90, base loss: 19769.98
[INFO 2017-06-27 20:53:15,930 main.py:51] epoch 4391, training loss: 11408.12, average training loss: 11567.67, base loss: 19769.53
[INFO 2017-06-27 20:53:16,311 main.py:51] epoch 4392, training loss: 10364.55, average training loss: 11566.41, base loss: 19769.14
[INFO 2017-06-27 20:53:16,765 main.py:51] epoch 4393, training loss: 12082.16, average training loss: 11567.16, base loss: 19771.59
[INFO 2017-06-27 20:53:17,167 main.py:51] epoch 4394, training loss: 10099.75, average training loss: 11563.90, base loss: 19766.10
[INFO 2017-06-27 20:53:17,560 main.py:51] epoch 4395, training loss: 11237.42, average training loss: 11563.64, base loss: 19765.97
[INFO 2017-06-27 20:53:18,031 main.py:51] epoch 4396, training loss: 11133.14, average training loss: 11563.36, base loss: 19765.60
[INFO 2017-06-27 20:53:18,426 main.py:51] epoch 4397, training loss: 10669.87, average training loss: 11562.18, base loss: 19764.59
[INFO 2017-06-27 20:53:18,804 main.py:51] epoch 4398, training loss: 12069.71, average training loss: 11561.51, base loss: 19763.70
[INFO 2017-06-27 20:53:19,262 main.py:51] epoch 4399, training loss: 10759.28, average training loss: 11562.12, base loss: 19764.91
[INFO 2017-06-27 20:53:19,263 main.py:53] epoch 4399, testing
[INFO 2017-06-27 20:53:21,085 main.py:105] average testing loss: 11696.98, base loss: 20436.54
[INFO 2017-06-27 20:53:21,085 main.py:106] improve_loss: 8739.56, improve_percent: 0.43
[INFO 2017-06-27 20:53:21,085 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:53:21,560 main.py:51] epoch 4400, training loss: 10523.64, average training loss: 11561.08, base loss: 19762.61
[INFO 2017-06-27 20:53:21,950 main.py:51] epoch 4401, training loss: 12276.66, average training loss: 11561.37, base loss: 19764.27
[INFO 2017-06-27 20:53:22,331 main.py:51] epoch 4402, training loss: 11150.95, average training loss: 11562.23, base loss: 19765.93
[INFO 2017-06-27 20:53:22,710 main.py:51] epoch 4403, training loss: 11956.62, average training loss: 11561.82, base loss: 19766.08
[INFO 2017-06-27 20:53:23,090 main.py:51] epoch 4404, training loss: 10420.72, average training loss: 11561.18, base loss: 19765.48
[INFO 2017-06-27 20:53:23,463 main.py:51] epoch 4405, training loss: 12141.42, average training loss: 11563.73, base loss: 19772.05
[INFO 2017-06-27 20:53:23,836 main.py:51] epoch 4406, training loss: 12232.54, average training loss: 11564.05, base loss: 19772.99
[INFO 2017-06-27 20:53:24,210 main.py:51] epoch 4407, training loss: 11160.70, average training loss: 11564.02, base loss: 19772.83
[INFO 2017-06-27 20:53:24,594 main.py:51] epoch 4408, training loss: 11888.95, average training loss: 11563.69, base loss: 19773.69
[INFO 2017-06-27 20:53:24,970 main.py:51] epoch 4409, training loss: 12381.36, average training loss: 11566.08, base loss: 19778.06
[INFO 2017-06-27 20:53:25,349 main.py:51] epoch 4410, training loss: 10710.89, average training loss: 11565.58, base loss: 19776.92
[INFO 2017-06-27 20:53:25,724 main.py:51] epoch 4411, training loss: 11956.03, average training loss: 11566.40, base loss: 19780.25
[INFO 2017-06-27 20:53:26,098 main.py:51] epoch 4412, training loss: 12739.08, average training loss: 11565.95, base loss: 19779.93
[INFO 2017-06-27 20:53:26,479 main.py:51] epoch 4413, training loss: 12332.10, average training loss: 11566.72, base loss: 19781.69
[INFO 2017-06-27 20:53:26,928 main.py:51] epoch 4414, training loss: 11434.44, average training loss: 11566.17, base loss: 19781.42
[INFO 2017-06-27 20:53:27,329 main.py:51] epoch 4415, training loss: 13462.93, average training loss: 11568.73, base loss: 19786.38
[INFO 2017-06-27 20:53:27,711 main.py:51] epoch 4416, training loss: 10619.85, average training loss: 11568.15, base loss: 19785.97
[INFO 2017-06-27 20:53:28,176 main.py:51] epoch 4417, training loss: 11596.83, average training loss: 11569.10, base loss: 19789.26
[INFO 2017-06-27 20:53:28,583 main.py:51] epoch 4418, training loss: 12090.15, average training loss: 11570.86, base loss: 19792.98
[INFO 2017-06-27 20:53:28,963 main.py:51] epoch 4419, training loss: 10582.94, average training loss: 11569.50, base loss: 19790.24
[INFO 2017-06-27 20:53:29,443 main.py:51] epoch 4420, training loss: 12499.86, average training loss: 11570.81, base loss: 19793.92
[INFO 2017-06-27 20:53:29,821 main.py:51] epoch 4421, training loss: 12278.99, average training loss: 11571.26, base loss: 19795.80
[INFO 2017-06-27 20:53:30,233 main.py:51] epoch 4422, training loss: 14201.53, average training loss: 11573.70, base loss: 19800.07
[INFO 2017-06-27 20:53:30,660 main.py:51] epoch 4423, training loss: 10984.62, average training loss: 11572.95, base loss: 19799.66
[INFO 2017-06-27 20:53:31,043 main.py:51] epoch 4424, training loss: 11315.21, average training loss: 11572.59, base loss: 19800.54
[INFO 2017-06-27 20:53:31,505 main.py:51] epoch 4425, training loss: 10665.47, average training loss: 11571.71, base loss: 19800.41
[INFO 2017-06-27 20:53:31,957 main.py:51] epoch 4426, training loss: 12154.31, average training loss: 11572.88, base loss: 19804.45
[INFO 2017-06-27 20:53:32,415 main.py:51] epoch 4427, training loss: 11544.15, average training loss: 11572.49, base loss: 19803.08
[INFO 2017-06-27 20:53:32,918 main.py:51] epoch 4428, training loss: 12431.65, average training loss: 11574.17, base loss: 19807.95
[INFO 2017-06-27 20:53:33,307 main.py:51] epoch 4429, training loss: 9944.47, average training loss: 11573.52, base loss: 19807.44
[INFO 2017-06-27 20:53:33,755 main.py:51] epoch 4430, training loss: 10759.43, average training loss: 11572.55, base loss: 19804.63
[INFO 2017-06-27 20:53:34,145 main.py:51] epoch 4431, training loss: 10583.60, average training loss: 11572.42, base loss: 19804.84
[INFO 2017-06-27 20:53:34,579 main.py:51] epoch 4432, training loss: 12692.75, average training loss: 11573.27, base loss: 19807.04
[INFO 2017-06-27 20:53:34,987 main.py:51] epoch 4433, training loss: 11735.57, average training loss: 11573.19, base loss: 19806.66
[INFO 2017-06-27 20:53:35,365 main.py:51] epoch 4434, training loss: 12092.18, average training loss: 11573.09, base loss: 19807.50
[INFO 2017-06-27 20:53:35,747 main.py:51] epoch 4435, training loss: 11298.71, average training loss: 11574.15, base loss: 19810.99
[INFO 2017-06-27 20:53:36,229 main.py:51] epoch 4436, training loss: 10909.79, average training loss: 11572.72, base loss: 19807.83
[INFO 2017-06-27 20:53:36,608 main.py:51] epoch 4437, training loss: 10991.99, average training loss: 11571.88, base loss: 19805.93
[INFO 2017-06-27 20:53:36,983 main.py:51] epoch 4438, training loss: 12378.91, average training loss: 11571.90, base loss: 19805.66
[INFO 2017-06-27 20:53:37,431 main.py:51] epoch 4439, training loss: 13013.96, average training loss: 11574.68, base loss: 19812.63
[INFO 2017-06-27 20:53:37,849 main.py:51] epoch 4440, training loss: 11315.08, average training loss: 11572.50, base loss: 19810.60
[INFO 2017-06-27 20:53:38,227 main.py:51] epoch 4441, training loss: 11142.58, average training loss: 11572.25, base loss: 19810.08
[INFO 2017-06-27 20:53:38,605 main.py:51] epoch 4442, training loss: 10601.50, average training loss: 11571.84, base loss: 19810.21
[INFO 2017-06-27 20:53:38,987 main.py:51] epoch 4443, training loss: 11424.07, average training loss: 11569.45, base loss: 19808.07
[INFO 2017-06-27 20:53:39,373 main.py:51] epoch 4444, training loss: 12914.19, average training loss: 11570.68, base loss: 19811.08
[INFO 2017-06-27 20:53:39,749 main.py:51] epoch 4445, training loss: 11669.89, average training loss: 11570.21, base loss: 19809.68
[INFO 2017-06-27 20:53:40,124 main.py:51] epoch 4446, training loss: 10398.98, average training loss: 11569.19, base loss: 19807.72
[INFO 2017-06-27 20:53:40,500 main.py:51] epoch 4447, training loss: 10192.86, average training loss: 11566.38, base loss: 19802.85
[INFO 2017-06-27 20:53:40,877 main.py:51] epoch 4448, training loss: 11126.03, average training loss: 11565.60, base loss: 19801.63
[INFO 2017-06-27 20:53:41,254 main.py:51] epoch 4449, training loss: 10182.93, average training loss: 11562.92, base loss: 19797.77
[INFO 2017-06-27 20:53:41,626 main.py:51] epoch 4450, training loss: 11156.93, average training loss: 11562.89, base loss: 19798.46
[INFO 2017-06-27 20:53:42,001 main.py:51] epoch 4451, training loss: 11729.37, average training loss: 11563.11, base loss: 19798.64
[INFO 2017-06-27 20:53:42,374 main.py:51] epoch 4452, training loss: 10215.39, average training loss: 11560.87, base loss: 19795.72
[INFO 2017-06-27 20:53:42,761 main.py:51] epoch 4453, training loss: 11815.29, average training loss: 11561.87, base loss: 19797.06
[INFO 2017-06-27 20:53:43,164 main.py:51] epoch 4454, training loss: 11361.61, average training loss: 11561.85, base loss: 19796.73
[INFO 2017-06-27 20:53:43,541 main.py:51] epoch 4455, training loss: 10057.27, average training loss: 11559.08, base loss: 19792.69
[INFO 2017-06-27 20:53:43,915 main.py:51] epoch 4456, training loss: 10940.76, average training loss: 11559.00, base loss: 19793.00
[INFO 2017-06-27 20:53:44,291 main.py:51] epoch 4457, training loss: 10677.97, average training loss: 11557.65, base loss: 19791.07
[INFO 2017-06-27 20:53:44,670 main.py:51] epoch 4458, training loss: 10523.85, average training loss: 11556.21, base loss: 19787.76
[INFO 2017-06-27 20:53:45,047 main.py:51] epoch 4459, training loss: 11162.10, average training loss: 11555.65, base loss: 19786.64
[INFO 2017-06-27 20:53:45,424 main.py:51] epoch 4460, training loss: 12226.25, average training loss: 11553.61, base loss: 19782.69
[INFO 2017-06-27 20:53:45,798 main.py:51] epoch 4461, training loss: 11449.75, average training loss: 11552.75, base loss: 19780.71
[INFO 2017-06-27 20:53:46,178 main.py:51] epoch 4462, training loss: 10990.92, average training loss: 11552.41, base loss: 19780.54
[INFO 2017-06-27 20:53:46,555 main.py:51] epoch 4463, training loss: 10353.85, average training loss: 11551.66, base loss: 19779.65
[INFO 2017-06-27 20:53:47,011 main.py:51] epoch 4464, training loss: 10108.35, average training loss: 11549.86, base loss: 19777.52
[INFO 2017-06-27 20:53:47,412 main.py:51] epoch 4465, training loss: 11447.19, average training loss: 11549.63, base loss: 19778.14
[INFO 2017-06-27 20:53:47,809 main.py:51] epoch 4466, training loss: 12348.33, average training loss: 11549.71, base loss: 19778.55
[INFO 2017-06-27 20:53:48,187 main.py:51] epoch 4467, training loss: 10604.87, average training loss: 11548.57, base loss: 19777.54
[INFO 2017-06-27 20:53:48,585 main.py:51] epoch 4468, training loss: 10635.04, average training loss: 11548.95, base loss: 19779.32
[INFO 2017-06-27 20:53:48,968 main.py:51] epoch 4469, training loss: 12575.44, average training loss: 11548.90, base loss: 19779.31
[INFO 2017-06-27 20:53:49,349 main.py:51] epoch 4470, training loss: 12807.47, average training loss: 11549.33, base loss: 19781.64
[INFO 2017-06-27 20:53:49,728 main.py:51] epoch 4471, training loss: 10005.33, average training loss: 11545.90, base loss: 19775.52
[INFO 2017-06-27 20:53:50,105 main.py:51] epoch 4472, training loss: 13020.09, average training loss: 11546.81, base loss: 19778.06
[INFO 2017-06-27 20:53:50,479 main.py:51] epoch 4473, training loss: 11487.82, average training loss: 11546.51, base loss: 19777.24
[INFO 2017-06-27 20:53:50,856 main.py:51] epoch 4474, training loss: 11785.36, average training loss: 11546.24, base loss: 19777.99
[INFO 2017-06-27 20:53:51,232 main.py:51] epoch 4475, training loss: 11498.21, average training loss: 11546.68, base loss: 19779.01
[INFO 2017-06-27 20:53:51,608 main.py:51] epoch 4476, training loss: 11265.57, average training loss: 11545.02, base loss: 19775.52
[INFO 2017-06-27 20:53:51,991 main.py:51] epoch 4477, training loss: 10758.69, average training loss: 11544.22, base loss: 19773.36
[INFO 2017-06-27 20:53:52,367 main.py:51] epoch 4478, training loss: 11220.51, average training loss: 11543.89, base loss: 19772.61
[INFO 2017-06-27 20:53:52,822 main.py:51] epoch 4479, training loss: 10531.04, average training loss: 11542.41, base loss: 19770.22
[INFO 2017-06-27 20:53:53,240 main.py:51] epoch 4480, training loss: 9882.03, average training loss: 11540.32, base loss: 19765.75
[INFO 2017-06-27 20:53:53,619 main.py:51] epoch 4481, training loss: 10424.81, average training loss: 11538.67, base loss: 19763.72
[INFO 2017-06-27 20:53:53,997 main.py:51] epoch 4482, training loss: 12629.35, average training loss: 11539.19, base loss: 19764.12
[INFO 2017-06-27 20:53:54,373 main.py:51] epoch 4483, training loss: 10133.42, average training loss: 11537.45, base loss: 19760.43
[INFO 2017-06-27 20:53:54,749 main.py:51] epoch 4484, training loss: 12354.33, average training loss: 11537.87, base loss: 19761.45
[INFO 2017-06-27 20:53:55,123 main.py:51] epoch 4485, training loss: 12009.39, average training loss: 11538.04, base loss: 19762.58
[INFO 2017-06-27 20:53:55,576 main.py:51] epoch 4486, training loss: 10851.22, average training loss: 11537.59, base loss: 19762.21
[INFO 2017-06-27 20:53:55,985 main.py:51] epoch 4487, training loss: 11379.01, average training loss: 11538.10, base loss: 19762.49
[INFO 2017-06-27 20:53:56,372 main.py:51] epoch 4488, training loss: 11322.86, average training loss: 11539.30, base loss: 19766.56
[INFO 2017-06-27 20:53:56,748 main.py:51] epoch 4489, training loss: 12332.97, average training loss: 11540.69, base loss: 19769.06
[INFO 2017-06-27 20:53:57,125 main.py:51] epoch 4490, training loss: 11093.25, average training loss: 11540.06, base loss: 19768.42
[INFO 2017-06-27 20:53:57,598 main.py:51] epoch 4491, training loss: 11660.20, average training loss: 11539.49, base loss: 19768.17
[INFO 2017-06-27 20:53:57,978 main.py:51] epoch 4492, training loss: 10920.95, average training loss: 11537.23, base loss: 19763.93
[INFO 2017-06-27 20:53:58,358 main.py:51] epoch 4493, training loss: 10488.01, average training loss: 11535.61, base loss: 19762.07
[INFO 2017-06-27 20:53:58,835 main.py:51] epoch 4494, training loss: 11504.00, average training loss: 11535.61, base loss: 19761.91
[INFO 2017-06-27 20:53:59,242 main.py:51] epoch 4495, training loss: 10892.39, average training loss: 11535.55, base loss: 19762.54
[INFO 2017-06-27 20:53:59,621 main.py:51] epoch 4496, training loss: 12483.74, average training loss: 11534.38, base loss: 19760.21
[INFO 2017-06-27 20:54:00,080 main.py:51] epoch 4497, training loss: 10621.96, average training loss: 11534.07, base loss: 19760.55
[INFO 2017-06-27 20:54:00,487 main.py:51] epoch 4498, training loss: 11213.70, average training loss: 11534.27, base loss: 19761.45
[INFO 2017-06-27 20:54:00,875 main.py:51] epoch 4499, training loss: 11710.34, average training loss: 11533.56, base loss: 19760.66
[INFO 2017-06-27 20:54:00,875 main.py:53] epoch 4499, testing
[INFO 2017-06-27 20:54:02,588 main.py:105] average testing loss: 11557.21, base loss: 20086.67
[INFO 2017-06-27 20:54:02,589 main.py:106] improve_loss: 8529.46, improve_percent: 0.42
[INFO 2017-06-27 20:54:02,590 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:54:02,992 main.py:51] epoch 4500, training loss: 10844.82, average training loss: 11533.95, base loss: 19761.70
[INFO 2017-06-27 20:54:03,372 main.py:51] epoch 4501, training loss: 10119.18, average training loss: 11533.69, base loss: 19762.25
[INFO 2017-06-27 20:54:03,759 main.py:51] epoch 4502, training loss: 10622.73, average training loss: 11532.65, base loss: 19759.77
[INFO 2017-06-27 20:54:04,144 main.py:51] epoch 4503, training loss: 10905.87, average training loss: 11532.68, base loss: 19760.37
[INFO 2017-06-27 20:54:04,521 main.py:51] epoch 4504, training loss: 12193.23, average training loss: 11533.54, base loss: 19762.21
[INFO 2017-06-27 20:54:04,926 main.py:51] epoch 4505, training loss: 12674.56, average training loss: 11534.05, base loss: 19763.98
[INFO 2017-06-27 20:54:05,302 main.py:51] epoch 4506, training loss: 10037.41, average training loss: 11530.42, base loss: 19757.89
[INFO 2017-06-27 20:54:05,680 main.py:51] epoch 4507, training loss: 11252.79, average training loss: 11529.81, base loss: 19758.13
[INFO 2017-06-27 20:54:06,056 main.py:51] epoch 4508, training loss: 11935.95, average training loss: 11530.42, base loss: 19761.41
[INFO 2017-06-27 20:54:06,438 main.py:51] epoch 4509, training loss: 11664.03, average training loss: 11530.96, base loss: 19762.14
[INFO 2017-06-27 20:54:06,814 main.py:51] epoch 4510, training loss: 10511.45, average training loss: 11529.60, base loss: 19758.48
[INFO 2017-06-27 20:54:07,191 main.py:51] epoch 4511, training loss: 10408.02, average training loss: 11528.82, base loss: 19757.28
[INFO 2017-06-27 20:54:07,569 main.py:51] epoch 4512, training loss: 10633.12, average training loss: 11528.06, base loss: 19756.50
[INFO 2017-06-27 20:54:07,947 main.py:51] epoch 4513, training loss: 12030.85, average training loss: 11528.81, base loss: 19759.22
[INFO 2017-06-27 20:54:08,325 main.py:51] epoch 4514, training loss: 11462.02, average training loss: 11528.96, base loss: 19760.49
[INFO 2017-06-27 20:54:08,698 main.py:51] epoch 4515, training loss: 10448.91, average training loss: 11528.35, base loss: 19760.01
[INFO 2017-06-27 20:54:09,074 main.py:51] epoch 4516, training loss: 11176.41, average training loss: 11526.00, base loss: 19757.42
[INFO 2017-06-27 20:54:09,450 main.py:51] epoch 4517, training loss: 11516.11, average training loss: 11525.11, base loss: 19757.67
[INFO 2017-06-27 20:54:09,828 main.py:51] epoch 4518, training loss: 10821.52, average training loss: 11524.32, base loss: 19754.57
[INFO 2017-06-27 20:54:10,205 main.py:51] epoch 4519, training loss: 10737.91, average training loss: 11523.55, base loss: 19754.67
[INFO 2017-06-27 20:54:10,578 main.py:51] epoch 4520, training loss: 10346.43, average training loss: 11523.44, base loss: 19755.39
[INFO 2017-06-27 20:54:10,951 main.py:51] epoch 4521, training loss: 11943.07, average training loss: 11523.78, base loss: 19756.95
[INFO 2017-06-27 20:54:11,327 main.py:51] epoch 4522, training loss: 10924.35, average training loss: 11524.81, base loss: 19760.00
[INFO 2017-06-27 20:54:11,703 main.py:51] epoch 4523, training loss: 12084.18, average training loss: 11525.71, base loss: 19761.89
[INFO 2017-06-27 20:54:12,080 main.py:51] epoch 4524, training loss: 11411.01, average training loss: 11525.12, base loss: 19761.39
[INFO 2017-06-27 20:54:12,459 main.py:51] epoch 4525, training loss: 12352.77, average training loss: 11527.17, base loss: 19766.45
[INFO 2017-06-27 20:54:12,832 main.py:51] epoch 4526, training loss: 10692.86, average training loss: 11524.59, base loss: 19762.49
[INFO 2017-06-27 20:54:13,207 main.py:51] epoch 4527, training loss: 11895.47, average training loss: 11524.77, base loss: 19761.57
[INFO 2017-06-27 20:54:13,583 main.py:51] epoch 4528, training loss: 12697.64, average training loss: 11525.84, base loss: 19763.23
[INFO 2017-06-27 20:54:13,959 main.py:51] epoch 4529, training loss: 9998.25, average training loss: 11525.78, base loss: 19762.47
[INFO 2017-06-27 20:54:14,336 main.py:51] epoch 4530, training loss: 10462.66, average training loss: 11525.08, base loss: 19761.96
[INFO 2017-06-27 20:54:14,715 main.py:51] epoch 4531, training loss: 12002.35, average training loss: 11524.93, base loss: 19762.77
[INFO 2017-06-27 20:54:15,093 main.py:51] epoch 4532, training loss: 11662.33, average training loss: 11525.30, base loss: 19762.02
[INFO 2017-06-27 20:54:15,469 main.py:51] epoch 4533, training loss: 10450.35, average training loss: 11522.38, base loss: 19757.82
[INFO 2017-06-27 20:54:15,844 main.py:51] epoch 4534, training loss: 10531.66, average training loss: 11521.97, base loss: 19756.48
[INFO 2017-06-27 20:54:16,221 main.py:51] epoch 4535, training loss: 12724.96, average training loss: 11524.73, base loss: 19762.21
[INFO 2017-06-27 20:54:16,598 main.py:51] epoch 4536, training loss: 11801.78, average training loss: 11524.11, base loss: 19762.44
[INFO 2017-06-27 20:54:16,975 main.py:51] epoch 4537, training loss: 11067.24, average training loss: 11524.28, base loss: 19763.14
[INFO 2017-06-27 20:54:17,348 main.py:51] epoch 4538, training loss: 11965.88, average training loss: 11524.66, base loss: 19766.21
[INFO 2017-06-27 20:54:17,831 main.py:51] epoch 4539, training loss: 10975.50, average training loss: 11523.57, base loss: 19764.78
[INFO 2017-06-27 20:54:18,210 main.py:51] epoch 4540, training loss: 10653.88, average training loss: 11523.37, base loss: 19766.04
[INFO 2017-06-27 20:54:18,597 main.py:51] epoch 4541, training loss: 10489.36, average training loss: 11521.24, base loss: 19761.51
[INFO 2017-06-27 20:54:19,061 main.py:51] epoch 4542, training loss: 13311.54, average training loss: 11521.81, base loss: 19761.54
[INFO 2017-06-27 20:54:19,464 main.py:51] epoch 4543, training loss: 11242.99, average training loss: 11521.03, base loss: 19761.31
[INFO 2017-06-27 20:54:19,858 main.py:51] epoch 4544, training loss: 11592.65, average training loss: 11520.07, base loss: 19758.88
[INFO 2017-06-27 20:54:20,335 main.py:51] epoch 4545, training loss: 11210.69, average training loss: 11519.86, base loss: 19758.43
[INFO 2017-06-27 20:54:20,739 main.py:51] epoch 4546, training loss: 10880.06, average training loss: 11520.07, base loss: 19759.20
[INFO 2017-06-27 20:54:21,129 main.py:51] epoch 4547, training loss: 11820.12, average training loss: 11520.15, base loss: 19758.65
[INFO 2017-06-27 20:54:21,511 main.py:51] epoch 4548, training loss: 11351.93, average training loss: 11521.19, base loss: 19761.29
[INFO 2017-06-27 20:54:21,943 main.py:51] epoch 4549, training loss: 12408.29, average training loss: 11521.49, base loss: 19761.67
[INFO 2017-06-27 20:54:22,321 main.py:51] epoch 4550, training loss: 11310.87, average training loss: 11521.92, base loss: 19764.28
[INFO 2017-06-27 20:54:22,703 main.py:51] epoch 4551, training loss: 12166.44, average training loss: 11522.83, base loss: 19766.43
[INFO 2017-06-27 20:54:23,079 main.py:51] epoch 4552, training loss: 9700.77, average training loss: 11520.63, base loss: 19762.73
[INFO 2017-06-27 20:54:23,532 main.py:51] epoch 4553, training loss: 11179.76, average training loss: 11519.50, base loss: 19759.39
[INFO 2017-06-27 20:54:23,947 main.py:51] epoch 4554, training loss: 11225.33, average training loss: 11520.05, base loss: 19761.69
[INFO 2017-06-27 20:54:24,346 main.py:51] epoch 4555, training loss: 11266.32, average training loss: 11517.92, base loss: 19758.55
[INFO 2017-06-27 20:54:24,739 main.py:51] epoch 4556, training loss: 12275.43, average training loss: 11519.67, base loss: 19762.71
[INFO 2017-06-27 20:54:25,202 main.py:51] epoch 4557, training loss: 12246.43, average training loss: 11519.75, base loss: 19764.55
[INFO 2017-06-27 20:54:25,586 main.py:51] epoch 4558, training loss: 12147.84, average training loss: 11519.84, base loss: 19765.15
[INFO 2017-06-27 20:54:25,986 main.py:51] epoch 4559, training loss: 11663.61, average training loss: 11519.31, base loss: 19765.15
[INFO 2017-06-27 20:54:26,366 main.py:51] epoch 4560, training loss: 10529.88, average training loss: 11519.28, base loss: 19766.17
[INFO 2017-06-27 20:54:26,745 main.py:51] epoch 4561, training loss: 11353.47, average training loss: 11519.64, base loss: 19767.57
[INFO 2017-06-27 20:54:27,121 main.py:51] epoch 4562, training loss: 12562.02, average training loss: 11519.87, base loss: 19768.79
[INFO 2017-06-27 20:54:27,581 main.py:51] epoch 4563, training loss: 11553.76, average training loss: 11521.14, base loss: 19773.45
[INFO 2017-06-27 20:54:27,985 main.py:51] epoch 4564, training loss: 12081.27, average training loss: 11520.88, base loss: 19772.61
[INFO 2017-06-27 20:54:28,373 main.py:51] epoch 4565, training loss: 10839.93, average training loss: 11519.75, base loss: 19771.62
[INFO 2017-06-27 20:54:28,757 main.py:51] epoch 4566, training loss: 11226.73, average training loss: 11520.07, base loss: 19773.45
[INFO 2017-06-27 20:54:29,134 main.py:51] epoch 4567, training loss: 12878.71, average training loss: 11521.88, base loss: 19778.41
[INFO 2017-06-27 20:54:29,512 main.py:51] epoch 4568, training loss: 10984.95, average training loss: 11521.16, base loss: 19776.09
[INFO 2017-06-27 20:54:29,889 main.py:51] epoch 4569, training loss: 11156.08, average training loss: 11520.27, base loss: 19775.78
[INFO 2017-06-27 20:54:30,265 main.py:51] epoch 4570, training loss: 10686.80, average training loss: 11519.05, base loss: 19772.96
[INFO 2017-06-27 20:54:30,714 main.py:51] epoch 4571, training loss: 10782.11, average training loss: 11518.12, base loss: 19771.93
[INFO 2017-06-27 20:54:31,120 main.py:51] epoch 4572, training loss: 11847.37, average training loss: 11520.09, base loss: 19777.54
[INFO 2017-06-27 20:54:31,498 main.py:51] epoch 4573, training loss: 11401.09, average training loss: 11519.29, base loss: 19775.95
[INFO 2017-06-27 20:54:31,876 main.py:51] epoch 4574, training loss: 10402.50, average training loss: 11519.10, base loss: 19775.55
[INFO 2017-06-27 20:54:32,256 main.py:51] epoch 4575, training loss: 9220.23, average training loss: 11516.24, base loss: 19769.41
[INFO 2017-06-27 20:54:32,715 main.py:51] epoch 4576, training loss: 10631.89, average training loss: 11516.06, base loss: 19770.02
[INFO 2017-06-27 20:54:33,115 main.py:51] epoch 4577, training loss: 12422.56, average training loss: 11516.39, base loss: 19769.69
[INFO 2017-06-27 20:54:33,493 main.py:51] epoch 4578, training loss: 12248.01, average training loss: 11516.94, base loss: 19772.17
[INFO 2017-06-27 20:54:33,908 main.py:51] epoch 4579, training loss: 10876.27, average training loss: 11516.44, base loss: 19774.44
[INFO 2017-06-27 20:54:34,316 main.py:51] epoch 4580, training loss: 10457.77, average training loss: 11514.60, base loss: 19772.03
[INFO 2017-06-27 20:54:34,705 main.py:51] epoch 4581, training loss: 10208.32, average training loss: 11512.92, base loss: 19768.84
[INFO 2017-06-27 20:54:35,138 main.py:51] epoch 4582, training loss: 10677.55, average training loss: 11511.66, base loss: 19766.63
[INFO 2017-06-27 20:54:35,525 main.py:51] epoch 4583, training loss: 10638.35, average training loss: 11511.82, base loss: 19766.28
[INFO 2017-06-27 20:54:35,994 main.py:51] epoch 4584, training loss: 11465.30, average training loss: 11511.68, base loss: 19766.84
[INFO 2017-06-27 20:54:36,404 main.py:51] epoch 4585, training loss: 10566.47, average training loss: 11511.14, base loss: 19767.30
[INFO 2017-06-27 20:54:36,873 main.py:51] epoch 4586, training loss: 10363.09, average training loss: 11510.84, base loss: 19769.08
[INFO 2017-06-27 20:54:37,277 main.py:51] epoch 4587, training loss: 10581.62, average training loss: 11511.68, base loss: 19771.88
[INFO 2017-06-27 20:54:37,749 main.py:51] epoch 4588, training loss: 12245.69, average training loss: 11512.87, base loss: 19775.27
[INFO 2017-06-27 20:54:38,179 main.py:51] epoch 4589, training loss: 13258.42, average training loss: 11514.44, base loss: 19779.41
[INFO 2017-06-27 20:54:38,651 main.py:51] epoch 4590, training loss: 10935.02, average training loss: 11512.80, base loss: 19776.84
[INFO 2017-06-27 20:54:39,052 main.py:51] epoch 4591, training loss: 10386.71, average training loss: 11510.13, base loss: 19772.29
[INFO 2017-06-27 20:54:39,468 main.py:51] epoch 4592, training loss: 14045.83, average training loss: 11512.60, base loss: 19778.16
[INFO 2017-06-27 20:54:39,871 main.py:51] epoch 4593, training loss: 13118.68, average training loss: 11514.89, base loss: 19782.48
[INFO 2017-06-27 20:54:40,250 main.py:51] epoch 4594, training loss: 11356.11, average training loss: 11513.92, base loss: 19781.14
[INFO 2017-06-27 20:54:40,722 main.py:51] epoch 4595, training loss: 10391.14, average training loss: 11514.17, base loss: 19781.40
[INFO 2017-06-27 20:54:41,099 main.py:51] epoch 4596, training loss: 11221.50, average training loss: 11514.86, base loss: 19782.78
[INFO 2017-06-27 20:54:41,530 main.py:51] epoch 4597, training loss: 11214.04, average training loss: 11514.42, base loss: 19782.74
[INFO 2017-06-27 20:54:41,945 main.py:51] epoch 4598, training loss: 11890.13, average training loss: 11513.13, base loss: 19780.42
[INFO 2017-06-27 20:54:42,329 main.py:51] epoch 4599, training loss: 10497.37, average training loss: 11513.36, base loss: 19781.17
[INFO 2017-06-27 20:54:42,329 main.py:53] epoch 4599, testing
[INFO 2017-06-27 20:54:44,181 main.py:105] average testing loss: 11276.71, base loss: 19773.78
[INFO 2017-06-27 20:54:44,182 main.py:106] improve_loss: 8497.07, improve_percent: 0.43
[INFO 2017-06-27 20:54:44,182 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:54:44,585 main.py:51] epoch 4600, training loss: 12719.82, average training loss: 11513.51, base loss: 19782.98
[INFO 2017-06-27 20:54:44,978 main.py:51] epoch 4601, training loss: 11071.20, average training loss: 11513.20, base loss: 19782.69
[INFO 2017-06-27 20:54:45,382 main.py:51] epoch 4602, training loss: 10917.12, average training loss: 11512.18, base loss: 19779.07
[INFO 2017-06-27 20:54:45,766 main.py:51] epoch 4603, training loss: 11085.78, average training loss: 11511.48, base loss: 19777.32
[INFO 2017-06-27 20:54:46,256 main.py:51] epoch 4604, training loss: 12443.78, average training loss: 11511.29, base loss: 19776.46
[INFO 2017-06-27 20:54:46,650 main.py:51] epoch 4605, training loss: 11905.92, average training loss: 11512.59, base loss: 19779.76
[INFO 2017-06-27 20:54:47,117 main.py:51] epoch 4606, training loss: 12426.56, average training loss: 11513.94, base loss: 19781.64
[INFO 2017-06-27 20:54:47,567 main.py:51] epoch 4607, training loss: 11832.83, average training loss: 11514.25, base loss: 19783.78
[INFO 2017-06-27 20:54:48,019 main.py:51] epoch 4608, training loss: 10863.79, average training loss: 11513.51, base loss: 19784.58
[INFO 2017-06-27 20:54:48,478 main.py:51] epoch 4609, training loss: 10262.95, average training loss: 11511.18, base loss: 19781.08
[INFO 2017-06-27 20:54:48,856 main.py:51] epoch 4610, training loss: 11424.31, average training loss: 11511.63, base loss: 19784.64
[INFO 2017-06-27 20:54:49,326 main.py:51] epoch 4611, training loss: 11102.08, average training loss: 11510.51, base loss: 19783.20
[INFO 2017-06-27 20:54:49,783 main.py:51] epoch 4612, training loss: 11524.14, average training loss: 11510.47, base loss: 19784.71
[INFO 2017-06-27 20:54:50,219 main.py:51] epoch 4613, training loss: 10849.90, average training loss: 11508.64, base loss: 19781.44
[INFO 2017-06-27 20:54:50,702 main.py:51] epoch 4614, training loss: 11743.52, average training loss: 11508.76, base loss: 19783.08
[INFO 2017-06-27 20:54:51,080 main.py:51] epoch 4615, training loss: 10761.44, average training loss: 11507.92, base loss: 19783.27
[INFO 2017-06-27 20:54:51,551 main.py:51] epoch 4616, training loss: 10241.75, average training loss: 11504.92, base loss: 19777.69
[INFO 2017-06-27 20:54:51,959 main.py:51] epoch 4617, training loss: 10256.61, average training loss: 11502.03, base loss: 19771.38
[INFO 2017-06-27 20:54:52,339 main.py:51] epoch 4618, training loss: 11307.35, average training loss: 11501.95, base loss: 19770.47
[INFO 2017-06-27 20:54:52,786 main.py:51] epoch 4619, training loss: 11236.51, average training loss: 11501.97, base loss: 19770.17
[INFO 2017-06-27 20:54:53,186 main.py:51] epoch 4620, training loss: 11616.63, average training loss: 11502.92, base loss: 19770.97
[INFO 2017-06-27 20:54:53,572 main.py:51] epoch 4621, training loss: 11603.96, average training loss: 11502.30, base loss: 19768.31
[INFO 2017-06-27 20:54:53,969 main.py:51] epoch 4622, training loss: 11381.75, average training loss: 11503.97, base loss: 19774.19
[INFO 2017-06-27 20:54:54,346 main.py:51] epoch 4623, training loss: 10493.50, average training loss: 11503.14, base loss: 19773.04
[INFO 2017-06-27 20:54:54,724 main.py:51] epoch 4624, training loss: 10370.51, average training loss: 11501.15, base loss: 19770.31
[INFO 2017-06-27 20:54:55,111 main.py:51] epoch 4625, training loss: 11882.94, average training loss: 11502.45, base loss: 19771.56
[INFO 2017-06-27 20:54:55,488 main.py:51] epoch 4626, training loss: 12096.00, average training loss: 11502.30, base loss: 19771.09
[INFO 2017-06-27 20:54:55,860 main.py:51] epoch 4627, training loss: 11366.11, average training loss: 11500.45, base loss: 19769.43
[INFO 2017-06-27 20:54:56,233 main.py:51] epoch 4628, training loss: 11094.61, average training loss: 11498.36, base loss: 19766.28
[INFO 2017-06-27 20:54:56,604 main.py:51] epoch 4629, training loss: 11163.92, average training loss: 11498.82, base loss: 19767.52
[INFO 2017-06-27 20:54:56,981 main.py:51] epoch 4630, training loss: 13390.26, average training loss: 11500.89, base loss: 19771.42
[INFO 2017-06-27 20:54:57,361 main.py:51] epoch 4631, training loss: 11797.35, average training loss: 11500.65, base loss: 19771.73
[INFO 2017-06-27 20:54:57,747 main.py:51] epoch 4632, training loss: 11365.07, average training loss: 11500.96, base loss: 19773.86
[INFO 2017-06-27 20:54:58,129 main.py:51] epoch 4633, training loss: 10898.98, average training loss: 11500.62, base loss: 19773.32
[INFO 2017-06-27 20:54:58,514 main.py:51] epoch 4634, training loss: 10970.95, average training loss: 11497.91, base loss: 19768.80
[INFO 2017-06-27 20:54:58,898 main.py:51] epoch 4635, training loss: 11064.85, average training loss: 11496.62, base loss: 19768.11
[INFO 2017-06-27 20:54:59,279 main.py:51] epoch 4636, training loss: 12772.36, average training loss: 11498.99, base loss: 19774.02
[INFO 2017-06-27 20:54:59,665 main.py:51] epoch 4637, training loss: 12141.94, average training loss: 11499.12, base loss: 19775.88
[INFO 2017-06-27 20:55:00,051 main.py:51] epoch 4638, training loss: 11493.01, average training loss: 11498.56, base loss: 19775.68
[INFO 2017-06-27 20:55:00,429 main.py:51] epoch 4639, training loss: 11706.96, average training loss: 11498.21, base loss: 19774.13
[INFO 2017-06-27 20:55:00,805 main.py:51] epoch 4640, training loss: 11345.73, average training loss: 11497.83, base loss: 19772.87
[INFO 2017-06-27 20:55:01,189 main.py:51] epoch 4641, training loss: 10639.02, average training loss: 11497.15, base loss: 19771.90
[INFO 2017-06-27 20:55:01,565 main.py:51] epoch 4642, training loss: 10874.01, average training loss: 11495.17, base loss: 19766.54
[INFO 2017-06-27 20:55:01,959 main.py:51] epoch 4643, training loss: 11246.25, average training loss: 11494.47, base loss: 19765.47
[INFO 2017-06-27 20:55:02,364 main.py:51] epoch 4644, training loss: 10183.89, average training loss: 11491.69, base loss: 19760.54
[INFO 2017-06-27 20:55:02,743 main.py:51] epoch 4645, training loss: 12771.26, average training loss: 11493.61, base loss: 19763.63
[INFO 2017-06-27 20:55:03,210 main.py:51] epoch 4646, training loss: 11838.81, average training loss: 11492.44, base loss: 19761.63
[INFO 2017-06-27 20:55:03,613 main.py:51] epoch 4647, training loss: 9190.20, average training loss: 11489.40, base loss: 19756.69
[INFO 2017-06-27 20:55:03,993 main.py:51] epoch 4648, training loss: 11260.36, average training loss: 11489.50, base loss: 19756.86
[INFO 2017-06-27 20:55:04,382 main.py:51] epoch 4649, training loss: 11388.47, average training loss: 11489.99, base loss: 19758.73
[INFO 2017-06-27 20:55:04,759 main.py:51] epoch 4650, training loss: 11422.98, average training loss: 11490.22, base loss: 19760.35
[INFO 2017-06-27 20:55:05,137 main.py:51] epoch 4651, training loss: 10514.90, average training loss: 11489.91, base loss: 19760.65
[INFO 2017-06-27 20:55:05,521 main.py:51] epoch 4652, training loss: 11008.03, average training loss: 11488.01, base loss: 19758.77
[INFO 2017-06-27 20:55:05,900 main.py:51] epoch 4653, training loss: 12247.31, average training loss: 11487.96, base loss: 19758.38
[INFO 2017-06-27 20:55:06,285 main.py:51] epoch 4654, training loss: 10748.50, average training loss: 11485.73, base loss: 19754.15
[INFO 2017-06-27 20:55:06,670 main.py:51] epoch 4655, training loss: 12311.70, average training loss: 11488.01, base loss: 19759.45
[INFO 2017-06-27 20:55:07,047 main.py:51] epoch 4656, training loss: 10451.67, average training loss: 11487.04, base loss: 19758.86
[INFO 2017-06-27 20:55:07,432 main.py:51] epoch 4657, training loss: 10211.29, average training loss: 11486.75, base loss: 19760.09
[INFO 2017-06-27 20:55:07,812 main.py:51] epoch 4658, training loss: 10252.05, average training loss: 11485.08, base loss: 19756.39
[INFO 2017-06-27 20:55:08,221 main.py:51] epoch 4659, training loss: 11498.42, average training loss: 11482.52, base loss: 19752.56
[INFO 2017-06-27 20:55:08,613 main.py:51] epoch 4660, training loss: 11967.89, average training loss: 11483.60, base loss: 19754.01
[INFO 2017-06-27 20:55:09,007 main.py:51] epoch 4661, training loss: 11363.74, average training loss: 11483.92, base loss: 19755.40
[INFO 2017-06-27 20:55:09,388 main.py:51] epoch 4662, training loss: 12015.16, average training loss: 11484.92, base loss: 19757.68
[INFO 2017-06-27 20:55:09,796 main.py:51] epoch 4663, training loss: 11367.64, average training loss: 11483.46, base loss: 19756.01
[INFO 2017-06-27 20:55:10,250 main.py:51] epoch 4664, training loss: 11903.73, average training loss: 11482.89, base loss: 19754.59
[INFO 2017-06-27 20:55:10,699 main.py:51] epoch 4665, training loss: 11185.29, average training loss: 11482.15, base loss: 19754.82
[INFO 2017-06-27 20:55:11,139 main.py:51] epoch 4666, training loss: 11764.57, average training loss: 11482.95, base loss: 19757.82
[INFO 2017-06-27 20:55:11,594 main.py:51] epoch 4667, training loss: 13455.57, average training loss: 11486.40, base loss: 19764.40
[INFO 2017-06-27 20:55:12,006 main.py:51] epoch 4668, training loss: 10762.37, average training loss: 11486.34, base loss: 19763.91
[INFO 2017-06-27 20:55:12,416 main.py:51] epoch 4669, training loss: 11856.23, average training loss: 11486.65, base loss: 19763.69
[INFO 2017-06-27 20:55:12,883 main.py:51] epoch 4670, training loss: 11174.07, average training loss: 11487.21, base loss: 19764.23
[INFO 2017-06-27 20:55:13,293 main.py:51] epoch 4671, training loss: 10468.75, average training loss: 11485.66, base loss: 19761.53
[INFO 2017-06-27 20:55:13,682 main.py:51] epoch 4672, training loss: 10337.21, average training loss: 11483.72, base loss: 19758.24
[INFO 2017-06-27 20:55:14,063 main.py:51] epoch 4673, training loss: 11987.57, average training loss: 11483.70, base loss: 19760.28
[INFO 2017-06-27 20:55:14,439 main.py:51] epoch 4674, training loss: 11635.82, average training loss: 11484.79, base loss: 19763.25
[INFO 2017-06-27 20:55:14,814 main.py:51] epoch 4675, training loss: 11916.22, average training loss: 11486.52, base loss: 19769.08
[INFO 2017-06-27 20:55:15,196 main.py:51] epoch 4676, training loss: 13918.83, average training loss: 11489.73, base loss: 19774.48
[INFO 2017-06-27 20:55:15,578 main.py:51] epoch 4677, training loss: 11657.20, average training loss: 11491.25, base loss: 19776.97
[INFO 2017-06-27 20:55:15,955 main.py:51] epoch 4678, training loss: 10484.99, average training loss: 11490.25, base loss: 19775.56
[INFO 2017-06-27 20:55:16,332 main.py:51] epoch 4679, training loss: 10544.56, average training loss: 11488.74, base loss: 19772.31
[INFO 2017-06-27 20:55:16,708 main.py:51] epoch 4680, training loss: 11215.13, average training loss: 11488.15, base loss: 19771.84
[INFO 2017-06-27 20:55:17,085 main.py:51] epoch 4681, training loss: 10364.09, average training loss: 11486.45, base loss: 19768.13
[INFO 2017-06-27 20:55:17,460 main.py:51] epoch 4682, training loss: 12998.62, average training loss: 11489.05, base loss: 19773.38
[INFO 2017-06-27 20:55:17,834 main.py:51] epoch 4683, training loss: 13375.65, average training loss: 11489.52, base loss: 19773.63
[INFO 2017-06-27 20:55:18,211 main.py:51] epoch 4684, training loss: 9796.47, average training loss: 11486.98, base loss: 19769.40
[INFO 2017-06-27 20:55:18,584 main.py:51] epoch 4685, training loss: 11388.16, average training loss: 11486.13, base loss: 19768.76
[INFO 2017-06-27 20:55:19,037 main.py:51] epoch 4686, training loss: 11363.56, average training loss: 11487.33, base loss: 19771.41
[INFO 2017-06-27 20:55:19,457 main.py:51] epoch 4687, training loss: 11696.67, average training loss: 11489.09, base loss: 19775.58
[INFO 2017-06-27 20:55:19,847 main.py:51] epoch 4688, training loss: 11736.35, average training loss: 11487.93, base loss: 19775.24
[INFO 2017-06-27 20:55:20,285 main.py:51] epoch 4689, training loss: 11241.18, average training loss: 11487.73, base loss: 19775.42
[INFO 2017-06-27 20:55:20,666 main.py:51] epoch 4690, training loss: 11303.93, average training loss: 11488.65, base loss: 19777.46
[INFO 2017-06-27 20:55:21,080 main.py:51] epoch 4691, training loss: 12270.91, average training loss: 11489.80, base loss: 19780.65
[INFO 2017-06-27 20:55:21,547 main.py:51] epoch 4692, training loss: 12509.74, average training loss: 11490.30, base loss: 19781.98
[INFO 2017-06-27 20:55:21,966 main.py:51] epoch 4693, training loss: 12854.80, average training loss: 11492.40, base loss: 19786.06
[INFO 2017-06-27 20:55:22,348 main.py:51] epoch 4694, training loss: 11176.39, average training loss: 11492.58, base loss: 19788.59
[INFO 2017-06-27 20:55:22,728 main.py:51] epoch 4695, training loss: 12261.09, average training loss: 11493.53, base loss: 19790.47
[INFO 2017-06-27 20:55:23,188 main.py:51] epoch 4696, training loss: 11888.38, average training loss: 11494.90, base loss: 19794.88
[INFO 2017-06-27 20:55:23,597 main.py:51] epoch 4697, training loss: 11742.62, average training loss: 11493.40, base loss: 19792.21
[INFO 2017-06-27 20:55:23,986 main.py:51] epoch 4698, training loss: 13426.86, average training loss: 11494.34, base loss: 19793.36
[INFO 2017-06-27 20:55:24,459 main.py:51] epoch 4699, training loss: 11539.79, average training loss: 11493.93, base loss: 19793.09
[INFO 2017-06-27 20:55:24,460 main.py:53] epoch 4699, testing
[INFO 2017-06-27 20:55:26,115 main.py:105] average testing loss: 11283.36, base loss: 20106.37
[INFO 2017-06-27 20:55:26,115 main.py:106] improve_loss: 8823.01, improve_percent: 0.44
[INFO 2017-06-27 20:55:26,115 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 20:55:26,128 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:55:26,586 main.py:51] epoch 4700, training loss: 12014.08, average training loss: 11495.03, base loss: 19796.71
[INFO 2017-06-27 20:55:26,985 main.py:51] epoch 4701, training loss: 10660.71, average training loss: 11494.18, base loss: 19795.65
[INFO 2017-06-27 20:55:27,362 main.py:51] epoch 4702, training loss: 11288.08, average training loss: 11493.98, base loss: 19794.68
[INFO 2017-06-27 20:55:27,743 main.py:51] epoch 4703, training loss: 11313.02, average training loss: 11490.81, base loss: 19788.47
[INFO 2017-06-27 20:55:28,120 main.py:51] epoch 4704, training loss: 11856.50, average training loss: 11491.31, base loss: 19790.28
[INFO 2017-06-27 20:55:28,500 main.py:51] epoch 4705, training loss: 11025.85, average training loss: 11489.42, base loss: 19786.56
[INFO 2017-06-27 20:55:28,877 main.py:51] epoch 4706, training loss: 11064.72, average training loss: 11489.55, base loss: 19786.62
[INFO 2017-06-27 20:55:29,324 main.py:51] epoch 4707, training loss: 10644.67, average training loss: 11487.47, base loss: 19782.08
[INFO 2017-06-27 20:55:29,707 main.py:51] epoch 4708, training loss: 12067.10, average training loss: 11486.84, base loss: 19783.43
[INFO 2017-06-27 20:55:30,090 main.py:51] epoch 4709, training loss: 11172.98, average training loss: 11487.12, base loss: 19783.48
[INFO 2017-06-27 20:55:30,531 main.py:51] epoch 4710, training loss: 10556.66, average training loss: 11484.45, base loss: 19777.24
[INFO 2017-06-27 20:55:30,945 main.py:51] epoch 4711, training loss: 11139.22, average training loss: 11484.13, base loss: 19777.77
[INFO 2017-06-27 20:55:31,323 main.py:51] epoch 4712, training loss: 12718.14, average training loss: 11486.39, base loss: 19783.06
[INFO 2017-06-27 20:55:31,715 main.py:51] epoch 4713, training loss: 11935.89, average training loss: 11487.12, base loss: 19784.79
[INFO 2017-06-27 20:55:32,122 main.py:51] epoch 4714, training loss: 10824.36, average training loss: 11487.19, base loss: 19785.58
[INFO 2017-06-27 20:55:32,499 main.py:51] epoch 4715, training loss: 11659.26, average training loss: 11486.52, base loss: 19785.64
[INFO 2017-06-27 20:55:32,942 main.py:51] epoch 4716, training loss: 10685.46, average training loss: 11485.43, base loss: 19782.91
[INFO 2017-06-27 20:55:33,337 main.py:51] epoch 4717, training loss: 10350.42, average training loss: 11485.94, base loss: 19784.08
[INFO 2017-06-27 20:55:33,724 main.py:51] epoch 4718, training loss: 11326.68, average training loss: 11486.44, base loss: 19787.25
[INFO 2017-06-27 20:55:34,136 main.py:51] epoch 4719, training loss: 10424.26, average training loss: 11485.54, base loss: 19786.29
[INFO 2017-06-27 20:55:34,535 main.py:51] epoch 4720, training loss: 10414.18, average training loss: 11484.84, base loss: 19786.55
[INFO 2017-06-27 20:55:35,019 main.py:51] epoch 4721, training loss: 11254.03, average training loss: 11483.56, base loss: 19786.51
[INFO 2017-06-27 20:55:35,401 main.py:51] epoch 4722, training loss: 10092.49, average training loss: 11482.86, base loss: 19786.34
[INFO 2017-06-27 20:55:35,878 main.py:51] epoch 4723, training loss: 12507.12, average training loss: 11483.59, base loss: 19788.86
[INFO 2017-06-27 20:55:36,280 main.py:51] epoch 4724, training loss: 11608.54, average training loss: 11483.08, base loss: 19789.23
[INFO 2017-06-27 20:55:36,661 main.py:51] epoch 4725, training loss: 11670.88, average training loss: 11482.13, base loss: 19786.63
[INFO 2017-06-27 20:55:37,077 main.py:51] epoch 4726, training loss: 11096.14, average training loss: 11481.26, base loss: 19784.77
[INFO 2017-06-27 20:55:37,520 main.py:51] epoch 4727, training loss: 9827.59, average training loss: 11478.91, base loss: 19780.30
[INFO 2017-06-27 20:55:37,900 main.py:51] epoch 4728, training loss: 11455.02, average training loss: 11478.94, base loss: 19779.53
[INFO 2017-06-27 20:55:38,364 main.py:51] epoch 4729, training loss: 12086.34, average training loss: 11478.99, base loss: 19779.67
[INFO 2017-06-27 20:55:38,774 main.py:51] epoch 4730, training loss: 10999.83, average training loss: 11479.71, base loss: 19781.78
[INFO 2017-06-27 20:55:39,182 main.py:51] epoch 4731, training loss: 12692.88, average training loss: 11482.17, base loss: 19788.04
[INFO 2017-06-27 20:55:39,605 main.py:51] epoch 4732, training loss: 10203.70, average training loss: 11481.03, base loss: 19786.15
[INFO 2017-06-27 20:55:39,988 main.py:51] epoch 4733, training loss: 12306.90, average training loss: 11481.81, base loss: 19789.53
[INFO 2017-06-27 20:55:40,366 main.py:51] epoch 4734, training loss: 10822.23, average training loss: 11481.05, base loss: 19788.15
[INFO 2017-06-27 20:55:40,744 main.py:51] epoch 4735, training loss: 9909.35, average training loss: 11480.08, base loss: 19786.77
[INFO 2017-06-27 20:55:41,156 main.py:51] epoch 4736, training loss: 12292.91, average training loss: 11480.79, base loss: 19788.42
[INFO 2017-06-27 20:55:41,536 main.py:51] epoch 4737, training loss: 12578.57, average training loss: 11481.53, base loss: 19789.68
[INFO 2017-06-27 20:55:41,916 main.py:51] epoch 4738, training loss: 12440.20, average training loss: 11480.99, base loss: 19787.82
[INFO 2017-06-27 20:55:42,294 main.py:51] epoch 4739, training loss: 11914.45, average training loss: 11482.06, base loss: 19788.51
[INFO 2017-06-27 20:55:42,671 main.py:51] epoch 4740, training loss: 11517.30, average training loss: 11482.75, base loss: 19788.73
[INFO 2017-06-27 20:55:43,052 main.py:51] epoch 4741, training loss: 11339.13, average training loss: 11482.32, base loss: 19788.34
[INFO 2017-06-27 20:55:43,428 main.py:51] epoch 4742, training loss: 11791.09, average training loss: 11482.68, base loss: 19788.65
[INFO 2017-06-27 20:55:43,806 main.py:51] epoch 4743, training loss: 13021.18, average training loss: 11485.46, base loss: 19794.37
[INFO 2017-06-27 20:55:44,180 main.py:51] epoch 4744, training loss: 12341.93, average training loss: 11485.52, base loss: 19794.35
[INFO 2017-06-27 20:55:44,551 main.py:51] epoch 4745, training loss: 11149.00, average training loss: 11483.50, base loss: 19789.76
[INFO 2017-06-27 20:55:44,930 main.py:51] epoch 4746, training loss: 12363.36, average training loss: 11484.62, base loss: 19791.87
[INFO 2017-06-27 20:55:45,305 main.py:51] epoch 4747, training loss: 12223.30, average training loss: 11485.45, base loss: 19792.65
[INFO 2017-06-27 20:55:45,681 main.py:51] epoch 4748, training loss: 11450.84, average training loss: 11483.63, base loss: 19790.30
[INFO 2017-06-27 20:55:46,056 main.py:51] epoch 4749, training loss: 10730.84, average training loss: 11482.70, base loss: 19788.41
[INFO 2017-06-27 20:55:46,431 main.py:51] epoch 4750, training loss: 10043.14, average training loss: 11480.95, base loss: 19786.20
[INFO 2017-06-27 20:55:46,810 main.py:51] epoch 4751, training loss: 12366.93, average training loss: 11482.94, base loss: 19790.16
[INFO 2017-06-27 20:55:47,185 main.py:51] epoch 4752, training loss: 13346.54, average training loss: 11486.39, base loss: 19798.51
[INFO 2017-06-27 20:55:47,556 main.py:51] epoch 4753, training loss: 11833.17, average training loss: 11487.17, base loss: 19800.84
[INFO 2017-06-27 20:55:47,928 main.py:51] epoch 4754, training loss: 12206.26, average training loss: 11487.16, base loss: 19801.06
[INFO 2017-06-27 20:55:48,300 main.py:51] epoch 4755, training loss: 11256.51, average training loss: 11486.51, base loss: 19801.29
[INFO 2017-06-27 20:55:48,677 main.py:51] epoch 4756, training loss: 9702.81, average training loss: 11486.44, base loss: 19801.26
[INFO 2017-06-27 20:55:49,052 main.py:51] epoch 4757, training loss: 9607.46, average training loss: 11485.66, base loss: 19799.51
[INFO 2017-06-27 20:55:49,430 main.py:51] epoch 4758, training loss: 10019.16, average training loss: 11485.20, base loss: 19798.28
[INFO 2017-06-27 20:55:49,805 main.py:51] epoch 4759, training loss: 9707.12, average training loss: 11481.13, base loss: 19790.40
[INFO 2017-06-27 20:55:50,186 main.py:51] epoch 4760, training loss: 11871.11, average training loss: 11483.97, base loss: 19796.55
[INFO 2017-06-27 20:55:50,593 main.py:51] epoch 4761, training loss: 11679.18, average training loss: 11483.77, base loss: 19797.34
[INFO 2017-06-27 20:55:50,979 main.py:51] epoch 4762, training loss: 12325.50, average training loss: 11485.11, base loss: 19799.32
[INFO 2017-06-27 20:55:51,353 main.py:51] epoch 4763, training loss: 13428.06, average training loss: 11488.01, base loss: 19805.94
[INFO 2017-06-27 20:55:51,730 main.py:51] epoch 4764, training loss: 11834.24, average training loss: 11487.82, base loss: 19805.92
[INFO 2017-06-27 20:55:52,104 main.py:51] epoch 4765, training loss: 10468.84, average training loss: 11485.67, base loss: 19801.61
[INFO 2017-06-27 20:55:52,475 main.py:51] epoch 4766, training loss: 12787.79, average training loss: 11486.23, base loss: 19803.35
[INFO 2017-06-27 20:55:52,849 main.py:51] epoch 4767, training loss: 11893.28, average training loss: 11485.16, base loss: 19801.36
[INFO 2017-06-27 20:55:53,222 main.py:51] epoch 4768, training loss: 13173.80, average training loss: 11486.95, base loss: 19804.18
[INFO 2017-06-27 20:55:53,605 main.py:51] epoch 4769, training loss: 12319.95, average training loss: 11488.75, base loss: 19807.75
[INFO 2017-06-27 20:55:53,978 main.py:51] epoch 4770, training loss: 10478.22, average training loss: 11488.09, base loss: 19807.64
[INFO 2017-06-27 20:55:54,353 main.py:51] epoch 4771, training loss: 12100.33, average training loss: 11488.65, base loss: 19809.19
[INFO 2017-06-27 20:55:54,727 main.py:51] epoch 4772, training loss: 11612.50, average training loss: 11488.82, base loss: 19809.54
[INFO 2017-06-27 20:55:55,104 main.py:51] epoch 4773, training loss: 10744.27, average training loss: 11487.48, base loss: 19806.20
[INFO 2017-06-27 20:55:55,479 main.py:51] epoch 4774, training loss: 11957.30, average training loss: 11488.03, base loss: 19807.29
[INFO 2017-06-27 20:55:55,855 main.py:51] epoch 4775, training loss: 10419.71, average training loss: 11487.18, base loss: 19807.06
[INFO 2017-06-27 20:55:56,231 main.py:51] epoch 4776, training loss: 11556.70, average training loss: 11486.83, base loss: 19806.94
[INFO 2017-06-27 20:55:56,604 main.py:51] epoch 4777, training loss: 12020.17, average training loss: 11488.62, base loss: 19811.73
[INFO 2017-06-27 20:55:56,978 main.py:51] epoch 4778, training loss: 10265.62, average training loss: 11487.27, base loss: 19809.54
[INFO 2017-06-27 20:55:57,355 main.py:51] epoch 4779, training loss: 11208.40, average training loss: 11485.30, base loss: 19804.94
[INFO 2017-06-27 20:55:57,730 main.py:51] epoch 4780, training loss: 11443.75, average training loss: 11484.88, base loss: 19804.84
[INFO 2017-06-27 20:55:58,114 main.py:51] epoch 4781, training loss: 10730.28, average training loss: 11485.33, base loss: 19803.99
[INFO 2017-06-27 20:55:58,497 main.py:51] epoch 4782, training loss: 10498.26, average training loss: 11483.92, base loss: 19802.06
[INFO 2017-06-27 20:55:58,871 main.py:51] epoch 4783, training loss: 12792.69, average training loss: 11484.12, base loss: 19802.07
[INFO 2017-06-27 20:55:59,246 main.py:51] epoch 4784, training loss: 12272.39, average training loss: 11484.14, base loss: 19804.25
[INFO 2017-06-27 20:55:59,619 main.py:51] epoch 4785, training loss: 10920.85, average training loss: 11482.48, base loss: 19802.49
[INFO 2017-06-27 20:55:59,996 main.py:51] epoch 4786, training loss: 11801.62, average training loss: 11482.53, base loss: 19802.80
[INFO 2017-06-27 20:56:00,374 main.py:51] epoch 4787, training loss: 11114.14, average training loss: 11483.62, base loss: 19806.58
[INFO 2017-06-27 20:56:00,750 main.py:51] epoch 4788, training loss: 11635.62, average training loss: 11480.77, base loss: 19801.31
[INFO 2017-06-27 20:56:01,124 main.py:51] epoch 4789, training loss: 9852.85, average training loss: 11479.06, base loss: 19798.55
[INFO 2017-06-27 20:56:01,496 main.py:51] epoch 4790, training loss: 11839.18, average training loss: 11480.15, base loss: 19801.00
[INFO 2017-06-27 20:56:01,870 main.py:51] epoch 4791, training loss: 11551.24, average training loss: 11481.47, base loss: 19804.82
[INFO 2017-06-27 20:56:02,250 main.py:51] epoch 4792, training loss: 12357.92, average training loss: 11481.67, base loss: 19805.41
[INFO 2017-06-27 20:56:02,626 main.py:51] epoch 4793, training loss: 10259.52, average training loss: 11479.41, base loss: 19801.95
[INFO 2017-06-27 20:56:03,006 main.py:51] epoch 4794, training loss: 12567.10, average training loss: 11479.92, base loss: 19802.96
[INFO 2017-06-27 20:56:03,384 main.py:51] epoch 4795, training loss: 10932.87, average training loss: 11479.61, base loss: 19802.27
[INFO 2017-06-27 20:56:03,760 main.py:51] epoch 4796, training loss: 11988.43, average training loss: 11481.00, base loss: 19805.06
[INFO 2017-06-27 20:56:04,132 main.py:51] epoch 4797, training loss: 10343.99, average training loss: 11479.53, base loss: 19803.20
[INFO 2017-06-27 20:56:04,506 main.py:51] epoch 4798, training loss: 11559.18, average training loss: 11478.46, base loss: 19801.51
[INFO 2017-06-27 20:56:04,885 main.py:51] epoch 4799, training loss: 10362.61, average training loss: 11478.70, base loss: 19803.00
[INFO 2017-06-27 20:56:04,885 main.py:53] epoch 4799, testing
[INFO 2017-06-27 20:56:06,506 main.py:105] average testing loss: 11291.00, base loss: 19391.42
[INFO 2017-06-27 20:56:06,507 main.py:106] improve_loss: 8100.42, improve_percent: 0.42
[INFO 2017-06-27 20:56:06,507 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:56:06,881 main.py:51] epoch 4800, training loss: 10539.05, average training loss: 11479.01, base loss: 19802.74
[INFO 2017-06-27 20:56:07,259 main.py:51] epoch 4801, training loss: 11790.01, average training loss: 11479.28, base loss: 19804.25
[INFO 2017-06-27 20:56:07,634 main.py:51] epoch 4802, training loss: 12123.78, average training loss: 11479.57, base loss: 19804.85
[INFO 2017-06-27 20:56:08,009 main.py:51] epoch 4803, training loss: 10961.30, average training loss: 11476.81, base loss: 19799.14
[INFO 2017-06-27 20:56:08,386 main.py:51] epoch 4804, training loss: 11948.03, average training loss: 11477.00, base loss: 19799.67
[INFO 2017-06-27 20:56:08,761 main.py:51] epoch 4805, training loss: 11178.92, average training loss: 11476.98, base loss: 19799.41
[INFO 2017-06-27 20:56:09,135 main.py:51] epoch 4806, training loss: 11057.55, average training loss: 11474.41, base loss: 19796.68
[INFO 2017-06-27 20:56:09,509 main.py:51] epoch 4807, training loss: 11688.52, average training loss: 11473.76, base loss: 19795.36
[INFO 2017-06-27 20:56:09,883 main.py:51] epoch 4808, training loss: 11983.34, average training loss: 11475.01, base loss: 19796.44
[INFO 2017-06-27 20:56:10,261 main.py:51] epoch 4809, training loss: 10994.93, average training loss: 11474.65, base loss: 19795.77
[INFO 2017-06-27 20:56:10,638 main.py:51] epoch 4810, training loss: 10301.68, average training loss: 11472.91, base loss: 19792.17
[INFO 2017-06-27 20:56:11,018 main.py:51] epoch 4811, training loss: 11919.99, average training loss: 11474.24, base loss: 19795.81
[INFO 2017-06-27 20:56:11,398 main.py:51] epoch 4812, training loss: 11708.89, average training loss: 11473.35, base loss: 19796.65
[INFO 2017-06-27 20:56:11,775 main.py:51] epoch 4813, training loss: 10674.23, average training loss: 11473.22, base loss: 19796.48
[INFO 2017-06-27 20:56:12,153 main.py:51] epoch 4814, training loss: 11052.72, average training loss: 11472.99, base loss: 19797.28
[INFO 2017-06-27 20:56:12,533 main.py:51] epoch 4815, training loss: 10989.37, average training loss: 11469.91, base loss: 19792.17
[INFO 2017-06-27 20:56:12,911 main.py:51] epoch 4816, training loss: 12397.68, average training loss: 11470.04, base loss: 19792.95
[INFO 2017-06-27 20:56:13,291 main.py:51] epoch 4817, training loss: 11119.14, average training loss: 11470.44, base loss: 19795.12
[INFO 2017-06-27 20:56:13,667 main.py:51] epoch 4818, training loss: 12261.22, average training loss: 11472.28, base loss: 19800.59
[INFO 2017-06-27 20:56:14,037 main.py:51] epoch 4819, training loss: 12580.18, average training loss: 11472.62, base loss: 19801.31
[INFO 2017-06-27 20:56:14,413 main.py:51] epoch 4820, training loss: 11268.27, average training loss: 11472.23, base loss: 19800.14
[INFO 2017-06-27 20:56:14,789 main.py:51] epoch 4821, training loss: 11040.10, average training loss: 11471.47, base loss: 19797.60
[INFO 2017-06-27 20:56:15,160 main.py:51] epoch 4822, training loss: 12491.89, average training loss: 11472.89, base loss: 19800.34
[INFO 2017-06-27 20:56:15,533 main.py:51] epoch 4823, training loss: 10302.23, average training loss: 11473.47, base loss: 19802.76
[INFO 2017-06-27 20:56:15,912 main.py:51] epoch 4824, training loss: 11476.54, average training loss: 11473.95, base loss: 19804.34
[INFO 2017-06-27 20:56:16,288 main.py:51] epoch 4825, training loss: 11846.52, average training loss: 11473.99, base loss: 19806.61
[INFO 2017-06-27 20:56:16,662 main.py:51] epoch 4826, training loss: 10730.90, average training loss: 11473.89, base loss: 19806.50
[INFO 2017-06-27 20:56:17,034 main.py:51] epoch 4827, training loss: 12811.42, average training loss: 11474.28, base loss: 19808.81
[INFO 2017-06-27 20:56:17,411 main.py:51] epoch 4828, training loss: 11048.25, average training loss: 11474.11, base loss: 19809.61
[INFO 2017-06-27 20:56:17,790 main.py:51] epoch 4829, training loss: 11092.16, average training loss: 11475.19, base loss: 19812.01
[INFO 2017-06-27 20:56:18,164 main.py:51] epoch 4830, training loss: 12301.97, average training loss: 11476.15, base loss: 19814.38
[INFO 2017-06-27 20:56:18,540 main.py:51] epoch 4831, training loss: 11553.13, average training loss: 11475.56, base loss: 19812.45
[INFO 2017-06-27 20:56:18,913 main.py:51] epoch 4832, training loss: 10066.86, average training loss: 11474.90, base loss: 19811.86
[INFO 2017-06-27 20:56:19,286 main.py:51] epoch 4833, training loss: 11346.78, average training loss: 11473.58, base loss: 19809.45
[INFO 2017-06-27 20:56:19,658 main.py:51] epoch 4834, training loss: 10443.84, average training loss: 11472.87, base loss: 19808.91
[INFO 2017-06-27 20:56:20,032 main.py:51] epoch 4835, training loss: 11478.10, average training loss: 11472.03, base loss: 19807.57
[INFO 2017-06-27 20:56:20,412 main.py:51] epoch 4836, training loss: 10172.89, average training loss: 11470.95, base loss: 19804.90
[INFO 2017-06-27 20:56:20,789 main.py:51] epoch 4837, training loss: 11095.22, average training loss: 11471.78, base loss: 19806.40
[INFO 2017-06-27 20:56:21,165 main.py:51] epoch 4838, training loss: 10793.37, average training loss: 11470.83, base loss: 19806.37
[INFO 2017-06-27 20:56:21,542 main.py:51] epoch 4839, training loss: 11164.86, average training loss: 11469.43, base loss: 19803.96
[INFO 2017-06-27 20:56:21,914 main.py:51] epoch 4840, training loss: 11151.16, average training loss: 11468.20, base loss: 19803.42
[INFO 2017-06-27 20:56:22,289 main.py:51] epoch 4841, training loss: 10723.33, average training loss: 11466.89, base loss: 19799.67
[INFO 2017-06-27 20:56:22,669 main.py:51] epoch 4842, training loss: 10925.87, average training loss: 11468.13, base loss: 19802.02
[INFO 2017-06-27 20:56:23,044 main.py:51] epoch 4843, training loss: 12405.83, average training loss: 11469.92, base loss: 19805.49
[INFO 2017-06-27 20:56:23,418 main.py:51] epoch 4844, training loss: 11431.47, average training loss: 11470.49, base loss: 19806.63
[INFO 2017-06-27 20:56:23,794 main.py:51] epoch 4845, training loss: 11486.54, average training loss: 11469.77, base loss: 19804.70
[INFO 2017-06-27 20:56:24,170 main.py:51] epoch 4846, training loss: 10803.88, average training loss: 11468.07, base loss: 19801.72
[INFO 2017-06-27 20:56:24,544 main.py:51] epoch 4847, training loss: 10198.06, average training loss: 11466.41, base loss: 19799.90
[INFO 2017-06-27 20:56:24,919 main.py:51] epoch 4848, training loss: 11577.51, average training loss: 11465.69, base loss: 19798.79
[INFO 2017-06-27 20:56:25,299 main.py:51] epoch 4849, training loss: 12114.90, average training loss: 11466.69, base loss: 19801.27
[INFO 2017-06-27 20:56:25,675 main.py:51] epoch 4850, training loss: 10774.36, average training loss: 11464.98, base loss: 19796.60
[INFO 2017-06-27 20:56:26,051 main.py:51] epoch 4851, training loss: 9914.87, average training loss: 11462.58, base loss: 19791.38
[INFO 2017-06-27 20:56:26,430 main.py:51] epoch 4852, training loss: 12776.43, average training loss: 11463.26, base loss: 19793.25
[INFO 2017-06-27 20:56:26,806 main.py:51] epoch 4853, training loss: 11308.40, average training loss: 11462.51, base loss: 19791.82
[INFO 2017-06-27 20:56:27,185 main.py:51] epoch 4854, training loss: 10378.57, average training loss: 11461.98, base loss: 19790.92
[INFO 2017-06-27 20:56:27,561 main.py:51] epoch 4855, training loss: 10548.21, average training loss: 11461.53, base loss: 19789.53
[INFO 2017-06-27 20:56:27,939 main.py:51] epoch 4856, training loss: 10490.67, average training loss: 11459.72, base loss: 19786.19
[INFO 2017-06-27 20:56:28,315 main.py:51] epoch 4857, training loss: 11019.28, average training loss: 11458.72, base loss: 19784.92
[INFO 2017-06-27 20:56:28,688 main.py:51] epoch 4858, training loss: 11030.40, average training loss: 11458.28, base loss: 19786.08
[INFO 2017-06-27 20:56:29,065 main.py:51] epoch 4859, training loss: 12057.83, average training loss: 11457.54, base loss: 19784.66
[INFO 2017-06-27 20:56:29,441 main.py:51] epoch 4860, training loss: 12560.15, average training loss: 11458.29, base loss: 19786.51
[INFO 2017-06-27 20:56:29,822 main.py:51] epoch 4861, training loss: 9538.99, average training loss: 11455.62, base loss: 19780.59
[INFO 2017-06-27 20:56:30,199 main.py:51] epoch 4862, training loss: 11790.09, average training loss: 11455.75, base loss: 19780.51
[INFO 2017-06-27 20:56:30,576 main.py:51] epoch 4863, training loss: 12375.92, average training loss: 11455.50, base loss: 19779.44
[INFO 2017-06-27 20:56:30,952 main.py:51] epoch 4864, training loss: 12201.59, average training loss: 11456.91, base loss: 19782.10
[INFO 2017-06-27 20:56:31,326 main.py:51] epoch 4865, training loss: 10744.70, average training loss: 11453.23, base loss: 19775.83
[INFO 2017-06-27 20:56:31,697 main.py:51] epoch 4866, training loss: 12772.20, average training loss: 11454.00, base loss: 19778.36
[INFO 2017-06-27 20:56:32,074 main.py:51] epoch 4867, training loss: 11548.54, average training loss: 11453.78, base loss: 19779.05
[INFO 2017-06-27 20:56:32,450 main.py:51] epoch 4868, training loss: 11712.81, average training loss: 11455.12, base loss: 19783.08
[INFO 2017-06-27 20:56:32,823 main.py:51] epoch 4869, training loss: 11903.85, average training loss: 11455.19, base loss: 19783.11
[INFO 2017-06-27 20:56:33,202 main.py:51] epoch 4870, training loss: 11069.46, average training loss: 11455.36, base loss: 19783.62
[INFO 2017-06-27 20:56:33,572 main.py:51] epoch 4871, training loss: 12394.77, average training loss: 11455.06, base loss: 19785.27
[INFO 2017-06-27 20:56:33,949 main.py:51] epoch 4872, training loss: 10780.02, average training loss: 11455.05, base loss: 19785.44
[INFO 2017-06-27 20:56:34,329 main.py:51] epoch 4873, training loss: 11061.81, average training loss: 11454.24, base loss: 19783.32
[INFO 2017-06-27 20:56:34,701 main.py:51] epoch 4874, training loss: 10962.45, average training loss: 11453.87, base loss: 19781.15
[INFO 2017-06-27 20:56:35,076 main.py:51] epoch 4875, training loss: 10106.07, average training loss: 11451.53, base loss: 19777.11
[INFO 2017-06-27 20:56:35,452 main.py:51] epoch 4876, training loss: 11737.58, average training loss: 11451.57, base loss: 19778.00
[INFO 2017-06-27 20:56:35,829 main.py:51] epoch 4877, training loss: 11212.85, average training loss: 11451.05, base loss: 19776.95
[INFO 2017-06-27 20:56:36,202 main.py:51] epoch 4878, training loss: 11448.04, average training loss: 11451.47, base loss: 19777.76
[INFO 2017-06-27 20:56:36,575 main.py:51] epoch 4879, training loss: 11354.57, average training loss: 11450.53, base loss: 19776.70
[INFO 2017-06-27 20:56:36,947 main.py:51] epoch 4880, training loss: 11444.07, average training loss: 11450.77, base loss: 19778.79
[INFO 2017-06-27 20:56:37,322 main.py:51] epoch 4881, training loss: 11493.95, average training loss: 11449.35, base loss: 19775.27
[INFO 2017-06-27 20:56:37,693 main.py:51] epoch 4882, training loss: 11267.27, average training loss: 11449.50, base loss: 19776.54
[INFO 2017-06-27 20:56:38,073 main.py:51] epoch 4883, training loss: 11516.01, average training loss: 11448.73, base loss: 19773.95
[INFO 2017-06-27 20:56:38,445 main.py:51] epoch 4884, training loss: 12333.87, average training loss: 11449.74, base loss: 19776.51
[INFO 2017-06-27 20:56:38,819 main.py:51] epoch 4885, training loss: 10872.92, average training loss: 11448.37, base loss: 19775.59
[INFO 2017-06-27 20:56:39,194 main.py:51] epoch 4886, training loss: 10810.21, average training loss: 11446.73, base loss: 19773.68
[INFO 2017-06-27 20:56:39,570 main.py:51] epoch 4887, training loss: 12372.79, average training loss: 11450.06, base loss: 19780.25
[INFO 2017-06-27 20:56:39,942 main.py:51] epoch 4888, training loss: 12123.78, average training loss: 11449.63, base loss: 19780.84
[INFO 2017-06-27 20:56:40,314 main.py:51] epoch 4889, training loss: 12407.91, average training loss: 11450.53, base loss: 19783.03
[INFO 2017-06-27 20:56:40,686 main.py:51] epoch 4890, training loss: 10674.67, average training loss: 11448.48, base loss: 19778.86
[INFO 2017-06-27 20:56:41,057 main.py:51] epoch 4891, training loss: 11786.42, average training loss: 11448.72, base loss: 19780.36
[INFO 2017-06-27 20:56:41,429 main.py:51] epoch 4892, training loss: 10968.21, average training loss: 11448.56, base loss: 19781.15
[INFO 2017-06-27 20:56:41,802 main.py:51] epoch 4893, training loss: 11311.92, average training loss: 11448.95, base loss: 19784.60
[INFO 2017-06-27 20:56:42,177 main.py:51] epoch 4894, training loss: 12391.00, average training loss: 11448.20, base loss: 19783.70
[INFO 2017-06-27 20:56:42,553 main.py:51] epoch 4895, training loss: 10779.36, average training loss: 11447.32, base loss: 19782.66
[INFO 2017-06-27 20:56:42,928 main.py:51] epoch 4896, training loss: 9557.61, average training loss: 11445.33, base loss: 19777.99
[INFO 2017-06-27 20:56:43,305 main.py:51] epoch 4897, training loss: 10867.87, average training loss: 11445.24, base loss: 19778.32
[INFO 2017-06-27 20:56:43,687 main.py:51] epoch 4898, training loss: 10503.44, average training loss: 11444.52, base loss: 19778.42
[INFO 2017-06-27 20:56:44,064 main.py:51] epoch 4899, training loss: 10935.77, average training loss: 11444.89, base loss: 19779.60
[INFO 2017-06-27 20:56:44,065 main.py:53] epoch 4899, testing
[INFO 2017-06-27 20:56:45,676 main.py:105] average testing loss: 11288.66, base loss: 20045.53
[INFO 2017-06-27 20:56:45,676 main.py:106] improve_loss: 8756.87, improve_percent: 0.44
[INFO 2017-06-27 20:56:45,676 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:56:46,051 main.py:51] epoch 4900, training loss: 11261.50, average training loss: 11446.43, base loss: 19782.96
[INFO 2017-06-27 20:56:46,511 main.py:51] epoch 4901, training loss: 10974.08, average training loss: 11446.37, base loss: 19785.01
[INFO 2017-06-27 20:56:46,934 main.py:51] epoch 4902, training loss: 11869.96, average training loss: 11445.89, base loss: 19784.98
[INFO 2017-06-27 20:56:47,313 main.py:51] epoch 4903, training loss: 10857.20, average training loss: 11445.88, base loss: 19786.33
[INFO 2017-06-27 20:56:47,691 main.py:51] epoch 4904, training loss: 10588.59, average training loss: 11445.18, base loss: 19783.96
[INFO 2017-06-27 20:56:48,071 main.py:51] epoch 4905, training loss: 12285.11, average training loss: 11445.41, base loss: 19784.87
[INFO 2017-06-27 20:56:48,457 main.py:51] epoch 4906, training loss: 11690.03, average training loss: 11447.09, base loss: 19788.52
[INFO 2017-06-27 20:56:48,837 main.py:51] epoch 4907, training loss: 13189.38, average training loss: 11447.10, base loss: 19788.64
[INFO 2017-06-27 20:56:49,212 main.py:51] epoch 4908, training loss: 13428.92, average training loss: 11448.40, base loss: 19790.25
[INFO 2017-06-27 20:56:49,586 main.py:51] epoch 4909, training loss: 11538.84, average training loss: 11449.93, base loss: 19792.76
[INFO 2017-06-27 20:56:49,961 main.py:51] epoch 4910, training loss: 10464.86, average training loss: 11448.54, base loss: 19791.55
[INFO 2017-06-27 20:56:50,339 main.py:51] epoch 4911, training loss: 11809.31, average training loss: 11450.17, base loss: 19795.02
[INFO 2017-06-27 20:56:50,712 main.py:51] epoch 4912, training loss: 10237.83, average training loss: 11449.53, base loss: 19794.18
[INFO 2017-06-27 20:56:51,182 main.py:51] epoch 4913, training loss: 12156.35, average training loss: 11449.13, base loss: 19795.11
[INFO 2017-06-27 20:56:51,577 main.py:51] epoch 4914, training loss: 10845.81, average training loss: 11449.51, base loss: 19797.52
[INFO 2017-06-27 20:56:51,978 main.py:51] epoch 4915, training loss: 11844.81, average training loss: 11448.27, base loss: 19795.04
[INFO 2017-06-27 20:56:52,357 main.py:51] epoch 4916, training loss: 11733.62, average training loss: 11447.98, base loss: 19794.14
[INFO 2017-06-27 20:56:52,734 main.py:51] epoch 4917, training loss: 9963.99, average training loss: 11445.60, base loss: 19790.26
[INFO 2017-06-27 20:56:53,111 main.py:51] epoch 4918, training loss: 12723.96, average training loss: 11446.34, base loss: 19792.42
[INFO 2017-06-27 20:56:53,492 main.py:51] epoch 4919, training loss: 10412.01, average training loss: 11444.02, base loss: 19789.04
[INFO 2017-06-27 20:56:53,865 main.py:51] epoch 4920, training loss: 13735.64, average training loss: 11444.86, base loss: 19790.62
[INFO 2017-06-27 20:56:54,341 main.py:51] epoch 4921, training loss: 12077.48, average training loss: 11445.82, base loss: 19793.31
[INFO 2017-06-27 20:56:54,736 main.py:51] epoch 4922, training loss: 12499.99, average training loss: 11446.77, base loss: 19795.47
[INFO 2017-06-27 20:56:55,114 main.py:51] epoch 4923, training loss: 13386.52, average training loss: 11449.76, base loss: 19802.25
[INFO 2017-06-27 20:56:55,602 main.py:51] epoch 4924, training loss: 10062.76, average training loss: 11448.03, base loss: 19797.31
[INFO 2017-06-27 20:56:55,985 main.py:51] epoch 4925, training loss: 11237.15, average training loss: 11448.14, base loss: 19799.02
[INFO 2017-06-27 20:56:56,457 main.py:51] epoch 4926, training loss: 10174.65, average training loss: 11446.91, base loss: 19796.40
[INFO 2017-06-27 20:56:56,890 main.py:51] epoch 4927, training loss: 9252.09, average training loss: 11445.46, base loss: 19794.52
[INFO 2017-06-27 20:56:57,294 main.py:51] epoch 4928, training loss: 11741.12, average training loss: 11445.75, base loss: 19794.86
[INFO 2017-06-27 20:56:57,734 main.py:51] epoch 4929, training loss: 10323.65, average training loss: 11445.47, base loss: 19794.38
[INFO 2017-06-27 20:56:58,145 main.py:51] epoch 4930, training loss: 11965.63, average training loss: 11446.68, base loss: 19796.91
[INFO 2017-06-27 20:56:58,529 main.py:51] epoch 4931, training loss: 10522.15, average training loss: 11447.07, base loss: 19797.82
[INFO 2017-06-27 20:56:58,919 main.py:51] epoch 4932, training loss: 9791.07, average training loss: 11445.73, base loss: 19795.29
[INFO 2017-06-27 20:56:59,299 main.py:51] epoch 4933, training loss: 9616.81, average training loss: 11443.21, base loss: 19790.66
[INFO 2017-06-27 20:56:59,672 main.py:51] epoch 4934, training loss: 10885.08, average training loss: 11441.77, base loss: 19790.23
[INFO 2017-06-27 20:57:00,049 main.py:51] epoch 4935, training loss: 11316.12, average training loss: 11442.20, base loss: 19793.50
[INFO 2017-06-27 20:57:00,526 main.py:51] epoch 4936, training loss: 10408.59, average training loss: 11441.58, base loss: 19792.36
[INFO 2017-06-27 20:57:00,932 main.py:51] epoch 4937, training loss: 11929.44, average training loss: 11444.08, base loss: 19798.41
[INFO 2017-06-27 20:57:01,410 main.py:51] epoch 4938, training loss: 11055.39, average training loss: 11444.22, base loss: 19799.43
[INFO 2017-06-27 20:57:01,811 main.py:51] epoch 4939, training loss: 10913.38, average training loss: 11444.37, base loss: 19799.92
[INFO 2017-06-27 20:57:02,191 main.py:51] epoch 4940, training loss: 11917.69, average training loss: 11444.24, base loss: 19800.54
[INFO 2017-06-27 20:57:02,635 main.py:51] epoch 4941, training loss: 12215.14, average training loss: 11445.01, base loss: 19802.47
[INFO 2017-06-27 20:57:03,044 main.py:51] epoch 4942, training loss: 11311.97, average training loss: 11444.50, base loss: 19802.33
[INFO 2017-06-27 20:57:03,424 main.py:51] epoch 4943, training loss: 11746.24, average training loss: 11445.30, base loss: 19806.32
[INFO 2017-06-27 20:57:03,828 main.py:51] epoch 4944, training loss: 11941.23, average training loss: 11445.84, base loss: 19807.97
[INFO 2017-06-27 20:57:04,206 main.py:51] epoch 4945, training loss: 11081.53, average training loss: 11444.70, base loss: 19806.46
[INFO 2017-06-27 20:57:04,591 main.py:51] epoch 4946, training loss: 10500.55, average training loss: 11443.28, base loss: 19802.93
[INFO 2017-06-27 20:57:04,970 main.py:51] epoch 4947, training loss: 12516.07, average training loss: 11443.94, base loss: 19804.82
[INFO 2017-06-27 20:57:05,347 main.py:51] epoch 4948, training loss: 11772.97, average training loss: 11444.87, base loss: 19807.47
[INFO 2017-06-27 20:57:05,779 main.py:51] epoch 4949, training loss: 11888.16, average training loss: 11445.12, base loss: 19807.62
[INFO 2017-06-27 20:57:06,193 main.py:51] epoch 4950, training loss: 10171.60, average training loss: 11444.20, base loss: 19806.15
[INFO 2017-06-27 20:57:06,580 main.py:51] epoch 4951, training loss: 10190.83, average training loss: 11439.98, base loss: 19797.75
[INFO 2017-06-27 20:57:07,043 main.py:51] epoch 4952, training loss: 11555.92, average training loss: 11439.35, base loss: 19797.22
[INFO 2017-06-27 20:57:07,430 main.py:51] epoch 4953, training loss: 12567.74, average training loss: 11439.71, base loss: 19800.23
[INFO 2017-06-27 20:57:07,816 main.py:51] epoch 4954, training loss: 10758.99, average training loss: 11438.94, base loss: 19798.18
[INFO 2017-06-27 20:57:08,216 main.py:51] epoch 4955, training loss: 11470.34, average training loss: 11439.31, base loss: 19799.17
[INFO 2017-06-27 20:57:08,621 main.py:51] epoch 4956, training loss: 11488.48, average training loss: 11439.21, base loss: 19800.93
[INFO 2017-06-27 20:57:09,000 main.py:51] epoch 4957, training loss: 11575.39, average training loss: 11438.88, base loss: 19800.56
[INFO 2017-06-27 20:57:09,383 main.py:51] epoch 4958, training loss: 12396.21, average training loss: 11440.33, base loss: 19802.84
[INFO 2017-06-27 20:57:09,756 main.py:51] epoch 4959, training loss: 9860.26, average training loss: 11438.25, base loss: 19797.63
[INFO 2017-06-27 20:57:10,135 main.py:51] epoch 4960, training loss: 10402.27, average training loss: 11436.00, base loss: 19792.00
[INFO 2017-06-27 20:57:10,521 main.py:51] epoch 4961, training loss: 11677.78, average training loss: 11436.84, base loss: 19793.75
[INFO 2017-06-27 20:57:10,901 main.py:51] epoch 4962, training loss: 11548.40, average training loss: 11434.54, base loss: 19789.88
[INFO 2017-06-27 20:57:11,278 main.py:51] epoch 4963, training loss: 10529.43, average training loss: 11434.13, base loss: 19790.42
[INFO 2017-06-27 20:57:11,656 main.py:51] epoch 4964, training loss: 10829.12, average training loss: 11433.36, base loss: 19788.74
[INFO 2017-06-27 20:57:12,034 main.py:51] epoch 4965, training loss: 10788.12, average training loss: 11431.71, base loss: 19785.66
[INFO 2017-06-27 20:57:12,412 main.py:51] epoch 4966, training loss: 11538.04, average training loss: 11432.84, base loss: 19788.01
[INFO 2017-06-27 20:57:12,790 main.py:51] epoch 4967, training loss: 11756.33, average training loss: 11431.48, base loss: 19784.10
[INFO 2017-06-27 20:57:13,165 main.py:51] epoch 4968, training loss: 10565.63, average training loss: 11432.47, base loss: 19785.91
[INFO 2017-06-27 20:57:13,549 main.py:51] epoch 4969, training loss: 11283.75, average training loss: 11432.42, base loss: 19788.31
[INFO 2017-06-27 20:57:13,930 main.py:51] epoch 4970, training loss: 11387.27, average training loss: 11432.92, base loss: 19790.14
[INFO 2017-06-27 20:57:14,303 main.py:51] epoch 4971, training loss: 9753.62, average training loss: 11430.54, base loss: 19785.13
[INFO 2017-06-27 20:57:14,681 main.py:51] epoch 4972, training loss: 11663.43, average training loss: 11430.13, base loss: 19784.19
[INFO 2017-06-27 20:57:15,059 main.py:51] epoch 4973, training loss: 11760.57, average training loss: 11430.71, base loss: 19784.75
[INFO 2017-06-27 20:57:15,538 main.py:51] epoch 4974, training loss: 10887.35, average training loss: 11428.98, base loss: 19781.11
[INFO 2017-06-27 20:57:15,939 main.py:51] epoch 4975, training loss: 13070.08, average training loss: 11430.43, base loss: 19784.11
[INFO 2017-06-27 20:57:16,321 main.py:51] epoch 4976, training loss: 11428.56, average training loss: 11430.41, base loss: 19784.49
[INFO 2017-06-27 20:57:16,710 main.py:51] epoch 4977, training loss: 11099.16, average training loss: 11431.06, base loss: 19784.72
[INFO 2017-06-27 20:57:17,096 main.py:51] epoch 4978, training loss: 14269.75, average training loss: 11433.97, base loss: 19792.09
[INFO 2017-06-27 20:57:17,476 main.py:51] epoch 4979, training loss: 10761.75, average training loss: 11433.73, base loss: 19791.29
[INFO 2017-06-27 20:57:17,853 main.py:51] epoch 4980, training loss: 10572.11, average training loss: 11432.17, base loss: 19788.11
[INFO 2017-06-27 20:57:18,231 main.py:51] epoch 4981, training loss: 11781.78, average training loss: 11433.11, base loss: 19790.34
[INFO 2017-06-27 20:57:18,607 main.py:51] epoch 4982, training loss: 12344.19, average training loss: 11434.58, base loss: 19795.12
[INFO 2017-06-27 20:57:18,983 main.py:51] epoch 4983, training loss: 12183.62, average training loss: 11435.35, base loss: 19796.35
[INFO 2017-06-27 20:57:19,361 main.py:51] epoch 4984, training loss: 11868.11, average training loss: 11435.20, base loss: 19796.47
[INFO 2017-06-27 20:57:19,773 main.py:51] epoch 4985, training loss: 11721.57, average training loss: 11435.95, base loss: 19796.93
[INFO 2017-06-27 20:57:20,152 main.py:51] epoch 4986, training loss: 9888.84, average training loss: 11434.83, base loss: 19796.02
[INFO 2017-06-27 20:57:20,527 main.py:51] epoch 4987, training loss: 10184.66, average training loss: 11433.93, base loss: 19792.90
[INFO 2017-06-27 20:57:20,904 main.py:51] epoch 4988, training loss: 11448.63, average training loss: 11433.55, base loss: 19792.35
[INFO 2017-06-27 20:57:21,284 main.py:51] epoch 4989, training loss: 11050.78, average training loss: 11433.29, base loss: 19792.59
[INFO 2017-06-27 20:57:21,656 main.py:51] epoch 4990, training loss: 11730.40, average training loss: 11433.68, base loss: 19793.96
[INFO 2017-06-27 20:57:22,030 main.py:51] epoch 4991, training loss: 12047.15, average training loss: 11433.28, base loss: 19793.36
[INFO 2017-06-27 20:57:22,404 main.py:51] epoch 4992, training loss: 14012.37, average training loss: 11435.31, base loss: 19797.08
[INFO 2017-06-27 20:57:22,781 main.py:51] epoch 4993, training loss: 10811.64, average training loss: 11434.74, base loss: 19796.28
[INFO 2017-06-27 20:57:23,156 main.py:51] epoch 4994, training loss: 12399.97, average training loss: 11436.80, base loss: 19801.57
[INFO 2017-06-27 20:57:23,535 main.py:51] epoch 4995, training loss: 11883.32, average training loss: 11437.82, base loss: 19803.44
[INFO 2017-06-27 20:57:23,908 main.py:51] epoch 4996, training loss: 11423.99, average training loss: 11436.52, base loss: 19799.68
[INFO 2017-06-27 20:57:24,281 main.py:51] epoch 4997, training loss: 11834.03, average training loss: 11435.58, base loss: 19799.15
[INFO 2017-06-27 20:57:24,652 main.py:51] epoch 4998, training loss: 10258.40, average training loss: 11434.48, base loss: 19798.04
[INFO 2017-06-27 20:57:25,030 main.py:51] epoch 4999, training loss: 11218.75, average training loss: 11434.66, base loss: 19797.98
[INFO 2017-06-27 20:57:25,030 main.py:53] epoch 4999, testing
[INFO 2017-06-27 20:57:26,641 main.py:105] average testing loss: 11422.90, base loss: 19849.13
[INFO 2017-06-27 20:57:26,641 main.py:106] improve_loss: 8426.23, improve_percent: 0.42
[INFO 2017-06-27 20:57:26,641 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:57:27,017 main.py:51] epoch 5000, training loss: 12615.59, average training loss: 11435.40, base loss: 19799.92
[INFO 2017-06-27 20:57:27,398 main.py:51] epoch 5001, training loss: 9721.16, average training loss: 11434.34, base loss: 19797.98
[INFO 2017-06-27 20:57:27,774 main.py:51] epoch 5002, training loss: 11356.07, average training loss: 11434.93, base loss: 19801.07
[INFO 2017-06-27 20:57:28,150 main.py:51] epoch 5003, training loss: 12065.30, average training loss: 11435.82, base loss: 19803.79
[INFO 2017-06-27 20:57:28,528 main.py:51] epoch 5004, training loss: 10071.58, average training loss: 11434.35, base loss: 19800.42
[INFO 2017-06-27 20:57:28,909 main.py:51] epoch 5005, training loss: 11877.38, average training loss: 11434.89, base loss: 19802.99
[INFO 2017-06-27 20:57:29,284 main.py:51] epoch 5006, training loss: 11539.51, average training loss: 11434.30, base loss: 19801.82
[INFO 2017-06-27 20:57:29,658 main.py:51] epoch 5007, training loss: 10214.26, average training loss: 11432.61, base loss: 19798.14
[INFO 2017-06-27 20:57:30,035 main.py:51] epoch 5008, training loss: 11259.85, average training loss: 11432.48, base loss: 19797.07
[INFO 2017-06-27 20:57:30,411 main.py:51] epoch 5009, training loss: 10628.09, average training loss: 11431.65, base loss: 19795.70
[INFO 2017-06-27 20:57:30,784 main.py:51] epoch 5010, training loss: 11410.66, average training loss: 11431.49, base loss: 19796.42
[INFO 2017-06-27 20:57:31,161 main.py:51] epoch 5011, training loss: 10585.19, average training loss: 11430.90, base loss: 19795.39
[INFO 2017-06-27 20:57:31,538 main.py:51] epoch 5012, training loss: 11909.44, average training loss: 11430.44, base loss: 19794.08
[INFO 2017-06-27 20:57:31,915 main.py:51] epoch 5013, training loss: 11372.60, average training loss: 11430.30, base loss: 19794.05
[INFO 2017-06-27 20:57:32,291 main.py:51] epoch 5014, training loss: 9949.83, average training loss: 11429.71, base loss: 19793.52
[INFO 2017-06-27 20:57:32,669 main.py:51] epoch 5015, training loss: 10917.68, average training loss: 11429.43, base loss: 19792.70
[INFO 2017-06-27 20:57:33,045 main.py:51] epoch 5016, training loss: 10593.37, average training loss: 11428.53, base loss: 19791.30
[INFO 2017-06-27 20:57:33,426 main.py:51] epoch 5017, training loss: 11437.23, average training loss: 11429.42, base loss: 19794.13
[INFO 2017-06-27 20:57:33,801 main.py:51] epoch 5018, training loss: 10166.15, average training loss: 11428.25, base loss: 19791.32
[INFO 2017-06-27 20:57:34,186 main.py:51] epoch 5019, training loss: 12013.19, average training loss: 11428.97, base loss: 19791.55
[INFO 2017-06-27 20:57:34,563 main.py:51] epoch 5020, training loss: 11726.87, average training loss: 11428.87, base loss: 19792.91
[INFO 2017-06-27 20:57:34,941 main.py:51] epoch 5021, training loss: 10906.29, average training loss: 11428.42, base loss: 19791.51
[INFO 2017-06-27 20:57:35,315 main.py:51] epoch 5022, training loss: 11834.50, average training loss: 11429.28, base loss: 19793.79
[INFO 2017-06-27 20:57:35,692 main.py:51] epoch 5023, training loss: 9909.41, average training loss: 11427.29, base loss: 19789.12
[INFO 2017-06-27 20:57:36,069 main.py:51] epoch 5024, training loss: 10759.01, average training loss: 11427.59, base loss: 19790.37
[INFO 2017-06-27 20:57:36,446 main.py:51] epoch 5025, training loss: 10223.78, average training loss: 11427.43, base loss: 19791.06
[INFO 2017-06-27 20:57:36,820 main.py:51] epoch 5026, training loss: 11539.75, average training loss: 11427.07, base loss: 19790.25
[INFO 2017-06-27 20:57:37,196 main.py:51] epoch 5027, training loss: 11479.50, average training loss: 11427.41, base loss: 19791.41
[INFO 2017-06-27 20:57:37,570 main.py:51] epoch 5028, training loss: 9631.69, average training loss: 11424.47, base loss: 19785.98
[INFO 2017-06-27 20:57:37,951 main.py:51] epoch 5029, training loss: 11124.13, average training loss: 11422.47, base loss: 19781.84
[INFO 2017-06-27 20:57:38,431 main.py:51] epoch 5030, training loss: 9796.66, average training loss: 11420.69, base loss: 19778.02
[INFO 2017-06-27 20:57:38,826 main.py:51] epoch 5031, training loss: 12757.17, average training loss: 11420.50, base loss: 19779.74
[INFO 2017-06-27 20:57:39,212 main.py:51] epoch 5032, training loss: 12659.16, average training loss: 11421.36, base loss: 19780.85
[INFO 2017-06-27 20:57:39,693 main.py:51] epoch 5033, training loss: 11960.19, average training loss: 11420.70, base loss: 19779.75
[INFO 2017-06-27 20:57:40,094 main.py:51] epoch 5034, training loss: 12989.14, average training loss: 11421.54, base loss: 19779.75
[INFO 2017-06-27 20:57:40,486 main.py:51] epoch 5035, training loss: 10805.18, average training loss: 11420.10, base loss: 19776.89
[INFO 2017-06-27 20:57:40,877 main.py:51] epoch 5036, training loss: 10731.34, average training loss: 11418.60, base loss: 19774.19
[INFO 2017-06-27 20:57:41,278 main.py:51] epoch 5037, training loss: 9641.83, average training loss: 11416.28, base loss: 19768.93
[INFO 2017-06-27 20:57:41,649 main.py:51] epoch 5038, training loss: 10593.68, average training loss: 11416.54, base loss: 19770.52
[INFO 2017-06-27 20:57:42,032 main.py:51] epoch 5039, training loss: 11085.89, average training loss: 11415.27, base loss: 19768.29
[INFO 2017-06-27 20:57:42,407 main.py:51] epoch 5040, training loss: 10680.22, average training loss: 11412.22, base loss: 19763.94
[INFO 2017-06-27 20:57:42,783 main.py:51] epoch 5041, training loss: 10984.59, average training loss: 11412.39, base loss: 19766.53
[INFO 2017-06-27 20:57:43,165 main.py:51] epoch 5042, training loss: 11622.55, average training loss: 11412.61, base loss: 19767.26
[INFO 2017-06-27 20:57:43,542 main.py:51] epoch 5043, training loss: 11081.40, average training loss: 11411.95, base loss: 19766.60
[INFO 2017-06-27 20:57:43,922 main.py:51] epoch 5044, training loss: 12124.31, average training loss: 11413.29, base loss: 19769.82
[INFO 2017-06-27 20:57:44,298 main.py:51] epoch 5045, training loss: 11615.73, average training loss: 11413.53, base loss: 19771.68
[INFO 2017-06-27 20:57:44,673 main.py:51] epoch 5046, training loss: 10764.67, average training loss: 11413.33, base loss: 19772.10
[INFO 2017-06-27 20:57:45,049 main.py:51] epoch 5047, training loss: 12546.42, average training loss: 11413.58, base loss: 19772.96
[INFO 2017-06-27 20:57:45,424 main.py:51] epoch 5048, training loss: 9608.05, average training loss: 11412.62, base loss: 19771.58
[INFO 2017-06-27 20:57:45,797 main.py:51] epoch 5049, training loss: 9951.38, average training loss: 11411.45, base loss: 19770.25
[INFO 2017-06-27 20:57:46,170 main.py:51] epoch 5050, training loss: 11943.34, average training loss: 11410.77, base loss: 19769.35
[INFO 2017-06-27 20:57:46,551 main.py:51] epoch 5051, training loss: 10614.14, average training loss: 11410.18, base loss: 19769.48
[INFO 2017-06-27 20:57:46,928 main.py:51] epoch 5052, training loss: 13314.76, average training loss: 11411.66, base loss: 19772.81
[INFO 2017-06-27 20:57:47,309 main.py:51] epoch 5053, training loss: 11572.77, average training loss: 11411.66, base loss: 19773.89
[INFO 2017-06-27 20:57:47,686 main.py:51] epoch 5054, training loss: 11527.88, average training loss: 11411.52, base loss: 19775.09
[INFO 2017-06-27 20:57:48,065 main.py:51] epoch 5055, training loss: 10536.38, average training loss: 11410.70, base loss: 19774.87
[INFO 2017-06-27 20:57:48,441 main.py:51] epoch 5056, training loss: 10365.62, average training loss: 11409.38, base loss: 19771.92
[INFO 2017-06-27 20:57:48,819 main.py:51] epoch 5057, training loss: 12321.99, average training loss: 11410.53, base loss: 19774.56
[INFO 2017-06-27 20:57:49,195 main.py:51] epoch 5058, training loss: 10629.33, average training loss: 11409.74, base loss: 19772.23
[INFO 2017-06-27 20:57:49,572 main.py:51] epoch 5059, training loss: 10934.87, average training loss: 11409.00, base loss: 19769.71
[INFO 2017-06-27 20:57:49,948 main.py:51] epoch 5060, training loss: 10459.12, average training loss: 11408.09, base loss: 19768.17
[INFO 2017-06-27 20:57:50,326 main.py:51] epoch 5061, training loss: 10741.14, average training loss: 11406.82, base loss: 19766.47
[INFO 2017-06-27 20:57:50,704 main.py:51] epoch 5062, training loss: 11226.69, average training loss: 11405.70, base loss: 19766.08
[INFO 2017-06-27 20:57:51,080 main.py:51] epoch 5063, training loss: 11558.39, average training loss: 11405.83, base loss: 19767.44
[INFO 2017-06-27 20:57:51,456 main.py:51] epoch 5064, training loss: 10817.41, average training loss: 11405.73, base loss: 19766.12
[INFO 2017-06-27 20:57:51,876 main.py:51] epoch 5065, training loss: 11430.22, average training loss: 11407.27, base loss: 19768.74
[INFO 2017-06-27 20:57:52,289 main.py:51] epoch 5066, training loss: 12527.87, average training loss: 11407.41, base loss: 19768.65
[INFO 2017-06-27 20:57:52,671 main.py:51] epoch 5067, training loss: 10736.35, average training loss: 11406.69, base loss: 19768.47
[INFO 2017-06-27 20:57:53,124 main.py:51] epoch 5068, training loss: 10040.11, average training loss: 11405.28, base loss: 19766.67
[INFO 2017-06-27 20:57:53,540 main.py:51] epoch 5069, training loss: 10536.22, average training loss: 11403.99, base loss: 19764.60
[INFO 2017-06-27 20:57:53,927 main.py:51] epoch 5070, training loss: 10393.55, average training loss: 11403.32, base loss: 19764.13
[INFO 2017-06-27 20:57:54,310 main.py:51] epoch 5071, training loss: 10881.21, average training loss: 11402.79, base loss: 19763.27
[INFO 2017-06-27 20:57:54,691 main.py:51] epoch 5072, training loss: 10903.17, average training loss: 11402.01, base loss: 19761.10
[INFO 2017-06-27 20:57:55,067 main.py:51] epoch 5073, training loss: 10527.24, average training loss: 11401.20, base loss: 19758.52
[INFO 2017-06-27 20:57:55,472 main.py:51] epoch 5074, training loss: 10969.11, average training loss: 11400.71, base loss: 19756.92
[INFO 2017-06-27 20:57:55,846 main.py:51] epoch 5075, training loss: 12006.39, average training loss: 11402.75, base loss: 19761.96
[INFO 2017-06-27 20:57:56,222 main.py:51] epoch 5076, training loss: 12758.69, average training loss: 11404.37, base loss: 19766.31
[INFO 2017-06-27 20:57:56,605 main.py:51] epoch 5077, training loss: 12127.74, average training loss: 11406.35, base loss: 19770.45
[INFO 2017-06-27 20:57:56,979 main.py:51] epoch 5078, training loss: 10575.38, average training loss: 11403.74, base loss: 19765.13
[INFO 2017-06-27 20:57:57,358 main.py:51] epoch 5079, training loss: 11447.47, average training loss: 11404.58, base loss: 19765.76
[INFO 2017-06-27 20:57:57,733 main.py:51] epoch 5080, training loss: 10902.58, average training loss: 11403.25, base loss: 19764.36
[INFO 2017-06-27 20:57:58,112 main.py:51] epoch 5081, training loss: 11211.96, average training loss: 11403.23, base loss: 19764.23
[INFO 2017-06-27 20:57:58,485 main.py:51] epoch 5082, training loss: 10399.77, average training loss: 11400.23, base loss: 19759.10
[INFO 2017-06-27 20:57:58,855 main.py:51] epoch 5083, training loss: 10178.17, average training loss: 11399.12, base loss: 19758.48
[INFO 2017-06-27 20:57:59,235 main.py:51] epoch 5084, training loss: 11701.42, average training loss: 11399.78, base loss: 19762.99
[INFO 2017-06-27 20:57:59,613 main.py:51] epoch 5085, training loss: 11431.44, average training loss: 11399.22, base loss: 19763.63
[INFO 2017-06-27 20:57:59,991 main.py:51] epoch 5086, training loss: 11954.57, average training loss: 11399.88, base loss: 19766.19
[INFO 2017-06-27 20:58:00,369 main.py:51] epoch 5087, training loss: 11494.29, average training loss: 11399.48, base loss: 19764.31
[INFO 2017-06-27 20:58:00,754 main.py:51] epoch 5088, training loss: 10518.06, average training loss: 11398.88, base loss: 19763.36
[INFO 2017-06-27 20:58:01,133 main.py:51] epoch 5089, training loss: 12174.91, average training loss: 11397.86, base loss: 19762.46
[INFO 2017-06-27 20:58:01,540 main.py:51] epoch 5090, training loss: 11727.04, average training loss: 11397.37, base loss: 19763.00
[INFO 2017-06-27 20:58:01,949 main.py:51] epoch 5091, training loss: 10143.69, average training loss: 11397.59, base loss: 19763.92
[INFO 2017-06-27 20:58:02,333 main.py:51] epoch 5092, training loss: 10758.04, average training loss: 11396.65, base loss: 19760.66
[INFO 2017-06-27 20:58:02,720 main.py:51] epoch 5093, training loss: 11043.99, average training loss: 11397.62, base loss: 19763.79
[INFO 2017-06-27 20:58:03,098 main.py:51] epoch 5094, training loss: 10928.29, average training loss: 11397.25, base loss: 19761.86
[INFO 2017-06-27 20:58:03,510 main.py:51] epoch 5095, training loss: 10956.03, average training loss: 11397.09, base loss: 19761.88
[INFO 2017-06-27 20:58:03,888 main.py:51] epoch 5096, training loss: 10824.32, average training loss: 11396.91, base loss: 19763.11
[INFO 2017-06-27 20:58:04,270 main.py:51] epoch 5097, training loss: 10630.60, average training loss: 11396.81, base loss: 19765.40
[INFO 2017-06-27 20:58:04,646 main.py:51] epoch 5098, training loss: 10892.30, average training loss: 11395.59, base loss: 19763.54
[INFO 2017-06-27 20:58:05,022 main.py:51] epoch 5099, training loss: 11156.94, average training loss: 11395.08, base loss: 19763.49
[INFO 2017-06-27 20:58:05,022 main.py:53] epoch 5099, testing
[INFO 2017-06-27 20:58:06,637 main.py:105] average testing loss: 11699.63, base loss: 20446.51
[INFO 2017-06-27 20:58:06,637 main.py:106] improve_loss: 8746.88, improve_percent: 0.43
[INFO 2017-06-27 20:58:06,637 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:58:07,011 main.py:51] epoch 5100, training loss: 11027.80, average training loss: 11395.91, base loss: 19764.51
[INFO 2017-06-27 20:58:07,466 main.py:51] epoch 5101, training loss: 9455.89, average training loss: 11392.53, base loss: 19757.94
[INFO 2017-06-27 20:58:07,879 main.py:51] epoch 5102, training loss: 12419.47, average training loss: 11392.30, base loss: 19757.67
[INFO 2017-06-27 20:58:08,265 main.py:51] epoch 5103, training loss: 11106.50, average training loss: 11390.66, base loss: 19754.96
[INFO 2017-06-27 20:58:08,673 main.py:51] epoch 5104, training loss: 11719.30, average training loss: 11391.85, base loss: 19758.02
[INFO 2017-06-27 20:58:09,058 main.py:51] epoch 5105, training loss: 11263.30, average training loss: 11391.54, base loss: 19757.85
[INFO 2017-06-27 20:58:09,434 main.py:51] epoch 5106, training loss: 12795.41, average training loss: 11392.82, base loss: 19759.68
[INFO 2017-06-27 20:58:09,808 main.py:51] epoch 5107, training loss: 10584.99, average training loss: 11392.12, base loss: 19758.09
[INFO 2017-06-27 20:58:10,184 main.py:51] epoch 5108, training loss: 10958.34, average training loss: 11392.10, base loss: 19758.48
[INFO 2017-06-27 20:58:10,596 main.py:51] epoch 5109, training loss: 12845.50, average training loss: 11395.30, base loss: 19766.36
[INFO 2017-06-27 20:58:10,971 main.py:51] epoch 5110, training loss: 10896.02, average training loss: 11395.35, base loss: 19765.93
[INFO 2017-06-27 20:58:11,344 main.py:51] epoch 5111, training loss: 11358.79, average training loss: 11395.88, base loss: 19768.39
[INFO 2017-06-27 20:58:11,723 main.py:51] epoch 5112, training loss: 9520.44, average training loss: 11390.70, base loss: 19758.16
[INFO 2017-06-27 20:58:12,099 main.py:51] epoch 5113, training loss: 11583.44, average training loss: 11391.62, base loss: 19761.03
[INFO 2017-06-27 20:58:12,477 main.py:51] epoch 5114, training loss: 9731.81, average training loss: 11389.07, base loss: 19755.56
[INFO 2017-06-27 20:58:12,852 main.py:51] epoch 5115, training loss: 10590.76, average training loss: 11389.22, base loss: 19755.91
[INFO 2017-06-27 20:58:13,229 main.py:51] epoch 5116, training loss: 12461.07, average training loss: 11390.48, base loss: 19761.46
[INFO 2017-06-27 20:58:13,607 main.py:51] epoch 5117, training loss: 11732.22, average training loss: 11392.14, base loss: 19765.56
[INFO 2017-06-27 20:58:13,984 main.py:51] epoch 5118, training loss: 10758.28, average training loss: 11389.46, base loss: 19761.55
[INFO 2017-06-27 20:58:14,366 main.py:51] epoch 5119, training loss: 10065.38, average training loss: 11387.83, base loss: 19758.29
[INFO 2017-06-27 20:58:14,746 main.py:51] epoch 5120, training loss: 10782.27, average training loss: 11388.39, base loss: 19760.26
[INFO 2017-06-27 20:58:15,120 main.py:51] epoch 5121, training loss: 12800.45, average training loss: 11389.44, base loss: 19763.93
[INFO 2017-06-27 20:58:15,502 main.py:51] epoch 5122, training loss: 11116.05, average training loss: 11388.67, base loss: 19763.60
[INFO 2017-06-27 20:58:15,880 main.py:51] epoch 5123, training loss: 10429.06, average training loss: 11388.89, base loss: 19764.73
[INFO 2017-06-27 20:58:16,265 main.py:51] epoch 5124, training loss: 12469.33, average training loss: 11389.90, base loss: 19766.93
[INFO 2017-06-27 20:58:16,714 main.py:51] epoch 5125, training loss: 10530.33, average training loss: 11389.65, base loss: 19766.99
[INFO 2017-06-27 20:58:17,095 main.py:51] epoch 5126, training loss: 10491.84, average training loss: 11388.91, base loss: 19764.78
[INFO 2017-06-27 20:58:17,497 main.py:51] epoch 5127, training loss: 11708.69, average training loss: 11389.48, base loss: 19768.47
[INFO 2017-06-27 20:58:17,968 main.py:51] epoch 5128, training loss: 12127.67, average training loss: 11390.78, base loss: 19772.56
[INFO 2017-06-27 20:58:18,383 main.py:51] epoch 5129, training loss: 12974.69, average training loss: 11391.02, base loss: 19773.69
[INFO 2017-06-27 20:58:18,769 main.py:51] epoch 5130, training loss: 12140.37, average training loss: 11391.67, base loss: 19775.26
[INFO 2017-06-27 20:58:19,149 main.py:51] epoch 5131, training loss: 11937.05, average training loss: 11391.12, base loss: 19775.58
[INFO 2017-06-27 20:58:19,551 main.py:51] epoch 5132, training loss: 10330.93, average training loss: 11389.60, base loss: 19773.47
[INFO 2017-06-27 20:58:19,937 main.py:51] epoch 5133, training loss: 11859.04, average training loss: 11389.31, base loss: 19773.59
[INFO 2017-06-27 20:58:20,321 main.py:51] epoch 5134, training loss: 9773.38, average training loss: 11387.22, base loss: 19767.97
[INFO 2017-06-27 20:58:20,702 main.py:51] epoch 5135, training loss: 9836.38, average training loss: 11386.00, base loss: 19765.95
[INFO 2017-06-27 20:58:21,088 main.py:51] epoch 5136, training loss: 10924.14, average training loss: 11384.81, base loss: 19763.97
[INFO 2017-06-27 20:58:21,469 main.py:51] epoch 5137, training loss: 11465.06, average training loss: 11384.28, base loss: 19763.42
[INFO 2017-06-27 20:58:21,849 main.py:51] epoch 5138, training loss: 11799.71, average training loss: 11383.18, base loss: 19763.24
[INFO 2017-06-27 20:58:22,307 main.py:51] epoch 5139, training loss: 10788.87, average training loss: 11383.97, base loss: 19764.76
[INFO 2017-06-27 20:58:22,710 main.py:51] epoch 5140, training loss: 10211.77, average training loss: 11381.36, base loss: 19758.72
[INFO 2017-06-27 20:58:23,113 main.py:51] epoch 5141, training loss: 10233.80, average training loss: 11379.94, base loss: 19756.17
[INFO 2017-06-27 20:58:23,501 main.py:51] epoch 5142, training loss: 11404.37, average training loss: 11380.74, base loss: 19758.86
[INFO 2017-06-27 20:58:23,895 main.py:51] epoch 5143, training loss: 11828.62, average training loss: 11381.06, base loss: 19758.68
[INFO 2017-06-27 20:58:24,275 main.py:51] epoch 5144, training loss: 10951.08, average training loss: 11379.31, base loss: 19755.14
[INFO 2017-06-27 20:58:24,651 main.py:51] epoch 5145, training loss: 11522.93, average training loss: 11379.00, base loss: 19753.71
[INFO 2017-06-27 20:58:25,032 main.py:51] epoch 5146, training loss: 13211.79, average training loss: 11378.90, base loss: 19754.69
[INFO 2017-06-27 20:58:25,422 main.py:51] epoch 5147, training loss: 10180.37, average training loss: 11378.19, base loss: 19752.21
[INFO 2017-06-27 20:58:25,804 main.py:51] epoch 5148, training loss: 12972.52, average training loss: 11377.29, base loss: 19752.43
[INFO 2017-06-27 20:58:26,206 main.py:51] epoch 5149, training loss: 11409.75, average training loss: 11377.17, base loss: 19752.83
[INFO 2017-06-27 20:58:26,613 main.py:51] epoch 5150, training loss: 11525.65, average training loss: 11376.36, base loss: 19749.97
[INFO 2017-06-27 20:58:26,992 main.py:51] epoch 5151, training loss: 10611.96, average training loss: 11375.20, base loss: 19748.56
[INFO 2017-06-27 20:58:27,371 main.py:51] epoch 5152, training loss: 11260.42, average training loss: 11373.80, base loss: 19747.23
[INFO 2017-06-27 20:58:27,782 main.py:51] epoch 5153, training loss: 11696.85, average training loss: 11375.51, base loss: 19752.63
[INFO 2017-06-27 20:58:28,158 main.py:51] epoch 5154, training loss: 12076.97, average training loss: 11375.30, base loss: 19753.45
[INFO 2017-06-27 20:58:28,535 main.py:51] epoch 5155, training loss: 10574.04, average training loss: 11374.53, base loss: 19750.64
[INFO 2017-06-27 20:58:28,919 main.py:51] epoch 5156, training loss: 10965.36, average training loss: 11373.89, base loss: 19751.16
[INFO 2017-06-27 20:58:29,295 main.py:51] epoch 5157, training loss: 11892.73, average training loss: 11374.65, base loss: 19754.45
[INFO 2017-06-27 20:58:29,780 main.py:51] epoch 5158, training loss: 11788.60, average training loss: 11373.94, base loss: 19754.26
[INFO 2017-06-27 20:58:30,163 main.py:51] epoch 5159, training loss: 12310.49, average training loss: 11374.50, base loss: 19756.24
[INFO 2017-06-27 20:58:30,616 main.py:51] epoch 5160, training loss: 11894.11, average training loss: 11375.09, base loss: 19758.70
[INFO 2017-06-27 20:58:31,051 main.py:51] epoch 5161, training loss: 11361.85, average training loss: 11374.17, base loss: 19757.44
[INFO 2017-06-27 20:58:31,457 main.py:51] epoch 5162, training loss: 12151.48, average training loss: 11374.85, base loss: 19759.60
[INFO 2017-06-27 20:58:31,834 main.py:51] epoch 5163, training loss: 10657.69, average training loss: 11372.82, base loss: 19755.39
[INFO 2017-06-27 20:58:32,234 main.py:51] epoch 5164, training loss: 11016.71, average training loss: 11372.83, base loss: 19756.70
[INFO 2017-06-27 20:58:32,606 main.py:51] epoch 5165, training loss: 11051.49, average training loss: 11372.37, base loss: 19755.81
[INFO 2017-06-27 20:58:32,983 main.py:51] epoch 5166, training loss: 11665.79, average training loss: 11371.98, base loss: 19754.47
[INFO 2017-06-27 20:58:33,358 main.py:51] epoch 5167, training loss: 10308.21, average training loss: 11371.66, base loss: 19752.63
[INFO 2017-06-27 20:58:33,730 main.py:51] epoch 5168, training loss: 10947.08, average training loss: 11373.24, base loss: 19756.13
[INFO 2017-06-27 20:58:34,103 main.py:51] epoch 5169, training loss: 11592.04, average training loss: 11372.52, base loss: 19755.77
[INFO 2017-06-27 20:58:34,483 main.py:51] epoch 5170, training loss: 11844.41, average training loss: 11371.92, base loss: 19754.58
[INFO 2017-06-27 20:58:34,859 main.py:51] epoch 5171, training loss: 10735.18, average training loss: 11371.56, base loss: 19754.66
[INFO 2017-06-27 20:58:35,234 main.py:51] epoch 5172, training loss: 10583.93, average training loss: 11371.32, base loss: 19755.53
[INFO 2017-06-27 20:58:35,611 main.py:51] epoch 5173, training loss: 12167.08, average training loss: 11371.89, base loss: 19759.04
[INFO 2017-06-27 20:58:35,991 main.py:51] epoch 5174, training loss: 11419.48, average training loss: 11372.41, base loss: 19760.48
[INFO 2017-06-27 20:58:36,366 main.py:51] epoch 5175, training loss: 13367.33, average training loss: 11372.89, base loss: 19762.53
[INFO 2017-06-27 20:58:36,736 main.py:51] epoch 5176, training loss: 11026.31, average training loss: 11370.34, base loss: 19758.62
[INFO 2017-06-27 20:58:37,113 main.py:51] epoch 5177, training loss: 10345.87, average training loss: 11368.03, base loss: 19754.04
[INFO 2017-06-27 20:58:37,490 main.py:51] epoch 5178, training loss: 12896.40, average training loss: 11370.17, base loss: 19759.42
[INFO 2017-06-27 20:58:37,868 main.py:51] epoch 5179, training loss: 10654.69, average training loss: 11369.87, base loss: 19758.60
[INFO 2017-06-27 20:58:38,246 main.py:51] epoch 5180, training loss: 9046.18, average training loss: 11366.05, base loss: 19751.79
[INFO 2017-06-27 20:58:38,624 main.py:51] epoch 5181, training loss: 11388.79, average training loss: 11364.46, base loss: 19750.05
[INFO 2017-06-27 20:58:39,005 main.py:51] epoch 5182, training loss: 11136.18, average training loss: 11365.84, base loss: 19752.29
[INFO 2017-06-27 20:58:39,382 main.py:51] epoch 5183, training loss: 11797.11, average training loss: 11365.49, base loss: 19750.83
[INFO 2017-06-27 20:58:39,800 main.py:51] epoch 5184, training loss: 12345.23, average training loss: 11367.41, base loss: 19754.29
[INFO 2017-06-27 20:58:40,203 main.py:51] epoch 5185, training loss: 10227.86, average training loss: 11365.18, base loss: 19751.22
[INFO 2017-06-27 20:58:40,587 main.py:51] epoch 5186, training loss: 12827.28, average training loss: 11368.66, base loss: 19758.52
[INFO 2017-06-27 20:58:40,964 main.py:51] epoch 5187, training loss: 11101.23, average training loss: 11368.82, base loss: 19759.58
[INFO 2017-06-27 20:58:41,364 main.py:51] epoch 5188, training loss: 12129.86, average training loss: 11369.70, base loss: 19762.49
[INFO 2017-06-27 20:58:41,747 main.py:51] epoch 5189, training loss: 11504.17, average training loss: 11370.09, base loss: 19763.16
[INFO 2017-06-27 20:58:42,126 main.py:51] epoch 5190, training loss: 9774.27, average training loss: 11368.24, base loss: 19758.43
[INFO 2017-06-27 20:58:42,503 main.py:51] epoch 5191, training loss: 10880.07, average training loss: 11367.47, base loss: 19755.39
[INFO 2017-06-27 20:58:42,879 main.py:51] epoch 5192, training loss: 9528.87, average training loss: 11365.83, base loss: 19750.48
[INFO 2017-06-27 20:58:43,256 main.py:51] epoch 5193, training loss: 11380.12, average training loss: 11367.32, base loss: 19754.11
[INFO 2017-06-27 20:58:43,632 main.py:51] epoch 5194, training loss: 10444.43, average training loss: 11367.25, base loss: 19754.57
[INFO 2017-06-27 20:58:44,007 main.py:51] epoch 5195, training loss: 11078.76, average training loss: 11367.27, base loss: 19754.67
[INFO 2017-06-27 20:58:44,385 main.py:51] epoch 5196, training loss: 11760.71, average training loss: 11367.13, base loss: 19754.68
[INFO 2017-06-27 20:58:44,759 main.py:51] epoch 5197, training loss: 9923.52, average training loss: 11364.44, base loss: 19748.96
[INFO 2017-06-27 20:58:45,131 main.py:51] epoch 5198, training loss: 11673.37, average training loss: 11365.08, base loss: 19751.48
[INFO 2017-06-27 20:58:45,503 main.py:51] epoch 5199, training loss: 12204.46, average training loss: 11365.52, base loss: 19752.85
[INFO 2017-06-27 20:58:45,504 main.py:53] epoch 5199, testing
[INFO 2017-06-27 20:58:47,102 main.py:105] average testing loss: 11559.29, base loss: 20519.87
[INFO 2017-06-27 20:58:47,102 main.py:106] improve_loss: 8960.58, improve_percent: 0.44
[INFO 2017-06-27 20:58:47,102 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:58:47,479 main.py:51] epoch 5200, training loss: 12633.08, average training loss: 11366.78, base loss: 19755.80
[INFO 2017-06-27 20:58:47,855 main.py:51] epoch 5201, training loss: 12196.77, average training loss: 11367.83, base loss: 19758.74
[INFO 2017-06-27 20:58:48,236 main.py:51] epoch 5202, training loss: 10522.03, average training loss: 11365.69, base loss: 19754.93
[INFO 2017-06-27 20:58:48,615 main.py:51] epoch 5203, training loss: 10711.45, average training loss: 11364.83, base loss: 19753.68
[INFO 2017-06-27 20:58:48,995 main.py:51] epoch 5204, training loss: 10980.25, average training loss: 11366.04, base loss: 19755.58
[INFO 2017-06-27 20:58:49,372 main.py:51] epoch 5205, training loss: 11527.66, average training loss: 11366.45, base loss: 19756.91
[INFO 2017-06-27 20:58:49,748 main.py:51] epoch 5206, training loss: 12643.47, average training loss: 11367.85, base loss: 19760.71
[INFO 2017-06-27 20:58:50,219 main.py:51] epoch 5207, training loss: 12660.83, average training loss: 11369.07, base loss: 19762.04
[INFO 2017-06-27 20:58:50,607 main.py:51] epoch 5208, training loss: 11126.08, average training loss: 11368.39, base loss: 19760.65
[INFO 2017-06-27 20:58:50,993 main.py:51] epoch 5209, training loss: 11573.69, average training loss: 11368.78, base loss: 19763.26
[INFO 2017-06-27 20:58:51,372 main.py:51] epoch 5210, training loss: 9559.82, average training loss: 11368.12, base loss: 19760.92
[INFO 2017-06-27 20:58:51,766 main.py:51] epoch 5211, training loss: 11013.55, average training loss: 11368.08, base loss: 19761.90
[INFO 2017-06-27 20:58:52,141 main.py:51] epoch 5212, training loss: 10262.91, average training loss: 11368.17, base loss: 19762.81
[INFO 2017-06-27 20:58:52,518 main.py:51] epoch 5213, training loss: 10969.41, average training loss: 11367.55, base loss: 19760.96
[INFO 2017-06-27 20:58:52,894 main.py:51] epoch 5214, training loss: 12511.31, average training loss: 11368.59, base loss: 19763.21
[INFO 2017-06-27 20:58:53,270 main.py:51] epoch 5215, training loss: 11582.63, average training loss: 11370.16, base loss: 19766.25
[INFO 2017-06-27 20:58:53,645 main.py:51] epoch 5216, training loss: 10642.84, average training loss: 11368.47, base loss: 19762.75
[INFO 2017-06-27 20:58:54,020 main.py:51] epoch 5217, training loss: 11331.02, average training loss: 11368.76, base loss: 19763.31
[INFO 2017-06-27 20:58:54,398 main.py:51] epoch 5218, training loss: 10492.42, average training loss: 11368.38, base loss: 19763.74
[INFO 2017-06-27 20:58:54,775 main.py:51] epoch 5219, training loss: 11811.57, average training loss: 11368.26, base loss: 19764.25
[INFO 2017-06-27 20:58:55,151 main.py:51] epoch 5220, training loss: 12368.86, average training loss: 11370.23, base loss: 19769.54
[INFO 2017-06-27 20:58:55,526 main.py:51] epoch 5221, training loss: 12117.13, average training loss: 11370.65, base loss: 19771.97
[INFO 2017-06-27 20:58:55,903 main.py:51] epoch 5222, training loss: 10764.27, average training loss: 11369.93, base loss: 19771.98
[INFO 2017-06-27 20:58:56,278 main.py:51] epoch 5223, training loss: 12317.80, average training loss: 11369.21, base loss: 19770.64
[INFO 2017-06-27 20:58:56,653 main.py:51] epoch 5224, training loss: 10030.72, average training loss: 11368.56, base loss: 19769.27
[INFO 2017-06-27 20:58:57,031 main.py:51] epoch 5225, training loss: 11294.53, average training loss: 11368.71, base loss: 19769.75
[INFO 2017-06-27 20:58:57,409 main.py:51] epoch 5226, training loss: 9971.09, average training loss: 11366.65, base loss: 19766.46
[INFO 2017-06-27 20:58:57,863 main.py:51] epoch 5227, training loss: 11312.41, average training loss: 11363.14, base loss: 19762.31
[INFO 2017-06-27 20:58:58,266 main.py:51] epoch 5228, training loss: 11634.64, average training loss: 11362.43, base loss: 19762.11
[INFO 2017-06-27 20:58:58,647 main.py:51] epoch 5229, training loss: 10659.49, average training loss: 11363.23, base loss: 19763.56
[INFO 2017-06-27 20:58:59,043 main.py:51] epoch 5230, training loss: 11511.97, average training loss: 11362.65, base loss: 19762.34
[INFO 2017-06-27 20:58:59,498 main.py:51] epoch 5231, training loss: 9841.85, average training loss: 11361.82, base loss: 19761.98
[INFO 2017-06-27 20:58:59,904 main.py:51] epoch 5232, training loss: 12901.41, average training loss: 11365.24, base loss: 19770.48
[INFO 2017-06-27 20:59:00,356 main.py:51] epoch 5233, training loss: 10367.99, average training loss: 11363.74, base loss: 19767.84
[INFO 2017-06-27 20:59:00,825 main.py:51] epoch 5234, training loss: 10520.75, average training loss: 11363.86, base loss: 19769.29
[INFO 2017-06-27 20:59:01,210 main.py:51] epoch 5235, training loss: 11907.85, average training loss: 11362.74, base loss: 19766.88
[INFO 2017-06-27 20:59:01,611 main.py:51] epoch 5236, training loss: 11457.87, average training loss: 11362.45, base loss: 19766.51
[INFO 2017-06-27 20:59:02,016 main.py:51] epoch 5237, training loss: 10555.65, average training loss: 11362.32, base loss: 19768.20
[INFO 2017-06-27 20:59:02,414 main.py:51] epoch 5238, training loss: 9942.35, average training loss: 11360.41, base loss: 19764.58
[INFO 2017-06-27 20:59:02,786 main.py:51] epoch 5239, training loss: 10135.87, average training loss: 11359.30, base loss: 19763.68
[INFO 2017-06-27 20:59:03,161 main.py:51] epoch 5240, training loss: 11153.84, average training loss: 11358.53, base loss: 19763.97
[INFO 2017-06-27 20:59:03,534 main.py:51] epoch 5241, training loss: 10839.09, average training loss: 11359.33, base loss: 19767.16
[INFO 2017-06-27 20:59:03,906 main.py:51] epoch 5242, training loss: 11260.40, average training loss: 11358.97, base loss: 19767.01
[INFO 2017-06-27 20:59:04,284 main.py:51] epoch 5243, training loss: 11154.22, average training loss: 11358.25, base loss: 19766.37
[INFO 2017-06-27 20:59:04,660 main.py:51] epoch 5244, training loss: 10804.11, average training loss: 11355.39, base loss: 19761.10
[INFO 2017-06-27 20:59:05,031 main.py:51] epoch 5245, training loss: 11046.24, average training loss: 11353.83, base loss: 19758.83
[INFO 2017-06-27 20:59:05,406 main.py:51] epoch 5246, training loss: 11226.07, average training loss: 11353.62, base loss: 19760.38
[INFO 2017-06-27 20:59:05,781 main.py:51] epoch 5247, training loss: 10657.68, average training loss: 11353.01, base loss: 19759.02
[INFO 2017-06-27 20:59:06,157 main.py:51] epoch 5248, training loss: 11403.58, average training loss: 11354.29, base loss: 19761.39
[INFO 2017-06-27 20:59:06,532 main.py:51] epoch 5249, training loss: 9812.90, average training loss: 11352.90, base loss: 19757.47
[INFO 2017-06-27 20:59:06,912 main.py:51] epoch 5250, training loss: 11498.13, average training loss: 11352.97, base loss: 19757.14
[INFO 2017-06-27 20:59:07,288 main.py:51] epoch 5251, training loss: 12498.31, average training loss: 11355.53, base loss: 19763.05
[INFO 2017-06-27 20:59:07,667 main.py:51] epoch 5252, training loss: 12957.09, average training loss: 11357.90, base loss: 19768.01
[INFO 2017-06-27 20:59:08,045 main.py:51] epoch 5253, training loss: 11198.06, average training loss: 11358.15, base loss: 19768.45
[INFO 2017-06-27 20:59:08,517 main.py:51] epoch 5254, training loss: 11446.58, average training loss: 11357.55, base loss: 19766.30
[INFO 2017-06-27 20:59:08,917 main.py:51] epoch 5255, training loss: 12277.57, average training loss: 11358.22, base loss: 19766.87
[INFO 2017-06-27 20:59:09,297 main.py:51] epoch 5256, training loss: 12059.34, average training loss: 11358.38, base loss: 19767.90
[INFO 2017-06-27 20:59:09,675 main.py:51] epoch 5257, training loss: 10601.18, average training loss: 11357.74, base loss: 19765.68
[INFO 2017-06-27 20:59:10,052 main.py:51] epoch 5258, training loss: 12150.95, average training loss: 11357.63, base loss: 19765.30
[INFO 2017-06-27 20:59:10,431 main.py:51] epoch 5259, training loss: 11367.66, average training loss: 11356.14, base loss: 19762.57
[INFO 2017-06-27 20:59:10,806 main.py:51] epoch 5260, training loss: 10002.06, average training loss: 11355.32, base loss: 19761.93
[INFO 2017-06-27 20:59:11,182 main.py:51] epoch 5261, training loss: 11013.43, average training loss: 11355.41, base loss: 19761.97
[INFO 2017-06-27 20:59:11,555 main.py:51] epoch 5262, training loss: 10435.66, average training loss: 11352.59, base loss: 19756.61
[INFO 2017-06-27 20:59:12,013 main.py:51] epoch 5263, training loss: 10811.59, average training loss: 11351.51, base loss: 19754.54
[INFO 2017-06-27 20:59:12,446 main.py:51] epoch 5264, training loss: 10026.24, average training loss: 11351.10, base loss: 19753.58
[INFO 2017-06-27 20:59:12,827 main.py:51] epoch 5265, training loss: 11051.42, average training loss: 11352.40, base loss: 19757.04
[INFO 2017-06-27 20:59:13,206 main.py:51] epoch 5266, training loss: 10682.53, average training loss: 11352.44, base loss: 19757.21
[INFO 2017-06-27 20:59:13,585 main.py:51] epoch 5267, training loss: 10774.33, average training loss: 11350.40, base loss: 19754.05
[INFO 2017-06-27 20:59:13,967 main.py:51] epoch 5268, training loss: 12719.49, average training loss: 11350.87, base loss: 19756.39
[INFO 2017-06-27 20:59:14,344 main.py:51] epoch 5269, training loss: 12934.98, average training loss: 11351.53, base loss: 19757.60
[INFO 2017-06-27 20:59:14,749 main.py:51] epoch 5270, training loss: 11722.88, average training loss: 11351.38, base loss: 19758.70
[INFO 2017-06-27 20:59:15,150 main.py:51] epoch 5271, training loss: 10594.89, average training loss: 11349.90, base loss: 19757.89
[INFO 2017-06-27 20:59:15,545 main.py:51] epoch 5272, training loss: 10272.89, average training loss: 11348.58, base loss: 19756.90
[INFO 2017-06-27 20:59:15,918 main.py:51] epoch 5273, training loss: 11776.20, average training loss: 11348.99, base loss: 19757.77
[INFO 2017-06-27 20:59:16,297 main.py:51] epoch 5274, training loss: 11370.75, average training loss: 11348.15, base loss: 19757.45
[INFO 2017-06-27 20:59:16,675 main.py:51] epoch 5275, training loss: 11363.86, average training loss: 11348.18, base loss: 19759.09
[INFO 2017-06-27 20:59:17,050 main.py:51] epoch 5276, training loss: 10985.69, average training loss: 11347.53, base loss: 19758.60
[INFO 2017-06-27 20:59:17,425 main.py:51] epoch 5277, training loss: 11536.46, average training loss: 11348.06, base loss: 19758.97
[INFO 2017-06-27 20:59:17,807 main.py:51] epoch 5278, training loss: 11579.24, average training loss: 11348.17, base loss: 19759.67
[INFO 2017-06-27 20:59:18,188 main.py:51] epoch 5279, training loss: 11239.49, average training loss: 11346.35, base loss: 19758.66
[INFO 2017-06-27 20:59:18,654 main.py:51] epoch 5280, training loss: 10439.32, average training loss: 11346.89, base loss: 19759.79
[INFO 2017-06-27 20:59:19,050 main.py:51] epoch 5281, training loss: 11239.41, average training loss: 11346.11, base loss: 19757.47
[INFO 2017-06-27 20:59:19,427 main.py:51] epoch 5282, training loss: 12598.57, average training loss: 11347.27, base loss: 19761.52
[INFO 2017-06-27 20:59:19,804 main.py:51] epoch 5283, training loss: 12848.37, average training loss: 11348.66, base loss: 19766.25
[INFO 2017-06-27 20:59:20,206 main.py:51] epoch 5284, training loss: 11717.14, average training loss: 11348.84, base loss: 19767.36
[INFO 2017-06-27 20:59:20,583 main.py:51] epoch 5285, training loss: 13462.99, average training loss: 11351.00, base loss: 19772.59
[INFO 2017-06-27 20:59:20,960 main.py:51] epoch 5286, training loss: 11129.32, average training loss: 11351.68, base loss: 19775.30
[INFO 2017-06-27 20:59:21,338 main.py:51] epoch 5287, training loss: 12835.06, average training loss: 11353.28, base loss: 19777.64
[INFO 2017-06-27 20:59:21,710 main.py:51] epoch 5288, training loss: 13428.38, average training loss: 11354.84, base loss: 19781.43
[INFO 2017-06-27 20:59:22,080 main.py:51] epoch 5289, training loss: 12382.02, average training loss: 11354.48, base loss: 19782.32
[INFO 2017-06-27 20:59:22,454 main.py:51] epoch 5290, training loss: 9571.93, average training loss: 11352.44, base loss: 19779.13
[INFO 2017-06-27 20:59:22,828 main.py:51] epoch 5291, training loss: 12346.43, average training loss: 11353.71, base loss: 19783.66
[INFO 2017-06-27 20:59:23,208 main.py:51] epoch 5292, training loss: 11164.29, average training loss: 11351.67, base loss: 19781.28
[INFO 2017-06-27 20:59:23,584 main.py:51] epoch 5293, training loss: 10993.67, average training loss: 11350.35, base loss: 19779.27
[INFO 2017-06-27 20:59:23,956 main.py:51] epoch 5294, training loss: 12022.93, average training loss: 11350.67, base loss: 19780.77
[INFO 2017-06-27 20:59:24,330 main.py:51] epoch 5295, training loss: 10876.69, average training loss: 11351.54, base loss: 19783.91
[INFO 2017-06-27 20:59:24,709 main.py:51] epoch 5296, training loss: 12030.71, average training loss: 11352.74, base loss: 19786.70
[INFO 2017-06-27 20:59:25,080 main.py:51] epoch 5297, training loss: 10991.84, average training loss: 11350.89, base loss: 19781.24
[INFO 2017-06-27 20:59:25,452 main.py:51] epoch 5298, training loss: 10598.38, average training loss: 11351.24, base loss: 19783.28
[INFO 2017-06-27 20:59:25,826 main.py:51] epoch 5299, training loss: 12108.82, average training loss: 11351.37, base loss: 19784.88
[INFO 2017-06-27 20:59:25,826 main.py:53] epoch 5299, testing
[INFO 2017-06-27 20:59:27,439 main.py:105] average testing loss: 10973.76, base loss: 19413.52
[INFO 2017-06-27 20:59:27,439 main.py:106] improve_loss: 8439.77, improve_percent: 0.43
[INFO 2017-06-27 20:59:27,440 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 20:59:27,815 main.py:51] epoch 5300, training loss: 11954.17, average training loss: 11353.71, base loss: 19790.83
[INFO 2017-06-27 20:59:28,187 main.py:51] epoch 5301, training loss: 11800.21, average training loss: 11353.18, base loss: 19789.33
[INFO 2017-06-27 20:59:28,558 main.py:51] epoch 5302, training loss: 10565.76, average training loss: 11352.05, base loss: 19786.47
[INFO 2017-06-27 20:59:28,930 main.py:51] epoch 5303, training loss: 11207.88, average training loss: 11351.41, base loss: 19785.10
[INFO 2017-06-27 20:59:29,313 main.py:51] epoch 5304, training loss: 12661.77, average training loss: 11352.76, base loss: 19789.37
[INFO 2017-06-27 20:59:29,689 main.py:51] epoch 5305, training loss: 11836.09, average training loss: 11352.10, base loss: 19787.56
[INFO 2017-06-27 20:59:30,065 main.py:51] epoch 5306, training loss: 12386.17, average training loss: 11353.23, base loss: 19789.80
[INFO 2017-06-27 20:59:30,442 main.py:51] epoch 5307, training loss: 13179.25, average training loss: 11355.48, base loss: 19795.17
[INFO 2017-06-27 20:59:30,816 main.py:51] epoch 5308, training loss: 11371.48, average training loss: 11354.93, base loss: 19794.75
[INFO 2017-06-27 20:59:31,194 main.py:51] epoch 5309, training loss: 10907.77, average training loss: 11354.60, base loss: 19794.13
[INFO 2017-06-27 20:59:31,573 main.py:51] epoch 5310, training loss: 11007.51, average training loss: 11354.81, base loss: 19793.54
[INFO 2017-06-27 20:59:31,956 main.py:51] epoch 5311, training loss: 10673.28, average training loss: 11353.60, base loss: 19792.69
[INFO 2017-06-27 20:59:32,331 main.py:51] epoch 5312, training loss: 10858.71, average training loss: 11353.37, base loss: 19793.83
[INFO 2017-06-27 20:59:32,706 main.py:51] epoch 5313, training loss: 11491.48, average training loss: 11354.64, base loss: 19796.77
[INFO 2017-06-27 20:59:33,084 main.py:51] epoch 5314, training loss: 11543.88, average training loss: 11356.51, base loss: 19800.28
[INFO 2017-06-27 20:59:33,461 main.py:51] epoch 5315, training loss: 10821.49, average training loss: 11355.52, base loss: 19797.76
[INFO 2017-06-27 20:59:33,833 main.py:51] epoch 5316, training loss: 10252.50, average training loss: 11355.31, base loss: 19797.38
[INFO 2017-06-27 20:59:34,206 main.py:51] epoch 5317, training loss: 11073.78, average training loss: 11355.10, base loss: 19798.03
[INFO 2017-06-27 20:59:34,578 main.py:51] epoch 5318, training loss: 10869.14, average training loss: 11354.15, base loss: 19796.52
[INFO 2017-06-27 20:59:34,949 main.py:51] epoch 5319, training loss: 9924.14, average training loss: 11352.49, base loss: 19794.50
[INFO 2017-06-27 20:59:35,321 main.py:51] epoch 5320, training loss: 12154.18, average training loss: 11353.71, base loss: 19795.86
[INFO 2017-06-27 20:59:35,693 main.py:51] epoch 5321, training loss: 10494.11, average training loss: 11352.53, base loss: 19794.77
[INFO 2017-06-27 20:59:36,073 main.py:51] epoch 5322, training loss: 11154.90, average training loss: 11350.69, base loss: 19791.73
[INFO 2017-06-27 20:59:36,450 main.py:51] epoch 5323, training loss: 9829.45, average training loss: 11349.11, base loss: 19789.99
[INFO 2017-06-27 20:59:36,821 main.py:51] epoch 5324, training loss: 10867.08, average training loss: 11348.89, base loss: 19788.92
[INFO 2017-06-27 20:59:37,199 main.py:51] epoch 5325, training loss: 12862.93, average training loss: 11349.25, base loss: 19790.67
[INFO 2017-06-27 20:59:37,580 main.py:51] epoch 5326, training loss: 11470.70, average training loss: 11348.87, base loss: 19791.25
[INFO 2017-06-27 20:59:37,957 main.py:51] epoch 5327, training loss: 12256.20, average training loss: 11349.95, base loss: 19793.72
[INFO 2017-06-27 20:59:38,333 main.py:51] epoch 5328, training loss: 12031.92, average training loss: 11351.23, base loss: 19797.28
[INFO 2017-06-27 20:59:38,704 main.py:51] epoch 5329, training loss: 10679.06, average training loss: 11349.46, base loss: 19793.83
[INFO 2017-06-27 20:59:39,083 main.py:51] epoch 5330, training loss: 11051.79, average training loss: 11346.89, base loss: 19790.04
[INFO 2017-06-27 20:59:39,465 main.py:51] epoch 5331, training loss: 11600.93, average training loss: 11347.23, base loss: 19791.18
[INFO 2017-06-27 20:59:39,843 main.py:51] epoch 5332, training loss: 11988.43, average training loss: 11348.30, base loss: 19793.68
[INFO 2017-06-27 20:59:40,224 main.py:51] epoch 5333, training loss: 10278.21, average training loss: 11347.39, base loss: 19792.91
[INFO 2017-06-27 20:59:40,602 main.py:51] epoch 5334, training loss: 11480.78, average training loss: 11348.49, base loss: 19796.22
[INFO 2017-06-27 20:59:41,052 main.py:51] epoch 5335, training loss: 10768.60, average training loss: 11347.12, base loss: 19792.64
[INFO 2017-06-27 20:59:41,453 main.py:51] epoch 5336, training loss: 11913.16, average training loss: 11347.04, base loss: 19793.84
[INFO 2017-06-27 20:59:41,837 main.py:51] epoch 5337, training loss: 11240.10, average training loss: 11347.32, base loss: 19795.76
[INFO 2017-06-27 20:59:42,213 main.py:51] epoch 5338, training loss: 10599.05, average training loss: 11346.64, base loss: 19793.75
[INFO 2017-06-27 20:59:42,690 main.py:51] epoch 5339, training loss: 11141.50, average training loss: 11346.33, base loss: 19791.66
[INFO 2017-06-27 20:59:43,089 main.py:51] epoch 5340, training loss: 10961.33, average training loss: 11345.95, base loss: 19790.48
[INFO 2017-06-27 20:59:43,472 main.py:51] epoch 5341, training loss: 11350.63, average training loss: 11345.17, base loss: 19789.05
[INFO 2017-06-27 20:59:43,857 main.py:51] epoch 5342, training loss: 10818.57, average training loss: 11343.44, base loss: 19784.88
[INFO 2017-06-27 20:59:44,233 main.py:51] epoch 5343, training loss: 10498.49, average training loss: 11342.49, base loss: 19784.53
[INFO 2017-06-27 20:59:44,640 main.py:51] epoch 5344, training loss: 11079.86, average training loss: 11342.29, base loss: 19783.89
[INFO 2017-06-27 20:59:45,015 main.py:51] epoch 5345, training loss: 10226.64, average training loss: 11340.67, base loss: 19780.25
[INFO 2017-06-27 20:59:45,390 main.py:51] epoch 5346, training loss: 10810.31, average training loss: 11339.94, base loss: 19780.68
[INFO 2017-06-27 20:59:45,768 main.py:51] epoch 5347, training loss: 11213.01, average training loss: 11340.11, base loss: 19780.01
[INFO 2017-06-27 20:59:46,142 main.py:51] epoch 5348, training loss: 9417.34, average training loss: 11338.26, base loss: 19778.31
[INFO 2017-06-27 20:59:46,519 main.py:51] epoch 5349, training loss: 10811.09, average training loss: 11336.82, base loss: 19776.27
[INFO 2017-06-27 20:59:46,894 main.py:51] epoch 5350, training loss: 9462.90, average training loss: 11334.65, base loss: 19772.49
[INFO 2017-06-27 20:59:47,270 main.py:51] epoch 5351, training loss: 11207.15, average training loss: 11334.93, base loss: 19773.80
[INFO 2017-06-27 20:59:47,644 main.py:51] epoch 5352, training loss: 12149.82, average training loss: 11336.04, base loss: 19777.11
[INFO 2017-06-27 20:59:48,018 main.py:51] epoch 5353, training loss: 10963.90, average training loss: 11336.16, base loss: 19778.20
[INFO 2017-06-27 20:59:48,407 main.py:51] epoch 5354, training loss: 12450.91, average training loss: 11337.02, base loss: 19781.02
[INFO 2017-06-27 20:59:48,809 main.py:51] epoch 5355, training loss: 11382.13, average training loss: 11336.08, base loss: 19779.13
[INFO 2017-06-27 20:59:49,211 main.py:51] epoch 5356, training loss: 11498.81, average training loss: 11336.70, base loss: 19782.87
[INFO 2017-06-27 20:59:49,596 main.py:51] epoch 5357, training loss: 10773.89, average training loss: 11336.50, base loss: 19783.37
[INFO 2017-06-27 20:59:49,973 main.py:51] epoch 5358, training loss: 9558.86, average training loss: 11333.18, base loss: 19776.82
[INFO 2017-06-27 20:59:50,379 main.py:51] epoch 5359, training loss: 9625.53, average training loss: 11331.03, base loss: 19771.64
[INFO 2017-06-27 20:59:50,758 main.py:51] epoch 5360, training loss: 11869.81, average training loss: 11332.07, base loss: 19773.28
[INFO 2017-06-27 20:59:51,138 main.py:51] epoch 5361, training loss: 10857.66, average training loss: 11331.27, base loss: 19773.02
[INFO 2017-06-27 20:59:51,516 main.py:51] epoch 5362, training loss: 10264.86, average training loss: 11330.59, base loss: 19771.91
[INFO 2017-06-27 20:59:51,891 main.py:51] epoch 5363, training loss: 10079.42, average training loss: 11329.86, base loss: 19771.32
[INFO 2017-06-27 20:59:52,269 main.py:51] epoch 5364, training loss: 11275.63, average training loss: 11330.82, base loss: 19773.99
[INFO 2017-06-27 20:59:52,650 main.py:51] epoch 5365, training loss: 12447.16, average training loss: 11330.22, base loss: 19772.97
[INFO 2017-06-27 20:59:53,025 main.py:51] epoch 5366, training loss: 11466.03, average training loss: 11331.52, base loss: 19775.97
[INFO 2017-06-27 20:59:53,402 main.py:51] epoch 5367, training loss: 10641.54, average training loss: 11330.92, base loss: 19775.72
[INFO 2017-06-27 20:59:53,777 main.py:51] epoch 5368, training loss: 10548.23, average training loss: 11330.07, base loss: 19773.37
[INFO 2017-06-27 20:59:54,153 main.py:51] epoch 5369, training loss: 10737.96, average training loss: 11330.08, base loss: 19774.62
[INFO 2017-06-27 20:59:54,526 main.py:51] epoch 5370, training loss: 10845.83, average training loss: 11329.48, base loss: 19775.40
[INFO 2017-06-27 20:59:54,903 main.py:51] epoch 5371, training loss: 10365.09, average training loss: 11328.00, base loss: 19773.76
[INFO 2017-06-27 20:59:55,278 main.py:51] epoch 5372, training loss: 11187.39, average training loss: 11328.55, base loss: 19775.77
[INFO 2017-06-27 20:59:55,661 main.py:51] epoch 5373, training loss: 9786.96, average training loss: 11324.88, base loss: 19769.98
[INFO 2017-06-27 20:59:56,034 main.py:51] epoch 5374, training loss: 11508.06, average training loss: 11324.30, base loss: 19768.21
[INFO 2017-06-27 20:59:56,407 main.py:51] epoch 5375, training loss: 10445.83, average training loss: 11323.40, base loss: 19766.72
[INFO 2017-06-27 20:59:56,783 main.py:51] epoch 5376, training loss: 11106.49, average training loss: 11323.35, base loss: 19766.60
[INFO 2017-06-27 20:59:57,260 main.py:51] epoch 5377, training loss: 10236.30, average training loss: 11322.06, base loss: 19764.52
[INFO 2017-06-27 20:59:57,645 main.py:51] epoch 5378, training loss: 12182.34, average training loss: 11321.65, base loss: 19763.95
[INFO 2017-06-27 20:59:58,023 main.py:51] epoch 5379, training loss: 11016.07, average training loss: 11319.32, base loss: 19759.00
[INFO 2017-06-27 20:59:58,405 main.py:51] epoch 5380, training loss: 11107.50, average training loss: 11318.97, base loss: 19761.22
[INFO 2017-06-27 20:59:58,782 main.py:51] epoch 5381, training loss: 9981.75, average training loss: 11317.26, base loss: 19759.13
[INFO 2017-06-27 20:59:59,176 main.py:51] epoch 5382, training loss: 11484.60, average training loss: 11317.95, base loss: 19761.93
[INFO 2017-06-27 20:59:59,559 main.py:51] epoch 5383, training loss: 10967.70, average training loss: 11317.73, base loss: 19763.23
[INFO 2017-06-27 20:59:59,940 main.py:51] epoch 5384, training loss: 12210.05, average training loss: 11319.75, base loss: 19768.10
[INFO 2017-06-27 21:00:00,320 main.py:51] epoch 5385, training loss: 11127.40, average training loss: 11318.75, base loss: 19766.57
[INFO 2017-06-27 21:00:00,695 main.py:51] epoch 5386, training loss: 10683.38, average training loss: 11318.59, base loss: 19766.74
[INFO 2017-06-27 21:00:01,073 main.py:51] epoch 5387, training loss: 10929.43, average training loss: 11317.89, base loss: 19766.15
[INFO 2017-06-27 21:00:01,449 main.py:51] epoch 5388, training loss: 10540.44, average training loss: 11316.48, base loss: 19763.49
[INFO 2017-06-27 21:00:01,827 main.py:51] epoch 5389, training loss: 13476.53, average training loss: 11318.27, base loss: 19766.63
[INFO 2017-06-27 21:00:02,209 main.py:51] epoch 5390, training loss: 11303.98, average training loss: 11317.34, base loss: 19765.47
[INFO 2017-06-27 21:00:02,602 main.py:51] epoch 5391, training loss: 11594.83, average training loss: 11317.53, base loss: 19766.27
[INFO 2017-06-27 21:00:03,061 main.py:51] epoch 5392, training loss: 9535.51, average training loss: 11316.70, base loss: 19764.30
[INFO 2017-06-27 21:00:03,469 main.py:51] epoch 5393, training loss: 11204.96, average training loss: 11315.82, base loss: 19762.87
[INFO 2017-06-27 21:00:03,848 main.py:51] epoch 5394, training loss: 11766.31, average training loss: 11317.49, base loss: 19767.03
[INFO 2017-06-27 21:00:04,228 main.py:51] epoch 5395, training loss: 9755.33, average training loss: 11316.00, base loss: 19764.96
[INFO 2017-06-27 21:00:04,698 main.py:51] epoch 5396, training loss: 11362.76, average training loss: 11316.23, base loss: 19765.79
[INFO 2017-06-27 21:00:05,086 main.py:51] epoch 5397, training loss: 11538.67, average training loss: 11317.10, base loss: 19767.79
[INFO 2017-06-27 21:00:05,464 main.py:51] epoch 5398, training loss: 10075.44, average training loss: 11315.11, base loss: 19764.38
[INFO 2017-06-27 21:00:05,858 main.py:51] epoch 5399, training loss: 11129.51, average training loss: 11315.48, base loss: 19766.04
[INFO 2017-06-27 21:00:05,858 main.py:53] epoch 5399, testing
[INFO 2017-06-27 21:00:07,524 main.py:105] average testing loss: 11523.83, base loss: 20673.80
[INFO 2017-06-27 21:00:07,525 main.py:106] improve_loss: 9149.97, improve_percent: 0.44
[INFO 2017-06-27 21:00:07,525 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:00:07,537 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 21:00:07,994 main.py:51] epoch 5400, training loss: 12538.44, average training loss: 11317.49, base loss: 19770.87
[INFO 2017-06-27 21:00:08,400 main.py:51] epoch 5401, training loss: 9874.88, average training loss: 11315.09, base loss: 19766.28
[INFO 2017-06-27 21:00:08,782 main.py:51] epoch 5402, training loss: 9874.32, average training loss: 11313.81, base loss: 19764.70
[INFO 2017-06-27 21:00:09,163 main.py:51] epoch 5403, training loss: 12750.93, average training loss: 11314.61, base loss: 19766.95
[INFO 2017-06-27 21:00:09,540 main.py:51] epoch 5404, training loss: 10514.28, average training loss: 11314.70, base loss: 19768.36
[INFO 2017-06-27 21:00:09,918 main.py:51] epoch 5405, training loss: 11169.18, average training loss: 11313.73, base loss: 19764.69
[INFO 2017-06-27 21:00:10,290 main.py:51] epoch 5406, training loss: 12674.70, average training loss: 11314.17, base loss: 19765.76
[INFO 2017-06-27 21:00:10,666 main.py:51] epoch 5407, training loss: 11563.02, average training loss: 11314.57, base loss: 19767.60
[INFO 2017-06-27 21:00:11,044 main.py:51] epoch 5408, training loss: 11024.30, average training loss: 11313.71, base loss: 19765.45
[INFO 2017-06-27 21:00:11,420 main.py:51] epoch 5409, training loss: 10775.49, average training loss: 11312.10, base loss: 19763.08
[INFO 2017-06-27 21:00:11,803 main.py:51] epoch 5410, training loss: 11326.73, average training loss: 11312.72, base loss: 19765.20
[INFO 2017-06-27 21:00:12,182 main.py:51] epoch 5411, training loss: 11467.54, average training loss: 11312.23, base loss: 19765.02
[INFO 2017-06-27 21:00:12,562 main.py:51] epoch 5412, training loss: 11548.54, average training loss: 11311.04, base loss: 19763.88
[INFO 2017-06-27 21:00:12,947 main.py:51] epoch 5413, training loss: 11485.49, average training loss: 11310.19, base loss: 19762.56
[INFO 2017-06-27 21:00:13,325 main.py:51] epoch 5414, training loss: 11028.86, average training loss: 11309.79, base loss: 19762.13
[INFO 2017-06-27 21:00:13,698 main.py:51] epoch 5415, training loss: 12162.89, average training loss: 11308.49, base loss: 19760.84
[INFO 2017-06-27 21:00:14,092 main.py:51] epoch 5416, training loss: 13367.62, average training loss: 11311.24, base loss: 19767.11
[INFO 2017-06-27 21:00:14,477 main.py:51] epoch 5417, training loss: 9529.37, average training loss: 11309.17, base loss: 19763.84
[INFO 2017-06-27 21:00:14,862 main.py:51] epoch 5418, training loss: 10781.20, average training loss: 11307.86, base loss: 19762.86
[INFO 2017-06-27 21:00:15,249 main.py:51] epoch 5419, training loss: 12534.12, average training loss: 11309.81, base loss: 19766.84
[INFO 2017-06-27 21:00:15,629 main.py:51] epoch 5420, training loss: 11254.65, average training loss: 11308.57, base loss: 19763.06
[INFO 2017-06-27 21:00:16,007 main.py:51] epoch 5421, training loss: 10926.16, average training loss: 11307.21, base loss: 19761.12
[INFO 2017-06-27 21:00:16,383 main.py:51] epoch 5422, training loss: 13264.67, average training loss: 11306.28, base loss: 19759.20
[INFO 2017-06-27 21:00:16,753 main.py:51] epoch 5423, training loss: 11566.48, average training loss: 11306.86, base loss: 19760.21
[INFO 2017-06-27 21:00:17,130 main.py:51] epoch 5424, training loss: 11726.23, average training loss: 11307.27, base loss: 19761.44
