[INFO 2017-06-27 21:02:51,082 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=25, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-64-few', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64-few', train_epoch=10000)
[INFO 2017-06-27 21:02:53,945 main.py:51] epoch 0, training loss: 41762.94, average training loss: 41762.94, base loss: 20271.77
[INFO 2017-06-27 21:02:54,681 main.py:51] epoch 1, training loss: 35821.89, average training loss: 38792.41, base loss: 17487.37
[INFO 2017-06-27 21:02:55,424 main.py:51] epoch 2, training loss: 35772.85, average training loss: 37785.89, base loss: 18884.08
[INFO 2017-06-27 21:02:56,165 main.py:51] epoch 3, training loss: 33315.13, average training loss: 36668.20, base loss: 19978.81
[INFO 2017-06-27 21:02:56,904 main.py:51] epoch 4, training loss: 27114.64, average training loss: 34757.49, base loss: 19404.51
[INFO 2017-06-27 21:02:57,673 main.py:51] epoch 5, training loss: 29600.65, average training loss: 33898.02, base loss: 20335.56
[INFO 2017-06-27 21:02:58,429 main.py:51] epoch 6, training loss: 25949.38, average training loss: 32762.50, base loss: 20261.89
[INFO 2017-06-27 21:02:59,178 main.py:51] epoch 7, training loss: 24682.82, average training loss: 31752.54, base loss: 20383.97
[INFO 2017-06-27 21:02:59,925 main.py:51] epoch 8, training loss: 21753.93, average training loss: 30641.58, base loss: 19975.32
[INFO 2017-06-27 21:03:00,673 main.py:51] epoch 9, training loss: 24160.39, average training loss: 29993.46, base loss: 20004.41
[INFO 2017-06-27 21:03:01,419 main.py:51] epoch 10, training loss: 22025.21, average training loss: 29269.07, base loss: 19896.29
[INFO 2017-06-27 21:03:02,164 main.py:51] epoch 11, training loss: 23525.95, average training loss: 28790.48, base loss: 19931.42
[INFO 2017-06-27 21:03:02,906 main.py:51] epoch 12, training loss: 22554.48, average training loss: 28310.79, base loss: 19923.45
[INFO 2017-06-27 21:03:03,671 main.py:51] epoch 13, training loss: 24363.97, average training loss: 28028.87, base loss: 20050.94
[INFO 2017-06-27 21:03:04,417 main.py:51] epoch 14, training loss: 20314.18, average training loss: 27514.56, base loss: 19954.84
[INFO 2017-06-27 21:03:05,182 main.py:51] epoch 15, training loss: 22551.06, average training loss: 27204.34, base loss: 20042.56
[INFO 2017-06-27 21:03:06,004 main.py:51] epoch 16, training loss: 22527.71, average training loss: 26929.25, base loss: 20136.83
[INFO 2017-06-27 21:03:06,892 main.py:51] epoch 17, training loss: 16953.37, average training loss: 26375.03, base loss: 19869.71
[INFO 2017-06-27 21:03:07,855 main.py:51] epoch 18, training loss: 18459.55, average training loss: 25958.43, base loss: 19757.79
[INFO 2017-06-27 21:03:08,825 main.py:51] epoch 19, training loss: 19391.92, average training loss: 25630.10, base loss: 19726.04
[INFO 2017-06-27 21:03:09,797 main.py:51] epoch 20, training loss: 23287.11, average training loss: 25518.53, base loss: 19908.59
[INFO 2017-06-27 21:03:10,770 main.py:51] epoch 21, training loss: 19323.84, average training loss: 25236.95, base loss: 19887.81
[INFO 2017-06-27 21:03:11,745 main.py:51] epoch 22, training loss: 16349.88, average training loss: 24850.56, base loss: 19711.76
[INFO 2017-06-27 21:03:12,716 main.py:51] epoch 23, training loss: 16517.88, average training loss: 24503.36, base loss: 19551.53
[INFO 2017-06-27 21:03:13,687 main.py:51] epoch 24, training loss: 19721.41, average training loss: 24312.09, base loss: 19587.34
[INFO 2017-06-27 21:03:14,660 main.py:51] epoch 25, training loss: 17638.27, average training loss: 24055.40, base loss: 19520.12
[INFO 2017-06-27 21:03:15,629 main.py:51] epoch 26, training loss: 18449.17, average training loss: 23847.76, base loss: 19510.73
[INFO 2017-06-27 21:03:16,603 main.py:51] epoch 27, training loss: 22317.96, average training loss: 23793.13, base loss: 19654.26
[INFO 2017-06-27 21:03:17,576 main.py:51] epoch 28, training loss: 17773.63, average training loss: 23585.56, base loss: 19609.60
[INFO 2017-06-27 21:03:18,549 main.py:51] epoch 29, training loss: 16949.80, average training loss: 23364.37, base loss: 19540.11
[INFO 2017-06-27 21:03:19,522 main.py:51] epoch 30, training loss: 20721.80, average training loss: 23279.12, base loss: 19609.15
[INFO 2017-06-27 21:03:20,491 main.py:51] epoch 31, training loss: 18224.84, average training loss: 23121.18, base loss: 19594.00
[INFO 2017-06-27 21:03:21,462 main.py:51] epoch 32, training loss: 19892.66, average training loss: 23023.34, base loss: 19642.98
[INFO 2017-06-27 21:03:22,435 main.py:51] epoch 33, training loss: 17657.38, average training loss: 22865.52, base loss: 19616.44
[INFO 2017-06-27 21:03:23,404 main.py:51] epoch 34, training loss: 18774.93, average training loss: 22748.65, base loss: 19611.21
[INFO 2017-06-27 21:03:24,376 main.py:51] epoch 35, training loss: 23028.81, average training loss: 22756.43, base loss: 19761.58
[INFO 2017-06-27 21:03:25,345 main.py:51] epoch 36, training loss: 19215.94, average training loss: 22660.74, base loss: 19774.23
[INFO 2017-06-27 21:03:26,315 main.py:51] epoch 37, training loss: 21049.93, average training loss: 22618.35, base loss: 19857.66
[INFO 2017-06-27 21:03:27,292 main.py:51] epoch 38, training loss: 19976.54, average training loss: 22550.61, base loss: 19895.95
[INFO 2017-06-27 21:03:28,264 main.py:51] epoch 39, training loss: 18067.91, average training loss: 22438.54, base loss: 19882.55
[INFO 2017-06-27 21:03:29,234 main.py:51] epoch 40, training loss: 17877.07, average training loss: 22327.29, base loss: 19866.33
[INFO 2017-06-27 21:03:30,204 main.py:51] epoch 41, training loss: 20209.10, average training loss: 22276.85, base loss: 19909.94
[INFO 2017-06-27 21:03:31,178 main.py:51] epoch 42, training loss: 18809.46, average training loss: 22196.22, base loss: 19922.55
[INFO 2017-06-27 21:03:32,152 main.py:51] epoch 43, training loss: 18691.55, average training loss: 22116.57, base loss: 19924.67
[INFO 2017-06-27 21:03:33,125 main.py:51] epoch 44, training loss: 17047.24, average training loss: 22003.91, base loss: 19890.33
[INFO 2017-06-27 21:03:34,099 main.py:51] epoch 45, training loss: 17555.35, average training loss: 21907.21, base loss: 19860.35
[INFO 2017-06-27 21:03:35,069 main.py:51] epoch 46, training loss: 17765.71, average training loss: 21819.09, base loss: 19847.10
[INFO 2017-06-27 21:03:36,042 main.py:51] epoch 47, training loss: 16180.88, average training loss: 21701.63, base loss: 19791.01
[INFO 2017-06-27 21:03:37,014 main.py:51] epoch 48, training loss: 19103.72, average training loss: 21648.61, base loss: 19803.35
[INFO 2017-06-27 21:03:37,985 main.py:51] epoch 49, training loss: 20316.45, average training loss: 21621.97, base loss: 19854.47
[INFO 2017-06-27 21:03:38,957 main.py:51] epoch 50, training loss: 22167.11, average training loss: 21632.65, base loss: 19937.62
[INFO 2017-06-27 21:03:39,927 main.py:51] epoch 51, training loss: 17197.10, average training loss: 21547.36, base loss: 19919.64
[INFO 2017-06-27 21:03:40,901 main.py:51] epoch 52, training loss: 18250.67, average training loss: 21485.15, base loss: 19925.93
[INFO 2017-06-27 21:03:41,872 main.py:51] epoch 53, training loss: 21504.47, average training loss: 21485.51, base loss: 19987.10
[INFO 2017-06-27 21:03:42,842 main.py:51] epoch 54, training loss: 17528.77, average training loss: 21413.57, base loss: 19964.51
[INFO 2017-06-27 21:03:43,814 main.py:51] epoch 55, training loss: 19017.32, average training loss: 21370.78, base loss: 19973.67
[INFO 2017-06-27 21:03:44,788 main.py:51] epoch 56, training loss: 17978.31, average training loss: 21311.26, base loss: 19965.27
[INFO 2017-06-27 21:03:45,763 main.py:51] epoch 57, training loss: 22174.06, average training loss: 21326.14, base loss: 20043.66
[INFO 2017-06-27 21:03:46,735 main.py:51] epoch 58, training loss: 16959.87, average training loss: 21252.13, base loss: 20004.86
[INFO 2017-06-27 21:03:47,705 main.py:51] epoch 59, training loss: 17618.24, average training loss: 21191.57, base loss: 19995.99
[INFO 2017-06-27 21:03:48,678 main.py:51] epoch 60, training loss: 18067.17, average training loss: 21140.35, base loss: 19988.28
[INFO 2017-06-27 21:03:49,653 main.py:51] epoch 61, training loss: 15985.19, average training loss: 21057.20, base loss: 19940.54
[INFO 2017-06-27 21:03:50,623 main.py:51] epoch 62, training loss: 20744.87, average training loss: 21052.24, base loss: 19985.47
[INFO 2017-06-27 21:03:51,602 main.py:51] epoch 63, training loss: 17845.36, average training loss: 21002.14, base loss: 19979.72
[INFO 2017-06-27 21:03:52,575 main.py:51] epoch 64, training loss: 18099.36, average training loss: 20957.48, base loss: 19975.62
[INFO 2017-06-27 21:03:53,547 main.py:51] epoch 65, training loss: 17464.83, average training loss: 20904.56, base loss: 19961.72
[INFO 2017-06-27 21:03:54,520 main.py:51] epoch 66, training loss: 19970.18, average training loss: 20890.61, base loss: 19996.04
[INFO 2017-06-27 21:03:55,493 main.py:51] epoch 67, training loss: 16549.95, average training loss: 20826.78, base loss: 19969.83
[INFO 2017-06-27 21:03:56,468 main.py:51] epoch 68, training loss: 16478.19, average training loss: 20763.76, base loss: 19936.50
[INFO 2017-06-27 21:03:57,441 main.py:51] epoch 69, training loss: 18733.60, average training loss: 20734.76, base loss: 19948.13
[INFO 2017-06-27 21:03:58,416 main.py:51] epoch 70, training loss: 16470.90, average training loss: 20674.70, base loss: 19915.59
[INFO 2017-06-27 21:03:59,392 main.py:51] epoch 71, training loss: 18335.27, average training loss: 20642.21, base loss: 19920.75
[INFO 2017-06-27 21:04:00,368 main.py:51] epoch 72, training loss: 15484.48, average training loss: 20571.56, base loss: 19876.71
[INFO 2017-06-27 21:04:01,339 main.py:51] epoch 73, training loss: 17219.85, average training loss: 20526.26, base loss: 19868.34
[INFO 2017-06-27 21:04:02,314 main.py:51] epoch 74, training loss: 16502.89, average training loss: 20472.62, base loss: 19850.96
[INFO 2017-06-27 21:04:03,286 main.py:51] epoch 75, training loss: 21144.84, average training loss: 20481.46, base loss: 19903.84
[INFO 2017-06-27 21:04:04,256 main.py:51] epoch 76, training loss: 18019.86, average training loss: 20449.49, base loss: 19905.24
[INFO 2017-06-27 21:04:05,230 main.py:51] epoch 77, training loss: 20705.42, average training loss: 20452.77, base loss: 19954.94
[INFO 2017-06-27 21:04:06,201 main.py:51] epoch 78, training loss: 17730.01, average training loss: 20418.31, base loss: 19957.59
[INFO 2017-06-27 21:04:07,173 main.py:51] epoch 79, training loss: 18672.83, average training loss: 20396.49, base loss: 19967.06
[INFO 2017-06-27 21:04:08,148 main.py:51] epoch 80, training loss: 16476.15, average training loss: 20348.09, base loss: 19943.00
[INFO 2017-06-27 21:04:09,119 main.py:51] epoch 81, training loss: 17884.18, average training loss: 20318.04, base loss: 19948.35
[INFO 2017-06-27 21:04:10,090 main.py:51] epoch 82, training loss: 16145.51, average training loss: 20267.77, base loss: 19924.27
[INFO 2017-06-27 21:04:11,061 main.py:51] epoch 83, training loss: 19601.81, average training loss: 20259.84, base loss: 19940.90
[INFO 2017-06-27 21:04:12,031 main.py:51] epoch 84, training loss: 18786.42, average training loss: 20242.51, base loss: 19957.52
[INFO 2017-06-27 21:04:13,002 main.py:51] epoch 85, training loss: 20851.07, average training loss: 20249.59, base loss: 20004.76
[INFO 2017-06-27 21:04:13,978 main.py:51] epoch 86, training loss: 17456.19, average training loss: 20217.48, base loss: 20002.06
[INFO 2017-06-27 21:04:14,957 main.py:51] epoch 87, training loss: 18199.38, average training loss: 20194.54, base loss: 20014.61
[INFO 2017-06-27 21:04:15,931 main.py:51] epoch 88, training loss: 21131.49, average training loss: 20205.07, base loss: 20067.44
[INFO 2017-06-27 21:04:16,903 main.py:51] epoch 89, training loss: 15953.16, average training loss: 20157.83, base loss: 20043.02
[INFO 2017-06-27 21:04:17,872 main.py:51] epoch 90, training loss: 16813.89, average training loss: 20121.08, base loss: 20031.75
[INFO 2017-06-27 21:04:18,845 main.py:51] epoch 91, training loss: 13736.05, average training loss: 20051.68, base loss: 19975.60
[INFO 2017-06-27 21:04:19,820 main.py:51] epoch 92, training loss: 17117.44, average training loss: 20020.13, base loss: 19956.82
[INFO 2017-06-27 21:04:20,796 main.py:51] epoch 93, training loss: 18931.83, average training loss: 20008.55, base loss: 19969.12
[INFO 2017-06-27 21:04:21,768 main.py:51] epoch 94, training loss: 17612.44, average training loss: 19983.33, base loss: 19968.68
[INFO 2017-06-27 21:04:22,745 main.py:51] epoch 95, training loss: 16579.29, average training loss: 19947.87, base loss: 19950.52
[INFO 2017-06-27 21:04:23,717 main.py:51] epoch 96, training loss: 18014.51, average training loss: 19927.94, base loss: 19957.15
[INFO 2017-06-27 21:04:24,688 main.py:51] epoch 97, training loss: 17371.70, average training loss: 19901.85, base loss: 19955.98
[INFO 2017-06-27 21:04:25,662 main.py:51] epoch 98, training loss: 17612.72, average training loss: 19878.73, base loss: 19956.90
[INFO 2017-06-27 21:04:26,635 main.py:51] epoch 99, training loss: 17414.07, average training loss: 19854.09, base loss: 19958.04
[INFO 2017-06-27 21:04:26,635 main.py:53] epoch 99, testing
[INFO 2017-06-27 21:04:30,853 main.py:105] average testing loss: 17525.98, base loss: 19964.19
[INFO 2017-06-27 21:04:30,853 main.py:106] improve_loss: 2438.22, improve_percent: 0.12
[INFO 2017-06-27 21:04:30,854 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:04:30,866 main.py:76] current best improved percent: 0.12
[INFO 2017-06-27 21:04:31,836 main.py:51] epoch 100, training loss: 15394.42, average training loss: 19809.93, base loss: 19937.41
[INFO 2017-06-27 21:04:32,805 main.py:51] epoch 101, training loss: 19574.51, average training loss: 19807.62, base loss: 19966.75
[INFO 2017-06-27 21:04:33,775 main.py:51] epoch 102, training loss: 17617.65, average training loss: 19786.36, base loss: 19968.87
[INFO 2017-06-27 21:04:34,746 main.py:51] epoch 103, training loss: 16087.17, average training loss: 19750.79, base loss: 19944.87
[INFO 2017-06-27 21:04:35,718 main.py:51] epoch 104, training loss: 17066.07, average training loss: 19725.22, base loss: 19936.91
[INFO 2017-06-27 21:04:36,689 main.py:51] epoch 105, training loss: 16298.12, average training loss: 19692.89, base loss: 19914.27
[INFO 2017-06-27 21:04:37,662 main.py:51] epoch 106, training loss: 20750.68, average training loss: 19702.78, base loss: 19951.46
[INFO 2017-06-27 21:04:38,635 main.py:51] epoch 107, training loss: 21415.19, average training loss: 19718.63, base loss: 19995.01
[INFO 2017-06-27 21:04:39,610 main.py:51] epoch 108, training loss: 16497.48, average training loss: 19689.08, base loss: 19980.26
[INFO 2017-06-27 21:04:40,584 main.py:51] epoch 109, training loss: 17494.78, average training loss: 19669.13, base loss: 19987.14
[INFO 2017-06-27 21:04:41,565 main.py:51] epoch 110, training loss: 16851.51, average training loss: 19643.75, base loss: 19978.82
[INFO 2017-06-27 21:04:42,540 main.py:51] epoch 111, training loss: 18139.41, average training loss: 19630.32, base loss: 19992.04
[INFO 2017-06-27 21:04:43,513 main.py:51] epoch 112, training loss: 18489.89, average training loss: 19620.22, base loss: 19999.73
[INFO 2017-06-27 21:04:44,488 main.py:51] epoch 113, training loss: 17745.97, average training loss: 19603.78, base loss: 20000.62
[INFO 2017-06-27 21:04:45,459 main.py:51] epoch 114, training loss: 20779.81, average training loss: 19614.01, base loss: 20036.28
[INFO 2017-06-27 21:04:46,434 main.py:51] epoch 115, training loss: 17773.92, average training loss: 19598.15, base loss: 20045.29
[INFO 2017-06-27 21:04:47,411 main.py:51] epoch 116, training loss: 17500.15, average training loss: 19580.22, base loss: 20045.64
[INFO 2017-06-27 21:04:48,382 main.py:51] epoch 117, training loss: 17134.21, average training loss: 19559.49, base loss: 20040.90
[INFO 2017-06-27 21:04:49,356 main.py:51] epoch 118, training loss: 18153.77, average training loss: 19547.67, base loss: 20044.80
[INFO 2017-06-27 21:04:50,328 main.py:51] epoch 119, training loss: 17787.58, average training loss: 19533.01, base loss: 20045.33
[INFO 2017-06-27 21:04:51,300 main.py:51] epoch 120, training loss: 16390.81, average training loss: 19507.04, base loss: 20042.66
[INFO 2017-06-27 21:04:52,277 main.py:51] epoch 121, training loss: 18946.57, average training loss: 19502.44, base loss: 20060.15
[INFO 2017-06-27 21:04:53,252 main.py:51] epoch 122, training loss: 17992.35, average training loss: 19490.17, base loss: 20067.09
[INFO 2017-06-27 21:04:54,224 main.py:51] epoch 123, training loss: 16025.81, average training loss: 19462.23, base loss: 20055.02
[INFO 2017-06-27 21:04:55,198 main.py:51] epoch 124, training loss: 17070.08, average training loss: 19443.09, base loss: 20056.18
[INFO 2017-06-27 21:04:56,170 main.py:51] epoch 125, training loss: 16596.16, average training loss: 19420.50, base loss: 20045.60
[INFO 2017-06-27 21:04:57,143 main.py:51] epoch 126, training loss: 15326.77, average training loss: 19388.26, base loss: 20024.38
[INFO 2017-06-27 21:04:58,116 main.py:51] epoch 127, training loss: 17400.98, average training loss: 19372.74, base loss: 20031.16
[INFO 2017-06-27 21:04:59,089 main.py:51] epoch 128, training loss: 18453.48, average training loss: 19365.61, base loss: 20041.51
[INFO 2017-06-27 21:05:00,062 main.py:51] epoch 129, training loss: 19495.61, average training loss: 19366.61, base loss: 20066.03
[INFO 2017-06-27 21:05:01,035 main.py:51] epoch 130, training loss: 15566.33, average training loss: 19337.60, base loss: 20048.08
[INFO 2017-06-27 21:05:02,009 main.py:51] epoch 131, training loss: 19504.15, average training loss: 19338.86, base loss: 20076.99
[INFO 2017-06-27 21:05:02,980 main.py:51] epoch 132, training loss: 17056.21, average training loss: 19321.70, base loss: 20071.10
[INFO 2017-06-27 21:05:03,951 main.py:51] epoch 133, training loss: 16597.73, average training loss: 19301.37, base loss: 20065.34
[INFO 2017-06-27 21:05:04,922 main.py:51] epoch 134, training loss: 17030.00, average training loss: 19284.55, base loss: 20057.49
[INFO 2017-06-27 21:05:05,894 main.py:51] epoch 135, training loss: 18094.50, average training loss: 19275.80, base loss: 20065.81
[INFO 2017-06-27 21:05:06,867 main.py:51] epoch 136, training loss: 17503.03, average training loss: 19262.86, base loss: 20069.02
[INFO 2017-06-27 21:05:07,838 main.py:51] epoch 137, training loss: 16490.05, average training loss: 19242.76, base loss: 20062.79
[INFO 2017-06-27 21:05:08,810 main.py:51] epoch 138, training loss: 15659.77, average training loss: 19216.99, base loss: 20046.80
[INFO 2017-06-27 21:05:09,784 main.py:51] epoch 139, training loss: 16452.18, average training loss: 19197.24, base loss: 20039.36
[INFO 2017-06-27 21:05:10,762 main.py:51] epoch 140, training loss: 18530.54, average training loss: 19192.51, base loss: 20054.97
[INFO 2017-06-27 21:05:11,735 main.py:51] epoch 141, training loss: 16453.06, average training loss: 19173.22, base loss: 20051.68
[INFO 2017-06-27 21:05:12,712 main.py:51] epoch 142, training loss: 14655.20, average training loss: 19141.62, base loss: 20027.81
[INFO 2017-06-27 21:05:13,688 main.py:51] epoch 143, training loss: 17424.43, average training loss: 19129.70, base loss: 20029.55
[INFO 2017-06-27 21:05:14,661 main.py:51] epoch 144, training loss: 16138.11, average training loss: 19109.07, base loss: 20019.42
[INFO 2017-06-27 21:05:15,635 main.py:51] epoch 145, training loss: 15520.42, average training loss: 19084.49, base loss: 20007.32
[INFO 2017-06-27 21:05:16,618 main.py:51] epoch 146, training loss: 14183.07, average training loss: 19051.14, base loss: 19984.26
[INFO 2017-06-27 21:05:17,595 main.py:51] epoch 147, training loss: 18010.84, average training loss: 19044.12, base loss: 19998.67
[INFO 2017-06-27 21:05:18,571 main.py:51] epoch 148, training loss: 19386.73, average training loss: 19046.41, base loss: 20018.28
[INFO 2017-06-27 21:05:19,547 main.py:51] epoch 149, training loss: 16998.09, average training loss: 19032.76, base loss: 20014.63
[INFO 2017-06-27 21:05:20,519 main.py:51] epoch 150, training loss: 15201.28, average training loss: 19007.39, base loss: 19988.21
[INFO 2017-06-27 21:05:21,495 main.py:51] epoch 151, training loss: 15381.74, average training loss: 18983.53, base loss: 19978.30
[INFO 2017-06-27 21:05:22,469 main.py:51] epoch 152, training loss: 15386.12, average training loss: 18960.02, base loss: 19969.75
[INFO 2017-06-27 21:05:23,445 main.py:51] epoch 153, training loss: 15523.26, average training loss: 18937.70, base loss: 19956.49
[INFO 2017-06-27 21:05:24,417 main.py:51] epoch 154, training loss: 17056.98, average training loss: 18925.57, base loss: 19958.58
[INFO 2017-06-27 21:05:25,388 main.py:51] epoch 155, training loss: 11577.19, average training loss: 18878.46, base loss: 19911.77
[INFO 2017-06-27 21:05:26,359 main.py:51] epoch 156, training loss: 18229.41, average training loss: 18874.33, base loss: 19928.43
[INFO 2017-06-27 21:05:27,332 main.py:51] epoch 157, training loss: 16949.94, average training loss: 18862.15, base loss: 19930.57
[INFO 2017-06-27 21:05:28,304 main.py:51] epoch 158, training loss: 15145.29, average training loss: 18838.77, base loss: 19918.60
[INFO 2017-06-27 21:05:29,273 main.py:51] epoch 159, training loss: 16969.14, average training loss: 18827.09, base loss: 19922.55
[INFO 2017-06-27 21:05:30,246 main.py:51] epoch 160, training loss: 16054.01, average training loss: 18809.86, base loss: 19911.30
[INFO 2017-06-27 21:05:31,221 main.py:51] epoch 161, training loss: 17421.80, average training loss: 18801.30, base loss: 19915.10
[INFO 2017-06-27 21:05:32,197 main.py:51] epoch 162, training loss: 20023.60, average training loss: 18808.80, base loss: 19946.37
[INFO 2017-06-27 21:05:33,170 main.py:51] epoch 163, training loss: 19258.04, average training loss: 18811.53, base loss: 19968.00
[INFO 2017-06-27 21:05:34,140 main.py:51] epoch 164, training loss: 16229.86, average training loss: 18795.89, base loss: 19966.21
[INFO 2017-06-27 21:05:35,114 main.py:51] epoch 165, training loss: 16088.52, average training loss: 18779.58, base loss: 19966.09
[INFO 2017-06-27 21:05:36,090 main.py:51] epoch 166, training loss: 17010.79, average training loss: 18768.99, base loss: 19967.65
[INFO 2017-06-27 21:05:37,067 main.py:51] epoch 167, training loss: 18146.22, average training loss: 18765.28, base loss: 19982.75
[INFO 2017-06-27 21:05:38,038 main.py:51] epoch 168, training loss: 16477.32, average training loss: 18751.74, base loss: 19979.48
[INFO 2017-06-27 21:05:39,011 main.py:51] epoch 169, training loss: 13418.64, average training loss: 18720.37, base loss: 19946.81
[INFO 2017-06-27 21:05:39,985 main.py:51] epoch 170, training loss: 16564.77, average training loss: 18707.76, base loss: 19947.95
[INFO 2017-06-27 21:05:40,958 main.py:51] epoch 171, training loss: 18373.84, average training loss: 18705.82, base loss: 19964.69
[INFO 2017-06-27 21:05:41,929 main.py:51] epoch 172, training loss: 19542.48, average training loss: 18710.66, base loss: 19986.77
[INFO 2017-06-27 21:05:42,909 main.py:51] epoch 173, training loss: 17187.63, average training loss: 18701.91, base loss: 19984.63
[INFO 2017-06-27 21:05:43,881 main.py:51] epoch 174, training loss: 17007.71, average training loss: 18692.23, base loss: 19985.68
[INFO 2017-06-27 21:05:44,854 main.py:51] epoch 175, training loss: 14724.12, average training loss: 18669.68, base loss: 19968.85
[INFO 2017-06-27 21:05:45,830 main.py:51] epoch 176, training loss: 15841.46, average training loss: 18653.70, base loss: 19961.83
[INFO 2017-06-27 21:05:46,807 main.py:51] epoch 177, training loss: 18214.24, average training loss: 18651.23, base loss: 19970.54
[INFO 2017-06-27 21:05:47,777 main.py:51] epoch 178, training loss: 14024.75, average training loss: 18625.39, base loss: 19956.75
[INFO 2017-06-27 21:05:48,754 main.py:51] epoch 179, training loss: 17149.61, average training loss: 18617.19, base loss: 19956.21
[INFO 2017-06-27 21:05:49,731 main.py:51] epoch 180, training loss: 14594.92, average training loss: 18594.96, base loss: 19938.85
[INFO 2017-06-27 21:05:50,709 main.py:51] epoch 181, training loss: 17514.25, average training loss: 18589.03, base loss: 19939.99
[INFO 2017-06-27 21:05:51,691 main.py:51] epoch 182, training loss: 18729.96, average training loss: 18589.80, base loss: 19955.77
[INFO 2017-06-27 21:05:52,671 main.py:51] epoch 183, training loss: 16272.46, average training loss: 18577.20, base loss: 19956.81
[INFO 2017-06-27 21:05:53,654 main.py:51] epoch 184, training loss: 14543.71, average training loss: 18555.40, base loss: 19942.53
[INFO 2017-06-27 21:05:54,634 main.py:51] epoch 185, training loss: 16877.86, average training loss: 18546.38, base loss: 19945.84
[INFO 2017-06-27 21:05:55,610 main.py:51] epoch 186, training loss: 15131.54, average training loss: 18528.12, base loss: 19936.55
[INFO 2017-06-27 21:05:56,583 main.py:51] epoch 187, training loss: 15902.63, average training loss: 18514.15, base loss: 19932.51
[INFO 2017-06-27 21:05:57,555 main.py:51] epoch 188, training loss: 15587.69, average training loss: 18498.67, base loss: 19929.44
[INFO 2017-06-27 21:05:58,533 main.py:51] epoch 189, training loss: 19059.82, average training loss: 18501.62, base loss: 19948.81
[INFO 2017-06-27 21:05:59,509 main.py:51] epoch 190, training loss: 17895.58, average training loss: 18498.45, base loss: 19957.39
[INFO 2017-06-27 21:06:00,482 main.py:51] epoch 191, training loss: 17409.05, average training loss: 18492.78, base loss: 19961.88
[INFO 2017-06-27 21:06:01,461 main.py:51] epoch 192, training loss: 13360.96, average training loss: 18466.19, base loss: 19939.77
[INFO 2017-06-27 21:06:02,441 main.py:51] epoch 193, training loss: 15916.66, average training loss: 18453.04, base loss: 19939.59
[INFO 2017-06-27 21:06:03,418 main.py:51] epoch 194, training loss: 16320.61, average training loss: 18442.11, base loss: 19944.68
[INFO 2017-06-27 21:06:04,402 main.py:51] epoch 195, training loss: 16896.51, average training loss: 18434.22, base loss: 19950.60
[INFO 2017-06-27 21:06:05,377 main.py:51] epoch 196, training loss: 16564.83, average training loss: 18424.73, base loss: 19951.65
[INFO 2017-06-27 21:06:06,358 main.py:51] epoch 197, training loss: 14792.09, average training loss: 18406.39, base loss: 19941.89
[INFO 2017-06-27 21:06:07,333 main.py:51] epoch 198, training loss: 15739.24, average training loss: 18392.98, base loss: 19938.24
[INFO 2017-06-27 21:06:08,308 main.py:51] epoch 199, training loss: 16770.46, average training loss: 18384.87, base loss: 19937.37
[INFO 2017-06-27 21:06:08,308 main.py:53] epoch 199, testing
[INFO 2017-06-27 21:06:12,565 main.py:105] average testing loss: 16160.41, base loss: 19504.05
[INFO 2017-06-27 21:06:12,565 main.py:106] improve_loss: 3343.63, improve_percent: 0.17
[INFO 2017-06-27 21:06:12,566 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:06:12,579 main.py:76] current best improved percent: 0.17
[INFO 2017-06-27 21:06:13,553 main.py:51] epoch 200, training loss: 16769.52, average training loss: 18376.84, base loss: 19942.96
[INFO 2017-06-27 21:06:14,528 main.py:51] epoch 201, training loss: 14029.96, average training loss: 18355.32, base loss: 19928.27
[INFO 2017-06-27 21:06:15,504 main.py:51] epoch 202, training loss: 18416.28, average training loss: 18355.62, base loss: 19940.62
[INFO 2017-06-27 21:06:16,484 main.py:51] epoch 203, training loss: 17895.59, average training loss: 18353.36, base loss: 19954.40
[INFO 2017-06-27 21:06:17,460 main.py:51] epoch 204, training loss: 15374.46, average training loss: 18338.83, base loss: 19946.22
[INFO 2017-06-27 21:06:18,434 main.py:51] epoch 205, training loss: 15901.12, average training loss: 18327.00, base loss: 19944.04
[INFO 2017-06-27 21:06:19,407 main.py:51] epoch 206, training loss: 16955.52, average training loss: 18320.37, base loss: 19950.28
[INFO 2017-06-27 21:06:20,385 main.py:51] epoch 207, training loss: 14171.01, average training loss: 18300.42, base loss: 19939.71
[INFO 2017-06-27 21:06:21,368 main.py:51] epoch 208, training loss: 16860.55, average training loss: 18293.53, base loss: 19947.60
[INFO 2017-06-27 21:06:22,346 main.py:51] epoch 209, training loss: 16359.13, average training loss: 18284.32, base loss: 19945.65
[INFO 2017-06-27 21:06:23,321 main.py:51] epoch 210, training loss: 18125.02, average training loss: 18283.57, base loss: 19957.53
[INFO 2017-06-27 21:06:24,302 main.py:51] epoch 211, training loss: 16063.92, average training loss: 18273.10, base loss: 19954.21
[INFO 2017-06-27 21:06:25,277 main.py:51] epoch 212, training loss: 17675.15, average training loss: 18270.29, base loss: 19966.06
[INFO 2017-06-27 21:06:26,255 main.py:51] epoch 213, training loss: 17164.17, average training loss: 18265.12, base loss: 19971.85
[INFO 2017-06-27 21:06:27,228 main.py:51] epoch 214, training loss: 16969.79, average training loss: 18259.10, base loss: 19977.93
[INFO 2017-06-27 21:06:28,204 main.py:51] epoch 215, training loss: 15804.42, average training loss: 18247.73, base loss: 19976.92
[INFO 2017-06-27 21:06:29,180 main.py:51] epoch 216, training loss: 15407.95, average training loss: 18234.65, base loss: 19973.21
[INFO 2017-06-27 21:06:30,154 main.py:51] epoch 217, training loss: 18179.22, average training loss: 18234.39, base loss: 19988.45
[INFO 2017-06-27 21:06:31,128 main.py:51] epoch 218, training loss: 16137.70, average training loss: 18224.82, base loss: 19993.62
[INFO 2017-06-27 21:06:32,104 main.py:51] epoch 219, training loss: 15587.07, average training loss: 18212.83, base loss: 19985.55
[INFO 2017-06-27 21:06:33,075 main.py:51] epoch 220, training loss: 13601.86, average training loss: 18191.96, base loss: 19970.38
[INFO 2017-06-27 21:06:34,054 main.py:51] epoch 221, training loss: 16559.00, average training loss: 18184.61, base loss: 19970.85
[INFO 2017-06-27 21:06:35,030 main.py:51] epoch 222, training loss: 14155.55, average training loss: 18166.54, base loss: 19957.02
[INFO 2017-06-27 21:06:36,007 main.py:51] epoch 223, training loss: 16125.00, average training loss: 18157.43, base loss: 19954.55
[INFO 2017-06-27 21:06:36,988 main.py:51] epoch 224, training loss: 16003.26, average training loss: 18147.85, base loss: 19952.92
[INFO 2017-06-27 21:06:37,962 main.py:51] epoch 225, training loss: 15634.23, average training loss: 18136.73, base loss: 19951.85
[INFO 2017-06-27 21:06:38,937 main.py:51] epoch 226, training loss: 14879.44, average training loss: 18122.38, base loss: 19947.13
[INFO 2017-06-27 21:06:39,915 main.py:51] epoch 227, training loss: 14651.54, average training loss: 18107.16, base loss: 19938.03
[INFO 2017-06-27 21:06:40,892 main.py:51] epoch 228, training loss: 16658.20, average training loss: 18100.83, base loss: 19936.74
[INFO 2017-06-27 21:06:41,868 main.py:51] epoch 229, training loss: 16442.40, average training loss: 18093.62, base loss: 19939.88
[INFO 2017-06-27 21:06:42,855 main.py:51] epoch 230, training loss: 15718.61, average training loss: 18083.34, base loss: 19932.45
[INFO 2017-06-27 21:06:43,836 main.py:51] epoch 231, training loss: 15065.80, average training loss: 18070.33, base loss: 19926.73
[INFO 2017-06-27 21:06:44,815 main.py:51] epoch 232, training loss: 17139.05, average training loss: 18066.33, base loss: 19931.87
[INFO 2017-06-27 21:06:45,792 main.py:51] epoch 233, training loss: 15307.51, average training loss: 18054.54, base loss: 19926.91
[INFO 2017-06-27 21:06:46,769 main.py:51] epoch 234, training loss: 17881.47, average training loss: 18053.81, base loss: 19933.15
[INFO 2017-06-27 21:06:47,744 main.py:51] epoch 235, training loss: 16248.16, average training loss: 18046.16, base loss: 19931.47
[INFO 2017-06-27 21:06:48,724 main.py:51] epoch 236, training loss: 17580.04, average training loss: 18044.19, base loss: 19943.03
[INFO 2017-06-27 21:06:49,704 main.py:51] epoch 237, training loss: 16268.28, average training loss: 18036.73, base loss: 19947.75
[INFO 2017-06-27 21:06:50,681 main.py:51] epoch 238, training loss: 15724.66, average training loss: 18027.05, base loss: 19944.98
[INFO 2017-06-27 21:06:51,659 main.py:51] epoch 239, training loss: 13390.18, average training loss: 18007.73, base loss: 19927.79
[INFO 2017-06-27 21:06:52,635 main.py:51] epoch 240, training loss: 14713.54, average training loss: 17994.07, base loss: 19921.70
[INFO 2017-06-27 21:06:53,608 main.py:51] epoch 241, training loss: 18247.51, average training loss: 17995.11, base loss: 19938.31
[INFO 2017-06-27 21:06:54,585 main.py:51] epoch 242, training loss: 16336.64, average training loss: 17988.29, base loss: 19940.73
[INFO 2017-06-27 21:06:55,561 main.py:51] epoch 243, training loss: 17131.81, average training loss: 17984.78, base loss: 19948.15
[INFO 2017-06-27 21:06:56,538 main.py:51] epoch 244, training loss: 14962.56, average training loss: 17972.44, base loss: 19943.99
[INFO 2017-06-27 21:06:57,515 main.py:51] epoch 245, training loss: 15295.56, average training loss: 17961.56, base loss: 19940.67
[INFO 2017-06-27 21:06:58,493 main.py:51] epoch 246, training loss: 15299.76, average training loss: 17950.78, base loss: 19933.93
[INFO 2017-06-27 21:06:59,471 main.py:51] epoch 247, training loss: 16745.83, average training loss: 17945.93, base loss: 19937.80
[INFO 2017-06-27 21:07:00,449 main.py:51] epoch 248, training loss: 16153.68, average training loss: 17938.73, base loss: 19937.00
[INFO 2017-06-27 21:07:01,425 main.py:51] epoch 249, training loss: 15732.64, average training loss: 17929.90, base loss: 19937.59
[INFO 2017-06-27 21:07:02,405 main.py:51] epoch 250, training loss: 15412.31, average training loss: 17919.87, base loss: 19936.09
[INFO 2017-06-27 21:07:03,380 main.py:51] epoch 251, training loss: 14119.01, average training loss: 17904.79, base loss: 19926.58
[INFO 2017-06-27 21:07:04,356 main.py:51] epoch 252, training loss: 16437.84, average training loss: 17898.99, base loss: 19934.47
[INFO 2017-06-27 21:07:05,334 main.py:51] epoch 253, training loss: 16091.43, average training loss: 17891.88, base loss: 19938.07
[INFO 2017-06-27 21:07:06,309 main.py:51] epoch 254, training loss: 14940.13, average training loss: 17880.30, base loss: 19934.22
[INFO 2017-06-27 21:07:07,285 main.py:51] epoch 255, training loss: 17874.97, average training loss: 17880.28, base loss: 19946.98
[INFO 2017-06-27 21:07:08,265 main.py:51] epoch 256, training loss: 14641.37, average training loss: 17867.68, base loss: 19940.90
[INFO 2017-06-27 21:07:09,242 main.py:51] epoch 257, training loss: 15219.80, average training loss: 17857.41, base loss: 19938.27
[INFO 2017-06-27 21:07:10,217 main.py:51] epoch 258, training loss: 13237.17, average training loss: 17839.57, base loss: 19926.57
[INFO 2017-06-27 21:07:11,193 main.py:51] epoch 259, training loss: 14115.72, average training loss: 17825.25, base loss: 19910.05
[INFO 2017-06-27 21:07:12,170 main.py:51] epoch 260, training loss: 16293.99, average training loss: 17819.39, base loss: 19913.16
[INFO 2017-06-27 21:07:13,151 main.py:51] epoch 261, training loss: 16838.53, average training loss: 17815.64, base loss: 19918.50
[INFO 2017-06-27 21:07:14,131 main.py:51] epoch 262, training loss: 17231.63, average training loss: 17813.42, base loss: 19925.83
[INFO 2017-06-27 21:07:15,107 main.py:51] epoch 263, training loss: 16682.23, average training loss: 17809.14, base loss: 19930.23
[INFO 2017-06-27 21:07:16,089 main.py:51] epoch 264, training loss: 16896.34, average training loss: 17805.69, base loss: 19931.85
[INFO 2017-06-27 21:07:17,062 main.py:51] epoch 265, training loss: 14673.77, average training loss: 17793.92, base loss: 19925.68
[INFO 2017-06-27 21:07:18,035 main.py:51] epoch 266, training loss: 16366.94, average training loss: 17788.57, base loss: 19925.47
[INFO 2017-06-27 21:07:19,012 main.py:51] epoch 267, training loss: 16667.87, average training loss: 17784.39, base loss: 19929.97
[INFO 2017-06-27 21:07:19,988 main.py:51] epoch 268, training loss: 15486.13, average training loss: 17775.85, base loss: 19924.25
[INFO 2017-06-27 21:07:20,965 main.py:51] epoch 269, training loss: 14630.04, average training loss: 17764.20, base loss: 19915.62
[INFO 2017-06-27 21:07:21,942 main.py:51] epoch 270, training loss: 20021.32, average training loss: 17772.53, base loss: 19935.24
[INFO 2017-06-27 21:07:22,923 main.py:51] epoch 271, training loss: 13476.38, average training loss: 17756.73, base loss: 19920.49
[INFO 2017-06-27 21:07:23,900 main.py:51] epoch 272, training loss: 16163.42, average training loss: 17750.89, base loss: 19917.32
[INFO 2017-06-27 21:07:24,875 main.py:51] epoch 273, training loss: 17854.35, average training loss: 17751.27, base loss: 19928.41
[INFO 2017-06-27 21:07:25,848 main.py:51] epoch 274, training loss: 14155.63, average training loss: 17738.20, base loss: 19919.02
[INFO 2017-06-27 21:07:26,819 main.py:51] epoch 275, training loss: 17266.26, average training loss: 17736.49, base loss: 19926.85
[INFO 2017-06-27 21:07:27,796 main.py:51] epoch 276, training loss: 14015.53, average training loss: 17723.05, base loss: 19915.41
[INFO 2017-06-27 21:07:28,768 main.py:51] epoch 277, training loss: 16916.73, average training loss: 17720.15, base loss: 19917.90
[INFO 2017-06-27 21:07:29,745 main.py:51] epoch 278, training loss: 16071.64, average training loss: 17714.24, base loss: 19918.86
[INFO 2017-06-27 21:07:30,723 main.py:51] epoch 279, training loss: 14059.54, average training loss: 17701.19, base loss: 19909.76
[INFO 2017-06-27 21:07:31,696 main.py:51] epoch 280, training loss: 14177.69, average training loss: 17688.65, base loss: 19903.52
[INFO 2017-06-27 21:07:32,674 main.py:51] epoch 281, training loss: 14382.45, average training loss: 17676.93, base loss: 19896.28
[INFO 2017-06-27 21:07:33,646 main.py:51] epoch 282, training loss: 14668.41, average training loss: 17666.30, base loss: 19892.04
[INFO 2017-06-27 21:07:34,623 main.py:51] epoch 283, training loss: 15699.46, average training loss: 17659.37, base loss: 19893.38
[INFO 2017-06-27 21:07:35,600 main.py:51] epoch 284, training loss: 18263.69, average training loss: 17661.49, base loss: 19907.17
[INFO 2017-06-27 21:07:36,572 main.py:51] epoch 285, training loss: 15578.74, average training loss: 17654.21, base loss: 19904.59
[INFO 2017-06-27 21:07:37,547 main.py:51] epoch 286, training loss: 15978.59, average training loss: 17648.37, base loss: 19902.79
[INFO 2017-06-27 21:07:38,519 main.py:51] epoch 287, training loss: 15786.67, average training loss: 17641.91, base loss: 19900.82
[INFO 2017-06-27 21:07:39,493 main.py:51] epoch 288, training loss: 15011.50, average training loss: 17632.81, base loss: 19902.03
[INFO 2017-06-27 21:07:40,469 main.py:51] epoch 289, training loss: 15007.20, average training loss: 17623.75, base loss: 19899.07
[INFO 2017-06-27 21:07:41,442 main.py:51] epoch 290, training loss: 16080.24, average training loss: 17618.45, base loss: 19900.62
[INFO 2017-06-27 21:07:42,421 main.py:51] epoch 291, training loss: 13472.88, average training loss: 17604.25, base loss: 19891.22
[INFO 2017-06-27 21:07:43,399 main.py:51] epoch 292, training loss: 16415.82, average training loss: 17600.20, base loss: 19890.13
[INFO 2017-06-27 21:07:44,377 main.py:51] epoch 293, training loss: 18746.79, average training loss: 17604.10, base loss: 19906.80
[INFO 2017-06-27 21:07:45,354 main.py:51] epoch 294, training loss: 14709.91, average training loss: 17594.28, base loss: 19901.51
[INFO 2017-06-27 21:07:46,330 main.py:51] epoch 295, training loss: 16426.98, average training loss: 17590.34, base loss: 19905.51
[INFO 2017-06-27 21:07:47,312 main.py:51] epoch 296, training loss: 16375.02, average training loss: 17586.25, base loss: 19910.20
[INFO 2017-06-27 21:07:48,289 main.py:51] epoch 297, training loss: 17901.83, average training loss: 17587.31, base loss: 19919.86
[INFO 2017-06-27 21:07:49,260 main.py:51] epoch 298, training loss: 16896.43, average training loss: 17585.00, base loss: 19921.56
[INFO 2017-06-27 21:07:50,238 main.py:51] epoch 299, training loss: 14433.97, average training loss: 17574.49, base loss: 19909.32
[INFO 2017-06-27 21:07:50,238 main.py:53] epoch 299, testing
[INFO 2017-06-27 21:07:54,466 main.py:105] average testing loss: 15861.68, base loss: 19762.26
[INFO 2017-06-27 21:07:54,466 main.py:106] improve_loss: 3900.58, improve_percent: 0.20
[INFO 2017-06-27 21:07:54,467 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:07:54,479 main.py:76] current best improved percent: 0.20
[INFO 2017-06-27 21:07:55,457 main.py:51] epoch 300, training loss: 15615.83, average training loss: 17567.99, base loss: 19911.48
[INFO 2017-06-27 21:07:56,431 main.py:51] epoch 301, training loss: 15091.68, average training loss: 17559.79, base loss: 19909.98
[INFO 2017-06-27 21:07:57,407 main.py:51] epoch 302, training loss: 14718.56, average training loss: 17550.41, base loss: 19904.90
[INFO 2017-06-27 21:07:58,378 main.py:51] epoch 303, training loss: 18445.73, average training loss: 17553.35, base loss: 19915.16
[INFO 2017-06-27 21:07:59,355 main.py:51] epoch 304, training loss: 15716.29, average training loss: 17547.33, base loss: 19918.35
[INFO 2017-06-27 21:08:00,330 main.py:51] epoch 305, training loss: 16045.35, average training loss: 17542.42, base loss: 19922.47
[INFO 2017-06-27 21:08:01,308 main.py:51] epoch 306, training loss: 15390.56, average training loss: 17535.41, base loss: 19919.91
[INFO 2017-06-27 21:08:02,280 main.py:51] epoch 307, training loss: 16272.41, average training loss: 17531.31, base loss: 19923.89
[INFO 2017-06-27 21:08:03,255 main.py:51] epoch 308, training loss: 16426.09, average training loss: 17527.74, base loss: 19925.83
[INFO 2017-06-27 21:08:04,231 main.py:51] epoch 309, training loss: 13910.33, average training loss: 17516.07, base loss: 19918.23
[INFO 2017-06-27 21:08:05,206 main.py:51] epoch 310, training loss: 16573.21, average training loss: 17513.04, base loss: 19922.46
[INFO 2017-06-27 21:08:06,182 main.py:51] epoch 311, training loss: 12884.14, average training loss: 17498.20, base loss: 19908.08
[INFO 2017-06-27 21:08:07,153 main.py:51] epoch 312, training loss: 17880.95, average training loss: 17499.42, base loss: 19919.88
[INFO 2017-06-27 21:08:08,129 main.py:51] epoch 313, training loss: 17540.85, average training loss: 17499.55, base loss: 19929.55
[INFO 2017-06-27 21:08:09,101 main.py:51] epoch 314, training loss: 15453.82, average training loss: 17493.06, base loss: 19929.52
[INFO 2017-06-27 21:08:10,079 main.py:51] epoch 315, training loss: 16188.53, average training loss: 17488.93, base loss: 19931.93
[INFO 2017-06-27 21:08:11,057 main.py:51] epoch 316, training loss: 14522.73, average training loss: 17479.57, base loss: 19927.55
[INFO 2017-06-27 21:08:12,033 main.py:51] epoch 317, training loss: 15107.49, average training loss: 17472.12, base loss: 19927.53
[INFO 2017-06-27 21:08:13,008 main.py:51] epoch 318, training loss: 15616.16, average training loss: 17466.30, base loss: 19928.31
[INFO 2017-06-27 21:08:13,982 main.py:51] epoch 319, training loss: 13719.51, average training loss: 17454.59, base loss: 19918.02
[INFO 2017-06-27 21:08:14,957 main.py:51] epoch 320, training loss: 16629.18, average training loss: 17452.02, base loss: 19925.40
[INFO 2017-06-27 21:08:15,932 main.py:51] epoch 321, training loss: 15592.88, average training loss: 17446.24, base loss: 19927.76
[INFO 2017-06-27 21:08:16,910 main.py:51] epoch 322, training loss: 15469.04, average training loss: 17440.12, base loss: 19929.92
[INFO 2017-06-27 21:08:17,883 main.py:51] epoch 323, training loss: 14913.89, average training loss: 17432.33, base loss: 19927.88
[INFO 2017-06-27 21:08:18,857 main.py:51] epoch 324, training loss: 13615.44, average training loss: 17420.58, base loss: 19916.88
[INFO 2017-06-27 21:08:19,834 main.py:51] epoch 325, training loss: 14492.26, average training loss: 17411.60, base loss: 19914.04
[INFO 2017-06-27 21:08:20,810 main.py:51] epoch 326, training loss: 18533.95, average training loss: 17415.03, base loss: 19925.73
[INFO 2017-06-27 21:08:21,783 main.py:51] epoch 327, training loss: 17016.54, average training loss: 17413.82, base loss: 19936.24
[INFO 2017-06-27 21:08:22,760 main.py:51] epoch 328, training loss: 18350.15, average training loss: 17416.66, base loss: 19947.24
[INFO 2017-06-27 21:08:23,732 main.py:51] epoch 329, training loss: 17044.94, average training loss: 17415.54, base loss: 19952.72
[INFO 2017-06-27 21:08:24,706 main.py:51] epoch 330, training loss: 15618.86, average training loss: 17410.11, base loss: 19950.79
[INFO 2017-06-27 21:08:25,683 main.py:51] epoch 331, training loss: 14464.77, average training loss: 17401.24, base loss: 19945.56
[INFO 2017-06-27 21:08:26,660 main.py:51] epoch 332, training loss: 15253.87, average training loss: 17394.79, base loss: 19943.04
[INFO 2017-06-27 21:08:27,642 main.py:51] epoch 333, training loss: 15636.00, average training loss: 17389.52, base loss: 19941.44
[INFO 2017-06-27 21:08:28,616 main.py:51] epoch 334, training loss: 14674.70, average training loss: 17381.42, base loss: 19939.52
[INFO 2017-06-27 21:08:29,593 main.py:51] epoch 335, training loss: 14034.04, average training loss: 17371.45, base loss: 19932.35
[INFO 2017-06-27 21:08:30,568 main.py:51] epoch 336, training loss: 15952.30, average training loss: 17367.24, base loss: 19933.75
[INFO 2017-06-27 21:08:31,544 main.py:51] epoch 337, training loss: 13383.96, average training loss: 17355.46, base loss: 19925.96
[INFO 2017-06-27 21:08:32,523 main.py:51] epoch 338, training loss: 15317.65, average training loss: 17349.45, base loss: 19924.52
[INFO 2017-06-27 21:08:33,500 main.py:51] epoch 339, training loss: 14948.84, average training loss: 17342.39, base loss: 19923.35
[INFO 2017-06-27 21:08:34,478 main.py:51] epoch 340, training loss: 15405.93, average training loss: 17336.71, base loss: 19922.50
[INFO 2017-06-27 21:08:35,455 main.py:51] epoch 341, training loss: 15353.66, average training loss: 17330.91, base loss: 19920.19
[INFO 2017-06-27 21:08:36,431 main.py:51] epoch 342, training loss: 15317.16, average training loss: 17325.04, base loss: 19918.00
[INFO 2017-06-27 21:08:37,412 main.py:51] epoch 343, training loss: 14800.20, average training loss: 17317.70, base loss: 19916.02
[INFO 2017-06-27 21:08:38,384 main.py:51] epoch 344, training loss: 15911.87, average training loss: 17313.62, base loss: 19919.07
[INFO 2017-06-27 21:08:39,362 main.py:51] epoch 345, training loss: 14036.40, average training loss: 17304.15, base loss: 19914.81
[INFO 2017-06-27 21:08:40,338 main.py:51] epoch 346, training loss: 15721.34, average training loss: 17299.59, base loss: 19918.68
[INFO 2017-06-27 21:08:41,317 main.py:51] epoch 347, training loss: 14416.85, average training loss: 17291.31, base loss: 19915.90
[INFO 2017-06-27 21:08:42,291 main.py:51] epoch 348, training loss: 16265.74, average training loss: 17288.37, base loss: 19918.47
[INFO 2017-06-27 21:08:43,265 main.py:51] epoch 349, training loss: 14744.50, average training loss: 17281.10, base loss: 19917.12
[INFO 2017-06-27 21:08:44,236 main.py:51] epoch 350, training loss: 14663.46, average training loss: 17273.64, base loss: 19915.93
[INFO 2017-06-27 21:08:45,209 main.py:51] epoch 351, training loss: 18103.93, average training loss: 17276.00, base loss: 19926.68
[INFO 2017-06-27 21:08:46,187 main.py:51] epoch 352, training loss: 14128.88, average training loss: 17267.09, base loss: 19921.22
[INFO 2017-06-27 21:08:47,162 main.py:51] epoch 353, training loss: 15152.31, average training loss: 17261.11, base loss: 19921.45
[INFO 2017-06-27 21:08:48,140 main.py:51] epoch 354, training loss: 15160.55, average training loss: 17255.20, base loss: 19923.60
[INFO 2017-06-27 21:08:49,119 main.py:51] epoch 355, training loss: 14354.07, average training loss: 17247.05, base loss: 19920.42
[INFO 2017-06-27 21:08:50,096 main.py:51] epoch 356, training loss: 14259.12, average training loss: 17238.68, base loss: 19917.13
[INFO 2017-06-27 21:08:51,070 main.py:51] epoch 357, training loss: 15634.93, average training loss: 17234.20, base loss: 19919.72
[INFO 2017-06-27 21:08:52,048 main.py:51] epoch 358, training loss: 14150.89, average training loss: 17225.61, base loss: 19915.89
[INFO 2017-06-27 21:08:53,028 main.py:51] epoch 359, training loss: 13609.82, average training loss: 17215.56, base loss: 19906.93
[INFO 2017-06-27 21:08:54,007 main.py:51] epoch 360, training loss: 15065.74, average training loss: 17209.61, base loss: 19905.68
[INFO 2017-06-27 21:08:54,984 main.py:51] epoch 361, training loss: 14902.38, average training loss: 17203.24, base loss: 19903.07
[INFO 2017-06-27 21:08:55,957 main.py:51] epoch 362, training loss: 13353.04, average training loss: 17192.63, base loss: 19894.67
[INFO 2017-06-27 21:08:56,939 main.py:51] epoch 363, training loss: 16281.95, average training loss: 17190.13, base loss: 19897.20
[INFO 2017-06-27 21:08:57,917 main.py:51] epoch 364, training loss: 17639.41, average training loss: 17191.36, base loss: 19905.59
[INFO 2017-06-27 21:08:58,890 main.py:51] epoch 365, training loss: 16129.96, average training loss: 17188.46, base loss: 19908.06
[INFO 2017-06-27 21:08:59,868 main.py:51] epoch 366, training loss: 14519.58, average training loss: 17181.19, base loss: 19905.66
[INFO 2017-06-27 21:09:00,850 main.py:51] epoch 367, training loss: 14402.40, average training loss: 17173.63, base loss: 19901.33
[INFO 2017-06-27 21:09:01,820 main.py:51] epoch 368, training loss: 13666.81, average training loss: 17164.13, base loss: 19893.89
[INFO 2017-06-27 21:09:02,795 main.py:51] epoch 369, training loss: 14161.83, average training loss: 17156.02, base loss: 19887.42
[INFO 2017-06-27 21:09:03,771 main.py:51] epoch 370, training loss: 13356.11, average training loss: 17145.77, base loss: 19878.95
[INFO 2017-06-27 21:09:04,748 main.py:51] epoch 371, training loss: 14588.37, average training loss: 17138.90, base loss: 19873.89
[INFO 2017-06-27 21:09:05,725 main.py:51] epoch 372, training loss: 14479.98, average training loss: 17131.77, base loss: 19871.25
[INFO 2017-06-27 21:09:06,698 main.py:51] epoch 373, training loss: 16962.18, average training loss: 17131.32, base loss: 19875.11
[INFO 2017-06-27 21:09:07,674 main.py:51] epoch 374, training loss: 17610.59, average training loss: 17132.60, base loss: 19881.65
[INFO 2017-06-27 21:09:08,648 main.py:51] epoch 375, training loss: 14780.97, average training loss: 17126.34, base loss: 19880.88
[INFO 2017-06-27 21:09:09,628 main.py:51] epoch 376, training loss: 14065.55, average training loss: 17118.22, base loss: 19875.62
[INFO 2017-06-27 21:09:10,605 main.py:51] epoch 377, training loss: 13345.11, average training loss: 17108.24, base loss: 19870.03
[INFO 2017-06-27 21:09:11,580 main.py:51] epoch 378, training loss: 14164.71, average training loss: 17100.47, base loss: 19867.98
[INFO 2017-06-27 21:09:12,557 main.py:51] epoch 379, training loss: 14610.40, average training loss: 17093.92, base loss: 19863.26
[INFO 2017-06-27 21:09:13,541 main.py:51] epoch 380, training loss: 13178.60, average training loss: 17083.65, base loss: 19853.16
[INFO 2017-06-27 21:09:14,517 main.py:51] epoch 381, training loss: 16267.01, average training loss: 17081.51, base loss: 19859.29
[INFO 2017-06-27 21:09:15,492 main.py:51] epoch 382, training loss: 15880.97, average training loss: 17078.37, base loss: 19864.54
[INFO 2017-06-27 21:09:16,469 main.py:51] epoch 383, training loss: 17696.25, average training loss: 17079.98, base loss: 19874.84
[INFO 2017-06-27 21:09:17,452 main.py:51] epoch 384, training loss: 13764.14, average training loss: 17071.37, base loss: 19870.77
[INFO 2017-06-27 21:09:18,428 main.py:51] epoch 385, training loss: 14655.23, average training loss: 17065.11, base loss: 19867.19
[INFO 2017-06-27 21:09:19,402 main.py:51] epoch 386, training loss: 14762.09, average training loss: 17059.16, base loss: 19862.27
[INFO 2017-06-27 21:09:20,380 main.py:51] epoch 387, training loss: 15240.43, average training loss: 17054.47, base loss: 19864.06
[INFO 2017-06-27 21:09:21,353 main.py:51] epoch 388, training loss: 13279.20, average training loss: 17044.77, base loss: 19855.52
[INFO 2017-06-27 21:09:22,333 main.py:51] epoch 389, training loss: 14614.85, average training loss: 17038.54, base loss: 19849.64
[INFO 2017-06-27 21:09:23,314 main.py:51] epoch 390, training loss: 14455.17, average training loss: 17031.93, base loss: 19846.98
[INFO 2017-06-27 21:09:24,290 main.py:51] epoch 391, training loss: 13410.55, average training loss: 17022.69, base loss: 19840.27
[INFO 2017-06-27 21:09:25,270 main.py:51] epoch 392, training loss: 13803.69, average training loss: 17014.50, base loss: 19835.66
[INFO 2017-06-27 21:09:26,248 main.py:51] epoch 393, training loss: 15222.77, average training loss: 17009.95, base loss: 19833.39
[INFO 2017-06-27 21:09:27,226 main.py:51] epoch 394, training loss: 15599.06, average training loss: 17006.38, base loss: 19835.45
[INFO 2017-06-27 21:09:28,200 main.py:51] epoch 395, training loss: 15672.09, average training loss: 17003.01, base loss: 19837.10
[INFO 2017-06-27 21:09:29,174 main.py:51] epoch 396, training loss: 14129.89, average training loss: 16995.77, base loss: 19834.39
[INFO 2017-06-27 21:09:30,161 main.py:51] epoch 397, training loss: 15441.44, average training loss: 16991.87, base loss: 19832.79
[INFO 2017-06-27 21:09:31,139 main.py:51] epoch 398, training loss: 18041.96, average training loss: 16994.50, base loss: 19841.81
[INFO 2017-06-27 21:09:32,117 main.py:51] epoch 399, training loss: 16050.45, average training loss: 16992.14, base loss: 19843.61
[INFO 2017-06-27 21:09:32,118 main.py:53] epoch 399, testing
[INFO 2017-06-27 21:09:36,357 main.py:105] average testing loss: 15479.62, base loss: 19932.09
[INFO 2017-06-27 21:09:36,358 main.py:106] improve_loss: 4452.47, improve_percent: 0.22
[INFO 2017-06-27 21:09:36,358 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:09:36,371 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 21:09:37,350 main.py:51] epoch 400, training loss: 15008.74, average training loss: 16987.19, base loss: 19841.59
[INFO 2017-06-27 21:09:38,326 main.py:51] epoch 401, training loss: 16662.79, average training loss: 16986.39, base loss: 19847.61
[INFO 2017-06-27 21:09:39,303 main.py:51] epoch 402, training loss: 14621.06, average training loss: 16980.52, base loss: 19846.53
[INFO 2017-06-27 21:09:40,279 main.py:51] epoch 403, training loss: 15881.90, average training loss: 16977.80, base loss: 19851.51
[INFO 2017-06-27 21:09:41,257 main.py:51] epoch 404, training loss: 14610.61, average training loss: 16971.95, base loss: 19849.18
[INFO 2017-06-27 21:09:42,237 main.py:51] epoch 405, training loss: 15181.02, average training loss: 16967.54, base loss: 19851.68
[INFO 2017-06-27 21:09:43,217 main.py:51] epoch 406, training loss: 12787.76, average training loss: 16957.27, base loss: 19840.61
[INFO 2017-06-27 21:09:44,194 main.py:51] epoch 407, training loss: 16218.90, average training loss: 16955.46, base loss: 19845.87
[INFO 2017-06-27 21:09:45,172 main.py:51] epoch 408, training loss: 17157.01, average training loss: 16955.96, base loss: 19853.68
[INFO 2017-06-27 21:09:46,145 main.py:51] epoch 409, training loss: 16561.19, average training loss: 16954.99, base loss: 19858.48
[INFO 2017-06-27 21:09:47,123 main.py:51] epoch 410, training loss: 15326.63, average training loss: 16951.03, base loss: 19858.93
[INFO 2017-06-27 21:09:48,100 main.py:51] epoch 411, training loss: 13529.50, average training loss: 16942.73, base loss: 19853.54
[INFO 2017-06-27 21:09:49,081 main.py:51] epoch 412, training loss: 14477.84, average training loss: 16936.76, base loss: 19851.30
[INFO 2017-06-27 21:09:50,056 main.py:51] epoch 413, training loss: 14194.52, average training loss: 16930.13, base loss: 19849.14
[INFO 2017-06-27 21:09:51,031 main.py:51] epoch 414, training loss: 13929.21, average training loss: 16922.90, base loss: 19844.97
[INFO 2017-06-27 21:09:52,014 main.py:51] epoch 415, training loss: 12911.82, average training loss: 16913.26, base loss: 19839.14
[INFO 2017-06-27 21:09:52,991 main.py:51] epoch 416, training loss: 15665.65, average training loss: 16910.27, base loss: 19841.15
[INFO 2017-06-27 21:09:53,966 main.py:51] epoch 417, training loss: 16118.04, average training loss: 16908.37, base loss: 19845.22
[INFO 2017-06-27 21:09:54,942 main.py:51] epoch 418, training loss: 14068.28, average training loss: 16901.60, base loss: 19840.86
[INFO 2017-06-27 21:09:55,920 main.py:51] epoch 419, training loss: 13615.67, average training loss: 16893.77, base loss: 19836.97
[INFO 2017-06-27 21:09:56,893 main.py:51] epoch 420, training loss: 14814.52, average training loss: 16888.83, base loss: 19834.64
[INFO 2017-06-27 21:09:57,870 main.py:51] epoch 421, training loss: 15946.79, average training loss: 16886.60, base loss: 19837.45
[INFO 2017-06-27 21:09:58,846 main.py:51] epoch 422, training loss: 15922.52, average training loss: 16884.32, base loss: 19842.18
[INFO 2017-06-27 21:09:59,832 main.py:51] epoch 423, training loss: 14672.80, average training loss: 16879.11, base loss: 19839.77
[INFO 2017-06-27 21:10:00,813 main.py:51] epoch 424, training loss: 16033.48, average training loss: 16877.12, base loss: 19842.82
[INFO 2017-06-27 21:10:01,794 main.py:51] epoch 425, training loss: 14264.20, average training loss: 16870.98, base loss: 19837.35
[INFO 2017-06-27 21:10:02,773 main.py:51] epoch 426, training loss: 15376.18, average training loss: 16867.48, base loss: 19837.96
[INFO 2017-06-27 21:10:03,758 main.py:51] epoch 427, training loss: 15242.86, average training loss: 16863.69, base loss: 19838.42
[INFO 2017-06-27 21:10:04,733 main.py:51] epoch 428, training loss: 14108.22, average training loss: 16857.26, base loss: 19834.13
[INFO 2017-06-27 21:10:05,709 main.py:51] epoch 429, training loss: 15308.01, average training loss: 16853.66, base loss: 19836.97
[INFO 2017-06-27 21:10:06,691 main.py:51] epoch 430, training loss: 13496.97, average training loss: 16845.87, base loss: 19832.65
[INFO 2017-06-27 21:10:07,663 main.py:51] epoch 431, training loss: 14638.26, average training loss: 16840.76, base loss: 19831.47
[INFO 2017-06-27 21:10:08,640 main.py:51] epoch 432, training loss: 14386.55, average training loss: 16835.09, base loss: 19829.06
[INFO 2017-06-27 21:10:09,621 main.py:51] epoch 433, training loss: 14889.75, average training loss: 16830.61, base loss: 19827.79
[INFO 2017-06-27 21:10:10,597 main.py:51] epoch 434, training loss: 15058.31, average training loss: 16826.54, base loss: 19830.24
[INFO 2017-06-27 21:10:11,573 main.py:51] epoch 435, training loss: 14338.76, average training loss: 16820.83, base loss: 19829.06
[INFO 2017-06-27 21:10:12,550 main.py:51] epoch 436, training loss: 14660.03, average training loss: 16815.89, base loss: 19828.30
[INFO 2017-06-27 21:10:13,527 main.py:51] epoch 437, training loss: 16732.45, average training loss: 16815.70, base loss: 19834.08
[INFO 2017-06-27 21:10:14,507 main.py:51] epoch 438, training loss: 13690.73, average training loss: 16808.58, base loss: 19827.89
[INFO 2017-06-27 21:10:15,484 main.py:51] epoch 439, training loss: 16715.53, average training loss: 16808.37, base loss: 19834.58
[INFO 2017-06-27 21:10:16,457 main.py:51] epoch 440, training loss: 16616.78, average training loss: 16807.93, base loss: 19836.64
[INFO 2017-06-27 21:10:17,437 main.py:51] epoch 441, training loss: 13008.94, average training loss: 16799.34, base loss: 19829.54
[INFO 2017-06-27 21:10:18,409 main.py:51] epoch 442, training loss: 16244.88, average training loss: 16798.09, base loss: 19834.52
[INFO 2017-06-27 21:10:19,393 main.py:51] epoch 443, training loss: 16636.09, average training loss: 16797.72, base loss: 19841.53
[INFO 2017-06-27 21:10:20,369 main.py:51] epoch 444, training loss: 14468.45, average training loss: 16792.49, base loss: 19838.63
[INFO 2017-06-27 21:10:21,345 main.py:51] epoch 445, training loss: 15000.74, average training loss: 16788.47, base loss: 19839.02
[INFO 2017-06-27 21:10:22,325 main.py:51] epoch 446, training loss: 11924.82, average training loss: 16777.59, base loss: 19827.10
[INFO 2017-06-27 21:10:23,304 main.py:51] epoch 447, training loss: 15099.00, average training loss: 16773.84, base loss: 19827.24
[INFO 2017-06-27 21:10:24,278 main.py:51] epoch 448, training loss: 16484.69, average training loss: 16773.20, base loss: 19830.05
[INFO 2017-06-27 21:10:25,255 main.py:51] epoch 449, training loss: 15171.42, average training loss: 16769.64, base loss: 19832.52
[INFO 2017-06-27 21:10:26,233 main.py:51] epoch 450, training loss: 13792.11, average training loss: 16763.04, base loss: 19829.61
[INFO 2017-06-27 21:10:27,205 main.py:51] epoch 451, training loss: 12566.50, average training loss: 16753.75, base loss: 19818.36
[INFO 2017-06-27 21:10:28,176 main.py:51] epoch 452, training loss: 16117.49, average training loss: 16752.35, base loss: 19822.50
[INFO 2017-06-27 21:10:29,150 main.py:51] epoch 453, training loss: 15601.46, average training loss: 16749.81, base loss: 19822.97
[INFO 2017-06-27 21:10:30,128 main.py:51] epoch 454, training loss: 13693.55, average training loss: 16743.09, base loss: 19818.78
[INFO 2017-06-27 21:10:31,112 main.py:51] epoch 455, training loss: 16277.09, average training loss: 16742.07, base loss: 19821.03
[INFO 2017-06-27 21:10:32,095 main.py:51] epoch 456, training loss: 12966.00, average training loss: 16733.81, base loss: 19816.43
[INFO 2017-06-27 21:10:33,077 main.py:51] epoch 457, training loss: 13549.91, average training loss: 16726.86, base loss: 19810.62
[INFO 2017-06-27 21:10:34,053 main.py:51] epoch 458, training loss: 15162.25, average training loss: 16723.45, base loss: 19812.54
[INFO 2017-06-27 21:10:35,029 main.py:51] epoch 459, training loss: 16046.38, average training loss: 16721.98, base loss: 19817.40
[INFO 2017-06-27 21:10:36,010 main.py:51] epoch 460, training loss: 15466.99, average training loss: 16719.26, base loss: 19817.53
[INFO 2017-06-27 21:10:36,992 main.py:51] epoch 461, training loss: 15959.06, average training loss: 16717.61, base loss: 19816.59
[INFO 2017-06-27 21:10:37,969 main.py:51] epoch 462, training loss: 14828.91, average training loss: 16713.53, base loss: 19815.66
[INFO 2017-06-27 21:10:38,946 main.py:51] epoch 463, training loss: 15881.05, average training loss: 16711.74, base loss: 19817.52
[INFO 2017-06-27 21:10:39,925 main.py:51] epoch 464, training loss: 15749.35, average training loss: 16709.67, base loss: 19821.93
[INFO 2017-06-27 21:10:40,900 main.py:51] epoch 465, training loss: 14407.56, average training loss: 16704.73, base loss: 19820.01
[INFO 2017-06-27 21:10:41,874 main.py:51] epoch 466, training loss: 14249.60, average training loss: 16699.47, base loss: 19818.16
[INFO 2017-06-27 21:10:42,852 main.py:51] epoch 467, training loss: 14499.84, average training loss: 16694.77, base loss: 19815.84
[INFO 2017-06-27 21:10:43,828 main.py:51] epoch 468, training loss: 17384.33, average training loss: 16696.24, base loss: 19823.09
[INFO 2017-06-27 21:10:44,815 main.py:51] epoch 469, training loss: 17531.33, average training loss: 16698.02, base loss: 19830.91
[INFO 2017-06-27 21:10:45,792 main.py:51] epoch 470, training loss: 15129.29, average training loss: 16694.69, base loss: 19833.33
[INFO 2017-06-27 21:10:46,770 main.py:51] epoch 471, training loss: 15292.45, average training loss: 16691.71, base loss: 19835.18
[INFO 2017-06-27 21:10:47,748 main.py:51] epoch 472, training loss: 13225.24, average training loss: 16684.39, base loss: 19830.66
[INFO 2017-06-27 21:10:48,724 main.py:51] epoch 473, training loss: 14155.68, average training loss: 16679.05, base loss: 19830.26
[INFO 2017-06-27 21:10:49,702 main.py:51] epoch 474, training loss: 15546.55, average training loss: 16676.67, base loss: 19830.68
[INFO 2017-06-27 21:10:50,683 main.py:51] epoch 475, training loss: 16154.19, average training loss: 16675.57, base loss: 19835.78
[INFO 2017-06-27 21:10:51,663 main.py:51] epoch 476, training loss: 16671.75, average training loss: 16675.56, base loss: 19840.16
[INFO 2017-06-27 21:10:52,637 main.py:51] epoch 477, training loss: 13831.85, average training loss: 16669.61, base loss: 19833.49
[INFO 2017-06-27 21:10:53,614 main.py:51] epoch 478, training loss: 13563.05, average training loss: 16663.13, base loss: 19827.11
[INFO 2017-06-27 21:10:54,599 main.py:51] epoch 479, training loss: 15769.42, average training loss: 16661.26, base loss: 19829.80
[INFO 2017-06-27 21:10:55,575 main.py:51] epoch 480, training loss: 15812.31, average training loss: 16659.50, base loss: 19833.20
[INFO 2017-06-27 21:10:56,550 main.py:51] epoch 481, training loss: 14529.81, average training loss: 16655.08, base loss: 19831.61
[INFO 2017-06-27 21:10:57,533 main.py:51] epoch 482, training loss: 15085.88, average training loss: 16651.83, base loss: 19834.02
[INFO 2017-06-27 21:10:58,513 main.py:51] epoch 483, training loss: 13757.28, average training loss: 16645.85, base loss: 19828.30
[INFO 2017-06-27 21:10:59,489 main.py:51] epoch 484, training loss: 12729.91, average training loss: 16637.78, base loss: 19822.21
[INFO 2017-06-27 21:11:00,470 main.py:51] epoch 485, training loss: 13194.70, average training loss: 16630.69, base loss: 19817.25
[INFO 2017-06-27 21:11:01,445 main.py:51] epoch 486, training loss: 15655.64, average training loss: 16628.69, base loss: 19819.37
[INFO 2017-06-27 21:11:02,430 main.py:51] epoch 487, training loss: 13385.36, average training loss: 16622.05, base loss: 19812.62
[INFO 2017-06-27 21:11:03,409 main.py:51] epoch 488, training loss: 14303.45, average training loss: 16617.30, base loss: 19811.48
[INFO 2017-06-27 21:11:04,383 main.py:51] epoch 489, training loss: 14681.96, average training loss: 16613.35, base loss: 19810.49
[INFO 2017-06-27 21:11:05,363 main.py:51] epoch 490, training loss: 13405.38, average training loss: 16606.82, base loss: 19806.20
[INFO 2017-06-27 21:11:06,342 main.py:51] epoch 491, training loss: 13868.30, average training loss: 16601.25, base loss: 19804.00
[INFO 2017-06-27 21:11:07,320 main.py:51] epoch 492, training loss: 14235.72, average training loss: 16596.46, base loss: 19804.45
[INFO 2017-06-27 21:11:08,297 main.py:51] epoch 493, training loss: 13989.38, average training loss: 16591.18, base loss: 19803.09
[INFO 2017-06-27 21:11:09,269 main.py:51] epoch 494, training loss: 15878.62, average training loss: 16589.74, base loss: 19809.46
[INFO 2017-06-27 21:11:10,249 main.py:51] epoch 495, training loss: 13368.37, average training loss: 16583.24, base loss: 19806.35
[INFO 2017-06-27 21:11:11,229 main.py:51] epoch 496, training loss: 14145.61, average training loss: 16578.34, base loss: 19802.12
[INFO 2017-06-27 21:11:12,202 main.py:51] epoch 497, training loss: 14153.40, average training loss: 16573.47, base loss: 19799.45
[INFO 2017-06-27 21:11:13,178 main.py:51] epoch 498, training loss: 14083.03, average training loss: 16568.48, base loss: 19797.51
[INFO 2017-06-27 21:11:14,156 main.py:51] epoch 499, training loss: 14035.31, average training loss: 16563.41, base loss: 19794.30
[INFO 2017-06-27 21:11:14,156 main.py:53] epoch 499, testing
[INFO 2017-06-27 21:11:18,398 main.py:105] average testing loss: 15009.33, base loss: 20224.90
[INFO 2017-06-27 21:11:18,398 main.py:106] improve_loss: 5215.57, improve_percent: 0.26
[INFO 2017-06-27 21:11:18,399 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:11:18,412 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 21:11:19,386 main.py:51] epoch 500, training loss: 14032.83, average training loss: 16558.36, base loss: 19791.50
[INFO 2017-06-27 21:11:20,365 main.py:51] epoch 501, training loss: 16190.79, average training loss: 16557.63, base loss: 19795.98
[INFO 2017-06-27 21:11:21,340 main.py:51] epoch 502, training loss: 17352.36, average training loss: 16559.21, base loss: 19803.33
[INFO 2017-06-27 21:11:22,321 main.py:51] epoch 503, training loss: 14355.38, average training loss: 16554.84, base loss: 19802.74
[INFO 2017-06-27 21:11:23,299 main.py:51] epoch 504, training loss: 15124.78, average training loss: 16552.01, base loss: 19807.16
[INFO 2017-06-27 21:11:24,276 main.py:51] epoch 505, training loss: 13573.59, average training loss: 16546.12, base loss: 19801.00
[INFO 2017-06-27 21:11:25,256 main.py:51] epoch 506, training loss: 14305.01, average training loss: 16541.70, base loss: 19799.02
[INFO 2017-06-27 21:11:26,233 main.py:51] epoch 507, training loss: 12918.46, average training loss: 16534.57, base loss: 19796.60
[INFO 2017-06-27 21:11:27,216 main.py:51] epoch 508, training loss: 14281.80, average training loss: 16530.14, base loss: 19794.77
[INFO 2017-06-27 21:11:28,194 main.py:51] epoch 509, training loss: 16759.45, average training loss: 16530.59, base loss: 19800.53
[INFO 2017-06-27 21:11:29,170 main.py:51] epoch 510, training loss: 15055.90, average training loss: 16527.70, base loss: 19802.45
[INFO 2017-06-27 21:11:30,155 main.py:51] epoch 511, training loss: 14501.61, average training loss: 16523.75, base loss: 19803.45
[INFO 2017-06-27 21:11:31,133 main.py:51] epoch 512, training loss: 16574.67, average training loss: 16523.85, base loss: 19808.90
[INFO 2017-06-27 21:11:32,112 main.py:51] epoch 513, training loss: 13457.28, average training loss: 16517.88, base loss: 19808.18
[INFO 2017-06-27 21:11:33,091 main.py:51] epoch 514, training loss: 14959.24, average training loss: 16514.85, base loss: 19809.73
[INFO 2017-06-27 21:11:34,069 main.py:51] epoch 515, training loss: 15700.60, average training loss: 16513.28, base loss: 19813.93
[INFO 2017-06-27 21:11:35,046 main.py:51] epoch 516, training loss: 15707.79, average training loss: 16511.72, base loss: 19818.16
[INFO 2017-06-27 21:11:36,021 main.py:51] epoch 517, training loss: 13053.38, average training loss: 16505.04, base loss: 19813.87
[INFO 2017-06-27 21:11:36,999 main.py:51] epoch 518, training loss: 14641.76, average training loss: 16501.45, base loss: 19815.82
[INFO 2017-06-27 21:11:37,978 main.py:51] epoch 519, training loss: 12911.99, average training loss: 16494.55, base loss: 19808.01
[INFO 2017-06-27 21:11:38,953 main.py:51] epoch 520, training loss: 14872.61, average training loss: 16491.44, base loss: 19807.32
[INFO 2017-06-27 21:11:39,933 main.py:51] epoch 521, training loss: 13370.43, average training loss: 16485.46, base loss: 19804.65
[INFO 2017-06-27 21:11:40,909 main.py:51] epoch 522, training loss: 14931.54, average training loss: 16482.49, base loss: 19804.29
[INFO 2017-06-27 21:11:41,881 main.py:51] epoch 523, training loss: 15062.53, average training loss: 16479.78, base loss: 19805.80
[INFO 2017-06-27 21:11:42,856 main.py:51] epoch 524, training loss: 14488.42, average training loss: 16475.98, base loss: 19805.77
[INFO 2017-06-27 21:11:43,835 main.py:51] epoch 525, training loss: 15393.93, average training loss: 16473.93, base loss: 19805.80
[INFO 2017-06-27 21:11:44,814 main.py:51] epoch 526, training loss: 13344.32, average training loss: 16467.99, base loss: 19803.38
[INFO 2017-06-27 21:11:45,795 main.py:51] epoch 527, training loss: 15030.01, average training loss: 16465.26, base loss: 19803.35
[INFO 2017-06-27 21:11:46,772 main.py:51] epoch 528, training loss: 14818.90, average training loss: 16462.15, base loss: 19802.58
[INFO 2017-06-27 21:11:47,747 main.py:51] epoch 529, training loss: 14854.10, average training loss: 16459.12, base loss: 19805.78
[INFO 2017-06-27 21:11:48,724 main.py:51] epoch 530, training loss: 14607.13, average training loss: 16455.63, base loss: 19805.28
[INFO 2017-06-27 21:11:49,711 main.py:51] epoch 531, training loss: 16355.37, average training loss: 16455.44, base loss: 19809.09
[INFO 2017-06-27 21:11:50,690 main.py:51] epoch 532, training loss: 14506.27, average training loss: 16451.78, base loss: 19805.83
[INFO 2017-06-27 21:11:51,668 main.py:51] epoch 533, training loss: 13837.61, average training loss: 16446.89, base loss: 19802.29
[INFO 2017-06-27 21:11:52,653 main.py:51] epoch 534, training loss: 13862.24, average training loss: 16442.06, base loss: 19801.56
[INFO 2017-06-27 21:11:53,633 main.py:51] epoch 535, training loss: 14662.56, average training loss: 16438.74, base loss: 19801.43
[INFO 2017-06-27 21:11:54,613 main.py:51] epoch 536, training loss: 15594.77, average training loss: 16437.17, base loss: 19806.49
[INFO 2017-06-27 21:11:55,591 main.py:51] epoch 537, training loss: 14326.06, average training loss: 16433.24, base loss: 19803.18
[INFO 2017-06-27 21:11:56,573 main.py:51] epoch 538, training loss: 15919.50, average training loss: 16432.29, base loss: 19805.25
[INFO 2017-06-27 21:11:57,549 main.py:51] epoch 539, training loss: 14774.21, average training loss: 16429.22, base loss: 19803.84
[INFO 2017-06-27 21:11:58,523 main.py:51] epoch 540, training loss: 14125.50, average training loss: 16424.96, base loss: 19801.12
[INFO 2017-06-27 21:11:59,500 main.py:51] epoch 541, training loss: 14687.24, average training loss: 16421.75, base loss: 19801.90
[INFO 2017-06-27 21:12:00,476 main.py:51] epoch 542, training loss: 15997.41, average training loss: 16420.97, base loss: 19804.15
[INFO 2017-06-27 21:12:01,456 main.py:51] epoch 543, training loss: 14587.58, average training loss: 16417.60, base loss: 19805.07
[INFO 2017-06-27 21:12:02,431 main.py:51] epoch 544, training loss: 15966.04, average training loss: 16416.77, base loss: 19808.52
[INFO 2017-06-27 21:12:03,411 main.py:51] epoch 545, training loss: 14273.88, average training loss: 16412.85, base loss: 19808.44
[INFO 2017-06-27 21:12:04,391 main.py:51] epoch 546, training loss: 17095.71, average training loss: 16414.10, base loss: 19814.61
[INFO 2017-06-27 21:12:05,371 main.py:51] epoch 547, training loss: 15026.55, average training loss: 16411.57, base loss: 19815.30
[INFO 2017-06-27 21:12:06,352 main.py:51] epoch 548, training loss: 14992.61, average training loss: 16408.98, base loss: 19817.39
[INFO 2017-06-27 21:12:07,329 main.py:51] epoch 549, training loss: 17108.20, average training loss: 16410.25, base loss: 19826.19
[INFO 2017-06-27 21:12:08,309 main.py:51] epoch 550, training loss: 15715.01, average training loss: 16408.99, base loss: 19828.91
[INFO 2017-06-27 21:12:09,289 main.py:51] epoch 551, training loss: 14528.11, average training loss: 16405.58, base loss: 19825.93
[INFO 2017-06-27 21:12:10,270 main.py:51] epoch 552, training loss: 13844.70, average training loss: 16400.95, base loss: 19825.31
[INFO 2017-06-27 21:12:11,249 main.py:51] epoch 553, training loss: 13849.94, average training loss: 16396.35, base loss: 19821.77
[INFO 2017-06-27 21:12:12,225 main.py:51] epoch 554, training loss: 15675.79, average training loss: 16395.05, base loss: 19824.63
[INFO 2017-06-27 21:12:13,202 main.py:51] epoch 555, training loss: 16196.53, average training loss: 16394.69, base loss: 19829.03
[INFO 2017-06-27 21:12:14,179 main.py:51] epoch 556, training loss: 14541.50, average training loss: 16391.36, base loss: 19829.57
[INFO 2017-06-27 21:12:15,156 main.py:51] epoch 557, training loss: 11802.04, average training loss: 16383.14, base loss: 19822.21
[INFO 2017-06-27 21:12:16,131 main.py:51] epoch 558, training loss: 15429.42, average training loss: 16381.43, base loss: 19824.68
[INFO 2017-06-27 21:12:17,109 main.py:51] epoch 559, training loss: 16822.20, average training loss: 16382.22, base loss: 19831.45
[INFO 2017-06-27 21:12:18,086 main.py:51] epoch 560, training loss: 13512.48, average training loss: 16377.11, base loss: 19830.94
[INFO 2017-06-27 21:12:19,062 main.py:51] epoch 561, training loss: 15171.12, average training loss: 16374.96, base loss: 19831.93
[INFO 2017-06-27 21:12:20,035 main.py:51] epoch 562, training loss: 13326.00, average training loss: 16369.54, base loss: 19829.12
[INFO 2017-06-27 21:12:21,007 main.py:51] epoch 563, training loss: 16813.92, average training loss: 16370.33, base loss: 19833.75
[INFO 2017-06-27 21:12:21,983 main.py:51] epoch 564, training loss: 14453.21, average training loss: 16366.94, base loss: 19832.95
[INFO 2017-06-27 21:12:22,961 main.py:51] epoch 565, training loss: 14758.60, average training loss: 16364.10, base loss: 19832.14
[INFO 2017-06-27 21:12:23,936 main.py:51] epoch 566, training loss: 15132.51, average training loss: 16361.93, base loss: 19832.63
[INFO 2017-06-27 21:12:24,913 main.py:51] epoch 567, training loss: 15361.25, average training loss: 16360.16, base loss: 19834.75
[INFO 2017-06-27 21:12:25,893 main.py:51] epoch 568, training loss: 13780.84, average training loss: 16355.63, base loss: 19832.08
[INFO 2017-06-27 21:12:26,872 main.py:51] epoch 569, training loss: 16155.85, average training loss: 16355.28, base loss: 19836.16
[INFO 2017-06-27 21:12:27,849 main.py:51] epoch 570, training loss: 14197.87, average training loss: 16351.50, base loss: 19833.99
[INFO 2017-06-27 21:12:28,826 main.py:51] epoch 571, training loss: 13991.03, average training loss: 16347.37, base loss: 19834.88
[INFO 2017-06-27 21:12:29,803 main.py:51] epoch 572, training loss: 15323.36, average training loss: 16345.59, base loss: 19838.07
[INFO 2017-06-27 21:12:30,781 main.py:51] epoch 573, training loss: 14604.70, average training loss: 16342.55, base loss: 19835.71
[INFO 2017-06-27 21:12:31,758 main.py:51] epoch 574, training loss: 13819.54, average training loss: 16338.17, base loss: 19835.78
[INFO 2017-06-27 21:12:32,735 main.py:51] epoch 575, training loss: 14196.78, average training loss: 16334.45, base loss: 19834.37
[INFO 2017-06-27 21:12:33,712 main.py:51] epoch 576, training loss: 15093.21, average training loss: 16332.30, base loss: 19836.71
[INFO 2017-06-27 21:12:34,686 main.py:51] epoch 577, training loss: 14612.89, average training loss: 16329.32, base loss: 19835.72
[INFO 2017-06-27 21:12:35,660 main.py:51] epoch 578, training loss: 14394.90, average training loss: 16325.98, base loss: 19834.59
[INFO 2017-06-27 21:12:36,641 main.py:51] epoch 579, training loss: 15634.01, average training loss: 16324.79, base loss: 19838.33
[INFO 2017-06-27 21:12:37,619 main.py:51] epoch 580, training loss: 12762.89, average training loss: 16318.66, base loss: 19833.57
[INFO 2017-06-27 21:12:38,596 main.py:51] epoch 581, training loss: 13186.72, average training loss: 16313.28, base loss: 19829.75
[INFO 2017-06-27 21:12:39,574 main.py:51] epoch 582, training loss: 13756.42, average training loss: 16308.89, base loss: 19825.53
[INFO 2017-06-27 21:12:40,554 main.py:51] epoch 583, training loss: 16750.18, average training loss: 16309.65, base loss: 19830.91
[INFO 2017-06-27 21:12:41,526 main.py:51] epoch 584, training loss: 15571.58, average training loss: 16308.39, base loss: 19834.10
[INFO 2017-06-27 21:12:42,503 main.py:51] epoch 585, training loss: 13220.58, average training loss: 16303.12, base loss: 19829.36
[INFO 2017-06-27 21:12:43,480 main.py:51] epoch 586, training loss: 13369.43, average training loss: 16298.12, base loss: 19826.83
[INFO 2017-06-27 21:12:44,457 main.py:51] epoch 587, training loss: 13870.82, average training loss: 16293.99, base loss: 19825.83
[INFO 2017-06-27 21:12:45,435 main.py:51] epoch 588, training loss: 15226.88, average training loss: 16292.18, base loss: 19829.47
[INFO 2017-06-27 21:12:46,410 main.py:51] epoch 589, training loss: 14415.15, average training loss: 16289.00, base loss: 19827.81
[INFO 2017-06-27 21:12:47,392 main.py:51] epoch 590, training loss: 14512.97, average training loss: 16285.99, base loss: 19830.08
[INFO 2017-06-27 21:12:48,377 main.py:51] epoch 591, training loss: 15508.36, average training loss: 16284.68, base loss: 19832.41
[INFO 2017-06-27 21:12:49,355 main.py:51] epoch 592, training loss: 14812.94, average training loss: 16282.20, base loss: 19832.77
[INFO 2017-06-27 21:12:50,330 main.py:51] epoch 593, training loss: 13950.59, average training loss: 16278.27, base loss: 19831.14
[INFO 2017-06-27 21:12:51,308 main.py:51] epoch 594, training loss: 14997.18, average training loss: 16276.12, base loss: 19833.24
[INFO 2017-06-27 21:12:52,284 main.py:51] epoch 595, training loss: 14313.06, average training loss: 16272.82, base loss: 19832.86
[INFO 2017-06-27 21:12:53,263 main.py:51] epoch 596, training loss: 14241.38, average training loss: 16269.42, base loss: 19832.87
[INFO 2017-06-27 21:12:54,238 main.py:51] epoch 597, training loss: 15614.75, average training loss: 16268.33, base loss: 19834.44
[INFO 2017-06-27 21:12:55,219 main.py:51] epoch 598, training loss: 15474.48, average training loss: 16267.00, base loss: 19838.38
[INFO 2017-06-27 21:12:56,192 main.py:51] epoch 599, training loss: 13449.49, average training loss: 16262.31, base loss: 19836.16
[INFO 2017-06-27 21:12:56,192 main.py:53] epoch 599, testing
[INFO 2017-06-27 21:13:00,426 main.py:105] average testing loss: 15048.16, base loss: 20809.68
[INFO 2017-06-27 21:13:00,426 main.py:106] improve_loss: 5761.53, improve_percent: 0.28
[INFO 2017-06-27 21:13:00,427 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:13:00,440 main.py:76] current best improved percent: 0.28
[INFO 2017-06-27 21:13:01,416 main.py:51] epoch 600, training loss: 13980.77, average training loss: 16258.51, base loss: 19834.96
[INFO 2017-06-27 21:13:02,394 main.py:51] epoch 601, training loss: 12488.85, average training loss: 16252.25, base loss: 19828.54
[INFO 2017-06-27 21:13:03,373 main.py:51] epoch 602, training loss: 14431.91, average training loss: 16249.23, base loss: 19828.24
[INFO 2017-06-27 21:13:04,353 main.py:51] epoch 603, training loss: 15509.98, average training loss: 16248.01, base loss: 19831.50
[INFO 2017-06-27 21:13:05,326 main.py:51] epoch 604, training loss: 13552.27, average training loss: 16243.55, base loss: 19830.22
[INFO 2017-06-27 21:13:06,300 main.py:51] epoch 605, training loss: 15326.83, average training loss: 16242.04, base loss: 19831.70
[INFO 2017-06-27 21:13:07,276 main.py:51] epoch 606, training loss: 12690.17, average training loss: 16236.19, base loss: 19829.05
[INFO 2017-06-27 21:13:08,258 main.py:51] epoch 607, training loss: 14321.79, average training loss: 16233.04, base loss: 19830.07
[INFO 2017-06-27 21:13:09,231 main.py:51] epoch 608, training loss: 13498.25, average training loss: 16228.55, base loss: 19827.51
[INFO 2017-06-27 21:13:10,206 main.py:51] epoch 609, training loss: 15076.41, average training loss: 16226.66, base loss: 19828.78
[INFO 2017-06-27 21:13:11,181 main.py:51] epoch 610, training loss: 12734.49, average training loss: 16220.94, base loss: 19825.94
[INFO 2017-06-27 21:13:12,161 main.py:51] epoch 611, training loss: 15368.07, average training loss: 16219.55, base loss: 19828.91
[INFO 2017-06-27 21:13:13,141 main.py:51] epoch 612, training loss: 13922.43, average training loss: 16215.80, base loss: 19828.21
[INFO 2017-06-27 21:13:14,121 main.py:51] epoch 613, training loss: 14033.85, average training loss: 16212.25, base loss: 19826.48
[INFO 2017-06-27 21:13:15,093 main.py:51] epoch 614, training loss: 13274.76, average training loss: 16207.47, base loss: 19825.60
[INFO 2017-06-27 21:13:16,066 main.py:51] epoch 615, training loss: 15136.81, average training loss: 16205.73, base loss: 19828.38
[INFO 2017-06-27 21:13:17,040 main.py:51] epoch 616, training loss: 13225.01, average training loss: 16200.90, base loss: 19825.09
[INFO 2017-06-27 21:13:18,017 main.py:51] epoch 617, training loss: 15277.22, average training loss: 16199.41, base loss: 19826.09
[INFO 2017-06-27 21:13:18,991 main.py:51] epoch 618, training loss: 14324.22, average training loss: 16196.38, base loss: 19826.66
[INFO 2017-06-27 21:13:19,966 main.py:51] epoch 619, training loss: 14174.77, average training loss: 16193.12, base loss: 19825.03
[INFO 2017-06-27 21:13:20,942 main.py:51] epoch 620, training loss: 14219.58, average training loss: 16189.94, base loss: 19823.57
[INFO 2017-06-27 21:13:21,918 main.py:51] epoch 621, training loss: 15309.57, average training loss: 16188.52, base loss: 19826.45
[INFO 2017-06-27 21:13:22,893 main.py:51] epoch 622, training loss: 13255.03, average training loss: 16183.81, base loss: 19822.42
[INFO 2017-06-27 21:13:23,867 main.py:51] epoch 623, training loss: 14558.59, average training loss: 16181.21, base loss: 19823.09
[INFO 2017-06-27 21:13:24,843 main.py:51] epoch 624, training loss: 14214.94, average training loss: 16178.06, base loss: 19822.46
[INFO 2017-06-27 21:13:25,820 main.py:51] epoch 625, training loss: 13557.04, average training loss: 16173.88, base loss: 19821.18
[INFO 2017-06-27 21:13:26,797 main.py:51] epoch 626, training loss: 13311.78, average training loss: 16169.31, base loss: 19820.48
[INFO 2017-06-27 21:13:27,771 main.py:51] epoch 627, training loss: 14784.33, average training loss: 16167.11, base loss: 19821.60
[INFO 2017-06-27 21:13:28,748 main.py:51] epoch 628, training loss: 13925.16, average training loss: 16163.54, base loss: 19818.57
[INFO 2017-06-27 21:13:29,727 main.py:51] epoch 629, training loss: 14758.71, average training loss: 16161.31, base loss: 19817.64
[INFO 2017-06-27 21:13:30,703 main.py:51] epoch 630, training loss: 12757.60, average training loss: 16155.92, base loss: 19813.11
[INFO 2017-06-27 21:13:31,684 main.py:51] epoch 631, training loss: 13850.59, average training loss: 16152.27, base loss: 19812.13
[INFO 2017-06-27 21:13:32,662 main.py:51] epoch 632, training loss: 14636.90, average training loss: 16149.88, base loss: 19813.28
[INFO 2017-06-27 21:13:33,642 main.py:51] epoch 633, training loss: 15646.50, average training loss: 16149.08, base loss: 19816.86
[INFO 2017-06-27 21:13:34,619 main.py:51] epoch 634, training loss: 13788.08, average training loss: 16145.37, base loss: 19815.26
[INFO 2017-06-27 21:13:35,596 main.py:51] epoch 635, training loss: 14448.56, average training loss: 16142.70, base loss: 19816.19
[INFO 2017-06-27 21:13:36,577 main.py:51] epoch 636, training loss: 12126.67, average training loss: 16136.39, base loss: 19810.43
[INFO 2017-06-27 21:13:37,554 main.py:51] epoch 637, training loss: 15077.71, average training loss: 16134.73, base loss: 19811.48
[INFO 2017-06-27 21:13:38,534 main.py:51] epoch 638, training loss: 15944.57, average training loss: 16134.44, base loss: 19816.87
[INFO 2017-06-27 21:13:39,519 main.py:51] epoch 639, training loss: 15259.39, average training loss: 16133.07, base loss: 19820.54
[INFO 2017-06-27 21:13:40,495 main.py:51] epoch 640, training loss: 13196.71, average training loss: 16128.49, base loss: 19818.86
[INFO 2017-06-27 21:13:41,471 main.py:51] epoch 641, training loss: 13911.15, average training loss: 16125.03, base loss: 19817.75
[INFO 2017-06-27 21:13:42,449 main.py:51] epoch 642, training loss: 13561.02, average training loss: 16121.05, base loss: 19816.86
[INFO 2017-06-27 21:13:43,427 main.py:51] epoch 643, training loss: 13190.50, average training loss: 16116.50, base loss: 19812.73
[INFO 2017-06-27 21:13:44,403 main.py:51] epoch 644, training loss: 14910.47, average training loss: 16114.63, base loss: 19813.03
[INFO 2017-06-27 21:13:45,391 main.py:51] epoch 645, training loss: 13952.21, average training loss: 16111.28, base loss: 19813.03
[INFO 2017-06-27 21:13:46,371 main.py:51] epoch 646, training loss: 13259.24, average training loss: 16106.87, base loss: 19811.56
[INFO 2017-06-27 21:13:47,346 main.py:51] epoch 647, training loss: 11132.48, average training loss: 16099.19, base loss: 19805.19
[INFO 2017-06-27 21:13:48,321 main.py:51] epoch 648, training loss: 13371.00, average training loss: 16094.99, base loss: 19803.30
[INFO 2017-06-27 21:13:49,299 main.py:51] epoch 649, training loss: 12337.28, average training loss: 16089.21, base loss: 19796.41
[INFO 2017-06-27 21:13:50,277 main.py:51] epoch 650, training loss: 14033.24, average training loss: 16086.05, base loss: 19796.76
[INFO 2017-06-27 21:13:51,253 main.py:51] epoch 651, training loss: 15030.88, average training loss: 16084.43, base loss: 19796.71
[INFO 2017-06-27 21:13:52,238 main.py:51] epoch 652, training loss: 14223.16, average training loss: 16081.58, base loss: 19796.70
[INFO 2017-06-27 21:13:53,217 main.py:51] epoch 653, training loss: 12658.09, average training loss: 16076.35, base loss: 19793.34
[INFO 2017-06-27 21:13:54,190 main.py:51] epoch 654, training loss: 16532.56, average training loss: 16077.04, base loss: 19797.98
[INFO 2017-06-27 21:13:55,165 main.py:51] epoch 655, training loss: 14791.88, average training loss: 16075.08, base loss: 19799.96
[INFO 2017-06-27 21:13:56,144 main.py:51] epoch 656, training loss: 14448.98, average training loss: 16072.61, base loss: 19802.37
[INFO 2017-06-27 21:13:57,117 main.py:51] epoch 657, training loss: 13233.91, average training loss: 16068.30, base loss: 19800.89
[INFO 2017-06-27 21:13:58,089 main.py:51] epoch 658, training loss: 12582.98, average training loss: 16063.01, base loss: 19797.83
[INFO 2017-06-27 21:13:59,062 main.py:51] epoch 659, training loss: 13219.75, average training loss: 16058.70, base loss: 19795.36
[INFO 2017-06-27 21:14:00,040 main.py:51] epoch 660, training loss: 16871.09, average training loss: 16059.93, base loss: 19802.27
[INFO 2017-06-27 21:14:01,024 main.py:51] epoch 661, training loss: 12455.53, average training loss: 16054.48, base loss: 19797.79
[INFO 2017-06-27 21:14:02,003 main.py:51] epoch 662, training loss: 15527.98, average training loss: 16053.69, base loss: 19799.71
[INFO 2017-06-27 21:14:02,983 main.py:51] epoch 663, training loss: 12625.20, average training loss: 16048.53, base loss: 19797.01
[INFO 2017-06-27 21:14:03,967 main.py:51] epoch 664, training loss: 13671.21, average training loss: 16044.95, base loss: 19795.46
[INFO 2017-06-27 21:14:04,941 main.py:51] epoch 665, training loss: 11360.65, average training loss: 16037.92, base loss: 19787.40
[INFO 2017-06-27 21:14:05,919 main.py:51] epoch 666, training loss: 14976.51, average training loss: 16036.33, base loss: 19790.70
[INFO 2017-06-27 21:14:06,896 main.py:51] epoch 667, training loss: 16823.95, average training loss: 16037.51, base loss: 19796.08
[INFO 2017-06-27 21:14:07,870 main.py:51] epoch 668, training loss: 14715.76, average training loss: 16035.53, base loss: 19795.94
[INFO 2017-06-27 21:14:08,848 main.py:51] epoch 669, training loss: 13253.02, average training loss: 16031.38, base loss: 19792.43
[INFO 2017-06-27 21:14:09,826 main.py:51] epoch 670, training loss: 14854.98, average training loss: 16029.62, base loss: 19791.41
[INFO 2017-06-27 21:14:10,801 main.py:51] epoch 671, training loss: 12886.19, average training loss: 16024.95, base loss: 19789.15
[INFO 2017-06-27 21:14:11,777 main.py:51] epoch 672, training loss: 14813.50, average training loss: 16023.15, base loss: 19791.88
[INFO 2017-06-27 21:14:12,752 main.py:51] epoch 673, training loss: 12708.56, average training loss: 16018.23, base loss: 19788.46
[INFO 2017-06-27 21:14:13,730 main.py:51] epoch 674, training loss: 13511.58, average training loss: 16014.51, base loss: 19785.73
[INFO 2017-06-27 21:14:14,708 main.py:51] epoch 675, training loss: 14693.31, average training loss: 16012.56, base loss: 19787.08
[INFO 2017-06-27 21:14:15,690 main.py:51] epoch 676, training loss: 14954.99, average training loss: 16011.00, base loss: 19788.31
[INFO 2017-06-27 21:14:16,670 main.py:51] epoch 677, training loss: 12882.39, average training loss: 16006.38, base loss: 19785.54
[INFO 2017-06-27 21:14:17,649 main.py:51] epoch 678, training loss: 15113.08, average training loss: 16005.07, base loss: 19788.48
[INFO 2017-06-27 21:14:18,622 main.py:51] epoch 679, training loss: 15460.01, average training loss: 16004.27, base loss: 19791.99
[INFO 2017-06-27 21:14:19,594 main.py:51] epoch 680, training loss: 15461.44, average training loss: 16003.47, base loss: 19794.77
[INFO 2017-06-27 21:14:20,572 main.py:51] epoch 681, training loss: 15065.36, average training loss: 16002.09, base loss: 19793.76
[INFO 2017-06-27 21:14:21,549 main.py:51] epoch 682, training loss: 11358.23, average training loss: 15995.29, base loss: 19786.25
[INFO 2017-06-27 21:14:22,530 main.py:51] epoch 683, training loss: 14297.31, average training loss: 15992.81, base loss: 19787.18
[INFO 2017-06-27 21:14:23,508 main.py:51] epoch 684, training loss: 14532.34, average training loss: 15990.68, base loss: 19787.19
[INFO 2017-06-27 21:14:24,491 main.py:51] epoch 685, training loss: 13919.77, average training loss: 15987.66, base loss: 19787.54
[INFO 2017-06-27 21:14:25,476 main.py:51] epoch 686, training loss: 13730.00, average training loss: 15984.37, base loss: 19785.83
[INFO 2017-06-27 21:14:26,462 main.py:51] epoch 687, training loss: 12206.28, average training loss: 15978.88, base loss: 19782.39
[INFO 2017-06-27 21:14:27,441 main.py:51] epoch 688, training loss: 15487.36, average training loss: 15978.17, base loss: 19785.61
[INFO 2017-06-27 21:14:28,419 main.py:51] epoch 689, training loss: 12908.86, average training loss: 15973.72, base loss: 19781.51
[INFO 2017-06-27 21:14:29,412 main.py:51] epoch 690, training loss: 14158.09, average training loss: 15971.09, base loss: 19782.58
[INFO 2017-06-27 21:14:30,395 main.py:51] epoch 691, training loss: 13485.20, average training loss: 15967.50, base loss: 19781.43
[INFO 2017-06-27 21:14:31,380 main.py:51] epoch 692, training loss: 14393.13, average training loss: 15965.23, base loss: 19781.83
[INFO 2017-06-27 21:14:32,364 main.py:51] epoch 693, training loss: 14034.75, average training loss: 15962.45, base loss: 19782.55
[INFO 2017-06-27 21:14:33,340 main.py:51] epoch 694, training loss: 14374.15, average training loss: 15960.16, base loss: 19783.39
[INFO 2017-06-27 21:14:34,315 main.py:51] epoch 695, training loss: 13868.38, average training loss: 15957.16, base loss: 19781.73
[INFO 2017-06-27 21:14:35,288 main.py:51] epoch 696, training loss: 18250.23, average training loss: 15960.45, base loss: 19790.73
[INFO 2017-06-27 21:14:36,262 main.py:51] epoch 697, training loss: 12265.05, average training loss: 15955.15, base loss: 19786.26
[INFO 2017-06-27 21:14:37,237 main.py:51] epoch 698, training loss: 13314.03, average training loss: 15951.37, base loss: 19781.79
[INFO 2017-06-27 21:14:38,214 main.py:51] epoch 699, training loss: 13069.52, average training loss: 15947.26, base loss: 19779.40
[INFO 2017-06-27 21:14:38,215 main.py:53] epoch 699, testing
[INFO 2017-06-27 21:14:42,454 main.py:105] average testing loss: 13721.24, base loss: 19486.33
[INFO 2017-06-27 21:14:42,454 main.py:106] improve_loss: 5765.09, improve_percent: 0.30
[INFO 2017-06-27 21:14:42,454 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:14:42,467 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 21:14:43,446 main.py:51] epoch 700, training loss: 14147.95, average training loss: 15944.69, base loss: 19780.54
[INFO 2017-06-27 21:14:44,424 main.py:51] epoch 701, training loss: 14222.73, average training loss: 15942.24, base loss: 19781.85
[INFO 2017-06-27 21:14:45,406 main.py:51] epoch 702, training loss: 14484.67, average training loss: 15940.16, base loss: 19784.08
[INFO 2017-06-27 21:14:46,386 main.py:51] epoch 703, training loss: 12319.18, average training loss: 15935.02, base loss: 19780.37
[INFO 2017-06-27 21:14:47,363 main.py:51] epoch 704, training loss: 13868.75, average training loss: 15932.09, base loss: 19779.52
[INFO 2017-06-27 21:14:48,342 main.py:51] epoch 705, training loss: 13847.10, average training loss: 15929.14, base loss: 19779.53
[INFO 2017-06-27 21:14:49,321 main.py:51] epoch 706, training loss: 13534.57, average training loss: 15925.75, base loss: 19778.84
[INFO 2017-06-27 21:14:50,295 main.py:51] epoch 707, training loss: 11977.56, average training loss: 15920.17, base loss: 19775.18
[INFO 2017-06-27 21:14:51,277 main.py:51] epoch 708, training loss: 14980.44, average training loss: 15918.85, base loss: 19777.19
[INFO 2017-06-27 21:14:52,251 main.py:51] epoch 709, training loss: 14520.82, average training loss: 15916.88, base loss: 19775.86
[INFO 2017-06-27 21:14:53,226 main.py:51] epoch 710, training loss: 16972.29, average training loss: 15918.36, base loss: 19781.44
[INFO 2017-06-27 21:14:54,200 main.py:51] epoch 711, training loss: 13320.54, average training loss: 15914.71, base loss: 19780.50
[INFO 2017-06-27 21:14:55,172 main.py:51] epoch 712, training loss: 12052.82, average training loss: 15909.30, base loss: 19777.67
[INFO 2017-06-27 21:14:56,151 main.py:51] epoch 713, training loss: 13224.75, average training loss: 15905.54, base loss: 19774.26
[INFO 2017-06-27 21:14:57,128 main.py:51] epoch 714, training loss: 13249.87, average training loss: 15901.82, base loss: 19770.54
[INFO 2017-06-27 21:14:58,103 main.py:51] epoch 715, training loss: 15032.49, average training loss: 15900.61, base loss: 19773.61
[INFO 2017-06-27 21:14:59,092 main.py:51] epoch 716, training loss: 15164.03, average training loss: 15899.58, base loss: 19775.67
[INFO 2017-06-27 21:15:00,071 main.py:51] epoch 717, training loss: 16906.42, average training loss: 15900.99, base loss: 19782.26
[INFO 2017-06-27 21:15:01,056 main.py:51] epoch 718, training loss: 16634.22, average training loss: 15902.00, base loss: 19787.14
[INFO 2017-06-27 21:15:02,035 main.py:51] epoch 719, training loss: 13346.73, average training loss: 15898.46, base loss: 19786.08
[INFO 2017-06-27 21:15:03,011 main.py:51] epoch 720, training loss: 13952.86, average training loss: 15895.76, base loss: 19785.54
[INFO 2017-06-27 21:15:03,992 main.py:51] epoch 721, training loss: 13981.58, average training loss: 15893.11, base loss: 19785.11
[INFO 2017-06-27 21:15:04,966 main.py:51] epoch 722, training loss: 15893.34, average training loss: 15893.11, base loss: 19790.89
[INFO 2017-06-27 21:15:05,943 main.py:51] epoch 723, training loss: 13091.04, average training loss: 15889.24, base loss: 19789.94
[INFO 2017-06-27 21:15:06,916 main.py:51] epoch 724, training loss: 14512.71, average training loss: 15887.34, base loss: 19789.63
[INFO 2017-06-27 21:15:07,892 main.py:51] epoch 725, training loss: 14293.01, average training loss: 15885.14, base loss: 19792.98
[INFO 2017-06-27 21:15:08,877 main.py:51] epoch 726, training loss: 12401.16, average training loss: 15880.35, base loss: 19789.67
[INFO 2017-06-27 21:15:09,855 main.py:51] epoch 727, training loss: 13135.37, average training loss: 15876.58, base loss: 19786.77
[INFO 2017-06-27 21:15:10,832 main.py:51] epoch 728, training loss: 15545.92, average training loss: 15876.13, base loss: 19790.74
[INFO 2017-06-27 21:15:11,804 main.py:51] epoch 729, training loss: 13449.03, average training loss: 15872.80, base loss: 19788.70
[INFO 2017-06-27 21:15:12,788 main.py:51] epoch 730, training loss: 13753.78, average training loss: 15869.90, base loss: 19787.98
[INFO 2017-06-27 21:15:13,767 main.py:51] epoch 731, training loss: 13197.94, average training loss: 15866.25, base loss: 19786.15
[INFO 2017-06-27 21:15:14,747 main.py:51] epoch 732, training loss: 12059.90, average training loss: 15861.06, base loss: 19782.02
[INFO 2017-06-27 21:15:15,724 main.py:51] epoch 733, training loss: 13247.79, average training loss: 15857.50, base loss: 19780.25
[INFO 2017-06-27 21:15:16,701 main.py:51] epoch 734, training loss: 14400.45, average training loss: 15855.52, base loss: 19780.59
[INFO 2017-06-27 21:15:17,679 main.py:51] epoch 735, training loss: 13200.17, average training loss: 15851.91, base loss: 19778.39
[INFO 2017-06-27 21:15:18,660 main.py:51] epoch 736, training loss: 14584.58, average training loss: 15850.19, base loss: 19778.03
[INFO 2017-06-27 21:15:19,636 main.py:51] epoch 737, training loss: 14592.85, average training loss: 15848.48, base loss: 19778.56
[INFO 2017-06-27 21:15:20,616 main.py:51] epoch 738, training loss: 14606.33, average training loss: 15846.80, base loss: 19780.17
[INFO 2017-06-27 21:15:21,591 main.py:51] epoch 739, training loss: 13963.02, average training loss: 15844.26, base loss: 19780.54
[INFO 2017-06-27 21:15:22,567 main.py:51] epoch 740, training loss: 13385.31, average training loss: 15840.94, base loss: 19779.93
[INFO 2017-06-27 21:15:23,539 main.py:51] epoch 741, training loss: 14021.96, average training loss: 15838.49, base loss: 19779.12
[INFO 2017-06-27 21:15:24,511 main.py:51] epoch 742, training loss: 13437.49, average training loss: 15835.26, base loss: 19777.07
[INFO 2017-06-27 21:15:25,489 main.py:51] epoch 743, training loss: 13505.51, average training loss: 15832.13, base loss: 19776.05
[INFO 2017-06-27 21:15:26,470 main.py:51] epoch 744, training loss: 15016.73, average training loss: 15831.03, base loss: 19777.49
[INFO 2017-06-27 21:15:27,442 main.py:51] epoch 745, training loss: 12267.20, average training loss: 15826.25, base loss: 19775.85
[INFO 2017-06-27 21:15:28,422 main.py:51] epoch 746, training loss: 12912.25, average training loss: 15822.35, base loss: 19774.62
[INFO 2017-06-27 21:15:29,399 main.py:51] epoch 747, training loss: 13695.10, average training loss: 15819.51, base loss: 19774.59
[INFO 2017-06-27 21:15:30,376 main.py:51] epoch 748, training loss: 14681.47, average training loss: 15817.99, base loss: 19776.93
[INFO 2017-06-27 21:15:31,362 main.py:51] epoch 749, training loss: 12619.36, average training loss: 15813.72, base loss: 19774.71
[INFO 2017-06-27 21:15:32,339 main.py:51] epoch 750, training loss: 13183.12, average training loss: 15810.22, base loss: 19772.08
[INFO 2017-06-27 21:15:33,314 main.py:51] epoch 751, training loss: 13554.65, average training loss: 15807.22, base loss: 19770.23
[INFO 2017-06-27 21:15:34,293 main.py:51] epoch 752, training loss: 13211.66, average training loss: 15803.78, base loss: 19770.03
[INFO 2017-06-27 21:15:35,275 main.py:51] epoch 753, training loss: 14433.53, average training loss: 15801.96, base loss: 19770.38
[INFO 2017-06-27 21:15:36,252 main.py:51] epoch 754, training loss: 14536.15, average training loss: 15800.28, base loss: 19771.50
[INFO 2017-06-27 21:15:37,233 main.py:51] epoch 755, training loss: 16031.81, average training loss: 15800.59, base loss: 19774.95
[INFO 2017-06-27 21:15:38,213 main.py:51] epoch 756, training loss: 14862.59, average training loss: 15799.35, base loss: 19778.61
[INFO 2017-06-27 21:15:39,194 main.py:51] epoch 757, training loss: 14709.32, average training loss: 15797.91, base loss: 19780.06
[INFO 2017-06-27 21:15:40,171 main.py:51] epoch 758, training loss: 13618.00, average training loss: 15795.04, base loss: 19780.73
[INFO 2017-06-27 21:15:41,145 main.py:51] epoch 759, training loss: 13038.11, average training loss: 15791.41, base loss: 19777.93
[INFO 2017-06-27 21:15:42,123 main.py:51] epoch 760, training loss: 14293.05, average training loss: 15789.44, base loss: 19777.89
[INFO 2017-06-27 21:15:43,099 main.py:51] epoch 761, training loss: 15834.67, average training loss: 15789.50, base loss: 19780.07
[INFO 2017-06-27 21:15:44,083 main.py:51] epoch 762, training loss: 13380.94, average training loss: 15786.34, base loss: 19779.37
[INFO 2017-06-27 21:15:45,060 main.py:51] epoch 763, training loss: 11875.58, average training loss: 15781.23, base loss: 19773.52
[INFO 2017-06-27 21:15:46,040 main.py:51] epoch 764, training loss: 13524.06, average training loss: 15778.28, base loss: 19772.14
[INFO 2017-06-27 21:15:47,014 main.py:51] epoch 765, training loss: 16689.33, average training loss: 15779.46, base loss: 19776.35
[INFO 2017-06-27 21:15:47,989 main.py:51] epoch 766, training loss: 12098.19, average training loss: 15774.67, base loss: 19771.35
[INFO 2017-06-27 21:15:48,965 main.py:51] epoch 767, training loss: 13580.11, average training loss: 15771.81, base loss: 19770.53
[INFO 2017-06-27 21:15:49,944 main.py:51] epoch 768, training loss: 12697.21, average training loss: 15767.81, base loss: 19768.93
[INFO 2017-06-27 21:15:50,923 main.py:51] epoch 769, training loss: 14029.78, average training loss: 15765.55, base loss: 19768.49
[INFO 2017-06-27 21:15:51,900 main.py:51] epoch 770, training loss: 12260.08, average training loss: 15761.01, base loss: 19765.42
[INFO 2017-06-27 21:15:52,876 main.py:51] epoch 771, training loss: 16940.70, average training loss: 15762.53, base loss: 19770.21
[INFO 2017-06-27 21:15:53,853 main.py:51] epoch 772, training loss: 13697.78, average training loss: 15759.86, base loss: 19768.10
[INFO 2017-06-27 21:15:54,832 main.py:51] epoch 773, training loss: 14208.04, average training loss: 15757.86, base loss: 19768.12
[INFO 2017-06-27 21:15:55,809 main.py:51] epoch 774, training loss: 15267.80, average training loss: 15757.23, base loss: 19771.26
[INFO 2017-06-27 21:15:56,790 main.py:51] epoch 775, training loss: 12872.78, average training loss: 15753.51, base loss: 19769.63
[INFO 2017-06-27 21:15:57,767 main.py:51] epoch 776, training loss: 13887.33, average training loss: 15751.11, base loss: 19769.64
[INFO 2017-06-27 21:15:58,743 main.py:51] epoch 777, training loss: 14713.40, average training loss: 15749.77, base loss: 19771.34
[INFO 2017-06-27 21:15:59,719 main.py:51] epoch 778, training loss: 13959.64, average training loss: 15747.47, base loss: 19770.91
[INFO 2017-06-27 21:16:00,703 main.py:51] epoch 779, training loss: 14653.60, average training loss: 15746.07, base loss: 19773.33
[INFO 2017-06-27 21:16:01,680 main.py:51] epoch 780, training loss: 15276.50, average training loss: 15745.47, base loss: 19774.11
[INFO 2017-06-27 21:16:02,665 main.py:51] epoch 781, training loss: 13709.21, average training loss: 15742.87, base loss: 19772.72
[INFO 2017-06-27 21:16:03,645 main.py:51] epoch 782, training loss: 14848.27, average training loss: 15741.72, base loss: 19774.00
[INFO 2017-06-27 21:16:04,623 main.py:51] epoch 783, training loss: 13888.91, average training loss: 15739.36, base loss: 19772.15
[INFO 2017-06-27 21:16:05,599 main.py:51] epoch 784, training loss: 13152.51, average training loss: 15736.07, base loss: 19767.95
[INFO 2017-06-27 21:16:06,573 main.py:51] epoch 785, training loss: 17305.74, average training loss: 15738.06, base loss: 19774.85
[INFO 2017-06-27 21:16:07,549 main.py:51] epoch 786, training loss: 15531.66, average training loss: 15737.80, base loss: 19777.18
[INFO 2017-06-27 21:16:08,521 main.py:51] epoch 787, training loss: 14032.05, average training loss: 15735.64, base loss: 19777.06
[INFO 2017-06-27 21:16:09,497 main.py:51] epoch 788, training loss: 13439.97, average training loss: 15732.73, base loss: 19775.70
[INFO 2017-06-27 21:16:10,473 main.py:51] epoch 789, training loss: 14453.80, average training loss: 15731.11, base loss: 19776.99
[INFO 2017-06-27 21:16:11,450 main.py:51] epoch 790, training loss: 12724.04, average training loss: 15727.31, base loss: 19774.96
[INFO 2017-06-27 21:16:12,430 main.py:51] epoch 791, training loss: 15117.54, average training loss: 15726.54, base loss: 19777.58
[INFO 2017-06-27 21:16:13,408 main.py:51] epoch 792, training loss: 14444.06, average training loss: 15724.92, base loss: 19778.85
[INFO 2017-06-27 21:16:14,380 main.py:51] epoch 793, training loss: 14772.33, average training loss: 15723.72, base loss: 19781.93
[INFO 2017-06-27 21:16:15,356 main.py:51] epoch 794, training loss: 13585.52, average training loss: 15721.03, base loss: 19781.48
[INFO 2017-06-27 21:16:16,335 main.py:51] epoch 795, training loss: 15587.70, average training loss: 15720.86, base loss: 19785.98
[INFO 2017-06-27 21:16:17,308 main.py:51] epoch 796, training loss: 13153.75, average training loss: 15717.64, base loss: 19783.62
[INFO 2017-06-27 21:16:18,284 main.py:51] epoch 797, training loss: 11229.80, average training loss: 15712.02, base loss: 19776.96
[INFO 2017-06-27 21:16:19,263 main.py:51] epoch 798, training loss: 12729.58, average training loss: 15708.28, base loss: 19774.86
[INFO 2017-06-27 21:16:20,242 main.py:51] epoch 799, training loss: 14081.98, average training loss: 15706.25, base loss: 19774.58
[INFO 2017-06-27 21:16:20,242 main.py:53] epoch 799, testing
[INFO 2017-06-27 21:16:24,477 main.py:105] average testing loss: 14029.24, base loss: 19701.07
[INFO 2017-06-27 21:16:24,477 main.py:106] improve_loss: 5671.83, improve_percent: 0.29
[INFO 2017-06-27 21:16:24,478 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 21:16:25,451 main.py:51] epoch 800, training loss: 11705.84, average training loss: 15701.26, base loss: 19769.22
[INFO 2017-06-27 21:16:26,431 main.py:51] epoch 801, training loss: 15041.23, average training loss: 15700.43, base loss: 19771.44
[INFO 2017-06-27 21:16:27,415 main.py:51] epoch 802, training loss: 16480.39, average training loss: 15701.41, base loss: 19775.84
[INFO 2017-06-27 21:16:28,390 main.py:51] epoch 803, training loss: 12028.38, average training loss: 15696.84, base loss: 19771.73
[INFO 2017-06-27 21:16:29,373 main.py:51] epoch 804, training loss: 15601.04, average training loss: 15696.72, base loss: 19774.98
[INFO 2017-06-27 21:16:30,351 main.py:51] epoch 805, training loss: 14883.85, average training loss: 15695.71, base loss: 19776.10
[INFO 2017-06-27 21:16:31,326 main.py:51] epoch 806, training loss: 15948.82, average training loss: 15696.02, base loss: 19779.42
[INFO 2017-06-27 21:16:32,301 main.py:51] epoch 807, training loss: 13614.74, average training loss: 15693.45, base loss: 19778.94
[INFO 2017-06-27 21:16:33,289 main.py:51] epoch 808, training loss: 13592.70, average training loss: 15690.85, base loss: 19779.71
[INFO 2017-06-27 21:16:34,264 main.py:51] epoch 809, training loss: 14663.05, average training loss: 15689.58, base loss: 19778.90
[INFO 2017-06-27 21:16:35,242 main.py:51] epoch 810, training loss: 12644.29, average training loss: 15685.83, base loss: 19775.16
[INFO 2017-06-27 21:16:36,218 main.py:51] epoch 811, training loss: 13616.00, average training loss: 15683.28, base loss: 19775.12
[INFO 2017-06-27 21:16:37,198 main.py:51] epoch 812, training loss: 14481.83, average training loss: 15681.80, base loss: 19775.25
[INFO 2017-06-27 21:16:38,173 main.py:51] epoch 813, training loss: 14603.05, average training loss: 15680.47, base loss: 19777.65
[INFO 2017-06-27 21:16:39,147 main.py:51] epoch 814, training loss: 15329.14, average training loss: 15680.04, base loss: 19779.43
[INFO 2017-06-27 21:16:40,119 main.py:51] epoch 815, training loss: 14450.86, average training loss: 15678.54, base loss: 19780.99
[INFO 2017-06-27 21:16:41,097 main.py:51] epoch 816, training loss: 10769.03, average training loss: 15672.53, base loss: 19773.96
[INFO 2017-06-27 21:16:42,072 main.py:51] epoch 817, training loss: 14991.35, average training loss: 15671.70, base loss: 19775.37
[INFO 2017-06-27 21:16:43,049 main.py:51] epoch 818, training loss: 12929.25, average training loss: 15668.35, base loss: 19773.60
[INFO 2017-06-27 21:16:44,033 main.py:51] epoch 819, training loss: 13475.80, average training loss: 15665.67, base loss: 19774.23
[INFO 2017-06-27 21:16:45,011 main.py:51] epoch 820, training loss: 13064.40, average training loss: 15662.50, base loss: 19774.09
[INFO 2017-06-27 21:16:45,989 main.py:51] epoch 821, training loss: 13731.15, average training loss: 15660.16, base loss: 19773.35
[INFO 2017-06-27 21:16:46,971 main.py:51] epoch 822, training loss: 12659.18, average training loss: 15656.51, base loss: 19770.86
[INFO 2017-06-27 21:16:47,946 main.py:51] epoch 823, training loss: 14681.87, average training loss: 15655.33, base loss: 19771.17
[INFO 2017-06-27 21:16:48,924 main.py:51] epoch 824, training loss: 12589.94, average training loss: 15651.61, base loss: 19768.45
[INFO 2017-06-27 21:16:49,899 main.py:51] epoch 825, training loss: 14067.17, average training loss: 15649.69, base loss: 19769.30
[INFO 2017-06-27 21:16:50,871 main.py:51] epoch 826, training loss: 15142.65, average training loss: 15649.08, base loss: 19771.87
[INFO 2017-06-27 21:16:51,843 main.py:51] epoch 827, training loss: 12761.35, average training loss: 15645.59, base loss: 19771.07
[INFO 2017-06-27 21:16:52,821 main.py:51] epoch 828, training loss: 12160.64, average training loss: 15641.39, base loss: 19767.92
[INFO 2017-06-27 21:16:53,798 main.py:51] epoch 829, training loss: 14647.24, average training loss: 15640.19, base loss: 19770.53
[INFO 2017-06-27 21:16:54,778 main.py:51] epoch 830, training loss: 12740.98, average training loss: 15636.70, base loss: 19769.29
[INFO 2017-06-27 21:16:55,755 main.py:51] epoch 831, training loss: 13404.85, average training loss: 15634.02, base loss: 19768.60
[INFO 2017-06-27 21:16:56,732 main.py:51] epoch 832, training loss: 11686.00, average training loss: 15629.28, base loss: 19765.19
[INFO 2017-06-27 21:16:57,709 main.py:51] epoch 833, training loss: 15086.94, average training loss: 15628.63, base loss: 19768.00
[INFO 2017-06-27 21:16:58,685 main.py:51] epoch 834, training loss: 14540.56, average training loss: 15627.33, base loss: 19768.28
[INFO 2017-06-27 21:16:59,659 main.py:51] epoch 835, training loss: 14591.99, average training loss: 15626.09, base loss: 19769.36
[INFO 2017-06-27 21:17:00,634 main.py:51] epoch 836, training loss: 13740.93, average training loss: 15623.83, base loss: 19769.94
[INFO 2017-06-27 21:17:01,608 main.py:51] epoch 837, training loss: 13814.88, average training loss: 15621.68, base loss: 19768.89
[INFO 2017-06-27 21:17:02,584 main.py:51] epoch 838, training loss: 13195.38, average training loss: 15618.78, base loss: 19768.14
[INFO 2017-06-27 21:17:03,561 main.py:51] epoch 839, training loss: 13697.80, average training loss: 15616.50, base loss: 19768.02
[INFO 2017-06-27 21:17:04,538 main.py:51] epoch 840, training loss: 11747.10, average training loss: 15611.90, base loss: 19762.59
[INFO 2017-06-27 21:17:05,516 main.py:51] epoch 841, training loss: 12854.45, average training loss: 15608.62, base loss: 19760.72
[INFO 2017-06-27 21:17:06,500 main.py:51] epoch 842, training loss: 14839.16, average training loss: 15607.71, base loss: 19762.64
[INFO 2017-06-27 21:17:07,479 main.py:51] epoch 843, training loss: 14265.16, average training loss: 15606.12, base loss: 19763.48
[INFO 2017-06-27 21:17:08,455 main.py:51] epoch 844, training loss: 14513.88, average training loss: 15604.83, base loss: 19765.19
[INFO 2017-06-27 21:17:09,430 main.py:51] epoch 845, training loss: 15078.97, average training loss: 15604.20, base loss: 19768.97
[INFO 2017-06-27 21:17:10,408 main.py:51] epoch 846, training loss: 13374.06, average training loss: 15601.57, base loss: 19767.73
[INFO 2017-06-27 21:17:11,382 main.py:51] epoch 847, training loss: 14373.40, average training loss: 15600.12, base loss: 19768.15
[INFO 2017-06-27 21:17:12,357 main.py:51] epoch 848, training loss: 13642.21, average training loss: 15597.82, base loss: 19768.18
[INFO 2017-06-27 21:17:13,333 main.py:51] epoch 849, training loss: 12143.62, average training loss: 15593.75, base loss: 19765.41
[INFO 2017-06-27 21:17:14,310 main.py:51] epoch 850, training loss: 11307.91, average training loss: 15588.72, base loss: 19760.67
[INFO 2017-06-27 21:17:15,288 main.py:51] epoch 851, training loss: 14223.55, average training loss: 15587.11, base loss: 19762.91
[INFO 2017-06-27 21:17:16,260 main.py:51] epoch 852, training loss: 14289.69, average training loss: 15585.59, base loss: 19764.19
[INFO 2017-06-27 21:17:17,235 main.py:51] epoch 853, training loss: 12219.03, average training loss: 15581.65, base loss: 19761.58
[INFO 2017-06-27 21:17:18,209 main.py:51] epoch 854, training loss: 15045.35, average training loss: 15581.02, base loss: 19763.46
[INFO 2017-06-27 21:17:19,184 main.py:51] epoch 855, training loss: 14705.14, average training loss: 15580.00, base loss: 19765.27
[INFO 2017-06-27 21:17:20,164 main.py:51] epoch 856, training loss: 13226.97, average training loss: 15577.26, base loss: 19765.64
[INFO 2017-06-27 21:17:21,140 main.py:51] epoch 857, training loss: 13069.93, average training loss: 15574.33, base loss: 19764.51
[INFO 2017-06-27 21:17:22,117 main.py:51] epoch 858, training loss: 12434.36, average training loss: 15570.68, base loss: 19763.74
[INFO 2017-06-27 21:17:23,096 main.py:51] epoch 859, training loss: 14548.17, average training loss: 15569.49, base loss: 19765.09
[INFO 2017-06-27 21:17:24,072 main.py:51] epoch 860, training loss: 13149.33, average training loss: 15566.68, base loss: 19763.53
[INFO 2017-06-27 21:17:25,045 main.py:51] epoch 861, training loss: 15080.35, average training loss: 15566.11, base loss: 19766.80
[INFO 2017-06-27 21:17:26,022 main.py:51] epoch 862, training loss: 12126.43, average training loss: 15562.13, base loss: 19763.91
[INFO 2017-06-27 21:17:27,004 main.py:51] epoch 863, training loss: 14137.92, average training loss: 15560.48, base loss: 19765.71
[INFO 2017-06-27 21:17:27,978 main.py:51] epoch 864, training loss: 14052.81, average training loss: 15558.74, base loss: 19766.84
[INFO 2017-06-27 21:17:28,954 main.py:51] epoch 865, training loss: 12558.50, average training loss: 15555.27, base loss: 19764.26
[INFO 2017-06-27 21:17:29,927 main.py:51] epoch 866, training loss: 13951.68, average training loss: 15553.42, base loss: 19764.63
[INFO 2017-06-27 21:17:30,900 main.py:51] epoch 867, training loss: 15254.42, average training loss: 15553.08, base loss: 19767.45
[INFO 2017-06-27 21:17:31,885 main.py:51] epoch 868, training loss: 15331.37, average training loss: 15552.82, base loss: 19771.00
[INFO 2017-06-27 21:17:32,861 main.py:51] epoch 869, training loss: 13573.24, average training loss: 15550.55, base loss: 19771.17
[INFO 2017-06-27 21:17:33,838 main.py:51] epoch 870, training loss: 13052.24, average training loss: 15547.68, base loss: 19770.81
[INFO 2017-06-27 21:17:34,815 main.py:51] epoch 871, training loss: 14605.27, average training loss: 15546.60, base loss: 19772.70
[INFO 2017-06-27 21:17:35,791 main.py:51] epoch 872, training loss: 12624.25, average training loss: 15543.25, base loss: 19771.14
[INFO 2017-06-27 21:17:36,767 main.py:51] epoch 873, training loss: 14956.63, average training loss: 15542.58, base loss: 19772.72
[INFO 2017-06-27 21:17:37,748 main.py:51] epoch 874, training loss: 14205.74, average training loss: 15541.05, base loss: 19772.64
[INFO 2017-06-27 21:17:38,725 main.py:51] epoch 875, training loss: 13055.90, average training loss: 15538.21, base loss: 19772.27
[INFO 2017-06-27 21:17:39,703 main.py:51] epoch 876, training loss: 16702.04, average training loss: 15539.54, base loss: 19777.67
[INFO 2017-06-27 21:17:40,676 main.py:51] epoch 877, training loss: 14974.01, average training loss: 15538.90, base loss: 19780.09
[INFO 2017-06-27 21:17:41,663 main.py:51] epoch 878, training loss: 13144.24, average training loss: 15536.17, base loss: 19778.41
[INFO 2017-06-27 21:17:42,639 main.py:51] epoch 879, training loss: 12882.84, average training loss: 15533.16, base loss: 19774.52
[INFO 2017-06-27 21:17:43,613 main.py:51] epoch 880, training loss: 13342.72, average training loss: 15530.67, base loss: 19773.46
[INFO 2017-06-27 21:17:44,592 main.py:51] epoch 881, training loss: 14480.57, average training loss: 15529.48, base loss: 19775.30
[INFO 2017-06-27 21:17:45,566 main.py:51] epoch 882, training loss: 13226.83, average training loss: 15526.87, base loss: 19774.57
[INFO 2017-06-27 21:17:46,545 main.py:51] epoch 883, training loss: 14731.86, average training loss: 15525.97, base loss: 19776.46
[INFO 2017-06-27 21:17:47,521 main.py:51] epoch 884, training loss: 11794.91, average training loss: 15521.76, base loss: 19773.72
[INFO 2017-06-27 21:17:48,501 main.py:51] epoch 885, training loss: 14565.21, average training loss: 15520.68, base loss: 19777.21
[INFO 2017-06-27 21:17:49,481 main.py:51] epoch 886, training loss: 11747.02, average training loss: 15516.42, base loss: 19773.11
[INFO 2017-06-27 21:17:50,454 main.py:51] epoch 887, training loss: 12939.33, average training loss: 15513.52, base loss: 19771.14
[INFO 2017-06-27 21:17:51,444 main.py:51] epoch 888, training loss: 13661.02, average training loss: 15511.44, base loss: 19772.39
[INFO 2017-06-27 21:17:52,424 main.py:51] epoch 889, training loss: 13674.08, average training loss: 15509.37, base loss: 19772.45
[INFO 2017-06-27 21:17:53,400 main.py:51] epoch 890, training loss: 12907.05, average training loss: 15506.45, base loss: 19770.99
[INFO 2017-06-27 21:17:54,379 main.py:51] epoch 891, training loss: 13088.82, average training loss: 15503.74, base loss: 19769.84
[INFO 2017-06-27 21:17:55,355 main.py:51] epoch 892, training loss: 12642.38, average training loss: 15500.54, base loss: 19768.37
[INFO 2017-06-27 21:17:56,329 main.py:51] epoch 893, training loss: 12941.03, average training loss: 15497.68, base loss: 19766.20
[INFO 2017-06-27 21:17:57,308 main.py:51] epoch 894, training loss: 14419.81, average training loss: 15496.47, base loss: 19767.49
[INFO 2017-06-27 21:17:58,290 main.py:51] epoch 895, training loss: 13968.25, average training loss: 15494.77, base loss: 19768.80
[INFO 2017-06-27 21:17:59,269 main.py:51] epoch 896, training loss: 13615.28, average training loss: 15492.67, base loss: 19768.81
[INFO 2017-06-27 21:18:00,253 main.py:51] epoch 897, training loss: 15512.98, average training loss: 15492.69, base loss: 19771.70
[INFO 2017-06-27 21:18:01,235 main.py:51] epoch 898, training loss: 14332.86, average training loss: 15491.40, base loss: 19773.71
[INFO 2017-06-27 21:18:02,212 main.py:51] epoch 899, training loss: 15010.06, average training loss: 15490.87, base loss: 19776.02
[INFO 2017-06-27 21:18:02,212 main.py:53] epoch 899, testing
[INFO 2017-06-27 21:18:06,465 main.py:105] average testing loss: 13765.05, base loss: 19896.14
[INFO 2017-06-27 21:18:06,466 main.py:106] improve_loss: 6131.09, improve_percent: 0.31
[INFO 2017-06-27 21:18:06,466 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:18:06,479 main.py:76] current best improved percent: 0.31
[INFO 2017-06-27 21:18:07,455 main.py:51] epoch 900, training loss: 14464.71, average training loss: 15489.73, base loss: 19777.36
[INFO 2017-06-27 21:18:08,433 main.py:51] epoch 901, training loss: 15625.36, average training loss: 15489.88, base loss: 19780.11
[INFO 2017-06-27 21:18:09,414 main.py:51] epoch 902, training loss: 14536.80, average training loss: 15488.82, base loss: 19781.08
[INFO 2017-06-27 21:18:10,391 main.py:51] epoch 903, training loss: 14530.83, average training loss: 15487.76, base loss: 19782.02
[INFO 2017-06-27 21:18:11,370 main.py:51] epoch 904, training loss: 13732.68, average training loss: 15485.83, base loss: 19783.32
[INFO 2017-06-27 21:18:12,346 main.py:51] epoch 905, training loss: 15134.05, average training loss: 15485.44, base loss: 19785.83
[INFO 2017-06-27 21:18:13,324 main.py:51] epoch 906, training loss: 12373.88, average training loss: 15482.01, base loss: 19783.59
[INFO 2017-06-27 21:18:14,300 main.py:51] epoch 907, training loss: 12716.43, average training loss: 15478.96, base loss: 19782.19
[INFO 2017-06-27 21:18:15,275 main.py:51] epoch 908, training loss: 12088.92, average training loss: 15475.23, base loss: 19779.51
[INFO 2017-06-27 21:18:16,259 main.py:51] epoch 909, training loss: 15122.17, average training loss: 15474.84, base loss: 19782.88
[INFO 2017-06-27 21:18:17,242 main.py:51] epoch 910, training loss: 13247.32, average training loss: 15472.40, base loss: 19782.87
[INFO 2017-06-27 21:18:18,217 main.py:51] epoch 911, training loss: 11627.12, average training loss: 15468.18, base loss: 19779.05
[INFO 2017-06-27 21:18:19,198 main.py:51] epoch 912, training loss: 13770.32, average training loss: 15466.32, base loss: 19780.44
[INFO 2017-06-27 21:18:20,181 main.py:51] epoch 913, training loss: 13414.00, average training loss: 15464.08, base loss: 19780.16
[INFO 2017-06-27 21:18:21,160 main.py:51] epoch 914, training loss: 13599.80, average training loss: 15462.04, base loss: 19778.04
[INFO 2017-06-27 21:18:22,139 main.py:51] epoch 915, training loss: 14892.41, average training loss: 15461.42, base loss: 19778.73
[INFO 2017-06-27 21:18:23,118 main.py:51] epoch 916, training loss: 12075.01, average training loss: 15457.72, base loss: 19776.24
[INFO 2017-06-27 21:18:24,095 main.py:51] epoch 917, training loss: 14400.75, average training loss: 15456.57, base loss: 19777.35
[INFO 2017-06-27 21:18:25,070 main.py:51] epoch 918, training loss: 12353.02, average training loss: 15453.20, base loss: 19775.19
[INFO 2017-06-27 21:18:26,046 main.py:51] epoch 919, training loss: 11741.01, average training loss: 15449.16, base loss: 19772.15
[INFO 2017-06-27 21:18:27,018 main.py:51] epoch 920, training loss: 14504.29, average training loss: 15448.13, base loss: 19773.05
[INFO 2017-06-27 21:18:27,992 main.py:51] epoch 921, training loss: 15282.00, average training loss: 15447.95, base loss: 19776.45
[INFO 2017-06-27 21:18:28,969 main.py:51] epoch 922, training loss: 14369.53, average training loss: 15446.79, base loss: 19777.13
[INFO 2017-06-27 21:18:29,946 main.py:51] epoch 923, training loss: 13492.96, average training loss: 15444.67, base loss: 19777.35
[INFO 2017-06-27 21:18:30,925 main.py:51] epoch 924, training loss: 12206.84, average training loss: 15441.17, base loss: 19775.18
[INFO 2017-06-27 21:18:31,898 main.py:51] epoch 925, training loss: 13966.78, average training loss: 15439.58, base loss: 19775.80
[INFO 2017-06-27 21:18:32,878 main.py:51] epoch 926, training loss: 13869.38, average training loss: 15437.89, base loss: 19777.21
[INFO 2017-06-27 21:18:33,853 main.py:51] epoch 927, training loss: 14410.92, average training loss: 15436.78, base loss: 19779.53
[INFO 2017-06-27 21:18:34,832 main.py:51] epoch 928, training loss: 12078.69, average training loss: 15433.16, base loss: 19776.34
[INFO 2017-06-27 21:18:35,809 main.py:51] epoch 929, training loss: 13506.36, average training loss: 15431.09, base loss: 19776.35
[INFO 2017-06-27 21:18:36,786 main.py:51] epoch 930, training loss: 14544.57, average training loss: 15430.14, base loss: 19777.50
[INFO 2017-06-27 21:18:37,761 main.py:51] epoch 931, training loss: 13214.45, average training loss: 15427.76, base loss: 19775.56
[INFO 2017-06-27 21:18:38,741 main.py:51] epoch 932, training loss: 15301.62, average training loss: 15427.63, base loss: 19778.37
[INFO 2017-06-27 21:18:39,722 main.py:51] epoch 933, training loss: 13299.77, average training loss: 15425.35, base loss: 19776.49
[INFO 2017-06-27 21:18:40,701 main.py:51] epoch 934, training loss: 12253.01, average training loss: 15421.96, base loss: 19774.91
[INFO 2017-06-27 21:18:41,682 main.py:51] epoch 935, training loss: 14796.86, average training loss: 15421.29, base loss: 19778.90
[INFO 2017-06-27 21:18:42,658 main.py:51] epoch 936, training loss: 13089.59, average training loss: 15418.80, base loss: 19778.84
[INFO 2017-06-27 21:18:43,642 main.py:51] epoch 937, training loss: 12639.04, average training loss: 15415.84, base loss: 19777.25
[INFO 2017-06-27 21:18:44,617 main.py:51] epoch 938, training loss: 13971.59, average training loss: 15414.30, base loss: 19778.86
[INFO 2017-06-27 21:18:45,595 main.py:51] epoch 939, training loss: 14066.26, average training loss: 15412.86, base loss: 19780.38
[INFO 2017-06-27 21:18:46,570 main.py:51] epoch 940, training loss: 15507.69, average training loss: 15412.97, base loss: 19782.37
[INFO 2017-06-27 21:18:47,544 main.py:51] epoch 941, training loss: 14659.86, average training loss: 15412.17, base loss: 19784.40
[INFO 2017-06-27 21:18:48,531 main.py:51] epoch 942, training loss: 13748.33, average training loss: 15410.40, base loss: 19785.55
[INFO 2017-06-27 21:18:49,508 main.py:51] epoch 943, training loss: 12686.18, average training loss: 15407.52, base loss: 19783.51
[INFO 2017-06-27 21:18:50,487 main.py:51] epoch 944, training loss: 15521.44, average training loss: 15407.64, base loss: 19787.92
[INFO 2017-06-27 21:18:51,463 main.py:51] epoch 945, training loss: 12573.39, average training loss: 15404.64, base loss: 19786.51
[INFO 2017-06-27 21:18:52,438 main.py:51] epoch 946, training loss: 14713.61, average training loss: 15403.91, base loss: 19786.93
[INFO 2017-06-27 21:18:53,420 main.py:51] epoch 947, training loss: 12183.26, average training loss: 15400.51, base loss: 19785.16
[INFO 2017-06-27 21:18:54,396 main.py:51] epoch 948, training loss: 13191.21, average training loss: 15398.18, base loss: 19784.19
[INFO 2017-06-27 21:18:55,377 main.py:51] epoch 949, training loss: 12339.09, average training loss: 15394.96, base loss: 19781.38
[INFO 2017-06-27 21:18:56,355 main.py:51] epoch 950, training loss: 14350.84, average training loss: 15393.87, base loss: 19782.33
[INFO 2017-06-27 21:18:57,333 main.py:51] epoch 951, training loss: 13506.16, average training loss: 15391.88, base loss: 19782.33
[INFO 2017-06-27 21:18:58,315 main.py:51] epoch 952, training loss: 14610.70, average training loss: 15391.06, base loss: 19783.07
[INFO 2017-06-27 21:18:59,295 main.py:51] epoch 953, training loss: 12512.07, average training loss: 15388.05, base loss: 19781.16
[INFO 2017-06-27 21:19:00,276 main.py:51] epoch 954, training loss: 13678.99, average training loss: 15386.26, base loss: 19781.46
[INFO 2017-06-27 21:19:01,254 main.py:51] epoch 955, training loss: 12295.37, average training loss: 15383.02, base loss: 19778.74
[INFO 2017-06-27 21:19:02,230 main.py:51] epoch 956, training loss: 12820.40, average training loss: 15380.35, base loss: 19776.41
[INFO 2017-06-27 21:19:03,208 main.py:51] epoch 957, training loss: 14438.01, average training loss: 15379.36, base loss: 19776.84
[INFO 2017-06-27 21:19:04,184 main.py:51] epoch 958, training loss: 14250.65, average training loss: 15378.19, base loss: 19777.95
[INFO 2017-06-27 21:19:05,163 main.py:51] epoch 959, training loss: 14041.06, average training loss: 15376.79, base loss: 19778.57
[INFO 2017-06-27 21:19:06,141 main.py:51] epoch 960, training loss: 14367.34, average training loss: 15375.74, base loss: 19780.21
[INFO 2017-06-27 21:19:07,118 main.py:51] epoch 961, training loss: 12282.17, average training loss: 15372.53, base loss: 19776.89
[INFO 2017-06-27 21:19:08,093 main.py:51] epoch 962, training loss: 13109.16, average training loss: 15370.18, base loss: 19775.67
[INFO 2017-06-27 21:19:09,074 main.py:51] epoch 963, training loss: 12912.94, average training loss: 15367.63, base loss: 19774.57
[INFO 2017-06-27 21:19:10,047 main.py:51] epoch 964, training loss: 12920.40, average training loss: 15365.09, base loss: 19772.62
[INFO 2017-06-27 21:19:11,022 main.py:51] epoch 965, training loss: 12246.22, average training loss: 15361.86, base loss: 19768.95
[INFO 2017-06-27 21:19:11,999 main.py:51] epoch 966, training loss: 14545.26, average training loss: 15361.02, base loss: 19770.59
[INFO 2017-06-27 21:19:12,974 main.py:51] epoch 967, training loss: 11603.73, average training loss: 15357.14, base loss: 19766.80
[INFO 2017-06-27 21:19:13,954 main.py:51] epoch 968, training loss: 12902.65, average training loss: 15354.60, base loss: 19764.72
[INFO 2017-06-27 21:19:14,929 main.py:51] epoch 969, training loss: 11577.96, average training loss: 15350.71, base loss: 19761.25
[INFO 2017-06-27 21:19:15,906 main.py:51] epoch 970, training loss: 14930.48, average training loss: 15350.28, base loss: 19763.79
[INFO 2017-06-27 21:19:16,882 main.py:51] epoch 971, training loss: 14554.78, average training loss: 15349.46, base loss: 19765.36
[INFO 2017-06-27 21:19:17,858 main.py:51] epoch 972, training loss: 13127.93, average training loss: 15347.18, base loss: 19764.43
[INFO 2017-06-27 21:19:18,834 main.py:51] epoch 973, training loss: 14141.21, average training loss: 15345.94, base loss: 19765.49
[INFO 2017-06-27 21:19:19,810 main.py:51] epoch 974, training loss: 13412.04, average training loss: 15343.95, base loss: 19766.25
[INFO 2017-06-27 21:19:20,787 main.py:51] epoch 975, training loss: 13402.85, average training loss: 15341.96, base loss: 19766.09
[INFO 2017-06-27 21:19:21,767 main.py:51] epoch 976, training loss: 14966.89, average training loss: 15341.58, base loss: 19768.09
[INFO 2017-06-27 21:19:22,740 main.py:51] epoch 977, training loss: 13358.80, average training loss: 15339.55, base loss: 19767.09
[INFO 2017-06-27 21:19:23,718 main.py:51] epoch 978, training loss: 14083.21, average training loss: 15338.27, base loss: 19768.61
[INFO 2017-06-27 21:19:24,700 main.py:51] epoch 979, training loss: 12765.65, average training loss: 15335.65, base loss: 19768.68
[INFO 2017-06-27 21:19:25,677 main.py:51] epoch 980, training loss: 13718.17, average training loss: 15334.00, base loss: 19768.96
[INFO 2017-06-27 21:19:26,653 main.py:51] epoch 981, training loss: 12018.98, average training loss: 15330.62, base loss: 19766.54
[INFO 2017-06-27 21:19:27,632 main.py:51] epoch 982, training loss: 11702.55, average training loss: 15326.93, base loss: 19762.91
[INFO 2017-06-27 21:19:28,611 main.py:51] epoch 983, training loss: 14898.08, average training loss: 15326.49, base loss: 19766.11
[INFO 2017-06-27 21:19:29,590 main.py:51] epoch 984, training loss: 14516.62, average training loss: 15325.67, base loss: 19767.43
[INFO 2017-06-27 21:19:30,572 main.py:51] epoch 985, training loss: 12871.20, average training loss: 15323.18, base loss: 19765.96
[INFO 2017-06-27 21:19:31,550 main.py:51] epoch 986, training loss: 13375.26, average training loss: 15321.21, base loss: 19766.06
[INFO 2017-06-27 21:19:32,527 main.py:51] epoch 987, training loss: 13737.32, average training loss: 15319.61, base loss: 19766.63
[INFO 2017-06-27 21:19:33,501 main.py:51] epoch 988, training loss: 13043.99, average training loss: 15317.30, base loss: 19766.44
[INFO 2017-06-27 21:19:34,473 main.py:51] epoch 989, training loss: 12575.03, average training loss: 15314.53, base loss: 19765.04
[INFO 2017-06-27 21:19:35,446 main.py:51] epoch 990, training loss: 14033.33, average training loss: 15313.24, base loss: 19766.22
[INFO 2017-06-27 21:19:36,422 main.py:51] epoch 991, training loss: 13669.28, average training loss: 15311.58, base loss: 19766.69
[INFO 2017-06-27 21:19:37,396 main.py:51] epoch 992, training loss: 13424.04, average training loss: 15309.68, base loss: 19766.79
[INFO 2017-06-27 21:19:38,369 main.py:51] epoch 993, training loss: 13281.10, average training loss: 15307.64, base loss: 19766.30
[INFO 2017-06-27 21:19:39,341 main.py:51] epoch 994, training loss: 13741.72, average training loss: 15306.07, base loss: 19766.04
[INFO 2017-06-27 21:19:40,316 main.py:51] epoch 995, training loss: 13234.76, average training loss: 15303.99, base loss: 19764.39
[INFO 2017-06-27 21:19:41,296 main.py:51] epoch 996, training loss: 12126.06, average training loss: 15300.80, base loss: 19762.11
[INFO 2017-06-27 21:19:42,273 main.py:51] epoch 997, training loss: 12166.87, average training loss: 15297.66, base loss: 19759.49
[INFO 2017-06-27 21:19:43,250 main.py:51] epoch 998, training loss: 15945.10, average training loss: 15298.31, base loss: 19763.25
[INFO 2017-06-27 21:19:44,226 main.py:51] epoch 999, training loss: 13271.44, average training loss: 15296.28, base loss: 19763.65
[INFO 2017-06-27 21:19:44,226 main.py:53] epoch 999, testing
[INFO 2017-06-27 21:19:48,470 main.py:105] average testing loss: 13596.06, base loss: 19609.01
[INFO 2017-06-27 21:19:48,471 main.py:106] improve_loss: 6012.95, improve_percent: 0.31
[INFO 2017-06-27 21:19:48,471 main.py:76] current best improved percent: 0.31
[INFO 2017-06-27 21:19:49,451 main.py:51] epoch 1000, training loss: 14387.26, average training loss: 15268.91, base loss: 19764.33
[INFO 2017-06-27 21:19:50,428 main.py:51] epoch 1001, training loss: 14875.87, average training loss: 15247.96, base loss: 19772.67
[INFO 2017-06-27 21:19:51,403 main.py:51] epoch 1002, training loss: 12509.60, average training loss: 15224.70, base loss: 19768.73
[INFO 2017-06-27 21:19:52,384 main.py:51] epoch 1003, training loss: 15575.67, average training loss: 15206.96, base loss: 19768.70
[INFO 2017-06-27 21:19:53,361 main.py:51] epoch 1004, training loss: 14260.79, average training loss: 15194.11, base loss: 19772.06
[INFO 2017-06-27 21:19:54,334 main.py:51] epoch 1005, training loss: 12813.72, average training loss: 15177.32, base loss: 19764.76
[INFO 2017-06-27 21:19:55,310 main.py:51] epoch 1006, training loss: 13320.79, average training loss: 15164.69, base loss: 19764.62
[INFO 2017-06-27 21:19:56,284 main.py:51] epoch 1007, training loss: 14304.85, average training loss: 15154.31, base loss: 19765.31
[INFO 2017-06-27 21:19:57,261 main.py:51] epoch 1008, training loss: 14459.16, average training loss: 15147.02, base loss: 19770.67
[INFO 2017-06-27 21:19:58,238 main.py:51] epoch 1009, training loss: 13953.46, average training loss: 15136.81, base loss: 19770.89
[INFO 2017-06-27 21:19:59,215 main.py:51] epoch 1010, training loss: 15438.34, average training loss: 15130.22, base loss: 19774.80
[INFO 2017-06-27 21:20:00,188 main.py:51] epoch 1011, training loss: 13573.35, average training loss: 15120.27, base loss: 19775.09
[INFO 2017-06-27 21:20:01,168 main.py:51] epoch 1012, training loss: 14227.38, average training loss: 15111.94, base loss: 19776.57
[INFO 2017-06-27 21:20:02,144 main.py:51] epoch 1013, training loss: 13918.00, average training loss: 15101.50, base loss: 19775.64
[INFO 2017-06-27 21:20:03,121 main.py:51] epoch 1014, training loss: 12442.91, average training loss: 15093.63, base loss: 19775.83
[INFO 2017-06-27 21:20:04,097 main.py:51] epoch 1015, training loss: 12969.00, average training loss: 15084.04, base loss: 19773.07
[INFO 2017-06-27 21:20:05,071 main.py:51] epoch 1016, training loss: 13095.02, average training loss: 15074.61, base loss: 19770.42
[INFO 2017-06-27 21:20:06,041 main.py:51] epoch 1017, training loss: 13603.42, average training loss: 15071.26, base loss: 19776.04
[INFO 2017-06-27 21:20:07,014 main.py:51] epoch 1018, training loss: 13560.59, average training loss: 15066.36, base loss: 19777.92
[INFO 2017-06-27 21:20:07,995 main.py:51] epoch 1019, training loss: 12733.61, average training loss: 15059.70, base loss: 19778.08
[INFO 2017-06-27 21:20:08,968 main.py:51] epoch 1020, training loss: 12478.31, average training loss: 15048.90, base loss: 19773.05
[INFO 2017-06-27 21:20:09,944 main.py:51] epoch 1021, training loss: 12187.31, average training loss: 15041.76, base loss: 19771.34
[INFO 2017-06-27 21:20:10,920 main.py:51] epoch 1022, training loss: 12980.34, average training loss: 15038.39, base loss: 19775.87
[INFO 2017-06-27 21:20:11,900 main.py:51] epoch 1023, training loss: 14146.49, average training loss: 15036.02, base loss: 19780.68
[INFO 2017-06-27 21:20:12,886 main.py:51] epoch 1024, training loss: 12893.10, average training loss: 15029.19, base loss: 19778.66
[INFO 2017-06-27 21:20:13,862 main.py:51] epoch 1025, training loss: 15698.39, average training loss: 15027.25, base loss: 19784.98
[INFO 2017-06-27 21:20:14,838 main.py:51] epoch 1026, training loss: 12266.44, average training loss: 15021.07, base loss: 19783.06
[INFO 2017-06-27 21:20:15,814 main.py:51] epoch 1027, training loss: 13004.32, average training loss: 15011.75, base loss: 19778.33
[INFO 2017-06-27 21:20:16,787 main.py:51] epoch 1028, training loss: 14708.86, average training loss: 15008.69, base loss: 19783.28
[INFO 2017-06-27 21:20:17,764 main.py:51] epoch 1029, training loss: 13188.13, average training loss: 15004.93, base loss: 19785.59
[INFO 2017-06-27 21:20:18,740 main.py:51] epoch 1030, training loss: 13266.33, average training loss: 14997.47, base loss: 19782.88
[INFO 2017-06-27 21:20:19,719 main.py:51] epoch 1031, training loss: 13364.89, average training loss: 14992.61, base loss: 19782.79
[INFO 2017-06-27 21:20:20,698 main.py:51] epoch 1032, training loss: 14757.09, average training loss: 14987.48, base loss: 19783.10
[INFO 2017-06-27 21:20:21,673 main.py:51] epoch 1033, training loss: 11646.56, average training loss: 14981.46, base loss: 19779.75
[INFO 2017-06-27 21:20:22,650 main.py:51] epoch 1034, training loss: 11583.63, average training loss: 14974.27, base loss: 19776.29
[INFO 2017-06-27 21:20:23,624 main.py:51] epoch 1035, training loss: 13115.71, average training loss: 14964.36, base loss: 19770.27
[INFO 2017-06-27 21:20:24,602 main.py:51] epoch 1036, training loss: 14296.86, average training loss: 14959.44, base loss: 19771.45
[INFO 2017-06-27 21:20:25,578 main.py:51] epoch 1037, training loss: 13783.95, average training loss: 14952.18, base loss: 19768.17
[INFO 2017-06-27 21:20:26,556 main.py:51] epoch 1038, training loss: 14942.12, average training loss: 14947.14, base loss: 19768.49
[INFO 2017-06-27 21:20:27,537 main.py:51] epoch 1039, training loss: 13384.15, average training loss: 14942.46, base loss: 19768.67
[INFO 2017-06-27 21:20:28,511 main.py:51] epoch 1040, training loss: 11991.13, average training loss: 14936.57, base loss: 19767.69
[INFO 2017-06-27 21:20:29,489 main.py:51] epoch 1041, training loss: 12354.02, average training loss: 14928.72, base loss: 19762.40
[INFO 2017-06-27 21:20:30,465 main.py:51] epoch 1042, training loss: 13411.15, average training loss: 14923.32, base loss: 19762.36
[INFO 2017-06-27 21:20:31,442 main.py:51] epoch 1043, training loss: 13089.37, average training loss: 14917.72, base loss: 19761.73
[INFO 2017-06-27 21:20:32,424 main.py:51] epoch 1044, training loss: 14942.15, average training loss: 14915.61, base loss: 19766.53
[INFO 2017-06-27 21:20:33,406 main.py:51] epoch 1045, training loss: 14564.32, average training loss: 14912.62, base loss: 19769.51
[INFO 2017-06-27 21:20:34,389 main.py:51] epoch 1046, training loss: 13628.99, average training loss: 14908.48, base loss: 19770.37
[INFO 2017-06-27 21:20:35,368 main.py:51] epoch 1047, training loss: 14128.15, average training loss: 14906.43, base loss: 19774.47
[INFO 2017-06-27 21:20:36,349 main.py:51] epoch 1048, training loss: 15093.92, average training loss: 14902.42, base loss: 19776.43
[INFO 2017-06-27 21:20:37,330 main.py:51] epoch 1049, training loss: 12789.15, average training loss: 14894.89, base loss: 19772.91
[INFO 2017-06-27 21:20:38,304 main.py:51] epoch 1050, training loss: 13709.44, average training loss: 14886.44, base loss: 19769.04
[INFO 2017-06-27 21:20:39,280 main.py:51] epoch 1051, training loss: 13461.19, average training loss: 14882.70, base loss: 19769.68
[INFO 2017-06-27 21:20:40,258 main.py:51] epoch 1052, training loss: 12165.88, average training loss: 14876.61, base loss: 19766.19
[INFO 2017-06-27 21:20:41,236 main.py:51] epoch 1053, training loss: 12672.91, average training loss: 14867.78, base loss: 19761.10
[INFO 2017-06-27 21:20:42,216 main.py:51] epoch 1054, training loss: 14177.93, average training loss: 14864.43, base loss: 19764.10
[INFO 2017-06-27 21:20:43,195 main.py:51] epoch 1055, training loss: 13297.34, average training loss: 14858.71, base loss: 19762.89
[INFO 2017-06-27 21:20:44,173 main.py:51] epoch 1056, training loss: 11986.30, average training loss: 14852.72, base loss: 19761.52
[INFO 2017-06-27 21:20:45,149 main.py:51] epoch 1057, training loss: 14359.78, average training loss: 14844.91, base loss: 19759.09
[INFO 2017-06-27 21:20:46,134 main.py:51] epoch 1058, training loss: 14814.21, average training loss: 14842.76, base loss: 19762.65
[INFO 2017-06-27 21:20:47,114 main.py:51] epoch 1059, training loss: 17018.92, average training loss: 14842.16, base loss: 19768.04
[INFO 2017-06-27 21:20:48,094 main.py:51] epoch 1060, training loss: 14749.02, average training loss: 14838.84, base loss: 19770.17
[INFO 2017-06-27 21:20:49,074 main.py:51] epoch 1061, training loss: 15057.49, average training loss: 14837.92, base loss: 19776.34
[INFO 2017-06-27 21:20:50,052 main.py:51] epoch 1062, training loss: 12997.85, average training loss: 14830.17, base loss: 19773.23
[INFO 2017-06-27 21:20:51,034 main.py:51] epoch 1063, training loss: 12295.71, average training loss: 14824.62, base loss: 19771.91
[INFO 2017-06-27 21:20:52,015 main.py:51] epoch 1064, training loss: 13011.35, average training loss: 14819.53, base loss: 19769.93
[INFO 2017-06-27 21:20:52,992 main.py:51] epoch 1065, training loss: 14684.93, average training loss: 14816.75, base loss: 19772.72
[INFO 2017-06-27 21:20:53,972 main.py:51] epoch 1066, training loss: 16969.32, average training loss: 14813.75, base loss: 19776.37
[INFO 2017-06-27 21:20:54,951 main.py:51] epoch 1067, training loss: 13327.60, average training loss: 14810.53, base loss: 19777.90
[INFO 2017-06-27 21:20:55,926 main.py:51] epoch 1068, training loss: 13336.92, average training loss: 14807.39, base loss: 19779.83
[INFO 2017-06-27 21:20:56,902 main.py:51] epoch 1069, training loss: 13605.68, average training loss: 14802.26, base loss: 19779.54
[INFO 2017-06-27 21:20:57,879 main.py:51] epoch 1070, training loss: 12582.69, average training loss: 14798.37, base loss: 19780.84
[INFO 2017-06-27 21:20:58,856 main.py:51] epoch 1071, training loss: 13941.71, average training loss: 14793.98, base loss: 19781.33
[INFO 2017-06-27 21:20:59,833 main.py:51] epoch 1072, training loss: 11461.74, average training loss: 14789.95, base loss: 19782.32
[INFO 2017-06-27 21:21:00,811 main.py:51] epoch 1073, training loss: 15165.38, average training loss: 14787.90, base loss: 19786.49
[INFO 2017-06-27 21:21:01,790 main.py:51] epoch 1074, training loss: 14680.95, average training loss: 14786.08, base loss: 19789.92
[INFO 2017-06-27 21:21:02,767 main.py:51] epoch 1075, training loss: 12068.58, average training loss: 14777.00, base loss: 19783.55
[INFO 2017-06-27 21:21:03,751 main.py:51] epoch 1076, training loss: 14068.55, average training loss: 14773.05, base loss: 19786.22
[INFO 2017-06-27 21:21:04,730 main.py:51] epoch 1077, training loss: 12037.75, average training loss: 14764.38, base loss: 19779.10
[INFO 2017-06-27 21:21:05,707 main.py:51] epoch 1078, training loss: 12141.10, average training loss: 14758.79, base loss: 19776.60
[INFO 2017-06-27 21:21:06,686 main.py:51] epoch 1079, training loss: 11811.03, average training loss: 14751.93, base loss: 19772.51
[INFO 2017-06-27 21:21:07,660 main.py:51] epoch 1080, training loss: 13490.24, average training loss: 14748.95, base loss: 19773.28
[INFO 2017-06-27 21:21:08,636 main.py:51] epoch 1081, training loss: 14521.66, average training loss: 14745.58, base loss: 19774.49
[INFO 2017-06-27 21:21:09,613 main.py:51] epoch 1082, training loss: 12706.70, average training loss: 14742.14, base loss: 19776.06
[INFO 2017-06-27 21:21:10,589 main.py:51] epoch 1083, training loss: 13283.92, average training loss: 14735.83, base loss: 19774.77
[INFO 2017-06-27 21:21:11,570 main.py:51] epoch 1084, training loss: 12568.65, average training loss: 14729.61, base loss: 19772.04
[INFO 2017-06-27 21:21:12,544 main.py:51] epoch 1085, training loss: 12869.75, average training loss: 14721.63, base loss: 19767.95
[INFO 2017-06-27 21:21:13,525 main.py:51] epoch 1086, training loss: 13005.97, average training loss: 14717.18, base loss: 19768.05
[INFO 2017-06-27 21:21:14,504 main.py:51] epoch 1087, training loss: 13974.90, average training loss: 14712.95, base loss: 19767.18
[INFO 2017-06-27 21:21:15,481 main.py:51] epoch 1088, training loss: 11914.20, average training loss: 14703.74, base loss: 19760.87
[INFO 2017-06-27 21:21:16,458 main.py:51] epoch 1089, training loss: 12672.22, average training loss: 14700.45, base loss: 19762.11
[INFO 2017-06-27 21:21:17,438 main.py:51] epoch 1090, training loss: 13631.33, average training loss: 14697.27, base loss: 19762.82
[INFO 2017-06-27 21:21:18,424 main.py:51] epoch 1091, training loss: 14150.89, average training loss: 14697.69, base loss: 19769.57
[INFO 2017-06-27 21:21:19,400 main.py:51] epoch 1092, training loss: 13847.70, average training loss: 14694.42, base loss: 19772.35
[INFO 2017-06-27 21:21:20,378 main.py:51] epoch 1093, training loss: 13929.05, average training loss: 14689.41, base loss: 19771.28
[INFO 2017-06-27 21:21:21,358 main.py:51] epoch 1094, training loss: 12615.90, average training loss: 14684.42, base loss: 19770.17
[INFO 2017-06-27 21:21:22,337 main.py:51] epoch 1095, training loss: 11063.56, average training loss: 14678.90, base loss: 19768.03
[INFO 2017-06-27 21:21:23,317 main.py:51] epoch 1096, training loss: 13338.46, average training loss: 14674.23, base loss: 19766.89
[INFO 2017-06-27 21:21:24,304 main.py:51] epoch 1097, training loss: 12066.96, average training loss: 14668.92, base loss: 19764.39
[INFO 2017-06-27 21:21:25,293 main.py:51] epoch 1098, training loss: 12458.15, average training loss: 14663.77, base loss: 19763.39
[INFO 2017-06-27 21:21:26,269 main.py:51] epoch 1099, training loss: 12971.32, average training loss: 14659.32, base loss: 19762.94
[INFO 2017-06-27 21:21:26,269 main.py:53] epoch 1099, testing
[INFO 2017-06-27 21:21:30,526 main.py:105] average testing loss: 13374.63, base loss: 20004.04
[INFO 2017-06-27 21:21:30,526 main.py:106] improve_loss: 6629.41, improve_percent: 0.33
[INFO 2017-06-27 21:21:30,526 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:21:30,539 main.py:76] current best improved percent: 0.33
[INFO 2017-06-27 21:21:31,515 main.py:51] epoch 1100, training loss: 13472.45, average training loss: 14657.40, base loss: 19765.56
[INFO 2017-06-27 21:21:32,493 main.py:51] epoch 1101, training loss: 12088.41, average training loss: 14649.92, base loss: 19761.48
[INFO 2017-06-27 21:21:33,470 main.py:51] epoch 1102, training loss: 14651.73, average training loss: 14646.95, base loss: 19763.14
[INFO 2017-06-27 21:21:34,454 main.py:51] epoch 1103, training loss: 10342.21, average training loss: 14641.20, base loss: 19759.93
[INFO 2017-06-27 21:21:35,434 main.py:51] epoch 1104, training loss: 12923.48, average training loss: 14637.06, base loss: 19760.61
[INFO 2017-06-27 21:21:36,416 main.py:51] epoch 1105, training loss: 12913.02, average training loss: 14633.68, base loss: 19762.64
[INFO 2017-06-27 21:21:37,393 main.py:51] epoch 1106, training loss: 13504.48, average training loss: 14626.43, base loss: 19758.41
[INFO 2017-06-27 21:21:38,372 main.py:51] epoch 1107, training loss: 13262.68, average training loss: 14618.28, base loss: 19753.36
[INFO 2017-06-27 21:21:39,351 main.py:51] epoch 1108, training loss: 12150.61, average training loss: 14613.93, base loss: 19752.82
[INFO 2017-06-27 21:21:40,335 main.py:51] epoch 1109, training loss: 13544.11, average training loss: 14609.98, base loss: 19752.19
[INFO 2017-06-27 21:21:41,313 main.py:51] epoch 1110, training loss: 14647.90, average training loss: 14607.78, base loss: 19754.51
[INFO 2017-06-27 21:21:42,290 main.py:51] epoch 1111, training loss: 13171.62, average training loss: 14602.81, base loss: 19753.15
[INFO 2017-06-27 21:21:43,266 main.py:51] epoch 1112, training loss: 14559.49, average training loss: 14598.88, base loss: 19755.70
[INFO 2017-06-27 21:21:44,242 main.py:51] epoch 1113, training loss: 12753.05, average training loss: 14593.89, base loss: 19755.28
[INFO 2017-06-27 21:21:45,218 main.py:51] epoch 1114, training loss: 13583.08, average training loss: 14586.69, base loss: 19751.84
[INFO 2017-06-27 21:21:46,198 main.py:51] epoch 1115, training loss: 13135.09, average training loss: 14582.05, base loss: 19750.85
[INFO 2017-06-27 21:21:47,174 main.py:51] epoch 1116, training loss: 12944.83, average training loss: 14577.50, base loss: 19749.57
[INFO 2017-06-27 21:21:48,149 main.py:51] epoch 1117, training loss: 12717.85, average training loss: 14573.08, base loss: 19749.93
[INFO 2017-06-27 21:21:49,132 main.py:51] epoch 1118, training loss: 14870.16, average training loss: 14569.80, base loss: 19752.56
[INFO 2017-06-27 21:21:50,114 main.py:51] epoch 1119, training loss: 13119.15, average training loss: 14565.13, base loss: 19751.20
[INFO 2017-06-27 21:21:51,089 main.py:51] epoch 1120, training loss: 12165.88, average training loss: 14560.90, base loss: 19748.49
[INFO 2017-06-27 21:21:52,073 main.py:51] epoch 1121, training loss: 13364.47, average training loss: 14555.32, base loss: 19747.61
[INFO 2017-06-27 21:21:53,050 main.py:51] epoch 1122, training loss: 14151.09, average training loss: 14551.48, base loss: 19748.23
[INFO 2017-06-27 21:21:54,028 main.py:51] epoch 1123, training loss: 13640.45, average training loss: 14549.09, base loss: 19750.60
[INFO 2017-06-27 21:21:55,005 main.py:51] epoch 1124, training loss: 12395.59, average training loss: 14544.42, base loss: 19747.86
[INFO 2017-06-27 21:21:55,984 main.py:51] epoch 1125, training loss: 14115.99, average training loss: 14541.94, base loss: 19751.46
[INFO 2017-06-27 21:21:56,960 main.py:51] epoch 1126, training loss: 12893.43, average training loss: 14539.51, base loss: 19754.20
[INFO 2017-06-27 21:21:57,936 main.py:51] epoch 1127, training loss: 13030.63, average training loss: 14535.13, base loss: 19753.92
[INFO 2017-06-27 21:21:58,911 main.py:51] epoch 1128, training loss: 13872.38, average training loss: 14530.55, base loss: 19755.27
[INFO 2017-06-27 21:21:59,887 main.py:51] epoch 1129, training loss: 12281.92, average training loss: 14523.34, base loss: 19750.73
[INFO 2017-06-27 21:22:00,861 main.py:51] epoch 1130, training loss: 12695.19, average training loss: 14520.47, base loss: 19751.32
[INFO 2017-06-27 21:22:01,841 main.py:51] epoch 1131, training loss: 11565.44, average training loss: 14512.53, base loss: 19744.21
[INFO 2017-06-27 21:22:02,817 main.py:51] epoch 1132, training loss: 13085.78, average training loss: 14508.56, base loss: 19744.23
[INFO 2017-06-27 21:22:03,792 main.py:51] epoch 1133, training loss: 14994.70, average training loss: 14506.96, base loss: 19749.30
[INFO 2017-06-27 21:22:04,772 main.py:51] epoch 1134, training loss: 14936.53, average training loss: 14504.86, base loss: 19753.23
[INFO 2017-06-27 21:22:05,750 main.py:51] epoch 1135, training loss: 12997.73, average training loss: 14499.77, base loss: 19750.99
[INFO 2017-06-27 21:22:06,727 main.py:51] epoch 1136, training loss: 13751.89, average training loss: 14496.02, base loss: 19751.01
[INFO 2017-06-27 21:22:07,701 main.py:51] epoch 1137, training loss: 13083.54, average training loss: 14492.61, base loss: 19751.72
[INFO 2017-06-27 21:22:08,679 main.py:51] epoch 1138, training loss: 14598.70, average training loss: 14491.55, base loss: 19756.22
[INFO 2017-06-27 21:22:09,657 main.py:51] epoch 1139, training loss: 13589.18, average training loss: 14488.68, base loss: 19758.67
[INFO 2017-06-27 21:22:10,636 main.py:51] epoch 1140, training loss: 13224.57, average training loss: 14483.38, base loss: 19755.55
[INFO 2017-06-27 21:22:11,620 main.py:51] epoch 1141, training loss: 13902.21, average training loss: 14480.83, base loss: 19755.56
[INFO 2017-06-27 21:22:12,597 main.py:51] epoch 1142, training loss: 12050.94, average training loss: 14478.22, base loss: 19756.54
[INFO 2017-06-27 21:22:13,574 main.py:51] epoch 1143, training loss: 12994.80, average training loss: 14473.79, base loss: 19755.75
[INFO 2017-06-27 21:22:14,552 main.py:51] epoch 1144, training loss: 11473.73, average training loss: 14469.13, base loss: 19754.39
[INFO 2017-06-27 21:22:15,531 main.py:51] epoch 1145, training loss: 13060.64, average training loss: 14466.67, base loss: 19755.63
[INFO 2017-06-27 21:22:16,512 main.py:51] epoch 1146, training loss: 11833.11, average training loss: 14464.32, base loss: 19756.53
[INFO 2017-06-27 21:22:17,487 main.py:51] epoch 1147, training loss: 13613.74, average training loss: 14459.92, base loss: 19753.91
[INFO 2017-06-27 21:22:18,463 main.py:51] epoch 1148, training loss: 11577.29, average training loss: 14452.11, base loss: 19747.47
[INFO 2017-06-27 21:22:19,438 main.py:51] epoch 1149, training loss: 14534.79, average training loss: 14449.65, base loss: 19749.30
[INFO 2017-06-27 21:22:20,421 main.py:51] epoch 1150, training loss: 12372.72, average training loss: 14446.82, base loss: 19753.11
[INFO 2017-06-27 21:22:21,402 main.py:51] epoch 1151, training loss: 13470.97, average training loss: 14444.91, base loss: 19754.61
[INFO 2017-06-27 21:22:22,377 main.py:51] epoch 1152, training loss: 11502.62, average training loss: 14441.03, base loss: 19754.46
[INFO 2017-06-27 21:22:23,352 main.py:51] epoch 1153, training loss: 11896.62, average training loss: 14437.40, base loss: 19754.54
[INFO 2017-06-27 21:22:24,328 main.py:51] epoch 1154, training loss: 14125.15, average training loss: 14434.47, base loss: 19756.21
[INFO 2017-06-27 21:22:25,306 main.py:51] epoch 1155, training loss: 13342.19, average training loss: 14436.23, base loss: 19764.64
[INFO 2017-06-27 21:22:26,283 main.py:51] epoch 1156, training loss: 13503.17, average training loss: 14431.51, base loss: 19763.50
[INFO 2017-06-27 21:22:27,266 main.py:51] epoch 1157, training loss: 14158.05, average training loss: 14428.72, base loss: 19763.54
[INFO 2017-06-27 21:22:28,239 main.py:51] epoch 1158, training loss: 11845.80, average training loss: 14425.42, base loss: 19763.42
[INFO 2017-06-27 21:22:29,215 main.py:51] epoch 1159, training loss: 13205.44, average training loss: 14421.65, base loss: 19762.51
[INFO 2017-06-27 21:22:30,191 main.py:51] epoch 1160, training loss: 13474.31, average training loss: 14419.07, base loss: 19763.57
[INFO 2017-06-27 21:22:31,173 main.py:51] epoch 1161, training loss: 14249.60, average training loss: 14415.90, base loss: 19764.80
[INFO 2017-06-27 21:22:32,148 main.py:51] epoch 1162, training loss: 14092.04, average training loss: 14409.97, base loss: 19760.89
[INFO 2017-06-27 21:22:33,123 main.py:51] epoch 1163, training loss: 13243.25, average training loss: 14403.95, base loss: 19757.56
[INFO 2017-06-27 21:22:34,097 main.py:51] epoch 1164, training loss: 12564.59, average training loss: 14400.29, base loss: 19757.81
[INFO 2017-06-27 21:22:35,075 main.py:51] epoch 1165, training loss: 11243.42, average training loss: 14395.44, base loss: 19753.60
[INFO 2017-06-27 21:22:36,047 main.py:51] epoch 1166, training loss: 12903.42, average training loss: 14391.34, base loss: 19752.69
[INFO 2017-06-27 21:22:37,017 main.py:51] epoch 1167, training loss: 14118.00, average training loss: 14387.31, base loss: 19751.76
[INFO 2017-06-27 21:22:37,988 main.py:51] epoch 1168, training loss: 15392.94, average training loss: 14386.22, base loss: 19756.74
[INFO 2017-06-27 21:22:38,960 main.py:51] epoch 1169, training loss: 13421.82, average training loss: 14386.23, base loss: 19762.81
[INFO 2017-06-27 21:22:39,937 main.py:51] epoch 1170, training loss: 13265.62, average training loss: 14382.93, base loss: 19762.69
[INFO 2017-06-27 21:22:40,918 main.py:51] epoch 1171, training loss: 13481.50, average training loss: 14378.04, base loss: 19760.97
[INFO 2017-06-27 21:22:41,893 main.py:51] epoch 1172, training loss: 12850.71, average training loss: 14371.34, base loss: 19755.16
[INFO 2017-06-27 21:22:42,864 main.py:51] epoch 1173, training loss: 12100.19, average training loss: 14366.26, base loss: 19753.59
[INFO 2017-06-27 21:22:43,838 main.py:51] epoch 1174, training loss: 13798.56, average training loss: 14363.05, base loss: 19754.51
[INFO 2017-06-27 21:22:44,812 main.py:51] epoch 1175, training loss: 15389.55, average training loss: 14363.71, base loss: 19761.25
[INFO 2017-06-27 21:22:45,789 main.py:51] epoch 1176, training loss: 13570.67, average training loss: 14361.44, base loss: 19763.22
[INFO 2017-06-27 21:22:46,765 main.py:51] epoch 1177, training loss: 13544.96, average training loss: 14356.77, base loss: 19762.26
[INFO 2017-06-27 21:22:47,738 main.py:51] epoch 1178, training loss: 13117.82, average training loss: 14355.87, base loss: 19765.00
[INFO 2017-06-27 21:22:48,716 main.py:51] epoch 1179, training loss: 12493.69, average training loss: 14351.21, base loss: 19765.09
[INFO 2017-06-27 21:22:49,689 main.py:51] epoch 1180, training loss: 12715.42, average training loss: 14349.33, base loss: 19766.93
[INFO 2017-06-27 21:22:50,666 main.py:51] epoch 1181, training loss: 13169.81, average training loss: 14344.99, base loss: 19766.29
[INFO 2017-06-27 21:22:51,641 main.py:51] epoch 1182, training loss: 14268.22, average training loss: 14340.52, base loss: 19765.46
[INFO 2017-06-27 21:22:52,612 main.py:51] epoch 1183, training loss: 14300.89, average training loss: 14338.55, base loss: 19768.46
[INFO 2017-06-27 21:22:53,587 main.py:51] epoch 1184, training loss: 11206.35, average training loss: 14335.22, base loss: 19768.35
[INFO 2017-06-27 21:22:54,561 main.py:51] epoch 1185, training loss: 13864.95, average training loss: 14332.20, base loss: 19768.50
[INFO 2017-06-27 21:22:55,534 main.py:51] epoch 1186, training loss: 13170.34, average training loss: 14330.24, base loss: 19770.29
[INFO 2017-06-27 21:22:56,509 main.py:51] epoch 1187, training loss: 13160.16, average training loss: 14327.50, base loss: 19771.78
[INFO 2017-06-27 21:22:57,493 main.py:51] epoch 1188, training loss: 13163.00, average training loss: 14325.07, base loss: 19771.58
[INFO 2017-06-27 21:22:58,464 main.py:51] epoch 1189, training loss: 13717.43, average training loss: 14319.73, base loss: 19768.68
[INFO 2017-06-27 21:22:59,437 main.py:51] epoch 1190, training loss: 13466.79, average training loss: 14315.30, base loss: 19768.49
[INFO 2017-06-27 21:23:00,412 main.py:51] epoch 1191, training loss: 13352.52, average training loss: 14311.25, base loss: 19767.68
[INFO 2017-06-27 21:23:01,387 main.py:51] epoch 1192, training loss: 15042.56, average training loss: 14312.93, base loss: 19775.28
[INFO 2017-06-27 21:23:02,363 main.py:51] epoch 1193, training loss: 15098.97, average training loss: 14312.11, base loss: 19778.50
[INFO 2017-06-27 21:23:03,341 main.py:51] epoch 1194, training loss: 13507.88, average training loss: 14309.30, base loss: 19776.85
[INFO 2017-06-27 21:23:04,314 main.py:51] epoch 1195, training loss: 11853.19, average training loss: 14304.25, base loss: 19772.43
[INFO 2017-06-27 21:23:05,286 main.py:51] epoch 1196, training loss: 15132.57, average training loss: 14302.82, base loss: 19774.69
[INFO 2017-06-27 21:23:06,259 main.py:51] epoch 1197, training loss: 13166.48, average training loss: 14301.20, base loss: 19776.13
[INFO 2017-06-27 21:23:07,234 main.py:51] epoch 1198, training loss: 13552.62, average training loss: 14299.01, base loss: 19776.40
[INFO 2017-06-27 21:23:08,210 main.py:51] epoch 1199, training loss: 13809.96, average training loss: 14296.05, base loss: 19778.16
[INFO 2017-06-27 21:23:08,210 main.py:53] epoch 1199, testing
[INFO 2017-06-27 21:23:12,467 main.py:105] average testing loss: 12459.50, base loss: 18770.02
[INFO 2017-06-27 21:23:12,467 main.py:106] improve_loss: 6310.51, improve_percent: 0.34
[INFO 2017-06-27 21:23:12,468 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:23:12,481 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 21:23:13,457 main.py:51] epoch 1200, training loss: 11769.88, average training loss: 14291.05, base loss: 19773.27
[INFO 2017-06-27 21:23:14,431 main.py:51] epoch 1201, training loss: 12623.46, average training loss: 14289.64, base loss: 19775.65
[INFO 2017-06-27 21:23:15,405 main.py:51] epoch 1202, training loss: 14369.64, average training loss: 14285.60, base loss: 19775.56
[INFO 2017-06-27 21:23:16,387 main.py:51] epoch 1203, training loss: 15124.30, average training loss: 14282.82, base loss: 19775.59
[INFO 2017-06-27 21:23:17,361 main.py:51] epoch 1204, training loss: 11931.09, average training loss: 14279.38, base loss: 19775.10
[INFO 2017-06-27 21:23:18,334 main.py:51] epoch 1205, training loss: 14007.50, average training loss: 14277.49, base loss: 19777.52
[INFO 2017-06-27 21:23:19,310 main.py:51] epoch 1206, training loss: 11890.97, average training loss: 14272.42, base loss: 19773.83
[INFO 2017-06-27 21:23:20,283 main.py:51] epoch 1207, training loss: 13433.70, average training loss: 14271.69, base loss: 19776.90
[INFO 2017-06-27 21:23:21,259 main.py:51] epoch 1208, training loss: 13344.31, average training loss: 14268.17, base loss: 19775.78
[INFO 2017-06-27 21:23:22,232 main.py:51] epoch 1209, training loss: 12705.17, average training loss: 14264.52, base loss: 19775.75
[INFO 2017-06-27 21:23:23,206 main.py:51] epoch 1210, training loss: 12532.16, average training loss: 14258.92, base loss: 19771.90
[INFO 2017-06-27 21:23:24,189 main.py:51] epoch 1211, training loss: 11856.06, average training loss: 14254.72, base loss: 19771.17
[INFO 2017-06-27 21:23:25,169 main.py:51] epoch 1212, training loss: 13207.85, average training loss: 14250.25, base loss: 19768.49
[INFO 2017-06-27 21:23:26,149 main.py:51] epoch 1213, training loss: 12270.09, average training loss: 14245.35, base loss: 19766.12
[INFO 2017-06-27 21:23:27,128 main.py:51] epoch 1214, training loss: 12876.12, average training loss: 14241.26, base loss: 19764.32
[INFO 2017-06-27 21:23:28,104 main.py:51] epoch 1215, training loss: 14361.65, average training loss: 14239.82, base loss: 19767.30
[INFO 2017-06-27 21:23:29,076 main.py:51] epoch 1216, training loss: 12500.83, average training loss: 14236.91, base loss: 19768.27
[INFO 2017-06-27 21:23:30,053 main.py:51] epoch 1217, training loss: 12793.30, average training loss: 14231.52, base loss: 19764.45
[INFO 2017-06-27 21:23:31,031 main.py:51] epoch 1218, training loss: 11498.91, average training loss: 14226.89, base loss: 19760.72
[INFO 2017-06-27 21:23:32,007 main.py:51] epoch 1219, training loss: 12065.17, average training loss: 14223.36, base loss: 19760.42
[INFO 2017-06-27 21:23:32,985 main.py:51] epoch 1220, training loss: 12207.10, average training loss: 14221.97, base loss: 19761.09
[INFO 2017-06-27 21:23:33,966 main.py:51] epoch 1221, training loss: 12291.49, average training loss: 14217.70, base loss: 19759.30
[INFO 2017-06-27 21:23:34,957 main.py:51] epoch 1222, training loss: 12896.01, average training loss: 14216.44, base loss: 19762.50
[INFO 2017-06-27 21:23:35,939 main.py:51] epoch 1223, training loss: 13556.16, average training loss: 14213.87, base loss: 19763.62
[INFO 2017-06-27 21:23:37,137 main.py:51] epoch 1224, training loss: 14484.64, average training loss: 14212.35, base loss: 19766.56
[INFO 2017-06-27 21:23:38,268 main.py:51] epoch 1225, training loss: 12643.77, average training loss: 14209.36, base loss: 19764.69
[INFO 2017-06-27 21:23:39,394 main.py:51] epoch 1226, training loss: 13715.93, average training loss: 14208.20, base loss: 19767.08
[INFO 2017-06-27 21:23:40,424 main.py:51] epoch 1227, training loss: 13132.56, average training loss: 14206.68, base loss: 19768.48
[INFO 2017-06-27 21:23:41,400 main.py:51] epoch 1228, training loss: 12952.51, average training loss: 14202.98, base loss: 19769.71
[INFO 2017-06-27 21:23:42,377 main.py:51] epoch 1229, training loss: 11969.22, average training loss: 14198.50, base loss: 19765.86
[INFO 2017-06-27 21:23:43,391 main.py:51] epoch 1230, training loss: 12781.21, average training loss: 14195.57, base loss: 19767.25
[INFO 2017-06-27 21:23:44,386 main.py:51] epoch 1231, training loss: 12790.48, average training loss: 14193.29, base loss: 19768.79
[INFO 2017-06-27 21:23:45,368 main.py:51] epoch 1232, training loss: 12898.80, average training loss: 14189.05, base loss: 19766.85
[INFO 2017-06-27 21:23:46,339 main.py:51] epoch 1233, training loss: 10990.37, average training loss: 14184.73, base loss: 19763.84
[INFO 2017-06-27 21:23:47,312 main.py:51] epoch 1234, training loss: 13575.14, average training loss: 14180.43, base loss: 19764.13
[INFO 2017-06-27 21:23:48,283 main.py:51] epoch 1235, training loss: 13829.55, average training loss: 14178.01, base loss: 19764.27
[INFO 2017-06-27 21:23:49,298 main.py:51] epoch 1236, training loss: 13700.92, average training loss: 14174.13, base loss: 19762.93
[INFO 2017-06-27 21:23:50,290 main.py:51] epoch 1237, training loss: 12179.58, average training loss: 14170.04, base loss: 19761.73
[INFO 2017-06-27 21:23:51,271 main.py:51] epoch 1238, training loss: 13465.06, average training loss: 14167.78, base loss: 19763.04
[INFO 2017-06-27 21:23:52,380 main.py:51] epoch 1239, training loss: 13771.08, average training loss: 14168.16, base loss: 19768.24
[INFO 2017-06-27 21:23:53,375 main.py:51] epoch 1240, training loss: 11346.83, average training loss: 14164.79, base loss: 19765.66
[INFO 2017-06-27 21:23:54,349 main.py:51] epoch 1241, training loss: 14009.27, average training loss: 14160.56, base loss: 19762.40
[INFO 2017-06-27 21:23:55,322 main.py:51] epoch 1242, training loss: 12105.70, average training loss: 14156.33, base loss: 19760.81
[INFO 2017-06-27 21:23:56,294 main.py:51] epoch 1243, training loss: 13314.25, average training loss: 14152.51, base loss: 19758.26
[INFO 2017-06-27 21:23:57,268 main.py:51] epoch 1244, training loss: 12996.68, average training loss: 14150.54, base loss: 19759.02
[INFO 2017-06-27 21:23:58,243 main.py:51] epoch 1245, training loss: 13679.96, average training loss: 14148.93, base loss: 19760.11
[INFO 2017-06-27 21:23:59,216 main.py:51] epoch 1246, training loss: 12751.47, average training loss: 14146.38, base loss: 19760.64
[INFO 2017-06-27 21:24:00,190 main.py:51] epoch 1247, training loss: 12859.42, average training loss: 14142.49, base loss: 19759.11
[INFO 2017-06-27 21:24:01,165 main.py:51] epoch 1248, training loss: 13295.45, average training loss: 14139.63, base loss: 19759.62
[INFO 2017-06-27 21:24:02,139 main.py:51] epoch 1249, training loss: 13222.75, average training loss: 14137.12, base loss: 19758.97
[INFO 2017-06-27 21:24:03,116 main.py:51] epoch 1250, training loss: 13679.60, average training loss: 14135.39, base loss: 19760.50
[INFO 2017-06-27 21:24:04,239 main.py:51] epoch 1251, training loss: 11234.14, average training loss: 14132.51, base loss: 19759.84
[INFO 2017-06-27 21:24:05,340 main.py:51] epoch 1252, training loss: 11896.84, average training loss: 14127.96, base loss: 19755.39
[INFO 2017-06-27 21:24:06,342 main.py:51] epoch 1253, training loss: 13486.56, average training loss: 14125.36, base loss: 19755.60
[INFO 2017-06-27 21:24:07,478 main.py:51] epoch 1254, training loss: 14412.08, average training loss: 14124.83, base loss: 19759.26
[INFO 2017-06-27 21:24:08,472 main.py:51] epoch 1255, training loss: 13605.06, average training loss: 14120.56, base loss: 19757.51
[INFO 2017-06-27 21:24:09,447 main.py:51] epoch 1256, training loss: 12297.34, average training loss: 14118.22, base loss: 19757.86
[INFO 2017-06-27 21:24:10,585 main.py:51] epoch 1257, training loss: 10234.94, average training loss: 14113.23, base loss: 19753.34
[INFO 2017-06-27 21:24:11,631 main.py:51] epoch 1258, training loss: 11468.30, average training loss: 14111.46, base loss: 19753.77
[INFO 2017-06-27 21:24:12,649 main.py:51] epoch 1259, training loss: 12974.68, average training loss: 14110.32, base loss: 19757.18
[INFO 2017-06-27 21:24:13,754 main.py:51] epoch 1260, training loss: 12763.30, average training loss: 14106.79, base loss: 19755.30
[INFO 2017-06-27 21:24:14,771 main.py:51] epoch 1261, training loss: 12966.15, average training loss: 14102.92, base loss: 19753.45
[INFO 2017-06-27 21:24:15,750 main.py:51] epoch 1262, training loss: 12783.86, average training loss: 14098.47, base loss: 19751.95
[INFO 2017-06-27 21:24:16,722 main.py:51] epoch 1263, training loss: 12329.39, average training loss: 14094.12, base loss: 19748.84
[INFO 2017-06-27 21:24:17,700 main.py:51] epoch 1264, training loss: 14702.28, average training loss: 14091.93, base loss: 19751.61
[INFO 2017-06-27 21:24:18,841 main.py:51] epoch 1265, training loss: 12359.00, average training loss: 14089.61, base loss: 19752.18
[INFO 2017-06-27 21:24:19,832 main.py:51] epoch 1266, training loss: 14273.91, average training loss: 14087.52, base loss: 19754.51
[INFO 2017-06-27 21:24:20,968 main.py:51] epoch 1267, training loss: 12770.86, average training loss: 14083.62, base loss: 19753.38
[INFO 2017-06-27 21:24:21,964 main.py:51] epoch 1268, training loss: 11702.81, average training loss: 14079.84, base loss: 19752.40
[INFO 2017-06-27 21:24:23,041 main.py:51] epoch 1269, training loss: 12707.92, average training loss: 14077.91, base loss: 19754.06
[INFO 2017-06-27 21:24:24,017 main.py:51] epoch 1270, training loss: 12527.97, average training loss: 14070.42, base loss: 19747.93
[INFO 2017-06-27 21:24:25,047 main.py:51] epoch 1271, training loss: 11734.55, average training loss: 14068.68, base loss: 19748.67
[INFO 2017-06-27 21:24:26,059 main.py:51] epoch 1272, training loss: 12671.07, average training loss: 14065.19, base loss: 19748.38
[INFO 2017-06-27 21:24:27,162 main.py:51] epoch 1273, training loss: 12536.66, average training loss: 14059.87, base loss: 19744.37
[INFO 2017-06-27 21:24:28,174 main.py:51] epoch 1274, training loss: 14681.33, average training loss: 14060.40, base loss: 19750.14
[INFO 2017-06-27 21:24:29,259 main.py:51] epoch 1275, training loss: 15027.74, average training loss: 14058.16, base loss: 19751.23
[INFO 2017-06-27 21:24:30,265 main.py:51] epoch 1276, training loss: 13148.88, average training loss: 14057.29, base loss: 19753.66
[INFO 2017-06-27 21:24:31,304 main.py:51] epoch 1277, training loss: 14534.22, average training loss: 14054.91, base loss: 19754.32
[INFO 2017-06-27 21:24:32,334 main.py:51] epoch 1278, training loss: 12634.11, average training loss: 14051.47, base loss: 19753.66
[INFO 2017-06-27 21:24:33,308 main.py:51] epoch 1279, training loss: 12927.61, average training loss: 14050.34, base loss: 19756.50
[INFO 2017-06-27 21:24:34,350 main.py:51] epoch 1280, training loss: 12600.98, average training loss: 14048.76, base loss: 19757.67
[INFO 2017-06-27 21:24:35,386 main.py:51] epoch 1281, training loss: 12876.81, average training loss: 14047.26, base loss: 19760.15
[INFO 2017-06-27 21:24:36,412 main.py:51] epoch 1282, training loss: 12569.81, average training loss: 14045.16, base loss: 19760.26
[INFO 2017-06-27 21:24:37,430 main.py:51] epoch 1283, training loss: 12740.61, average training loss: 14042.20, base loss: 19758.58
[INFO 2017-06-27 21:24:38,424 main.py:51] epoch 1284, training loss: 12071.67, average training loss: 14036.01, base loss: 19752.82
[INFO 2017-06-27 21:24:39,534 main.py:51] epoch 1285, training loss: 14787.34, average training loss: 14035.22, base loss: 19755.87
[INFO 2017-06-27 21:24:40,626 main.py:51] epoch 1286, training loss: 12663.36, average training loss: 14031.90, base loss: 19756.60
[INFO 2017-06-27 21:24:41,653 main.py:51] epoch 1287, training loss: 14849.70, average training loss: 14030.96, base loss: 19759.96
[INFO 2017-06-27 21:24:42,628 main.py:51] epoch 1288, training loss: 14844.32, average training loss: 14030.80, base loss: 19762.69
[INFO 2017-06-27 21:24:43,598 main.py:51] epoch 1289, training loss: 13524.37, average training loss: 14029.31, base loss: 19764.57
[INFO 2017-06-27 21:24:44,614 main.py:51] epoch 1290, training loss: 13499.64, average training loss: 14026.73, base loss: 19765.42
[INFO 2017-06-27 21:24:45,604 main.py:51] epoch 1291, training loss: 13145.28, average training loss: 14026.40, base loss: 19767.58
[INFO 2017-06-27 21:24:46,679 main.py:51] epoch 1292, training loss: 11554.44, average training loss: 14021.54, base loss: 19764.34
[INFO 2017-06-27 21:24:47,804 main.py:51] epoch 1293, training loss: 13289.71, average training loss: 14016.09, base loss: 19758.92
[INFO 2017-06-27 21:24:48,954 main.py:51] epoch 1294, training loss: 13012.00, average training loss: 14014.39, base loss: 19760.47
[INFO 2017-06-27 21:24:49,988 main.py:51] epoch 1295, training loss: 11773.48, average training loss: 14009.73, base loss: 19757.66
[INFO 2017-06-27 21:24:51,074 main.py:51] epoch 1296, training loss: 14037.57, average training loss: 14007.40, base loss: 19757.84
[INFO 2017-06-27 21:24:52,154 main.py:51] epoch 1297, training loss: 13962.92, average training loss: 14003.46, base loss: 19755.95
[INFO 2017-06-27 21:24:53,169 main.py:51] epoch 1298, training loss: 12608.10, average training loss: 13999.17, base loss: 19755.26
[INFO 2017-06-27 21:24:54,264 main.py:51] epoch 1299, training loss: 13583.07, average training loss: 13998.32, base loss: 19759.83
[INFO 2017-06-27 21:24:54,264 main.py:53] epoch 1299, testing
[INFO 2017-06-27 21:24:58,733 main.py:105] average testing loss: 12650.34, base loss: 18751.26
[INFO 2017-06-27 21:24:58,734 main.py:106] improve_loss: 6100.92, improve_percent: 0.33
[INFO 2017-06-27 21:24:58,734 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 21:24:59,728 main.py:51] epoch 1300, training loss: 14499.91, average training loss: 13997.20, base loss: 19761.66
[INFO 2017-06-27 21:25:00,698 main.py:51] epoch 1301, training loss: 13758.63, average training loss: 13995.87, base loss: 19763.69
[INFO 2017-06-27 21:25:01,704 main.py:51] epoch 1302, training loss: 12348.66, average training loss: 13993.50, base loss: 19763.90
[INFO 2017-06-27 21:25:02,737 main.py:51] epoch 1303, training loss: 13814.64, average training loss: 13988.87, base loss: 19763.46
[INFO 2017-06-27 21:25:03,726 main.py:51] epoch 1304, training loss: 12499.56, average training loss: 13985.65, base loss: 19763.57
[INFO 2017-06-27 21:25:04,701 main.py:51] epoch 1305, training loss: 14657.93, average training loss: 13984.27, base loss: 19763.56
[INFO 2017-06-27 21:25:05,825 main.py:51] epoch 1306, training loss: 11839.19, average training loss: 13980.71, base loss: 19761.33
[INFO 2017-06-27 21:25:06,834 main.py:51] epoch 1307, training loss: 13283.73, average training loss: 13977.72, base loss: 19761.02
[INFO 2017-06-27 21:25:07,874 main.py:51] epoch 1308, training loss: 13396.67, average training loss: 13974.70, base loss: 19761.74
[INFO 2017-06-27 21:25:08,873 main.py:51] epoch 1309, training loss: 13488.04, average training loss: 13974.27, base loss: 19763.68
[INFO 2017-06-27 21:25:09,955 main.py:51] epoch 1310, training loss: 13271.09, average training loss: 13970.97, base loss: 19764.04
[INFO 2017-06-27 21:25:10,932 main.py:51] epoch 1311, training loss: 13830.81, average training loss: 13971.92, base loss: 19769.73
[INFO 2017-06-27 21:25:11,908 main.py:51] epoch 1312, training loss: 13201.31, average training loss: 13967.24, base loss: 19765.97
[INFO 2017-06-27 21:25:12,882 main.py:51] epoch 1313, training loss: 12770.65, average training loss: 13962.47, base loss: 19762.83
[INFO 2017-06-27 21:25:13,853 main.py:51] epoch 1314, training loss: 11905.55, average training loss: 13958.92, base loss: 19760.74
[INFO 2017-06-27 21:25:14,826 main.py:51] epoch 1315, training loss: 12744.28, average training loss: 13955.48, base loss: 19759.51
[INFO 2017-06-27 21:25:15,796 main.py:51] epoch 1316, training loss: 13221.19, average training loss: 13954.17, base loss: 19760.84
[INFO 2017-06-27 21:25:16,863 main.py:51] epoch 1317, training loss: 12430.42, average training loss: 13951.50, base loss: 19758.55
[INFO 2017-06-27 21:25:17,949 main.py:51] epoch 1318, training loss: 12888.96, average training loss: 13948.77, base loss: 19757.98
[INFO 2017-06-27 21:25:18,986 main.py:51] epoch 1319, training loss: 13483.02, average training loss: 13948.53, base loss: 19761.54
[INFO 2017-06-27 21:25:20,074 main.py:51] epoch 1320, training loss: 12863.08, average training loss: 13944.77, base loss: 19758.81
[INFO 2017-06-27 21:25:21,185 main.py:51] epoch 1321, training loss: 13976.46, average training loss: 13943.15, base loss: 19759.75
[INFO 2017-06-27 21:25:22,234 main.py:51] epoch 1322, training loss: 14770.29, average training loss: 13942.45, base loss: 19762.18
[INFO 2017-06-27 21:25:23,230 main.py:51] epoch 1323, training loss: 14114.64, average training loss: 13941.65, base loss: 19765.05
[INFO 2017-06-27 21:25:24,298 main.py:51] epoch 1324, training loss: 12264.36, average training loss: 13940.30, base loss: 19767.21
[INFO 2017-06-27 21:25:25,422 main.py:51] epoch 1325, training loss: 12275.70, average training loss: 13938.09, base loss: 19767.66
[INFO 2017-06-27 21:25:26,441 main.py:51] epoch 1326, training loss: 12245.92, average training loss: 13931.80, base loss: 19763.31
[INFO 2017-06-27 21:25:27,471 main.py:51] epoch 1327, training loss: 14489.74, average training loss: 13929.27, base loss: 19762.99
[INFO 2017-06-27 21:25:28,451 main.py:51] epoch 1328, training loss: 13248.03, average training loss: 13924.17, base loss: 19759.44
[INFO 2017-06-27 21:25:29,525 main.py:51] epoch 1329, training loss: 13679.80, average training loss: 13920.80, base loss: 19759.75
[INFO 2017-06-27 21:25:30,510 main.py:51] epoch 1330, training loss: 11763.96, average training loss: 13916.95, base loss: 19758.54
[INFO 2017-06-27 21:25:31,479 main.py:51] epoch 1331, training loss: 14973.64, average training loss: 13917.46, base loss: 19763.20
[INFO 2017-06-27 21:25:32,449 main.py:51] epoch 1332, training loss: 12673.48, average training loss: 13914.88, base loss: 19762.66
[INFO 2017-06-27 21:25:33,434 main.py:51] epoch 1333, training loss: 12291.90, average training loss: 13911.53, base loss: 19761.46
[INFO 2017-06-27 21:25:34,405 main.py:51] epoch 1334, training loss: 14295.35, average training loss: 13911.15, base loss: 19764.73
[INFO 2017-06-27 21:25:35,376 main.py:51] epoch 1335, training loss: 12489.70, average training loss: 13909.61, base loss: 19766.05
[INFO 2017-06-27 21:25:36,347 main.py:51] epoch 1336, training loss: 12584.39, average training loss: 13906.24, base loss: 19764.86
[INFO 2017-06-27 21:25:37,317 main.py:51] epoch 1337, training loss: 12416.35, average training loss: 13905.27, base loss: 19767.35
[INFO 2017-06-27 21:25:38,292 main.py:51] epoch 1338, training loss: 14501.19, average training loss: 13904.46, base loss: 19771.03
[INFO 2017-06-27 21:25:39,263 main.py:51] epoch 1339, training loss: 13711.33, average training loss: 13903.22, base loss: 19772.15
[INFO 2017-06-27 21:25:40,232 main.py:51] epoch 1340, training loss: 12110.53, average training loss: 13899.92, base loss: 19770.52
[INFO 2017-06-27 21:25:41,205 main.py:51] epoch 1341, training loss: 13089.74, average training loss: 13897.66, base loss: 19771.17
[INFO 2017-06-27 21:25:42,177 main.py:51] epoch 1342, training loss: 11966.69, average training loss: 13894.31, base loss: 19769.86
[INFO 2017-06-27 21:25:43,148 main.py:51] epoch 1343, training loss: 13408.41, average training loss: 13892.92, base loss: 19771.32
[INFO 2017-06-27 21:25:44,121 main.py:51] epoch 1344, training loss: 13151.57, average training loss: 13890.16, base loss: 19770.19
[INFO 2017-06-27 21:25:45,091 main.py:51] epoch 1345, training loss: 10859.03, average training loss: 13886.98, base loss: 19768.14
[INFO 2017-06-27 21:25:46,061 main.py:51] epoch 1346, training loss: 14430.29, average training loss: 13885.69, base loss: 19770.77
[INFO 2017-06-27 21:25:47,036 main.py:51] epoch 1347, training loss: 13385.32, average training loss: 13884.66, base loss: 19773.96
[INFO 2017-06-27 21:25:48,007 main.py:51] epoch 1348, training loss: 11932.14, average training loss: 13880.32, base loss: 19771.13
[INFO 2017-06-27 21:25:48,980 main.py:51] epoch 1349, training loss: 13046.03, average training loss: 13878.63, base loss: 19771.71
[INFO 2017-06-27 21:25:50,043 main.py:51] epoch 1350, training loss: 13330.55, average training loss: 13877.29, base loss: 19773.08
[INFO 2017-06-27 21:25:51,019 main.py:51] epoch 1351, training loss: 12849.96, average training loss: 13872.04, base loss: 19769.82
[INFO 2017-06-27 21:25:51,989 main.py:51] epoch 1352, training loss: 12876.60, average training loss: 13870.79, base loss: 19771.98
[INFO 2017-06-27 21:25:52,971 main.py:51] epoch 1353, training loss: 11869.80, average training loss: 13867.50, base loss: 19769.91
[INFO 2017-06-27 21:25:53,944 main.py:51] epoch 1354, training loss: 13505.85, average training loss: 13865.85, base loss: 19769.38
[INFO 2017-06-27 21:25:54,914 main.py:51] epoch 1355, training loss: 12220.93, average training loss: 13863.72, base loss: 19769.42
[INFO 2017-06-27 21:25:55,884 main.py:51] epoch 1356, training loss: 12072.88, average training loss: 13861.53, base loss: 19768.56
[INFO 2017-06-27 21:25:56,857 main.py:51] epoch 1357, training loss: 12723.88, average training loss: 13858.62, base loss: 19767.05
[INFO 2017-06-27 21:25:57,830 main.py:51] epoch 1358, training loss: 11531.90, average training loss: 13856.00, base loss: 19765.53
[INFO 2017-06-27 21:25:58,801 main.py:51] epoch 1359, training loss: 12734.40, average training loss: 13855.12, base loss: 19768.19
[INFO 2017-06-27 21:25:59,774 main.py:51] epoch 1360, training loss: 13034.00, average training loss: 13853.09, base loss: 19768.03
[INFO 2017-06-27 21:26:00,750 main.py:51] epoch 1361, training loss: 13248.92, average training loss: 13851.44, base loss: 19769.63
[INFO 2017-06-27 21:26:01,725 main.py:51] epoch 1362, training loss: 13876.38, average training loss: 13851.96, base loss: 19773.29
[INFO 2017-06-27 21:26:02,695 main.py:51] epoch 1363, training loss: 12912.39, average training loss: 13848.59, base loss: 19772.88
[INFO 2017-06-27 21:26:03,665 main.py:51] epoch 1364, training loss: 12318.88, average training loss: 13843.27, base loss: 19768.47
[INFO 2017-06-27 21:26:04,635 main.py:51] epoch 1365, training loss: 12379.12, average training loss: 13839.52, base loss: 19765.85
[INFO 2017-06-27 21:26:05,611 main.py:51] epoch 1366, training loss: 12082.57, average training loss: 13837.08, base loss: 19765.31
[INFO 2017-06-27 21:26:06,583 main.py:51] epoch 1367, training loss: 12616.79, average training loss: 13835.30, base loss: 19766.49
[INFO 2017-06-27 21:26:07,555 main.py:51] epoch 1368, training loss: 14845.26, average training loss: 13836.48, base loss: 19773.38
[INFO 2017-06-27 21:26:08,525 main.py:51] epoch 1369, training loss: 13133.67, average training loss: 13835.45, base loss: 19775.85
[INFO 2017-06-27 21:26:09,493 main.py:51] epoch 1370, training loss: 11977.83, average training loss: 13834.07, base loss: 19778.40
[INFO 2017-06-27 21:26:10,471 main.py:51] epoch 1371, training loss: 13397.86, average training loss: 13832.88, base loss: 19783.66
[INFO 2017-06-27 21:26:11,451 main.py:51] epoch 1372, training loss: 12082.88, average training loss: 13830.48, base loss: 19783.72
[INFO 2017-06-27 21:26:12,547 main.py:51] epoch 1373, training loss: 12479.33, average training loss: 13826.00, base loss: 19780.66
[INFO 2017-06-27 21:26:13,558 main.py:51] epoch 1374, training loss: 12827.25, average training loss: 13821.22, base loss: 19776.60
[INFO 2017-06-27 21:26:14,540 main.py:51] epoch 1375, training loss: 12933.32, average training loss: 13819.37, base loss: 19776.91
[INFO 2017-06-27 21:26:15,517 main.py:51] epoch 1376, training loss: 13608.11, average training loss: 13818.91, base loss: 19779.89
[INFO 2017-06-27 21:26:16,491 main.py:51] epoch 1377, training loss: 11539.70, average training loss: 13817.11, base loss: 19780.34
[INFO 2017-06-27 21:26:17,571 main.py:51] epoch 1378, training loss: 13453.66, average training loss: 13816.40, base loss: 19781.98
[INFO 2017-06-27 21:26:18,591 main.py:51] epoch 1379, training loss: 12698.88, average training loss: 13814.48, base loss: 19784.20
[INFO 2017-06-27 21:26:19,594 main.py:51] epoch 1380, training loss: 12189.10, average training loss: 13813.49, base loss: 19787.54
[INFO 2017-06-27 21:26:20,669 main.py:51] epoch 1381, training loss: 11710.17, average training loss: 13808.94, base loss: 19783.31
[INFO 2017-06-27 21:26:21,693 main.py:51] epoch 1382, training loss: 13843.25, average training loss: 13806.90, base loss: 19784.35
[INFO 2017-06-27 21:26:22,720 main.py:51] epoch 1383, training loss: 12529.26, average training loss: 13801.73, base loss: 19780.91
[INFO 2017-06-27 21:26:23,697 main.py:51] epoch 1384, training loss: 12527.38, average training loss: 13800.50, base loss: 19781.23
[INFO 2017-06-27 21:26:24,672 main.py:51] epoch 1385, training loss: 14795.73, average training loss: 13800.64, base loss: 19786.25
[INFO 2017-06-27 21:26:25,652 main.py:51] epoch 1386, training loss: 12743.95, average training loss: 13798.62, base loss: 19789.02
[INFO 2017-06-27 21:26:26,625 main.py:51] epoch 1387, training loss: 13062.91, average training loss: 13796.44, base loss: 19788.80
[INFO 2017-06-27 21:26:27,599 main.py:51] epoch 1388, training loss: 13444.08, average training loss: 13796.61, base loss: 19793.63
[INFO 2017-06-27 21:26:28,674 main.py:51] epoch 1389, training loss: 13414.35, average training loss: 13795.41, base loss: 19796.67
[INFO 2017-06-27 21:26:29,659 main.py:51] epoch 1390, training loss: 11795.15, average training loss: 13792.75, base loss: 19796.33
[INFO 2017-06-27 21:26:30,740 main.py:51] epoch 1391, training loss: 13412.44, average training loss: 13792.75, base loss: 19799.97
[INFO 2017-06-27 21:26:31,836 main.py:51] epoch 1392, training loss: 11743.47, average training loss: 13790.69, base loss: 19798.45
[INFO 2017-06-27 21:26:32,870 main.py:51] epoch 1393, training loss: 13217.17, average training loss: 13788.68, base loss: 19801.09
[INFO 2017-06-27 21:26:33,895 main.py:51] epoch 1394, training loss: 11705.47, average training loss: 13784.79, base loss: 19798.95
[INFO 2017-06-27 21:26:34,975 main.py:51] epoch 1395, training loss: 12724.95, average training loss: 13781.84, base loss: 19798.08
[INFO 2017-06-27 21:26:36,076 main.py:51] epoch 1396, training loss: 13710.81, average training loss: 13781.42, base loss: 19802.44
[INFO 2017-06-27 21:26:37,074 main.py:51] epoch 1397, training loss: 13476.26, average training loss: 13779.46, base loss: 19805.04
[INFO 2017-06-27 21:26:38,161 main.py:51] epoch 1398, training loss: 12741.20, average training loss: 13774.16, base loss: 19801.91
[INFO 2017-06-27 21:26:39,259 main.py:51] epoch 1399, training loss: 12525.70, average training loss: 13770.63, base loss: 19799.68
[INFO 2017-06-27 21:26:39,259 main.py:53] epoch 1399, testing
[INFO 2017-06-27 21:26:43,676 main.py:105] average testing loss: 12671.99, base loss: 19543.41
[INFO 2017-06-27 21:26:43,676 main.py:106] improve_loss: 6871.42, improve_percent: 0.35
[INFO 2017-06-27 21:26:43,677 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:26:43,690 main.py:76] current best improved percent: 0.35
[INFO 2017-06-27 21:26:44,781 main.py:51] epoch 1400, training loss: 11417.48, average training loss: 13767.04, base loss: 19798.77
[INFO 2017-06-27 21:26:45,794 main.py:51] epoch 1401, training loss: 11901.18, average training loss: 13762.28, base loss: 19794.06
[INFO 2017-06-27 21:26:46,826 main.py:51] epoch 1402, training loss: 13215.33, average training loss: 13760.87, base loss: 19795.91
[INFO 2017-06-27 21:26:47,885 main.py:51] epoch 1403, training loss: 13806.90, average training loss: 13758.80, base loss: 19795.88
[INFO 2017-06-27 21:26:48,901 main.py:51] epoch 1404, training loss: 12801.63, average training loss: 13756.99, base loss: 19797.17
[INFO 2017-06-27 21:26:49,926 main.py:51] epoch 1405, training loss: 11900.29, average training loss: 13753.71, base loss: 19795.09
[INFO 2017-06-27 21:26:50,939 main.py:51] epoch 1406, training loss: 13356.30, average training loss: 13754.28, base loss: 19799.34
[INFO 2017-06-27 21:26:52,019 main.py:51] epoch 1407, training loss: 10782.69, average training loss: 13748.84, base loss: 19792.82
[INFO 2017-06-27 21:26:53,117 main.py:51] epoch 1408, training loss: 12522.72, average training loss: 13744.21, base loss: 19788.33
[INFO 2017-06-27 21:26:54,190 main.py:51] epoch 1409, training loss: 11200.04, average training loss: 13738.84, base loss: 19783.04
[INFO 2017-06-27 21:26:55,182 main.py:51] epoch 1410, training loss: 12174.04, average training loss: 13735.69, base loss: 19782.06
[INFO 2017-06-27 21:26:56,259 main.py:51] epoch 1411, training loss: 12129.30, average training loss: 13734.29, base loss: 19783.08
[INFO 2017-06-27 21:26:57,236 main.py:51] epoch 1412, training loss: 13160.54, average training loss: 13732.97, base loss: 19784.21
[INFO 2017-06-27 21:26:58,258 main.py:51] epoch 1413, training loss: 11895.10, average training loss: 13730.67, base loss: 19782.94
[INFO 2017-06-27 21:26:59,251 main.py:51] epoch 1414, training loss: 12254.75, average training loss: 13729.00, base loss: 19782.67
[INFO 2017-06-27 21:27:00,231 main.py:51] epoch 1415, training loss: 12335.48, average training loss: 13728.42, base loss: 19783.78
[INFO 2017-06-27 21:27:01,204 main.py:51] epoch 1416, training loss: 12751.79, average training loss: 13725.51, base loss: 19783.43
[INFO 2017-06-27 21:27:02,183 main.py:51] epoch 1417, training loss: 11666.33, average training loss: 13721.06, base loss: 19780.32
[INFO 2017-06-27 21:27:03,153 main.py:51] epoch 1418, training loss: 12562.25, average training loss: 13719.55, base loss: 19782.37
[INFO 2017-06-27 21:27:04,124 main.py:51] epoch 1419, training loss: 11936.66, average training loss: 13717.87, base loss: 19781.45
[INFO 2017-06-27 21:27:05,095 main.py:51] epoch 1420, training loss: 12027.13, average training loss: 13715.09, base loss: 19781.24
[INFO 2017-06-27 21:27:06,065 main.py:51] epoch 1421, training loss: 11791.55, average training loss: 13710.93, base loss: 19779.20
[INFO 2017-06-27 21:27:07,036 main.py:51] epoch 1422, training loss: 12013.06, average training loss: 13707.02, base loss: 19777.01
[INFO 2017-06-27 21:27:08,015 main.py:51] epoch 1423, training loss: 12502.21, average training loss: 13704.85, base loss: 19777.63
[INFO 2017-06-27 21:27:08,989 main.py:51] epoch 1424, training loss: 12653.90, average training loss: 13701.47, base loss: 19776.33
[INFO 2017-06-27 21:27:09,961 main.py:51] epoch 1425, training loss: 11566.17, average training loss: 13698.77, base loss: 19776.03
[INFO 2017-06-27 21:27:10,932 main.py:51] epoch 1426, training loss: 11715.69, average training loss: 13695.11, base loss: 19773.91
[INFO 2017-06-27 21:27:11,903 main.py:51] epoch 1427, training loss: 12050.50, average training loss: 13691.92, base loss: 19773.03
[INFO 2017-06-27 21:27:12,875 main.py:51] epoch 1428, training loss: 11506.83, average training loss: 13689.32, base loss: 19772.59
[INFO 2017-06-27 21:27:13,847 main.py:51] epoch 1429, training loss: 11022.52, average training loss: 13685.03, base loss: 19767.91
[INFO 2017-06-27 21:27:14,820 main.py:51] epoch 1430, training loss: 11795.20, average training loss: 13683.33, base loss: 19767.58
[INFO 2017-06-27 21:27:15,793 main.py:51] epoch 1431, training loss: 10174.48, average training loss: 13678.87, base loss: 19763.37
[INFO 2017-06-27 21:27:16,763 main.py:51] epoch 1432, training loss: 13066.80, average training loss: 13677.55, base loss: 19766.00
[INFO 2017-06-27 21:27:17,736 main.py:51] epoch 1433, training loss: 10972.25, average training loss: 13673.63, base loss: 19763.52
[INFO 2017-06-27 21:27:18,708 main.py:51] epoch 1434, training loss: 13398.27, average training loss: 13671.97, base loss: 19763.72
[INFO 2017-06-27 21:27:19,680 main.py:51] epoch 1435, training loss: 12973.21, average training loss: 13670.61, base loss: 19765.58
[INFO 2017-06-27 21:27:20,651 main.py:51] epoch 1436, training loss: 11108.64, average training loss: 13667.05, base loss: 19764.51
[INFO 2017-06-27 21:27:21,621 main.py:51] epoch 1437, training loss: 12218.08, average training loss: 13662.54, base loss: 19761.35
[INFO 2017-06-27 21:27:22,597 main.py:51] epoch 1438, training loss: 13220.50, average training loss: 13662.07, base loss: 19764.56
[INFO 2017-06-27 21:27:23,568 main.py:51] epoch 1439, training loss: 12216.53, average training loss: 13657.57, base loss: 19760.38
[INFO 2017-06-27 21:27:24,543 main.py:51] epoch 1440, training loss: 13830.46, average training loss: 13654.78, base loss: 19761.32
[INFO 2017-06-27 21:27:25,513 main.py:51] epoch 1441, training loss: 12421.99, average training loss: 13654.20, base loss: 19763.55
[INFO 2017-06-27 21:27:26,488 main.py:51] epoch 1442, training loss: 11925.49, average training loss: 13649.88, base loss: 19759.82
[INFO 2017-06-27 21:27:27,461 main.py:51] epoch 1443, training loss: 13360.59, average training loss: 13646.60, base loss: 19758.26
[INFO 2017-06-27 21:27:28,485 main.py:51] epoch 1444, training loss: 11975.55, average training loss: 13644.11, base loss: 19758.23
[INFO 2017-06-27 21:27:29,478 main.py:51] epoch 1445, training loss: 12044.40, average training loss: 13641.15, base loss: 19756.24
[INFO 2017-06-27 21:27:30,537 main.py:51] epoch 1446, training loss: 12104.19, average training loss: 13641.33, base loss: 19759.98
[INFO 2017-06-27 21:27:31,519 main.py:51] epoch 1447, training loss: 13462.95, average training loss: 13639.70, base loss: 19761.29
[INFO 2017-06-27 21:27:32,642 main.py:51] epoch 1448, training loss: 13053.08, average training loss: 13636.26, base loss: 19760.72
[INFO 2017-06-27 21:27:33,621 main.py:51] epoch 1449, training loss: 13108.20, average training loss: 13634.20, base loss: 19761.21
[INFO 2017-06-27 21:27:34,595 main.py:51] epoch 1450, training loss: 12770.78, average training loss: 13633.18, base loss: 19762.49
[INFO 2017-06-27 21:27:35,572 main.py:51] epoch 1451, training loss: 14087.60, average training loss: 13634.70, base loss: 19768.63
[INFO 2017-06-27 21:27:36,544 main.py:51] epoch 1452, training loss: 12485.20, average training loss: 13631.07, base loss: 19766.51
[INFO 2017-06-27 21:27:37,515 main.py:51] epoch 1453, training loss: 13054.97, average training loss: 13628.52, base loss: 19767.23
[INFO 2017-06-27 21:27:38,486 main.py:51] epoch 1454, training loss: 13373.58, average training loss: 13628.20, base loss: 19768.62
[INFO 2017-06-27 21:27:39,456 main.py:51] epoch 1455, training loss: 13358.81, average training loss: 13625.28, base loss: 19769.11
[INFO 2017-06-27 21:27:40,430 main.py:51] epoch 1456, training loss: 11568.93, average training loss: 13623.89, base loss: 19769.54
[INFO 2017-06-27 21:27:41,404 main.py:51] epoch 1457, training loss: 12492.90, average training loss: 13622.83, base loss: 19771.05
[INFO 2017-06-27 21:27:42,383 main.py:51] epoch 1458, training loss: 11593.12, average training loss: 13619.26, base loss: 19767.39
[INFO 2017-06-27 21:27:43,356 main.py:51] epoch 1459, training loss: 12063.39, average training loss: 13615.28, base loss: 19764.61
[INFO 2017-06-27 21:27:44,326 main.py:51] epoch 1460, training loss: 12708.08, average training loss: 13612.52, base loss: 19764.47
[INFO 2017-06-27 21:27:45,300 main.py:51] epoch 1461, training loss: 11613.66, average training loss: 13608.17, base loss: 19762.91
[INFO 2017-06-27 21:27:46,271 main.py:51] epoch 1462, training loss: 11456.77, average training loss: 13604.80, base loss: 19760.58
[INFO 2017-06-27 21:27:47,250 main.py:51] epoch 1463, training loss: 13478.97, average training loss: 13602.40, base loss: 19760.49
[INFO 2017-06-27 21:27:48,220 main.py:51] epoch 1464, training loss: 11292.22, average training loss: 13597.94, base loss: 19755.63
[INFO 2017-06-27 21:27:49,192 main.py:51] epoch 1465, training loss: 11520.94, average training loss: 13595.06, base loss: 19754.71
[INFO 2017-06-27 21:27:50,165 main.py:51] epoch 1466, training loss: 10996.44, average training loss: 13591.80, base loss: 19752.10
[INFO 2017-06-27 21:27:51,137 main.py:51] epoch 1467, training loss: 11756.54, average training loss: 13589.06, base loss: 19751.75
[INFO 2017-06-27 21:27:52,109 main.py:51] epoch 1468, training loss: 12532.90, average training loss: 13584.21, base loss: 19747.46
[INFO 2017-06-27 21:27:53,081 main.py:51] epoch 1469, training loss: 11647.32, average training loss: 13578.32, base loss: 19741.50
[INFO 2017-06-27 21:27:54,054 main.py:51] epoch 1470, training loss: 12750.30, average training loss: 13575.94, base loss: 19741.09
[INFO 2017-06-27 21:27:55,028 main.py:51] epoch 1471, training loss: 13128.88, average training loss: 13573.78, base loss: 19740.59
[INFO 2017-06-27 21:27:56,000 main.py:51] epoch 1472, training loss: 15460.27, average training loss: 13576.02, base loss: 19747.66
[INFO 2017-06-27 21:27:56,973 main.py:51] epoch 1473, training loss: 12957.72, average training loss: 13574.82, base loss: 19749.14
[INFO 2017-06-27 21:27:57,945 main.py:51] epoch 1474, training loss: 13664.15, average training loss: 13572.94, base loss: 19750.79
[INFO 2017-06-27 21:27:58,917 main.py:51] epoch 1475, training loss: 12729.60, average training loss: 13569.51, base loss: 19748.55
[INFO 2017-06-27 21:27:59,890 main.py:51] epoch 1476, training loss: 12707.87, average training loss: 13565.55, base loss: 19746.25
[INFO 2017-06-27 21:28:00,865 main.py:51] epoch 1477, training loss: 11848.20, average training loss: 13563.56, base loss: 19748.21
[INFO 2017-06-27 21:28:01,840 main.py:51] epoch 1478, training loss: 11963.49, average training loss: 13561.96, base loss: 19748.34
[INFO 2017-06-27 21:28:02,817 main.py:51] epoch 1479, training loss: 11032.75, average training loss: 13557.23, base loss: 19743.78
[INFO 2017-06-27 21:28:03,789 main.py:51] epoch 1480, training loss: 11842.16, average training loss: 13553.26, base loss: 19739.58
[INFO 2017-06-27 21:28:04,765 main.py:51] epoch 1481, training loss: 11682.10, average training loss: 13550.41, base loss: 19737.81
[INFO 2017-06-27 21:28:05,736 main.py:51] epoch 1482, training loss: 12385.16, average training loss: 13547.71, base loss: 19735.56
[INFO 2017-06-27 21:28:06,705 main.py:51] epoch 1483, training loss: 12902.60, average training loss: 13546.85, base loss: 19737.98
[INFO 2017-06-27 21:28:07,677 main.py:51] epoch 1484, training loss: 10601.26, average training loss: 13544.73, base loss: 19737.51
[INFO 2017-06-27 21:28:08,648 main.py:51] epoch 1485, training loss: 12495.99, average training loss: 13544.03, base loss: 19739.78
[INFO 2017-06-27 21:28:09,621 main.py:51] epoch 1486, training loss: 11917.20, average training loss: 13540.29, base loss: 19738.19
[INFO 2017-06-27 21:28:10,593 main.py:51] epoch 1487, training loss: 11859.35, average training loss: 13538.76, base loss: 19739.15
[INFO 2017-06-27 21:28:11,566 main.py:51] epoch 1488, training loss: 12662.76, average training loss: 13537.12, base loss: 19739.60
[INFO 2017-06-27 21:28:12,541 main.py:51] epoch 1489, training loss: 12079.20, average training loss: 13534.52, base loss: 19738.46
[INFO 2017-06-27 21:28:13,510 main.py:51] epoch 1490, training loss: 11034.69, average training loss: 13532.15, base loss: 19737.16
[INFO 2017-06-27 21:28:14,480 main.py:51] epoch 1491, training loss: 12568.64, average training loss: 13530.85, base loss: 19737.26
[INFO 2017-06-27 21:28:15,453 main.py:51] epoch 1492, training loss: 12676.08, average training loss: 13529.29, base loss: 19735.99
[INFO 2017-06-27 21:28:16,426 main.py:51] epoch 1493, training loss: 14152.27, average training loss: 13529.45, base loss: 19738.72
[INFO 2017-06-27 21:28:17,401 main.py:51] epoch 1494, training loss: 12344.01, average training loss: 13525.92, base loss: 19735.41
[INFO 2017-06-27 21:28:18,373 main.py:51] epoch 1495, training loss: 12601.89, average training loss: 13525.15, base loss: 19736.33
[INFO 2017-06-27 21:28:19,344 main.py:51] epoch 1496, training loss: 11859.55, average training loss: 13522.86, base loss: 19737.71
[INFO 2017-06-27 21:28:20,317 main.py:51] epoch 1497, training loss: 12063.11, average training loss: 13520.77, base loss: 19738.52
[INFO 2017-06-27 21:28:21,293 main.py:51] epoch 1498, training loss: 11515.23, average training loss: 13518.21, base loss: 19737.52
[INFO 2017-06-27 21:28:22,269 main.py:51] epoch 1499, training loss: 13494.94, average training loss: 13517.67, base loss: 19741.24
[INFO 2017-06-27 21:28:22,269 main.py:53] epoch 1499, testing
[INFO 2017-06-27 21:28:26,481 main.py:105] average testing loss: 12699.20, base loss: 19786.09
[INFO 2017-06-27 21:28:26,481 main.py:106] improve_loss: 7086.90, improve_percent: 0.36
[INFO 2017-06-27 21:28:26,482 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:28:26,495 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 21:28:27,467 main.py:51] epoch 1500, training loss: 11956.71, average training loss: 13515.59, base loss: 19740.71
[INFO 2017-06-27 21:28:28,439 main.py:51] epoch 1501, training loss: 12873.54, average training loss: 13512.27, base loss: 19738.55
[INFO 2017-06-27 21:28:29,411 main.py:51] epoch 1502, training loss: 14151.25, average training loss: 13509.07, base loss: 19737.68
[INFO 2017-06-27 21:28:30,382 main.py:51] epoch 1503, training loss: 14011.13, average training loss: 13508.73, base loss: 19740.20
[INFO 2017-06-27 21:28:31,356 main.py:51] epoch 1504, training loss: 11137.30, average training loss: 13504.74, base loss: 19734.78
[INFO 2017-06-27 21:28:32,330 main.py:51] epoch 1505, training loss: 11552.24, average training loss: 13502.72, base loss: 19735.83
[INFO 2017-06-27 21:28:33,302 main.py:51] epoch 1506, training loss: 13244.54, average training loss: 13501.66, base loss: 19737.02
[INFO 2017-06-27 21:28:34,276 main.py:51] epoch 1507, training loss: 12469.14, average training loss: 13501.21, base loss: 19738.16
[INFO 2017-06-27 21:28:35,252 main.py:51] epoch 1508, training loss: 11890.78, average training loss: 13498.82, base loss: 19737.31
[INFO 2017-06-27 21:28:36,226 main.py:51] epoch 1509, training loss: 14196.60, average training loss: 13496.25, base loss: 19736.98
[INFO 2017-06-27 21:28:37,203 main.py:51] epoch 1510, training loss: 13192.96, average training loss: 13494.39, base loss: 19736.06
[INFO 2017-06-27 21:28:38,175 main.py:51] epoch 1511, training loss: 12838.89, average training loss: 13492.73, base loss: 19734.77
[INFO 2017-06-27 21:28:39,149 main.py:51] epoch 1512, training loss: 13021.21, average training loss: 13489.18, base loss: 19732.97
[INFO 2017-06-27 21:28:40,124 main.py:51] epoch 1513, training loss: 13096.15, average training loss: 13488.81, base loss: 19734.09
[INFO 2017-06-27 21:28:41,100 main.py:51] epoch 1514, training loss: 12441.98, average training loss: 13486.30, base loss: 19732.59
[INFO 2017-06-27 21:28:42,081 main.py:51] epoch 1515, training loss: 13014.54, average training loss: 13483.61, base loss: 19729.78
[INFO 2017-06-27 21:28:43,059 main.py:51] epoch 1516, training loss: 12879.63, average training loss: 13480.78, base loss: 19729.20
[INFO 2017-06-27 21:28:44,039 main.py:51] epoch 1517, training loss: 12764.85, average training loss: 13480.49, base loss: 19731.62
[INFO 2017-06-27 21:28:45,019 main.py:51] epoch 1518, training loss: 12642.34, average training loss: 13478.50, base loss: 19730.97
[INFO 2017-06-27 21:28:45,998 main.py:51] epoch 1519, training loss: 11013.53, average training loss: 13476.60, base loss: 19732.40
[INFO 2017-06-27 21:28:46,974 main.py:51] epoch 1520, training loss: 11829.18, average training loss: 13473.55, base loss: 19731.09
[INFO 2017-06-27 21:28:47,948 main.py:51] epoch 1521, training loss: 12125.52, average training loss: 13472.31, base loss: 19731.67
[INFO 2017-06-27 21:28:48,924 main.py:51] epoch 1522, training loss: 13404.96, average training loss: 13470.78, base loss: 19733.58
[INFO 2017-06-27 21:28:49,900 main.py:51] epoch 1523, training loss: 12995.45, average training loss: 13468.71, base loss: 19733.11
[INFO 2017-06-27 21:28:50,873 main.py:51] epoch 1524, training loss: 11435.53, average training loss: 13465.66, base loss: 19731.11
[INFO 2017-06-27 21:28:51,848 main.py:51] epoch 1525, training loss: 11681.61, average training loss: 13461.95, base loss: 19729.94
[INFO 2017-06-27 21:28:52,826 main.py:51] epoch 1526, training loss: 11822.56, average training loss: 13460.43, base loss: 19728.64
[INFO 2017-06-27 21:28:53,805 main.py:51] epoch 1527, training loss: 10608.09, average training loss: 13456.01, base loss: 19722.60
[INFO 2017-06-27 21:28:54,781 main.py:51] epoch 1528, training loss: 12926.51, average training loss: 13454.11, base loss: 19722.98
[INFO 2017-06-27 21:28:55,758 main.py:51] epoch 1529, training loss: 11147.93, average training loss: 13450.41, base loss: 19719.24
[INFO 2017-06-27 21:28:56,735 main.py:51] epoch 1530, training loss: 12950.29, average training loss: 13448.75, base loss: 19720.70
[INFO 2017-06-27 21:28:57,707 main.py:51] epoch 1531, training loss: 12785.52, average training loss: 13445.18, base loss: 19719.00
[INFO 2017-06-27 21:28:58,682 main.py:51] epoch 1532, training loss: 13609.48, average training loss: 13444.28, base loss: 19723.00
[INFO 2017-06-27 21:28:59,663 main.py:51] epoch 1533, training loss: 13145.93, average training loss: 13443.59, base loss: 19725.58
[INFO 2017-06-27 21:29:00,640 main.py:51] epoch 1534, training loss: 13008.21, average training loss: 13442.74, base loss: 19725.91
[INFO 2017-06-27 21:29:01,619 main.py:51] epoch 1535, training loss: 11119.15, average training loss: 13439.19, base loss: 19721.94
[INFO 2017-06-27 21:29:02,594 main.py:51] epoch 1536, training loss: 12522.85, average training loss: 13436.12, base loss: 19719.11
[INFO 2017-06-27 21:29:03,573 main.py:51] epoch 1537, training loss: 11487.67, average training loss: 13433.28, base loss: 19718.19
[INFO 2017-06-27 21:29:04,546 main.py:51] epoch 1538, training loss: 13093.83, average training loss: 13430.46, base loss: 19719.34
[INFO 2017-06-27 21:29:05,519 main.py:51] epoch 1539, training loss: 14291.75, average training loss: 13429.98, base loss: 19723.31
[INFO 2017-06-27 21:29:06,497 main.py:51] epoch 1540, training loss: 12718.83, average training loss: 13428.57, base loss: 19726.34
[INFO 2017-06-27 21:29:07,473 main.py:51] epoch 1541, training loss: 13872.81, average training loss: 13427.76, base loss: 19728.20
[INFO 2017-06-27 21:29:08,449 main.py:51] epoch 1542, training loss: 13408.55, average training loss: 13425.17, base loss: 19728.33
[INFO 2017-06-27 21:29:09,426 main.py:51] epoch 1543, training loss: 13918.80, average training loss: 13424.50, base loss: 19730.91
[INFO 2017-06-27 21:29:10,402 main.py:51] epoch 1544, training loss: 13813.80, average training loss: 13422.35, base loss: 19731.66
[INFO 2017-06-27 21:29:11,379 main.py:51] epoch 1545, training loss: 13786.21, average training loss: 13421.86, base loss: 19734.81
[INFO 2017-06-27 21:29:12,356 main.py:51] epoch 1546, training loss: 13148.73, average training loss: 13417.91, base loss: 19732.73
[INFO 2017-06-27 21:29:13,333 main.py:51] epoch 1547, training loss: 12750.20, average training loss: 13415.63, base loss: 19732.08
[INFO 2017-06-27 21:29:14,312 main.py:51] epoch 1548, training loss: 12557.05, average training loss: 13413.20, base loss: 19731.42
[INFO 2017-06-27 21:29:15,287 main.py:51] epoch 1549, training loss: 11998.15, average training loss: 13408.09, base loss: 19725.84
[INFO 2017-06-27 21:29:16,264 main.py:51] epoch 1550, training loss: 11946.95, average training loss: 13404.32, base loss: 19724.13
[INFO 2017-06-27 21:29:17,239 main.py:51] epoch 1551, training loss: 12146.33, average training loss: 13401.94, base loss: 19726.11
[INFO 2017-06-27 21:29:18,215 main.py:51] epoch 1552, training loss: 11858.05, average training loss: 13399.95, base loss: 19724.15
[INFO 2017-06-27 21:29:19,194 main.py:51] epoch 1553, training loss: 12435.79, average training loss: 13398.54, base loss: 19725.10
[INFO 2017-06-27 21:29:20,167 main.py:51] epoch 1554, training loss: 12326.76, average training loss: 13395.19, base loss: 19723.11
[INFO 2017-06-27 21:29:21,141 main.py:51] epoch 1555, training loss: 12576.55, average training loss: 13391.57, base loss: 19720.75
[INFO 2017-06-27 21:29:22,120 main.py:51] epoch 1556, training loss: 13649.40, average training loss: 13390.68, base loss: 19722.91
[INFO 2017-06-27 21:29:23,097 main.py:51] epoch 1557, training loss: 11681.90, average training loss: 13390.56, base loss: 19725.17
[INFO 2017-06-27 21:29:24,071 main.py:51] epoch 1558, training loss: 11267.93, average training loss: 13386.40, base loss: 19720.44
[INFO 2017-06-27 21:29:25,048 main.py:51] epoch 1559, training loss: 11121.46, average training loss: 13380.69, base loss: 19713.21
[INFO 2017-06-27 21:29:26,024 main.py:51] epoch 1560, training loss: 12223.00, average training loss: 13379.41, base loss: 19713.33
[INFO 2017-06-27 21:29:27,000 main.py:51] epoch 1561, training loss: 11317.57, average training loss: 13375.55, base loss: 19710.30
[INFO 2017-06-27 21:29:27,975 main.py:51] epoch 1562, training loss: 13721.68, average training loss: 13375.95, base loss: 19713.27
[INFO 2017-06-27 21:29:28,954 main.py:51] epoch 1563, training loss: 13412.58, average training loss: 13372.55, base loss: 19711.32
[INFO 2017-06-27 21:29:29,930 main.py:51] epoch 1564, training loss: 13591.85, average training loss: 13371.68, base loss: 19712.94
[INFO 2017-06-27 21:29:30,906 main.py:51] epoch 1565, training loss: 11410.85, average training loss: 13368.34, base loss: 19711.73
[INFO 2017-06-27 21:29:31,880 main.py:51] epoch 1566, training loss: 12532.68, average training loss: 13365.74, base loss: 19710.88
[INFO 2017-06-27 21:29:32,857 main.py:51] epoch 1567, training loss: 12877.99, average training loss: 13363.25, base loss: 19710.14
[INFO 2017-06-27 21:29:33,832 main.py:51] epoch 1568, training loss: 14188.54, average training loss: 13363.66, base loss: 19714.11
[INFO 2017-06-27 21:29:34,810 main.py:51] epoch 1569, training loss: 11710.96, average training loss: 13359.22, base loss: 19710.12
[INFO 2017-06-27 21:29:35,787 main.py:51] epoch 1570, training loss: 14522.29, average training loss: 13359.54, base loss: 19715.23
[INFO 2017-06-27 21:29:36,762 main.py:51] epoch 1571, training loss: 11874.96, average training loss: 13357.42, base loss: 19713.93
[INFO 2017-06-27 21:29:37,740 main.py:51] epoch 1572, training loss: 13019.86, average training loss: 13355.12, base loss: 19712.45
[INFO 2017-06-27 21:29:38,716 main.py:51] epoch 1573, training loss: 10735.78, average training loss: 13351.25, base loss: 19709.28
[INFO 2017-06-27 21:29:39,692 main.py:51] epoch 1574, training loss: 12358.70, average training loss: 13349.79, base loss: 19708.38
[INFO 2017-06-27 21:29:40,667 main.py:51] epoch 1575, training loss: 11016.09, average training loss: 13346.61, base loss: 19707.29
[INFO 2017-06-27 21:29:41,641 main.py:51] epoch 1576, training loss: 12989.14, average training loss: 13344.51, base loss: 19705.67
[INFO 2017-06-27 21:29:42,618 main.py:51] epoch 1577, training loss: 12132.98, average training loss: 13342.03, base loss: 19705.20
[INFO 2017-06-27 21:29:43,594 main.py:51] epoch 1578, training loss: 12305.21, average training loss: 13339.94, base loss: 19704.85
[INFO 2017-06-27 21:29:44,571 main.py:51] epoch 1579, training loss: 11505.03, average training loss: 13335.81, base loss: 19700.18
[INFO 2017-06-27 21:29:45,548 main.py:51] epoch 1580, training loss: 13089.91, average training loss: 13336.14, base loss: 19704.67
[INFO 2017-06-27 21:29:46,526 main.py:51] epoch 1581, training loss: 11183.94, average training loss: 13334.13, base loss: 19705.04
[INFO 2017-06-27 21:29:47,501 main.py:51] epoch 1582, training loss: 13202.96, average training loss: 13333.58, base loss: 19708.16
[INFO 2017-06-27 21:29:48,476 main.py:51] epoch 1583, training loss: 12084.30, average training loss: 13328.91, base loss: 19703.33
[INFO 2017-06-27 21:29:49,455 main.py:51] epoch 1584, training loss: 10945.68, average training loss: 13324.29, base loss: 19698.22
[INFO 2017-06-27 21:29:50,438 main.py:51] epoch 1585, training loss: 13293.52, average training loss: 13324.36, base loss: 19702.83
[INFO 2017-06-27 21:29:51,416 main.py:51] epoch 1586, training loss: 12528.93, average training loss: 13323.52, base loss: 19704.13
[INFO 2017-06-27 21:29:52,391 main.py:51] epoch 1587, training loss: 12539.35, average training loss: 13322.19, base loss: 19705.61
[INFO 2017-06-27 21:29:53,368 main.py:51] epoch 1588, training loss: 13478.44, average training loss: 13320.44, base loss: 19705.66
[INFO 2017-06-27 21:29:54,346 main.py:51] epoch 1589, training loss: 12150.82, average training loss: 13318.18, base loss: 19706.53
[INFO 2017-06-27 21:29:55,320 main.py:51] epoch 1590, training loss: 12940.53, average training loss: 13316.60, base loss: 19706.94
[INFO 2017-06-27 21:29:56,293 main.py:51] epoch 1591, training loss: 12475.73, average training loss: 13313.57, base loss: 19705.77
[INFO 2017-06-27 21:29:57,267 main.py:51] epoch 1592, training loss: 13598.88, average training loss: 13312.36, base loss: 19708.40
[INFO 2017-06-27 21:29:58,242 main.py:51] epoch 1593, training loss: 13540.14, average training loss: 13311.95, base loss: 19711.83
[INFO 2017-06-27 21:29:59,219 main.py:51] epoch 1594, training loss: 11429.79, average training loss: 13308.38, base loss: 19707.47
[INFO 2017-06-27 21:30:00,197 main.py:51] epoch 1595, training loss: 14316.53, average training loss: 13308.38, base loss: 19711.23
[INFO 2017-06-27 21:30:01,174 main.py:51] epoch 1596, training loss: 12050.81, average training loss: 13306.19, base loss: 19709.46
[INFO 2017-06-27 21:30:02,154 main.py:51] epoch 1597, training loss: 12499.23, average training loss: 13303.08, base loss: 19708.27
[INFO 2017-06-27 21:30:03,132 main.py:51] epoch 1598, training loss: 12500.15, average training loss: 13300.10, base loss: 19705.88
[INFO 2017-06-27 21:30:04,113 main.py:51] epoch 1599, training loss: 12998.54, average training loss: 13299.65, base loss: 19707.32
[INFO 2017-06-27 21:30:04,113 main.py:53] epoch 1599, testing
[INFO 2017-06-27 21:30:08,327 main.py:105] average testing loss: 12970.32, base loss: 20448.74
[INFO 2017-06-27 21:30:08,327 main.py:106] improve_loss: 7478.42, improve_percent: 0.37
[INFO 2017-06-27 21:30:08,327 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:30:08,351 main.py:76] current best improved percent: 0.37
[INFO 2017-06-27 21:30:09,348 main.py:51] epoch 1600, training loss: 13730.09, average training loss: 13299.40, base loss: 19710.59
[INFO 2017-06-27 21:30:10,320 main.py:51] epoch 1601, training loss: 11531.88, average training loss: 13298.44, base loss: 19712.07
[INFO 2017-06-27 21:30:11,291 main.py:51] epoch 1602, training loss: 10441.18, average training loss: 13294.45, base loss: 19707.51
[INFO 2017-06-27 21:30:12,263 main.py:51] epoch 1603, training loss: 13681.73, average training loss: 13292.62, base loss: 19707.06
[INFO 2017-06-27 21:30:13,235 main.py:51] epoch 1604, training loss: 12998.22, average training loss: 13292.07, base loss: 19708.81
[INFO 2017-06-27 21:30:14,207 main.py:51] epoch 1605, training loss: 11301.17, average training loss: 13288.04, base loss: 19705.40
[INFO 2017-06-27 21:30:15,181 main.py:51] epoch 1606, training loss: 10920.98, average training loss: 13286.27, base loss: 19704.21
[INFO 2017-06-27 21:30:16,153 main.py:51] epoch 1607, training loss: 14364.41, average training loss: 13286.32, base loss: 19706.97
[INFO 2017-06-27 21:30:17,125 main.py:51] epoch 1608, training loss: 13098.39, average training loss: 13285.92, base loss: 19710.10
[INFO 2017-06-27 21:30:18,095 main.py:51] epoch 1609, training loss: 11689.10, average training loss: 13282.53, base loss: 19707.27
[INFO 2017-06-27 21:30:19,065 main.py:51] epoch 1610, training loss: 11445.96, average training loss: 13281.24, base loss: 19706.43
[INFO 2017-06-27 21:30:20,037 main.py:51] epoch 1611, training loss: 13030.82, average training loss: 13278.90, base loss: 19705.48
[INFO 2017-06-27 21:30:21,009 main.py:51] epoch 1612, training loss: 10265.64, average training loss: 13275.25, base loss: 19701.67
[INFO 2017-06-27 21:30:21,982 main.py:51] epoch 1613, training loss: 13625.96, average training loss: 13274.84, base loss: 19703.45
[INFO 2017-06-27 21:30:22,954 main.py:51] epoch 1614, training loss: 12020.47, average training loss: 13273.59, base loss: 19703.35
[INFO 2017-06-27 21:30:23,928 main.py:51] epoch 1615, training loss: 11726.70, average training loss: 13270.18, base loss: 19699.69
[INFO 2017-06-27 21:30:24,900 main.py:51] epoch 1616, training loss: 12674.08, average training loss: 13269.62, base loss: 19701.52
[INFO 2017-06-27 21:30:25,869 main.py:51] epoch 1617, training loss: 11942.01, average training loss: 13266.29, base loss: 19699.99
[INFO 2017-06-27 21:30:26,839 main.py:51] epoch 1618, training loss: 10985.91, average training loss: 13262.95, base loss: 19696.80
[INFO 2017-06-27 21:30:27,810 main.py:51] epoch 1619, training loss: 14395.17, average training loss: 13263.17, base loss: 19701.64
[INFO 2017-06-27 21:30:28,782 main.py:51] epoch 1620, training loss: 13023.07, average training loss: 13261.98, base loss: 19702.76
[INFO 2017-06-27 21:30:29,753 main.py:51] epoch 1621, training loss: 11255.54, average training loss: 13257.92, base loss: 19697.71
[INFO 2017-06-27 21:30:30,727 main.py:51] epoch 1622, training loss: 12043.74, average training loss: 13256.71, base loss: 19698.61
[INFO 2017-06-27 21:30:31,702 main.py:51] epoch 1623, training loss: 11130.05, average training loss: 13253.28, base loss: 19694.56
[INFO 2017-06-27 21:30:32,673 main.py:51] epoch 1624, training loss: 11688.22, average training loss: 13250.75, base loss: 19693.75
[INFO 2017-06-27 21:30:33,645 main.py:51] epoch 1625, training loss: 13070.00, average training loss: 13250.27, base loss: 19695.98
[INFO 2017-06-27 21:30:34,619 main.py:51] epoch 1626, training loss: 11537.79, average training loss: 13248.49, base loss: 19694.30
[INFO 2017-06-27 21:30:35,592 main.py:51] epoch 1627, training loss: 12812.48, average training loss: 13246.52, base loss: 19694.21
[INFO 2017-06-27 21:30:36,562 main.py:51] epoch 1628, training loss: 11599.32, average training loss: 13244.20, base loss: 19693.35
[INFO 2017-06-27 21:30:37,538 main.py:51] epoch 1629, training loss: 13773.72, average training loss: 13243.21, base loss: 19697.12
[INFO 2017-06-27 21:30:38,514 main.py:51] epoch 1630, training loss: 12034.61, average training loss: 13242.49, base loss: 19698.78
[INFO 2017-06-27 21:30:39,488 main.py:51] epoch 1631, training loss: 12101.39, average training loss: 13240.74, base loss: 19698.57
[INFO 2017-06-27 21:30:40,462 main.py:51] epoch 1632, training loss: 13685.11, average training loss: 13239.79, base loss: 19699.75
[INFO 2017-06-27 21:30:41,432 main.py:51] epoch 1633, training loss: 11835.72, average training loss: 13235.98, base loss: 19697.32
[INFO 2017-06-27 21:30:42,407 main.py:51] epoch 1634, training loss: 11753.78, average training loss: 13233.94, base loss: 19696.70
[INFO 2017-06-27 21:30:43,378 main.py:51] epoch 1635, training loss: 13189.07, average training loss: 13232.68, base loss: 19698.31
[INFO 2017-06-27 21:30:44,351 main.py:51] epoch 1636, training loss: 10327.06, average training loss: 13230.88, base loss: 19698.09
[INFO 2017-06-27 21:30:45,321 main.py:51] epoch 1637, training loss: 13066.62, average training loss: 13228.87, base loss: 19698.08
[INFO 2017-06-27 21:30:46,293 main.py:51] epoch 1638, training loss: 11255.76, average training loss: 13224.18, base loss: 19689.94
[INFO 2017-06-27 21:30:47,268 main.py:51] epoch 1639, training loss: 12337.35, average training loss: 13221.26, base loss: 19687.71
[INFO 2017-06-27 21:30:48,238 main.py:51] epoch 1640, training loss: 12777.98, average training loss: 13220.84, base loss: 19689.24
[INFO 2017-06-27 21:30:49,210 main.py:51] epoch 1641, training loss: 12553.70, average training loss: 13219.48, base loss: 19690.13
[INFO 2017-06-27 21:30:50,181 main.py:51] epoch 1642, training loss: 13445.16, average training loss: 13219.37, base loss: 19690.66
[INFO 2017-06-27 21:30:51,151 main.py:51] epoch 1643, training loss: 11334.42, average training loss: 13217.51, base loss: 19689.99
[INFO 2017-06-27 21:30:52,126 main.py:51] epoch 1644, training loss: 12629.45, average training loss: 13215.23, base loss: 19691.24
[INFO 2017-06-27 21:30:53,095 main.py:51] epoch 1645, training loss: 12324.25, average training loss: 13213.60, base loss: 19690.96
[INFO 2017-06-27 21:30:54,066 main.py:51] epoch 1646, training loss: 13309.67, average training loss: 13213.65, base loss: 19694.34
[INFO 2017-06-27 21:30:55,040 main.py:51] epoch 1647, training loss: 13133.17, average training loss: 13215.65, base loss: 19699.99
[INFO 2017-06-27 21:30:56,012 main.py:51] epoch 1648, training loss: 15125.38, average training loss: 13217.41, base loss: 19706.58
[INFO 2017-06-27 21:30:56,984 main.py:51] epoch 1649, training loss: 12406.76, average training loss: 13217.48, base loss: 19710.33
[INFO 2017-06-27 21:30:57,958 main.py:51] epoch 1650, training loss: 13068.42, average training loss: 13216.51, base loss: 19710.72
[INFO 2017-06-27 21:30:58,933 main.py:51] epoch 1651, training loss: 11770.49, average training loss: 13213.25, base loss: 19710.36
[INFO 2017-06-27 21:30:59,902 main.py:51] epoch 1652, training loss: 12172.05, average training loss: 13211.20, base loss: 19709.18
[INFO 2017-06-27 21:31:00,873 main.py:51] epoch 1653, training loss: 12004.24, average training loss: 13210.55, base loss: 19711.05
[INFO 2017-06-27 21:31:01,846 main.py:51] epoch 1654, training loss: 11416.68, average training loss: 13205.43, base loss: 19706.40
[INFO 2017-06-27 21:31:02,816 main.py:51] epoch 1655, training loss: 13287.17, average training loss: 13203.93, base loss: 19706.74
[INFO 2017-06-27 21:31:03,784 main.py:51] epoch 1656, training loss: 11911.70, average training loss: 13201.39, base loss: 19704.61
[INFO 2017-06-27 21:31:04,760 main.py:51] epoch 1657, training loss: 10242.33, average training loss: 13198.40, base loss: 19701.23
[INFO 2017-06-27 21:31:05,737 main.py:51] epoch 1658, training loss: 10821.41, average training loss: 13196.64, base loss: 19700.16
[INFO 2017-06-27 21:31:06,709 main.py:51] epoch 1659, training loss: 12446.33, average training loss: 13195.86, base loss: 19701.66
[INFO 2017-06-27 21:31:07,681 main.py:51] epoch 1660, training loss: 12884.81, average training loss: 13191.88, base loss: 19696.78
[INFO 2017-06-27 21:31:08,650 main.py:51] epoch 1661, training loss: 11713.81, average training loss: 13191.14, base loss: 19699.29
[INFO 2017-06-27 21:31:09,629 main.py:51] epoch 1662, training loss: 13435.21, average training loss: 13189.04, base loss: 19700.20
[INFO 2017-06-27 21:31:10,600 main.py:51] epoch 1663, training loss: 12618.78, average training loss: 13189.04, base loss: 19702.61
[INFO 2017-06-27 21:31:11,573 main.py:51] epoch 1664, training loss: 12652.20, average training loss: 13188.02, base loss: 19702.97
[INFO 2017-06-27 21:31:12,545 main.py:51] epoch 1665, training loss: 13651.81, average training loss: 13190.31, base loss: 19710.26
[INFO 2017-06-27 21:31:13,516 main.py:51] epoch 1666, training loss: 11940.38, average training loss: 13187.27, base loss: 19707.28
[INFO 2017-06-27 21:31:14,487 main.py:51] epoch 1667, training loss: 12016.66, average training loss: 13182.47, base loss: 19702.63
[INFO 2017-06-27 21:31:15,457 main.py:51] epoch 1668, training loss: 11399.21, average training loss: 13179.15, base loss: 19700.93
[INFO 2017-06-27 21:31:16,428 main.py:51] epoch 1669, training loss: 11930.18, average training loss: 13177.83, base loss: 19702.04
[INFO 2017-06-27 21:31:17,403 main.py:51] epoch 1670, training loss: 10700.60, average training loss: 13173.67, base loss: 19699.84
[INFO 2017-06-27 21:31:18,374 main.py:51] epoch 1671, training loss: 12232.87, average training loss: 13173.02, base loss: 19700.22
[INFO 2017-06-27 21:31:19,343 main.py:51] epoch 1672, training loss: 12621.38, average training loss: 13170.83, base loss: 19698.53
[INFO 2017-06-27 21:31:20,314 main.py:51] epoch 1673, training loss: 10541.52, average training loss: 13168.66, base loss: 19697.56
[INFO 2017-06-27 21:31:21,287 main.py:51] epoch 1674, training loss: 12198.03, average training loss: 13167.35, base loss: 19699.49
[INFO 2017-06-27 21:31:22,260 main.py:51] epoch 1675, training loss: 10855.22, average training loss: 13163.51, base loss: 19695.51
[INFO 2017-06-27 21:31:23,231 main.py:51] epoch 1676, training loss: 11950.20, average training loss: 13160.50, base loss: 19694.01
[INFO 2017-06-27 21:31:24,201 main.py:51] epoch 1677, training loss: 12381.79, average training loss: 13160.00, base loss: 19695.72
[INFO 2017-06-27 21:31:25,172 main.py:51] epoch 1678, training loss: 13737.06, average training loss: 13158.63, base loss: 19696.92
[INFO 2017-06-27 21:31:26,142 main.py:51] epoch 1679, training loss: 14436.53, average training loss: 13157.60, base loss: 19698.94
[INFO 2017-06-27 21:31:27,113 main.py:51] epoch 1680, training loss: 13659.96, average training loss: 13155.80, base loss: 19699.77
[INFO 2017-06-27 21:31:28,084 main.py:51] epoch 1681, training loss: 13582.34, average training loss: 13154.32, base loss: 19703.19
[INFO 2017-06-27 21:31:29,058 main.py:51] epoch 1682, training loss: 12951.19, average training loss: 13155.91, base loss: 19708.37
[INFO 2017-06-27 21:31:30,027 main.py:51] epoch 1683, training loss: 10049.89, average training loss: 13151.66, base loss: 19702.54
[INFO 2017-06-27 21:31:31,002 main.py:51] epoch 1684, training loss: 13244.11, average training loss: 13150.38, base loss: 19704.69
[INFO 2017-06-27 21:31:31,973 main.py:51] epoch 1685, training loss: 11882.93, average training loss: 13148.34, base loss: 19703.93
[INFO 2017-06-27 21:31:32,944 main.py:51] epoch 1686, training loss: 10714.44, average training loss: 13145.32, base loss: 19702.29
[INFO 2017-06-27 21:31:33,912 main.py:51] epoch 1687, training loss: 11706.80, average training loss: 13144.82, base loss: 19703.88
[INFO 2017-06-27 21:31:34,887 main.py:51] epoch 1688, training loss: 12827.92, average training loss: 13142.16, base loss: 19703.04
[INFO 2017-06-27 21:31:35,856 main.py:51] epoch 1689, training loss: 12924.51, average training loss: 13142.18, base loss: 19707.23
[INFO 2017-06-27 21:31:36,831 main.py:51] epoch 1690, training loss: 12218.33, average training loss: 13140.24, base loss: 19704.70
[INFO 2017-06-27 21:31:37,805 main.py:51] epoch 1691, training loss: 11630.56, average training loss: 13138.39, base loss: 19703.51
[INFO 2017-06-27 21:31:38,776 main.py:51] epoch 1692, training loss: 10648.47, average training loss: 13134.64, base loss: 19700.35
[INFO 2017-06-27 21:31:39,746 main.py:51] epoch 1693, training loss: 12765.20, average training loss: 13133.37, base loss: 19700.69
[INFO 2017-06-27 21:31:40,720 main.py:51] epoch 1694, training loss: 12312.14, average training loss: 13131.31, base loss: 19698.34
[INFO 2017-06-27 21:31:41,692 main.py:51] epoch 1695, training loss: 12573.41, average training loss: 13130.01, base loss: 19700.07
[INFO 2017-06-27 21:31:42,662 main.py:51] epoch 1696, training loss: 12215.49, average training loss: 13123.98, base loss: 19693.59
[INFO 2017-06-27 21:31:43,634 main.py:51] epoch 1697, training loss: 13766.01, average training loss: 13125.48, base loss: 19698.98
[INFO 2017-06-27 21:31:44,607 main.py:51] epoch 1698, training loss: 11146.72, average training loss: 13123.31, base loss: 19699.66
[INFO 2017-06-27 21:31:45,576 main.py:51] epoch 1699, training loss: 11479.50, average training loss: 13121.72, base loss: 19700.39
[INFO 2017-06-27 21:31:45,576 main.py:53] epoch 1699, testing
[INFO 2017-06-27 21:31:49,786 main.py:105] average testing loss: 11981.71, base loss: 18994.10
[INFO 2017-06-27 21:31:49,786 main.py:106] improve_loss: 7012.40, improve_percent: 0.37
[INFO 2017-06-27 21:31:49,787 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:31:49,800 main.py:76] current best improved percent: 0.37
[INFO 2017-06-27 21:31:50,774 main.py:51] epoch 1700, training loss: 11546.96, average training loss: 13119.12, base loss: 19699.40
[INFO 2017-06-27 21:31:51,745 main.py:51] epoch 1701, training loss: 11155.71, average training loss: 13116.06, base loss: 19697.34
[INFO 2017-06-27 21:31:52,718 main.py:51] epoch 1702, training loss: 11949.10, average training loss: 13113.52, base loss: 19695.89
[INFO 2017-06-27 21:31:53,690 main.py:51] epoch 1703, training loss: 10816.58, average training loss: 13112.02, base loss: 19694.84
[INFO 2017-06-27 21:31:54,660 main.py:51] epoch 1704, training loss: 11555.71, average training loss: 13109.70, base loss: 19694.18
[INFO 2017-06-27 21:31:55,631 main.py:51] epoch 1705, training loss: 13155.43, average training loss: 13109.01, base loss: 19696.31
[INFO 2017-06-27 21:31:56,602 main.py:51] epoch 1706, training loss: 12437.85, average training loss: 13107.92, base loss: 19696.49
[INFO 2017-06-27 21:31:57,574 main.py:51] epoch 1707, training loss: 11718.22, average training loss: 13107.66, base loss: 19698.24
[INFO 2017-06-27 21:31:58,546 main.py:51] epoch 1708, training loss: 11641.67, average training loss: 13104.32, base loss: 19695.10
[INFO 2017-06-27 21:31:59,517 main.py:51] epoch 1709, training loss: 11236.91, average training loss: 13101.03, base loss: 19692.94
[INFO 2017-06-27 21:32:00,487 main.py:51] epoch 1710, training loss: 12449.03, average training loss: 13096.51, base loss: 19689.53
[INFO 2017-06-27 21:32:01,461 main.py:51] epoch 1711, training loss: 12139.14, average training loss: 13095.33, base loss: 19690.17
[INFO 2017-06-27 21:32:02,432 main.py:51] epoch 1712, training loss: 11764.60, average training loss: 13095.04, base loss: 19690.89
[INFO 2017-06-27 21:32:03,402 main.py:51] epoch 1713, training loss: 13337.69, average training loss: 13095.15, base loss: 19695.70
[INFO 2017-06-27 21:32:04,375 main.py:51] epoch 1714, training loss: 12932.73, average training loss: 13094.84, base loss: 19699.91
[INFO 2017-06-27 21:32:05,345 main.py:51] epoch 1715, training loss: 11426.44, average training loss: 13091.23, base loss: 19695.68
[INFO 2017-06-27 21:32:06,319 main.py:51] epoch 1716, training loss: 13453.94, average training loss: 13089.52, base loss: 19696.03
[INFO 2017-06-27 21:32:07,291 main.py:51] epoch 1717, training loss: 12248.80, average training loss: 13084.86, base loss: 19691.97
[INFO 2017-06-27 21:32:08,266 main.py:51] epoch 1718, training loss: 11993.74, average training loss: 13080.22, base loss: 19688.26
[INFO 2017-06-27 21:32:09,238 main.py:51] epoch 1719, training loss: 13877.65, average training loss: 13080.75, base loss: 19691.57
[INFO 2017-06-27 21:32:10,216 main.py:51] epoch 1720, training loss: 13326.46, average training loss: 13080.13, base loss: 19693.69
[INFO 2017-06-27 21:32:11,187 main.py:51] epoch 1721, training loss: 12904.18, average training loss: 13079.05, base loss: 19696.00
[INFO 2017-06-27 21:32:12,158 main.py:51] epoch 1722, training loss: 12384.50, average training loss: 13075.54, base loss: 19692.25
[INFO 2017-06-27 21:32:13,129 main.py:51] epoch 1723, training loss: 13286.77, average training loss: 13075.74, base loss: 19694.34
[INFO 2017-06-27 21:32:14,098 main.py:51] epoch 1724, training loss: 13289.69, average training loss: 13074.51, base loss: 19697.08
[INFO 2017-06-27 21:32:15,070 main.py:51] epoch 1725, training loss: 12302.23, average training loss: 13072.52, base loss: 19693.67
[INFO 2017-06-27 21:32:16,039 main.py:51] epoch 1726, training loss: 12582.54, average training loss: 13072.70, base loss: 19696.18
[INFO 2017-06-27 21:32:17,014 main.py:51] epoch 1727, training loss: 11753.66, average training loss: 13071.32, base loss: 19697.90
[INFO 2017-06-27 21:32:17,984 main.py:51] epoch 1728, training loss: 13812.28, average training loss: 13069.59, base loss: 19697.08
[INFO 2017-06-27 21:32:18,956 main.py:51] epoch 1729, training loss: 11175.07, average training loss: 13067.31, base loss: 19695.38
[INFO 2017-06-27 21:32:19,927 main.py:51] epoch 1730, training loss: 11718.68, average training loss: 13065.28, base loss: 19694.61
[INFO 2017-06-27 21:32:20,896 main.py:51] epoch 1731, training loss: 13863.86, average training loss: 13065.95, base loss: 19699.11
[INFO 2017-06-27 21:32:21,868 main.py:51] epoch 1732, training loss: 11489.12, average training loss: 13065.37, base loss: 19699.77
[INFO 2017-06-27 21:32:22,838 main.py:51] epoch 1733, training loss: 12230.29, average training loss: 13064.36, base loss: 19701.36
[INFO 2017-06-27 21:32:23,812 main.py:51] epoch 1734, training loss: 13458.60, average training loss: 13063.42, base loss: 19703.42
[INFO 2017-06-27 21:32:24,784 main.py:51] epoch 1735, training loss: 12678.41, average training loss: 13062.89, base loss: 19706.80
[INFO 2017-06-27 21:32:25,757 main.py:51] epoch 1736, training loss: 11209.46, average training loss: 13059.52, base loss: 19704.86
[INFO 2017-06-27 21:32:26,735 main.py:51] epoch 1737, training loss: 12167.88, average training loss: 13057.09, base loss: 19704.30
[INFO 2017-06-27 21:32:27,711 main.py:51] epoch 1738, training loss: 12463.79, average training loss: 13054.95, base loss: 19703.35
[INFO 2017-06-27 21:32:28,684 main.py:51] epoch 1739, training loss: 13419.05, average training loss: 13054.41, base loss: 19705.26
[INFO 2017-06-27 21:32:29,654 main.py:51] epoch 1740, training loss: 13932.01, average training loss: 13054.95, base loss: 19708.71
[INFO 2017-06-27 21:32:30,630 main.py:51] epoch 1741, training loss: 13993.14, average training loss: 13054.92, base loss: 19714.14
[INFO 2017-06-27 21:32:31,603 main.py:51] epoch 1742, training loss: 11526.98, average training loss: 13053.01, base loss: 19714.35
[INFO 2017-06-27 21:32:32,579 main.py:51] epoch 1743, training loss: 11797.57, average training loss: 13051.31, base loss: 19713.57
[INFO 2017-06-27 21:32:33,555 main.py:51] epoch 1744, training loss: 11825.30, average training loss: 13048.11, base loss: 19711.50
[INFO 2017-06-27 21:32:34,524 main.py:51] epoch 1745, training loss: 10219.77, average training loss: 13046.07, base loss: 19707.65
[INFO 2017-06-27 21:32:35,496 main.py:51] epoch 1746, training loss: 11350.00, average training loss: 13044.51, base loss: 19706.93
[INFO 2017-06-27 21:32:36,469 main.py:51] epoch 1747, training loss: 13384.63, average training loss: 13044.19, base loss: 19708.79
[INFO 2017-06-27 21:32:37,442 main.py:51] epoch 1748, training loss: 13188.52, average training loss: 13042.70, base loss: 19707.87
[INFO 2017-06-27 21:32:38,413 main.py:51] epoch 1749, training loss: 12281.05, average training loss: 13042.36, base loss: 19709.42
[INFO 2017-06-27 21:32:39,386 main.py:51] epoch 1750, training loss: 12311.19, average training loss: 13041.49, base loss: 19711.47
[INFO 2017-06-27 21:32:40,355 main.py:51] epoch 1751, training loss: 11994.43, average training loss: 13039.93, base loss: 19711.33
[INFO 2017-06-27 21:32:41,327 main.py:51] epoch 1752, training loss: 11332.36, average training loss: 13038.05, base loss: 19709.50
[INFO 2017-06-27 21:32:42,301 main.py:51] epoch 1753, training loss: 12342.16, average training loss: 13035.96, base loss: 19708.79
[INFO 2017-06-27 21:32:43,273 main.py:51] epoch 1754, training loss: 11454.16, average training loss: 13032.88, base loss: 19705.23
[INFO 2017-06-27 21:32:44,244 main.py:51] epoch 1755, training loss: 11113.47, average training loss: 13027.96, base loss: 19700.47
[INFO 2017-06-27 21:32:45,214 main.py:51] epoch 1756, training loss: 13065.16, average training loss: 13026.16, base loss: 19698.43
[INFO 2017-06-27 21:32:46,185 main.py:51] epoch 1757, training loss: 12873.20, average training loss: 13024.33, base loss: 19698.00
[INFO 2017-06-27 21:32:47,160 main.py:51] epoch 1758, training loss: 12537.21, average training loss: 13023.25, base loss: 19698.59
[INFO 2017-06-27 21:32:48,135 main.py:51] epoch 1759, training loss: 11690.31, average training loss: 13021.90, base loss: 19699.26
[INFO 2017-06-27 21:32:49,111 main.py:51] epoch 1760, training loss: 11964.05, average training loss: 13019.57, base loss: 19697.84
[INFO 2017-06-27 21:32:50,083 main.py:51] epoch 1761, training loss: 12318.53, average training loss: 13016.05, base loss: 19696.73
[INFO 2017-06-27 21:32:51,056 main.py:51] epoch 1762, training loss: 13075.41, average training loss: 13015.75, base loss: 19699.39
[INFO 2017-06-27 21:32:52,031 main.py:51] epoch 1763, training loss: 13346.55, average training loss: 13017.22, base loss: 19706.46
[INFO 2017-06-27 21:32:53,007 main.py:51] epoch 1764, training loss: 12047.74, average training loss: 13015.74, base loss: 19707.44
[INFO 2017-06-27 21:32:53,981 main.py:51] epoch 1765, training loss: 12225.62, average training loss: 13011.28, base loss: 19704.35
[INFO 2017-06-27 21:32:54,957 main.py:51] epoch 1766, training loss: 11786.45, average training loss: 13010.97, base loss: 19707.21
[INFO 2017-06-27 21:32:55,929 main.py:51] epoch 1767, training loss: 12234.20, average training loss: 13009.62, base loss: 19708.16
[INFO 2017-06-27 21:32:56,901 main.py:51] epoch 1768, training loss: 13113.18, average training loss: 13010.04, base loss: 19710.99
[INFO 2017-06-27 21:32:57,906 main.py:51] epoch 1769, training loss: 14136.26, average training loss: 13010.14, base loss: 19713.59
[INFO 2017-06-27 21:32:58,884 main.py:51] epoch 1770, training loss: 13205.32, average training loss: 13011.09, base loss: 19717.47
[INFO 2017-06-27 21:32:59,855 main.py:51] epoch 1771, training loss: 11235.18, average training loss: 13005.38, base loss: 19711.00
[INFO 2017-06-27 21:33:00,825 main.py:51] epoch 1772, training loss: 13353.54, average training loss: 13005.04, base loss: 19715.21
[INFO 2017-06-27 21:33:01,799 main.py:51] epoch 1773, training loss: 12317.95, average training loss: 13003.15, base loss: 19715.62
[INFO 2017-06-27 21:33:02,771 main.py:51] epoch 1774, training loss: 12605.29, average training loss: 13000.49, base loss: 19713.78
[INFO 2017-06-27 21:33:03,742 main.py:51] epoch 1775, training loss: 12043.83, average training loss: 12999.66, base loss: 19714.30
[INFO 2017-06-27 21:33:04,713 main.py:51] epoch 1776, training loss: 14001.99, average training loss: 12999.77, base loss: 19717.72
[INFO 2017-06-27 21:33:05,689 main.py:51] epoch 1777, training loss: 11674.09, average training loss: 12996.73, base loss: 19715.02
[INFO 2017-06-27 21:33:06,667 main.py:51] epoch 1778, training loss: 11540.30, average training loss: 12994.31, base loss: 19715.15
[INFO 2017-06-27 21:33:07,640 main.py:51] epoch 1779, training loss: 13273.07, average training loss: 12992.93, base loss: 19715.79
[INFO 2017-06-27 21:33:08,611 main.py:51] epoch 1780, training loss: 12126.59, average training loss: 12989.78, base loss: 19715.97
[INFO 2017-06-27 21:33:09,584 main.py:51] epoch 1781, training loss: 12309.03, average training loss: 12988.38, base loss: 19717.56
[INFO 2017-06-27 21:33:10,557 main.py:51] epoch 1782, training loss: 12246.07, average training loss: 12985.78, base loss: 19715.35
[INFO 2017-06-27 21:33:11,534 main.py:51] epoch 1783, training loss: 12407.17, average training loss: 12984.30, base loss: 19716.76
[INFO 2017-06-27 21:33:12,528 main.py:51] epoch 1784, training loss: 12508.51, average training loss: 12983.65, base loss: 19721.33
[INFO 2017-06-27 21:33:13,502 main.py:51] epoch 1785, training loss: 10918.44, average training loss: 12977.27, base loss: 19713.01
[INFO 2017-06-27 21:33:14,476 main.py:51] epoch 1786, training loss: 11600.79, average training loss: 12973.34, base loss: 19710.01
[INFO 2017-06-27 21:33:15,453 main.py:51] epoch 1787, training loss: 10223.27, average training loss: 12969.53, base loss: 19706.11
[INFO 2017-06-27 21:33:16,421 main.py:51] epoch 1788, training loss: 11109.39, average training loss: 12967.20, base loss: 19704.81
[INFO 2017-06-27 21:33:17,393 main.py:51] epoch 1789, training loss: 12665.83, average training loss: 12965.41, base loss: 19704.78
[INFO 2017-06-27 21:33:18,363 main.py:51] epoch 1790, training loss: 13429.29, average training loss: 12966.11, base loss: 19707.87
[INFO 2017-06-27 21:33:19,333 main.py:51] epoch 1791, training loss: 9397.59, average training loss: 12960.39, base loss: 19699.44
[INFO 2017-06-27 21:33:20,303 main.py:51] epoch 1792, training loss: 11619.45, average training loss: 12957.57, base loss: 19696.72
[INFO 2017-06-27 21:33:21,273 main.py:51] epoch 1793, training loss: 11852.19, average training loss: 12954.65, base loss: 19694.06
[INFO 2017-06-27 21:33:22,244 main.py:51] epoch 1794, training loss: 11666.45, average training loss: 12952.73, base loss: 19693.55
[INFO 2017-06-27 21:33:23,216 main.py:51] epoch 1795, training loss: 10991.04, average training loss: 12948.13, base loss: 19686.35
[INFO 2017-06-27 21:33:24,189 main.py:51] epoch 1796, training loss: 12869.06, average training loss: 12947.85, base loss: 19689.44
[INFO 2017-06-27 21:33:25,163 main.py:51] epoch 1797, training loss: 12798.19, average training loss: 12949.42, base loss: 19695.60
[INFO 2017-06-27 21:33:26,135 main.py:51] epoch 1798, training loss: 12854.29, average training loss: 12949.54, base loss: 19698.17
[INFO 2017-06-27 21:33:27,108 main.py:51] epoch 1799, training loss: 11004.30, average training loss: 12946.46, base loss: 19696.25
[INFO 2017-06-27 21:33:27,108 main.py:53] epoch 1799, testing
[INFO 2017-06-27 21:33:31,324 main.py:105] average testing loss: 12168.47, base loss: 19727.79
[INFO 2017-06-27 21:33:31,324 main.py:106] improve_loss: 7559.31, improve_percent: 0.38
[INFO 2017-06-27 21:33:31,325 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:33:31,338 main.py:76] current best improved percent: 0.38
[INFO 2017-06-27 21:33:32,311 main.py:51] epoch 1800, training loss: 12653.93, average training loss: 12947.41, base loss: 19701.73
[INFO 2017-06-27 21:33:33,285 main.py:51] epoch 1801, training loss: 11559.03, average training loss: 12943.93, base loss: 19698.82
[INFO 2017-06-27 21:33:34,257 main.py:51] epoch 1802, training loss: 13132.23, average training loss: 12940.58, base loss: 19697.75
[INFO 2017-06-27 21:33:35,231 main.py:51] epoch 1803, training loss: 13260.29, average training loss: 12941.81, base loss: 19702.21
[INFO 2017-06-27 21:33:36,206 main.py:51] epoch 1804, training loss: 11900.70, average training loss: 12938.11, base loss: 19700.59
[INFO 2017-06-27 21:33:37,176 main.py:51] epoch 1805, training loss: 11547.84, average training loss: 12934.78, base loss: 19698.19
[INFO 2017-06-27 21:33:38,153 main.py:51] epoch 1806, training loss: 11616.66, average training loss: 12930.45, base loss: 19695.32
[INFO 2017-06-27 21:33:39,124 main.py:51] epoch 1807, training loss: 12975.11, average training loss: 12929.81, base loss: 19698.41
[INFO 2017-06-27 21:33:40,099 main.py:51] epoch 1808, training loss: 12630.10, average training loss: 12928.84, base loss: 19698.07
[INFO 2017-06-27 21:33:41,071 main.py:51] epoch 1809, training loss: 11666.39, average training loss: 12925.85, base loss: 19697.75
[INFO 2017-06-27 21:33:42,041 main.py:51] epoch 1810, training loss: 14307.08, average training loss: 12927.51, base loss: 19703.84
[INFO 2017-06-27 21:33:43,012 main.py:51] epoch 1811, training loss: 12152.59, average training loss: 12926.05, base loss: 19703.25
[INFO 2017-06-27 21:33:43,986 main.py:51] epoch 1812, training loss: 12881.17, average training loss: 12924.45, base loss: 19704.72
[INFO 2017-06-27 21:33:44,959 main.py:51] epoch 1813, training loss: 10141.21, average training loss: 12919.98, base loss: 19698.58
[INFO 2017-06-27 21:33:45,933 main.py:51] epoch 1814, training loss: 11847.10, average training loss: 12916.50, base loss: 19696.63
[INFO 2017-06-27 21:33:46,906 main.py:51] epoch 1815, training loss: 12354.72, average training loss: 12914.41, base loss: 19696.75
[INFO 2017-06-27 21:33:47,879 main.py:51] epoch 1816, training loss: 11115.12, average training loss: 12914.75, base loss: 19699.32
[INFO 2017-06-27 21:33:48,856 main.py:51] epoch 1817, training loss: 10442.50, average training loss: 12910.20, base loss: 19693.96
[INFO 2017-06-27 21:33:49,826 main.py:51] epoch 1818, training loss: 11540.73, average training loss: 12908.81, base loss: 19694.14
[INFO 2017-06-27 21:33:50,803 main.py:51] epoch 1819, training loss: 11703.56, average training loss: 12907.04, base loss: 19691.84
[INFO 2017-06-27 21:33:51,778 main.py:51] epoch 1820, training loss: 11706.74, average training loss: 12905.68, base loss: 19690.95
[INFO 2017-06-27 21:33:52,750 main.py:51] epoch 1821, training loss: 12051.88, average training loss: 12904.01, base loss: 19691.21
[INFO 2017-06-27 21:33:53,723 main.py:51] epoch 1822, training loss: 11864.13, average training loss: 12903.21, base loss: 19693.72
[INFO 2017-06-27 21:33:54,700 main.py:51] epoch 1823, training loss: 11160.82, average training loss: 12899.69, base loss: 19691.42
[INFO 2017-06-27 21:33:55,678 main.py:51] epoch 1824, training loss: 11728.01, average training loss: 12898.83, base loss: 19693.18
[INFO 2017-06-27 21:33:56,654 main.py:51] epoch 1825, training loss: 11877.86, average training loss: 12896.64, base loss: 19691.90
[INFO 2017-06-27 21:33:57,626 main.py:51] epoch 1826, training loss: 13662.63, average training loss: 12895.16, base loss: 19693.45
[INFO 2017-06-27 21:33:58,599 main.py:51] epoch 1827, training loss: 11638.42, average training loss: 12894.03, base loss: 19693.24
[INFO 2017-06-27 21:33:59,577 main.py:51] epoch 1828, training loss: 12145.15, average training loss: 12894.02, base loss: 19696.32
[INFO 2017-06-27 21:34:00,556 main.py:51] epoch 1829, training loss: 11713.17, average training loss: 12891.09, base loss: 19693.43
[INFO 2017-06-27 21:34:01,531 main.py:51] epoch 1830, training loss: 11857.74, average training loss: 12890.20, base loss: 19692.06
[INFO 2017-06-27 21:34:02,511 main.py:51] epoch 1831, training loss: 11541.98, average training loss: 12888.34, base loss: 19691.33
[INFO 2017-06-27 21:34:03,487 main.py:51] epoch 1832, training loss: 13866.35, average training loss: 12890.52, base loss: 19696.00
[INFO 2017-06-27 21:34:04,462 main.py:51] epoch 1833, training loss: 12714.79, average training loss: 12888.15, base loss: 19694.04
[INFO 2017-06-27 21:34:05,434 main.py:51] epoch 1834, training loss: 13006.31, average training loss: 12886.61, base loss: 19696.19
[INFO 2017-06-27 21:34:06,405 main.py:51] epoch 1835, training loss: 13623.17, average training loss: 12885.64, base loss: 19697.24
[INFO 2017-06-27 21:34:07,378 main.py:51] epoch 1836, training loss: 12456.14, average training loss: 12884.36, base loss: 19696.79
[INFO 2017-06-27 21:34:08,353 main.py:51] epoch 1837, training loss: 12462.20, average training loss: 12883.01, base loss: 19698.11
[INFO 2017-06-27 21:34:09,324 main.py:51] epoch 1838, training loss: 10832.21, average training loss: 12880.64, base loss: 19695.19
[INFO 2017-06-27 21:34:10,300 main.py:51] epoch 1839, training loss: 12140.35, average training loss: 12879.09, base loss: 19694.58
[INFO 2017-06-27 21:34:11,279 main.py:51] epoch 1840, training loss: 12371.79, average training loss: 12879.71, base loss: 19699.40
[INFO 2017-06-27 21:34:12,253 main.py:51] epoch 1841, training loss: 11834.08, average training loss: 12878.69, base loss: 19699.78
[INFO 2017-06-27 21:34:13,229 main.py:51] epoch 1842, training loss: 12657.22, average training loss: 12876.51, base loss: 19698.16
[INFO 2017-06-27 21:34:14,201 main.py:51] epoch 1843, training loss: 11576.00, average training loss: 12873.82, base loss: 19697.36
[INFO 2017-06-27 21:34:15,171 main.py:51] epoch 1844, training loss: 12220.83, average training loss: 12871.53, base loss: 19696.24
[INFO 2017-06-27 21:34:16,144 main.py:51] epoch 1845, training loss: 13934.41, average training loss: 12870.38, base loss: 19695.45
[INFO 2017-06-27 21:34:17,118 main.py:51] epoch 1846, training loss: 11661.64, average training loss: 12868.67, base loss: 19695.02
[INFO 2017-06-27 21:34:18,088 main.py:51] epoch 1847, training loss: 12092.95, average training loss: 12866.39, base loss: 19693.93
[INFO 2017-06-27 21:34:19,060 main.py:51] epoch 1848, training loss: 11961.90, average training loss: 12864.71, base loss: 19692.71
[INFO 2017-06-27 21:34:20,032 main.py:51] epoch 1849, training loss: 11384.26, average training loss: 12863.95, base loss: 19692.66
[INFO 2017-06-27 21:34:21,007 main.py:51] epoch 1850, training loss: 12913.84, average training loss: 12865.56, base loss: 19697.86
[INFO 2017-06-27 21:34:21,984 main.py:51] epoch 1851, training loss: 10838.80, average training loss: 12862.17, base loss: 19693.14
[INFO 2017-06-27 21:34:22,959 main.py:51] epoch 1852, training loss: 11125.75, average training loss: 12859.01, base loss: 19690.56
[INFO 2017-06-27 21:34:23,930 main.py:51] epoch 1853, training loss: 13003.37, average training loss: 12859.79, base loss: 19694.35
[INFO 2017-06-27 21:34:24,901 main.py:51] epoch 1854, training loss: 11136.21, average training loss: 12855.88, base loss: 19690.83
[INFO 2017-06-27 21:34:25,876 main.py:51] epoch 1855, training loss: 12451.35, average training loss: 12853.63, base loss: 19689.68
[INFO 2017-06-27 21:34:26,852 main.py:51] epoch 1856, training loss: 13785.55, average training loss: 12854.19, base loss: 19693.04
[INFO 2017-06-27 21:34:27,821 main.py:51] epoch 1857, training loss: 12696.20, average training loss: 12853.81, base loss: 19694.75
[INFO 2017-06-27 21:34:28,792 main.py:51] epoch 1858, training loss: 12580.48, average training loss: 12853.96, base loss: 19696.13
[INFO 2017-06-27 21:34:29,762 main.py:51] epoch 1859, training loss: 12409.51, average training loss: 12851.82, base loss: 19695.30
[INFO 2017-06-27 21:34:30,738 main.py:51] epoch 1860, training loss: 13365.89, average training loss: 12852.04, base loss: 19699.45
[INFO 2017-06-27 21:34:31,709 main.py:51] epoch 1861, training loss: 11670.62, average training loss: 12848.63, base loss: 19695.41
[INFO 2017-06-27 21:34:32,681 main.py:51] epoch 1862, training loss: 11560.70, average training loss: 12848.06, base loss: 19696.37
[INFO 2017-06-27 21:34:33,655 main.py:51] epoch 1863, training loss: 12558.23, average training loss: 12846.48, base loss: 19696.77
[INFO 2017-06-27 21:34:34,623 main.py:51] epoch 1864, training loss: 11919.71, average training loss: 12844.35, base loss: 19694.70
[INFO 2017-06-27 21:34:35,597 main.py:51] epoch 1865, training loss: 12057.66, average training loss: 12843.85, base loss: 19697.26
[INFO 2017-06-27 21:34:36,568 main.py:51] epoch 1866, training loss: 11812.78, average training loss: 12841.71, base loss: 19696.28
[INFO 2017-06-27 21:34:37,542 main.py:51] epoch 1867, training loss: 13142.42, average training loss: 12839.60, base loss: 19696.60
[INFO 2017-06-27 21:34:38,514 main.py:51] epoch 1868, training loss: 11710.21, average training loss: 12835.98, base loss: 19691.46
[INFO 2017-06-27 21:34:39,489 main.py:51] epoch 1869, training loss: 13857.33, average training loss: 12836.26, base loss: 19694.75
[INFO 2017-06-27 21:34:40,461 main.py:51] epoch 1870, training loss: 11801.88, average training loss: 12835.01, base loss: 19693.95
[INFO 2017-06-27 21:34:41,431 main.py:51] epoch 1871, training loss: 11412.71, average training loss: 12831.82, base loss: 19691.38
[INFO 2017-06-27 21:34:42,402 main.py:51] epoch 1872, training loss: 11817.20, average training loss: 12831.01, base loss: 19691.72
[INFO 2017-06-27 21:34:43,372 main.py:51] epoch 1873, training loss: 13423.01, average training loss: 12829.48, base loss: 19692.71
[INFO 2017-06-27 21:34:44,345 main.py:51] epoch 1874, training loss: 13131.39, average training loss: 12828.40, base loss: 19694.65
[INFO 2017-06-27 21:34:45,316 main.py:51] epoch 1875, training loss: 12264.95, average training loss: 12827.61, base loss: 19695.09
[INFO 2017-06-27 21:34:46,289 main.py:51] epoch 1876, training loss: 12956.97, average training loss: 12823.87, base loss: 19691.16
[INFO 2017-06-27 21:34:47,262 main.py:51] epoch 1877, training loss: 12311.54, average training loss: 12821.20, base loss: 19688.40
[INFO 2017-06-27 21:34:48,232 main.py:51] epoch 1878, training loss: 14652.13, average training loss: 12822.71, base loss: 19694.79
[INFO 2017-06-27 21:34:49,202 main.py:51] epoch 1879, training loss: 13963.66, average training loss: 12823.79, base loss: 19701.31
[INFO 2017-06-27 21:34:50,173 main.py:51] epoch 1880, training loss: 10832.24, average training loss: 12821.28, base loss: 19698.74
[INFO 2017-06-27 21:34:51,144 main.py:51] epoch 1881, training loss: 12788.56, average training loss: 12819.59, base loss: 19698.13
[INFO 2017-06-27 21:34:52,115 main.py:51] epoch 1882, training loss: 15653.59, average training loss: 12822.02, base loss: 19705.10
[INFO 2017-06-27 21:34:53,091 main.py:51] epoch 1883, training loss: 13032.71, average training loss: 12820.32, base loss: 19705.77
[INFO 2017-06-27 21:34:54,066 main.py:51] epoch 1884, training loss: 11074.63, average training loss: 12819.60, base loss: 19706.29
[INFO 2017-06-27 21:34:55,037 main.py:51] epoch 1885, training loss: 12450.05, average training loss: 12817.48, base loss: 19704.79
[INFO 2017-06-27 21:34:56,008 main.py:51] epoch 1886, training loss: 12513.76, average training loss: 12818.25, base loss: 19709.87
[INFO 2017-06-27 21:34:56,985 main.py:51] epoch 1887, training loss: 11846.50, average training loss: 12817.16, base loss: 19710.98
[INFO 2017-06-27 21:34:57,954 main.py:51] epoch 1888, training loss: 13615.89, average training loss: 12817.11, base loss: 19712.28
[INFO 2017-06-27 21:34:58,924 main.py:51] epoch 1889, training loss: 10471.23, average training loss: 12813.91, base loss: 19708.55
[INFO 2017-06-27 21:34:59,897 main.py:51] epoch 1890, training loss: 11929.65, average training loss: 12812.93, base loss: 19708.16
[INFO 2017-06-27 21:35:00,870 main.py:51] epoch 1891, training loss: 11309.57, average training loss: 12811.15, base loss: 19707.67
[INFO 2017-06-27 21:35:01,843 main.py:51] epoch 1892, training loss: 11271.07, average training loss: 12809.78, base loss: 19706.87
[INFO 2017-06-27 21:35:02,821 main.py:51] epoch 1893, training loss: 11880.52, average training loss: 12808.72, base loss: 19708.02
[INFO 2017-06-27 21:35:03,793 main.py:51] epoch 1894, training loss: 11666.66, average training loss: 12805.97, base loss: 19706.64
[INFO 2017-06-27 21:35:04,773 main.py:51] epoch 1895, training loss: 12983.61, average training loss: 12804.98, base loss: 19706.55
[INFO 2017-06-27 21:35:05,754 main.py:51] epoch 1896, training loss: 10613.72, average training loss: 12801.98, base loss: 19703.78
[INFO 2017-06-27 21:35:06,738 main.py:51] epoch 1897, training loss: 12927.89, average training loss: 12799.39, base loss: 19702.46
[INFO 2017-06-27 21:35:07,725 main.py:51] epoch 1898, training loss: 11399.28, average training loss: 12796.46, base loss: 19698.76
[INFO 2017-06-27 21:35:08,704 main.py:51] epoch 1899, training loss: 11006.02, average training loss: 12792.46, base loss: 19694.51
[INFO 2017-06-27 21:35:08,704 main.py:53] epoch 1899, testing
[INFO 2017-06-27 21:35:13,414 main.py:105] average testing loss: 12057.07, base loss: 19543.74
[INFO 2017-06-27 21:35:13,414 main.py:106] improve_loss: 7486.68, improve_percent: 0.38
[INFO 2017-06-27 21:35:13,415 main.py:76] current best improved percent: 0.38
[INFO 2017-06-27 21:35:14,396 main.py:51] epoch 1900, training loss: 14632.67, average training loss: 12792.62, base loss: 19697.08
[INFO 2017-06-27 21:35:15,372 main.py:51] epoch 1901, training loss: 11234.40, average training loss: 12788.23, base loss: 19692.08
[INFO 2017-06-27 21:35:16,345 main.py:51] epoch 1902, training loss: 12661.68, average training loss: 12786.36, base loss: 19692.73
[INFO 2017-06-27 21:35:17,316 main.py:51] epoch 1903, training loss: 11454.70, average training loss: 12783.28, base loss: 19691.39
[INFO 2017-06-27 21:35:18,286 main.py:51] epoch 1904, training loss: 12507.29, average training loss: 12782.06, base loss: 19690.31
[INFO 2017-06-27 21:35:19,260 main.py:51] epoch 1905, training loss: 11183.18, average training loss: 12778.11, base loss: 19686.98
[INFO 2017-06-27 21:35:20,227 main.py:51] epoch 1906, training loss: 10887.00, average training loss: 12776.62, base loss: 19687.79
[INFO 2017-06-27 21:35:21,201 main.py:51] epoch 1907, training loss: 13262.04, average training loss: 12777.17, base loss: 19692.05
[INFO 2017-06-27 21:35:22,176 main.py:51] epoch 1908, training loss: 11014.87, average training loss: 12776.09, base loss: 19692.63
[INFO 2017-06-27 21:35:23,155 main.py:51] epoch 1909, training loss: 11807.46, average training loss: 12772.78, base loss: 19688.52
[INFO 2017-06-27 21:35:24,128 main.py:51] epoch 1910, training loss: 12055.38, average training loss: 12771.58, base loss: 19689.09
[INFO 2017-06-27 21:35:25,098 main.py:51] epoch 1911, training loss: 11378.53, average training loss: 12771.34, base loss: 19689.97
[INFO 2017-06-27 21:35:26,073 main.py:51] epoch 1912, training loss: 12466.71, average training loss: 12770.03, base loss: 19688.95
[INFO 2017-06-27 21:35:27,043 main.py:51] epoch 1913, training loss: 11190.52, average training loss: 12767.81, base loss: 19687.76
[INFO 2017-06-27 21:35:28,013 main.py:51] epoch 1914, training loss: 12054.60, average training loss: 12766.26, base loss: 19689.58
[INFO 2017-06-27 21:35:28,985 main.py:51] epoch 1915, training loss: 11781.93, average training loss: 12763.15, base loss: 19688.45
[INFO 2017-06-27 21:35:29,956 main.py:51] epoch 1916, training loss: 12129.50, average training loss: 12763.21, base loss: 19691.04
[INFO 2017-06-27 21:35:30,928 main.py:51] epoch 1917, training loss: 12725.32, average training loss: 12761.53, base loss: 19690.73
[INFO 2017-06-27 21:35:31,902 main.py:51] epoch 1918, training loss: 12745.23, average training loss: 12761.92, base loss: 19693.79
[INFO 2017-06-27 21:35:32,870 main.py:51] epoch 1919, training loss: 13277.29, average training loss: 12763.46, base loss: 19699.04
[INFO 2017-06-27 21:35:33,841 main.py:51] epoch 1920, training loss: 11645.46, average training loss: 12760.60, base loss: 19696.97
[INFO 2017-06-27 21:35:34,813 main.py:51] epoch 1921, training loss: 11785.13, average training loss: 12757.10, base loss: 19692.80
[INFO 2017-06-27 21:35:35,782 main.py:51] epoch 1922, training loss: 12902.40, average training loss: 12755.64, base loss: 19693.66
[INFO 2017-06-27 21:35:36,754 main.py:51] epoch 1923, training loss: 10778.78, average training loss: 12752.92, base loss: 19690.95
[INFO 2017-06-27 21:35:37,732 main.py:51] epoch 1924, training loss: 11754.70, average training loss: 12752.47, base loss: 19692.46
[INFO 2017-06-27 21:35:38,706 main.py:51] epoch 1925, training loss: 11438.46, average training loss: 12749.94, base loss: 19689.41
[INFO 2017-06-27 21:35:39,676 main.py:51] epoch 1926, training loss: 10966.59, average training loss: 12747.04, base loss: 19686.57
[INFO 2017-06-27 21:35:40,647 main.py:51] epoch 1927, training loss: 11775.27, average training loss: 12744.40, base loss: 19682.83
[INFO 2017-06-27 21:35:41,616 main.py:51] epoch 1928, training loss: 11595.02, average training loss: 12743.92, base loss: 19683.61
[INFO 2017-06-27 21:35:42,593 main.py:51] epoch 1929, training loss: 11907.11, average training loss: 12742.32, base loss: 19682.87
[INFO 2017-06-27 21:35:43,680 main.py:51] epoch 1930, training loss: 12492.67, average training loss: 12740.27, base loss: 19683.38
[INFO 2017-06-27 21:35:44,657 main.py:51] epoch 1931, training loss: 12673.43, average training loss: 12739.73, base loss: 19685.51
[INFO 2017-06-27 21:35:45,681 main.py:51] epoch 1932, training loss: 11793.68, average training loss: 12736.22, base loss: 19682.72
[INFO 2017-06-27 21:35:46,676 main.py:51] epoch 1933, training loss: 10688.09, average training loss: 12733.61, base loss: 19681.18
[INFO 2017-06-27 21:35:47,790 main.py:51] epoch 1934, training loss: 12879.61, average training loss: 12734.24, base loss: 19683.14
[INFO 2017-06-27 21:35:48,805 main.py:51] epoch 1935, training loss: 14677.82, average training loss: 12734.12, base loss: 19683.64
[INFO 2017-06-27 21:35:49,904 main.py:51] epoch 1936, training loss: 11096.38, average training loss: 12732.12, base loss: 19681.64
[INFO 2017-06-27 21:35:51,014 main.py:51] epoch 1937, training loss: 12198.80, average training loss: 12731.68, base loss: 19682.57
[INFO 2017-06-27 21:35:52,034 main.py:51] epoch 1938, training loss: 12275.31, average training loss: 12729.99, base loss: 19680.65
[INFO 2017-06-27 21:35:53,072 main.py:51] epoch 1939, training loss: 10801.33, average training loss: 12726.72, base loss: 19676.18
[INFO 2017-06-27 21:35:54,175 main.py:51] epoch 1940, training loss: 10895.46, average training loss: 12722.11, base loss: 19671.65
[INFO 2017-06-27 21:35:55,168 main.py:51] epoch 1941, training loss: 12330.93, average training loss: 12719.78, base loss: 19669.93
[INFO 2017-06-27 21:35:56,313 main.py:51] epoch 1942, training loss: 11832.89, average training loss: 12717.87, base loss: 19668.43
[INFO 2017-06-27 21:35:57,316 main.py:51] epoch 1943, training loss: 12824.82, average training loss: 12718.00, base loss: 19670.87
[INFO 2017-06-27 21:35:58,292 main.py:51] epoch 1944, training loss: 12903.79, average training loss: 12715.39, base loss: 19667.11
[INFO 2017-06-27 21:35:59,262 main.py:51] epoch 1945, training loss: 12772.50, average training loss: 12715.59, base loss: 19670.26
[INFO 2017-06-27 21:36:00,238 main.py:51] epoch 1946, training loss: 13235.07, average training loss: 12714.11, base loss: 19672.16
[INFO 2017-06-27 21:36:01,209 main.py:51] epoch 1947, training loss: 11273.70, average training loss: 12713.20, base loss: 19671.84
[INFO 2017-06-27 21:36:02,180 main.py:51] epoch 1948, training loss: 11457.28, average training loss: 12711.46, base loss: 19670.98
[INFO 2017-06-27 21:36:03,152 main.py:51] epoch 1949, training loss: 12742.83, average training loss: 12711.87, base loss: 19675.48
[INFO 2017-06-27 21:36:04,292 main.py:51] epoch 1950, training loss: 14212.36, average training loss: 12711.73, base loss: 19679.25
[INFO 2017-06-27 21:36:05,298 main.py:51] epoch 1951, training loss: 13268.54, average training loss: 12711.49, base loss: 19682.35
[INFO 2017-06-27 21:36:06,404 main.py:51] epoch 1952, training loss: 11455.26, average training loss: 12708.34, base loss: 19679.39
[INFO 2017-06-27 21:36:07,424 main.py:51] epoch 1953, training loss: 12503.22, average training loss: 12708.33, base loss: 19682.32
[INFO 2017-06-27 21:36:08,492 main.py:51] epoch 1954, training loss: 12617.55, average training loss: 12707.27, base loss: 19683.83
[INFO 2017-06-27 21:36:09,602 main.py:51] epoch 1955, training loss: 13192.00, average training loss: 12708.16, base loss: 19688.28
[INFO 2017-06-27 21:36:10,596 main.py:51] epoch 1956, training loss: 12219.65, average training loss: 12707.56, base loss: 19689.87
[INFO 2017-06-27 21:36:11,637 main.py:51] epoch 1957, training loss: 11741.58, average training loss: 12704.87, base loss: 19688.25
[INFO 2017-06-27 21:36:12,643 main.py:51] epoch 1958, training loss: 12665.38, average training loss: 12703.28, base loss: 19688.94
[INFO 2017-06-27 21:36:13,759 main.py:51] epoch 1959, training loss: 11513.34, average training loss: 12700.75, base loss: 19687.62
[INFO 2017-06-27 21:36:14,876 main.py:51] epoch 1960, training loss: 12441.73, average training loss: 12698.83, base loss: 19687.61
[INFO 2017-06-27 21:36:15,892 main.py:51] epoch 1961, training loss: 12094.55, average training loss: 12698.64, base loss: 19690.66
[INFO 2017-06-27 21:36:16,959 main.py:51] epoch 1962, training loss: 12781.02, average training loss: 12698.31, base loss: 19693.01
[INFO 2017-06-27 21:36:18,081 main.py:51] epoch 1963, training loss: 13119.78, average training loss: 12698.52, base loss: 19696.50
[INFO 2017-06-27 21:36:19,109 main.py:51] epoch 1964, training loss: 10057.05, average training loss: 12695.65, base loss: 19694.45
[INFO 2017-06-27 21:36:20,124 main.py:51] epoch 1965, training loss: 11734.56, average training loss: 12695.14, base loss: 19697.19
[INFO 2017-06-27 21:36:21,261 main.py:51] epoch 1966, training loss: 12621.24, average training loss: 12693.22, base loss: 19696.20
[INFO 2017-06-27 21:36:22,242 main.py:51] epoch 1967, training loss: 11649.45, average training loss: 12693.26, base loss: 19699.28
[INFO 2017-06-27 21:36:23,211 main.py:51] epoch 1968, training loss: 14839.21, average training loss: 12695.20, base loss: 19705.34
[INFO 2017-06-27 21:36:24,181 main.py:51] epoch 1969, training loss: 12288.26, average training loss: 12695.91, base loss: 19709.18
[INFO 2017-06-27 21:36:25,153 main.py:51] epoch 1970, training loss: 12225.02, average training loss: 12693.21, base loss: 19706.65
[INFO 2017-06-27 21:36:26,124 main.py:51] epoch 1971, training loss: 10776.21, average training loss: 12689.43, base loss: 19701.56
[INFO 2017-06-27 21:36:27,194 main.py:51] epoch 1972, training loss: 11338.08, average training loss: 12687.64, base loss: 19701.04
[INFO 2017-06-27 21:36:28,242 main.py:51] epoch 1973, training loss: 12917.70, average training loss: 12686.41, base loss: 19701.43
[INFO 2017-06-27 21:36:29,214 main.py:51] epoch 1974, training loss: 12131.48, average training loss: 12685.13, base loss: 19700.67
[INFO 2017-06-27 21:36:30,189 main.py:51] epoch 1975, training loss: 11091.00, average training loss: 12682.82, base loss: 19699.28
[INFO 2017-06-27 21:36:31,276 main.py:51] epoch 1976, training loss: 11815.22, average training loss: 12679.67, base loss: 19696.15
[INFO 2017-06-27 21:36:32,262 main.py:51] epoch 1977, training loss: 12015.85, average training loss: 12678.33, base loss: 19698.44
[INFO 2017-06-27 21:36:33,387 main.py:51] epoch 1978, training loss: 12225.41, average training loss: 12676.47, base loss: 19697.05
[INFO 2017-06-27 21:36:34,447 main.py:51] epoch 1979, training loss: 12331.87, average training loss: 12676.04, base loss: 19696.88
[INFO 2017-06-27 21:36:35,476 main.py:51] epoch 1980, training loss: 11005.32, average training loss: 12673.32, base loss: 19694.56
[INFO 2017-06-27 21:36:36,588 main.py:51] epoch 1981, training loss: 11298.71, average training loss: 12672.60, base loss: 19695.96
[INFO 2017-06-27 21:36:37,698 main.py:51] epoch 1982, training loss: 12418.49, average training loss: 12673.32, base loss: 19700.37
[INFO 2017-06-27 21:36:38,683 main.py:51] epoch 1983, training loss: 12884.43, average training loss: 12671.30, base loss: 19698.47
[INFO 2017-06-27 21:36:39,655 main.py:51] epoch 1984, training loss: 11077.24, average training loss: 12667.86, base loss: 19695.83
[INFO 2017-06-27 21:36:40,633 main.py:51] epoch 1985, training loss: 11729.20, average training loss: 12666.72, base loss: 19696.91
[INFO 2017-06-27 21:36:41,604 main.py:51] epoch 1986, training loss: 12396.88, average training loss: 12665.74, base loss: 19697.13
[INFO 2017-06-27 21:36:42,626 main.py:51] epoch 1987, training loss: 12718.46, average training loss: 12664.73, base loss: 19697.52
[INFO 2017-06-27 21:36:43,625 main.py:51] epoch 1988, training loss: 12125.97, average training loss: 12663.81, base loss: 19698.01
[INFO 2017-06-27 21:36:44,600 main.py:51] epoch 1989, training loss: 11653.82, average training loss: 12662.89, base loss: 19698.40
[INFO 2017-06-27 21:36:45,643 main.py:51] epoch 1990, training loss: 10992.89, average training loss: 12659.85, base loss: 19694.72
[INFO 2017-06-27 21:36:46,640 main.py:51] epoch 1991, training loss: 12899.02, average training loss: 12659.08, base loss: 19695.21
[INFO 2017-06-27 21:36:47,721 main.py:51] epoch 1992, training loss: 11188.84, average training loss: 12656.84, base loss: 19693.53
[INFO 2017-06-27 21:36:48,789 main.py:51] epoch 1993, training loss: 11168.25, average training loss: 12654.73, base loss: 19692.59
[INFO 2017-06-27 21:36:49,782 main.py:51] epoch 1994, training loss: 11764.34, average training loss: 12652.75, base loss: 19692.42
[INFO 2017-06-27 21:36:50,863 main.py:51] epoch 1995, training loss: 14195.18, average training loss: 12653.71, base loss: 19698.45
[INFO 2017-06-27 21:36:51,857 main.py:51] epoch 1996, training loss: 12529.53, average training loss: 12654.11, base loss: 19701.73
[INFO 2017-06-27 21:36:52,827 main.py:51] epoch 1997, training loss: 13085.00, average training loss: 12655.03, base loss: 19705.94
[INFO 2017-06-27 21:36:53,797 main.py:51] epoch 1998, training loss: 11713.63, average training loss: 12650.80, base loss: 19702.27
[INFO 2017-06-27 21:36:54,899 main.py:51] epoch 1999, training loss: 11663.03, average training loss: 12649.19, base loss: 19701.27
[INFO 2017-06-27 21:36:54,899 main.py:53] epoch 1999, testing
[INFO 2017-06-27 21:36:59,143 main.py:105] average testing loss: 12147.01, base loss: 19969.50
[INFO 2017-06-27 21:36:59,143 main.py:106] improve_loss: 7822.49, improve_percent: 0.39
[INFO 2017-06-27 21:36:59,144 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:36:59,157 main.py:76] current best improved percent: 0.39
[INFO 2017-06-27 21:37:00,246 main.py:51] epoch 2000, training loss: 11592.15, average training loss: 12646.40, base loss: 19698.71
[INFO 2017-06-27 21:37:01,263 main.py:51] epoch 2001, training loss: 11614.07, average training loss: 12643.14, base loss: 19693.77
[INFO 2017-06-27 21:37:02,238 main.py:51] epoch 2002, training loss: 10997.04, average training loss: 12641.62, base loss: 19692.65
[INFO 2017-06-27 21:37:03,209 main.py:51] epoch 2003, training loss: 12502.84, average training loss: 12638.55, base loss: 19689.82
[INFO 2017-06-27 21:37:04,291 main.py:51] epoch 2004, training loss: 9708.65, average training loss: 12634.00, base loss: 19684.43
[INFO 2017-06-27 21:37:05,279 main.py:51] epoch 2005, training loss: 11591.24, average training loss: 12632.78, base loss: 19685.58
[INFO 2017-06-27 21:37:06,363 main.py:51] epoch 2006, training loss: 11594.36, average training loss: 12631.05, base loss: 19684.75
[INFO 2017-06-27 21:37:07,352 main.py:51] epoch 2007, training loss: 11639.26, average training loss: 12628.38, base loss: 19681.73
[INFO 2017-06-27 21:37:08,331 main.py:51] epoch 2008, training loss: 13242.91, average training loss: 12627.17, base loss: 19681.07
[INFO 2017-06-27 21:37:09,358 main.py:51] epoch 2009, training loss: 10923.93, average training loss: 12624.14, base loss: 19677.47
[INFO 2017-06-27 21:37:10,376 main.py:51] epoch 2010, training loss: 11181.78, average training loss: 12619.88, base loss: 19672.50
[INFO 2017-06-27 21:37:11,354 main.py:51] epoch 2011, training loss: 11699.70, average training loss: 12618.01, base loss: 19670.84
[INFO 2017-06-27 21:37:12,475 main.py:51] epoch 2012, training loss: 10947.42, average training loss: 12614.73, base loss: 19666.96
[INFO 2017-06-27 21:37:13,468 main.py:51] epoch 2013, training loss: 12864.07, average training loss: 12613.67, base loss: 19666.71
[INFO 2017-06-27 21:37:14,444 main.py:51] epoch 2014, training loss: 10658.62, average training loss: 12611.89, base loss: 19664.46
[INFO 2017-06-27 21:37:15,417 main.py:51] epoch 2015, training loss: 11207.95, average training loss: 12610.13, base loss: 19664.64
[INFO 2017-06-27 21:37:16,392 main.py:51] epoch 2016, training loss: 12677.70, average training loss: 12609.71, base loss: 19666.38
[INFO 2017-06-27 21:37:17,366 main.py:51] epoch 2017, training loss: 13134.20, average training loss: 12609.24, base loss: 19667.97
[INFO 2017-06-27 21:37:18,354 main.py:51] epoch 2018, training loss: 12398.00, average training loss: 12608.08, base loss: 19668.99
[INFO 2017-06-27 21:37:19,330 main.py:51] epoch 2019, training loss: 12408.61, average training loss: 12607.75, base loss: 19669.84
[INFO 2017-06-27 21:37:20,305 main.py:51] epoch 2020, training loss: 12701.54, average training loss: 12607.98, base loss: 19672.71
[INFO 2017-06-27 21:37:21,278 main.py:51] epoch 2021, training loss: 11944.74, average training loss: 12607.73, base loss: 19673.29
[INFO 2017-06-27 21:37:22,256 main.py:51] epoch 2022, training loss: 12371.62, average training loss: 12607.13, base loss: 19672.33
[INFO 2017-06-27 21:37:23,230 main.py:51] epoch 2023, training loss: 13567.26, average training loss: 12606.55, base loss: 19673.37
[INFO 2017-06-27 21:37:24,205 main.py:51] epoch 2024, training loss: 11469.78, average training loss: 12605.12, base loss: 19673.77
[INFO 2017-06-27 21:37:25,179 main.py:51] epoch 2025, training loss: 11704.25, average training loss: 12601.13, base loss: 19669.60
[INFO 2017-06-27 21:37:26,157 main.py:51] epoch 2026, training loss: 12299.56, average training loss: 12601.16, base loss: 19672.11
[INFO 2017-06-27 21:37:27,130 main.py:51] epoch 2027, training loss: 11734.47, average training loss: 12599.89, base loss: 19672.94
[INFO 2017-06-27 21:37:28,102 main.py:51] epoch 2028, training loss: 13029.28, average training loss: 12598.21, base loss: 19671.92
[INFO 2017-06-27 21:37:29,077 main.py:51] epoch 2029, training loss: 11708.18, average training loss: 12596.73, base loss: 19670.81
[INFO 2017-06-27 21:37:30,050 main.py:51] epoch 2030, training loss: 13732.51, average training loss: 12597.20, base loss: 19675.13
[INFO 2017-06-27 21:37:31,027 main.py:51] epoch 2031, training loss: 11729.63, average training loss: 12595.56, base loss: 19674.76
[INFO 2017-06-27 21:37:32,001 main.py:51] epoch 2032, training loss: 11344.55, average training loss: 12592.15, base loss: 19671.24
[INFO 2017-06-27 21:37:33,008 main.py:51] epoch 2033, training loss: 13454.44, average training loss: 12593.96, base loss: 19678.43
[INFO 2017-06-27 21:37:34,023 main.py:51] epoch 2034, training loss: 11289.47, average training loss: 12593.67, base loss: 19681.85
[INFO 2017-06-27 21:37:35,067 main.py:51] epoch 2035, training loss: 12595.10, average training loss: 12593.14, base loss: 19684.44
[INFO 2017-06-27 21:37:36,090 main.py:51] epoch 2036, training loss: 13196.67, average training loss: 12592.04, base loss: 19685.69
[INFO 2017-06-27 21:37:37,181 main.py:51] epoch 2037, training loss: 12188.32, average training loss: 12590.45, base loss: 19686.61
[INFO 2017-06-27 21:37:38,199 main.py:51] epoch 2038, training loss: 12517.28, average training loss: 12588.02, base loss: 19685.43
[INFO 2017-06-27 21:37:39,311 main.py:51] epoch 2039, training loss: 11828.12, average training loss: 12586.47, base loss: 19684.57
[INFO 2017-06-27 21:37:40,292 main.py:51] epoch 2040, training loss: 9990.63, average training loss: 12584.47, base loss: 19681.66
[INFO 2017-06-27 21:37:41,379 main.py:51] epoch 2041, training loss: 11587.90, average training loss: 12583.70, base loss: 19685.01
[INFO 2017-06-27 21:37:42,458 main.py:51] epoch 2042, training loss: 12960.45, average training loss: 12583.25, base loss: 19685.27
[INFO 2017-06-27 21:37:43,523 main.py:51] epoch 2043, training loss: 11217.56, average training loss: 12581.38, base loss: 19683.52
[INFO 2017-06-27 21:37:44,513 main.py:51] epoch 2044, training loss: 12508.17, average training loss: 12578.94, base loss: 19681.86
[INFO 2017-06-27 21:37:45,483 main.py:51] epoch 2045, training loss: 11823.00, average training loss: 12576.20, base loss: 19679.82
[INFO 2017-06-27 21:37:46,456 main.py:51] epoch 2046, training loss: 12615.61, average training loss: 12575.19, base loss: 19680.42
[INFO 2017-06-27 21:37:47,427 main.py:51] epoch 2047, training loss: 11534.20, average training loss: 12572.60, base loss: 19678.52
[INFO 2017-06-27 21:37:48,398 main.py:51] epoch 2048, training loss: 11642.95, average training loss: 12569.15, base loss: 19675.05
[INFO 2017-06-27 21:37:49,372 main.py:51] epoch 2049, training loss: 11847.25, average training loss: 12568.20, base loss: 19675.56
[INFO 2017-06-27 21:37:50,351 main.py:51] epoch 2050, training loss: 13222.09, average training loss: 12567.72, base loss: 19677.41
[INFO 2017-06-27 21:37:51,322 main.py:51] epoch 2051, training loss: 11439.53, average training loss: 12565.69, base loss: 19675.98
[INFO 2017-06-27 21:37:52,294 main.py:51] epoch 2052, training loss: 11698.78, average training loss: 12565.23, base loss: 19677.53
[INFO 2017-06-27 21:37:53,267 main.py:51] epoch 2053, training loss: 12617.57, average training loss: 12565.17, base loss: 19680.32
[INFO 2017-06-27 21:37:54,242 main.py:51] epoch 2054, training loss: 11421.97, average training loss: 12562.42, base loss: 19677.78
[INFO 2017-06-27 21:37:55,221 main.py:51] epoch 2055, training loss: 11649.11, average training loss: 12560.77, base loss: 19677.51
[INFO 2017-06-27 21:37:56,193 main.py:51] epoch 2056, training loss: 10412.45, average training loss: 12559.19, base loss: 19675.77
[INFO 2017-06-27 21:37:57,163 main.py:51] epoch 2057, training loss: 11396.55, average training loss: 12556.23, base loss: 19672.92
[INFO 2017-06-27 21:37:58,134 main.py:51] epoch 2058, training loss: 11602.78, average training loss: 12553.02, base loss: 19670.57
[INFO 2017-06-27 21:37:59,105 main.py:51] epoch 2059, training loss: 12408.87, average training loss: 12548.41, base loss: 19665.67
[INFO 2017-06-27 21:38:00,079 main.py:51] epoch 2060, training loss: 12390.26, average training loss: 12546.05, base loss: 19664.30
[INFO 2017-06-27 21:38:01,050 main.py:51] epoch 2061, training loss: 12463.15, average training loss: 12543.46, base loss: 19661.52
[INFO 2017-06-27 21:38:02,025 main.py:51] epoch 2062, training loss: 10394.34, average training loss: 12540.85, base loss: 19657.85
[INFO 2017-06-27 21:38:02,999 main.py:51] epoch 2063, training loss: 12156.76, average training loss: 12540.71, base loss: 19658.96
[INFO 2017-06-27 21:38:03,968 main.py:51] epoch 2064, training loss: 11418.18, average training loss: 12539.12, base loss: 19659.73
[INFO 2017-06-27 21:38:04,943 main.py:51] epoch 2065, training loss: 11097.03, average training loss: 12535.53, base loss: 19656.65
[INFO 2017-06-27 21:38:05,914 main.py:51] epoch 2066, training loss: 13385.13, average training loss: 12531.95, base loss: 19654.24
[INFO 2017-06-27 21:38:06,890 main.py:51] epoch 2067, training loss: 12364.69, average training loss: 12530.99, base loss: 19656.47
[INFO 2017-06-27 21:38:07,960 main.py:51] epoch 2068, training loss: 13010.47, average training loss: 12530.66, base loss: 19659.05
[INFO 2017-06-27 21:38:08,957 main.py:51] epoch 2069, training loss: 11272.18, average training loss: 12528.33, base loss: 19655.28
[INFO 2017-06-27 21:38:09,931 main.py:51] epoch 2070, training loss: 13071.13, average training loss: 12528.81, base loss: 19658.76
[INFO 2017-06-27 21:38:10,909 main.py:51] epoch 2071, training loss: 10448.04, average training loss: 12525.32, base loss: 19655.99
[INFO 2017-06-27 21:38:11,884 main.py:51] epoch 2072, training loss: 11505.12, average training loss: 12525.36, base loss: 19657.47
[INFO 2017-06-27 21:38:12,857 main.py:51] epoch 2073, training loss: 13439.59, average training loss: 12523.64, base loss: 19657.95
[INFO 2017-06-27 21:38:13,831 main.py:51] epoch 2074, training loss: 10656.42, average training loss: 12519.61, base loss: 19652.56
[INFO 2017-06-27 21:38:14,805 main.py:51] epoch 2075, training loss: 11654.48, average training loss: 12519.20, base loss: 19653.98
[INFO 2017-06-27 21:38:15,774 main.py:51] epoch 2076, training loss: 10861.00, average training loss: 12515.99, base loss: 19647.88
[INFO 2017-06-27 21:38:16,748 main.py:51] epoch 2077, training loss: 12118.32, average training loss: 12516.07, base loss: 19651.25
[INFO 2017-06-27 21:38:17,721 main.py:51] epoch 2078, training loss: 13486.84, average training loss: 12517.42, base loss: 19655.37
[INFO 2017-06-27 21:38:18,816 main.py:51] epoch 2079, training loss: 11863.04, average training loss: 12517.47, base loss: 19658.12
[INFO 2017-06-27 21:38:19,965 main.py:51] epoch 2080, training loss: 12747.36, average training loss: 12516.73, base loss: 19660.97
[INFO 2017-06-27 21:38:20,953 main.py:51] epoch 2081, training loss: 13065.33, average training loss: 12515.27, base loss: 19661.69
[INFO 2017-06-27 21:38:22,032 main.py:51] epoch 2082, training loss: 10789.69, average training loss: 12513.35, base loss: 19659.37
[INFO 2017-06-27 21:38:23,011 main.py:51] epoch 2083, training loss: 12777.77, average training loss: 12512.85, base loss: 19661.79
[INFO 2017-06-27 21:38:24,120 main.py:51] epoch 2084, training loss: 12741.96, average training loss: 12513.02, base loss: 19664.63
[INFO 2017-06-27 21:38:25,169 main.py:51] epoch 2085, training loss: 11661.11, average training loss: 12511.81, base loss: 19663.63
[INFO 2017-06-27 21:38:26,162 main.py:51] epoch 2086, training loss: 12164.62, average training loss: 12510.97, base loss: 19663.79
[INFO 2017-06-27 21:38:27,245 main.py:51] epoch 2087, training loss: 11705.78, average training loss: 12508.70, base loss: 19664.32
[INFO 2017-06-27 21:38:28,323 main.py:51] epoch 2088, training loss: 11875.08, average training loss: 12508.66, base loss: 19664.61
[INFO 2017-06-27 21:38:29,405 main.py:51] epoch 2089, training loss: 12155.33, average training loss: 12508.15, base loss: 19665.72
[INFO 2017-06-27 21:38:30,464 main.py:51] epoch 2090, training loss: 9753.38, average training loss: 12504.27, base loss: 19660.87
[INFO 2017-06-27 21:38:31,468 main.py:51] epoch 2091, training loss: 11999.37, average training loss: 12502.12, base loss: 19659.83
[INFO 2017-06-27 21:38:32,561 main.py:51] epoch 2092, training loss: 11765.57, average training loss: 12500.03, base loss: 19658.91
[INFO 2017-06-27 21:38:33,642 main.py:51] epoch 2093, training loss: 11865.16, average training loss: 12497.97, base loss: 19659.15
[INFO 2017-06-27 21:38:34,730 main.py:51] epoch 2094, training loss: 12348.96, average training loss: 12497.70, base loss: 19661.35
[INFO 2017-06-27 21:38:35,748 main.py:51] epoch 2095, training loss: 13684.88, average training loss: 12500.32, base loss: 19668.54
[INFO 2017-06-27 21:38:36,723 main.py:51] epoch 2096, training loss: 13190.34, average training loss: 12500.18, base loss: 19670.60
[INFO 2017-06-27 21:38:37,718 main.py:51] epoch 2097, training loss: 11648.04, average training loss: 12499.76, base loss: 19672.47
[INFO 2017-06-27 21:38:38,698 main.py:51] epoch 2098, training loss: 11676.49, average training loss: 12498.98, base loss: 19673.02
[INFO 2017-06-27 21:38:39,670 main.py:51] epoch 2099, training loss: 12533.11, average training loss: 12498.54, base loss: 19675.41
[INFO 2017-06-27 21:38:39,670 main.py:53] epoch 2099, testing
[INFO 2017-06-27 21:38:43,873 main.py:105] average testing loss: 11925.76, base loss: 19569.53
[INFO 2017-06-27 21:38:43,873 main.py:106] improve_loss: 7643.77, improve_percent: 0.39
[INFO 2017-06-27 21:38:43,873 main.py:76] current best improved percent: 0.39
[INFO 2017-06-27 21:38:44,846 main.py:51] epoch 2100, training loss: 12313.06, average training loss: 12497.38, base loss: 19673.90
[INFO 2017-06-27 21:38:45,817 main.py:51] epoch 2101, training loss: 12253.11, average training loss: 12497.54, base loss: 19676.41
[INFO 2017-06-27 21:38:46,788 main.py:51] epoch 2102, training loss: 11902.30, average training loss: 12494.79, base loss: 19674.52
[INFO 2017-06-27 21:38:47,761 main.py:51] epoch 2103, training loss: 13876.85, average training loss: 12498.33, base loss: 19683.46
[INFO 2017-06-27 21:38:48,732 main.py:51] epoch 2104, training loss: 12479.29, average training loss: 12497.88, base loss: 19683.10
[INFO 2017-06-27 21:38:49,703 main.py:51] epoch 2105, training loss: 13212.95, average training loss: 12498.18, base loss: 19686.29
[INFO 2017-06-27 21:38:50,674 main.py:51] epoch 2106, training loss: 12938.71, average training loss: 12497.62, base loss: 19687.99
[INFO 2017-06-27 21:38:51,647 main.py:51] epoch 2107, training loss: 13018.27, average training loss: 12497.37, base loss: 19689.10
[INFO 2017-06-27 21:38:52,664 main.py:51] epoch 2108, training loss: 13128.25, average training loss: 12498.35, base loss: 19692.41
[INFO 2017-06-27 21:38:53,673 main.py:51] epoch 2109, training loss: 11408.48, average training loss: 12496.22, base loss: 19690.55
[INFO 2017-06-27 21:38:54,647 main.py:51] epoch 2110, training loss: 12275.56, average training loss: 12493.84, base loss: 19689.76
[INFO 2017-06-27 21:38:55,784 main.py:51] epoch 2111, training loss: 13312.04, average training loss: 12493.98, base loss: 19691.36
[INFO 2017-06-27 21:38:56,879 main.py:51] epoch 2112, training loss: 12071.92, average training loss: 12491.50, base loss: 19688.11
[INFO 2017-06-27 21:38:57,890 main.py:51] epoch 2113, training loss: 12732.26, average training loss: 12491.48, base loss: 19689.99
[INFO 2017-06-27 21:38:58,981 main.py:51] epoch 2114, training loss: 13023.96, average training loss: 12490.92, base loss: 19691.52
[INFO 2017-06-27 21:39:00,016 main.py:51] epoch 2115, training loss: 10081.76, average training loss: 12487.86, base loss: 19686.62
[INFO 2017-06-27 21:39:01,016 main.py:51] epoch 2116, training loss: 12322.83, average training loss: 12487.24, base loss: 19689.02
[INFO 2017-06-27 21:39:01,994 main.py:51] epoch 2117, training loss: 11160.89, average training loss: 12485.68, base loss: 19686.64
[INFO 2017-06-27 21:39:02,977 main.py:51] epoch 2118, training loss: 13509.81, average training loss: 12484.32, base loss: 19686.42
[INFO 2017-06-27 21:39:03,952 main.py:51] epoch 2119, training loss: 14383.48, average training loss: 12485.59, base loss: 19692.75
[INFO 2017-06-27 21:39:04,927 main.py:51] epoch 2120, training loss: 13330.74, average training loss: 12486.75, base loss: 19698.91
[INFO 2017-06-27 21:39:05,899 main.py:51] epoch 2121, training loss: 11520.70, average training loss: 12484.91, base loss: 19696.01
[INFO 2017-06-27 21:39:06,875 main.py:51] epoch 2122, training loss: 11660.28, average training loss: 12482.42, base loss: 19693.02
[INFO 2017-06-27 21:39:07,853 main.py:51] epoch 2123, training loss: 11863.03, average training loss: 12480.64, base loss: 19691.12
[INFO 2017-06-27 21:39:08,939 main.py:51] epoch 2124, training loss: 11820.14, average training loss: 12480.07, base loss: 19693.27
[INFO 2017-06-27 21:39:10,059 main.py:51] epoch 2125, training loss: 10819.47, average training loss: 12476.77, base loss: 19688.62
[INFO 2017-06-27 21:39:11,037 main.py:51] epoch 2126, training loss: 12805.99, average training loss: 12476.68, base loss: 19689.58
[INFO 2017-06-27 21:39:12,113 main.py:51] epoch 2127, training loss: 11723.70, average training loss: 12475.37, base loss: 19687.10
[INFO 2017-06-27 21:39:13,091 main.py:51] epoch 2128, training loss: 11554.08, average training loss: 12473.06, base loss: 19683.56
[INFO 2017-06-27 21:39:14,188 main.py:51] epoch 2129, training loss: 12802.66, average training loss: 12473.58, base loss: 19686.39
[INFO 2017-06-27 21:39:15,165 main.py:51] epoch 2130, training loss: 12526.19, average training loss: 12473.41, base loss: 19688.21
[INFO 2017-06-27 21:39:16,141 main.py:51] epoch 2131, training loss: 13148.69, average training loss: 12474.99, base loss: 19694.10
[INFO 2017-06-27 21:39:17,167 main.py:51] epoch 2132, training loss: 12021.85, average training loss: 12473.93, base loss: 19695.31
[INFO 2017-06-27 21:39:18,198 main.py:51] epoch 2133, training loss: 10798.22, average training loss: 12469.73, base loss: 19687.62
[INFO 2017-06-27 21:39:19,212 main.py:51] epoch 2134, training loss: 12846.67, average training loss: 12467.64, base loss: 19686.43
[INFO 2017-06-27 21:39:20,289 main.py:51] epoch 2135, training loss: 13571.74, average training loss: 12468.22, base loss: 19690.88
[INFO 2017-06-27 21:39:21,319 main.py:51] epoch 2136, training loss: 11952.86, average training loss: 12466.42, base loss: 19689.72
[INFO 2017-06-27 21:39:22,340 main.py:51] epoch 2137, training loss: 12899.52, average training loss: 12466.23, base loss: 19691.80
[INFO 2017-06-27 21:39:23,358 main.py:51] epoch 2138, training loss: 11332.70, average training loss: 12462.97, base loss: 19688.91
[INFO 2017-06-27 21:39:24,376 main.py:51] epoch 2139, training loss: 13032.17, average training loss: 12462.41, base loss: 19689.21
[INFO 2017-06-27 21:39:25,461 main.py:51] epoch 2140, training loss: 11874.86, average training loss: 12461.06, base loss: 19689.68
[INFO 2017-06-27 21:39:26,522 main.py:51] epoch 2141, training loss: 12903.57, average training loss: 12460.06, base loss: 19692.40
[INFO 2017-06-27 21:39:27,520 main.py:51] epoch 2142, training loss: 10907.99, average training loss: 12458.92, base loss: 19691.88
[INFO 2017-06-27 21:39:28,600 main.py:51] epoch 2143, training loss: 10678.91, average training loss: 12456.60, base loss: 19690.49
[INFO 2017-06-27 21:39:29,677 main.py:51] epoch 2144, training loss: 13233.14, average training loss: 12458.36, base loss: 19696.02
[INFO 2017-06-27 21:39:30,695 main.py:51] epoch 2145, training loss: 11536.72, average training loss: 12456.84, base loss: 19696.57
[INFO 2017-06-27 21:39:31,725 main.py:51] epoch 2146, training loss: 11295.23, average training loss: 12456.30, base loss: 19697.52
[INFO 2017-06-27 21:39:32,723 main.py:51] epoch 2147, training loss: 11524.45, average training loss: 12454.21, base loss: 19696.43
[INFO 2017-06-27 21:39:33,829 main.py:51] epoch 2148, training loss: 10943.49, average training loss: 12453.58, base loss: 19697.03
[INFO 2017-06-27 21:39:34,922 main.py:51] epoch 2149, training loss: 12291.61, average training loss: 12451.33, base loss: 19696.22
[INFO 2017-06-27 21:39:35,977 main.py:51] epoch 2150, training loss: 10428.54, average training loss: 12449.39, base loss: 19692.66
[INFO 2017-06-27 21:39:36,977 main.py:51] epoch 2151, training loss: 10627.16, average training loss: 12446.55, base loss: 19690.05
[INFO 2017-06-27 21:39:38,055 main.py:51] epoch 2152, training loss: 12320.80, average training loss: 12447.36, base loss: 19690.95
[INFO 2017-06-27 21:39:39,081 main.py:51] epoch 2153, training loss: 11441.74, average training loss: 12446.91, base loss: 19691.39
[INFO 2017-06-27 21:39:40,082 main.py:51] epoch 2154, training loss: 12000.18, average training loss: 12444.78, base loss: 19689.67
[INFO 2017-06-27 21:39:41,148 main.py:51] epoch 2155, training loss: 13622.01, average training loss: 12445.06, base loss: 19691.87
[INFO 2017-06-27 21:39:42,171 main.py:51] epoch 2156, training loss: 12639.54, average training loss: 12444.20, base loss: 19691.95
[INFO 2017-06-27 21:39:43,178 main.py:51] epoch 2157, training loss: 12707.51, average training loss: 12442.75, base loss: 19692.16
[INFO 2017-06-27 21:39:44,156 main.py:51] epoch 2158, training loss: 13254.72, average training loss: 12444.16, base loss: 19697.08
[INFO 2017-06-27 21:39:45,255 main.py:51] epoch 2159, training loss: 12131.20, average training loss: 12443.08, base loss: 19697.23
[INFO 2017-06-27 21:39:46,323 main.py:51] epoch 2160, training loss: 11607.76, average training loss: 12441.22, base loss: 19696.86
[INFO 2017-06-27 21:39:47,333 main.py:51] epoch 2161, training loss: 10673.63, average training loss: 12437.64, base loss: 19692.45
[INFO 2017-06-27 21:39:48,310 main.py:51] epoch 2162, training loss: 12190.98, average training loss: 12435.74, base loss: 19691.30
[INFO 2017-06-27 21:39:49,290 main.py:51] epoch 2163, training loss: 11112.92, average training loss: 12433.61, base loss: 19689.26
[INFO 2017-06-27 21:39:50,267 main.py:51] epoch 2164, training loss: 12522.34, average training loss: 12433.57, base loss: 19689.05
[INFO 2017-06-27 21:39:51,243 main.py:51] epoch 2165, training loss: 10674.97, average training loss: 12433.00, base loss: 19690.87
[INFO 2017-06-27 21:39:52,228 main.py:51] epoch 2166, training loss: 11966.54, average training loss: 12432.06, base loss: 19690.92
[INFO 2017-06-27 21:39:53,203 main.py:51] epoch 2167, training loss: 13468.34, average training loss: 12431.41, base loss: 19691.54
[INFO 2017-06-27 21:39:54,178 main.py:51] epoch 2168, training loss: 11227.15, average training loss: 12427.25, base loss: 19685.63
[INFO 2017-06-27 21:39:55,158 main.py:51] epoch 2169, training loss: 13670.91, average training loss: 12427.50, base loss: 19688.49
[INFO 2017-06-27 21:39:56,132 main.py:51] epoch 2170, training loss: 10322.01, average training loss: 12424.55, base loss: 19684.80
[INFO 2017-06-27 21:39:57,103 main.py:51] epoch 2171, training loss: 12976.86, average training loss: 12424.05, base loss: 19685.51
[INFO 2017-06-27 21:39:58,078 main.py:51] epoch 2172, training loss: 12337.58, average training loss: 12423.53, base loss: 19687.82
[INFO 2017-06-27 21:39:59,051 main.py:51] epoch 2173, training loss: 10846.72, average training loss: 12422.28, base loss: 19688.16
[INFO 2017-06-27 21:40:00,026 main.py:51] epoch 2174, training loss: 12487.06, average training loss: 12420.97, base loss: 19688.02
[INFO 2017-06-27 21:40:01,000 main.py:51] epoch 2175, training loss: 12596.49, average training loss: 12418.18, base loss: 19684.81
[INFO 2017-06-27 21:40:01,978 main.py:51] epoch 2176, training loss: 12117.62, average training loss: 12416.72, base loss: 19685.50
[INFO 2017-06-27 21:40:02,950 main.py:51] epoch 2177, training loss: 11071.09, average training loss: 12414.25, base loss: 19682.20
[INFO 2017-06-27 21:40:03,922 main.py:51] epoch 2178, training loss: 11692.18, average training loss: 12412.82, base loss: 19680.14
[INFO 2017-06-27 21:40:04,891 main.py:51] epoch 2179, training loss: 12131.64, average training loss: 12412.46, base loss: 19680.09
[INFO 2017-06-27 21:40:05,865 main.py:51] epoch 2180, training loss: 12146.24, average training loss: 12411.89, base loss: 19682.00
[INFO 2017-06-27 21:40:06,837 main.py:51] epoch 2181, training loss: 12438.63, average training loss: 12411.16, base loss: 19684.39
[INFO 2017-06-27 21:40:07,808 main.py:51] epoch 2182, training loss: 11516.71, average training loss: 12408.41, base loss: 19681.34
[INFO 2017-06-27 21:40:08,782 main.py:51] epoch 2183, training loss: 11059.51, average training loss: 12405.17, base loss: 19676.77
[INFO 2017-06-27 21:40:09,753 main.py:51] epoch 2184, training loss: 10805.63, average training loss: 12404.77, base loss: 19677.98
[INFO 2017-06-27 21:40:10,727 main.py:51] epoch 2185, training loss: 12985.55, average training loss: 12403.89, base loss: 19679.79
[INFO 2017-06-27 21:40:11,697 main.py:51] epoch 2186, training loss: 14252.54, average training loss: 12404.97, base loss: 19684.53
[INFO 2017-06-27 21:40:12,670 main.py:51] epoch 2187, training loss: 12386.17, average training loss: 12404.20, base loss: 19685.47
[INFO 2017-06-27 21:40:13,642 main.py:51] epoch 2188, training loss: 11391.63, average training loss: 12402.43, base loss: 19684.04
[INFO 2017-06-27 21:40:14,620 main.py:51] epoch 2189, training loss: 12262.29, average training loss: 12400.97, base loss: 19685.17
[INFO 2017-06-27 21:40:15,594 main.py:51] epoch 2190, training loss: 13216.59, average training loss: 12400.72, base loss: 19685.72
[INFO 2017-06-27 21:40:16,573 main.py:51] epoch 2191, training loss: 12172.25, average training loss: 12399.54, base loss: 19686.12
[INFO 2017-06-27 21:40:17,549 main.py:51] epoch 2192, training loss: 12179.39, average training loss: 12396.68, base loss: 19683.50
[INFO 2017-06-27 21:40:18,526 main.py:51] epoch 2193, training loss: 12338.77, average training loss: 12393.92, base loss: 19681.91
[INFO 2017-06-27 21:40:19,501 main.py:51] epoch 2194, training loss: 13350.46, average training loss: 12393.76, base loss: 19685.30
[INFO 2017-06-27 21:40:20,475 main.py:51] epoch 2195, training loss: 12317.24, average training loss: 12394.22, base loss: 19690.19
[INFO 2017-06-27 21:40:21,446 main.py:51] epoch 2196, training loss: 12145.72, average training loss: 12391.24, base loss: 19687.56
[INFO 2017-06-27 21:40:22,418 main.py:51] epoch 2197, training loss: 11812.68, average training loss: 12389.88, base loss: 19687.43
[INFO 2017-06-27 21:40:23,390 main.py:51] epoch 2198, training loss: 10920.18, average training loss: 12387.25, base loss: 19685.79
[INFO 2017-06-27 21:40:24,361 main.py:51] epoch 2199, training loss: 11407.36, average training loss: 12384.85, base loss: 19682.28
[INFO 2017-06-27 21:40:24,362 main.py:53] epoch 2199, testing
[INFO 2017-06-27 21:40:28,567 main.py:105] average testing loss: 12235.53, base loss: 20694.28
[INFO 2017-06-27 21:40:28,568 main.py:106] improve_loss: 8458.76, improve_percent: 0.41
[INFO 2017-06-27 21:40:28,568 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:40:28,581 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 21:40:29,554 main.py:51] epoch 2200, training loss: 12853.41, average training loss: 12385.93, base loss: 19687.61
[INFO 2017-06-27 21:40:30,524 main.py:51] epoch 2201, training loss: 12513.63, average training loss: 12385.82, base loss: 19689.69
[INFO 2017-06-27 21:40:31,495 main.py:51] epoch 2202, training loss: 11727.24, average training loss: 12383.18, base loss: 19685.88
[INFO 2017-06-27 21:40:32,465 main.py:51] epoch 2203, training loss: 13482.73, average training loss: 12381.54, base loss: 19686.26
[INFO 2017-06-27 21:40:33,438 main.py:51] epoch 2204, training loss: 13512.62, average training loss: 12383.12, base loss: 19690.58
[INFO 2017-06-27 21:40:34,411 main.py:51] epoch 2205, training loss: 11220.98, average training loss: 12380.33, base loss: 19686.72
[INFO 2017-06-27 21:40:35,385 main.py:51] epoch 2206, training loss: 11740.58, average training loss: 12380.18, base loss: 19688.44
[INFO 2017-06-27 21:40:36,363 main.py:51] epoch 2207, training loss: 12003.39, average training loss: 12378.75, base loss: 19687.44
[INFO 2017-06-27 21:40:37,342 main.py:51] epoch 2208, training loss: 11621.39, average training loss: 12377.03, base loss: 19686.42
[INFO 2017-06-27 21:40:38,339 main.py:51] epoch 2209, training loss: 12593.88, average training loss: 12376.92, base loss: 19688.29
[INFO 2017-06-27 21:40:39,346 main.py:51] epoch 2210, training loss: 11459.70, average training loss: 12375.84, base loss: 19688.63
[INFO 2017-06-27 21:40:40,488 main.py:51] epoch 2211, training loss: 12015.40, average training loss: 12376.00, base loss: 19689.91
[INFO 2017-06-27 21:40:41,613 main.py:51] epoch 2212, training loss: 12618.31, average training loss: 12375.41, base loss: 19691.57
[INFO 2017-06-27 21:40:42,754 main.py:51] epoch 2213, training loss: 13219.10, average training loss: 12376.36, base loss: 19694.96
[INFO 2017-06-27 21:40:43,774 main.py:51] epoch 2214, training loss: 11676.20, average training loss: 12375.16, base loss: 19693.70
[INFO 2017-06-27 21:40:44,747 main.py:51] epoch 2215, training loss: 11589.83, average training loss: 12372.39, base loss: 19690.38
[INFO 2017-06-27 21:40:45,729 main.py:51] epoch 2216, training loss: 11884.45, average training loss: 12371.78, base loss: 19689.92
[INFO 2017-06-27 21:40:46,702 main.py:51] epoch 2217, training loss: 10914.89, average training loss: 12369.90, base loss: 19688.77
[INFO 2017-06-27 21:40:47,681 main.py:51] epoch 2218, training loss: 10839.40, average training loss: 12369.24, base loss: 19687.81
[INFO 2017-06-27 21:40:48,651 main.py:51] epoch 2219, training loss: 12012.71, average training loss: 12369.19, base loss: 19689.35
[INFO 2017-06-27 21:40:49,624 main.py:51] epoch 2220, training loss: 12143.98, average training loss: 12369.12, base loss: 19692.93
[INFO 2017-06-27 21:40:50,597 main.py:51] epoch 2221, training loss: 12683.36, average training loss: 12369.51, base loss: 19695.60
[INFO 2017-06-27 21:40:51,571 main.py:51] epoch 2222, training loss: 12276.26, average training loss: 12368.89, base loss: 19696.58
[INFO 2017-06-27 21:40:52,560 main.py:51] epoch 2223, training loss: 11110.94, average training loss: 12366.45, base loss: 19694.72
[INFO 2017-06-27 21:40:53,530 main.py:51] epoch 2224, training loss: 12433.86, average training loss: 12364.40, base loss: 19692.59
[INFO 2017-06-27 21:40:54,509 main.py:51] epoch 2225, training loss: 10657.78, average training loss: 12362.41, base loss: 19692.19
[INFO 2017-06-27 21:40:55,587 main.py:51] epoch 2226, training loss: 12943.30, average training loss: 12361.64, base loss: 19692.35
[INFO 2017-06-27 21:40:56,586 main.py:51] epoch 2227, training loss: 10456.06, average training loss: 12358.96, base loss: 19689.06
[INFO 2017-06-27 21:40:57,575 main.py:51] epoch 2228, training loss: 12185.13, average training loss: 12358.20, base loss: 19688.52
[INFO 2017-06-27 21:40:58,545 main.py:51] epoch 2229, training loss: 13036.22, average training loss: 12359.26, base loss: 19694.49
[INFO 2017-06-27 21:40:59,521 main.py:51] epoch 2230, training loss: 12886.80, average training loss: 12359.37, base loss: 19695.57
[INFO 2017-06-27 21:41:00,493 main.py:51] epoch 2231, training loss: 11614.15, average training loss: 12358.19, base loss: 19693.63
[INFO 2017-06-27 21:41:01,468 main.py:51] epoch 2232, training loss: 13054.30, average training loss: 12358.35, base loss: 19697.50
[INFO 2017-06-27 21:41:02,544 main.py:51] epoch 2233, training loss: 12390.16, average training loss: 12359.75, base loss: 19703.19
[INFO 2017-06-27 21:41:03,539 main.py:51] epoch 2234, training loss: 13032.15, average training loss: 12359.20, base loss: 19704.05
[INFO 2017-06-27 21:41:04,512 main.py:51] epoch 2235, training loss: 10745.42, average training loss: 12356.12, base loss: 19703.26
[INFO 2017-06-27 21:41:05,484 main.py:51] epoch 2236, training loss: 12679.79, average training loss: 12355.10, base loss: 19702.94
[INFO 2017-06-27 21:41:06,456 main.py:51] epoch 2237, training loss: 12128.72, average training loss: 12355.05, base loss: 19703.85
[INFO 2017-06-27 21:41:07,426 main.py:51] epoch 2238, training loss: 12293.23, average training loss: 12353.88, base loss: 19703.35
[INFO 2017-06-27 21:41:08,397 main.py:51] epoch 2239, training loss: 10711.86, average training loss: 12350.82, base loss: 19698.73
[INFO 2017-06-27 21:41:09,368 main.py:51] epoch 2240, training loss: 9925.35, average training loss: 12349.40, base loss: 19698.59
[INFO 2017-06-27 21:41:10,342 main.py:51] epoch 2241, training loss: 10667.02, average training loss: 12346.05, base loss: 19695.37
[INFO 2017-06-27 21:41:11,311 main.py:51] epoch 2242, training loss: 10427.56, average training loss: 12344.38, base loss: 19693.28
[INFO 2017-06-27 21:41:12,286 main.py:51] epoch 2243, training loss: 10818.64, average training loss: 12341.88, base loss: 19691.27
[INFO 2017-06-27 21:41:13,256 main.py:51] epoch 2244, training loss: 12264.01, average training loss: 12341.15, base loss: 19691.46
[INFO 2017-06-27 21:41:14,226 main.py:51] epoch 2245, training loss: 12754.01, average training loss: 12340.22, base loss: 19691.66
[INFO 2017-06-27 21:41:15,196 main.py:51] epoch 2246, training loss: 12151.06, average training loss: 12339.62, base loss: 19692.32
[INFO 2017-06-27 21:41:16,166 main.py:51] epoch 2247, training loss: 12017.00, average training loss: 12338.78, base loss: 19692.86
[INFO 2017-06-27 21:41:17,136 main.py:51] epoch 2248, training loss: 12676.05, average training loss: 12338.16, base loss: 19693.83
[INFO 2017-06-27 21:41:18,110 main.py:51] epoch 2249, training loss: 12835.43, average training loss: 12337.77, base loss: 19696.15
[INFO 2017-06-27 21:41:19,165 main.py:51] epoch 2250, training loss: 11255.66, average training loss: 12335.35, base loss: 19692.71
[INFO 2017-06-27 21:41:20,201 main.py:51] epoch 2251, training loss: 12887.86, average training loss: 12337.00, base loss: 19697.14
[INFO 2017-06-27 21:41:21,175 main.py:51] epoch 2252, training loss: 10611.92, average training loss: 12335.72, base loss: 19696.43
[INFO 2017-06-27 21:41:22,154 main.py:51] epoch 2253, training loss: 12764.71, average training loss: 12334.99, base loss: 19695.91
[INFO 2017-06-27 21:41:23,127 main.py:51] epoch 2254, training loss: 13896.22, average training loss: 12334.48, base loss: 19696.68
[INFO 2017-06-27 21:41:24,098 main.py:51] epoch 2255, training loss: 11135.50, average training loss: 12332.01, base loss: 19694.05
[INFO 2017-06-27 21:41:25,067 main.py:51] epoch 2256, training loss: 11908.57, average training loss: 12331.62, base loss: 19695.50
[INFO 2017-06-27 21:41:26,038 main.py:51] epoch 2257, training loss: 12547.15, average training loss: 12333.93, base loss: 19700.77
[INFO 2017-06-27 21:41:27,010 main.py:51] epoch 2258, training loss: 12697.87, average training loss: 12335.16, base loss: 19704.54
[INFO 2017-06-27 21:41:27,987 main.py:51] epoch 2259, training loss: 11407.85, average training loss: 12333.60, base loss: 19704.10
[INFO 2017-06-27 21:41:28,956 main.py:51] epoch 2260, training loss: 11853.80, average training loss: 12332.69, base loss: 19705.32
[INFO 2017-06-27 21:41:29,925 main.py:51] epoch 2261, training loss: 11075.72, average training loss: 12330.80, base loss: 19703.87
[INFO 2017-06-27 21:41:31,022 main.py:51] epoch 2262, training loss: 12961.52, average training loss: 12330.97, base loss: 19705.72
[INFO 2017-06-27 21:41:32,004 main.py:51] epoch 2263, training loss: 9839.14, average training loss: 12328.48, base loss: 19702.90
[INFO 2017-06-27 21:41:33,090 main.py:51] epoch 2264, training loss: 11048.09, average training loss: 12324.83, base loss: 19697.43
[INFO 2017-06-27 21:41:34,191 main.py:51] epoch 2265, training loss: 11743.61, average training loss: 12324.21, base loss: 19698.51
[INFO 2017-06-27 21:41:35,179 main.py:51] epoch 2266, training loss: 12240.06, average training loss: 12322.18, base loss: 19696.14
[INFO 2017-06-27 21:41:36,205 main.py:51] epoch 2267, training loss: 11993.49, average training loss: 12321.40, base loss: 19696.63
[INFO 2017-06-27 21:41:37,222 main.py:51] epoch 2268, training loss: 12367.72, average training loss: 12322.07, base loss: 19700.34
[INFO 2017-06-27 21:41:38,298 main.py:51] epoch 2269, training loss: 13270.22, average training loss: 12322.63, base loss: 19703.12
[INFO 2017-06-27 21:41:39,429 main.py:51] epoch 2270, training loss: 10566.97, average training loss: 12320.67, base loss: 19701.55
[INFO 2017-06-27 21:41:40,422 main.py:51] epoch 2271, training loss: 11185.22, average training loss: 12320.12, base loss: 19704.05
[INFO 2017-06-27 21:41:41,504 main.py:51] epoch 2272, training loss: 12962.13, average training loss: 12320.41, base loss: 19706.78
[INFO 2017-06-27 21:41:42,540 main.py:51] epoch 2273, training loss: 11914.97, average training loss: 12319.79, base loss: 19708.63
[INFO 2017-06-27 21:41:43,545 main.py:51] epoch 2274, training loss: 12564.37, average training loss: 12317.67, base loss: 19706.97
[INFO 2017-06-27 21:41:44,521 main.py:51] epoch 2275, training loss: 11851.84, average training loss: 12314.50, base loss: 19705.03
[INFO 2017-06-27 21:41:45,492 main.py:51] epoch 2276, training loss: 11486.98, average training loss: 12312.83, base loss: 19705.00
[INFO 2017-06-27 21:41:46,468 main.py:51] epoch 2277, training loss: 10527.93, average training loss: 12308.83, base loss: 19700.83
[INFO 2017-06-27 21:41:47,444 main.py:51] epoch 2278, training loss: 11898.76, average training loss: 12308.09, base loss: 19702.80
[INFO 2017-06-27 21:41:48,416 main.py:51] epoch 2279, training loss: 14258.35, average training loss: 12309.42, base loss: 19707.02
[INFO 2017-06-27 21:41:49,392 main.py:51] epoch 2280, training loss: 9995.83, average training loss: 12306.82, base loss: 19702.89
[INFO 2017-06-27 21:41:50,364 main.py:51] epoch 2281, training loss: 12240.81, average training loss: 12306.18, base loss: 19703.58
[INFO 2017-06-27 21:41:51,336 main.py:51] epoch 2282, training loss: 11309.83, average training loss: 12304.92, base loss: 19704.29
[INFO 2017-06-27 21:41:52,358 main.py:51] epoch 2283, training loss: 12041.60, average training loss: 12304.22, base loss: 19707.42
[INFO 2017-06-27 21:41:53,349 main.py:51] epoch 2284, training loss: 10648.37, average training loss: 12302.80, base loss: 19706.47
[INFO 2017-06-27 21:41:54,370 main.py:51] epoch 2285, training loss: 10947.13, average training loss: 12298.96, base loss: 19701.54
[INFO 2017-06-27 21:41:55,377 main.py:51] epoch 2286, training loss: 11190.45, average training loss: 12297.49, base loss: 19699.20
[INFO 2017-06-27 21:41:56,352 main.py:51] epoch 2287, training loss: 13443.99, average training loss: 12296.08, base loss: 19698.64
[INFO 2017-06-27 21:41:57,323 main.py:51] epoch 2288, training loss: 12878.69, average training loss: 12294.11, base loss: 19696.80
[INFO 2017-06-27 21:41:58,293 main.py:51] epoch 2289, training loss: 11625.49, average training loss: 12292.22, base loss: 19694.23
[INFO 2017-06-27 21:41:59,271 main.py:51] epoch 2290, training loss: 10884.39, average training loss: 12289.60, base loss: 19689.88
[INFO 2017-06-27 21:42:00,351 main.py:51] epoch 2291, training loss: 11307.02, average training loss: 12287.76, base loss: 19689.33
[INFO 2017-06-27 21:42:01,332 main.py:51] epoch 2292, training loss: 12354.91, average training loss: 12288.56, base loss: 19694.17
[INFO 2017-06-27 21:42:02,304 main.py:51] epoch 2293, training loss: 11049.00, average training loss: 12286.32, base loss: 19692.92
[INFO 2017-06-27 21:42:03,286 main.py:51] epoch 2294, training loss: 11910.55, average training loss: 12285.22, base loss: 19692.68
[INFO 2017-06-27 21:42:04,257 main.py:51] epoch 2295, training loss: 12002.53, average training loss: 12285.45, base loss: 19694.69
[INFO 2017-06-27 21:42:05,231 main.py:51] epoch 2296, training loss: 12298.45, average training loss: 12283.71, base loss: 19694.38
[INFO 2017-06-27 21:42:06,204 main.py:51] epoch 2297, training loss: 12911.51, average training loss: 12282.66, base loss: 19694.13
[INFO 2017-06-27 21:42:07,173 main.py:51] epoch 2298, training loss: 12595.12, average training loss: 12282.65, base loss: 19694.00
[INFO 2017-06-27 21:42:08,149 main.py:51] epoch 2299, training loss: 11667.84, average training loss: 12280.73, base loss: 19693.32
[INFO 2017-06-27 21:42:08,149 main.py:53] epoch 2299, testing
[INFO 2017-06-27 21:42:12,359 main.py:105] average testing loss: 11543.48, base loss: 18796.12
[INFO 2017-06-27 21:42:12,359 main.py:106] improve_loss: 7252.64, improve_percent: 0.39
[INFO 2017-06-27 21:42:12,360 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 21:42:13,330 main.py:51] epoch 2300, training loss: 10408.41, average training loss: 12276.64, base loss: 19688.53
[INFO 2017-06-27 21:42:14,305 main.py:51] epoch 2301, training loss: 11057.22, average training loss: 12273.94, base loss: 19685.15
[INFO 2017-06-27 21:42:15,277 main.py:51] epoch 2302, training loss: 12457.39, average training loss: 12274.05, base loss: 19688.25
[INFO 2017-06-27 21:42:16,248 main.py:51] epoch 2303, training loss: 12258.45, average training loss: 12272.49, base loss: 19688.13
[INFO 2017-06-27 21:42:17,219 main.py:51] epoch 2304, training loss: 10752.59, average training loss: 12270.74, base loss: 19684.35
[INFO 2017-06-27 21:42:18,188 main.py:51] epoch 2305, training loss: 9933.11, average training loss: 12266.02, base loss: 19678.85
[INFO 2017-06-27 21:42:19,160 main.py:51] epoch 2306, training loss: 11570.18, average training loss: 12265.75, base loss: 19681.53
[INFO 2017-06-27 21:42:20,133 main.py:51] epoch 2307, training loss: 9714.33, average training loss: 12262.18, base loss: 19676.70
[INFO 2017-06-27 21:42:21,104 main.py:51] epoch 2308, training loss: 11935.14, average training loss: 12260.72, base loss: 19676.10
[INFO 2017-06-27 21:42:22,076 main.py:51] epoch 2309, training loss: 13266.20, average training loss: 12260.50, base loss: 19678.98
[INFO 2017-06-27 21:42:23,046 main.py:51] epoch 2310, training loss: 12360.71, average training loss: 12259.59, base loss: 19679.37
[INFO 2017-06-27 21:42:24,017 main.py:51] epoch 2311, training loss: 12033.08, average training loss: 12257.79, base loss: 19677.57
[INFO 2017-06-27 21:42:24,996 main.py:51] epoch 2312, training loss: 11027.26, average training loss: 12255.61, base loss: 19676.33
[INFO 2017-06-27 21:42:26,077 main.py:51] epoch 2313, training loss: 11435.48, average training loss: 12254.28, base loss: 19676.00
[INFO 2017-06-27 21:42:27,051 main.py:51] epoch 2314, training loss: 11669.66, average training loss: 12254.04, base loss: 19679.31
[INFO 2017-06-27 21:42:28,021 main.py:51] epoch 2315, training loss: 11798.83, average training loss: 12253.10, base loss: 19679.88
[INFO 2017-06-27 21:42:29,000 main.py:51] epoch 2316, training loss: 11903.37, average training loss: 12251.78, base loss: 19679.00
[INFO 2017-06-27 21:42:30,088 main.py:51] epoch 2317, training loss: 11875.31, average training loss: 12251.23, base loss: 19680.85
[INFO 2017-06-27 21:42:31,086 main.py:51] epoch 2318, training loss: 12042.76, average training loss: 12250.38, base loss: 19682.15
[INFO 2017-06-27 21:42:32,060 main.py:51] epoch 2319, training loss: 11846.14, average training loss: 12248.74, base loss: 19682.50
[INFO 2017-06-27 21:42:33,029 main.py:51] epoch 2320, training loss: 12321.68, average training loss: 12248.20, base loss: 19682.94
[INFO 2017-06-27 21:42:34,002 main.py:51] epoch 2321, training loss: 13026.95, average training loss: 12247.25, base loss: 19684.08
[INFO 2017-06-27 21:42:34,976 main.py:51] epoch 2322, training loss: 12835.00, average training loss: 12245.32, base loss: 19681.80
[INFO 2017-06-27 21:42:35,992 main.py:51] epoch 2323, training loss: 12107.48, average training loss: 12243.31, base loss: 19681.26
[INFO 2017-06-27 21:42:36,999 main.py:51] epoch 2324, training loss: 14176.27, average training loss: 12245.22, base loss: 19686.66
[INFO 2017-06-27 21:42:37,979 main.py:51] epoch 2325, training loss: 10879.84, average training loss: 12243.82, base loss: 19684.34
[INFO 2017-06-27 21:42:38,964 main.py:51] epoch 2326, training loss: 12902.28, average training loss: 12244.48, base loss: 19687.08
[INFO 2017-06-27 21:42:39,945 main.py:51] epoch 2327, training loss: 13026.04, average training loss: 12243.02, base loss: 19687.00
[INFO 2017-06-27 21:42:40,916 main.py:51] epoch 2328, training loss: 12402.95, average training loss: 12242.17, base loss: 19686.88
[INFO 2017-06-27 21:42:41,890 main.py:51] epoch 2329, training loss: 12897.06, average training loss: 12241.39, base loss: 19686.86
[INFO 2017-06-27 21:42:42,862 main.py:51] epoch 2330, training loss: 12696.48, average training loss: 12242.32, base loss: 19688.90
[INFO 2017-06-27 21:42:43,837 main.py:51] epoch 2331, training loss: 11895.40, average training loss: 12239.24, base loss: 19685.03
[INFO 2017-06-27 21:42:44,811 main.py:51] epoch 2332, training loss: 12500.68, average training loss: 12239.07, base loss: 19687.03
[INFO 2017-06-27 21:42:45,783 main.py:51] epoch 2333, training loss: 11298.00, average training loss: 12238.08, base loss: 19687.79
[INFO 2017-06-27 21:42:46,764 main.py:51] epoch 2334, training loss: 11123.54, average training loss: 12234.91, base loss: 19683.22
[INFO 2017-06-27 21:42:47,736 main.py:51] epoch 2335, training loss: 11486.12, average training loss: 12233.90, base loss: 19682.77
[INFO 2017-06-27 21:42:48,709 main.py:51] epoch 2336, training loss: 11786.87, average training loss: 12233.10, base loss: 19683.12
[INFO 2017-06-27 21:42:49,687 main.py:51] epoch 2337, training loss: 14173.54, average training loss: 12234.86, base loss: 19686.46
[INFO 2017-06-27 21:42:50,659 main.py:51] epoch 2338, training loss: 12393.80, average training loss: 12232.75, base loss: 19684.28
[INFO 2017-06-27 21:42:51,628 main.py:51] epoch 2339, training loss: 11304.22, average training loss: 12230.35, base loss: 19682.03
[INFO 2017-06-27 21:42:52,601 main.py:51] epoch 2340, training loss: 11105.54, average training loss: 12229.34, base loss: 19682.32
[INFO 2017-06-27 21:42:53,570 main.py:51] epoch 2341, training loss: 10612.42, average training loss: 12226.86, base loss: 19678.89
[INFO 2017-06-27 21:42:54,546 main.py:51] epoch 2342, training loss: 10040.60, average training loss: 12224.94, base loss: 19677.26
[INFO 2017-06-27 21:42:55,621 main.py:51] epoch 2343, training loss: 12381.39, average training loss: 12223.91, base loss: 19678.53
[INFO 2017-06-27 21:42:56,618 main.py:51] epoch 2344, training loss: 10903.84, average training loss: 12221.66, base loss: 19676.14
[INFO 2017-06-27 21:42:57,633 main.py:51] epoch 2345, training loss: 11135.87, average training loss: 12221.94, base loss: 19678.60
[INFO 2017-06-27 21:42:58,627 main.py:51] epoch 2346, training loss: 12380.94, average training loss: 12219.89, base loss: 19674.84
[INFO 2017-06-27 21:42:59,599 main.py:51] epoch 2347, training loss: 10399.75, average training loss: 12216.91, base loss: 19669.67
[INFO 2017-06-27 21:43:00,625 main.py:51] epoch 2348, training loss: 11976.02, average training loss: 12216.95, base loss: 19673.41
[INFO 2017-06-27 21:43:01,652 main.py:51] epoch 2349, training loss: 11340.69, average training loss: 12215.24, base loss: 19671.54
[INFO 2017-06-27 21:43:02,747 main.py:51] epoch 2350, training loss: 11943.46, average training loss: 12213.86, base loss: 19671.01
[INFO 2017-06-27 21:43:03,759 main.py:51] epoch 2351, training loss: 11931.87, average training loss: 12212.94, base loss: 19670.13
[INFO 2017-06-27 21:43:04,730 main.py:51] epoch 2352, training loss: 12747.05, average training loss: 12212.81, base loss: 19671.47
[INFO 2017-06-27 21:43:05,709 main.py:51] epoch 2353, training loss: 12380.59, average training loss: 12213.32, base loss: 19675.57
[INFO 2017-06-27 21:43:06,685 main.py:51] epoch 2354, training loss: 12059.65, average training loss: 12211.87, base loss: 19675.35
[INFO 2017-06-27 21:43:07,657 main.py:51] epoch 2355, training loss: 12761.80, average training loss: 12212.41, base loss: 19678.81
[INFO 2017-06-27 21:43:08,626 main.py:51] epoch 2356, training loss: 11668.09, average training loss: 12212.01, base loss: 19678.84
[INFO 2017-06-27 21:43:09,602 main.py:51] epoch 2357, training loss: 11924.74, average training loss: 12211.21, base loss: 19680.78
[INFO 2017-06-27 21:43:10,575 main.py:51] epoch 2358, training loss: 11346.02, average training loss: 12211.03, base loss: 19682.95
[INFO 2017-06-27 21:43:11,547 main.py:51] epoch 2359, training loss: 12063.42, average training loss: 12210.35, base loss: 19682.53
[INFO 2017-06-27 21:43:12,520 main.py:51] epoch 2360, training loss: 13176.11, average training loss: 12210.50, base loss: 19686.97
[INFO 2017-06-27 21:43:13,495 main.py:51] epoch 2361, training loss: 11677.60, average training loss: 12208.92, base loss: 19685.56
[INFO 2017-06-27 21:43:14,469 main.py:51] epoch 2362, training loss: 10192.44, average training loss: 12205.24, base loss: 19680.55
[INFO 2017-06-27 21:43:15,554 main.py:51] epoch 2363, training loss: 13313.66, average training loss: 12205.64, base loss: 19683.50
[INFO 2017-06-27 21:43:16,602 main.py:51] epoch 2364, training loss: 13144.04, average training loss: 12206.47, base loss: 19687.87
[INFO 2017-06-27 21:43:17,621 main.py:51] epoch 2365, training loss: 10026.77, average training loss: 12204.12, base loss: 19686.43
[INFO 2017-06-27 21:43:18,735 main.py:51] epoch 2366, training loss: 11999.43, average training loss: 12204.03, base loss: 19689.23
[INFO 2017-06-27 21:43:19,831 main.py:51] epoch 2367, training loss: 9938.54, average training loss: 12201.35, base loss: 19685.58
[INFO 2017-06-27 21:43:20,843 main.py:51] epoch 2368, training loss: 12927.02, average training loss: 12199.44, base loss: 19683.90
[INFO 2017-06-27 21:43:21,813 main.py:51] epoch 2369, training loss: 11101.55, average training loss: 12197.40, base loss: 19682.27
[INFO 2017-06-27 21:43:22,796 main.py:51] epoch 2370, training loss: 11502.30, average training loss: 12196.93, base loss: 19683.23
[INFO 2017-06-27 21:43:23,769 main.py:51] epoch 2371, training loss: 13184.17, average training loss: 12196.71, base loss: 19683.26
[INFO 2017-06-27 21:43:24,745 main.py:51] epoch 2372, training loss: 12542.22, average training loss: 12197.17, base loss: 19685.79
[INFO 2017-06-27 21:43:25,731 main.py:51] epoch 2373, training loss: 10823.35, average training loss: 12195.52, base loss: 19685.39
[INFO 2017-06-27 21:43:26,705 main.py:51] epoch 2374, training loss: 12774.51, average training loss: 12195.46, base loss: 19687.28
[INFO 2017-06-27 21:43:27,686 main.py:51] epoch 2375, training loss: 12250.39, average training loss: 12194.78, base loss: 19687.63
[INFO 2017-06-27 21:43:28,662 main.py:51] epoch 2376, training loss: 11395.43, average training loss: 12192.57, base loss: 19685.58
[INFO 2017-06-27 21:43:29,638 main.py:51] epoch 2377, training loss: 13077.59, average training loss: 12194.11, base loss: 19689.86
[INFO 2017-06-27 21:43:30,614 main.py:51] epoch 2378, training loss: 12620.55, average training loss: 12193.27, base loss: 19691.33
[INFO 2017-06-27 21:43:31,590 main.py:51] epoch 2379, training loss: 12959.17, average training loss: 12193.53, base loss: 19693.41
[INFO 2017-06-27 21:43:32,565 main.py:51] epoch 2380, training loss: 10813.50, average training loss: 12192.16, base loss: 19692.55
[INFO 2017-06-27 21:43:33,542 main.py:51] epoch 2381, training loss: 12390.37, average training loss: 12192.84, base loss: 19695.59
[INFO 2017-06-27 21:43:34,520 main.py:51] epoch 2382, training loss: 11210.89, average training loss: 12190.21, base loss: 19691.47
[INFO 2017-06-27 21:43:35,494 main.py:51] epoch 2383, training loss: 10616.83, average training loss: 12188.29, base loss: 19687.64
[INFO 2017-06-27 21:43:36,470 main.py:51] epoch 2384, training loss: 10934.89, average training loss: 12186.70, base loss: 19686.78
[INFO 2017-06-27 21:43:37,445 main.py:51] epoch 2385, training loss: 11430.18, average training loss: 12183.34, base loss: 19683.12
[INFO 2017-06-27 21:43:38,421 main.py:51] epoch 2386, training loss: 10418.77, average training loss: 12181.01, base loss: 19679.21
[INFO 2017-06-27 21:43:39,394 main.py:51] epoch 2387, training loss: 11331.51, average training loss: 12179.28, base loss: 19678.62
[INFO 2017-06-27 21:43:40,368 main.py:51] epoch 2388, training loss: 12308.65, average training loss: 12178.14, base loss: 19677.71
[INFO 2017-06-27 21:43:41,346 main.py:51] epoch 2389, training loss: 11656.88, average training loss: 12176.39, base loss: 19676.70
[INFO 2017-06-27 21:43:42,330 main.py:51] epoch 2390, training loss: 12269.07, average training loss: 12176.86, base loss: 19680.08
[INFO 2017-06-27 21:43:43,309 main.py:51] epoch 2391, training loss: 14008.01, average training loss: 12177.46, base loss: 19684.47
[INFO 2017-06-27 21:43:44,285 main.py:51] epoch 2392, training loss: 12873.54, average training loss: 12178.59, base loss: 19690.64
[INFO 2017-06-27 21:43:45,264 main.py:51] epoch 2393, training loss: 11322.43, average training loss: 12176.69, base loss: 19688.19
[INFO 2017-06-27 21:43:46,320 main.py:51] epoch 2394, training loss: 10683.30, average training loss: 12175.67, base loss: 19686.06
[INFO 2017-06-27 21:43:47,370 main.py:51] epoch 2395, training loss: 14690.88, average training loss: 12177.64, base loss: 19692.28
[INFO 2017-06-27 21:43:48,400 main.py:51] epoch 2396, training loss: 12585.89, average training loss: 12176.51, base loss: 19690.60
[INFO 2017-06-27 21:43:49,401 main.py:51] epoch 2397, training loss: 11849.26, average training loss: 12174.88, base loss: 19688.50
[INFO 2017-06-27 21:43:50,522 main.py:51] epoch 2398, training loss: 10348.28, average training loss: 12172.49, base loss: 19684.44
[INFO 2017-06-27 21:43:51,562 main.py:51] epoch 2399, training loss: 12174.12, average training loss: 12172.14, base loss: 19686.34
[INFO 2017-06-27 21:43:51,562 main.py:53] epoch 2399, testing
[INFO 2017-06-27 21:43:56,038 main.py:105] average testing loss: 12238.58, base loss: 20277.84
[INFO 2017-06-27 21:43:56,038 main.py:106] improve_loss: 8039.26, improve_percent: 0.40
[INFO 2017-06-27 21:43:56,039 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 21:43:57,117 main.py:51] epoch 2400, training loss: 11116.64, average training loss: 12171.84, base loss: 19686.73
[INFO 2017-06-27 21:43:58,128 main.py:51] epoch 2401, training loss: 14233.57, average training loss: 12174.17, base loss: 19693.36
[INFO 2017-06-27 21:43:59,105 main.py:51] epoch 2402, training loss: 11688.65, average training loss: 12172.64, base loss: 19691.27
[INFO 2017-06-27 21:44:00,076 main.py:51] epoch 2403, training loss: 12660.97, average training loss: 12171.50, base loss: 19691.12
[INFO 2017-06-27 21:44:01,047 main.py:51] epoch 2404, training loss: 11770.75, average training loss: 12170.47, base loss: 19690.91
[INFO 2017-06-27 21:44:02,016 main.py:51] epoch 2405, training loss: 11776.55, average training loss: 12170.34, base loss: 19692.73
[INFO 2017-06-27 21:44:02,987 main.py:51] epoch 2406, training loss: 11882.67, average training loss: 12168.87, base loss: 19692.94
[INFO 2017-06-27 21:44:03,962 main.py:51] epoch 2407, training loss: 11124.22, average training loss: 12169.21, base loss: 19695.02
[INFO 2017-06-27 21:44:04,933 main.py:51] epoch 2408, training loss: 12583.88, average training loss: 12169.27, base loss: 19697.25
[INFO 2017-06-27 21:44:05,910 main.py:51] epoch 2409, training loss: 11829.54, average training loss: 12169.90, base loss: 19700.12
[INFO 2017-06-27 21:44:06,885 main.py:51] epoch 2410, training loss: 10927.99, average training loss: 12168.66, base loss: 19697.64
[INFO 2017-06-27 21:44:07,857 main.py:51] epoch 2411, training loss: 11587.13, average training loss: 12168.11, base loss: 19698.97
[INFO 2017-06-27 21:44:08,881 main.py:51] epoch 2412, training loss: 10832.65, average training loss: 12165.79, base loss: 19696.39
[INFO 2017-06-27 21:44:09,872 main.py:51] epoch 2413, training loss: 11123.91, average training loss: 12165.01, base loss: 19697.51
[INFO 2017-06-27 21:44:10,863 main.py:51] epoch 2414, training loss: 14279.16, average training loss: 12167.04, base loss: 19703.96
[INFO 2017-06-27 21:44:11,838 main.py:51] epoch 2415, training loss: 11236.99, average training loss: 12165.94, base loss: 19702.92
[INFO 2017-06-27 21:44:12,813 main.py:51] epoch 2416, training loss: 11114.80, average training loss: 12164.30, base loss: 19700.94
[INFO 2017-06-27 21:44:13,784 main.py:51] epoch 2417, training loss: 13250.25, average training loss: 12165.89, base loss: 19705.58
[INFO 2017-06-27 21:44:14,770 main.py:51] epoch 2418, training loss: 12067.04, average training loss: 12165.39, base loss: 19707.11
[INFO 2017-06-27 21:44:15,741 main.py:51] epoch 2419, training loss: 11796.99, average training loss: 12165.25, base loss: 19709.09
[INFO 2017-06-27 21:44:16,714 main.py:51] epoch 2420, training loss: 11871.47, average training loss: 12165.10, base loss: 19709.98
[INFO 2017-06-27 21:44:17,743 main.py:51] epoch 2421, training loss: 11864.35, average training loss: 12165.17, base loss: 19711.22
[INFO 2017-06-27 21:44:18,759 main.py:51] epoch 2422, training loss: 11887.74, average training loss: 12165.04, base loss: 19711.18
[INFO 2017-06-27 21:44:19,842 main.py:51] epoch 2423, training loss: 12719.88, average training loss: 12165.26, base loss: 19712.43
[INFO 2017-06-27 21:44:20,822 main.py:51] epoch 2424, training loss: 11952.25, average training loss: 12164.56, base loss: 19713.12
[INFO 2017-06-27 21:44:21,802 main.py:51] epoch 2425, training loss: 10179.81, average training loss: 12163.17, base loss: 19712.77
[INFO 2017-06-27 21:44:22,773 main.py:51] epoch 2426, training loss: 13839.90, average training loss: 12165.30, base loss: 19718.84
[INFO 2017-06-27 21:44:23,743 main.py:51] epoch 2427, training loss: 11362.74, average training loss: 12164.61, base loss: 19717.86
[INFO 2017-06-27 21:44:24,716 main.py:51] epoch 2428, training loss: 11923.26, average training loss: 12165.03, base loss: 19720.18
[INFO 2017-06-27 21:44:25,688 main.py:51] epoch 2429, training loss: 10990.00, average training loss: 12164.99, base loss: 19722.84
[INFO 2017-06-27 21:44:26,662 main.py:51] epoch 2430, training loss: 11265.26, average training loss: 12164.46, base loss: 19724.73
[INFO 2017-06-27 21:44:27,694 main.py:51] epoch 2431, training loss: 12706.26, average training loss: 12167.00, base loss: 19732.61
[INFO 2017-06-27 21:44:28,703 main.py:51] epoch 2432, training loss: 11642.04, average training loss: 12165.57, base loss: 19732.31
[INFO 2017-06-27 21:44:29,812 main.py:51] epoch 2433, training loss: 11548.11, average training loss: 12166.15, base loss: 19734.90
[INFO 2017-06-27 21:44:30,911 main.py:51] epoch 2434, training loss: 11865.26, average training loss: 12164.61, base loss: 19734.85
[INFO 2017-06-27 21:44:31,930 main.py:51] epoch 2435, training loss: 10936.56, average training loss: 12162.58, base loss: 19731.61
[INFO 2017-06-27 21:44:32,919 main.py:51] epoch 2436, training loss: 10527.95, average training loss: 12162.00, base loss: 19729.85
[INFO 2017-06-27 21:44:33,892 main.py:51] epoch 2437, training loss: 10542.64, average training loss: 12160.32, base loss: 19727.65
[INFO 2017-06-27 21:44:34,867 main.py:51] epoch 2438, training loss: 11300.51, average training loss: 12158.40, base loss: 19725.98
[INFO 2017-06-27 21:44:35,842 main.py:51] epoch 2439, training loss: 11549.42, average training loss: 12157.73, base loss: 19725.99
[INFO 2017-06-27 21:44:36,816 main.py:51] epoch 2440, training loss: 11439.26, average training loss: 12155.34, base loss: 19723.63
[INFO 2017-06-27 21:44:37,890 main.py:51] epoch 2441, training loss: 12622.33, average training loss: 12155.54, base loss: 19726.09
[INFO 2017-06-27 21:44:38,889 main.py:51] epoch 2442, training loss: 12958.97, average training loss: 12156.58, base loss: 19729.70
[INFO 2017-06-27 21:44:39,924 main.py:51] epoch 2443, training loss: 12745.94, average training loss: 12155.96, base loss: 19730.51
[INFO 2017-06-27 21:44:40,899 main.py:51] epoch 2444, training loss: 11096.36, average training loss: 12155.08, base loss: 19730.23
[INFO 2017-06-27 21:44:42,013 main.py:51] epoch 2445, training loss: 10773.24, average training loss: 12153.81, base loss: 19729.96
[INFO 2017-06-27 21:44:43,008 main.py:51] epoch 2446, training loss: 11834.05, average training loss: 12153.54, base loss: 19731.87
[INFO 2017-06-27 21:44:44,090 main.py:51] epoch 2447, training loss: 12445.50, average training loss: 12152.52, base loss: 19731.71
[INFO 2017-06-27 21:44:45,114 main.py:51] epoch 2448, training loss: 11987.50, average training loss: 12151.46, base loss: 19730.95
[INFO 2017-06-27 21:44:46,137 main.py:51] epoch 2449, training loss: 10516.78, average training loss: 12148.87, base loss: 19726.34
[INFO 2017-06-27 21:44:47,203 main.py:51] epoch 2450, training loss: 11525.46, average training loss: 12147.62, base loss: 19725.98
[INFO 2017-06-27 21:44:48,222 main.py:51] epoch 2451, training loss: 12665.04, average training loss: 12146.20, base loss: 19727.02
[INFO 2017-06-27 21:44:49,307 main.py:51] epoch 2452, training loss: 12734.65, average training loss: 12146.45, base loss: 19729.92
[INFO 2017-06-27 21:44:50,408 main.py:51] epoch 2453, training loss: 11032.63, average training loss: 12144.43, base loss: 19727.84
[INFO 2017-06-27 21:44:51,478 main.py:51] epoch 2454, training loss: 12545.64, average training loss: 12143.60, base loss: 19730.45
[INFO 2017-06-27 21:44:52,512 main.py:51] epoch 2455, training loss: 12270.22, average training loss: 12142.51, base loss: 19728.90
[INFO 2017-06-27 21:44:53,557 main.py:51] epoch 2456, training loss: 11549.04, average training loss: 12142.49, base loss: 19729.61
[INFO 2017-06-27 21:44:54,549 main.py:51] epoch 2457, training loss: 11473.95, average training loss: 12141.47, base loss: 19728.89
[INFO 2017-06-27 21:44:55,621 main.py:51] epoch 2458, training loss: 10461.16, average training loss: 12140.34, base loss: 19729.55
[INFO 2017-06-27 21:44:56,666 main.py:51] epoch 2459, training loss: 11502.55, average training loss: 12139.78, base loss: 19729.71
[INFO 2017-06-27 21:44:57,673 main.py:51] epoch 2460, training loss: 10085.27, average training loss: 12137.16, base loss: 19726.77
[INFO 2017-06-27 21:44:58,755 main.py:51] epoch 2461, training loss: 13573.70, average training loss: 12139.12, base loss: 19733.07
[INFO 2017-06-27 21:44:59,745 main.py:51] epoch 2462, training loss: 11775.07, average training loss: 12139.43, base loss: 19735.27
[INFO 2017-06-27 21:45:00,779 main.py:51] epoch 2463, training loss: 11844.90, average training loss: 12137.80, base loss: 19734.19
[INFO 2017-06-27 21:45:01,774 main.py:51] epoch 2464, training loss: 11072.32, average training loss: 12137.58, base loss: 19736.07
[INFO 2017-06-27 21:45:02,802 main.py:51] epoch 2465, training loss: 11454.73, average training loss: 12137.51, base loss: 19737.56
[INFO 2017-06-27 21:45:03,875 main.py:51] epoch 2466, training loss: 13081.08, average training loss: 12139.60, base loss: 19742.84
[INFO 2017-06-27 21:45:04,893 main.py:51] epoch 2467, training loss: 11353.20, average training loss: 12139.19, base loss: 19743.31
[INFO 2017-06-27 21:45:05,924 main.py:51] epoch 2468, training loss: 10816.18, average training loss: 12137.48, base loss: 19741.68
[INFO 2017-06-27 21:45:06,921 main.py:51] epoch 2469, training loss: 12181.98, average training loss: 12138.01, base loss: 19745.49
[INFO 2017-06-27 21:45:07,897 main.py:51] epoch 2470, training loss: 11796.39, average training loss: 12137.06, base loss: 19744.17
[INFO 2017-06-27 21:45:08,868 main.py:51] epoch 2471, training loss: 12514.62, average training loss: 12136.44, base loss: 19745.86
[INFO 2017-06-27 21:45:09,847 main.py:51] epoch 2472, training loss: 11607.08, average training loss: 12132.59, base loss: 19740.54
[INFO 2017-06-27 21:45:10,819 main.py:51] epoch 2473, training loss: 11608.45, average training loss: 12131.24, base loss: 19737.69
[INFO 2017-06-27 21:45:11,794 main.py:51] epoch 2474, training loss: 12081.37, average training loss: 12129.66, base loss: 19737.17
[INFO 2017-06-27 21:45:12,765 main.py:51] epoch 2475, training loss: 11509.33, average training loss: 12128.44, base loss: 19737.49
[INFO 2017-06-27 21:45:13,737 main.py:51] epoch 2476, training loss: 10762.51, average training loss: 12126.49, base loss: 19736.56
[INFO 2017-06-27 21:45:14,708 main.py:51] epoch 2477, training loss: 12304.31, average training loss: 12126.95, base loss: 19739.62
[INFO 2017-06-27 21:45:15,680 main.py:51] epoch 2478, training loss: 11163.21, average training loss: 12126.15, base loss: 19741.09
[INFO 2017-06-27 21:45:16,654 main.py:51] epoch 2479, training loss: 10735.32, average training loss: 12125.85, base loss: 19741.28
[INFO 2017-06-27 21:45:17,626 main.py:51] epoch 2480, training loss: 11882.57, average training loss: 12125.89, base loss: 19744.03
[INFO 2017-06-27 21:45:18,596 main.py:51] epoch 2481, training loss: 12641.31, average training loss: 12126.85, base loss: 19748.89
[INFO 2017-06-27 21:45:19,568 main.py:51] epoch 2482, training loss: 11906.33, average training loss: 12126.37, base loss: 19750.90
[INFO 2017-06-27 21:45:20,543 main.py:51] epoch 2483, training loss: 11371.51, average training loss: 12124.84, base loss: 19750.93
[INFO 2017-06-27 21:45:21,520 main.py:51] epoch 2484, training loss: 11615.04, average training loss: 12125.86, base loss: 19754.27
[INFO 2017-06-27 21:45:22,495 main.py:51] epoch 2485, training loss: 10748.01, average training loss: 12124.11, base loss: 19751.85
[INFO 2017-06-27 21:45:23,476 main.py:51] epoch 2486, training loss: 11660.50, average training loss: 12123.85, base loss: 19753.16
[INFO 2017-06-27 21:45:24,448 main.py:51] epoch 2487, training loss: 11716.53, average training loss: 12123.71, base loss: 19755.74
[INFO 2017-06-27 21:45:25,423 main.py:51] epoch 2488, training loss: 12039.93, average training loss: 12123.09, base loss: 19755.28
[INFO 2017-06-27 21:45:26,520 main.py:51] epoch 2489, training loss: 10817.47, average training loss: 12121.82, base loss: 19754.42
[INFO 2017-06-27 21:45:27,497 main.py:51] epoch 2490, training loss: 11785.46, average training loss: 12122.57, base loss: 19758.67
[INFO 2017-06-27 21:45:28,588 main.py:51] epoch 2491, training loss: 11051.82, average training loss: 12121.06, base loss: 19758.87
[INFO 2017-06-27 21:45:29,561 main.py:51] epoch 2492, training loss: 9731.20, average training loss: 12118.11, base loss: 19756.05
[INFO 2017-06-27 21:45:30,537 main.py:51] epoch 2493, training loss: 11152.44, average training loss: 12115.11, base loss: 19753.02
[INFO 2017-06-27 21:45:31,513 main.py:51] epoch 2494, training loss: 10480.71, average training loss: 12113.25, base loss: 19751.39
[INFO 2017-06-27 21:45:32,484 main.py:51] epoch 2495, training loss: 11728.10, average training loss: 12112.38, base loss: 19751.74
[INFO 2017-06-27 21:45:33,452 main.py:51] epoch 2496, training loss: 12229.85, average training loss: 12112.75, base loss: 19753.15
[INFO 2017-06-27 21:45:34,425 main.py:51] epoch 2497, training loss: 10648.05, average training loss: 12111.33, base loss: 19751.47
[INFO 2017-06-27 21:45:35,396 main.py:51] epoch 2498, training loss: 10988.24, average training loss: 12110.80, base loss: 19751.76
[INFO 2017-06-27 21:45:36,373 main.py:51] epoch 2499, training loss: 11539.21, average training loss: 12108.85, base loss: 19749.92
[INFO 2017-06-27 21:45:36,373 main.py:53] epoch 2499, testing
[INFO 2017-06-27 21:45:40,588 main.py:105] average testing loss: 11873.98, base loss: 20291.33
[INFO 2017-06-27 21:45:40,588 main.py:106] improve_loss: 8417.35, improve_percent: 0.41
[INFO 2017-06-27 21:45:40,588 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:45:40,601 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 21:45:41,578 main.py:51] epoch 2500, training loss: 11380.76, average training loss: 12108.27, base loss: 19751.90
[INFO 2017-06-27 21:45:42,552 main.py:51] epoch 2501, training loss: 12002.34, average training loss: 12107.40, base loss: 19752.54
[INFO 2017-06-27 21:45:43,525 main.py:51] epoch 2502, training loss: 11396.78, average training loss: 12104.65, base loss: 19748.79
[INFO 2017-06-27 21:45:44,503 main.py:51] epoch 2503, training loss: 10684.25, average training loss: 12101.32, base loss: 19744.73
[INFO 2017-06-27 21:45:45,582 main.py:51] epoch 2504, training loss: 12462.79, average training loss: 12102.65, base loss: 19749.11
[INFO 2017-06-27 21:45:46,560 main.py:51] epoch 2505, training loss: 13081.83, average training loss: 12104.17, base loss: 19753.83
[INFO 2017-06-27 21:45:47,539 main.py:51] epoch 2506, training loss: 11274.85, average training loss: 12102.21, base loss: 19752.42
[INFO 2017-06-27 21:45:48,514 main.py:51] epoch 2507, training loss: 11457.68, average training loss: 12101.19, base loss: 19752.00
[INFO 2017-06-27 21:45:49,492 main.py:51] epoch 2508, training loss: 11025.59, average training loss: 12100.33, base loss: 19753.66
[INFO 2017-06-27 21:45:50,473 main.py:51] epoch 2509, training loss: 12021.19, average training loss: 12098.15, base loss: 19751.29
[INFO 2017-06-27 21:45:51,450 main.py:51] epoch 2510, training loss: 10326.45, average training loss: 12095.29, base loss: 19746.76
[INFO 2017-06-27 21:45:52,558 main.py:51] epoch 2511, training loss: 12025.81, average training loss: 12094.47, base loss: 19747.53
[INFO 2017-06-27 21:45:53,694 main.py:51] epoch 2512, training loss: 11311.57, average training loss: 12092.76, base loss: 19746.02
[INFO 2017-06-27 21:45:54,775 main.py:51] epoch 2513, training loss: 12117.47, average training loss: 12091.79, base loss: 19745.34
[INFO 2017-06-27 21:45:55,904 main.py:51] epoch 2514, training loss: 12217.58, average training loss: 12091.56, base loss: 19746.81
[INFO 2017-06-27 21:45:56,905 main.py:51] epoch 2515, training loss: 11387.34, average training loss: 12089.93, base loss: 19746.40
[INFO 2017-06-27 21:45:57,878 main.py:51] epoch 2516, training loss: 11696.97, average training loss: 12088.75, base loss: 19745.69
[INFO 2017-06-27 21:45:58,852 main.py:51] epoch 2517, training loss: 12374.39, average training loss: 12088.36, base loss: 19747.32
[INFO 2017-06-27 21:45:59,830 main.py:51] epoch 2518, training loss: 10248.64, average training loss: 12085.97, base loss: 19744.80
[INFO 2017-06-27 21:46:00,800 main.py:51] epoch 2519, training loss: 11962.71, average training loss: 12086.92, base loss: 19749.50
[INFO 2017-06-27 21:46:01,773 main.py:51] epoch 2520, training loss: 11699.99, average training loss: 12086.79, base loss: 19750.96
[INFO 2017-06-27 21:46:02,745 main.py:51] epoch 2521, training loss: 10509.22, average training loss: 12085.17, base loss: 19748.54
[INFO 2017-06-27 21:46:03,721 main.py:51] epoch 2522, training loss: 11597.28, average training loss: 12083.36, base loss: 19745.67
[INFO 2017-06-27 21:46:04,693 main.py:51] epoch 2523, training loss: 14629.48, average training loss: 12085.00, base loss: 19750.11
[INFO 2017-06-27 21:46:05,664 main.py:51] epoch 2524, training loss: 11797.94, average training loss: 12085.36, base loss: 19752.78
[INFO 2017-06-27 21:46:06,635 main.py:51] epoch 2525, training loss: 12305.21, average training loss: 12085.98, base loss: 19755.12
[INFO 2017-06-27 21:46:07,608 main.py:51] epoch 2526, training loss: 13549.08, average training loss: 12087.71, base loss: 19761.36
[INFO 2017-06-27 21:46:08,627 main.py:51] epoch 2527, training loss: 13070.45, average training loss: 12090.17, base loss: 19769.79
[INFO 2017-06-27 21:46:09,619 main.py:51] epoch 2528, training loss: 9721.95, average training loss: 12086.97, base loss: 19764.95
[INFO 2017-06-27 21:46:10,595 main.py:51] epoch 2529, training loss: 11176.24, average training loss: 12087.00, base loss: 19766.95
[INFO 2017-06-27 21:46:11,677 main.py:51] epoch 2530, training loss: 10775.68, average training loss: 12084.82, base loss: 19763.64
[INFO 2017-06-27 21:46:12,691 main.py:51] epoch 2531, training loss: 11271.85, average training loss: 12083.31, base loss: 19762.01
[INFO 2017-06-27 21:46:13,662 main.py:51] epoch 2532, training loss: 12394.29, average training loss: 12082.09, base loss: 19761.05
[INFO 2017-06-27 21:46:14,633 main.py:51] epoch 2533, training loss: 11162.45, average training loss: 12080.11, base loss: 19758.11
[INFO 2017-06-27 21:46:15,609 main.py:51] epoch 2534, training loss: 13332.09, average training loss: 12080.43, base loss: 19761.70
[INFO 2017-06-27 21:46:16,594 main.py:51] epoch 2535, training loss: 10082.97, average training loss: 12079.40, base loss: 19761.41
[INFO 2017-06-27 21:46:17,566 main.py:51] epoch 2536, training loss: 11939.09, average training loss: 12078.81, base loss: 19763.19
[INFO 2017-06-27 21:46:18,542 main.py:51] epoch 2537, training loss: 11528.23, average training loss: 12078.85, base loss: 19766.29
[INFO 2017-06-27 21:46:19,517 main.py:51] epoch 2538, training loss: 11332.41, average training loss: 12077.09, base loss: 19762.72
[INFO 2017-06-27 21:46:20,489 main.py:51] epoch 2539, training loss: 12063.42, average training loss: 12074.86, base loss: 19759.99
[INFO 2017-06-27 21:46:21,463 main.py:51] epoch 2540, training loss: 11299.87, average training loss: 12073.44, base loss: 19757.99
[INFO 2017-06-27 21:46:22,437 main.py:51] epoch 2541, training loss: 11114.20, average training loss: 12070.69, base loss: 19753.86
[INFO 2017-06-27 21:46:23,409 main.py:51] epoch 2542, training loss: 11135.00, average training loss: 12068.41, base loss: 19751.54
[INFO 2017-06-27 21:46:24,384 main.py:51] epoch 2543, training loss: 11173.72, average training loss: 12065.67, base loss: 19747.21
[INFO 2017-06-27 21:46:25,364 main.py:51] epoch 2544, training loss: 11096.95, average training loss: 12062.95, base loss: 19742.84
[INFO 2017-06-27 21:46:26,338 main.py:51] epoch 2545, training loss: 13520.79, average training loss: 12062.68, base loss: 19742.82
[INFO 2017-06-27 21:46:27,311 main.py:51] epoch 2546, training loss: 11640.36, average training loss: 12061.18, base loss: 19741.52
[INFO 2017-06-27 21:46:28,285 main.py:51] epoch 2547, training loss: 12504.95, average training loss: 12060.93, base loss: 19744.34
[INFO 2017-06-27 21:46:29,261 main.py:51] epoch 2548, training loss: 10464.09, average training loss: 12058.84, base loss: 19742.75
[INFO 2017-06-27 21:46:30,232 main.py:51] epoch 2549, training loss: 12360.91, average training loss: 12059.20, base loss: 19744.85
[INFO 2017-06-27 21:46:31,202 main.py:51] epoch 2550, training loss: 10928.83, average training loss: 12058.18, base loss: 19744.15
[INFO 2017-06-27 21:46:32,175 main.py:51] epoch 2551, training loss: 10285.37, average training loss: 12056.32, base loss: 19740.11
[INFO 2017-06-27 21:46:33,147 main.py:51] epoch 2552, training loss: 13345.15, average training loss: 12057.81, base loss: 19745.99
[INFO 2017-06-27 21:46:34,119 main.py:51] epoch 2553, training loss: 10676.09, average training loss: 12056.05, base loss: 19744.46
[INFO 2017-06-27 21:46:35,092 main.py:51] epoch 2554, training loss: 11993.94, average training loss: 12055.72, base loss: 19746.82
[INFO 2017-06-27 21:46:36,075 main.py:51] epoch 2555, training loss: 10452.72, average training loss: 12053.59, base loss: 19744.68
[INFO 2017-06-27 21:46:37,046 main.py:51] epoch 2556, training loss: 11607.31, average training loss: 12051.55, base loss: 19741.70
[INFO 2017-06-27 21:46:38,017 main.py:51] epoch 2557, training loss: 11133.83, average training loss: 12051.00, base loss: 19742.39
[INFO 2017-06-27 21:46:38,990 main.py:51] epoch 2558, training loss: 11561.78, average training loss: 12051.30, base loss: 19746.09
[INFO 2017-06-27 21:46:39,964 main.py:51] epoch 2559, training loss: 12448.88, average training loss: 12052.62, base loss: 19752.05
[INFO 2017-06-27 21:46:40,936 main.py:51] epoch 2560, training loss: 11910.38, average training loss: 12052.31, base loss: 19751.69
[INFO 2017-06-27 21:46:41,910 main.py:51] epoch 2561, training loss: 12335.98, average training loss: 12053.33, base loss: 19756.13
[INFO 2017-06-27 21:46:42,886 main.py:51] epoch 2562, training loss: 10480.10, average training loss: 12050.09, base loss: 19752.70
[INFO 2017-06-27 21:46:43,861 main.py:51] epoch 2563, training loss: 12211.24, average training loss: 12048.89, base loss: 19753.33
[INFO 2017-06-27 21:46:44,838 main.py:51] epoch 2564, training loss: 12209.87, average training loss: 12047.50, base loss: 19753.10
[INFO 2017-06-27 21:46:45,810 main.py:51] epoch 2565, training loss: 10462.64, average training loss: 12046.56, base loss: 19752.99
[INFO 2017-06-27 21:46:46,782 main.py:51] epoch 2566, training loss: 11679.91, average training loss: 12045.70, base loss: 19754.03
[INFO 2017-06-27 21:46:47,754 main.py:51] epoch 2567, training loss: 11870.17, average training loss: 12044.70, base loss: 19753.72
[INFO 2017-06-27 21:46:48,730 main.py:51] epoch 2568, training loss: 11342.89, average training loss: 12041.85, base loss: 19750.35
[INFO 2017-06-27 21:46:49,705 main.py:51] epoch 2569, training loss: 11108.11, average training loss: 12041.25, base loss: 19750.35
[INFO 2017-06-27 21:46:50,678 main.py:51] epoch 2570, training loss: 10312.93, average training loss: 12037.04, base loss: 19743.32
[INFO 2017-06-27 21:46:51,653 main.py:51] epoch 2571, training loss: 11878.95, average training loss: 12037.04, base loss: 19743.83
[INFO 2017-06-27 21:46:52,626 main.py:51] epoch 2572, training loss: 12893.05, average training loss: 12036.92, base loss: 19746.26
[INFO 2017-06-27 21:46:53,598 main.py:51] epoch 2573, training loss: 11303.84, average training loss: 12037.48, base loss: 19749.37
[INFO 2017-06-27 21:46:54,572 main.py:51] epoch 2574, training loss: 11017.22, average training loss: 12036.14, base loss: 19747.94
[INFO 2017-06-27 21:46:55,546 main.py:51] epoch 2575, training loss: 10756.23, average training loss: 12035.88, base loss: 19748.69
[INFO 2017-06-27 21:46:56,518 main.py:51] epoch 2576, training loss: 10700.70, average training loss: 12033.59, base loss: 19746.45
[INFO 2017-06-27 21:46:57,492 main.py:51] epoch 2577, training loss: 14023.93, average training loss: 12035.48, base loss: 19752.09
[INFO 2017-06-27 21:46:58,465 main.py:51] epoch 2578, training loss: 12312.54, average training loss: 12035.49, base loss: 19754.16
[INFO 2017-06-27 21:46:59,460 main.py:51] epoch 2579, training loss: 11818.51, average training loss: 12035.81, base loss: 19756.93
[INFO 2017-06-27 21:47:00,488 main.py:51] epoch 2580, training loss: 9908.17, average training loss: 12032.62, base loss: 19750.31
[INFO 2017-06-27 21:47:01,485 main.py:51] epoch 2581, training loss: 11424.17, average training loss: 12032.86, base loss: 19752.79
[INFO 2017-06-27 21:47:02,493 main.py:51] epoch 2582, training loss: 14143.36, average training loss: 12033.80, base loss: 19757.14
[INFO 2017-06-27 21:47:03,480 main.py:51] epoch 2583, training loss: 11813.16, average training loss: 12033.53, base loss: 19760.39
[INFO 2017-06-27 21:47:04,464 main.py:51] epoch 2584, training loss: 13102.28, average training loss: 12035.69, base loss: 19766.21
[INFO 2017-06-27 21:47:05,444 main.py:51] epoch 2585, training loss: 10716.71, average training loss: 12033.11, base loss: 19762.60
[INFO 2017-06-27 21:47:06,417 main.py:51] epoch 2586, training loss: 11427.64, average training loss: 12032.01, base loss: 19762.13
[INFO 2017-06-27 21:47:07,391 main.py:51] epoch 2587, training loss: 11752.75, average training loss: 12031.22, base loss: 19760.89
[INFO 2017-06-27 21:47:08,374 main.py:51] epoch 2588, training loss: 12059.80, average training loss: 12029.81, base loss: 19759.67
[INFO 2017-06-27 21:47:09,351 main.py:51] epoch 2589, training loss: 12977.60, average training loss: 12030.63, base loss: 19763.05
[INFO 2017-06-27 21:47:10,321 main.py:51] epoch 2590, training loss: 11516.31, average training loss: 12029.21, base loss: 19760.05
[INFO 2017-06-27 21:47:11,298 main.py:51] epoch 2591, training loss: 11906.01, average training loss: 12028.64, base loss: 19759.92
[INFO 2017-06-27 21:47:12,272 main.py:51] epoch 2592, training loss: 13806.71, average training loss: 12028.85, base loss: 19761.05
[INFO 2017-06-27 21:47:13,298 main.py:51] epoch 2593, training loss: 11660.42, average training loss: 12026.97, base loss: 19759.05
[INFO 2017-06-27 21:47:14,311 main.py:51] epoch 2594, training loss: 11356.70, average training loss: 12026.89, base loss: 19761.48
[INFO 2017-06-27 21:47:15,291 main.py:51] epoch 2595, training loss: 11139.61, average training loss: 12023.72, base loss: 19756.37
[INFO 2017-06-27 21:47:16,265 main.py:51] epoch 2596, training loss: 12439.16, average training loss: 12024.11, base loss: 19760.01
[INFO 2017-06-27 21:47:17,238 main.py:51] epoch 2597, training loss: 10776.21, average training loss: 12022.38, base loss: 19758.74
[INFO 2017-06-27 21:47:18,209 main.py:51] epoch 2598, training loss: 10553.10, average training loss: 12020.44, base loss: 19756.82
[INFO 2017-06-27 21:47:19,177 main.py:51] epoch 2599, training loss: 11817.80, average training loss: 12019.25, base loss: 19755.91
[INFO 2017-06-27 21:47:19,177 main.py:53] epoch 2599, testing
[INFO 2017-06-27 21:47:23,479 main.py:105] average testing loss: 11720.09, base loss: 19857.81
[INFO 2017-06-27 21:47:23,479 main.py:106] improve_loss: 8137.73, improve_percent: 0.41
[INFO 2017-06-27 21:47:23,480 main.py:76] current best improved percent: 0.41
[INFO 2017-06-27 21:47:24,473 main.py:51] epoch 2600, training loss: 11002.60, average training loss: 12016.53, base loss: 19751.51
[INFO 2017-06-27 21:47:25,455 main.py:51] epoch 2601, training loss: 14725.62, average training loss: 12019.72, base loss: 19760.99
[INFO 2017-06-27 21:47:26,427 main.py:51] epoch 2602, training loss: 11529.46, average training loss: 12020.81, base loss: 19765.89
[INFO 2017-06-27 21:47:27,402 main.py:51] epoch 2603, training loss: 10629.94, average training loss: 12017.76, base loss: 19762.74
[INFO 2017-06-27 21:47:28,424 main.py:51] epoch 2604, training loss: 12171.20, average training loss: 12016.93, base loss: 19764.28
[INFO 2017-06-27 21:47:29,432 main.py:51] epoch 2605, training loss: 11384.69, average training loss: 12017.01, base loss: 19766.18
[INFO 2017-06-27 21:47:30,410 main.py:51] epoch 2606, training loss: 11197.98, average training loss: 12017.29, base loss: 19766.70
[INFO 2017-06-27 21:47:31,490 main.py:51] epoch 2607, training loss: 11767.26, average training loss: 12014.69, base loss: 19763.52
[INFO 2017-06-27 21:47:32,518 main.py:51] epoch 2608, training loss: 12680.93, average training loss: 12014.28, base loss: 19765.16
[INFO 2017-06-27 21:47:33,568 main.py:51] epoch 2609, training loss: 10640.10, average training loss: 12013.23, base loss: 19764.92
[INFO 2017-06-27 21:47:34,584 main.py:51] epoch 2610, training loss: 12360.01, average training loss: 12014.14, base loss: 19769.93
[INFO 2017-06-27 21:47:35,559 main.py:51] epoch 2611, training loss: 11640.23, average training loss: 12012.75, base loss: 19768.93
[INFO 2017-06-27 21:47:36,531 main.py:51] epoch 2612, training loss: 12049.13, average training loss: 12014.53, base loss: 19773.93
[INFO 2017-06-27 21:47:37,502 main.py:51] epoch 2613, training loss: 11195.50, average training loss: 12012.10, base loss: 19772.14
[INFO 2017-06-27 21:47:38,473 main.py:51] epoch 2614, training loss: 10468.23, average training loss: 12010.55, base loss: 19769.29
[INFO 2017-06-27 21:47:39,496 main.py:51] epoch 2615, training loss: 10629.51, average training loss: 12009.45, base loss: 19769.84
[INFO 2017-06-27 21:47:40,493 main.py:51] epoch 2616, training loss: 10779.90, average training loss: 12007.56, base loss: 19768.67
[INFO 2017-06-27 21:47:41,474 main.py:51] epoch 2617, training loss: 12175.44, average training loss: 12007.79, base loss: 19771.47
[INFO 2017-06-27 21:47:42,454 main.py:51] epoch 2618, training loss: 11433.20, average training loss: 12008.24, base loss: 19773.09
[INFO 2017-06-27 21:47:43,430 main.py:51] epoch 2619, training loss: 11940.81, average training loss: 12005.79, base loss: 19770.22
[INFO 2017-06-27 21:47:44,409 main.py:51] epoch 2620, training loss: 12096.71, average training loss: 12004.86, base loss: 19770.37
[INFO 2017-06-27 21:47:45,387 main.py:51] epoch 2621, training loss: 11030.95, average training loss: 12004.64, base loss: 19772.54
[INFO 2017-06-27 21:47:46,360 main.py:51] epoch 2622, training loss: 11262.07, average training loss: 12003.85, base loss: 19772.84
[INFO 2017-06-27 21:47:47,335 main.py:51] epoch 2623, training loss: 11784.64, average training loss: 12004.51, base loss: 19777.63
[INFO 2017-06-27 21:47:48,305 main.py:51] epoch 2624, training loss: 10945.03, average training loss: 12003.77, base loss: 19777.23
[INFO 2017-06-27 21:47:49,284 main.py:51] epoch 2625, training loss: 12463.93, average training loss: 12003.16, base loss: 19777.72
[INFO 2017-06-27 21:47:50,261 main.py:51] epoch 2626, training loss: 11087.03, average training loss: 12002.71, base loss: 19777.90
[INFO 2017-06-27 21:47:51,237 main.py:51] epoch 2627, training loss: 11508.61, average training loss: 12001.40, base loss: 19777.42
[INFO 2017-06-27 21:47:52,213 main.py:51] epoch 2628, training loss: 10991.41, average training loss: 12000.80, base loss: 19778.61
[INFO 2017-06-27 21:47:53,200 main.py:51] epoch 2629, training loss: 12371.69, average training loss: 11999.39, base loss: 19778.02
[INFO 2017-06-27 21:47:54,260 main.py:51] epoch 2630, training loss: 12032.15, average training loss: 11999.39, base loss: 19780.22
[INFO 2017-06-27 21:47:55,406 main.py:51] epoch 2631, training loss: 11075.45, average training loss: 11998.37, base loss: 19780.42
[INFO 2017-06-27 21:47:56,468 main.py:51] epoch 2632, training loss: 11376.09, average training loss: 11996.06, base loss: 19779.42
[INFO 2017-06-27 21:47:57,670 main.py:51] epoch 2633, training loss: 9459.78, average training loss: 11993.68, base loss: 19774.21
[INFO 2017-06-27 21:47:58,708 main.py:51] epoch 2634, training loss: 11244.92, average training loss: 11993.17, base loss: 19774.02
[INFO 2017-06-27 21:47:59,680 main.py:51] epoch 2635, training loss: 11846.16, average training loss: 11991.83, base loss: 19773.41
[INFO 2017-06-27 21:48:00,663 main.py:51] epoch 2636, training loss: 11850.63, average training loss: 11993.35, base loss: 19777.87
[INFO 2017-06-27 21:48:01,640 main.py:51] epoch 2637, training loss: 11075.54, average training loss: 11991.36, base loss: 19776.40
[INFO 2017-06-27 21:48:02,617 main.py:51] epoch 2638, training loss: 11344.25, average training loss: 11991.45, base loss: 19780.69
[INFO 2017-06-27 21:48:03,589 main.py:51] epoch 2639, training loss: 11245.37, average training loss: 11990.36, base loss: 19779.34
[INFO 2017-06-27 21:48:04,565 main.py:51] epoch 2640, training loss: 10078.08, average training loss: 11987.66, base loss: 19775.95
[INFO 2017-06-27 21:48:05,540 main.py:51] epoch 2641, training loss: 11487.90, average training loss: 11986.59, base loss: 19774.33
[INFO 2017-06-27 21:48:06,510 main.py:51] epoch 2642, training loss: 10597.12, average training loss: 11983.74, base loss: 19772.83
[INFO 2017-06-27 21:48:07,483 main.py:51] epoch 2643, training loss: 14317.39, average training loss: 11986.73, base loss: 19781.80
[INFO 2017-06-27 21:48:08,500 main.py:51] epoch 2644, training loss: 10892.13, average training loss: 11984.99, base loss: 19779.01
[INFO 2017-06-27 21:48:09,493 main.py:51] epoch 2645, training loss: 11757.00, average training loss: 11984.42, base loss: 19779.74
[INFO 2017-06-27 21:48:10,464 main.py:51] epoch 2646, training loss: 10710.88, average training loss: 11981.82, base loss: 19776.37
[INFO 2017-06-27 21:48:11,445 main.py:51] epoch 2647, training loss: 14350.08, average training loss: 11983.04, base loss: 19780.19
[INFO 2017-06-27 21:48:12,420 main.py:51] epoch 2648, training loss: 11010.76, average training loss: 11978.93, base loss: 19771.90
[INFO 2017-06-27 21:48:13,395 main.py:51] epoch 2649, training loss: 12532.46, average training loss: 11979.05, base loss: 19774.80
[INFO 2017-06-27 21:48:14,368 main.py:51] epoch 2650, training loss: 11003.46, average training loss: 11976.99, base loss: 19772.67
[INFO 2017-06-27 21:48:15,339 main.py:51] epoch 2651, training loss: 11337.13, average training loss: 11976.55, base loss: 19771.73
[INFO 2017-06-27 21:48:16,313 main.py:51] epoch 2652, training loss: 11183.46, average training loss: 11975.57, base loss: 19770.85
[INFO 2017-06-27 21:48:17,283 main.py:51] epoch 2653, training loss: 11836.24, average training loss: 11975.40, base loss: 19771.78
[INFO 2017-06-27 21:48:18,265 main.py:51] epoch 2654, training loss: 11108.15, average training loss: 11975.09, base loss: 19772.44
[INFO 2017-06-27 21:48:19,311 main.py:51] epoch 2655, training loss: 12796.12, average training loss: 11974.60, base loss: 19772.38
[INFO 2017-06-27 21:48:20,293 main.py:51] epoch 2656, training loss: 9925.37, average training loss: 11972.61, base loss: 19768.57
[INFO 2017-06-27 21:48:21,273 main.py:51] epoch 2657, training loss: 12125.00, average training loss: 11974.49, base loss: 19774.59
[INFO 2017-06-27 21:48:22,254 main.py:51] epoch 2658, training loss: 10766.10, average training loss: 11974.44, base loss: 19775.02
[INFO 2017-06-27 21:48:23,228 main.py:51] epoch 2659, training loss: 12101.32, average training loss: 11974.09, base loss: 19775.49
[INFO 2017-06-27 21:48:24,249 main.py:51] epoch 2660, training loss: 11246.30, average training loss: 11972.46, base loss: 19775.11
[INFO 2017-06-27 21:48:25,241 main.py:51] epoch 2661, training loss: 11987.42, average training loss: 11972.73, base loss: 19776.55
[INFO 2017-06-27 21:48:26,322 main.py:51] epoch 2662, training loss: 13010.54, average training loss: 11972.30, base loss: 19776.18
[INFO 2017-06-27 21:48:27,405 main.py:51] epoch 2663, training loss: 11899.87, average training loss: 11971.59, base loss: 19776.31
[INFO 2017-06-27 21:48:28,423 main.py:51] epoch 2664, training loss: 10734.32, average training loss: 11969.67, base loss: 19775.15
[INFO 2017-06-27 21:48:29,498 main.py:51] epoch 2665, training loss: 11708.83, average training loss: 11967.72, base loss: 19773.58
[INFO 2017-06-27 21:48:30,613 main.py:51] epoch 2666, training loss: 11567.43, average training loss: 11967.35, base loss: 19774.68
[INFO 2017-06-27 21:48:31,624 main.py:51] epoch 2667, training loss: 12990.51, average training loss: 11968.33, base loss: 19778.59
[INFO 2017-06-27 21:48:32,603 main.py:51] epoch 2668, training loss: 12037.56, average training loss: 11968.96, base loss: 19780.56
[INFO 2017-06-27 21:48:33,574 main.py:51] epoch 2669, training loss: 13851.44, average training loss: 11970.89, base loss: 19786.28
[INFO 2017-06-27 21:48:34,546 main.py:51] epoch 2670, training loss: 11285.93, average training loss: 11971.47, base loss: 19788.84
[INFO 2017-06-27 21:48:35,521 main.py:51] epoch 2671, training loss: 10872.43, average training loss: 11970.11, base loss: 19788.11
[INFO 2017-06-27 21:48:36,499 main.py:51] epoch 2672, training loss: 10414.68, average training loss: 11967.90, base loss: 19786.47
[INFO 2017-06-27 21:48:37,475 main.py:51] epoch 2673, training loss: 12001.54, average training loss: 11969.36, base loss: 19790.96
[INFO 2017-06-27 21:48:38,450 main.py:51] epoch 2674, training loss: 12486.41, average training loss: 11969.65, base loss: 19793.06
[INFO 2017-06-27 21:48:39,422 main.py:51] epoch 2675, training loss: 11083.50, average training loss: 11969.88, base loss: 19794.77
[INFO 2017-06-27 21:48:40,394 main.py:51] epoch 2676, training loss: 11072.90, average training loss: 11969.00, base loss: 19793.74
[INFO 2017-06-27 21:48:41,367 main.py:51] epoch 2677, training loss: 11908.90, average training loss: 11968.53, base loss: 19794.00
[INFO 2017-06-27 21:48:42,340 main.py:51] epoch 2678, training loss: 12734.92, average training loss: 11967.53, base loss: 19792.03
[INFO 2017-06-27 21:48:43,315 main.py:51] epoch 2679, training loss: 11449.67, average training loss: 11964.54, base loss: 19787.28
[INFO 2017-06-27 21:48:44,290 main.py:51] epoch 2680, training loss: 11397.14, average training loss: 11962.28, base loss: 19785.44
[INFO 2017-06-27 21:48:45,261 main.py:51] epoch 2681, training loss: 11606.79, average training loss: 11960.30, base loss: 19781.89
[INFO 2017-06-27 21:48:46,233 main.py:51] epoch 2682, training loss: 11613.61, average training loss: 11958.96, base loss: 19781.51
[INFO 2017-06-27 21:48:47,203 main.py:51] epoch 2683, training loss: 11680.21, average training loss: 11960.60, base loss: 19786.52
[INFO 2017-06-27 21:48:48,175 main.py:51] epoch 2684, training loss: 11283.95, average training loss: 11958.63, base loss: 19783.18
[INFO 2017-06-27 21:48:49,192 main.py:51] epoch 2685, training loss: 11368.47, average training loss: 11958.12, base loss: 19783.25
[INFO 2017-06-27 21:48:50,185 main.py:51] epoch 2686, training loss: 11709.53, average training loss: 11959.12, base loss: 19786.77
[INFO 2017-06-27 21:48:51,283 main.py:51] epoch 2687, training loss: 11503.79, average training loss: 11958.91, base loss: 19788.60
[INFO 2017-06-27 21:48:52,323 main.py:51] epoch 2688, training loss: 11361.02, average training loss: 11957.45, base loss: 19787.61
[INFO 2017-06-27 21:48:53,299 main.py:51] epoch 2689, training loss: 12155.80, average training loss: 11956.68, base loss: 19787.91
[INFO 2017-06-27 21:48:54,282 main.py:51] epoch 2690, training loss: 12146.17, average training loss: 11956.60, base loss: 19791.28
[INFO 2017-06-27 21:48:55,256 main.py:51] epoch 2691, training loss: 10645.75, average training loss: 11955.62, base loss: 19791.77
[INFO 2017-06-27 21:48:56,230 main.py:51] epoch 2692, training loss: 12467.45, average training loss: 11957.44, base loss: 19796.70
[INFO 2017-06-27 21:48:57,199 main.py:51] epoch 2693, training loss: 11345.31, average training loss: 11956.02, base loss: 19796.39
[INFO 2017-06-27 21:48:58,171 main.py:51] epoch 2694, training loss: 11108.57, average training loss: 11954.82, base loss: 19797.61
[INFO 2017-06-27 21:48:59,140 main.py:51] epoch 2695, training loss: 10607.09, average training loss: 11952.85, base loss: 19794.37
[INFO 2017-06-27 21:49:00,112 main.py:51] epoch 2696, training loss: 11922.42, average training loss: 11952.56, base loss: 19795.37
[INFO 2017-06-27 21:49:01,084 main.py:51] epoch 2697, training loss: 12014.04, average training loss: 11950.80, base loss: 19793.42
[INFO 2017-06-27 21:49:02,054 main.py:51] epoch 2698, training loss: 12512.30, average training loss: 11952.17, base loss: 19797.87
[INFO 2017-06-27 21:49:03,022 main.py:51] epoch 2699, training loss: 11382.60, average training loss: 11952.07, base loss: 19799.30
[INFO 2017-06-27 21:49:03,022 main.py:53] epoch 2699, testing
[INFO 2017-06-27 21:49:07,388 main.py:105] average testing loss: 11356.41, base loss: 19613.53
[INFO 2017-06-27 21:49:07,388 main.py:106] improve_loss: 8257.13, improve_percent: 0.42
[INFO 2017-06-27 21:49:07,389 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:49:07,406 main.py:76] current best improved percent: 0.42
[INFO 2017-06-27 21:49:08,412 main.py:51] epoch 2700, training loss: 10324.21, average training loss: 11950.85, base loss: 19797.60
[INFO 2017-06-27 21:49:09,385 main.py:51] epoch 2701, training loss: 10614.63, average training loss: 11950.31, base loss: 19798.11
[INFO 2017-06-27 21:49:10,487 main.py:51] epoch 2702, training loss: 10472.44, average training loss: 11948.83, base loss: 19795.57
[INFO 2017-06-27 21:49:11,482 main.py:51] epoch 2703, training loss: 11503.11, average training loss: 11949.52, base loss: 19798.56
[INFO 2017-06-27 21:49:12,569 main.py:51] epoch 2704, training loss: 10924.23, average training loss: 11948.89, base loss: 19796.87
[INFO 2017-06-27 21:49:13,552 main.py:51] epoch 2705, training loss: 11487.31, average training loss: 11947.22, base loss: 19794.23
[INFO 2017-06-27 21:49:14,669 main.py:51] epoch 2706, training loss: 11932.94, average training loss: 11946.71, base loss: 19795.34
[INFO 2017-06-27 21:49:15,643 main.py:51] epoch 2707, training loss: 12802.58, average training loss: 11947.80, base loss: 19798.68
[INFO 2017-06-27 21:49:16,612 main.py:51] epoch 2708, training loss: 11332.54, average training loss: 11947.49, base loss: 19798.92
[INFO 2017-06-27 21:49:17,587 main.py:51] epoch 2709, training loss: 10901.53, average training loss: 11947.15, base loss: 19800.60
[INFO 2017-06-27 21:49:18,586 main.py:51] epoch 2710, training loss: 11418.51, average training loss: 11946.12, base loss: 19799.85
[INFO 2017-06-27 21:49:19,558 main.py:51] epoch 2711, training loss: 11576.89, average training loss: 11945.56, base loss: 19799.75
[INFO 2017-06-27 21:49:20,537 main.py:51] epoch 2712, training loss: 11120.90, average training loss: 11944.92, base loss: 19799.75
[INFO 2017-06-27 21:49:21,512 main.py:51] epoch 2713, training loss: 10402.85, average training loss: 11941.98, base loss: 19795.59
[INFO 2017-06-27 21:49:22,486 main.py:51] epoch 2714, training loss: 10062.16, average training loss: 11939.11, base loss: 19790.24
[INFO 2017-06-27 21:49:23,458 main.py:51] epoch 2715, training loss: 11467.76, average training loss: 11939.15, base loss: 19792.54
[INFO 2017-06-27 21:49:24,433 main.py:51] epoch 2716, training loss: 11942.80, average training loss: 11937.64, base loss: 19791.40
[INFO 2017-06-27 21:49:25,404 main.py:51] epoch 2717, training loss: 13541.19, average training loss: 11938.93, base loss: 19794.35
[INFO 2017-06-27 21:49:26,375 main.py:51] epoch 2718, training loss: 12156.20, average training loss: 11939.10, base loss: 19796.60
[INFO 2017-06-27 21:49:27,350 main.py:51] epoch 2719, training loss: 12007.99, average training loss: 11937.23, base loss: 19793.73
[INFO 2017-06-27 21:49:28,326 main.py:51] epoch 2720, training loss: 10398.66, average training loss: 11934.30, base loss: 19787.85
[INFO 2017-06-27 21:49:29,306 main.py:51] epoch 2721, training loss: 11483.13, average training loss: 11932.88, base loss: 19785.67
[INFO 2017-06-27 21:49:30,279 main.py:51] epoch 2722, training loss: 11999.67, average training loss: 11932.49, base loss: 19785.82
[INFO 2017-06-27 21:49:31,254 main.py:51] epoch 2723, training loss: 11355.18, average training loss: 11930.56, base loss: 19782.94
[INFO 2017-06-27 21:49:32,228 main.py:51] epoch 2724, training loss: 10904.94, average training loss: 11928.18, base loss: 19778.63
[INFO 2017-06-27 21:49:33,206 main.py:51] epoch 2725, training loss: 11896.51, average training loss: 11927.77, base loss: 19780.29
[INFO 2017-06-27 21:49:34,182 main.py:51] epoch 2726, training loss: 10209.45, average training loss: 11925.40, base loss: 19777.53
[INFO 2017-06-27 21:49:35,160 main.py:51] epoch 2727, training loss: 11196.69, average training loss: 11924.84, base loss: 19778.12
[INFO 2017-06-27 21:49:36,221 main.py:51] epoch 2728, training loss: 13092.44, average training loss: 11924.12, base loss: 19778.53
[INFO 2017-06-27 21:49:37,280 main.py:51] epoch 2729, training loss: 11618.22, average training loss: 11924.57, base loss: 19780.95
[INFO 2017-06-27 21:49:38,307 main.py:51] epoch 2730, training loss: 12708.53, average training loss: 11925.56, base loss: 19783.99
[INFO 2017-06-27 21:49:39,402 main.py:51] epoch 2731, training loss: 11970.13, average training loss: 11923.66, base loss: 19781.55
[INFO 2017-06-27 21:49:40,420 main.py:51] epoch 2732, training loss: 12624.10, average training loss: 11924.80, base loss: 19785.73
[INFO 2017-06-27 21:49:41,419 main.py:51] epoch 2733, training loss: 11313.81, average training loss: 11923.88, base loss: 19784.49
[INFO 2017-06-27 21:49:42,533 main.py:51] epoch 2734, training loss: 12593.07, average training loss: 11923.01, base loss: 19784.92
[INFO 2017-06-27 21:49:43,536 main.py:51] epoch 2735, training loss: 12456.72, average training loss: 11922.79, base loss: 19786.03
[INFO 2017-06-27 21:49:44,572 main.py:51] epoch 2736, training loss: 10528.68, average training loss: 11922.11, base loss: 19787.31
[INFO 2017-06-27 21:49:45,559 main.py:51] epoch 2737, training loss: 11955.52, average training loss: 11921.90, base loss: 19787.81
[INFO 2017-06-27 21:49:46,663 main.py:51] epoch 2738, training loss: 12658.66, average training loss: 11922.09, base loss: 19789.86
[INFO 2017-06-27 21:49:47,687 main.py:51] epoch 2739, training loss: 11694.77, average training loss: 11920.37, base loss: 19788.42
[INFO 2017-06-27 21:49:48,685 main.py:51] epoch 2740, training loss: 10981.69, average training loss: 11917.42, base loss: 19784.19
[INFO 2017-06-27 21:49:49,666 main.py:51] epoch 2741, training loss: 11256.91, average training loss: 11914.68, base loss: 19779.40
[INFO 2017-06-27 21:49:50,641 main.py:51] epoch 2742, training loss: 11807.56, average training loss: 11914.96, base loss: 19781.66
[INFO 2017-06-27 21:49:51,613 main.py:51] epoch 2743, training loss: 12084.92, average training loss: 11915.25, base loss: 19783.84
[INFO 2017-06-27 21:49:52,585 main.py:51] epoch 2744, training loss: 11485.49, average training loss: 11914.91, base loss: 19784.21
[INFO 2017-06-27 21:49:53,559 main.py:51] epoch 2745, training loss: 10981.60, average training loss: 11915.67, base loss: 19788.94
[INFO 2017-06-27 21:49:54,528 main.py:51] epoch 2746, training loss: 11075.46, average training loss: 11915.40, base loss: 19789.45
[INFO 2017-06-27 21:49:55,604 main.py:51] epoch 2747, training loss: 12584.85, average training loss: 11914.60, base loss: 19789.64
[INFO 2017-06-27 21:49:56,578 main.py:51] epoch 2748, training loss: 12312.82, average training loss: 11913.72, base loss: 19791.01
[INFO 2017-06-27 21:49:57,549 main.py:51] epoch 2749, training loss: 11450.02, average training loss: 11912.89, base loss: 19790.23
[INFO 2017-06-27 21:49:58,520 main.py:51] epoch 2750, training loss: 11582.28, average training loss: 11912.16, base loss: 19790.26
[INFO 2017-06-27 21:49:59,494 main.py:51] epoch 2751, training loss: 11282.61, average training loss: 11911.45, base loss: 19790.73
[INFO 2017-06-27 21:50:00,555 main.py:51] epoch 2752, training loss: 12857.15, average training loss: 11912.98, base loss: 19796.29
[INFO 2017-06-27 21:50:01,565 main.py:51] epoch 2753, training loss: 10863.07, average training loss: 11911.50, base loss: 19796.65
[INFO 2017-06-27 21:50:02,535 main.py:51] epoch 2754, training loss: 10899.34, average training loss: 11910.94, base loss: 19797.76
[INFO 2017-06-27 21:50:03,523 main.py:51] epoch 2755, training loss: 10166.21, average training loss: 11910.00, base loss: 19796.63
[INFO 2017-06-27 21:50:04,497 main.py:51] epoch 2756, training loss: 10591.98, average training loss: 11907.52, base loss: 19795.29
[INFO 2017-06-27 21:50:05,471 main.py:51] epoch 2757, training loss: 11624.10, average training loss: 11906.27, base loss: 19795.03
[INFO 2017-06-27 21:50:06,493 main.py:51] epoch 2758, training loss: 12149.92, average training loss: 11905.89, base loss: 19794.06
[INFO 2017-06-27 21:50:07,530 main.py:51] epoch 2759, training loss: 10263.88, average training loss: 11904.46, base loss: 19792.56
[INFO 2017-06-27 21:50:08,555 main.py:51] epoch 2760, training loss: 11428.16, average training loss: 11903.92, base loss: 19794.87
[INFO 2017-06-27 21:50:09,621 main.py:51] epoch 2761, training loss: 10497.25, average training loss: 11902.10, base loss: 19791.71
[INFO 2017-06-27 21:50:10,646 main.py:51] epoch 2762, training loss: 12942.60, average training loss: 11901.97, base loss: 19792.38
[INFO 2017-06-27 21:50:11,723 main.py:51] epoch 2763, training loss: 12115.80, average training loss: 11900.74, base loss: 19791.90
[INFO 2017-06-27 21:50:12,820 main.py:51] epoch 2764, training loss: 11307.07, average training loss: 11900.00, base loss: 19790.67
[INFO 2017-06-27 21:50:13,815 main.py:51] epoch 2765, training loss: 10059.17, average training loss: 11897.83, base loss: 19787.95
[INFO 2017-06-27 21:50:14,904 main.py:51] epoch 2766, training loss: 10584.08, average training loss: 11896.63, base loss: 19787.71
[INFO 2017-06-27 21:50:15,885 main.py:51] epoch 2767, training loss: 10559.03, average training loss: 11894.95, base loss: 19785.01
[INFO 2017-06-27 21:50:16,954 main.py:51] epoch 2768, training loss: 12329.48, average training loss: 11894.17, base loss: 19786.67
[INFO 2017-06-27 21:50:17,952 main.py:51] epoch 2769, training loss: 10997.98, average training loss: 11891.03, base loss: 19782.93
[INFO 2017-06-27 21:50:18,975 main.py:51] epoch 2770, training loss: 12636.31, average training loss: 11890.46, base loss: 19784.03
[INFO 2017-06-27 21:50:19,985 main.py:51] epoch 2771, training loss: 10814.08, average training loss: 11890.04, base loss: 19785.05
[INFO 2017-06-27 21:50:20,962 main.py:51] epoch 2772, training loss: 10815.61, average training loss: 11887.50, base loss: 19780.73
[INFO 2017-06-27 21:50:22,034 main.py:51] epoch 2773, training loss: 9585.09, average training loss: 11884.77, base loss: 19776.19
[INFO 2017-06-27 21:50:23,099 main.py:51] epoch 2774, training loss: 11279.72, average training loss: 11883.45, base loss: 19774.47
[INFO 2017-06-27 21:50:24,100 main.py:51] epoch 2775, training loss: 10986.16, average training loss: 11882.39, base loss: 19774.20
[INFO 2017-06-27 21:50:25,165 main.py:51] epoch 2776, training loss: 11941.21, average training loss: 11880.33, base loss: 19772.17
[INFO 2017-06-27 21:50:26,211 main.py:51] epoch 2777, training loss: 10338.74, average training loss: 11878.99, base loss: 19771.87
[INFO 2017-06-27 21:50:27,242 main.py:51] epoch 2778, training loss: 10879.83, average training loss: 11878.33, base loss: 19769.89
[INFO 2017-06-27 21:50:28,242 main.py:51] epoch 2779, training loss: 11763.39, average training loss: 11876.82, base loss: 19766.32
[INFO 2017-06-27 21:50:29,217 main.py:51] epoch 2780, training loss: 12396.64, average training loss: 11877.09, base loss: 19767.81
[INFO 2017-06-27 21:50:30,196 main.py:51] epoch 2781, training loss: 11084.85, average training loss: 11875.87, base loss: 19766.90
[INFO 2017-06-27 21:50:31,166 main.py:51] epoch 2782, training loss: 10657.84, average training loss: 11874.28, base loss: 19765.47
[INFO 2017-06-27 21:50:32,141 main.py:51] epoch 2783, training loss: 9859.95, average training loss: 11871.73, base loss: 19761.78
[INFO 2017-06-27 21:50:33,111 main.py:51] epoch 2784, training loss: 10592.10, average training loss: 11869.82, base loss: 19758.25
[INFO 2017-06-27 21:50:34,086 main.py:51] epoch 2785, training loss: 11795.06, average training loss: 11870.69, base loss: 19762.50
[INFO 2017-06-27 21:50:35,062 main.py:51] epoch 2786, training loss: 12316.18, average training loss: 11871.41, base loss: 19765.44
[INFO 2017-06-27 21:50:36,037 main.py:51] epoch 2787, training loss: 10278.84, average training loss: 11871.46, base loss: 19767.82
[INFO 2017-06-27 21:50:37,008 main.py:51] epoch 2788, training loss: 11444.12, average training loss: 11871.80, base loss: 19771.04
[INFO 2017-06-27 21:50:37,982 main.py:51] epoch 2789, training loss: 11491.43, average training loss: 11870.62, base loss: 19771.46
[INFO 2017-06-27 21:50:38,958 main.py:51] epoch 2790, training loss: 12777.57, average training loss: 11869.97, base loss: 19772.87
[INFO 2017-06-27 21:50:39,929 main.py:51] epoch 2791, training loss: 11195.39, average training loss: 11871.77, base loss: 19777.85
[INFO 2017-06-27 21:50:40,901 main.py:51] epoch 2792, training loss: 10574.17, average training loss: 11870.72, base loss: 19777.64
[INFO 2017-06-27 21:50:41,874 main.py:51] epoch 2793, training loss: 10577.23, average training loss: 11869.45, base loss: 19776.06
[INFO 2017-06-27 21:50:42,846 main.py:51] epoch 2794, training loss: 11317.50, average training loss: 11869.10, base loss: 19776.20
[INFO 2017-06-27 21:50:43,816 main.py:51] epoch 2795, training loss: 11186.99, average training loss: 11869.30, base loss: 19779.24
[INFO 2017-06-27 21:50:44,787 main.py:51] epoch 2796, training loss: 10936.13, average training loss: 11867.36, base loss: 19777.92
[INFO 2017-06-27 21:50:45,759 main.py:51] epoch 2797, training loss: 13485.79, average training loss: 11868.05, base loss: 19780.28
[INFO 2017-06-27 21:50:46,731 main.py:51] epoch 2798, training loss: 10742.35, average training loss: 11865.94, base loss: 19777.53
[INFO 2017-06-27 21:50:47,703 main.py:51] epoch 2799, training loss: 11469.71, average training loss: 11866.40, base loss: 19780.06
[INFO 2017-06-27 21:50:47,704 main.py:53] epoch 2799, testing
[INFO 2017-06-27 21:50:52,007 main.py:105] average testing loss: 11603.79, base loss: 19784.31
[INFO 2017-06-27 21:50:52,008 main.py:106] improve_loss: 8180.51, improve_percent: 0.41
[INFO 2017-06-27 21:50:52,008 main.py:76] current best improved percent: 0.42
[INFO 2017-06-27 21:50:52,979 main.py:51] epoch 2800, training loss: 10416.23, average training loss: 11864.17, base loss: 19775.36
[INFO 2017-06-27 21:50:53,952 main.py:51] epoch 2801, training loss: 11016.98, average training loss: 11863.62, base loss: 19774.35
[INFO 2017-06-27 21:50:54,924 main.py:51] epoch 2802, training loss: 11370.81, average training loss: 11861.86, base loss: 19771.09
[INFO 2017-06-27 21:50:55,898 main.py:51] epoch 2803, training loss: 10180.21, average training loss: 11858.78, base loss: 19767.40
[INFO 2017-06-27 21:50:56,874 main.py:51] epoch 2804, training loss: 12389.67, average training loss: 11859.27, base loss: 19767.43
[INFO 2017-06-27 21:50:57,848 main.py:51] epoch 2805, training loss: 12424.91, average training loss: 11860.15, base loss: 19769.81
[INFO 2017-06-27 21:50:58,823 main.py:51] epoch 2806, training loss: 11171.76, average training loss: 11859.70, base loss: 19769.14
[INFO 2017-06-27 21:50:59,803 main.py:51] epoch 2807, training loss: 10978.43, average training loss: 11857.71, base loss: 19763.30
[INFO 2017-06-27 21:51:00,892 main.py:51] epoch 2808, training loss: 11527.74, average training loss: 11856.61, base loss: 19762.56
[INFO 2017-06-27 21:51:01,995 main.py:51] epoch 2809, training loss: 12823.56, average training loss: 11857.76, base loss: 19766.66
[INFO 2017-06-27 21:51:02,973 main.py:51] epoch 2810, training loss: 11401.69, average training loss: 11854.86, base loss: 19762.91
[INFO 2017-06-27 21:51:03,953 main.py:51] epoch 2811, training loss: 11116.51, average training loss: 11853.82, base loss: 19764.13
[INFO 2017-06-27 21:51:04,928 main.py:51] epoch 2812, training loss: 12421.88, average training loss: 11853.36, base loss: 19764.69
[INFO 2017-06-27 21:51:05,902 main.py:51] epoch 2813, training loss: 12191.65, average training loss: 11855.41, base loss: 19770.72
[INFO 2017-06-27 21:51:06,878 main.py:51] epoch 2814, training loss: 11869.71, average training loss: 11855.43, base loss: 19772.38
[INFO 2017-06-27 21:51:07,854 main.py:51] epoch 2815, training loss: 11783.81, average training loss: 11854.86, base loss: 19770.92
[INFO 2017-06-27 21:51:08,874 main.py:51] epoch 2816, training loss: 12345.39, average training loss: 11856.09, base loss: 19775.32
[INFO 2017-06-27 21:51:09,891 main.py:51] epoch 2817, training loss: 10408.95, average training loss: 11856.06, base loss: 19777.34
[INFO 2017-06-27 21:51:10,865 main.py:51] epoch 2818, training loss: 11581.04, average training loss: 11856.10, base loss: 19778.81
[INFO 2017-06-27 21:51:11,944 main.py:51] epoch 2819, training loss: 12382.78, average training loss: 11856.78, base loss: 19782.58
[INFO 2017-06-27 21:51:12,963 main.py:51] epoch 2820, training loss: 12095.79, average training loss: 11857.17, base loss: 19785.03
[INFO 2017-06-27 21:51:13,960 main.py:51] epoch 2821, training loss: 11835.80, average training loss: 11856.95, base loss: 19785.49
[INFO 2017-06-27 21:51:14,961 main.py:51] epoch 2822, training loss: 11703.15, average training loss: 11856.79, base loss: 19785.16
[INFO 2017-06-27 21:51:16,037 main.py:51] epoch 2823, training loss: 13307.75, average training loss: 11858.94, base loss: 19790.17
[INFO 2017-06-27 21:51:17,083 main.py:51] epoch 2824, training loss: 11404.87, average training loss: 11858.62, base loss: 19789.26
[INFO 2017-06-27 21:51:18,054 main.py:51] epoch 2825, training loss: 10933.24, average training loss: 11857.67, base loss: 19788.66
[INFO 2017-06-27 21:51:19,041 main.py:51] epoch 2826, training loss: 11323.01, average training loss: 11855.33, base loss: 19785.04
[INFO 2017-06-27 21:51:20,022 main.py:51] epoch 2827, training loss: 10530.04, average training loss: 11854.22, base loss: 19783.36
[INFO 2017-06-27 21:51:20,997 main.py:51] epoch 2828, training loss: 11612.64, average training loss: 11853.69, base loss: 19782.00
[INFO 2017-06-27 21:51:21,971 main.py:51] epoch 2829, training loss: 10400.72, average training loss: 11852.38, base loss: 19780.49
[INFO 2017-06-27 21:51:22,943 main.py:51] epoch 2830, training loss: 10258.18, average training loss: 11850.78, base loss: 19779.80
[INFO 2017-06-27 21:51:23,918 main.py:51] epoch 2831, training loss: 12978.68, average training loss: 11852.22, base loss: 19783.28
[INFO 2017-06-27 21:51:24,892 main.py:51] epoch 2832, training loss: 10628.28, average training loss: 11848.98, base loss: 19778.95
[INFO 2017-06-27 21:51:25,868 main.py:51] epoch 2833, training loss: 11519.96, average training loss: 11847.78, base loss: 19778.59
[INFO 2017-06-27 21:51:26,840 main.py:51] epoch 2834, training loss: 11767.67, average training loss: 11846.54, base loss: 19776.31
[INFO 2017-06-27 21:51:27,812 main.py:51] epoch 2835, training loss: 11717.44, average training loss: 11844.64, base loss: 19775.17
[INFO 2017-06-27 21:51:28,790 main.py:51] epoch 2836, training loss: 11391.54, average training loss: 11843.57, base loss: 19774.91
[INFO 2017-06-27 21:51:29,768 main.py:51] epoch 2837, training loss: 10908.87, average training loss: 11842.02, base loss: 19773.05
[INFO 2017-06-27 21:51:30,742 main.py:51] epoch 2838, training loss: 9826.73, average training loss: 11841.01, base loss: 19771.44
[INFO 2017-06-27 21:51:31,717 main.py:51] epoch 2839, training loss: 11098.80, average training loss: 11839.97, base loss: 19771.38
[INFO 2017-06-27 21:51:32,793 main.py:51] epoch 2840, training loss: 11698.39, average training loss: 11839.30, base loss: 19771.46
[INFO 2017-06-27 21:51:33,862 main.py:51] epoch 2841, training loss: 11710.40, average training loss: 11839.18, base loss: 19774.01
[INFO 2017-06-27 21:51:34,883 main.py:51] epoch 2842, training loss: 10419.10, average training loss: 11836.94, base loss: 19772.46
[INFO 2017-06-27 21:51:35,866 main.py:51] epoch 2843, training loss: 13407.62, average training loss: 11838.77, base loss: 19776.53
[INFO 2017-06-27 21:51:36,849 main.py:51] epoch 2844, training loss: 10440.56, average training loss: 11836.99, base loss: 19774.27
[INFO 2017-06-27 21:51:37,825 main.py:51] epoch 2845, training loss: 12148.76, average training loss: 11835.20, base loss: 19771.96
[INFO 2017-06-27 21:51:38,794 main.py:51] epoch 2846, training loss: 9995.61, average training loss: 11833.54, base loss: 19769.26
[INFO 2017-06-27 21:51:39,764 main.py:51] epoch 2847, training loss: 11522.20, average training loss: 11832.97, base loss: 19770.24
[INFO 2017-06-27 21:51:40,741 main.py:51] epoch 2848, training loss: 11296.77, average training loss: 11832.30, base loss: 19771.00
[INFO 2017-06-27 21:51:41,716 main.py:51] epoch 2849, training loss: 11075.78, average training loss: 11831.99, base loss: 19771.89
[INFO 2017-06-27 21:51:42,694 main.py:51] epoch 2850, training loss: 11464.01, average training loss: 11830.54, base loss: 19770.60
[INFO 2017-06-27 21:51:43,719 main.py:51] epoch 2851, training loss: 11171.56, average training loss: 11830.88, base loss: 19772.89
[INFO 2017-06-27 21:51:44,717 main.py:51] epoch 2852, training loss: 11355.44, average training loss: 11831.11, base loss: 19773.88
[INFO 2017-06-27 21:51:45,691 main.py:51] epoch 2853, training loss: 11801.01, average training loss: 11829.90, base loss: 19771.34
[INFO 2017-06-27 21:51:46,770 main.py:51] epoch 2854, training loss: 12335.36, average training loss: 11831.10, base loss: 19774.30
[INFO 2017-06-27 21:51:47,748 main.py:51] epoch 2855, training loss: 11656.43, average training loss: 11830.31, base loss: 19773.73
[INFO 2017-06-27 21:51:48,837 main.py:51] epoch 2856, training loss: 10795.40, average training loss: 11827.32, base loss: 19769.39
[INFO 2017-06-27 21:51:49,818 main.py:51] epoch 2857, training loss: 11364.27, average training loss: 11825.99, base loss: 19768.34
[INFO 2017-06-27 21:51:50,794 main.py:51] epoch 2858, training loss: 12435.90, average training loss: 11825.84, base loss: 19770.44
[INFO 2017-06-27 21:51:51,769 main.py:51] epoch 2859, training loss: 13522.15, average training loss: 11826.95, base loss: 19774.21
[INFO 2017-06-27 21:51:52,741 main.py:51] epoch 2860, training loss: 10498.64, average training loss: 11824.09, base loss: 19769.56
[INFO 2017-06-27 21:51:53,712 main.py:51] epoch 2861, training loss: 12216.46, average training loss: 11824.63, base loss: 19772.64
[INFO 2017-06-27 21:51:54,688 main.py:51] epoch 2862, training loss: 12452.70, average training loss: 11825.52, base loss: 19777.06
[INFO 2017-06-27 21:51:55,658 main.py:51] epoch 2863, training loss: 12681.73, average training loss: 11825.65, base loss: 19777.21
[INFO 2017-06-27 21:51:56,630 main.py:51] epoch 2864, training loss: 12496.16, average training loss: 11826.22, base loss: 19779.86
[INFO 2017-06-27 21:51:57,605 main.py:51] epoch 2865, training loss: 11357.63, average training loss: 11825.52, base loss: 19778.39
[INFO 2017-06-27 21:51:58,576 main.py:51] epoch 2866, training loss: 11361.79, average training loss: 11825.07, base loss: 19778.57
[INFO 2017-06-27 21:51:59,548 main.py:51] epoch 2867, training loss: 10429.74, average training loss: 11822.36, base loss: 19773.58
[INFO 2017-06-27 21:52:00,520 main.py:51] epoch 2868, training loss: 12118.19, average training loss: 11822.77, base loss: 19776.47
[INFO 2017-06-27 21:52:01,494 main.py:51] epoch 2869, training loss: 11698.63, average training loss: 11820.61, base loss: 19773.33
[INFO 2017-06-27 21:52:02,466 main.py:51] epoch 2870, training loss: 14334.17, average training loss: 11823.14, base loss: 19779.96
[INFO 2017-06-27 21:52:03,447 main.py:51] epoch 2871, training loss: 12297.17, average training loss: 11824.03, base loss: 19782.80
[INFO 2017-06-27 21:52:04,421 main.py:51] epoch 2872, training loss: 11674.72, average training loss: 11823.88, base loss: 19785.03
[INFO 2017-06-27 21:52:05,390 main.py:51] epoch 2873, training loss: 10195.29, average training loss: 11820.66, base loss: 19779.60
[INFO 2017-06-27 21:52:06,367 main.py:51] epoch 2874, training loss: 10378.51, average training loss: 11817.90, base loss: 19774.81
[INFO 2017-06-27 21:52:07,442 main.py:51] epoch 2875, training loss: 11040.37, average training loss: 11816.68, base loss: 19772.48
[INFO 2017-06-27 21:52:08,598 main.py:51] epoch 2876, training loss: 10770.75, average training loss: 11814.49, base loss: 19769.02
[INFO 2017-06-27 21:52:09,699 main.py:51] epoch 2877, training loss: 12443.22, average training loss: 11814.62, base loss: 19772.23
[INFO 2017-06-27 21:52:10,760 main.py:51] epoch 2878, training loss: 13007.07, average training loss: 11812.98, base loss: 19769.69
[INFO 2017-06-27 21:52:11,770 main.py:51] epoch 2879, training loss: 10789.47, average training loss: 11809.81, base loss: 19764.27
[INFO 2017-06-27 21:52:12,817 main.py:51] epoch 2880, training loss: 11603.89, average training loss: 11810.58, base loss: 19768.44
[INFO 2017-06-27 21:52:13,887 main.py:51] epoch 2881, training loss: 10640.12, average training loss: 11808.43, base loss: 19766.30
[INFO 2017-06-27 21:52:14,976 main.py:51] epoch 2882, training loss: 10712.83, average training loss: 11803.49, base loss: 19758.32
[INFO 2017-06-27 21:52:16,033 main.py:51] epoch 2883, training loss: 10426.96, average training loss: 11800.88, base loss: 19753.47
[INFO 2017-06-27 21:52:17,031 main.py:51] epoch 2884, training loss: 12463.14, average training loss: 11802.27, base loss: 19757.35
[INFO 2017-06-27 21:52:18,005 main.py:51] epoch 2885, training loss: 11091.04, average training loss: 11800.91, base loss: 19754.53
[INFO 2017-06-27 21:52:18,984 main.py:51] epoch 2886, training loss: 11001.20, average training loss: 11799.40, base loss: 19752.26
[INFO 2017-06-27 21:52:19,982 main.py:51] epoch 2887, training loss: 11797.97, average training loss: 11799.35, base loss: 19753.57
[INFO 2017-06-27 21:52:20,953 main.py:51] epoch 2888, training loss: 11062.62, average training loss: 11796.80, base loss: 19750.24
[INFO 2017-06-27 21:52:21,923 main.py:51] epoch 2889, training loss: 10710.34, average training loss: 11797.04, base loss: 19752.80
[INFO 2017-06-27 21:52:22,893 main.py:51] epoch 2890, training loss: 10094.03, average training loss: 11795.20, base loss: 19751.25
[INFO 2017-06-27 21:52:23,866 main.py:51] epoch 2891, training loss: 11688.87, average training loss: 11795.58, base loss: 19753.57
[INFO 2017-06-27 21:52:24,836 main.py:51] epoch 2892, training loss: 11547.93, average training loss: 11795.86, base loss: 19756.30
[INFO 2017-06-27 21:52:25,813 main.py:51] epoch 2893, training loss: 12364.46, average training loss: 11796.34, base loss: 19759.59
[INFO 2017-06-27 21:52:26,784 main.py:51] epoch 2894, training loss: 10675.08, average training loss: 11795.35, base loss: 19758.59
[INFO 2017-06-27 21:52:27,758 main.py:51] epoch 2895, training loss: 11724.46, average training loss: 11794.09, base loss: 19757.38
[INFO 2017-06-27 21:52:28,730 main.py:51] epoch 2896, training loss: 12170.63, average training loss: 11795.65, base loss: 19760.10
[INFO 2017-06-27 21:52:29,705 main.py:51] epoch 2897, training loss: 10891.53, average training loss: 11793.61, base loss: 19757.87
[INFO 2017-06-27 21:52:30,677 main.py:51] epoch 2898, training loss: 11194.43, average training loss: 11793.41, base loss: 19760.30
[INFO 2017-06-27 21:52:31,652 main.py:51] epoch 2899, training loss: 11681.23, average training loss: 11794.08, base loss: 19761.74
[INFO 2017-06-27 21:52:31,652 main.py:53] epoch 2899, testing
[INFO 2017-06-27 21:52:36,005 main.py:105] average testing loss: 11418.61, base loss: 19593.79
[INFO 2017-06-27 21:52:36,005 main.py:106] improve_loss: 8175.19, improve_percent: 0.42
[INFO 2017-06-27 21:52:36,006 main.py:76] current best improved percent: 0.42
[INFO 2017-06-27 21:52:36,982 main.py:51] epoch 2900, training loss: 12607.17, average training loss: 11792.06, base loss: 19760.53
[INFO 2017-06-27 21:52:37,958 main.py:51] epoch 2901, training loss: 12597.35, average training loss: 11793.42, base loss: 19765.27
[INFO 2017-06-27 21:52:38,930 main.py:51] epoch 2902, training loss: 11236.65, average training loss: 11791.99, base loss: 19762.66
[INFO 2017-06-27 21:52:39,903 main.py:51] epoch 2903, training loss: 10196.80, average training loss: 11790.74, base loss: 19760.60
[INFO 2017-06-27 21:52:40,882 main.py:51] epoch 2904, training loss: 12225.09, average training loss: 11790.45, base loss: 19763.84
[INFO 2017-06-27 21:52:41,853 main.py:51] epoch 2905, training loss: 10907.34, average training loss: 11790.18, base loss: 19764.34
[INFO 2017-06-27 21:52:42,824 main.py:51] epoch 2906, training loss: 12915.42, average training loss: 11792.21, base loss: 19769.10
[INFO 2017-06-27 21:52:43,797 main.py:51] epoch 2907, training loss: 11574.71, average training loss: 11790.52, base loss: 19766.41
[INFO 2017-06-27 21:52:44,769 main.py:51] epoch 2908, training loss: 11559.43, average training loss: 11791.06, base loss: 19768.82
[INFO 2017-06-27 21:52:45,745 main.py:51] epoch 2909, training loss: 11173.58, average training loss: 11790.43, base loss: 19769.97
[INFO 2017-06-27 21:52:46,723 main.py:51] epoch 2910, training loss: 11708.61, average training loss: 11790.08, base loss: 19770.10
[INFO 2017-06-27 21:52:47,803 main.py:51] epoch 2911, training loss: 11528.09, average training loss: 11790.23, base loss: 19773.70
[INFO 2017-06-27 21:52:48,793 main.py:51] epoch 2912, training loss: 11365.39, average training loss: 11789.13, base loss: 19773.29
[INFO 2017-06-27 21:52:49,774 main.py:51] epoch 2913, training loss: 10410.27, average training loss: 11788.35, base loss: 19772.40
[INFO 2017-06-27 21:52:50,747 main.py:51] epoch 2914, training loss: 10198.00, average training loss: 11786.49, base loss: 19770.21
[INFO 2017-06-27 21:52:51,719 main.py:51] epoch 2915, training loss: 12894.50, average training loss: 11787.61, base loss: 19774.74
[INFO 2017-06-27 21:52:52,696 main.py:51] epoch 2916, training loss: 10378.99, average training loss: 11785.86, base loss: 19771.97
[INFO 2017-06-27 21:52:53,669 main.py:51] epoch 2917, training loss: 9567.59, average training loss: 11782.70, base loss: 19766.49
[INFO 2017-06-27 21:52:54,646 main.py:51] epoch 2918, training loss: 11334.51, average training loss: 11781.29, base loss: 19765.15
[INFO 2017-06-27 21:52:55,618 main.py:51] epoch 2919, training loss: 9728.92, average training loss: 11777.74, base loss: 19758.71
[INFO 2017-06-27 21:52:56,591 main.py:51] epoch 2920, training loss: 12743.64, average training loss: 11778.84, base loss: 19762.51
[INFO 2017-06-27 21:52:57,565 main.py:51] epoch 2921, training loss: 11590.02, average training loss: 11778.64, base loss: 19763.70
[INFO 2017-06-27 21:52:58,537 main.py:51] epoch 2922, training loss: 10302.50, average training loss: 11776.04, base loss: 19759.99
[INFO 2017-06-27 21:52:59,510 main.py:51] epoch 2923, training loss: 11599.38, average training loss: 11776.86, base loss: 19762.89
[INFO 2017-06-27 21:53:00,483 main.py:51] epoch 2924, training loss: 11137.60, average training loss: 11776.25, base loss: 19763.05
[INFO 2017-06-27 21:53:01,458 main.py:51] epoch 2925, training loss: 12007.84, average training loss: 11776.82, base loss: 19766.14
[INFO 2017-06-27 21:53:02,484 main.py:51] epoch 2926, training loss: 11552.50, average training loss: 11777.40, base loss: 19768.14
[INFO 2017-06-27 21:53:03,485 main.py:51] epoch 2927, training loss: 11514.34, average training loss: 11777.14, base loss: 19769.67
[INFO 2017-06-27 21:53:04,467 main.py:51] epoch 2928, training loss: 10777.74, average training loss: 11776.32, base loss: 19771.03
[INFO 2017-06-27 21:53:05,441 main.py:51] epoch 2929, training loss: 10973.28, average training loss: 11775.39, base loss: 19771.03
[INFO 2017-06-27 21:53:06,427 main.py:51] epoch 2930, training loss: 12554.67, average training loss: 11775.45, base loss: 19771.18
[INFO 2017-06-27 21:53:07,404 main.py:51] epoch 2931, training loss: 10431.66, average training loss: 11773.21, base loss: 19767.39
[INFO 2017-06-27 21:53:08,375 main.py:51] epoch 2932, training loss: 10681.85, average training loss: 11772.10, base loss: 19764.88
[INFO 2017-06-27 21:53:09,348 main.py:51] epoch 2933, training loss: 11711.76, average training loss: 11773.12, base loss: 19768.17
[INFO 2017-06-27 21:53:10,324 main.py:51] epoch 2934, training loss: 11609.25, average training loss: 11771.85, base loss: 19767.38
[INFO 2017-06-27 21:53:11,401 main.py:51] epoch 2935, training loss: 12781.90, average training loss: 11769.95, base loss: 19767.15
[INFO 2017-06-27 21:53:12,414 main.py:51] epoch 2936, training loss: 9626.38, average training loss: 11768.48, base loss: 19765.67
[INFO 2017-06-27 21:53:13,387 main.py:51] epoch 2937, training loss: 9829.08, average training loss: 11766.11, base loss: 19763.10
[INFO 2017-06-27 21:53:14,408 main.py:51] epoch 2938, training loss: 12628.59, average training loss: 11766.47, base loss: 19766.17
[INFO 2017-06-27 21:53:15,405 main.py:51] epoch 2939, training loss: 11597.53, average training loss: 11767.26, base loss: 19769.85
[INFO 2017-06-27 21:53:16,405 main.py:51] epoch 2940, training loss: 12381.97, average training loss: 11768.75, base loss: 19774.87
[INFO 2017-06-27 21:53:17,378 main.py:51] epoch 2941, training loss: 11810.89, average training loss: 11768.23, base loss: 19776.85
[INFO 2017-06-27 21:53:18,350 main.py:51] epoch 2942, training loss: 11426.09, average training loss: 11767.82, base loss: 19777.55
[INFO 2017-06-27 21:53:19,324 main.py:51] epoch 2943, training loss: 12322.29, average training loss: 11767.32, base loss: 19778.84
[INFO 2017-06-27 21:53:20,405 main.py:51] epoch 2944, training loss: 11571.12, average training loss: 11765.99, base loss: 19779.36
[INFO 2017-06-27 21:53:21,396 main.py:51] epoch 2945, training loss: 11997.69, average training loss: 11765.21, base loss: 19779.46
[INFO 2017-06-27 21:53:22,387 main.py:51] epoch 2946, training loss: 10823.36, average training loss: 11762.80, base loss: 19776.25
[INFO 2017-06-27 21:53:23,364 main.py:51] epoch 2947, training loss: 12938.00, average training loss: 11764.47, base loss: 19782.18
[INFO 2017-06-27 21:53:24,336 main.py:51] epoch 2948, training loss: 10882.62, average training loss: 11763.89, base loss: 19782.35
[INFO 2017-06-27 21:53:25,307 main.py:51] epoch 2949, training loss: 12096.22, average training loss: 11763.25, base loss: 19782.43
[INFO 2017-06-27 21:53:26,278 main.py:51] epoch 2950, training loss: 12002.30, average training loss: 11761.04, base loss: 19779.08
[INFO 2017-06-27 21:53:27,250 main.py:51] epoch 2951, training loss: 10501.26, average training loss: 11758.27, base loss: 19773.36
[INFO 2017-06-27 21:53:28,225 main.py:51] epoch 2952, training loss: 10171.44, average training loss: 11756.98, base loss: 19773.29
[INFO 2017-06-27 21:53:29,196 main.py:51] epoch 2953, training loss: 12518.04, average training loss: 11757.00, base loss: 19775.39
[INFO 2017-06-27 21:53:30,166 main.py:51] epoch 2954, training loss: 11542.88, average training loss: 11755.92, base loss: 19773.63
[INFO 2017-06-27 21:53:31,141 main.py:51] epoch 2955, training loss: 13210.79, average training loss: 11755.94, base loss: 19775.31
[INFO 2017-06-27 21:53:32,112 main.py:51] epoch 2956, training loss: 11908.29, average training loss: 11755.63, base loss: 19777.07
[INFO 2017-06-27 21:53:33,085 main.py:51] epoch 2957, training loss: 11352.90, average training loss: 11755.24, base loss: 19777.83
[INFO 2017-06-27 21:53:34,057 main.py:51] epoch 2958, training loss: 11311.53, average training loss: 11753.89, base loss: 19774.75
[INFO 2017-06-27 21:53:35,136 main.py:51] epoch 2959, training loss: 12944.55, average training loss: 11755.32, base loss: 19778.76
[INFO 2017-06-27 21:53:36,111 main.py:51] epoch 2960, training loss: 10837.92, average training loss: 11753.72, base loss: 19775.88
[INFO 2017-06-27 21:53:37,090 main.py:51] epoch 2961, training loss: 11634.78, average training loss: 11753.26, base loss: 19776.60
[INFO 2017-06-27 21:53:38,060 main.py:51] epoch 2962, training loss: 10487.72, average training loss: 11750.96, base loss: 19772.98
[INFO 2017-06-27 21:53:39,029 main.py:51] epoch 2963, training loss: 12856.10, average training loss: 11750.70, base loss: 19773.75
[INFO 2017-06-27 21:53:40,005 main.py:51] epoch 2964, training loss: 10779.32, average training loss: 11751.42, base loss: 19774.64
[INFO 2017-06-27 21:53:40,980 main.py:51] epoch 2965, training loss: 12782.44, average training loss: 11752.47, base loss: 19778.99
[INFO 2017-06-27 21:53:41,953 main.py:51] epoch 2966, training loss: 10929.78, average training loss: 11750.78, base loss: 19777.60
[INFO 2017-06-27 21:53:42,924 main.py:51] epoch 2967, training loss: 12298.92, average training loss: 11751.43, base loss: 19780.37
[INFO 2017-06-27 21:53:43,898 main.py:51] epoch 2968, training loss: 10955.88, average training loss: 11747.54, base loss: 19774.84
[INFO 2017-06-27 21:53:44,926 main.py:51] epoch 2969, training loss: 11319.37, average training loss: 11746.58, base loss: 19774.81
[INFO 2017-06-27 21:53:45,951 main.py:51] epoch 2970, training loss: 12062.39, average training loss: 11746.41, base loss: 19775.73
[INFO 2017-06-27 21:53:46,953 main.py:51] epoch 2971, training loss: 11589.62, average training loss: 11747.23, base loss: 19780.30
[INFO 2017-06-27 21:53:47,928 main.py:51] epoch 2972, training loss: 11359.20, average training loss: 11747.25, base loss: 19780.96
[INFO 2017-06-27 21:53:48,902 main.py:51] epoch 2973, training loss: 10893.90, average training loss: 11745.22, base loss: 19778.75
[INFO 2017-06-27 21:53:49,876 main.py:51] epoch 2974, training loss: 10764.82, average training loss: 11743.86, base loss: 19777.17
[INFO 2017-06-27 21:53:50,850 main.py:51] epoch 2975, training loss: 10438.73, average training loss: 11743.21, base loss: 19777.02
[INFO 2017-06-27 21:53:51,820 main.py:51] epoch 2976, training loss: 13367.15, average training loss: 11744.76, base loss: 19781.51
[INFO 2017-06-27 21:53:52,794 main.py:51] epoch 2977, training loss: 10625.08, average training loss: 11743.37, base loss: 19778.63
[INFO 2017-06-27 21:53:53,769 main.py:51] epoch 2978, training loss: 12519.44, average training loss: 11743.66, base loss: 19780.58
[INFO 2017-06-27 21:53:54,744 main.py:51] epoch 2979, training loss: 10975.65, average training loss: 11742.30, base loss: 19779.64
[INFO 2017-06-27 21:53:55,713 main.py:51] epoch 2980, training loss: 11123.84, average training loss: 11742.42, base loss: 19780.29
[INFO 2017-06-27 21:53:56,686 main.py:51] epoch 2981, training loss: 11596.14, average training loss: 11742.72, base loss: 19781.06
[INFO 2017-06-27 21:53:57,660 main.py:51] epoch 2982, training loss: 11203.64, average training loss: 11741.51, base loss: 19778.70
[INFO 2017-06-27 21:53:58,632 main.py:51] epoch 2983, training loss: 12407.08, average training loss: 11741.03, base loss: 19779.32
[INFO 2017-06-27 21:53:59,607 main.py:51] epoch 2984, training loss: 13489.72, average training loss: 11743.44, base loss: 19785.34
[INFO 2017-06-27 21:54:00,625 main.py:51] epoch 2985, training loss: 11992.70, average training loss: 11743.70, base loss: 19787.08
[INFO 2017-06-27 21:54:01,617 main.py:51] epoch 2986, training loss: 12476.50, average training loss: 11743.78, base loss: 19786.98
[INFO 2017-06-27 21:54:02,714 main.py:51] epoch 2987, training loss: 13644.46, average training loss: 11744.71, base loss: 19789.75
[INFO 2017-06-27 21:54:03,732 main.py:51] epoch 2988, training loss: 11915.12, average training loss: 11744.50, base loss: 19791.65
[INFO 2017-06-27 21:54:04,814 main.py:51] epoch 2989, training loss: 10397.75, average training loss: 11743.24, base loss: 19791.05
[INFO 2017-06-27 21:54:05,805 main.py:51] epoch 2990, training loss: 12887.86, average training loss: 11745.14, base loss: 19796.33
[INFO 2017-06-27 21:54:06,788 main.py:51] epoch 2991, training loss: 11103.59, average training loss: 11743.34, base loss: 19794.17
[INFO 2017-06-27 21:54:07,758 main.py:51] epoch 2992, training loss: 11829.08, average training loss: 11743.98, base loss: 19796.28
[INFO 2017-06-27 21:54:08,730 main.py:51] epoch 2993, training loss: 12772.20, average training loss: 11745.59, base loss: 19800.25
[INFO 2017-06-27 21:54:09,705 main.py:51] epoch 2994, training loss: 9698.26, average training loss: 11743.52, base loss: 19795.41
[INFO 2017-06-27 21:54:10,678 main.py:51] epoch 2995, training loss: 12122.14, average training loss: 11741.45, base loss: 19792.00
[INFO 2017-06-27 21:54:11,709 main.py:51] epoch 2996, training loss: 11190.00, average training loss: 11740.11, base loss: 19790.45
[INFO 2017-06-27 21:54:12,710 main.py:51] epoch 2997, training loss: 12628.25, average training loss: 11739.65, base loss: 19791.53
[INFO 2017-06-27 21:54:13,682 main.py:51] epoch 2998, training loss: 9825.46, average training loss: 11737.76, base loss: 19787.48
[INFO 2017-06-27 21:54:14,793 main.py:51] epoch 2999, training loss: 10645.68, average training loss: 11736.75, base loss: 19786.76
[INFO 2017-06-27 21:54:14,793 main.py:53] epoch 2999, testing
[INFO 2017-06-27 21:54:19,156 main.py:105] average testing loss: 12039.40, base loss: 21102.93
[INFO 2017-06-27 21:54:19,156 main.py:106] improve_loss: 9063.53, improve_percent: 0.43
[INFO 2017-06-27 21:54:19,156 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:54:19,170 main.py:76] current best improved percent: 0.43
[INFO 2017-06-27 21:54:20,141 main.py:51] epoch 3000, training loss: 11776.05, average training loss: 11736.93, base loss: 19790.02
[INFO 2017-06-27 21:54:21,237 main.py:51] epoch 3001, training loss: 12419.28, average training loss: 11737.73, base loss: 19795.06
[INFO 2017-06-27 21:54:22,216 main.py:51] epoch 3002, training loss: 11564.19, average training loss: 11738.30, base loss: 19798.94
[INFO 2017-06-27 21:54:23,195 main.py:51] epoch 3003, training loss: 11423.99, average training loss: 11737.22, base loss: 19797.87
[INFO 2017-06-27 21:54:24,174 main.py:51] epoch 3004, training loss: 11260.07, average training loss: 11738.77, base loss: 19803.71
[INFO 2017-06-27 21:54:25,146 main.py:51] epoch 3005, training loss: 12419.95, average training loss: 11739.60, base loss: 19806.66
[INFO 2017-06-27 21:54:26,117 main.py:51] epoch 3006, training loss: 9889.44, average training loss: 11737.90, base loss: 19803.15
[INFO 2017-06-27 21:54:27,090 main.py:51] epoch 3007, training loss: 11416.62, average training loss: 11737.68, base loss: 19803.72
[INFO 2017-06-27 21:54:28,064 main.py:51] epoch 3008, training loss: 11483.26, average training loss: 11735.92, base loss: 19803.39
[INFO 2017-06-27 21:54:29,039 main.py:51] epoch 3009, training loss: 13304.08, average training loss: 11738.30, base loss: 19810.77
[INFO 2017-06-27 21:54:30,010 main.py:51] epoch 3010, training loss: 12016.99, average training loss: 11739.13, base loss: 19813.67
[INFO 2017-06-27 21:54:30,987 main.py:51] epoch 3011, training loss: 11231.31, average training loss: 11738.66, base loss: 19813.74
[INFO 2017-06-27 21:54:31,960 main.py:51] epoch 3012, training loss: 12906.23, average training loss: 11740.62, base loss: 19819.06
[INFO 2017-06-27 21:54:32,938 main.py:51] epoch 3013, training loss: 9687.48, average training loss: 11737.44, base loss: 19814.06
[INFO 2017-06-27 21:54:33,910 main.py:51] epoch 3014, training loss: 11941.83, average training loss: 11738.73, base loss: 19817.18
[INFO 2017-06-27 21:54:34,881 main.py:51] epoch 3015, training loss: 11204.32, average training loss: 11738.72, base loss: 19817.87
[INFO 2017-06-27 21:54:35,856 main.py:51] epoch 3016, training loss: 12926.72, average training loss: 11738.97, base loss: 19820.24
[INFO 2017-06-27 21:54:36,831 main.py:51] epoch 3017, training loss: 11643.05, average training loss: 11737.48, base loss: 19818.31
[INFO 2017-06-27 21:54:37,801 main.py:51] epoch 3018, training loss: 12860.44, average training loss: 11737.94, base loss: 19821.15
[INFO 2017-06-27 21:54:38,780 main.py:51] epoch 3019, training loss: 11079.41, average training loss: 11736.62, base loss: 19820.46
[INFO 2017-06-27 21:54:39,751 main.py:51] epoch 3020, training loss: 11659.30, average training loss: 11735.57, base loss: 19819.46
[INFO 2017-06-27 21:54:40,724 main.py:51] epoch 3021, training loss: 12744.10, average training loss: 11736.37, base loss: 19823.95
[INFO 2017-06-27 21:54:41,699 main.py:51] epoch 3022, training loss: 11728.99, average training loss: 11735.73, base loss: 19825.32
[INFO 2017-06-27 21:54:42,678 main.py:51] epoch 3023, training loss: 11903.72, average training loss: 11734.07, base loss: 19824.65
[INFO 2017-06-27 21:54:43,650 main.py:51] epoch 3024, training loss: 11525.98, average training loss: 11734.12, base loss: 19826.23
[INFO 2017-06-27 21:54:44,621 main.py:51] epoch 3025, training loss: 11125.05, average training loss: 11733.54, base loss: 19825.36
[INFO 2017-06-27 21:54:45,593 main.py:51] epoch 3026, training loss: 10630.36, average training loss: 11731.87, base loss: 19823.28
[INFO 2017-06-27 21:54:46,565 main.py:51] epoch 3027, training loss: 10279.93, average training loss: 11730.42, base loss: 19821.08
[INFO 2017-06-27 21:54:47,537 main.py:51] epoch 3028, training loss: 11939.13, average training loss: 11729.33, base loss: 19819.93
[INFO 2017-06-27 21:54:48,508 main.py:51] epoch 3029, training loss: 11511.08, average training loss: 11729.13, base loss: 19821.58
[INFO 2017-06-27 21:54:49,483 main.py:51] epoch 3030, training loss: 11306.85, average training loss: 11726.71, base loss: 19817.36
[INFO 2017-06-27 21:54:50,455 main.py:51] epoch 3031, training loss: 11665.20, average training loss: 11726.64, base loss: 19820.87
[INFO 2017-06-27 21:54:51,431 main.py:51] epoch 3032, training loss: 10643.78, average training loss: 11725.94, base loss: 19821.40
[INFO 2017-06-27 21:54:52,403 main.py:51] epoch 3033, training loss: 11109.14, average training loss: 11723.60, base loss: 19818.41
[INFO 2017-06-27 21:54:53,377 main.py:51] epoch 3034, training loss: 11034.60, average training loss: 11723.34, base loss: 19819.03
[INFO 2017-06-27 21:54:54,347 main.py:51] epoch 3035, training loss: 11599.28, average training loss: 11722.35, base loss: 19817.19
[INFO 2017-06-27 21:54:55,315 main.py:51] epoch 3036, training loss: 10725.68, average training loss: 11719.87, base loss: 19813.02
[INFO 2017-06-27 21:54:56,286 main.py:51] epoch 3037, training loss: 11304.35, average training loss: 11718.99, base loss: 19811.38
[INFO 2017-06-27 21:54:57,260 main.py:51] epoch 3038, training loss: 12012.22, average training loss: 11718.49, base loss: 19812.72
[INFO 2017-06-27 21:54:58,232 main.py:51] epoch 3039, training loss: 10385.25, average training loss: 11717.04, base loss: 19811.69
[INFO 2017-06-27 21:54:59,210 main.py:51] epoch 3040, training loss: 11074.69, average training loss: 11718.13, base loss: 19814.69
[INFO 2017-06-27 21:55:00,179 main.py:51] epoch 3041, training loss: 10002.68, average training loss: 11716.54, base loss: 19812.04
[INFO 2017-06-27 21:55:01,147 main.py:51] epoch 3042, training loss: 12879.37, average training loss: 11716.46, base loss: 19814.20
[INFO 2017-06-27 21:55:02,127 main.py:51] epoch 3043, training loss: 10897.44, average training loss: 11716.14, base loss: 19815.69
[INFO 2017-06-27 21:55:03,100 main.py:51] epoch 3044, training loss: 12752.29, average training loss: 11716.38, base loss: 19816.23
[INFO 2017-06-27 21:55:04,069 main.py:51] epoch 3045, training loss: 11990.36, average training loss: 11716.55, base loss: 19818.19
[INFO 2017-06-27 21:55:05,043 main.py:51] epoch 3046, training loss: 10723.01, average training loss: 11714.66, base loss: 19817.09
[INFO 2017-06-27 21:55:06,019 main.py:51] epoch 3047, training loss: 11430.36, average training loss: 11714.56, base loss: 19817.12
[INFO 2017-06-27 21:55:06,991 main.py:51] epoch 3048, training loss: 11942.68, average training loss: 11714.86, base loss: 19818.93
[INFO 2017-06-27 21:55:07,965 main.py:51] epoch 3049, training loss: 11987.76, average training loss: 11715.00, base loss: 19820.88
[INFO 2017-06-27 21:55:08,937 main.py:51] epoch 3050, training loss: 11240.21, average training loss: 11713.01, base loss: 19817.69
[INFO 2017-06-27 21:55:09,916 main.py:51] epoch 3051, training loss: 11371.64, average training loss: 11712.95, base loss: 19819.28
[INFO 2017-06-27 21:55:10,893 main.py:51] epoch 3052, training loss: 12125.36, average training loss: 11713.37, base loss: 19822.59
[INFO 2017-06-27 21:55:11,867 main.py:51] epoch 3053, training loss: 11388.88, average training loss: 11712.14, base loss: 19819.60
[INFO 2017-06-27 21:55:12,846 main.py:51] epoch 3054, training loss: 13477.30, average training loss: 11714.20, base loss: 19824.29
[INFO 2017-06-27 21:55:13,819 main.py:51] epoch 3055, training loss: 10581.84, average training loss: 11713.13, base loss: 19822.93
[INFO 2017-06-27 21:55:14,790 main.py:51] epoch 3056, training loss: 11465.48, average training loss: 11714.18, base loss: 19826.54
[INFO 2017-06-27 21:55:15,762 main.py:51] epoch 3057, training loss: 12322.85, average training loss: 11715.11, base loss: 19828.78
[INFO 2017-06-27 21:55:16,735 main.py:51] epoch 3058, training loss: 11329.36, average training loss: 11714.84, base loss: 19828.87
[INFO 2017-06-27 21:55:17,708 main.py:51] epoch 3059, training loss: 12633.46, average training loss: 11715.06, base loss: 19831.31
[INFO 2017-06-27 21:55:18,684 main.py:51] epoch 3060, training loss: 11589.84, average training loss: 11714.26, base loss: 19831.78
[INFO 2017-06-27 21:55:19,653 main.py:51] epoch 3061, training loss: 11180.78, average training loss: 11712.98, base loss: 19829.64
[INFO 2017-06-27 21:55:20,623 main.py:51] epoch 3062, training loss: 11800.85, average training loss: 11714.39, base loss: 19835.05
[INFO 2017-06-27 21:55:21,595 main.py:51] epoch 3063, training loss: 10531.53, average training loss: 11712.76, base loss: 19833.03
[INFO 2017-06-27 21:55:22,568 main.py:51] epoch 3064, training loss: 11132.36, average training loss: 11712.47, base loss: 19834.60
[INFO 2017-06-27 21:55:23,542 main.py:51] epoch 3065, training loss: 10726.56, average training loss: 11712.10, base loss: 19834.05
[INFO 2017-06-27 21:55:24,516 main.py:51] epoch 3066, training loss: 10170.24, average training loss: 11708.89, base loss: 19828.30
[INFO 2017-06-27 21:55:25,492 main.py:51] epoch 3067, training loss: 11827.93, average training loss: 11708.35, base loss: 19826.28
[INFO 2017-06-27 21:55:26,465 main.py:51] epoch 3068, training loss: 13441.82, average training loss: 11708.78, base loss: 19827.81
[INFO 2017-06-27 21:55:27,440 main.py:51] epoch 3069, training loss: 11362.84, average training loss: 11708.87, base loss: 19830.44
[INFO 2017-06-27 21:55:28,416 main.py:51] epoch 3070, training loss: 11310.16, average training loss: 11707.11, base loss: 19828.72
[INFO 2017-06-27 21:55:29,386 main.py:51] epoch 3071, training loss: 9175.16, average training loss: 11705.84, base loss: 19825.52
[INFO 2017-06-27 21:55:30,359 main.py:51] epoch 3072, training loss: 9834.59, average training loss: 11704.17, base loss: 19821.03
[INFO 2017-06-27 21:55:31,332 main.py:51] epoch 3073, training loss: 12045.80, average training loss: 11702.78, base loss: 19817.57
[INFO 2017-06-27 21:55:32,304 main.py:51] epoch 3074, training loss: 10373.72, average training loss: 11702.49, base loss: 19818.42
[INFO 2017-06-27 21:55:33,276 main.py:51] epoch 3075, training loss: 11001.57, average training loss: 11701.84, base loss: 19817.95
[INFO 2017-06-27 21:55:34,252 main.py:51] epoch 3076, training loss: 11360.77, average training loss: 11702.34, base loss: 19822.09
[INFO 2017-06-27 21:55:35,224 main.py:51] epoch 3077, training loss: 10782.88, average training loss: 11701.01, base loss: 19820.02
[INFO 2017-06-27 21:55:36,197 main.py:51] epoch 3078, training loss: 10520.00, average training loss: 11698.04, base loss: 19816.56
[INFO 2017-06-27 21:55:37,168 main.py:51] epoch 3079, training loss: 12181.56, average training loss: 11698.36, base loss: 19818.07
[INFO 2017-06-27 21:55:38,140 main.py:51] epoch 3080, training loss: 11004.57, average training loss: 11696.61, base loss: 19815.70
[INFO 2017-06-27 21:55:39,110 main.py:51] epoch 3081, training loss: 10349.23, average training loss: 11693.90, base loss: 19809.94
[INFO 2017-06-27 21:55:40,084 main.py:51] epoch 3082, training loss: 12142.75, average training loss: 11695.25, base loss: 19813.71
[INFO 2017-06-27 21:55:41,056 main.py:51] epoch 3083, training loss: 10910.18, average training loss: 11693.38, base loss: 19810.42
[INFO 2017-06-27 21:55:42,027 main.py:51] epoch 3084, training loss: 11164.46, average training loss: 11691.81, base loss: 19808.00
[INFO 2017-06-27 21:55:42,998 main.py:51] epoch 3085, training loss: 11622.28, average training loss: 11691.77, base loss: 19809.62
[INFO 2017-06-27 21:55:43,974 main.py:51] epoch 3086, training loss: 11182.13, average training loss: 11690.78, base loss: 19808.02
[INFO 2017-06-27 21:55:44,945 main.py:51] epoch 3087, training loss: 10362.97, average training loss: 11689.44, base loss: 19804.17
[INFO 2017-06-27 21:55:45,917 main.py:51] epoch 3088, training loss: 11218.65, average training loss: 11688.79, base loss: 19805.77
[INFO 2017-06-27 21:55:46,889 main.py:51] epoch 3089, training loss: 11152.20, average training loss: 11687.78, base loss: 19805.22
[INFO 2017-06-27 21:55:47,861 main.py:51] epoch 3090, training loss: 10096.80, average training loss: 11688.13, base loss: 19807.79
[INFO 2017-06-27 21:55:48,830 main.py:51] epoch 3091, training loss: 11445.82, average training loss: 11687.57, base loss: 19807.51
[INFO 2017-06-27 21:55:49,803 main.py:51] epoch 3092, training loss: 11544.23, average training loss: 11687.35, base loss: 19808.12
[INFO 2017-06-27 21:55:50,776 main.py:51] epoch 3093, training loss: 13585.40, average training loss: 11689.07, base loss: 19812.52
[INFO 2017-06-27 21:55:51,745 main.py:51] epoch 3094, training loss: 11975.09, average training loss: 11688.70, base loss: 19812.25
[INFO 2017-06-27 21:55:52,717 main.py:51] epoch 3095, training loss: 11064.58, average training loss: 11686.08, base loss: 19806.92
[INFO 2017-06-27 21:55:53,688 main.py:51] epoch 3096, training loss: 11097.24, average training loss: 11683.98, base loss: 19804.61
[INFO 2017-06-27 21:55:54,663 main.py:51] epoch 3097, training loss: 10253.92, average training loss: 11682.59, base loss: 19803.63
[INFO 2017-06-27 21:55:55,633 main.py:51] epoch 3098, training loss: 12314.83, average training loss: 11683.23, base loss: 19805.89
[INFO 2017-06-27 21:55:56,605 main.py:51] epoch 3099, training loss: 11312.04, average training loss: 11682.01, base loss: 19802.77
[INFO 2017-06-27 21:55:56,605 main.py:53] epoch 3099, testing
[INFO 2017-06-27 21:56:00,807 main.py:105] average testing loss: 11261.54, base loss: 19632.43
[INFO 2017-06-27 21:56:00,807 main.py:106] improve_loss: 8370.89, improve_percent: 0.43
[INFO 2017-06-27 21:56:00,807 main.py:76] current best improved percent: 0.43
[INFO 2017-06-27 21:56:01,778 main.py:51] epoch 3100, training loss: 11047.47, average training loss: 11680.74, base loss: 19802.13
[INFO 2017-06-27 21:56:02,753 main.py:51] epoch 3101, training loss: 13537.94, average training loss: 11682.03, base loss: 19804.02
[INFO 2017-06-27 21:56:03,726 main.py:51] epoch 3102, training loss: 11305.80, average training loss: 11681.43, base loss: 19804.19
[INFO 2017-06-27 21:56:04,698 main.py:51] epoch 3103, training loss: 10803.63, average training loss: 11678.36, base loss: 19798.62
[INFO 2017-06-27 21:56:05,669 main.py:51] epoch 3104, training loss: 12921.93, average training loss: 11678.80, base loss: 19801.49
[INFO 2017-06-27 21:56:06,641 main.py:51] epoch 3105, training loss: 11078.15, average training loss: 11676.66, base loss: 19797.07
[INFO 2017-06-27 21:56:07,616 main.py:51] epoch 3106, training loss: 11213.77, average training loss: 11674.94, base loss: 19795.06
[INFO 2017-06-27 21:56:08,591 main.py:51] epoch 3107, training loss: 11926.96, average training loss: 11673.85, base loss: 19795.40
[INFO 2017-06-27 21:56:09,563 main.py:51] epoch 3108, training loss: 11719.35, average training loss: 11672.44, base loss: 19795.81
[INFO 2017-06-27 21:56:10,536 main.py:51] epoch 3109, training loss: 11137.37, average training loss: 11672.17, base loss: 19797.63
[INFO 2017-06-27 21:56:11,507 main.py:51] epoch 3110, training loss: 10732.72, average training loss: 11670.63, base loss: 19795.51
[INFO 2017-06-27 21:56:12,478 main.py:51] epoch 3111, training loss: 11426.14, average training loss: 11668.74, base loss: 19793.59
[INFO 2017-06-27 21:56:13,456 main.py:51] epoch 3112, training loss: 12065.34, average training loss: 11668.73, base loss: 19795.17
[INFO 2017-06-27 21:56:14,442 main.py:51] epoch 3113, training loss: 10396.21, average training loss: 11666.40, base loss: 19792.13
[INFO 2017-06-27 21:56:15,425 main.py:51] epoch 3114, training loss: 9407.96, average training loss: 11662.78, base loss: 19785.56
[INFO 2017-06-27 21:56:16,438 main.py:51] epoch 3115, training loss: 13000.71, average training loss: 11665.70, base loss: 19793.75
[INFO 2017-06-27 21:56:17,596 main.py:51] epoch 3116, training loss: 11195.67, average training loss: 11664.57, base loss: 19790.56
[INFO 2017-06-27 21:56:18,699 main.py:51] epoch 3117, training loss: 11455.12, average training loss: 11664.87, base loss: 19793.63
[INFO 2017-06-27 21:56:19,847 main.py:51] epoch 3118, training loss: 12874.79, average training loss: 11664.23, base loss: 19793.39
[INFO 2017-06-27 21:56:20,851 main.py:51] epoch 3119, training loss: 12391.16, average training loss: 11662.24, base loss: 19789.31
[INFO 2017-06-27 21:56:21,827 main.py:51] epoch 3120, training loss: 10785.11, average training loss: 11659.69, base loss: 19785.02
[INFO 2017-06-27 21:56:22,807 main.py:51] epoch 3121, training loss: 12632.01, average training loss: 11660.81, base loss: 19789.36
[INFO 2017-06-27 21:56:23,878 main.py:51] epoch 3122, training loss: 10995.72, average training loss: 11660.14, base loss: 19789.36
[INFO 2017-06-27 21:56:24,927 main.py:51] epoch 3123, training loss: 9514.09, average training loss: 11657.79, base loss: 19785.68
[INFO 2017-06-27 21:56:25,936 main.py:51] epoch 3124, training loss: 12470.74, average training loss: 11658.44, base loss: 19787.84
[INFO 2017-06-27 21:56:27,039 main.py:51] epoch 3125, training loss: 11277.11, average training loss: 11658.90, base loss: 19789.27
[INFO 2017-06-27 21:56:28,017 main.py:51] epoch 3126, training loss: 11840.02, average training loss: 11657.93, base loss: 19788.82
[INFO 2017-06-27 21:56:29,004 main.py:51] epoch 3127, training loss: 12900.61, average training loss: 11659.11, base loss: 19794.82
[INFO 2017-06-27 21:56:29,980 main.py:51] epoch 3128, training loss: 10269.02, average training loss: 11657.83, base loss: 19793.03
[INFO 2017-06-27 21:56:30,951 main.py:51] epoch 3129, training loss: 11112.28, average training loss: 11656.14, base loss: 19791.62
[INFO 2017-06-27 21:56:31,922 main.py:51] epoch 3130, training loss: 12287.62, average training loss: 11655.90, base loss: 19793.58
[INFO 2017-06-27 21:56:32,892 main.py:51] epoch 3131, training loss: 10699.42, average training loss: 11653.45, base loss: 19788.68
[INFO 2017-06-27 21:56:33,863 main.py:51] epoch 3132, training loss: 10334.73, average training loss: 11651.76, base loss: 19785.01
[INFO 2017-06-27 21:56:34,837 main.py:51] epoch 3133, training loss: 10599.28, average training loss: 11651.56, base loss: 19785.14
[INFO 2017-06-27 21:56:35,813 main.py:51] epoch 3134, training loss: 10617.59, average training loss: 11649.33, base loss: 19781.07
[INFO 2017-06-27 21:56:36,795 main.py:51] epoch 3135, training loss: 15239.40, average training loss: 11651.00, base loss: 19785.17
[INFO 2017-06-27 21:56:37,770 main.py:51] epoch 3136, training loss: 10990.09, average training loss: 11650.04, base loss: 19784.67
[INFO 2017-06-27 21:56:38,742 main.py:51] epoch 3137, training loss: 11872.55, average training loss: 11649.01, base loss: 19783.99
[INFO 2017-06-27 21:56:39,826 main.py:51] epoch 3138, training loss: 11779.45, average training loss: 11649.46, base loss: 19785.37
[INFO 2017-06-27 21:56:40,829 main.py:51] epoch 3139, training loss: 10885.18, average training loss: 11647.31, base loss: 19782.26
[INFO 2017-06-27 21:56:41,800 main.py:51] epoch 3140, training loss: 12153.43, average training loss: 11647.59, base loss: 19784.47
[INFO 2017-06-27 21:56:42,783 main.py:51] epoch 3141, training loss: 10727.96, average training loss: 11645.41, base loss: 19780.04
[INFO 2017-06-27 21:56:43,759 main.py:51] epoch 3142, training loss: 11078.11, average training loss: 11645.58, base loss: 19782.60
[INFO 2017-06-27 21:56:44,730 main.py:51] epoch 3143, training loss: 10041.46, average training loss: 11644.95, base loss: 19781.47
[INFO 2017-06-27 21:56:45,703 main.py:51] epoch 3144, training loss: 9929.31, average training loss: 11641.64, base loss: 19774.07
[INFO 2017-06-27 21:56:46,673 main.py:51] epoch 3145, training loss: 10995.81, average training loss: 11641.10, base loss: 19773.39
[INFO 2017-06-27 21:56:47,646 main.py:51] epoch 3146, training loss: 13210.86, average training loss: 11643.02, base loss: 19778.06
[INFO 2017-06-27 21:56:48,619 main.py:51] epoch 3147, training loss: 11814.05, average training loss: 11643.31, base loss: 19781.14
[INFO 2017-06-27 21:56:49,591 main.py:51] epoch 3148, training loss: 13536.62, average training loss: 11645.90, base loss: 19788.10
[INFO 2017-06-27 21:56:50,676 main.py:51] epoch 3149, training loss: 9789.65, average training loss: 11643.40, base loss: 19784.23
[INFO 2017-06-27 21:56:51,692 main.py:51] epoch 3150, training loss: 12415.01, average training loss: 11645.38, base loss: 19789.27
[INFO 2017-06-27 21:56:52,668 main.py:51] epoch 3151, training loss: 13178.20, average training loss: 11647.93, base loss: 19795.26
[INFO 2017-06-27 21:56:53,650 main.py:51] epoch 3152, training loss: 10558.42, average training loss: 11646.17, base loss: 19793.90
[INFO 2017-06-27 21:56:54,625 main.py:51] epoch 3153, training loss: 12716.71, average training loss: 11647.45, base loss: 19798.10
[INFO 2017-06-27 21:56:55,598 main.py:51] epoch 3154, training loss: 11480.48, average training loss: 11646.93, base loss: 19797.30
[INFO 2017-06-27 21:56:56,574 main.py:51] epoch 3155, training loss: 11383.69, average training loss: 11644.69, base loss: 19793.62
[INFO 2017-06-27 21:56:57,553 main.py:51] epoch 3156, training loss: 11470.99, average training loss: 11643.52, base loss: 19792.25
[INFO 2017-06-27 21:56:58,529 main.py:51] epoch 3157, training loss: 12455.72, average training loss: 11643.27, base loss: 19794.74
[INFO 2017-06-27 21:56:59,502 main.py:51] epoch 3158, training loss: 11377.07, average training loss: 11641.39, base loss: 19792.67
[INFO 2017-06-27 21:57:00,480 main.py:51] epoch 3159, training loss: 10514.43, average training loss: 11639.77, base loss: 19790.48
[INFO 2017-06-27 21:57:01,596 main.py:51] epoch 3160, training loss: 11326.37, average training loss: 11639.49, base loss: 19790.67
[INFO 2017-06-27 21:57:02,586 main.py:51] epoch 3161, training loss: 10245.29, average training loss: 11639.07, base loss: 19791.21
[INFO 2017-06-27 21:57:03,571 main.py:51] epoch 3162, training loss: 10993.94, average training loss: 11637.87, base loss: 19790.19
[INFO 2017-06-27 21:57:04,546 main.py:51] epoch 3163, training loss: 12708.87, average training loss: 11639.46, base loss: 19794.96
[INFO 2017-06-27 21:57:05,520 main.py:51] epoch 3164, training loss: 12923.61, average training loss: 11639.87, base loss: 19798.86
[INFO 2017-06-27 21:57:06,494 main.py:51] epoch 3165, training loss: 10206.12, average training loss: 11639.40, base loss: 19798.79
[INFO 2017-06-27 21:57:07,471 main.py:51] epoch 3166, training loss: 12121.85, average training loss: 11639.55, base loss: 19799.55
[INFO 2017-06-27 21:57:08,444 main.py:51] epoch 3167, training loss: 10626.02, average training loss: 11636.71, base loss: 19795.63
[INFO 2017-06-27 21:57:09,415 main.py:51] epoch 3168, training loss: 10993.03, average training loss: 11636.48, base loss: 19796.73
[INFO 2017-06-27 21:57:10,445 main.py:51] epoch 3169, training loss: 12354.15, average training loss: 11635.16, base loss: 19796.90
[INFO 2017-06-27 21:57:11,466 main.py:51] epoch 3170, training loss: 11592.52, average training loss: 11636.43, base loss: 19801.10
[INFO 2017-06-27 21:57:12,442 main.py:51] epoch 3171, training loss: 12113.75, average training loss: 11635.57, base loss: 19801.14
[INFO 2017-06-27 21:57:13,415 main.py:51] epoch 3172, training loss: 10867.54, average training loss: 11634.10, base loss: 19799.50
[INFO 2017-06-27 21:57:14,393 main.py:51] epoch 3173, training loss: 10404.04, average training loss: 11633.65, base loss: 19798.63
[INFO 2017-06-27 21:57:15,367 main.py:51] epoch 3174, training loss: 10439.99, average training loss: 11631.61, base loss: 19796.13
[INFO 2017-06-27 21:57:16,340 main.py:51] epoch 3175, training loss: 10607.56, average training loss: 11629.62, base loss: 19792.86
[INFO 2017-06-27 21:57:17,314 main.py:51] epoch 3176, training loss: 11678.78, average training loss: 11629.18, base loss: 19791.44
[INFO 2017-06-27 21:57:18,289 main.py:51] epoch 3177, training loss: 12703.62, average training loss: 11630.81, base loss: 19797.23
[INFO 2017-06-27 21:57:19,264 main.py:51] epoch 3178, training loss: 13293.81, average training loss: 11632.41, base loss: 19802.16
[INFO 2017-06-27 21:57:20,369 main.py:51] epoch 3179, training loss: 11045.30, average training loss: 11631.33, base loss: 19801.56
[INFO 2017-06-27 21:57:21,373 main.py:51] epoch 3180, training loss: 10939.39, average training loss: 11630.12, base loss: 19800.64
[INFO 2017-06-27 21:57:22,343 main.py:51] epoch 3181, training loss: 12455.56, average training loss: 11630.14, base loss: 19800.62
[INFO 2017-06-27 21:57:23,318 main.py:51] epoch 3182, training loss: 12065.42, average training loss: 11630.68, base loss: 19802.82
[INFO 2017-06-27 21:57:24,297 main.py:51] epoch 3183, training loss: 11870.38, average training loss: 11631.50, base loss: 19804.55
[INFO 2017-06-27 21:57:25,273 main.py:51] epoch 3184, training loss: 11706.63, average training loss: 11632.40, base loss: 19806.64
[INFO 2017-06-27 21:57:26,252 main.py:51] epoch 3185, training loss: 11921.32, average training loss: 11631.33, base loss: 19804.48
[INFO 2017-06-27 21:57:27,226 main.py:51] epoch 3186, training loss: 10748.70, average training loss: 11627.83, base loss: 19798.82
[INFO 2017-06-27 21:57:28,318 main.py:51] epoch 3187, training loss: 12162.23, average training loss: 11627.60, base loss: 19798.20
[INFO 2017-06-27 21:57:29,318 main.py:51] epoch 3188, training loss: 10924.21, average training loss: 11627.14, base loss: 19799.02
[INFO 2017-06-27 21:57:30,432 main.py:51] epoch 3189, training loss: 11150.13, average training loss: 11626.03, base loss: 19797.56
[INFO 2017-06-27 21:57:31,411 main.py:51] epoch 3190, training loss: 11990.75, average training loss: 11624.80, base loss: 19796.71
[INFO 2017-06-27 21:57:32,399 main.py:51] epoch 3191, training loss: 11325.78, average training loss: 11623.95, base loss: 19795.90
[INFO 2017-06-27 21:57:33,375 main.py:51] epoch 3192, training loss: 12859.75, average training loss: 11624.63, base loss: 19798.96
[INFO 2017-06-27 21:57:34,348 main.py:51] epoch 3193, training loss: 10756.08, average training loss: 11623.05, base loss: 19794.58
[INFO 2017-06-27 21:57:35,322 main.py:51] epoch 3194, training loss: 11438.42, average training loss: 11621.14, base loss: 19791.62
[INFO 2017-06-27 21:57:36,362 main.py:51] epoch 3195, training loss: 10537.04, average training loss: 11619.36, base loss: 19789.07
[INFO 2017-06-27 21:57:37,391 main.py:51] epoch 3196, training loss: 12652.77, average training loss: 11619.87, base loss: 19790.99
[INFO 2017-06-27 21:57:38,462 main.py:51] epoch 3197, training loss: 11851.77, average training loss: 11619.90, base loss: 19793.35
[INFO 2017-06-27 21:57:39,496 main.py:51] epoch 3198, training loss: 11639.94, average training loss: 11620.62, base loss: 19796.53
[INFO 2017-06-27 21:57:40,562 main.py:51] epoch 3199, training loss: 12296.96, average training loss: 11621.51, base loss: 19800.36
[INFO 2017-06-27 21:57:40,562 main.py:53] epoch 3199, testing
[INFO 2017-06-27 21:57:44,878 main.py:105] average testing loss: 11360.32, base loss: 20154.47
[INFO 2017-06-27 21:57:44,878 main.py:106] improve_loss: 8794.15, improve_percent: 0.44
[INFO 2017-06-27 21:57:44,879 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 21:57:44,892 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 21:57:45,866 main.py:51] epoch 3200, training loss: 11269.34, average training loss: 11619.93, base loss: 19797.05
[INFO 2017-06-27 21:57:46,941 main.py:51] epoch 3201, training loss: 11604.46, average training loss: 11619.02, base loss: 19795.27
[INFO 2017-06-27 21:57:47,930 main.py:51] epoch 3202, training loss: 10631.59, average training loss: 11617.92, base loss: 19796.55
[INFO 2017-06-27 21:57:48,905 main.py:51] epoch 3203, training loss: 11131.07, average training loss: 11615.57, base loss: 19791.34
[INFO 2017-06-27 21:57:49,879 main.py:51] epoch 3204, training loss: 10896.60, average training loss: 11612.96, base loss: 19788.28
[INFO 2017-06-27 21:57:50,856 main.py:51] epoch 3205, training loss: 13284.51, average training loss: 11615.02, base loss: 19793.76
[INFO 2017-06-27 21:57:51,832 main.py:51] epoch 3206, training loss: 11815.83, average training loss: 11615.10, base loss: 19794.17
[INFO 2017-06-27 21:57:52,814 main.py:51] epoch 3207, training loss: 13140.40, average training loss: 11616.23, base loss: 19798.63
[INFO 2017-06-27 21:57:53,794 main.py:51] epoch 3208, training loss: 11717.75, average training loss: 11616.33, base loss: 19799.28
[INFO 2017-06-27 21:57:54,776 main.py:51] epoch 3209, training loss: 10884.02, average training loss: 11614.62, base loss: 19797.21
[INFO 2017-06-27 21:57:55,881 main.py:51] epoch 3210, training loss: 11370.57, average training loss: 11614.53, base loss: 19797.80
[INFO 2017-06-27 21:57:57,027 main.py:51] epoch 3211, training loss: 10067.96, average training loss: 11612.58, base loss: 19794.45
[INFO 2017-06-27 21:57:58,121 main.py:51] epoch 3212, training loss: 11742.03, average training loss: 11611.71, base loss: 19794.72
[INFO 2017-06-27 21:57:59,220 main.py:51] epoch 3213, training loss: 11153.50, average training loss: 11609.64, base loss: 19792.34
[INFO 2017-06-27 21:58:00,194 main.py:51] epoch 3214, training loss: 9675.88, average training loss: 11607.64, base loss: 19791.56
[INFO 2017-06-27 21:58:01,169 main.py:51] epoch 3215, training loss: 11195.64, average training loss: 11607.25, base loss: 19791.08
[INFO 2017-06-27 21:58:02,145 main.py:51] epoch 3216, training loss: 11895.60, average training loss: 11607.26, base loss: 19792.76
[INFO 2017-06-27 21:58:03,116 main.py:51] epoch 3217, training loss: 11685.46, average training loss: 11608.03, base loss: 19794.66
[INFO 2017-06-27 21:58:04,089 main.py:51] epoch 3218, training loss: 12402.77, average training loss: 11609.59, base loss: 19801.18
[INFO 2017-06-27 21:58:05,059 main.py:51] epoch 3219, training loss: 10931.76, average training loss: 11608.51, base loss: 19799.73
[INFO 2017-06-27 21:58:06,031 main.py:51] epoch 3220, training loss: 11324.31, average training loss: 11607.69, base loss: 19797.95
[INFO 2017-06-27 21:58:07,003 main.py:51] epoch 3221, training loss: 10727.28, average training loss: 11605.73, base loss: 19796.30
[INFO 2017-06-27 21:58:08,032 main.py:51] epoch 3222, training loss: 10215.45, average training loss: 11603.67, base loss: 19792.94
[INFO 2017-06-27 21:58:09,041 main.py:51] epoch 3223, training loss: 10270.38, average training loss: 11602.83, base loss: 19792.18
[INFO 2017-06-27 21:58:10,023 main.py:51] epoch 3224, training loss: 11917.15, average training loss: 11602.32, base loss: 19792.93
[INFO 2017-06-27 21:58:11,025 main.py:51] epoch 3225, training loss: 11024.67, average training loss: 11602.68, base loss: 19794.42
[INFO 2017-06-27 21:58:12,056 main.py:51] epoch 3226, training loss: 9924.28, average training loss: 11599.66, base loss: 19790.11
[INFO 2017-06-27 21:58:13,030 main.py:51] epoch 3227, training loss: 11918.74, average training loss: 11601.13, base loss: 19795.00
[INFO 2017-06-27 21:58:14,000 main.py:51] epoch 3228, training loss: 11172.12, average training loss: 11600.11, base loss: 19794.46
[INFO 2017-06-27 21:58:15,078 main.py:51] epoch 3229, training loss: 11134.88, average training loss: 11598.21, base loss: 19791.24
[INFO 2017-06-27 21:58:16,052 main.py:51] epoch 3230, training loss: 10797.81, average training loss: 11596.12, base loss: 19788.79
[INFO 2017-06-27 21:58:17,074 main.py:51] epoch 3231, training loss: 12274.11, average training loss: 11596.78, base loss: 19791.23
[INFO 2017-06-27 21:58:18,066 main.py:51] epoch 3232, training loss: 10380.36, average training loss: 11594.11, base loss: 19786.29
[INFO 2017-06-27 21:58:19,133 main.py:51] epoch 3233, training loss: 11505.89, average training loss: 11593.23, base loss: 19785.03
[INFO 2017-06-27 21:58:20,137 main.py:51] epoch 3234, training loss: 12222.45, average training loss: 11592.42, base loss: 19784.50
[INFO 2017-06-27 21:58:21,128 main.py:51] epoch 3235, training loss: 11689.85, average training loss: 11593.36, base loss: 19786.31
[INFO 2017-06-27 21:58:22,099 main.py:51] epoch 3236, training loss: 12170.50, average training loss: 11592.85, base loss: 19787.18
[INFO 2017-06-27 21:58:23,071 main.py:51] epoch 3237, training loss: 10691.16, average training loss: 11591.41, base loss: 19784.07
[INFO 2017-06-27 21:58:24,045 main.py:51] epoch 3238, training loss: 11790.11, average training loss: 11590.91, base loss: 19785.56
[INFO 2017-06-27 21:58:25,017 main.py:51] epoch 3239, training loss: 11387.33, average training loss: 11591.59, base loss: 19789.49
[INFO 2017-06-27 21:58:25,997 main.py:51] epoch 3240, training loss: 9785.71, average training loss: 11591.45, base loss: 19790.53
[INFO 2017-06-27 21:58:26,971 main.py:51] epoch 3241, training loss: 12481.53, average training loss: 11593.26, base loss: 19794.84
[INFO 2017-06-27 21:58:27,945 main.py:51] epoch 3242, training loss: 12224.36, average training loss: 11595.06, base loss: 19800.25
[INFO 2017-06-27 21:58:28,920 main.py:51] epoch 3243, training loss: 10485.29, average training loss: 11594.72, base loss: 19801.71
[INFO 2017-06-27 21:58:29,896 main.py:51] epoch 3244, training loss: 11756.61, average training loss: 11594.22, base loss: 19802.88
[INFO 2017-06-27 21:58:30,878 main.py:51] epoch 3245, training loss: 10459.30, average training loss: 11591.92, base loss: 19799.55
[INFO 2017-06-27 21:58:31,855 main.py:51] epoch 3246, training loss: 10943.06, average training loss: 11590.71, base loss: 19799.33
[INFO 2017-06-27 21:58:32,830 main.py:51] epoch 3247, training loss: 10155.52, average training loss: 11588.85, base loss: 19797.04
[INFO 2017-06-27 21:58:33,862 main.py:51] epoch 3248, training loss: 10629.25, average training loss: 11586.81, base loss: 19795.30
[INFO 2017-06-27 21:58:34,857 main.py:51] epoch 3249, training loss: 11653.23, average training loss: 11585.62, base loss: 19794.70
[INFO 2017-06-27 21:58:35,846 main.py:51] epoch 3250, training loss: 12462.70, average training loss: 11586.83, base loss: 19801.00
[INFO 2017-06-27 21:58:36,899 main.py:51] epoch 3251, training loss: 10727.76, average training loss: 11584.67, base loss: 19797.82
[INFO 2017-06-27 21:58:37,990 main.py:51] epoch 3252, training loss: 10336.38, average training loss: 11584.40, base loss: 19799.44
[INFO 2017-06-27 21:58:38,991 main.py:51] epoch 3253, training loss: 11150.90, average training loss: 11582.78, base loss: 19797.72
[INFO 2017-06-27 21:58:39,967 main.py:51] epoch 3254, training loss: 9848.26, average training loss: 11578.73, base loss: 19791.27
[INFO 2017-06-27 21:58:40,937 main.py:51] epoch 3255, training loss: 11486.49, average training loss: 11579.08, base loss: 19791.68
[INFO 2017-06-27 21:58:41,910 main.py:51] epoch 3256, training loss: 11661.98, average training loss: 11578.84, base loss: 19792.69
[INFO 2017-06-27 21:58:42,881 main.py:51] epoch 3257, training loss: 12525.62, average training loss: 11578.82, base loss: 19795.24
[INFO 2017-06-27 21:58:43,855 main.py:51] epoch 3258, training loss: 10959.26, average training loss: 11577.08, base loss: 19794.03
[INFO 2017-06-27 21:58:44,829 main.py:51] epoch 3259, training loss: 10320.30, average training loss: 11575.99, base loss: 19793.06
[INFO 2017-06-27 21:58:45,804 main.py:51] epoch 3260, training loss: 10068.40, average training loss: 11574.20, base loss: 19790.64
[INFO 2017-06-27 21:58:46,778 main.py:51] epoch 3261, training loss: 12448.68, average training loss: 11575.58, base loss: 19795.58
[INFO 2017-06-27 21:58:47,753 main.py:51] epoch 3262, training loss: 12130.62, average training loss: 11574.75, base loss: 19795.43
[INFO 2017-06-27 21:58:48,723 main.py:51] epoch 3263, training loss: 10381.80, average training loss: 11575.29, base loss: 19797.89
[INFO 2017-06-27 21:58:49,696 main.py:51] epoch 3264, training loss: 11115.78, average training loss: 11575.36, base loss: 19800.36
[INFO 2017-06-27 21:58:50,667 main.py:51] epoch 3265, training loss: 11222.13, average training loss: 11574.84, base loss: 19799.88
[INFO 2017-06-27 21:58:51,644 main.py:51] epoch 3266, training loss: 11055.18, average training loss: 11573.65, base loss: 19799.29
[INFO 2017-06-27 21:58:52,746 main.py:51] epoch 3267, training loss: 10952.59, average training loss: 11572.61, base loss: 19798.55
[INFO 2017-06-27 21:58:53,759 main.py:51] epoch 3268, training loss: 10738.17, average training loss: 11570.98, base loss: 19795.62
[INFO 2017-06-27 21:58:54,742 main.py:51] epoch 3269, training loss: 10800.31, average training loss: 11568.51, base loss: 19791.64
[INFO 2017-06-27 21:58:55,719 main.py:51] epoch 3270, training loss: 10254.07, average training loss: 11568.20, base loss: 19791.26
[INFO 2017-06-27 21:58:56,693 main.py:51] epoch 3271, training loss: 10393.30, average training loss: 11567.41, base loss: 19790.40
[INFO 2017-06-27 21:58:57,665 main.py:51] epoch 3272, training loss: 12231.83, average training loss: 11566.68, base loss: 19788.71
[INFO 2017-06-27 21:58:58,641 main.py:51] epoch 3273, training loss: 11519.03, average training loss: 11566.28, base loss: 19789.01
[INFO 2017-06-27 21:58:59,762 main.py:51] epoch 3274, training loss: 11598.68, average training loss: 11565.31, base loss: 19787.44
[INFO 2017-06-27 21:59:00,751 main.py:51] epoch 3275, training loss: 9706.81, average training loss: 11563.17, base loss: 19781.91
[INFO 2017-06-27 21:59:01,723 main.py:51] epoch 3276, training loss: 11904.31, average training loss: 11563.59, base loss: 19784.89
[INFO 2017-06-27 21:59:02,696 main.py:51] epoch 3277, training loss: 10649.82, average training loss: 11563.71, base loss: 19785.57
[INFO 2017-06-27 21:59:03,669 main.py:51] epoch 3278, training loss: 10757.68, average training loss: 11562.57, base loss: 19783.06
[INFO 2017-06-27 21:59:04,661 main.py:51] epoch 3279, training loss: 11135.68, average training loss: 11559.44, base loss: 19778.39
[INFO 2017-06-27 21:59:05,769 main.py:51] epoch 3280, training loss: 12191.02, average training loss: 11561.64, base loss: 19785.53
[INFO 2017-06-27 21:59:06,763 main.py:51] epoch 3281, training loss: 11179.02, average training loss: 11560.58, base loss: 19784.53
[INFO 2017-06-27 21:59:07,733 main.py:51] epoch 3282, training loss: 14324.07, average training loss: 11563.59, base loss: 19791.93
[INFO 2017-06-27 21:59:08,717 main.py:51] epoch 3283, training loss: 11653.68, average training loss: 11563.20, base loss: 19791.61
[INFO 2017-06-27 21:59:09,688 main.py:51] epoch 3284, training loss: 11863.42, average training loss: 11564.42, base loss: 19796.15
[INFO 2017-06-27 21:59:10,660 main.py:51] epoch 3285, training loss: 12369.81, average training loss: 11565.84, base loss: 19801.42
[INFO 2017-06-27 21:59:11,630 main.py:51] epoch 3286, training loss: 11379.16, average training loss: 11566.03, base loss: 19803.41
[INFO 2017-06-27 21:59:12,601 main.py:51] epoch 3287, training loss: 12747.35, average training loss: 11565.33, base loss: 19803.86
[INFO 2017-06-27 21:59:13,572 main.py:51] epoch 3288, training loss: 10280.47, average training loss: 11562.74, base loss: 19800.18
[INFO 2017-06-27 21:59:14,544 main.py:51] epoch 3289, training loss: 11122.42, average training loss: 11562.23, base loss: 19800.54
[INFO 2017-06-27 21:59:15,515 main.py:51] epoch 3290, training loss: 11904.14, average training loss: 11563.25, base loss: 19804.49
[INFO 2017-06-27 21:59:16,493 main.py:51] epoch 3291, training loss: 11002.56, average training loss: 11562.95, base loss: 19804.95
[INFO 2017-06-27 21:59:17,465 main.py:51] epoch 3292, training loss: 9921.49, average training loss: 11560.51, base loss: 19800.32
[INFO 2017-06-27 21:59:18,437 main.py:51] epoch 3293, training loss: 9857.02, average training loss: 11559.32, base loss: 19798.21
[INFO 2017-06-27 21:59:19,409 main.py:51] epoch 3294, training loss: 10018.01, average training loss: 11557.43, base loss: 19796.50
[INFO 2017-06-27 21:59:20,379 main.py:51] epoch 3295, training loss: 11605.73, average training loss: 11557.03, base loss: 19796.71
[INFO 2017-06-27 21:59:21,349 main.py:51] epoch 3296, training loss: 12715.46, average training loss: 11557.45, base loss: 19799.53
[INFO 2017-06-27 21:59:22,322 main.py:51] epoch 3297, training loss: 11307.85, average training loss: 11555.85, base loss: 19798.88
[INFO 2017-06-27 21:59:23,295 main.py:51] epoch 3298, training loss: 11289.82, average training loss: 11554.54, base loss: 19799.28
[INFO 2017-06-27 21:59:24,270 main.py:51] epoch 3299, training loss: 12312.37, average training loss: 11555.19, base loss: 19802.02
[INFO 2017-06-27 21:59:24,270 main.py:53] epoch 3299, testing
[INFO 2017-06-27 21:59:28,492 main.py:105] average testing loss: 11461.25, base loss: 19855.25
[INFO 2017-06-27 21:59:28,492 main.py:106] improve_loss: 8394.00, improve_percent: 0.42
[INFO 2017-06-27 21:59:28,492 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 21:59:29,490 main.py:51] epoch 3300, training loss: 11018.59, average training loss: 11555.80, base loss: 19803.98
[INFO 2017-06-27 21:59:30,507 main.py:51] epoch 3301, training loss: 12673.46, average training loss: 11557.41, base loss: 19808.62
[INFO 2017-06-27 21:59:31,487 main.py:51] epoch 3302, training loss: 10423.29, average training loss: 11555.38, base loss: 19804.76
[INFO 2017-06-27 21:59:32,460 main.py:51] epoch 3303, training loss: 11245.72, average training loss: 11554.37, base loss: 19801.37
[INFO 2017-06-27 21:59:33,432 main.py:51] epoch 3304, training loss: 10396.15, average training loss: 11554.01, base loss: 19802.65
[INFO 2017-06-27 21:59:34,406 main.py:51] epoch 3305, training loss: 10391.69, average training loss: 11554.47, base loss: 19806.03
[INFO 2017-06-27 21:59:35,379 main.py:51] epoch 3306, training loss: 10870.54, average training loss: 11553.77, base loss: 19805.42
[INFO 2017-06-27 21:59:36,355 main.py:51] epoch 3307, training loss: 10991.08, average training loss: 11555.04, base loss: 19807.53
[INFO 2017-06-27 21:59:37,328 main.py:51] epoch 3308, training loss: 12835.60, average training loss: 11555.94, base loss: 19809.63
[INFO 2017-06-27 21:59:38,303 main.py:51] epoch 3309, training loss: 10806.74, average training loss: 11553.49, base loss: 19806.38
[INFO 2017-06-27 21:59:39,378 main.py:51] epoch 3310, training loss: 10669.58, average training loss: 11551.79, base loss: 19802.15
[INFO 2017-06-27 21:59:40,359 main.py:51] epoch 3311, training loss: 11056.66, average training loss: 11550.82, base loss: 19801.52
[INFO 2017-06-27 21:59:41,416 main.py:51] epoch 3312, training loss: 11313.13, average training loss: 11551.10, base loss: 19804.24
[INFO 2017-06-27 21:59:42,447 main.py:51] epoch 3313, training loss: 10746.14, average training loss: 11550.41, base loss: 19803.11
[INFO 2017-06-27 21:59:43,555 main.py:51] epoch 3314, training loss: 11060.44, average training loss: 11549.81, base loss: 19800.88
[INFO 2017-06-27 21:59:44,550 main.py:51] epoch 3315, training loss: 10370.18, average training loss: 11548.38, base loss: 19798.20
[INFO 2017-06-27 21:59:45,631 main.py:51] epoch 3316, training loss: 11454.59, average training loss: 11547.93, base loss: 19799.30
[INFO 2017-06-27 21:59:46,629 main.py:51] epoch 3317, training loss: 10144.64, average training loss: 11546.20, base loss: 19797.23
[INFO 2017-06-27 21:59:47,707 main.py:51] epoch 3318, training loss: 9502.72, average training loss: 11543.66, base loss: 19792.27
[INFO 2017-06-27 21:59:48,709 main.py:51] epoch 3319, training loss: 11165.78, average training loss: 11542.98, base loss: 19792.10
[INFO 2017-06-27 21:59:49,683 main.py:51] epoch 3320, training loss: 10083.19, average training loss: 11540.74, base loss: 19788.55
[INFO 2017-06-27 21:59:50,654 main.py:51] epoch 3321, training loss: 10299.46, average training loss: 11538.01, base loss: 19783.30
[INFO 2017-06-27 21:59:51,628 main.py:51] epoch 3322, training loss: 11683.82, average training loss: 11536.86, base loss: 19785.33
[INFO 2017-06-27 21:59:52,700 main.py:51] epoch 3323, training loss: 10403.36, average training loss: 11535.16, base loss: 19782.04
[INFO 2017-06-27 21:59:53,720 main.py:51] epoch 3324, training loss: 10886.28, average training loss: 11531.87, base loss: 19777.98
[INFO 2017-06-27 21:59:54,829 main.py:51] epoch 3325, training loss: 12261.89, average training loss: 11533.25, base loss: 19781.48
[INFO 2017-06-27 21:59:55,910 main.py:51] epoch 3326, training loss: 10379.57, average training loss: 11530.72, base loss: 19777.84
[INFO 2017-06-27 21:59:56,932 main.py:51] epoch 3327, training loss: 11549.58, average training loss: 11529.25, base loss: 19776.05
[INFO 2017-06-27 21:59:58,013 main.py:51] epoch 3328, training loss: 10763.36, average training loss: 11527.61, base loss: 19773.66
[INFO 2017-06-27 21:59:59,076 main.py:51] epoch 3329, training loss: 10897.49, average training loss: 11525.61, base loss: 19769.58
[INFO 2017-06-27 22:00:00,094 main.py:51] epoch 3330, training loss: 9874.11, average training loss: 11522.79, base loss: 19766.64
[INFO 2017-06-27 22:00:01,177 main.py:51] epoch 3331, training loss: 9948.04, average training loss: 11520.84, base loss: 19764.94
[INFO 2017-06-27 22:00:02,228 main.py:51] epoch 3332, training loss: 10801.70, average training loss: 11519.14, base loss: 19763.15
[INFO 2017-06-27 22:00:03,231 main.py:51] epoch 3333, training loss: 11198.83, average training loss: 11519.04, base loss: 19764.37
[INFO 2017-06-27 22:00:04,324 main.py:51] epoch 3334, training loss: 11711.78, average training loss: 11519.63, base loss: 19765.56
[INFO 2017-06-27 22:00:05,369 main.py:51] epoch 3335, training loss: 12445.24, average training loss: 11520.59, base loss: 19769.43
[INFO 2017-06-27 22:00:06,390 main.py:51] epoch 3336, training loss: 10948.05, average training loss: 11519.75, base loss: 19771.08
[INFO 2017-06-27 22:00:07,379 main.py:51] epoch 3337, training loss: 11473.95, average training loss: 11517.05, base loss: 19768.23
[INFO 2017-06-27 22:00:08,369 main.py:51] epoch 3338, training loss: 11506.63, average training loss: 11516.16, base loss: 19767.77
[INFO 2017-06-27 22:00:09,345 main.py:51] epoch 3339, training loss: 10340.60, average training loss: 11515.20, base loss: 19767.35
[INFO 2017-06-27 22:00:10,320 main.py:51] epoch 3340, training loss: 11445.50, average training loss: 11515.54, base loss: 19769.69
[INFO 2017-06-27 22:00:11,290 main.py:51] epoch 3341, training loss: 10946.74, average training loss: 11515.87, base loss: 19772.13
[INFO 2017-06-27 22:00:12,261 main.py:51] epoch 3342, training loss: 12066.81, average training loss: 11517.90, base loss: 19777.49
[INFO 2017-06-27 22:00:13,239 main.py:51] epoch 3343, training loss: 11173.49, average training loss: 11516.69, base loss: 19775.45
[INFO 2017-06-27 22:00:14,213 main.py:51] epoch 3344, training loss: 9959.71, average training loss: 11515.75, base loss: 19775.34
[INFO 2017-06-27 22:00:15,189 main.py:51] epoch 3345, training loss: 11227.15, average training loss: 11515.84, base loss: 19775.86
[INFO 2017-06-27 22:00:16,164 main.py:51] epoch 3346, training loss: 9990.24, average training loss: 11513.45, base loss: 19772.07
[INFO 2017-06-27 22:00:17,136 main.py:51] epoch 3347, training loss: 11215.51, average training loss: 11514.26, base loss: 19774.69
[INFO 2017-06-27 22:00:18,108 main.py:51] epoch 3348, training loss: 11127.30, average training loss: 11513.42, base loss: 19772.57
[INFO 2017-06-27 22:00:19,078 main.py:51] epoch 3349, training loss: 12433.35, average training loss: 11514.51, base loss: 19777.08
[INFO 2017-06-27 22:00:20,051 main.py:51] epoch 3350, training loss: 10553.76, average training loss: 11513.12, base loss: 19775.07
[INFO 2017-06-27 22:00:21,031 main.py:51] epoch 3351, training loss: 11718.90, average training loss: 11512.91, base loss: 19776.47
[INFO 2017-06-27 22:00:22,115 main.py:51] epoch 3352, training loss: 12005.00, average training loss: 11512.16, base loss: 19776.37
[INFO 2017-06-27 22:00:23,153 main.py:51] epoch 3353, training loss: 11116.90, average training loss: 11510.90, base loss: 19774.60
[INFO 2017-06-27 22:00:24,149 main.py:51] epoch 3354, training loss: 10912.95, average training loss: 11509.75, base loss: 19771.72
[INFO 2017-06-27 22:00:25,244 main.py:51] epoch 3355, training loss: 10286.17, average training loss: 11507.28, base loss: 19765.91
[INFO 2017-06-27 22:00:26,269 main.py:51] epoch 3356, training loss: 10020.84, average training loss: 11505.63, base loss: 19766.11
[INFO 2017-06-27 22:00:27,249 main.py:51] epoch 3357, training loss: 12327.94, average training loss: 11506.03, base loss: 19767.73
[INFO 2017-06-27 22:00:28,250 main.py:51] epoch 3358, training loss: 10987.11, average training loss: 11505.67, base loss: 19766.89
[INFO 2017-06-27 22:00:29,227 main.py:51] epoch 3359, training loss: 10613.70, average training loss: 11504.22, base loss: 19767.24
[INFO 2017-06-27 22:00:30,200 main.py:51] epoch 3360, training loss: 11209.64, average training loss: 11502.26, base loss: 19763.44
[INFO 2017-06-27 22:00:31,178 main.py:51] epoch 3361, training loss: 10732.43, average training loss: 11501.31, base loss: 19762.91
[INFO 2017-06-27 22:00:32,153 main.py:51] epoch 3362, training loss: 13488.09, average training loss: 11504.61, base loss: 19771.60
[INFO 2017-06-27 22:00:33,132 main.py:51] epoch 3363, training loss: 11285.09, average training loss: 11502.58, base loss: 19767.09
[INFO 2017-06-27 22:00:34,106 main.py:51] epoch 3364, training loss: 11918.43, average training loss: 11501.35, base loss: 19766.08
[INFO 2017-06-27 22:00:35,083 main.py:51] epoch 3365, training loss: 11538.56, average training loss: 11502.87, base loss: 19769.09
[INFO 2017-06-27 22:00:36,060 main.py:51] epoch 3366, training loss: 11440.92, average training loss: 11502.31, base loss: 19767.02
[INFO 2017-06-27 22:00:37,037 main.py:51] epoch 3367, training loss: 11807.05, average training loss: 11504.18, base loss: 19771.68
[INFO 2017-06-27 22:00:38,015 main.py:51] epoch 3368, training loss: 10103.59, average training loss: 11501.35, base loss: 19765.54
[INFO 2017-06-27 22:00:38,991 main.py:51] epoch 3369, training loss: 10947.45, average training loss: 11501.20, base loss: 19766.62
[INFO 2017-06-27 22:00:39,965 main.py:51] epoch 3370, training loss: 9439.30, average training loss: 11499.14, base loss: 19761.54
[INFO 2017-06-27 22:00:40,940 main.py:51] epoch 3371, training loss: 10222.07, average training loss: 11496.17, base loss: 19756.37
[INFO 2017-06-27 22:00:41,919 main.py:51] epoch 3372, training loss: 11055.25, average training loss: 11494.69, base loss: 19755.56
[INFO 2017-06-27 22:00:42,892 main.py:51] epoch 3373, training loss: 11569.04, average training loss: 11495.43, base loss: 19758.20
[INFO 2017-06-27 22:00:43,865 main.py:51] epoch 3374, training loss: 13938.35, average training loss: 11496.60, base loss: 19762.39
[INFO 2017-06-27 22:00:44,844 main.py:51] epoch 3375, training loss: 12154.40, average training loss: 11496.50, base loss: 19763.67
[INFO 2017-06-27 22:00:45,821 main.py:51] epoch 3376, training loss: 11020.23, average training loss: 11496.13, base loss: 19763.17
[INFO 2017-06-27 22:00:46,797 main.py:51] epoch 3377, training loss: 11549.81, average training loss: 11494.60, base loss: 19760.77
[INFO 2017-06-27 22:00:47,773 main.py:51] epoch 3378, training loss: 12663.66, average training loss: 11494.64, base loss: 19759.46
[INFO 2017-06-27 22:00:48,746 main.py:51] epoch 3379, training loss: 10994.81, average training loss: 11492.68, base loss: 19755.05
[INFO 2017-06-27 22:00:49,724 main.py:51] epoch 3380, training loss: 11375.88, average training loss: 11493.24, base loss: 19756.65
[INFO 2017-06-27 22:00:50,697 main.py:51] epoch 3381, training loss: 12173.20, average training loss: 11493.02, base loss: 19756.31
[INFO 2017-06-27 22:00:51,672 main.py:51] epoch 3382, training loss: 10786.41, average training loss: 11492.60, base loss: 19755.71
[INFO 2017-06-27 22:00:52,650 main.py:51] epoch 3383, training loss: 11562.45, average training loss: 11493.54, base loss: 19758.55
[INFO 2017-06-27 22:00:53,627 main.py:51] epoch 3384, training loss: 12713.22, average training loss: 11495.32, base loss: 19762.75
[INFO 2017-06-27 22:00:54,603 main.py:51] epoch 3385, training loss: 10543.08, average training loss: 11494.43, base loss: 19760.60
[INFO 2017-06-27 22:00:55,583 main.py:51] epoch 3386, training loss: 10851.69, average training loss: 11494.87, base loss: 19762.11
[INFO 2017-06-27 22:00:56,560 main.py:51] epoch 3387, training loss: 12111.96, average training loss: 11495.65, base loss: 19762.68
[INFO 2017-06-27 22:00:57,534 main.py:51] epoch 3388, training loss: 10665.39, average training loss: 11494.00, base loss: 19760.99
[INFO 2017-06-27 22:00:58,514 main.py:51] epoch 3389, training loss: 12535.11, average training loss: 11494.88, base loss: 19763.87
[INFO 2017-06-27 22:00:59,491 main.py:51] epoch 3390, training loss: 10588.65, average training loss: 11493.20, base loss: 19760.82
[INFO 2017-06-27 22:01:00,463 main.py:51] epoch 3391, training loss: 10399.26, average training loss: 11489.59, base loss: 19753.21
[INFO 2017-06-27 22:01:01,433 main.py:51] epoch 3392, training loss: 9224.85, average training loss: 11485.94, base loss: 19746.12
[INFO 2017-06-27 22:01:02,408 main.py:51] epoch 3393, training loss: 11178.76, average training loss: 11485.80, base loss: 19746.64
[INFO 2017-06-27 22:01:03,385 main.py:51] epoch 3394, training loss: 11153.13, average training loss: 11486.27, base loss: 19749.45
[INFO 2017-06-27 22:01:04,356 main.py:51] epoch 3395, training loss: 11012.79, average training loss: 11482.59, base loss: 19743.09
[INFO 2017-06-27 22:01:05,331 main.py:51] epoch 3396, training loss: 11750.96, average training loss: 11481.76, base loss: 19743.53
[INFO 2017-06-27 22:01:06,309 main.py:51] epoch 3397, training loss: 13853.80, average training loss: 11483.76, base loss: 19748.65
[INFO 2017-06-27 22:01:07,285 main.py:51] epoch 3398, training loss: 11076.80, average training loss: 11484.49, base loss: 19751.05
[INFO 2017-06-27 22:01:08,261 main.py:51] epoch 3399, training loss: 11595.83, average training loss: 11483.91, base loss: 19751.16
[INFO 2017-06-27 22:01:08,262 main.py:53] epoch 3399, testing
[INFO 2017-06-27 22:01:12,474 main.py:105] average testing loss: 10905.79, base loss: 19185.18
[INFO 2017-06-27 22:01:12,474 main.py:106] improve_loss: 8279.39, improve_percent: 0.43
[INFO 2017-06-27 22:01:12,474 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 22:01:13,450 main.py:51] epoch 3400, training loss: 11117.23, average training loss: 11483.91, base loss: 19752.44
[INFO 2017-06-27 22:01:14,432 main.py:51] epoch 3401, training loss: 10986.09, average training loss: 11480.67, base loss: 19746.97
[INFO 2017-06-27 22:01:15,415 main.py:51] epoch 3402, training loss: 11490.46, average training loss: 11480.47, base loss: 19748.29
[INFO 2017-06-27 22:01:16,497 main.py:51] epoch 3403, training loss: 11052.86, average training loss: 11478.86, base loss: 19744.94
[INFO 2017-06-27 22:01:17,599 main.py:51] epoch 3404, training loss: 10745.04, average training loss: 11477.83, base loss: 19744.15
[INFO 2017-06-27 22:01:18,609 main.py:51] epoch 3405, training loss: 12751.84, average training loss: 11478.81, base loss: 19746.51
[INFO 2017-06-27 22:01:19,590 main.py:51] epoch 3406, training loss: 10097.67, average training loss: 11477.02, base loss: 19744.16
[INFO 2017-06-27 22:01:20,566 main.py:51] epoch 3407, training loss: 10843.63, average training loss: 11476.74, base loss: 19745.34
[INFO 2017-06-27 22:01:21,538 main.py:51] epoch 3408, training loss: 11582.41, average training loss: 11475.74, base loss: 19746.14
[INFO 2017-06-27 22:01:22,512 main.py:51] epoch 3409, training loss: 10829.17, average training loss: 11474.74, base loss: 19744.85
[INFO 2017-06-27 22:01:23,483 main.py:51] epoch 3410, training loss: 11225.77, average training loss: 11475.04, base loss: 19748.86
[INFO 2017-06-27 22:01:24,455 main.py:51] epoch 3411, training loss: 14282.56, average training loss: 11477.73, base loss: 19753.75
[INFO 2017-06-27 22:01:25,428 main.py:51] epoch 3412, training loss: 10562.63, average training loss: 11477.46, base loss: 19753.76
[INFO 2017-06-27 22:01:26,573 main.py:51] epoch 3413, training loss: 9644.08, average training loss: 11475.98, base loss: 19750.82
[INFO 2017-06-27 22:01:27,709 main.py:51] epoch 3414, training loss: 10590.70, average training loss: 11472.30, base loss: 19744.97
[INFO 2017-06-27 22:01:28,716 main.py:51] epoch 3415, training loss: 11383.07, average training loss: 11472.44, base loss: 19747.62
[INFO 2017-06-27 22:01:29,828 main.py:51] epoch 3416, training loss: 10208.88, average training loss: 11471.54, base loss: 19746.59
[INFO 2017-06-27 22:01:30,809 main.py:51] epoch 3417, training loss: 9769.26, average training loss: 11468.05, base loss: 19740.18
[INFO 2017-06-27 22:01:31,791 main.py:51] epoch 3418, training loss: 10488.16, average training loss: 11466.48, base loss: 19737.21
[INFO 2017-06-27 22:01:32,766 main.py:51] epoch 3419, training loss: 11063.89, average training loss: 11465.74, base loss: 19736.65
[INFO 2017-06-27 22:01:33,741 main.py:51] epoch 3420, training loss: 11982.04, average training loss: 11465.85, base loss: 19738.79
[INFO 2017-06-27 22:01:34,714 main.py:51] epoch 3421, training loss: 12195.79, average training loss: 11466.18, base loss: 19739.78
[INFO 2017-06-27 22:01:35,695 main.py:51] epoch 3422, training loss: 13474.56, average training loss: 11467.77, base loss: 19744.70
[INFO 2017-06-27 22:01:36,667 main.py:51] epoch 3423, training loss: 11668.27, average training loss: 11466.72, base loss: 19745.11
[INFO 2017-06-27 22:01:37,642 main.py:51] epoch 3424, training loss: 11543.01, average training loss: 11466.31, base loss: 19744.69
[INFO 2017-06-27 22:01:38,624 main.py:51] epoch 3425, training loss: 11680.92, average training loss: 11467.81, base loss: 19748.63
[INFO 2017-06-27 22:01:39,685 main.py:51] epoch 3426, training loss: 10993.46, average training loss: 11464.97, base loss: 19744.31
[INFO 2017-06-27 22:01:40,741 main.py:51] epoch 3427, training loss: 11811.03, average training loss: 11465.41, base loss: 19748.38
[INFO 2017-06-27 22:01:41,762 main.py:51] epoch 3428, training loss: 11231.22, average training loss: 11464.72, base loss: 19748.88
[INFO 2017-06-27 22:01:42,874 main.py:51] epoch 3429, training loss: 10754.22, average training loss: 11464.49, base loss: 19747.65
[INFO 2017-06-27 22:01:43,983 main.py:51] epoch 3430, training loss: 10150.15, average training loss: 11463.37, base loss: 19746.19
[INFO 2017-06-27 22:01:45,108 main.py:51] epoch 3431, training loss: 11958.58, average training loss: 11462.62, base loss: 19744.99
[INFO 2017-06-27 22:01:46,192 main.py:51] epoch 3432, training loss: 10170.17, average training loss: 11461.15, base loss: 19741.85
[INFO 2017-06-27 22:01:47,193 main.py:51] epoch 3433, training loss: 11358.53, average training loss: 11460.96, base loss: 19742.96
[INFO 2017-06-27 22:01:48,169 main.py:51] epoch 3434, training loss: 10949.26, average training loss: 11460.05, base loss: 19741.50
[INFO 2017-06-27 22:01:49,140 main.py:51] epoch 3435, training loss: 13146.10, average training loss: 11462.26, base loss: 19747.58
[INFO 2017-06-27 22:01:50,126 main.py:51] epoch 3436, training loss: 12162.11, average training loss: 11463.89, base loss: 19751.88
[INFO 2017-06-27 22:01:51,097 main.py:51] epoch 3437, training loss: 11139.86, average training loss: 11464.49, base loss: 19754.08
[INFO 2017-06-27 22:01:52,067 main.py:51] epoch 3438, training loss: 10806.99, average training loss: 11463.99, base loss: 19754.81
[INFO 2017-06-27 22:01:53,040 main.py:51] epoch 3439, training loss: 11631.98, average training loss: 11464.08, base loss: 19755.99
[INFO 2017-06-27 22:01:54,014 main.py:51] epoch 3440, training loss: 11085.11, average training loss: 11463.72, base loss: 19756.66
[INFO 2017-06-27 22:01:54,985 main.py:51] epoch 3441, training loss: 9914.28, average training loss: 11461.01, base loss: 19751.66
[INFO 2017-06-27 22:01:55,962 main.py:51] epoch 3442, training loss: 11338.57, average training loss: 11459.39, base loss: 19749.91
[INFO 2017-06-27 22:01:56,934 main.py:51] epoch 3443, training loss: 11331.36, average training loss: 11457.98, base loss: 19747.35
[INFO 2017-06-27 22:01:57,905 main.py:51] epoch 3444, training loss: 10400.25, average training loss: 11457.28, base loss: 19747.48
[INFO 2017-06-27 22:01:58,882 main.py:51] epoch 3445, training loss: 12899.14, average training loss: 11459.41, base loss: 19753.69
[INFO 2017-06-27 22:01:59,856 main.py:51] epoch 3446, training loss: 11593.76, average training loss: 11459.17, base loss: 19754.47
[INFO 2017-06-27 22:02:00,828 main.py:51] epoch 3447, training loss: 10635.48, average training loss: 11457.36, base loss: 19750.92
[INFO 2017-06-27 22:02:01,802 main.py:51] epoch 3448, training loss: 10647.00, average training loss: 11456.02, base loss: 19749.87
[INFO 2017-06-27 22:02:02,771 main.py:51] epoch 3449, training loss: 12967.02, average training loss: 11458.47, base loss: 19757.41
[INFO 2017-06-27 22:02:03,747 main.py:51] epoch 3450, training loss: 11491.56, average training loss: 11458.43, base loss: 19758.94
[INFO 2017-06-27 22:02:04,722 main.py:51] epoch 3451, training loss: 10680.53, average training loss: 11456.45, base loss: 19757.66
[INFO 2017-06-27 22:02:05,694 main.py:51] epoch 3452, training loss: 11121.70, average training loss: 11454.84, base loss: 19755.50
[INFO 2017-06-27 22:02:06,666 main.py:51] epoch 3453, training loss: 10906.02, average training loss: 11454.71, base loss: 19756.30
[INFO 2017-06-27 22:02:07,636 main.py:51] epoch 3454, training loss: 10338.20, average training loss: 11452.50, base loss: 19752.79
[INFO 2017-06-27 22:02:08,609 main.py:51] epoch 3455, training loss: 12596.00, average training loss: 11452.83, base loss: 19756.36
[INFO 2017-06-27 22:02:09,580 main.py:51] epoch 3456, training loss: 10595.62, average training loss: 11451.87, base loss: 19755.99
[INFO 2017-06-27 22:02:10,554 main.py:51] epoch 3457, training loss: 10833.08, average training loss: 11451.23, base loss: 19756.07
[INFO 2017-06-27 22:02:11,522 main.py:51] epoch 3458, training loss: 11151.75, average training loss: 11451.92, base loss: 19757.68
[INFO 2017-06-27 22:02:12,492 main.py:51] epoch 3459, training loss: 11142.79, average training loss: 11451.56, base loss: 19757.29
[INFO 2017-06-27 22:02:13,468 main.py:51] epoch 3460, training loss: 13284.68, average training loss: 11454.76, base loss: 19764.13
[INFO 2017-06-27 22:02:14,483 main.py:51] epoch 3461, training loss: 11570.35, average training loss: 11452.76, base loss: 19759.71
[INFO 2017-06-27 22:02:15,475 main.py:51] epoch 3462, training loss: 11983.63, average training loss: 11452.97, base loss: 19761.54
[INFO 2017-06-27 22:02:16,516 main.py:51] epoch 3463, training loss: 11629.02, average training loss: 11452.75, base loss: 19763.54
[INFO 2017-06-27 22:02:17,530 main.py:51] epoch 3464, training loss: 10897.80, average training loss: 11452.58, base loss: 19765.03
[INFO 2017-06-27 22:02:18,502 main.py:51] epoch 3465, training loss: 11783.83, average training loss: 11452.91, base loss: 19766.56
[INFO 2017-06-27 22:02:19,592 main.py:51] epoch 3466, training loss: 10392.71, average training loss: 11450.22, base loss: 19763.47
[INFO 2017-06-27 22:02:20,570 main.py:51] epoch 3467, training loss: 11050.09, average training loss: 11449.92, base loss: 19762.56
[INFO 2017-06-27 22:02:21,549 main.py:51] epoch 3468, training loss: 10003.21, average training loss: 11449.10, base loss: 19762.49
[INFO 2017-06-27 22:02:22,530 main.py:51] epoch 3469, training loss: 12185.72, average training loss: 11449.11, base loss: 19763.94
[INFO 2017-06-27 22:02:23,502 main.py:51] epoch 3470, training loss: 12281.75, average training loss: 11449.59, base loss: 19766.69
[INFO 2017-06-27 22:02:24,474 main.py:51] epoch 3471, training loss: 11303.37, average training loss: 11448.38, base loss: 19764.50
[INFO 2017-06-27 22:02:25,453 main.py:51] epoch 3472, training loss: 11606.78, average training loss: 11448.38, base loss: 19765.21
[INFO 2017-06-27 22:02:26,425 main.py:51] epoch 3473, training loss: 10682.00, average training loss: 11447.45, base loss: 19765.61
[INFO 2017-06-27 22:02:27,398 main.py:51] epoch 3474, training loss: 10686.46, average training loss: 11446.06, base loss: 19762.49
[INFO 2017-06-27 22:02:28,372 main.py:51] epoch 3475, training loss: 12089.91, average training loss: 11446.64, base loss: 19764.40
[INFO 2017-06-27 22:02:29,349 main.py:51] epoch 3476, training loss: 10037.38, average training loss: 11445.92, base loss: 19762.53
[INFO 2017-06-27 22:02:30,322 main.py:51] epoch 3477, training loss: 12459.66, average training loss: 11446.07, base loss: 19763.34
[INFO 2017-06-27 22:02:31,426 main.py:51] epoch 3478, training loss: 11762.85, average training loss: 11446.67, base loss: 19766.09
[INFO 2017-06-27 22:02:32,420 main.py:51] epoch 3479, training loss: 10241.45, average training loss: 11446.18, base loss: 19766.66
[INFO 2017-06-27 22:02:33,394 main.py:51] epoch 3480, training loss: 10636.38, average training loss: 11444.93, base loss: 19765.31
[INFO 2017-06-27 22:02:34,370 main.py:51] epoch 3481, training loss: 9692.03, average training loss: 11441.98, base loss: 19759.74
[INFO 2017-06-27 22:02:35,344 main.py:51] epoch 3482, training loss: 10817.87, average training loss: 11440.89, base loss: 19758.41
[INFO 2017-06-27 22:02:36,323 main.py:51] epoch 3483, training loss: 12046.74, average training loss: 11441.57, base loss: 19760.25
[INFO 2017-06-27 22:02:37,295 main.py:51] epoch 3484, training loss: 11263.47, average training loss: 11441.22, base loss: 19760.17
[INFO 2017-06-27 22:02:38,267 main.py:51] epoch 3485, training loss: 11355.66, average training loss: 11441.82, base loss: 19762.79
[INFO 2017-06-27 22:02:39,240 main.py:51] epoch 3486, training loss: 11753.73, average training loss: 11441.92, base loss: 19763.70
[INFO 2017-06-27 22:02:40,212 main.py:51] epoch 3487, training loss: 10207.59, average training loss: 11440.41, base loss: 19761.44
[INFO 2017-06-27 22:02:41,186 main.py:51] epoch 3488, training loss: 10929.60, average training loss: 11439.30, base loss: 19761.82
[INFO 2017-06-27 22:02:42,158 main.py:51] epoch 3489, training loss: 9913.35, average training loss: 11438.39, base loss: 19761.71
[INFO 2017-06-27 22:02:43,129 main.py:51] epoch 3490, training loss: 10260.25, average training loss: 11436.87, base loss: 19758.82
[INFO 2017-06-27 22:02:44,101 main.py:51] epoch 3491, training loss: 10786.48, average training loss: 11436.60, base loss: 19759.04
[INFO 2017-06-27 22:02:45,076 main.py:51] epoch 3492, training loss: 12437.14, average training loss: 11439.31, base loss: 19764.41
[INFO 2017-06-27 22:02:46,048 main.py:51] epoch 3493, training loss: 11109.95, average training loss: 11439.27, base loss: 19765.80
[INFO 2017-06-27 22:02:47,020 main.py:51] epoch 3494, training loss: 9661.65, average training loss: 11438.45, base loss: 19763.21
[INFO 2017-06-27 22:02:47,993 main.py:51] epoch 3495, training loss: 10604.11, average training loss: 11437.32, base loss: 19761.44
[INFO 2017-06-27 22:02:48,964 main.py:51] epoch 3496, training loss: 10615.60, average training loss: 11435.71, base loss: 19758.50
[INFO 2017-06-27 22:02:49,943 main.py:51] epoch 3497, training loss: 10643.28, average training loss: 11435.70, base loss: 19758.87
[INFO 2017-06-27 22:02:50,918 main.py:51] epoch 3498, training loss: 10247.85, average training loss: 11434.96, base loss: 19758.83
[INFO 2017-06-27 22:02:51,941 main.py:51] epoch 3499, training loss: 9748.68, average training loss: 11433.17, base loss: 19755.11
[INFO 2017-06-27 22:02:51,941 main.py:53] epoch 3499, testing
[INFO 2017-06-27 22:02:56,409 main.py:105] average testing loss: 11066.22, base loss: 19754.89
[INFO 2017-06-27 22:02:56,409 main.py:106] improve_loss: 8688.67, improve_percent: 0.44
[INFO 2017-06-27 22:02:56,410 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 22:02:56,428 main.py:76] current best improved percent: 0.44
[INFO 2017-06-27 22:02:57,519 main.py:51] epoch 3500, training loss: 10870.79, average training loss: 11432.66, base loss: 19753.77
[INFO 2017-06-27 22:02:58,501 main.py:51] epoch 3501, training loss: 11797.50, average training loss: 11432.46, base loss: 19754.87
[INFO 2017-06-27 22:02:59,570 main.py:51] epoch 3502, training loss: 12992.04, average training loss: 11434.05, base loss: 19759.64
[INFO 2017-06-27 22:03:00,580 main.py:51] epoch 3503, training loss: 10446.36, average training loss: 11433.82, base loss: 19759.15
[INFO 2017-06-27 22:03:01,554 main.py:51] epoch 3504, training loss: 10413.66, average training loss: 11431.77, base loss: 19755.92
[INFO 2017-06-27 22:03:02,530 main.py:51] epoch 3505, training loss: 13395.88, average training loss: 11432.08, base loss: 19757.29
[INFO 2017-06-27 22:03:03,507 main.py:51] epoch 3506, training loss: 11497.51, average training loss: 11432.30, base loss: 19759.96
[INFO 2017-06-27 22:03:04,480 main.py:51] epoch 3507, training loss: 11332.13, average training loss: 11432.18, base loss: 19760.06
[INFO 2017-06-27 22:03:05,463 main.py:51] epoch 3508, training loss: 10548.79, average training loss: 11431.70, base loss: 19759.85
[INFO 2017-06-27 22:03:06,446 main.py:51] epoch 3509, training loss: 10187.49, average training loss: 11429.87, base loss: 19757.46
[INFO 2017-06-27 22:03:07,505 main.py:51] epoch 3510, training loss: 11605.28, average training loss: 11431.15, base loss: 19761.92
[INFO 2017-06-27 22:03:08,642 main.py:51] epoch 3511, training loss: 11267.78, average training loss: 11430.39, base loss: 19761.76
[INFO 2017-06-27 22:03:09,722 main.py:51] epoch 3512, training loss: 10181.19, average training loss: 11429.26, base loss: 19759.19
[INFO 2017-06-27 22:03:10,861 main.py:51] epoch 3513, training loss: 12800.71, average training loss: 11429.94, base loss: 19762.17
[INFO 2017-06-27 22:03:11,881 main.py:51] epoch 3514, training loss: 12285.65, average training loss: 11430.01, base loss: 19763.68
[INFO 2017-06-27 22:03:12,857 main.py:51] epoch 3515, training loss: 10499.22, average training loss: 11429.12, base loss: 19762.20
[INFO 2017-06-27 22:03:13,924 main.py:51] epoch 3516, training loss: 10766.32, average training loss: 11428.19, base loss: 19759.86
[INFO 2017-06-27 22:03:15,073 main.py:51] epoch 3517, training loss: 12126.51, average training loss: 11427.94, base loss: 19760.03
[INFO 2017-06-27 22:03:16,094 main.py:51] epoch 3518, training loss: 11285.02, average training loss: 11428.98, base loss: 19762.27
[INFO 2017-06-27 22:03:17,066 main.py:51] epoch 3519, training loss: 10616.19, average training loss: 11427.63, base loss: 19759.43
[INFO 2017-06-27 22:03:18,044 main.py:51] epoch 3520, training loss: 10435.56, average training loss: 11426.37, base loss: 19758.71
[INFO 2017-06-27 22:03:19,019 main.py:51] epoch 3521, training loss: 10979.95, average training loss: 11426.84, base loss: 19760.18
[INFO 2017-06-27 22:03:19,991 main.py:51] epoch 3522, training loss: 11890.45, average training loss: 11427.13, base loss: 19762.11
[INFO 2017-06-27 22:03:20,963 main.py:51] epoch 3523, training loss: 10800.57, average training loss: 11423.30, base loss: 19757.31
[INFO 2017-06-27 22:03:21,939 main.py:51] epoch 3524, training loss: 10386.48, average training loss: 11421.89, base loss: 19755.82
[INFO 2017-06-27 22:03:23,028 main.py:51] epoch 3525, training loss: 10945.03, average training loss: 11420.53, base loss: 19754.22
[INFO 2017-06-27 22:03:24,072 main.py:51] epoch 3526, training loss: 11543.34, average training loss: 11418.53, base loss: 19752.02
[INFO 2017-06-27 22:03:25,067 main.py:51] epoch 3527, training loss: 9840.78, average training loss: 11415.30, base loss: 19745.77
[INFO 2017-06-27 22:03:26,152 main.py:51] epoch 3528, training loss: 10538.46, average training loss: 11416.11, base loss: 19749.67
[INFO 2017-06-27 22:03:27,238 main.py:51] epoch 3529, training loss: 11491.87, average training loss: 11416.43, base loss: 19750.31
[INFO 2017-06-27 22:03:28,232 main.py:51] epoch 3530, training loss: 9759.00, average training loss: 11415.41, base loss: 19748.40
[INFO 2017-06-27 22:03:29,204 main.py:51] epoch 3531, training loss: 9627.15, average training loss: 11413.77, base loss: 19744.94
[INFO 2017-06-27 22:03:30,186 main.py:51] epoch 3532, training loss: 11679.93, average training loss: 11413.05, base loss: 19744.91
[INFO 2017-06-27 22:03:31,155 main.py:51] epoch 3533, training loss: 10269.68, average training loss: 11412.16, base loss: 19744.44
[INFO 2017-06-27 22:03:32,126 main.py:51] epoch 3534, training loss: 10814.50, average training loss: 11409.64, base loss: 19740.08
[INFO 2017-06-27 22:03:33,098 main.py:51] epoch 3535, training loss: 11924.47, average training loss: 11411.48, base loss: 19745.86
[INFO 2017-06-27 22:03:34,070 main.py:51] epoch 3536, training loss: 11486.44, average training loss: 11411.03, base loss: 19743.88
[INFO 2017-06-27 22:03:35,040 main.py:51] epoch 3537, training loss: 11108.82, average training loss: 11410.61, base loss: 19742.14
[INFO 2017-06-27 22:03:36,013 main.py:51] epoch 3538, training loss: 11363.08, average training loss: 11410.64, base loss: 19744.45
[INFO 2017-06-27 22:03:36,983 main.py:51] epoch 3539, training loss: 10666.19, average training loss: 11409.24, base loss: 19743.09
[INFO 2017-06-27 22:03:37,953 main.py:51] epoch 3540, training loss: 10410.59, average training loss: 11408.36, base loss: 19742.90
[INFO 2017-06-27 22:03:38,925 main.py:51] epoch 3541, training loss: 11106.61, average training loss: 11408.35, base loss: 19745.99
[INFO 2017-06-27 22:03:39,897 main.py:51] epoch 3542, training loss: 11029.63, average training loss: 11408.24, base loss: 19747.81
[INFO 2017-06-27 22:03:40,867 main.py:51] epoch 3543, training loss: 10553.18, average training loss: 11407.62, base loss: 19746.78
[INFO 2017-06-27 22:03:41,839 main.py:51] epoch 3544, training loss: 11326.04, average training loss: 11407.85, base loss: 19749.20
[INFO 2017-06-27 22:03:42,810 main.py:51] epoch 3545, training loss: 11875.32, average training loss: 11406.21, base loss: 19747.71
[INFO 2017-06-27 22:03:43,781 main.py:51] epoch 3546, training loss: 9853.34, average training loss: 11404.42, base loss: 19744.87
[INFO 2017-06-27 22:03:44,753 main.py:51] epoch 3547, training loss: 10498.88, average training loss: 11402.41, base loss: 19741.37
[INFO 2017-06-27 22:03:45,722 main.py:51] epoch 3548, training loss: 10358.56, average training loss: 11402.31, base loss: 19740.13
[INFO 2017-06-27 22:03:46,695 main.py:51] epoch 3549, training loss: 11255.10, average training loss: 11401.20, base loss: 19738.31
[INFO 2017-06-27 22:03:47,666 main.py:51] epoch 3550, training loss: 11595.05, average training loss: 11401.87, base loss: 19740.02
[INFO 2017-06-27 22:03:48,637 main.py:51] epoch 3551, training loss: 11095.23, average training loss: 11402.68, base loss: 19744.63
[INFO 2017-06-27 22:03:49,607 main.py:51] epoch 3552, training loss: 13241.11, average training loss: 11402.57, base loss: 19744.95
[INFO 2017-06-27 22:03:50,581 main.py:51] epoch 3553, training loss: 11142.43, average training loss: 11403.04, base loss: 19747.25
[INFO 2017-06-27 22:03:51,602 main.py:51] epoch 3554, training loss: 10160.06, average training loss: 11401.21, base loss: 19743.29
[INFO 2017-06-27 22:03:52,609 main.py:51] epoch 3555, training loss: 11505.71, average training loss: 11402.26, base loss: 19746.67
[INFO 2017-06-27 22:03:53,689 main.py:51] epoch 3556, training loss: 12116.64, average training loss: 11402.77, base loss: 19749.12
[INFO 2017-06-27 22:03:54,672 main.py:51] epoch 3557, training loss: 10424.58, average training loss: 11402.06, base loss: 19748.53
[INFO 2017-06-27 22:03:55,649 main.py:51] epoch 3558, training loss: 12459.87, average training loss: 11402.96, base loss: 19751.33
[INFO 2017-06-27 22:03:56,619 main.py:51] epoch 3559, training loss: 10855.73, average training loss: 11401.36, base loss: 19746.89
[INFO 2017-06-27 22:03:57,590 main.py:51] epoch 3560, training loss: 12031.80, average training loss: 11401.49, base loss: 19750.39
[INFO 2017-06-27 22:03:58,664 main.py:51] epoch 3561, training loss: 12674.12, average training loss: 11401.82, base loss: 19751.97
[INFO 2017-06-27 22:03:59,754 main.py:51] epoch 3562, training loss: 11958.51, average training loss: 11403.30, base loss: 19755.88
[INFO 2017-06-27 22:04:00,856 main.py:51] epoch 3563, training loss: 10820.35, average training loss: 11401.91, base loss: 19753.73
[INFO 2017-06-27 22:04:01,847 main.py:51] epoch 3564, training loss: 11652.10, average training loss: 11401.35, base loss: 19755.53
[INFO 2017-06-27 22:04:02,827 main.py:51] epoch 3565, training loss: 11575.55, average training loss: 11402.47, base loss: 19756.54
[INFO 2017-06-27 22:04:03,804 main.py:51] epoch 3566, training loss: 11567.05, average training loss: 11402.35, base loss: 19757.99
[INFO 2017-06-27 22:04:04,776 main.py:51] epoch 3567, training loss: 11358.81, average training loss: 11401.84, base loss: 19759.48
[INFO 2017-06-27 22:04:05,753 main.py:51] epoch 3568, training loss: 12755.05, average training loss: 11403.25, base loss: 19763.04
[INFO 2017-06-27 22:04:06,730 main.py:51] epoch 3569, training loss: 11642.45, average training loss: 11403.79, base loss: 19765.66
[INFO 2017-06-27 22:04:07,704 main.py:51] epoch 3570, training loss: 11559.03, average training loss: 11405.03, base loss: 19770.31
[INFO 2017-06-27 22:04:08,678 main.py:51] epoch 3571, training loss: 11244.33, average training loss: 11404.40, base loss: 19772.11
[INFO 2017-06-27 22:04:09,651 main.py:51] epoch 3572, training loss: 11201.20, average training loss: 11402.71, base loss: 19768.29
[INFO 2017-06-27 22:04:10,624 main.py:51] epoch 3573, training loss: 10943.42, average training loss: 11402.35, base loss: 19769.15
[INFO 2017-06-27 22:04:11,597 main.py:51] epoch 3574, training loss: 13131.28, average training loss: 11404.46, base loss: 19776.25
[INFO 2017-06-27 22:04:12,569 main.py:51] epoch 3575, training loss: 11284.96, average training loss: 11404.99, base loss: 19776.99
[INFO 2017-06-27 22:04:13,541 main.py:51] epoch 3576, training loss: 11511.26, average training loss: 11405.80, base loss: 19780.05
[INFO 2017-06-27 22:04:14,518 main.py:51] epoch 3577, training loss: 10675.29, average training loss: 11402.45, base loss: 19773.87
[INFO 2017-06-27 22:04:15,489 main.py:51] epoch 3578, training loss: 13698.18, average training loss: 11403.84, base loss: 19778.86
[INFO 2017-06-27 22:04:16,466 main.py:51] epoch 3579, training loss: 10530.47, average training loss: 11402.55, base loss: 19777.00
[INFO 2017-06-27 22:04:17,438 main.py:51] epoch 3580, training loss: 11378.00, average training loss: 11404.02, base loss: 19782.23
[INFO 2017-06-27 22:04:18,414 main.py:51] epoch 3581, training loss: 11402.69, average training loss: 11404.00, base loss: 19782.33
[INFO 2017-06-27 22:04:19,493 main.py:51] epoch 3582, training loss: 10787.19, average training loss: 11400.64, base loss: 19775.66
[INFO 2017-06-27 22:04:20,507 main.py:51] epoch 3583, training loss: 12274.71, average training loss: 11401.10, base loss: 19777.02
[INFO 2017-06-27 22:04:21,485 main.py:51] epoch 3584, training loss: 11445.40, average training loss: 11399.45, base loss: 19775.73
[INFO 2017-06-27 22:04:22,456 main.py:51] epoch 3585, training loss: 10610.27, average training loss: 11399.34, base loss: 19776.24
[INFO 2017-06-27 22:04:23,426 main.py:51] epoch 3586, training loss: 11633.25, average training loss: 11399.55, base loss: 19777.80
[INFO 2017-06-27 22:04:24,397 main.py:51] epoch 3587, training loss: 11150.93, average training loss: 11398.94, base loss: 19778.51
[INFO 2017-06-27 22:04:25,368 main.py:51] epoch 3588, training loss: 11187.74, average training loss: 11398.07, base loss: 19777.61
[INFO 2017-06-27 22:04:26,337 main.py:51] epoch 3589, training loss: 10664.14, average training loss: 11395.76, base loss: 19772.19
[INFO 2017-06-27 22:04:27,316 main.py:51] epoch 3590, training loss: 12131.04, average training loss: 11396.37, base loss: 19774.96
[INFO 2017-06-27 22:04:28,288 main.py:51] epoch 3591, training loss: 11286.71, average training loss: 11395.75, base loss: 19774.70
[INFO 2017-06-27 22:04:29,268 main.py:51] epoch 3592, training loss: 11463.39, average training loss: 11393.41, base loss: 19771.05
[INFO 2017-06-27 22:04:30,340 main.py:51] epoch 3593, training loss: 10807.32, average training loss: 11392.56, base loss: 19769.52
[INFO 2017-06-27 22:04:31,382 main.py:51] epoch 3594, training loss: 11683.45, average training loss: 11392.88, base loss: 19772.31
[INFO 2017-06-27 22:04:32,401 main.py:51] epoch 3595, training loss: 11689.60, average training loss: 11393.43, base loss: 19775.02
[INFO 2017-06-27 22:04:33,376 main.py:51] epoch 3596, training loss: 11531.15, average training loss: 11392.53, base loss: 19775.08
[INFO 2017-06-27 22:04:34,359 main.py:51] epoch 3597, training loss: 12319.80, average training loss: 11394.07, base loss: 19779.96
[INFO 2017-06-27 22:04:35,333 main.py:51] epoch 3598, training loss: 11369.96, average training loss: 11394.89, base loss: 19781.93
[INFO 2017-06-27 22:04:36,311 main.py:51] epoch 3599, training loss: 10031.24, average training loss: 11393.10, base loss: 19780.04
[INFO 2017-06-27 22:04:36,312 main.py:53] epoch 3599, testing
[INFO 2017-06-27 22:04:40,519 main.py:105] average testing loss: 11758.28, base loss: 21491.23
[INFO 2017-06-27 22:04:40,519 main.py:106] improve_loss: 9732.95, improve_percent: 0.45
[INFO 2017-06-27 22:04:40,519 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 22:04:40,535 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:04:41,506 main.py:51] epoch 3600, training loss: 10615.15, average training loss: 11392.71, base loss: 19779.96
[INFO 2017-06-27 22:04:42,586 main.py:51] epoch 3601, training loss: 10340.50, average training loss: 11388.33, base loss: 19771.17
[INFO 2017-06-27 22:04:43,651 main.py:51] epoch 3602, training loss: 11026.18, average training loss: 11387.82, base loss: 19770.35
[INFO 2017-06-27 22:04:44,662 main.py:51] epoch 3603, training loss: 11111.93, average training loss: 11388.31, base loss: 19772.39
[INFO 2017-06-27 22:04:45,640 main.py:51] epoch 3604, training loss: 11986.10, average training loss: 11388.12, base loss: 19770.34
[INFO 2017-06-27 22:04:46,616 main.py:51] epoch 3605, training loss: 9219.31, average training loss: 11385.96, base loss: 19766.60
[INFO 2017-06-27 22:04:47,596 main.py:51] epoch 3606, training loss: 10047.25, average training loss: 11384.81, base loss: 19766.55
[INFO 2017-06-27 22:04:48,573 main.py:51] epoch 3607, training loss: 11925.25, average training loss: 11384.96, base loss: 19768.65
[INFO 2017-06-27 22:04:49,554 main.py:51] epoch 3608, training loss: 10880.18, average training loss: 11383.16, base loss: 19764.68
[INFO 2017-06-27 22:04:50,584 main.py:51] epoch 3609, training loss: 12163.65, average training loss: 11384.69, base loss: 19769.62
[INFO 2017-06-27 22:04:51,734 main.py:51] epoch 3610, training loss: 11330.40, average training loss: 11383.66, base loss: 19767.60
[INFO 2017-06-27 22:04:52,832 main.py:51] epoch 3611, training loss: 10570.12, average training loss: 11382.59, base loss: 19767.44
[INFO 2017-06-27 22:04:53,977 main.py:51] epoch 3612, training loss: 12054.39, average training loss: 11382.59, base loss: 19768.94
[INFO 2017-06-27 22:04:54,971 main.py:51] epoch 3613, training loss: 11752.13, average training loss: 11383.15, base loss: 19771.84
[INFO 2017-06-27 22:04:55,944 main.py:51] epoch 3614, training loss: 10736.39, average training loss: 11383.42, base loss: 19775.76
[INFO 2017-06-27 22:04:56,917 main.py:51] epoch 3615, training loss: 11508.32, average training loss: 11384.30, base loss: 19777.53
[INFO 2017-06-27 22:04:57,937 main.py:51] epoch 3616, training loss: 10636.85, average training loss: 11384.15, base loss: 19776.94
[INFO 2017-06-27 22:04:58,941 main.py:51] epoch 3617, training loss: 10444.43, average training loss: 11382.42, base loss: 19773.71
[INFO 2017-06-27 22:04:59,921 main.py:51] epoch 3618, training loss: 11873.05, average training loss: 11382.86, base loss: 19776.93
[INFO 2017-06-27 22:05:00,899 main.py:51] epoch 3619, training loss: 11927.22, average training loss: 11382.85, base loss: 19777.66
[INFO 2017-06-27 22:05:01,879 main.py:51] epoch 3620, training loss: 12385.16, average training loss: 11383.14, base loss: 19779.84
[INFO 2017-06-27 22:05:02,972 main.py:51] epoch 3621, training loss: 13222.36, average training loss: 11385.33, base loss: 19785.68
[INFO 2017-06-27 22:05:04,010 main.py:51] epoch 3622, training loss: 11651.10, average training loss: 11385.72, base loss: 19786.35
[INFO 2017-06-27 22:05:04,987 main.py:51] epoch 3623, training loss: 10772.67, average training loss: 11384.70, base loss: 19784.21
[INFO 2017-06-27 22:05:05,964 main.py:51] epoch 3624, training loss: 12282.19, average training loss: 11386.04, base loss: 19789.29
[INFO 2017-06-27 22:05:06,940 main.py:51] epoch 3625, training loss: 12091.00, average training loss: 11385.67, base loss: 19790.94
[INFO 2017-06-27 22:05:07,911 main.py:51] epoch 3626, training loss: 11287.01, average training loss: 11385.87, base loss: 19793.01
[INFO 2017-06-27 22:05:08,885 main.py:51] epoch 3627, training loss: 12072.45, average training loss: 11386.43, base loss: 19795.57
[INFO 2017-06-27 22:05:09,856 main.py:51] epoch 3628, training loss: 9795.51, average training loss: 11385.24, base loss: 19793.72
[INFO 2017-06-27 22:05:10,826 main.py:51] epoch 3629, training loss: 11139.40, average training loss: 11384.00, base loss: 19789.65
[INFO 2017-06-27 22:05:11,797 main.py:51] epoch 3630, training loss: 11029.76, average training loss: 11383.00, base loss: 19788.64
[INFO 2017-06-27 22:05:12,767 main.py:51] epoch 3631, training loss: 10640.67, average training loss: 11382.57, base loss: 19787.52
[INFO 2017-06-27 22:05:13,740 main.py:51] epoch 3632, training loss: 10802.00, average training loss: 11381.99, base loss: 19785.48
[INFO 2017-06-27 22:05:14,714 main.py:51] epoch 3633, training loss: 12338.29, average training loss: 11384.87, base loss: 19793.11
[INFO 2017-06-27 22:05:15,692 main.py:51] epoch 3634, training loss: 12335.49, average training loss: 11385.96, base loss: 19797.85
[INFO 2017-06-27 22:05:16,664 main.py:51] epoch 3635, training loss: 10866.77, average training loss: 11384.98, base loss: 19795.69
[INFO 2017-06-27 22:05:17,636 main.py:51] epoch 3636, training loss: 11624.75, average training loss: 11384.76, base loss: 19795.67
[INFO 2017-06-27 22:05:18,658 main.py:51] epoch 3637, training loss: 9546.39, average training loss: 11383.23, base loss: 19792.97
[INFO 2017-06-27 22:05:19,673 main.py:51] epoch 3638, training loss: 10615.70, average training loss: 11382.50, base loss: 19792.62
[INFO 2017-06-27 22:05:20,729 main.py:51] epoch 3639, training loss: 10780.58, average training loss: 11382.03, base loss: 19792.99
[INFO 2017-06-27 22:05:21,703 main.py:51] epoch 3640, training loss: 9896.97, average training loss: 11381.85, base loss: 19792.65
[INFO 2017-06-27 22:05:22,681 main.py:51] epoch 3641, training loss: 10119.90, average training loss: 11380.48, base loss: 19792.48
[INFO 2017-06-27 22:05:23,663 main.py:51] epoch 3642, training loss: 11533.51, average training loss: 11381.42, base loss: 19795.63
[INFO 2017-06-27 22:05:24,636 main.py:51] epoch 3643, training loss: 8804.72, average training loss: 11375.91, base loss: 19783.99
[INFO 2017-06-27 22:05:25,608 main.py:51] epoch 3644, training loss: 11606.06, average training loss: 11376.62, base loss: 19786.14
[INFO 2017-06-27 22:05:26,583 main.py:51] epoch 3645, training loss: 11506.82, average training loss: 11376.37, base loss: 19784.97
[INFO 2017-06-27 22:05:27,555 main.py:51] epoch 3646, training loss: 10738.12, average training loss: 11376.40, base loss: 19784.46
[INFO 2017-06-27 22:05:28,530 main.py:51] epoch 3647, training loss: 11038.18, average training loss: 11373.09, base loss: 19778.47
[INFO 2017-06-27 22:05:29,506 main.py:51] epoch 3648, training loss: 10693.26, average training loss: 11372.77, base loss: 19779.82
[INFO 2017-06-27 22:05:30,478 main.py:51] epoch 3649, training loss: 11463.51, average training loss: 11371.70, base loss: 19778.56
[INFO 2017-06-27 22:05:31,449 main.py:51] epoch 3650, training loss: 13634.24, average training loss: 11374.33, base loss: 19785.12
[INFO 2017-06-27 22:05:32,425 main.py:51] epoch 3651, training loss: 10115.83, average training loss: 11373.11, base loss: 19784.49
[INFO 2017-06-27 22:05:33,401 main.py:51] epoch 3652, training loss: 10985.22, average training loss: 11372.91, base loss: 19784.85
[INFO 2017-06-27 22:05:34,374 main.py:51] epoch 3653, training loss: 10493.83, average training loss: 11371.57, base loss: 19781.15
[INFO 2017-06-27 22:05:35,348 main.py:51] epoch 3654, training loss: 10808.98, average training loss: 11371.27, base loss: 19781.41
[INFO 2017-06-27 22:05:36,323 main.py:51] epoch 3655, training loss: 11848.21, average training loss: 11370.32, base loss: 19782.29
[INFO 2017-06-27 22:05:37,296 main.py:51] epoch 3656, training loss: 10125.57, average training loss: 11370.52, base loss: 19784.83
[INFO 2017-06-27 22:05:38,270 main.py:51] epoch 3657, training loss: 10716.53, average training loss: 11369.11, base loss: 19783.03
[INFO 2017-06-27 22:05:39,243 main.py:51] epoch 3658, training loss: 11024.51, average training loss: 11369.37, base loss: 19784.43
[INFO 2017-06-27 22:05:40,220 main.py:51] epoch 3659, training loss: 10194.48, average training loss: 11367.47, base loss: 19782.53
[INFO 2017-06-27 22:05:41,199 main.py:51] epoch 3660, training loss: 11693.00, average training loss: 11367.91, base loss: 19783.34
[INFO 2017-06-27 22:05:42,177 main.py:51] epoch 3661, training loss: 10867.46, average training loss: 11366.79, base loss: 19782.54
[INFO 2017-06-27 22:05:43,150 main.py:51] epoch 3662, training loss: 11274.52, average training loss: 11365.06, base loss: 19781.18
[INFO 2017-06-27 22:05:44,128 main.py:51] epoch 3663, training loss: 11217.99, average training loss: 11364.38, base loss: 19779.87
[INFO 2017-06-27 22:05:45,104 main.py:51] epoch 3664, training loss: 11098.29, average training loss: 11364.74, base loss: 19780.90
[INFO 2017-06-27 22:05:46,182 main.py:51] epoch 3665, training loss: 11712.22, average training loss: 11364.74, base loss: 19781.53
[INFO 2017-06-27 22:05:47,171 main.py:51] epoch 3666, training loss: 10630.42, average training loss: 11363.81, base loss: 19780.70
[INFO 2017-06-27 22:05:48,154 main.py:51] epoch 3667, training loss: 9573.42, average training loss: 11360.39, base loss: 19774.68
[INFO 2017-06-27 22:05:49,129 main.py:51] epoch 3668, training loss: 11495.57, average training loss: 11359.85, base loss: 19774.84
[INFO 2017-06-27 22:05:50,103 main.py:51] epoch 3669, training loss: 11222.52, average training loss: 11357.22, base loss: 19770.40
[INFO 2017-06-27 22:05:51,077 main.py:51] epoch 3670, training loss: 10423.46, average training loss: 11356.36, base loss: 19769.21
[INFO 2017-06-27 22:05:52,052 main.py:51] epoch 3671, training loss: 12125.54, average training loss: 11357.61, base loss: 19773.50
[INFO 2017-06-27 22:05:53,026 main.py:51] epoch 3672, training loss: 11410.80, average training loss: 11358.60, base loss: 19776.36
[INFO 2017-06-27 22:05:54,000 main.py:51] epoch 3673, training loss: 12085.70, average training loss: 11358.69, base loss: 19777.89
[INFO 2017-06-27 22:05:54,975 main.py:51] epoch 3674, training loss: 10746.43, average training loss: 11356.95, base loss: 19774.71
[INFO 2017-06-27 22:05:55,957 main.py:51] epoch 3675, training loss: 11320.41, average training loss: 11357.19, base loss: 19776.02
[INFO 2017-06-27 22:05:56,927 main.py:51] epoch 3676, training loss: 10704.27, average training loss: 11356.82, base loss: 19777.16
[INFO 2017-06-27 22:05:57,899 main.py:51] epoch 3677, training loss: 10548.60, average training loss: 11355.46, base loss: 19775.80
[INFO 2017-06-27 22:05:58,870 main.py:51] epoch 3678, training loss: 12100.48, average training loss: 11354.82, base loss: 19777.48
[INFO 2017-06-27 22:05:59,842 main.py:51] epoch 3679, training loss: 11111.46, average training loss: 11354.48, base loss: 19778.03
[INFO 2017-06-27 22:06:00,814 main.py:51] epoch 3680, training loss: 11948.46, average training loss: 11355.04, base loss: 19778.76
[INFO 2017-06-27 22:06:01,789 main.py:51] epoch 3681, training loss: 11025.72, average training loss: 11354.45, base loss: 19780.59
[INFO 2017-06-27 22:06:02,761 main.py:51] epoch 3682, training loss: 10888.08, average training loss: 11353.73, base loss: 19780.75
[INFO 2017-06-27 22:06:03,839 main.py:51] epoch 3683, training loss: 12091.40, average training loss: 11354.14, base loss: 19783.47
[INFO 2017-06-27 22:06:04,899 main.py:51] epoch 3684, training loss: 10441.02, average training loss: 11353.30, base loss: 19782.92
[INFO 2017-06-27 22:06:05,892 main.py:51] epoch 3685, training loss: 10459.23, average training loss: 11352.39, base loss: 19781.25
[INFO 2017-06-27 22:06:06,983 main.py:51] epoch 3686, training loss: 12625.17, average training loss: 11353.30, base loss: 19783.79
[INFO 2017-06-27 22:06:08,058 main.py:51] epoch 3687, training loss: 12879.80, average training loss: 11354.68, base loss: 19786.96
[INFO 2017-06-27 22:06:09,067 main.py:51] epoch 3688, training loss: 11460.64, average training loss: 11354.78, base loss: 19786.09
[INFO 2017-06-27 22:06:10,147 main.py:51] epoch 3689, training loss: 10529.39, average training loss: 11353.15, base loss: 19783.50
[INFO 2017-06-27 22:06:11,226 main.py:51] epoch 3690, training loss: 10414.46, average training loss: 11351.42, base loss: 19781.74
[INFO 2017-06-27 22:06:12,331 main.py:51] epoch 3691, training loss: 11461.59, average training loss: 11352.24, base loss: 19783.90
[INFO 2017-06-27 22:06:13,376 main.py:51] epoch 3692, training loss: 11664.47, average training loss: 11351.43, base loss: 19782.96
[INFO 2017-06-27 22:06:14,351 main.py:51] epoch 3693, training loss: 9644.34, average training loss: 11349.73, base loss: 19779.08
[INFO 2017-06-27 22:06:15,338 main.py:51] epoch 3694, training loss: 10531.72, average training loss: 11349.16, base loss: 19778.53
[INFO 2017-06-27 22:06:16,315 main.py:51] epoch 3695, training loss: 11070.59, average training loss: 11349.62, base loss: 19781.66
[INFO 2017-06-27 22:06:17,291 main.py:51] epoch 3696, training loss: 12925.91, average training loss: 11350.62, base loss: 19784.54
[INFO 2017-06-27 22:06:18,265 main.py:51] epoch 3697, training loss: 10919.39, average training loss: 11349.53, base loss: 19783.40
[INFO 2017-06-27 22:06:19,238 main.py:51] epoch 3698, training loss: 11708.11, average training loss: 11348.72, base loss: 19781.95
[INFO 2017-06-27 22:06:20,218 main.py:51] epoch 3699, training loss: 12997.89, average training loss: 11350.34, base loss: 19786.80
[INFO 2017-06-27 22:06:20,218 main.py:53] epoch 3699, testing
[INFO 2017-06-27 22:06:24,429 main.py:105] average testing loss: 10995.96, base loss: 19617.63
[INFO 2017-06-27 22:06:24,429 main.py:106] improve_loss: 8621.66, improve_percent: 0.44
[INFO 2017-06-27 22:06:24,429 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:06:25,404 main.py:51] epoch 3700, training loss: 11756.98, average training loss: 11351.77, base loss: 19789.73
[INFO 2017-06-27 22:06:26,378 main.py:51] epoch 3701, training loss: 11894.79, average training loss: 11353.05, base loss: 19791.37
[INFO 2017-06-27 22:06:27,355 main.py:51] epoch 3702, training loss: 10474.16, average training loss: 11353.05, base loss: 19793.09
[INFO 2017-06-27 22:06:28,325 main.py:51] epoch 3703, training loss: 10631.01, average training loss: 11352.18, base loss: 19794.21
[INFO 2017-06-27 22:06:29,306 main.py:51] epoch 3704, training loss: 11411.86, average training loss: 11352.67, base loss: 19798.15
[INFO 2017-06-27 22:06:30,319 main.py:51] epoch 3705, training loss: 11106.56, average training loss: 11352.29, base loss: 19799.24
[INFO 2017-06-27 22:06:31,329 main.py:51] epoch 3706, training loss: 11173.38, average training loss: 11351.53, base loss: 19798.43
[INFO 2017-06-27 22:06:32,305 main.py:51] epoch 3707, training loss: 12032.12, average training loss: 11350.76, base loss: 19798.72
[INFO 2017-06-27 22:06:33,412 main.py:51] epoch 3708, training loss: 11249.01, average training loss: 11350.68, base loss: 19800.23
[INFO 2017-06-27 22:06:34,395 main.py:51] epoch 3709, training loss: 10025.29, average training loss: 11349.80, base loss: 19799.12
[INFO 2017-06-27 22:06:35,366 main.py:51] epoch 3710, training loss: 9709.47, average training loss: 11348.09, base loss: 19795.26
[INFO 2017-06-27 22:06:36,437 main.py:51] epoch 3711, training loss: 11415.45, average training loss: 11347.93, base loss: 19795.03
[INFO 2017-06-27 22:06:37,491 main.py:51] epoch 3712, training loss: 10673.09, average training loss: 11347.48, base loss: 19794.88
[INFO 2017-06-27 22:06:38,486 main.py:51] epoch 3713, training loss: 10426.69, average training loss: 11347.50, base loss: 19795.12
[INFO 2017-06-27 22:06:39,461 main.py:51] epoch 3714, training loss: 11814.23, average training loss: 11349.26, base loss: 19801.04
[INFO 2017-06-27 22:06:40,440 main.py:51] epoch 3715, training loss: 11553.42, average training loss: 11349.34, base loss: 19801.77
[INFO 2017-06-27 22:06:41,416 main.py:51] epoch 3716, training loss: 9892.65, average training loss: 11347.29, base loss: 19798.29
[INFO 2017-06-27 22:06:42,389 main.py:51] epoch 3717, training loss: 11446.62, average training loss: 11345.20, base loss: 19796.03
[INFO 2017-06-27 22:06:43,363 main.py:51] epoch 3718, training loss: 10197.79, average training loss: 11343.24, base loss: 19791.34
[INFO 2017-06-27 22:06:44,340 main.py:51] epoch 3719, training loss: 11481.45, average training loss: 11342.71, base loss: 19792.99
[INFO 2017-06-27 22:06:45,314 main.py:51] epoch 3720, training loss: 10391.48, average training loss: 11342.71, base loss: 19794.90
[INFO 2017-06-27 22:06:46,400 main.py:51] epoch 3721, training loss: 11189.98, average training loss: 11342.41, base loss: 19795.21
[INFO 2017-06-27 22:06:47,408 main.py:51] epoch 3722, training loss: 10522.74, average training loss: 11340.94, base loss: 19793.48
[INFO 2017-06-27 22:06:48,381 main.py:51] epoch 3723, training loss: 9758.14, average training loss: 11339.34, base loss: 19790.55
[INFO 2017-06-27 22:06:49,361 main.py:51] epoch 3724, training loss: 9401.91, average training loss: 11337.84, base loss: 19788.83
[INFO 2017-06-27 22:06:50,337 main.py:51] epoch 3725, training loss: 11172.77, average training loss: 11337.11, base loss: 19788.31
[INFO 2017-06-27 22:06:51,308 main.py:51] epoch 3726, training loss: 10499.99, average training loss: 11337.40, base loss: 19789.96
[INFO 2017-06-27 22:06:52,282 main.py:51] epoch 3727, training loss: 10951.66, average training loss: 11337.16, base loss: 19789.44
[INFO 2017-06-27 22:06:53,256 main.py:51] epoch 3728, training loss: 11440.87, average training loss: 11335.51, base loss: 19788.51
[INFO 2017-06-27 22:06:54,227 main.py:51] epoch 3729, training loss: 12650.54, average training loss: 11336.54, base loss: 19792.83
[INFO 2017-06-27 22:06:55,199 main.py:51] epoch 3730, training loss: 10488.20, average training loss: 11334.32, base loss: 19790.09
[INFO 2017-06-27 22:06:56,170 main.py:51] epoch 3731, training loss: 9889.42, average training loss: 11332.24, base loss: 19787.90
[INFO 2017-06-27 22:06:57,144 main.py:51] epoch 3732, training loss: 10785.97, average training loss: 11330.40, base loss: 19785.16
[INFO 2017-06-27 22:06:58,173 main.py:51] epoch 3733, training loss: 11250.80, average training loss: 11330.34, base loss: 19786.37
[INFO 2017-06-27 22:06:59,167 main.py:51] epoch 3734, training loss: 9147.88, average training loss: 11326.89, base loss: 19779.76
[INFO 2017-06-27 22:07:00,142 main.py:51] epoch 3735, training loss: 10914.83, average training loss: 11325.35, base loss: 19776.58
[INFO 2017-06-27 22:07:01,126 main.py:51] epoch 3736, training loss: 11039.32, average training loss: 11325.86, base loss: 19776.71
[INFO 2017-06-27 22:07:02,099 main.py:51] epoch 3737, training loss: 12224.94, average training loss: 11326.13, base loss: 19778.99
[INFO 2017-06-27 22:07:03,071 main.py:51] epoch 3738, training loss: 9684.18, average training loss: 11323.15, base loss: 19773.59
[INFO 2017-06-27 22:07:04,047 main.py:51] epoch 3739, training loss: 12552.60, average training loss: 11324.01, base loss: 19776.63
[INFO 2017-06-27 22:07:05,021 main.py:51] epoch 3740, training loss: 11106.08, average training loss: 11324.14, base loss: 19778.35
[INFO 2017-06-27 22:07:05,997 main.py:51] epoch 3741, training loss: 9910.11, average training loss: 11322.79, base loss: 19776.49
[INFO 2017-06-27 22:07:07,074 main.py:51] epoch 3742, training loss: 11170.45, average training loss: 11322.15, base loss: 19775.23
[INFO 2017-06-27 22:07:08,111 main.py:51] epoch 3743, training loss: 11926.47, average training loss: 11321.99, base loss: 19774.73
[INFO 2017-06-27 22:07:09,110 main.py:51] epoch 3744, training loss: 11299.78, average training loss: 11321.81, base loss: 19776.18
[INFO 2017-06-27 22:07:10,085 main.py:51] epoch 3745, training loss: 9788.99, average training loss: 11320.62, base loss: 19772.46
[INFO 2017-06-27 22:07:11,073 main.py:51] epoch 3746, training loss: 10713.46, average training loss: 11320.25, base loss: 19773.49
[INFO 2017-06-27 22:07:12,047 main.py:51] epoch 3747, training loss: 11904.52, average training loss: 11319.57, base loss: 19774.44
[INFO 2017-06-27 22:07:13,020 main.py:51] epoch 3748, training loss: 10585.34, average training loss: 11317.85, base loss: 19770.71
[INFO 2017-06-27 22:07:13,998 main.py:51] epoch 3749, training loss: 11239.91, average training loss: 11317.64, base loss: 19772.67
[INFO 2017-06-27 22:07:14,973 main.py:51] epoch 3750, training loss: 11691.20, average training loss: 11317.74, base loss: 19773.68
[INFO 2017-06-27 22:07:15,947 main.py:51] epoch 3751, training loss: 12111.58, average training loss: 11318.57, base loss: 19775.89
[INFO 2017-06-27 22:07:16,920 main.py:51] epoch 3752, training loss: 10473.98, average training loss: 11316.19, base loss: 19769.76
[INFO 2017-06-27 22:07:17,893 main.py:51] epoch 3753, training loss: 11364.45, average training loss: 11316.69, base loss: 19770.63
[INFO 2017-06-27 22:07:18,867 main.py:51] epoch 3754, training loss: 9807.70, average training loss: 11315.60, base loss: 19770.16
[INFO 2017-06-27 22:07:19,842 main.py:51] epoch 3755, training loss: 11110.81, average training loss: 11316.54, base loss: 19773.79
[INFO 2017-06-27 22:07:20,816 main.py:51] epoch 3756, training loss: 12141.80, average training loss: 11318.09, base loss: 19777.42
[INFO 2017-06-27 22:07:21,790 main.py:51] epoch 3757, training loss: 11514.16, average training loss: 11317.98, base loss: 19777.89
[INFO 2017-06-27 22:07:22,765 main.py:51] epoch 3758, training loss: 11399.52, average training loss: 11317.23, base loss: 19778.30
[INFO 2017-06-27 22:07:23,744 main.py:51] epoch 3759, training loss: 10815.11, average training loss: 11317.79, base loss: 19779.06
[INFO 2017-06-27 22:07:24,722 main.py:51] epoch 3760, training loss: 11619.08, average training loss: 11317.98, base loss: 19779.88
[INFO 2017-06-27 22:07:25,797 main.py:51] epoch 3761, training loss: 12716.71, average training loss: 11320.20, base loss: 19785.77
[INFO 2017-06-27 22:07:26,944 main.py:51] epoch 3762, training loss: 11815.02, average training loss: 11319.07, base loss: 19783.99
[INFO 2017-06-27 22:07:27,995 main.py:51] epoch 3763, training loss: 10931.62, average training loss: 11317.88, base loss: 19779.94
[INFO 2017-06-27 22:07:28,987 main.py:51] epoch 3764, training loss: 9835.95, average training loss: 11316.41, base loss: 19778.91
[INFO 2017-06-27 22:07:30,109 main.py:51] epoch 3765, training loss: 11319.29, average training loss: 11317.67, base loss: 19781.25
[INFO 2017-06-27 22:07:31,122 main.py:51] epoch 3766, training loss: 10968.45, average training loss: 11318.06, base loss: 19781.62
[INFO 2017-06-27 22:07:32,198 main.py:51] epoch 3767, training loss: 10848.21, average training loss: 11318.35, base loss: 19782.92
[INFO 2017-06-27 22:07:33,229 main.py:51] epoch 3768, training loss: 11091.77, average training loss: 11317.11, base loss: 19779.42
[INFO 2017-06-27 22:07:34,314 main.py:51] epoch 3769, training loss: 10695.25, average training loss: 11316.81, base loss: 19781.15
[INFO 2017-06-27 22:07:35,337 main.py:51] epoch 3770, training loss: 11729.25, average training loss: 11315.90, base loss: 19782.04
[INFO 2017-06-27 22:07:36,406 main.py:51] epoch 3771, training loss: 11218.51, average training loss: 11316.30, base loss: 19784.15
[INFO 2017-06-27 22:07:37,430 main.py:51] epoch 3772, training loss: 10910.65, average training loss: 11316.40, base loss: 19784.26
[INFO 2017-06-27 22:07:38,504 main.py:51] epoch 3773, training loss: 10991.01, average training loss: 11317.80, base loss: 19787.92
[INFO 2017-06-27 22:07:39,548 main.py:51] epoch 3774, training loss: 10452.46, average training loss: 11316.98, base loss: 19786.24
[INFO 2017-06-27 22:07:40,542 main.py:51] epoch 3775, training loss: 11283.11, average training loss: 11317.27, base loss: 19787.37
[INFO 2017-06-27 22:07:41,645 main.py:51] epoch 3776, training loss: 11959.99, average training loss: 11317.29, base loss: 19787.56
[INFO 2017-06-27 22:07:42,621 main.py:51] epoch 3777, training loss: 12112.53, average training loss: 11319.07, base loss: 19791.44
[INFO 2017-06-27 22:07:43,592 main.py:51] epoch 3778, training loss: 10853.46, average training loss: 11319.04, base loss: 19792.93
[INFO 2017-06-27 22:07:44,562 main.py:51] epoch 3779, training loss: 11109.03, average training loss: 11318.39, base loss: 19794.27
[INFO 2017-06-27 22:07:45,534 main.py:51] epoch 3780, training loss: 10572.28, average training loss: 11316.56, base loss: 19790.86
[INFO 2017-06-27 22:07:46,510 main.py:51] epoch 3781, training loss: 12290.94, average training loss: 11317.77, base loss: 19793.75
[INFO 2017-06-27 22:07:47,479 main.py:51] epoch 3782, training loss: 11013.85, average training loss: 11318.12, base loss: 19796.75
[INFO 2017-06-27 22:07:48,449 main.py:51] epoch 3783, training loss: 10367.24, average training loss: 11318.63, base loss: 19798.90
[INFO 2017-06-27 22:07:49,423 main.py:51] epoch 3784, training loss: 9652.11, average training loss: 11317.69, base loss: 19797.43
[INFO 2017-06-27 22:07:50,395 main.py:51] epoch 3785, training loss: 11792.40, average training loss: 11317.69, base loss: 19797.74
[INFO 2017-06-27 22:07:51,366 main.py:51] epoch 3786, training loss: 11577.42, average training loss: 11316.95, base loss: 19796.21
[INFO 2017-06-27 22:07:52,338 main.py:51] epoch 3787, training loss: 9992.18, average training loss: 11316.66, base loss: 19795.15
[INFO 2017-06-27 22:07:53,310 main.py:51] epoch 3788, training loss: 11216.00, average training loss: 11316.43, base loss: 19794.50
[INFO 2017-06-27 22:07:54,282 main.py:51] epoch 3789, training loss: 11198.64, average training loss: 11316.14, base loss: 19793.36
[INFO 2017-06-27 22:07:55,359 main.py:51] epoch 3790, training loss: 9749.70, average training loss: 11313.11, base loss: 19787.77
[INFO 2017-06-27 22:07:56,397 main.py:51] epoch 3791, training loss: 9787.32, average training loss: 11311.71, base loss: 19786.42
[INFO 2017-06-27 22:07:57,392 main.py:51] epoch 3792, training loss: 11062.96, average training loss: 11312.19, base loss: 19787.91
[INFO 2017-06-27 22:07:58,481 main.py:51] epoch 3793, training loss: 10660.74, average training loss: 11312.28, base loss: 19787.95
[INFO 2017-06-27 22:07:59,473 main.py:51] epoch 3794, training loss: 10719.17, average training loss: 11311.68, base loss: 19787.70
[INFO 2017-06-27 22:08:00,526 main.py:51] epoch 3795, training loss: 10198.27, average training loss: 11310.69, base loss: 19786.05
[INFO 2017-06-27 22:08:01,545 main.py:51] epoch 3796, training loss: 11381.41, average training loss: 11311.14, base loss: 19785.83
[INFO 2017-06-27 22:08:02,649 main.py:51] epoch 3797, training loss: 11715.63, average training loss: 11309.37, base loss: 19783.54
[INFO 2017-06-27 22:08:03,628 main.py:51] epoch 3798, training loss: 10329.77, average training loss: 11308.95, base loss: 19782.67
[INFO 2017-06-27 22:08:04,599 main.py:51] epoch 3799, training loss: 9742.00, average training loss: 11307.23, base loss: 19779.53
[INFO 2017-06-27 22:08:04,600 main.py:53] epoch 3799, testing
[INFO 2017-06-27 22:08:09,109 main.py:105] average testing loss: 11457.63, base loss: 20689.72
[INFO 2017-06-27 22:08:09,109 main.py:106] improve_loss: 9232.09, improve_percent: 0.45
[INFO 2017-06-27 22:08:09,110 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:08:10,180 main.py:51] epoch 3800, training loss: 10376.48, average training loss: 11307.19, base loss: 19780.86
[INFO 2017-06-27 22:08:11,229 main.py:51] epoch 3801, training loss: 9918.52, average training loss: 11306.09, base loss: 19780.73
[INFO 2017-06-27 22:08:12,315 main.py:51] epoch 3802, training loss: 10507.17, average training loss: 11305.22, base loss: 19781.46
[INFO 2017-06-27 22:08:13,309 main.py:51] epoch 3803, training loss: 11263.33, average training loss: 11306.31, base loss: 19784.37
[INFO 2017-06-27 22:08:14,335 main.py:51] epoch 3804, training loss: 10056.03, average training loss: 11303.97, base loss: 19781.04
[INFO 2017-06-27 22:08:15,340 main.py:51] epoch 3805, training loss: 10381.36, average training loss: 11301.93, base loss: 19778.42
[INFO 2017-06-27 22:08:16,420 main.py:51] epoch 3806, training loss: 11710.51, average training loss: 11302.47, base loss: 19780.49
[INFO 2017-06-27 22:08:17,500 main.py:51] epoch 3807, training loss: 13285.25, average training loss: 11304.78, base loss: 19788.28
[INFO 2017-06-27 22:08:18,492 main.py:51] epoch 3808, training loss: 11996.41, average training loss: 11305.24, base loss: 19789.93
[INFO 2017-06-27 22:08:19,476 main.py:51] epoch 3809, training loss: 9922.83, average training loss: 11302.34, base loss: 19782.96
[INFO 2017-06-27 22:08:20,452 main.py:51] epoch 3810, training loss: 10656.12, average training loss: 11301.60, base loss: 19782.09
[INFO 2017-06-27 22:08:21,431 main.py:51] epoch 3811, training loss: 10644.36, average training loss: 11301.13, base loss: 19780.96
[INFO 2017-06-27 22:08:22,488 main.py:51] epoch 3812, training loss: 10801.08, average training loss: 11299.51, base loss: 19778.06
[INFO 2017-06-27 22:08:23,661 main.py:51] epoch 3813, training loss: 9656.40, average training loss: 11296.97, base loss: 19772.37
[INFO 2017-06-27 22:08:24,662 main.py:51] epoch 3814, training loss: 12848.83, average training loss: 11297.95, base loss: 19776.02
[INFO 2017-06-27 22:08:25,635 main.py:51] epoch 3815, training loss: 11489.83, average training loss: 11297.66, base loss: 19777.80
[INFO 2017-06-27 22:08:26,738 main.py:51] epoch 3816, training loss: 10857.88, average training loss: 11296.17, base loss: 19776.49
[INFO 2017-06-27 22:08:27,729 main.py:51] epoch 3817, training loss: 10361.94, average training loss: 11296.12, base loss: 19777.20
[INFO 2017-06-27 22:08:28,701 main.py:51] epoch 3818, training loss: 12564.79, average training loss: 11297.10, base loss: 19780.45
[INFO 2017-06-27 22:08:29,777 main.py:51] epoch 3819, training loss: 10423.87, average training loss: 11295.15, base loss: 19775.58
[INFO 2017-06-27 22:08:30,819 main.py:51] epoch 3820, training loss: 10890.06, average training loss: 11293.94, base loss: 19773.19
[INFO 2017-06-27 22:08:31,814 main.py:51] epoch 3821, training loss: 10326.12, average training loss: 11292.43, base loss: 19772.17
[INFO 2017-06-27 22:08:32,896 main.py:51] epoch 3822, training loss: 11618.98, average training loss: 11292.35, base loss: 19773.63
[INFO 2017-06-27 22:08:33,871 main.py:51] epoch 3823, training loss: 10858.58, average training loss: 11289.90, base loss: 19769.98
[INFO 2017-06-27 22:08:34,950 main.py:51] epoch 3824, training loss: 11011.04, average training loss: 11289.50, base loss: 19770.73
[INFO 2017-06-27 22:08:35,987 main.py:51] epoch 3825, training loss: 10692.84, average training loss: 11289.26, base loss: 19770.24
[INFO 2017-06-27 22:08:36,978 main.py:51] epoch 3826, training loss: 13838.17, average training loss: 11291.78, base loss: 19777.24
[INFO 2017-06-27 22:08:38,075 main.py:51] epoch 3827, training loss: 10724.41, average training loss: 11291.97, base loss: 19777.98
[INFO 2017-06-27 22:08:39,136 main.py:51] epoch 3828, training loss: 11921.14, average training loss: 11292.28, base loss: 19780.07
[INFO 2017-06-27 22:08:40,276 main.py:51] epoch 3829, training loss: 10705.05, average training loss: 11292.58, base loss: 19781.03
[INFO 2017-06-27 22:08:41,344 main.py:51] epoch 3830, training loss: 11006.13, average training loss: 11293.33, base loss: 19784.16
[INFO 2017-06-27 22:08:42,352 main.py:51] epoch 3831, training loss: 11452.09, average training loss: 11291.81, base loss: 19782.39
[INFO 2017-06-27 22:08:43,428 main.py:51] epoch 3832, training loss: 12343.04, average training loss: 11293.52, base loss: 19788.32
[INFO 2017-06-27 22:08:44,492 main.py:51] epoch 3833, training loss: 12243.80, average training loss: 11294.24, base loss: 19790.34
[INFO 2017-06-27 22:08:45,615 main.py:51] epoch 3834, training loss: 10892.37, average training loss: 11293.37, base loss: 19791.16
[INFO 2017-06-27 22:08:46,668 main.py:51] epoch 3835, training loss: 11247.94, average training loss: 11292.90, base loss: 19789.65
[INFO 2017-06-27 22:08:47,693 main.py:51] epoch 3836, training loss: 11612.94, average training loss: 11293.12, base loss: 19791.87
[INFO 2017-06-27 22:08:48,670 main.py:51] epoch 3837, training loss: 11775.97, average training loss: 11293.99, base loss: 19794.65
[INFO 2017-06-27 22:08:49,739 main.py:51] epoch 3838, training loss: 10847.56, average training loss: 11295.01, base loss: 19799.37
[INFO 2017-06-27 22:08:50,800 main.py:51] epoch 3839, training loss: 11801.27, average training loss: 11295.71, base loss: 19802.86
[INFO 2017-06-27 22:08:51,827 main.py:51] epoch 3840, training loss: 11797.28, average training loss: 11295.81, base loss: 19804.54
[INFO 2017-06-27 22:08:52,906 main.py:51] epoch 3841, training loss: 12015.60, average training loss: 11296.12, base loss: 19805.24
[INFO 2017-06-27 22:08:54,007 main.py:51] epoch 3842, training loss: 11418.07, average training loss: 11297.11, base loss: 19807.68
[INFO 2017-06-27 22:08:55,025 main.py:51] epoch 3843, training loss: 10288.28, average training loss: 11294.00, base loss: 19802.06
[INFO 2017-06-27 22:08:56,041 main.py:51] epoch 3844, training loss: 11445.91, average training loss: 11295.00, base loss: 19804.93
[INFO 2017-06-27 22:08:57,048 main.py:51] epoch 3845, training loss: 10019.73, average training loss: 11292.87, base loss: 19802.05
[INFO 2017-06-27 22:08:58,070 main.py:51] epoch 3846, training loss: 9643.95, average training loss: 11292.52, base loss: 19803.01
[INFO 2017-06-27 22:08:59,088 main.py:51] epoch 3847, training loss: 11190.45, average training loss: 11292.19, base loss: 19802.68
[INFO 2017-06-27 22:09:00,125 main.py:51] epoch 3848, training loss: 11945.35, average training loss: 11292.84, base loss: 19804.78
[INFO 2017-06-27 22:09:01,129 main.py:51] epoch 3849, training loss: 11060.88, average training loss: 11292.82, base loss: 19805.87
[INFO 2017-06-27 22:09:02,210 main.py:51] epoch 3850, training loss: 11420.74, average training loss: 11292.78, base loss: 19807.14
[INFO 2017-06-27 22:09:03,259 main.py:51] epoch 3851, training loss: 10224.95, average training loss: 11291.83, base loss: 19805.50
[INFO 2017-06-27 22:09:04,258 main.py:51] epoch 3852, training loss: 10855.48, average training loss: 11291.33, base loss: 19807.08
[INFO 2017-06-27 22:09:05,349 main.py:51] epoch 3853, training loss: 11222.39, average training loss: 11290.75, base loss: 19808.07
[INFO 2017-06-27 22:09:06,419 main.py:51] epoch 3854, training loss: 11520.32, average training loss: 11289.94, base loss: 19808.17
[INFO 2017-06-27 22:09:07,498 main.py:51] epoch 3855, training loss: 11183.36, average training loss: 11289.47, base loss: 19809.10
[INFO 2017-06-27 22:09:08,524 main.py:51] epoch 3856, training loss: 11224.50, average training loss: 11289.89, base loss: 19809.67
[INFO 2017-06-27 22:09:09,589 main.py:51] epoch 3857, training loss: 12416.74, average training loss: 11290.95, base loss: 19813.80
[INFO 2017-06-27 22:09:10,734 main.py:51] epoch 3858, training loss: 11385.21, average training loss: 11289.90, base loss: 19811.86
[INFO 2017-06-27 22:09:11,843 main.py:51] epoch 3859, training loss: 11750.22, average training loss: 11288.12, base loss: 19809.22
[INFO 2017-06-27 22:09:12,899 main.py:51] epoch 3860, training loss: 12492.54, average training loss: 11290.12, base loss: 19814.73
[INFO 2017-06-27 22:09:13,911 main.py:51] epoch 3861, training loss: 10757.83, average training loss: 11288.66, base loss: 19810.79
[INFO 2017-06-27 22:09:15,040 main.py:51] epoch 3862, training loss: 11152.52, average training loss: 11287.36, base loss: 19807.63
[INFO 2017-06-27 22:09:16,085 main.py:51] epoch 3863, training loss: 10393.16, average training loss: 11285.07, base loss: 19805.16
[INFO 2017-06-27 22:09:17,099 main.py:51] epoch 3864, training loss: 10591.37, average training loss: 11283.17, base loss: 19803.10
[INFO 2017-06-27 22:09:18,082 main.py:51] epoch 3865, training loss: 11352.46, average training loss: 11283.16, base loss: 19803.71
[INFO 2017-06-27 22:09:19,165 main.py:51] epoch 3866, training loss: 12057.67, average training loss: 11283.86, base loss: 19806.47
[INFO 2017-06-27 22:09:20,218 main.py:51] epoch 3867, training loss: 9928.62, average training loss: 11283.36, base loss: 19806.87
[INFO 2017-06-27 22:09:21,260 main.py:51] epoch 3868, training loss: 10821.53, average training loss: 11282.06, base loss: 19806.61
[INFO 2017-06-27 22:09:22,352 main.py:51] epoch 3869, training loss: 12775.39, average training loss: 11283.14, base loss: 19810.48
[INFO 2017-06-27 22:09:23,422 main.py:51] epoch 3870, training loss: 9875.00, average training loss: 11278.68, base loss: 19801.29
[INFO 2017-06-27 22:09:24,461 main.py:51] epoch 3871, training loss: 11162.63, average training loss: 11277.54, base loss: 19798.39
[INFO 2017-06-27 22:09:25,483 main.py:51] epoch 3872, training loss: 10758.53, average training loss: 11276.63, base loss: 19795.88
[INFO 2017-06-27 22:09:26,562 main.py:51] epoch 3873, training loss: 10685.06, average training loss: 11277.12, base loss: 19797.51
[INFO 2017-06-27 22:09:27,546 main.py:51] epoch 3874, training loss: 10208.95, average training loss: 11276.95, base loss: 19798.71
[INFO 2017-06-27 22:09:28,522 main.py:51] epoch 3875, training loss: 11167.29, average training loss: 11277.07, base loss: 19801.56
[INFO 2017-06-27 22:09:29,539 main.py:51] epoch 3876, training loss: 10061.44, average training loss: 11276.36, base loss: 19802.33
[INFO 2017-06-27 22:09:30,555 main.py:51] epoch 3877, training loss: 10288.91, average training loss: 11274.21, base loss: 19797.71
[INFO 2017-06-27 22:09:31,649 main.py:51] epoch 3878, training loss: 10604.05, average training loss: 11271.81, base loss: 19793.50
[INFO 2017-06-27 22:09:32,699 main.py:51] epoch 3879, training loss: 11157.80, average training loss: 11272.17, base loss: 19797.04
[INFO 2017-06-27 22:09:33,692 main.py:51] epoch 3880, training loss: 10184.35, average training loss: 11270.76, base loss: 19794.15
[INFO 2017-06-27 22:09:34,767 main.py:51] epoch 3881, training loss: 11023.31, average training loss: 11271.14, base loss: 19794.46
[INFO 2017-06-27 22:09:35,864 main.py:51] epoch 3882, training loss: 11004.41, average training loss: 11271.43, base loss: 19795.44
[INFO 2017-06-27 22:09:36,907 main.py:51] epoch 3883, training loss: 10452.47, average training loss: 11271.46, base loss: 19796.74
[INFO 2017-06-27 22:09:37,910 main.py:51] epoch 3884, training loss: 10205.21, average training loss: 11269.20, base loss: 19791.93
[INFO 2017-06-27 22:09:38,973 main.py:51] epoch 3885, training loss: 10573.88, average training loss: 11268.68, base loss: 19792.36
[INFO 2017-06-27 22:09:39,979 main.py:51] epoch 3886, training loss: 10135.31, average training loss: 11267.81, base loss: 19789.67
[INFO 2017-06-27 22:09:40,954 main.py:51] epoch 3887, training loss: 11810.05, average training loss: 11267.83, base loss: 19791.06
[INFO 2017-06-27 22:09:41,928 main.py:51] epoch 3888, training loss: 11308.75, average training loss: 11268.07, base loss: 19791.88
[INFO 2017-06-27 22:09:42,899 main.py:51] epoch 3889, training loss: 11057.71, average training loss: 11268.42, base loss: 19793.12
[INFO 2017-06-27 22:09:43,881 main.py:51] epoch 3890, training loss: 12172.58, average training loss: 11270.50, base loss: 19799.23
[INFO 2017-06-27 22:09:44,854 main.py:51] epoch 3891, training loss: 12013.79, average training loss: 11270.82, base loss: 19800.83
[INFO 2017-06-27 22:09:45,825 main.py:51] epoch 3892, training loss: 11481.28, average training loss: 11270.76, base loss: 19800.34
[INFO 2017-06-27 22:09:46,796 main.py:51] epoch 3893, training loss: 11539.68, average training loss: 11269.93, base loss: 19800.20
[INFO 2017-06-27 22:09:47,771 main.py:51] epoch 3894, training loss: 11315.58, average training loss: 11270.57, base loss: 19801.53
[INFO 2017-06-27 22:09:48,746 main.py:51] epoch 3895, training loss: 10093.52, average training loss: 11268.94, base loss: 19798.82
[INFO 2017-06-27 22:09:49,716 main.py:51] epoch 3896, training loss: 12159.02, average training loss: 11268.93, base loss: 19801.31
[INFO 2017-06-27 22:09:50,691 main.py:51] epoch 3897, training loss: 9834.35, average training loss: 11267.87, base loss: 19799.13
[INFO 2017-06-27 22:09:51,662 main.py:51] epoch 3898, training loss: 10799.73, average training loss: 11267.48, base loss: 19797.61
[INFO 2017-06-27 22:09:52,633 main.py:51] epoch 3899, training loss: 11595.99, average training loss: 11267.39, base loss: 19799.27
[INFO 2017-06-27 22:09:52,633 main.py:53] epoch 3899, testing
[INFO 2017-06-27 22:09:57,089 main.py:105] average testing loss: 11179.78, base loss: 19948.48
[INFO 2017-06-27 22:09:57,089 main.py:106] improve_loss: 8768.69, improve_percent: 0.44
[INFO 2017-06-27 22:09:57,089 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:09:58,062 main.py:51] epoch 3900, training loss: 12641.22, average training loss: 11267.43, base loss: 19800.64
[INFO 2017-06-27 22:09:59,087 main.py:51] epoch 3901, training loss: 11515.42, average training loss: 11266.34, base loss: 19800.97
[INFO 2017-06-27 22:10:00,082 main.py:51] epoch 3902, training loss: 13162.31, average training loss: 11268.27, base loss: 19806.55
[INFO 2017-06-27 22:10:01,062 main.py:51] epoch 3903, training loss: 11209.96, average training loss: 11269.28, base loss: 19809.27
[INFO 2017-06-27 22:10:02,043 main.py:51] epoch 3904, training loss: 11331.48, average training loss: 11268.39, base loss: 19807.03
[INFO 2017-06-27 22:10:03,016 main.py:51] epoch 3905, training loss: 11994.38, average training loss: 11269.48, base loss: 19808.01
[INFO 2017-06-27 22:10:03,988 main.py:51] epoch 3906, training loss: 11397.32, average training loss: 11267.96, base loss: 19803.96
[INFO 2017-06-27 22:10:04,964 main.py:51] epoch 3907, training loss: 11602.04, average training loss: 11267.99, base loss: 19803.43
[INFO 2017-06-27 22:10:05,940 main.py:51] epoch 3908, training loss: 11064.34, average training loss: 11267.49, base loss: 19801.32
[INFO 2017-06-27 22:10:06,913 main.py:51] epoch 3909, training loss: 9729.60, average training loss: 11266.05, base loss: 19798.42
[INFO 2017-06-27 22:10:08,011 main.py:51] epoch 3910, training loss: 10702.97, average training loss: 11265.04, base loss: 19796.09
[INFO 2017-06-27 22:10:09,019 main.py:51] epoch 3911, training loss: 10212.90, average training loss: 11263.73, base loss: 19792.90
[INFO 2017-06-27 22:10:09,992 main.py:51] epoch 3912, training loss: 11359.09, average training loss: 11263.72, base loss: 19793.28
[INFO 2017-06-27 22:10:10,973 main.py:51] epoch 3913, training loss: 11013.19, average training loss: 11264.32, base loss: 19794.93
[INFO 2017-06-27 22:10:11,946 main.py:51] epoch 3914, training loss: 11586.80, average training loss: 11265.71, base loss: 19798.35
[INFO 2017-06-27 22:10:12,920 main.py:51] epoch 3915, training loss: 10836.42, average training loss: 11263.65, base loss: 19793.57
[INFO 2017-06-27 22:10:14,000 main.py:51] epoch 3916, training loss: 10465.86, average training loss: 11263.74, base loss: 19793.67
[INFO 2017-06-27 22:10:15,048 main.py:51] epoch 3917, training loss: 10515.51, average training loss: 11264.69, base loss: 19796.33
[INFO 2017-06-27 22:10:16,048 main.py:51] epoch 3918, training loss: 12529.97, average training loss: 11265.88, base loss: 19799.68
[INFO 2017-06-27 22:10:17,119 main.py:51] epoch 3919, training loss: 10791.19, average training loss: 11266.95, base loss: 19802.15
[INFO 2017-06-27 22:10:18,125 main.py:51] epoch 3920, training loss: 10321.70, average training loss: 11264.52, base loss: 19797.08
[INFO 2017-06-27 22:10:19,101 main.py:51] epoch 3921, training loss: 12886.07, average training loss: 11265.82, base loss: 19800.65
[INFO 2017-06-27 22:10:20,079 main.py:51] epoch 3922, training loss: 10317.62, average training loss: 11265.84, base loss: 19801.38
[INFO 2017-06-27 22:10:21,053 main.py:51] epoch 3923, training loss: 10243.03, average training loss: 11264.48, base loss: 19797.07
[INFO 2017-06-27 22:10:22,028 main.py:51] epoch 3924, training loss: 10453.55, average training loss: 11263.80, base loss: 19796.35
[INFO 2017-06-27 22:10:23,007 main.py:51] epoch 3925, training loss: 9438.48, average training loss: 11261.23, base loss: 19792.15
[INFO 2017-06-27 22:10:23,992 main.py:51] epoch 3926, training loss: 11502.17, average training loss: 11261.18, base loss: 19793.42
[INFO 2017-06-27 22:10:25,078 main.py:51] epoch 3927, training loss: 10346.02, average training loss: 11260.01, base loss: 19791.59
[INFO 2017-06-27 22:10:26,215 main.py:51] epoch 3928, training loss: 12267.88, average training loss: 11261.50, base loss: 19795.63
[INFO 2017-06-27 22:10:27,284 main.py:51] epoch 3929, training loss: 9158.01, average training loss: 11259.68, base loss: 19792.42
[INFO 2017-06-27 22:10:28,449 main.py:51] epoch 3930, training loss: 11310.71, average training loss: 11258.44, base loss: 19791.39
[INFO 2017-06-27 22:10:29,443 main.py:51] epoch 3931, training loss: 12125.52, average training loss: 11260.13, base loss: 19797.65
[INFO 2017-06-27 22:10:30,543 main.py:51] epoch 3932, training loss: 13021.15, average training loss: 11262.47, base loss: 19803.98
[INFO 2017-06-27 22:10:31,527 main.py:51] epoch 3933, training loss: 11009.01, average training loss: 11261.77, base loss: 19803.98
[INFO 2017-06-27 22:10:32,616 main.py:51] epoch 3934, training loss: 11885.96, average training loss: 11262.05, base loss: 19805.10
[INFO 2017-06-27 22:10:33,710 main.py:51] epoch 3935, training loss: 12223.50, average training loss: 11261.49, base loss: 19803.36
[INFO 2017-06-27 22:10:34,719 main.py:51] epoch 3936, training loss: 11472.01, average training loss: 11263.33, base loss: 19808.31
[INFO 2017-06-27 22:10:35,721 main.py:51] epoch 3937, training loss: 13151.08, average training loss: 11266.65, base loss: 19815.10
[INFO 2017-06-27 22:10:36,695 main.py:51] epoch 3938, training loss: 11534.56, average training loss: 11265.56, base loss: 19813.44
[INFO 2017-06-27 22:10:37,666 main.py:51] epoch 3939, training loss: 12152.39, average training loss: 11266.12, base loss: 19815.65
[INFO 2017-06-27 22:10:38,639 main.py:51] epoch 3940, training loss: 11358.87, average training loss: 11265.09, base loss: 19813.78
[INFO 2017-06-27 22:10:39,613 main.py:51] epoch 3941, training loss: 9900.36, average training loss: 11263.18, base loss: 19808.16
[INFO 2017-06-27 22:10:40,584 main.py:51] epoch 3942, training loss: 10745.29, average training loss: 11262.50, base loss: 19806.90
[INFO 2017-06-27 22:10:41,558 main.py:51] epoch 3943, training loss: 11422.87, average training loss: 11261.60, base loss: 19805.86
[INFO 2017-06-27 22:10:42,533 main.py:51] epoch 3944, training loss: 11985.79, average training loss: 11262.02, base loss: 19808.33
[INFO 2017-06-27 22:10:43,504 main.py:51] epoch 3945, training loss: 11394.37, average training loss: 11261.41, base loss: 19808.36
[INFO 2017-06-27 22:10:44,476 main.py:51] epoch 3946, training loss: 11491.26, average training loss: 11262.08, base loss: 19809.66
[INFO 2017-06-27 22:10:45,451 main.py:51] epoch 3947, training loss: 11576.28, average training loss: 11260.72, base loss: 19805.99
[INFO 2017-06-27 22:10:46,422 main.py:51] epoch 3948, training loss: 11903.63, average training loss: 11261.74, base loss: 19809.33
[INFO 2017-06-27 22:10:47,394 main.py:51] epoch 3949, training loss: 10497.87, average training loss: 11260.14, base loss: 19806.25
[INFO 2017-06-27 22:10:48,368 main.py:51] epoch 3950, training loss: 10276.61, average training loss: 11258.42, base loss: 19802.98
[INFO 2017-06-27 22:10:49,342 main.py:51] epoch 3951, training loss: 11265.61, average training loss: 11259.18, base loss: 19805.48
[INFO 2017-06-27 22:10:50,314 main.py:51] epoch 3952, training loss: 12201.80, average training loss: 11261.21, base loss: 19810.45
[INFO 2017-06-27 22:10:51,290 main.py:51] epoch 3953, training loss: 10155.16, average training loss: 11258.85, base loss: 19805.06
[INFO 2017-06-27 22:10:52,374 main.py:51] epoch 3954, training loss: 11357.39, average training loss: 11258.66, base loss: 19806.32
[INFO 2017-06-27 22:10:53,349 main.py:51] epoch 3955, training loss: 11521.38, average training loss: 11256.97, base loss: 19804.15
[INFO 2017-06-27 22:10:54,323 main.py:51] epoch 3956, training loss: 11245.09, average training loss: 11256.31, base loss: 19804.00
[INFO 2017-06-27 22:10:55,300 main.py:51] epoch 3957, training loss: 9994.28, average training loss: 11254.95, base loss: 19800.83
[INFO 2017-06-27 22:10:56,276 main.py:51] epoch 3958, training loss: 10724.83, average training loss: 11254.36, base loss: 19800.85
[INFO 2017-06-27 22:10:57,246 main.py:51] epoch 3959, training loss: 10424.53, average training loss: 11251.84, base loss: 19795.70
[INFO 2017-06-27 22:10:58,220 main.py:51] epoch 3960, training loss: 11762.34, average training loss: 11252.77, base loss: 19797.04
[INFO 2017-06-27 22:10:59,192 main.py:51] epoch 3961, training loss: 10456.21, average training loss: 11251.59, base loss: 19795.69
[INFO 2017-06-27 22:11:00,173 main.py:51] epoch 3962, training loss: 12369.01, average training loss: 11253.47, base loss: 19801.23
[INFO 2017-06-27 22:11:01,243 main.py:51] epoch 3963, training loss: 10761.61, average training loss: 11251.38, base loss: 19798.26
[INFO 2017-06-27 22:11:02,445 main.py:51] epoch 3964, training loss: 11357.72, average training loss: 11251.96, base loss: 19801.28
[INFO 2017-06-27 22:11:03,485 main.py:51] epoch 3965, training loss: 10879.03, average training loss: 11250.05, base loss: 19797.66
[INFO 2017-06-27 22:11:04,482 main.py:51] epoch 3966, training loss: 10362.29, average training loss: 11249.48, base loss: 19797.47
[INFO 2017-06-27 22:11:05,579 main.py:51] epoch 3967, training loss: 10403.44, average training loss: 11247.59, base loss: 19793.31
[INFO 2017-06-27 22:11:06,617 main.py:51] epoch 3968, training loss: 11506.54, average training loss: 11248.14, base loss: 19794.77
[INFO 2017-06-27 22:11:07,618 main.py:51] epoch 3969, training loss: 9920.78, average training loss: 11246.74, base loss: 19791.69
[INFO 2017-06-27 22:11:08,596 main.py:51] epoch 3970, training loss: 11446.25, average training loss: 11246.12, base loss: 19791.26
[INFO 2017-06-27 22:11:09,572 main.py:51] epoch 3971, training loss: 10323.86, average training loss: 11244.86, base loss: 19788.83
[INFO 2017-06-27 22:11:10,549 main.py:51] epoch 3972, training loss: 11299.22, average training loss: 11244.80, base loss: 19790.22
[INFO 2017-06-27 22:11:11,641 main.py:51] epoch 3973, training loss: 9686.36, average training loss: 11243.59, base loss: 19788.74
[INFO 2017-06-27 22:11:12,635 main.py:51] epoch 3974, training loss: 10071.01, average training loss: 11242.90, base loss: 19788.75
[INFO 2017-06-27 22:11:13,727 main.py:51] epoch 3975, training loss: 11438.08, average training loss: 11243.90, base loss: 19791.15
[INFO 2017-06-27 22:11:14,785 main.py:51] epoch 3976, training loss: 10206.83, average training loss: 11240.74, base loss: 19785.46
[INFO 2017-06-27 22:11:15,810 main.py:51] epoch 3977, training loss: 11246.50, average training loss: 11241.36, base loss: 19786.59
[INFO 2017-06-27 22:11:16,883 main.py:51] epoch 3978, training loss: 10728.17, average training loss: 11239.57, base loss: 19784.17
[INFO 2017-06-27 22:11:17,883 main.py:51] epoch 3979, training loss: 10410.93, average training loss: 11239.00, base loss: 19783.00
[INFO 2017-06-27 22:11:18,869 main.py:51] epoch 3980, training loss: 12041.87, average training loss: 11239.92, base loss: 19786.98
[INFO 2017-06-27 22:11:19,843 main.py:51] epoch 3981, training loss: 10433.07, average training loss: 11238.76, base loss: 19785.81
[INFO 2017-06-27 22:11:20,814 main.py:51] epoch 3982, training loss: 10752.38, average training loss: 11238.31, base loss: 19786.28
[INFO 2017-06-27 22:11:21,785 main.py:51] epoch 3983, training loss: 9876.00, average training loss: 11235.77, base loss: 19781.29
[INFO 2017-06-27 22:11:22,760 main.py:51] epoch 3984, training loss: 10717.37, average training loss: 11233.00, base loss: 19775.60
[INFO 2017-06-27 22:11:23,733 main.py:51] epoch 3985, training loss: 10132.28, average training loss: 11231.14, base loss: 19772.53
[INFO 2017-06-27 22:11:24,714 main.py:51] epoch 3986, training loss: 11203.02, average training loss: 11229.87, base loss: 19771.62
[INFO 2017-06-27 22:11:25,797 main.py:51] epoch 3987, training loss: 11034.46, average training loss: 11227.26, base loss: 19767.90
[INFO 2017-06-27 22:11:26,838 main.py:51] epoch 3988, training loss: 10795.13, average training loss: 11226.14, base loss: 19764.69
[INFO 2017-06-27 22:11:27,836 main.py:51] epoch 3989, training loss: 10381.16, average training loss: 11226.12, base loss: 19764.85
[INFO 2017-06-27 22:11:28,897 main.py:51] epoch 3990, training loss: 10966.15, average training loss: 11224.20, base loss: 19763.24
[INFO 2017-06-27 22:11:29,921 main.py:51] epoch 3991, training loss: 9920.70, average training loss: 11223.02, base loss: 19762.05
[INFO 2017-06-27 22:11:30,921 main.py:51] epoch 3992, training loss: 10864.03, average training loss: 11222.05, base loss: 19759.96
[INFO 2017-06-27 22:11:32,002 main.py:51] epoch 3993, training loss: 12491.64, average training loss: 11221.77, base loss: 19760.48
[INFO 2017-06-27 22:11:32,985 main.py:51] epoch 3994, training loss: 11009.27, average training loss: 11223.08, base loss: 19767.80
[INFO 2017-06-27 22:11:34,067 main.py:51] epoch 3995, training loss: 11177.08, average training loss: 11222.14, base loss: 19767.49
[INFO 2017-06-27 22:11:35,059 main.py:51] epoch 3996, training loss: 9932.03, average training loss: 11220.88, base loss: 19765.01
[INFO 2017-06-27 22:11:36,034 main.py:51] epoch 3997, training loss: 9838.43, average training loss: 11218.09, base loss: 19759.24
[INFO 2017-06-27 22:11:37,054 main.py:51] epoch 3998, training loss: 10382.75, average training loss: 11218.65, base loss: 19760.93
[INFO 2017-06-27 22:11:38,052 main.py:51] epoch 3999, training loss: 10790.26, average training loss: 11218.79, base loss: 19761.61
[INFO 2017-06-27 22:11:38,052 main.py:53] epoch 3999, testing
[INFO 2017-06-27 22:11:42,398 main.py:105] average testing loss: 11030.33, base loss: 19904.01
[INFO 2017-06-27 22:11:42,398 main.py:106] improve_loss: 8873.69, improve_percent: 0.45
[INFO 2017-06-27 22:11:42,400 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:11:43,396 main.py:51] epoch 4000, training loss: 10678.22, average training loss: 11217.69, base loss: 19759.24
[INFO 2017-06-27 22:11:44,368 main.py:51] epoch 4001, training loss: 11477.80, average training loss: 11216.75, base loss: 19755.84
[INFO 2017-06-27 22:11:45,338 main.py:51] epoch 4002, training loss: 11103.71, average training loss: 11216.29, base loss: 19756.06
[INFO 2017-06-27 22:11:46,310 main.py:51] epoch 4003, training loss: 11251.86, average training loss: 11216.12, base loss: 19756.58
[INFO 2017-06-27 22:11:47,370 main.py:51] epoch 4004, training loss: 11348.34, average training loss: 11216.21, base loss: 19756.96
[INFO 2017-06-27 22:11:48,408 main.py:51] epoch 4005, training loss: 9692.32, average training loss: 11213.48, base loss: 19752.73
[INFO 2017-06-27 22:11:49,495 main.py:51] epoch 4006, training loss: 13049.59, average training loss: 11216.64, base loss: 19761.77
[INFO 2017-06-27 22:11:50,474 main.py:51] epoch 4007, training loss: 12152.28, average training loss: 11217.38, base loss: 19765.45
[INFO 2017-06-27 22:11:51,444 main.py:51] epoch 4008, training loss: 10260.28, average training loss: 11216.15, base loss: 19762.69
[INFO 2017-06-27 22:11:52,415 main.py:51] epoch 4009, training loss: 9805.69, average training loss: 11212.66, base loss: 19754.77
[INFO 2017-06-27 22:11:53,386 main.py:51] epoch 4010, training loss: 10902.97, average training loss: 11211.54, base loss: 19753.02
[INFO 2017-06-27 22:11:54,366 main.py:51] epoch 4011, training loss: 11140.27, average training loss: 11211.45, base loss: 19753.92
[INFO 2017-06-27 22:11:55,338 main.py:51] epoch 4012, training loss: 10357.53, average training loss: 11208.90, base loss: 19749.22
[INFO 2017-06-27 22:11:56,312 main.py:51] epoch 4013, training loss: 10545.67, average training loss: 11209.76, base loss: 19752.08
[INFO 2017-06-27 22:11:57,283 main.py:51] epoch 4014, training loss: 11362.54, average training loss: 11209.18, base loss: 19752.34
[INFO 2017-06-27 22:11:58,256 main.py:51] epoch 4015, training loss: 11027.58, average training loss: 11209.00, base loss: 19752.67
[INFO 2017-06-27 22:11:59,359 main.py:51] epoch 4016, training loss: 11486.38, average training loss: 11207.56, base loss: 19750.19
[INFO 2017-06-27 22:12:00,363 main.py:51] epoch 4017, training loss: 9870.56, average training loss: 11205.79, base loss: 19746.12
[INFO 2017-06-27 22:12:01,338 main.py:51] epoch 4018, training loss: 12267.38, average training loss: 11205.20, base loss: 19745.44
[INFO 2017-06-27 22:12:02,418 main.py:51] epoch 4019, training loss: 11258.97, average training loss: 11205.38, base loss: 19746.20
[INFO 2017-06-27 22:12:03,523 main.py:51] epoch 4020, training loss: 11526.83, average training loss: 11205.24, base loss: 19745.83
[INFO 2017-06-27 22:12:04,587 main.py:51] epoch 4021, training loss: 11896.06, average training loss: 11204.40, base loss: 19746.20
[INFO 2017-06-27 22:12:05,579 main.py:51] epoch 4022, training loss: 11151.56, average training loss: 11203.82, base loss: 19745.60
[INFO 2017-06-27 22:12:06,704 main.py:51] epoch 4023, training loss: 10510.92, average training loss: 11202.43, base loss: 19742.92
[INFO 2017-06-27 22:12:07,787 main.py:51] epoch 4024, training loss: 10306.44, average training loss: 11201.21, base loss: 19740.42
[INFO 2017-06-27 22:12:08,851 main.py:51] epoch 4025, training loss: 11043.45, average training loss: 11201.13, base loss: 19741.69
[INFO 2017-06-27 22:12:09,845 main.py:51] epoch 4026, training loss: 10600.12, average training loss: 11201.10, base loss: 19742.24
[INFO 2017-06-27 22:12:10,931 main.py:51] epoch 4027, training loss: 9298.91, average training loss: 11200.11, base loss: 19739.05
[INFO 2017-06-27 22:12:12,023 main.py:51] epoch 4028, training loss: 10579.86, average training loss: 11198.75, base loss: 19736.57
[INFO 2017-06-27 22:12:13,118 main.py:51] epoch 4029, training loss: 11200.17, average training loss: 11198.44, base loss: 19736.57
[INFO 2017-06-27 22:12:14,110 main.py:51] epoch 4030, training loss: 10571.36, average training loss: 11197.71, base loss: 19736.32
[INFO 2017-06-27 22:12:15,170 main.py:51] epoch 4031, training loss: 10694.63, average training loss: 11196.74, base loss: 19733.47
[INFO 2017-06-27 22:12:16,161 main.py:51] epoch 4032, training loss: 11542.38, average training loss: 11197.64, base loss: 19735.66
[INFO 2017-06-27 22:12:17,137 main.py:51] epoch 4033, training loss: 10624.94, average training loss: 11197.15, base loss: 19734.78
[INFO 2017-06-27 22:12:18,111 main.py:51] epoch 4034, training loss: 9824.62, average training loss: 11195.94, base loss: 19731.84
[INFO 2017-06-27 22:12:19,081 main.py:51] epoch 4035, training loss: 11984.58, average training loss: 11196.33, base loss: 19732.87
[INFO 2017-06-27 22:12:20,052 main.py:51] epoch 4036, training loss: 11292.41, average training loss: 11196.89, base loss: 19734.08
[INFO 2017-06-27 22:12:21,028 main.py:51] epoch 4037, training loss: 11684.94, average training loss: 11197.27, base loss: 19735.68
[INFO 2017-06-27 22:12:22,093 main.py:51] epoch 4038, training loss: 11489.61, average training loss: 11196.75, base loss: 19734.50
[INFO 2017-06-27 22:12:23,112 main.py:51] epoch 4039, training loss: 11899.37, average training loss: 11198.27, base loss: 19738.95
[INFO 2017-06-27 22:12:24,143 main.py:51] epoch 4040, training loss: 12534.06, average training loss: 11199.73, base loss: 19744.29
[INFO 2017-06-27 22:12:25,121 main.py:51] epoch 4041, training loss: 9554.28, average training loss: 11199.28, base loss: 19743.04
[INFO 2017-06-27 22:12:26,114 main.py:51] epoch 4042, training loss: 10099.23, average training loss: 11196.50, base loss: 19737.89
[INFO 2017-06-27 22:12:27,092 main.py:51] epoch 4043, training loss: 10302.27, average training loss: 11195.90, base loss: 19736.91
[INFO 2017-06-27 22:12:28,066 main.py:51] epoch 4044, training loss: 9711.49, average training loss: 11192.86, base loss: 19731.83
[INFO 2017-06-27 22:12:29,139 main.py:51] epoch 4045, training loss: 10836.36, average training loss: 11191.71, base loss: 19729.83
[INFO 2017-06-27 22:12:30,169 main.py:51] epoch 4046, training loss: 10673.57, average training loss: 11191.66, base loss: 19729.11
[INFO 2017-06-27 22:12:31,275 main.py:51] epoch 4047, training loss: 9644.20, average training loss: 11189.87, base loss: 19726.83
[INFO 2017-06-27 22:12:32,284 main.py:51] epoch 4048, training loss: 12039.00, average training loss: 11189.97, base loss: 19727.12
[INFO 2017-06-27 22:12:33,365 main.py:51] epoch 4049, training loss: 9884.59, average training loss: 11187.86, base loss: 19722.38
[INFO 2017-06-27 22:12:34,449 main.py:51] epoch 4050, training loss: 10137.31, average training loss: 11186.76, base loss: 19720.03
[INFO 2017-06-27 22:12:35,465 main.py:51] epoch 4051, training loss: 10818.71, average training loss: 11186.21, base loss: 19718.20
[INFO 2017-06-27 22:12:36,535 main.py:51] epoch 4052, training loss: 11420.58, average training loss: 11185.50, base loss: 19717.16
[INFO 2017-06-27 22:12:37,533 main.py:51] epoch 4053, training loss: 10583.18, average training loss: 11184.70, base loss: 19718.09
[INFO 2017-06-27 22:12:38,523 main.py:51] epoch 4054, training loss: 11305.20, average training loss: 11182.53, base loss: 19714.71
[INFO 2017-06-27 22:12:39,520 main.py:51] epoch 4055, training loss: 10540.38, average training loss: 11182.48, base loss: 19715.46
[INFO 2017-06-27 22:12:40,494 main.py:51] epoch 4056, training loss: 11817.80, average training loss: 11182.84, base loss: 19717.78
[INFO 2017-06-27 22:12:41,600 main.py:51] epoch 4057, training loss: 10607.44, average training loss: 11181.12, base loss: 19714.60
[INFO 2017-06-27 22:12:42,600 main.py:51] epoch 4058, training loss: 10349.61, average training loss: 11180.14, base loss: 19714.26
[INFO 2017-06-27 22:12:43,711 main.py:51] epoch 4059, training loss: 10155.91, average training loss: 11177.66, base loss: 19709.43
[INFO 2017-06-27 22:12:44,689 main.py:51] epoch 4060, training loss: 8919.19, average training loss: 11174.99, base loss: 19702.87
[INFO 2017-06-27 22:12:45,665 main.py:51] epoch 4061, training loss: 12085.31, average training loss: 11175.90, base loss: 19704.74
[INFO 2017-06-27 22:12:46,640 main.py:51] epoch 4062, training loss: 10713.86, average training loss: 11174.81, base loss: 19702.34
[INFO 2017-06-27 22:12:47,613 main.py:51] epoch 4063, training loss: 11812.60, average training loss: 11176.09, base loss: 19706.94
[INFO 2017-06-27 22:12:48,585 main.py:51] epoch 4064, training loss: 11693.75, average training loss: 11176.65, base loss: 19708.44
[INFO 2017-06-27 22:12:49,558 main.py:51] epoch 4065, training loss: 11070.09, average training loss: 11177.00, base loss: 19710.29
[INFO 2017-06-27 22:12:50,533 main.py:51] epoch 4066, training loss: 10821.36, average training loss: 11177.65, base loss: 19711.13
[INFO 2017-06-27 22:12:51,505 main.py:51] epoch 4067, training loss: 10392.53, average training loss: 11176.21, base loss: 19709.47
[INFO 2017-06-27 22:12:52,479 main.py:51] epoch 4068, training loss: 10111.82, average training loss: 11172.88, base loss: 19703.13
[INFO 2017-06-27 22:12:53,451 main.py:51] epoch 4069, training loss: 13070.26, average training loss: 11174.59, base loss: 19707.32
[INFO 2017-06-27 22:12:54,423 main.py:51] epoch 4070, training loss: 10958.60, average training loss: 11174.24, base loss: 19705.30
[INFO 2017-06-27 22:12:55,397 main.py:51] epoch 4071, training loss: 10176.35, average training loss: 11175.24, base loss: 19708.31
[INFO 2017-06-27 22:12:56,371 main.py:51] epoch 4072, training loss: 11340.19, average training loss: 11176.75, base loss: 19714.69
[INFO 2017-06-27 22:12:57,345 main.py:51] epoch 4073, training loss: 10940.10, average training loss: 11175.64, base loss: 19714.51
[INFO 2017-06-27 22:12:58,317 main.py:51] epoch 4074, training loss: 10836.82, average training loss: 11176.10, base loss: 19716.62
[INFO 2017-06-27 22:12:59,294 main.py:51] epoch 4075, training loss: 11796.07, average training loss: 11176.90, base loss: 19720.41
[INFO 2017-06-27 22:13:00,270 main.py:51] epoch 4076, training loss: 10988.09, average training loss: 11176.52, base loss: 19719.98
[INFO 2017-06-27 22:13:01,246 main.py:51] epoch 4077, training loss: 11679.45, average training loss: 11177.42, base loss: 19723.53
[INFO 2017-06-27 22:13:02,221 main.py:51] epoch 4078, training loss: 10093.04, average training loss: 11176.99, base loss: 19723.21
[INFO 2017-06-27 22:13:03,194 main.py:51] epoch 4079, training loss: 11061.50, average training loss: 11175.87, base loss: 19721.05
[INFO 2017-06-27 22:13:04,171 main.py:51] epoch 4080, training loss: 12134.60, average training loss: 11177.00, base loss: 19725.24
[INFO 2017-06-27 22:13:05,146 main.py:51] epoch 4081, training loss: 11477.71, average training loss: 11178.13, base loss: 19727.93
[INFO 2017-06-27 22:13:06,240 main.py:51] epoch 4082, training loss: 10633.37, average training loss: 11176.62, base loss: 19724.58
[INFO 2017-06-27 22:13:07,216 main.py:51] epoch 4083, training loss: 10107.73, average training loss: 11175.82, base loss: 19723.75
[INFO 2017-06-27 22:13:08,190 main.py:51] epoch 4084, training loss: 11604.14, average training loss: 11176.26, base loss: 19726.30
[INFO 2017-06-27 22:13:09,165 main.py:51] epoch 4085, training loss: 10287.51, average training loss: 11174.93, base loss: 19724.34
[INFO 2017-06-27 22:13:10,242 main.py:51] epoch 4086, training loss: 11255.67, average training loss: 11175.00, base loss: 19725.82
[INFO 2017-06-27 22:13:11,263 main.py:51] epoch 4087, training loss: 10667.22, average training loss: 11175.30, base loss: 19728.50
[INFO 2017-06-27 22:13:12,354 main.py:51] epoch 4088, training loss: 10570.94, average training loss: 11174.66, base loss: 19727.30
[INFO 2017-06-27 22:13:13,387 main.py:51] epoch 4089, training loss: 10465.83, average training loss: 11173.97, base loss: 19726.88
[INFO 2017-06-27 22:13:14,381 main.py:51] epoch 4090, training loss: 10859.29, average training loss: 11174.73, base loss: 19729.48
[INFO 2017-06-27 22:13:15,456 main.py:51] epoch 4091, training loss: 10435.33, average training loss: 11173.72, base loss: 19728.20
[INFO 2017-06-27 22:13:16,578 main.py:51] epoch 4092, training loss: 11869.09, average training loss: 11174.05, base loss: 19728.59
[INFO 2017-06-27 22:13:17,577 main.py:51] epoch 4093, training loss: 10655.48, average training loss: 11171.12, base loss: 19723.38
[INFO 2017-06-27 22:13:18,650 main.py:51] epoch 4094, training loss: 11003.25, average training loss: 11170.14, base loss: 19723.31
[INFO 2017-06-27 22:13:19,750 main.py:51] epoch 4095, training loss: 10769.75, average training loss: 11169.85, base loss: 19723.98
[INFO 2017-06-27 22:13:20,775 main.py:51] epoch 4096, training loss: 11847.98, average training loss: 11170.60, base loss: 19725.92
[INFO 2017-06-27 22:13:21,748 main.py:51] epoch 4097, training loss: 11218.20, average training loss: 11171.57, base loss: 19728.42
[INFO 2017-06-27 22:13:22,728 main.py:51] epoch 4098, training loss: 10723.73, average training loss: 11169.97, base loss: 19724.89
[INFO 2017-06-27 22:13:23,700 main.py:51] epoch 4099, training loss: 12194.25, average training loss: 11170.86, base loss: 19728.71
[INFO 2017-06-27 22:13:23,700 main.py:53] epoch 4099, testing
[INFO 2017-06-27 22:13:27,920 main.py:105] average testing loss: 10758.45, base loss: 19384.85
[INFO 2017-06-27 22:13:27,920 main.py:106] improve_loss: 8626.40, improve_percent: 0.45
[INFO 2017-06-27 22:13:27,920 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:13:28,893 main.py:51] epoch 4100, training loss: 10608.28, average training loss: 11170.42, base loss: 19730.12
[INFO 2017-06-27 22:13:29,867 main.py:51] epoch 4101, training loss: 11536.57, average training loss: 11168.42, base loss: 19727.52
[INFO 2017-06-27 22:13:30,836 main.py:51] epoch 4102, training loss: 11561.12, average training loss: 11168.67, base loss: 19728.05
[INFO 2017-06-27 22:13:31,812 main.py:51] epoch 4103, training loss: 10591.66, average training loss: 11168.46, base loss: 19729.26
[INFO 2017-06-27 22:13:32,783 main.py:51] epoch 4104, training loss: 11632.52, average training loss: 11167.17, base loss: 19728.89
[INFO 2017-06-27 22:13:33,758 main.py:51] epoch 4105, training loss: 10331.51, average training loss: 11166.42, base loss: 19729.16
[INFO 2017-06-27 22:13:34,727 main.py:51] epoch 4106, training loss: 10875.43, average training loss: 11166.08, base loss: 19729.01
[INFO 2017-06-27 22:13:35,697 main.py:51] epoch 4107, training loss: 10731.64, average training loss: 11164.89, base loss: 19726.90
[INFO 2017-06-27 22:13:36,672 main.py:51] epoch 4108, training loss: 10796.81, average training loss: 11163.97, base loss: 19724.26
[INFO 2017-06-27 22:13:37,651 main.py:51] epoch 4109, training loss: 10639.58, average training loss: 11163.47, base loss: 19724.19
[INFO 2017-06-27 22:13:38,625 main.py:51] epoch 4110, training loss: 10278.59, average training loss: 11163.01, base loss: 19723.47
[INFO 2017-06-27 22:13:39,600 main.py:51] epoch 4111, training loss: 10239.90, average training loss: 11161.83, base loss: 19722.02
[INFO 2017-06-27 22:13:40,576 main.py:51] epoch 4112, training loss: 11171.52, average training loss: 11160.93, base loss: 19721.66
[INFO 2017-06-27 22:13:41,552 main.py:51] epoch 4113, training loss: 11655.50, average training loss: 11162.19, base loss: 19723.63
[INFO 2017-06-27 22:13:42,526 main.py:51] epoch 4114, training loss: 13788.00, average training loss: 11166.57, base loss: 19734.33
[INFO 2017-06-27 22:13:43,501 main.py:51] epoch 4115, training loss: 10667.44, average training loss: 11164.24, base loss: 19730.36
[INFO 2017-06-27 22:13:44,524 main.py:51] epoch 4116, training loss: 10731.90, average training loss: 11163.78, base loss: 19730.29
[INFO 2017-06-27 22:13:45,528 main.py:51] epoch 4117, training loss: 12360.50, average training loss: 11164.68, base loss: 19733.24
[INFO 2017-06-27 22:13:46,500 main.py:51] epoch 4118, training loss: 13113.24, average training loss: 11164.92, base loss: 19734.65
[INFO 2017-06-27 22:13:47,474 main.py:51] epoch 4119, training loss: 11779.70, average training loss: 11164.31, base loss: 19733.42
[INFO 2017-06-27 22:13:48,446 main.py:51] epoch 4120, training loss: 11895.03, average training loss: 11165.42, base loss: 19736.48
[INFO 2017-06-27 22:13:49,445 main.py:51] epoch 4121, training loss: 11902.12, average training loss: 11164.69, base loss: 19735.49
[INFO 2017-06-27 22:13:50,460 main.py:51] epoch 4122, training loss: 11361.61, average training loss: 11165.06, base loss: 19736.60
[INFO 2017-06-27 22:13:51,487 main.py:51] epoch 4123, training loss: 10519.26, average training loss: 11166.06, base loss: 19739.76
[INFO 2017-06-27 22:13:52,482 main.py:51] epoch 4124, training loss: 10864.88, average training loss: 11164.45, base loss: 19737.61
[INFO 2017-06-27 22:13:53,458 main.py:51] epoch 4125, training loss: 10831.29, average training loss: 11164.01, base loss: 19738.17
[INFO 2017-06-27 22:13:54,433 main.py:51] epoch 4126, training loss: 12703.68, average training loss: 11164.87, base loss: 19740.76
[INFO 2017-06-27 22:13:55,410 main.py:51] epoch 4127, training loss: 11015.48, average training loss: 11162.99, base loss: 19736.93
[INFO 2017-06-27 22:13:56,386 main.py:51] epoch 4128, training loss: 10612.30, average training loss: 11163.33, base loss: 19739.75
[INFO 2017-06-27 22:13:57,362 main.py:51] epoch 4129, training loss: 11297.99, average training loss: 11163.52, base loss: 19739.99
[INFO 2017-06-27 22:13:58,342 main.py:51] epoch 4130, training loss: 10630.84, average training loss: 11161.86, base loss: 19737.53
[INFO 2017-06-27 22:13:59,318 main.py:51] epoch 4131, training loss: 11054.08, average training loss: 11162.21, base loss: 19740.08
[INFO 2017-06-27 22:14:00,292 main.py:51] epoch 4132, training loss: 9992.05, average training loss: 11161.87, base loss: 19740.02
[INFO 2017-06-27 22:14:01,271 main.py:51] epoch 4133, training loss: 11515.10, average training loss: 11162.79, base loss: 19744.84
[INFO 2017-06-27 22:14:02,244 main.py:51] epoch 4134, training loss: 9074.91, average training loss: 11161.24, base loss: 19741.94
[INFO 2017-06-27 22:14:03,218 main.py:51] epoch 4135, training loss: 11842.25, average training loss: 11157.85, base loss: 19736.98
[INFO 2017-06-27 22:14:04,191 main.py:51] epoch 4136, training loss: 11541.06, average training loss: 11158.40, base loss: 19739.11
[INFO 2017-06-27 22:14:05,167 main.py:51] epoch 4137, training loss: 10492.42, average training loss: 11157.02, base loss: 19737.41
[INFO 2017-06-27 22:14:06,142 main.py:51] epoch 4138, training loss: 10894.09, average training loss: 11156.13, base loss: 19736.05
[INFO 2017-06-27 22:14:07,116 main.py:51] epoch 4139, training loss: 10743.34, average training loss: 11155.99, base loss: 19736.93
[INFO 2017-06-27 22:14:08,199 main.py:51] epoch 4140, training loss: 10385.52, average training loss: 11154.22, base loss: 19734.18
[INFO 2017-06-27 22:14:09,225 main.py:51] epoch 4141, training loss: 10596.26, average training loss: 11154.09, base loss: 19735.28
[INFO 2017-06-27 22:14:10,228 main.py:51] epoch 4142, training loss: 11074.88, average training loss: 11154.09, base loss: 19735.96
[INFO 2017-06-27 22:14:11,344 main.py:51] epoch 4143, training loss: 11552.03, average training loss: 11155.60, base loss: 19740.67
[INFO 2017-06-27 22:14:12,418 main.py:51] epoch 4144, training loss: 10071.34, average training loss: 11155.74, base loss: 19742.76
[INFO 2017-06-27 22:14:13,511 main.py:51] epoch 4145, training loss: 10471.67, average training loss: 11155.22, base loss: 19741.88
[INFO 2017-06-27 22:14:14,572 main.py:51] epoch 4146, training loss: 13373.69, average training loss: 11155.38, base loss: 19743.06
[INFO 2017-06-27 22:14:15,640 main.py:51] epoch 4147, training loss: 11325.89, average training loss: 11154.89, base loss: 19741.56
[INFO 2017-06-27 22:14:16,671 main.py:51] epoch 4148, training loss: 10364.96, average training loss: 11151.72, base loss: 19736.39
[INFO 2017-06-27 22:14:17,647 main.py:51] epoch 4149, training loss: 12027.69, average training loss: 11153.96, base loss: 19741.75
[INFO 2017-06-27 22:14:18,622 main.py:51] epoch 4150, training loss: 10458.31, average training loss: 11152.00, base loss: 19740.01
[INFO 2017-06-27 22:14:19,601 main.py:51] epoch 4151, training loss: 12223.35, average training loss: 11151.05, base loss: 19738.52
[INFO 2017-06-27 22:14:20,575 main.py:51] epoch 4152, training loss: 11777.24, average training loss: 11152.27, base loss: 19742.18
[INFO 2017-06-27 22:14:21,548 main.py:51] epoch 4153, training loss: 11116.93, average training loss: 11150.67, base loss: 19739.54
[INFO 2017-06-27 22:14:22,517 main.py:51] epoch 4154, training loss: 10304.03, average training loss: 11149.49, base loss: 19737.91
[INFO 2017-06-27 22:14:23,490 main.py:51] epoch 4155, training loss: 10826.49, average training loss: 11148.93, base loss: 19738.85
[INFO 2017-06-27 22:14:24,465 main.py:51] epoch 4156, training loss: 10436.60, average training loss: 11147.90, base loss: 19737.58
[INFO 2017-06-27 22:14:25,439 main.py:51] epoch 4157, training loss: 10983.03, average training loss: 11146.42, base loss: 19734.59
[INFO 2017-06-27 22:14:26,411 main.py:51] epoch 4158, training loss: 10611.25, average training loss: 11145.66, base loss: 19731.92
[INFO 2017-06-27 22:14:27,382 main.py:51] epoch 4159, training loss: 11204.56, average training loss: 11146.35, base loss: 19734.16
[INFO 2017-06-27 22:14:28,354 main.py:51] epoch 4160, training loss: 11365.62, average training loss: 11146.39, base loss: 19736.38
[INFO 2017-06-27 22:14:29,329 main.py:51] epoch 4161, training loss: 11988.14, average training loss: 11148.13, base loss: 19740.54
[INFO 2017-06-27 22:14:30,304 main.py:51] epoch 4162, training loss: 11426.20, average training loss: 11148.56, base loss: 19742.50
[INFO 2017-06-27 22:14:31,280 main.py:51] epoch 4163, training loss: 10862.27, average training loss: 11146.72, base loss: 19738.79
[INFO 2017-06-27 22:14:32,254 main.py:51] epoch 4164, training loss: 11228.60, average training loss: 11145.02, base loss: 19736.00
[INFO 2017-06-27 22:14:33,230 main.py:51] epoch 4165, training loss: 10185.03, average training loss: 11145.00, base loss: 19737.10
[INFO 2017-06-27 22:14:34,198 main.py:51] epoch 4166, training loss: 10112.97, average training loss: 11142.99, base loss: 19736.73
[INFO 2017-06-27 22:14:35,174 main.py:51] epoch 4167, training loss: 11233.58, average training loss: 11143.60, base loss: 19739.20
[INFO 2017-06-27 22:14:36,150 main.py:51] epoch 4168, training loss: 10313.82, average training loss: 11142.92, base loss: 19737.64
[INFO 2017-06-27 22:14:37,126 main.py:51] epoch 4169, training loss: 12094.27, average training loss: 11142.66, base loss: 19736.21
[INFO 2017-06-27 22:14:38,098 main.py:51] epoch 4170, training loss: 11649.77, average training loss: 11142.72, base loss: 19737.72
[INFO 2017-06-27 22:14:39,073 main.py:51] epoch 4171, training loss: 11901.37, average training loss: 11142.51, base loss: 19738.11
[INFO 2017-06-27 22:14:40,044 main.py:51] epoch 4172, training loss: 8682.85, average training loss: 11140.32, base loss: 19734.72
[INFO 2017-06-27 22:14:41,015 main.py:51] epoch 4173, training loss: 10532.36, average training loss: 11140.45, base loss: 19736.07
[INFO 2017-06-27 22:14:41,992 main.py:51] epoch 4174, training loss: 10180.37, average training loss: 11140.19, base loss: 19734.67
[INFO 2017-06-27 22:14:42,963 main.py:51] epoch 4175, training loss: 11415.45, average training loss: 11141.00, base loss: 19738.40
[INFO 2017-06-27 22:14:43,938 main.py:51] epoch 4176, training loss: 10897.85, average training loss: 11140.22, base loss: 19739.47
[INFO 2017-06-27 22:14:44,911 main.py:51] epoch 4177, training loss: 11163.16, average training loss: 11138.68, base loss: 19736.75
[INFO 2017-06-27 22:14:45,884 main.py:51] epoch 4178, training loss: 10520.57, average training loss: 11135.90, base loss: 19732.66
[INFO 2017-06-27 22:14:46,861 main.py:51] epoch 4179, training loss: 10438.36, average training loss: 11135.30, base loss: 19731.23
[INFO 2017-06-27 22:14:47,834 main.py:51] epoch 4180, training loss: 10753.33, average training loss: 11135.11, base loss: 19731.60
[INFO 2017-06-27 22:14:48,862 main.py:51] epoch 4181, training loss: 11141.83, average training loss: 11133.80, base loss: 19729.38
[INFO 2017-06-27 22:14:49,900 main.py:51] epoch 4182, training loss: 10347.36, average training loss: 11132.08, base loss: 19726.45
[INFO 2017-06-27 22:14:50,881 main.py:51] epoch 4183, training loss: 10179.29, average training loss: 11130.39, base loss: 19723.80
[INFO 2017-06-27 22:14:51,900 main.py:51] epoch 4184, training loss: 11116.62, average training loss: 11129.80, base loss: 19723.10
[INFO 2017-06-27 22:14:52,900 main.py:51] epoch 4185, training loss: 10403.84, average training loss: 11128.28, base loss: 19721.11
[INFO 2017-06-27 22:14:53,880 main.py:51] epoch 4186, training loss: 11231.58, average training loss: 11128.76, base loss: 19721.35
[INFO 2017-06-27 22:14:54,853 main.py:51] epoch 4187, training loss: 10350.41, average training loss: 11126.95, base loss: 19718.29
[INFO 2017-06-27 22:14:55,836 main.py:51] epoch 4188, training loss: 11854.09, average training loss: 11127.88, base loss: 19722.28
[INFO 2017-06-27 22:14:56,807 main.py:51] epoch 4189, training loss: 9259.77, average training loss: 11125.99, base loss: 19717.10
[INFO 2017-06-27 22:14:57,787 main.py:51] epoch 4190, training loss: 10369.13, average training loss: 11124.37, base loss: 19714.10
[INFO 2017-06-27 22:14:58,766 main.py:51] epoch 4191, training loss: 11011.94, average training loss: 11124.05, base loss: 19713.79
[INFO 2017-06-27 22:14:59,742 main.py:51] epoch 4192, training loss: 13435.60, average training loss: 11124.63, base loss: 19714.73
[INFO 2017-06-27 22:15:00,712 main.py:51] epoch 4193, training loss: 10624.82, average training loss: 11124.50, base loss: 19715.76
[INFO 2017-06-27 22:15:01,683 main.py:51] epoch 4194, training loss: 9536.33, average training loss: 11122.60, base loss: 19713.29
[INFO 2017-06-27 22:15:02,655 main.py:51] epoch 4195, training loss: 12020.17, average training loss: 11124.08, base loss: 19716.74
[INFO 2017-06-27 22:15:03,634 main.py:51] epoch 4196, training loss: 10914.51, average training loss: 11122.34, base loss: 19714.53
[INFO 2017-06-27 22:15:04,716 main.py:51] epoch 4197, training loss: 11056.20, average training loss: 11121.55, base loss: 19713.16
[INFO 2017-06-27 22:15:05,752 main.py:51] epoch 4198, training loss: 10149.21, average training loss: 11120.06, base loss: 19710.07
[INFO 2017-06-27 22:15:06,749 main.py:51] epoch 4199, training loss: 9812.11, average training loss: 11117.57, base loss: 19704.89
[INFO 2017-06-27 22:15:06,749 main.py:53] epoch 4199, testing
[INFO 2017-06-27 22:15:11,117 main.py:105] average testing loss: 11120.01, base loss: 19812.52
[INFO 2017-06-27 22:15:11,117 main.py:106] improve_loss: 8692.51, improve_percent: 0.44
[INFO 2017-06-27 22:15:11,117 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:15:12,090 main.py:51] epoch 4200, training loss: 10614.01, average training loss: 11116.91, base loss: 19705.40
[INFO 2017-06-27 22:15:13,061 main.py:51] epoch 4201, training loss: 11000.43, average training loss: 11116.31, base loss: 19704.87
[INFO 2017-06-27 22:15:14,037 main.py:51] epoch 4202, training loss: 10867.06, average training loss: 11116.55, base loss: 19704.48
[INFO 2017-06-27 22:15:15,011 main.py:51] epoch 4203, training loss: 11869.28, average training loss: 11117.28, base loss: 19708.30
[INFO 2017-06-27 22:15:15,983 main.py:51] epoch 4204, training loss: 9692.67, average training loss: 11116.08, base loss: 19706.25
[INFO 2017-06-27 22:15:16,955 main.py:51] epoch 4205, training loss: 12217.10, average training loss: 11115.01, base loss: 19705.61
[INFO 2017-06-27 22:15:17,925 main.py:51] epoch 4206, training loss: 9631.11, average training loss: 11112.83, base loss: 19702.95
[INFO 2017-06-27 22:15:18,895 main.py:51] epoch 4207, training loss: 11290.65, average training loss: 11110.98, base loss: 19699.15
[INFO 2017-06-27 22:15:19,871 main.py:51] epoch 4208, training loss: 10753.93, average training loss: 11110.01, base loss: 19698.02
[INFO 2017-06-27 22:15:20,943 main.py:51] epoch 4209, training loss: 11666.65, average training loss: 11110.80, base loss: 19700.16
[INFO 2017-06-27 22:15:21,978 main.py:51] epoch 4210, training loss: 11625.65, average training loss: 11111.05, base loss: 19701.88
[INFO 2017-06-27 22:15:23,085 main.py:51] epoch 4211, training loss: 11561.33, average training loss: 11112.55, base loss: 19706.62
[INFO 2017-06-27 22:15:24,162 main.py:51] epoch 4212, training loss: 10781.85, average training loss: 11111.59, base loss: 19703.41
[INFO 2017-06-27 22:15:25,150 main.py:51] epoch 4213, training loss: 13388.58, average training loss: 11113.82, base loss: 19708.87
[INFO 2017-06-27 22:15:26,125 main.py:51] epoch 4214, training loss: 10664.42, average training loss: 11114.81, base loss: 19709.70
[INFO 2017-06-27 22:15:27,201 main.py:51] epoch 4215, training loss: 10849.38, average training loss: 11114.46, base loss: 19710.54
[INFO 2017-06-27 22:15:28,180 main.py:51] epoch 4216, training loss: 10649.82, average training loss: 11113.22, base loss: 19708.57
[INFO 2017-06-27 22:15:29,153 main.py:51] epoch 4217, training loss: 11953.24, average training loss: 11113.49, base loss: 19710.70
[INFO 2017-06-27 22:15:30,135 main.py:51] epoch 4218, training loss: 10488.63, average training loss: 11111.57, base loss: 19706.48
[INFO 2017-06-27 22:15:31,108 main.py:51] epoch 4219, training loss: 11876.23, average training loss: 11112.52, base loss: 19710.08
[INFO 2017-06-27 22:15:32,080 main.py:51] epoch 4220, training loss: 11976.27, average training loss: 11113.17, base loss: 19713.03
[INFO 2017-06-27 22:15:33,055 main.py:51] epoch 4221, training loss: 10662.42, average training loss: 11113.10, base loss: 19712.71
[INFO 2017-06-27 22:15:34,029 main.py:51] epoch 4222, training loss: 9643.10, average training loss: 11112.53, base loss: 19712.65
[INFO 2017-06-27 22:15:35,005 main.py:51] epoch 4223, training loss: 11584.84, average training loss: 11113.84, base loss: 19715.62
[INFO 2017-06-27 22:15:36,136 main.py:51] epoch 4224, training loss: 10801.84, average training loss: 11112.73, base loss: 19713.66
[INFO 2017-06-27 22:15:37,130 main.py:51] epoch 4225, training loss: 10571.02, average training loss: 11112.28, base loss: 19713.05
[INFO 2017-06-27 22:15:38,100 main.py:51] epoch 4226, training loss: 11903.90, average training loss: 11114.26, base loss: 19717.73
[INFO 2017-06-27 22:15:39,075 main.py:51] epoch 4227, training loss: 9790.07, average training loss: 11112.13, base loss: 19713.89
[INFO 2017-06-27 22:15:40,170 main.py:51] epoch 4228, training loss: 10682.12, average training loss: 11111.64, base loss: 19714.67
[INFO 2017-06-27 22:15:41,251 main.py:51] epoch 4229, training loss: 11306.81, average training loss: 11111.81, base loss: 19714.73
[INFO 2017-06-27 22:15:42,323 main.py:51] epoch 4230, training loss: 12414.12, average training loss: 11113.43, base loss: 19720.17
[INFO 2017-06-27 22:15:43,331 main.py:51] epoch 4231, training loss: 11668.82, average training loss: 11112.82, base loss: 19721.82
[INFO 2017-06-27 22:15:44,406 main.py:51] epoch 4232, training loss: 11671.29, average training loss: 11114.11, base loss: 19724.68
[INFO 2017-06-27 22:15:45,512 main.py:51] epoch 4233, training loss: 10864.59, average training loss: 11113.47, base loss: 19723.97
[INFO 2017-06-27 22:15:46,583 main.py:51] epoch 4234, training loss: 10403.88, average training loss: 11111.65, base loss: 19721.50
[INFO 2017-06-27 22:15:47,582 main.py:51] epoch 4235, training loss: 11029.46, average training loss: 11110.99, base loss: 19720.56
[INFO 2017-06-27 22:15:48,686 main.py:51] epoch 4236, training loss: 10314.10, average training loss: 11109.13, base loss: 19717.22
[INFO 2017-06-27 22:15:49,726 main.py:51] epoch 4237, training loss: 10486.35, average training loss: 11108.93, base loss: 19718.15
[INFO 2017-06-27 22:15:50,751 main.py:51] epoch 4238, training loss: 10397.22, average training loss: 11107.54, base loss: 19714.92
[INFO 2017-06-27 22:15:51,790 main.py:51] epoch 4239, training loss: 11933.40, average training loss: 11108.08, base loss: 19717.35
[INFO 2017-06-27 22:15:52,894 main.py:51] epoch 4240, training loss: 10627.72, average training loss: 11108.92, base loss: 19720.24
[INFO 2017-06-27 22:15:53,940 main.py:51] epoch 4241, training loss: 9569.70, average training loss: 11106.01, base loss: 19714.41
[INFO 2017-06-27 22:15:54,941 main.py:51] epoch 4242, training loss: 11706.81, average training loss: 11105.50, base loss: 19712.44
[INFO 2017-06-27 22:15:56,021 main.py:51] epoch 4243, training loss: 12675.00, average training loss: 11107.68, base loss: 19717.92
[INFO 2017-06-27 22:15:57,016 main.py:51] epoch 4244, training loss: 10527.05, average training loss: 11106.46, base loss: 19716.25
[INFO 2017-06-27 22:15:57,987 main.py:51] epoch 4245, training loss: 9878.61, average training loss: 11105.87, base loss: 19716.07
[INFO 2017-06-27 22:15:59,062 main.py:51] epoch 4246, training loss: 11566.85, average training loss: 11106.50, base loss: 19718.16
[INFO 2017-06-27 22:16:00,134 main.py:51] epoch 4247, training loss: 11036.47, average training loss: 11107.38, base loss: 19720.71
[INFO 2017-06-27 22:16:01,128 main.py:51] epoch 4248, training loss: 9445.24, average training loss: 11106.20, base loss: 19717.94
[INFO 2017-06-27 22:16:02,157 main.py:51] epoch 4249, training loss: 11861.02, average training loss: 11106.40, base loss: 19718.52
[INFO 2017-06-27 22:16:03,175 main.py:51] epoch 4250, training loss: 9433.40, average training loss: 11103.37, base loss: 19710.26
[INFO 2017-06-27 22:16:04,261 main.py:51] epoch 4251, training loss: 12894.88, average training loss: 11105.54, base loss: 19715.97
[INFO 2017-06-27 22:16:05,259 main.py:51] epoch 4252, training loss: 12532.28, average training loss: 11107.74, base loss: 19721.36
[INFO 2017-06-27 22:16:06,365 main.py:51] epoch 4253, training loss: 11357.34, average training loss: 11107.94, base loss: 19723.06
[INFO 2017-06-27 22:16:07,356 main.py:51] epoch 4254, training loss: 11449.30, average training loss: 11109.54, base loss: 19728.09
[INFO 2017-06-27 22:16:08,431 main.py:51] epoch 4255, training loss: 10119.19, average training loss: 11108.18, base loss: 19728.22
[INFO 2017-06-27 22:16:09,516 main.py:51] epoch 4256, training loss: 10543.58, average training loss: 11107.06, base loss: 19726.87
[INFO 2017-06-27 22:16:10,582 main.py:51] epoch 4257, training loss: 10452.19, average training loss: 11104.99, base loss: 19722.85
[INFO 2017-06-27 22:16:11,588 main.py:51] epoch 4258, training loss: 11063.29, average training loss: 11105.09, base loss: 19723.72
[INFO 2017-06-27 22:16:12,664 main.py:51] epoch 4259, training loss: 9957.38, average training loss: 11104.73, base loss: 19723.58
[INFO 2017-06-27 22:16:13,783 main.py:51] epoch 4260, training loss: 10578.14, average training loss: 11105.24, base loss: 19725.11
[INFO 2017-06-27 22:16:14,841 main.py:51] epoch 4261, training loss: 10309.23, average training loss: 11103.10, base loss: 19719.63
[INFO 2017-06-27 22:16:15,867 main.py:51] epoch 4262, training loss: 10727.05, average training loss: 11101.69, base loss: 19717.16
[INFO 2017-06-27 22:16:16,952 main.py:51] epoch 4263, training loss: 10139.90, average training loss: 11101.45, base loss: 19717.79
[INFO 2017-06-27 22:16:17,937 main.py:51] epoch 4264, training loss: 11647.69, average training loss: 11101.98, base loss: 19718.07
[INFO 2017-06-27 22:16:19,012 main.py:51] epoch 4265, training loss: 10063.46, average training loss: 11100.82, base loss: 19715.94
[INFO 2017-06-27 22:16:20,169 main.py:51] epoch 4266, training loss: 10772.83, average training loss: 11100.54, base loss: 19716.93
[INFO 2017-06-27 22:16:21,185 main.py:51] epoch 4267, training loss: 12575.21, average training loss: 11102.16, base loss: 19720.04
[INFO 2017-06-27 22:16:22,159 main.py:51] epoch 4268, training loss: 12016.22, average training loss: 11103.44, base loss: 19724.01
[INFO 2017-06-27 22:16:23,135 main.py:51] epoch 4269, training loss: 10991.91, average training loss: 11103.63, base loss: 19726.17
[INFO 2017-06-27 22:16:24,107 main.py:51] epoch 4270, training loss: 10920.22, average training loss: 11104.30, base loss: 19727.74
[INFO 2017-06-27 22:16:25,076 main.py:51] epoch 4271, training loss: 9909.14, average training loss: 11103.82, base loss: 19726.18
[INFO 2017-06-27 22:16:26,054 main.py:51] epoch 4272, training loss: 10667.36, average training loss: 11102.25, base loss: 19724.32
[INFO 2017-06-27 22:16:27,025 main.py:51] epoch 4273, training loss: 10529.25, average training loss: 11101.26, base loss: 19721.38
[INFO 2017-06-27 22:16:27,999 main.py:51] epoch 4274, training loss: 10870.90, average training loss: 11100.53, base loss: 19721.61
[INFO 2017-06-27 22:16:29,077 main.py:51] epoch 4275, training loss: 10134.12, average training loss: 11100.96, base loss: 19724.84
[INFO 2017-06-27 22:16:30,058 main.py:51] epoch 4276, training loss: 10044.35, average training loss: 11099.10, base loss: 19720.07
[INFO 2017-06-27 22:16:31,037 main.py:51] epoch 4277, training loss: 11042.21, average training loss: 11099.49, base loss: 19721.39
[INFO 2017-06-27 22:16:32,010 main.py:51] epoch 4278, training loss: 10862.67, average training loss: 11099.60, base loss: 19721.89
[INFO 2017-06-27 22:16:32,987 main.py:51] epoch 4279, training loss: 12250.96, average training loss: 11100.71, base loss: 19724.73
[INFO 2017-06-27 22:16:33,963 main.py:51] epoch 4280, training loss: 11562.43, average training loss: 11100.09, base loss: 19723.38
[INFO 2017-06-27 22:16:34,939 main.py:51] epoch 4281, training loss: 10221.21, average training loss: 11099.13, base loss: 19720.92
[INFO 2017-06-27 22:16:35,921 main.py:51] epoch 4282, training loss: 12027.58, average training loss: 11096.83, base loss: 19715.43
[INFO 2017-06-27 22:16:36,896 main.py:51] epoch 4283, training loss: 9934.34, average training loss: 11095.11, base loss: 19711.47
[INFO 2017-06-27 22:16:37,872 main.py:51] epoch 4284, training loss: 10637.11, average training loss: 11093.89, base loss: 19709.08
[INFO 2017-06-27 22:16:38,843 main.py:51] epoch 4285, training loss: 10432.25, average training loss: 11091.95, base loss: 19705.07
[INFO 2017-06-27 22:16:39,815 main.py:51] epoch 4286, training loss: 9500.04, average training loss: 11090.07, base loss: 19701.33
[INFO 2017-06-27 22:16:40,788 main.py:51] epoch 4287, training loss: 10360.99, average training loss: 11087.68, base loss: 19698.44
[INFO 2017-06-27 22:16:41,783 main.py:51] epoch 4288, training loss: 11366.59, average training loss: 11088.77, base loss: 19701.60
[INFO 2017-06-27 22:16:42,757 main.py:51] epoch 4289, training loss: 11017.94, average training loss: 11088.66, base loss: 19702.45
[INFO 2017-06-27 22:16:43,731 main.py:51] epoch 4290, training loss: 11864.76, average training loss: 11088.62, base loss: 19703.77
[INFO 2017-06-27 22:16:44,702 main.py:51] epoch 4291, training loss: 10626.81, average training loss: 11088.25, base loss: 19703.03
[INFO 2017-06-27 22:16:45,676 main.py:51] epoch 4292, training loss: 10901.15, average training loss: 11089.23, base loss: 19706.19
[INFO 2017-06-27 22:16:46,647 main.py:51] epoch 4293, training loss: 10710.13, average training loss: 11090.08, base loss: 19709.25
[INFO 2017-06-27 22:16:47,620 main.py:51] epoch 4294, training loss: 11211.74, average training loss: 11091.28, base loss: 19710.60
[INFO 2017-06-27 22:16:48,595 main.py:51] epoch 4295, training loss: 10366.34, average training loss: 11090.04, base loss: 19708.77
[INFO 2017-06-27 22:16:49,572 main.py:51] epoch 4296, training loss: 10918.83, average training loss: 11088.24, base loss: 19705.41
[INFO 2017-06-27 22:16:50,543 main.py:51] epoch 4297, training loss: 11941.79, average training loss: 11088.87, base loss: 19707.10
[INFO 2017-06-27 22:16:51,519 main.py:51] epoch 4298, training loss: 11442.83, average training loss: 11089.03, base loss: 19707.79
[INFO 2017-06-27 22:16:52,494 main.py:51] epoch 4299, training loss: 10023.64, average training loss: 11086.74, base loss: 19701.17
[INFO 2017-06-27 22:16:52,494 main.py:53] epoch 4299, testing
[INFO 2017-06-27 22:16:56,711 main.py:105] average testing loss: 10988.84, base loss: 19940.60
[INFO 2017-06-27 22:16:56,711 main.py:106] improve_loss: 8951.75, improve_percent: 0.45
[INFO 2017-06-27 22:16:56,712 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:16:57,686 main.py:51] epoch 4300, training loss: 10291.70, average training loss: 11086.01, base loss: 19700.77
[INFO 2017-06-27 22:16:58,662 main.py:51] epoch 4301, training loss: 10868.77, average training loss: 11084.21, base loss: 19698.40
[INFO 2017-06-27 22:16:59,631 main.py:51] epoch 4302, training loss: 9900.00, average training loss: 11083.68, base loss: 19697.16
[INFO 2017-06-27 22:17:00,605 main.py:51] epoch 4303, training loss: 10655.14, average training loss: 11083.09, base loss: 19697.16
[INFO 2017-06-27 22:17:01,577 main.py:51] epoch 4304, training loss: 10778.85, average training loss: 11083.48, base loss: 19698.13
[INFO 2017-06-27 22:17:02,549 main.py:51] epoch 4305, training loss: 11587.22, average training loss: 11084.67, base loss: 19700.42
[INFO 2017-06-27 22:17:03,521 main.py:51] epoch 4306, training loss: 11772.31, average training loss: 11085.57, base loss: 19703.05
[INFO 2017-06-27 22:17:04,496 main.py:51] epoch 4307, training loss: 10336.35, average training loss: 11084.92, base loss: 19703.44
[INFO 2017-06-27 22:17:05,470 main.py:51] epoch 4308, training loss: 11383.31, average training loss: 11083.47, base loss: 19702.30
[INFO 2017-06-27 22:17:06,443 main.py:51] epoch 4309, training loss: 10057.77, average training loss: 11082.72, base loss: 19701.37
[INFO 2017-06-27 22:17:07,419 main.py:51] epoch 4310, training loss: 11469.66, average training loss: 11083.52, base loss: 19704.92
[INFO 2017-06-27 22:17:08,395 main.py:51] epoch 4311, training loss: 10933.56, average training loss: 11083.39, base loss: 19705.62
[INFO 2017-06-27 22:17:09,368 main.py:51] epoch 4312, training loss: 13021.45, average training loss: 11085.10, base loss: 19709.26
[INFO 2017-06-27 22:17:10,348 main.py:51] epoch 4313, training loss: 10604.14, average training loss: 11084.96, base loss: 19709.28
[INFO 2017-06-27 22:17:11,323 main.py:51] epoch 4314, training loss: 10310.87, average training loss: 11084.21, base loss: 19707.43
[INFO 2017-06-27 22:17:12,297 main.py:51] epoch 4315, training loss: 10128.84, average training loss: 11083.97, base loss: 19709.12
[INFO 2017-06-27 22:17:13,270 main.py:51] epoch 4316, training loss: 10792.99, average training loss: 11083.31, base loss: 19708.08
[INFO 2017-06-27 22:17:14,242 main.py:51] epoch 4317, training loss: 9152.57, average training loss: 11082.32, base loss: 19705.92
[INFO 2017-06-27 22:17:15,217 main.py:51] epoch 4318, training loss: 11361.89, average training loss: 11084.17, base loss: 19709.63
[INFO 2017-06-27 22:17:16,187 main.py:51] epoch 4319, training loss: 10266.39, average training loss: 11083.27, base loss: 19707.02
[INFO 2017-06-27 22:17:17,163 main.py:51] epoch 4320, training loss: 12050.90, average training loss: 11085.24, base loss: 19713.12
[INFO 2017-06-27 22:17:18,138 main.py:51] epoch 4321, training loss: 11631.38, average training loss: 11086.57, base loss: 19717.36
[INFO 2017-06-27 22:17:19,110 main.py:51] epoch 4322, training loss: 11433.03, average training loss: 11086.32, base loss: 19715.87
[INFO 2017-06-27 22:17:20,082 main.py:51] epoch 4323, training loss: 10569.53, average training loss: 11086.49, base loss: 19716.54
[INFO 2017-06-27 22:17:21,054 main.py:51] epoch 4324, training loss: 10184.23, average training loss: 11085.79, base loss: 19714.15
[INFO 2017-06-27 22:17:22,026 main.py:51] epoch 4325, training loss: 10422.27, average training loss: 11083.95, base loss: 19711.59
[INFO 2017-06-27 22:17:22,996 main.py:51] epoch 4326, training loss: 9964.25, average training loss: 11083.53, base loss: 19711.93
[INFO 2017-06-27 22:17:23,970 main.py:51] epoch 4327, training loss: 10360.87, average training loss: 11082.34, base loss: 19708.41
[INFO 2017-06-27 22:17:24,940 main.py:51] epoch 4328, training loss: 9978.36, average training loss: 11081.56, base loss: 19708.75
[INFO 2017-06-27 22:17:25,913 main.py:51] epoch 4329, training loss: 9285.11, average training loss: 11079.95, base loss: 19706.68
[INFO 2017-06-27 22:17:26,886 main.py:51] epoch 4330, training loss: 10352.29, average training loss: 11080.42, base loss: 19708.27
[INFO 2017-06-27 22:17:27,861 main.py:51] epoch 4331, training loss: 10119.68, average training loss: 11080.60, base loss: 19708.81
[INFO 2017-06-27 22:17:28,838 main.py:51] epoch 4332, training loss: 11580.02, average training loss: 11081.37, base loss: 19711.05
[INFO 2017-06-27 22:17:29,814 main.py:51] epoch 4333, training loss: 11679.02, average training loss: 11081.86, base loss: 19712.63
[INFO 2017-06-27 22:17:30,791 main.py:51] epoch 4334, training loss: 12743.67, average training loss: 11082.89, base loss: 19716.45
[INFO 2017-06-27 22:17:31,764 main.py:51] epoch 4335, training loss: 10076.42, average training loss: 11080.52, base loss: 19712.37
[INFO 2017-06-27 22:17:32,737 main.py:51] epoch 4336, training loss: 9044.28, average training loss: 11078.61, base loss: 19705.42
[INFO 2017-06-27 22:17:33,713 main.py:51] epoch 4337, training loss: 9914.20, average training loss: 11077.05, base loss: 19702.88
[INFO 2017-06-27 22:17:34,689 main.py:51] epoch 4338, training loss: 11438.81, average training loss: 11076.99, base loss: 19705.05
[INFO 2017-06-27 22:17:35,667 main.py:51] epoch 4339, training loss: 9849.39, average training loss: 11076.50, base loss: 19704.81
[INFO 2017-06-27 22:17:36,644 main.py:51] epoch 4340, training loss: 10813.00, average training loss: 11075.86, base loss: 19704.25
[INFO 2017-06-27 22:17:37,618 main.py:51] epoch 4341, training loss: 11924.85, average training loss: 11076.84, base loss: 19707.67
[INFO 2017-06-27 22:17:38,591 main.py:51] epoch 4342, training loss: 10629.54, average training loss: 11075.40, base loss: 19704.89
[INFO 2017-06-27 22:17:39,568 main.py:51] epoch 4343, training loss: 11383.99, average training loss: 11075.61, base loss: 19705.79
[INFO 2017-06-27 22:17:40,543 main.py:51] epoch 4344, training loss: 11033.21, average training loss: 11076.69, base loss: 19708.36
[INFO 2017-06-27 22:17:41,514 main.py:51] epoch 4345, training loss: 9018.95, average training loss: 11074.48, base loss: 19705.73
[INFO 2017-06-27 22:17:42,488 main.py:51] epoch 4346, training loss: 12581.32, average training loss: 11077.07, base loss: 19712.91
[INFO 2017-06-27 22:17:43,466 main.py:51] epoch 4347, training loss: 10638.80, average training loss: 11076.49, base loss: 19712.29
[INFO 2017-06-27 22:17:44,438 main.py:51] epoch 4348, training loss: 10836.06, average training loss: 11076.20, base loss: 19712.20
[INFO 2017-06-27 22:17:45,410 main.py:51] epoch 4349, training loss: 11192.43, average training loss: 11074.96, base loss: 19709.26
[INFO 2017-06-27 22:17:46,382 main.py:51] epoch 4350, training loss: 10691.90, average training loss: 11075.10, base loss: 19711.52
[INFO 2017-06-27 22:17:47,360 main.py:51] epoch 4351, training loss: 11719.30, average training loss: 11075.10, base loss: 19712.44
[INFO 2017-06-27 22:17:48,332 main.py:51] epoch 4352, training loss: 11392.69, average training loss: 11074.49, base loss: 19711.92
[INFO 2017-06-27 22:17:49,304 main.py:51] epoch 4353, training loss: 9890.30, average training loss: 11073.26, base loss: 19709.52
[INFO 2017-06-27 22:17:50,272 main.py:51] epoch 4354, training loss: 10851.10, average training loss: 11073.20, base loss: 19711.71
[INFO 2017-06-27 22:17:51,246 main.py:51] epoch 4355, training loss: 11062.42, average training loss: 11073.98, base loss: 19715.57
[INFO 2017-06-27 22:17:52,223 main.py:51] epoch 4356, training loss: 11424.70, average training loss: 11075.38, base loss: 19718.65
[INFO 2017-06-27 22:17:53,198 main.py:51] epoch 4357, training loss: 9984.64, average training loss: 11073.04, base loss: 19714.33
[INFO 2017-06-27 22:17:54,182 main.py:51] epoch 4358, training loss: 10684.09, average training loss: 11072.73, base loss: 19715.47
[INFO 2017-06-27 22:17:55,158 main.py:51] epoch 4359, training loss: 9314.18, average training loss: 11071.43, base loss: 19711.72
[INFO 2017-06-27 22:17:56,133 main.py:51] epoch 4360, training loss: 11801.80, average training loss: 11072.03, base loss: 19714.35
[INFO 2017-06-27 22:17:57,105 main.py:51] epoch 4361, training loss: 10419.98, average training loss: 11071.71, base loss: 19713.83
[INFO 2017-06-27 22:17:58,077 main.py:51] epoch 4362, training loss: 10369.76, average training loss: 11068.60, base loss: 19708.42
[INFO 2017-06-27 22:17:59,050 main.py:51] epoch 4363, training loss: 11171.08, average training loss: 11068.48, base loss: 19710.43
[INFO 2017-06-27 22:18:00,036 main.py:51] epoch 4364, training loss: 10662.02, average training loss: 11067.22, base loss: 19709.23
[INFO 2017-06-27 22:18:01,012 main.py:51] epoch 4365, training loss: 10316.63, average training loss: 11066.00, base loss: 19708.30
[INFO 2017-06-27 22:18:02,122 main.py:51] epoch 4366, training loss: 9820.29, average training loss: 11064.38, base loss: 19706.93
[INFO 2017-06-27 22:18:03,273 main.py:51] epoch 4367, training loss: 12980.28, average training loss: 11065.56, base loss: 19710.76
[INFO 2017-06-27 22:18:04,349 main.py:51] epoch 4368, training loss: 11073.63, average training loss: 11066.53, base loss: 19715.34
[INFO 2017-06-27 22:18:05,458 main.py:51] epoch 4369, training loss: 12745.74, average training loss: 11068.32, base loss: 19719.72
[INFO 2017-06-27 22:18:06,435 main.py:51] epoch 4370, training loss: 13035.78, average training loss: 11071.92, base loss: 19729.69
[INFO 2017-06-27 22:18:07,414 main.py:51] epoch 4371, training loss: 12019.99, average training loss: 11073.72, base loss: 19733.24
[INFO 2017-06-27 22:18:08,386 main.py:51] epoch 4372, training loss: 11083.40, average training loss: 11073.75, base loss: 19732.58
[INFO 2017-06-27 22:18:09,360 main.py:51] epoch 4373, training loss: 10301.71, average training loss: 11072.48, base loss: 19730.59
[INFO 2017-06-27 22:18:10,332 main.py:51] epoch 4374, training loss: 10788.13, average training loss: 11069.33, base loss: 19726.46
[INFO 2017-06-27 22:18:11,308 main.py:51] epoch 4375, training loss: 11940.68, average training loss: 11069.12, base loss: 19727.31
[INFO 2017-06-27 22:18:12,280 main.py:51] epoch 4376, training loss: 11285.12, average training loss: 11069.38, base loss: 19729.21
[INFO 2017-06-27 22:18:13,253 main.py:51] epoch 4377, training loss: 11227.46, average training loss: 11069.06, base loss: 19730.96
[INFO 2017-06-27 22:18:14,224 main.py:51] epoch 4378, training loss: 11686.73, average training loss: 11068.08, base loss: 19732.51
[INFO 2017-06-27 22:18:15,201 main.py:51] epoch 4379, training loss: 10701.01, average training loss: 11067.79, base loss: 19733.67
[INFO 2017-06-27 22:18:16,175 main.py:51] epoch 4380, training loss: 10811.39, average training loss: 11067.22, base loss: 19732.13
[INFO 2017-06-27 22:18:17,147 main.py:51] epoch 4381, training loss: 10980.32, average training loss: 11066.03, base loss: 19730.07
[INFO 2017-06-27 22:18:18,120 main.py:51] epoch 4382, training loss: 11165.18, average training loss: 11066.41, base loss: 19732.84
[INFO 2017-06-27 22:18:19,092 main.py:51] epoch 4383, training loss: 10387.07, average training loss: 11065.23, base loss: 19732.07
[INFO 2017-06-27 22:18:20,062 main.py:51] epoch 4384, training loss: 10810.86, average training loss: 11063.33, base loss: 19729.32
[INFO 2017-06-27 22:18:21,034 main.py:51] epoch 4385, training loss: 11670.40, average training loss: 11064.46, base loss: 19732.58
[INFO 2017-06-27 22:18:22,005 main.py:51] epoch 4386, training loss: 10206.13, average training loss: 11063.81, base loss: 19733.11
[INFO 2017-06-27 22:18:22,979 main.py:51] epoch 4387, training loss: 12385.81, average training loss: 11064.09, base loss: 19736.47
[INFO 2017-06-27 22:18:23,950 main.py:51] epoch 4388, training loss: 11234.92, average training loss: 11064.66, base loss: 19737.23
[INFO 2017-06-27 22:18:24,921 main.py:51] epoch 4389, training loss: 12709.56, average training loss: 11064.83, base loss: 19740.24
[INFO 2017-06-27 22:18:25,897 main.py:51] epoch 4390, training loss: 11615.12, average training loss: 11065.86, base loss: 19741.74
[INFO 2017-06-27 22:18:26,947 main.py:51] epoch 4391, training loss: 9243.28, average training loss: 11064.70, base loss: 19738.54
[INFO 2017-06-27 22:18:27,924 main.py:51] epoch 4392, training loss: 10458.51, average training loss: 11065.93, base loss: 19741.05
[INFO 2017-06-27 22:18:28,897 main.py:51] epoch 4393, training loss: 9215.44, average training loss: 11063.97, base loss: 19736.45
[INFO 2017-06-27 22:18:29,867 main.py:51] epoch 4394, training loss: 11157.57, average training loss: 11063.98, base loss: 19737.82
[INFO 2017-06-27 22:18:30,848 main.py:51] epoch 4395, training loss: 11893.93, average training loss: 11064.86, base loss: 19739.67
[INFO 2017-06-27 22:18:31,822 main.py:51] epoch 4396, training loss: 10629.77, average training loss: 11063.74, base loss: 19737.26
[INFO 2017-06-27 22:18:32,793 main.py:51] epoch 4397, training loss: 10172.95, average training loss: 11060.05, base loss: 19729.94
[INFO 2017-06-27 22:18:33,765 main.py:51] epoch 4398, training loss: 11038.47, average training loss: 11060.02, base loss: 19730.95
[INFO 2017-06-27 22:18:34,736 main.py:51] epoch 4399, training loss: 10282.60, average training loss: 11058.70, base loss: 19728.35
[INFO 2017-06-27 22:18:34,736 main.py:53] epoch 4399, testing
[INFO 2017-06-27 22:18:38,952 main.py:105] average testing loss: 10777.90, base loss: 19610.33
[INFO 2017-06-27 22:18:38,952 main.py:106] improve_loss: 8832.43, improve_percent: 0.45
[INFO 2017-06-27 22:18:38,952 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:18:39,923 main.py:51] epoch 4400, training loss: 10348.30, average training loss: 11057.93, base loss: 19727.15
[INFO 2017-06-27 22:18:40,897 main.py:51] epoch 4401, training loss: 11117.40, average training loss: 11058.07, base loss: 19728.32
[INFO 2017-06-27 22:18:41,869 main.py:51] epoch 4402, training loss: 12186.59, average training loss: 11058.76, base loss: 19730.49
[INFO 2017-06-27 22:18:42,840 main.py:51] epoch 4403, training loss: 11108.21, average training loss: 11058.82, base loss: 19731.38
[INFO 2017-06-27 22:18:43,810 main.py:51] epoch 4404, training loss: 10744.44, average training loss: 11058.82, base loss: 19731.69
[INFO 2017-06-27 22:18:44,780 main.py:51] epoch 4405, training loss: 13051.89, average training loss: 11059.12, base loss: 19732.72
[INFO 2017-06-27 22:18:45,751 main.py:51] epoch 4406, training loss: 10949.66, average training loss: 11059.97, base loss: 19734.69
[INFO 2017-06-27 22:18:46,723 main.py:51] epoch 4407, training loss: 11499.51, average training loss: 11060.62, base loss: 19737.99
[INFO 2017-06-27 22:18:47,695 main.py:51] epoch 4408, training loss: 10762.21, average training loss: 11059.80, base loss: 19735.74
[INFO 2017-06-27 22:18:48,667 main.py:51] epoch 4409, training loss: 12025.94, average training loss: 11061.00, base loss: 19739.79
[INFO 2017-06-27 22:18:49,635 main.py:51] epoch 4410, training loss: 10965.06, average training loss: 11060.74, base loss: 19739.65
[INFO 2017-06-27 22:18:50,608 main.py:51] epoch 4411, training loss: 11129.95, average training loss: 11057.59, base loss: 19735.75
[INFO 2017-06-27 22:18:51,583 main.py:51] epoch 4412, training loss: 10816.47, average training loss: 11057.84, base loss: 19738.13
[INFO 2017-06-27 22:18:52,558 main.py:51] epoch 4413, training loss: 10798.56, average training loss: 11059.00, base loss: 19741.85
[INFO 2017-06-27 22:18:53,635 main.py:51] epoch 4414, training loss: 10631.23, average training loss: 11059.04, base loss: 19742.00
[INFO 2017-06-27 22:18:54,634 main.py:51] epoch 4415, training loss: 11317.02, average training loss: 11058.97, base loss: 19742.52
[INFO 2017-06-27 22:18:55,624 main.py:51] epoch 4416, training loss: 10869.96, average training loss: 11059.63, base loss: 19744.90
[INFO 2017-06-27 22:18:56,597 main.py:51] epoch 4417, training loss: 10979.86, average training loss: 11060.84, base loss: 19747.97
[INFO 2017-06-27 22:18:57,570 main.py:51] epoch 4418, training loss: 11387.19, average training loss: 11061.74, base loss: 19750.59
[INFO 2017-06-27 22:18:58,542 main.py:51] epoch 4419, training loss: 11016.46, average training loss: 11061.69, base loss: 19751.79
[INFO 2017-06-27 22:18:59,518 main.py:51] epoch 4420, training loss: 9432.32, average training loss: 11059.14, base loss: 19746.06
[INFO 2017-06-27 22:19:00,491 main.py:51] epoch 4421, training loss: 12048.00, average training loss: 11059.00, base loss: 19746.24
[INFO 2017-06-27 22:19:01,465 main.py:51] epoch 4422, training loss: 12056.16, average training loss: 11057.58, base loss: 19745.08
[INFO 2017-06-27 22:19:02,440 main.py:51] epoch 4423, training loss: 10770.10, average training loss: 11056.68, base loss: 19743.09
[INFO 2017-06-27 22:19:03,415 main.py:51] epoch 4424, training loss: 10479.50, average training loss: 11055.62, base loss: 19741.69
[INFO 2017-06-27 22:19:04,386 main.py:51] epoch 4425, training loss: 10385.38, average training loss: 11054.32, base loss: 19739.44
[INFO 2017-06-27 22:19:05,359 main.py:51] epoch 4426, training loss: 11341.39, average training loss: 11054.67, base loss: 19740.71
[INFO 2017-06-27 22:19:06,334 main.py:51] epoch 4427, training loss: 9591.45, average training loss: 11052.45, base loss: 19735.51
[INFO 2017-06-27 22:19:07,309 main.py:51] epoch 4428, training loss: 11253.98, average training loss: 11052.47, base loss: 19734.60
[INFO 2017-06-27 22:19:08,290 main.py:51] epoch 4429, training loss: 9664.22, average training loss: 11051.38, base loss: 19732.81
[INFO 2017-06-27 22:19:09,395 main.py:51] epoch 4430, training loss: 9902.39, average training loss: 11051.13, base loss: 19732.16
[INFO 2017-06-27 22:19:10,401 main.py:51] epoch 4431, training loss: 11976.65, average training loss: 11051.15, base loss: 19730.80
[INFO 2017-06-27 22:19:11,391 main.py:51] epoch 4432, training loss: 10573.09, average training loss: 11051.55, base loss: 19731.57
[INFO 2017-06-27 22:19:12,363 main.py:51] epoch 4433, training loss: 9303.91, average training loss: 11049.50, base loss: 19727.65
[INFO 2017-06-27 22:19:13,334 main.py:51] epoch 4434, training loss: 11110.22, average training loss: 11049.66, base loss: 19728.53
[INFO 2017-06-27 22:19:14,316 main.py:51] epoch 4435, training loss: 9310.84, average training loss: 11045.83, base loss: 19721.11
[INFO 2017-06-27 22:19:15,400 main.py:51] epoch 4436, training loss: 9618.78, average training loss: 11043.28, base loss: 19715.78
[INFO 2017-06-27 22:19:16,381 main.py:51] epoch 4437, training loss: 10085.28, average training loss: 11042.23, base loss: 19715.53
[INFO 2017-06-27 22:19:17,364 main.py:51] epoch 4438, training loss: 10412.56, average training loss: 11041.83, base loss: 19715.92
[INFO 2017-06-27 22:19:18,336 main.py:51] epoch 4439, training loss: 9464.14, average training loss: 11039.67, base loss: 19712.11
[INFO 2017-06-27 22:19:19,307 main.py:51] epoch 4440, training loss: 11534.32, average training loss: 11040.11, base loss: 19712.95
[INFO 2017-06-27 22:19:20,275 main.py:51] epoch 4441, training loss: 9688.24, average training loss: 11039.89, base loss: 19713.46
[INFO 2017-06-27 22:19:21,254 main.py:51] epoch 4442, training loss: 11283.94, average training loss: 11039.83, base loss: 19713.69
[INFO 2017-06-27 22:19:22,343 main.py:51] epoch 4443, training loss: 9682.33, average training loss: 11038.19, base loss: 19710.92
[INFO 2017-06-27 22:19:23,402 main.py:51] epoch 4444, training loss: 10487.49, average training loss: 11038.27, base loss: 19710.47
[INFO 2017-06-27 22:19:24,412 main.py:51] epoch 4445, training loss: 12018.25, average training loss: 11037.39, base loss: 19708.52
[INFO 2017-06-27 22:19:25,398 main.py:51] epoch 4446, training loss: 11554.30, average training loss: 11037.35, base loss: 19709.06
[INFO 2017-06-27 22:19:26,367 main.py:51] epoch 4447, training loss: 10596.90, average training loss: 11037.31, base loss: 19710.46
[INFO 2017-06-27 22:19:27,339 main.py:51] epoch 4448, training loss: 11798.75, average training loss: 11038.47, base loss: 19713.55
[INFO 2017-06-27 22:19:28,314 main.py:51] epoch 4449, training loss: 11025.21, average training loss: 11036.52, base loss: 19709.78
[INFO 2017-06-27 22:19:29,391 main.py:51] epoch 4450, training loss: 11346.07, average training loss: 11036.38, base loss: 19709.25
[INFO 2017-06-27 22:19:30,388 main.py:51] epoch 4451, training loss: 10698.25, average training loss: 11036.40, base loss: 19707.07
[INFO 2017-06-27 22:19:31,366 main.py:51] epoch 4452, training loss: 10081.70, average training loss: 11035.36, base loss: 19703.90
[INFO 2017-06-27 22:19:32,337 main.py:51] epoch 4453, training loss: 11251.83, average training loss: 11035.70, base loss: 19705.19
[INFO 2017-06-27 22:19:33,309 main.py:51] epoch 4454, training loss: 10207.45, average training loss: 11035.57, base loss: 19705.29
[INFO 2017-06-27 22:19:34,280 main.py:51] epoch 4455, training loss: 10645.59, average training loss: 11033.62, base loss: 19701.35
[INFO 2017-06-27 22:19:35,250 main.py:51] epoch 4456, training loss: 10452.53, average training loss: 11033.48, base loss: 19701.82
[INFO 2017-06-27 22:19:36,225 main.py:51] epoch 4457, training loss: 10580.09, average training loss: 11033.22, base loss: 19704.23
[INFO 2017-06-27 22:19:37,198 main.py:51] epoch 4458, training loss: 11189.59, average training loss: 11033.26, base loss: 19704.56
[INFO 2017-06-27 22:19:38,173 main.py:51] epoch 4459, training loss: 10128.02, average training loss: 11032.25, base loss: 19703.64
[INFO 2017-06-27 22:19:39,145 main.py:51] epoch 4460, training loss: 11605.80, average training loss: 11030.57, base loss: 19700.74
[INFO 2017-06-27 22:19:40,116 main.py:51] epoch 4461, training loss: 9719.48, average training loss: 11028.72, base loss: 19697.50
[INFO 2017-06-27 22:19:41,088 main.py:51] epoch 4462, training loss: 9469.29, average training loss: 11026.20, base loss: 19692.96
[INFO 2017-06-27 22:19:42,060 main.py:51] epoch 4463, training loss: 11092.39, average training loss: 11025.67, base loss: 19692.82
[INFO 2017-06-27 22:19:43,031 main.py:51] epoch 4464, training loss: 9841.66, average training loss: 11024.61, base loss: 19689.49
[INFO 2017-06-27 22:19:44,002 main.py:51] epoch 4465, training loss: 10380.42, average training loss: 11023.21, base loss: 19687.82
[INFO 2017-06-27 22:19:44,979 main.py:51] epoch 4466, training loss: 9661.75, average training loss: 11022.48, base loss: 19686.09
[INFO 2017-06-27 22:19:45,954 main.py:51] epoch 4467, training loss: 11989.38, average training loss: 11023.42, base loss: 19691.86
[INFO 2017-06-27 22:19:46,926 main.py:51] epoch 4468, training loss: 10625.27, average training loss: 11024.04, base loss: 19693.55
[INFO 2017-06-27 22:19:47,900 main.py:51] epoch 4469, training loss: 10309.96, average training loss: 11022.16, base loss: 19689.04
[INFO 2017-06-27 22:19:48,977 main.py:51] epoch 4470, training loss: 11795.20, average training loss: 11021.68, base loss: 19688.29
[INFO 2017-06-27 22:19:49,981 main.py:51] epoch 4471, training loss: 10663.30, average training loss: 11021.04, base loss: 19688.54
[INFO 2017-06-27 22:19:50,963 main.py:51] epoch 4472, training loss: 12215.20, average training loss: 11021.64, base loss: 19691.27
[INFO 2017-06-27 22:19:51,936 main.py:51] epoch 4473, training loss: 10358.91, average training loss: 11021.32, base loss: 19690.70
[INFO 2017-06-27 22:19:52,905 main.py:51] epoch 4474, training loss: 10150.79, average training loss: 11020.78, base loss: 19690.53
[INFO 2017-06-27 22:19:53,877 main.py:51] epoch 4475, training loss: 10884.52, average training loss: 11019.58, base loss: 19687.07
[INFO 2017-06-27 22:19:54,849 main.py:51] epoch 4476, training loss: 10654.15, average training loss: 11020.20, base loss: 19688.93
[INFO 2017-06-27 22:19:55,821 main.py:51] epoch 4477, training loss: 9576.66, average training loss: 11017.31, base loss: 19683.63
[INFO 2017-06-27 22:19:56,790 main.py:51] epoch 4478, training loss: 9425.87, average training loss: 11014.98, base loss: 19678.63
[INFO 2017-06-27 22:19:57,760 main.py:51] epoch 4479, training loss: 10973.34, average training loss: 11015.71, base loss: 19680.64
[INFO 2017-06-27 22:19:58,732 main.py:51] epoch 4480, training loss: 12794.29, average training loss: 11017.87, base loss: 19684.89
[INFO 2017-06-27 22:19:59,704 main.py:51] epoch 4481, training loss: 11218.49, average training loss: 11019.39, base loss: 19688.43
[INFO 2017-06-27 22:20:00,675 main.py:51] epoch 4482, training loss: 11180.52, average training loss: 11019.76, base loss: 19689.64
[INFO 2017-06-27 22:20:01,647 main.py:51] epoch 4483, training loss: 10683.44, average training loss: 11018.39, base loss: 19688.12
[INFO 2017-06-27 22:20:02,621 main.py:51] epoch 4484, training loss: 12466.41, average training loss: 11019.59, base loss: 19691.11
[INFO 2017-06-27 22:20:03,594 main.py:51] epoch 4485, training loss: 10195.98, average training loss: 11018.44, base loss: 19689.46
[INFO 2017-06-27 22:20:04,573 main.py:51] epoch 4486, training loss: 12913.99, average training loss: 11019.60, base loss: 19692.96
[INFO 2017-06-27 22:20:05,551 main.py:51] epoch 4487, training loss: 11831.25, average training loss: 11021.22, base loss: 19697.13
[INFO 2017-06-27 22:20:06,525 main.py:51] epoch 4488, training loss: 10575.67, average training loss: 11020.87, base loss: 19696.33
[INFO 2017-06-27 22:20:07,497 main.py:51] epoch 4489, training loss: 11951.63, average training loss: 11022.90, base loss: 19701.83
[INFO 2017-06-27 22:20:08,469 main.py:51] epoch 4490, training loss: 11585.08, average training loss: 11024.23, base loss: 19705.03
[INFO 2017-06-27 22:20:09,442 main.py:51] epoch 4491, training loss: 10792.12, average training loss: 11024.23, base loss: 19705.43
[INFO 2017-06-27 22:20:10,421 main.py:51] epoch 4492, training loss: 10919.94, average training loss: 11022.72, base loss: 19704.18
[INFO 2017-06-27 22:20:11,394 main.py:51] epoch 4493, training loss: 11320.06, average training loss: 11022.93, base loss: 19705.43
[INFO 2017-06-27 22:20:12,371 main.py:51] epoch 4494, training loss: 11538.89, average training loss: 11024.80, base loss: 19711.54
[INFO 2017-06-27 22:20:13,348 main.py:51] epoch 4495, training loss: 10195.84, average training loss: 11024.40, base loss: 19712.24
[INFO 2017-06-27 22:20:14,322 main.py:51] epoch 4496, training loss: 9579.43, average training loss: 11023.36, base loss: 19711.05
[INFO 2017-06-27 22:20:15,299 main.py:51] epoch 4497, training loss: 10417.14, average training loss: 11023.13, base loss: 19711.59
[INFO 2017-06-27 22:20:16,391 main.py:51] epoch 4498, training loss: 11732.96, average training loss: 11024.62, base loss: 19714.68
[INFO 2017-06-27 22:20:17,380 main.py:51] epoch 4499, training loss: 11891.01, average training loss: 11026.76, base loss: 19720.27
[INFO 2017-06-27 22:20:17,380 main.py:53] epoch 4499, testing
[INFO 2017-06-27 22:20:21,621 main.py:105] average testing loss: 10848.89, base loss: 19715.79
[INFO 2017-06-27 22:20:21,621 main.py:106] improve_loss: 8866.90, improve_percent: 0.45
[INFO 2017-06-27 22:20:21,622 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:20:22,590 main.py:51] epoch 4500, training loss: 10242.80, average training loss: 11026.13, base loss: 19720.04
[INFO 2017-06-27 22:20:23,568 main.py:51] epoch 4501, training loss: 11061.12, average training loss: 11025.40, base loss: 19718.18
[INFO 2017-06-27 22:20:24,543 main.py:51] epoch 4502, training loss: 10871.15, average training loss: 11023.28, base loss: 19712.89
[INFO 2017-06-27 22:20:25,523 main.py:51] epoch 4503, training loss: 12442.62, average training loss: 11025.27, base loss: 19717.59
[INFO 2017-06-27 22:20:26,494 main.py:51] epoch 4504, training loss: 11012.72, average training loss: 11025.87, base loss: 19721.47
[INFO 2017-06-27 22:20:27,471 main.py:51] epoch 4505, training loss: 11380.49, average training loss: 11023.86, base loss: 19717.76
[INFO 2017-06-27 22:20:28,443 main.py:51] epoch 4506, training loss: 9702.62, average training loss: 11022.06, base loss: 19714.77
[INFO 2017-06-27 22:20:29,415 main.py:51] epoch 4507, training loss: 12436.10, average training loss: 11023.16, base loss: 19719.15
[INFO 2017-06-27 22:20:30,388 main.py:51] epoch 4508, training loss: 10443.83, average training loss: 11023.06, base loss: 19719.00
[INFO 2017-06-27 22:20:31,361 main.py:51] epoch 4509, training loss: 10655.84, average training loss: 11023.53, base loss: 19720.97
[INFO 2017-06-27 22:20:32,334 main.py:51] epoch 4510, training loss: 11214.48, average training loss: 11023.14, base loss: 19721.06
[INFO 2017-06-27 22:20:33,306 main.py:51] epoch 4511, training loss: 8865.92, average training loss: 11020.74, base loss: 19716.49
[INFO 2017-06-27 22:20:34,279 main.py:51] epoch 4512, training loss: 9620.39, average training loss: 11020.17, base loss: 19716.01
[INFO 2017-06-27 22:20:35,251 main.py:51] epoch 4513, training loss: 11290.82, average training loss: 11018.66, base loss: 19714.62
[INFO 2017-06-27 22:20:36,226 main.py:51] epoch 4514, training loss: 10221.46, average training loss: 11016.60, base loss: 19710.87
[INFO 2017-06-27 22:20:37,248 main.py:51] epoch 4515, training loss: 11951.80, average training loss: 11018.05, base loss: 19715.19
[INFO 2017-06-27 22:20:38,267 main.py:51] epoch 4516, training loss: 10417.10, average training loss: 11017.70, base loss: 19715.78
[INFO 2017-06-27 22:20:39,245 main.py:51] epoch 4517, training loss: 11080.29, average training loss: 11016.66, base loss: 19713.52
[INFO 2017-06-27 22:20:40,219 main.py:51] epoch 4518, training loss: 11129.83, average training loss: 11016.50, base loss: 19713.53
[INFO 2017-06-27 22:20:41,193 main.py:51] epoch 4519, training loss: 11431.68, average training loss: 11017.32, base loss: 19715.40
[INFO 2017-06-27 22:20:42,235 main.py:51] epoch 4520, training loss: 11372.90, average training loss: 11018.26, base loss: 19717.17
[INFO 2017-06-27 22:20:43,209 main.py:51] epoch 4521, training loss: 10758.42, average training loss: 11018.03, base loss: 19718.91
[INFO 2017-06-27 22:20:44,182 main.py:51] epoch 4522, training loss: 10377.40, average training loss: 11016.52, base loss: 19716.99
[INFO 2017-06-27 22:20:45,172 main.py:51] epoch 4523, training loss: 10841.17, average training loss: 11016.56, base loss: 19715.41
[INFO 2017-06-27 22:20:46,144 main.py:51] epoch 4524, training loss: 10677.59, average training loss: 11016.85, base loss: 19716.28
[INFO 2017-06-27 22:20:47,115 main.py:51] epoch 4525, training loss: 9168.92, average training loss: 11015.08, base loss: 19711.99
[INFO 2017-06-27 22:20:48,086 main.py:51] epoch 4526, training loss: 9863.54, average training loss: 11013.40, base loss: 19709.02
[INFO 2017-06-27 22:20:49,066 main.py:51] epoch 4527, training loss: 10458.75, average training loss: 11014.01, base loss: 19711.42
[INFO 2017-06-27 22:20:50,144 main.py:51] epoch 4528, training loss: 10342.91, average training loss: 11013.82, base loss: 19710.41
[INFO 2017-06-27 22:20:51,117 main.py:51] epoch 4529, training loss: 10646.78, average training loss: 11012.97, base loss: 19710.10
[INFO 2017-06-27 22:20:52,089 main.py:51] epoch 4530, training loss: 10675.09, average training loss: 11013.89, base loss: 19715.30
[INFO 2017-06-27 22:20:53,158 main.py:51] epoch 4531, training loss: 11453.18, average training loss: 11015.72, base loss: 19720.37
[INFO 2017-06-27 22:20:54,249 main.py:51] epoch 4532, training loss: 10067.92, average training loss: 11014.10, base loss: 19716.95
[INFO 2017-06-27 22:20:55,321 main.py:51] epoch 4533, training loss: 12444.69, average training loss: 11016.28, base loss: 19723.54
[INFO 2017-06-27 22:20:56,313 main.py:51] epoch 4534, training loss: 10491.06, average training loss: 11015.96, base loss: 19723.62
[INFO 2017-06-27 22:20:57,287 main.py:51] epoch 4535, training loss: 9673.03, average training loss: 11013.70, base loss: 19720.33
[INFO 2017-06-27 22:20:58,259 main.py:51] epoch 4536, training loss: 11965.85, average training loss: 11014.18, base loss: 19723.40
[INFO 2017-06-27 22:20:59,346 main.py:51] epoch 4537, training loss: 10599.01, average training loss: 11013.67, base loss: 19724.10
[INFO 2017-06-27 22:21:00,407 main.py:51] epoch 4538, training loss: 10563.95, average training loss: 11012.87, base loss: 19721.79
[INFO 2017-06-27 22:21:01,404 main.py:51] epoch 4539, training loss: 11914.33, average training loss: 11014.12, base loss: 19724.19
[INFO 2017-06-27 22:21:02,485 main.py:51] epoch 4540, training loss: 10502.76, average training loss: 11014.21, base loss: 19724.75
[INFO 2017-06-27 22:21:03,461 main.py:51] epoch 4541, training loss: 9678.74, average training loss: 11012.79, base loss: 19720.33
[INFO 2017-06-27 22:21:04,436 main.py:51] epoch 4542, training loss: 10634.52, average training loss: 11012.39, base loss: 19719.06
[INFO 2017-06-27 22:21:05,462 main.py:51] epoch 4543, training loss: 10717.51, average training loss: 11012.56, base loss: 19721.68
[INFO 2017-06-27 22:21:06,489 main.py:51] epoch 4544, training loss: 10064.94, average training loss: 11011.29, base loss: 19718.53
[INFO 2017-06-27 22:21:07,583 main.py:51] epoch 4545, training loss: 11376.06, average training loss: 11010.80, base loss: 19719.42
[INFO 2017-06-27 22:21:08,657 main.py:51] epoch 4546, training loss: 11750.24, average training loss: 11012.69, base loss: 19725.06
[INFO 2017-06-27 22:21:09,742 main.py:51] epoch 4547, training loss: 10895.97, average training loss: 11013.09, base loss: 19726.18
[INFO 2017-06-27 22:21:10,759 main.py:51] epoch 4548, training loss: 10740.26, average training loss: 11013.47, base loss: 19728.13
[INFO 2017-06-27 22:21:11,837 main.py:51] epoch 4549, training loss: 10956.34, average training loss: 11013.17, base loss: 19729.04
[INFO 2017-06-27 22:21:12,814 main.py:51] epoch 4550, training loss: 9627.60, average training loss: 11011.21, base loss: 19724.80
[INFO 2017-06-27 22:21:13,913 main.py:51] epoch 4551, training loss: 10946.85, average training loss: 11011.06, base loss: 19723.78
[INFO 2017-06-27 22:21:14,890 main.py:51] epoch 4552, training loss: 8625.79, average training loss: 11006.44, base loss: 19714.89
[INFO 2017-06-27 22:21:15,861 main.py:51] epoch 4553, training loss: 10228.37, average training loss: 11005.53, base loss: 19715.36
[INFO 2017-06-27 22:21:16,836 main.py:51] epoch 4554, training loss: 10436.13, average training loss: 11005.80, base loss: 19716.46
[INFO 2017-06-27 22:21:17,807 main.py:51] epoch 4555, training loss: 11282.82, average training loss: 11005.58, base loss: 19715.86
[INFO 2017-06-27 22:21:18,778 main.py:51] epoch 4556, training loss: 11722.43, average training loss: 11005.19, base loss: 19714.87
[INFO 2017-06-27 22:21:19,755 main.py:51] epoch 4557, training loss: 10974.95, average training loss: 11005.74, base loss: 19717.78
[INFO 2017-06-27 22:21:20,726 main.py:51] epoch 4558, training loss: 11063.89, average training loss: 11004.34, base loss: 19715.95
[INFO 2017-06-27 22:21:21,699 main.py:51] epoch 4559, training loss: 10149.06, average training loss: 11003.63, base loss: 19716.53
[INFO 2017-06-27 22:21:22,670 main.py:51] epoch 4560, training loss: 11353.56, average training loss: 11002.96, base loss: 19714.32
[INFO 2017-06-27 22:21:23,640 main.py:51] epoch 4561, training loss: 11621.29, average training loss: 11001.90, base loss: 19713.34
[INFO 2017-06-27 22:21:24,614 main.py:51] epoch 4562, training loss: 10216.63, average training loss: 11000.16, base loss: 19709.28
[INFO 2017-06-27 22:21:25,585 main.py:51] epoch 4563, training loss: 12621.38, average training loss: 11001.96, base loss: 19714.26
[INFO 2017-06-27 22:21:26,557 main.py:51] epoch 4564, training loss: 11352.82, average training loss: 11001.66, base loss: 19713.86
[INFO 2017-06-27 22:21:27,529 main.py:51] epoch 4565, training loss: 12412.70, average training loss: 11002.50, base loss: 19717.81
[INFO 2017-06-27 22:21:28,504 main.py:51] epoch 4566, training loss: 9631.08, average training loss: 11000.56, base loss: 19712.94
[INFO 2017-06-27 22:21:29,476 main.py:51] epoch 4567, training loss: 10894.24, average training loss: 11000.10, base loss: 19712.28
[INFO 2017-06-27 22:21:30,447 main.py:51] epoch 4568, training loss: 10026.06, average training loss: 10997.37, base loss: 19707.19
[INFO 2017-06-27 22:21:31,422 main.py:51] epoch 4569, training loss: 11168.74, average training loss: 10996.90, base loss: 19707.37
[INFO 2017-06-27 22:21:32,401 main.py:51] epoch 4570, training loss: 10024.08, average training loss: 10995.36, base loss: 19703.07
[INFO 2017-06-27 22:21:33,374 main.py:51] epoch 4571, training loss: 11254.22, average training loss: 10995.37, base loss: 19703.09
[INFO 2017-06-27 22:21:34,345 main.py:51] epoch 4572, training loss: 12113.44, average training loss: 10996.28, base loss: 19707.18
[INFO 2017-06-27 22:21:35,430 main.py:51] epoch 4573, training loss: 11315.67, average training loss: 10996.66, base loss: 19708.99
[INFO 2017-06-27 22:21:36,494 main.py:51] epoch 4574, training loss: 10933.10, average training loss: 10994.46, base loss: 19704.39
[INFO 2017-06-27 22:21:37,496 main.py:51] epoch 4575, training loss: 10130.98, average training loss: 10993.30, base loss: 19703.38
[INFO 2017-06-27 22:21:38,469 main.py:51] epoch 4576, training loss: 11212.59, average training loss: 10993.01, base loss: 19702.72
[INFO 2017-06-27 22:21:39,526 main.py:51] epoch 4577, training loss: 10350.06, average training loss: 10992.68, base loss: 19703.82
[INFO 2017-06-27 22:21:40,551 main.py:51] epoch 4578, training loss: 11513.29, average training loss: 10990.50, base loss: 19699.20
[INFO 2017-06-27 22:21:41,573 main.py:51] epoch 4579, training loss: 12369.34, average training loss: 10992.33, base loss: 19703.29
[INFO 2017-06-27 22:21:42,592 main.py:51] epoch 4580, training loss: 11476.43, average training loss: 10992.43, base loss: 19704.00
[INFO 2017-06-27 22:21:43,587 main.py:51] epoch 4581, training loss: 11789.61, average training loss: 10992.82, base loss: 19706.26
[INFO 2017-06-27 22:21:44,563 main.py:51] epoch 4582, training loss: 10883.25, average training loss: 10992.92, base loss: 19709.03
[INFO 2017-06-27 22:21:45,532 main.py:51] epoch 4583, training loss: 11403.38, average training loss: 10992.04, base loss: 19707.32
[INFO 2017-06-27 22:21:46,511 main.py:51] epoch 4584, training loss: 10894.48, average training loss: 10991.49, base loss: 19705.33
[INFO 2017-06-27 22:21:47,483 main.py:51] epoch 4585, training loss: 9605.20, average training loss: 10990.49, base loss: 19702.53
[INFO 2017-06-27 22:21:48,460 main.py:51] epoch 4586, training loss: 11134.31, average training loss: 10989.99, base loss: 19701.92
[INFO 2017-06-27 22:21:49,436 main.py:51] epoch 4587, training loss: 9563.47, average training loss: 10988.40, base loss: 19698.24
[INFO 2017-06-27 22:21:50,409 main.py:51] epoch 4588, training loss: 10827.74, average training loss: 10988.04, base loss: 19698.13
[INFO 2017-06-27 22:21:51,381 main.py:51] epoch 4589, training loss: 10467.54, average training loss: 10987.85, base loss: 19698.59
[INFO 2017-06-27 22:21:52,354 main.py:51] epoch 4590, training loss: 9968.50, average training loss: 10985.68, base loss: 19694.58
[INFO 2017-06-27 22:21:53,328 main.py:51] epoch 4591, training loss: 10288.76, average training loss: 10984.68, base loss: 19692.62
[INFO 2017-06-27 22:21:54,303 main.py:51] epoch 4592, training loss: 10518.40, average training loss: 10983.74, base loss: 19691.35
[INFO 2017-06-27 22:21:55,274 main.py:51] epoch 4593, training loss: 10127.70, average training loss: 10983.06, base loss: 19690.58
[INFO 2017-06-27 22:21:56,248 main.py:51] epoch 4594, training loss: 10027.95, average training loss: 10981.40, base loss: 19685.19
[INFO 2017-06-27 22:21:57,225 main.py:51] epoch 4595, training loss: 12597.38, average training loss: 10982.31, base loss: 19687.13
[INFO 2017-06-27 22:21:58,198 main.py:51] epoch 4596, training loss: 9523.50, average training loss: 10980.30, base loss: 19681.86
[INFO 2017-06-27 22:21:59,168 main.py:51] epoch 4597, training loss: 9795.43, average training loss: 10977.78, base loss: 19675.89
[INFO 2017-06-27 22:22:00,139 main.py:51] epoch 4598, training loss: 9795.57, average training loss: 10976.21, base loss: 19672.68
[INFO 2017-06-27 22:22:01,110 main.py:51] epoch 4599, training loss: 10686.27, average training loss: 10976.86, base loss: 19676.13
[INFO 2017-06-27 22:22:01,110 main.py:53] epoch 4599, testing
[INFO 2017-06-27 22:22:05,329 main.py:105] average testing loss: 10577.92, base loss: 19033.40
[INFO 2017-06-27 22:22:05,329 main.py:106] improve_loss: 8455.48, improve_percent: 0.44
[INFO 2017-06-27 22:22:05,329 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:22:06,307 main.py:51] epoch 4600, training loss: 10547.64, average training loss: 10976.79, base loss: 19676.41
[INFO 2017-06-27 22:22:07,279 main.py:51] epoch 4601, training loss: 11298.88, average training loss: 10977.75, base loss: 19679.55
[INFO 2017-06-27 22:22:08,256 main.py:51] epoch 4602, training loss: 9870.25, average training loss: 10976.60, base loss: 19677.46
[INFO 2017-06-27 22:22:09,227 main.py:51] epoch 4603, training loss: 11529.52, average training loss: 10977.01, base loss: 19677.74
[INFO 2017-06-27 22:22:10,200 main.py:51] epoch 4604, training loss: 10179.43, average training loss: 10975.21, base loss: 19674.03
[INFO 2017-06-27 22:22:11,171 main.py:51] epoch 4605, training loss: 10641.50, average training loss: 10976.63, base loss: 19677.31
[INFO 2017-06-27 22:22:12,144 main.py:51] epoch 4606, training loss: 10611.12, average training loss: 10977.19, base loss: 19678.15
[INFO 2017-06-27 22:22:13,115 main.py:51] epoch 4607, training loss: 10780.73, average training loss: 10976.05, base loss: 19675.41
[INFO 2017-06-27 22:22:14,085 main.py:51] epoch 4608, training loss: 11215.30, average training loss: 10976.38, base loss: 19676.84
[INFO 2017-06-27 22:22:15,056 main.py:51] epoch 4609, training loss: 10209.76, average training loss: 10974.43, base loss: 19672.89
[INFO 2017-06-27 22:22:16,027 main.py:51] epoch 4610, training loss: 12342.04, average training loss: 10975.44, base loss: 19676.23
[INFO 2017-06-27 22:22:17,004 main.py:51] epoch 4611, training loss: 12021.34, average training loss: 10976.89, base loss: 19678.41
[INFO 2017-06-27 22:22:17,977 main.py:51] epoch 4612, training loss: 12804.38, average training loss: 10977.64, base loss: 19680.37
[INFO 2017-06-27 22:22:18,947 main.py:51] epoch 4613, training loss: 10593.72, average training loss: 10976.48, base loss: 19677.68
[INFO 2017-06-27 22:22:19,920 main.py:51] epoch 4614, training loss: 10459.83, average training loss: 10976.21, base loss: 19676.08
[INFO 2017-06-27 22:22:20,893 main.py:51] epoch 4615, training loss: 11866.00, average training loss: 10976.57, base loss: 19677.37
[INFO 2017-06-27 22:22:21,865 main.py:51] epoch 4616, training loss: 10085.90, average training loss: 10976.01, base loss: 19677.26
[INFO 2017-06-27 22:22:22,839 main.py:51] epoch 4617, training loss: 11856.28, average training loss: 10977.43, base loss: 19680.06
[INFO 2017-06-27 22:22:23,819 main.py:51] epoch 4618, training loss: 9777.43, average training loss: 10975.33, base loss: 19675.84
[INFO 2017-06-27 22:22:24,791 main.py:51] epoch 4619, training loss: 11926.40, average training loss: 10975.33, base loss: 19676.78
[INFO 2017-06-27 22:22:25,761 main.py:51] epoch 4620, training loss: 11083.16, average training loss: 10974.03, base loss: 19675.46
[INFO 2017-06-27 22:22:26,733 main.py:51] epoch 4621, training loss: 11293.07, average training loss: 10972.10, base loss: 19671.44
[INFO 2017-06-27 22:22:27,706 main.py:51] epoch 4622, training loss: 12301.42, average training loss: 10972.75, base loss: 19674.73
[INFO 2017-06-27 22:22:28,677 main.py:51] epoch 4623, training loss: 9596.27, average training loss: 10971.57, base loss: 19673.25
[INFO 2017-06-27 22:22:29,648 main.py:51] epoch 4624, training loss: 11221.85, average training loss: 10970.51, base loss: 19671.62
[INFO 2017-06-27 22:22:30,623 main.py:51] epoch 4625, training loss: 9592.11, average training loss: 10968.01, base loss: 19664.92
[INFO 2017-06-27 22:22:31,694 main.py:51] epoch 4626, training loss: 11018.73, average training loss: 10967.74, base loss: 19665.01
[INFO 2017-06-27 22:22:32,701 main.py:51] epoch 4627, training loss: 10980.07, average training loss: 10966.65, base loss: 19661.71
[INFO 2017-06-27 22:22:33,812 main.py:51] epoch 4628, training loss: 12313.54, average training loss: 10969.17, base loss: 19667.56
[INFO 2017-06-27 22:22:34,934 main.py:51] epoch 4629, training loss: 11426.66, average training loss: 10969.46, base loss: 19670.27
[INFO 2017-06-27 22:22:36,005 main.py:51] epoch 4630, training loss: 11893.26, average training loss: 10970.32, base loss: 19672.11
[INFO 2017-06-27 22:22:37,011 main.py:51] epoch 4631, training loss: 10463.08, average training loss: 10970.14, base loss: 19673.34
[INFO 2017-06-27 22:22:38,132 main.py:51] epoch 4632, training loss: 10337.21, average training loss: 10969.68, base loss: 19672.47
[INFO 2017-06-27 22:22:39,201 main.py:51] epoch 4633, training loss: 11601.15, average training loss: 10968.94, base loss: 19672.94
[INFO 2017-06-27 22:22:40,239 main.py:51] epoch 4634, training loss: 11359.29, average training loss: 10967.97, base loss: 19670.98
[INFO 2017-06-27 22:22:41,361 main.py:51] epoch 4635, training loss: 10825.66, average training loss: 10967.92, base loss: 19671.74
[INFO 2017-06-27 22:22:42,348 main.py:51] epoch 4636, training loss: 10795.23, average training loss: 10967.09, base loss: 19670.04
[INFO 2017-06-27 22:22:43,467 main.py:51] epoch 4637, training loss: 11371.60, average training loss: 10968.92, base loss: 19675.43
[INFO 2017-06-27 22:22:44,637 main.py:51] epoch 4638, training loss: 10429.89, average training loss: 10968.73, base loss: 19675.28
[INFO 2017-06-27 22:22:45,693 main.py:51] epoch 4639, training loss: 9803.97, average training loss: 10967.76, base loss: 19674.82
[INFO 2017-06-27 22:22:46,690 main.py:51] epoch 4640, training loss: 10980.27, average training loss: 10968.84, base loss: 19678.20
[INFO 2017-06-27 22:22:47,775 main.py:51] epoch 4641, training loss: 11808.31, average training loss: 10970.53, base loss: 19681.33
[INFO 2017-06-27 22:22:48,870 main.py:51] epoch 4642, training loss: 10953.61, average training loss: 10969.95, base loss: 19679.17
[INFO 2017-06-27 22:22:49,878 main.py:51] epoch 4643, training loss: 10902.78, average training loss: 10972.05, base loss: 19684.93
[INFO 2017-06-27 22:22:50,959 main.py:51] epoch 4644, training loss: 10720.32, average training loss: 10971.16, base loss: 19683.53
[INFO 2017-06-27 22:22:52,054 main.py:51] epoch 4645, training loss: 10843.57, average training loss: 10970.50, base loss: 19684.39
[INFO 2017-06-27 22:22:53,116 main.py:51] epoch 4646, training loss: 10232.83, average training loss: 10969.99, base loss: 19683.67
[INFO 2017-06-27 22:22:54,109 main.py:51] epoch 4647, training loss: 10107.50, average training loss: 10969.06, base loss: 19681.21
[INFO 2017-06-27 22:22:55,083 main.py:51] epoch 4648, training loss: 10325.43, average training loss: 10968.69, base loss: 19680.20
[INFO 2017-06-27 22:22:56,067 main.py:51] epoch 4649, training loss: 10722.12, average training loss: 10967.95, base loss: 19678.71
[INFO 2017-06-27 22:22:57,038 main.py:51] epoch 4650, training loss: 11314.78, average training loss: 10965.63, base loss: 19673.85
[INFO 2017-06-27 22:22:58,011 main.py:51] epoch 4651, training loss: 10869.39, average training loss: 10966.39, base loss: 19675.66
[INFO 2017-06-27 22:22:58,983 main.py:51] epoch 4652, training loss: 13031.50, average training loss: 10968.43, base loss: 19682.39
[INFO 2017-06-27 22:22:59,957 main.py:51] epoch 4653, training loss: 10510.08, average training loss: 10968.45, base loss: 19684.08
[INFO 2017-06-27 22:23:00,928 main.py:51] epoch 4654, training loss: 10686.46, average training loss: 10968.33, base loss: 19684.27
[INFO 2017-06-27 22:23:01,901 main.py:51] epoch 4655, training loss: 11656.67, average training loss: 10968.14, base loss: 19684.01
[INFO 2017-06-27 22:23:02,877 main.py:51] epoch 4656, training loss: 10923.58, average training loss: 10968.93, base loss: 19686.03
[INFO 2017-06-27 22:23:03,967 main.py:51] epoch 4657, training loss: 11592.43, average training loss: 10969.81, base loss: 19687.95
[INFO 2017-06-27 22:23:05,054 main.py:51] epoch 4658, training loss: 12478.07, average training loss: 10971.26, base loss: 19693.33
[INFO 2017-06-27 22:23:06,063 main.py:51] epoch 4659, training loss: 12625.36, average training loss: 10973.69, base loss: 19699.24
[INFO 2017-06-27 22:23:07,035 main.py:51] epoch 4660, training loss: 10425.95, average training loss: 10972.43, base loss: 19698.73
[INFO 2017-06-27 22:23:08,004 main.py:51] epoch 4661, training loss: 10681.68, average training loss: 10972.24, base loss: 19696.45
[INFO 2017-06-27 22:23:08,979 main.py:51] epoch 4662, training loss: 10638.88, average training loss: 10971.61, base loss: 19694.12
[INFO 2017-06-27 22:23:09,951 main.py:51] epoch 4663, training loss: 12096.35, average training loss: 10972.48, base loss: 19698.73
[INFO 2017-06-27 22:23:10,924 main.py:51] epoch 4664, training loss: 10207.02, average training loss: 10971.59, base loss: 19698.83
[INFO 2017-06-27 22:23:11,895 main.py:51] epoch 4665, training loss: 10685.77, average training loss: 10970.57, base loss: 19699.23
[INFO 2017-06-27 22:23:12,870 main.py:51] epoch 4666, training loss: 10857.35, average training loss: 10970.79, base loss: 19699.73
[INFO 2017-06-27 22:23:13,849 main.py:51] epoch 4667, training loss: 10954.25, average training loss: 10972.17, base loss: 19703.76
[INFO 2017-06-27 22:23:14,817 main.py:51] epoch 4668, training loss: 8691.45, average training loss: 10969.37, base loss: 19699.42
[INFO 2017-06-27 22:23:15,788 main.py:51] epoch 4669, training loss: 11185.98, average training loss: 10969.33, base loss: 19699.90
[INFO 2017-06-27 22:23:16,762 main.py:51] epoch 4670, training loss: 11146.71, average training loss: 10970.06, base loss: 19701.70
[INFO 2017-06-27 22:23:17,734 main.py:51] epoch 4671, training loss: 11410.40, average training loss: 10969.34, base loss: 19701.46
[INFO 2017-06-27 22:23:18,708 main.py:51] epoch 4672, training loss: 11074.42, average training loss: 10969.01, base loss: 19700.44
[INFO 2017-06-27 22:23:19,680 main.py:51] epoch 4673, training loss: 9265.87, average training loss: 10966.19, base loss: 19693.63
[INFO 2017-06-27 22:23:20,652 main.py:51] epoch 4674, training loss: 10943.89, average training loss: 10966.38, base loss: 19695.42
[INFO 2017-06-27 22:23:21,627 main.py:51] epoch 4675, training loss: 9855.86, average training loss: 10964.92, base loss: 19691.83
[INFO 2017-06-27 22:23:22,603 main.py:51] epoch 4676, training loss: 10420.25, average training loss: 10964.63, base loss: 19691.02
[INFO 2017-06-27 22:23:23,586 main.py:51] epoch 4677, training loss: 9762.06, average training loss: 10963.85, base loss: 19688.95
[INFO 2017-06-27 22:23:24,564 main.py:51] epoch 4678, training loss: 10736.12, average training loss: 10962.48, base loss: 19686.37
[INFO 2017-06-27 22:23:25,546 main.py:51] epoch 4679, training loss: 10207.86, average training loss: 10961.58, base loss: 19685.33
[INFO 2017-06-27 22:23:26,536 main.py:51] epoch 4680, training loss: 11974.54, average training loss: 10961.61, base loss: 19686.89
[INFO 2017-06-27 22:23:27,511 main.py:51] epoch 4681, training loss: 10516.13, average training loss: 10961.10, base loss: 19684.77
[INFO 2017-06-27 22:23:28,483 main.py:51] epoch 4682, training loss: 9880.14, average training loss: 10960.09, base loss: 19682.13
[INFO 2017-06-27 22:23:29,457 main.py:51] epoch 4683, training loss: 10534.16, average training loss: 10958.53, base loss: 19679.41
[INFO 2017-06-27 22:23:30,427 main.py:51] epoch 4684, training loss: 10413.10, average training loss: 10958.50, base loss: 19680.64
[INFO 2017-06-27 22:23:31,401 main.py:51] epoch 4685, training loss: 10278.76, average training loss: 10958.32, base loss: 19681.52
[INFO 2017-06-27 22:23:32,380 main.py:51] epoch 4686, training loss: 10985.05, average training loss: 10956.68, base loss: 19678.42
[INFO 2017-06-27 22:23:33,463 main.py:51] epoch 4687, training loss: 9974.73, average training loss: 10953.78, base loss: 19671.64
[INFO 2017-06-27 22:23:34,504 main.py:51] epoch 4688, training loss: 11251.05, average training loss: 10953.57, base loss: 19673.11
[INFO 2017-06-27 22:23:35,537 main.py:51] epoch 4689, training loss: 9299.82, average training loss: 10952.34, base loss: 19670.44
[INFO 2017-06-27 22:23:36,520 main.py:51] epoch 4690, training loss: 11534.79, average training loss: 10953.46, base loss: 19671.79
[INFO 2017-06-27 22:23:37,497 main.py:51] epoch 4691, training loss: 10711.23, average training loss: 10952.71, base loss: 19669.97
[INFO 2017-06-27 22:23:38,478 main.py:51] epoch 4692, training loss: 10820.38, average training loss: 10951.86, base loss: 19669.20
[INFO 2017-06-27 22:23:39,456 main.py:51] epoch 4693, training loss: 10356.00, average training loss: 10952.58, base loss: 19671.56
[INFO 2017-06-27 22:23:40,426 main.py:51] epoch 4694, training loss: 10688.34, average training loss: 10952.73, base loss: 19672.18
[INFO 2017-06-27 22:23:41,400 main.py:51] epoch 4695, training loss: 11688.10, average training loss: 10953.35, base loss: 19675.17
[INFO 2017-06-27 22:23:42,372 main.py:51] epoch 4696, training loss: 10608.38, average training loss: 10951.03, base loss: 19670.58
[INFO 2017-06-27 22:23:43,344 main.py:51] epoch 4697, training loss: 11441.67, average training loss: 10951.55, base loss: 19673.24
[INFO 2017-06-27 22:23:44,316 main.py:51] epoch 4698, training loss: 10741.08, average training loss: 10950.59, base loss: 19671.84
[INFO 2017-06-27 22:23:45,287 main.py:51] epoch 4699, training loss: 10887.63, average training loss: 10948.48, base loss: 19667.01
[INFO 2017-06-27 22:23:45,287 main.py:53] epoch 4699, testing
[INFO 2017-06-27 22:23:49,502 main.py:105] average testing loss: 10541.34, base loss: 18995.95
[INFO 2017-06-27 22:23:49,502 main.py:106] improve_loss: 8454.61, improve_percent: 0.45
[INFO 2017-06-27 22:23:49,502 main.py:76] current best improved percent: 0.45
[INFO 2017-06-27 22:23:50,478 main.py:51] epoch 4700, training loss: 11065.11, average training loss: 10947.79, base loss: 19666.34
[INFO 2017-06-27 22:23:51,448 main.py:51] epoch 4701, training loss: 11388.91, average training loss: 10947.28, base loss: 19666.31
[INFO 2017-06-27 22:23:52,423 main.py:51] epoch 4702, training loss: 11085.73, average training loss: 10947.89, base loss: 19667.18
[INFO 2017-06-27 22:23:53,396 main.py:51] epoch 4703, training loss: 10026.04, average training loss: 10947.29, base loss: 19664.58
[INFO 2017-06-27 22:23:54,368 main.py:51] epoch 4704, training loss: 10497.58, average training loss: 10946.37, base loss: 19662.19
[INFO 2017-06-27 22:23:55,346 main.py:51] epoch 4705, training loss: 10885.15, average training loss: 10946.15, base loss: 19662.01
[INFO 2017-06-27 22:23:56,318 main.py:51] epoch 4706, training loss: 11441.24, average training loss: 10946.42, base loss: 19663.04
[INFO 2017-06-27 22:23:57,291 main.py:51] epoch 4707, training loss: 10871.49, average training loss: 10945.26, base loss: 19660.54
[INFO 2017-06-27 22:23:58,263 main.py:51] epoch 4708, training loss: 10869.73, average training loss: 10944.88, base loss: 19660.16
[INFO 2017-06-27 22:23:59,236 main.py:51] epoch 4709, training loss: 11026.89, average training loss: 10945.88, base loss: 19664.07
[INFO 2017-06-27 22:24:00,207 main.py:51] epoch 4710, training loss: 11389.58, average training loss: 10947.56, base loss: 19670.41
[INFO 2017-06-27 22:24:01,181 main.py:51] epoch 4711, training loss: 9608.62, average training loss: 10945.75, base loss: 19667.91
[INFO 2017-06-27 22:24:02,151 main.py:51] epoch 4712, training loss: 10578.34, average training loss: 10945.66, base loss: 19668.73
[INFO 2017-06-27 22:24:03,125 main.py:51] epoch 4713, training loss: 11246.58, average training loss: 10946.48, base loss: 19670.74
[INFO 2017-06-27 22:24:04,101 main.py:51] epoch 4714, training loss: 11534.07, average training loss: 10946.20, base loss: 19670.14
[INFO 2017-06-27 22:24:05,077 main.py:51] epoch 4715, training loss: 11578.61, average training loss: 10946.22, base loss: 19670.54
[INFO 2017-06-27 22:24:06,048 main.py:51] epoch 4716, training loss: 10666.13, average training loss: 10947.00, base loss: 19673.29
[INFO 2017-06-27 22:24:07,025 main.py:51] epoch 4717, training loss: 11177.51, average training loss: 10946.73, base loss: 19672.88
[INFO 2017-06-27 22:24:07,996 main.py:51] epoch 4718, training loss: 12126.50, average training loss: 10948.66, base loss: 19677.66
[INFO 2017-06-27 22:24:08,967 main.py:51] epoch 4719, training loss: 11336.18, average training loss: 10948.51, base loss: 19678.52
[INFO 2017-06-27 22:24:09,940 main.py:51] epoch 4720, training loss: 10155.56, average training loss: 10948.28, base loss: 19678.86
[INFO 2017-06-27 22:24:10,915 main.py:51] epoch 4721, training loss: 11334.36, average training loss: 10948.42, base loss: 19680.02
[INFO 2017-06-27 22:24:11,886 main.py:51] epoch 4722, training loss: 11276.06, average training loss: 10949.17, base loss: 19681.32
[INFO 2017-06-27 22:24:12,859 main.py:51] epoch 4723, training loss: 9749.67, average training loss: 10949.16, base loss: 19683.47
[INFO 2017-06-27 22:24:13,834 main.py:51] epoch 4724, training loss: 10031.72, average training loss: 10949.79, base loss: 19685.27
[INFO 2017-06-27 22:24:14,808 main.py:51] epoch 4725, training loss: 10053.39, average training loss: 10948.68, base loss: 19682.46
[INFO 2017-06-27 22:24:15,782 main.py:51] epoch 4726, training loss: 10679.54, average training loss: 10948.85, base loss: 19684.11
[INFO 2017-06-27 22:24:16,754 main.py:51] epoch 4727, training loss: 12495.59, average training loss: 10950.40, base loss: 19688.44
[INFO 2017-06-27 22:24:17,729 main.py:51] epoch 4728, training loss: 12048.60, average training loss: 10951.01, base loss: 19689.80
[INFO 2017-06-27 22:24:18,752 main.py:51] epoch 4729, training loss: 10818.75, average training loss: 10949.17, base loss: 19686.65
[INFO 2017-06-27 22:24:19,764 main.py:51] epoch 4730, training loss: 11077.70, average training loss: 10949.76, base loss: 19689.19
[INFO 2017-06-27 22:24:20,739 main.py:51] epoch 4731, training loss: 11743.31, average training loss: 10951.62, base loss: 19693.56
[INFO 2017-06-27 22:24:21,818 main.py:51] epoch 4732, training loss: 10992.53, average training loss: 10951.82, base loss: 19694.71
[INFO 2017-06-27 22:24:22,837 main.py:51] epoch 4733, training loss: 9804.57, average training loss: 10950.38, base loss: 19690.83
[INFO 2017-06-27 22:24:23,974 main.py:51] epoch 4734, training loss: 9377.68, average training loss: 10950.61, base loss: 19690.90
[INFO 2017-06-27 22:24:25,002 main.py:51] epoch 4735, training loss: 11489.38, average training loss: 10951.18, base loss: 19691.93
[INFO 2017-06-27 22:24:26,048 main.py:51] epoch 4736, training loss: 11735.54, average training loss: 10951.88, base loss: 19695.50
[INFO 2017-06-27 22:24:27,153 main.py:51] epoch 4737, training loss: 11847.17, average training loss: 10951.50, base loss: 19694.71
[INFO 2017-06-27 22:24:28,159 main.py:51] epoch 4738, training loss: 12217.50, average training loss: 10954.03, base loss: 19699.92
[INFO 2017-06-27 22:24:29,238 main.py:51] epoch 4739, training loss: 10123.19, average training loss: 10951.61, base loss: 19693.59
[INFO 2017-06-27 22:24:30,347 main.py:51] epoch 4740, training loss: 9853.58, average training loss: 10950.35, base loss: 19690.92
[INFO 2017-06-27 22:24:31,473 main.py:51] epoch 4741, training loss: 12385.50, average training loss: 10952.83, base loss: 19695.04
[INFO 2017-06-27 22:24:32,491 main.py:51] epoch 4742, training loss: 10940.35, average training loss: 10952.60, base loss: 19695.06
[INFO 2017-06-27 22:24:33,588 main.py:51] epoch 4743, training loss: 10819.16, average training loss: 10951.49, base loss: 19694.82
[INFO 2017-06-27 22:24:34,664 main.py:51] epoch 4744, training loss: 11892.94, average training loss: 10952.08, base loss: 19696.48
[INFO 2017-06-27 22:24:35,741 main.py:51] epoch 4745, training loss: 9957.53, average training loss: 10952.25, base loss: 19698.48
[INFO 2017-06-27 22:24:36,735 main.py:51] epoch 4746, training loss: 9834.17, average training loss: 10951.37, base loss: 19696.35
[INFO 2017-06-27 22:24:37,811 main.py:51] epoch 4747, training loss: 11689.12, average training loss: 10951.16, base loss: 19695.82
[INFO 2017-06-27 22:24:38,789 main.py:51] epoch 4748, training loss: 12723.61, average training loss: 10953.30, base loss: 19701.82
[INFO 2017-06-27 22:24:39,821 main.py:51] epoch 4749, training loss: 9976.86, average training loss: 10952.03, base loss: 19699.33
[INFO 2017-06-27 22:24:40,822 main.py:51] epoch 4750, training loss: 10170.88, average training loss: 10950.51, base loss: 19696.79
[INFO 2017-06-27 22:24:41,917 main.py:51] epoch 4751, training loss: 9852.20, average training loss: 10948.25, base loss: 19692.55
[INFO 2017-06-27 22:24:42,908 main.py:51] epoch 4752, training loss: 10467.22, average training loss: 10948.25, base loss: 19695.17
[INFO 2017-06-27 22:24:43,882 main.py:51] epoch 4753, training loss: 10789.09, average training loss: 10947.67, base loss: 19694.70
[INFO 2017-06-27 22:24:44,967 main.py:51] epoch 4754, training loss: 10899.58, average training loss: 10948.76, base loss: 19697.74
[INFO 2017-06-27 22:24:46,043 main.py:51] epoch 4755, training loss: 10403.89, average training loss: 10948.06, base loss: 19696.82
[INFO 2017-06-27 22:24:47,109 main.py:51] epoch 4756, training loss: 11059.37, average training loss: 10946.97, base loss: 19693.23
[INFO 2017-06-27 22:24:48,113 main.py:51] epoch 4757, training loss: 10235.29, average training loss: 10945.69, base loss: 19690.40
[INFO 2017-06-27 22:24:49,189 main.py:51] epoch 4758, training loss: 11289.19, average training loss: 10945.58, base loss: 19691.36
[INFO 2017-06-27 22:24:50,253 main.py:51] epoch 4759, training loss: 11042.22, average training loss: 10945.81, base loss: 19692.40
[INFO 2017-06-27 22:24:51,299 main.py:51] epoch 4760, training loss: 10819.74, average training loss: 10945.01, base loss: 19690.82
[INFO 2017-06-27 22:24:52,296 main.py:51] epoch 4761, training loss: 10084.81, average training loss: 10942.38, base loss: 19685.60
[INFO 2017-06-27 22:24:53,406 main.py:51] epoch 4762, training loss: 10481.34, average training loss: 10941.05, base loss: 19683.71
[INFO 2017-06-27 22:24:54,450 main.py:51] epoch 4763, training loss: 10796.55, average training loss: 10940.91, base loss: 19686.42
[INFO 2017-06-27 22:24:55,443 main.py:51] epoch 4764, training loss: 10833.39, average training loss: 10941.91, base loss: 19688.49
[INFO 2017-06-27 22:24:56,536 main.py:51] epoch 4765, training loss: 11817.07, average training loss: 10942.41, base loss: 19690.79
[INFO 2017-06-27 22:24:57,519 main.py:51] epoch 4766, training loss: 10210.69, average training loss: 10941.65, base loss: 19690.95
[INFO 2017-06-27 22:24:58,590 main.py:51] epoch 4767, training loss: 10374.56, average training loss: 10941.18, base loss: 19691.21
[INFO 2017-06-27 22:24:59,601 main.py:51] epoch 4768, training loss: 12593.81, average training loss: 10942.68, base loss: 19696.51
[INFO 2017-06-27 22:25:00,574 main.py:51] epoch 4769, training loss: 10780.69, average training loss: 10942.76, base loss: 19695.59
[INFO 2017-06-27 22:25:01,556 main.py:51] epoch 4770, training loss: 10264.27, average training loss: 10941.30, base loss: 19691.32
[INFO 2017-06-27 22:25:02,532 main.py:51] epoch 4771, training loss: 11930.99, average training loss: 10942.01, base loss: 19693.26
[INFO 2017-06-27 22:25:03,510 main.py:51] epoch 4772, training loss: 11164.37, average training loss: 10942.26, base loss: 19696.85
[INFO 2017-06-27 22:25:04,487 main.py:51] epoch 4773, training loss: 9766.42, average training loss: 10941.04, base loss: 19693.85
[INFO 2017-06-27 22:25:05,462 main.py:51] epoch 4774, training loss: 10629.30, average training loss: 10941.22, base loss: 19696.89
[INFO 2017-06-27 22:25:06,433 main.py:51] epoch 4775, training loss: 10152.06, average training loss: 10940.09, base loss: 19694.69
[INFO 2017-06-27 22:25:07,418 main.py:51] epoch 4776, training loss: 12079.01, average training loss: 10940.20, base loss: 19696.59
[INFO 2017-06-27 22:25:08,392 main.py:51] epoch 4777, training loss: 9815.19, average training loss: 10937.91, base loss: 19691.88
[INFO 2017-06-27 22:25:09,370 main.py:51] epoch 4778, training loss: 9804.47, average training loss: 10936.86, base loss: 19690.35
[INFO 2017-06-27 22:25:10,344 main.py:51] epoch 4779, training loss: 10074.10, average training loss: 10935.82, base loss: 19687.04
[INFO 2017-06-27 22:25:11,316 main.py:51] epoch 4780, training loss: 10657.88, average training loss: 10935.91, base loss: 19687.66
[INFO 2017-06-27 22:25:12,291 main.py:51] epoch 4781, training loss: 10028.51, average training loss: 10933.65, base loss: 19683.06
[INFO 2017-06-27 22:25:13,265 main.py:51] epoch 4782, training loss: 11657.73, average training loss: 10934.29, base loss: 19684.86
[INFO 2017-06-27 22:25:14,242 main.py:51] epoch 4783, training loss: 10825.94, average training loss: 10934.75, base loss: 19686.40
[INFO 2017-06-27 22:25:15,219 main.py:51] epoch 4784, training loss: 10503.19, average training loss: 10935.60, base loss: 19689.30
[INFO 2017-06-27 22:25:16,199 main.py:51] epoch 4785, training loss: 10145.92, average training loss: 10933.95, base loss: 19685.81
[INFO 2017-06-27 22:25:17,193 main.py:51] epoch 4786, training loss: 11114.63, average training loss: 10933.49, base loss: 19686.72
[INFO 2017-06-27 22:25:18,218 main.py:51] epoch 4787, training loss: 11227.57, average training loss: 10934.73, base loss: 19689.69
[INFO 2017-06-27 22:25:19,338 main.py:51] epoch 4788, training loss: 11246.77, average training loss: 10934.76, base loss: 19689.90
[INFO 2017-06-27 22:25:20,314 main.py:51] epoch 4789, training loss: 10020.71, average training loss: 10933.58, base loss: 19688.20
[INFO 2017-06-27 22:25:21,295 main.py:51] epoch 4790, training loss: 11226.45, average training loss: 10935.06, base loss: 19691.84
[INFO 2017-06-27 22:25:22,266 main.py:51] epoch 4791, training loss: 10569.83, average training loss: 10935.84, base loss: 19693.99
[INFO 2017-06-27 22:25:23,241 main.py:51] epoch 4792, training loss: 10925.07, average training loss: 10935.70, base loss: 19695.78
[INFO 2017-06-27 22:25:24,211 main.py:51] epoch 4793, training loss: 9671.38, average training loss: 10934.71, base loss: 19694.93
[INFO 2017-06-27 22:25:25,185 main.py:51] epoch 4794, training loss: 10937.78, average training loss: 10934.93, base loss: 19696.71
[INFO 2017-06-27 22:25:26,159 main.py:51] epoch 4795, training loss: 11323.24, average training loss: 10936.05, base loss: 19701.03
[INFO 2017-06-27 22:25:27,234 main.py:51] epoch 4796, training loss: 10802.35, average training loss: 10935.48, base loss: 19699.28
[INFO 2017-06-27 22:25:28,253 main.py:51] epoch 4797, training loss: 11343.19, average training loss: 10935.10, base loss: 19699.87
[INFO 2017-06-27 22:25:29,234 main.py:51] epoch 4798, training loss: 11875.30, average training loss: 10936.65, base loss: 19706.37
[INFO 2017-06-27 22:25:30,210 main.py:51] epoch 4799, training loss: 10465.35, average training loss: 10937.37, base loss: 19708.42
[INFO 2017-06-27 22:25:30,210 main.py:53] epoch 4799, testing
[INFO 2017-06-27 22:25:34,637 main.py:105] average testing loss: 10756.13, base loss: 19897.15
[INFO 2017-06-27 22:25:34,637 main.py:106] improve_loss: 9141.03, improve_percent: 0.46
[INFO 2017-06-27 22:25:34,638 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 22:25:34,651 main.py:76] current best improved percent: 0.46
[INFO 2017-06-27 22:25:35,736 main.py:51] epoch 4800, training loss: 11391.94, average training loss: 10938.39, base loss: 19710.69
[INFO 2017-06-27 22:25:36,712 main.py:51] epoch 4801, training loss: 10337.47, average training loss: 10938.81, base loss: 19711.02
[INFO 2017-06-27 22:25:37,692 main.py:51] epoch 4802, training loss: 10534.30, average training loss: 10938.83, base loss: 19710.72
[INFO 2017-06-27 22:25:38,666 main.py:51] epoch 4803, training loss: 10077.75, average training loss: 10937.65, base loss: 19710.66
[INFO 2017-06-27 22:25:39,642 main.py:51] epoch 4804, training loss: 10008.09, average training loss: 10937.60, base loss: 19710.56
[INFO 2017-06-27 22:25:40,615 main.py:51] epoch 4805, training loss: 10717.85, average training loss: 10937.94, base loss: 19711.67
[INFO 2017-06-27 22:25:41,591 main.py:51] epoch 4806, training loss: 9633.98, average training loss: 10935.86, base loss: 19707.69
[INFO 2017-06-27 22:25:42,565 main.py:51] epoch 4807, training loss: 10789.90, average training loss: 10933.36, base loss: 19703.16
[INFO 2017-06-27 22:25:43,543 main.py:51] epoch 4808, training loss: 11588.06, average training loss: 10932.96, base loss: 19703.64
[INFO 2017-06-27 22:25:44,522 main.py:51] epoch 4809, training loss: 10163.98, average training loss: 10933.20, base loss: 19706.56
[INFO 2017-06-27 22:25:45,502 main.py:51] epoch 4810, training loss: 11460.46, average training loss: 10934.00, base loss: 19709.81
[INFO 2017-06-27 22:25:46,601 main.py:51] epoch 4811, training loss: 11191.94, average training loss: 10934.55, base loss: 19711.61
[INFO 2017-06-27 22:25:47,734 main.py:51] epoch 4812, training loss: 10406.17, average training loss: 10934.15, base loss: 19711.36
[INFO 2017-06-27 22:25:48,811 main.py:51] epoch 4813, training loss: 10817.45, average training loss: 10935.32, base loss: 19714.40
[INFO 2017-06-27 22:25:49,931 main.py:51] epoch 4814, training loss: 11080.43, average training loss: 10933.55, base loss: 19710.40
[INFO 2017-06-27 22:25:50,926 main.py:51] epoch 4815, training loss: 10320.88, average training loss: 10932.38, base loss: 19706.51
[INFO 2017-06-27 22:25:51,918 main.py:51] epoch 4816, training loss: 9981.60, average training loss: 10931.50, base loss: 19704.02
[INFO 2017-06-27 22:25:52,938 main.py:51] epoch 4817, training loss: 9506.63, average training loss: 10930.65, base loss: 19702.61
[INFO 2017-06-27 22:25:53,997 main.py:51] epoch 4818, training loss: 11768.17, average training loss: 10929.85, base loss: 19700.83
[INFO 2017-06-27 22:25:55,039 main.py:51] epoch 4819, training loss: 11701.00, average training loss: 10931.13, base loss: 19706.74
[INFO 2017-06-27 22:25:56,054 main.py:51] epoch 4820, training loss: 12669.11, average training loss: 10932.91, base loss: 19711.44
[INFO 2017-06-27 22:25:57,031 main.py:51] epoch 4821, training loss: 10205.03, average training loss: 10932.78, base loss: 19710.39
[INFO 2017-06-27 22:25:58,005 main.py:51] epoch 4822, training loss: 9832.39, average training loss: 10931.00, base loss: 19707.68
[INFO 2017-06-27 22:25:59,024 main.py:51] epoch 4823, training loss: 10243.96, average training loss: 10930.38, base loss: 19706.73
[INFO 2017-06-27 22:26:00,020 main.py:51] epoch 4824, training loss: 11291.36, average training loss: 10930.66, base loss: 19708.49
[INFO 2017-06-27 22:26:01,032 main.py:51] epoch 4825, training loss: 10881.66, average training loss: 10930.85, base loss: 19711.62
[INFO 2017-06-27 22:26:02,043 main.py:51] epoch 4826, training loss: 9883.71, average training loss: 10926.90, base loss: 19701.43
[INFO 2017-06-27 22:26:03,020 main.py:51] epoch 4827, training loss: 10945.86, average training loss: 10927.12, base loss: 19702.43
[INFO 2017-06-27 22:26:03,995 main.py:51] epoch 4828, training loss: 12161.42, average training loss: 10927.36, base loss: 19705.43
[INFO 2017-06-27 22:26:05,014 main.py:51] epoch 4829, training loss: 9520.18, average training loss: 10926.18, base loss: 19704.05
[INFO 2017-06-27 22:26:06,010 main.py:51] epoch 4830, training loss: 10725.29, average training loss: 10925.89, base loss: 19704.19
[INFO 2017-06-27 22:26:07,095 main.py:51] epoch 4831, training loss: 9474.16, average training loss: 10923.92, base loss: 19700.42
[INFO 2017-06-27 22:26:08,075 main.py:51] epoch 4832, training loss: 10935.11, average training loss: 10922.51, base loss: 19698.28
[INFO 2017-06-27 22:26:09,165 main.py:51] epoch 4833, training loss: 10348.83, average training loss: 10920.61, base loss: 19693.61
[INFO 2017-06-27 22:26:10,275 main.py:51] epoch 4834, training loss: 10603.03, average training loss: 10920.32, base loss: 19691.39
[INFO 2017-06-27 22:26:11,347 main.py:51] epoch 4835, training loss: 11282.52, average training loss: 10920.36, base loss: 19692.36
[INFO 2017-06-27 22:26:12,348 main.py:51] epoch 4836, training loss: 10475.48, average training loss: 10919.22, base loss: 19689.93
[INFO 2017-06-27 22:26:13,421 main.py:51] epoch 4837, training loss: 11504.88, average training loss: 10918.95, base loss: 19689.12
[INFO 2017-06-27 22:26:14,545 main.py:51] epoch 4838, training loss: 10666.67, average training loss: 10918.77, base loss: 19688.95
[INFO 2017-06-27 22:26:15,560 main.py:51] epoch 4839, training loss: 11264.75, average training loss: 10918.23, base loss: 19687.51
[INFO 2017-06-27 22:26:16,575 main.py:51] epoch 4840, training loss: 12297.35, average training loss: 10918.73, base loss: 19687.99
[INFO 2017-06-27 22:26:17,569 main.py:51] epoch 4841, training loss: 11919.97, average training loss: 10918.64, base loss: 19688.16
[INFO 2017-06-27 22:26:18,648 main.py:51] epoch 4842, training loss: 11422.12, average training loss: 10918.64, base loss: 19689.93
[INFO 2017-06-27 22:26:19,624 main.py:51] epoch 4843, training loss: 11363.35, average training loss: 10919.72, base loss: 19693.79
[INFO 2017-06-27 22:26:20,595 main.py:51] epoch 4844, training loss: 12924.24, average training loss: 10921.19, base loss: 19699.25
[INFO 2017-06-27 22:26:21,617 main.py:51] epoch 4845, training loss: 10585.24, average training loss: 10921.76, base loss: 19700.84
[INFO 2017-06-27 22:26:22,648 main.py:51] epoch 4846, training loss: 10100.72, average training loss: 10922.22, base loss: 19702.14
[INFO 2017-06-27 22:26:23,625 main.py:51] epoch 4847, training loss: 10170.96, average training loss: 10921.20, base loss: 19699.83
[INFO 2017-06-27 22:26:24,722 main.py:51] epoch 4848, training loss: 12363.93, average training loss: 10921.62, base loss: 19701.89
[INFO 2017-06-27 22:26:25,724 main.py:51] epoch 4849, training loss: 12545.21, average training loss: 10923.10, base loss: 19705.62
[INFO 2017-06-27 22:26:26,801 main.py:51] epoch 4850, training loss: 12750.62, average training loss: 10924.43, base loss: 19709.60
[INFO 2017-06-27 22:26:27,823 main.py:51] epoch 4851, training loss: 11214.40, average training loss: 10925.42, base loss: 19711.96
[INFO 2017-06-27 22:26:28,820 main.py:51] epoch 4852, training loss: 10352.09, average training loss: 10924.92, base loss: 19710.12
[INFO 2017-06-27 22:26:29,797 main.py:51] epoch 4853, training loss: 10606.40, average training loss: 10924.30, base loss: 19710.01
[INFO 2017-06-27 22:26:30,863 main.py:51] epoch 4854, training loss: 10378.69, average training loss: 10923.16, base loss: 19708.28
[INFO 2017-06-27 22:26:31,917 main.py:51] epoch 4855, training loss: 11293.07, average training loss: 10923.27, base loss: 19707.91
[INFO 2017-06-27 22:26:32,909 main.py:51] epoch 4856, training loss: 10364.79, average training loss: 10922.41, base loss: 19706.62
[INFO 2017-06-27 22:26:33,883 main.py:51] epoch 4857, training loss: 10244.20, average training loss: 10920.24, base loss: 19701.52
[INFO 2017-06-27 22:26:34,856 main.py:51] epoch 4858, training loss: 11996.88, average training loss: 10920.85, base loss: 19703.69
[INFO 2017-06-27 22:26:35,828 main.py:51] epoch 4859, training loss: 11536.05, average training loss: 10920.63, base loss: 19704.22
[INFO 2017-06-27 22:26:36,802 main.py:51] epoch 4860, training loss: 11889.90, average training loss: 10920.03, base loss: 19702.95
[INFO 2017-06-27 22:26:37,868 main.py:51] epoch 4861, training loss: 10308.05, average training loss: 10919.58, base loss: 19703.88
[INFO 2017-06-27 22:26:38,868 main.py:51] epoch 4862, training loss: 10899.36, average training loss: 10919.33, base loss: 19705.00
[INFO 2017-06-27 22:26:39,899 main.py:51] epoch 4863, training loss: 11288.39, average training loss: 10920.22, base loss: 19707.81
[INFO 2017-06-27 22:26:40,967 main.py:51] epoch 4864, training loss: 10250.05, average training loss: 10919.88, base loss: 19707.53
[INFO 2017-06-27 22:26:42,075 main.py:51] epoch 4865, training loss: 9651.99, average training loss: 10918.18, base loss: 19705.71
[INFO 2017-06-27 22:26:43,094 main.py:51] epoch 4866, training loss: 10604.75, average training loss: 10916.73, base loss: 19703.10
[INFO 2017-06-27 22:26:44,162 main.py:51] epoch 4867, training loss: 12136.64, average training loss: 10918.94, base loss: 19708.36
[INFO 2017-06-27 22:26:45,276 main.py:51] epoch 4868, training loss: 10106.61, average training loss: 10918.22, base loss: 19705.89
[INFO 2017-06-27 22:26:46,320 main.py:51] epoch 4869, training loss: 11031.16, average training loss: 10916.48, base loss: 19701.58
[INFO 2017-06-27 22:26:47,314 main.py:51] epoch 4870, training loss: 10378.49, average training loss: 10916.98, base loss: 19703.86
[INFO 2017-06-27 22:26:48,289 main.py:51] epoch 4871, training loss: 12210.13, average training loss: 10918.03, base loss: 19707.28
[INFO 2017-06-27 22:26:49,259 main.py:51] epoch 4872, training loss: 9189.00, average training loss: 10916.46, base loss: 19705.30
[INFO 2017-06-27 22:26:50,324 main.py:51] epoch 4873, training loss: 12356.40, average training loss: 10918.13, base loss: 19709.97
[INFO 2017-06-27 22:26:51,328 main.py:51] epoch 4874, training loss: 10150.52, average training loss: 10918.07, base loss: 19710.59
[INFO 2017-06-27 22:26:52,347 main.py:51] epoch 4875, training loss: 9963.98, average training loss: 10916.87, base loss: 19708.99
[INFO 2017-06-27 22:26:53,356 main.py:51] epoch 4876, training loss: 10487.69, average training loss: 10917.29, base loss: 19711.07
[INFO 2017-06-27 22:26:54,432 main.py:51] epoch 4877, training loss: 10573.60, average training loss: 10917.58, base loss: 19712.23
[INFO 2017-06-27 22:26:55,433 main.py:51] epoch 4878, training loss: 11813.09, average training loss: 10918.79, base loss: 19716.73
[INFO 2017-06-27 22:26:56,458 main.py:51] epoch 4879, training loss: 11841.86, average training loss: 10919.47, base loss: 19717.68
[INFO 2017-06-27 22:26:57,454 main.py:51] epoch 4880, training loss: 11001.84, average training loss: 10920.29, base loss: 19719.47
[INFO 2017-06-27 22:26:58,533 main.py:51] epoch 4881, training loss: 9791.93, average training loss: 10919.06, base loss: 19717.11
[INFO 2017-06-27 22:26:59,589 main.py:51] epoch 4882, training loss: 11997.23, average training loss: 10920.05, base loss: 19720.28
[INFO 2017-06-27 22:27:00,607 main.py:51] epoch 4883, training loss: 11773.76, average training loss: 10921.37, base loss: 19724.25
[INFO 2017-06-27 22:27:01,581 main.py:51] epoch 4884, training loss: 11340.17, average training loss: 10922.51, base loss: 19728.07
[INFO 2017-06-27 22:27:02,555 main.py:51] epoch 4885, training loss: 10400.52, average training loss: 10922.33, base loss: 19728.56
[INFO 2017-06-27 22:27:03,527 main.py:51] epoch 4886, training loss: 11862.56, average training loss: 10924.06, base loss: 19734.30
[INFO 2017-06-27 22:27:04,603 main.py:51] epoch 4887, training loss: 11747.04, average training loss: 10924.00, base loss: 19734.41
[INFO 2017-06-27 22:27:05,585 main.py:51] epoch 4888, training loss: 9591.28, average training loss: 10922.28, base loss: 19731.61
[INFO 2017-06-27 22:27:06,559 main.py:51] epoch 4889, training loss: 10171.25, average training loss: 10921.39, base loss: 19729.81
[INFO 2017-06-27 22:27:07,530 main.py:51] epoch 4890, training loss: 10258.32, average training loss: 10919.48, base loss: 19725.37
[INFO 2017-06-27 22:27:08,549 main.py:51] epoch 4891, training loss: 11317.53, average training loss: 10918.78, base loss: 19724.04
[INFO 2017-06-27 22:27:09,583 main.py:51] epoch 4892, training loss: 9659.16, average training loss: 10916.96, base loss: 19721.71
[INFO 2017-06-27 22:27:10,660 main.py:51] epoch 4893, training loss: 10470.53, average training loss: 10915.89, base loss: 19719.16
[INFO 2017-06-27 22:27:11,689 main.py:51] epoch 4894, training loss: 11950.70, average training loss: 10916.53, base loss: 19721.90
[INFO 2017-06-27 22:27:12,738 main.py:51] epoch 4895, training loss: 10541.49, average training loss: 10916.98, base loss: 19723.68
[INFO 2017-06-27 22:27:13,763 main.py:51] epoch 4896, training loss: 10923.81, average training loss: 10915.74, base loss: 19722.25
[INFO 2017-06-27 22:27:14,838 main.py:51] epoch 4897, training loss: 11065.73, average training loss: 10916.97, base loss: 19727.23
[INFO 2017-06-27 22:27:15,933 main.py:51] epoch 4898, training loss: 10047.22, average training loss: 10916.22, base loss: 19726.20
[INFO 2017-06-27 22:27:17,006 main.py:51] epoch 4899, training loss: 10368.71, average training loss: 10914.99, base loss: 19722.80
[INFO 2017-06-27 22:27:17,006 main.py:53] epoch 4899, testing
[INFO 2017-06-27 22:27:21,490 main.py:105] average testing loss: 11004.79, base loss: 20713.09
[INFO 2017-06-27 22:27:21,491 main.py:106] improve_loss: 9708.29, improve_percent: 0.47
[INFO 2017-06-27 22:27:21,492 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 22:27:21,516 main.py:76] current best improved percent: 0.47
[INFO 2017-06-27 22:27:22,555 main.py:51] epoch 4900, training loss: 9619.88, average training loss: 10911.97, base loss: 19715.61
[INFO 2017-06-27 22:27:23,583 main.py:51] epoch 4901, training loss: 11820.70, average training loss: 10912.28, base loss: 19714.77
[INFO 2017-06-27 22:27:24,556 main.py:51] epoch 4902, training loss: 10094.02, average training loss: 10909.21, base loss: 19708.27
[INFO 2017-06-27 22:27:25,527 main.py:51] epoch 4903, training loss: 9850.30, average training loss: 10907.85, base loss: 19706.77
[INFO 2017-06-27 22:27:26,505 main.py:51] epoch 4904, training loss: 10689.09, average training loss: 10907.21, base loss: 19706.12
[INFO 2017-06-27 22:27:27,492 main.py:51] epoch 4905, training loss: 10568.44, average training loss: 10905.78, base loss: 19705.21
[INFO 2017-06-27 22:27:28,487 main.py:51] epoch 4906, training loss: 11381.62, average training loss: 10905.76, base loss: 19706.34
[INFO 2017-06-27 22:27:29,469 main.py:51] epoch 4907, training loss: 9978.51, average training loss: 10904.14, base loss: 19704.55
[INFO 2017-06-27 22:27:30,523 main.py:51] epoch 4908, training loss: 10970.11, average training loss: 10904.05, base loss: 19705.69
[INFO 2017-06-27 22:27:31,665 main.py:51] epoch 4909, training loss: 10226.27, average training loss: 10904.54, base loss: 19707.22
[INFO 2017-06-27 22:27:32,730 main.py:51] epoch 4910, training loss: 9196.33, average training loss: 10903.04, base loss: 19705.62
[INFO 2017-06-27 22:27:33,861 main.py:51] epoch 4911, training loss: 10192.79, average training loss: 10903.02, base loss: 19706.64
[INFO 2017-06-27 22:27:34,879 main.py:51] epoch 4912, training loss: 10092.13, average training loss: 10901.75, base loss: 19705.54
[INFO 2017-06-27 22:27:35,856 main.py:51] epoch 4913, training loss: 10784.30, average training loss: 10901.52, base loss: 19705.70
[INFO 2017-06-27 22:27:36,898 main.py:51] epoch 4914, training loss: 11203.51, average training loss: 10901.14, base loss: 19706.94
[INFO 2017-06-27 22:27:37,907 main.py:51] epoch 4915, training loss: 9875.81, average training loss: 10900.18, base loss: 19705.77
[INFO 2017-06-27 22:27:38,880 main.py:51] epoch 4916, training loss: 12043.48, average training loss: 10901.75, base loss: 19711.52
[INFO 2017-06-27 22:27:39,851 main.py:51] epoch 4917, training loss: 9873.61, average training loss: 10901.11, base loss: 19712.14
[INFO 2017-06-27 22:27:40,821 main.py:51] epoch 4918, training loss: 9904.51, average training loss: 10898.49, base loss: 19707.57
[INFO 2017-06-27 22:27:41,794 main.py:51] epoch 4919, training loss: 10832.92, average training loss: 10898.53, base loss: 19708.27
[INFO 2017-06-27 22:27:42,770 main.py:51] epoch 4920, training loss: 10787.35, average training loss: 10898.99, base loss: 19711.46
[INFO 2017-06-27 22:27:43,744 main.py:51] epoch 4921, training loss: 11753.04, average training loss: 10897.86, base loss: 19710.38
[INFO 2017-06-27 22:27:44,810 main.py:51] epoch 4922, training loss: 10493.72, average training loss: 10898.04, base loss: 19710.31
[INFO 2017-06-27 22:27:45,855 main.py:51] epoch 4923, training loss: 10294.34, average training loss: 10898.09, base loss: 19713.11
[INFO 2017-06-27 22:27:46,847 main.py:51] epoch 4924, training loss: 10138.79, average training loss: 10897.77, base loss: 19713.06
[INFO 2017-06-27 22:27:47,825 main.py:51] epoch 4925, training loss: 10952.27, average training loss: 10899.29, base loss: 19717.32
[INFO 2017-06-27 22:27:48,799 main.py:51] epoch 4926, training loss: 12028.40, average training loss: 10899.81, base loss: 19718.31
[INFO 2017-06-27 22:27:49,772 main.py:51] epoch 4927, training loss: 9377.98, average training loss: 10898.85, base loss: 19716.36
[INFO 2017-06-27 22:27:50,749 main.py:51] epoch 4928, training loss: 10665.05, average training loss: 10897.24, base loss: 19713.95
[INFO 2017-06-27 22:27:51,720 main.py:51] epoch 4929, training loss: 10096.27, average training loss: 10898.18, base loss: 19715.56
[INFO 2017-06-27 22:27:52,692 main.py:51] epoch 4930, training loss: 11602.07, average training loss: 10898.47, base loss: 19716.58
[INFO 2017-06-27 22:27:53,666 main.py:51] epoch 4931, training loss: 11030.87, average training loss: 10897.38, base loss: 19714.61
[INFO 2017-06-27 22:27:54,639 main.py:51] epoch 4932, training loss: 11429.71, average training loss: 10895.79, base loss: 19712.17
[INFO 2017-06-27 22:27:55,611 main.py:51] epoch 4933, training loss: 10974.65, average training loss: 10895.75, base loss: 19712.57
[INFO 2017-06-27 22:27:56,702 main.py:51] epoch 4934, training loss: 11058.44, average training loss: 10894.92, base loss: 19712.78
[INFO 2017-06-27 22:27:57,684 main.py:51] epoch 4935, training loss: 10100.25, average training loss: 10892.80, base loss: 19707.84
[INFO 2017-06-27 22:27:58,662 main.py:51] epoch 4936, training loss: 10937.61, average training loss: 10892.27, base loss: 19707.10
[INFO 2017-06-27 22:27:59,734 main.py:51] epoch 4937, training loss: 9538.02, average training loss: 10888.65, base loss: 19700.17
[INFO 2017-06-27 22:28:00,811 main.py:51] epoch 4938, training loss: 10118.22, average training loss: 10887.24, base loss: 19697.99
[INFO 2017-06-27 22:28:01,818 main.py:51] epoch 4939, training loss: 10567.57, average training loss: 10885.65, base loss: 19694.17
[INFO 2017-06-27 22:28:02,793 main.py:51] epoch 4940, training loss: 11694.30, average training loss: 10885.99, base loss: 19696.57
[INFO 2017-06-27 22:28:03,792 main.py:51] epoch 4941, training loss: 11107.65, average training loss: 10887.20, base loss: 19699.87
[INFO 2017-06-27 22:28:04,821 main.py:51] epoch 4942, training loss: 8966.06, average training loss: 10885.42, base loss: 19696.03
[INFO 2017-06-27 22:28:05,798 main.py:51] epoch 4943, training loss: 10773.38, average training loss: 10884.77, base loss: 19695.57
[INFO 2017-06-27 22:28:06,770 main.py:51] epoch 4944, training loss: 10650.61, average training loss: 10883.43, base loss: 19691.81
[INFO 2017-06-27 22:28:07,872 main.py:51] epoch 4945, training loss: 10195.06, average training loss: 10882.23, base loss: 19688.79
[INFO 2017-06-27 22:28:08,899 main.py:51] epoch 4946, training loss: 10739.78, average training loss: 10881.48, base loss: 19688.24
[INFO 2017-06-27 22:28:09,973 main.py:51] epoch 4947, training loss: 11667.14, average training loss: 10881.57, base loss: 19690.29
[INFO 2017-06-27 22:28:11,035 main.py:51] epoch 4948, training loss: 9575.51, average training loss: 10879.24, base loss: 19686.45
[INFO 2017-06-27 22:28:12,159 main.py:51] epoch 4949, training loss: 10330.36, average training loss: 10879.08, base loss: 19687.08
[INFO 2017-06-27 22:28:13,194 main.py:51] epoch 4950, training loss: 10822.00, average training loss: 10879.62, base loss: 19688.87
[INFO 2017-06-27 22:28:14,167 main.py:51] epoch 4951, training loss: 10686.09, average training loss: 10879.04, base loss: 19690.04
[INFO 2017-06-27 22:28:15,137 main.py:51] epoch 4952, training loss: 10602.31, average training loss: 10877.44, base loss: 19686.65
[INFO 2017-06-27 22:28:16,108 main.py:51] epoch 4953, training loss: 10600.41, average training loss: 10877.89, base loss: 19689.62
[INFO 2017-06-27 22:28:17,198 main.py:51] epoch 4954, training loss: 10080.03, average training loss: 10876.61, base loss: 19687.31
[INFO 2017-06-27 22:28:18,176 main.py:51] epoch 4955, training loss: 10098.28, average training loss: 10875.19, base loss: 19684.80
[INFO 2017-06-27 22:28:19,193 main.py:51] epoch 4956, training loss: 11022.23, average training loss: 10874.96, base loss: 19684.09
[INFO 2017-06-27 22:28:20,206 main.py:51] epoch 4957, training loss: 11679.29, average training loss: 10876.65, base loss: 19688.72
[INFO 2017-06-27 22:28:21,285 main.py:51] epoch 4958, training loss: 10186.99, average training loss: 10876.11, base loss: 19689.41
[INFO 2017-06-27 22:28:22,403 main.py:51] epoch 4959, training loss: 11261.06, average training loss: 10876.95, base loss: 19692.28
[INFO 2017-06-27 22:28:23,396 main.py:51] epoch 4960, training loss: 11750.90, average training loss: 10876.94, base loss: 19695.08
[INFO 2017-06-27 22:28:24,486 main.py:51] epoch 4961, training loss: 11111.13, average training loss: 10877.59, base loss: 19696.30
[INFO 2017-06-27 22:28:25,553 main.py:51] epoch 4962, training loss: 9117.14, average training loss: 10874.34, base loss: 19688.81
[INFO 2017-06-27 22:28:26,685 main.py:51] epoch 4963, training loss: 10801.61, average training loss: 10874.38, base loss: 19688.40
[INFO 2017-06-27 22:28:27,702 main.py:51] epoch 4964, training loss: 10543.79, average training loss: 10873.57, base loss: 19687.39
[INFO 2017-06-27 22:28:28,684 main.py:51] epoch 4965, training loss: 9668.67, average training loss: 10872.36, base loss: 19684.22
[INFO 2017-06-27 22:28:29,657 main.py:51] epoch 4966, training loss: 12093.92, average training loss: 10874.09, base loss: 19688.02
[INFO 2017-06-27 22:28:30,733 main.py:51] epoch 4967, training loss: 9819.28, average training loss: 10873.50, base loss: 19688.37
[INFO 2017-06-27 22:28:31,757 main.py:51] epoch 4968, training loss: 10277.14, average training loss: 10872.27, base loss: 19688.14
[INFO 2017-06-27 22:28:32,750 main.py:51] epoch 4969, training loss: 8838.86, average training loss: 10871.19, base loss: 19685.20
[INFO 2017-06-27 22:28:33,841 main.py:51] epoch 4970, training loss: 11243.25, average training loss: 10870.99, base loss: 19686.28
[INFO 2017-06-27 22:28:34,916 main.py:51] epoch 4971, training loss: 12520.88, average training loss: 10873.19, base loss: 19691.38
[INFO 2017-06-27 22:28:35,966 main.py:51] epoch 4972, training loss: 11167.93, average training loss: 10873.05, base loss: 19690.92
[INFO 2017-06-27 22:28:36,960 main.py:51] epoch 4973, training loss: 11730.99, average training loss: 10875.10, base loss: 19694.88
[INFO 2017-06-27 22:28:37,936 main.py:51] epoch 4974, training loss: 9769.41, average training loss: 10874.80, base loss: 19693.50
[INFO 2017-06-27 22:28:38,952 main.py:51] epoch 4975, training loss: 10253.47, average training loss: 10873.61, base loss: 19691.26
[INFO 2017-06-27 22:28:39,969 main.py:51] epoch 4976, training loss: 9091.79, average training loss: 10872.50, base loss: 19689.91
[INFO 2017-06-27 22:28:41,035 main.py:51] epoch 4977, training loss: 10556.15, average training loss: 10871.81, base loss: 19689.68
[INFO 2017-06-27 22:28:42,051 main.py:51] epoch 4978, training loss: 12253.62, average training loss: 10873.33, base loss: 19694.29
[INFO 2017-06-27 22:28:43,092 main.py:51] epoch 4979, training loss: 11608.36, average training loss: 10874.53, base loss: 19696.84
[INFO 2017-06-27 22:28:44,157 main.py:51] epoch 4980, training loss: 11584.84, average training loss: 10874.07, base loss: 19696.07
[INFO 2017-06-27 22:28:45,158 main.py:51] epoch 4981, training loss: 10926.07, average training loss: 10874.57, base loss: 19698.70
[INFO 2017-06-27 22:28:46,247 main.py:51] epoch 4982, training loss: 12360.77, average training loss: 10876.17, base loss: 19701.91
[INFO 2017-06-27 22:28:47,323 main.py:51] epoch 4983, training loss: 10315.33, average training loss: 10876.61, base loss: 19704.87
[INFO 2017-06-27 22:28:48,348 main.py:51] epoch 4984, training loss: 10096.03, average training loss: 10875.99, base loss: 19704.87
[INFO 2017-06-27 22:28:49,431 main.py:51] epoch 4985, training loss: 11088.67, average training loss: 10876.95, base loss: 19708.16
[INFO 2017-06-27 22:28:50,560 main.py:51] epoch 4986, training loss: 11527.01, average training loss: 10877.27, base loss: 19710.59
[INFO 2017-06-27 22:28:51,624 main.py:51] epoch 4987, training loss: 10295.54, average training loss: 10876.53, base loss: 19707.99
[INFO 2017-06-27 22:28:52,618 main.py:51] epoch 4988, training loss: 10274.38, average training loss: 10876.01, base loss: 19707.25
[INFO 2017-06-27 22:28:53,687 main.py:51] epoch 4989, training loss: 9810.06, average training loss: 10875.44, base loss: 19706.66
[INFO 2017-06-27 22:28:54,790 main.py:51] epoch 4990, training loss: 10116.46, average training loss: 10874.59, base loss: 19703.68
[INFO 2017-06-27 22:28:55,851 main.py:51] epoch 4991, training loss: 10931.97, average training loss: 10875.60, base loss: 19706.65
[INFO 2017-06-27 22:28:56,847 main.py:51] epoch 4992, training loss: 10076.06, average training loss: 10874.82, base loss: 19705.18
[INFO 2017-06-27 22:28:57,934 main.py:51] epoch 4993, training loss: 10838.85, average training loss: 10873.16, base loss: 19702.52
[INFO 2017-06-27 22:28:59,006 main.py:51] epoch 4994, training loss: 11161.59, average training loss: 10873.32, base loss: 19701.47
[INFO 2017-06-27 22:29:00,165 main.py:51] epoch 4995, training loss: 9846.08, average training loss: 10871.98, base loss: 19698.77
[INFO 2017-06-27 22:29:01,284 main.py:51] epoch 4996, training loss: 9903.41, average training loss: 10871.96, base loss: 19699.58
[INFO 2017-06-27 22:29:02,346 main.py:51] epoch 4997, training loss: 11342.80, average training loss: 10873.46, base loss: 19703.54
[INFO 2017-06-27 22:29:03,344 main.py:51] epoch 4998, training loss: 10116.00, average training loss: 10873.19, base loss: 19704.08
[INFO 2017-06-27 22:29:04,437 main.py:51] epoch 4999, training loss: 13278.24, average training loss: 10875.68, base loss: 19710.77
[INFO 2017-06-27 22:29:04,437 main.py:53] epoch 4999, testing
[INFO 2017-06-27 22:29:08,991 main.py:105] average testing loss: 11049.07, base loss: 20281.45
[INFO 2017-06-27 22:29:08,992 main.py:106] improve_loss: 9232.38, improve_percent: 0.46
[INFO 2017-06-27 22:29:08,993 main.py:76] current best improved percent: 0.47
[INFO 2017-06-27 22:29:10,017 main.py:51] epoch 5000, training loss: 10492.40, average training loss: 10875.50, base loss: 19710.18
[INFO 2017-06-27 22:29:11,015 main.py:51] epoch 5001, training loss: 10586.64, average training loss: 10874.60, base loss: 19708.45
[INFO 2017-06-27 22:29:12,056 main.py:51] epoch 5002, training loss: 10686.39, average training loss: 10874.19, base loss: 19707.11
[INFO 2017-06-27 22:29:13,131 main.py:51] epoch 5003, training loss: 11745.21, average training loss: 10874.68, base loss: 19709.46
[INFO 2017-06-27 22:29:14,191 main.py:51] epoch 5004, training loss: 11087.45, average training loss: 10874.42, base loss: 19708.91
[INFO 2017-06-27 22:29:15,204 main.py:51] epoch 5005, training loss: 10725.23, average training loss: 10875.45, base loss: 19710.81
[INFO 2017-06-27 22:29:16,280 main.py:51] epoch 5006, training loss: 11177.16, average training loss: 10873.58, base loss: 19706.84
[INFO 2017-06-27 22:29:17,390 main.py:51] epoch 5007, training loss: 10700.67, average training loss: 10872.13, base loss: 19704.18
[INFO 2017-06-27 22:29:18,482 main.py:51] epoch 5008, training loss: 10995.48, average training loss: 10872.86, base loss: 19706.68
[INFO 2017-06-27 22:29:19,482 main.py:51] epoch 5009, training loss: 12514.32, average training loss: 10875.57, base loss: 19715.82
[INFO 2017-06-27 22:29:20,513 main.py:51] epoch 5010, training loss: 11985.92, average training loss: 10876.66, base loss: 19719.60
[INFO 2017-06-27 22:29:21,506 main.py:51] epoch 5011, training loss: 11157.64, average training loss: 10876.67, base loss: 19721.44
[INFO 2017-06-27 22:29:22,540 main.py:51] epoch 5012, training loss: 9475.92, average training loss: 10875.79, base loss: 19720.65
[INFO 2017-06-27 22:29:23,538 main.py:51] epoch 5013, training loss: 10763.69, average training loss: 10876.01, base loss: 19721.43
[INFO 2017-06-27 22:29:24,513 main.py:51] epoch 5014, training loss: 10541.94, average training loss: 10875.19, base loss: 19719.13
[INFO 2017-06-27 22:29:25,492 main.py:51] epoch 5015, training loss: 10667.50, average training loss: 10874.83, base loss: 19718.87
[INFO 2017-06-27 22:29:26,468 main.py:51] epoch 5016, training loss: 10160.45, average training loss: 10873.50, base loss: 19717.21
[INFO 2017-06-27 22:29:27,437 main.py:51] epoch 5017, training loss: 10336.58, average training loss: 10873.97, base loss: 19719.38
[INFO 2017-06-27 22:29:28,409 main.py:51] epoch 5018, training loss: 11060.21, average training loss: 10872.76, base loss: 19717.29
[INFO 2017-06-27 22:29:29,382 main.py:51] epoch 5019, training loss: 10887.85, average training loss: 10872.39, base loss: 19718.40
[INFO 2017-06-27 22:29:30,464 main.py:51] epoch 5020, training loss: 10903.27, average training loss: 10871.77, base loss: 19718.13
[INFO 2017-06-27 22:29:31,468 main.py:51] epoch 5021, training loss: 11007.00, average training loss: 10870.88, base loss: 19715.44
[INFO 2017-06-27 22:29:32,560 main.py:51] epoch 5022, training loss: 10456.44, average training loss: 10870.18, base loss: 19713.80
[INFO 2017-06-27 22:29:33,653 main.py:51] epoch 5023, training loss: 10794.64, average training loss: 10870.47, base loss: 19714.68
[INFO 2017-06-27 22:29:34,691 main.py:51] epoch 5024, training loss: 10045.59, average training loss: 10870.21, base loss: 19715.20
[INFO 2017-06-27 22:29:35,745 main.py:51] epoch 5025, training loss: 12024.07, average training loss: 10871.19, base loss: 19717.08
[INFO 2017-06-27 22:29:36,791 main.py:51] epoch 5026, training loss: 10775.01, average training loss: 10871.36, base loss: 19718.05
[INFO 2017-06-27 22:29:37,865 main.py:51] epoch 5027, training loss: 12058.01, average training loss: 10874.12, base loss: 19726.92
[INFO 2017-06-27 22:29:38,997 main.py:51] epoch 5028, training loss: 11069.50, average training loss: 10874.61, base loss: 19729.59
[INFO 2017-06-27 22:29:40,002 main.py:51] epoch 5029, training loss: 11758.47, average training loss: 10875.17, base loss: 19732.15
[INFO 2017-06-27 22:29:40,970 main.py:51] epoch 5030, training loss: 9848.83, average training loss: 10874.45, base loss: 19731.33
[INFO 2017-06-27 22:29:41,996 main.py:51] epoch 5031, training loss: 11860.55, average training loss: 10875.61, base loss: 19734.12
[INFO 2017-06-27 22:29:42,994 main.py:51] epoch 5032, training loss: 10151.64, average training loss: 10874.22, base loss: 19733.03
[INFO 2017-06-27 22:29:44,096 main.py:51] epoch 5033, training loss: 9683.42, average training loss: 10873.28, base loss: 19732.65
[INFO 2017-06-27 22:29:45,181 main.py:51] epoch 5034, training loss: 10471.42, average training loss: 10873.93, base loss: 19734.67
[INFO 2017-06-27 22:29:46,211 main.py:51] epoch 5035, training loss: 12375.80, average training loss: 10874.32, base loss: 19737.56
[INFO 2017-06-27 22:29:47,314 main.py:51] epoch 5036, training loss: 11058.60, average training loss: 10874.08, base loss: 19738.09
[INFO 2017-06-27 22:29:48,399 main.py:51] epoch 5037, training loss: 10249.94, average training loss: 10872.65, base loss: 19735.24
[INFO 2017-06-27 22:29:49,469 main.py:51] epoch 5038, training loss: 9730.57, average training loss: 10870.89, base loss: 19732.01
[INFO 2017-06-27 22:29:50,494 main.py:51] epoch 5039, training loss: 10248.24, average training loss: 10869.24, base loss: 19728.94
[INFO 2017-06-27 22:29:51,583 main.py:51] epoch 5040, training loss: 12263.61, average training loss: 10868.97, base loss: 19726.74
[INFO 2017-06-27 22:29:52,651 main.py:51] epoch 5041, training loss: 10247.80, average training loss: 10869.66, base loss: 19729.58
[INFO 2017-06-27 22:29:53,696 main.py:51] epoch 5042, training loss: 11228.31, average training loss: 10870.79, base loss: 19733.79
[INFO 2017-06-27 22:29:54,763 main.py:51] epoch 5043, training loss: 10817.15, average training loss: 10871.30, base loss: 19736.47
[INFO 2017-06-27 22:29:55,773 main.py:51] epoch 5044, training loss: 11159.13, average training loss: 10872.75, base loss: 19740.61
[INFO 2017-06-27 22:29:56,854 main.py:51] epoch 5045, training loss: 11166.63, average training loss: 10873.08, base loss: 19743.29
[INFO 2017-06-27 22:29:57,868 main.py:51] epoch 5046, training loss: 10399.25, average training loss: 10872.81, base loss: 19742.98
[INFO 2017-06-27 22:29:58,943 main.py:51] epoch 5047, training loss: 11294.34, average training loss: 10874.46, base loss: 19746.19
[INFO 2017-06-27 22:30:00,020 main.py:51] epoch 5048, training loss: 10800.68, average training loss: 10873.22, base loss: 19745.09
[INFO 2017-06-27 22:30:01,154 main.py:51] epoch 5049, training loss: 10419.19, average training loss: 10873.75, base loss: 19748.31
[INFO 2017-06-27 22:30:02,169 main.py:51] epoch 5050, training loss: 10274.25, average training loss: 10873.89, base loss: 19750.78
[INFO 2017-06-27 22:30:03,147 main.py:51] epoch 5051, training loss: 10739.85, average training loss: 10873.81, base loss: 19752.45
[INFO 2017-06-27 22:30:04,127 main.py:51] epoch 5052, training loss: 11050.02, average training loss: 10873.44, base loss: 19753.53
[INFO 2017-06-27 22:30:05,100 main.py:51] epoch 5053, training loss: 11633.03, average training loss: 10874.49, base loss: 19756.44
[INFO 2017-06-27 22:30:06,074 main.py:51] epoch 5054, training loss: 9907.95, average training loss: 10873.10, base loss: 19752.70
[INFO 2017-06-27 22:30:07,043 main.py:51] epoch 5055, training loss: 11589.15, average training loss: 10874.14, base loss: 19755.55
[INFO 2017-06-27 22:30:08,013 main.py:51] epoch 5056, training loss: 11287.09, average training loss: 10873.61, base loss: 19754.10
[INFO 2017-06-27 22:30:08,985 main.py:51] epoch 5057, training loss: 9425.76, average training loss: 10872.43, base loss: 19752.26
[INFO 2017-06-27 22:30:09,964 main.py:51] epoch 5058, training loss: 11342.68, average training loss: 10873.42, base loss: 19754.10
[INFO 2017-06-27 22:30:10,939 main.py:51] epoch 5059, training loss: 10092.30, average training loss: 10873.36, base loss: 19754.46
[INFO 2017-06-27 22:30:11,916 main.py:51] epoch 5060, training loss: 10795.69, average training loss: 10875.24, base loss: 19759.80
[INFO 2017-06-27 22:30:12,889 main.py:51] epoch 5061, training loss: 10038.50, average training loss: 10873.19, base loss: 19757.50
[INFO 2017-06-27 22:30:13,867 main.py:51] epoch 5062, training loss: 11311.66, average training loss: 10873.79, base loss: 19758.19
[INFO 2017-06-27 22:30:14,840 main.py:51] epoch 5063, training loss: 10750.68, average training loss: 10872.73, base loss: 19755.90
[INFO 2017-06-27 22:30:15,814 main.py:51] epoch 5064, training loss: 10687.06, average training loss: 10871.72, base loss: 19754.71
[INFO 2017-06-27 22:30:16,791 main.py:51] epoch 5065, training loss: 10553.78, average training loss: 10871.20, base loss: 19753.58
[INFO 2017-06-27 22:30:17,765 main.py:51] epoch 5066, training loss: 11721.96, average training loss: 10872.10, base loss: 19756.79
[INFO 2017-06-27 22:30:18,739 main.py:51] epoch 5067, training loss: 10565.28, average training loss: 10872.28, base loss: 19758.26
[INFO 2017-06-27 22:30:19,713 main.py:51] epoch 5068, training loss: 9450.97, average training loss: 10871.62, base loss: 19758.25
[INFO 2017-06-27 22:30:20,686 main.py:51] epoch 5069, training loss: 9923.86, average training loss: 10868.47, base loss: 19753.13
[INFO 2017-06-27 22:30:21,658 main.py:51] epoch 5070, training loss: 9441.41, average training loss: 10866.95, base loss: 19750.42
[INFO 2017-06-27 22:30:22,634 main.py:51] epoch 5071, training loss: 10793.33, average training loss: 10867.57, base loss: 19752.45
[INFO 2017-06-27 22:30:23,605 main.py:51] epoch 5072, training loss: 9955.85, average training loss: 10866.19, base loss: 19749.74
[INFO 2017-06-27 22:30:24,579 main.py:51] epoch 5073, training loss: 10789.85, average training loss: 10866.03, base loss: 19748.50
[INFO 2017-06-27 22:30:25,551 main.py:51] epoch 5074, training loss: 11711.36, average training loss: 10866.91, base loss: 19751.04
[INFO 2017-06-27 22:30:26,526 main.py:51] epoch 5075, training loss: 11273.06, average training loss: 10866.39, base loss: 19749.22
[INFO 2017-06-27 22:30:27,497 main.py:51] epoch 5076, training loss: 10795.66, average training loss: 10866.19, base loss: 19748.84
[INFO 2017-06-27 22:30:28,468 main.py:51] epoch 5077, training loss: 11141.60, average training loss: 10865.66, base loss: 19747.89
[INFO 2017-06-27 22:30:29,442 main.py:51] epoch 5078, training loss: 12025.92, average training loss: 10867.59, base loss: 19753.64
[INFO 2017-06-27 22:30:30,419 main.py:51] epoch 5079, training loss: 10653.60, average training loss: 10867.18, base loss: 19754.13
[INFO 2017-06-27 22:30:31,392 main.py:51] epoch 5080, training loss: 10210.82, average training loss: 10865.26, base loss: 19749.09
[INFO 2017-06-27 22:30:32,366 main.py:51] epoch 5081, training loss: 10925.19, average training loss: 10864.70, base loss: 19749.97
[INFO 2017-06-27 22:30:33,336 main.py:51] epoch 5082, training loss: 11145.19, average training loss: 10865.22, base loss: 19753.84
[INFO 2017-06-27 22:30:34,311 main.py:51] epoch 5083, training loss: 9845.84, average training loss: 10864.95, base loss: 19753.05
[INFO 2017-06-27 22:30:35,286 main.py:51] epoch 5084, training loss: 11067.32, average training loss: 10864.42, base loss: 19753.21
[INFO 2017-06-27 22:30:36,262 main.py:51] epoch 5085, training loss: 9694.33, average training loss: 10863.82, base loss: 19751.95
[INFO 2017-06-27 22:30:37,236 main.py:51] epoch 5086, training loss: 9554.85, average training loss: 10862.12, base loss: 19749.01
[INFO 2017-06-27 22:30:38,214 main.py:51] epoch 5087, training loss: 11934.09, average training loss: 10863.39, base loss: 19752.54
[INFO 2017-06-27 22:30:39,217 main.py:51] epoch 5088, training loss: 10371.30, average training loss: 10863.19, base loss: 19752.28
[INFO 2017-06-27 22:30:40,208 main.py:51] epoch 5089, training loss: 10150.54, average training loss: 10862.88, base loss: 19750.08
[INFO 2017-06-27 22:30:41,262 main.py:51] epoch 5090, training loss: 11589.43, average training loss: 10863.61, base loss: 19751.69
[INFO 2017-06-27 22:30:42,392 main.py:51] epoch 5091, training loss: 11650.93, average training loss: 10864.82, base loss: 19754.80
[INFO 2017-06-27 22:30:43,478 main.py:51] epoch 5092, training loss: 10680.09, average training loss: 10863.63, base loss: 19752.59
[INFO 2017-06-27 22:30:44,627 main.py:51] epoch 5093, training loss: 11170.82, average training loss: 10864.15, base loss: 19753.97
[INFO 2017-06-27 22:30:45,627 main.py:51] epoch 5094, training loss: 9975.14, average training loss: 10863.12, base loss: 19751.69
[INFO 2017-06-27 22:30:46,597 main.py:51] epoch 5095, training loss: 9479.87, average training loss: 10861.83, base loss: 19749.25
[INFO 2017-06-27 22:30:47,570 main.py:51] epoch 5096, training loss: 10252.29, average training loss: 10860.23, base loss: 19747.58
[INFO 2017-06-27 22:30:48,549 main.py:51] epoch 5097, training loss: 10752.86, average training loss: 10859.77, base loss: 19745.38
[INFO 2017-06-27 22:30:49,521 main.py:51] epoch 5098, training loss: 10267.12, average training loss: 10859.31, base loss: 19744.73
[INFO 2017-06-27 22:30:50,615 main.py:51] epoch 5099, training loss: 12289.17, average training loss: 10859.41, base loss: 19745.50
[INFO 2017-06-27 22:30:50,615 main.py:53] epoch 5099, testing
[INFO 2017-06-27 22:30:55,247 main.py:105] average testing loss: 10730.32, base loss: 19773.89
[INFO 2017-06-27 22:30:55,248 main.py:106] improve_loss: 9043.57, improve_percent: 0.46
[INFO 2017-06-27 22:30:55,248 main.py:76] current best improved percent: 0.47
[INFO 2017-06-27 22:30:56,343 main.py:51] epoch 5100, training loss: 10937.15, average training loss: 10859.74, base loss: 19745.52
[INFO 2017-06-27 22:30:57,412 main.py:51] epoch 5101, training loss: 12061.74, average training loss: 10860.26, base loss: 19747.27
[INFO 2017-06-27 22:30:58,407 main.py:51] epoch 5102, training loss: 12022.61, average training loss: 10860.72, base loss: 19748.46
[INFO 2017-06-27 22:30:59,381 main.py:51] epoch 5103, training loss: 12030.52, average training loss: 10862.16, base loss: 19752.53
[INFO 2017-06-27 22:31:00,350 main.py:51] epoch 5104, training loss: 10201.30, average training loss: 10860.73, base loss: 19748.48
[INFO 2017-06-27 22:31:01,322 main.py:51] epoch 5105, training loss: 9908.58, average training loss: 10860.31, base loss: 19748.09
[INFO 2017-06-27 22:31:02,297 main.py:51] epoch 5106, training loss: 10780.48, average training loss: 10860.21, base loss: 19749.13
[INFO 2017-06-27 22:31:03,269 main.py:51] epoch 5107, training loss: 10318.20, average training loss: 10859.80, base loss: 19749.11
[INFO 2017-06-27 22:31:04,241 main.py:51] epoch 5108, training loss: 11673.60, average training loss: 10860.68, base loss: 19751.89
[INFO 2017-06-27 22:31:05,216 main.py:51] epoch 5109, training loss: 10088.22, average training loss: 10860.12, base loss: 19749.87
[INFO 2017-06-27 22:31:06,192 main.py:51] epoch 5110, training loss: 12172.73, average training loss: 10862.02, base loss: 19754.13
[INFO 2017-06-27 22:31:07,267 main.py:51] epoch 5111, training loss: 9406.48, average training loss: 10861.19, base loss: 19752.87
[INFO 2017-06-27 22:31:08,326 main.py:51] epoch 5112, training loss: 10531.42, average training loss: 10860.55, base loss: 19750.90
[INFO 2017-06-27 22:31:09,353 main.py:51] epoch 5113, training loss: 10565.54, average training loss: 10859.46, base loss: 19749.73
[INFO 2017-06-27 22:31:10,434 main.py:51] epoch 5114, training loss: 10512.00, average training loss: 10856.18, base loss: 19742.18
[INFO 2017-06-27 22:31:11,410 main.py:51] epoch 5115, training loss: 10445.52, average training loss: 10855.96, base loss: 19742.58
[INFO 2017-06-27 22:31:12,487 main.py:51] epoch 5116, training loss: 10458.45, average training loss: 10855.68, base loss: 19745.25
[INFO 2017-06-27 22:31:13,519 main.py:51] epoch 5117, training loss: 10716.84, average training loss: 10854.04, base loss: 19740.30
[INFO 2017-06-27 22:31:14,513 main.py:51] epoch 5118, training loss: 9498.76, average training loss: 10850.43, base loss: 19732.11
[INFO 2017-06-27 22:31:15,580 main.py:51] epoch 5119, training loss: 11207.19, average training loss: 10849.85, base loss: 19733.65
[INFO 2017-06-27 22:31:16,580 main.py:51] epoch 5120, training loss: 10935.69, average training loss: 10848.89, base loss: 19733.00
[INFO 2017-06-27 22:31:17,554 main.py:51] epoch 5121, training loss: 10061.42, average training loss: 10847.05, base loss: 19729.39
[INFO 2017-06-27 22:31:18,528 main.py:51] epoch 5122, training loss: 10059.75, average training loss: 10845.75, base loss: 19728.10
[INFO 2017-06-27 22:31:19,503 main.py:51] epoch 5123, training loss: 11500.21, average training loss: 10846.73, base loss: 19731.05
[INFO 2017-06-27 22:31:20,479 main.py:51] epoch 5124, training loss: 10631.25, average training loss: 10846.50, base loss: 19731.01
[INFO 2017-06-27 22:31:21,458 main.py:51] epoch 5125, training loss: 10096.66, average training loss: 10845.76, base loss: 19729.96
[INFO 2017-06-27 22:31:22,553 main.py:51] epoch 5126, training loss: 11593.51, average training loss: 10844.65, base loss: 19728.59
[INFO 2017-06-27 22:31:23,674 main.py:51] epoch 5127, training loss: 10407.03, average training loss: 10844.05, base loss: 19727.15
[INFO 2017-06-27 22:31:24,654 main.py:51] epoch 5128, training loss: 10729.28, average training loss: 10844.16, base loss: 19726.02
[INFO 2017-06-27 22:31:25,627 main.py:51] epoch 5129, training loss: 10140.92, average training loss: 10843.01, base loss: 19723.89
[INFO 2017-06-27 22:31:26,599 main.py:51] epoch 5130, training loss: 10653.19, average training loss: 10843.03, base loss: 19723.44
[INFO 2017-06-27 22:31:27,574 main.py:51] epoch 5131, training loss: 12136.71, average training loss: 10844.11, base loss: 19726.64
[INFO 2017-06-27 22:31:28,644 main.py:51] epoch 5132, training loss: 10684.43, average training loss: 10844.80, base loss: 19728.93
[INFO 2017-06-27 22:31:29,652 main.py:51] epoch 5133, training loss: 11122.15, average training loss: 10844.41, base loss: 19727.44
[INFO 2017-06-27 22:31:30,626 main.py:51] epoch 5134, training loss: 11476.36, average training loss: 10846.81, base loss: 19734.26
[INFO 2017-06-27 22:31:31,597 main.py:51] epoch 5135, training loss: 10931.71, average training loss: 10845.90, base loss: 19732.29
[INFO 2017-06-27 22:31:32,572 main.py:51] epoch 5136, training loss: 10078.99, average training loss: 10844.44, base loss: 19729.14
[INFO 2017-06-27 22:31:33,548 main.py:51] epoch 5137, training loss: 10631.03, average training loss: 10844.58, base loss: 19729.32
[INFO 2017-06-27 22:31:34,652 main.py:51] epoch 5138, training loss: 10879.82, average training loss: 10844.56, base loss: 19729.69
[INFO 2017-06-27 22:31:35,750 main.py:51] epoch 5139, training loss: 10230.16, average training loss: 10844.05, base loss: 19728.57
[INFO 2017-06-27 22:31:36,748 main.py:51] epoch 5140, training loss: 9367.11, average training loss: 10843.03, base loss: 19726.28
[INFO 2017-06-27 22:31:37,846 main.py:51] epoch 5141, training loss: 10868.62, average training loss: 10843.30, base loss: 19727.59
[INFO 2017-06-27 22:31:38,824 main.py:51] epoch 5142, training loss: 11002.62, average training loss: 10843.23, base loss: 19727.42
[INFO 2017-06-27 22:31:39,798 main.py:51] epoch 5143, training loss: 10471.18, average training loss: 10842.15, base loss: 19725.87
[INFO 2017-06-27 22:31:40,771 main.py:51] epoch 5144, training loss: 10557.71, average training loss: 10842.64, base loss: 19727.94
[INFO 2017-06-27 22:31:41,754 main.py:51] epoch 5145, training loss: 11090.93, average training loss: 10843.26, base loss: 19730.46
[INFO 2017-06-27 22:31:42,772 main.py:51] epoch 5146, training loss: 11638.36, average training loss: 10841.52, base loss: 19727.26
[INFO 2017-06-27 22:31:43,768 main.py:51] epoch 5147, training loss: 10774.81, average training loss: 10840.97, base loss: 19727.15
[INFO 2017-06-27 22:31:44,878 main.py:51] epoch 5148, training loss: 11695.45, average training loss: 10842.30, base loss: 19730.14
[INFO 2017-06-27 22:31:45,871 main.py:51] epoch 5149, training loss: 10063.70, average training loss: 10840.34, base loss: 19725.41
[INFO 2017-06-27 22:31:46,882 main.py:51] epoch 5150, training loss: 11053.96, average training loss: 10840.93, base loss: 19726.48
[INFO 2017-06-27 22:31:47,898 main.py:51] epoch 5151, training loss: 10242.75, average training loss: 10838.95, base loss: 19723.06
[INFO 2017-06-27 22:31:48,925 main.py:51] epoch 5152, training loss: 11257.84, average training loss: 10838.43, base loss: 19722.20
[INFO 2017-06-27 22:31:49,932 main.py:51] epoch 5153, training loss: 10028.83, average training loss: 10837.34, base loss: 19719.65
[INFO 2017-06-27 22:31:51,026 main.py:51] epoch 5154, training loss: 11266.40, average training loss: 10838.31, base loss: 19722.87
[INFO 2017-06-27 22:31:52,109 main.py:51] epoch 5155, training loss: 10663.05, average training loss: 10838.14, base loss: 19721.16
[INFO 2017-06-27 22:31:53,138 main.py:51] epoch 5156, training loss: 11467.73, average training loss: 10839.17, base loss: 19723.98
[INFO 2017-06-27 22:31:54,112 main.py:51] epoch 5157, training loss: 10185.51, average training loss: 10838.38, base loss: 19722.37
[INFO 2017-06-27 22:31:55,089 main.py:51] epoch 5158, training loss: 10764.01, average training loss: 10838.53, base loss: 19724.67
[INFO 2017-06-27 22:31:56,112 main.py:51] epoch 5159, training loss: 10696.83, average training loss: 10838.02, base loss: 19724.12
[INFO 2017-06-27 22:31:57,120 main.py:51] epoch 5160, training loss: 10628.41, average training loss: 10837.28, base loss: 19723.18
[INFO 2017-06-27 22:31:58,097 main.py:51] epoch 5161, training loss: 13287.45, average training loss: 10838.58, base loss: 19725.93
[INFO 2017-06-27 22:31:59,071 main.py:51] epoch 5162, training loss: 10777.33, average training loss: 10837.93, base loss: 19724.71
[INFO 2017-06-27 22:32:00,051 main.py:51] epoch 5163, training loss: 9917.38, average training loss: 10836.99, base loss: 19722.44
[INFO 2017-06-27 22:32:01,065 main.py:51] epoch 5164, training loss: 10043.58, average training loss: 10835.80, base loss: 19719.61
[INFO 2017-06-27 22:32:02,093 main.py:51] epoch 5165, training loss: 10708.06, average training loss: 10836.33, base loss: 19721.60
[INFO 2017-06-27 22:32:03,070 main.py:51] epoch 5166, training loss: 11176.47, average training loss: 10837.39, base loss: 19721.75
[INFO 2017-06-27 22:32:04,042 main.py:51] epoch 5167, training loss: 12100.94, average training loss: 10838.26, base loss: 19722.82
[INFO 2017-06-27 22:32:05,018 main.py:51] epoch 5168, training loss: 10686.51, average training loss: 10838.63, base loss: 19723.82
[INFO 2017-06-27 22:32:06,094 main.py:51] epoch 5169, training loss: 11413.34, average training loss: 10837.95, base loss: 19722.51
[INFO 2017-06-27 22:32:07,094 main.py:51] epoch 5170, training loss: 12666.08, average training loss: 10838.97, base loss: 19724.60
[INFO 2017-06-27 22:32:08,070 main.py:51] epoch 5171, training loss: 10323.58, average training loss: 10837.39, base loss: 19720.13
[INFO 2017-06-27 22:32:09,041 main.py:51] epoch 5172, training loss: 11043.20, average training loss: 10839.75, base loss: 19724.33
[INFO 2017-06-27 22:32:10,016 main.py:51] epoch 5173, training loss: 10919.40, average training loss: 10840.14, base loss: 19726.97
[INFO 2017-06-27 22:32:11,071 main.py:51] epoch 5174, training loss: 10311.14, average training loss: 10840.27, base loss: 19729.07
[INFO 2017-06-27 22:32:12,179 main.py:51] epoch 5175, training loss: 9781.38, average training loss: 10838.63, base loss: 19725.75
[INFO 2017-06-27 22:32:13,190 main.py:51] epoch 5176, training loss: 11418.85, average training loss: 10839.15, base loss: 19725.98
[INFO 2017-06-27 22:32:14,173 main.py:51] epoch 5177, training loss: 9801.21, average training loss: 10837.79, base loss: 19722.92
[INFO 2017-06-27 22:32:15,253 main.py:51] epoch 5178, training loss: 10693.90, average training loss: 10837.97, base loss: 19723.06
[INFO 2017-06-27 22:32:16,256 main.py:51] epoch 5179, training loss: 9832.11, average training loss: 10837.36, base loss: 19722.54
[INFO 2017-06-27 22:32:17,356 main.py:51] epoch 5180, training loss: 9831.70, average training loss: 10836.44, base loss: 19720.09
[INFO 2017-06-27 22:32:18,332 main.py:51] epoch 5181, training loss: 10598.15, average training loss: 10835.89, base loss: 19719.92
[INFO 2017-06-27 22:32:19,302 main.py:51] epoch 5182, training loss: 10745.92, average training loss: 10836.29, base loss: 19722.24
[INFO 2017-06-27 22:32:20,280 main.py:51] epoch 5183, training loss: 11298.32, average training loss: 10837.41, base loss: 19725.65
[INFO 2017-06-27 22:32:21,256 main.py:51] epoch 5184, training loss: 10726.90, average training loss: 10837.02, base loss: 19724.91
[INFO 2017-06-27 22:32:22,346 main.py:51] epoch 5185, training loss: 11216.60, average training loss: 10837.83, base loss: 19728.68
[INFO 2017-06-27 22:32:23,431 main.py:51] epoch 5186, training loss: 11657.80, average training loss: 10838.26, base loss: 19731.63
[INFO 2017-06-27 22:32:24,437 main.py:51] epoch 5187, training loss: 9997.01, average training loss: 10837.91, base loss: 19731.16
[INFO 2017-06-27 22:32:25,412 main.py:51] epoch 5188, training loss: 10145.68, average training loss: 10836.20, base loss: 19727.22
[INFO 2017-06-27 22:32:26,388 main.py:51] epoch 5189, training loss: 9936.12, average training loss: 10836.88, base loss: 19730.13
[INFO 2017-06-27 22:32:27,359 main.py:51] epoch 5190, training loss: 10839.71, average training loss: 10837.35, base loss: 19731.43
[INFO 2017-06-27 22:32:28,334 main.py:51] epoch 5191, training loss: 10601.71, average training loss: 10836.94, base loss: 19731.63
[INFO 2017-06-27 22:32:29,309 main.py:51] epoch 5192, training loss: 10680.50, average training loss: 10834.18, base loss: 19726.01
[INFO 2017-06-27 22:32:30,279 main.py:51] epoch 5193, training loss: 11563.58, average training loss: 10835.12, base loss: 19729.69
[INFO 2017-06-27 22:32:31,361 main.py:51] epoch 5194, training loss: 9864.22, average training loss: 10835.45, base loss: 19730.10
[INFO 2017-06-27 22:32:32,352 main.py:51] epoch 5195, training loss: 10637.03, average training loss: 10834.06, base loss: 19727.44
[INFO 2017-06-27 22:32:33,436 main.py:51] epoch 5196, training loss: 9799.08, average training loss: 10832.95, base loss: 19725.18
[INFO 2017-06-27 22:32:34,465 main.py:51] epoch 5197, training loss: 10845.12, average training loss: 10832.74, base loss: 19725.32
[INFO 2017-06-27 22:32:35,465 main.py:51] epoch 5198, training loss: 11419.88, average training loss: 10834.01, base loss: 19728.35
[INFO 2017-06-27 22:32:36,535 main.py:51] epoch 5199, training loss: 10970.94, average training loss: 10835.17, base loss: 19732.51
[INFO 2017-06-27 22:32:36,535 main.py:53] epoch 5199, testing
[INFO 2017-06-27 22:32:40,966 main.py:105] average testing loss: 10502.85, base loss: 19626.34
[INFO 2017-06-27 22:32:40,966 main.py:106] improve_loss: 9123.50, improve_percent: 0.46
[INFO 2017-06-27 22:32:40,967 main.py:76] current best improved percent: 0.47
[INFO 2017-06-27 22:32:41,937 main.py:51] epoch 5200, training loss: 9509.50, average training loss: 10834.06, base loss: 19730.36
[INFO 2017-06-27 22:32:43,008 main.py:51] epoch 5201, training loss: 11000.06, average training loss: 10834.06, base loss: 19731.30
[INFO 2017-06-27 22:32:43,998 main.py:51] epoch 5202, training loss: 11472.95, average training loss: 10834.67, base loss: 19734.25
[INFO 2017-06-27 22:32:45,031 main.py:51] epoch 5203, training loss: 10236.83, average training loss: 10833.04, base loss: 19731.99
[INFO 2017-06-27 22:32:46,066 main.py:51] epoch 5204, training loss: 10609.82, average training loss: 10833.95, base loss: 19733.96
[INFO 2017-06-27 22:32:47,139 main.py:51] epoch 5205, training loss: 10636.41, average training loss: 10832.37, base loss: 19731.97
[INFO 2017-06-27 22:32:48,255 main.py:51] epoch 5206, training loss: 10193.22, average training loss: 10832.93, base loss: 19733.90
[INFO 2017-06-27 22:32:49,268 main.py:51] epoch 5207, training loss: 11504.29, average training loss: 10833.15, base loss: 19735.61
[INFO 2017-06-27 22:32:50,241 main.py:51] epoch 5208, training loss: 11624.37, average training loss: 10834.02, base loss: 19738.93
[INFO 2017-06-27 22:32:51,214 main.py:51] epoch 5209, training loss: 10216.01, average training loss: 10832.57, base loss: 19735.92
[INFO 2017-06-27 22:32:52,186 main.py:51] epoch 5210, training loss: 10868.36, average training loss: 10831.81, base loss: 19734.93
[INFO 2017-06-27 22:32:53,163 main.py:51] epoch 5211, training loss: 10643.94, average training loss: 10830.89, base loss: 19733.32
[INFO 2017-06-27 22:32:54,133 main.py:51] epoch 5212, training loss: 11035.19, average training loss: 10831.15, base loss: 19734.95
[INFO 2017-06-27 22:32:55,106 main.py:51] epoch 5213, training loss: 10504.00, average training loss: 10828.26, base loss: 19728.23
[INFO 2017-06-27 22:32:56,077 main.py:51] epoch 5214, training loss: 10832.60, average training loss: 10828.43, base loss: 19730.84
[INFO 2017-06-27 22:32:57,156 main.py:51] epoch 5215, training loss: 10722.15, average training loss: 10828.30, base loss: 19730.91
[INFO 2017-06-27 22:32:58,142 main.py:51] epoch 5216, training loss: 10292.32, average training loss: 10827.94, base loss: 19730.88
[INFO 2017-06-27 22:32:59,222 main.py:51] epoch 5217, training loss: 10429.53, average training loss: 10826.42, base loss: 19728.31
[INFO 2017-06-27 22:33:00,354 main.py:51] epoch 5218, training loss: 11604.38, average training loss: 10827.54, base loss: 19731.04
[INFO 2017-06-27 22:33:01,404 main.py:51] epoch 5219, training loss: 10717.31, average training loss: 10826.38, base loss: 19730.09
[INFO 2017-06-27 22:33:02,405 main.py:51] epoch 5220, training loss: 12755.85, average training loss: 10827.16, base loss: 19732.56
[INFO 2017-06-27 22:33:03,381 main.py:51] epoch 5221, training loss: 10879.18, average training loss: 10827.37, base loss: 19731.96
[INFO 2017-06-27 22:33:04,354 main.py:51] epoch 5222, training loss: 11001.69, average training loss: 10828.73, base loss: 19735.17
[INFO 2017-06-27 22:33:05,337 main.py:51] epoch 5223, training loss: 9561.33, average training loss: 10826.71, base loss: 19731.00
[INFO 2017-06-27 22:33:06,422 main.py:51] epoch 5224, training loss: 12192.10, average training loss: 10828.10, base loss: 19735.41
[INFO 2017-06-27 22:33:07,442 main.py:51] epoch 5225, training loss: 12069.11, average training loss: 10829.60, base loss: 19738.48
[INFO 2017-06-27 22:33:08,414 main.py:51] epoch 5226, training loss: 11439.36, average training loss: 10829.13, base loss: 19736.91
[INFO 2017-06-27 22:33:09,393 main.py:51] epoch 5227, training loss: 9928.63, average training loss: 10829.27, base loss: 19737.59
[INFO 2017-06-27 22:33:10,366 main.py:51] epoch 5228, training loss: 10341.83, average training loss: 10828.93, base loss: 19734.43
[INFO 2017-06-27 22:33:11,342 main.py:51] epoch 5229, training loss: 12810.56, average training loss: 10830.44, base loss: 19739.68
[INFO 2017-06-27 22:33:12,318 main.py:51] epoch 5230, training loss: 10629.68, average training loss: 10828.65, base loss: 19736.76
[INFO 2017-06-27 22:33:13,321 main.py:51] epoch 5231, training loss: 9512.04, average training loss: 10826.49, base loss: 19731.76
[INFO 2017-06-27 22:33:14,347 main.py:51] epoch 5232, training loss: 11276.27, average training loss: 10826.10, base loss: 19733.50
[INFO 2017-06-27 22:33:15,325 main.py:51] epoch 5233, training loss: 11016.16, average training loss: 10826.25, base loss: 19733.99
[INFO 2017-06-27 22:33:16,300 main.py:51] epoch 5234, training loss: 11044.94, average training loss: 10826.89, base loss: 19736.26
[INFO 2017-06-27 22:33:17,278 main.py:51] epoch 5235, training loss: 10551.13, average training loss: 10826.41, base loss: 19735.96
[INFO 2017-06-27 22:33:18,302 main.py:51] epoch 5236, training loss: 11134.21, average training loss: 10827.23, base loss: 19737.82
[INFO 2017-06-27 22:33:19,300 main.py:51] epoch 5237, training loss: 9710.13, average training loss: 10826.46, base loss: 19734.86
[INFO 2017-06-27 22:33:20,298 main.py:51] epoch 5238, training loss: 10404.17, average training loss: 10826.46, base loss: 19735.52
[INFO 2017-06-27 22:33:21,277 main.py:51] epoch 5239, training loss: 10594.35, average training loss: 10825.13, base loss: 19731.39
[INFO 2017-06-27 22:33:22,253 main.py:51] epoch 5240, training loss: 10959.13, average training loss: 10825.46, base loss: 19732.32
[INFO 2017-06-27 22:33:23,230 main.py:51] epoch 5241, training loss: 10670.11, average training loss: 10826.56, base loss: 19736.82
[INFO 2017-06-27 22:33:24,308 main.py:51] epoch 5242, training loss: 12441.80, average training loss: 10827.29, base loss: 19740.86
[INFO 2017-06-27 22:33:25,282 main.py:51] epoch 5243, training loss: 10790.29, average training loss: 10825.41, base loss: 19736.41
[INFO 2017-06-27 22:33:26,310 main.py:51] epoch 5244, training loss: 10872.99, average training loss: 10825.75, base loss: 19737.60
[INFO 2017-06-27 22:33:27,336 main.py:51] epoch 5245, training loss: 9864.65, average training loss: 10825.74, base loss: 19739.02
[INFO 2017-06-27 22:33:28,310 main.py:51] epoch 5246, training loss: 11993.99, average training loss: 10826.17, base loss: 19741.69
[INFO 2017-06-27 22:33:29,282 main.py:51] epoch 5247, training loss: 10886.00, average training loss: 10826.02, base loss: 19740.82
[INFO 2017-06-27 22:33:30,390 main.py:51] epoch 5248, training loss: 10567.23, average training loss: 10827.14, base loss: 19744.24
[INFO 2017-06-27 22:33:31,367 main.py:51] epoch 5249, training loss: 10369.59, average training loss: 10825.65, base loss: 19742.24
[INFO 2017-06-27 22:33:32,463 main.py:51] epoch 5250, training loss: 11056.48, average training loss: 10827.27, base loss: 19747.43
[INFO 2017-06-27 22:33:33,482 main.py:51] epoch 5251, training loss: 9719.89, average training loss: 10824.09, base loss: 19741.51
[INFO 2017-06-27 22:33:34,576 main.py:51] epoch 5252, training loss: 10668.92, average training loss: 10822.23, base loss: 19738.07
[INFO 2017-06-27 22:33:35,553 main.py:51] epoch 5253, training loss: 10646.53, average training loss: 10821.52, base loss: 19737.14
[INFO 2017-06-27 22:33:36,566 main.py:51] epoch 5254, training loss: 9883.70, average training loss: 10819.95, base loss: 19733.68
[INFO 2017-06-27 22:33:37,564 main.py:51] epoch 5255, training loss: 11072.73, average training loss: 10820.91, base loss: 19734.87
[INFO 2017-06-27 22:33:38,588 main.py:51] epoch 5256, training loss: 9406.14, average training loss: 10819.77, base loss: 19731.70
[INFO 2017-06-27 22:33:39,589 main.py:51] epoch 5257, training loss: 10200.13, average training loss: 10819.52, base loss: 19731.01
[INFO 2017-06-27 22:33:40,669 main.py:51] epoch 5258, training loss: 9982.16, average training loss: 10818.44, base loss: 19728.37
[INFO 2017-06-27 22:33:41,763 main.py:51] epoch 5259, training loss: 10818.86, average training loss: 10819.30, base loss: 19731.07
[INFO 2017-06-27 22:33:42,777 main.py:51] epoch 5260, training loss: 9452.22, average training loss: 10818.17, base loss: 19728.11
[INFO 2017-06-27 22:33:43,855 main.py:51] epoch 5261, training loss: 10408.32, average training loss: 10818.27, base loss: 19729.63
[INFO 2017-06-27 22:33:44,867 main.py:51] epoch 5262, training loss: 10593.95, average training loss: 10818.14, base loss: 19731.37
[INFO 2017-06-27 22:33:45,949 main.py:51] epoch 5263, training loss: 12341.02, average training loss: 10820.34, base loss: 19736.47
[INFO 2017-06-27 22:33:46,925 main.py:51] epoch 5264, training loss: 10319.61, average training loss: 10819.01, base loss: 19735.28
[INFO 2017-06-27 22:33:47,902 main.py:51] epoch 5265, training loss: 8968.56, average training loss: 10817.92, base loss: 19733.61
[INFO 2017-06-27 22:33:48,878 main.py:51] epoch 5266, training loss: 10142.63, average training loss: 10817.29, base loss: 19733.22
[INFO 2017-06-27 22:33:49,854 main.py:51] epoch 5267, training loss: 11483.61, average training loss: 10816.20, base loss: 19730.60
[INFO 2017-06-27 22:33:50,825 main.py:51] epoch 5268, training loss: 10698.60, average training loss: 10814.88, base loss: 19727.90
[INFO 2017-06-27 22:33:51,803 main.py:51] epoch 5269, training loss: 10434.74, average training loss: 10814.32, base loss: 19726.15
[INFO 2017-06-27 22:33:52,828 main.py:51] epoch 5270, training loss: 9594.66, average training loss: 10813.00, base loss: 19724.38
[INFO 2017-06-27 22:33:53,845 main.py:51] epoch 5271, training loss: 10038.33, average training loss: 10813.12, base loss: 19726.21
[INFO 2017-06-27 22:33:54,819 main.py:51] epoch 5272, training loss: 10936.55, average training loss: 10813.39, base loss: 19729.66
[INFO 2017-06-27 22:33:55,790 main.py:51] epoch 5273, training loss: 11000.75, average training loss: 10813.87, base loss: 19732.00
[INFO 2017-06-27 22:33:56,866 main.py:51] epoch 5274, training loss: 10539.43, average training loss: 10813.53, base loss: 19732.07
[INFO 2017-06-27 22:33:57,864 main.py:51] epoch 5275, training loss: 11801.24, average training loss: 10815.20, base loss: 19736.13
[INFO 2017-06-27 22:33:58,890 main.py:51] epoch 5276, training loss: 9864.81, average training loss: 10815.02, base loss: 19737.04
[INFO 2017-06-27 22:33:59,895 main.py:51] epoch 5277, training loss: 10032.17, average training loss: 10814.01, base loss: 19734.96
[INFO 2017-06-27 22:34:00,980 main.py:51] epoch 5278, training loss: 11009.67, average training loss: 10814.16, base loss: 19735.00
[INFO 2017-06-27 22:34:02,056 main.py:51] epoch 5279, training loss: 11641.51, average training loss: 10813.55, base loss: 19735.20
[INFO 2017-06-27 22:34:03,048 main.py:51] epoch 5280, training loss: 10108.87, average training loss: 10812.10, base loss: 19732.33
[INFO 2017-06-27 22:34:04,135 main.py:51] epoch 5281, training loss: 11779.07, average training loss: 10813.65, base loss: 19736.90
[INFO 2017-06-27 22:34:05,152 main.py:51] epoch 5282, training loss: 10983.93, average training loss: 10812.61, base loss: 19736.16
[INFO 2017-06-27 22:34:06,175 main.py:51] epoch 5283, training loss: 10535.08, average training loss: 10813.21, base loss: 19737.93
[INFO 2017-06-27 22:34:07,262 main.py:51] epoch 5284, training loss: 10831.49, average training loss: 10813.40, base loss: 19738.12
[INFO 2017-06-27 22:34:08,335 main.py:51] epoch 5285, training loss: 10107.22, average training loss: 10813.08, base loss: 19737.77
[INFO 2017-06-27 22:34:09,403 main.py:51] epoch 5286, training loss: 9890.97, average training loss: 10813.47, base loss: 19739.84
[INFO 2017-06-27 22:34:10,398 main.py:51] epoch 5287, training loss: 10705.62, average training loss: 10813.81, base loss: 19739.57
[INFO 2017-06-27 22:34:11,463 main.py:51] epoch 5288, training loss: 9886.50, average training loss: 10812.33, base loss: 19736.98
[INFO 2017-06-27 22:34:12,567 main.py:51] epoch 5289, training loss: 10463.83, average training loss: 10811.78, base loss: 19738.26
[INFO 2017-06-27 22:34:13,594 main.py:51] epoch 5290, training loss: 10531.77, average training loss: 10810.45, base loss: 19736.14
[INFO 2017-06-27 22:34:14,671 main.py:51] epoch 5291, training loss: 11340.74, average training loss: 10811.16, base loss: 19738.98
[INFO 2017-06-27 22:34:15,698 main.py:51] epoch 5292, training loss: 10371.16, average training loss: 10810.63, base loss: 19738.42
[INFO 2017-06-27 22:34:16,705 main.py:51] epoch 5293, training loss: 10451.02, average training loss: 10810.37, base loss: 19739.24
[INFO 2017-06-27 22:34:17,797 main.py:51] epoch 5294, training loss: 11010.09, average training loss: 10810.17, base loss: 19741.31
[INFO 2017-06-27 22:34:18,969 main.py:51] epoch 5295, training loss: 10932.83, average training loss: 10810.74, base loss: 19742.61
[INFO 2017-06-27 22:34:20,040 main.py:51] epoch 5296, training loss: 11033.72, average training loss: 10810.85, base loss: 19742.42
[INFO 2017-06-27 22:34:21,038 main.py:51] epoch 5297, training loss: 10070.54, average training loss: 10808.98, base loss: 19738.34
[INFO 2017-06-27 22:34:22,121 main.py:51] epoch 5298, training loss: 9680.30, average training loss: 10807.22, base loss: 19735.22
[INFO 2017-06-27 22:34:23,223 main.py:51] epoch 5299, training loss: 10305.00, average training loss: 10807.50, base loss: 19737.98
[INFO 2017-06-27 22:34:23,224 main.py:53] epoch 5299, testing
[INFO 2017-06-27 22:34:27,749 main.py:105] average testing loss: 11080.66, base loss: 20937.89
[INFO 2017-06-27 22:34:27,749 main.py:106] improve_loss: 9857.23, improve_percent: 0.47
[INFO 2017-06-27 22:34:27,749 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 22:34:27,763 main.py:76] current best improved percent: 0.47
[INFO 2017-06-27 22:34:28,835 main.py:51] epoch 5300, training loss: 11505.09, average training loss: 10808.71, base loss: 19740.26
[INFO 2017-06-27 22:34:29,844 main.py:51] epoch 5301, training loss: 11319.15, average training loss: 10809.16, base loss: 19739.48
[INFO 2017-06-27 22:34:30,905 main.py:51] epoch 5302, training loss: 11160.55, average training loss: 10810.42, base loss: 19743.25
[INFO 2017-06-27 22:34:31,946 main.py:51] epoch 5303, training loss: 9961.57, average training loss: 10809.73, base loss: 19743.31
[INFO 2017-06-27 22:34:32,951 main.py:51] epoch 5304, training loss: 9055.75, average training loss: 10808.01, base loss: 19739.01
[INFO 2017-06-27 22:34:34,024 main.py:51] epoch 5305, training loss: 11195.52, average training loss: 10807.62, base loss: 19737.97
[INFO 2017-06-27 22:34:35,058 main.py:51] epoch 5306, training loss: 10358.15, average training loss: 10806.20, base loss: 19734.54
[INFO 2017-06-27 22:34:36,059 main.py:51] epoch 5307, training loss: 12437.37, average training loss: 10808.30, base loss: 19739.73
[INFO 2017-06-27 22:34:37,130 main.py:51] epoch 5308, training loss: 10662.72, average training loss: 10807.58, base loss: 19738.44
[INFO 2017-06-27 22:34:38,199 main.py:51] epoch 5309, training loss: 11461.43, average training loss: 10808.99, base loss: 19742.72
[INFO 2017-06-27 22:34:39,195 main.py:51] epoch 5310, training loss: 11001.17, average training loss: 10808.52, base loss: 19741.74
[INFO 2017-06-27 22:34:40,287 main.py:51] epoch 5311, training loss: 10765.76, average training loss: 10808.35, base loss: 19742.41
[INFO 2017-06-27 22:34:41,304 main.py:51] epoch 5312, training loss: 10840.51, average training loss: 10806.17, base loss: 19736.19
[INFO 2017-06-27 22:34:42,397 main.py:51] epoch 5313, training loss: 11594.65, average training loss: 10807.16, base loss: 19739.76
[INFO 2017-06-27 22:34:43,475 main.py:51] epoch 5314, training loss: 10680.66, average training loss: 10807.53, base loss: 19743.60
[INFO 2017-06-27 22:34:44,509 main.py:51] epoch 5315, training loss: 10080.82, average training loss: 10807.48, base loss: 19742.54
[INFO 2017-06-27 22:34:45,543 main.py:51] epoch 5316, training loss: 9988.01, average training loss: 10806.68, base loss: 19741.07
[INFO 2017-06-27 22:34:46,648 main.py:51] epoch 5317, training loss: 11566.67, average training loss: 10809.09, base loss: 19747.44
[INFO 2017-06-27 22:34:47,701 main.py:51] epoch 5318, training loss: 10924.68, average training loss: 10808.65, base loss: 19747.75
[INFO 2017-06-27 22:34:48,708 main.py:51] epoch 5319, training loss: 9812.90, average training loss: 10808.20, base loss: 19747.53
[INFO 2017-06-27 22:34:49,678 main.py:51] epoch 5320, training loss: 12749.30, average training loss: 10808.90, base loss: 19749.47
[INFO 2017-06-27 22:34:50,754 main.py:51] epoch 5321, training loss: 9524.02, average training loss: 10806.79, base loss: 19745.18
[INFO 2017-06-27 22:34:51,823 main.py:51] epoch 5322, training loss: 11995.76, average training loss: 10807.35, base loss: 19747.47
[INFO 2017-06-27 22:34:52,893 main.py:51] epoch 5323, training loss: 9129.39, average training loss: 10805.91, base loss: 19744.65
[INFO 2017-06-27 22:34:53,886 main.py:51] epoch 5324, training loss: 12143.91, average training loss: 10807.87, base loss: 19750.37
[INFO 2017-06-27 22:34:54,873 main.py:51] epoch 5325, training loss: 9751.25, average training loss: 10807.20, base loss: 19749.92
[INFO 2017-06-27 22:34:55,848 main.py:51] epoch 5326, training loss: 9404.40, average training loss: 10806.64, base loss: 19746.77
[INFO 2017-06-27 22:34:56,822 main.py:51] epoch 5327, training loss: 10734.27, average training loss: 10807.02, base loss: 19747.43
[INFO 2017-06-27 22:34:57,797 main.py:51] epoch 5328, training loss: 11137.45, average training loss: 10808.17, base loss: 19749.68
[INFO 2017-06-27 22:34:58,780 main.py:51] epoch 5329, training loss: 10382.96, average training loss: 10809.27, base loss: 19751.83
[INFO 2017-06-27 22:34:59,763 main.py:51] epoch 5330, training loss: 13298.28, average training loss: 10812.22, base loss: 19757.54
[INFO 2017-06-27 22:35:00,862 main.py:51] epoch 5331, training loss: 11073.57, average training loss: 10813.17, base loss: 19759.91
[INFO 2017-06-27 22:35:01,996 main.py:51] epoch 5332, training loss: 11550.81, average training loss: 10813.14, base loss: 19759.70
[INFO 2017-06-27 22:35:03,058 main.py:51] epoch 5333, training loss: 10171.49, average training loss: 10811.64, base loss: 19756.08
[INFO 2017-06-27 22:35:04,229 main.py:51] epoch 5334, training loss: 11108.50, average training loss: 10810.00, base loss: 19753.65
[INFO 2017-06-27 22:35:05,221 main.py:51] epoch 5335, training loss: 11854.76, average training loss: 10811.78, base loss: 19757.94
[INFO 2017-06-27 22:35:06,199 main.py:51] epoch 5336, training loss: 11155.70, average training loss: 10813.89, base loss: 19763.19
[INFO 2017-06-27 22:35:07,219 main.py:51] epoch 5337, training loss: 10659.49, average training loss: 10814.64, base loss: 19764.97
[INFO 2017-06-27 22:35:08,212 main.py:51] epoch 5338, training loss: 10825.62, average training loss: 10814.02, base loss: 19761.90
[INFO 2017-06-27 22:35:09,286 main.py:51] epoch 5339, training loss: 9877.60, average training loss: 10814.05, base loss: 19761.72
[INFO 2017-06-27 22:35:10,312 main.py:51] epoch 5340, training loss: 11217.40, average training loss: 10814.45, base loss: 19762.71
[INFO 2017-06-27 22:35:11,311 main.py:51] epoch 5341, training loss: 11297.25, average training loss: 10813.83, base loss: 19761.28
[INFO 2017-06-27 22:35:12,354 main.py:51] epoch 5342, training loss: 10317.67, average training loss: 10813.52, base loss: 19761.79
[INFO 2017-06-27 22:35:13,464 main.py:51] epoch 5343, training loss: 11222.45, average training loss: 10813.35, base loss: 19761.44
[INFO 2017-06-27 22:35:14,497 main.py:51] epoch 5344, training loss: 11347.00, average training loss: 10813.67, base loss: 19762.56
[INFO 2017-06-27 22:35:15,501 main.py:51] epoch 5345, training loss: 10014.14, average training loss: 10814.66, base loss: 19764.23
[INFO 2017-06-27 22:35:16,576 main.py:51] epoch 5346, training loss: 10583.98, average training loss: 10812.67, base loss: 19759.25
[INFO 2017-06-27 22:35:17,549 main.py:51] epoch 5347, training loss: 10333.85, average training loss: 10812.36, base loss: 19759.97
[INFO 2017-06-27 22:35:18,576 main.py:51] epoch 5348, training loss: 10737.20, average training loss: 10812.26, base loss: 19760.56
[INFO 2017-06-27 22:35:19,594 main.py:51] epoch 5349, training loss: 9819.15, average training loss: 10810.89, base loss: 19759.09
[INFO 2017-06-27 22:35:20,664 main.py:51] epoch 5350, training loss: 12003.33, average training loss: 10812.20, base loss: 19761.52
[INFO 2017-06-27 22:35:21,700 main.py:51] epoch 5351, training loss: 9909.39, average training loss: 10810.39, base loss: 19757.56
[INFO 2017-06-27 22:35:22,714 main.py:51] epoch 5352, training loss: 9391.23, average training loss: 10808.39, base loss: 19752.89
[INFO 2017-06-27 22:35:23,690 main.py:51] epoch 5353, training loss: 11564.82, average training loss: 10810.06, base loss: 19756.30
[INFO 2017-06-27 22:35:24,664 main.py:51] epoch 5354, training loss: 12302.61, average training loss: 10811.51, base loss: 19759.72
[INFO 2017-06-27 22:35:25,727 main.py:51] epoch 5355, training loss: 10759.57, average training loss: 10811.21, base loss: 19758.31
[INFO 2017-06-27 22:35:26,785 main.py:51] epoch 5356, training loss: 10637.60, average training loss: 10810.42, base loss: 19756.79
[INFO 2017-06-27 22:35:27,800 main.py:51] epoch 5357, training loss: 10682.32, average training loss: 10811.12, base loss: 19758.16
[INFO 2017-06-27 22:35:28,870 main.py:51] epoch 5358, training loss: 10856.22, average training loss: 10811.29, base loss: 19759.26
[INFO 2017-06-27 22:35:29,903 main.py:51] epoch 5359, training loss: 9899.00, average training loss: 10811.88, base loss: 19762.48
[INFO 2017-06-27 22:35:30,914 main.py:51] epoch 5360, training loss: 10134.98, average training loss: 10810.21, base loss: 19757.80
[INFO 2017-06-27 22:35:31,989 main.py:51] epoch 5361, training loss: 10149.70, average training loss: 10809.94, base loss: 19757.71
[INFO 2017-06-27 22:35:33,060 main.py:51] epoch 5362, training loss: 12186.33, average training loss: 10811.76, base loss: 19761.91
[INFO 2017-06-27 22:35:34,079 main.py:51] epoch 5363, training loss: 9184.23, average training loss: 10809.77, base loss: 19756.65
[INFO 2017-06-27 22:35:35,094 main.py:51] epoch 5364, training loss: 12417.15, average training loss: 10811.53, base loss: 19759.80
[INFO 2017-06-27 22:35:36,109 main.py:51] epoch 5365, training loss: 9780.68, average training loss: 10810.99, base loss: 19759.88
[INFO 2017-06-27 22:35:37,175 main.py:51] epoch 5366, training loss: 10202.60, average training loss: 10811.37, base loss: 19761.59
[INFO 2017-06-27 22:35:38,210 main.py:51] epoch 5367, training loss: 9622.49, average training loss: 10808.02, base loss: 19754.71
[INFO 2017-06-27 22:35:39,246 main.py:51] epoch 5368, training loss: 11646.77, average training loss: 10808.59, base loss: 19755.76
[INFO 2017-06-27 22:35:40,275 main.py:51] epoch 5369, training loss: 9812.06, average training loss: 10805.65, base loss: 19748.74
[INFO 2017-06-27 22:35:41,345 main.py:51] epoch 5370, training loss: 10396.94, average training loss: 10803.02, base loss: 19743.56
[INFO 2017-06-27 22:35:42,390 main.py:51] epoch 5371, training loss: 11021.09, average training loss: 10802.02, base loss: 19742.60
[INFO 2017-06-27 22:35:43,387 main.py:51] epoch 5372, training loss: 8917.92, average training loss: 10799.85, base loss: 19738.70
[INFO 2017-06-27 22:35:44,470 main.py:51] epoch 5373, training loss: 10472.00, average training loss: 10800.02, base loss: 19739.53
[INFO 2017-06-27 22:35:45,487 main.py:51] epoch 5374, training loss: 11328.68, average training loss: 10800.56, base loss: 19740.11
[INFO 2017-06-27 22:35:46,568 main.py:51] epoch 5375, training loss: 10161.02, average training loss: 10798.78, base loss: 19735.80
[INFO 2017-06-27 22:35:47,588 main.py:51] epoch 5376, training loss: 11625.81, average training loss: 10799.12, base loss: 19736.98
[INFO 2017-06-27 22:35:48,656 main.py:51] epoch 5377, training loss: 10428.39, average training loss: 10798.32, base loss: 19734.42
[INFO 2017-06-27 22:35:49,707 main.py:51] epoch 5378, training loss: 10539.62, average training loss: 10797.18, base loss: 19731.93
[INFO 2017-06-27 22:35:50,730 main.py:51] epoch 5379, training loss: 10065.83, average training loss: 10796.54, base loss: 19730.44
[INFO 2017-06-27 22:35:51,832 main.py:51] epoch 5380, training loss: 11367.02, average training loss: 10797.10, base loss: 19732.87
[INFO 2017-06-27 22:35:52,934 main.py:51] epoch 5381, training loss: 9322.33, average training loss: 10795.44, base loss: 19731.86
[INFO 2017-06-27 22:35:53,968 main.py:51] epoch 5382, training loss: 9502.10, average training loss: 10793.78, base loss: 19728.81
[INFO 2017-06-27 22:35:54,964 main.py:51] epoch 5383, training loss: 10929.23, average training loss: 10794.32, base loss: 19731.28
[INFO 2017-06-27 22:35:56,022 main.py:51] epoch 5384, training loss: 9936.85, average training loss: 10793.44, base loss: 19729.60
[INFO 2017-06-27 22:35:57,056 main.py:51] epoch 5385, training loss: 9760.13, average training loss: 10791.53, base loss: 19727.50
[INFO 2017-06-27 22:35:58,048 main.py:51] epoch 5386, training loss: 10486.73, average training loss: 10791.81, base loss: 19728.36
[INFO 2017-06-27 22:35:59,120 main.py:51] epoch 5387, training loss: 10755.96, average training loss: 10790.19, base loss: 19724.55
[INFO 2017-06-27 22:36:00,160 main.py:51] epoch 5388, training loss: 10704.60, average training loss: 10789.65, base loss: 19724.40
[INFO 2017-06-27 22:36:01,171 main.py:51] epoch 5389, training loss: 9921.20, average training loss: 10786.87, base loss: 19716.70
[INFO 2017-06-27 22:36:02,244 main.py:51] epoch 5390, training loss: 10616.67, average training loss: 10785.87, base loss: 19715.04
[INFO 2017-06-27 22:36:03,310 main.py:51] epoch 5391, training loss: 10214.71, average training loss: 10786.84, base loss: 19719.30
[INFO 2017-06-27 22:36:04,368 main.py:51] epoch 5392, training loss: 11541.35, average training loss: 10787.92, base loss: 19724.21
[INFO 2017-06-27 22:36:05,358 main.py:51] epoch 5393, training loss: 11167.61, average training loss: 10789.87, base loss: 19730.60
[INFO 2017-06-27 22:36:06,415 main.py:51] epoch 5394, training loss: 10193.42, average training loss: 10788.91, base loss: 19727.71
[INFO 2017-06-27 22:36:07,409 main.py:51] epoch 5395, training loss: 11157.93, average training loss: 10788.17, base loss: 19726.36
[INFO 2017-06-27 22:36:08,384 main.py:51] epoch 5396, training loss: 10379.01, average training loss: 10787.92, base loss: 19724.80
[INFO 2017-06-27 22:36:09,358 main.py:51] epoch 5397, training loss: 10644.11, average training loss: 10788.39, base loss: 19727.33
[INFO 2017-06-27 22:36:10,334 main.py:51] epoch 5398, training loss: 11207.85, average training loss: 10788.56, base loss: 19728.78
[INFO 2017-06-27 22:36:11,305 main.py:51] epoch 5399, training loss: 10038.71, average training loss: 10788.32, base loss: 19730.30
[INFO 2017-06-27 22:36:11,306 main.py:53] epoch 5399, testing
[INFO 2017-06-27 22:36:15,515 main.py:105] average testing loss: 10330.51, base loss: 19622.22
[INFO 2017-06-27 22:36:15,515 main.py:106] improve_loss: 9291.71, improve_percent: 0.47
[INFO 2017-06-27 22:36:15,516 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 22:36:15,529 main.py:76] current best improved percent: 0.47
[INFO 2017-06-27 22:36:16,510 main.py:51] epoch 5400, training loss: 10873.57, average training loss: 10788.85, base loss: 19731.32
[INFO 2017-06-27 22:36:17,489 main.py:51] epoch 5401, training loss: 11866.59, average training loss: 10789.59, base loss: 19734.76
[INFO 2017-06-27 22:36:18,461 main.py:51] epoch 5402, training loss: 11106.76, average training loss: 10788.51, base loss: 19733.85
[INFO 2017-06-27 22:36:19,433 main.py:51] epoch 5403, training loss: 11427.81, average training loss: 10788.83, base loss: 19736.80
[INFO 2017-06-27 22:36:20,512 main.py:51] epoch 5404, training loss: 10209.10, average training loss: 10788.30, base loss: 19736.48
[INFO 2017-06-27 22:36:21,620 main.py:51] epoch 5405, training loss: 10563.97, average training loss: 10785.81, base loss: 19731.90
[INFO 2017-06-27 22:36:22,755 main.py:51] epoch 5406, training loss: 10186.71, average training loss: 10785.05, base loss: 19732.18
[INFO 2017-06-27 22:36:23,735 main.py:51] epoch 5407, training loss: 10905.13, average training loss: 10784.45, base loss: 19730.39
[INFO 2017-06-27 22:36:24,716 main.py:51] epoch 5408, training loss: 10716.96, average training loss: 10784.41, base loss: 19730.99
[INFO 2017-06-27 22:36:25,687 main.py:51] epoch 5409, training loss: 10069.72, average training loss: 10782.45, base loss: 19726.66
[INFO 2017-06-27 22:36:26,659 main.py:51] epoch 5410, training loss: 9820.60, average training loss: 10781.31, base loss: 19724.09
[INFO 2017-06-27 22:36:27,742 main.py:51] epoch 5411, training loss: 11449.43, average training loss: 10781.63, base loss: 19724.08
[INFO 2017-06-27 22:36:28,738 main.py:51] epoch 5412, training loss: 12400.36, average training loss: 10783.21, base loss: 19727.75
[INFO 2017-06-27 22:36:29,713 main.py:51] epoch 5413, training loss: 9459.14, average training loss: 10781.87, base loss: 19724.75
[INFO 2017-06-27 22:36:30,694 main.py:51] epoch 5414, training loss: 10142.27, average training loss: 10781.38, base loss: 19724.26
[INFO 2017-06-27 22:36:31,690 main.py:51] epoch 5415, training loss: 11750.91, average training loss: 10781.82, base loss: 19725.57
[INFO 2017-06-27 22:36:32,719 main.py:51] epoch 5416, training loss: 9478.77, average training loss: 10780.43, base loss: 19721.82
[INFO 2017-06-27 22:36:33,793 main.py:51] epoch 5417, training loss: 10849.09, average training loss: 10780.29, base loss: 19721.77
[INFO 2017-06-27 22:36:34,777 main.py:51] epoch 5418, training loss: 10529.13, average training loss: 10779.44, base loss: 19719.71
[INFO 2017-06-27 22:36:35,751 main.py:51] epoch 5419, training loss: 10652.81, average training loss: 10779.07, base loss: 19720.06
[INFO 2017-06-27 22:36:36,823 main.py:51] epoch 5420, training loss: 10781.77, average training loss: 10780.42, base loss: 19723.90
[INFO 2017-06-27 22:36:37,797 main.py:51] epoch 5421, training loss: 10905.81, average training loss: 10779.28, base loss: 19722.66
[INFO 2017-06-27 22:36:38,907 main.py:51] epoch 5422, training loss: 11425.56, average training loss: 10778.65, base loss: 19720.00
[INFO 2017-06-27 22:36:39,898 main.py:51] epoch 5423, training loss: 10262.86, average training loss: 10778.14, base loss: 19719.88
[INFO 2017-06-27 22:36:40,871 main.py:51] epoch 5424, training loss: 10516.13, average training loss: 10778.18, base loss: 19720.06
[INFO 2017-06-27 22:36:41,845 main.py:51] epoch 5425, training loss: 9501.05, average training loss: 10777.29, base loss: 19718.01
[INFO 2017-06-27 22:36:42,927 main.py:51] epoch 5426, training loss: 9400.90, average training loss: 10775.35, base loss: 19715.03
[INFO 2017-06-27 22:36:44,009 main.py:51] epoch 5427, training loss: 10906.25, average training loss: 10776.67, base loss: 19719.39
[INFO 2017-06-27 22:36:45,002 main.py:51] epoch 5428, training loss: 9807.77, average training loss: 10775.22, base loss: 19717.30
[INFO 2017-06-27 22:36:45,976 main.py:51] epoch 5429, training loss: 10535.08, average training loss: 10776.09, base loss: 19721.01
[INFO 2017-06-27 22:36:46,953 main.py:51] epoch 5430, training loss: 13116.78, average training loss: 10779.31, base loss: 19728.63
[INFO 2017-06-27 22:36:47,923 main.py:51] epoch 5431, training loss: 9616.29, average training loss: 10776.95, base loss: 19725.36
[INFO 2017-06-27 22:36:48,905 main.py:51] epoch 5432, training loss: 10714.84, average training loss: 10777.09, base loss: 19726.26
[INFO 2017-06-27 22:36:49,883 main.py:51] epoch 5433, training loss: 10842.24, average training loss: 10778.63, base loss: 19729.65
[INFO 2017-06-27 22:36:50,860 main.py:51] epoch 5434, training loss: 11603.89, average training loss: 10779.12, base loss: 19730.92
[INFO 2017-06-27 22:36:51,841 main.py:51] epoch 5435, training loss: 10505.76, average training loss: 10780.32, base loss: 19734.07
[INFO 2017-06-27 22:36:52,823 main.py:51] epoch 5436, training loss: 13082.19, average training loss: 10783.78, base loss: 19742.60
[INFO 2017-06-27 22:36:53,797 main.py:51] epoch 5437, training loss: 10340.74, average training loss: 10784.04, base loss: 19743.03
[INFO 2017-06-27 22:36:54,773 main.py:51] epoch 5438, training loss: 9657.91, average training loss: 10783.28, base loss: 19740.58
[INFO 2017-06-27 22:36:55,750 main.py:51] epoch 5439, training loss: 10809.84, average training loss: 10784.63, base loss: 19745.30
[INFO 2017-06-27 22:36:56,728 main.py:51] epoch 5440, training loss: 10024.91, average training loss: 10783.12, base loss: 19743.51
[INFO 2017-06-27 22:36:57,701 main.py:51] epoch 5441, training loss: 9871.74, average training loss: 10783.30, base loss: 19744.51
[INFO 2017-06-27 22:36:58,677 main.py:51] epoch 5442, training loss: 10556.10, average training loss: 10782.57, base loss: 19744.88
[INFO 2017-06-27 22:36:59,654 main.py:51] epoch 5443, training loss: 11322.10, average training loss: 10784.21, base loss: 19749.94
[INFO 2017-06-27 22:37:00,626 main.py:51] epoch 5444, training loss: 10824.25, average training loss: 10784.55, base loss: 19753.65
[INFO 2017-06-27 22:37:01,595 main.py:51] epoch 5445, training loss: 11375.56, average training loss: 10783.91, base loss: 19752.97
[INFO 2017-06-27 22:37:02,570 main.py:51] epoch 5446, training loss: 10200.37, average training loss: 10782.55, base loss: 19750.27
[INFO 2017-06-27 22:37:03,540 main.py:51] epoch 5447, training loss: 11233.64, average training loss: 10783.19, base loss: 19752.52
[INFO 2017-06-27 22:37:04,514 main.py:51] epoch 5448, training loss: 9119.82, average training loss: 10780.51, base loss: 19746.24
[INFO 2017-06-27 22:37:05,491 main.py:51] epoch 5449, training loss: 11191.46, average training loss: 10780.68, base loss: 19746.20
[INFO 2017-06-27 22:37:06,468 main.py:51] epoch 5450, training loss: 10704.46, average training loss: 10780.03, base loss: 19746.19
[INFO 2017-06-27 22:37:07,442 main.py:51] epoch 5451, training loss: 10272.90, average training loss: 10779.61, base loss: 19746.32
[INFO 2017-06-27 22:37:08,415 main.py:51] epoch 5452, training loss: 10301.04, average training loss: 10779.83, base loss: 19748.07
[INFO 2017-06-27 22:37:09,390 main.py:51] epoch 5453, training loss: 10051.87, average training loss: 10778.63, base loss: 19745.65
[INFO 2017-06-27 22:37:10,370 main.py:51] epoch 5454, training loss: 12032.20, average training loss: 10780.45, base loss: 19750.42
[INFO 2017-06-27 22:37:11,347 main.py:51] epoch 5455, training loss: 10486.38, average training loss: 10780.29, base loss: 19750.96
[INFO 2017-06-27 22:37:12,320 main.py:51] epoch 5456, training loss: 10216.13, average training loss: 10780.06, base loss: 19750.58
[INFO 2017-06-27 22:37:13,295 main.py:51] epoch 5457, training loss: 10418.83, average training loss: 10779.90, base loss: 19749.76
