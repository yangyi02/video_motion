[INFO 2017-06-28 17:59:51,372 main.py:176] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-06-28 17:59:55,187 main.py:51] epoch 0, training loss: 41954.23, average training loss: 41954.23, base loss: 21390.64
[INFO 2017-06-28 17:59:56,374 main.py:51] epoch 1, training loss: 36825.91, average training loss: 39390.07, base loss: 21481.69
[INFO 2017-06-28 17:59:57,403 main.py:51] epoch 2, training loss: 34638.79, average training loss: 37806.31, base loss: 21048.33
[INFO 2017-06-28 17:59:58,553 main.py:51] epoch 3, training loss: 31378.06, average training loss: 36199.25, base loss: 21616.55
[INFO 2017-06-28 17:59:59,661 main.py:51] epoch 4, training loss: 29356.29, average training loss: 34830.66, base loss: 21363.59
[INFO 2017-06-28 18:00:00,909 main.py:51] epoch 5, training loss: 27041.03, average training loss: 33532.38, base loss: 21237.26
[INFO 2017-06-28 18:00:02,013 main.py:51] epoch 6, training loss: 25854.62, average training loss: 32435.56, base loss: 20840.01
[INFO 2017-06-28 18:00:03,104 main.py:51] epoch 7, training loss: 24129.00, average training loss: 31397.24, base loss: 20777.92
[INFO 2017-06-28 18:00:04,173 main.py:51] epoch 8, training loss: 24353.11, average training loss: 30614.56, base loss: 20954.58
[INFO 2017-06-28 18:00:05,342 main.py:51] epoch 9, training loss: 24110.64, average training loss: 29964.17, base loss: 21157.38
[INFO 2017-06-28 18:00:06,544 main.py:51] epoch 10, training loss: 22769.82, average training loss: 29310.14, base loss: 21135.20
[INFO 2017-06-28 18:00:07,659 main.py:51] epoch 11, training loss: 21830.33, average training loss: 28686.82, base loss: 21036.98
[INFO 2017-06-28 18:00:08,790 main.py:51] epoch 12, training loss: 22551.56, average training loss: 28214.88, base loss: 21085.16
[INFO 2017-06-28 18:00:09,997 main.py:51] epoch 13, training loss: 20948.56, average training loss: 27695.85, base loss: 21035.38
[INFO 2017-06-28 18:00:11,304 main.py:51] epoch 14, training loss: 22259.58, average training loss: 27333.44, base loss: 21146.79
[INFO 2017-06-28 18:00:12,640 main.py:51] epoch 15, training loss: 20943.72, average training loss: 26934.08, base loss: 21171.32
[INFO 2017-06-28 18:00:13,994 main.py:51] epoch 16, training loss: 20444.00, average training loss: 26552.31, base loss: 21127.38
[INFO 2017-06-28 18:00:15,370 main.py:51] epoch 17, training loss: 19398.92, average training loss: 26154.90, base loss: 21054.89
[INFO 2017-06-28 18:00:16,796 main.py:51] epoch 18, training loss: 19989.58, average training loss: 25830.41, base loss: 21038.49
[INFO 2017-06-28 18:00:18,172 main.py:51] epoch 19, training loss: 19641.53, average training loss: 25520.96, base loss: 21017.14
[INFO 2017-06-28 18:00:19,551 main.py:51] epoch 20, training loss: 20246.59, average training loss: 25269.80, base loss: 21032.25
[INFO 2017-06-28 18:00:20,900 main.py:51] epoch 21, training loss: 19616.28, average training loss: 25012.83, base loss: 21038.45
[INFO 2017-06-28 18:00:22,272 main.py:51] epoch 22, training loss: 22170.06, average training loss: 24889.23, base loss: 21163.66
[INFO 2017-06-28 18:00:23,485 main.py:51] epoch 23, training loss: 19775.92, average training loss: 24676.17, base loss: 21194.94
[INFO 2017-06-28 18:00:24,833 main.py:51] epoch 24, training loss: 19391.04, average training loss: 24464.77, base loss: 21205.47
[INFO 2017-06-28 18:00:26,120 main.py:51] epoch 25, training loss: 19067.39, average training loss: 24257.18, base loss: 21188.83
[INFO 2017-06-28 18:00:27,462 main.py:51] epoch 26, training loss: 19327.59, average training loss: 24074.60, base loss: 21219.35
[INFO 2017-06-28 18:00:28,817 main.py:51] epoch 27, training loss: 20203.62, average training loss: 23936.35, base loss: 21271.77
[INFO 2017-06-28 18:00:30,193 main.py:51] epoch 28, training loss: 20425.57, average training loss: 23815.29, base loss: 21298.50
[INFO 2017-06-28 18:00:31,603 main.py:51] epoch 29, training loss: 18205.42, average training loss: 23628.29, base loss: 21260.01
[INFO 2017-06-28 18:00:32,883 main.py:51] epoch 30, training loss: 18845.01, average training loss: 23473.99, base loss: 21257.82
[INFO 2017-06-28 18:00:34,280 main.py:51] epoch 31, training loss: 20460.02, average training loss: 23379.81, base loss: 21314.32
[INFO 2017-06-28 18:00:35,695 main.py:51] epoch 32, training loss: 18025.79, average training loss: 23217.56, base loss: 21274.00
[INFO 2017-06-28 18:00:37,047 main.py:51] epoch 33, training loss: 19370.85, average training loss: 23104.42, base loss: 21302.20
[INFO 2017-06-28 18:00:38,378 main.py:51] epoch 34, training loss: 17801.60, average training loss: 22952.91, base loss: 21267.09
[INFO 2017-06-28 18:00:39,743 main.py:51] epoch 35, training loss: 19960.03, average training loss: 22869.78, base loss: 21299.50
[INFO 2017-06-28 18:00:41,115 main.py:51] epoch 36, training loss: 18540.29, average training loss: 22752.77, base loss: 21284.92
[INFO 2017-06-28 18:00:42,495 main.py:51] epoch 37, training loss: 21054.77, average training loss: 22708.08, base loss: 21354.50
[INFO 2017-06-28 18:00:43,818 main.py:51] epoch 38, training loss: 18002.06, average training loss: 22587.41, base loss: 21323.80
[INFO 2017-06-28 18:00:45,348 main.py:51] epoch 39, training loss: 19155.85, average training loss: 22501.63, base loss: 21327.09
[INFO 2017-06-28 18:00:46,703 main.py:51] epoch 40, training loss: 19036.95, average training loss: 22417.12, base loss: 21349.88
[INFO 2017-06-28 18:00:48,036 main.py:51] epoch 41, training loss: 21289.70, average training loss: 22390.28, base loss: 21426.06
[INFO 2017-06-28 18:00:49,382 main.py:51] epoch 42, training loss: 20499.49, average training loss: 22346.31, base loss: 21483.33
[INFO 2017-06-28 18:00:50,771 main.py:51] epoch 43, training loss: 19241.60, average training loss: 22275.74, base loss: 21511.23
[INFO 2017-06-28 18:00:52,099 main.py:51] epoch 44, training loss: 19561.24, average training loss: 22215.42, base loss: 21532.68
[INFO 2017-06-28 18:00:53,445 main.py:51] epoch 45, training loss: 18997.30, average training loss: 22145.46, base loss: 21559.13
[INFO 2017-06-28 18:00:54,763 main.py:51] epoch 46, training loss: 16411.07, average training loss: 22023.45, base loss: 21516.35
[INFO 2017-06-28 18:00:56,085 main.py:51] epoch 47, training loss: 16721.88, average training loss: 21913.01, base loss: 21480.50
[INFO 2017-06-28 18:00:57,349 main.py:51] epoch 48, training loss: 18436.38, average training loss: 21842.05, base loss: 21503.93
[INFO 2017-06-28 18:00:58,749 main.py:51] epoch 49, training loss: 17099.93, average training loss: 21747.21, base loss: 21485.50
[INFO 2017-06-28 18:01:00,111 main.py:51] epoch 50, training loss: 20525.08, average training loss: 21723.25, base loss: 21540.70
[INFO 2017-06-28 18:01:01,453 main.py:51] epoch 51, training loss: 16811.05, average training loss: 21628.78, base loss: 21526.44
[INFO 2017-06-28 18:01:02,846 main.py:51] epoch 52, training loss: 17374.04, average training loss: 21548.50, base loss: 21513.28
[INFO 2017-06-28 18:01:04,176 main.py:51] epoch 53, training loss: 17635.74, average training loss: 21476.05, base loss: 21507.42
[INFO 2017-06-28 18:01:05,459 main.py:51] epoch 54, training loss: 20369.30, average training loss: 21455.92, base loss: 21560.89
[INFO 2017-06-28 18:01:06,864 main.py:51] epoch 55, training loss: 16460.23, average training loss: 21366.71, base loss: 21528.09
[INFO 2017-06-28 18:01:08,230 main.py:51] epoch 56, training loss: 17876.36, average training loss: 21305.48, base loss: 21531.29
[INFO 2017-06-28 18:01:09,525 main.py:51] epoch 57, training loss: 20344.38, average training loss: 21288.91, base loss: 21574.28
[INFO 2017-06-28 18:01:10,897 main.py:51] epoch 58, training loss: 18125.53, average training loss: 21235.29, base loss: 21581.42
[INFO 2017-06-28 18:01:12,353 main.py:51] epoch 59, training loss: 17330.23, average training loss: 21170.21, base loss: 21567.52
[INFO 2017-06-28 18:01:13,727 main.py:51] epoch 60, training loss: 17073.23, average training loss: 21103.05, base loss: 21556.21
[INFO 2017-06-28 18:01:15,043 main.py:51] epoch 61, training loss: 15489.10, average training loss: 21012.50, base loss: 21516.87
[INFO 2017-06-28 18:01:16,448 main.py:51] epoch 62, training loss: 15832.95, average training loss: 20930.28, base loss: 21484.05
[INFO 2017-06-28 18:01:17,871 main.py:51] epoch 63, training loss: 16361.17, average training loss: 20858.89, base loss: 21465.07
[INFO 2017-06-28 18:01:19,213 main.py:51] epoch 64, training loss: 19471.75, average training loss: 20837.55, base loss: 21491.64
[INFO 2017-06-28 18:01:20,527 main.py:51] epoch 65, training loss: 16493.00, average training loss: 20771.72, base loss: 21470.63
[INFO 2017-06-28 18:01:21,923 main.py:51] epoch 66, training loss: 15960.21, average training loss: 20699.91, base loss: 21438.67
[INFO 2017-06-28 18:01:23,281 main.py:51] epoch 67, training loss: 17807.85, average training loss: 20657.38, base loss: 21443.00
[INFO 2017-06-28 18:01:24,624 main.py:51] epoch 68, training loss: 16665.30, average training loss: 20599.52, base loss: 21432.22
[INFO 2017-06-28 18:01:26,178 main.py:51] epoch 69, training loss: 17654.81, average training loss: 20557.46, base loss: 21442.46
[INFO 2017-06-28 18:01:27,483 main.py:51] epoch 70, training loss: 18448.73, average training loss: 20527.76, base loss: 21467.96
[INFO 2017-06-28 18:01:28,850 main.py:51] epoch 71, training loss: 17474.05, average training loss: 20485.34, base loss: 21472.69
[INFO 2017-06-28 18:01:30,144 main.py:51] epoch 72, training loss: 16038.97, average training loss: 20424.43, base loss: 21448.44
[INFO 2017-06-28 18:01:31,537 main.py:51] epoch 73, training loss: 17164.58, average training loss: 20380.38, base loss: 21448.36
[INFO 2017-06-28 18:01:32,859 main.py:51] epoch 74, training loss: 17175.51, average training loss: 20337.65, base loss: 21445.90
[INFO 2017-06-28 18:01:34,189 main.py:51] epoch 75, training loss: 16216.44, average training loss: 20283.42, base loss: 21446.35
[INFO 2017-06-28 18:01:35,571 main.py:51] epoch 76, training loss: 15275.45, average training loss: 20218.38, base loss: 21427.02
[INFO 2017-06-28 18:01:36,915 main.py:51] epoch 77, training loss: 15894.97, average training loss: 20162.96, base loss: 21409.46
[INFO 2017-06-28 18:01:38,226 main.py:51] epoch 78, training loss: 17820.46, average training loss: 20133.30, base loss: 21413.01
[INFO 2017-06-28 18:01:39,566 main.py:51] epoch 79, training loss: 17641.91, average training loss: 20102.16, base loss: 21415.66
[INFO 2017-06-28 18:01:40,922 main.py:51] epoch 80, training loss: 19191.71, average training loss: 20090.92, base loss: 21455.47
[INFO 2017-06-28 18:01:42,242 main.py:51] epoch 81, training loss: 15775.05, average training loss: 20038.29, base loss: 21431.57
[INFO 2017-06-28 18:01:43,516 main.py:51] epoch 82, training loss: 15960.02, average training loss: 19989.15, base loss: 21414.87
[INFO 2017-06-28 18:01:44,810 main.py:51] epoch 83, training loss: 16981.22, average training loss: 19953.34, base loss: 21428.63
[INFO 2017-06-28 18:01:46,148 main.py:51] epoch 84, training loss: 17627.86, average training loss: 19925.99, base loss: 21425.88
[INFO 2017-06-28 18:01:47,435 main.py:51] epoch 85, training loss: 17538.00, average training loss: 19898.22, base loss: 21436.65
[INFO 2017-06-28 18:01:48,826 main.py:51] epoch 86, training loss: 16451.13, average training loss: 19858.60, base loss: 21425.10
[INFO 2017-06-28 18:01:50,133 main.py:51] epoch 87, training loss: 17080.08, average training loss: 19827.02, base loss: 21427.77
[INFO 2017-06-28 18:01:51,445 main.py:51] epoch 88, training loss: 15967.67, average training loss: 19783.66, base loss: 21415.02
[INFO 2017-06-28 18:01:52,753 main.py:51] epoch 89, training loss: 17668.55, average training loss: 19760.16, base loss: 21429.39
[INFO 2017-06-28 18:01:54,196 main.py:51] epoch 90, training loss: 17402.32, average training loss: 19734.25, base loss: 21429.60
[INFO 2017-06-28 18:01:55,579 main.py:51] epoch 91, training loss: 16592.30, average training loss: 19700.10, base loss: 21427.46
[INFO 2017-06-28 18:01:57,027 main.py:51] epoch 92, training loss: 17744.57, average training loss: 19679.07, base loss: 21452.10
[INFO 2017-06-28 18:01:58,352 main.py:51] epoch 93, training loss: 16971.26, average training loss: 19650.26, base loss: 21454.83
[INFO 2017-06-28 18:01:59,714 main.py:51] epoch 94, training loss: 16827.04, average training loss: 19620.54, base loss: 21459.30
[INFO 2017-06-28 18:02:01,131 main.py:51] epoch 95, training loss: 16971.43, average training loss: 19592.95, base loss: 21455.58
[INFO 2017-06-28 18:02:02,380 main.py:51] epoch 96, training loss: 18051.70, average training loss: 19577.06, base loss: 21466.10
[INFO 2017-06-28 18:02:03,724 main.py:51] epoch 97, training loss: 16003.09, average training loss: 19540.59, base loss: 21448.34
[INFO 2017-06-28 18:02:05,111 main.py:51] epoch 98, training loss: 16556.24, average training loss: 19510.45, base loss: 21450.21
[INFO 2017-06-28 18:02:06,443 main.py:51] epoch 99, training loss: 17043.92, average training loss: 19485.78, base loss: 21452.99
[INFO 2017-06-28 18:02:06,443 main.py:53] epoch 99, testing
[INFO 2017-06-28 18:02:12,698 main.py:105] average testing loss: 18390.79, base loss: 22350.54
[INFO 2017-06-28 18:02:12,698 main.py:106] improve_loss: 3959.76, improve_percent: 0.18
[INFO 2017-06-28 18:02:12,699 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:02:12,711 main.py:76] current best improved percent: 0.18
[INFO 2017-06-28 18:02:14,054 main.py:51] epoch 100, training loss: 16300.65, average training loss: 19454.25, base loss: 21451.74
[INFO 2017-06-28 18:02:15,409 main.py:51] epoch 101, training loss: 16349.21, average training loss: 19423.80, base loss: 21450.60
[INFO 2017-06-28 18:02:16,653 main.py:51] epoch 102, training loss: 16840.78, average training loss: 19398.73, base loss: 21448.14
[INFO 2017-06-28 18:02:17,991 main.py:51] epoch 103, training loss: 16027.89, average training loss: 19366.31, base loss: 21442.06
[INFO 2017-06-28 18:02:19,340 main.py:51] epoch 104, training loss: 16864.50, average training loss: 19342.49, base loss: 21445.50
[INFO 2017-06-28 18:02:20,634 main.py:51] epoch 105, training loss: 17877.99, average training loss: 19328.67, base loss: 21462.49
[INFO 2017-06-28 18:02:21,992 main.py:51] epoch 106, training loss: 15517.83, average training loss: 19293.06, base loss: 21446.36
[INFO 2017-06-28 18:02:23,448 main.py:51] epoch 107, training loss: 15493.41, average training loss: 19257.87, base loss: 21432.70
[INFO 2017-06-28 18:02:24,749 main.py:51] epoch 108, training loss: 17366.37, average training loss: 19240.52, base loss: 21446.74
[INFO 2017-06-28 18:02:26,060 main.py:51] epoch 109, training loss: 16719.23, average training loss: 19217.60, base loss: 21449.46
[INFO 2017-06-28 18:02:27,304 main.py:51] epoch 110, training loss: 16540.13, average training loss: 19193.48, base loss: 21459.62
[INFO 2017-06-28 18:02:28,665 main.py:51] epoch 111, training loss: 18191.19, average training loss: 19184.53, base loss: 21472.06
[INFO 2017-06-28 18:02:29,893 main.py:51] epoch 112, training loss: 16356.93, average training loss: 19159.51, base loss: 21462.71
[INFO 2017-06-28 18:02:31,315 main.py:51] epoch 113, training loss: 16792.18, average training loss: 19138.74, base loss: 21466.79
[INFO 2017-06-28 18:02:32,611 main.py:51] epoch 114, training loss: 16592.99, average training loss: 19116.60, base loss: 21465.53
[INFO 2017-06-28 18:02:33,888 main.py:51] epoch 115, training loss: 14864.34, average training loss: 19079.95, base loss: 21449.06
[INFO 2017-06-28 18:02:35,139 main.py:51] epoch 116, training loss: 17579.16, average training loss: 19067.12, base loss: 21453.49
[INFO 2017-06-28 18:02:36,394 main.py:51] epoch 117, training loss: 15302.50, average training loss: 19035.22, base loss: 21438.57
[INFO 2017-06-28 18:02:37,778 main.py:51] epoch 118, training loss: 17179.69, average training loss: 19019.62, base loss: 21448.66
[INFO 2017-06-28 18:02:39,151 main.py:51] epoch 119, training loss: 17805.14, average training loss: 19009.50, base loss: 21463.18
[INFO 2017-06-28 18:02:40,491 main.py:51] epoch 120, training loss: 17749.60, average training loss: 18999.09, base loss: 21472.08
[INFO 2017-06-28 18:02:41,779 main.py:51] epoch 121, training loss: 17671.06, average training loss: 18988.20, base loss: 21477.13
[INFO 2017-06-28 18:02:43,128 main.py:51] epoch 122, training loss: 17197.80, average training loss: 18973.65, base loss: 21482.69
[INFO 2017-06-28 18:02:44,516 main.py:51] epoch 123, training loss: 15801.02, average training loss: 18948.06, base loss: 21479.18
[INFO 2017-06-28 18:02:46,014 main.py:51] epoch 124, training loss: 15721.72, average training loss: 18922.25, base loss: 21475.06
[INFO 2017-06-28 18:02:47,502 main.py:51] epoch 125, training loss: 17093.82, average training loss: 18907.74, base loss: 21486.95
[INFO 2017-06-28 18:02:48,926 main.py:51] epoch 126, training loss: 16136.51, average training loss: 18885.92, base loss: 21480.58
[INFO 2017-06-28 18:02:50,299 main.py:51] epoch 127, training loss: 16494.22, average training loss: 18867.23, base loss: 21485.97
[INFO 2017-06-28 18:02:51,663 main.py:51] epoch 128, training loss: 14839.03, average training loss: 18836.01, base loss: 21471.09
[INFO 2017-06-28 18:02:52,989 main.py:51] epoch 129, training loss: 14979.40, average training loss: 18806.34, base loss: 21459.24
[INFO 2017-06-28 18:02:54,448 main.py:51] epoch 130, training loss: 16427.72, average training loss: 18788.18, base loss: 21460.45
[INFO 2017-06-28 18:02:55,794 main.py:51] epoch 131, training loss: 16496.63, average training loss: 18770.82, base loss: 21461.72
[INFO 2017-06-28 18:02:57,092 main.py:51] epoch 132, training loss: 17093.38, average training loss: 18758.21, base loss: 21474.93
[INFO 2017-06-28 18:02:58,393 main.py:51] epoch 133, training loss: 15816.34, average training loss: 18736.26, base loss: 21470.79
[INFO 2017-06-28 18:02:59,740 main.py:51] epoch 134, training loss: 14746.16, average training loss: 18706.70, base loss: 21452.65
[INFO 2017-06-28 18:03:01,090 main.py:51] epoch 135, training loss: 15563.51, average training loss: 18683.59, base loss: 21443.30
[INFO 2017-06-28 18:03:02,379 main.py:51] epoch 136, training loss: 14991.81, average training loss: 18656.64, base loss: 21437.25
[INFO 2017-06-28 18:03:03,631 main.py:51] epoch 137, training loss: 16720.85, average training loss: 18642.61, base loss: 21450.17
[INFO 2017-06-28 18:03:04,888 main.py:51] epoch 138, training loss: 15815.40, average training loss: 18622.28, base loss: 21455.45
[INFO 2017-06-28 18:03:06,117 main.py:51] epoch 139, training loss: 15269.89, average training loss: 18598.33, base loss: 21447.78
[INFO 2017-06-28 18:03:07,455 main.py:51] epoch 140, training loss: 16444.35, average training loss: 18583.05, base loss: 21455.35
[INFO 2017-06-28 18:03:08,839 main.py:51] epoch 141, training loss: 14644.48, average training loss: 18555.32, base loss: 21446.73
[INFO 2017-06-28 18:03:10,175 main.py:51] epoch 142, training loss: 15184.91, average training loss: 18531.75, base loss: 21436.30
[INFO 2017-06-28 18:03:11,458 main.py:51] epoch 143, training loss: 14923.47, average training loss: 18506.69, base loss: 21445.61
[INFO 2017-06-28 18:03:12,746 main.py:51] epoch 144, training loss: 14537.40, average training loss: 18479.32, base loss: 21435.77
[INFO 2017-06-28 18:03:14,137 main.py:51] epoch 145, training loss: 16662.74, average training loss: 18466.87, base loss: 21444.21
[INFO 2017-06-28 18:03:15,471 main.py:51] epoch 146, training loss: 15869.00, average training loss: 18449.20, base loss: 21454.30
[INFO 2017-06-28 18:03:16,748 main.py:51] epoch 147, training loss: 17203.76, average training loss: 18440.79, base loss: 21472.31
[INFO 2017-06-28 18:03:18,059 main.py:51] epoch 148, training loss: 16949.51, average training loss: 18430.78, base loss: 21478.16
[INFO 2017-06-28 18:03:19,371 main.py:51] epoch 149, training loss: 15442.30, average training loss: 18410.85, base loss: 21475.38
[INFO 2017-06-28 18:03:20,676 main.py:51] epoch 150, training loss: 15725.38, average training loss: 18393.07, base loss: 21467.56
[INFO 2017-06-28 18:03:22,073 main.py:51] epoch 151, training loss: 14746.98, average training loss: 18369.08, base loss: 21452.57
[INFO 2017-06-28 18:03:23,453 main.py:51] epoch 152, training loss: 16343.42, average training loss: 18355.84, base loss: 21465.97
[INFO 2017-06-28 18:03:24,754 main.py:51] epoch 153, training loss: 15835.47, average training loss: 18339.48, base loss: 21471.43
[INFO 2017-06-28 18:03:26,098 main.py:51] epoch 154, training loss: 15986.99, average training loss: 18324.30, base loss: 21473.85
[INFO 2017-06-28 18:03:27,400 main.py:51] epoch 155, training loss: 15545.10, average training loss: 18306.48, base loss: 21468.86
[INFO 2017-06-28 18:03:28,744 main.py:51] epoch 156, training loss: 16289.35, average training loss: 18293.64, base loss: 21479.23
[INFO 2017-06-28 18:03:30,035 main.py:51] epoch 157, training loss: 15581.74, average training loss: 18276.47, base loss: 21481.26
[INFO 2017-06-28 18:03:31,541 main.py:51] epoch 158, training loss: 17388.40, average training loss: 18270.89, base loss: 21490.44
[INFO 2017-06-28 18:03:32,887 main.py:51] epoch 159, training loss: 16607.33, average training loss: 18260.49, base loss: 21497.49
[INFO 2017-06-28 18:03:34,267 main.py:51] epoch 160, training loss: 14942.04, average training loss: 18239.88, base loss: 21488.73
[INFO 2017-06-28 18:03:35,565 main.py:51] epoch 161, training loss: 15664.15, average training loss: 18223.98, base loss: 21490.94
[INFO 2017-06-28 18:03:36,838 main.py:51] epoch 162, training loss: 14919.62, average training loss: 18203.71, base loss: 21481.77
[INFO 2017-06-28 18:03:38,303 main.py:51] epoch 163, training loss: 15592.82, average training loss: 18187.79, base loss: 21478.31
[INFO 2017-06-28 18:03:39,555 main.py:51] epoch 164, training loss: 14406.65, average training loss: 18164.87, base loss: 21473.05
[INFO 2017-06-28 18:03:40,837 main.py:51] epoch 165, training loss: 14492.62, average training loss: 18142.75, base loss: 21467.67
[INFO 2017-06-28 18:03:42,167 main.py:51] epoch 166, training loss: 17736.39, average training loss: 18140.31, base loss: 21481.19
[INFO 2017-06-28 18:03:43,450 main.py:51] epoch 167, training loss: 14674.70, average training loss: 18119.69, base loss: 21478.52
[INFO 2017-06-28 18:03:44,788 main.py:51] epoch 168, training loss: 15755.40, average training loss: 18105.70, base loss: 21480.07
[INFO 2017-06-28 18:03:46,254 main.py:51] epoch 169, training loss: 15449.34, average training loss: 18090.07, base loss: 21480.61
[INFO 2017-06-28 18:03:47,740 main.py:51] epoch 170, training loss: 15653.34, average training loss: 18075.82, base loss: 21478.45
[INFO 2017-06-28 18:03:48,940 main.py:51] epoch 171, training loss: 15358.55, average training loss: 18060.02, base loss: 21477.38
[INFO 2017-06-28 18:03:50,457 main.py:51] epoch 172, training loss: 15271.47, average training loss: 18043.90, base loss: 21470.08
[INFO 2017-06-28 18:03:51,787 main.py:51] epoch 173, training loss: 14768.50, average training loss: 18025.08, base loss: 21468.30
[INFO 2017-06-28 18:03:53,067 main.py:51] epoch 174, training loss: 15323.69, average training loss: 18009.64, base loss: 21465.94
[INFO 2017-06-28 18:03:54,345 main.py:51] epoch 175, training loss: 16427.23, average training loss: 18000.65, base loss: 21470.48
[INFO 2017-06-28 18:03:55,648 main.py:51] epoch 176, training loss: 16599.93, average training loss: 17992.74, base loss: 21484.08
[INFO 2017-06-28 18:03:57,031 main.py:51] epoch 177, training loss: 15189.77, average training loss: 17976.99, base loss: 21482.91
[INFO 2017-06-28 18:03:58,377 main.py:51] epoch 178, training loss: 16384.93, average training loss: 17968.10, base loss: 21497.03
[INFO 2017-06-28 18:03:59,677 main.py:51] epoch 179, training loss: 15463.25, average training loss: 17954.18, base loss: 21502.81
[INFO 2017-06-28 18:04:00,979 main.py:51] epoch 180, training loss: 14503.31, average training loss: 17935.12, base loss: 21496.50
[INFO 2017-06-28 18:04:02,253 main.py:51] epoch 181, training loss: 14752.08, average training loss: 17917.63, base loss: 21492.40
[INFO 2017-06-28 18:04:03,642 main.py:51] epoch 182, training loss: 16891.36, average training loss: 17912.02, base loss: 21504.85
[INFO 2017-06-28 18:04:04,935 main.py:51] epoch 183, training loss: 15320.68, average training loss: 17897.93, base loss: 21508.90
[INFO 2017-06-28 18:04:06,282 main.py:51] epoch 184, training loss: 14446.75, average training loss: 17879.28, base loss: 21501.79
[INFO 2017-06-28 18:04:07,553 main.py:51] epoch 185, training loss: 15680.38, average training loss: 17867.46, base loss: 21498.29
[INFO 2017-06-28 18:04:08,804 main.py:51] epoch 186, training loss: 16012.17, average training loss: 17857.54, base loss: 21506.86
[INFO 2017-06-28 18:04:09,989 main.py:51] epoch 187, training loss: 15244.97, average training loss: 17843.64, base loss: 21505.98
[INFO 2017-06-28 18:04:11,388 main.py:51] epoch 188, training loss: 14389.50, average training loss: 17825.36, base loss: 21499.65
[INFO 2017-06-28 18:04:12,777 main.py:51] epoch 189, training loss: 15712.15, average training loss: 17814.24, base loss: 21503.31
[INFO 2017-06-28 18:04:14,133 main.py:51] epoch 190, training loss: 15774.20, average training loss: 17803.56, base loss: 21509.86
[INFO 2017-06-28 18:04:15,497 main.py:51] epoch 191, training loss: 14630.78, average training loss: 17787.04, base loss: 21506.12
[INFO 2017-06-28 18:04:16,849 main.py:51] epoch 192, training loss: 15662.96, average training loss: 17776.03, base loss: 21506.48
[INFO 2017-06-28 18:04:18,205 main.py:51] epoch 193, training loss: 15328.78, average training loss: 17763.42, base loss: 21498.28
[INFO 2017-06-28 18:04:19,637 main.py:51] epoch 194, training loss: 15988.07, average training loss: 17754.31, base loss: 21507.90
[INFO 2017-06-28 18:04:21,068 main.py:51] epoch 195, training loss: 17624.12, average training loss: 17753.65, base loss: 21525.85
[INFO 2017-06-28 18:04:22,379 main.py:51] epoch 196, training loss: 16459.68, average training loss: 17747.08, base loss: 21536.38
[INFO 2017-06-28 18:04:23,651 main.py:51] epoch 197, training loss: 13857.64, average training loss: 17727.44, base loss: 21529.06
[INFO 2017-06-28 18:04:24,853 main.py:51] epoch 198, training loss: 15012.51, average training loss: 17713.79, base loss: 21529.06
[INFO 2017-06-28 18:04:26,419 main.py:51] epoch 199, training loss: 14461.26, average training loss: 17697.53, base loss: 21525.08
[INFO 2017-06-28 18:04:26,420 main.py:53] epoch 199, testing
[INFO 2017-06-28 18:04:32,589 main.py:105] average testing loss: 15660.59, base loss: 20521.81
[INFO 2017-06-28 18:04:32,589 main.py:106] improve_loss: 4861.22, improve_percent: 0.24
[INFO 2017-06-28 18:04:32,590 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:04:32,603 main.py:76] current best improved percent: 0.24
[INFO 2017-06-28 18:04:33,868 main.py:51] epoch 200, training loss: 14707.39, average training loss: 17682.65, base loss: 21522.29
[INFO 2017-06-28 18:04:35,252 main.py:51] epoch 201, training loss: 14504.45, average training loss: 17666.92, base loss: 21518.12
[INFO 2017-06-28 18:04:36,509 main.py:51] epoch 202, training loss: 13978.55, average training loss: 17648.75, base loss: 21506.09
[INFO 2017-06-28 18:04:37,952 main.py:51] epoch 203, training loss: 16098.56, average training loss: 17641.15, base loss: 21512.11
[INFO 2017-06-28 18:04:39,148 main.py:51] epoch 204, training loss: 15975.64, average training loss: 17633.03, base loss: 21512.20
[INFO 2017-06-28 18:04:40,478 main.py:51] epoch 205, training loss: 16442.87, average training loss: 17627.25, base loss: 21520.72
[INFO 2017-06-28 18:04:41,773 main.py:51] epoch 206, training loss: 16262.82, average training loss: 17620.66, base loss: 21526.45
[INFO 2017-06-28 18:04:43,064 main.py:51] epoch 207, training loss: 16222.11, average training loss: 17613.93, base loss: 21537.60
[INFO 2017-06-28 18:04:44,546 main.py:51] epoch 208, training loss: 15820.37, average training loss: 17605.35, base loss: 21535.39
[INFO 2017-06-28 18:04:45,900 main.py:51] epoch 209, training loss: 15580.76, average training loss: 17595.71, base loss: 21526.16
[INFO 2017-06-28 18:04:47,252 main.py:51] epoch 210, training loss: 16429.91, average training loss: 17590.19, base loss: 21532.49
[INFO 2017-06-28 18:04:48,625 main.py:51] epoch 211, training loss: 16162.66, average training loss: 17583.45, base loss: 21536.64
[INFO 2017-06-28 18:04:49,980 main.py:51] epoch 212, training loss: 15133.57, average training loss: 17571.95, base loss: 21532.84
[INFO 2017-06-28 18:04:51,318 main.py:51] epoch 213, training loss: 14684.52, average training loss: 17558.46, base loss: 21525.43
[INFO 2017-06-28 18:04:52,647 main.py:51] epoch 214, training loss: 16246.40, average training loss: 17552.36, base loss: 21530.16
[INFO 2017-06-28 18:04:53,922 main.py:51] epoch 215, training loss: 16070.11, average training loss: 17545.49, base loss: 21537.84
[INFO 2017-06-28 18:04:55,168 main.py:51] epoch 216, training loss: 15232.26, average training loss: 17534.83, base loss: 21537.79
[INFO 2017-06-28 18:04:56,392 main.py:51] epoch 217, training loss: 14562.23, average training loss: 17521.20, base loss: 21533.68
[INFO 2017-06-28 18:04:57,697 main.py:51] epoch 218, training loss: 16797.05, average training loss: 17517.89, base loss: 21543.87
[INFO 2017-06-28 18:04:58,995 main.py:51] epoch 219, training loss: 16750.77, average training loss: 17514.40, base loss: 21549.09
[INFO 2017-06-28 18:05:00,327 main.py:51] epoch 220, training loss: 15457.16, average training loss: 17505.10, base loss: 21559.45
[INFO 2017-06-28 18:05:01,691 main.py:51] epoch 221, training loss: 15207.79, average training loss: 17494.75, base loss: 21549.72
[INFO 2017-06-28 18:05:02,922 main.py:51] epoch 222, training loss: 14095.03, average training loss: 17479.50, base loss: 21542.63
[INFO 2017-06-28 18:05:04,191 main.py:51] epoch 223, training loss: 17940.00, average training loss: 17481.56, base loss: 21556.51
[INFO 2017-06-28 18:05:05,509 main.py:51] epoch 224, training loss: 14316.14, average training loss: 17467.49, base loss: 21555.80
[INFO 2017-06-28 18:05:06,796 main.py:51] epoch 225, training loss: 16575.16, average training loss: 17463.54, base loss: 21566.89
[INFO 2017-06-28 18:05:08,210 main.py:51] epoch 226, training loss: 16150.93, average training loss: 17457.76, base loss: 21572.41
[INFO 2017-06-28 18:05:09,439 main.py:51] epoch 227, training loss: 15124.21, average training loss: 17447.52, base loss: 21570.32
[INFO 2017-06-28 18:05:10,716 main.py:51] epoch 228, training loss: 15148.79, average training loss: 17437.49, base loss: 21578.26
[INFO 2017-06-28 18:05:11,990 main.py:51] epoch 229, training loss: 14485.65, average training loss: 17424.65, base loss: 21579.21
[INFO 2017-06-28 18:05:13,399 main.py:51] epoch 230, training loss: 16137.35, average training loss: 17419.08, base loss: 21585.59
[INFO 2017-06-28 18:05:14,650 main.py:51] epoch 231, training loss: 13896.33, average training loss: 17403.89, base loss: 21582.77
[INFO 2017-06-28 18:05:15,846 main.py:51] epoch 232, training loss: 17157.78, average training loss: 17402.84, base loss: 21586.12
[INFO 2017-06-28 18:05:17,158 main.py:51] epoch 233, training loss: 16519.47, average training loss: 17399.06, base loss: 21597.01
[INFO 2017-06-28 18:05:18,315 main.py:51] epoch 234, training loss: 16404.90, average training loss: 17394.83, base loss: 21595.42
[INFO 2017-06-28 18:05:19,691 main.py:51] epoch 235, training loss: 14380.47, average training loss: 17382.06, base loss: 21586.61
[INFO 2017-06-28 18:05:20,928 main.py:51] epoch 236, training loss: 14568.47, average training loss: 17370.19, base loss: 21586.33
[INFO 2017-06-28 18:05:22,154 main.py:51] epoch 237, training loss: 15345.71, average training loss: 17361.68, base loss: 21587.34
[INFO 2017-06-28 18:05:23,363 main.py:51] epoch 238, training loss: 15583.55, average training loss: 17354.24, base loss: 21591.84
[INFO 2017-06-28 18:05:24,639 main.py:51] epoch 239, training loss: 14853.40, average training loss: 17343.82, base loss: 21592.08
[INFO 2017-06-28 18:05:25,958 main.py:51] epoch 240, training loss: 15958.84, average training loss: 17338.08, base loss: 21601.69
[INFO 2017-06-28 18:05:27,221 main.py:51] epoch 241, training loss: 16665.09, average training loss: 17335.29, base loss: 21612.42
[INFO 2017-06-28 18:05:28,562 main.py:51] epoch 242, training loss: 14864.46, average training loss: 17325.13, base loss: 21606.42
[INFO 2017-06-28 18:05:29,937 main.py:51] epoch 243, training loss: 15794.24, average training loss: 17318.85, base loss: 21613.84
[INFO 2017-06-28 18:05:31,207 main.py:51] epoch 244, training loss: 15845.22, average training loss: 17312.84, base loss: 21616.29
[INFO 2017-06-28 18:05:32,354 main.py:51] epoch 245, training loss: 15118.18, average training loss: 17303.92, base loss: 21614.38
[INFO 2017-06-28 18:05:33,594 main.py:51] epoch 246, training loss: 14249.38, average training loss: 17291.55, base loss: 21615.49
[INFO 2017-06-28 18:05:34,888 main.py:51] epoch 247, training loss: 14480.08, average training loss: 17280.21, base loss: 21609.87
[INFO 2017-06-28 18:05:36,259 main.py:51] epoch 248, training loss: 16402.15, average training loss: 17276.69, base loss: 21614.31
[INFO 2017-06-28 18:05:37,579 main.py:51] epoch 249, training loss: 15126.88, average training loss: 17268.09, base loss: 21610.83
[INFO 2017-06-28 18:05:38,872 main.py:51] epoch 250, training loss: 13952.65, average training loss: 17254.88, base loss: 21609.37
[INFO 2017-06-28 18:05:40,122 main.py:51] epoch 251, training loss: 16080.90, average training loss: 17250.22, base loss: 21614.64
[INFO 2017-06-28 18:05:41,338 main.py:51] epoch 252, training loss: 14988.00, average training loss: 17241.28, base loss: 21615.45
[INFO 2017-06-28 18:05:42,673 main.py:51] epoch 253, training loss: 15607.55, average training loss: 17234.85, base loss: 21621.93
[INFO 2017-06-28 18:05:44,001 main.py:51] epoch 254, training loss: 13922.46, average training loss: 17221.86, base loss: 21617.57
[INFO 2017-06-28 18:05:45,272 main.py:51] epoch 255, training loss: 14775.14, average training loss: 17212.30, base loss: 21612.87
[INFO 2017-06-28 18:05:46,579 main.py:51] epoch 256, training loss: 15301.45, average training loss: 17204.86, base loss: 21614.03
[INFO 2017-06-28 18:05:47,897 main.py:51] epoch 257, training loss: 14528.60, average training loss: 17194.49, base loss: 21614.54
[INFO 2017-06-28 18:05:49,300 main.py:51] epoch 258, training loss: 14667.92, average training loss: 17184.74, base loss: 21613.71
[INFO 2017-06-28 18:05:50,650 main.py:51] epoch 259, training loss: 18226.13, average training loss: 17188.74, base loss: 21624.51
[INFO 2017-06-28 18:05:52,013 main.py:51] epoch 260, training loss: 14521.88, average training loss: 17178.52, base loss: 21624.99
[INFO 2017-06-28 18:05:53,320 main.py:51] epoch 261, training loss: 14714.16, average training loss: 17169.12, base loss: 21627.51
[INFO 2017-06-28 18:05:54,616 main.py:51] epoch 262, training loss: 16003.74, average training loss: 17164.69, base loss: 21630.47
[INFO 2017-06-28 18:05:56,013 main.py:51] epoch 263, training loss: 13371.40, average training loss: 17150.32, base loss: 21624.14
[INFO 2017-06-28 18:05:57,192 main.py:51] epoch 264, training loss: 13558.06, average training loss: 17136.76, base loss: 21615.02
[INFO 2017-06-28 18:05:58,418 main.py:51] epoch 265, training loss: 14145.94, average training loss: 17125.52, base loss: 21615.11
[INFO 2017-06-28 18:05:59,819 main.py:51] epoch 266, training loss: 14997.32, average training loss: 17117.55, base loss: 21614.21
[INFO 2017-06-28 18:06:01,132 main.py:51] epoch 267, training loss: 13698.14, average training loss: 17104.79, base loss: 21610.32
[INFO 2017-06-28 18:06:02,435 main.py:51] epoch 268, training loss: 16036.87, average training loss: 17100.82, base loss: 21615.90
[INFO 2017-06-28 18:06:03,704 main.py:51] epoch 269, training loss: 16885.48, average training loss: 17100.02, base loss: 21625.01
[INFO 2017-06-28 18:06:04,969 main.py:51] epoch 270, training loss: 15534.89, average training loss: 17094.25, base loss: 21629.66
[INFO 2017-06-28 18:06:06,212 main.py:51] epoch 271, training loss: 14432.56, average training loss: 17084.46, base loss: 21625.22
[INFO 2017-06-28 18:06:07,512 main.py:51] epoch 272, training loss: 15139.74, average training loss: 17077.34, base loss: 21626.49
[INFO 2017-06-28 18:06:08,887 main.py:51] epoch 273, training loss: 13021.42, average training loss: 17062.53, base loss: 21617.99
[INFO 2017-06-28 18:06:10,136 main.py:51] epoch 274, training loss: 15184.11, average training loss: 17055.70, base loss: 21618.44
[INFO 2017-06-28 18:06:11,495 main.py:51] epoch 275, training loss: 15424.10, average training loss: 17049.79, base loss: 21622.05
[INFO 2017-06-28 18:06:12,861 main.py:51] epoch 276, training loss: 13898.80, average training loss: 17038.42, base loss: 21616.91
[INFO 2017-06-28 18:06:14,317 main.py:51] epoch 277, training loss: 15671.81, average training loss: 17033.50, base loss: 21617.66
[INFO 2017-06-28 18:06:15,625 main.py:51] epoch 278, training loss: 13750.84, average training loss: 17021.73, base loss: 21612.38
[INFO 2017-06-28 18:06:16,898 main.py:51] epoch 279, training loss: 15421.81, average training loss: 17016.02, base loss: 21618.38
[INFO 2017-06-28 18:06:18,225 main.py:51] epoch 280, training loss: 14452.01, average training loss: 17006.90, base loss: 21619.12
[INFO 2017-06-28 18:06:19,425 main.py:51] epoch 281, training loss: 15220.50, average training loss: 17000.56, base loss: 21612.08
[INFO 2017-06-28 18:06:20,719 main.py:51] epoch 282, training loss: 15305.62, average training loss: 16994.57, base loss: 21609.93
[INFO 2017-06-28 18:06:22,124 main.py:51] epoch 283, training loss: 15234.09, average training loss: 16988.37, base loss: 21613.69
[INFO 2017-06-28 18:06:23,339 main.py:51] epoch 284, training loss: 14862.86, average training loss: 16980.91, base loss: 21616.50
[INFO 2017-06-28 18:06:24,604 main.py:51] epoch 285, training loss: 14394.38, average training loss: 16971.87, base loss: 21606.63
[INFO 2017-06-28 18:06:25,986 main.py:51] epoch 286, training loss: 15358.56, average training loss: 16966.25, base loss: 21603.93
[INFO 2017-06-28 18:06:27,472 main.py:51] epoch 287, training loss: 16037.15, average training loss: 16963.02, base loss: 21610.30
[INFO 2017-06-28 18:06:28,749 main.py:51] epoch 288, training loss: 15018.68, average training loss: 16956.30, base loss: 21606.65
[INFO 2017-06-28 18:06:30,051 main.py:51] epoch 289, training loss: 14564.77, average training loss: 16948.05, base loss: 21608.48
[INFO 2017-06-28 18:06:31,415 main.py:51] epoch 290, training loss: 15740.72, average training loss: 16943.90, base loss: 21612.70
[INFO 2017-06-28 18:06:32,743 main.py:51] epoch 291, training loss: 13854.28, average training loss: 16933.32, base loss: 21609.17
[INFO 2017-06-28 18:06:34,164 main.py:51] epoch 292, training loss: 14507.33, average training loss: 16925.04, base loss: 21610.99
[INFO 2017-06-28 18:06:35,515 main.py:51] epoch 293, training loss: 15548.98, average training loss: 16920.36, base loss: 21617.68
[INFO 2017-06-28 18:06:36,850 main.py:51] epoch 294, training loss: 15167.58, average training loss: 16914.42, base loss: 21617.12
[INFO 2017-06-28 18:06:38,289 main.py:51] epoch 295, training loss: 13093.49, average training loss: 16901.51, base loss: 21609.34
[INFO 2017-06-28 18:06:39,663 main.py:51] epoch 296, training loss: 14641.53, average training loss: 16893.90, base loss: 21606.01
[INFO 2017-06-28 18:06:41,039 main.py:51] epoch 297, training loss: 12566.23, average training loss: 16879.38, base loss: 21597.18
[INFO 2017-06-28 18:06:42,399 main.py:51] epoch 298, training loss: 14847.99, average training loss: 16872.58, base loss: 21602.54
[INFO 2017-06-28 18:06:43,711 main.py:51] epoch 299, training loss: 14162.52, average training loss: 16863.55, base loss: 21601.11
[INFO 2017-06-28 18:06:43,711 main.py:53] epoch 299, testing
[INFO 2017-06-28 18:06:49,655 main.py:105] average testing loss: 16325.43, base loss: 22403.26
[INFO 2017-06-28 18:06:49,655 main.py:106] improve_loss: 6077.83, improve_percent: 0.27
[INFO 2017-06-28 18:06:49,656 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:06:49,669 main.py:76] current best improved percent: 0.27
[INFO 2017-06-28 18:06:50,974 main.py:51] epoch 300, training loss: 14454.78, average training loss: 16855.55, base loss: 21599.59
[INFO 2017-06-28 18:06:52,237 main.py:51] epoch 301, training loss: 14321.04, average training loss: 16847.15, base loss: 21598.20
[INFO 2017-06-28 18:06:53,684 main.py:51] epoch 302, training loss: 13233.82, average training loss: 16835.23, base loss: 21589.33
[INFO 2017-06-28 18:06:55,114 main.py:51] epoch 303, training loss: 14847.79, average training loss: 16828.69, base loss: 21586.12
[INFO 2017-06-28 18:06:56,510 main.py:51] epoch 304, training loss: 13551.38, average training loss: 16817.95, base loss: 21580.85
[INFO 2017-06-28 18:06:57,792 main.py:51] epoch 305, training loss: 15134.49, average training loss: 16812.45, base loss: 21586.38
[INFO 2017-06-28 18:06:59,281 main.py:51] epoch 306, training loss: 15045.82, average training loss: 16806.69, base loss: 21587.08
[INFO 2017-06-28 18:07:00,727 main.py:51] epoch 307, training loss: 15061.30, average training loss: 16801.02, base loss: 21590.00
[INFO 2017-06-28 18:07:02,188 main.py:51] epoch 308, training loss: 13771.68, average training loss: 16791.22, base loss: 21582.92
[INFO 2017-06-28 18:07:03,621 main.py:51] epoch 309, training loss: 15846.32, average training loss: 16788.17, base loss: 21584.67
[INFO 2017-06-28 18:07:04,910 main.py:51] epoch 310, training loss: 16982.04, average training loss: 16788.80, base loss: 21592.50
[INFO 2017-06-28 18:07:06,247 main.py:51] epoch 311, training loss: 14411.17, average training loss: 16781.17, base loss: 21594.84
[INFO 2017-06-28 18:07:07,550 main.py:51] epoch 312, training loss: 13399.64, average training loss: 16770.37, base loss: 21587.66
[INFO 2017-06-28 18:07:08,960 main.py:51] epoch 313, training loss: 15368.39, average training loss: 16765.91, base loss: 21589.29
[INFO 2017-06-28 18:07:10,352 main.py:51] epoch 314, training loss: 14537.94, average training loss: 16758.83, base loss: 21586.60
[INFO 2017-06-28 18:07:11,632 main.py:51] epoch 315, training loss: 19380.97, average training loss: 16767.13, base loss: 21603.20
[INFO 2017-06-28 18:07:12,952 main.py:51] epoch 316, training loss: 14396.79, average training loss: 16759.65, base loss: 21604.04
[INFO 2017-06-28 18:07:14,277 main.py:51] epoch 317, training loss: 13841.32, average training loss: 16750.48, base loss: 21598.98
[INFO 2017-06-28 18:07:15,700 main.py:51] epoch 318, training loss: 14590.64, average training loss: 16743.71, base loss: 21599.87
[INFO 2017-06-28 18:07:17,123 main.py:51] epoch 319, training loss: 15300.48, average training loss: 16739.20, base loss: 21600.13
[INFO 2017-06-28 18:07:18,402 main.py:51] epoch 320, training loss: 15478.72, average training loss: 16735.27, base loss: 21605.46
[INFO 2017-06-28 18:07:19,783 main.py:51] epoch 321, training loss: 14547.43, average training loss: 16728.47, base loss: 21606.85
[INFO 2017-06-28 18:07:21,178 main.py:51] epoch 322, training loss: 15250.87, average training loss: 16723.90, base loss: 21610.21
[INFO 2017-06-28 18:07:22,699 main.py:51] epoch 323, training loss: 15441.64, average training loss: 16719.94, base loss: 21613.20
[INFO 2017-06-28 18:07:24,095 main.py:51] epoch 324, training loss: 15044.06, average training loss: 16714.79, base loss: 21610.87
[INFO 2017-06-28 18:07:25,384 main.py:51] epoch 325, training loss: 13864.25, average training loss: 16706.04, base loss: 21610.45
[INFO 2017-06-28 18:07:26,864 main.py:51] epoch 326, training loss: 15915.49, average training loss: 16703.62, base loss: 21613.91
[INFO 2017-06-28 18:07:28,296 main.py:51] epoch 327, training loss: 13648.53, average training loss: 16694.31, base loss: 21608.34
[INFO 2017-06-28 18:07:29,599 main.py:51] epoch 328, training loss: 14271.51, average training loss: 16686.95, base loss: 21606.18
[INFO 2017-06-28 18:07:30,852 main.py:51] epoch 329, training loss: 15639.12, average training loss: 16683.77, base loss: 21606.40
[INFO 2017-06-28 18:07:32,380 main.py:51] epoch 330, training loss: 14900.47, average training loss: 16678.38, base loss: 21612.95
[INFO 2017-06-28 18:07:33,604 main.py:51] epoch 331, training loss: 14326.39, average training loss: 16671.30, base loss: 21608.88
[INFO 2017-06-28 18:07:34,917 main.py:51] epoch 332, training loss: 14364.09, average training loss: 16664.37, base loss: 21607.77
[INFO 2017-06-28 18:07:36,178 main.py:51] epoch 333, training loss: 15035.21, average training loss: 16659.49, base loss: 21613.75
[INFO 2017-06-28 18:07:37,543 main.py:51] epoch 334, training loss: 16142.91, average training loss: 16657.95, base loss: 21619.19
[INFO 2017-06-28 18:07:38,874 main.py:51] epoch 335, training loss: 14704.98, average training loss: 16652.14, base loss: 21624.25
[INFO 2017-06-28 18:07:40,367 main.py:51] epoch 336, training loss: 13304.50, average training loss: 16642.20, base loss: 21616.36
[INFO 2017-06-28 18:07:41,635 main.py:51] epoch 337, training loss: 13621.48, average training loss: 16633.27, base loss: 21609.62
[INFO 2017-06-28 18:07:42,996 main.py:51] epoch 338, training loss: 15957.73, average training loss: 16631.27, base loss: 21612.03
[INFO 2017-06-28 18:07:44,348 main.py:51] epoch 339, training loss: 14635.18, average training loss: 16625.40, base loss: 21611.83
[INFO 2017-06-28 18:07:45,625 main.py:51] epoch 340, training loss: 14103.30, average training loss: 16618.01, base loss: 21607.25
[INFO 2017-06-28 18:07:47,098 main.py:51] epoch 341, training loss: 14568.08, average training loss: 16612.01, base loss: 21607.82
[INFO 2017-06-28 18:07:48,459 main.py:51] epoch 342, training loss: 13648.07, average training loss: 16603.37, base loss: 21605.04
[INFO 2017-06-28 18:07:49,760 main.py:51] epoch 343, training loss: 15110.63, average training loss: 16599.03, base loss: 21603.23
[INFO 2017-06-28 18:07:51,062 main.py:51] epoch 344, training loss: 12759.55, average training loss: 16587.90, base loss: 21597.46
[INFO 2017-06-28 18:07:52,705 main.py:51] epoch 345, training loss: 13767.20, average training loss: 16579.75, base loss: 21594.92
[INFO 2017-06-28 18:07:54,153 main.py:51] epoch 346, training loss: 13208.95, average training loss: 16570.04, base loss: 21593.19
[INFO 2017-06-28 18:07:55,723 main.py:51] epoch 347, training loss: 16971.41, average training loss: 16571.19, base loss: 21600.19
[INFO 2017-06-28 18:07:57,096 main.py:51] epoch 348, training loss: 13400.25, average training loss: 16562.11, base loss: 21594.21
[INFO 2017-06-28 18:07:58,455 main.py:51] epoch 349, training loss: 16958.91, average training loss: 16563.24, base loss: 21605.55
[INFO 2017-06-28 18:07:59,730 main.py:51] epoch 350, training loss: 15311.51, average training loss: 16559.67, base loss: 21602.86
[INFO 2017-06-28 18:08:01,006 main.py:51] epoch 351, training loss: 14374.72, average training loss: 16553.47, base loss: 21605.26
[INFO 2017-06-28 18:08:02,427 main.py:51] epoch 352, training loss: 14769.37, average training loss: 16548.41, base loss: 21604.17
[INFO 2017-06-28 18:08:03,778 main.py:51] epoch 353, training loss: 13388.02, average training loss: 16539.48, base loss: 21600.40
[INFO 2017-06-28 18:08:05,220 main.py:51] epoch 354, training loss: 15657.98, average training loss: 16537.00, base loss: 21602.33
[INFO 2017-06-28 18:08:06,703 main.py:51] epoch 355, training loss: 14112.03, average training loss: 16530.19, base loss: 21601.14
[INFO 2017-06-28 18:08:08,136 main.py:51] epoch 356, training loss: 14179.64, average training loss: 16523.60, base loss: 21597.68
[INFO 2017-06-28 18:08:09,456 main.py:51] epoch 357, training loss: 16559.87, average training loss: 16523.71, base loss: 21600.71
[INFO 2017-06-28 18:08:10,800 main.py:51] epoch 358, training loss: 14202.33, average training loss: 16517.24, base loss: 21600.58
[INFO 2017-06-28 18:08:12,322 main.py:51] epoch 359, training loss: 13750.80, average training loss: 16509.56, base loss: 21597.56
[INFO 2017-06-28 18:08:13,750 main.py:51] epoch 360, training loss: 13737.75, average training loss: 16501.88, base loss: 21599.23
[INFO 2017-06-28 18:08:15,194 main.py:51] epoch 361, training loss: 13761.89, average training loss: 16494.31, base loss: 21600.54
[INFO 2017-06-28 18:08:16,639 main.py:51] epoch 362, training loss: 13967.43, average training loss: 16487.35, base loss: 21601.11
[INFO 2017-06-28 18:08:17,986 main.py:51] epoch 363, training loss: 15062.33, average training loss: 16483.43, base loss: 21603.43
[INFO 2017-06-28 18:08:19,216 main.py:51] epoch 364, training loss: 13290.34, average training loss: 16474.68, base loss: 21600.99
[INFO 2017-06-28 18:08:20,531 main.py:51] epoch 365, training loss: 16293.22, average training loss: 16474.19, base loss: 21610.28
[INFO 2017-06-28 18:08:21,850 main.py:51] epoch 366, training loss: 13740.48, average training loss: 16466.74, base loss: 21608.79
[INFO 2017-06-28 18:08:23,248 main.py:51] epoch 367, training loss: 14188.24, average training loss: 16460.55, base loss: 21608.87
[INFO 2017-06-28 18:08:24,635 main.py:51] epoch 368, training loss: 14963.83, average training loss: 16456.49, base loss: 21609.62
[INFO 2017-06-28 18:08:26,007 main.py:51] epoch 369, training loss: 16394.87, average training loss: 16456.33, base loss: 21615.74
[INFO 2017-06-28 18:08:27,335 main.py:51] epoch 370, training loss: 14316.72, average training loss: 16450.56, base loss: 21613.18
[INFO 2017-06-28 18:08:28,579 main.py:51] epoch 371, training loss: 14028.51, average training loss: 16444.05, base loss: 21614.29
[INFO 2017-06-28 18:08:30,022 main.py:51] epoch 372, training loss: 14267.85, average training loss: 16438.21, base loss: 21612.10
[INFO 2017-06-28 18:08:31,262 main.py:51] epoch 373, training loss: 14093.75, average training loss: 16431.94, base loss: 21609.34
[INFO 2017-06-28 18:08:32,605 main.py:51] epoch 374, training loss: 14150.16, average training loss: 16425.86, base loss: 21604.33
[INFO 2017-06-28 18:08:33,943 main.py:51] epoch 375, training loss: 14022.66, average training loss: 16419.47, base loss: 21599.80
[INFO 2017-06-28 18:08:35,510 main.py:51] epoch 376, training loss: 15411.66, average training loss: 16416.79, base loss: 21602.81
[INFO 2017-06-28 18:08:36,926 main.py:51] epoch 377, training loss: 15546.07, average training loss: 16414.49, base loss: 21607.49
[INFO 2017-06-28 18:08:38,273 main.py:51] epoch 378, training loss: 15158.18, average training loss: 16411.18, base loss: 21609.49
[INFO 2017-06-28 18:08:39,754 main.py:51] epoch 379, training loss: 16510.15, average training loss: 16411.44, base loss: 21616.01
[INFO 2017-06-28 18:08:41,200 main.py:51] epoch 380, training loss: 15281.81, average training loss: 16408.47, base loss: 21615.41
[INFO 2017-06-28 18:08:42,640 main.py:51] epoch 381, training loss: 13939.79, average training loss: 16402.01, base loss: 21613.63
[INFO 2017-06-28 18:08:44,006 main.py:51] epoch 382, training loss: 13724.60, average training loss: 16395.02, base loss: 21610.81
[INFO 2017-06-28 18:08:45,447 main.py:51] epoch 383, training loss: 13417.27, average training loss: 16387.26, base loss: 21606.82
[INFO 2017-06-28 18:08:46,833 main.py:51] epoch 384, training loss: 17031.99, average training loss: 16388.94, base loss: 21613.81
[INFO 2017-06-28 18:08:48,255 main.py:51] epoch 385, training loss: 16102.82, average training loss: 16388.20, base loss: 21617.09
[INFO 2017-06-28 18:08:49,686 main.py:51] epoch 386, training loss: 14239.70, average training loss: 16382.65, base loss: 21615.75
[INFO 2017-06-28 18:08:51,059 main.py:51] epoch 387, training loss: 14438.37, average training loss: 16377.63, base loss: 21621.25
[INFO 2017-06-28 18:08:52,453 main.py:51] epoch 388, training loss: 15211.01, average training loss: 16374.64, base loss: 21622.83
[INFO 2017-06-28 18:08:53,764 main.py:51] epoch 389, training loss: 14126.26, average training loss: 16368.87, base loss: 21619.21
[INFO 2017-06-28 18:08:55,201 main.py:51] epoch 390, training loss: 14621.68, average training loss: 16364.40, base loss: 21619.88
[INFO 2017-06-28 18:08:56,577 main.py:51] epoch 391, training loss: 16278.71, average training loss: 16364.18, base loss: 21626.54
[INFO 2017-06-28 18:08:58,027 main.py:51] epoch 392, training loss: 14876.88, average training loss: 16360.40, base loss: 21630.83
[INFO 2017-06-28 18:08:59,525 main.py:51] epoch 393, training loss: 15515.19, average training loss: 16358.25, base loss: 21634.86
[INFO 2017-06-28 18:09:00,970 main.py:51] epoch 394, training loss: 14198.66, average training loss: 16352.79, base loss: 21635.93
[INFO 2017-06-28 18:09:02,465 main.py:51] epoch 395, training loss: 17011.29, average training loss: 16354.45, base loss: 21641.96
[INFO 2017-06-28 18:09:03,713 main.py:51] epoch 396, training loss: 14843.91, average training loss: 16350.64, base loss: 21642.23
[INFO 2017-06-28 18:09:05,135 main.py:51] epoch 397, training loss: 13864.42, average training loss: 16344.40, base loss: 21640.07
[INFO 2017-06-28 18:09:06,497 main.py:51] epoch 398, training loss: 15196.41, average training loss: 16341.52, base loss: 21641.19
[INFO 2017-06-28 18:09:07,985 main.py:51] epoch 399, training loss: 14673.51, average training loss: 16337.35, base loss: 21643.59
[INFO 2017-06-28 18:09:07,985 main.py:53] epoch 399, testing
[INFO 2017-06-28 18:09:14,245 main.py:105] average testing loss: 15793.38, base loss: 22188.65
[INFO 2017-06-28 18:09:14,245 main.py:106] improve_loss: 6395.27, improve_percent: 0.29
[INFO 2017-06-28 18:09:14,246 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:09:14,259 main.py:76] current best improved percent: 0.29
[INFO 2017-06-28 18:09:15,613 main.py:51] epoch 400, training loss: 12606.25, average training loss: 16328.05, base loss: 21637.15
[INFO 2017-06-28 18:09:16,915 main.py:51] epoch 401, training loss: 14619.91, average training loss: 16323.80, base loss: 21639.56
[INFO 2017-06-28 18:09:18,517 main.py:51] epoch 402, training loss: 14049.72, average training loss: 16318.15, base loss: 21638.98
[INFO 2017-06-28 18:09:19,853 main.py:51] epoch 403, training loss: 15373.96, average training loss: 16315.82, base loss: 21641.38
[INFO 2017-06-28 18:09:21,154 main.py:51] epoch 404, training loss: 13290.29, average training loss: 16308.35, base loss: 21635.40
[INFO 2017-06-28 18:09:22,367 main.py:51] epoch 405, training loss: 14086.13, average training loss: 16302.87, base loss: 21631.05
[INFO 2017-06-28 18:09:23,732 main.py:51] epoch 406, training loss: 14703.55, average training loss: 16298.94, base loss: 21630.67
[INFO 2017-06-28 18:09:24,993 main.py:51] epoch 407, training loss: 15816.71, average training loss: 16297.76, base loss: 21633.34
[INFO 2017-06-28 18:09:26,415 main.py:51] epoch 408, training loss: 15013.61, average training loss: 16294.62, base loss: 21634.27
[INFO 2017-06-28 18:09:27,776 main.py:51] epoch 409, training loss: 14906.31, average training loss: 16291.24, base loss: 21634.50
[INFO 2017-06-28 18:09:29,100 main.py:51] epoch 410, training loss: 14135.84, average training loss: 16285.99, base loss: 21630.70
[INFO 2017-06-28 18:09:30,625 main.py:51] epoch 411, training loss: 13382.94, average training loss: 16278.95, base loss: 21627.27
[INFO 2017-06-28 18:09:31,923 main.py:51] epoch 412, training loss: 14275.78, average training loss: 16274.10, base loss: 21625.27
[INFO 2017-06-28 18:09:33,308 main.py:51] epoch 413, training loss: 14418.12, average training loss: 16269.61, base loss: 21625.37
[INFO 2017-06-28 18:09:34,688 main.py:51] epoch 414, training loss: 15396.00, average training loss: 16267.51, base loss: 21626.99
[INFO 2017-06-28 18:09:36,051 main.py:51] epoch 415, training loss: 13149.60, average training loss: 16260.01, base loss: 21623.48
[INFO 2017-06-28 18:09:37,280 main.py:51] epoch 416, training loss: 16608.86, average training loss: 16260.85, base loss: 21628.64
[INFO 2017-06-28 18:09:38,511 main.py:51] epoch 417, training loss: 15324.30, average training loss: 16258.61, base loss: 21627.03
[INFO 2017-06-28 18:09:39,873 main.py:51] epoch 418, training loss: 14838.63, average training loss: 16255.22, base loss: 21630.02
[INFO 2017-06-28 18:09:41,286 main.py:51] epoch 419, training loss: 14659.13, average training loss: 16251.42, base loss: 21627.29
[INFO 2017-06-28 18:09:42,682 main.py:51] epoch 420, training loss: 16252.70, average training loss: 16251.42, base loss: 21631.70
[INFO 2017-06-28 18:09:43,936 main.py:51] epoch 421, training loss: 13445.08, average training loss: 16244.77, base loss: 21629.00
[INFO 2017-06-28 18:09:45,270 main.py:51] epoch 422, training loss: 13472.70, average training loss: 16238.22, base loss: 21624.70
[INFO 2017-06-28 18:09:46,638 main.py:51] epoch 423, training loss: 12508.26, average training loss: 16229.42, base loss: 21618.73
[INFO 2017-06-28 18:09:48,046 main.py:51] epoch 424, training loss: 15591.88, average training loss: 16227.92, base loss: 21621.46
[INFO 2017-06-28 18:09:49,360 main.py:51] epoch 425, training loss: 14524.38, average training loss: 16223.92, base loss: 21621.22
[INFO 2017-06-28 18:09:50,682 main.py:51] epoch 426, training loss: 14088.21, average training loss: 16218.92, base loss: 21617.03
[INFO 2017-06-28 18:09:52,015 main.py:51] epoch 427, training loss: 13869.25, average training loss: 16213.43, base loss: 21617.74
[INFO 2017-06-28 18:09:53,253 main.py:51] epoch 428, training loss: 15184.30, average training loss: 16211.03, base loss: 21622.54
[INFO 2017-06-28 18:09:54,648 main.py:51] epoch 429, training loss: 13245.32, average training loss: 16204.13, base loss: 21618.29
[INFO 2017-06-28 18:09:56,218 main.py:51] epoch 430, training loss: 13902.07, average training loss: 16198.79, base loss: 21615.70
[INFO 2017-06-28 18:09:57,471 main.py:51] epoch 431, training loss: 13303.19, average training loss: 16192.09, base loss: 21607.63
[INFO 2017-06-28 18:09:58,873 main.py:51] epoch 432, training loss: 13256.33, average training loss: 16185.31, base loss: 21602.86
[INFO 2017-06-28 18:10:00,192 main.py:51] epoch 433, training loss: 15603.20, average training loss: 16183.97, base loss: 21607.36
[INFO 2017-06-28 18:10:01,526 main.py:51] epoch 434, training loss: 13871.33, average training loss: 16178.65, base loss: 21606.70
[INFO 2017-06-28 18:10:03,069 main.py:51] epoch 435, training loss: 15362.40, average training loss: 16176.78, base loss: 21610.63
[INFO 2017-06-28 18:10:04,472 main.py:51] epoch 436, training loss: 15174.10, average training loss: 16174.49, base loss: 21613.43
[INFO 2017-06-28 18:10:05,996 main.py:51] epoch 437, training loss: 13250.04, average training loss: 16167.81, base loss: 21608.76
[INFO 2017-06-28 18:10:07,193 main.py:51] epoch 438, training loss: 13639.22, average training loss: 16162.05, base loss: 21607.57
[INFO 2017-06-28 18:10:08,475 main.py:51] epoch 439, training loss: 15389.37, average training loss: 16160.29, base loss: 21608.27
[INFO 2017-06-28 18:10:09,885 main.py:51] epoch 440, training loss: 14508.10, average training loss: 16156.55, base loss: 21606.51
[INFO 2017-06-28 18:10:11,358 main.py:51] epoch 441, training loss: 13502.62, average training loss: 16150.54, base loss: 21607.29
[INFO 2017-06-28 18:10:12,737 main.py:51] epoch 442, training loss: 14377.98, average training loss: 16146.54, base loss: 21607.75
[INFO 2017-06-28 18:10:14,033 main.py:51] epoch 443, training loss: 13368.07, average training loss: 16140.28, base loss: 21604.03
[INFO 2017-06-28 18:10:15,383 main.py:51] epoch 444, training loss: 15391.08, average training loss: 16138.60, base loss: 21608.33
[INFO 2017-06-28 18:10:16,638 main.py:51] epoch 445, training loss: 14617.34, average training loss: 16135.19, base loss: 21607.99
[INFO 2017-06-28 18:10:17,932 main.py:51] epoch 446, training loss: 14367.08, average training loss: 16131.23, base loss: 21610.14
[INFO 2017-06-28 18:10:19,317 main.py:51] epoch 447, training loss: 13695.08, average training loss: 16125.80, base loss: 21608.79
[INFO 2017-06-28 18:10:20,644 main.py:51] epoch 448, training loss: 15530.34, average training loss: 16124.47, base loss: 21611.49
[INFO 2017-06-28 18:10:22,212 main.py:51] epoch 449, training loss: 14804.80, average training loss: 16121.54, base loss: 21611.03
[INFO 2017-06-28 18:10:23,626 main.py:51] epoch 450, training loss: 13838.52, average training loss: 16116.47, base loss: 21610.63
[INFO 2017-06-28 18:10:25,064 main.py:51] epoch 451, training loss: 13395.14, average training loss: 16110.45, base loss: 21605.98
[INFO 2017-06-28 18:10:26,496 main.py:51] epoch 452, training loss: 14583.36, average training loss: 16107.08, base loss: 21607.30
[INFO 2017-06-28 18:10:27,987 main.py:51] epoch 453, training loss: 14607.92, average training loss: 16103.78, base loss: 21607.90
[INFO 2017-06-28 18:10:29,275 main.py:51] epoch 454, training loss: 13056.38, average training loss: 16097.08, base loss: 21603.97
[INFO 2017-06-28 18:10:30,682 main.py:51] epoch 455, training loss: 14017.97, average training loss: 16092.52, base loss: 21602.55
[INFO 2017-06-28 18:10:32,294 main.py:51] epoch 456, training loss: 13837.57, average training loss: 16087.59, base loss: 21599.94
[INFO 2017-06-28 18:10:33,563 main.py:51] epoch 457, training loss: 14440.26, average training loss: 16083.99, base loss: 21600.79
[INFO 2017-06-28 18:10:34,841 main.py:51] epoch 458, training loss: 14840.98, average training loss: 16081.28, base loss: 21605.42
[INFO 2017-06-28 18:10:36,504 main.py:51] epoch 459, training loss: 14324.40, average training loss: 16077.47, base loss: 21604.15
[INFO 2017-06-28 18:10:37,736 main.py:51] epoch 460, training loss: 13696.75, average training loss: 16072.30, base loss: 21602.58
[INFO 2017-06-28 18:10:39,184 main.py:51] epoch 461, training loss: 13430.89, average training loss: 16066.58, base loss: 21603.93
[INFO 2017-06-28 18:10:40,503 main.py:51] epoch 462, training loss: 15790.36, average training loss: 16065.99, base loss: 21608.01
[INFO 2017-06-28 18:10:41,772 main.py:51] epoch 463, training loss: 15649.84, average training loss: 16065.09, base loss: 21611.68
[INFO 2017-06-28 18:10:43,381 main.py:51] epoch 464, training loss: 15372.48, average training loss: 16063.60, base loss: 21614.85
[INFO 2017-06-28 18:10:44,692 main.py:51] epoch 465, training loss: 14731.99, average training loss: 16060.74, base loss: 21617.22
[INFO 2017-06-28 18:10:45,960 main.py:51] epoch 466, training loss: 15559.29, average training loss: 16059.67, base loss: 21621.67
[INFO 2017-06-28 18:10:47,211 main.py:51] epoch 467, training loss: 14099.71, average training loss: 16055.48, base loss: 21622.72
[INFO 2017-06-28 18:10:48,457 main.py:51] epoch 468, training loss: 15469.64, average training loss: 16054.23, base loss: 21625.11
[INFO 2017-06-28 18:10:49,980 main.py:51] epoch 469, training loss: 14571.34, average training loss: 16051.08, base loss: 21624.06
[INFO 2017-06-28 18:10:51,319 main.py:51] epoch 470, training loss: 14179.63, average training loss: 16047.10, base loss: 21621.88
[INFO 2017-06-28 18:10:52,824 main.py:51] epoch 471, training loss: 14330.10, average training loss: 16043.47, base loss: 21621.76
[INFO 2017-06-28 18:10:54,254 main.py:51] epoch 472, training loss: 15144.12, average training loss: 16041.57, base loss: 21627.08
[INFO 2017-06-28 18:10:55,592 main.py:51] epoch 473, training loss: 13715.02, average training loss: 16036.66, base loss: 21627.15
[INFO 2017-06-28 18:10:57,053 main.py:51] epoch 474, training loss: 14180.27, average training loss: 16032.75, base loss: 21624.18
[INFO 2017-06-28 18:10:58,518 main.py:51] epoch 475, training loss: 14728.48, average training loss: 16030.01, base loss: 21625.83
[INFO 2017-06-28 18:11:00,020 main.py:51] epoch 476, training loss: 12387.88, average training loss: 16022.37, base loss: 21622.67
[INFO 2017-06-28 18:11:01,391 main.py:51] epoch 477, training loss: 15385.76, average training loss: 16021.04, base loss: 21624.67
[INFO 2017-06-28 18:11:02,818 main.py:51] epoch 478, training loss: 15167.76, average training loss: 16019.26, base loss: 21628.83
[INFO 2017-06-28 18:11:04,243 main.py:51] epoch 479, training loss: 14446.04, average training loss: 16015.98, base loss: 21629.91
[INFO 2017-06-28 18:11:05,729 main.py:51] epoch 480, training loss: 13410.68, average training loss: 16010.57, base loss: 21627.44
[INFO 2017-06-28 18:11:07,042 main.py:51] epoch 481, training loss: 14553.60, average training loss: 16007.54, base loss: 21628.01
[INFO 2017-06-28 18:11:08,496 main.py:51] epoch 482, training loss: 13804.21, average training loss: 16002.98, base loss: 21624.93
[INFO 2017-06-28 18:11:09,810 main.py:51] epoch 483, training loss: 13778.33, average training loss: 15998.39, base loss: 21628.41
[INFO 2017-06-28 18:11:11,260 main.py:51] epoch 484, training loss: 14657.73, average training loss: 15995.62, base loss: 21632.13
[INFO 2017-06-28 18:11:12,614 main.py:51] epoch 485, training loss: 13160.54, average training loss: 15989.79, base loss: 21629.52
[INFO 2017-06-28 18:11:14,253 main.py:51] epoch 486, training loss: 15488.46, average training loss: 15988.76, base loss: 21630.83
[INFO 2017-06-28 18:11:15,554 main.py:51] epoch 487, training loss: 15506.13, average training loss: 15987.77, base loss: 21632.29
[INFO 2017-06-28 18:11:16,825 main.py:51] epoch 488, training loss: 14275.48, average training loss: 15984.27, base loss: 21630.03
[INFO 2017-06-28 18:11:18,157 main.py:51] epoch 489, training loss: 13854.33, average training loss: 15979.92, base loss: 21630.71
[INFO 2017-06-28 18:11:19,606 main.py:51] epoch 490, training loss: 16486.95, average training loss: 15980.95, base loss: 21632.39
[INFO 2017-06-28 18:11:20,832 main.py:51] epoch 491, training loss: 13888.35, average training loss: 15976.70, base loss: 21630.70
[INFO 2017-06-28 18:11:22,373 main.py:51] epoch 492, training loss: 13941.50, average training loss: 15972.57, base loss: 21630.28
[INFO 2017-06-28 18:11:23,730 main.py:51] epoch 493, training loss: 15337.39, average training loss: 15971.29, base loss: 21631.07
[INFO 2017-06-28 18:11:25,323 main.py:51] epoch 494, training loss: 13856.02, average training loss: 15967.01, base loss: 21627.61
[INFO 2017-06-28 18:11:26,707 main.py:51] epoch 495, training loss: 14895.21, average training loss: 15964.85, base loss: 21629.82
[INFO 2017-06-28 18:11:28,035 main.py:51] epoch 496, training loss: 14009.14, average training loss: 15960.92, base loss: 21626.89
[INFO 2017-06-28 18:11:29,533 main.py:51] epoch 497, training loss: 14397.88, average training loss: 15957.78, base loss: 21626.94
[INFO 2017-06-28 18:11:30,889 main.py:51] epoch 498, training loss: 14037.77, average training loss: 15953.93, base loss: 21627.23
[INFO 2017-06-28 18:11:32,325 main.py:51] epoch 499, training loss: 14762.58, average training loss: 15951.55, base loss: 21629.09
[INFO 2017-06-28 18:11:32,325 main.py:53] epoch 499, testing
[INFO 2017-06-28 18:11:38,813 main.py:105] average testing loss: 15540.82, base loss: 22168.75
[INFO 2017-06-28 18:11:38,813 main.py:106] improve_loss: 6627.92, improve_percent: 0.30
[INFO 2017-06-28 18:11:38,815 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:11:38,828 main.py:76] current best improved percent: 0.30
[INFO 2017-06-28 18:11:40,109 main.py:51] epoch 500, training loss: 14140.77, average training loss: 15947.93, base loss: 21628.74
[INFO 2017-06-28 18:11:41,475 main.py:51] epoch 501, training loss: 15020.34, average training loss: 15946.09, base loss: 21628.68
[INFO 2017-06-28 18:11:42,899 main.py:51] epoch 502, training loss: 15073.01, average training loss: 15944.35, base loss: 21630.04
[INFO 2017-06-28 18:11:44,437 main.py:51] epoch 503, training loss: 12883.14, average training loss: 15938.28, base loss: 21623.94
[INFO 2017-06-28 18:11:45,824 main.py:51] epoch 504, training loss: 15141.42, average training loss: 15936.70, base loss: 21624.60
[INFO 2017-06-28 18:11:47,041 main.py:51] epoch 505, training loss: 14189.10, average training loss: 15933.24, base loss: 21625.22
[INFO 2017-06-28 18:11:48,331 main.py:51] epoch 506, training loss: 13501.20, average training loss: 15928.45, base loss: 21624.95
[INFO 2017-06-28 18:11:49,703 main.py:51] epoch 507, training loss: 15656.75, average training loss: 15927.91, base loss: 21629.10
[INFO 2017-06-28 18:11:51,255 main.py:51] epoch 508, training loss: 14087.60, average training loss: 15924.30, base loss: 21630.61
[INFO 2017-06-28 18:11:52,605 main.py:51] epoch 509, training loss: 13478.74, average training loss: 15919.50, base loss: 21629.65
[INFO 2017-06-28 18:11:54,130 main.py:51] epoch 510, training loss: 12585.59, average training loss: 15912.98, base loss: 21627.94
[INFO 2017-06-28 18:11:55,387 main.py:51] epoch 511, training loss: 15253.73, average training loss: 15911.69, base loss: 21630.73
[INFO 2017-06-28 18:11:56,748 main.py:51] epoch 512, training loss: 14623.34, average training loss: 15909.18, base loss: 21631.64
[INFO 2017-06-28 18:11:58,186 main.py:51] epoch 513, training loss: 16305.34, average training loss: 15909.95, base loss: 21634.83
[INFO 2017-06-28 18:11:59,608 main.py:51] epoch 514, training loss: 13861.10, average training loss: 15905.97, base loss: 21631.71
[INFO 2017-06-28 18:12:00,909 main.py:51] epoch 515, training loss: 15564.49, average training loss: 15905.31, base loss: 21635.89
[INFO 2017-06-28 18:12:02,221 main.py:51] epoch 516, training loss: 15383.04, average training loss: 15904.30, base loss: 21640.93
[INFO 2017-06-28 18:12:03,523 main.py:51] epoch 517, training loss: 14798.18, average training loss: 15902.16, base loss: 21641.40
[INFO 2017-06-28 18:12:04,941 main.py:51] epoch 518, training loss: 14470.02, average training loss: 15899.40, base loss: 21643.14
[INFO 2017-06-28 18:12:06,304 main.py:51] epoch 519, training loss: 14343.78, average training loss: 15896.41, base loss: 21645.79
[INFO 2017-06-28 18:12:07,636 main.py:51] epoch 520, training loss: 14848.29, average training loss: 15894.40, base loss: 21646.83
[INFO 2017-06-28 18:12:08,901 main.py:51] epoch 521, training loss: 13091.86, average training loss: 15889.03, base loss: 21645.99
[INFO 2017-06-28 18:12:10,214 main.py:51] epoch 522, training loss: 14937.74, average training loss: 15887.21, base loss: 21647.43
[INFO 2017-06-28 18:12:11,468 main.py:51] epoch 523, training loss: 15241.39, average training loss: 15885.98, base loss: 21647.52
[INFO 2017-06-28 18:12:12,862 main.py:51] epoch 524, training loss: 13862.55, average training loss: 15882.13, base loss: 21645.38
[INFO 2017-06-28 18:12:14,254 main.py:51] epoch 525, training loss: 13402.78, average training loss: 15877.41, base loss: 21642.25
[INFO 2017-06-28 18:12:15,676 main.py:51] epoch 526, training loss: 13106.08, average training loss: 15872.15, base loss: 21640.88
[INFO 2017-06-28 18:12:16,970 main.py:51] epoch 527, training loss: 14142.94, average training loss: 15868.88, base loss: 21641.67
[INFO 2017-06-28 18:12:18,313 main.py:51] epoch 528, training loss: 15670.85, average training loss: 15868.50, base loss: 21646.60
[INFO 2017-06-28 18:12:19,649 main.py:51] epoch 529, training loss: 14566.00, average training loss: 15866.05, base loss: 21647.83
[INFO 2017-06-28 18:12:21,089 main.py:51] epoch 530, training loss: 13382.73, average training loss: 15861.37, base loss: 21644.61
[INFO 2017-06-28 18:12:22,474 main.py:51] epoch 531, training loss: 14364.65, average training loss: 15858.56, base loss: 21643.83
[INFO 2017-06-28 18:12:23,694 main.py:51] epoch 532, training loss: 13562.90, average training loss: 15854.25, base loss: 21641.58
[INFO 2017-06-28 18:12:25,072 main.py:51] epoch 533, training loss: 17059.55, average training loss: 15856.51, base loss: 21649.33
[INFO 2017-06-28 18:12:26,404 main.py:51] epoch 534, training loss: 15477.83, average training loss: 15855.80, base loss: 21652.18
[INFO 2017-06-28 18:12:27,742 main.py:51] epoch 535, training loss: 14910.38, average training loss: 15854.04, base loss: 21653.98
[INFO 2017-06-28 18:12:29,305 main.py:51] epoch 536, training loss: 16032.46, average training loss: 15854.37, base loss: 21655.60
[INFO 2017-06-28 18:12:30,817 main.py:51] epoch 537, training loss: 14607.08, average training loss: 15852.05, base loss: 21658.13
[INFO 2017-06-28 18:12:32,412 main.py:51] epoch 538, training loss: 14973.73, average training loss: 15850.42, base loss: 21658.57
[INFO 2017-06-28 18:12:33,711 main.py:51] epoch 539, training loss: 14416.12, average training loss: 15847.76, base loss: 21659.07
[INFO 2017-06-28 18:12:35,100 main.py:51] epoch 540, training loss: 13616.72, average training loss: 15843.64, base loss: 21655.72
[INFO 2017-06-28 18:12:36,826 main.py:51] epoch 541, training loss: 13706.48, average training loss: 15839.70, base loss: 21655.86
[INFO 2017-06-28 18:12:38,248 main.py:51] epoch 542, training loss: 14577.28, average training loss: 15837.37, base loss: 21656.78
[INFO 2017-06-28 18:12:39,630 main.py:51] epoch 543, training loss: 14785.80, average training loss: 15835.44, base loss: 21655.02
[INFO 2017-06-28 18:12:40,990 main.py:51] epoch 544, training loss: 15970.83, average training loss: 15835.69, base loss: 21661.58
[INFO 2017-06-28 18:12:42,337 main.py:51] epoch 545, training loss: 15134.67, average training loss: 15834.40, base loss: 21662.19
[INFO 2017-06-28 18:12:43,602 main.py:51] epoch 546, training loss: 14393.21, average training loss: 15831.77, base loss: 21661.81
[INFO 2017-06-28 18:12:44,857 main.py:51] epoch 547, training loss: 15695.76, average training loss: 15831.52, base loss: 21667.81
[INFO 2017-06-28 18:12:46,229 main.py:51] epoch 548, training loss: 13658.43, average training loss: 15827.56, base loss: 21665.34
[INFO 2017-06-28 18:12:47,643 main.py:51] epoch 549, training loss: 15657.02, average training loss: 15827.25, base loss: 21666.63
[INFO 2017-06-28 18:12:49,175 main.py:51] epoch 550, training loss: 13202.72, average training loss: 15822.49, base loss: 21665.34
[INFO 2017-06-28 18:12:50,555 main.py:51] epoch 551, training loss: 14194.24, average training loss: 15819.54, base loss: 21665.41
[INFO 2017-06-28 18:12:51,920 main.py:51] epoch 552, training loss: 14354.97, average training loss: 15816.89, base loss: 21664.50
[INFO 2017-06-28 18:12:53,193 main.py:51] epoch 553, training loss: 13712.42, average training loss: 15813.09, base loss: 21662.86
[INFO 2017-06-28 18:12:54,514 main.py:51] epoch 554, training loss: 13347.96, average training loss: 15808.65, base loss: 21663.16
[INFO 2017-06-28 18:12:55,778 main.py:51] epoch 555, training loss: 15614.22, average training loss: 15808.30, base loss: 21666.52
[INFO 2017-06-28 18:12:57,104 main.py:51] epoch 556, training loss: 13463.59, average training loss: 15804.09, base loss: 21667.48
[INFO 2017-06-28 18:12:58,363 main.py:51] epoch 557, training loss: 13456.66, average training loss: 15799.88, base loss: 21667.27
[INFO 2017-06-28 18:12:59,638 main.py:51] epoch 558, training loss: 15834.39, average training loss: 15799.95, base loss: 21672.26
[INFO 2017-06-28 18:13:01,027 main.py:51] epoch 559, training loss: 14224.23, average training loss: 15797.13, base loss: 21673.04
[INFO 2017-06-28 18:13:02,606 main.py:51] epoch 560, training loss: 14545.67, average training loss: 15794.90, base loss: 21675.05
[INFO 2017-06-28 18:13:04,058 main.py:51] epoch 561, training loss: 13787.18, average training loss: 15791.33, base loss: 21676.15
[INFO 2017-06-28 18:13:05,405 main.py:51] epoch 562, training loss: 13516.50, average training loss: 15787.29, base loss: 21675.44
[INFO 2017-06-28 18:13:06,633 main.py:51] epoch 563, training loss: 13646.85, average training loss: 15783.49, base loss: 21673.78
[INFO 2017-06-28 18:13:08,070 main.py:51] epoch 564, training loss: 14815.21, average training loss: 15781.78, base loss: 21675.69
[INFO 2017-06-28 18:13:09,429 main.py:51] epoch 565, training loss: 14695.34, average training loss: 15779.86, base loss: 21677.10
[INFO 2017-06-28 18:13:10,913 main.py:51] epoch 566, training loss: 14771.41, average training loss: 15778.08, base loss: 21679.15
[INFO 2017-06-28 18:13:12,219 main.py:51] epoch 567, training loss: 14167.19, average training loss: 15775.25, base loss: 21678.16
[INFO 2017-06-28 18:13:13,665 main.py:51] epoch 568, training loss: 13684.69, average training loss: 15771.57, base loss: 21681.06
[INFO 2017-06-28 18:13:14,996 main.py:51] epoch 569, training loss: 13306.25, average training loss: 15767.25, base loss: 21679.00
[INFO 2017-06-28 18:13:16,325 main.py:51] epoch 570, training loss: 13807.48, average training loss: 15763.81, base loss: 21678.22
[INFO 2017-06-28 18:13:17,986 main.py:51] epoch 571, training loss: 15451.93, average training loss: 15763.27, base loss: 21682.78
[INFO 2017-06-28 18:13:19,331 main.py:51] epoch 572, training loss: 14103.26, average training loss: 15760.37, base loss: 21679.58
[INFO 2017-06-28 18:13:20,730 main.py:51] epoch 573, training loss: 14477.13, average training loss: 15758.14, base loss: 21680.02
[INFO 2017-06-28 18:13:22,043 main.py:51] epoch 574, training loss: 13826.87, average training loss: 15754.78, base loss: 21678.65
[INFO 2017-06-28 18:13:23,512 main.py:51] epoch 575, training loss: 12959.22, average training loss: 15749.92, base loss: 21674.70
[INFO 2017-06-28 18:13:24,847 main.py:51] epoch 576, training loss: 13007.33, average training loss: 15745.17, base loss: 21672.33
[INFO 2017-06-28 18:13:26,063 main.py:51] epoch 577, training loss: 13228.06, average training loss: 15740.82, base loss: 21667.71
[INFO 2017-06-28 18:13:27,411 main.py:51] epoch 578, training loss: 13770.62, average training loss: 15737.41, base loss: 21668.31
[INFO 2017-06-28 18:13:28,751 main.py:51] epoch 579, training loss: 14176.18, average training loss: 15734.72, base loss: 21666.31
[INFO 2017-06-28 18:13:30,117 main.py:51] epoch 580, training loss: 14502.29, average training loss: 15732.60, base loss: 21666.68
[INFO 2017-06-28 18:13:31,363 main.py:51] epoch 581, training loss: 14035.93, average training loss: 15729.68, base loss: 21667.04
[INFO 2017-06-28 18:13:32,577 main.py:51] epoch 582, training loss: 14000.49, average training loss: 15726.72, base loss: 21664.12
[INFO 2017-06-28 18:13:33,877 main.py:51] epoch 583, training loss: 14780.88, average training loss: 15725.10, base loss: 21664.77
[INFO 2017-06-28 18:13:35,291 main.py:51] epoch 584, training loss: 13086.22, average training loss: 15720.59, base loss: 21661.68
[INFO 2017-06-28 18:13:36,683 main.py:51] epoch 585, training loss: 13814.57, average training loss: 15717.34, base loss: 21657.77
[INFO 2017-06-28 18:13:38,100 main.py:51] epoch 586, training loss: 15402.81, average training loss: 15716.80, base loss: 21661.21
[INFO 2017-06-28 18:13:39,566 main.py:51] epoch 587, training loss: 15401.55, average training loss: 15716.26, base loss: 21663.33
[INFO 2017-06-28 18:13:40,860 main.py:51] epoch 588, training loss: 14938.79, average training loss: 15714.94, base loss: 21663.74
[INFO 2017-06-28 18:13:42,210 main.py:51] epoch 589, training loss: 14173.50, average training loss: 15712.33, base loss: 21662.78
[INFO 2017-06-28 18:13:43,715 main.py:51] epoch 590, training loss: 14990.93, average training loss: 15711.11, base loss: 21663.84
[INFO 2017-06-28 18:13:45,065 main.py:51] epoch 591, training loss: 14122.80, average training loss: 15708.43, base loss: 21666.23
[INFO 2017-06-28 18:13:46,337 main.py:51] epoch 592, training loss: 13642.87, average training loss: 15704.94, base loss: 21663.89
[INFO 2017-06-28 18:13:47,598 main.py:51] epoch 593, training loss: 15414.46, average training loss: 15704.46, base loss: 21666.50
[INFO 2017-06-28 18:13:49,151 main.py:51] epoch 594, training loss: 16538.58, average training loss: 15705.86, base loss: 21669.35
[INFO 2017-06-28 18:13:50,519 main.py:51] epoch 595, training loss: 13955.40, average training loss: 15702.92, base loss: 21668.22
[INFO 2017-06-28 18:13:51,969 main.py:51] epoch 596, training loss: 14184.10, average training loss: 15700.38, base loss: 21668.56
[INFO 2017-06-28 18:13:53,484 main.py:51] epoch 597, training loss: 14458.40, average training loss: 15698.30, base loss: 21670.69
[INFO 2017-06-28 18:13:55,183 main.py:51] epoch 598, training loss: 13561.33, average training loss: 15694.73, base loss: 21666.29
[INFO 2017-06-28 18:13:56,583 main.py:51] epoch 599, training loss: 12246.13, average training loss: 15688.98, base loss: 21662.40
[INFO 2017-06-28 18:13:56,583 main.py:53] epoch 599, testing
[INFO 2017-06-28 18:14:03,313 main.py:105] average testing loss: 16093.89, base loss: 22759.97
[INFO 2017-06-28 18:14:03,314 main.py:106] improve_loss: 6666.08, improve_percent: 0.29
[INFO 2017-06-28 18:14:03,314 main.py:76] current best improved percent: 0.30
[INFO 2017-06-28 18:14:04,786 main.py:51] epoch 600, training loss: 12816.63, average training loss: 15684.20, base loss: 21659.34
[INFO 2017-06-28 18:14:06,043 main.py:51] epoch 601, training loss: 13371.70, average training loss: 15680.36, base loss: 21656.30
[INFO 2017-06-28 18:14:07,349 main.py:51] epoch 602, training loss: 13672.47, average training loss: 15677.03, base loss: 21655.15
[INFO 2017-06-28 18:14:08,589 main.py:51] epoch 603, training loss: 14419.66, average training loss: 15674.95, base loss: 21655.00
[INFO 2017-06-28 18:14:09,980 main.py:51] epoch 604, training loss: 13606.65, average training loss: 15671.53, base loss: 21653.11
[INFO 2017-06-28 18:14:11,509 main.py:51] epoch 605, training loss: 14697.86, average training loss: 15669.93, base loss: 21652.10
[INFO 2017-06-28 18:14:12,965 main.py:51] epoch 606, training loss: 12341.22, average training loss: 15664.44, base loss: 21647.41
[INFO 2017-06-28 18:14:14,404 main.py:51] epoch 607, training loss: 14773.38, average training loss: 15662.98, base loss: 21650.29
[INFO 2017-06-28 18:14:15,867 main.py:51] epoch 608, training loss: 13859.39, average training loss: 15660.02, base loss: 21651.09
[INFO 2017-06-28 18:14:17,244 main.py:51] epoch 609, training loss: 13890.71, average training loss: 15657.11, base loss: 21649.40
[INFO 2017-06-28 18:14:18,557 main.py:51] epoch 610, training loss: 13236.92, average training loss: 15653.15, base loss: 21646.94
[INFO 2017-06-28 18:14:19,867 main.py:51] epoch 611, training loss: 13695.76, average training loss: 15649.96, base loss: 21645.56
[INFO 2017-06-28 18:14:21,094 main.py:51] epoch 612, training loss: 13070.63, average training loss: 15645.75, base loss: 21641.69
[INFO 2017-06-28 18:14:22,409 main.py:51] epoch 613, training loss: 12973.75, average training loss: 15641.40, base loss: 21640.29
[INFO 2017-06-28 18:14:23,709 main.py:51] epoch 614, training loss: 13598.57, average training loss: 15638.07, base loss: 21639.37
[INFO 2017-06-28 18:14:25,048 main.py:51] epoch 615, training loss: 15338.83, average training loss: 15637.59, base loss: 21639.15
[INFO 2017-06-28 18:14:26,358 main.py:51] epoch 616, training loss: 13631.32, average training loss: 15634.34, base loss: 21640.77
[INFO 2017-06-28 18:14:27,683 main.py:51] epoch 617, training loss: 12480.24, average training loss: 15629.23, base loss: 21638.36
[INFO 2017-06-28 18:14:29,204 main.py:51] epoch 618, training loss: 13878.60, average training loss: 15626.40, base loss: 21639.49
[INFO 2017-06-28 18:14:30,588 main.py:51] epoch 619, training loss: 13890.27, average training loss: 15623.60, base loss: 21638.50
[INFO 2017-06-28 18:14:31,878 main.py:51] epoch 620, training loss: 13249.00, average training loss: 15619.78, base loss: 21636.06
[INFO 2017-06-28 18:14:33,246 main.py:51] epoch 621, training loss: 15406.76, average training loss: 15619.44, base loss: 21638.54
[INFO 2017-06-28 18:14:34,646 main.py:51] epoch 622, training loss: 12349.92, average training loss: 15614.19, base loss: 21633.55
[INFO 2017-06-28 18:14:35,837 main.py:51] epoch 623, training loss: 14117.16, average training loss: 15611.79, base loss: 21635.94
[INFO 2017-06-28 18:14:37,199 main.py:51] epoch 624, training loss: 13194.38, average training loss: 15607.92, base loss: 21633.35
[INFO 2017-06-28 18:14:38,874 main.py:51] epoch 625, training loss: 12599.54, average training loss: 15603.12, base loss: 21631.72
[INFO 2017-06-28 18:14:40,150 main.py:51] epoch 626, training loss: 15053.03, average training loss: 15602.24, base loss: 21634.02
[INFO 2017-06-28 18:14:41,487 main.py:51] epoch 627, training loss: 13637.25, average training loss: 15599.11, base loss: 21634.49
[INFO 2017-06-28 18:14:42,825 main.py:51] epoch 628, training loss: 14426.28, average training loss: 15597.25, base loss: 21634.87
[INFO 2017-06-28 18:14:44,066 main.py:51] epoch 629, training loss: 12797.46, average training loss: 15592.80, base loss: 21634.52
[INFO 2017-06-28 18:14:45,444 main.py:51] epoch 630, training loss: 13255.05, average training loss: 15589.10, base loss: 21633.79
[INFO 2017-06-28 18:14:46,870 main.py:51] epoch 631, training loss: 13718.06, average training loss: 15586.14, base loss: 21632.50
[INFO 2017-06-28 18:14:48,265 main.py:51] epoch 632, training loss: 13944.62, average training loss: 15583.54, base loss: 21631.58
[INFO 2017-06-28 18:14:49,656 main.py:51] epoch 633, training loss: 13036.24, average training loss: 15579.53, base loss: 21632.27
[INFO 2017-06-28 18:14:51,070 main.py:51] epoch 634, training loss: 13406.20, average training loss: 15576.10, base loss: 21630.19
[INFO 2017-06-28 18:14:52,507 main.py:51] epoch 635, training loss: 14645.32, average training loss: 15574.64, base loss: 21631.77
[INFO 2017-06-28 18:14:53,714 main.py:51] epoch 636, training loss: 14876.70, average training loss: 15573.54, base loss: 21632.13
[INFO 2017-06-28 18:14:55,024 main.py:51] epoch 637, training loss: 14834.64, average training loss: 15572.39, base loss: 21636.17
[INFO 2017-06-28 18:14:56,375 main.py:51] epoch 638, training loss: 14582.66, average training loss: 15570.84, base loss: 21634.18
[INFO 2017-06-28 18:14:57,719 main.py:51] epoch 639, training loss: 15030.56, average training loss: 15569.99, base loss: 21638.44
[INFO 2017-06-28 18:14:59,090 main.py:51] epoch 640, training loss: 14910.10, average training loss: 15568.96, base loss: 21640.22
[INFO 2017-06-28 18:15:00,413 main.py:51] epoch 641, training loss: 15624.10, average training loss: 15569.05, base loss: 21643.57
[INFO 2017-06-28 18:15:01,705 main.py:51] epoch 642, training loss: 13483.73, average training loss: 15565.81, base loss: 21642.80
[INFO 2017-06-28 18:15:03,056 main.py:51] epoch 643, training loss: 12576.88, average training loss: 15561.17, base loss: 21640.40
[INFO 2017-06-28 18:15:04,452 main.py:51] epoch 644, training loss: 15698.52, average training loss: 15561.38, base loss: 21643.64
[INFO 2017-06-28 18:15:05,745 main.py:51] epoch 645, training loss: 15093.53, average training loss: 15560.65, base loss: 21643.13
[INFO 2017-06-28 18:15:07,362 main.py:51] epoch 646, training loss: 15563.05, average training loss: 15560.66, base loss: 21645.08
[INFO 2017-06-28 18:15:08,619 main.py:51] epoch 647, training loss: 14047.89, average training loss: 15558.32, base loss: 21645.03
[INFO 2017-06-28 18:15:09,952 main.py:51] epoch 648, training loss: 13659.33, average training loss: 15555.40, base loss: 21645.64
[INFO 2017-06-28 18:15:11,647 main.py:51] epoch 649, training loss: 14517.02, average training loss: 15553.80, base loss: 21646.13
[INFO 2017-06-28 18:15:12,974 main.py:51] epoch 650, training loss: 14654.30, average training loss: 15552.42, base loss: 21645.50
[INFO 2017-06-28 18:15:14,350 main.py:51] epoch 651, training loss: 14189.97, average training loss: 15550.33, base loss: 21643.71
[INFO 2017-06-28 18:15:15,830 main.py:51] epoch 652, training loss: 17351.81, average training loss: 15553.09, base loss: 21647.10
[INFO 2017-06-28 18:15:17,175 main.py:51] epoch 653, training loss: 13614.43, average training loss: 15550.12, base loss: 21648.95
[INFO 2017-06-28 18:15:18,436 main.py:51] epoch 654, training loss: 14459.86, average training loss: 15548.46, base loss: 21648.54
[INFO 2017-06-28 18:15:19,725 main.py:51] epoch 655, training loss: 15138.88, average training loss: 15547.83, base loss: 21651.26
[INFO 2017-06-28 18:15:21,046 main.py:51] epoch 656, training loss: 15325.79, average training loss: 15547.50, base loss: 21655.67
[INFO 2017-06-28 18:15:22,444 main.py:51] epoch 657, training loss: 15282.07, average training loss: 15547.09, base loss: 21658.05
[INFO 2017-06-28 18:15:23,766 main.py:51] epoch 658, training loss: 15076.01, average training loss: 15546.38, base loss: 21660.78
[INFO 2017-06-28 18:15:25,111 main.py:51] epoch 659, training loss: 14171.08, average training loss: 15544.29, base loss: 21662.04
[INFO 2017-06-28 18:15:26,533 main.py:51] epoch 660, training loss: 14238.31, average training loss: 15542.32, base loss: 21662.32
[INFO 2017-06-28 18:15:27,813 main.py:51] epoch 661, training loss: 15055.63, average training loss: 15541.58, base loss: 21664.49
[INFO 2017-06-28 18:15:29,117 main.py:51] epoch 662, training loss: 13360.13, average training loss: 15538.29, base loss: 21660.46
[INFO 2017-06-28 18:15:30,490 main.py:51] epoch 663, training loss: 14755.99, average training loss: 15537.11, base loss: 21662.61
[INFO 2017-06-28 18:15:31,945 main.py:51] epoch 664, training loss: 14003.34, average training loss: 15534.81, base loss: 21663.84
[INFO 2017-06-28 18:15:33,355 main.py:51] epoch 665, training loss: 13159.72, average training loss: 15531.24, base loss: 21662.77
[INFO 2017-06-28 18:15:34,836 main.py:51] epoch 666, training loss: 14603.69, average training loss: 15529.85, base loss: 21662.88
[INFO 2017-06-28 18:15:36,149 main.py:51] epoch 667, training loss: 13714.62, average training loss: 15527.13, base loss: 21663.36
[INFO 2017-06-28 18:15:37,690 main.py:51] epoch 668, training loss: 15619.93, average training loss: 15527.27, base loss: 21666.08
[INFO 2017-06-28 18:15:38,932 main.py:51] epoch 669, training loss: 14734.80, average training loss: 15526.09, base loss: 21665.88
[INFO 2017-06-28 18:15:40,216 main.py:51] epoch 670, training loss: 14064.46, average training loss: 15523.91, base loss: 21664.39
[INFO 2017-06-28 18:15:41,476 main.py:51] epoch 671, training loss: 13552.82, average training loss: 15520.98, base loss: 21662.25
[INFO 2017-06-28 18:15:42,800 main.py:51] epoch 672, training loss: 14160.38, average training loss: 15518.96, base loss: 21664.30
[INFO 2017-06-28 18:15:44,165 main.py:51] epoch 673, training loss: 13203.17, average training loss: 15515.52, base loss: 21663.08
[INFO 2017-06-28 18:15:45,447 main.py:51] epoch 674, training loss: 13438.55, average training loss: 15512.44, base loss: 21662.57
[INFO 2017-06-28 18:15:46,726 main.py:51] epoch 675, training loss: 15013.81, average training loss: 15511.71, base loss: 21663.36
[INFO 2017-06-28 18:15:48,126 main.py:51] epoch 676, training loss: 13551.53, average training loss: 15508.81, base loss: 21661.70
[INFO 2017-06-28 18:15:49,516 main.py:51] epoch 677, training loss: 13203.92, average training loss: 15505.41, base loss: 21660.46
[INFO 2017-06-28 18:15:50,966 main.py:51] epoch 678, training loss: 14226.09, average training loss: 15503.53, base loss: 21662.22
[INFO 2017-06-28 18:15:52,287 main.py:51] epoch 679, training loss: 13343.75, average training loss: 15500.35, base loss: 21661.45
[INFO 2017-06-28 18:15:53,687 main.py:51] epoch 680, training loss: 15430.25, average training loss: 15500.25, base loss: 21663.02
[INFO 2017-06-28 18:15:55,263 main.py:51] epoch 681, training loss: 15103.79, average training loss: 15499.67, base loss: 21663.14
[INFO 2017-06-28 18:15:56,633 main.py:51] epoch 682, training loss: 13369.41, average training loss: 15496.55, base loss: 21659.98
[INFO 2017-06-28 18:15:57,968 main.py:51] epoch 683, training loss: 14652.46, average training loss: 15495.31, base loss: 21662.38
[INFO 2017-06-28 18:15:59,458 main.py:51] epoch 684, training loss: 12864.10, average training loss: 15491.47, base loss: 21659.67
[INFO 2017-06-28 18:16:00,803 main.py:51] epoch 685, training loss: 12414.33, average training loss: 15486.99, base loss: 21656.78
[INFO 2017-06-28 18:16:02,029 main.py:51] epoch 686, training loss: 14264.07, average training loss: 15485.21, base loss: 21656.66
[INFO 2017-06-28 18:16:03,276 main.py:51] epoch 687, training loss: 13398.98, average training loss: 15482.17, base loss: 21654.90
[INFO 2017-06-28 18:16:04,671 main.py:51] epoch 688, training loss: 14916.71, average training loss: 15481.35, base loss: 21655.39
[INFO 2017-06-28 18:16:06,068 main.py:51] epoch 689, training loss: 16399.54, average training loss: 15482.68, base loss: 21659.99
[INFO 2017-06-28 18:16:07,263 main.py:51] epoch 690, training loss: 13528.10, average training loss: 15479.86, base loss: 21659.86
[INFO 2017-06-28 18:16:08,538 main.py:51] epoch 691, training loss: 14215.91, average training loss: 15478.03, base loss: 21661.03
[INFO 2017-06-28 18:16:10,040 main.py:51] epoch 692, training loss: 14155.26, average training loss: 15476.12, base loss: 21664.18
[INFO 2017-06-28 18:16:11,416 main.py:51] epoch 693, training loss: 13286.61, average training loss: 15472.97, base loss: 21662.28
[INFO 2017-06-28 18:16:12,715 main.py:51] epoch 694, training loss: 12906.39, average training loss: 15469.27, base loss: 21661.00
[INFO 2017-06-28 18:16:14,203 main.py:51] epoch 695, training loss: 13391.67, average training loss: 15466.29, base loss: 21661.64
[INFO 2017-06-28 18:16:15,514 main.py:51] epoch 696, training loss: 13254.30, average training loss: 15463.11, base loss: 21658.87
[INFO 2017-06-28 18:16:17,017 main.py:51] epoch 697, training loss: 13711.42, average training loss: 15460.60, base loss: 21658.51
[INFO 2017-06-28 18:16:18,423 main.py:51] epoch 698, training loss: 14551.53, average training loss: 15459.30, base loss: 21657.64
[INFO 2017-06-28 18:16:19,756 main.py:51] epoch 699, training loss: 16251.34, average training loss: 15460.44, base loss: 21661.26
[INFO 2017-06-28 18:16:19,756 main.py:53] epoch 699, testing
[INFO 2017-06-28 18:16:26,042 main.py:105] average testing loss: 14986.39, base loss: 21850.75
[INFO 2017-06-28 18:16:26,042 main.py:106] improve_loss: 6864.37, improve_percent: 0.31
[INFO 2017-06-28 18:16:26,043 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:16:26,056 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 18:16:27,363 main.py:51] epoch 700, training loss: 13499.42, average training loss: 15457.64, base loss: 21660.69
[INFO 2017-06-28 18:16:28,746 main.py:51] epoch 701, training loss: 13390.29, average training loss: 15454.69, base loss: 21659.10
[INFO 2017-06-28 18:16:30,298 main.py:51] epoch 702, training loss: 12491.47, average training loss: 15450.48, base loss: 21658.06
[INFO 2017-06-28 18:16:31,756 main.py:51] epoch 703, training loss: 13021.93, average training loss: 15447.03, base loss: 21657.45
[INFO 2017-06-28 18:16:33,208 main.py:51] epoch 704, training loss: 14330.72, average training loss: 15445.45, base loss: 21657.02
[INFO 2017-06-28 18:16:34,866 main.py:51] epoch 705, training loss: 12789.30, average training loss: 15441.68, base loss: 21655.16
[INFO 2017-06-28 18:16:36,149 main.py:51] epoch 706, training loss: 14689.22, average training loss: 15440.62, base loss: 21655.38
[INFO 2017-06-28 18:16:37,589 main.py:51] epoch 707, training loss: 15745.86, average training loss: 15441.05, base loss: 21659.47
[INFO 2017-06-28 18:16:38,919 main.py:51] epoch 708, training loss: 13491.93, average training loss: 15438.30, base loss: 21661.37
[INFO 2017-06-28 18:16:40,267 main.py:51] epoch 709, training loss: 12963.01, average training loss: 15434.81, base loss: 21661.65
[INFO 2017-06-28 18:16:41,582 main.py:51] epoch 710, training loss: 13725.54, average training loss: 15432.41, base loss: 21660.50
[INFO 2017-06-28 18:16:42,830 main.py:51] epoch 711, training loss: 13247.13, average training loss: 15429.34, base loss: 21657.96
[INFO 2017-06-28 18:16:44,176 main.py:51] epoch 712, training loss: 14430.63, average training loss: 15427.94, base loss: 21658.56
[INFO 2017-06-28 18:16:45,432 main.py:51] epoch 713, training loss: 14116.09, average training loss: 15426.10, base loss: 21659.22
[INFO 2017-06-28 18:16:46,811 main.py:51] epoch 714, training loss: 13939.49, average training loss: 15424.02, base loss: 21660.37
[INFO 2017-06-28 18:16:48,154 main.py:51] epoch 715, training loss: 15455.29, average training loss: 15424.07, base loss: 21660.08
[INFO 2017-06-28 18:16:49,433 main.py:51] epoch 716, training loss: 13485.42, average training loss: 15421.36, base loss: 21660.52
[INFO 2017-06-28 18:16:50,685 main.py:51] epoch 717, training loss: 13762.32, average training loss: 15419.05, base loss: 21662.20
[INFO 2017-06-28 18:16:51,958 main.py:51] epoch 718, training loss: 14930.10, average training loss: 15418.37, base loss: 21663.27
[INFO 2017-06-28 18:16:53,256 main.py:51] epoch 719, training loss: 14048.54, average training loss: 15416.47, base loss: 21662.05
[INFO 2017-06-28 18:16:54,667 main.py:51] epoch 720, training loss: 12983.94, average training loss: 15413.10, base loss: 21659.18
[INFO 2017-06-28 18:16:56,010 main.py:51] epoch 721, training loss: 13235.55, average training loss: 15410.08, base loss: 21658.16
[INFO 2017-06-28 18:16:57,477 main.py:51] epoch 722, training loss: 14487.34, average training loss: 15408.80, base loss: 21658.86
[INFO 2017-06-28 18:16:58,749 main.py:51] epoch 723, training loss: 13629.62, average training loss: 15406.35, base loss: 21657.56
[INFO 2017-06-28 18:17:00,014 main.py:51] epoch 724, training loss: 14995.83, average training loss: 15405.78, base loss: 21660.40
[INFO 2017-06-28 18:17:01,213 main.py:51] epoch 725, training loss: 13618.00, average training loss: 15403.32, base loss: 21660.83
[INFO 2017-06-28 18:17:02,503 main.py:51] epoch 726, training loss: 15068.11, average training loss: 15402.86, base loss: 21664.06
[INFO 2017-06-28 18:17:03,743 main.py:51] epoch 727, training loss: 14060.45, average training loss: 15401.01, base loss: 21663.13
[INFO 2017-06-28 18:17:05,093 main.py:51] epoch 728, training loss: 13515.68, average training loss: 15398.43, base loss: 21663.56
[INFO 2017-06-28 18:17:06,557 main.py:51] epoch 729, training loss: 14083.90, average training loss: 15396.63, base loss: 21664.25
[INFO 2017-06-28 18:17:07,861 main.py:51] epoch 730, training loss: 14878.71, average training loss: 15395.92, base loss: 21664.67
[INFO 2017-06-28 18:17:09,204 main.py:51] epoch 731, training loss: 14122.86, average training loss: 15394.18, base loss: 21662.73
[INFO 2017-06-28 18:17:10,596 main.py:51] epoch 732, training loss: 13608.50, average training loss: 15391.74, base loss: 21661.19
[INFO 2017-06-28 18:17:12,100 main.py:51] epoch 733, training loss: 14584.21, average training loss: 15390.64, base loss: 21663.56
[INFO 2017-06-28 18:17:13,484 main.py:51] epoch 734, training loss: 13475.93, average training loss: 15388.04, base loss: 21662.33
[INFO 2017-06-28 18:17:14,763 main.py:51] epoch 735, training loss: 12313.34, average training loss: 15383.86, base loss: 21659.24
[INFO 2017-06-28 18:17:16,182 main.py:51] epoch 736, training loss: 13172.49, average training loss: 15380.86, base loss: 21656.07
[INFO 2017-06-28 18:17:17,678 main.py:51] epoch 737, training loss: 13052.47, average training loss: 15377.70, base loss: 21655.46
[INFO 2017-06-28 18:17:19,091 main.py:51] epoch 738, training loss: 13869.05, average training loss: 15375.66, base loss: 21653.56
[INFO 2017-06-28 18:17:20,530 main.py:51] epoch 739, training loss: 12316.33, average training loss: 15371.53, base loss: 21650.02
[INFO 2017-06-28 18:17:21,916 main.py:51] epoch 740, training loss: 13881.23, average training loss: 15369.52, base loss: 21650.94
[INFO 2017-06-28 18:17:23,477 main.py:51] epoch 741, training loss: 13158.33, average training loss: 15366.54, base loss: 21649.99
[INFO 2017-06-28 18:17:24,893 main.py:51] epoch 742, training loss: 15198.01, average training loss: 15366.31, base loss: 21651.24
[INFO 2017-06-28 18:17:26,352 main.py:51] epoch 743, training loss: 13132.49, average training loss: 15363.31, base loss: 21650.88
[INFO 2017-06-28 18:17:27,743 main.py:51] epoch 744, training loss: 13821.59, average training loss: 15361.24, base loss: 21649.70
[INFO 2017-06-28 18:17:29,394 main.py:51] epoch 745, training loss: 13488.04, average training loss: 15358.73, base loss: 21648.98
[INFO 2017-06-28 18:17:30,929 main.py:51] epoch 746, training loss: 13740.00, average training loss: 15356.56, base loss: 21648.55
[INFO 2017-06-28 18:17:32,282 main.py:51] epoch 747, training loss: 13720.83, average training loss: 15354.37, base loss: 21648.04
[INFO 2017-06-28 18:17:33,612 main.py:51] epoch 748, training loss: 12466.67, average training loss: 15350.52, base loss: 21646.00
[INFO 2017-06-28 18:17:34,957 main.py:51] epoch 749, training loss: 13656.06, average training loss: 15348.26, base loss: 21646.15
[INFO 2017-06-28 18:17:36,288 main.py:51] epoch 750, training loss: 13762.71, average training loss: 15346.15, base loss: 21645.96
[INFO 2017-06-28 18:17:37,683 main.py:51] epoch 751, training loss: 14450.63, average training loss: 15344.96, base loss: 21648.16
[INFO 2017-06-28 18:17:38,931 main.py:51] epoch 752, training loss: 13584.32, average training loss: 15342.62, base loss: 21646.84
[INFO 2017-06-28 18:17:40,216 main.py:51] epoch 753, training loss: 15347.42, average training loss: 15342.63, base loss: 21648.92
[INFO 2017-06-28 18:17:41,400 main.py:51] epoch 754, training loss: 14670.62, average training loss: 15341.73, base loss: 21650.05
[INFO 2017-06-28 18:17:42,786 main.py:51] epoch 755, training loss: 14548.97, average training loss: 15340.69, base loss: 21651.53
[INFO 2017-06-28 18:17:44,113 main.py:51] epoch 756, training loss: 13567.34, average training loss: 15338.34, base loss: 21650.86
[INFO 2017-06-28 18:17:45,408 main.py:51] epoch 757, training loss: 13594.34, average training loss: 15336.04, base loss: 21650.53
[INFO 2017-06-28 18:17:46,766 main.py:51] epoch 758, training loss: 15246.77, average training loss: 15335.93, base loss: 21651.67
[INFO 2017-06-28 18:17:48,092 main.py:51] epoch 759, training loss: 14401.67, average training loss: 15334.70, base loss: 21652.04
[INFO 2017-06-28 18:17:49,509 main.py:51] epoch 760, training loss: 15579.26, average training loss: 15335.02, base loss: 21655.34
[INFO 2017-06-28 18:17:50,985 main.py:51] epoch 761, training loss: 14709.68, average training loss: 15334.20, base loss: 21656.98
[INFO 2017-06-28 18:17:52,495 main.py:51] epoch 762, training loss: 14904.99, average training loss: 15333.63, base loss: 21659.53
[INFO 2017-06-28 18:17:53,943 main.py:51] epoch 763, training loss: 13568.08, average training loss: 15331.32, base loss: 21657.71
[INFO 2017-06-28 18:17:55,352 main.py:51] epoch 764, training loss: 14180.41, average training loss: 15329.82, base loss: 21656.68
[INFO 2017-06-28 18:17:56,644 main.py:51] epoch 765, training loss: 13743.30, average training loss: 15327.75, base loss: 21656.72
[INFO 2017-06-28 18:17:58,086 main.py:51] epoch 766, training loss: 13755.98, average training loss: 15325.70, base loss: 21655.46
[INFO 2017-06-28 18:17:59,401 main.py:51] epoch 767, training loss: 15834.48, average training loss: 15326.36, base loss: 21656.92
[INFO 2017-06-28 18:18:00,877 main.py:51] epoch 768, training loss: 14586.43, average training loss: 15325.40, base loss: 21657.91
[INFO 2017-06-28 18:18:02,112 main.py:51] epoch 769, training loss: 13047.92, average training loss: 15322.44, base loss: 21656.35
[INFO 2017-06-28 18:18:03,399 main.py:51] epoch 770, training loss: 13694.50, average training loss: 15320.33, base loss: 21655.48
[INFO 2017-06-28 18:18:04,668 main.py:51] epoch 771, training loss: 15425.29, average training loss: 15320.47, base loss: 21657.43
[INFO 2017-06-28 18:18:05,970 main.py:51] epoch 772, training loss: 14095.75, average training loss: 15318.88, base loss: 21659.25
[INFO 2017-06-28 18:18:07,449 main.py:51] epoch 773, training loss: 13657.53, average training loss: 15316.73, base loss: 21657.57
[INFO 2017-06-28 18:18:08,754 main.py:51] epoch 774, training loss: 13895.37, average training loss: 15314.90, base loss: 21657.83
[INFO 2017-06-28 18:18:10,048 main.py:51] epoch 775, training loss: 14689.86, average training loss: 15314.10, base loss: 21659.46
[INFO 2017-06-28 18:18:11,743 main.py:51] epoch 776, training loss: 12667.98, average training loss: 15310.69, base loss: 21656.16
[INFO 2017-06-28 18:18:13,116 main.py:51] epoch 777, training loss: 14475.61, average training loss: 15309.62, base loss: 21656.42
[INFO 2017-06-28 18:18:14,527 main.py:51] epoch 778, training loss: 15525.85, average training loss: 15309.89, base loss: 21659.77
[INFO 2017-06-28 18:18:15,895 main.py:51] epoch 779, training loss: 13788.12, average training loss: 15307.94, base loss: 21657.63
[INFO 2017-06-28 18:18:17,307 main.py:51] epoch 780, training loss: 15213.15, average training loss: 15307.82, base loss: 21659.68
[INFO 2017-06-28 18:18:18,639 main.py:51] epoch 781, training loss: 13170.34, average training loss: 15305.09, base loss: 21659.76
[INFO 2017-06-28 18:18:20,193 main.py:51] epoch 782, training loss: 13490.41, average training loss: 15302.77, base loss: 21658.52
[INFO 2017-06-28 18:18:21,471 main.py:51] epoch 783, training loss: 14052.23, average training loss: 15301.18, base loss: 21658.86
[INFO 2017-06-28 18:18:22,917 main.py:51] epoch 784, training loss: 13324.77, average training loss: 15298.66, base loss: 21658.05
[INFO 2017-06-28 18:18:24,283 main.py:51] epoch 785, training loss: 14716.02, average training loss: 15297.92, base loss: 21659.21
[INFO 2017-06-28 18:18:25,706 main.py:51] epoch 786, training loss: 13709.99, average training loss: 15295.90, base loss: 21658.21
[INFO 2017-06-28 18:18:27,010 main.py:51] epoch 787, training loss: 13480.26, average training loss: 15293.59, base loss: 21657.19
[INFO 2017-06-28 18:18:28,285 main.py:51] epoch 788, training loss: 14002.06, average training loss: 15291.96, base loss: 21656.54
[INFO 2017-06-28 18:18:29,625 main.py:51] epoch 789, training loss: 14036.90, average training loss: 15290.37, base loss: 21657.10
[INFO 2017-06-28 18:18:30,916 main.py:51] epoch 790, training loss: 12691.58, average training loss: 15287.08, base loss: 21655.44
[INFO 2017-06-28 18:18:32,162 main.py:51] epoch 791, training loss: 13948.77, average training loss: 15285.39, base loss: 21656.97
[INFO 2017-06-28 18:18:33,642 main.py:51] epoch 792, training loss: 14617.31, average training loss: 15284.55, base loss: 21658.05
[INFO 2017-06-28 18:18:34,938 main.py:51] epoch 793, training loss: 13293.90, average training loss: 15282.04, base loss: 21657.32
[INFO 2017-06-28 18:18:36,230 main.py:51] epoch 794, training loss: 13602.00, average training loss: 15279.93, base loss: 21656.82
[INFO 2017-06-28 18:18:37,566 main.py:51] epoch 795, training loss: 13372.76, average training loss: 15277.53, base loss: 21655.17
[INFO 2017-06-28 18:18:38,953 main.py:51] epoch 796, training loss: 12961.52, average training loss: 15274.63, base loss: 21654.88
[INFO 2017-06-28 18:18:40,534 main.py:51] epoch 797, training loss: 13765.96, average training loss: 15272.74, base loss: 21653.42
[INFO 2017-06-28 18:18:41,793 main.py:51] epoch 798, training loss: 13766.82, average training loss: 15270.85, base loss: 21653.16
[INFO 2017-06-28 18:18:43,236 main.py:51] epoch 799, training loss: 12655.58, average training loss: 15267.58, base loss: 21651.69
[INFO 2017-06-28 18:18:43,236 main.py:53] epoch 799, testing
[INFO 2017-06-28 18:18:49,607 main.py:105] average testing loss: 15032.54, base loss: 21707.56
[INFO 2017-06-28 18:18:49,607 main.py:106] improve_loss: 6675.02, improve_percent: 0.31
[INFO 2017-06-28 18:18:49,608 main.py:76] current best improved percent: 0.31
[INFO 2017-06-28 18:18:50,850 main.py:51] epoch 800, training loss: 15953.42, average training loss: 15268.44, base loss: 21652.63
[INFO 2017-06-28 18:18:52,114 main.py:51] epoch 801, training loss: 12551.83, average training loss: 15265.05, base loss: 21651.23
[INFO 2017-06-28 18:18:53,384 main.py:51] epoch 802, training loss: 12249.87, average training loss: 15261.30, base loss: 21648.60
[INFO 2017-06-28 18:18:54,623 main.py:51] epoch 803, training loss: 15031.57, average training loss: 15261.01, base loss: 21650.20
[INFO 2017-06-28 18:18:55,933 main.py:51] epoch 804, training loss: 13241.55, average training loss: 15258.50, base loss: 21648.30
[INFO 2017-06-28 18:18:57,254 main.py:51] epoch 805, training loss: 14826.99, average training loss: 15257.97, base loss: 21649.93
[INFO 2017-06-28 18:18:58,587 main.py:51] epoch 806, training loss: 14261.05, average training loss: 15256.73, base loss: 21649.39
[INFO 2017-06-28 18:19:00,017 main.py:51] epoch 807, training loss: 14466.50, average training loss: 15255.76, base loss: 21650.26
[INFO 2017-06-28 18:19:01,304 main.py:51] epoch 808, training loss: 14306.64, average training loss: 15254.58, base loss: 21650.40
[INFO 2017-06-28 18:19:02,655 main.py:51] epoch 809, training loss: 12794.12, average training loss: 15251.54, base loss: 21647.59
[INFO 2017-06-28 18:19:03,979 main.py:51] epoch 810, training loss: 13101.58, average training loss: 15248.89, base loss: 21646.80
[INFO 2017-06-28 18:19:05,337 main.py:51] epoch 811, training loss: 14760.02, average training loss: 15248.29, base loss: 21647.58
[INFO 2017-06-28 18:19:06,625 main.py:51] epoch 812, training loss: 11661.63, average training loss: 15243.88, base loss: 21644.82
[INFO 2017-06-28 18:19:08,031 main.py:51] epoch 813, training loss: 14668.23, average training loss: 15243.17, base loss: 21646.95
[INFO 2017-06-28 18:19:09,213 main.py:51] epoch 814, training loss: 14492.30, average training loss: 15242.25, base loss: 21647.42
[INFO 2017-06-28 18:19:10,594 main.py:51] epoch 815, training loss: 14883.33, average training loss: 15241.81, base loss: 21648.96
[INFO 2017-06-28 18:19:12,046 main.py:51] epoch 816, training loss: 15028.06, average training loss: 15241.55, base loss: 21651.18
[INFO 2017-06-28 18:19:13,459 main.py:51] epoch 817, training loss: 13233.71, average training loss: 15239.10, base loss: 21649.08
[INFO 2017-06-28 18:19:14,722 main.py:51] epoch 818, training loss: 13922.62, average training loss: 15237.49, base loss: 21649.94
[INFO 2017-06-28 18:19:15,989 main.py:51] epoch 819, training loss: 15033.18, average training loss: 15237.24, base loss: 21652.21
[INFO 2017-06-28 18:19:17,315 main.py:51] epoch 820, training loss: 13548.08, average training loss: 15235.18, base loss: 21652.99
[INFO 2017-06-28 18:19:18,681 main.py:51] epoch 821, training loss: 16190.88, average training loss: 15236.34, base loss: 21657.29
[INFO 2017-06-28 18:19:20,152 main.py:51] epoch 822, training loss: 13224.47, average training loss: 15233.90, base loss: 21657.81
[INFO 2017-06-28 18:19:21,461 main.py:51] epoch 823, training loss: 13842.92, average training loss: 15232.21, base loss: 21658.36
[INFO 2017-06-28 18:19:22,798 main.py:51] epoch 824, training loss: 12872.97, average training loss: 15229.35, base loss: 21658.17
[INFO 2017-06-28 18:19:24,125 main.py:51] epoch 825, training loss: 13937.33, average training loss: 15227.79, base loss: 21657.91
[INFO 2017-06-28 18:19:25,477 main.py:51] epoch 826, training loss: 13358.12, average training loss: 15225.53, base loss: 21657.58
[INFO 2017-06-28 18:19:26,821 main.py:51] epoch 827, training loss: 14432.59, average training loss: 15224.57, base loss: 21659.32
[INFO 2017-06-28 18:19:28,352 main.py:51] epoch 828, training loss: 13944.70, average training loss: 15223.03, base loss: 21658.49
[INFO 2017-06-28 18:19:29,825 main.py:51] epoch 829, training loss: 14038.69, average training loss: 15221.60, base loss: 21659.61
[INFO 2017-06-28 18:19:31,120 main.py:51] epoch 830, training loss: 13524.24, average training loss: 15219.56, base loss: 21659.74
[INFO 2017-06-28 18:19:32,633 main.py:51] epoch 831, training loss: 13315.79, average training loss: 15217.27, base loss: 21660.72
[INFO 2017-06-28 18:19:34,038 main.py:51] epoch 832, training loss: 15186.43, average training loss: 15217.23, base loss: 21661.20
[INFO 2017-06-28 18:19:35,508 main.py:51] epoch 833, training loss: 14972.42, average training loss: 15216.94, base loss: 21663.77
[INFO 2017-06-28 18:19:36,864 main.py:51] epoch 834, training loss: 13676.63, average training loss: 15215.09, base loss: 21664.57
[INFO 2017-06-28 18:19:38,202 main.py:51] epoch 835, training loss: 12832.52, average training loss: 15212.24, base loss: 21661.50
[INFO 2017-06-28 18:19:39,660 main.py:51] epoch 836, training loss: 12182.36, average training loss: 15208.62, base loss: 21658.77
[INFO 2017-06-28 18:19:40,915 main.py:51] epoch 837, training loss: 13645.75, average training loss: 15206.76, base loss: 21656.28
[INFO 2017-06-28 18:19:42,228 main.py:51] epoch 838, training loss: 14937.04, average training loss: 15206.44, base loss: 21656.96
[INFO 2017-06-28 18:19:43,801 main.py:51] epoch 839, training loss: 13011.47, average training loss: 15203.82, base loss: 21655.82
[INFO 2017-06-28 18:19:45,173 main.py:51] epoch 840, training loss: 13794.50, average training loss: 15202.15, base loss: 21656.12
[INFO 2017-06-28 18:19:46,580 main.py:51] epoch 841, training loss: 14349.07, average training loss: 15201.13, base loss: 21655.88
[INFO 2017-06-28 18:19:48,001 main.py:51] epoch 842, training loss: 15312.83, average training loss: 15201.27, base loss: 21659.09
[INFO 2017-06-28 18:19:49,418 main.py:51] epoch 843, training loss: 13695.71, average training loss: 15199.48, base loss: 21658.57
[INFO 2017-06-28 18:19:50,725 main.py:51] epoch 844, training loss: 14387.36, average training loss: 15198.52, base loss: 21660.06
[INFO 2017-06-28 18:19:52,165 main.py:51] epoch 845, training loss: 14211.97, average training loss: 15197.36, base loss: 21661.31
[INFO 2017-06-28 18:19:53,455 main.py:51] epoch 846, training loss: 13503.53, average training loss: 15195.36, base loss: 21660.76
[INFO 2017-06-28 18:19:54,813 main.py:51] epoch 847, training loss: 13258.86, average training loss: 15193.07, base loss: 21660.49
[INFO 2017-06-28 18:19:56,239 main.py:51] epoch 848, training loss: 12681.46, average training loss: 15190.11, base loss: 21659.71
[INFO 2017-06-28 18:19:57,639 main.py:51] epoch 849, training loss: 11636.98, average training loss: 15185.93, base loss: 21657.96
[INFO 2017-06-28 18:19:59,154 main.py:51] epoch 850, training loss: 13644.61, average training loss: 15184.12, base loss: 21659.38
[INFO 2017-06-28 18:20:00,559 main.py:51] epoch 851, training loss: 13936.82, average training loss: 15182.66, base loss: 21659.59
[INFO 2017-06-28 18:20:01,864 main.py:51] epoch 852, training loss: 12621.59, average training loss: 15179.66, base loss: 21656.09
[INFO 2017-06-28 18:20:03,266 main.py:51] epoch 853, training loss: 13145.63, average training loss: 15177.27, base loss: 21654.82
[INFO 2017-06-28 18:20:04,928 main.py:51] epoch 854, training loss: 13914.57, average training loss: 15175.80, base loss: 21654.91
[INFO 2017-06-28 18:20:06,295 main.py:51] epoch 855, training loss: 12836.66, average training loss: 15173.06, base loss: 21653.57
[INFO 2017-06-28 18:20:07,713 main.py:51] epoch 856, training loss: 14935.27, average training loss: 15172.79, base loss: 21655.57
[INFO 2017-06-28 18:20:09,188 main.py:51] epoch 857, training loss: 13275.47, average training loss: 15170.58, base loss: 21653.98
[INFO 2017-06-28 18:20:10,537 main.py:51] epoch 858, training loss: 13719.60, average training loss: 15168.89, base loss: 21654.06
[INFO 2017-06-28 18:20:11,949 main.py:51] epoch 859, training loss: 12945.92, average training loss: 15166.30, base loss: 21652.32
[INFO 2017-06-28 18:20:13,353 main.py:51] epoch 860, training loss: 14677.07, average training loss: 15165.73, base loss: 21652.63
[INFO 2017-06-28 18:20:14,758 main.py:51] epoch 861, training loss: 14191.48, average training loss: 15164.60, base loss: 21654.13
[INFO 2017-06-28 18:20:16,294 main.py:51] epoch 862, training loss: 12161.65, average training loss: 15161.12, base loss: 21650.79
[INFO 2017-06-28 18:20:17,702 main.py:51] epoch 863, training loss: 14202.22, average training loss: 15160.01, base loss: 21650.57
[INFO 2017-06-28 18:20:19,248 main.py:51] epoch 864, training loss: 14307.21, average training loss: 15159.03, base loss: 21649.13
[INFO 2017-06-28 18:20:20,567 main.py:51] epoch 865, training loss: 13339.40, average training loss: 15156.93, base loss: 21648.71
[INFO 2017-06-28 18:20:21,874 main.py:51] epoch 866, training loss: 13139.99, average training loss: 15154.60, base loss: 21647.09
[INFO 2017-06-28 18:20:23,290 main.py:51] epoch 867, training loss: 13245.71, average training loss: 15152.40, base loss: 21646.66
[INFO 2017-06-28 18:20:24,580 main.py:51] epoch 868, training loss: 15775.46, average training loss: 15153.12, base loss: 21649.89
[INFO 2017-06-28 18:20:25,843 main.py:51] epoch 869, training loss: 12671.14, average training loss: 15150.27, base loss: 21648.94
[INFO 2017-06-28 18:20:27,149 main.py:51] epoch 870, training loss: 13621.45, average training loss: 15148.51, base loss: 21649.95
[INFO 2017-06-28 18:20:28,547 main.py:51] epoch 871, training loss: 15591.21, average training loss: 15149.02, base loss: 21651.47
[INFO 2017-06-28 18:20:29,870 main.py:51] epoch 872, training loss: 14844.05, average training loss: 15148.67, base loss: 21650.16
[INFO 2017-06-28 18:20:31,258 main.py:51] epoch 873, training loss: 14089.02, average training loss: 15147.46, base loss: 21651.15
[INFO 2017-06-28 18:20:32,531 main.py:51] epoch 874, training loss: 14690.61, average training loss: 15146.93, base loss: 21651.30
[INFO 2017-06-28 18:20:33,783 main.py:51] epoch 875, training loss: 14479.63, average training loss: 15146.17, base loss: 21651.31
[INFO 2017-06-28 18:20:35,126 main.py:51] epoch 876, training loss: 13401.68, average training loss: 15144.18, base loss: 21651.72
[INFO 2017-06-28 18:20:36,391 main.py:51] epoch 877, training loss: 12863.69, average training loss: 15141.59, base loss: 21650.86
[INFO 2017-06-28 18:20:37,774 main.py:51] epoch 878, training loss: 14528.53, average training loss: 15140.89, base loss: 21652.29
[INFO 2017-06-28 18:20:39,203 main.py:51] epoch 879, training loss: 15476.36, average training loss: 15141.27, base loss: 21654.96
[INFO 2017-06-28 18:20:40,601 main.py:51] epoch 880, training loss: 16534.31, average training loss: 15142.85, base loss: 21657.80
[INFO 2017-06-28 18:20:41,889 main.py:51] epoch 881, training loss: 15266.58, average training loss: 15142.99, base loss: 21659.57
[INFO 2017-06-28 18:20:43,284 main.py:51] epoch 882, training loss: 14323.26, average training loss: 15142.06, base loss: 21658.13
[INFO 2017-06-28 18:20:44,469 main.py:51] epoch 883, training loss: 12994.28, average training loss: 15139.63, base loss: 21655.39
[INFO 2017-06-28 18:20:45,784 main.py:51] epoch 884, training loss: 15304.50, average training loss: 15139.82, base loss: 21657.04
[INFO 2017-06-28 18:20:47,200 main.py:51] epoch 885, training loss: 13252.34, average training loss: 15137.69, base loss: 21656.57
[INFO 2017-06-28 18:20:48,535 main.py:51] epoch 886, training loss: 12683.65, average training loss: 15134.92, base loss: 21655.12
[INFO 2017-06-28 18:20:49,961 main.py:51] epoch 887, training loss: 15717.28, average training loss: 15135.58, base loss: 21656.40
[INFO 2017-06-28 18:20:51,510 main.py:51] epoch 888, training loss: 13025.38, average training loss: 15133.20, base loss: 21654.82
[INFO 2017-06-28 18:20:52,849 main.py:51] epoch 889, training loss: 13448.57, average training loss: 15131.31, base loss: 21652.16
[INFO 2017-06-28 18:20:54,260 main.py:51] epoch 890, training loss: 14501.00, average training loss: 15130.60, base loss: 21652.58
[INFO 2017-06-28 18:20:55,532 main.py:51] epoch 891, training loss: 14109.83, average training loss: 15129.46, base loss: 21650.92
[INFO 2017-06-28 18:20:56,889 main.py:51] epoch 892, training loss: 12916.92, average training loss: 15126.98, base loss: 21650.23
[INFO 2017-06-28 18:20:58,249 main.py:51] epoch 893, training loss: 14507.06, average training loss: 15126.29, base loss: 21649.79
[INFO 2017-06-28 18:20:59,506 main.py:51] epoch 894, training loss: 13328.46, average training loss: 15124.28, base loss: 21648.50
[INFO 2017-06-28 18:21:00,830 main.py:51] epoch 895, training loss: 12984.23, average training loss: 15121.89, base loss: 21647.31
[INFO 2017-06-28 18:21:02,266 main.py:51] epoch 896, training loss: 14912.18, average training loss: 15121.66, base loss: 21649.00
[INFO 2017-06-28 18:21:03,617 main.py:51] epoch 897, training loss: 13723.29, average training loss: 15120.10, base loss: 21650.17
[INFO 2017-06-28 18:21:04,997 main.py:51] epoch 898, training loss: 12538.89, average training loss: 15117.23, base loss: 21649.56
[INFO 2017-06-28 18:21:06,372 main.py:51] epoch 899, training loss: 13882.50, average training loss: 15115.86, base loss: 21650.96
[INFO 2017-06-28 18:21:06,373 main.py:53] epoch 899, testing
[INFO 2017-06-28 18:21:12,583 main.py:105] average testing loss: 15695.67, base loss: 23202.80
[INFO 2017-06-28 18:21:12,583 main.py:106] improve_loss: 7507.13, improve_percent: 0.32
[INFO 2017-06-28 18:21:12,584 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:21:12,597 main.py:76] current best improved percent: 0.32
[INFO 2017-06-28 18:21:14,111 main.py:51] epoch 900, training loss: 14688.49, average training loss: 15115.38, base loss: 21651.13
[INFO 2017-06-28 18:21:15,427 main.py:51] epoch 901, training loss: 15384.83, average training loss: 15115.68, base loss: 21651.86
[INFO 2017-06-28 18:21:16,716 main.py:51] epoch 902, training loss: 13513.54, average training loss: 15113.91, base loss: 21651.01
[INFO 2017-06-28 18:21:18,151 main.py:51] epoch 903, training loss: 14542.47, average training loss: 15113.28, base loss: 21652.87
[INFO 2017-06-28 18:21:19,475 main.py:51] epoch 904, training loss: 13682.22, average training loss: 15111.69, base loss: 21653.42
[INFO 2017-06-28 18:21:20,906 main.py:51] epoch 905, training loss: 13845.49, average training loss: 15110.30, base loss: 21652.96
[INFO 2017-06-28 18:21:22,284 main.py:51] epoch 906, training loss: 13506.85, average training loss: 15108.53, base loss: 21653.82
[INFO 2017-06-28 18:21:23,630 main.py:51] epoch 907, training loss: 13380.57, average training loss: 15106.63, base loss: 21653.51
[INFO 2017-06-28 18:21:25,090 main.py:51] epoch 908, training loss: 12170.66, average training loss: 15103.40, base loss: 21650.12
[INFO 2017-06-28 18:21:26,511 main.py:51] epoch 909, training loss: 13400.31, average training loss: 15101.52, base loss: 21648.63
[INFO 2017-06-28 18:21:27,921 main.py:51] epoch 910, training loss: 12090.30, average training loss: 15098.22, base loss: 21646.17
[INFO 2017-06-28 18:21:29,393 main.py:51] epoch 911, training loss: 13539.58, average training loss: 15096.51, base loss: 21647.14
[INFO 2017-06-28 18:21:30,699 main.py:51] epoch 912, training loss: 15334.08, average training loss: 15096.77, base loss: 21649.71
[INFO 2017-06-28 18:21:32,044 main.py:51] epoch 913, training loss: 15085.15, average training loss: 15096.76, base loss: 21652.43
[INFO 2017-06-28 18:21:33,367 main.py:51] epoch 914, training loss: 14569.90, average training loss: 15096.18, base loss: 21652.78
[INFO 2017-06-28 18:21:34,691 main.py:51] epoch 915, training loss: 14610.21, average training loss: 15095.65, base loss: 21654.60
[INFO 2017-06-28 18:21:36,033 main.py:51] epoch 916, training loss: 14072.29, average training loss: 15094.54, base loss: 21656.15
[INFO 2017-06-28 18:21:37,387 main.py:51] epoch 917, training loss: 13209.81, average training loss: 15092.48, base loss: 21655.45
[INFO 2017-06-28 18:21:38,708 main.py:51] epoch 918, training loss: 14841.81, average training loss: 15092.21, base loss: 21656.11
[INFO 2017-06-28 18:21:40,062 main.py:51] epoch 919, training loss: 12614.95, average training loss: 15089.52, base loss: 21653.62
[INFO 2017-06-28 18:21:41,297 main.py:51] epoch 920, training loss: 13748.06, average training loss: 15088.06, base loss: 21652.01
[INFO 2017-06-28 18:21:42,721 main.py:51] epoch 921, training loss: 12886.43, average training loss: 15085.67, base loss: 21650.59
[INFO 2017-06-28 18:21:44,092 main.py:51] epoch 922, training loss: 14313.79, average training loss: 15084.84, base loss: 21653.31
[INFO 2017-06-28 18:21:45,449 main.py:51] epoch 923, training loss: 13973.04, average training loss: 15083.63, base loss: 21652.45
[INFO 2017-06-28 18:21:46,707 main.py:51] epoch 924, training loss: 14019.84, average training loss: 15082.48, base loss: 21653.74
[INFO 2017-06-28 18:21:48,090 main.py:51] epoch 925, training loss: 12355.69, average training loss: 15079.54, base loss: 21651.17
[INFO 2017-06-28 18:21:49,423 main.py:51] epoch 926, training loss: 12435.18, average training loss: 15076.69, base loss: 21648.06
[INFO 2017-06-28 18:21:50,770 main.py:51] epoch 927, training loss: 16249.31, average training loss: 15077.95, base loss: 21651.65
[INFO 2017-06-28 18:21:52,030 main.py:51] epoch 928, training loss: 14593.35, average training loss: 15077.43, base loss: 21652.30
[INFO 2017-06-28 18:21:53,334 main.py:51] epoch 929, training loss: 13440.81, average training loss: 15075.67, base loss: 21651.92
[INFO 2017-06-28 18:21:54,659 main.py:51] epoch 930, training loss: 14557.64, average training loss: 15075.11, base loss: 21652.94
[INFO 2017-06-28 18:21:56,034 main.py:51] epoch 931, training loss: 13275.53, average training loss: 15073.18, base loss: 21651.88
[INFO 2017-06-28 18:21:57,246 main.py:51] epoch 932, training loss: 14878.07, average training loss: 15072.97, base loss: 21652.12
[INFO 2017-06-28 18:21:58,556 main.py:51] epoch 933, training loss: 13790.62, average training loss: 15071.60, base loss: 21651.71
[INFO 2017-06-28 18:21:59,969 main.py:51] epoch 934, training loss: 13524.38, average training loss: 15069.94, base loss: 21654.11
[INFO 2017-06-28 18:22:01,257 main.py:51] epoch 935, training loss: 14320.11, average training loss: 15069.14, base loss: 21653.91
[INFO 2017-06-28 18:22:02,577 main.py:51] epoch 936, training loss: 12221.98, average training loss: 15066.10, base loss: 21651.93
[INFO 2017-06-28 18:22:03,947 main.py:51] epoch 937, training loss: 13255.88, average training loss: 15064.17, base loss: 21650.34
[INFO 2017-06-28 18:22:05,235 main.py:51] epoch 938, training loss: 14150.43, average training loss: 15063.20, base loss: 21649.96
[INFO 2017-06-28 18:22:06,620 main.py:51] epoch 939, training loss: 14516.76, average training loss: 15062.62, base loss: 21651.04
[INFO 2017-06-28 18:22:08,052 main.py:51] epoch 940, training loss: 13293.55, average training loss: 15060.74, base loss: 21651.21
[INFO 2017-06-28 18:22:09,566 main.py:51] epoch 941, training loss: 14204.19, average training loss: 15059.83, base loss: 21652.63
[INFO 2017-06-28 18:22:10,915 main.py:51] epoch 942, training loss: 14374.45, average training loss: 15059.10, base loss: 21653.45
[INFO 2017-06-28 18:22:12,399 main.py:51] epoch 943, training loss: 13694.00, average training loss: 15057.66, base loss: 21651.84
[INFO 2017-06-28 18:22:13,811 main.py:51] epoch 944, training loss: 13183.96, average training loss: 15055.67, base loss: 21651.83
[INFO 2017-06-28 18:22:15,146 main.py:51] epoch 945, training loss: 12930.71, average training loss: 15053.43, base loss: 21652.61
[INFO 2017-06-28 18:22:16,584 main.py:51] epoch 946, training loss: 14402.37, average training loss: 15052.74, base loss: 21653.35
[INFO 2017-06-28 18:22:17,901 main.py:51] epoch 947, training loss: 13622.54, average training loss: 15051.23, base loss: 21653.59
[INFO 2017-06-28 18:22:19,207 main.py:51] epoch 948, training loss: 14795.02, average training loss: 15050.96, base loss: 21657.27
[INFO 2017-06-28 18:22:20,480 main.py:51] epoch 949, training loss: 13387.81, average training loss: 15049.21, base loss: 21657.57
[INFO 2017-06-28 18:22:21,760 main.py:51] epoch 950, training loss: 13285.83, average training loss: 15047.36, base loss: 21658.10
[INFO 2017-06-28 18:22:23,118 main.py:51] epoch 951, training loss: 14690.05, average training loss: 15046.98, base loss: 21660.50
[INFO 2017-06-28 18:22:24,520 main.py:51] epoch 952, training loss: 13948.77, average training loss: 15045.83, base loss: 21660.87
[INFO 2017-06-28 18:22:25,868 main.py:51] epoch 953, training loss: 12759.41, average training loss: 15043.43, base loss: 21660.38
[INFO 2017-06-28 18:22:27,149 main.py:51] epoch 954, training loss: 12921.92, average training loss: 15041.21, base loss: 21659.35
[INFO 2017-06-28 18:22:28,434 main.py:51] epoch 955, training loss: 12445.26, average training loss: 15038.50, base loss: 21656.97
[INFO 2017-06-28 18:22:29,684 main.py:51] epoch 956, training loss: 13604.99, average training loss: 15037.00, base loss: 21657.26
[INFO 2017-06-28 18:22:31,130 main.py:51] epoch 957, training loss: 13270.20, average training loss: 15035.15, base loss: 21655.46
[INFO 2017-06-28 18:22:32,540 main.py:51] epoch 958, training loss: 13025.67, average training loss: 15033.06, base loss: 21654.47
[INFO 2017-06-28 18:22:33,854 main.py:51] epoch 959, training loss: 14965.82, average training loss: 15032.99, base loss: 21656.62
[INFO 2017-06-28 18:22:35,146 main.py:51] epoch 960, training loss: 13340.41, average training loss: 15031.23, base loss: 21655.56
[INFO 2017-06-28 18:22:36,581 main.py:51] epoch 961, training loss: 15005.29, average training loss: 15031.20, base loss: 21657.59
[INFO 2017-06-28 18:22:37,999 main.py:51] epoch 962, training loss: 12881.78, average training loss: 15028.97, base loss: 21655.54
[INFO 2017-06-28 18:22:39,243 main.py:51] epoch 963, training loss: 14205.84, average training loss: 15028.11, base loss: 21656.45
[INFO 2017-06-28 18:22:40,585 main.py:51] epoch 964, training loss: 13610.48, average training loss: 15026.65, base loss: 21655.39
[INFO 2017-06-28 18:22:42,031 main.py:51] epoch 965, training loss: 13159.67, average training loss: 15024.71, base loss: 21654.60
[INFO 2017-06-28 18:22:43,425 main.py:51] epoch 966, training loss: 13092.74, average training loss: 15022.71, base loss: 21655.68
[INFO 2017-06-28 18:22:44,665 main.py:51] epoch 967, training loss: 14071.97, average training loss: 15021.73, base loss: 21656.97
[INFO 2017-06-28 18:22:46,027 main.py:51] epoch 968, training loss: 12284.67, average training loss: 15018.91, base loss: 21654.86
[INFO 2017-06-28 18:22:47,345 main.py:51] epoch 969, training loss: 12879.53, average training loss: 15016.70, base loss: 21652.93
[INFO 2017-06-28 18:22:48,687 main.py:51] epoch 970, training loss: 13889.85, average training loss: 15015.54, base loss: 21652.50
[INFO 2017-06-28 18:22:49,997 main.py:51] epoch 971, training loss: 14016.79, average training loss: 15014.51, base loss: 21652.75
[INFO 2017-06-28 18:22:51,327 main.py:51] epoch 972, training loss: 15900.35, average training loss: 15015.42, base loss: 21655.71
[INFO 2017-06-28 18:22:52,640 main.py:51] epoch 973, training loss: 13839.57, average training loss: 15014.22, base loss: 21655.81
[INFO 2017-06-28 18:22:53,993 main.py:51] epoch 974, training loss: 13537.83, average training loss: 15012.70, base loss: 21656.01
[INFO 2017-06-28 18:22:55,261 main.py:51] epoch 975, training loss: 14542.07, average training loss: 15012.22, base loss: 21656.49
[INFO 2017-06-28 18:22:56,623 main.py:51] epoch 976, training loss: 12900.63, average training loss: 15010.06, base loss: 21655.15
[INFO 2017-06-28 18:22:58,005 main.py:51] epoch 977, training loss: 13276.38, average training loss: 15008.29, base loss: 21655.15
[INFO 2017-06-28 18:22:59,322 main.py:51] epoch 978, training loss: 15411.53, average training loss: 15008.70, base loss: 21657.55
[INFO 2017-06-28 18:23:00,790 main.py:51] epoch 979, training loss: 12764.21, average training loss: 15006.41, base loss: 21657.93
[INFO 2017-06-28 18:23:02,010 main.py:51] epoch 980, training loss: 12195.63, average training loss: 15003.54, base loss: 21656.91
[INFO 2017-06-28 18:23:03,350 main.py:51] epoch 981, training loss: 14148.90, average training loss: 15002.67, base loss: 21657.46
[INFO 2017-06-28 18:23:04,741 main.py:51] epoch 982, training loss: 14670.78, average training loss: 15002.34, base loss: 21657.94
[INFO 2017-06-28 18:23:06,032 main.py:51] epoch 983, training loss: 14277.56, average training loss: 15001.60, base loss: 21657.71
[INFO 2017-06-28 18:23:07,418 main.py:51] epoch 984, training loss: 14734.10, average training loss: 15001.33, base loss: 21659.58
[INFO 2017-06-28 18:23:08,780 main.py:51] epoch 985, training loss: 14252.81, average training loss: 15000.57, base loss: 21659.15
[INFO 2017-06-28 18:23:10,091 main.py:51] epoch 986, training loss: 14020.70, average training loss: 14999.58, base loss: 21657.21
[INFO 2017-06-28 18:23:11,481 main.py:51] epoch 987, training loss: 13324.85, average training loss: 14997.88, base loss: 21657.85
[INFO 2017-06-28 18:23:12,750 main.py:51] epoch 988, training loss: 12954.58, average training loss: 14995.81, base loss: 21656.74
[INFO 2017-06-28 18:23:14,049 main.py:51] epoch 989, training loss: 14315.62, average training loss: 14995.13, base loss: 21656.76
[INFO 2017-06-28 18:23:15,544 main.py:51] epoch 990, training loss: 12822.90, average training loss: 14992.94, base loss: 21654.94
[INFO 2017-06-28 18:23:16,797 main.py:51] epoch 991, training loss: 14388.67, average training loss: 14992.33, base loss: 21654.67
[INFO 2017-06-28 18:23:18,056 main.py:51] epoch 992, training loss: 14411.72, average training loss: 14991.74, base loss: 21657.47
[INFO 2017-06-28 18:23:19,364 main.py:51] epoch 993, training loss: 12803.80, average training loss: 14989.54, base loss: 21655.33
[INFO 2017-06-28 18:23:20,705 main.py:51] epoch 994, training loss: 15050.87, average training loss: 14989.60, base loss: 21657.98
[INFO 2017-06-28 18:23:22,080 main.py:51] epoch 995, training loss: 13828.76, average training loss: 14988.44, base loss: 21658.38
[INFO 2017-06-28 18:23:23,443 main.py:51] epoch 996, training loss: 13809.98, average training loss: 14987.25, base loss: 21657.47
[INFO 2017-06-28 18:23:24,825 main.py:51] epoch 997, training loss: 13882.14, average training loss: 14986.15, base loss: 21655.87
[INFO 2017-06-28 18:23:26,099 main.py:51] epoch 998, training loss: 13870.40, average training loss: 14985.03, base loss: 21656.41
[INFO 2017-06-28 18:23:27,402 main.py:51] epoch 999, training loss: 15320.73, average training loss: 14985.37, base loss: 21658.70
[INFO 2017-06-28 18:23:27,402 main.py:53] epoch 999, testing
[INFO 2017-06-28 18:23:33,369 main.py:105] average testing loss: 14908.82, base loss: 22358.27
[INFO 2017-06-28 18:23:33,369 main.py:106] improve_loss: 7449.45, improve_percent: 0.33
[INFO 2017-06-28 18:23:33,370 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:23:33,383 main.py:76] current best improved percent: 0.33
[INFO 2017-06-28 18:23:34,668 main.py:51] epoch 1000, training loss: 13239.71, average training loss: 14956.65, base loss: 21657.60
[INFO 2017-06-28 18:23:35,863 main.py:51] epoch 1001, training loss: 12412.78, average training loss: 14932.24, base loss: 21656.41
[INFO 2017-06-28 18:23:37,359 main.py:51] epoch 1002, training loss: 14339.64, average training loss: 14911.94, base loss: 21657.56
[INFO 2017-06-28 18:23:38,609 main.py:51] epoch 1003, training loss: 16221.05, average training loss: 14896.78, base loss: 21658.82
[INFO 2017-06-28 18:23:39,912 main.py:51] epoch 1004, training loss: 14440.74, average training loss: 14881.87, base loss: 21660.86
[INFO 2017-06-28 18:23:41,196 main.py:51] epoch 1005, training loss: 14061.98, average training loss: 14868.89, base loss: 21663.35
[INFO 2017-06-28 18:23:42,586 main.py:51] epoch 1006, training loss: 14422.70, average training loss: 14857.46, base loss: 21667.66
[INFO 2017-06-28 18:23:43,817 main.py:51] epoch 1007, training loss: 13147.54, average training loss: 14846.47, base loss: 21667.20
[INFO 2017-06-28 18:23:45,228 main.py:51] epoch 1008, training loss: 12503.54, average training loss: 14834.62, base loss: 21663.16
[INFO 2017-06-28 18:23:46,495 main.py:51] epoch 1009, training loss: 16036.93, average training loss: 14826.55, base loss: 21664.39
[INFO 2017-06-28 18:23:47,929 main.py:51] epoch 1010, training loss: 14652.37, average training loss: 14818.43, base loss: 21667.04
[INFO 2017-06-28 18:23:49,253 main.py:51] epoch 1011, training loss: 14570.18, average training loss: 14811.17, base loss: 21670.00
[INFO 2017-06-28 18:23:50,710 main.py:51] epoch 1012, training loss: 15085.63, average training loss: 14803.71, base loss: 21669.42
[INFO 2017-06-28 18:23:51,972 main.py:51] epoch 1013, training loss: 14631.44, average training loss: 14797.39, base loss: 21670.65
[INFO 2017-06-28 18:23:53,288 main.py:51] epoch 1014, training loss: 13410.77, average training loss: 14788.54, base loss: 21668.75
[INFO 2017-06-28 18:23:54,627 main.py:51] epoch 1015, training loss: 13937.29, average training loss: 14781.53, base loss: 21668.95
[INFO 2017-06-28 18:23:55,908 main.py:51] epoch 1016, training loss: 12984.15, average training loss: 14774.07, base loss: 21670.41
[INFO 2017-06-28 18:23:57,189 main.py:51] epoch 1017, training loss: 14136.24, average training loss: 14768.81, base loss: 21672.44
[INFO 2017-06-28 18:23:58,600 main.py:51] epoch 1018, training loss: 12822.96, average training loss: 14761.65, base loss: 21673.09
[INFO 2017-06-28 18:23:59,845 main.py:51] epoch 1019, training loss: 13241.22, average training loss: 14755.25, base loss: 21674.68
[INFO 2017-06-28 18:24:01,187 main.py:51] epoch 1020, training loss: 12098.66, average training loss: 14747.10, base loss: 21673.28
[INFO 2017-06-28 18:24:02,584 main.py:51] epoch 1021, training loss: 12927.46, average training loss: 14740.41, base loss: 21673.38
[INFO 2017-06-28 18:24:04,017 main.py:51] epoch 1022, training loss: 13855.65, average training loss: 14732.09, base loss: 21672.36
[INFO 2017-06-28 18:24:05,288 main.py:51] epoch 1023, training loss: 13128.28, average training loss: 14725.45, base loss: 21670.05
[INFO 2017-06-28 18:24:06,737 main.py:51] epoch 1024, training loss: 14082.82, average training loss: 14720.14, base loss: 21670.74
[INFO 2017-06-28 18:24:08,091 main.py:51] epoch 1025, training loss: 14254.05, average training loss: 14715.33, base loss: 21673.33
[INFO 2017-06-28 18:24:09,276 main.py:51] epoch 1026, training loss: 11749.37, average training loss: 14707.75, base loss: 21670.90
[INFO 2017-06-28 18:24:10,512 main.py:51] epoch 1027, training loss: 16234.26, average training loss: 14703.78, base loss: 21673.14
[INFO 2017-06-28 18:24:11,816 main.py:51] epoch 1028, training loss: 15878.78, average training loss: 14699.23, base loss: 21676.93
[INFO 2017-06-28 18:24:13,169 main.py:51] epoch 1029, training loss: 12187.19, average training loss: 14693.21, base loss: 21677.24
[INFO 2017-06-28 18:24:14,496 main.py:51] epoch 1030, training loss: 13577.34, average training loss: 14687.94, base loss: 21677.76
[INFO 2017-06-28 18:24:15,770 main.py:51] epoch 1031, training loss: 14649.44, average training loss: 14682.13, base loss: 21677.49
[INFO 2017-06-28 18:24:17,126 main.py:51] epoch 1032, training loss: 14138.34, average training loss: 14678.25, base loss: 21680.67
[INFO 2017-06-28 18:24:18,454 main.py:51] epoch 1033, training loss: 12433.67, average training loss: 14671.31, base loss: 21679.30
[INFO 2017-06-28 18:24:19,752 main.py:51] epoch 1034, training loss: 13391.80, average training loss: 14666.90, base loss: 21680.77
[INFO 2017-06-28 18:24:21,013 main.py:51] epoch 1035, training loss: 12984.93, average training loss: 14659.92, base loss: 21677.60
[INFO 2017-06-28 18:24:22,377 main.py:51] epoch 1036, training loss: 14423.86, average training loss: 14655.81, base loss: 21678.19
[INFO 2017-06-28 18:24:23,756 main.py:51] epoch 1037, training loss: 12908.96, average training loss: 14647.66, base loss: 21673.66
[INFO 2017-06-28 18:24:25,035 main.py:51] epoch 1038, training loss: 16315.69, average training loss: 14645.98, base loss: 21679.22
[INFO 2017-06-28 18:24:26,295 main.py:51] epoch 1039, training loss: 15887.25, average training loss: 14642.71, base loss: 21681.28
[INFO 2017-06-28 18:24:27,653 main.py:51] epoch 1040, training loss: 15051.39, average training loss: 14638.72, base loss: 21680.36
[INFO 2017-06-28 18:24:28,919 main.py:51] epoch 1041, training loss: 13225.81, average training loss: 14630.66, base loss: 21677.83
[INFO 2017-06-28 18:24:30,298 main.py:51] epoch 1042, training loss: 14654.74, average training loss: 14624.81, base loss: 21677.89
[INFO 2017-06-28 18:24:31,626 main.py:51] epoch 1043, training loss: 12732.76, average training loss: 14618.30, base loss: 21675.20
[INFO 2017-06-28 18:24:32,969 main.py:51] epoch 1044, training loss: 14491.51, average training loss: 14613.23, base loss: 21673.82
[INFO 2017-06-28 18:24:34,223 main.py:51] epoch 1045, training loss: 12722.01, average training loss: 14606.96, base loss: 21671.85
[INFO 2017-06-28 18:24:35,560 main.py:51] epoch 1046, training loss: 14788.92, average training loss: 14605.34, base loss: 21676.96
[INFO 2017-06-28 18:24:36,896 main.py:51] epoch 1047, training loss: 12966.79, average training loss: 14601.58, base loss: 21677.66
[INFO 2017-06-28 18:24:38,271 main.py:51] epoch 1048, training loss: 12715.00, average training loss: 14595.86, base loss: 21674.55
[INFO 2017-06-28 18:24:39,526 main.py:51] epoch 1049, training loss: 13775.59, average training loss: 14592.54, base loss: 21674.86
[INFO 2017-06-28 18:24:40,785 main.py:51] epoch 1050, training loss: 12679.61, average training loss: 14584.69, base loss: 21671.62
[INFO 2017-06-28 18:24:42,070 main.py:51] epoch 1051, training loss: 13199.86, average training loss: 14581.08, base loss: 21670.82
[INFO 2017-06-28 18:24:43,312 main.py:51] epoch 1052, training loss: 14291.33, average training loss: 14578.00, base loss: 21673.24
[INFO 2017-06-28 18:24:44,698 main.py:51] epoch 1053, training loss: 12987.33, average training loss: 14573.35, base loss: 21671.39
[INFO 2017-06-28 18:24:45,971 main.py:51] epoch 1054, training loss: 13110.81, average training loss: 14566.09, base loss: 21666.63
[INFO 2017-06-28 18:24:47,425 main.py:51] epoch 1055, training loss: 13361.34, average training loss: 14562.99, base loss: 21667.93
[INFO 2017-06-28 18:24:48,709 main.py:51] epoch 1056, training loss: 13961.06, average training loss: 14559.08, base loss: 21668.01
[INFO 2017-06-28 18:24:49,930 main.py:51] epoch 1057, training loss: 12129.99, average training loss: 14550.86, base loss: 21663.01
[INFO 2017-06-28 18:24:51,222 main.py:51] epoch 1058, training loss: 15643.86, average training loss: 14548.38, base loss: 21665.30
[INFO 2017-06-28 18:24:52,510 main.py:51] epoch 1059, training loss: 13165.63, average training loss: 14544.22, base loss: 21664.91
[INFO 2017-06-28 18:24:53,774 main.py:51] epoch 1060, training loss: 14935.90, average training loss: 14542.08, base loss: 21667.35
[INFO 2017-06-28 18:24:55,027 main.py:51] epoch 1061, training loss: 15104.85, average training loss: 14541.69, base loss: 21670.89
[INFO 2017-06-28 18:24:56,332 main.py:51] epoch 1062, training loss: 13201.21, average training loss: 14539.06, base loss: 21672.72
[INFO 2017-06-28 18:24:57,647 main.py:51] epoch 1063, training loss: 12715.53, average training loss: 14535.42, base loss: 21671.95
[INFO 2017-06-28 18:24:58,922 main.py:51] epoch 1064, training loss: 14558.28, average training loss: 14530.50, base loss: 21671.80
[INFO 2017-06-28 18:25:00,221 main.py:51] epoch 1065, training loss: 15211.71, average training loss: 14529.22, base loss: 21672.15
[INFO 2017-06-28 18:25:01,632 main.py:51] epoch 1066, training loss: 13329.99, average training loss: 14526.59, base loss: 21674.13
[INFO 2017-06-28 18:25:02,904 main.py:51] epoch 1067, training loss: 13236.52, average training loss: 14522.02, base loss: 21671.54
[INFO 2017-06-28 18:25:04,177 main.py:51] epoch 1068, training loss: 13454.31, average training loss: 14518.81, base loss: 21672.97
[INFO 2017-06-28 18:25:05,501 main.py:51] epoch 1069, training loss: 14303.98, average training loss: 14515.46, base loss: 21672.60
[INFO 2017-06-28 18:25:06,777 main.py:51] epoch 1070, training loss: 14302.50, average training loss: 14511.31, base loss: 21672.51
[INFO 2017-06-28 18:25:08,119 main.py:51] epoch 1071, training loss: 13130.41, average training loss: 14506.97, base loss: 21671.59
[INFO 2017-06-28 18:25:09,501 main.py:51] epoch 1072, training loss: 13286.15, average training loss: 14504.22, base loss: 21673.61
[INFO 2017-06-28 18:25:10,726 main.py:51] epoch 1073, training loss: 14177.25, average training loss: 14501.23, base loss: 21674.60
[INFO 2017-06-28 18:25:11,943 main.py:51] epoch 1074, training loss: 14516.67, average training loss: 14498.57, base loss: 21677.00
[INFO 2017-06-28 18:25:13,245 main.py:51] epoch 1075, training loss: 14815.17, average training loss: 14497.17, base loss: 21678.45
[INFO 2017-06-28 18:25:14,488 main.py:51] epoch 1076, training loss: 13791.65, average training loss: 14495.68, base loss: 21678.19
[INFO 2017-06-28 18:25:15,739 main.py:51] epoch 1077, training loss: 12904.99, average training loss: 14492.69, base loss: 21678.89
[INFO 2017-06-28 18:25:17,055 main.py:51] epoch 1078, training loss: 14345.89, average training loss: 14489.22, base loss: 21680.51
[INFO 2017-06-28 18:25:18,275 main.py:51] epoch 1079, training loss: 13938.24, average training loss: 14485.52, base loss: 21681.30
[INFO 2017-06-28 18:25:19,614 main.py:51] epoch 1080, training loss: 14669.71, average training loss: 14480.99, base loss: 21678.57
[INFO 2017-06-28 18:25:20,812 main.py:51] epoch 1081, training loss: 16122.72, average training loss: 14481.34, base loss: 21683.25
[INFO 2017-06-28 18:25:22,041 main.py:51] epoch 1082, training loss: 13115.86, average training loss: 14478.50, base loss: 21683.45
[INFO 2017-06-28 18:25:23,304 main.py:51] epoch 1083, training loss: 12261.56, average training loss: 14473.78, base loss: 21681.11
[INFO 2017-06-28 18:25:24,552 main.py:51] epoch 1084, training loss: 13336.10, average training loss: 14469.49, base loss: 21681.05
[INFO 2017-06-28 18:25:25,942 main.py:51] epoch 1085, training loss: 12980.03, average training loss: 14464.93, base loss: 21678.79
[INFO 2017-06-28 18:25:27,163 main.py:51] epoch 1086, training loss: 13384.50, average training loss: 14461.86, base loss: 21680.37
[INFO 2017-06-28 18:25:28,380 main.py:51] epoch 1087, training loss: 14183.86, average training loss: 14458.97, base loss: 21682.16
[INFO 2017-06-28 18:25:29,754 main.py:51] epoch 1088, training loss: 13898.26, average training loss: 14456.90, base loss: 21683.75
[INFO 2017-06-28 18:25:31,176 main.py:51] epoch 1089, training loss: 13834.00, average training loss: 14453.06, base loss: 21682.34
[INFO 2017-06-28 18:25:32,429 main.py:51] epoch 1090, training loss: 14051.41, average training loss: 14449.71, base loss: 21683.36
[INFO 2017-06-28 18:25:33,734 main.py:51] epoch 1091, training loss: 12520.81, average training loss: 14445.64, base loss: 21682.94
[INFO 2017-06-28 18:25:34,920 main.py:51] epoch 1092, training loss: 12490.76, average training loss: 14440.39, base loss: 21681.29
[INFO 2017-06-28 18:25:36,196 main.py:51] epoch 1093, training loss: 14300.16, average training loss: 14437.71, base loss: 21682.14
[INFO 2017-06-28 18:25:37,422 main.py:51] epoch 1094, training loss: 11877.98, average training loss: 14432.76, base loss: 21680.22
[INFO 2017-06-28 18:25:38,720 main.py:51] epoch 1095, training loss: 13761.31, average training loss: 14429.55, base loss: 21680.91
[INFO 2017-06-28 18:25:39,936 main.py:51] epoch 1096, training loss: 14430.87, average training loss: 14425.93, base loss: 21681.40
[INFO 2017-06-28 18:25:41,319 main.py:51] epoch 1097, training loss: 14646.23, average training loss: 14424.58, base loss: 21685.76
[INFO 2017-06-28 18:25:42,631 main.py:51] epoch 1098, training loss: 14531.37, average training loss: 14422.55, base loss: 21686.09
[INFO 2017-06-28 18:25:43,903 main.py:51] epoch 1099, training loss: 13633.52, average training loss: 14419.14, base loss: 21686.81
[INFO 2017-06-28 18:25:43,904 main.py:53] epoch 1099, testing
[INFO 2017-06-28 18:25:50,185 main.py:105] average testing loss: 15161.85, base loss: 22884.81
[INFO 2017-06-28 18:25:50,185 main.py:106] improve_loss: 7722.96, improve_percent: 0.34
[INFO 2017-06-28 18:25:50,186 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:25:50,199 main.py:76] current best improved percent: 0.34
[INFO 2017-06-28 18:25:51,476 main.py:51] epoch 1100, training loss: 14542.34, average training loss: 14417.38, base loss: 21688.14
[INFO 2017-06-28 18:25:52,831 main.py:51] epoch 1101, training loss: 13786.65, average training loss: 14414.82, base loss: 21687.88
[INFO 2017-06-28 18:25:54,044 main.py:51] epoch 1102, training loss: 12624.58, average training loss: 14410.60, base loss: 21688.23
[INFO 2017-06-28 18:25:55,423 main.py:51] epoch 1103, training loss: 14412.79, average training loss: 14408.99, base loss: 21689.27
[INFO 2017-06-28 18:25:56,751 main.py:51] epoch 1104, training loss: 14501.07, average training loss: 14406.63, base loss: 21690.34
[INFO 2017-06-28 18:25:57,975 main.py:51] epoch 1105, training loss: 13475.55, average training loss: 14402.22, base loss: 21689.15
[INFO 2017-06-28 18:25:59,310 main.py:51] epoch 1106, training loss: 13402.50, average training loss: 14400.11, base loss: 21689.87
[INFO 2017-06-28 18:26:00,675 main.py:51] epoch 1107, training loss: 13404.76, average training loss: 14398.02, base loss: 21692.25
[INFO 2017-06-28 18:26:01,983 main.py:51] epoch 1108, training loss: 16347.23, average training loss: 14397.00, base loss: 21694.45
[INFO 2017-06-28 18:26:03,293 main.py:51] epoch 1109, training loss: 12900.76, average training loss: 14393.18, base loss: 21694.70
[INFO 2017-06-28 18:26:04,580 main.py:51] epoch 1110, training loss: 12796.36, average training loss: 14389.44, base loss: 21693.20
[INFO 2017-06-28 18:26:05,888 main.py:51] epoch 1111, training loss: 13784.19, average training loss: 14385.03, base loss: 21691.58
[INFO 2017-06-28 18:26:07,239 main.py:51] epoch 1112, training loss: 13956.86, average training loss: 14382.63, base loss: 21692.93
[INFO 2017-06-28 18:26:08,494 main.py:51] epoch 1113, training loss: 12935.59, average training loss: 14378.77, base loss: 21692.43
[INFO 2017-06-28 18:26:10,072 main.py:51] epoch 1114, training loss: 13927.64, average training loss: 14376.11, base loss: 21693.58
[INFO 2017-06-28 18:26:11,431 main.py:51] epoch 1115, training loss: 14365.35, average training loss: 14375.61, base loss: 21697.58
[INFO 2017-06-28 18:26:12,786 main.py:51] epoch 1116, training loss: 12157.73, average training loss: 14370.19, base loss: 21694.88
[INFO 2017-06-28 18:26:14,048 main.py:51] epoch 1117, training loss: 11346.35, average training loss: 14366.23, base loss: 21693.84
[INFO 2017-06-28 18:26:15,310 main.py:51] epoch 1118, training loss: 13180.38, average training loss: 14362.23, base loss: 21691.64
[INFO 2017-06-28 18:26:16,650 main.py:51] epoch 1119, training loss: 12733.61, average training loss: 14357.16, base loss: 21689.15
[INFO 2017-06-28 18:26:17,950 main.py:51] epoch 1120, training loss: 17226.77, average training loss: 14356.64, base loss: 21691.22
[INFO 2017-06-28 18:26:19,186 main.py:51] epoch 1121, training loss: 13347.40, average training loss: 14352.32, base loss: 21689.03
[INFO 2017-06-28 18:26:20,467 main.py:51] epoch 1122, training loss: 13108.35, average training loss: 14348.23, base loss: 21688.64
[INFO 2017-06-28 18:26:21,799 main.py:51] epoch 1123, training loss: 12580.44, average training loss: 14345.01, base loss: 21685.60
[INFO 2017-06-28 18:26:23,017 main.py:51] epoch 1124, training loss: 12033.95, average training loss: 14341.32, base loss: 21684.35
[INFO 2017-06-28 18:26:24,434 main.py:51] epoch 1125, training loss: 14040.77, average training loss: 14338.26, base loss: 21683.28
[INFO 2017-06-28 18:26:25,640 main.py:51] epoch 1126, training loss: 13486.12, average training loss: 14335.61, base loss: 21684.22
[INFO 2017-06-28 18:26:26,946 main.py:51] epoch 1127, training loss: 14012.43, average training loss: 14333.13, base loss: 21683.45
[INFO 2017-06-28 18:26:28,267 main.py:51] epoch 1128, training loss: 13952.49, average training loss: 14332.25, base loss: 21686.42
[INFO 2017-06-28 18:26:29,746 main.py:51] epoch 1129, training loss: 13921.22, average training loss: 14331.19, base loss: 21687.60
[INFO 2017-06-28 18:26:30,981 main.py:51] epoch 1130, training loss: 16071.33, average training loss: 14330.83, base loss: 21690.01
[INFO 2017-06-28 18:26:32,342 main.py:51] epoch 1131, training loss: 14121.99, average training loss: 14328.46, base loss: 21691.41
[INFO 2017-06-28 18:26:33,634 main.py:51] epoch 1132, training loss: 12474.60, average training loss: 14323.84, base loss: 21686.05
[INFO 2017-06-28 18:26:34,987 main.py:51] epoch 1133, training loss: 13306.42, average training loss: 14321.33, base loss: 21686.07
[INFO 2017-06-28 18:26:36,261 main.py:51] epoch 1134, training loss: 12809.14, average training loss: 14319.39, base loss: 21688.97
[INFO 2017-06-28 18:26:37,684 main.py:51] epoch 1135, training loss: 13473.76, average training loss: 14317.30, base loss: 21689.98
[INFO 2017-06-28 18:26:38,998 main.py:51] epoch 1136, training loss: 12908.33, average training loss: 14315.22, base loss: 21689.43
[INFO 2017-06-28 18:26:40,415 main.py:51] epoch 1137, training loss: 13947.07, average training loss: 14312.44, base loss: 21688.31
[INFO 2017-06-28 18:26:41,709 main.py:51] epoch 1138, training loss: 15229.96, average training loss: 14311.86, base loss: 21689.56
[INFO 2017-06-28 18:26:43,104 main.py:51] epoch 1139, training loss: 12313.69, average training loss: 14308.90, base loss: 21687.48
[INFO 2017-06-28 18:26:44,549 main.py:51] epoch 1140, training loss: 13326.29, average training loss: 14305.78, base loss: 21685.19
[INFO 2017-06-28 18:26:45,955 main.py:51] epoch 1141, training loss: 13670.52, average training loss: 14304.81, base loss: 21685.99
[INFO 2017-06-28 18:26:47,226 main.py:51] epoch 1142, training loss: 14445.60, average training loss: 14304.07, base loss: 21688.58
[INFO 2017-06-28 18:26:48,571 main.py:51] epoch 1143, training loss: 11640.10, average training loss: 14300.79, base loss: 21685.77
[INFO 2017-06-28 18:26:49,923 main.py:51] epoch 1144, training loss: 13523.03, average training loss: 14299.77, base loss: 21688.02
[INFO 2017-06-28 18:26:51,250 main.py:51] epoch 1145, training loss: 14113.31, average training loss: 14297.22, base loss: 21687.24
[INFO 2017-06-28 18:26:52,531 main.py:51] epoch 1146, training loss: 13291.46, average training loss: 14294.65, base loss: 21685.72
[INFO 2017-06-28 18:26:53,810 main.py:51] epoch 1147, training loss: 14567.83, average training loss: 14292.01, base loss: 21684.80
[INFO 2017-06-28 18:26:55,107 main.py:51] epoch 1148, training loss: 14199.17, average training loss: 14289.26, base loss: 21684.83
[INFO 2017-06-28 18:26:56,346 main.py:51] epoch 1149, training loss: 13923.70, average training loss: 14287.74, base loss: 21686.61
[INFO 2017-06-28 18:26:57,556 main.py:51] epoch 1150, training loss: 13748.17, average training loss: 14285.76, base loss: 21689.97
[INFO 2017-06-28 18:26:58,785 main.py:51] epoch 1151, training loss: 15595.42, average training loss: 14286.61, base loss: 21695.44
[INFO 2017-06-28 18:27:00,156 main.py:51] epoch 1152, training loss: 13215.07, average training loss: 14283.48, base loss: 21692.12
[INFO 2017-06-28 18:27:01,462 main.py:51] epoch 1153, training loss: 13529.32, average training loss: 14281.18, base loss: 21691.35
[INFO 2017-06-28 18:27:02,804 main.py:51] epoch 1154, training loss: 13844.35, average training loss: 14279.04, base loss: 21693.01
[INFO 2017-06-28 18:27:04,172 main.py:51] epoch 1155, training loss: 13982.28, average training loss: 14277.47, base loss: 21694.55
[INFO 2017-06-28 18:27:05,519 main.py:51] epoch 1156, training loss: 13776.08, average training loss: 14274.96, base loss: 21693.38
[INFO 2017-06-28 18:27:06,775 main.py:51] epoch 1157, training loss: 13796.11, average training loss: 14273.17, base loss: 21692.96
[INFO 2017-06-28 18:27:08,066 main.py:51] epoch 1158, training loss: 14419.89, average training loss: 14270.21, base loss: 21692.79
[INFO 2017-06-28 18:27:09,306 main.py:51] epoch 1159, training loss: 13664.11, average training loss: 14267.26, base loss: 21690.34
[INFO 2017-06-28 18:27:10,566 main.py:51] epoch 1160, training loss: 13433.89, average training loss: 14265.75, base loss: 21691.31
[INFO 2017-06-28 18:27:11,825 main.py:51] epoch 1161, training loss: 13337.05, average training loss: 14263.43, base loss: 21691.10
[INFO 2017-06-28 18:27:13,163 main.py:51] epoch 1162, training loss: 13987.99, average training loss: 14262.50, base loss: 21692.93
[INFO 2017-06-28 18:27:14,545 main.py:51] epoch 1163, training loss: 13835.61, average training loss: 14260.74, base loss: 21694.60
[INFO 2017-06-28 18:27:15,963 main.py:51] epoch 1164, training loss: 12690.70, average training loss: 14259.02, base loss: 21695.89
[INFO 2017-06-28 18:27:17,297 main.py:51] epoch 1165, training loss: 15131.37, average training loss: 14259.66, base loss: 21699.00
[INFO 2017-06-28 18:27:18,522 main.py:51] epoch 1166, training loss: 13061.33, average training loss: 14254.99, base loss: 21694.97
[INFO 2017-06-28 18:27:19,798 main.py:51] epoch 1167, training loss: 13916.86, average training loss: 14254.23, base loss: 21695.32
[INFO 2017-06-28 18:27:21,123 main.py:51] epoch 1168, training loss: 13187.10, average training loss: 14251.66, base loss: 21695.34
[INFO 2017-06-28 18:27:22,606 main.py:51] epoch 1169, training loss: 14727.90, average training loss: 14250.94, base loss: 21697.76
[INFO 2017-06-28 18:27:23,872 main.py:51] epoch 1170, training loss: 13901.84, average training loss: 14249.19, base loss: 21698.27
[INFO 2017-06-28 18:27:25,150 main.py:51] epoch 1171, training loss: 13686.99, average training loss: 14247.52, base loss: 21697.93
[INFO 2017-06-28 18:27:26,410 main.py:51] epoch 1172, training loss: 16398.10, average training loss: 14248.64, base loss: 21701.79
[INFO 2017-06-28 18:27:27,692 main.py:51] epoch 1173, training loss: 14145.52, average training loss: 14248.02, base loss: 21701.80
[INFO 2017-06-28 18:27:29,047 main.py:51] epoch 1174, training loss: 13930.23, average training loss: 14246.63, base loss: 21704.03
[INFO 2017-06-28 18:27:30,393 main.py:51] epoch 1175, training loss: 14480.20, average training loss: 14244.68, base loss: 21703.78
[INFO 2017-06-28 18:27:31,754 main.py:51] epoch 1176, training loss: 13851.14, average training loss: 14241.93, base loss: 21702.35
[INFO 2017-06-28 18:27:33,058 main.py:51] epoch 1177, training loss: 12523.42, average training loss: 14239.26, base loss: 21702.95
[INFO 2017-06-28 18:27:34,412 main.py:51] epoch 1178, training loss: 13094.58, average training loss: 14235.97, base loss: 21699.73
[INFO 2017-06-28 18:27:35,763 main.py:51] epoch 1179, training loss: 15655.63, average training loss: 14236.17, base loss: 21700.05
[INFO 2017-06-28 18:27:37,174 main.py:51] epoch 1180, training loss: 14839.77, average training loss: 14236.50, base loss: 21701.22
[INFO 2017-06-28 18:27:38,431 main.py:51] epoch 1181, training loss: 13880.64, average training loss: 14235.63, base loss: 21702.83
[INFO 2017-06-28 18:27:39,781 main.py:51] epoch 1182, training loss: 13225.90, average training loss: 14231.96, base loss: 21699.77
[INFO 2017-06-28 18:27:40,922 main.py:51] epoch 1183, training loss: 12361.92, average training loss: 14229.01, base loss: 21697.58
[INFO 2017-06-28 18:27:42,196 main.py:51] epoch 1184, training loss: 13980.62, average training loss: 14228.54, base loss: 21699.54
[INFO 2017-06-28 18:27:43,374 main.py:51] epoch 1185, training loss: 13511.73, average training loss: 14226.37, base loss: 21700.27
[INFO 2017-06-28 18:27:44,603 main.py:51] epoch 1186, training loss: 15168.53, average training loss: 14225.53, base loss: 21701.94
[INFO 2017-06-28 18:27:45,942 main.py:51] epoch 1187, training loss: 13750.07, average training loss: 14224.03, base loss: 21701.74
[INFO 2017-06-28 18:27:47,202 main.py:51] epoch 1188, training loss: 14628.35, average training loss: 14224.27, base loss: 21703.86
[INFO 2017-06-28 18:27:48,515 main.py:51] epoch 1189, training loss: 13032.79, average training loss: 14221.59, base loss: 21700.84
[INFO 2017-06-28 18:27:49,810 main.py:51] epoch 1190, training loss: 14233.51, average training loss: 14220.05, base loss: 21699.81
[INFO 2017-06-28 18:27:51,217 main.py:51] epoch 1191, training loss: 14174.99, average training loss: 14219.60, base loss: 21701.07
[INFO 2017-06-28 18:27:52,588 main.py:51] epoch 1192, training loss: 13887.32, average training loss: 14217.82, base loss: 21702.16
[INFO 2017-06-28 18:27:53,845 main.py:51] epoch 1193, training loss: 12675.36, average training loss: 14215.17, base loss: 21703.13
[INFO 2017-06-28 18:27:55,096 main.py:51] epoch 1194, training loss: 14536.34, average training loss: 14213.71, base loss: 21702.55
[INFO 2017-06-28 18:27:56,333 main.py:51] epoch 1195, training loss: 12338.17, average training loss: 14208.43, base loss: 21698.76
[INFO 2017-06-28 18:27:57,802 main.py:51] epoch 1196, training loss: 13634.35, average training loss: 14205.60, base loss: 21697.15
[INFO 2017-06-28 18:27:59,089 main.py:51] epoch 1197, training loss: 14139.53, average training loss: 14205.89, base loss: 21699.77
[INFO 2017-06-28 18:28:00,442 main.py:51] epoch 1198, training loss: 13454.80, average training loss: 14204.33, base loss: 21697.26
[INFO 2017-06-28 18:28:01,724 main.py:51] epoch 1199, training loss: 13597.63, average training loss: 14203.46, base loss: 21697.65
[INFO 2017-06-28 18:28:01,724 main.py:53] epoch 1199, testing
[INFO 2017-06-28 18:28:07,786 main.py:105] average testing loss: 15158.07, base loss: 22534.81
[INFO 2017-06-28 18:28:07,786 main.py:106] improve_loss: 7376.74, improve_percent: 0.33
[INFO 2017-06-28 18:28:07,787 main.py:76] current best improved percent: 0.34
[INFO 2017-06-28 18:28:09,081 main.py:51] epoch 1200, training loss: 12360.14, average training loss: 14201.12, base loss: 21696.96
[INFO 2017-06-28 18:28:10,415 main.py:51] epoch 1201, training loss: 14013.03, average training loss: 14200.63, base loss: 21699.96
[INFO 2017-06-28 18:28:11,737 main.py:51] epoch 1202, training loss: 13629.87, average training loss: 14200.28, base loss: 21702.65
[INFO 2017-06-28 18:28:13,109 main.py:51] epoch 1203, training loss: 12516.11, average training loss: 14196.69, base loss: 21699.55
[INFO 2017-06-28 18:28:14,467 main.py:51] epoch 1204, training loss: 14137.22, average training loss: 14194.86, base loss: 21701.21
[INFO 2017-06-28 18:28:15,739 main.py:51] epoch 1205, training loss: 13347.07, average training loss: 14191.76, base loss: 21699.66
[INFO 2017-06-28 18:28:16,965 main.py:51] epoch 1206, training loss: 14820.85, average training loss: 14190.32, base loss: 21700.32
[INFO 2017-06-28 18:28:18,373 main.py:51] epoch 1207, training loss: 12861.77, average training loss: 14186.96, base loss: 21697.25
[INFO 2017-06-28 18:28:19,712 main.py:51] epoch 1208, training loss: 13311.49, average training loss: 14184.45, base loss: 21696.90
[INFO 2017-06-28 18:28:20,962 main.py:51] epoch 1209, training loss: 13743.70, average training loss: 14182.61, base loss: 21697.88
[INFO 2017-06-28 18:28:22,371 main.py:51] epoch 1210, training loss: 13509.89, average training loss: 14179.69, base loss: 21697.40
[INFO 2017-06-28 18:28:23,547 main.py:51] epoch 1211, training loss: 14227.14, average training loss: 14177.76, base loss: 21697.59
[INFO 2017-06-28 18:28:24,978 main.py:51] epoch 1212, training loss: 14036.31, average training loss: 14176.66, base loss: 21699.65
[INFO 2017-06-28 18:28:26,342 main.py:51] epoch 1213, training loss: 12922.24, average training loss: 14174.90, base loss: 21699.73
[INFO 2017-06-28 18:28:27,682 main.py:51] epoch 1214, training loss: 12149.48, average training loss: 14170.80, base loss: 21696.14
[INFO 2017-06-28 18:28:28,898 main.py:51] epoch 1215, training loss: 13112.44, average training loss: 14167.84, base loss: 21694.00
[INFO 2017-06-28 18:28:30,150 main.py:51] epoch 1216, training loss: 14183.70, average training loss: 14166.79, base loss: 21694.51
[INFO 2017-06-28 18:28:31,558 main.py:51] epoch 1217, training loss: 14890.42, average training loss: 14167.12, base loss: 21696.44
[INFO 2017-06-28 18:28:32,852 main.py:51] epoch 1218, training loss: 14721.43, average training loss: 14165.05, base loss: 21696.19
[INFO 2017-06-28 18:28:34,086 main.py:51] epoch 1219, training loss: 12997.95, average training loss: 14161.29, base loss: 21694.58
[INFO 2017-06-28 18:28:35,439 main.py:51] epoch 1220, training loss: 12463.37, average training loss: 14158.30, base loss: 21690.77
[INFO 2017-06-28 18:28:36,685 main.py:51] epoch 1221, training loss: 13738.03, average training loss: 14156.83, base loss: 21693.15
[INFO 2017-06-28 18:28:38,005 main.py:51] epoch 1222, training loss: 13079.05, average training loss: 14155.81, base loss: 21693.70
[INFO 2017-06-28 18:28:39,315 main.py:51] epoch 1223, training loss: 13893.06, average training loss: 14151.77, base loss: 21690.88
[INFO 2017-06-28 18:28:40,606 main.py:51] epoch 1224, training loss: 13381.75, average training loss: 14150.83, base loss: 21691.69
[INFO 2017-06-28 18:28:41,820 main.py:51] epoch 1225, training loss: 14491.35, average training loss: 14148.75, base loss: 21690.10
[INFO 2017-06-28 18:28:43,109 main.py:51] epoch 1226, training loss: 13803.41, average training loss: 14146.40, base loss: 21688.09
[INFO 2017-06-28 18:28:44,491 main.py:51] epoch 1227, training loss: 13222.83, average training loss: 14144.50, base loss: 21688.89
[INFO 2017-06-28 18:28:45,752 main.py:51] epoch 1228, training loss: 12669.04, average training loss: 14142.02, base loss: 21686.88
[INFO 2017-06-28 18:28:47,133 main.py:51] epoch 1229, training loss: 15740.12, average training loss: 14143.27, base loss: 21689.96
[INFO 2017-06-28 18:28:48,495 main.py:51] epoch 1230, training loss: 13406.51, average training loss: 14140.54, base loss: 21686.33
[INFO 2017-06-28 18:28:49,747 main.py:51] epoch 1231, training loss: 13652.49, average training loss: 14140.30, base loss: 21687.76
[INFO 2017-06-28 18:28:51,020 main.py:51] epoch 1232, training loss: 13372.83, average training loss: 14136.51, base loss: 21685.69
[INFO 2017-06-28 18:28:52,254 main.py:51] epoch 1233, training loss: 14227.47, average training loss: 14134.22, base loss: 21683.73
[INFO 2017-06-28 18:28:53,587 main.py:51] epoch 1234, training loss: 12840.98, average training loss: 14130.66, base loss: 21683.48
[INFO 2017-06-28 18:28:54,932 main.py:51] epoch 1235, training loss: 12863.38, average training loss: 14129.14, base loss: 21685.73
[INFO 2017-06-28 18:28:56,262 main.py:51] epoch 1236, training loss: 14813.71, average training loss: 14129.39, base loss: 21687.69
[INFO 2017-06-28 18:28:57,526 main.py:51] epoch 1237, training loss: 12781.58, average training loss: 14126.82, base loss: 21687.01
[INFO 2017-06-28 18:28:58,854 main.py:51] epoch 1238, training loss: 14232.50, average training loss: 14125.47, base loss: 21687.81
[INFO 2017-06-28 18:29:00,114 main.py:51] epoch 1239, training loss: 13035.03, average training loss: 14123.65, base loss: 21688.05
[INFO 2017-06-28 18:29:01,454 main.py:51] epoch 1240, training loss: 13966.23, average training loss: 14121.66, base loss: 21686.10
[INFO 2017-06-28 18:29:02,733 main.py:51] epoch 1241, training loss: 12830.59, average training loss: 14117.83, base loss: 21683.60
[INFO 2017-06-28 18:29:04,122 main.py:51] epoch 1242, training loss: 12806.04, average training loss: 14115.77, base loss: 21684.11
[INFO 2017-06-28 18:29:05,432 main.py:51] epoch 1243, training loss: 13542.52, average training loss: 14113.52, base loss: 21681.71
[INFO 2017-06-28 18:29:06,693 main.py:51] epoch 1244, training loss: 14321.94, average training loss: 14111.99, base loss: 21683.89
[INFO 2017-06-28 18:29:08,031 main.py:51] epoch 1245, training loss: 13771.97, average training loss: 14110.65, base loss: 21683.94
[INFO 2017-06-28 18:29:09,281 main.py:51] epoch 1246, training loss: 12217.03, average training loss: 14108.61, base loss: 21681.14
[INFO 2017-06-28 18:29:10,523 main.py:51] epoch 1247, training loss: 14181.19, average training loss: 14108.32, base loss: 21681.58
[INFO 2017-06-28 18:29:11,762 main.py:51] epoch 1248, training loss: 14135.74, average training loss: 14106.05, base loss: 21680.96
[INFO 2017-06-28 18:29:13,144 main.py:51] epoch 1249, training loss: 14645.32, average training loss: 14105.57, base loss: 21680.93
[INFO 2017-06-28 18:29:14,460 main.py:51] epoch 1250, training loss: 14500.91, average training loss: 14106.12, base loss: 21682.90
[INFO 2017-06-28 18:29:15,743 main.py:51] epoch 1251, training loss: 13591.83, average training loss: 14103.63, base loss: 21679.65
[INFO 2017-06-28 18:29:17,053 main.py:51] epoch 1252, training loss: 12637.51, average training loss: 14101.28, base loss: 21679.08
[INFO 2017-06-28 18:29:18,286 main.py:51] epoch 1253, training loss: 13741.13, average training loss: 14099.41, base loss: 21679.09
[INFO 2017-06-28 18:29:19,610 main.py:51] epoch 1254, training loss: 15416.00, average training loss: 14100.90, base loss: 21681.24
[INFO 2017-06-28 18:29:20,855 main.py:51] epoch 1255, training loss: 13502.18, average training loss: 14099.63, base loss: 21682.94
[INFO 2017-06-28 18:29:22,271 main.py:51] epoch 1256, training loss: 12686.69, average training loss: 14097.02, base loss: 21681.73
[INFO 2017-06-28 18:29:23,677 main.py:51] epoch 1257, training loss: 13451.43, average training loss: 14095.94, base loss: 21683.01
[INFO 2017-06-28 18:29:25,079 main.py:51] epoch 1258, training loss: 13215.26, average training loss: 14094.49, base loss: 21683.71
[INFO 2017-06-28 18:29:26,293 main.py:51] epoch 1259, training loss: 13481.54, average training loss: 14089.74, base loss: 21679.40
[INFO 2017-06-28 18:29:27,570 main.py:51] epoch 1260, training loss: 15448.37, average training loss: 14090.67, base loss: 21681.65
[INFO 2017-06-28 18:29:28,853 main.py:51] epoch 1261, training loss: 13594.12, average training loss: 14089.55, base loss: 21681.86
[INFO 2017-06-28 18:29:30,263 main.py:51] epoch 1262, training loss: 13652.59, average training loss: 14087.20, base loss: 21682.12
[INFO 2017-06-28 18:29:31,594 main.py:51] epoch 1263, training loss: 13111.42, average training loss: 14086.94, base loss: 21683.79
[INFO 2017-06-28 18:29:32,821 main.py:51] epoch 1264, training loss: 14360.30, average training loss: 14087.74, base loss: 21686.18
[INFO 2017-06-28 18:29:34,176 main.py:51] epoch 1265, training loss: 13505.81, average training loss: 14087.10, base loss: 21687.49
[INFO 2017-06-28 18:29:35,459 main.py:51] epoch 1266, training loss: 15078.02, average training loss: 14087.18, base loss: 21689.48
[INFO 2017-06-28 18:29:36,792 main.py:51] epoch 1267, training loss: 14588.61, average training loss: 14088.07, base loss: 21690.80
[INFO 2017-06-28 18:29:37,998 main.py:51] epoch 1268, training loss: 12607.62, average training loss: 14084.64, base loss: 21688.06
[INFO 2017-06-28 18:29:39,304 main.py:51] epoch 1269, training loss: 14252.99, average training loss: 14082.01, base loss: 21687.42
[INFO 2017-06-28 18:29:40,618 main.py:51] epoch 1270, training loss: 11944.78, average training loss: 14078.42, base loss: 21683.40
[INFO 2017-06-28 18:29:41,952 main.py:51] epoch 1271, training loss: 12201.36, average training loss: 14076.19, base loss: 21684.14
[INFO 2017-06-28 18:29:43,340 main.py:51] epoch 1272, training loss: 13919.25, average training loss: 14074.97, base loss: 21684.34
[INFO 2017-06-28 18:29:44,664 main.py:51] epoch 1273, training loss: 12474.34, average training loss: 14074.42, base loss: 21686.19
[INFO 2017-06-28 18:29:46,047 main.py:51] epoch 1274, training loss: 13391.70, average training loss: 14072.63, base loss: 21685.83
[INFO 2017-06-28 18:29:47,272 main.py:51] epoch 1275, training loss: 12845.77, average training loss: 14070.05, base loss: 21684.27
[INFO 2017-06-28 18:29:48,647 main.py:51] epoch 1276, training loss: 14333.84, average training loss: 14070.48, base loss: 21687.18
[INFO 2017-06-28 18:29:49,915 main.py:51] epoch 1277, training loss: 14087.75, average training loss: 14068.90, base loss: 21688.59
[INFO 2017-06-28 18:29:51,290 main.py:51] epoch 1278, training loss: 13846.20, average training loss: 14068.99, base loss: 21691.62
[INFO 2017-06-28 18:29:52,632 main.py:51] epoch 1279, training loss: 16374.95, average training loss: 14069.95, base loss: 21693.27
[INFO 2017-06-28 18:29:53,955 main.py:51] epoch 1280, training loss: 13646.01, average training loss: 14069.14, base loss: 21693.01
[INFO 2017-06-28 18:29:55,249 main.py:51] epoch 1281, training loss: 14541.35, average training loss: 14068.46, base loss: 21694.79
[INFO 2017-06-28 18:29:56,698 main.py:51] epoch 1282, training loss: 15151.05, average training loss: 14068.31, base loss: 21698.97
[INFO 2017-06-28 18:29:57,991 main.py:51] epoch 1283, training loss: 14946.74, average training loss: 14068.02, base loss: 21698.54
[INFO 2017-06-28 18:29:59,337 main.py:51] epoch 1284, training loss: 13220.82, average training loss: 14066.38, base loss: 21696.94
[INFO 2017-06-28 18:30:00,681 main.py:51] epoch 1285, training loss: 13898.12, average training loss: 14065.88, base loss: 21700.92
[INFO 2017-06-28 18:30:02,028 main.py:51] epoch 1286, training loss: 14562.42, average training loss: 14065.09, base loss: 21703.94
[INFO 2017-06-28 18:30:03,498 main.py:51] epoch 1287, training loss: 16256.44, average training loss: 14065.31, base loss: 21704.22
[INFO 2017-06-28 18:30:05,081 main.py:51] epoch 1288, training loss: 13052.25, average training loss: 14063.34, base loss: 21704.52
[INFO 2017-06-28 18:30:06,375 main.py:51] epoch 1289, training loss: 14081.72, average training loss: 14062.86, base loss: 21705.19
[INFO 2017-06-28 18:30:07,623 main.py:51] epoch 1290, training loss: 14587.36, average training loss: 14061.70, base loss: 21705.77
[INFO 2017-06-28 18:30:08,880 main.py:51] epoch 1291, training loss: 13117.52, average training loss: 14060.97, base loss: 21706.18
[INFO 2017-06-28 18:30:10,109 main.py:51] epoch 1292, training loss: 13216.68, average training loss: 14059.68, base loss: 21705.78
[INFO 2017-06-28 18:30:11,414 main.py:51] epoch 1293, training loss: 14729.12, average training loss: 14058.86, base loss: 21704.71
[INFO 2017-06-28 18:30:12,802 main.py:51] epoch 1294, training loss: 13361.45, average training loss: 14057.05, base loss: 21704.50
[INFO 2017-06-28 18:30:14,039 main.py:51] epoch 1295, training loss: 15003.13, average training loss: 14058.96, base loss: 21707.88
[INFO 2017-06-28 18:30:15,307 main.py:51] epoch 1296, training loss: 13557.81, average training loss: 14057.88, base loss: 21709.84
[INFO 2017-06-28 18:30:16,664 main.py:51] epoch 1297, training loss: 13787.85, average training loss: 14059.10, base loss: 21713.42
[INFO 2017-06-28 18:30:17,988 main.py:51] epoch 1298, training loss: 12113.65, average training loss: 14056.36, base loss: 21709.85
[INFO 2017-06-28 18:30:19,264 main.py:51] epoch 1299, training loss: 12897.80, average training loss: 14055.10, base loss: 21709.59
[INFO 2017-06-28 18:30:19,264 main.py:53] epoch 1299, testing
[INFO 2017-06-28 18:30:25,182 main.py:105] average testing loss: 14773.86, base loss: 22231.77
[INFO 2017-06-28 18:30:25,182 main.py:106] improve_loss: 7457.91, improve_percent: 0.34
[INFO 2017-06-28 18:30:25,182 main.py:76] current best improved percent: 0.34
[INFO 2017-06-28 18:30:26,502 main.py:51] epoch 1300, training loss: 12920.73, average training loss: 14053.56, base loss: 21709.36
[INFO 2017-06-28 18:30:27,780 main.py:51] epoch 1301, training loss: 15181.55, average training loss: 14054.42, base loss: 21712.54
[INFO 2017-06-28 18:30:29,235 main.py:51] epoch 1302, training loss: 12757.09, average training loss: 14053.95, base loss: 21714.37
[INFO 2017-06-28 18:30:30,533 main.py:51] epoch 1303, training loss: 13145.54, average training loss: 14052.25, base loss: 21713.50
[INFO 2017-06-28 18:30:31,808 main.py:51] epoch 1304, training loss: 12418.73, average training loss: 14051.11, base loss: 21713.74
[INFO 2017-06-28 18:30:33,172 main.py:51] epoch 1305, training loss: 13052.81, average training loss: 14049.03, base loss: 21711.70
[INFO 2017-06-28 18:30:34,509 main.py:51] epoch 1306, training loss: 13334.06, average training loss: 14047.32, base loss: 21711.69
[INFO 2017-06-28 18:30:35,791 main.py:51] epoch 1307, training loss: 13573.67, average training loss: 14045.83, base loss: 21709.85
[INFO 2017-06-28 18:30:37,077 main.py:51] epoch 1308, training loss: 13400.90, average training loss: 14045.46, base loss: 21711.37
[INFO 2017-06-28 18:30:38,441 main.py:51] epoch 1309, training loss: 12294.30, average training loss: 14041.91, base loss: 21709.00
[INFO 2017-06-28 18:30:39,793 main.py:51] epoch 1310, training loss: 13317.25, average training loss: 14038.24, base loss: 21705.87
[INFO 2017-06-28 18:30:41,218 main.py:51] epoch 1311, training loss: 15071.36, average training loss: 14038.90, base loss: 21706.06
[INFO 2017-06-28 18:30:42,602 main.py:51] epoch 1312, training loss: 14898.67, average training loss: 14040.40, base loss: 21709.95
[INFO 2017-06-28 18:30:43,896 main.py:51] epoch 1313, training loss: 13464.93, average training loss: 14038.50, base loss: 21709.31
[INFO 2017-06-28 18:30:45,301 main.py:51] epoch 1314, training loss: 12570.38, average training loss: 14036.53, base loss: 21708.10
[INFO 2017-06-28 18:30:46,523 main.py:51] epoch 1315, training loss: 13832.84, average training loss: 14030.98, base loss: 21704.87
[INFO 2017-06-28 18:30:47,799 main.py:51] epoch 1316, training loss: 13320.91, average training loss: 14029.91, base loss: 21705.24
[INFO 2017-06-28 18:30:49,077 main.py:51] epoch 1317, training loss: 12765.25, average training loss: 14028.83, base loss: 21705.96
[INFO 2017-06-28 18:30:50,277 main.py:51] epoch 1318, training loss: 12492.85, average training loss: 14026.73, base loss: 21704.70
[INFO 2017-06-28 18:30:51,578 main.py:51] epoch 1319, training loss: 13230.91, average training loss: 14024.66, base loss: 21703.41
[INFO 2017-06-28 18:30:52,876 main.py:51] epoch 1320, training loss: 13525.50, average training loss: 14022.71, base loss: 21702.38
[INFO 2017-06-28 18:30:54,252 main.py:51] epoch 1321, training loss: 15423.71, average training loss: 14023.59, base loss: 21705.45
[INFO 2017-06-28 18:30:55,466 main.py:51] epoch 1322, training loss: 14160.51, average training loss: 14022.50, base loss: 21704.75
[INFO 2017-06-28 18:30:56,775 main.py:51] epoch 1323, training loss: 12278.81, average training loss: 14019.33, base loss: 21702.51
[INFO 2017-06-28 18:30:58,119 main.py:51] epoch 1324, training loss: 13040.47, average training loss: 14017.33, base loss: 21703.95
[INFO 2017-06-28 18:30:59,379 main.py:51] epoch 1325, training loss: 12939.56, average training loss: 14016.41, base loss: 21702.77
[INFO 2017-06-28 18:31:00,798 main.py:51] epoch 1326, training loss: 13955.27, average training loss: 14014.45, base loss: 21701.23
[INFO 2017-06-28 18:31:02,173 main.py:51] epoch 1327, training loss: 13404.25, average training loss: 14014.20, base loss: 21701.86
[INFO 2017-06-28 18:31:03,547 main.py:51] epoch 1328, training loss: 12454.39, average training loss: 14012.38, base loss: 21702.23
[INFO 2017-06-28 18:31:04,832 main.py:51] epoch 1329, training loss: 14734.24, average training loss: 14011.48, base loss: 21702.76
[INFO 2017-06-28 18:31:06,104 main.py:51] epoch 1330, training loss: 12793.99, average training loss: 14009.37, base loss: 21699.96
[INFO 2017-06-28 18:31:07,402 main.py:51] epoch 1331, training loss: 13852.76, average training loss: 14008.90, base loss: 21701.15
[INFO 2017-06-28 18:31:08,768 main.py:51] epoch 1332, training loss: 13424.94, average training loss: 14007.96, base loss: 21702.72
[INFO 2017-06-28 18:31:10,077 main.py:51] epoch 1333, training loss: 12419.14, average training loss: 14005.34, base loss: 21700.53
[INFO 2017-06-28 18:31:11,349 main.py:51] epoch 1334, training loss: 13278.35, average training loss: 14002.48, base loss: 21699.57
[INFO 2017-06-28 18:31:12,833 main.py:51] epoch 1335, training loss: 14082.76, average training loss: 14001.86, base loss: 21699.39
[INFO 2017-06-28 18:31:14,115 main.py:51] epoch 1336, training loss: 13767.47, average training loss: 14002.32, base loss: 21702.25
[INFO 2017-06-28 18:31:15,317 main.py:51] epoch 1337, training loss: 13727.66, average training loss: 14002.43, base loss: 21704.63
[INFO 2017-06-28 18:31:16,692 main.py:51] epoch 1338, training loss: 12650.72, average training loss: 13999.12, base loss: 21702.95
[INFO 2017-06-28 18:31:18,052 main.py:51] epoch 1339, training loss: 12749.26, average training loss: 13997.23, base loss: 21702.54
[INFO 2017-06-28 18:31:19,389 main.py:51] epoch 1340, training loss: 12690.92, average training loss: 13995.82, base loss: 21703.55
[INFO 2017-06-28 18:31:20,756 main.py:51] epoch 1341, training loss: 12643.09, average training loss: 13993.90, base loss: 21702.29
[INFO 2017-06-28 18:31:22,067 main.py:51] epoch 1342, training loss: 13360.51, average training loss: 13993.61, base loss: 21702.75
[INFO 2017-06-28 18:31:23,325 main.py:51] epoch 1343, training loss: 13174.66, average training loss: 13991.67, base loss: 21702.29
[INFO 2017-06-28 18:31:24,671 main.py:51] epoch 1344, training loss: 14402.71, average training loss: 13993.32, base loss: 21704.89
[INFO 2017-06-28 18:31:25,942 main.py:51] epoch 1345, training loss: 13283.63, average training loss: 13992.83, base loss: 21705.54
[INFO 2017-06-28 18:31:27,183 main.py:51] epoch 1346, training loss: 13768.74, average training loss: 13993.39, base loss: 21707.18
[INFO 2017-06-28 18:31:28,573 main.py:51] epoch 1347, training loss: 14344.03, average training loss: 13990.77, base loss: 21706.68
[INFO 2017-06-28 18:31:29,789 main.py:51] epoch 1348, training loss: 12610.14, average training loss: 13989.98, base loss: 21707.68
[INFO 2017-06-28 18:31:31,067 main.py:51] epoch 1349, training loss: 14390.57, average training loss: 13987.41, base loss: 21705.23
[INFO 2017-06-28 18:31:32,316 main.py:51] epoch 1350, training loss: 14489.11, average training loss: 13986.58, base loss: 21706.70
[INFO 2017-06-28 18:31:33,597 main.py:51] epoch 1351, training loss: 14609.46, average training loss: 13986.82, base loss: 21708.38
[INFO 2017-06-28 18:31:34,961 main.py:51] epoch 1352, training loss: 13886.76, average training loss: 13985.94, base loss: 21709.43
[INFO 2017-06-28 18:31:36,183 main.py:51] epoch 1353, training loss: 13773.50, average training loss: 13986.32, base loss: 21711.40
[INFO 2017-06-28 18:31:37,423 main.py:51] epoch 1354, training loss: 12921.56, average training loss: 13983.59, base loss: 21709.41
[INFO 2017-06-28 18:31:38,644 main.py:51] epoch 1355, training loss: 13023.13, average training loss: 13982.50, base loss: 21710.30
[INFO 2017-06-28 18:31:39,866 main.py:51] epoch 1356, training loss: 11956.42, average training loss: 13980.27, base loss: 21709.42
[INFO 2017-06-28 18:31:41,198 main.py:51] epoch 1357, training loss: 12452.54, average training loss: 13976.17, base loss: 21707.72
[INFO 2017-06-28 18:31:42,492 main.py:51] epoch 1358, training loss: 13837.78, average training loss: 13975.80, base loss: 21707.65
[INFO 2017-06-28 18:31:43,702 main.py:51] epoch 1359, training loss: 13729.80, average training loss: 13975.78, base loss: 21709.56
[INFO 2017-06-28 18:31:45,022 main.py:51] epoch 1360, training loss: 13740.73, average training loss: 13975.78, base loss: 21710.74
[INFO 2017-06-28 18:31:46,313 main.py:51] epoch 1361, training loss: 12734.46, average training loss: 13974.76, base loss: 21711.17
[INFO 2017-06-28 18:31:47,582 main.py:51] epoch 1362, training loss: 12084.33, average training loss: 13972.87, base loss: 21708.47
[INFO 2017-06-28 18:31:48,839 main.py:51] epoch 1363, training loss: 15362.77, average training loss: 13973.17, base loss: 21711.05
[INFO 2017-06-28 18:31:50,075 main.py:51] epoch 1364, training loss: 14991.78, average training loss: 13974.87, base loss: 21715.16
[INFO 2017-06-28 18:31:51,411 main.py:51] epoch 1365, training loss: 14124.41, average training loss: 13972.71, base loss: 21711.25
[INFO 2017-06-28 18:31:52,760 main.py:51] epoch 1366, training loss: 13996.62, average training loss: 13972.96, base loss: 21711.83
[INFO 2017-06-28 18:31:54,171 main.py:51] epoch 1367, training loss: 13199.57, average training loss: 13971.97, base loss: 21711.43
[INFO 2017-06-28 18:31:55,458 main.py:51] epoch 1368, training loss: 13581.42, average training loss: 13970.59, base loss: 21710.84
[INFO 2017-06-28 18:31:56,764 main.py:51] epoch 1369, training loss: 12065.40, average training loss: 13966.26, base loss: 21705.20
[INFO 2017-06-28 18:31:58,021 main.py:51] epoch 1370, training loss: 12867.08, average training loss: 13964.81, base loss: 21705.58
[INFO 2017-06-28 18:31:59,255 main.py:51] epoch 1371, training loss: 13857.72, average training loss: 13964.64, base loss: 21707.01
[INFO 2017-06-28 18:32:00,543 main.py:51] epoch 1372, training loss: 12782.00, average training loss: 13963.16, base loss: 21706.89
[INFO 2017-06-28 18:32:01,828 main.py:51] epoch 1373, training loss: 13462.73, average training loss: 13962.52, base loss: 21707.59
[INFO 2017-06-28 18:32:03,086 main.py:51] epoch 1374, training loss: 13944.95, average training loss: 13962.32, base loss: 21710.10
[INFO 2017-06-28 18:32:04,308 main.py:51] epoch 1375, training loss: 12525.18, average training loss: 13960.82, base loss: 21711.93
[INFO 2017-06-28 18:32:05,593 main.py:51] epoch 1376, training loss: 13918.75, average training loss: 13959.33, base loss: 21711.39
[INFO 2017-06-28 18:32:06,910 main.py:51] epoch 1377, training loss: 13641.04, average training loss: 13957.42, base loss: 21709.57
[INFO 2017-06-28 18:32:08,249 main.py:51] epoch 1378, training loss: 12525.56, average training loss: 13954.79, base loss: 21708.34
[INFO 2017-06-28 18:32:09,600 main.py:51] epoch 1379, training loss: 14126.23, average training loss: 13952.41, base loss: 21706.35
[INFO 2017-06-28 18:32:10,954 main.py:51] epoch 1380, training loss: 15075.60, average training loss: 13952.20, base loss: 21708.56
[INFO 2017-06-28 18:32:12,242 main.py:51] epoch 1381, training loss: 13849.98, average training loss: 13952.11, base loss: 21709.24
[INFO 2017-06-28 18:32:13,605 main.py:51] epoch 1382, training loss: 13287.52, average training loss: 13951.67, base loss: 21710.55
[INFO 2017-06-28 18:32:14,925 main.py:51] epoch 1383, training loss: 12441.96, average training loss: 13950.70, base loss: 21711.54
[INFO 2017-06-28 18:32:16,253 main.py:51] epoch 1384, training loss: 13921.92, average training loss: 13947.59, base loss: 21708.79
[INFO 2017-06-28 18:32:17,608 main.py:51] epoch 1385, training loss: 15256.25, average training loss: 13946.74, base loss: 21708.98
[INFO 2017-06-28 18:32:18,839 main.py:51] epoch 1386, training loss: 12663.73, average training loss: 13945.17, base loss: 21709.37
[INFO 2017-06-28 18:32:20,124 main.py:51] epoch 1387, training loss: 14582.99, average training loss: 13945.31, base loss: 21707.83
[INFO 2017-06-28 18:32:21,468 main.py:51] epoch 1388, training loss: 13589.94, average training loss: 13943.69, base loss: 21706.34
[INFO 2017-06-28 18:32:22,787 main.py:51] epoch 1389, training loss: 13422.05, average training loss: 13942.99, base loss: 21708.19
[INFO 2017-06-28 18:32:24,082 main.py:51] epoch 1390, training loss: 12606.66, average training loss: 13940.97, base loss: 21706.17
[INFO 2017-06-28 18:32:25,441 main.py:51] epoch 1391, training loss: 12404.96, average training loss: 13937.10, base loss: 21703.52
[INFO 2017-06-28 18:32:26,785 main.py:51] epoch 1392, training loss: 13769.89, average training loss: 13935.99, base loss: 21702.86
[INFO 2017-06-28 18:32:28,014 main.py:51] epoch 1393, training loss: 13196.03, average training loss: 13933.67, base loss: 21698.98
[INFO 2017-06-28 18:32:29,222 main.py:51] epoch 1394, training loss: 15553.48, average training loss: 13935.03, base loss: 21700.43
[INFO 2017-06-28 18:32:30,516 main.py:51] epoch 1395, training loss: 12890.09, average training loss: 13930.90, base loss: 21698.14
[INFO 2017-06-28 18:32:31,883 main.py:51] epoch 1396, training loss: 13313.21, average training loss: 13929.37, base loss: 21697.88
[INFO 2017-06-28 18:32:33,280 main.py:51] epoch 1397, training loss: 13519.78, average training loss: 13929.03, base loss: 21699.02
[INFO 2017-06-28 18:32:34,551 main.py:51] epoch 1398, training loss: 12734.26, average training loss: 13926.57, base loss: 21696.87
[INFO 2017-06-28 18:32:35,879 main.py:51] epoch 1399, training loss: 12831.21, average training loss: 13924.72, base loss: 21696.53
[INFO 2017-06-28 18:32:35,879 main.py:53] epoch 1399, testing
[INFO 2017-06-28 18:32:41,986 main.py:105] average testing loss: 15181.43, base loss: 22604.51
[INFO 2017-06-28 18:32:41,986 main.py:106] improve_loss: 7423.08, improve_percent: 0.33
[INFO 2017-06-28 18:32:41,987 main.py:76] current best improved percent: 0.34
[INFO 2017-06-28 18:32:43,258 main.py:51] epoch 1400, training loss: 13105.32, average training loss: 13925.22, base loss: 21696.75
[INFO 2017-06-28 18:32:44,541 main.py:51] epoch 1401, training loss: 14395.27, average training loss: 13925.00, base loss: 21696.22
[INFO 2017-06-28 18:32:45,876 main.py:51] epoch 1402, training loss: 12850.41, average training loss: 13923.80, base loss: 21697.00
[INFO 2017-06-28 18:32:47,103 main.py:51] epoch 1403, training loss: 14782.22, average training loss: 13923.21, base loss: 21697.21
[INFO 2017-06-28 18:32:48,554 main.py:51] epoch 1404, training loss: 13153.89, average training loss: 13923.07, base loss: 21698.44
[INFO 2017-06-28 18:32:50,012 main.py:51] epoch 1405, training loss: 12254.40, average training loss: 13921.24, base loss: 21698.10
[INFO 2017-06-28 18:32:51,401 main.py:51] epoch 1406, training loss: 13279.83, average training loss: 13919.82, base loss: 21698.58
[INFO 2017-06-28 18:32:52,672 main.py:51] epoch 1407, training loss: 13064.37, average training loss: 13917.06, base loss: 21697.51
[INFO 2017-06-28 18:32:54,038 main.py:51] epoch 1408, training loss: 13590.64, average training loss: 13915.64, base loss: 21697.08
[INFO 2017-06-28 18:32:55,316 main.py:51] epoch 1409, training loss: 14569.26, average training loss: 13915.30, base loss: 21698.75
[INFO 2017-06-28 18:32:56,588 main.py:51] epoch 1410, training loss: 13749.82, average training loss: 13914.92, base loss: 21701.58
[INFO 2017-06-28 18:32:57,955 main.py:51] epoch 1411, training loss: 12776.16, average training loss: 13914.31, base loss: 21703.59
[INFO 2017-06-28 18:32:59,234 main.py:51] epoch 1412, training loss: 13247.08, average training loss: 13913.28, base loss: 21704.66
[INFO 2017-06-28 18:33:00,478 main.py:51] epoch 1413, training loss: 13234.21, average training loss: 13912.10, base loss: 21704.27
[INFO 2017-06-28 18:33:01,829 main.py:51] epoch 1414, training loss: 12575.70, average training loss: 13909.28, base loss: 21701.39
[INFO 2017-06-28 18:33:03,117 main.py:51] epoch 1415, training loss: 13793.46, average training loss: 13909.92, base loss: 21702.49
[INFO 2017-06-28 18:33:04,329 main.py:51] epoch 1416, training loss: 13194.48, average training loss: 13906.51, base loss: 21699.09
[INFO 2017-06-28 18:33:05,692 main.py:51] epoch 1417, training loss: 13658.08, average training loss: 13904.84, base loss: 21699.91
[INFO 2017-06-28 18:33:07,014 main.py:51] epoch 1418, training loss: 12894.98, average training loss: 13902.90, base loss: 21697.65
[INFO 2017-06-28 18:33:08,275 main.py:51] epoch 1419, training loss: 11893.83, average training loss: 13900.13, base loss: 21698.01
[INFO 2017-06-28 18:33:09,610 main.py:51] epoch 1420, training loss: 15852.97, average training loss: 13899.73, base loss: 21698.60
[INFO 2017-06-28 18:33:10,890 main.py:51] epoch 1421, training loss: 15200.07, average training loss: 13901.49, base loss: 21702.69
[INFO 2017-06-28 18:33:12,180 main.py:51] epoch 1422, training loss: 14570.92, average training loss: 13902.59, base loss: 21707.05
[INFO 2017-06-28 18:33:13,472 main.py:51] epoch 1423, training loss: 13217.65, average training loss: 13903.30, base loss: 21709.50
[INFO 2017-06-28 18:33:14,866 main.py:51] epoch 1424, training loss: 13911.25, average training loss: 13901.61, base loss: 21709.60
[INFO 2017-06-28 18:33:16,060 main.py:51] epoch 1425, training loss: 13689.50, average training loss: 13900.78, base loss: 21708.50
[INFO 2017-06-28 18:33:17,294 main.py:51] epoch 1426, training loss: 13321.72, average training loss: 13900.01, base loss: 21709.39
[INFO 2017-06-28 18:33:18,670 main.py:51] epoch 1427, training loss: 14028.48, average training loss: 13900.17, base loss: 21710.01
[INFO 2017-06-28 18:33:19,989 main.py:51] epoch 1428, training loss: 12745.49, average training loss: 13897.73, base loss: 21707.79
[INFO 2017-06-28 18:33:21,220 main.py:51] epoch 1429, training loss: 13115.78, average training loss: 13897.60, base loss: 21707.53
[INFO 2017-06-28 18:33:22,511 main.py:51] epoch 1430, training loss: 13616.38, average training loss: 13897.32, base loss: 21709.43
[INFO 2017-06-28 18:33:23,850 main.py:51] epoch 1431, training loss: 12442.21, average training loss: 13896.46, base loss: 21710.72
[INFO 2017-06-28 18:33:25,188 main.py:51] epoch 1432, training loss: 13249.62, average training loss: 13896.45, base loss: 21712.06
[INFO 2017-06-28 18:33:26,474 main.py:51] epoch 1433, training loss: 14405.36, average training loss: 13895.25, base loss: 21710.84
[INFO 2017-06-28 18:33:27,968 main.py:51] epoch 1434, training loss: 13632.40, average training loss: 13895.01, base loss: 21711.63
[INFO 2017-06-28 18:33:29,282 main.py:51] epoch 1435, training loss: 13558.14, average training loss: 13893.21, base loss: 21709.17
[INFO 2017-06-28 18:33:30,521 main.py:51] epoch 1436, training loss: 13363.85, average training loss: 13891.40, base loss: 21708.04
[INFO 2017-06-28 18:33:31,796 main.py:51] epoch 1437, training loss: 15344.40, average training loss: 13893.49, base loss: 21711.46
[INFO 2017-06-28 18:33:33,097 main.py:51] epoch 1438, training loss: 12620.05, average training loss: 13892.47, base loss: 21711.47
[INFO 2017-06-28 18:33:34,339 main.py:51] epoch 1439, training loss: 13709.52, average training loss: 13890.79, base loss: 21710.93
[INFO 2017-06-28 18:33:35,685 main.py:51] epoch 1440, training loss: 12914.39, average training loss: 13889.20, base loss: 21710.82
[INFO 2017-06-28 18:33:37,003 main.py:51] epoch 1441, training loss: 13828.72, average training loss: 13889.53, base loss: 21710.40
[INFO 2017-06-28 18:33:38,282 main.py:51] epoch 1442, training loss: 13158.86, average training loss: 13888.31, base loss: 21709.74
[INFO 2017-06-28 18:33:39,555 main.py:51] epoch 1443, training loss: 14405.88, average training loss: 13889.35, base loss: 21712.88
[INFO 2017-06-28 18:33:40,902 main.py:51] epoch 1444, training loss: 14177.90, average training loss: 13888.13, base loss: 21712.08
[INFO 2017-06-28 18:33:42,202 main.py:51] epoch 1445, training loss: 12828.85, average training loss: 13886.34, base loss: 21711.64
[INFO 2017-06-28 18:33:43,632 main.py:51] epoch 1446, training loss: 12991.58, average training loss: 13884.97, base loss: 21710.45
[INFO 2017-06-28 18:33:44,876 main.py:51] epoch 1447, training loss: 12840.78, average training loss: 13884.11, base loss: 21711.04
[INFO 2017-06-28 18:33:46,276 main.py:51] epoch 1448, training loss: 13881.47, average training loss: 13882.47, base loss: 21710.11
[INFO 2017-06-28 18:33:47,600 main.py:51] epoch 1449, training loss: 14797.13, average training loss: 13882.46, base loss: 21711.34
[INFO 2017-06-28 18:33:48,801 main.py:51] epoch 1450, training loss: 13802.58, average training loss: 13882.42, base loss: 21713.19
[INFO 2017-06-28 18:33:50,214 main.py:51] epoch 1451, training loss: 12462.58, average training loss: 13881.49, base loss: 21715.67
[INFO 2017-06-28 18:33:51,553 main.py:51] epoch 1452, training loss: 11850.26, average training loss: 13878.76, base loss: 21713.09
[INFO 2017-06-28 18:33:52,975 main.py:51] epoch 1453, training loss: 11710.85, average training loss: 13875.86, base loss: 21710.60
[INFO 2017-06-28 18:33:54,341 main.py:51] epoch 1454, training loss: 13248.78, average training loss: 13876.05, base loss: 21713.16
[INFO 2017-06-28 18:33:55,615 main.py:51] epoch 1455, training loss: 12952.14, average training loss: 13874.99, base loss: 21712.01
[INFO 2017-06-28 18:33:56,908 main.py:51] epoch 1456, training loss: 14330.55, average training loss: 13875.48, base loss: 21713.51
[INFO 2017-06-28 18:33:58,165 main.py:51] epoch 1457, training loss: 13314.33, average training loss: 13874.35, base loss: 21713.74
[INFO 2017-06-28 18:33:59,462 main.py:51] epoch 1458, training loss: 13137.44, average training loss: 13872.65, base loss: 21711.48
[INFO 2017-06-28 18:34:00,793 main.py:51] epoch 1459, training loss: 12985.97, average training loss: 13871.31, base loss: 21710.49
[INFO 2017-06-28 18:34:02,060 main.py:51] epoch 1460, training loss: 12466.61, average training loss: 13870.08, base loss: 21708.64
[INFO 2017-06-28 18:34:03,316 main.py:51] epoch 1461, training loss: 14508.86, average training loss: 13871.16, base loss: 21708.45
[INFO 2017-06-28 18:34:04,599 main.py:51] epoch 1462, training loss: 12731.92, average training loss: 13868.10, base loss: 21705.74
[INFO 2017-06-28 18:34:05,857 main.py:51] epoch 1463, training loss: 12173.31, average training loss: 13864.62, base loss: 21701.84
[INFO 2017-06-28 18:34:07,226 main.py:51] epoch 1464, training loss: 13274.36, average training loss: 13862.53, base loss: 21699.38
[INFO 2017-06-28 18:34:08,497 main.py:51] epoch 1465, training loss: 12612.06, average training loss: 13860.41, base loss: 21697.08
[INFO 2017-06-28 18:34:09,786 main.py:51] epoch 1466, training loss: 12251.05, average training loss: 13857.10, base loss: 21692.93
[INFO 2017-06-28 18:34:11,094 main.py:51] epoch 1467, training loss: 14446.83, average training loss: 13857.44, base loss: 21693.05
[INFO 2017-06-28 18:34:12,520 main.py:51] epoch 1468, training loss: 15213.10, average training loss: 13857.19, base loss: 21694.36
[INFO 2017-06-28 18:34:13,794 main.py:51] epoch 1469, training loss: 12400.64, average training loss: 13855.02, base loss: 21693.39
[INFO 2017-06-28 18:34:15,032 main.py:51] epoch 1470, training loss: 13457.25, average training loss: 13854.29, base loss: 21694.80
[INFO 2017-06-28 18:34:16,311 main.py:51] epoch 1471, training loss: 12496.06, average training loss: 13852.46, base loss: 21694.51
[INFO 2017-06-28 18:34:17,632 main.py:51] epoch 1472, training loss: 12758.53, average training loss: 13850.08, base loss: 21690.62
[INFO 2017-06-28 18:34:18,789 main.py:51] epoch 1473, training loss: 12625.76, average training loss: 13848.99, base loss: 21691.69
[INFO 2017-06-28 18:34:20,197 main.py:51] epoch 1474, training loss: 13325.37, average training loss: 13848.13, base loss: 21693.38
[INFO 2017-06-28 18:34:21,496 main.py:51] epoch 1475, training loss: 14743.07, average training loss: 13848.15, base loss: 21692.50
[INFO 2017-06-28 18:34:22,996 main.py:51] epoch 1476, training loss: 12160.09, average training loss: 13847.92, base loss: 21694.00
[INFO 2017-06-28 18:34:24,266 main.py:51] epoch 1477, training loss: 13754.95, average training loss: 13846.29, base loss: 21692.66
[INFO 2017-06-28 18:34:25,608 main.py:51] epoch 1478, training loss: 14165.24, average training loss: 13845.28, base loss: 21692.74
[INFO 2017-06-28 18:34:26,877 main.py:51] epoch 1479, training loss: 12505.75, average training loss: 13843.34, base loss: 21690.28
[INFO 2017-06-28 18:34:28,225 main.py:51] epoch 1480, training loss: 16487.29, average training loss: 13846.42, base loss: 21695.65
[INFO 2017-06-28 18:34:29,556 main.py:51] epoch 1481, training loss: 12199.36, average training loss: 13844.07, base loss: 21693.96
[INFO 2017-06-28 18:34:30,919 main.py:51] epoch 1482, training loss: 12125.18, average training loss: 13842.39, base loss: 21693.54
[INFO 2017-06-28 18:34:32,241 main.py:51] epoch 1483, training loss: 12309.76, average training loss: 13840.92, base loss: 21691.27
[INFO 2017-06-28 18:34:33,557 main.py:51] epoch 1484, training loss: 13494.27, average training loss: 13839.76, base loss: 21689.44
[INFO 2017-06-28 18:34:34,820 main.py:51] epoch 1485, training loss: 12971.83, average training loss: 13839.57, base loss: 21690.36
[INFO 2017-06-28 18:34:36,084 main.py:51] epoch 1486, training loss: 12841.66, average training loss: 13836.92, base loss: 21689.44
[INFO 2017-06-28 18:34:37,367 main.py:51] epoch 1487, training loss: 12343.01, average training loss: 13833.76, base loss: 21685.58
[INFO 2017-06-28 18:34:38,666 main.py:51] epoch 1488, training loss: 11548.77, average training loss: 13831.03, base loss: 21684.31
[INFO 2017-06-28 18:34:39,965 main.py:51] epoch 1489, training loss: 13480.32, average training loss: 13830.66, base loss: 21683.48
[INFO 2017-06-28 18:34:41,295 main.py:51] epoch 1490, training loss: 13177.90, average training loss: 13827.35, base loss: 21681.49
[INFO 2017-06-28 18:34:42,553 main.py:51] epoch 1491, training loss: 15188.43, average training loss: 13828.65, base loss: 21682.52
[INFO 2017-06-28 18:34:43,832 main.py:51] epoch 1492, training loss: 12243.50, average training loss: 13826.95, base loss: 21680.65
[INFO 2017-06-28 18:34:45,184 main.py:51] epoch 1493, training loss: 11952.28, average training loss: 13823.56, base loss: 21677.07
[INFO 2017-06-28 18:34:46,476 main.py:51] epoch 1494, training loss: 14266.49, average training loss: 13823.97, base loss: 21678.69
[INFO 2017-06-28 18:34:47,726 main.py:51] epoch 1495, training loss: 14278.77, average training loss: 13823.36, base loss: 21679.51
[INFO 2017-06-28 18:34:49,076 main.py:51] epoch 1496, training loss: 13640.97, average training loss: 13822.99, base loss: 21680.05
[INFO 2017-06-28 18:34:50,443 main.py:51] epoch 1497, training loss: 13998.00, average training loss: 13822.59, base loss: 21680.78
[INFO 2017-06-28 18:34:51,707 main.py:51] epoch 1498, training loss: 14969.48, average training loss: 13823.52, base loss: 21682.54
[INFO 2017-06-28 18:34:53,018 main.py:51] epoch 1499, training loss: 14091.28, average training loss: 13822.85, base loss: 21683.33
[INFO 2017-06-28 18:34:53,018 main.py:53] epoch 1499, testing
[INFO 2017-06-28 18:34:59,131 main.py:105] average testing loss: 14684.97, base loss: 21998.44
[INFO 2017-06-28 18:34:59,131 main.py:106] improve_loss: 7313.47, improve_percent: 0.33
[INFO 2017-06-28 18:34:59,131 main.py:76] current best improved percent: 0.34
[INFO 2017-06-28 18:35:00,515 main.py:51] epoch 1500, training loss: 13415.19, average training loss: 13822.12, base loss: 21683.74
[INFO 2017-06-28 18:35:01,734 main.py:51] epoch 1501, training loss: 13788.31, average training loss: 13820.89, base loss: 21682.53
[INFO 2017-06-28 18:35:03,004 main.py:51] epoch 1502, training loss: 14578.22, average training loss: 13820.40, base loss: 21682.90
[INFO 2017-06-28 18:35:04,362 main.py:51] epoch 1503, training loss: 14043.35, average training loss: 13821.56, base loss: 21687.38
[INFO 2017-06-28 18:35:05,704 main.py:51] epoch 1504, training loss: 13720.94, average training loss: 13820.14, base loss: 21688.58
[INFO 2017-06-28 18:35:07,070 main.py:51] epoch 1505, training loss: 13233.95, average training loss: 13819.18, base loss: 21688.33
[INFO 2017-06-28 18:35:08,301 main.py:51] epoch 1506, training loss: 13611.48, average training loss: 13819.29, base loss: 21689.21
[INFO 2017-06-28 18:35:09,625 main.py:51] epoch 1507, training loss: 13197.88, average training loss: 13816.83, base loss: 21687.09
[INFO 2017-06-28 18:35:10,857 main.py:51] epoch 1508, training loss: 13112.90, average training loss: 13815.86, base loss: 21684.90
[INFO 2017-06-28 18:35:12,163 main.py:51] epoch 1509, training loss: 13201.61, average training loss: 13815.58, base loss: 21686.41
[INFO 2017-06-28 18:35:13,483 main.py:51] epoch 1510, training loss: 14933.15, average training loss: 13817.93, base loss: 21690.56
[INFO 2017-06-28 18:35:14,784 main.py:51] epoch 1511, training loss: 14169.85, average training loss: 13816.85, base loss: 21690.85
[INFO 2017-06-28 18:35:15,996 main.py:51] epoch 1512, training loss: 12563.32, average training loss: 13814.79, base loss: 21690.36
[INFO 2017-06-28 18:35:17,381 main.py:51] epoch 1513, training loss: 13157.97, average training loss: 13811.64, base loss: 21688.47
[INFO 2017-06-28 18:35:18,719 main.py:51] epoch 1514, training loss: 13077.77, average training loss: 13810.86, base loss: 21691.03
[INFO 2017-06-28 18:35:20,025 main.py:51] epoch 1515, training loss: 12641.74, average training loss: 13807.93, base loss: 21688.02
[INFO 2017-06-28 18:35:21,283 main.py:51] epoch 1516, training loss: 12423.70, average training loss: 13804.97, base loss: 21685.43
[INFO 2017-06-28 18:35:22,525 main.py:51] epoch 1517, training loss: 11907.56, average training loss: 13802.08, base loss: 21682.21
[INFO 2017-06-28 18:35:23,820 main.py:51] epoch 1518, training loss: 14205.38, average training loss: 13801.82, base loss: 21682.43
[INFO 2017-06-28 18:35:25,139 main.py:51] epoch 1519, training loss: 12853.25, average training loss: 13800.33, base loss: 21680.01
[INFO 2017-06-28 18:35:26,567 main.py:51] epoch 1520, training loss: 12957.23, average training loss: 13798.44, base loss: 21678.48
[INFO 2017-06-28 18:35:27,946 main.py:51] epoch 1521, training loss: 14237.67, average training loss: 13799.58, base loss: 21680.70
[INFO 2017-06-28 18:35:29,215 main.py:51] epoch 1522, training loss: 12300.15, average training loss: 13796.94, base loss: 21678.45
[INFO 2017-06-28 18:35:30,648 main.py:51] epoch 1523, training loss: 13508.42, average training loss: 13795.21, base loss: 21678.24
[INFO 2017-06-28 18:35:31,932 main.py:51] epoch 1524, training loss: 11715.29, average training loss: 13793.06, base loss: 21677.77
[INFO 2017-06-28 18:35:33,330 main.py:51] epoch 1525, training loss: 13002.46, average training loss: 13792.66, base loss: 21679.39
[INFO 2017-06-28 18:35:34,697 main.py:51] epoch 1526, training loss: 15413.38, average training loss: 13794.97, base loss: 21683.16
[INFO 2017-06-28 18:35:35,919 main.py:51] epoch 1527, training loss: 13984.71, average training loss: 13794.81, base loss: 21684.30
[INFO 2017-06-28 18:35:37,264 main.py:51] epoch 1528, training loss: 14562.09, average training loss: 13793.70, base loss: 21682.33
[INFO 2017-06-28 18:35:38,548 main.py:51] epoch 1529, training loss: 13794.29, average training loss: 13792.93, base loss: 21680.80
[INFO 2017-06-28 18:35:39,901 main.py:51] epoch 1530, training loss: 13089.25, average training loss: 13792.64, base loss: 21681.78
[INFO 2017-06-28 18:35:41,125 main.py:51] epoch 1531, training loss: 12016.45, average training loss: 13790.29, base loss: 21679.64
[INFO 2017-06-28 18:35:42,299 main.py:51] epoch 1532, training loss: 15729.89, average training loss: 13792.46, base loss: 21681.24
[INFO 2017-06-28 18:35:43,639 main.py:51] epoch 1533, training loss: 14268.12, average training loss: 13789.67, base loss: 21678.12
[INFO 2017-06-28 18:35:44,902 main.py:51] epoch 1534, training loss: 11885.56, average training loss: 13786.07, base loss: 21674.47
[INFO 2017-06-28 18:35:46,179 main.py:51] epoch 1535, training loss: 12706.11, average training loss: 13783.87, base loss: 21672.34
[INFO 2017-06-28 18:35:47,459 main.py:51] epoch 1536, training loss: 13512.28, average training loss: 13781.35, base loss: 21671.67
[INFO 2017-06-28 18:35:48,776 main.py:51] epoch 1537, training loss: 12409.93, average training loss: 13779.15, base loss: 21667.54
[INFO 2017-06-28 18:35:50,148 main.py:51] epoch 1538, training loss: 13466.09, average training loss: 13777.64, base loss: 21665.75
[INFO 2017-06-28 18:35:51,454 main.py:51] epoch 1539, training loss: 13688.63, average training loss: 13776.92, base loss: 21665.64
[INFO 2017-06-28 18:35:52,686 main.py:51] epoch 1540, training loss: 13861.94, average training loss: 13777.16, base loss: 21668.83
[INFO 2017-06-28 18:35:53,983 main.py:51] epoch 1541, training loss: 13891.13, average training loss: 13777.35, base loss: 21668.85
[INFO 2017-06-28 18:35:55,240 main.py:51] epoch 1542, training loss: 12764.09, average training loss: 13775.53, base loss: 21667.89
[INFO 2017-06-28 18:35:56,543 main.py:51] epoch 1543, training loss: 14766.26, average training loss: 13775.51, base loss: 21670.09
[INFO 2017-06-28 18:35:57,925 main.py:51] epoch 1544, training loss: 13853.15, average training loss: 13773.40, base loss: 21667.02
[INFO 2017-06-28 18:35:59,226 main.py:51] epoch 1545, training loss: 13638.50, average training loss: 13771.90, base loss: 21667.51
[INFO 2017-06-28 18:36:00,517 main.py:51] epoch 1546, training loss: 13274.39, average training loss: 13770.78, base loss: 21668.41
[INFO 2017-06-28 18:36:01,799 main.py:51] epoch 1547, training loss: 14120.50, average training loss: 13769.21, base loss: 21666.43
[INFO 2017-06-28 18:36:03,085 main.py:51] epoch 1548, training loss: 13750.78, average training loss: 13769.30, base loss: 21668.20
[INFO 2017-06-28 18:36:04,400 main.py:51] epoch 1549, training loss: 13280.59, average training loss: 13766.92, base loss: 21666.96
[INFO 2017-06-28 18:36:05,776 main.py:51] epoch 1550, training loss: 13761.35, average training loss: 13767.48, base loss: 21668.37
[INFO 2017-06-28 18:36:07,188 main.py:51] epoch 1551, training loss: 14036.84, average training loss: 13767.32, base loss: 21668.91
[INFO 2017-06-28 18:36:08,472 main.py:51] epoch 1552, training loss: 16405.21, average training loss: 13769.37, base loss: 21673.20
[INFO 2017-06-28 18:36:09,710 main.py:51] epoch 1553, training loss: 13217.44, average training loss: 13768.88, base loss: 21674.66
[INFO 2017-06-28 18:36:10,968 main.py:51] epoch 1554, training loss: 13646.17, average training loss: 13769.18, base loss: 21675.88
[INFO 2017-06-28 18:36:12,236 main.py:51] epoch 1555, training loss: 12925.78, average training loss: 13766.49, base loss: 21673.00
[INFO 2017-06-28 18:36:13,640 main.py:51] epoch 1556, training loss: 12014.36, average training loss: 13765.04, base loss: 21670.80
[INFO 2017-06-28 18:36:14,912 main.py:51] epoch 1557, training loss: 12854.20, average training loss: 13764.44, base loss: 21670.29
[INFO 2017-06-28 18:36:16,236 main.py:51] epoch 1558, training loss: 12481.62, average training loss: 13761.08, base loss: 21666.20
[INFO 2017-06-28 18:36:17,505 main.py:51] epoch 1559, training loss: 12819.76, average training loss: 13759.68, base loss: 21663.98
[INFO 2017-06-28 18:36:18,818 main.py:51] epoch 1560, training loss: 11396.01, average training loss: 13756.53, base loss: 21659.36
[INFO 2017-06-28 18:36:20,216 main.py:51] epoch 1561, training loss: 15759.09, average training loss: 13758.50, base loss: 21660.55
[INFO 2017-06-28 18:36:21,497 main.py:51] epoch 1562, training loss: 12059.79, average training loss: 13757.05, base loss: 21658.15
[INFO 2017-06-28 18:36:22,842 main.py:51] epoch 1563, training loss: 13125.77, average training loss: 13756.52, base loss: 21657.26
[INFO 2017-06-28 18:36:24,232 main.py:51] epoch 1564, training loss: 13020.15, average training loss: 13754.73, base loss: 21656.61
[INFO 2017-06-28 18:36:25,467 main.py:51] epoch 1565, training loss: 13746.37, average training loss: 13753.78, base loss: 21656.21
[INFO 2017-06-28 18:36:26,800 main.py:51] epoch 1566, training loss: 13590.48, average training loss: 13752.60, base loss: 21655.41
[INFO 2017-06-28 18:36:28,100 main.py:51] epoch 1567, training loss: 14717.47, average training loss: 13753.15, base loss: 21657.40
[INFO 2017-06-28 18:36:29,458 main.py:51] epoch 1568, training loss: 13549.15, average training loss: 13753.01, base loss: 21654.49
[INFO 2017-06-28 18:36:30,817 main.py:51] epoch 1569, training loss: 15758.55, average training loss: 13755.47, base loss: 21657.76
[INFO 2017-06-28 18:36:32,111 main.py:51] epoch 1570, training loss: 13985.32, average training loss: 13755.64, base loss: 21660.36
[INFO 2017-06-28 18:36:33,492 main.py:51] epoch 1571, training loss: 12315.66, average training loss: 13752.51, base loss: 21658.20
[INFO 2017-06-28 18:36:34,830 main.py:51] epoch 1572, training loss: 12844.03, average training loss: 13751.25, base loss: 21660.26
[INFO 2017-06-28 18:36:36,103 main.py:51] epoch 1573, training loss: 13035.70, average training loss: 13749.81, base loss: 21659.52
[INFO 2017-06-28 18:36:37,379 main.py:51] epoch 1574, training loss: 13690.75, average training loss: 13749.67, base loss: 21660.46
[INFO 2017-06-28 18:36:38,625 main.py:51] epoch 1575, training loss: 12048.40, average training loss: 13748.76, base loss: 21660.28
[INFO 2017-06-28 18:36:39,843 main.py:51] epoch 1576, training loss: 14721.17, average training loss: 13750.47, base loss: 21664.45
[INFO 2017-06-28 18:36:41,155 main.py:51] epoch 1577, training loss: 14228.00, average training loss: 13751.47, base loss: 21667.35
[INFO 2017-06-28 18:36:42,416 main.py:51] epoch 1578, training loss: 12340.25, average training loss: 13750.04, base loss: 21665.74
[INFO 2017-06-28 18:36:43,724 main.py:51] epoch 1579, training loss: 12762.74, average training loss: 13748.63, base loss: 21665.59
[INFO 2017-06-28 18:36:45,104 main.py:51] epoch 1580, training loss: 12616.01, average training loss: 13746.74, base loss: 21664.83
[INFO 2017-06-28 18:36:46,459 main.py:51] epoch 1581, training loss: 14446.03, average training loss: 13747.15, base loss: 21666.98
[INFO 2017-06-28 18:36:47,808 main.py:51] epoch 1582, training loss: 13920.98, average training loss: 13747.07, base loss: 21669.16
[INFO 2017-06-28 18:36:49,106 main.py:51] epoch 1583, training loss: 12298.57, average training loss: 13744.59, base loss: 21665.82
[INFO 2017-06-28 18:36:50,365 main.py:51] epoch 1584, training loss: 14487.30, average training loss: 13745.99, base loss: 21669.78
[INFO 2017-06-28 18:36:51,631 main.py:51] epoch 1585, training loss: 11394.80, average training loss: 13743.57, base loss: 21670.89
[INFO 2017-06-28 18:36:52,988 main.py:51] epoch 1586, training loss: 12223.07, average training loss: 13740.39, base loss: 21666.49
[INFO 2017-06-28 18:36:54,273 main.py:51] epoch 1587, training loss: 12726.72, average training loss: 13737.72, base loss: 21664.51
[INFO 2017-06-28 18:36:55,610 main.py:51] epoch 1588, training loss: 12789.25, average training loss: 13735.57, base loss: 21663.42
[INFO 2017-06-28 18:36:57,030 main.py:51] epoch 1589, training loss: 13356.36, average training loss: 13734.75, base loss: 21663.29
[INFO 2017-06-28 18:36:58,342 main.py:51] epoch 1590, training loss: 12831.99, average training loss: 13732.59, base loss: 21662.77
[INFO 2017-06-28 18:36:59,504 main.py:51] epoch 1591, training loss: 14078.46, average training loss: 13732.55, base loss: 21662.39
[INFO 2017-06-28 18:37:00,808 main.py:51] epoch 1592, training loss: 13790.73, average training loss: 13732.70, base loss: 21663.46
[INFO 2017-06-28 18:37:02,169 main.py:51] epoch 1593, training loss: 13689.89, average training loss: 13730.97, base loss: 21661.78
[INFO 2017-06-28 18:37:03,564 main.py:51] epoch 1594, training loss: 13281.10, average training loss: 13727.71, base loss: 21660.83
[INFO 2017-06-28 18:37:04,872 main.py:51] epoch 1595, training loss: 14029.83, average training loss: 13727.79, base loss: 21662.56
[INFO 2017-06-28 18:37:06,107 main.py:51] epoch 1596, training loss: 12991.60, average training loss: 13726.60, base loss: 21661.96
[INFO 2017-06-28 18:37:07,678 main.py:51] epoch 1597, training loss: 15328.90, average training loss: 13727.47, base loss: 21663.95
[INFO 2017-06-28 18:37:09,047 main.py:51] epoch 1598, training loss: 12629.31, average training loss: 13726.54, base loss: 21665.92
[INFO 2017-06-28 18:37:10,361 main.py:51] epoch 1599, training loss: 14184.19, average training loss: 13728.47, base loss: 21668.86
[INFO 2017-06-28 18:37:10,361 main.py:53] epoch 1599, testing
[INFO 2017-06-28 18:37:16,260 main.py:105] average testing loss: 13859.26, base loss: 21700.73
[INFO 2017-06-28 18:37:16,260 main.py:106] improve_loss: 7841.46, improve_percent: 0.36
[INFO 2017-06-28 18:37:16,261 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 18:37:16,274 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:37:17,574 main.py:51] epoch 1600, training loss: 13296.20, average training loss: 13728.95, base loss: 21670.72
[INFO 2017-06-28 18:37:18,875 main.py:51] epoch 1601, training loss: 12734.79, average training loss: 13728.32, base loss: 21670.85
[INFO 2017-06-28 18:37:20,232 main.py:51] epoch 1602, training loss: 14223.47, average training loss: 13728.87, base loss: 21671.67
[INFO 2017-06-28 18:37:21,473 main.py:51] epoch 1603, training loss: 14268.94, average training loss: 13728.72, base loss: 21673.94
[INFO 2017-06-28 18:37:22,812 main.py:51] epoch 1604, training loss: 14174.96, average training loss: 13729.28, base loss: 21675.59
[INFO 2017-06-28 18:37:24,163 main.py:51] epoch 1605, training loss: 12917.96, average training loss: 13727.50, base loss: 21674.83
[INFO 2017-06-28 18:37:25,522 main.py:51] epoch 1606, training loss: 12362.81, average training loss: 13727.53, base loss: 21676.72
[INFO 2017-06-28 18:37:26,955 main.py:51] epoch 1607, training loss: 13709.80, average training loss: 13726.46, base loss: 21674.00
[INFO 2017-06-28 18:37:28,371 main.py:51] epoch 1608, training loss: 13379.17, average training loss: 13725.98, base loss: 21672.64
[INFO 2017-06-28 18:37:29,747 main.py:51] epoch 1609, training loss: 12961.02, average training loss: 13725.05, base loss: 21673.46
[INFO 2017-06-28 18:37:31,074 main.py:51] epoch 1610, training loss: 13528.20, average training loss: 13725.34, base loss: 21676.10
[INFO 2017-06-28 18:37:32,470 main.py:51] epoch 1611, training loss: 13862.11, average training loss: 13725.51, base loss: 21677.34
[INFO 2017-06-28 18:37:33,695 main.py:51] epoch 1612, training loss: 13576.39, average training loss: 13726.02, base loss: 21680.23
[INFO 2017-06-28 18:37:35,016 main.py:51] epoch 1613, training loss: 14407.61, average training loss: 13727.45, base loss: 21682.79
[INFO 2017-06-28 18:37:36,364 main.py:51] epoch 1614, training loss: 14391.66, average training loss: 13728.24, base loss: 21682.66
[INFO 2017-06-28 18:37:37,646 main.py:51] epoch 1615, training loss: 13410.69, average training loss: 13726.32, base loss: 21682.82
[INFO 2017-06-28 18:37:38,958 main.py:51] epoch 1616, training loss: 12512.94, average training loss: 13725.20, base loss: 21680.46
[INFO 2017-06-28 18:37:40,193 main.py:51] epoch 1617, training loss: 14487.31, average training loss: 13727.20, base loss: 21683.01
[INFO 2017-06-28 18:37:41,449 main.py:51] epoch 1618, training loss: 13254.95, average training loss: 13726.58, base loss: 21682.74
[INFO 2017-06-28 18:37:42,705 main.py:51] epoch 1619, training loss: 12742.86, average training loss: 13725.43, base loss: 21681.54
[INFO 2017-06-28 18:37:44,043 main.py:51] epoch 1620, training loss: 13148.84, average training loss: 13725.33, base loss: 21684.00
[INFO 2017-06-28 18:37:45,277 main.py:51] epoch 1621, training loss: 15049.69, average training loss: 13724.98, base loss: 21684.45
[INFO 2017-06-28 18:37:46,621 main.py:51] epoch 1622, training loss: 14637.91, average training loss: 13727.26, base loss: 21688.61
[INFO 2017-06-28 18:37:48,009 main.py:51] epoch 1623, training loss: 13244.63, average training loss: 13726.39, base loss: 21687.76
[INFO 2017-06-28 18:37:49,292 main.py:51] epoch 1624, training loss: 13005.63, average training loss: 13726.20, base loss: 21689.85
[INFO 2017-06-28 18:37:50,651 main.py:51] epoch 1625, training loss: 12264.70, average training loss: 13725.87, base loss: 21690.18
[INFO 2017-06-28 18:37:51,956 main.py:51] epoch 1626, training loss: 13793.92, average training loss: 13724.61, base loss: 21689.16
[INFO 2017-06-28 18:37:53,232 main.py:51] epoch 1627, training loss: 14302.29, average training loss: 13725.27, base loss: 21689.59
[INFO 2017-06-28 18:37:54,619 main.py:51] epoch 1628, training loss: 13286.08, average training loss: 13724.13, base loss: 21688.59
[INFO 2017-06-28 18:37:55,893 main.py:51] epoch 1629, training loss: 12372.45, average training loss: 13723.71, base loss: 21686.26
[INFO 2017-06-28 18:37:57,236 main.py:51] epoch 1630, training loss: 16009.22, average training loss: 13726.46, base loss: 21689.13
[INFO 2017-06-28 18:37:58,462 main.py:51] epoch 1631, training loss: 12797.29, average training loss: 13725.54, base loss: 21688.05
[INFO 2017-06-28 18:37:59,871 main.py:51] epoch 1632, training loss: 13637.39, average training loss: 13725.23, base loss: 21689.59
[INFO 2017-06-28 18:38:01,230 main.py:51] epoch 1633, training loss: 12649.58, average training loss: 13724.85, base loss: 21687.37
[INFO 2017-06-28 18:38:02,560 main.py:51] epoch 1634, training loss: 14378.83, average training loss: 13725.82, base loss: 21689.69
[INFO 2017-06-28 18:38:03,843 main.py:51] epoch 1635, training loss: 12788.43, average training loss: 13723.96, base loss: 21688.65
[INFO 2017-06-28 18:38:05,203 main.py:51] epoch 1636, training loss: 13743.27, average training loss: 13722.83, base loss: 21688.47
[INFO 2017-06-28 18:38:06,517 main.py:51] epoch 1637, training loss: 15272.14, average training loss: 13723.27, base loss: 21688.55
[INFO 2017-06-28 18:38:07,780 main.py:51] epoch 1638, training loss: 13184.13, average training loss: 13721.87, base loss: 21690.89
[INFO 2017-06-28 18:38:09,123 main.py:51] epoch 1639, training loss: 15105.77, average training loss: 13721.94, base loss: 21688.83
[INFO 2017-06-28 18:38:10,491 main.py:51] epoch 1640, training loss: 14882.87, average training loss: 13721.92, base loss: 21688.84
[INFO 2017-06-28 18:38:11,811 main.py:51] epoch 1641, training loss: 13755.84, average training loss: 13720.05, base loss: 21687.00
[INFO 2017-06-28 18:38:13,182 main.py:51] epoch 1642, training loss: 12627.74, average training loss: 13719.19, base loss: 21685.92
[INFO 2017-06-28 18:38:14,529 main.py:51] epoch 1643, training loss: 13982.34, average training loss: 13720.60, base loss: 21689.32
[INFO 2017-06-28 18:38:15,812 main.py:51] epoch 1644, training loss: 13197.16, average training loss: 13718.10, base loss: 21685.83
[INFO 2017-06-28 18:38:17,064 main.py:51] epoch 1645, training loss: 13198.90, average training loss: 13716.20, base loss: 21685.27
[INFO 2017-06-28 18:38:18,406 main.py:51] epoch 1646, training loss: 12809.77, average training loss: 13713.45, base loss: 21684.10
[INFO 2017-06-28 18:38:19,614 main.py:51] epoch 1647, training loss: 14447.58, average training loss: 13713.85, base loss: 21686.40
[INFO 2017-06-28 18:38:20,925 main.py:51] epoch 1648, training loss: 13462.30, average training loss: 13713.65, base loss: 21686.12
[INFO 2017-06-28 18:38:22,169 main.py:51] epoch 1649, training loss: 13138.65, average training loss: 13712.27, base loss: 21685.02
[INFO 2017-06-28 18:38:23,477 main.py:51] epoch 1650, training loss: 12363.42, average training loss: 13709.98, base loss: 21684.93
[INFO 2017-06-28 18:38:24,804 main.py:51] epoch 1651, training loss: 13964.98, average training loss: 13709.76, base loss: 21687.20
[INFO 2017-06-28 18:38:26,192 main.py:51] epoch 1652, training loss: 12508.52, average training loss: 13704.91, base loss: 21685.61
[INFO 2017-06-28 18:38:27,478 main.py:51] epoch 1653, training loss: 13642.08, average training loss: 13704.94, base loss: 21684.66
[INFO 2017-06-28 18:38:28,865 main.py:51] epoch 1654, training loss: 12334.88, average training loss: 13702.82, base loss: 21683.85
[INFO 2017-06-28 18:38:30,146 main.py:51] epoch 1655, training loss: 12734.74, average training loss: 13700.41, base loss: 21681.19
[INFO 2017-06-28 18:38:31,440 main.py:51] epoch 1656, training loss: 12839.41, average training loss: 13697.93, base loss: 21677.92
[INFO 2017-06-28 18:38:32,729 main.py:51] epoch 1657, training loss: 12823.19, average training loss: 13695.47, base loss: 21674.03
[INFO 2017-06-28 18:38:34,002 main.py:51] epoch 1658, training loss: 13117.87, average training loss: 13693.51, base loss: 21670.87
[INFO 2017-06-28 18:38:35,323 main.py:51] epoch 1659, training loss: 13934.93, average training loss: 13693.27, base loss: 21672.09
[INFO 2017-06-28 18:38:36,618 main.py:51] epoch 1660, training loss: 12305.75, average training loss: 13691.34, base loss: 21669.50
[INFO 2017-06-28 18:38:37,958 main.py:51] epoch 1661, training loss: 12619.19, average training loss: 13688.90, base loss: 21666.66
[INFO 2017-06-28 18:38:39,379 main.py:51] epoch 1662, training loss: 13148.54, average training loss: 13688.69, base loss: 21669.14
[INFO 2017-06-28 18:38:40,752 main.py:51] epoch 1663, training loss: 12997.33, average training loss: 13686.93, base loss: 21668.17
[INFO 2017-06-28 18:38:42,077 main.py:51] epoch 1664, training loss: 13259.58, average training loss: 13686.19, base loss: 21668.07
[INFO 2017-06-28 18:38:43,314 main.py:51] epoch 1665, training loss: 13415.27, average training loss: 13686.45, base loss: 21670.05
[INFO 2017-06-28 18:38:44,677 main.py:51] epoch 1666, training loss: 13487.70, average training loss: 13685.33, base loss: 21670.93
[INFO 2017-06-28 18:38:46,036 main.py:51] epoch 1667, training loss: 13986.37, average training loss: 13685.60, base loss: 21671.63
[INFO 2017-06-28 18:38:47,337 main.py:51] epoch 1668, training loss: 14817.53, average training loss: 13684.80, base loss: 21671.40
[INFO 2017-06-28 18:38:48,671 main.py:51] epoch 1669, training loss: 12386.58, average training loss: 13682.45, base loss: 21671.20
[INFO 2017-06-28 18:38:49,972 main.py:51] epoch 1670, training loss: 13536.27, average training loss: 13681.92, base loss: 21671.28
[INFO 2017-06-28 18:38:51,242 main.py:51] epoch 1671, training loss: 13400.62, average training loss: 13681.77, base loss: 21671.82
[INFO 2017-06-28 18:38:52,551 main.py:51] epoch 1672, training loss: 13295.21, average training loss: 13680.91, base loss: 21670.29
[INFO 2017-06-28 18:38:53,750 main.py:51] epoch 1673, training loss: 13558.08, average training loss: 13681.26, base loss: 21671.91
[INFO 2017-06-28 18:38:55,051 main.py:51] epoch 1674, training loss: 13385.38, average training loss: 13681.21, base loss: 21673.16
[INFO 2017-06-28 18:38:56,321 main.py:51] epoch 1675, training loss: 13917.88, average training loss: 13680.11, base loss: 21673.92
[INFO 2017-06-28 18:38:57,609 main.py:51] epoch 1676, training loss: 12747.44, average training loss: 13679.31, base loss: 21672.76
[INFO 2017-06-28 18:38:58,973 main.py:51] epoch 1677, training loss: 13717.63, average training loss: 13679.82, base loss: 21672.32
[INFO 2017-06-28 18:39:00,319 main.py:51] epoch 1678, training loss: 13397.44, average training loss: 13678.99, base loss: 21670.13
[INFO 2017-06-28 18:39:01,735 main.py:51] epoch 1679, training loss: 11849.41, average training loss: 13677.50, base loss: 21667.80
[INFO 2017-06-28 18:39:03,102 main.py:51] epoch 1680, training loss: 13133.68, average training loss: 13675.20, base loss: 21665.64
[INFO 2017-06-28 18:39:04,424 main.py:51] epoch 1681, training loss: 13198.92, average training loss: 13673.30, base loss: 21665.53
[INFO 2017-06-28 18:39:05,786 main.py:51] epoch 1682, training loss: 12242.05, average training loss: 13672.17, base loss: 21667.09
[INFO 2017-06-28 18:39:07,120 main.py:51] epoch 1683, training loss: 11330.53, average training loss: 13668.85, base loss: 21662.77
[INFO 2017-06-28 18:39:08,368 main.py:51] epoch 1684, training loss: 13691.01, average training loss: 13669.67, base loss: 21666.01
[INFO 2017-06-28 18:39:09,618 main.py:51] epoch 1685, training loss: 12160.30, average training loss: 13669.42, base loss: 21667.32
[INFO 2017-06-28 18:39:10,834 main.py:51] epoch 1686, training loss: 12336.89, average training loss: 13667.49, base loss: 21664.80
[INFO 2017-06-28 18:39:12,240 main.py:51] epoch 1687, training loss: 13860.82, average training loss: 13667.95, base loss: 21666.35
[INFO 2017-06-28 18:39:13,568 main.py:51] epoch 1688, training loss: 13337.96, average training loss: 13666.38, base loss: 21664.62
[INFO 2017-06-28 18:39:14,871 main.py:51] epoch 1689, training loss: 12856.09, average training loss: 13662.83, base loss: 21660.51
[INFO 2017-06-28 18:39:16,062 main.py:51] epoch 1690, training loss: 11949.76, average training loss: 13661.25, base loss: 21659.25
[INFO 2017-06-28 18:39:17,371 main.py:51] epoch 1691, training loss: 13878.45, average training loss: 13660.92, base loss: 21658.11
[INFO 2017-06-28 18:39:18,646 main.py:51] epoch 1692, training loss: 12706.21, average training loss: 13659.47, base loss: 21656.32
[INFO 2017-06-28 18:39:20,022 main.py:51] epoch 1693, training loss: 13811.59, average training loss: 13659.99, base loss: 21657.88
[INFO 2017-06-28 18:39:21,480 main.py:51] epoch 1694, training loss: 12696.45, average training loss: 13659.78, base loss: 21658.11
[INFO 2017-06-28 18:39:22,809 main.py:51] epoch 1695, training loss: 13261.98, average training loss: 13659.65, base loss: 21659.18
[INFO 2017-06-28 18:39:24,143 main.py:51] epoch 1696, training loss: 13312.45, average training loss: 13659.71, base loss: 21660.61
[INFO 2017-06-28 18:39:25,434 main.py:51] epoch 1697, training loss: 13224.78, average training loss: 13659.22, base loss: 21660.33
[INFO 2017-06-28 18:39:26,704 main.py:51] epoch 1698, training loss: 13383.05, average training loss: 13658.06, base loss: 21660.47
[INFO 2017-06-28 18:39:28,028 main.py:51] epoch 1699, training loss: 12849.87, average training loss: 13654.65, base loss: 21657.08
[INFO 2017-06-28 18:39:28,028 main.py:53] epoch 1699, testing
[INFO 2017-06-28 18:39:34,157 main.py:105] average testing loss: 14724.44, base loss: 21973.16
[INFO 2017-06-28 18:39:34,157 main.py:106] improve_loss: 7248.72, improve_percent: 0.33
[INFO 2017-06-28 18:39:34,158 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:39:35,399 main.py:51] epoch 1700, training loss: 11622.94, average training loss: 13652.78, base loss: 21656.16
[INFO 2017-06-28 18:39:36,691 main.py:51] epoch 1701, training loss: 12914.76, average training loss: 13652.30, base loss: 21657.00
[INFO 2017-06-28 18:39:38,049 main.py:51] epoch 1702, training loss: 12410.93, average training loss: 13652.22, base loss: 21656.84
[INFO 2017-06-28 18:39:39,407 main.py:51] epoch 1703, training loss: 12841.25, average training loss: 13652.04, base loss: 21656.27
[INFO 2017-06-28 18:39:40,795 main.py:51] epoch 1704, training loss: 13019.40, average training loss: 13650.73, base loss: 21656.42
[INFO 2017-06-28 18:39:42,100 main.py:51] epoch 1705, training loss: 14594.24, average training loss: 13652.53, base loss: 21659.52
[INFO 2017-06-28 18:39:43,475 main.py:51] epoch 1706, training loss: 13398.21, average training loss: 13651.24, base loss: 21659.87
[INFO 2017-06-28 18:39:44,893 main.py:51] epoch 1707, training loss: 13404.99, average training loss: 13648.90, base loss: 21656.03
[INFO 2017-06-28 18:39:46,220 main.py:51] epoch 1708, training loss: 13358.01, average training loss: 13648.77, base loss: 21655.30
[INFO 2017-06-28 18:39:47,523 main.py:51] epoch 1709, training loss: 13805.01, average training loss: 13649.61, base loss: 21655.34
[INFO 2017-06-28 18:39:48,842 main.py:51] epoch 1710, training loss: 14191.98, average training loss: 13650.08, base loss: 21657.05
[INFO 2017-06-28 18:39:50,142 main.py:51] epoch 1711, training loss: 12671.59, average training loss: 13649.50, base loss: 21657.71
[INFO 2017-06-28 18:39:51,347 main.py:51] epoch 1712, training loss: 15583.48, average training loss: 13650.65, base loss: 21659.88
[INFO 2017-06-28 18:39:52,702 main.py:51] epoch 1713, training loss: 13013.22, average training loss: 13649.55, base loss: 21658.70
[INFO 2017-06-28 18:39:53,933 main.py:51] epoch 1714, training loss: 12748.16, average training loss: 13648.36, base loss: 21657.32
[INFO 2017-06-28 18:39:55,232 main.py:51] epoch 1715, training loss: 14929.08, average training loss: 13647.83, base loss: 21660.47
[INFO 2017-06-28 18:39:56,440 main.py:51] epoch 1716, training loss: 12472.20, average training loss: 13646.82, base loss: 21659.56
[INFO 2017-06-28 18:39:57,681 main.py:51] epoch 1717, training loss: 12869.94, average training loss: 13645.93, base loss: 21657.67
[INFO 2017-06-28 18:39:59,031 main.py:51] epoch 1718, training loss: 12219.25, average training loss: 13643.22, base loss: 21655.11
[INFO 2017-06-28 18:40:00,366 main.py:51] epoch 1719, training loss: 13784.30, average training loss: 13642.95, base loss: 21657.24
[INFO 2017-06-28 18:40:01,635 main.py:51] epoch 1720, training loss: 13288.30, average training loss: 13643.26, base loss: 21658.48
[INFO 2017-06-28 18:40:02,924 main.py:51] epoch 1721, training loss: 14137.21, average training loss: 13644.16, base loss: 21659.90
[INFO 2017-06-28 18:40:04,290 main.py:51] epoch 1722, training loss: 13638.02, average training loss: 13643.31, base loss: 21658.81
[INFO 2017-06-28 18:40:05,613 main.py:51] epoch 1723, training loss: 12402.64, average training loss: 13642.08, base loss: 21659.00
[INFO 2017-06-28 18:40:06,916 main.py:51] epoch 1724, training loss: 13059.67, average training loss: 13640.15, base loss: 21655.87
[INFO 2017-06-28 18:40:08,168 main.py:51] epoch 1725, training loss: 13782.06, average training loss: 13640.31, base loss: 21654.57
[INFO 2017-06-28 18:40:09,578 main.py:51] epoch 1726, training loss: 14120.13, average training loss: 13639.36, base loss: 21653.10
[INFO 2017-06-28 18:40:10,865 main.py:51] epoch 1727, training loss: 13552.02, average training loss: 13638.85, base loss: 21654.27
[INFO 2017-06-28 18:40:12,228 main.py:51] epoch 1728, training loss: 13288.03, average training loss: 13638.63, base loss: 21654.22
[INFO 2017-06-28 18:40:13,574 main.py:51] epoch 1729, training loss: 12887.56, average training loss: 13637.43, base loss: 21654.16
[INFO 2017-06-28 18:40:14,872 main.py:51] epoch 1730, training loss: 13781.75, average training loss: 13636.33, base loss: 21654.89
[INFO 2017-06-28 18:40:16,150 main.py:51] epoch 1731, training loss: 13366.80, average training loss: 13635.58, base loss: 21656.48
[INFO 2017-06-28 18:40:17,443 main.py:51] epoch 1732, training loss: 14568.39, average training loss: 13636.54, base loss: 21659.67
[INFO 2017-06-28 18:40:18,739 main.py:51] epoch 1733, training loss: 11917.73, average training loss: 13633.87, base loss: 21656.70
[INFO 2017-06-28 18:40:20,060 main.py:51] epoch 1734, training loss: 15174.81, average training loss: 13635.57, base loss: 21660.85
[INFO 2017-06-28 18:40:21,405 main.py:51] epoch 1735, training loss: 13529.21, average training loss: 13636.79, base loss: 21662.51
[INFO 2017-06-28 18:40:22,698 main.py:51] epoch 1736, training loss: 12116.26, average training loss: 13635.73, base loss: 21664.03
[INFO 2017-06-28 18:40:24,005 main.py:51] epoch 1737, training loss: 13500.19, average training loss: 13636.18, base loss: 21664.19
[INFO 2017-06-28 18:40:25,381 main.py:51] epoch 1738, training loss: 13433.81, average training loss: 13635.74, base loss: 21665.94
[INFO 2017-06-28 18:40:26,708 main.py:51] epoch 1739, training loss: 12774.02, average training loss: 13636.20, base loss: 21667.32
[INFO 2017-06-28 18:40:27,989 main.py:51] epoch 1740, training loss: 10815.51, average training loss: 13633.13, base loss: 21663.80
[INFO 2017-06-28 18:40:29,313 main.py:51] epoch 1741, training loss: 12981.69, average training loss: 13632.96, base loss: 21665.91
[INFO 2017-06-28 18:40:30,633 main.py:51] epoch 1742, training loss: 13306.45, average training loss: 13631.07, base loss: 21664.79
[INFO 2017-06-28 18:40:31,926 main.py:51] epoch 1743, training loss: 13572.29, average training loss: 13631.51, base loss: 21667.59
[INFO 2017-06-28 18:40:33,375 main.py:51] epoch 1744, training loss: 13449.51, average training loss: 13631.13, base loss: 21667.97
[INFO 2017-06-28 18:40:34,587 main.py:51] epoch 1745, training loss: 13412.04, average training loss: 13631.06, base loss: 21670.30
[INFO 2017-06-28 18:40:35,966 main.py:51] epoch 1746, training loss: 13512.12, average training loss: 13630.83, base loss: 21670.07
[INFO 2017-06-28 18:40:37,309 main.py:51] epoch 1747, training loss: 12325.78, average training loss: 13629.43, base loss: 21668.34
[INFO 2017-06-28 18:40:38,616 main.py:51] epoch 1748, training loss: 12868.31, average training loss: 13629.84, base loss: 21669.24
[INFO 2017-06-28 18:40:39,873 main.py:51] epoch 1749, training loss: 13357.87, average training loss: 13629.54, base loss: 21669.21
[INFO 2017-06-28 18:40:41,149 main.py:51] epoch 1750, training loss: 13159.64, average training loss: 13628.93, base loss: 21668.92
[INFO 2017-06-28 18:40:42,517 main.py:51] epoch 1751, training loss: 13059.24, average training loss: 13627.54, base loss: 21665.49
[INFO 2017-06-28 18:40:43,854 main.py:51] epoch 1752, training loss: 13355.12, average training loss: 13627.31, base loss: 21667.32
[INFO 2017-06-28 18:40:45,244 main.py:51] epoch 1753, training loss: 12693.31, average training loss: 13624.66, base loss: 21664.91
[INFO 2017-06-28 18:40:46,575 main.py:51] epoch 1754, training loss: 13742.13, average training loss: 13623.73, base loss: 21664.86
[INFO 2017-06-28 18:40:47,945 main.py:51] epoch 1755, training loss: 12965.27, average training loss: 13622.15, base loss: 21664.26
[INFO 2017-06-28 18:40:49,313 main.py:51] epoch 1756, training loss: 12961.51, average training loss: 13621.54, base loss: 21664.09
[INFO 2017-06-28 18:40:50,720 main.py:51] epoch 1757, training loss: 12672.74, average training loss: 13620.62, base loss: 21664.57
[INFO 2017-06-28 18:40:52,016 main.py:51] epoch 1758, training loss: 12963.72, average training loss: 13618.34, base loss: 21663.91
[INFO 2017-06-28 18:40:53,228 main.py:51] epoch 1759, training loss: 12963.94, average training loss: 13616.90, base loss: 21664.16
[INFO 2017-06-28 18:40:54,596 main.py:51] epoch 1760, training loss: 13179.83, average training loss: 13614.50, base loss: 21662.59
[INFO 2017-06-28 18:40:55,834 main.py:51] epoch 1761, training loss: 12802.49, average training loss: 13612.59, base loss: 21660.20
[INFO 2017-06-28 18:40:57,155 main.py:51] epoch 1762, training loss: 13171.43, average training loss: 13610.86, base loss: 21657.21
[INFO 2017-06-28 18:40:58,454 main.py:51] epoch 1763, training loss: 13771.02, average training loss: 13611.06, base loss: 21657.89
[INFO 2017-06-28 18:40:59,755 main.py:51] epoch 1764, training loss: 13035.31, average training loss: 13609.92, base loss: 21658.52
[INFO 2017-06-28 18:41:01,040 main.py:51] epoch 1765, training loss: 14533.53, average training loss: 13610.71, base loss: 21660.46
[INFO 2017-06-28 18:41:02,270 main.py:51] epoch 1766, training loss: 13036.32, average training loss: 13609.99, base loss: 21660.34
[INFO 2017-06-28 18:41:03,496 main.py:51] epoch 1767, training loss: 12942.26, average training loss: 13607.10, base loss: 21658.10
[INFO 2017-06-28 18:41:04,819 main.py:51] epoch 1768, training loss: 13251.03, average training loss: 13605.76, base loss: 21656.18
[INFO 2017-06-28 18:41:06,062 main.py:51] epoch 1769, training loss: 12686.39, average training loss: 13605.40, base loss: 21656.64
[INFO 2017-06-28 18:41:07,382 main.py:51] epoch 1770, training loss: 13018.63, average training loss: 13604.72, base loss: 21657.63
[INFO 2017-06-28 18:41:08,838 main.py:51] epoch 1771, training loss: 13841.35, average training loss: 13603.14, base loss: 21655.60
[INFO 2017-06-28 18:41:10,093 main.py:51] epoch 1772, training loss: 13361.19, average training loss: 13602.40, base loss: 21654.51
[INFO 2017-06-28 18:41:11,406 main.py:51] epoch 1773, training loss: 13058.17, average training loss: 13601.81, base loss: 21655.98
[INFO 2017-06-28 18:41:12,705 main.py:51] epoch 1774, training loss: 13853.02, average training loss: 13601.76, base loss: 21656.57
[INFO 2017-06-28 18:41:14,051 main.py:51] epoch 1775, training loss: 15185.63, average training loss: 13602.26, base loss: 21656.65
[INFO 2017-06-28 18:41:15,397 main.py:51] epoch 1776, training loss: 13283.27, average training loss: 13602.87, base loss: 21658.18
[INFO 2017-06-28 18:41:16,834 main.py:51] epoch 1777, training loss: 13082.99, average training loss: 13601.48, base loss: 21658.80
[INFO 2017-06-28 18:41:18,074 main.py:51] epoch 1778, training loss: 11733.42, average training loss: 13597.69, base loss: 21654.93
[INFO 2017-06-28 18:41:19,264 main.py:51] epoch 1779, training loss: 12254.00, average training loss: 13596.15, base loss: 21654.91
[INFO 2017-06-28 18:41:20,717 main.py:51] epoch 1780, training loss: 16331.20, average training loss: 13597.27, base loss: 21656.06
[INFO 2017-06-28 18:41:22,047 main.py:51] epoch 1781, training loss: 13461.04, average training loss: 13597.56, base loss: 21656.20
[INFO 2017-06-28 18:41:23,421 main.py:51] epoch 1782, training loss: 12014.80, average training loss: 13596.09, base loss: 21655.21
[INFO 2017-06-28 18:41:24,780 main.py:51] epoch 1783, training loss: 12046.62, average training loss: 13594.08, base loss: 21653.97
[INFO 2017-06-28 18:41:26,069 main.py:51] epoch 1784, training loss: 14475.54, average training loss: 13595.23, base loss: 21657.62
[INFO 2017-06-28 18:41:27,378 main.py:51] epoch 1785, training loss: 12589.84, average training loss: 13593.11, base loss: 21656.61
[INFO 2017-06-28 18:41:28,604 main.py:51] epoch 1786, training loss: 15687.73, average training loss: 13595.08, base loss: 21660.66
[INFO 2017-06-28 18:41:29,861 main.py:51] epoch 1787, training loss: 13273.50, average training loss: 13594.88, base loss: 21660.67
[INFO 2017-06-28 18:41:31,326 main.py:51] epoch 1788, training loss: 11924.71, average training loss: 13592.80, base loss: 21659.37
[INFO 2017-06-28 18:41:32,601 main.py:51] epoch 1789, training loss: 12798.04, average training loss: 13591.56, base loss: 21657.09
[INFO 2017-06-28 18:41:33,889 main.py:51] epoch 1790, training loss: 13327.31, average training loss: 13592.20, base loss: 21657.81
[INFO 2017-06-28 18:41:35,111 main.py:51] epoch 1791, training loss: 11870.71, average training loss: 13590.12, base loss: 21655.54
[INFO 2017-06-28 18:41:36,523 main.py:51] epoch 1792, training loss: 12959.29, average training loss: 13588.46, base loss: 21654.80
[INFO 2017-06-28 18:41:37,959 main.py:51] epoch 1793, training loss: 12394.12, average training loss: 13587.56, base loss: 21653.77
[INFO 2017-06-28 18:41:39,360 main.py:51] epoch 1794, training loss: 13340.49, average training loss: 13587.30, base loss: 21653.63
[INFO 2017-06-28 18:41:40,640 main.py:51] epoch 1795, training loss: 13174.70, average training loss: 13587.10, base loss: 21655.94
[INFO 2017-06-28 18:41:42,027 main.py:51] epoch 1796, training loss: 13081.72, average training loss: 13587.22, base loss: 21656.26
[INFO 2017-06-28 18:41:43,257 main.py:51] epoch 1797, training loss: 12896.46, average training loss: 13586.35, base loss: 21658.89
[INFO 2017-06-28 18:41:44,550 main.py:51] epoch 1798, training loss: 12393.80, average training loss: 13584.98, base loss: 21657.68
[INFO 2017-06-28 18:41:45,804 main.py:51] epoch 1799, training loss: 13819.99, average training loss: 13586.14, base loss: 21660.37
[INFO 2017-06-28 18:41:45,804 main.py:53] epoch 1799, testing
[INFO 2017-06-28 18:41:51,826 main.py:105] average testing loss: 14532.62, base loss: 22412.90
[INFO 2017-06-28 18:41:51,827 main.py:106] improve_loss: 7880.28, improve_percent: 0.35
[INFO 2017-06-28 18:41:51,827 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:41:53,262 main.py:51] epoch 1800, training loss: 14422.52, average training loss: 13584.61, base loss: 21661.97
[INFO 2017-06-28 18:41:54,602 main.py:51] epoch 1801, training loss: 13326.26, average training loss: 13585.39, base loss: 21663.37
[INFO 2017-06-28 18:41:55,902 main.py:51] epoch 1802, training loss: 13621.75, average training loss: 13586.76, base loss: 21664.70
[INFO 2017-06-28 18:41:57,179 main.py:51] epoch 1803, training loss: 11708.61, average training loss: 13583.44, base loss: 21661.97
[INFO 2017-06-28 18:41:58,508 main.py:51] epoch 1804, training loss: 13037.67, average training loss: 13583.23, base loss: 21662.75
[INFO 2017-06-28 18:41:59,871 main.py:51] epoch 1805, training loss: 12319.03, average training loss: 13580.72, base loss: 21660.68
[INFO 2017-06-28 18:42:01,207 main.py:51] epoch 1806, training loss: 14099.67, average training loss: 13580.56, base loss: 21663.54
[INFO 2017-06-28 18:42:02,574 main.py:51] epoch 1807, training loss: 14996.58, average training loss: 13581.09, base loss: 21663.73
[INFO 2017-06-28 18:42:03,935 main.py:51] epoch 1808, training loss: 14747.79, average training loss: 13581.53, base loss: 21665.74
[INFO 2017-06-28 18:42:05,357 main.py:51] epoch 1809, training loss: 12961.16, average training loss: 13581.70, base loss: 21667.96
[INFO 2017-06-28 18:42:06,668 main.py:51] epoch 1810, training loss: 13994.59, average training loss: 13582.59, base loss: 21670.43
[INFO 2017-06-28 18:42:08,016 main.py:51] epoch 1811, training loss: 13779.77, average training loss: 13581.61, base loss: 21670.00
[INFO 2017-06-28 18:42:09,245 main.py:51] epoch 1812, training loss: 13958.43, average training loss: 13583.91, base loss: 21675.14
[INFO 2017-06-28 18:42:10,641 main.py:51] epoch 1813, training loss: 13295.18, average training loss: 13582.54, base loss: 21672.19
[INFO 2017-06-28 18:42:11,922 main.py:51] epoch 1814, training loss: 12472.92, average training loss: 13580.52, base loss: 21669.57
[INFO 2017-06-28 18:42:13,238 main.py:51] epoch 1815, training loss: 16624.96, average training loss: 13582.26, base loss: 21672.45
[INFO 2017-06-28 18:42:14,541 main.py:51] epoch 1816, training loss: 12471.77, average training loss: 13579.70, base loss: 21669.30
[INFO 2017-06-28 18:42:15,909 main.py:51] epoch 1817, training loss: 13311.46, average training loss: 13579.78, base loss: 21672.29
[INFO 2017-06-28 18:42:17,311 main.py:51] epoch 1818, training loss: 13368.38, average training loss: 13579.23, base loss: 21672.04
[INFO 2017-06-28 18:42:18,635 main.py:51] epoch 1819, training loss: 13401.54, average training loss: 13577.60, base loss: 21671.73
[INFO 2017-06-28 18:42:20,018 main.py:51] epoch 1820, training loss: 12707.26, average training loss: 13576.76, base loss: 21670.75
[INFO 2017-06-28 18:42:21,318 main.py:51] epoch 1821, training loss: 12205.81, average training loss: 13572.77, base loss: 21665.99
[INFO 2017-06-28 18:42:22,647 main.py:51] epoch 1822, training loss: 14517.48, average training loss: 13574.06, base loss: 21666.68
[INFO 2017-06-28 18:42:24,017 main.py:51] epoch 1823, training loss: 12631.97, average training loss: 13572.85, base loss: 21664.80
[INFO 2017-06-28 18:42:25,278 main.py:51] epoch 1824, training loss: 13308.97, average training loss: 13573.29, base loss: 21666.50
[INFO 2017-06-28 18:42:26,569 main.py:51] epoch 1825, training loss: 12497.65, average training loss: 13571.85, base loss: 21665.75
[INFO 2017-06-28 18:42:27,771 main.py:51] epoch 1826, training loss: 11884.32, average training loss: 13570.37, base loss: 21664.09
[INFO 2017-06-28 18:42:29,240 main.py:51] epoch 1827, training loss: 12579.71, average training loss: 13568.52, base loss: 21663.03
[INFO 2017-06-28 18:42:30,539 main.py:51] epoch 1828, training loss: 13630.46, average training loss: 13568.21, base loss: 21663.83
[INFO 2017-06-28 18:42:31,876 main.py:51] epoch 1829, training loss: 13308.56, average training loss: 13567.48, base loss: 21663.61
[INFO 2017-06-28 18:42:33,325 main.py:51] epoch 1830, training loss: 12950.77, average training loss: 13566.90, base loss: 21664.06
[INFO 2017-06-28 18:42:34,533 main.py:51] epoch 1831, training loss: 12753.46, average training loss: 13566.34, base loss: 21663.25
[INFO 2017-06-28 18:42:35,899 main.py:51] epoch 1832, training loss: 11614.96, average training loss: 13562.77, base loss: 21661.54
[INFO 2017-06-28 18:42:37,255 main.py:51] epoch 1833, training loss: 14457.82, average training loss: 13562.26, base loss: 21661.48
[INFO 2017-06-28 18:42:38,615 main.py:51] epoch 1834, training loss: 12454.83, average training loss: 13561.03, base loss: 21661.89
[INFO 2017-06-28 18:42:39,858 main.py:51] epoch 1835, training loss: 12672.78, average training loss: 13560.87, base loss: 21664.33
[INFO 2017-06-28 18:42:41,217 main.py:51] epoch 1836, training loss: 13197.35, average training loss: 13561.89, base loss: 21665.25
[INFO 2017-06-28 18:42:42,549 main.py:51] epoch 1837, training loss: 13404.77, average training loss: 13561.65, base loss: 21668.39
[INFO 2017-06-28 18:42:43,889 main.py:51] epoch 1838, training loss: 14692.77, average training loss: 13561.40, base loss: 21668.76
[INFO 2017-06-28 18:42:45,265 main.py:51] epoch 1839, training loss: 14346.81, average training loss: 13562.74, base loss: 21671.59
[INFO 2017-06-28 18:42:46,682 main.py:51] epoch 1840, training loss: 12471.08, average training loss: 13561.42, base loss: 21670.39
[INFO 2017-06-28 18:42:47,937 main.py:51] epoch 1841, training loss: 13495.09, average training loss: 13560.56, base loss: 21670.20
[INFO 2017-06-28 18:42:49,156 main.py:51] epoch 1842, training loss: 12688.86, average training loss: 13557.94, base loss: 21665.44
[INFO 2017-06-28 18:42:50,434 main.py:51] epoch 1843, training loss: 13370.57, average training loss: 13557.61, base loss: 21667.21
[INFO 2017-06-28 18:42:51,729 main.py:51] epoch 1844, training loss: 14099.80, average training loss: 13557.32, base loss: 21666.54
[INFO 2017-06-28 18:42:52,984 main.py:51] epoch 1845, training loss: 14065.35, average training loss: 13557.18, base loss: 21666.38
[INFO 2017-06-28 18:42:54,241 main.py:51] epoch 1846, training loss: 11882.13, average training loss: 13555.56, base loss: 21663.44
[INFO 2017-06-28 18:42:55,563 main.py:51] epoch 1847, training loss: 14000.32, average training loss: 13556.30, base loss: 21665.92
[INFO 2017-06-28 18:42:56,829 main.py:51] epoch 1848, training loss: 13557.65, average training loss: 13557.17, base loss: 21667.55
[INFO 2017-06-28 18:42:58,162 main.py:51] epoch 1849, training loss: 13537.04, average training loss: 13559.07, base loss: 21669.23
[INFO 2017-06-28 18:42:59,427 main.py:51] epoch 1850, training loss: 13159.00, average training loss: 13558.59, base loss: 21668.61
[INFO 2017-06-28 18:43:00,666 main.py:51] epoch 1851, training loss: 13015.41, average training loss: 13557.67, base loss: 21667.56
[INFO 2017-06-28 18:43:01,934 main.py:51] epoch 1852, training loss: 12812.20, average training loss: 13557.86, base loss: 21669.95
[INFO 2017-06-28 18:43:03,197 main.py:51] epoch 1853, training loss: 12405.32, average training loss: 13557.12, base loss: 21670.00
[INFO 2017-06-28 18:43:04,475 main.py:51] epoch 1854, training loss: 12945.08, average training loss: 13556.15, base loss: 21669.21
[INFO 2017-06-28 18:43:05,833 main.py:51] epoch 1855, training loss: 12792.77, average training loss: 13556.10, base loss: 21670.36
[INFO 2017-06-28 18:43:07,231 main.py:51] epoch 1856, training loss: 13089.82, average training loss: 13554.26, base loss: 21667.68
[INFO 2017-06-28 18:43:08,555 main.py:51] epoch 1857, training loss: 14141.73, average training loss: 13555.13, base loss: 21669.68
[INFO 2017-06-28 18:43:10,000 main.py:51] epoch 1858, training loss: 12395.92, average training loss: 13553.80, base loss: 21667.55
[INFO 2017-06-28 18:43:11,281 main.py:51] epoch 1859, training loss: 14446.43, average training loss: 13555.30, base loss: 21669.09
[INFO 2017-06-28 18:43:12,493 main.py:51] epoch 1860, training loss: 12808.42, average training loss: 13553.43, base loss: 21668.91
[INFO 2017-06-28 18:43:13,853 main.py:51] epoch 1861, training loss: 14403.61, average training loss: 13553.65, base loss: 21671.28
[INFO 2017-06-28 18:43:15,186 main.py:51] epoch 1862, training loss: 13378.74, average training loss: 13554.86, base loss: 21673.68
[INFO 2017-06-28 18:43:16,586 main.py:51] epoch 1863, training loss: 13931.86, average training loss: 13554.59, base loss: 21673.64
[INFO 2017-06-28 18:43:17,866 main.py:51] epoch 1864, training loss: 14402.86, average training loss: 13554.69, base loss: 21676.29
[INFO 2017-06-28 18:43:19,112 main.py:51] epoch 1865, training loss: 12699.55, average training loss: 13554.05, base loss: 21676.83
[INFO 2017-06-28 18:43:20,360 main.py:51] epoch 1866, training loss: 14028.84, average training loss: 13554.94, base loss: 21679.17
[INFO 2017-06-28 18:43:21,710 main.py:51] epoch 1867, training loss: 13078.26, average training loss: 13554.77, base loss: 21679.35
[INFO 2017-06-28 18:43:23,149 main.py:51] epoch 1868, training loss: 13012.54, average training loss: 13552.01, base loss: 21677.00
[INFO 2017-06-28 18:43:24,487 main.py:51] epoch 1869, training loss: 11865.72, average training loss: 13551.20, base loss: 21675.49
[INFO 2017-06-28 18:43:25,796 main.py:51] epoch 1870, training loss: 13293.38, average training loss: 13550.87, base loss: 21673.64
[INFO 2017-06-28 18:43:27,027 main.py:51] epoch 1871, training loss: 12243.18, average training loss: 13547.53, base loss: 21670.97
[INFO 2017-06-28 18:43:28,279 main.py:51] epoch 1872, training loss: 14189.14, average training loss: 13546.87, base loss: 21673.15
[INFO 2017-06-28 18:43:29,588 main.py:51] epoch 1873, training loss: 11846.10, average training loss: 13544.63, base loss: 21669.90
[INFO 2017-06-28 18:43:30,956 main.py:51] epoch 1874, training loss: 13359.45, average training loss: 13543.30, base loss: 21669.32
[INFO 2017-06-28 18:43:32,261 main.py:51] epoch 1875, training loss: 12412.05, average training loss: 13541.23, base loss: 21669.19
[INFO 2017-06-28 18:43:33,587 main.py:51] epoch 1876, training loss: 14595.25, average training loss: 13542.42, base loss: 21669.79
[INFO 2017-06-28 18:43:34,805 main.py:51] epoch 1877, training loss: 13711.52, average training loss: 13543.27, base loss: 21670.30
[INFO 2017-06-28 18:43:36,098 main.py:51] epoch 1878, training loss: 13632.47, average training loss: 13542.37, base loss: 21669.51
[INFO 2017-06-28 18:43:37,375 main.py:51] epoch 1879, training loss: 12576.41, average training loss: 13539.47, base loss: 21667.53
[INFO 2017-06-28 18:43:38,672 main.py:51] epoch 1880, training loss: 12987.48, average training loss: 13535.93, base loss: 21664.51
[INFO 2017-06-28 18:43:39,998 main.py:51] epoch 1881, training loss: 12338.95, average training loss: 13533.00, base loss: 21662.00
[INFO 2017-06-28 18:43:41,388 main.py:51] epoch 1882, training loss: 13065.52, average training loss: 13531.74, base loss: 21663.84
[INFO 2017-06-28 18:43:42,762 main.py:51] epoch 1883, training loss: 12613.51, average training loss: 13531.36, base loss: 21663.94
[INFO 2017-06-28 18:43:44,042 main.py:51] epoch 1884, training loss: 12184.73, average training loss: 13528.24, base loss: 21662.95
[INFO 2017-06-28 18:43:45,385 main.py:51] epoch 1885, training loss: 13297.59, average training loss: 13528.29, base loss: 21663.19
[INFO 2017-06-28 18:43:46,695 main.py:51] epoch 1886, training loss: 12935.62, average training loss: 13528.54, base loss: 21663.94
[INFO 2017-06-28 18:43:48,014 main.py:51] epoch 1887, training loss: 14592.86, average training loss: 13527.41, base loss: 21663.83
[INFO 2017-06-28 18:43:49,220 main.py:51] epoch 1888, training loss: 11521.54, average training loss: 13525.91, base loss: 21662.36
[INFO 2017-06-28 18:43:50,474 main.py:51] epoch 1889, training loss: 13667.96, average training loss: 13526.13, base loss: 21665.22
[INFO 2017-06-28 18:43:51,836 main.py:51] epoch 1890, training loss: 13521.53, average training loss: 13525.15, base loss: 21665.88
[INFO 2017-06-28 18:43:53,219 main.py:51] epoch 1891, training loss: 12616.92, average training loss: 13523.66, base loss: 21665.75
[INFO 2017-06-28 18:43:54,530 main.py:51] epoch 1892, training loss: 12441.95, average training loss: 13523.18, base loss: 21665.34
[INFO 2017-06-28 18:43:55,868 main.py:51] epoch 1893, training loss: 13609.38, average training loss: 13522.28, base loss: 21666.10
[INFO 2017-06-28 18:43:57,183 main.py:51] epoch 1894, training loss: 12189.41, average training loss: 13521.15, base loss: 21665.23
[INFO 2017-06-28 18:43:58,482 main.py:51] epoch 1895, training loss: 13158.08, average training loss: 13521.32, base loss: 21666.68
[INFO 2017-06-28 18:43:59,815 main.py:51] epoch 1896, training loss: 13381.41, average training loss: 13519.79, base loss: 21664.98
[INFO 2017-06-28 18:44:01,036 main.py:51] epoch 1897, training loss: 11959.88, average training loss: 13518.03, base loss: 21663.16
[INFO 2017-06-28 18:44:02,320 main.py:51] epoch 1898, training loss: 15279.85, average training loss: 13520.77, base loss: 21665.92
[INFO 2017-06-28 18:44:03,719 main.py:51] epoch 1899, training loss: 13593.05, average training loss: 13520.48, base loss: 21664.85
[INFO 2017-06-28 18:44:03,719 main.py:53] epoch 1899, testing
[INFO 2017-06-28 18:44:09,788 main.py:105] average testing loss: 14453.08, base loss: 21891.77
[INFO 2017-06-28 18:44:09,789 main.py:106] improve_loss: 7438.69, improve_percent: 0.34
[INFO 2017-06-28 18:44:09,789 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:44:11,086 main.py:51] epoch 1900, training loss: 12788.46, average training loss: 13518.58, base loss: 21663.89
[INFO 2017-06-28 18:44:12,399 main.py:51] epoch 1901, training loss: 12238.98, average training loss: 13515.43, base loss: 21661.54
[INFO 2017-06-28 18:44:13,644 main.py:51] epoch 1902, training loss: 12309.95, average training loss: 13514.23, base loss: 21661.03
[INFO 2017-06-28 18:44:14,892 main.py:51] epoch 1903, training loss: 14364.39, average training loss: 13514.05, base loss: 21660.08
[INFO 2017-06-28 18:44:16,161 main.py:51] epoch 1904, training loss: 13068.12, average training loss: 13513.43, base loss: 21659.52
[INFO 2017-06-28 18:44:17,481 main.py:51] epoch 1905, training loss: 12060.60, average training loss: 13511.65, base loss: 21658.71
[INFO 2017-06-28 18:44:18,791 main.py:51] epoch 1906, training loss: 12600.23, average training loss: 13510.74, base loss: 21657.08
[INFO 2017-06-28 18:44:20,055 main.py:51] epoch 1907, training loss: 11759.12, average training loss: 13509.12, base loss: 21656.06
[INFO 2017-06-28 18:44:21,425 main.py:51] epoch 1908, training loss: 13316.70, average training loss: 13510.27, base loss: 21660.31
[INFO 2017-06-28 18:44:22,744 main.py:51] epoch 1909, training loss: 12775.33, average training loss: 13509.64, base loss: 21661.89
[INFO 2017-06-28 18:44:24,040 main.py:51] epoch 1910, training loss: 12811.17, average training loss: 13510.36, base loss: 21663.44
[INFO 2017-06-28 18:44:25,380 main.py:51] epoch 1911, training loss: 13699.41, average training loss: 13510.52, base loss: 21662.14
[INFO 2017-06-28 18:44:26,656 main.py:51] epoch 1912, training loss: 13891.69, average training loss: 13509.08, base loss: 21661.83
[INFO 2017-06-28 18:44:27,944 main.py:51] epoch 1913, training loss: 12920.28, average training loss: 13506.92, base loss: 21659.44
[INFO 2017-06-28 18:44:29,228 main.py:51] epoch 1914, training loss: 13539.37, average training loss: 13505.89, base loss: 21660.21
[INFO 2017-06-28 18:44:30,432 main.py:51] epoch 1915, training loss: 12884.00, average training loss: 13504.16, base loss: 21658.48
[INFO 2017-06-28 18:44:31,760 main.py:51] epoch 1916, training loss: 14109.43, average training loss: 13504.20, base loss: 21657.84
[INFO 2017-06-28 18:44:32,960 main.py:51] epoch 1917, training loss: 13671.70, average training loss: 13504.66, base loss: 21660.25
[INFO 2017-06-28 18:44:34,435 main.py:51] epoch 1918, training loss: 13146.57, average training loss: 13502.96, base loss: 21661.09
[INFO 2017-06-28 18:44:35,767 main.py:51] epoch 1919, training loss: 12638.18, average training loss: 13502.99, base loss: 21662.55
[INFO 2017-06-28 18:44:37,121 main.py:51] epoch 1920, training loss: 12023.88, average training loss: 13501.26, base loss: 21662.97
[INFO 2017-06-28 18:44:38,428 main.py:51] epoch 1921, training loss: 14855.76, average training loss: 13503.23, base loss: 21666.82
[INFO 2017-06-28 18:44:39,753 main.py:51] epoch 1922, training loss: 11754.66, average training loss: 13500.67, base loss: 21662.19
[INFO 2017-06-28 18:44:41,018 main.py:51] epoch 1923, training loss: 13714.81, average training loss: 13500.41, base loss: 21663.64
[INFO 2017-06-28 18:44:42,334 main.py:51] epoch 1924, training loss: 12364.08, average training loss: 13498.76, base loss: 21661.46
[INFO 2017-06-28 18:44:43,703 main.py:51] epoch 1925, training loss: 13787.55, average training loss: 13500.19, base loss: 21665.76
[INFO 2017-06-28 18:44:45,043 main.py:51] epoch 1926, training loss: 12469.80, average training loss: 13500.23, base loss: 21667.01
[INFO 2017-06-28 18:44:46,361 main.py:51] epoch 1927, training loss: 12638.56, average training loss: 13496.61, base loss: 21662.69
[INFO 2017-06-28 18:44:47,597 main.py:51] epoch 1928, training loss: 12969.19, average training loss: 13494.99, base loss: 21662.82
[INFO 2017-06-28 18:44:48,925 main.py:51] epoch 1929, training loss: 13050.67, average training loss: 13494.60, base loss: 21663.48
[INFO 2017-06-28 18:44:50,256 main.py:51] epoch 1930, training loss: 13596.19, average training loss: 13493.64, base loss: 21662.44
[INFO 2017-06-28 18:44:51,559 main.py:51] epoch 1931, training loss: 12834.54, average training loss: 13493.20, base loss: 21663.62
[INFO 2017-06-28 18:44:52,889 main.py:51] epoch 1932, training loss: 12766.31, average training loss: 13491.09, base loss: 21661.80
[INFO 2017-06-28 18:44:54,118 main.py:51] epoch 1933, training loss: 13063.98, average training loss: 13490.36, base loss: 21662.21
[INFO 2017-06-28 18:44:55,347 main.py:51] epoch 1934, training loss: 11928.76, average training loss: 13488.76, base loss: 21658.32
[INFO 2017-06-28 18:44:56,643 main.py:51] epoch 1935, training loss: 13686.84, average training loss: 13488.13, base loss: 21658.04
[INFO 2017-06-28 18:44:57,957 main.py:51] epoch 1936, training loss: 12716.62, average training loss: 13488.63, base loss: 21659.85
[INFO 2017-06-28 18:44:59,308 main.py:51] epoch 1937, training loss: 13327.58, average training loss: 13488.70, base loss: 21660.72
[INFO 2017-06-28 18:45:00,581 main.py:51] epoch 1938, training loss: 13665.94, average training loss: 13488.21, base loss: 21660.97
[INFO 2017-06-28 18:45:01,908 main.py:51] epoch 1939, training loss: 12749.05, average training loss: 13486.44, base loss: 21659.67
[INFO 2017-06-28 18:45:03,181 main.py:51] epoch 1940, training loss: 13881.24, average training loss: 13487.03, base loss: 21660.54
[INFO 2017-06-28 18:45:04,546 main.py:51] epoch 1941, training loss: 13779.59, average training loss: 13486.61, base loss: 21659.75
[INFO 2017-06-28 18:45:05,804 main.py:51] epoch 1942, training loss: 15107.95, average training loss: 13487.34, base loss: 21660.38
[INFO 2017-06-28 18:45:07,067 main.py:51] epoch 1943, training loss: 14129.17, average training loss: 13487.78, base loss: 21663.89
[INFO 2017-06-28 18:45:08,366 main.py:51] epoch 1944, training loss: 13873.09, average training loss: 13488.47, base loss: 21665.63
[INFO 2017-06-28 18:45:09,614 main.py:51] epoch 1945, training loss: 12308.16, average training loss: 13487.84, base loss: 21662.57
[INFO 2017-06-28 18:45:10,947 main.py:51] epoch 1946, training loss: 12443.62, average training loss: 13485.88, base loss: 21660.36
[INFO 2017-06-28 18:45:12,282 main.py:51] epoch 1947, training loss: 13990.19, average training loss: 13486.25, base loss: 21660.56
[INFO 2017-06-28 18:45:13,585 main.py:51] epoch 1948, training loss: 13004.54, average training loss: 13484.46, base loss: 21655.96
[INFO 2017-06-28 18:45:14,897 main.py:51] epoch 1949, training loss: 12304.29, average training loss: 13483.38, base loss: 21654.56
[INFO 2017-06-28 18:45:16,244 main.py:51] epoch 1950, training loss: 12837.45, average training loss: 13482.93, base loss: 21653.60
[INFO 2017-06-28 18:45:17,470 main.py:51] epoch 1951, training loss: 13549.83, average training loss: 13481.79, base loss: 21652.90
[INFO 2017-06-28 18:45:18,781 main.py:51] epoch 1952, training loss: 14458.64, average training loss: 13482.30, base loss: 21653.54
[INFO 2017-06-28 18:45:20,054 main.py:51] epoch 1953, training loss: 14719.07, average training loss: 13484.26, base loss: 21656.41
[INFO 2017-06-28 18:45:21,493 main.py:51] epoch 1954, training loss: 13774.91, average training loss: 13485.11, base loss: 21659.23
[INFO 2017-06-28 18:45:22,691 main.py:51] epoch 1955, training loss: 13956.19, average training loss: 13486.62, base loss: 21661.37
[INFO 2017-06-28 18:45:24,029 main.py:51] epoch 1956, training loss: 13869.96, average training loss: 13486.89, base loss: 21661.64
[INFO 2017-06-28 18:45:25,327 main.py:51] epoch 1957, training loss: 14160.22, average training loss: 13487.78, base loss: 21665.71
[INFO 2017-06-28 18:45:26,558 main.py:51] epoch 1958, training loss: 13947.65, average training loss: 13488.70, base loss: 21668.85
[INFO 2017-06-28 18:45:27,901 main.py:51] epoch 1959, training loss: 12575.96, average training loss: 13486.31, base loss: 21665.78
[INFO 2017-06-28 18:45:29,186 main.py:51] epoch 1960, training loss: 12963.74, average training loss: 13485.93, base loss: 21666.41
[INFO 2017-06-28 18:45:30,477 main.py:51] epoch 1961, training loss: 13552.99, average training loss: 13484.48, base loss: 21665.10
[INFO 2017-06-28 18:45:31,796 main.py:51] epoch 1962, training loss: 14948.84, average training loss: 13486.55, base loss: 21669.30
[INFO 2017-06-28 18:45:33,124 main.py:51] epoch 1963, training loss: 13349.32, average training loss: 13485.69, base loss: 21668.88
[INFO 2017-06-28 18:45:34,398 main.py:51] epoch 1964, training loss: 12905.61, average training loss: 13484.99, base loss: 21669.76
[INFO 2017-06-28 18:45:35,756 main.py:51] epoch 1965, training loss: 13845.80, average training loss: 13485.67, base loss: 21669.80
[INFO 2017-06-28 18:45:37,094 main.py:51] epoch 1966, training loss: 12305.42, average training loss: 13484.89, base loss: 21666.80
[INFO 2017-06-28 18:45:38,408 main.py:51] epoch 1967, training loss: 13870.17, average training loss: 13484.68, base loss: 21666.63
[INFO 2017-06-28 18:45:39,812 main.py:51] epoch 1968, training loss: 12736.30, average training loss: 13485.14, base loss: 21668.24
[INFO 2017-06-28 18:45:41,148 main.py:51] epoch 1969, training loss: 12286.49, average training loss: 13484.54, base loss: 21669.17
[INFO 2017-06-28 18:45:42,457 main.py:51] epoch 1970, training loss: 14177.29, average training loss: 13484.83, base loss: 21671.55
[INFO 2017-06-28 18:45:43,753 main.py:51] epoch 1971, training loss: 12262.98, average training loss: 13483.08, base loss: 21672.07
[INFO 2017-06-28 18:45:45,040 main.py:51] epoch 1972, training loss: 12253.10, average training loss: 13479.43, base loss: 21668.33
[INFO 2017-06-28 18:45:46,361 main.py:51] epoch 1973, training loss: 12809.26, average training loss: 13478.40, base loss: 21667.65
[INFO 2017-06-28 18:45:47,733 main.py:51] epoch 1974, training loss: 11791.49, average training loss: 13476.65, base loss: 21665.12
[INFO 2017-06-28 18:45:49,026 main.py:51] epoch 1975, training loss: 13431.62, average training loss: 13475.54, base loss: 21666.48
[INFO 2017-06-28 18:45:50,396 main.py:51] epoch 1976, training loss: 13756.39, average training loss: 13476.40, base loss: 21669.01
[INFO 2017-06-28 18:45:51,778 main.py:51] epoch 1977, training loss: 13388.21, average training loss: 13476.51, base loss: 21668.55
[INFO 2017-06-28 18:45:53,003 main.py:51] epoch 1978, training loss: 12974.65, average training loss: 13474.07, base loss: 21664.97
[INFO 2017-06-28 18:45:54,365 main.py:51] epoch 1979, training loss: 12154.24, average training loss: 13473.46, base loss: 21663.87
[INFO 2017-06-28 18:45:55,677 main.py:51] epoch 1980, training loss: 12916.81, average training loss: 13474.18, base loss: 21665.41
[INFO 2017-06-28 18:45:57,040 main.py:51] epoch 1981, training loss: 13992.42, average training loss: 13474.03, base loss: 21666.46
[INFO 2017-06-28 18:45:58,317 main.py:51] epoch 1982, training loss: 13368.13, average training loss: 13472.72, base loss: 21667.61
[INFO 2017-06-28 18:45:59,637 main.py:51] epoch 1983, training loss: 12434.38, average training loss: 13470.88, base loss: 21668.03
[INFO 2017-06-28 18:46:00,890 main.py:51] epoch 1984, training loss: 12277.25, average training loss: 13468.42, base loss: 21664.64
[INFO 2017-06-28 18:46:02,206 main.py:51] epoch 1985, training loss: 12635.01, average training loss: 13466.81, base loss: 21663.48
[INFO 2017-06-28 18:46:03,454 main.py:51] epoch 1986, training loss: 13107.72, average training loss: 13465.89, base loss: 21665.44
[INFO 2017-06-28 18:46:04,771 main.py:51] epoch 1987, training loss: 13957.71, average training loss: 13466.53, base loss: 21666.82
[INFO 2017-06-28 18:46:06,204 main.py:51] epoch 1988, training loss: 15064.59, average training loss: 13468.64, base loss: 21669.61
[INFO 2017-06-28 18:46:07,473 main.py:51] epoch 1989, training loss: 13727.70, average training loss: 13468.05, base loss: 21670.88
[INFO 2017-06-28 18:46:08,820 main.py:51] epoch 1990, training loss: 13975.04, average training loss: 13469.20, base loss: 21673.44
[INFO 2017-06-28 18:46:10,054 main.py:51] epoch 1991, training loss: 12961.32, average training loss: 13467.77, base loss: 21673.35
[INFO 2017-06-28 18:46:11,318 main.py:51] epoch 1992, training loss: 13236.52, average training loss: 13466.60, base loss: 21670.87
[INFO 2017-06-28 18:46:12,604 main.py:51] epoch 1993, training loss: 12223.07, average training loss: 13466.02, base loss: 21672.02
[INFO 2017-06-28 18:46:13,871 main.py:51] epoch 1994, training loss: 13111.56, average training loss: 13464.08, base loss: 21670.47
[INFO 2017-06-28 18:46:15,200 main.py:51] epoch 1995, training loss: 14909.87, average training loss: 13465.16, base loss: 21671.24
[INFO 2017-06-28 18:46:16,513 main.py:51] epoch 1996, training loss: 12082.41, average training loss: 13463.43, base loss: 21671.17
[INFO 2017-06-28 18:46:17,745 main.py:51] epoch 1997, training loss: 13804.93, average training loss: 13463.35, base loss: 21673.41
[INFO 2017-06-28 18:46:19,035 main.py:51] epoch 1998, training loss: 14256.61, average training loss: 13463.74, base loss: 21672.85
[INFO 2017-06-28 18:46:20,354 main.py:51] epoch 1999, training loss: 12626.49, average training loss: 13461.05, base loss: 21670.24
[INFO 2017-06-28 18:46:20,354 main.py:53] epoch 1999, testing
[INFO 2017-06-28 18:46:26,375 main.py:105] average testing loss: 15178.51, base loss: 23078.74
[INFO 2017-06-28 18:46:26,375 main.py:106] improve_loss: 7900.23, improve_percent: 0.34
[INFO 2017-06-28 18:46:26,375 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:46:27,710 main.py:51] epoch 2000, training loss: 12815.14, average training loss: 13460.62, base loss: 21671.58
[INFO 2017-06-28 18:46:29,046 main.py:51] epoch 2001, training loss: 12051.39, average training loss: 13460.26, base loss: 21671.08
[INFO 2017-06-28 18:46:30,340 main.py:51] epoch 2002, training loss: 11558.31, average training loss: 13457.48, base loss: 21669.22
[INFO 2017-06-28 18:46:31,628 main.py:51] epoch 2003, training loss: 14554.21, average training loss: 13455.81, base loss: 21667.31
[INFO 2017-06-28 18:46:33,050 main.py:51] epoch 2004, training loss: 14520.69, average training loss: 13455.89, base loss: 21667.89
[INFO 2017-06-28 18:46:34,242 main.py:51] epoch 2005, training loss: 13098.53, average training loss: 13454.93, base loss: 21665.89
[INFO 2017-06-28 18:46:35,466 main.py:51] epoch 2006, training loss: 12634.76, average training loss: 13453.14, base loss: 21662.87
[INFO 2017-06-28 18:46:36,718 main.py:51] epoch 2007, training loss: 12846.62, average training loss: 13452.84, base loss: 21664.07
[INFO 2017-06-28 18:46:38,020 main.py:51] epoch 2008, training loss: 13174.35, average training loss: 13453.51, base loss: 21668.36
[INFO 2017-06-28 18:46:39,333 main.py:51] epoch 2009, training loss: 12296.01, average training loss: 13449.77, base loss: 21666.73
[INFO 2017-06-28 18:46:40,655 main.py:51] epoch 2010, training loss: 11696.16, average training loss: 13446.81, base loss: 21663.17
[INFO 2017-06-28 18:46:41,977 main.py:51] epoch 2011, training loss: 13401.68, average training loss: 13445.64, base loss: 21663.47
[INFO 2017-06-28 18:46:43,387 main.py:51] epoch 2012, training loss: 14413.65, average training loss: 13444.97, base loss: 21665.98
[INFO 2017-06-28 18:46:44,685 main.py:51] epoch 2013, training loss: 13631.70, average training loss: 13443.97, base loss: 21667.40
[INFO 2017-06-28 18:46:46,048 main.py:51] epoch 2014, training loss: 12906.46, average training loss: 13443.47, base loss: 21668.69
[INFO 2017-06-28 18:46:47,315 main.py:51] epoch 2015, training loss: 13433.37, average training loss: 13442.96, base loss: 21668.26
[INFO 2017-06-28 18:46:48,674 main.py:51] epoch 2016, training loss: 13990.33, average training loss: 13443.97, base loss: 21667.00
[INFO 2017-06-28 18:46:49,950 main.py:51] epoch 2017, training loss: 13312.08, average training loss: 13443.15, base loss: 21666.05
[INFO 2017-06-28 18:46:51,309 main.py:51] epoch 2018, training loss: 12535.77, average training loss: 13442.86, base loss: 21666.13
[INFO 2017-06-28 18:46:52,606 main.py:51] epoch 2019, training loss: 13405.34, average training loss: 13443.02, base loss: 21666.29
[INFO 2017-06-28 18:46:53,788 main.py:51] epoch 2020, training loss: 14252.05, average training loss: 13445.18, base loss: 21669.65
[INFO 2017-06-28 18:46:55,071 main.py:51] epoch 2021, training loss: 14330.80, average training loss: 13446.58, base loss: 21670.98
[INFO 2017-06-28 18:46:56,483 main.py:51] epoch 2022, training loss: 14003.85, average training loss: 13446.73, base loss: 21670.21
[INFO 2017-06-28 18:46:57,926 main.py:51] epoch 2023, training loss: 12631.05, average training loss: 13446.23, base loss: 21670.13
[INFO 2017-06-28 18:46:59,242 main.py:51] epoch 2024, training loss: 13688.62, average training loss: 13445.84, base loss: 21670.49
[INFO 2017-06-28 18:47:00,656 main.py:51] epoch 2025, training loss: 13382.57, average training loss: 13444.97, base loss: 21669.18
[INFO 2017-06-28 18:47:01,970 main.py:51] epoch 2026, training loss: 13805.54, average training loss: 13447.02, base loss: 21672.09
[INFO 2017-06-28 18:47:03,317 main.py:51] epoch 2027, training loss: 11724.46, average training loss: 13442.51, base loss: 21666.57
[INFO 2017-06-28 18:47:04,729 main.py:51] epoch 2028, training loss: 12912.54, average training loss: 13439.55, base loss: 21661.38
[INFO 2017-06-28 18:47:06,052 main.py:51] epoch 2029, training loss: 13654.54, average training loss: 13441.01, base loss: 21661.94
[INFO 2017-06-28 18:47:07,278 main.py:51] epoch 2030, training loss: 15169.22, average training loss: 13442.61, base loss: 21664.67
[INFO 2017-06-28 18:47:08,540 main.py:51] epoch 2031, training loss: 12784.69, average training loss: 13440.74, base loss: 21663.27
[INFO 2017-06-28 18:47:09,856 main.py:51] epoch 2032, training loss: 12776.71, average training loss: 13439.38, base loss: 21659.39
[INFO 2017-06-28 18:47:11,118 main.py:51] epoch 2033, training loss: 12561.70, average training loss: 13439.51, base loss: 21660.89
[INFO 2017-06-28 18:47:12,428 main.py:51] epoch 2034, training loss: 12212.21, average training loss: 13438.33, base loss: 21659.47
[INFO 2017-06-28 18:47:13,780 main.py:51] epoch 2035, training loss: 14484.14, average training loss: 13439.83, base loss: 21663.98
[INFO 2017-06-28 18:47:15,165 main.py:51] epoch 2036, training loss: 13852.00, average training loss: 13439.25, base loss: 21664.59
[INFO 2017-06-28 18:47:16,504 main.py:51] epoch 2037, training loss: 12810.60, average training loss: 13439.16, base loss: 21665.35
[INFO 2017-06-28 18:47:17,730 main.py:51] epoch 2038, training loss: 13141.10, average training loss: 13435.98, base loss: 21660.64
[INFO 2017-06-28 18:47:19,025 main.py:51] epoch 2039, training loss: 12904.19, average training loss: 13433.00, base loss: 21657.12
[INFO 2017-06-28 18:47:20,388 main.py:51] epoch 2040, training loss: 12489.01, average training loss: 13430.44, base loss: 21656.09
[INFO 2017-06-28 18:47:21,653 main.py:51] epoch 2041, training loss: 12974.94, average training loss: 13430.19, base loss: 21655.68
[INFO 2017-06-28 18:47:23,017 main.py:51] epoch 2042, training loss: 12214.10, average training loss: 13427.74, base loss: 21651.63
[INFO 2017-06-28 18:47:24,343 main.py:51] epoch 2043, training loss: 13092.70, average training loss: 13428.10, base loss: 21653.59
[INFO 2017-06-28 18:47:25,636 main.py:51] epoch 2044, training loss: 14285.90, average training loss: 13427.90, base loss: 21656.10
[INFO 2017-06-28 18:47:26,981 main.py:51] epoch 2045, training loss: 13909.60, average training loss: 13429.09, base loss: 21657.34
[INFO 2017-06-28 18:47:28,248 main.py:51] epoch 2046, training loss: 13132.22, average training loss: 13427.43, base loss: 21653.71
[INFO 2017-06-28 18:47:29,516 main.py:51] epoch 2047, training loss: 12961.98, average training loss: 13427.43, base loss: 21652.98
[INFO 2017-06-28 18:47:30,693 main.py:51] epoch 2048, training loss: 13184.66, average training loss: 13427.89, base loss: 21655.50
[INFO 2017-06-28 18:47:31,988 main.py:51] epoch 2049, training loss: 12768.20, average training loss: 13426.89, base loss: 21656.31
[INFO 2017-06-28 18:47:33,225 main.py:51] epoch 2050, training loss: 12759.00, average training loss: 13426.97, base loss: 21655.54
[INFO 2017-06-28 18:47:34,554 main.py:51] epoch 2051, training loss: 13791.54, average training loss: 13427.56, base loss: 21657.48
[INFO 2017-06-28 18:47:35,812 main.py:51] epoch 2052, training loss: 15321.36, average training loss: 13428.59, base loss: 21658.32
[INFO 2017-06-28 18:47:37,024 main.py:51] epoch 2053, training loss: 13359.01, average training loss: 13428.96, base loss: 21660.94
[INFO 2017-06-28 18:47:38,437 main.py:51] epoch 2054, training loss: 13412.63, average training loss: 13429.26, base loss: 21663.81
[INFO 2017-06-28 18:47:39,704 main.py:51] epoch 2055, training loss: 14054.76, average training loss: 13429.96, base loss: 21664.96
[INFO 2017-06-28 18:47:41,038 main.py:51] epoch 2056, training loss: 14893.51, average training loss: 13430.89, base loss: 21666.20
[INFO 2017-06-28 18:47:42,453 main.py:51] epoch 2057, training loss: 13259.50, average training loss: 13432.02, base loss: 21667.55
[INFO 2017-06-28 18:47:43,763 main.py:51] epoch 2058, training loss: 12995.29, average training loss: 13429.37, base loss: 21665.67
[INFO 2017-06-28 18:47:45,141 main.py:51] epoch 2059, training loss: 12576.37, average training loss: 13428.78, base loss: 21667.01
[INFO 2017-06-28 18:47:46,525 main.py:51] epoch 2060, training loss: 13687.32, average training loss: 13427.53, base loss: 21665.01
[INFO 2017-06-28 18:47:47,791 main.py:51] epoch 2061, training loss: 12592.33, average training loss: 13425.02, base loss: 21663.32
[INFO 2017-06-28 18:47:49,135 main.py:51] epoch 2062, training loss: 14143.18, average training loss: 13425.96, base loss: 21665.54
[INFO 2017-06-28 18:47:50,498 main.py:51] epoch 2063, training loss: 11151.82, average training loss: 13424.40, base loss: 21665.30
[INFO 2017-06-28 18:47:51,784 main.py:51] epoch 2064, training loss: 13020.03, average training loss: 13422.86, base loss: 21663.90
[INFO 2017-06-28 18:47:53,098 main.py:51] epoch 2065, training loss: 12524.41, average training loss: 13420.17, base loss: 21664.13
[INFO 2017-06-28 18:47:54,526 main.py:51] epoch 2066, training loss: 14166.88, average training loss: 13421.01, base loss: 21665.38
[INFO 2017-06-28 18:47:55,846 main.py:51] epoch 2067, training loss: 14750.04, average training loss: 13422.52, base loss: 21668.47
[INFO 2017-06-28 18:47:57,169 main.py:51] epoch 2068, training loss: 12873.10, average training loss: 13421.94, base loss: 21668.04
[INFO 2017-06-28 18:47:58,416 main.py:51] epoch 2069, training loss: 14252.87, average training loss: 13421.89, base loss: 21668.53
[INFO 2017-06-28 18:47:59,729 main.py:51] epoch 2070, training loss: 13943.74, average training loss: 13421.53, base loss: 21667.27
[INFO 2017-06-28 18:48:01,089 main.py:51] epoch 2071, training loss: 12695.83, average training loss: 13421.10, base loss: 21667.57
[INFO 2017-06-28 18:48:02,467 main.py:51] epoch 2072, training loss: 13670.98, average training loss: 13421.48, base loss: 21667.21
[INFO 2017-06-28 18:48:03,726 main.py:51] epoch 2073, training loss: 12104.09, average training loss: 13419.41, base loss: 21664.58
[INFO 2017-06-28 18:48:04,991 main.py:51] epoch 2074, training loss: 12335.06, average training loss: 13417.23, base loss: 21662.22
[INFO 2017-06-28 18:48:06,258 main.py:51] epoch 2075, training loss: 13078.51, average training loss: 13415.49, base loss: 21661.48
[INFO 2017-06-28 18:48:07,515 main.py:51] epoch 2076, training loss: 12215.17, average training loss: 13413.91, base loss: 21662.77
[INFO 2017-06-28 18:48:08,794 main.py:51] epoch 2077, training loss: 13307.79, average training loss: 13414.32, base loss: 21663.41
[INFO 2017-06-28 18:48:10,137 main.py:51] epoch 2078, training loss: 12182.89, average training loss: 13412.15, base loss: 21661.17
[INFO 2017-06-28 18:48:11,396 main.py:51] epoch 2079, training loss: 13249.37, average training loss: 13411.46, base loss: 21659.91
[INFO 2017-06-28 18:48:12,635 main.py:51] epoch 2080, training loss: 12093.12, average training loss: 13408.89, base loss: 21659.49
[INFO 2017-06-28 18:48:13,946 main.py:51] epoch 2081, training loss: 13717.16, average training loss: 13406.48, base loss: 21657.73
[INFO 2017-06-28 18:48:15,247 main.py:51] epoch 2082, training loss: 13949.15, average training loss: 13407.31, base loss: 21659.45
[INFO 2017-06-28 18:48:16,584 main.py:51] epoch 2083, training loss: 12023.67, average training loss: 13407.08, base loss: 21659.18
[INFO 2017-06-28 18:48:18,069 main.py:51] epoch 2084, training loss: 13118.48, average training loss: 13406.86, base loss: 21659.83
[INFO 2017-06-28 18:48:19,372 main.py:51] epoch 2085, training loss: 12746.98, average training loss: 13406.63, base loss: 21659.57
[INFO 2017-06-28 18:48:20,713 main.py:51] epoch 2086, training loss: 13426.13, average training loss: 13406.67, base loss: 21658.67
[INFO 2017-06-28 18:48:21,993 main.py:51] epoch 2087, training loss: 12631.74, average training loss: 13405.12, base loss: 21656.72
[INFO 2017-06-28 18:48:23,256 main.py:51] epoch 2088, training loss: 13709.58, average training loss: 13404.93, base loss: 21657.56
[INFO 2017-06-28 18:48:24,527 main.py:51] epoch 2089, training loss: 14269.08, average training loss: 13405.36, base loss: 21658.75
[INFO 2017-06-28 18:48:25,821 main.py:51] epoch 2090, training loss: 11112.62, average training loss: 13402.42, base loss: 21656.26
[INFO 2017-06-28 18:48:26,993 main.py:51] epoch 2091, training loss: 12090.89, average training loss: 13401.99, base loss: 21656.01
[INFO 2017-06-28 18:48:28,310 main.py:51] epoch 2092, training loss: 12095.15, average training loss: 13401.60, base loss: 21654.13
[INFO 2017-06-28 18:48:29,610 main.py:51] epoch 2093, training loss: 15719.96, average training loss: 13403.02, base loss: 21655.26
[INFO 2017-06-28 18:48:30,908 main.py:51] epoch 2094, training loss: 14399.97, average training loss: 13405.54, base loss: 21657.41
[INFO 2017-06-28 18:48:32,190 main.py:51] epoch 2095, training loss: 15235.79, average training loss: 13407.01, base loss: 21659.54
[INFO 2017-06-28 18:48:33,465 main.py:51] epoch 2096, training loss: 11619.53, average training loss: 13404.20, base loss: 21657.44
[INFO 2017-06-28 18:48:34,837 main.py:51] epoch 2097, training loss: 14683.06, average training loss: 13404.24, base loss: 21655.16
[INFO 2017-06-28 18:48:36,057 main.py:51] epoch 2098, training loss: 13526.83, average training loss: 13403.24, base loss: 21655.66
[INFO 2017-06-28 18:48:37,370 main.py:51] epoch 2099, training loss: 13788.51, average training loss: 13403.39, base loss: 21655.86
[INFO 2017-06-28 18:48:37,370 main.py:53] epoch 2099, testing
[INFO 2017-06-28 18:48:43,344 main.py:105] average testing loss: 15283.63, base loss: 22986.95
[INFO 2017-06-28 18:48:43,344 main.py:106] improve_loss: 7703.32, improve_percent: 0.34
[INFO 2017-06-28 18:48:43,345 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:48:44,644 main.py:51] epoch 2100, training loss: 12566.82, average training loss: 13401.41, base loss: 21652.78
[INFO 2017-06-28 18:48:45,857 main.py:51] epoch 2101, training loss: 13953.40, average training loss: 13401.58, base loss: 21653.64
[INFO 2017-06-28 18:48:47,212 main.py:51] epoch 2102, training loss: 13243.77, average training loss: 13402.20, base loss: 21654.50
[INFO 2017-06-28 18:48:48,521 main.py:51] epoch 2103, training loss: 14032.24, average training loss: 13401.82, base loss: 21655.40
[INFO 2017-06-28 18:48:49,850 main.py:51] epoch 2104, training loss: 13130.78, average training loss: 13400.45, base loss: 21653.66
[INFO 2017-06-28 18:48:51,135 main.py:51] epoch 2105, training loss: 12810.55, average training loss: 13399.78, base loss: 21652.42
[INFO 2017-06-28 18:48:52,450 main.py:51] epoch 2106, training loss: 11872.03, average training loss: 13398.25, base loss: 21652.37
[INFO 2017-06-28 18:48:53,794 main.py:51] epoch 2107, training loss: 13909.36, average training loss: 13398.76, base loss: 21653.57
[INFO 2017-06-28 18:48:55,073 main.py:51] epoch 2108, training loss: 12155.58, average training loss: 13394.57, base loss: 21649.84
[INFO 2017-06-28 18:48:56,306 main.py:51] epoch 2109, training loss: 12651.83, average training loss: 13394.32, base loss: 21649.21
[INFO 2017-06-28 18:48:57,597 main.py:51] epoch 2110, training loss: 11953.12, average training loss: 13393.48, base loss: 21648.23
[INFO 2017-06-28 18:48:59,039 main.py:51] epoch 2111, training loss: 12771.21, average training loss: 13392.46, base loss: 21648.12
[INFO 2017-06-28 18:49:00,336 main.py:51] epoch 2112, training loss: 13513.78, average training loss: 13392.02, base loss: 21648.58
[INFO 2017-06-28 18:49:01,687 main.py:51] epoch 2113, training loss: 13375.82, average training loss: 13392.46, base loss: 21649.02
[INFO 2017-06-28 18:49:02,954 main.py:51] epoch 2114, training loss: 13112.08, average training loss: 13391.64, base loss: 21648.39
[INFO 2017-06-28 18:49:04,379 main.py:51] epoch 2115, training loss: 14096.23, average training loss: 13391.37, base loss: 21647.25
[INFO 2017-06-28 18:49:05,688 main.py:51] epoch 2116, training loss: 13519.17, average training loss: 13392.74, base loss: 21651.29
[INFO 2017-06-28 18:49:07,046 main.py:51] epoch 2117, training loss: 13225.44, average training loss: 13394.62, base loss: 21655.10
[INFO 2017-06-28 18:49:08,341 main.py:51] epoch 2118, training loss: 12998.93, average training loss: 13394.43, base loss: 21657.22
[INFO 2017-06-28 18:49:09,609 main.py:51] epoch 2119, training loss: 13448.60, average training loss: 13395.15, base loss: 21657.91
[INFO 2017-06-28 18:49:10,875 main.py:51] epoch 2120, training loss: 11815.42, average training loss: 13389.74, base loss: 21651.77
[INFO 2017-06-28 18:49:12,136 main.py:51] epoch 2121, training loss: 13098.16, average training loss: 13389.49, base loss: 21652.37
[INFO 2017-06-28 18:49:13,407 main.py:51] epoch 2122, training loss: 13304.30, average training loss: 13389.68, base loss: 21651.94
[INFO 2017-06-28 18:49:14,823 main.py:51] epoch 2123, training loss: 13214.08, average training loss: 13390.32, base loss: 21656.79
[INFO 2017-06-28 18:49:16,168 main.py:51] epoch 2124, training loss: 12152.18, average training loss: 13390.44, base loss: 21656.38
[INFO 2017-06-28 18:49:17,367 main.py:51] epoch 2125, training loss: 13427.80, average training loss: 13389.82, base loss: 21657.00
[INFO 2017-06-28 18:49:18,761 main.py:51] epoch 2126, training loss: 13350.85, average training loss: 13389.69, base loss: 21656.81
[INFO 2017-06-28 18:49:19,964 main.py:51] epoch 2127, training loss: 14275.36, average training loss: 13389.95, base loss: 21658.45
[INFO 2017-06-28 18:49:21,251 main.py:51] epoch 2128, training loss: 13471.98, average training loss: 13389.47, base loss: 21657.58
[INFO 2017-06-28 18:49:22,634 main.py:51] epoch 2129, training loss: 12016.74, average training loss: 13387.57, base loss: 21656.03
[INFO 2017-06-28 18:49:24,189 main.py:51] epoch 2130, training loss: 12571.99, average training loss: 13384.07, base loss: 21651.80
[INFO 2017-06-28 18:49:25,469 main.py:51] epoch 2131, training loss: 13056.68, average training loss: 13383.00, base loss: 21650.19
[INFO 2017-06-28 18:49:26,882 main.py:51] epoch 2132, training loss: 13599.36, average training loss: 13384.13, base loss: 21654.42
[INFO 2017-06-28 18:49:28,280 main.py:51] epoch 2133, training loss: 12408.79, average training loss: 13383.23, base loss: 21653.50
[INFO 2017-06-28 18:49:29,690 main.py:51] epoch 2134, training loss: 13512.43, average training loss: 13383.93, base loss: 21654.88
[INFO 2017-06-28 18:49:30,972 main.py:51] epoch 2135, training loss: 12011.52, average training loss: 13382.47, base loss: 21654.22
[INFO 2017-06-28 18:49:32,187 main.py:51] epoch 2136, training loss: 12261.29, average training loss: 13381.82, base loss: 21655.24
[INFO 2017-06-28 18:49:33,512 main.py:51] epoch 2137, training loss: 13642.25, average training loss: 13381.52, base loss: 21654.68
[INFO 2017-06-28 18:49:34,829 main.py:51] epoch 2138, training loss: 13518.60, average training loss: 13379.81, base loss: 21654.98
[INFO 2017-06-28 18:49:36,062 main.py:51] epoch 2139, training loss: 15128.61, average training loss: 13382.62, base loss: 21660.93
[INFO 2017-06-28 18:49:37,332 main.py:51] epoch 2140, training loss: 12267.00, average training loss: 13381.56, base loss: 21661.67
[INFO 2017-06-28 18:49:38,668 main.py:51] epoch 2141, training loss: 13250.51, average training loss: 13381.14, base loss: 21662.92
[INFO 2017-06-28 18:49:40,123 main.py:51] epoch 2142, training loss: 12673.68, average training loss: 13379.37, base loss: 21662.47
[INFO 2017-06-28 18:49:41,356 main.py:51] epoch 2143, training loss: 13075.43, average training loss: 13380.80, base loss: 21663.49
[INFO 2017-06-28 18:49:42,844 main.py:51] epoch 2144, training loss: 13242.68, average training loss: 13380.52, base loss: 21661.70
[INFO 2017-06-28 18:49:44,060 main.py:51] epoch 2145, training loss: 13493.70, average training loss: 13379.90, base loss: 21662.86
[INFO 2017-06-28 18:49:45,357 main.py:51] epoch 2146, training loss: 11966.68, average training loss: 13378.58, base loss: 21661.66
[INFO 2017-06-28 18:49:46,659 main.py:51] epoch 2147, training loss: 13846.04, average training loss: 13377.86, base loss: 21660.64
[INFO 2017-06-28 18:49:47,978 main.py:51] epoch 2148, training loss: 13007.07, average training loss: 13376.67, base loss: 21659.00
[INFO 2017-06-28 18:49:49,251 main.py:51] epoch 2149, training loss: 13304.22, average training loss: 13376.05, base loss: 21657.07
[INFO 2017-06-28 18:49:50,471 main.py:51] epoch 2150, training loss: 12988.69, average training loss: 13375.29, base loss: 21654.25
[INFO 2017-06-28 18:49:51,802 main.py:51] epoch 2151, training loss: 11559.03, average training loss: 13371.25, base loss: 21648.67
[INFO 2017-06-28 18:49:53,184 main.py:51] epoch 2152, training loss: 12963.64, average training loss: 13371.00, base loss: 21650.19
[INFO 2017-06-28 18:49:54,463 main.py:51] epoch 2153, training loss: 13338.99, average training loss: 13370.81, base loss: 21651.40
[INFO 2017-06-28 18:49:55,722 main.py:51] epoch 2154, training loss: 14210.81, average training loss: 13371.18, base loss: 21650.59
[INFO 2017-06-28 18:49:57,148 main.py:51] epoch 2155, training loss: 14401.75, average training loss: 13371.60, base loss: 21651.41
[INFO 2017-06-28 18:49:58,506 main.py:51] epoch 2156, training loss: 13496.42, average training loss: 13371.32, base loss: 21652.63
[INFO 2017-06-28 18:49:59,941 main.py:51] epoch 2157, training loss: 13513.36, average training loss: 13371.03, base loss: 21653.56
[INFO 2017-06-28 18:50:01,233 main.py:51] epoch 2158, training loss: 13893.54, average training loss: 13370.51, base loss: 21652.60
[INFO 2017-06-28 18:50:02,506 main.py:51] epoch 2159, training loss: 12083.88, average training loss: 13368.93, base loss: 21652.97
[INFO 2017-06-28 18:50:03,920 main.py:51] epoch 2160, training loss: 12288.72, average training loss: 13367.78, base loss: 21651.75
[INFO 2017-06-28 18:50:05,258 main.py:51] epoch 2161, training loss: 13822.09, average training loss: 13368.27, base loss: 21653.87
[INFO 2017-06-28 18:50:06,488 main.py:51] epoch 2162, training loss: 14388.52, average training loss: 13368.67, base loss: 21655.72
[INFO 2017-06-28 18:50:07,775 main.py:51] epoch 2163, training loss: 12557.74, average training loss: 13367.39, base loss: 21654.52
[INFO 2017-06-28 18:50:08,990 main.py:51] epoch 2164, training loss: 11938.46, average training loss: 13366.64, base loss: 21653.26
[INFO 2017-06-28 18:50:10,334 main.py:51] epoch 2165, training loss: 13093.08, average training loss: 13364.60, base loss: 21652.95
[INFO 2017-06-28 18:50:11,662 main.py:51] epoch 2166, training loss: 12933.54, average training loss: 13364.47, base loss: 21656.49
[INFO 2017-06-28 18:50:12,975 main.py:51] epoch 2167, training loss: 13195.75, average training loss: 13363.75, base loss: 21657.13
[INFO 2017-06-28 18:50:14,243 main.py:51] epoch 2168, training loss: 13670.48, average training loss: 13364.23, base loss: 21657.86
[INFO 2017-06-28 18:50:15,529 main.py:51] epoch 2169, training loss: 14081.49, average training loss: 13363.59, base loss: 21656.02
[INFO 2017-06-28 18:50:17,000 main.py:51] epoch 2170, training loss: 13899.14, average training loss: 13363.58, base loss: 21657.23
[INFO 2017-06-28 18:50:18,318 main.py:51] epoch 2171, training loss: 14044.11, average training loss: 13363.94, base loss: 21659.37
[INFO 2017-06-28 18:50:19,675 main.py:51] epoch 2172, training loss: 12613.51, average training loss: 13360.16, base loss: 21657.06
[INFO 2017-06-28 18:50:20,912 main.py:51] epoch 2173, training loss: 12847.86, average training loss: 13358.86, base loss: 21656.40
[INFO 2017-06-28 18:50:22,215 main.py:51] epoch 2174, training loss: 13114.25, average training loss: 13358.04, base loss: 21654.00
[INFO 2017-06-28 18:50:23,513 main.py:51] epoch 2175, training loss: 12849.89, average training loss: 13356.41, base loss: 21653.67
[INFO 2017-06-28 18:50:24,856 main.py:51] epoch 2176, training loss: 12866.02, average training loss: 13355.43, base loss: 21652.70
[INFO 2017-06-28 18:50:26,157 main.py:51] epoch 2177, training loss: 13898.00, average training loss: 13356.80, base loss: 21652.78
[INFO 2017-06-28 18:50:27,404 main.py:51] epoch 2178, training loss: 13100.97, average training loss: 13356.81, base loss: 21654.59
[INFO 2017-06-28 18:50:28,702 main.py:51] epoch 2179, training loss: 13784.70, average training loss: 13354.94, base loss: 21655.88
[INFO 2017-06-28 18:50:30,054 main.py:51] epoch 2180, training loss: 13568.40, average training loss: 13353.67, base loss: 21654.94
[INFO 2017-06-28 18:50:31,335 main.py:51] epoch 2181, training loss: 12080.83, average training loss: 13351.87, base loss: 21652.02
[INFO 2017-06-28 18:50:32,647 main.py:51] epoch 2182, training loss: 12756.48, average training loss: 13351.40, base loss: 21652.40
[INFO 2017-06-28 18:50:34,024 main.py:51] epoch 2183, training loss: 14541.44, average training loss: 13353.58, base loss: 21657.10
[INFO 2017-06-28 18:50:35,372 main.py:51] epoch 2184, training loss: 12927.39, average training loss: 13352.52, base loss: 21657.52
[INFO 2017-06-28 18:50:36,799 main.py:51] epoch 2185, training loss: 13625.18, average training loss: 13352.64, base loss: 21658.08
[INFO 2017-06-28 18:50:38,105 main.py:51] epoch 2186, training loss: 12294.06, average training loss: 13349.76, base loss: 21654.82
[INFO 2017-06-28 18:50:39,447 main.py:51] epoch 2187, training loss: 13986.46, average training loss: 13350.00, base loss: 21654.82
[INFO 2017-06-28 18:50:40,834 main.py:51] epoch 2188, training loss: 12689.71, average training loss: 13348.06, base loss: 21652.62
[INFO 2017-06-28 18:50:42,044 main.py:51] epoch 2189, training loss: 13080.10, average training loss: 13348.11, base loss: 21653.64
[INFO 2017-06-28 18:50:43,343 main.py:51] epoch 2190, training loss: 12728.79, average training loss: 13346.60, base loss: 21652.24
[INFO 2017-06-28 18:50:44,681 main.py:51] epoch 2191, training loss: 13727.50, average training loss: 13346.15, base loss: 21652.50
[INFO 2017-06-28 18:50:45,941 main.py:51] epoch 2192, training loss: 13133.39, average training loss: 13345.40, base loss: 21651.18
[INFO 2017-06-28 18:50:47,261 main.py:51] epoch 2193, training loss: 12571.19, average training loss: 13345.30, base loss: 21651.01
[INFO 2017-06-28 18:50:48,654 main.py:51] epoch 2194, training loss: 13533.84, average training loss: 13344.29, base loss: 21651.49
[INFO 2017-06-28 18:50:49,931 main.py:51] epoch 2195, training loss: 11531.48, average training loss: 13343.49, base loss: 21648.54
[INFO 2017-06-28 18:50:51,257 main.py:51] epoch 2196, training loss: 13699.10, average training loss: 13343.55, base loss: 21647.52
[INFO 2017-06-28 18:50:52,572 main.py:51] epoch 2197, training loss: 12354.51, average training loss: 13341.77, base loss: 21645.24
[INFO 2017-06-28 18:50:53,790 main.py:51] epoch 2198, training loss: 15616.37, average training loss: 13343.93, base loss: 21650.80
[INFO 2017-06-28 18:50:55,063 main.py:51] epoch 2199, training loss: 11433.96, average training loss: 13341.76, base loss: 21649.42
[INFO 2017-06-28 18:50:55,063 main.py:53] epoch 2199, testing
[INFO 2017-06-28 18:51:01,358 main.py:105] average testing loss: 14088.09, base loss: 22051.26
[INFO 2017-06-28 18:51:01,358 main.py:106] improve_loss: 7963.16, improve_percent: 0.36
[INFO 2017-06-28 18:51:01,359 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:51:02,628 main.py:51] epoch 2200, training loss: 14751.47, average training loss: 13344.16, base loss: 21652.52
[INFO 2017-06-28 18:51:03,913 main.py:51] epoch 2201, training loss: 13873.15, average training loss: 13344.02, base loss: 21652.33
[INFO 2017-06-28 18:51:05,268 main.py:51] epoch 2202, training loss: 12627.89, average training loss: 13343.01, base loss: 21651.12
[INFO 2017-06-28 18:51:06,752 main.py:51] epoch 2203, training loss: 11604.59, average training loss: 13342.10, base loss: 21650.32
[INFO 2017-06-28 18:51:08,051 main.py:51] epoch 2204, training loss: 14175.75, average training loss: 13342.14, base loss: 21650.71
[INFO 2017-06-28 18:51:09,275 main.py:51] epoch 2205, training loss: 13100.34, average training loss: 13341.89, base loss: 21651.19
[INFO 2017-06-28 18:51:10,526 main.py:51] epoch 2206, training loss: 12200.05, average training loss: 13339.27, base loss: 21648.22
[INFO 2017-06-28 18:51:11,961 main.py:51] epoch 2207, training loss: 13084.49, average training loss: 13339.50, base loss: 21648.64
[INFO 2017-06-28 18:51:13,306 main.py:51] epoch 2208, training loss: 12732.33, average training loss: 13338.92, base loss: 21647.42
[INFO 2017-06-28 18:51:14,585 main.py:51] epoch 2209, training loss: 13001.87, average training loss: 13338.18, base loss: 21647.95
[INFO 2017-06-28 18:51:15,988 main.py:51] epoch 2210, training loss: 13345.38, average training loss: 13338.01, base loss: 21645.24
[INFO 2017-06-28 18:51:17,286 main.py:51] epoch 2211, training loss: 13001.39, average training loss: 13336.79, base loss: 21644.65
[INFO 2017-06-28 18:51:18,571 main.py:51] epoch 2212, training loss: 15400.75, average training loss: 13338.15, base loss: 21647.02
[INFO 2017-06-28 18:51:19,865 main.py:51] epoch 2213, training loss: 13460.79, average training loss: 13338.69, base loss: 21650.07
[INFO 2017-06-28 18:51:21,211 main.py:51] epoch 2214, training loss: 13247.47, average training loss: 13339.79, base loss: 21653.44
[INFO 2017-06-28 18:51:22,454 main.py:51] epoch 2215, training loss: 12782.45, average training loss: 13339.46, base loss: 21653.91
[INFO 2017-06-28 18:51:23,728 main.py:51] epoch 2216, training loss: 13883.34, average training loss: 13339.16, base loss: 21654.25
[INFO 2017-06-28 18:51:25,104 main.py:51] epoch 2217, training loss: 13599.11, average training loss: 13337.86, base loss: 21653.15
[INFO 2017-06-28 18:51:26,394 main.py:51] epoch 2218, training loss: 13880.72, average training loss: 13337.02, base loss: 21652.13
[INFO 2017-06-28 18:51:27,653 main.py:51] epoch 2219, training loss: 12844.04, average training loss: 13336.87, base loss: 21651.50
[INFO 2017-06-28 18:51:28,970 main.py:51] epoch 2220, training loss: 15416.32, average training loss: 13339.82, base loss: 21656.96
[INFO 2017-06-28 18:51:30,222 main.py:51] epoch 2221, training loss: 12945.67, average training loss: 13339.03, base loss: 21656.73
[INFO 2017-06-28 18:51:31,545 main.py:51] epoch 2222, training loss: 12562.47, average training loss: 13338.51, base loss: 21657.26
[INFO 2017-06-28 18:51:32,827 main.py:51] epoch 2223, training loss: 12192.30, average training loss: 13336.81, base loss: 21654.53
[INFO 2017-06-28 18:51:34,139 main.py:51] epoch 2224, training loss: 12585.34, average training loss: 13336.02, base loss: 21653.39
[INFO 2017-06-28 18:51:35,416 main.py:51] epoch 2225, training loss: 11601.80, average training loss: 13333.13, base loss: 21650.19
[INFO 2017-06-28 18:51:36,707 main.py:51] epoch 2226, training loss: 13928.67, average training loss: 13333.25, base loss: 21652.44
[INFO 2017-06-28 18:51:37,961 main.py:51] epoch 2227, training loss: 13224.51, average training loss: 13333.25, base loss: 21653.91
[INFO 2017-06-28 18:51:39,254 main.py:51] epoch 2228, training loss: 13813.59, average training loss: 13334.40, base loss: 21656.35
[INFO 2017-06-28 18:51:40,517 main.py:51] epoch 2229, training loss: 13828.08, average training loss: 13332.49, base loss: 21652.88
[INFO 2017-06-28 18:51:41,812 main.py:51] epoch 2230, training loss: 11912.57, average training loss: 13330.99, base loss: 21654.38
[INFO 2017-06-28 18:51:43,171 main.py:51] epoch 2231, training loss: 12104.58, average training loss: 13329.44, base loss: 21650.83
[INFO 2017-06-28 18:51:44,480 main.py:51] epoch 2232, training loss: 13875.22, average training loss: 13329.95, base loss: 21653.06
[INFO 2017-06-28 18:51:45,897 main.py:51] epoch 2233, training loss: 13833.12, average training loss: 13329.55, base loss: 21653.40
[INFO 2017-06-28 18:51:47,255 main.py:51] epoch 2234, training loss: 13592.68, average training loss: 13330.30, base loss: 21655.18
[INFO 2017-06-28 18:51:48,566 main.py:51] epoch 2235, training loss: 13562.99, average training loss: 13331.00, base loss: 21657.58
[INFO 2017-06-28 18:51:49,881 main.py:51] epoch 2236, training loss: 11917.09, average training loss: 13328.11, base loss: 21652.94
[INFO 2017-06-28 18:51:51,134 main.py:51] epoch 2237, training loss: 14974.37, average training loss: 13330.30, base loss: 21655.18
[INFO 2017-06-28 18:51:52,363 main.py:51] epoch 2238, training loss: 12432.40, average training loss: 13328.50, base loss: 21651.83
[INFO 2017-06-28 18:51:53,624 main.py:51] epoch 2239, training loss: 15165.89, average training loss: 13330.63, base loss: 21652.22
[INFO 2017-06-28 18:51:54,897 main.py:51] epoch 2240, training loss: 14038.59, average training loss: 13330.70, base loss: 21653.70
[INFO 2017-06-28 18:51:56,185 main.py:51] epoch 2241, training loss: 12559.91, average training loss: 13330.43, base loss: 21652.83
[INFO 2017-06-28 18:51:57,492 main.py:51] epoch 2242, training loss: 15902.81, average training loss: 13333.53, base loss: 21656.13
[INFO 2017-06-28 18:51:58,743 main.py:51] epoch 2243, training loss: 13054.62, average training loss: 13333.04, base loss: 21657.85
[INFO 2017-06-28 18:52:00,093 main.py:51] epoch 2244, training loss: 14608.16, average training loss: 13333.33, base loss: 21657.55
[INFO 2017-06-28 18:52:01,427 main.py:51] epoch 2245, training loss: 11420.73, average training loss: 13330.98, base loss: 21655.55
[INFO 2017-06-28 18:52:02,703 main.py:51] epoch 2246, training loss: 13227.18, average training loss: 13331.99, base loss: 21659.07
[INFO 2017-06-28 18:52:04,024 main.py:51] epoch 2247, training loss: 13486.26, average training loss: 13331.29, base loss: 21660.62
[INFO 2017-06-28 18:52:05,401 main.py:51] epoch 2248, training loss: 12390.25, average training loss: 13329.55, base loss: 21659.31
[INFO 2017-06-28 18:52:06,594 main.py:51] epoch 2249, training loss: 12946.10, average training loss: 13327.85, base loss: 21659.04
[INFO 2017-06-28 18:52:07,815 main.py:51] epoch 2250, training loss: 13616.16, average training loss: 13326.96, base loss: 21655.28
[INFO 2017-06-28 18:52:09,136 main.py:51] epoch 2251, training loss: 13829.46, average training loss: 13327.20, base loss: 21658.82
[INFO 2017-06-28 18:52:10,474 main.py:51] epoch 2252, training loss: 11905.52, average training loss: 13326.47, base loss: 21657.97
[INFO 2017-06-28 18:52:11,822 main.py:51] epoch 2253, training loss: 13565.80, average training loss: 13326.29, base loss: 21656.94
[INFO 2017-06-28 18:52:13,107 main.py:51] epoch 2254, training loss: 14304.10, average training loss: 13325.18, base loss: 21657.19
[INFO 2017-06-28 18:52:14,352 main.py:51] epoch 2255, training loss: 12376.20, average training loss: 13324.05, base loss: 21656.14
[INFO 2017-06-28 18:52:15,670 main.py:51] epoch 2256, training loss: 12869.84, average training loss: 13324.24, base loss: 21654.98
[INFO 2017-06-28 18:52:16,962 main.py:51] epoch 2257, training loss: 13372.52, average training loss: 13324.16, base loss: 21651.31
[INFO 2017-06-28 18:52:18,236 main.py:51] epoch 2258, training loss: 13393.83, average training loss: 13324.34, base loss: 21650.68
[INFO 2017-06-28 18:52:19,496 main.py:51] epoch 2259, training loss: 13757.84, average training loss: 13324.61, base loss: 21652.63
[INFO 2017-06-28 18:52:20,742 main.py:51] epoch 2260, training loss: 14065.82, average training loss: 13323.23, base loss: 21651.59
[INFO 2017-06-28 18:52:22,058 main.py:51] epoch 2261, training loss: 14292.71, average training loss: 13323.93, base loss: 21652.31
[INFO 2017-06-28 18:52:23,320 main.py:51] epoch 2262, training loss: 13470.18, average training loss: 13323.75, base loss: 21651.39
[INFO 2017-06-28 18:52:24,624 main.py:51] epoch 2263, training loss: 11968.67, average training loss: 13322.60, base loss: 21649.46
[INFO 2017-06-28 18:52:25,868 main.py:51] epoch 2264, training loss: 12575.15, average training loss: 13320.82, base loss: 21649.12
[INFO 2017-06-28 18:52:27,140 main.py:51] epoch 2265, training loss: 14240.07, average training loss: 13321.55, base loss: 21649.68
[INFO 2017-06-28 18:52:28,380 main.py:51] epoch 2266, training loss: 11706.01, average training loss: 13318.18, base loss: 21646.29
[INFO 2017-06-28 18:52:29,636 main.py:51] epoch 2267, training loss: 12075.75, average training loss: 13315.67, base loss: 21643.63
[INFO 2017-06-28 18:52:30,910 main.py:51] epoch 2268, training loss: 13713.21, average training loss: 13316.77, base loss: 21645.76
[INFO 2017-06-28 18:52:32,191 main.py:51] epoch 2269, training loss: 11954.20, average training loss: 13314.48, base loss: 21641.16
[INFO 2017-06-28 18:52:33,440 main.py:51] epoch 2270, training loss: 11968.25, average training loss: 13314.50, base loss: 21643.28
[INFO 2017-06-28 18:52:34,627 main.py:51] epoch 2271, training loss: 14455.16, average training loss: 13316.75, base loss: 21645.71
[INFO 2017-06-28 18:52:36,004 main.py:51] epoch 2272, training loss: 13329.15, average training loss: 13316.16, base loss: 21645.08
[INFO 2017-06-28 18:52:37,310 main.py:51] epoch 2273, training loss: 11845.31, average training loss: 13315.53, base loss: 21644.67
[INFO 2017-06-28 18:52:38,617 main.py:51] epoch 2274, training loss: 12275.43, average training loss: 13314.42, base loss: 21645.29
[INFO 2017-06-28 18:52:39,960 main.py:51] epoch 2275, training loss: 12957.65, average training loss: 13314.53, base loss: 21644.40
[INFO 2017-06-28 18:52:41,379 main.py:51] epoch 2276, training loss: 13679.05, average training loss: 13313.87, base loss: 21644.52
[INFO 2017-06-28 18:52:42,693 main.py:51] epoch 2277, training loss: 15544.46, average training loss: 13315.33, base loss: 21645.70
[INFO 2017-06-28 18:52:43,962 main.py:51] epoch 2278, training loss: 14103.20, average training loss: 13315.59, base loss: 21644.35
[INFO 2017-06-28 18:52:45,220 main.py:51] epoch 2279, training loss: 13932.17, average training loss: 13313.15, base loss: 21641.59
[INFO 2017-06-28 18:52:46,550 main.py:51] epoch 2280, training loss: 14081.23, average training loss: 13313.58, base loss: 21643.11
[INFO 2017-06-28 18:52:47,871 main.py:51] epoch 2281, training loss: 13785.17, average training loss: 13312.82, base loss: 21645.03
[INFO 2017-06-28 18:52:49,218 main.py:51] epoch 2282, training loss: 13033.14, average training loss: 13310.71, base loss: 21642.16
[INFO 2017-06-28 18:52:50,620 main.py:51] epoch 2283, training loss: 13519.56, average training loss: 13309.28, base loss: 21640.80
[INFO 2017-06-28 18:52:51,912 main.py:51] epoch 2284, training loss: 11893.66, average training loss: 13307.95, base loss: 21639.18
[INFO 2017-06-28 18:52:53,147 main.py:51] epoch 2285, training loss: 13082.98, average training loss: 13307.14, base loss: 21637.97
[INFO 2017-06-28 18:52:54,450 main.py:51] epoch 2286, training loss: 13895.19, average training loss: 13306.47, base loss: 21636.41
[INFO 2017-06-28 18:52:55,793 main.py:51] epoch 2287, training loss: 13252.19, average training loss: 13303.47, base loss: 21633.94
[INFO 2017-06-28 18:52:57,130 main.py:51] epoch 2288, training loss: 12207.99, average training loss: 13302.62, base loss: 21633.09
[INFO 2017-06-28 18:52:58,456 main.py:51] epoch 2289, training loss: 11532.33, average training loss: 13300.07, base loss: 21628.70
[INFO 2017-06-28 18:52:59,773 main.py:51] epoch 2290, training loss: 11596.98, average training loss: 13297.08, base loss: 21624.63
[INFO 2017-06-28 18:53:01,242 main.py:51] epoch 2291, training loss: 12434.55, average training loss: 13296.40, base loss: 21624.06
[INFO 2017-06-28 18:53:02,610 main.py:51] epoch 2292, training loss: 13336.85, average training loss: 13296.52, base loss: 21624.42
[INFO 2017-06-28 18:53:03,992 main.py:51] epoch 2293, training loss: 13778.01, average training loss: 13295.57, base loss: 21624.97
[INFO 2017-06-28 18:53:05,306 main.py:51] epoch 2294, training loss: 12073.02, average training loss: 13294.28, base loss: 21624.47
[INFO 2017-06-28 18:53:06,634 main.py:51] epoch 2295, training loss: 13524.97, average training loss: 13292.80, base loss: 21625.23
[INFO 2017-06-28 18:53:08,022 main.py:51] epoch 2296, training loss: 12325.49, average training loss: 13291.57, base loss: 21621.11
[INFO 2017-06-28 18:53:09,405 main.py:51] epoch 2297, training loss: 12480.55, average training loss: 13290.26, base loss: 21618.94
[INFO 2017-06-28 18:53:10,784 main.py:51] epoch 2298, training loss: 11668.10, average training loss: 13289.82, base loss: 21618.48
[INFO 2017-06-28 18:53:11,957 main.py:51] epoch 2299, training loss: 13178.15, average training loss: 13290.10, base loss: 21620.17
[INFO 2017-06-28 18:53:11,958 main.py:53] epoch 2299, testing
[INFO 2017-06-28 18:53:18,017 main.py:105] average testing loss: 15173.31, base loss: 22688.09
[INFO 2017-06-28 18:53:18,017 main.py:106] improve_loss: 7514.77, improve_percent: 0.33
[INFO 2017-06-28 18:53:18,018 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:53:19,205 main.py:51] epoch 2300, training loss: 14596.41, average training loss: 13291.77, base loss: 21622.00
[INFO 2017-06-28 18:53:20,494 main.py:51] epoch 2301, training loss: 14381.88, average training loss: 13290.97, base loss: 21621.02
[INFO 2017-06-28 18:53:21,796 main.py:51] epoch 2302, training loss: 10876.12, average training loss: 13289.09, base loss: 21619.26
[INFO 2017-06-28 18:53:23,142 main.py:51] epoch 2303, training loss: 11946.57, average training loss: 13287.89, base loss: 21621.50
[INFO 2017-06-28 18:53:24,480 main.py:51] epoch 2304, training loss: 12605.71, average training loss: 13288.08, base loss: 21623.48
[INFO 2017-06-28 18:53:25,757 main.py:51] epoch 2305, training loss: 13629.17, average training loss: 13288.66, base loss: 21625.98
[INFO 2017-06-28 18:53:27,150 main.py:51] epoch 2306, training loss: 14324.63, average training loss: 13289.65, base loss: 21626.80
[INFO 2017-06-28 18:53:28,428 main.py:51] epoch 2307, training loss: 14085.09, average training loss: 13290.16, base loss: 21627.18
[INFO 2017-06-28 18:53:29,693 main.py:51] epoch 2308, training loss: 12736.87, average training loss: 13289.49, base loss: 21625.88
[INFO 2017-06-28 18:53:31,082 main.py:51] epoch 2309, training loss: 13180.33, average training loss: 13290.38, base loss: 21629.51
[INFO 2017-06-28 18:53:32,306 main.py:51] epoch 2310, training loss: 12756.57, average training loss: 13289.82, base loss: 21631.04
[INFO 2017-06-28 18:53:33,631 main.py:51] epoch 2311, training loss: 12436.58, average training loss: 13287.18, base loss: 21629.31
[INFO 2017-06-28 18:53:34,884 main.py:51] epoch 2312, training loss: 14034.38, average training loss: 13286.32, base loss: 21628.55
[INFO 2017-06-28 18:53:36,083 main.py:51] epoch 2313, training loss: 14059.55, average training loss: 13286.91, base loss: 21628.87
[INFO 2017-06-28 18:53:37,398 main.py:51] epoch 2314, training loss: 13941.39, average training loss: 13288.29, base loss: 21631.53
[INFO 2017-06-28 18:53:38,666 main.py:51] epoch 2315, training loss: 12385.01, average training loss: 13286.84, base loss: 21628.48
[INFO 2017-06-28 18:53:39,897 main.py:51] epoch 2316, training loss: 12942.82, average training loss: 13286.46, base loss: 21628.18
[INFO 2017-06-28 18:53:41,146 main.py:51] epoch 2317, training loss: 13148.88, average training loss: 13286.84, base loss: 21630.25
[INFO 2017-06-28 18:53:42,471 main.py:51] epoch 2318, training loss: 12469.53, average training loss: 13286.82, base loss: 21630.52
[INFO 2017-06-28 18:53:43,878 main.py:51] epoch 2319, training loss: 12081.91, average training loss: 13285.67, base loss: 21631.66
[INFO 2017-06-28 18:53:45,099 main.py:51] epoch 2320, training loss: 12811.77, average training loss: 13284.96, base loss: 21630.66
[INFO 2017-06-28 18:53:46,410 main.py:51] epoch 2321, training loss: 12476.32, average training loss: 13282.01, base loss: 21626.67
[INFO 2017-06-28 18:53:47,639 main.py:51] epoch 2322, training loss: 12863.37, average training loss: 13280.71, base loss: 21625.60
[INFO 2017-06-28 18:53:48,939 main.py:51] epoch 2323, training loss: 13545.70, average training loss: 13281.98, base loss: 21627.90
[INFO 2017-06-28 18:53:50,274 main.py:51] epoch 2324, training loss: 14560.94, average training loss: 13283.50, base loss: 21630.38
[INFO 2017-06-28 18:53:51,707 main.py:51] epoch 2325, training loss: 12530.89, average training loss: 13283.09, base loss: 21630.72
[INFO 2017-06-28 18:53:53,073 main.py:51] epoch 2326, training loss: 12512.21, average training loss: 13281.65, base loss: 21629.34
[INFO 2017-06-28 18:53:54,313 main.py:51] epoch 2327, training loss: 11941.58, average training loss: 13280.19, base loss: 21628.64
[INFO 2017-06-28 18:53:55,569 main.py:51] epoch 2328, training loss: 13210.37, average training loss: 13280.94, base loss: 21628.88
[INFO 2017-06-28 18:53:56,844 main.py:51] epoch 2329, training loss: 14628.15, average training loss: 13280.84, base loss: 21630.38
[INFO 2017-06-28 18:53:58,189 main.py:51] epoch 2330, training loss: 12427.42, average training loss: 13280.47, base loss: 21630.21
[INFO 2017-06-28 18:53:59,465 main.py:51] epoch 2331, training loss: 12627.48, average training loss: 13279.24, base loss: 21631.20
[INFO 2017-06-28 18:54:00,685 main.py:51] epoch 2332, training loss: 13058.80, average training loss: 13278.88, base loss: 21629.39
[INFO 2017-06-28 18:54:02,027 main.py:51] epoch 2333, training loss: 14239.64, average training loss: 13280.70, base loss: 21633.43
[INFO 2017-06-28 18:54:03,366 main.py:51] epoch 2334, training loss: 13686.41, average training loss: 13281.11, base loss: 21633.24
[INFO 2017-06-28 18:54:04,916 main.py:51] epoch 2335, training loss: 12595.90, average training loss: 13279.62, base loss: 21632.36
[INFO 2017-06-28 18:54:06,202 main.py:51] epoch 2336, training loss: 14445.87, average training loss: 13280.30, base loss: 21634.15
[INFO 2017-06-28 18:54:07,564 main.py:51] epoch 2337, training loss: 12507.66, average training loss: 13279.08, base loss: 21633.13
[INFO 2017-06-28 18:54:08,888 main.py:51] epoch 2338, training loss: 12050.61, average training loss: 13278.48, base loss: 21631.85
[INFO 2017-06-28 18:54:10,233 main.py:51] epoch 2339, training loss: 12491.14, average training loss: 13278.22, base loss: 21631.27
[INFO 2017-06-28 18:54:11,474 main.py:51] epoch 2340, training loss: 13837.78, average training loss: 13279.37, base loss: 21632.08
[INFO 2017-06-28 18:54:12,831 main.py:51] epoch 2341, training loss: 14042.58, average training loss: 13280.77, base loss: 21633.45
[INFO 2017-06-28 18:54:14,071 main.py:51] epoch 2342, training loss: 12295.05, average training loss: 13279.70, base loss: 21634.61
[INFO 2017-06-28 18:54:15,357 main.py:51] epoch 2343, training loss: 11542.66, average training loss: 13278.07, base loss: 21633.12
[INFO 2017-06-28 18:54:16,732 main.py:51] epoch 2344, training loss: 14742.98, average training loss: 13278.41, base loss: 21635.23
[INFO 2017-06-28 18:54:17,957 main.py:51] epoch 2345, training loss: 12194.57, average training loss: 13277.32, base loss: 21634.61
[INFO 2017-06-28 18:54:19,277 main.py:51] epoch 2346, training loss: 12458.17, average training loss: 13276.01, base loss: 21632.30
[INFO 2017-06-28 18:54:20,697 main.py:51] epoch 2347, training loss: 13763.15, average training loss: 13275.43, base loss: 21632.53
[INFO 2017-06-28 18:54:22,001 main.py:51] epoch 2348, training loss: 13686.42, average training loss: 13276.50, base loss: 21632.64
[INFO 2017-06-28 18:54:23,441 main.py:51] epoch 2349, training loss: 12474.13, average training loss: 13274.59, base loss: 21629.85
[INFO 2017-06-28 18:54:24,876 main.py:51] epoch 2350, training loss: 14252.61, average training loss: 13274.35, base loss: 21631.53
[INFO 2017-06-28 18:54:26,151 main.py:51] epoch 2351, training loss: 13902.81, average training loss: 13273.65, base loss: 21630.50
[INFO 2017-06-28 18:54:27,524 main.py:51] epoch 2352, training loss: 13155.12, average training loss: 13272.91, base loss: 21630.37
[INFO 2017-06-28 18:54:28,874 main.py:51] epoch 2353, training loss: 13926.63, average training loss: 13273.07, base loss: 21630.83
[INFO 2017-06-28 18:54:30,204 main.py:51] epoch 2354, training loss: 12409.11, average training loss: 13272.55, base loss: 21631.42
[INFO 2017-06-28 18:54:31,498 main.py:51] epoch 2355, training loss: 12908.16, average training loss: 13272.44, base loss: 21629.75
[INFO 2017-06-28 18:54:32,728 main.py:51] epoch 2356, training loss: 14054.57, average training loss: 13274.54, base loss: 21633.69
[INFO 2017-06-28 18:54:33,991 main.py:51] epoch 2357, training loss: 13676.66, average training loss: 13275.76, base loss: 21634.32
[INFO 2017-06-28 18:54:35,241 main.py:51] epoch 2358, training loss: 11666.55, average training loss: 13273.59, base loss: 21632.76
[INFO 2017-06-28 18:54:36,545 main.py:51] epoch 2359, training loss: 13656.36, average training loss: 13273.52, base loss: 21632.58
[INFO 2017-06-28 18:54:37,885 main.py:51] epoch 2360, training loss: 13272.64, average training loss: 13273.05, base loss: 21630.65
[INFO 2017-06-28 18:54:39,240 main.py:51] epoch 2361, training loss: 12219.39, average training loss: 13272.53, base loss: 21630.12
[INFO 2017-06-28 18:54:40,617 main.py:51] epoch 2362, training loss: 14181.50, average training loss: 13274.63, base loss: 21631.69
[INFO 2017-06-28 18:54:41,839 main.py:51] epoch 2363, training loss: 12355.50, average training loss: 13271.62, base loss: 21629.46
[INFO 2017-06-28 18:54:43,205 main.py:51] epoch 2364, training loss: 13431.45, average training loss: 13270.06, base loss: 21625.78
[INFO 2017-06-28 18:54:44,580 main.py:51] epoch 2365, training loss: 12760.95, average training loss: 13268.70, base loss: 21626.84
[INFO 2017-06-28 18:54:46,010 main.py:51] epoch 2366, training loss: 12762.21, average training loss: 13267.47, base loss: 21626.83
[INFO 2017-06-28 18:54:47,274 main.py:51] epoch 2367, training loss: 14149.33, average training loss: 13268.42, base loss: 21629.09
[INFO 2017-06-28 18:54:48,522 main.py:51] epoch 2368, training loss: 12723.64, average training loss: 13267.56, base loss: 21628.59
[INFO 2017-06-28 18:54:49,924 main.py:51] epoch 2369, training loss: 12718.58, average training loss: 13268.21, base loss: 21632.39
[INFO 2017-06-28 18:54:51,209 main.py:51] epoch 2370, training loss: 12455.47, average training loss: 13267.80, base loss: 21634.02
[INFO 2017-06-28 18:54:52,492 main.py:51] epoch 2371, training loss: 14022.42, average training loss: 13267.96, base loss: 21632.71
[INFO 2017-06-28 18:54:53,762 main.py:51] epoch 2372, training loss: 13071.38, average training loss: 13268.25, base loss: 21634.68
[INFO 2017-06-28 18:54:55,036 main.py:51] epoch 2373, training loss: 12308.01, average training loss: 13267.10, base loss: 21633.95
[INFO 2017-06-28 18:54:56,381 main.py:51] epoch 2374, training loss: 11563.51, average training loss: 13264.72, base loss: 21632.25
[INFO 2017-06-28 18:54:57,648 main.py:51] epoch 2375, training loss: 13158.52, average training loss: 13265.35, base loss: 21633.22
[INFO 2017-06-28 18:54:58,974 main.py:51] epoch 2376, training loss: 13482.99, average training loss: 13264.91, base loss: 21633.37
[INFO 2017-06-28 18:55:00,311 main.py:51] epoch 2377, training loss: 13567.11, average training loss: 13264.84, base loss: 21634.85
[INFO 2017-06-28 18:55:01,797 main.py:51] epoch 2378, training loss: 12361.32, average training loss: 13264.68, base loss: 21634.07
[INFO 2017-06-28 18:55:03,092 main.py:51] epoch 2379, training loss: 13274.47, average training loss: 13263.82, base loss: 21633.11
[INFO 2017-06-28 18:55:04,446 main.py:51] epoch 2380, training loss: 13010.60, average training loss: 13261.76, base loss: 21630.92
[INFO 2017-06-28 18:55:05,674 main.py:51] epoch 2381, training loss: 11685.25, average training loss: 13259.59, base loss: 21629.36
[INFO 2017-06-28 18:55:07,066 main.py:51] epoch 2382, training loss: 13180.33, average training loss: 13259.49, base loss: 21630.35
[INFO 2017-06-28 18:55:08,419 main.py:51] epoch 2383, training loss: 11474.43, average training loss: 13258.52, base loss: 21627.46
[INFO 2017-06-28 18:55:09,789 main.py:51] epoch 2384, training loss: 12566.91, average training loss: 13257.17, base loss: 21627.74
[INFO 2017-06-28 18:55:11,156 main.py:51] epoch 2385, training loss: 13660.66, average training loss: 13255.57, base loss: 21628.01
[INFO 2017-06-28 18:55:12,481 main.py:51] epoch 2386, training loss: 14500.07, average training loss: 13257.41, base loss: 21629.84
[INFO 2017-06-28 18:55:13,775 main.py:51] epoch 2387, training loss: 12445.70, average training loss: 13255.27, base loss: 21626.90
[INFO 2017-06-28 18:55:15,118 main.py:51] epoch 2388, training loss: 14480.96, average training loss: 13256.16, base loss: 21627.74
[INFO 2017-06-28 18:55:16,451 main.py:51] epoch 2389, training loss: 12057.14, average training loss: 13254.79, base loss: 21625.82
[INFO 2017-06-28 18:55:17,679 main.py:51] epoch 2390, training loss: 13406.01, average training loss: 13255.59, base loss: 21626.58
[INFO 2017-06-28 18:55:19,096 main.py:51] epoch 2391, training loss: 12212.88, average training loss: 13255.40, base loss: 21626.50
[INFO 2017-06-28 18:55:20,468 main.py:51] epoch 2392, training loss: 14308.49, average training loss: 13255.94, base loss: 21627.15
[INFO 2017-06-28 18:55:21,764 main.py:51] epoch 2393, training loss: 14447.85, average training loss: 13257.19, base loss: 21630.36
[INFO 2017-06-28 18:55:23,052 main.py:51] epoch 2394, training loss: 13497.13, average training loss: 13255.14, base loss: 21629.57
[INFO 2017-06-28 18:55:24,246 main.py:51] epoch 2395, training loss: 13442.33, average training loss: 13255.69, base loss: 21630.79
[INFO 2017-06-28 18:55:25,552 main.py:51] epoch 2396, training loss: 13426.68, average training loss: 13255.80, base loss: 21630.49
[INFO 2017-06-28 18:55:26,941 main.py:51] epoch 2397, training loss: 14678.26, average training loss: 13256.96, base loss: 21629.92
[INFO 2017-06-28 18:55:28,280 main.py:51] epoch 2398, training loss: 13385.15, average training loss: 13257.61, base loss: 21633.03
[INFO 2017-06-28 18:55:29,505 main.py:51] epoch 2399, training loss: 12312.79, average training loss: 13257.09, base loss: 21631.85
[INFO 2017-06-28 18:55:29,505 main.py:53] epoch 2399, testing
[INFO 2017-06-28 18:55:35,508 main.py:105] average testing loss: 13818.46, base loss: 21343.91
[INFO 2017-06-28 18:55:35,508 main.py:106] improve_loss: 7525.44, improve_percent: 0.35
[INFO 2017-06-28 18:55:35,509 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:55:36,870 main.py:51] epoch 2400, training loss: 13597.49, average training loss: 13257.58, base loss: 21634.46
[INFO 2017-06-28 18:55:38,105 main.py:51] epoch 2401, training loss: 12936.84, average training loss: 13256.13, base loss: 21632.68
[INFO 2017-06-28 18:55:39,563 main.py:51] epoch 2402, training loss: 14461.85, average training loss: 13257.74, base loss: 21634.65
[INFO 2017-06-28 18:55:40,890 main.py:51] epoch 2403, training loss: 14605.05, average training loss: 13257.56, base loss: 21635.61
[INFO 2017-06-28 18:55:42,228 main.py:51] epoch 2404, training loss: 12606.96, average training loss: 13257.01, base loss: 21635.58
[INFO 2017-06-28 18:55:43,560 main.py:51] epoch 2405, training loss: 12698.82, average training loss: 13257.46, base loss: 21636.78
[INFO 2017-06-28 18:55:44,905 main.py:51] epoch 2406, training loss: 12413.76, average training loss: 13256.59, base loss: 21635.42
[INFO 2017-06-28 18:55:46,191 main.py:51] epoch 2407, training loss: 14798.03, average training loss: 13258.33, base loss: 21637.80
[INFO 2017-06-28 18:55:47,501 main.py:51] epoch 2408, training loss: 16522.39, average training loss: 13261.26, base loss: 21642.30
[INFO 2017-06-28 18:55:48,837 main.py:51] epoch 2409, training loss: 12396.92, average training loss: 13259.09, base loss: 21639.27
[INFO 2017-06-28 18:55:50,173 main.py:51] epoch 2410, training loss: 12506.49, average training loss: 13257.84, base loss: 21637.14
[INFO 2017-06-28 18:55:51,487 main.py:51] epoch 2411, training loss: 12634.92, average training loss: 13257.70, base loss: 21636.08
[INFO 2017-06-28 18:55:52,801 main.py:51] epoch 2412, training loss: 15978.02, average training loss: 13260.43, base loss: 21637.86
[INFO 2017-06-28 18:55:54,203 main.py:51] epoch 2413, training loss: 12818.07, average training loss: 13260.02, base loss: 21638.34
[INFO 2017-06-28 18:55:55,430 main.py:51] epoch 2414, training loss: 12279.05, average training loss: 13259.72, base loss: 21639.42
[INFO 2017-06-28 18:55:56,803 main.py:51] epoch 2415, training loss: 12446.10, average training loss: 13258.37, base loss: 21638.59
[INFO 2017-06-28 18:55:58,067 main.py:51] epoch 2416, training loss: 14308.16, average training loss: 13259.49, base loss: 21640.73
[INFO 2017-06-28 18:55:59,364 main.py:51] epoch 2417, training loss: 11873.17, average training loss: 13257.70, base loss: 21640.21
[INFO 2017-06-28 18:56:00,586 main.py:51] epoch 2418, training loss: 14177.71, average training loss: 13258.98, base loss: 21643.34
[INFO 2017-06-28 18:56:01,792 main.py:51] epoch 2419, training loss: 13992.07, average training loss: 13261.08, base loss: 21644.81
[INFO 2017-06-28 18:56:03,075 main.py:51] epoch 2420, training loss: 11087.69, average training loss: 13256.32, base loss: 21638.71
[INFO 2017-06-28 18:56:04,329 main.py:51] epoch 2421, training loss: 11196.47, average training loss: 13252.31, base loss: 21633.38
[INFO 2017-06-28 18:56:05,612 main.py:51] epoch 2422, training loss: 12437.72, average training loss: 13250.18, base loss: 21630.49
[INFO 2017-06-28 18:56:06,990 main.py:51] epoch 2423, training loss: 14812.74, average training loss: 13251.77, base loss: 21632.21
[INFO 2017-06-28 18:56:08,197 main.py:51] epoch 2424, training loss: 13376.38, average training loss: 13251.24, base loss: 21632.88
[INFO 2017-06-28 18:56:09,642 main.py:51] epoch 2425, training loss: 14323.36, average training loss: 13251.87, base loss: 21635.48
[INFO 2017-06-28 18:56:11,013 main.py:51] epoch 2426, training loss: 13051.60, average training loss: 13251.60, base loss: 21637.10
[INFO 2017-06-28 18:56:12,352 main.py:51] epoch 2427, training loss: 13281.23, average training loss: 13250.86, base loss: 21637.55
[INFO 2017-06-28 18:56:13,661 main.py:51] epoch 2428, training loss: 13939.23, average training loss: 13252.05, base loss: 21639.15
[INFO 2017-06-28 18:56:14,993 main.py:51] epoch 2429, training loss: 12707.38, average training loss: 13251.64, base loss: 21640.57
[INFO 2017-06-28 18:56:16,195 main.py:51] epoch 2430, training loss: 13078.82, average training loss: 13251.10, base loss: 21639.08
[INFO 2017-06-28 18:56:17,496 main.py:51] epoch 2431, training loss: 13180.78, average training loss: 13251.84, base loss: 21641.95
[INFO 2017-06-28 18:56:18,808 main.py:51] epoch 2432, training loss: 13059.20, average training loss: 13251.65, base loss: 21642.90
[INFO 2017-06-28 18:56:20,052 main.py:51] epoch 2433, training loss: 11524.71, average training loss: 13248.77, base loss: 21639.98
[INFO 2017-06-28 18:56:21,417 main.py:51] epoch 2434, training loss: 14049.43, average training loss: 13249.19, base loss: 21640.17
[INFO 2017-06-28 18:56:22,698 main.py:51] epoch 2435, training loss: 12586.66, average training loss: 13248.22, base loss: 21639.23
[INFO 2017-06-28 18:56:24,106 main.py:51] epoch 2436, training loss: 12800.61, average training loss: 13247.65, base loss: 21639.17
[INFO 2017-06-28 18:56:25,386 main.py:51] epoch 2437, training loss: 11386.70, average training loss: 13243.70, base loss: 21635.64
[INFO 2017-06-28 18:56:26,647 main.py:51] epoch 2438, training loss: 15487.53, average training loss: 13246.56, base loss: 21636.81
[INFO 2017-06-28 18:56:27,870 main.py:51] epoch 2439, training loss: 12663.99, average training loss: 13245.52, base loss: 21636.20
[INFO 2017-06-28 18:56:29,281 main.py:51] epoch 2440, training loss: 12902.25, average training loss: 13245.51, base loss: 21637.98
[INFO 2017-06-28 18:56:30,505 main.py:51] epoch 2441, training loss: 13229.61, average training loss: 13244.91, base loss: 21638.22
[INFO 2017-06-28 18:56:31,912 main.py:51] epoch 2442, training loss: 13867.70, average training loss: 13245.62, base loss: 21640.43
[INFO 2017-06-28 18:56:33,342 main.py:51] epoch 2443, training loss: 14022.12, average training loss: 13245.23, base loss: 21639.45
[INFO 2017-06-28 18:56:34,639 main.py:51] epoch 2444, training loss: 13030.69, average training loss: 13244.08, base loss: 21638.85
[INFO 2017-06-28 18:56:35,935 main.py:51] epoch 2445, training loss: 13036.89, average training loss: 13244.29, base loss: 21638.79
[INFO 2017-06-28 18:56:37,250 main.py:51] epoch 2446, training loss: 14305.25, average training loss: 13245.61, base loss: 21638.70
[INFO 2017-06-28 18:56:38,607 main.py:51] epoch 2447, training loss: 15386.25, average training loss: 13248.15, base loss: 21641.21
[INFO 2017-06-28 18:56:39,849 main.py:51] epoch 2448, training loss: 12311.28, average training loss: 13246.58, base loss: 21639.39
[INFO 2017-06-28 18:56:41,085 main.py:51] epoch 2449, training loss: 13553.92, average training loss: 13245.34, base loss: 21639.15
[INFO 2017-06-28 18:56:42,329 main.py:51] epoch 2450, training loss: 12914.78, average training loss: 13244.45, base loss: 21637.56
[INFO 2017-06-28 18:56:43,706 main.py:51] epoch 2451, training loss: 12152.82, average training loss: 13244.14, base loss: 21635.69
[INFO 2017-06-28 18:56:44,991 main.py:51] epoch 2452, training loss: 11275.12, average training loss: 13243.57, base loss: 21634.67
[INFO 2017-06-28 18:56:46,369 main.py:51] epoch 2453, training loss: 12382.24, average training loss: 13244.24, base loss: 21636.17
[INFO 2017-06-28 18:56:47,722 main.py:51] epoch 2454, training loss: 12643.20, average training loss: 13243.63, base loss: 21635.46
[INFO 2017-06-28 18:56:48,995 main.py:51] epoch 2455, training loss: 13092.86, average training loss: 13243.77, base loss: 21637.37
[INFO 2017-06-28 18:56:50,352 main.py:51] epoch 2456, training loss: 13100.60, average training loss: 13242.54, base loss: 21637.26
[INFO 2017-06-28 18:56:51,652 main.py:51] epoch 2457, training loss: 13081.58, average training loss: 13242.31, base loss: 21635.88
[INFO 2017-06-28 18:56:52,992 main.py:51] epoch 2458, training loss: 12623.00, average training loss: 13241.79, base loss: 21635.55
[INFO 2017-06-28 18:56:54,298 main.py:51] epoch 2459, training loss: 13879.36, average training loss: 13242.69, base loss: 21638.59
[INFO 2017-06-28 18:56:55,571 main.py:51] epoch 2460, training loss: 14064.98, average training loss: 13244.29, base loss: 21642.08
[INFO 2017-06-28 18:56:56,921 main.py:51] epoch 2461, training loss: 12392.37, average training loss: 13242.17, base loss: 21640.95
[INFO 2017-06-28 18:56:58,160 main.py:51] epoch 2462, training loss: 13577.77, average training loss: 13243.02, base loss: 21641.97
[INFO 2017-06-28 18:56:59,495 main.py:51] epoch 2463, training loss: 12954.50, average training loss: 13243.80, base loss: 21643.12
[INFO 2017-06-28 18:57:00,778 main.py:51] epoch 2464, training loss: 13086.21, average training loss: 13243.61, base loss: 21644.13
[INFO 2017-06-28 18:57:02,179 main.py:51] epoch 2465, training loss: 13733.42, average training loss: 13244.73, base loss: 21646.77
[INFO 2017-06-28 18:57:03,560 main.py:51] epoch 2466, training loss: 11867.54, average training loss: 13244.35, base loss: 21646.05
[INFO 2017-06-28 18:57:04,856 main.py:51] epoch 2467, training loss: 11738.52, average training loss: 13241.64, base loss: 21643.64
[INFO 2017-06-28 18:57:06,263 main.py:51] epoch 2468, training loss: 12264.05, average training loss: 13238.69, base loss: 21639.31
[INFO 2017-06-28 18:57:07,591 main.py:51] epoch 2469, training loss: 13432.30, average training loss: 13239.72, base loss: 21641.27
[INFO 2017-06-28 18:57:08,959 main.py:51] epoch 2470, training loss: 13098.12, average training loss: 13239.36, base loss: 21640.46
[INFO 2017-06-28 18:57:10,179 main.py:51] epoch 2471, training loss: 13064.96, average training loss: 13239.93, base loss: 21640.28
[INFO 2017-06-28 18:57:11,460 main.py:51] epoch 2472, training loss: 14349.59, average training loss: 13241.52, base loss: 21642.19
[INFO 2017-06-28 18:57:12,835 main.py:51] epoch 2473, training loss: 13887.67, average training loss: 13242.78, base loss: 21641.23
[INFO 2017-06-28 18:57:14,240 main.py:51] epoch 2474, training loss: 13223.78, average training loss: 13242.68, base loss: 21640.89
[INFO 2017-06-28 18:57:15,489 main.py:51] epoch 2475, training loss: 11948.51, average training loss: 13239.89, base loss: 21640.42
[INFO 2017-06-28 18:57:16,741 main.py:51] epoch 2476, training loss: 13112.07, average training loss: 13240.84, base loss: 21641.03
[INFO 2017-06-28 18:57:18,071 main.py:51] epoch 2477, training loss: 12707.08, average training loss: 13239.79, base loss: 21641.07
[INFO 2017-06-28 18:57:19,437 main.py:51] epoch 2478, training loss: 14618.07, average training loss: 13240.24, base loss: 21640.37
[INFO 2017-06-28 18:57:20,796 main.py:51] epoch 2479, training loss: 12461.24, average training loss: 13240.20, base loss: 21641.79
[INFO 2017-06-28 18:57:22,033 main.py:51] epoch 2480, training loss: 13579.91, average training loss: 13237.29, base loss: 21636.35
[INFO 2017-06-28 18:57:23,473 main.py:51] epoch 2481, training loss: 14244.19, average training loss: 13239.34, base loss: 21639.51
[INFO 2017-06-28 18:57:24,824 main.py:51] epoch 2482, training loss: 13722.70, average training loss: 13240.93, base loss: 21643.73
[INFO 2017-06-28 18:57:26,192 main.py:51] epoch 2483, training loss: 13255.68, average training loss: 13241.88, base loss: 21644.35
[INFO 2017-06-28 18:57:27,475 main.py:51] epoch 2484, training loss: 12850.95, average training loss: 13241.24, base loss: 21644.35
[INFO 2017-06-28 18:57:28,796 main.py:51] epoch 2485, training loss: 12777.04, average training loss: 13241.04, base loss: 21644.85
[INFO 2017-06-28 18:57:30,074 main.py:51] epoch 2486, training loss: 14126.17, average training loss: 13242.33, base loss: 21647.10
[INFO 2017-06-28 18:57:31,353 main.py:51] epoch 2487, training loss: 13208.08, average training loss: 13243.19, base loss: 21651.48
[INFO 2017-06-28 18:57:32,642 main.py:51] epoch 2488, training loss: 13207.81, average training loss: 13244.85, base loss: 21653.91
[INFO 2017-06-28 18:57:33,929 main.py:51] epoch 2489, training loss: 12826.93, average training loss: 13244.20, base loss: 21654.29
[INFO 2017-06-28 18:57:35,382 main.py:51] epoch 2490, training loss: 12359.28, average training loss: 13243.38, base loss: 21653.49
[INFO 2017-06-28 18:57:36,633 main.py:51] epoch 2491, training loss: 13174.82, average training loss: 13241.37, base loss: 21653.70
[INFO 2017-06-28 18:57:37,975 main.py:51] epoch 2492, training loss: 12955.47, average training loss: 13242.08, base loss: 21655.57
[INFO 2017-06-28 18:57:39,282 main.py:51] epoch 2493, training loss: 12735.52, average training loss: 13242.86, base loss: 21658.82
[INFO 2017-06-28 18:57:40,760 main.py:51] epoch 2494, training loss: 12941.60, average training loss: 13241.54, base loss: 21658.89
[INFO 2017-06-28 18:57:42,065 main.py:51] epoch 2495, training loss: 13318.46, average training loss: 13240.58, base loss: 21658.35
[INFO 2017-06-28 18:57:43,365 main.py:51] epoch 2496, training loss: 12022.75, average training loss: 13238.96, base loss: 21658.28
[INFO 2017-06-28 18:57:44,690 main.py:51] epoch 2497, training loss: 12658.40, average training loss: 13237.62, base loss: 21657.54
[INFO 2017-06-28 18:57:46,039 main.py:51] epoch 2498, training loss: 13535.62, average training loss: 13236.18, base loss: 21657.35
[INFO 2017-06-28 18:57:47,351 main.py:51] epoch 2499, training loss: 12970.50, average training loss: 13235.06, base loss: 21656.08
[INFO 2017-06-28 18:57:47,352 main.py:53] epoch 2499, testing
[INFO 2017-06-28 18:57:53,368 main.py:105] average testing loss: 14805.52, base loss: 22559.97
[INFO 2017-06-28 18:57:53,368 main.py:106] improve_loss: 7754.45, improve_percent: 0.34
[INFO 2017-06-28 18:57:53,368 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 18:57:54,791 main.py:51] epoch 2500, training loss: 12927.46, average training loss: 13234.58, base loss: 21656.68
[INFO 2017-06-28 18:57:56,166 main.py:51] epoch 2501, training loss: 14151.59, average training loss: 13234.94, base loss: 21661.74
[INFO 2017-06-28 18:57:57,480 main.py:51] epoch 2502, training loss: 13384.23, average training loss: 13233.74, base loss: 21660.53
[INFO 2017-06-28 18:57:58,838 main.py:51] epoch 2503, training loss: 13742.65, average training loss: 13233.44, base loss: 21660.02
[INFO 2017-06-28 18:58:00,105 main.py:51] epoch 2504, training loss: 12751.44, average training loss: 13232.47, base loss: 21657.79
[INFO 2017-06-28 18:58:01,318 main.py:51] epoch 2505, training loss: 13122.50, average training loss: 13232.36, base loss: 21658.00
[INFO 2017-06-28 18:58:02,572 main.py:51] epoch 2506, training loss: 12630.69, average training loss: 13231.38, base loss: 21657.04
[INFO 2017-06-28 18:58:03,929 main.py:51] epoch 2507, training loss: 12461.63, average training loss: 13230.65, base loss: 21655.01
[INFO 2017-06-28 18:58:05,199 main.py:51] epoch 2508, training loss: 12733.67, average training loss: 13230.27, base loss: 21654.63
[INFO 2017-06-28 18:58:06,467 main.py:51] epoch 2509, training loss: 13490.71, average training loss: 13230.56, base loss: 21654.12
[INFO 2017-06-28 18:58:07,779 main.py:51] epoch 2510, training loss: 12901.39, average training loss: 13228.52, base loss: 21650.90
[INFO 2017-06-28 18:58:09,057 main.py:51] epoch 2511, training loss: 13263.39, average training loss: 13227.62, base loss: 21650.43
[INFO 2017-06-28 18:58:10,479 main.py:51] epoch 2512, training loss: 13542.28, average training loss: 13228.60, base loss: 21650.33
[INFO 2017-06-28 18:58:11,781 main.py:51] epoch 2513, training loss: 13805.58, average training loss: 13229.24, base loss: 21652.05
[INFO 2017-06-28 18:58:13,112 main.py:51] epoch 2514, training loss: 12298.92, average training loss: 13228.47, base loss: 21650.85
[INFO 2017-06-28 18:58:14,402 main.py:51] epoch 2515, training loss: 12667.75, average training loss: 13228.49, base loss: 21651.98
[INFO 2017-06-28 18:58:15,633 main.py:51] epoch 2516, training loss: 14272.66, average training loss: 13230.34, base loss: 21654.15
[INFO 2017-06-28 18:58:16,988 main.py:51] epoch 2517, training loss: 13532.72, average training loss: 13231.97, base loss: 21658.62
[INFO 2017-06-28 18:58:18,343 main.py:51] epoch 2518, training loss: 13477.58, average training loss: 13231.24, base loss: 21657.38
[INFO 2017-06-28 18:58:19,660 main.py:51] epoch 2519, training loss: 13551.10, average training loss: 13231.94, base loss: 21658.83
[INFO 2017-06-28 18:58:21,054 main.py:51] epoch 2520, training loss: 14826.84, average training loss: 13233.81, base loss: 21661.19
[INFO 2017-06-28 18:58:22,390 main.py:51] epoch 2521, training loss: 13458.62, average training loss: 13233.03, base loss: 21659.49
[INFO 2017-06-28 18:58:23,790 main.py:51] epoch 2522, training loss: 10963.95, average training loss: 13231.69, base loss: 21658.02
[INFO 2017-06-28 18:58:25,133 main.py:51] epoch 2523, training loss: 11956.74, average training loss: 13230.14, base loss: 21657.14
[INFO 2017-06-28 18:58:26,387 main.py:51] epoch 2524, training loss: 14551.73, average training loss: 13232.97, base loss: 21659.83
[INFO 2017-06-28 18:58:27,843 main.py:51] epoch 2525, training loss: 13607.66, average training loss: 13233.58, base loss: 21660.23
[INFO 2017-06-28 18:58:29,270 main.py:51] epoch 2526, training loss: 13001.33, average training loss: 13231.17, base loss: 21658.22
[INFO 2017-06-28 18:58:30,628 main.py:51] epoch 2527, training loss: 14263.67, average training loss: 13231.45, base loss: 21657.63
[INFO 2017-06-28 18:58:31,996 main.py:51] epoch 2528, training loss: 12461.31, average training loss: 13229.35, base loss: 21656.06
[INFO 2017-06-28 18:58:33,324 main.py:51] epoch 2529, training loss: 12667.53, average training loss: 13228.22, base loss: 21656.48
[INFO 2017-06-28 18:58:34,634 main.py:51] epoch 2530, training loss: 12503.91, average training loss: 13227.63, base loss: 21655.92
[INFO 2017-06-28 18:58:35,998 main.py:51] epoch 2531, training loss: 13147.19, average training loss: 13228.76, base loss: 21659.23
[INFO 2017-06-28 18:58:37,441 main.py:51] epoch 2532, training loss: 12954.70, average training loss: 13225.99, base loss: 21660.20
[INFO 2017-06-28 18:58:38,825 main.py:51] epoch 2533, training loss: 12171.23, average training loss: 13223.89, base loss: 21657.32
[INFO 2017-06-28 18:58:40,119 main.py:51] epoch 2534, training loss: 13254.49, average training loss: 13225.26, base loss: 21660.00
[INFO 2017-06-28 18:58:41,443 main.py:51] epoch 2535, training loss: 13385.35, average training loss: 13225.94, base loss: 21660.00
[INFO 2017-06-28 18:58:42,854 main.py:51] epoch 2536, training loss: 12938.35, average training loss: 13225.37, base loss: 21660.35
[INFO 2017-06-28 18:58:44,128 main.py:51] epoch 2537, training loss: 12139.07, average training loss: 13225.10, base loss: 21661.53
[INFO 2017-06-28 18:58:45,511 main.py:51] epoch 2538, training loss: 13604.87, average training loss: 13225.23, base loss: 21663.58
[INFO 2017-06-28 18:58:46,949 main.py:51] epoch 2539, training loss: 12009.12, average training loss: 13223.56, base loss: 21661.58
[INFO 2017-06-28 18:58:48,299 main.py:51] epoch 2540, training loss: 13439.29, average training loss: 13223.13, base loss: 21659.26
[INFO 2017-06-28 18:58:49,735 main.py:51] epoch 2541, training loss: 14191.18, average training loss: 13223.43, base loss: 21662.66
[INFO 2017-06-28 18:58:50,953 main.py:51] epoch 2542, training loss: 12665.82, average training loss: 13223.33, base loss: 21663.01
[INFO 2017-06-28 18:58:52,374 main.py:51] epoch 2543, training loss: 11807.80, average training loss: 13220.38, base loss: 21660.07
[INFO 2017-06-28 18:58:53,831 main.py:51] epoch 2544, training loss: 14199.53, average training loss: 13220.72, base loss: 21660.99
[INFO 2017-06-28 18:58:55,021 main.py:51] epoch 2545, training loss: 12135.22, average training loss: 13219.22, base loss: 21659.40
[INFO 2017-06-28 18:58:56,481 main.py:51] epoch 2546, training loss: 12458.71, average training loss: 13218.40, base loss: 21658.48
[INFO 2017-06-28 18:58:57,885 main.py:51] epoch 2547, training loss: 13640.93, average training loss: 13217.92, base loss: 21656.36
[INFO 2017-06-28 18:58:59,241 main.py:51] epoch 2548, training loss: 11591.15, average training loss: 13215.76, base loss: 21654.14
[INFO 2017-06-28 18:59:00,559 main.py:51] epoch 2549, training loss: 12302.11, average training loss: 13214.79, base loss: 21653.00
[INFO 2017-06-28 18:59:01,888 main.py:51] epoch 2550, training loss: 12726.07, average training loss: 13213.75, base loss: 21650.73
[INFO 2017-06-28 18:59:03,164 main.py:51] epoch 2551, training loss: 12900.76, average training loss: 13212.61, base loss: 21650.91
[INFO 2017-06-28 18:59:04,567 main.py:51] epoch 2552, training loss: 13173.11, average training loss: 13209.38, base loss: 21647.08
[INFO 2017-06-28 18:59:06,033 main.py:51] epoch 2553, training loss: 13446.31, average training loss: 13209.61, base loss: 21646.29
[INFO 2017-06-28 18:59:07,347 main.py:51] epoch 2554, training loss: 13021.17, average training loss: 13208.99, base loss: 21644.93
[INFO 2017-06-28 18:59:08,655 main.py:51] epoch 2555, training loss: 12946.76, average training loss: 13209.01, base loss: 21644.72
[INFO 2017-06-28 18:59:09,872 main.py:51] epoch 2556, training loss: 12564.83, average training loss: 13209.56, base loss: 21645.50
[INFO 2017-06-28 18:59:11,082 main.py:51] epoch 2557, training loss: 12036.91, average training loss: 13208.74, base loss: 21645.64
[INFO 2017-06-28 18:59:12,489 main.py:51] epoch 2558, training loss: 14768.20, average training loss: 13211.03, base loss: 21649.43
[INFO 2017-06-28 18:59:13,933 main.py:51] epoch 2559, training loss: 14562.29, average training loss: 13212.77, base loss: 21652.76
[INFO 2017-06-28 18:59:15,232 main.py:51] epoch 2560, training loss: 12526.17, average training loss: 13213.90, base loss: 21653.65
[INFO 2017-06-28 18:59:16,577 main.py:51] epoch 2561, training loss: 11574.64, average training loss: 13209.72, base loss: 21650.81
[INFO 2017-06-28 18:59:17,894 main.py:51] epoch 2562, training loss: 12766.91, average training loss: 13210.42, base loss: 21653.53
[INFO 2017-06-28 18:59:19,100 main.py:51] epoch 2563, training loss: 12700.83, average training loss: 13210.00, base loss: 21655.68
[INFO 2017-06-28 18:59:20,318 main.py:51] epoch 2564, training loss: 13403.94, average training loss: 13210.38, base loss: 21656.21
[INFO 2017-06-28 18:59:21,903 main.py:51] epoch 2565, training loss: 13701.43, average training loss: 13210.34, base loss: 21656.65
[INFO 2017-06-28 18:59:23,172 main.py:51] epoch 2566, training loss: 12426.09, average training loss: 13209.17, base loss: 21654.59
[INFO 2017-06-28 18:59:24,520 main.py:51] epoch 2567, training loss: 13316.63, average training loss: 13207.77, base loss: 21652.79
[INFO 2017-06-28 18:59:25,787 main.py:51] epoch 2568, training loss: 12482.88, average training loss: 13206.70, base loss: 21653.03
[INFO 2017-06-28 18:59:27,297 main.py:51] epoch 2569, training loss: 12699.20, average training loss: 13203.65, base loss: 21649.36
[INFO 2017-06-28 18:59:28,653 main.py:51] epoch 2570, training loss: 13144.50, average training loss: 13202.80, base loss: 21645.94
[INFO 2017-06-28 18:59:29,999 main.py:51] epoch 2571, training loss: 12889.40, average training loss: 13203.38, base loss: 21645.95
[INFO 2017-06-28 18:59:31,324 main.py:51] epoch 2572, training loss: 14563.17, average training loss: 13205.10, base loss: 21646.70
[INFO 2017-06-28 18:59:32,631 main.py:51] epoch 2573, training loss: 12091.96, average training loss: 13204.15, base loss: 21645.47
[INFO 2017-06-28 18:59:34,054 main.py:51] epoch 2574, training loss: 12709.52, average training loss: 13203.17, base loss: 21645.10
[INFO 2017-06-28 18:59:35,514 main.py:51] epoch 2575, training loss: 13486.72, average training loss: 13204.61, base loss: 21648.80
[INFO 2017-06-28 18:59:36,907 main.py:51] epoch 2576, training loss: 12728.28, average training loss: 13202.62, base loss: 21646.14
[INFO 2017-06-28 18:59:38,225 main.py:51] epoch 2577, training loss: 12518.28, average training loss: 13200.91, base loss: 21645.25
[INFO 2017-06-28 18:59:39,496 main.py:51] epoch 2578, training loss: 12587.61, average training loss: 13201.16, base loss: 21645.23
[INFO 2017-06-28 18:59:40,729 main.py:51] epoch 2579, training loss: 13543.96, average training loss: 13201.94, base loss: 21646.15
[INFO 2017-06-28 18:59:42,026 main.py:51] epoch 2580, training loss: 14058.45, average training loss: 13203.38, base loss: 21648.67
[INFO 2017-06-28 18:59:43,332 main.py:51] epoch 2581, training loss: 14813.50, average training loss: 13203.75, base loss: 21648.17
[INFO 2017-06-28 18:59:44,608 main.py:51] epoch 2582, training loss: 11903.60, average training loss: 13201.73, base loss: 21645.55
[INFO 2017-06-28 18:59:45,969 main.py:51] epoch 2583, training loss: 14050.32, average training loss: 13203.48, base loss: 21649.12
[INFO 2017-06-28 18:59:47,326 main.py:51] epoch 2584, training loss: 11977.97, average training loss: 13200.97, base loss: 21644.97
[INFO 2017-06-28 18:59:48,538 main.py:51] epoch 2585, training loss: 11474.77, average training loss: 13201.05, base loss: 21644.20
[INFO 2017-06-28 18:59:49,862 main.py:51] epoch 2586, training loss: 13808.96, average training loss: 13202.64, base loss: 21647.08
[INFO 2017-06-28 18:59:51,271 main.py:51] epoch 2587, training loss: 12578.06, average training loss: 13202.49, base loss: 21647.95
[INFO 2017-06-28 18:59:52,535 main.py:51] epoch 2588, training loss: 12136.43, average training loss: 13201.84, base loss: 21648.16
[INFO 2017-06-28 18:59:53,767 main.py:51] epoch 2589, training loss: 13179.76, average training loss: 13201.66, base loss: 21649.02
[INFO 2017-06-28 18:59:55,019 main.py:51] epoch 2590, training loss: 12515.07, average training loss: 13201.34, base loss: 21649.27
[INFO 2017-06-28 18:59:56,299 main.py:51] epoch 2591, training loss: 12943.05, average training loss: 13200.21, base loss: 21647.99
[INFO 2017-06-28 18:59:57,611 main.py:51] epoch 2592, training loss: 12664.48, average training loss: 13199.08, base loss: 21647.04
[INFO 2017-06-28 18:59:58,957 main.py:51] epoch 2593, training loss: 12071.59, average training loss: 13197.46, base loss: 21646.87
[INFO 2017-06-28 19:00:00,258 main.py:51] epoch 2594, training loss: 11291.04, average training loss: 13195.47, base loss: 21643.46
[INFO 2017-06-28 19:00:01,476 main.py:51] epoch 2595, training loss: 11951.98, average training loss: 13193.39, base loss: 21641.93
[INFO 2017-06-28 19:00:02,709 main.py:51] epoch 2596, training loss: 13509.09, average training loss: 13193.91, base loss: 21643.60
[INFO 2017-06-28 19:00:04,013 main.py:51] epoch 2597, training loss: 12828.45, average training loss: 13191.41, base loss: 21640.64
[INFO 2017-06-28 19:00:05,334 main.py:51] epoch 2598, training loss: 13378.52, average training loss: 13192.16, base loss: 21641.49
[INFO 2017-06-28 19:00:06,568 main.py:51] epoch 2599, training loss: 13781.62, average training loss: 13191.76, base loss: 21642.45
[INFO 2017-06-28 19:00:06,568 main.py:53] epoch 2599, testing
[INFO 2017-06-28 19:00:12,662 main.py:105] average testing loss: 14900.57, base loss: 23121.53
[INFO 2017-06-28 19:00:12,662 main.py:106] improve_loss: 8220.95, improve_percent: 0.36
[INFO 2017-06-28 19:00:12,663 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 19:00:13,936 main.py:51] epoch 2600, training loss: 12547.10, average training loss: 13191.01, base loss: 21643.86
[INFO 2017-06-28 19:00:15,128 main.py:51] epoch 2601, training loss: 12260.69, average training loss: 13190.54, base loss: 21644.66
[INFO 2017-06-28 19:00:16,444 main.py:51] epoch 2602, training loss: 13426.28, average training loss: 13189.74, base loss: 21644.37
[INFO 2017-06-28 19:00:17,660 main.py:51] epoch 2603, training loss: 13759.80, average training loss: 13189.23, base loss: 21643.48
[INFO 2017-06-28 19:00:19,049 main.py:51] epoch 2604, training loss: 13089.07, average training loss: 13188.14, base loss: 21643.89
[INFO 2017-06-28 19:00:20,308 main.py:51] epoch 2605, training loss: 12061.96, average training loss: 13187.29, base loss: 21643.97
[INFO 2017-06-28 19:00:21,584 main.py:51] epoch 2606, training loss: 11939.50, average training loss: 13186.86, base loss: 21643.63
[INFO 2017-06-28 19:00:23,033 main.py:51] epoch 2607, training loss: 12872.17, average training loss: 13186.03, base loss: 21644.47
[INFO 2017-06-28 19:00:24,371 main.py:51] epoch 2608, training loss: 13493.18, average training loss: 13186.14, base loss: 21644.41
[INFO 2017-06-28 19:00:25,731 main.py:51] epoch 2609, training loss: 13766.23, average training loss: 13186.95, base loss: 21645.59
[INFO 2017-06-28 19:00:27,019 main.py:51] epoch 2610, training loss: 13509.42, average training loss: 13186.93, base loss: 21645.91
[INFO 2017-06-28 19:00:28,303 main.py:51] epoch 2611, training loss: 12931.04, average training loss: 13186.00, base loss: 21644.86
[INFO 2017-06-28 19:00:29,578 main.py:51] epoch 2612, training loss: 12542.15, average training loss: 13184.96, base loss: 21643.63
[INFO 2017-06-28 19:00:31,147 main.py:51] epoch 2613, training loss: 13804.15, average training loss: 13184.36, base loss: 21643.76
[INFO 2017-06-28 19:00:32,427 main.py:51] epoch 2614, training loss: 14400.90, average training loss: 13184.37, base loss: 21646.82
[INFO 2017-06-28 19:00:33,888 main.py:51] epoch 2615, training loss: 12849.88, average training loss: 13183.81, base loss: 21646.74
[INFO 2017-06-28 19:00:35,233 main.py:51] epoch 2616, training loss: 13360.24, average training loss: 13184.65, base loss: 21648.26
[INFO 2017-06-28 19:00:36,524 main.py:51] epoch 2617, training loss: 14485.40, average training loss: 13184.65, base loss: 21650.29
[INFO 2017-06-28 19:00:37,811 main.py:51] epoch 2618, training loss: 13158.08, average training loss: 13184.55, base loss: 21650.11
[INFO 2017-06-28 19:00:39,150 main.py:51] epoch 2619, training loss: 14704.78, average training loss: 13186.52, base loss: 21654.54
[INFO 2017-06-28 19:00:40,410 main.py:51] epoch 2620, training loss: 13912.58, average training loss: 13187.28, base loss: 21654.36
[INFO 2017-06-28 19:00:41,755 main.py:51] epoch 2621, training loss: 12416.40, average training loss: 13184.65, base loss: 21650.10
[INFO 2017-06-28 19:00:43,289 main.py:51] epoch 2622, training loss: 12372.13, average training loss: 13182.38, base loss: 21647.87
[INFO 2017-06-28 19:00:44,533 main.py:51] epoch 2623, training loss: 13024.23, average training loss: 13182.16, base loss: 21648.03
[INFO 2017-06-28 19:00:45,945 main.py:51] epoch 2624, training loss: 13612.33, average training loss: 13182.77, base loss: 21647.98
[INFO 2017-06-28 19:00:47,275 main.py:51] epoch 2625, training loss: 11964.69, average training loss: 13182.47, base loss: 21647.92
[INFO 2017-06-28 19:00:48,743 main.py:51] epoch 2626, training loss: 13200.90, average training loss: 13181.87, base loss: 21647.48
[INFO 2017-06-28 19:00:50,258 main.py:51] epoch 2627, training loss: 13453.84, average training loss: 13181.03, base loss: 21647.35
[INFO 2017-06-28 19:00:51,786 main.py:51] epoch 2628, training loss: 13158.26, average training loss: 13180.90, base loss: 21648.13
[INFO 2017-06-28 19:00:53,340 main.py:51] epoch 2629, training loss: 13363.11, average training loss: 13181.89, base loss: 21650.76
[INFO 2017-06-28 19:00:54,841 main.py:51] epoch 2630, training loss: 12617.23, average training loss: 13178.50, base loss: 21648.12
[INFO 2017-06-28 19:00:56,154 main.py:51] epoch 2631, training loss: 13821.14, average training loss: 13179.52, base loss: 21652.33
[INFO 2017-06-28 19:00:57,443 main.py:51] epoch 2632, training loss: 13480.42, average training loss: 13179.36, base loss: 21652.16
[INFO 2017-06-28 19:00:58,877 main.py:51] epoch 2633, training loss: 12954.33, average training loss: 13179.67, base loss: 21654.85
[INFO 2017-06-28 19:01:00,331 main.py:51] epoch 2634, training loss: 12460.01, average training loss: 13177.75, base loss: 21652.13
[INFO 2017-06-28 19:01:01,594 main.py:51] epoch 2635, training loss: 12232.33, average training loss: 13177.19, base loss: 21649.86
[INFO 2017-06-28 19:01:02,858 main.py:51] epoch 2636, training loss: 11822.96, average training loss: 13175.27, base loss: 21647.57
[INFO 2017-06-28 19:01:04,176 main.py:51] epoch 2637, training loss: 16229.63, average training loss: 13176.23, base loss: 21647.77
[INFO 2017-06-28 19:01:05,469 main.py:51] epoch 2638, training loss: 13067.73, average training loss: 13176.11, base loss: 21646.61
[INFO 2017-06-28 19:01:06,828 main.py:51] epoch 2639, training loss: 14183.26, average training loss: 13175.19, base loss: 21647.32
[INFO 2017-06-28 19:01:08,248 main.py:51] epoch 2640, training loss: 15428.88, average training loss: 13175.74, base loss: 21647.84
[INFO 2017-06-28 19:01:09,743 main.py:51] epoch 2641, training loss: 13210.51, average training loss: 13175.19, base loss: 21646.68
[INFO 2017-06-28 19:01:11,174 main.py:51] epoch 2642, training loss: 13535.84, average training loss: 13176.10, base loss: 21648.89
[INFO 2017-06-28 19:01:12,787 main.py:51] epoch 2643, training loss: 13424.60, average training loss: 13175.54, base loss: 21647.22
[INFO 2017-06-28 19:01:14,082 main.py:51] epoch 2644, training loss: 13571.74, average training loss: 13175.92, base loss: 21648.08
[INFO 2017-06-28 19:01:15,331 main.py:51] epoch 2645, training loss: 12184.96, average training loss: 13174.90, base loss: 21648.11
[INFO 2017-06-28 19:01:16,747 main.py:51] epoch 2646, training loss: 12808.20, average training loss: 13174.90, base loss: 21647.73
[INFO 2017-06-28 19:01:18,076 main.py:51] epoch 2647, training loss: 11532.99, average training loss: 13171.99, base loss: 21643.22
[INFO 2017-06-28 19:01:19,528 main.py:51] epoch 2648, training loss: 13446.52, average training loss: 13171.97, base loss: 21644.46
[INFO 2017-06-28 19:01:20,845 main.py:51] epoch 2649, training loss: 12193.45, average training loss: 13171.03, base loss: 21644.04
[INFO 2017-06-28 19:01:22,176 main.py:51] epoch 2650, training loss: 13538.88, average training loss: 13172.20, base loss: 21645.59
[INFO 2017-06-28 19:01:23,734 main.py:51] epoch 2651, training loss: 13298.79, average training loss: 13171.54, base loss: 21642.62
[INFO 2017-06-28 19:01:25,059 main.py:51] epoch 2652, training loss: 12820.79, average training loss: 13171.85, base loss: 21642.20
[INFO 2017-06-28 19:01:26,369 main.py:51] epoch 2653, training loss: 13100.95, average training loss: 13171.31, base loss: 21640.60
[INFO 2017-06-28 19:01:27,724 main.py:51] epoch 2654, training loss: 12594.97, average training loss: 13171.57, base loss: 21639.71
[INFO 2017-06-28 19:01:29,173 main.py:51] epoch 2655, training loss: 12602.62, average training loss: 13171.43, base loss: 21640.64
[INFO 2017-06-28 19:01:30,638 main.py:51] epoch 2656, training loss: 14011.79, average training loss: 13172.61, base loss: 21641.91
[INFO 2017-06-28 19:01:31,874 main.py:51] epoch 2657, training loss: 13508.03, average training loss: 13173.29, base loss: 21644.91
[INFO 2017-06-28 19:01:33,269 main.py:51] epoch 2658, training loss: 12215.79, average training loss: 13172.39, base loss: 21645.06
[INFO 2017-06-28 19:01:34,742 main.py:51] epoch 2659, training loss: 11681.24, average training loss: 13170.14, base loss: 21641.06
[INFO 2017-06-28 19:01:36,140 main.py:51] epoch 2660, training loss: 11530.24, average training loss: 13169.36, base loss: 21641.79
[INFO 2017-06-28 19:01:37,469 main.py:51] epoch 2661, training loss: 12306.18, average training loss: 13169.05, base loss: 21642.06
[INFO 2017-06-28 19:01:38,802 main.py:51] epoch 2662, training loss: 12357.85, average training loss: 13168.26, base loss: 21640.53
[INFO 2017-06-28 19:01:40,193 main.py:51] epoch 2663, training loss: 13154.56, average training loss: 13168.41, base loss: 21639.97
[INFO 2017-06-28 19:01:41,520 main.py:51] epoch 2664, training loss: 12928.72, average training loss: 13168.08, base loss: 21640.57
[INFO 2017-06-28 19:01:42,856 main.py:51] epoch 2665, training loss: 12767.31, average training loss: 13167.44, base loss: 21639.88
[INFO 2017-06-28 19:01:44,195 main.py:51] epoch 2666, training loss: 12129.87, average training loss: 13166.08, base loss: 21637.82
[INFO 2017-06-28 19:01:45,381 main.py:51] epoch 2667, training loss: 13447.88, average training loss: 13165.54, base loss: 21638.17
[INFO 2017-06-28 19:01:46,699 main.py:51] epoch 2668, training loss: 12049.98, average training loss: 13162.77, base loss: 21634.32
[INFO 2017-06-28 19:01:48,045 main.py:51] epoch 2669, training loss: 11915.97, average training loss: 13162.30, base loss: 21632.78
[INFO 2017-06-28 19:01:49,272 main.py:51] epoch 2670, training loss: 14111.84, average training loss: 13162.88, base loss: 21632.20
[INFO 2017-06-28 19:01:50,511 main.py:51] epoch 2671, training loss: 11729.05, average training loss: 13161.20, base loss: 21630.74
[INFO 2017-06-28 19:01:51,845 main.py:51] epoch 2672, training loss: 13241.85, average training loss: 13161.15, base loss: 21630.83
[INFO 2017-06-28 19:01:53,212 main.py:51] epoch 2673, training loss: 12891.89, average training loss: 13160.49, base loss: 21629.51
[INFO 2017-06-28 19:01:54,680 main.py:51] epoch 2674, training loss: 12124.23, average training loss: 13159.22, base loss: 21627.74
[INFO 2017-06-28 19:01:56,172 main.py:51] epoch 2675, training loss: 13040.12, average training loss: 13158.35, base loss: 21626.08
[INFO 2017-06-28 19:01:57,503 main.py:51] epoch 2676, training loss: 13212.02, average training loss: 13158.81, base loss: 21627.11
[INFO 2017-06-28 19:01:58,720 main.py:51] epoch 2677, training loss: 13832.27, average training loss: 13158.93, base loss: 21627.81
[INFO 2017-06-28 19:02:00,102 main.py:51] epoch 2678, training loss: 11918.67, average training loss: 13157.45, base loss: 21627.01
[INFO 2017-06-28 19:02:01,545 main.py:51] epoch 2679, training loss: 12782.92, average training loss: 13158.38, base loss: 21630.74
[INFO 2017-06-28 19:02:02,856 main.py:51] epoch 2680, training loss: 13497.25, average training loss: 13158.74, base loss: 21631.72
[INFO 2017-06-28 19:02:04,146 main.py:51] epoch 2681, training loss: 11886.19, average training loss: 13157.43, base loss: 21629.24
[INFO 2017-06-28 19:02:05,463 main.py:51] epoch 2682, training loss: 13120.99, average training loss: 13158.31, base loss: 21630.48
[INFO 2017-06-28 19:02:06,828 main.py:51] epoch 2683, training loss: 13718.62, average training loss: 13160.70, base loss: 21634.04
[INFO 2017-06-28 19:02:08,029 main.py:51] epoch 2684, training loss: 12675.30, average training loss: 13159.68, base loss: 21629.84
[INFO 2017-06-28 19:02:09,622 main.py:51] epoch 2685, training loss: 11770.28, average training loss: 13159.29, base loss: 21629.47
[INFO 2017-06-28 19:02:10,888 main.py:51] epoch 2686, training loss: 13776.51, average training loss: 13160.73, base loss: 21634.08
[INFO 2017-06-28 19:02:12,320 main.py:51] epoch 2687, training loss: 13215.39, average training loss: 13160.09, base loss: 21635.87
[INFO 2017-06-28 19:02:13,812 main.py:51] epoch 2688, training loss: 12035.43, average training loss: 13158.78, base loss: 21637.37
[INFO 2017-06-28 19:02:15,393 main.py:51] epoch 2689, training loss: 12975.40, average training loss: 13158.90, base loss: 21637.19
[INFO 2017-06-28 19:02:16,759 main.py:51] epoch 2690, training loss: 12857.93, average training loss: 13159.81, base loss: 21638.71
[INFO 2017-06-28 19:02:18,425 main.py:51] epoch 2691, training loss: 14348.93, average training loss: 13160.28, base loss: 21641.25
[INFO 2017-06-28 19:02:19,749 main.py:51] epoch 2692, training loss: 13457.34, average training loss: 13161.03, base loss: 21641.26
[INFO 2017-06-28 19:02:21,040 main.py:51] epoch 2693, training loss: 12754.29, average training loss: 13159.98, base loss: 21638.81
[INFO 2017-06-28 19:02:22,467 main.py:51] epoch 2694, training loss: 12545.64, average training loss: 13159.83, base loss: 21639.18
[INFO 2017-06-28 19:02:23,725 main.py:51] epoch 2695, training loss: 13313.55, average training loss: 13159.88, base loss: 21639.26
[INFO 2017-06-28 19:02:25,050 main.py:51] epoch 2696, training loss: 12646.31, average training loss: 13159.21, base loss: 21638.97
[INFO 2017-06-28 19:02:26,731 main.py:51] epoch 2697, training loss: 13164.23, average training loss: 13159.15, base loss: 21640.20
[INFO 2017-06-28 19:02:28,090 main.py:51] epoch 2698, training loss: 11929.32, average training loss: 13157.70, base loss: 21638.81
[INFO 2017-06-28 19:02:29,507 main.py:51] epoch 2699, training loss: 13269.92, average training loss: 13158.12, base loss: 21640.03
[INFO 2017-06-28 19:02:29,507 main.py:53] epoch 2699, testing
[INFO 2017-06-28 19:02:36,112 main.py:105] average testing loss: 13710.52, base loss: 21314.83
[INFO 2017-06-28 19:02:36,113 main.py:106] improve_loss: 7604.31, improve_percent: 0.36
[INFO 2017-06-28 19:02:36,113 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 19:02:37,542 main.py:51] epoch 2700, training loss: 12216.33, average training loss: 13158.71, base loss: 21640.28
[INFO 2017-06-28 19:02:38,796 main.py:51] epoch 2701, training loss: 11950.94, average training loss: 13157.75, base loss: 21639.75
[INFO 2017-06-28 19:02:40,153 main.py:51] epoch 2702, training loss: 12922.98, average training loss: 13158.26, base loss: 21640.83
[INFO 2017-06-28 19:02:41,555 main.py:51] epoch 2703, training loss: 12536.76, average training loss: 13157.95, base loss: 21641.98
[INFO 2017-06-28 19:02:42,856 main.py:51] epoch 2704, training loss: 13974.35, average training loss: 13158.91, base loss: 21644.14
[INFO 2017-06-28 19:02:44,178 main.py:51] epoch 2705, training loss: 11591.59, average training loss: 13155.91, base loss: 21640.13
[INFO 2017-06-28 19:02:45,451 main.py:51] epoch 2706, training loss: 12231.49, average training loss: 13154.74, base loss: 21638.89
[INFO 2017-06-28 19:02:46,956 main.py:51] epoch 2707, training loss: 14364.33, average training loss: 13155.70, base loss: 21643.14
[INFO 2017-06-28 19:02:48,510 main.py:51] epoch 2708, training loss: 11716.04, average training loss: 13154.06, base loss: 21640.87
[INFO 2017-06-28 19:02:50,099 main.py:51] epoch 2709, training loss: 13491.04, average training loss: 13153.74, base loss: 21641.99
[INFO 2017-06-28 19:02:51,652 main.py:51] epoch 2710, training loss: 12476.33, average training loss: 13152.03, base loss: 21641.79
[INFO 2017-06-28 19:02:53,044 main.py:51] epoch 2711, training loss: 11696.01, average training loss: 13151.05, base loss: 21640.30
[INFO 2017-06-28 19:02:54,373 main.py:51] epoch 2712, training loss: 13467.08, average training loss: 13148.93, base loss: 21639.87
[INFO 2017-06-28 19:02:55,685 main.py:51] epoch 2713, training loss: 13756.33, average training loss: 13149.68, base loss: 21641.23
[INFO 2017-06-28 19:02:57,100 main.py:51] epoch 2714, training loss: 13035.93, average training loss: 13149.97, base loss: 21641.60
[INFO 2017-06-28 19:02:58,602 main.py:51] epoch 2715, training loss: 12386.27, average training loss: 13147.42, base loss: 21638.34
[INFO 2017-06-28 19:02:59,850 main.py:51] epoch 2716, training loss: 11833.21, average training loss: 13146.78, base loss: 21638.08
[INFO 2017-06-28 19:03:01,032 main.py:51] epoch 2717, training loss: 13107.60, average training loss: 13147.02, base loss: 21638.97
[INFO 2017-06-28 19:03:02,365 main.py:51] epoch 2718, training loss: 11937.25, average training loss: 13146.74, base loss: 21638.69
[INFO 2017-06-28 19:03:03,911 main.py:51] epoch 2719, training loss: 13044.58, average training loss: 13146.00, base loss: 21638.07
[INFO 2017-06-28 19:03:05,371 main.py:51] epoch 2720, training loss: 13977.15, average training loss: 13146.69, base loss: 21640.32
[INFO 2017-06-28 19:03:06,886 main.py:51] epoch 2721, training loss: 13662.71, average training loss: 13146.21, base loss: 21640.17
[INFO 2017-06-28 19:03:08,289 main.py:51] epoch 2722, training loss: 11631.15, average training loss: 13144.21, base loss: 21638.47
[INFO 2017-06-28 19:03:09,701 main.py:51] epoch 2723, training loss: 12676.83, average training loss: 13144.48, base loss: 21637.45
[INFO 2017-06-28 19:03:11,048 main.py:51] epoch 2724, training loss: 13038.16, average training loss: 13144.46, base loss: 21639.29
[INFO 2017-06-28 19:03:12,340 main.py:51] epoch 2725, training loss: 13175.17, average training loss: 13143.85, base loss: 21641.48
[INFO 2017-06-28 19:03:13,808 main.py:51] epoch 2726, training loss: 15811.51, average training loss: 13145.54, base loss: 21644.94
[INFO 2017-06-28 19:03:15,311 main.py:51] epoch 2727, training loss: 12941.45, average training loss: 13144.93, base loss: 21642.16
[INFO 2017-06-28 19:03:16,770 main.py:51] epoch 2728, training loss: 12319.32, average training loss: 13143.97, base loss: 21641.16
[INFO 2017-06-28 19:03:18,104 main.py:51] epoch 2729, training loss: 12194.43, average training loss: 13143.27, base loss: 21640.20
[INFO 2017-06-28 19:03:19,539 main.py:51] epoch 2730, training loss: 14113.88, average training loss: 13143.60, base loss: 21640.84
[INFO 2017-06-28 19:03:20,932 main.py:51] epoch 2731, training loss: 12615.08, average training loss: 13142.85, base loss: 21640.27
[INFO 2017-06-28 19:03:22,246 main.py:51] epoch 2732, training loss: 13253.60, average training loss: 13141.54, base loss: 21639.37
[INFO 2017-06-28 19:03:23,591 main.py:51] epoch 2733, training loss: 11783.49, average training loss: 13141.40, base loss: 21639.93
[INFO 2017-06-28 19:03:24,848 main.py:51] epoch 2734, training loss: 13737.88, average training loss: 13139.97, base loss: 21638.24
[INFO 2017-06-28 19:03:26,292 main.py:51] epoch 2735, training loss: 12830.98, average training loss: 13139.27, base loss: 21637.53
[INFO 2017-06-28 19:03:27,674 main.py:51] epoch 2736, training loss: 12969.82, average training loss: 13140.12, base loss: 21638.60
[INFO 2017-06-28 19:03:29,043 main.py:51] epoch 2737, training loss: 12774.88, average training loss: 13139.40, base loss: 21639.92
[INFO 2017-06-28 19:03:30,500 main.py:51] epoch 2738, training loss: 13035.75, average training loss: 13139.00, base loss: 21638.52
[INFO 2017-06-28 19:03:31,758 main.py:51] epoch 2739, training loss: 12982.91, average training loss: 13139.21, base loss: 21640.49
[INFO 2017-06-28 19:03:33,158 main.py:51] epoch 2740, training loss: 11929.56, average training loss: 13140.32, base loss: 21642.63
[INFO 2017-06-28 19:03:34,465 main.py:51] epoch 2741, training loss: 12800.66, average training loss: 13140.14, base loss: 21640.74
[INFO 2017-06-28 19:03:35,857 main.py:51] epoch 2742, training loss: 12586.81, average training loss: 13139.42, base loss: 21641.83
[INFO 2017-06-28 19:03:37,224 main.py:51] epoch 2743, training loss: 13212.56, average training loss: 13139.06, base loss: 21639.34
[INFO 2017-06-28 19:03:38,570 main.py:51] epoch 2744, training loss: 12458.73, average training loss: 13138.07, base loss: 21639.11
[INFO 2017-06-28 19:03:39,967 main.py:51] epoch 2745, training loss: 11816.66, average training loss: 13136.47, base loss: 21637.38
[INFO 2017-06-28 19:03:41,414 main.py:51] epoch 2746, training loss: 12072.52, average training loss: 13135.04, base loss: 21635.09
[INFO 2017-06-28 19:03:42,737 main.py:51] epoch 2747, training loss: 13149.49, average training loss: 13135.86, base loss: 21635.98
[INFO 2017-06-28 19:03:44,050 main.py:51] epoch 2748, training loss: 12938.59, average training loss: 13135.93, base loss: 21637.61
[INFO 2017-06-28 19:03:45,380 main.py:51] epoch 2749, training loss: 12636.95, average training loss: 13135.21, base loss: 21637.73
[INFO 2017-06-28 19:03:46,686 main.py:51] epoch 2750, training loss: 13390.24, average training loss: 13135.44, base loss: 21639.83
[INFO 2017-06-28 19:03:47,999 main.py:51] epoch 2751, training loss: 12992.82, average training loss: 13135.37, base loss: 21641.38
[INFO 2017-06-28 19:03:49,582 main.py:51] epoch 2752, training loss: 12788.50, average training loss: 13134.81, base loss: 21638.55
[INFO 2017-06-28 19:03:51,004 main.py:51] epoch 2753, training loss: 12934.68, average training loss: 13135.05, base loss: 21639.22
[INFO 2017-06-28 19:03:52,379 main.py:51] epoch 2754, training loss: 13051.88, average training loss: 13134.36, base loss: 21639.52
[INFO 2017-06-28 19:03:53,590 main.py:51] epoch 2755, training loss: 13366.78, average training loss: 13134.76, base loss: 21638.56
[INFO 2017-06-28 19:03:54,930 main.py:51] epoch 2756, training loss: 12143.72, average training loss: 13133.94, base loss: 21637.88
[INFO 2017-06-28 19:03:56,190 main.py:51] epoch 2757, training loss: 11733.16, average training loss: 13133.00, base loss: 21635.49
[INFO 2017-06-28 19:03:57,535 main.py:51] epoch 2758, training loss: 12260.38, average training loss: 13132.30, base loss: 21635.17
[INFO 2017-06-28 19:03:58,753 main.py:51] epoch 2759, training loss: 14435.39, average training loss: 13133.77, base loss: 21636.21
[INFO 2017-06-28 19:04:00,090 main.py:51] epoch 2760, training loss: 11621.35, average training loss: 13132.21, base loss: 21633.55
[INFO 2017-06-28 19:04:01,471 main.py:51] epoch 2761, training loss: 12953.88, average training loss: 13132.36, base loss: 21636.07
[INFO 2017-06-28 19:04:02,993 main.py:51] epoch 2762, training loss: 12558.51, average training loss: 13131.75, base loss: 21636.76
[INFO 2017-06-28 19:04:04,326 main.py:51] epoch 2763, training loss: 14280.46, average training loss: 13132.26, base loss: 21639.76
[INFO 2017-06-28 19:04:05,862 main.py:51] epoch 2764, training loss: 12902.81, average training loss: 13132.13, base loss: 21639.87
[INFO 2017-06-28 19:04:07,330 main.py:51] epoch 2765, training loss: 12163.38, average training loss: 13129.76, base loss: 21635.41
[INFO 2017-06-28 19:04:08,717 main.py:51] epoch 2766, training loss: 12367.67, average training loss: 13129.09, base loss: 21634.47
[INFO 2017-06-28 19:04:10,042 main.py:51] epoch 2767, training loss: 13213.29, average training loss: 13129.36, base loss: 21634.88
[INFO 2017-06-28 19:04:11,411 main.py:51] epoch 2768, training loss: 13903.69, average training loss: 13130.01, base loss: 21637.36
[INFO 2017-06-28 19:04:12,991 main.py:51] epoch 2769, training loss: 14350.09, average training loss: 13131.67, base loss: 21639.51
[INFO 2017-06-28 19:04:14,314 main.py:51] epoch 2770, training loss: 12273.63, average training loss: 13130.93, base loss: 21638.29
[INFO 2017-06-28 19:04:15,742 main.py:51] epoch 2771, training loss: 13908.61, average training loss: 13131.00, base loss: 21639.56
[INFO 2017-06-28 19:04:17,176 main.py:51] epoch 2772, training loss: 15221.91, average training loss: 13132.86, base loss: 21640.92
[INFO 2017-06-28 19:04:18,506 main.py:51] epoch 2773, training loss: 13414.24, average training loss: 13133.21, base loss: 21641.65
[INFO 2017-06-28 19:04:19,760 main.py:51] epoch 2774, training loss: 12315.84, average training loss: 13131.68, base loss: 21638.04
[INFO 2017-06-28 19:04:21,171 main.py:51] epoch 2775, training loss: 10977.47, average training loss: 13127.47, base loss: 21634.81
[INFO 2017-06-28 19:04:22,520 main.py:51] epoch 2776, training loss: 12737.69, average training loss: 13126.92, base loss: 21636.05
[INFO 2017-06-28 19:04:23,821 main.py:51] epoch 2777, training loss: 13677.12, average training loss: 13127.52, base loss: 21634.12
[INFO 2017-06-28 19:04:25,065 main.py:51] epoch 2778, training loss: 11876.44, average training loss: 13127.66, base loss: 21633.45
[INFO 2017-06-28 19:04:26,335 main.py:51] epoch 2779, training loss: 14478.41, average training loss: 13129.88, base loss: 21636.70
[INFO 2017-06-28 19:04:27,776 main.py:51] epoch 2780, training loss: 12482.45, average training loss: 13126.04, base loss: 21632.37
[INFO 2017-06-28 19:04:29,095 main.py:51] epoch 2781, training loss: 13466.58, average training loss: 13126.04, base loss: 21634.12
[INFO 2017-06-28 19:04:30,448 main.py:51] epoch 2782, training loss: 12848.08, average training loss: 13126.87, base loss: 21635.14
[INFO 2017-06-28 19:04:31,876 main.py:51] epoch 2783, training loss: 11961.01, average training loss: 13126.79, base loss: 21636.58
[INFO 2017-06-28 19:04:33,167 main.py:51] epoch 2784, training loss: 13715.34, average training loss: 13126.03, base loss: 21633.69
[INFO 2017-06-28 19:04:34,393 main.py:51] epoch 2785, training loss: 13909.66, average training loss: 13127.35, base loss: 21634.50
[INFO 2017-06-28 19:04:35,713 main.py:51] epoch 2786, training loss: 13286.69, average training loss: 13124.95, base loss: 21631.29
[INFO 2017-06-28 19:04:37,055 main.py:51] epoch 2787, training loss: 12433.54, average training loss: 13124.11, base loss: 21630.92
[INFO 2017-06-28 19:04:38,352 main.py:51] epoch 2788, training loss: 13041.67, average training loss: 13125.22, base loss: 21633.33
[INFO 2017-06-28 19:04:39,869 main.py:51] epoch 2789, training loss: 13236.38, average training loss: 13125.66, base loss: 21634.66
[INFO 2017-06-28 19:04:41,228 main.py:51] epoch 2790, training loss: 14223.26, average training loss: 13126.56, base loss: 21636.19
[INFO 2017-06-28 19:04:42,667 main.py:51] epoch 2791, training loss: 12099.49, average training loss: 13126.79, base loss: 21635.32
[INFO 2017-06-28 19:04:44,220 main.py:51] epoch 2792, training loss: 13248.72, average training loss: 13127.08, base loss: 21635.52
[INFO 2017-06-28 19:04:45,555 main.py:51] epoch 2793, training loss: 13455.22, average training loss: 13128.14, base loss: 21639.22
[INFO 2017-06-28 19:04:46,946 main.py:51] epoch 2794, training loss: 13920.55, average training loss: 13128.72, base loss: 21641.12
[INFO 2017-06-28 19:04:48,244 main.py:51] epoch 2795, training loss: 12006.69, average training loss: 13127.55, base loss: 21639.50
[INFO 2017-06-28 19:04:49,520 main.py:51] epoch 2796, training loss: 13810.13, average training loss: 13128.28, base loss: 21640.18
[INFO 2017-06-28 19:04:50,781 main.py:51] epoch 2797, training loss: 12888.70, average training loss: 13128.27, base loss: 21639.87
[INFO 2017-06-28 19:04:52,172 main.py:51] epoch 2798, training loss: 12226.28, average training loss: 13128.10, base loss: 21640.17
[INFO 2017-06-28 19:04:53,471 main.py:51] epoch 2799, training loss: 13728.77, average training loss: 13128.01, base loss: 21639.52
[INFO 2017-06-28 19:04:53,471 main.py:53] epoch 2799, testing
[INFO 2017-06-28 19:04:59,491 main.py:105] average testing loss: 14317.33, base loss: 21755.77
[INFO 2017-06-28 19:04:59,491 main.py:106] improve_loss: 7438.44, improve_percent: 0.34
[INFO 2017-06-28 19:04:59,492 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 19:05:00,725 main.py:51] epoch 2800, training loss: 12188.19, average training loss: 13125.78, base loss: 21635.62
[INFO 2017-06-28 19:05:02,005 main.py:51] epoch 2801, training loss: 12945.79, average training loss: 13125.40, base loss: 21634.85
[INFO 2017-06-28 19:05:03,361 main.py:51] epoch 2802, training loss: 14178.72, average training loss: 13125.95, base loss: 21636.31
[INFO 2017-06-28 19:05:04,660 main.py:51] epoch 2803, training loss: 12549.08, average training loss: 13126.79, base loss: 21637.37
[INFO 2017-06-28 19:05:05,885 main.py:51] epoch 2804, training loss: 11687.82, average training loss: 13125.44, base loss: 21637.00
[INFO 2017-06-28 19:05:07,217 main.py:51] epoch 2805, training loss: 12717.33, average training loss: 13125.84, base loss: 21638.31
[INFO 2017-06-28 19:05:08,593 main.py:51] epoch 2806, training loss: 11848.70, average training loss: 13123.59, base loss: 21634.60
[INFO 2017-06-28 19:05:09,899 main.py:51] epoch 2807, training loss: 12963.51, average training loss: 13121.56, base loss: 21634.08
[INFO 2017-06-28 19:05:11,266 main.py:51] epoch 2808, training loss: 13595.56, average training loss: 13120.41, base loss: 21631.63
[INFO 2017-06-28 19:05:12,570 main.py:51] epoch 2809, training loss: 13866.96, average training loss: 13121.31, base loss: 21631.94
[INFO 2017-06-28 19:05:13,876 main.py:51] epoch 2810, training loss: 13432.59, average training loss: 13120.75, base loss: 21630.26
[INFO 2017-06-28 19:05:15,147 main.py:51] epoch 2811, training loss: 12470.76, average training loss: 13119.44, base loss: 21629.24
[INFO 2017-06-28 19:05:16,546 main.py:51] epoch 2812, training loss: 12934.06, average training loss: 13118.42, base loss: 21626.34
[INFO 2017-06-28 19:05:18,177 main.py:51] epoch 2813, training loss: 13021.31, average training loss: 13118.14, base loss: 21627.39
[INFO 2017-06-28 19:05:19,519 main.py:51] epoch 2814, training loss: 12720.28, average training loss: 13118.39, base loss: 21629.49
[INFO 2017-06-28 19:05:20,959 main.py:51] epoch 2815, training loss: 13609.05, average training loss: 13115.37, base loss: 21625.53
[INFO 2017-06-28 19:05:22,338 main.py:51] epoch 2816, training loss: 14551.51, average training loss: 13117.45, base loss: 21627.97
[INFO 2017-06-28 19:05:23,665 main.py:51] epoch 2817, training loss: 12331.24, average training loss: 13116.47, base loss: 21625.01
[INFO 2017-06-28 19:05:25,036 main.py:51] epoch 2818, training loss: 12410.09, average training loss: 13115.52, base loss: 21623.08
[INFO 2017-06-28 19:05:26,340 main.py:51] epoch 2819, training loss: 12754.95, average training loss: 13114.87, base loss: 21622.06
[INFO 2017-06-28 19:05:27,529 main.py:51] epoch 2820, training loss: 13677.50, average training loss: 13115.84, base loss: 21623.50
[INFO 2017-06-28 19:05:28,851 main.py:51] epoch 2821, training loss: 15147.04, average training loss: 13118.78, base loss: 21627.76
[INFO 2017-06-28 19:05:30,106 main.py:51] epoch 2822, training loss: 13781.02, average training loss: 13118.04, base loss: 21628.69
[INFO 2017-06-28 19:05:31,355 main.py:51] epoch 2823, training loss: 11981.67, average training loss: 13117.39, base loss: 21628.50
[INFO 2017-06-28 19:05:32,658 main.py:51] epoch 2824, training loss: 13026.00, average training loss: 13117.11, base loss: 21626.99
[INFO 2017-06-28 19:05:34,008 main.py:51] epoch 2825, training loss: 14633.05, average training loss: 13119.25, base loss: 21629.10
[INFO 2017-06-28 19:05:35,287 main.py:51] epoch 2826, training loss: 12088.20, average training loss: 13119.45, base loss: 21628.59
[INFO 2017-06-28 19:05:36,705 main.py:51] epoch 2827, training loss: 13432.67, average training loss: 13120.30, base loss: 21627.66
[INFO 2017-06-28 19:05:38,075 main.py:51] epoch 2828, training loss: 13544.68, average training loss: 13120.22, base loss: 21628.16
[INFO 2017-06-28 19:05:39,444 main.py:51] epoch 2829, training loss: 12528.96, average training loss: 13119.44, base loss: 21626.64
[INFO 2017-06-28 19:05:40,668 main.py:51] epoch 2830, training loss: 13252.17, average training loss: 13119.74, base loss: 21626.22
[INFO 2017-06-28 19:05:41,997 main.py:51] epoch 2831, training loss: 13874.96, average training loss: 13120.86, base loss: 21628.18
[INFO 2017-06-28 19:05:43,403 main.py:51] epoch 2832, training loss: 12453.83, average training loss: 13121.70, base loss: 21627.90
[INFO 2017-06-28 19:05:44,749 main.py:51] epoch 2833, training loss: 13077.06, average training loss: 13120.32, base loss: 21626.97
[INFO 2017-06-28 19:05:46,105 main.py:51] epoch 2834, training loss: 13415.07, average training loss: 13121.28, base loss: 21626.17
[INFO 2017-06-28 19:05:47,394 main.py:51] epoch 2835, training loss: 13485.26, average training loss: 13122.09, base loss: 21627.16
[INFO 2017-06-28 19:05:48,629 main.py:51] epoch 2836, training loss: 13401.67, average training loss: 13122.30, base loss: 21628.12
[INFO 2017-06-28 19:05:49,942 main.py:51] epoch 2837, training loss: 12340.44, average training loss: 13121.23, base loss: 21625.30
[INFO 2017-06-28 19:05:51,323 main.py:51] epoch 2838, training loss: 13967.96, average training loss: 13120.51, base loss: 21624.89
[INFO 2017-06-28 19:05:52,556 main.py:51] epoch 2839, training loss: 12755.24, average training loss: 13118.92, base loss: 21622.92
[INFO 2017-06-28 19:05:53,868 main.py:51] epoch 2840, training loss: 12058.60, average training loss: 13118.50, base loss: 21622.01
[INFO 2017-06-28 19:05:55,148 main.py:51] epoch 2841, training loss: 12654.58, average training loss: 13117.66, base loss: 21623.31
[INFO 2017-06-28 19:05:56,597 main.py:51] epoch 2842, training loss: 12524.63, average training loss: 13117.50, base loss: 21625.34
[INFO 2017-06-28 19:05:57,958 main.py:51] epoch 2843, training loss: 14921.59, average training loss: 13119.05, base loss: 21626.34
[INFO 2017-06-28 19:05:59,285 main.py:51] epoch 2844, training loss: 12806.79, average training loss: 13117.76, base loss: 21625.92
[INFO 2017-06-28 19:06:00,686 main.py:51] epoch 2845, training loss: 12677.25, average training loss: 13116.37, base loss: 21623.19
[INFO 2017-06-28 19:06:02,067 main.py:51] epoch 2846, training loss: 13521.46, average training loss: 13118.01, base loss: 21627.69
[INFO 2017-06-28 19:06:03,512 main.py:51] epoch 2847, training loss: 13951.68, average training loss: 13117.96, base loss: 21627.86
[INFO 2017-06-28 19:06:05,011 main.py:51] epoch 2848, training loss: 12420.30, average training loss: 13116.82, base loss: 21626.23
[INFO 2017-06-28 19:06:06,415 main.py:51] epoch 2849, training loss: 14837.82, average training loss: 13118.12, base loss: 21628.33
[INFO 2017-06-28 19:06:07,734 main.py:51] epoch 2850, training loss: 12103.17, average training loss: 13117.07, base loss: 21625.23
[INFO 2017-06-28 19:06:09,111 main.py:51] epoch 2851, training loss: 13875.21, average training loss: 13117.93, base loss: 21624.74
[INFO 2017-06-28 19:06:10,478 main.py:51] epoch 2852, training loss: 12844.68, average training loss: 13117.96, base loss: 21627.58
[INFO 2017-06-28 19:06:11,762 main.py:51] epoch 2853, training loss: 12684.41, average training loss: 13118.24, base loss: 21629.64
[INFO 2017-06-28 19:06:13,159 main.py:51] epoch 2854, training loss: 14087.89, average training loss: 13119.38, base loss: 21632.57
[INFO 2017-06-28 19:06:14,424 main.py:51] epoch 2855, training loss: 12233.35, average training loss: 13118.82, base loss: 21632.09
[INFO 2017-06-28 19:06:15,642 main.py:51] epoch 2856, training loss: 12717.59, average training loss: 13118.45, base loss: 21631.74
[INFO 2017-06-28 19:06:16,957 main.py:51] epoch 2857, training loss: 12993.28, average training loss: 13117.30, base loss: 21630.96
[INFO 2017-06-28 19:06:18,366 main.py:51] epoch 2858, training loss: 13711.89, average training loss: 13118.62, base loss: 21635.89
[INFO 2017-06-28 19:06:19,683 main.py:51] epoch 2859, training loss: 12243.06, average training loss: 13116.41, base loss: 21636.25
[INFO 2017-06-28 19:06:20,986 main.py:51] epoch 2860, training loss: 13392.83, average training loss: 13117.00, base loss: 21636.44
[INFO 2017-06-28 19:06:22,322 main.py:51] epoch 2861, training loss: 12898.93, average training loss: 13115.49, base loss: 21633.42
[INFO 2017-06-28 19:06:23,606 main.py:51] epoch 2862, training loss: 12681.86, average training loss: 13114.80, base loss: 21633.43
[INFO 2017-06-28 19:06:24,933 main.py:51] epoch 2863, training loss: 11674.68, average training loss: 13112.54, base loss: 21632.52
[INFO 2017-06-28 19:06:26,177 main.py:51] epoch 2864, training loss: 12394.50, average training loss: 13110.53, base loss: 21630.14
[INFO 2017-06-28 19:06:27,539 main.py:51] epoch 2865, training loss: 13053.62, average training loss: 13110.88, base loss: 21629.16
[INFO 2017-06-28 19:06:28,829 main.py:51] epoch 2866, training loss: 12538.26, average training loss: 13109.39, base loss: 21627.29
[INFO 2017-06-28 19:06:30,298 main.py:51] epoch 2867, training loss: 13166.71, average training loss: 13109.48, base loss: 21626.17
[INFO 2017-06-28 19:06:31,674 main.py:51] epoch 2868, training loss: 12307.67, average training loss: 13108.78, base loss: 21625.24
[INFO 2017-06-28 19:06:32,964 main.py:51] epoch 2869, training loss: 12649.20, average training loss: 13109.56, base loss: 21626.78
[INFO 2017-06-28 19:06:34,288 main.py:51] epoch 2870, training loss: 12781.95, average training loss: 13109.05, base loss: 21627.46
[INFO 2017-06-28 19:06:35,628 main.py:51] epoch 2871, training loss: 13469.06, average training loss: 13110.28, base loss: 21630.17
[INFO 2017-06-28 19:06:36,869 main.py:51] epoch 2872, training loss: 13733.00, average training loss: 13109.82, base loss: 21628.93
[INFO 2017-06-28 19:06:38,160 main.py:51] epoch 2873, training loss: 13515.23, average training loss: 13111.49, base loss: 21633.74
[INFO 2017-06-28 19:06:39,601 main.py:51] epoch 2874, training loss: 13251.22, average training loss: 13111.38, base loss: 21634.43
[INFO 2017-06-28 19:06:41,005 main.py:51] epoch 2875, training loss: 12954.71, average training loss: 13111.92, base loss: 21634.01
[INFO 2017-06-28 19:06:42,491 main.py:51] epoch 2876, training loss: 12313.14, average training loss: 13109.64, base loss: 21631.65
[INFO 2017-06-28 19:06:43,766 main.py:51] epoch 2877, training loss: 11987.93, average training loss: 13107.92, base loss: 21630.25
[INFO 2017-06-28 19:06:45,064 main.py:51] epoch 2878, training loss: 12134.59, average training loss: 13106.42, base loss: 21628.40
[INFO 2017-06-28 19:06:46,485 main.py:51] epoch 2879, training loss: 11378.23, average training loss: 13105.22, base loss: 21626.51
[INFO 2017-06-28 19:06:47,797 main.py:51] epoch 2880, training loss: 13238.55, average training loss: 13105.47, base loss: 21626.69
[INFO 2017-06-28 19:06:49,192 main.py:51] epoch 2881, training loss: 13519.13, average training loss: 13106.65, base loss: 21628.67
[INFO 2017-06-28 19:06:50,450 main.py:51] epoch 2882, training loss: 13489.96, average training loss: 13107.08, base loss: 21628.50
[INFO 2017-06-28 19:06:51,839 main.py:51] epoch 2883, training loss: 12816.23, average training loss: 13107.28, base loss: 21632.17
[INFO 2017-06-28 19:06:53,217 main.py:51] epoch 2884, training loss: 12950.30, average training loss: 13108.05, base loss: 21631.60
[INFO 2017-06-28 19:06:54,541 main.py:51] epoch 2885, training loss: 12886.01, average training loss: 13107.63, base loss: 21632.95
[INFO 2017-06-28 19:06:55,816 main.py:51] epoch 2886, training loss: 12909.57, average training loss: 13107.61, base loss: 21634.13
[INFO 2017-06-28 19:06:57,122 main.py:51] epoch 2887, training loss: 13910.58, average training loss: 13106.93, base loss: 21634.11
[INFO 2017-06-28 19:06:58,372 main.py:51] epoch 2888, training loss: 12935.63, average training loss: 13108.34, base loss: 21637.09
[INFO 2017-06-28 19:06:59,662 main.py:51] epoch 2889, training loss: 13290.52, average training loss: 13107.96, base loss: 21634.67
[INFO 2017-06-28 19:07:01,119 main.py:51] epoch 2890, training loss: 14492.65, average training loss: 13108.93, base loss: 21635.56
[INFO 2017-06-28 19:07:02,467 main.py:51] epoch 2891, training loss: 12158.15, average training loss: 13108.47, base loss: 21634.78
[INFO 2017-06-28 19:07:03,894 main.py:51] epoch 2892, training loss: 12278.63, average training loss: 13108.31, base loss: 21633.86
[INFO 2017-06-28 19:07:05,376 main.py:51] epoch 2893, training loss: 12789.63, average training loss: 13107.49, base loss: 21634.33
[INFO 2017-06-28 19:07:06,694 main.py:51] epoch 2894, training loss: 12166.42, average training loss: 13107.47, base loss: 21635.09
[INFO 2017-06-28 19:07:08,034 main.py:51] epoch 2895, training loss: 12919.96, average training loss: 13107.23, base loss: 21635.53
[INFO 2017-06-28 19:07:09,284 main.py:51] epoch 2896, training loss: 13169.60, average training loss: 13107.02, base loss: 21635.61
[INFO 2017-06-28 19:07:10,629 main.py:51] epoch 2897, training loss: 13024.53, average training loss: 13108.08, base loss: 21635.68
[INFO 2017-06-28 19:07:11,988 main.py:51] epoch 2898, training loss: 14129.35, average training loss: 13106.93, base loss: 21634.37
[INFO 2017-06-28 19:07:13,280 main.py:51] epoch 2899, training loss: 12943.48, average training loss: 13106.28, base loss: 21634.54
[INFO 2017-06-28 19:07:13,281 main.py:53] epoch 2899, testing
[INFO 2017-06-28 19:07:19,627 main.py:105] average testing loss: 14329.00, base loss: 21956.50
[INFO 2017-06-28 19:07:19,627 main.py:106] improve_loss: 7627.50, improve_percent: 0.35
[INFO 2017-06-28 19:07:19,628 main.py:76] current best improved percent: 0.36
[INFO 2017-06-28 19:07:20,848 main.py:51] epoch 2900, training loss: 13857.11, average training loss: 13107.35, base loss: 21634.17
[INFO 2017-06-28 19:07:22,096 main.py:51] epoch 2901, training loss: 13782.57, average training loss: 13108.90, base loss: 21636.10
[INFO 2017-06-28 19:07:23,601 main.py:51] epoch 2902, training loss: 13100.02, average training loss: 13109.69, base loss: 21638.17
[INFO 2017-06-28 19:07:24,916 main.py:51] epoch 2903, training loss: 13789.22, average training loss: 13109.11, base loss: 21638.67
[INFO 2017-06-28 19:07:26,163 main.py:51] epoch 2904, training loss: 11491.16, average training loss: 13107.53, base loss: 21636.96
[INFO 2017-06-28 19:07:27,371 main.py:51] epoch 2905, training loss: 12346.44, average training loss: 13107.82, base loss: 21637.34
[INFO 2017-06-28 19:07:28,648 main.py:51] epoch 2906, training loss: 13195.82, average training loss: 13108.41, base loss: 21639.02
[INFO 2017-06-28 19:07:30,043 main.py:51] epoch 2907, training loss: 12508.34, average training loss: 13109.16, base loss: 21639.69
[INFO 2017-06-28 19:07:31,357 main.py:51] epoch 2908, training loss: 13401.21, average training loss: 13109.25, base loss: 21638.28
[INFO 2017-06-28 19:07:32,614 main.py:51] epoch 2909, training loss: 12891.93, average training loss: 13109.36, base loss: 21637.49
[INFO 2017-06-28 19:07:33,898 main.py:51] epoch 2910, training loss: 14835.89, average training loss: 13111.39, base loss: 21641.73
[INFO 2017-06-28 19:07:35,165 main.py:51] epoch 2911, training loss: 12392.90, average training loss: 13110.08, base loss: 21642.04
[INFO 2017-06-28 19:07:36,459 main.py:51] epoch 2912, training loss: 13192.30, average training loss: 13109.38, base loss: 21642.60
[INFO 2017-06-28 19:07:37,865 main.py:51] epoch 2913, training loss: 11721.59, average training loss: 13108.18, base loss: 21643.00
[INFO 2017-06-28 19:07:39,223 main.py:51] epoch 2914, training loss: 14275.49, average training loss: 13108.92, base loss: 21642.65
[INFO 2017-06-28 19:07:40,532 main.py:51] epoch 2915, training loss: 13127.72, average training loss: 13109.16, base loss: 21643.42
[INFO 2017-06-28 19:07:41,920 main.py:51] epoch 2916, training loss: 13200.74, average training loss: 13108.26, base loss: 21642.71
[INFO 2017-06-28 19:07:43,207 main.py:51] epoch 2917, training loss: 11423.52, average training loss: 13106.01, base loss: 21637.98
[INFO 2017-06-28 19:07:44,498 main.py:51] epoch 2918, training loss: 11605.89, average training loss: 13104.47, base loss: 21635.67
[INFO 2017-06-28 19:07:45,856 main.py:51] epoch 2919, training loss: 13775.77, average training loss: 13105.60, base loss: 21636.36
[INFO 2017-06-28 19:07:47,208 main.py:51] epoch 2920, training loss: 12400.12, average training loss: 13105.98, base loss: 21634.05
[INFO 2017-06-28 19:07:48,571 main.py:51] epoch 2921, training loss: 12683.38, average training loss: 13103.81, base loss: 21630.36
[INFO 2017-06-28 19:07:49,850 main.py:51] epoch 2922, training loss: 12592.78, average training loss: 13104.65, base loss: 21631.68
[INFO 2017-06-28 19:07:51,124 main.py:51] epoch 2923, training loss: 15889.47, average training loss: 13106.82, base loss: 21634.39
[INFO 2017-06-28 19:07:52,453 main.py:51] epoch 2924, training loss: 11668.11, average training loss: 13106.13, base loss: 21633.94
[INFO 2017-06-28 19:07:53,790 main.py:51] epoch 2925, training loss: 13310.40, average training loss: 13105.65, base loss: 21632.56
[INFO 2017-06-28 19:07:54,986 main.py:51] epoch 2926, training loss: 14079.42, average training loss: 13107.26, base loss: 21637.04
[INFO 2017-06-28 19:07:56,433 main.py:51] epoch 2927, training loss: 14403.05, average training loss: 13109.02, base loss: 21639.33
[INFO 2017-06-28 19:07:57,714 main.py:51] epoch 2928, training loss: 12789.80, average training loss: 13108.84, base loss: 21638.55
[INFO 2017-06-28 19:07:58,949 main.py:51] epoch 2929, training loss: 13887.23, average training loss: 13109.68, base loss: 21639.34
[INFO 2017-06-28 19:08:00,224 main.py:51] epoch 2930, training loss: 13624.14, average training loss: 13109.71, base loss: 21642.76
[INFO 2017-06-28 19:08:01,513 main.py:51] epoch 2931, training loss: 13380.83, average training loss: 13110.25, base loss: 21640.87
[INFO 2017-06-28 19:08:02,896 main.py:51] epoch 2932, training loss: 14222.07, average training loss: 13111.71, base loss: 21643.96
[INFO 2017-06-28 19:08:04,409 main.py:51] epoch 2933, training loss: 13093.86, average training loss: 13111.74, base loss: 21644.42
[INFO 2017-06-28 19:08:05,878 main.py:51] epoch 2934, training loss: 13996.53, average training loss: 13113.81, base loss: 21647.34
[INFO 2017-06-28 19:08:07,272 main.py:51] epoch 2935, training loss: 12323.01, average training loss: 13112.44, base loss: 21645.00
[INFO 2017-06-28 19:08:08,634 main.py:51] epoch 2936, training loss: 12946.68, average training loss: 13112.67, base loss: 21644.64
[INFO 2017-06-28 19:08:09,935 main.py:51] epoch 2937, training loss: 15393.72, average training loss: 13114.74, base loss: 21648.25
[INFO 2017-06-28 19:08:11,336 main.py:51] epoch 2938, training loss: 13400.65, average training loss: 13114.47, base loss: 21648.80
[INFO 2017-06-28 19:08:12,687 main.py:51] epoch 2939, training loss: 13221.14, average training loss: 13114.95, base loss: 21648.85
[INFO 2017-06-28 19:08:14,109 main.py:51] epoch 2940, training loss: 12899.40, average training loss: 13113.96, base loss: 21648.84
[INFO 2017-06-28 19:08:15,466 main.py:51] epoch 2941, training loss: 13253.99, average training loss: 13113.44, base loss: 21649.26
[INFO 2017-06-28 19:08:16,698 main.py:51] epoch 2942, training loss: 13228.81, average training loss: 13111.56, base loss: 21648.05
[INFO 2017-06-28 19:08:18,117 main.py:51] epoch 2943, training loss: 11545.22, average training loss: 13108.98, base loss: 21643.10
[INFO 2017-06-28 19:08:19,387 main.py:51] epoch 2944, training loss: 12180.68, average training loss: 13107.28, base loss: 21640.21
[INFO 2017-06-28 19:08:20,780 main.py:51] epoch 2945, training loss: 14356.87, average training loss: 13109.33, base loss: 21642.79
[INFO 2017-06-28 19:08:22,015 main.py:51] epoch 2946, training loss: 12169.33, average training loss: 13109.06, base loss: 21643.09
[INFO 2017-06-28 19:08:23,382 main.py:51] epoch 2947, training loss: 13750.67, average training loss: 13108.82, base loss: 21642.54
[INFO 2017-06-28 19:08:24,658 main.py:51] epoch 2948, training loss: 13767.62, average training loss: 13109.58, base loss: 21644.68
[INFO 2017-06-28 19:08:25,984 main.py:51] epoch 2949, training loss: 13524.61, average training loss: 13110.80, base loss: 21646.16
[INFO 2017-06-28 19:08:27,389 main.py:51] epoch 2950, training loss: 13209.44, average training loss: 13111.17, base loss: 21646.39
[INFO 2017-06-28 19:08:28,690 main.py:51] epoch 2951, training loss: 11699.39, average training loss: 13109.32, base loss: 21642.73
[INFO 2017-06-28 19:08:29,999 main.py:51] epoch 2952, training loss: 13285.63, average training loss: 13108.15, base loss: 21641.98
[INFO 2017-06-28 19:08:31,300 main.py:51] epoch 2953, training loss: 11864.82, average training loss: 13105.30, base loss: 21639.32
[INFO 2017-06-28 19:08:32,568 main.py:51] epoch 2954, training loss: 11243.68, average training loss: 13102.76, base loss: 21635.94
[INFO 2017-06-28 19:08:34,031 main.py:51] epoch 2955, training loss: 13009.57, average training loss: 13101.82, base loss: 21635.50
[INFO 2017-06-28 19:08:35,375 main.py:51] epoch 2956, training loss: 12306.27, average training loss: 13100.25, base loss: 21633.48
[INFO 2017-06-28 19:08:36,556 main.py:51] epoch 2957, training loss: 11626.90, average training loss: 13097.72, base loss: 21630.40
[INFO 2017-06-28 19:08:37,853 main.py:51] epoch 2958, training loss: 11575.99, average training loss: 13095.35, base loss: 21626.40
[INFO 2017-06-28 19:08:39,060 main.py:51] epoch 2959, training loss: 11506.00, average training loss: 13094.28, base loss: 21624.20
[INFO 2017-06-28 19:08:40,242 main.py:51] epoch 2960, training loss: 12332.42, average training loss: 13093.65, base loss: 21624.81
[INFO 2017-06-28 19:08:41,453 main.py:51] epoch 2961, training loss: 12276.05, average training loss: 13092.37, base loss: 21622.52
[INFO 2017-06-28 19:08:42,736 main.py:51] epoch 2962, training loss: 12827.81, average training loss: 13090.25, base loss: 21620.89
[INFO 2017-06-28 19:08:44,019 main.py:51] epoch 2963, training loss: 13486.68, average training loss: 13090.39, base loss: 21620.08
[INFO 2017-06-28 19:08:45,376 main.py:51] epoch 2964, training loss: 14633.70, average training loss: 13092.12, base loss: 21623.33
[INFO 2017-06-28 19:08:46,711 main.py:51] epoch 2965, training loss: 12813.53, average training loss: 13091.08, base loss: 21624.30
[INFO 2017-06-28 19:08:48,023 main.py:51] epoch 2966, training loss: 12781.75, average training loss: 13091.56, base loss: 21625.73
[INFO 2017-06-28 19:08:49,425 main.py:51] epoch 2967, training loss: 12543.98, average training loss: 13090.23, base loss: 21625.23
[INFO 2017-06-28 19:08:50,802 main.py:51] epoch 2968, training loss: 13034.89, average training loss: 13090.53, base loss: 21623.86
[INFO 2017-06-28 19:08:52,113 main.py:51] epoch 2969, training loss: 11952.42, average training loss: 13090.20, base loss: 21624.07
[INFO 2017-06-28 19:08:53,464 main.py:51] epoch 2970, training loss: 12193.99, average training loss: 13088.21, base loss: 21620.73
[INFO 2017-06-28 19:08:54,784 main.py:51] epoch 2971, training loss: 12708.55, average training loss: 13088.66, base loss: 21619.14
[INFO 2017-06-28 19:08:56,147 main.py:51] epoch 2972, training loss: 13733.56, average training loss: 13090.14, base loss: 21621.49
[INFO 2017-06-28 19:08:57,522 main.py:51] epoch 2973, training loss: 13238.05, average training loss: 13090.57, base loss: 21623.92
[INFO 2017-06-28 19:08:58,802 main.py:51] epoch 2974, training loss: 12863.88, average training loss: 13091.64, base loss: 21626.64
[INFO 2017-06-28 19:09:00,135 main.py:51] epoch 2975, training loss: 11861.90, average training loss: 13090.07, base loss: 21623.86
[INFO 2017-06-28 19:09:01,581 main.py:51] epoch 2976, training loss: 12081.67, average training loss: 13088.40, base loss: 21622.32
[INFO 2017-06-28 19:09:02,953 main.py:51] epoch 2977, training loss: 12435.50, average training loss: 13087.44, base loss: 21621.66
[INFO 2017-06-28 19:09:04,226 main.py:51] epoch 2978, training loss: 13545.65, average training loss: 13088.02, base loss: 21623.86
[INFO 2017-06-28 19:09:05,486 main.py:51] epoch 2979, training loss: 13341.36, average training loss: 13089.20, base loss: 21624.71
[INFO 2017-06-28 19:09:06,768 main.py:51] epoch 2980, training loss: 12746.11, average training loss: 13089.03, base loss: 21624.76
[INFO 2017-06-28 19:09:08,099 main.py:51] epoch 2981, training loss: 14295.74, average training loss: 13089.34, base loss: 21623.58
[INFO 2017-06-28 19:09:09,533 main.py:51] epoch 2982, training loss: 11919.19, average training loss: 13087.89, base loss: 21620.33
[INFO 2017-06-28 19:09:10,858 main.py:51] epoch 2983, training loss: 14692.06, average training loss: 13090.14, base loss: 21622.83
[INFO 2017-06-28 19:09:12,215 main.py:51] epoch 2984, training loss: 12413.76, average training loss: 13090.28, base loss: 21624.29
[INFO 2017-06-28 19:09:13,526 main.py:51] epoch 2985, training loss: 12538.71, average training loss: 13090.18, base loss: 21625.01
[INFO 2017-06-28 19:09:14,862 main.py:51] epoch 2986, training loss: 11318.05, average training loss: 13088.39, base loss: 21624.73
[INFO 2017-06-28 19:09:16,149 main.py:51] epoch 2987, training loss: 11166.28, average training loss: 13085.60, base loss: 21620.24
[INFO 2017-06-28 19:09:17,584 main.py:51] epoch 2988, training loss: 12889.64, average training loss: 13083.43, base loss: 21619.13
[INFO 2017-06-28 19:09:18,979 main.py:51] epoch 2989, training loss: 12502.12, average training loss: 13082.20, base loss: 21617.67
[INFO 2017-06-28 19:09:20,247 main.py:51] epoch 2990, training loss: 13769.40, average training loss: 13082.00, base loss: 21618.89
[INFO 2017-06-28 19:09:21,660 main.py:51] epoch 2991, training loss: 12056.17, average training loss: 13081.09, base loss: 21616.90
[INFO 2017-06-28 19:09:22,985 main.py:51] epoch 2992, training loss: 13018.57, average training loss: 13080.87, base loss: 21617.29
[INFO 2017-06-28 19:09:24,290 main.py:51] epoch 2993, training loss: 12942.32, average training loss: 13081.59, base loss: 21618.39
[INFO 2017-06-28 19:09:25,625 main.py:51] epoch 2994, training loss: 12880.72, average training loss: 13081.36, base loss: 21616.46
[INFO 2017-06-28 19:09:26,902 main.py:51] epoch 2995, training loss: 11688.58, average training loss: 13078.14, base loss: 21612.79
[INFO 2017-06-28 19:09:28,298 main.py:51] epoch 2996, training loss: 13340.58, average training loss: 13079.40, base loss: 21613.97
[INFO 2017-06-28 19:09:29,598 main.py:51] epoch 2997, training loss: 12665.63, average training loss: 13078.26, base loss: 21613.84
[INFO 2017-06-28 19:09:30,948 main.py:51] epoch 2998, training loss: 13225.26, average training loss: 13077.23, base loss: 21613.95
[INFO 2017-06-28 19:09:32,311 main.py:51] epoch 2999, training loss: 12626.18, average training loss: 13077.23, base loss: 21613.49
[INFO 2017-06-28 19:09:32,311 main.py:53] epoch 2999, testing
[INFO 2017-06-28 19:09:38,389 main.py:105] average testing loss: 14008.25, base loss: 22121.28
[INFO 2017-06-28 19:09:38,389 main.py:106] improve_loss: 8113.03, improve_percent: 0.37
[INFO 2017-06-28 19:09:38,390 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 19:09:38,403 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:09:39,745 main.py:51] epoch 3000, training loss: 14601.28, average training loss: 13079.01, base loss: 21615.69
[INFO 2017-06-28 19:09:40,969 main.py:51] epoch 3001, training loss: 13150.08, average training loss: 13080.11, base loss: 21618.16
[INFO 2017-06-28 19:09:42,310 main.py:51] epoch 3002, training loss: 13215.21, average training loss: 13081.77, base loss: 21621.63
[INFO 2017-06-28 19:09:43,630 main.py:51] epoch 3003, training loss: 12602.70, average training loss: 13079.82, base loss: 21620.34
[INFO 2017-06-28 19:09:44,936 main.py:51] epoch 3004, training loss: 13268.66, average training loss: 13078.57, base loss: 21619.61
[INFO 2017-06-28 19:09:46,170 main.py:51] epoch 3005, training loss: 14346.07, average training loss: 13079.81, base loss: 21622.45
[INFO 2017-06-28 19:09:47,491 main.py:51] epoch 3006, training loss: 11481.50, average training loss: 13078.66, base loss: 21624.37
[INFO 2017-06-28 19:09:48,842 main.py:51] epoch 3007, training loss: 12951.67, average training loss: 13078.77, base loss: 21624.44
[INFO 2017-06-28 19:09:50,131 main.py:51] epoch 3008, training loss: 12017.07, average training loss: 13077.61, base loss: 21620.78
[INFO 2017-06-28 19:09:51,411 main.py:51] epoch 3009, training loss: 12257.57, average training loss: 13077.57, base loss: 21619.05
[INFO 2017-06-28 19:09:52,794 main.py:51] epoch 3010, training loss: 12463.84, average training loss: 13078.34, base loss: 21621.23
[INFO 2017-06-28 19:09:54,104 main.py:51] epoch 3011, training loss: 12023.91, average training loss: 13076.96, base loss: 21618.18
[INFO 2017-06-28 19:09:55,498 main.py:51] epoch 3012, training loss: 11995.45, average training loss: 13074.54, base loss: 21614.46
[INFO 2017-06-28 19:09:56,774 main.py:51] epoch 3013, training loss: 13947.60, average training loss: 13074.86, base loss: 21613.85
[INFO 2017-06-28 19:09:58,152 main.py:51] epoch 3014, training loss: 14341.93, average training loss: 13076.29, base loss: 21616.06
[INFO 2017-06-28 19:09:59,456 main.py:51] epoch 3015, training loss: 13130.50, average training loss: 13075.99, base loss: 21618.17
[INFO 2017-06-28 19:10:00,866 main.py:51] epoch 3016, training loss: 12387.09, average training loss: 13074.39, base loss: 21619.58
[INFO 2017-06-28 19:10:02,136 main.py:51] epoch 3017, training loss: 12737.85, average training loss: 13073.81, base loss: 21620.71
[INFO 2017-06-28 19:10:03,478 main.py:51] epoch 3018, training loss: 12875.46, average training loss: 13074.15, base loss: 21620.38
[INFO 2017-06-28 19:10:04,826 main.py:51] epoch 3019, training loss: 12128.16, average training loss: 13072.88, base loss: 21618.57
[INFO 2017-06-28 19:10:06,053 main.py:51] epoch 3020, training loss: 13537.72, average training loss: 13072.16, base loss: 21616.49
[INFO 2017-06-28 19:10:07,307 main.py:51] epoch 3021, training loss: 11890.85, average training loss: 13069.72, base loss: 21612.92
[INFO 2017-06-28 19:10:08,536 main.py:51] epoch 3022, training loss: 12918.76, average training loss: 13068.64, base loss: 21611.91
[INFO 2017-06-28 19:10:09,821 main.py:51] epoch 3023, training loss: 14622.41, average training loss: 13070.63, base loss: 21615.01
[INFO 2017-06-28 19:10:11,152 main.py:51] epoch 3024, training loss: 13334.51, average training loss: 13070.27, base loss: 21613.79
[INFO 2017-06-28 19:10:12,553 main.py:51] epoch 3025, training loss: 13223.54, average training loss: 13070.11, base loss: 21613.83
[INFO 2017-06-28 19:10:13,817 main.py:51] epoch 3026, training loss: 12743.08, average training loss: 13069.05, base loss: 21614.09
[INFO 2017-06-28 19:10:15,181 main.py:51] epoch 3027, training loss: 14046.73, average training loss: 13071.37, base loss: 21617.44
[INFO 2017-06-28 19:10:16,448 main.py:51] epoch 3028, training loss: 12864.52, average training loss: 13071.33, base loss: 21617.49
[INFO 2017-06-28 19:10:17,804 main.py:51] epoch 3029, training loss: 12338.35, average training loss: 13070.01, base loss: 21618.47
[INFO 2017-06-28 19:10:19,104 main.py:51] epoch 3030, training loss: 13803.85, average training loss: 13068.64, base loss: 21616.44
[INFO 2017-06-28 19:10:20,382 main.py:51] epoch 3031, training loss: 12293.04, average training loss: 13068.15, base loss: 21615.85
[INFO 2017-06-28 19:10:21,630 main.py:51] epoch 3032, training loss: 14572.10, average training loss: 13069.95, base loss: 21617.63
[INFO 2017-06-28 19:10:22,936 main.py:51] epoch 3033, training loss: 12461.91, average training loss: 13069.85, base loss: 21616.44
[INFO 2017-06-28 19:10:24,193 main.py:51] epoch 3034, training loss: 11526.01, average training loss: 13069.16, base loss: 21616.44
[INFO 2017-06-28 19:10:25,529 main.py:51] epoch 3035, training loss: 12637.89, average training loss: 13067.32, base loss: 21612.69
[INFO 2017-06-28 19:10:26,844 main.py:51] epoch 3036, training loss: 14463.77, average training loss: 13067.93, base loss: 21615.38
[INFO 2017-06-28 19:10:28,126 main.py:51] epoch 3037, training loss: 13457.74, average training loss: 13068.58, base loss: 21616.83
[INFO 2017-06-28 19:10:29,387 main.py:51] epoch 3038, training loss: 13044.28, average training loss: 13068.48, base loss: 21618.41
[INFO 2017-06-28 19:10:30,694 main.py:51] epoch 3039, training loss: 15041.08, average training loss: 13070.62, base loss: 21621.17
[INFO 2017-06-28 19:10:32,000 main.py:51] epoch 3040, training loss: 13370.66, average training loss: 13071.50, base loss: 21622.71
[INFO 2017-06-28 19:10:33,338 main.py:51] epoch 3041, training loss: 12739.82, average training loss: 13071.26, base loss: 21623.23
[INFO 2017-06-28 19:10:34,677 main.py:51] epoch 3042, training loss: 11752.21, average training loss: 13070.80, base loss: 21623.33
[INFO 2017-06-28 19:10:35,971 main.py:51] epoch 3043, training loss: 12821.91, average training loss: 13070.53, base loss: 21623.49
[INFO 2017-06-28 19:10:37,291 main.py:51] epoch 3044, training loss: 12933.14, average training loss: 13069.18, base loss: 21622.29
[INFO 2017-06-28 19:10:38,605 main.py:51] epoch 3045, training loss: 13328.26, average training loss: 13068.59, base loss: 21622.15
[INFO 2017-06-28 19:10:39,939 main.py:51] epoch 3046, training loss: 11707.12, average training loss: 13067.17, base loss: 21621.95
[INFO 2017-06-28 19:10:41,164 main.py:51] epoch 3047, training loss: 11924.24, average training loss: 13066.13, base loss: 21621.76
[INFO 2017-06-28 19:10:42,479 main.py:51] epoch 3048, training loss: 14322.02, average training loss: 13067.27, base loss: 21623.33
[INFO 2017-06-28 19:10:43,917 main.py:51] epoch 3049, training loss: 11975.24, average training loss: 13066.48, base loss: 21622.79
[INFO 2017-06-28 19:10:45,310 main.py:51] epoch 3050, training loss: 12479.76, average training loss: 13066.20, base loss: 21621.77
[INFO 2017-06-28 19:10:46,662 main.py:51] epoch 3051, training loss: 12601.08, average training loss: 13065.01, base loss: 21622.15
[INFO 2017-06-28 19:10:47,927 main.py:51] epoch 3052, training loss: 13098.41, average training loss: 13062.78, base loss: 21620.38
[INFO 2017-06-28 19:10:49,262 main.py:51] epoch 3053, training loss: 14058.27, average training loss: 13063.48, base loss: 21621.37
[INFO 2017-06-28 19:10:50,642 main.py:51] epoch 3054, training loss: 12398.15, average training loss: 13062.47, base loss: 21620.62
[INFO 2017-06-28 19:10:51,892 main.py:51] epoch 3055, training loss: 12686.58, average training loss: 13061.10, base loss: 21619.52
[INFO 2017-06-28 19:10:53,295 main.py:51] epoch 3056, training loss: 12221.62, average training loss: 13058.43, base loss: 21616.40
[INFO 2017-06-28 19:10:54,589 main.py:51] epoch 3057, training loss: 12706.76, average training loss: 13057.88, base loss: 21617.60
[INFO 2017-06-28 19:10:55,998 main.py:51] epoch 3058, training loss: 11295.20, average training loss: 13056.18, base loss: 21614.42
[INFO 2017-06-28 19:10:57,351 main.py:51] epoch 3059, training loss: 11763.66, average training loss: 13055.36, base loss: 21613.63
[INFO 2017-06-28 19:10:58,567 main.py:51] epoch 3060, training loss: 13208.45, average training loss: 13054.88, base loss: 21613.80
[INFO 2017-06-28 19:11:00,022 main.py:51] epoch 3061, training loss: 13861.20, average training loss: 13056.15, base loss: 21615.91
[INFO 2017-06-28 19:11:01,395 main.py:51] epoch 3062, training loss: 13101.98, average training loss: 13055.11, base loss: 21615.36
[INFO 2017-06-28 19:11:02,777 main.py:51] epoch 3063, training loss: 13717.45, average training loss: 13057.68, base loss: 21619.15
[INFO 2017-06-28 19:11:04,067 main.py:51] epoch 3064, training loss: 13560.48, average training loss: 13058.22, base loss: 21620.70
[INFO 2017-06-28 19:11:05,453 main.py:51] epoch 3065, training loss: 11523.46, average training loss: 13057.22, base loss: 21619.98
[INFO 2017-06-28 19:11:06,816 main.py:51] epoch 3066, training loss: 12180.65, average training loss: 13055.23, base loss: 21617.54
[INFO 2017-06-28 19:11:08,215 main.py:51] epoch 3067, training loss: 12641.14, average training loss: 13053.12, base loss: 21615.35
[INFO 2017-06-28 19:11:09,494 main.py:51] epoch 3068, training loss: 12131.67, average training loss: 13052.38, base loss: 21613.88
[INFO 2017-06-28 19:11:10,866 main.py:51] epoch 3069, training loss: 11951.33, average training loss: 13050.08, base loss: 21611.45
[INFO 2017-06-28 19:11:12,301 main.py:51] epoch 3070, training loss: 12808.79, average training loss: 13048.94, base loss: 21610.29
[INFO 2017-06-28 19:11:13,723 main.py:51] epoch 3071, training loss: 13240.20, average training loss: 13049.49, base loss: 21611.27
[INFO 2017-06-28 19:11:14,966 main.py:51] epoch 3072, training loss: 13230.47, average training loss: 13049.05, base loss: 21612.23
[INFO 2017-06-28 19:11:16,265 main.py:51] epoch 3073, training loss: 12952.87, average training loss: 13049.90, base loss: 21614.42
[INFO 2017-06-28 19:11:17,622 main.py:51] epoch 3074, training loss: 11895.30, average training loss: 13049.46, base loss: 21614.22
[INFO 2017-06-28 19:11:18,938 main.py:51] epoch 3075, training loss: 11557.76, average training loss: 13047.94, base loss: 21612.23
[INFO 2017-06-28 19:11:20,259 main.py:51] epoch 3076, training loss: 12169.48, average training loss: 13047.89, base loss: 21612.87
[INFO 2017-06-28 19:11:21,525 main.py:51] epoch 3077, training loss: 12468.23, average training loss: 13047.05, base loss: 21613.35
[INFO 2017-06-28 19:11:22,962 main.py:51] epoch 3078, training loss: 12940.69, average training loss: 13047.81, base loss: 21614.92
[INFO 2017-06-28 19:11:24,241 main.py:51] epoch 3079, training loss: 13163.33, average training loss: 13047.72, base loss: 21615.06
[INFO 2017-06-28 19:11:25,410 main.py:51] epoch 3080, training loss: 12117.29, average training loss: 13047.75, base loss: 21613.37
[INFO 2017-06-28 19:11:26,691 main.py:51] epoch 3081, training loss: 13592.78, average training loss: 13047.62, base loss: 21612.24
[INFO 2017-06-28 19:11:28,103 main.py:51] epoch 3082, training loss: 11784.44, average training loss: 13045.46, base loss: 21609.86
[INFO 2017-06-28 19:11:29,400 main.py:51] epoch 3083, training loss: 12978.08, average training loss: 13046.41, base loss: 21612.31
[INFO 2017-06-28 19:11:30,663 main.py:51] epoch 3084, training loss: 12258.93, average training loss: 13045.55, base loss: 21612.12
[INFO 2017-06-28 19:11:31,903 main.py:51] epoch 3085, training loss: 13445.29, average training loss: 13046.25, base loss: 21614.32
[INFO 2017-06-28 19:11:33,274 main.py:51] epoch 3086, training loss: 12763.91, average training loss: 13045.59, base loss: 21615.75
[INFO 2017-06-28 19:11:34,526 main.py:51] epoch 3087, training loss: 13082.41, average training loss: 13046.04, base loss: 21615.97
[INFO 2017-06-28 19:11:35,859 main.py:51] epoch 3088, training loss: 11965.99, average training loss: 13044.30, base loss: 21613.27
[INFO 2017-06-28 19:11:37,049 main.py:51] epoch 3089, training loss: 13567.82, average training loss: 13043.59, base loss: 21613.64
[INFO 2017-06-28 19:11:38,381 main.py:51] epoch 3090, training loss: 11854.88, average training loss: 13044.34, base loss: 21615.35
[INFO 2017-06-28 19:11:39,659 main.py:51] epoch 3091, training loss: 13885.38, average training loss: 13046.13, base loss: 21617.38
[INFO 2017-06-28 19:11:40,850 main.py:51] epoch 3092, training loss: 11168.79, average training loss: 13045.20, base loss: 21617.16
[INFO 2017-06-28 19:11:42,098 main.py:51] epoch 3093, training loss: 11846.63, average training loss: 13041.33, base loss: 21615.72
[INFO 2017-06-28 19:11:43,366 main.py:51] epoch 3094, training loss: 12914.14, average training loss: 13039.85, base loss: 21615.93
[INFO 2017-06-28 19:11:44,666 main.py:51] epoch 3095, training loss: 12710.91, average training loss: 13037.32, base loss: 21613.19
[INFO 2017-06-28 19:11:45,909 main.py:51] epoch 3096, training loss: 12897.38, average training loss: 13038.60, base loss: 21613.36
[INFO 2017-06-28 19:11:47,172 main.py:51] epoch 3097, training loss: 13692.36, average training loss: 13037.61, base loss: 21614.36
[INFO 2017-06-28 19:11:48,485 main.py:51] epoch 3098, training loss: 14026.13, average training loss: 13038.11, base loss: 21615.71
[INFO 2017-06-28 19:11:49,776 main.py:51] epoch 3099, training loss: 13136.45, average training loss: 13037.46, base loss: 21615.09
[INFO 2017-06-28 19:11:49,776 main.py:53] epoch 3099, testing
[INFO 2017-06-28 19:11:56,062 main.py:105] average testing loss: 14805.73, base loss: 22863.72
[INFO 2017-06-28 19:11:56,063 main.py:106] improve_loss: 8057.99, improve_percent: 0.35
[INFO 2017-06-28 19:11:56,063 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:11:57,350 main.py:51] epoch 3100, training loss: 12621.63, average training loss: 13037.51, base loss: 21617.05
[INFO 2017-06-28 19:11:58,715 main.py:51] epoch 3101, training loss: 12768.34, average training loss: 13036.32, base loss: 21616.96
[INFO 2017-06-28 19:12:00,024 main.py:51] epoch 3102, training loss: 12837.34, average training loss: 13035.92, base loss: 21616.86
[INFO 2017-06-28 19:12:01,542 main.py:51] epoch 3103, training loss: 11151.36, average training loss: 13033.04, base loss: 21613.17
[INFO 2017-06-28 19:12:02,929 main.py:51] epoch 3104, training loss: 14319.08, average training loss: 13034.23, base loss: 21616.55
[INFO 2017-06-28 19:12:04,238 main.py:51] epoch 3105, training loss: 12749.46, average training loss: 13034.16, base loss: 21616.11
[INFO 2017-06-28 19:12:05,619 main.py:51] epoch 3106, training loss: 11496.45, average training loss: 13033.79, base loss: 21616.65
[INFO 2017-06-28 19:12:06,906 main.py:51] epoch 3107, training loss: 13359.31, average training loss: 13033.24, base loss: 21616.14
[INFO 2017-06-28 19:12:08,407 main.py:51] epoch 3108, training loss: 11778.53, average training loss: 13032.86, base loss: 21615.08
[INFO 2017-06-28 19:12:09,775 main.py:51] epoch 3109, training loss: 12946.81, average training loss: 13033.16, base loss: 21614.90
[INFO 2017-06-28 19:12:11,037 main.py:51] epoch 3110, training loss: 13367.12, average training loss: 13034.57, base loss: 21616.92
[INFO 2017-06-28 19:12:12,366 main.py:51] epoch 3111, training loss: 13503.85, average training loss: 13035.30, base loss: 21617.93
[INFO 2017-06-28 19:12:13,674 main.py:51] epoch 3112, training loss: 12183.23, average training loss: 13033.97, base loss: 21616.95
[INFO 2017-06-28 19:12:15,051 main.py:51] epoch 3113, training loss: 12475.16, average training loss: 13033.07, base loss: 21615.95
[INFO 2017-06-28 19:12:16,353 main.py:51] epoch 3114, training loss: 12441.17, average training loss: 13032.40, base loss: 21613.89
[INFO 2017-06-28 19:12:17,695 main.py:51] epoch 3115, training loss: 14002.78, average training loss: 13032.31, base loss: 21613.15
[INFO 2017-06-28 19:12:18,965 main.py:51] epoch 3116, training loss: 12144.56, average training loss: 13030.93, base loss: 21611.34
[INFO 2017-06-28 19:12:20,211 main.py:51] epoch 3117, training loss: 12242.37, average training loss: 13029.95, base loss: 21608.52
[INFO 2017-06-28 19:12:21,517 main.py:51] epoch 3118, training loss: 12862.37, average training loss: 13029.81, base loss: 21606.96
[INFO 2017-06-28 19:12:22,849 main.py:51] epoch 3119, training loss: 13563.66, average training loss: 13029.93, base loss: 21608.51
[INFO 2017-06-28 19:12:24,214 main.py:51] epoch 3120, training loss: 11338.26, average training loss: 13029.45, base loss: 21610.86
[INFO 2017-06-28 19:12:25,507 main.py:51] epoch 3121, training loss: 13459.99, average training loss: 13029.81, base loss: 21611.96
[INFO 2017-06-28 19:12:26,903 main.py:51] epoch 3122, training loss: 14331.43, average training loss: 13030.84, base loss: 21613.19
[INFO 2017-06-28 19:12:28,243 main.py:51] epoch 3123, training loss: 13127.06, average training loss: 13030.75, base loss: 21612.04
[INFO 2017-06-28 19:12:29,564 main.py:51] epoch 3124, training loss: 15940.63, average training loss: 13034.54, base loss: 21619.67
[INFO 2017-06-28 19:12:30,773 main.py:51] epoch 3125, training loss: 14654.83, average training loss: 13035.77, base loss: 21620.94
[INFO 2017-06-28 19:12:32,167 main.py:51] epoch 3126, training loss: 12650.99, average training loss: 13035.07, base loss: 21619.33
[INFO 2017-06-28 19:12:33,394 main.py:51] epoch 3127, training loss: 13114.11, average training loss: 13033.91, base loss: 21617.14
[INFO 2017-06-28 19:12:34,620 main.py:51] epoch 3128, training loss: 12448.59, average training loss: 13032.88, base loss: 21615.95
[INFO 2017-06-28 19:12:36,004 main.py:51] epoch 3129, training loss: 12244.68, average training loss: 13033.11, base loss: 21615.68
[INFO 2017-06-28 19:12:37,439 main.py:51] epoch 3130, training loss: 12904.48, average training loss: 13033.45, base loss: 21619.06
[INFO 2017-06-28 19:12:38,769 main.py:51] epoch 3131, training loss: 13376.68, average training loss: 13033.77, base loss: 21621.16
[INFO 2017-06-28 19:12:40,102 main.py:51] epoch 3132, training loss: 14226.71, average training loss: 13034.39, base loss: 21622.49
[INFO 2017-06-28 19:12:41,489 main.py:51] epoch 3133, training loss: 13653.53, average training loss: 13035.64, base loss: 21624.87
[INFO 2017-06-28 19:12:42,975 main.py:51] epoch 3134, training loss: 12816.48, average training loss: 13034.94, base loss: 21623.69
[INFO 2017-06-28 19:12:44,289 main.py:51] epoch 3135, training loss: 13506.37, average training loss: 13036.44, base loss: 21625.77
[INFO 2017-06-28 19:12:45,554 main.py:51] epoch 3136, training loss: 12566.96, average training loss: 13036.74, base loss: 21625.77
[INFO 2017-06-28 19:12:46,996 main.py:51] epoch 3137, training loss: 12579.31, average training loss: 13035.68, base loss: 21626.22
[INFO 2017-06-28 19:12:48,338 main.py:51] epoch 3138, training loss: 12924.47, average training loss: 13035.08, base loss: 21624.30
[INFO 2017-06-28 19:12:49,724 main.py:51] epoch 3139, training loss: 12945.55, average training loss: 13032.90, base loss: 21621.12
[INFO 2017-06-28 19:12:51,069 main.py:51] epoch 3140, training loss: 13458.82, average training loss: 13034.09, base loss: 21621.87
[INFO 2017-06-28 19:12:52,396 main.py:51] epoch 3141, training loss: 11904.96, average training loss: 13032.75, base loss: 21619.14
[INFO 2017-06-28 19:12:53,661 main.py:51] epoch 3142, training loss: 13434.52, average training loss: 13033.51, base loss: 21619.67
[INFO 2017-06-28 19:12:55,044 main.py:51] epoch 3143, training loss: 12299.55, average training loss: 13032.73, base loss: 21620.43
[INFO 2017-06-28 19:12:56,424 main.py:51] epoch 3144, training loss: 12446.59, average training loss: 13031.94, base loss: 21621.63
[INFO 2017-06-28 19:12:57,641 main.py:51] epoch 3145, training loss: 12510.09, average training loss: 13030.95, base loss: 21619.70
[INFO 2017-06-28 19:12:58,964 main.py:51] epoch 3146, training loss: 12773.63, average training loss: 13031.76, base loss: 21621.09
[INFO 2017-06-28 19:13:00,545 main.py:51] epoch 3147, training loss: 14346.94, average training loss: 13032.26, base loss: 21621.39
[INFO 2017-06-28 19:13:01,822 main.py:51] epoch 3148, training loss: 13718.69, average training loss: 13032.97, base loss: 21624.70
[INFO 2017-06-28 19:13:03,186 main.py:51] epoch 3149, training loss: 12873.96, average training loss: 13032.54, base loss: 21625.00
[INFO 2017-06-28 19:13:04,601 main.py:51] epoch 3150, training loss: 15652.17, average training loss: 13035.21, base loss: 21628.76
[INFO 2017-06-28 19:13:05,923 main.py:51] epoch 3151, training loss: 13259.25, average training loss: 13036.91, base loss: 21632.61
[INFO 2017-06-28 19:13:07,310 main.py:51] epoch 3152, training loss: 13495.20, average training loss: 13037.44, base loss: 21632.76
[INFO 2017-06-28 19:13:08,647 main.py:51] epoch 3153, training loss: 13336.70, average training loss: 13037.44, base loss: 21633.42
[INFO 2017-06-28 19:13:09,937 main.py:51] epoch 3154, training loss: 13973.35, average training loss: 13037.20, base loss: 21633.38
[INFO 2017-06-28 19:13:11,301 main.py:51] epoch 3155, training loss: 13247.10, average training loss: 13036.04, base loss: 21633.86
[INFO 2017-06-28 19:13:12,755 main.py:51] epoch 3156, training loss: 12569.24, average training loss: 13035.12, base loss: 21633.34
[INFO 2017-06-28 19:13:14,084 main.py:51] epoch 3157, training loss: 12711.20, average training loss: 13034.31, base loss: 21633.52
[INFO 2017-06-28 19:13:15,406 main.py:51] epoch 3158, training loss: 13307.73, average training loss: 13033.73, base loss: 21632.48
[INFO 2017-06-28 19:13:16,733 main.py:51] epoch 3159, training loss: 12947.79, average training loss: 13034.59, base loss: 21631.73
[INFO 2017-06-28 19:13:18,153 main.py:51] epoch 3160, training loss: 13265.64, average training loss: 13035.57, base loss: 21635.34
[INFO 2017-06-28 19:13:19,407 main.py:51] epoch 3161, training loss: 13209.77, average training loss: 13034.96, base loss: 21633.94
[INFO 2017-06-28 19:13:20,789 main.py:51] epoch 3162, training loss: 13164.48, average training loss: 13033.73, base loss: 21633.94
[INFO 2017-06-28 19:13:22,122 main.py:51] epoch 3163, training loss: 12542.24, average training loss: 13033.72, base loss: 21634.04
[INFO 2017-06-28 19:13:23,365 main.py:51] epoch 3164, training loss: 12074.70, average training loss: 13033.85, base loss: 21634.89
[INFO 2017-06-28 19:13:24,807 main.py:51] epoch 3165, training loss: 12594.42, average training loss: 13033.35, base loss: 21632.07
[INFO 2017-06-28 19:13:26,183 main.py:51] epoch 3166, training loss: 12427.04, average training loss: 13032.85, base loss: 21629.43
[INFO 2017-06-28 19:13:27,544 main.py:51] epoch 3167, training loss: 12526.09, average training loss: 13032.18, base loss: 21628.58
[INFO 2017-06-28 19:13:28,919 main.py:51] epoch 3168, training loss: 12725.77, average training loss: 13031.23, base loss: 21627.53
[INFO 2017-06-28 19:13:30,379 main.py:51] epoch 3169, training loss: 12672.66, average training loss: 13029.82, base loss: 21628.36
[INFO 2017-06-28 19:13:31,725 main.py:51] epoch 3170, training loss: 12223.06, average training loss: 13028.15, base loss: 21626.36
[INFO 2017-06-28 19:13:33,110 main.py:51] epoch 3171, training loss: 13799.78, average training loss: 13027.90, base loss: 21625.73
[INFO 2017-06-28 19:13:34,425 main.py:51] epoch 3172, training loss: 12332.70, average training loss: 13027.62, base loss: 21624.16
[INFO 2017-06-28 19:13:35,732 main.py:51] epoch 3173, training loss: 15071.56, average training loss: 13029.85, base loss: 21628.20
[INFO 2017-06-28 19:13:36,976 main.py:51] epoch 3174, training loss: 13136.61, average training loss: 13029.87, base loss: 21629.35
[INFO 2017-06-28 19:13:38,292 main.py:51] epoch 3175, training loss: 12383.46, average training loss: 13029.40, base loss: 21628.09
[INFO 2017-06-28 19:13:39,557 main.py:51] epoch 3176, training loss: 12092.54, average training loss: 13028.63, base loss: 21627.19
[INFO 2017-06-28 19:13:40,877 main.py:51] epoch 3177, training loss: 13147.68, average training loss: 13027.88, base loss: 21626.50
[INFO 2017-06-28 19:13:42,163 main.py:51] epoch 3178, training loss: 13728.40, average training loss: 13028.51, base loss: 21625.81
[INFO 2017-06-28 19:13:43,450 main.py:51] epoch 3179, training loss: 11775.56, average training loss: 13026.50, base loss: 21620.96
[INFO 2017-06-28 19:13:44,858 main.py:51] epoch 3180, training loss: 14701.89, average training loss: 13027.63, base loss: 21623.76
[INFO 2017-06-28 19:13:46,215 main.py:51] epoch 3181, training loss: 13569.11, average training loss: 13029.12, base loss: 21626.23
[INFO 2017-06-28 19:13:47,484 main.py:51] epoch 3182, training loss: 13307.10, average training loss: 13029.67, base loss: 21627.78
[INFO 2017-06-28 19:13:48,707 main.py:51] epoch 3183, training loss: 13659.67, average training loss: 13028.79, base loss: 21624.94
[INFO 2017-06-28 19:13:49,983 main.py:51] epoch 3184, training loss: 12513.90, average training loss: 13028.37, base loss: 21622.45
[INFO 2017-06-28 19:13:51,252 main.py:51] epoch 3185, training loss: 12672.44, average training loss: 13027.42, base loss: 21621.60
[INFO 2017-06-28 19:13:52,448 main.py:51] epoch 3186, training loss: 13164.36, average training loss: 13028.29, base loss: 21621.88
[INFO 2017-06-28 19:13:53,817 main.py:51] epoch 3187, training loss: 10940.58, average training loss: 13025.25, base loss: 21618.59
[INFO 2017-06-28 19:13:55,189 main.py:51] epoch 3188, training loss: 12088.69, average training loss: 13024.65, base loss: 21618.68
[INFO 2017-06-28 19:13:56,545 main.py:51] epoch 3189, training loss: 12728.30, average training loss: 13024.29, base loss: 21620.77
[INFO 2017-06-28 19:13:57,971 main.py:51] epoch 3190, training loss: 12405.27, average training loss: 13023.97, base loss: 21622.21
[INFO 2017-06-28 19:13:59,312 main.py:51] epoch 3191, training loss: 11554.47, average training loss: 13021.80, base loss: 21619.70
[INFO 2017-06-28 19:14:00,663 main.py:51] epoch 3192, training loss: 13852.72, average training loss: 13022.52, base loss: 21620.77
[INFO 2017-06-28 19:14:02,075 main.py:51] epoch 3193, training loss: 12712.00, average training loss: 13022.66, base loss: 21622.63
[INFO 2017-06-28 19:14:03,353 main.py:51] epoch 3194, training loss: 13028.63, average training loss: 13022.15, base loss: 21623.51
[INFO 2017-06-28 19:14:04,722 main.py:51] epoch 3195, training loss: 14771.14, average training loss: 13025.39, base loss: 21629.61
[INFO 2017-06-28 19:14:06,105 main.py:51] epoch 3196, training loss: 14028.27, average training loss: 13025.72, base loss: 21633.19
[INFO 2017-06-28 19:14:07,405 main.py:51] epoch 3197, training loss: 13774.56, average training loss: 13027.14, base loss: 21634.92
[INFO 2017-06-28 19:14:08,768 main.py:51] epoch 3198, training loss: 15150.92, average training loss: 13026.68, base loss: 21635.68
[INFO 2017-06-28 19:14:10,122 main.py:51] epoch 3199, training loss: 12517.32, average training loss: 13027.76, base loss: 21637.72
[INFO 2017-06-28 19:14:10,122 main.py:53] epoch 3199, testing
[INFO 2017-06-28 19:14:16,070 main.py:105] average testing loss: 14012.31, base loss: 21693.65
[INFO 2017-06-28 19:14:16,070 main.py:106] improve_loss: 7681.34, improve_percent: 0.35
[INFO 2017-06-28 19:14:16,071 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:14:17,370 main.py:51] epoch 3200, training loss: 13541.54, average training loss: 13026.55, base loss: 21636.30
[INFO 2017-06-28 19:14:18,671 main.py:51] epoch 3201, training loss: 12414.86, average training loss: 13025.09, base loss: 21634.51
[INFO 2017-06-28 19:14:20,094 main.py:51] epoch 3202, training loss: 13246.54, average training loss: 13025.71, base loss: 21635.32
[INFO 2017-06-28 19:14:21,437 main.py:51] epoch 3203, training loss: 14346.55, average training loss: 13028.45, base loss: 21638.67
[INFO 2017-06-28 19:14:22,726 main.py:51] epoch 3204, training loss: 12368.76, average training loss: 13026.64, base loss: 21635.99
[INFO 2017-06-28 19:14:24,042 main.py:51] epoch 3205, training loss: 14219.49, average training loss: 13027.76, base loss: 21637.72
[INFO 2017-06-28 19:14:25,261 main.py:51] epoch 3206, training loss: 12842.03, average training loss: 13028.41, base loss: 21638.83
[INFO 2017-06-28 19:14:26,715 main.py:51] epoch 3207, training loss: 13000.58, average training loss: 13028.32, base loss: 21639.09
[INFO 2017-06-28 19:14:27,951 main.py:51] epoch 3208, training loss: 12647.49, average training loss: 13028.24, base loss: 21640.58
[INFO 2017-06-28 19:14:29,365 main.py:51] epoch 3209, training loss: 14069.62, average training loss: 13029.30, base loss: 21639.44
[INFO 2017-06-28 19:14:30,768 main.py:51] epoch 3210, training loss: 13278.35, average training loss: 13029.24, base loss: 21641.39
[INFO 2017-06-28 19:14:32,093 main.py:51] epoch 3211, training loss: 13322.78, average training loss: 13029.56, base loss: 21641.24
[INFO 2017-06-28 19:14:33,452 main.py:51] epoch 3212, training loss: 12053.58, average training loss: 13026.21, base loss: 21637.03
[INFO 2017-06-28 19:14:34,722 main.py:51] epoch 3213, training loss: 13885.44, average training loss: 13026.64, base loss: 21635.16
[INFO 2017-06-28 19:14:36,039 main.py:51] epoch 3214, training loss: 12301.76, average training loss: 13025.69, base loss: 21631.96
[INFO 2017-06-28 19:14:37,317 main.py:51] epoch 3215, training loss: 12411.27, average training loss: 13025.32, base loss: 21632.55
[INFO 2017-06-28 19:14:38,624 main.py:51] epoch 3216, training loss: 13322.09, average training loss: 13024.76, base loss: 21632.32
[INFO 2017-06-28 19:14:39,905 main.py:51] epoch 3217, training loss: 14253.85, average training loss: 13025.41, base loss: 21634.29
[INFO 2017-06-28 19:14:41,123 main.py:51] epoch 3218, training loss: 12182.76, average training loss: 13023.71, base loss: 21632.15
[INFO 2017-06-28 19:14:42,406 main.py:51] epoch 3219, training loss: 11917.50, average training loss: 13022.79, base loss: 21633.32
[INFO 2017-06-28 19:14:43,706 main.py:51] epoch 3220, training loss: 12294.97, average training loss: 13019.67, base loss: 21629.76
[INFO 2017-06-28 19:14:45,062 main.py:51] epoch 3221, training loss: 14879.48, average training loss: 13021.60, base loss: 21629.85
[INFO 2017-06-28 19:14:46,293 main.py:51] epoch 3222, training loss: 12416.82, average training loss: 13021.46, base loss: 21630.69
[INFO 2017-06-28 19:14:47,489 main.py:51] epoch 3223, training loss: 12731.12, average training loss: 13021.99, base loss: 21633.95
[INFO 2017-06-28 19:14:48,779 main.py:51] epoch 3224, training loss: 13089.90, average training loss: 13022.50, base loss: 21634.13
[INFO 2017-06-28 19:14:50,050 main.py:51] epoch 3225, training loss: 12214.54, average training loss: 13023.11, base loss: 21634.79
[INFO 2017-06-28 19:14:51,468 main.py:51] epoch 3226, training loss: 12014.23, average training loss: 13021.20, base loss: 21631.10
[INFO 2017-06-28 19:14:52,748 main.py:51] epoch 3227, training loss: 12915.13, average training loss: 13020.89, base loss: 21630.16
[INFO 2017-06-28 19:14:53,994 main.py:51] epoch 3228, training loss: 12242.66, average training loss: 13019.32, base loss: 21625.88
[INFO 2017-06-28 19:14:55,328 main.py:51] epoch 3229, training loss: 13848.68, average training loss: 13019.34, base loss: 21628.39
[INFO 2017-06-28 19:14:56,620 main.py:51] epoch 3230, training loss: 12332.94, average training loss: 13019.76, base loss: 21629.94
[INFO 2017-06-28 19:14:57,961 main.py:51] epoch 3231, training loss: 13051.57, average training loss: 13020.70, base loss: 21634.03
[INFO 2017-06-28 19:14:59,174 main.py:51] epoch 3232, training loss: 11534.37, average training loss: 13018.36, base loss: 21631.46
[INFO 2017-06-28 19:15:00,405 main.py:51] epoch 3233, training loss: 13521.89, average training loss: 13018.05, base loss: 21632.27
[INFO 2017-06-28 19:15:01,674 main.py:51] epoch 3234, training loss: 12748.88, average training loss: 13017.21, base loss: 21631.49
[INFO 2017-06-28 19:15:03,141 main.py:51] epoch 3235, training loss: 14259.29, average training loss: 13017.90, base loss: 21630.75
[INFO 2017-06-28 19:15:04,458 main.py:51] epoch 3236, training loss: 11560.91, average training loss: 13017.55, base loss: 21631.71
[INFO 2017-06-28 19:15:05,758 main.py:51] epoch 3237, training loss: 14417.61, average training loss: 13016.99, base loss: 21632.04
[INFO 2017-06-28 19:15:07,023 main.py:51] epoch 3238, training loss: 12197.38, average training loss: 13016.76, base loss: 21634.62
[INFO 2017-06-28 19:15:08,461 main.py:51] epoch 3239, training loss: 12766.95, average training loss: 13014.36, base loss: 21633.66
[INFO 2017-06-28 19:15:09,773 main.py:51] epoch 3240, training loss: 12371.72, average training loss: 13012.69, base loss: 21631.82
[INFO 2017-06-28 19:15:11,094 main.py:51] epoch 3241, training loss: 13574.37, average training loss: 13013.71, base loss: 21633.12
[INFO 2017-06-28 19:15:12,268 main.py:51] epoch 3242, training loss: 11803.91, average training loss: 13009.61, base loss: 21628.64
[INFO 2017-06-28 19:15:13,615 main.py:51] epoch 3243, training loss: 12111.81, average training loss: 13008.66, base loss: 21626.79
[INFO 2017-06-28 19:15:15,018 main.py:51] epoch 3244, training loss: 13408.38, average training loss: 13007.46, base loss: 21624.23
[INFO 2017-06-28 19:15:16,255 main.py:51] epoch 3245, training loss: 13004.60, average training loss: 13009.05, base loss: 21626.16
[INFO 2017-06-28 19:15:17,504 main.py:51] epoch 3246, training loss: 14324.86, average training loss: 13010.15, base loss: 21627.14
[INFO 2017-06-28 19:15:18,934 main.py:51] epoch 3247, training loss: 13160.65, average training loss: 13009.82, base loss: 21624.92
[INFO 2017-06-28 19:15:20,297 main.py:51] epoch 3248, training loss: 12730.51, average training loss: 13010.16, base loss: 21625.55
[INFO 2017-06-28 19:15:21,638 main.py:51] epoch 3249, training loss: 13017.92, average training loss: 13010.23, base loss: 21627.49
[INFO 2017-06-28 19:15:22,935 main.py:51] epoch 3250, training loss: 12176.81, average training loss: 13008.79, base loss: 21628.50
[INFO 2017-06-28 19:15:24,247 main.py:51] epoch 3251, training loss: 13074.26, average training loss: 13008.04, base loss: 21627.42
[INFO 2017-06-28 19:15:25,547 main.py:51] epoch 3252, training loss: 13464.81, average training loss: 13009.60, base loss: 21630.06
[INFO 2017-06-28 19:15:26,936 main.py:51] epoch 3253, training loss: 12767.24, average training loss: 13008.80, base loss: 21629.33
[INFO 2017-06-28 19:15:28,328 main.py:51] epoch 3254, training loss: 13061.73, average training loss: 13007.56, base loss: 21627.85
[INFO 2017-06-28 19:15:29,649 main.py:51] epoch 3255, training loss: 12986.89, average training loss: 13008.17, base loss: 21626.97
[INFO 2017-06-28 19:15:31,049 main.py:51] epoch 3256, training loss: 13085.86, average training loss: 13008.38, base loss: 21629.41
[INFO 2017-06-28 19:15:32,385 main.py:51] epoch 3257, training loss: 12969.31, average training loss: 13007.98, base loss: 21632.30
[INFO 2017-06-28 19:15:33,692 main.py:51] epoch 3258, training loss: 12561.69, average training loss: 13007.15, base loss: 21633.03
[INFO 2017-06-28 19:15:34,879 main.py:51] epoch 3259, training loss: 12392.73, average training loss: 13005.78, base loss: 21632.10
[INFO 2017-06-28 19:15:36,210 main.py:51] epoch 3260, training loss: 13678.45, average training loss: 13005.39, base loss: 21631.74
[INFO 2017-06-28 19:15:37,480 main.py:51] epoch 3261, training loss: 12117.02, average training loss: 13003.22, base loss: 21629.92
[INFO 2017-06-28 19:15:38,737 main.py:51] epoch 3262, training loss: 12881.01, average training loss: 13002.63, base loss: 21629.37
[INFO 2017-06-28 19:15:40,021 main.py:51] epoch 3263, training loss: 13883.93, average training loss: 13004.55, base loss: 21632.42
[INFO 2017-06-28 19:15:41,310 main.py:51] epoch 3264, training loss: 13691.05, average training loss: 13005.66, base loss: 21634.59
[INFO 2017-06-28 19:15:42,645 main.py:51] epoch 3265, training loss: 13297.25, average training loss: 13004.72, base loss: 21635.08
[INFO 2017-06-28 19:15:43,924 main.py:51] epoch 3266, training loss: 11977.94, average training loss: 13004.99, base loss: 21636.17
[INFO 2017-06-28 19:15:45,279 main.py:51] epoch 3267, training loss: 15523.65, average training loss: 13008.44, base loss: 21640.20
[INFO 2017-06-28 19:15:46,656 main.py:51] epoch 3268, training loss: 12658.85, average training loss: 13007.38, base loss: 21640.01
[INFO 2017-06-28 19:15:48,026 main.py:51] epoch 3269, training loss: 14253.23, average training loss: 13009.68, base loss: 21642.94
[INFO 2017-06-28 19:15:49,435 main.py:51] epoch 3270, training loss: 13365.15, average training loss: 13011.08, base loss: 21644.05
[INFO 2017-06-28 19:15:50,689 main.py:51] epoch 3271, training loss: 12244.72, average training loss: 13008.87, base loss: 21640.94
[INFO 2017-06-28 19:15:52,089 main.py:51] epoch 3272, training loss: 14364.03, average training loss: 13009.90, base loss: 21642.53
[INFO 2017-06-28 19:15:53,397 main.py:51] epoch 3273, training loss: 12807.47, average training loss: 13010.87, base loss: 21643.00
[INFO 2017-06-28 19:15:54,753 main.py:51] epoch 3274, training loss: 13287.73, average training loss: 13011.88, base loss: 21641.80
[INFO 2017-06-28 19:15:56,043 main.py:51] epoch 3275, training loss: 14217.23, average training loss: 13013.14, base loss: 21645.92
[INFO 2017-06-28 19:15:57,400 main.py:51] epoch 3276, training loss: 12456.42, average training loss: 13011.92, base loss: 21643.27
[INFO 2017-06-28 19:15:58,758 main.py:51] epoch 3277, training loss: 13118.70, average training loss: 13009.49, base loss: 21641.57
[INFO 2017-06-28 19:16:00,011 main.py:51] epoch 3278, training loss: 13546.49, average training loss: 13008.93, base loss: 21640.77
[INFO 2017-06-28 19:16:01,346 main.py:51] epoch 3279, training loss: 14007.80, average training loss: 13009.01, base loss: 21642.66
[INFO 2017-06-28 19:16:02,733 main.py:51] epoch 3280, training loss: 11964.30, average training loss: 13006.89, base loss: 21640.55
[INFO 2017-06-28 19:16:04,021 main.py:51] epoch 3281, training loss: 12526.03, average training loss: 13005.63, base loss: 21638.62
[INFO 2017-06-28 19:16:05,358 main.py:51] epoch 3282, training loss: 14096.34, average training loss: 13006.70, base loss: 21640.54
[INFO 2017-06-28 19:16:06,748 main.py:51] epoch 3283, training loss: 13101.16, average training loss: 13006.28, base loss: 21641.98
[INFO 2017-06-28 19:16:08,004 main.py:51] epoch 3284, training loss: 14236.96, average training loss: 13008.62, base loss: 21644.67
[INFO 2017-06-28 19:16:09,304 main.py:51] epoch 3285, training loss: 12752.63, average training loss: 13008.29, base loss: 21646.15
[INFO 2017-06-28 19:16:10,678 main.py:51] epoch 3286, training loss: 12898.67, average training loss: 13007.29, base loss: 21646.96
[INFO 2017-06-28 19:16:12,048 main.py:51] epoch 3287, training loss: 13361.22, average training loss: 13007.40, base loss: 21647.93
[INFO 2017-06-28 19:16:13,322 main.py:51] epoch 3288, training loss: 13947.35, average training loss: 13009.14, base loss: 21653.69
[INFO 2017-06-28 19:16:14,508 main.py:51] epoch 3289, training loss: 13280.38, average training loss: 13010.89, base loss: 21656.50
[INFO 2017-06-28 19:16:15,748 main.py:51] epoch 3290, training loss: 12525.74, average training loss: 13011.82, base loss: 21659.06
[INFO 2017-06-28 19:16:17,147 main.py:51] epoch 3291, training loss: 14325.56, average training loss: 13013.71, base loss: 21661.42
[INFO 2017-06-28 19:16:18,436 main.py:51] epoch 3292, training loss: 13469.86, average training loss: 13013.84, base loss: 21662.21
[INFO 2017-06-28 19:16:19,705 main.py:51] epoch 3293, training loss: 12962.27, average training loss: 13013.03, base loss: 21660.83
[INFO 2017-06-28 19:16:20,872 main.py:51] epoch 3294, training loss: 12155.64, average training loss: 13013.11, base loss: 21660.35
[INFO 2017-06-28 19:16:22,199 main.py:51] epoch 3295, training loss: 12204.00, average training loss: 13011.79, base loss: 21659.07
[INFO 2017-06-28 19:16:23,546 main.py:51] epoch 3296, training loss: 12311.31, average training loss: 13011.77, base loss: 21660.81
[INFO 2017-06-28 19:16:24,968 main.py:51] epoch 3297, training loss: 12713.91, average training loss: 13012.01, base loss: 21662.94
[INFO 2017-06-28 19:16:26,312 main.py:51] epoch 3298, training loss: 11951.20, average training loss: 13012.29, base loss: 21664.85
[INFO 2017-06-28 19:16:27,614 main.py:51] epoch 3299, training loss: 12715.48, average training loss: 13011.83, base loss: 21662.31
[INFO 2017-06-28 19:16:27,614 main.py:53] epoch 3299, testing
[INFO 2017-06-28 19:16:33,840 main.py:105] average testing loss: 14064.44, base loss: 21854.80
[INFO 2017-06-28 19:16:33,840 main.py:106] improve_loss: 7790.36, improve_percent: 0.36
[INFO 2017-06-28 19:16:33,840 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:16:35,164 main.py:51] epoch 3300, training loss: 12091.96, average training loss: 13009.32, base loss: 21659.68
[INFO 2017-06-28 19:16:36,519 main.py:51] epoch 3301, training loss: 13162.93, average training loss: 13008.11, base loss: 21659.59
[INFO 2017-06-28 19:16:37,900 main.py:51] epoch 3302, training loss: 14733.37, average training loss: 13011.96, base loss: 21664.10
[INFO 2017-06-28 19:16:39,253 main.py:51] epoch 3303, training loss: 13258.01, average training loss: 13013.27, base loss: 21664.38
[INFO 2017-06-28 19:16:40,589 main.py:51] epoch 3304, training loss: 15277.94, average training loss: 13015.95, base loss: 21664.95
[INFO 2017-06-28 19:16:42,062 main.py:51] epoch 3305, training loss: 12734.11, average training loss: 13015.05, base loss: 21661.58
[INFO 2017-06-28 19:16:43,476 main.py:51] epoch 3306, training loss: 13557.03, average training loss: 13014.28, base loss: 21661.94
[INFO 2017-06-28 19:16:44,750 main.py:51] epoch 3307, training loss: 12709.86, average training loss: 13012.91, base loss: 21662.72
[INFO 2017-06-28 19:16:46,104 main.py:51] epoch 3308, training loss: 12209.69, average training loss: 13012.38, base loss: 21663.87
[INFO 2017-06-28 19:16:47,481 main.py:51] epoch 3309, training loss: 12490.61, average training loss: 13011.69, base loss: 21661.90
[INFO 2017-06-28 19:16:48,926 main.py:51] epoch 3310, training loss: 12688.05, average training loss: 13011.62, base loss: 21661.84
[INFO 2017-06-28 19:16:50,310 main.py:51] epoch 3311, training loss: 12922.17, average training loss: 13012.11, base loss: 21661.88
[INFO 2017-06-28 19:16:51,575 main.py:51] epoch 3312, training loss: 12568.15, average training loss: 13010.64, base loss: 21660.90
[INFO 2017-06-28 19:16:52,952 main.py:51] epoch 3313, training loss: 13385.95, average training loss: 13009.97, base loss: 21662.19
[INFO 2017-06-28 19:16:54,283 main.py:51] epoch 3314, training loss: 13793.56, average training loss: 13009.82, base loss: 21663.90
[INFO 2017-06-28 19:16:55,608 main.py:51] epoch 3315, training loss: 11372.52, average training loss: 13008.81, base loss: 21663.99
[INFO 2017-06-28 19:16:56,955 main.py:51] epoch 3316, training loss: 13045.72, average training loss: 13008.91, base loss: 21661.68
[INFO 2017-06-28 19:16:58,349 main.py:51] epoch 3317, training loss: 12385.12, average training loss: 13008.15, base loss: 21660.40
[INFO 2017-06-28 19:16:59,592 main.py:51] epoch 3318, training loss: 13349.11, average training loss: 13009.03, base loss: 21662.83
[INFO 2017-06-28 19:17:00,941 main.py:51] epoch 3319, training loss: 12176.78, average training loss: 13009.12, base loss: 21662.13
[INFO 2017-06-28 19:17:02,190 main.py:51] epoch 3320, training loss: 14175.80, average training loss: 13010.49, base loss: 21663.75
[INFO 2017-06-28 19:17:03,481 main.py:51] epoch 3321, training loss: 13100.03, average training loss: 13011.11, base loss: 21664.41
[INFO 2017-06-28 19:17:04,921 main.py:51] epoch 3322, training loss: 13273.27, average training loss: 13011.52, base loss: 21666.56
[INFO 2017-06-28 19:17:06,285 main.py:51] epoch 3323, training loss: 11279.46, average training loss: 13009.25, base loss: 21663.91
[INFO 2017-06-28 19:17:07,610 main.py:51] epoch 3324, training loss: 12321.41, average training loss: 13007.01, base loss: 21661.33
[INFO 2017-06-28 19:17:08,874 main.py:51] epoch 3325, training loss: 13322.62, average training loss: 13007.81, base loss: 21661.91
[INFO 2017-06-28 19:17:10,272 main.py:51] epoch 3326, training loss: 13340.42, average training loss: 13008.63, base loss: 21663.83
[INFO 2017-06-28 19:17:11,487 main.py:51] epoch 3327, training loss: 13931.03, average training loss: 13010.62, base loss: 21667.46
[INFO 2017-06-28 19:17:12,882 main.py:51] epoch 3328, training loss: 12168.08, average training loss: 13009.58, base loss: 21666.80
[INFO 2017-06-28 19:17:14,142 main.py:51] epoch 3329, training loss: 13387.22, average training loss: 13008.34, base loss: 21666.32
[INFO 2017-06-28 19:17:15,442 main.py:51] epoch 3330, training loss: 12399.57, average training loss: 13008.31, base loss: 21665.83
[INFO 2017-06-28 19:17:16,672 main.py:51] epoch 3331, training loss: 12578.47, average training loss: 13008.26, base loss: 21665.03
[INFO 2017-06-28 19:17:18,070 main.py:51] epoch 3332, training loss: 13913.05, average training loss: 13009.12, base loss: 21667.19
[INFO 2017-06-28 19:17:19,414 main.py:51] epoch 3333, training loss: 14280.44, average training loss: 13009.16, base loss: 21666.28
[INFO 2017-06-28 19:17:20,658 main.py:51] epoch 3334, training loss: 11661.35, average training loss: 13007.13, base loss: 21664.10
[INFO 2017-06-28 19:17:21,858 main.py:51] epoch 3335, training loss: 14499.94, average training loss: 13009.04, base loss: 21667.41
[INFO 2017-06-28 19:17:23,147 main.py:51] epoch 3336, training loss: 12628.04, average training loss: 13007.22, base loss: 21665.99
[INFO 2017-06-28 19:17:24,457 main.py:51] epoch 3337, training loss: 14407.26, average training loss: 13009.12, base loss: 21667.70
[INFO 2017-06-28 19:17:25,676 main.py:51] epoch 3338, training loss: 12881.29, average training loss: 13009.95, base loss: 21669.61
[INFO 2017-06-28 19:17:26,940 main.py:51] epoch 3339, training loss: 12790.07, average training loss: 13010.25, base loss: 21670.35
[INFO 2017-06-28 19:17:28,150 main.py:51] epoch 3340, training loss: 13096.26, average training loss: 13009.51, base loss: 21670.71
[INFO 2017-06-28 19:17:29,463 main.py:51] epoch 3341, training loss: 12597.38, average training loss: 13008.06, base loss: 21669.95
[INFO 2017-06-28 19:17:30,819 main.py:51] epoch 3342, training loss: 13860.22, average training loss: 13009.63, base loss: 21669.66
[INFO 2017-06-28 19:17:32,044 main.py:51] epoch 3343, training loss: 12487.05, average training loss: 13010.57, base loss: 21672.11
[INFO 2017-06-28 19:17:33,345 main.py:51] epoch 3344, training loss: 12919.54, average training loss: 13008.75, base loss: 21668.26
[INFO 2017-06-28 19:17:34,609 main.py:51] epoch 3345, training loss: 13253.98, average training loss: 13009.81, base loss: 21668.94
[INFO 2017-06-28 19:17:35,927 main.py:51] epoch 3346, training loss: 13432.56, average training loss: 13010.78, base loss: 21671.70
[INFO 2017-06-28 19:17:37,296 main.py:51] epoch 3347, training loss: 13474.83, average training loss: 13010.49, base loss: 21670.19
[INFO 2017-06-28 19:17:38,658 main.py:51] epoch 3348, training loss: 13095.57, average training loss: 13009.90, base loss: 21672.36
[INFO 2017-06-28 19:17:39,907 main.py:51] epoch 3349, training loss: 13165.90, average training loss: 13010.59, base loss: 21674.44
[INFO 2017-06-28 19:17:41,184 main.py:51] epoch 3350, training loss: 12418.22, average training loss: 13008.76, base loss: 21670.72
[INFO 2017-06-28 19:17:42,493 main.py:51] epoch 3351, training loss: 14003.98, average training loss: 13008.86, base loss: 21671.83
[INFO 2017-06-28 19:17:43,842 main.py:51] epoch 3352, training loss: 14278.75, average training loss: 13009.98, base loss: 21672.78
[INFO 2017-06-28 19:17:45,194 main.py:51] epoch 3353, training loss: 12751.52, average training loss: 13008.81, base loss: 21672.13
[INFO 2017-06-28 19:17:46,535 main.py:51] epoch 3354, training loss: 12991.57, average training loss: 13009.39, base loss: 21674.93
[INFO 2017-06-28 19:17:47,868 main.py:51] epoch 3355, training loss: 13716.00, average training loss: 13010.20, base loss: 21676.35
[INFO 2017-06-28 19:17:49,219 main.py:51] epoch 3356, training loss: 12361.20, average training loss: 13008.51, base loss: 21674.83
[INFO 2017-06-28 19:17:50,400 main.py:51] epoch 3357, training loss: 11767.86, average training loss: 13006.60, base loss: 21674.60
[INFO 2017-06-28 19:17:51,745 main.py:51] epoch 3358, training loss: 13025.18, average training loss: 13007.96, base loss: 21675.92
[INFO 2017-06-28 19:17:52,959 main.py:51] epoch 3359, training loss: 13429.05, average training loss: 13007.73, base loss: 21675.17
[INFO 2017-06-28 19:17:54,358 main.py:51] epoch 3360, training loss: 13814.55, average training loss: 13008.27, base loss: 21675.16
[INFO 2017-06-28 19:17:55,638 main.py:51] epoch 3361, training loss: 12535.00, average training loss: 13008.59, base loss: 21675.11
[INFO 2017-06-28 19:17:56,866 main.py:51] epoch 3362, training loss: 12956.89, average training loss: 13007.36, base loss: 21675.86
[INFO 2017-06-28 19:17:58,144 main.py:51] epoch 3363, training loss: 14994.87, average training loss: 13010.00, base loss: 21678.08
[INFO 2017-06-28 19:17:59,502 main.py:51] epoch 3364, training loss: 11906.25, average training loss: 13008.48, base loss: 21675.94
[INFO 2017-06-28 19:18:00,755 main.py:51] epoch 3365, training loss: 12444.48, average training loss: 13008.16, base loss: 21675.24
[INFO 2017-06-28 19:18:02,072 main.py:51] epoch 3366, training loss: 13905.35, average training loss: 13009.30, base loss: 21676.08
[INFO 2017-06-28 19:18:03,354 main.py:51] epoch 3367, training loss: 11675.79, average training loss: 13006.83, base loss: 21672.10
[INFO 2017-06-28 19:18:04,663 main.py:51] epoch 3368, training loss: 14282.74, average training loss: 13008.39, base loss: 21674.06
[INFO 2017-06-28 19:18:05,971 main.py:51] epoch 3369, training loss: 13429.15, average training loss: 13009.10, base loss: 21673.79
[INFO 2017-06-28 19:18:07,288 main.py:51] epoch 3370, training loss: 12555.04, average training loss: 13009.20, base loss: 21672.64
[INFO 2017-06-28 19:18:08,588 main.py:51] epoch 3371, training loss: 13339.74, average training loss: 13008.52, base loss: 21672.83
[INFO 2017-06-28 19:18:09,989 main.py:51] epoch 3372, training loss: 13146.08, average training loss: 13008.59, base loss: 21670.59
[INFO 2017-06-28 19:18:11,311 main.py:51] epoch 3373, training loss: 12732.31, average training loss: 13009.01, base loss: 21671.55
[INFO 2017-06-28 19:18:12,640 main.py:51] epoch 3374, training loss: 13049.96, average training loss: 13010.50, base loss: 21671.62
[INFO 2017-06-28 19:18:13,985 main.py:51] epoch 3375, training loss: 12380.70, average training loss: 13009.72, base loss: 21670.42
[INFO 2017-06-28 19:18:15,407 main.py:51] epoch 3376, training loss: 12855.54, average training loss: 13009.10, base loss: 21670.06
[INFO 2017-06-28 19:18:16,692 main.py:51] epoch 3377, training loss: 14561.11, average training loss: 13010.09, base loss: 21669.31
[INFO 2017-06-28 19:18:18,116 main.py:51] epoch 3378, training loss: 13806.16, average training loss: 13011.53, base loss: 21671.74
[INFO 2017-06-28 19:18:19,518 main.py:51] epoch 3379, training loss: 11903.12, average training loss: 13010.16, base loss: 21670.17
[INFO 2017-06-28 19:18:20,883 main.py:51] epoch 3380, training loss: 11917.92, average training loss: 13009.07, base loss: 21668.40
[INFO 2017-06-28 19:18:22,250 main.py:51] epoch 3381, training loss: 12003.45, average training loss: 13009.39, base loss: 21669.01
[INFO 2017-06-28 19:18:23,618 main.py:51] epoch 3382, training loss: 14791.06, average training loss: 13011.00, base loss: 21671.62
[INFO 2017-06-28 19:18:24,870 main.py:51] epoch 3383, training loss: 14673.87, average training loss: 13014.20, base loss: 21676.84
[INFO 2017-06-28 19:18:26,085 main.py:51] epoch 3384, training loss: 13578.59, average training loss: 13015.21, base loss: 21678.82
[INFO 2017-06-28 19:18:27,361 main.py:51] epoch 3385, training loss: 14030.77, average training loss: 13015.58, base loss: 21677.91
[INFO 2017-06-28 19:18:28,643 main.py:51] epoch 3386, training loss: 12813.64, average training loss: 13013.89, base loss: 21674.04
[INFO 2017-06-28 19:18:29,955 main.py:51] epoch 3387, training loss: 11697.04, average training loss: 13013.15, base loss: 21676.06
[INFO 2017-06-28 19:18:31,207 main.py:51] epoch 3388, training loss: 12892.17, average training loss: 13011.56, base loss: 21675.97
[INFO 2017-06-28 19:18:32,539 main.py:51] epoch 3389, training loss: 13261.81, average training loss: 13012.76, base loss: 21676.65
[INFO 2017-06-28 19:18:33,794 main.py:51] epoch 3390, training loss: 12461.39, average training loss: 13011.82, base loss: 21676.92
[INFO 2017-06-28 19:18:35,131 main.py:51] epoch 3391, training loss: 12384.92, average training loss: 13011.99, base loss: 21676.11
[INFO 2017-06-28 19:18:36,534 main.py:51] epoch 3392, training loss: 11923.50, average training loss: 13009.60, base loss: 21672.19
[INFO 2017-06-28 19:18:37,875 main.py:51] epoch 3393, training loss: 14190.25, average training loss: 13009.35, base loss: 21673.08
[INFO 2017-06-28 19:18:39,132 main.py:51] epoch 3394, training loss: 12291.61, average training loss: 13008.14, base loss: 21671.49
[INFO 2017-06-28 19:18:40,527 main.py:51] epoch 3395, training loss: 11370.19, average training loss: 13006.07, base loss: 21667.89
[INFO 2017-06-28 19:18:41,848 main.py:51] epoch 3396, training loss: 13999.86, average training loss: 13006.64, base loss: 21670.25
[INFO 2017-06-28 19:18:43,149 main.py:51] epoch 3397, training loss: 12823.69, average training loss: 13004.79, base loss: 21670.50
[INFO 2017-06-28 19:18:44,476 main.py:51] epoch 3398, training loss: 10757.71, average training loss: 13002.16, base loss: 21668.27
[INFO 2017-06-28 19:18:45,753 main.py:51] epoch 3399, training loss: 12478.74, average training loss: 13002.33, base loss: 21667.31
[INFO 2017-06-28 19:18:45,754 main.py:53] epoch 3399, testing
[INFO 2017-06-28 19:18:51,854 main.py:105] average testing loss: 13962.91, base loss: 21537.75
[INFO 2017-06-28 19:18:51,854 main.py:106] improve_loss: 7574.84, improve_percent: 0.35
[INFO 2017-06-28 19:18:51,855 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:18:53,151 main.py:51] epoch 3400, training loss: 12693.58, average training loss: 13001.42, base loss: 21666.15
[INFO 2017-06-28 19:18:54,523 main.py:51] epoch 3401, training loss: 12813.46, average training loss: 13001.30, base loss: 21666.81
[INFO 2017-06-28 19:18:55,881 main.py:51] epoch 3402, training loss: 12989.06, average training loss: 12999.83, base loss: 21665.85
[INFO 2017-06-28 19:18:57,176 main.py:51] epoch 3403, training loss: 11840.94, average training loss: 12997.06, base loss: 21661.19
[INFO 2017-06-28 19:18:58,485 main.py:51] epoch 3404, training loss: 13453.14, average training loss: 12997.91, base loss: 21663.28
[INFO 2017-06-28 19:18:59,739 main.py:51] epoch 3405, training loss: 12668.16, average training loss: 12997.88, base loss: 21663.72
[INFO 2017-06-28 19:19:01,032 main.py:51] epoch 3406, training loss: 11961.27, average training loss: 12997.42, base loss: 21664.93
[INFO 2017-06-28 19:19:02,536 main.py:51] epoch 3407, training loss: 13611.03, average training loss: 12996.24, base loss: 21662.28
[INFO 2017-06-28 19:19:03,873 main.py:51] epoch 3408, training loss: 12149.71, average training loss: 12991.87, base loss: 21657.07
[INFO 2017-06-28 19:19:05,218 main.py:51] epoch 3409, training loss: 13585.13, average training loss: 12993.05, base loss: 21659.01
[INFO 2017-06-28 19:19:06,513 main.py:51] epoch 3410, training loss: 11440.57, average training loss: 12991.99, base loss: 21658.31
[INFO 2017-06-28 19:19:07,730 main.py:51] epoch 3411, training loss: 12660.44, average training loss: 12992.01, base loss: 21658.70
[INFO 2017-06-28 19:19:09,038 main.py:51] epoch 3412, training loss: 13557.43, average training loss: 12989.59, base loss: 21656.32
[INFO 2017-06-28 19:19:10,511 main.py:51] epoch 3413, training loss: 13567.08, average training loss: 12990.34, base loss: 21655.85
[INFO 2017-06-28 19:19:11,766 main.py:51] epoch 3414, training loss: 13214.74, average training loss: 12991.28, base loss: 21657.39
[INFO 2017-06-28 19:19:13,068 main.py:51] epoch 3415, training loss: 14713.18, average training loss: 12993.54, base loss: 21660.04
[INFO 2017-06-28 19:19:14,458 main.py:51] epoch 3416, training loss: 12195.31, average training loss: 12991.43, base loss: 21659.02
[INFO 2017-06-28 19:19:15,698 main.py:51] epoch 3417, training loss: 12098.03, average training loss: 12991.66, base loss: 21659.67
[INFO 2017-06-28 19:19:17,034 main.py:51] epoch 3418, training loss: 12187.43, average training loss: 12989.67, base loss: 21656.27
[INFO 2017-06-28 19:19:18,427 main.py:51] epoch 3419, training loss: 14784.97, average training loss: 12990.46, base loss: 21657.60
[INFO 2017-06-28 19:19:19,754 main.py:51] epoch 3420, training loss: 13665.67, average training loss: 12993.04, base loss: 21662.63
[INFO 2017-06-28 19:19:21,072 main.py:51] epoch 3421, training loss: 13621.74, average training loss: 12995.46, base loss: 21665.33
[INFO 2017-06-28 19:19:22,489 main.py:51] epoch 3422, training loss: 11550.96, average training loss: 12994.58, base loss: 21663.37
[INFO 2017-06-28 19:19:23,787 main.py:51] epoch 3423, training loss: 13726.51, average training loss: 12993.49, base loss: 21661.81
[INFO 2017-06-28 19:19:25,360 main.py:51] epoch 3424, training loss: 13591.56, average training loss: 12993.70, base loss: 21660.13
[INFO 2017-06-28 19:19:26,706 main.py:51] epoch 3425, training loss: 13711.88, average training loss: 12993.09, base loss: 21660.50
[INFO 2017-06-28 19:19:28,043 main.py:51] epoch 3426, training loss: 12147.94, average training loss: 12992.19, base loss: 21659.83
[INFO 2017-06-28 19:19:29,422 main.py:51] epoch 3427, training loss: 12125.83, average training loss: 12991.03, base loss: 21657.22
[INFO 2017-06-28 19:19:30,682 main.py:51] epoch 3428, training loss: 12672.39, average training loss: 12989.77, base loss: 21654.63
[INFO 2017-06-28 19:19:31,991 main.py:51] epoch 3429, training loss: 13166.00, average training loss: 12990.23, base loss: 21657.96
[INFO 2017-06-28 19:19:33,334 main.py:51] epoch 3430, training loss: 11905.39, average training loss: 12989.05, base loss: 21658.02
[INFO 2017-06-28 19:19:34,590 main.py:51] epoch 3431, training loss: 12943.40, average training loss: 12988.81, base loss: 21657.21
[INFO 2017-06-28 19:19:35,962 main.py:51] epoch 3432, training loss: 13211.33, average training loss: 12988.97, base loss: 21657.00
[INFO 2017-06-28 19:19:37,294 main.py:51] epoch 3433, training loss: 12506.45, average training loss: 12989.95, base loss: 21661.12
[INFO 2017-06-28 19:19:38,571 main.py:51] epoch 3434, training loss: 12889.20, average training loss: 12988.79, base loss: 21660.33
[INFO 2017-06-28 19:19:39,887 main.py:51] epoch 3435, training loss: 12998.96, average training loss: 12989.20, base loss: 21662.49
[INFO 2017-06-28 19:19:41,156 main.py:51] epoch 3436, training loss: 12793.16, average training loss: 12989.19, base loss: 21662.32
[INFO 2017-06-28 19:19:42,494 main.py:51] epoch 3437, training loss: 11617.57, average training loss: 12989.42, base loss: 21663.19
[INFO 2017-06-28 19:19:43,719 main.py:51] epoch 3438, training loss: 12665.77, average training loss: 12986.60, base loss: 21663.77
[INFO 2017-06-28 19:19:45,019 main.py:51] epoch 3439, training loss: 12248.57, average training loss: 12986.19, base loss: 21662.70
[INFO 2017-06-28 19:19:46,280 main.py:51] epoch 3440, training loss: 11671.26, average training loss: 12984.96, base loss: 21660.05
[INFO 2017-06-28 19:19:47,692 main.py:51] epoch 3441, training loss: 12025.73, average training loss: 12983.75, base loss: 21656.22
[INFO 2017-06-28 19:19:49,090 main.py:51] epoch 3442, training loss: 14259.00, average training loss: 12984.14, base loss: 21655.36
[INFO 2017-06-28 19:19:50,337 main.py:51] epoch 3443, training loss: 12450.73, average training loss: 12982.57, base loss: 21655.75
[INFO 2017-06-28 19:19:51,648 main.py:51] epoch 3444, training loss: 14459.43, average training loss: 12984.00, base loss: 21655.22
[INFO 2017-06-28 19:19:52,953 main.py:51] epoch 3445, training loss: 14204.53, average training loss: 12985.17, base loss: 21658.24
[INFO 2017-06-28 19:19:54,198 main.py:51] epoch 3446, training loss: 13877.09, average training loss: 12984.74, base loss: 21660.26
[INFO 2017-06-28 19:19:55,460 main.py:51] epoch 3447, training loss: 12234.74, average training loss: 12981.59, base loss: 21657.40
[INFO 2017-06-28 19:19:56,840 main.py:51] epoch 3448, training loss: 11818.84, average training loss: 12981.10, base loss: 21657.58
[INFO 2017-06-28 19:19:58,109 main.py:51] epoch 3449, training loss: 13385.48, average training loss: 12980.93, base loss: 21657.83
[INFO 2017-06-28 19:19:59,435 main.py:51] epoch 3450, training loss: 12802.08, average training loss: 12980.82, base loss: 21657.27
[INFO 2017-06-28 19:20:00,788 main.py:51] epoch 3451, training loss: 11919.41, average training loss: 12980.58, base loss: 21657.23
[INFO 2017-06-28 19:20:02,121 main.py:51] epoch 3452, training loss: 12802.16, average training loss: 12982.11, base loss: 21660.29
[INFO 2017-06-28 19:20:03,399 main.py:51] epoch 3453, training loss: 12055.58, average training loss: 12981.78, base loss: 21660.97
[INFO 2017-06-28 19:20:04,875 main.py:51] epoch 3454, training loss: 13874.15, average training loss: 12983.01, base loss: 21662.09
[INFO 2017-06-28 19:20:06,231 main.py:51] epoch 3455, training loss: 13173.82, average training loss: 12983.09, base loss: 21664.67
[INFO 2017-06-28 19:20:07,448 main.py:51] epoch 3456, training loss: 14248.41, average training loss: 12984.24, base loss: 21667.09
[INFO 2017-06-28 19:20:08,789 main.py:51] epoch 3457, training loss: 13352.79, average training loss: 12984.51, base loss: 21668.14
[INFO 2017-06-28 19:20:10,072 main.py:51] epoch 3458, training loss: 12112.49, average training loss: 12984.00, base loss: 21667.62
[INFO 2017-06-28 19:20:11,283 main.py:51] epoch 3459, training loss: 12037.20, average training loss: 12982.16, base loss: 21666.51
[INFO 2017-06-28 19:20:12,621 main.py:51] epoch 3460, training loss: 12522.25, average training loss: 12980.62, base loss: 21667.28
[INFO 2017-06-28 19:20:13,921 main.py:51] epoch 3461, training loss: 14156.38, average training loss: 12982.38, base loss: 21669.53
[INFO 2017-06-28 19:20:15,223 main.py:51] epoch 3462, training loss: 11718.75, average training loss: 12980.52, base loss: 21667.43
[INFO 2017-06-28 19:20:16,551 main.py:51] epoch 3463, training loss: 12237.91, average training loss: 12979.81, base loss: 21667.89
[INFO 2017-06-28 19:20:17,882 main.py:51] epoch 3464, training loss: 12145.48, average training loss: 12978.87, base loss: 21666.95
[INFO 2017-06-28 19:20:19,260 main.py:51] epoch 3465, training loss: 12173.41, average training loss: 12977.31, base loss: 21664.79
[INFO 2017-06-28 19:20:20,618 main.py:51] epoch 3466, training loss: 12650.63, average training loss: 12978.09, base loss: 21666.94
[INFO 2017-06-28 19:20:21,873 main.py:51] epoch 3467, training loss: 12178.99, average training loss: 12978.53, base loss: 21668.49
[INFO 2017-06-28 19:20:23,166 main.py:51] epoch 3468, training loss: 11213.52, average training loss: 12977.48, base loss: 21669.04
[INFO 2017-06-28 19:20:24,452 main.py:51] epoch 3469, training loss: 12138.03, average training loss: 12976.18, base loss: 21667.58
[INFO 2017-06-28 19:20:25,809 main.py:51] epoch 3470, training loss: 12528.00, average training loss: 12975.61, base loss: 21667.21
[INFO 2017-06-28 19:20:27,159 main.py:51] epoch 3471, training loss: 12221.47, average training loss: 12974.77, base loss: 21666.87
[INFO 2017-06-28 19:20:28,345 main.py:51] epoch 3472, training loss: 12903.47, average training loss: 12973.32, base loss: 21665.53
[INFO 2017-06-28 19:20:29,623 main.py:51] epoch 3473, training loss: 11504.65, average training loss: 12970.94, base loss: 21664.88
[INFO 2017-06-28 19:20:30,980 main.py:51] epoch 3474, training loss: 11828.13, average training loss: 12969.55, base loss: 21663.78
[INFO 2017-06-28 19:20:32,256 main.py:51] epoch 3475, training loss: 12206.58, average training loss: 12969.80, base loss: 21664.72
[INFO 2017-06-28 19:20:33,550 main.py:51] epoch 3476, training loss: 12693.29, average training loss: 12969.38, base loss: 21664.30
[INFO 2017-06-28 19:20:34,886 main.py:51] epoch 3477, training loss: 11787.16, average training loss: 12968.46, base loss: 21663.62
[INFO 2017-06-28 19:20:36,215 main.py:51] epoch 3478, training loss: 12882.06, average training loss: 12966.73, base loss: 21662.35
[INFO 2017-06-28 19:20:37,546 main.py:51] epoch 3479, training loss: 13146.08, average training loss: 12967.41, base loss: 21663.37
[INFO 2017-06-28 19:20:38,862 main.py:51] epoch 3480, training loss: 11801.68, average training loss: 12965.64, base loss: 21663.90
[INFO 2017-06-28 19:20:40,238 main.py:51] epoch 3481, training loss: 13872.07, average training loss: 12965.26, base loss: 21662.29
[INFO 2017-06-28 19:20:41,491 main.py:51] epoch 3482, training loss: 12624.26, average training loss: 12964.16, base loss: 21658.85
[INFO 2017-06-28 19:20:42,819 main.py:51] epoch 3483, training loss: 12526.90, average training loss: 12963.44, base loss: 21658.54
[INFO 2017-06-28 19:20:44,168 main.py:51] epoch 3484, training loss: 11521.29, average training loss: 12962.11, base loss: 21656.08
[INFO 2017-06-28 19:20:45,510 main.py:51] epoch 3485, training loss: 13648.43, average training loss: 12962.98, base loss: 21657.29
[INFO 2017-06-28 19:20:46,940 main.py:51] epoch 3486, training loss: 12432.26, average training loss: 12961.28, base loss: 21654.83
[INFO 2017-06-28 19:20:48,165 main.py:51] epoch 3487, training loss: 12043.06, average training loss: 12960.12, base loss: 21651.86
[INFO 2017-06-28 19:20:49,474 main.py:51] epoch 3488, training loss: 13192.22, average training loss: 12960.10, base loss: 21652.95
[INFO 2017-06-28 19:20:50,818 main.py:51] epoch 3489, training loss: 12023.68, average training loss: 12959.30, base loss: 21652.65
[INFO 2017-06-28 19:20:52,024 main.py:51] epoch 3490, training loss: 14206.51, average training loss: 12961.15, base loss: 21654.55
[INFO 2017-06-28 19:20:53,377 main.py:51] epoch 3491, training loss: 12656.56, average training loss: 12960.63, base loss: 21653.45
[INFO 2017-06-28 19:20:54,695 main.py:51] epoch 3492, training loss: 13147.95, average training loss: 12960.82, base loss: 21654.21
[INFO 2017-06-28 19:20:56,068 main.py:51] epoch 3493, training loss: 11562.75, average training loss: 12959.65, base loss: 21651.25
[INFO 2017-06-28 19:20:57,380 main.py:51] epoch 3494, training loss: 13292.24, average training loss: 12960.00, base loss: 21651.47
[INFO 2017-06-28 19:20:58,644 main.py:51] epoch 3495, training loss: 13114.94, average training loss: 12959.80, base loss: 21648.37
[INFO 2017-06-28 19:20:59,905 main.py:51] epoch 3496, training loss: 14104.45, average training loss: 12961.88, base loss: 21651.48
[INFO 2017-06-28 19:21:01,144 main.py:51] epoch 3497, training loss: 14856.51, average training loss: 12964.08, base loss: 21652.69
[INFO 2017-06-28 19:21:02,388 main.py:51] epoch 3498, training loss: 12641.07, average training loss: 12963.18, base loss: 21650.52
[INFO 2017-06-28 19:21:03,615 main.py:51] epoch 3499, training loss: 11418.55, average training loss: 12961.63, base loss: 21648.07
[INFO 2017-06-28 19:21:03,616 main.py:53] epoch 3499, testing
[INFO 2017-06-28 19:21:09,771 main.py:105] average testing loss: 14022.56, base loss: 22132.61
[INFO 2017-06-28 19:21:09,771 main.py:106] improve_loss: 8110.05, improve_percent: 0.37
[INFO 2017-06-28 19:21:09,771 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:21:11,060 main.py:51] epoch 3500, training loss: 13029.21, average training loss: 12961.73, base loss: 21647.32
[INFO 2017-06-28 19:21:12,334 main.py:51] epoch 3501, training loss: 12922.31, average training loss: 12960.50, base loss: 21643.69
[INFO 2017-06-28 19:21:13,737 main.py:51] epoch 3502, training loss: 13554.38, average training loss: 12960.67, base loss: 21645.91
[INFO 2017-06-28 19:21:15,126 main.py:51] epoch 3503, training loss: 11020.23, average training loss: 12957.95, base loss: 21642.78
[INFO 2017-06-28 19:21:16,378 main.py:51] epoch 3504, training loss: 13692.48, average training loss: 12958.89, base loss: 21644.61
[INFO 2017-06-28 19:21:17,633 main.py:51] epoch 3505, training loss: 12522.50, average training loss: 12958.29, base loss: 21643.93
[INFO 2017-06-28 19:21:19,074 main.py:51] epoch 3506, training loss: 13522.30, average training loss: 12959.18, base loss: 21643.13
[INFO 2017-06-28 19:21:20,394 main.py:51] epoch 3507, training loss: 12797.45, average training loss: 12959.52, base loss: 21645.12
[INFO 2017-06-28 19:21:21,808 main.py:51] epoch 3508, training loss: 13673.10, average training loss: 12960.46, base loss: 21648.43
[INFO 2017-06-28 19:21:23,138 main.py:51] epoch 3509, training loss: 13381.12, average training loss: 12960.35, base loss: 21648.02
[INFO 2017-06-28 19:21:24,470 main.py:51] epoch 3510, training loss: 11856.54, average training loss: 12959.30, base loss: 21646.40
[INFO 2017-06-28 19:21:25,847 main.py:51] epoch 3511, training loss: 12258.95, average training loss: 12958.30, base loss: 21644.50
[INFO 2017-06-28 19:21:27,104 main.py:51] epoch 3512, training loss: 11749.05, average training loss: 12956.51, base loss: 21642.75
[INFO 2017-06-28 19:21:28,407 main.py:51] epoch 3513, training loss: 12485.11, average training loss: 12955.18, base loss: 21642.97
[INFO 2017-06-28 19:21:29,830 main.py:51] epoch 3514, training loss: 12966.92, average training loss: 12955.85, base loss: 21643.47
[INFO 2017-06-28 19:21:31,108 main.py:51] epoch 3515, training loss: 14197.78, average training loss: 12957.38, base loss: 21644.86
[INFO 2017-06-28 19:21:32,378 main.py:51] epoch 3516, training loss: 12094.64, average training loss: 12955.20, base loss: 21640.71
[INFO 2017-06-28 19:21:33,768 main.py:51] epoch 3517, training loss: 12375.48, average training loss: 12954.05, base loss: 21638.34
[INFO 2017-06-28 19:21:35,191 main.py:51] epoch 3518, training loss: 13490.48, average training loss: 12954.06, base loss: 21639.75
[INFO 2017-06-28 19:21:36,575 main.py:51] epoch 3519, training loss: 11789.52, average training loss: 12952.30, base loss: 21639.47
[INFO 2017-06-28 19:21:37,878 main.py:51] epoch 3520, training loss: 14603.16, average training loss: 12952.08, base loss: 21639.68
[INFO 2017-06-28 19:21:39,133 main.py:51] epoch 3521, training loss: 13021.62, average training loss: 12951.64, base loss: 21639.18
[INFO 2017-06-28 19:21:40,472 main.py:51] epoch 3522, training loss: 12909.41, average training loss: 12953.58, base loss: 21642.27
[INFO 2017-06-28 19:21:41,770 main.py:51] epoch 3523, training loss: 11275.96, average training loss: 12952.90, base loss: 21642.48
[INFO 2017-06-28 19:21:43,042 main.py:51] epoch 3524, training loss: 13510.12, average training loss: 12951.86, base loss: 21643.32
[INFO 2017-06-28 19:21:44,359 main.py:51] epoch 3525, training loss: 12988.52, average training loss: 12951.24, base loss: 21643.03
[INFO 2017-06-28 19:21:45,711 main.py:51] epoch 3526, training loss: 13971.69, average training loss: 12952.21, base loss: 21643.40
[INFO 2017-06-28 19:21:47,014 main.py:51] epoch 3527, training loss: 12169.93, average training loss: 12950.12, base loss: 21641.57
[INFO 2017-06-28 19:21:48,358 main.py:51] epoch 3528, training loss: 12641.76, average training loss: 12950.30, base loss: 21641.32
[INFO 2017-06-28 19:21:49,600 main.py:51] epoch 3529, training loss: 13287.94, average training loss: 12950.92, base loss: 21641.77
[INFO 2017-06-28 19:21:50,908 main.py:51] epoch 3530, training loss: 13462.02, average training loss: 12951.88, base loss: 21643.00
[INFO 2017-06-28 19:21:52,159 main.py:51] epoch 3531, training loss: 12914.39, average training loss: 12951.64, base loss: 21642.42
[INFO 2017-06-28 19:21:53,490 main.py:51] epoch 3532, training loss: 13576.09, average training loss: 12952.27, base loss: 21643.62
[INFO 2017-06-28 19:21:54,832 main.py:51] epoch 3533, training loss: 12036.73, average training loss: 12952.13, base loss: 21644.59
[INFO 2017-06-28 19:21:56,087 main.py:51] epoch 3534, training loss: 13154.41, average training loss: 12952.03, base loss: 21642.76
[INFO 2017-06-28 19:21:57,395 main.py:51] epoch 3535, training loss: 12624.37, average training loss: 12951.27, base loss: 21642.16
[INFO 2017-06-28 19:21:58,692 main.py:51] epoch 3536, training loss: 11970.58, average training loss: 12950.30, base loss: 21640.07
[INFO 2017-06-28 19:22:00,038 main.py:51] epoch 3537, training loss: 11811.22, average training loss: 12949.98, base loss: 21639.77
[INFO 2017-06-28 19:22:01,333 main.py:51] epoch 3538, training loss: 12803.62, average training loss: 12949.17, base loss: 21639.51
[INFO 2017-06-28 19:22:02,603 main.py:51] epoch 3539, training loss: 12623.27, average training loss: 12949.79, base loss: 21641.21
[INFO 2017-06-28 19:22:03,824 main.py:51] epoch 3540, training loss: 13065.58, average training loss: 12949.41, base loss: 21642.56
[INFO 2017-06-28 19:22:05,242 main.py:51] epoch 3541, training loss: 12516.30, average training loss: 12947.74, base loss: 21638.21
[INFO 2017-06-28 19:22:06,485 main.py:51] epoch 3542, training loss: 12212.77, average training loss: 12947.29, base loss: 21638.36
[INFO 2017-06-28 19:22:07,736 main.py:51] epoch 3543, training loss: 11510.15, average training loss: 12946.99, base loss: 21638.59
[INFO 2017-06-28 19:22:09,010 main.py:51] epoch 3544, training loss: 12919.90, average training loss: 12945.71, base loss: 21636.40
[INFO 2017-06-28 19:22:10,318 main.py:51] epoch 3545, training loss: 12732.61, average training loss: 12946.31, base loss: 21637.07
[INFO 2017-06-28 19:22:11,538 main.py:51] epoch 3546, training loss: 13004.02, average training loss: 12946.85, base loss: 21638.63
[INFO 2017-06-28 19:22:12,848 main.py:51] epoch 3547, training loss: 13498.46, average training loss: 12946.71, base loss: 21638.34
[INFO 2017-06-28 19:22:14,227 main.py:51] epoch 3548, training loss: 12483.74, average training loss: 12947.60, base loss: 21639.85
[INFO 2017-06-28 19:22:15,565 main.py:51] epoch 3549, training loss: 12444.92, average training loss: 12947.74, base loss: 21639.49
[INFO 2017-06-28 19:22:16,825 main.py:51] epoch 3550, training loss: 12734.12, average training loss: 12947.75, base loss: 21640.67
[INFO 2017-06-28 19:22:18,048 main.py:51] epoch 3551, training loss: 12505.62, average training loss: 12947.36, base loss: 21638.62
[INFO 2017-06-28 19:22:19,379 main.py:51] epoch 3552, training loss: 11952.11, average training loss: 12946.14, base loss: 21639.04
[INFO 2017-06-28 19:22:20,725 main.py:51] epoch 3553, training loss: 12552.13, average training loss: 12945.24, base loss: 21639.39
[INFO 2017-06-28 19:22:22,072 main.py:51] epoch 3554, training loss: 12442.50, average training loss: 12944.66, base loss: 21639.70
[INFO 2017-06-28 19:22:23,324 main.py:51] epoch 3555, training loss: 12154.29, average training loss: 12943.87, base loss: 21640.01
[INFO 2017-06-28 19:22:24,735 main.py:51] epoch 3556, training loss: 12475.87, average training loss: 12943.78, base loss: 21639.98
[INFO 2017-06-28 19:22:26,126 main.py:51] epoch 3557, training loss: 13041.38, average training loss: 12944.79, base loss: 21639.86
[INFO 2017-06-28 19:22:27,390 main.py:51] epoch 3558, training loss: 13184.78, average training loss: 12943.20, base loss: 21639.18
[INFO 2017-06-28 19:22:28,767 main.py:51] epoch 3559, training loss: 13364.88, average training loss: 12942.01, base loss: 21638.21
[INFO 2017-06-28 19:22:30,077 main.py:51] epoch 3560, training loss: 14817.96, average training loss: 12944.30, base loss: 21643.74
[INFO 2017-06-28 19:22:31,406 main.py:51] epoch 3561, training loss: 14078.71, average training loss: 12946.80, base loss: 21645.29
[INFO 2017-06-28 19:22:32,708 main.py:51] epoch 3562, training loss: 12706.29, average training loss: 12946.74, base loss: 21645.53
[INFO 2017-06-28 19:22:34,048 main.py:51] epoch 3563, training loss: 12347.93, average training loss: 12946.39, base loss: 21645.07
[INFO 2017-06-28 19:22:35,344 main.py:51] epoch 3564, training loss: 11939.97, average training loss: 12944.92, base loss: 21642.25
[INFO 2017-06-28 19:22:36,655 main.py:51] epoch 3565, training loss: 13561.85, average training loss: 12944.78, base loss: 21643.62
[INFO 2017-06-28 19:22:37,960 main.py:51] epoch 3566, training loss: 12730.31, average training loss: 12945.09, base loss: 21645.37
[INFO 2017-06-28 19:22:39,197 main.py:51] epoch 3567, training loss: 12390.76, average training loss: 12944.16, base loss: 21646.65
[INFO 2017-06-28 19:22:40,560 main.py:51] epoch 3568, training loss: 11218.75, average training loss: 12942.90, base loss: 21645.11
[INFO 2017-06-28 19:22:41,785 main.py:51] epoch 3569, training loss: 12345.62, average training loss: 12942.55, base loss: 21645.65
[INFO 2017-06-28 19:22:43,044 main.py:51] epoch 3570, training loss: 14533.81, average training loss: 12943.93, base loss: 21648.59
[INFO 2017-06-28 19:22:44,283 main.py:51] epoch 3571, training loss: 12774.07, average training loss: 12943.82, base loss: 21648.69
[INFO 2017-06-28 19:22:45,551 main.py:51] epoch 3572, training loss: 12018.69, average training loss: 12941.27, base loss: 21646.40
[INFO 2017-06-28 19:22:46,876 main.py:51] epoch 3573, training loss: 13631.02, average training loss: 12942.81, base loss: 21650.29
[INFO 2017-06-28 19:22:48,162 main.py:51] epoch 3574, training loss: 13979.12, average training loss: 12944.08, base loss: 21651.87
[INFO 2017-06-28 19:22:49,458 main.py:51] epoch 3575, training loss: 13907.12, average training loss: 12944.50, base loss: 21653.11
[INFO 2017-06-28 19:22:50,712 main.py:51] epoch 3576, training loss: 13063.27, average training loss: 12944.84, base loss: 21653.40
[INFO 2017-06-28 19:22:51,988 main.py:51] epoch 3577, training loss: 14253.53, average training loss: 12946.57, base loss: 21654.94
[INFO 2017-06-28 19:22:53,259 main.py:51] epoch 3578, training loss: 12733.28, average training loss: 12946.72, base loss: 21656.19
[INFO 2017-06-28 19:22:54,607 main.py:51] epoch 3579, training loss: 13532.60, average training loss: 12946.71, base loss: 21656.95
[INFO 2017-06-28 19:22:55,959 main.py:51] epoch 3580, training loss: 11942.54, average training loss: 12944.59, base loss: 21654.03
[INFO 2017-06-28 19:22:57,257 main.py:51] epoch 3581, training loss: 12568.92, average training loss: 12942.35, base loss: 21651.84
[INFO 2017-06-28 19:22:58,498 main.py:51] epoch 3582, training loss: 13292.10, average training loss: 12943.74, base loss: 21656.05
[INFO 2017-06-28 19:22:59,793 main.py:51] epoch 3583, training loss: 12526.45, average training loss: 12942.21, base loss: 21655.40
[INFO 2017-06-28 19:23:01,115 main.py:51] epoch 3584, training loss: 13375.67, average training loss: 12943.61, base loss: 21656.91
[INFO 2017-06-28 19:23:02,473 main.py:51] epoch 3585, training loss: 13162.30, average training loss: 12945.30, base loss: 21659.66
[INFO 2017-06-28 19:23:03,785 main.py:51] epoch 3586, training loss: 11350.70, average training loss: 12942.84, base loss: 21657.35
[INFO 2017-06-28 19:23:05,129 main.py:51] epoch 3587, training loss: 11370.74, average training loss: 12941.63, base loss: 21655.55
[INFO 2017-06-28 19:23:06,372 main.py:51] epoch 3588, training loss: 12945.47, average training loss: 12942.44, base loss: 21658.43
[INFO 2017-06-28 19:23:07,680 main.py:51] epoch 3589, training loss: 12790.93, average training loss: 12942.05, base loss: 21657.25
[INFO 2017-06-28 19:23:09,097 main.py:51] epoch 3590, training loss: 13396.25, average training loss: 12942.93, base loss: 21655.78
[INFO 2017-06-28 19:23:10,447 main.py:51] epoch 3591, training loss: 13356.65, average training loss: 12943.35, base loss: 21656.20
[INFO 2017-06-28 19:23:11,791 main.py:51] epoch 3592, training loss: 13294.51, average training loss: 12943.98, base loss: 21659.30
[INFO 2017-06-28 19:23:13,079 main.py:51] epoch 3593, training loss: 12364.05, average training loss: 12944.27, base loss: 21660.34
[INFO 2017-06-28 19:23:14,438 main.py:51] epoch 3594, training loss: 13174.57, average training loss: 12946.15, base loss: 21662.92
[INFO 2017-06-28 19:23:15,748 main.py:51] epoch 3595, training loss: 10960.21, average training loss: 12945.16, base loss: 21662.02
[INFO 2017-06-28 19:23:17,011 main.py:51] epoch 3596, training loss: 13847.81, average training loss: 12945.50, base loss: 21662.17
[INFO 2017-06-28 19:23:18,470 main.py:51] epoch 3597, training loss: 11807.91, average training loss: 12944.48, base loss: 21659.97
[INFO 2017-06-28 19:23:19,699 main.py:51] epoch 3598, training loss: 14991.10, average training loss: 12946.09, base loss: 21662.32
[INFO 2017-06-28 19:23:20,949 main.py:51] epoch 3599, training loss: 13665.97, average training loss: 12945.98, base loss: 21663.04
[INFO 2017-06-28 19:23:20,950 main.py:53] epoch 3599, testing
[INFO 2017-06-28 19:23:27,076 main.py:105] average testing loss: 14410.66, base loss: 22259.82
[INFO 2017-06-28 19:23:27,076 main.py:106] improve_loss: 7849.16, improve_percent: 0.35
[INFO 2017-06-28 19:23:27,077 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:23:28,366 main.py:51] epoch 3600, training loss: 13764.88, average training loss: 12947.19, base loss: 21661.98
[INFO 2017-06-28 19:23:29,717 main.py:51] epoch 3601, training loss: 12825.51, average training loss: 12947.76, base loss: 21663.76
[INFO 2017-06-28 19:23:30,961 main.py:51] epoch 3602, training loss: 12708.11, average training loss: 12947.04, base loss: 21662.79
[INFO 2017-06-28 19:23:32,249 main.py:51] epoch 3603, training loss: 13995.14, average training loss: 12947.28, base loss: 21662.66
[INFO 2017-06-28 19:23:33,590 main.py:51] epoch 3604, training loss: 12615.76, average training loss: 12946.80, base loss: 21662.01
[INFO 2017-06-28 19:23:35,030 main.py:51] epoch 3605, training loss: 12785.91, average training loss: 12947.53, base loss: 21663.08
[INFO 2017-06-28 19:23:36,278 main.py:51] epoch 3606, training loss: 12933.61, average training loss: 12948.52, base loss: 21665.08
[INFO 2017-06-28 19:23:37,572 main.py:51] epoch 3607, training loss: 15375.97, average training loss: 12951.02, base loss: 21667.21
[INFO 2017-06-28 19:23:38,911 main.py:51] epoch 3608, training loss: 12390.71, average training loss: 12949.92, base loss: 21667.22
[INFO 2017-06-28 19:23:40,310 main.py:51] epoch 3609, training loss: 11122.23, average training loss: 12947.28, base loss: 21664.95
[INFO 2017-06-28 19:23:41,633 main.py:51] epoch 3610, training loss: 12763.79, average training loss: 12946.53, base loss: 21663.88
[INFO 2017-06-28 19:23:43,005 main.py:51] epoch 3611, training loss: 13100.86, average training loss: 12946.70, base loss: 21664.93
[INFO 2017-06-28 19:23:44,193 main.py:51] epoch 3612, training loss: 12480.91, average training loss: 12946.64, base loss: 21665.27
[INFO 2017-06-28 19:23:45,497 main.py:51] epoch 3613, training loss: 14639.82, average training loss: 12947.48, base loss: 21664.69
[INFO 2017-06-28 19:23:46,744 main.py:51] epoch 3614, training loss: 11937.40, average training loss: 12945.01, base loss: 21661.84
[INFO 2017-06-28 19:23:48,238 main.py:51] epoch 3615, training loss: 14229.21, average training loss: 12946.39, base loss: 21663.55
[INFO 2017-06-28 19:23:49,662 main.py:51] epoch 3616, training loss: 13175.36, average training loss: 12946.21, base loss: 21664.22
[INFO 2017-06-28 19:23:50,954 main.py:51] epoch 3617, training loss: 13061.65, average training loss: 12944.78, base loss: 21661.47
[INFO 2017-06-28 19:23:52,216 main.py:51] epoch 3618, training loss: 12544.89, average training loss: 12944.17, base loss: 21659.88
[INFO 2017-06-28 19:23:53,567 main.py:51] epoch 3619, training loss: 12613.20, average training loss: 12942.08, base loss: 21657.43
[INFO 2017-06-28 19:23:54,799 main.py:51] epoch 3620, training loss: 13101.75, average training loss: 12941.27, base loss: 21657.23
[INFO 2017-06-28 19:23:56,107 main.py:51] epoch 3621, training loss: 12494.66, average training loss: 12941.35, base loss: 21659.37
[INFO 2017-06-28 19:23:57,396 main.py:51] epoch 3622, training loss: 13091.53, average training loss: 12942.07, base loss: 21660.99
[INFO 2017-06-28 19:23:58,844 main.py:51] epoch 3623, training loss: 13299.13, average training loss: 12942.34, base loss: 21660.77
[INFO 2017-06-28 19:24:00,222 main.py:51] epoch 3624, training loss: 12195.70, average training loss: 12940.92, base loss: 21658.77
[INFO 2017-06-28 19:24:01,444 main.py:51] epoch 3625, training loss: 14515.46, average training loss: 12943.48, base loss: 21661.01
[INFO 2017-06-28 19:24:02,656 main.py:51] epoch 3626, training loss: 12730.20, average training loss: 12943.00, base loss: 21660.43
[INFO 2017-06-28 19:24:03,943 main.py:51] epoch 3627, training loss: 12440.78, average training loss: 12941.99, base loss: 21659.64
[INFO 2017-06-28 19:24:05,085 main.py:51] epoch 3628, training loss: 13038.67, average training loss: 12941.87, base loss: 21658.03
[INFO 2017-06-28 19:24:06,313 main.py:51] epoch 3629, training loss: 12931.57, average training loss: 12941.44, base loss: 21659.22
[INFO 2017-06-28 19:24:07,673 main.py:51] epoch 3630, training loss: 13131.28, average training loss: 12941.95, base loss: 21660.81
[INFO 2017-06-28 19:24:09,017 main.py:51] epoch 3631, training loss: 11983.31, average training loss: 12940.12, base loss: 21656.21
[INFO 2017-06-28 19:24:10,262 main.py:51] epoch 3632, training loss: 12182.17, average training loss: 12938.82, base loss: 21655.57
[INFO 2017-06-28 19:24:11,564 main.py:51] epoch 3633, training loss: 13112.88, average training loss: 12938.98, base loss: 21655.69
[INFO 2017-06-28 19:24:12,816 main.py:51] epoch 3634, training loss: 12864.18, average training loss: 12939.38, base loss: 21654.70
[INFO 2017-06-28 19:24:14,110 main.py:51] epoch 3635, training loss: 12504.75, average training loss: 12939.65, base loss: 21657.47
[INFO 2017-06-28 19:24:15,484 main.py:51] epoch 3636, training loss: 12532.90, average training loss: 12940.36, base loss: 21659.79
[INFO 2017-06-28 19:24:16,737 main.py:51] epoch 3637, training loss: 12652.05, average training loss: 12936.79, base loss: 21656.96
[INFO 2017-06-28 19:24:18,156 main.py:51] epoch 3638, training loss: 12776.68, average training loss: 12936.49, base loss: 21656.67
[INFO 2017-06-28 19:24:19,521 main.py:51] epoch 3639, training loss: 13713.19, average training loss: 12936.02, base loss: 21657.10
[INFO 2017-06-28 19:24:20,860 main.py:51] epoch 3640, training loss: 12842.13, average training loss: 12933.44, base loss: 21654.06
[INFO 2017-06-28 19:24:22,163 main.py:51] epoch 3641, training loss: 12596.77, average training loss: 12932.82, base loss: 21654.07
[INFO 2017-06-28 19:24:23,526 main.py:51] epoch 3642, training loss: 14558.23, average training loss: 12933.85, base loss: 21654.92
[INFO 2017-06-28 19:24:24,783 main.py:51] epoch 3643, training loss: 11793.34, average training loss: 12932.22, base loss: 21652.57
[INFO 2017-06-28 19:24:26,099 main.py:51] epoch 3644, training loss: 12946.20, average training loss: 12931.59, base loss: 21654.29
[INFO 2017-06-28 19:24:27,401 main.py:51] epoch 3645, training loss: 12543.17, average training loss: 12931.95, base loss: 21655.01
[INFO 2017-06-28 19:24:28,703 main.py:51] epoch 3646, training loss: 12111.66, average training loss: 12931.25, base loss: 21654.45
[INFO 2017-06-28 19:24:30,002 main.py:51] epoch 3647, training loss: 11022.06, average training loss: 12930.74, base loss: 21654.79
[INFO 2017-06-28 19:24:31,312 main.py:51] epoch 3648, training loss: 12371.21, average training loss: 12929.67, base loss: 21653.16
[INFO 2017-06-28 19:24:32,578 main.py:51] epoch 3649, training loss: 12491.56, average training loss: 12929.96, base loss: 21654.06
[INFO 2017-06-28 19:24:33,912 main.py:51] epoch 3650, training loss: 13815.66, average training loss: 12930.24, base loss: 21655.32
[INFO 2017-06-28 19:24:35,228 main.py:51] epoch 3651, training loss: 13525.59, average training loss: 12930.47, base loss: 21658.84
[INFO 2017-06-28 19:24:36,473 main.py:51] epoch 3652, training loss: 11908.82, average training loss: 12929.56, base loss: 21658.14
[INFO 2017-06-28 19:24:37,872 main.py:51] epoch 3653, training loss: 11614.21, average training loss: 12928.07, base loss: 21658.31
[INFO 2017-06-28 19:24:39,183 main.py:51] epoch 3654, training loss: 11747.07, average training loss: 12927.22, base loss: 21658.63
[INFO 2017-06-28 19:24:40,432 main.py:51] epoch 3655, training loss: 11375.02, average training loss: 12925.99, base loss: 21656.25
[INFO 2017-06-28 19:24:41,793 main.py:51] epoch 3656, training loss: 11683.15, average training loss: 12923.66, base loss: 21653.87
[INFO 2017-06-28 19:24:43,184 main.py:51] epoch 3657, training loss: 12676.26, average training loss: 12922.83, base loss: 21653.62
[INFO 2017-06-28 19:24:44,436 main.py:51] epoch 3658, training loss: 13922.20, average training loss: 12924.54, base loss: 21655.12
[INFO 2017-06-28 19:24:45,732 main.py:51] epoch 3659, training loss: 13042.84, average training loss: 12925.90, base loss: 21657.48
[INFO 2017-06-28 19:24:46,991 main.py:51] epoch 3660, training loss: 14178.47, average training loss: 12928.55, base loss: 21661.77
[INFO 2017-06-28 19:24:48,303 main.py:51] epoch 3661, training loss: 12778.59, average training loss: 12929.02, base loss: 21662.58
[INFO 2017-06-28 19:24:49,589 main.py:51] epoch 3662, training loss: 11972.92, average training loss: 12928.64, base loss: 21662.70
[INFO 2017-06-28 19:24:50,901 main.py:51] epoch 3663, training loss: 11911.46, average training loss: 12927.39, base loss: 21661.51
[INFO 2017-06-28 19:24:52,193 main.py:51] epoch 3664, training loss: 11900.50, average training loss: 12926.36, base loss: 21659.37
[INFO 2017-06-28 19:24:53,473 main.py:51] epoch 3665, training loss: 12504.53, average training loss: 12926.10, base loss: 21658.78
[INFO 2017-06-28 19:24:54,832 main.py:51] epoch 3666, training loss: 12945.96, average training loss: 12926.92, base loss: 21660.92
[INFO 2017-06-28 19:24:56,110 main.py:51] epoch 3667, training loss: 12263.28, average training loss: 12925.73, base loss: 21660.80
[INFO 2017-06-28 19:24:57,428 main.py:51] epoch 3668, training loss: 13391.58, average training loss: 12927.08, base loss: 21663.00
[INFO 2017-06-28 19:24:58,746 main.py:51] epoch 3669, training loss: 12992.52, average training loss: 12928.15, base loss: 21665.31
[INFO 2017-06-28 19:25:00,147 main.py:51] epoch 3670, training loss: 11397.37, average training loss: 12925.44, base loss: 21664.73
[INFO 2017-06-28 19:25:01,536 main.py:51] epoch 3671, training loss: 14225.30, average training loss: 12927.93, base loss: 21668.67
[INFO 2017-06-28 19:25:02,935 main.py:51] epoch 3672, training loss: 12266.89, average training loss: 12926.96, base loss: 21668.66
[INFO 2017-06-28 19:25:04,262 main.py:51] epoch 3673, training loss: 12663.28, average training loss: 12926.73, base loss: 21669.34
[INFO 2017-06-28 19:25:05,634 main.py:51] epoch 3674, training loss: 12707.76, average training loss: 12927.31, base loss: 21670.56
[INFO 2017-06-28 19:25:06,960 main.py:51] epoch 3675, training loss: 12565.82, average training loss: 12926.84, base loss: 21671.49
[INFO 2017-06-28 19:25:08,210 main.py:51] epoch 3676, training loss: 11745.41, average training loss: 12925.37, base loss: 21670.91
[INFO 2017-06-28 19:25:09,554 main.py:51] epoch 3677, training loss: 13446.79, average training loss: 12924.99, base loss: 21671.28
[INFO 2017-06-28 19:25:10,868 main.py:51] epoch 3678, training loss: 12321.06, average training loss: 12925.39, base loss: 21672.39
[INFO 2017-06-28 19:25:12,096 main.py:51] epoch 3679, training loss: 12881.67, average training loss: 12925.49, base loss: 21670.94
[INFO 2017-06-28 19:25:13,381 main.py:51] epoch 3680, training loss: 12871.45, average training loss: 12924.86, base loss: 21671.53
[INFO 2017-06-28 19:25:14,548 main.py:51] epoch 3681, training loss: 12813.09, average training loss: 12925.79, base loss: 21673.84
[INFO 2017-06-28 19:25:15,871 main.py:51] epoch 3682, training loss: 14359.06, average training loss: 12927.03, base loss: 21674.95
[INFO 2017-06-28 19:25:17,188 main.py:51] epoch 3683, training loss: 12552.72, average training loss: 12925.86, base loss: 21672.50
[INFO 2017-06-28 19:25:18,594 main.py:51] epoch 3684, training loss: 13125.51, average training loss: 12926.31, base loss: 21674.76
[INFO 2017-06-28 19:25:19,870 main.py:51] epoch 3685, training loss: 12428.64, average training loss: 12926.97, base loss: 21677.05
[INFO 2017-06-28 19:25:21,286 main.py:51] epoch 3686, training loss: 13325.30, average training loss: 12926.52, base loss: 21675.76
[INFO 2017-06-28 19:25:22,550 main.py:51] epoch 3687, training loss: 14413.24, average training loss: 12927.72, base loss: 21675.27
[INFO 2017-06-28 19:25:23,939 main.py:51] epoch 3688, training loss: 12372.57, average training loss: 12928.05, base loss: 21674.92
[INFO 2017-06-28 19:25:25,298 main.py:51] epoch 3689, training loss: 12175.60, average training loss: 12927.25, base loss: 21674.71
[INFO 2017-06-28 19:25:26,604 main.py:51] epoch 3690, training loss: 12859.64, average training loss: 12927.26, base loss: 21674.47
[INFO 2017-06-28 19:25:27,897 main.py:51] epoch 3691, training loss: 12050.78, average training loss: 12924.96, base loss: 21672.91
[INFO 2017-06-28 19:25:29,280 main.py:51] epoch 3692, training loss: 12352.66, average training loss: 12923.85, base loss: 21671.71
[INFO 2017-06-28 19:25:30,483 main.py:51] epoch 3693, training loss: 13715.27, average training loss: 12924.81, base loss: 21675.25
[INFO 2017-06-28 19:25:31,905 main.py:51] epoch 3694, training loss: 13852.87, average training loss: 12926.12, base loss: 21677.62
[INFO 2017-06-28 19:25:33,161 main.py:51] epoch 3695, training loss: 13806.21, average training loss: 12926.61, base loss: 21675.61
[INFO 2017-06-28 19:25:34,419 main.py:51] epoch 3696, training loss: 11858.45, average training loss: 12925.83, base loss: 21674.44
[INFO 2017-06-28 19:25:35,792 main.py:51] epoch 3697, training loss: 13045.01, average training loss: 12925.71, base loss: 21672.71
[INFO 2017-06-28 19:25:37,107 main.py:51] epoch 3698, training loss: 13205.47, average training loss: 12926.98, base loss: 21674.51
[INFO 2017-06-28 19:25:38,463 main.py:51] epoch 3699, training loss: 13435.13, average training loss: 12927.15, base loss: 21675.05
[INFO 2017-06-28 19:25:38,463 main.py:53] epoch 3699, testing
[INFO 2017-06-28 19:25:44,329 main.py:105] average testing loss: 13741.18, base loss: 21642.40
[INFO 2017-06-28 19:25:44,329 main.py:106] improve_loss: 7901.22, improve_percent: 0.37
[INFO 2017-06-28 19:25:44,329 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:25:45,610 main.py:51] epoch 3700, training loss: 13365.40, average training loss: 12928.30, base loss: 21675.73
[INFO 2017-06-28 19:25:46,902 main.py:51] epoch 3701, training loss: 13831.50, average training loss: 12930.18, base loss: 21677.48
[INFO 2017-06-28 19:25:48,185 main.py:51] epoch 3702, training loss: 12473.53, average training loss: 12929.73, base loss: 21677.09
[INFO 2017-06-28 19:25:49,584 main.py:51] epoch 3703, training loss: 13185.18, average training loss: 12930.38, base loss: 21677.80
[INFO 2017-06-28 19:25:50,858 main.py:51] epoch 3704, training loss: 12725.63, average training loss: 12929.13, base loss: 21675.62
[INFO 2017-06-28 19:25:52,074 main.py:51] epoch 3705, training loss: 12932.08, average training loss: 12930.47, base loss: 21677.32
[INFO 2017-06-28 19:25:53,328 main.py:51] epoch 3706, training loss: 12466.17, average training loss: 12930.70, base loss: 21677.44
[INFO 2017-06-28 19:25:54,747 main.py:51] epoch 3707, training loss: 14359.86, average training loss: 12930.70, base loss: 21674.99
[INFO 2017-06-28 19:25:56,106 main.py:51] epoch 3708, training loss: 14457.69, average training loss: 12933.44, base loss: 21678.83
[INFO 2017-06-28 19:25:57,506 main.py:51] epoch 3709, training loss: 12018.84, average training loss: 12931.97, base loss: 21675.78
[INFO 2017-06-28 19:25:58,729 main.py:51] epoch 3710, training loss: 12146.29, average training loss: 12931.64, base loss: 21675.11
[INFO 2017-06-28 19:26:00,169 main.py:51] epoch 3711, training loss: 12738.52, average training loss: 12932.68, base loss: 21676.19
[INFO 2017-06-28 19:26:01,434 main.py:51] epoch 3712, training loss: 13167.20, average training loss: 12932.38, base loss: 21675.21
[INFO 2017-06-28 19:26:02,713 main.py:51] epoch 3713, training loss: 14716.97, average training loss: 12933.34, base loss: 21676.04
[INFO 2017-06-28 19:26:04,062 main.py:51] epoch 3714, training loss: 12674.62, average training loss: 12932.98, base loss: 21674.34
[INFO 2017-06-28 19:26:05,399 main.py:51] epoch 3715, training loss: 12768.10, average training loss: 12933.36, base loss: 21675.51
[INFO 2017-06-28 19:26:06,711 main.py:51] epoch 3716, training loss: 11516.33, average training loss: 12933.05, base loss: 21674.07
[INFO 2017-06-28 19:26:07,920 main.py:51] epoch 3717, training loss: 14251.95, average training loss: 12934.19, base loss: 21675.72
[INFO 2017-06-28 19:26:09,203 main.py:51] epoch 3718, training loss: 12494.50, average training loss: 12934.75, base loss: 21678.71
[INFO 2017-06-28 19:26:10,548 main.py:51] epoch 3719, training loss: 12794.49, average training loss: 12934.50, base loss: 21678.53
[INFO 2017-06-28 19:26:11,964 main.py:51] epoch 3720, training loss: 12709.59, average training loss: 12933.23, base loss: 21677.82
[INFO 2017-06-28 19:26:13,201 main.py:51] epoch 3721, training loss: 11825.22, average training loss: 12931.39, base loss: 21675.10
[INFO 2017-06-28 19:26:14,681 main.py:51] epoch 3722, training loss: 13523.84, average training loss: 12933.28, base loss: 21678.49
[INFO 2017-06-28 19:26:15,936 main.py:51] epoch 3723, training loss: 13047.08, average training loss: 12933.65, base loss: 21680.65
[INFO 2017-06-28 19:26:17,244 main.py:51] epoch 3724, training loss: 12052.13, average training loss: 12932.67, base loss: 21678.66
[INFO 2017-06-28 19:26:18,620 main.py:51] epoch 3725, training loss: 11874.60, average training loss: 12931.37, base loss: 21676.24
[INFO 2017-06-28 19:26:19,954 main.py:51] epoch 3726, training loss: 13347.56, average training loss: 12928.90, base loss: 21672.17
[INFO 2017-06-28 19:26:21,305 main.py:51] epoch 3727, training loss: 12309.80, average training loss: 12928.27, base loss: 21673.75
[INFO 2017-06-28 19:26:22,698 main.py:51] epoch 3728, training loss: 12352.04, average training loss: 12928.31, base loss: 21674.61
[INFO 2017-06-28 19:26:24,016 main.py:51] epoch 3729, training loss: 12539.10, average training loss: 12928.65, base loss: 21674.60
[INFO 2017-06-28 19:26:25,332 main.py:51] epoch 3730, training loss: 13421.32, average training loss: 12927.96, base loss: 21673.13
[INFO 2017-06-28 19:26:26,712 main.py:51] epoch 3731, training loss: 12850.43, average training loss: 12928.19, base loss: 21672.28
[INFO 2017-06-28 19:26:28,054 main.py:51] epoch 3732, training loss: 11592.54, average training loss: 12926.53, base loss: 21668.95
[INFO 2017-06-28 19:26:29,568 main.py:51] epoch 3733, training loss: 11603.71, average training loss: 12926.35, base loss: 21668.48
[INFO 2017-06-28 19:26:30,835 main.py:51] epoch 3734, training loss: 12815.08, average training loss: 12925.43, base loss: 21666.83
[INFO 2017-06-28 19:26:32,164 main.py:51] epoch 3735, training loss: 12755.20, average training loss: 12925.35, base loss: 21667.96
[INFO 2017-06-28 19:26:33,472 main.py:51] epoch 3736, training loss: 12408.72, average training loss: 12924.79, base loss: 21666.28
[INFO 2017-06-28 19:26:34,830 main.py:51] epoch 3737, training loss: 12170.19, average training loss: 12924.19, base loss: 21662.64
[INFO 2017-06-28 19:26:36,112 main.py:51] epoch 3738, training loss: 13011.11, average training loss: 12924.16, base loss: 21663.35
[INFO 2017-06-28 19:26:37,361 main.py:51] epoch 3739, training loss: 12134.42, average training loss: 12923.31, base loss: 21661.56
[INFO 2017-06-28 19:26:38,745 main.py:51] epoch 3740, training loss: 12048.70, average training loss: 12923.43, base loss: 21661.37
[INFO 2017-06-28 19:26:40,065 main.py:51] epoch 3741, training loss: 12791.74, average training loss: 12923.42, base loss: 21662.44
[INFO 2017-06-28 19:26:41,418 main.py:51] epoch 3742, training loss: 12133.62, average training loss: 12922.97, base loss: 21661.42
[INFO 2017-06-28 19:26:42,667 main.py:51] epoch 3743, training loss: 14014.19, average training loss: 12923.77, base loss: 21663.74
[INFO 2017-06-28 19:26:44,026 main.py:51] epoch 3744, training loss: 13059.33, average training loss: 12924.37, base loss: 21663.60
[INFO 2017-06-28 19:26:45,410 main.py:51] epoch 3745, training loss: 14246.28, average training loss: 12926.80, base loss: 21666.83
[INFO 2017-06-28 19:26:46,693 main.py:51] epoch 3746, training loss: 12734.37, average training loss: 12927.46, base loss: 21670.82
[INFO 2017-06-28 19:26:48,010 main.py:51] epoch 3747, training loss: 14388.26, average training loss: 12928.70, base loss: 21675.07
[INFO 2017-06-28 19:26:49,360 main.py:51] epoch 3748, training loss: 12697.36, average training loss: 12928.46, base loss: 21674.04
[INFO 2017-06-28 19:26:50,679 main.py:51] epoch 3749, training loss: 13306.63, average training loss: 12929.13, base loss: 21674.93
[INFO 2017-06-28 19:26:52,028 main.py:51] epoch 3750, training loss: 12619.01, average training loss: 12928.36, base loss: 21673.58
[INFO 2017-06-28 19:26:53,342 main.py:51] epoch 3751, training loss: 13327.38, average training loss: 12928.70, base loss: 21674.46
[INFO 2017-06-28 19:26:54,605 main.py:51] epoch 3752, training loss: 13212.87, average training loss: 12929.12, base loss: 21676.58
[INFO 2017-06-28 19:26:55,898 main.py:51] epoch 3753, training loss: 13069.88, average training loss: 12929.25, base loss: 21676.82
[INFO 2017-06-28 19:26:57,225 main.py:51] epoch 3754, training loss: 13686.00, average training loss: 12929.89, base loss: 21675.30
[INFO 2017-06-28 19:26:58,618 main.py:51] epoch 3755, training loss: 13217.10, average training loss: 12929.74, base loss: 21676.65
[INFO 2017-06-28 19:26:59,912 main.py:51] epoch 3756, training loss: 14235.53, average training loss: 12931.83, base loss: 21680.41
[INFO 2017-06-28 19:27:01,214 main.py:51] epoch 3757, training loss: 13011.83, average training loss: 12933.11, base loss: 21683.88
[INFO 2017-06-28 19:27:02,526 main.py:51] epoch 3758, training loss: 12887.13, average training loss: 12933.74, base loss: 21683.29
[INFO 2017-06-28 19:27:03,864 main.py:51] epoch 3759, training loss: 11939.74, average training loss: 12931.24, base loss: 21680.16
[INFO 2017-06-28 19:27:05,059 main.py:51] epoch 3760, training loss: 12906.64, average training loss: 12932.53, base loss: 21680.51
[INFO 2017-06-28 19:27:06,401 main.py:51] epoch 3761, training loss: 12643.96, average training loss: 12932.22, base loss: 21679.72
[INFO 2017-06-28 19:27:07,808 main.py:51] epoch 3762, training loss: 12722.79, average training loss: 12932.38, base loss: 21681.03
[INFO 2017-06-28 19:27:09,113 main.py:51] epoch 3763, training loss: 12437.11, average training loss: 12930.54, base loss: 21677.99
[INFO 2017-06-28 19:27:10,457 main.py:51] epoch 3764, training loss: 12876.69, average training loss: 12930.51, base loss: 21677.57
[INFO 2017-06-28 19:27:11,744 main.py:51] epoch 3765, training loss: 12890.23, average training loss: 12931.24, base loss: 21679.62
[INFO 2017-06-28 19:27:12,987 main.py:51] epoch 3766, training loss: 13407.18, average training loss: 12932.28, base loss: 21680.00
[INFO 2017-06-28 19:27:14,355 main.py:51] epoch 3767, training loss: 13505.77, average training loss: 12932.57, base loss: 21681.93
[INFO 2017-06-28 19:27:15,663 main.py:51] epoch 3768, training loss: 12259.25, average training loss: 12930.93, base loss: 21679.81
[INFO 2017-06-28 19:27:16,889 main.py:51] epoch 3769, training loss: 13094.99, average training loss: 12929.67, base loss: 21678.84
[INFO 2017-06-28 19:27:18,174 main.py:51] epoch 3770, training loss: 12759.55, average training loss: 12930.16, base loss: 21680.56
[INFO 2017-06-28 19:27:19,520 main.py:51] epoch 3771, training loss: 13148.86, average training loss: 12929.40, base loss: 21679.68
[INFO 2017-06-28 19:27:20,924 main.py:51] epoch 3772, training loss: 14246.60, average training loss: 12928.42, base loss: 21678.82
[INFO 2017-06-28 19:27:22,384 main.py:51] epoch 3773, training loss: 13048.37, average training loss: 12928.06, base loss: 21678.02
[INFO 2017-06-28 19:27:23,645 main.py:51] epoch 3774, training loss: 13109.04, average training loss: 12928.85, base loss: 21680.84
[INFO 2017-06-28 19:27:24,979 main.py:51] epoch 3775, training loss: 12116.95, average training loss: 12929.99, base loss: 21681.70
[INFO 2017-06-28 19:27:26,329 main.py:51] epoch 3776, training loss: 13168.87, average training loss: 12930.42, base loss: 21682.12
[INFO 2017-06-28 19:27:27,712 main.py:51] epoch 3777, training loss: 13385.40, average training loss: 12930.13, base loss: 21683.08
[INFO 2017-06-28 19:27:29,009 main.py:51] epoch 3778, training loss: 11930.25, average training loss: 12930.18, base loss: 21684.11
[INFO 2017-06-28 19:27:30,217 main.py:51] epoch 3779, training loss: 13834.98, average training loss: 12929.54, base loss: 21683.38
[INFO 2017-06-28 19:27:31,566 main.py:51] epoch 3780, training loss: 13090.15, average training loss: 12930.15, base loss: 21687.79
[INFO 2017-06-28 19:27:32,899 main.py:51] epoch 3781, training loss: 13843.50, average training loss: 12930.52, base loss: 21687.72
[INFO 2017-06-28 19:27:34,306 main.py:51] epoch 3782, training loss: 12577.43, average training loss: 12930.25, base loss: 21690.43
[INFO 2017-06-28 19:27:35,569 main.py:51] epoch 3783, training loss: 11308.98, average training loss: 12929.60, base loss: 21688.35
[INFO 2017-06-28 19:27:36,796 main.py:51] epoch 3784, training loss: 14446.26, average training loss: 12930.33, base loss: 21690.24
[INFO 2017-06-28 19:27:38,181 main.py:51] epoch 3785, training loss: 12627.18, average training loss: 12929.05, base loss: 21690.17
[INFO 2017-06-28 19:27:39,382 main.py:51] epoch 3786, training loss: 12086.39, average training loss: 12927.85, base loss: 21688.84
[INFO 2017-06-28 19:27:40,843 main.py:51] epoch 3787, training loss: 11983.83, average training loss: 12927.40, base loss: 21689.96
[INFO 2017-06-28 19:27:42,204 main.py:51] epoch 3788, training loss: 12698.03, average training loss: 12927.05, base loss: 21690.45
[INFO 2017-06-28 19:27:43,499 main.py:51] epoch 3789, training loss: 13666.71, average training loss: 12927.49, base loss: 21694.08
[INFO 2017-06-28 19:27:44,846 main.py:51] epoch 3790, training loss: 13541.69, average training loss: 12926.80, base loss: 21691.91
[INFO 2017-06-28 19:27:46,253 main.py:51] epoch 3791, training loss: 11868.36, average training loss: 12926.57, base loss: 21692.29
[INFO 2017-06-28 19:27:47,538 main.py:51] epoch 3792, training loss: 12895.48, average training loss: 12926.22, base loss: 21691.95
[INFO 2017-06-28 19:27:48,789 main.py:51] epoch 3793, training loss: 12345.05, average training loss: 12925.11, base loss: 21686.81
[INFO 2017-06-28 19:27:50,173 main.py:51] epoch 3794, training loss: 12099.60, average training loss: 12923.29, base loss: 21683.95
[INFO 2017-06-28 19:27:51,483 main.py:51] epoch 3795, training loss: 12078.11, average training loss: 12923.36, base loss: 21683.78
[INFO 2017-06-28 19:27:52,765 main.py:51] epoch 3796, training loss: 13060.88, average training loss: 12922.61, base loss: 21681.82
[INFO 2017-06-28 19:27:54,082 main.py:51] epoch 3797, training loss: 12094.18, average training loss: 12921.82, base loss: 21679.89
[INFO 2017-06-28 19:27:55,391 main.py:51] epoch 3798, training loss: 13349.97, average training loss: 12922.94, base loss: 21681.54
[INFO 2017-06-28 19:27:56,630 main.py:51] epoch 3799, training loss: 12147.76, average training loss: 12921.36, base loss: 21681.13
[INFO 2017-06-28 19:27:56,630 main.py:53] epoch 3799, testing
[INFO 2017-06-28 19:28:02,671 main.py:105] average testing loss: 14604.14, base loss: 22332.31
[INFO 2017-06-28 19:28:02,671 main.py:106] improve_loss: 7728.17, improve_percent: 0.35
[INFO 2017-06-28 19:28:02,671 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:28:04,008 main.py:51] epoch 3800, training loss: 13564.18, average training loss: 12922.73, base loss: 21683.75
[INFO 2017-06-28 19:28:05,356 main.py:51] epoch 3801, training loss: 12599.57, average training loss: 12922.39, base loss: 21683.28
[INFO 2017-06-28 19:28:06,623 main.py:51] epoch 3802, training loss: 12378.32, average training loss: 12920.59, base loss: 21681.29
[INFO 2017-06-28 19:28:07,922 main.py:51] epoch 3803, training loss: 12267.86, average training loss: 12920.31, base loss: 21678.34
[INFO 2017-06-28 19:28:09,197 main.py:51] epoch 3804, training loss: 12366.19, average training loss: 12920.98, base loss: 21679.32
[INFO 2017-06-28 19:28:10,480 main.py:51] epoch 3805, training loss: 13570.76, average training loss: 12921.84, base loss: 21680.06
[INFO 2017-06-28 19:28:11,764 main.py:51] epoch 3806, training loss: 12415.38, average training loss: 12922.40, base loss: 21680.00
[INFO 2017-06-28 19:28:13,122 main.py:51] epoch 3807, training loss: 12939.47, average training loss: 12922.38, base loss: 21679.80
[INFO 2017-06-28 19:28:14,558 main.py:51] epoch 3808, training loss: 13039.02, average training loss: 12921.82, base loss: 21680.94
[INFO 2017-06-28 19:28:15,940 main.py:51] epoch 3809, training loss: 11920.04, average training loss: 12919.88, base loss: 21680.84
[INFO 2017-06-28 19:28:17,291 main.py:51] epoch 3810, training loss: 12843.01, average training loss: 12919.29, base loss: 21680.53
[INFO 2017-06-28 19:28:18,540 main.py:51] epoch 3811, training loss: 12450.22, average training loss: 12919.27, base loss: 21679.72
[INFO 2017-06-28 19:28:19,804 main.py:51] epoch 3812, training loss: 11408.54, average training loss: 12917.74, base loss: 21678.89
[INFO 2017-06-28 19:28:21,223 main.py:51] epoch 3813, training loss: 13506.08, average training loss: 12918.23, base loss: 21679.31
[INFO 2017-06-28 19:28:22,592 main.py:51] epoch 3814, training loss: 15117.70, average training loss: 12920.62, base loss: 21682.24
[INFO 2017-06-28 19:28:23,907 main.py:51] epoch 3815, training loss: 12731.79, average training loss: 12919.75, base loss: 21681.62
[INFO 2017-06-28 19:28:25,167 main.py:51] epoch 3816, training loss: 13305.81, average training loss: 12918.50, base loss: 21682.03
[INFO 2017-06-28 19:28:26,513 main.py:51] epoch 3817, training loss: 11604.03, average training loss: 12917.77, base loss: 21683.41
[INFO 2017-06-28 19:28:27,847 main.py:51] epoch 3818, training loss: 13019.30, average training loss: 12918.38, base loss: 21683.26
[INFO 2017-06-28 19:28:29,162 main.py:51] epoch 3819, training loss: 13044.04, average training loss: 12918.67, base loss: 21683.02
[INFO 2017-06-28 19:28:30,538 main.py:51] epoch 3820, training loss: 12081.75, average training loss: 12917.08, base loss: 21680.34
[INFO 2017-06-28 19:28:31,899 main.py:51] epoch 3821, training loss: 13020.34, average training loss: 12914.95, base loss: 21678.06
[INFO 2017-06-28 19:28:33,181 main.py:51] epoch 3822, training loss: 12820.48, average training loss: 12913.99, base loss: 21675.50
[INFO 2017-06-28 19:28:34,444 main.py:51] epoch 3823, training loss: 12947.62, average training loss: 12914.96, base loss: 21677.35
[INFO 2017-06-28 19:28:35,817 main.py:51] epoch 3824, training loss: 12392.38, average training loss: 12914.32, base loss: 21676.93
[INFO 2017-06-28 19:28:37,144 main.py:51] epoch 3825, training loss: 13052.49, average training loss: 12912.74, base loss: 21676.79
[INFO 2017-06-28 19:28:38,352 main.py:51] epoch 3826, training loss: 11983.67, average training loss: 12912.64, base loss: 21678.39
[INFO 2017-06-28 19:28:39,563 main.py:51] epoch 3827, training loss: 13042.84, average training loss: 12912.25, base loss: 21678.78
[INFO 2017-06-28 19:28:40,848 main.py:51] epoch 3828, training loss: 11846.44, average training loss: 12910.55, base loss: 21676.55
[INFO 2017-06-28 19:28:42,125 main.py:51] epoch 3829, training loss: 12775.19, average training loss: 12910.79, base loss: 21677.78
[INFO 2017-06-28 19:28:43,468 main.py:51] epoch 3830, training loss: 13402.76, average training loss: 12910.95, base loss: 21678.30
[INFO 2017-06-28 19:28:44,824 main.py:51] epoch 3831, training loss: 13764.99, average training loss: 12910.84, base loss: 21677.73
[INFO 2017-06-28 19:28:46,176 main.py:51] epoch 3832, training loss: 12646.53, average training loss: 12911.03, base loss: 21678.81
[INFO 2017-06-28 19:28:47,490 main.py:51] epoch 3833, training loss: 12606.28, average training loss: 12910.56, base loss: 21676.98
[INFO 2017-06-28 19:28:48,828 main.py:51] epoch 3834, training loss: 12171.96, average training loss: 12909.31, base loss: 21676.98
[INFO 2017-06-28 19:28:50,173 main.py:51] epoch 3835, training loss: 12335.54, average training loss: 12908.16, base loss: 21677.14
[INFO 2017-06-28 19:28:51,615 main.py:51] epoch 3836, training loss: 12179.92, average training loss: 12906.94, base loss: 21677.16
[INFO 2017-06-28 19:28:52,947 main.py:51] epoch 3837, training loss: 12429.38, average training loss: 12907.03, base loss: 21677.77
[INFO 2017-06-28 19:28:54,280 main.py:51] epoch 3838, training loss: 14171.98, average training loss: 12907.24, base loss: 21680.04
[INFO 2017-06-28 19:28:55,568 main.py:51] epoch 3839, training loss: 12699.99, average training loss: 12907.18, base loss: 21678.88
[INFO 2017-06-28 19:28:56,763 main.py:51] epoch 3840, training loss: 12428.29, average training loss: 12907.55, base loss: 21679.25
[INFO 2017-06-28 19:28:58,123 main.py:51] epoch 3841, training loss: 12897.85, average training loss: 12907.79, base loss: 21679.52
[INFO 2017-06-28 19:28:59,450 main.py:51] epoch 3842, training loss: 13253.19, average training loss: 12908.52, base loss: 21681.11
[INFO 2017-06-28 19:29:00,772 main.py:51] epoch 3843, training loss: 13271.41, average training loss: 12906.87, base loss: 21680.60
[INFO 2017-06-28 19:29:02,198 main.py:51] epoch 3844, training loss: 12593.15, average training loss: 12906.66, base loss: 21679.99
[INFO 2017-06-28 19:29:03,466 main.py:51] epoch 3845, training loss: 11897.21, average training loss: 12905.88, base loss: 21680.38
[INFO 2017-06-28 19:29:04,776 main.py:51] epoch 3846, training loss: 11965.38, average training loss: 12904.32, base loss: 21680.04
[INFO 2017-06-28 19:29:06,160 main.py:51] epoch 3847, training loss: 12062.72, average training loss: 12902.43, base loss: 21676.09
[INFO 2017-06-28 19:29:07,468 main.py:51] epoch 3848, training loss: 13249.49, average training loss: 12903.26, base loss: 21679.14
[INFO 2017-06-28 19:29:08,849 main.py:51] epoch 3849, training loss: 12750.69, average training loss: 12901.17, base loss: 21675.19
[INFO 2017-06-28 19:29:10,256 main.py:51] epoch 3850, training loss: 13577.12, average training loss: 12902.65, base loss: 21678.41
[INFO 2017-06-28 19:29:11,579 main.py:51] epoch 3851, training loss: 14415.06, average training loss: 12903.19, base loss: 21683.31
[INFO 2017-06-28 19:29:12,842 main.py:51] epoch 3852, training loss: 12358.83, average training loss: 12902.70, base loss: 21681.11
[INFO 2017-06-28 19:29:14,100 main.py:51] epoch 3853, training loss: 11785.88, average training loss: 12901.80, base loss: 21678.34
[INFO 2017-06-28 19:29:15,533 main.py:51] epoch 3854, training loss: 11384.47, average training loss: 12899.10, base loss: 21674.22
[INFO 2017-06-28 19:29:16,826 main.py:51] epoch 3855, training loss: 12891.87, average training loss: 12899.76, base loss: 21673.83
[INFO 2017-06-28 19:29:18,078 main.py:51] epoch 3856, training loss: 12954.45, average training loss: 12900.00, base loss: 21676.04
[INFO 2017-06-28 19:29:19,460 main.py:51] epoch 3857, training loss: 13287.02, average training loss: 12900.29, base loss: 21676.38
[INFO 2017-06-28 19:29:20,843 main.py:51] epoch 3858, training loss: 12398.13, average training loss: 12898.98, base loss: 21674.40
[INFO 2017-06-28 19:29:22,156 main.py:51] epoch 3859, training loss: 12645.07, average training loss: 12899.38, base loss: 21673.26
[INFO 2017-06-28 19:29:23,416 main.py:51] epoch 3860, training loss: 13721.86, average training loss: 12899.71, base loss: 21673.55
[INFO 2017-06-28 19:29:24,747 main.py:51] epoch 3861, training loss: 12787.53, average training loss: 12899.60, base loss: 21673.08
[INFO 2017-06-28 19:29:26,166 main.py:51] epoch 3862, training loss: 14558.94, average training loss: 12901.47, base loss: 21676.46
[INFO 2017-06-28 19:29:27,493 main.py:51] epoch 3863, training loss: 13978.58, average training loss: 12903.78, base loss: 21680.08
[INFO 2017-06-28 19:29:28,907 main.py:51] epoch 3864, training loss: 14127.06, average training loss: 12905.51, base loss: 21681.75
[INFO 2017-06-28 19:29:30,276 main.py:51] epoch 3865, training loss: 12056.07, average training loss: 12904.51, base loss: 21681.97
[INFO 2017-06-28 19:29:31,606 main.py:51] epoch 3866, training loss: 11636.32, average training loss: 12903.61, base loss: 21681.03
[INFO 2017-06-28 19:29:32,920 main.py:51] epoch 3867, training loss: 13940.73, average training loss: 12904.38, base loss: 21683.88
[INFO 2017-06-28 19:29:34,236 main.py:51] epoch 3868, training loss: 12827.09, average training loss: 12904.90, base loss: 21684.59
[INFO 2017-06-28 19:29:35,576 main.py:51] epoch 3869, training loss: 14990.50, average training loss: 12907.24, base loss: 21686.39
[INFO 2017-06-28 19:29:36,854 main.py:51] epoch 3870, training loss: 11647.02, average training loss: 12906.11, base loss: 21685.48
[INFO 2017-06-28 19:29:38,289 main.py:51] epoch 3871, training loss: 12307.25, average training loss: 12904.95, base loss: 21683.84
[INFO 2017-06-28 19:29:39,632 main.py:51] epoch 3872, training loss: 12023.91, average training loss: 12903.24, base loss: 21683.46
[INFO 2017-06-28 19:29:40,885 main.py:51] epoch 3873, training loss: 12037.26, average training loss: 12901.76, base loss: 21679.91
[INFO 2017-06-28 19:29:42,213 main.py:51] epoch 3874, training loss: 12538.86, average training loss: 12901.05, base loss: 21678.67
[INFO 2017-06-28 19:29:43,513 main.py:51] epoch 3875, training loss: 11909.58, average training loss: 12900.00, base loss: 21677.83
[INFO 2017-06-28 19:29:44,714 main.py:51] epoch 3876, training loss: 12390.62, average training loss: 12900.08, base loss: 21678.30
[INFO 2017-06-28 19:29:46,029 main.py:51] epoch 3877, training loss: 11939.93, average training loss: 12900.03, base loss: 21677.48
[INFO 2017-06-28 19:29:47,387 main.py:51] epoch 3878, training loss: 13860.46, average training loss: 12901.76, base loss: 21682.13
[INFO 2017-06-28 19:29:48,710 main.py:51] epoch 3879, training loss: 12513.17, average training loss: 12902.89, base loss: 21682.79
[INFO 2017-06-28 19:29:50,071 main.py:51] epoch 3880, training loss: 12004.33, average training loss: 12901.66, base loss: 21682.60
[INFO 2017-06-28 19:29:51,327 main.py:51] epoch 3881, training loss: 11293.05, average training loss: 12899.43, base loss: 21681.14
[INFO 2017-06-28 19:29:52,691 main.py:51] epoch 3882, training loss: 12432.50, average training loss: 12898.38, base loss: 21680.49
[INFO 2017-06-28 19:29:53,966 main.py:51] epoch 3883, training loss: 11774.40, average training loss: 12897.33, base loss: 21676.13
[INFO 2017-06-28 19:29:55,178 main.py:51] epoch 3884, training loss: 13280.71, average training loss: 12897.66, base loss: 21676.73
[INFO 2017-06-28 19:29:56,471 main.py:51] epoch 3885, training loss: 12979.29, average training loss: 12897.76, base loss: 21676.12
[INFO 2017-06-28 19:29:57,738 main.py:51] epoch 3886, training loss: 13224.90, average training loss: 12898.07, base loss: 21676.89
[INFO 2017-06-28 19:29:59,029 main.py:51] epoch 3887, training loss: 13267.06, average training loss: 12897.43, base loss: 21678.58
[INFO 2017-06-28 19:30:00,319 main.py:51] epoch 3888, training loss: 12461.41, average training loss: 12896.96, base loss: 21679.22
[INFO 2017-06-28 19:30:01,661 main.py:51] epoch 3889, training loss: 12212.27, average training loss: 12895.88, base loss: 21681.43
[INFO 2017-06-28 19:30:02,993 main.py:51] epoch 3890, training loss: 13689.76, average training loss: 12895.07, base loss: 21681.13
[INFO 2017-06-28 19:30:04,301 main.py:51] epoch 3891, training loss: 12798.34, average training loss: 12895.71, base loss: 21683.27
[INFO 2017-06-28 19:30:05,615 main.py:51] epoch 3892, training loss: 11926.81, average training loss: 12895.36, base loss: 21684.17
[INFO 2017-06-28 19:30:06,851 main.py:51] epoch 3893, training loss: 12202.11, average training loss: 12894.78, base loss: 21681.18
[INFO 2017-06-28 19:30:08,154 main.py:51] epoch 3894, training loss: 11676.27, average training loss: 12894.28, base loss: 21681.03
[INFO 2017-06-28 19:30:09,442 main.py:51] epoch 3895, training loss: 11418.61, average training loss: 12892.78, base loss: 21677.90
[INFO 2017-06-28 19:30:10,648 main.py:51] epoch 3896, training loss: 11979.75, average training loss: 12891.59, base loss: 21677.63
[INFO 2017-06-28 19:30:11,975 main.py:51] epoch 3897, training loss: 11936.71, average training loss: 12890.51, base loss: 21678.15
[INFO 2017-06-28 19:30:13,254 main.py:51] epoch 3898, training loss: 13168.27, average training loss: 12889.54, base loss: 21679.45
[INFO 2017-06-28 19:30:14,581 main.py:51] epoch 3899, training loss: 13138.80, average training loss: 12889.74, base loss: 21679.49
[INFO 2017-06-28 19:30:14,581 main.py:53] epoch 3899, testing
[INFO 2017-06-28 19:30:20,643 main.py:105] average testing loss: 13995.53, base loss: 22033.45
[INFO 2017-06-28 19:30:20,643 main.py:106] improve_loss: 8037.92, improve_percent: 0.36
[INFO 2017-06-28 19:30:20,643 main.py:76] current best improved percent: 0.37
[INFO 2017-06-28 19:30:21,943 main.py:51] epoch 3900, training loss: 14125.13, average training loss: 12890.01, base loss: 21681.62
[INFO 2017-06-28 19:30:23,175 main.py:51] epoch 3901, training loss: 12212.87, average training loss: 12888.44, base loss: 21679.67
[INFO 2017-06-28 19:30:24,373 main.py:51] epoch 3902, training loss: 12415.30, average training loss: 12887.75, base loss: 21677.22
[INFO 2017-06-28 19:30:25,757 main.py:51] epoch 3903, training loss: 12886.84, average training loss: 12886.85, base loss: 21673.95
[INFO 2017-06-28 19:30:27,084 main.py:51] epoch 3904, training loss: 12762.96, average training loss: 12888.12, base loss: 21676.45
[INFO 2017-06-28 19:30:28,290 main.py:51] epoch 3905, training loss: 12623.95, average training loss: 12888.40, base loss: 21676.51
[INFO 2017-06-28 19:30:29,662 main.py:51] epoch 3906, training loss: 11723.78, average training loss: 12886.93, base loss: 21674.33
[INFO 2017-06-28 19:30:30,850 main.py:51] epoch 3907, training loss: 11943.49, average training loss: 12886.36, base loss: 21673.67
[INFO 2017-06-28 19:30:32,089 main.py:51] epoch 3908, training loss: 11814.33, average training loss: 12884.78, base loss: 21672.59
[INFO 2017-06-28 19:30:33,335 main.py:51] epoch 3909, training loss: 12292.21, average training loss: 12884.18, base loss: 21672.30
[INFO 2017-06-28 19:30:34,753 main.py:51] epoch 3910, training loss: 12791.08, average training loss: 12882.13, base loss: 21668.60
[INFO 2017-06-28 19:30:36,019 main.py:51] epoch 3911, training loss: 12085.15, average training loss: 12881.82, base loss: 21667.94
[INFO 2017-06-28 19:30:37,464 main.py:51] epoch 3912, training loss: 13692.54, average training loss: 12882.32, base loss: 21665.81
[INFO 2017-06-28 19:30:38,740 main.py:51] epoch 3913, training loss: 13373.77, average training loss: 12883.98, base loss: 21665.82
[INFO 2017-06-28 19:30:40,130 main.py:51] epoch 3914, training loss: 11840.93, average training loss: 12881.54, base loss: 21663.86
[INFO 2017-06-28 19:30:41,564 main.py:51] epoch 3915, training loss: 11740.64, average training loss: 12880.16, base loss: 21662.01
[INFO 2017-06-28 19:30:42,974 main.py:51] epoch 3916, training loss: 11671.00, average training loss: 12878.63, base loss: 21661.21
[INFO 2017-06-28 19:30:44,289 main.py:51] epoch 3917, training loss: 12982.71, average training loss: 12880.18, base loss: 21663.20
[INFO 2017-06-28 19:30:45,562 main.py:51] epoch 3918, training loss: 13567.26, average training loss: 12882.15, base loss: 21665.05
[INFO 2017-06-28 19:30:46,870 main.py:51] epoch 3919, training loss: 12576.94, average training loss: 12880.95, base loss: 21665.53
[INFO 2017-06-28 19:30:48,158 main.py:51] epoch 3920, training loss: 12699.14, average training loss: 12881.25, base loss: 21668.16
[INFO 2017-06-28 19:30:49,560 main.py:51] epoch 3921, training loss: 11851.14, average training loss: 12880.41, base loss: 21667.98
[INFO 2017-06-28 19:30:50,852 main.py:51] epoch 3922, training loss: 11676.99, average training loss: 12879.50, base loss: 21666.68
[INFO 2017-06-28 19:30:52,175 main.py:51] epoch 3923, training loss: 13388.36, average training loss: 12877.00, base loss: 21663.12
[INFO 2017-06-28 19:30:53,477 main.py:51] epoch 3924, training loss: 12607.90, average training loss: 12877.94, base loss: 21664.36
[INFO 2017-06-28 19:30:54,867 main.py:51] epoch 3925, training loss: 12768.22, average training loss: 12877.39, base loss: 21663.57
[INFO 2017-06-28 19:30:56,176 main.py:51] epoch 3926, training loss: 11987.72, average training loss: 12875.30, base loss: 21660.32
[INFO 2017-06-28 19:30:57,506 main.py:51] epoch 3927, training loss: 13689.68, average training loss: 12874.59, base loss: 21660.15
[INFO 2017-06-28 19:30:58,797 main.py:51] epoch 3928, training loss: 11619.06, average training loss: 12873.42, base loss: 21659.69
[INFO 2017-06-28 19:31:00,062 main.py:51] epoch 3929, training loss: 13473.59, average training loss: 12873.01, base loss: 21658.98
[INFO 2017-06-28 19:31:01,419 main.py:51] epoch 3930, training loss: 12936.77, average training loss: 12872.32, base loss: 21655.59
[INFO 2017-06-28 19:31:02,699 main.py:51] epoch 3931, training loss: 11661.04, average training loss: 12870.60, base loss: 21656.27
[INFO 2017-06-28 19:31:03,976 main.py:51] epoch 3932, training loss: 13276.20, average training loss: 12869.65, base loss: 21654.56
[INFO 2017-06-28 19:31:05,293 main.py:51] epoch 3933, training loss: 12822.44, average training loss: 12869.38, base loss: 21654.04
[INFO 2017-06-28 19:31:06,608 main.py:51] epoch 3934, training loss: 12883.25, average training loss: 12868.27, base loss: 21651.66
[INFO 2017-06-28 19:31:07,898 main.py:51] epoch 3935, training loss: 13127.23, average training loss: 12869.07, base loss: 21655.44
[INFO 2017-06-28 19:31:09,169 main.py:51] epoch 3936, training loss: 12990.16, average training loss: 12869.12, base loss: 21655.74
[INFO 2017-06-28 19:31:10,451 main.py:51] epoch 3937, training loss: 11620.34, average training loss: 12865.34, base loss: 21650.93
[INFO 2017-06-28 19:31:11,812 main.py:51] epoch 3938, training loss: 11548.85, average training loss: 12863.49, base loss: 21650.22
[INFO 2017-06-28 19:31:13,267 main.py:51] epoch 3939, training loss: 13396.97, average training loss: 12863.67, base loss: 21650.80
[INFO 2017-06-28 19:31:14,563 main.py:51] epoch 3940, training loss: 11944.94, average training loss: 12862.71, base loss: 21649.64
[INFO 2017-06-28 19:31:16,003 main.py:51] epoch 3941, training loss: 12034.87, average training loss: 12861.49, base loss: 21646.73
[INFO 2017-06-28 19:31:17,292 main.py:51] epoch 3942, training loss: 13940.88, average training loss: 12862.20, base loss: 21648.51
[INFO 2017-06-28 19:31:18,594 main.py:51] epoch 3943, training loss: 12225.03, average training loss: 12862.88, base loss: 21648.85
[INFO 2017-06-28 19:31:19,800 main.py:51] epoch 3944, training loss: 13149.98, average training loss: 12863.85, base loss: 21650.30
[INFO 2017-06-28 19:31:21,027 main.py:51] epoch 3945, training loss: 12778.42, average training loss: 12862.28, base loss: 21650.50
[INFO 2017-06-28 19:31:22,254 main.py:51] epoch 3946, training loss: 14265.04, average training loss: 12864.37, base loss: 21653.63
[INFO 2017-06-28 19:31:23,504 main.py:51] epoch 3947, training loss: 13751.15, average training loss: 12864.37, base loss: 21653.93
[INFO 2017-06-28 19:31:24,879 main.py:51] epoch 3948, training loss: 13033.32, average training loss: 12863.64, base loss: 21653.73
[INFO 2017-06-28 19:31:26,168 main.py:51] epoch 3949, training loss: 12206.29, average training loss: 12862.32, base loss: 21653.28
[INFO 2017-06-28 19:31:27,520 main.py:51] epoch 3950, training loss: 12485.92, average training loss: 12861.60, base loss: 21654.87
[INFO 2017-06-28 19:31:28,804 main.py:51] epoch 3951, training loss: 11792.88, average training loss: 12861.69, base loss: 21655.25
[INFO 2017-06-28 19:31:30,101 main.py:51] epoch 3952, training loss: 13230.95, average training loss: 12861.63, base loss: 21654.92
[INFO 2017-06-28 19:31:31,385 main.py:51] epoch 3953, training loss: 12151.52, average training loss: 12861.92, base loss: 21653.30
[INFO 2017-06-28 19:31:32,720 main.py:51] epoch 3954, training loss: 13813.27, average training loss: 12864.49, base loss: 21655.68
[INFO 2017-06-28 19:31:34,089 main.py:51] epoch 3955, training loss: 11978.11, average training loss: 12863.46, base loss: 21655.23
[INFO 2017-06-28 19:31:35,371 main.py:51] epoch 3956, training loss: 12846.05, average training loss: 12864.00, base loss: 21655.76
[INFO 2017-06-28 19:31:36,659 main.py:51] epoch 3957, training loss: 12480.67, average training loss: 12864.85, base loss: 21656.33
[INFO 2017-06-28 19:31:37,957 main.py:51] epoch 3958, training loss: 12584.21, average training loss: 12865.86, base loss: 21657.51
[INFO 2017-06-28 19:31:39,308 main.py:51] epoch 3959, training loss: 12154.01, average training loss: 12866.51, base loss: 21659.00
[INFO 2017-06-28 19:31:40,566 main.py:51] epoch 3960, training loss: 11960.22, average training loss: 12866.14, base loss: 21657.46
[INFO 2017-06-28 19:31:41,883 main.py:51] epoch 3961, training loss: 12689.91, average training loss: 12866.55, base loss: 21660.44
[INFO 2017-06-28 19:31:43,153 main.py:51] epoch 3962, training loss: 11524.78, average training loss: 12865.25, base loss: 21658.96
[INFO 2017-06-28 19:31:44,577 main.py:51] epoch 3963, training loss: 12502.38, average training loss: 12864.26, base loss: 21659.00
[INFO 2017-06-28 19:31:45,875 main.py:51] epoch 3964, training loss: 12964.77, average training loss: 12862.59, base loss: 21654.84
[INFO 2017-06-28 19:31:47,262 main.py:51] epoch 3965, training loss: 11659.39, average training loss: 12861.44, base loss: 21652.98
[INFO 2017-06-28 19:31:48,529 main.py:51] epoch 3966, training loss: 13531.63, average training loss: 12862.19, base loss: 21653.45
[INFO 2017-06-28 19:31:49,760 main.py:51] epoch 3967, training loss: 13172.06, average training loss: 12862.82, base loss: 21653.96
[INFO 2017-06-28 19:31:51,071 main.py:51] epoch 3968, training loss: 12867.18, average training loss: 12862.65, base loss: 21656.83
[INFO 2017-06-28 19:31:52,412 main.py:51] epoch 3969, training loss: 13185.82, average training loss: 12863.88, base loss: 21655.77
[INFO 2017-06-28 19:31:53,716 main.py:51] epoch 3970, training loss: 13446.34, average training loss: 12865.14, base loss: 21657.93
[INFO 2017-06-28 19:31:55,030 main.py:51] epoch 3971, training loss: 12714.70, average training loss: 12865.14, base loss: 21657.82
[INFO 2017-06-28 19:31:56,349 main.py:51] epoch 3972, training loss: 11940.67, average training loss: 12863.35, base loss: 21654.69
[INFO 2017-06-28 19:31:57,564 main.py:51] epoch 3973, training loss: 13131.11, average training loss: 12863.24, base loss: 21652.72
[INFO 2017-06-28 19:31:58,853 main.py:51] epoch 3974, training loss: 13345.84, average training loss: 12863.72, base loss: 21655.24
[INFO 2017-06-28 19:32:00,205 main.py:51] epoch 3975, training loss: 12470.84, average training loss: 12864.33, base loss: 21655.63
[INFO 2017-06-28 19:32:01,546 main.py:51] epoch 3976, training loss: 12473.50, average training loss: 12864.72, base loss: 21654.52
[INFO 2017-06-28 19:32:02,914 main.py:51] epoch 3977, training loss: 13718.38, average training loss: 12866.01, base loss: 21656.48
[INFO 2017-06-28 19:32:04,194 main.py:51] epoch 3978, training loss: 11650.57, average training loss: 12864.11, base loss: 21653.85
[INFO 2017-06-28 19:32:05,507 main.py:51] epoch 3979, training loss: 12658.69, average training loss: 12863.43, base loss: 21653.50
[INFO 2017-06-28 19:32:06,948 main.py:51] epoch 3980, training loss: 12351.32, average training loss: 12863.04, base loss: 21652.73
[INFO 2017-06-28 19:32:08,214 main.py:51] epoch 3981, training loss: 12134.01, average training loss: 12860.87, base loss: 21652.08
[INFO 2017-06-28 19:32:09,531 main.py:51] epoch 3982, training loss: 11847.74, average training loss: 12860.80, base loss: 21652.16
[INFO 2017-06-28 19:32:10,877 main.py:51] epoch 3983, training loss: 12474.57, average training loss: 12858.58, base loss: 21648.76
[INFO 2017-06-28 19:32:12,128 main.py:51] epoch 3984, training loss: 11716.68, average training loss: 12857.89, base loss: 21649.56
[INFO 2017-06-28 19:32:13,436 main.py:51] epoch 3985, training loss: 12485.68, average training loss: 12857.83, base loss: 21650.26
[INFO 2017-06-28 19:32:14,705 main.py:51] epoch 3986, training loss: 11398.35, average training loss: 12857.91, base loss: 21648.44
[INFO 2017-06-28 19:32:16,041 main.py:51] epoch 3987, training loss: 12084.15, average training loss: 12858.83, base loss: 21651.18
[INFO 2017-06-28 19:32:17,413 main.py:51] epoch 3988, training loss: 12293.19, average training loss: 12858.24, base loss: 21651.08
[INFO 2017-06-28 19:32:18,796 main.py:51] epoch 3989, training loss: 12142.22, average training loss: 12857.88, base loss: 21650.98
[INFO 2017-06-28 19:32:20,213 main.py:51] epoch 3990, training loss: 12001.03, average training loss: 12856.11, base loss: 21649.61
[INFO 2017-06-28 19:32:21,498 main.py:51] epoch 3991, training loss: 13662.46, average training loss: 12857.71, base loss: 21652.32
[INFO 2017-06-28 19:32:22,855 main.py:51] epoch 3992, training loss: 12856.67, average training loss: 12857.55, base loss: 21651.90
[INFO 2017-06-28 19:32:24,159 main.py:51] epoch 3993, training loss: 12970.96, average training loss: 12857.58, base loss: 21652.74
[INFO 2017-06-28 19:32:25,384 main.py:51] epoch 3994, training loss: 12272.08, average training loss: 12856.97, base loss: 21653.95
[INFO 2017-06-28 19:32:26,685 main.py:51] epoch 3995, training loss: 14390.58, average training loss: 12859.67, base loss: 21659.70
[INFO 2017-06-28 19:32:28,114 main.py:51] epoch 3996, training loss: 13760.13, average training loss: 12860.09, base loss: 21661.27
[INFO 2017-06-28 19:32:29,411 main.py:51] epoch 3997, training loss: 13860.89, average training loss: 12861.29, base loss: 21662.30
[INFO 2017-06-28 19:32:30,751 main.py:51] epoch 3998, training loss: 12582.30, average training loss: 12860.65, base loss: 21660.11
[INFO 2017-06-28 19:32:32,039 main.py:51] epoch 3999, training loss: 12651.36, average training loss: 12860.67, base loss: 21659.87
[INFO 2017-06-28 19:32:32,039 main.py:53] epoch 3999, testing
[INFO 2017-06-28 19:32:38,071 main.py:105] average testing loss: 13169.47, base loss: 21296.58
[INFO 2017-06-28 19:32:38,071 main.py:106] improve_loss: 8127.11, improve_percent: 0.38
[INFO 2017-06-28 19:32:38,072 main.py:72] model save to ./model/final.pth
[INFO 2017-06-28 19:32:38,084 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:32:39,343 main.py:51] epoch 4000, training loss: 11811.20, average training loss: 12857.88, base loss: 21657.78
[INFO 2017-06-28 19:32:40,668 main.py:51] epoch 4001, training loss: 12338.93, average training loss: 12857.07, base loss: 21655.94
[INFO 2017-06-28 19:32:41,963 main.py:51] epoch 4002, training loss: 12969.87, average training loss: 12856.82, base loss: 21654.39
[INFO 2017-06-28 19:32:43,285 main.py:51] epoch 4003, training loss: 13345.12, average training loss: 12857.57, base loss: 21655.09
[INFO 2017-06-28 19:32:44,650 main.py:51] epoch 4004, training loss: 13834.22, average training loss: 12858.13, base loss: 21655.23
[INFO 2017-06-28 19:32:46,009 main.py:51] epoch 4005, training loss: 11150.69, average training loss: 12854.94, base loss: 21652.09
[INFO 2017-06-28 19:32:47,340 main.py:51] epoch 4006, training loss: 13220.08, average training loss: 12856.68, base loss: 21651.87
[INFO 2017-06-28 19:32:48,615 main.py:51] epoch 4007, training loss: 13031.05, average training loss: 12856.76, base loss: 21651.84
[INFO 2017-06-28 19:32:50,032 main.py:51] epoch 4008, training loss: 11425.61, average training loss: 12856.16, base loss: 21653.31
[INFO 2017-06-28 19:32:51,348 main.py:51] epoch 4009, training loss: 12561.95, average training loss: 12856.47, base loss: 21654.56
[INFO 2017-06-28 19:32:52,678 main.py:51] epoch 4010, training loss: 12508.09, average training loss: 12856.51, base loss: 21653.09
[INFO 2017-06-28 19:32:53,977 main.py:51] epoch 4011, training loss: 12041.06, average training loss: 12856.53, base loss: 21653.03
[INFO 2017-06-28 19:32:55,247 main.py:51] epoch 4012, training loss: 11977.93, average training loss: 12856.51, base loss: 21653.82
[INFO 2017-06-28 19:32:56,620 main.py:51] epoch 4013, training loss: 12803.25, average training loss: 12855.37, base loss: 21653.44
[INFO 2017-06-28 19:32:57,829 main.py:51] epoch 4014, training loss: 12746.42, average training loss: 12853.77, base loss: 21649.73
[INFO 2017-06-28 19:32:59,192 main.py:51] epoch 4015, training loss: 12958.00, average training loss: 12853.60, base loss: 21646.51
[INFO 2017-06-28 19:33:00,487 main.py:51] epoch 4016, training loss: 12838.87, average training loss: 12854.05, base loss: 21645.61
[INFO 2017-06-28 19:33:01,844 main.py:51] epoch 4017, training loss: 12951.79, average training loss: 12854.27, base loss: 21646.19
[INFO 2017-06-28 19:33:03,218 main.py:51] epoch 4018, training loss: 13187.72, average training loss: 12854.58, base loss: 21646.78
[INFO 2017-06-28 19:33:04,523 main.py:51] epoch 4019, training loss: 12280.11, average training loss: 12854.73, base loss: 21647.56
[INFO 2017-06-28 19:33:05,840 main.py:51] epoch 4020, training loss: 14168.63, average training loss: 12855.36, base loss: 21649.54
[INFO 2017-06-28 19:33:07,181 main.py:51] epoch 4021, training loss: 13230.79, average training loss: 12856.70, base loss: 21653.64
[INFO 2017-06-28 19:33:08,500 main.py:51] epoch 4022, training loss: 11903.87, average training loss: 12855.69, base loss: 21653.25
[INFO 2017-06-28 19:33:09,844 main.py:51] epoch 4023, training loss: 12468.29, average training loss: 12853.53, base loss: 21653.91
[INFO 2017-06-28 19:33:11,165 main.py:51] epoch 4024, training loss: 12038.19, average training loss: 12852.24, base loss: 21655.03
[INFO 2017-06-28 19:33:12,492 main.py:51] epoch 4025, training loss: 13901.22, average training loss: 12852.91, base loss: 21656.35
[INFO 2017-06-28 19:33:13,808 main.py:51] epoch 4026, training loss: 12222.17, average training loss: 12852.39, base loss: 21654.70
[INFO 2017-06-28 19:33:15,153 main.py:51] epoch 4027, training loss: 13503.07, average training loss: 12851.85, base loss: 21655.37
[INFO 2017-06-28 19:33:16,510 main.py:51] epoch 4028, training loss: 12255.79, average training loss: 12851.24, base loss: 21655.83
[INFO 2017-06-28 19:33:17,886 main.py:51] epoch 4029, training loss: 12370.32, average training loss: 12851.27, base loss: 21654.55
[INFO 2017-06-28 19:33:19,200 main.py:51] epoch 4030, training loss: 12169.96, average training loss: 12849.64, base loss: 21653.35
[INFO 2017-06-28 19:33:20,508 main.py:51] epoch 4031, training loss: 12543.84, average training loss: 12849.89, base loss: 21653.43
[INFO 2017-06-28 19:33:21,876 main.py:51] epoch 4032, training loss: 13343.35, average training loss: 12848.66, base loss: 21653.65
[INFO 2017-06-28 19:33:23,101 main.py:51] epoch 4033, training loss: 12150.54, average training loss: 12848.35, base loss: 21652.52
[INFO 2017-06-28 19:33:24,479 main.py:51] epoch 4034, training loss: 12692.97, average training loss: 12849.52, base loss: 21653.59
[INFO 2017-06-28 19:33:25,870 main.py:51] epoch 4035, training loss: 11871.30, average training loss: 12848.75, base loss: 21654.31
[INFO 2017-06-28 19:33:27,183 main.py:51] epoch 4036, training loss: 12801.09, average training loss: 12847.09, base loss: 21650.95
[INFO 2017-06-28 19:33:28,568 main.py:51] epoch 4037, training loss: 12451.59, average training loss: 12846.08, base loss: 21650.50
[INFO 2017-06-28 19:33:29,840 main.py:51] epoch 4038, training loss: 11489.71, average training loss: 12844.53, base loss: 21647.26
[INFO 2017-06-28 19:33:31,194 main.py:51] epoch 4039, training loss: 13150.18, average training loss: 12842.63, base loss: 21647.86
[INFO 2017-06-28 19:33:32,497 main.py:51] epoch 4040, training loss: 12647.47, average training loss: 12841.91, base loss: 21647.23
[INFO 2017-06-28 19:33:33,890 main.py:51] epoch 4041, training loss: 12260.93, average training loss: 12841.43, base loss: 21647.92
[INFO 2017-06-28 19:33:35,324 main.py:51] epoch 4042, training loss: 12900.15, average training loss: 12842.58, base loss: 21648.93
[INFO 2017-06-28 19:33:36,676 main.py:51] epoch 4043, training loss: 13107.26, average training loss: 12842.87, base loss: 21649.94
[INFO 2017-06-28 19:33:38,128 main.py:51] epoch 4044, training loss: 12588.25, average training loss: 12842.52, base loss: 21648.09
[INFO 2017-06-28 19:33:39,440 main.py:51] epoch 4045, training loss: 11359.42, average training loss: 12840.55, base loss: 21645.71
[INFO 2017-06-28 19:33:40,722 main.py:51] epoch 4046, training loss: 13206.23, average training loss: 12842.05, base loss: 21647.59
[INFO 2017-06-28 19:33:41,981 main.py:51] epoch 4047, training loss: 14203.30, average training loss: 12844.33, base loss: 21650.37
[INFO 2017-06-28 19:33:43,436 main.py:51] epoch 4048, training loss: 12167.51, average training loss: 12842.18, base loss: 21647.69
[INFO 2017-06-28 19:33:44,713 main.py:51] epoch 4049, training loss: 12541.81, average training loss: 12842.74, base loss: 21648.89
[INFO 2017-06-28 19:33:46,087 main.py:51] epoch 4050, training loss: 14232.95, average training loss: 12844.50, base loss: 21650.90
[INFO 2017-06-28 19:33:47,445 main.py:51] epoch 4051, training loss: 12445.02, average training loss: 12844.34, base loss: 21649.12
[INFO 2017-06-28 19:33:48,784 main.py:51] epoch 4052, training loss: 12594.27, average training loss: 12843.84, base loss: 21648.86
[INFO 2017-06-28 19:33:50,212 main.py:51] epoch 4053, training loss: 12545.17, average training loss: 12842.32, base loss: 21646.32
[INFO 2017-06-28 19:33:51,550 main.py:51] epoch 4054, training loss: 13604.67, average training loss: 12843.53, base loss: 21647.94
[INFO 2017-06-28 19:33:52,941 main.py:51] epoch 4055, training loss: 13046.32, average training loss: 12843.89, base loss: 21649.71
[INFO 2017-06-28 19:33:54,178 main.py:51] epoch 4056, training loss: 13119.25, average training loss: 12844.79, base loss: 21652.49
[INFO 2017-06-28 19:33:55,418 main.py:51] epoch 4057, training loss: 12606.24, average training loss: 12844.69, base loss: 21652.06
[INFO 2017-06-28 19:33:56,684 main.py:51] epoch 4058, training loss: 13419.14, average training loss: 12846.81, base loss: 21654.34
[INFO 2017-06-28 19:33:57,975 main.py:51] epoch 4059, training loss: 11893.03, average training loss: 12846.94, base loss: 21654.08
[INFO 2017-06-28 19:33:59,330 main.py:51] epoch 4060, training loss: 12385.15, average training loss: 12846.12, base loss: 21654.33
[INFO 2017-06-28 19:34:00,692 main.py:51] epoch 4061, training loss: 12757.15, average training loss: 12845.01, base loss: 21652.79
[INFO 2017-06-28 19:34:02,141 main.py:51] epoch 4062, training loss: 12552.02, average training loss: 12844.46, base loss: 21649.52
[INFO 2017-06-28 19:34:03,517 main.py:51] epoch 4063, training loss: 11498.98, average training loss: 12842.24, base loss: 21645.69
[INFO 2017-06-28 19:34:04,733 main.py:51] epoch 4064, training loss: 13017.81, average training loss: 12841.70, base loss: 21643.87
[INFO 2017-06-28 19:34:06,068 main.py:51] epoch 4065, training loss: 12945.80, average training loss: 12843.12, base loss: 21645.14
[INFO 2017-06-28 19:34:07,300 main.py:51] epoch 4066, training loss: 13156.13, average training loss: 12844.10, base loss: 21646.97
[INFO 2017-06-28 19:34:08,517 main.py:51] epoch 4067, training loss: 11796.82, average training loss: 12843.25, base loss: 21647.03
[INFO 2017-06-28 19:34:09,824 main.py:51] epoch 4068, training loss: 12909.09, average training loss: 12844.03, base loss: 21648.08
[INFO 2017-06-28 19:34:11,228 main.py:51] epoch 4069, training loss: 11673.92, average training loss: 12843.75, base loss: 21646.99
[INFO 2017-06-28 19:34:12,571 main.py:51] epoch 4070, training loss: 11085.53, average training loss: 12842.03, base loss: 21644.23
[INFO 2017-06-28 19:34:13,937 main.py:51] epoch 4071, training loss: 14259.21, average training loss: 12843.05, base loss: 21646.87
[INFO 2017-06-28 19:34:15,320 main.py:51] epoch 4072, training loss: 11805.64, average training loss: 12841.62, base loss: 21645.37
[INFO 2017-06-28 19:34:16,664 main.py:51] epoch 4073, training loss: 13714.58, average training loss: 12842.39, base loss: 21645.99
[INFO 2017-06-28 19:34:18,015 main.py:51] epoch 4074, training loss: 12986.93, average training loss: 12843.48, base loss: 21647.55
[INFO 2017-06-28 19:34:19,500 main.py:51] epoch 4075, training loss: 12740.04, average training loss: 12844.66, base loss: 21649.13
[INFO 2017-06-28 19:34:20,824 main.py:51] epoch 4076, training loss: 13105.21, average training loss: 12845.60, base loss: 21649.47
[INFO 2017-06-28 19:34:22,093 main.py:51] epoch 4077, training loss: 13304.43, average training loss: 12846.43, base loss: 21652.54
[INFO 2017-06-28 19:34:23,354 main.py:51] epoch 4078, training loss: 11946.36, average training loss: 12845.44, base loss: 21651.45
[INFO 2017-06-28 19:34:24,789 main.py:51] epoch 4079, training loss: 12119.40, average training loss: 12844.39, base loss: 21651.69
[INFO 2017-06-28 19:34:26,106 main.py:51] epoch 4080, training loss: 11870.48, average training loss: 12844.15, base loss: 21652.14
[INFO 2017-06-28 19:34:27,518 main.py:51] epoch 4081, training loss: 12487.52, average training loss: 12843.04, base loss: 21651.82
[INFO 2017-06-28 19:34:28,794 main.py:51] epoch 4082, training loss: 13091.48, average training loss: 12844.35, base loss: 21654.88
[INFO 2017-06-28 19:34:30,080 main.py:51] epoch 4083, training loss: 12241.44, average training loss: 12843.61, base loss: 21653.12
[INFO 2017-06-28 19:34:31,487 main.py:51] epoch 4084, training loss: 12616.50, average training loss: 12843.97, base loss: 21653.65
[INFO 2017-06-28 19:34:32,765 main.py:51] epoch 4085, training loss: 12281.67, average training loss: 12842.81, base loss: 21652.67
[INFO 2017-06-28 19:34:34,103 main.py:51] epoch 4086, training loss: 11993.84, average training loss: 12842.04, base loss: 21651.44
[INFO 2017-06-28 19:34:35,506 main.py:51] epoch 4087, training loss: 11547.59, average training loss: 12840.50, base loss: 21650.09
[INFO 2017-06-28 19:34:36,800 main.py:51] epoch 4088, training loss: 13564.78, average training loss: 12842.10, base loss: 21651.90
[INFO 2017-06-28 19:34:38,097 main.py:51] epoch 4089, training loss: 12064.97, average training loss: 12840.60, base loss: 21650.75
[INFO 2017-06-28 19:34:39,466 main.py:51] epoch 4090, training loss: 13723.69, average training loss: 12842.47, base loss: 21651.23
[INFO 2017-06-28 19:34:40,648 main.py:51] epoch 4091, training loss: 11934.74, average training loss: 12840.52, base loss: 21650.38
[INFO 2017-06-28 19:34:41,899 main.py:51] epoch 4092, training loss: 13084.69, average training loss: 12842.43, base loss: 21652.93
[INFO 2017-06-28 19:34:43,182 main.py:51] epoch 4093, training loss: 13005.80, average training loss: 12843.59, base loss: 21653.23
[INFO 2017-06-28 19:34:44,516 main.py:51] epoch 4094, training loss: 13009.93, average training loss: 12843.69, base loss: 21652.85
[INFO 2017-06-28 19:34:45,819 main.py:51] epoch 4095, training loss: 12330.62, average training loss: 12843.31, base loss: 21652.61
[INFO 2017-06-28 19:34:47,177 main.py:51] epoch 4096, training loss: 12817.02, average training loss: 12843.23, base loss: 21654.19
[INFO 2017-06-28 19:34:48,494 main.py:51] epoch 4097, training loss: 12171.19, average training loss: 12841.70, base loss: 21653.53
[INFO 2017-06-28 19:34:49,777 main.py:51] epoch 4098, training loss: 13039.02, average training loss: 12840.72, base loss: 21651.88
[INFO 2017-06-28 19:34:51,138 main.py:51] epoch 4099, training loss: 14589.76, average training loss: 12842.17, base loss: 21654.07
[INFO 2017-06-28 19:34:51,138 main.py:53] epoch 4099, testing
[INFO 2017-06-28 19:34:57,125 main.py:105] average testing loss: 13865.83, base loss: 21864.17
[INFO 2017-06-28 19:34:57,125 main.py:106] improve_loss: 7998.33, improve_percent: 0.37
[INFO 2017-06-28 19:34:57,126 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:34:58,372 main.py:51] epoch 4100, training loss: 11311.43, average training loss: 12840.86, base loss: 21652.33
[INFO 2017-06-28 19:34:59,609 main.py:51] epoch 4101, training loss: 12600.88, average training loss: 12840.69, base loss: 21650.19
[INFO 2017-06-28 19:35:00,923 main.py:51] epoch 4102, training loss: 14171.30, average training loss: 12842.03, base loss: 21652.01
[INFO 2017-06-28 19:35:02,240 main.py:51] epoch 4103, training loss: 12983.91, average training loss: 12843.86, base loss: 21654.40
[INFO 2017-06-28 19:35:03,561 main.py:51] epoch 4104, training loss: 12083.74, average training loss: 12841.62, base loss: 21651.29
[INFO 2017-06-28 19:35:04,876 main.py:51] epoch 4105, training loss: 13697.61, average training loss: 12842.57, base loss: 21653.71
[INFO 2017-06-28 19:35:06,217 main.py:51] epoch 4106, training loss: 12145.20, average training loss: 12843.22, base loss: 21653.82
[INFO 2017-06-28 19:35:07,499 main.py:51] epoch 4107, training loss: 13498.20, average training loss: 12843.36, base loss: 21653.09
[INFO 2017-06-28 19:35:08,778 main.py:51] epoch 4108, training loss: 13035.26, average training loss: 12844.62, base loss: 21655.07
[INFO 2017-06-28 19:35:10,085 main.py:51] epoch 4109, training loss: 13044.28, average training loss: 12844.71, base loss: 21655.85
[INFO 2017-06-28 19:35:11,361 main.py:51] epoch 4110, training loss: 12326.85, average training loss: 12843.67, base loss: 21655.58
[INFO 2017-06-28 19:35:12,809 main.py:51] epoch 4111, training loss: 13594.09, average training loss: 12843.76, base loss: 21656.41
[INFO 2017-06-28 19:35:14,018 main.py:51] epoch 4112, training loss: 14991.41, average training loss: 12846.57, base loss: 21660.20
[INFO 2017-06-28 19:35:15,257 main.py:51] epoch 4113, training loss: 13685.80, average training loss: 12847.78, base loss: 21661.30
[INFO 2017-06-28 19:35:16,543 main.py:51] epoch 4114, training loss: 12539.60, average training loss: 12847.88, base loss: 21662.09
[INFO 2017-06-28 19:35:17,839 main.py:51] epoch 4115, training loss: 13147.36, average training loss: 12847.03, base loss: 21663.62
[INFO 2017-06-28 19:35:19,129 main.py:51] epoch 4116, training loss: 11265.10, average training loss: 12846.15, base loss: 21661.84
[INFO 2017-06-28 19:35:20,466 main.py:51] epoch 4117, training loss: 13529.27, average training loss: 12847.43, base loss: 21665.59
[INFO 2017-06-28 19:35:21,716 main.py:51] epoch 4118, training loss: 14571.52, average training loss: 12849.14, base loss: 21669.83
[INFO 2017-06-28 19:35:23,094 main.py:51] epoch 4119, training loss: 12227.73, average training loss: 12847.81, base loss: 21667.94
[INFO 2017-06-28 19:35:24,410 main.py:51] epoch 4120, training loss: 13343.91, average training loss: 12849.81, base loss: 21669.14
[INFO 2017-06-28 19:35:25,670 main.py:51] epoch 4121, training loss: 11589.78, average training loss: 12847.94, base loss: 21667.90
[INFO 2017-06-28 19:35:26,996 main.py:51] epoch 4122, training loss: 12172.65, average training loss: 12845.78, base loss: 21665.43
[INFO 2017-06-28 19:35:28,373 main.py:51] epoch 4123, training loss: 12740.57, average training loss: 12845.40, base loss: 21665.17
[INFO 2017-06-28 19:35:29,789 main.py:51] epoch 4124, training loss: 13303.02, average training loss: 12842.76, base loss: 21661.68
[INFO 2017-06-28 19:35:31,103 main.py:51] epoch 4125, training loss: 12877.54, average training loss: 12840.98, base loss: 21659.01
[INFO 2017-06-28 19:35:32,434 main.py:51] epoch 4126, training loss: 12465.07, average training loss: 12840.80, base loss: 21659.86
[INFO 2017-06-28 19:35:33,745 main.py:51] epoch 4127, training loss: 14738.62, average training loss: 12842.42, base loss: 21662.92
[INFO 2017-06-28 19:35:35,184 main.py:51] epoch 4128, training loss: 12819.37, average training loss: 12842.79, base loss: 21663.45
[INFO 2017-06-28 19:35:36,480 main.py:51] epoch 4129, training loss: 12822.70, average training loss: 12843.37, base loss: 21665.26
[INFO 2017-06-28 19:35:37,783 main.py:51] epoch 4130, training loss: 13721.80, average training loss: 12844.19, base loss: 21664.54
[INFO 2017-06-28 19:35:39,043 main.py:51] epoch 4131, training loss: 12160.61, average training loss: 12842.97, base loss: 21662.19
[INFO 2017-06-28 19:35:40,376 main.py:51] epoch 4132, training loss: 12498.19, average training loss: 12841.24, base loss: 21660.25
[INFO 2017-06-28 19:35:41,683 main.py:51] epoch 4133, training loss: 12275.75, average training loss: 12839.86, base loss: 21659.82
[INFO 2017-06-28 19:35:42,886 main.py:51] epoch 4134, training loss: 10784.33, average training loss: 12837.83, base loss: 21656.36
[INFO 2017-06-28 19:35:44,236 main.py:51] epoch 4135, training loss: 13183.57, average training loss: 12837.51, base loss: 21655.09
[INFO 2017-06-28 19:35:45,560 main.py:51] epoch 4136, training loss: 12147.46, average training loss: 12837.09, base loss: 21654.42
[INFO 2017-06-28 19:35:46,961 main.py:51] epoch 4137, training loss: 13669.57, average training loss: 12838.18, base loss: 21656.69
[INFO 2017-06-28 19:35:48,265 main.py:51] epoch 4138, training loss: 12661.88, average training loss: 12837.92, base loss: 21656.78
[INFO 2017-06-28 19:35:49,667 main.py:51] epoch 4139, training loss: 13964.92, average training loss: 12838.94, base loss: 21658.78
[INFO 2017-06-28 19:35:51,079 main.py:51] epoch 4140, training loss: 13086.52, average training loss: 12838.56, base loss: 21660.91
[INFO 2017-06-28 19:35:52,328 main.py:51] epoch 4141, training loss: 12707.04, average training loss: 12839.37, base loss: 21665.19
[INFO 2017-06-28 19:35:53,778 main.py:51] epoch 4142, training loss: 12391.76, average training loss: 12838.32, base loss: 21664.60
[INFO 2017-06-28 19:35:55,072 main.py:51] epoch 4143, training loss: 10916.25, average training loss: 12836.94, base loss: 21661.90
[INFO 2017-06-28 19:35:56,482 main.py:51] epoch 4144, training loss: 12300.29, average training loss: 12836.79, base loss: 21663.66
[INFO 2017-06-28 19:35:57,790 main.py:51] epoch 4145, training loss: 11917.24, average training loss: 12836.20, base loss: 21663.38
[INFO 2017-06-28 19:35:59,119 main.py:51] epoch 4146, training loss: 13066.36, average training loss: 12836.49, base loss: 21662.37
[INFO 2017-06-28 19:36:00,374 main.py:51] epoch 4147, training loss: 12827.75, average training loss: 12834.98, base loss: 21661.09
[INFO 2017-06-28 19:36:01,654 main.py:51] epoch 4148, training loss: 14739.55, average training loss: 12836.00, base loss: 21661.83
[INFO 2017-06-28 19:36:03,149 main.py:51] epoch 4149, training loss: 12868.79, average training loss: 12835.99, base loss: 21661.95
[INFO 2017-06-28 19:36:04,421 main.py:51] epoch 4150, training loss: 12839.28, average training loss: 12833.18, base loss: 21659.89
[INFO 2017-06-28 19:36:05,811 main.py:51] epoch 4151, training loss: 12895.59, average training loss: 12832.81, base loss: 21659.82
[INFO 2017-06-28 19:36:07,029 main.py:51] epoch 4152, training loss: 12875.08, average training loss: 12832.19, base loss: 21659.37
[INFO 2017-06-28 19:36:08,452 main.py:51] epoch 4153, training loss: 12671.15, average training loss: 12831.53, base loss: 21657.99
[INFO 2017-06-28 19:36:09,764 main.py:51] epoch 4154, training loss: 12460.01, average training loss: 12830.02, base loss: 21655.33
[INFO 2017-06-28 19:36:11,042 main.py:51] epoch 4155, training loss: 13296.86, average training loss: 12830.06, base loss: 21654.74
[INFO 2017-06-28 19:36:12,318 main.py:51] epoch 4156, training loss: 12883.40, average training loss: 12830.38, base loss: 21654.40
[INFO 2017-06-28 19:36:13,599 main.py:51] epoch 4157, training loss: 14589.93, average training loss: 12832.26, base loss: 21655.36
[INFO 2017-06-28 19:36:14,911 main.py:51] epoch 4158, training loss: 12737.18, average training loss: 12831.69, base loss: 21657.42
[INFO 2017-06-28 19:36:16,198 main.py:51] epoch 4159, training loss: 13379.41, average training loss: 12832.12, base loss: 21659.81
[INFO 2017-06-28 19:36:17,372 main.py:51] epoch 4160, training loss: 12506.20, average training loss: 12831.36, base loss: 21657.36
[INFO 2017-06-28 19:36:18,769 main.py:51] epoch 4161, training loss: 13274.98, average training loss: 12831.42, base loss: 21657.08
[INFO 2017-06-28 19:36:20,068 main.py:51] epoch 4162, training loss: 13598.17, average training loss: 12831.86, base loss: 21655.95
[INFO 2017-06-28 19:36:21,312 main.py:51] epoch 4163, training loss: 13147.95, average training loss: 12832.46, base loss: 21656.76
[INFO 2017-06-28 19:36:22,613 main.py:51] epoch 4164, training loss: 13256.03, average training loss: 12833.65, base loss: 21655.18
[INFO 2017-06-28 19:36:23,866 main.py:51] epoch 4165, training loss: 11901.24, average training loss: 12832.95, base loss: 21655.23
[INFO 2017-06-28 19:36:25,172 main.py:51] epoch 4166, training loss: 13358.86, average training loss: 12833.88, base loss: 21656.39
[INFO 2017-06-28 19:36:26,580 main.py:51] epoch 4167, training loss: 12435.11, average training loss: 12833.79, base loss: 21656.64
[INFO 2017-06-28 19:36:28,034 main.py:51] epoch 4168, training loss: 12308.09, average training loss: 12833.38, base loss: 21654.89
[INFO 2017-06-28 19:36:29,299 main.py:51] epoch 4169, training loss: 13061.41, average training loss: 12833.76, base loss: 21653.01
[INFO 2017-06-28 19:36:30,551 main.py:51] epoch 4170, training loss: 11624.09, average training loss: 12833.17, base loss: 21652.33
[INFO 2017-06-28 19:36:31,815 main.py:51] epoch 4171, training loss: 12758.49, average training loss: 12832.12, base loss: 21651.57
[INFO 2017-06-28 19:36:33,204 main.py:51] epoch 4172, training loss: 14623.68, average training loss: 12834.41, base loss: 21655.01
[INFO 2017-06-28 19:36:34,467 main.py:51] epoch 4173, training loss: 13479.02, average training loss: 12832.82, base loss: 21652.45
[INFO 2017-06-28 19:36:35,663 main.py:51] epoch 4174, training loss: 12637.83, average training loss: 12832.32, base loss: 21650.58
[INFO 2017-06-28 19:36:37,020 main.py:51] epoch 4175, training loss: 13423.56, average training loss: 12833.36, base loss: 21651.83
[INFO 2017-06-28 19:36:38,342 main.py:51] epoch 4176, training loss: 12538.10, average training loss: 12833.81, base loss: 21652.79
[INFO 2017-06-28 19:36:39,630 main.py:51] epoch 4177, training loss: 12113.65, average training loss: 12832.78, base loss: 21652.29
[INFO 2017-06-28 19:36:40,940 main.py:51] epoch 4178, training loss: 13454.91, average training loss: 12832.50, base loss: 21652.43
[INFO 2017-06-28 19:36:42,308 main.py:51] epoch 4179, training loss: 12078.90, average training loss: 12832.80, base loss: 21653.23
[INFO 2017-06-28 19:36:43,601 main.py:51] epoch 4180, training loss: 12447.23, average training loss: 12830.55, base loss: 21650.95
[INFO 2017-06-28 19:36:44,975 main.py:51] epoch 4181, training loss: 12233.09, average training loss: 12829.21, base loss: 21649.18
[INFO 2017-06-28 19:36:46,329 main.py:51] epoch 4182, training loss: 14941.77, average training loss: 12830.85, base loss: 21650.31
[INFO 2017-06-28 19:36:47,697 main.py:51] epoch 4183, training loss: 14118.93, average training loss: 12831.31, base loss: 21651.97
[INFO 2017-06-28 19:36:48,950 main.py:51] epoch 4184, training loss: 12620.59, average training loss: 12831.41, base loss: 21653.49
[INFO 2017-06-28 19:36:50,256 main.py:51] epoch 4185, training loss: 12769.31, average training loss: 12831.51, base loss: 21654.28
[INFO 2017-06-28 19:36:51,619 main.py:51] epoch 4186, training loss: 13122.76, average training loss: 12831.47, base loss: 21654.90
[INFO 2017-06-28 19:36:52,995 main.py:51] epoch 4187, training loss: 13446.63, average training loss: 12833.98, base loss: 21659.17
[INFO 2017-06-28 19:36:54,232 main.py:51] epoch 4188, training loss: 12496.66, average training loss: 12834.38, base loss: 21660.82
[INFO 2017-06-28 19:36:55,594 main.py:51] epoch 4189, training loss: 12361.10, average training loss: 12834.02, base loss: 21659.11
[INFO 2017-06-28 19:36:56,877 main.py:51] epoch 4190, training loss: 11712.37, average training loss: 12833.32, base loss: 21657.79
[INFO 2017-06-28 19:36:58,112 main.py:51] epoch 4191, training loss: 13605.19, average training loss: 12835.37, base loss: 21661.18
[INFO 2017-06-28 19:36:59,397 main.py:51] epoch 4192, training loss: 12399.10, average training loss: 12833.92, base loss: 21658.69
[INFO 2017-06-28 19:37:00,650 main.py:51] epoch 4193, training loss: 12433.56, average training loss: 12833.64, base loss: 21657.32
[INFO 2017-06-28 19:37:01,890 main.py:51] epoch 4194, training loss: 13804.57, average training loss: 12834.42, base loss: 21656.90
[INFO 2017-06-28 19:37:03,145 main.py:51] epoch 4195, training loss: 12000.94, average training loss: 12831.65, base loss: 21653.20
[INFO 2017-06-28 19:37:04,430 main.py:51] epoch 4196, training loss: 13663.97, average training loss: 12831.28, base loss: 21650.89
[INFO 2017-06-28 19:37:05,719 main.py:51] epoch 4197, training loss: 11455.85, average training loss: 12828.97, base loss: 21648.31
[INFO 2017-06-28 19:37:07,028 main.py:51] epoch 4198, training loss: 12517.97, average training loss: 12826.33, base loss: 21643.61
[INFO 2017-06-28 19:37:08,368 main.py:51] epoch 4199, training loss: 12476.30, average training loss: 12826.29, base loss: 21643.27
[INFO 2017-06-28 19:37:08,369 main.py:53] epoch 4199, testing
[INFO 2017-06-28 19:37:14,680 main.py:105] average testing loss: 13938.01, base loss: 21890.25
[INFO 2017-06-28 19:37:14,681 main.py:106] improve_loss: 7952.23, improve_percent: 0.36
[INFO 2017-06-28 19:37:14,681 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:37:15,908 main.py:51] epoch 4200, training loss: 14538.87, average training loss: 12827.29, base loss: 21645.01
[INFO 2017-06-28 19:37:17,202 main.py:51] epoch 4201, training loss: 12469.57, average training loss: 12827.34, base loss: 21644.82
[INFO 2017-06-28 19:37:18,488 main.py:51] epoch 4202, training loss: 11257.00, average training loss: 12825.35, base loss: 21642.83
[INFO 2017-06-28 19:37:19,985 main.py:51] epoch 4203, training loss: 14403.11, average training loss: 12825.41, base loss: 21643.85
[INFO 2017-06-28 19:37:21,343 main.py:51] epoch 4204, training loss: 13266.45, average training loss: 12826.31, base loss: 21646.28
[INFO 2017-06-28 19:37:22,714 main.py:51] epoch 4205, training loss: 13024.97, average training loss: 12825.11, base loss: 21644.49
[INFO 2017-06-28 19:37:24,079 main.py:51] epoch 4206, training loss: 12879.55, average training loss: 12825.15, base loss: 21642.98
[INFO 2017-06-28 19:37:25,338 main.py:51] epoch 4207, training loss: 11686.60, average training loss: 12823.84, base loss: 21643.67
[INFO 2017-06-28 19:37:26,638 main.py:51] epoch 4208, training loss: 11963.81, average training loss: 12823.15, base loss: 21642.17
[INFO 2017-06-28 19:37:27,976 main.py:51] epoch 4209, training loss: 13046.60, average training loss: 12822.13, base loss: 21644.05
[INFO 2017-06-28 19:37:29,284 main.py:51] epoch 4210, training loss: 12617.82, average training loss: 12821.47, base loss: 21644.12
[INFO 2017-06-28 19:37:30,507 main.py:51] epoch 4211, training loss: 13773.45, average training loss: 12821.92, base loss: 21646.20
[INFO 2017-06-28 19:37:31,865 main.py:51] epoch 4212, training loss: 14167.36, average training loss: 12824.03, base loss: 21649.87
[INFO 2017-06-28 19:37:33,308 main.py:51] epoch 4213, training loss: 12136.14, average training loss: 12822.29, base loss: 21649.65
[INFO 2017-06-28 19:37:34,660 main.py:51] epoch 4214, training loss: 12219.57, average training loss: 12822.20, base loss: 21651.88
[INFO 2017-06-28 19:37:35,990 main.py:51] epoch 4215, training loss: 13199.05, average training loss: 12822.99, base loss: 21651.00
[INFO 2017-06-28 19:37:37,316 main.py:51] epoch 4216, training loss: 12289.00, average training loss: 12821.96, base loss: 21649.55
[INFO 2017-06-28 19:37:38,677 main.py:51] epoch 4217, training loss: 11197.23, average training loss: 12818.90, base loss: 21645.07
[INFO 2017-06-28 19:37:39,847 main.py:51] epoch 4218, training loss: 12239.38, average training loss: 12818.96, base loss: 21646.12
[INFO 2017-06-28 19:37:41,224 main.py:51] epoch 4219, training loss: 12485.18, average training loss: 12819.53, base loss: 21644.33
[INFO 2017-06-28 19:37:42,545 main.py:51] epoch 4220, training loss: 12690.66, average training loss: 12819.92, base loss: 21643.15
[INFO 2017-06-28 19:37:43,916 main.py:51] epoch 4221, training loss: 12776.20, average training loss: 12817.82, base loss: 21643.96
[INFO 2017-06-28 19:37:45,232 main.py:51] epoch 4222, training loss: 12286.61, average training loss: 12817.69, base loss: 21643.05
[INFO 2017-06-28 19:37:46,659 main.py:51] epoch 4223, training loss: 11586.33, average training loss: 12816.54, base loss: 21641.75
[INFO 2017-06-28 19:37:47,905 main.py:51] epoch 4224, training loss: 12721.58, average training loss: 12816.17, base loss: 21643.40
[INFO 2017-06-28 19:37:49,167 main.py:51] epoch 4225, training loss: 13259.15, average training loss: 12817.22, base loss: 21646.96
[INFO 2017-06-28 19:37:50,505 main.py:51] epoch 4226, training loss: 13379.18, average training loss: 12818.58, base loss: 21649.96
[INFO 2017-06-28 19:37:51,838 main.py:51] epoch 4227, training loss: 13624.11, average training loss: 12819.29, base loss: 21650.74
[INFO 2017-06-28 19:37:53,181 main.py:51] epoch 4228, training loss: 13045.84, average training loss: 12820.10, base loss: 21654.51
[INFO 2017-06-28 19:37:54,473 main.py:51] epoch 4229, training loss: 13335.46, average training loss: 12819.58, base loss: 21653.83
[INFO 2017-06-28 19:37:55,737 main.py:51] epoch 4230, training loss: 13792.75, average training loss: 12821.04, base loss: 21654.95
[INFO 2017-06-28 19:37:57,039 main.py:51] epoch 4231, training loss: 12454.26, average training loss: 12820.45, base loss: 21653.25
[INFO 2017-06-28 19:37:58,370 main.py:51] epoch 4232, training loss: 15196.60, average training loss: 12824.11, base loss: 21658.13
[INFO 2017-06-28 19:37:59,754 main.py:51] epoch 4233, training loss: 12555.96, average training loss: 12823.14, base loss: 21655.71
[INFO 2017-06-28 19:38:00,999 main.py:51] epoch 4234, training loss: 13067.74, average training loss: 12823.46, base loss: 21656.07
[INFO 2017-06-28 19:38:02,222 main.py:51] epoch 4235, training loss: 12101.75, average training loss: 12821.30, base loss: 21652.93
[INFO 2017-06-28 19:38:03,495 main.py:51] epoch 4236, training loss: 12975.60, average training loss: 12822.72, base loss: 21655.12
[INFO 2017-06-28 19:38:04,742 main.py:51] epoch 4237, training loss: 12459.28, average training loss: 12820.76, base loss: 21650.70
[INFO 2017-06-28 19:38:06,026 main.py:51] epoch 4238, training loss: 11872.73, average training loss: 12820.43, base loss: 21647.88
[INFO 2017-06-28 19:38:07,254 main.py:51] epoch 4239, training loss: 13193.40, average training loss: 12820.86, base loss: 21647.69
[INFO 2017-06-28 19:38:08,542 main.py:51] epoch 4240, training loss: 11319.64, average training loss: 12819.81, base loss: 21646.75
[INFO 2017-06-28 19:38:09,852 main.py:51] epoch 4241, training loss: 12573.09, average training loss: 12818.81, base loss: 21645.48
[INFO 2017-06-28 19:38:11,125 main.py:51] epoch 4242, training loss: 12055.44, average training loss: 12819.06, base loss: 21646.88
[INFO 2017-06-28 19:38:12,368 main.py:51] epoch 4243, training loss: 14497.93, average training loss: 12821.45, base loss: 21650.98
[INFO 2017-06-28 19:38:13,688 main.py:51] epoch 4244, training loss: 12225.54, average training loss: 12820.26, base loss: 21649.29
[INFO 2017-06-28 19:38:15,023 main.py:51] epoch 4245, training loss: 14218.26, average training loss: 12821.48, base loss: 21652.30
[INFO 2017-06-28 19:38:16,340 main.py:51] epoch 4246, training loss: 13411.78, average training loss: 12820.56, base loss: 21651.16
[INFO 2017-06-28 19:38:17,558 main.py:51] epoch 4247, training loss: 12604.13, average training loss: 12820.01, base loss: 21652.07
[INFO 2017-06-28 19:38:18,767 main.py:51] epoch 4248, training loss: 12982.18, average training loss: 12820.26, base loss: 21654.82
[INFO 2017-06-28 19:38:20,017 main.py:51] epoch 4249, training loss: 13791.91, average training loss: 12821.03, base loss: 21655.37
[INFO 2017-06-28 19:38:21,314 main.py:51] epoch 4250, training loss: 12316.90, average training loss: 12821.17, base loss: 21656.04
[INFO 2017-06-28 19:38:22,813 main.py:51] epoch 4251, training loss: 11714.05, average training loss: 12819.81, base loss: 21653.51
[INFO 2017-06-28 19:38:24,072 main.py:51] epoch 4252, training loss: 11940.36, average training loss: 12818.29, base loss: 21651.70
[INFO 2017-06-28 19:38:25,391 main.py:51] epoch 4253, training loss: 12448.91, average training loss: 12817.97, base loss: 21651.47
[INFO 2017-06-28 19:38:26,706 main.py:51] epoch 4254, training loss: 13105.17, average training loss: 12818.01, base loss: 21652.08
[INFO 2017-06-28 19:38:28,101 main.py:51] epoch 4255, training loss: 12068.08, average training loss: 12817.09, base loss: 21652.14
[INFO 2017-06-28 19:38:29,486 main.py:51] epoch 4256, training loss: 13740.44, average training loss: 12817.75, base loss: 21653.18
[INFO 2017-06-28 19:38:30,836 main.py:51] epoch 4257, training loss: 13723.50, average training loss: 12818.50, base loss: 21654.19
[INFO 2017-06-28 19:38:32,127 main.py:51] epoch 4258, training loss: 12957.56, average training loss: 12818.90, base loss: 21652.67
[INFO 2017-06-28 19:38:33,334 main.py:51] epoch 4259, training loss: 12312.68, average training loss: 12818.82, base loss: 21652.59
[INFO 2017-06-28 19:38:34,789 main.py:51] epoch 4260, training loss: 12418.60, average training loss: 12817.56, base loss: 21651.97
[INFO 2017-06-28 19:38:36,053 main.py:51] epoch 4261, training loss: 11577.45, average training loss: 12817.02, base loss: 21651.53
[INFO 2017-06-28 19:38:37,342 main.py:51] epoch 4262, training loss: 12027.76, average training loss: 12816.17, base loss: 21650.86
[INFO 2017-06-28 19:38:38,608 main.py:51] epoch 4263, training loss: 12837.41, average training loss: 12815.12, base loss: 21650.51
[INFO 2017-06-28 19:38:39,870 main.py:51] epoch 4264, training loss: 12400.47, average training loss: 12813.83, base loss: 21649.29
[INFO 2017-06-28 19:38:41,127 main.py:51] epoch 4265, training loss: 12817.71, average training loss: 12813.35, base loss: 21647.38
[INFO 2017-06-28 19:38:42,441 main.py:51] epoch 4266, training loss: 11842.22, average training loss: 12813.21, base loss: 21646.53
[INFO 2017-06-28 19:38:43,824 main.py:51] epoch 4267, training loss: 13515.87, average training loss: 12811.21, base loss: 21647.18
[INFO 2017-06-28 19:38:45,223 main.py:51] epoch 4268, training loss: 12827.92, average training loss: 12811.37, base loss: 21646.77
[INFO 2017-06-28 19:38:46,533 main.py:51] epoch 4269, training loss: 11798.09, average training loss: 12808.92, base loss: 21644.41
[INFO 2017-06-28 19:38:47,840 main.py:51] epoch 4270, training loss: 11776.78, average training loss: 12807.33, base loss: 21643.98
[INFO 2017-06-28 19:38:49,130 main.py:51] epoch 4271, training loss: 14524.02, average training loss: 12809.61, base loss: 21646.29
[INFO 2017-06-28 19:38:50,414 main.py:51] epoch 4272, training loss: 12920.86, average training loss: 12808.17, base loss: 21644.61
[INFO 2017-06-28 19:38:51,626 main.py:51] epoch 4273, training loss: 11783.23, average training loss: 12807.14, base loss: 21642.06
[INFO 2017-06-28 19:38:52,901 main.py:51] epoch 4274, training loss: 12676.49, average training loss: 12806.53, base loss: 21642.62
[INFO 2017-06-28 19:38:54,299 main.py:51] epoch 4275, training loss: 12692.10, average training loss: 12805.01, base loss: 21638.72
[INFO 2017-06-28 19:38:55,571 main.py:51] epoch 4276, training loss: 11342.13, average training loss: 12803.89, base loss: 21638.14
[INFO 2017-06-28 19:38:56,858 main.py:51] epoch 4277, training loss: 12651.48, average training loss: 12803.43, base loss: 21636.54
[INFO 2017-06-28 19:38:58,221 main.py:51] epoch 4278, training loss: 12861.49, average training loss: 12802.74, base loss: 21636.95
[INFO 2017-06-28 19:38:59,477 main.py:51] epoch 4279, training loss: 11687.89, average training loss: 12800.42, base loss: 21634.64
[INFO 2017-06-28 19:39:00,892 main.py:51] epoch 4280, training loss: 14004.79, average training loss: 12802.46, base loss: 21638.00
[INFO 2017-06-28 19:39:02,224 main.py:51] epoch 4281, training loss: 12945.25, average training loss: 12802.88, base loss: 21637.47
[INFO 2017-06-28 19:39:03,496 main.py:51] epoch 4282, training loss: 14330.04, average training loss: 12803.11, base loss: 21638.30
[INFO 2017-06-28 19:39:04,718 main.py:51] epoch 4283, training loss: 12979.25, average training loss: 12802.99, base loss: 21636.75
[INFO 2017-06-28 19:39:06,033 main.py:51] epoch 4284, training loss: 13670.75, average training loss: 12802.43, base loss: 21636.27
[INFO 2017-06-28 19:39:07,414 main.py:51] epoch 4285, training loss: 13932.30, average training loss: 12803.61, base loss: 21635.20
[INFO 2017-06-28 19:39:08,714 main.py:51] epoch 4286, training loss: 13616.33, average training loss: 12804.32, base loss: 21634.17
[INFO 2017-06-28 19:39:09,984 main.py:51] epoch 4287, training loss: 12869.66, average training loss: 12803.83, base loss: 21633.79
[INFO 2017-06-28 19:39:11,397 main.py:51] epoch 4288, training loss: 12986.05, average training loss: 12802.87, base loss: 21629.89
[INFO 2017-06-28 19:39:12,766 main.py:51] epoch 4289, training loss: 15671.90, average training loss: 12805.26, base loss: 21633.52
[INFO 2017-06-28 19:39:14,039 main.py:51] epoch 4290, training loss: 13584.74, average training loss: 12806.32, base loss: 21634.43
[INFO 2017-06-28 19:39:15,353 main.py:51] epoch 4291, training loss: 12163.67, average training loss: 12804.16, base loss: 21631.14
[INFO 2017-06-28 19:39:16,781 main.py:51] epoch 4292, training loss: 12915.43, average training loss: 12803.60, base loss: 21629.12
[INFO 2017-06-28 19:39:18,028 main.py:51] epoch 4293, training loss: 13548.76, average training loss: 12804.19, base loss: 21628.86
[INFO 2017-06-28 19:39:19,285 main.py:51] epoch 4294, training loss: 12344.31, average training loss: 12804.38, base loss: 21630.05
[INFO 2017-06-28 19:39:20,580 main.py:51] epoch 4295, training loss: 13270.78, average training loss: 12805.45, base loss: 21630.10
[INFO 2017-06-28 19:39:21,867 main.py:51] epoch 4296, training loss: 12542.23, average training loss: 12805.68, base loss: 21631.28
[INFO 2017-06-28 19:39:23,161 main.py:51] epoch 4297, training loss: 12437.73, average training loss: 12805.40, base loss: 21631.34
[INFO 2017-06-28 19:39:24,584 main.py:51] epoch 4298, training loss: 13082.56, average training loss: 12806.53, base loss: 21632.58
[INFO 2017-06-28 19:39:25,881 main.py:51] epoch 4299, training loss: 12678.69, average training loss: 12806.50, base loss: 21634.03
[INFO 2017-06-28 19:39:25,881 main.py:53] epoch 4299, testing
[INFO 2017-06-28 19:39:31,782 main.py:105] average testing loss: 14317.76, base loss: 21692.10
[INFO 2017-06-28 19:39:31,782 main.py:106] improve_loss: 7374.34, improve_percent: 0.34
[INFO 2017-06-28 19:39:31,783 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:39:33,100 main.py:51] epoch 4300, training loss: 12880.10, average training loss: 12807.28, base loss: 21636.74
[INFO 2017-06-28 19:39:34,394 main.py:51] epoch 4301, training loss: 13155.59, average training loss: 12807.28, base loss: 21635.34
[INFO 2017-06-28 19:39:35,701 main.py:51] epoch 4302, training loss: 13200.11, average training loss: 12805.74, base loss: 21635.83
[INFO 2017-06-28 19:39:37,076 main.py:51] epoch 4303, training loss: 12522.95, average training loss: 12805.01, base loss: 21633.45
[INFO 2017-06-28 19:39:38,379 main.py:51] epoch 4304, training loss: 13262.49, average training loss: 12802.99, base loss: 21633.23
[INFO 2017-06-28 19:39:39,610 main.py:51] epoch 4305, training loss: 12812.52, average training loss: 12803.07, base loss: 21633.86
[INFO 2017-06-28 19:39:40,944 main.py:51] epoch 4306, training loss: 12910.92, average training loss: 12802.43, base loss: 21631.69
[INFO 2017-06-28 19:39:42,228 main.py:51] epoch 4307, training loss: 13646.65, average training loss: 12803.36, base loss: 21631.88
[INFO 2017-06-28 19:39:43,526 main.py:51] epoch 4308, training loss: 12706.26, average training loss: 12803.86, base loss: 21632.84
[INFO 2017-06-28 19:39:44,903 main.py:51] epoch 4309, training loss: 12257.30, average training loss: 12803.63, base loss: 21631.92
[INFO 2017-06-28 19:39:46,298 main.py:51] epoch 4310, training loss: 12731.47, average training loss: 12803.67, base loss: 21632.20
[INFO 2017-06-28 19:39:47,492 main.py:51] epoch 4311, training loss: 11808.59, average training loss: 12802.55, base loss: 21631.70
[INFO 2017-06-28 19:39:48,828 main.py:51] epoch 4312, training loss: 13584.15, average training loss: 12803.57, base loss: 21633.70
[INFO 2017-06-28 19:39:50,077 main.py:51] epoch 4313, training loss: 11998.34, average training loss: 12802.18, base loss: 21631.51
[INFO 2017-06-28 19:39:51,448 main.py:51] epoch 4314, training loss: 13290.90, average training loss: 12801.68, base loss: 21627.96
[INFO 2017-06-28 19:39:52,717 main.py:51] epoch 4315, training loss: 13259.65, average training loss: 12803.57, base loss: 21630.27
[INFO 2017-06-28 19:39:54,087 main.py:51] epoch 4316, training loss: 14258.71, average training loss: 12804.78, base loss: 21634.85
[INFO 2017-06-28 19:39:55,307 main.py:51] epoch 4317, training loss: 12539.92, average training loss: 12804.94, base loss: 21633.67
[INFO 2017-06-28 19:39:56,584 main.py:51] epoch 4318, training loss: 12312.36, average training loss: 12803.90, base loss: 21630.14
[INFO 2017-06-28 19:39:57,863 main.py:51] epoch 4319, training loss: 11999.60, average training loss: 12803.72, base loss: 21629.80
[INFO 2017-06-28 19:39:59,246 main.py:51] epoch 4320, training loss: 12382.37, average training loss: 12801.93, base loss: 21627.72
[INFO 2017-06-28 19:40:00,557 main.py:51] epoch 4321, training loss: 12103.60, average training loss: 12800.93, base loss: 21626.73
[INFO 2017-06-28 19:40:01,818 main.py:51] epoch 4322, training loss: 12731.31, average training loss: 12800.39, base loss: 21625.40
[INFO 2017-06-28 19:40:03,200 main.py:51] epoch 4323, training loss: 12906.44, average training loss: 12802.02, base loss: 21626.80
[INFO 2017-06-28 19:40:04,533 main.py:51] epoch 4324, training loss: 12685.72, average training loss: 12802.38, base loss: 21626.25
[INFO 2017-06-28 19:40:05,842 main.py:51] epoch 4325, training loss: 13408.71, average training loss: 12802.47, base loss: 21626.40
[INFO 2017-06-28 19:40:07,300 main.py:51] epoch 4326, training loss: 12947.32, average training loss: 12802.07, base loss: 21626.19
[INFO 2017-06-28 19:40:08,641 main.py:51] epoch 4327, training loss: 12969.50, average training loss: 12801.11, base loss: 21624.05
[INFO 2017-06-28 19:40:09,874 main.py:51] epoch 4328, training loss: 13910.09, average training loss: 12802.85, base loss: 21625.17
[INFO 2017-06-28 19:40:11,181 main.py:51] epoch 4329, training loss: 12618.52, average training loss: 12802.09, base loss: 21623.80
[INFO 2017-06-28 19:40:12,459 main.py:51] epoch 4330, training loss: 13073.47, average training loss: 12802.76, base loss: 21624.21
[INFO 2017-06-28 19:40:13,772 main.py:51] epoch 4331, training loss: 12592.17, average training loss: 12802.77, base loss: 21624.34
[INFO 2017-06-28 19:40:15,132 main.py:51] epoch 4332, training loss: 14160.08, average training loss: 12803.02, base loss: 21625.02
[INFO 2017-06-28 19:40:16,444 main.py:51] epoch 4333, training loss: 12721.18, average training loss: 12801.46, base loss: 21622.17
[INFO 2017-06-28 19:40:17,874 main.py:51] epoch 4334, training loss: 12128.79, average training loss: 12801.93, base loss: 21622.96
[INFO 2017-06-28 19:40:19,122 main.py:51] epoch 4335, training loss: 13472.36, average training loss: 12800.90, base loss: 21620.85
[INFO 2017-06-28 19:40:20,418 main.py:51] epoch 4336, training loss: 12296.39, average training loss: 12800.57, base loss: 21619.13
[INFO 2017-06-28 19:40:21,812 main.py:51] epoch 4337, training loss: 13259.33, average training loss: 12799.42, base loss: 21618.40
[INFO 2017-06-28 19:40:23,088 main.py:51] epoch 4338, training loss: 12895.87, average training loss: 12799.44, base loss: 21620.11
[INFO 2017-06-28 19:40:24,512 main.py:51] epoch 4339, training loss: 13476.35, average training loss: 12800.12, base loss: 21621.34
[INFO 2017-06-28 19:40:25,877 main.py:51] epoch 4340, training loss: 11815.40, average training loss: 12798.84, base loss: 21619.51
[INFO 2017-06-28 19:40:27,223 main.py:51] epoch 4341, training loss: 12206.76, average training loss: 12798.45, base loss: 21620.68
[INFO 2017-06-28 19:40:28,558 main.py:51] epoch 4342, training loss: 12233.95, average training loss: 12796.82, base loss: 21618.66
[INFO 2017-06-28 19:40:29,764 main.py:51] epoch 4343, training loss: 12167.34, average training loss: 12796.50, base loss: 21617.73
[INFO 2017-06-28 19:40:30,978 main.py:51] epoch 4344, training loss: 12551.09, average training loss: 12796.14, base loss: 21618.64
[INFO 2017-06-28 19:40:32,340 main.py:51] epoch 4345, training loss: 13088.17, average training loss: 12795.97, base loss: 21619.57
[INFO 2017-06-28 19:40:33,652 main.py:51] epoch 4346, training loss: 13845.27, average training loss: 12796.38, base loss: 21619.82
[INFO 2017-06-28 19:40:34,969 main.py:51] epoch 4347, training loss: 12642.32, average training loss: 12795.55, base loss: 21618.11
[INFO 2017-06-28 19:40:36,291 main.py:51] epoch 4348, training loss: 12571.83, average training loss: 12795.03, base loss: 21616.48
[INFO 2017-06-28 19:40:37,674 main.py:51] epoch 4349, training loss: 12021.50, average training loss: 12793.88, base loss: 21616.39
[INFO 2017-06-28 19:40:39,025 main.py:51] epoch 4350, training loss: 11870.49, average training loss: 12793.34, base loss: 21617.84
[INFO 2017-06-28 19:40:40,438 main.py:51] epoch 4351, training loss: 12651.25, average training loss: 12791.98, base loss: 21615.74
[INFO 2017-06-28 19:40:41,743 main.py:51] epoch 4352, training loss: 11095.11, average training loss: 12788.80, base loss: 21610.87
[INFO 2017-06-28 19:40:43,118 main.py:51] epoch 4353, training loss: 12354.78, average training loss: 12788.40, base loss: 21611.00
[INFO 2017-06-28 19:40:44,480 main.py:51] epoch 4354, training loss: 12771.32, average training loss: 12788.18, base loss: 21607.61
[INFO 2017-06-28 19:40:45,852 main.py:51] epoch 4355, training loss: 11316.66, average training loss: 12785.78, base loss: 21606.62
[INFO 2017-06-28 19:40:47,112 main.py:51] epoch 4356, training loss: 13182.73, average training loss: 12786.60, base loss: 21607.51
[INFO 2017-06-28 19:40:48,521 main.py:51] epoch 4357, training loss: 11233.47, average training loss: 12786.07, base loss: 21606.54
[INFO 2017-06-28 19:40:49,850 main.py:51] epoch 4358, training loss: 11967.79, average training loss: 12785.01, base loss: 21607.47
[INFO 2017-06-28 19:40:51,272 main.py:51] epoch 4359, training loss: 11350.77, average training loss: 12782.93, base loss: 21605.15
[INFO 2017-06-28 19:40:52,601 main.py:51] epoch 4360, training loss: 12663.41, average training loss: 12781.78, base loss: 21605.88
[INFO 2017-06-28 19:40:54,048 main.py:51] epoch 4361, training loss: 12563.25, average training loss: 12781.81, base loss: 21604.76
[INFO 2017-06-28 19:40:55,364 main.py:51] epoch 4362, training loss: 12813.67, average training loss: 12781.67, base loss: 21605.32
[INFO 2017-06-28 19:40:56,690 main.py:51] epoch 4363, training loss: 12584.11, average training loss: 12779.26, base loss: 21601.33
[INFO 2017-06-28 19:40:57,992 main.py:51] epoch 4364, training loss: 12456.38, average training loss: 12779.81, base loss: 21603.32
[INFO 2017-06-28 19:40:59,356 main.py:51] epoch 4365, training loss: 13037.89, average training loss: 12780.40, base loss: 21603.20
[INFO 2017-06-28 19:41:00,629 main.py:51] epoch 4366, training loss: 13656.43, average training loss: 12780.15, base loss: 21604.08
[INFO 2017-06-28 19:41:02,000 main.py:51] epoch 4367, training loss: 11002.88, average training loss: 12779.48, base loss: 21604.16
[INFO 2017-06-28 19:41:03,362 main.py:51] epoch 4368, training loss: 12009.38, average training loss: 12777.21, base loss: 21602.68
[INFO 2017-06-28 19:41:04,723 main.py:51] epoch 4369, training loss: 13398.45, average training loss: 12777.17, base loss: 21603.82
[INFO 2017-06-28 19:41:06,020 main.py:51] epoch 4370, training loss: 14797.39, average training loss: 12779.42, base loss: 21606.32
[INFO 2017-06-28 19:41:07,444 main.py:51] epoch 4371, training loss: 13835.10, average training loss: 12779.91, base loss: 21605.34
[INFO 2017-06-28 19:41:08,723 main.py:51] epoch 4372, training loss: 11434.35, average training loss: 12778.20, base loss: 21605.82
[INFO 2017-06-28 19:41:09,982 main.py:51] epoch 4373, training loss: 13228.95, average training loss: 12778.70, base loss: 21606.83
[INFO 2017-06-28 19:41:11,282 main.py:51] epoch 4374, training loss: 12950.48, average training loss: 12778.60, base loss: 21607.44
[INFO 2017-06-28 19:41:12,613 main.py:51] epoch 4375, training loss: 13094.04, average training loss: 12779.31, base loss: 21609.27
[INFO 2017-06-28 19:41:13,838 main.py:51] epoch 4376, training loss: 13190.93, average training loss: 12779.65, base loss: 21610.36
[INFO 2017-06-28 19:41:15,120 main.py:51] epoch 4377, training loss: 11724.94, average training loss: 12776.81, base loss: 21608.55
[INFO 2017-06-28 19:41:16,405 main.py:51] epoch 4378, training loss: 11911.86, average training loss: 12774.92, base loss: 21606.18
[INFO 2017-06-28 19:41:17,781 main.py:51] epoch 4379, training loss: 11787.38, average training loss: 12774.80, base loss: 21608.67
[INFO 2017-06-28 19:41:19,151 main.py:51] epoch 4380, training loss: 12964.91, average training loss: 12775.85, base loss: 21609.48
[INFO 2017-06-28 19:41:20,344 main.py:51] epoch 4381, training loss: 13633.28, average training loss: 12777.48, base loss: 21611.76
[INFO 2017-06-28 19:41:21,660 main.py:51] epoch 4382, training loss: 13330.02, average training loss: 12776.02, base loss: 21608.81
[INFO 2017-06-28 19:41:22,923 main.py:51] epoch 4383, training loss: 12303.42, average training loss: 12773.65, base loss: 21606.83
[INFO 2017-06-28 19:41:24,311 main.py:51] epoch 4384, training loss: 12831.50, average training loss: 12772.90, base loss: 21604.45
[INFO 2017-06-28 19:41:25,569 main.py:51] epoch 4385, training loss: 13031.73, average training loss: 12771.90, base loss: 21604.10
[INFO 2017-06-28 19:41:26,938 main.py:51] epoch 4386, training loss: 13828.84, average training loss: 12772.91, base loss: 21606.32
[INFO 2017-06-28 19:41:28,264 main.py:51] epoch 4387, training loss: 12107.40, average training loss: 12773.33, base loss: 21605.57
[INFO 2017-06-28 19:41:29,634 main.py:51] epoch 4388, training loss: 12571.45, average training loss: 12773.00, base loss: 21605.67
[INFO 2017-06-28 19:41:30,987 main.py:51] epoch 4389, training loss: 12675.03, average training loss: 12772.42, base loss: 21607.64
[INFO 2017-06-28 19:41:32,233 main.py:51] epoch 4390, training loss: 12566.62, average training loss: 12772.52, base loss: 21607.66
[INFO 2017-06-28 19:41:33,604 main.py:51] epoch 4391, training loss: 13231.42, average training loss: 12773.37, base loss: 21609.93
[INFO 2017-06-28 19:41:34,928 main.py:51] epoch 4392, training loss: 12766.99, average training loss: 12774.21, base loss: 21613.05
[INFO 2017-06-28 19:41:36,263 main.py:51] epoch 4393, training loss: 13084.22, average training loss: 12773.11, base loss: 21611.97
[INFO 2017-06-28 19:41:37,533 main.py:51] epoch 4394, training loss: 12667.29, average training loss: 12773.48, base loss: 21613.69
[INFO 2017-06-28 19:41:38,768 main.py:51] epoch 4395, training loss: 13887.10, average training loss: 12776.00, base loss: 21617.81
[INFO 2017-06-28 19:41:39,969 main.py:51] epoch 4396, training loss: 11200.59, average training loss: 12773.20, base loss: 21613.10
[INFO 2017-06-28 19:41:41,342 main.py:51] epoch 4397, training loss: 12692.81, average training loss: 12773.07, base loss: 21612.44
[INFO 2017-06-28 19:41:42,676 main.py:51] epoch 4398, training loss: 12690.35, average training loss: 12775.00, base loss: 21613.02
[INFO 2017-06-28 19:41:44,047 main.py:51] epoch 4399, training loss: 14380.80, average training loss: 12776.90, base loss: 21616.19
[INFO 2017-06-28 19:41:44,047 main.py:53] epoch 4399, testing
[INFO 2017-06-28 19:41:50,100 main.py:105] average testing loss: 14483.29, base loss: 22756.67
[INFO 2017-06-28 19:41:50,100 main.py:106] improve_loss: 8273.39, improve_percent: 0.36
[INFO 2017-06-28 19:41:50,100 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:41:51,396 main.py:51] epoch 4400, training loss: 13070.60, average training loss: 12777.28, base loss: 21616.45
[INFO 2017-06-28 19:41:52,711 main.py:51] epoch 4401, training loss: 12873.29, average training loss: 12777.34, base loss: 21617.52
[INFO 2017-06-28 19:41:54,105 main.py:51] epoch 4402, training loss: 12930.89, average training loss: 12777.28, base loss: 21615.35
[INFO 2017-06-28 19:41:55,399 main.py:51] epoch 4403, training loss: 13509.05, average training loss: 12778.95, base loss: 21617.82
[INFO 2017-06-28 19:41:56,734 main.py:51] epoch 4404, training loss: 12372.67, average training loss: 12777.87, base loss: 21617.68
[INFO 2017-06-28 19:41:57,975 main.py:51] epoch 4405, training loss: 11659.43, average training loss: 12776.86, base loss: 21617.06
[INFO 2017-06-28 19:41:59,292 main.py:51] epoch 4406, training loss: 13367.72, average training loss: 12778.27, base loss: 21618.23
[INFO 2017-06-28 19:42:00,639 main.py:51] epoch 4407, training loss: 13185.21, average training loss: 12777.84, base loss: 21618.12
[INFO 2017-06-28 19:42:01,981 main.py:51] epoch 4408, training loss: 13412.63, average training loss: 12779.11, base loss: 21620.98
[INFO 2017-06-28 19:42:03,258 main.py:51] epoch 4409, training loss: 12886.19, average training loss: 12778.41, base loss: 21621.29
[INFO 2017-06-28 19:42:04,471 main.py:51] epoch 4410, training loss: 12935.14, average training loss: 12779.90, base loss: 21623.74
[INFO 2017-06-28 19:42:05,855 main.py:51] epoch 4411, training loss: 12276.01, average training loss: 12779.52, base loss: 21623.64
[INFO 2017-06-28 19:42:07,089 main.py:51] epoch 4412, training loss: 12973.30, average training loss: 12778.93, base loss: 21623.78
[INFO 2017-06-28 19:42:08,343 main.py:51] epoch 4413, training loss: 12255.94, average training loss: 12777.62, base loss: 21622.62
[INFO 2017-06-28 19:42:09,652 main.py:51] epoch 4414, training loss: 12893.20, average training loss: 12777.30, base loss: 21623.25
[INFO 2017-06-28 19:42:11,067 main.py:51] epoch 4415, training loss: 11460.14, average training loss: 12774.05, base loss: 21620.42
[INFO 2017-06-28 19:42:12,321 main.py:51] epoch 4416, training loss: 13568.98, average training loss: 12775.42, base loss: 21620.80
[INFO 2017-06-28 19:42:13,657 main.py:51] epoch 4417, training loss: 12149.16, average training loss: 12775.47, base loss: 21620.26
[INFO 2017-06-28 19:42:14,979 main.py:51] epoch 4418, training loss: 12706.07, average training loss: 12775.99, base loss: 21621.86
[INFO 2017-06-28 19:42:16,406 main.py:51] epoch 4419, training loss: 12435.04, average training loss: 12773.64, base loss: 21621.09
[INFO 2017-06-28 19:42:17,757 main.py:51] epoch 4420, training loss: 13136.81, average training loss: 12773.11, base loss: 21619.82
[INFO 2017-06-28 19:42:19,094 main.py:51] epoch 4421, training loss: 12064.52, average training loss: 12771.55, base loss: 21618.85
[INFO 2017-06-28 19:42:20,530 main.py:51] epoch 4422, training loss: 12518.01, average training loss: 12772.52, base loss: 21620.85
[INFO 2017-06-28 19:42:21,830 main.py:51] epoch 4423, training loss: 12419.98, average training loss: 12771.21, base loss: 21619.81
[INFO 2017-06-28 19:42:23,204 main.py:51] epoch 4424, training loss: 13668.21, average training loss: 12771.29, base loss: 21622.47
[INFO 2017-06-28 19:42:24,424 main.py:51] epoch 4425, training loss: 12531.42, average training loss: 12770.11, base loss: 21619.60
[INFO 2017-06-28 19:42:25,898 main.py:51] epoch 4426, training loss: 12907.12, average training loss: 12770.87, base loss: 21619.53
[INFO 2017-06-28 19:42:27,251 main.py:51] epoch 4427, training loss: 12547.57, average training loss: 12771.29, base loss: 21621.27
[INFO 2017-06-28 19:42:28,581 main.py:51] epoch 4428, training loss: 12856.99, average training loss: 12771.48, base loss: 21621.89
[INFO 2017-06-28 19:42:29,800 main.py:51] epoch 4429, training loss: 13769.52, average training loss: 12772.08, base loss: 21619.85
[INFO 2017-06-28 19:42:31,012 main.py:51] epoch 4430, training loss: 13636.34, average training loss: 12773.81, base loss: 21620.87
[INFO 2017-06-28 19:42:32,351 main.py:51] epoch 4431, training loss: 12971.27, average training loss: 12773.84, base loss: 21621.12
[INFO 2017-06-28 19:42:33,654 main.py:51] epoch 4432, training loss: 12101.35, average training loss: 12772.73, base loss: 21621.21
[INFO 2017-06-28 19:42:34,960 main.py:51] epoch 4433, training loss: 12153.83, average training loss: 12772.38, base loss: 21619.48
[INFO 2017-06-28 19:42:36,219 main.py:51] epoch 4434, training loss: 13457.16, average training loss: 12772.94, base loss: 21621.18
[INFO 2017-06-28 19:42:37,559 main.py:51] epoch 4435, training loss: 12944.83, average training loss: 12772.89, base loss: 21622.74
[INFO 2017-06-28 19:42:38,924 main.py:51] epoch 4436, training loss: 13483.64, average training loss: 12773.58, base loss: 21626.35
[INFO 2017-06-28 19:42:40,237 main.py:51] epoch 4437, training loss: 13530.20, average training loss: 12775.49, base loss: 21629.28
[INFO 2017-06-28 19:42:41,602 main.py:51] epoch 4438, training loss: 11640.96, average training loss: 12774.47, base loss: 21627.16
[INFO 2017-06-28 19:42:42,899 main.py:51] epoch 4439, training loss: 13855.43, average training loss: 12776.07, base loss: 21632.06
[INFO 2017-06-28 19:42:44,342 main.py:51] epoch 4440, training loss: 11930.57, average training loss: 12776.33, base loss: 21632.85
[INFO 2017-06-28 19:42:45,633 main.py:51] epoch 4441, training loss: 13204.02, average training loss: 12777.51, base loss: 21635.66
[INFO 2017-06-28 19:42:46,967 main.py:51] epoch 4442, training loss: 13222.04, average training loss: 12776.48, base loss: 21634.95
[INFO 2017-06-28 19:42:48,279 main.py:51] epoch 4443, training loss: 12378.75, average training loss: 12776.40, base loss: 21632.57
[INFO 2017-06-28 19:42:49,675 main.py:51] epoch 4444, training loss: 13711.14, average training loss: 12775.66, base loss: 21633.49
[INFO 2017-06-28 19:42:51,038 main.py:51] epoch 4445, training loss: 12318.59, average training loss: 12773.77, base loss: 21630.82
[INFO 2017-06-28 19:42:52,290 main.py:51] epoch 4446, training loss: 13099.87, average training loss: 12772.99, base loss: 21629.33
[INFO 2017-06-28 19:42:53,577 main.py:51] epoch 4447, training loss: 12395.29, average training loss: 12773.15, base loss: 21628.27
[INFO 2017-06-28 19:42:54,900 main.py:51] epoch 4448, training loss: 11959.69, average training loss: 12773.29, base loss: 21628.76
[INFO 2017-06-28 19:42:56,227 main.py:51] epoch 4449, training loss: 12157.82, average training loss: 12772.07, base loss: 21627.03
[INFO 2017-06-28 19:42:57,764 main.py:51] epoch 4450, training loss: 11962.92, average training loss: 12771.23, base loss: 21626.46
[INFO 2017-06-28 19:42:59,101 main.py:51] epoch 4451, training loss: 12864.18, average training loss: 12772.17, base loss: 21628.62
[INFO 2017-06-28 19:43:00,378 main.py:51] epoch 4452, training loss: 13421.67, average training loss: 12772.79, base loss: 21629.68
[INFO 2017-06-28 19:43:01,649 main.py:51] epoch 4453, training loss: 13153.70, average training loss: 12773.89, base loss: 21629.70
[INFO 2017-06-28 19:43:02,956 main.py:51] epoch 4454, training loss: 12463.78, average training loss: 12772.48, base loss: 21628.30
[INFO 2017-06-28 19:43:04,332 main.py:51] epoch 4455, training loss: 12242.73, average training loss: 12771.55, base loss: 21624.45
[INFO 2017-06-28 19:43:05,640 main.py:51] epoch 4456, training loss: 13466.64, average training loss: 12770.77, base loss: 21623.31
[INFO 2017-06-28 19:43:06,979 main.py:51] epoch 4457, training loss: 13269.56, average training loss: 12770.68, base loss: 21624.05
[INFO 2017-06-28 19:43:08,298 main.py:51] epoch 4458, training loss: 14467.20, average training loss: 12773.04, base loss: 21626.98
[INFO 2017-06-28 19:43:09,654 main.py:51] epoch 4459, training loss: 12997.54, average training loss: 12774.00, base loss: 21627.79
[INFO 2017-06-28 19:43:10,955 main.py:51] epoch 4460, training loss: 12772.24, average training loss: 12774.25, base loss: 21626.25
[INFO 2017-06-28 19:43:12,277 main.py:51] epoch 4461, training loss: 13461.20, average training loss: 12773.55, base loss: 21625.70
[INFO 2017-06-28 19:43:13,575 main.py:51] epoch 4462, training loss: 12451.04, average training loss: 12774.28, base loss: 21628.97
[INFO 2017-06-28 19:43:14,908 main.py:51] epoch 4463, training loss: 12099.31, average training loss: 12774.15, base loss: 21629.87
[INFO 2017-06-28 19:43:16,097 main.py:51] epoch 4464, training loss: 12529.45, average training loss: 12774.53, base loss: 21628.99
[INFO 2017-06-28 19:43:17,244 main.py:51] epoch 4465, training loss: 12556.73, average training loss: 12774.91, base loss: 21629.08
[INFO 2017-06-28 19:43:18,538 main.py:51] epoch 4466, training loss: 13485.76, average training loss: 12775.75, base loss: 21632.51
[INFO 2017-06-28 19:43:19,873 main.py:51] epoch 4467, training loss: 12926.03, average training loss: 12776.50, base loss: 21632.88
[INFO 2017-06-28 19:43:21,056 main.py:51] epoch 4468, training loss: 12638.57, average training loss: 12777.92, base loss: 21635.34
[INFO 2017-06-28 19:43:22,332 main.py:51] epoch 4469, training loss: 12969.99, average training loss: 12778.75, base loss: 21634.89
[INFO 2017-06-28 19:43:23,730 main.py:51] epoch 4470, training loss: 12862.99, average training loss: 12779.09, base loss: 21636.44
[INFO 2017-06-28 19:43:25,110 main.py:51] epoch 4471, training loss: 12311.97, average training loss: 12779.18, base loss: 21637.59
[INFO 2017-06-28 19:43:26,412 main.py:51] epoch 4472, training loss: 13659.88, average training loss: 12779.93, base loss: 21640.70
[INFO 2017-06-28 19:43:27,611 main.py:51] epoch 4473, training loss: 12853.28, average training loss: 12781.28, base loss: 21641.06
[INFO 2017-06-28 19:43:28,929 main.py:51] epoch 4474, training loss: 11834.06, average training loss: 12781.29, base loss: 21640.89
[INFO 2017-06-28 19:43:30,338 main.py:51] epoch 4475, training loss: 12982.94, average training loss: 12782.07, base loss: 21640.93
[INFO 2017-06-28 19:43:31,591 main.py:51] epoch 4476, training loss: 11540.19, average training loss: 12780.91, base loss: 21638.84
[INFO 2017-06-28 19:43:33,014 main.py:51] epoch 4477, training loss: 12053.01, average training loss: 12781.18, base loss: 21637.81
[INFO 2017-06-28 19:43:34,249 main.py:51] epoch 4478, training loss: 12673.30, average training loss: 12780.97, base loss: 21637.87
[INFO 2017-06-28 19:43:35,547 main.py:51] epoch 4479, training loss: 14208.68, average training loss: 12782.03, base loss: 21638.89
[INFO 2017-06-28 19:43:36,893 main.py:51] epoch 4480, training loss: 12272.82, average training loss: 12782.50, base loss: 21639.19
[INFO 2017-06-28 19:43:38,232 main.py:51] epoch 4481, training loss: 14640.15, average training loss: 12783.27, base loss: 21639.83
[INFO 2017-06-28 19:43:39,505 main.py:51] epoch 4482, training loss: 12504.36, average training loss: 12783.15, base loss: 21641.19
[INFO 2017-06-28 19:43:40,763 main.py:51] epoch 4483, training loss: 12859.09, average training loss: 12783.48, base loss: 21640.96
[INFO 2017-06-28 19:43:42,038 main.py:51] epoch 4484, training loss: 14277.18, average training loss: 12786.24, base loss: 21646.71
[INFO 2017-06-28 19:43:43,319 main.py:51] epoch 4485, training loss: 11401.32, average training loss: 12783.99, base loss: 21644.41
[INFO 2017-06-28 19:43:44,691 main.py:51] epoch 4486, training loss: 15306.96, average training loss: 12786.87, base loss: 21648.82
[INFO 2017-06-28 19:43:46,102 main.py:51] epoch 4487, training loss: 11936.80, average training loss: 12786.76, base loss: 21649.31
[INFO 2017-06-28 19:43:47,405 main.py:51] epoch 4488, training loss: 12191.64, average training loss: 12785.76, base loss: 21649.05
[INFO 2017-06-28 19:43:48,930 main.py:51] epoch 4489, training loss: 15615.63, average training loss: 12789.35, base loss: 21652.00
[INFO 2017-06-28 19:43:50,278 main.py:51] epoch 4490, training loss: 12427.57, average training loss: 12787.57, base loss: 21649.99
[INFO 2017-06-28 19:43:51,568 main.py:51] epoch 4491, training loss: 11478.37, average training loss: 12786.39, base loss: 21648.01
[INFO 2017-06-28 19:43:52,815 main.py:51] epoch 4492, training loss: 13533.79, average training loss: 12786.78, base loss: 21649.13
[INFO 2017-06-28 19:43:54,067 main.py:51] epoch 4493, training loss: 11586.37, average training loss: 12786.80, base loss: 21650.94
[INFO 2017-06-28 19:43:55,268 main.py:51] epoch 4494, training loss: 13284.97, average training loss: 12786.80, base loss: 21651.46
[INFO 2017-06-28 19:43:56,611 main.py:51] epoch 4495, training loss: 11813.18, average training loss: 12785.50, base loss: 21651.56
[INFO 2017-06-28 19:43:57,863 main.py:51] epoch 4496, training loss: 13723.51, average training loss: 12785.11, base loss: 21650.40
[INFO 2017-06-28 19:43:59,189 main.py:51] epoch 4497, training loss: 13499.02, average training loss: 12783.76, base loss: 21650.62
[INFO 2017-06-28 19:44:00,512 main.py:51] epoch 4498, training loss: 13099.73, average training loss: 12784.22, base loss: 21652.34
[INFO 2017-06-28 19:44:01,839 main.py:51] epoch 4499, training loss: 12054.94, average training loss: 12784.85, base loss: 21652.56
[INFO 2017-06-28 19:44:01,839 main.py:53] epoch 4499, testing
[INFO 2017-06-28 19:44:07,992 main.py:105] average testing loss: 13813.82, base loss: 22087.90
[INFO 2017-06-28 19:44:07,992 main.py:106] improve_loss: 8274.08, improve_percent: 0.37
[INFO 2017-06-28 19:44:07,993 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:44:09,263 main.py:51] epoch 4500, training loss: 12764.00, average training loss: 12784.59, base loss: 21652.25
[INFO 2017-06-28 19:44:10,446 main.py:51] epoch 4501, training loss: 11444.75, average training loss: 12783.11, base loss: 21650.48
[INFO 2017-06-28 19:44:11,731 main.py:51] epoch 4502, training loss: 13318.19, average training loss: 12782.87, base loss: 21648.25
[INFO 2017-06-28 19:44:13,026 main.py:51] epoch 4503, training loss: 12634.85, average training loss: 12784.49, base loss: 21651.75
[INFO 2017-06-28 19:44:14,309 main.py:51] epoch 4504, training loss: 11593.57, average training loss: 12782.39, base loss: 21649.14
[INFO 2017-06-28 19:44:15,653 main.py:51] epoch 4505, training loss: 12002.30, average training loss: 12781.87, base loss: 21648.24
[INFO 2017-06-28 19:44:17,080 main.py:51] epoch 4506, training loss: 14180.59, average training loss: 12782.53, base loss: 21652.46
[INFO 2017-06-28 19:44:18,372 main.py:51] epoch 4507, training loss: 12492.22, average training loss: 12782.22, base loss: 21651.05
[INFO 2017-06-28 19:44:19,663 main.py:51] epoch 4508, training loss: 11550.37, average training loss: 12780.10, base loss: 21647.44
[INFO 2017-06-28 19:44:21,015 main.py:51] epoch 4509, training loss: 13013.68, average training loss: 12779.73, base loss: 21648.75
[INFO 2017-06-28 19:44:22,518 main.py:51] epoch 4510, training loss: 13621.52, average training loss: 12781.50, base loss: 21652.35
[INFO 2017-06-28 19:44:23,828 main.py:51] epoch 4511, training loss: 11755.76, average training loss: 12780.99, base loss: 21651.55
[INFO 2017-06-28 19:44:25,230 main.py:51] epoch 4512, training loss: 11512.14, average training loss: 12780.76, base loss: 21651.58
[INFO 2017-06-28 19:44:26,768 main.py:51] epoch 4513, training loss: 13070.66, average training loss: 12781.34, base loss: 21650.14
[INFO 2017-06-28 19:44:27,977 main.py:51] epoch 4514, training loss: 12578.38, average training loss: 12780.95, base loss: 21649.85
[INFO 2017-06-28 19:44:29,438 main.py:51] epoch 4515, training loss: 12781.81, average training loss: 12779.54, base loss: 21649.59
[INFO 2017-06-28 19:44:30,825 main.py:51] epoch 4516, training loss: 12904.38, average training loss: 12780.35, base loss: 21652.93
[INFO 2017-06-28 19:44:32,202 main.py:51] epoch 4517, training loss: 14462.48, average training loss: 12782.43, base loss: 21656.10
[INFO 2017-06-28 19:44:33,510 main.py:51] epoch 4518, training loss: 12009.93, average training loss: 12780.95, base loss: 21654.57
[INFO 2017-06-28 19:44:34,999 main.py:51] epoch 4519, training loss: 12754.97, average training loss: 12781.92, base loss: 21655.02
[INFO 2017-06-28 19:44:36,375 main.py:51] epoch 4520, training loss: 11372.35, average training loss: 12778.69, base loss: 21652.25
[INFO 2017-06-28 19:44:37,690 main.py:51] epoch 4521, training loss: 12347.25, average training loss: 12778.01, base loss: 21651.99
[INFO 2017-06-28 19:44:39,019 main.py:51] epoch 4522, training loss: 13577.79, average training loss: 12778.68, base loss: 21653.86
[INFO 2017-06-28 19:44:40,279 main.py:51] epoch 4523, training loss: 13335.91, average training loss: 12780.74, base loss: 21654.49
[INFO 2017-06-28 19:44:41,581 main.py:51] epoch 4524, training loss: 12377.53, average training loss: 12779.61, base loss: 21651.75
[INFO 2017-06-28 19:44:42,884 main.py:51] epoch 4525, training loss: 12974.83, average training loss: 12779.60, base loss: 21652.62
[INFO 2017-06-28 19:44:44,220 main.py:51] epoch 4526, training loss: 12562.90, average training loss: 12778.19, base loss: 21650.43
[INFO 2017-06-28 19:44:45,526 main.py:51] epoch 4527, training loss: 11992.00, average training loss: 12778.01, base loss: 21651.87
[INFO 2017-06-28 19:44:46,864 main.py:51] epoch 4528, training loss: 14345.57, average training loss: 12779.71, base loss: 21655.14
[INFO 2017-06-28 19:44:48,137 main.py:51] epoch 4529, training loss: 12017.28, average training loss: 12778.44, base loss: 21653.79
[INFO 2017-06-28 19:44:49,648 main.py:51] epoch 4530, training loss: 12019.23, average training loss: 12777.00, base loss: 21651.95
[INFO 2017-06-28 19:44:50,969 main.py:51] epoch 4531, training loss: 13024.30, average training loss: 12777.11, base loss: 21652.40
[INFO 2017-06-28 19:44:52,430 main.py:51] epoch 4532, training loss: 12157.67, average training loss: 12775.69, base loss: 21649.70
[INFO 2017-06-28 19:44:53,819 main.py:51] epoch 4533, training loss: 13467.93, average training loss: 12777.12, base loss: 21652.25
[INFO 2017-06-28 19:44:55,091 main.py:51] epoch 4534, training loss: 12666.15, average training loss: 12776.63, base loss: 21652.96
[INFO 2017-06-28 19:44:56,326 main.py:51] epoch 4535, training loss: 12575.11, average training loss: 12776.58, base loss: 21654.75
[INFO 2017-06-28 19:44:57,722 main.py:51] epoch 4536, training loss: 13861.18, average training loss: 12778.48, base loss: 21657.70
[INFO 2017-06-28 19:44:59,096 main.py:51] epoch 4537, training loss: 12327.00, average training loss: 12778.99, base loss: 21659.98
[INFO 2017-06-28 19:45:00,444 main.py:51] epoch 4538, training loss: 12752.93, average training loss: 12778.94, base loss: 21658.89
[INFO 2017-06-28 19:45:01,762 main.py:51] epoch 4539, training loss: 13513.58, average training loss: 12779.83, base loss: 21660.31
[INFO 2017-06-28 19:45:03,078 main.py:51] epoch 4540, training loss: 13034.06, average training loss: 12779.80, base loss: 21660.50
[INFO 2017-06-28 19:45:04,335 main.py:51] epoch 4541, training loss: 11380.41, average training loss: 12778.66, base loss: 21659.51
[INFO 2017-06-28 19:45:05,567 main.py:51] epoch 4542, training loss: 13365.97, average training loss: 12779.82, base loss: 21659.87
[INFO 2017-06-28 19:45:07,029 main.py:51] epoch 4543, training loss: 12125.58, average training loss: 12780.43, base loss: 21660.75
[INFO 2017-06-28 19:45:08,426 main.py:51] epoch 4544, training loss: 12984.72, average training loss: 12780.50, base loss: 21662.60
[INFO 2017-06-28 19:45:09,838 main.py:51] epoch 4545, training loss: 12251.47, average training loss: 12780.02, base loss: 21662.82
[INFO 2017-06-28 19:45:11,212 main.py:51] epoch 4546, training loss: 14096.21, average training loss: 12781.11, base loss: 21663.27
[INFO 2017-06-28 19:45:12,481 main.py:51] epoch 4547, training loss: 13094.91, average training loss: 12780.70, base loss: 21664.10
[INFO 2017-06-28 19:45:13,746 main.py:51] epoch 4548, training loss: 12828.91, average training loss: 12781.05, base loss: 21664.11
[INFO 2017-06-28 19:45:15,013 main.py:51] epoch 4549, training loss: 11182.37, average training loss: 12779.79, base loss: 21663.06
[INFO 2017-06-28 19:45:16,341 main.py:51] epoch 4550, training loss: 13823.87, average training loss: 12780.88, base loss: 21664.79
[INFO 2017-06-28 19:45:17,655 main.py:51] epoch 4551, training loss: 12710.40, average training loss: 12781.08, base loss: 21666.55
[INFO 2017-06-28 19:45:18,960 main.py:51] epoch 4552, training loss: 12508.26, average training loss: 12781.64, base loss: 21667.41
[INFO 2017-06-28 19:45:20,411 main.py:51] epoch 4553, training loss: 12962.36, average training loss: 12782.05, base loss: 21667.01
[INFO 2017-06-28 19:45:21,919 main.py:51] epoch 4554, training loss: 13354.32, average training loss: 12782.96, base loss: 21667.90
[INFO 2017-06-28 19:45:23,224 main.py:51] epoch 4555, training loss: 13381.31, average training loss: 12784.19, base loss: 21668.75
[INFO 2017-06-28 19:45:24,563 main.py:51] epoch 4556, training loss: 11685.60, average training loss: 12783.40, base loss: 21667.53
[INFO 2017-06-28 19:45:25,851 main.py:51] epoch 4557, training loss: 12777.45, average training loss: 12783.13, base loss: 21669.01
[INFO 2017-06-28 19:45:27,117 main.py:51] epoch 4558, training loss: 11667.98, average training loss: 12781.62, base loss: 21665.65
[INFO 2017-06-28 19:45:28,353 main.py:51] epoch 4559, training loss: 12863.84, average training loss: 12781.11, base loss: 21664.35
[INFO 2017-06-28 19:45:29,804 main.py:51] epoch 4560, training loss: 13378.63, average training loss: 12779.68, base loss: 21662.53
[INFO 2017-06-28 19:45:31,104 main.py:51] epoch 4561, training loss: 11466.49, average training loss: 12777.06, base loss: 21660.84
[INFO 2017-06-28 19:45:32,413 main.py:51] epoch 4562, training loss: 14129.86, average training loss: 12778.49, base loss: 21663.12
[INFO 2017-06-28 19:45:33,721 main.py:51] epoch 4563, training loss: 10874.03, average training loss: 12777.01, base loss: 21661.74
[INFO 2017-06-28 19:45:34,918 main.py:51] epoch 4564, training loss: 11898.29, average training loss: 12776.97, base loss: 21661.79
[INFO 2017-06-28 19:45:36,163 main.py:51] epoch 4565, training loss: 12743.84, average training loss: 12776.15, base loss: 21659.96
[INFO 2017-06-28 19:45:37,509 main.py:51] epoch 4566, training loss: 11258.34, average training loss: 12774.68, base loss: 21657.05
[INFO 2017-06-28 19:45:39,018 main.py:51] epoch 4567, training loss: 12749.98, average training loss: 12775.04, base loss: 21656.10
[INFO 2017-06-28 19:45:40,397 main.py:51] epoch 4568, training loss: 13088.47, average training loss: 12776.91, base loss: 21660.23
[INFO 2017-06-28 19:45:41,755 main.py:51] epoch 4569, training loss: 12328.11, average training loss: 12776.89, base loss: 21660.43
[INFO 2017-06-28 19:45:43,071 main.py:51] epoch 4570, training loss: 13140.70, average training loss: 12775.50, base loss: 21659.87
[INFO 2017-06-28 19:45:44,373 main.py:51] epoch 4571, training loss: 12330.03, average training loss: 12775.06, base loss: 21658.50
[INFO 2017-06-28 19:45:45,610 main.py:51] epoch 4572, training loss: 13814.05, average training loss: 12776.85, base loss: 21660.87
[INFO 2017-06-28 19:45:46,956 main.py:51] epoch 4573, training loss: 12490.86, average training loss: 12775.71, base loss: 21657.46
[INFO 2017-06-28 19:45:48,319 main.py:51] epoch 4574, training loss: 12557.80, average training loss: 12774.29, base loss: 21657.30
[INFO 2017-06-28 19:45:49,509 main.py:51] epoch 4575, training loss: 12085.90, average training loss: 12772.47, base loss: 21654.38
[INFO 2017-06-28 19:45:50,834 main.py:51] epoch 4576, training loss: 12095.19, average training loss: 12771.50, base loss: 21652.90
[INFO 2017-06-28 19:45:52,192 main.py:51] epoch 4577, training loss: 12546.68, average training loss: 12769.79, base loss: 21651.09
[INFO 2017-06-28 19:45:53,455 main.py:51] epoch 4578, training loss: 13477.95, average training loss: 12770.54, base loss: 21653.26
[INFO 2017-06-28 19:45:54,861 main.py:51] epoch 4579, training loss: 12179.01, average training loss: 12769.18, base loss: 21652.83
[INFO 2017-06-28 19:45:56,182 main.py:51] epoch 4580, training loss: 12399.47, average training loss: 12769.64, base loss: 21654.51
[INFO 2017-06-28 19:45:57,592 main.py:51] epoch 4581, training loss: 12774.63, average training loss: 12769.85, base loss: 21655.21
[INFO 2017-06-28 19:45:58,851 main.py:51] epoch 4582, training loss: 12017.85, average training loss: 12768.57, base loss: 21653.24
[INFO 2017-06-28 19:46:00,302 main.py:51] epoch 4583, training loss: 12800.42, average training loss: 12768.85, base loss: 21654.44
[INFO 2017-06-28 19:46:01,731 main.py:51] epoch 4584, training loss: 13324.48, average training loss: 12768.80, base loss: 21655.24
[INFO 2017-06-28 19:46:03,040 main.py:51] epoch 4585, training loss: 13036.02, average training loss: 12768.67, base loss: 21654.51
[INFO 2017-06-28 19:46:04,439 main.py:51] epoch 4586, training loss: 11891.74, average training loss: 12769.21, base loss: 21655.03
[INFO 2017-06-28 19:46:05,776 main.py:51] epoch 4587, training loss: 11533.97, average training loss: 12769.37, base loss: 21654.55
[INFO 2017-06-28 19:46:07,083 main.py:51] epoch 4588, training loss: 12383.25, average training loss: 12768.81, base loss: 21650.41
[INFO 2017-06-28 19:46:08,486 main.py:51] epoch 4589, training loss: 13255.21, average training loss: 12769.28, base loss: 21652.71
[INFO 2017-06-28 19:46:09,900 main.py:51] epoch 4590, training loss: 13217.21, average training loss: 12769.10, base loss: 21654.24
[INFO 2017-06-28 19:46:11,244 main.py:51] epoch 4591, training loss: 12221.13, average training loss: 12767.96, base loss: 21654.34
[INFO 2017-06-28 19:46:12,582 main.py:51] epoch 4592, training loss: 12374.33, average training loss: 12767.04, base loss: 21652.29
[INFO 2017-06-28 19:46:13,926 main.py:51] epoch 4593, training loss: 12161.40, average training loss: 12766.84, base loss: 21649.64
[INFO 2017-06-28 19:46:15,329 main.py:51] epoch 4594, training loss: 12411.33, average training loss: 12766.07, base loss: 21649.41
[INFO 2017-06-28 19:46:16,771 main.py:51] epoch 4595, training loss: 11423.95, average training loss: 12766.54, base loss: 21648.38
[INFO 2017-06-28 19:46:18,239 main.py:51] epoch 4596, training loss: 13116.60, average training loss: 12765.81, base loss: 21647.16
[INFO 2017-06-28 19:46:19,604 main.py:51] epoch 4597, training loss: 11664.33, average training loss: 12765.66, base loss: 21646.95
[INFO 2017-06-28 19:46:21,098 main.py:51] epoch 4598, training loss: 13428.21, average training loss: 12764.10, base loss: 21643.92
[INFO 2017-06-28 19:46:22,483 main.py:51] epoch 4599, training loss: 13201.08, average training loss: 12763.64, base loss: 21642.46
[INFO 2017-06-28 19:46:22,483 main.py:53] epoch 4599, testing
[INFO 2017-06-28 19:46:28,339 main.py:105] average testing loss: 13848.21, base loss: 22213.95
[INFO 2017-06-28 19:46:28,339 main.py:106] improve_loss: 8365.74, improve_percent: 0.38
[INFO 2017-06-28 19:46:28,340 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:46:29,756 main.py:51] epoch 4600, training loss: 11771.31, average training loss: 12761.64, base loss: 21641.31
[INFO 2017-06-28 19:46:31,317 main.py:51] epoch 4601, training loss: 12044.70, average training loss: 12760.86, base loss: 21639.77
[INFO 2017-06-28 19:46:32,717 main.py:51] epoch 4602, training loss: 13256.33, average training loss: 12761.41, base loss: 21640.95
[INFO 2017-06-28 19:46:34,242 main.py:51] epoch 4603, training loss: 12777.56, average training loss: 12760.19, base loss: 21638.54
[INFO 2017-06-28 19:46:35,550 main.py:51] epoch 4604, training loss: 12384.74, average training loss: 12759.96, base loss: 21637.57
[INFO 2017-06-28 19:46:36,975 main.py:51] epoch 4605, training loss: 13648.26, average training loss: 12760.82, base loss: 21640.06
[INFO 2017-06-28 19:46:38,311 main.py:51] epoch 4606, training loss: 13145.37, average training loss: 12761.04, base loss: 21639.77
[INFO 2017-06-28 19:46:39,560 main.py:51] epoch 4607, training loss: 12039.03, average training loss: 12757.70, base loss: 21635.81
[INFO 2017-06-28 19:46:40,889 main.py:51] epoch 4608, training loss: 12461.82, average training loss: 12757.77, base loss: 21635.48
[INFO 2017-06-28 19:46:42,107 main.py:51] epoch 4609, training loss: 11917.51, average training loss: 12758.56, base loss: 21635.34
[INFO 2017-06-28 19:46:43,430 main.py:51] epoch 4610, training loss: 12695.18, average training loss: 12758.50, base loss: 21635.99
[INFO 2017-06-28 19:46:44,723 main.py:51] epoch 4611, training loss: 11738.30, average training loss: 12757.13, base loss: 21633.67
[INFO 2017-06-28 19:46:46,088 main.py:51] epoch 4612, training loss: 12550.58, average training loss: 12757.20, base loss: 21634.62
[INFO 2017-06-28 19:46:47,333 main.py:51] epoch 4613, training loss: 12388.87, average training loss: 12754.95, base loss: 21633.23
[INFO 2017-06-28 19:46:48,688 main.py:51] epoch 4614, training loss: 11853.99, average training loss: 12754.87, base loss: 21634.23
[INFO 2017-06-28 19:46:49,933 main.py:51] epoch 4615, training loss: 12308.13, average training loss: 12752.95, base loss: 21633.12
[INFO 2017-06-28 19:46:51,346 main.py:51] epoch 4616, training loss: 14222.85, average training loss: 12754.00, base loss: 21633.00
[INFO 2017-06-28 19:46:52,806 main.py:51] epoch 4617, training loss: 12787.60, average training loss: 12753.72, base loss: 21633.51
[INFO 2017-06-28 19:46:54,284 main.py:51] epoch 4618, training loss: 12795.32, average training loss: 12753.97, base loss: 21634.05
[INFO 2017-06-28 19:46:55,622 main.py:51] epoch 4619, training loss: 11847.10, average training loss: 12753.21, base loss: 21633.57
[INFO 2017-06-28 19:46:56,942 main.py:51] epoch 4620, training loss: 12230.28, average training loss: 12752.33, base loss: 21632.00
[INFO 2017-06-28 19:46:58,412 main.py:51] epoch 4621, training loss: 12650.39, average training loss: 12752.49, base loss: 21632.44
[INFO 2017-06-28 19:46:59,702 main.py:51] epoch 4622, training loss: 13239.44, average training loss: 12752.64, base loss: 21631.64
[INFO 2017-06-28 19:47:01,029 main.py:51] epoch 4623, training loss: 11472.98, average training loss: 12750.81, base loss: 21629.31
[INFO 2017-06-28 19:47:02,355 main.py:51] epoch 4624, training loss: 13230.93, average training loss: 12751.85, base loss: 21630.59
[INFO 2017-06-28 19:47:03,769 main.py:51] epoch 4625, training loss: 12808.95, average training loss: 12750.14, base loss: 21629.55
[INFO 2017-06-28 19:47:05,115 main.py:51] epoch 4626, training loss: 12116.93, average training loss: 12749.53, base loss: 21628.55
[INFO 2017-06-28 19:47:06,432 main.py:51] epoch 4627, training loss: 12285.32, average training loss: 12749.37, base loss: 21628.58
[INFO 2017-06-28 19:47:07,865 main.py:51] epoch 4628, training loss: 11781.84, average training loss: 12748.11, base loss: 21630.24
[INFO 2017-06-28 19:47:09,210 main.py:51] epoch 4629, training loss: 13730.85, average training loss: 12748.91, base loss: 21631.10
[INFO 2017-06-28 19:47:10,604 main.py:51] epoch 4630, training loss: 13165.74, average training loss: 12748.95, base loss: 21630.04
[INFO 2017-06-28 19:47:11,941 main.py:51] epoch 4631, training loss: 12148.40, average training loss: 12749.11, base loss: 21632.22
[INFO 2017-06-28 19:47:13,342 main.py:51] epoch 4632, training loss: 12227.99, average training loss: 12749.16, base loss: 21632.52
[INFO 2017-06-28 19:47:14,934 main.py:51] epoch 4633, training loss: 13047.88, average training loss: 12749.09, base loss: 21630.20
[INFO 2017-06-28 19:47:16,355 main.py:51] epoch 4634, training loss: 12204.23, average training loss: 12748.43, base loss: 21632.68
[INFO 2017-06-28 19:47:17,752 main.py:51] epoch 4635, training loss: 11272.53, average training loss: 12747.20, base loss: 21630.26
[INFO 2017-06-28 19:47:19,069 main.py:51] epoch 4636, training loss: 13222.41, average training loss: 12747.89, base loss: 21629.66
[INFO 2017-06-28 19:47:20,578 main.py:51] epoch 4637, training loss: 13706.40, average training loss: 12748.95, base loss: 21631.43
[INFO 2017-06-28 19:47:21,989 main.py:51] epoch 4638, training loss: 14710.20, average training loss: 12750.88, base loss: 21633.12
[INFO 2017-06-28 19:47:23,279 main.py:51] epoch 4639, training loss: 11935.17, average training loss: 12749.10, base loss: 21628.45
[INFO 2017-06-28 19:47:24,641 main.py:51] epoch 4640, training loss: 12979.94, average training loss: 12749.24, base loss: 21630.34
[INFO 2017-06-28 19:47:25,999 main.py:51] epoch 4641, training loss: 11902.77, average training loss: 12748.55, base loss: 21629.87
[INFO 2017-06-28 19:47:27,291 main.py:51] epoch 4642, training loss: 12834.72, average training loss: 12746.82, base loss: 21629.74
[INFO 2017-06-28 19:47:28,726 main.py:51] epoch 4643, training loss: 12489.33, average training loss: 12747.52, base loss: 21633.12
[INFO 2017-06-28 19:47:30,248 main.py:51] epoch 4644, training loss: 11607.08, average training loss: 12746.18, base loss: 21629.80
[INFO 2017-06-28 19:47:31,582 main.py:51] epoch 4645, training loss: 13348.79, average training loss: 12746.98, base loss: 21632.00
[INFO 2017-06-28 19:47:32,806 main.py:51] epoch 4646, training loss: 10991.60, average training loss: 12745.86, base loss: 21631.05
[INFO 2017-06-28 19:47:34,011 main.py:51] epoch 4647, training loss: 12783.97, average training loss: 12747.63, base loss: 21633.03
[INFO 2017-06-28 19:47:35,277 main.py:51] epoch 4648, training loss: 13626.91, average training loss: 12748.88, base loss: 21634.99
[INFO 2017-06-28 19:47:36,751 main.py:51] epoch 4649, training loss: 12503.19, average training loss: 12748.89, base loss: 21634.37
[INFO 2017-06-28 19:47:38,190 main.py:51] epoch 4650, training loss: 12143.36, average training loss: 12747.22, base loss: 21633.52
[INFO 2017-06-28 19:47:39,469 main.py:51] epoch 4651, training loss: 12652.41, average training loss: 12746.35, base loss: 21632.64
[INFO 2017-06-28 19:47:40,750 main.py:51] epoch 4652, training loss: 13798.40, average training loss: 12748.24, base loss: 21633.58
[INFO 2017-06-28 19:47:42,196 main.py:51] epoch 4653, training loss: 12880.16, average training loss: 12749.50, base loss: 21636.03
[INFO 2017-06-28 19:47:43,744 main.py:51] epoch 4654, training loss: 12537.87, average training loss: 12750.29, base loss: 21636.97
[INFO 2017-06-28 19:47:45,067 main.py:51] epoch 4655, training loss: 11931.52, average training loss: 12750.85, base loss: 21638.45
[INFO 2017-06-28 19:47:46,631 main.py:51] epoch 4656, training loss: 13302.52, average training loss: 12752.47, base loss: 21639.98
[INFO 2017-06-28 19:47:47,930 main.py:51] epoch 4657, training loss: 13340.44, average training loss: 12753.13, base loss: 21637.26
[INFO 2017-06-28 19:47:49,283 main.py:51] epoch 4658, training loss: 13921.19, average training loss: 12753.13, base loss: 21638.18
[INFO 2017-06-28 19:47:50,582 main.py:51] epoch 4659, training loss: 12336.56, average training loss: 12752.43, base loss: 21638.26
[INFO 2017-06-28 19:47:52,014 main.py:51] epoch 4660, training loss: 13196.29, average training loss: 12751.44, base loss: 21636.92
[INFO 2017-06-28 19:47:53,441 main.py:51] epoch 4661, training loss: 12825.14, average training loss: 12751.49, base loss: 21637.99
[INFO 2017-06-28 19:47:54,886 main.py:51] epoch 4662, training loss: 13575.75, average training loss: 12753.09, base loss: 21638.15
[INFO 2017-06-28 19:47:56,164 main.py:51] epoch 4663, training loss: 12181.73, average training loss: 12753.36, base loss: 21638.64
[INFO 2017-06-28 19:47:57,386 main.py:51] epoch 4664, training loss: 12584.30, average training loss: 12754.05, base loss: 21640.25
[INFO 2017-06-28 19:47:58,761 main.py:51] epoch 4665, training loss: 11326.13, average training loss: 12752.87, base loss: 21637.92
[INFO 2017-06-28 19:48:00,141 main.py:51] epoch 4666, training loss: 13489.29, average training loss: 12753.41, base loss: 21637.34
[INFO 2017-06-28 19:48:01,677 main.py:51] epoch 4667, training loss: 12457.78, average training loss: 12753.61, base loss: 21636.53
[INFO 2017-06-28 19:48:02,942 main.py:51] epoch 4668, training loss: 13678.41, average training loss: 12753.89, base loss: 21638.64
[INFO 2017-06-28 19:48:04,208 main.py:51] epoch 4669, training loss: 12971.61, average training loss: 12753.87, base loss: 21640.63
[INFO 2017-06-28 19:48:05,513 main.py:51] epoch 4670, training loss: 13941.15, average training loss: 12756.42, base loss: 21645.95
[INFO 2017-06-28 19:48:06,792 main.py:51] epoch 4671, training loss: 11839.64, average training loss: 12754.03, base loss: 21643.04
[INFO 2017-06-28 19:48:08,098 main.py:51] epoch 4672, training loss: 13248.26, average training loss: 12755.01, base loss: 21643.28
[INFO 2017-06-28 19:48:09,487 main.py:51] epoch 4673, training loss: 12269.82, average training loss: 12754.62, base loss: 21643.09
[INFO 2017-06-28 19:48:10,858 main.py:51] epoch 4674, training loss: 13028.42, average training loss: 12754.94, base loss: 21644.49
[INFO 2017-06-28 19:48:12,347 main.py:51] epoch 4675, training loss: 14406.02, average training loss: 12756.78, base loss: 21647.08
[INFO 2017-06-28 19:48:13,753 main.py:51] epoch 4676, training loss: 12171.67, average training loss: 12757.21, base loss: 21648.03
[INFO 2017-06-28 19:48:15,125 main.py:51] epoch 4677, training loss: 12424.38, average training loss: 12756.18, base loss: 21648.49
[INFO 2017-06-28 19:48:16,360 main.py:51] epoch 4678, training loss: 12907.71, average training loss: 12756.77, base loss: 21651.20
[INFO 2017-06-28 19:48:17,796 main.py:51] epoch 4679, training loss: 11960.97, average training loss: 12755.85, base loss: 21650.30
[INFO 2017-06-28 19:48:19,341 main.py:51] epoch 4680, training loss: 13592.26, average training loss: 12756.57, base loss: 21652.86
[INFO 2017-06-28 19:48:20,921 main.py:51] epoch 4681, training loss: 13406.39, average training loss: 12757.16, base loss: 21654.36
[INFO 2017-06-28 19:48:22,240 main.py:51] epoch 4682, training loss: 11851.75, average training loss: 12754.66, base loss: 21652.53
[INFO 2017-06-28 19:48:23,701 main.py:51] epoch 4683, training loss: 12443.97, average training loss: 12754.55, base loss: 21654.98
[INFO 2017-06-28 19:48:25,136 main.py:51] epoch 4684, training loss: 12953.09, average training loss: 12754.38, base loss: 21656.11
[INFO 2017-06-28 19:48:26,492 main.py:51] epoch 4685, training loss: 13977.14, average training loss: 12755.92, base loss: 21655.53
[INFO 2017-06-28 19:48:27,975 main.py:51] epoch 4686, training loss: 12776.10, average training loss: 12755.38, base loss: 21656.76
[INFO 2017-06-28 19:48:29,267 main.py:51] epoch 4687, training loss: 12322.51, average training loss: 12753.28, base loss: 21654.12
[INFO 2017-06-28 19:48:30,633 main.py:51] epoch 4688, training loss: 13527.25, average training loss: 12754.44, base loss: 21654.70
[INFO 2017-06-28 19:48:32,081 main.py:51] epoch 4689, training loss: 13088.26, average training loss: 12755.35, base loss: 21656.46
[INFO 2017-06-28 19:48:33,478 main.py:51] epoch 4690, training loss: 13423.84, average training loss: 12755.92, base loss: 21656.67
[INFO 2017-06-28 19:48:34,907 main.py:51] epoch 4691, training loss: 12942.50, average training loss: 12756.81, base loss: 21656.00
[INFO 2017-06-28 19:48:36,315 main.py:51] epoch 4692, training loss: 13070.47, average training loss: 12757.53, base loss: 21657.20
[INFO 2017-06-28 19:48:37,609 main.py:51] epoch 4693, training loss: 12471.96, average training loss: 12756.28, base loss: 21655.44
[INFO 2017-06-28 19:48:38,848 main.py:51] epoch 4694, training loss: 11269.83, average training loss: 12753.70, base loss: 21650.46
[INFO 2017-06-28 19:48:40,337 main.py:51] epoch 4695, training loss: 14791.28, average training loss: 12754.68, base loss: 21653.42
[INFO 2017-06-28 19:48:41,775 main.py:51] epoch 4696, training loss: 13399.10, average training loss: 12756.22, base loss: 21655.47
[INFO 2017-06-28 19:48:43,211 main.py:51] epoch 4697, training loss: 12668.83, average training loss: 12755.85, base loss: 21657.84
[INFO 2017-06-28 19:48:44,613 main.py:51] epoch 4698, training loss: 12867.79, average training loss: 12755.51, base loss: 21658.33
[INFO 2017-06-28 19:48:46,181 main.py:51] epoch 4699, training loss: 13009.10, average training loss: 12755.09, base loss: 21659.55
[INFO 2017-06-28 19:48:46,181 main.py:53] epoch 4699, testing
[INFO 2017-06-28 19:48:52,693 main.py:105] average testing loss: 13388.73, base loss: 21248.81
[INFO 2017-06-28 19:48:52,693 main.py:106] improve_loss: 7860.08, improve_percent: 0.37
[INFO 2017-06-28 19:48:52,694 main.py:76] current best improved percent: 0.38
[INFO 2017-06-28 19:48:54,083 main.py:51] epoch 4700, training loss: 10839.18, average training loss: 12752.56, base loss: 21656.72
[INFO 2017-06-28 19:48:55,323 main.py:51] epoch 4701, training loss: 12079.06, average training loss: 12750.81, base loss: 21653.39
[INFO 2017-06-28 19:48:56,779 main.py:51] epoch 4702, training loss: 13318.66, average training loss: 12751.65, base loss: 21653.90
[INFO 2017-06-28 19:48:58,265 main.py:51] epoch 4703, training loss: 13293.29, average training loss: 12751.76, base loss: 21653.11
[INFO 2017-06-28 19:48:59,617 main.py:51] epoch 4704, training loss: 13087.69, average training loss: 12752.12, base loss: 21652.99
[INFO 2017-06-28 19:49:00,972 main.py:51] epoch 4705, training loss: 13259.71, average training loss: 12752.45, base loss: 21653.40
[INFO 2017-06-28 19:49:02,545 main.py:51] epoch 4706, training loss: 14529.76, average training loss: 12754.51, base loss: 21656.60
[INFO 2017-06-28 19:49:03,873 main.py:51] epoch 4707, training loss: 13230.05, average training loss: 12753.38, base loss: 21655.34
[INFO 2017-06-28 19:49:05,345 main.py:51] epoch 4708, training loss: 11782.83, average training loss: 12750.71, base loss: 21652.15
[INFO 2017-06-28 19:49:06,851 main.py:51] epoch 4709, training loss: 11797.94, average training loss: 12750.49, base loss: 21653.15
[INFO 2017-06-28 19:49:08,056 main.py:51] epoch 4710, training loss: 12493.53, average training loss: 12750.83, base loss: 21653.36
[INFO 2017-06-28 19:49:09,534 main.py:51] epoch 4711, training loss: 13370.85, average training loss: 12751.47, base loss: 21654.96
[INFO 2017-06-28 19:49:11,140 main.py:51] epoch 4712, training loss: 13897.81, average training loss: 12752.20, base loss: 21655.78
[INFO 2017-06-28 19:49:12,744 main.py:51] epoch 4713, training loss: 12434.01, average training loss: 12749.91, base loss: 21655.04
[INFO 2017-06-28 19:49:14,117 main.py:51] epoch 4714, training loss: 13354.78, average training loss: 12750.59, base loss: 21657.16
[INFO 2017-06-28 19:49:15,418 main.py:51] epoch 4715, training loss: 11942.87, average training loss: 12749.77, base loss: 21656.70
[INFO 2017-06-28 19:49:16,686 main.py:51] epoch 4716, training loss: 11588.63, average training loss: 12749.84, base loss: 21657.59
[INFO 2017-06-28 19:49:18,034 main.py:51] epoch 4717, training loss: 12240.04, average training loss: 12747.83, base loss: 21654.45
